{"cell_type":{"d764ea49":"code","676528df":"code","2ece4a13":"code","8d960b15":"code","b3a33001":"code","3ed354ec":"code","efb1ef4b":"code","c774cebc":"code","fd975900":"code","6fdf1c13":"code","e077ffe4":"code","1ddf69b4":"code","920b877a":"code","9e3b251b":"code","fac6ebde":"code","da62242f":"code","6fb228ba":"code","a76dfee0":"code","a3e8acb1":"code","9b9a9deb":"code","662b30d6":"code","35790261":"code","dd3ec814":"code","5d22b8a5":"code","a3aac153":"code","5fddb776":"code","c3b4880a":"code","126b1911":"code","5e13451c":"code","8b53e9a7":"code","4cdeede1":"code","2da49b72":"code","9e6edd4a":"code","73754ea9":"markdown","444ef960":"markdown","2b24337d":"markdown","41926a92":"markdown","cf449672":"markdown","7c77ad50":"markdown","0f54b126":"markdown","70444bb8":"markdown","f51e458b":"markdown","ef4b55d3":"markdown","519192d1":"markdown","e858b2cc":"markdown","b1a27be8":"markdown","96b22f3e":"markdown","2846645c":"markdown","366a4bac":"markdown"},"source":{"d764ea49":"import warnings\nimport os\nimport numpy as np \nimport pandas as pd\nfrom pandas.plotting import scatter_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.figure_factory as ff","676528df":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2ece4a13":"train_df = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv')\nprint('Total number of entries in the train dataset are:', len(train_df))\ntrain_df.head()","8d960b15":"fea_df = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/features.csv', index_col = 0)\nprint('Total number of features are:', len(fea_df))\nfea_df.head()","b3a33001":"train_df.info()","3ed354ec":"fea_df.info()","efb1ef4b":"train_df.shape","c774cebc":"fea_df.shape","fd975900":"train_df.dtypes","6fdf1c13":"fea_df.dtypes","e077ffe4":"train_df.describe()","1ddf69b4":"fea_df.describe()","920b877a":"train_df.isnull().sum()","9e3b251b":"train_df.isna().head()","fac6ebde":"fea_df.isnull().sum()","da62242f":"fea_df.isna().head()","6fb228ba":"px.bar(x = train_df.isnull().sum().index,y = train_df.isnull().sum().values, labels = dict(x = \"Attributes\", y = \"Number of Missing Values\"), title= 'Missing Data')","a76dfee0":"null_weights = (train_df['weight'] == 0).sum()\ntotal_weights = len(train_df['weight'])\nnull_weights_per = null_weights \/ total_weights * 100\n\nprint(f'The null weights account to {round(null_weights_per)}%')","a3e8acb1":"plt.figure(figsize = (15, 6))\nplt.pie(((train_df.weight==0).mean(),(1-(train_df.weight==0).mean())), explode = (0, 0.1), labels=(f'Zero Weights\\n{round((train_df.weight==0).mean()*100,2)}%',f'Non Zero Weights\\n{round((1-(train_df.weight==0).mean())*100,2)}%'.format()), colors = ['#990000', '#006600'])\nplt.show()","9b9a9deb":"date_weight_df = pd.DataFrame({'Date' : np.unique(train_df['date'].values), 'Zero_Weights' : train_df[train_df['weight'] == 0.0].groupby(['date']).size().values, 'Non_Zero_Weights' : train_df[train_df['weight'] != 0.0].groupby(['date']).size().values})\ndate_weight_df.head()","662b30d6":"warnings.filterwarnings('ignore')\n\nfig = plt.figure(figsize=(505, 100))\n\nplt.xticks(rotation ='vertical', fontsize = 60)\nplt.yticks(fontsize = 200)\n\nax = fig.add_subplot(111) \nax2 = ax.twinx() \n\ndate_weight_df.Non_Zero_Weights.plot(kind='bar',color='#00ff00',ax=ax, position = 0)\ndate_weight_df.Zero_Weights.plot(kind='bar',color='#ff0000', ax=ax2, position = 1)\n\nax.grid(None, axis = 'x')\nax2.grid(None)\n\nax.set_ylabel('Non Zero Weights', fontsize = 300)\nax2.set_ylabel('Zero Weights', fontsize = 300)\nax.set_xlabel('Time (In Days)',fontsize = 300)\nfig.suptitle('Zero Weights Vs Non Zero Weights per Day', fontsize = 500)\n\nax.set_xlim(-1, 505)\n\nplt.show()","35790261":"(fea_df * 1).T.style.background_gradient(cmap = 'YlGnBu')","dd3ec814":"fea_tags = fea_df.sum(axis = 1)\nfea_tags_dict = {'Features' : fea_tags.index.values, 'Tag Count' : fea_tags.values}\nfea_tags_df = pd.DataFrame(fea_tags_dict)\nplt.figure(figsize = (130, 25))\nplt.xlabel('Features', fontsize = 100)\nplt.ylabel('Tag Count', fontsize = 100)\nplt.title('Tag Counts of Features', fontsize = 120)\nplt.xticks(rotation ='vertical', fontsize = 50)\nplt.yticks(fontsize = 50)\nsns.barplot(x = 'Features', y = 'Tag Count', data = fea_tags_df, palette = \"rocket\")\nplt.show()","5d22b8a5":"fea_corr = train_df.iloc[:, 7 : 137].corr()","a3aac153":"px.imshow(fea_corr, labels = dict(x = \"Features\", y = \"Features\"), width = 1000, height = 1000, title = \"Correlation between Features\")","5fddb776":"plt.figure(figsize = (20, 5))\nfig = sns.heatmap(train_df.corr().iloc[2 : 7, 7 : -6], cmap = 'Paired')\nfig.set(xlabel = 'Resps', ylabel = 'Features', title = 'Correlation between Resps and Features')\nplt.show()","c3b4880a":"sns.pairplot(fea_corr.iloc[17 : 27, 17 : 27])\nplt.show()","126b1911":"for i in range(17, 27):\n\n    fig, axes = plt.subplots(2, 2, figsize=(12,12))\n\n    sns.scatterplot(data = train_df, x = f\"feature_{str(i)}\", y = \"resp\", ax = axes[0, 0], color = 'red')\n    axes[0,0].set_title(f\"Feature {str(i)} and Resp\", fontsize = 12)\n    axes[0,0].legend(labels=[f'Feature {str(i)}'])\n\n    sns.scatterplot(data = train_df, x = f\"feature_{str(i)}\", y = \"resp_1\", ax = axes[0, 1], color = 'blue')\n    axes[0,1].set_title(f\"Feature {str(i)} and Resp 1\", fontsize = 12)\n    axes[0,1].legend(labels=[f'Feature {str(i)}'])\n\n    sns.scatterplot(data = train_df, x = f\"feature_{str(i)}\", y = \"resp_2\", ax = axes[1, 0], color = 'green')\n    axes[1,0].set_title(f\"Feature {str(i)} and Resp 2\", fontsize = 12)\n    axes[1,0].legend(labels=[f'Feature {str(i)}'])\n\n    sns.scatterplot(data = train_df, x = f\"feature_{str(i)}\", y = \"resp_3\", ax = axes[1, 1], color = 'yellow')\n    axes[1,1].set_title(f\"Feature {str(i)} and Resp 3\", fontsize = 12)\n    axes[1,1].legend(labels=[f'Feature {str(i)}'])\n    \n    plt.show()","5e13451c":"matrix = np.triu(train_df.iloc[ : , 2 : 7].corr())\nplt.figure(figsize = (8, 8))\nfig = sns.heatmap(train_df.iloc[ : , 2 : 7].corr(), annot=True, mask=matrix, cmap = \"viridis\")\nfig.set(xlabel = 'Resps', ylabel = 'Resps', title = 'Correlation between Resps')\nplt.show()","8b53e9a7":"resps = train_df['resp']\nresps1 = train_df['resp_1']\nresps2 = train_df['resp_2']\nresps3 = train_df['resp_3']\nresps4 = train_df['resp_4']\nweights = train_df['weight'].cumsum()\n\nreturns = resps.cumsum() * weights\nreturns1 = resps1.cumsum() * weights\nreturns2 = resps2.cumsum() * weights\nreturns3 = resps3.cumsum() * weights\nreturns4 = resps4.cumsum() * weights\n\nplt.figure(figsize = (10, 5))\nreturns.plot(lw = 2, color = 'darkolivegreen')\nreturns1.plot(lw = 2, color = 'sienna')\nreturns2.plot(lw = 2, color = 'darkslategray')\nreturns3.plot(lw = 2, color = 'darkmagenta')\nreturns4.plot(lw = 2, color = 'darkgoldenrod')\n\nplt.legend(['resp', 'resp1', 'resp2', 'resp3', 'resp4'])\nplt.xlabel('Time (In Days)', fontsize = 15)\nplt.ylabel('Cumulative Resps', fontsize = 15)\nplt.title('Trade Returns', fontsize = 20)\nplt.show()","4cdeede1":"train_df['weight_x_resp'] = train_df['weight'] * train_df['resp']\ntrain_df['weight_x_resp1'] = train_df['weight'] * train_df['resp_1'] \ntrain_df['weight_x_resp2'] = train_df['weight'] * train_df['resp_2'] \ntrain_df['weight_x_resp3'] = train_df['weight'] * train_df['resp_3'] \ntrain_df['weight_x_resp4'] = train_df['weight'] * train_df['resp_4'] \n\nreturns = (1 + train_df.groupby('date')['weight_x_resp'].mean()).cumprod()\nreturns1 = (1 + train_df.groupby('date')['weight_x_resp1'].mean()).cumprod()\nreturns2 = (1 + train_df.groupby('date')['weight_x_resp2'].mean()).cumprod()\nreturns3 = (1 + train_df.groupby('date')['weight_x_resp3'].mean()).cumprod()\nreturns4 = (1 + train_df.groupby('date')['weight_x_resp4'].mean()).cumprod()\n\nplt.figure(figsize = (10, 5))\nreturns.plot(lw = 2, color = 'darkolivegreen')\nreturns1.plot(lw = 2, color = 'sienna')\nreturns2.plot(lw = 2, color = 'darkslategray')\nreturns3.plot(lw = 2, color = 'darkmagenta')\nreturns4.plot(lw = 2, color = 'darkgoldenrod')\n\nplt.legend(['resp', 'resp1', 'resp2', 'resp3', 'resp4'])\nplt.xlabel('Time (In Days)', fontsize = 15)\nplt.ylabel('Cumulative Resps', fontsize = 15)\nplt.title('Trade Returns', fontsize = 20)\nplt.show()","2da49b72":"per_day_trades = train_df.groupby(['date'])['ts_id'].count()\ntime_df = pd.DataFrame({'Date' : per_day_trades.index.values, 'Number of ts_ids' : per_day_trades.values})\n\npx.bar(time_df, x = 'Date', y = 'Number of ts_ids', title = 'Total number of ts_ids per day', color = 'Number of ts_ids')","9e6edd4a":"time_df.describe().style.background_gradient(cmap = 'inferno')","73754ea9":"# **Missing Data**","444ef960":"Features seem to be forming clusters in the above correlation matrix. Features 17 to 26, 27 to 36, and 120 to 129 are some of the many examples shown. These are positively inclined to eachother. In a cluster, the intra cluster distance is lower than the inter cluster distance. Similarly, certain features are clearly negatively related to other features. Amongst the neutral grid, the postive and negative associations stand out!","2b24337d":"# **ts_ids**","41926a92":"# **Preliminary Analysis**","cf449672":"# **Resps**","7c77ad50":"Features 7, 8, 17, 18, 27, 28 seem have very high number of missing values. They follow a pattern of 7s and 8s in the unit digit. Similarly features 72, 78, 84, 90, 96, 102, 108, 114 have high missing values too. They also follow a pattern of multiples of 6. These patterns are interesting to notice","0f54b126":"# **Features**","70444bb8":"# **Tags**","f51e458b":"# **TIME SERIES ANALYSIS**\n\n![thumb76_76-min.png](attachment:414d43fd-7bfa-468a-bc57-9a6a55017f71.png)\n\n**Time series analysis is the examination of a series of data points ordered in time. Time is often independent to other features in the data and these usually help in forecasting future trends. There are many angles to time series data and can get quite complex and overwhleming while aprroching it. A good example of time series data is the stock market data.**","ef4b55d3":"We can observe that the non zero weighed entries account to a much higher ratio when compared to zero weighed entries per day over time. Overall, the trend has been that way. However, we can also see that in a handful of days, zero weighed entries are higher than the non weighed entries. For example, on day 168. ","519192d1":"Features are either postively, negatively, or neutrally correlated to Resps. A pattern can be observed in the above heatmap which allows us to explore and dig deeper into their distributions. All the greens indicate negative association, blues indicate very negative association, reds indicate no association, oranges indicate positive association, purples, yellow, and brown indicate increasingly postive association in order.","e858b2cc":"Features 72 to 119 all have 4 tags. In addition features 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, and 36 also have 4 tags which are features that are multiples of 2. On the flip side, features 41, 42, 43, 47, 48, 51, 52, 64, and 69 have only 1 tag.","b1a27be8":"# **Description**\n\n**This notebook intends to simplify difficult time series data from the Jane Street Market using Descriptive Statistics and Exploratory Data Analysis. This involves statistics, exploring each feature and its relationship with others, visualizing them, and finally drawing conclusions. Please upvote and share the notebook if you found it helpful in any way. Thank you in advance!**\n\n **FYI: This notebook is a work in progress, hence keep an eye out for upcoming updates!**\n\n![saupload_haRo2cyOqWSIcZvXAEYu2WIPrvBz4aFTB_bs6SughZCxWLGdo6G68WOBVJ6rkA9dCGXamwyO3E94sHzZxBto39wexVWOA1G_yUk6bgYM1fdBigkxZlG4cdA89G6RNA7rz_543iJn-min.png](attachment:a253d0bc-0123-4563-920d-4de9faf2c65d.png)\n\n# **Content**\n\n* Import Packages\n* Data Loading\n* Preliminary Analysis\n* Missing Data\n* Weight\n* Tags\n* Features\n* ts_id","96b22f3e":"# **Import Packages**","2846645c":"# **Data Loading**","366a4bac":"# **Weights**"}}