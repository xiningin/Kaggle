{"cell_type":{"87d29fe7":"code","a6cb66ae":"code","d27bd410":"code","8e88dc04":"code","1802afe1":"code","4cae6b5f":"code","67afe5c5":"code","41406b55":"code","a2f93937":"code","ad31808b":"code","6e99d97c":"code","653b460e":"code","4f478a6c":"code","fbe25b29":"code","8a1db10d":"code","ec64ccd3":"code","c0cb9adb":"code","40d147f7":"code","03041c03":"code","a1b2935a":"code","c0e37d01":"code","5b96c2f1":"code","a228d435":"code","8e533142":"code","e5ff0054":"code","c5875fb7":"code","b20dfceb":"code","3edba34e":"code","a0b50b8e":"code","38cdeb77":"code","8374f0b7":"code","10f8e671":"code","5d2401a0":"markdown","cbb8577a":"markdown","ff7291b0":"markdown","f0926f25":"markdown","9c5e89ea":"markdown","2db5ab62":"markdown","7a5aafc9":"markdown","92b7beca":"markdown","f79afaa1":"markdown","6dc22188":"markdown","2ada15ac":"markdown","63312dfb":"markdown","37e25426":"markdown","b80cf20d":"markdown","e755e8a2":"markdown"},"source":{"87d29fe7":"!pip install innvestigate ","a6cb66ae":"%matplotlib inline\n\nimport imp\nimport keras.backend\nimport keras.models\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport os\nimport pickle\nimport time\nimport keras\n\nfrom keras.datasets import mnist\nfrom keras.models import Model\nfrom keras import optimizers\n\nfrom matplotlib import cm, transforms\n\nimport innvestigate\nimport innvestigate.applications\nimport innvestigate.applications.mnist\nimport innvestigate.utils as iutils\nimport innvestigate.utils.visualizations as ivis\nfrom innvestigate.utils.tests.networks import base as network_base","d27bd410":"%%bash\nif [ ! -d \".\/stanfordSentimentTreebank\" ]; then\n    curl -L http:\/\/nlp.stanford.edu\/~socherr\/stanfordSentimentTreebank.zip -O && unzip stanfordSentimentTreebank.zip\nelse\n    echo \"The data is already there. Skip downloading!!\"\nfi","8e88dc04":"%%bash\nif [ ! -e \".\/stanfordSentimentTreebank\/embeddings.npy\" ]; then\n    curl -L https:\/\/github.com\/ArrasL\/LRP_for_LSTM\/raw\/master\/model\/embeddings.npy -o stanfordSentimentTreebank\/embeddings.npy &&\n        curl -L https:\/\/github.com\/ArrasL\/LRP_for_LSTM\/raw\/master\/model\/vocab -o stanfordSentimentTreebank\/vocab\nelse\n    echo \"The data is already there. Skip downloading!!\"\nfi","1802afe1":"!pip install ftfy","4cae6b5f":"from ftfy import fix_encoding","67afe5c5":"DATA_PATH = '.\/stanfordSentimentTreebank'","41406b55":"with open('%s\/vocab' % DATA_PATH, 'rb') as f:\n    vocabs = pickle.load(f) \n    total_vocabs = len(vocabs) \n\n    # Unknown vocabs are set to <UNK>.\n    encoder = dict(zip(['<UNK>'] + vocabs, range(0, len(vocabs) +1)))\n    decoder = dict(zip(encoder.values(), encoder.keys()))\n    \n    print('We have %d vocabs.' % len(encoder))","a2f93937":"pretrained_embedding = np.load('%s\/embeddings.npy' % DATA_PATH)\n\n# Unknown vocabs will have embedding weights of zero.\nembedding = np.zeros((pretrained_embedding.shape[0]+1, pretrained_embedding.shape[1]))\nembedding[1:, :] = pretrained_embedding","ad31808b":"# load all necessary files\ndf_reviews = pd.read_csv('%s\/datasetSentences.txt' % DATA_PATH, sep='\\t')\n\ndf_reviews['phase'] = df_reviews.sentence.apply(lambda s: fix_encoding(s))\\\n    .apply(lambda s: s.replace('-LRB-', '(').replace('-RRB-', ')'))\n\ndf_reviews['sostr'] = pd.read_csv('%s\/SOStr.txt' % DATA_PATH,\n                                  sep='\\t',encoding='utf-8',\n                                  header=None, names=['sostr']\n                                 )\n\ndf_reviews['splitset_label'] = pd.read_csv('%s\/datasetSplit.txt' % DATA_PATH,\n                                           sep=',', header=0\n                                          )['splitset_label']\n\n\ndf_phases = pd.read_csv('%s\/dictionary.txt' % DATA_PATH,\n                        sep='|', names=['phase', 'phase_id']\n                       )\n\ndf_sentiment_labels = pd.read_csv('%s\/sentiment_labels.txt' % DATA_PATH,\n                                  sep='|', names=['phase_id', 'sentiment_value'],\n                                  header=0\n                                 )\n\ndf_reviews_with_sentiment_value = df_reviews.merge(df_phases, how='inner', on=['phase'])\\\n    .merge(df_sentiment_labels, on='phase_id')\n\ndf_reviews_with_sentiment_value[:5]","6e99d97c":"def sentiment_discretizer(sentiment_value):\n    if 0 <= sentiment_value <= 0.2:\n        return 'very_negative'\n    elif 0.2 < sentiment_value <= 0.4:\n        return 'negative'\n    elif 0.4 < sentiment_value <= 0.6:\n        return 'neutral'\n    elif 0.6 < sentiment_value <= 0.8:\n        return 'positive'\n    elif 0.8 < sentiment_value <= 1:\n        return 'very_positive'\n    \ndf_reviews_with_sentiment_value['label'] = df_reviews_with_sentiment_value.sentiment_value.apply(sentiment_discretizer)","653b460e":"df_reviews_with_sentiment_value[:5]","4f478a6c":"len(df_reviews_with_sentiment_value)","fbe25b29":"LABEL_MAPPING = {\n    'very_negative': 0,\n    'negative': 0,\n    'positive': 1, \n    'very_positive': 1\n}\n\nLABEL_IDX_TO_NAME = {\n    0: 'negative',\n    1: 'positive'\n}","8a1db10d":"# Please use the following mappings for Five-class Classification\n# LABEL_MAPPING = dict(zip(['very_negative', 'negative', 'neutral', 'positive', 'very_positive'], range(5)))\n# LABEL_IDX_TO_NAME = dict(zip(LABEL_MAPPING.values(), LABEL_MAPPING.keys()))","ec64ccd3":"NUM_CLASSES = len(set(LABEL_MAPPING.values()))\nprint('We have %d classes.' % NUM_CLASSES)","c0cb9adb":"filtered_indices = df_reviews_with_sentiment_value.label.apply(lambda l: l in LABEL_MAPPING)\n\ndf_reviews_with_sentiment_value_filtered = df_reviews_with_sentiment_value.loc[filtered_indices].copy()\ndf_reviews_with_sentiment_value_filtered.loc[:, 'label_idx'] = df_reviews_with_sentiment_value_filtered.label\\\n    .apply(lambda l: LABEL_MAPPING[l])\n\ndf_reviews_with_sentiment_value_filtered[:5]","40d147f7":"SPLIT_LABEL_MAPPING = {\n    'training' : 1,\n    'testing': 2,\n    'validation': 3\n}\n\nMAX_SEQ_LENGTH = 40\nEMBEDDING_DIM = embedding.shape[1]","03041c03":"def prepare_dataset(ds):\n    filtered_indices = df_reviews_with_sentiment_value_filtered.splitset_label == SPLIT_LABEL_MAPPING[ds]\n    \n    reviews_in_ds = df_reviews_with_sentiment_value_filtered[filtered_indices]\n    \n    xd = np.zeros((len(reviews_in_ds), MAX_SEQ_LENGTH, EMBEDDING_DIM))\n    y = reviews_in_ds.label_idx.values.astype(int)\n    \n    reviews = []\n    for i, sostr in enumerate(reviews_in_ds.sostr.values):\n        sostr = sostr.lower()\n        review = []\n        for j, v in enumerate(sostr.split('|')[:MAX_SEQ_LENGTH]):\n            if v in encoder:\n                e_idx = encoder[v]\n            else:\n                e_idx = 0\n            \n            xd[i, j, :] = embedding[e_idx]\n            review.append(e_idx)\n        reviews.append(review)\n        \n\n    return dict(\n        x4d=np.expand_dims(xd, axis=1),\n        y=y,\n        encoded_reviews=reviews\n    )\n    \n\nDATASETS = dict()\n\nfor ds in ['training', 'testing', 'validation']:\n    DATASETS[ds] = prepare_dataset(ds)\n","a1b2935a":"len(df_reviews_with_sentiment_value), len(df_reviews_with_sentiment_value_filtered)","c0e37d01":"print('We have %d reviews in the training set, and %d reviews in the testing set' % \n      (len(DATASETS['training']['x4d']), len(DATASETS['testing']['x4d']))\n     )","5b96c2f1":"sample_idx = 1225\n\nprint('Review(ID=%d): %s' %\n      (sample_idx, ' '.join(map(lambda x: decoder[x], DATASETS['training']['encoded_reviews'][sample_idx]))))","a228d435":"def build_network(input_shape, output_n, activation=None, dense_unit=256, dropout_rate=0.25):\n    if activation:\n        activation = \"relu\"\n\n    net = {}\n    net[\"in\"] = network_base.input_layer(shape=input_shape)\n    net[\"conv\"] = keras.layers.Conv2D(filters=100, kernel_size=(1,2), strides=(1, 1), padding='valid')(net[\"in\"])\n    net[\"pool\"] = keras.layers.MaxPooling2D(pool_size=(1, input_shape[2]-1), strides=(1,1))(net[\"conv\"])\n    net[\"out\"] = network_base.dense_layer(keras.layers.Flatten()(net[\"pool\"]), units=output_n, activation=activation)\n    net[\"sm_out\"] = network_base.softmax(net[\"out\"])\n\n\n    net.update({\n        \"input_shape\": input_shape,\n\n        \"output_n\": output_n,\n    })\n    return net\n\nnet = build_network((None, 1, MAX_SEQ_LENGTH, EMBEDDING_DIM), NUM_CLASSES)\nmodel_without_softmax, model_with_softmax = Model(inputs=net['in'], outputs=net['out']), Model(inputs=net['in'], outputs=net['sm_out'])","8e533142":"def to_one_hot(y):\n    return keras.utils.to_categorical(y, NUM_CLASSES)\n\ndef train_model(model,  batch_size=128, epochs=20):\n    \n    x_train = DATASETS['training']['x4d']\n    y_train = to_one_hot(DATASETS['training']['y'])\n    \n    x_test = DATASETS['testing']['x4d']\n    y_test = to_one_hot(DATASETS['testing']['y'])\n    \n    x_val = DATASETS['validation']['x4d']\n    y_val = to_one_hot(DATASETS['validation']['y'])\n    \n    model.compile(loss='categorical_crossentropy',\n                  optimizer=optimizers.Adam(),\n                  metrics=['accuracy'])\n\n    history = model.fit(x_train, y_train,\n                        batch_size=batch_size,\n                        epochs=epochs,\n                        verbose=1,\n                        validation_data=(x_val, y_val),\n                        shuffle=True\n                       )\n    score = model.evaluate(x_test, y_test, verbose=0)\n    print('Test loss:', score[0])\n    print('Test accuracy:', score[1])","e5ff0054":"train_model(model_with_softmax, batch_size=256, epochs=10)","c5875fb7":"model_without_softmax.set_weights(model_with_softmax.get_weights())","b20dfceb":"# Specify methods that you would like to use to explain the model. \n# Please refer to iNNvestigate's documents for available methods.\nmethods = ['gradient', 'lrp.z', 'lrp.alpha_2_beta_1', 'pattern.attribution']\nkwargs = [{}, {}, {}, {'pattern_type': 'relu'}]","3edba34e":"# build an analyzer for each method\nanalyzers = []\n\nfor method, kws in zip(methods, kwargs):\n    analyzer = innvestigate.create_analyzer(method, model_without_softmax, **kws)\n    analyzer.fit(DATASETS['training']['x4d'], batch_size=256, verbose=1)\n    analyzers.append(analyzer)","a0b50b8e":"# specify indices of reviews that we want to investigate\ntest_sample_indices = [97, 175, 1793, 1186, 354, 1043]\n\ntest_sample_preds = [None]*len(test_sample_indices)\n\n# a variable to store analysis results.\nanalysis = np.zeros([len(test_sample_indices), len(analyzers), 1, MAX_SEQ_LENGTH])\n\nfor i, ridx in enumerate(test_sample_indices):\n\n    x, y = DATASETS['testing']['x4d'][ridx], DATASETS['testing']['y'][ridx]\n\n    t_start = time.time()\n    x = x.reshape((1, 1, MAX_SEQ_LENGTH, EMBEDDING_DIM))    \n\n    presm = model_without_softmax.predict_on_batch(x)[0] #forward pass without softmax\n    prob = model_with_softmax.predict_on_batch(x)[0] #forward pass with softmax\n    y_hat = prob.argmax()\n    test_sample_preds[i] = y_hat\n    \n    for aidx, analyzer in enumerate(analyzers):\n\n        a = np.squeeze(analyzer.analyze(x))\n        a = np.sum(a, axis=1)\n\n        analysis[i, aidx] = a\n    t_elapsed = time.time() - t_start\n    print('Review %d (%.4fs)'% (ridx, t_elapsed))","38cdeb77":"# This is a utility method visualizing the relevance scores of each word to the network's prediction. \n# one might skip understanding the function, and see its output first.\ndef plot_text_heatmap(words, scores, title=\"\", width=10, height=0.2, verbose=0, max_word_per_line=20):\n    fig = plt.figure(figsize=(width, height))\n    \n    ax = plt.gca()\n\n    ax.set_title(title, loc='left')\n    tokens = words\n    if verbose > 0:\n        print('len words : %d | len scores : %d' % (len(words), len(scores)))\n\n    cmap = plt.cm.ScalarMappable(cmap=cm.bwr)\n    cmap.set_clim(0, 1)\n    \n    canvas = ax.figure.canvas\n    t = ax.transData\n\n    # normalize scores to the followings:\n    # - negative scores in [0, 0.5]\n    # - positive scores in (0.5, 1]\n    normalized_scores = 0.5 * scores \/ np.max(np.abs(scores)) + 0.5\n    \n    if verbose > 1:\n        print('Raw score')\n        print(scores)\n        print('Normalized score')\n        print(normalized_scores)\n\n    # make sure the heatmap doesn't overlap with the title\n    loc_y = -0.2\n\n    for i, token in enumerate(tokens):\n        *rgb, _ = cmap.to_rgba(normalized_scores[i], bytes=True)\n        color = '#%02x%02x%02x' % tuple(rgb)\n        \n        text = ax.text(0.0, loc_y, token,\n                       bbox={\n                           'facecolor': color,\n                           'pad': 5.0,\n                           'linewidth': 1,\n                           'boxstyle': 'round,pad=0.5'\n                       }, transform=t)\n\n        text.draw(canvas.get_renderer())\n        ex = text.get_window_extent()\n        \n        # create a new line if the line exceeds the length\n        if (i+1) % max_word_per_line == 0:\n            loc_y = loc_y -  2.5\n            t = ax.transData\n        else:\n            t = transforms.offset_copy(text._transform, x=ex.width+15, units='dots')\n\n    if verbose == 0:\n        ax.axis('off')","8374f0b7":"plot_text_heatmap(\n    \"I really love this movie but not in the beginning\".split(' '),\n    np.array([0.02, 0.2, 0.5, 0.1, 0.1, 0.1, -0.2, 0.05, 0.00, 0.08])\n)\n\n# \"love\" is shaded with strong red because its relevance score is rather high\n# \"not\" is highlighted in light blue because of its negative score.","10f8e671":"# Traverse over the analysis results and visualize them.\nfor i, idx in enumerate(test_sample_indices):\n\n    words = [decoder[t] for t in list(DATASETS['testing']['encoded_reviews'][idx])]\n    \n    print('Review(id=%d): %s' % (idx, ' '.join(words)))\n    y_true = DATASETS['testing']['y'][idx]\n    y_pred = test_sample_preds[i]\n\n    print(\"Pred class : %s %s\" %\n          (LABEL_IDX_TO_NAME[y_pred], '\u2713' if y_pred == y_true else '\u2717 (%s)' % LABEL_IDX_TO_NAME[y_true])\n         )\n                                \n    for j, method in enumerate(methods):\n        plot_text_heatmap(words, analysis[i, j].reshape(-1), title='Method: %s' % method, verbose=0)\n        plt.show()","5d2401a0":"# Model Construction\n\nOur classifier is a convolutional neural network, which was experimented in [Arras et al. (2017b)][arras2]. As shown below, the architecture has a convoluationa layer, convolving word embeddings of every two words, followed by a max pooling layer and a softmax layer.\n\n![][arch]\n\n[arch]: https:\/\/i.imgur.com\/YQDfS5P.png\n[arras2]: https:\/\/journals.plos.org\/plosone\/article?id=10.1371\/journal.pone.0181142","cbb8577a":"Lastly, due to an encoding issue in the dataset, we need to install the `ftfy` package for fixing the issue.","ff7291b0":"# Model Analysis and Visualization\n\nAt this stage, we have a trained model and are ready to explain it via **iNNvestigate**'s analyzers.","f0926f25":"Because the goal of this notebook is to demonstrate how to apply **iNNvestigate** to text, we will simplify the problem (Five-class Classification) to a binary classification. Class 0 will contain very_negative and negative reviews, while Class 1 include very_positive and positive reviews. Neutral reviews are excluded.","9c5e89ea":"# Data Preparation\n\n## Downloading The Dataset and Word Embedding\n\nIn the first step, we download the dataset from `http:\/\/nlp.stanford.edu\/~socherr\/stanfordSentimentTreebank.zip`.\n\nFor an *Unix* environment, one can achieve it by using the command below.","2db5ab62":"In this example, we are going to build a sentiment analysis classifer, inspired by experiments in [Arras et al. (2017a)][arras] and [Arras et al. (2017b)][arras2]. In particular, we are going to predict sentiments of movie reviews, and apply explanation methods provided by iNNvestigate to analyze how words in each review influence the review's sentiment prediction.\n\nThe dataset that we are going to use is [Standford Sentiment Treebank][standford], which has reviews in five categories: *very negative, negative, neutral, positive,* and *very positive*. In this example, we are interested in building a binary classifier, classifying negative and positive reviews (including their extreme classes). Neutral reviews are excluded. Nevertheless, we still provide a possibilty to build a five-class classifer in Section 2.2.\n\n\nThis example is organized as follows: First, we obtain the dataset from the source as well as pretrained word embedding. Secondly, we prepare the dataset for training a neural network. Then, we construct a neural network model, receiving reviews as input and predicting their sentiments. Finally, we apply various explanation methods implemented in iNNvestigate to explain decisions from a trained model. The figure below is explanations of a review that we expect to see: red indicates a high relevance score in favour of the prediction, while blue is the opposite.\n\n![][sample]\n\n[arras]: http:\/\/www.aclweb.org\/anthology\/W16-1601\n[arras2]: https:\/\/journals.plos.org\/plosone\/article?id=10.1371\/journal.pone.0181142\n[standford]: https:\/\/nlp.stanford.edu\/sentiment\/\n[sample]: https:\/\/i.imgur.com\/IRQL5oh.png","7a5aafc9":"Secondly, as we are going to use an embedding layer as the first layer of our model, embedding weights are required. Fortuanately, there are pretrained word embedding available for this dataset, so we do not need to train it from scratch. The pre-trained embedding is provided by L. Arras at [LRP_for_LSTM][leila_lstm_repo]. Therefore, we can simply download the embedding weights and vocabulary files.\n```\n- https:\/\/github.com\/ArrasL\/LRP_for_LSTM\/raw\/master\/model\/embeddings.npy\n- https:\/\/github.com\/ArrasL\/LRP_for_LSTM\/raw\/master\/model\/vocab\n```\n\n[leila_lstm_repo]: https:\/\/github.com\/ArrasL\/LRP_for_LSTM\/tree\/master\/model","92b7beca":"## Visualization\n\nTo this point, we have all analysis results from iNNvestigate's analyzers, and we are now ready to visualize them in a insightful way. We will use relevance scores from explanation methods to highlight the words in each review. \n\nWe will use  the *blue-white-red (bwr)* color map for this purpose. Hence, words that have a positive score to the prediction are be shaded in *red*, while  negative-contribution or zero-contribution words are then highlighted in *blue*, and *white*, respectively.\n","f79afaa1":"Next, we need to decretize `sentiment_values` to labels. According to `.\/stanfordSentimentTreebank\/README.md`, we should use the  below discretization scheme.\n\n| Label | Sentiment Value Range |\n|-------|-----------------------|\n|   very_negative    |    [0, 0.2]                   |\n|    negative   |         (0.2, 0.4]              |\n|    neutral   |         (0.4, 0.6]               |\n|    positive   |        (0.6, 0.8]               |\n|    very_positive   |         (0.8, 1]               |\n\n","6dc22188":"## Data Preprocessing","2ada15ac":"iNNvestigate can be installed with the following commands. The library is based on Keras and therefore requires a supported Keras-backend o(Currently only the Tensorflow backend is supported. We test with Python 3.6, Tensorflow 1.12 and Cuda 9.x.):\n\npip install innvestigate\n# Installing Keras backend\npip install [tensorflow | theano | cntk]\nTo use the example scripts and notebooks one additionally needs to install the package matplotlib:\n\npip install matplotlib\nThe library's tests can be executed via:\n\ngit clone https:\/\/github.com\/albermax\/innvestigate.git\ncd innvestigate\npython setup.py test","63312dfb":"\n### An Example of  The Visualization\nLet assume we have this review `I really love this movie but not in the beginning`. It is predicted as `positive`, and the relevance scores are distributed as follows: \n\n```\n        I 0.20\n    really 0.20\n      love 0.50\n      this 0.10\n     movie 0.10\n       but 0.10\n       not -0.20\n        in 0.05\n       the 0.05\n beginning 0.08\n```","37e25426":"# Introduction","b80cf20d":"## Spiting Training, Testing, and Validation Set.","e755e8a2":"if you are using tensorflow tf2 this will note work for you so you should install tf==1.12.0"}}