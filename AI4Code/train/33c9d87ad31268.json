{"cell_type":{"50fd556b":"code","713c6309":"code","51f85fa3":"code","b32311fe":"code","9c2ee123":"code","5c59606e":"code","5810c7dd":"code","41252cdc":"code","083a4fcc":"code","61cf96be":"markdown","c6c4bba1":"markdown","57e511a1":"markdown","499514cf":"markdown","80e2f8ec":"markdown","e4c9cc05":"markdown","5ae725a8":"markdown","58cb393b":"markdown","dbd09ab1":"markdown","f510d52c":"markdown","b4881605":"markdown","28649e48":"markdown","a8faf205":"markdown","1e0cd077":"markdown","bd57a50d":"markdown","6b5e9e48":"markdown"},"source":{"50fd556b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    print(dirname)\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","713c6309":"#additional imports\nimport glob\nimport json\nimport gensim\nimport nltk\nimport pickle #for writing to output\nimport gc","51f85fa3":"def readarticle(filepath):\n    paperdata = {\"paper_id\" : None, \"title\" : None, \"abstract\" : None}\n    with open(filepath) as file:\n        filedata = json.load(file)\n        paperdata[\"paper_id\"] = filedata[\"paper_id\"]\n        paperdata[\"title\"] = filedata[\"metadata\"][\"title\"]\n                \n        if \"abstract\" in filedata:\n            abstract = []\n            for paragraph in filedata[\"abstract\"]:\n                abstract.append(paragraph[\"text\"])\n            abstract = '\\n'.join(abstract)\n            paperdata[\"abstract\"] = abstract\n            #print(abstract)\n        else:\n            paperdata[\"abstract\"] = []\n            #print(\"no abstract\")\n\n        #body_text = []\n        #for paragraph in filedata[\"body_text\"]:\n        #    body_text.append(paragraph[\"text\"])\n        #body_text = '\\n'.join(body_text)\n        #paperdata[\"body_text\"] = body_text\n        \n    return paperdata\n\ndef read_multiple(jsonfiles_pathnames):\n    papers = {\"paper_id\" : [], \"title\" : [], \"abstract\" : []}\n    for filepath in jsonfiles_pathnames:\n        paperdata = readarticle(filepath)\n        if len(paperdata[\"abstract\"]) > 0: \n            papers[\"paper_id\"].append(paperdata[\"paper_id\"])\n            papers[\"title\"].append(paperdata[\"title\"])\n            papers[\"abstract\"].append(paperdata[\"abstract\"])\n            #papers[\"body_text\"].append(paperdata[\"body_text\"])\n            #print(\"not none\")\n        #else:\n            #print(\"none\")\n    print(len(papers[\"paper_id\"]))\n    print(len(papers[\"title\"]))\n    print(len(papers[\"abstract\"]))\n    gc.collect()\n    return papers","b32311fe":"def make_bigram(tokenized_data, min_count = 5, threshold = 100):\n    bigram_phrases = gensim.models.Phrases(tokenized_data, min_count = min_count, threshold = threshold)\n    #after Phrases a Phraser is faster to access\n    bigram = gensim.models.phrases.Phraser(bigram_phrases)\n    gc.collect()\n    return bigram\n\ndef make_trigram(tokenized_data, min_count = 5, threshold = 100):\n    bigram_phrases = gensim.models.Phrases(tokenized_data, min_count = min_count, threshold = threshold)\n    trigram_phrases = gensim.models.Phrases(bigram_phrases[tokenized_data], threshold = 100)\n    #after Phrases a Phraser is faster to access\n    trigram = gensim.models.phrases.Phraser(trigram_phrases)\n    gc.collect()\n    return trigram\n    ","9c2ee123":"def readjson_retbodytext(jsonfiles_pathnames):\n    print(\"reading json files\")\n    documents = read_multiple(jsonfiles_pathnames)\n    print(\"writing documents dictionary to output for use in another kernel\")\n    with open(\"documents_dict.pkl\", 'wb') as f:\n        pickle.dump(documents, f)\n    print(\"done writing documents dict.  Format is paper_id, title, body_text\")\n    gc.collect()\n    return documents[\"abstract\"]\n    \ndef open_tokenize(jsonfiles_pathnames):\n    \n    body_text = readjson_retbodytext(jsonfiles_pathnames)\n    \n    print(\"removing stopwords, steming, and tokenizing.  This is expensive\")\n    tokenized_documents = gensim.parsing.preprocessing.preprocess_documents(body_text)\n    print(\"done preprocessing documents. now writing to output to be used in another documents\")\n    with open(\"tokenized_documents.pkl\", 'wb') as f:\n        pickle.dump(tokenized_documents, f)\n    print(\"done writing file\")\n    \n    gc.collect()\n    return tokenized_documents\n    ","5c59606e":"metadata = pd.read_csv(\"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\")\n#metadata.head()\n#print(metadata.shape)\nmetadata.info()","5810c7dd":"jsonfiles_pathnames = glob.glob(\"\/kaggle\/input\/CORD-19-research-challenge\/**\/*.json\", recursive = True)\nprint(len(jsonfiles_pathnames))","41252cdc":"\ntokenized_documents = open_tokenize(jsonfiles_pathnames)","083a4fcc":"print(\"creating bigram model.  this is expensive, skipping trigrams\")\nbigram_model = make_bigram(tokenized_documents)\nprint(\"done creating bigram model. Writing model\")\nwith open(\"bigram_model.pkl\", 'wb') as f:\n    pickle.dump(bigram_model, f)\nprint(\"done writing bigram model\")","61cf96be":"# Create bigram model","c6c4bba1":"# Remove stopwords, stem, and tokenize documents","57e511a1":"# Definitions for reading the json files.","499514cf":"In this section we create and save the bigram model.  This just contains the word pair.  We will have to run the tokenized documents through the model in order to create a bigram tokenized version of the documents.  We will do that in another step therefore we save the model here.","80e2f8ec":"In this section of the code we have additional imports to the standard ones above.  Glob will be used to find all the files we need and json will be used to open the full documents of the CORD-19 dataset.  Gensim and nltk will be used for language preprocessing tasks.  Pickle will be used to write results to output so that we can break up the preprocessing and model learning tasks and speed up testing and applying changes.","e4c9cc05":"# Bigram and Trigram definitions","5ae725a8":"# Additional Imports","58cb393b":"Reading in the documents was done inside the same function that tokenizes the documents so that the raw documents can go out of scope.  This is done to reduce memory usage.  We will only be looking at papers that have a json file.  We will be saving the resulting dictionary for use later. ","dbd09ab1":"This is the preprocessing portion of an effor to optimize a Latent Dirichlet Allocation (LDA) topic model.  We break up this effort into multiple steps in order to speed up testing and making changes.  This also uses less memory and is less likely to time out.  \n\nA topic model learns a topic-feature matrix of abstract topics and features (word or ngrams) and a document-topic matrix of documents and topics, from a document-feature matrix of documents and features.  From this factorization we achieve statistical feature vectors for each topic and topic vectors for each document in the training corpus.  We can then find topic vectors for the questions we would like to ask the corpus of documents.  We will use the closest matching documents in the CORD-19 dataset in an attempt to answer the task questions.  LDA assumes a Dirichlet prior on topic-feature and document-topic distributions.  In other words it assumes each topic is defined by and small collection of words or ngrams and that each documnent consists of a small number of topics ","f510d52c":"Stopwords are words common to all text, such as \"and\" and \"the,\"  and therefore we remove them.  Stemming reduces words to their root form.  For example, \"infected\" and \"infecting\" will both be reduced to \"infect.\"  Tokenization converts the body text of each paper into a vector of whitespace-separated text (words).  These tokens will be treated as semantic features, particularly after stemming.  When we create the document-feature matrix (term frequency corpus) the exact text of the word (or bigram) will no longer be observed, as these features define the row vectors of that matrix.  Here, we save the tokenized documents.","b4881605":"# Look at metadata","28649e48":"# collect file paths","a8faf205":"# Introduction","1e0cd077":"Below are definitions for making bigrams (sequences of 2 words) and trigrams (collections of 3 words).  We will be skipping trigrams, but the definition is included in case it is desired later.","bd57a50d":"# Definition for tokenizing documents ","6b5e9e48":"The purpose of these definitions is to read all json files in a path and return a dictionary containing entries for the paper's id, title, abstract, and body text.  These json files are the papers that we have full body text for. \n\nNow that we are up to 50k+ documents we keep running out of memory"}}