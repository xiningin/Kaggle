{"cell_type":{"62066893":"code","2d63812d":"code","d7dea765":"code","ab351075":"code","76ad2034":"code","a53ec56f":"code","ffd8d416":"code","c97b4ee1":"code","3cecc06c":"code","4c5e9068":"code","c7f2ce14":"code","11a309b3":"code","4fafe5e2":"code","f97e5e72":"code","588c0d80":"code","8fb0c1a1":"code","2867fddc":"markdown","e4a2fcfd":"markdown","dc852c92":"markdown","b577bc03":"markdown","df3cff30":"markdown","0520ac87":"markdown","f4fbf17b":"markdown","9eccc49a":"markdown","55989f5a":"markdown","987f18c1":"markdown","9b9fc6cd":"markdown","4fa8d108":"markdown","776aad53":"markdown","41ba1541":"markdown","a5fed319":"markdown","f5fe8a9e":"markdown","51a1abda":"markdown","67391df0":"markdown","78a3e7d1":"markdown","cde313f6":"markdown","dfb8c67e":"markdown","ebe02bcd":"markdown","06f4cca4":"markdown","2b159721":"markdown","5c8cc122":"markdown","2b992dc4":"markdown","c3f95782":"markdown","a174967c":"markdown","86622ad5":"markdown","8ee07fa6":"markdown","aa8fe278":"markdown","f5899e92":"markdown","8d533881":"markdown","88713365":"markdown","cdd0697b":"markdown","e115fbfa":"markdown","e54ae198":"markdown","df85e0c4":"markdown","82125d56":"markdown","ed532d55":"markdown","4a56b1bb":"markdown","2bffbb53":"markdown","2107b264":"markdown","39d53740":"markdown","af324a85":"markdown","b27b1aa0":"markdown","d62c1a9b":"markdown","c02da022":"markdown"},"source":{"62066893":"import pandas as pd  \nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression \nfrom statsmodels.formula.api import ols  \nfrom statsmodels.stats.stattools import durbin_watson\nfrom matplotlib import pyplot as plt \nimport warnings \nwarnings.filterwarnings(\"ignore\")","2d63812d":"df = pd.DataFrame({'rating': [90, 85, 82, 88, 94, 90, 76, 75, 87, 86],\n                   'points': [25, 20, 14, 16, 27, 20, 12, 15, 14, 19],\n                   'assists': [5, 7, 7, 8, 5, 7, 6, 9, 9, 5],\n                   'rebounds': [11, 8, 10, 6, 6, 9, 6, 10, 10, 7]})","d7dea765":"df.head()","ab351075":"X = df.drop(\"rating\",axis=1) \nY = df[\"rating\"]","76ad2034":"x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size=0.5,random_state=3)","a53ec56f":"lr = LinearRegression()","ffd8d416":"lr.fit(x_train,y_train)","c97b4ee1":"lr.score(x_test,y_test)*100","3cecc06c":"df1 = sns.load_dataset(\"iris\")","4c5e9068":"a = [1,2,3,4,5,6]  ","c7f2ce14":"b = [i*2 for i in a]","11a309b3":"df1.columns","4fafe5e2":"sns.scatterplot(x=a,y=b,cmap=\"green\")","f97e5e72":"model = ols('rating ~ points + assists + rebounds', data=df).fit()","588c0d80":"model.summary()","8fb0c1a1":"durbin_watson(model.resid)","2867fddc":"<h1 style = \"font-family: Comic Sans MS;color:blue\" >What to do if this assumption is violated <\/h1>","e4a2fcfd":"<img src=\"https:\/\/fourpillarfreedom.com\/wp-content\/uploads\/2019\/05\/qqplot3.jpg\"><\/img>","dc852c92":"The next assumption of linear regression is that the residuals are independent. This is mostly relevant when working with time series data. Ideally, we don\u2019t want there to be a pattern among consecutive residuals. For example, residuals shouldn\u2019t steadily grow larger as time goes on.","b577bc03":"<h2 style='border:3px solid DodgerRed;background-color:#ff5cad;font-family:comic sans ms'><center>All You Need Related to Machine learning--->Linear Regression<\/center><\/h2>","df3cff30":"If the normality assumption is violated, you have a few options:\n\n - First, verify that any outliers aren\u2019t having a huge impact on the distribution. If there are outliers present, make sure that they are real values and that they aren\u2019t data entry errors.\n - Next, you can apply a nonlinear transformation to the independent and\/or dependent variable. Common examples include taking the log, the square root, or the reciprocal of the independent and\/or dependent variable.","0520ac87":"# Inspiration :","f4fbf17b":"2. You can also check the normality assumption using formal statistical tests like Shapiro-Wilk, Kolmogorov-Smironov, Jarque-Barre, or D\u2019Agostino-Pearson. However, keep in mind that these tests are sensitive to large sample sizes \u2013 that is, they often conclude that the residuals are not normal when your sample size is large. This is why it\u2019s often easier to just use graphical methods like a Q-Q plot to check this assumption.\n\n","9eccc49a":"Notice how the residuals become much more spread out as the fitted values get larger. ","55989f5a":"**SuperVised learning Algorithms**  \n\n\n**------------------------------------------------**\n<ol>\n    <b><li>Linear Regression<\/li><\/b>\n    <b><li>Logistic Regression <\/li><b>\n    <b><li>Decision Tree<\/li><\/b> \n    <b><li>Ensemble Techniques<\/li><\/b>\n    <b><li>Bagging algorithms<\/li><\/b>\n    <b><li>Boosting algorithms<\/li><\/b>\n    <b><li>Xgboost<\/li><\/b>\n    <b><li>Gradient boosting<\/li><\/b>\n<\/ol>","987f18c1":"The next assumption of linear regression is that the residuals are normally distributed. ","9b9fc6cd":"<p style = \"font-family: Comic Sans MS;color:apple green\"> \n 1. Transform the dependent variable: \n   One common transformation is to simply take the log of the dependent variable. For example, if we are using population size (independent variable) to predict the number of flower shops in a city (dependent variable), we may instead try to use population size to predict the log of the number of flower shops in a city. Using the log of the dependent variable, rather than the original dependent variable, often causes heteroskedasticity to go away.<\/p>","4fa8d108":"There are three common ways to fix heteroscedasticity:","776aad53":"The simplest way to detect heteroscedasticity is by creating a fitted value vs. residual plot. ","41ba1541":"<p style = \"font-family: Comic Sans MS;color:black\">The first assumption of linear regression is that there is a linear relationship between the independent variable, x, and the independent variable, y.<\/p>","a5fed319":"<h3 style = \"font-family: Comic Sans MS;color:Red\">How to check:<\/h3>","f5fe8a9e":"<h1 style = \"font-family: Comic Sans MS;color:Green\">Work flow of Linear Regression:<\/h1>","51a1abda":"<h1 style = \"font-family: Comic Sans MS;color:blue\" >What to do if this assumption is violated <\/h1>","67391df0":"# 1. Linear Relationship :","78a3e7d1":"<h1 style = \"font-family: Comic Sans MS;color:Green\">Assumptions:<\/h1>","cde313f6":"**Before learning about linear regression, let us get ourselves accustomed to regression. Regression is a method of modelling a target value based on independent predictors. It is a statistical tool which is used to find out the relationship between the outcome variable also known as the dependent variable, and one or more variable often called as independent variables.  This method is mostly used for forecasting and finding out cause and effect relationship between variables. Regression techniques mostly differ based on the number of independent variables and the type of relationship between the independent and dependent variables.**","dfb8c67e":"<h3 style = \"font-family: Comic Sans MS;color:Red\">How to check:<\/h3>","ebe02bcd":"<h1 style = \"font-family: Comic Sans MS;color:blue\" >What to do if this assumption is violated <\/h1>","06f4cca4":"<h1 style = \"font-family: Comic Sans MS;color:Green\">Assumption 3: Homoscedasticity<\/h1>","2b159721":"**1.https:\/\/www.youtube.com\/watch?v=1-OGRohmH2s&list=PLZoTAELRMXVPBTrWtJkn3wWQxZkmTXGwe&index=29** \n**2.https:\/\/www.youtube.com\/watch?v=5rvnlZWzox8&list=PLZoTAELRMXVPBTrWtJkn3wWQxZkmTXGwe&index=34**\n**3.https:\/\/www.youtube.com\/watch?v=NAPhUDjgG_s&list=PLZoTAELRMXVPBTrWtJkn3wWQxZkmTXGwe&index=32**\n**4.https:\/\/www.youtube.com\/watch?v=WuuyD3Yr-js&list=PLZoTAELRMXVPBTrWtJkn3wWQxZkmTXGwe&index=35**\n**5.https:\/\/www.youtube.com\/watch?v=BqzgUnrNhFM&list=PLZoTAELRMXVPBTrWtJkn3wWQxZkmTXGwe&index=33**","5c8cc122":"<h1 style = \"font-family: Comic Sans MS;color:yellow\">Assumption 4: Normality<\/h1>","2b992dc4":"<p style = \"font-family: Comic Sans MS;color:apple green\"> \n    2. Redefine the dependent variable.  One common way to redefine the dependent variable is to use a rate, rather than the raw value. For example, instead of using the population size to predict the number of flower shops in a city, we may instead use population size to predict the number of flower shops per capita. In most cases, this reduces the variability that naturally occurs among larger populations since we\u2019re measuring the number of flower shops per person, rather than the sheer amount of flower shops.<\/p>","c3f95782":"** For Theoretical Understanding :**","a174967c":"**Regression is performed when the dependent variable is of continuous data type and Predictors or independent variables could be of any data type like continuous, nominal\/categorical etc. Regression method tries to find the best fit line which shows the relationship between the dependent variable and predictors with least error.**\n\n**In regression, the output\/dependent variable is the function of an independent variable and the coefficient and the error term.**\n\n","86622ad5":"**This Notebook entirely divided into subparts which was specified over here \nFor linear Regression --> How it works \n                          Assumptions  \n                          Model Building\n                          Advantages \n                          Disadvantages**","8ee07fa6":"<h1 style = \"font-family: Comic Sans MS;color:pink\">Reference:<\/h1>","aa8fe278":"Once you fit a regression line to a set of data, you can then create a scatterplot that shows the fitted values of the model vs. the residuals of those fitted values. The scatterplot below shows a typical fitted value vs. residual plot in which heteroscedasticity is present.","f5899e92":"1. Check the assumption visually using Q-Q plots.\n\nA Q-Q plot, short for quantile-quantile plot, is a type of plot that we can use to determine whether or not the residuals of a model follow a normal distribution. If the points on the plot roughly form a straight diagonal line, then the normality assumption is met.","8d533881":"<h2 style='border:3px solid DodgerRed;background-color:#9d0aea;font-family:comic sans ms'><center>Linear Regression<\/center><\/h2>","88713365":"<h1 style = \"font-family: Comic Sans MS;color:blue\" >What to do if this assumption is violated<\/h1>","cdd0697b":"<h3 style = \"font-family: Comic Sans MS;color:Red\">How to check:<\/h3>","e115fbfa":"<p style = \"font-family: Comic Sans MS;color:apple green\">3. Use weighted regression. Another way to fix heteroscedasticity is to use weighted regression. This type of regression assigns a weight to each data point based on the variance of its fitted value. Essentially, this gives small weights to data points that have higher variances, which shrinks their squared residuals. When the proper weights are used, this can eliminate the problem of heteroscedasticity.<\/p>","e54ae198":" - For positive serial correlation, consider adding lags of the dependent and\/or independent variable to the model.\n - For negative serial correlation, check to make sure that none of your variables are overdifferenced.\n - For seasonal correlation, consider adding seasonal dummy variables to the model.","df85e0c4":"Depending on the nature of the way this assumption is violated, you have a few options:","82125d56":"There are two common ways to check if this assumption is met:","ed532d55":"The simplest way to test if this assumption is met is to look at a residual time series plot, which is a plot of residuals vs. time. Ideally, most of the residual autocorrelations should fall within the 95% confidence bands around zero, which are located at about +\/- 2-over the square root of n, where n is the sample size. You can also formally test if this assumption is met using the Durbin-Watson test.","4a56b1bb":"<h1 style = \"font-family: Comic Sans MS;color:Green\">Assumption 2: Independence<\/h1>","2bffbb53":"<h1 style = \"font-family: Comic Sans MS;color:red\">When it is used:<\/h1>","2107b264":"<h3 style = \"font-family: Comic Sans MS;color:Red\">How to check:<\/h3>","39d53740":"The next assumption of linear regression is that the residuals have constant variance at every level of x. This is known as homoscedasticity.  When this is not the case, the residuals are said to suffer from heteroscedasticity.\n\nWhen heteroscedasticity is present in a regression analysis, the results of the analysis become hard to trust. Specifically, heteroscedasticity increases the variance of the regression coefficient estimates, but the regression model doesn\u2019t pick up on this. This makes it much more likely for a regression model to declare that a term in the model is statistically significant, when in fact it is not.","af324a85":"<p  style = \"font-family: Comic Sans MS;color:blue\" >The easiest way to detect if this assumption is met is to create a scatter plot of x vs. y. This allows you to visually see if there is a linear relationship between the two variables. If it looks like the points in the plot could fall along a straight line, then there exists some type of linear relationship between the two variables and this assumption is met.<\/p>\n\nFor example, the points in the plot below look like they fall on roughly a straight line, which indicates that there is a linear relationship between x and y:","b27b1aa0":"**Many of the people are explaining about machine learning models but what's the problem is some people are well about \nfeature engineering,feature selection,deployement part,modeling part and web scraping part,problem that was faced beginners are they are not able to find all in one place so that i decided to make this notebook helpful for beginners as well as experts.**","d62c1a9b":"<p style = \"font-family: Comic Sans MS;color:apple green\">If you create a scatter plot of values for x and y and see that there is not a linear relationship between the two variables, then you have a couple options:<\/p>\n\n1. Apply a nonlinear transformation to the independent and\/or dependent variable. Common examples include taking the log, the square root, or the reciprocal of the independent and\/or dependent variable.\n\n2. Add another independent variable to the model. For example, if the plot of x vs. y has a parabolic shape then it might make sense to add X2 as an additional independent variable in the model.","c02da022":"<img src=\"https:\/\/fourpillarfreedom.com\/wp-content\/uploads\/2019\/02\/het1.jpg\"><\/img>"}}