{"cell_type":{"e715d414":"code","a2ae1857":"code","a978a3b8":"code","8c021565":"code","d5eca28d":"code","6d8cc887":"code","799c47f1":"code","390e834e":"code","1093f5e5":"code","37907282":"code","b9299f6d":"code","fa6203a3":"code","7859f113":"code","7869f332":"code","67cfe541":"code","18a6df98":"code","ec7d1319":"code","90227f10":"code","59b64a92":"code","2570cf31":"code","0573fe38":"code","9504dc13":"code","e9d51647":"code","ff924ddf":"code","257c4322":"code","4a6e55dc":"code","e28b0d2a":"code","939f8c06":"code","b7035858":"code","7b48c1da":"code","25a0f3e4":"code","0fdd2808":"code","74eb8078":"markdown","bb6cf4b1":"markdown","5a653d34":"markdown","14b933a7":"markdown","45b204c3":"markdown","8e230de8":"markdown","20a45adf":"markdown","2e87bdd7":"markdown","24c3346f":"markdown","becdeb3d":"markdown","10612ad2":"markdown","a124ff18":"markdown","d8ad71b5":"markdown","b34d0ce0":"markdown","b5dcd9af":"markdown","54173f3a":"markdown","ac6a17da":"markdown","1e69b696":"markdown","3e31e89c":"markdown"},"source":{"e715d414":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a2ae1857":"train = pd.read_csv('\/kaggle\/input\/cap-4611-2021-fall-assignment-02\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/cap-4611-2021-fall-assignment-02\/eval.csv')\n\ntest.head()","a978a3b8":"#test.info()","8c021565":"#Copy test id\ntestId = test['id']\n\n#drop id from sets\ntrain.drop('id', axis = 1, inplace= True)\ntest.drop('id', axis = 1, inplace= True)\n\ntrain.info()","d5eca28d":"train['esrb_rating'].unique()","6d8cc887":"trainE= train['esrb_rating']== 'E'\n#- \ntrain[trainE].sum()","799c47f1":"trainE.value_counts()","390e834e":"drop1 = train['use_of_drugs_and_alcohol'] == 1\n#train[drop1].sum()\ntrainDrop = trainE * drop1\n#train[trainDrop].sum()\ntrain = train.drop(train.loc[trainDrop].index)\n#train[train['esrb_rating']== 'E'].sum()","1093f5e5":"trainET= train['esrb_rating']== 'ET'\n#- \ntrain[trainET].sum()","37907282":"trainET.value_counts()","b9299f6d":"trainT= train['esrb_rating']== 'T'\n#- \ntrain[trainT].sum()","fa6203a3":"trainT.value_counts()","7859f113":"trainM= train['esrb_rating']== 'M'\n#- \ntrain[trainM].sum()","7869f332":"trainM.value_counts()","67cfe541":"train.describe()","18a6df98":"train.sum()","ec7d1319":"test.sum()","90227f10":"#drop the object features\ntrain.drop('title', axis = 1, inplace= True)","59b64a92":"#- \ntrain.drop('console', axis = 1, inplace= True)\n#- \ntest.drop('console', axis = 1, inplace= True)","2570cf31":"#- \ntrain.drop('no_descriptors', axis = 1, inplace= True)\n#- \ntest.drop('no_descriptors', axis = 1, inplace= True)","0573fe38":"#- \ntrain['esrb_rating'].replace({'E':1,'ET':2,'T':3,'M':4} , inplace= True)","9504dc13":"test.info()","e9d51647":"train.info()","ff924ddf":"def fitModel(model, name):\n    model.fit(Xtrain, ytrain)\n    print(name + \" training score: \")\n    print(model.score(Xtrain, ytrain))\n    print()\n    ypred = model.predict(Xtest)\n    print(name + \" testing score:\")\n    print(accuracy_score(ytest, ypred))\n    print()\n    print(name + \" description:\")\n    df = pd.DataFrame({'expected': ytest, 'prediction': ypred})\n    print(df.describe())","257c4322":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ny = train['esrb_rating']\nX = train.drop('esrb_rating', axis = 1)\n\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size= .3, stratify= y)#, random_state= 42","4a6e55dc":"from sklearn.linear_model import LogisticRegression\n#- lr = LogisticRegression(multi_class= 'multinomial' ,solver='lbfgs', C=100, penalty = 'l2', max_iter= 5000) #tune hyperparameters\n#- lr = LogisticRegression(solver='liblinear', C= 100, penalty = 'l1') #tune hyperparameters\n#- fitModel(lr,'lr')\n\n\nlrModel = LogisticRegression( solver='liblinear', penalty = 'l1')\n                        \n#- \nlrParams = {'C' : [5, 6, 7, 10, 50, 100]}\n\n#- \nlr = GridSearchCV(estimator = lrModel, param_grid = lrParams, scoring = 'accuracy', cv = 15, refit = 'true')\nlr.fit(Xtrain, ytrain)\n\n#- \nlrBoost = GradientBoostingClassifier(init=lr.best_estimator_, n_estimators=100)\n#- \nfitModel(lrBoost, 'lrBoost')","e28b0d2a":"from sklearn.svm import SVC\n#- sv = SVC(kernel= 'rbf',degree = 41, C= 100, gamma= 100, max_iter= 5000)\n#- fitModel(sv,'sv')\n\n#- \nsvModel = SVC(max_iter= 3000)                        \n#- \nsvParams = { 'C' : [6, 50, 100, 200], 'degree': [3, 5, 10, 41]}#'kernel': ['rbf', 'poly', 'linear'],, 'gamma' : ['scale','auto']\n#- \nsv = GridSearchCV(estimator = svModel, param_grid = svParams, scoring = 'accuracy', cv = 15, refit = 'true')\n#- \nsv.fit(Xtrain, ytrain)\n#- svBoost = GradientBoostingClassifier(init=sv.best_estimator_, n_estimators=100)\n#- fitModel(svBoost, 'svBoost')\n\n#- \nfitModel(sv.best_estimator_,'sv')","939f8c06":"from sklearn.neighbors import KNeighborsClassifier\n#-kn = KNeighborsClassifier(n_neighbors = 18, leaf_size= 2000, p = 5)#\n#-fitModel(kn,'kn')\n\n#- \nknModel = KNeighborsClassifier()                       \n#- \nknParams = {'n_neighbors' : [5, 7, 9, 10, 20, 30], 'leaf_size':[30, 100, 200],'p':[2, 10, 100]}\n#- \nkn = GridSearchCV(estimator = knModel, param_grid = knParams, scoring = 'accuracy', cv = 15, refit = 'true')\n#- \nkn.fit(Xtrain, ytrain)\n#- \nknBoost = GradientBoostingClassifier(init=kn.best_estimator_, n_estimators=100)\n#- \nfitModel(knBoost, 'knBoost')","b7035858":"from sklearn.tree import DecisionTreeClassifier\n#- dt = DecisionTreeClassifier(min_samples_leaf = 1)#\n#- fitModel(dt, 'dt')\n\n#- \ndtModel = DecisionTreeClassifier(criterion = 'gini')                        \n#- \ndtParams = {'max_depth': [ 5, 8, 10, 20, 100], 'min_samples_leaf':[2, 5, 8, 10, 50, 100]}\n#- \ndt = GridSearchCV(estimator = dtModel, param_grid = dtParams, scoring = 'accuracy', cv = 15, refit = 'true')\n#- \ndt.fit(Xtrain, ytrain)\n#- \ndtBoost = GradientBoostingClassifier(init=dt.best_estimator_, n_estimators=100)\n#- \nfitModel(dtBoost, 'dtBoost')","7b48c1da":"from sklearn.ensemble import RandomForestClassifier\n#-rf = RandomForestClassifier(criterion = 'gini',n_estimators = 1000, max_depth = 9, max_features = 'auto', min_samples_leaf = 0.000001)#\n#-fitModel(rf, 'rf')\n\n#- \nrfModel = RandomForestClassifier()                        \n#- \nrfParams = {'n_estimators':[5, 10, 20], 'max_depth': [ 5, 8, 10], 'min_samples_leaf':[5, 8, 10], 'max_features':[5, 10, 20, 'auto']}#-fitGrid(rf, 'rf', rfParams)\n#- \nrf = GridSearchCV(estimator = rfModel, param_grid = rfParams, scoring = 'accuracy', cv = 15, refit = 'true')\n#- \nrf.fit(Xtrain, ytrain)\n#- \nrfBoost = GradientBoostingClassifier(init=rf.best_estimator_, n_estimators=100)\n#- \nfitModel(rfBoost, 'rfBoost')","25a0f3e4":"def bestPrediction(models):\n    bestModel = models[0]\n    for model in models:\n        if model.score(Xtest, ytest) > bestModel.score(Xtest, ytest):\n            bestModel = model\n    print(\"The best score was: \")\n    print(bestModel.score(Xtest, ytest))\n    print()\n    return bestModel.predict(test)        ","0fdd2808":"models = [lrBoost, sv.best_estimator_, knBoost, dtBoost, rfBoost]\n\npred = bestPrediction(models)\nprint(pred.dtype)\n\npred = np.where(pred == 1, 'E', pred) # from this point on the numbers in the array are considered strings, therfore you must adress the as strings\npred = np.where(pred == '2', 'ET', pred) \npred = np.where(pred == '3', 'T', pred)\npred = np.where(pred == '4', 'M', pred)\n\noutput = pd.DataFrame({'id': testId, 'esrb_rating': pred})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\nprint(output)","74eb8078":">Inspecting the subset of data of each possible target value we can see that there are some categories that have a minimun representation depending on the desired rating. I'm going to have that in mind in case my test accuracy is too low for my liking.","bb6cf4b1":">Given that the test data set does not contain a 'titles' column, we can safely assume this column will not be neccessary. Not that games are rated based on titles eitherway. ","5a653d34":">In case one of the models cannot predict an object type data, we will turn all the target values into integers","14b933a7":"# **Decision Tree**","45b204c3":">The 'no descriptor' column literally has no useful info, let's drop it too","8e230de8":"# **Javier Torres Notebook:**","20a45adf":"# **Loading the Data**","2e87bdd7":">As you can see, there are no apparent missing values in this data set","24c3346f":"# **Logistic Regression**","becdeb3d":"# **Data tranfomation and Feature Engineering**\nWith explanation","10612ad2":"# **K Nearest Neighbors**","a124ff18":">Other that that, we can see the 'console' column. A little of research will tell us that esrb ratings is not dependant on whether the game is on a console or not. We can drop this column from the test and training set","d8ad71b5":"# **Checking For Outliers**","b34d0ce0":"# **Random Forest**","b5dcd9af":"# **Support Vector Machine**","54173f3a":"The use of drugs and alcohol does not seem very appropriate for a rate for everyone game, so let's drop it","ac6a17da":"# **Checking For Missing Data**","1e69b696":"# **Select the best Model**","3e31e89c":"# Building the Models"}}