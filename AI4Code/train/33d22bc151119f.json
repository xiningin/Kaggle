{"cell_type":{"20950d29":"code","a6870abc":"code","a289c1e8":"code","ae8cc004":"code","4207af44":"code","55fff018":"code","9d0afe79":"code","981cee9c":"code","63c08128":"code","d193059e":"code","a51ea15a":"code","88ceb974":"code","c9a68e14":"code","7bc17aa4":"code","0e59528b":"code","ce616432":"code","685d36c4":"code","0cb1a352":"code","f7d37fcb":"code","e0402a12":"code","52987171":"code","92422315":"code","b3be269a":"code","94982d39":"code","5213f46c":"code","708c4a15":"code","61230faf":"code","c655391f":"code","a201ade5":"code","ca913651":"code","c1342d93":"code","efec28b9":"code","517e52e4":"code","5941a843":"code","8ddd3993":"code","b2bcc434":"code","551ec86e":"code","974d16d7":"code","a0280ccd":"code","69238162":"code","0a651efb":"code","140b3574":"code","8e3ac43b":"code","f9a2a6ce":"code","47b145cf":"code","7cb35a9c":"markdown","fb8cb8db":"markdown","d8802a63":"markdown","f0c6ec3f":"markdown","6902a243":"markdown","06942e9e":"markdown","7cde34c6":"markdown","ce769daf":"markdown","a5db9d4a":"markdown","c6c2500b":"markdown","c8db7499":"markdown","ebbbcc26":"markdown","ebb2ed27":"markdown","7696caac":"markdown","bfad33d8":"markdown","85bb0873":"markdown","f46fe44b":"markdown","7c36750d":"markdown","5d2d7fa6":"markdown","e6d09705":"markdown","08584499":"markdown","a2323b21":"markdown","07b320c7":"markdown","cd089859":"markdown","80b992ef":"markdown","d80685b7":"markdown","a04c2c91":"markdown","bd44cfd6":"markdown","e24865cc":"markdown","665e9dd4":"markdown","c303c86f":"markdown","7c1d0ec9":"markdown","589ff4a7":"markdown","387d12ca":"markdown","d464d47c":"markdown","c9ecc1d2":"markdown"},"source":{"20950d29":"import pandas as pd\nPATH = '..\/input\/lending-club-loan-data\/loan.csv'\nimport warnings\nwarnings.filterwarnings('ignore')","a6870abc":"raw_data = pd.read_csv(PATH)\npd.options.display.max_columns = 2000\nraw_data.head()","a289c1e8":"data = raw_data[['loan_amnt', 'term', 'int_rate', 'installment', \n                 'grade', 'sub_grade', 'emp_length', \n                 'home_ownership', 'annual_inc', 'verification_status', \n                 'purpose', 'dti', 'delinq_2yrs', 'delinq_amnt', \n                 'chargeoff_within_12_mths',  'tax_liens',  \n                 'acc_now_delinq', 'inq_last_12m', 'open_il_24m', \n                 'loan_status']]\ndata.head()","ae8cc004":"data.isnull().sum()","4207af44":"data = data.dropna(axis = 'index', \n                   subset = ['annual_inc', 'dti', 'delinq_2yrs', \n                             'delinq_amnt', 'chargeoff_within_12_mths', \n                             'tax_liens', 'acc_now_delinq'])\ndata.isnull().sum()","55fff018":"data = data.fillna(value = {'emp_length' : 'no_info', \n                            'inq_last_12m' : 'no_info', \n                            'open_il_24m':'no_info'}) ","9d0afe79":"import seaborn as sn\nimport matplotlib.pyplot as plt\n%matplotlib notebook\n\ndata_for_corr = data.assign(term = data.term.astype('category').cat.codes,\n                            grade = data.grade.astype('category').cat.codes,\n                            sub_grade = data.sub_grade.astype('category').cat.codes,\n                            emp_length = data.emp_length.astype('category').cat.codes,\n                            home_ownership = data.home_ownership.astype('category').cat.codes,\n                            verification_status = data.verification_status.astype('category').cat.codes,\n                            purpose = data.purpose.astype('category').cat.codes,\n                            loan_status = data.loan_status.astype('category').cat.codes,\n                            inq_last_12m = data.inq_last_12m.astype('category').cat.codes,\n                            open_il_24m = data.open_il_24m.astype('category').cat.codes\n                            )\n\ncorr_matrix = data_for_corr.corr()\nplt.figure(figsize=(18,14))\nsn.heatmap(corr_matrix, annot=True, cmap = 'Blues',vmin=-0.1, vmax=1)\nplt.title('Correlation matrix')\nplt.show()","981cee9c":"data = data.drop(columns = ['installment', 'grade', 'sub_grade', 'open_il_24m'])","63c08128":"data_inq_nan = data[data.inq_last_12m == 'no_info']\npd.DataFrame({'where inq_last_12m=NaN': data_inq_nan['loan_status'].value_counts()\/len(data_inq_nan), \n              'all dataset'           : data['loan_status'].value_counts()\/len(data)}\n             ).style.format('{:.2f}')","d193059e":"data_3_statuses = data[(data.loan_status == 'Fully Paid') | (data.loan_status == 'Charged Off') | \n                 (data.loan_status == 'Default')]\ndata_inq_nan = data_3_statuses[data_3_statuses.inq_last_12m == 'no_info']\npd.DataFrame({'where inq_last_12m=NaN': \n              data_inq_nan['loan_status'].value_counts()\/len(data_inq_nan), \n              'all dataset': \n              data_3_statuses['loan_status'].value_counts()\/len(data_3_statuses)}\n             ).style.format('{:.2f}')","a51ea15a":"data = data.drop(columns = 'inq_last_12m')","88ceb974":"data['loan_status'].value_counts()","c9a68e14":"data = data[(data.loan_status == 'Fully Paid') | (data.loan_status == 'Charged Off') | \n                 (data.loan_status == 'Default')]\ndata['loan_status'] = data['loan_status'].replace(to_replace = ['Fully Paid', 'Charged Off', 'Default'], \n                                                       value = [0, 1, 1])\ndata['loan_status'].value_counts()","7bc17aa4":"data.describe().style.format('{:.2f}')","0e59528b":"X = data.iloc[:, 0:-1].values\ny = data.iloc[:, -1].values","ce616432":"from sklearn.preprocessing import LabelEncoder\n\nlabelencoder_X = LabelEncoder()\nX[:, 1] = labelencoder_X.fit_transform(X[:, 1])\nX[:, 3] = labelencoder_X.fit_transform(X[:, 3])","685d36c4":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nct = ColumnTransformer([('one_hot_encoder', OneHotEncoder(categories='auto'), [4, 6, 7])],\n                       remainder='passthrough')\nX = ct.fit_transform(X)","0cb1a352":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)","f7d37fcb":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","e0402a12":"def prepare_data(data):\n    X = data.iloc[:, 0:-1].values\n    y = data.iloc[:, -1].values\n    \n    labelencoder_X = LabelEncoder()\n    X[:, 1] = labelencoder_X.fit_transform(X[:, 1])\n    X[:, 3] = labelencoder_X.fit_transform(X[:, 3])\n    \n    ct = ColumnTransformer([('one_hot_encoder', OneHotEncoder(categories='auto'), [4, 6, 7])],\n                       remainder='passthrough')\n    X = ct.fit_transform(X)\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n    \n    sc = StandardScaler()\n    X_train = sc.fit_transform(X_train)\n    X_test = sc.transform(X_test)\n    \n    return X_train, X_test, y_train, y_test","52987171":"#X_train, X_test, y_train, y_test = prepare_data(data)","92422315":"from sklearn.naive_bayes import GaussianNB\n\nclassifier_naive_bayes = GaussianNB()\nclassifier_naive_bayes.fit(X_train, y_train)\ny_pred = classifier_naive_bayes.predict(X_test)","b3be269a":"from sklearn.metrics import confusion_matrix\n\ndef build_conf_matrix(title):\n    conf_matrix = pd.DataFrame(confusion_matrix(y_test, y_pred, normalize='pred'))\n    sn.heatmap(conf_matrix, annot=True, cmap='Blues',vmin=0, vmax=1)\n    plt.title(title)\n    plt.show()\n    \nbuild_conf_matrix(title='Naive Bayes, imbalanced dataset')","94982d39":"data_loan_status_1 = data[data['loan_status'] == 1]\ndata_loan_status_0 = data[data['loan_status'] == 0].sample(n=len(data_loan_status_1))\ndata_balanced = data_loan_status_1.append(data_loan_status_0) ","5213f46c":"X_train, X_test, y_train, y_test = prepare_data(data_balanced)","708c4a15":"classifier_naive_bayes = GaussianNB()\nclassifier_naive_bayes.fit(X_train, y_train)\ny_pred = classifier_naive_bayes.predict(X_test)","61230faf":"build_conf_matrix(title='Naive Bayes, balanced dataset')","c655391f":"from sklearn.ensemble import RandomForestClassifier\n\nclassifier_rand_forest = RandomForestClassifier(n_estimators = 10, criterion = 'entropy')\nclassifier_rand_forest.fit(X_train, y_train)\ny_pred = classifier_rand_forest.predict(X_test)","a201ade5":"build_conf_matrix(title='Random Forest, 10 trees')","ca913651":"from sklearn.model_selection import GridSearchCV\n\nparameters = [ {'n_estimators':[10, 50, 100, 200], 'criterion':['entropy', 'gini']}]\ngrid_search = GridSearchCV(estimator = classifier_rand_forest, \n                                 param_grid = parameters,\n                                 scoring = 'accuracy',\n                                 cv = 2,\n                                 n_jobs = -1,\n                                 verbose = 5)\ngrid_search = grid_search.fit(X_train, y_train)","c1342d93":"grid_search.best_params_","efec28b9":"grid_search.best_score_","517e52e4":"classifier_rand_forest = RandomForestClassifier(n_estimators = 200, criterion = 'gini')\nclassifier_rand_forest.fit(X_train, y_train)\ny_pred = classifier_rand_forest.predict(X_test)","5941a843":"build_conf_matrix(title='Random Forest, 200 trees, gini')","8ddd3993":"from sklearn.linear_model import LogisticRegression\n\nclassifier_log_reg = LogisticRegression()\nclassifier_log_reg.fit(X_train, y_train)\ny_pred = classifier_log_reg.predict(X_test)","b2bcc434":"build_conf_matrix(title='Logistic Regression')","551ec86e":"from sklearn.model_selection import cross_val_score\n\naccuracies = cross_val_score(estimator = classifier_naive_bayes, X = X_train, y = y_train, cv = 5)\n('Naive Bayes',{'accuracy':accuracies.mean(), 'std':accuracies.std()})","974d16d7":"accuracies = cross_val_score(estimator = classifier_log_reg, X = X_train, y = y_train, cv = 5)\n('Logistic Regression',{'accuracy':accuracies.mean(), 'std':accuracies.std()})","a0280ccd":"accuracies = cross_val_score(estimator = classifier_rand_forest, X = X_train, y = y_train, cv = 2)\n('Random Forest',{'accuracy':accuracies.mean(), 'std':accuracies.std()})","69238162":"data_loan_status_1 = data[data['loan_status'] == 1].sample(n=10000)\ndata_loan_status_0 = data[data['loan_status'] == 0].sample(n=len(data_loan_status_1))\ndata_small = data_loan_status_1.append(data_loan_status_0)","0a651efb":"X = data_small.iloc[:, [0, 2]].values\ny = data_small.iloc[:, -1].values","140b3574":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","8e3ac43b":"classifier_log_reg = LogisticRegression()\nclassifier_log_reg.fit(X_train, y_train)\ny_pred = classifier_log_reg.predict(X_test)","f9a2a6ce":"build_conf_matrix(title='Logistic Regression, small dataset with 2 columns')","47b145cf":"from matplotlib.colors import ListedColormap\nimport numpy as np\nX_set, y_set = X_train, y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 0.3, stop = X_set[:, 0].max() + 0.3, step = 0.01),\n                             np.arange(start = X_set[:, 1].min() - 0.3, stop = X_set[:, 1].max() + 0.3, step = 0.01))\nplt.contourf(X1, X2, classifier_log_reg.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('#024000', '#600000')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                c = ListedColormap(('#4dcb49', '#ff3b3b'))(i), label = j, s=5)\nplt.xlabel('loan_amnt')\nplt.ylabel('int_rate')\nplt.show()","7cb35a9c":"For easy of use, lets put this all together in a function:","fb8cb8db":"# **Predicting loan defaults**\nPurpose of this work is to build machine learning models for loan default prediction. I will use LendingClub dataset to train the models.\n\n\n**LendingClub**\n\nLendingClub is an American lending company, headquartered in San Francisco. Company offers loans between 1,000 and 40,000 for standard loan period of 3 years.\n\n**Dataset**\n\nThe dataset contains complete loan data for all loans issued through the 2007-2019. Each row represents a loan, dataset has 145 columns with all possible variables including the current loan status (Current, Late, Fully Paid, etc.), wich will be the target value for prediction. Total number of loans is 2,2M.\n","d8802a63":"Lets try Logistic Regression:","f0c6ec3f":"I should mention that I ran grid search 3 times in different versions of this notebook so far, I got gini 2 times and entropy once, so these criterions are close in terms of accuracy.\nBut this time we got model with gini criterion again.","6902a243":"For machine learning model we will need to split up the dataset into **X** and **y**, where **y** is a target feature array, and **X** is dataset without a target feature.","06942e9e":"# **Conclusion**\n\nLogistic Regressoin is our best model in terms of accuracy, although its not much better than Naive Bayes or Random Forest models.\nFrom the plot we saw, that model is doing its best to separate default and paid loans, but we still got maximum accuracy of 64,5%. We need to find different ways to improve the prediction accuracy.\n","7cde34c6":"At first, lets see what we are working with:","ce769daf":"Split for train and test partitions:","a5db9d4a":"Some columns have few rows with NaNs, we will drop these rows:","c6c2500b":"# **Modeling**\n\nWe will use Naive Bayes, Random Forest and Logistic Regression for our prediction.\n\nLets start with Naive Bayes model.","c8db7499":"Plot with data points and classifier separation:","ebbbcc26":"On the other hand, columns **emp_length**, **inq_last_12m** and **open_il_24m** have too many NaNs to drop them out, we will now change their type to string.\nAlso, **inq_last_12m** and **open_il_24m** have same amount of nulls, lets remember to sort this out later.","ebb2ed27":"Lets use **loan_amnt** and **int_rate** columns as X:","7696caac":"And perform scaling (y_train and y_test dont need scaling, because they are binary anyway):","bfad33d8":"Lets use GridSearch to find any potential improvement for Random Forest model:","85bb0873":"Categorical columns **term** and **emp_length** have *comparable* values, they need to be encoded with LabelEncoder:","f46fe44b":"Interesting, we got pretty high accuracy even with just 2 columns.","7c36750d":"Slightly better result than other models.\n\nTo summurize models accuracy we will use cross validation score: ","5d2d7fa6":"Much better, although accuracy is not that great.\n\nRandom Forest model:","e6d09705":"Columns **home_ownership**, **verification_status** and **purpose** on the other hand have *incomparable* values, encode them with ColumnTransformer:","08584499":"As we can see, some columns have a strong correlation:\n1. **loan_amnt** with **installment**\n2. **int_rate** with **grade** and **sub_grade**\n3. And as expected **inq_last_12m** with **open_il_24m** \n\nWe will drop this columns to prevent multicollinearity:","a2323b21":"Actually it is similar. The correlation is explained by low number of Current statuses in these loans. Giving the fact, that this column has 866k nulls with not much useful information in it, we will drop it.","07b320c7":"# **Data Preparation**\nAs mentioned, dataset has 145 columns. We won't need all of them for the model, lets select the necessary ones:","cd089859":"**Null values**\n\nLets see the number of null values in each column we selected:","80b992ef":"As classifier lets use a Logistic Regression model:","d80685b7":"A short summary:","a04c2c91":"**Green and red points** represent paid and default loans, **green and red regions** represent model classification.\nAs we see, there is no room for improvement: default and paid loans are well mixed up - the model cant separate them.  Model is actually doing its best to separate major groups of loans.","bd44cfd6":"We got maximum accuracy of 64,5% with Logistic Regression, this is not great by any means. \n\nLets try to make a small 2D dataset so we can see data and model job on the plot. Maybe we could find a way to improve our models. \n\nDataset with 10k rows will be enough to see the general picture and not to overwhelm plot with points.\n","e24865cc":"**Loan status**\n\nNow lets inspect **loan_status** column.","665e9dd4":"**Correlation Matrix**\n\nNow lets build a correlation matrix to show dependencies between columns:","c303c86f":"As we see, our model puts everything into 0. Lets balance data and try again:","7c1d0ec9":"We see, that dataset is inbalanced, this could be a problem with modeling. Lets come back to this later.","589ff4a7":"Wow, we also see *some* correlation between **inq_last_12m** and **loan_status**! Lets inspect this, maybe this column can be very useful for prediction.\n\nLets compare percent of **loan_status** in subset, where **inq_last_12m** is null and in whole dataset:","387d12ca":"We see, that loans with *i**nq_last_12m**=NaN* have few Current statuses, this may be explained by the fact that these are old loans, they are either Charged Off or Fully Paid.\n\nNow lets check if their default rate is any different:","d464d47c":"But not a big improvement anyway compared to 10 trees model","c9ecc1d2":"We can only use Fully Paid, Charged Off or Default loans for prediction. Others need to be omitted. \n\nLets also encode **loan_status** with binary number, where 1 indicates Default loan."}}