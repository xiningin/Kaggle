{"cell_type":{"387c0f00":"code","b71f3479":"code","107a2e81":"code","510cd04a":"code","0569f3ce":"code","6a970481":"code","3c4a06fc":"code","80f22f8e":"code","e428939b":"code","388106e0":"code","399c75f3":"code","be8f45ed":"code","10d7af90":"code","e76235d1":"code","118966cf":"code","90fd6963":"code","490ad4c5":"code","ab2e0f2f":"code","80cd247f":"code","d6a08fca":"code","528a7c51":"code","92c1e6a6":"code","b9512893":"code","294d86fe":"code","2ac69501":"code","534f9943":"code","c1f87b43":"code","e66058ae":"code","83bd7031":"code","a2e8f913":"code","8baffc8c":"code","59123543":"code","71c03eb3":"code","a66cd921":"code","026847f6":"code","566e9b39":"code","ad033c84":"code","5f6c4511":"code","8fbd578d":"code","d815557b":"code","55188536":"code","67adc952":"code","9a2fe90d":"code","d25d5eb3":"code","8f8b2585":"code","86d02681":"code","8cccb007":"code","a73c1c77":"code","16a3d0ac":"code","813ef828":"code","de8c94c4":"code","6ea85f1e":"code","a2b25916":"code","fb4ccda0":"code","f5dd525d":"code","4cd8b06b":"code","121e417b":"code","4011ab9e":"code","a2e0ba4d":"code","8c717584":"code","c78294de":"code","314760f4":"code","448f2d57":"code","9af2ad50":"code","7d8d2e0c":"code","3a964dc6":"markdown","78b548fc":"markdown","4948d2db":"markdown","32198af3":"markdown","1099773c":"markdown","cd93397e":"markdown","cd2fc38f":"markdown","e5d26673":"markdown","0ff10752":"markdown","b88501af":"markdown","fabb0e4b":"markdown","b65f9c89":"markdown","2f4b339d":"markdown","786c08a3":"markdown","979f345a":"markdown","83d3d7e2":"markdown","21ea8f78":"markdown","a79e80bf":"markdown","881b9425":"markdown","65a35707":"markdown","f86be220":"markdown","73397fa8":"markdown","a192594e":"markdown","51d19cab":"markdown","18d8d261":"markdown"},"source":{"387c0f00":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n\nfrom sklearn.model_selection import KFold,cross_val_score, RepeatedStratifiedKFold,StratifiedKFold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.preprocessing import OneHotEncoder,StandardScaler,PowerTransformer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.dummy import DummyClassifier\n# from imblearn.over_sampling import SMOTE\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport plotly \nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.offline as py\nfrom plotly.offline import iplot\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\n\nimport missingno as msno\n\nplt.rcParams[\"figure.figsize\"] = (12,8)\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","b71f3479":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, plot_confusion_matrix\nfrom sklearn.metrics import plot_roc_curve, plot_precision_recall_curve, roc_auc_score, auc, roc_curve\nfrom sklearn.impute import SimpleImputer","107a2e81":"df = pd.read_csv(\"\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\")","510cd04a":"df.head()","0569f3ce":"df.shape","6a970481":"print (f' We have {df.shape[0]} instances with the {df.shape[1]-1} features and 1 output variable.')","3c4a06fc":"df.info()","80f22f8e":"df.describe()","e428939b":"df.duplicated().sum()","388106e0":"df.stroke.value_counts()","399c75f3":"df.stroke.value_counts(normalize=True)*100","be8f45ed":"y = df['stroke']\nprint(f'Percentage of patient has a stroke: % {round(y.value_counts(normalize=True)[1]*100,2)} --> ({y.value_counts()[1]} patient)\\nPercentage of patient does not have a stroke: % {round(y.value_counts(normalize=True)[0]*100,2)} --> ({y.value_counts()[0]} patient)')","10d7af90":"df = df.drop('id',axis=1)","e76235d1":"df.age.sort_values()","118966cf":"round(df.age.sort_values())[:100]","90fd6963":"df[df['stroke'] == 1]['age'].nsmallest(10)","490ad4c5":"# df = df[df['age'] >= 14]","ab2e0f2f":"# df.shape\n# print(f\"14 ya\u015f ve alt\u0131 ki\u015filer d\u00fc\u015f\u00fcnce {df.shape[0]} sat\u0131r kald\u0131. {5110-df.shape[0]} tane sat\u0131r d\u00fc\u015fm\u00fc\u015f  olduk.\")","80cd247f":"df.gender.unique()","d6a08fca":"df[df.gender==\"Other\"]","528a7c51":"df = df.drop(index = 3116)","92c1e6a6":"df.info()","b9512893":"df.describe()","294d86fe":"df.smoking_status.value_counts()","2ac69501":"df['stroke'].value_counts().plot(kind = 'bar');","534f9943":"sns.heatmap(df.corr(),annot = True);","c1f87b43":"sns.boxplot(data = df, x = 'stroke', y = 'age');","e66058ae":"df.head()","83bd7031":"sns.pairplot(df,hue='stroke');","a2e8f913":"df_cat = df.select_dtypes(include=\"object\").columns\ndf_cat","8baffc8c":"df = pd.get_dummies(df, drop_first=True)\ndf","59123543":"df.shape","71c03eb3":"df.bmi.isnull().sum()","a66cd921":"X = df.drop('stroke',axis=1)\ny = df['stroke']","026847f6":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler","566e9b39":" X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","ad033c84":"# conda install -c conda-forge missingno","5f6c4511":"import missingno as msno","8fbd578d":"msno.bar(X_train);","d815557b":"msno.matrix(X_train);","55188536":"msno.bar(X_test);","67adc952":"X_train.describe()","9a2fe90d":"sns.boxplot(X_train.bmi);","d25d5eb3":"X_train","8f8b2585":"from sklearn.impute import SimpleImputer","86d02681":"imputer = SimpleImputer(missing_values=np.nan, strategy=\"median\")\n\nX_train['bmi'] = imputer.fit_transform(X_train['bmi'].values.reshape(-1,1))[:,0]","8cccb007":"X_test['bmi'] = imputer.fit_transform(X_test['bmi'].values.reshape(-1,1))[:,0]","a73c1c77":"print(X_train.isnull().sum(), X_test.isnull().sum())","16a3d0ac":"X_train['bmi']","813ef828":"scaler = MinMaxScaler()","de8c94c4":"X_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","6ea85f1e":"from sklearn.linear_model import LogisticRegression","a2b25916":"log_model = LogisticRegression(class_weight='balanced')","fb4ccda0":"log_model.fit(X_train_scaled,y_train)","f5dd525d":"y_pred = log_model.predict(X_test_scaled)","4cd8b06b":"y_pred_proba = log_model.predict_proba(X_test_scaled)","121e417b":"test_data = pd.concat([X_test, y_test], axis=1)\ntest_data[\"pred\"] = y_pred\ntest_data[\"pred_proba\"] = y_pred_proba[:,1]","4011ab9e":"test_data.sample(10)","a2e0ba4d":"from sklearn.metrics import confusion_matrix, classification_report","8c717584":"print(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","c78294de":"from sklearn.model_selection import cross_validate","314760f4":"model = LogisticRegression(class_weight = \"balanced\")\n\nscores = cross_validate(model, X_train_scaled, y_train, scoring = ['accuracy', 'precision','recall','f1'], cv = 10)\ndf_scores = pd.DataFrame(scores, index = range(1, 11))\n#df_scores\ndf_scores.mean()[2:]","448f2d57":"y_pred_proba = log_model.predict_proba(X_test_scaled)\n\nfp_rate, tp_rate, thresholds = roc_curve(y_test, y_pred_proba[:,1])\n\noptimal_idx = np.argmax(tp_rate - fp_rate)\noptimal_threshold = thresholds[optimal_idx]\noptimal_threshold\n\ntest_data = pd.concat([X_test, y_test], axis=1)\n\ntest_data[\"pred_proba\"] = y_pred_proba[:,1]\n\ntest_data[\"pred\"] = y_pred\n\ntest_data[\"pred2\"] = test_data[\"pred_proba\"].apply(lambda x : 1 if x >= optimal_threshold else 0)\n\ny_pred2 = test_data[\"pred2\"]\n\nprint(classification_report(y_test,y_pred2))","9af2ad50":"optimal_threshold","7d8d2e0c":"print(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred2))","3a964dc6":"Let's start with importing our libraries and models first.","78b548fc":"## Exploratory Data Analysis (EDA) and Visualization","4948d2db":"As can be seen from the graph above, it would be more sensible to fill the missing values in **bmi** column with the median.","32198af3":"It seemed to me that \"Other\" gender type was kind a error. Therfore, I decided to drop it.","1099773c":"When we examine our data, we see that the data in the age column contains meaningless values that might deviate our estimations. For example, when we retrieve the 10 youngest people with 'stroke', we see 1.320 years old and 14.00 years old. I think it would be appropriate to drop everyone under the age of 14 to make more meaningful predictions using this data.","cd93397e":"We have no duplicated row.","cd2fc38f":"## Modelling ","e5d26673":"We need to convert these object-tpype columns into numeric columns.","0ff10752":"- We have developed model to predict classification problem.\n\n- First, we made the detailed exploratory analysis.\n\n- We have decided which metric to use.\n\n- We analyzed both target and features in detail.\n\n- We transform categorical variables into numeric so we can use them in the model.\n\n- We transform numerical variables to reduce skewness and get close to normal distribution.\n\n- We define our functions to use in our model.\n\n- We use cross validation model to evaluate our models.\n\n- We use pipeline to avoid data leakage.\n\n- We looked at the results of the each model and selected the best one for the problem in hand.\n\n- We have seen two different methods to use when developing model on the imbalanced data.\n\n- After this point it is up to you to develop and improve the models. Have fun reading.","b88501af":"We imputed train and test sets seperately so that we did not have data leakage.","fabb0e4b":"Since **id** column has no effect on suffering from a stroke we dropped that column.","b65f9c89":"We fit_transform() our train dataset while only tranforming test datset.","2f4b339d":"In this ML model, I created a model that can predict those who might face the danger of suffering from stroke. I hope it will be beneficial for all you guys.","786c08a3":"If you have error message upon running \"missingno\", run the following code!!!","979f345a":" # HELLO EVERYONE \ud83d\ude4b\u200d\u2642\ufe0f ","83d3d7e2":"## Scaling","21ea8f78":"## Train | Test Split and Filling None","a79e80bf":"- We have **binary classification** problem.\n- We make prection on the target variable **stroke**\n- And we will build a model to get best prediction on the stroke variable.","881b9425":"# Conclusion","65a35707":"# Cross Validation","f86be220":"- Almost %95 of the instances of our target variable is 'No stroke'\n\n- 4861 patient does not have a stroke\n\n- %5 of the instances of our target variable is 'Stroke'\n\n- 249 patient have a stroke.\n\n- We have imbalanced data (that is why we need to focus on **recall** rate.","73397fa8":"**Context**\n\n- According to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths. This dataset is used to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. Each row in the data provides relavant information about the patient.\n\n**Attribute Information**\n1. **id**: unique identifier\n\n2. **gender**: \"Male\", \"Female\" or \"Other\"\n\n3. **age**: age of the patient\n\n4. **hypertension**: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n\n5. **heart_disease**: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n\n6. **ever_married**: \"No\" or \"Yes\"\n\n7. **work_type**: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n\n8. **Residence_type**: \"Rural\" or \"Urban\"\n\n9. **avg_glucose_level**: average glucose level in blood\n\n10. **bmi**: body mass index\n\n11. **smoking_status**: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n\n12. **stroke**: 1 if the patient had a stroke or 0 if not\n\n*Note: \"Unknown\" in smoking_status means that the information is unavailable for this patient\n\nReference: https:\/\/www.kaggle.com\/fedesoriano\/stroke-prediction-dataset ","a192594e":"To prevent **data leakage** we filled 201 missing values after train-test split.","51d19cab":"- In our dataset, we have both numerical and categorical variables.\n- It is essential to see whether columns are correctly inferred.\n- The most important one to look for is our target variable **stroke**\n- **stroke** is detected as an integer, not as an object.\n- Target variable is coded as **1** for positive cases (has a stroke) and **0** for negative cases (does not have a stroke).\n- Both **Hypertension** and **heart disease** are detected as an integer, not as an object.\n- Just remember from the data definition part, they are coded as 1 for the positive cases(has hypertension\/heart disease).\n- And **0** for the negative cases (does not have hypertension\/heart disease)\n- We don't need to change them, but it is good to see and be aware of it.\n- In addition to them, we have 3 categorical variables, which we have to encode as numerical.","18d8d261":"## Reading Data"}}