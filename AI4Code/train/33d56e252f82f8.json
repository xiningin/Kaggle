{"cell_type":{"4d61b558":"code","28bb0104":"code","f1009a47":"code","a0c09702":"code","5e5e76dc":"code","4117703d":"code","deae1448":"code","f6f9f4cf":"code","b9e2723c":"code","33d57030":"markdown","48be3e0a":"markdown"},"source":{"4d61b558":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\ntry:\n    from torchsummary import summary\nexcept ImportError:\n    !pip3 install torchsummary\n    from torchsummary import summary\n    \nimport os\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms.functional as TF\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom tqdm import tqdm\nimport torch.optim as optim","28bb0104":"img_dir = \"..\/input\/chest-xray-masks-and-labels\/data\/Lung Segmentation\/CXR_png\"\nmask_dir = \"..\/input\/chest-xray-masks-and-labels\/data\/Lung Segmentation\/masks\"\n! mkdir saved_images\n","f1009a47":"class ConsecutiveConvolution(nn.Module):\n    def __init__(self,input_channel,out_channel):\n        super(ConsecutiveConvolution,self).__init__()\n        self.conv = nn.Sequential(\n            \n            nn.Conv2d(input_channel,out_channel,3,1,1,bias=False),\n            nn.BatchNorm2d(out_channel),\n            nn.ReLU6(inplace=True),\n            \n            nn.Conv2d(out_channel,out_channel,3,1,1,bias=False),\n            nn.BatchNorm2d(out_channel),\n            nn.ReLU6(inplace=True),            \n        \n        )\n        \n    def forward(self,x):\n        return self.conv(x)","a0c09702":"class UNet(nn.Module):\n    def __init__(self,input_channel, output_channel, features = [64,128,256,512]):\n        super(UNet,self).__init__()\n        self.pool = nn.MaxPool2d(kernel_size=2,stride=2)\n        self.encoder = nn.ModuleList()\n        self.decoder = nn.ModuleList()\n\n        # initialize the encoder\n        for feat in features:\n            self.encoder.append(\n                ConsecutiveConvolution(input_channel, feat)    \n            )\n            input_channel = feat\n        \n        #initialize the decoder \n        for feat in reversed(features):\n            # the authors used transpose convolution\n            self.decoder.append(nn.ConvTranspose2d(feat*2, feat, kernel_size=2, stride=2))\n            self.decoder.append(ConsecutiveConvolution(feat*2, feat))\n        \n        #bottleneck\n        self.bottleneck = ConsecutiveConvolution(features[-1],features[-1]*2)\n        \n        #output layer\n        self.final_layer = nn.Conv2d(features[0],output_channel,kernel_size=1)\n        \n    def forward(self,x):\n        skip_connections = []\n        \n        #encoding\n        for layers in self.encoder:\n            x = layers(x)\n            #skip connection to be used in recreation \n            skip_connections.append(x)\n\n            x = self.pool(x)\n        \n        x = self.bottleneck(x)\n        \n        skip_connections = skip_connections[::-1]\n        \n        \n        for idx in range(0,len(self.decoder),2):\n            \n            \n            x = self.decoder[idx](x)\n            skip_connection = skip_connections[idx\/\/2]\n            \n    \n            if x.shape != skip_connection.shape[2:]:\n                x = TF.resize(x,size=skip_connection.shape[2:])\n            \n            concat_skip = torch.cat((skip_connection,x),dim=1)\n#             print(concat_skip.shape)\n#             print(self.decoder[idx+1])\n\n            x = self.decoder[idx+1](concat_skip)\n        \n        return self.final_layer(x)\n            ","5e5e76dc":"from PIL import Image\nfrom torch.utils.data import Dataset\n\nclass CXRDataset(Dataset):\n    '''\n    CAUTON: Some masks of the images from img_dir are missing. Hence, only processing those images whose masks are available\n    '''\n    def __init__(self, image_dir, mask_dir,type=\"train\",split_ratio=0.2, transform=None):\n        self.image_dir = image_dir\n        self.mask_dir = mask_dir\n        self.transform = transform\n        self.masks = os.listdir(mask_dir)\n        \n        #a very standard \"meh\" way of train-test split\n        if type==\"train\":\n            self.masks = self.masks[:int(len(self.masks)*(1-split_ratio))]\n\n        else:\n            self.masks = self.masks[int(len(self.masks)*(1-split_ratio)):]\n\n\n    def __len__(self):\n        return len(self.masks)\n\n    def __getitem__(self, index):\n        \n        mask_path = os.path.join(self.mask_dir, self.masks[index])\n        img_path = os.path.join(self.image_dir, self.masks[index].replace(\"_mask.png\", \".png\"))\n        image = np.array(Image.open(img_path).convert(\"RGB\"))\n        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n        mask[mask == 255.0] = 1.0\n\n        if self.transform is not None:\n            augmentations = self.transform(image=image, mask=mask)\n            image = augmentations[\"image\"]\n            mask = augmentations[\"mask\"]\n\n        return image, mask","4117703d":"import torchvision\nfrom torch.utils.data import DataLoader\n\ndef save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\ndef load_checkpoint(checkpoint, model):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n\ndef get_loaders(\n    image_dir,\n    mask_dir,\n    batch_size,\n    train_transform,\n    val_transform,\n    num_workers=4,\n    pin_memory=True,):\n    \n    \n    train_ds = CXRDataset(\n        image_dir=image_dir,\n        mask_dir=mask_dir,\n        type=\"train\",\n        transform=train_transform,\n    )\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n        shuffle=True,\n    )\n\n    val_ds = CXRDataset(\n        image_dir=image_dir,\n        mask_dir=mask_dir,\n        type=\"test\",\n        transform=val_transform,\n    )\n\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n        shuffle=False,\n    )\n\n    return train_loader, val_loader\n\ndef check_accuracy(loader, model, device=\"cuda\"):\n    num_correct = 0\n    num_pixels = 0\n    dice_score = 0\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device)\n            y = y.to(device).unsqueeze(1)\n            preds = torch.sigmoid(model(x))\n#             print(preds.shape)\n            preds = (preds > 0.5).float()\n            num_correct += (preds == y).sum()\n            num_pixels += torch.numel(preds)\n            dice_score += (2 * (preds * y).sum()) \/ (\n                (preds + y).sum() + 1e-8\n            )\n\n    print(f\"Got {num_correct}\/{num_pixels} with acc {num_correct\/num_pixels*100:.2f}\")\n    print(f\"Dice score: {dice_score\/len(loader)}\")\n    model.train()\n\ndef save_predictions_as_imgs(loader, model, folder=\"saved_images\/\", device=\"cuda\"):\n    \n    model.eval()\n    for idx, (x, y) in enumerate(loader):\n        x = x.to(device=device)\n        with torch.no_grad():\n            preds = torch.sigmoid(model(x))\n#             preds.shape\n            preds = (preds > 0.5).float()\n        torchvision.utils.save_image(\n            preds, f\"{folder}\/pred_{idx}.png\"\n        )\n        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}{idx}.png\")\n\n    model.train()","deae1448":"# hyperparams\nlr = 1e-4\ndev = \"cuda\"\nbatch_size = 16\nepochs = 10\nworkers= 4\nimg_h = 512\nimg_w = 512\npin_mem= True\nload_model = False\n","f6f9f4cf":"def train_fn(loader, model, optimizer, loss_fn, scaler):\n    loop = tqdm(loader)\n\n    for batch_idx, (data, targets) in enumerate(loop):\n        data = data.to(device=dev)\n        targets = targets.float().unsqueeze(1).to(device=dev)\n\n        # forward\n        with torch.cuda.amp.autocast():\n            predictions = model(data)\n            loss = loss_fn(predictions, targets)\n\n        # backward\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        # update tqdm loop\n        loop.set_postfix(loss=loss.item())","b9e2723c":"train_transform = A.Compose(\n        [\n            A.Resize(height=img_h, width=img_w),\n            A.Rotate(limit=35, p=1.0),\n            A.HorizontalFlip(p=0.5),\n            A.VerticalFlip(p=0.1),\n            A.Normalize(\n                mean=[0.0, 0.0, 0.0],\n                std=[1.0, 1.0, 1.0],\n                max_pixel_value=255.0,\n            ),\n            ToTensorV2(),\n        ],\n    )\n\nval_transforms = A.Compose(\n    [\n        A.Resize(height=img_h, width=img_w),\n        A.Normalize(\n            mean=[0.0, 0.0, 0.0],\n            std=[1.0, 1.0, 1.0],\n            max_pixel_value=255.0,\n        ),\n        ToTensorV2(),\n    ],\n)\n\nmodel = UNet(input_channel=3, output_channel=1).to(dev)\nloss_fn = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\ntrain_loader, val_loader = get_loaders(\n    img_dir,\n    mask_dir,\n    batch_size,\n    train_transform,\n    val_transforms,\n    workers,\n    pin_mem,\n)\n\n# if LOAD_MODEL:\n#     load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model)\n\n\ncheck_accuracy(val_loader, model, device=dev)\nscaler = torch.cuda.amp.GradScaler()\n\nfor epoch in range(epochs):\n    train_fn(train_loader, model, optimizer, loss_fn, scaler)\n\n    # save model\n    checkpoint = {\n        \"state_dict\": model.state_dict(),\n        \"optimizer\":optimizer.state_dict(),\n    }\n    save_checkpoint(checkpoint)\n\n    # check accuracy\n    check_accuracy(val_loader, model, device=dev)\n\n    # print some examples to a folder\n    save_predictions_as_imgs(val_loader, model, folder=\"saved_images\/\", device=dev)","33d57030":"## Training UNets in Pytorch","48be3e0a":"References: \n\nhttps:\/\/paperswithcode.com\/paper\/u-net-convolutional-networks-for-biomedical\n\n\nhttps:\/\/github.com\/aladdinpersson\/Machine-Learning-Collection\/blob\/master\/ML\/Pytorch\/image_segmentation\/"}}