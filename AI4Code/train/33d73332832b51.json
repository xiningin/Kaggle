{"cell_type":{"a5c6f351":"code","85b695f4":"code","badd1e3e":"code","620bada1":"code","ffe2ae46":"code","587782d4":"code","fd5a1471":"code","084ece78":"code","2f94c37f":"code","eee1bca5":"code","4711f698":"code","7fc5ea4e":"code","e560a768":"code","06632b3d":"code","96fa9bcf":"code","d073e75e":"code","094bcf5e":"code","5ea72ab6":"code","f176993e":"code","2755cf36":"code","19897466":"code","7203285d":"code","ff2fbded":"code","b80f6cf3":"code","b60eade8":"code","b5a2583a":"code","944e606b":"code","79137da7":"code","167f1def":"code","e8ba3c48":"code","64f7cdb4":"code","474c27a3":"code","2af04eb9":"code","b6ad6329":"code","e727ec21":"code","8e7c207a":"code","0012b304":"code","e9ab6ff0":"code","8e665411":"code","40d38315":"code","219ed5e3":"code","edad5ec6":"code","5fb0fb4e":"code","ae792c4e":"code","afb6b2f7":"code","1f06a09d":"code","f45ad6a0":"code","27b199a8":"code","ae1c7e4f":"code","1a5469e7":"code","704b6832":"code","5cbe6552":"code","6b78791d":"code","86a6454f":"code","842281e3":"code","755e30a6":"code","519560b8":"code","72495f0a":"code","682fad15":"code","1cc2e876":"code","11a7f94e":"code","3f47d0a1":"code","6d53856f":"code","a25b2eaf":"code","20c60a3c":"code","a260f7bb":"code","92b38074":"code","7fc471b9":"code","f30cadaf":"markdown","6efbaacb":"markdown","923269ad":"markdown","3adc80ce":"markdown","1c92fd9e":"markdown","bfe6cce1":"markdown","f0610d4c":"markdown","7a57ff43":"markdown","273c871e":"markdown","8c7e28f0":"markdown","58e919fb":"markdown","3cc1ebf2":"markdown","00737d64":"markdown","0a9a660d":"markdown","d14b3fd7":"markdown","38b5a243":"markdown","092e5701":"markdown","aad70808":"markdown","18a2ae3d":"markdown","35fddfa9":"markdown","17d9def2":"markdown","58829e5c":"markdown","094035b4":"markdown","b9f661ba":"markdown","6cf634d1":"markdown","9a72a82a":"markdown","bfd44095":"markdown","50e8778e":"markdown","6778f2f1":"markdown","3352303f":"markdown","6893713a":"markdown","3f8965cd":"markdown","a4aef3c8":"markdown","f2799146":"markdown","5c13041b":"markdown","35ed52d5":"markdown","b7986bcc":"markdown","7f556247":"markdown","1f253160":"markdown","f7eacf57":"markdown","b9c39ceb":"markdown","85c233fe":"markdown","12e3dd35":"markdown","ea9b47a5":"markdown","5780abba":"markdown","ed60bddf":"markdown","8002089f":"markdown","c54dee68":"markdown","8671c6dc":"markdown","09d3b51f":"markdown","e67f8956":"markdown"},"source":{"a5c6f351":"# Pandas library in python to read the csv file.\nimport pandas as pd\n\n# for numerical computaions use numpy library\nimport numpy as np\n\n# data visualization\nimport missingno as msno\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\n# Algorithms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\n \nfrom sklearn.metrics import accuracy_score  #for accuracy_score\nfrom sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nfrom sklearn.metrics import confusion_matrix #for confusion matrix","85b695f4":"# Create a pandas dataframe and assign it to variable.\ntitanic = pd.read_csv('..\/input\/titanic\/train.csv')\ntitanic_test = pd.read_csv('..\/input\/titanic\/test.csv')","badd1e3e":"# Print first 5 rows of the dataframe.\ntitanic.head()","620bada1":"# Print Last 5 rows of the dataframe.\ntitanic_test.tail() ","ffe2ae46":"# gives shape of datase in (rows,columns)\ntitanic.shape","587782d4":"# Describe gives us statistical information about numerical columns in the dataset\ntitanic.describe()","fd5a1471":"# unique values or range for feature set\nprint('Genders:', titanic['Sex'].unique())\nprint('Embarked:', titanic['Embarked'].unique())\nprint('Pclass:', titanic['Pclass'].unique())\nprint('Survived:', titanic['Survived'].unique())\nprint('SibSp Range:', titanic['SibSp'].min(),'-',titanic['SibSp'].max())\nprint('Parch Range:', titanic['Parch'].min(),'-',titanic['Parch'].max())\nprint('Family size range:', (titanic['Parch']+titanic['SibSp']).min(),'-',(titanic['Parch']+titanic['SibSp']).max())\nprint('Fare Range:', titanic['Fare'].min(),'-',titanic['Fare'].max())","084ece78":"# info method provides information about dataset like \n# total values in each column, null\/not null, datatype, memory occupied etc\ntitanic.info()","2f94c37f":"msno.matrix(titanic)","eee1bca5":"msno.matrix(titanic_test)","4711f698":"# Let's write a function to print the total percentage of the missing values.\n# (This can be a good excercise for beginers to try to write sample function like this)\n\n# This function takes a Dataframe (df) as input and returns two columns,total missing values and total missing alues percentage\ndef missing_data(df):\n    total = df.isnull().sum().sort_values(ascending = False)\n    percent = round(df.isnull().sum().sort_values(ascending = False) * 100 \/len(df),2)\n    return pd.concat([total,percent], axis = 1 ,keys = ['total','percent'])","7fc5ea4e":"missing_data(titanic)","e560a768":"# check missing values in test dataset\nmissing_data(titanic_test)","06632b3d":"# COMPLETING: complete or delete missing values in train and test dataset\ndataset = [titanic,titanic_test]\n\nfor data in dataset:\n    # coplete missing age with median\n    data['Age'].fillna(data['Age'].median(),inplace = True)\n    \n    # complete Embarked with mode\n    data['Embarked'].fillna(data['Embarked'].mode()[0], inplace = True)\n    \n    # complete missing Fare with median\n    data['Fare'].fillna(data['Fare'].median(),inplace = True)","96fa9bcf":"missing_data(titanic)","d073e75e":"titanic.drop(['Cabin'], axis=1, inplace = True)\ntitanic_test.drop(['Cabin'],axis=1,inplace=True)","094bcf5e":"titanic.head()","5ea72ab6":"titanic_test.head()","f176993e":"missing_data(titanic)","2755cf36":"net_Survived=titanic['Survived'].value_counts().to_frame().reset_index().rename(columns={'index':'Survived','Survived':'count'})","19897466":"fig = go.Figure([go.Pie(labels=net_Survived['Survived'], values=net_Survived['count'])])\n\nfig.update_traces(hoverinfo='label+percent', textinfo='value+percent', textfont_size=15,insidetextorientation='radial')\n\nfig.update_layout(title=\"Travellers survived on titanic\",title_x=0.5)\nfig.show()","7203285d":"age_analysis=titanic[titanic['Survived']==1]['Sex'].value_counts().reset_index().rename(columns={'index':'Sex','Sex':'count'})","ff2fbded":"fig = go.Figure(go.Bar(x=age_analysis['Sex'],y=age_analysis['count']))\nfig.update_layout(autosize=False,width=400,height=500,title_text='Analysis of Survived travellers by gender',xaxis_title=\"sex\",yaxis_title=\"count\",paper_bgcolor=\"lightsteelblue\")\nfig.show()","b80f6cf3":"def draw(graph):\n    for p in graph.patches:\n        height = p.get_height()\n        graph.text(p.get_x()+p.get_width()\/2., height + 5,height ,ha= \"center\")","b60eade8":"sns.set(style=\"darkgrid\")\nplt.figure(figsize = (5, 6))\nx = sns.countplot(titanic['Sex'])\ndraw(x)","b5a2583a":"plt.figure(figsize = (10, 6))\ngraph  = sns.countplot(y = \"Embarked\", hue =\"Survived\", data = titanic)\nfor p in graph.patches:\n        Total = '{:,.0f}'.format(p.get_width())\n        x = p.get_x() + p.get_width() + 0.02\n        y = p.get_y() + p.get_height()\/2\n        graph.annotate(Total, (x, y))","944e606b":"FGrid = sns.FacetGrid(titanic, row='Pclass', aspect=2)\nFGrid.map(sns.pointplot, 'Embarked', 'Survived', 'Sex', palette=None,  order=None, hue_order=None)\nFGrid.add_legend()","79137da7":"titanic.drop(['Embarked'], axis=1, inplace = True)\ntitanic_test.drop(['Embarked'],axis=1,inplace=True)","167f1def":"titanic=titanic.dropna()\ntitanic['age_category']=np.where((titanic['Age']<19),\"below 19\",\n                                 np.where((titanic['Age']>18)&(titanic['Age']<=30),\"19-30\",\n                                    np.where((titanic['Age']>30)&(titanic['Age']<=50),\"31-50\",\n                                                np.where(titanic['Age']>50,\"Above 50\",\"NULL\"))))\nage=titanic['age_category'].value_counts().to_frame().reset_index().rename(columns={'index':'age_category','age_category':'Count'})","e8ba3c48":"titanic_age=titanic['age_category'].value_counts().to_frame().reset_index().rename(columns={'index':'age_category','age_category':'count'})","64f7cdb4":"colors=['pink','teal','orange','green']\nfig = go.Figure([go.Pie(labels=titanic_age['age_category'], values=titanic_age['count'])])\nfig.update_traces(hoverinfo='label+percent', textinfo='percent+label', textfont_size=15,\n                 marker=dict(colors=colors, line=dict(color='#000000', width=2)))\nfig.update_layout(title=\"Titanic Age Categories\",title_x=0.5)\nfig.show()","474c27a3":"titanic['survived_or_not']=np.where(titanic['Survived']==1,\"Survived\",np.where(titanic['Survived']==0,\"Died\",\"null\")) # .head(2)'\n\nsun_df=titanic[['Sex','survived_or_not','age_category','Fare']].groupby(['Sex','survived_or_not','age_category']).agg('sum').reset_index()","2af04eb9":"fig = px.sunburst(sun_df, path=['Sex','survived_or_not','age_category'], values='Fare')\nfig.update_layout(title=\"Titanic dataset distribution by Drilldown (Sex, Survived, Age Categories)\",title_x=0.5)\nfig.show()","b6ad6329":"sur_age=titanic[titanic['Survived']==1]['Age']\nun_age=titanic[titanic['Survived']==0]['Age']","e727ec21":"fig = go.Figure(go.Box(y=sur_age,name=\"Age\")) \nfig.update_layout(title=\"Distribution of Age by Survived travellers\", autosize=False, width=600, height=700)\nfig.show()","8e7c207a":"fig = go.Figure(go.Box(y=un_age,name=\"Age\")) \nfig.update_layout(title=\"Distribution of Age By Unsurvived tarvellers\", autosize=False, width=600, height=700)\nfig.show()","0012b304":"ax = sns.countplot(y=\"Pclass\", hue=\"Survived\", data=titanic, palette=\"Set1\")\nfor p in ax.patches:\n        Total = '{:,.0f}'.format(p.get_width())\n        x = p.get_x() + p.get_width() + 0.02\n        y = p.get_y() + p.get_height()\/2\n        ax.annotate(Total, (x, y))","e9ab6ff0":"# combine test and train as single to apply some function, we will use it again in Data Preprocessing\nall_data=[titanic,titanic_test]\n\nfor dataset in all_data:\n    dataset['Family'] = dataset['SibSp'] + dataset['Parch'] + 1","8e665411":"sns.set(style=\"darkgrid\")\nplt.figure(figsize = (7, 6))\nx = sns.countplot(titanic['Family'])\ndraw(x)","40d38315":"surfamily_size = titanic[titanic['Survived'] == 1]","219ed5e3":"fig = go.Figure(data=go.Violin(y=surfamily_size['Family'],\n                               marker_color=\"blue\",\n                               x0='Family size'))\n\nfig.update_layout(title=\"Survived travellers family size\")\nfig.show()","edad5ec6":"unfamily_size = titanic[titanic['Survived'] == 0]","5fb0fb4e":"fig = go.Figure(data=go.Violin(y=unfamily_size['Family'],\n                               marker_color=\"blue\",\n                               x0='Family size'))\n\nfig.update_layout(title=\"Unsurvived travellers family size\")\nfig.show()","ae792c4e":"axes = sns.factorplot('Family','Age','Survived',\n                      data=titanic, aspect = 2,kind='bar', orient='v',palette=\"Set2\")","afb6b2f7":"# create bin for age features. \nfor dataset in all_data:\n    dataset['Age_bin'] = pd.cut(dataset['Age'], bins=[0,12,20,40,120], labels=['Children','Teenage','Adult','Elder'])","1f06a09d":"plt.figure(figsize = (8, 5))\nbin = sns.countplot(x='Age_bin', hue='Survived', data=titanic,palette=\"Set1\")\ndraw(bin)","f45ad6a0":"for dataset in all_data:\n    dataset['Fare_bin'] = pd.cut(dataset['Fare'], bins=[0,10,50,100,550], labels=['Low_fare','medium_fare','Average_fare','high_fare'])","27b199a8":"plt.figure(figsize = (8, 5))\nsns.countplot(x='Pclass', hue='Fare_bin', data=titanic)","ae1c7e4f":"pd.DataFrame(abs(titanic.corr()['Survived']).sort_values(ascending = False))","1a5469e7":"# Generate a mask for the upper triangle (taken from seaborn example gallery)\ncorr=titanic.corr()  #['Survived']\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nplt.subplots(figsize = (14,8))\nsns.heatmap(corr, \n            annot=True,\n            mask = mask,\n            cmap = 'Blues',\n            linewidths=.9, \n            linecolor='white',\n            vmax = 0.3,\n            fmt='.2f',\n            center = 0,\n            square=True)\nplt.yticks(rotation = 0)\nplt.title(\"Correlation Matrix\", y = 1,fontsize = 25, pad = 20);","704b6832":"titanic.info()","5cbe6552":"drop_col= [\"survived_or_not\",\"age_category\"]\ntitanic.drop(drop_col,axis=1,inplace=True)","6b78791d":"# Convert \u2018Sex\u2019 feature into numeric.\ngenders = {\"male\": 0, \"female\": 1}\n\nfor dataset in all_data:\n    dataset['Sex'] = dataset['Sex'].map(genders)\ntitanic['Sex'].value_counts()","86a6454f":"for dataset in all_data:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[ dataset['Age'] <= 15, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 15) & (dataset['Age'] <= 20), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 20) & (dataset['Age'] <= 26), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 26) & (dataset['Age'] <= 28), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 28) & (dataset['Age'] <= 35), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 35) & (dataset['Age'] <= 45), 'Age'] = 5\n    dataset.loc[ dataset['Age'] > 45, 'Age'] = 6\ntitanic['Age'].value_counts()","842281e3":"for dataset in all_data:\n    drop_column = ['Age_bin','Fare','Name','Ticket', 'PassengerId','SibSp','Parch','Fare_bin']\n    dataset.drop(drop_column, axis=1, inplace = True)","755e30a6":"all_features = titanic.drop(\"Survived\",axis=1)\nTargete = titanic[\"Survived\"]\nX_train,X_test,y_train,y_test = train_test_split(all_features,Targete,test_size=0.3,random_state=0)\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","519560b8":"titanic.head()","72495f0a":"model = LogisticRegression()\nmodel.fit(X_train,y_train)\nprediction_lr=model.predict(X_test)\nLog_acc = round(accuracy_score(prediction_lr,y_test)*100,2)\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nLog_cv_acc=cross_val_score(model,all_features,Targete,cv=10,scoring='accuracy')\n\nprint('The accuracy of the Logistic Regression is',Log_acc)\nprint('The cross validated score for Logistic REgression is:',round(Log_cv_acc.mean()*100,2))","682fad15":"knn = KNeighborsClassifier(n_neighbors = 3) \nknn.fit(X_train, y_train)  \nY_pred = knn.predict(X_test)  \nacc_knn = round(knn.score(X_train, y_train) * 100, 2)\nkfold = KFold(n_splits=10, random_state=22) \nresult_knn=cross_val_score(model,all_features,Targete,cv=10,scoring='accuracy')\n\nprint('The accuracy of the K Nearst Neighbors Classifier is',round(accuracy_score(Y_pred,y_test)*100,2))\nprint('The cross validated score for K Nearest Neighbors Classifier is:',round(result_knn.mean()*100,2))","1cc2e876":"from sklearn.naive_bayes import GaussianNB\nmodel= GaussianNB()\nmodel.fit(X_train,y_train)\nprediction_gnb=model.predict(X_test) \nnb_acc = round(accuracy_score(prediction_gnb,y_test)*100,2)\nkfold = KFold(n_splits=12, random_state=22)\nresult_gnb=cross_val_score(model,all_features,Targete,cv=12,scoring='accuracy')\n\nprint('The accuracy of the Gaussian Naive Bayes Classifier is',nb_acc)\nprint('The cross validated score for Gaussian Naive Bayes classifier is:',round(result_gnb.mean()*100,2))","11a7f94e":"linear_svc = LinearSVC()\nlinear_svc.fit(X_train, y_train)\n\nY_pred = linear_svc.predict(X_test)\n\nacc_linear_svc = round(linear_svc.score(X_train, y_train) * 100, 2)\nkfold = KFold(n_splits=5, random_state=22)\nresult_svm=cross_val_score(model,all_features,Targete,cv=10,scoring='accuracy')\n\nprint('The accuracy of the Support Vector Machines Classifier is',acc_linear_svc)\nprint('The cross validated score for Support Vector Machines Classifier is:',round(result_svm.mean()*100,2))","3f47d0a1":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, y_train)\n\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, y_train)\nacc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\n\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nresult_rm=cross_val_score(model,all_features,Targete,cv=10,scoring='accuracy')\n\nprint('The accuracy of the Random Forest Classifier is',acc_random_forest)\nprint('The cross validated score for Random Forest Classifier is:',round(result_rm.mean()*100,2))","6d53856f":"decision_tree = DecisionTreeClassifier() \ndecision_tree.fit(X_train, y_train)\nY_pred = decision_tree.predict(X_test) \nacc_decision_tree = round(decision_tree.score(X_train, y_train) * 100, 2)\n\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nresult_rm=cross_val_score(model,all_features,Targete,cv=10,scoring='accuracy')\n\nprint('The accuracy of the Random Forest Classifier is',acc_decision_tree)\nprint('The cross validated score for Random Forest Classifier is:',round(result_rm.mean()*100,2))","a25b2eaf":"results = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes',  \n              'Decision Tree'],\n    'Score': [acc_linear_svc, acc_knn, Log_acc, \n              acc_random_forest, nb_acc, acc_decision_tree]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Model')\nresult_df.head(9)","20c60a3c":"predictions = cross_val_predict(random_forest, X_train, y_train, cv=3)\nc_mat = confusion_matrix(y_train, predictions)\nprint(c_mat)","a260f7bb":"# we will see our confusion matrix in percentage.\nsns.heatmap(c_mat\/np.sum(c_mat), annot=True, \n            fmt='.2%', cmap='Blues')","92b38074":"from sklearn.metrics import precision_score, recall_score\nprint(\"Precision:\", precision_score(y_train, predictions))\nprint(\"Recall:\",recall_score(y_train, predictions))","7fc471b9":"from sklearn.metrics import f1_score\nf1_score(y_train, predictions)","f30cadaf":"Random forest and Descision tree are the best models for us here.","6efbaacb":"As we created new fetures form existing one, so we remove that one.\n\nDropping SibSp & Parch because we have family now. same way Age.\nWe also going to remove some other features like passenger id in list, Ticket number and Name.","923269ad":"we can see  that single person have a high survival probability.\n\nFrom plot we can say  smaller family higher survival rate.\n\n**same way we can check for unsurvied travellers family size.**","3adc80ce":"As per above plot we can say that if you are traveling alone or family of 2 members and your age is around 30 than your survival chance is almost 50%. While you have family of 5 members and all are around 30 then your survival chance very high.\n\nAlso we can see that survival chance is next to zero if your family members more than 7.\n\nSo family & age features are very important.","1c92fd9e":"### 4. Age Analysis","bfe6cce1":"<img src=\"https:\/\/i.ibb.co\/WFKW312\/titanic-conspiracy-rms-olympic-gettyimages-1055101284.jpg\" alt=\"titanic\" style=\"width:700px;height:400px;\">","f0610d4c":"### Note: Column \"Cabin\" has more than 75% of missing values in both train and test dataset.\n#### Suggestion: Not to impute missing data in columns, which have more than 40% of missing data.","7a57ff43":"We already have Survived column so we drop the colom with name \"survived_or_not\" ","273c871e":"##  Load the Data","8c7e28f0":"### 6. Decision tree","58e919fb":"There is no Survived column here which is our target varible we are trying to predict.\n\n","3cc1ebf2":"Here we see that Pclass is contributing to a persons chance of survival, especially if this person is in class 1. We will create another pclass plot below.\n\n### 5. SibSp and Parch Analysis\n\nSibSp and Parch would make more sense, Parents not let child die, Bond of Blood relation always help each other first, rather than helping others they think about them self and them family member.\n\nCreate new feature Family Size as a combination of SibSp and Parch","00737d64":"**Lets clear our vision by another graph.**","0a9a660d":"Here, 549 Travellers died in tragedy and 342 Travellers save their lives. About 38% of people alive in tragedy.\n\n### 2. Gender Analysis\n\nLets Check how many male and female survived on titanic.","d14b3fd7":"On port S and C has more survival chances. Also we can see women have a high survival probability.\n\nBased on this senario Embarked looks to be correlated with survival, depending on the Pclass.\nMore chances of surviaval if travellers are In Pclass 1.\n\nFrom which location passenger start journey does it matter or its more important that passenger is on Titanic, no matter from where you start journey. \n\nas we know that **At 2:20 a.m. on April 15, 1912, the British ocean liner Titanic sinks into the North Atlantic Ocean.**\nIts night time, high cold weather **(The temperature of the water was -2.2 degrees Celsius when Titanic was sinking)**, and Job-Location\/Rest-room(Passenger class) allocated to everyone on Titanic.\n\nWe can use Embarked as feature here for getting high accuracy but logically its doesn't matter. so we drop it out.\n\nAs a part of data science you have to think 360 degree angle, some features are important but logically its not, so thats why you must have domain knowledge for feature selection.","38b5a243":"## All about Data \n\n\n<span style='font-family:Georgia'>\n    <table>\n        <tr>\n            <th>Variable<\/th>\n            <th>Key<\/th>\n            <th>Definition<\/th>\n        <\/tr>\n        <tr>\n            <td>survival<\/td>\n            <td>0 = No, 1 = Yes<\/td>\n            <td>Whether person survived or not<\/td>\n        <\/tr>\n        <tr>\n            <td>pclass<\/td>\n            <td>1 = 1st, 2 = 2nd, 3 = 3rd<\/td>\n            <td>1st = Upper,2nd = Middle,3rd = Lower<\/td>\n        <\/tr>\n        <tr>\n            <td>sex<\/td>\n            <td>male,female<\/td>\n            <td>sex of the passenger<\/td>\n        <\/tr>\n        <tr>\n            <td>Age<\/td>\n            <td>Continuous varivale<\/td>\n            <td>Age in years<\/td>\n        <\/tr>\n        <tr>\n            <td>sibsp<\/td>\n            <td>numeric values<\/td>\n            <td># siblings \/ spouses aboard the Titanic<br>Sibling = brother, sister, stepbrother, stepsister<br>Spouse = husband, wife<\/td>\n        <\/tr>\n        <tr>\n            <td>parch<\/td>\n            <td>numeric values<\/td>\n            <td># parents \/ children aboard the Titanic<br> Parent = mother, father <br> Child = daughter, son, stepdaughter, stepson <br>Some children travelled only with a nanny, therefore parch=0 for them<\/td>\n        <\/tr>\n        <tr>\n            <td>ticket<\/td>\n            <td>numeric values<\/td>\n            <td>Ticket number<\/td>\n        <\/tr>\n        <tr>\n            <td>fare<\/td>\n            <td>numeric values<\/td>\n            <td>Passenger fare<\/td>\n        <\/tr>\n        <tr>\n            <td>cabin<\/td>\n            <td>numeric values<\/td>\n            <td>Cabin number<\/td>\n        <\/tr>\n        <tr>\n            <td>embarked<\/td>\n            <td>C = Cherbourg, Q = Queenstown, S = Southampton<\/td>\n            <td>Port of Embarkment<\/td>\n        <\/tr>\n    <\/table>  \n<\/span>\n    ","092e5701":"Clearly we can see that survival chance of female is more than male.Because total nuumber of male 577 and survived male are only 109.total nuumber of male 314 and survived female are only 233.\n\n### 3.Embarked and Fare Analysis\n\nEmbardked means From which location passengers go on board to Titanic.\n\nHere we have three embarkment point\n\n* C = Cherbourg\n\n* Q = Queenstown\n\n* S = Southampton","aad70808":"We plot this graph to check outliers of age column.\n\nHere, we can see that average age of survived person near to 30.\n\nand maximum survived passenger's age range lies between 22 to 35 years.\n\n#### what is Outlier?\nOutlier is an observation that appears far away and diverges from an overall pattern in a sample.\n\n<img src=\"https:\/\/i.ibb.co\/HNjCZ4s\/images-mod1-spread11.gif\" alt=\"outlier\" width=\"500\" height=\"350\">\n<ul>\n\nIn our plot there are blue points above upper fence,those are all ouliers.\nMeans our age range is 0.42 to 56 and rare age is above 56.Those are 58,60,62,63 and 80.\n\nThis plot show us value of min, max, median and quartile ranges. \n\n##### Lets do same way  check this for unsurvived passengers.","18a2ae3d":"We can see that Age, Embarked and cabin has missing values. now, lets check missing values for test data.","35fddfa9":" Also Embarked and cabin has missing values.\n \n ##  Missing Values \n \n First we will visulize missing values.In which column missing values are present?","17d9def2":"### Confusion Matrix\n\nA confusion matrix also call error matrix.\nTable that is often used to describe the performance of a classification model.\nwe will check for Random forest.\n\n\n<img src=\"https:\/\/i.ibb.co\/XJr46rH\/Confusion-Matrix.png\" alt=\"Confusion-Matrix\" width=\"390\" height=\"350\">","58829e5c":"We can say Low fare is only in 3rd Pclass.Medium fare travellers are in all class.High fare travellers are only in 1st class.\n\nPclass and Fare correlated with other.So we can drop one of them.\n\nBut if we think logically how much price I paid for ticket is not correlate to my survival chance.\n\nSo,basically we should drop fare.For surety we will create correlation matrix first.\n\n## Correlation\n\n### What is correaltion?\n\n correlation is a measure of how strongly one variable depends on another.\n \n **1.Positive correlation:**\n\nA positive correlation is a relationship between two variables in which both variables move in the same direction.\nTherefore, when one variable increases as the other variable increases, or one variable decreases while the other decreases.see it with example of age vs salary.\n\n<img src=\"https:\/\/i.ibb.co\/pKnm57p\/age-vs-salary.png\" alt=\"positive correlation\" width=\"450\" height=\"300\" data-load=\"full\" style=\"\">\n\n**2.Negative correlation**\n\nA negaitive correlation is a relationship between two variables in which both variables move in the opposite direction.\nTherefore, when one variable increases as the other variable decreases, or one variable decreases while the other increases.\n\n<img src=\"https:\/\/i.ibb.co\/Jv85tfs\/scatter-plot-negative-correlation.png\" alt=\"negative correlation\" width=\"400\" height=\"300\" data-load=\"full\" style=\"\">\n\n**Note:** only the numeric features are compared as it is obvious that we cannot correlate between alphabets or strings. ","094035b4":"#### How are the Age spread for travellers?","b9f661ba":"### 1. Logistic Regression:","6cf634d1":"### About RMS Titanic\n\nThe reason the titanic is often referred to as 'RMS Titanic' is because of Royal Mail Ship.\nThe RMS Titanic, a luxury steamship, sank in the early hours of April 15, 1912, off the coast of Newfoundland in the North Atlantic after sideswiping an iceberg during its maiden voyage. Of the 2,240 passengers and crew on board, more than 1,500 lost their lives in the disaster. \n\nWe will go through the whole process of creating a machine learning model on the famous Titanic dataset, which is used by many people all over the world. It provides information on the fate of passengers on the Titanic, summarized according to economic status (class), sex, age and survival. In this challenge, we will predict whether a passenger on the titanic would have been survived or not.","9a72a82a":"<div class=\"alert alert-block alert-info\">\n    <span style='font-family:Georgia'>\n       \n## Conclusion\n\nThis was my very first Kaggle competition and climbing up the leaderboard one step at a time was definitely a really nice journey.\n\nThank you for taking the time to read through my first exploration of a Kaggle dataset.\n\nplease feel free to leave me any comments with regards if you found this notebook useful or you just liked it. I would really appreciate it!\n\n<h2 style=\"color:green\"><center>Plz Upvote !!","bfd44095":"here we predicted 333 travellers not survive and that actually not survive which is correct(**True Negative**).and 50 where wrongly classified as not survived (false positives).\n\nwe predicted that 167 travellers survive and that is actually true.(**True Positive:**)\nwe predicted that 75 travellers classified as survived .(False Negative)","50e8778e":"### Precision and Recall:\n\nprecision refers to the percentage of results which are relevant, recall refers to the percentage of total relevant results correctly classified by our algorithm.","6778f2f1":"we can check from count if there are missing values in columns, here 'age' has missing values.\n\nAlso we can see that 38% out of the training-set survived in Titanic.\n\nWe can also see that the passenger's age range from 0.4 to 80.","3352303f":"## Feature Engineering\n\nFeature engineering is the art of converting raw data into useful features.To help us get a better performance, we can create new features based on the original features of our dataset.\n\nwe will see first which are not numeric data and than after convert them into numeric data.","6893713a":"### 5. Random Forest:","3f8965cd":"## Table of content:\n\n* About RMS Titanic\n* All about Data\n* Import Necessary Libraries\n* Load the data\n* Data analysis\n* Handle Missing Values\n* Data Exploration\/ Visualizing\n* Correlation & Matrix\n* Feature Engineering\n* Predictive Modeling\n> 1. Logistic Regression\n> 2. KNN Classifier\n> 3. Gaussian Naive Bayes\n> 4. Support Vector Machine(SVM)\n> 5. Random Forest\n> 6. Decision Tree\n* Confusion Matrix","a4aef3c8":"### 4. Linear Support Vector Machine:","f2799146":"## Which is the best Model ?","5c13041b":"## Predictive Modeling\n\nAfter all the preprocessing, we are now ready for building and evaluating different Machine Learning models.\n\nWe have seen some insights from the data analysis. But with that, we cannot accurately predict whether a passenger will survive or die. So now we will predict whether a Passenger will survive or not using some great Classification Algorithms.","35ed52d5":"## Data Analysis","b7986bcc":"## Import  Necessary Libraries","7f556247":"Embarked seems to be correlated with survival, depending on the gender.\n\nBut just think, Is these all parameters are important for survivalance of people?\nIs there a correlation between port of embarkment and survival chances?\n\nTo find answers of these questions we plot some other graphs related to Embarked,Fare ,class and age.\n","1f253160":"### 2. K Nearest Neighbor:","f7eacf57":"## Data Exploration\/ Visulization\n\n### 1. Survival Analysis\n\nLet's check through plotting how many passenger & crew survived on ship.","b9c39ceb":"Some insight in Fare information\n\n* A higher family size doesn't necessarily indicate higher Fare.\n\n* Passengers in class 2 and 3 paid a fare of under 100 bucks.\n\n* Most passengers paid under 50 bucks of fare.\n\nIt seems that passenger fare depends on their travel class in our model.lets prove with prove.","85c233fe":"### F-Score\n\nThe F-score is computed with the harmonic mean of precision and recall. Note that it assigns much more weight to low values. As a result of that, the classifier will only get a high F-score, if both recall and precision are high.","12e3dd35":" we will see how to deal with these missing valus next.","ea9b47a5":"cabin, age and fare has missing values in test data.","5780abba":"### 4. Passanger Class(Pclass) analysis \n\nwe will check whether Upper class or lower class affect survival rate.","ed60bddf":"**Now we will see whether Age is considerable with family size for higher probability of survival?**","8002089f":"### 3. Gaussian Naive Bayes","c54dee68":"# Welcome to Simple guide Kernel\n\nI am quite a newcomer to the Kaggle scene as well and the first proper kaggle script.The Titanic dataset is a prime candidate for introducing the concept of Machine learning as many newcomers like me to Kaggle start out here. \nThe objective of this notebook is to follow a step-by-step workflow, explaining each step.\n\nI hope that anyone, regardless of their python skills can find something useful and helpful.\nplease feel free to leave me any comments with regards to how I can improve.\n\n<h2 style=\"color:blue\"><center> Don't forget to upvote if you like it! It's free!! ","8671c6dc":"Out of 342 survived travellers there are 233 female and 109 male.We can see that survival chance of female is more than  male.but for surety check total number of male and female. ","09d3b51f":"now, lets check missing values for test data.","e67f8956":"Here, Survived count is higher for Adult.\nfor children and teenage is almost eual chance of survival.\n\n### 6. Fare Analysis\n\nwe are going to create bins for different fare price level."}}