{"cell_type":{"1999b2eb":"code","f09251e7":"code","ab2cd7d0":"code","05c7ff56":"code","a6cefda2":"code","274991be":"code","12869ff2":"code","2b9e9b14":"code","79e8fd39":"code","8be8f258":"code","632aaf80":"code","c0437308":"code","4e40953d":"code","f06f9620":"code","1a9aea2e":"code","9743b8b0":"code","84714f06":"code","83d709c3":"code","4c0eb0e4":"code","f1e7afbd":"code","6ff8666d":"code","a71d78b9":"code","8ae4dadf":"code","f693890a":"code","c2479206":"code","115e20e0":"code","9993b30c":"code","c3096da8":"code","be5fc7f1":"code","7f955745":"code","f34bc80a":"code","2fedb3c1":"code","25a20d7a":"code","910db8ca":"code","7bfc99f9":"code","a88be22f":"code","577ec02a":"code","b3534aec":"code","be310062":"code","283d1e37":"code","dbd04967":"code","804cc7c6":"code","ec1181cb":"code","c36ebc9a":"code","e9d6699b":"code","40b8700c":"code","d6524bd5":"code","16028cc8":"code","06f19285":"code","d0e1a777":"code","31a1a426":"code","5308863e":"code","3d73adff":"code","1cbc9e15":"code","259de9f1":"code","15e4fb0d":"code","272122e2":"code","7f48d083":"code","749d71ca":"code","968fa645":"code","efa2d751":"code","6bbc1aab":"code","a95b3879":"code","256cea98":"code","99b6c1c6":"code","11909321":"code","c62ebadb":"code","354bf562":"code","50e40b05":"code","86b1e1d7":"code","b701800d":"code","68befedb":"code","3662ffb0":"code","2ffff0e3":"code","013ed687":"code","23ee7b86":"code","ef3ebd85":"code","e43380e9":"code","973e87ed":"code","1e09ed55":"code","b0280c5e":"code","96509c2e":"code","db1078d9":"code","5b4dc38b":"code","e0f76529":"code","4e9f4f98":"code","3078da28":"code","c9a415f1":"code","85e9a1a4":"code","c4eef979":"code","48107fa2":"code","b3ffcf3e":"code","4291457b":"code","e740a8ec":"code","725541b8":"code","dc7b725b":"code","5d0abdee":"code","abb9943c":"code","da1b1652":"code","59353fdc":"code","9ed93a6b":"code","89c0c747":"code","1ef9a2ed":"code","c59be479":"markdown","eab60995":"markdown","3ded7a93":"markdown","1c487059":"markdown","e3446c3b":"markdown","dbec4f5e":"markdown","1c9c4674":"markdown","dde9c4e7":"markdown","953953e8":"markdown","e6735723":"markdown","c6d8b6f0":"markdown","63968d46":"markdown","bf1ddd3a":"markdown","61b618bf":"markdown","b3f6be64":"markdown","cf874b59":"markdown","acedab63":"markdown","07ac0825":"markdown","2d08ca03":"markdown","16e436a6":"markdown","a3c676c4":"markdown","3bb1ac9e":"markdown","d7e8798d":"markdown","17b68ef6":"markdown","e4428b68":"markdown","1a61a920":"markdown","1c90e495":"markdown","4b587c30":"markdown","37303c05":"markdown","867f85cb":"markdown","5a4efb32":"markdown","f0971def":"markdown","e32f0b21":"markdown","18c4170e":"markdown","b3d36e86":"markdown","0b207958":"markdown","fb6d7b4d":"markdown","558df7e4":"markdown","febe2cbf":"markdown","588a6972":"markdown","8834674b":"markdown","f0711cd1":"markdown","58bf073e":"markdown","5b345d6e":"markdown","809b7c2b":"markdown","e03d868b":"markdown","ac126977":"markdown","244d0cd0":"markdown","973c76ff":"markdown","48e685f2":"markdown","7bfdf820":"markdown","78428f56":"markdown","90fcb481":"markdown","5a4b4927":"markdown","5397bb6c":"markdown","9bec093d":"markdown","cd460465":"markdown","576c7dd9":"markdown","52287eb9":"markdown","565944d5":"markdown","fd12f483":"markdown","d87c75b1":"markdown","d9f62eee":"markdown","b8ef6db2":"markdown","c445bc02":"markdown","ce4d6402":"markdown","f4897898":"markdown","59db021d":"markdown","e1005eb7":"markdown","10201ed8":"markdown","068628e4":"markdown","9dd96e07":"markdown","d1438a0e":"markdown","2cfaaf60":"markdown","29b1433d":"markdown","cfc7c11c":"markdown","84172420":"markdown","3afc471a":"markdown","8bbac996":"markdown","144c5948":"markdown","ba92348a":"markdown"},"source":{"1999b2eb":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f09251e7":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nimport xgboost\nimport xgboost as xgb\nimport lightgbm as lgbm\n\nimport optuna\nfrom optuna.integration import LightGBMPruningCallback\nfrom sklearn.model_selection import StratifiedKFold\n\nimport shap\n\nfrom scipy.stats import normaltest\nfrom statsmodels.graphics.gofplots import qqplot\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.metrics import r2_score, accuracy_score\n\nseed=42\n\nsns.set_style(\"whitegrid\")\nsns.despine()\nsns.set_color_codes('bright')\n\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlesize=14, titlepad=10)","ab2cd7d0":"df=pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/audi.csv',skipinitialspace=True)","05c7ff56":"df.head()","a6cefda2":"df=df[['model','year','transmission','fuelType','mileage','tax','mpg','engineSize','price']]","274991be":"df.head(10)","12869ff2":"df.info()","2b9e9b14":"df['year'] = df['year'].astype(object)","79e8fd39":"df_clean = df.copy()","8be8f258":"df_models=df.drop(['price','mileage','tax'], axis=1).drop_duplicates().sort_values(by=['model','year','fuelType'])\r\ndf_models.head()","632aaf80":"def num_plots_all(df):\n    fig, ax = plt.subplots(2,5, figsize=(15,5), gridspec_kw={\"height_ratios\": (.2, .8)})\n\n    sns.boxplot(x='mileage', data=df, ax=ax[0,0])\n    ax[0,0].set_title('Mileage distribution')\n    ax[0,0].set(yticks=[], xticks=[])\n    ax[0,0].axis('off')\n\n\n    sns.boxplot(x='tax', data=df, ax=ax[0,1])\n    ax[0,1].set_title('Tax distribution')\n    ax[0,1].set(yticks=[], xticks=[])\n    ax[0,1].axis('off')\n\n    sns.boxplot(x='mpg', data=df, ax=ax[0,2])\n    ax[0,2].set_title('MPG distribution')\n    ax[0,2].set(yticks=[], xticks=[])\n    ax[0,2].axis('off')\n\n    sns.boxplot(x='engineSize', data=df, ax=ax[0,3])\n    ax[0,3].set_title('EngineSize distribution')\n    ax[0,3].set(yticks=[], xticks=[])\n    ax[0,3].axis('off')\n\n    sns.boxplot(x='price', data=df, ax=ax[0,4])\n    ax[0,4].set_title('Price distribution')\n    ax[0,4].set(yticks=[], xticks=[])\n    ax[0,4].axis('off')\n    \n    sns.histplot(x='mileage', data=df, ax=ax[1,0])\n\n    sns.histplot(x='tax', data=df, ax=ax[1,1])\n\n    sns.histplot(x='mpg', data=df, ax=ax[1,2])\n\n    sns.histplot(x='engineSize', data=df, ax=ax[1,3])\n\n    sns.histplot(x='price', data=df, ax=ax[1,4])\n\n    plt.show()","c0437308":"num_plots_all(df)","4e40953d":"df.describe()","f06f9620":"def num_plot(df, col):\n    fig, ax = plt.subplots(2, 1, sharex=True, figsize=(8,5),gridspec_kw={\"height_ratios\": (.2, .8)})\n    ax[0].set_title(col + 'distribution',fontsize=18)\n    sns.boxplot(x=col, data=df, ax=ax[0])\n    ax[0].set(yticks=[])\n    sns.histplot(x=col, data=df, ax=ax[1])\n    ax[1].set_xlabel(col, fontsize=16)\n    plt.tight_layout()\n    plt.show()","1a9aea2e":"df_clean = df","9743b8b0":"plt.figure(figsize=(4.5,4))\nsns.heatmap(df.corr(), cmap='RdBu', annot=True, vmin=-1, vmax=1)\nplt.title('Correlation Matrix')","84714f06":"num_plot(df_clean,'mileage')","83d709c3":"df_clean = df_clean[df_clean['mileage'] < 200000]","4c0eb0e4":"num_plot(df_clean,'mileage')","f1e7afbd":"df_clean = df_clean[df_clean['mileage'] < 150000]","6ff8666d":"num_plot(df_clean,'mileage')","a71d78b9":"print('We removed {} outliers!'.format(len(df) - len(df_clean)))","8ae4dadf":"df[df['mileage'] >= 150000]","f693890a":"num_plot(df_clean, 'mpg')","c2479206":"df_clean[df_clean['mpg']>100]","115e20e0":"df_hyb = df[df['fuelType'] == 'Hybrid']","9993b30c":"print('There are {} Hybrid cars in the dataset, which are {} % of the total cars'.format(len(df_hyb),len(df_hyb)\/len(df)))","c3096da8":"df_clean = df_clean[df_clean['mpg'] < 100]","be5fc7f1":"num_plot(df_clean,'mpg')","7f955745":"qqplot(df_clean.mpg, line='s')\nplt.show()","f34bc80a":"stat, p = normaltest(df_clean.mpg)\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\n# interpret\nalpha = 0.05\nif p > alpha:\n\tprint('Sample looks Gaussian (fail to reject H0)')\nelse:\n\tprint('Sample does not look Gaussian (reject H0)')","2fedb3c1":"print('We removed {} outliers!'.format(len(df) - len(df_clean)))","25a20d7a":"num_plot(df_clean, 'tax')","910db8ca":"df_tax_high = df_clean[df_clean['tax'] > 500]","7bfc99f9":"len(df_tax_high)","a88be22f":"num_plots_all(df_clean)","577ec02a":"num_plots_all(df_tax_high)","b3534aec":"df_clean = df_clean[df_clean['tax'] < 500]","be310062":"df_tax_low =  df_clean[df_clean['tax'] < 100]","283d1e37":"len(df_tax_low)","dbd04967":"num_plots_all(df_clean)","804cc7c6":"num_plots_all(df_tax_low)","ec1181cb":"num_plot(df_clean, 'engineSize')","c36ebc9a":"df_size_small = df_clean[df_clean['engineSize'] == 0]","e9d6699b":"len(df_size_small)","40b8700c":"df_size_small['model'].value_counts()","d6524bd5":"df_q3 = df_clean[df_clean['model']=='Q3']","16028cc8":"len(df_q3)","06f19285":"num_plots_all(df_q3)","d0e1a777":"df_clean= df_clean[df_clean['engineSize'] > 0]","31a1a426":"df_size_big = df_clean[df_clean['engineSize'] > 5]","5308863e":"df_size_big","3d73adff":"len(df_size_big)","1cbc9e15":"df_clean[df_clean['model']=='R8']","259de9f1":"print('So far we have removed {} outliers !'.format(len(df)-len(df_clean)))","15e4fb0d":"num_plot(df_clean,'price')","272122e2":"plt.hist(np.log(df_clean['price']))\nplt.show()","7f48d083":"price_log = np.log(df_clean['price'])","749d71ca":"qqplot(price_log, line='s')\nplt.show()","968fa645":"stat, p = normaltest(price_log)\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\n# interpret\nalpha = 0.05\nif p > alpha:\n\tprint('Sample looks Gaussian (fail to reject H0)')\nelse:\n\tprint('Sample does not look Gaussian (reject H0)')","efa2d751":"sns.heatmap(df_clean.corr(), cmap='RdBu', annot=True, vmin=-1, vmax=1)\nplt.title('Correlation Matrix')","6bbc1aab":"sns.scatterplot(y='price', x='mpg', data=df_clean, hue='fuelType')\nplt.show()","a95b3879":"sns.scatterplot(y='price', x='engineSize', data=df_clean, hue='transmission')\nplt.show()","256cea98":"sns.scatterplot(y='price', x='mileage', data=df_clean)\nplt.show()","99b6c1c6":"sns.scatterplot(y='mpg', x='tax', data=df_clean)\nplt.show()","11909321":"print('In total we removed {} outliers!'.format(len(df)-len(df_clean)))","c62ebadb":"print('Percentage of removed outliers with respect to dataframe size: {}%'.format(np.round((len(df)-len(df_clean))\/len(df),3)))","354bf562":"sns.boxplot(x='fuelType', y='price', data=df_clean, hue='transmission');\nplt.xlabel('FuelType');\nplt.legend(loc='upper right')\nplt.title('Price vs Fueltype by Transmission');","50e40b05":"sns.boxplot(x='transmission', y='price', data=df_clean, hue='fuelType');\nplt.xlabel('Transmission');\nplt.title('Price vs Transmission by Fueltype');\nplt.legend(loc='upper left')","86b1e1d7":"df_clean.head()","b701800d":"df_clean.describe()","68befedb":"encoded_df = df_clean.copy()","3662ffb0":"encoded_df['year'] = encoded_df['year'].astype(int)","2ffff0e3":"encoded_df['year'].sort_values()","013ed687":"encoded_df['year'] = encoded_df['year'] - 1997","23ee7b86":"objList = encoded_df.select_dtypes(include = \"object\").columns\nprint (objList)","ef3ebd85":"le = LabelEncoder()\nencoded_df[objList] = encoded_df[objList].apply(le.fit_transform)","e43380e9":"X=encoded_df.drop('price', axis = 1).values","973e87ed":"y=encoded_df['price'].values","1e09ed55":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = seed)","b0280c5e":"X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = seed)","96509c2e":"def objective(trial):\n    param = {\n        'metric': 'rmse', \n        'random_state': seed,\n        'n_estimators': trial.suggest_categorical('n_estimators',[1000,1500,2000,2500,3000,4000,5000]),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.006,0.01,0.02,0.05,0.1,0.15]),\n        'max_depth': trial.suggest_categorical('max_depth', [3,4,5,6,7,8,9,10,15]),\n        'num_leaves' : trial.suggest_int('num_leaves', 1, 1000),\n        'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n        'cat_smooth' : trial.suggest_int('min_data_per_groups', 1, 100),\n        'verbosity':-1\n    }\n    model = lgbm.LGBMRegressor(**param)\n    \n    model.fit(X_train, y_train, eval_set=[(X_val, y_val)],callbacks=[lgbm.early_stopping(stopping_rounds=50)])  \n    preds = model.predict(X_val)\n    rmse = MSE(y_val, preds, squared=False)\n    \n    return rmse","db1078d9":"study = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=50)\n\nprint(\"Number of finished trials: \", len(study.trials))\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nhp = study.best_params\nprint(\"  Value: {}\".format(trial.value))\nprint(\"  Params: \")\n\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","5b4dc38b":"lgbm_model = lgbm.LGBMRegressor(**hp)","e0f76529":"lgbm_model.fit(X_train, y_train, eval_set=[(X_val, y_val)],callbacks=[lgbm.early_stopping(stopping_rounds=50)])","4e9f4f98":"y_pred_lgbm = lgbm_model.predict(X_test)","3078da28":"print(\"Best rmse:\", np.sqrt(MSE(y_pred_lgbm ,y_test)))\nprint(\"R2 using LightGBM: \", r2_score(y_test, y_pred_lgbm ))","c9a415f1":"def objective(trial):\n    \n    param = {\n        'objective' : 'reg:squarederror',\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.005,0.01,0.05,0.1,0.15]),\n        'n_estimators': trial.suggest_categorical('n_estimators', [1000,1500,2000,2500,3000,3500]),\n        'max_depth': trial.suggest_categorical('max_depth', [3,4,5,6]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n    }\n\n    model = xgboost.XGBRegressor(**param)    \n    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=50, verbose=False)   \n    preds = model.predict(X_val)\n    rmse = MSE(y_val, preds, squared=False)\n\n    return rmse","85e9a1a4":"study = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=50)\n\nprint(\"Number of finished trials: \", len(study.trials))\nprint(\"Best trial:\")\ntrial = study.best_trial\nhp = study.best_params\nprint(\"  Value: {}\".format(trial.value))\nprint(\"  Params: \")\n\n\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","c4eef979":"xgb_model = xgboost.XGBRegressor(**hp)","48107fa2":"xgb_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=50)","b3ffcf3e":"y_pred_xgb = xgb_model.predict(X_test)","4291457b":"print(\"Best rmse:\", np.sqrt(MSE(y_pred_xgb,y_test)))\nprint(\"R2 using XGBoost: \", r2_score(y_test, y_pred_xgb))","e740a8ec":"plt.figure(figsize = (5.5,5))\nsns.scatterplot(x=y_test, y=y_pred_lgbm, label='LightGBM, R2 {:.2f} %'.format(r2_score(y_test, y_pred_lgbm)*100), alpha=0.8, color='r')\nsns.scatterplot(x=y_test, y=y_pred_xgb, label='XGBoost, R2 {:.2f} %'.format(r2_score(y_test, y_pred_xgb)*100), alpha=0.8, color='b')\nplt.plot([0, 140000], [0, 140000], linestyle='--')\nplt.axis([0, 140000, 0, 140000])\nplt.xlabel('Y Test')\nplt.ylabel('Predicted Y')\nplt.legend(loc='upper left', fontsize=11,fancybox=True, shadow=True, frameon=True)\nplt.text(75000,10000,'RMSE\\nLightGBM: {:.2f}$\\nXGBoost: {:.2f}$'.format(MSE(y_pred_lgbm,y_test,squared=False),\n                                                                        MSE(y_pred_xgb,y_test,squared=False)),\n         fontsize=11,\n         bbox=dict(facecolor='white', alpha=1))\nplt.title('LightGBM vs XGBoost Regression')\nplt.show()","725541b8":"print(\"RMSE using LightGBM: {} $\".format(np.round(np.sqrt(MSE(y_pred_lgbm,y_test)),2)))\nprint(\"RMSE using XGBoost: {} $\".format(np.round(np.sqrt(MSE(y_pred_xgb,y_test)),2)))","dc7b725b":"print(\"R2 using LightGBM: {} % \".format(np.round(r2_score(y_test, y_pred_lgbm)*100,2)))\nprint(\"R2 using XGBoost: {} % \".format(np.round(r2_score(y_test, y_pred_xgb)*100,2)))","5d0abdee":"xgb_model.feature_names = encoded_df.drop('price', axis = 1).columns","abb9943c":"feat_df= pd.DataFrame({'feature': xgb_model.feature_names,'importance':xgb_model.feature_importances_*100})","da1b1652":"sorted_df=feat_df.sort_values('importance', ascending=False)","59353fdc":"plt.figure(figsize=(7,4.5))\nax =sns.barplot(y='importance', x='feature', data=sorted_df.head(10), palette='mako')\nplt.title('Feature Importance to predict price by XGBoost')\nplt.ylabel('Feature Importance (%)')\nax.bar_label(ax.containers[0], fmt='%.2f')\nplt.show()","9ed93a6b":"xgb_explainer = shap.TreeExplainer(\n    xgb_model, X_train, feature_names = encoded_df.drop('price', axis = 1).columns)","89c0c747":"shap_values = xgb_explainer.shap_values(X_train, y_train)","1ef9a2ed":"shap.summary_plot(shap_values, X_train, feature_names=encoded_df.drop('price', axis = 1).columns);","c59be479":"Before starting the analysis, it could be interesting to check the correlation matrix first !","eab60995":"# Data Preparation for modeling","3ded7a93":"The following project aims to analyze a dataset containing data about audi cars to extract useful insights and develop a ML algorithm to predict the price of a used audi car given some features.<br>\nThe Project is strucured as follows:<br>\n\n- Data Cleaning\n- Exploratory Data Analysis\n- Data Preparation for ML modeling\n- ML modeling:\n    - Light GBM with OPTUNA\n    - XGBOOST with OPTUNA\n- Results Summary\n- Feature Importance analysis with XGBOOST and SHAP values","1c487059":"## Train - Validation split","e3446c3b":"There are still some values over 150'000 that we can remove !","dbec4f5e":"# Results Summary","1c9c4674":"- mileage : the mean is around 25k, a too high value due to the presence of the outlier value equals to 323'000 miles. We will remove this outlier immediately.\n- tax: the mean is around 126, and similarly to mileage this is due to the presence of some very high values.\n- mpg : the mean is around 50, which is compatible with the distribution we can see: it looks almost gaussian, we can remove the extreme values ( over mpg=100, and check for normality )\n- enginesize: there are some cars with a 0 value, which is impossible. Probably it refers to missing values, which however are very few as we can see from the distribution.\n- price: it has a clear long tail behaviour, with some prices over 100'000 $","dde9c4e7":"# Enginesize Analysis","953953e8":"We also create a copy of the dataframe, which will store the cleaned version of the dataframe itself","e6735723":"Apparently the mpg and engineSize are not uniquely identified by the car model, year, transmission and fuelType.","c6d8b6f0":"We should investigate Multivariate outliers for the features with high correlations:<br>\n- price-mpg<br>\n- price-enginesize<br>\n- price-mileage<br>\n- tax-mpg<br>","63968d46":"By looking at the histograms, boxplots and summary statistics we can have a first glance at the numerical features and say:","bf1ddd3a":"It looks like there are some cars with a mpg value higher than 100, which are those cars?","61b618bf":"# Main Results Summary Dashboard:","b3f6be64":"It looks like all of these cars with a very big engine are Audi R8!","cf874b59":"# LightGBM + OPTUNA","acedab63":"Indeed, there are very few hybrid cars in the dataset. We will see if all of them will be removed after the outlier cleaning.","07ac0825":"**The distributions does not seem to be normal with 95% confidence.** <br>\nIn particular, from the QQplot we can see that the problem lies mostly in the tails of the distribution. The distribution is still a bit right skewed.","2d08ca03":"First, we define a function to plot both the distribution and boxplot to be able to identify outliers more easily.","16e436a6":"During the model deployment, I noticed that the values in the model columns had a blank space in front. For this reason, I decided to add \"skipinitialspace=True\" during the creation of the dataframe.","a3c676c4":"We can see that cars Automatic and Semi-Auto transmission are more expensive, while cars with a manual transimission are cheaper.","3bb1ac9e":"To analyze the numerical features, we will first define a function thats plots the hisogram and boxplots of all the numerical features.","d7e8798d":"## Train - Test split","17b68ef6":"# XGBoost + OPTUNA","e4428b68":"The distribution is not normal but close enough. We will see if we can improve the ML predicting performance by log transforming the price variable.\nEDIT: By log transforming 'price', the prediction performance of both XGBoost and LightGBM seems to be inferior. This is probably due to the fact that tree-based algorithms such as XGBoost andLightGBM do not get particular benefits from this transformation.","1a61a920":"It looks like that these cars also have quite high values for enginesize (identified by outliers in the df_clean's boxplot).","1c90e495":"It looks there are two outlier clusters: tax values over 500 and lower than 100. We will analyze these two clusters","4b587c30":"Then we can proceed with encoding the categorical variables.","37303c05":"It looks the majority of these cars are A3, Q5 or Q7. In particular they all have Automatic or semi automatic transmission and most of them have Hybrid fueltype.","867f85cb":"<img src=\"https:\/\/i.imgur.com\/0F1SLIj.png\" width=\"900px\">","5a4efb32":"# How is transmission related to price?","f0971def":"We can subtract the first year in chronological order, 1997, to all the years in order to squeeze the interval.","e32f0b21":"## Low tax cars:","18c4170e":"# Outlier Detection","b3d36e86":"**Thanks for reading my notebook ! Let me know if you have questions or if you want me to check out your works !!**","0b207958":"# How is fueltype related to price?","fb6d7b4d":"From these plots we can see that the numerical features present a so called 'long tail' distribution, with some outliers.","558df7e4":"Then, I will sort it by 'importance', creating a new sorted dataframe by importance","febe2cbf":"# Feature Importance by SHAP","588a6972":"I will reorder the columns to group the categorical variables first and then the numerical ones","8834674b":"We will analyze all the hybrid cars present in the dataframe.","f0711cd1":"# Mileage analysis","58bf073e":"## High Tax value","5b345d6e":"By looking at this plot, we can see more clearly the effect of each feature on the predicted price:\n- MPG: Cars with lower mpg tend to cost more, which makes sense: in fact, more expensive cars usually have a more powerful engine and consume more oil, with a consequent lower mpg.\n- Year: More recent cars tend to cost more and older cars tends to cost less.\n- engineSize: Cars with higher engine size tend to cost more, for similar reasons as why cars with low MPG tends to cost more.\n- mileage: Higher mileage tends to affect negatively the cars price.","809b7c2b":"We cannot see particular outliers that can potentially degrade the ML algorithm performance. We won't drop any more values.","e03d868b":"In particular, these values are not particularly different from others besides the low value of tax. We will keep them.","ac126977":"Finally, SHAP values will analyzed to get a deeper feature importance explanation.","244d0cd0":"## Categorical Encoding","973c76ff":"**Data analysis:**\n- Cars with Automatic and Semi-Auto transmission are more expensive, while cars with a manual transimission are cheaper. In particular, it is possible to say that while cars with manual transmission have a more similar price, cars with automatic or semi-auto transmission have a wider range of prices.\n- All the numeric features have a strong right skewed distribution\n- Pearson correlation analysis:\n    - There is a quite moderate negative correlation (-0.6) between the mpg and the price. This means that cars with high mpg tends to be cheaper.\n    - There is a quite moderate negative correlation (-0.54) between the mileage and the price. This means that cars with a higher mileage tend to be cheaper.\n    - There is a small positive correlation (+0.36) also between the tax and the price. Cars with higher tax tends to cost more.\n    - There is a moderate positive correlation (0.59) also between the enginesize and the price. Cars with higher enginesize tends to cost more.\n    - There is a moderate positive correlation (0.4) between the mpg and mileage: cars with high mpg tends to have a higher mileage\n    - There is a quite moderate negative correlation (-0.64) between tax and mpg. Cars with high mpg tends to have lower tax.\n- MPG and engineSize are not uniquely identified by the car model, year, transmission and fuelType.\n- Among cars cars with a mpg value higher than 100, the great majority are A3, Q5 or Q7. In particular they all have Automatic or semi automatic transmission and most of them have Hybrid fueltype.\n- Cars with low values of tax have small price (kinda gaussian with mean around 12000$)\n- By log transforming Salary, the distribution is still not normal but close enough. However, by log transforming 'price', the prediction performance of both XGBoost and LightGBM seems to be inferior. This is probably due to the fact that tree-based algorithms such as XGBoost andLightGBM do not get particular benefits from this transformation.","48e685f2":"## Multivariate Outlier Detection","7bfdf820":"Great, the mpg column looks very clean now. Moreover, the distribution of mpg looks somewhat normal, let's check how 'normal' it is !","78428f56":"The categorical features will be transformed to numeric values before training the algorithms.","90fcb481":"It looks like there are cars with enginesize=0 and few cars with enginesize over 5, we will investigate more!","5a4b4927":"Which are the cars we removed?","5397bb6c":"First, we need to understand if excluding 'mileage', which heavily depends how much a car has been used, the other features uniquely identify a car or not.","9bec093d":"First, we create a copy of the dataframe called 'encoded_df' which will host the numerical and categorical encoded features.","cd460465":"# Price Analysis","576c7dd9":"# Tax Analysis","52287eb9":"## Numerical features first analysis","565944d5":"From this plot it is possible to assert that cars with an Automatic or Semi-Automatic transmission are on average more expensive than cars with manual transmission. In particular, it is possible to say that while cars with manual transmission have a more similar price, cars with automatic or semi-auto transmission have a wider range of prices.","fd12f483":"## Prepare the data for learning","d87c75b1":"It looks like enginesize, year and mpg are the most important factors to predict the price. Together, they account for almost 70% of price explainability.","d9f62eee":"I will create dataframe to host the feature names and importance from XGBoost","b8ef6db2":"We can remove these 40 outliers !","c445bc02":"Ok, all of the R8 in this dataset have this big engine. We will keep this data.","ce4d6402":"# Feature Importance","f4897898":"**Prediction :**\n- **Both Light GBM and XGBoost achieve very good results in terms of RMSE (around 2200$) and R2 (over 96%).**\n- **By looking at the Feature importance plot, we can see that enginesize, year and mpg are the most important factors to predict the price. Together, they account for almost 70% of price explainability.**\n\nMoreover,the SHAP values summary plot shows more deeply the effect of each feature on the predicted price:\n- MPG: Cars with lower mpg tend to cost more, which makes sense: in fact, more expensive cars usually have a more powerful engine and consume more oil, with a consequent lower mpg.\n- Year: More recent cars tend to cost more and older cars tends to cost less.\n- engineSize: Cars with higher engine size tend to cost more, for similar reasons as why cars with low MPG tends to cost more.\n- mileage: Higher mileage tends to affect negatively the cars price.","59db021d":"From the heatmap, we can say the following about the price:<br>\n- There is a quite moderate negative correlation (-0.6) between the mpg and the price. This means that cars with high mpg tends to be cheaper.<br>\n- There is a quite moderate negative correlation (-0.54) between the mileage and the price. This means that cars with a higher mileage tend to be cheaper.<br>\n- There is a small positive correlation (+0.36) also between the tax and the price. Cars with higher tax tends to cost more.\n- Finally, there is a moderate positive correlation (0.59) also between the enginesize and the price. Cars with higher enginesize tends to cost more.\n\nOther interesting information not related to the price are:<br>\n- There is a moderate positive correlation (0.4) between the mpg and mileage: cars with high mpg tends to have a higher mileage<br>\n- There is a quite moderate negative correlation (-0.64) between tax and mpg. Cars with high mpg tends to have lower tax.\n\n\n\n","e1005eb7":"It looks like there are no missing values.","10201ed8":"It looks like these cars with low values of tax have small price (kinda gaussian with mean around 12000$)","068628e4":"We continue with the outlier removal from the mpg column.","9dd96e07":"# Used Audi cars deep analysis + price prediction with LightGBM and XGBOOST with OPTUNA hyperparameter optimization","d1438a0e":"Finally, I can plot the features' importance:","2cfaaf60":"These cars, with a very high mileage value, also have a very high value of tax and a very low value of price. We could understand this behaviour also from the correlation matrix.","29b1433d":"# Exploratory Data Analysis","cfc7c11c":"# Miles per Gallon (MPG) Analysis","84172420":"The distribution looks like a 'long tail'. Using a log transformation can we normalize it?","3afc471a":"![audi dashboard 2.jpg](attachment:92447375-955b-48b9-a964-cfb0d3e1f71b.jpg)","8bbac996":"Then, we transform 'year' column into integer type.","144c5948":"We can see that LightGBM and XGBoost performe very similarly in terms of RMSE and R2.","ba92348a":"In the case of Q3 it looks like these samples with enginesize=0 are outliers. For this reason we will consider them outliers also for the other models and drop them."}}