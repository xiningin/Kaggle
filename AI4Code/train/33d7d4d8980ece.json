{"cell_type":{"f053ae0d":"code","f72c997d":"code","72d018ee":"code","65030bb6":"code","f2b2291c":"code","f2ccc444":"code","e2789076":"code","736fe35b":"code","670ea1f8":"code","f813758d":"code","68b43663":"code","375d615d":"code","1e7c0844":"code","4db35ded":"code","f31f60cc":"code","e49f61c2":"code","0df2e827":"code","9a75962b":"code","14eb3ae0":"code","1766856b":"code","f0d3204b":"code","11482d4e":"code","2a18f549":"code","1c7faef7":"code","a00b7206":"code","24b92f17":"code","e267f8f2":"code","ad203b65":"code","105bc2bf":"code","3755238d":"code","0a8201d1":"code","003db88c":"markdown","9d4a99ab":"markdown","19ba2768":"markdown","bf9231b0":"markdown","c7902b87":"markdown","d92ca0f8":"markdown","159af2f8":"markdown","9b47bcde":"markdown","a158b114":"markdown","a15ee8e5":"markdown","40a69ff4":"markdown","a5e0b45b":"markdown","39fabc5c":"markdown","2c31c396":"markdown","ae30fcd3":"markdown","7c8e4861":"markdown","5dd830ce":"markdown","295c3528":"markdown","dc1221d8":"markdown","5981630e":"markdown","eff984be":"markdown"},"source":{"f053ae0d":"# Pandas : librairie de manipulation de donn\u00e9es\n# NumPy : librairie de calcul scientifique\n# MatPlotLib : librairie de visualisation et graphiques\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score,auc, accuracy_score\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import train_test_split\n\nfrom IPython.core.display import HTML # permet d'afficher du code html dans jupyter","f72c997d":"def scale_feat(df,cont_feat) :\n    df1=df\n    scaler = preprocessing.RobustScaler()\n    df1[cont_feat] = scaler.fit_transform(df1[cont_feat])\n    return df1","72d018ee":"from sklearn.model_selection import learning_curve\ndef plot_learning_curve(est, X_train, y_train) :\n    train_sizes, train_scores, test_scores = learning_curve(estimator=est, X=X_train, y=y_train, train_sizes=np.linspace(0.1, 1.0, 10),\n                                                        cv=5,\n                                                        n_jobs=-1)\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n    plt.figure(figsize=(8,10))\n    plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='training accuracy')\n    plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')\n    plt.plot(train_sizes, test_mean,color='green', linestyle='--',marker='s', markersize=5,label='validation accuracy')\n    plt.fill_between(train_sizes,test_mean + test_std,test_mean - test_std,alpha=0.15, color='green')\n    plt.grid(b='on')\n    plt.xlabel('Number of training samples')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='lower right')\n    plt.ylim([0.6, 1.0])\n    plt.show()","65030bb6":"def plot_roc_curve(est,X_test,y_test) :\n    probas = est.predict_proba(X_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,probas[:, 1])\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    plt.figure(figsize=(8,8))\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f'% roc_auc)\n    plt.legend(loc='lower right')\n    plt.plot([0,1],[0,1],'r--')        # plus mauvaise courbe\n    plt.plot([0,0,1],[0,1,1],'g:')     # meilleure courbe\n    plt.xlim([-0.05,1.2])\n    plt.ylim([-0.05,1.2])\n    plt.ylabel('Taux de vrais positifs')\n    plt.xlabel('Taux de faux positifs')\n    plt.show","f2b2291c":"df = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")","f2ccc444":"df.head().T","e2789076":"df.count()","736fe35b":"df.info()","670ea1f8":"df.describe()","f813758d":"df.isnull().values.sum()","68b43663":"df=scale_feat(df,list(set(df.columns)-{'Class'}))","375d615d":"df.describe()","1e7c0844":"for col in ['V1','V2','V3','V4','V5'] :\n    plt.figure(figsize=[10,5])\n    sns.kdeplot(df[col])","4db35ded":"from sklearn.model_selection import train_test_split\nX = df.drop(['Class'], axis=1)\ny = df.Class\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","f31f60cc":"from sklearn import tree\ndtc = tree.DecisionTreeClassifier()\ndtc.fit(X_train,y_train)\ny_dtc = dtc.predict(X_test)","e49f61c2":"print(classification_report(y_test, y_dtc))","0df2e827":"cm = confusion_matrix(y_test, y_dtc)\nprint(cm)","9a75962b":"df.Class.value_counts()","14eb3ae0":"from imblearn.under_sampling import RandomUnderSampler \n\n\nrus = RandomUnderSampler()\nX_train, y_train = rus.fit_resample(X_train, y_train)\n","1766856b":"y_train.value_counts()","f0d3204b":"dtc = tree.DecisionTreeClassifier()\ndtc.fit(X_train,y_train)\ny_dtc = dtc.predict(X_test)\n\nprint(classification_report(y_test, y_dtc))\n\ncm = confusion_matrix(y_test, y_dtc)\nprint(cm)","11482d4e":"X = df.drop(['Class'], axis=1)\ny = df.Class\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","2a18f549":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE()\nX_train, y_train = smote.fit_resample(X_train, y_train)","1c7faef7":"y_train.value_counts()","a00b7206":"dtc = tree.DecisionTreeClassifier()\ndtc.fit(X_train,y_train)\ny_dtc = dtc.predict(X_test)\n","24b92f17":"print(classification_report(y_test, y_dtc))\n","e267f8f2":"cm = confusion_matrix(y_test, y_dtc)\nprint(cm)","ad203b65":"X = df.drop(['Class'], axis=1)\ny = df.Class\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","105bc2bf":"df.Class.value_counts()","3755238d":"from xgboost import XGBClassifier\nxgb = XGBClassifier(scale_pos_weight=284315\/492)\nxgb.fit(X_train,y_train)\ny_xgb = xgb.predict(X_test)","0a8201d1":"print(classification_report(y_test, y_xgb))\ncm = confusion_matrix(y_test, y_xgb)\nprint(cm)","003db88c":"Avec cette methode on obtient des bons r\u00e9sultats ","9d4a99ab":"Notre dataset est vraiment d\u00e9s\u00e9quilibr\u00e9","19ba2768":"La methode de surechantillonnage presente des bons resultats mais pas encore suffisante en ce qui concernt la detection des fraudes ","bf9231b0":"On v\u00e9rifie s'il y a des valeurs ind\u00e9termin\u00e9es dans le dataset :","c7902b87":"Fonction pour tracer la courbe ROC :","d92ca0f8":"On se comptente d'utiliser la methode de decision Tree et non les forets aleatoires car notre dataset est colossale","159af2f8":"# Echantillonnage de donn\u00e9es : **Credit card fraud**","9b47bcde":"Fonction pour tracer les courbes d'apprentissage sur l'ensemble d'apprentissage et l'ensemble de validation :","a158b114":"Donc apparament on n'a pas besoin d'appliquer une transformation sur les distributions.\n","a15ee8e5":"Fonction pour standardiser les donn\u00e9es quantitatives","40a69ff4":"On a moins de donn\u00e9es d'apprentissage, mais les r\u00e9sultats sont plut\u00f4t meilleurs .\n","a5e0b45b":"On essayera de construire des nouvelles donn\u00e9es fictifs pour \u00e9quilibrer notre dataset ","39fabc5c":"## D\u00e9cision Tree\n","2c31c396":"## Traitement des donn\u00e9es ","ae30fcd3":"### XGBoost pond\u00e9r\u00e9\n","7c8e4861":"On a un probl\u00e8me avec la d\u00e9tection des fraudes du au d\u00e9s\u00e9quilibre du dataset","5dd830ce":"On obtient un dataset minime mais equilibr\u00e9 ","295c3528":"## Librairies et fonctions utiles","dc1221d8":"### Sous-\u00e9chantillonage","5981630e":"### Sur\u00e9chantillonnage","eff984be":"On \"sous-\u00e9chantillonne la classe majoritaire\""}}