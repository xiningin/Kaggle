{"cell_type":{"978a05c5":"code","36c8c431":"code","c446e8b7":"code","5e18c69b":"code","b8fb4952":"code","4955016f":"code","8c0cecd8":"code","6b0d15e4":"code","2ae04dd6":"code","aab4bc42":"code","1a18f2d2":"code","6befe2d0":"code","2e5e1670":"code","4b0e0256":"code","d71fe2ec":"code","b9cc903f":"code","ae832374":"code","9b8e1f52":"code","da39bbf5":"code","98a4ce16":"code","f2af87f1":"code","ce156c81":"code","8cdf1fbb":"code","95661154":"code","6b69e2c6":"code","af005776":"code","16afed3f":"code","6eabb7f9":"code","1cf6e5b3":"code","bc4a886d":"code","9da4e3de":"code","06cf3627":"code","9824301f":"code","532852a3":"code","db42dde2":"code","e14dd746":"code","3d8b5fe3":"code","22cad223":"code","f417df09":"code","35674065":"code","58f8bc71":"code","c109a3e3":"code","46d4bd84":"code","781426fc":"code","87c5190d":"code","646844db":"code","35ca9cd3":"code","95b77e27":"code","f56df611":"code","1f31a1a7":"code","8edf9893":"code","cd9f19c4":"code","1a6d18c4":"code","e2bfa139":"code","f6cf429b":"code","662943fb":"code","9a06ebc8":"code","a8f4971d":"code","aa0f7c37":"code","a5bf7118":"code","536e03e8":"code","b6c3ebf0":"code","5713e5cc":"code","733d321e":"code","76268c47":"code","54c2b835":"code","b775bf06":"code","3ad82081":"code","569a4299":"code","a91b0034":"markdown","2921cb47":"markdown","8e8dba33":"markdown","53a4114a":"markdown","04d3e7b3":"markdown","211e1c6e":"markdown","51e7bd37":"markdown","25fae4f5":"markdown","e843fbd5":"markdown","64eef4d0":"markdown","6835dc39":"markdown","1cd3e58a":"markdown","665b3e08":"markdown","270026a6":"markdown","edda3e13":"markdown","aa8278c7":"markdown","930ac320":"markdown","6deac4cc":"markdown","4a5c5b62":"markdown","6f3e4441":"markdown","a097acd7":"markdown","d00eaadd":"markdown","3d09cdfc":"markdown","581c0d0e":"markdown","45cad73b":"markdown","157a544a":"markdown","97293122":"markdown","4d821161":"markdown","bb56c624":"markdown","0875a466":"markdown","4ff661b7":"markdown","a37d4173":"markdown","7f2c8916":"markdown","b9acb832":"markdown","3f6c69fc":"markdown","983d1574":"markdown","603ca396":"markdown","1aebfe83":"markdown","846b473c":"markdown","b26d0e9f":"markdown","b3efc79b":"markdown","e27acdbe":"markdown","9bce3e2a":"markdown","cc3295f7":"markdown","8235a749":"markdown"},"source":{"978a05c5":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os","36c8c431":"print(os.listdir(\"..\/input\")) #to check the name of the directory inside which we have our files","c446e8b7":"from glob import glob #glob is used to extract the files from a particular folder\nfiles = glob('..\/input\/IDC_regular_ps50_idx5\/**\/*', recursive=True) ","5e18c69b":"print(files[0]) ","b8fb4952":"extention=list() #will store end 3 letters of all the file names (extentions)\nfor image in files:\n    ext=image[-3:]\n    if ext not in extention:\n        extention.append(ext)\nalpha_ext=list()\nfor ex in extention: #any valid image will have extention in alphabets \n    if ex.isalpha() == True: #this line checks for such alphabet extentions\n        alpha_ext.append(ex)\nprint(alpha_ext)","4955016f":"from glob import glob\nData = glob('..\/input\/IDC_regular_ps50_idx5\/**\/*.png', recursive=True)  #we extract only png files","8c0cecd8":"del(files) #We don't need the files variable , so delete it.\nprint(len(Data))","6b0d15e4":"\nfrom PIL import Image #adds support for opening, manipulating, and saving many different image file formats\nfrom tqdm import tqdm #adds progress bar for the loops\ndimentions=list()\nx=1\nfor images in (Data):\n    dim = Image.open(images)\n    size= dim.size\n    if size not in dimentions:\n        dimentions.append(size)\n        x+=1\n    if(x>3): #going through all the images will take up lot of memory, so therefore we will check until we get three different dimentions.\n        break\nprint(dimentions)\n","2ae04dd6":"import cv2 #used for computer vision tasks such as reading image from file, changing color channels etc\nimport matplotlib.pyplot as plt #for plotting various graph, images etc.\ndef view_images(image): #function to view an image\n    image_cv = cv2.imread(image) #reads an image\n    plt.imshow(cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB)); #displays an image\nview_images(Data[52])","aab4bc42":"def plot_images(photos) : #to plot multiple image\n    x=0\n    for image in photos:\n        image_cv = cv2.imread(image)\n        plt.subplot(5, 5, x+1)\n        plt.imshow(cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB));\n        plt.axis('off');\n        x+=1\nplot_images(Data[:25])","1a18f2d2":"def hist_plot(image): #to plot histogram of pixel values present in an image VS intensities\n    img = cv2.imread(image)\n    plt.subplot(2, 2,1)\n    view_images(image)\n    plt.subplot(2, 2,2)\n    plt.hist(img.ravel()) \n    plt.xlabel('Pixel Values')\n    plt.ylabel('Intensity')\nhist_plot(Data[29])\n    ","6befe2d0":"from tqdm import tqdm\nimport csv #to open and write csv files\nData_output=list()\nData_output.append([\"Classes\"])\nfor file_name in tqdm(Data):\n    Data_output.append([file_name[-10:-4]])\nwith open(\"output.csv\", \"w\") as f:\n    writer = csv.writer(f)\n    for val in Data_output:\n        writer.writerows([val])","2e5e1670":"from IPython.display import display # Allows the use of display() for DataFrames\ndata_output = pd.read_csv(\"output.csv\")\ndisplay(data_output.head(5))\nprint(data_output.shape)","4b0e0256":"def class_output(images,x,i):  #to display image along with their labels\n    fig = plt.figure()\n    ax = plt.subplot(2, 2,i)\n    ax.set_title(data_output.loc[x].item())\n    view_images(images)\n    i+=1\n    return\nk=0 #we have to show only one image of class0 therefore this variable is to check that\nl=0 #we have to show only one image of class1 therefore this variable to check that\ni=0 #for subplot position\nfor x in range(1,len(Data)):\n    if(data_output.loc[x].item()==\"class0\" and k!=1):\n        k+=1\n        i+=1\n        class_output(Data[x],x,i)\n    elif(data_output.loc[x].item()==\"class1\" and l!=1):\n        l+=1\n        i+=1\n        class_output(Data[x],x,i)\n    elif(k==0 or l==0):\n        continue\n    else:\n        break","d71fe2ec":"def vis_data(photos,a) :\n    x=0\n    beta=0\n    for image in photos:\n        image_cv = cv2.imread(image)\n        fig=plt.figure(figsize=(50,50))\n        ax=plt.subplot(2, 5, x+1)\n        view_images(images)\n        x+=1\n        beta+=1\nplot_images(Data[0:20])","b9cc903f":"class1 = data_output[(data_output[\"Classes\"]==\"class1\" )].shape[0]\nclass0 = data_output[(data_output[\"Classes\"]==\"class0\" )].shape[0]\nobjects=[\"class1\",\"class0\"]\ny_pos = np.arange(len(objects))\ncount=[class1,class0]\nplt.bar(y_pos, count, align='center', alpha=0.5)\nplt.xticks(y_pos, objects)\nplt.ylabel('Number of images')\nplt.title('Class distribution')\n \nplt.show()","ae832374":"percent_class1=class1\/len(Data)\npercent_class0=class0\/len(Data)\nprint(\"Total Class1 images :\",class1)\nprint(\"Total Class0 images :\",class0)\nprint(\"Percent of class 0 images : \", percent_class0*100)\nprint(\"Percent of class 1 images : \", percent_class1*100)","9b8e1f52":"from sklearn.utils import shuffle #to shuffle the data\nData,data_output= shuffle(Data,data_output)","da39bbf5":"from tqdm import tqdm\ndata=list()\nfor img in tqdm(Data):\n    image_ar = cv2.imread(img)\n    data.append(cv2.resize(image_ar,(50,50),interpolation=cv2.INTER_CUBIC))","98a4ce16":"data_output=data_output.replace(to_replace=\"class0\",value=0)\ndata_output=data_output.replace(to_replace=\"class1\",value=1)","f2af87f1":"from keras.utils import to_categorical #to hot encode the output labels\ndata_output_encoded =to_categorical(data_output, num_classes=2)\nprint(data_output_encoded.shape)","ce156c81":"from sklearn.model_selection import train_test_split\ndata=np.array(data)\nX_train, X_test, Y_train, Y_test = train_test_split(data, data_output_encoded, test_size=0.3)\nprint(\"Number of train files\",len(X_train))\nprint(\"Number of test files\",len(X_test))\nprint(\"Number of train_target files\",len(Y_train))\nprint(\"Number of  test_target  files\",len(Y_test))","8cdf1fbb":"X_train=X_train[0:70000]\nY_train=Y_train[0:70000]\nX_test=X_test[0:30000]\nY_test=Y_test[0:30000]","95661154":"from keras.utils import to_categorical #to hot encode the data\nfrom imblearn.under_sampling import RandomUnderSampler #For performing undersampling\n\nX_train_shape = X_train.shape[1]*X_train.shape[2]*X_train.shape[3]\nX_test_shape = X_test.shape[1]*X_test.shape[2]*X_test.shape[3]\nX_train_Flat = X_train.reshape(X_train.shape[0], X_train_shape)\nX_test_Flat = X_test.reshape(X_test.shape[0], X_test_shape)\n\nrandom_US = RandomUnderSampler(ratio='auto') #Constructor of the class to perform undersampling\nX_train_RUS, Y_train_RUS = random_US.fit_sample(X_train_Flat, Y_train) #resamples the dataset\nX_test_RUS, Y_test_RUS = random_US.fit_sample(X_test_Flat, Y_test) #resamples the dataset\ndel(X_train_Flat,X_test_Flat)\n\nclass1=1\nclass0=0\n\nfor i in range(0,len(Y_train_RUS)): \n    if(Y_train_RUS[i]==1):\n        class1+=1\nfor i in range(0,len(Y_train_RUS)): \n    if(Y_train_RUS[i]==0):\n        class0+=1\n#For Plotting the distribution of classes\nclasses=[\"class1\",\"class0\"]\ny_pos = np.arange(len(classes))\ncount=[class1,class0]\nplt.bar(y_pos, count, align='center', alpha=0.5)\nplt.xticks(y_pos, objects)\nplt.ylabel('Number of images')\nplt.title('Class distribution')\n \nplt.show()\n\n\n#hot encoding them\nY_train_encoded = to_categorical(Y_train_RUS, num_classes = 2)\nY_test_encoded = to_categorical(Y_test_RUS, num_classes = 2)\n\ndel(Y_train_RUS,Y_test_RUS)\n\nfor i in range(len(X_train_RUS)):\n    X_train_RUS_Reshaped = X_train_RUS.reshape(len(X_train_RUS),50,50,3)\ndel(X_train_RUS)\n\nfor i in range(len(X_test_RUS)):\n    X_test_RUS_Reshaped = X_test_RUS.reshape(len(X_test_RUS),50,50,3)\ndel(X_test_RUS)\n","6b69e2c6":"X_test, X_valid, Y_test, Y_valid = train_test_split(X_test_RUS_Reshaped, Y_test_encoded, test_size=0.2,shuffle=True)","af005776":"print(\"Number of train files\",len(X_train_RUS_Reshaped))\nprint(\"Number of valid files\",len(X_valid))\nprint(\"Number of train_target files\",len(Y_train_encoded))\nprint(\"Number of  valid_target  files\",len(Y_valid))\nprint(\"Number of test files\",len(X_test))\nprint(\"Number of  test_target  files\",len(Y_test))","16afed3f":"from sklearn.utils import shuffle\nX_train,Y_train= shuffle(X_train_RUS_Reshaped,Y_train_encoded)","6eabb7f9":"display(Y_train_encoded.shape)\ndisplay(Y_test.shape)\ndisplay(Y_valid.shape)","1cf6e5b3":"print(\"Training Data Shape:\", X_train.shape)\nprint(\"Validation Data Shape:\", X_valid.shape)\nprint(\"Testing Data Shape:\", X_test.shape)\nprint(\"Training Label Data Shape:\", Y_train.shape)\nprint(\"Validation Label Data Shape:\", Y_valid.shape)\nprint(\"Testing Label Data Shape:\", Y_test.shape)","bc4a886d":"import itertools #create iterators for effective looping\n#Plotting the confusion matrix for checking the accuracy of the model\ndef plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    print(cm)\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","9da4e3de":"from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D #Import layers for the model\nfrom keras.layers import Dropout, Flatten, Dense \nfrom keras.models import Sequential #Our model will be Sequential\n\nmodel = Sequential()\nmodel.add(Conv2D(filters=32,kernel_size=(3,3),strides=2,padding='same',activation='relu',input_shape=(50,50,3)))\nmodel.add(Flatten()) #Flattens the matrix into a vector\nmodel.add(Dense(2, activation='softmax')) \nmodel.summary()","06cf3627":"model.compile(optimizer= 'adam', loss='categorical_crossentropy', metrics=['accuracy']) #Compiling the model","9824301f":"from keras.callbacks import ModelCheckpoint  #Checkpoint to save the best weights of the model.\ncheckpointer = ModelCheckpoint(filepath='weights.best.cnn.hdf5', \n                               verbose=1, save_best_only=True) \nmodel.fit(X_train, Y_train, \n          validation_data=(X_valid, Y_valid),\n          epochs=3, batch_size=128, callbacks=[checkpointer], verbose=2,shuffle=True)","532852a3":"model.load_weights('weights.best.cnn.hdf5') #Load the saved weights from file.","db42dde2":"predictions = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in tqdm(X_test)]","e14dd746":"from sklearn.metrics import confusion_matrix #to plot confusion matrix\nclass_names=['IDC(-)','IDC(+)']\ncnf_matrix_bench=confusion_matrix(np.argmax(Y_test, axis=1), np.array(predictions))\nplot_confusion_matrix(cnf_matrix_bench, classes=class_names,\n                      title='Confusion matrix')","3d8b5fe3":"from keras.preprocessing.image import ImageDataGenerator  #For Image argumentaton\ndatagen = ImageDataGenerator(\n        shear_range=0.2,\n        rotation_range=40,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        zoom_range=0.2,\n        rescale=1\/255.0,\n        horizontal_flip=True,\n        vertical_flip=True)","22cad223":"X_valid_e=X_valid\/255.0 #rescaling X_valid\nX_test_e=X_test\/255.0 #rescaling X_Test","f417df09":"from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom keras.layers import Dropout, Flatten, Dense\nfrom keras.models import Sequential\n\nargum_model = Sequential()\nargum_model.add(Conv2D(filters=32,kernel_size=(3,3),strides=2,padding='same',activation='relu',input_shape=X_train.shape[1:]))\nargum_model.add(Dropout(0.15))\nargum_model.add(MaxPooling2D(pool_size=2,strides=2))\nargum_model.add(Conv2D(filters=64,kernel_size=(3,3),strides=2,padding='same',activation='relu'))\nargum_model.add(Dropout(0.25))\nargum_model.add(Conv2D(filters=128,kernel_size=(3,3),strides=2,padding='same',activation='relu'))\nargum_model.add(Dropout(0.35))\nargum_model.add(Conv2D(filters=512,kernel_size=(3,3),strides=2,padding='same',activation='relu'))\nargum_model.add(Dropout(0.45))\nargum_model.add(Flatten())\nargum_model.add(Dense(2, activation='softmax'))\nargum_model.summary()","35674065":"argum_model.compile(loss='categorical_crossentropy', optimizer='AdaDelta', metrics=['accuracy'])","58f8bc71":"from keras.callbacks import ModelCheckpoint\ncheckpointer = ModelCheckpoint(filepath='weights.bestarg.hdf5', verbose=1, save_best_only=True)","c109a3e3":"batch_size=32\nepochs=20\nargum_model.fit_generator(datagen.flow(X_train, Y_train, batch_size), \n          validation_data=(X_valid_e, Y_valid), steps_per_epoch=len(X_train) \/ batch_size,\n          epochs=epochs,callbacks=[checkpointer], verbose=0)","46d4bd84":"batch_size=64\nepochs=5\nargum_model.fit_generator(datagen.flow(X_train, Y_train, batch_size), \n          validation_data=(X_valid_e, Y_valid), steps_per_epoch=len(X_train) \/ batch_size,\n          epochs=epochs,callbacks=[checkpointer], verbose=0)","781426fc":"argum_model.load_weights('weights.bestarg.hdf5')","87c5190d":"predictions_arg = [np.argmax(argum_model.predict(np.expand_dims(feature, axis=0))) for feature in tqdm(X_test_e)]","646844db":"from sklearn.metrics import confusion_matrix\nclass_names=['IDC(-)','IDC(+)']\ncnf_matrix_Arg=confusion_matrix(np.argmax(Y_test, axis=1), np.array(predictions_arg))\nplot_confusion_matrix(cnf_matrix_Arg, classes=class_names,\n                      title='Confusion matrix with data argumentation')","35ca9cd3":"from keras.applications.vgg19 import VGG19 #downloading model for transfer learning\narg_model = VGG19(include_top=False,weights='imagenet',input_shape=(50,50,3))","95b77e27":"from keras.applications.vgg19 import preprocess_input #preprocessing the input so that it could work with the downloaded model\nbottleneck_train=arg_model.predict(preprocess_input(X_train),batch_size=50,verbose=1) #calculating bottleneck features, this inshure that we hold the weights of bottom layers","f56df611":"from keras.applications.vgg19 import preprocess_input\nbottleneck_valid=arg_model.predict(preprocess_input(X_valid),batch_size=50,verbose=1)","1f31a1a7":"from keras.applications.vgg19 import preprocess_input\nbottleneck_test=arg_model.predict(preprocess_input(X_test),batch_size=50,verbose=1)","8edf9893":"from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom keras.layers import Dropout, Flatten, Dense\nfrom keras.models import Sequential\n\nmodel_transfer = Sequential()\nmodel_transfer.add(GlobalAveragePooling2D(input_shape=bottleneck_train.shape[1:]))\nmodel_transfer.add(Dense(32,activation='relu'))\nmodel_transfer.add(Dropout(0.15))\nmodel_transfer.add(Dense(64,activation='relu'))\nmodel_transfer.add(Dropout(0.20))\nmodel_transfer.add(Dense(128,activation='relu'))\nmodel_transfer.add(Dropout(0.25))\nmodel_transfer.add(Dense(256,activation='relu'))\nmodel_transfer.add(Dropout(0.35))\nmodel_transfer.add(Dense(512,activation='relu'))\nmodel_transfer.add(Dropout(0.45))\n\nmodel_transfer.add(Dense(2, activation='softmax'))\n\nmodel_transfer.summary()","cd9f19c4":"model_transfer.compile(loss='categorical_crossentropy', optimizer='AdaDelta', metrics=['accuracy'])","1a6d18c4":"from keras.callbacks import ModelCheckpoint\ncheckpointer = ModelCheckpoint(filepath='weights.bestarg.tranfer.hdf5', verbose=1, save_best_only=True)","e2bfa139":"batch_size=32\nepochs=20\nmodel_transfer.fit(bottleneck_train, Y_train, batch_size,\n          validation_data=(bottleneck_valid, Y_valid),\n          epochs=epochs,callbacks=[checkpointer], verbose=1)","f6cf429b":"model_transfer.load_weights('weights.bestarg.tranfer.hdf5')","662943fb":"predictions_transfer = [np.argmax(model_transfer.predict(np.expand_dims(feature, axis=0))) for feature in bottleneck_test]","9a06ebc8":"from sklearn.metrics import confusion_matrix\nclass_names=['IDC(-)','IDC(+)']\ncnf_matrix_transfer=confusion_matrix(np.argmax(Y_test, axis=1), np.array(predictions_transfer))\nplot_confusion_matrix(cnf_matrix_transfer, classes=class_names,\n                      title='Confusion matrix')","a8f4971d":"#Bar chart to compare different models\ntp=0\nfor i in range(0,len(Y_test)): #Number of positive cases\n    if(np.argmax(Y_test[i])==1):\n        tp+=1\n#Senstivity of models\nconfusion_bench_s=cnf_matrix_bench[1][1]\/tp *100 \nconfusion_Arg_s=cnf_matrix_Arg[1][1]\/tp *100\nconfusion_transfer_s=cnf_matrix_transfer[1][1]\/tp *100\n\nclasses=[\"benchmark\",\"data argum\",\"transfer lerning\"]\nobjects=[\"benchmark\",\"data argum\",\"transfer lerning\"]\ny_pos = np.arange(len(classes))\ncount=[confusion_bench_s,confusion_Arg_s,confusion_transfer_s]\nplt.bar(y_pos, count, align='center', alpha=0.5)\nplt.xticks(y_pos, objects)\nplt.ylabel('Percentage')\nplt.title('Sensitivity')\n\nplt.show()\n","aa0f7c37":"tp=0\ntn=0\nfor i in range(0,len(Y_test)):  #Number of postive cases\n    if(np.argmax(Y_test[i])==1): \n        tp+=1\nfor i in range(0,len(Y_test)): #number of negative cases\n    if(np.argmax(Y_test[i])==0):\n        tn+=1\nconfusion_bench=cnf_matrix_bench[0][0]\/tn *100\nconfusion_Arg=cnf_matrix_Arg[0][0]\/tn *100\nconfusion_transfer=cnf_matrix_transfer[0][0]\/tn *100\nclasses=[\"benchmark\",\"data argum\",\"transfer lerning\"]\nobjects=[\"benchmark\",\"data argum\",\"transfer lerning\"]\ny_pos = np.arange(len(classes))\ncount=[confusion_bench,confusion_Arg,confusion_transfer]\nplt.bar(y_pos, count, align='center', alpha=0.5)\nplt.xticks(y_pos, objects)\nplt.ylabel('Percentage')\nplt.title('Specificity')\n\nplt.show()\n","a5bf7118":"col=['Models','Senstivity','Specificity']\nresults=pd.DataFrame(columns=col) #dataframe to store the results\nresults.loc[0]=['Bench',confusion_bench_s,confusion_bench]\nresults.loc[1]=['Image Arg model',confusion_Arg_s,confusion_Arg]\nresults.loc[2]=['Transfer Learning model',confusion_transfer_s,confusion_transfer]","536e03e8":"display(results)","b6c3ebf0":"X=np.array(data[0:100000]).astype('float32')\/255\nprint(X.shape[1:])","5713e5cc":"from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom keras.layers import Dropout, Flatten, Dense\nfrom keras.models import Sequential\ndef create_network(): #Function to create a network\n    argum_model = Sequential()\n    argum_model.add(Conv2D(filters=32,kernel_size=(3,3),strides=2,padding='same',activation='relu',input_shape=X.shape[1:]))\n    argum_model.add(Dropout(0.15))\n    argum_model.add(MaxPooling2D(pool_size=2,strides=2))\n    argum_model.add(Conv2D(filters=64,kernel_size=(3,3),strides=2,padding='same',activation='relu'))\n    argum_model.add(Dropout(0.25))\n    argum_model.add(Conv2D(filters=128,kernel_size=(3,3),strides=2,padding='same',activation='relu'))\n    argum_model.add(Dropout(0.35))\n    argum_model.add(Conv2D(filters=512,kernel_size=(3,3),strides=2,padding='same',activation='relu'))\n    argum_model.add(Dropout(0.45))\n    argum_model.add(Flatten())\n    argum_model.add(Dense(2, activation='softmax'))\n    argum_model.compile(loss='categorical_crossentropy', optimizer='AdaDelta', metrics=['accuracy'])\n    return argum_model","733d321e":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\ncnn = KerasClassifier(build_fn=create_network,  #Keras classifier to make it work with sklearn\n                                 epochs=10, \n                                 batch_size=100, \n                                 verbose=0)","76268c47":"\ncross_val_score(cnn, X, data_output_encoded[0:100000], cv=5) #K-Fold cross validation","54c2b835":"def predict(image):\n    predictions_im = [np.argmax(argum_model.predict(np.expand_dims(image,axis=0)))]\n    if(predictions_im==1):\n        print('IDC DETECTED ')\n    else:\n        print('IDC Negative')","b775bf06":"a=0\nb=0\nc=0\nd=0\nfor i in range(len(X_train)):\n    if(np.argmax(Y_train[i])==1 and predictions_arg[i]==1 and a==0):\n        a+=1\n        img = Image.fromarray(X_train[i])\n        ax=plt.subplot(2, 4, 1)\n        ax.set_title('True Positive')\n        plt.imshow(img)\n        \n    elif(np.argmax(Y_train[i])==1 and predictions_arg[i]==0 and b==0):\n        b+=1\n        img = Image.fromarray(X_train[i])\n        ax1=plt.subplot(2, 4, 2)\n        ax1.set_title('False Negative')\n        plt.imshow(img)\n    elif(np.argmax(Y_train[i])==0 and predictions_arg[i]==0 and c==0):\n        c+=1\n        img = Image.fromarray(X_train[i])\n        ax2=plt.subplot(2, 4, 3)\n        ax2.set_title('True Negative')\n        plt.imshow(img)\n    elif(np.argmax(Y_train[i])==0 and predictions_arg[i]==1 and d==0):\n        d+=1\n        img = Image.fromarray(X_train[i])\n        ax3=plt.subplot(2, 4, 4)\n        ax3.set_title('False Positive')\n        plt.imshow(img)\n    elif(a>0 and b>0 and c>0 and d>0):\n        break\n        ","3ad82081":"predict(X_train[820])\nimg = Image.fromarray(X_train[820])\nclass_a=['IDC+' if np.argmax(Y_train[820])==1 else 'IDC-']\nprint('Actual : ',class_a)\nplt.imshow(img)","569a4299":"predict(X_train[10])\nimg = Image.fromarray(X_train[10])\nclass_a=['IDC+' if np.argmax(Y_train[10])==1 else 'IDC-']\nprint('Actual : ',class_a)\nplt.imshow(img)","a91b0034":"> **Robustness of the Model**","2921cb47":"> ***Now we will plot the confusion matrix :***","8e8dba33":"Next Step is that we will check whether the dimentions of all the images are same or different","53a4114a":"Below code reads the data from output.csv and displays it","04d3e7b3":"In the next step we will OneHot encode our data to better work with neural networks.","211e1c6e":"We also need a validation set inorder to check overfitting. We can do two things either split test set further into valid set or split train se into valid set.","51e7bd37":"> **Code Conclusion **: We have total of 277524 image files","25fae4f5":"We will first shuffle are images to remove any patterns if present and then load them.","e843fbd5":"**Breast Cancer Detection**\n![](https:\/\/blogs.nvidia.com\/wp-content\/uploads\/2018\/01\/AI_Mammographie.jpg)","64eef4d0":"Now we will split our data into training set and testing set.","6835dc39":"> **Code Conclusion :**  There are only png extentions which are present in alphabets therefore it means that we have only one image extention files with *.png* extentions. Therefore we will load only that.","1cd3e58a":"We would encode our output data which is present as Class1 and Class0 to 1 and 0.","665b3e08":"***Domain Background*** : \n\tBreast Cancer is the most common type of cancer in woman worldwide accounting for 20% of all cases.\n    \n>     In 2012 it resulted in 1.68 million new cases and 522,000 deaths.\n    \nOne of the major problems is that women often neglect the symptoms, which could cause more adverse effects on them thus lowering the survival chances. In developed countries, the survival rate is although high, but it is an area of concern in the developing countries where the 5-year survival rates are poor. In India, there are about one million cases every year and the five-year survival of stage IV breast cancer is about 10%. Therefore it is very important to detect the signs as early as possible. \n    \n>     Invasive ductal carcinoma (IDC) is the most common form of breast cancer.\n   \n   About 80% of all breast cancers are invasive ductal carcinomas. Doctors often do the biopsy or a scan if they detect signs of IDC. The cost of testing for breast cancer sets one back with $5000, which is a very big amount for poor families and also manual identification of presence and extent of breast cancer by a pathologist is critical. Therefore automation of detection of breast cancer using Histopathology images could reduce cost and time as well as improve the accuracy of the test. This is an active research field lot of research papers and articles are present online one that I like is -(https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC5453426\/) as they used deep learning approach to study on histology images and achieved the sensitivity of 95 which is greater than many pathologists (~90). This shows the power of automation and how it could help in the detection of breast cancer.\n\n","270026a6":"> ***Code Conclusion : *** We can see that the dimentions of images are not equal therefore we would make it all equal  to work bettter with our network.","edda3e13":"**Local Directory**","aa8278c7":"\n***Image Argumentation***","930ac320":"**Transfer Learning**","6deac4cc":"***Data Extraction and Visualization***","4a5c5b62":"We will go for spliting testing set into validation set.","6f3e4441":"> ***BENCHMARK MODEL: *** A simple CNN model","a097acd7":"> ***Code Conclusion :*** We can see that we have an unbalanced class and which is a common problem when we have medical data, therefore this is one another problem that we have to deal with later.","d00eaadd":"**Data Exploration**","3d09cdfc":"> Result : We can see that tranfer learning and image argumentation both are doing a great job, they both have senstivity and specificity o about 0.8.","581c0d0e":">** Code Conclusion :** 5-Fold Cross Validations is performed and as we can see that the difference in validation score is near about 0.01 , which is very small therefore we can say that our model is robust.","45cad73b":"To check the robustness of the model we would perform K Fold CV on the dataset and if the validation socre on each fold does nt change much we can say that the model i robust","157a544a":"\nIn data exploration we will first check the name of the files.","97293122":"**IMPORT FILES**","4d821161":"We have a large dataset and we will work with neural networks, therefore for better debugging we will use only a part of data, considering limited RAM and non GPU processor, this will not cost us much as we would also be using under sampling methods and image argumentation to deal with class imbalances and moderate data.","bb56c624":"We explore the name of the directory inside which our datafiles are present.","0875a466":"We will now do undersampling, to treat our data for class imbalances. The Code inspiration for undersampling is taken from a notebook - https:\/\/www.kaggle.com\/paultimothymooney\/predict-idc-in-breast-cancer-histology-images","4ff661b7":"We will now add image argumentation to our data, so that it may be set for wider range of domain","a37d4173":"We will also rescale our image pixels, from range of 0-255.0 to 0-1.","7f2c8916":"Now lets look at the color ranges that our images have","b9acb832":"> ***Now we will plot the confusion matrix :***","3f6c69fc":"Now we have our three sets of train, valid and test. We will now create our benchmark model.","983d1574":"Comparision is made between three algorithms with repect to false positives and false negatives. As considering dealing with cancer , the algorithm must give less false negative, at its a matter of death and life.","603ca396":"> ***Code Conclusion :*** We can see that images are very small, though they are cropped images, its hard for human eye to understand them without using some high costly machines. ","1aebfe83":"Next step is we need to extract the class names in which each files belong from its file names. We will save it in output.csv file.","846b473c":"> *Class1* represents** IDC(+)** and* Class0* represents** IDC(-)**","b26d0e9f":"> ***Data Processing  *** ","b3efc79b":"> ***Now we will plot the confusion matrix :***","e27acdbe":"We will now add transfer learning from various models","9bce3e2a":"> We need to now preprocess our image file. We change pixels range from 0-255 to 0-1.","cc3295f7":"Algorithm for passing a new image and predicting whether it has breast cancer or not.","8235a749":"> ***Code Conclusion :*** From the above image we can conclude that brighter region is more than the darken region in our image.  "}}