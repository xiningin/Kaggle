{"cell_type":{"c2d2105c":"code","d22f25d0":"code","46a8b40c":"code","d45d295c":"code","ebc4fcad":"code","6ee2a1e3":"code","ee8c7aaf":"code","8d9ca6e0":"code","a7d74ae5":"code","e313c0b1":"code","d0ffa570":"markdown","b11bddae":"markdown"},"source":{"c2d2105c":"import os\nimport random\n\nimport numpy as np\nfrom keras.utils import np_utils\n\nimport tensorflow as tf\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers import Dropout, BatchNormalization, LeakyReLU\nfrom tensorflow.keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt","d22f25d0":"# settings to get reproducible results, still the results are not entirely reproducible.\nSEED = 42\nos.environ['PYTHONHASHSEED']=str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)","46a8b40c":"def preprocess_data(X, normalize=True):\n    X = X.astype('float32')\n\n    if normalize:\n        X \/= 255.\n\n    return X.reshape(*X.shape, 1)","d45d295c":"(X_train, y_train), (X_valid, y_valid) = mnist.load_data()\n\nX_train = preprocess_data(X_train)\nX_valid = preprocess_data(X_valid)\n\ny_train = np_utils.to_categorical(y_train)\ny_valid = np_utils.to_categorical(y_valid)\n\nimg_width = X_train.shape[1]\nimg_height = X_train.shape[2]\nimg_depth = X_train.shape[3]\nnum_classes = y_train.shape[1]","ebc4fcad":"def build_cnn_8k(optim='adam', loss='categorical_crossentropy'):\n    '''\n    Total Trainable params: 7,968 (< 8k)\n    '''\n    net = Sequential(name='cnn_8k')\n\n    net.add(\n        Conv2D(\n            filters=32,\n            kernel_size=(3,3),\n            input_shape=(img_width, img_height, img_depth),\n            name='conv2d_1'\n        )\n    )\n    net.add(LeakyReLU(name='leaky_relu_1'))\n    net.add(BatchNormalization(name='batchnorm_1'))\n    net.add(MaxPooling2D(pool_size=(2,2), name='max_pool_1'))\n\n    net.add(\n        Conv2D(\n            filters=14,\n            kernel_size=(3,3),\n            name='conv2d_2'\n        )\n    )\n    net.add(LeakyReLU(name='leaky_relu_2'))\n    net.add(BatchNormalization(name='batchnorm_2'))\n    net.add(MaxPooling2D(pool_size=(2,2), name='max_pool_2'))\n\n    net.add(Flatten(name='flatten_layer'))\n    net.add(Dropout(0.2, name='dropout_1'))\n    net.add(Dense(num_classes, activation='softmax', name='dense_out'))\n    \n    net.compile(\n        loss=loss,\n        optimizer=optim,\n        metrics=['accuracy']\n    )\n    \n    net.summary()\n    return net","6ee2a1e3":"early_stopping = EarlyStopping(\n    monitor='val_accuracy',\n    mode='max',\n    min_delta=0.00005,\n    baseline=0.98,\n    patience=10,\n    restore_best_weights=True,\n    verbose=1\n)\n\nlr_scheduler = ReduceLROnPlateau(\n    monitor='val_accuracy',\n    patience=4,\n    factor=0.5,\n    min_lr=1e-6,\n    verbose=1\n)\n\ncallbacks = [\n    early_stopping,\n    lr_scheduler,\n]","ee8c7aaf":"loss = 'categorical_crossentropy'\noptim = optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name='Nadam')\nbatch_size = 128\nepochs = 40\n\nmodel = build_cnn_8k(optim, loss)\nhistory = model.fit(\n    x=X_train,\n    y=y_train,\n    validation_data=(X_valid, y_valid),\n    batch_size=batch_size,\n    epochs=epochs,\n    callbacks=callbacks,\n    use_multiprocessing=True\n)","8d9ca6e0":"sns.set()","a7d74ae5":"fig = plt.figure(0, (12, 4))\n\nax = plt.subplot(1, 2, 1)\nsns.lineplot(history.epoch, history.history['accuracy'], label='train')\nsns.lineplot(history.epoch, history.history['val_accuracy'], label='valid')\nplt.title('Accuracy')\nplt.tight_layout()\n\nax = plt.subplot(1, 2, 2)\nsns.lineplot(history.epoch, history.history['loss'], label='train')\nsns.lineplot(history.epoch, history.history['val_loss'], label='valid')\nplt.title('Loss')\nplt.tight_layout()\n\nplt.show()","e313c0b1":"import pandas as pd\ndf_accu = pd.DataFrame({'train': history.history['accuracy'], 'valid': history.history['val_accuracy']})\ndf_loss = pd.DataFrame({'train': history.history['loss'], 'valid': history.history['val_loss']})\n\nfig = plt.figure(0, (14, 4))\nax = plt.subplot(1, 2, 1)\nsns.violinplot(x=\"variable\", y=\"value\", data=pd.melt(df_accu), showfliers=False)\nplt.title('Accuracy')\nplt.tight_layout()\n\nax = plt.subplot(1, 2, 2)\nsns.violinplot(x=\"variable\", y=\"value\", data=pd.melt(df_loss), showfliers=False)\nplt.title('Loss')\nplt.tight_layout()\n\nplt.show()","d0ffa570":"## **MNIST Digit Recognition**\n\nDigit-MNIST is the most fundamental and easiest dataset possible for deep learning purposes. And we have seen [here](https:\/\/www.kaggle.com\/gauravsharma99\/getting-started-with-digit-mnist-using-keras) that without much efforts we achieved almost 99.5% accuracy on this dataset (which is not even the standard 60k MNIST). Also with this same model and configuration we may have achieved even higher accuracy with the 60k MNIST.\n\nAlthough we have achieved pretty good accuracy in such an easy dataset but for such we an easy dataset our model has parameters in the range of 100k-200k, which is way more for such an easy task. So can we do better i.e., reducing the model size significantly and simultaneously retaining the accuracy in the same range.\n\nRecently I was given a task by some organization to achieve an accuracy of atleast 99.4% on the 60k MNIST. You might think what's good about that, even handicapped models can achieve an accuracy of around 98-99% in MNIST without doing anything. But the constraint was to achieve such an high accuracy using a model having **atmost 8k parameters**. Although I wasn't able to touch the 99.4% bar but I got an best accuracy of around 99.2% having less than 8k parameters in the given time limit. And I was sure if the model have given enough time then 99.4% accuracy is achievable afterall no one design a task which is un-achievable. The organization may already achieved that and that's why gave us such a task.\n\nBut the idea here is that before jumping to deep networks we first try to achieve the goal in minimal model possible, just like we say in ML i.e., never jump to complex models but first try simple models like linear models. So using such low number of parameters we achieved pretty good accuracy. The benefits are many like easy to inspect, debug, load, train etc. Because the number of layers & parameters are now very less so we can inspect and debug(if needed) our network very easily.\n\nI thought it's worth sharing the notebook, it's very simple and I didn't do much.\n\n\n#### Model Architecture\nI used **only convolutional layers with no dense layer** due to the constraint on number of trainable parameters. Due to this the network has only **7968 trainable parameters**. Also **MaxPooling** helped a lot in reducing the trainable parameters.\n\n#### Model Generalization\nFor model generalization I used **BatchNormalization** and **Dropout** in the architecture. While during the training of network I used **ReduceLROnPlateau** and **EarlyStopping**.<br><br>\n\nI used various different settings like cropping 28x28 image down to 20x20 and using ImageDataGenerator but didn't get any significant improvements for this small network and hence finally retained to these configurations.\nUnder all these configurations I achieved the best **validation accuracy of almost 99.2%** for the given\nneural network. The training accuracy is also in the range of 99%.<br><br>","b11bddae":"The outliers in the plots are of `initial epochs`."}}