{"cell_type":{"4609e242":"code","37b3f8b4":"code","ef6e1dc5":"code","69d2de3c":"code","6b51fca4":"code","eb5c901f":"code","71475980":"code","add7f4a0":"code","8e33482c":"code","42fb6aa9":"code","ba0fc578":"code","d881c0bc":"code","c3dc9c4a":"code","aec3ed8d":"code","5d02fb69":"code","a20abc5d":"code","75f79a73":"code","b6db1b0d":"code","bddc8746":"code","12cde73e":"code","5c8faa74":"code","f1452f62":"code","7b05dfc5":"code","cf612d4f":"code","7221b547":"code","95a9b327":"code","97e86c90":"code","1837f738":"code","67bbd80e":"code","60d25545":"code","b6b2b1fd":"code","aeedda5f":"code","5e630c8f":"code","5ab3f3c5":"code","a017c07a":"code","f996debf":"code","aa58f667":"code","c6bdf9c5":"code","405d30a6":"code","65ec5c00":"code","56b6cf2c":"code","6067e6ce":"code","a23a845e":"code","7aaa45e6":"code","7f3ed353":"code","7b576562":"markdown","e7517ac7":"markdown","ba3d0240":"markdown","3dded7a7":"markdown","b6f7ab9f":"markdown","a5c76b0e":"markdown","d197481b":"markdown"},"source":{"4609e242":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","37b3f8b4":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\nprint(\"Number of rows and columns in train set : \",train_df.shape)\nprint(\"Number of rows and columns in test set : \",test_df.shape)","ef6e1dc5":"train_df.head()","69d2de3c":"train_df.target.value_counts(dropna=False, normalize=True).plot(kind=\"bar\")","6b51fca4":"corr_df=train_df.iloc[:,1:].corr()","eb5c901f":"import seaborn as sns\nsns.set(style=\"white\")\nmask = np.zeros_like(corr_df.iloc[:,1:], dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr_df.iloc[:,1:], mask=mask, cmap=cmap, vmax=.2, center=0,\n            square=True, linewidths=.5)","71475980":"corr_df.loc[corr_df.target>0.05].index[1:]","add7f4a0":"corr_target=corr_df.loc[corr_df.target>0.05]['target'].iloc[1:]\ncorr_target.plot(kind='bar')","8e33482c":"train_df.iloc[:,2:].describe()","42fb6aa9":"train_df_out_rem=train_df.iloc[:,1:]","ba0fc578":"train_df_out_rem.shape","d881c0bc":"train_df_out_rem=train_df_out_rem.reset_index(drop=True)","c3dc9c4a":"from sklearn.ensemble import RandomForestClassifier","aec3ed8d":"clf = RandomForestClassifier(n_estimators=100, max_depth=8,random_state=0)","5d02fb69":"clf.fit(train_df_out_rem.iloc[:,1:], train_df_out_rem.iloc[:,0])","a20abc5d":"importance_dict=dict(zip(list(train_df_out_rem.columns[1:]),list(clf.feature_importances_)))","75f79a73":"importance_dict_imp={k: v for k, v in importance_dict.items() if v >0.01}","b6db1b0d":"features=list(importance_dict_imp.keys())\nindices = np.argsort(list(importance_dict_imp.values()))\nfeatures_sorted=list(importance_dict_imp.values())\nfeatures_sorted_upd=[features_sorted[i] for i in indices]\nfeatures_names_upd=[features[i] for i in indices]\nplt.figure(figsize = (6,12))\nplt.title('Feature Importances')\nplt.barh(features_names_upd, features_sorted_upd, color='b', align='center')","bddc8746":"from sklearn.preprocessing import PolynomialFeatures","12cde73e":"poly = PolynomialFeatures(interaction_only=True)","5c8faa74":"poly_features=poly.fit_transform(train_df_out_rem[features_names_upd])","f1452f62":"poly_features_df=pd.DataFrame(poly_features)\npoly_features_df.columns=['Imp_feature_'+str(i) for i in range(poly_features.shape[1])]\npoly_features_df.head()","7b05dfc5":"other_cols=train_df_out_rem.loc[:, ~train_df_out_rem.columns.isin(features_names_upd)]","cf612d4f":"other_cols.shape,poly_features_df.shape","7221b547":"train_final_df=pd.concat([poly_features_df,other_cols],axis=1)","95a9b327":"train_final_df.target.value_counts(dropna=False)","97e86c90":"train_x=train_final_df.loc[:, train_final_df.columns != 'target']\ntrain_y=train_final_df.target","1837f738":"train_x.shape,train_y.shape","67bbd80e":"poly_features_test=poly.transform(test_df[features_names_upd])","60d25545":"poly_features_test.shape","b6b2b1fd":"poly_features_test_df=pd.DataFrame(poly_features_test)\npoly_features_test_df.columns=['Imp_feature_'+str(i) for i in range(poly_features_test.shape[1])]\npoly_features_test_df.head()","aeedda5f":"other_cols_test=test_df.loc[:, ~test_df.columns.isin(features_names_upd)]","5e630c8f":"id_code=other_cols_test.ID_code","5ab3f3c5":"other_cols_test=other_cols_test.drop(columns=['ID_code'])","a017c07a":"final_test_df=pd.concat([poly_features_test_df,other_cols_test],axis=1)","f996debf":"final_test_df.head()","aa58f667":"# reduce memory\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df","c6bdf9c5":"train_x=reduce_mem_usage(train_x)\nfinal_test_df=reduce_mem_usage(final_test_df)","405d30a6":"import gc\ngc.collect()\ndel train_df,test_df,train_df_out_rem","65ec5c00":"from sklearn.metrics import roc_auc_score \nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport lightgbm as lgb","56b6cf2c":"def kfold_lightgbm(train_df,target,test_df, num_folds, stratified = True, debug= False):\n    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n\n    # Cross validation model\n    if stratified:\n        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=326)\n    else:\n        folds = KFold(n_splits= num_folds, shuffle=True, random_state=326)\n\n    # Create arrays and dataframes to store results\n    oof_preds = np.zeros(train_df.shape[0])\n    sub_preds = np.zeros(test_df.shape[0])\n    feature_importance_df = pd.DataFrame()\n\n    # k-fold\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df,target)):\n        train_x, train_y = train_df.iloc[train_idx], target[train_idx]\n        valid_x, valid_y = train_df.iloc[valid_idx], target[valid_idx]\n\n        # set data structure\n        lgb_train = lgb.Dataset(train_x,\n                                label=train_y,\n                                free_raw_data=False)\n        lgb_test = lgb.Dataset(valid_x,\n                               label=valid_y,\n                               free_raw_data=False)\n\n        # params optimized by optuna\n        params  = {\n        'num_leaves': 20,\n        'max_bin': 60,\n        'min_data_in_leaf': 17,\n        'learning_rate': 0.07,\n        'min_sum_hessian_in_leaf': 0.000446,\n        'bagging_fraction': 1.0, \n        'bagging_freq': 5, \n        'lambda_l1': 4,\n        'lambda_l2': 1,\n        'min_gain_to_split': 0.15,\n        'max_depth': 18,\n        'save_binary': True,\n        'objective': 'binary',\n        'boosting_type': 'gbdt',\n        'verbose': 1,\n        'metric': 'auc',\n        'is_unbalance': True,\n    }\n\n        reg = lgb.train(\n                        params,\n                        lgb_train,\n                        valid_sets=[lgb_test],\n                        valid_names=[ 'test'],\n                        num_boost_round=10000,\n                        early_stopping_rounds= 200,\n                        verbose_eval=100\n                        )\n\n        oof_preds[valid_idx] = reg.predict(valid_x, num_iteration=reg.best_iteration)\n        sub_preds += reg.predict(test_df, num_iteration=reg.best_iteration) \/ folds.n_splits\n\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = list(test_df.columns)\n        fold_importance_df[\"importance\"] = np.log1p(reg.feature_importance(importance_type='gain', iteration=reg.best_iteration))\n        fold_importance_df[\"fold\"] = n_fold + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n        del reg, train_x, train_y, valid_x, valid_y\n        gc.collect()\n    return sub_preds,oof_preds","6067e6ce":"test_preds,train_preds=kfold_lightgbm(train_x,train_y.values,final_test_df, 10)","a23a845e":"roc_auc_score(train_y.values,train_preds)","7aaa45e6":"subm=pd.read_csv('..\/input\/sample_submission.csv')","7f3ed353":"subm.target=test_preds\nsubm.to_csv('submission.csv',index=False)","7b576562":"### Feature Engineering","e7517ac7":"### Kernel Overview and Key Take aways:\n\n* Target variable is Imbalanced\n\n* The independent features are not correated to each other\n\n* Created baseline RF model and have taken top features \n\n* Added polynomial features on important variables acquired from RF model as part of Feature Engineering\n\n* Applied 10 fold CV of LGBM \n\n**Please upvote if you find this kernel helpful.**\n\n\n    ","ba3d0240":"### 10 Fold CV LGB","3dded7a7":"### The above columns are relatively high in correlation with target variable","b6f7ab9f":"### Test data preparation","a5c76b0e":"#### An Imbalanced Class problem ","d197481b":"### No correlation between independent variables."}}