{"cell_type":{"9926a808":"code","6c8cfb66":"code","c7b4d9e0":"code","56866b59":"code","bbe08cfe":"code","8cea278f":"code","90ec6947":"code","4cb5e2e9":"code","bfdad4a6":"code","776e5025":"code","e46ba0aa":"code","609c9bc9":"code","3e6631b7":"code","90a71443":"code","dbc67de5":"code","5458ddc3":"code","a887f9b6":"code","bfd47e31":"code","083c0470":"code","7ad7d12a":"code","389e28f4":"code","1d47b569":"code","55f721a1":"code","27d1f1fc":"code","1136c4d0":"code","1b0963c0":"code","4d2741b7":"code","1847186a":"markdown"},"source":{"9926a808":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6c8cfb66":"df = pd.read_csv(\"..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")\ndf.head()","c7b4d9e0":"df.groupby(\"quality\")[\"quality\"].count()","56866b59":"df.shape","bbe08cfe":"#focus only on wine with quality of 5, 6, 7\ndf_sel = df[(df['quality']==5) | (df['quality']==6) | (df['quality']==7)]\ndf_sel.shape","8cea278f":"df_sel.isnull().any() #\u0e40\u0e0a\u0e47\u0e04 missing data","90ec6947":"df_sel.isnull().sum(axis=0)","4cb5e2e9":"#O as Original\nXo = df_sel.drop(['quality'],axis=1).values\nyo = df_sel['quality'].values","bfdad4a6":"Xo.shape","776e5025":"yo.shape","e46ba0aa":"np.unique(yo)","609c9bc9":"yo[yo==5] = 0\nyo[yo==6] = 1\nyo[yo==7] = 2\nnp.unique(yo)","3e6631b7":"[sum(yo == item) for item in np.unique(yo)]","90a71443":"from sklearn.model_selection import train_test_split\nX_train, X_test, y, yt = train_test_split(Xo, yo, test_size=0.2, random_state=0)","dbc67de5":"from sklearn.model_selection import train_test_split\nX_train, X_test, y, yt = train_test_split(Xo, yo, test_size=0.2, random_state=0)","5458ddc3":"from sklearn.model_selection import train_test_split\nX_train, X_val, y, yv = train_test_split(X_train, y, test_size=0.2, random_state=0)","a887f9b6":"from sklearn.preprocessing import MinMaxScaler\nscl = MinMaxScaler()\nscl.fit(X_train)\nX_train_norm = scl.transform(X_train)\nX_val_norm = scl.transform(X_val)\nX_test_norm = scl.transform(X_test)","bfd47e31":"num_label = len(np.unique(y)) \ny_multi = (np.arange(num_label) == y[:,None]).astype(np.float32)\nyv_multi = (np.arange(num_label) == yv[:,None]).astype(np.float32)\nyt_multi = (np.arange(num_label) == yt[:,None]).astype(np.float32)","083c0470":"sample_size, input_size = X_train_norm.shape","7ad7d12a":"import tensorflow as tf\ndef create_model(input_size, num_label):\n    tf.random.set_seed(0)\n    tf.compat.v1.reset_default_graph()\n    model = tf.keras.models.Sequential([\n      tf.keras.layers.Dense(16, activation='relu', input_shape=(input_size,)),\n      tf.keras.layers.Dense(8, activation='relu'),\n      tf.keras.layers.Dense(2, activation='relu'),\n      tf.keras.layers.Dropout(0.2),\n      tf.keras.layers.Dense(num_label, activation='softmax')    \n    ])\n    return model","389e28f4":"model = create_model(input_size, num_label)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\ntrain_acc = list()\nval_acc = list()\nfor i in range(0,250):\n    history = model.fit(X_train_norm, y_multi, epochs= 2, batch_size = int(sample_size\/4), validation_data= (X_val_norm, yv_multi))\n    tmp_avg = np.mean(history.history['loss'])\n    tmp_avg_val = np.mean(history.history['val_loss'])\n    train_acc.append(tmp_avg)\n    val_acc.append(tmp_avg_val)","1d47b569":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nplt.figure(num=None, figsize=(16, 8), dpi=90, facecolor='w', edgecolor='k')\nplt.plot()\nplt.plot(train_acc)\nplt.plot(val_acc)\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['Train', 'Val'], loc='upper left')\nplt.show()","55f721a1":"yp = model.predict(X_train_norm)\nyp = np.argmax(yp, 1)\nsum(yp == y)\/len(y)","27d1f1fc":"yp = model.predict(X_test_norm)\nyp = np.argmax(yp, 1)\nsum(yp == yt)\/len(yt)","1136c4d0":"#confusion_matrix\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ndef make_confusion_matrix(cf,\n                          group_names=None,\n                          categories='auto',\n                          count=True,\n                          percent=True,\n                          cbar=True,\n                          xyticks=True,\n                          xyplotlabels=True,\n                          sum_stats=True,\n                          figsize=None,\n                          cmap='Blues',\n                          title=None):\n    '''\n    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n    Arguments\n    ---------\n    cf:            confusion matrix to be passed in\n    group_names:   List of strings that represent the labels row by row to be shown in each square.\n    categories:    List of strings containing the categories to be displayed on the x,y axis. Default is 'auto'\n    count:         If True, show the raw number in the confusion matrix. Default is True.\n    normalize:     If True, show the proportions for each category. Default is True.\n    cbar:          If True, show the color bar. The cbar values are based off the values in the confusion matrix.\n                   Default is True.\n    xyticks:       If True, show x and y ticks. Default is True.\n    xyplotlabels:  If True, show 'True Label' and 'Predicted Label' on the figure. Default is True.\n    sum_stats:     If True, display summary statistics below the figure. Default is True.\n    figsize:       Tuple representing the figure size. Default will be the matplotlib rcParams value.\n    cmap:          Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues'\n                   See http:\/\/matplotlib.org\/examples\/color\/colormaps_reference.html\n                   \n    title:         Title for the heatmap. Default is None.\n    '''\n\n\n    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n    blanks = ['' for i in range(cf.size)]\n\n    if group_names and len(group_names)==cf.size:\n        group_labels = [\"{}\\n\".format(value) for value in group_names]\n    else:\n        group_labels = blanks\n\n    if count:\n        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n    else:\n        group_counts = blanks\n\n    if percent:\n        group_percentages = [\"{0:.2%}\".format(value) for value in (cf\/cf.astype(np.float).sum(axis=0)).flatten()]\n    else:\n        group_percentages = blanks\n\n    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n\n\n    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n    if sum_stats:\n        #Accuracy is sum of diagonal divided by total observations\n        accuracy  = np.trace(cf) \/ float(np.sum(cf))\n\n        #if it is a binary confusion matrix, show some more stats\n        if len(cf)==2:\n            #Metrics for Binary Confusion Matrices\n            precision = cf[1,1] \/ sum(cf[:,1])\n            recall    = cf[1,1] \/ sum(cf[1,:])\n            f1_score  = 2*precision*recall \/ (precision + recall)\n            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n                accuracy,precision,recall,f1_score)\n        else:\n            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n    else:\n        stats_text = \"\"\n\n\n    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n    if figsize==None:\n        #Get default figure size if not set\n        figsize = plt.rcParams.get('figure.figsize')\n\n    if xyticks==False:\n        #Do not show categories if xyticks is False\n        categories=False\n\n\n    # MAKE THE HEATMAP VISUALIZATION\n    plt.figure(figsize=figsize)\n    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n\n    if xyplotlabels:\n        plt.ylabel('True label')\n        plt.xlabel('Predicted label' + stats_text)\n    else:\n        plt.xlabel(stats_text)\n    \n    if title:\n        plt.title(title)","1b0963c0":"labelnames = ['5', '6', '7']","4d2741b7":"from sklearn.metrics import confusion_matrix\ncf_matrix = confusion_matrix(yt, yp)\nmake_confusion_matrix(cf_matrix, cmap='Blues', categories=labelnames)","1847186a":"#### Q3. Create Deep Learning Model to predict the quality of wine. Our model will focus only on wine with quality of 5, 6, 7 (quality = label)\n<b><font color=red>The difference of Train and Test accuracy must be lower than 10% to get full score.<\/font><\/b>"}}