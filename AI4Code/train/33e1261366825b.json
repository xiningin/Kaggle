{"cell_type":{"10b5325b":"code","84060637":"code","5ec19d2c":"code","cfdeb66d":"code","49454d24":"code","743cef69":"code","a21f00c0":"code","9c2c09c9":"code","57b28b74":"code","0662b421":"code","1a740fdd":"code","2e21ff35":"code","5a210365":"code","d49456f8":"code","33e70c54":"code","a314e844":"code","94311c56":"code","6f5e4f8f":"code","0a787b00":"code","70d11ef4":"code","7d9aa0b1":"code","ea643a49":"code","72ef241e":"code","40d2e602":"code","243ba4cc":"code","5aad6cd5":"markdown","510a3076":"markdown","fb8a5304":"markdown","54c95d5d":"markdown","957b4361":"markdown"},"source":{"10b5325b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","84060637":"%%time\ntrain = pd.read_csv('..\/input\/train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})","5ec19d2c":"train.head(5)","cfdeb66d":"train.shape","49454d24":"partial_train = train[::20]","743cef69":"partial_train.head(5)","a21f00c0":"partial_train['time_to_failure'].shape","9c2c09c9":"figure, axes1 = plt.subplots(figsize=(18,10))\n\nplt.title(\"Seismic Data Trends with 5% sample of original data\")\n\nplt.plot(partial_train['acoustic_data'], color='r')\naxes1.set_ylabel('Acoustic Data', color='r')\nplt.legend(['Acoustic Data'])\n\naxes2 = axes1.twinx()\nplt.plot(partial_train['time_to_failure'], color='g')\naxes2.set_ylabel('Time to Failure', color='g')\nplt.legend(['Time to Failure'])","57b28b74":"# list of features to be engineered\n\nfeatures = ['mean','max','variance','min', 'stdev', 'max-min-diff',\n            'max-mean-diff', 'mean-change-abs', 'abs-max', 'abs-min',\n            'std-first-50000', 'std-last-50000', 'mean-first-50000',\n            'mean-last-50000', 'max-first-50000', 'max-last-50000',\n            'min-first-50000', 'min-last-50000']","0662b421":"# Feature Engineering\n\nrows = 150000\nsegments = int(np.floor(train.shape[0] \/ rows))\n\nX = pd.DataFrame(index=range(segments), dtype=np.float64,\n                       columns=features)\nY = pd.DataFrame(index=range(segments), dtype=np.float64,\n                       columns=['time_to_failure'])\n\nfor segment in range(segments):\n    seg = train.iloc[segment*rows:segment*rows+rows]\n    x = seg['acoustic_data'].values\n    y = seg['time_to_failure'].values[-1]\n    \n    Y.loc[segment, 'time_to_failure'] = y\n    \n    X.loc[segment, 'mean'] = x.mean()\n    X.loc[segment, 'stdev'] = x.std()\n    X.loc[segment, 'variance'] = np.var(x)\n    X.loc[segment, 'max'] = x.max()\n    X.loc[segment, 'min'] = x.min()\n    X.loc[segment, 'max-min-diff'] = x.max()-x.min()\n    X.loc[segment, 'max-mean-diff'] = x.max()-x.mean()\n    X.loc[segment, 'mean-change-abs'] = np.mean(np.diff(x))\n    X.loc[segment, 'abs-min'] = np.abs(x).min()\n    X.loc[segment, 'abs-max'] = np.abs(x).max()\n    X.loc[segment, 'std-first-50000'] = x[:50000].std()\n    X.loc[segment, 'std-last-50000'] = x[-50000:].std()\n    X.loc[segment, 'mean-first-50000'] = x[:50000].min()\n    X.loc[segment, 'mean-last-50000'] = x[-50000:].mean()\n    X.loc[segment, 'max-first-50000'] = x[:50000].max()\n    X.loc[segment, 'max-last-50000'] = x[-50000:].max()\n    X.loc[segment, 'min-first-50000'] = x[:50000].min()\n    X.loc[segment, 'min-last-50000'] = x[-50000:].min()","1a740fdd":"X.head(5)","2e21ff35":"data = pd.concat([X,Y],axis=1)","5a210365":"sns.set(rc={'figure.figsize': (18,12)})\nsns.pairplot(data)","d49456f8":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler","33e70c54":"X_train,X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state=1210)","a314e844":"scaler = StandardScaler()\nscaler.fit(X_train)\nX_train_sc = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns)\n\nscaler.fit(X_test)\nX_test_sc = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)","94311c56":"lr = LinearRegression()","6f5e4f8f":"lr.fit(X_train_sc,y_train)","0a787b00":"pred = lr.predict(X_test_sc)","70d11ef4":"mean_absolute_error(y_test, pred)","7d9aa0b1":"from lightgbm import LGBMRegressor\nparams = {'num_leaves': 54,'min_data_in_leaf': 79,'objective': 'huber',\n         'max_depth': -1, 'learning_rate': 0.01, \"boosting\": \"gbdt\",\n         # \"feature_fraction\": 0.8354507676881442,\n         \"bagging_freq\": 3,\"bagging_fraction\": 0.8126672064208567,\n         \"bagging_seed\": 11,\"metric\": 'mae',\n         \"verbosity\": -1,'reg_alpha': 1.1302650970728192,\n         'reg_lambda': 0.3603427518866501}","ea643a49":"lgbm = LGBMRegressor(nthread=4,n_estimators=10000,\n            learning_rate=0.01,num_leaves=54,\n            colsample_bytree=0.9497036,subsample=0.8715623,\n            max_depth=8,reg_alpha=0.04,\n            reg_lambda=0.073,min_child_weight=40,silent=-1,verbose=-1,)","72ef241e":"lgbm.fit(X_train, y_train)","40d2e602":"pred_lgbm = lgbm.predict(X_test)","243ba4cc":"mean_absolute_error(y_test,pred_lgbm)","5aad6cd5":"The data is huge, training data contains nearly **600 million rows** and that is A LOT of data to understand. I am just curious to know how long Kaggle servers will take to read this data.","510a3076":"# Data Preparation\nIt took about 6 million datapoints to get one failure. Each of test csv files contains 150,000 datapoints. Thus, we should be able to get about **629145480 \/ 150000 = 4194** samples (subsets similar to our test dataset for training our model) before the first failure in our training set. Their corresponding labels should be the time_to_failure at the last datapoint in each subset. We would use a regression model to predict the time to failure for each corresponding subset sample.","fb8a5304":"# General information\nForecasting earthquakes is one of the most important problems in Earth science because of their devastating consequences. Current scientific studies related to earthquake forecasting focus on three key points: **when** the event will occur, **where** it will occur, and **how** large it will be. In this competition we try to predict time left to the next laboratory earthquake based on seismic signal data to answer the question of **when** earthquake will occur.\n\nTraining data represents one huge signal, but in test data we have many separate chunks, for each of which we need to predict time to failure.","54c95d5d":"The score can definitely be improved a lot. I will come back with more insights and update this kernel for better scores. Maybe using multiple predictors and the averaging their results will help reduce the error.","957b4361":"## Summary\nData is **HUGE!** <br>\nBefore every Failure, **there is a peak** in Acoustic Data. <br>\nSince we only have acoustic data to predict the time to failure, we need to **generate some features**. <br>\nI would use **Feature Engineering** to generate some common statistical features like **Mean, Variance, Max, Min, Std. Dev.** of our acoustic data. <br>\nSince the test data is **segmented into chunks of data**, it is better to segment our training data into chunks and then generate the features. <br>\n"}}