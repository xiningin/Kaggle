{"cell_type":{"871d7f2c":"code","8a554134":"code","2f6c8918":"code","b50a623d":"code","a0df5e1f":"code","f95bd727":"code","a6b45d0c":"code","c60593cc":"code","f85ad05b":"code","79942f6d":"code","1abd04f1":"code","47af691f":"code","021b2300":"code","e2f038bd":"code","28996d1b":"code","94f4bc54":"code","441d6d67":"code","42c474d3":"code","f96ae110":"code","02f2c823":"code","050854b2":"code","d1f5cbed":"code","9adb4e5a":"code","124c3914":"code","2f67167a":"code","48ebaa84":"code","95ecae80":"code","e5da5816":"code","8f3b535a":"code","b92a5f8a":"code","38d68336":"code","444833e6":"code","c88a3c42":"code","a15d7834":"code","8f62873a":"code","405183f0":"code","fdddb7ff":"code","dac58cc1":"code","f6a3b550":"code","d7d28469":"code","46c3036c":"code","f4b812f1":"code","9032b0cc":"code","97b791f1":"code","2e6a042f":"code","c18f0aa1":"code","236bf44f":"code","7c859221":"code","ea94e2f3":"code","a4bf3a7f":"code","6a484541":"code","b69393fe":"code","3bec83b4":"code","dd5a54ee":"code","eb7cc34e":"code","27fb0fff":"code","ad832943":"markdown"},"source":{"871d7f2c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8a554134":"from __future__ import absolute_import, division, print_function, unicode_literals\ntry:\n  # %tensorflow_version only exists in Colab.\n  %tensorflow_version 2.x\nexcept Exception:\n  pass\nimport tensorflow as tf\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\n\nmpl.rcParams['figure.figsize'] = (8, 6)\nmpl.rcParams['axes.grid'] = False","2f6c8918":"tf.__version__","b50a623d":"train = pd.read_csv('\/kaggle\/input\/ltfs-2\/train_fwYjLYX.csv')\ntest = pd.read_csv('\/kaggle\/input\/ltfs-2\/test_1eLl9Yf.csv')","a0df5e1f":"train['application_date']=pd.to_datetime(train['application_date'],format=\"%Y-%m-%d\")\ntest['application_date']=pd.to_datetime(test['application_date'],format=\"%Y-%m-%d\")","f95bd727":"train=train.groupby(['application_date','segment']).sum().reset_index()\n\n","a6b45d0c":"train['weekday']=train['application_date'].apply(lambda x : x.weekday())\ntrain['day']=train['application_date'].apply(lambda x : x.day)\ntrain['month']=train['application_date'].apply(lambda x : x.month)\n#train['year']=train['application_date'].apply(lambda x : x.year)\n\ntest['weekday']=test['application_date'].apply(lambda x : x.weekday())\ntest['day']=test['application_date'].apply(lambda x : x.day)\ntest['month']=test['application_date'].apply(lambda x : x.month)\n#test['year']=test['application_date'].apply(lambda x : x.year)","c60593cc":"train_seg1 = train[train['segment']==1]\ntrain_seg2 = train[train['segment']==2]","f85ad05b":"idx = pd.date_range('2017-04-01', '2019-07-05')\ntrain_seg1.index=train_seg1.application_date\ntrain_seg1.drop(['application_date'],axis=1,inplace=True)\ntrain_seg1=train_seg1.reindex(idx,method='ffill')\ntrain_seg1.head()","79942f6d":"idx = pd.date_range('2017-04-01', '2019-07-23')\ntrain_seg2.index=train_seg2.application_date\ntrain_seg2.drop(['application_date'],axis=1,inplace=True)\ntrain_seg2=train_seg2.reindex(idx,method='ffill')\ntrain_seg2.head()","1abd04f1":"idx = pd.date_range('2019-07-06', '2019-10-24')\ntest.index=test.application_date\ntest.drop(['application_date'],axis=1,inplace=True)\n#test=test.reindex(idx,method='ffill')\ntest.head()","47af691f":"test.head()","021b2300":"train_seg1.head()","e2f038bd":"features_considered = ['case_count','weekday','day','month']\ntarget = ['case_count']","28996d1b":"features_s1 = train_seg1[features_considered]\nfeatures_s1.head()","94f4bc54":"features_s2 = train_seg2[features_considered]\nfeatures_s2.head()","441d6d67":"features_s1.plot(subplots=True)","42c474d3":"features_s2.plot(subplots=True)","f96ae110":"train_seg1_split = int(int(train_seg1.shape[0])\/1.2)\ntrain_seg2_split = int(int(train_seg2.shape[0])\/1.2)","02f2c823":"TRAIN_SPLIT_s1=train_seg1_split\nTRAIN_SPLIT_s2=train_seg2_split","050854b2":"dataset_s1 = features_s1.values\n#data_mean_s1 = dataset_s1[:TRAIN_SPLIT_s1].mean(axis=0)\n#data_std_s1 = dataset_s1[:TRAIN_SPLIT_s1].std(axis=0)\n#dataset_s1 = (dataset_s1-data_mean_s1)\/data_std_s1","d1f5cbed":"dataset_s2 = features_s2.values\n#data_mean_s2 = dataset_s2[:TRAIN_SPLIT_s2].mean(axis=0)\n#data_std_s2 = dataset_s2[:TRAIN_SPLIT_s2].std(axis=0)\n#dataset_s2 = (dataset_s2-data_mean_s2)\/data_std_s2","9adb4e5a":"dataset_s2[0]","124c3914":"#data_mean_s2","2f67167a":"dataset_s1[4]","48ebaa84":"\ndef split_sequences(sequences, n_steps):\n    X, y = list(), list()\n    for i in range(len(sequences)):\n    # find the end of this pattern\n        end_ix = i + n_steps\n    # check if we are beyond the dataset\n        if end_ix > len(sequences):\n            break\n    # gather input and output parts of the pattern\n        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n        X.append(seq_x)\n        y.append(seq_y)\n    return np.array(X), np.array(y)","95ecae80":"train_seg1.head()","e5da5816":"dataset_s1[0]","8f3b535a":"dataset_s2[0]","b92a5f8a":"in_seq1_s1 = np.array(list(train_seg1['weekday']))\nin_seq2_s1 = np.array(list(train_seg1['day']))\nin_seq3_s1 = np.array(list(train_seg1['month']))\n#in_seq4_s1 = np.array(list(train_seg1['year']))\nout_seq_s1 = np.array(list(train_seg1['case_count']))\n#in_seq1 = np.array([1,2,3])\n#in_seq2 = np.array([5,6,7])\n#in_seq3 = np.array([8,9,0])\n\nin_seq1_s1 = in_seq1_s1.reshape((len(in_seq1_s1), 1))\nin_seq2_s1 = in_seq2_s1.reshape((len(in_seq2_s1), 1))\nin_seq3_s1 = in_seq3_s1.reshape((len(in_seq3_s1), 1))\n#in_seq4_s1 = in_seq4_s1.reshape((len(in_seq4_s1), 1))\nout_seq_s1 = out_seq_s1.reshape((len(out_seq_s1), 1))\n\n#dataset_s1 = np.hstack((in_seq1_s1, in_seq2_s1, in_seq3_s1,in_seq4_s1,out_seq_s1))\ndataset_s1 = np.hstack((in_seq1_s1, in_seq2_s1, in_seq3_s1,out_seq_s1))\nn_steps = 10\n# convert into input\/output\nX_s1, y_s1 = split_sequences(dataset_s1, n_steps)\n\n","38d68336":"in_seq1_s2 = np.array(list(train_seg2['weekday']))\nin_seq2_s2 = np.array(list(train_seg2['day']))\nin_seq3_s2 = np.array(list(train_seg2['month']))\n#in_seq4_s2 = np.array(list(train_seg2['year']))\nout_seq_s2 = np.array(list(train_seg2['case_count']))\n#in_seq1 = np.array([1,2,3])\n#in_seq2 = np.array([5,6,7])\n#in_seq3 = np.array([8,9,0])\n\nin_seq1_s2 = in_seq1_s2.reshape((len(in_seq1_s2), 1))\nin_seq2_s2 = in_seq2_s2.reshape((len(in_seq2_s2), 1))\nin_seq3_s2 = in_seq3_s2.reshape((len(in_seq3_s2), 1))\n#in_seq4_s2 = in_seq4_s2.reshape((len(in_seq4_s2), 1))\nout_seq_s2 = out_seq_s2.reshape((len(out_seq_s2), 1))\n\n#dataset_s2 = np.hstack((in_seq1_s2, in_seq2_s2, in_seq3_s2,in_seq4_s2,out_seq_s2))\ndataset_s2 = np.hstack((in_seq1_s2, in_seq2_s2, in_seq3_s2,out_seq_s2))\nn_steps = 10\n# convert into input\/output\nX_s2, y_s2 = split_sequences(dataset_s2, n_steps)\n#n_features_s2 = X_s2.shape[2]\n#n_input_s2 = X_s2.shape[1] * X_s2.shape[2]\n#X_s2 = X_s2.reshape((X_s2.shape[0], n_input_s2))\n\n\n","444833e6":"X_s2.shape","c88a3c42":"test_s1 = test[test['segment']==1]\n#x_input_test_s1 = [[x,y,z,z1] for x,y,z,z1 in zip(list(test_s1['weekday']),list(test_s1['day']),list(test_s1['month']),list(test_s1['year']))]\nx_input_test_s1 = [[x,y,z] for x,y,z in zip(list(test_s1['weekday']),list(test_s1['day']),list(test_s1['month']))]\n\ntest_s2 = test[test['segment']==2]\nx_input_test_s2 = [[x,y,z] for x,y,z in zip(list(test_s2['weekday']),list(test_s2['day']),list(test_s2['month']))]","a15d7834":"x_input_test_final_s1 = []\ni = 1\nthval = len(x_input_test_s1)-n_steps\n#print(thval)\nfor x in range(len(x_input_test_s1)):\n    #print(thval-x,thval)\n    if (thval-x)<0:\n        #print(x_input_test_final_s1[-1])\n        val = x_input_test_s1[x:x+10] + list([x_input_test_s1[x]])*np.abs(thval-x)\n        x_input_test_final_s1.append(val)\n    else:\n        x_input_test_final_s1.append(x_input_test_s1[x:x+10])     ","8f62873a":"x_input_test_final_s1[-3]","405183f0":"len(x_input_test_final_s1[-1])","fdddb7ff":"x_input_test_final_s2 = []\ni = 1\nthval = len(x_input_test_s2)-n_steps\n#print(thval)\nfor x in range(len(x_input_test_s2)):\n    #print(thval-x,thval)\n    if (thval-x)<0:\n        #print(x_input_test_final_s1[-1])\n        val = x_input_test_s2[x:x+10] + list([x_input_test_s2[x]])*np.abs(thval-x)\n        x_input_test_final_s2.append(val)\n    else:\n        x_input_test_final_s2.append(x_input_test_s2[x:x+10])     ","dac58cc1":"print(X_s1.shape, y_s1.shape)\n# summarize the data\nfor i in range(len(X_s1)):\n    print(X_s1[i], y_s1[i])","f6a3b550":"print(X_s2.shape, y_s2.shape)\n# summarize the data\nfor i in range(len(X_s2)):\n    print(X_s2[i], y_s2[i])","d7d28469":"train_seg2.tail()","46c3036c":"#n_features_s1,n_features_s2","f4b812f1":"n_input_s1 = X_s1.shape[1] * X_s1.shape[2]\nX_s1 = X_s1.reshape((X_s1.shape[0], n_input_s1))\n\nmodel = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Dense(100, activation='relu', input_dim=n_input_s1))\n#model.add(tf.keras.layers.Dense(1))\n#model.add(tf.keras.layers.GRU(20, return_sequences=True))\n#model.add(tf.keras.layers.LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n#model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1)))\nmodel.add(tf.keras.layers.Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\n\nmodel.fit(X_s1, y_s1, epochs=2000,verbose=0)","9032b0cc":"results_s1 = []\nfor input_val in x_input_test_final_s1:\n    val = np.array(input_val)\n    yhat = val.reshape((1, n_input_s1))\n    yhat = model.predict(yhat)\n    results_s1.append(yhat[0][0])","97b791f1":"results_s1","2e6a042f":"train_seg1.tail(25)","c18f0aa1":"x_input_test_final_s1","236bf44f":"n_input_s2 = X_s2.shape[1] * X_s2.shape[2]\nX_s2 = X_s2.reshape((X_s2.shape[0], n_input_s2))\n\nmodel_s2 = tf.keras.Sequential()\nmodel_s2.add(tf.keras.layers.Dense(100, activation='relu', input_dim=n_input_s2))\nmodel_s2.add(tf.keras.layers.Dense(1))\nmodel_s2.compile(optimizer='adam', loss='mse')\n\nmodel_s2.fit(X_s2, y_s2, epochs=2000,verbose=0)","7c859221":"x_input = np.array([[ 5,  6,  7],[ 6,  7,  7],[ 0,  8,  7]])","ea94e2f3":"results_s2 = []\nfor input_val in x_input_test_final_s2:\n    val = np.array(input_val)\n    yhat = val.reshape((1, n_input_s2))\n    yhat = model_s2.predict(yhat)\n    results_s2.append(yhat[0][0])","a4bf3a7f":"results_s2","6a484541":"x_input_test_final_s2","b69393fe":"train_seg2.tail(45)","3bec83b4":"test_s1['case_count']= results_s1\ntest_s2['case_count']= results_s2\nsubmission = pd.concat([test_s1,test_s2])","dd5a54ee":"submission = submission.reset_index().sort_values('id')[['id','application_date','segment','case_count']]\nsubmission.head()","eb7cc34e":"submission.to_csv('submission.csv',index=False)","27fb0fff":"submission.shape","ad832943":"# Time series forecasting"}}