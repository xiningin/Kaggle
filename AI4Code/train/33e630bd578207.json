{"cell_type":{"7bd684fe":"code","b22d9d2c":"code","9a0569d2":"code","15cbdf27":"code","38e21693":"code","3d4eade5":"code","41b029c3":"code","67042c35":"code","08550deb":"code","f8992496":"code","85dcf710":"code","b14d8375":"code","08912ff6":"code","dd81f6c2":"code","88a4af9b":"code","a1a210b3":"code","77dea5de":"code","3b3fe441":"code","6c16fbdf":"code","1e8a4c89":"code","3f3f0d81":"code","e9aa8aea":"code","dcaa5dc4":"code","345b4a8e":"code","3cfc810e":"code","20470d9e":"code","9a19bea3":"code","93f19d30":"code","b0ff7126":"code","4f1cd1c9":"code","0cf9d0fc":"code","c37c89b5":"code","a1463fff":"markdown","0fe3c23b":"markdown","4e827753":"markdown","3565dd6b":"markdown","14af4800":"markdown","ef660f34":"markdown","78e92d6e":"markdown","7a45b308":"markdown","f2cc4978":"markdown","9d27af0a":"markdown","c2d16ad2":"markdown","dba6866f":"markdown"},"source":{"7bd684fe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b22d9d2c":"# import Libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\n% matplotlib inline","9a0569d2":"# Read and load Data\ntrain_data = pd.read_csv(\"..\/input\/train.csv\")\n","15cbdf27":"# check index of dataframe\ntrain_data.columns\n","38e21693":"# Describe Stastistics Data\ntrain_data.describe()","3d4eade5":"#PLot Histogram for 'SalePrice'\nsns.distplot(train_data['SalePrice'])","41b029c3":"# Skewness and Kurtosis\nprint(\"Skewness : %f\" % train_data['SalePrice'].skew())\nprint(\"Kurtosis : %f\" % train_data['SalePrice'].kurt())","67042c35":"# Correlation Matrix(Heat map)\ncorrmat = train_data.corr()\nf, ax = plt.subplots(figsize = (12,12))\nsns.heatmap(corrmat,cmap = \"Greens\",vmax = 0.8, square = True)\n","08550deb":"correlations = train_data.corr()\ncorrelations = correlations[\"SalePrice\"].sort_values(ascending=False)\nfeatures = correlations.index[1:6]\ncorrelations.head(10)","f8992496":"#'SalePrice' Correlation Matrix\nk = 10\ncols = corrmat.nlargest(k , 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train_data[cols].values.T)\nsns.set(font_scale = 1.00)\nhm = sns.clustermap(cm , cmap = \"Greens\",cbar = True,square = True,\n                 yticklabels = cols.values, xticklabels = cols.values)","85dcf710":"# SCATTER PLOT\nsns.set()\nexpensive = train_data['SalePrice'].quantile(0.9) # 90th percentile\ntrain_data['expensive'] = train_data['SalePrice'].apply(lambda x: 'Expensive' if x > expensive else 'Not-expensive')\nsns.pairplot(train_data[[\"expensive\", \"SalePrice\", \"OverallQual\", \"TotalBsmtSF\", \"GarageArea\",\"GrLivArea\",\"YearBuilt\" ]]\n                ,size = 2.5, kind = 'scatter', hue = 'expensive') \nplt.show()\n","b14d8375":"# Missing Data\ntotal = train_data.isnull().sum().sort_values(ascending = False)\npercent = (train_data.isnull().sum()\/train_data.isnull().count()).sort_values(ascending = False)\nmissing_data = pd.concat([total,percent], axis = 1, keys = ['Total', 'Percent'])\nmissing_data.head(20)","08912ff6":"# Cleaning Missing Data\ntrain_data = train_data.drop((missing_data[missing_data['Total'] > 1]).index,1)\ntrain_data = train_data.drop(train_data.loc[train_data['Electrical'].isnull()].index)\n# Check there is no missing data\ntrain_data.isnull().sum().max() ","dd81f6c2":"# Standaradising data\nSalePrice_scaled = StandardScaler().fit_transform(train_data['SalePrice'][: , np.newaxis]);\nlow = SalePrice_scaled[SalePrice_scaled[:,0].argsort()][: 10]\nhigh = SalePrice_scaled[SalePrice_scaled[:,0].argsort()][-10 :]\nprint(\"low_range:\", low)\nprint(\"high_range:\", high)","88a4af9b":"#Analyse SalePrice\/GrLiveArea\ndata = pd.concat([train_data['SalePrice'], train_data['GrLivArea']], axis = 1)\ndata.plot.scatter(x ='GrLivArea', y= 'SalePrice', ylim = (0,800000)); #, alpha=0.3);","a1a210b3":"# Deleting points\ntrain_data.sort_values(by = 'GrLivArea', ascending  = False)[: 2]\ntrain_data = train_data.drop(train_data[train_data['Id'] == 1299].index)\ntrain_data = train_data.drop(train_data[train_data['Id'] == 524].index)","77dea5de":"# Histogram and normal probability plot\nsns.distplot(train_data['SalePrice'], fit = norm)\nfig = plt.figure()\nres = stats.probplot(train_data['SalePrice'],plot = plt)","3b3fe441":"#Applying log transformation\ntrain_data['SalePrice'] = np.log(train_data['SalePrice'])","6c16fbdf":"#Transformed histogram and normal probability plot\n\nsns.distplot(train_data['SalePrice'], fit = norm)\nfig = plt.figure()\nres = stats.probplot(train_data['SalePrice'],plot = plt)","1e8a4c89":"#pairplot top 6 correlated columns with 'SalePrice'\nsns.set()\nsns.pairplot(train_data[[\"SalePrice\", \"OverallQual\",\"TotalBsmtSF\",\"GarageArea\",\"GrLivArea\",\"YearBuilt\" ]],\n                diag_kind = 'kde' ,size = 2.5) \nplt.show()\n","3f3f0d81":"y= train_data['SalePrice']\n\nfeatures_name =[\"OverallQual\",\"TotalBsmtSF\", \"GarageArea\",\"GrLivArea\",\"YearBuilt\"] \nX= train_data[features_name]\n","e9aa8aea":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)\n","dcaa5dc4":"#Lets Normalize the train data\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)","345b4a8e":"from sklearn.model_selection import KFold, cross_val_score\n\nkfolds = KFold(n_splits=5, shuffle=True, random_state=42)\n\ndef cv_rmse(model):\n    rmse = np.sqrt(-cross_val_score(model, X_train,y_train,\n                                   scoring=\"neg_mean_squared_error\", \n                                   cv = kfolds))\n    return(rmse)","3cfc810e":"# Linear Regression\nfrom sklearn import linear_model\nfrom sklearn.metrics import r2_score\n\n# Create linear regression object\nregr = linear_model.LinearRegression()\n\n# Train the model using the training sets\nregr.fit(X_train, y_train)\n\n# Make predictions using the testing set\ny_pred = regr.predict(X_test)\nprint('The accuracy of the Linear Regression is',r2_score(y_test,y_pred))","20470d9e":"import xgboost as xgb\nxg_reg = xgb.XGBRegressor(\n                 colsample_bytree=0.2,\n                 gamma=0.0,\n                 learning_rate=0.05,\n                 max_depth=6,\n                 min_child_weight=1.5,\n                 n_estimators=7200,                                                                  \n                 reg_alpha=0.9,\n                 reg_lambda=0.6,\n                 subsample=0.2,\n                 seed=42,\n                 silent=1)\nxg_reg.fit(X_train,y_train)\n\npreds = xg_reg.predict(X_test)\nprint('The accuracy of the xgboost is',r2_score(y_test,preds))\nprint(\" rmse : \",cv_rmse(xg_reg).mean(),'\\n')","9a19bea3":"from sklearn.model_selection import GridSearchCV\nimport xgboost as xgb\nparams = {\n        'min_child_weight': [1, 5],\n        'gamma': [0.5, 1],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }\n\n# Initialisation de XGB et GridSearch\nxgb = xgb.XGBRegressor(nthread=1, learning_rate=0.02, n_estimators=600) \ngrid = GridSearchCV(xgb, params)\n\n# Lancer la gridsearch\ngrid.fit(X_train, y_train)\n\n# Prendre le mod\u00e8le avec les meilleurs param\u00e8tres\nclf = grid.best_estimator_\n\n# Entrainer le meilleur algorithme \u00e0 notre jeu de donn\u00e9es\nclf.fit(X_train, y_train)\n\n# Afficher le score R2\nprint(r2_score(y_train, clf.predict(X_train))) \nprint(\" rmse : \",cv_rmse(clf).mean(),'\\n')","93f19d30":"from sklearn import linear_model\nfrom sklearn import svm\n\nclassifiers = [\n    svm.SVR(),\n    linear_model.BayesianRidge(),\n    linear_model.ARDRegression(),\n    linear_model.TheilSenRegressor(),\n    linear_model.LinearRegression()]\ntrainingData    = X_train\ntrainingScores  = y_train\npredictionData  = X_test\n\nfor item in classifiers:\n    print(item)\n    clf = item\n    clf.fit(trainingData, trainingScores)\n    y_predict = clf.predict(predictionData)\n    print('Accuracy are',r2_score(y_test,y_predict), '\\n')","b0ff7126":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report\nimport xgboost as xgb\nfrom sklearn.ensemble import BaggingRegressor\nmodel1 =xgb.XGBRegressor(\n                 colsample_bytree=0.2,\n                 gamma=0.0,\n                 learning_rate=0.05,\n                 max_depth=6,\n                 min_child_weight=1.5,\n                 n_estimators=7200,                                                                  \n                 reg_alpha=0.9,\n                 reg_lambda=0.6,\n                 subsample=0.2,\n                 seed=42,\n                 silent=1)\nmodel2 = linear_model.LinearRegression()\nmodel3 = linear_model.ElasticNetCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], \n                                   l1_ratio=[.01, .1, .5, .9, .99], max_iter=5000)\n\n\nmodel_1 = model1.fit(X_train,y_train)\nmodel_2 = model2.fit(X_train,y_train)\nmodel_3 = model3.fit(X_train,y_train)\n\n\npred1=model1.predict(X_test)\npred2=model2.predict(X_test)\npred3=model3.predict(X_test)\n\n\nscore1 =  model1.score(X_test,pred1)\nscore2 =  model2.score(X_test,pred2)\nscore3 =  model3.score(X_test,pred3)\n\nfinalpred=(pred1+pred2+pred3)\/3\nfinalscore = (score1 + score2 +score3)\/3\nprint(\" XGB: \",cv_rmse(model_1).mean(),'\\n')\nprint(\"LR rmse : \",cv_rmse(model_2).mean(),'\\n')\nprint(\"en rmse : \",cv_rmse(model_3).mean(),'\\n')\nprint('Accuracy are',r2_score(y_test,finalpred), '\\n')\nprint('Final score is',finalscore)","4f1cd1c9":"import  lightgbm as lgb\nimport time\nparameters = {\n                'max_depth': 1,'min_data_in_leaf': 85,'feature_fraction': 0.80,'bagging_fraction':0.8,'boosting_type':'gbdt',\n                'learning_rate': 0.1, 'num_leaves': 30,'subsample': 0.8,'lambda_l2': 4,'objective': 'regression_l2',\n                'application':'regression','num_boost_round':5000,'zero_as_missing': True,\n                'early_stopping_rounds':100,'metric': 'mae','seed': 2\n             }\ntrain_data = lgb.Dataset(X_train, y_train, silent=False)\ntest_data = lgb.Dataset(X_test, y_test, silent=False)\nlgb_model = lgb.train(parameters, train_set = train_data,verbose_eval=500, valid_sets=test_data)","0cf9d0fc":"df_test = pd.read_csv(\"..\/input\/test.csv\")\ndf_test =df_test.reset_index()\ndf_test[features_name].isnull().sum() #detect missing values\n #complete missing TotalBsmtSF with median\ndf_test['TotalBsmtSF'].fillna(df_test['TotalBsmtSF'].median(), inplace = True)\n #complete missing GarageArea  with median\ndf_test['GarageArea'].fillna(df_test['GarageArea'].median(), inplace = True)\n","c37c89b5":"test_X = df_test[features_name]\n\npred=np.expm1(lgb_model.predict(test_X))\nmy_submission = pd.DataFrame({'Id': df_test.Id, 'SalePrice': pred})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","a1463fff":" **HEATMAP**\n  \n  A heatmap is a graphical representation of data that uses a system of color-coding to represent different values.  A simple heat map provides an immediate visual summary of information.","0fe3c23b":"Let's import the basic libraries for our analysis","4e827753":"**Cleaning missing data **\n\nIn statistics, missing data, or missing values, occur when no data value is stored for the variable in an observation. Missing data are a common occurrence and can have a significant effect on the conclusions that can be drawn from the data.\n The goal of cleaning operations is to prevent problems caused by missing data that can arise when training a model.","3565dd6b":"The red color shows the distribution of expensive houses and the blue color data shows the distribution of non-expensive houses. ","14af4800":"**Scatter Plot**\n\nA scatter plot is a graph used to determine whether there is a relationship between paired data. \n If y tends to increase as x increases, then the paired data are said to be a positive correlation.  If y tends to decrease as   x increases, the paired data are said to be a negative correlation.  If the points show no linear pattern, the paired data are said to have relatively no correlation.","ef660f34":"**OUTLIERS : - **\n\nFocusing on outliers, defined by Gladwell as people who do not fit into our normal understanding of achievement. Outliers deals with exceptional people, especially those who are smart, rich, and successful, and those who operate at the extreme outer edge of what is statistically plausible. An outlier is a data point that is distant from other similar points. They may be due to variability in the measurement or may indicate experimental errors.  If possible, outliers should be excluded from the data set. We'll do a quick analysis through the standard deviation of 'SalePrice' and a set of scatter plots.","78e92d6e":"Let's have a look at the distribution of 'SalePrice' by plotting  a simple histogram","7a45b308":"**CLUSTER MAP:-** \nWith the distance between each pair of samples computed, we need clustering algorithms to join them into groups. Hierarchical clustering is one of the many clustering algorithms available to do this. Each sample is assigned to its own group and then the algorithm continues iteratively, joining the two most similar clusters at each step, and continuing until there is just one group.\n\n\nAccording to our clustermap, following variables are most correlated with 'SalePrice':\n\n1. 'OverallQual', 'GarageCars','GarageArea','1stFloor' and 'TotalBsmtSF' are strongly correlated with 'SalePrice'.\n2. 'GarageCars' and 'GarageArea' are in same group. You'll never be able to distinguish them. Therefore, we just need one of these variables in our analysis (we can keep 'GarageArea' since its correlation with 'SalePrice' is higher).\n3.'TotalBsmtSF' and '1stFloor' also seem to be in a same group. We can keep 'TotalBsmtSF'..\n4.YearBuilt', TotRmsAbvGrd' and 'GrLivArea' are slightly correlated with 'SalePrice'.\n5.'TotRmsAbvGrd' and 'GrLivArea' are also in a same group. So let's take 'GrLivArea'","f2cc4978":"'SalePrice' is extended towards the right side more which shows positive skewness.\nLog transformations are best used to solve the problem.","9d27af0a":"Let's inspect the correlations to get a better idea of which columns correlate the most with the Sale Price of the house.","c2d16ad2":"**Skewness: - **\n\nThe term \u2018skewness\u2019 is used to mean the absence of symmetry from the mean of the dataset. Skewness is used to indicate the shape of the distribution of data.In a skewed distribution, the curve is extended to either left or right side. So, when the plot is extended towards the right side more, it denotes positive skewness. On the other hand, when the plot is stretched more towards the left direction, then it is called as negative skewness.\n\n**Kurtosis:-**  \nIn statistics, kurtosis is defined as the parameter of relative sharpness of the peak of the probability distribution curve.It is used to indicate the flatness or peakedness of the frequency distribution curve and measures the tails or outliers of the distribution.Positive kurtosis represents that the distribution is more peaked than the normal distribution, whereas negative kurtosis shows that the distribution is less peaked than the normal distribution.","dba6866f":"Let's try an interesting angle of 'SalePrice'. Let's create a new column called 'expensive' which tag 'SalePrice' as expensive if its price is greater than 90 percentile of 'SalePrice'  "}}