{"cell_type":{"b8e858a0":"code","67403c69":"code","69fc4e19":"code","58224a84":"code","11669191":"code","711f5ec1":"code","7135a46a":"code","412d8e2d":"code","d9cdb671":"code","3e5cd208":"code","2288ea26":"code","30087cdd":"code","fef203dd":"code","45a55604":"markdown","f1aa6a6d":"markdown","d1b32d02":"markdown","d1998468":"markdown","cae2c077":"markdown","b82a2973":"markdown","9c66ecd3":"markdown","68eb334c":"markdown","e5aa7d56":"markdown","a0a7c467":"markdown","105a00f0":"markdown","1fa03494":"markdown","9fdd305e":"markdown","1f3bfb21":"markdown"},"source":{"b8e858a0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport matplotlib.pyplot as plt\n\nimport pymc3 as pm\nimport arviz as az\n\naz.style.use('arviz-darkgrid')\n\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.linear_model import Ridge","67403c69":"df = pd.read_csv('\/kaggle\/input\/students-performance-in-exams\/StudentsPerformance.csv')\ndf.head(5)","69fc4e19":"df = df.drop(columns=['reading score', 'writing score'])\ny = df['math score']\nX = df[['gender', 'race\/ethnicity', 'parental level of education', 'lunch', 'test preparation course']]","58224a84":"X_ohe = OneHotEncoder(drop='first', sparse=False).fit_transform(X)","11669191":"X_train, X_test, y_train, y_test = train_test_split(\n    X_ohe, y, test_size=0.1, random_state=123\n)","711f5ec1":"DIM = 12\n\nwith pm.Model() as robust_linreg_model:\n    X_data = np.array([pm.Data(f\"X_data_{i+1}\", X_train[:, i]) for i in range(DIM)])\n    w0 = pm.Normal('w0', mu=0, sd=np.array(20))\n    w = np.array([pm.Normal(f'w{i+1}', mu=0, sd=np.array(10)) for i in range(DIM)])\n    sigma = pm.HalfNormal('sigma', sd=2)\n    nu = pm.Exponential('nu', 1)\n    outputs = pm.StudentT('y', mu=w0 + np.sum(w*X_data), sd=sigma, nu=nu, observed=y_train)\n    \npm.model_to_graphviz(robust_linreg_model)","7135a46a":"with robust_linreg_model:\n    inf_data_robust = pm.sample(draws=2000, tune=2000, chains=2, cores=2, \n                         return_inferencedata=True)","412d8e2d":"az.summary(inf_data_robust, round_to=2)","d9cdb671":"az.plot_trace(inf_data_robust, compact=False)","3e5cd208":"az.plot_forest(inf_data_robust,\n               model_names = ['Robust Linreg'],\n               hdi_prob=0.95, figsize=(6, 4));","2288ea26":"with robust_linreg_model:\n    pm.set_data({f'X_data_{i+1}': X_test[:, i] for i in range(12)})\n    samples_train = pm.sample_posterior_predictive(inf_data_robust)","30087cdd":"from scipy.stats import mode\nmse(y_test, mode(np.rint(samples_train['y']))[0][0])","fef203dd":"mse(y_test, Ridge().fit(X_train, y_train).predict(X_test))","45a55604":"... and sampling is also well-looking.","f1aa6a6d":"## Intro","d1b32d02":"## Bayesian model","d1998468":"# Analysis of student's perfomance in math using Bayesian models","cae2c077":"Just common train-test split.","b82a2973":"Let's evaluate our prediction using mean squared error. But before doing this we need to round our predictions and somehow pick the final prediction. Let's say it would be the most common one.","9c66ecd3":"Let's just read data and have a first look on it","68eb334c":"Let's OHE them. ``drop='first'`` argument means that the first category will be encoded as zeros, so that the number of columns for the feature with $n$ categories will be $n - 1$.","e5aa7d56":"Comparing this to common Ridge Regression we see, that our model does it's job not much worse. That's cool :)","a0a7c467":"Okay, let's create Bayesian model:\n$$\nbias \\sim \\mathcal{N}(0, 20),\n$$\n$$\nw_i \\sim \\mathcal{N}(0, 5),\n$$\n$$\n\\sigma \\sim |\\mathcal{N}(0, 2)|,\n$$\n$$\n\\nu \\sim Exp(1),\n$$\n$$\ny \\sim t(\\nu = \\nu, \\mu = bias + w \\cdot X, sd = \\sigma).\n$$\n\n<p style=\"text-align: center;\"><img src=\"https:\/\/i.ibb.co\/vvr7t5m\/bayes-model-diag-page-0001.jpg\" alt=\"Model\" border=\"0\"><\/p>","105a00f0":"Confidence intervals are small enough","1fa03494":"`r_hat` is perfect","9fdd305e":"There are three possible targets, but we'll be working with only one of them: math score.\n\nFirst of all we need to specify the task to be solved. Task specification for such kind of target is a well-known interview question, and the preffered answer is that this one is a *regression* problem. The reason is that classification algorithms cannot compare two mistakes, i.e. in case when correct answer is 65 the prediction of 64 is the same bad as of 25 from the classifications metrics point of view, but it's pretty obvious, that the prediction of 64 is not bad at all when the other one is just terrible.\n\nOkay, so we specify a target column and features columns. It should be noted, that all features are categorical.","1f3bfb21":"## Predictions"}}