{"cell_type":{"04ac6ffc":"code","85932a49":"code","9278e9dd":"code","6653f433":"code","bf414ec3":"code","5065143b":"code","8188fa09":"code","8516cd11":"code","8a981e2d":"code","e8b8b333":"code","3ce5547f":"code","1641312b":"code","cb53e046":"code","dc6fbefd":"code","293483c6":"code","3e339224":"code","d3dcc949":"code","ae02e26c":"code","9579fca5":"code","86205d1d":"code","a0a8266f":"code","1d9fe3be":"code","be1c84c5":"code","3d4d6576":"code","ebbba4a0":"code","53f6a051":"code","ba94a58f":"code","a32622ae":"code","24417839":"code","fab6f661":"code","63f4bb38":"code","40dc4a58":"code","ac67f506":"code","fac67fe4":"code","6a6e4817":"code","e42d6d07":"code","5b37f03e":"markdown","46e1982f":"markdown","f59fc45b":"markdown","8b9f7bb4":"markdown","b9ee9865":"markdown","2811b760":"markdown","5b75e2fc":"markdown","5d231ffa":"markdown"},"source":{"04ac6ffc":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom scipy.stats import norm, skew","85932a49":"#Import data\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\n","9278e9dd":"#Inspect train data\ntrain.head()","6653f433":"#detailed info about column types and missing values\ntrain.info()","bf414ec3":"#Isolate y, drop categorical variables, drop Na rows. Separate y and X\ndata = train\ndata = data.select_dtypes(exclude=['object'])\ndata = data.dropna(axis=0)\ny = data[\"SalePrice\"]\nX = data.drop([\"SalePrice\"], axis=1)","5065143b":"from sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error","8188fa09":"#Always check amount of na before starting\ndata.isnull().sum().sum()","8516cd11":"X_train, X_test, y_train, y_test = train_test_split(X, y)","8a981e2d":"#RF_1\nmodel_rf1 = RandomForestRegressor(n_estimators = 500, random_state=17)\nmodel_rf1.fit(X_train, y_train)","e8b8b333":"rf1_pred = model_rf1.predict(X_test)","3ce5547f":"mae_rf1 = mean_absolute_error(rf1_pred, y_test)\nprint(mae_rf1)","1641312b":"#XGB_1\nmodel_xgb1 = XGBRegressor(n_estimators=500, learning_rate=0.1 ,random_state = 17)\nmodel_xgb1.fit(X_train, y_train)","cb53e046":"xgb1_pred = model_xgb1.predict(X_test)","dc6fbefd":"mae_xgb1 = mean_absolute_error(xgb1_pred, y_test)\nprint(mae_xgb1)","293483c6":"#make a submission for the competition\npredictor_cols = X_train.columns\ntest = test[predictor_cols]\nxgb1_finalpred = model_xgb1.predict(test)\nprint(xgb1_finalpred)\nmy_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': xgb1_finalpred})\nmy_submission.to_csv('xgb1_pred.csv', index=False)","3e339224":"test = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\n#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\nntrain = train.shape[0]\nntest = test.shape[0]\ny = train[\"SalePrice\"]\ndata = pd.concat([train, test], sort = True).reset_index(drop=True)","d3dcc949":"#EDA\nfig, (ax1, ax2) = plt.subplots(1,2,figsize=(20,8))\nsns.distplot(y_train, bins=50,  rug = True, ax = ax1)\nsns.regplot(data[\"SalePrice\"], data[\"GrLivArea\"], ax = ax2)\nplt.show()","ae02e26c":"#inspect correlation: select only numeric cols\nnumeric_cols = data.select_dtypes(exclude=['object'])\nnum_corr = numeric_cols.corr()\nfig, ax = plt.subplots(figsize=(15,15))\nsns.heatmap(num_corr, linewidths =.5)\nplt.show()","9579fca5":"#Finally drop SalePrice column, which we have previously stored in variable y\ndata.drop(['SalePrice'], axis=1, inplace=True)","86205d1d":"nullcols = data.isnull().sum().sort_values(ascending=False)\nnullcols = pd.DataFrame(nullcols, columns=[\"Total\"])\nnullcols[nullcols[\"Total\"] > 0]","a0a8266f":"#considering the great amount of NA I drop some variables\n#also drop utilities\ndata.drop(\"PoolQC\", axis = 1, inplace = True)\ndata.drop(\"MiscFeature\", axis = 1, inplace = True)\ndata.drop(\"Alley\", axis = 1, inplace = True)\ndata.drop(\"Fence\", axis = 1, inplace = True)\ndata = data.drop(['Utilities'], axis=1)\n#dealing with other NA\ndata['FireplaceQu'] = data['FireplaceQu'].fillna(\"No\")\ndata['LotFrontage'] = data['LotFrontage'].fillna(0)\ndata['GarageCond'] = data['GarageCond'].fillna(\"No\")\ndata['GarageType'] = data['GarageType'].fillna(\"No\")\ndata['GarageYrBlt'].fillna((data['GarageYrBlt'].mean()), inplace=True)\ndata['GarageFinish'] = data['GarageFinish'].fillna(\"No\")\ndata['GarageQual'] = data['GarageQual'].fillna(\"No\")\ndata['BsmtExposure'] = data['BsmtExposure'].fillna(\"No\")\ndata['BsmtFinType2'] = data['BsmtFinType2'].fillna(\"No\")\ndata['BsmtFinType1'] = data['BsmtFinType1'].fillna(\"No\")\ndata['BsmtCond'] = data['BsmtCond'].fillna(\"No\")\ndata['BsmtQual'] = data['BsmtQual'].fillna(\"No\")\ndata['MasVnrArea'] = data['MasVnrArea'].fillna(\"No\")\ndata['MasVnrType'] = data['MasVnrType'].fillna(\"No\")\ndata['Electrical'] = data['Electrical'].fillna(\"SBrkr\")\ndata['MSZoning'] = data['MSZoning'].fillna(data['MSZoning'].mode()[0])\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    data[col] = data[col].fillna(0)\ndata[\"Functional\"] = data[\"Functional\"].fillna(\"Typ\")\ndata['Exterior1st'] = data['Exterior1st'].fillna(data['Exterior1st'].mode()[0])\ndata['Exterior2nd'] = data['Exterior2nd'].fillna(data['Exterior2nd'].mode()[0])\nfor col in ('GarageArea', 'GarageCars'):\n    data[col] = data[col].fillna(0)\ndata['KitchenQual'] = data['KitchenQual'].fillna(data['KitchenQual'].mode()[0])\ndata['SaleType'] = data['SaleType'].fillna(data['SaleType'].mode()[0])","1d9fe3be":"data.isnull().sum().sum()","be1c84c5":"#transforming numerical variables that are actually categorical in the correct format\ndata['MSSubClass'] = data['MSSubClass'].apply(str)\ndata['OverallCond'] = data['OverallCond'].astype(str)\ndata['YrSold'] = data['YrSold'].astype(str)\ndata['MoSold'] = data['MoSold'].astype(str)","3d4d6576":"#Check which categorical variables are left and process them\ncategorical_variables = data.select_dtypes(include=['object'])\ncat_cols = categorical_variables.columns\ncat_cols","ebbba4a0":"#select meaningful columns to be labeled\nfrom sklearn.preprocessing import LabelEncoder\ncols = ('BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC',  'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor col in cols:\n    encoder = LabelEncoder() \n    encoder.fit(list(data[col].values)) \n    data[col] = encoder.transform(list(data[col].values))\n#add total house area variable\ndata['TotalSF'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']\ndata.info()\n","53f6a051":"numeric_feats = data.dtypes[data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","ba94a58f":"skewness = skewness[abs(skewness)>0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    data[feat] = boxcox1p(data[feat], lam)","a32622ae":"#add dummies\ndata = pd.get_dummies(data)    \ndata.info()","24417839":"#Models\ntrain = data[:ntrain]\ntest = data[ntrain:]\nX = train\nX_train, X_test, y_train, y_test = train_test_split(X, y)","fab6f661":"model_xgb2 = XGBRegressor(n_estimators=500, learning_rate=0.1 ,random_state = 17)","63f4bb38":"model_xgb2.fit(X_train, y_train)","40dc4a58":"xgb2_pred = model_xgb2.predict(X_test)\nmae_xgb2 = mean_absolute_error(xgb2_pred, y_test)\nprint(mae_xgb2)","ac67f506":"predictor_cols = X_train.columns\ntest = test[predictor_cols]\nxgb2_finalpred = model_xgb2.predict(test)\nprint(xgb2_finalpred)\nmy_submission2 = pd.DataFrame({'Id': test_ID, 'SalePrice': xgb2_finalpred})\nmy_submission2.to_csv('xgb2_pred.csv', index=False)","fac67fe4":"#improving the model with hyperparameters\n\nfrom sklearn.pipeline import Pipeline\nmy_pipeline = Pipeline(steps=[('model', XGBRegressor(colsample_bytree = 0.46, gamma = 0.047, \n                             learning_rate = 0.05, max_depth = 3, \n                             min_child_weight= 1.8, n_estimators = 2200,\n                             reg_alpha = 0.47, reg_lambda = 0.86,\n                             subsample = 0.52, random_state =17))\n                             ])\n\nfrom sklearn.model_selection import cross_val_score\nscores = -1 * cross_val_score(my_pipeline, X_train, y_train,\n                              cv=5,\n                              scoring='neg_mean_absolute_error')\nprint(\"Average MAE score (across experiments):\")\nprint(scores.mean())","6a6e4817":"model_xgb3 = XGBRegressor(colsample_bytree = 0.46, gamma = 0.047, \n                             learning_rate = 0.05, max_depth = 3, \n                             min_child_weight= 1.8, n_estimators = 2200,\n                             reg_alpha = 0.47, reg_lambda = 0.86,\n                             subsample = 0.52, random_state =17)\nmodel_xgb3.fit(X_train, y_train)","e42d6d07":"predictor_cols = X_train.columns\ntest = test[predictor_cols]\nxgb3_finalpred = model_xgb3.predict(test)\nprint(xgb3_finalpred)\nmy_submission3 = pd.DataFrame({'Id': test_ID, 'SalePrice': xgb3_finalpred})\nmy_submission3.to_csv('xgb3_pred.csv', index=False)","5b37f03e":"*Apply XGB_1 to test dataset to make a submission*","46e1982f":"![](http:\/\/www.aspbologna.it\/images\/Housing.jpg)","f59fc45b":"**Models**","8b9f7bb4":"****First simple test with minimal preprocessing****","b9ee9865":"**Descriptive Analysis**","2811b760":"Credits:\n- https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning\n- https:\/\/www.kaggle.com\/learn\/data-visualization\n- https:\/\/www.kaggle.com\/dansbecker\/submitting-from-a-kernel\n- https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\n- other notebooks from the competition","5b75e2fc":"**...now do it again with better preprocessing and new ideas!**","5d231ffa":"**Inspect NA values**"}}