{"cell_type":{"77d7979b":"code","05763b95":"code","5444ef13":"code","1752e636":"code","8f5df155":"code","c6c9b274":"code","42061f8a":"code","c239256d":"code","5369db98":"code","bbda38dd":"code","bd07868f":"code","a0ca52da":"code","97d67144":"code","48d1a862":"code","2cd07b64":"code","df5d5667":"code","dbc0f070":"code","cf512e53":"code","d2bb367f":"code","c60a211f":"code","472f8d1f":"code","cda584d5":"code","46facbbe":"code","fb6602ce":"code","2bd29bd8":"code","bf0c1c43":"code","f81749c6":"code","271e6465":"code","acb92190":"code","7040a270":"code","06d8b844":"code","0e697f3c":"code","45760e62":"code","f8e7b539":"code","7ff80436":"code","11574cf5":"code","ad8d1c53":"code","e3d09205":"code","1057a9ea":"code","75924db1":"code","998b3fc7":"code","fef3e342":"code","a2f6b2bf":"code","9c7ff742":"code","a77c6ad1":"code","372f1cb6":"code","f6d01ad5":"code","b26e4b1a":"code","e3efd228":"code","c4980c8f":"code","9ab79a09":"code","d6411fd6":"code","b5de3b4c":"code","f8342167":"code","b2ebc5e9":"code","3794e9ea":"code","323dc36f":"code","d73c2fb9":"code","f6343456":"code","81d13954":"code","57ffdd0c":"code","9d6c3239":"code","79d04790":"code","4d4e813a":"code","22cc9f77":"code","c1047ae0":"code","b8332c65":"code","1b4abda8":"code","043ea508":"code","6db43e2a":"code","fe8c7668":"code","344d75ab":"code","3b48726d":"code","a9465023":"code","0f3e9a5c":"code","1704a2df":"code","77851ec4":"code","3c244611":"code","fc5dc0f8":"code","c2c11b94":"code","587b65fd":"code","e9b9951a":"code","6e019b02":"code","4923aa83":"code","9207bec5":"code","2845f6ca":"markdown","ff567eaa":"markdown","a0169d13":"markdown","5d02bf3e":"markdown","ae4afd14":"markdown","9131a9eb":"markdown","b925ddde":"markdown","6841eeb7":"markdown","1a2c5aa4":"markdown","c9464dc3":"markdown","53a329eb":"markdown","359b4ff5":"markdown","088e6eff":"markdown","40d5c993":"markdown","92fe5a59":"markdown","f1db242d":"markdown","bf982016":"markdown","e8422de0":"markdown","c761f4e4":"markdown","bd642a65":"markdown","12fc0d21":"markdown","dfe715ba":"markdown","62f91723":"markdown","55a15882":"markdown","d113638f":"markdown","f8525225":"markdown","846dc7f1":"markdown","4a160725":"markdown","ebddf89f":"markdown","4b84f3ea":"markdown","3733a007":"markdown","f39f6517":"markdown","96867ce1":"markdown","51da55af":"markdown","d1769793":"markdown","ad61f9ea":"markdown"},"source":{"77d7979b":"import numpy as np # linear algebra\nimport pandas as pd \nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport missingno\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection, metrics, preprocessing\n\n#Machine Learning\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n\n#preprocessing\nfrom sklearn.impute import KNNImputer","05763b95":"df = pd.read_csv('..\/input\/diabetes-dataset\/diabetes2.csv')\ndf.dtypes","5444ef13":"df.head()","1752e636":"df.shape","8f5df155":"df.isnull().sum()","c6c9b274":"df.describe()","42061f8a":"missingno.matrix(df) #nice way of visualizing missing values","c239256d":"plt.figure()\nax = sns.distplot(df['Pregnancies'][df.Outcome == 1], color =\"darkturquoise\", rug = True)\nsns.distplot(df['Pregnancies'][df.Outcome == 0], color =\"lightcoral\",rug = True)\nplt.legend(['Diabetes', 'No Diabetes'])","5369db98":"df['Pregnancies'].value_counts()\n#df['Pregnancies'].unique()","bbda38dd":"sns.boxplot(x = df['Pregnancies'])","bd07868f":"plt.figure()\nax = sns.distplot(df['Glucose'][df.Outcome == 1], color =\"darkturquoise\", rug = True)\nsns.distplot(df['Glucose'][df.Outcome == 0], color =\"lightcoral\", rug = True)\nplt.legend(['Diabetes', 'No Diabetes'])","a0ca52da":"min(df['Glucose']) ","97d67144":"df[df['Glucose'] == 0]","48d1a862":"df[df['Glucose'].lt(60)]","2cd07b64":"df['Glucose'].value_counts() #not significant","df5d5667":"sns.boxplot(x = df['Glucose'])","dbc0f070":"plt.figure()\nax = sns.distplot(df['BloodPressure'][df.Outcome == 1], color =\"darkturquoise\", rug=True)\nsns.distplot(df['BloodPressure'][df.Outcome == 0], color =\"lightcoral\", rug=True)\nplt.legend(['Diabetes', 'No Diabetes'])","cf512e53":"min(df['BloodPressure']) #Again, seems absurd","d2bb367f":"print(df.loc[df['BloodPressure'] == 0].shape[0])\nprint(df.loc[df['BloodPressure'] == 0].shape[0]\/df.shape[0])","c60a211f":"df[df['BloodPressure'].lt(40)]","472f8d1f":"df[df['BloodPressure'].lt(40)].shape","cda584d5":"df[df['BloodPressure'].gt(120)]","46facbbe":"sns.boxplot(x = df['BloodPressure'])","fb6602ce":"plt.figure()\nax = sns.distplot(df['SkinThickness'][df.Outcome == 1], color =\"darkturquoise\", rug=True)\nsns.distplot(df['SkinThickness'][df.Outcome == 0], color =\"lightcoral\", rug=True)\nplt.legend(['Diabetes', 'No Diabetes'])","2bd29bd8":"sns.boxplot(x = df['SkinThickness'])","bf0c1c43":"df[df['SkinThickness'] == 0].shape","f81749c6":"df[df['SkinThickness'].lt(2)]","271e6465":"plt.figure()\nax = sns.distplot(df['Insulin'][df.Outcome == 1], color =\"darkturquoise\", rug=True)\nsns.distplot(df['Insulin'][df.Outcome == 0], color =\"lightcoral\", rug=True)\nplt.legend(['Diabetes', 'No Diabetes'])","acb92190":"sns.boxplot(x = df['Insulin'])","7040a270":"df[df['Insulin'].lt(16)]","06d8b844":"df[df['Insulin'] == 0]","0e697f3c":"df[df['Insulin'] == 0].shape","45760e62":"plt.figure()\nax = sns.distplot(df['BMI'][df.Outcome == 1], color =\"darkturquoise\", rug=True)\nsns.distplot(df['BMI'][df.Outcome == 0], color =\"lightcoral\", rug=True)\nplt.legend(['Diabetes', 'No Diabetes'])","f8e7b539":"sns.boxplot(x = df['BMI'])","7ff80436":"df[df.BMI == 0]","11574cf5":"df[df.BMI == 0].shape","ad8d1c53":"plt.figure()\nax = sns.distplot(df['DiabetesPedigreeFunction'][df.Outcome == 1], color =\"darkturquoise\", rug=True)\nsns.distplot(df['DiabetesPedigreeFunction'][df.Outcome == 0], color =\"lightcoral\", rug=True)\nplt.legend(['Diabetes', 'No Diabetes'])","e3d09205":"sns.boxplot(x = df['DiabetesPedigreeFunction'])","1057a9ea":"plt.figure()\nax = sns.distplot(df['Age'][df.Outcome == 1], color =\"darkturquoise\", rug=True)\nsns.distplot(df['Age'][df.Outcome == 0], color =\"lightcoral\", rug=True)\nsns.distplot(df['Age'], color =\"green\", rug=True)\nplt.legend(['Diabetes', 'No Diabetes', 'all'])","75924db1":"sns.boxplot(x = df['Age'])","998b3fc7":"df_with_na = df.copy(deep = True)\ndf_with_na['Insulin'] = df['Insulin'].map(lambda i: np.nan if i==0 else i)\ndf_with_na['SkinThickness'] = df['SkinThickness'].map(lambda i: np.nan if i==0 else i)\ndf_with_na['BloodPressure'] = df['BloodPressure'].map(lambda i: np.nan if i==0 else i)\ndf_with_na['BMI'] = df['BMI'].map(lambda i: np.nan if i==0 else i)\ndf_with_na['Glucose'] = df['Glucose'].map(lambda i: np.nan if i==0 else i)\n\nmissingno.matrix(df_with_na) #nice way of visualizing missing values","fef3e342":"sns.heatmap(df.corr(), annot = True)","a2f6b2bf":"sns.heatmap(df_with_na.corr(), annot = True)","9c7ff742":"sns.pairplot(df, hue='Outcome')","a77c6ad1":"clf = RandomForestClassifier()\nclf.fit(df.drop('Outcome', axis = 1), df['Outcome'])\nplt.figure()\nimportance = clf.feature_importances_\nprint(df.drop('Outcome', axis=1).columns)\nprint(clf.feature_importances_)\nimportance = pd.DataFrame(importance, index=df.drop('Outcome', axis=1).columns, columns=[\"Importance\"])\nimportance.sort_values(by='Importance', ascending=True).plot(kind='barh', figsize=(20,len(importance)\/2))","372f1cb6":"df_no_BMI0 = df[df.BMI != 0]\n\nclf = RandomForestClassifier()\nclf.fit(df_no_BMI0.drop('Outcome', axis = 1), df_no_BMI0['Outcome'])\nplt.figure()\nimportance = clf.feature_importances_\nprint(df_no_BMI0.drop('Outcome', axis=1).columns)\nprint(clf.feature_importances_)\nimportance = pd.DataFrame(importance, index=df_no_BMI0.drop('Outcome', axis=1).columns, columns=[\"Importance\"])\nimportance.sort_values(by='Importance', ascending=True).plot(kind='barh', figsize=(20,len(importance)\/2))\n","f6d01ad5":"df_no_BP0 = df[df.BloodPressure != 0]\n\nclf = RandomForestClassifier()\nclf.fit(df_no_BP0.drop('Outcome', axis = 1), df_no_BP0['Outcome'])\nplt.figure()\nimportance = clf.feature_importances_\nprint(df_no_BP0.drop('Outcome', axis=1).columns)\nprint(clf.feature_importances_)\nimportance = pd.DataFrame(importance, index=df_no_BP0.drop('Outcome', axis=1).columns, columns=[\"Importance\"])\nimportance.sort_values(by='Importance', ascending=True).plot(kind='barh', figsize=(20,len(importance)\/2))","b26e4b1a":"df_no_IN0 = df[df.Insulin != 0]\n\nclf = RandomForestClassifier()\nclf.fit(df_no_IN0.drop('Outcome', axis = 1), df_no_IN0['Outcome'])\nplt.figure()\nimportance = clf.feature_importances_\nprint(df_no_IN0.drop('Outcome', axis=1).columns)\nprint(clf.feature_importances_)\nimportance = pd.DataFrame(importance, index=df_no_IN0.drop('Outcome', axis=1).columns, columns=[\"Importance\"])\nimportance.sort_values(by='Importance', ascending=True).plot(kind='barh', figsize=(20,len(importance)\/2))","e3efd228":"df_no_SK0 = df[df.SkinThickness != 0]\n\nclf = RandomForestClassifier()\nclf.fit(df_no_SK0.drop('Outcome', axis = 1), df_no_SK0['Outcome'])\nplt.figure()\nimportance = clf.feature_importances_\nprint(df_no_SK0.drop('Outcome', axis=1).columns)\nprint(clf.feature_importances_)\nimportance = pd.DataFrame(importance, index=df_no_SK0.drop('Outcome', axis=1).columns, columns=[\"Importance\"])\nimportance.sort_values(by='Importance', ascending=True).plot(kind='barh', figsize=(20,len(importance)\/2))","c4980c8f":"df_no_missing = df_with_na.dropna()\nprint(df_no_missing.shape)\n\nclf = RandomForestClassifier()\nclf.fit(df_no_missing.drop('Outcome', axis = 1), df_no_missing['Outcome'])\nplt.figure()\nimportance = clf.feature_importances_\nprint(df_no_missing.drop('Outcome', axis=1).columns)\nprint(clf.feature_importances_)\nimportance = pd.DataFrame(importance, index=df_no_missing.drop('Outcome', axis=1).columns, columns=[\"Importance\"])\nimportance.sort_values(by='Importance', ascending=True).plot(kind='barh', figsize=(20,len(importance)\/2))","9ab79a09":"X, X_test, y, y_test = train_test_split(df.drop('Outcome', axis= 1), df['Outcome'], test_size=0.20, random_state=42)\nX, X_test_with_na, y, _ = train_test_split(df_with_na.drop('Outcome', axis= 1), df_with_na['Outcome'], test_size=0.20, random_state=42)","d6411fd6":"df_train = pd.concat([X, y], axis = 1)\ndf_train_without_missing = df_train.dropna()\n\ny_wo_missing_train = df_train_without_missing['Outcome']\nX_wo_missing_train = df_train_without_missing.drop(columns = ['Outcome'])\n\nscaler = preprocessing.StandardScaler().fit(X_wo_missing_train)\n\n#DATASET 1\nX_wo_missing_train = scaler.transform(X_wo_missing_train)\nX_wo_missing_test = scaler.transform(X_test)\nX_wo_missing_test_with_na = scaler.transform(X_test_with_na)\n\nprint(X.shape)\nprint(df_train_without_missing.shape)","b5de3b4c":"imputer = KNNImputer(n_neighbors=5)\nX_knn_imp = imputer.fit_transform(X)\ndf_knn_imp = pd.DataFrame(X_knn_imp, columns = df.drop('Outcome', axis = 1).columns) #DATASET 2\n\nclf = RandomForestClassifier()\nclf.fit(df_knn_imp, y)\nplt.figure()\nimportance = clf.feature_importances_\nprint(df_knn_imp.columns)\nprint(clf.feature_importances_)\nimportance = pd.DataFrame(importance, index=df_knn_imp.columns, columns=[\"Importance\"])\nimportance.sort_values(by='Importance', ascending=True).plot(kind='barh', figsize=(20,len(importance)\/2))\n\nscaler = preprocessing.StandardScaler().fit(df_knn_imp)\nX_knn_imp_train = scaler.transform(df_knn_imp)\nX_knn_imp_test = scaler.transform(X_test)","f8342167":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\nimp = IterativeImputer(max_iter = 10, random_state = 42)\nimp.fit(X)\nX_iter_imp = imp.transform(X)\nX_iter_imp = pd.DataFrame(X_iter_imp, columns = X.columns)\n\n\nclf = RandomForestClassifier()\nclf.fit(X_iter_imp, y)\nplt.figure()\nimportance = clf.feature_importances_\nprint(X_iter_imp.columns)\nprint(clf.feature_importances_)\nimportance = pd.DataFrame(importance, index=X_iter_imp.columns, columns=[\"Importance\"])\nimportance.sort_values(by='Importance', ascending=True).plot(kind='barh', figsize=(20,len(importance)\/2))\n\nscaler = preprocessing.StandardScaler().fit(X_iter_imp)\nX_iter_imp_train = scaler.transform(X_iter_imp)\nX_iter_imp_test = scaler.transform(X_test)","b2ebc5e9":"# \ndef fit_ml_algo(algo, X_train, y_train, X_test, y_test, cv):\n    model = algo.fit(X_train, y_train)\n    test_prediction = model.predict(X_test)\n    test_probs = model.predict_proba(X_test)[:,1]\n    train_accuracy = model.score(X_train, y_train)*100\n    test_accuracy = model.score(X_test, y_test)*100\n    train_prediction = model_selection.cross_val_predict(algo, X_train, y_train, cv = 10, n_jobs = -1)\n    acc_cv = metrics.accuracy_score(y_train, train_prediction)*100\n    model_scores = model_selection.cross_val_score(LogisticRegression(), X_train, y_train, cv = 10, n_jobs = -1)\n\n    print(\"Cross Validation accuracy: (%0.2f) %0.4f (+\/- %0.4f)\" % (acc_cv, model_scores.mean(), model_scores.std() * 2))\n    print('Model Test Accuracy: %0.2f   Model Train Accuracy: %0.2f'%(test_accuracy, train_accuracy))\n    print(metrics.classification_report(y_test, test_prediction))\n    print(\"Confusion matrix\")\n    print(metrics.confusion_matrix(y_test,test_prediction))\n    \n    return train_prediction, test_prediction, test_probs\n    \n    \n# calculate the fpr and tpr for all thresholds of the classification\ndef plot_roc_curve(y_test, preds):\n    fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n    roc_auc = metrics.auc(fpr, tpr)\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([-0.01, 1.01])\n    plt.ylim([-0.01, 1.01])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()\n    \n    \n# Adding gridsearch report creating code\ndef report(results, n_top = 5):\n    for i in range(1, n_top +1 ):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                  results['mean_test_score'][candidate],\n                  results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")","3794e9ea":"c = [0.01, 0.1, 1, 5, 10]\nparam_grid = [{'C': c, 'penalty': ['l2'], 'solver': ['liblinear', 'newton-cg','saga','lbfgs']}, {'C': c, 'penalty': ['l1'], 'solver': ['liblinear', 'saga']}]\n\nlg_grid = model_selection.GridSearchCV(LogisticRegression(), param_grid, n_jobs = -1)\nlg_grid.fit(X_train_wo_msng, y_train_wo_msng)\n\nreport(lg_grid.cv_results_)","323dc36f":"train_prediction_1, test_prediction_1, test_probs_1 = fit_ml_algo(LogisticRegression(C = 0.1, penalty = 'l1', solver = 'liblinear'), X_train_without_missing_tf, y_train_without_missing, X_test_tf, y_test, cv = 10)","d73c2fb9":"plot_roc_curve(y_test, test_probs_1)","f6343456":"c = [0.01, 0.1, 1, 5, 10]\nparam_grid = [{'C': c, 'penalty': ['l2'], 'solver': ['liblinear', 'newton-cg','saga','lbfgs']}, {'C': c, 'penalty': ['l1'], 'solver': ['liblinear', 'saga']}]\n\nlg_grid = model_selection.GridSearchCV(LogisticRegression(), param_grid, n_jobs = -1)\nlg_grid.fit(X_knn_imp_train, y)\n\nreport(lg_grid.cv_results_)","81d13954":"train_prediction_2, test_prediction_2, test_probs_2 = fit_ml_algo(LogisticRegression(C = 0.1, penalty = 'l1', solver = 'liblinear'), X_knn_imp_train, y, X_knn_imp_test, y_test, cv = 10)","57ffdd0c":"plot_roc_curve(y_test, test_probs_2)","9d6c3239":"c = [0.01, 0.1, 1, 5, 10]\nparam_grid = [{'C': c, 'penalty': ['l2'], 'solver': ['liblinear', 'newton-cg','saga','lbfgs']}, {'C': c, 'penalty': ['l1'], 'solver': ['liblinear', 'saga']}]\n\nlg_grid = model_selection.GridSearchCV(LogisticRegression(), param_grid, n_jobs = -1)\nlg_grid.fit(X_iter_imp_train, y)\n\nreport(lg_grid.cv_results_)","79d04790":"train_prediction_3, test_prediction_3, test_probs_3 = fit_ml_algo(LogisticRegression(C = 0.1, penalty = 'l1', solver = 'saga'), X_iter_imp_train, y, X_iter_imp_test, y_test, cv = 10)","4d4e813a":"plot_roc_curve(y_test, test_probs_3)","22cc9f77":"#gridsearch params\nc = [0.01, 0.1, 1, 5, 10]\nparam_grid = [{'C': c, 'kernel': ['linear', 'poly','rbf','sigmoid']}]","c1047ae0":"svc_grid = model_selection.GridSearchCV(SVC(), param_grid, n_jobs = -1)\nsvc_grid.fit(X_train_wo_msng, y_train_wo_msng)\nreport(svc_grid.cv_results_)","b8332c65":"train_prediction, test_prediction, test_probs = fit_ml_algo(SVC(C =5, kernel = 'linear', probability = True), X_train_wo_msng, y_train_wo_msng, X_test_tf, y_test, cv = 10)","1b4abda8":"plot_roc_curve(y_test, test_probs)","043ea508":"svc_grid = model_selection.GridSearchCV(SVC(), param_grid, n_jobs = -1)\nsvc_grid.fit(X_knn_imp_train, y)\nreport(svc_grid.cv_results_)","6db43e2a":"train_prediction, test_prediction, test_probs = fit_ml_algo(SVC(C = 0.01, kernel = 'linear', probability = True), X_knn_imp_train, y, X_knn_imp_test, y_test, cv = 10)","fe8c7668":"plot_roc_curve(y_test, test_probs)","344d75ab":"svc_grid = model_selection.GridSearchCV(SVC(), param_grid, n_jobs = -1)\nsvc_grid.fit(X_iter_imp_train, y)\nreport(svc_grid.cv_results_)","3b48726d":"train_prediction, test_prediction, test_probs = fit_ml_algo(SVC(C = 0.01, kernel = 'linear', probability = True), X_iter_imp_train, y, X_iter_imp_test, y_test, cv = 10)","a9465023":"plot_roc_curve(y_test, test_probs)","0f3e9a5c":"params = {'min_child_weight': [1, 5, 10], 'gamma': [0.5, 1, 1.5, 2, 5], 'subsample': [0.6, 0.8, 1.0], 'colsample_bytree': [0.6, 0.8, 1.0], 'max_depth': [3, 4, 5]}\nestimator = XGBClassifier(objective= 'binary:logistic', nthread=4, seed=42)\nxg_grid = model_selection.GridSearchCV(estimator=estimator, param_grid=params, scoring = 'roc_auc', n_jobs = 10, cv = 10, verbose=True)","1704a2df":"xg_grid.fit(X_wo_missing_train, y_wo_missing_train)","77851ec4":"best_estimator = xg_grid.best_estimator_\nreport(xg_grid.cv_results_)","3c244611":"train_prediction, test_prediction, test_probs = fit_ml_algo(best_estimator, X_wo_missing_train, y_wo_missing_train, X_wo_missing_test_with_na, y_test, cv = 10)","fc5dc0f8":"plot_roc_curve(y_test, test_probs)","c2c11b94":"xg_grid.fit(X_knn_imp_train, y)","587b65fd":"best_estimator = xg_grid.best_estimator_\nreport(xg_grid.cv_results_)","e9b9951a":"train_prediction, test_prediction, test_probs = fit_ml_algo(best_estimator, X_knn_imp_train, y, X_knn_imp_test, y_test, cv = 10)","6e019b02":"xg_grid.fit(X_iter_imp_train, y)","4923aa83":"best_estimator = xg_grid.best_estimator_\nreport(xg_grid.cv_results_)","9207bec5":"train_prediction, test_prediction, test_probs = fit_ml_algo(best_estimator, X_iter_imp_train, y, X_iter_imp_test, y_test, cv = 10)","2845f6ca":"Let's look at the insulin data (370 missing)","ff567eaa":"## DATASET 3","a0169d13":"### SVM: Dataset 1","5d02bf3e":"Distplot shows both the histogram and the KDE together. From the graph, number of pregnancies show some distinguishability(?) ","ae4afd14":"Since 35 rows have Blood pressure = 0, it would not be wise to drop them \n\n# Feature: SkinThickness","9131a9eb":"## XGBoost\n\n1. Documentation: https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#module-xgboost.sklearn\n\n2. Hyperparameter grid search in XGBoost: https:\/\/www.kaggle.com\/tilii7\/hyperparameter-grid-search-with-xgboost\n\n3. ","b925ddde":"We saw that there are many missing values in the columns, but they have been imputed with zero. Let's remove them and visualize ","6841eeb7":"## Dataset 1: data without missing values","1a2c5aa4":"It can be seen that there's high correlation between BMI and SkinThickness. Since Skinthickness has many missing values, maybe we can drop it. \n\n# Bivariate Analysis","c9464dc3":"Remains more or less the same. Let's check it with Blood pressure (35 zero values)","53a329eb":"## Dataset 3","359b4ff5":"Normal Glucose should be between 60 to 140, 0 seems absurd","088e6eff":"No two features have very good distinguishing property\n\n# Feature importance analysis\n\nWe could see there are many missing values for many columns. Let's see how we can handle it[](http:\/\/)","40d5c993":"# Feature: DiabetesPedigreeFunction","92fe5a59":"## DATASET 2","f1db242d":"# Objective: Classify people with diabetes or not using  different features","bf982016":"# Feature: Glucose","e8422de0":"We can't drop 227 rows as well\n\n# Feature: Insulin","c761f4e4":"Dropping the 0 values show significant increase in the feature importance of Insulin. Shows we need to handle it somehow","bd642a65":"### XGBoost: Dataset 1","12fc0d21":"So we see after removing the missing values, the feature importance list looks like this. Glucose, insulin and age seem to be the three most important features.\n\n## Data preprocessing:\n\nWe saw missing values in the following columns:\n1. Glucose: 5\n2. BMI: 11\n3. Insulin: 370\n4. SkinThickness: 227\n5. Blood Pressure: 35\n\nSteps:\n1. Remove the missing values of Glucose and BMI (both very important features) [DATASET 1]\n2. Drop SkinThickness because the feature importance is low [DATASET 2]\n3. Create a train test split (test split should not contain any missing value)\n4. Use data imputation \n\nReferences:\n1. General overview of popular methods: https:\/\/towardsdatascience.com\/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779\n> > \n2. Imputation using KNN: https:\/\/datascienceplus.com\/knnimputer-for-missing-value-imputation-in-python-using-scikit-learn\/\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.KNNImputer.html\n\n\n\n## DATASET 1","dfe715ba":"# Feature: Age","62f91723":"### XGBoost: Dataset 3","55a15882":"### SVM: Dataset 3","d113638f":"# **Feature: Pregnancies**","f8525225":"We know BMI has 11 zero values. Let's see if the feature importance increases if we remove those values","846dc7f1":"Changes the importances of other features, but skin thickness feature importance remains low","4a160725":"### XGBoost: Dataset 2","ebddf89f":"## Exploratory Data Analysis\n\nReferences:\n1. https:\/\/github.com\/dformoso\/sklearn-classification\/blob\/master\/Data%20Science%20Workbook%20-%20Census%20Income%20Dataset.ipynb\n2. https:\/\/github.com\/mrdbourke\/your-first-kaggle-submission\/blob\/master\/kaggle-titanic-dataset-example-submission-workflow.ipynb\nAssociated video: https:\/\/www.youtube.com\/watch?v=f1y9wDDxWnA&feature=youtu.be","4b84f3ea":"## SVM\n\n1. Documentation:\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html#sklearn.svm.SVC\n\n2. ROC for SVC:\nhttps:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_roc.html","3733a007":"## Dataset 2: ","f39f6517":"### SVM: Dataset 2","96867ce1":"Since only five rows have Glucose 0, it can be removed\n\n# Feature: BloodPressure","51da55af":"# Model training: \n\nSteps: \n1. Choose dataset\n2. Scale train and test data\n3. Do gridsearch (if applicable) for best hyperparamters\n\nReferences: \n1. Scaling: https:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html\n2. Cross validation: \nhttps:\/\/stats.stackexchange.com\/questions\/411290\/how-to-use-a-cross-validated-model-for-prediction\nhttps:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html\n3. Grid Search:\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV\n4. Randomized Search vs Grid Search: \n    https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_randomized_search.html\n    https:\/\/stackoverflow.com\/questions\/57426633\/what-is-randomsearchcv-and-gridsearchcv\n5. ROC Curves:\nhttps:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_roc.html\n\n6. Logistic regression:\n    a. Documentation : https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html\n    b. Which solver to use: \n    https:\/\/towardsdatascience.com\/dont-sweat-the-solver-stuff-aea7cddc3451\n    https:\/\/stackoverflow.com\/questions\/38640109\/logistic-regression-python-solvers-defintions\/52388406#52388406\n\n## Utils:","d1769793":"Shows that there are no null values in the ","ad61f9ea":"# Feature: BMI"}}