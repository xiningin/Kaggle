{"cell_type":{"7fe99277":"code","0e567148":"code","a00ed745":"code","6c12e095":"code","5efd2f09":"code","ed82dfcc":"code","5a4c29a7":"code","17fc86b0":"code","f2a277a2":"code","15169547":"code","11ada53e":"code","084612af":"code","81286dc5":"code","486d5bdf":"code","9fb50905":"code","c061a05c":"code","ba15c0e9":"code","0efba2a4":"code","83426f7a":"code","199e7b71":"code","0a9991d8":"code","4a423fb0":"code","948245a9":"code","047ed0f4":"code","c9c0aa97":"code","5413d87b":"code","e3a8cbac":"code","376be780":"code","ea86e64f":"code","1062baa6":"code","64ebdbc3":"code","e3b05210":"code","6ec58f1f":"code","28ffd48b":"code","a6404cc9":"code","29a28fa0":"code","8c88dec1":"code","0a238426":"code","94ed0cac":"code","6f33ded9":"code","a865273a":"code","5aa42376":"code","7135c106":"code","fd14c30b":"code","d32e3ca9":"code","865399b4":"code","774d5218":"code","34c84ef4":"code","654aea83":"code","31eb5d2d":"code","ad365563":"code","e9ac5daa":"code","56ff886d":"code","137a42e4":"code","35ef5b48":"code","1e3873f5":"code","58779d3d":"code","750957a3":"code","7f07c2e0":"code","c61062b3":"code","1a6c8d7c":"code","81bb0683":"code","0e68eeee":"markdown","54b6c257":"markdown","b4107f48":"markdown","ea5042e6":"markdown","7f483e42":"markdown","b8ff2b33":"markdown","5f9cdcf0":"markdown","e57ccba4":"markdown","ac5931da":"markdown","b0bb1ddd":"markdown","9dc8fe2c":"markdown","308178c6":"markdown","aa7cba59":"markdown","25cd04af":"markdown","89c13538":"markdown","5898b30d":"markdown","9e524fe4":"markdown","7c1f9d80":"markdown","8b4f9395":"markdown","d97f88bb":"markdown","4933becf":"markdown","c4c1d67c":"markdown","8ae493dd":"markdown","1318da4e":"markdown","6ce38480":"markdown","5b2d8b5b":"markdown","4a518552":"markdown"},"source":{"7fe99277":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0e567148":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\nimport os","a00ed745":"df0 = pd.read_csv(\"\/kaggle\/input\/predict-the-disease\/Training.csv\")\nprint(\"Dataset with rows {} and columns {}\".format(df0.shape[0],df0.shape[1]))\ndf0.head()","6c12e095":"df1 = pd.read_csv(\"\/kaggle\/input\/predict-the-disease\/Testing.csv\")\nprint(\"Dataset with rows {} and columns {}\".format(df1.shape[0],df1.shape[1]))\ndf1.head()","5efd2f09":"df0.describe()","ed82dfcc":"df0.isnull().sum()","5a4c29a7":"# Training data\ndf0.prognosis.value_counts()","17fc86b0":"# Testing data\ndf1.prognosis.value_counts()","f2a277a2":"df0.prognosis.value_counts(normalize=True)","15169547":"df1.prognosis.value_counts(normalize=True)","11ada53e":"plt.figure(figsize=(18,5))\nsns.countplot('prognosis',data=df0)\nplt.show()","084612af":"X= df0.drop([\"prognosis\"],axis=1)\ny = df0['prognosis']","81286dc5":"X1= df1.drop([\"prognosis\"],axis=1)\ny1 = df1['prognosis']","486d5bdf":"plt.figure(figsize=(20,20))\nsns.heatmap(X.corr(),annot=True)","9fb50905":"# initializing the pca\nfrom sklearn import decomposition\npca = decomposition.PCA()","c061a05c":"# configuring the parameteres\n# the number of components = 2\npca.n_components = 2\npca_data = pca.fit_transform(X)\n\n# pca_reduced will contain the 2-d projects of simple data\nprint(\"shape of pca_reduced.shape = \", pca_data.shape)","ba15c0e9":"# attaching the label for each 2-d data point \npca_data = np.vstack((pca_data.T, y)).T\n\n# creating a new data fram which help us in ploting the result data\npca_df = pd.DataFrame(data=pca_data, columns=(\"1st_principal\", \"2nd_principal\", \"prognosis\"))\nsns.FacetGrid(pca_df, hue=\"prognosis\", size=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()\nplt.show()","0efba2a4":"# PCA for dimensionality redcution (non-visualization)\n\npca.n_components = 132\npca_data = pca.fit_transform(X)\n\npercentage_var_explained = pca.explained_variance_ \/ np.sum(pca.explained_variance_);\n\ncum_var_explained = np.cumsum(percentage_var_explained)\n\n# Plot the PCA spectrum\nplt.figure(1, figsize=(6, 4))\n\nplt.clf()\nplt.plot(cum_var_explained, linewidth=2)\nplt.axis('tight')\nplt.grid()\nplt.xlabel('n_components')\nplt.ylabel('Cumulative_explained_variance')\nplt.show()\n\n","83426f7a":"# data prepararion\nfrom wordcloud import WordCloud \nx2011 = df0.prognosis\nplt.subplots(figsize=(8,8))\nwordcloud = WordCloud(\n                          background_color='white',\n                          width=800,\n                          height=800\n                         ).generate(\" \".join(x2011))\nplt.title('Diseases',size=30)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","199e7b71":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression","0a9991d8":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\nfrom sklearn.model_selection import cross_val_score","4a423fb0":"train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.33, random_state=42)","948245a9":"rfc_mod = RandomForestClassifier(random_state=42, class_weight='balanced').fit(train_X, train_y)","047ed0f4":"y_pred_rfc = rfc_mod.predict(val_X)\ny_pred_rfc","c9c0aa97":"y_pred_rfc1 = rfc_mod.predict(X1)\ny_pred_rfc1","5413d87b":"print(\"Accuracy Score:\", accuracy_score(y_pred_rfc, val_y))\nprint('cross validation:',cross_val_score(rfc_mod, X, y, cv=3).mean())\nprint(\"F1 Score :\",f1_score(y_pred_rfc,val_y,average = \"weighted\"))\nprint('Report:\\n',classification_report(val_y, y_pred_rfc))\nprint('Confusion Matrix: \\n',confusion_matrix(val_y, y_pred_rfc))","e3a8cbac":"print(\"Accuracy Score:\", accuracy_score(y_pred_rfc1, y1))\nprint('cross validation:',cross_val_score(rfc_mod, X, y, cv=3).mean())\nprint(\"F1 Score :\",f1_score(y_pred_rfc,val_y,average = \"weighted\"))\nprint('Report:\\n',classification_report(val_y, y_pred_rfc))\nprint('Confusion Matrix: \\n',confusion_matrix(val_y, y_pred_rfc))","376be780":"importances=rfc_mod.feature_importances_\nfeature_importances=pd.Series(importances, index=train_X.columns).sort_values(ascending=False)\nplt.figure(figsize=(10,7))\nsns.barplot(x=feature_importances[0:20], y=feature_importances.index[0:20])\nplt.title('Feature Importance RFC Model',size=20)\nplt.ylabel(\"Features\")\nplt.show()","ea86e64f":"from sklearn.feature_selection import RFE","1062baa6":"rfe = RFE(rfc_mod, 20)\nrfe.fit(train_X,train_y)","64ebdbc3":"train_X.columns[rfe.support_]","e3b05210":"colm = train_X.columns[rfe.support_]","6ec58f1f":"rfc_mod.fit(train_X[colm],train_y)","28ffd48b":"y_pred_rfc2 = rfc_mod.predict(val_X[colm])\ny_pred_rfc2","a6404cc9":"print(\"Accuracy Score:\", accuracy_score(y_pred_rfc2, val_y))\nprint(\"F1 Score :\",f1_score(y_pred_rfc2,val_y,average = \"weighted\"))\nprint('Report:\\n',classification_report(val_y, y_pred_rfc2))\nprint('Confusion Matrix: \\n',confusion_matrix(val_y, y_pred_rfc2))","29a28fa0":"dectre_mod = DecisionTreeClassifier(random_state=42, class_weight='balanced').fit(train_X,train_y)","8c88dec1":"y_pred_dectre = dectre_mod.predict(val_X)\ny_pred_dectre","0a238426":"y_pred_dectre1 = dectre_mod.predict(X1)\ny_pred_dectre1","94ed0cac":"print(\"Accuracy Score:\", accuracy_score(y_pred_dectre, val_y))\nprint(\"F1 Score :\",f1_score(y_pred_dectre,val_y,average = \"weighted\"))\nprint('Report:\\n',classification_report(val_y, y_pred_dectre))\nprint('Confusion Matrix: \\n',confusion_matrix(val_y, y_pred_dectre))","6f33ded9":"print(\"Accuracy Score:\", accuracy_score(y_pred_dectre1, y1))\nprint(\"F1 Score :\",f1_score(y_pred_dectre1,y1,average = \"weighted\"))\nprint('Report:\\n',classification_report(y1, y_pred_dectre1))\nprint('Confusion Matrix: \\n',confusion_matrix(y1, y_pred_dectre1))","a865273a":"importances=dectre_mod.feature_importances_\nfeature_importances=pd.Series(importances, index=train_X.columns).sort_values(ascending=False)\nplt.figure(figsize=(10,7))\nsns.barplot(x=feature_importances[0:20], y=feature_importances.index[0:20])\nplt.title('Feature Importance Decision Tree Model',size=20)\nplt.ylabel(\"Features\")\nplt.show()","5aa42376":"rfe = RFE(dectre_mod, 20)\nrfe.fit(train_X,train_y)","7135c106":"train_X.columns[rfe.support_]","fd14c30b":"col = train_X.columns[rfe.support_]","d32e3ca9":"dectre_mod.fit(train_X[col],train_y)","865399b4":"y_pred_dectre2 = dectre_mod.predict(val_X[col])\ny_pred_dectre2","774d5218":"print(\"Accuracy Score:\", accuracy_score(y_pred_dectre2, val_y))\nprint(\"F1 Score :\",f1_score(y_pred_dectre2,val_y,average = \"weighted\"))\nprint('Report:\\n',classification_report(val_y, y_pred_dectre2))\nprint('Confusion Matrix: \\n',confusion_matrix(val_y, y_pred_dectre2))","34c84ef4":"logreg_mod = LogisticRegression(random_state=42, solver='lbfgs', class_weight='balanced').fit(train_X,train_y)","654aea83":"y_pred_logreg = logreg_mod.predict(val_X)\ny_pred_logreg","31eb5d2d":"y_pred_logreg1 = logreg_mod.predict(X1)\ny_pred_logreg1","ad365563":"print(\"Accuracy Score:\", accuracy_score(y_pred_logreg, val_y))\nprint(\"F1 Score :\",f1_score(y_pred_logreg,val_y,average = \"weighted\"))\nprint('Report:\\n',classification_report(val_y, y_pred_logreg))\nprint('Confusion Matrix: \\n',confusion_matrix(val_y, y_pred_logreg))","e9ac5daa":"print(\"Accuracy Score:\", accuracy_score(y_pred_logreg1, y1))\nprint(\"F1 Score :\",f1_score(y_pred_logreg1,y1,average = \"weighted\"))\nprint('Report:\\n',classification_report(y1, y_pred_logreg1))\nprint('Confusion Matrix: \\n',confusion_matrix(y1, y_pred_logreg1))","56ff886d":"rfe = RFE(logreg_mod, 20)\nrfe.fit(train_X,train_y)","137a42e4":"train_X.columns[rfe.support_]","35ef5b48":"cols = train_X.columns[rfe.support_]","1e3873f5":"logreg_mod.fit(train_X[cols],train_y)","58779d3d":"y_pred_logreg2 = logreg_mod.predict(val_X[cols])\ny_pred_logreg2","750957a3":"for i in y_pred_logreg2:\n    print(i)","7f07c2e0":"print(\"Accuracy Score:\", accuracy_score(y_pred_logreg2, val_y))\nprint(\"F1 Score :\",f1_score(y_pred_logreg2,val_y,average = \"weighted\"))\nprint('Report:\\n',classification_report(val_y, y_pred_logreg2))\nprint('Confusion Matrix: \\n',confusion_matrix(val_y, y_pred_logreg2))","c61062b3":"y_pred_logreg3 = logreg_mod.predict(X1[cols])\ny_pred_logreg3","1a6c8d7c":"for i in y_pred_logreg3:\n    print(i)","81bb0683":"print(\"Accuracy Score:\", accuracy_score(y_pred_logreg3, y1))\nprint(\"F1 Score :\",f1_score(y_pred_logreg3,y1,average = \"weighted\"))\nprint('Report:\\n',classification_report(y1, y_pred_logreg3))\nprint('Confusion Matrix: \\n',confusion_matrix(y1, y_pred_logreg3))","0e68eeee":"### Decision Tree Classifier","54b6c257":"#### Hence, there are no missing value.","b4107f48":"## PCA","ea5042e6":"## Using Recursive Feature Elimination","7f483e42":"### Thus, the Training dataset is balanced with respect to Target variable: Prognosis.","b8ff2b33":"## Reading the CSV file","5f9cdcf0":"### PCA for Data visualization","e57ccba4":"### Feature importance according to Random Forest Classifier","ac5931da":"## Table of Content:\n   * [Introduction](#intro)\n   * [Import Packages](#importpackages)    \n   * [Reading the CSV file](#readcsv)   \n   * [Exploratory Data Analysis](#eda)    \n   * [Data Visualization](#dv)  \n       * [PCA](#pca)   \n           *    [PCA for Data visualization](#pcadv)\n           *    [PCA for dimensionality reduction](#pcadr) \n   * [Import Metrics](#importmetric)\n   * [Model Building](#model) \n       * [Random Forest Classifier](#rfc)\n            * [Feature importance according to Random Forest Classifier](#featurerfc)   \n            * [Using Recursive Feature Elimination](#rferfc) \n       * [Decision Tree Classifier](#dtc) \n            * [Feature importance according to Decision Tree Classifier](#featuredtc)   \n            * [Using Recursive Feature Elimination](#rfedtc) \n       * [Logistic Regression](#lr)   \n            * [Using Recursive Feature Elimination](#rfelr) \n   * [Conclusion](#conclusion)             ","b0bb1ddd":"### Random Forest Classifier","9dc8fe2c":"#### From this we can easily classify the disease using PCA.","308178c6":"### Import Metrics","aa7cba59":"## Exploratory Data Analysis","25cd04af":"## Using Recursive Feature Elimination","89c13538":"## Feature importance according to Decision Tree Classifier","5898b30d":"### Distribution of TARGET variable : PROGNOSIS ie. total number of disease of each type","9e524fe4":"## Conclusion:\n* Logistic regression with one vs rest gives the best accuracy in both dataset ie Validation data and Testing Data.\n* Feature importance obtained from Feature importance according to Random Forest classifier is alomst same as features obtained from logistic regression using RFE.","7c1f9d80":"### Logistic Regression","8b4f9395":"## Import Packages","d97f88bb":"## Introduction:\n### The task is to predict the disease with high accuracy rate with the help of list of symptoms provided by patient.\n","4933becf":"## Data Visualization","c4c1d67c":"## Using Recursive Feature Elimination","8ae493dd":"## Model Building","1318da4e":"### PCA for dimensionality reduction","6ce38480":"### Also, the Testing dataset is balanced too with respect to Target variable:Prognosis","5b2d8b5b":"<img src=\"http:\/\/imgur.com\/mLsI6kc.jpg\" width=\"500\">","4a518552":"#### Although explained Variance is not a good metric for classification problem but it gives fair-enough idea about dataset."}}