{"cell_type":{"a19b08dc":"code","17cf7677":"code","f968e291":"code","f4b2bef4":"code","53275aa3":"code","f6937225":"code","11f535d1":"code","4057cbc1":"code","200f3dca":"code","8384fb13":"code","6a61ec9e":"code","8add32f5":"code","140f4964":"code","513ba8d4":"code","bdab3647":"code","1acec93e":"code","80ec969f":"code","43b31b3f":"code","f48ef3f8":"code","1440b47d":"code","5048deed":"code","12c4b323":"code","3db7badc":"code","a1acec7b":"code","1ca4ad95":"code","594c4893":"code","2278a823":"code","633b07dd":"code","4d2448af":"code","52c12f90":"code","00402168":"code","1e31ddeb":"code","e302c7d4":"code","560b136d":"code","0ec6602d":"markdown","169ca835":"markdown"},"source":{"a19b08dc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nfrom collections import defaultdict\nimport warnings\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline as imbpipeline\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Input, Dropout\nfrom tensorflow.data import AUTOTUNE\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","17cf7677":"RANDOM_SEED = 2\nwarnings.filterwarnings('ignore')","f968e291":"df = pd.read_csv('\/kaggle\/input\/cirrhosis-prediction-dataset\/cirrhosis.csv')\ndf.head()","f4b2bef4":"df.info()","53275aa3":"plt.figure(figsize = (20, 16))\nsns.heatmap(df.isnull())","f6937225":"df = df.drop(df[df.Stage.isnull()].index)\ndf.describe().transpose()","11f535d1":"for column in df.columns:\n    print(column, '---->', df[column].dtype)","4057cbc1":"stage_corr = df.corr()['Stage'][:-1]\nnumerical = stage_corr.loc[abs(stage_corr) > 0.15].index\nstage_corr","200f3dca":"fig, axes = plt.subplots(3, 3, figsize = (10, 10))\naxes = axes.flatten()\nshort_df = df.iloc[: 305]\nfor i, column in enumerate(df.loc[:, :'Stage'].select_dtypes('O')):\n    data = short_df[column].groupby(df['Stage']).value_counts(normalize = True).rename('proportion').to_frame().reset_index()\n    sns.barplot(x = column, y = 'proportion', hue = 'Stage', data = data, ax = axes[i])","8384fb13":"cat = ['Status', 'Drug', 'Hepatomegaly', 'Spiders', 'Edema']\ndf = df[[*numerical,*cat, 'Stage']]\ndf","6a61ec9e":"pipe = imbpipeline([('imputer', SimpleImputer(strategy = 'median')), ('scaler', StandardScaler())])\nmodels = [DecisionTreeClassifier, RandomForestClassifier, XGBClassifier]\nmodel_names = ['Decision Tree', 'Random Forest', 'XGB']","8add32f5":"def fit_model(train_indices, val_indices, model):\n    X_train = X.iloc[train_indices]\n    X_val = X.iloc[val_indices]\n    y_train = y.iloc[train_indices]\n    y_val = y.iloc[val_indices]\n    if model == XGBClassifier:\n            mod = model(use_label_encoder = False)\n    else:\n        mod = model()\n    mod.fit(X_train, y_train)\n    return X_val, y_val, mod","140f4964":"X = df[[*numerical, *cat]]\ny = df['Stage']\nfor column in cat:\n    if X[column].isnull().sum() != 0:\n        X[column].iloc[:312] = LabelEncoder().fit_transform(X[column].iloc[:312])\n    else:\n        X[column] = LabelEncoder().fit_transform(X[column])\nknn = KNNImputer(missing_values = np.nan, n_neighbors = 1)\nX[cat] = KNNImputer(missing_values = np.nan, n_neighbors = 1).fit_transform(X[cat])\nX[numerical] = pipe.fit_transform(X[numerical])","513ba8d4":"y = y.astype('int')\ny = y - 1\ny.value_counts()","bdab3647":"skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = RANDOM_SEED)\nscore_dic = defaultdict(list)\n\nfor train, val in skf.split(X, y):\n    for i, model in enumerate(models):\n        X_val, y_val, mod = fit_model(train, val,model)\n        preds = mod.predict_proba(X_val)\n        t = roc_auc_score(y_true = y_val, y_score = preds, multi_class = 'ovr')\n        score_dic[model_names[i] + 'ovr'].append(t)","1acec93e":"score_dic","80ec969f":"smote = SMOTE(k_neighbors = 3)\nX_train, X_val, y_train, y_val = train_test_split(X, y, train_size = 0.8)\nX_copy, y_copy = smote.fit_resample(X_train, y_train)\n\nX_val_copy, y_val_copy = smote.fit_resample(X_val, y_val)\nforest = RandomForestClassifier()\n\nparam_grid = [{'n_estimators': [3, 10, 30, 50, 75, 100], 'criterion': ['gini', 'entropy'], 'max_depth': [1, 2, 3,4, 6, 8, 10, 20], },]\ngrid_search = GridSearchCV(forest, param_grid, cv = 5, scoring = 'roc_auc_ovr', return_train_score = True)\ngrid_search.fit(X_copy, y_copy)","43b31b3f":"grid_search.best_params_, grid_search.best_score_","f48ef3f8":"grid_search.score(X_val_copy, y_val_copy) #Seems like I'm overfitting?","1440b47d":"xgb = XGBClassifier(use_label_encoder = False, eval_metric = 'mlogloss')\nparam_grid_xgb = [{'eta' : [0.005, 0.05, 0.1, 0.3, 0.5], 'max_depth' : [2, 4, 6, 8, 10], 'lambda': [0.25, 0.5, 1, 1.5, 2]}]\ngrid_search_xgb = GridSearchCV(xgb, param_grid_xgb, cv = 5, scoring = 'roc_auc_ovr', return_train_score = True)\ngrid_search_xgb.fit(X_copy, y_copy)","5048deed":"grid_search_xgb.best_params_, grid_search_xgb.best_score_","12c4b323":"grid_search_xgb.score(X_val_copy, y_val_copy)","3db7badc":"X = X[numerical]\nsmote = SMOTE(k_neighbors = 3)\nX_train, X_val, y_train, y_val = train_test_split(X, y, train_size = 0.7)\nX_copy, y_copy = smote.fit_resample(X_train, y_train)\n\nX_val_copy, y_val_copy = smote.fit_resample(X_val, y_val)\nforest = RandomForestClassifier()\n\nparam_grid = [{'n_estimators': [3, 10, 30, 50, 75, 100], 'criterion': ['gini', 'entropy'], 'max_depth': [1, 2, 3,4, 6, 8, 10, 20], },]\ngrid_search = GridSearchCV(forest, param_grid, cv = 5, scoring = 'roc_auc_ovr', return_train_score = True)\ngrid_search.fit(X_copy, y_copy)","a1acec7b":"grid_search.best_params_, grid_search.best_score_","1ca4ad95":"grid_search.score(X_val_copy, y_val_copy)","594c4893":"xgb = XGBClassifier(use_label_encoder = False, eval_metric = 'mlogloss')\nparam_grid_xgb = [{'eta' : [0.005, 0.05, 0.1, 0.3, 0.5], 'max_depth' : [2, 4, 6, 8, 10], 'lambda': [0.25, 0.5, 1, 1.5, 2]}]\ngrid_search_xgb = GridSearchCV(xgb, param_grid_xgb, cv = 5, scoring = 'roc_auc_ovr', return_train_score = True)\ngrid_search_xgb.fit(X_copy, y_copy)","2278a823":"grid_search_xgb.score(X_val_copy, y_val_copy)","633b07dd":"forest = RandomForestClassifier(criterion = 'entropy', max_depth = 20, n_estimators = 100)\nforest.fit(X_copy, y_copy)","4d2448af":"df = pd.read_csv('\/kaggle\/input\/cirrhosis-prediction-dataset\/cirrhosis.csv')\nX = df[numerical].loc[(df['Stage'].notnull())]\nX = pipe.transform(X)\ny = df['Stage'].loc[(df['Stage'].notnull())].astype(int)\npreds = forest.predict(X)\npred_proba = forest.predict_proba(X)\npreds += 1\npreds\n","52c12f90":"fig, ax = plt.subplots(figsize = (10, 10))\nconf = confusion_matrix(y, preds)\nconf_norm = conf.astype(float)\/conf.sum(axis = 1)[:, np.newaxis]\nsns.heatmap(conf_norm)\nplt.title('Normalized Confusion Matrix')","00402168":"score = roc_auc_score(y, pred_proba, multi_class = 'ovr')\nscore","1e31ddeb":"sm = 0\nstage = forest.predict(pipe.transform(df[numerical]))\nstage += 1\nlabel = df['ID']\nreturn_df = pd.DataFrame({'label': label, 'stage': stage})\ncompare_df = pd.DataFrame({'Original': df['Stage'], 'Predict': stage})","e302c7d4":"return_df\n","560b136d":"fig, axes = plt.subplots(figsize = (10, 10))\nsns.scatterplot(y = range(len(df)), x = df['Stage'])\nsns.scatterplot(y = range(len(df)), x = stage + .2)","0ec6602d":"My gridsearch best_score during 5 fold cross validation is significantly higher than that of the score for the held out validation set (.87 vs .65)\n\nThen the roc_auc_score for the whole, unaugmented set is much higher than the best during the cross validation (.95 vs .89)\n\nIf we go for correctness, I have somewhere around 65 mislabellings which is around 15% of the dataset\n\nI'm going to have to think for a bit for other steps","169ca835":"<font size= '4'>Status, Drug, Hepatomegaly, Spiders, and Edema have somewhat large differences in the class representation between stages\nWe'll keep thse<font>"}}