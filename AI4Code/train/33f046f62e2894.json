{"cell_type":{"0cd3ddbb":"code","4fba3ce7":"code","67b0c469":"code","78f1cd0d":"code","61f7af92":"code","86fc2f1d":"code","c9e60681":"code","ad289021":"code","a554bc21":"code","4be5cbaa":"code","53261509":"code","be3361a0":"code","a9081a41":"code","9382c017":"code","68884676":"code","0fdc71b6":"code","0c9c0209":"code","b6265034":"code","985e0eb1":"code","0ce25c71":"code","37427ed3":"code","1aa912a2":"code","24255c1a":"code","066d31ac":"code","ff1a6845":"code","2809fa3a":"code","a86d9373":"code","9ee9c786":"code","069ab199":"code","1a784d7c":"code","f97a3013":"code","d3e8343f":"code","4cd4efc7":"markdown","205fe955":"markdown","15d5bf81":"markdown","3f01bef1":"markdown","2f3d285e":"markdown","bccdd152":"markdown","8a0345be":"markdown","70d96ae2":"markdown","df00d7a6":"markdown","d5ccc65e":"markdown","e83912bf":"markdown","9949c42b":"markdown","fc4b25bd":"markdown","36ac9560":"markdown","22e2259e":"markdown","1494179d":"markdown"},"source":{"0cd3ddbb":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\n\n# Try ploty libraries\nimport plotly.tools as tls\nimport warnings\n\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\nfrom collections import Counter\nwarnings.filterwarnings('ignore')\n\n# import plotly.graph_objs as go\n# import plotly.tools as tls\n# import plotly.plotly as plpl","4fba3ce7":"train = pd.read_csv(\"..\/input\/porto-seguro-safe-driver-prediction\/train.csv\")\ntest = pd.read_csv(\"..\/input\/porto-seguro-safe-driver-prediction\/test.csv\")\ntrain.head(20)","67b0c469":"test.head()","78f1cd0d":"pd.set_option('precision',3)\ntrain.describe()","61f7af92":"# Check if there is any null information anywhere\ntrain.isnull().any().any()","86fc2f1d":"train_cp = train\ntrain_cp = train_cp.replace(-1, np.NaN)\n\ndata = train","c9e60681":"colwithnan = train_cp.columns[train_cp.isnull().any()].tolist()\nprint(\"This dataset has %s Rows. \\n\" % (train_cp.shape[0]))\nfor col in colwithnan:\n    print(\"Column: %s has %s NaN\" % (col, train_cp[col].isnull().sum()))","ad289021":"target_count = train.target.value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])\nprint('Proportion:', round(target_count[0] \/ target_count[1], 2), ': 1')\n\ntarget_count.plot(kind='bar', title='Count (target)');","a554bc21":"train_float = train.select_dtypes(include=['float64'])\ntrain_int = train.select_dtypes(include=['int64'])\nCounter(train.dtypes.values)","4be5cbaa":"colormap = plt.cm.jet\nplt.figure(figsize=(16,12))\nplt.title('Pearson correlation of continuous features', y=1.05, size=15)\nsns.heatmap(train_float.corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)","53261509":"colormap = plt.cm.jet\ncotrain_float = train_float.drop(['ps_calc_03', 'ps_calc_02', 'ps_calc_01'], axis=1)\nplt.figure(figsize=(16,12))\nplt.title('Pearson correlation of continuous features', y=1.05, size=15)\nsns.heatmap(cotrain_float.corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)","be3361a0":"colormap = plt.cm.jet\nplt.figure(figsize=(21,16))\nplt.title('Pearson correlation of categorical features', y=1.05, size=15)\nsns.heatmap(train_int.corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=False)","a9081a41":"colormap = plt.cm.jet\ncotrain = train_int.drop(['id','target', 'ps_calc_04', 'ps_calc_05', 'ps_calc_06', 'ps_calc_07', 'ps_calc_08', 'ps_calc_09', 'ps_calc_10', 'ps_calc_11', 'ps_calc_12', 'ps_calc_13', 'ps_calc_14', 'ps_calc_15_bin', 'ps_calc_16_bin', 'ps_calc_17_bin', 'ps_calc_18_bin', 'ps_calc_19_bin', 'ps_calc_20_bin'], axis=1)\nplt.figure(figsize=(21,16))\nplt.title('Pearson correlation of int features withot ps_calc', y=1.05, size=12)\nsns.heatmap(cotrain.corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=False)","9382c017":"colormap = plt.cm.jet\n# train = train.drop(['id', 'target'], axis=1)\nplt.figure(figsize=(25,25))\nplt.title('Pearson correlation of All the features', y=1.05, size=15)\nsns.heatmap(train.corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=False)","68884676":"train_float.describe()","0fdc71b6":"train_float.plot(kind='box', subplots=True, layout=(2,5), sharey=False, figsize=(18,18))\nplt.show()\n","0c9c0209":"train_int.plot(kind='box', subplots=True, layout=(10,5), sharey=False, figsize=(18,90))\nplt.show()","b6265034":"# Check the binary features\nbin_col = [col for col in train.columns if '_bin' in col]\nzeros = []\nones = []\nfor col in bin_col:\n    zeros.append((train[col]==0).sum())\n    ones.append((train[col]==1).sum())","985e0eb1":"trace1 = go.Bar(\n    x=bin_col,\n    y=zeros ,\n    name='Zero count'\n)\ntrace2 = go.Bar(\n    x=bin_col,\n    y=ones,\n    name='One count'\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    barmode='stack',\n    title='Count of 1 and 0 in binary variables'\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='stacked-bar')","0ce25c71":"train_int = train_int.drop(['id', 'target'], axis=1)\ntrain_int = train_int.drop(bin_col, axis=1)\nsome_bin = train_int.describe()\nsome_bin","37427ed3":"cat_asbin = []\nfor col in some_bin:\n    #print(some_bin[col]['max'])\n    if (some_bin[col]['max']==1):\n        if ((some_bin[col]['min']==0) or (some_bin[col]['min']==-1)):\n            cat_asbin.append(col)\ncat_asbin","1aa912a2":"cat_zeros = []\ncat_ones = []\nfor col in cat_asbin:\n    cat_zeros.append((train[col]==0).sum())\n    cat_ones.append((train[col]==1).sum())","24255c1a":"trace1 = go.Bar(\n    x=cat_asbin,\n    y=cat_zeros ,\n    name='Zero count'\n)\ntrace2 = go.Bar(\n    x=cat_asbin,\n    y=cat_ones,\n    name='One count'\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    barmode='stack',\n    title='Count of 1 and 0 in binary variables'\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='stacked-bar')","066d31ac":"colormap = plt.cm.jet\ncotrainnb = cotrain.drop(['ps_ind_10_bin','ps_ind_11_bin','ps_ind_12_bin', 'ps_ind_13_bin'], axis=1)\nplt.figure(figsize=(21,16))\nplt.title('Taking away some binary data', y=1.05, size=12)\nsns.heatmap(cotrainnb.corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=False)","ff1a6845":"cat_col = [col for col in train.columns if '_cat' in col]\ncatds = train[cat_col]\nncatds = catds.drop(cat_asbin, axis=1)\nncatds.describe()","2809fa3a":"from plotly import tools\nhist_cat = []\n\nfor col in ncatds:\n    hist_cat.append(go.Histogram(x=ncatds[col], opacity=0.75, name =col))\n\nfig = tools.make_subplots(rows=len(hist_cat), cols=1)\n\nfor i in range(0,len(hist_cat),1):\n    fig.append_trace(hist_cat[i], i+1, 1)\n    \nfig['layout'].update(height=1500, width=750, title='Cat Features Histogram')\npy.iplot(fig, filename='cat-histogram')","a86d9373":"no_cat = some_bin.drop(ncatds, axis=1)\nno_cat.describe()","9ee9c786":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\n\n# Test options and evaluation metric\nnum_folds = 10\nseed = 8\nscoring = 'accuracy'\n\nX = train.drop(['id','target'], axis=1)\nY = train.target\n\nvalidation_size = 0.3\nX_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size, random_state=seed)","069ab199":"models = [('LR', LogisticRegression()), \n          ('LDA', LinearDiscriminantAnalysis()),\n          #('KNN', KNeighborsClassifier()),\n          ('CART', DecisionTreeClassifier()),\n          ('NB', GaussianNB())]\nresults = []\nnames = []\nfor name, model in models:\n    print(\"Training model %s\" %(name))\n    model.fit(X_train, Y_train)\n    result = model.score(X_validation, Y_validation)\n    #kfold = KFold(n_splits=num_folds, random_state=seed)\n    #cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    #results.append(cv_results)\n    #names.append(name)\n    msg = \"Classifier score %s: %f\" % (name, result)\n    print(msg)\nprint(\"----- Training Done -----\")","1a784d7c":"pipelines = [('ScaledLR', Pipeline([('Scaler', StandardScaler()), ('LR', LogisticRegression())])),\n             ('ScaledLDA', Pipeline([('Scaler', StandardScaler()), ('LDA', LinearDiscriminantAnalysis())])),\n             # ('ScaledKNN', Pipeline([('Scaler', StandardScaler()), ('KNN', KNeighborsClassifier())])),\n             ('ScaledCART', Pipeline([('Scaler', StandardScaler()), ('CART', DecisionTreeClassifier())])),\n             ('ScaledNB', Pipeline([('Scaler', StandardScaler()), ('NB', GaussianNB())]))]\nresults = []\nnames = []\nfor name, model in pipelines:\n    print(\"Training model %s\" %(name))\n    model.fit(X_train, Y_train)\n    result = model.score(X_validation, Y_validation)\n    #kfold = KFold(n_splits=num_folds, random_state=seed)\n    #cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    #results.append(cv_results)\n    #names.append(name)\n    msg = \"Classifier score %s: %f\" % (name, result)\n    print(msg)\nprint(\"----- Training Done -----\")","f97a3013":"ensembles = [('ABC', AdaBoostClassifier()), \n             ('GBM', GradientBoostingClassifier()),\n             ('RFC', RandomForestClassifier()),\n             ('ETC', ExtraTreesClassifier())]\nresults = []\nnames = []\n\nfor name, model in ensembles:\n    print(\"Training model %s\" %(name))\n    model.fit(X_train, Y_train)\n    result = model.score(X_validation, Y_validation)\n    #kfold = KFold(n_splits=num_folds, random_state=seed)\n    #cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    #results.append(cv_results)\n    #names.append(name)\n    msg = \"Classifier score %s: %f\" % (name, result)\n    print(msg)\nprint(\"----- Training Done -----\")","d3e8343f":"toplot = []\nfor name, model in ensembles:\n    trace = go.Bar(x=model.feature_importances_,\n                   y=X_validation.columns,\n                   orientation='h',\n                   textposition = 'auto',\n                   name=name\n                  )\n    toplot.append(trace)\n\nlayout = dict(\n        title = 'Barplot of features importance',\n        width = 900, height = 2000,\n        barmode='group')\n\nfig = go.Figure(data=toplot, layout=layout)\npy.iplot(fig, filename='features-figure')\n    ","4cd4efc7":"We have real meaningfull information about the float features, in some cases the data is spread out, in some others not that much.","205fe955":"This graph is basically telling, features ps_ind_10_bin, 11, 12 and 13 are basically useless. I will remove them from the feature correlaton map and redraw it, to see if I can spot something.","15d5bf81":"now standardising the data, and applying some non linear algorithms:","3f01bef1":"ps_car_07_cat is totally unbalanced as well as 08, I think 03 and 05 will be more balanced if we took away the NULL.","2f3d285e":"We can see huge class imbalance\n","bccdd152":"Feature inspection","8a0345be":"We can see here some interesting information:\n\nFeatures 05 and 04 pretty much zeros. (75% - 0)                          \nFeature 10 is pretty much ones.","70d96ae2":"From the chart we can see that everything pale green is not correlated, either negative or positive.  If we already know they don't correlate we should take as many features as possible from this correlation chart to spend quality time on the features that are correlated. For example ps_cal_04 to ps_calc_20_bin you can just take them away. (It would be a good idea to retire id and target).","df00d7a6":"some statistical info","d5ccc65e":"Now lets looks into the continous feaetures, and see if we find anything interesting","e83912bf":"Binary features\n","9949c42b":"lets create a baseline of performance on this data and check some different algorithms:\n","fc4b25bd":"If you see green, the features are no correlated, but if you see something not green, you will identify some correlation.\n\nThey are somehow correlated ps_reg_01 with 02 and 03:\n\nps_reg_01, ps_reg_03 = 0.64\nps_reg_02, ps_reg_03 = 0.53\nps_reg_01, ps_reg_02 = 0.47\nYou will think since, 12 and 15 are related with 13 you will see some form of correlation between them, but is close to zero as you can see in the chart.\n\nps_car_13, ps_car_12 = 0.67\nps_car_13 and ps_car_15 = 0.53\nSince the features ps_calc_03, 02 and 01 are not correlated to anything, I will retire them to have a more condensed graphic.","36ac9560":"checking for null","22e2259e":"also previously we found that:\nColumn: ps_car_02_cat has 5 NaN                    \nColumn: ps_car_03_cat has 411231 NaN              \nColumn: ps_car_05_cat has 266551 NaN                \nColumn: ps_car_07_cat has 11489 NaN","1494179d":"Feature importance"}}