{"cell_type":{"93816d3f":"code","9e6dfb4c":"code","6868bd6f":"code","47092fad":"code","177655b1":"code","1d83b279":"code","e27849b4":"code","414b6bbf":"code","7e192fcb":"code","978538da":"code","a098983e":"code","5000a124":"code","b789be19":"code","e42bbe4a":"code","67e85b2d":"code","64cf314d":"code","eebdadb3":"code","4bca6288":"code","7064507e":"code","0c862aa3":"code","4495a389":"code","85dbaa26":"code","6702967d":"code","e1f4963c":"code","48ed3e12":"code","3bdaa98b":"code","fa70cdf3":"code","63cae87c":"code","76fa8ece":"code","320d8b90":"markdown","1ddd6aaf":"markdown","d9a28758":"markdown"},"source":{"93816d3f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport warnings\n\nfrom fancyimpute import SimpleFill\n\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.decomposition import PCA, LatentDirichletAllocation\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.tree import DecisionTreeRegressor\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)","9e6dfb4c":"#product data\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\n#sales, exchange rates, social network data\nsales = pd.read_csv('..\/input\/sales.csv')\n\n#website navigation data\nnavigation = pd.read_csv('..\/input\/navigation.csv')\n\n#product images vectorized with ResNet50\nvimages = pd.read_csv('..\/input\/vimages.csv')","6868bd6f":"sales_float_columns = sales.dtypes[sales.dtypes == 'float64'].index.tolist()\nsales.loc[:,sales_float_columns] = SimpleFill(fill_method='random').fit_transform(sales.loc[:,sales_float_columns])","47092fad":"navigation.loc[navigation.website_version_zone_number.isna(), 'website_version_zone_number'] = 'unknown'\nnavigation.loc[navigation.website_version_country_number.isna(), 'website_version_country_number'] = 'unknown'","177655b1":"train.loc[train.color.isna(), 'color'] = 'unknown'\ntest.loc[test.color.isna(), 'color'] = 'unknown'","1d83b279":"currency_and_social_columns = sales.columns[9:].tolist()","e27849b4":"first_day = sales.loc[sales.Date == 'Day_1',:]","414b6bbf":"all_currency_and_social = sales.groupby('sku_hash').mean()[currency_and_social_columns]\nfirst_day_currency_and_social = first_day.groupby('sku_hash').mean()[currency_and_social_columns]\nfirst_day_currency_and_social.columns = ['first_day_' + col for col in first_day_currency_and_social.columns]","7e192fcb":"all_sales = sales.groupby('sku_hash').sum()['sales_quantity']\nall_sales = pd.DataFrame(all_sales)\nfirst_day_sales = first_day.groupby(['sku_hash', 'day_transaction_date', 'Month_transaction']).sum()['sales_quantity']\nfirst_day_sales = pd.DataFrame(first_day_sales)\nfirst_day_sales.columns = ['first_day_sales']\nfirst_day_sales.reset_index(inplace=True)\nfirst_day_sales.set_index('sku_hash', inplace=True)","978538da":"sales_data = pd.merge(all_sales, first_day_sales, left_index=True, right_index=True)\nsales_data = pd.merge(sales_data, all_currency_and_social, left_index=True, right_index=True)\nsales_data = pd.merge(sales_data, first_day_currency_and_social, left_index=True, right_index=True)","a098983e":"monthDict = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', 7:'Jul', 8:'Aug', 9:'Sep', 10:'Oct', 11:'Nov', 12:'Dec'}\nsales_data.Month_transaction = sales_data.Month_transaction.astype('object').map(monthDict)","5000a124":"sales_data.head()","b789be19":"first_day_navigation = navigation.loc[navigation.Date == 'Day 1',:]\nfirst_day_views = first_day_navigation.groupby('sku_hash').sum()[['page_views', 'addtocart']]\nfirst_day_views.columns = ['first_day_page_views', 'first_day_addtocart']\nviews = navigation.groupby('sku_hash').sum()[['page_views', 'addtocart']]\nnavigation_data = pd.merge(views, first_day_views, left_index=True, right_index=True)","e42bbe4a":"sales_data.sales_quantity = sales_data.sales_quantity.astype('float64')\nsales_data.first_day_sales = sales_data.first_day_sales.astype('float64')","67e85b2d":"sales_data['sales_quantity_log'] = (sales_data.sales_quantity + 1).apply(np.log)\nsales_data['first_day_sales_log'] = (sales_data.first_day_sales + 1).apply(np.log)","64cf314d":"train_data = pd.merge(train, sales_data, left_on='sku_hash', right_index=True)\ntrain_data = pd.merge(train_data, navigation_data, how='left', left_on='sku_hash', right_index=True)\ntrain_data = pd.merge(train_data, vimages, left_on='sku_hash', right_on='sku_hash')","eebdadb3":"test_data = pd.merge(test, sales_data, left_on='sku_hash', right_index=True)\ntest_data = pd.merge(test_data, navigation_data, how='left', left_on='sku_hash', right_index=True)\ntest_data = pd.merge(test_data, vimages, left_on='sku_hash', right_on='sku_hash')","4bca6288":"train_data[navigation_data.columns] = train_data[navigation_data.columns].fillna(0)\ntest_data[navigation_data.columns] = test_data[navigation_data.columns].fillna(0)","7064507e":"\n\nclass PandasSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, columns=None, dtype=None, inverse=False,\n                 return_vector=True):\n        self.dtype = dtype\n        self.columns = columns\n        self.inverse = inverse\n        self.return_vector = return_vector\n\n        if isinstance(self.columns, str):\n            self.columns = [self.columns]\n\n    def check_condition(self, x, col):\n        cond = (self.dtype is not None and x[col].dtype == self.dtype) or \\\n               (self.columns is not None and col in self.columns)\n        return self.inverse ^ cond\n\n    def fit(self, x, y=None):\n        return self\n\n    def _check_if_all_columns_present(self, x):\n        if not self.inverse and self.columns is not None:\n            missing_columns = set(self.columns) - set(x.columns)\n            if len(missing_columns) > 0:\n                missing_columns_ = ','.join(col for col in missing_columns)\n                raise KeyError('Keys are missing in the record: %s' %\n                               missing_columns_)\n\n    def transform(self, x):\n        # check if x is a pandas DataFrame\n        if not isinstance(x, pd.DataFrame):\n            raise KeyError('Input is not a pandas DataFrame')\n\n        selected_cols = []\n        for col in x.columns:\n            if self.check_condition(x, col):\n                selected_cols.append(col)\n\n        # if the column was selected and inversed = False make sure the column\n        # is in the DataFrame\n        self._check_if_all_columns_present(x)\n\n        # if only 1 column is returned return a vector instead of a dataframe\n        if len(selected_cols) == 1 and self.return_vector:\n            return list(x[selected_cols[0]])\n        else:\n            return x[selected_cols]","0c862aa3":"train_data1 = train_data.loc[train_data.month == 1, :].copy()\ntrain_data1.drop(['month', 'sku_hash', 'ID'], axis=1, inplace=True)\n\nX_test1 = test_data.loc[test_data.month == 1, :].copy()\nX_test1.drop(['month', 'sku_hash'], axis=1, inplace=True)\nX_test1.set_index('ID', inplace=True)\n\ny_train1 = (train_data1.target + 1).apply(np.log)\nX_train1 = train_data1.drop('target', axis=1)\n\ntrain_data2 = train_data.loc[train_data.month == 2, :].copy()\ntrain_data2.drop(['month', 'sku_hash', 'ID'], axis=1, inplace=True)\n\nX_test2 = test_data.loc[test_data.month == 2, :].copy()\nX_test2.drop(['month', 'sku_hash'], axis=1, inplace=True)\nX_test2.set_index('ID', inplace=True)\n\ny_train2 = (train_data2.target + 1).apply(np.log)\nX_train2 = train_data2.drop('target', axis=1)\n\n\ntrain_data3 = train_data.loc[train_data.month == 3, :].copy()\ntrain_data3.drop(['month', 'sku_hash', 'ID'], axis=1, inplace=True)\n\nX_test3 = test_data.loc[test_data.month == 3, :].copy()\nX_test3.drop(['month', 'sku_hash'], axis=1, inplace=True)\nX_test3.set_index('ID', inplace=True)\n\ny_train3 = (train_data3.target + 1).apply(np.log)\nX_train3 = train_data3.drop('target', axis=1)","4495a389":"images_cols = vimages.columns[1:].tolist()\nfloat_cols = X_train1.dtypes[X_train1.dtypes == 'float64'].index.tolist()\nfloat_cols = list(set(float_cols) - set(images_cols))\nfloat_cols.remove('sales_quantity_log')\nfloat_cols.remove('first_day_sales_log')\nfloat_cols.remove('sales_quantity')\nfloat_cols.remove('first_day_sales')","85dbaa26":"categorical_cols = X_train1.dtypes[X_train1.dtypes == 'object'].index.tolist()\ncategorical_cols.remove('en_US_description')\ncategorical_cols.remove('color')","6702967d":"model = make_pipeline(\n    make_union(\n        make_pipeline(PandasSelector(columns='en_US_description'), \n                      CountVectorizer(stop_words='english'),\n                      LatentDirichletAllocation(n_components=10)),\n        make_pipeline(PandasSelector(columns='color'), \n                      CountVectorizer()\n                     ),\n        make_pipeline(PandasSelector(columns=images_cols), \n                      PCA(10)),\n        make_pipeline(PandasSelector(columns=float_cols),\n                      PCA(10)),\n        make_pipeline(PandasSelector(columns=['sales_quantity_log', \n                                              'first_day_sales_log', \n                                              'sales_quantity', \n                                              'first_day_sales'])),\n        make_pipeline(PandasSelector(columns=categorical_cols), \n                      OneHotEncoder(handle_unknown='ignore'),\n                      LatentDirichletAllocation(n_components=10))\n    ),\n    SelectFromModel(RandomForestRegressor(n_estimators=100)),\n    DecisionTreeRegressor()\n)","e1f4963c":"params = {'decisiontreeregressor__min_samples_split': [40, 60, 80],\n          'decisiontreeregressor__max_depth': [4, 6, 8]}","48ed3e12":"gs = GridSearchCV(model, param_grid=params, cv=4, verbose=3, n_jobs=-1)\n\ngs.fit(X_train1, y_train1)\ny_test1 = gs.predict(X_test1)\n\nprint('metric cv: ', np.round(np.sqrt(gs.best_score_),4))\nprint('metric train: ', np.round(np.sqrt(mean_squared_error(y_train1, gs.predict(X_train1))),4))\nprint('params: ', gs.best_params_)","3bdaa98b":"gs.fit(X_train2, y_train2)\ny_test2 = gs.predict(X_test2)\n\nprint('metric cv: ', np.round(np.sqrt(gs.best_score_),4))\nprint('metric train: ', np.round(np.sqrt(mean_squared_error(y_train2, gs.predict(X_train2))),4))\nprint('params: ', gs.best_params_)","fa70cdf3":"gs.fit(X_train3, y_train3)\ny_test3 = gs.predict(X_test3)\n\nprint('metric cv: ', np.round(np.sqrt(gs.best_score_),4))\nprint('metric train: ', np.round(np.sqrt(mean_squared_error(y_train3, gs.predict(X_train3))),4))\nprint('params: ', gs.best_params_)","63cae87c":"y_test1 = pd.Series(y_test1)\ny_test1 = (y_test1).apply(np.exp)  - 1\ny_test1.index = X_test1.index\n\ny_test2 = pd.Series(y_test2)\ny_test2 = (y_test2).apply(np.exp)  - 1\ny_test2.index = X_test2.index\n\ny_test3 = pd.Series(y_test3)\ny_test3 = (y_test3).apply(np.exp) - 1\ny_test3.index = X_test3.index\n\nsubmission = pd.DataFrame(pd.concat([y_test1, y_test2, y_test3]))\nsubmission.columns = ['target']","76fa8ece":"submission.to_csv('submission.csv')","320d8b90":"# Data preparation","1ddd6aaf":"## separate models for each prediction month","d9a28758":"# Modeling\n## utils\nfrom https:\/\/github.com\/pjankiewicz\/PandasSelector"}}