{"cell_type":{"f75efb3e":"code","c195e23e":"code","06bb276f":"code","1d2ae8e2":"code","acc7a2d5":"code","94c67082":"code","9f9ab709":"code","a30c02a2":"code","99ee6f19":"code","d34188dc":"code","8904fea8":"code","04108cf0":"code","dffee662":"code","116cf74c":"code","b001a6c0":"code","6bb67413":"code","d25ee039":"code","4aeb4a55":"code","d2fff5f1":"code","9dba976d":"code","6a623674":"code","febe74ce":"code","06ecd8da":"code","fa5c9352":"code","311f3e2d":"code","d5425f19":"code","9b48f9e6":"code","ccfe279e":"code","b99cd741":"code","bf940390":"markdown","527df0f0":"markdown","769aef80":"markdown","6c2ad112":"markdown","1aa5f580":"markdown","a346af52":"markdown","e2743f60":"markdown","70aaa01f":"markdown","28e537ae":"markdown","dde53f47":"markdown"},"source":{"f75efb3e":"#Sample output\nprint(generate_text(model, 'Et tu, Brute?', gen_size = 1000))","c195e23e":"#importing libraries and packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf","06bb276f":"#load the dataset containing shakespeare's works\ntext = open('..\/input\/shakespeare.txt', 'r').read()\nprint(text[:1000])","1d2ae8e2":"total_char = len(list(text))\nunique_char = len(set(text))\n\ntext_cleaned = ''\nalphalist = [chr(i) for i in range(ord('A'), ord('z')+1)]\n\nfor i in text:\n    if i.isalpha():\n        text_cleaned += i\n    else:\n        text_cleaned += ' '\n\nchar_count = {}\nfor i in text_cleaned.split():\n    if i in char_count:\n        char_count[i] += 1\n    else:\n        char_count[i] = 1\n        \ndf = pd.DataFrame(char_count.items(), columns=['Words','Count'])\ndf.sort_values('Count', axis=0, ascending=False, inplace=True)\ndf.reset_index(drop=True, inplace=True)","acc7a2d5":"print('Total Characters: ', total_char)\nprint('Unique Characters: ', unique_char)\nprint('Most used words:')\ndisplay(df.head(10))","94c67082":"#Get all the unique characters\nvocab = sorted(set(text))\nvocab_size = len(vocab)\nprint(vocab)\nprint('Total uniques characters: ',vocab_size)","9f9ab709":"#Map characters to numbers and numbers to characters\nchar_to_ind = {u:i for i,u in enumerate(vocab)}\nind_to_char = {i:u for i,u in enumerate(vocab)}\nprint(char_to_ind)\nprint('\\n')\nprint(ind_to_char)","a30c02a2":"#encode the first 1000 characters as numbers\nencoded_text = np.array([char_to_ind[c] for c in text])\nprint(encoded_text[:1000])","99ee6f19":"#number of sequences to generate\nseq_len = 120\ntotal_num_seq = len(text)\/\/(seq_len+1)\nprint('Total Number of Sequences: ', total_num_seq)","d34188dc":"#Create training sequences\n#tf.data.Dataset.from_tensor_slices function converts a text vector\n#into a stream of character indices\nchar_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n\nfor i in char_dataset.take(500):\n    print(ind_to_char[int(i)],end=\"\")","8904fea8":"#batch method converts these individual character calls into sequences\n#which we can feed in as a batch\n#we use seq_len+1 because we will use seq_len characters\n#and shift them one step forward\n#drop remainder drops the remaining characters < batch_size\nsequences = char_dataset.batch(seq_len+1, drop_remainder=True)","04108cf0":"#this function will grab a sequence\n#take the [0:n-1] characters as input text\n#take the [1:n] characters as target text\n#return a tuple of both\ndef create_seq_targets(seq):\n    input_txt = seq[:-1]\n    target_txt = seq[1:]\n    return input_txt, target_txt","dffee662":"#this will convert the series of sequences into\n#a series of tuple containing input and target text\ndataset = sequences.map(create_seq_targets)","116cf74c":"for input_txt, target_txt in dataset.take(1):\n    print(''.join([ind_to_char[i] for i in np.array(input_txt)]))\n    print('\\n')\n    print(''.join([ind_to_char[i] for i in np.array(target_txt)]))\n\n#the target is shifter 1 character forward\n#the last character is a space and is thus not visible","b001a6c0":"batch_size = 128 #number of sequence tuples in each batch\nbuffer_size = 10000 #shuffle this many sequences in the dataset\n\n#first shuffle the dataset and divide it into batches\n#drop the last sequences < batch_size\ndataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)","6bb67413":"#count the number of batches\n#i couldn't find a function to do it in O(1)\n#please let me know\n\nx = 0\nfor i in dataset:\n    x += 1","d25ee039":"print('Total Batches:', x)\nprint('Sequences in each batch: ', batch_size)\nprint('Characters in each sequence:', seq_len)\nprint('Characters in dataset: ', len(list(text)))","4aeb4a55":"#importing keras modules\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM,Dense,Embedding,Dropout,GRU\nfrom tensorflow.keras.losses import sparse_categorical_crossentropy","d2fff5f1":"#using sparse_categorical_crossentropy because\n#out predictions will be numbers and not one hot encodings\n#we need to define a custom loss function so that we can change\n#the from_logits parameter to True\ndef sparse_cat_loss(y_true, y_pred):\n    return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)","9dba976d":"def create_model(batch_size):\n    vocab_size_func = vocab_size\n    embed_dim = 64 #the embedding dimension\n    rnn_neurons = 1024 #number of rnn units\n    batch_size_func = batch_size\n    \n    model = Sequential()\n    \n    model.add(Embedding(vocab_size_func, \n                        embed_dim, \n                        batch_input_shape=[batch_size_func, None]))\n    model.add(GRU(rnn_neurons, \n                  return_sequences=True, \n                  stateful=True, \n                  recurrent_initializer='glorot_uniform'))\n    \n    model.add(Dense(vocab_size_func))    \n    model.compile(optimizer='adam', loss=sparse_cat_loss)    \n    \n    return model","6a623674":"model = create_model(batch_size)\nmodel.summary()","febe74ce":"#note this will generate random characters\n#dataset.take(1) contains 1 batch = 128 sequence tuples\n#model will output 120 characters per sequence\n#in the form of probability of those 84 vocab characters\nfor ex_input, ex_target in dataset.take(1):\n    ex_pred = model(ex_input)\nprint(ex_pred.shape)\n\n#changes the character probabilities to integers\nsampled_indices = tf.random.categorical(ex_pred[0], num_samples=1)\n\n#maps those integers to characters\nchar_pred = ''.join([ind_to_char[int(i)] for i in sampled_indices])\n\nprint(char_pred)","06ecd8da":"#training the model\nmodel.fit(dataset, epochs=30, verbose=1)","fa5c9352":"#save the model\nmodel.save('shakespeare.h5')","311f3e2d":"# importing load_model to load the keras model\nfrom tensorflow.keras.models import load_model","d5425f19":"#create a new model with a batch size of 1\nmodel = create_model(batch_size=1)\n\n#load the weights from the previous model to our new model\nmodel.load_weights('shakespeare.h5')\n\n#build the model\nmodel.build(tf.TensorShape([1, None]))\n\n#view model summary\nprint(model.summary())","9b48f9e6":"#function to generate text based on an input text\n#we enter the text on which our output will be based\n#we define how many characters we want in output\n\ndef generate_text(model, start_seed, gen_size=100):\n    num_generate = gen_size\n    input_eval = [char_to_ind[s] for s in start_seed]\n    input_eval = tf.expand_dims(input_eval, 0)\n    \n    text_generated = []\n    \n    model.reset_states()\n    \n    for i in range(num_generate):\n        predictions = model(input_eval)\n        predictions = tf.squeeze(predictions, 0)\n        \n        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n        \n        input_eval = tf.expand_dims([predicted_id], 0)\n        \n        text_generated.append(ind_to_char[predicted_id])\n        \n    return (start_seed + ''.join(text_generated))","ccfe279e":"#generate a text based on input\n#note that, this out is not part of the dataset\n#but completely auto generated\nauto_text = generate_text(model, 'How art thou?', gen_size = 1000)\nprint(auto_text)","b99cd741":"#download the saved model\nmodel.save('shakespeare.h5')\nfrom IPython.display import FileLink\nFileLink(r'shakespeare.h5')","bf940390":"# Generating text","527df0f0":"#### As we have  created the model, random weights and biases have been assigned, so before begining training lets first check whether the model is working or not.","769aef80":"# Generating training batches","6c2ad112":"# Training the Model","1aa5f580":"# Text Preprocessing","a346af52":"# Creating the Model","e2743f60":"####  I have provided a sample output at the top, please go through the entire notebook, to understand how it all works. ","70aaa01f":"# Training Sequence","28e537ae":"## Virtual Shakespeare - Natural Language Processing\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/a\/a2\/Shakespeare.jpg\" width=\"200\" height=\"200\"\/>\n\n#### Hola folks! Don't be fooled by the clickbait title.<br> In this quest to recreate Shakespeare, I was partly successful. Actually, no I wasn't.<br> Still, I was able to create a 3 year old Shakespeare, with some broken words and a bit of a hate for grammar.\n\n#### Okay, coming to this notebook. I have used a dataset of Shakespeare's works to train a model, which on giving an input text, will produce an output text of desired length.\n\n#### In the output text, you can observe that they follow the same structure as shakespeare's works, uses similar words and similar dialogues.<br> This output text is completely computer generated and not a subset of the dataset. \n\n#### The dataset has over 5 million characters, and 84 distinct characters.\n\n##### I will not be documenting the entire notebook, although comments have been provided for easy understanding.<br>As I am still learning NLP, some part of this notebook might not be correct or it may contain things that don't make sense. I'm really sorry about those. \n\n### Corrections and improvements to this notebook are most welcome. It will really help me in learning more.\n\n### Do upvote it, if you like it!","dde53f47":"# Dataset Statistics"}}