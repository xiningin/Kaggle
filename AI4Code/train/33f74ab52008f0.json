{"cell_type":{"805c6b21":"code","422e828f":"code","e64fb25f":"code","14a41108":"code","dd98d726":"code","95ec1e8c":"code","c44fc00c":"code","e69af18b":"code","7fc0b819":"code","c9454d85":"code","cd6978ba":"code","216ffa02":"code","b6f042f1":"code","9181b486":"code","73b71c84":"code","f4eac28a":"code","207045de":"code","93e16db1":"code","bc9f2215":"code","56df84ee":"code","b1a5c0d0":"code","7a0c6576":"code","fd282696":"code","42a70db1":"code","0ac4d462":"markdown","e383f6aa":"markdown","d9199f95":"markdown","876e3387":"markdown"},"source":{"805c6b21":"import os\nimport sys\nimport operator\nimport numpy as np\nimport pandas as pd\nfrom scipy import sparse\nimport xgboost as xgb\nfrom sklearn import model_selection, preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer","422e828f":"# hold-out\nfrom sklearn.model_selection import train_test_split\n\n# K\u6298\u4ea4\u53c9\u9a8c\u8bc1\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import RepeatedKFold\n\n# K\u6298\u5206\u5e03\u4fdd\u6301\u4ea4\u53c9\u9a8c\u8bc1\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n\n# \u65f6\u95f4\u5e8f\u5217\u5212\u5206\u65b9\u6cd5\nfrom sklearn.model_selection import TimeSeriesSplit\n\n# booststrap \u91c7\u6837\nfrom sklearn.utils import resample","e64fb25f":"# X = np.zeros((20, 5))\n# Y = np.array([1, 2, 3, 4] * 5)\n# print(X, Y)\n\nX = np.zeros((20, 5))\nY = np.array([1]*5 + [2]*5 + [3]*5 + [4]*5)\nprint(X, Y)","14a41108":"# \u76f4\u63a5\u6309\u7167\u6bd4\u4f8b\u62c6\u5206\ntrain_X, val_X, train_y, val_y = train_test_split(X, Y, test_size = 0.2)\nprint(train_y, val_y)\n\n# \u6309\u7167\u6bd4\u4f8b & \u6807\u7b7e\u5206\u5e03\u5212\u5206\ntrain_X, val_X, train_y, val_y = train_test_split(X, Y, test_size = 0.2, stratify=Y)\nprint(train_y, val_y)","dd98d726":"kf = KFold(n_splits=5)\nfor train_idx, test_idx, in kf.split(X, Y):\n    print(train_idx, test_idx)\n    print('Label', Y[test_idx])\n    print('')","95ec1e8c":"kf = RepeatedKFold(n_splits=5, n_repeats=2)\nfor train_idx, test_idx, in kf.split(X, Y):\n    print(train_idx, test_idx)\n    print('Label', Y[test_idx])\n    print('')","c44fc00c":"kf = StratifiedKFold(n_splits=5)\nfor train_idx, test_idx, in kf.split(X, Y):\n    print(train_idx, test_idx)\n    print('Label', Y[test_idx])\n    print('')","e69af18b":"kf = RepeatedStratifiedKFold(n_splits=5, n_repeats=2)\nfor train_idx, test_idx, in kf.split(X, Y):\n    print(train_idx, test_idx)\n    print('Label', Y[test_idx])\n    print('')","7fc0b819":"kf = TimeSeriesSplit(n_splits=5)\nfor train_idx, test_idx, in kf.split(X, Y):\n    print(train_idx, test_idx)\n    print('Label', Y[test_idx])\n    print('')","c9454d85":"train_X, train_Y = resample(X, Y, n_samples=16)\nval_X, val_Y = resample(X, Y, n_samples=4)\nprint(train_Y, val_Y)","cd6978ba":"! unzip ..\/input\/two-sigma-connect-rental-listing-inquiries\/train.json.zip\n! unzip ..\/input\/two-sigma-connect-rental-listing-inquiries\/test.json.zip","216ffa02":"!ls .\/","b6f042f1":"train_df = pd.read_json('.\/train.json')\ntest_df = pd.read_json('.\/test.json')\nprint(train_df.shape)\nprint(test_df.shape)\n\nfeatures_to_use  = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\", \n                    \"num_photos\", \"num_features\", \"num_description_words\",\"created_year\", \n                    \"created_month\", \"created_day\", \"listing_id\", \"created_hour\"]\n\n# count of photos #\ntrain_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\ntest_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n\n# count of \"features\" #\ntrain_df[\"num_features\"] = train_df[\"features\"].apply(len)\ntest_df[\"num_features\"] = test_df[\"features\"].apply(len)\n\n# count of words present in description column #\ntrain_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\ntest_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n\n# convert the created column to datetime object so as to extract more features \ntrain_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\ntest_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n\n# Let us extract some features like year, month, day, hour from date columns #\ntrain_df[\"created_year\"] = train_df[\"created\"].dt.year\ntest_df[\"created_year\"] = test_df[\"created\"].dt.year\ntrain_df[\"created_month\"] = train_df[\"created\"].dt.month\ntest_df[\"created_month\"] = test_df[\"created\"].dt.month\ntrain_df[\"created_day\"] = train_df[\"created\"].dt.day\ntest_df[\"created_day\"] = test_df[\"created\"].dt.day\ntrain_df[\"created_hour\"] = train_df[\"created\"].dt.hour\ntest_df[\"created_hour\"] = test_df[\"created\"].dt.hour","9181b486":"categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\"]\nfor f in categorical:\n        if train_df[f].dtype=='object':\n            #print(f)\n            lbl = preprocessing.LabelEncoder()\n            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n            train_df[f] = lbl.transform(list(train_df[f].values))\n            test_df[f] = lbl.transform(list(test_df[f].values))\n            features_to_use.append(f)","73b71c84":"train_df['features'] = train_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\ntest_df['features'] = test_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\nprint(train_df[\"features\"].head())\ntfidf = CountVectorizer(stop_words='english', max_features=200)\ntr_sparse = tfidf.fit_transform(train_df[\"features\"])\nte_sparse = tfidf.transform(test_df[\"features\"])\n\ntrain_X = sparse.hstack([train_df[features_to_use], tr_sparse]).tocsr()\ntest_X = sparse.hstack([test_df[features_to_use], te_sparse]).tocsr()\n\ntarget_num_map = {'high':0, 'medium':1, 'low':2}\ntrain_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\nprint(train_X.shape, test_X.shape)","f4eac28a":"from warnings import filterwarnings\nfilterwarnings('ignore')\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.preprocessing import StandardScaler","207045de":"clf = LogisticRegression()\nclf = RandomForestClassifier()\nclf = LGBMClassifier()\n# clf = XGBClassifier()","93e16db1":"# \u8fd9\u5199\u4e86\u4e00\u4e2abug\uff0c\u4f60\u80fd\u6539\u597d\u5417\uff1f\ntrain_X = StandardScaler().fit_transform(train_df[features_to_use])\ntest_X = StandardScaler().fit_transform(test_df[features_to_use])\n\nkf = StratifiedKFold(n_splits=5)\ntest_pred = None\nfor train_idx, test_idx, in kf.split(train_X, train_df['interest_level']):\n    \n    print(train_idx, test_idx)\n    clf.fit(train_X[train_idx], train_y[train_idx])\n    print('Val loss', log_loss(train_y[test_idx], \n                   clf.predict_proba(train_X[test_idx])))\n    \n    if test_pred is None:\n        test_pred = clf.predict_proba(test_X)\n    else:\n        test_pred += clf.predict_proba(test_X)\n\ntest_pred \/= 5","bc9f2215":"train_X = train_df[features_to_use].values\ntest_X = test_df[features_to_use].values\n\nkf = StratifiedKFold(n_splits=5)\ntest_pred = None\nfor train_idx, test_idx, in kf.split(train_X, train_df['interest_level']):\n    \n    print(train_idx, test_idx)\n    clf.fit(train_X[train_idx], train_y[train_idx])\n    print('Val loss', log_loss(train_y[test_idx], \n                   clf.predict_proba(train_X[test_idx])))\n    \n    if test_pred is None:\n        test_pred = clf.predict_proba(test_X)\n    else:\n        test_pred += clf.predict_proba(test_X)\n\ntest_pred \/= 5","56df84ee":"# lightGBM\nclf = LGBMClassifier(learning_rate=0.05, n_estimators=2000, n_jobs=2)\n\ntrain_X = train_df[features_to_use].values\ntest_X = test_df[features_to_use].values\n\nkf = StratifiedKFold(n_splits=5)\ntest_pred = None\nfor train_idx, test_idx, in kf.split(train_X, train_df['interest_level']):\n    \n    print(train_idx, test_idx)\n    clf.fit(train_X[train_idx], train_y[train_idx], \n            eval_set=[(train_X[test_idx], train_y[test_idx]), (train_X[test_idx], train_y[test_idx])],\n           verbose=50, early_stopping_rounds=50)\n    print('Val loss', log_loss(train_y[test_idx], \n                   clf.predict_proba(train_X[test_idx])))\n    \n    if test_pred is None:\n        test_pred = clf.predict_proba(test_X)\n    else:\n        test_pred += clf.predict_proba(test_X)\n\ntest_pred \/= 5","b1a5c0d0":"out_df = pd.DataFrame(test_pred)\nout_df.columns = [\"high\", \"medium\", \"low\"]\nout_df[\"listing_id\"] = test_df.listing_id.values\nout_df.to_csv(\"xgb_starter2.csv\", index=False)","7a0c6576":"LGBMClassifier?","fd282696":"from sklearn.metrics import make_scorer\ndef my_scorer(clf, X, y_true):\n    class_labels = clf.classes_\n    y_pred_proba = clf.predict_proba(X)\n    return log_loss(y_true, y_pred_proba)\n\nfrom sklearn.model_selection import GridSearchCV\nparameters = {\n    'num_leaves':( 4, 8, 16, 32), \n    'subsample':(0.75, 0.85, 0.95),\n    'min_child_samples': (5, 10, 15)\n}\n\nclf = GridSearchCV(LGBMClassifier(), param_grid=parameters, n_jobs=6, scoring=my_scorer, cv=5)\nclf.fit(train_X, train_y)","42a70db1":"from sklearn.metrics import make_scorer\ndef my_scorer(clf, X, y_true):\n    class_labels = clf.classes_\n    y_pred_proba = clf.predict_proba(X)\n    return log_loss(y_true, y_pred_proba)\n\nfrom sklearn.model_selection import RandomizedSearchCV\nparameters = {\n    'num_leaves':( 4, 8, 16, 32), \n    'subsample':[0.75, 1],\n    'min_child_samples': (5, 10, 15)\n}\n\nclf = GridSearchCV(LGBMClassifier(), param_grid=parameters, \n                   n_jobs=6, scoring=my_scorer, cv=StratifiedKFold(n_splits=5))\nclf.fit(train_X, train_y)","0ac4d462":"# \u6570\u636e\u5212\u5206\u65b9\u6cd5","e383f6aa":"# \u53c2\u6570\u641c\u7d22","d9199f95":"# \u63d0\u53d6 Two-Sigma\u6bd4\u8d5b\u7279\u5f81","876e3387":"# \u9605\u8bfb\u94fe\u63a5\n\n- https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html\n- https:\/\/lightgbm.readthedocs.io\/en\/latest\/Python-API.html\n- https:\/\/xgboost.readthedocs.io\/en\/latest\/\/python\/index.html\n\n\n- https:\/\/github.com\/fmfn\/BayesianOptimization"}}