{"cell_type":{"37832c3c":"code","eb69f1af":"code","b05748c6":"code","163b5509":"code","d0f1cf9b":"code","f83cd9bc":"code","30e79af1":"code","5c82478d":"code","0d1558b4":"code","293a1d11":"code","ee5226f1":"code","de04210c":"code","9e267cef":"code","a45ce501":"code","ebee6044":"code","a0267729":"code","d91f8709":"code","df6bf414":"code","f9952a96":"code","0df60813":"code","9389fb1b":"code","c51bdcf8":"code","e9dadda5":"code","3d59db1f":"code","5bc6a721":"code","84b5818e":"code","9dc9a02d":"code","33e30173":"code","56842aa0":"code","428a6b5a":"code","8ab23003":"code","aa678102":"code","45b41683":"code","e8d84e00":"code","3dfd95ec":"code","0a0b8c3e":"markdown","92bdbafd":"markdown","c29a7051":"markdown","da338696":"markdown","149dc96f":"markdown","850f08e4":"markdown","26cea2ad":"markdown","ba1bcba9":"markdown","abaf4a35":"markdown","310cd6ab":"markdown","cbde871b":"markdown","2890e93b":"markdown","55081fd0":"markdown","fe6fd0b7":"markdown","6e44618f":"markdown","93bcf08e":"markdown","6f36b0ee":"markdown","90f0eb13":"markdown"},"source":{"37832c3c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve","eb69f1af":"#Read data from csv file\ndf = pd.read_csv('..\/input\/fetal-health-classification\/fetal_health.csv')","b05748c6":"#Preview raw data\ndf.head(10).T","163b5509":"#Rename columns for easier use and representation\ncol_names = ['FHR', 'ACC', 'FM', 'UC', 'LD', 'SD', 'PD', 'ASTV', 'MSTV',\n               'ALTV', 'MLTV', 'Hist_Width', 'Hist_Min', 'Hist_Max', 'Hist_Peaks', 'Hist_Zeros', \n                'Hist_Mode', 'Hist_Mean', 'Hist_Median', 'Hist_Variance', 'Hist_Tendency', 'FH']\ndf.columns = col_names","d0f1cf9b":"#Check for null entries\nnull_count = df.columns.isna().sum()\nprint(\"Number of null entries:\\n\", null_count)","f83cd9bc":"#Basic data structure (data types and number of entries)\ndf.info()","30e79af1":"#Summary statistics for the data\ndf.describe().T","5c82478d":"#Plot histograms or all given features\nhist_plot = df.hist(figsize = (25,25))\nplt.show()","0d1558b4":"# Plot histogram of fetal health (target variable)\nplt.rcParams['figure.figsize'] = (7,7)\nsns.countplot(df['FH'])\nax = plt.gca()","293a1d11":"#Generate pairplot for data\nplt.rcParams['figure.figsize'] = (20,20)\nsns.pairplot(data=df, hue='FH',diag_kind='hist')","ee5226f1":"#Plot probabilistic relation between features and target variable (Fetal Health)\nsns.violinplot(df['FH'], df['FHR'])\nplt.show()\nsns.violinplot(df['FH'], df['ACC'])\nplt.show()\nsns.violinplot(df['FH'], df['FM'])\nplt.show()\nsns.violinplot(df['FH'], df['UC'])\nplt.show()\nsns.violinplot(df['FH'], df['LD'])\nplt.show()\nsns.violinplot(df['FH'], df['SD'])\nplt.show()\nsns.violinplot(df['FH'], df['PD'])\nplt.show()\nsns.violinplot(df['FH'], df['ASTV'])\nplt.show()\nsns.violinplot(df['FH'], df['MSTV'])\nplt.show()\nsns.violinplot(df['FH'], df['ALTV'])\nplt.show()\nsns.violinplot(df['FH'], df['MLTV'])\nplt.show()","de04210c":"# Plot heatmap to determine correlation between all features\nax=plt.subplots(figsize=(15,15))\nsns.heatmap(df.corr(), annot=True)","9e267cef":"#Get correlation of all features to fetus health (target variable)\nax=plt.subplots(figsize=(25,2))\nsns.heatmap(df.corr().sort_values(by=[\"FH\"], ascending=False).head(1),annot=True)\nplt.show()","a45ce501":"#Split data into X and y\nX_raw = df.drop('FH', axis=1)\ny = df['FH']","ebee6044":"#Scale X data\nscale_X = StandardScaler()\ncol_names.remove('FH')\nX = pd.DataFrame(scale_X.fit_transform(X_raw), columns = col_names)","a0267729":"#Preview scaled data\nX.head()","d91f8709":"#Split data into train and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)","df6bf414":"# define model and parameters for gridsearch\nknn = KNeighborsClassifier()\nk_list = np.arange(1,30,2)\nweights = ['uniform', 'distance']\nmetric = ['euclidean', 'manhattan', 'minkowski']\n\n#define grid search\ngrid = dict(n_neighbors=k_list,weights=weights,metric=metric)\ncv = RepeatedStratifiedKFold(n_splits=20, n_repeats=5, random_state=1)\ngrid_search = GridSearchCV(estimator=knn, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)","f9952a96":"grid_result = grid_search.fit(X_train, y_train)","0df60813":"# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","9389fb1b":"# define model and parameters for gridsearch\nsvm_clf = SVC()\nkernel = ['linear','poly', 'rbf', 'sigmoid']\nC = [100, 50, 10, 1.0, 0.1, 0.01, 0.001]\ngamma = ['scale']\n\n# define grid search\ngrid = dict(kernel=kernel,C=C,gamma=gamma)\ncv = RepeatedStratifiedKFold(n_splits=20, n_repeats=5, random_state=1)\ngrid_search = GridSearchCV(estimator=svm_clf, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)","c51bdcf8":"grid_result = grid_search.fit(X_train, y_train)","e9dadda5":"# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","3d59db1f":"# define models and parameters for gridsearch\nlog_reg = LogisticRegression()\nsolvers = ['newton-cg','lbfgs','liblinear','sag','saga']\npenalty = ['l1','l2','elasticnet','none']\nC = [100, 50, 10, 1.0, 0.1, 0.01, 0.001]\n\n# define grid search\ngrid = dict(solver=solvers,penalty=penalty,C=C)\ncv = RepeatedStratifiedKFold(n_splits=20, n_repeats=5, random_state=1)\ngrid_search = GridSearchCV(estimator=log_reg, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)","5bc6a721":"grid_result = grid_search.fit(X_train, y_train)","84b5818e":"# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","9dc9a02d":"# define model and parameters for gridsearch\nrf_clf = RandomForestClassifier()\nn_estimators = [100,150,200]\nmax_features = ['sqrt', 'log2']\nbootstrap = [True]\nmax_depth = [50,60,70,80]\n\n# define grid search\ngrid = dict(n_estimators=n_estimators,max_features=max_features,bootstrap=bootstrap,max_depth=max_depth)\ncv = RepeatedStratifiedKFold(n_splits=20, n_repeats=5, random_state=1)\ngrid_search = GridSearchCV(estimator=rf_clf, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)","33e30173":"grid_result = grid_search.fit(X_train, y_train)","56842aa0":"# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","428a6b5a":"# Fit knn classifier with gridsearch results\nknn = KNeighborsClassifier(metric='manhattan',n_neighbors=7,weights='distance')\nknn.fit(X_train, y_train)\n\n# Run prediction\ny_pred_knn = knn.predict(X_test)","8ab23003":"# Fit svm classifier with gridsearch results\nsvm_clf = SVC(C=50,gamma='scale',kernel='rbf')\nsvm_clf.fit(X_train, y_train)\n\n# Run prediction\ny_pred_svm = svm_clf.predict(X_test)","aa678102":"# Fit logistic regression classifier with gridsearch results\nlog_reg = LogisticRegression(C=0.1,penalty='l2',solver='newton-cg')\nlog_reg.fit(X_train, y_train)\n\n# Run prediction\ny_pred_lgr = log_reg.predict(X_test)","45b41683":"# Fit random forest classifier with gridsearch results\nrf_clf = RandomForestClassifier(bootstrap=True, max_depth=80, max_features='sqrt', n_estimators=150)\nrf_clf.fit(X_train, y_train)\n\n# Run prediction\ny_pred_rfc = rf_clf.predict(X_test)","e8d84e00":"#Compile final predictions from all models\npred_model_names = [\"KNN Model\",\"SVM Model\",\"Logistic Regression\",\"Random Forest\"]\ny_pred_list = [y_pred_knn,y_pred_svm,y_pred_lgr,y_pred_rfc]","3dfd95ec":"#Evaluate predictions by each classifier\ni=0\nprint(\"=\"*70)\nfor y_pred in y_pred_list:\n    print(pred_model_names[i])\n    i += 1\n    print(\"-\"*65)\n    print(\"Confusion Matrix \\n\", confusion_matrix(y_test,y_pred))\n    print(\"-\"*65)\n    print(\"Classification Report \\n\", classification_report(y_test,y_pred))\n    print(\"-\"*65)\n    print('Accuracy Score:',accuracy_score(y_test,y_pred))\n    print(\"=\"*70)","0a0b8c3e":"|Old Columns Names                                     | New Columns Names  |\n|------------------------------------------------------|--------------------|  \n|baseline value                                        |  FHR               |\n|accelerations                                         |  ACC               |\n|fetal_movement                                        |  FM                |\n|uterine_contractions                                  |  UC                |\n|light_decelerations                                   |  LD                |\n|severe_decelerations                                  |  SD                |\n|prolongued_decelerations                              |  PD                |\n|abnormal_short_term_variability                       |  ASTV              |\n|mean_value_of_short_term_variability                  |  MSTV              |\n|percentage_of_time_with_abnormal_long_term_variability|  ALTV              |\n|mean_value_of_long_term_variability                   |  MLTV              |\n|histogram_width                                       |  Hist_Width        |\n|histogram_min                                         |  Hist_Min          |\n|histogram_max                                         |  Hist_Max          |\n|histogram_number_of_peaks                             |  Hist_Peaks        |\n|histogram_number_of_zeroes                            |  Hist_Zeros        |\n|histogram_mode                                        |  Hist_Mode         |\n|histogram_mean                                        |  Hist_Mean         |\n|histogram_median                                      |  Hist_Median       |\n|histogram_variance                                    |  Hist_Variance     |\n|histogram_tendency                                    |  Hist_Tendency     |\n|fetal_health                                          |  FH                |\n\n","92bdbafd":"### 1. K-Nearest Neighbours (KNN) Classifier","c29a7051":"## Preprocess Data","da338696":"## Importing the Data","149dc96f":"## Process Data","850f08e4":"Before constructing our models, we will conduct an exploratory analysis of our data primarily making inferences based off of data visualizations. This will allow us to see if there are any important correlations we can leverage while developing our classifiers.","26cea2ad":"Now we will process the data to use it in all of our ML models. This requires doing:\n1. Split data into X (feature data) and y (target variable) \n2. Scaling data (using standard scaler)\n3. Splitting our X and y into their respective training and testing sets","ba1bcba9":"## Data Analysis","abaf4a35":"## Project Plan\n\n1. Import libraries required\n2. Import the data to the notebook\n3. Preprocess data to format it for analysis\n4. Generate data visualization for initial evaluation\n5. Process data for ML models\n6. Develop ML models using gridsearch\n7. Test and evaluate ML classifiers using gridsearch parameters\n\nThe classification models used will be:\n    1. K Nearest Neighbours\n    2. Support Vector Machine\n    3. Logistic Regression\n    4. Random Forest    \n\nThe evaluation methods used will be:\n    1. Confusion Matrix\n    2. CLassification Report (Precision, Recall and F1-Score)\n    3. Accuracy Rate\n ","310cd6ab":"These are the libraries required for data processing, data visualization, developing ML models, and developing evaluation metrics.","cbde871b":"### 2. Support Vector Machines (SVM) Classifier","2890e93b":"## Import Libraries","55081fd0":"## Test and Evaluate Models","fe6fd0b7":"## Develop and Train ML Models","6e44618f":"To access our data more easily, we will rename the column names to shorter labels.","93bcf08e":"### 4. Random Forest Classifier","6f36b0ee":"## Introduction\n\nIn this notebook we predict whether the health of a fetus is classified as normal, suspect, or pathological based on CTG data. To do this we will implement multiple machine learning classifiers and evaluate methods.\n\nOur goal is to successfully predict fetal health condition given CTG data. This means obtaining results with the highest accuracy and lowest misclassification and error rate.","90f0eb13":"### 3. Logistic Regression Classifier"}}