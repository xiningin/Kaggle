{"cell_type":{"cfa39feb":"code","bd798995":"code","31cda7df":"code","864f8fa4":"code","b7f60027":"code","f07594f2":"markdown","c1f35d53":"markdown"},"source":{"cfa39feb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bd798995":"from kaggle_environments.envs.hungry_geese.hungry_geese import greedy_agent, random_agent\nfrom kaggle_environments import evaluate, make\nenv = make(\"hungry_geese\")\nenv.reset()\nenv.run([random_agent, random_agent, random_agent, random_agent])\nenv.render(mode=\"ipython\", width=800, height=700)","31cda7df":"from kaggle_environments.envs.hungry_geese.hungry_geese import greedy_agent, random_agent\nfrom kaggle_environments import evaluate, make\nenv = make(\"hungry_geese\")\nenv.reset()\nenv.run([greedy_agent, greedy_agent, greedy_agent, greedy_agent])\nenv.render(mode=\"ipython\", width=800, height=700)","864f8fa4":"%%writefile submission.py\n\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col, adjacent_positions, translate, min_distance\nimport numpy as np\nfrom random import choice\n\n\nclass StrategicGreedyAgent:\n    def __init__(self, configuration: Configuration):\n        self.configuration = configuration\n        self.last_action = None\n\n    def __call__(self, observation: Observation):\n        rows, columns = self.configuration.rows, self.configuration.columns\n\n        food = observation.food\n        geese = observation.geese\n        opponents = [\n            goose\n            for index, goose in enumerate(geese)\n            if index != observation.index and len(goose) > 0\n        ]\n\n        # Don't move adjacent to any heads\n        head_adjacent_positions = {\n            opponent_head_adjacent\n            for opponent in opponents\n            for opponent_head in [opponent[0]]\n            for opponent_head_adjacent in adjacent_positions(opponent_head, columns, rows)\n        }\n        # Don't move into any bodies\n        bodies = {position for goose in geese for position in goose}\n        \n        position = geese[observation.index][0] \n        \n        # Don't get stuck\n        stuck_actions = []\n        for action in Action:\n            new_position = translate(position, action, columns, rows)\n            next_actions = [next_action for next_action in Action if next_action != action.opposite()]\n            num_moves = 0\n            for next_action in next_actions:\n                next_position = translate(new_position, next_action, columns, rows)\n                if next_position not in head_adjacent_positions and next_position not in bodies:\n                    num_moves += 1\n            if num_moves <= 1:\n                stuck_actions.append(next_action)\n        # safety layer - make sure goose can do something!\n        if len(stuck_actions) == 4:\n            stuck_actions = []\n        \n        # Move to the closest food\n        actions = {\n            action: min_distance(new_position, food, columns)\n            for action in Action\n            for new_position in [translate(position, action, columns, rows)]\n            if (\n                new_position not in head_adjacent_positions and\n                new_position not in bodies and\n                action not in stuck_actions and \n                (self.last_action is None or action != self.last_action.opposite())\n            )\n        }\n        \n        action = min(actions, key=actions.get) if any(actions) else choice([action for action in Action])\n        self.last_action = action\n        return action.name\n\n\ncached_greedy_agents = {}\n\n\ndef agent(obs, config):\n    index = obs[\"index\"]\n    if index not in cached_greedy_agents:\n        cached_greedy_agents[index] = StrategicGreedyAgent(Configuration(config))\n    return cached_greedy_agents[index](Observation(obs))\n\n","b7f60027":"from kaggle_environments.envs.hungry_geese.hungry_geese import greedy_agent, random_agent\nfrom kaggle_environments import evaluate, make\nenv = make(\"hungry_geese\")\nenv.reset()\nenv.run(['submission.py', 'submission.py', 'submission.py', 'submission.py'])\nenv.render(mode=\"ipython\", width=500, height=400)","f07594f2":"# Simple Baseline: Improve Greedy Agent with 1-step lookahead\n\nAs a start, I will make a simple improvement to the greedy agent: have it look 1 step ahead. \n\n1. Don't get stuck. In the game matching greedy agents against eachother, agents often lost when they got \"stuck\" and couldn't avoid moving into an adversary or a body. We can look ahead 1 step and avoid doing that, if possible.\n\n2. Be adversarial. If agent can move to make another agent \"stuck\", do so.","c1f35d53":"# Play default agents\nJust to get a sense of how the game works, play two types of default agents against each other: a greedy agent and a random agent."}}