{"cell_type":{"806d4598":"code","ed38578e":"code","b9da28fb":"code","d08bc2a3":"code","c69a0f67":"code","755b8daa":"code","ba91b4e4":"code","50d76319":"code","44ce6303":"code","436da84c":"code","efe15dbb":"code","f1d456cf":"code","d48792d5":"code","6fab0df8":"code","53659561":"code","26f85fe4":"code","ff883ae2":"code","05a0993f":"code","9f694b84":"code","459e8e72":"code","f070d481":"code","57e6f5f4":"code","d2e8e733":"code","e3f16b7b":"code","1f325589":"markdown","4450f1e7":"markdown","7bdf4b3c":"markdown","a59b1513":"markdown","549397f3":"markdown","4c3d1920":"markdown","3ce522b6":"markdown","fcfd832a":"markdown","f1c89984":"markdown","f8c1ae31":"markdown","f352fa00":"markdown","23d0ea29":"markdown","cee2e74c":"markdown","060ee735":"markdown","bc780377":"markdown","19474ffe":"markdown","874ec584":"markdown","3d3decec":"markdown","95c6049b":"markdown"},"source":{"806d4598":"import os\nfrom multiprocessing import Pool\n\nimport tqdm\nimport glob\n\nimport numpy as np\nimport pandas as pd\nimport cufflinks as cf\nimport kerastuner as kt\n\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.preprocessing import QuantileTransformer\n\ncf.go_offline()","ed38578e":"%load_ext memory_profiler","b9da28fb":"SEED = 0\nOUTPUT_DIR = '.'\n\nkaggle_input_path = '..\/input\/optiver-realized-volatility-prediction'\nif os.path.exists(kaggle_input_path):\n    DATA_DIR = kaggle_input_path\nelse:\n    # running locally\n    DATA_DIR = 'data'\n    \nDATA_DIR","d08bc2a3":"orders_files = sorted(glob.glob(f'{DATA_DIR}\/book_train.parquet\/*') + glob.glob(f'{DATA_DIR}\/book_test.parquet\/*'))\ntrades_files = sorted(glob.glob(f'{DATA_DIR}\/trade_train.parquet\/*') + glob.glob(f'{DATA_DIR}\/trade_test.parquet\/*'))\n\nfiles_df = pd.concat([\n    pd.Series(orders_files, name='orders_file'),\n    pd.Series(trades_files, name='trades_file')\n], axis=1)\nfiles_df['stock_id'] = (files_df['orders_file'].str.split('=').str[-1]).astype(int)\nfiles_df['stock_id2'] = (files_df['trades_file'].str.split('=').str[-1]).astype(int)\nfiles_df['is_train'] = files_df['orders_file'].str.contains('train')\nassert files_df.eval('stock_id == stock_id2').all(), 'StockId of files do not match'","c69a0f67":"def augment_trades(trades_df):\n    \"\"\"Supplement dataframe with columns computed by values from other columns\n    \n    Parameters\n    ----------\n    trades_df: pandas.DataFrame\n        DataFrame with columns: ['time_id', 'seconds_in_bucket', 'price', 'size', 'order_count']\n    \"\"\"\n    trades_df['notional'] = trades_df.eval('price * size')\n    trades_df['log_return'] = trades_df.groupby('time_id')['price'].apply(log_return)\n    return trades_df\n\n\ndef log_return(df):\n    \"\"\"Log return of prices\n    \n    Parameters\n    ----------\n    df: pandas.Series\n    \"\"\"\n    return df.apply(np.log).diff()\n\n\ndef augment_book(book_df):\n    \"\"\"Supplement dataframe with columns computed by values from other columns\n    \n    Parameters\n    ----------\n    trades_df: pandas.DataFrame\n        DataFrame with columns: \n        ['time_id', 'seconds_in_bucket', \n         'bid_price1', 'ask_price1', \n         'bid_price2', 'ask_price2', \n         'bid_size1', 'ask_size1', \n         'bid_size2', 'ask_size2']\n    \"\"\"\n    book_df['midpoint'] = book_df.eval('(bid_price1 + ask_price1)\/2')\n    book_df['weighted_midpoint_1'] = book_df.eval('(bid_price1*ask_size1 + ask_price1*bid_size1)\/(ask_size1 + bid_size1)')\n    book_df['weighted_midpoint_2'] = book_df.eval('(bid_price2*ask_size2 + ask_price2*bid_size2)\/(ask_size2 + bid_size2)')\n    book_df['log_return_1'] = book_df.groupby('time_id')['weighted_midpoint_1'].apply(log_return)\n    book_df['log_return_2'] = book_df.groupby('time_id')['weighted_midpoint_2'].apply(log_return)\n    book_df['volume_imbalance_1'] = book_df.eval('(ask_size1 - bid_size1)\/(ask_size1 + bid_size1)')\n    book_df['volume_imbalance_2'] = book_df.eval('(ask_size1 + ask_size2 - bid_size1 - bid_size2)\/(ask_size1 + bid_size1 + ask_size2 + bid_size2)')\n    book_df['abs_volume_imbalance_2'] = book_df.eval('(ask_size1 + ask_size2 - bid_size1 - bid_size2)').abs()\n    book_df['liquidity_near_bbo'] = book_df.eval('bid_size1 + bid_size2 + ask_size1 + ask_size2')\n    book_df['abs_mid_diff'] = book_df.eval('abs(weighted_midpoint_2 - weighted_midpoint_1)')\n    book_df['relative_spread'] = book_df.eval('(ask_price1 - bid_price1)\/midpoint')\n    book_df['bid_spread'] = book_df.eval('bid_price1 - bid_price2')\n    book_df['ask_spread'] = book_df.eval('ask_price1 - ask_price2')\n    return book_df","755b8daa":"BUCKET_TIME = 100\n\n\ndef count_unique(x):\n    \"\"\"Number of unique values in numpy array \/ pandas series\"\"\"\n    return len(np.unique(x))\n\n\ndef realised_vol(df):\n    \"\"\"Realised volatility where the input is the log return of a price series\"\"\"\n    return np.sqrt(df.apply(np.square).sum())\n\n\ndef order_metrics(book_df):\n    \"\"\"Order book related features\n    \n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame with `time_id` as index and each column is a computed feature.\n    \"\"\"\n    feature_dict = {\n        'log_return_1': [realised_vol],\n        'log_return_2': [realised_vol],\n        'abs_mid_diff': [np.mean],\n        'relative_spread': [np.mean],\n        'bid_spread': [np.mean],\n        'ask_spread': [np.mean],\n        'abs_volume_imbalance_2': [np.mean],\n        'liquidity_near_bbo': [np.mean],\n        'weighted_midpoint_1': [np.mean],\n        \n        'time_id': ['count'], \n        'bid_size1': 'mean', \n        'ask_size1': 'mean', \n        'bid_size2': 'mean', \n        'ask_size2': 'mean', \n        'volume_imbalance_1': 'mean', \n        'volume_imbalance_2': 'mean',\n        'liquidity_near_bbo': 'mean'\n    }\n    \n    return _metrics(book_df, feature_dict)\n\n\ndef trade_metrics(trade_df):\n    \"\"\"Trade related features\n    \n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame with `time_id` as index and each column is a computed feature.\n    \"\"\"\n    feature_dict = {\n        'log_return': [realised_vol],\n        'seconds_in_bucket': [count_unique, 'count'],\n        'order_count': ['sum', 'mean'], \n        'size': ['sum', 'mean'], \n        'notional': ['sum', 'mean'], \n        'price': ['mean', 'first', 'last', 'min', 'max']\n    }\n    \n    results_df = _metrics(trade_df, feature_dict)\n    \n    suffixes = ['0', '300', 'bucket_0_100', 'bucket_100_200', 'bucket_200_300', 'bucket_300_400', 'bucket_400_500', 'bucket_500_600']\n    for suffix in suffixes:\n        results_df[f'vwap|mean|{suffix}'] = results_df[f'notional|mean|{suffix}'] \/ results_df[f'size|mean|{suffix}']\n    \n    return results_df\n\n\ndef _metrics(df, feature_dict):\n    \"\"\"Compute features on a dataframe\n    \n    Parameters\n    ----------\n    df: pandas.DataFrame\n        DataFrame with either order book data or trades data\n    feature_dict: dict\n        Dictionary of {<column name>: <list of aggregation methods to apply>}\n    \"\"\"\n    results = []\n\n    for earliest_time in [0, 300]:\n        result = df.query('seconds_in_bucket >= @earliest_time').groupby('time_id').agg(feature_dict)\n        result.columns = [f'{\"|\".join(col)}|{earliest_time}' for col in result.columns]\n        results.append(result)       \n        \n    \n    n_buckets = int(600\/BUCKET_TIME)\n    \n    for bucket_index in range(n_buckets):\n        start = BUCKET_TIME * bucket_index\n        end = BUCKET_TIME * (bucket_index + 1)\n        result = df.query('@start <= seconds_in_bucket < @end').groupby('time_id').agg(feature_dict)\n        result.columns = [f'{\"|\".join(col)}|bucket_{start}_{end}' for col in result.columns]\n        results.append(result)\n    \n    results_df = pd.concat(results, axis=1)\n    return results_df\n","ba91b4e4":"def compute_metrics(row):\n    trade_metrics_df = _compute_trade_metrics(row['trades_file'])\n    order_metrics_df = _compute_order_metrics(row['orders_file'])\n    metrics_df = pd.concat([trade_metrics_df, order_metrics_df], axis=1)\n    metrics_df = metrics_df.assign(stock_id=row['stock_id'], is_train=row['is_train']).reset_index().set_index(['stock_id', 'time_id']).reset_index()\n    return metrics_df\n\n\ndef _compute_trade_metrics(filename):\n    trades_df = pd.read_parquet(filename)   \n    trades_df = augment_trades(trades_df)\n    metrics_df = trade_metrics(trades_df)\n    return _prefix_columns(metrics_df, 'trade')\n\n\ndef _compute_order_metrics(filename):\n    book_df = pd.read_parquet(filename)   \n    book_df = augment_book(book_df)\n    metrics_df = order_metrics(book_df)\n    return _prefix_columns(metrics_df, 'order')\n\n\ndef _prefix_columns(df, prefix):\n    df.columns = [f'{prefix}|{col}' for col in df.columns]\n    return df\n\n\ndef merge_training_data(results_df):\n    train_target_df = pd.read_csv(f'{DATA_DIR}\/train.csv')\n    combined_df = pd.merge(results_df, train_target_df, on=['stock_id', 'time_id'], how='left').set_index(['stock_id', 'time_id', 'target', 'is_train']).reset_index()\n    return combined_df","50d76319":"%%time\n%%memit\n\nwith Pool() as pool:\n    input_rows = files_df.to_dict('records')\n    results = list(tqdm.tqdm(pool.imap(compute_metrics, input_rows), total=len(input_rows)))\n    results_df = pd.concat(results)\n    combined_df = merge_training_data(results_df)\n\n    # Save to file\n    (\n        combined_df\n        .reset_index(drop=True)\n        .astype('float32')\n        .astype({'stock_id': int, 'time_id': int, 'is_train': bool})\n        .to_feather(f'{OUTPUT_DIR}\/metrics3.feather')\n    )\n\n    del results\n    del results_df\n    del combined_df","44ce6303":"%%memit\n\ntry:\n    del df\nexcept NameError:\n    print('df not known')\n    pass\n\nCOLUMNS_TO_DROP = [\n    #'stock_id', \n    'time_id', \n    'target', \n    'is_train'\n]\n\ndf = pd.read_feather(f'{OUTPUT_DIR}\/metrics3.feather')\ndf_train = df.query('is_train').copy().sort_values(by='time_id').reset_index(drop=True)\ndf_test = df.query('not is_train').copy().reset_index(drop=True)","436da84c":"%%memit\n\ndef split_feature_target(df):\n    features_df = df.drop(columns=COLUMNS_TO_DROP)\n    target = df['target']\n    return features_df, target\n\n\nstock_id_mean_target_map = df_train.groupby(['stock_id'])['target'].mean()\nstock_id_mean_target_map\n\nstock_time_test_df = df_test[['stock_id', 'time_id']].copy()  # used in output file\n\nX_train, y_train = split_feature_target(df_train)\nX_test, _ = split_feature_target(df_test)\n\ncolNames = [col for col in list(df.columns)\n            if col not in {\"stock_id\", \"time_id\", \"target\", \"row_id\", \"is_train\"}]\n\n\nX_train['num_nan'] = X_train[colNames].isna().sum(axis=1)\nX_test['num_nan'] = X_test[colNames].isna().sum(axis=1)\n\ncolNames += ['num_nan']\n\nqt_train = []\nfor col in tqdm.tqdm(colNames):\n    qt = QuantileTransformer(random_state=21,n_quantiles=2000, output_distribution='normal')\n    X_train[col] = qt.fit_transform(X_train[[col]]).astype('float32')\n    X_test[col] = qt.transform(X_test[[col]]).astype('float32')  \n    qt_train.append(qt)\n    \nX_train = X_train.fillna(0).astype('float32').astype({'stock_id': int})\nX_test = X_test.fillna(0).astype('float32').astype({'stock_id': int})\n\nprint('transformed data')\n\n# clean up\ndel df\ndel df_train\ndel df_test\nprint('cleaned up df')","efe15dbb":"#suffixes = [f'bucket_{i*100}_{(i+1)*100}' for i in range(6)]\nbucket_suffixes = [\n    'bucket_0_100',\n    'bucket_100_200',\n    'bucket_200_300',\n    'bucket_300_400',\n    'bucket_400_500',\n    'bucket_500_600']\n\n\n#base_metrics = [row[:-13] for row in df.filter(like='|bucket_0_100').columns.tolist()]\nbase_metrics = [\n    'trade|log_return|realised_vol',\n    'trade|seconds_in_bucket|count_unique',\n    'trade|seconds_in_bucket|count',\n    'trade|order_count|sum',\n    'trade|order_count|mean',\n    'trade|size|sum',\n    'trade|size|mean',\n    'trade|notional|sum',\n    'trade|notional|mean',\n    'trade|price|mean',\n    'trade|price|first',\n    'trade|price|last',\n    'trade|price|min',\n    'trade|price|max',\n    'trade|vwap|mean',\n    'order|log_return_1|realised_vol',\n    'order|log_return_2|realised_vol',\n    'order|abs_mid_diff|mean',\n    'order|relative_spread|mean',\n    'order|bid_spread|mean',\n    'order|ask_spread|mean',\n    'order|abs_volume_imbalance_2|mean',\n    'order|liquidity_near_bbo|mean',\n    'order|weighted_midpoint_1|mean',\n    'order|time_id|count',\n    'order|bid_size1|mean',\n    'order|ask_size1|mean',\n    'order|bid_size2|mean',\n    'order|ask_size2|mean',\n    'order|volume_imbalance_1|mean',\n    'order|volume_imbalance_2|mean',\n]\n\n\ndef cubify(df):\n    \"\"\"Transform the data into a 3 dimensional tensor (<num samples>, <base_metrics>, <suffixes>) instead of a 2d representation\n    \n    Parameters\n    ----------\n    df: pandas.DataFrame\n    \n    Returns\n    -------\n    numpy.ndarray\n        Returns an array of shape (<num samples>, <base_metrics>, <suffixes>)\n    \"\"\"\n    cube = []\n\n    for base_metric in base_metrics:\n        metrics = [f'{base_metric}|{suffix}' for suffix in bucket_suffixes]\n        cube.append(df[metrics].values)\n\n    arr = np.swapaxes(np.array(cube), 0, 1)\n    return arr","f1d456cf":"%%memit\npass","d48792d5":"def get_prediction(model, X):\n    \"\"\"Get prediction of a model given an input X\"\"\"\n    X_cat, X = X\n    if isinstance(X, pd.DataFrame):\n        X = X.values\n    y_predict = np.mean(\n        model([X_cat.values, X], training=False)[2].numpy(), \n    axis=1)\n\n    results = pd.Series(y_predict, index=X_cat.index)\n    results.name = 'prediction'\n    return results\n    \n\ndef get_result(model, X, y):\n    \"\"\"Get the RMSPE of a model for a given input `X` and target `y`\"\"\"\n    results = get_prediction(model, X)\n    return score(results, y)\n\n\ndef score(results, y):\n    results_df = pd.concat([results, y], axis=1)\n    results_df['sq_pc_error'] = results_df.eval('(target - prediction) \/ target').apply(np.square)\n    return np.sqrt(results_df['sq_pc_error'].sum() \/ results_df['sq_pc_error'].shape[0])","6fab0df8":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, GaussianNoise\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n\nMAX_ENCODING_VAL = 127  # max stock id + 1\n\n\ndef create_model(hidden_units, \n                 dropout_rates,\n                 learning_rate, \n                 num_columns, \n                 num_labels, \n                 stock_embedding_size,\n                 kernel_size,\n                 num_kernels):\n    \n    stock_id_input = tf.keras.Input(shape=(1,), name='stock_id')\n    features_input = tf.keras.layers.Input(shape=(len(base_metrics), len(bucket_suffixes), 1,))\n    \n    cnn_layer = tf.keras.layers.Conv1D(filters=num_kernels,\n                                       kernel_size=kernel_size,\n                                       activation='swish',\n                                       input_shape=(\n                                           len(base_metrics),\n                                           len(bucket_suffixes),\n                                       )\n                                      )(features_input)\n    convolved_features = tf.keras.layers.Flatten()(cnn_layer)\n\n\n    stock_embedded = tf.keras.layers.Embedding(MAX_ENCODING_VAL, \n                                               stock_embedding_size, \n                                               input_length=1, \n                                               name='stock_embedding')(stock_id_input)\n\n    stock_flattened = tf.keras.layers.Flatten()(stock_embedded)\n    concat_input = tf.keras.layers.Concatenate()([stock_flattened, convolved_features])\n    norm_input = tf.keras.layers.BatchNormalization()(concat_input)\n\n    encoder = tf.keras.layers.GaussianNoise(dropout_rates[0])(norm_input)\n    encoder = tf.keras.layers.Dense(hidden_units[0])(encoder)\n    encoder = tf.keras.layers.BatchNormalization()(encoder)\n    encoder = tf.keras.layers.Activation('swish')(encoder)\n\n    decoder = tf.keras.layers.Dropout(dropout_rates[1])(encoder)\n    decoder = tf.keras.layers.Dense(num_columns, name = 'decoder')(decoder)\n    \n    x_ae = tf.keras.layers.Dense(hidden_units[1])(decoder)\n    x_ae = tf.keras.layers.BatchNormalization()(x_ae)\n    x_ae = tf.keras.layers.Activation('swish')(x_ae)\n    x_ae = tf.keras.layers.Dropout(dropout_rates[2])(x_ae)\n    \n    out_ae = tf.keras.layers.Dense(num_labels, activation = 'linear', name = 'ae_action')(x_ae)\n    \n    x = tf.keras.layers.Concatenate()([norm_input, encoder])\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(dropout_rates[3])(x)\n    \n    for i in range(2, len(hidden_units)):\n        x = tf.keras.layers.Dense(hidden_units[i])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation('swish')(x)\n        x = tf.keras.layers.Dropout(dropout_rates[i + 2])(x)\n    \n    out = tf.keras.layers.Dense(num_labels, activation = 'linear', name = 'action')(x)\n\n    \n    model_encoder = tf.keras.models.Model(inputs=[stock_id_input, features_input], outputs=encoder)\n    \n    model = tf.keras.models.Model(inputs=[stock_id_input, features_input], outputs=[decoder, out_ae, out])\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n        loss=tf.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mean_squared_error\"),\n        loss_weights=[1, 1, 1],\n    )\n\n    return model, model_encoder\n\n\n\n\n#############\n# Callbacks #\n#############\n\nclass NEpochLogger(tf.keras.callbacks.Callback):\n    \"\"\"\n    A Logger that log average performance per `display` steps.\n    https:\/\/github.com\/keras-team\/keras\/issues\/2850#issuecomment-371353851\n    \"\"\"\n    def __init__(self, display):\n        self.display = display\n\n    def on_epoch_end(self, epoch, logs={}):       \n        if epoch % self.display == 0:\n            print(f'Epoch {epoch}  \\t|\\t', {k: round(v, 6) for k, v in logs.items()})\n            logs.clear()\n\n\ndef get_callbacks():\n    callback_early_stopping = tf.keras.callbacks.EarlyStopping(\n        monitor='val_action_loss', \n        patience=50,\n        verbose=1,\n        mode='min', \n    )\n\n    callback_plateau = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor='val_action_loss', \n        factor=0.5, \n        patience=10, \n        verbose=1,\n        mode='min',\n        min_lr=1e-7,\n        cooldown=5,\n    )\n    callback_epoch_logger = NEpochLogger(50)\n    \n    return [callback_early_stopping, callback_plateau, callback_epoch_logger]","53659561":"def get_default_parameters():\n    return {\n        'dropout_rates': [0.022391, # noise\n                          0.05, 0.05, 0.05, 0.05, 0.05, 0.05],\n        'hidden_units': [\n            20,  # number of encoded features\n            50,  # layer for output of encoded features\n            64, 10  # layers for output model\n        ],\n        'learning_rate': 1e-3,\n        'num_columns': X_train.shape[1] - 1,\n        'num_labels': 1,\n        'stock_embedding_size': 50,\n        'kernel_size': 3,\n        'num_kernels': 4,\n    }\n\n\nmodel, encoder = create_model(**get_default_parameters())\nmodel.summary()\n\ntf.keras.utils.plot_model(\n    model,\n    show_shapes=False,\n    show_dtype=False,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)","26f85fe4":"import gc\ngc.collect()","ff883ae2":"class CVTuner(kt.engine.tuner.Tuner):       \n    \n    def run_trial(self, trial, X, y, batch_size=32, epochs=1, callbacks=None):\n        \"\"\"Run trial with cross validation\n        \n        Note: X data is transformed before using in model in the following way:\n        'stock_id' column is split out from the main bulk of the features\n        The other features are reshaped into a tensor such that \n          - horizontally adjacent cells are the same feature but for a different time bucket\n          - vertically adjacent cells are different features\n        \"\"\"\n       \n        tf.random.set_seed(SEED)\n        np.random.seed(SEED)\n        \n        val_losses = []\n        oof_scores = []\n        \n        kf = KFold(\n            n_splits=5,\n            shuffle=False\n        )\n        \n        for fold, (train_index, validate_index) in enumerate(kf.split(X_train, y_train)):\n            X = X_train.loc[train_index]\n            y = y_train.loc[train_index]\n            X_val = X_train.loc[validate_index]\n            y_val = y_train.loc[validate_index]\n\n            X_cat = X.pop('stock_id')\n            X_cat_val = X_val.pop('stock_id')\n            \n            X_cube = cubify(X)\n            X_cube_val = cubify(X_val)\n            \n            model = self.hypermodel.build(trial.hyperparameters)\n                            \n            sample_weight = 1\/y**2\n            hist = model.fit([X_cat, X_cube], [X, y, y], \n                             epochs=epochs,\n                             sample_weight=sample_weight,\n                             batch_size=batch_size,\n                             validation_data=([X_cat_val, X_cube_val], [X_val, y_val, y_val], 1\/np.square(y_val)),\n                             callbacks=callbacks,\n                             shuffle=True,\n                             verbose=0)\n\n            val_losses.append([hist.history[k][-1] for k in hist.history])\n            oof_score = get_result(model, [X_cat_val, X_cube_val], y_val)\n            oof_scores.append(oof_score)\n            print(f'Fold {fold}, oof_score: {round(oof_score, 4)}')\n            \n        val_losses = np.asarray(val_losses)\n        metrics = {k:np.mean(val_losses[:,i]) for i,k in enumerate(hist.history.keys())}\n        metrics['oof_score'] = np.mean(oof_scores)\n        \n        self.oracle.update_trial(trial.trial_id, metrics)\n        self.save_model(trial.trial_id, model)\n\n\ndef get_create_model(hp):\n    # overwrite parameters for hyperparameter tuning\n    parameters = get_default_parameters()\n    #     parameters['stock_embedding_size'] = hp.Int('stock_embedding_size', 2, 130)\n    #     parameters['kernel_size'] = hp.Int('kernel_size', 2, 6)\n    #     parameters['num_kernels'] = hp.Int('num_kernels', 2, 20)\n    parameters['hidden_units'][2] = hp.Int('num_hidden_units_out_layer_1', 16, 200)\n    return create_model(**parameters)[0]\n\n\ndef get_tuner(project_name, max_number_of_trials):\n    return CVTuner(\n        hypermodel=get_create_model,\n        oracle=kt.oracles.BayesianOptimization(\n            objective= kt.Objective('oof_score', direction='min'),\n            num_initial_points=7,\n            max_trials=max_number_of_trials),\n        project_name=project_name,\n    )\n\ndef hyper_param_search(X, y, project_name, max_number_of_trials):\n    tuner = get_tuner(project_name, max_number_of_trials)\n    tuner.search((X,), (y,), batch_size=2000, epochs=1000, callbacks=get_callbacks())\n\n    hp = tuner.get_best_hyperparameters(1)[0]\n    display(tuner.project_name)\n    display(hp.values)\n    return tuner\n\n\n    # Run hyperparameter tuning\n# tuner = hyper_param_search(X_train, y_train, 'hyperparameter_cnn_num_hidden_units_out_layer_1', max_number_of_trials=30)\n\n\n    # Plot Results\n# tuner = get_tuner(project_name='hyperparameter_cnn_num_hidden_units_out_layer_1', max_number_of_trials=0)\n# hp_results_df = pd.DataFrame([{'score': row.score, **row.hyperparameters.values} for row in tuner.oracle.get_best_trials(-1)])\n# hp_results_df.set_index('num_hidden_units_out_layer_1').sort_index().iplot()","05a0993f":"%%time\nimport numpy as np\nimport tensorflow as tf\nimport random as python_random\n\nnp.random.seed(SEED)\npython_random.seed(SEED)\ntf.random.set_seed(SEED)\n\n\nkf = KFold(\n    n_splits=5,\n    shuffle=False\n)\nmodels = []\n\nfor fold, (train_index, validate_index) in enumerate(kf.split(X_train, y_train)):\n    X = X_train.loc[train_index]\n    y = y_train.loc[train_index]\n    X_val = X_train.loc[validate_index]\n    y_val = y_train.loc[validate_index]\n    \n    X_cat = X.pop('stock_id')\n    X_cat_val = X_val.pop('stock_id')   \n\n    sample_weight = 1\/y**2\n\n    X_cube = cubify(X)\n    X_cube_val = cubify(X_val)\n\n    model, encoder = create_model(**get_default_parameters())\n    model.fit([X_cat, X_cube], [X, y, y], \n          epochs=1000,\n          sample_weight=sample_weight,\n          batch_size=2000,\n          validation_data=([X_cat_val, X_cube_val], [X_val, y_val, y_val], 1\/np.square(y_val)),\n          callbacks=get_callbacks(),\n          shuffle=True,\n          verbose=0)\n\n    fold_score = get_result(model, [X_cat, X_cube], y)\n    oof_score = get_result(model, [X_cat_val, X_cube_val], y_val)\n\n    print('\\n')\n    print(f'fold {fold}, out of fold score: {oof_score}')\n    print('\\n\\n')\n    models.append({'model': model, 'fold_score': fold_score, 'oof_score': oof_score})","9f694b84":"print(np.mean([row['oof_score'] for row in models]).round(4))\npd.DataFrame(models)","459e8e72":"predictions = []\n\nX_final = X_test.copy(deep=True)\nX_final_cat = X_final.pop('stock_id')\nX_final_cube = cubify(X_final)\n\nfor model_map in models:\n    model = model_map['model']\n    predictions.append(get_prediction(model, [X_final_cat, X_final_cube]))\n    \nprediction = pd.concat(predictions, axis=1).median(axis=1)\nprediction.name = 'prediction'","f070d481":"prediction_df = pd.concat([stock_time_test_df, prediction], axis=1).astype({'prediction': 'float64'})\nprediction_df","57e6f5f4":"prediction_df.dtypes","d2e8e733":"submission_df = pd.read_csv(f'{DATA_DIR}\/test.csv')\nsubmission_df = submission_df.merge(prediction_df, how='left', on=['stock_id', 'time_id'])\n\n# Fill null predictions with mean stock target and then mean target\nsubmission_df['prediction'] = submission_df['prediction'].fillna(submission_df['stock_id'].map(stock_id_mean_target_map))\nsubmission_df = submission_df[['row_id', 'prediction']].rename(columns={'prediction': 'target'})\nsubmission_df.to_csv('submission.csv', index=False)","e3f16b7b":"submission_df","1f325589":"## Hyperparameter tuning\n\n(The line that runs this is currently commented out)","4450f1e7":"## Load Dataset","7bdf4b3c":"# Modelling","a59b1513":"# Overview\n\n## Feature Engineering\n### Order and Trade features\nFor each stock, features were generated by grouping orders\/trades by `time_id` and performing an aggregation function on a given column.  This methodology was applied to data chunked into separate time buckets.\n\n### Market Aggregates\nAlthough I found the inclusion of market aggregates decreased the cross validation score, the public leaderboard score was significantly worse.  \nI suspect there was some leakage that I did not account for in the cross validation or there is some technical difference in the test set that does not work with the approach I tried.\n\n\n## Model\n\n### Structure\nThe model used is a Neural network (NN) that combines a couple of different ideas:\n\n* Encoded features are used with the original features in the final output NN\n* `stock_id` values are transformed with an Embedding layer - this could group similar stocks together and also reduces the dimensional space in comparison to one-hot-encoding\n* Features have been computed for different time buckets e.g. (data between 0 seconds and 100 seconds, etc).  A 1d convolutional layer is applied along the time-bucket dimension of these features.\n\nHigh-level structure\n```\nInput Features -> Encoded Features -> Decoded Features -> output_0\n       \\            \/\n       Concat Features -> (Layers) -> output_final\n```\nAuto-encoder model attempts to minimise the error of `Decoded Features`, `output_0` and `output_final` with respect to, input features, target, and target.\n\n\n### Training Model\n\n* A sample weighting of `1\/y^2` was used such that root mean square percentage error is minimised (as specified in https:\/\/www.kaggle.com\/c\/optiver-realized-volatility-prediction\/overview\/evaluation)\n* Dynamic Learning Rate - When validation errors are no longer improving the learning rate is reduced to help converge towards better values\n* Early stopping - When validation errors are no longer improving (within a given number of epochs) the training is stopped early\n\n\n## Cross Validation\nKFold cross validation was used with 5 folds.  \nData was sorted by time_id and KFolds were not shuffled - this was chosen such that the time_ids included in the validation set were nearly entirely distinct from the training set\n\n\n## Hyperparameter Tuning\nKeras Tuner with a custom `run_trial` method was used for tuning hyperparameters.  The custom `run_trial` method utilises KFold cross validation as described above and the score of a trial is the mean of the out-of-fold (OOF) scores.\n\n\n## Output Predictions\n\n* The median of each prediction from cross validation models is used.  The median was preferred here as there is an asymmetric relationship between prediction and score.  (e.g. using actual_value +\/- const. does not give the same score)\n* Null values were replaced by optimal constant value as described in my other notebook here - https:\/\/www.kaggle.com\/cldavies\/single-value-baseline\n\n\n## References\n\n* Model - https:\/\/www.kaggle.com\/gogo827jz\/jane-street-supervised-autoencoder-mlp\/comments#Jane-Street:-Supervised-Autoencoder-MLP\n* Metrics - https:\/\/www.kaggle.com\/tommy1028\/lightgbm-starter-with-feature-engineering-idea\n\n\n------","549397f3":"## Transform \/ Normalise Features","4c3d1920":"## Selected Features and Reshaping","3ce522b6":"## Summary and Plot Model","fcfd832a":"## Cross Validation (5-fold KFold)","f1c89984":"## Generate Metrics","f8c1ae31":"### Imports","f352fa00":"# OOF Score","23d0ea29":"## Augment columns","cee2e74c":"### Constants\n","060ee735":"## Order and Trade Metrics","bc780377":"## Compute Metrics","19474ffe":"## Auto-encoder Model\n\n(plot of model below)","874ec584":"## Create Submission File","3d3decec":"## Data files","95c6049b":"## Scoring"}}