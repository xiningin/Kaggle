{"cell_type":{"0a70a028":"code","5f4ae679":"code","ad7c6490":"code","4eb0ae60":"code","3006804b":"code","daecf03d":"code","0d1db335":"code","5c50934d":"code","2ff7cd41":"code","b1c1bb85":"code","ecde9310":"code","00285e06":"code","179aa5be":"code","24b08b13":"code","0b9acabc":"code","00dd913e":"code","8c448d7c":"code","31619bad":"markdown","cba2b14d":"markdown","2dbb086b":"markdown","a38ba0cb":"markdown","d04e47b5":"markdown","d4676579":"markdown","7ebe166b":"markdown","0fcc6790":"markdown","b8495133":"markdown","1697e02b":"markdown"},"source":{"0a70a028":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","5f4ae679":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import svm\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import cross_val_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns","ad7c6490":"df = pd.read_csv(\"..\/input\/winequality-red.csv\")\ndf.head()","4eb0ae60":"df.info()","3006804b":"df.describe()","daecf03d":"scaler = StandardScaler()\nd = scaler.fit_transform(df) \nd = pd.DataFrame(data=d, columns=df.columns)\nprint(scaler.mean_)\n# scaler.transform(d)","0d1db335":"sns.pairplot(d)","5c50934d":"# g = sns.PairGrid(d)\n# g.map_diag(plt.hist)\n# g.map_offdiag(plt.scatter);","2ff7cd41":"g = sns.clustermap(d, \n#                    method='average',\n#                    metric='euclidean', \n                   z_score=None, \n                   standard_scale=1, \n                   figsize=None, \n                   row_cluster=True, \n                   col_cluster=True, \n                   row_linkage=None, \n                   col_linkage=None, \n                   row_colors=None, \n                   col_colors=None, \n                   mask=None,\n#                    cmap=\"mako\"\n                   robust=True\n                  )","b1c1bb85":"sns.set(style=\"white\")\nsns.clustermap(d.corr(), \n               pivot_kws=None, \n#                method='average', \n#                metric='euclidean', \n               z_score=None, \n               standard_scale=None,\n               figsize=None,\n               cbar_kws=None, \n               row_cluster=True, \n               col_cluster=True, \n               row_linkage=None, \n               col_linkage=None,\n               row_colors=None, \n               col_colors=None, \n               mask=None,\n               center=0,\n               cmap=\"vlag\",\n               linewidths=.75, \n#                figsize=(13, 13)\n              )\n","ecde9310":"# Compute the correlation matrix\ncorr = d.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nsns.set(style=\"white\")\nsns.heatmap(corr,\n         vmin=None,\n         vmax=None,\n         cmap=cmap,\n         center=None,\n         robust=True,\n         annot=True, \n#          fmt='.2g',\n         annot_kws=None, \n#          linewidths=0.5, \n#          linecolor='white',\n         cbar=True,\n         cbar_kws={\"shrink\": .5},\n         cbar_ax=None, \n         square=True, \n         xticklabels='auto',\n         yticklabels='auto', \n         mask=mask, \n         ax=None)","00285e06":"y = df[\"quality\"]\ndf.drop([\"quality\"], axis=1)\nX = df.values","179aa5be":"def print_performance(clf):\n    # print(\"*\"*100)\n    # print(\"{}{}{}\".format(\"*\"*40,\"Performance\", \"*\"*40))\n    print(\"{}\".format(\"Performance\"))\n    print(\"*\"*100)\n    print(\"Score            : {}\".format(clf.score(X, y)))\n    print(\"Best Estimator   : {}\".format(clf.best_estimator_))\n    print(\"Best Score       : {}\".format(clf.best_score_))\n    print(\"Best Params      : {}\".format(clf.best_params_))\n    print(\"Best Index       : {}\".format(clf.best_index_))\n    # print(\"Scorer           : {}\".format(clf.scorer_))\n    print(\"Refit Time       : {}\".format(clf.refit_time_))\n    # print(\"CV Results       : {}\".format(clf.cv_results_))\n\n    params = clf.get_params()\n    best_estimator = clf.best_estimator_\n    cv_results = clf.cv_results_\n    \n    return params, best_estimator, cv_results","24b08b13":"# parameters = {'kernel':('linear', 'poly', 'rbf', 'sigmoid', 'precomputed'), \n#               'degree': np.arrange(10),\n#               'C':np.arrange(10)}\n\nparameters = {'kernel':('linear', 'rbf'), \n              'degree': [1, 10],\n              'C': [1, 10]}\n\n# svr = svm.SVR(kernel='rbf',\n#               degree=3, \n#               gamma='auto',\n#               coef0=0.0,\n#               tol=0.001,\n#               C=1.0, \n#               epsilon=0.1,\n#               shrinking=True,\n#               cache_size=200, \n#               verbose=False, \n#               max_iter=-1)\n\n\nsvr = svm.SVR(gamma='auto')\n\nclf = GridSearchCV(estimator=svr, \n                   param_grid=parameters,\n                   scoring=None, \n                   fit_params=None, \n                   n_jobs=None,\n                   iid='warn',\n                   refit=True,\n                   cv=5,\n                   verbose=0,\n                   pre_dispatch='2*n_jobs',\n                   error_score='raise-deprecating',\n                   return_train_score='warn')\n\nclf.fit(X, y)\n \nparams, best_estimator, cv_results = print_performance(clf)","0b9acabc":"parameters = {'kernel':('linear', 'rbf'), \n              'degree': np.arange(1, 10),\n              'C': np.arange(1, 10)}\n\n# svr = svm.SVR(kernel='rbf',\n#               degree=3, \n#               gamma='auto',\n#               coef0=0.0,\n#               tol=0.001,\n#               C=1.0, \n#               epsilon=0.1,\n#               shrinking=True,\n#               cache_size=200, \n#               verbose=False, \n#               max_iter=-1)\n\nsvr = svm.SVR(gamma='auto')\n\nclf = GridSearchCV(estimator=svr, \n                   param_grid=parameters,\n                   scoring=None, \n                   fit_params=None, \n                   n_jobs=-1,\n                   iid='warn',\n                   refit=True,\n                   cv=5,\n                   verbose=1,\n                   pre_dispatch='2*n_jobs',\n                   error_score='raise-deprecating',\n                   return_train_score='warn')\n\n\nclf.fit(X, y)\n\nparams, best_estimator, cv_results = print_performance(clf)","00dd913e":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nreg = LinearRegression()\n\n# svr = svm.SVR(kernel='linear',\n#               degree=3, \n#               gamma='auto',\n#               coef0=0.0,\n#               tol=0.001,\n#               C=1.0, \n#               epsilon=0.1,\n#               shrinking=True,\n#               cache_size=200, \n#               verbose=False, \n#               max_iter=-1)\n\n# best estimator found using grid search cv\nsvr = svm.SVR(C=1, cache_size=200, coef0=0.0, degree=1, epsilon=0.1, gamma='auto',\n  kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n\n# clf = svr\nclf = reg\n\nprint(\"Cross Val Score            : {}\".format(cross_val_score(clf, X, y, cv=5)))\n\nclf.fit(X_train, y_train)\nprint(\"Score (training data only) : {}\".format(clf.score(X_train, y_train)))\n\ny_pred = clf.predict(X_test)\nprint(\"Mean Squared Error         : {}\".format(mean_squared_error(y_test, y_pred)))\n      ","8c448d7c":"x = np.arange(len(y_pred))\nplt.plot(x, y_test-y_pred)\nplt.title(\"Prediction Difference (le-14)\")\nplt.show()","31619bad":"**Training**","cba2b14d":"Cluster map of all features","2dbb086b":"Heatmap of scaled feature correleation  ","a38ba0cb":"Cluster map of scaled feature correleation  ","d04e47b5":"Scale data for visualization","d4676579":"Grid search for best estimator and parameters in a range - (1, 10) for linear and radial kernel","7ebe166b":"Grid search for best estimator and parameters for linear and radial kernel","0fcc6790":"Linear regression has shown much better result","b8495133":"Plot of difference between actual value and predicted value without scaling","1697e02b":"Pair plot of all features"}}