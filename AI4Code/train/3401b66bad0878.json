{"cell_type":{"d96a4e91":"code","e9e8138a":"code","ce83cfc2":"code","32cd82c1":"code","caa5b196":"code","f1037144":"code","eb6aafe8":"code","971d9b93":"code","4f230d79":"code","d61ecd15":"code","e77116c5":"code","8e0ddfb9":"code","69fa4aa8":"code","a98c740d":"code","049daa23":"code","05e59e73":"code","0a5748b4":"code","e471dfe1":"code","eace9284":"code","8f825a3c":"code","1acd93ef":"code","cd0562a8":"code","62b0d928":"code","425178d3":"code","5d2c3026":"code","c025a93c":"code","6dbe592b":"code","8f2e7ca6":"code","61961587":"code","85c37887":"code","068c8b2c":"code","b042274b":"code","9317318a":"code","94ace8ee":"code","dc051ae5":"code","fb62d366":"code","3cdfe826":"code","c09709ea":"code","38fc931c":"code","cdc46218":"code","0b10111a":"code","7109ea21":"code","e1727194":"code","67ccfba9":"code","ca78c66a":"code","dfd2535a":"code","e840dd38":"code","87cbc605":"code","f9b7cee4":"code","07982fb6":"code","5fa467cf":"code","8de511a5":"code","d8e648c8":"code","7c3f5f78":"markdown","6181dc4a":"markdown","46109de8":"markdown","75c9b586":"markdown","87e40c0c":"markdown","8787cf37":"markdown","5a84537f":"markdown","0cba9f65":"markdown","05e79ded":"markdown","69f86973":"markdown","7cdbe840":"markdown","cf731727":"markdown","e9807687":"markdown","231e027c":"markdown","eb3b8ef5":"markdown","aa4b8ed1":"markdown","4d97b5f6":"markdown","bd8667f1":"markdown","5a493264":"markdown","f297c2cd":"markdown","d5e4553d":"markdown","7a37d2c9":"markdown","60685859":"markdown","c7088a66":"markdown","76b1ae0c":"markdown","1ec527c5":"markdown","41145275":"markdown","3344cf56":"markdown","d3d16287":"markdown","f82f4241":"markdown","681448a0":"markdown","ded15ce2":"markdown","72ad25cb":"markdown","1add58c3":"markdown","9da590a0":"markdown","d6f54d86":"markdown","aadcc9a1":"markdown","468e3d13":"markdown","1b275ce8":"markdown","d29c12f5":"markdown","808e14af":"markdown","b238404c":"markdown","683440b0":"markdown"},"source":{"d96a4e91":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e9e8138a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer\n# from sklearn.preprocessing import Imputer\n# Imputer 3 \ubc84\uc804 \uc774\ud6c4 \uc0ac\uc6a9\ub418\uc9c0 \uc54a\uc73c\uba70 0.22\uc5d0\uc11c \uc81c\uac70 \ub428\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.utils import shuffle\nfrom sklearn.ensemble import RandomForestClassifier\n\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)","ce83cfc2":"DEBUG = True","32cd82c1":"if DEBUG:\n    NROWS = 50000\nelse :\n    NROWS = None","caa5b196":"%%time\n\ntrain = pd.read_csv('..\/input\/porto-seguro-safe-driver-prediction\/train.csv', nrows=NROWS)\ntest = pd.read_csv('..\/input\/porto-seguro-safe-driver-prediction\/test.csv', nrows=NROWS)\n\ntrain = train.sample(frac=0.2)","f1037144":"train.shape","eb6aafe8":"from sklearn.model_selection import StratifiedKFold\n\nfold = StratifiedKFold(n_splits=10, random_state=1980)","971d9b93":"for trn_idx, val_idx in fold.split(train, train['target']):\n    break","4f230d79":"%%time\n#  NROWS = 10000\ntrain = pd.read_csv('..\/input\/porto-seguro-safe-driver-prediction\/train.csv', nrows=NROWS)\ntest = pd.read_csv('..\/input\/porto-seguro-safe-driver-prediction\/test.csv', nrows=NROWS)","d61ecd15":"train = train.iloc[trn_idx]","e77116c5":"train","8e0ddfb9":"train.head()","69fa4aa8":"train.shape","a98c740d":"train.tail()","049daa23":"cat_cols = [col for col in train.columns if 'cat' in col]","05e59e73":"cat_cols","0a5748b4":"train[cat_cols[0]].value_counts()","e471dfe1":"for col in cat_cols:\n#     print(col, train[col].value_counts().shape[0])\n    print(col, train[col].nunique())","eace9284":"train.drop_duplicates()\ntrain.shape","8f825a3c":"test.shape","1acd93ef":"train.info()","cd0562a8":"data = []\n\nfor f in train.columns:\n    if f == 'target':\n        role = 'target'\n    elif f == 'id':\n        role = 'id'\n    else:\n        role = 'input'\n        \n    # Defining the level\n    if 'bin' in f or f == 'target':\n        level = 'binary'\n    elif 'cat' in f or f == 'id':\n        level = 'nominal'\n    elif train[f].dtype == float:\n        level = 'interval'\n    elif train[f].dtype == int:\n        level = 'ordinal'\n        \n    # Initialize keep to True for all variables except for id\n    keep = True\n    if f == 'id':\n        keep = False\n        \n    #Defining the data type\n    dtype = train[f].dtype\n    \n    #Creating a Dict that contains all the metadata for the variable\n    f_dict = {\n        'varname' : f,\n        'role' : role,\n        'level' : level,\n        'keep' : keep,\n        'dtype' : dtype\n    }\n    data.append(f_dict)","62b0d928":"meta = pd.DataFrame(data, )","425178d3":"meta = pd.DataFrame(data, columns=['varname', 'role', 'level', 'keep', 'dtype'])","5d2c3026":"meta.set_index('varname', inplace=True)","c025a93c":"meta","6dbe592b":"meta.loc[(meta.level=='nominal') & (meta.keep)].index","8f2e7ca6":"pd.DataFrame({'count':meta.groupby(['role', 'level'])['role'].size()}).reset_index()","61961587":"meta.groupby(['role', 'level'])['role'].size()","85c37887":"v = meta[(meta.level == 'interval') & (meta.keep)].index\ntrain[v].describe()","068c8b2c":"train.isnull().sum()","b042274b":"v = meta[(meta.level == 'ordinal') & (meta.keep)].index\ntrain[v].describe()","9317318a":"desired_apriori = 0.1\n\n# Get the indices per target value\nidx_0 = train[train.target == 0].index\nidx_1 = train[train.target == 1].index\n\n# Get original number of records per target value\nnb_0 = len(train.loc[idx_0])\nnb_1 = len(train.loc[idx_1])\n\n# Calculate the undersampling rate and resulting number of records with target=0\n\nundersampling_rate = ((1-desired_apriori)*nb_1) \/ (nb_0*desired_apriori)\n\nundersampled_nb_0 = int(undersampling_rate*nb_0)\nprint('Rate to understand records with target=0: {}'.format(undersampling_rate))\nprint('Number of records with target=0 after undersampling: {}'.format(undersampled_nb_0))\n\n# Randomly select records with target=0 to get at the desired a priori\nundersampled_idx = shuffle(idx_0, random_state=37, n_samples=undersampled_nb_0)\n\n# Construct list with remaining indices\nidx_list = list(undersampled_idx) + list(idx_1)\n\n# Return undersample data frame\ntrain = train.loc[idx_list].reset_index(drop=True) # .reset_index(drop=True) \ub294 \uc778\ub371\uc2a4\uac12 \uc815\ub825\ud558\ub294 \ubc29\ubc95","94ace8ee":"\n#####################################\n# \uc218\uce58\uac00 \uc774\uc0c1\ud574\uc11c \ucc98\uc74c\ubd80\ud130 \ud655\uc778 \ud544\uc694\ud568\n#####################################\nvars_with_missing = []\n\nfor f in train.columns:\n    missings = train[train[f] == -1][f].count()\n    if missings > 0:\n        vars_with_missing.append(f)\n        missings_perc = missings\/train.shape[0]\n        \n        print('Variable {} has records ({:.2%}) with missing values'.format(f, missings, missings_perc))\n        \nprint('In total, there are {} variables with missing values'.format(len(vars_with_missing)))","dc051ae5":"# Dropping the variables with too many missing values\nvars_to_drop = ['ps_car_03_cat', 'ps_car_05_cat']\ntrain.drop(vars_to_drop, inplace=True, axis=1)\nmeta.loc[(vars_to_drop),'keep'] = False # Updateing the meta\n\n# imputing with the mean or mode\nfrom sklearn.impute import SimpleImputer\nmean_imp = SimpleImputer(missing_values=-1, strategy='mean', axis=0)\nmode_imp = SimpleImputer(missing_values=-1, strategy='most_frequent', axis=0)\ntrain['ps_reg_03'] = mean_imp.fit_transform(train[['ps_reg_03']]).ravel()\ntrain['ps_car_12'] = mean_imp.fit_transform(train[['ps_car_12']]).ravel()\ntrain['ps_car_14'] = mean_imp.fit_transform(train[['ps_car_14']]).ravel()\ntrain['ps_car_11'] = mean_imp.fit_transform(train[['ps_car_11']]).ravel()\n","fb62d366":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\n\nfor f in v:\n    dist_values = train[f].value_counts().shape[0]\n    print('Valiable {} has {} distinct values'.format(f, dist_values))","3cdfe826":"# Script by https:\/\/www.kaggle.com\/ogrellier\n# code : https:\/\/www.kaggle.com\/ogrellier\/python-target-encoding-for-categorical-features\n\ndef add_noise(series, noise_level):\n    return series * (1+noise_level * np.random.randn(len(series)))\n\ndef target_encode(trn_series=None,\n                 tst_series=None,\n                 target=None,\n                 min_samples_leaf=1,\n                 smoothing=1,\n                 noise_level=0):\n    \"\"\"\n    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n    https:\/\/kaggle2.blob.core.windows.net\/forum-message-attachments\/225952\/7441\/high%20catgoricals.pdf\n    trn_series : training categorical feature as a pd.Series\n    tst_series : test categorical feature as a pd.Series\n    target : target data as a pd.Series\n    min_samples_leaf (int) : minimum samples to take category average into account smoothing (int) : smoothing effect to balance categorical average vs prior\n    \"\"\"\n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    temp = pd.concat([trn_series, target], axis=1)\n    # Compute target mean\n    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n    # Compute smoothing\n    smoothing = 1 \/ (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) \/ smoothing))\n    # Apply average function to all target data\n    prior = target.mean()\n    # The bigger the count the less full_avg is taken into account\n    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n    # Apply averages to trn and tst series\n    ft_trn_series= pd.merge(\n        trn_series.to_frame(trn_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=trn_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_trn_series.index = trn_series.index\n    ft_tst_series = pd.merge(\n        tst_series.to_frame(tst_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=tst_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd\/merge does not keep the index so restore it\n    ft_tst_series.index = tst_series.index\n    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)","c09709ea":"train_encoded, test_encoded = target_encode(train[\"ps_car_11_cat\"],\n                            test[\"ps_car_11_cat\"],\n                            target=train.target,\n                            min_samples_leaf=100,\n                            smoothing=10,\n                            noise_level=0.01)\ntrain['ps_car_11_cat_te'] = train_encoded\ntrain.drop('ps_car_11_cat', axis=1, inplace=True)\nmeta.loc['ps_car_11_cat', 'keep'] = False # Updating the meta\ntest['ps_car_11_cat_te'] = test_encoded\ntest.drop('ps_car_11_cat', axis=1, inplace=True)","38fc931c":"v = meta[(meta.level == 'nominal') & (meta.keep)].index\n\nfor f in v:\n    plt.figure()\n    fig, ax = plt. subplots(figsize=(20,10))\n    # Calculate the precentage of target=1 per category value\n    cat_perc = train[[f, 'target']].groupby([f], as_index=False).mean()\n    cat_perc.sort_values(by = 'target', ascending=False, inplace=True)\n    # Bar plot\n    # Order the bars descending on target mean\n    sns.barplot(ax=ax, x=f, y='target', data=cat_perc, order=cat_perc[f])\n    plt.ylabel('% target', fontsize=18)\n    plt.xlabel(f, fontsize=18)\n    plt.tick_params(axis='both', which='major', labelsize=18)\n    plt.show();","cdc46218":"def corr_heatmap(v):\n    correlations = train[v].corr()\n    \n    # Creat color map ranging beween two colors\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n    \n    fig, ax = plt.subplots(figsize=(10,10))\n    sns.heatmap(correlations, cmap=cmap, vmax=1.0, center=0, fmt='.2f',\n                square=True, linewidths=.5, annot=True, cbar_kws={\"shrink\": .75})\n    plt.show();\n    \nv = meta[(meta.level == 'interval') & (meta.keep)].index\ncorr_heatmap(v)","0b10111a":"s = train.sample(frac=0.1)","7109ea21":"sns.lmplot(x='ps_reg_02', y='ps_reg_03', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","e1727194":"sns.lmplot(x='ps_car_12', y='ps_car_13', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","67ccfba9":"sns.lmplot(x='ps_car_12', y='ps_car_14', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","ca78c66a":"sns.lmplot(x='ps_car_15', y='ps_car_13', data=s, hue='target', palette='Set1', scatter_kws={'alpha':0.3})\nplt.show()","dfd2535a":"v = meta[(meta.level == 'ordinal') & (meta.keep)].index\ncorr_heatmap(v)","e840dd38":"v = meta[(meta.level =='nominal') & (meta.keep)].index\nprint('Before dummification we have {} variables in train.'.format(train.shape[1]))\ntrain = pd.get_dummies(train, columns=v, drop_first=True)\nprint('After dummification we have {} variables in train'.format(train.shape[1]))","87cbc605":"v = meta[(meta.level =='interval') & (meta.keep)].index\npoly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\ninteractions = pd.DataFrame(data=poly.fit_transform(train[v], columns=poly.get_feature_names(interactions.drop(v, axis=1, inplace=True)) #Remove the original columns\n#Concat the interaction variables tothe train data\nprint('Before creating interactions we have {} variables in train'.format(train.shape[1]))\ntrain = pd.concat([train, interactions], axis=1)\nprint('After creating interactions we have {} variables in train'.format(train.shape[1]))","f9b7cee4":"selector = VarianceThreshold(threshold=.01)\nselector.fit(train.drop(['id', 'target'], axis=1)) # Fit to train without id and target variables\n\nf = np.vectorize(lambda x : not x) # Function to toggle boolean array elements\n\nv = train.drop(['id', 'target'], axis=1).columns[f(selector.get_support())]\nprint(' {} variables have too low variance.'.format(len(v)))\nprint('These variables are {}'.format(list(v)))","07982fb6":"X_train = train.drop(['id', 'target'], axis=1)\ny_train = train['target']\n\nfeat_labels = X_train.columns\n\nrf = RandomForestClassifier(n_estimators=1000, random_state=0, n_jobs=-1)\n\nrf.fit(X_train, y_train)\nimportances = rf.feature_importances_\n\nindices = np.argsort(rf.feature_importances_)[::-1]\n\nfor f in range(X_train.shape[1]):\n    print(\"%2d) %-*s %f\" (f +1, 30, feat_labels[indices[f]], importances[indices[f]]))","5fa467cf":"sfm = SelectFromModel(rf, threshold='median', prefit=True)\nprint('Number of features before selection: {}'.format(X_train.shape[1]))\nn_features = sfm.trainsform(X_train).shape[1]\nprint('Numver of features after selection: {}'.format(n_features))\nselected_vars = list(feat_labels[sfm.get_support()])","8de511a5":"train = train[selected_vars + ['target']]","d8e648c8":"scaler = StandardScaler()\nscaler.fit_trainsform(train.drop(['target'], axis=1))","7c3f5f78":"## Selecting features with a Random Forest and SelectFromModel\n\nHere we'll base feature selection on the feature importances of a random forest. With Sklearn's SelectFromModel you can then specify how many variables you want to keep. You can set a threshold on the level of feature importance manually, Byt we'll simply select the top 50% best variables.\n\n> The code in the cell below is borrowed from the GitHub repo of Sebastian Raschka. This repo contains code samples of his book Python Machine Learning, which is an absolute must to read.","6181dc4a":"We have 59 variables and 595,212 rows. Let's see if we have the same number of variables in the test data.\n\nLet's see if there are duplicate rows in the training data.","46109de8":"# Data Quality Checks\n\n**Checking missing values**\n\nMissing are represented as -1","75c9b586":"* A priori in the train data is 3.645%, which is **strongly imbalanced.**\n* From the means we can condlude that for most variables the value is zero in most cases.","87e40c0c":"# Conclusion\nHopefully this notebook helped you with some tips on how to start with this competition. Feel free to vote for it. And if you have questions, post a comment.","8787cf37":"# Descriptive statistics\n\nWe can also apply the describe method on the dataframe. However, it doesn't make much sense to calculate the mean, std, ... on categorical variables and the id variable. We'll explore the categorical variables visually later.\n\nThanks to our meta file we can easily select the variables on which we want to compute the descriptive statistics. To keep things clear, we'll do this per data type","5a84537f":"## Interval variables\nchecking the correlations between interval variables. A heatmap is a good way to visualize the correlation between variables.\n\nThe code below is based on an example by Michael Waskom.","0cba9f65":"## Creating interaction variables","05e79ded":"## ps_car_12 and ps_car_13","69f86973":"We indeed see the following\n\n* binary variables\n* categorical variables of which the category values are integers\n* other variables with integer or float values\n* variables with -1 representing missing values\n* the target variable and an ID variable\n\nLet's look at the number of rows and columns in the rain data","7cdbe840":"Here is an except of the data description for the competition:\n\n* Features that belong to **similar groupings are tagged** as such in the feature names (e.g., ind, reg, car, calc),\n* Feature names include the postfix **bin** to indicate binary features and **cat** to indicate categorical features.\n* Features **without these designations are either continuous or ordinal.**\n* Values of **-1** indicate that the feature was **missing** from the observation.\n* The **target** columns signifies whether or not a claim was filed for that policy holder.\n\nOk, that's important information to get us started. Let's have a quick look at the first and last rows to confirm all of this.","cf731727":"So, creating dummy variavles adds 52 variables to the training set.","e9807687":"Again, with the info() method we see that the data type is integer or float. No null values are present in the data set. that's normal because missing values are replaced by -1. We'll look into that later.","231e027c":"**Interval variables**","eb3b8ef5":"## ps_reg_02 and ps_reg_03\nAs the regression line shows, there is a linear relationship between variables. Thanks to the *hue* parameter we can see that the regression lines for target=0 and target=1 are the same.","aa4b8ed1":"# Metadata\nTo facilitate the data management, we'll store meta-information about the variables in a DataFrame. This will be helpful when we want to select specific variables for analysis, visualization, modeling, ...\n\nConcretely we will store:\n\n* **role**: input, ID, target\n* **level**: nominal, interval, ordinal, binal\n* **keep**: True or False\n* **dtype**: int, float, str\n","4d97b5f6":"* **ps_car_03_cat abd ps_car_05_cat**have a large proportion of records with missing values. Remove these vaiables.\n* for the other categorical vaiables with missing values, we can leave the missing value -1 as such.\n* **ps_reg_03** (continuous) has missing values for 18% of all records. Replace by the mean.\n* **ps_car_11** (ordinal) has only 5 records with missing values. Replace by the mode.\n* ps_car_12 (continuous) has only 1 records with missing value. Replace by the mean.\n* **ps_car_14** (continuius) has missing values for 7% of all records. Replace by the mean.\n\nmissing value\ub294 \ud568\ubd80\ub85c \ucc44\uc6b0\uba74 \uc548\ub428\n\n**missing value \ucc44\uc6b0\ub294 \ubc29\ubc95**\n\uc138\ubd80\uc801\uc73c\ub85c \uadf8\ub8f9\ud654 \nmissing value \uc790\uccb4\ub97c \ud559\uc2b5","bd8667f1":"# Feature scaling\nAs mentioned before, we can apply standard scaling to the training data. Some classifiers perform better when this is done.","5a493264":"Only  **ps_car_11_cat** has many distinct values, although it is still reasonable.\n\n**EDIT** : nickycan made an excellent remark on the fact that my first solution could lead to data leakage He also pointed me to another kermel made by oliver which deals with that. I therefor replaced this part with the kernel of oliver. All credits go to him.\nIt is so great what you can learn by participating in the Kaggle competitions :)","f297c2cd":"# Loading data","d5e4553d":"# Feature engineering\n\n## Creating dummy variables\n\nThe values of the categorical variables do not represent any order or magnitude. For instance, category 2 is not twice the value of category 1. Therefore we can create dummy variables to deal with that. We drop the first dummy variable as this information can be derived from the other dummy variables generated for the categories of the original variable.","7a37d2c9":"# Feature selection\n\n## Removing features with low or zero variance\n\nPersonally, I prefer to let the classifier algorithm chose which features to keep. But there is one thing that we can do ourselves.\nThat is removing features with no or a very low variance. Shlearn has a handy method to do that: **VarianceThreshold**. By default it removes features with zero variance. This will not be applicable for this competition as we saw there are no zero-variance variables in the previous steps. But if we would remove features with less than 1% variance, we would remove 31 variables.","60685859":"* Example to extract all nominal variables that are not dropped","c7088a66":"## ps_car_12 and ps_car_14","76b1ae0c":"There are a strong correlations beween the vaiables:\n\n- ps_reg_02 and ps_reg_03 (0.7)   ### \uc65c \ub610 \ub0b4 \uc218\uce58\uc640 \ub2e4\ub974\uc9c0? \ub098\ub294 0.53 \ub098\uc624\ub294\ub370..###\n- ps_car_12 and ps_car13 (0.67)\n- ps_car_12 and ps_car14 (0.58)\n- ps_car_13 and ps_car15 (0.67)\n\n\nSeaborn has some handy plots to visualize the (linear) relationship between variables. We could use a pairplot to visualize the relationshop beween the variables. But because the heatmap already showed the limited number of correlated variables, we'll look at each of the highly correlated variables separately.\n\n**NOTE** : I take a sample of the train data to speed up the process.\n","1ec527c5":"**\uc778\ucf54\ub529\uc740 \ub2e4\uc591\ud55c \ubc29\ubc95\uc73c\ub85c \uc801\uc6a9\ud574 \ubcf8 \ud6c4, \ucd5c\uc801\ud654 \ud558\ub294 \uac83\uc774 \uac00\uc7a5 \uc88b\uc740 \ubc29\ubc95**\n\nFrequency Encoding\n\nMean Encoding - overfitting\uc744 \ub9c9\uae30 \uc704\ud574, \ub178\uc774\uc988\ub97c \ub123\uc5b4\uc8fc\uae30\ub3c4 \ud568\n\nTarget Encoding ","41145275":"With SelectFromModel we can specify which prefit classifier to use and what the threshold is for the feature importances.\n\n\nWith the *get_support* method we can then limit the number of variables in the train data.","3344cf56":"# Checking the cardinality of the categorical vairables\n\n\n* Cardinality refers to the number of differnect values in a variable\n* As we will creat dummy variables form the categorical variables later on.\n* We need to chec whether there are variables with many distinct values","d3d16287":"For the ordinal variables we do not see many correlations. we could, on the other hand, look at how the distrubutions are when grouping by the target value.","f82f4241":"## ps_car_13 and ps_car_15","681448a0":"## Checking the correlations beween ordinal variables","ded15ce2":"# Data at first sight","72ad25cb":"This adds extra interaction variables to the rain data. Thanks to the *get_feature_names* method we can assign column names to these new variables.","1add58c3":"We are missing one variable in the test set, but this is the target variable. So that's fine.\nLet's now invesigate how many variables of each type we have.\n\nSo later on we can create dummy variables for the 14 categorical variables. the bin variables are already binary and do not need dummification.","9da590a0":"As we can see from the **variables with missing values**, it is a good idea to keep the missing values as a aseparate category value, indtead of replacing them by the mode for instance. The customers with a missing value appear to have a much higher (in some cases much lower) probabiity to ask for an insurance claim.","d6f54d86":"# Exploartory Data Visualization (EDA)\n\n## Categorical variables\n\nLet's look into the categorical variables and the proportion of customers with target = 1","aadcc9a1":"Alright, so now what? How can we decide which of the correlated variables to keep? We could perform Principal Componet Analysis (PCA) on the variables to reduce the dimensions. In the AllState Claims Severity Competition I made this kernel to do that. But as the number of correlated variables is rather low, we will let the model do the heavy-lifting.","468e3d13":"* Introduction\n \n* This notebook aims at getting a good insight in the data for the Porto Seguro compitition. Besides that, it gives some tips and tricks to prepare your data for modeling. The notebook consists of the following main sections.\n\n* The notebook consists of the following main section","1b275ce8":"We would lose many variables if we would select based on variance. But because we do not have so many variables, we'll let the classifier shose. For data sets with many more viables this could reduce the processing time.\n\nSklearn also comes with other feature selection methods. One of these methods is SelecFromModel in which you let another classifier select the best features and continue with these. Below I'll show you how to do that with a Random Forest.","d29c12f5":"Below the number of ariables per role and level are displayed","808e14af":"* train : 600,000\n* test : 900,000","b238404c":"# Handling imbalanced classes\n\nAs we mentioned above the proportion of records with target=1 is far less than target=0. This can lead to a model that has great accuracy but does have any added value in practice. Two possible strategies to deal with this problem are:\n\n- oversampling records with target=1\n- undersampling records with target=0\n\nThere are many more strategies of course and MachineLearningMastery.com gives a nice overview. As we have a rather large training set. we can go for **undersampling**.","683440b0":"**reg variables**\n\n- only ps_reg_03 has missing values\n- the range (min to max) differs between the variables. We could apply scaling\n  (e.g. StandardScaler), but it depends on the classifier we will want to use.\n  \n  \n**car variables**\n\n - ps_car_12 and ps_car_15 have missing values\n - again, the range differs and we could apply scaling.\n \n \n**calc variables**\n \n - ps_car_12 and ps_car_15 have missing values\n - again, the range differs and we could apply scaling.\n \n \n**Overall**, we can see that the range of the interval variables is rather small. Perhaps some transformation (e.g. log) is already applied in order to anonymize the data?"}}