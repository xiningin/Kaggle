{"cell_type":{"b45b15bc":"code","e9411cc8":"code","24990454":"code","df41b807":"code","aa643306":"code","bb3b59ad":"code","399ab352":"code","453eafbe":"code","a7ee2ed1":"code","6b3e0e66":"code","023af57f":"code","95eb1980":"code","a3fea759":"code","5cb5860d":"code","25cf490e":"code","19ebe331":"code","ea693fe0":"code","73e07947":"code","395259e7":"code","24e214fb":"code","3cd476e1":"code","30bea334":"code","bed45a5d":"code","5d82b406":"code","6ed7f1cd":"code","0a8bdffc":"code","6dc3635d":"code","c958b3ae":"code","a3fbd294":"code","3997840a":"markdown","5889eb74":"markdown","d8a8b4e1":"markdown","d0a555d1":"markdown","3f4a767e":"markdown","3cb95962":"markdown","21bbd851":"markdown","53435fdd":"markdown","0d3065d6":"markdown","a539b9e0":"markdown","7583b7d2":"markdown"},"source":{"b45b15bc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e9411cc8":"tr = pd.read_csv('\/kaggle\/input\/banking-dataset-marketing-targets\/train.csv',sep=';')\nte = pd.read_csv('\/kaggle\/input\/banking-dataset-marketing-targets\/test.csv',sep=';')\n","24990454":"tr.info()","df41b807":"tr['y'].value_counts()","aa643306":"# check for nan values\ntr.isna().sum()","bb3b59ad":"tr.isnull().sum()","399ab352":"tr.describe(include='all')","453eafbe":"tr.head(5)","a7ee2ed1":"# convert all categorical features into numeric\ndf = tr.copy()\n","6b3e0e66":"df.convert_dtypes().dtypes                                                     \n","023af57f":"df.head(5)","95eb1980":"from sklearn.preprocessing import LabelEncoder\n\ndfc = df.apply(LabelEncoder().fit_transform)\ndfc.head(5)","a3fea759":"te = te.apply(LabelEncoder().fit_transform)\nte.head(5)","5cb5860d":"from sklearn.model_selection import train_test_split\nx = dfc\ny = dfc['y'].values\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state = 33)","25cf490e":"print(y_train, y_test)","19ebe331":"import matplotlib.pyplot as plt\n# colors = {'0':'rose', 1:'green'}\n# plt.scatter(x_test[:,0], x_test[:,1], c = y_test)\n# plt.show()","ea693fe0":"from sklearn.metrics import accuracy_score\nimport pandas as pd\n\n\ndef sample_data(params):\n    return random.sample(range(0,params),10)\n\ndef RandomSearchCV(x_train,y_train,classifier, param_range, folds):\n    \n    train_scores, test_scores = [], []\n    param_list = {'n_neighbors': sample_data(param_range)}\n\n    for k in tqdm(param_list['n_neighbors']):\n        train_score_folds, test_score_folds = [], []\n        for j in range(0, folds):\n            each_fold = (len(x_train)\/ folds)\n            limit = int(each_fold)\n            test_groups = list( set( list( range( each_fold*j, each_fold*(j+1) ) ) ) )\n            train_groups  = list(set(list(range(1, len(x_train)))) - set(test_groups))\n\n#             test_groups = list(set(list(range((limit*j),(limit*(j+1))))))\n            \n#             train_groups = list(set(list(range(1,len(x_train)))) - set(test_groups))\n            x_train = x_train[train_groups]\n            y_train = y_train[train_groups]\n            x_test = x_train[test_groups]\n            y_test = y_train[test_groups]\n\n            classifier.n_neighbors = k\n            classifier.fit(x_train, y_train)\n\n            y_pred = classifier.predict(x_test)\n            test_score_folds.append(accuracy_score(y_test, y_pred))\n\n            y_pred = classifier.predict(x_train)\n            train_score_folds.append(accuracy_score(y_train, y_pred))\n          \n        train_scores.append(np.mean(np.array(train_score_folds)))\n        test_scores.append(np.mean(np.array(test_score_folds)))\n    return train_scores, test_scores","73e07947":"# from sklearn.neighbors import KNeighborsClassifier\n# import matplotlib.pyplot as plt\n# import random\n# import warnings\n# warnings.filterwarnings(\"ignore\")\n# from tqdm import tqdm\n\n# classifier = KNeighborsClassifier()\n# folds = 20\n# params = 50\n# # params = {'n_neighbors': sorted(random.sample(range(1,50), 10)) }\n# print('Shape of X_train and y_train',x_train[:1000].shape, y_train[:1000].shape)\n# test_scores, train_scores = RandomSearchCV(x_train[:1000],y_train[:1000],classifier, params, folds)\n\n# print(\"\\n Train_score \", train_scores)\n# print(\"Test score \", test_scores)","395259e7":"# plot hyper-parameter vs accuracy plot and choose the best hyperparameter\n\n# plt.plot(params['n_neighbors'], train_scores, label = 'train_curve')\n# plt.plot(params['n_neighbors'], np.array(test_scores), label = 'test_curve')\n# plt.title('Hyper parameter vs accuracy')\n# plt.ylabel('Accuracy')\n# plt.xlabel('Hyper parameter')\n# plt.legend()\n# plt.show()","24e214fb":"# understanding this code line by line is not that importent \ndef plot_decision_boundary(X1, X2, y, clf):\n        # Create color maps\n    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n    cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n\n    x_min, x_max = X1.min() - 1, X1.max() + 1\n    y_min, y_max = X2.min() - 1, X2.max() + 1\n    \n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n    # Plot also the training points\n    plt.scatter(X1, X2, c=y, cmap=cmap_bold)\n    \n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.title(\"2-Class classification (k = %i)\" % (clf.n_neighbors))\n    plt.show()","3cd476e1":"# from matplotlib.colors import ListedColormap\n# neigh = KNeighborsClassifier(n_neighbors = 25)\n# neigh.fit(x_train, y_train)\n# plot_decision_boundary(x_train[:, 0], x_train[:, 1], y_train, neigh)","30bea334":"from warnings import simplefilter\nsimplefilter(action='ignore', category=FutureWarning)","bed45a5d":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler\n\ngs = {'max_depth': [50,75,100],\n     'max_features':['auto','sqrt'],\n     'criterion':['entropy','gini'],\n     'min_samples_leaf':[1,5,10],\n     'min_samples_split':[2,5,10],\n      'n_estimators':[100,200,500,1000]\n     }\nclf = RandomForestClassifier()\ns = MinMaxScaler()\nxrs = s.fit_transform(x_train)\nxes = s.fit_transform(x_test)\nmodel = GridSearchCV(estimator = clf, param_grid = gs, cv = 5, verbose = 5, n_jobs = 30)\nmodel.fit(xrs, y_train)\npred = model.best_estimator_.predict(xes)\n\n    ","5d82b406":"print('Best params for Random forest classification using Grid Search CV')\nprint('\\n Best Estimator',model.best_estimator_)\nprint('\\n Best Gini score', model.best_score_*2 - 1)\nprint('\\n Best Hyper parameters', model.best_params_)\n","6ed7f1cd":"import seaborn as sns\nresult_df = pd.DataFrame(model.cv_results_)\nscores = np.array(result_df.mean_test_score).reshape(6,6)\nsns.heatmap(scores, annot=True)","0a8bdffc":"from sklearn.preprocessing import MinMaxScaler\n\ns = MinMaxScaler()\nxrs = s.fit_transform(x_train)\nxes = s.fit_transform(x_test)\n\nfrom sklearn.model_selection import RandomizedSearchCV\ngs = {'max_depth': [50,75,100],\n     'max_features':['auto','sqrt'],\n     'criterion':['entropy','gini'],\n     'min_samples_leaf':[1,5,10],\n     'min_samples_split':[2,5,10],\n      'n_estimators':[100,200,500,1000]\n     }\nclf = RandomForestClassifier()\nmodel2 = RandomizedSearchCV(estimator = clf, param_distributions = gs, n_iter =10,cv = 2, \n                           verbose = 5, random_state=100, n_jobs = -1)\nmodel2.fit(xrs,y_train)","6dc3635d":"import seaborn as sns\ntable  = pd.pivot_table(pd.DataFrame(model2.cv_results_),\n                       values = 'mean_test_score',\n                       index = 'param_n_estimators',\n                       columns = 'param_criterion')\nsns.heatmap(table)","c958b3ae":"print('Best params for Random forest classification using Random Search CV')\nprint('\\n Best Estimator',model2.best_estimator_)\nprint('\\n Best Gini score', model2.best_score_*2 - 1)\nprint('\\n Best Hyper parameters', model2.best_params_)","a3fbd294":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report,confusion_matrix\npred2 = model2.predict(xes)\n\nprint('\\n Accuracy score', accuracy_score(y_test, pred2))\n\nprint('\\n Confusion_matrix',confusion_matrix(y_test,pred2))\nprint('\\n Classification report',classification_report(y_test, pred2))","3997840a":"Great! We achieved a perfect classifier on the test set! The best part is, we did not have to perform complex feature engineering. Just the basic one-hot encoding and label encoding on categorical variables.","5889eb74":"Encode all data types of 'object' data type","d8a8b4e1":"# Random Search\n\nIn Random Search, we create a grid of hyperparameters and train\/test our model on just some random combination of these hyperparameters.\n\nWhen performing Machine Learning tasks, we generally divide our dataset in training and test sets. This is done so that to test our model after having trained it (in this way we can check it\u2019s performances when working with unseen data). When using Cross-Validation, we divide our training set into N other partitions to make sure our model is not overfitting our data.\n\nOne of the most common used Cross-Validation methods is K-Fold Validation. In K-Fold, we divide our training set into N partitions and then iteratively train our model using N-1 partitions and test it with the left-over partition (at each iteration we change the left-over partition). Once having trained N times the model we then average the training results obtained in each iteration to obtain our overall training performance results.","d0a555d1":"![conversion rate](https:\/\/cdn.corporatefinanceinstitute.com\/assets\/customer-conversion-rate-1024x683.jpeg)","3f4a767e":"# Custom Search and hyper parameter tuning","3cb95962":"# About Dataset: Marketing Targets - Banking dataset\n\n## Context\nTerm deposits are a major source of income for a bank. A term deposit is a cash investment held at a financial institution. Your money is invested for an agreed rate of interest over a fixed amount of time, or term. The bank has various outreach plans to sell term deposits to their customers such as email marketing, advertisements, telephonic marketing, and digital marketing.\n\nTelephonic marketing campaigns still remain one of the most effective way to reach out to people. However, they require huge investment as large call centers are hired to actually execute these campaigns. Hence, it is crucial to identify the customers most likely to convert beforehand so that they can be specifically targeted via call.\n\nThe data is related to direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict if the client will subscribe to a term deposit (variable y).\n\n## Content\nThe data is related to the direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed by the customer or not. The data folder contains two datasets:-\n\ntrain.csv: 45,211 rows and 18 columns ordered by date (from May 2008 to November 2010)\ntest.csv: 4521 rows and 18 columns with 10% of the examples (4521), randomly selected from train.csv\nDetailed Column Descriptions\nbank client data:\n\n1 - age (numeric)\n2 - job : type of job (categorical: \"admin.\",\"unknown\",\"unemployed\",\"management\",\"housemaid\",\"entrepreneur\",\"student\",\n\"blue-collar\",\"self-employed\",\"retired\",\"technician\",\"services\")\n3 - marital : marital status (categorical: \"married\",\"divorced\",\"single\"; note: \"divorced\" means divorced or widowed)\n4 - education (categorical: \"unknown\",\"secondary\",\"primary\",\"tertiary\")\n5 - default: has credit in default? (binary: \"yes\",\"no\")\n6 - balance: average yearly balance, in euros (numeric)\n7 - housing: has housing loan? (binary: \"yes\",\"no\")\n8 - loan: has personal loan? (binary: \"yes\",\"no\")\n## related with the last contact of the current campaign:\n9 - contact: contact communication type (categorical: \"unknown\",\"telephone\",\"cellular\")\n10 - day: last contact day of the month (numeric)\n11 - month: last contact month of year (categorical: \"jan\", \"feb\", \"mar\", \u2026, \"nov\", \"dec\")\n12 - duration: last contact duration, in seconds (numeric)\n## other attributes:\n13 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n14 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric, -1 means client was not previously contacted)\n15 - previous: number of contacts performed before this campaign and for this client (numeric)\n16 - poutcome: outcome of the previous marketing campaign (categorical: \"unknown\",\"other\",\"failure\",\"success\")\n\nOutput variable (desired target):\n17 - y - has the client subscribed a term deposit? (binary: \"yes\",\"no\")\n\nMissing Attribute Values: None\n\n## Citation\nThis dataset is publicly available for research. It has been picked up from the UCI Machine Learning with random sampling and a few additional columns.\n\nPlease add this citation if you use this dataset for any further analysis.\n\nS. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014\n\n## Past Usage\nThe full dataset was described and analyzed in:\n\nS. Moro, R. Laureano and P. Cortez. Using Data Mining for Bank Direct Marketing: An Application of the CRISP-DM Methodology.\nIn P. Novais et al. (Eds.), Proceedings of the European Simulation and Modelling Conference - ESM'2011, pp. 117-121, Guimar\u00e3es, Portugal, October, 2011. EUROSIS.\nAcknowledgement\nCreated by: Paulo Cortez (Univ. Minho) and S\u00e9rgio Moro (ISCTE-IUL) @ 2012. Thanks to Berkin Kaplano\u011flu for helping with the proper column descriptions.","21bbd851":"# Implementing Custom Random Search CV","53435fdd":"# Grid search with random forest classifer\n\nGridSearchCV :For given values, GridSearchCV exhaustively considers all parameter combinations. The grid search provided by GridSearchCV exhaustively generates candidates from a grid of parameter values specified with the param_grid parameter. For instance, the following param_grid specifies that it has one grid to be explored that is a linear kernel with alpha values in [0.0002,0.0003,0.0004,0.0005,0.0006,0.0007,0.0009], and 'max_iter' i.e maximum 10000 iterations.\n\nparam_grid = {'alpha':[0.01,0.001,0.0001,0.0002,0.0003,0.0004,0.0005,0.0006,0.0007,0.0009],'max_iter':[10000]}","0d3065d6":"# Customer Conversion in Marketing\n\nCustomer conversion rate, a key measure of marketing performance, is the rate at which potential customers take a specific desired action. In e-commerce or online marketing, customer conversion aims to progress prospective customers towards being paying customers. \n\n> According to Marketing Sherpa, a conversion is \u201cthe point at which a recipient of a marketing message performs a desired action.\u201d In other words, a conversion is simply getting someone to respond to your call-to-action.\n\nA conversion in marketing is when a visitor to your website completes a desired goal. In this way, they convert from visitors to leads or, if they purchase something, to customers. A conversion occurs when someone changes from a passive visitor to an active, interested visitor or customer.\n\nSome examples of conversions include: **Signing up for emails. Filling out a contact form. Visiting certain pages on your website.**\n\n[Ref: Customer conversion in finance](https:\/\/vwo.com\/blog\/conversion-marketing\/)","a539b9e0":"Most of the feature attributes are of 'object' type. These attributes need to be encoded or dropped. The feature 'y' is the target variable and remaining attributes are dependent features.","7583b7d2":"\n* Hyperparameter tuning is critical for the correct functioning of Machine Learning (ML) models. \n* Hyperparameters are points of choice or configuration that allow a machine learning model to be customized for a specific task or dataset.\n\n> **Hyperparameter:** Model configuration argument specified by the developer to guide the learning process for a specific dataset.\n        \n        \n* Specifically, it provides the RandomizedSearchCV for random search and GridSearchCV for grid search. Both techniques evaluate models for a given hyperparameter vector using cross-validation, hence the \u201cCV\u201d suffix of each class name.\n*  The goal of the optimization procedure is to find a vector that results in the best performance of the model after learning, such as maximum accuracy or minimum error.\n* A range of different optimization algorithms may be used, although two of the simplest and most common methods are random search and grid search.\n\n    >*  **Random Search**  Define a search space as a bounded domain of hyperparameter values and randomly sample points in that domain.\n    >* **Grid Search**  Define a search space as a grid of hyperparameter values and evaluate every position in the grid.\n\n![Hyper parameter](https:\/\/miro.medium.com\/max\/1998\/1*eKSUWwWCGvrUyT592oX-vg.png)\n\n\nRef:\n1. [Hyper parameter tuning](https:\/\/machinelearningmastery.com\/hyperparameter-optimization-with-random-search-and-grid-search\/)\n2. [Parameter tuning](https:\/\/www.kaggle.com\/pavansanagapati\/automated-hyperparameter-tuning)"}}