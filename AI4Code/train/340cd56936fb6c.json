{"cell_type":{"008feca1":"code","cd9d027d":"code","01fa9f74":"code","8762fd8b":"code","1b94c4e3":"code","e03a50e6":"code","f61f3216":"code","24ed3363":"code","27f1ca04":"code","d6df3c81":"code","50834712":"code","b0154d4c":"code","169a4806":"code","7b3820dc":"code","5c06bb44":"code","f2ee9f16":"code","dfa46e82":"code","c715d4d0":"code","01432536":"code","30fe063c":"code","e241eb8f":"code","d22778fa":"code","a03e8e92":"code","63c73083":"code","fc133738":"code","d6411753":"code","2a2de5f4":"code","e16bbd95":"code","c840f316":"code","2272c514":"code","6fd67674":"code","c2a0974e":"code","f9e1cde3":"code","dbd16079":"code","7e232c5c":"code","fb784cca":"code","18d56ff2":"code","6998f019":"code","7fc742d9":"code","db8feb39":"code","507e71fe":"code","0dcaea5d":"code","e7656259":"code","a8619d6c":"code","3709279c":"code","7dea5c20":"code","74c8e1e2":"code","342c875c":"code","2b17d23c":"code","baa0bd6d":"code","4fe71fdd":"code","f7c2474b":"code","0debca4c":"code","431327bc":"code","ab7dbadd":"code","3cfc0926":"code","db35a3f7":"code","0c11534b":"code","6f996d39":"markdown","81b5a634":"markdown","6d38e765":"markdown","0f0d98e9":"markdown","fb805059":"markdown","9d6d291f":"markdown","1e6d952b":"markdown","b0ea8407":"markdown","b49be4e6":"markdown","2a7e2384":"markdown","ff8b33d5":"markdown","c70b41ed":"markdown","5cf4e911":"markdown","bd827af9":"markdown","8f933de4":"markdown","5da385ab":"markdown","3b2bb4a7":"markdown"},"source":{"008feca1":"# Import libraries \nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Import models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Model evalution\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve","cd9d027d":"data = pd.read_csv('\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')\n\n# Deep Copy\ndf = data.copy()","01fa9f74":"\ndf.head()","8762fd8b":"df.info()","1b94c4e3":"df.isna().sum()","e03a50e6":"df.columns","f61f3216":"df.output.value_counts()","24ed3363":"pd.crosstab(df.sex, df.output)","27f1ca04":"pd.crosstab(df.sex, df.output).plot(kind='bar', figsize=(10, 6), color=['lightblue', 'salmon'])\n\nplt.title('Heart Disease frequency for Sex')\nplt.ylabel('Frequency')\nplt.xticks(rotation=0)\nplt.legend(['Female', 'Male'])\nplt.xlabel('0 = Not Disease, 1 = Disease')","d6df3c81":"plt.figure(figsize=(10, 7))\nsns.scatterplot(x=df.age, y=df.thalachh, hue=df.output, alpha=0.5)\nplt.title('Heart Disease Age vs Max Heart Rate')\nplt.ylabel('Max Heart Rate')\nplt.xlabel('Age')\nplt.legend(['Not disease', 'Disease'])","50834712":"pd.crosstab(df.cp, df.output).plot(kind='bar', figsize=(10,8), color=['lightblue', 'salmon'])\nplt.ylabel('Frequency')\nplt.xlabel('Chest Pain Type')\nplt.title('Heart Disease Chest Pain Frequency')\nplt.legend(['Note Disease', 'Disease'])\nplt.xticks(rotation=0)","b0154d4c":"df.describe()","169a4806":"df.corr()","7b3820dc":"plt.figure(figsize=(15, 10))\nsns.heatmap(data=df.corr(), annot=True, fmt='.2f', cmap='YlOrRd', linewidths=0.5)","5c06bb44":"df.head()","f2ee9f16":"# Split Data\nx = df.drop(columns=['output'])\n\ny = df['output']","dfa46e82":"x","c715d4d0":"y","01432536":"# split into train and test\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)","30fe063c":"x_train.shape, x_test.shape, y_train.shape, y_test.shape","e241eb8f":"model_selection = {'Logistic Regression': LogisticRegression(),\n                   'K Neighbors Classifier': KNeighborsClassifier(),\n                   'Random Forest Classifier': RandomForestClassifier()}\n\ndef fit_and_score(x_train, x_test, y_train, y_test):\n    # Make empty Score dic\n    score = {}\n    # Model fit\n    for name, model in model_selection.items():\n        # Fit the data in to model\n        model.fit(x_train, y_train)\n        # save the score \n        score[name] = model.score(x_test, y_test)\n    return score","d22778fa":"model_scores = fit_and_score(x_train=x_train, x_test=x_test, y_train=y_train, y_test=y_test)\n\nmodel_scores","a03e8e92":"model_compare = pd.DataFrame(model_scores, index=['accuracy'])\n\nmodel_compare.T.plot(kind='bar')","63c73083":"# KNN tune\ntrain_score = []\ntest_score = []\n\n# Create a list of different values for K-Neighbors\nneighbors = range(1, 21)\n\n# Setup KNN instant\nknn = KNeighborsClassifier()\n\n# Loop through different n_neighbors\nfor i in neighbors:\n    knn.set_params(n_neighbors=i)\n    \n    # Fit the algorithm\n    knn.fit(x_train, y_train)\n    \n    # update training score list\n    train_score.append(knn.score(x_train, y_train))\n    \n    # update test score list\n    test_score.append(knn.score(x_test, y_test))\n    ","fc133738":"train_score","d6411753":"test_score","2a2de5f4":"plt.figure(figsize=(15, 8))\n\nsns.lineplot(x=neighbors, y=train_score)\nsns.lineplot(x=neighbors, y=test_score)\nplt.xticks(np.arange(1,21))\nplt.xlabel('Different values of Neighbors')\nplt.ylabel('Model Score')\nplt.legend(['train_score', 'test_score'])\n\nprint(f'Maximum KNN Score on the test data:{max(test_score)*100:.2f}%')","e16bbd95":"# Create a hyperparameter grid for Logistic Regression\nlog_reg_grid = {'C': np.logspace(-4, 4, 20),\n                'solver': ['liblinear']}\n\n# Create a hyperparameter grid for RandomForestClassifier\nrf_grid = {'n_estimators': np.arange(10, 1000, 50),\n           'max_depth': [None, 3, 5, 10],\n           'min_samples_split': np.arange(2, 20, 2),\n           'min_samples_leaf': np.arange(1, 20, 2)}","c840f316":"# setup random hyperparameter search for LogisticRegression\nrs_log_reg = RandomizedSearchCV(LogisticRegression(),\n                                param_distributions=log_reg_grid,\n                                cv=5,\n                                n_iter=20,\n                                verbose=True, \n                                random_state=42)\n\n# fit random hyperparameter search model for LogisticRegression\nrs_log_reg.fit(x_train, y_train)","2272c514":"# Find the best hyperparameters\nrs_log_reg.best_params_","6fd67674":"# Evaluate the randomized search randomforestClassifier model\nrs_log_reg.score(x_test, y_test)","c2a0974e":"# Setup random hyperparameter search for RandomForestClassifier\nrs_rf = RandomizedSearchCV(RandomForestClassifier(),\n                           param_distributions=rf_grid,\n                           cv=5,\n                           n_iter=20,\n                           verbose=True,\n                           random_state=42)\n\n# Fit random hyperparameter search model for RandomForestClassifier\nrs_rf.fit(x_train, y_train)","f9e1cde3":"# Find the best hyperparameters\nrs_rf.best_params_","dbd16079":"# Evaluate the randomized search randomforestClassifier model\nrs_rf.score(x_test, y_test)","7e232c5c":"# Diffrent hyperparameters for our LogisticRegression model\nlog_reg_grid = {'C': np.logspace(-4,4,30),\n                'solver': ['liblinear']}\n\n# setup grid hyperparameter search for LogisticRegression\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                          param_grid= log_reg_grid,\n                          cv=5,\n                          verbose=True)\n\n# Fit grid hyperparameter search model\ngs_log_reg.fit(x_train, y_train)","fb784cca":"# Check the best hyperparameters\ngs_log_reg.best_params_","18d56ff2":"# Evaluate the grid search LogisticRegression model\ngs_log_reg.score(x_test, y_test)","6998f019":"# make predictions with tuned model\ny_preds = gs_log_reg.predict(x_test)\n\ny_preds","7fc742d9":"y_test","db8feb39":"plot_roc_curve(gs_log_reg, x_test, y_test)","507e71fe":"# Confusion matrix\nconfusion_matrix(y_test, y_preds)","0dcaea5d":"plt.figure(figsize=(3,3))\nsns.heatmap(confusion_matrix(y_test, y_preds), annot=True, cbar=False)\nplt.xlabel('True Value')\nplt.ylabel('Predicted Value')","e7656259":"print(classification_report(y_test, y_preds))","a8619d6c":"# Check best hyperparameters \ngs_log_reg.best_params_","3709279c":"# Create a new classifier with best parameters\nclf = LogisticRegression(C=0.20433597178569418,\n                         solver='liblinear')","7dea5c20":"# Cross-validated accuracy\ncv_acc = cross_val_score(clf, x, y, cv=5, scoring='accuracy')\n\ncv_acc = np.mean(cv_acc)\ncv_acc","74c8e1e2":"# Cross-validated precision\ncv_precision = cross_val_score(clf, x, y, cv=5, scoring='precision')\n\ncv_precision = np.mean(cv_precision)\ncv_precision","342c875c":"# Cross-validated recall\ncv_recall = cross_val_score(clf, x, y, cv=5, scoring='recall')\n\ncv_recall = np.mean(cv_recall)\ncv_recall","2b17d23c":"# Cross-validated f1\ncv_f1 = cross_val_score(clf, x, y, cv=5, scoring='f1')\n\ncv_f1 = np.mean(cv_f1)\ncv_f1","baa0bd6d":"# Visualize cross-validated metrics\ncv_metrics = pd.DataFrame({'Accuracy': cv_acc,\n                           'Precision': cv_precision,\n                           'Recall': cv_recall,\n                           'F1': cv_f1},\n                          index=[0])\ncv_metrics.T.plot.bar(title='Cross-validated classification metrics', legend=False)","4fe71fdd":"# Fit an instance of LogisticRegression\nclf = LogisticRegression(C=0.20433597178569418,\n                         solver='liblinear')\n\nclf.fit(x_train, y_train)","f7c2474b":"# Check coef_\nclf.coef_","0debca4c":"# Match coef's of features to columns\nfeature_dict = dict(zip(df.columns, list(clf.coef_[0])))\nfeature_dict","431327bc":"# Visualize feature importance\nfeature_df = pd.DataFrame(feature_dict, index=[0])\nfeature_df.T.plot.bar(title='Feature Importance', legend=False)","ab7dbadd":"x_raw = df.drop(columns=['output'])\ny_raw = df['output']","3cfc0926":"# Create Data Empty Dataframe\ninput_ = {'age':0, 'sex':0, 'cp':0, 'trtbps':0, 'chol':0, 'fbs':0, 'restecg':0, 'thalachh':0, 'exng':0, 'oldpeak':0, \n     'slp':0, 'caa':0, 'thall':0}\noutput = {'Logistic Regression': 0, 'K Neighbors Classifier': 0, 'Random Forest Classifier': 0}\n\n# Create Data Empty Dataframe\nfinal = {'age':0, 'sex':0, 'cp':0, 'trtbps':0, 'chol':0, 'fbs':0, 'restecg':0, 'thalachh':0, 'exng':0, 'oldpeak':0, \n     'slp':0, 'caa':0, 'thall':0, 'Logistic Regression': 0, 'K Neighbors Classifier': 0, 'Random Forest Classifier': 0}\ndata_final = pd.DataFrame(columns=['age', 'sex', 'cp', 'trtbps', 'chol', 'fbs', 'restecg', 'thalachh', 'exng', 'oldpeak', \n                                   'slp', 'caa', 'thall', 'Logistic Regression', 'K Neighbors Classifier', \n                                   'Random Forest Classifier'])\n\n\ndef input_data():\n    # Make temporary dataframe\n    temp = pd.DataFrame(columns=['age', 'sex', 'cp', 'trtbps', 'chol', 'fbs', 'restecg', 'thalachh', 'exng', \n                                   'oldpeak', 'slp', 'caa', 'thall'])\n    \n    # Make some intruction\n    intruction = {\"age\": \"Age: age in years\",\n                  \"sex\": \"Sex: sex (1 = male; 0 = female)\",\n                  \"cp\": \"\"\"Chest Pain: chest pain type\n-- Value 0: typical angina\n-- Value 1: atypical angina\n-- Value 2: non-anginal pain\n-- Value 3: asymptomatic\"\"\",\n                  \"trtbps\": \"Trest_bps: resting blood pressure (in mm Hg on admission to the hospital)\",\n                  \"chol\": \"Cholestoral: serum cholestoral in mg\/dl\",\n                  \"fbs\": \"Fasting Blood Sugar: (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\",\n                  \"restecg\": '''Resting Electrocardiographic Results:\n-- Value 0: normal\n-- Value 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n-- Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria''',\n                  \"thalachh\": \"Thalach: maximum heart rate achieved\",\n                  \"exng\": \"Ex_Ang: exercise induced angina (1 = yes; 0 = no)\",\n                  \"oldpeak\": \"Old_Peak: ST depression induced by exercise relative to rest\",\n                  \"slp\": \"Slope: the slope of the peak exercise ST segment\",\n                  \"caa\": \"CA: number of major vessels (0-3) colored by flourosopy\",\n                  \"thall\": \"Thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\",\n                  \"output\": \"\"\"Output: diagnosis of heart disease (angiographic disease status)\n-- Value 0: < 50% diameter narrowing\n-- Value 1: > 50% diameter narrowing\"\"\"}\n    \n    # Input your data\n    for name, val in input_.items():\n        print(intruction[name])\n        input_[name] = input(f'{name} : ')\n    \n    # Input all data into dataframe\n    temp = temp.append(input_, ignore_index=True)\n    \n    # Conver all value into float\n    temp = temp.astype(np.float64)\n    \n    # Make Model \n    models = {'Logistic Regression': LogisticRegression(),\n              'K Neighbors Classifier': KNeighborsClassifier(),\n              'Random Forest Classifier': RandomForestClassifier()}\n    \n    # Set random seed\n    np.random.seed(42)\n    # Model fit and pridict optput\n    for name, model in models.items():\n        model.fit(x_raw, y_raw)  # fit the model\n        \n        # predict value\n        y_preds = model.predict(temp)\n#         if name == 'K Neighbors Classifier':\n#             y_preds = model.predict_proba(temp)\n#         else:\n#             y_preds = model.predict_log_proba(temp)\n            \n        output[name] = y_preds\n    \n    # save data in final\n    for name, val in input_.items():\n        final[name] = input_[name]\n    for name, val in output.items():\n        final[name] = output[name]\n    ","db35a3f7":"# input_data()","0c11534b":"# data_final = data_final.append(final, ignore_index=True)\n\n# data_final","6f996d39":"## Heart Desease Frequency per Chest Pain Type\n\n* cp : Chest Pain type chest pain type\n    * Value 0: typical angina\n    * Value 1: atypical angina\n    * Value 2: non-anginal pain\n    * Value 3: asymptomatic","81b5a634":"## Age vs Max Heart Rate for Heart Disease","6d38e765":"Now hyperparameter grids setup for each models ansd tune them using RandomizedSearchCV...","0f0d98e9":"## Calculate evaluation metrics using cross-validation\n\ncalculate accuracy, precision, recall and f1-score model using cross-validation and `cross_val_score()`.","fb805059":"## Hyperparameter Tuning with GridSearchCV\n\nLogisticRegression model provides the best scores so far,  therefore improve again using GridSearchCV...","9d6d291f":"## Hyperparameter tunning with RandomizedSearchCV\n\nNow tunning following:\n* Logistic Regression()\n* Random Forest Classifier()\n\n... using RandomizedSearchCV","1e6d952b":"# Load Data\n","b0ea8407":"# Data Exploration","b49be4e6":"# For improve accuracy following steps :\n* Hypyterparameter tuning\n* feature importance\n* Confusion Matrix\n* Cross Validation\n* Precision\n* Recall\n* F1 score\n* Classification Report\n* ROC Curve\n* Area under the curve (AUC)\n\n### HyperParameters Tuning (by hand)","2a7e2384":"# Modelling","ff8b33d5":"### About this dataset\n* Age : Age of the patient\n* Sex : Sex of the patient\n* exang: exercise induced angina (1 = yes; 0 = no)\n* ca: number of major vessels (0-3)\n* cp : Chest Pain type chest pain type\n    * Value 0: typical angina\n    * Value 1: atypical angina\n    * Value 2: non-anginal pain\n    * Value 3: asymptomatic\n\n* trtbps : resting blood pressure (in mm Hg)\n* chol : cholestoral in mg\/dl fetched via BMI sensor\n* fbs : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n* rest_ecg : resting electrocardiographic results\n    * Value 0: normal\n    * Value 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n    * Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n\n* thalach : maximum heart rate achieved\n* output : 0= less chance of heart attack 1= more chance of heart attack","c70b41ed":"# Prediction","5cf4e911":"## Model selection","bd827af9":"## Heart Disease frequency according to Sex","8f933de4":"Now tuned LogisticRegression(), Let's do the same for RandomForestClassifer()...","5da385ab":"## Evaluting our tuned machine learning classifier, beyond accuracy\n\n* ROC curve and AUC score\n* Confusion matrix\n* classification report\n* Precision\n* Recall\n* F1-Score\n\n....and it would be great it cross-validaton was used where possible\n\nto make comaparistions and evaluate our trained model, first we need to make predictions.","3b2bb4a7":"# Heart Attack Analysis & Prediction Dataset\n## A dataset for heart attack classification"}}