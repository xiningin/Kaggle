{"cell_type":{"34aa0e79":"code","ca98d135":"code","863ee8a0":"code","a7021db6":"code","128fe24e":"code","8c7bc298":"code","66609d1b":"code","6b190c7e":"code","a59c1346":"code","cc1da3aa":"code","17ccd8a6":"code","7076b74c":"code","f539b760":"code","22c0c42d":"code","58bdb0a0":"code","27e7d186":"code","8244c816":"code","0a2cd232":"code","c6a67845":"code","679220e5":"code","e0acaec7":"code","911aed3a":"code","5cf1118d":"code","68a01d20":"code","4211a977":"code","d0432edc":"code","eb0f7dac":"code","7a328120":"code","58d971da":"code","4b79a9f1":"code","084cad63":"code","cb02c7a9":"code","9c398bb4":"code","df9f61ac":"code","773622ff":"code","c985a744":"code","106a6fcd":"code","4c63f17d":"code","683b0f55":"code","faa05c50":"markdown","4f59fbe4":"markdown","3cf4c836":"markdown","8f65d20f":"markdown","faefb4b3":"markdown","97411e5e":"markdown","97b1d563":"markdown","8aa7ec91":"markdown","c4100ab5":"markdown","8c40d02b":"markdown","ffd763f6":"markdown","5eb90f10":"markdown","c8e750ca":"markdown","aead3613":"markdown","4cbfe32f":"markdown","20150e05":"markdown","b4f51108":"markdown","8d838640":"markdown","6758f693":"markdown","4f5699b0":"markdown","4f23636e":"markdown","11b2919c":"markdown","f7e485ec":"markdown","0f2f849f":"markdown","ffb6bb9a":"markdown","9fc2e6da":"markdown","522c32c6":"markdown","1ac2cec1":"markdown","33df3531":"markdown","b54c9796":"markdown","36ea5a44":"markdown","1665e7d0":"markdown","5d032add":"markdown","7078536c":"markdown","035176b6":"markdown"},"source":{"34aa0e79":"#Data Manipulation Libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random\nimport re #regular expressions\n#Progress bar\nfrom tqdm import tqdm\n\nfrom datetime import datetime\n\n#Read Images\nimport os\nfrom skimage import io\nfrom PIL import Image\n# import cv2 # When open cv was used, there was an error in getting array from image. Using Pillow eliminated the error.\n\n#Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Model Pre-processing\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n\n#Modelling\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout\nfrom sklearn.metrics import  r2_score,roc_auc_score,f1_score,recall_score,precision_score,classification_report, confusion_matrix,log_loss","ca98d135":"# Increase rows and columns visible on the notebook\npd.set_option('display.max_rows', 5000)\npd.set_option('display.max_columns', 50)\n\n# import required libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\")","863ee8a0":"#Function to upload the Raw training images\ndef upload_raw_train_images(image_path, wheat_categories):\n    images = []   \n    labels = []\n    # Loop across the three directories having wheat images.\n    for category in wheat_categories:  \n        print(\"Category:\",category)\n        # Append the wheat category directory into the main path\n        full_image_path = image_path +  category + \"\/\"\n        # Retrieve the filenames from the all the three wheat directories. OS package used.\n        image_file_names = [os.path.join(full_image_path, f) for f in os.listdir(full_image_path)]\n        # Read the image pixels\n        for file in image_file_names[0:5]:\n            image=io.imread(file) #io package from SKimage package\n            print(image.shape)\n            images.append(np.array(image))\n            labels.append(category)\n    return images, labels\nwheat_categories = ['healthy_wheat', 'stem_rust', 'leaf_rust'] \nraw_train_images, raw_train_labels = upload_raw_train_images('\/kaggle\/input\/cgiar-computer-vision-for-crop-disease\/ICLR\/train\/train\/',wheat_categories)","a7021db6":"#Function to upload the resized training images\ndef upload_train_images(image_path, wheat_categories ,height, width):\n    images = []\n    labels = []\n    \n    # Loop across the three directories having wheat images.\n    for category in wheat_categories:\n        \n        # Append the wheat category directory into the main path\n        full_image_path = image_path +  category + \"\/\"\n        # Retrieve the filenames from the all the three wheat directories. OS package used.\n        image_file_names = [os.path.join(full_image_path, f) for f in os.listdir(full_image_path)]\n        # Read the image pixels\n        for file in image_file_names:\n            image=io.imread(file) #io package from SKimage package\n            # Append image into list\n            image_from_array = Image.fromarray(image, 'RGB')\n            #Resize image\n            size_image = image_from_array.resize((height, width))\n            #Append image into list\n            images.append(np.array(size_image))\n            # Label for each image as per directory\n            labels.append(category)\n        \n    return images, labels\n\n## Invoke the function\n#Image resize parameters\nheight = 256\nwidth = 256\n#Get number of classes\nwheat_categories = ['healthy_wheat', 'stem_rust', 'leaf_rust'] \ntrain_images, train_labels = upload_train_images('\/kaggle\/input\/cgiar-computer-vision-for-crop-disease\/ICLR\/train\/train\/',wheat_categories,height,width)\n#Size and dimension of output image and labels\ntrain_images = np.array(train_images)\ntrain_labels = np.array(train_labels)","128fe24e":"print(\"Shape of training images is \" + str(train_images.shape))\nprint(\"Shape of training labels is \" + str(train_labels.shape))","8c7bc298":"def show_train_images(images, labels, images_count):\n     for i in range(images_count):\n        \n        index = int(random.random() * len(images))\n        plt.axis('off')\n        plt.imshow(images[index])\n        plt.show()\n        \n        print(\"Size of this image is \" + str(images[index].shape))\n        print(\"Class of the image is \" + str(labels[index]))\n\n#Execute the function\nprint(\"Train images, sizes and cass labels\")\nshow_train_images(train_images, train_labels, 3)","66609d1b":"# a function to show the image batch\ndef show_batch_train_images(images,labels):\n    plt.figure(figsize=(15,15))\n    for n in range(20):\n        ax = plt.subplot(5,5,n+1)\n        index = int(random.random() * len(images))\n        plt.imshow(images[index])\n        plt.title(labels[index])\n#         plt.title(CLASS_NAMES[labels[n]==1][0].title())\n#         print(\"Size of this image is \" + str(images[index].shape))\n        plt.axis('off')\n\nshow_batch_train_images(train_images,train_labels)","6b190c7e":"#Categories of Images\npd.Series(train_labels).value_counts().reset_index().values.tolist()","a59c1346":"# Plot chart\nsns.countplot(train_labels)\nplt.show()","cc1da3aa":"#Class Weights\nfrom sklearn.utils import class_weight\nclass_weights = class_weight.compute_class_weight('balanced',\n                                                 np.unique(train_labels),\n                                                 train_labels)\n\nprint(np.unique(train_labels))\nclass_weights","17ccd8a6":"#Label encoding to change \nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ntrain_labels_enc = label_encoder.fit_transform(train_labels)\ntrain_labels_enc","7076b74c":"#Convert the predicted labels to categorical type\ntrain_labels_cat = to_categorical(train_labels_enc)\n\n#Display the categorical training labels\nprint(train_labels_cat[1])\nprint(train_labels_cat[300])\nprint(train_labels_cat[600])","f539b760":"#Normalize the image pixels\ntrain_images = train_images.astype('float32')\/255 ","22c0c42d":"# Training to have 90% and validation 10%. High value of training taken so that we have ample training images. \n# The more the images, the better the model\nX_train,X_valid,Y_train,Y_valid = train_test_split(train_images,train_labels_cat,test_size = 0.1,random_state=None)\n\nprint(\"X Train count is \",len(X_train),\"Shape\",X_train.shape, \" and Y train count \",len(Y_train), \"Shape\", Y_train.shape )\nprint(\"X validation count is \",len(X_valid), \"Shape\",X_valid.shape,\" and Y validation count \", len(Y_valid), \"Shape\",Y_valid.shape)","58bdb0a0":"#Define the CNN Model\n#Sequential API to add one layer at a time starting from the input.\nmodel = Sequential()\n# Convolution layer with 32 filters first Conv2D layer.  \n# Each filter transforms a part of the image using the kernel filter. The kernel filter matrix is applied on the whole image.\n# Relu activation function used to add non linearity to the network.\nmodel.add(Conv2D(filters=32, kernel_size=(5,5), activation='relu', input_shape=X_train.shape[1:]))\n# Convolution layer with 64 filters second Conv2D layer \nmodel.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n# Max pooling applied. Reduces the size of the image by half. Is a downsampling filter which looks at the 2 neighboring pixels and picks the maximal value\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n# Drop applied as a regularization method, where a proportion of nodes in the layer are randomly ignored by setting their wieghts to zero for each training sample.\n# This drops randomly a proportion of the network and forces the network to learn features in a distributed way. This improves generalization and reduces overfitting.\nmodel.add(Dropout(rate=0.25))\nmodel.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2, 2)))\nmodel.add(Dropout(rate=0.25))\n# Flatten to convert the final feature maps into a one single 1D vector. Needed so as to make use of fully connected layers after some convolutional\/maxpool layers.\n# It combines all the found local features of the previous convolutional layers.\nmodel.add(Flatten())\n#Dense layer applied to create a fully-connected artificial neural networks classifier.\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(rate=0.5))\n#Neural net outputs distribution of probability of each class.\nmodel.add(Dense(3, activation='softmax'))\nmodel.summary()","27e7d186":"#Compilation of the model\nmodel.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01), \n                    loss=tf.keras.losses.categorical_crossentropy, \n                    metrics = [tf.keras.metrics.categorical_accuracy])","8244c816":"#Using ten epochs for the training and saving the accuracy for each epoch\nhistory = model.fit(X_train, Y_train, batch_size=32, epochs=12,\n                    validation_data=(X_valid, Y_valid),class_weight=class_weights) #,validation_split = 0.2, callbacks=callbacks,","0a2cd232":"#Display of the accuracy and the loss values\nplt.figure(0)\nplt.plot(history.history['categorical_accuracy'], label='training accuracy')\nplt.plot(history.history['val_categorical_accuracy'], label='val accuracy')\nplt.title('Accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend()\n\nplt.figure(1)\nplt.plot(history.history['loss'], label='training loss')\nplt.plot(history.history['val_loss'], label='val loss')\nplt.title('Loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()\nplt.show()","c6a67845":"# Create dictionary and dataframe to hold results for various models\ndict = {'Model':['Baseline CNN' ,'Mobile Net V2', 'Data Augmentation'], \n        'AUC': [0,0,0],\n        'Log Loss':[0,0,0], \n        'F1 score':[0,0,0], \n        'Recall':[0,0,0], \n        'Precision':[0,0,0]} \ndf_results = pd.DataFrame(dict,columns = ['Model','Log Loss','AUC','F1 score','Recall','Precision'])\n\n\n# Function to calculate Results for each model\ndef model_results(model_type,y_test_data, y_prediction_data, y_test_class, y_pred_class):\n    \n    index_val = df_results[df_results['Model']==model_type].index\n    \n    #Asign scores to dataframe\n    df_results.loc[index_val,'AUC'] = roc_auc_score(y_test_data, y_prediction_data)\n    df_results.loc[index_val,'Log Loss'] = log_loss(Y_valid, y_prediction_data)\n    df_results.loc[index_val,'F1 score'] = f1_score(y_test_class, y_pred_class,average='weighted')\n    df_results.loc[index_val,'Recall'] = recall_score(y_test_class, y_pred_class,average='weighted')\n    df_results.loc[index_val,'Precision'] = precision_score(y_test_class, y_pred_class,average='weighted')\n\n    return(df_results)","679220e5":"#Baseline Prediction\ny_prediction = model.predict(X_valid) # make predictions\n\n#Baseline Results\ndominant_y_valid=np.argmax(Y_valid, axis=1)\ndominant_y_predict=np.argmax(y_prediction, axis=1)\n\nmodel_results('Baseline CNN',Y_valid, y_prediction,dominant_y_valid,dominant_y_predict)","e0acaec7":"#Confusion Matrix\nimport itertools\nfrom sklearn.metrics import confusion_matrix\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=75) \n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        \n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            plt.text(j, i, cm[i, j],\n            horizontalalignment=\"center\",\n            color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\nclass_names = range(3)\n# cm = confusion_matrix(rounded_Y_valid , rounded_Y_predict_trf)\ncm = confusion_matrix(dominant_y_valid , dominant_y_predict)\nY_valid, y_predict_trf\nplt.figure(2)\nplt.figure(figsize=(5,5))\nplot_confusion_matrix(cm, classes=class_names, title='Confusion matrix')","911aed3a":"# Create the base model from the pre-trained model MobileNet V2\nbase_model = tf.keras.applications.MobileNetV2(input_shape=X_train.shape[1:],\n                                               include_top=False,\n                                               weights='imagenet')","5cf1118d":"#To use weights in the pre-trained model\nbase_model.trainable = False \n\n#Define the pre-trained model\npretrained_model = tf.keras.Sequential([base_model,tf.keras.layers.GlobalAveragePooling2D(),tf.keras.layers.Dense(3, activation=\"softmax\")])\n\npretrained_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.01), loss=tf.keras.losses.categorical_crossentropy, \n                         metrics = [tf.keras.metrics.categorical_accuracy])\n\npretrained_model.summary()","68a01d20":"#Fit the pretrained model to the  data\nhistory_trf = pretrained_model.fit(X_train, Y_train, epochs=5,batch_size=32 , \n                validation_data=(X_valid, Y_valid), class_weight=class_weights)","4211a977":"#Display of the accuracy and the loss values\nplt.figure(0)\nplt.plot(history_trf.history['categorical_accuracy'], label='training accuracy')\nplt.plot(history_trf.history['val_categorical_accuracy'], label='val accuracy')\nplt.title('Accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend()\n\nplt.figure(1)\nplt.plot(history_trf.history['loss'], label='training loss')\nplt.plot(history_trf.history['val_loss'], label='val loss')\nplt.title('Loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()\nplt.show()","d0432edc":"#Mobile Net V2 Prediction\ny_prediction_trf = pretrained_model.predict(X_valid) # make predictions\n\n#Baseline Results\ndominant_y_valid=np.argmax(Y_valid, axis=1)\ndominant_y_predict=np.argmax(y_prediction_trf, axis=1)\n\nmodel_results('Mobile Net V2',Y_valid, y_prediction_trf,dominant_y_valid,dominant_y_predict)","eb0f7dac":"print(classification_report(dominant_y_valid , dominant_y_predict))","7a328120":"#Confusion Matrix\nimport itertools\nfrom sklearn.metrics import confusion_matrix\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=75) \n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        \n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n            plt.text(j, i, cm[i, j],\n            horizontalalignment=\"center\",\n            color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\nclass_names = range(3)\n# cm = confusion_matrix(rounded_Y_valid , rounded_Y_predict_trf)\ncm = confusion_matrix(dominant_y_valid , dominant_y_predict)\nY_valid, y_predict_trf\nplt.figure(2)\nplt.figure(figsize=(5,5))\nplot_confusion_matrix(cm, classes=class_names, title='Mobile Net V2 Confusion matrix')","58d971da":"# Generate more image data\nfrom keras.preprocessing.image import ImageDataGenerator\ntrain_generator = ImageDataGenerator(rescale = 1\/255, zoom_range = 0.3,horizontal_flip = True,rotation_range = 30)\nval_generator = ImageDataGenerator(rescale = 1\/255)\ntrain_generator = train_generator.flow(np.array(X_train),Y_train,batch_size = 32,shuffle = False)\nval_generator = val_generator.flow(np.array(X_valid),Y_valid,batch_size = 32,shuffle = False)\n\n# Train and test the model\nhistory_idg = pretrained_model.fit_generator(train_generator,\n                                   epochs = 10,\n                                   shuffle = False, \n                                   steps_per_epoch=3,\n                                   validation_steps=1,\n                                   validation_data=val_generator,\n                                   class_weight=class_weights)","4b79a9f1":"#Display of the accuracy and the loss values\nplt.figure(0)\nplt.plot(history_idg.history['categorical_accuracy'], label='training accuracy')\n# plt.plot(history_idg.history['val_categorical_accuracy'], label='val accuracy')\nplt.title('Accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend()\n\nplt.figure(1)\nplt.plot(history_idg.history['loss'], label='training loss')\n# plt.plot(history_idg.history['val_loss'], label='val loss')\nplt.title('Loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()\nplt.show()","084cad63":"# Prediction\ny_prediction_idg = pretrained_model.predict(X_valid) # make predictions\n\nlogloss = log_loss(Y_valid, y_prediction_idg)\nlogloss","cb02c7a9":"#Function to upload the test images\ndef upload_test_images(image_path, height, width):\n    test_images = []\n    test_image_paths = []\n        # Retrieve the filenames from the all the test directory\n    test_image_file_names = [os.path.join(image_path, f) for f in os.listdir(image_path)]\n        # Read the image pixels\n    for file in test_image_file_names:\n        test_image=io.imread(file)\n        # Append image into list\n        test_image_from_array = Image.fromarray(test_image, 'RGB')\n        #Resize image\n        test_size_image = test_image_from_array.resize((height, width))\n        #Append image into list\n        test_images.append(np.array(test_size_image))\n        test_image_paths.append(file)\n    return test_images,test_image_paths\n\n## Invoke the function\n#Image resize parameters\nheight = 256\nwidth = 256\ntest_images,test_image_paths = upload_test_images('\/kaggle\/input\/cgiar-computer-vision-for-crop-disease\/ICLR\/test\/test\/',height,width)\ntest_images = np.array(test_images)","9c398bb4":"#Size and dimension of test image\nprint(\"Shape of test images is \" + str(test_images.shape))\n# Check image paths\ntest_image_paths[0:5]","df9f61ac":"# use regular expressions to extract the name of image\nimage_names = []\nfor i in test_image_paths:\n#     name = i\n    i = re.sub(\"[^A-Z0-9]\", \"\", str(i))\n    i = i.replace(\"JPG\", \"\")\n    i = i.replace(\"PNG\", \"\")\n    i = i.replace(\"JPEG\", \"\")\n    i = i.replace(\"JFIF\", \"\")\n    i = i.replace(\"JFIF\", \"\")\n    i.strip()\n    image_names.append(i)\n\n#View images\nimage_names[0:5]","773622ff":"#Prediction for all images\ny_prediction = model.predict_proba(test_images) # make predictions\ny_prediction[400:500]","c985a744":"# Prediction for all images per test image\ntest_images = np.array(test_images)\npreds = []\nfor img in tqdm(test_images):\n    img = img[np.newaxis,:] # add a new dimension\n    prediction = pretrained_model.predict_proba(img) # make predictions predict_proba\n    preds.append(prediction) \npreds","106a6fcd":"#healthwheat =0 stem_rust = 2 ,leaf_rst =1\n# create a dummy dataset\nhealthy_wheat = pd.Series(range(610), name=\"healthy_wheat\", dtype=np.float32)\nstem_rust = pd.Series(range(610), name=\"stem_rust\", dtype=np.float32)\nleaf_rust = pd.Series(range(610), name=\"leaf_rust\", dtype=np.float32)\nsubmission = pd.concat([healthy_wheat,stem_rust,leaf_rust], axis=1)\n\nfor i in range(0 ,len(preds)):\n    submission.loc[i] = preds[i]","4c63f17d":"#Append the image names to the result output\nsubmission[\"ID\"] = image_names","683b0f55":"submission","faa05c50":"# 1.0 Computer Vision for Crop Disease Detection\n\n## Defining the question\n\n### Specifying the Question\n\nCreate an image recognition model to detect wheat rust. Model to recognize if:\n- Wheat is healthy\n- Has stem rust \n- Leaf rust.\n\nClassifier algorithm used is Artificial Neural Networks.\n\nThe model will enable facilitate monitoring of wheat crops and detect presence of wheat rust on stem or leaves through avenues such as smartphone images.\nThis will be a major breakthrough in the ability to monitor and control plant diseases like wheat rust that affect African livelihoods. \n\nCurrently, the farmers are dependent on agricultral proffessionals such as extension officers or agrovets for advise. These proffessionals are not easily accessible due to cost or lack of government extension services.\n\n\n### Metric for success\n- Log Loss\n- AUC- area under the curve\n\n### Understanding the context\nWhat is wheat rust?\n\nIs a plant disease that affects wheat. If affects any above-ground plant mainly stem, leaves sheaths, glumes, awns and even seed, leading to the production of pustules that contain thousands of dry yellow-orange to reddish-brown or black spores. These pustules give the appearance of \u201crust\u201d on the plant.\n\nWhat causes wheat rust?\n\nIs caused by rust fungi. Water on the leaf surface from intermittent rains or heavy dews and temperatures conducive for germination and growth of the pathogen are required for disease development.\n\nWhat is the impact of wheat rust?\nReducing crop yields and hence affecting the livelihoods of farmers and decreasing food security across the continent. \n\nHow to Manage Rust?\n- Spray fungicides\n- Destroy infected crop\n- Field Scouting\n\n\nPrevention\n- Plant rust resistant crops\n- Destroy infected crops to avoid spread.\n\nPreventing the disease totally is not possible in all scenarios because:\n- Constant changes in strains (races) of the pathogens\n- In many situations, the varieties remained resistant for only three to four years before showing signs of susceptibility\n\n### Recording the experimental design\n\nCRISP- DM methodology will be applied. Below steps will be undertaken to create the classifer.\n\n- Business understanding - understanding the background\n- Data understanding \n- Exploratory data analysis\n- Feature engineering\n- Data modelling\n- Model interpretation\n\n### Data relevance\nData is sourced from Zindi a datascience competition platform. In turn, the data was availed to Zindi by \nBulk of the data was collected in-field by CIMMYT(International Maize and Wheat Improvement Center) which is a non-profit research and training institution dedicated to development of improved varieties of wheat and maize as well as CIMMYT partners in Ethiopia and Tanzania. \n\nRemainder of the data was sourced from public images found on Google Images.\n","4f59fbe4":"Graph of accuracy and loss for training and validation","3cf4c836":"#### b) Classification Report","8f65d20f":"Results\n\na) To upload the above submission on zindi so as to get the outcome versus test.\n\nb) Combine image data augmentation and ","faefb4b3":"c) Fitting","97411e5e":"Image name is part of full image URL as above. We will seperate the name from the image path as below","97b1d563":"Import the train images from the 3 image directories. The three directories  are healthy_wheat, stem_rust and leaf_rust. Each of the directory has images which are healthy or have rust on either the stem or leaf. Therebeing, the name of the directory corresponds will be marked as the label of the image.","8aa7ec91":"### 6.2 Image Data Augmentation\n\nWe will generate more image data using ImageDataGenerator. The Image data generator package artificially creates training images through different ways of processing or combination of multiple processing, such as random rotation, shifts, shear and flips, etc.","c4100ab5":"Above shows that the data is imbalanced since it's not evenly distributed across all classes Healthy wheat has the least number of photos while stemp rust has the largest number of photos\n\nAs data is imbalanced, class weight to be calculated and passed as argument during fitting of the model","8c40d02b":"### Define the CNN model\nConvolutional Neural Networks algorith was designed to map image data to an output variable hence is the best algorithm to use.\n\nThe benefit of using CNNs is their ability to develop an internal representation of a n-dimensional image. This allows the model to learn position and scale across different images, which is important when working with images.","ffd763f6":"b) Train The model","5eb90f10":"### c) Display sample training images\n\na) Individual images","c8e750ca":"#### Mobile Net V2 Transfer Running Results\n#### a) AUC and Log Loss","aead3613":"#### Confusion Matrix","4cbfe32f":"## 2.0 Libraries Importation","20150e05":"Convert the encoded dependent values to categorical types. Reason is because ANN works best with categorical values","b4f51108":"## 6.0 Challenging the solution\n### 6.1 Transfer Learning : Model to use is MobileNetV2\n\nWith transfer learning, instead of starting the learning process from scratch, you start from patterns that have been learned when solving a different problem. This way you leverage previous learnings and avoid starting from scratch.\n\nMore about MobileNetV2 here  - > https:\/\/ai.googleblog.com\/2018\/04\/mobilenetv2-next-generation-of-on.html\n\na) Import the MobileNetV2 from keras","8d838640":"#### a) Label Encoding. \n\nThe train labels are string variables of three types i.e  healthy_wheat, stem_rust and leaf_rust. These are encoded to convert them to numerical Encoding will faciliate converting the labels to categorical variables","6758f693":"## 4.0 Images Pre-processing\n\nIn addition to images resizing done during importation, below preparation activities done before modelling.","4f5699b0":"Check the count and size of images uploaded","4f23636e":"### b) Resized Training images upload","11b2919c":"#### c) Split the test and validation.\n\nThe validation set will be used to test overfitting in our model. The test images cannot be used as they do not have labels.**","f7e485ec":"## 7.0 Subject the model to test data\n\na) Import the test data from test directory","0f2f849f":"Visualize the images distribution per label","ffb6bb9a":"b) Display batch images","9fc2e6da":"### d) Categories of Training Images","522c32c6":"### a) Image height, width and channel exploration","1ac2cec1":"We loaded the raw images for processing but encountered memory challenges. The training images took 15.5GB of ram out of available 16GB. We therefore took an decision to resize the images to 256 by 256.\n\n\n**Rationale**\nInitially, we resized to 30 by 30 but the images became too distorted\/dull. We increased to 512 but encountered computational challenges during modelling whereby GPU memory capacity(16GB) was being exhausted. \nOptimal size was hence found to be 256 by 256.\n\nImages observed to be of channel 3. Visual exploration also confirms they are coloured","33df3531":"### Optimize and compile the model\n\nOPTIMIZER: ADAM applied to minimize the loss function.\n\nLOSS: categorical_crossentropy - multi-class log loss\n\nMetrics: Categorical accuracy as it's classification problem","b54c9796":"Baseline Model Accuracy","36ea5a44":"## 3.0 Import Images","1665e7d0":"Training.\n\nClass weight parameter specified for to rectify class imbalance","5d032add":"#### b) Normalization\n\nBenefits of normalization\n1. Reduce the effect of illumination's differences.\n2. CNN converges faster on [0..1] data than on [0..255].","7078536c":"TO overcome class imbalances, below class weights will be applied to the model during fitting.\n\n2.05: healthy wheat\n\n0.81: leaf rust\n\n0.77: stem rust","035176b6":"## 5.0 Baseline Model"}}