{"cell_type":{"15075217":"code","c57d968a":"code","b073b708":"code","5229c529":"code","d3e307b3":"code","4e2e3fc5":"code","e7e778a6":"code","d95a81dd":"code","9e2910d4":"code","3d3674aa":"code","b415c76a":"code","8ac9692b":"code","43c1ec0c":"code","984a383a":"code","eaee91a9":"code","0eee4a82":"code","c737acd0":"code","d4e3ce53":"code","1e242211":"code","2806c373":"code","859a2a3e":"code","d94293c8":"code","66e241d6":"code","51ceb3f4":"code","49198aeb":"code","be23150b":"markdown","57c69ecb":"markdown","19675d03":"markdown","2c0761f8":"markdown","4dc1f3cc":"markdown","41c88a42":"markdown","643daf65":"markdown"},"source":{"15075217":"import os\nimport platform\nimport numpy as np\nimport pandas as pd\nimport random as python_random\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\n\nos.environ['PYTHONHASHSEED'] = '73'\npython_random.seed(73)\nnp.random.seed(73)\n\nprint(platform.platform())\n%matplotlib inline","c57d968a":"### TPU Check\nimport tensorflow as tf\ntf.random.set_seed(73)\nTPU_INIT = False\n\nif TPU_INIT:\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n        tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    \n    except ValueError:\n        raise BaseException('ERROR: Not connected to a TPU runtime!')\nelse:\n    !nvidia-smi\n    \nclear_output()\nprint(\"Tensorflow version \" + tf.__version__)","b073b708":"MyDrive = '\/kaggle\/working'\nclear_output()\n\nDataDir = '\/kaggle\/input\/a-large-scale-fish-dataset\/Fish_Dataset\/Fish_Dataset'\nos.listdir(DataDir)","5229c529":"import cv2\nimport time\nimport imageio\nimport imgaug.augmenters as iaa\nimport imgaug as ia\nfrom tqdm import tqdm\nia.seed(73)\n\nIMG_SIZE = 224\nColorCh = 3\n\nCATEGORIES = []\nfor list_ in os.listdir(DataDir):\n    if not '.' in list_:\n        CATEGORIES.append(list_)\n\nprint(CATEGORIES)","d3e307b3":"def isValid(text):\n    supported_types = ['.png', '.jpg', '.jpeg']\n    for img_type in supported_types:\n        if img_type in text:\n            return True\n        else:\n            return False\n\nimport random\nrandom.seed(73)\n\ndef prepareData(Dir, split_ratio):\n    X = []\n    y = []\n    Frame = []\n    \n    flip = iaa.Fliplr(1.0)\n    zoom = iaa.Affine(scale=1)\n    random_brightness = iaa.Multiply((1, 1.2))\n    rotate = iaa.Affine(rotate=(-20, 20))\n    \n    for i, category in enumerate(CATEGORIES):\n        path = os.path.join(Dir, category, (category))        \n        if not os.path.isdir(path):\n            pass\n        \n        else:\n            class_num = CATEGORIES.index(category)\n            limit = 500 # images from each class\n            img_list = os.listdir(path)[0:limit]\n            random.shuffle(img_list)\n            \n            for img in tqdm(img_list):\n                if isValid(img):\n                    orig_img = cv2.imread(os.path.join(path,img) , cv2.IMREAD_COLOR)\n                    image_aug = cv2.resize(orig_img, (IMG_SIZE, IMG_SIZE), \n                                           interpolation = cv2.INTER_CUBIC)\n                    \n                    image_aug = flip(image = image_aug)\n                    image_aug = random_brightness(image = image_aug)\n                    image_aug = zoom(image = image_aug)\n                    image_aug = rotate(image = image_aug)\n                    \n                    image_aug = cv2.cvtColor(image_aug, cv2.COLOR_BGR2RGB)\n                    X.append(image_aug)\n                    y.append(class_num)\n                else:\n                    pass\n \n    if len(X) > 0:\n        train_len = int(len(X) * split_ratio)\n        \n        features = pd.DataFrame((np.array(X)).reshape(-1, IMG_SIZE * IMG_SIZE * ColorCh))\n        labels = pd.DataFrame({'label': y})\n        \n        Frame = pd.concat([features, labels], axis=1).sample(frac = 1, random_state=73)     \n        train_df, test_df = Frame[train_len:], Frame[:train_len]\n        \n        return train_df, test_df","4e2e3fc5":"train_df, test_df = prepareData(DataDir, split_ratio=0.2)","e7e778a6":"import matplotlib.pyplot as plt\n\ndef print_images(samples):\n    images = samples.drop([\"label\"],axis = 1).to_numpy()\n    labels = samples['label'].to_numpy()\n    \n    fig=plt.figure(figsize=(20, 8))\n    columns = 4\n    rows = 1\n    \n    for i, image in enumerate(images):\n        fig.add_subplot(rows,columns,i + 1)\n        title = '{}'.format(CATEGORIES[labels[i]])\n        Sample_image = image.reshape(IMG_SIZE, IMG_SIZE, ColorCh)\n        plt.imshow(Sample_image, cmap='gray')\n        plt.title(title)\n        \n    plt.show()\n        \nlabel_count = pd.Series(test_df['label'].values.ravel()).value_counts()\nfor label in list(label_count.index.values):\n    samples = (test_df[test_df['label']==int(label)].iloc[0:4])\n    print_images(samples)\n\n%matplotlib inline","d95a81dd":"print('> DataFrame shape: ',train_df.shape)\nprint('> {} image data '.format(train_df.shape[0]))\nprint('> {} --> ({} * {} * {}) pixels + 1 label for each image (1d scaled)\\n'.format(train_df.shape[1], IMG_SIZE,IMG_SIZE, ColorCh))\n\nlabel_count_train = pd.Series(train_df['label'].values.ravel()).value_counts()\nn_classes = len(label_count_train)\nprint('> Total Classes: {}'.format(n_classes))","9e2910d4":"print('> label count for train')\nfor i in range(len(label_count_train)):\n    print('> {} : {}'.format(CATEGORIES[i], label_count_train[i]))","3d3674aa":"print('> label count for test')\nlabel_count_test = pd.Series(test_df['label'].values.ravel()).value_counts()\nfor i in range(len(label_count_test)):\n    print('> {} : {}'.format(CATEGORIES[i], label_count_test[i]))","b415c76a":"X_train = train_df.drop([\"label\"],axis = 1).to_numpy().reshape(-1,IMG_SIZE,IMG_SIZE,ColorCh).astype(np.float32) \/ 255.0\ny_train = train_df['label']\n\nX_test = test_df.drop([\"label\"],axis = 1).to_numpy().reshape(-1,IMG_SIZE,IMG_SIZE,ColorCh).astype(np.float32) \/ 255.0\ny_test = test_df['label']","8ac9692b":"# data summary\nprint('> {} train images'.format(X_train.shape[0]))\nprint('> {} test images'.format(X_test.shape[0]))\n\ninput_shape = X_train.shape[1:]\nprint('> input shape:', input_shape)","43c1ec0c":"from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Add\nfrom tensorflow.keras.layers import InputLayer, Input, Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D, Activation, MaxPool2D, ZeroPadding2D\nfrom keras.layers.normalization import BatchNormalization\nfrom tensorflow.keras.models import Model, Sequential\nfrom keras import activations\nfrom keras import regularizers","984a383a":"kernel_regularizer = regularizers.l2(0.0001)\nfinal_activation = 'softmax'\nentropy = 'sparse_categorical_crossentropy'","eaee91a9":"def vgg_block(num_convs, num_channels):\n    block = tf.keras.models.Sequential()\n    for _ in range(num_convs):\n        block.add(Conv2D(filters=num_channels, kernel_size = (3,3), padding=\"same\"))\n        block.add(BatchNormalization())\n        block.add(Activation('relu'))\n        \n    block.add(MaxPooling2D(pool_size=2, strides=2))\n    return block\n\ndef VGG_MODEL(n_layers):\n    supported_layers = [11, 13, 16, 19]\n    \n    if not n_layers in supported_layers:\n        print('not supported')\n        return\n    \n    if n_layers == 11:\n        conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))\n\n    if n_layers == 13:\n        conv_arch = ((2, 64), (2, 128), (2, 256), (2, 512), (2, 512))\n        \n    if n_layers == 16:\n        conv_arch = ((2, 64), (2, 128), (3, 256), (3, 512), (3, 512))\n    \n    if n_layers == 19:\n        conv_arch = ((2, 64), (2, 128), (4, 256), (4, 512), (4, 512))\n    \n    \n    vgg_model = Sequential()\n    vgg_model.add(Input(shape=input_shape))\n    \n    for (num_convs, num_channels) in conv_arch:\n        vgg_model.add(vgg_block(num_convs, num_channels))\n    \n    vgg_model.add(Flatten())\n    \n    vgg_model.add(Dense(units=4096))\n    vgg_model.add(BatchNormalization())\n    vgg_model.add(Activation('relu'))\n    vgg_model.add(Dropout(0.5, seed=73))\n    \n    vgg_model.add(Dense(units=4096))\n    vgg_model.add(BatchNormalization())\n    vgg_model.add(Activation('relu'))\n    vgg_model.add(Dropout(0.5, seed=73))\n    \n    vgg_model.add(Dense(units=n_classes, activation=final_activation))\n    \n    return vgg_model","0eee4a82":"### **Print Model Results**\n\ndef print_graph(item, index, history):\n    train_values = history.history[item][0:index]\n    plt.plot(train_values)\n    test_values = history.history['val_' + item][0:index]\n    plt.plot(test_values)\n    plt.legend(['training','validation'])\n    plt.title('Training and validation '+ item)\n    plt.xlabel('epoch')\n    plt.show()\n\n\ndef get_best_epoch(test_loss, history):\n    for key, item in enumerate(history.history.items()):\n        (name, arr) = item\n        if name == 'val_loss':\n            for i in range(len(arr)):\n                if round(test_loss, 2) == round(arr[i], 2):\n                    return i\n        \n\ndef model_summary(model, history):\n    print('---'*30)\n    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n\n    if history:\n        index = get_best_epoch(test_loss, history)\n        print('Best Epochs: ', index)\n\n        train_accuracy = history.history['accuracy'][index]\n        train_loss = history.history['loss'][index]\n\n        print('Accuracy on train:',train_accuracy,'\\tLoss on train:',train_loss)\n        print('Accuracy on test:',test_accuracy,'\\tLoss on test:',test_loss)\n        print_graph('loss', index, history)\n        print_graph('accuracy', index, history)\n        print('---'*30)","c737acd0":"## **Call Backs**\nfrom tensorflow.keras.callbacks import Callback, ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping, ReduceLROnPlateau\n          \nEPOCHS = 50\npatience = 5\n\nstart_lr = 0.00001\nmin_lr = 0.00001\nmax_lr = 0.00005\nbatch_size = 16\n\nif TPU_INIT:\n    max_lr = max_lr * tpu_strategy.num_replicas_in_sync\n    batch_size = batch_size * tpu_strategy.num_replicas_in_sync\n\n\nrampup_epochs = 5\nsustain_epochs = 0\nexp_decay = .8\n\ndef lrfn(epoch):\n    if epoch < rampup_epochs:\n        return (max_lr - start_lr)\/rampup_epochs * epoch + start_lr\n    elif epoch < rampup_epochs + sustain_epochs:\n        return max_lr\n    else:\n        return (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr\n\n    \nclass myCallback(Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if ((logs.get('accuracy')>=0.999) | (logs.get('loss') <= 0.01)):\n            print(\"\\nLimits Reached cancelling training!\")\n            self.model.stop_training = True\n\n            \n            \nend_callback = myCallback()\n\nlr_plat = ReduceLROnPlateau(patience = 2, mode = 'min')\n\nlr_callback = LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=False)\n\nearly_stopping = EarlyStopping(patience = patience, monitor='val_loss',\n                                 mode='min', restore_best_weights=True, \n                                 verbose = 1, min_delta = .00075)\n\n\ncheckpoint_filepath = 'Fish_Weights.h5'\n\nmodel_checkpoints = ModelCheckpoint(filepath=checkpoint_filepath,\n                                        save_weights_only=True,\n                                        monitor='val_loss',\n                                        mode='min',\n                                        verbose = 1,\n                                        save_best_only=True)\n\nimport datetime\nlog_dir=\"logs\/fit\/\" + '_' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")  \ntensorboard_callback = TensorBoard(log_dir = log_dir, write_graph=True, histogram_freq=1)\n\ncallbacks = [end_callback, lr_callback, model_checkpoints, tensorboard_callback, early_stopping, lr_plat]\n    \nif TPU_INIT:\n        callbacks = [end_callback, lr_callback, model_checkpoints, early_stopping, lr_plat]\n        ","d4e3ce53":"\ndef CompileModel(model):\n    model.summary()\n    model.compile(optimizer='adam', loss=entropy,metrics=['accuracy'])\n    return model\n\n\ndef FitModel(model, name):\n\n    history = model.fit(X_train,y_train,\n                              epochs=EPOCHS,\n                              callbacks=callbacks,\n                              batch_size = batch_size,\n                              validation_data = (X_test, y_test),\n                              )\n    \n    model.load_weights(checkpoint_filepath)\n\n    final_accuracy_avg = np.mean(history.history[\"val_accuracy\"][-5:])\n\n    final_loss = history.history[\"val_loss\"][-1]\n  \n    group = {history: 'history', name: 'name', model: 'model', final_accuracy_avg:'acc', final_loss:'loss'}\n\n    print('\\n')\n    print('---'*30)\n    print(name,' Model')\n    print('Total Epochs :', len(history.history['loss']))\n    print('Accuracy on train:',history.history['accuracy'][-1],'\\tLoss on train:',history.history['loss'][-1])\n    print('Accuracy on val:', final_accuracy_avg ,'\\tLoss on val:', final_loss)\n    print('---'*30)\n\n    return model, history","1e242211":"skip_training = False\n\nmode = 'proMode'\n\ndef BuildModel():\n    if TPU_INIT:\n        with tpu_strategy.scope():\n            prepared_model = VGG_MODEL(16)\n            compiled_model = CompileModel(prepared_model)\n    else:\n        prepared_model = VGG_MODEL(16)\n        compiled_model = CompileModel(prepared_model)\n\n    return compiled_model\n\ncompiled_model = BuildModel()\nmodel, history = FitModel(compiled_model, 'vgg')\nmodel_summary(model, history)","2806c373":"model.save('fish_model.h5')","859a2a3e":"import seaborn as sns\nfrom sklearn import metrics\n\ndef test_set_results(pred_value, X, y):    \n    corr_pred = metrics.confusion_matrix(y, pred_value)\n    \n    fig=plt.figure(figsize=(10, 8))\n    sns.heatmap(corr_pred,annot=True, fmt=\"d\",cmap=\"Blues\")\n    plt.show()\n    ","d94293c8":"from sklearn.metrics import roc_curve, roc_auc_score, plot_roc_curve, accuracy_score, classification_report, confusion_matrix\n\ndef printResults(model, X, y):  \n    predictions = model.predict(X, batch_size=4)\n    pred = np.argmax(predictions, axis=1)\n    pred = [CATEGORIES[k] for k in pred]\n    \n    target_names = ['negative','positive']\n    print(classification_report(y, predictions.argmax(axis=1), target_names=CATEGORIES))\n    test_set_results(predictions.argmax(axis=1), X, y)","66e241d6":"def eval_model(model, X, y, name):\n    test_loss, test_accuracy = model.evaluate(X, y, verbose=0)\n    print('Final Accuracy on {}:'.format(name),test_accuracy,'\\tLoss on {}:'.format(name),test_loss)\n    print('---'*30)\n\npath = 'fish_model.h5'\nprint('\\nloading at {}'.format(path))\n\nif os.path.exists(path):\n    model = tf.keras.models.load_model(path)\n    print('---'*30)\n    eval_model(model, X_test, y_test, 'test')\n    printResults(model, X_test, y_test)","51ceb3f4":"from IPython.display import FileLink","49198aeb":"# tensorflow model\nFileLink(r'.\/fish_model.h5')","be23150b":"## **Model Evaluation on the TestSet**","57c69ecb":"## **Compile** and **Fit Model**","19675d03":"## **VGG Arch [11, 13, 16, 19]**","2c0761f8":"![](https:\/\/miro.medium.com\/max\/1658\/1*4F-9zrU07yhwj6gChX_q-Q.png)","4dc1f3cc":"## **Data Preparation & Argumentation**","41c88a42":"## **Build Model**","643daf65":"## **Data Summary**"}}