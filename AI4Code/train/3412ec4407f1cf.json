{"cell_type":{"1e015488":"code","f4080333":"code","edf60c59":"code","fcdf4e64":"code","2a831627":"code","9ea2ecd5":"code","1cd6bd1a":"code","eba9d357":"code","a600275a":"code","4b567656":"code","16929a0a":"code","a7720986":"code","25de044c":"code","1d25672c":"code","af2126c8":"code","e16c6569":"code","ebba1249":"code","132d8cad":"code","9bb936a4":"code","3b7c5656":"code","7c7bf2bf":"code","129ce156":"code","6a9ece43":"code","ea7efb2e":"markdown","29f6a2ca":"markdown","5e21be54":"markdown","f3db2d49":"markdown","870208a8":"markdown","c0d51cb7":"markdown","b49ac60c":"markdown","6630ada8":"markdown"},"source":{"1e015488":"!pip install nltk\nimport os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport nltk \nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport re\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom textblob import TextBlob \nfrom sklearn.feature_extraction.text import TfidfVectorizer\n#!pip install xgboost\nfrom xgboost import XGBRegressor \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('ggplot')","f4080333":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)","edf60c59":"data = pd.read_csv(r'..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv')\ndata.head()\nn = len(data)","fcdf4e64":"\n\ntrain_data = pd.read_csv(r'..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv')\n#train_data = train_data[:n]\ntrain_data.head()\ntext = data['text']\n\n","2a831627":"#train_data['score'] = score\ntrain_data.head()","9ea2ecd5":"#   Sentence length in \ntext.str.len().hist()","1cd6bd1a":"#Word Length\narr = [ ]\nfor i in text:\n    tmp = i.split(' ')\n    arr.append(len(tmp))\nplt.figure(figsize = (10,8))\nplt.hist(arr)","eba9d357":"stop = set(stopwords.words('english'))\ncorpus = []\ntmp = []\nfor i in text : \n    tmp.append(i.split())\n    \nfor i in tmp:\n    for words in i :\n        corpus.append(words)\nfrom collections import defaultdict\ndic = defaultdict(int)\nfor i in corpus: \n    if i in stop :\n        dic[i] = dic[i] + 1\nval = dic.values()\nval = sorted(val,reverse = True)\nval = val[0:11]\nd = {}\nfor i in list(dic.keys()):\n    if dic[i] in val: \n        d[i] = dic[i]\n# Top 10 Stopwords in corpus\nplt.figure(figsize=(10,8))\nplt.bar(list(d.keys()), list(d.values()), color = 'Red')","a600275a":"from nltk.util import ngrams\nimport collections\ntex = ''\n#word_tokenize = nltk.download('word_tokenize')\nfor i in text : \n    tex = tex + str(i.strip('[]'))\ntok = tex.split()\nb_grams = ngrams(tok, 2) \nt_grams = ngrams(tok,3)\nbigrams = collections.Counter(b_grams)\ntrigrams = collections.Counter(t_grams)\nbi= bigrams.most_common(10)\nti= trigrams.most_common(10)","4b567656":"def plot_ngrams(bi):\n    word = []\n    idx = []\n    for i in range(len(bi)):\n        word.append(str(bi[i][0]))\n        idx.append(bi[i][1])\n    plt.figure(figsize = (10,8))\n    plt.bar(word,idx, color = 'Green')\n    #plt.xticks(word, word, rotation = 'vertical')\n    plt.xlabel('Ngrams')\n    plt.ylabel('Frequency')\n    plt.title('Ngram with Frequency')\n    plt.show()\n#Bigrams\nplot_ngrams(bi)","16929a0a":"#Trigrams\nplot_ngrams(ti)","a7720986":"#Stopwords\nimport nltk\nnltk.download('stopwords')\nstop = set(stopwords.words('english'))","25de044c":"def lowercase(te):\n    tmp = []\n    #Complete \n    for i in te : \n        tmp.append(i.lower())\n    return tmp\ndef remove_symbols(text):\n    #commplete\n    tmp = []\n    for i in text: \n        #i = re.sub(r'\\n','',i)\n       # i = re.sub(r'\"\"', ' ',i)\n        i = re.sub(r'[^\\w]',' ',i) #Remove all types of symbols from string\n        tmp.append(i.replace(') ',''))\n    return tmp\ndef remove_stopwords(text): \n    tmp = []\n    for i in text: \n        te = str(i)\n        words = word_tokenize(te)\n        token_without = [word for word in words if not word in stop]\n        s = ''\n        for w in token_without : \n            s = s + w + ' '\n        tmp.append(s)\n    return tmp\ndef stemming(text): \n    tmp = []\n    for i in text : \n        te = str(i)\n        lemmatizer = WordNetLemmatizer()\n        words = word_tokenize(te)\n        s = ''\n        for w in words:\n            rw = lemmatizer.lemmatize(w)\n            s = s + rw + ' '\n        tmp.append(s)\n    return tmp\ndef stemming_lem_stop(text):\n    tmp = []\n    for i in text:\n        te = str(i)\n        words = word_tokenize(te)\n        ps = PorterStemmer()\n        lemmatizer = WordNetLemmatizer()\n        token_without = [word for word in words if not word in stop] #Remove Stopwords from token\n        s = ''\n        for w in token_without: \n            rootword = ps.stem(w)\n            rw = lemmatizer.lemmatize(rootword) #Lemmatization\n            s = s + rw + ' '\n        tmp.append(s)\n    return tmp\n\n","1d25672c":"train1 = lowercase(train_data['comment_text'])\ntrain2 = remove_symbols(train1)\ntrain4 = stemming(train2)\ntrain3 = remove_stopwords(train4)\ntest1 = lowercase(text)\ntest2 = remove_symbols(test1)\ntest4 = stemming(test2)\ntest3 = remove_stopwords(test4)","af2126c8":"pol = []\nfor i in train3:\n    analysis = TextBlob(i)\n    x = analysis.sentiment.polarity\n    NewValue = (((x - (-1)) * 1) \/ 2) + 0 #NewValue = (((OldValue - OldMin) * NewRange) \/ OldRange) + NewMin\n    if NewValue == 0.5 :\n        pol.append(0)\n    else : \n        pol.append(round(NewValue,2))","e16c6569":"com = []\ndi = {}\npol1 = []\nfor i in range(len(data['comment_id'])): \n    com.append(data['comment_id'][i])\n","ebba1249":"tfid = TfidfVectorizer(max_features = 50, min_df= 3, max_df=0.5, analyzer = 'word')\nres = tfid.fit_transform(train3).toarray()\nres1 = tfid.fit_transform(test3).toarray()","132d8cad":"# Define initial best params and MAE\\\nfrom numpy import arange\n\nparams = {\n    # Parameters that we are going to tune.\n    'max_depth':6,\n    'min_child_weight': 1,\n    'eta':.3,\n    'subsample': 1,\n    'colsample_bytree': 1,\n    # Other parameters\n    #'objective':'reg:linear',\n}\ngridsearch_params = [\n    (max_depth, min_child_weight, eta)\n    for max_depth in range(9,12)\n    for min_child_weight in range(5,8)\n    for eta in arange(0.1,1,0.1)\n]\nfrom sklearn.metrics import mean_absolute_error\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\ndtrain = xgb.DMatrix(res,pol)\nmin_mae = float(\"Inf\")\nbest_params = None\nfor max_depth, min_child_weight, eta in gridsearch_params:\n    print(\"CV with max_depth={}, min_child_weight={}, eta={}\".format(\n                             max_depth,\n                             min_child_weight,\n                             eta ))    # Update our parameters\n    params['max_depth'] = max_depth\n    params['min_child_weight'] = min_child_weight\n    params['eta'] = eta\n # Run CV\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        #num_boost_round=num_boost_round,\n        seed=42,\n        nfold=5,\n        metrics={'mae'},\n        early_stopping_rounds=5\n    )    # Update best MAE\n    mean_mae = cv_results['test-mae-mean'].min()\n    boost_rounds = cv_results['test-mae-mean'].argmin()\n    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n    if mean_mae < min_mae:\n        min_mae = mean_mae\n        best_params = (max_depth,min_child_weight, eta)\n        print(\"Best params: {}, {}, {}, MAE: {}\".format(best_params[0], best_params[1],best_params[2], min_mae))","9bb936a4":"d = best_params[0]\nc = best_params[1]\ne = best_params[2]","3b7c5656":"model = XGBRegressor(n_estimators = 1000,max_depth = d,min_child_weight= c,eta = e, subsample=0.7, colsample_bytree=0.8)\nmodel.fit(res,pol)\ny_pred = abs(model.predict(res1))","7c7bf2bf":"di = {'comment_id': com, 'score': y_pred}\ndf = pd.DataFrame(di)","129ce156":"df.head()","6a9ece43":"#Output file\ndf.to_csv(\"submission.csv\", index=False)","ea7efb2e":"<a href=\"https:\/\/fontmeme.com\/saw-font\/\"><img src=\"https:\/\/fontmeme.com\/permalink\/211129\/78fa88cb341b3f050567ea9e93dbde80.png\" alt=\"saw-font\" border=\"0\"><\/a>","29f6a2ca":"<img src= \"https:\/\/clickhole.com\/wp-content\/uploads\/2018\/11\/mdc4jnl2amnpmnajblxn.jpg\" alt =\"Jigsaw\" style='width: 1000px; height: 500px' align = 'center'>","5e21be54":"<a href=\"https:\/\/fontmeme.com\/saw-font\/\"><img src=\"https:\/\/fontmeme.com\/permalink\/211129\/81b94964d2426cbf298cc2086f45660a.png\" alt=\"saw-font\" border=\"0\"><\/a>","f3db2d49":"<a href=\"https:\/\/fontmeme.com\/saw-font\/\"><img src=\"https:\/\/fontmeme.com\/permalink\/211129\/9567265046ea503e73e1222c8bfde7c1.png\" alt=\"saw-font\" border=\"0\"><\/a>","870208a8":"<a href=\"https:\/\/fontmeme.com\/saw-font\/\"><img src=\"https:\/\/fontmeme.com\/permalink\/211129\/02a90505225940c50c884464c1593339.png\" alt=\"saw-font\" border=\"0\"><\/a>","c0d51cb7":"<a href=\"https:\/\/fontmeme.com\/saw-font\/\"><img src=\"https:\/\/fontmeme.com\/permalink\/211129\/3889dcaf8d87a0ebe7d2b7cef1c9e617.png\" alt=\"saw-font\" border=\"0\"><\/a>\nHere, I had calculated the score for training set with the help of TextBlob Sentiment Analysis \nfrom Jigsaw Toxic Comment Classification Competition Datasets","b49ac60c":"<a href=\"https:\/\/fontmeme.com\/saw-font\/\"><img src=\"https:\/\/fontmeme.com\/permalink\/211129\/08e28926ea46733dc2b519f7d145a7ab.png\" alt=\"saw-font\" border=\"0\"><\/a>","6630ada8":"<a href=\"https:\/\/fontmeme.com\/saw-font\/\"><img src=\"https:\/\/fontmeme.com\/permalink\/211129\/18143700fb8aa5b735bd7a3d0587da71.png\" alt=\"saw-font\" border=\"0\"><\/a>"}}