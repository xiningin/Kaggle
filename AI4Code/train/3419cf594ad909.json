{"cell_type":{"969193d5":"code","126a0cf3":"code","3fe482b3":"code","c88ac083":"code","8b13f8fb":"code","0816d54e":"code","e98f5199":"code","7e4721e7":"code","78083a54":"code","02081668":"code","6043597c":"code","c4a8d114":"markdown","0dcd39a7":"markdown","52351306":"markdown"},"source":{"969193d5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","126a0cf3":"train_data = pd.read_csv(\"\/kaggle\/input\/home-data-for-ml-course\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/home-data-for-ml-course\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/home-data-for-ml-course\/sample_submission.csv\")\ntrain_data","3fe482b3":"y_train = train_data.SalePrice\nX_train = train_data.drop(['SalePrice'], axis = 1)\nX_train.drop(['Id'], axis = 1, inplace = True)\nX_test = test_data.drop(['Id'], axis = 1)","c88ac083":"num_cols = [col for col in X_train.columns if X_train[col].dtype in ['int64', 'float64']]\ncat_cols = [col for col in X_train.columns if X_train[col].nunique() < 10 and \n            X_train[col].dtype == \"object\"]","8b13f8fb":"all_cols = cat_cols + num_cols\nX_train = X_train[all_cols].copy()\nX_test = X_test[all_cols].copy()\n\nX_train","0816d54e":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\nnum_pipeline = Pipeline([\n    ('num_imputer', SimpleImputer(strategy = 'median')), \n    ('num_scaler', StandardScaler())\n])\n\ncat_pipeline = Pipeline([\n    ('cat_imputer', SimpleImputer(strategy = 'most_frequent')), \n    ('cat_encoder', OneHotEncoder(sparse = False, handle_unknown = 'ignore')), \n    ('cat_scaler', StandardScaler())\n])\n\npreprocess_pipeline = ColumnTransformer([\n    ('num', num_pipeline, num_cols), \n    ('cat', cat_pipeline, cat_cols), \n])","e98f5199":"X_train = preprocess_pipeline.fit_transform(X_train)\nX_test = preprocess_pipeline.transform(X_test)","7e4721e7":"from xgboost import XGBRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nxgb_reg = XGBRegressor(n_estimators = 150, learning_rate = 0.10, n_jobs = -1, random_state = 42)\n\nparam_distribs = [{\n    'max_depth': randint(2, 6), \n    'min_child_weight': randint(2, 5),\n    'gamma':[i \/ 10.0 for i in range(3, 6)], \n}]\n\nrnd_search = RandomizedSearchCV(xgb_reg, param_distributions = param_distribs, n_iter = 10, cv = 5, scoring = 'neg_mean_absolute_error', \n                             random_state = 42)\nrnd_search.fit(X_train, y_train)","78083a54":"final_model = rnd_search.best_estimator_\nfinal_model","02081668":"cvres = rnd_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","6043597c":"y_pred = final_model.predict(X_test)\nsubmission.SalePrice = y_pred\nsubmission.to_csv('submission.csv',index = False)","c4a8d114":"## Pipelines","0dcd39a7":"## EDA","52351306":"## XGBRegressor + RandomizedSearchCV"}}