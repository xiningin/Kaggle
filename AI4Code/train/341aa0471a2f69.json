{"cell_type":{"0fce367e":"code","d0cba6c2":"code","afd9638b":"code","28de3f64":"code","cb0d652c":"code","a90e14de":"code","b26b753d":"code","209cc5a4":"code","f1fa8f56":"code","cb744237":"code","4960fc89":"code","f1275fdf":"code","fa3acdde":"code","3ad8530f":"code","07474323":"code","57a36e08":"code","b901ac1d":"code","fff9a1f4":"code","8b4bb81c":"code","90e8e07b":"code","040ee4db":"code","71bba2ec":"code","fefb0c66":"code","a0efa995":"code","771a0f8a":"code","1059231a":"code","d850e9d8":"code","43068e14":"code","f959f1b8":"code","bb9f9509":"code","f4acbd9a":"code","257c2628":"code","9d2f434f":"markdown","7be46040":"markdown","9fceabf1":"markdown","1c9a4f89":"markdown","24d6865f":"markdown","04292005":"markdown","e5aec551":"markdown","19e1c283":"markdown","2b6a89e9":"markdown","c91081a9":"markdown","2ca7a1f8":"markdown","71aa49d4":"markdown","36a64eea":"markdown","44ad3dd8":"markdown","89d7705d":"markdown"},"source":{"0fce367e":"# import libraries for data analysis\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","d0cba6c2":"# list data files that are connected to the kernel\nimport os\nos.listdir('..\/input\/')","afd9638b":"# read the train.csv file into a datframe\ndf_train = pd.read_csv('..\/input\/train.csv')\nprint('Shape: ', df_train.shape)\ndf_train.head()","28de3f64":"# read the test.csv file into a datframe\ndf_test = pd.read_csv('..\/input\/test.csv')\nprint('Shape: ', df_test.shape)\ndf_test.head()","cb0d652c":"df_train.info()","a90e14de":"df_test.info()","b26b753d":"# number of each type of column\ndf_train.dtypes.value_counts()","209cc5a4":"plt.scatter(df_train['GrLivArea'], df_train['SalePrice'])","f1fa8f56":"# remove outliers from train dataset\ndf_train = df_train[df_train['GrLivArea'] < 4000]\n\nprint('Shape: ', df_train.shape)","cb744237":"# create df_full by merging train and test data\ndf_full = df_train.append(df_test, sort=False)\nprint('Shape: ', df_full.shape)","4960fc89":"# remove target value from the full dataset\ndf_full = df_full.drop(['SalePrice'], axis=1)\n\nprint('Shape: ', df_full.shape)","f1275fdf":"# create new feature 'RemodAdd' which contains 1 if some remodeling or additions have been done, else 0\ndf_full['RemodAdd'] = df_full.apply(lambda x: '0' if x['YearRemodAdd'] == x['YearBuilt'] else '1', axis=1)\n\n# create new features which contains 1 if the feature exists, else 0\ndf_full['hasPool'] = df_full.apply(lambda x: '1' if x['PoolArea'] > 0 else '1', axis=1)\ndf_full['has2ndFloor'] = df_full.apply(lambda x: '1' if x['2ndFlrSF'] > 0 else '1', axis=1)\ndf_full['hasGarage'] = df_full.apply(lambda x: '1' if x['GarageArea'] > 0 else '1', axis=1)\ndf_full['hasBsmt'] = df_full.apply(lambda x: '1' if x['TotalBsmtSF'] > 0 else '1', axis=1)\ndf_full['hasFireplace'] = df_full.apply(lambda x: '1' if x['Fireplaces'] > 0 else '1', axis=1)\n\n# change the type to numeric\ndf_full[['RemodAdd', 'hasPool', 'has2ndFloor', 'hasGarage', 'hasBsmt', 'hasFireplace']] = df_full[['RemodAdd', 'hasPool', 'has2ndFloor', 'hasGarage', 'hasBsmt', 'hasFireplace']].apply(pd.to_numeric)","fa3acdde":"# create a list with all ordinal features having standardized quality descriptions\nquality_categories = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence']\n\n# create a mapping list for the standardized values of the quality_category features\nquality_mapping = {\n    'Ex': '5', 'Gd': '4', 'TA': '3', 'Fa': '2', 'Po': '1', 'NA': '0',\n    'Av': '3', 'Mn': '2', 'No': '1', # specific to BsmtExposure feature\n    'GdPrv': '4', 'MnPrv': '3', 'GdWo': '2', 'MnWw': '1' # specific to Fence feature\n}\n\n# create a for loop to replace the standardized values with digits\n# loop over featues from the list quality_categories in the full dataset\nfor col in df_full[quality_categories]:\n    # fill NaN's with string 'NA'\n    df_full[col].fillna('NA', inplace=True)\n    # replace the values according to quality_mapping\n    df_full.replace({col: quality_mapping}, inplace=True)\n    # create new column as 'OldName_bin' where the data type is numeric\n    df_full[col + '_bin'] = df_full[col].apply(pd.to_numeric)","3ad8530f":"# create a list with all the other ordinal features\nordinal_categories = ['Utilities', 'BsmtFinType1', 'BsmtFinType2', 'Functional', 'GarageFinish']\n\n# import encoder library\nfrom sklearn.preprocessing import LabelEncoder\n\n# create a for loop to encode the categorical features\n# loop over featues from the list ordinal_categories in the full dataset\nfor col in df_full[ordinal_categories]:\n    # fill NaN's with string 'NA'\n    df_full[col].fillna('NA', inplace=True)\n    # define the encoder instance\n    enc = LabelEncoder()\n    # create new column as 'OldName_bin' with the encoded values\n    df_full[col + '_bin'] = enc.fit_transform(df_full[col].astype(str))","07474323":"# combine previously encoded features into a list\ncolumns_to_drop = quality_categories + ordinal_categories\n\n# drop already encoded features\ndf_full.drop(columns_to_drop, axis=1, inplace=True)\n\n# check number of each type of column\ndf_full.dtypes.value_counts()","57a36e08":"nulls = np.sum(df_full.isnull())\nnullcols = nulls.loc[(nulls != 0)]\ndtypes = df_full.dtypes\ndtypes2 = dtypes.loc[(nulls != 0)]\ninfo = pd.concat([nullcols, dtypes2], axis=1).sort_values(by=0, ascending=False)\n\nprint(info)\nprint(\"There are\", len(nullcols), \"columns with missing values\")","b901ac1d":"# drop columns with high amount of missing values\ndf_full.drop(['MiscFeature','Alley'], axis=1, inplace=True)\n\n# create a list of columns for using the most common value method\ncolumn_nans = ['GarageType', 'MSZoning']\nmost_common = ['Electrical', 'Exterior2nd', 'Exterior1st', 'SaleType']\n\n# fill columns with 'NA' or 'None'\ndf_full.loc[:,column_nans] = df_full.loc[:,column_nans].fillna('NA')\ndf_full.loc[:, 'MasVnrType'] = df_full.loc[:, 'MasVnrType'].fillna('None')\n\n# create a for loop to fill some columns with the most common value\n# loop over featues from the list most_common in the full dataset\nfor col in df_full[most_common]:\n    # fill NaN's with most common value\n    df_full[col].fillna(df_full[col].mode()[0], inplace=True)","fff9a1f4":"# create a list of all numeric columns\nnumeric_columns = df_full._get_numeric_data().columns\n\n# import libraries\nfrom sklearn.impute import SimpleImputer\n\n# define instance\nimp = SimpleImputer(strategy='median')\n\n# impute \ndf_full[numeric_columns] = imp.fit_transform(df_full[numeric_columns])","8b4bb81c":"# one-hot encoding of categorical features\ndf_full = pd.get_dummies(df_full)\n\nprint('Full data shape: ', df_full.shape)","90e8e07b":"# create a copy of the dataframe and add back the target\ndf_corrs = df_full.copy()\ndf_corrs['SalePrice'] = df_train['SalePrice']\n\n# calculate all correlations in train data\ncorrs = df_corrs.corr()\ncorrs = corrs.sort_values('SalePrice', ascending = False)\n\n# 10 most positive correlations\npd.DataFrame(corrs['SalePrice'].head(10))","040ee4db":"# 10 most negative correlations\npd.DataFrame(corrs['SalePrice'].dropna().tail(10))","71bba2ec":"# set the threshold\nthreshold = 0.8\n\n# empty dictionary to hold correlated variables\nabove_threshold_vars = {}\n\n# for each column, record the variables that are above the threshold\nfor col in corrs:\n    above_threshold_vars[col] = list(corrs.index[corrs[col] > threshold])","fefb0c66":"# track columns to remove and columns already examined\ncols_to_remove = []\ncols_seen = []\ncols_to_remove_pair = []\n\n# iterate through columns and correlated columns\nfor key, value in above_threshold_vars.items():\n    # keep track of columns already examined\n    cols_seen.append(key)\n    for x in value:\n        if x == key:\n            next\n        else:\n            # only want to remove one in a pair\n            if x not in cols_seen:\n                cols_to_remove.append(x)\n                cols_to_remove_pair.append(key)\n            \ncols_to_remove = list(set(cols_to_remove))\nprint('Number of columns to remove: ', len(cols_to_remove))","a0efa995":"df_full_corrs_removed = df_full.drop(columns = cols_to_remove)\n\nprint('Full data with removed corrs shape: ', df_full_corrs_removed.shape)","771a0f8a":"# split the data from df_full back into train and test datasets using the length of df_train\ntrain = df_full_corrs_removed.iloc[:len(df_train),:]\ntest = df_full_corrs_removed.iloc[len(df_train):,:]\n\n# extract the labels\ntrain_labels = df_train['SalePrice']\n\n# log transform the labels:\ntrain_labels_log = np.log1p(train_labels)\n\n# align train and test data, keep only columns present in both dataframes\ntrain, test = train.align(test, join = 'inner', axis = 1)\n\nprint('Train shape: ', train.shape)\nprint('Test shape: ', test.shape)","1059231a":"# import libraries\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.pipeline import Pipeline\n\n# define the number number of splits, alphas\nkfolds = KFold(n_splits=5, random_state=42)\n\n# define a function to calculate the RMSE (root mean squared logarithmic error) for a given model\ndef rmse_cv(model):\n    rmse = np.sqrt(-cross_val_score(model, train, train_labels, scoring=\"neg_mean_squared_error\", cv = kfolds))\n    return(rmse)","d850e9d8":"# import libraries\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import RidgeCV\n\n# define a function to select the best alpha for the ridge model\ndef ridge_selector(k):\n    # create a pipeline for the ridge model\n    ridge_model = Pipeline([\n        ('scl', RobustScaler()),\n        ('ridge', RidgeCV(alphas = [k], cv= kfolds))\n    ]).fit(train, train_labels)\n    # use the rmse_cv function to calculate the RMSE\n    ridge_rmse = rmse_cv(ridge_model).mean()\n    \n    return(ridge_rmse)\n\n# create a list of alpha values to try out\nalphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 20, 30, 40, 60, 80]\n\n# create empty list\nridge_scores = []\n\nfor alpha in alphas:\n    score = ridge_selector(alpha)\n    ridge_scores.append(score)","43068e14":"plt.plot(alphas, ridge_scores, label='Ridge')\nplt.legend('center')\nplt.xlabel('alpha')\nplt.ylabel('score')\n\nridge_score_table = pd.DataFrame(ridge_scores, alphas, columns=['RMSE'])\nridge_score_table","f959f1b8":"# import libraries\nfrom sklearn.linear_model import LassoCV\n\nalphas = [5, 1, 0.1, 0.001, 0.0005]\n\nlasso_rmse = rmse_cv(LassoCV(alphas = alphas)).mean()\n\nprint('Lowest average RMSE is:', lasso_rmse.min())","bb9f9509":"from lightgbm import LGBMRegressor\n\nlgbm_model = Pipeline([\n    ('scl', RobustScaler()),\n    ('lightgbm', LGBMRegressor(objective='regression',\n                               n_estimators=1000,\n                               learning_rate=0.05,\n                               num_leaves=5,\n                               max_bin = 55,\n                               bagging_fraction = 0.8,\n                               bagging_freq = 5,\n                               feature_fraction = 0.2319,\n                               feature_fraction_seed=9,\n                               bagging_seed=9,\n                               min_data_in_leaf =6, \n                               min_sum_hessian_in_leaf = 11))\n                              \n]).fit(train, train_labels)\n\nrmse_cv(lgbm_model).mean()","f4acbd9a":"# predict on the test data\npreds = lgbm_model.predict(test)","257c2628":"# make a submission dataframe\nsubmit = df_test.loc[:, ['Id']]\nsubmit.loc[:, 'SalePrice'] = preds\n\n# Save the submission dataframe\nsubmit.to_csv('submission.csv', index = False)","9d2f434f":"# Modeling","7be46040":"When submitted, the ridge regression model scores **0.15602.**\n\n### Lasso model","9fceabf1":"### Collinear Features","1c9a4f89":"When submitted, the lasso model scores **0.16005**\n\n### LightGBM","24d6865f":"### Outliers\n\nThe [official documentation](http:\/\/ww2.amstat.org\/publications\/jse\/v19n3\/Decock\/DataDocumentation.txt) recommends removing all houses above 4000 square feet.","04292005":"### Aligning Train and Test Data","e5aec551":"# House Price Prediction\n\n# Intro\n\nThis notebook demonstrates common ML techniques such as Feature Engineering and Modeling using linear models such as RidgeCV and LassoCV as well as the more advanced LightGBM framework which ***got a score of 0.12219 (top 22%).***\n\nSome parts of this code have been adapted from [Laurenstc's kernel](https:\/\/www.kaggle.com\/laurenstc\/top-2-of-leaderboard-advanced-fe).","19e1c283":"# Exploratory Data Analysis & Data Cleaning","2b6a89e9":"### Correlations","c91081a9":"### Encoding Categorical Features","2ca7a1f8":"# Feature Engineering & Selection\n\n### Merge both datasets","71aa49d4":"### Ridge model","36a64eea":"### Creating New Features","44ad3dd8":"### Missing Values","89d7705d":"### Encoding Ordinal Features"}}