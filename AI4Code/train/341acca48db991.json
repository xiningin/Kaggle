{"cell_type":{"c8636fac":"code","a83df577":"code","88d454cf":"code","56b78d6b":"code","dd8d0a38":"code","b5921deb":"code","bc3a62c8":"code","1c52c24c":"code","64f8405a":"code","469bdd1c":"code","cc5372ef":"code","5617e862":"code","2c220b25":"code","d2af94ce":"code","a4a1076b":"code","010f5902":"code","71874eb0":"code","81374ac7":"code","b6fb3d30":"code","8fe15720":"code","2c91fd96":"code","1af24955":"code","b9d8605a":"code","e193b45c":"code","78fdc7a2":"code","dd44dc47":"code","f35671e1":"code","3e95182b":"code","e513292f":"code","e64e5a29":"code","f5bb2edc":"code","e7e1d553":"code","cf170cf9":"markdown","6999668e":"markdown","4a273c85":"markdown","55a03795":"markdown","c418910e":"markdown","2b7e62c3":"markdown","70e3549b":"markdown","dc957d02":"markdown","1f69a279":"markdown"},"source":{"c8636fac":"# Load libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline \nimport seaborn as sns\nimport folium\nimport os\nfrom folium.plugins import HeatMap\nfrom haversine import haversine, Unit\n# Import data\ndata = pd.read_csv('\/kaggle\/input\/crimes-in-boston\/crime.csv', encoding='latin-1')\nraw_data=data\n# Peek\ndata.head()\n","a83df577":"data['DISTRICT'].unique()","88d454cf":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","56b78d6b":"data.describe()","dd8d0a38":"# Keep only data from complete years (2016, 2017)\ndata = data.loc[data['YEAR'].isin([2016,2017])]\n\n# Keep only data on UCR Part One offenses\ndata = data.loc[data['UCR_PART'] == 'Part One']\n\n# Remove unused columns\ndata = data.drop(['INCIDENT_NUMBER','OFFENSE_CODE','UCR_PART','Location'], axis=1)\n\n# Convert OCCURED_ON_DATE to datetime\ndata['OCCURRED_ON_DATE'] = pd.to_datetime(data['OCCURRED_ON_DATE'])\n\n# Fill in nans in SHOOTING column\ndata.SHOOTING.fillna('N', inplace=True)\n\n# Convert DAY_OF_WEEK to an ordered category\ndata.DAY_OF_WEEK = pd.Categorical(data.DAY_OF_WEEK, \n              categories=['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'],\n              ordered=True)\n\n# Replace -1 values in Lat\/Long with Nan\ndata.Lat.replace(-1, None, inplace=True)\ndata.Long.replace(-1, None, inplace=True)\n\n# Rename columns to something easier to type (the all-caps are annoying!)\nrename = {'OFFENSE_CODE_GROUP':'Group',\n         'OFFENSE_DESCRIPTION':'Description',\n         'DISTRICT':'District',\n         'REPORTING_AREA':'Area',\n         'SHOOTING':'Shooting',\n         'OCCURRED_ON_DATE':'Date',\n         'YEAR':'Year',\n         'MONTH':'Month',\n         'DAY_OF_WEEK':'Day',\n         'HOUR':'Hour',\n         'STREET':'Street'}\ndata.rename(index=str, columns=rename, inplace=True)\n\n# Check\ndata.head()","b5921deb":"# A few more data checks\ndata.dtypes\ndata.isnull().sum()\ndata.shape\n","bc3a62c8":"# Countplot for crime types\nsns.catplot(y='Group',\n           kind='count',\n            height=8, \n            aspect=1.5,\n            order=data.Group.value_counts().index,\n           data=data)","1c52c24c":"# Crimes by hour of the day\nsns.catplot(x='Hour',\n           kind='count',\n            height=8.27, \n            aspect=3,\n            #color='blue',\n           data=data)\nplt.xticks(size=30)\nplt.yticks(size=30)\nplt.xlabel('Hour', fontsize=40)\nplt.ylabel('Count', fontsize=40)","64f8405a":"# Crimes by day of the week\nsns.catplot(x='Day',\n           kind='count',\n            height=8, \n            aspect=3,\n           data=data)\nplt.xticks(size=30)\nplt.yticks(size=30)\nplt.xlabel('')\nplt.ylabel('Count', fontsize=40)","469bdd1c":"# # Crimes by month of year\n# months = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n# sns.catplot(x='Month',\n#            kind='count',\n#             height=8, \n#             aspect=3,\n#             #color='gray',\n#            data=data)\n# plt.xticks(np.arange(12), months, size=30)\n# plt.yticks(size=30)\n# plt.xlabel('')\n# plt.ylabel('Count', fontsize=40)","cc5372ef":"# # Create data for plotting\n# data['Day_of_year'] = data.Date.dt.dayofyear\n# data_holidays = data[data.Year == 2017].groupby(['Day_of_year']).size().reset_index(name='counts')\n\n# # Dates of major U.S. holidays in 2017\n# holidays = pd.Series(['2017-01-01', # New Years Day\n#                      '2017-01-16', # MLK Day\n#                      '2017-03-17', # St. Patrick's Day\n#                      '2017-04-17', # Boston marathon\n#                      '2017-05-29', # Memorial Day\n#                      '2017-07-04', # Independence Day\n#                      '2017-09-04', # Labor Day\n#                      '2017-10-10', # Veterans Day\n#                      '2017-11-23', # Thanksgiving\n#                      '2017-12-25']) # Christmas\n# holidays = pd.to_datetime(holidays).dt.dayofyear\n# holidays_names = ['NY',\n#                  'MLK',\n#                  'St Pats',\n#                  'Marathon',\n#                  'Mem',\n#                  'July 4',\n#                  'Labor',\n#                  'Vets',\n#                  'Thnx',\n#                  'Xmas']\n\n# import datetime as dt\n# # Plot crimes and holidays\n# fig, ax = plt.subplots(figsize=(11,6))\n# sns.lineplot(x='Day_of_year',\n#             y='counts',\n#             ax=ax,\n#             data=data_holidays)\n# plt.xlabel('Day of the year')\n# plt.vlines(holidays, 20, 80, alpha=0.5, color ='r')\n# for i in range(len(holidays)):\n#     plt.text(x=holidays[i], y=82, s=holidays_names[i])","5617e862":"# fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (18, 7))\n# # Simple scatterplot\n# sns.scatterplot(x='Lat',\n#                y='Long',\n#                 alpha=0.01,\n#                data=data,\n#                ax = axes[0])\n# # Plot districts\n# sns.scatterplot(x='Lat',\n#                y='Long',\n#                 hue='District',\n#                 alpha=0.1,\n#                data=data,\n#                ax = axes[1])","2c220b25":"# Plot districts\nsns.scatterplot(x='Lat',\n               y='Long',\n                hue='District',\n                alpha=0.1,\n               data=data)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2)","d2af94ce":"# Create basic Folium crime map\ncrime_map = folium.Map(location=[42.3125,-71.0875], \n                       tiles = \"Stamen Toner\",\n                      zoom_start = 11)\n\n# Add data for heatmp \ndata_heatmap = data[data.Year == 2017]\ndata_heatmap = data[['Lat','Long']]\ndata_heatmap = data.dropna(axis=0, subset=['Lat','Long'])\ndata_heatmap = [[row['Lat'],row['Long']] for index, row in data_heatmap.iterrows()]\nHeatMap(data_heatmap, radius=10).add_to(crime_map)\n\n# Plot!\ncrime_map","a4a1076b":"import os\nimport pandas as pd\nimport json\nimport geopandas as gpd\n\n\ngdf = gpd.read_file('\/kaggle\/input\/geo-json-boston\/Police_Districts.geojson')\ngdf.head()\nstate_data = raw_data[['DISTRICT','INCIDENT_NUMBER']].groupby('DISTRICT').agg('count') \nstate_data = state_data \/ state_data.sum()\nmerged = gdf.merge(state_data, left_on='DISTRICT', right_on='DISTRICT')\n","010f5902":"result_data = pd.read_csv('\/kaggle\/input\/distributiond\/sensitivity_specialist.csv', encoding='latin-1')\nresult_data['DISTRICT'] = result_data['Unnamed: 0']\nresult_data.drop('Unnamed: 0', inplace=True, axis= 1)\nresult_data\n\nmerged = merged.merge(result_data, left_on = 'DISTRICT', right_on = 'DISTRICT')","71874eb0":"merged","81374ac7":"merged.iloc[:, [1, 9]]","b6fb3d30":"spatial_gdf = gpd.GeoDataFrame(merged.iloc[:, [1, 8]])\ncrime_distribution = merged.iloc[:, [1, 9]]\nspecialist_dist = merged.iloc[:, [1, 10]]\nofficer_dist = merged.iloc[:, [1, 11]]\ngeo_str = spatial_gdf.to_json()","8fe15720":"import os\nimport pandas as pd\nimport json\nimport geopandas as gpd\n\n\n# Create basic Folium crime map\ncrime_map = folium.Map(location=[42.3125,-71.0875], \n                       tiles=\"Cartodb Positron\",\n                      zoom_start = 12)\n\n\nm1 = folium.Choropleth(\n    geo_data=geo_str,\n    name='crime',\n    data=crime_distribution,\n    columns=['DISTRICT', 'INCIDENT_NUMBER'],\n    key_on='properties.DISTRICT',\n    fill_color='OrRd',\n    fill_opacity=0.6,\n    line_opacity=0.6,\n    legend_name='Crime Rate (%)'\n).add_to(crime_map)\n\n\nm2 = folium.Choropleth(\n    geo_data=geo_str,\n    name='officer',\n    data=specialist_dist,\n    columns=['DISTRICT', 'specialist'],\n    key_on='properties.DISTRICT',\n    fill_color='OrRd',\n    fill_opacity=0.6,\n    line_opacity=0.6,\n    legend_name='officer allocation rate'\n).add_to(crime_map)\n\nm3 = folium.Choropleth(\n    geo_data=geo_str,\n    name='specialist',\n    data=officer_dist,\n    columns=['DISTRICT', 'office'],\n    key_on='properties.DISTRICT',\n    fill_color='OrRd',\n    fill_opacity=0.6,\n    line_opacity=0.6,\n    legend_name='specialist allocation rate'\n).add_to(crime_map)\n\ncrime_map.add_child(m1).add_child(m2).add_child(m3)\nfolium.LayerControl().add_to(crime_map)\n\ncrime_map.save('folium_chloropleth_country.html')\n","2c91fd96":"\n# crime_map = folium.Map(location=[42.3125,-71.0875], \n#                       tiles = \"Stamen Toner\",\n#                       zoom_start = 12)\n\n\n# folium.Choropleth(\n#     geo_data=geo_str,\n#     name='choropleth',\n#     data=crime_distribution,\n#     columns=['DISTRICT', 'INCIDENT_NUMBER'],\n#     key_on='properties.DISTRICT',\n#     fill_color='OrRd',\n#     fill_opacity=0.6,\n#     line_opacity=0.6,\n#     legend_name='Crime Rate (%)'\n# ).add_to(crime_map)\n\n# folium.LayerControl().add_to(crime_map)\n\n# crime_map.save('folium_chloropleth_country2.html')\n","1af24955":"# state_data = pd.read_csv('\/kaggle\/input\/distributiond\/sensitivity_specialist.csv', encoding='latin-1')\n# state_data['DISTRICT'] = state_data['Unnamed: 0']\n# state_data.drop('Unnamed: 0', inplace=True, axis= 1)","b9d8605a":"# import os\n# import pandas as pd\n# import json\n# import geopandas as gpd\n\n# gdf = gpd.read_file('\/kaggle\/input\/geo-json-boston\/Police_Districts.geojson')\n# merged = gdf.merge(state_data, left_on='DISTRICT', right_on='DISTRICT')\n# spatial_gdf = gpd.GeoDataFrame(merged.iloc[:, [1, 8]])\n# merged","e193b45c":"# styledata = {}\n# #merged['opacity'] = 0.5\n# for i in merged.index:\n#     df = pd.DataFrame(\n#      {'color': merged.iloc[i,9:18].transpose().values,\n#       'opacity': [0.6]*9}, index=[p * 1000 for p in range(1,10)])\n#     #df.index.name = 'speciality_resource'\n#     styledata[i] = df.rename({str(i):'color'}, axis=1)\n    \n# # for i in range(1,10):\n# #     df = merged[[str(i),'opacity']]\n# #     df.index.name = \"speciality_resource\"\n# #     styledata[i] = df.rename({str(i):'color'}, axis=1)","78fdc7a2":"# styledata.get(0)","dd44dc47":"# from branca.colormap import linear\n\n# max_color, min_color = 0, 0\n\n# for time, stydat in styledata.items():\n#     max_color = max(max_color, stydat['color'].max())\n#     min_color = min(min_color, stydat['color'].min())\n        \n# cmap = linear.BuPu_06.scale(min_color, max_color)\n\n# for country, stydat in styledata.items():\n#     stydat['color'] = stydat['color'].apply(cmap)\n#     stydat['opacity'] = (stydat['opacity'])","f35671e1":"# stydat","3e95182b":"# styledict = {\n#     str(record): stydat.to_dict(orient='index') for\n#     record, stydat in styledata.items()\n# }","e513292f":"# from folium.plugins import TimeSliderChoropleth\n# crime_map = folium.Map(location=[42.3125,-71.0875], \n#                       tiles = \"Stamen Toner\",\n#                       zoom_start = 12)\n\n# g = TimeSliderChoropleth(\n#     spatial_gdf.to_json(),\n#     styledict=styledict, overlay = True\n# ).add_to(crime_map)\n# folium.LayerControl().add_to(crime_map)\n\n# # crime_map.choropleth(\n# #     geo_data=geo_str,\n# #     name='choropleth',\n# #     data=crime_distribution,\n# #     columns=['DISTRICT', 'INCIDENT_NUMBER'],\n# #     key_on='properties.DISTRICT',\n# #     fill_color='BuPu',\n# #     fill_opacity=0.0,\n# #     line_opacity=0.0,\n# #     legend_name='Crime Rate (%)'\n# # )\n\n# # folium.LayerControl().add_to(crime_map)\n\n# crime_map\n# crime_map.save('folium_dynamic_chloropleth.html')","e64e5a29":"# import geopandas as gpd\n\n\n# assert 'naturalearth_lowres' in gpd.datasets.available\n# datapath = gpd.datasets.get_path('naturalearth_lowres')\n# gdf = gpd.read_file(datapath)\n# %matplotlib inline\n\n# ax = gdf.plot(figsize=(10, 10))\n\n# import pandas as pd\n\n\n# n_periods, n_sample = 48, 40\n\n# assert n_sample < n_periods\n\n# datetime_index = pd.date_range('2016-1-1', periods=n_periods, freq='M')\n# dt_index_epochs = datetime_index.astype(int) \/\/ 10**9\n# dt_index = dt_index_epochs.astype('U10')\n\n# dt_index\n# styledata = {}\n\n# for country in gdf.index:\n#     df = pd.DataFrame(\n#         {'color': np.random.normal(size=n_periods),\n#          'opacity': np.random.normal(size=n_periods)},\n#         index=dt_index\n#     )\n#     df = df.cumsum()\n#     df.sample(n_sample, replace=False).sort_index()\n#     styledata[country] = df\n    \n# max_color, min_color, max_opacity, min_opacity = 0, 0, 0, 0\n\n# for country, data in styledata.items():\n#     max_color = max(max_color, data['color'].max())\n#     min_color = min(max_color, data['color'].min())\n#     max_opacity = max(max_color, data['opacity'].max())\n#     max_opacity = min(max_color, data['opacity'].max())\n    \n# from branca.colormap import linear\n\n\n# cmap = linear.PuRd_09.scale(min_color, max_color)\n\n\n# def norm(x):\n#     return (x - x.min()) \/ (x.max() - x.min())\n\n\n# for country, data in styledata.items():\n#     data['color'] = data['color'].apply(cmap)\n#     data['opacity'] = norm(data['opacity'])\n# styledict = {\n#     str(country): data.to_dict(orient='index') for\n#     country, data in styledata.items()\n# }\n\n# m = folium.Map([0, 0], tiles='Stamen Toner', zoom_start=2)\n\n# g = TimeSliderChoropleth(\n#     gdf.to_json(),\n#     styledict=styledict,\n\n# ).add_to(m)\n\n# #m.save(os.path.join('results', 'TimeSliderChoropleth.html'))\n\n# m","f5bb2edc":"import pandas as pd\nsensitivity_specialist = pd.read_csv(\"..\/input\/sensitivity_specialist.csv\")","e7e1d553":"import pandas as pd\nsensitivity_specialist = pd.read_csv(\"..\/input\/sensitivity_specialist.csv\")","cf170cf9":"First, let's clean up and simplify this data set. I am going to focus on the two years with complete data (2016 and 2017). I will also narrow in on [UCR Part One](https:\/\/www.ucrdatatool.gov\/offenses.cfm) offenses, which include only the most serious crimes.","6999668e":"Larceny is by far the most common serious crime, and homicides are pretty rare. \n\n# When do serious crimes occur?\n\nWe can consider patterns across several different time scales: hours of the day, days of the week, and months of the year.","4a273c85":"# Conclusions\n\nIn summary, this EDA shows:\n\n* Larceny is by far the most common type of serious crime.\n* Serious crimes are most likely to occur in the afternoon and evening.\n* Serious crimes are most likely to occur on Friday and least likely to occur on Sunday.\n* Serious crimes are most likely to occur in the summer and early fall, and least likely to occur in the winter (with the exeption of January, which has a crime rate more similar to the summer).\n* There is no obvious connection between major holidays and crime rates.\n* Serious crimes are most common in the city center, especially districts A1 and D4.\n\nThis EDA just scratches the surface of the dataset. Further analyses could explore how different types of crimes vary in time and space. I didn't even consider the less serious UCR Part Two and Part Three crimes, which are far more common than Part One crimes, but include interesting categories such as drug crimes. Another interesting direction would be to combine this with other data about Boston, such as demography or even the [weather](http:\/\/www.chicagotribune.com\/news\/data\/ct-crime-heat-analysis-htmlstory.html), to investigate what factors predict crime rates across time and space.","55a03795":"That looks like Boston alright. If you are at all familiar with Boston, you will not be too surprised to see that downtown Boston has the darkest points, but there are also some localities outside of the city center that have especially high crime rates. \n\nLet's make another scatterplot, but this time we'll color points by district to see which districts have the highest crime rates.","c418910e":"# Types of serious crimes\n\nLet's start by checking the frequency of different types of crimes. Since we have subsetted to only 'serious' crimes, there are only 9 different types of offenses - much more manageable than the 67 we started with.","2b7e62c3":"Crimes rates are low between 1-8 in the morning, and gradually rise throughout the day, peaking around 6 pm. There is some variation across days of the week, with Friday having the highest crime rate and Sunday having the lowest. The month also seems to have some influence, with the winter months of February-April having the lowest crime rates, and the summer\/early fall months of June-October having the highest crime rates. There is also a spike in crime rates in the month of January. \n\nAre any other temporal factors associated with crime? [According to some crime experts](https:\/\/www.oxygen.com\/homicide-for-the-holidays\/blogs\/its-the-most-dangerous-time-of-the-year-why-do-crimes-increase), several types of crime tend to increase around the holidays, particularly larsony and robbery. This can occur for many reasons: crowded shopping centers create more cover for thieves, travelers leave their homes vulnerable to burglary, and increased alcohol and drug use can raise the likelihood of conflict-related crime. Let's see if there is any evidence for this in our data, focusing in on the year 2017. I also added in a couple of days that are known to be especially rowdy in Boston, even though they aren't official holidays: St. Patrick's Day and the Boston Marathon.","70e3549b":"# Introduction\n\nThis EDA looks at crime incident reports in the city of Boston from June 2015 to September 2018. I use Folium for plotting an interactive heatmap of Boston, and seaborn for everything else.\n\nThe data is originally provided by Boston's open data hub, [Analyze Boston](https:\/\/data.boston.gov\/dataset\/crime-incident-reports-august-2015-to-date-source-new-system). This [kernel](https:\/\/www.kaggle.com\/kosovanolexandr\/crimes-in-boston-multiclass-clustering) by [Kosovan Olexandr](https:\/\/www.kaggle.com\/kosovanolexandr) helped me get started with this dataset, and this [other kernel](https:\/\/www.kaggle.com\/daveianhickey\/how-to-folium-for-maps-heatmaps-time-analysis) by [Dave Fisher-Hickey](https:\/\/www.kaggle.com\/daveianhickey) helped me get started with Folium. ","dc957d02":"We can now associate high crime rates with particular districts, most noteably A1 and D4, which correspond to the most crowded areas of downtown Boston. There is also a very high crime region visibe in district D14.\n\nLet's make things pretty by using Folium to make an interactive heatmap of Boston crimes. I will use the 2017 data only for this plot.","1f69a279":"Hm, I'm not seeing any clear signals here. In fact, many of these holidays appear to line up with especially low crime rates, particularly Thanksgiving and Christmas. Of course, this is data from just a single year, and detecting an association between a given holiday and crime rates would require a lot more data and a model that accounts for other factors. However, this does cause me to question the general idea that crime increases surrounding holidays - if that *is* true, it isn't super obvious from a birds-eye view of the data. Even the entire [\"holiday season\"](https:\/\/www.cpss.net\/about\/blog\/2013\/11\/stay-safe-crime-rates-increase-during-holiday-season\/) from Thanksgiving to Christmas doesn't seem to be especially elevated compared to the summer.  \n\n# Where do serious crimes occur?\n\nWe can use the latitude and longitude columns to plot the location of crimes in Boston. By setting the alpha parameter to a very small value, we can see that there are some crime 'hotspots'. "}}