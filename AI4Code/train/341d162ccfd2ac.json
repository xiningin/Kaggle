{"cell_type":{"e5189bdd":"code","417d9dbf":"code","443886bc":"code","35adefb4":"code","a887ec98":"code","8ce40c5a":"code","f4f24d53":"code","9f2357ca":"code","ce0f79e2":"code","a298ca8b":"code","a5bb48ca":"code","39581581":"code","2301f4c1":"code","ff67ed82":"code","ec677db4":"code","a9b5cfbe":"code","7dd1e816":"code","53f8e9b1":"code","0fe6b5d1":"code","1d9ad025":"code","1fc98eff":"code","ffc04ca8":"code","0b9b40a6":"code","9847f612":"code","5ead91f2":"code","52412ca9":"code","5a8d413f":"code","266590ae":"code","febfdca0":"code","d16ab923":"code","14c2c85a":"code","027b560f":"code","221267bc":"code","615fb99b":"code","c06aa355":"code","10963a82":"code","407032c2":"code","a5e67daa":"code","0b8c2e20":"code","6f38e446":"code","7d5fed27":"code","a0674592":"code","29fd789d":"code","f2b81eed":"code","583a42dc":"code","f2089d1c":"code","4bde8c84":"code","0f21059d":"code","e9638f55":"code","8c3f9962":"code","d638a504":"code","20ca90cb":"code","721ec32f":"code","f2b47231":"code","0cc14021":"code","77cdd31e":"code","59254769":"code","2b4dc56c":"code","201d8d49":"code","3a0cd0a3":"code","04eb7fc7":"code","743c8e89":"code","2a23cc05":"code","f38044f4":"code","c7d319e6":"markdown","fdac885a":"markdown","65c9458e":"markdown","501575b3":"markdown","fd38ff6e":"markdown","a08bc64d":"markdown","0ec65857":"markdown","9f1cade5":"markdown","a716c05b":"markdown","156f10d9":"markdown","c5f27db7":"markdown","6d567c8b":"markdown","c6a483e9":"markdown","4657b32c":"markdown","793bc7e1":"markdown","78ca0bab":"markdown"},"source":{"e5189bdd":"#Importing Data manipulation and plotting modules\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom xgboost import plot_importance\nimport seaborn as sns\nimport os","417d9dbf":"#Importing libraries for Data pre-processing\nfrom sklearn.preprocessing import StandardScaler as ss","443886bc":"#Importing model for Dimentionality Reduction\nfrom sklearn.decomposition import PCA","35adefb4":"#Importing libraries for performance measures\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import auc, roc_curve","a887ec98":"#Importing libraries For data splitting\nfrom sklearn.model_selection import train_test_split","8ce40c5a":"#Importing libraries for Model pipelining\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline","f4f24d53":"#Importing libraries for model parameter search and hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom bayes_opt import BayesianOptimization\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.model_selection import cross_val_score\nimport eli5\nfrom eli5.sklearn import PermutationImportance","9f2357ca":"#Importing Miscelaneous libraries\nimport time\nimport os\nfrom scipy.stats import uniform","ce0f79e2":"#Display all rows\npd.set_option('display.max_columns', 100)","a298ca8b":"os.chdir(\"..\/input\") \ndata = pd.read_csv(\"winequalityN.csv\")","a5bb48ca":"data.head()","39581581":"data.info()","2301f4c1":"data.shape","ff67ed82":"data.isnull().any()","ec677db4":"data.dropna(axis=0,inplace=True)","a9b5cfbe":"data.isnull().any()","7dd1e816":"data.shape","53f8e9b1":"g = sns.pairplot(data,palette=\"husl\",diag_kind=\"kde\",hue='type')","0fe6b5d1":"plt.figure(figsize=(12, 12))\nsns.heatmap(data.corr(),annot=True,vmin=-1,cmap='YlGnBu')","1d9ad025":"plt.figure(figsize=(12,10))\nsns.barplot(x=data.type, y=data.alcohol,data=data, hue=data.quality,palette='spring')\nplt.xlabel('Type',fontsize=16)\nplt.ylabel('Quality',fontsize=16)","1fc98eff":"#Assuming wines with quality greater than 6 are best\n\nwine_data = data.shape[0]\nprint(\"Total number of wine data: \"+str(wine_data))\nhigh_quality = data.loc[(data.quality)> 6]\nprint(\"Best quality wine entries: \"+str(high_quality.shape[0]))\ngood_quality = data.loc[(data.quality)<5]\nprint(\"Good quality wine entries: \"+str(good_quality.shape[0]))\nend_of_month_wine = data.loc[(data.quality) < 3]\nprint(\"Mediocre quality wine entries: \"+str(good_quality.shape[0]))","ffc04ca8":"table = data.pivot_table('alcohol',index='quality',columns='type')\nq = table.quantile(0.90)\ndf = table[table < q]\nplt.figure(figsize=(12,8))\nplt.xticks(rotation=45)\nax = sns.boxplot(data=data)","0b9b40a6":"table_fa = data.pivot_table('fixed acidity',index='quality',columns='type',aggfunc='sum').sum(axis=1)\ntable_va = data.pivot_table('volatile acidity',index='quality',columns='type',aggfunc='sum').sum(axis=1)\ntable_ca = data.pivot_table('citric acid',index='quality',columns='type',aggfunc='sum').sum(axis=1)\ntable_rs = data.pivot_table('residual sugar',index='quality',columns='type',aggfunc='sum').sum(axis=1)\ntable_chl = data.pivot_table('chlorides',index='quality',columns='type',aggfunc='sum').sum(axis=1)\ntable_tsd = data.pivot_table('total sulfur dioxide',index='quality',columns='type',aggfunc='sum').sum(axis=1)\ntable_den = data.pivot_table('density',index='quality',columns='type',aggfunc='sum').sum(axis=1)\ntable_pH = data.pivot_table('pH',index='quality',columns='type',aggfunc='sum').sum(axis=1)\ntable_sul = data.pivot_table('sulphates',index='quality',columns='type',aggfunc='sum').sum(axis=1)\ntable_alcohol = data.pivot_table('alcohol',index='quality',columns='type',aggfunc='sum').sum(axis=1)\nqual = table_alcohol.index.astype(int)\nlbl = ['fa','va','ca','rs','chl','tsd','den','pH','sul','alcohol']\n\nplt.figure(figsize=(12,14))\nax = sns.pointplot(x=qual, y=table_fa, color='y',scale=0.7)\nax = sns.pointplot(x=qual, y=table_va, color='b',scale=0.7)\nax = sns.pointplot(x=qual, y=table_ca, color='m', scale=1.0)\nax = sns.pointplot(x=qual, y=table_rs, color='thistle', scale=1.0)\nax = sns.pointplot(x=qual, y=table_chl, color='g', scale=1.0)\nax = sns.pointplot(x=qual, y=table_tsd, color='coral',scale=0.7)\nax = sns.pointplot(x=qual, y=table_den, color='olive', scale=1.0)\nax = sns.pointplot(x=qual, y=table_pH, color='sienna', scale=1.0)\nax = sns.pointplot(x=qual, y=table_sul, color='m', scale=1.0)\nax = sns.pointplot(x=qual, y=table_alcohol, color='springgreen', scale=1.0)\n\nax.set_xlabel(xlabel='Quality', fontsize=16)\nax.set_ylabel(ylabel='Quantity of each component', fontsize=16)\nax.legend(handles=ax.lines[::len(qual)+1],labels=lbl,fontsize=12)\nplt.show();","9847f612":"df = data.groupby(['quality'])\ndf_mean = df['fixed acidity','volatile acidity','citric acid'].aggregate(np.mean)\ndf_mean.plot(figsize=(10,8))\nplt.xlabel('Quality',fontsize=16)\nplt.ylabel('Quantity',fontsize=16)","5ead91f2":"fig = plt.figure(figsize=(12, 10));\nax = fig.add_subplot(1,1,1)\nbp = data.boxplot(grid=False,layout=(2,2),column=['fixed acidity','volatile acidity','citric acid','alcohol'], by=['quality'],ax=ax)\n","52412ca9":"#Last 13 columns as predictors\nX = data.iloc[ :, 1:14]\nX.head(2)","5a8d413f":"#First column as target\ny = data.iloc[ : , 0]\ny.head()","266590ae":"y = y.map({'white':1, 'red' : 0})\ny.dtype","febfdca0":"#Divide dataset into Training data and validation data\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.35,\n                                                    shuffle = True\n                                                    )","d16ab923":"xgb_list = [('ss',ss()),         #Scaling parameters\n            ('pca',PCA()),       #Instantiate PCA\n            ('xgb',XGBClassifier(silent = False,  #Instantiate XGB classifier with 2 cpu threads \n                                  n_jobs=2))]","14c2c85a":"#Instantiate Pipeline object\nxgb_pipeline = Pipeline(xgb_list)\n","027b560f":"#A parameter grid for grid search\nparameter_gs = {'xgb__learning_rate':  [0.04, 0.07],\n              'xgb__n_estimators':   [150,  200],\n              'xgb__max_depth':      [3,5],\n              'pca__n_components' : [7,12]\n              }","221267bc":"grid_search = GridSearchCV(xgb_pipeline,            # XGB pipeline object\n                   parameter_gs,         # 2*2*2*2 parameter grid\n                   n_jobs = 2,         # No. of parallel cpu threads\n                   cv =2 ,             # No. of folds\n                   verbose =2,      \n                   scoring = ['accuracy', 'roc_auc'],  # Performance metrics\n                   refit = 'roc_auc'   # Refitting final model those which maximise auc\n                   )","615fb99b":"#Fitting data to Pipeline\nstart = time.time()\ngrid_search.fit(X_train, y_train)   \nend = time.time()\n(end - start)\/60 ","c06aa355":"f\"Best score: {grid_search.best_score_} \"","10963a82":"f\"Best parameter set {grid_search.best_params_}\"","407032c2":"plt.bar(grid_search.best_params_.keys(), grid_search.best_params_.values(), color='g')\nplt.xticks(rotation=10)","a5e67daa":"y_pred = grid_search.predict(X_test)\ny_pred","0b8c2e20":"accuracy = accuracy_score(y_test, y_pred)","6f38e446":"f\"Accuracy: {accuracy * 100.0}\"","7d5fed27":"#A parameter set for random search\nparameter_rs = {'xgb__learning_rate':  uniform(0, 1),\n              'xgb__n_estimators':   range(60,120),\n              'xgb__max_depth':      range(4,7),\n              'pca__n_components' : range(5,7)}","a0674592":"random_search = RandomizedSearchCV(xgb_pipeline,\n                        param_distributions=parameter_rs,\n                        scoring= ['roc_auc', 'accuracy'],\n                        n_iter=15,          # Max combination of\n                        verbose = 3,\n                        refit = 'roc_auc',\n                        n_jobs = 2,          # No. of parallel cpu threads\n                        cv = 2               # No of folds.\n                        )","29fd789d":"#Fitting data to Pipeline\nstart = time.time()\nrandom_search.fit(X_train, y_train)\nend = time.time()\n(end - start)\/60","f2b81eed":"f\"Best score: {random_search.best_score_} \"","583a42dc":"f\"Best parameter set: {random_search.best_params_} \"","f2089d1c":"plt.bar(random_search.best_params_.keys(), random_search.best_params_.values(), color='g')\nplt.xticks(rotation=10)","4bde8c84":"y_pred = random_search.predict(X_test)\ny_pred","0f21059d":"accuracy = accuracy_score(y_test, y_pred)\nf\"Accuracy: {accuracy * 100.0}\"","e9638f55":"parameter_bo = {\n           'learning_rate':  (0, 1),            \n           'n_estimators':   (60,120),         \n           'max_depth':      (4,7),            \n           'n_components' :  (5,7)\n            }","8c3f9962":"\ndef xg_eval(learning_rate,n_estimators, max_depth,n_components):\n    #Make Pipeling for BO\n    pipe_xg1 = make_pipeline (ss(),\n                              PCA(n_components=int(round(n_components))),\n                              XGBClassifier(\n                                           silent = False,\n                                           n_jobs=2,\n                                           learning_rate=learning_rate,\n                                           max_depth=int(round(max_depth)),\n                                           n_estimators=int(round(n_estimators))\n                                           )\n                             )\n    #Fitting into pipeline \n    cv_result = cross_val_score(estimator = pipe_xg1,\n                                X= X_train,\n                                y = y_train,\n                                cv = 2,\n                                n_jobs = 2,\n                                scoring = 'f1'\n                                ).mean()             # taking mean of all results\n\n    return cv_result       #Returning final mean of all results of cross val score","d638a504":"bayesian_opt = BayesianOptimization(\n                             xg_eval,     \n                             parameter_bo   \n                             )","20ca90cb":"start = time.time()","721ec32f":"bayesian_opt.maximize(init_points=5,\n               n_iter=15,        \n               )","f2b47231":"f\"Best parameter set: {bayesian_opt.max} \"","0cc14021":"bayesian_opt.max.values()","77cdd31e":"for features in bayesian_opt.max.values(): \n    print(features)","59254769":"features","2b4dc56c":"plt.bar(features.keys(), features.values(), color='g')\nplt.xticks(rotation=10)","201d8d49":"#Model with parameters of grid search\nmodel_gs = XGBClassifier(\n                    learning_rate = grid_search.best_params_['xgb__learning_rate'],\n                    max_depth = grid_search.best_params_['xgb__max_depth'],\n                    n_estimators=grid_search.best_params_['xgb__n_estimators']\n                    )\n\n#Model with parameters of random search\nmodel_rs = XGBClassifier(\n                    learning_rate = random_search.best_params_['xgb__learning_rate'],\n                    max_depth = random_search.best_params_['xgb__max_depth'],\n                    n_estimators=random_search.best_params_['xgb__n_estimators']\n                    )\n\n#Model with parameters of bayesian optimization\nmodel_bo = XGBClassifier(\n                    learning_rate = int(features['learning_rate']),\n                    max_depth = int(features['max_depth']),\n                    n_estimators=int(features['n_estimators'])\n                    )","3a0cd0a3":"start = time.time()\nmodel_gs.fit(X_train, y_train)\nmodel_rs.fit(X_train, y_train)\nmodel_bo.fit(X_train, y_train)","04eb7fc7":"y_pred_gs = model_gs.predict(X_test)\ny_pred_rs = model_rs.predict(X_test)\ny_pred_bo = model_bo.predict(X_test)","743c8e89":"accuracy_gs = accuracy_score(y_test, y_pred_gs)\naccuracy_rs = accuracy_score(y_test, y_pred_rs)\naccuracy_bo = accuracy_score(y_test, y_pred_gs)","2a23cc05":"print(\"Grid search Accuracy: \"+str(accuracy_gs))\nprint(\"Grid search Accuracy: \"+str(accuracy_rs))\nprint(\"Bayesian Optimization Accuracy: \"+str(accuracy_bo))","f38044f4":"model_gs.feature_importances_\nmodel_rs.feature_importances_\nmodel_bo.feature_importances_\nplot_importance(model_gs)\nplot_importance(model_rs)\nplot_importance(model_bo)\nplt.show()","c7d319e6":"#**Get to know data and deleting rows having null values **","fdac885a":"#**Exploring Dataset**","65c9458e":"###**The fixed acidity content is more as compared to volatile acid and citric acid. Combination of these acids contribute to the taste of wine.","501575b3":"##**Bayesian Optimization**","fd38ff6e":"###**Wine types are separated and each component measure is compared. We can observe that alcohol quantity is slightly more in white wine.**","a08bc64d":"##**Random Search**","0ec65857":"###**We know that older the wine, better it tastes. Sulphur dioxide acts as a preservative and antioxidant which is widely used in winemaking.If it is carefully balanced out with pH then the quality of wine is preserved on storage. Hence we can observe that Total sulphur dioxide dominates all other wine components.It is followed by alcohol and density.**","9f1cade5":"###**As we can see the figure shows many co-dependencies between features. Some observations -**\n###**1. Negative correlation between - i)Fixed acidity & pH, ii)density & alcohol.**\n###**2. No correlation - i)alcohol & pH, ii)alcohol &  fixed acidity, iii)alcohol & volatile acidity**\n###**3. Positive correlation - i)density and fixed acidity, ii)density & residual sugar, iii)Citric acid & fixed acidity:initially no correlation; but gng forward positive correlation**\n###**","a716c05b":"#**Loading dataset**","156f10d9":"#**Import necessary libraries**","c5f27db7":"#**Split data as predictors and target**","6d567c8b":"#**Fitting parameters into our model and Feature Importance**","c6a483e9":"###**It can be observed higher the alcohol content better is the wine quality**","4657b32c":"##**Grid Search**","793bc7e1":"#**Hyperparameter tuning and Pipelining using XGBoost **","78ca0bab":"###**Heatmap gives us more insights. Different shades of blue depict different levels of co-relationships between features.**\n###**Positive values show linear or direct co-relationships between features. Higher the values stronger the co-relationships.**\n###**They will be dark blue.**\n###**On the other hand, negative values show an inverse co-relationship i.e as value of one feature increases the other one decreases.**\n###**As the shade of blue decreases so does the co-relationship between the respective features.**\n###**As you can see, the positive values show direct co-relationships between features.** \n###**The higher the values, the stronger these relationships are\u200a\u2014\u200athey\u2019ll be more bluish. **\n###**That means, if one feature increases, the other one also tends to increase, and vice-versa.**\n###**The squares that have negative values show an inverse co-relationship. **\n###**The more negative these values get, the more inversely proportional they are, and they\u2019ll be more blue. **\n###**This means that if the value of one feature is higher, the value of the other one gets lower.**\n###**Finally, values close to zero indicate almost no co-dependency between those sets of features.**"}}