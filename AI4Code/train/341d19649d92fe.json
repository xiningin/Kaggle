{"cell_type":{"353926df":"code","1928b010":"code","002c53f6":"code","51f81d21":"code","9873e312":"code","1a79817f":"code","6a0b6606":"code","ede56c10":"code","551cc213":"code","564f868c":"code","0afc1deb":"code","8477396c":"code","1f3c1f19":"code","25db6480":"code","4d0892d5":"code","37bb33a2":"code","34b61eaa":"code","bd51da08":"code","1e1c9855":"markdown","431970a7":"markdown","10610de6":"markdown","849d14d5":"markdown","81386a08":"markdown","7afbece2":"markdown","d0e5bd06":"markdown","26422de3":"markdown","8692bf13":"markdown","b1f827ad":"markdown","7eca0737":"markdown","7f3a12de":"markdown","53e60b6d":"markdown","6af8aa44":"markdown","5c1f6a13":"markdown","4a7ac1ea":"markdown","740e2d49":"markdown","279bce2d":"markdown","2561d317":"markdown","0c09725e":"markdown","92f33f71":"markdown","76ff7975":"markdown","b9afa1d1":"markdown","4e62e862":"markdown","099fae31":"markdown","62591abe":"markdown","695225bd":"markdown"},"source":{"353926df":"%matplotlib inline\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score","1928b010":"df = pd.read_csv('..\/input\/mushrooms.csv')\ndf.head()","002c53f6":"df.describe()","51f81d21":"df.drop(['veil-type'], axis=1, inplace=True)","9873e312":"for col in df.columns:\n    if len(df[col].value_counts()) == 2:\n        le = LabelEncoder()\n        le.fit(df[col])\n        df[col] = le.transform(df[col])\ndf.head()","1a79817f":"df = pd.get_dummies(df)\ndf.head()","6a0b6606":"y = df['class'].to_frame()\nX = df.drop('class', axis=1)","ede56c10":"y.head()","551cc213":"X.head()","564f868c":"scaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)","0afc1deb":"X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, stratify=y, random_state=19)","8477396c":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train.values.ravel())\ny_pred_test = logreg.predict(X_test)\nprint('Accuracy of Logistic Regression classifier on the test set: {:.2f}'.format(accuracy_score(y_test, y_pred_test)))","1f3c1f19":"scores = cross_val_score(logreg, X_train, y_train.values.ravel(), cv=StratifiedShuffleSplit(n_splits=10, test_size=0.3, random_state=19), scoring='accuracy')\nprint('Accuracy of Logistic Regression classifier using 10-fold cross-validation: {}'.format(scores.mean()))","25db6480":"features_coeffs = pd.DataFrame(logreg.coef_, columns=X.columns, index=['coefficients'])\nfeatures_coeffs.sort_values('coefficients', axis=1, ascending=False, inplace=True)\nfeatures_coeffs.T.head()","4d0892d5":"features_coeffs.T.tail()","37bb33a2":"def plot_features_containing(feature_name):\n    categories = X.columns[X.columns.str.contains(feature_name)]\n    edible_num = []\n    poisonous_num = []\n    for cat in categories:\n        y[X[cat]==0]\n        edible_count = sum((y[X[cat]==1]==0).values[:,0])\n        poisonous_count = sum(X[cat]==1) - edible_count\n        edible_num.append(edible_count)\n        poisonous_num.append(poisonous_count)\n    odor_df = pd.DataFrame(index=categories, columns=['edible', 'poisonous'])\n    odor_df.edible = edible_num\n    odor_df.poisonous = poisonous_num\n    odor_df.plot(kind='bar')\nplot_features_containing('odor')","34b61eaa":"plot_features_containing('spore-print-color')","bd51da08":"plot_features_containing('cap-color')","1e1c9855":"odor: almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s\n\nVery interesting! Seems like, at least in our dataset set:\n* All mushrooms with an almond or anise odor are edible\n* All mushrooms with a creosote, fishy, musty, pungent or spicy odor are poisonous (or unknown edibility)\n* Most mushrooms with no odor are edible. But not all of them!\n\nOf course, this is just what our dataset tells us. It doesn't necessarily mean that any new mushroom we find out there will obey these rules ","431970a7":"This time it doesn't achieve the perfect score, but it's pretty damn close.\n\nThus, it seems like the relationship between the features and the edibility of the mushrooms is __highly linear__. There is really no point in trying other models different from this logistic regression.\n\nWhat we can do is investigate what are the most immportant features in deciding whether a mushroom is edible or not.","10610de6":"# Poisonous Mushroom Classification","849d14d5":"### Most relevant features","81386a08":"### Creating training and sets sets","7afbece2":"_If you like this kernel, please upvote it! It really makes a difference for those of us who are starting here on Kaggle :)_ ","d0e5bd06":"It seems like the logistic regression achieved the maximum accuracy possible: 100%\n\nI have to admit that this made me go back and check my code and logical reasoning a couple times. But no, it simply means that the given features are a really good indicator of the edibility of mushrooms.\n\nStill, we should run the logistic regression again, but this time using _cross-validation_, to ensure that we are not overfitting the data. A simple 10-fold cross validation should do.\n","26422de3":"### Standardising our features\n\nIt is generally considered a _good practice_ to standardise our features (convert them to have zero-mean and unit variance). Most of the times, the difference will be small, but, in any case, it still never hurts to do so.","8692bf13":"For _spore-print-color_ we have quite a similar picture, although perhaps not as extreme as with _odor_. This is what we expected, since these are the 2 features with the highest coefficients in our logistic regression.\n\nIn fact, if we do the same for a feature different from these two, the distribution will probably not be as extreme as for these last 2.\n\nLet's check this.","b1f827ad":"### Importing all libraries","7eca0737":"* We fitted a logistic regression model and achieved near perfect accuracy, so there was no need to try with more complex models.\n\n* Our algorithm identified specific traits (particularly regarding _odor_) that seem to heavily influence the chance that a mushroom is edible or not.\n\n* Even though experts have determined that is that there is no simple set of rules to determine whether a mushroom is edible or not, it seems like with this algorithm we can get pretty close.\n\nNevertheless, it is important to keep in mind that __these results apply only to this dataset__, and don't necessarily mean that there aren't any mushrooms out there which don't follow these rules.\n\nSo, if you're ever lost and stranded in a forest, don't attempt to eat anything just because a machine tells you to do so! _Stay safe out there_.","7f3a12de":"### Separating labels from features","53e60b6d":"Interesting. Seems like _odor_ and _spore-print-color_ play an important role in deciding whether a mushroom is edible or not. Let's confirm this:","6af8aa44":"Most Machine Learning algorithms require _numerical features_. However, our dataset is composed of _categorical features_. We now proceed to convert these to numerical.","5c1f6a13":"#### One Hot Encoding\n\nFor the remaining features, we can use a technique called One Hot Encoding.\n\nEssentially, this consists on creating a new binary feature representing each category. For instance, from the feature _cap surface_, which has 4 unique values (f, g, y and s), we create 4 binary features (cap_surface_f, cap_surface_g, cap_surface_y and cap_surface_s) indicating whether the category they represent was indeed that one or not. This means that, for any given instance (row), we will have exactly one of these 4 features equal to 1, and the other 3 equal to 0.\n\nOne Hot Encoding is really simple to perform with the _pandas_ package:","4a7ac1ea":"#### Label Encoding\n\nA typical approach is to perform _Label Encoding_. This is nothing more than just assigning a number to each category, that is:\n\n(cat_a, cat_b, cat_c, etc.) \u2192 (0, 1, 2, etc.)\n\nThis technique works:\n* When the features are binary (only have 2 unique values)\n* When the features are _ordinal categorical_ (that is, when the categories can be ranked). A good example would be a feature called _t-shirt size_ with 3 unique values _small_, _medium_ or _large_, which have an intrinsic order.\n\n__However__, in our case, only some of our features have 2 unique values (most of them have more), and none of them are _ordinal categorical_ (in fact they they are _nominal categorical_, which means they have no intrinsic order).\n\nTherefore, we will only apply Label Encoding to those features with a binary set of values:\n\n","740e2d49":"We will separate our data into a training set (70%) and a test set (30%). This is a very standard approach in Machine Learning.\n\nThe _stratify_ option ensures that the ratio of edible to poisonois mushrooms in our dataset remains the same in both training and test sets. The *random_state* parameter is simply a seed for the algorithm to use (if we didn't specify one, it would create different training and test sets every time we run it)","279bce2d":"Indeed, we see a much more balanced distribution, which suggests that cap-color does not play such an important role in determining the edibility of a mushroom.","2561d317":"### Logistic Regression","0c09725e":"We notice that the column _veil-type_ has only 1 unique value - that is, all 8124 mushroom instances have the same veil-color.\n\nIt thus becomes an irrelevant feature, so we proceed to remove it","92f33f71":"### Dataset: loading and initial inspection","76ff7975":"X will now contain our features, and y our labels (0 for edible and 1 for poisonous\/unknown)","b9afa1d1":"### Short dataset description:\n\nThis dataset contains 8124 entries corresponding to 23 species of gilled mushrooms from North America. Each species is identified as definitely edible (e), definitely poisonous (p), or of unknown edibility and not recommended (also p). Each entry has 22 features related to the physical characteristics of the mushroom.","4e62e862":"Since this is now a supervised learning binary classification problem, it makes perfect sense to start by running a simple _logistic regression_.\n\nA logistic regression simply predicts the probability of an instance (row) belonging to the default class, which can then be snapped into a 0 or 1 classification. Off we go.","099fae31":"We can see how it has converted some of the features to values of 0 or 1. More importantly, our labels (the _class_ column) are now 0=e, and 1=p.","62591abe":"## Conclusion","695225bd":"### Converting categorical data to numerical"}}