{"cell_type":{"30e48680":"code","1662dc10":"code","6479423d":"code","b3ad8bab":"code","01a22c0a":"code","7cc24a42":"code","5367669e":"code","b9c48274":"code","24e2a847":"code","9a5bfd16":"code","5f4dbd70":"code","22a9df64":"code","8adf72fb":"code","b094e7c6":"code","3380d383":"code","f57e1b74":"code","f8dd4001":"code","edd47698":"code","f15f2f87":"code","6262aedd":"code","38ceb696":"code","198cc7b8":"code","ba8de268":"code","c1b1e321":"code","4a5e7c35":"code","9cb7ee90":"code","f433eb38":"code","7a3c250a":"code","bcf97c0c":"code","5aa2e905":"code","ce7a1312":"code","07e36cd7":"code","7b74d2fc":"code","5524acf1":"code","18111d8b":"code","52ba6bfd":"code","3ecf1317":"code","f414d50b":"code","a6e47508":"code","834a0fb3":"code","c0f8371e":"code","e1f25611":"code","2fcd57cd":"code","dc634591":"code","98343dc2":"code","4942c548":"markdown","ffb19e33":"markdown","4975d7b4":"markdown","21bc7a21":"markdown","95f9213c":"markdown","63185e37":"markdown","13d53e2e":"markdown","e867b3c9":"markdown","b5ceee56":"markdown","3b43cfb3":"markdown","b01aaa0e":"markdown","8feed6c3":"markdown","f949ab51":"markdown","8d34ed7b":"markdown","40a9c30a":"markdown","18f658b8":"markdown","641bce80":"markdown","a540fc08":"markdown","893818ac":"markdown","d0816ee9":"markdown","8625c7fb":"markdown","99393c22":"markdown","0d9393d0":"markdown","0a114877":"markdown","981a5cf5":"markdown","3de03885":"markdown","5c7d977b":"markdown","e2412627":"markdown","5e68747b":"markdown","c6779aa5":"markdown","ea4c61d3":"markdown","e446208c":"markdown","82d81390":"markdown"},"source":{"30e48680":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline ","1662dc10":"import warnings\nwarnings.filterwarnings(\"ignore\")","6479423d":"sns.set_palette(\"bright\")","b3ad8bab":"data = pd.read_csv(\"..\/input\/amazon_alexa.tsv\", sep=\"\\t\")","01a22c0a":"data.head()","7cc24a42":"data.columns","5367669e":"data['rating'].unique()","b9c48274":"type(data['date'][0]) , data['date'][0]","24e2a847":"data['date'] = pd.to_datetime(data['date'])\ndata['date'][0]","9a5bfd16":"dates = data['date']\nonly_dates = []\nfor date in dates:\n    only_dates.append(date.date())\n\ndata['only_dates'] = only_dates\ndata['only_dates'][0]","5f4dbd70":"only_year = []\nfor date in dates:\n    only_year.append(date.year)\ndata['year'] = only_year\n\n\nonly_month = []\nfor date in dates:\n    only_month.append(date.month)\ndata['month'] = only_month\n\n# 1 -> monday\n# 7 -> sunday\nonly_weekday = []\nfor date in dates:\n    only_weekday.append(date.isoweekday())\ndata['day_of_week'] = only_weekday","22a9df64":"reviews = data['verified_reviews']\nlen_review = []\nfor review in reviews:\n    len_review.append(len(review))\n\ndata['len_of_reviews'] = len_review","8adf72fb":"data['len_of_reviews'][0], data['verified_reviews'][0]","b094e7c6":"data.columns","3380d383":"plt.figure(figsize=(15,7))\nplt.bar(height = data.groupby('rating').count()['date'], x = sorted(data['rating'].unique(), reverse= False))\nplt.xlabel(\"Ratings\")\nplt.ylabel(\"Count\")\nplt.title(\"Count of Ratings\")\nplt.show()","f57e1b74":"plt.figure(figsize=(15,7))\nsns.countplot(x=\"rating\", hue=\"feedback\", data=data)\nplt.show()","f8dd4001":"plt.figure(figsize=(15,7))\nsns.barplot(x=\"rating\", y=\"variation\", hue=\"feedback\", data=data, estimator= sum, ci = None)\nplt.show()","edd47698":"plt.figure(figsize=(15,7))\nsns.barplot(x=\"rating\", y=\"variation\", hue=\"feedback\", data=data, ci = None)\nplt.show()","f15f2f87":"plt.figure(figsize=(15,7))\nsns.barplot(y=\"rating\", x=\"month\", hue=\"feedback\", data=data, ci = None, estimator= sum)\nplt.show()","6262aedd":"plt.figure(figsize=(15,7))\nsns.barplot(y=\"rating\", x=\"month\", hue=\"feedback\", data=data, ci = None)\nplt.show()","38ceb696":"plt.figure(figsize=(15,7))\nsns.countplot(x=\"day_of_week\", hue=\"feedback\", data=data)\nplt.show()","198cc7b8":"plt.figure(figsize=(15,7))\nsns.barplot(y=\"rating\", x=\"day_of_week\", hue=\"feedback\", data=data, ci = None)\nplt.show()","ba8de268":"plt.figure(figsize=(15,7))\nsns.countplot(x=\"feedback\", data=data)\nplt.show()","c1b1e321":"plt.figure(figsize=(15,7))\nsns.distplot(data[data['feedback'] == 0]['len_of_reviews'], label = 'Feedback - 0')\nsns.distplot(data[data['feedback'] == 1]['len_of_reviews'], label = 'Feedback - 1')\nplt.legend()\nplt.show()","4a5e7c35":"from sklearn.feature_extraction.text import TfidfVectorizer","9cb7ee90":"tdf = TfidfVectorizer(stop_words='english')","f433eb38":"pd.DataFrame(tdf.fit_transform(data['verified_reviews']).toarray())","7a3c250a":"tdf_data = pd.DataFrame(tdf.fit_transform(data['verified_reviews']).toarray())","bcf97c0c":"pd.get_dummies(data['variation'], drop_first= True)","5aa2e905":"one_hot_data = pd.get_dummies(data['variation'])","ce7a1312":"X = pd.concat([one_hot_data, tdf_data, data['month'], data['day_of_week'], data['len_of_reviews']], axis=1)","07e36cd7":"X.head()","7b74d2fc":"y = data['feedback']","5524acf1":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold, cross_val_score\n\nrf = RandomForestClassifier()\n\nk_fold = KFold(n_splits=5)\n\ncross_val_score(rf, X, y, cv=k_fold, scoring='accuracy')","18111d8b":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","52ba6bfd":"rf = RandomForestClassifier()\nfit_model = rf.fit(X_train, y_train)","3ecf1317":"t = zip(fit_model.feature_importances_, X_train.columns)\nt1 = reversed(sorted(t , key=lambda x: x[0]))\ni = 0\nfor element in t1:\n    if (i < 10):\n        print(element)\n        i = i + 1","f414d50b":"y_pred = rf.predict(X_test)","a6e47508":"from sklearn.model_selection import learning_curve\ntrain_sizes, train_scores, test_scores = learning_curve(RandomForestClassifier(), X_train, y_train, scoring='f1', train_sizes=np.linspace(0.1, 1.0, 20), cv = 3)\n\ntrain_scores = np.mean(train_scores, axis = 1)\ntest_scores = np.mean(test_scores, axis = 1)\n\nplt.plot(train_sizes, train_scores, 'o-', label=\"Training score\")\nplt.plot(train_sizes, test_scores, 'o-', label=\"Cross-validation score\")\nplt.legend();","834a0fb3":"from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score","c0f8371e":"print(\"==============================================\")\nprint(\"For Random Forest Classifier:\\n\")\nprint(\"Accuracy Score: \",accuracy_score(y_test, y_pred))\nprint(\"Precision Score: \",precision_score(y_test, y_pred))\nprint(\"Recall Score: \",recall_score(y_test, y_pred))\nprint(\"F1 Score: \",f1_score(y_test, y_pred))\nprint(\"Confusion Matrix:\\t \\n\",confusion_matrix(y_test, y_pred))\n\nprint(\"==============================================\")","e1f25611":"from sklearn.ensemble import GradientBoostingClassifier\ngbc = GradientBoostingClassifier()\ngbc.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\n\nprint(\"==============================================\")\nprint(\"For Gradient Boosting Classifier:\\n\")\nprint(\"Accuracy Score: \",accuracy_score(y_test, y_pred))\nprint(\"Precision Score: \",precision_score(y_test, y_pred))\nprint(\"Recall Score: \",recall_score(y_test, y_pred))\nprint(\"F1 Score: \",f1_score(y_test, y_pred))\nprint(\"Confusion Matrix:\\t \\n\",confusion_matrix(y_test, y_pred))\nprint(\"==============================================\")","2fcd57cd":"results = pd.DataFrame(data = {'Y Test': y_test, 'Y Predictions': y_pred})","dc634591":"results.head()","98343dc2":"results.to_csv('Results.csv')","4942c548":"### Importing Libraries","ffb19e33":"* On applying a hue of feedback, we can detect that reviews which have a rating of more than 2, result in a positive feedback (1). \n* We will be removing this column from the training set, we would prefer the learning algorithm not to capitalize on this feature.","4975d7b4":"### Feature Engineering:","21bc7a21":"#### Extracting *Year, Month, Day of the Week* from date.\n* We will be using these features later in the model.\n* We will extract month, year and day of the week into separate columns.","95f9213c":"* On changing the aggregation function to mean(default), average rating seems to be 4.5 for every positive feedback review.","63185e37":"* When we take month into consideration, most orders in this dataset comes from the month of July.","13d53e2e":"## Amazon Alexa Reviews Analysis","e867b3c9":"Rating column has values:","b5ceee56":"\n#### Learning Curve:","3b43cfb3":"### K Fold Cross Validation:\n* K Fold cross validation gives a good idea on how is our selected model performing on different chunks of data.\n* We are getting perfect scores through cross validation, as a result we would not be performing hyper parameter tuning.","b01aaa0e":"<br>\n### Exploring the dataset \nThis data set has five columns:\n* rating\n* date\n* variation\n* verfied_reviews\n* feedback\n\nWe will explore each column with the help of charts and how does it impact our target column **feedback**.","8feed6c3":"* The project analyzes reviews by users of **Amazon\u2019s Alexa products**. \n* Using **Natural Language Processing** on the product reviews and some additional features, a machine learning model should be able to predict if the feedback is **positive (1) or negative (0).**","f949ab51":"* When day of the week is considered, it seems that Monday happens to be the day when most people write their reviews.\n* This can relate to prime delivery guarantee within two days, and the most frequent day of ordering being on Saturday or the weekend.","8d34ed7b":"* Overall this dataset is imbalanced towards negative reviews.\n* Therefore the important score to look at would be the **F1 Score**, on how the model performed.","40a9c30a":"#### Estimating length of the reviews \n* Calculating the length of text proves to be an important feature for classifying text in a Natural Language Processing problem.","18f658b8":"### Conclusions: \n* Feature Engineering is the most crucial step when it comes to Natural Language Processing. \n* Switching Count Vectorizer with a TDF IF Vectorizer also made a difference on F1 score. \n","641bce80":"* The bar plot of rating with respect to variation highlights that black dot is the most frequently ordered product and also most liked.","a540fc08":"* And the target vector **y**.","893818ac":"* Now, we can just concat all the features which we intend to use into a singe dataframe called **X**.","d0816ee9":"### Importing Data","8625c7fb":"### Visualizing your Exploratory Data Analysis:\n\n* With the help of this graph we can detect that the number of 5 rating review is high in this dataset. <br>\n* In other words it seems that customers are very much happy with Alexa products.","99393c22":"* One of the most important methods of random forest classifier in scikit learn is **feature_importances_**.\n* Let us have a look at the top 10 features.","0d9393d0":"### [Dataset link](https:\/\/www.kaggle.com\/sid321axn\/amazon-alexa-reviews)","0a114877":"#### Updated Column List:\n* As a result, we have added new columns in our dataset.","981a5cf5":"#### Gradient Boosting Classifier: ","3de03885":"#### Random Forest Classifier:","5c7d977b":"* Finally the length column, which depicts that customers with negative review tend to write a longer review.","e2412627":"* The primary methods used are **Random Forrest and Gradient Boosting** for this dataset. ","5e68747b":"#### Converting *date* attribute from string to datetime.date datatype\nWe will be using date column for feature engineering, so it would be a good idea if we convert this column from a **string** datatype to a **datetime.date** datatype.","c6779aa5":"### One Hot Encoding: <br>\n\n* For variation we will be using one hot encoding, which can be expalined by the image below.\n\n![ohe](https:\/\/i.imgur.com\/mtimFxh.png)\n<br>\n* One important thing to take care about it no matter how many dummy variables you end up having, just make sure that drop any one variable.\n* You can do this by setting **drop_first = True**.\n* This problem is sometimes stated as dummy variable trap.","ea4c61d3":"#### TfidfVectorizer:<br>\n\n* Since we cannot directly insert text data into out machine learning models, we will have to use a vectorizer.\n* The most vectorizer for any text data happens to be Count-Vectorizer, because it is easy to understand and relate to.\n* We will use Term frequency inverse document frequency (TF-IDF) vectorizer for this dataset.\n* The formula is as:\n![tdf](https:\/\/skymind.ai\/images\/wiki\/tfidf.png)","e446208c":"### Data Preprocessing:","82d81390":"* Changing the average function to mean again does not highlight anything important, just the fact that the products have high ratings."}}