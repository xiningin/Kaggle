{"cell_type":{"dc74cd65":"code","41a927e5":"code","5f7b8ed7":"code","96bcd50b":"code","015d5491":"code","46ae5a5a":"code","ed9b64d4":"code","bae725fa":"code","f114dc1e":"code","4385e7d1":"code","189051fc":"code","625d7ea1":"code","d9ab7608":"code","042f5b48":"code","c9b8eefc":"code","929af60d":"code","3877a1cd":"code","193f5f68":"code","2b6afa48":"code","2a1dee5e":"code","7258a778":"code","93ceeb83":"code","16739a96":"code","b9b0844b":"code","839c5462":"code","9d59b1ea":"code","64892a08":"code","c5181671":"code","21e9b31a":"code","ea8052ea":"code","a2bc731d":"code","d4bc9964":"code","10363c15":"code","4327ac2c":"code","f9cd59e6":"code","72e60e20":"code","3d33252f":"code","669c636d":"code","e76ac2f6":"code","ae118656":"code","a8cdc3e0":"code","0b9241ff":"code","03126347":"code","9ef9046b":"code","3054232b":"code","0add8eb0":"code","53331347":"code","a318f63d":"code","6a72bf72":"code","4d1f7425":"code","d6e7fc19":"code","5a2f3a0f":"code","c0e38c3d":"code","9e0e5e56":"code","dcfb1b27":"code","3d5895ba":"code","f50a5292":"code","cd2623d4":"code","f021127e":"code","b121a589":"code","e6130849":"code","e1bc42d9":"code","292948bf":"code","26b210dd":"code","828f2d7f":"code","fe8a2cc9":"code","7ec423db":"code","3d04968e":"code","0feb65d1":"code","16851d13":"code","a686789d":"code","d69e93ba":"code","1b3889ca":"code","455923ed":"code","0d9651aa":"code","a1b47b28":"code","8408f631":"code","17eee4d4":"code","93905aeb":"code","380adc19":"code","2f70d356":"code","4de519c6":"code","7f46eca9":"code","881cba69":"code","b08a2e5d":"code","2789f60e":"code","20af727d":"code","df30fdb5":"code","26dc774a":"code","e60d3e06":"code","2efcb41b":"code","5f74601b":"code","c5dcc1fa":"code","cd4c7a46":"code","c4d44769":"code","52e9f95d":"code","b68a805f":"code","06df1de9":"code","5d57a952":"code","681dfe6f":"code","ab819a43":"code","37701555":"code","2e6fb12a":"code","0217b49a":"code","f975ffcd":"code","c9d9b138":"code","c84939f4":"code","3b0cdf55":"code","cf8d4251":"code","537a6716":"code","5b14d880":"code","790ac0f4":"code","d801f2ef":"code","c6241769":"code","bf7beaea":"code","f2b90566":"code","6cb9b526":"code","a6a9ff08":"code","d2c2be76":"markdown","2213b2d7":"markdown","64c554b2":"markdown","a098eb1b":"markdown","6e0f1935":"markdown","3f140b87":"markdown","21fe473d":"markdown","26060569":"markdown","71264d2e":"markdown"},"source":{"dc74cd65":"%%time\n!cd ..\/input\/apex-master\/apex-master\/apex-master\/ && pip install --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" .","41a927e5":"import shutil\nimport sys\n# copy our file into the working directory\nsys.path.insert(0, \"..\/input\/pytorch-pretrained-bert-2\/pytorch-pretrained-bert-master\/pytorch-pretrained-BERT-master\/\")","5f7b8ed7":"DEBUG = False\n\nbatch_size = 256\n\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 2 * LSTM_UNITS\nn_seeds = 1\nn_splits = 10\nn_epochs = 15\nEMBED_SIZE = 300\nSUBGROUP_NEGATIVE_WEIGHT_COEF = 1\nBACKGROUND_POSITIVE_WEIGHT_COEF = 0\n\nENSEMBLE_START_EPOCH = 3\n\nMAX_LEN = 300\n\nEMB_DROPOUT = 0.3\nMIDDLE_DROPOUT = 0.3\n\nif DEBUG:\n    DEBUG_DATA_SIZE = 1000","96bcd50b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","015d5491":"MODEL_PATH = \"..\/input\/lstmgru-bs256-crawl-no-appos-predicted-ids-r\/results\"\nos.listdir(MODEL_PATH)[:10]","46ae5a5a":"import numpy as np\nimport pandas as pd\nimport os\nimport time\nimport gc\nimport random\nfrom tqdm import tqdm_notebook as tqdm\nfrom contextlib import contextmanager\nfrom fastprogress import master_bar, progress_bar\nfrom keras.preprocessing import sequence\nfrom keras import preprocessing\nimport torch\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.nn import functional as F\nimport matplotlib.pyplot as plt\nimport joblib\n%matplotlib inline","ed9b64d4":"from contextlib import contextmanager\nimport sys, os\n\n@contextmanager\ndef suppress_stdout():\n    with open(os.devnull, \"w\") as devnull:\n        old_stdout = sys.stdout\n        sys.stdout = devnull\n        try:  \n            yield\n        finally:\n            sys.stdout = old_stdout","bae725fa":"import re\n# \u3053\u308c\u3060\u3068\u3001'\u306fembedding\u306b\u7d50\u69cb\u5165\u3063\u3066\u308b\u306e\u306b\u9664\u5916\u3055\u308c\u3061\u3083\u3046\u3002\u3000\u3088\u304f\u306a\u3044\u306e\u3067 ' \u3060\u3051\u629c\u3044\u305f\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3', \n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', \n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', \n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', \n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', '\\n', '\\r']\n\ndef clean_text(x: str) -> str:\n    for punct in puncts:\n        if punct in x:\n            x = x.replace(punct, ' {} '.format(punct))\n    return x\n\ndef clean_numbers(x):\n    return re.sub('\\d+', ' ', x)\n\nif DEBUG:\n    test = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv', nrows=DEBUG_DATA_SIZE)\nelse:\n    test = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')\n\nx_test = test['comment_text'].apply(lambda x: clean_text(x))\ngc.collect()","f114dc1e":"aux_col_list = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']","4385e7d1":"%%time\nwith open('..\/input\/reducing-oov-of-crawl300d2m-no-appos-result\/jigsaw-crawl-300d-2M.joblib', 'rb') as f:\n    crawl_emb_dict = joblib.load(f)","189051fc":"import operator\nfrom typing import Dict, List\ndef build_vocab(texts: pd.DataFrame) -> Dict[str, int]:\n    \"\"\"\n    \n    Parameters\n    -----\n    texts: pandas.Series\n        question text\u306e\u5217\n        \n    Returns\n    -----\n    dict: \n        \u5358\u8a9e\u3068\u30ab\u30a6\u30f3\u30c8\n    \n    \"\"\"\n    sentences = texts.apply(lambda x: x.split()).values\n    vocab = {}\n    for sentence in tqdm(sentences):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab\n\ndef check_coverage(vocab: Dict[str, int], embeddings_index: Dict) -> List[str]:\n    \"\"\"\n    Parameters\n    -----\n    vocab: dict\n        \u5358\u8a9e\u3068\u30ab\u30a6\u30f3\u30c8\n    embeddings_index: dict\n        load_embed\u306e\u51fa\u529b\n        \n    Returns:\n        list:\n            embeddings\u306b\u5165\u3063\u3066\u306a\u3044\u5358\u8a9e\n    \"\"\"\n    known_words = {}\n    unknown_words = {}\n    nb_known_words = 0\n    nb_unknown_words = 0\n    for word in tqdm(vocab.keys()):\n        try:\n            known_words[word] = embeddings_index[word]\n            nb_known_words += vocab[word]\n        except:\n            unknown_words[word] = vocab[word]\n            nb_unknown_words += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(known_words) \/ float(len(vocab))))\n    print('Found embeddings for  {:.2%} of all text'.format(float(nb_known_words) \/ (nb_known_words + nb_unknown_words)))\n    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n\n    return unknown_words\n\nfrom nltk.stem import PorterStemmer\np_stemmer = PorterStemmer()\nfrom nltk.stem.lancaster import LancasterStemmer\nl_stemmer = LancasterStemmer()\nfrom nltk.stem import SnowballStemmer\ns_stemmer = SnowballStemmer(\"english\")\n\nimport copy\ndef edits1(word):\n    \"\"\"\n    word\u306e\u7de8\u96c6\u8ddd\u96e21\u306e\u5358\u8a9e\u306e\u30ea\u30b9\u30c8\u3092\u8fd4\u3059\n    \"\"\"\n    letters    = 'abcdefghijklmnopqrstuvwxyz'\n    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n    deletes    = [L + R[1:]               for L, R in splits if R]\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n    inserts    = [L + c + R               for L, R in splits for c in letters]\n    return set(deletes + transposes + replaces + inserts)\n\ndef known(words, embed): \n    \"The subset of `words` that appear in the dictionary of WORDS.\"\n    return set(w for w in words if w in embed)\n\ndef spellcheck(word, word_rank_dict):\n    return min(known(edits1(word), word_rank_dict), key=lambda w: word_rank_dict[w])\n\n\nimport unicodedata\npunct_mapping = {\"\u2018\": \"'\", \"\u20b9\": \"e\", \"\u00b4\": \"'\", \"\u00b0\": \"\",\n                 \"\u20ac\": \"e\", \"\u2122\": \"tm\", \"\u221a\": \" sqrt \", \"\u00d7\": \"x\",\n                 \"\u00b2\": \"2\", \"\u2014\": \"-\", \"\u2013\": \"-\", \"\u2019\": \"'\",\n                 \"_\": \"-\", \"`\": \"'\", '\u201c': '\"', '\u201d': '\"',\n                 '\u201c': '\"', \"\u00a3\": \"e\", '\u221e': 'infinity',\n                 '\u03b8': 'theta', '\u00f7': '\/', '\u03b1': 'alpha',\n                 '\u2022': '.', '\u00e0': 'a', '\u2212': '-', '\u03b2': 'beta',\n                 '\u2205': '', '\u00b3': '3', '\u03c0': 'pi', }\ndef process_stemmer(vocab, embed):\n    oov_word_set = set()\n    for word in tqdm(vocab.keys()):\n        vector = embed.get(word, None)\n        if vector is not None:\n            continue\n\n        vector = embed.get(word.lower(), None)\n        if vector is not None:\n            embed[word] = vector\n            continue\n\n        vector = embed.get(word.upper(), None)\n        if vector is not None:\n            embed[word] = vector\n            continue\n\n        vector = embed.get(word.capitalize(), None)\n        if vector is not None:\n            embed[word] = vector\n            continue\n            \n        corr_word = punct_mapping.get(word, None)\n        if corr_word is not None:\n            vector = embed.get(corr_word, None)\n            if vector is not None:\n                embed[word] = vector\n                continue\n        \n        try:\n            vector = embed.get(p_stemmer.stem(word), None)\n        except:\n            vector = embed.get(p_stemmer.stem(word.decode('utf-8')), None)\n        if vector is not None:\n            embed[word] = vector\n            continue\n            \n        try:\n            vector = embed.get(l_stemmer.stem(word), None)\n        except:\n            vector = embed.get(l_stemmer.stem(word.decode('utf-8')), None)\n        if vector is not None:\n            embed[word] = vector\n            continue\n        \n        try:\n            vector = embed.get(s_stemmer.stem(word), None)\n        except:\n            vector = embed.get(s_stemmer.stem(word.decode('utf-8')), None)     \n        if vector is not None:\n            embed[word] = vector\n            continue\n        \n        oov_word_set.add(word)\n            \n    return embed, oov_word_set\n\ndef process_small_capital(vocab, embed, oov_set):\n    oov_word_set = set()\n    for word in tqdm(vocab.keys()):\n        if word not in oov_set:\n            continue\n        \n        char_list = []\n        any_small_capitial = False\n        for char in word:\n            try:\n                uni_name = unicodedata.name(char)\n            except ValueError:\n                continue\n\n            if 'LATIN LETTER SMALL CAPITAL' in uni_name:\n                char = uni_name[-1]\n                any_small_capitial = True\n            if 'CYRILLIC SMALL LETTER GHE WITH STROKE' in uni_name:\n                char = 'F'\n                any_small_capitial = True\n\n            char_list.append(char)\n\n        if not any_small_capitial:\n            oov_word_set.add(word)\n            continue\n            \n        legit_word = ''.join(char_list)\n        \n        vector = embed.get(legit_word, None)\n        if vector is not None:\n            embed[word] = vector\n            continue\n\n        vector = embed.get(legit_word.lower(), None)\n        if vector is not None:\n            embed[word] = vector\n            continue\n\n        vector = embed.get(legit_word.upper(), None)\n        if vector is not None:\n            embed[word] = vector\n            continue\n\n        vector = embed.get(legit_word.capitalize(), None)\n        if vector is not None:\n            embed[word] = vector\n            continue\n            \n        corr_word = punct_mapping.get(legit_word, None)\n        if corr_word is not None:\n            vector = embed.get(corr_word, None)\n            if vector is not None:\n                embed[word] = vector\n                continue\n        \n        try:\n            vector = embed.get(p_stemmer.stem(legit_word), None)\n        except:\n            vector = embed.get(p_stemmer.stem(legit_word.decode('utf-8')), None)\n        if vector is not None:\n            embed[word] = vector\n            continue\n            \n        try:\n            vector = embed.get(l_stemmer.stem(legit_word), None)\n        except:\n            vector = embed.get(l_stemmer.stem(legit_word.decode('utf-8')), None)\n        if vector is not None:\n            embed[word] = vector\n            continue\n        \n        try:\n            vector = embed.get(s_stemmer.stem(legit_word), None)\n        except:\n            vector = embed.get(s_stemmer.stem(legit_word.decode('utf-8')), None)\n                    \n        if vector is not None:\n            embed[word] = vector\n            continue\n\n        oov_word_set.add(word)\n        \n    return embed, oov_word_set\n\ndef process_spellcheck(vocab, embed, word_rank_dict, oov_set):\n    oov_word_set = set()\n    \n    for word in tqdm(vocab.keys()):\n        if word not in oov_set:\n            continue\n            \n        try:\n            vector = embed.get(spellcheck(word, word_rank_dict), None)\n        except:\n            continue\n        if vector is not None:\n            embed[word] = vector\n            continue\n            \n        oov_word_set.add(word)\n            \n    return embed, oov_word_set\n\ndef make_word_rank(embed):\n    word_rank = {}\n    for i, word in enumerate(embed):\n        word_rank[word] = i\n    return word_rank\n\ndef head(enumerable, n=10):\n    for i, item in enumerate(enumerable):\n        print(str(i) + '\\n', item)\n        if i > n:\n            return\n","625d7ea1":"%%time\nvocab = build_vocab(x_test)\noov = check_coverage(vocab, crawl_emb_dict)\nhead(oov)","d9ab7608":"%%time\nemb, oov_stemmer = process_stemmer(vocab, crawl_emb_dict)\noov = check_coverage(vocab, emb)\nhead(oov)","042f5b48":"%%time\nemb, oov_small_capital = process_small_capital(vocab, emb, oov_stemmer)\noov = check_coverage(vocab, emb)\nhead(oov)","c9b8eefc":"%%time\nword_rank = make_word_rank(emb)\nemb, oov_spell = process_spellcheck(vocab, emb, word_rank, oov_small_capital)\noov = check_coverage(vocab, emb)\nhead(oov)","929af60d":"with open('..\/input\/googleprofanitywords\/list.txt', 'r') as f:\n    p_words = f.readlines()\n    \nset_puncts = set(puncts)\n\np_word_set = set([t.replace('\\n', '') for t in p_words])\n\ndef sentence_fetures(text):\n    word_list = text.split()\n    word_count = len(word_list)\n    n_upper = len([word for word in word_list if any([c.isupper() for c in word])])\n    n_unique = len(set(word_list))\n    n_ex = word_list.count('!')\n    n_que = word_list.count('?')\n    n_puncts = len([word for word in word_list if word in set_puncts])\n    n_prof = len([word for word in word_list if word in p_word_set])\n    n_oov = len([word for word in word_list if word not in crawl_emb_dict])\n    \n    return word_count, n_upper, n_unique, n_ex, n_que, n_puncts, n_prof, n_oov\n\nsentence_feature_cols = ['word_count', 'n_upper', 'n_unique', 'n_ex', 'n_que', 'n_puncts', 'n_prof', 'n_oov']\nfrom collections import defaultdict\nfeature_dict = defaultdict(list)\nfor text in progress_bar(x_test):\n    feature_list = sentence_fetures(text)\n    for i_feature, feature_name in enumerate(sentence_feature_cols):\n        feature_dict[sentence_feature_cols[i_feature]].append(feature_list[i_feature])\n        \nsentence_df = pd.DataFrame.from_dict(feature_dict)\nfor col in ['n_upper', 'n_unique', 'n_ex', 'n_que', 'n_puncts', 'n_prof', 'n_oov']:\n    sentence_df[col + '_ratio'] = sentence_df[col] \/ sentence_df['word_count']\nsentence_df.head()\n","3877a1cd":"fig, ax = plt.subplots(figsize=(10, 10))\ntmp = sentence_df.hist(ax=ax)","193f5f68":"sentence_feature_mat = sentence_df.drop(columns=['n_oov', 'n_oov_ratio']).values\ngc.collect()","2b6afa48":"%%time\n\nx_test = x_test.apply(lambda x: clean_numbers(x))\n\ntokenizer = preprocessing.text.Tokenizer(filters=\"\", lower=False)\ntokenizer.fit_on_texts(list(x_test))\n\nx_test = tokenizer.texts_to_sequences(x_test)\n\nmax_features = len(tokenizer.word_index) + 1\nmax_features\n\ndef build_matrix(word_index, embedding_dict):\n    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_dict[word]\n        except KeyError:\n            unknown_words.append(word)\n    return embedding_matrix, unknown_words\n\nemb_mat_crawl, oov = build_matrix(tokenizer.word_index, emb)\ndel crawl_emb_dict, oov, emb\ngc.collect()\n\nemb_mat_torch = torch.tensor(emb_mat_crawl, dtype=torch.float32).cuda()\ndel emb_mat_crawl\ngc.collect()\n","2a1dee5e":"import math\nclass DynamicBucketIterator(object):\n    def __init__(self, data, label, capacity, pad_token, shuffle, length_quantile, max_batch_size, for_bert):\n        self.data = data\n        self.label = label\n        self.pad_token = pad_token\n        self.capacity = capacity\n        self.shuffle = shuffle\n        self.length_quantile = length_quantile\n        self.for_bert = for_bert\n        \n        self.index_sorted = sorted(range(len(self.data)), key=lambda i: len(self.data[i]))\n        \n        old_separator_index = 0\n        self.separator_index_list = [0]\n        for i_sample in range(len(self.data)):\n            sample_index = self.index_sorted[i_sample]\n            sample = self.data[sample_index]\n            current_batch_size = i_sample - old_separator_index + 1\n            if min(len(sample), MAX_LEN) * current_batch_size <= self.capacity and current_batch_size <= max_batch_size:\n                pass\n            else:\n                old_separator_index = i_sample\n                self.separator_index_list.append(i_sample)\n                \n        self.separator_index_list.append(len(self.data)) # [0, ..., start_separator_index, end_separator_index, ..., len(data)]\n        \n        if not self.shuffle:\n            self.bucket_index = range(self.__len__())\n        \n        self.reset_index()\n\n    def reset_index(self):\n        self.i_batch = 0\n        \n        if self.shuffle:\n            self.index_sorted = sorted(np.random.permutation(len(self.data)), key=lambda i: len(self.data[i]))\n            self.bucket_index = np.random.permutation(self.__len__())\n    \n    def __len__(self):\n        return len(self.separator_index_list) - 1\n    \n    def __iter__(self):\n        return self\n    \n    def __next__(self):\n        try:\n            i_bucket = self.bucket_index[self.i_batch]\n        except IndexError as e:\n            self.reset_index()\n            raise StopIteration\n            \n        start_index, end_index = self.separator_index_list[i_bucket : i_bucket + 2]\n        \n        index_batch = self.index_sorted[start_index : end_index]\n\n        raw_batch_data = [self.data[i] for i in index_batch]\n        \n        batch_label = self.label[index_batch]\n        \n        math.ceil(1)\n        \n        max_len = int(math.ceil(np.quantile([len(x) for x in raw_batch_data], self.length_quantile)))\n        max_len = min([max_len, MAX_LEN])\n        if max_len == 0:\n            max_len = 1\n        \n        if self.for_bert:\n            segment_id_batch = np.zeros((len(raw_batch_data), max_len))\n            padded_batch = []\n            input_mask_batch = []\n            for sample in raw_batch_data:\n                input_mask = [1] * len(sample) + [0] * (max_len - len(sample))\n                input_mask_batch.append(input_mask[:max_len])\n\n                sample = sample + [self.pad_token for _ in range(max_len - len(sample))]\n                padded_batch.append(sample[:max_len])\n\n            self.i_batch += 1\n\n            return padded_batch, segment_id_batch, input_mask_batch, batch_label, index_batch\n        \n        else:\n            padded_batch = []\n            for sample in raw_batch_data:\n                sample = sample + [self.pad_token for _ in range(max_len - len(sample))]\n                padded_batch.append(sample[:max_len])\n\n            self.i_batch += 1\n\n            return padded_batch, batch_label, index_batch","7258a778":"class SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x\n    \nclass GRULayer(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(GRULayer, self).__init__()\n        \n        self.gru = nn.GRU(input_size=input_size,\n                          hidden_size=hidden_size,\n                          bias=True,\n                          bidirectional=True,\n                          batch_first=True)\n        \n        self.init_weights()\n        \n    def init_weights(self):\n        ih = (param.data for name, param in self.named_parameters() if 'weight_ih' in name)\n        hh = (param.data for name, param in self.named_parameters() if 'weight_hh' in name)\n        b = (param.data for name, param in self.named_parameters() if 'bias' in name)\n        for k in ih:\n            nn.init.xavier_uniform_(k)\n        for k in hh:\n            nn.init.orthogonal_(k)\n        for k in b:\n            nn.init.constant_(k, 0)\n\n    def forward(self, x):\n        gru_outputs, gru_state = self.gru(x)\n        return gru_outputs, gru_state\n    \n    \nclass NeuralNet(nn.Module):\n    def __init__(self, embed_size, num_aux_targets, num_sentence_features):\n        super(NeuralNet, self).__init__()\n        self.embedding_dropout = SpatialDropout(EMB_DROPOUT)\n        \n        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n        self.middle_dropout = SpatialDropout(MIDDLE_DROPOUT)\n        \n        self.lstm2 = GRULayer(LSTM_UNITS * 2, LSTM_UNITS)\n        \n        self.linear_sentence1 = nn.Linear(num_sentence_features, num_sentence_features)\n        \n        n_hidden = LSTM_UNITS * 2 + num_sentence_features\n        self.linear1 = nn.Linear(n_hidden, n_hidden) # bi-max, bi-ave, sentence_features\n            \n        self.linear_out = nn.Linear(n_hidden, 1)\n        self.linear_aux_out = nn.Linear(n_hidden, num_aux_targets)\n        \n    def forward(self, x, sentence_features):\n        h_embedding = self.embedding_dropout(x)\n        \n        h_lstm1, _ = self.lstm1(h_embedding)\n        \n        h_lstm1 = self.middle_dropout(h_lstm1)\n        \n        h_lstm2, _ = self.lstm2(h_lstm1)\n        \n        # global max pooling\n        max_pool, _ = torch.max(h_lstm2, 1)\n            \n        h_sentence = self.linear_sentence1(sentence_features)\n        h_cat = torch.cat((max_pool, h_sentence), 1)\n        \n        h_conc_linear1  = F.relu(self.linear1(h_cat))\n        \n        hidden = h_cat + h_conc_linear1\n        \n        result = self.linear_out(hidden)\n        aux_result = self.linear_aux_out(hidden)\n        out = torch.cat([result, aux_result], 1)\n        \n        return out","93ceeb83":"def sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))\n\nOOF_TRAIN_COL = 'oof_train'\nSUBGROUP_AUC_COL = 'subgroup_auc'\nBPSN_AUC_COL = 'bpsn_auc'  # stands for background positive, subgroup negative\nBNSP_AUC_COL = 'bnsp_auc'  # stands for background negative, subgroup positive\nfrom sklearn import metrics\ndef compute_auc(y_true, y_pred):\n    try:\n        return metrics.roc_auc_score(y_true, y_pred)\n    except ValueError:\n        return np.nan\n'model'\ndef compute_subgroup_auc(df, subgroup_col, label_col, oof_col):\n    subgroup_examples = df[df[subgroup_col]]\n    return compute_auc(subgroup_examples[label_col], subgroup_examples[oof_col])\n\ndef compute_bpsn_auc(df, subgroup_col, label_col, oof_col):\n    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n    subgroup_negative_examples = df[df[subgroup_col] & ~df[label_col]]\n    non_subgroup_positive_examples = df[~df[subgroup_col] & df[label_col]]\n    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n    return compute_auc(examples[label_col], examples[oof_col])\n\ndef compute_bnsp_auc(df, subgroup_col, label_col, oof_col):\n    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n    subgroup_positive_examples = df[df[subgroup_col] & df[label_col]]\n    non_subgroup_negative_examples = df[~df[subgroup_col] & ~df[label_col]]\n    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n    return compute_auc(examples[label_col], examples[oof_col])\n\ndef compute_bias_metrics_for_model(df,\n                                   subgroup_list,\n                                   oof_col,\n                                   label_col,\n                                   include_asegs=False):\n    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n    record_list = []\n    for subgroup in subgroup_list:\n        record = {\n            'subgroup': subgroup,\n            'subgroup_size': len(df[df[subgroup]])\n        }\n        record[SUBGROUP_AUC_COL] = compute_subgroup_auc(df, subgroup, label_col, oof_col)\n        record[BPSN_AUC_COL] = compute_bpsn_auc(df, subgroup, label_col, oof_col)\n        record[BNSP_AUC_COL] = compute_bnsp_auc(df, subgroup, label_col, oof_col)\n        record_list.append(record)\n    return pd.DataFrame(record_list).sort_values('subgroup_auc', ascending=True)\n\nTOXICITY_COLUMN = 'target'\nidentity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n# Convert taget and identity columns to booleans\ndef convert_to_bool(df, col_name):\n    df[col_name] = np.where(df[col_name] >= 0.5, True, False)\n    \ndef convert_dataframe_to_bool(df):\n    bool_df = df.copy()\n    for col in ['target'] + identity_columns:\n        convert_to_bool(bool_df, col)\n    return bool_df\n\ndef calculate_overall_auc(df, model_name):\n    true_labels = df[TOXICITY_COLUMN]\n    predicted_labels = df[model_name]\n    return metrics.roc_auc_score(true_labels, predicted_labels)\n\ndef power_mean(series, p):\n    total = sum(np.power(series, p))\n    return np.power(total \/ len(series), 1 \/ p)\n\ndef get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n    bias_score = np.average([\n        power_mean(bias_df[SUBGROUP_AUC_COL], POWER),\n        power_mean(bias_df[BPSN_AUC_COL], POWER),\n        power_mean(bias_df[BNSP_AUC_COL], POWER)\n    ])\n    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)\n\ndef get_various_auc(valid_df, y_pred):\n    valid_df = convert_dataframe_to_bool(valid_df.fillna(0))\n    valid_df.loc[:, OOF_TRAIN_COL] = y_pred\n    valid_df = convert_dataframe_to_bool(valid_df.fillna(0))\n    bias_metrics_df = compute_bias_metrics_for_model(valid_df, identity_columns, OOF_TRAIN_COL, TOXICITY_COLUMN)\n    overall_auc = calculate_overall_auc(valid_df, OOF_TRAIN_COL)\n    return get_final_metric(bias_metrics_df, overall_auc), overall_auc, bias_metrics_df","16739a96":"gc.collect()\ntorch.cuda.empty_cache()\ntorch.cuda.memory_allocated()","b9b0844b":"from sklearn.preprocessing import StandardScaler\nloss_fn=nn.BCEWithLogitsLoss(reduction='sum')\n\nauc_array = np.zeros((n_seeds, n_splits, n_epochs))\n\ntest_array = np.zeros((n_seeds, n_splits, n_epochs, len(x_test)))\n\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=n_splits, shuffle=True)\nfor i_seed in range(n_seeds):\n    print(f'start seed {i_seed}')\n\n    for i_fold, (dev_index, val_index) in enumerate(kf.split(x_test)): # \u4f7f\u3063\u3066\u308b\u306e\u306f i_fold\u3060\u3051\n        print(f'start fold {i_fold}')\n        scaler = joblib.load(os.path.join(MODEL_PATH, f'scaler-seed{i_seed}-fold{i_fold}.joblib'))\n        test_sentence_feature_mat = scaler.transform(sentence_feature_mat)\n\n        test_loader = DynamicBucketIterator(x_test, \n                                            torch.tensor(np.concatenate([np.zeros((len(x_test), 1+len(aux_col_list))),\n                                                         test_sentence_feature_mat], axis=1),dtype=torch.float32).cuda(),\n                                            capacity=MAX_LEN*batch_size, pad_token=0, shuffle=False,\n                                            length_quantile=1, max_batch_size=10000000, for_bert=False)\n        \n        print(torch.cuda.memory_allocated())\n        for i_epoch in range(ENSEMBLE_START_EPOCH, n_epochs):\n            start_time = time.time()\n            model = NeuralNet(EMBED_SIZE, len(aux_col_list), sentence_feature_mat.shape[-1])\n            load_path = os.path.join(MODEL_PATH, f'seed{i_seed}-fold{i_fold}-epoch{i_epoch}.torchModelState')\n            model.load_state_dict(torch.load(load_path))\n            model.cuda()\n            \n            model.eval()\n            \n            epoch_test_pred = np.zeros(len(x_test))\n            mem_list = []\n            for batch in test_loader:\n                x_batch = batch[0]\n                y_batch = batch[1]\n                index_batch = batch[2]\n                \n                y_true_batch = y_batch[:, :1+len(aux_col_list)]\n                sample_weight_batch = y_batch[:, 1+len(aux_col_list)]\n                sentence_feature_batch = y_batch[:, -sentence_feature_mat.shape[-1]:]\n                \n                if len(x_batch) >= 32:\n                    y_pred = model(emb_mat_torch[x_batch], sentence_feature_batch)\n                else:\n                    # \u30a4\u30df\u30d5\u3060\u3051\u3069\u3001len(x_batch)\u304c32\u672a\u6e80\u3060\u3068\u30d0\u30b0\u308b\n                    len_last = len(x_batch)\n                    x_batch.extend([[0 for _ in range(len(x_batch[0]))] for _ in range(32)])\n                    y_pred = model(emb_mat_torch[x_batch[:32]][:len_last], sentence_feature_batch)\n                \n                mem_list.append(torch.cuda.memory_allocated())\n                epoch_test_pred[index_batch] = sigmoid(y_pred[:, 0].detach().cpu().numpy())\n                \n                del y_pred\n                torch.cuda.empty_cache()\n                            \n            test_array[i_seed, i_fold, i_epoch] = epoch_test_pred\n            \n            elapsed_time = time.time() - start_time\n            \n            del model\n            gc.collect()\n            torch.cuda.empty_cache()\n            print(f'epoch {i_epoch}: {elapsed_time: 0.3f}, {max(mem_list)\/10**9}')\n            \n\n        del epoch_test_pred\n        gc.collect()\n        torch.cuda.empty_cache()","839c5462":"test_pred_rnn = np.mean(test_array[:, :, ENSEMBLE_START_EPOCH:], axis=(0, 1, 2))\nplt.hist(test_pred_rnn, bins=20)","9d59b1ea":"del test_loader, emb_mat_torch, test_array\ngc.collect()\ntorch.cuda.empty_cache()\ntorch.cuda.memory_allocated()","64892a08":"i_seed = 0\n    \nSENTENCE_FEAURE_USED = ['word_count', 'n_unique', 'n_unique_ratio']\n\nbatch_size = 32\n\nOUT_DROPOUT = 0.3\n\nBERT_HIDDEN_SIZE = 768\n\nMAX_LEN = 220","c5181671":"import numpy as np\nimport pandas as pd\nimport os\nimport time\nimport gc\nimport random\nfrom tqdm import tqdm\ntqdm.pandas()\nfrom contextlib import contextmanager\nfrom fastprogress import master_bar, progress_bar\nfrom keras.preprocessing import text, sequence\nimport torch\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.nn import functional as F\nimport matplotlib.pyplot as plt\n%matplotlib inline","21e9b31a":"sentence_feature_mat = sentence_df[SENTENCE_FEAURE_USED].values\ngc.collect()","ea8052ea":"%%time\nif DEBUG:\n    test = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv', nrows=DEBUG_DATA_SIZE)\nelse:\n    test = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')\n\nx_test = test['comment_text']\ndel test\ngc.collect()","a2bc731d":"import torch\nfrom pytorch_pretrained_bert import GPT2Tokenizer, GPT2Model\n\n# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\nimport logging\nlogging.basicConfig(level=logging.INFO)","d4bc9964":"tokenizer = GPT2Tokenizer(vocab_file='..\/input\/gpt2-models\/vocab.json', merges_file='..\/input\/gpt2-models\/merges.txt')","10363c15":"%%time\nx_test = x_test.progress_apply(lambda x: tokenizer.encode(x[:MAX_LEN]))","4327ac2c":"class NeuralNet(nn.Module):\n    def __init__(self, num_aux_targets, num_sentence_features):\n        super(NeuralNet, self).__init__()\n        self.gpt2 = GPT2Model.from_pretrained('..\/input\/gpt2-models\/')\n        self.dropout = nn.Dropout(OUT_DROPOUT)\n        \n        self.linear_sentence1 = nn.Linear(num_sentence_features, num_sentence_features)\n        \n        n_hidden = BERT_HIDDEN_SIZE + num_sentence_features\n        self.linear1 = nn.Linear(n_hidden, n_hidden)\n        \n        self.linear_out = nn.Linear(n_hidden, 1)\n        self.linear_aux_out = nn.Linear(n_hidden, num_aux_targets)\n        \n    def forward(self, x_features, sentence_features):\n        \n        h_gpt2, _ = self.gpt2(x_features) # (bsz, seq_len, hidden_size)\n        \n        h_gpt2, _ = torch.max(h_gpt2, 1) # (bsz, hidden_size)\n        \n        h_sentence = self.linear_sentence1(sentence_features)\n        \n        h_cat = torch.cat((h_gpt2, h_sentence), 1)\n        \n        h_conc_linear1  = F.relu(self.linear1(h_cat))\n        \n        hidden = h_cat + h_conc_linear1\n        \n        result = self.linear_out(hidden)\n        aux_result = self.linear_aux_out(hidden)\n        out = torch.cat([result, aux_result], 1)\n        \n        return out","f9cd59e6":"MODEL_PATH = '..\/input\/gpt2-f1e1-max-results'\nos.listdir(MODEL_PATH)","72e60e20":"from sklearn.preprocessing import StandardScaler\nloss_fn=nn.BCEWithLogitsLoss(reduction='sum')\n\naux_col_list = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n\ntest_dict = {}\nfold_list = [1]\ni_epoch = 1\nstart_time = time.time()\nfor i_fold in fold_list:\n    fold_start_time = time.time()\n    \n    print(f'start fold {i_fold}')\n    scaler = joblib.load(os.path.join(MODEL_PATH, f'scaler-seed{0}-fold{i_fold}.joblib'))\n    test_sentence_feature_mat = scaler.transform(sentence_feature_mat)\n\n    test_loader = DynamicBucketIterator(x_test, \n                                        torch.tensor(np.concatenate([np.zeros((len(x_test), 1+len(aux_col_list))), test_sentence_feature_mat], axis=1),dtype=torch.float32).cuda(),\n                                        capacity=MAX_LEN*batch_size, pad_token=0, shuffle=False, length_quantile=1, max_batch_size=2048, for_bert=False)\n\n    print(torch.cuda.memory_allocated())\n\n    model = NeuralNet(len(aux_col_list), sentence_feature_mat.shape[-1])\n    load_path = os.path.join(MODEL_PATH, f'seed{0}-fold{i_fold}-epoch{i_epoch}.torchModelState')\n    model.load_state_dict(torch.load(load_path)['model_state_dict'])\n    model.cuda()\n\n    model.eval()\n\n    epoch_test_pred = np.zeros(len(x_test))\n    eval_start_time = time.time()\n    for batch in progress_bar(test_loader):\n        x_batch = torch.tensor(batch[0], dtype=torch.long).cuda()\n        y_batch = batch[1]\n        index_batch = batch[2]\n        \n        y_true_batch = y_batch[:, :1+len(aux_col_list)]\n        sample_weight_batch = y_batch[:, 1+len(aux_col_list)]\n        sentence_feature_batch = y_batch[:, -sentence_feature_mat.shape[-1]:]\n\n        y_pred = model(x_batch, sentence_feature_batch)\n    #                 print('after_prediction', torch.cuda.memory_allocated())\n        epoch_test_pred[index_batch] = sigmoid(y_pred[:, 0].detach().cpu().numpy())\n        del x_batch, y_pred\n        torch.cuda.empty_cache()\n    print(f'uncased {i_fold} finishd in {time.time() - fold_start_time}', file=open('time.txt', 'a'))\n\n    test_dict[i_fold] = epoch_test_pred\n    del model, epoch_test_pred\n    gc.collect()\n    torch.cuda.empty_cache()\n    \nprint(f'uncased finishd in {time.time() - start_time}', file=open('time.txt', 'a'))\n    \n","3d33252f":"test_pred_gpt2_max = np.mean(list(test_dict.values()), axis=0)\n\nplt.hist(test_pred_gpt2_max, bins=20)\n\n","669c636d":"del test_loader, test_dict\ngc.collect()\ntorch.cuda.empty_cache()\ntorch.cuda.memory_allocated()","e76ac2f6":"i_seed = 0\n    \nSENTENCE_FEAURE_USED = ['word_count', 'n_unique', 'n_unique_ratio']\n\nbatch_size = 32\n\nOUT_DROPOUT = 0.3\n\nBERT_HIDDEN_SIZE = 768\n\nBERT_MODEL_PATH = '..\/input\/bert-pretrained-models\/uncased_l-12_h-768_a-12\/uncased_L-12_H-768_A-12\/'\nBERT_DO_LOWER = 'uncased' in BERT_MODEL_PATH\n\nMAX_LEN = 220\n","ae118656":"import numpy as np\nimport pandas as pd\nimport os\nimport time\nimport gc\nimport random\nfrom contextlib import contextmanager\nfrom fastprogress import master_bar, progress_bar\nfrom keras.preprocessing import sequence\nfrom keras import preprocessing\nfrom tqdm import tqdm\ntqdm.pandas()\nimport torch\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.nn import functional as F\nimport matplotlib.pyplot as plt\nimport joblib\n%matplotlib inline","a8cdc3e0":"%%time\nif DEBUG:\n    test = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv', nrows=DEBUG_DATA_SIZE)\nelse:\n    test = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')\n\nx_test = test['comment_text']\ndel test\ngc.collect()","0b9241ff":"sentence_df.head(10)","03126347":"sentence_feature_mat = sentence_df[SENTENCE_FEAURE_USED].values\ngc.collect()","9ef9046b":"WORK_DIR = \"..\/working\/\"\nos.listdir(WORK_DIR)","3054232b":"import torch\nfrom pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\nfrom pytorch_pretrained_bert import BertTokenizer, BertModel\n\n# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\nimport logging\nlogging.basicConfig(level=logging.INFO)\n","0add8eb0":"BERT_DO_LOWER","53331347":"tokenizer = BertTokenizer.from_pretrained(\n    BERT_MODEL_PATH, cache_dir=None, do_lower_case=BERT_DO_LOWER)","a318f63d":"def tokenize(text, max_len, tokenizer):\n    tokenized_text = tokenizer.tokenize(text)[:max_len-2]\n    return [\"[CLS]\"]+tokenized_text+[\"[SEP]\"]","6a72bf72":"%%time\n# clean\u3057\u305f\u306e\u306f x_test_cleaned\u306b\u5165\u3063\u3066\u3066\u3001x_test \u306f\u305d\u306e\u307e\u307e\nfrom tqdm._tqdm_notebook import tqdm_notebook\ntqdm_notebook.pandas()\nx_test = x_test.progress_apply(lambda x: tokenize(x, MAX_LEN, tokenizer))","4d1f7425":"max(len(t) for t in x_test)","d6e7fc19":"x_test.head(20)","5a2f3a0f":"x_test = x_test.progress_apply(lambda x: tokenizer.convert_tokens_to_ids(x))","c0e38c3d":"x_test.head(20)","9e0e5e56":"del tokenizer\ngc.collect()","dcfb1b27":"torch.cuda.empty_cache()\ntorch.cuda.memory_allocated()","3d5895ba":"with suppress_stdout():\n    convert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n        BERT_MODEL_PATH + 'bert_model.ckpt',\n    BERT_MODEL_PATH + 'bert_config.json',\n    WORK_DIR + 'pytorch_model.bin')\n\nshutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'bert_config.json')","f50a5292":"gc.collect()\ntorch.cuda.empty_cache()\ntorch.cuda.memory_allocated()","cd2623d4":"from glob import glob\ndef get_model_path(i_seed, i_fold, i_epoch, base_path):\n    for dir_name in glob(base_path + '*'):\n        if (i_seed == int(re.match('.*s\\d', dir_name).group()[-1]) and\n            i_fold == int(re.match('.*f\\d', dir_name).group()[-1]) and\n            i_epoch == int(re.match('.*e\\d', dir_name).group()[-1])\n           ):\n            return dir_name\n            ","f021127e":"get_model_path(0, 0, 1, '..\/input\/base-uncased')","b121a589":"class NeuralNet(nn.Module):\n    def __init__(self, num_aux_targets, num_sentence_features):\n        super(NeuralNet, self).__init__()\n        self.bert = BertModel.from_pretrained(WORK_DIR)\n        self.dropout = nn.Dropout(OUT_DROPOUT)\n        \n        self.linear_sentence1 = nn.Linear(num_sentence_features, num_sentence_features)\n        \n        n_hidden = BERT_HIDDEN_SIZE + num_sentence_features\n        self.linear1 = nn.Linear(n_hidden, n_hidden)\n        \n        self.linear_out = nn.Linear(n_hidden, 1)\n        self.linear_aux_out = nn.Linear(n_hidden, num_aux_targets)\n        \n    def forward(self, x_features, sentence_features):\n        \n        _, bert_output = self.bert(*x_features, output_all_encoded_layers=False)\n        \n        bert_output = self.dropout(bert_output)\n        \n        h_sentence = self.linear_sentence1(sentence_features)\n        \n        h_cat = torch.cat((bert_output, h_sentence), 1)\n        \n        h_conc_linear1  = F.relu(self.linear1(h_cat))\n        \n        hidden = h_cat + h_conc_linear1\n        \n        result = self.linear_out(hidden)\n        aux_result = self.linear_aux_out(hidden)\n        out = torch.cat([result, aux_result], 1)\n        \n        return out","e6130849":"from sklearn.preprocessing import StandardScaler\nloss_fn=nn.BCEWithLogitsLoss(reduction='sum')\n\naux_col_list = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n\ntest_dict = {}\nfold_list = [0]\n\ni_epoch = 2\nstart_time = time.time()\nfor i_fold in fold_list:\n    fold_start_time = time.time()\n    MODEL_PATH = '..\/input\/bert-base-uncased-ftpe18-e2-5e6'\n    print(f'start fold {i_fold}')\n    scaler = joblib.load(os.path.join(MODEL_PATH, f'scaler-seed{0}-fold{i_fold}.joblib'))\n    test_sentence_feature_mat = scaler.transform(sentence_feature_mat)\n\n    test_loader = DynamicBucketIterator(x_test, \n                                        torch.tensor(np.concatenate([np.zeros((len(x_test), 1+len(aux_col_list))), test_sentence_feature_mat], axis=1),dtype=torch.float32).cuda(),\n                                        capacity=MAX_LEN*batch_size, pad_token=0, shuffle=False, length_quantile=1, max_batch_size=2048, for_bert=True)\n\n    print(torch.cuda.memory_allocated())\n\n    model = NeuralNet(len(aux_col_list), sentence_feature_mat.shape[-1])\n    load_path = os.path.join(MODEL_PATH, f'seed{0}-fold{i_fold}-epoch{i_epoch}.torchModelState')\n    model.load_state_dict(torch.load(load_path)['model_state_dict'])\n    model.cuda()\n\n    model.eval()\n\n    epoch_test_pred = np.zeros(len(x_test))\n    eval_start_time = time.time()\n    for batch in test_loader:\n        x_batch = batch[0]\n        segment_id_batch = batch[1]\n        input_mask_batch = batch[2]\n        y_batch = batch[3]\n        index_batch = batch[4]\n        y_true_batch = y_batch[:, :1+len(aux_col_list)]\n        sample_weight_batch = y_batch[:, 1+len(aux_col_list)]\n        sentence_feature_batch = y_batch[:, -sentence_feature_mat.shape[-1]:]\n        x_features = [torch.tensor(feature, dtype=torch.long).cuda() for feature in [x_batch, segment_id_batch, input_mask_batch]]\n    #                 print('x_features', torch.cuda.memory_allocated())\n\n        y_pred = model(x_features, sentence_feature_batch)\n    #                 print('after_prediction', torch.cuda.memory_allocated())\n        epoch_test_pred[index_batch] = sigmoid(y_pred[:, 0].detach().cpu().numpy())\n        del x_features, y_pred\n        torch.cuda.empty_cache()\n    print(f'uncased {i_fold} finishd in {time.time() - fold_start_time}', file=open('time.txt', 'a'))\n\n    test_dict[i_fold] = epoch_test_pred\n    del model, epoch_test_pred\n    gc.collect()\n    torch.cuda.empty_cache()\n    \nprint(f'uncased finishd in {time.time() - start_time}', file=open('time.txt', 'a'))","e1bc42d9":"test_pred_base_uncased_ftp13 = np.mean(list(test_dict.values()), axis=0)\n\nplt.hist(test_pred_base_uncased_ftp13, bins=20)\n","292948bf":"class NeuralNet(nn.Module):\n    def __init__(self, num_aux_targets, num_sentence_features):\n        super(NeuralNet, self).__init__()\n        self.bert_model = BertModel.from_pretrained(WORK_DIR)\n        self.dropout = nn.Dropout(OUT_DROPOUT)\n        \n        self.linear_sentence1 = nn.Linear(num_sentence_features, num_sentence_features)\n        \n        n_hidden = BERT_HIDDEN_SIZE + num_sentence_features\n        self.linear1 = nn.Linear(n_hidden, n_hidden)\n        \n        self.linear_out = nn.Linear(n_hidden, 1)\n        self.linear_aux_out = nn.Linear(n_hidden, num_aux_targets)\n        \n    def forward(self, x_features, sentence_features):\n        \n        _, bert_output = self.bert_model(*x_features, output_all_encoded_layers=False)\n        \n        bert_output = self.dropout(bert_output)\n        \n        h_sentence = self.linear_sentence1(sentence_features)\n        \n        h_cat = torch.cat((bert_output, h_sentence), 1)\n        \n        h_conc_linear1  = F.relu(self.linear1(h_cat))\n        \n        hidden = h_cat + h_conc_linear1\n        \n        result = self.linear_out(hidden)\n        aux_result = self.linear_aux_out(hidden)\n        out = torch.cat([result, aux_result], 1)\n        \n        return out","26b210dd":"from sklearn.preprocessing import StandardScaler\nloss_fn=nn.BCEWithLogitsLoss(reduction='sum')\n\naux_col_list = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n\ntest_dict = {}\nfold_list = [1, 2]\n\ni_epoch = 1\nstart_time = time.time()\nfor i_fold in fold_list:\n    fold_start_time = time.time()\n    MODEL_PATH = get_model_path(i_seed, i_fold, i_epoch, '..\/input\/base-uncased')\n    print(f'start fold {i_fold}')\n    scaler = joblib.load(os.path.join(MODEL_PATH, f'scaler-seed{0}-fold{i_fold}.joblib'))\n    test_sentence_feature_mat = scaler.transform(sentence_feature_mat)\n\n    test_loader = DynamicBucketIterator(x_test, \n                                        torch.tensor(np.concatenate([np.zeros((len(x_test), 1+len(aux_col_list))), test_sentence_feature_mat], axis=1),dtype=torch.float32).cuda(),\n                                        capacity=MAX_LEN*batch_size, pad_token=0, shuffle=False, length_quantile=1, max_batch_size=2048, for_bert=True)\n\n    print(torch.cuda.memory_allocated())\n\n    model = NeuralNet(len(aux_col_list), sentence_feature_mat.shape[-1])\n    load_path = os.path.join(MODEL_PATH, f'seed{0}-fold{i_fold}-epoch{i_epoch}.torchModelState')\n    model.load_state_dict(torch.load(load_path)['model_state_dict'])\n    model.cuda()\n\n    model.eval()\n\n    epoch_test_pred = np.zeros(len(x_test))\n    eval_start_time = time.time()\n    for batch in test_loader:\n        x_batch = batch[0]\n        segment_id_batch = batch[1]\n        input_mask_batch = batch[2]\n        y_batch = batch[3]\n        index_batch = batch[4]\n        y_true_batch = y_batch[:, :1+len(aux_col_list)]\n        sample_weight_batch = y_batch[:, 1+len(aux_col_list)]\n        sentence_feature_batch = y_batch[:, -sentence_feature_mat.shape[-1]:]\n        x_features = [torch.tensor(feature, dtype=torch.long).cuda() for feature in [x_batch, segment_id_batch, input_mask_batch]]\n    #                 print('x_features', torch.cuda.memory_allocated())\n\n        y_pred = model(x_features, sentence_feature_batch)\n    #                 print('after_prediction', torch.cuda.memory_allocated())\n        epoch_test_pred[index_batch] = sigmoid(y_pred[:, 0].detach().cpu().numpy())\n        del x_features, y_pred\n        torch.cuda.empty_cache()\n    print(f'uncased {i_fold} finishd in {time.time() - fold_start_time}', file=open('time.txt', 'a'))\n\n    test_dict[i_fold] = epoch_test_pred\n    del model, epoch_test_pred\n    gc.collect()\n    torch.cuda.empty_cache()\n    \nprint(f'uncased finishd in {time.time() - start_time}', file=open('time.txt', 'a'))\n    \n","828f2d7f":"test_pred_base_uncased = np.mean(list(test_dict.values()), axis=0)\n\nplt.hist(test_pred_base_uncased, bins=20)\n\ndel test_loader, test_dict\ngc.collect()\ntorch.cuda.empty_cache()\ntorch.cuda.memory_allocated()\n","fe8a2cc9":"fold_list = [4, 5]\n\ni_seed = 0\n    \nSENTENCE_FEAURE_USED = ['word_count', 'n_unique', 'n_unique_ratio']\n\nbatch_size = 32\n\nOUT_DROPOUT = 0.3\n\nBERT_HIDDEN_SIZE = 768\n\nBERT_MODEL_PATH = '..\/input\/bert-pretrained-models\/cased_l-12_h-768_a-12\/cased_L-12_H-768_A-12\/'\nBERT_DO_LOWER = 'uncased' in BERT_MODEL_PATH\n\nMAX_LEN = 220","7ec423db":"if DEBUG:\n    test = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv', nrows=DEBUG_DATA_SIZE)\nelse:\n    test = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')\n\nx_test = test['comment_text']\ndel test\ngc.collect()","3d04968e":"BERT_DO_LOWER","0feb65d1":"tokenizer = BertTokenizer.from_pretrained(\n    BERT_MODEL_PATH, cache_dir=None, do_lower_case=BERT_DO_LOWER)","16851d13":"def tokenize(text, max_len, tokenizer):\n    tokenized_text = tokenizer.tokenize(text)[:max_len-2]\n    return [\"[CLS]\"]+tokenized_text+[\"[SEP]\"]\nx_test = x_test.progress_apply(lambda x: tokenize(x, MAX_LEN, tokenizer))","a686789d":"max(len(t) for t in x_test)","d69e93ba":"x_test.head(20)","1b3889ca":"x_test = x_test.progress_apply(lambda x: tokenizer.convert_tokens_to_ids(x))","455923ed":"x_test.head(20)","0d9651aa":"del tokenizer\ngc.collect()","a1b47b28":"torch.cuda.empty_cache()\ntorch.cuda.memory_allocated()","8408f631":"with suppress_stdout():\n    convert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n        BERT_MODEL_PATH + 'bert_model.ckpt',\n    BERT_MODEL_PATH + 'bert_config.json',\n    WORK_DIR + 'pytorch_model.bin')\n\nshutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'bert_config.json')","17eee4d4":"gc.collect()\ntorch.cuda.empty_cache()\ntorch.cuda.memory_allocated()","93905aeb":"get_model_path(i_seed, 4, 1, '..\/input\/base-cased')","380adc19":"from sklearn.preprocessing import StandardScaler\nloss_fn=nn.BCEWithLogitsLoss(reduction='sum')\n\ntest_dict = {}\n\ni_epoch = 1\nstart_time = time.time()\nfor i_fold in fold_list:\n    fold_start_time = time.time()\n    MODEL_PATH = get_model_path(i_seed, i_fold, i_epoch, '..\/input\/base-cased')\n    print(f'start fold {i_fold}')\n    scaler = joblib.load(os.path.join(MODEL_PATH, f'scaler-seed{0}-fold{i_fold}.joblib'))\n    test_sentence_feature_mat = scaler.transform(sentence_feature_mat)\n\n    test_loader = DynamicBucketIterator(x_test, \n                                        torch.tensor(np.concatenate([np.zeros((len(x_test), 1+len(aux_col_list))), test_sentence_feature_mat], axis=1),dtype=torch.float32).cuda(),\n                                        capacity=MAX_LEN*batch_size, pad_token=0, shuffle=False, length_quantile=1, max_batch_size=2048, for_bert=True)\n\n    print(torch.cuda.memory_allocated())\n\n    model = NeuralNet(len(aux_col_list), sentence_feature_mat.shape[-1])\n    load_path = os.path.join(MODEL_PATH, f'seed{0}-fold{i_fold}-epoch{i_epoch}.torchModelState')\n    model.load_state_dict(torch.load(load_path)['model_state_dict'])\n    model.cuda()\n\n    model.eval()\n\n    epoch_test_pred = np.zeros(len(x_test))\n    eval_start_time = time.time()\n    for batch in test_loader:\n        x_batch = batch[0]\n        segment_id_batch = batch[1]\n        input_mask_batch = batch[2]\n        y_batch = batch[3]\n        index_batch = batch[4]\n        y_true_batch = y_batch[:, :1+len(aux_col_list)]\n        sample_weight_batch = y_batch[:, 1+len(aux_col_list)]\n        sentence_feature_batch = y_batch[:, -sentence_feature_mat.shape[-1]:]\n        x_features = [torch.tensor(feature, dtype=torch.long).cuda() for feature in [x_batch, segment_id_batch, input_mask_batch]]\n    #                 print('x_features', torch.cuda.memory_allocated())\n    #                 timer.stamp(f'x_features')\n\n        y_pred = model(x_features, sentence_feature_batch)\n    #                 print('after_prediction', torch.cuda.memory_allocated())\n    #                 timer.stamp(f'after_prediction')\n        epoch_test_pred[index_batch] = sigmoid(y_pred[:, 0].detach().cpu().numpy())\n        del x_features, y_pred\n        torch.cuda.empty_cache()\n    print(f'uncased {i_fold} finishd in {time.time() - fold_start_time}', file=open('time.txt', 'a'))\n\n    test_dict[i_fold] = epoch_test_pred\n    del model, epoch_test_pred\n    gc.collect()\n    torch.cuda.empty_cache()\n    \nprint(f'uncased finishd in {time.time() - start_time}', file=open('time.txt', 'a'))\n    ","2f70d356":"test_pred_base_CASED = np.mean(list(test_dict.values()), axis=0)","4de519c6":"del test_loader, test_dict\ngc.collect()\ntorch.cuda.empty_cache()\ntorch.cuda.memory_allocated()","7f46eca9":"batch_size = 8\nn_seeds = 1\nn_splits = 10\nn_epochs = 1\n\nTRAIN_ON_N_SPLITS = 1\n\nEMBED_SIZE = 300\nSUBGROUP_NEGATIVE_WEIGHT_COEF = 1\nBACKGROUND_POSITIVE_WEIGHT_COEF = 0\n\nBERT_HIDDEN_SIZE = 1024\n\nBERT_MODEL_PATH = '..\/input\/bert-pretrained-models\/uncased_l-24_h-1024_a-16\/uncased_L-24_H-1024_A-16\/'\nBERT_DO_LOWER = 'uncased' in BERT_MODEL_PATH\n\nENSEMBLE_START_EPOCH = 0\n\nOUT_DROPOUT = 0.3\n\nMAX_LEN = 220\n    \n","881cba69":"MODEL_PATH = \"..\/input\/bert-large-5\"\nos.listdir(MODEL_PATH)","b08a2e5d":"%%time\n\nimport re\n# \u3053\u308c\u3060\u3068\u3001'\u306fembedding\u306b\u7d50\u69cb\u5165\u3063\u3066\u308b\u306e\u306b\u9664\u5916\u3055\u308c\u3061\u3083\u3046\u3002\u3000\u3088\u304f\u306a\u3044\u306e\u3067 ' \u3060\u3051\u629c\u3044\u305f\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3', \n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', \n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', \n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', \n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', '\\n', '\\r']\n\ndef clean_text(x: str) -> str:\n    for punct in puncts:\n        if punct in x:\n            x = x.replace(punct, ' {} '.format(punct))\n    return x\n\ndef clean_numbers(x):\n    return re.sub('\\d+', ' ', x)\n\nif DEBUG:\n    test = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv', nrows=DEBUG_DATA_SIZE)\nelse:\n    test = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')\n\nx_test_clean = test['comment_text'].apply(lambda x: clean_text(x)).apply(lambda x: clean_numbers(x))\ndel test\n\ngc.collect()\n\nx_test_clean.head(20)\n\ndef add_cls_sep(text):\n    return \"[CLS] \" + text + \" [SEP]\"\n\nx_test = x_test_clean.apply(lambda x: add_cls_sep(x))\n\nx_test.head(10)\n","2789f60e":"BERT_DO_LOWER","20af727d":"\ntokenizer = BertTokenizer.from_pretrained(\n    BERT_MODEL_PATH, cache_dir=None, do_lower_case=BERT_DO_LOWER)\n\ntqdm.pandas()\nx_test = x_test.progress_apply(lambda x: tokenizer.tokenize(x))\n\nx_test.head(10)","df30fdb5":"x_test = x_test.progress_apply(lambda x: tokenizer.convert_tokens_to_ids(x)[:MAX_LEN])\n\nx_test.head(10)","26dc774a":"del tokenizer\ngc.collect()","e60d3e06":"torch.cuda.empty_cache()\ntorch.cuda.memory_allocated()","2efcb41b":"with suppress_stdout():\n    convert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n        BERT_MODEL_PATH + 'bert_model.ckpt',\n    BERT_MODEL_PATH + 'bert_config.json',\n    WORK_DIR + 'pytorch_model.bin')\n\nshutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'bert_config.json')","5f74601b":"class NeuralNet(nn.Module):\n    def __init__(self, num_aux_targets, num_sentence_features):\n        super(NeuralNet, self).__init__()\n        self.bert_model = BertModel.from_pretrained(WORK_DIR)\n        self.dropout = nn.Dropout(OUT_DROPOUT)\n        \n        self.linear_sentence1 = nn.Linear(num_sentence_features, num_sentence_features)\n        \n        n_hidden = BERT_HIDDEN_SIZE + num_sentence_features\n        self.linear1 = nn.Linear(n_hidden, n_hidden)\n        \n        self.linear_out = nn.Linear(n_hidden, 1)\n        self.linear_aux_out = nn.Linear(n_hidden, num_aux_targets)\n        \n    def forward(self, x_features, sentence_features):\n        \n        _, bert_output = self.bert_model(*x_features, output_all_encoded_layers=False)\n        \n        bert_output = self.dropout(bert_output)\n        \n        h_sentence = self.linear_sentence1(sentence_features)\n        \n        h_cat = torch.cat((bert_output, h_sentence), 1)\n        \n        h_conc_linear1  = F.relu(self.linear1(h_cat))\n        \n        hidden = h_cat + h_conc_linear1\n        \n        result = self.linear_out(hidden)\n        aux_result = self.linear_aux_out(hidden)\n        out = torch.cat([result, aux_result], 1)\n        \n        return out\n        ","c5dcc1fa":"aux_col_list = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']","cd4c7a46":"sentence_feature_mat = sentence_df.drop(columns=['n_oov']).values\n","c4d44769":"gc.collect()\ntorch.cuda.empty_cache()\ntorch.cuda.memory_allocated()","52e9f95d":"from sklearn.preprocessing import StandardScaler\nloss_fn=nn.BCEWithLogitsLoss(reduction='sum')\n\ntest_array = np.zeros(len(x_test))\n\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=n_splits, shuffle=True)\nfor i_seed in range(n_seeds):\n    print(f'start seed {i_seed}')\n\n    for i_fold, (dev_index, val_index) in enumerate(kf.split(x_test)): # \u4f7f\u3063\u3066\u308b\u306e\u306f i_fold\u3060\u3051\n        if i_fold >= TRAIN_ON_N_SPLITS:\n            break\n            \n        print(f'start fold {i_fold}')\n        scaler = joblib.load(os.path.join(MODEL_PATH, f'scaler-seed{0}-fold{0}.joblib'))\n        test_sentence_feature_mat = scaler.transform(sentence_feature_mat)\n\n        test_loader = DynamicBucketIterator(x_test, \n                                            torch.tensor(np.concatenate([np.zeros((len(x_test), 1+len(aux_col_list))),\n                                                                         test_sentence_feature_mat], axis=1),dtype=torch.float32).cuda(),\n                                            capacity=MAX_LEN*batch_size, pad_token=0, shuffle=False, length_quantile=1, max_batch_size=2048, for_bert=True)\n        \n        print(torch.cuda.memory_allocated())\n        mb = master_bar(range(n_epochs))\n\n        i_epoch = 1\n        \n        model = NeuralNet(len(aux_col_list), sentence_feature_mat.shape[-1])\n        load_path = os.path.join(MODEL_PATH, f'seed{0}-fold{0}-epoch{i_epoch*4+3}.torchModelState')\n        model.load_state_dict(torch.load(load_path))\n        model.cuda()\n        print(i_epoch)\n        start_time = time.time()\n\n        model.eval()\n\n        epoch_test_pred = np.zeros(len(x_test))\n        for batch in progress_bar(test_loader):\n            x_batch = batch[0]\n            segment_id_batch = batch[1]\n            input_mask_batch = batch[2]\n            y_batch = batch[3]\n            index_batch = batch[4]\n            y_true_batch = y_batch[:, :1+len(aux_col_list)]\n            sample_weight_batch = y_batch[:, 1+len(aux_col_list)]\n            sentence_feature_batch = y_batch[:, -sentence_feature_mat.shape[-1]:]\n\n            x_features = [torch.tensor(feature, dtype=torch.long).cuda() for feature in [x_batch, segment_id_batch, input_mask_batch]]\n#                 print('x_features', torch.cuda.memory_allocated())\n#                 timer.stamp(f'x_features')\n            y_pred = model(x_features, sentence_feature_batch)\n#                 print('after_prediction', torch.cuda.memory_allocated())\n#                 timer.stamp(f'after_prediction')\n\n            epoch_test_pred[index_batch] = sigmoid(y_pred[:, 0].detach().cpu().numpy())\n            del x_features, y_pred\n            torch.cuda.empty_cache()\n\n        test_array = epoch_test_pred\n        elapsed_time = time.time() - start_time\n        del model, epoch_test_pred\n        gc.collect()\n        torch.cuda.empty_cache()\n","b68a805f":"test_pred_large_uncased = test_array","06df1de9":"plt.hist(test_pred_large_uncased, bins=20)","5d57a952":"del test_loader\ngc.collect()\ntorch.cuda.empty_cache()\ntorch.cuda.memory_allocated()\n","681dfe6f":"batch_size = 8\nn_seeds = 1\nn_splits = 10\nn_epochs = 1\n\nTRAIN_ON_N_SPLITS = 1\n\nEMBED_SIZE = 300\nSUBGROUP_NEGATIVE_WEIGHT_COEF = 1\nBACKGROUND_POSITIVE_WEIGHT_COEF = 0\n\nBERT_HIDDEN_SIZE = 1024\n\nBERT_MODEL_PATH = '..\/input\/bert-pretrained-models\/cased_l-24_h-1024_a-16\/cased_L-24_H-1024_A-16\/'\nBERT_DO_LOWER = 'uncased' in BERT_MODEL_PATH\n\nENSEMBLE_START_EPOCH = 0\n\nOUT_DROPOUT = 0.3\n\nMAX_LEN = 220\n    \n","ab819a43":"MODEL_PATH = \"..\/input\/large-cased-1\"\nos.listdir(MODEL_PATH)","37701555":"%%time\n\nimport re\n# \u3053\u308c\u3060\u3068\u3001'\u306fembedding\u306b\u7d50\u69cb\u5165\u3063\u3066\u308b\u306e\u306b\u9664\u5916\u3055\u308c\u3061\u3083\u3046\u3002\u3000\u3088\u304f\u306a\u3044\u306e\u3067 ' \u3060\u3051\u629c\u3044\u305f\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3', \n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', \n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', \n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', \n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', '\\n', '\\r']\n\ndef clean_text(x: str) -> str:\n    for punct in puncts:\n        if punct in x:\n            x = x.replace(punct, ' {} '.format(punct))\n    return x\n\ndef clean_numbers(x):\n    return re.sub('\\d+', ' ', x)\n\nif DEBUG:\n    test = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv', nrows=DEBUG_DATA_SIZE)\nelse:\n    test = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')\n\nx_test_clean = test['comment_text'].apply(lambda x: clean_text(x)).apply(lambda x: clean_numbers(x))\ndel test\n\ngc.collect()\n\nx_test_clean.head(20)\n\ndef add_cls_sep(text):\n    return \"[CLS] \" + text + \" [SEP]\"\n\nx_test = x_test_clean.apply(lambda x: add_cls_sep(x))\n\nx_test.head(10)\n","2e6fb12a":"BERT_DO_LOWER","0217b49a":"\ntokenizer = BertTokenizer.from_pretrained(\n    BERT_MODEL_PATH, cache_dir=None, do_lower_case=BERT_DO_LOWER)\n\ntqdm.pandas()\nx_test = x_test.progress_apply(lambda x: tokenizer.tokenize(x))\n\nx_test.head(10)","f975ffcd":"x_test = x_test.progress_apply(lambda x: tokenizer.convert_tokens_to_ids(x)[:MAX_LEN])\n\nx_test.head(10)","c9d9b138":"del tokenizer\ngc.collect()","c84939f4":"torch.cuda.empty_cache()\ntorch.cuda.memory_allocated()","3b0cdf55":"with suppress_stdout():\n    convert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n        BERT_MODEL_PATH + 'bert_model.ckpt',\n    BERT_MODEL_PATH + 'bert_config.json',\n    WORK_DIR + 'pytorch_model.bin')\n\nshutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'bert_config.json')","cf8d4251":"class NeuralNet(nn.Module):\n    def __init__(self, num_aux_targets, num_sentence_features):\n        super(NeuralNet, self).__init__()\n        self.bert_model = BertModel.from_pretrained(WORK_DIR)\n        self.dropout = nn.Dropout(OUT_DROPOUT)\n        \n        self.linear_sentence1 = nn.Linear(num_sentence_features, num_sentence_features)\n        \n        n_hidden = BERT_HIDDEN_SIZE + num_sentence_features\n        self.linear1 = nn.Linear(n_hidden, n_hidden)\n        \n        self.linear_out = nn.Linear(n_hidden, 1)\n        self.linear_aux_out = nn.Linear(n_hidden, num_aux_targets)\n        \n    def forward(self, x_features, sentence_features):\n        \n        _, bert_output = self.bert_model(*x_features, output_all_encoded_layers=False)\n        \n        bert_output = self.dropout(bert_output)\n        \n        h_sentence = self.linear_sentence1(sentence_features)\n        \n        h_cat = torch.cat((bert_output, h_sentence), 1)\n        \n        h_conc_linear1  = F.relu(self.linear1(h_cat))\n        \n        hidden = h_cat + h_conc_linear1\n        \n        result = self.linear_out(hidden)\n        aux_result = self.linear_aux_out(hidden)\n        out = torch.cat([result, aux_result], 1)\n        \n        return out\n        ","537a6716":"aux_col_list = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']","5b14d880":"sentence_feature_mat = sentence_df.drop(columns=['n_oov']).values\n","790ac0f4":"gc.collect()\ntorch.cuda.empty_cache()\ntorch.cuda.memory_allocated()","d801f2ef":"from sklearn.preprocessing import StandardScaler\nloss_fn=nn.BCEWithLogitsLoss(reduction='sum')\n\ntest_array = np.zeros(len(x_test))\n\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=n_splits, shuffle=True)\nfor i_seed in range(n_seeds):\n    print(f'start seed {i_seed}')\n\n    for i_fold, (dev_index, val_index) in enumerate(kf.split(x_test)): # \u4f7f\u3063\u3066\u308b\u306e\u306f i_fold\u3060\u3051\n        if i_fold >= TRAIN_ON_N_SPLITS:\n            break\n            \n        print(f'start fold {i_fold}')\n        scaler = joblib.load(os.path.join(MODEL_PATH, f'scaler-seed{0}-fold{0}.joblib'))\n        test_sentence_feature_mat = scaler.transform(sentence_feature_mat)\n\n        test_loader = DynamicBucketIterator(x_test, \n                                            torch.tensor(np.concatenate([np.zeros((len(x_test), 1+len(aux_col_list))),\n                                                                         test_sentence_feature_mat], axis=1),dtype=torch.float32).cuda(),\n                                            capacity=MAX_LEN*batch_size, pad_token=0, shuffle=False, length_quantile=1, max_batch_size=2048, for_bert=True)\n        \n        print(torch.cuda.memory_allocated())\n        mb = master_bar(range(n_epochs))\n\n        i_epoch = 1\n        \n        model = NeuralNet(len(aux_col_list), sentence_feature_mat.shape[-1])\n        load_path = os.path.join(MODEL_PATH, f'seed{0}-fold{0}-epoch{i_epoch*4+3}.torchModelState')\n        model.load_state_dict(torch.load(load_path))\n        model.cuda()\n        print(i_epoch)\n        start_time = time.time()\n\n        model.eval()\n\n        epoch_test_pred = np.zeros(len(x_test))\n        for batch in progress_bar(test_loader):\n            x_batch = batch[0]\n            segment_id_batch = batch[1]\n            input_mask_batch = batch[2]\n            y_batch = batch[3]\n            index_batch = batch[4]\n            y_true_batch = y_batch[:, :1+len(aux_col_list)]\n            sample_weight_batch = y_batch[:, 1+len(aux_col_list)]\n            sentence_feature_batch = y_batch[:, -sentence_feature_mat.shape[-1]:]\n\n            x_features = [torch.tensor(feature, dtype=torch.long).cuda() for feature in [x_batch, segment_id_batch, input_mask_batch]]\n#                 print('x_features', torch.cuda.memory_allocated())\n#                 timer.stamp(f'x_features')\n            y_pred = model(x_features, sentence_feature_batch)\n#                 print('after_prediction', torch.cuda.memory_allocated())\n#                 timer.stamp(f'after_prediction')\n\n            epoch_test_pred[index_batch] = sigmoid(y_pred[:, 0].detach().cpu().numpy())\n            del x_features, y_pred\n            torch.cuda.empty_cache()\n\n        test_array = epoch_test_pred\n        elapsed_time = time.time() - start_time\n        del model, epoch_test_pred\n        gc.collect()\n        torch.cuda.empty_cache()\n","c6241769":"test_pred_large_CASED = test_array","bf7beaea":"plt.hist(test_pred_large_CASED, bins=20)","f2b90566":"pred_list = [test_pred_rnn,\n             test_pred_large_uncased, test_pred_large_CASED,\n             test_pred_base_uncased_ftp13,\n             test_pred_base_uncased, test_pred_base_CASED,\n             test_pred_gpt2_max]\n\ncorr_mat = np.corrcoef(pred_list)\nprint(corr_mat)\n\nimport seaborn as sns\nsns.heatmap(corr_mat, cmap='viridis')\n\ntest_pred = np.average(pred_list,\n                       weights=[0.5, 2, 2, 1, 1.5, 1.5, 0.5], axis=0)\n","6cb9b526":"plt.hist(test_pred, bins=20)","a6a9ff08":"df_submit = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/sample_submission.csv')\ndf_submit.prediction = test_pred\ndf_submit.to_csv('submission.csv', index=False)","d2c2be76":"# BERT Large CASED f0","2213b2d7":"# GPT-2 max","64c554b2":"# base uncased f1, 2","a098eb1b":"# base CASED f4, 5","6e0f1935":"# Large uncased f0e1","3f140b87":"# LSTM-GRU bs-256 crawl ROOV TEST","21fe473d":"- lstm-gru with predicted ids by lstm-gru\n- gpt2 max\n- base uncased f1, 2\n- base CASED f4, 5\n- large uncased 5 f0\n- large CASED 2 f1\n\n\u3081\u3063\u3061\u3083\u901f\u304f\u306a\u3063\u3066\u308b\uff1f\u3082\u3063\u304b\u3044\u6295\u3052\u308b\n\n\u306a\u3093\u304b\u901f\u304f\u306a\u3063\u3066\u308b\u304b\u3089\u3082\u3046\u4e00\u500b\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3059\u308b","26060569":"## Reduce OOV","71264d2e":"## Sentence features"}}