{"cell_type":{"d686a2d4":"code","0edb28af":"code","a89f0932":"code","8b07f27e":"code","2cfb7ce0":"code","914e2d87":"code","fda84aba":"code","fbec2f15":"code","79f77646":"code","8f36881c":"code","4cce9702":"code","57a7dd72":"code","5fdf2e2d":"code","ee510038":"code","0625c9f2":"code","b60c2bd2":"code","718b5c8f":"code","fdaa2b96":"code","e9aaab70":"code","69f366c0":"code","b2c00bf3":"code","b713306e":"code","4b38a62e":"code","4fb59d67":"code","b3cfd49a":"code","8c24db56":"code","f1000b2e":"code","9dfd6c8b":"code","47e35e34":"code","ebcafc5b":"code","44317f36":"code","71554eed":"code","359372d8":"code","de94f29f":"markdown","c39be0dc":"markdown","d5255731":"markdown","d5f338d2":"markdown","141b3fdc":"markdown","3b96bc70":"markdown","0d288ef6":"markdown","2ac95913":"markdown","5469b978":"markdown","4ce67908":"markdown"},"source":{"d686a2d4":"import pandas as pd\nimport numpy as np","0edb28af":"pwd","a89f0932":"data1=pd.read_csv('..\/input\/bcd-real\/data (1).csv')","8b07f27e":"data1.head()","2cfb7ce0":"data1.shape\ndata1.dtypes\ndata1.info()\ndata1.drop(['Unnamed: 32','id'],axis=1,inplace=True)\ndata1.diagnosis=[1 if each==\"M\" else 0 for each in data1.diagnosis]\ny=data1.loc[:,\"diagnosis\"]\ny\nX=data1.loc[:,data1.columns!=\"diagnosis\"]\nX\ntype(X)\ntype(y)\nprint(X.shape,y.shape)","914e2d87":"from sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n%matplotlib inline\nX_train,X_test,y_train,y_test=train_test_split(X,y, test_size = 0.297, random_state = 42)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","fda84aba":"X_train.shape[1]\ny_train.value_counts()\ny_train.value_counts()\nprint(y_train.value_counts()\/X_train.shape[0])\ndata1['diagnosis'].value_counts() \/len(data1)*100\nprint(y_test.value_counts()\/X_test.shape[0])\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')","fbec2f15":"from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=2)\nkmeans.fit(X_train, y_train)","79f77646":"train_predictions = kmeans.predict(X_train)\ntest_predictions = kmeans.predict(X_test)","8f36881c":"#Train data accuracy\nfrom sklearn.metrics import accuracy_score,f1_score\nfrom sklearn.metrics import confusion_matrix\n\nprint(\"Train-Data Confusion Matrix: \\n\", confusion_matrix(y_train, train_predictions))\nprint(\"\\nTrain-Data Accuracy\",accuracy_score(y_train,train_predictions))\nprint(\"\\nTrain data f1-score for class '1'\",f1_score(y_train,train_predictions,pos_label=1))\nprint(\"\\nTrain data f1-score for class '2'\",f1_score(y_train,train_predictions,pos_label=0))\n\n#Test data accuracy\nprint(\"\\n\\n \\n\\n\")\n\nprint(\"Test-Data Confusion Matrix: \\n\", confusion_matrix(y_test, test_predictions))\nprint(\"\\nTest-Data Accuracy\",accuracy_score(y_test,test_predictions))\nprint(\"\\nTest data f1-score for class '1'\",f1_score(y_test,test_predictions,pos_label=1))\nprint(\"\\nTest data f1-score for class '2'\",f1_score(y_test,test_predictions,pos_label=0))","4cce9702":"from sklearn.neighbors import KNeighborsClassifier ","57a7dd72":"knn = KNeighborsClassifier(n_neighbors=7)\nknn.fit(X_train, y_train)\ntrain_predictions = knn.predict(X_train)\ntest_predictions = knn.predict(X_test)","5fdf2e2d":"#Train data accuracy\n\nprint(\"Train-Data Confusion Matrix: \\n\", confusion_matrix(y_train, train_predictions))\nprint(\"\\nTrain-Data Accuracy\",accuracy_score(y_train,train_predictions))\nprint(\"\\nTrain data f1-score for class '1'\",f1_score(y_train,train_predictions,pos_label=1))\nprint(\"\\nTrain data f1-score for class '2'\",f1_score(y_train,train_predictions,pos_label=0))\n\n#Test data accuracy\nprint(\"\\n\\n \\n\\n\")\n\nprint(\"Test-Data Confusion Matrix: \\n\", confusion_matrix(y_test, test_predictions))\nprint(\"\\nTest-Data Accuracy\",accuracy_score(y_test,test_predictions))\nprint(\"\\nTest data f1-score for class '1'\",f1_score(y_test,test_predictions,pos_label=1))\nprint(\"\\nTest data f1-score for class '2'\",f1_score(y_test,test_predictions,pos_label=0))","ee510038":"# Load the data set\ndf = pd.read_csv('..\/input\/bcd-real\/data (1).csv')","0625c9f2":"df.head()\n# drop ID - we don't need that\ndf.drop('id',axis=1,inplace=True)\n# there's an additional column 'unnamed' with no data. delete as well\ndf.drop('Unnamed: 32',axis=1,inplace=True)\ndf.describe().T\n","b60c2bd2":"# check for missing data\nall_data_na = (df.isnull().sum() \/ len(df)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","718b5c8f":"from scipy.stats import skew \n\n# find skewed features\nnumeric_feats = df.dtypes[df.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = df[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","fdaa2b96":"# \"unskew\" the features\nskewness = skewness[abs(skewness) > 0.75]\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    df[feat] = boxcox1p(df[feat], lam)","e9aaab70":"# Replace the target values (M = malignant, B = benign) with 1 for malignant and 0 for begnin tumors\ndf['diagnosis']= df['diagnosis'].map({'M':1,'B':0})","69f366c0":"# split into target (y) and features (X)\ny = df['diagnosis']\nX = df.drop('diagnosis',axis=1)","b2c00bf3":"# Scale the feature values\nfrom sklearn.preprocessing import StandardScaler\nX = StandardScaler().fit_transform(X)","b713306e":"# Split into train and test data sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.297, random_state = 42)\nprint(len(y_train))","4b38a62e":"# scoring function for the model\n# we score with the mean AUC from 5 folds of the training data\nfrom sklearn.model_selection import KFold, cross_val_score\ndef score_model(model):\n    kf = KFold(5, shuffle=True, random_state=42).get_n_splits(X_train)\n    model_score = np.mean(cross_val_score(model, X_train, y_train, scoring=\"recall\", cv = kf))\n    return((type(model).__name__,model_score))","4fb59d67":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import SGDClassifier\n\nmodel_scores = pd.DataFrame(columns=['model','Recall score'])\nclfs = [KNeighborsClassifier(), SGDClassifier(), GradientBoostingClassifier(),LGBMClassifier(),MLPClassifier(), SVC(), GaussianProcessClassifier(), DecisionTreeClassifier()\n        , RandomForestClassifier(), AdaBoostClassifier(), GaussianNB(), QuadraticDiscriminantAnalysis(), XGBClassifier()]\nfor clf in clfs:\n    sc = score_model(clf)\n    model_scores = model_scores.append({'model':sc[0],'Recall score':sc[1]},ignore_index=True)","b3cfd49a":"model_scores.sort_values('Recall score',ascending=False)","8c24db56":"# SVC\nfrom sklearn.model_selection import GridSearchCV\n\nparameter_space = {\n    'C': np.logspace(-2, 10, 20,base=2),\n    'gamma': np.logspace(-9, 3, 13),\n}\ngrid_search = GridSearchCV(SVC(kernel='rbf'), parameter_space, n_jobs=-1, cv=5,scoring=\"recall\")\ngrid_search.fit(X_train, y_train)\ngrid_search.best_params_\n","f1000b2e":"# MLP\nparameter_space = {\n    'hidden_layer_sizes': [(20,20), (100,), (50,), (30,)],\n    'activation': ['tanh', 'relu'],\n    'solver': ['sgd', 'adam'],\n    'alpha': [0.0001, 0.05],\n    'learning_rate': ['constant','adaptive'],\n}\ngrid_search = GridSearchCV(MLPClassifier(), parameter_space, n_jobs=-1, cv=5,scoring=\"recall\")\ngrid_search.fit(X_train, y_train)\ngrid_search.best_params_","9dfd6c8b":"model_scores = pd.DataFrame(columns=['model','Recall score'])\nclfs = [SVC(kernel=\"rbf\", gamma=0.001, C=47),MLPClassifier(max_iter=300,activation='tanh',hidden_layer_sizes=(30,),alpha=0.05, learning_rate=\"adaptive\",solver=\"sgd\")]\nfor clf in clfs:\n    sc = score_model(clf)\n    model_scores = model_scores.append({'model':sc[0],'Recall score':sc[1]},ignore_index=True) ","47e35e34":"model_scores","ebcafc5b":"# train and then validate with the test data set\nfrom sklearn.metrics import recall_score\n\nclf = SVC(kernel=\"rbf\", gamma=0.001, C=47.8)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint(\"Test Recall: {}\".format(recall_score(y_test, y_pred)))\nprint(len(y_pred))","44317f36":"# calculate the confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncnf_matrix = confusion_matrix(y_test,y_pred)\nprint(cnf_matrix)","71554eed":"# Confusion matrix with Seaborn\nimport seaborn as sns\nsns.set()\n\nclass_names = [0,1]\nfig,ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks,class_names)\nplt.yticks(tick_marks,class_names)\n\n#create a heat map\nsns.heatmap(pd.DataFrame(cnf_matrix), annot = True, cmap = 'YlGnBu',\n           fmt = 'g')\nax.xaxis.set_label_position('top')\nplt.tight_layout()\nplt.title('Confusion matrix for Classifier+', y = 1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.show()","359372d8":"from sklearn.metrics import precision_score\nfrom sklearn.metrics import accuracy_score\nprint(\"Precision: {}\".format(precision_score(y_test,y_pred)))\nprint(\"Accuracy: {}\".format(accuracy_score(y_test,y_pred)))","de94f29f":"<a id=\"section-four\"><\/a>\n# Classifier+","c39be0dc":"<a id=\"section-three\"><\/a>\n# KNN (k-nearest neighbor)\n","d5255731":"> # **Hyperparameter Tuning**","d5f338d2":"> # **Splitting Train & Test Data**","141b3fdc":"<a id=\"section-two\"><\/a>\n# K-means\n","3b96bc70":"> # **Final Model**","0d288ef6":"Credits to @Almatrafi","2ac95913":"<a id=\"section-one\"><\/a>\n# Data Wrangling\n\n","5469b978":"# Breast Cancer Dataset Analysis (Classification Models)\n\nDetermine which features of data (measurements) are most important for diagnosing breast cancer.\n\nTest and evaluate the performance of two different classification models:\n\u2022\tKNN (k-nearest neighbor)\n\u2022\tK-means\n\nAnd implementing new advanced classifier method to maximize the accuracy of the algorithm.\n\nImprove the quality of life and reduce the cancer related fatalities\n\n\n* [Data Wrangling](#section-one)\n* [K-Means](#section-two)\n* [KNN (k-nearest neighbor)](#section-three)\n* [Classifier+](#section-four)","4ce67908":"> # **Training Models**"}}