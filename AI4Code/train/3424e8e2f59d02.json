{"cell_type":{"02807c75":"code","015fc3d4":"code","8db07e9f":"code","39d740b2":"code","98724f34":"code","4327abc8":"code","cc3844af":"code","c2a51760":"code","cd1e4eac":"code","0a6366b3":"code","4dab18e3":"code","73ea3420":"code","4074323c":"code","ba5ffb1f":"markdown","61659a95":"markdown","7ac78725":"markdown","ef5451a6":"markdown","7a23e009":"markdown","69018900":"markdown","0e3e9992":"markdown","d294eb5c":"markdown","ecb20ef8":"markdown","69ef427f":"markdown","b11a0dfe":"markdown","bc0f78c6":"markdown","ab3deb33":"markdown","859b1f17":"markdown","18e1ff4c":"markdown","d4b7688c":"markdown","f300ec23":"markdown","4100b487":"markdown"},"source":{"02807c75":"# In this lesson we will explore the train_test_split module\n# Therefore we need no more than the module itself and NumPy\nimport numpy as np\nfrom sklearn.model_selection import train_test_split","015fc3d4":"a = np.arange(1,101)","8db07e9f":"a","39d740b2":"b = np.arange(501,601)\nb","98724f34":"train_test_split(a, shuffle = False)\n\n# when we give \"shuffle = False\" then there will be no shuffling and data is split in the ratio,\n# by default it takes 75:25 which gives us total 100","4327abc8":"# Let's check out how this works, by default shuffle=True\ntrain_test_split(a)\n\n#Here we haven't specified the \"random_state = integer\" as a result if u run the code cell again and again \n#then the output will change and the integers will change it's position everytime(try it urself). so u need to speciy a \n#random_state which can be any integer(1,2,3, or anyyy..), so everytime it will be the same order.\n#while writing and working with our code we will be running the code cell many times, so we need to specify random_state","cc3844af":"a_train, a_test, b_train, b_test = train_test_split(a, b, test_size=0.2, random_state=20)","c2a51760":"# Let's check the shapes\n# Basically, we are checking how does the 'test_size' work\na_train.shape, a_test.shape","cd1e4eac":"a_train","0a6366b3":"a_test","4dab18e3":"b_train.shape, b_test.shape","73ea3420":"b_train","4074323c":"b_test","ba5ffb1f":"Let's generate a new data frame 'a' which will contain all integers from 1 to 100\nThe method np.arange works like the built-in method 'range' with the difference it \ncreates an array","61659a95":"## closely observe the above image\n\n**1.  The 1st element in a_train , b_train are from the same position of a & b i.e 38, 538 are positioned at 37th place when indexing starting from 0 in both a & b.**\n\n**2.  The same logic follows for both the train and test and all elemnts follow the same pattern as the random_state is same they follow the same random shuffle.**\n\n**3.  So we can get the same results everytime any no. of times we may run the code.**","7ac78725":"## Split the data\nFull documentation: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html","ef5451a6":"## I hope most of the doubts have been cleared for u. if u still have any feel free to comment below so that make a notebook to clear them.","7a23e009":" ## 1.numpy.arange(start, stop, step, dtype=None) Return evenly spaced values within a given interval.\n\nValues are generated within the half-open interval [start, stop) (in other words, the interval including start but excluding stop). For integer arguments the function is equivalent to the Python built-in range function, but returns an ndarray rather than a list.","69018900":"For better understanding of current notebook for beginners go through the links:\n\n [1.1 Data Preprocessing](http:\/\/www.kaggle.com\/saikrishna20\/data-preprocessing-tools)\n\n\n[1.2 Simple linear Regression](https:\/\/www.kaggle.com\/saikrishna20\/1-2-simple-linear-regression) \n\n\n[1.3 Multiple linear Regression with Backward Elimination](http:\/\/www.kaggle.com\/saikrishna20\/1-3-multiple-linear-regression-backward-eliminat)\n\n[1.4 Polynomial Linear Regression](https:\/\/www.kaggle.com\/saikrishna20\/1-4-polynomial-linear-regression)\n\n[1.5 Support Vector Regression (SVR)](https:\/\/www.kaggle.com\/saikrishna20\/1-5-support-vector-regression-svr\/edit\/run\/37240657)\n\n[1.6 Decision Tree Regressor](https:\/\/www.kaggle.com\/saikrishna20\/1-6-decision-tree-regression)\n\n[1.7 Random Forest Regression](https:\/\/www.kaggle.com\/saikrishna20\/1-7-random-forest-regression)\n\n[1.8 Regression Model Selection](https:\/\/www.kaggle.com\/saikrishna20\/1-8-regression-model-selection)\n\nIt basically tells u about the preprocessing & different models of Regression which will help u in understanding this notebook better","0e3e9992":"Similarly, let's create another ndarray 'b', which will contain integers from 501 to 600\nWe have intentionally picked these numbers so we can easily compare the two\nObviously, the difference between the elements of the two arrays is 500 for\nany two corresponding elements","d294eb5c":"In the graph below:\nDots are the real values\/ target \/ y\n\nyellow line is the predicted model.\n\nBlue line - the line\/ model which the data actually follows","ecb20ef8":"![Screenshot%20%2872%29.png](attachment:Screenshot%20%2872%29.png)","69ef427f":"# Like this notebook then upvote it.\n# Need to improve it then comment below.\n# Enjoy Machine Learning","b11a0dfe":"## Generate some random data we are going to split","bc0f78c6":"# underfitting & overfitting\n\n**These 2 concepts are inter-related to each other, Ok broadly speaking** \n\n## 1. Over fitting means our regression has focused on the particular data set so much it has missed the point.\n\n## 2. Under fitting on the other hand means the model has not captured the underlying logic of the data. It doesn't know what to do    and therefore provides an answer that is far from correct.\n\n**let's explain this with graphs as it is both more intuitive and cooler.**","ab3deb33":"**There are several different arguments we can set when we employ this method\nMost often, we have inputs and targets, so we have to split 2 different arrays\nwe are simulating this situation by splitting 'a' and 'b'**\n\n**You can specify the 'test_size' or the 'train_size' (but the latter is deprecated and will be removed)\nessentially the two have the same meaning \nCommon splits are 75-25, 80-20, 85-15, 90-10**\n\n**Finally, you should always employ a 'random_state'\nIn this way you ensure that when you are splitting the data you will always get the SAME random shuffle**\n\n**Note 2 arrays will be split into 4\nThe order is train1, test1, train2, test2 \nIt is very useful to store them in 4 variables, so we can later use them**","859b1f17":"# 1.9 Train_Test_Split\n\n# what's underfitting & overfitting ?\n\n# what happens when we do train_test_split ?\n\n# why do we do train_test_split ?\n\n**keep going and u will find out in detail**","18e1ff4c":"## Explore the result","d4b7688c":"![Capture001.JPG](attachment:Capture001.JPG)","f300ec23":"**1. We can certainly say a linear model would be an under fitting model if It provides an answer but does not capture the underlying logic of the data. It doesn't have strong predictive power, Under fitted models are clumsy and have a low accuracy. You will quickly realize that either there are no relationships to be found or you need a different model.**\n\n**2. Here we can see several data points a good algorithm would result in a model that looks like this and is not perfect but it's very close to the actual relationship.**\n\n**3. We can say overfitting refers to models that are so super good at modeling the data that they fit or at least come very near each observation. The problem is that the random noise is captured inside an overfunded model.**\n\n\n## Now under fitting is easy to spot.You have almost no accuracy whatsoever.\n\n## Overfitting is much harder though as the accuracy of the models seems outstanding.\n\n## There is one popular solution to overfitting though we can split our initial data set into two training and test splits like 90 percent training and 10 percent test or 80, 20.\n\n# It works like this:\n## We create the regression on the training data. After we have the coefficients we test the model on the test data by assessing the accuracy. The whole point is that the model has never seen the test data set. Therefore it can not over fit on it.","4100b487":"## Import the relevant libraries"}}