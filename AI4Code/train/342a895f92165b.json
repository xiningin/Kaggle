{"cell_type":{"aeac2752":"code","ea90d635":"code","8d3bec2f":"code","faefef59":"code","06b330c3":"code","ab122a84":"code","6478eac3":"code","f05da574":"code","1ed7713c":"code","ba2611f1":"code","135ea591":"code","4a557a7f":"code","3bb5b93e":"code","26c84bf1":"code","82612ef2":"code","66357a43":"code","c4669806":"code","2c94892e":"code","08f27fa2":"markdown","202f312e":"markdown","a06e2ff5":"markdown","45028cb4":"markdown","1ef8642b":"markdown","b85109e2":"markdown","d5a62485":"markdown","fc580101":"markdown","e17d8724":"markdown","cfba3648":"markdown","5940a002":"markdown","ad66cd2e":"markdown","d3be5ea2":"markdown","4af0e6b1":"markdown","355fe5fc":"markdown","b173c2ce":"markdown","5c30bfa1":"markdown","db8af828":"markdown","9ca91845":"markdown"},"source":{"aeac2752":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n# Import the libraries we'll use below.\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport pandas as pd\nimport seaborn as sns  # for nicer plots\nsns.set(style=\"darkgrid\")  # default style\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import metrics\nfrom keras import regularizers","ea90d635":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","8d3bec2f":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","faefef59":"def generateBaselineOutputNobodySurvives(df):\n    #  The baseline is that Nobody survived the tragedy Titanic.\n    #  1502 out of 2224 people died in the tragic event.\n    #  We choose 68% of the people died\n    output = pd.DataFrame({'PassengerId': df.PassengerId, 'Survived': np.full((len(df)), 0)})\n    return output\n\noutput_train = generateBaselineOutputNobodySurvives(train_data)\noutput_test = generateBaselineOutputNobodySurvives(test_data)","06b330c3":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import log_loss\nprint(\"MSE baseline: \", mean_squared_error(train_data[\"Survived\"], output_train[\"Survived\"]))\nprint(\"LogLoss baseline\", log_loss(train_data[\"Survived\"], output_train[\"Survived\"]))","ab122a84":"output_test.to_csv('baseline_submission.csv', index=False)\nprint(\"Your baseline submission was successfully saved!\")","6478eac3":"print(\"Correlation factors:\")\nprint(train_data.corr(method ='pearson')['Survived'])","f05da574":"###  Data Preprocessing\ntrain_data_copy = train_data.copy()\ntest_data_copy = test_data.copy()\n\n###  Data Preprocessing\n\n##  Remove all the NaN values from all columns\n# Change NaN in Ages column to the median values\nmedian_age = train_data[\"Age\"].median()\ntrain_data[\"Age\"] = train_data[\"Age\"].replace(np.nan, median_age)\n#  Change NaN in the Embarked column `Southampton` or `S` which is the heighest in frequency.\ntrain_data[\"Embarked\"] = train_data[\"Embarked\"].replace(np.nan, 'S')\n\n##  Drop Unnecessary tables with bad co-relation\n#  Drop Name table as it is not co-related with one's survival.\n#  Drop the Cabin column beacause it is very sparse.\n#  Drop the ticket number table, since its numeric value does not really mean anything.\nfor feature in ['PassengerId', 'Name','Cabin', 'Ticket']:\n    train_data.drop(feature, axis=1, inplace=True)\n\n\n##  Apply zscore to the Fare, Pclass, SibSp, Parch and Age column column\nfrom scipy import stats\nfor feature in ['Fare', 'Pclass', 'SibSp', 'Parch', 'Age']:\n    train_data[feature] = stats.zscore(train_data[feature])\n\n##  Apply one-hot encoding for `Sex` and `Embarked` feature\nfor feature in ['Sex', 'Embarked']:\n    one_hot = pd.get_dummies(train_data[feature])\n    train_data = train_data.drop(feature,axis = 1)\n    train_data = train_data.join(one_hot)","1ed7713c":"print(\"Correlations after transformation and one-hot:\")\nprint(train_data.corr(method ='pearson')['Survived'])","ba2611f1":"#  Separiting X_train and Y_train\nY_train = pd.DataFrame(train_data[\"Survived\"]).to_numpy().flatten()\ntrain_data.drop(\"Survived\", axis=1, inplace=True)\nX_train = train_data.to_numpy()","135ea591":"##  Helper Plot function for plots\ndef plot_history(history):\n  plt.ylabel('Loss')\n  plt.xlabel('Epoch')\n  plt.xticks(range(0, len(history['loss'] + 1)))\n  plt.plot(history['loss'], label=\"training\", marker='o')\n  plt.plot(history['val_loss'], label=\"validation\", marker='o')\n  plt.legend()\n  plt.show()","4a557a7f":"##  Function to Build the model\ndef build_model(input_shape, learning_rate=0.01):\n    \"\"\"Build a TF logistic regression model using Keras.\n\n    Args:\n    input_shape: The shape of the model's input. \n    learning_rate: The desired learning rate for SGD.\n\n    Returns:\n    model: A tf.keras model (graph).\n    \"\"\"\n    # This is not strictly necessary, but each time you build a model, TF adds\n    # new nodes (rather than overwriting), so the colab session can end up\n    # storing lots of copies of the graph when you only care about the most\n    # recent. Also, as there is some randomness built into training with SGD,\n    # setting a random seed ensures that results are the same on each identical\n    # training run.\n    tf.keras.backend.clear_session()\n    np.random.seed(0)\n    tf.compat.v1.set_random_seed(0)\n\n    # Build a model using keras.Sequential. While this is intended for neural\n    # networks (which may have multiple layers), we want just a single layer for\n    # logistic regression.\n    model = keras.Sequential()\n\n    # Keras layers can do pre-processing.\n    model.add(keras.layers.Flatten(input_shape=input_shape))\n\n    model.add(keras.layers.Dense(\n      units=512,                   # number of units\/neurons\n      use_bias=True,               # use a bias param\n      activation=\"relu\",          # apply the relu function \n    ))\n    \n    model.add(keras.layers.Dense(\n      units=256,                   # number of units\/neurons\n      use_bias=True,               # use a bias param\n      activation=\"relu\",            # apply the relu function\n    ))\n    \n    model.add(keras.layers.Dense(\n      units=128,                   # number of units\/neurons\n      use_bias=True,               # use a bias param\n      activation=\"relu\",            # apply the relu function\n    ))\n    # This layer constructs the linear set of parameters for each input feature\n    # (as well as a bias), and applies a sigmoid to the result. The result is\n    # binary logistic regression.\n    model.add(keras.layers.Dense(\n      units=1,                     # output dim (for binary classification)\n      use_bias=True,               # use a bias param\n      activation=\"sigmoid\"         # apply the sigmoid function!\n    ))\n\n    # Finally, we compile the model. This finalizes the graph for training.\n    # We specify the binary_crossentropy loss (equivalent to log loss).\n    # Notice that we are including 'binary accuracy' as one of the metrics that we\n    # ask Tensorflow to report when evaluating the model.\n    model.compile(loss='binary_crossentropy', \n                optimizer='adam', \n                metrics=[metrics.binary_accuracy])\n\n    return model","3bb5b93e":"##  Train the model\nmodel = build_model(input_shape=X_train[0].shape, learning_rate=0.01)\n\n# Fit the model.\nhistory = model.fit(\n  x = X_train,   # our binary training examples\n  y = Y_train,   # corresponding binary labels\n  epochs=5,             # number of passes through the training data\n  batch_size=64,        # mini-batch size for SGD\n  validation_split=0.1, # use a fraction of the examples for validation\n  verbose=1             # display some progress output during training\n  )\n\nhistory = pd.DataFrame(history.history)\ndisplay(history)\nplot_history(history)","26c84bf1":"from sklearn.metrics import confusion_matrix\ntrain_predictions = model.predict(X_train).flatten()\n\nthresholds = [0.3,0.49, 0.5, 0.51, 0.52, 0.7]\n\ngroup_names = [\"True Neg\", \"False Pos\", \"False Neg\" , \"True Pos\"]\n\nfor threshold in thresholds:\n    train_predictions_copy = np.copy(train_predictions)\n    train_predictions_copy[train_predictions < threshold] = 0.0\n    train_predictions_copy[train_predictions >= threshold] = 1.0\n    \n    cf_matrix = confusion_matrix(Y_train, train_predictions_copy)\n    tn, fp, fn, tp = cf_matrix.ravel()\n    \n    group_counts = [\"{0:0.0f}\".format(value) for value in\n                cf_matrix.flatten()]\n    group_percentages = [\"{0:.2%}\".format(value) for value in\n                     cf_matrix.flatten()\/np.sum(cf_matrix)]\n    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n\n    plt.figure()\n    plt.title(\n      \"Threshold : \" + str(threshold) + \"\\n\"\n      + \"Accuracy : \" + str((tp+tn)\/(tp+tn+fp+fn)) + \"\\n\"\n      + \"Precision : \" + str((tp)\/(tp+fp)) + \"\\n\"\n      + \"Recall : \" + str((tp)\/(tp+fn)))\n    sns.heatmap(cf_matrix, annot=labels, fmt=\"\", cmap='Blues')","82612ef2":"#  Trying to find outliers\npd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\ntrain_data_copy['Prediction'] = train_predictions\ntrain_data_copy['Difference'] = abs(train_data_copy['Survived'] - train_data_copy['Prediction'])\n\ntrain_data_copy = train_data_copy[['Difference', 'Survived','Prediction', 'Pclass', 'Sex', 'Age', 'Fare', 'SibSp', 'Parch', 'Embarked']]\ntrain_data_copy = train_data_copy.sort_values('Difference')\ntrain_data_copy.head()","66357a43":"##  Evaluate the model\n\n###  Data Preprocessing\ntest_data_copy = test_data.copy()\n##  Drop Unnecessary tables with bad co-relation\n#  Drop Name table as it is not co-related with one's survival.\n#  Drop the Cabin column beacause it is very sparse.\n#  Drop the ticket number table, since its numeric value does not really mean anything.\nfor feature in ['PassengerId', 'Name','Cabin', 'Ticket']:\n    test_data.drop(feature, axis=1, inplace=True)\n\n##  Remove all the NaN values from all columns\n# Change NaN in Ages column to the median values\ntest_data[\"Age\"] = test_data[\"Age\"].replace(np.nan, median_age)\n#  Change NaN in the Embarked column `Southampton` or `S` which is the heighest in frequency.\ntest_data[\"Embarked\"] = test_data[\"Embarked\"].replace(np.nan, 'S')\ntest_data[\"Fare\"] = test_data[\"Fare\"].replace(np.nan, train_data_copy[\"Fare\"].median())\n\n##  Apply zscore to the Fare, Pclass, SibSp, Parch and Age column column\nfrom scipy import stats\nfor feature in ['Fare', 'Pclass', 'SibSp', 'Parch', 'Age']:\n    test_data[feature] = stats.zscore(test_data[feature])\n\n##  Apply one-hot encoding for `Sex` and `Embarked` feature\nfor feature in ['Sex', 'Embarked']:\n    one_hot = pd.get_dummies(test_data[feature])\n    test_data = test_data.drop(feature,axis = 1)\n    test_data = test_data.join(one_hot)\n    \nX_test = test_data.to_numpy()\ntest_data.head()","c4669806":"test_data_copy.head()","2c94892e":"test_predictions = model.predict(X_test).flatten()\ntest_predictions_actual_values = np.array([1 if val >= 0.51 else 0 for val in test_predictions])\n\noutput_test_logit = pd.DataFrame({'PassengerId': test_data_copy.PassengerId, 'Survived': test_predictions_actual_values})\n\noutput_test_logit.to_csv('logit_submission.csv', index=False)\nprint(\"Your logit submission was successfully saved!\")\n","08f27fa2":"2. **Error:**Most of the passengers with the highest fares survied. But our model predicted that they died. \n* **Reason:** This could be because their fare is an `outlier`.\nMean Fare: 32.2\nStandard Deviation:49.7\nNumber of SDs: Their fare (512.3292) is 9.7 ((512.3292 - 32.2) \/ 49.7) SDs away from the mean. So, this is clearly and outlier.\n\n|Pid| Difference|\tSurvived|\tPrediction|\tPclass|\tSex|\tAge|\tFare|\tSibSp|\tParch|\tEmbarked|\n|-------|--------|---|----------|----|------|--------|--------|--|---|---|\n|258|\t0.868960|\t1|\t0.131040|\t1|\tfemale|\t35.00|\t512.3292|\t0|\t0|\tC|\n|679|\t0.880555|\t1|\t0.119445|\t1|\tmale|\t36.00|\t512.3292|\t0|\t1|\tC|\n|737|\t0.622958|\t1|\t0.377042|\t1|\tmale|\t35.00|\t512.3292|\t0|\t0|\tC****|","202f312e":"# Conclusion:\nThis simple logistic regression model gave us an accuracy score of 0.78468 while our baseline was 0.62679. So, our model did a pretty good job.","a06e2ff5":"# Setup","45028cb4":"# Variable Notes\n**pclass**: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\n**age**: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n**sibsp**: The dataset defines family relations in this way...\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n**parch**: The dataset defines family relations in this way...\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them.","1ef8642b":"# Peekaboo\nWe now would see how our Train data looks like. We can see we have 12 columns:","b85109e2":"# Attempted Models & Experiments\n1. Random Decision Forest: It gave us an accuracy socre of 71% we tried tuning all our hyper parameters but could not get it higher.\n\n```python\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n```\n\n2. Unnormalised Logistic Regression: Since, Random decision forest was not improving we tried change our model to a logit function and carried Logistic regression moving forward. Without any feature transformation we were able to get about 73% accuracy.\n\n```python\nfor feature in ['PassengerId', 'Name','Cabin', 'Ticket']:\n    train_data.drop(feature, axis=1, inplace=True)\n\n#Remove all the NaN values from all columns\n#Change NaN in Ages column to the median values\nmedian_age = train_data[\"Age\"].median()\ntrain_data[\"Age\"] = train_data[\"Age\"].replace(np.nan, median_age)\n#Change NaN in the Embarked column `Southampton` or `S` which is the heighest in frequency.\ntrain_data[\"Embarked\"] = train_data[\"Embarked\"].replace(np.nan, 'S')\n```\n\n3. Normalised and One-Hot: Then we normalised the features like `'Fare', 'SibSp', 'Parch', 'Age'` and applied One-Hot encoding on `Sex, Embarked and Pclass`. We then received the accuracy of 77.5%\n\n```python\nfrom scipy import stats\nfor feature in ['Fare', 'SibSp', 'Parch', 'Age']:\n    train_data[feature] = stats.zscore(train_data[feature])\n\n##  Apply one-hot encoding for `Sex` and `Embarked` feature\nfor feature in ['Sex', 'Embarked', 'Pclass']:\n    one_hot = pd.get_dummies(train_data[feature])\n    train_data = train_data.drop(feature,axis = 1)\n    train_data = train_data.join(one_hot)\n```\n\n4. Added regularisation: We then added L2 regularisation to the model but surprisingly our model's accuracy decreased by 1.89%, This could be because we had not much overfitting to begin with.\n\n```python\n    model.add(keras.layers.Dense(\n      units=64,                                   # number of units\/neurons\n      use_bias=True,                              # use a bias param\n      activation=\"relu\",                          # apply the relu function\n      kernel_regularizer=regularizers.l2(0.01)    # addred L2 regularisaiton\n    ))\n```\n\n5. Moved `Pclass` from One-hot to Z-Score: Then we removed `Pclass` one hot encoding and all regularisation and applied zscore to `Pclass` this gave us the highest accuracy of 78.5% so, we sticked to this model.\n\n```python\nfrom scipy import stats\nfor feature in ['Fare', 'Pclass', 'SibSp', 'Parch', 'Age']:\n    train_data[feature] = stats.zscore(train_data[feature])\n\n#  Apply one-hot encoding for `Sex` and `Embarked` feature\nfor feature in ['Sex', 'Embarked']:\n    one_hot = pd.get_dummies(train_data[feature])\n    train_data = train_data.drop(feature,axis = 1)\n    train_data = train_data.join(one_hot)\n```\n","d5a62485":"# Error Analysis\n1) **Error**: Most of the error has been seen in (Young, single males with low economic status) - They were marked as dead but they actually survived.\n* **Reason**: Our reasoning is that since they were single the coloumns `SibSp` and `Parch` were equal to 0. Such sparseness might have reduced the models weight. Sicne, `Fare` has a positive correlation with Survival, these gentleman with low fares might have been unlikely to survive acording to our mode.\n\n\n|Pid| Difference|\tSurvived|\tPrediction|\tPclass|\tSex|\tAge|\tFare|\tSibSp|\tParch|\tEmbarked|\n|-------|--------|---|----------|----|------|--------|--------|--|---|---|\n|286\t|0.889239|\t1|\t0.110761|\t3|\tmale|\t30.00|\t9.5000|\t0|\t0|\tS|\n|107\t|0.889103|\t1|\t0.110897|\t3|\tmale|\tNaN\t |  7.7750|\t0|\t0|\tS|\n|804\t|0.888957|\t1|\t0.111043|\t3|\tmale|\t27.00|\t6.9750|\t0|\t0|\tS|\n|444\t|0.888799|\t1|\t0.111632|\t3|\tmale|\t29.00|\t9.5000|\t0|\t0|\tS|\n|146\t|0.888239|\t1|\t0.111761|\t3|\tmale|\t27.00|\t7.7958|\t0|\t0|\tS|\n","fc580101":"# Preprocessing Test Data","e17d8724":"# The Data\n\n|Variable|\tDefinition                                  |\tKey|\n|--------|----------------------------------------------|---|\n|survival|\tSurvival                                    |\t0 = No, 1 = Yes|\n|pclass  |\tTicket class                                |\t1 = 1st, 2 = 2nd, 3 = 3rd|\n|sex     |\tSex\t                                        ||\n|Age     |\tAge in years\t                            ||\n|sibsp   |\t# of siblings \/ spouses aboard the Titanic\t||\n|parch   |\t# of parents \/ children aboard the Titanic\t||\n|ticket  |\tTicket number\t                            ||\n|fare    |\tPassenger fare                            \t||\n|cabin   |\tCabin number                                ||\t\n|embarked|\tPort of Embarkation                         |C = Cherbourg, Q = Queenstown, S = Southampton|\n","cfba3648":"# The Task\n\nGiven an example that carries information about a passenger in Titanic, we should be able to pridict if the person survived the tragic accident or not.","5940a002":"# Submission","ad66cd2e":"# Feature Transformation and Data Analysis\n1. Remove all the NaN:\n    * Age: Replace NaN Age with median Age to maintain normal distribution\n    * Replace Embarked with most repeated element 'S'\n2. Drop non essential features like: 'PassengerId', 'Name','Cabin', 'Ticket'\n3. Apply z-score: 'Fare', 'Pclass', 'SibSp', 'Parch', 'Age'\n4. Apply one-hot: 'Sex', 'Embarked'","d3be5ea2":"# Challenges\n1. Getting ourselves familiar with the Kaggle environment\n2. Trying come up with models to fit such scenario.\n3. Apply our knowledge about Binary classification to build a model\n4. Feature Transformation: \n    * What feature transformation should be applied? Log Scaling, clipping or Z-score?\n    * What should we replace NaN values with?\n    * What should we do with the outliers?\n    * How to convert contextual features into One-hot?\n    * Chosing between One-hot versus Label encoding.\n5. Which featues to keep and which to exclude?\n       ","4af0e6b1":"# Build the model","355fe5fc":"# Error Analysis - Baseline","b173c2ce":"# Confusion Matrix\n### Finding the right Threshold","5c30bfa1":"# Introduction\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 which is **68%** of the passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nWe are trying to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).","db8af828":"# Baseline\nThe baseline is that `Nobody survived the tragedy Titanic`.\n*       1502 out of 2224 people died in the tragic event.\n*       Since 68% of the people died we set our baseline as everyone died in the event.","9ca91845":"# Train the model"}}