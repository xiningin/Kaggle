{"cell_type":{"80afa2d6":"code","39fec463":"code","9f8e7c1e":"code","ea6967d9":"code","2e547f87":"code","2e79cc86":"code","5b823ae6":"code","5013e75d":"code","09776124":"code","818537da":"code","e1f23ac1":"code","756f45e5":"code","e341d807":"code","b1ac18a5":"code","df6995ea":"code","4f8809e5":"code","a6eb1333":"code","38a7935c":"code","8a9f7db0":"code","86a90cf0":"code","c24fac92":"code","30082ca6":"markdown","81463b62":"markdown","b59671c3":"markdown","4c58eb70":"markdown","d8ba4582":"markdown","0f4ebba3":"markdown","546ef333":"markdown","74348d0f":"markdown","09d52d22":"markdown"},"source":{"80afa2d6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","39fec463":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_x = pd.read_csv('..\/input\/titanic\/test.csv')\nsub = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\ndf = pd.concat([train,test_x], sort = False)  \n\ndf.head()","9f8e7c1e":"CabinFill = df[df['Cabin'].notnull()]\nCabinNull = df[df['Cabin'].isnull()]\nCabinFill['Cabin'] = CabinFill['Cabin'].astype(str).str[0]\ndf = pd.concat([CabinFill, CabinNull], sort = False).sort_values(['PassengerId'])\n\ndf['Sex'] = pd.Categorical(df.Sex).codes\ndf['Embarked'] = pd.Categorical(df.Embarked).codes\ndf['Cabin'] = pd.Categorical(df.Cabin).codes\n\ndf['FamilySize'] = df['Parch'] + df['SibSp'] + 1\ndf['IsAlone'] = 0\ndf.loc[df['FamilySize'] == 1, 'IsAlone'] = 1\n\ndf.head(10)","ea6967d9":"df.isnull().sum()","2e547f87":"df[['Age', 'Fare']].hist()","2e79cc86":"df['Age'].fillna((df['Age']).median(), inplace=True)\ndf['Fare'].fillna((df['Fare']).median(), inplace=True)\ndf.head(10)","5b823ae6":"df['Title'] = df['Name'].map(lambda name: name.split(',')[1].split('.')[0].strip())\ndf['Sur'] = df['Name'].map(lambda name: name.split(',')[0])\n\ndf['Title'].value_counts()","5013e75d":"titles_dummies = pd.get_dummies(df['Title'], prefix='Title')\nsur_dummies = pd.get_dummies(df['Sur'], prefix='Sur')\ndf = pd.concat([df, titles_dummies, sur_dummies], axis=1)\ndf.drop(['Title', 'Sur'], axis=1, inplace=True)\ndf.head()","09776124":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss, accuracy_score\nimport lightgbm as lgb\n\nlogloss, accuracy, y_preds = [],[],[]\n\ncv_train = np.zeros((len(train),))\ndrop_cols = ['Name','PassengerId','Ticket','Survived','Fare','Cabin','Embarked',\n            'Age','SibSp','Parch']\n\ntrain = df[:len(train)]\ntrain_x = train.drop(drop_cols, axis=1)\ntrain_y = train['Survived']\n\ntest = df[len(train):]\ntest_x = test.drop(drop_cols, axis=1)\n\nparams = {'objective': 'binary', 'seed': 71, 'verbose': 0, 'metrics': 'binary_logloss'}\nnum_round = 100\ncategorical_features = []\n\nkf = StratifiedKFold(n_splits=4, shuffle=True, random_state=71)\nfor tr_idx, va_idx in kf.split(train_x, train_y):\n    tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n    tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n    lgb_train = lgb.Dataset(tr_x, tr_y)\n    lgb_eval = lgb.Dataset(va_x, va_y)\n    \n    model = lgb.train(params, lgb_train, num_boost_round=num_round,\n                     categorical_feature=categorical_features,\n                     valid_names=['train', 'valid'], valid_sets=[lgb_train, lgb_eval],\n                     early_stopping_rounds=20)\n    \n    va_pred = model.predict(va_x, num_iteration=model.best_iteration)\n    cv_train[va_idx] = va_pred\n    logloss.append(log_loss(va_y, va_pred))\n    \n    va_pred = (va_pred > 0.5).astype(int)\n    accuracy.append(accuracy_score(va_y, va_pred))\n    \n    y_pred = model.predict(test_x, num_iteration=model.best_iteration)\n    \n    y_preds.append(y_pred)\n    \nprint(np.mean(logloss))\nprint(np.mean(accuracy))","818537da":"y_pred_cv = (cv_train > 0.5).astype(int)\naccuracy_score(train_y, y_pred_cv)","e1f23ac1":"sub_y_lgb = sum(y_preds) \/ len(y_preds) \nsub_y_lgb = (sub_y_lgb > 0.5).astype(int)\nsub['sub_y_lgb'] = sub_y_lgb","756f45e5":"from sklearn.linear_model import LogisticRegression\naccuracy, y_preds_LR = [],[]\ncv_train = np.zeros((len(train),))\ndrop_cols = ['Name','PassengerId','Ticket','Survived','Fare','Cabin','Embarked',\n            'Age','SibSp','Parch']\n\ntrain = df[:len(train)]\ntrain_x = train.drop(drop_cols, axis=1)\ntrain_y = train['Survived']\n\ntest = df[len(train):]\ntest_x = test.drop(drop_cols, axis=1)\n\n\nkf = StratifiedKFold(n_splits=4, shuffle=True, random_state=71)\nfor tr_idx, va_idx in kf.split(train_x, train_y):\n    tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n    tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n    \n    model = LogisticRegression(penalty='l2', solver='sag', random_state=0)\n    model.fit(tr_x, tr_y)\n    \n    va_pred = model.predict(va_x)\n    cv_train[va_idx] = va_pred\n    \n    va_pred = (va_pred > 0.5).astype(int)\n    accuracy.append(accuracy_score(va_y, va_pred))\n    \n    y_pred = model.predict(test_x)\n    \n    y_preds_LR.append(y_pred)\n    \nprint(np.mean(accuracy))","e341d807":"sub_y_LR = sum(y_preds_LR) \/ len(y_preds_LR) \nsub_y_LR = (sub_y_LR > 0.5).astype(int)\nsub['sub_y_LR'] = sub_y_LR","b1ac18a5":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\ndrop_cols = ['Name','PassengerId','Ticket','Survived']\n\ntrain = df[:len(train)]\nX = train.drop(drop_cols, axis=1)\ny = train['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=0)\n\nmodel = RandomForestClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nfti = model.feature_importances_ \n\nprint(accuracy)","df6995ea":"drop_cols = ['Name','PassengerId','Ticket','Survived']\ntest = df[len(train):]\ntest_x = test.drop(drop_cols, axis=1)\n\nsub_y_RF = model.predict(test_x)\nsub['sub_y_RF'] = sub_y_RF","4f8809e5":"import tensorflow as tf # neural networks\nfrom keras.wrappers.scikit_learn import KerasClassifier # neural networks\nfrom keras.models import Sequential  # neural networks\nfrom keras.layers import Dense, Activation, Dropout  # neural networks# dummies\nfrom numpy.random import seed # setting random seeds\n\ndrop_cols = ['Name','PassengerId','Ticket','Survived','Fare','Cabin','Embarked',\n            'Age','SibSp','Parch']\nx_NN = df.drop(drop_cols, axis=1)\n# splitting\nX_dummies_train = x_NN.iloc[0:890]\nX_dummies_test = x_NN.iloc[891:]\n\n# creating labels column\nY = df.iloc[0:890][\"Survived\"]","a6eb1333":"def create_neural_net(in_shape, lyrs=[4], act='relu', opt='Adam', dr=0.0):\n    # set random seed for reproducibility\n    seed(37556)\n    tf.random.set_seed(37556)\n    \n    # initialize the model object\n    model = Sequential()\n    \n    # create first hidden layer\n    model.add(Dense(lyrs[0], input_dim=in_shape, activation=act))\n    \n    # create additional hidden layers\n    for i in range(1,len(lyrs)):\n        model.add(Dense(lyrs[i], activation=act))\n    \n    # add dropout, default is none\n    model.add(Dropout(dr))\n    \n    # create output layer\n    model.add(Dense(1, activation='sigmoid'))  # output layer\n    \n    model.compile(loss='binary_crossentropy', optimizer=opt,\n                  metrics=['accuracy'])\n    \n    return model","38a7935c":"single_net = create_neural_net(X_dummies_train.shape[1], lyrs =[4])\nsingle_net.summary()","8a9f7db0":"training = single_net.fit(X_dummies_train, Y, epochs=100, batch_size=32,\n                         validation_split=0.25, verbose=0)\n# outputting the validation accuracy\nval_acc = np.mean(training.history['val_accuracy'])\nprint(\"\\n%s: %.2f%%\" % ('val_acc', val_acc*100))","86a90cf0":"# initializing the mdoel using best selected hyper-parameters\nnn = KerasClassifier(build_fn=create_neural_net, \n                        in_shape = X_dummies_train.shape[1],\n                        lyrs=[12, 8, 4], epochs=50, dr=0.1, batch_size=1, \n                         verbose=0)\n# fitting to entire training set\nnn.fit(X_dummies_train, Y)\n# predicting the entire test set\nsub['sub_y_NN'] = nn.predict(X_dummies_test).astype('int32')","c24fac92":"model_cols = ['sub_y_lgb','sub_y_RF','sub_y_NN','sub_y_LR']\nsub['Survived'] = np.sum(sub[model_cols], axis=1)\nsub['Survived'] = (sub['Survived'] >= 3).astype(int)\nsub.drop(model_cols, axis=1, inplace=True)\nsub.to_csv('sub_title_ensembled.csv', index=False)\nsub.head()","30082ca6":"# Ensemble\n\nWe build 4 models, using Random Forest, LightGBM, Logistic Regression, and Neural Network.\nSo let's ensemble these. \n\nI determined that if 3 out of 4 models predict each passenger as alive, s\/he is alive.","81463b62":"# Processing missing values","b59671c3":"# Processing Categorical Features","4c58eb70":"# Logistic regression","d8ba4582":"# Random Forest","0f4ebba3":"# Neural Network","546ef333":"# Processing 'Name' with One-Hot Encoding","74348d0f":"# LightGBM with Cross Validation\n\nI strongly recommend using `StratifiedKFold` rather than usual `KFold`, because it can contain the same ratio of those who servived and didn't in each fold.","09d52d22":"# Top 5%: With Simple Cross Validation and Ensemble\n\nIn this notebook, we're going to bulid several ML models, using Random Forest, LightGBM, Logistic Regression,\n\nand Neural Network. And we will attempt cross validation for LightGBM and Logistic Regression model.\n\nFinally we ensemble those 4 models in a pretty simple way."}}