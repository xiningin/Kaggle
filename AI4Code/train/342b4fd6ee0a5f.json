{"cell_type":{"cf962f75":"code","b8c3598b":"code","8822fabe":"code","d7d0b869":"code","6d5baf03":"code","7cc9f394":"code","bc0eeb50":"code","86be97ad":"code","57d90405":"code","cf471ad9":"code","e769cce5":"code","503129cb":"code","2d3d555a":"code","0b5e4696":"code","48b0ebcb":"code","42aa89b0":"code","954e284a":"code","422f2bca":"code","83030171":"code","8b88fac0":"code","3d89f73b":"code","f54ba2a4":"code","fd725f56":"code","dbc936d6":"code","327dda0b":"code","210c18e5":"code","e1d0da51":"code","3a73d33e":"code","324e6db5":"code","0d360d82":"code","6873143d":"code","78e7321e":"code","de21e310":"code","c7d94dbf":"code","e40ae211":"code","5bcd825c":"code","a1889515":"code","7f9b2e82":"code","d4c41f05":"code","202e2709":"markdown","ee911179":"markdown","dbc39cd0":"markdown","54c29377":"markdown","388431ee":"markdown","d70c580d":"markdown","27cd1cda":"markdown"},"source":{"cf962f75":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b8c3598b":"train_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-feb-2021\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-feb-2021\/test.csv')\n","8822fabe":"#train_df2  = train_df[train_df.target<9.523992259999998]\n#train_df2","d7d0b869":"print(train_df.shape,test_df.shape)","6d5baf03":"df = pd.concat([train_df.drop(['target','id'],axis=1),test_df.drop(['id'],axis=1)],axis=0)\ndf.head()","7cc9f394":"df.isnull().sum()","bc0eeb50":"def divideFeatures(df):\n    numerical_features = df.select_dtypes(include=[np.number])\n    categorical_features = df.select_dtypes(include=[np.object])\n    return numerical_features, categorical_features","86be97ad":"cont_features, cat_features =divideFeatures(df)","57d90405":"# boxplots of numerical features for outlier detection\n\nfig = plt.figure(figsize=(16,16))\nfor i in range(len(cont_features.columns)):\n    fig.add_subplot(5, 4, i+1)\n    sns.boxplot(y=cont_features.iloc[:,i])\nplt.tight_layout()\nplt.show()","cf471ad9":"# distplots for categorical data\n\nfig = plt.figure(figsize=(16,10))\nfor i in range(len(cat_features.columns[:])):\n    fig.add_subplot(5, 4, i+1)\n    cat_features.iloc[:,i].hist()\n    plt.xlabel(cat_features.columns[i])\nplt.tight_layout()\nplt.show()","e769cce5":"# correlation heatmap for all features\nplt.figure(figsize=(18,10),dpi=200)\ncorr = train_df.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr, mask = mask, annot=True)\n\nplt.show()","503129cb":"# plot sample skewed feature\nplt.figure(figsize=(10,4))\nsns.distplot(train_df['target'])\nplt.show()","2d3d555a":"skewed_features = cont_features.apply(lambda x: x.skew()).sort_values(ascending=False)\nskewed_features","0b5e4696":"print(train_df.shape,test_df.shape,df.shape)","48b0ebcb":"from sklearn.preprocessing import OneHotEncoder , LabelEncoder\nfrom sklearn.pipeline import make_pipeline ,Pipeline\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.compose import make_column_transformer ,ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split ,cross_val_score,validation_curve,learning_curve\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import GridSearchCV ,RandomizedSearchCV\nfrom sklearn.tree import plot_tree\nfrom sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,GradientBoostingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import LogisticRegression\n","42aa89b0":"one = OneHotEncoder(handle_unknown='ignore')\nscaler =StandardScaler()\n\nfs = SelectKBest()\nle =LabelEncoder()","954e284a":"print(df.select_dtypes(include='object').columns)\ndf.select_dtypes(exclude='object').columns","422f2bca":"cat_features.columns\n","83030171":"def lencode(df):\n    for i in df.columns:\n        for k in cat_features.columns:\n            if i ==k:\n                df[i]= le.fit_transform(df[i])\n        \n        \n        ","8b88fac0":"lencode(df)\ndf.head()\n","3d89f73b":"X_train = df[:300000] \nXf_test = df[300000:]\ny = train_df['target']","f54ba2a4":"X_train, X_test, y_train, y_test = train_test_split(X_train, y, test_size=0.1, random_state=42)","fd725f56":"#ct = make_column_transformer((one,['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8','cat9']),\n                             #(scaler,['id', 'cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6','cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']),\n                              #remainder='passthrough')","dbc936d6":"X_train.head()","327dda0b":"\nlinear = LinearRegression()\nrandom = RandomForestRegressor(n_estimators=100,random_state=42,n_jobs=-1,max_features=4)\n","210c18e5":"import xgboost as xgb\nxgbr = xgb.XGBRegressor(tree_method='gpu_hist')\n","e1d0da51":"import optuna\ndef objective(trial):\n    params = {\n        'random_state': 0,\n        'n_estimators': trial.suggest_categorical('n_estimators', [10000]),\n        'max_depth': trial.suggest_int('max_depth', 3, 8),\n        'learning_rate': trial.suggest_float('learning_rate', 0.001, 1.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10),\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10),\n        'gamma': trial.suggest_float('gamma', 0.0, 10),\n        'subsample': trial.suggest_categorical('subsample', [0.8, 0.9, 1.0]),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.1, 0.2, 0.3, 0.4, 0.5]),\n        'tree_method':'gpu_hist'    # comment this line if GPU is off\n    }\n    model = xgb.XGBRegressor(**params) \n    model.fit(X_train, y_train, eval_set=[(X_test,y_test)], early_stopping_rounds=1000, verbose=0)\n    y_pred = model.predict(X_test)\n    rmse = mean_squared_error(y_test, y_pred, squared=False)\n    \n    return rmse","3a73d33e":"%%time\nstudy = optuna.create_study(direction='minimize',sampler=optuna.samplers.TPESampler(seed=0))\nstudy.optimize(objective, n_trials=100)\nprint('Number of finished trials:', len(study.trials))\nprint('Best parameters:', study.best_trial.params)\nprint('Best RMSE:', study.best_trial.value)","324e6db5":"optuna.visualization.plot_optimization_history(study)","0d360d82":"params = study.best_params\nparams['random_state'] = 0\nparams['n_estimators'] = 10000\nparams['tree_method'] = 'gpu_hist'\n\nmodel3 = xgb.XGBRegressor(**params)\nmodel3.fit(X_train,y_train,eval_set=[(X_test, y_test)],early_stopping_rounds=1000,verbose=2)\n","6873143d":"\nmodel = make_pipeline(linear)\nmodel.fit(X_train,y_train)\n\nmodel2 = make_pipeline(xgbr)\nmodel2.fit(X_train,y_train)","78e7321e":"models ={'linear':model}\nmodels['xgbr'] = model2\nmodels['optuna_xbg'] =model3","de21e310":"def rmse(y_test,y_pred):\n    rmse= np.sqrt(mean_squared_error(y_test,y_pred))\n    return rmse\n","c7d94dbf":"from sklearn.model_selection import KFold\nfrom sklearn.base import clone\n\ndef cross_val_rmse(model):\n    model =clone(model)\n    five_fold = KFold(n_splits=5)\n    rmse_val =[]\n    for tr_ind,val_ind in five_fold.split(X_train):\n        model.fit(X_train.iloc[tr_ind,:],y.iloc[tr_ind])\n        rmse_val.append(rmse(y.iloc[val_ind],model.predict(X_train.iloc[val_ind,:])))\n        \n    return np.mean(rmse_val)","e40ae211":"import plotly.graph_objects as go\n\ndef compare_models(models):\n    training_rmse = [rmse(y_train,model.predict(X_train)) for model in models.values()]\n    validate_rmse = [cross_val_rmse(model) for model in models.values()] \n    testing_rmse = [rmse(y_test,model.predict(X_test)) for model in models.values()]\n    names = list(models.keys())\n    fig = go.Figure([\n                      go.Bar(x=names ,y=training_rmse ,name='traing_rmse'),\n                      go.Bar(x=names ,y=validate_rmse ,name='validate_rmse'),\n                      go.Bar(x=names ,y=testing_rmse ,name='testing_rmse',opacity=.3)])\n    return fig","5bcd825c":"fig = compare_models(models)\nfig.update_yaxes(range=[0,2],title=\"RMSE\")\nfig.show()","a1889515":"y_pred = model3.predict(X_test)\ny_pred =np.round(y_pred,6)\nprint('min',y_pred.min())\nprint('max',y_pred.max())\nRMSE = np.sqrt(mean_squared_error(y_test,y_pred))\nprint('RMSE',RMSE)\nprint(y_pred.tolist()) ","7f9b2e82":"ys_pred =model3.predict(Xf_test)\nys_pred =np.round(ys_pred,6)\nys_pred.tolist()","d4c41f05":"sub = pd.DataFrame({'id':test_df.id,'target':ys_pred})\nsub.to_csv('TPS.csv',index=False)\nsub.head()","202e2709":"## Modeling","ee911179":"## Datasets Loading....","dbc39cd0":"## submit","54c29377":"## EDA","388431ee":"import warnings\nwarnings.filterwarnings(\"ignore\")\nfig = plt.figure(figsize=(16,30))\nfor i in range(len(cont_features.columns)):\n    fig.add_subplot(10,5,i+1)\n    sns.distplot(cont_features.iloc[:,i])\n    plt.xlabel(cont_features.columns[i])\nplt.tight_layout()\nplt.show();\n    ","d70c580d":"## Upvote if u like :)","27cd1cda":"LAYOUT :)   \n\n                 EDA       :\n                 \n                 PIPELINE  :\n                 \n                              ML MODELS :)\n                              LinearRegression\n                                                          \n                              XGBoostingRegressor\n                              XGBoostingRegressor with optuna\n                              \n                 ALL IN PLOT:\n                              RMSE of All MOdels\n                 PREDICTIONS:"}}