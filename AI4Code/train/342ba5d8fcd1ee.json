{"cell_type":{"df5eec8c":"code","a270a822":"code","6724e74c":"code","32ab773a":"code","06e2be0f":"code","c258a182":"code","fc82745f":"code","609cf111":"code","66d9567e":"code","fe757269":"code","40a2c608":"code","6e627e76":"code","63accfb8":"code","7795848e":"code","b68d8bbd":"code","7e3f8c61":"code","ac5642ff":"code","acddde6a":"code","3b0f42cc":"code","98583ff8":"code","056dca27":"code","33d1f65a":"code","7382a46b":"code","1e044a0d":"code","5a3f99e9":"code","0a1f28b0":"code","d47756de":"code","a0f2040d":"code","9423ed42":"code","e7ff1653":"code","bf317d5d":"code","5f9767dc":"code","4190c164":"code","5a0a562a":"code","a48dbbff":"code","83199d66":"code","37701e31":"code","f5d39aa9":"code","6ee13c7a":"code","2bdb9a61":"code","9b61fda3":"code","e2fe48a5":"code","30938dc1":"code","aa0490a1":"code","80cc72e6":"code","071e46da":"code","25dbd12b":"code","88d8f0c7":"code","6d48775c":"code","f07e68d6":"markdown","e5086c7c":"markdown","5b5aa706":"markdown","363c9e0e":"markdown","8ba1b4d8":"markdown","8623b612":"markdown","c0471820":"markdown","2d06d258":"markdown","6dcabc8d":"markdown","270fb40a":"markdown","7b654a1f":"markdown","f518dfdf":"markdown","38b7384b":"markdown","caa46c55":"markdown","47413190":"markdown","80739200":"markdown","0b71ef90":"markdown","563397a4":"markdown","facaca4d":"markdown","4621f113":"markdown","49cbdebc":"markdown","1ddbc13d":"markdown","e27144de":"markdown","80835db8":"markdown","e3d57e9b":"markdown","18c33eb0":"markdown"},"source":{"df5eec8c":"from pandas import *\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib notebook\nfrom scipy.stats import mode\n\nimport os\nprint(os.listdir(\"..\/input\"))","a270a822":"diabetes = pd.read_csv('..\/input\/pima-diabetes\/diabetes.csv')","6724e74c":"diabetes.head(5)","32ab773a":"diabetes.dtypes","06e2be0f":"memory = diabetes.memory_usage()\nprint(memory)\nprint(\"Total Memory Usage = \",sum(memory))","c258a182":"diabetes.iloc[:,0:9] = diabetes.iloc[:,0:9].astype('float16')","fc82745f":"memory = diabetes.memory_usage()\nprint(memory)\nprint(\"Total Memory Usage = \",sum(memory))","609cf111":"diabetes.describe()","66d9567e":"diabetes[\"Outcome\"].value_counts()","fe757269":"fig, axs = plt.subplots()\nsns.boxplot(data=diabetes,orient='h',palette=\"Set2\")\nplt.show()","40a2c608":"q75, q25 = np.percentile(diabetes[\"Insulin\"], [75 ,25])\niqr = q75-q25\nprint(\"IQR\",iqr)\nwhisker = q75 + (1.5*iqr)\nprint(\"Upper whisker\",whisker)","6e627e76":"diabetes[\"Insulin\"] = diabetes[\"Insulin\"].clip(upper=whisker)","63accfb8":"fig, axs = plt.subplots()\nsns.boxplot(data=diabetes,orient='h',palette=\"Set2\")\nplt.show()","7795848e":"diabetes.head()","b68d8bbd":"diabetes.columns","7e3f8c61":"\nprint((diabetes[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age']] == 0).sum())","ac5642ff":"diabetes.loc[:,['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI']] = diabetes[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI']].replace(0, np.NaN)\ndiabetes.head()","acddde6a":"diabetes.isnull().sum()","3b0f42cc":"print(\"Size before dropping NaN rows\",diabetes.shape,\"\\n\")\n\nnan_dropped = diabetes.dropna()\n\nprint(nan_dropped.isnull().sum())\nprint(\"\\nSize after dropping NaN rows\",nan_dropped.shape)","98583ff8":"diabetes.isnull().mean()","056dca27":"print(\"Size before dropping NaN rows\",diabetes.shape,\"\\n\")\n\ncol_dropped = diabetes.loc[:, diabetes.isnull().mean() < .4]\nrow_dropped = diabetes.loc[diabetes.isnull().mean(axis=1) < .4, :]\n\nprint(nan_dropped.isnull().sum())\nprint(\"\\nSize after dropping Columns with rows\",col_dropped.shape)\nprint(\"Size after dropping Columns with rows\",row_dropped.shape)","33d1f65a":"bins = [0,25,30,35,40,100]\n\ngroup_names = ['malnutrition', 'Under-Weight', 'Healthy', 'Over-Wight',\"Obese\"]\ndiabetes['BMI_Class'] = pd.cut(diabetes['BMI'], bins, labels=group_names)\ndiabetes.head(10)","7382a46b":"diabetes.dtypes","1e044a0d":"#Impute the values:\ndiabetes['BMI_Class'].fillna((diabetes['BMI_Class']).mode()[0], inplace=True)\ndiabetes['Insulin'].fillna((diabetes['Insulin']).mean(), inplace=True)\ndiabetes['Pregnancies'].fillna((diabetes['Pregnancies']).median(), inplace=True)\n\n# #Now check the #missing values again to confirm:\nprint(diabetes.isnull().sum())","5a3f99e9":"vector = np.random.chisquare(1,500)\nprint(\"Mean\",np.mean(vector))\nprint(\"SD\",np.std(vector))\nprint(\"Range\",max(vector)-min(vector))","0a1f28b0":"from sklearn.preprocessing import MinMaxScaler\nrange_scaler = MinMaxScaler()\nrange_scaler.fit(vector.reshape(-1,1))\nrange_scaled_vector = range_scaler.transform(vector.reshape(-1,1))\nprint(\"Mean\",np.mean(range_scaled_vector))\nprint(\"SD\",np.std(range_scaled_vector))\nprint(\"Range\",max(range_scaled_vector)-min(range_scaled_vector))","d47756de":"from sklearn.preprocessing import StandardScaler\nstandardizer = StandardScaler()\nstandardizer.fit(vector.reshape(-1,1))\nstd_scaled_vector = standardizer.transform(vector.reshape(-1,1))\nprint(\"Mean\",int(np.mean(std_scaled_vector)))\nprint(\"SD\",int(np.std(std_scaled_vector)))\nprint(\"Range\",max(std_scaled_vector)-min(std_scaled_vector))","a0f2040d":"dummified_data = pd.concat([diabetes.iloc[:,:-1],pd.get_dummies(diabetes['BMI_Class'])],axis=1)\ndummified_data.head()","9423ed42":"vector.shape","e7ff1653":"row_vector = vector.reshape(-1,1)\nrow_vector.shape","bf317d5d":"col_vector = vector.reshape(1,-1)\ncol_vector.shape","5f9767dc":"matrix = vector.reshape(10,50)\nmatrix.shape","4190c164":"#Determine pivot table\ndf = pd.DataFrame({\"A\": [\"foo\", \"foo\", \"foo\", \"foo\", \"foo\",\n                       \"bar\", \"bar\", \"bar\", \"bar\"],\n                    \"B\": [\"one\", \"one\", \"one\", \"two\", \"two\",\n                          \"one\", \"one\", \"two\", \"two\"],\n                    \"C\": [\"small\", \"large\", \"large\", \"small\",\n                          \"small\", \"large\", \"small\", \"small\",\n                          \"large\"],\n                    \"D\": [1, 2, 2, 3, 3, 4, 5, 6, 7],\n                    \"E\": [2, 4, 5, 5, 6, 6, 8, 9, 9]})","5a0a562a":"df.head(3)","a48dbbff":"table = pivot_table(df, values='D', index=['A', 'B'],\n                  columns=['C'], aggfunc=np.sum)\ntable\n","83199d66":"pd.crosstab(df[\"D\"],df[\"B\"],margins=True)","37701e31":"df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],\n                        'B': ['B0', 'B1', 'B2', 'B3'],\n                        'C': ['C0', 'C1', 'C2', 'C3'],\n                        'D': ['D0', 'D1', 'D2', 'D3']},\n                       index=[0, 1, 2, 3])\n    \n\ndf2 = pd.DataFrame({'A': ['A4', 'A5', 'A6', 'A7'],\n                        'B': ['B4', 'B5', 'B6', 'B7'],\n                        'C': ['C4', 'C5', 'C6', 'C7'],\n                        'D': ['D4', 'D5', 'D6', 'D7']},\n                       index=[4, 5, 6, 7])\n    \n\ndf3 = pd.DataFrame({'A': ['A8', 'A9', 'A10', 'A11'],\n                       'B': ['B8', 'B9', 'B10', 'B11'],\n                        'C': ['C8', 'C9', 'C10', 'C11'],\n                        'D': ['D8', 'D9', 'D10', 'D11']},\n                       index=[8, 9, 10, 11])\n    \n\nframes = [df1, df2, df3]\n\nresult = pd.concat(frames)","f5d39aa9":"grade = pd.read_csv(\"..\/input\/grades\/Grade.csv\")\ngrade.head()","6ee13c7a":"grade.describe()","2bdb9a61":"fig, axs = plt.subplots()\nsns.regplot(x=\"StudentId\",y=\"OverallPct1\",data=grade,scatter=True,fit_reg=False)\nplt.show()","9b61fda3":"fig, axs = plt.subplots()\nsns.boxplot(data=grade.iloc[:,1:-1],orient='h',palette=\"Set2\")\nplt.show()","e2fe48a5":"# Basic Plots\nfig, axs = plt.subplots(ncols=3)\nsns.distplot(grade['English1'],ax=axs[0])\nsns.distplot(grade['Math1'],ax=axs[1])\nsns.distplot(grade['Science1'],ax=axs[2])\nplt.show()","30938dc1":"# Axis range and granularity are very important\nfig, axs = plt.subplots(ncols=3)\nsns.distplot(grade['English1'],ax=axs[0],bins=20)\nsns.distplot(grade['Math1'],ax=axs[1],bins=20)\nsns.distplot(grade['Science1'],ax=axs[2],bins=20)\naxs[0].set_xlim(0,100)\naxs[1].set_xlim(0,100)\naxs[2].set_xlim(0,100)\naxs[0].set_ylim(0,0.12)\naxs[1].set_ylim(0,0.12)\naxs[2].set_ylim(0,0.12)\nplt.show()","aa0490a1":"#Asthetics\nfig, axs = plt.subplots()\nsns.distplot(grade['English1'], bins=10, rug=True, rug_kws={\"color\": \"red\"},\n             kde_kws={\"color\": \"black\", \"lw\": 2, \"label\": \"KDE\"},\n             hist_kws={\"histtype\": \"step\", \"lw\": 3,\"color\": \"green\"})\nplt.show()","80cc72e6":"fig, axs = plt.subplots()\nnormal_dist = np.random.randn(1,1000)\nnormal_plot = sns.distplot(normal_dist)\nnormal_plot.set(xlabel='Value', ylabel='Frequency')\nplt.show()","071e46da":"fig, axs = plt.subplots()\nvector = np.random.chisquare(1,500)\nvector_plot = sns.distplot(vector)\nvector_plot.set(xlabel='Value', ylabel='Probability')\nplt.show()","25dbd12b":"iris = pd.read_csv(\"..\/input\/iris-dataset\/Iris.csv\")\niris.head()\n","88d8f0c7":"fig, axs = plt.subplots(nrows=2)\nsns.swarmplot(x=\"Species\", y=\"SepalLengthCm\",ax=axs[0], data=iris)\nsns.swarmplot(x=\"Species\", y=\"PetalLengthCm\",ax=axs[1], data=iris)\nplt.show();","6d48775c":"plt.figure(1)\nplt.figure(figsize = (12,8))\nplt.scatter(iris['PetalLengthCm'], iris['PetalWidthCm'], s=np.array(iris.Species == 'Iris-setosa'), marker='^', c='green', linewidths=5)\nplt.scatter(iris['PetalLengthCm'], iris['PetalWidthCm'], s=np.array(iris.Species == 'Iris-versicolor'), marker='^', c='orange', linewidths=5)\nplt.scatter(iris['PetalLengthCm'], iris['PetalWidthCm'], s=np.array(iris.Species == 'Iris-virginica'), marker='o', c='blue', linewidths=5)\nplt.xlabel('PetalLengthCm')\nplt.ylabel('PetalWidthCm')\nplt.legend(loc = 'upper left', labels = ['Setosa', 'versicolor', 'virginica'])\nplt.show();","f07e68d6":"### Reshape","e5086c7c":"### Pivot Table","5b5aa706":"### Dummification","363c9e0e":"#### 1. Some constant value that is considered \"normal\" in the domain\n#### 2. Summary statistic like Mean, Median, Mode\n#### 3. A value estimated by algorithm or predictive model","8ba1b4d8":"### Plotting","8623b612":"## Check for outliers","c0471820":"## Drop row\/columns having more than certain percentage of NaNs","2d06d258":"### Imputing missing values","6dcabc8d":"#### We can impute the missing values by many ways. Here are the 3 most common ways we can do","270fb40a":"#### Check if the data types are as expected","7b654a1f":"http:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.pivot_table.html\n","f518dfdf":"### Merging dataframes","38b7384b":"### Standardization\n","caa46c55":"### Dealing with outliers","47413190":"## Check for Summary Statistics","80739200":"### **Scaling**","0b71ef90":"https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/merging.html","563397a4":"### Data Exploration","facaca4d":"## Dealing with missing values","4621f113":"**For other plots and exploratory data analysis, please refer to my kernel here.**\n\nhttps:\/\/www.kaggle.com\/shravankoninti\/starter-code-and-eda-on-iris-species\n![](http:\/\/)","49cbdebc":"### Crosstab","1ddbc13d":"**The main purpose of this tutorial is to highlight the importance of pre-processing steps we need to follow for any data analysis. Sometimes we tend to forget the syntax of python program for pre-processing. The dataset is used as per my convenience in showing the desired output. I am hopeful this tutorial will be a quick reference for everyone**","e27144de":"## A. Drop rows having NaN","80835db8":"### Binning","e3d57e9b":"### Check missing values","18c33eb0":"### Memory Optimzations"}}