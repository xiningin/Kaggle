{"cell_type":{"37d6b3dd":"code","08d47ed4":"code","fd6ebd1a":"code","15307640":"code","bc940e77":"code","9b6d63ea":"code","1f4c5c94":"code","c01464d8":"code","211cebef":"code","9512ff50":"code","2be75d95":"code","56e3140c":"code","9ae65952":"code","2042dad6":"code","c12afcd6":"code","16763187":"code","fc3869a2":"code","13daf985":"code","f1f16446":"code","85e3c2ce":"code","6ab644a5":"code","e14c8469":"code","1276f278":"code","077af39f":"code","1a75a798":"code","06cf1bb8":"markdown","e15704c6":"markdown","3027c4be":"markdown","406b5f6f":"markdown","e357b644":"markdown","90c9aad0":"markdown","5811237d":"markdown","e62784e1":"markdown","a928909e":"markdown","f75878ac":"markdown","9a8d859b":"markdown"},"source":{"37d6b3dd":"import random\nrandom.seed(123)\n\nimport pandas as pd\nimport numpy as np\nimport datatable as dt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom scipy.stats import chi2_contingency\nimport numpy as np","08d47ed4":"# using datatable for faster loading\n\ntrain = dt.fread(r'..\/input\/tabular-playground-series-oct-2021\/train.csv').to_pandas()\ntest = dt.fread(r'..\/input\/tabular-playground-series-oct-2021\/test.csv').to_pandas()\nsub = dt.fread(r'..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv').to_pandas()","fd6ebd1a":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                else:\n                    df[col] = df[col].astype(np.float32)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","15307640":"train_1 = reduce_mem_usage(train)\ntest_1= reduce_mem_usage(test)","bc940e77":"train_data = train_1.copy()\ntest_data = test_1.copy()","9b6d63ea":"# our data has 287 columns (including id and target)\n\ntrain_data.head()","1f4c5c94":"# 1 million rows, with 240 numerical columns and 46 boolean ones\n\ntrain_data.info() ","c01464d8":"# no null values in train and test data\n\nprint('Number of nulls in train data: ',train_data.isna().sum().sum())\nprint('Number of nulls in test data: ',test_data.isna().sum().sum())","211cebef":"# segregating train data as per different data types - id is the only int32 column\n\ntrain_data_float = train_data.select_dtypes(include = 'float16')\ntrain_data_boolean = train_data.select_dtypes(include = 'bool')\n\ntrain_data_float= pd.concat([train_data_float,train_data['target']],axis=1)","9512ff50":"# overall, all features have values between 0 and 1, so no need to normalise them\n# I checked min, max and mean values to come to this conclusion\n\ntrain_data_float.min().sort_values(ascending=False)","2be75d95":"# low correlation with the target variable - these are the top 10 most correlated\n\nnp.abs(train_data_float.corr()['target']).sort_values(ascending=False).head(10)","56e3140c":"# plotting a few of the top correlation variables\n\nfig = plt.figure(figsize=(30,14))\nfig, ax = plt.subplots(2,2)\nfig.tight_layout()\n\nsns.distplot(train_data_float['f58'],ax=ax[0][0])\nsns.distplot(train_data_float['f69'],ax=ax[0][1])\nsns.distplot(train_data_float['f156'],ax=ax[1][0]) # only this is left skewed\nsns.distplot(train_data_float['f58'],ax=ax[1][1])","9ae65952":"# checking % of 1's in our boolean data - top 10 as per number of 1's\n# our target variable is very balanced\n\n(train_data_boolean.sum()\/len(train_data)*100.0).sort_values(ascending=False).head(10)","2042dad6":"# target variable - highly balanced\n\nsns.countplot(train_data_boolean['target'])","c12afcd6":"# scatterplot of the target variable\n\nsns.scatterplot(train_data_boolean['f22'])","16763187":"# top 10 correlated boolean variables with our target - only f22 seems really significant\n\nnp.abs(train_data_boolean.corr()['target']).sort_values(ascending=False).head(10)","fc3869a2":"# checking if any other variables are strongly related with f22\n# It is highly independent of all other variables. Interesting\n\nnp.abs(train_data_boolean.corr()['f22']).sort_values(ascending=False).head(10)","13daf985":"# Cramer's V correlation matrix - building a function\n\ndef cramers_V(var1,var2):\n    crosstab =np.array(pd.crosstab(var1,var2, rownames=None, colnames=None)) # Cross table building\n    stat = chi2_contingency(crosstab)[0] # Keeping of the test statistic of the Chi2 test\n    obs = np.sum(crosstab) # Number of observations\n    mini = min(crosstab.shape)-1 # Take the minimum value between the columns and the rows of the cross table\n    return (stat\/(obs*mini))","f1f16446":"rows= []\n\nfor var1 in train_data_boolean:\n    col = []\n    for var2 in train_data_boolean :\n        cramers =cramers_V(train_data_boolean[var1], train_data_boolean[var2]) # Cramer's V test\n        col.append(round(cramers,2)) # Keeping of the rounded value of the Cramer's V  \n    rows.append(col)\n\ncramers_results = np.array(rows)\ndf = pd.DataFrame(cramers_results,columns = train_data_boolean.columns,index = train_data_boolean.columns)\ndf ","85e3c2ce":"# we see nothing useful here, other than with f22\n\ndf['target'].sort_values(ascending=False)","6ab644a5":"#  plotting f22 with our target variable - we see the negative correlation here\n# even those with majority 1s seem to have some relation with our target - quite balanced\n\nfig = plt.figure(figsize=(30,14))\nfig, ax = plt.subplots(2,2)\nfig.tight_layout()\nsns.countplot(train_data_boolean['f22'],hue=train_data_boolean['target'],ax=ax[0][0])\nsns.countplot(train_data_boolean['f253'],hue=train_data_boolean['target'],ax=ax[0][1])\nsns.countplot(train_data_boolean['f246'],hue=train_data_boolean['target'],ax=ax[1][0])\nsns.countplot(train_data_boolean['f247'],hue=train_data_boolean['target'],ax=ax[1][1])","e14c8469":"# I will work with the boolean features (other than f22) to come up with something\n# ones is simply the sum of Trues across the rows, for variables other than f22\n# vote is majority voting of all those columns (if most are 1's, then vote will be 1)\n\ntrain_data_boolean['ones'] = train_data_boolean.drop(['f22','target'],axis=1).sum(axis=1)\ntrain_data_boolean['vote'] = train_data_boolean['ones'].apply(lambda x: 1 if (x\/47*100.0)>50 else 0)","1276f278":"# a-ha, 'ones' may be a decent predictor too (+0.04), 'vote' not so much\n# I had created \"ones\", once with f22 and once without it, excluding f22 it seems better.\n\nnp.abs(train_data_boolean.corr()['target']).sort_values(ascending=False).head(10)","077af39f":"# checking for continuous variables - memory issues always\n\ntrain_data_float['min'] = train_data_float.min(axis=1)\ntrain_data_float['max'] = train_data_float.max(axis=1)\ntrain_data_float['sum'] = train_data_float.sum(axis=1)\ntrain_data_float['std'] = train_data_float.std(axis=1)","1a75a798":"np.abs(train_data_float.corr()['target']).sort_values(ascending=False).head(10)","06cf1bb8":"<div style=\"background-color:rgba(215, 159, 21, 0.5);\">\n    <h1><center>Importing Libraries and Data<\/center><\/h1>\n<\/div>","e15704c6":"<div style=\"background-color:rgba(215, 159, 21, 0.5);\">\n    <h1><center>Take-aways and Action Points<\/center><\/h1>\n<\/div>","3027c4be":"<div style=\"background-color:rgba(215, 159, 21, 0.5);\">\n    <h1><center>Please upvote if you like my work. Thanks :)<\/center><\/h1>\n<\/div>","406b5f6f":"**Take-aways**\n\n1. The data is big - **1 million rows** and 280+ columns (numerical and binary both)\n2. The target is **highly balanced**.\n3. There are **no missing values** to be treated -all continuous features are already scaled (0 to 1)\n4. The variables have **low correlation** with the target and with each other\n5. **f179 and f22** have decent correlation - f22 is something worth looking at more.\n6. Variables with high no. of 1s are **distributed very evenly** with respect to the target variable.\n\n**Action Points**\n\n1. **No feature engineering** required for my first run - but may include 'ones' to see if better.\n2. Check feature importances through my model - may use for **selection** in the future runs.\n3. Look out for f22 and f179 in the feature importances\n4. Use **StratifiedKFold** for creating balanced multiple folds while model run.\n5. Will see the performance of XGB, CatBoost, LightGBM and RandomForest and decide ensemble.","e357b644":"<div style=\"background-color:rgba(215, 159, 21, 0.5);\">\n    <h1><center>Reduce Memory Usage<\/center><\/h1>\n<\/div>","90c9aad0":"Please scroll to the end to see the key take-aways and my action points for the first model run.","5811237d":"<div style=\"background-color:rgba(215, 159, 21, 0.5);\">\n    <h1><center>Check for float data<\/center><\/h1>\n<\/div>","e62784e1":"<div style=\"background-color:rgba(215, 159, 21, 0.5);\">\n    <h1><center>Feature Engineering Experiment<\/center><\/h1>\n<\/div>","a928909e":"<div style=\"background-color:rgba(215, 159, 21, 0.5);\">\n    <h1><center>Basic Data Check<\/center><\/h1>\n<\/div>","f75878ac":"**I have done some experiments with the new feature in the following notebook -\nhttps:\/\/www.kaggle.com\/raahulsaxena\/tps-oct-21-feature-added-catboost-baseline\nPlease upvote if you find it useful.**","9a8d859b":"<div style=\"background-color:rgba(215, 159, 21, 0.5);\">\n    <h1><center>Check for boolean data<\/center><\/h1>\n<\/div>"}}