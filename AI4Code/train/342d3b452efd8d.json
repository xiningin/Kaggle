{"cell_type":{"b7b6767b":"code","bc01c427":"code","273b1697":"code","a748cd23":"code","d548c825":"code","51ac66ee":"code","20691841":"code","d2ac52b3":"markdown","13ff3b0c":"markdown","f2384ef6":"markdown","86b53bce":"markdown","e59de544":"markdown","606c7be7":"markdown","44dfffdc":"markdown","d4087545":"markdown","c2a99c77":"markdown","b09cd13f":"markdown"},"source":{"b7b6767b":"import numpy as np\nimport math\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split","bc01c427":"path = '..\/input\/surgical-dataset-binary-classification\/Surgical-deepnet.csv'\ndf = pd.read_csv(path)\ndf = shuffle(df)\ndf.head()","273b1697":"# Generate indices for splits\n\ntest_ind = round(len(df)*0.25)\ntrain_ind = test_ind + round(len(df)*0.01)\nunlabeled_ind = train_ind + round(len(df)*0.74)\n\n\n# Partition the data\n\ntest = df.iloc[:test_ind]\ntrain = df.iloc[test_ind:train_ind]\nunlabeled = df.iloc[train_ind:unlabeled_ind]\n\n\n# Assign data to train, test, and unlabeled sets\n\nX_train = train.drop('complication', axis=1)\ny_train = train.complication\n\nX_unlabeled = unlabeled.drop('complication', axis=1)\n\nX_test = test.drop('complication', axis=1)\ny_test = test.complication\n\n\n# Check dimensions of data after splitting\n\nprint(f\"X_train dimensions: {X_train.shape}\")\nprint(f\"y_train dimensions: {y_train.shape}\\n\")\n\nprint(f\"X_test dimensions: {X_test.shape}\")\nprint(f\"y_test dimensions: {y_test.shape}\\n\")\n\nprint(f\"X_unlabeled dimensions: {X_unlabeled.shape}\")","a748cd23":"y_train.value_counts().plot(kind='bar')\nplt.xticks([0,1], ['No Complication', 'Complication'])\nplt.ylabel('Count')","d548c825":"clf = LogisticRegression(max_iter=1000)\n\nclf.fit(X_train, y_train)\ny_hat_test = clf.predict(X_test)\ny_hat_train = clf.predict(X_train)\n\ntrain_f1 = f1_score(y_train, y_hat_train)\ntest_f1 = f1_score(y_test, y_hat_test)\n\nprint(f\"Train f1 Score: {train_f1}\")\nprint(f\"Test f1 Score: {test_f1}\")\n\nplot_confusion_matrix(clf, X_test, y_test, cmap='Blues', normalize='true', display_labels=['No Comp.', 'Complication']);","51ac66ee":"# Initiate iteration counter\niterations = 0\n\n# Containers to hold f1_scores and # of pseudo-labels\ntrain_f1s = []\ntest_f1s = []\npseudo_labels = []\n\n# Assign value to initiate while loop\nhigh_prob = [1] \n\n# Loop will run until there are no more high-probability pseudo-labels\nwhile len(high_prob) > 0:\n        \n    # Fit classifier and make train\/test predictions\n    clf = LogisticRegression(max_iter=1000)\n    clf.fit(X_train, y_train)\n    y_hat_train = clf.predict(X_train)\n    y_hat_test = clf.predict(X_test)\n\n    # Calculate and print iteration # and f1 scores, and store f1 scores\n    train_f1 = f1_score(y_train, y_hat_train)\n    test_f1 = f1_score(y_test, y_hat_test)\n    print(f\"Iteration {iterations}\")\n    print(f\"Train f1: {train_f1}\")\n    print(f\"Test f1: {test_f1}\")\n    train_f1s.append(train_f1)\n    test_f1s.append(test_f1)\n   \n    # Generate predictions and probabilities for unlabeled data\n    print(f\"Now predicting labels for unlabeled data...\")\n\n    pred_probs = clf.predict_proba(X_unlabeled)\n    preds = clf.predict(X_unlabeled)\n    prob_0 = pred_probs[:,0]\n    prob_1 = pred_probs[:,1]\n\n    # Store predictions and probabilities in dataframe\n    df_pred_prob = pd.DataFrame([])\n    df_pred_prob['preds'] = preds\n    df_pred_prob['prob_0'] = prob_0\n    df_pred_prob['prob_1'] = prob_1\n    df_pred_prob.index = X_unlabeled.index\n    \n    # Separate predictions with > 99% probability\n    high_prob = pd.concat([df_pred_prob.loc[df_pred_prob['prob_0'] > 0.99],\n                           df_pred_prob.loc[df_pred_prob['prob_1'] > 0.99]],\n                          axis=0)\n    \n    print(f\"{len(high_prob)} high-probability predictions added to training data.\")\n    \n    pseudo_labels.append(len(high_prob))\n\n    # Add pseudo-labeled data to training data\n    X_train = pd.concat([X_train, X_unlabeled.loc[high_prob.index]], axis=0)\n    y_train = pd.concat([y_train, high_prob.preds])      \n    \n    # Drop pseudo-labeled instances from unlabeled data\n    X_unlabeled = X_unlabeled.drop(index=high_prob.index)\n    print(f\"{len(X_unlabeled)} unlabeled instances remaining.\\n\")\n    \n    # Update iteration counter\n    iterations += 1","20691841":"fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(6,8))\nax1.plot(range(iterations), test_f1s)\nax1.set_ylabel('f1 Score')\nax2.bar(x=range(iterations), height=pseudo_labels)\nax2.set_ylabel('Pseudo-Labels Created')\nax2.set_xlabel('# Iterations');\n\nplot_confusion_matrix(clf, X_test, y_test, cmap='Blues', normalize='true',\n                     display_labels=['No Comp.', 'Complication']);","d2ac52b3":"<h1 id=\"reference\" style=\"color:black; background:white; border:0.5px dotted;\"> \n    <center>Reference\n        <a class=\"anchor-link\" href=\"#reference\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","13ff3b0c":"<h1 id=\"analysis\" style=\"color:black; background:white; border:0.5px dotted;\"> \n    <center>Analysis\n        <a class=\"anchor-link\" href=\"#analysis\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","f2384ef6":"Use the logistic regression classifier to predict on the test data.","86b53bce":"<h1 id=\"supervised\" style=\"color:black; background:white; border:0.5px dotted;\"> \n    <center>Supervised Learning\n        <a class=\"anchor-link\" href=\"#supervised\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","e59de544":"<h1 id=\"dataset\" style=\"color:black; background:white; border:0.5px dotted;\"> \n    <center>Dataset\n        <a class=\"anchor-link\" href=\"#dataset\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","606c7be7":"Doug Steen - [Medium](https:\/\/towardsdatascience.com\/a-gentle-introduction-to-self-training-and-semi-supervised-learning-ceee73178b38)","44dfffdc":"<b>Pseudocode<\/b>:<br>\n1. Train Logistic Regression classifer on the labeled trainning data.\n2. Use the classifer to predict labels for all unlabeled data.\n3. Concatenante the pseudo-labeled data with the labeled training data.\n4. Use trained classifer to make predictions for the labeled test data.","d4087545":"<h1 id=\"distribution\" style=\"color:black; background:white; border:0.5px dotted;\"> \n    <center>Distribution\n        <a class=\"anchor-link\" href=\"#distribution\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","c2a99c77":"<pre style=\"border: 1px dashed;\">\n<div style=\"margin-left: 35%;\">\n     _____\n    [IIIII]\n     )\"\"\"(\n    \/     \\\n   \/       \\\n   |`-...-'|\n   |asprin |\n _ |`-...-'j    _\n(\\)`-.___.(I) _(\/)\n  (I)  (\/)(I)(\\)\n     (I)        \n\n<b>Surgical<\/b>\n      Semi-Supervised Learning\n<\/div>\n<\/pre>","b09cd13f":"<h1 id=\"semisupervised\" style=\"color:black; background:white; border:0.5px dotted;\"> \n    <center>Semi-Supervised Learning\n        <a class=\"anchor-link\" href=\"#semisupervised\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>"}}