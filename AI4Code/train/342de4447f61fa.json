{"cell_type":{"7623098b":"code","ccd41406":"code","cc85f650":"code","4a686b97":"code","67ecc053":"code","9d4aab41":"code","0b737985":"code","2362bbf1":"code","be47e253":"code","95abf34a":"code","6c2cc63f":"code","808baa34":"code","a8ef30b5":"code","6ba98dfe":"code","f1f7d3ee":"code","ff57022e":"code","14af5e45":"code","bbb21d4b":"code","7ce1f33f":"code","0414f490":"code","47c8029a":"code","9efcd552":"code","f393b438":"code","b8fe2321":"code","1b2ae601":"code","93174681":"code","d194b481":"code","571211eb":"code","8ffadb25":"code","49a78e08":"code","d95c45bb":"code","35fdce0b":"code","7cc8d199":"code","cbff8263":"code","534dfdbe":"code","3fb8393b":"markdown","b5bf985d":"markdown","51fcac5b":"markdown","b93bd2eb":"markdown","dd01d667":"markdown","369e39c7":"markdown","6afd322b":"markdown","a614d896":"markdown","9fd58840":"markdown","17da6eec":"markdown","3345e981":"markdown","17ac34ec":"markdown","5244a45e":"markdown","c83732c3":"markdown","1736bc8d":"markdown","e6435cdf":"markdown","35f752aa":"markdown","d664127d":"markdown"},"source":{"7623098b":"import os\nprint(os.listdir(\"..\/input\"))","ccd41406":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.formula.api as smf\nfrom sklearn.preprocessing import PolynomialFeatures\nimport random\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error","cc85f650":"odf = pd.read_csv('..\/input\/diamonds.csv')","4a686b97":"odf.head()","67ecc053":"# Dropping first column of dataframe\nodf.drop(columns=odf.columns[0], axis = 1, inplace=True)\nodf.head(10)","9d4aab41":"odf.describe()","0b737985":"# Categorizing columns according to data types\ncategorical = odf.columns[[1,2,3]]\nto_predict = 'price'\nnumeric = odf.columns[[0,4,5,6,7,8,9]]","2362bbf1":"# Checking for missing values in dataset\nfor c in odf.columns:\n    print('Total Missing values in \\'{}\\' are {}.'.format(c,odf[c].isna().sum()))","be47e253":"# Replacing zero values of x, y and z with NaNs. After then NaN will be dropped\n\nodf[['x','y','z']] = odf[['x','y','z']].replace(0,np.NaN)\n\nodf.dropna(inplace=True)","95abf34a":"plt.figure(figsize=(15,12))\nfor i in range(1,8):\n    plt.subplot(3, 3, i)\n    plt.scatter(odf['price'], odf[numeric[i-1]], s= 1)\n    plt.xlabel(to_predict)\n    plt.ylabel(numeric[i-1])","6c2cc63f":"odf.drop(odf[odf.y > 20].index, inplace=True) # Dropping outliers from 'y' column\nodf.drop(odf[odf.z > 20].index, inplace=True) # Dropping outliers from 'z' column\n\nplt.figure(figsize=(15,12))\nfor i in range(1,8):\n    plt.subplot(3, 3, i)\n    plt.scatter(odf['price'], odf[numeric[i-1]], s= 1)\n    plt.xlabel(to_predict)\n    plt.ylabel(numeric[i-1])","808baa34":"p = sns.heatmap(odf.corr('spearman'), center=0, cmap = 'RdYlGn')","a8ef30b5":"# Making dummy variables for categorical Columns\nodfd = pd.get_dummies(data=odf, columns=categorical)","6ba98dfe":"# Dropping Extra caegoricals\nlst = ['color_D','cut_Fair','clarity_IF']\nidx = []\nfor i in lst:\n    # Removing D color from color column, Fair cut and IF clarity column\n    idx.append(odfd.columns.get_loc(i))\n    \nodfd.drop(columns=odfd.columns[idx], axis = 1, inplace=True)","f1f7d3ee":"# Rearranging columns of dataframe\n\ncol = ['carat', 'depth', 'table', 'x', 'y', 'z', 'cut_Good', 'cut_Ideal', 'cut_Premium', 'cut_Very Good', 'color_E', \n       'color_F', 'color_G', 'color_H', 'color_I', 'color_J', 'clarity_I1', 'clarity_SI1', 'clarity_SI2', 'clarity_VS1',\n       'clarity_VS2', 'clarity_VVS1', 'clarity_VVS2', 'price']\n\nodfd = odfd[col]","ff57022e":"plt.figure(figsize=(25,12))\np = sns.heatmap(data=odfd.corr(method='spearman'), annot=True, cmap='RdYlGn', center=0)","14af5e45":"# Automating backward elimination technique\n\ndef DoBackwardElimination(the_regressor, X, y, minP2eliminate):\n    \n    assert np.shape(X)[0] == np.shape(y)[0], 'Length of X and y do not match'\n    assert minP2eliminate > 0, 'Minimum P value to eliminate cannot be zero or negative'\n    \n    original_list = list(range(0, np.shape(the_regressor.pvalues)[0]))\n    \n    max_p = 10        # Initializing with random value of maximum P value\n    i = 0\n    r2adjusted = []   # Will store R Square adjusted value for each loop\n    r2 = []           # Will store R Square value  for each loop\n    list_of_originallist = [] # Will store modified index of X at each loop\n    classifiers_list = [] # fitted classifiers at each loop\n    \n    while max_p >= minP2eliminate:\n        \n        p_values = list(the_regressor.pvalues)\n        r2adjusted.append(the_regressor.rsquared_adj)\n        r2.append(the_regressor.rsquared)\n        list_of_originallist.append(original_list)\n        \n        max_p = max(p_values)\n        max_p_idx = p_values.index(max_p)\n        \n        if max_p_idx == 0:\n            \n            temp_p = set(p_values)\n            \n            # removing the largest element from temp list\n            temp_p.remove(max(temp_p))\n            \n            max_p = max(temp_p)\n            max_p_idx = p_values.index(max_p)\n            \n            print('Index value 0 found!! Next index value is {}'.format(max_p_idx))\n            \n            if max_p < minP2eliminate:\n                \n                print('Max P value found less than 0.1 with 0 index ...Loop Ends!!')\n                \n                break\n                \n        if max_p < minP2eliminate:\n            \n            print('Max P value found less than 0.1 without 0 index...Loop Ends!!')\n            \n            break\n        \n        val_at_idx = original_list[max_p_idx]\n        \n        idx_in_org_lst = original_list.index(val_at_idx)\n        \n        original_list.remove(val_at_idx)\n        \n        print('Popped column index out of original array is {} with P-Value {}'.format(val_at_idx, np.round(np.array(p_values)[max_p_idx], decimals= 4)))\n        \n        X_new = X[:, original_list]\n        \n        the_regressor = smf.OLS(endog = y, exog = X_new).fit()\n        classifiers_list.append(the_regressor)\n        \n        print('==================================================================================================')\n        \n    return classifiers_list, r2, r2adjusted, list_of_originallist","bbb21d4b":"# Preprocessing data\n\nX = odfd.iloc[:,:-1].values          # Selecting all columns except last one that is 'price'.\ny = odfd['price'].values\n\n# # Adding constant values at start of array X\n# X = np.append(arr = np.ones((X.shape[0], 1)).astype(int), values=X, axis=1)","7ce1f33f":"# Scaling input data\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nmscalar = MinMaxScaler()\n\nX_minmax_scaled = mscalar.fit_transform(X)","0414f490":"# Adding constant values at start of array X\nX_minmax_scaled = np.append(arr = np.ones((X_minmax_scaled.shape[0], 1)).astype(int), values=X_minmax_scaled, axis=1)","47c8029a":"X_lst = list(range(0, X.shape[1]))\nX_opt = X_minmax_scaled[:,X_lst]\nflag = 2 # Done intentionally in order to stop popping 5th column from input array X","9efcd552":"# To be used when doing manual elimination\n# Eliminating columns according to P values from summary to make new X_opt\nidx_to_pop = 5\nif flag == 1:\n    X_lst.pop(element_to_pop)\n    flag = 99\n    \nX_opt = X_minmax_scaled[:,X_lst]","f393b438":"# Splitting data for taining and testing\nX_train, X_test, y_train, y_test = train_test_split(X_opt, y, test_size=0.25, random_state=22)","b8fe2321":"# Fitting the regressor\nregressor_SLR_OLS = smf.OLS(endog = y_train, exog = X_train).fit()\n\n# Looking at the summary of regressor\nprint(regressor_SLR_OLS.summary())","1b2ae601":"sns.set(style='ticks')\nplt.figure(figsize=(14,8))\ny_pred_train = regressor_SLR_OLS.predict(X_train)\ny_pred_test = regressor_SLR_OLS.predict(X_test)\n\nplt.scatter(y_train, y_train-y_pred_train, s= 2, c= 'R', alpha=0.8)\nplt.scatter(y_test, y_test-y_pred_test, s= 2, c= 'B', alpha=0.8)\nplt.plot([0, 18000], [0,0], '-k', linewidth = 3, alpha = 0.3)\np = plt.xlim((0, 18000))\np = plt.ylim((-20000, 20000))\np = plt.legend(['Reference Line','Training Data Residual', 'Testing Data Residual'])\np = plt.title('Residual Graph')","93174681":"def Calculate_Error(original_values, predicted_values):\n    assert len(original_values) == len(predicted_values), 'Both list should have same length'\n    temp = 0\n    error = 0\n    n = len(original_values)\n    for o, p in zip(original_values, predicted_values):\n        temp = temp + ((o-p)**2)\n        \n    temp = temp\/n\n    error = np.sqrt(temp)\n    return error\n        ","d194b481":"train_data_error = Calculate_Error(original_values=y_train, predicted_values=y_pred_train)\ntest_data_error = Calculate_Error(original_values=y_test, predicted_values=y_pred_test)\n\nprint('MSE for training data is {}'.format(np.round(train_data_error, 4)))\nprint('MSE for testing data is {}'.format(np.round(test_data_error, 4)))","571211eb":"random.seed(22)\nplt.figure(figsize=(15, 10))\nrand_nums1 = random.sample(range(0, len(y_train)), 50)\nplt.subplot(2,1,1)\n# plt.figure(figsize=(15,10))\np = plt.plot(range(0, 50), y_train[rand_nums1, ], color = 'red')\np = plt.plot(range(0, 50), y_pred_train[rand_nums1, ], color = 'blue')\nplt.title(r'$ \\mathrm{\\mathsf{Training Data Output}}$')\nplt.legend(['Original Output', 'Predicted Output'])\nplt.xlabel(r'$Observation  Number \\longrightarrow$')\nplt.ylabel(r'$Output \\longrightarrow$')\nplt.ylim(-5000, 25000)\n\n\nrand_nums2 = random.sample(range(0, len(y_test)), 50)\nplt.subplot(2,1,2)\np = plt.plot(range(0, 50), y_test[rand_nums2, ], color = 'red')\np = plt.plot(range(0, 50), y_pred_test[rand_nums2, ], color = 'blue')\nplt.title(r'$ \\mathrm{\\mathsf{Testing Data Output}}$')\nplt.legend(['Original Output', 'Predicted Output'])\nplt.xlabel(r'$Observation  Number \\longrightarrow$')\nplt.ylabel(r'$Output \\longrightarrow$')\nplt.ylim(-5000, 25000)\n\nplt.subplots_adjust(hspace=0.4)","8ffadb25":"training_residual = []\nfor o, p in zip(y_train[rand_nums1, ], y_pred_train[rand_nums1, ]):\n    training_residual.append(o-p)\n    \ntesting_residual = []\nfor o, p in zip(y_test[rand_nums2, ], y_pred_test[rand_nums2, ]):\n    testing_residual.append(o-p)","49a78e08":"plt.figure(figsize=(15,10))\n\nplt.subplot(2,1,1)\n# p = plt.bar(list(range(0, 50)),training_residual, align='center', width = 1, fill= False, edgecolor = 'k')\np = plt.scatter(list(range(0, 50)),training_residual)\np = plt.plot([0, 50], [0, 0], '-k')\nplt.text(30,3500, 'Mean of residual is {}'.format(np.round(np.mean(training_residual), 4)))\nplt.ylim(-5000, 5000)\nplt.xlim(0,50)\nplt.title('Training Residuals')\nplt.xlabel(r'$Observation  Number \\longrightarrow$')\nplt.ylabel(r'$Residual \\longrightarrow$')\n\n\nplt.subplot(2,1,2)\n# p = plt.bar(list(range(0, 50)),testing_residual, align='center', width = 1, fill= False, edgecolor = 'k')\np = plt.scatter(list(range(0, 50)),testing_residual)\np = plt.plot([0, 50], [0, 0], '-k')\nplt.text(30,3500, 'Mean of residual is {}'.format(np.round(np.mean(testing_residual), 4)))\nplt.ylim(-5000, 5000)\nplt.xlim(0,50)\nplt.title('Testing Residuals')\nplt.xlabel(r'$Observation  Number \\longrightarrow$')\nplt.ylabel(r'$Residual \\longrightarrow$')\n\nplt.subplots_adjust(hspace= 0.4)","d95c45bb":"print('Mean of \\'price\\' for training data is {}.'.format(np.round(np.mean(y_train),4)))\nprint('Mean of residual of \\'price\\' for training data is {}.'.format(np.round(np.mean(training_residual),4)))\n\nprint('Mean of \\'price\\' for testing data is {}.'.format(np.round(np.mean(y_test),4)))\nprint('Mean of residual of \\'price\\' for training data is {}.'.format(np.round(np.mean(testing_residual),4)))","35fdce0b":"# Now it's time to do backward elimination and check wether it improves performance of our regression model\n\nregressor_list, r2, r2adjusted, list_of_changes = DoBackwardElimination(the_regressor=regressor_SLR_OLS, \n                                                                        X= X_train, y= y_train, minP2eliminate = 0.05)","7cc8d199":"new_list = list_of_changes[1]\nnew_regressor = regressor_list[0]\nprint(new_regressor.summary())","cbff8263":"y_pred_train_new = new_regressor.predict(X_train[:, new_list])\ny_pred_test_new = new_regressor.predict(X_test[:, new_list])\n\ntraining_residual_new = []\nfor o, p in zip(y_train[rand_nums1, ], y_pred_train_new[rand_nums1, ]):\n    training_residual_new.append(o-p)\n    \ntesting_residual_new = []\nfor o, p in zip(y_test[rand_nums2, ], y_pred_test_new[rand_nums2, ]):\n    testing_residual_new.append(o-p)","534dfdbe":"print('Mean of \\'price\\' for training data is {}.'.format(np.round(np.mean(y_train),4)))\nprint('Mean of residual of \\'price\\' for training data is {}.'.format(np.round(np.mean(training_residual_new),4)))\n\nprint('Mean of \\'price\\' for testing data is {}.'.format(np.round(np.mean(y_test),4)))\nprint('Mean of residual of \\'price\\' for training data is {}.'.format(np.round(np.mean(testing_residual_new),4)))","3fb8393b":"<a id='conclusion_be'><\/a>\n* From above values of residual means, it is evident that eliminating a feature actually not improving the model efficiency rather decreasing it's performance.\n* For both training and testing data mean residual increased by 0.02% and 0.04% respectively, which is not a very big value but still not what we crave for.","b5bf985d":"* From above figure, It looks like 'y' and 'z' have some outliers. Dropping them might increase the accuracy.","51fcac5b":"<a id='summary'><\/a>\n## SUMMARY:\n**The best model for this dataset is 'regressor_SLR_OLS' with R Square and Adjusted R Square of 0.921**","b93bd2eb":"### From figures above it seems like few independent variables or features don't have linear relationship with the dependent variable.","dd01d667":"<a id='origvspred_graph'><\/a>\n## Original v\/s Predicted values graph ","369e39c7":"<a id='residual_graph'><\/a>\n## Residual Graph without using Backward Elimination","6afd322b":"<a id='heatmap_withdummy'><\/a>\n## Heatmap with Dummy Variables","a614d896":"<a id='heatmap_nodummy'><\/a>\n##  Heatmap without dummy variables","9fd58840":"* From above figure it can be seen that carat is highly correlated to x, y, x and vice-versa. Apart from that, dependent variable is also highly corelated to these four variables.","17da6eec":"<a id='OLS_regressor'><\/a>\n## Simple Linear Regression using OLS","3345e981":"<a id='conclusion'><\/a>\n### Following things can be concluded regarding model on the bases of above values:\n* Mean of residual for training data is 183.83 which is about 4.7% of mean diamond price in training data.\n* Same is with the residual for testing data set. It's residual is about 1.24% of mean diamond price in same set.\n* Also, mean price of diamonds in both training and testing dataset is almost equal thus not causing any bias in residual.","17ac34ec":"## In order to increase efficiency of this model, we will go for other techniques like polynomial fitting in next notebook.","5244a45e":"### Heatmap above shows a strong correlation between various independent (as assumed) variables.","c83732c3":"<a id='residual_plot'><\/a>\n## Residual Scatter Plot","1736bc8d":"## This code is for demonstration of using regression model (in my case Ordinary Least Square (OLS) model from statsmodels library) along with Backward Elimination Technique.","e6435cdf":"<a id='rms_function'><\/a>\n## Function for calculating RMS Error","35f752aa":"### Content\n\n* *[Heatmap without dummy variables](#heatmap_nodummy)*\n\n* *[Heatmap with Dummy Variables](#heatmap_withdummy)*\n\n* *[Function for Automatic Backward Elimination](#elimination_code)*\n\n* *[Simple Linear Regression using OLS](#OLS_regressor)*\n\n* *[Residual Graph without Backward Elimination](#residual_graph)*\n\n* *[Function for calculating RMS Error](#rms_function)*\n\n* *[Original v\/s Predicted values graph](#origvspred_graph)*\n\n* *[Residual Scatter Plot](#residual_plot)*\n\n* *[Conclusion of Model (without using Backward Elimination) ](#conclusion)*\n\n* *[Conclusion of Model (after using Backward Elimination) ](#conclusion_be)*\n\n* *[Summary](#summary)*","d664127d":"<a id='elimination_code'><\/a>\n## Function for Automatic Backward Elimination"}}