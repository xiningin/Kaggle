{"cell_type":{"5cb7dcdd":"code","de4192db":"code","e0e9f081":"code","27700470":"code","640c4c74":"code","121217a9":"code","cb9b5362":"code","868dcaa0":"code","2571e4cb":"code","005393bd":"code","fe9a8a67":"code","c7a6f2e1":"code","52d7594e":"code","418e59f4":"code","03d73fc9":"code","1fa82dab":"code","fabb4905":"code","252d8ce8":"code","f60818d4":"code","c0f1c587":"code","484cbed2":"code","6da76b58":"code","199da612":"code","9d69b505":"code","584cdd61":"code","003a0f5f":"code","2b747426":"code","21ed052c":"code","9a88de36":"code","c1d24521":"code","ce6b9d65":"code","c5a83118":"code","5faaeba7":"code","17df6953":"code","8a4888ff":"code","2277a5fd":"code","dea87fc4":"code","fc8b2de2":"code","140ff838":"code","223dc75f":"markdown","ba4a9881":"markdown","9d601b85":"markdown","8be87c59":"markdown","5bf3c6d0":"markdown","34c9d062":"markdown","a570573d":"markdown","b928b7ec":"markdown","b8eef0e0":"markdown","784b894b":"markdown"},"source":{"5cb7dcdd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom scipy.cluster.hierarchy import dendrogram,linkage\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","de4192db":"dataset=pd.read_csv(\"..\/input\/heart.csv\")","e0e9f081":"dataset.head()","27700470":"sns.countplot(x=dataset.sex,palette=\"bwr\")\nplt.show()","640c4c74":"dataset.sex.value_counts().index.values","121217a9":"# AND\nprint(\"Female count {} and %{}\".format(len(dataset[dataset.sex==0]),(len(dataset[dataset.sex==0]*100)\/len(dataset.sex)*100)))\n# AND\nprint(\"Male count {} and %{}\".format(len(dataset[dataset.sex==1]),(len(dataset[dataset.sex==1]*100)\/len(dataset.sex)*100)))","cb9b5362":"sns.countplot(x=dataset.target, palette=\"bwr\")\nplt.show()","868dcaa0":"# AND\nprint(\"Havent Heart Disease count {} and %{}\".format(len(dataset[dataset.target==0]),(len(dataset[dataset.target==0]*100)\/len(dataset.target)*100)))\n# AND\nprint(\"Have Heart Disease count {} and %{}\".format(len(dataset[dataset.target==1]),(len(dataset[dataset.target==1]*100)\/len(dataset.target)*100)))","2571e4cb":"pd.crosstab(dataset.age,dataset.target).plot(kind=\"bar\",figsize=(20,6))\nplt.title('Heart Disease Frequency for Ages')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.savefig('heartDiseaseAndAges.png')\nplt.show()","005393bd":"pd.crosstab(dataset.sex,dataset.target).plot(kind=\"bar\",figsize=(15,6),color=['#1CA53B','#AA1111' ])\nplt.title('Heart Disease Frequency for Sex')\nplt.xlabel('Sex (0 = Female, 1 = Male)')\nplt.xticks(rotation=0)\nplt.legend([\"Haven't Disease\", \"Have Disease\"])\nplt.ylabel('Frequency')\nplt.show()","fe9a8a67":"#dataset.drop([\"age\"],axis=1,inplace=True)","c7a6f2e1":"dataset.head()","52d7594e":"x_data=dataset.drop([\"target\"],axis=1)\ny=dataset.target.values","418e59f4":"#Normalize\nx=(x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))","03d73fc9":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)","1fa82dab":"x_train.shape,x_test.shape # All 303 Rows","fabb4905":"x_train=x_train.T\ny_train=y_train.T\nx_test=x_test.T\ny_test=y_test.T\nx_train.shape,x_test.shape ,y_train.shape , y_test.shape","252d8ce8":"def initialize_weight_bias(x_train):\n    weight=np.full((x_train.shape[0],1),0.1)\n    bias=0.0\n    return weight,bias","f60818d4":"def sigmoid(z):\n    y_head=1\/(1+np.exp(-z))\n    return y_head","c0f1c587":"def forward_backward_propagation(x_train,y_train,weight,bias):\n    y_head= sigmoid(np.dot(weight.T,x_train) + bias)\n    loss = -(y_train*np.log(y_head) + (1-y_train)*np.log(1-y_head))\n    cost = np.sum(loss) \/ x_train.shape[1]\n    \n    #backward propagation\n    derivative_weight = np.dot(x_train,((y_head-y_train).T))\/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]\n    gradients = {\"Derivative Weight\" : derivative_weight, \"Derivative Bias\" : derivative_bias}\n    \n    return cost,gradients ","484cbed2":"def update(x_train,y_train,weight,bias,learningRate,iteration):\n    costlist=[]\n    cost2list2=[]\n    index=[]\n    for i in range(iteration):\n        cost,gradients = forward_backward_propagation(x_train,y_train,weight,bias)\n        costlist.append(cost)\n        weight = weight - learningRate * gradients[\"Derivative Weight\"]\n        bias = bias - learningRate * gradients[\"Derivative Bias\"]\n        if i%10==0:\n            cost2list2.append(cost)\n            index.append(i)\n            print(\"Cost after iteration %i: %f\" %(i,cost))\n    parameters = {\"weight\": weight,\"bias\": bias}\n    plt.plot(index,cost2list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters,gradients,costlist","6da76b58":"def predict(x_test,weight,bias):\n    z=sigmoid(np.dot(weight.T,x_test)+bias)\n    Y_prediction=np.zeros((1,x_test.shape[1]))\n    for i in range(z.shape[1]):\n        if(z[0,i]<=0.5):\n            Y_prediction[0,i]=0\n        else:\n            Y_prediction[0,i]=1\n    return Y_prediction","199da612":"def logistic_regression(x_train,y_train,x_test,y_test,learning_rate,num_iterations):\n    weight,bias=initialize_weight_bias(x_train)\n    parameters,gradients,cost_list=update(x_train,y_train,weight,bias,learning_rate,num_iterations)\n    y_prediction_test=predict(x_test,parameters[\"weight\"],parameters[\"bias\"])\n    #\n    print(\"test accuracy : {} \".format(100-np.mean(np.abs(y_prediction_test-y_test))*100))","9d69b505":"logistic_regression(x_train,y_train,x_test,y_test,learning_rate=1,num_iterations=150)","584cdd61":"lr=LogisticRegression()\nlr.fit(x_train.T,y_train.T)\nprint(\"Accuracy : \",lr.score(x_test.T,y_test.T))\n# YEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEY -.- ^.^","003a0f5f":"x_train2,x_test2,y_train2,y_test2=train_test_split(x,y,test_size=0.2,random_state=0)\nfrom sklearn.neighbors import KNeighborsClassifier\nscore=[]\nfor each in range(1,20):\n    knn01=KNeighborsClassifier(n_neighbors=each)\n    knn01.fit(x_train2,y_train2)\n    score.append(knn01.score(x_test2,y_test2))\nplt.plot(range(1,20),score,color=\"red\")\nplt.show()","2b747426":"knn=KNeighborsClassifier(n_neighbors=7)\nknn.fit(x_train2,y_train2)\nprint(\"Accuracy :%{}\".format((knn.score(x_test2,y_test2))*100))","21ed052c":"# Support Vector Machine\nfrom sklearn.svm import SVC\n\nsvm=SVC(random_state=1)\nsvm.fit(x_train2,y_train2)\n\n# %% print\n\nprint(\"accuracy of svm algo:\" ,svm.score(x_test2,y_test2))","9a88de36":"\nfrom sklearn.naive_bayes import GaussianNB\n\nnaive=GaussianNB();\nnaive.fit(x_train2,y_train2)\n\n# %% print\nprint(\"accuracy of naivebayes algo:\" ,naive.score(x_test2,y_test2))","c1d24521":"from keras.wrappers.scikit_learn import KerasClassifier\n# %%\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n# %%\ndef build_classifier():\n    classifier = Sequential() # initialize neural network\n    classifier.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu', input_dim = x_train2.shape[1]))\n    classifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu'))\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return classifier\nclassifier = KerasClassifier(build_fn = build_classifier, epochs = 100)\naccuracies = cross_val_score(estimator = classifier, X = x_train2, y = y_train2, cv = 3)\nmean = accuracies.mean()\nvariance = accuracies.std()\nprint(\"Accuracy mean: \"+ str(mean))\nprint(\"Accuracy variance: \"+ str(variance))","ce6b9d65":"from sklearn.cluster import KMeans\nwcss=[]\ndatasetkmeans=dataset.drop([\"target\"],axis=1)\nfor each in range(1,15):\n    kmeans=KMeans(n_clusters=each)\n    kmeans.fit(datasetkmeans)\n    wcss.append(kmeans.inertia_)\n    \nplt.plot(range(1,15),wcss)\nplt.xlabel(\"number of k value\")\nplt.ylabel(\"WCSS\")\nplt.show()\n# %% for K=2  \n","c5a83118":"kmeans2=KMeans(n_clusters=2)\nclusters=kmeans2.fit_predict(datasetkmeans)\ndatasetkmeans[\"label\"]=clusters\nplt.scatter(datasetkmeans.trestbps[datasetkmeans.label==0],datasetkmeans.chol[datasetkmeans.label==0],color=\"red\",label=\"Havent Heart Disease\")\nplt.scatter(datasetkmeans.trestbps[datasetkmeans.label==1],datasetkmeans.chol[datasetkmeans.label==1],color=\"blue\",label=\"Have Heart Disease\")\nplt.title(\"Manual\")\nplt.legend()\nplt.show()\n\nplt.scatter(dataset.trestbps[dataset.target==0],dataset.chol[dataset.target==0],color=\"red\",label=\"Havent Heart Disease\")\nplt.scatter(dataset.trestbps[dataset.target==1],dataset.chol[dataset.target==1],color=\"blue\",label=\"Have Heart Disease\")\nplt.title(\"Auto\")\nplt.legend()\nplt.show()","5faaeba7":"dataset.head()","17df6953":"from scipy.cluster.hierarchy import dendrogram,linkage\ndatasetexcept=dataset.drop([\"target\"],axis=1)\nmerg=linkage(dataset,method=\"ward\")\ndendrogram(merg,leaf_rotation=90)\nplt.xlabel(\"Data Point\")\nplt.ylabel(\"Euclidean Distance\")\nplt.show()\n# %% HCC\nfrom sklearn.cluster import AgglomerativeClustering\n\nhierarcical=AgglomerativeClustering(n_clusters=2,affinity=\"euclidean\",linkage=\"ward\")\n\nclusters=hierarcical.fit_predict(x)\n\ndataset[\"label\"]=clusters\n\nplt.scatter(dataset.trestbps[dataset.label==0],dataset.chol[dataset.label==0],color=\"red\",label=\"Havent Heart Disease\")\nplt.scatter(dataset.trestbps[dataset.label==1],dataset.chol[dataset.label==1],color=\"blue\",label=\"Have Heart Disease\")\nplt.title(\"Manual\")\nplt.legend()\nplt.show()\n\nplt.scatter(dataset.trestbps[dataset.target==0],dataset.chol[dataset.target==0],color=\"red\",label=\"Havent Heart Disease\")\nplt.scatter(dataset.trestbps[dataset.target==1],dataset.chol[dataset.target==1],color=\"blue\",label=\"Have Heart Disease\")\nplt.title(\"Auto\")\nplt.legend()\nplt.show()\nfitaa=list(hierarcical.fit_predict(X=dataset.label.values.reshape(-1,1),y=dataset.target.values.reshape(-1,1)))\nfitbb=dataset.target\ndictionaryab={\"predict\":fitaa,\"real\":fitbb}\ndatasetab=pd.DataFrame(dictionaryab)\nlen(datasetab[datasetab.predict==datasetab.real]),len(datasetab.predict),len(datasetab.real)\n# TOO BAD like %33","8a4888ff":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=8) # n_neighbors K de\u011feridir.\n\n# %% K Fold CV K=10\n\nfrom sklearn.model_selection import cross_val_score\n\naccuracies=cross_val_score(estimator=knn,X=x_train2,y=y_train2,cv=10)\nprint(\"Average accuracy :\",np.mean(accuracies))\nprint(\"Average Std :\",np.std(accuracies))\n\n# %% TEST\nknn.fit(x_train2,y_train2)\nprint(\"Test accuracy {}\".format(knn.score(x_test2,y_test2)))\n\n# %% Grid Search K-Fold CV\n\nfrom sklearn.model_selection import GridSearchCV\n\ngrid={\"n_neighbors\":np.arange(1,50)}\nknn=KNeighborsClassifier()\nknn_cv=GridSearchCV(knn,grid,cv=10)\nknn_cv.fit(x,y)\n\n# %% print hyperparamet KNN algorithm K De\u011feri\n\nprint(\"tuned hyperparameter K:\",knn_cv.best_params_)\n\nprint(\"tuned parametreye g\u00f6re en iyi accuracy(best_score)  :\",knn_cv.best_score_)\n","2277a5fd":"# GROUP BY \"target\"\ndataset.groupby(\"target\").mean()","dea87fc4":"# VALUE COUNTS\ndataset.target.value_counts()","fc8b2de2":"#DEC\u0130S\u0130ON TREE\nfrom sklearn.tree import DecisionTreeClassifier\ndt=DecisionTreeClassifier()\ndt.fit(x_train2,y_train2)\nprint(\"Score : \",dt.score(x_test2,y_test2))","140ff838":"# %% Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier(n_estimators=100,random_state=42)\nrf.fit(x_train2,y_train2)\nprint(\"random forest algo result : \",rf.score(x_test2,y_test2))","223dc75f":"****Deep Learning - ANN - With Keras Libraries****","ba4a9881":"****Decision Tree****","9d601b85":"****Random Forest****","8be87c59":"****K-Fold Cross Validation****","5bf3c6d0":"****K-Means Clustering****","34c9d062":"**SUPPORT VECTOR MACHINE**","a570573d":"***GAUSSIAN***","b928b7ec":"****KNN****","b8eef0e0":"****LOGISTIC REGRESSION****","784b894b":"****Hierarcical Clustering****"}}