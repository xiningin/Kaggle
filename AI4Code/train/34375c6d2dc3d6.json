{"cell_type":{"ac864e26":"code","3f2a0174":"code","3b338a39":"code","e1bd6848":"code","cac3a4d6":"code","eb314c2b":"code","51e6c86d":"code","f5089d62":"code","b0223e76":"markdown","6a590fcf":"markdown","635f4154":"markdown","27220f5e":"markdown","4e2f8db5":"markdown","b169b2d5":"markdown","f279b33a":"markdown","00df4683":"markdown","6f08f883":"markdown","3d160cfc":"markdown"},"source":{"ac864e26":"# Import necessary modules for data analysis and data visualization. \n# Data analysis modules\n# Pandas is probably the most popular and important modules for any work related to data management. \nimport pandas as pd\n\n# numpy is a great library for doing mathmetical operations. \nimport numpy as np\n\n# Some visualization libraries\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\n## Importing the datasets\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\n\n\n## Some other snippit of codes to get the setting right \n## This is so that the chart created by matplotlib can be shown in the jupyter notebook. \n%matplotlib inline \n%config InlineBackend.figure_format = 'retina' ## This is preferable for retina display. \nimport warnings ## importing warnings library. \nwarnings.filterwarnings('ignore') ## Ignore warning\nimport os ## imporing os\nprint(os.listdir(\"..\/input\/\")) ","3f2a0174":"## Take a look at the overview of the dataset. \ntrain.sample(5)","3b338a39":"test.sample(5)","e1bd6848":"## saving passenger id in advance in order to submit later. \npassengerid = test.PassengerId\n\n## Replacing the null values in the Embarked column with the mode. \ntrain.Embarked.fillna(\"C\", inplace=True)\n\n## Concat train and test into a variable \"all_data\"\nsurvivers = train.Survived\n\ntrain.drop([\"Survived\"],axis=1, inplace=True)\n\nall_data = pd.concat([train,test], ignore_index=False)\n\n## Assign all the null values to N\nall_data.Cabin.fillna(\"N\", inplace=True)\n\nall_data.Cabin = [i[0] for i in all_data.Cabin]\n\nwith_N = all_data[all_data.Cabin == \"N\"]\n\nwithout_N = all_data[all_data.Cabin != \"N\"]\n\nall_data.groupby(\"Cabin\")['Fare'].mean().sort_values()\n\ndef cabin_estimator(i):\n    a = 0\n    if i<16:\n        a = \"G\"\n    elif i>=16 and i<27:\n        a = \"F\"\n    elif i>=27 and i<38:\n        a = \"T\"\n    elif i>=38 and i<47:\n        a = \"A\"\n    elif i>= 47 and i<53:\n        a = \"E\"\n    elif i>= 53 and i<54:\n        a = \"D\"\n    elif i>=54 and i<116:\n        a = 'C'\n    else:\n        a = \"B\"\n    return a\n    \n\n##applying cabin estimator function. \nwith_N['Cabin'] = with_N.Fare.apply(lambda x: cabin_estimator(x))\n\n## getting back train. \nall_data = pd.concat([with_N, without_N], axis=0)\n\n## PassengerId helps us separate train and test. \nall_data.sort_values(by = 'PassengerId', inplace=True)\n\n## Separating train and test from all_data. \ntrain = all_data[:891]\n\ntest = all_data[891:]\n\n# adding saved target variable with train. \ntrain['Survived'] = survivers\n\nmissing_value = test[(test.Pclass == 3) & (test.Embarked == \"S\") & (test.Sex == \"male\")].Fare.mean()\n## replace the test.fare null values with test.fare mean\ntest.Fare.fillna(missing_value, inplace=True)\n\n## dropping the three outliers where Fare is over $500 \ntrain = train[train.Fare < 500]\n\n# Placing 0 for female and \n# 1 for male in the \"Sex\" column. \ntrain['Sex'] = train.Sex.apply(lambda x: 0 if x == \"female\" else 1)\ntest['Sex'] = test.Sex.apply(lambda x: 0 if x == \"female\" else 1)\n\n# Creating a new colomn with a \ntrain['name_length'] = [len(i) for i in train.Name]\ntest['name_length'] = [len(i) for i in test.Name]\n\ndef name_length_group(size):\n    a = ''\n    if (size <=20):\n        a = 'short'\n    elif (size <=35):\n        a = 'medium'\n    elif (size <=45):\n        a = 'good'\n    else:\n        a = 'long'\n    return a\n\n\ntrain['nLength_group'] = train['name_length'].map(name_length_group)\ntest['nLength_group'] = test['name_length'].map(name_length_group)\n\n## Here \"map\" is python's built-in function. \n## \"map\" function basically takes a function and \n## returns an iterable list\/tuple or in this case series. \n## However,\"map\" can also be used like map(function) e.g. map(name_length_group) \n## or map(function, iterable{list, tuple}) e.g. map(name_length_group, train[feature]]). \n## However, here we don't need to use parameter(\"size\") for name_length_group because when we \n## used the map function like \".map\" with a series before dot, we are basically hinting that series \n## and the iterable. This is similar to .append approach in python. list.append(a) meaning applying append on list. \n\n## cuts the column by given bins based on the range of name_length\n#group_names = ['short', 'medium', 'good', 'long']\n#train['name_len_group'] = pd.cut(train['name_length'], bins = 4, labels=group_names)\n\n## Title\n## get the title from the name\ntrain[\"title\"] = [i.split('.')[0] for i in train.Name]\ntrain[\"title\"] = [i.split(',')[1] for i in train.title]\ntest[\"title\"] = [i.split('.')[0] for i in test.Name]\ntest[\"title\"]= [i.split(',')[1] for i in test.title]\n\n#rare_title = ['the Countess','Capt','Lady','Sir','Jonkheer','Don','Major','Col']\n#train.Name = ['rare' for i in train.Name for j in rare_title if i == j]\n## train Data\ntrain[\"title\"] = [i.replace('Ms', 'Miss') for i in train.title]\ntrain[\"title\"] = [i.replace('Mlle', 'Miss') for i in train.title]\ntrain[\"title\"] = [i.replace('Mme', 'Mrs') for i in train.title]\ntrain[\"title\"] = [i.replace('Dr', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Col', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Major', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Don', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Jonkheer', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Sir', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Lady', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Capt', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('the Countess', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Rev', 'rare') for i in train.title]\n\n\n\n#rare_title = ['the Countess','Capt','Lady','Sir','Jonkheer','Don','Major','Col']\n#train.Name = ['rare' for i in train.Name for j in rare_title if i == j]\n## test data\ntest['title'] = [i.replace('Ms', 'Miss') for i in test.title]\ntest['title'] = [i.replace('Dr', 'rare') for i in test.title]\ntest['title'] = [i.replace('Col', 'rare') for i in test.title]\ntest['title'] = [i.replace('Dona', 'rare') for i in test.title]\ntest['title'] = [i.replace('Rev', 'rare') for i in test.title]\n\n## Family_size seems like a good feature to create\ntrain['family_size'] = train.SibSp + train.Parch+1\ntest['family_size'] = test.SibSp + test.Parch+1\n\ndef family_group(size):\n    a = ''\n    if (size <= 1):\n        a = 'loner'\n    elif (size <= 4):\n        a = 'small'\n    else:\n        a = 'large'\n    return a\n\ntrain['family_group'] = train['family_size'].map(family_group)\ntest['family_group'] = test['family_size'].map(family_group)\n\ntrain['is_alone'] = [1 if i<2 else 0 for i in train.family_size]\ntest['is_alone'] = [1 if i<2 else 0 for i in test.family_size]\n\ntrain.drop(['Ticket'], axis=1, inplace=True)\n\ntest.drop(['Ticket'], axis=1, inplace=True)\n\n## Calculating fare based on family size. \ntrain['calculated_fare'] = train.Fare\/train.family_size\ntest['calculated_fare'] = test.Fare\/test.family_size\n\ndef fare_group(fare):\n    a= ''\n    if fare <= 4:\n        a = 'Very_low'\n    elif fare <= 10:\n        a = 'low'\n    elif fare <= 20:\n        a = 'mid'\n    elif fare <= 45:\n        a = 'high'\n    else:\n        a = \"very_high\"\n    return a\n\ntrain['fare_group'] = train['calculated_fare'].map(fare_group)\ntest['fare_group'] = test['calculated_fare'].map(fare_group)\n\n#train['fare_group'] = pd.cut(train['calculated_fare'], bins = 4, labels=groups)\n\ntrain.drop(['PassengerId'], axis=1, inplace=True)\n\ntest.drop(['PassengerId'], axis=1, inplace=True)\n\n\ntrain = pd.get_dummies(train, columns=['title',\"Pclass\", 'Cabin','Embarked','nLength_group', 'family_group', 'fare_group'], drop_first=False)\ntest = pd.get_dummies(test, columns=['title',\"Pclass\",'Cabin','Embarked','nLength_group', 'family_group', 'fare_group'], drop_first=False)\ntrain.drop(['family_size','Name', 'Fare','name_length'], axis=1, inplace=True)\ntest.drop(['Name','family_size',\"Fare\",'name_length'], axis=1, inplace=True)\n\n## rearranging the columns so that I can easily use the dataframe to predict the missing age values. \ntrain = pd.concat([train[[\"Survived\", \"Age\", \"Sex\",\"SibSp\",\"Parch\"]], train.loc[:,\"is_alone\":]], axis=1)\ntest = pd.concat([test[[\"Age\", \"Sex\"]], test.loc[:,\"SibSp\":]], axis=1)\n\n## Importing RandomForestRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n## writing a function that takes a dataframe with missing values and outputs it by filling the missing values. \ndef completing_age(df):\n    ## gettting all the features except survived\n    age_df = df.loc[:,\"Age\":] \n    \n    temp_train = age_df.loc[age_df.Age.notnull()] ## df with age values\n    temp_test = age_df.loc[age_df.Age.isnull()] ## df without age values\n    \n    y = temp_train.Age.values ## setting target variables(age) in y \n    x = temp_train.loc[:, \"Sex\":].values\n    \n    rfr = RandomForestRegressor(n_estimators=1500, n_jobs=-1)\n    rfr.fit(x, y)\n    \n    predicted_age = rfr.predict(temp_test.loc[:, \"Sex\":])\n    \n    df.loc[df.Age.isnull(), \"Age\"] = predicted_age\n    \n\n    return df\n\n## Implementing the completing_age function in both train and test dataset. \ncompleting_age(train)\ncompleting_age(test);\n\n## create bins for age\ndef age_group_fun(age):\n    a = ''\n    if age <= 1:\n        a = 'infant'\n    elif age <= 4: \n        a = 'toddler'\n    elif age <= 13:\n        a = 'child'\n    elif age <= 18:\n        a = 'teenager'\n    elif age <= 35:\n        a = 'Young_Adult'\n    elif age <= 45:\n        a = 'adult'\n    elif age <= 55:\n        a = 'middle_aged'\n    elif age <= 65:\n        a = 'senior_citizen'\n    else:\n        a = 'old'\n    return a\n        \n## Applying \"age_group_fun\" function to the \"Age\" column.\ntrain['age_group'] = train['Age'].map(age_group_fun)\ntest['age_group'] = test['Age'].map(age_group_fun)\n\n## Creating dummies for \"age_group\" feature. \ntrain = pd.get_dummies(train,columns=['age_group'], drop_first=True)\ntest = pd.get_dummies(test,columns=['age_group'], drop_first=True);\n\n\"\"\"train.drop('Age', axis=1, inplace=True)\ntest.drop('Age', axis=1, inplace=True)\"\"\"\n\n# separating our independent and dependent variable\nX = train.drop(['Survived'], axis = 1)\ny = train[\"Survived\"]\n\n\n#age_filled_data_nor = NuclearNormMinimization().complete(df1)\n#Data_1 = pd.DataFrame(age_filled_data, columns = df1.columns)\n#pd.DataFrame(zip(Data[\"Age\"],Data_1[\"Age\"],df[\"Age\"]))\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = .33, random_state = 0)\n\n# Feature Scaling\n## We will be using standardscaler to transform\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\n## transforming \"X_train\"\nX_train = sc.fit_transform(X_train)\n## transforming \"X_test\"\nX_test = sc.transform(X_test)\n\n## transforming \"The testset\"\n#test = sc.transform(test)\n\n## changing calculated_fare type\n#train.calculated_fare = train.calculated_fare.astype(float)\n\n## Using StratifiedShuffleSplit\n## We can use KFold, StratifiedShuffleSplit, StratiriedKFold or ShuffleSplit, They are all close cousins. look at sklearn userguide for more info.   \nfrom sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\ncv = StratifiedShuffleSplit(n_splits = 10, test_size = .25, random_state = 0 ) # run model 10x with 60\/30 split intentionally leaving out 10%\n## Using standard scale for the whole dataset.\n\n## saving the feature names for decision tree display\ncolumn_names = X.columns\n\nX = sc.fit_transform(X)","cac3a4d6":"from sklearn.ensemble import AdaBoostClassifier\n\nadaBoost = AdaBoostClassifier(base_estimator=None,\n                              learning_rate=1.0,\n                              n_estimators=100)\n\nadaBoost.fit(X_train, y_train)\n\ny_pred = adaBoost.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\n\naccuracy_score(y_test, y_pred)","eb314c2b":"\nfrom sklearn.model_selection import GridSearchCV\nn_estimators = [100,140,145,150,160, 170,175,180,185];\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\nlearning_r = [0.1,1,0.01,0.5]\n\nparameters = {'n_estimators':n_estimators,\n              'learning_rate':learning_r\n              \n        }\ngrid = GridSearchCV(AdaBoostClassifier(base_estimator= None, ## If None, then the base estimator is a decision tree.\n                                     ),\n                                 param_grid=parameters,\n                                 cv=cv,\n                                 n_jobs = -1)\ngrid.fit(X,y) ","51e6c86d":"print (grid.best_score_)\nprint (grid.best_params_)\nprint (grid.best_estimator_)","f5089d62":"adaBoost_grid = grid.best_estimator_\nadaBoost_grid.score(X,y)","b0223e76":"<h1>Prerequisites for understanding AdaBoost Classifier<\/h1>\n<ul>\n    <li>[Decision Tree](https:\/\/www.kaggle.com\/masumrumi\/decision-tree-with-titanic-dataset)<\/li>\n    <li>Random Forest( it helps understanding Adaboost better as we compare and contrust)<\/li>\n<\/ul>\n\n\n<h1>AdaBoost Classifier<\/h1>\n\n***\nAdaBoost is an <b>ensemble model<\/b> and is quite different from Bagging. Let's point out the core concepts. \n\n* AdaBoost combines a lot of \"weak learners\"(they are also called stump; a tree with only one node and two leaves) to make classifications.\n* This base model fitting with weak decision trees is an iterative process where each stump is chained one after the other; <b>It cannot run in parallel(a significant drawback of AdaBoost).<\/b>\n* <b>Some stumps get more say in the final classifications than others:<\/b> The model use weights that are assigned to each data point\/raw indicating their \"importance.\" Sample with higher weights have a higher influence on the total error of the next model and gets more priority. The first stump starts with uniformly distributed weight which means, in the beginning, every datapoint have an equal amount of weights. \n* <b>Each stump is made by taking the previous stump's mistakes into account:<\/b> After each iteration weights gets re-calculated to take the errors\/misclassifications from the last stump into consideration. \n* The final prediction is typically constructed by a weighted vote where weights for each base model depends on their training errors or misclassification rates. \n\n\nTo illustrate what we have talked about so far let's look at the following visualization. \n\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/0*paPv7vXuq4eBHZY7.png\">\n<h5 align=\"right\"> Source: Diogo(Medium)<\/h5>\n\n\n\n### Random Forest VS Ada Boost\n* Random Forest uses full grown trees while Adaboost uses stumps(one root node with two leafs)\n* In a Random Forest all the trees have similar amount of say, while in Adaboost some trees have more say than the other. \n* In a random forest the order of the tree does not matter, while in Adaboost the order is important(especially since each tree is built by taking the error of the previous error). \n","6a590fcf":"## Why\/Why not to use boosting?\n\n---\n\n### Pros\n\n- Achieves higher performance than bagging when hyper-parameters tuned properly.\n- Can be used for classification and regression equally well.\n- Easily handles mixed data types.\n- Can use \"robust\" loss functions that make the model resistant to outliers.\n\n---\n\n### Cons\n\n- Difficult and time-consuming to properly tune hyper-parameters.\n- Cannot be parallelized like bagging (bad scalability when vast amounts of data).\n- More risk of overfitting compared to bagging.\n\n\nResources: \n* http:\/\/mccormickml.com\/2013\/12\/13\/adaboost-tutorial\/\n* <a href=\"http:\/\/rob.schapire.net\/papers\/explaining-adaboost.pdf\">Explaining AdaBoost by Robert Schapire(One of the original author of AdaBoost)<\/a> \n* <a href=\"https:\/\/www.youtube.com\/watch?v=LsK-xG1cLYA\">StatQuest<\/a>\n* <a href=\"https:\/\/www.youtube.com\/watch?v=-DUxtdeCiB4\">Principles of Machine Learning | AdaBoost(Video)<\/a>","635f4154":"I am going to do all sorts of preparation ( including data munging, preparation, replacing NULL values, standard scaling...dummy variables) on the Titanic dataset to make it ready for the machine learning algorithm. If you would like to find out how I did it step-by-step. Please click <a href=\"https:\/\/www.kaggle.com\/masumrumi\/a-statistical-analysis-ml-workflow-of-titanic\"> here.<\/a>","27220f5e":"Let's look at some of the hyperparameters of AdaBoost. \n\n><b>base_estimator : object, optional (default=None)<\/b>\nThe base estimator from which the boosted ensemble is built. Support for sample weighting is required, as well as proper `classes_` and `n_classes_` attributes. If `None`, then the base estimator is `DecisionTreeClassifier(max_depth=1)`\n\n><b>n_estimators : integer, optional (default=50)<\/b>\nThe maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early.\n\n><b>learning_rate : float, optional (default=1.)<\/b>\nLearning rate shrinks the contribution of each classifier by `learning_rate`. There is a trade-off between `learning_rate` and `n_estimators`.\n\n><b>algorithm : {\u2018SAMME\u2019, \u2018SAMME.R\u2019}, optional (default=\u2019SAMME.R\u2019)<\/b>\nIf \u2018SAMME.R\u2019 then use the SAMME.R real boosting algorithm. `base_estimator` must support calculation of class probabilities. If \u2018SAMME\u2019 then use the SAMME discrete boosting algorithm. The SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations.\n\n><b>random_state : int, RandomState instance or None, optional (default=None)<\/b>\nIf int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.\n\n<h4 align=\"right\">Source: Sklearn<\/h4>","4e2f8db5":"<img src=\"https:\/\/slideplayer.com\/slide\/9092209\/27\/images\/20\/Algorithm+Adaboost+-+Example.jpg\"  Width=\"800\">\n<h5 align=\"right\"> Source: SlidePlayer<\/h5>","b169b2d5":"<h3>Using GridSearch and Cross-Validation with the whole dataset<\/h3>\nIf you want know more about GridSearch and Cross validation please read <a href=\"https:\/\/www.kaggle.com\/masumrumi\/a-statistical-analysis-ml-workflow-of-titanic\">this<\/a> kernel. \n","f279b33a":"## Let's dive into the nitty-gritty stuff about AdaBoost:\n***\n> We start by assigning a sample weight for each of the samples, this is uniformly distributed among all datapoints. For example, let's say, we start with a sample of ten data points. This means each data point will have 1\/10 weights.  \n\n> Then, we determine the best feature to split the dataset. This is determined by using a matrix called Gini index(basics from decision tree). The Gini index is a matrix between 0 and 1. Ideally, Gini index of 0 means there is a perfect split and the feature was able to unmix the lables completely and Gini index of 1 means the feature did a terrible job splitting the data. The lower the Gini index, the better unmixed the label is and therefore, better split. The feature with the lowest Gini index are choosen to be the first feature to split the data(hence creating the first stump) in the AdaBoost stump chain.   \n***\n> Then, we need to determine how much say a stump will have in the final classification and how we can calculate that.\n* We learn how much say a stump has in the final classification by calculating how well it classified the samples.\n* The <b>Total Error<\/b> for a stump is the sum of the weights associated with the incorrectly classified samples. For example, Once the uniform weight is distributed, let's say we create our first stump and find two incorrect predictions. To calculate the total error, we add up those two misclassified weights. Here we get 1\/10 + 1\/10 = 2\/10 or 1\/5. Hence, this is our total error. We can also think about it as...\n### $$ \\epsilon_t = \\frac{\\text{misclassifications}_t}{\\text{observations}_t} $$\n* Since the weight is uniformly distributed(all add up to 1) among all data points, the total error will always be between 0(perfect stump) and 1(horrible stump).\n* We use the total error to determine the amount of say a stump has in the final classification using the following formula\n \n### $$ \\alpha_t = \\frac{1}{2}ln \\left(\\frac{1-\\epsilon_t}{\\epsilon_t}\\right) \\text{where } \\epsilon_t < 1$$\n\nHere...\n* $\\epsilon_t$ = Misclassification rate for the current classifier\n* $\\alpha_t$ = Amount of Say\n* $\\epsilon_t$ = Total error\n\n\n\nWe can draw a graph to determine the amount of say using the value of total error(0 to 1)\n\n<img src=\"http:\/\/chrisjmccormick.files.wordpress.com\/2013\/12\/adaboost_alphacurve.png\">\n<h5 align=\"right\"> Source: Chris McCormick<\/h5>\n\n* The blue line tells us the amount of say for <b>Total Error(Error rate)<\/b> between 0 and 1. \n* When the stump does a reasonably good job, and the <b>total error<\/b> is minimal, then the <b>amount of say(Alpha)<\/b> is relatively large, and the alpha value is positive. \n* When the stump does an average job(similar to a coin flip\/the ratio of getting correct and incorrect ~50%), then the <b>total error<\/b> is ~0.5. In this case the <b>amount of say<\/b> is <b>0<\/b>.\n* When the error rate is high let's say close to 1, then the <b>amount of say<\/b> will be negative, which means if the stump outputs a value as \"survived\" the included weight will turn that value into \"not survived.\"\n\nP.S. If the <b>Total Error<\/b> is 1 or 0, then this equation will freak out. A small amount of error is added to prevent this from happening. \n \n ***\n> <b>Third<\/b>, We need to learn how to modify the weights so that the next stump will take the errors that the current stump made into the account. The equation for calculating the new sample weight is as follows. \n### $$ New Sample Weight = Sample Weight + e^{\\alpha_t}$$\nHere the $\\alpha_t(AmountOfSay)$ can be positive or negative depending on whether the sample was correctly classified or misclassified by the current stump. We want to increase the sample weight of the misclassified samples; hinting the next stump to put more emphasis on those. Inversely, we want to decrease the sample weight of the correctly classified samples; hinting the next stump to put less emphasis on those. \n\nThe following equation help us to do this calculation. \n### $$ D_{t+1}(i) = D_t(i) e^{-\\alpha_t y_i h_t(x_i)} $$\n\nHere, \n* $D_{t+1}(i)$ = New Sample Weight. \n* $D_t(i)$ = Current Sample weight.\n* $\\alpha_t$ = Amount of Say, alpha value, this is the coefficient that gets updated in each iteration and \n* $y_i h_t(x_i)$ = place holder for 1 if stump correctly classified, -1 if misclassified. \n\nFinally, we put together the combined classifier, which is \n### $$ AdaBoost(X) = sign\\left(\\sum_{t=1}^T\\alpha_t h_t(X)\\right) $$ \n\nHere, \n\n$AdaBoost(X)$ is the classification predictions for $y$ using predictor matrix $X$\n\n$T$ is the set of \"weak learners.\"\n\n$\\alpha_t$ is the contribution weight for weak learner $t$\n\n$h_t(X)$ is the prediction of weak learner $t$\n\nand $y$ is binary **with values -1 and 1**\n\n\nP.S. Since the stump barely captures essential specs about the dataset, the model is highly biased in the beginning. However, as the chain of stumps continues and at the end of the process, AdaBoost becomes a strong tree and reduces both bias and variance.\n\nWe have learned a lot about AdaBoost. Let's now apply what we have learned into Titanic dataset. ","00df4683":"**> Sample train dataset**","6f08f883":"## <div style=\"text-align: center\" > A Statistical Analysis & Machine Learning Workflow of Titanic <\/div>\n<div style=\"text-align: center\"> Being a part of Kaggle gives me unlimited access to learn, share and grow as a Data Scientist. In this kernel, I want to solve <font color=\"red\"><b>Titanic competition<\/b><\/font>, a popular machine learning dataset using <b>AdaBoost Ensemble technique<\/b>. This kernel is a part of my machine learning series articles. If you would like to find out more about other machine learning models, please checkout this <a href=\"https:\/\/www.kaggle.com\/masumrumi\/a-statistical-analysis-ml-workflow-of-titanic\/edit\/run\/13339359\">this<\/a> kernel. <\/b> I will also describe how best to evaluate model results along with many other tips. So let's get started.<\/div>\n\n\n***\n<div style=\"text-align:center\"> If there are any recommendations\/changes you would like to see in this notebook, please <b>leave a comment<\/b>. Any feedback\/constructive criticism would be genuinely appreciated. <b>This notebook is always a work in progress. So, please stay tuned for more to come.<\/b><\/div>\n\n\n<div style=\"text-align:center\">If you like this notebook or find this notebook helpful, Please feel free to <font color=\"red\"><b>UPVOTE<\/b><\/font> and\/or <font color=\"Blue\"><b>leave a comment.<\/b><\/font><\/div><br>\n\n<div style=\"text-align: center\"><b>You can also Fork and Run this kernel from <a href=\"https:\/\/github.com\/masumrumi\">Github<\/b><\/a>\n    <\/div>\n\n### <div style=\"text-align: center\">Stay Tuned for More to Come!!<\/div>","3d160cfc":"**> Sample test dataset**"}}