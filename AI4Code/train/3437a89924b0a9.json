{"cell_type":{"821bb93d":"code","378e0899":"code","43fb3982":"code","992bcb7d":"code","bd4363ed":"code","ff8769d4":"code","02e890a2":"code","dd17d5c9":"code","082da17d":"code","192d0b7c":"code","a6f57bb1":"code","6cbce05a":"code","a1801b03":"code","418d0d99":"code","29222527":"code","006b924e":"code","9ecf6373":"code","110262bf":"code","0da68811":"code","7eb90b2b":"code","100625a1":"code","ef1727be":"code","4029e482":"code","c4b05a19":"code","1c0b1ef8":"code","d6d77881":"code","91bf10ae":"code","d0146e7e":"code","bea67300":"markdown","fce65282":"markdown","3815d86c":"markdown","4672460c":"markdown","976a62fb":"markdown","1981373c":"markdown","ce4842d0":"markdown","256dd5cd":"markdown","59b088a6":"markdown","6ba6e724":"markdown","cd9de4e3":"markdown","f5638f35":"markdown","43468d4a":"markdown","10c7bdf5":"markdown","517e2420":"markdown"},"source":{"821bb93d":"# Import \n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.preprocessing import StandardScaler\n\n%matplotlib inline \n\nRANDOM_SEED = 33\n\nplt.style.use('bmh')","378e0899":"# We started the analysis process, studying the available data\n\nimport os\nprint(os.listdir(\"..\/input\"))\ndf = pd.read_csv('..\/input\/creditcard.csv') \ndf.head()","43fb3982":"df.info()","992bcb7d":"# Remove unnecessary columns\ndf2 = df.drop('Time', axis=1)\ndf2.head()","bd4363ed":"# density por normed (deprecated)\n\nbins=80\nplt.figure(figsize=(20,4))\nplt.hist(df2.Class[df2.Class==1],bins=bins,density=True,alpha=0.8,label='Fraud',color='red')\nplt.hist(df2.Class[df2.Class==0],bins=bins,density=True,alpha=0.8,label='Not Fraud',color='blue')\nplt.legend(loc='upper right')\nplt.xlabel('Valor')\nplt.ylabel('% de Registros')\nplt.title('Transacciones vs Valor')\nplt.show()","ff8769d4":"print(df2['Class'].value_counts())\nsns.countplot(x = 'Class', data = df2)\n\nplt.title(\"Fraud class histogram\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\")","02e890a2":"# We generate the correlation matrix and look only at those variables with a high correlation level\n\ncorr_base = df2.corr() \nplt.figure(figsize=(12, 10))\n\nsns.heatmap(corr_base[(corr_base >= 0.5) | (corr_base <= -0.4)], \n            cmap='viridis', vmax=1.0, vmin=-1.0, linewidths=0.1,\n            annot=True, annot_kws={\"size\": 8}, square=True);","dd17d5c9":"# We study the rest of variables: Class= 1:Fraud , Class= 0:No Fraud\ny = df2.Class\nx = df2.drop('Class',axis=1)","082da17d":"#PCA      \n\nfrom sklearn.decomposition import PCA as sklearnPCA\nsklearn_pca = sklearnPCA(n_components=29, whiten=True)\nsklearn_pca.fit(x)\nfeatures_pca = pd.DataFrame(data = sklearn_pca.transform(x))","192d0b7c":"n_dim = 29\nplt.figure(figsize=(12, 5))\nrects1 = plt.bar(np.arange(n_dim),sklearn_pca.explained_variance_, color='r')\nprint(sklearn_pca.explained_variance_) ","a6f57bb1":"# Group of features. The generation of these graphs takes some computing time.\n\nx_scaled=(x-x.min())\/(x.max()-x.min()) \nsub_df1=pd.concat([y,x_scaled.iloc[:,0:10]],axis=1)\nsub_df2=pd.concat([y,x_scaled.iloc[:,10:20]],axis=1)\nsub_df3=pd.concat([y,x_scaled.iloc[:,20:30]],axis=1)\n\nsub_df11=pd.melt(sub_df1,id_vars=\"Class\",var_name=\"Variable\",value_name='Valor')\nsub_df22=pd.melt(sub_df2,id_vars=\"Class\",var_name=\"Variable\",value_name='Valor')\nsub_df33=pd.melt(sub_df3,id_vars=\"Class\",var_name=\"Variable\",value_name='Valor')\n\nplt.figure(figsize=(20,8))\nsns.violinplot(x=\"Variable\",y=\"Valor\",hue=\"Class\",data=sub_df11, split=True)\nplt.figure(figsize=(20,8))\nsns.violinplot(x=\"Variable\",y=\"Valor\",hue=\"Class\",data=sub_df22, split=True)\nplt.figure(figsize=(20,8))\nsns.violinplot(x=\"Variable\",y=\"Valor\",hue=\"Class\",data=sub_df33, split=True)\nplt.figure(figsize=(20,8))\n","6cbce05a":"count_classes = pd.value_counts(df2['Class'], sort = True).sort_index()\nlabels = 'Fraud', 'Not Fraud'\nsizes = [count_classes[1]\/(count_classes[1]+count_classes[0]), count_classes[0]\/(count_classes[1]+count_classes[0])]\nexplode = (0, 0.5,)  \ncolors = ['red', 'lightblue']\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, explode=explode, colors=colors, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=45)\nax1.axis('equal')  \nplt.title(\"Distribution of the Dataset in labeled classes\")\nplt.show()","a1801b03":"df2.shape","418d0d99":"tt = df2.describe().transpose()\ntt[(tt['max']>1) & (tt['min']< -1)]\n","29222527":"plt.figure(figsize=(20,8))\nplt.hist(df2.Amount, bins=50)","006b924e":"# We normalize all the columns\n\ncolumns_to_norm = ['V1','V2','V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', \n                   'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']","9ecf6373":"from sklearn import preprocessing\nmin_max_scaler = preprocessing.MinMaxScaler() \ndf2[columns_to_norm]=min_max_scaler.fit_transform(df2[columns_to_norm])","110262bf":"tt = df2.describe().transpose()\ntt[(tt['max']>1) & (tt['min']< -1)]","0da68811":"# We generate a help function for the rest of modules of face to visualize the arrays of confusion.\n\nfrom sklearn.metrics import confusion_matrix, classification_report, auc, precision_recall_curve, roc_curve\ndef plot_confusion_matrix(y_test, pred):\n    \n    y_test_legit = y_test.value_counts()[0]\n    y_test_fraud = y_test.value_counts()[1]\n    \n    cfn_matrix = confusion_matrix(y_test, pred)\n    cfn_norm_matrix = np.array([[1.0 \/ y_test_legit,1.0\/y_test_legit],[1.0\/y_test_fraud,1.0\/y_test_fraud]])\n    norm_cfn_matrix = cfn_matrix * cfn_norm_matrix\n\n    fig = plt.figure(figsize=(12,5))\n    ax = fig.add_subplot(1,2,1)\n    sns.heatmap(cfn_matrix,cmap='coolwarm_r',linewidths=0.5,annot=True,ax=ax)\n    plt.title('Confusion matrix')\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n    ax = fig.add_subplot(1,2,2)\n    sns.heatmap(norm_cfn_matrix,cmap='coolwarm_r',linewidths=0.5,annot=True,ax=ax)\n\n    plt.title('Standardized Confusion matrix')\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()\n    \n    print('---Report de classifition---')\n    print(classification_report(y_test,pred))","7eb90b2b":"from sklearn.model_selection import train_test_split\n\nX_train, X_test = train_test_split(df2, test_size=0.2, random_state=RANDOM_SEED)\nY_train = X_train['Class']\nX_train = X_train.drop(['Class'], axis=1)\nY_test = X_test['Class']\nX_test = X_test.drop(['Class'], axis=1)","100625a1":"from sklearn import metrics\n\nsgd_clf=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n       learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,\n       n_jobs=1, penalty='l2', power_t=0.5, random_state=42, shuffle=True,\n       tol=None, verbose=0, warm_start=False)\n\nsgd_clf.fit(X_train, Y_train) \nY_train_predicted=sgd_clf.predict(X_train)\nY_test_predicted=sgd_clf.predict(X_test)\n\nplot_confusion_matrix(Y_test, Y_test_predicted)","ef1727be":"from sklearn.utils import shuffle\n\nTrain_Data= pd.concat([X_train, Y_train], axis=1)\nX_1 =Train_Data[ Train_Data[\"Class\"]==1 ]\nX_0=Train_Data[Train_Data[\"Class\"]==0]\n\nX_0=shuffle(X_0,random_state=42).reset_index(drop=True)\nX_1=shuffle(X_1,random_state=42).reset_index(drop=True)\n\nALPHA=1.15 \n\nX_0=X_0.iloc[:round(len(X_1)*ALPHA),:]\ndata_d=pd.concat([X_1, X_0])\n\ncount_classes = pd.value_counts(data_d['Class'], sort = True).sort_index()\nlabels = 'Fraud', 'Not Fraud'\nsizes = [count_classes[1]\/(count_classes[1]+count_classes[0]), count_classes[0]\/(count_classes[1]+count_classes[0])]\nexplode = (0, 0.05,)\ncolors = ['red', 'lightblue']\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, explode=explode, colors=colors, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=45)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nplt.title(\"Distribuci\u00f3n del dataset en clases\")\nplt.show()","4029e482":"data_d.head()","c4b05a19":"data_d.shape","1c0b1ef8":"# Convertimos el dataframe a matriz(array).\ndataset=data_d.values","d6d77881":"Y_d=data_d['Class']\nX_d=data_d.drop(['Class'],axis=1)","91bf10ae":"sgd_clf_d=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n       learning_rate='optimal', loss='hinge', max_iter=5, n_iter=None,\n       n_jobs=1, penalty='l2', power_t=0.5, random_state=42, shuffle=True,\n       tol=None, verbose=0, warm_start=False)\n\nsgd_clf_d.fit(X_d, Y_d) \nY_test_predicted=sgd_clf_d.predict(X_test)\n\nplot_confusion_matrix(Y_test, Y_test_predicted)","d0146e7e":"from sklearn.ensemble import IsolationForest\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import make_blobs\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nrf =RandomForestClassifier(n_estimators=100, max_depth=None, random_state=0, n_jobs=-1)\nrf.fit(X_d, Y_d) \nY_test_predicted=rf.predict(X_test)\n\nplot_confusion_matrix(Y_test, Y_test_predicted)","bea67300":"\n\n\n\n\n#### We will execute the same balanced dataset with a more complex and powerful model such as RandomForest, and analyze the results.","fce65282":"At this point we have left the dataset with only those variables that a-priori can provide some type of information regarding fraud. Now let's analyze the dataset at the information type level.\n\nAs you will see below, a common problem that we find in the operational analytics is that the dataset is unbalanced, so there are many more samples of a class or label, than of the other class.\n\nThis is especially important when you detect anomalies or situations of fraud detection. There are many more legal transactions  that fraudulent.","3815d86c":"We will analyze within the dataset those columns whose minimum value is less than -1 and maximum value greater than 1. \nTo do this we will support the function \"describe\" a DataFrame that gives us all this information and more.","4672460c":"#### Data preparation for learning and launch of Model\n\nThe first step is to separate data in training and test data. In this exercise a simple approximation is used, separating the data in 80% training, 20% test. More advanced methods can be applied, including later validation data or a greater randominzaci\u00f3n of the data to be obtained, but this approximation is sufficient for this case.\n\nIt is necessary to separate the data into variables and labels.\n","976a62fb":"The dataset is very unbalanced. There are techniques to improve the balance of the same, but the most important thing is that these types of situations should be taken into account when analyzing the results of the models that we apply. We will see it when we analyze the results obtained.","1981373c":"### Exploratory Data Analysis (EDA)","ce4842d0":"### This is my first kernel. If you liked my kernel, so far I appreciate a vote in favor.","256dd5cd":" Dataset (df2) is totally unbalanced.","59b088a6":"##  Credit Card Fraud Detection","6ba6e724":"### Correlation of variables","cd9de4e3":"#### DataSet Normalization","f5638f35":"Let's apply a regression model from start. It is a linear regularization model that uses a Stochastic Gradient Descent (SGD).It is a model that is strong enough for a first test.\n","43468d4a":"\n### DataSet balancing and data normalization\n","10c7bdf5":"#### Unbalance the dataset\n\nA strategy to follow is to reduce the number of correct samples, to try to balance the dataset. Since it is not an especially large dataset it is necessary to be careful with this operation, because reducing drastically the number of samples will penalize the model by not having enough data for its learning.","517e2420":"There are no more variables to normalize, all are in the expected ranges.\n"}}