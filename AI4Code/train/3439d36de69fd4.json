{"cell_type":{"07c59782":"code","184a12f3":"code","f34b9085":"code","b744df5b":"code","fd881fc3":"code","52fe1acd":"code","0e314528":"code","c70f02ee":"code","94c5dc7a":"code","d7c56086":"code","06f973aa":"code","ac79e94b":"code","a02f714e":"code","e035c512":"code","ed695f43":"code","9968fd29":"code","b60d14ed":"code","4c20858f":"code","c901ff18":"code","dbab601c":"code","94f962a9":"code","940f03d5":"code","0a87865d":"code","252eeffb":"markdown","a51082f1":"markdown","c5972330":"markdown","296de754":"markdown","457e0e81":"markdown","5ab79203":"markdown","c3b41b0e":"markdown","4c01c125":"markdown","b69ebbb5":"markdown","fa7f0c51":"markdown","68d3baba":"markdown","96626090":"markdown"},"source":{"07c59782":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","184a12f3":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib\n\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\nfrom scipy.stats.stats import pearsonr\n%matplotlib inline","f34b9085":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\ntrain_df.head()","b744df5b":"# remove 'Id' and 'SalePrice' columns\nall_feature_df = pd.concat((train_df.loc[:,'MSSubClass':'SaleCondition'],\n                      test_df.loc[:,'MSSubClass':'SaleCondition']))","fd881fc3":"from scipy.stats import norm\nimport seaborn as sns\n\nfig, ax =plt.subplots(1,2)\nwidth, height = fig.get_size_inches()\nfig.set_size_inches(width*2, height)\nsns.distplot(train_df['SalePrice'], ax=ax[0], fit=norm)\nsns.distplot(np.log(train_df[('SalePrice')]+1), ax=ax[1], fit= norm)\n#fig.show()","52fe1acd":"#log transform the target:\ntrain_df[\"SalePrice\"] = np.log1p(train_df[\"SalePrice\"])\n\n#log transform skewed numeric features:\nnumeric_features = all_feature_df.dtypes[all_feature_df.dtypes != \"object\"].index\n\nskew_value_for_numeric_features = train_df[numeric_features].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_features = skew_value_for_numeric_features[skew_value_for_numeric_features > 0.75].index\nall_feature_df[skewed_features] = np.log1p(all_feature_df[skewed_features])","0e314528":"# one-hot-encoding for categorical variables\nall_feature_df = pd.get_dummies(all_feature_df)","c70f02ee":"# fill null values of numerical columns with mean of the features\nall_feature_df = all_feature_df.fillna(all_feature_df.mean())","94c5dc7a":"#creating matrices for sklearn:\nX_train = all_feature_df[:train_df.shape[0]]\nX_test = all_feature_df[train_df.shape[0]:]\ny = train_df.SalePrice","d7c56086":"corrmat = train_df.corr().abs()\nk = 15 #number of variables for heatmap\ncols_index = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ntrain_mod_df = train_df[cols_index]\ncorrmat = train_mod_df.corr()\n\nfig, ax = plt.subplots(figsize=(10,10))\nhm = sns.heatmap(corrmat,yticklabels=cols_index.values, xticklabels=cols_index.values,  annot=True,annot_kws={'size': 10}, ax=ax, square= True )\nplt.show()","06f973aa":"sns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageArea', '1stFlrSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train_df[cols], height = 2.5)\nplt.show();","ac79e94b":"from sklearn.metrics import mean_squared_error\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","a02f714e":"from sklearn.model_selection import train_test_split\nX_train_cv, X_test_cv, y_train_cv, y_test_cv = train_test_split(X_train,y, random_state= 0)","e035c512":"from  sklearn.linear_model import LassoCV\nfrom  sklearn.linear_model import Lasso\n\nlasso = LassoCV(alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, \n                          0.3, 0.6, 1, 3, 6, 10, 30, 60, 100], \n                max_iter = 100000, cv = 10)\nlasso.fit(X_train_cv, y_train_cv)\nalpha = lasso.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Try again for more precision with alphas centered around \" + str(alpha))\nlasso = LassoCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, \n                          alpha * .85, alpha * .9, alpha * .95, alpha, alpha * 1.05, \n                          alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, alpha * 1.35, \n                          alpha * 1.4], \n                max_iter = 50000, cv = 10)\nlasso.fit(X_train_cv, y_train_cv)\nalpha = lasso.alpha_\nprint(\"Best alpha :\", alpha)\n\ny_train_las = lasso.predict(X_train_cv)\ny_test_las = lasso.predict(X_test_cv)\nprint(\"LRMSE on training set=\"+str(rmsle(y_train_cv,y_train_las)))\nprint(\"LRMSE on test set=\"+str(rmsle(y_test_cv,y_test_las )))","ed695f43":"from sklearn.ensemble import RandomForestRegressor\n\n\nregRF = RandomForestRegressor(n_estimators=1000,  max_features = 'sqrt', criterion='mse', random_state=0 )\nregRF.fit(X_train_cv, y_train_cv)\ny_train_pred = regRF.predict(X_train_cv)\ny_test_pred = regRF.predict(X_test_cv)\nprint(\"LRMSE on training set=\"+str(rmsle(y_train_cv,y_train_pred)))\nprint(\"LRMSE on test set=\"+str(rmsle(y_test_cv,y_test_pred )))","9968fd29":"from sklearn.ensemble import GradientBoostingRegressor\nregGB = GradientBoostingRegressor(n_estimators=1000,  max_features = 'sqrt', criterion='mse', random_state=0 )\nregGB.fit(X_train_cv, y_train_cv)\ny_train_pred = regGB.predict(X_train_cv)\ny_test_pred = regGB.predict(X_test_cv)\nprint(\"LRMSE on training set=\"+str(rmsle(y_train_cv,y_train_pred)))\nprint(\"LRMSE on test set=\"+str(rmsle(y_test_cv,y_test_pred )))","b60d14ed":"import xgboost as xgb\ndtrain_cv = xgb.DMatrix(X_train_cv, label = y_train_cv)\ndtest_cv = xgb.DMatrix(X_test_cv, label = y_test_cv)\n\n\nparams = {\"max_depth\":5, \"eta\":0.1, \"silent\":1,\"eval_metric\": \"rmse\" }\nmin_rmse = float('inf')\nfor eta in [.3, .2, .1, .05, .01, .005]:\n    print(\"CV with eta={}\".format(eta))\n\n    params['eta'] = eta\n    \n    cv_results = xgb.cv(\n            params,\n            dtrain_cv,\n            num_boost_round=999,\n            seed=42,\n            nfold=5,\n            metrics=['rmse'],\n            early_stopping_rounds=10,\n            verbose_eval = 0\n            )\n\n    # Update best score\n    mean_rmse = cv_results['test-rmse-mean'].min()\n    boost_rounds = cv_results['test-rmse-mean'].idxmin()\n    print(\"\\RMSE {} for {} rounds\\n\".format(mean_rmse, boost_rounds))\n    if mean_rmse < min_rmse:\n        min_rmse = mean_rmse\n        best_params = eta\n        best_boost_round = boost_rounds\n\nprint(\"Best params: {}, RMSE: {}, Boost Rounds: {}\".format(best_params, min_rmse,best_boost_round ))\n\n\n","4c20858f":"params['eta'] = best_params\nnum_boost_rounds = best_boost_round\nwatchlist = [(dtrain_cv, 'train'), (dtest_cv, 'test')]\nresult=dict()\nmodel_xg = xgb.train(\n    params,\n    dtrain_cv,\n    num_boost_round=num_boost_rounds,\n    evals = watchlist,\n    verbose_eval = False,\n    evals_result = result\n    )\ntrain_data = result['train']['rmse']\ntest_data = result['test']['rmse']\n\nplt.Figure()\nplt.plot(train_data)\nplt.plot(test_data)\nplt.xlabel('rounds')\nplt.ylabel('train\/test error')\nplt.legend(['train', 'test'])\nplt.show()\ny_train_pred = model_xg.predict(dtrain_cv)\ny_test_pred = model_xg.predict(dtest_cv)\n\nprint(\"LRMSE on training set=\"+str(rmsle(y_train_cv,y_train_pred)))\nprint(\"LRMSE on test set=\"+str(rmsle(y_test_cv,y_test_pred )))","c901ff18":"from sklearn.base import clone\n\nclass AveragingModels:\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        # Train cloned base models\n        self.models_ = [clone(x) for x in self.models]\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   \n","dbab601c":"regRF = RandomForestRegressor(n_estimators=500, max_features = 'sqrt', random_state=0 )\nregGB = GradientBoostingRegressor(n_estimators=100, max_features = 'sqrt',max_depth=3)\nlasso = Lasso( alpha=float(6e-05) )\n\navg_model = AveragingModels([regRF, regGB, lasso])\navg_model.fit(X_train_cv,y_train_cv)\n\ny_train_pred = avg_model.predict(X_train_cv)\ny_test_pred = avg_model.predict(X_test_cv)\nprint(\"LRMSE on training set=\"+str(rmsle(y_train_cv,y_train_pred)))\nprint(\"LRMSE on test set=\"+str(rmsle(y_test_cv,y_test_pred )))","94f962a9":"sub = pd.DataFrame()\ndtest = xgb.DMatrix(X_test)\nprediction_avg = np.exp(avg_model.predict(X_test))-1\nprediction_xg = np.exp(model_xg.predict(dtest) ) -1\n\nprediction = (0.25* prediction_xg) + (0.75*prediction_avg)\n\nsub['Id'] = test_df['Id']\nsub['SalePrice'] = prediction\nsub.to_csv('submission.csv',index=False)","940f03d5":"from sklearn.model_selection import KFold\nclass StackingModels:\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=32)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                instance.fit(X.iloc[train_index], y.iloc[train_index])\n                y_pred = instance.predict(X.iloc[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                self.base_models_[i].append(instance)\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_ = clone(self.meta_model)\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        \n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","0a87865d":"\nmodel = StackingModels([regRF, regGB, lasso], regGB )\nmodel.fit(X_train_cv, y_train_cv)\ny_train_pred = model.predict(X_train_cv)\ny_test_pred = model.predict(X_test_cv)\nprint(\"LRMSE on training set=\"+str(rmsle(y_train_cv,y_train_pred)))\nprint(\"LRMSE on test set=\"+str(rmsle(y_test_cv,y_test_pred )))\n","252eeffb":"### Data Preprocessing","a51082f1":"### Gradient Boosting","c5972330":"### XGBoost","296de754":"### Lasso","457e0e81":"## References\n1.  **Data Preprocessing :**  https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models\/notebook\n\n2.  **XG boost Paramter tuning : **  https:\/\/cambridgespark.com\/content\/tutorials\/hyperparameter-tuning-in-xgboost\/index.html","5ab79203":"### Modeling\nAs a first step to modeling let us check how numeric features are correlated with each other and the target: 'SalePrice'","c3b41b0e":"We observe the skew in target. We see that it is heavily skewed. We perform log transform to make it normal. This has two benefits:\n1. Most statistical techniques for modeling make the assumtion that the data being modeled is normally distributed. Hence, the transformation on data gives better  predictions when these techniques are used. \n2.  Log transformation also reduces the impacts of outliers.\n\nWe repeat this process for all heavily skewed numeric features.","4c01c125":"### Ensemble- Stacking\nThis involves using multiple  models to make predictions and then using those predictions are features in a meta-model. Though I have not used this for submission to  the competition, including the technique here for completion.","b69ebbb5":"### Ensemble - Averaging\nWe then average the predictions of all the models.  This prevents overfitting that might result from using only a single model and hence provides better predictions","fa7f0c51":"### Random Forest","68d3baba":"We identify the numeric features that are most strongly correlated with the target ( To keep things simple, we do not use categorical variables). We exclude the ones that have high correlation with the selected features.\nWe then ditermine the relation of these variables with 'SalePrice' and amngst themselves using pairplot. You will notice that there is nearly linear relation of each feature with SalePrice. We also notice that log transformation has made the relation homoscedastic i.e. evenly ditributed across the regression line ( constant variance).\n\nWe hence, decide to model this data using linear regression and decision trees techniques","96626090":"## Introduction\n\n**Objectives:**\nThis notebook is aimed to be a self-tutorial in following areas:\n- Ensemble learning  \n- Data cleaning and transformation\n- XG Boost parameter tuning\n\n**Author:** Ritu Pande\n\n**Note:**  This notebook  refers many existing kernels and tutorials online. I have tried to provide link to those in the References section"}}