{"cell_type":{"196a6e25":"code","b46e48c3":"code","8a2440a7":"code","10146b0d":"code","5a14f491":"code","6acbe5ff":"code","dba2b750":"code","b189044a":"code","91f785b1":"code","6b139319":"code","79b3044e":"code","b5f5bbba":"code","fc8d8d3f":"code","12ba2b33":"code","e475b1fb":"code","5965accb":"code","9d93aacc":"code","8d274d4f":"code","19b7682f":"code","602374ad":"code","0ccfc06e":"code","eaa59c41":"code","35be1531":"code","0b5ec1a1":"code","4489ed94":"code","70b1195e":"markdown","ff2daf73":"markdown","81b27dfc":"markdown","4c522498":"markdown","f3f46631":"markdown","5a9dc439":"markdown","fd259498":"markdown","99c14f3d":"markdown","a439accc":"markdown","73ea6223":"markdown","3e548004":"markdown","e953e96b":"markdown","e08e0b6e":"markdown","b6555cc7":"markdown","ff462d3a":"markdown","c47bd280":"markdown","329074c9":"markdown","826dd935":"markdown","130f0c3e":"markdown","1b23a24d":"markdown","d809a275":"markdown","664b3ca1":"markdown","3116a23a":"markdown","be326e88":"markdown","eef61546":"markdown"},"source":{"196a6e25":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import linregress\nfrom sklearn import metrics\nimport statsmodels.api as sm\nimport statsmodels.stats.diagnostic as smd\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import gaussian_kde\nfrom statsmodels.graphics.gofplots import ProbPlot\nfrom scipy import stats","b46e48c3":"# Load CSV file data into a dataframe.\ndf = pd.read_csv(\"\/kaggle\/input\/pga-tour-20102018-data\/2019_data.csv\")\ndf.head()","8a2440a7":"# Transpose the statistic variables such that there is 1 golfer per row, each with columns for\n# every statistic variable.\ndf = df.set_index(['Player Name', 'Variable', 'Date'])['Value'].unstack('Variable').reset_index()\n\n# Typecast the Date column to datetime objects so they can be quantified.\ndf['Date'] = pd.to_datetime(df['Date'])\n\n# Select data from 8\/25\/19, which was the end of the 2018-2019 PGA season.\ndf = df[(df['Date'] == '2019-08-25')]\n\n# Typecast data points to numeric data types, except Player Name and Date columns.\ndf.iloc[:, 2:] = df.iloc[:, 2:].apply(pd.to_numeric, errors='coerce').fillna(0)\n\ndf.head()","10146b0d":"# Check all of the statistic variables available in the dataset.\nfor col in df.columns[2:]:\n    print(col)","5a14f491":"# Only keep the average, percentage, and points variables, otherwise there will be many 1.0 co-efficients cluttering the analysis.\nfor col in df.columns:\n    if 'AVG' not in col and '%' not in col and 'POINTS' not in col:\n        del df[col]","6acbe5ff":"# Since the data is zero-inflated, replace 0 values with NaN values before analyzing correlations. Otherwise the correlations will also be inflated.\n\ncorr_df = pd.concat([df.iloc[:, :2], df.iloc[:, 2:].replace(0, np.nan)], axis=1)\n\ncorr_df.head()","dba2b750":"dependent_var = 'Scoring Average - (AVG)'","b189044a":"# Create a sorted Pierson correlation matrix to understand variable pair relationships.\n\nmatrix_df = corr_df.corr()\nunstack_matrix = matrix_df.unstack()\nsorted_matrix = unstack_matrix.sort_values(kind='quicksort', ascending=True, na_position='first').dropna()\n  \nprint('ALL CORRELATIONS ARE BETWEEN \\\"{}\\\" AND AN ARBITRARY VARIABLE'.format(dependent_var))\nprint('='*95+'\\n')\n    \ncount = 0\nfor pair, val in sorted_matrix.items():\n    if pair[1] == dependent_var and count < 20:\n        print('{:68} PIERSON CO-EFF.'.format(pair[0] + ' ,'))\n        print('{:68} {}'.format(pair[1], val))\n        print('-'*88)\n        count += 1","91f785b1":"# Select the highly correlated pairs that contain the dependent variable.\n\npairs = []\n\nfor pair, val in sorted_matrix.items():\n    var1, var2 = pair\n    if var2 == dependent_var:\n        pairs.append([var1, var2, val])\n        \nprint(*(pair for pair in pairs[:10]), sep='\\n')","6b139319":"# Test the significance of the correlations with the dependent variable using p-values of the co-effs.\n\nlin_regress_dict = {}\n\nfor pair in pairs:\n    var1_list = df[pair[0]].values.tolist()\n    var2_list = df[pair[1]].values.tolist()\n    (slope, intercept, r_value, p_value, std_err) = linregress(var1_list, var2_list)\n    \n    key_name = \"{}, {}\".format(pair[0], pair[1])\n    lin_regress_dict[key_name] = ((slope, intercept, r_value, p_value, std_err))\n    \n# Keep the most significantly correlated pairs\nfor key, val in list(lin_regress_dict.items()):\n    if val[3] > 0.05:  # p-value > 0.05\n        del lin_regress_dict[key]\n        \ncount = 0\nfor k,v in lin_regress_dict.items():\n    if count <= 5:\n        print('{} :\\n{}\\n'.format(k, v))\n        count += 1","79b3044e":"# Sort the correlated pairs by p-value.\nsorted_pvalues = sorted(lin_regress_dict.items(), key=lambda x: x[1][3])\n\n# Print the most significantly correlated variables to the dependent variable.\nprint('VARIABLE CORRELATIONS TO \\\"{}\\\", SORTED BY P-VALUE\\n'.format(dependent_var))\nprint('{:66} {:13} {}'.format('VARIABLE NAME', 'R-VALUE', 'P-VALUE'))\nprint('='*88)\n\nfor pair in sorted_pvalues[:10]:\n    var1, var2 = pair[0].split(', ')\n    slope, intercept, r_value, p_value, std_err = pair[1]\n    \n    print('{:60}   |   {:6.4f}   |   {:4}'.format(var1, r_value.round(4), p_value))\n    print('-'*88)  ","b5f5bbba":"independent_var = 'Driving Distance - (AVG.)'","fc8d8d3f":"# Use original dataframe because we can't input NaN values into the OLS model. We can simply remove\n# the zeros from the original dataframe.\nold_x = pd.DataFrame(df[independent_var])\nold_y = pd.DataFrame(df[dependent_var])\n\nnew_df = pd.concat([old_x, old_y], axis=1)\n\n# Drop entire variables that contain zero values.\nfor idx, row in new_df.iterrows():\n    if row[1] <= 0:\n        new_df.drop(idx, axis=0, inplace=True)\n\nnew_x = pd.DataFrame(new_df[independent_var])\nnew_y = pd.DataFrame(new_df[dependent_var])\n\nnew_df.head()","12ba2b33":"# I decided to skip cross validation because the population size is so small to begin with.\n#x_train, x_test, y_train, y_test = train_test_split(new_x, new_y, test_size=0.2)\n# Concatenate the variables into one dataframe for easier reference.\n#train_df = pd.concat([x_train, y_train], axis=1)\n\ntrain_df = pd.concat([new_x, new_y], axis=1)\n\ntrain_df.head()","e475b1fb":"# Generate an Ordinary Least Squares regression model.\n\nX = sm.add_constant(new_x)\nY = new_y\n\nols_model = sm.OLS(Y, X).fit()\n\nprint(ols_model.summary())","5965accb":"# Set variables for info from the model, to use for analysis.\n\n# Model fitted values.\nols_model_fitted_y = ols_model.fittedvalues\n\n# Model residuals.\nols_model_residuals = ols_model.resid\n\n# Normalized residuals.\nols_model_norm_residuals = ols_model.get_influence().resid_studentized_internal\n\n# Absolute squared normalized residuals.\nols_model_norm_residuals_abs_sqrt = np.sqrt(np.abs(ols_model_norm_residuals))\n\n# Leverage.\nols_model_leverage = ols_model.get_influence().hat_matrix_diag","9d93aacc":"# Regression plot.\n\nplt.style.use('seaborn')\n\nfig2, ax1 = plt.subplots(1, 1, figsize=(12, 8))\n\nax1.scatter(new_x, new_y, alpha=0.7)\nax1.plot(new_x, ols_model_fitted_y, color='red', linewidth=2)\n\nax1.set_title('Regression')\nax1.set_xlabel(independent_var)\nax1.set_ylabel(dependent_var)\nax1.text(272, 69.5,'y = -0.0234x + 77.8675', fontsize=20)","8d274d4f":"# Residuals density plot\n\nmean = np.mean(ols_model_residuals)\nstd = np.std(ols_model_residuals)\n\nkde = gaussian_kde(ols_model_residuals)\ncovf = kde.covariance_factor()\nbw = covf * std\n     \nfig3, ax1 = plt.subplots(1, 1, figsize=(12, 8))\n\nsns.distplot(ols_model_residuals, kde_kws={'bw': bw})\n\nax1.set_title('Residual Density')\nax1.xaxis.set_ticks(np.arange(-2.604, 2.604, 0.651))\nax1.text(1, 0.5, \"mean = {:.4f}\\nstd = {:.4f}\".format(mean, std), fontsize=18)","19b7682f":"# Test if residuals are normally distributed using a test that factors skew and kurtosis.\n\ns, pval = stats.normaltest(ols_model_residuals)\n\nif pval > 0.05:\n    print('There is NOT enough evidence to conclude that the distribution is NOT normally distributed.')\nelse:\n    print('There is enough evidence to conclude that the distribution is NOT normally distributed')","602374ad":"# Residuals vs Fitted plot.\n\nfig5, ax1 = plt.subplots(1, 1, figsize=(12, 8))\n\nsns.residplot(ols_model_fitted_y, train_df[dependent_var], lowess=True, ax=ax1, \\\n              scatter_kws={'alpha': 0.6}, line_kws={'color': 'red', 'lw': 2, 'alpha': 0.5})\n\nols_model_fitted_y\nax1.set_title('Residuals vs Fitted')\nax1.set_xlabel('Fitted Values')\nax1.set_ylabel('Residuals')","0ccfc06e":"# Normal Q-Q plot.\n\nfig4, ax1 = plt.subplots(figsize=(12, 8))\n\nQQ = ProbPlot(ols_model_norm_residuals)\n\nQQ.qqplot(line='45', alpha=0.3, lw=1, color='#4c72b0', ax=ax1)\n\nax1.set_title('Normal Q-Q')\nax1.set_xlabel('Theoretical Quantiles')\nax1.set_ylabel('Standardized Residuals')","eaa59c41":"# Scale-Location plot.\n\nfig6, ax1 = plt.subplots(1, 1, figsize=(12, 8))\n\nsns.regplot(ols_model_fitted_y, ols_model_norm_residuals_abs_sqrt, ci=False, \\\n            lowess=True, scatter_kws={'alpha': 0.6}, ax=ax1, \\\n            line_kws={'color': 'red', 'lw': 2, 'alpha': 0.5})\n\nax1.set_xlim(min(ols_model_fitted_y)-0.05, max(ols_model_fitted_y)+0.05)\nax1.set_title('Scale-Location')\nax1.set_xlabel('Fitted Values')\nax1.set_ylabel('Standardized Residuals')","35be1531":"# Use Breush-Pagan Test to check for heteroskedasticity.\n\ntest = smd.het_breuschpagan(ols_model_residuals, ols_model.model.exog)\n\nif test[1] > 0.05:\n    print('There is not enough evidence to conclude that there is heteroskedasticity in the data.')\nelse:\n    print('There is enough evidence to conclude that there is heteroskedasticity in the data.')","0b5ec1a1":"# Residuals vs Leverage plot.\n\nfig7, ax1 = plt.subplots(figsize=(12,8))\n\nplt.scatter(ols_model_leverage, ols_model_norm_residuals, alpha=0.5)\n\nsns.regplot(ols_model_leverage, ols_model_norm_residuals, ax=ax1, \\\n              scatter=False, ci=False, lowess=True, \\\n              line_kws={'color': 'red', 'lw': 2, 'alpha': 0.5})\n\nax1.set_xlim(min(ols_model_leverage)-0.001, max(ols_model_leverage)+0.001)\nax1.set_title('Residuals vs Leverage')\nax1.set_xlabel('Leverage')\nax1.set_ylabel('Standardized Residuals')","4489ed94":"# Use regression equation to roughly predict scoring average improvement based on increase in driving distance.\n\npredictions = [(x,\n               (lambda x: -0.0486*x)(x),\n               (lambda x: -0.0486*x)(x)*0.079) for x in range(5, 101, 5)]\n\nprint('{:20} {:16}'.format('YARDS ADDED TO', 'SCORING AVG'))\nprint('{:20} {:20} {}'.format('AVG DRIVE DIST', 'IMPROVEMENT', 'ADJUSTED'))\nprint('='*50)\n\nfor x, y, z in predictions:\n    print('{:>14} {:17.2f} {:17.2f}'.format('+'+str(x)+' yards', y, z))","70b1195e":"The goal of this notebook is to use ordinary least squares regression to provide golfers with a meaningful way to improve their scoring average. I want to make clear that this regression likely won't be great for predicting scoring averages. There are many variables (nearly 1500 in the dataset) that can be used to predict someone's average golf score. Due to this, it's unlikely that any single variable is going to be able to reliably predict an all-encompassing statistic such as scoring average. So all I want to do is see if there's some facet of golf that players can focus on improving. If they target that part of their game, then perhaps thier scoring average will consequently improve . Thus this OLS regression will aim to justify some sort of mild linear relationship between a predictor variable and scoring average.","ff2daf73":"<a id='data_explore'><\/a>","81b27dfc":"As hinted by the model summary, the distribution of the residuals appears to be normal.","4c522498":"## Data Cleanup\n[Back to Table of Contents](#ToC)","f3f46631":"<a id='data_clean'><\/a>","5a9dc439":"[](http:\/\/)<a id='OLS_analysis'><\/a>","fd259498":"After looking through the correlations, I decided to choose average driving distance as the predictor for the regression. There are many variables that have a better linear relationship with scoring average, but most are not meaningful. There are many variables that are the same statistic but are measured differently. For example, \"Top 10 Finishes - (1ST)\" and \"Top 10 Finishes - (TOP 10)\". This correlation will be high because when someone finishes in first place, they're also finishing in the top 10. Another example of a pair that isn't significant for our use would be \"Birdie Average - (AVG)\" and \"Scoring Average - (AVG)\". These two are always mutually inclusive because in order to increase or decrease one, the other is going to follow suit. Besides that, advising someone to increase their birdie average in order to improve their scoring average is essentially just telling them to improve their scoring average. I was looking for a predictor that's a more specific part of someone's golf game, so they can focus on it and hopefully improve their overall game.","99c14f3d":"## OLS Model Analysis\n[Back to Table of Contents](#ToC)","a439accc":"There is a clear negative linear relationship between the predictor and response variable.","73ea6223":"The final analysis plot doesn't show any points that could be outliers. There are a handful that have more leverage than the majority of points, but comparatively, it isn't much more leverage.","3e548004":"<a id='summary'><\/a>","e953e96b":"# Ordinary Least Squares Regression\n[Back to Table of Contents](#ToC)\n\nIn this case, it's a good idea to also remove the zeros from the data before using it for the OLS model. The cluster of zeros could\ncreate significant bias with the model because there are so many of them.","e08e0b6e":"It's important to note that in golf, a lower score is a better score. So we're looking for a strong negative linear relationship. That's to say that as the predictor variable increases, the scoring average will decrease.","b6555cc7":"# Table of Contents\n* [Data Exploration](#data_explore)\n    * [Data Cleanup](#data_clean)\n    * [Correlation Analysis](#corr_analysis)\n* [Ordinary Least Squares Regression Model](#OLS)\n    * [OLS Model Analysis](#OLS_analysis)     \n* [Summary](#summary)  ","ff462d3a":"This capitalizes on the normal distribution assumption. Visually, atleast 95% of the residuals are within 2 standard deviations of the mean. There are a few points in the top right that grab my attention. They're at the edge of the distribution and they have larger residual values than the rest. These points should be examined in the proceding plots to determine if they have too much influence on the model.","c47bd280":"<a id='ToC'><\/a>","329074c9":"## Correlation Analysis\n[Back to Table of Contents](#ToC)\n\nI chose scoring average as the dependent variable because I figured it would be a good measure for overall golf performance. Now from this point, the task is to find another statistical variable that has a linear relationship to scoring average. If the relationship is strong enough and significant, then it can be used as the independent variable for predicting in the regression model.","826dd935":"The residuals certainly appear homoskedastic, points are uniformly centered around the 0.0 center line.","130f0c3e":"<a id='corr_analysis'><\/a>","1b23a24d":"The adjusted co-efficient of determination is very low, but that was expected. Golf is a complicated sport, there are many different parts of golf that all contribute towards overall performance. This model explains about 7.9% of the variation in scoring average. Although this model could never be used for reliable predicting, it can still be useful. The remainder of the summary looks good, the error distribution seems to be Gaussian. ","d809a275":"# Data Exploration\n[Back to Table of Contents](#ToC)\n\nThis dataset consists of many measures for individual golfers on the PGA Tour. Each golfer has data for nearly 1500 variables, which is also grouped by date. \nSo after each week, each golfer's data is updated and a new entry is made for them.","664b3ca1":"# Summary\n[Back to Table of Contents](#ToC)\n\nOverall, the model fulfills the ordinary least squares regression requirements. The model's errors are: independent of driving distance, have constant variance, a mean of zero, are not correlated with eachother, and have a normal distribution. Also, the predictor and response variables have a linear relationship and there aren't any large outliers from the model. The only caveat of the model is it's accuracy, which doesn't negate its usefullness. An adjusted r^2 value means that golfers can expect to affect their scoring average by 7.9%. So if they can increase their average driving distance by 20 yards, their scoring average would improve by -0.08. It's not much but as stated earlier, golf has many moving parts. The analysis of this dataset taught me that there isn't one aspect of golf that leads to better scores. Truly every part of golf, from driving to putting, plays an almost equal role in overall performance.\n\nThe next step towards finding accurate and significant predictors for golf success, would be to make a multiple regression model. If we can find several predictors, then together they can provide a more accurate prediction. Naturally we would be taking into account more aspects of golf, therefore more of the overall variation in scoring average could be accounted for. The problem would be to sort through the multicollinearity of this data set. Many variables are closely linked, so we must be careful when choosing predictors. ","3116a23a":"<a id='OLS'><\/a>","be326e88":"The data seems to be zero-inflated. This makes sense because there are more than 1000 golfers in the dataset, and many either don't play often, or certain data just isn't collected from them because they're lesser known players.","eef61546":"In this plot, there appears to be possible heteroskedasticity. Perhaps the independent variable data did not have constant variance?"}}