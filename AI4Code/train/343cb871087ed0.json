{"cell_type":{"5e6eb2bc":"code","602fc27a":"code","e2df7e84":"code","af2c7b87":"code","d113c805":"code","2de208c2":"code","14d03c99":"code","84468928":"code","d1ec178c":"code","92c93cb9":"code","bf74d572":"code","84e224ad":"code","7ba569f6":"code","30a628a7":"code","02b3e9b1":"code","297aa1cf":"code","714b0d31":"code","8b2d6b18":"code","aee18181":"code","388d83ac":"code","f3f01133":"code","cbdf1da7":"code","d54e5733":"code","e5ca7d36":"code","3006a61e":"code","353a1781":"code","8780f99f":"code","25204bde":"code","868e5fd9":"code","aa8d1631":"code","cf19ca42":"code","ae2f84a0":"code","d567dd8b":"code","73106d37":"code","244ce9d5":"code","f8141f21":"code","148efd88":"code","4cbc7e77":"code","7ea53f90":"code","d985d95b":"code","2031a228":"code","d0329c2f":"code","4b88f198":"markdown","db2315d6":"markdown","0420cc58":"markdown","30830b99":"markdown","6a829cb6":"markdown","181bc99d":"markdown","e396a2f1":"markdown","c11ea515":"markdown","709e101a":"markdown","66d1ce8a":"markdown","b305b405":"markdown","8cedf981":"markdown","5ab03566":"markdown","f88a2c6d":"markdown","ac59a769":"markdown","af305f43":"markdown","444f6336":"markdown","aee92372":"markdown","1dea6ab8":"markdown","f3707ea4":"markdown","6d11c821":"markdown","569af760":"markdown","4133c250":"markdown","c248c9ff":"markdown","ad5d1f5f":"markdown","4000ffef":"markdown","791bb323":"markdown","e22cac26":"markdown","4882d79f":"markdown","477e5787":"markdown","aa14d744":"markdown","3bdfb5ab":"markdown","01e6f73e":"markdown","5b7ccf9d":"markdown","cf45b988":"markdown","e377848d":"markdown","10a99602":"markdown","59a66e22":"markdown","d6e281f9":"markdown","60e00db1":"markdown","530be281":"markdown","542b4f0e":"markdown","58849f5f":"markdown","b1796d22":"markdown","09a5b445":"markdown","70245156":"markdown","5cafeda9":"markdown","ab58c39f":"markdown","ff96c795":"markdown","587799b0":"markdown","df60de30":"markdown","476b9e27":"markdown","4b5943cd":"markdown","bf01acdb":"markdown","a00d544b":"markdown","634378b0":"markdown","648f1049":"markdown","9ef53f1c":"markdown","d613a293":"markdown","789a3931":"markdown","0807278c":"markdown","4c755f87":"markdown","2b10654a":"markdown","c7923e26":"markdown","122b6aed":"markdown","cd4c5834":"markdown","a2596e78":"markdown","9994d0ab":"markdown","ce0b19ff":"markdown","9165a39c":"markdown","141d1ae2":"markdown","826b95f1":"markdown","7aab4962":"markdown","704a8560":"markdown","b99781a7":"markdown","5020aad1":"markdown","b4bf4133":"markdown","e4eba9e8":"markdown","7d0bab3d":"markdown","32098e22":"markdown","b297aba9":"markdown","8a220e72":"markdown","3bd8bb35":"markdown","b71d1249":"markdown","9fb8e04a":"markdown","92ea8446":"markdown","713ced95":"markdown"},"source":{"5e6eb2bc":"from IPython.display import YouTubeVideo\nYouTubeVideo('5cFUZ03Sbhc', width=800, height=400)","602fc27a":"import pandas as pd\nimport numpy as np\nimport os\nimport squarify\n# visualization\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport missingno as msno\nimport networkx as nx\nimport plotly.offline as py\nimport plotly.tools as tls\nimport plotly.graph_objs as go\npy.offline.init_notebook_mode(connected=True)\n\nfrom IPython.core.display import display, HTML\nimport plotly.graph_objs as go\nimport plotly.express as px\nfrom bokeh.io import output_notebook, show\nfrom bokeh.layouts import column, row\nfrom bokeh.models import LinearAxis\nfrom bokeh.palettes import Spectral11\nfrom bokeh.plotting import figure\nfrom bokeh.models.ranges import Range1d\nfrom ipywidgets import interact, interactive, fixed\noutput_notebook()\nimport ipywidgets as widgets\nfrom IPython.display import display\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n#Importing the 2019 Dataset\nquestion = pd.read_csv('..\/input\/kaggle-survey-2019\/questions_only.csv')\nmcq = pd.read_csv('..\/input\/kaggle-survey-2019\/multiple_choice_responses.csv')\nbase_dir = '..\/input\/kaggle-survey-2018\/'\nfileName = 'multipleChoiceResponses.csv'\nfilePath = os.path.join(base_dir,fileName)\nsurvey2_expanded = pd.read_csv(filePath) \nresponsesOnly_expanded = survey2_expanded[1:]\nsurveySchemaPath = os.path.join(base_dir,'SurveySchema.csv')\nsurveySchema = pd.read_csv(surveySchemaPath)\nmcq18 = os.path.join(base_dir,'multipleChoiceResponses.csv')\nmcq18 = pd.read_csv(mcq18)\n#Importing the 2018 Dataset\n#Importing the 2017 Dataset\nmcq17=pd.read_csv('..\/input\/kaggle-survey-2017\/multipleChoiceResponses.csv',encoding='ISO-8859-1')","e2df7e84":"# To display all the columns of dataframe\npd.set_option('display.max_columns', 500)\nmcq.head()","af2c7b87":"#lets see the shape of the dataset\nmcq.shape","d113c805":"#lets see list of questions answered by respondents\n\nq = ''.join([f'<li>{i}<\/li>' for i in question.T[0][1:]])\ndisplay(HTML(f'<h1 style=\"color:#20639B\">Question List of  Kaggle ML & DS Survey 2019<\/h1><ol>{q}<\/ol>'))","2de208c2":"pt1 = mcq[['Q1', 'Q2']]\npt1 = pt1.rename(columns={\"Q1\": \"Age\", \"Q2\": \"Gender\"})\npt1.drop(0, axis=0, inplace=True)\n# plotting to create pie chart \nplt.figure(figsize=(18,12))\nplt.subplot(221)\npt1[\"Gender\"].value_counts().plot.pie(autopct = \"%1.0f%%\",colors = sns.color_palette(\"prism\",5),startangle = 60,labels=[\"Male\",\"Female\",\"Prefer not to say\",\"Prefer to self-describe\"],\nwedgeprops={\"linewidth\":2,\"edgecolor\":\"k\"},explode=[.1,.1,.2,.3],shadow =True)\nplt.title(\"Distribution of Gender\")\nplt.subplot(222)\nax = pt1[\"Age\"].value_counts().plot(kind=\"barh\")\n\nplt.title(\"Age wise distribution\")\nplt.show()","14d03c99":"plt.figure(figsize=(12,6))\nsorted_age=['18-21','22-24','25-29','30-34','35-39','40-44','45-49','50-54','55-59','60-69','70+']\nsns.countplot(x='Age', hue='Gender',order=sorted_age, data=pt1 )\nplt.title('Age wise Gender distribution',fontsize=18)\nplt.ylabel('Number of People', fontsize = 16.0) # Y label\nplt.xlabel('Age', fontsize = 16) # X label\nplt.show()","84468928":"plt.figure(figsize=(12,6))\npt2 = mcq[['Q2','Q4', 'Q5']]\npt2 = pt2.rename(columns={\"Q2\":\"Gender\",\"Q4\": \"Education\", \"Q5\": \"title\"})\npt2.drop(0, axis=0, inplace=True)\n\n# Replacing the ambigious education names with easy to use names\npt2['Education'].replace(\n                                                   {'Master\u2019s degree':'MS',\n                                                    'Bachelor\u2019s degree':'Grad',\n                                                    \"Doctoral degree\":'Doct',\n                                                    \"Some college\/university study without earning a bachelor\u2019s degree\":'Under Grad',\n                                                    \"Professional degree\":\"Professional\",\n                                                   \"I prefer not to answer\":\"Prefer NA\",\n                                                   \"No formal education past high school\":\"No edu\"},inplace=True)\n\nsns.countplot(x='Education', data=pt2 )\nplt.title('Education wise distribution',fontsize=18)\nplt.ylabel('Number of People', fontsize = 16.0) # Y label\nplt.xlabel('Education', fontsize = 16) # X label\nplt.show()\n\nplt.figure(figsize=(12,6))\n\nsns.countplot(y='title', data=pt2 )\nplt.title(\"Job Profile Distribution\", fontsize=18)\n\nplt.ylabel('Job Profile', fontsize = 16.0) # Y label\nplt.xlabel('Number of People', fontsize = 16) # X label\nplt.show()","d1ec178c":"plt.figure(figsize=(16,8))\n\nsns.countplot(x='title',hue='Education', data=pt2 )\nplt.title(\"Job Profile vs Education Distribution\", fontsize=18)\n\nplt.ylabel('Number of People', fontsize = 16.0) # Y label\nplt.xlabel('Job Profile', fontsize = 16) # X label\nplt.xticks(rotation=45)\nplt.show()","92c93cb9":"# segmenting income into five different groups lower, lower middle, upper middle, higher and very higher\nlow = dict.fromkeys(['$0-999','1,000-1,999','2,000-2,999','3,000-3,999','4,000-4,999','5,000-7,499','7,500-9,999'], 'Lower Income group')    \nlow_mid = dict.fromkeys(['10,000-14,999','15,000-19,999','20,000-24,999','25,000-29,999','30,000-39,999','40,000-49,999'], 'Lower Middle Income group')    \nup_mid = dict.fromkeys(['50,000-59,999','60,000-69,999','70,000-79,999','80,000-89,999','90,000-99,999'], 'Upper Middle Income group')    \nhigh = dict.fromkeys(['100,000-124,999','125,000-149,999','150,000-199,999','200,000-249,999'], 'High Income group')    \nvery_high = dict.fromkeys(['250,000-299,999','300,000-500,000','> $500,000'], 'Very High Income group')    \n\nmcq['Q10'] = mcq['Q10'].replace(low)\nmcq['Q10'] = mcq['Q10'].replace(low_mid)\nmcq['Q10'] = mcq['Q10'].replace(up_mid)\nmcq['Q10'] = mcq['Q10'].replace(high)\nmcq['Q10'] = mcq['Q10'].replace(very_high)\n\nsns.set_context(rc = {'patch.linewidth': 0.0})\nsns.barplot(mcq['Q10'].value_counts()[0:5].values,mcq['Q10'].value_counts()[0:5].index,palette=('bright'),color = '#007b7f')\n\nplt.xticks(rotation=90)\nfig=plt.gcf()\nfig.set_size_inches(12,6)\nplt.title('Income Distribution')\nplt.show()","bf74d572":"pt2 = mcq[['Q5','Q10']]\npt2 = pt2.rename(columns={\"Q5\":\"title\",\"Q10\": \"Income\"})\npt2.drop(0, axis=0, inplace=True)\nplt.figure(figsize=(16,8))\nsns.countplot(x='title',hue='Income', data=pt2 )\nplt.title(\"Job Profile wise Income Distribution\", fontsize=18)\n\nplt.ylabel('Number of People', fontsize = 16.0) # Y label\nplt.xlabel('Job Profile', fontsize = 16) # X label\nplt.xticks(rotation=65)\nplt.show()","84e224ad":"def return_percentage(data,question_part):\n    \"\"\"Calculates percent of each value in a given column\"\"\"\n    total = data[question_part].count()\n    counts_df= data[question_part].value_counts().to_frame()\n    percentage_df = (counts_df*100)\/total\n    return percentage_df\n\ndef plot_bar(data,question,title,x_axis_title):\n    df = return_percentage(data,question)\n    \n    trace1 = go.Bar(\n                    y = df.index,\n                    x = df[question][0:10],\n                    orientation='h',\n                    marker = dict(color='LightSkyBlue',line=dict(color='black',width=1)),\n                    text = df.index)\n    data = [trace1]\n    layout = go.Layout(barmode = \"group\",title=title,width=800, height=500, \n                       xaxis= dict(title=x_axis_title),\n                       yaxis=dict(autorange=\"reversed\"),\n                       showlegend=False)\n    fig = go.Figure(data = data, layout = layout)\n    fig.show()\n\n\npt_country = mcq[['Q3']]\npt_country = pt_country.rename(columns={\"Q3\": \"Country\"})\npt_country.drop(0, axis=0, inplace=True)\n# Replacing the ambigious countries name with Standard names\npt_country['Country'].replace( {'Viet Nam':'Vietnam',\n                                \"People 's Republic of China\":'China',\n                                \"United Kingdom of Great Britain and Northern Ireland\":'United Kingdom',\n                                \"Hong Kong (S.A.R.)\":\"Hong Kong\"},inplace=True)\n# filtering top 10 countries on the basis of total number of  respondents\ntop10= pt_country['Country'].value_counts().head(10)\nfig = px.choropleth(top10.values, locations=top10.index,\n                    locationmode='country names',\n                    color=top10.values,\n                    color_continuous_scale=px.colors.sequential.Plasma)\nfig.update_layout(title=\"Top 10 Countrywise Distribution of Respondents\")\nfig.show()\nplot_bar(pt_country,'Country','Top 10 countries of Respondents','Percentage of Respondents')","7ba569f6":"db_prog = [\"Python\",\"R \",\"SQL\",\"C\",\"C++\",\"Java\",\"Javascript\",\"TypeScript\", \"Bash\",\n      \"MATLAB\"]\nprog_count = [sum(~mcq.Q18_Part_1.isna()), sum(~mcq.Q18_Part_2.isna()), sum(~mcq.Q18_Part_3.isna()), \n            sum(~mcq.Q18_Part_4.isna()), sum(~mcq.Q18_Part_5.isna()),\n             sum(~mcq.Q18_Part_6.isna()), sum(~mcq.Q18_Part_7.isna()), sum(~mcq.Q18_Part_8.isna()), \n            sum(~mcq.Q18_Part_9.isna()), sum(~mcq.Q18_Part_10.isna())]\n\ndf_prog = pd.DataFrame({\"Language\": db_prog, \"percentage\": np.array(prog_count) * 100 \/ mcq.shape[0]})\ndf_prog.sort_values(\"percentage\", ascending=False, inplace=True)\nsns.barplot(y='Language',x='percentage', data=df_prog ,palette=('bright'))\nplt.xticks(rotation=90)\nfig=plt.gcf()\nfig.set_size_inches(10,8)\nplt.title('Programming Language used in regualar basis',fontsize=18)\nplt.show()\nsns.barplot(mcq['Q19'].value_counts()[0:12].values,mcq['Q19'].value_counts()[0:12].index,palette=('inferno'))\nplt.xticks(rotation=90)\nfig=plt.gcf()\nfig.set_size_inches(10,8)\nplt.title('Recommended Programming Language for Aspiring Data Scientists',fontsize=18)\nplt.show()","30a628a7":"fig = go.Figure()\n\nfig.add_trace(go.Funnel(\n    name = 'India',\n    y = [\"Python\",\"R\",\"SQL\",\"C++\",\"C\",\"MATLAB\",\"Java\",\"Javascript\",\"Bash\"],\n    x = [2728,230,122,63,53,35,12,10,6],\n    textinfo = \"value+percent initial\"))\n\nfig.add_trace(go.Funnel(\n    name = 'United States',\n    orientation = \"h\",\n    y = [\"Python\",\"R\",\"SQL\",\"C++\",\"MATLAB\",\"C\",\"Java\",\"Javascript\",\"Bash\"],\n    x = [1765, 260, 240, 23, 17,12,13,7,6],\n    textposition = \"inside\",\n    textinfo = \"value+percent previous\"))\n\nfig.add_trace(go.Funnel(\n    name = 'China',\n    orientation = \"h\",\n    y = [\"Python\",\"R\",\"SQL\",\"C++\",\"MATLAB\",\"C\",\"Java\",\"Javascript\",\"Bash\"],\n    x = [326, 10, 11, 6, 12, 11,5,0,2],\n    textposition = \"outside\",\n    textinfo = \"value+percent total\"))\nfig.update_layout(\n                  title_text=\"Pragramming Language Distribution of India vs US vs China\", width=1200, height=600)\n\nfig.show()","02b3e9b1":"from IPython.display import YouTubeVideo\nYouTubeVideo('Og847HVwRSI', width=800, height=400)\n","297aa1cf":"plt.style.use('fivethirtyeight')\nsns.barplot(mcq['Q15'].value_counts()[0:7].values,mcq['Q15'].value_counts()[0:7].index,palette=('bright'))\nplt.xticks(rotation=90)\nfig=plt.gcf()\nfig.set_size_inches(10,8)\nplt.title('Coding Experience in years')\nplt.show()","714b0d31":"plt.figure(figsize=(14,8))\nptML = mcq[['Q15','Q10']]\nptML = ptML.rename(columns={\"Q15\":\"code_experience\",\"Q10\": \"Income\"})\nptML.drop(0, axis=0, inplace=True)\nsorted_exp=['I have never written code','< 1 years','1-2 years','3-5 years','5-10 years','10-20 years','20+ years']\nsns.countplot(x='code_experience',hue='Income',order=sorted_exp, data=ptML )\nplt.title(\"Coding Experience wise Income Distribution\", fontsize=18)\n\nplt.ylabel('Number of People', fontsize = 16.0) # Y label\nplt.xlabel('Coding Experience in years', fontsize = 16) # X label\nplt.xticks(rotation=45)\nplt.show()","8b2d6b18":"\ndb = [\"MySQL\",\"PostgreSQL \",\"SQLite\",\"MS SQL Server\",\"Oracle\",\"MS Access\",\"AWS Relational DB\",\"AWS DynamoDB\", \"Azure SQL\",\n      \"Google Cloud SQL\"]\ndb_count = [sum(~mcq.Q34_Part_1.isna()), sum(~mcq.Q34_Part_2.isna()), sum(~mcq.Q34_Part_3.isna()), \n            sum(~mcq.Q34_Part_4.isna()), sum(~mcq.Q34_Part_5.isna()),\n             sum(~mcq.Q34_Part_6.isna()), sum(~mcq.Q34_Part_7.isna()), sum(~mcq.Q34_Part_8.isna()), \n            sum(~mcq.Q34_Part_9.isna()), sum(~mcq.Q34_Part_10.isna())]\n\ndf_db = pd.DataFrame({\"Databases\": db, \"db_percentage\": np.array(db_count) * 100 \/ mcq.shape[0]})\ndf_db.sort_values(\"db_percentage\", ascending=False, inplace=True)\n\ntrace=go.Scatter(x=df_db['Databases'].tolist(),y=df_db['db_percentage'].tolist(),mode=\"markers\",marker=dict(symbol='hexagram',sizemode = 'diameter',\n        sizeref = 1,\n        size = 30,\n        colorscale='Portland',\n        reversescale=True,\n        showscale=True,\n        color = df_db['db_percentage'].tolist(),))\nlayout = dict(title={'text': \"Popular Databases in Use\",'y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'},yaxis=dict(title='Count',linecolor='rgba(255,255,255, 0.8)',showgrid=True,gridcolor='rgba(255,255,255,0.2)'),\n               xaxis= dict(title= 'Donor States',linecolor='rgba(255,255,255, 0.8)',showgrid=True,gridcolor='rgba(128,128,0,0.2)'),margin=go.Margin(l=50,r=30),paper_bgcolor='rgb(105,105,105)',\n               plot_bgcolor='rgb(105,105,105)',font= {'color': '#FFFFFF'})\ndata=[trace]\n\n\nupdatemenus=list([\n    dict(\n        buttons=list([   \n            dict(\n                args=[{'marker.symbol':'hexagram'}],\n                label='Hexagram',\n                method='restyle'\n            ),\n            dict(\n                args=[{'marker.symbol':'Circle'}],\n                label='Circle',\n                method='restyle'\n            ),\n            dict(\n                args=[{'marker.symbol':'triangle-up'}],\n                label='Triangle',\n                method='restyle'\n            ) \n        ]),\n        direction = 'down',\n        pad = {'r': 10, 't': 10},\n        showactive = True,\n        x = 0.01,\n        xanchor = 'left',\n        y = 1.15,\n        yanchor = 'top' \n    ),\n])\n\nannotations = list([\n    dict(text='Select Marker type:', x=0.1, y=1.2, yref='paper', align='center', showarrow=False)\n])\nlayout['updatemenus'] = updatemenus\nlayout['annotations'] = annotations\n\nfig=go.Figure(data=data,layout=layout)\n\npy.iplot(fig)","aee18181":"media = [\"Twitter\", \"HackerNews\", \"Reddit\", \"Kaggle\", \"Forums\", \"YouTube\", \"Podcasts\", \"Blogs\", \"Journals\", \"Slack\"]\nmedia_count = [sum(~mcq.Q12_Part_1.isna()), sum(~mcq.Q12_Part_2.isna()), sum(~mcq.Q12_Part_3.isna()), sum(~mcq.Q12_Part_4.isna()), sum(~mcq.Q12_Part_5.isna()),\n               sum(~mcq.Q12_Part_6.isna()), sum(~mcq.Q12_Part_7.isna()), sum(~mcq.Q12_Part_8.isna()), sum(~mcq.Q12_Part_9.isna()), sum(~mcq.Q12_Part_10.isna())]\n\npt_media = pd.DataFrame({\"media\": media, \"media_percentage\": np.array(media_count) * 100 \/ mcq.shape[0]})\npt_media.sort_values(\"media_percentage\", ascending=False, inplace=True)\nplt.style.use('fivethirtyeight')\nsns.barplot(pt_media['media_percentage'],pt_media['media'],palette=('coolwarm'))\nplt.xticks(rotation=90)\nfig=plt.gcf()\nfig.set_size_inches(10,8)\nplt.title('Media Sources Responsible for Learning Datascience')\nplt.show()","388d83ac":"platform = [\"Udacity\", \"Coursera\", \"edX\", \"DataCamp\", \"DataQuest\", \"Kaggle\", \"Fast.ai\", \"Udemy\", \"LinkedIn\", \"University\"]\nplatform_count = [sum(~mcq.Q13_Part_1.isna()), sum(~mcq.Q13_Part_2.isna()), sum(~mcq.Q13_Part_3.isna()), sum(~mcq.Q13_Part_4.isna()), sum(~mcq.Q13_Part_5.isna()),\n                  sum(~mcq.Q13_Part_6.isna()), sum(~mcq.Q13_Part_7.isna()), sum(~mcq.Q13_Part_8.isna()), sum(~mcq.Q13_Part_9.isna()), sum(~mcq.Q13_Part_10.isna())]\n\npt_platform = pd.DataFrame({\"platform\": platform, \"platform_percentage\": np.array(platform_count) * 100 \/ mcq.shape[0]})\npt_platform.sort_values(\"platform_percentage\", ascending=False, inplace=True)\nplt.style.use('fivethirtyeight')\nsns.barplot(pt_platform['platform_percentage'],pt_platform['platform'],palette=('cubehelix'))\nplt.xticks(rotation=90)\nfig=plt.gcf()\nfig.set_size_inches(10,8)\nplt.title('Platforms responsible for Learning Datascience')\nplt.show()","f3f01133":"sns.barplot(mcq['Q8'].value_counts()[0:6].values,mcq['Q8'].value_counts()[0:6].index,palette=('inferno'))\nplt.xticks(rotation=90)\nfig=plt.gcf()\nfig.set_size_inches(10,8)\nplt.title('Machine Learning working status in Industries')\nplt.show()","cbdf1da7":"pt_mlexp = mcq[['Q23']]\npt_mlexp = pt_mlexp.rename(columns={\"Q23\": \"ML Experience\"})\npt_mlexp.drop(0, axis=0, inplace=True)\nplot_bar(pt_mlexp,'ML Experience','Machine Learning Experience of Respondents','Percentage of Respondents')","d54e5733":"pt_ds = mcq[['Q7']]\npt_ds = pt_ds.rename(columns={\"Q7\": \"DS\"})\npt_ds.drop(0, axis=0, inplace=True)\nplot_bar(pt_ds,'DS','Plot showing people responsible for Datascience workload','Percentage of Respondents')","e5ca7d36":"plt.figure(figsize=(14,8))\nptML = mcq[['Q23','Q10']]\nptML = ptML.rename(columns={\"Q23\":\"ML_experience\",\"Q10\": \"Income\"})\nptML.drop(0, axis=0, inplace=True)\nsorted_exp=['< 1 years','1-2 years','2-3 years','3-4 years','4-5 years','5-10 years','10-15 years','20+ years']\nsns.countplot(x='ML_experience',hue='Income',order=sorted_exp, data=ptML )\nplt.title(\"ML Experience wise Income Distribution\", fontsize=18)\n\nplt.ylabel('Number of People', fontsize = 16.0) # Y label\nplt.xlabel('ML Experience in years', fontsize = 16) # X label\nplt.xticks(rotation=45)\nplt.show()","3006a61e":"import plotly.graph_objects as go\nlabels_time = [\"Analyze and understand data\", \"Build and\/or run the data infrastructure\", \"Build prototypes to explore applying ML to new areas\", \"Build machine learning service to improve my product\", \"Experimentation and iteration to improve existing ML models\", \"Research\", \"None\"]\ntime_count = [sum(~mcq.Q9_Part_1.isna()), sum(~mcq.Q9_Part_2.isna()), sum(~mcq.Q9_Part_3.isna()), sum(~mcq.Q9_Part_4.isna()), sum(~mcq.Q9_Part_5.isna()),\n             sum(~mcq.Q9_Part_6.isna()), sum(~mcq.Q9_Part_7.isna())]\n\ncolors = ['gold', 'mediumturquoise', 'darkorange', 'lightgreen','blue','yellow','darkgreen']\n\n\nfig = go.Figure(data=[go.Pie(labels=[\"Analyze and understand data\", \"Build and\/or run the data infrastructure\", \"Build prototypes to explore applying ML to new areas\", \"Build machine learning service to improve my product\", \"Experimentation and iteration to improve existing ML models\", \"Research\", \"None\"],\n                             values=[sum(~mcq.Q9_Part_1.isna()), sum(~mcq.Q9_Part_2.isna()), sum(~mcq.Q9_Part_3.isna()), sum(~mcq.Q9_Part_4.isna()), sum(~mcq.Q9_Part_5.isna()),\n             sum(~mcq.Q9_Part_6.isna()), sum(~mcq.Q16_Part_7.isna())])])\nfig.update_layout(\n    title={\n        'text': \"Activities taking important part of time at workplace\",\n        'y':0.9,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'}\n        )\nfig.show()\n","353a1781":"ml_exp=pd.concat([pd.crosstab(mcq['Q9_Part_1'][1:],mcq['Q23']),pd.crosstab(mcq['Q9_Part_2'][1:],mcq['Q23']),pd.crosstab(mcq['Q9_Part_3'][1:],mcq['Q23']),\n                  pd.crosstab(mcq['Q9_Part_4'][1:],mcq['Q23']),pd.crosstab(mcq['Q9_Part_5'][1:],mcq['Q23']),pd.crosstab(mcq['Q9_Part_6'][1:],mcq['Q23']),\n                  pd.crosstab(mcq['Q9_Part_7'][1:],mcq['Q23'])])\nml_exp=ml_exp[['< 1 years','1-2 years','2-3 years','3-4 years','4-5 years','5-10 years','10-15 years']]\nml_exp=ml_exp.fillna(0)\n# replacing index values for ease\nml_exp.rename(index={'Analyze and understand data to influence product or business decisions':'Analyze data',\n'Build and\/or run the data infrastructure that my business uses for storing, analyzing, and operationalizing data':'Build data infra',\n'Build prototypes to explore applying machine learning to new areas':'Build prototypes',\n'Build and\/or run a machine learning service that operationally improves my product or workflows':'Build ML serv. to improve prod.',\n'Experimentation and iteration to improve existing ML models':'Expermient',\n 'Do research that advances the state of the art of machine learning':'Research',\n 'None of these activities are an important part of my role at work':'None'},inplace=True)\n\ntrace1 = go.Bar(\n    x=ml_exp.index,\n    y=ml_exp['< 1 years'].values,\n    name='< 1 years',\n    marker=dict(\n    color=\"#66545e\")\n)\ntrace2 = go.Bar(\n    x=ml_exp.index,\n    y=ml_exp['1-2 years'].values,\n    name='1-2 years',\n    marker=dict(\n    color='#15b2b3')\n)\ntrace3 = go.Bar(\n    x=ml_exp.index,\n    y=ml_exp['2-3 years'].values,\n    name='2-3 years',\n    marker=dict(\n    color='#ff920c')\n)\ntrace4 = go.Bar(\n    x=ml_exp.index,\n    y=ml_exp['3-4 years'].values,\n    name='3-4 years',\n    marker=dict(\n    color='#f05e68')\n)\ntrace5 = go.Bar(\n    x=ml_exp.index,\n    y=ml_exp['4-5 years'].values,\n    name='4-5 years',\n    marker=dict(\n    color='#00a3e0')\n)\ntrace6 = go.Bar(\n    x=ml_exp.index,\n    y=ml_exp['5-10 years'].values,\n    name='5-10 years',\n    marker=dict(\n    color='#4d0e20')\n)\ntrace7 = go.Bar(\n    x=ml_exp.index,\n    y=ml_exp['10-15 years'].values,\n    name='10-15 years',\n    marker=dict(\n    color='#b35a00')\n)\n\n\ndata = [trace1, trace2,trace3,trace4,trace5,trace6,trace7]\nlayout = go.Layout(\n    barmode='group',height=700,width=1400,title='ML Experience wise Activities Distribution',yaxis_title='Number of people',xaxis_title=''\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.offline.iplot(fig, filename='grouped-bar')","8780f99f":"fig = plt.figure(figsize=(14,8))\npt3 = mcq[['Q8','Q10', 'Q11']]\npt3 = pt3.rename(columns={\"Q8\":\"ML\",\"Q10\": \"income\", \"Q11\": \"money_ml\"})\npt3.drop(0, axis=0, inplace=True)\n# Replacing the ambigious ML culture with easy to use names\npt3['ML'].replace(\n                                                   {'We are exploring ML methods (and may one day put a model into production)':'Exploring',\n                                                    'We recently started using ML methods (i.e., models in production for less than 2 years)':'Recent Start',\n                                                    \"We have well established ML methods (i.e., models in production for more than 2 years)\":'Established',\n                                                    \"No (we do not use ML methods)\":'No',\n                                                    \"We use ML methods for generating insights (but do not put working models into production)\":\"Only Insights\",\n                                                   \"I do not know\":\"Dnt Know\"},inplace=True)\n\nsorted_exp = ['$0 (USD)','$1-$99','$100-$999','$1000-$9,999','$10,000-$99,999','> $100,000 ($USD)']\nsns.countplot(x='money_ml', hue='ML', data=pt3, order = sorted_exp )\nplt.title('ML working status vs ML expenditure at workplace')\nplt.ylabel('Frequency', fontsize = 20) # Y label\nplt.xlabel('Yearly expenditure in US $') # X label\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()","25204bde":"labels_ide = [\"Jupyter\", \"RStudio\", \"PyCharm\", \"Atom\", \"MATLAB\", \"VisualStudio\", \"Spyder\", \"Vim\", \"Notepad++\", \"SublimeText\"]\nide_count = [sum(~mcq.Q16_Part_1.isna()), sum(~mcq.Q16_Part_2.isna()), sum(~mcq.Q16_Part_3.isna()), sum(~mcq.Q16_Part_4.isna()), sum(~mcq.Q16_Part_5.isna()),\n             sum(~mcq.Q16_Part_6.isna()), sum(~mcq.Q16_Part_7.isna()), sum(~mcq.Q16_Part_8.isna()), sum(~mcq.Q16_Part_9.isna()), sum(~mcq.Q16_Part_10.isna())]\n\npt_ide = pd.DataFrame({\"ide\": labels_ide, \"ide_percentage\": np.array(ide_count) * 100 \/ mcq.shape[0]})\npt_ide.sort_values(\"ide_percentage\", ascending=False, inplace=True)\n\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=labels_ide, values=ide_count, hole=.3)])\n\nfig.update_layout(\n    title_text=\"IDE Popularity among Respondents\",\n    # Add annotations in the center of the donut pies.\n   annotations=[dict(text='IDE', x=0.5, y=0.5, font_size=18, showarrow=False)])\nfig.show()","868e5fd9":"ml_tool=pd.concat([pd.crosstab(mcq['Q16_Part_1'][1:],mcq['Q23']),pd.crosstab(mcq['Q16_Part_2'][1:],mcq['Q23']),pd.crosstab(mcq['Q16_Part_3'][1:],mcq['Q23']),\n                  pd.crosstab(mcq['Q16_Part_4'][1:],mcq['Q23']),pd.crosstab(mcq['Q16_Part_5'][1:],mcq['Q23']),pd.crosstab(mcq['Q16_Part_6'][1:],mcq['Q23']),\n                  pd.crosstab(mcq['Q16_Part_7'][1:],mcq['Q23'])])\nml_tool=ml_tool[['< 1 years','1-2 years','2-3 years','3-4 years','4-5 years','5-10 years','10-15 years']]\nml_tool=ml_tool.fillna(0)\nml_tool.rename(index={'Jupyter (JupyterLab, Jupyter Notebooks, etc) ':'Jupyter Notebook',\n' Visual Studio \/ Visual Studio Code ':'Visual Studio'},inplace=True)\n\ntrace1 = go.Bar(\n    x=ml_tool.index,\n    y=ml_tool['< 1 years'].values,\n    name='< 1 years',\n    marker=dict(\n    color=\"#66545e\")\n)\ntrace2 = go.Bar(\n    x=ml_tool.index,\n    y=ml_tool['1-2 years'].values,\n    name='1-2 years',\n    marker=dict(\n    color='#15b2b3')\n)\ntrace3 = go.Bar(\n    x=ml_tool.index,\n    y=ml_tool['2-3 years'].values,\n    name='2-3 years',\n    marker=dict(\n    color='#ff920c')\n)\ntrace4 = go.Bar(\n    x=ml_tool.index,\n    y=ml_tool['3-4 years'].values,\n    name='3-4 years',\n    marker=dict(\n    color='#f05e68')\n)\ntrace5 = go.Bar(\n    x=ml_tool.index,\n    y=ml_tool['4-5 years'].values,\n    name='4-5 years',\n    marker=dict(\n    color='#00a3e0')\n)\ntrace6 = go.Bar(\n    x=ml_tool.index,\n    y=ml_tool['5-10 years'].values,\n    name='5-10 years',\n    marker=dict(\n    color='#4d0e20')\n)\ntrace7 = go.Bar(\n    x=ml_tool.index,\n    y=ml_tool['10-15 years'].values,\n    name='10-15 years',\n    marker=dict(\n    color='#b35a00')\n)\n\n\ndata = [trace1, trace2,trace3,trace4,trace5,trace6,trace7]\nlayout = go.Layout(\n    barmode='group',height=700,width=1000,title='Machine Learning Experience wise IDE Usage',yaxis_title='Number of people',xaxis_title=''\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.offline.iplot(fig, filename='grouped-bar')","aa8d1631":"labels_viz = [\"GGplot\/ggplot2\",\"Matplotlib\",\"Altair\",\"Shiny\",\"D3.js\",\"Plotly\",\"Bokeh\",\"Seaborn\", \"Geoplotlib\", \"Leaflet\/Folium\"]\nviz_count = [sum(~mcq.Q20_Part_1.isna()), sum(~mcq.Q20_Part_2.isna()), sum(~mcq.Q20_Part_3.isna()), sum(~mcq.Q20_Part_4.isna()), sum(~mcq.Q20_Part_5.isna()),\n             sum(~mcq.Q20_Part_6.isna()), sum(~mcq.Q20_Part_7.isna()), sum(~mcq.Q20_Part_8.isna()), sum(~mcq.Q20_Part_9.isna()), sum(~mcq.Q20_Part_10.isna())]\n\npt_viz = pd.DataFrame({\"viz\": labels_viz, \"viz_percentage\": np.array(viz_count) * 100 \/ mcq.shape[0]})\npt_viz.sort_values(\"viz_percentage\", ascending=False, inplace=True)\n\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=labels_viz, values=viz_count, hole=.3)])\n\nfig.update_layout(\n    title_text=\"Data Viz Libraries Popularity\",\n    # Add annotations in the center of the donut pies.\n   annotations=[dict(text='Data Viz', x=0.50, y=0.5, font_size=18, showarrow=False)])\nfig.show()\n","cf19ca42":"pt18 = mcq[['Q14']]\npt18 = pt18.rename(columns={\"Q14\":\"data_analysis\"})\npt18.drop(0, axis=0, inplace=True)\n\npt18['data_analysis'].replace( {\"Local development environments (RStudio, JupyterLab, etc.)\":'RStudio\/JupyterLab',\n                                \"Basic statistical software (Microsoft Excel, Google Sheets, etc.)\":'MS Excel\/Google Sheets',\n                                                     \"Cloud-based data software & APIs (AWS, GCP, Azure, etc.)\":'AWS\/GCP\/Azure',\n                                \"Advanced statistical software (SPSS, SAS, etc.)\":'SPSS\/SAS',\n                                \"Business intelligence software (Salesforce, Tableau, Spotfire, etc.)\":'Salesforce\/Tableau\/Spotfire'\n                                \n                                                    },inplace=True)\n\n\nplot_bar(pt18,'data_analysis','Data Analysis Tools popular among Data Scientists','Percentage of Respondents')\n","ae2f84a0":"labels_ml = [\"Linear\/Logistic Regression\",\"Decision Tree\/Random Forest \",\"XGboost\/LGBM\",\"Bayesian\",\"Evolutionary\",\"DNNs\",\"CNNs\",\"GANs\", \"RNNs\", \"BERT\/gpt-2\"]\nml_count = [sum(~mcq.Q24_Part_1.isna()), sum(~mcq.Q24_Part_2.isna()), sum(~mcq.Q24_Part_3.isna()), sum(~mcq.Q24_Part_4.isna()), sum(~mcq.Q24_Part_5.isna()),\n             sum(~mcq.Q24_Part_6.isna()), sum(~mcq.Q24_Part_7.isna()), sum(~mcq.Q24_Part_8.isna()), sum(~mcq.Q24_Part_9.isna()), sum(~mcq.Q24_Part_10.isna())]\n\npt_ml = pd.DataFrame({\"ML Algorithms\": labels_ml, \"Percentage Used\": np.array(ml_count) * 100 \/ mcq.shape[0]})\npt_ml.sort_values(\"Percentage Used\", ascending=False, inplace=True)\n\ntrace=go.Scatter(x=pt_ml['ML Algorithms'].tolist(),y=pt_ml['Percentage Used'].tolist(),mode=\"markers\",marker=dict(symbol='hexagram',sizemode = 'diameter',\n        sizeref = 1,\n        size = 30,\n        colorscale='Portland',\n        reversescale=True,\n        showscale=True,\n        color = ml_count,))\nlayout = dict(title={'text': \"Popular ML Algorithms among Datascientists\",'y':0.9,'x':0.5,'xanchor': 'center','yanchor': 'top'},yaxis=dict(title='Count',linecolor='rgba(255,255,255, 0.8)',showgrid=True,gridcolor='rgba(255,255,255,0.2)'),\n               xaxis= dict(title= 'Machine Learning Algorithms',linecolor='rgba(255,255,255, 0.8)',showgrid=True,gridcolor='rgba(128,128,0,0.2)'),margin=go.Margin(l=50,r=30),paper_bgcolor='rgb(105,105,105)',\n               plot_bgcolor='rgb(105,105,105)',font= {'color': '#FFFFFF'})\ndata=[trace]\n\n\nupdatemenus=list([\n    dict(\n        buttons=list([   \n            dict(\n                args=[{'marker.symbol':'hexagram'}],\n                label='Hexagram',\n                method='restyle'\n            ),\n            dict(\n                args=[{'marker.symbol':'Circle'}],\n                label='Circle',\n                method='restyle'\n            ),\n            dict(\n                args=[{'marker.symbol':'triangle-up'}],\n                label='Triangle',\n                method='restyle'\n            ) \n        ]),\n        direction = 'down',\n        pad = {'r': 10, 't': 10},\n        showactive = True,\n        x = 0.01,\n        xanchor = 'left',\n        y = 1.15,\n        yanchor = 'top' \n    ),\n])\n\nannotations = list([\n    dict(text='Select Marker type:', x=0.1, y=1.2, yref='paper', align='center', showarrow=False)\n])\nlayout['updatemenus'] = updatemenus\nlayout['annotations'] = annotations\n\nfig=go.Figure(data=data,layout=layout)\n\npy.iplot(fig)","d567dd8b":"exp_ml=pd.concat([pd.crosstab(mcq['Q24_Part_1'][1:],mcq['Q23']),pd.crosstab(mcq['Q24_Part_2'][1:],mcq['Q23']),\n                   pd.crosstab(mcq['Q24_Part_3'][1:],mcq['Q23']),pd.crosstab(mcq['Q24_Part_4'][1:],mcq['Q23']),\n                    pd.crosstab(mcq['Q24_Part_5'][1:],mcq['Q23']),pd.crosstab(mcq['Q24_Part_6'][1:],mcq['Q23']),\n                 pd.crosstab(mcq['Q24_Part_7'][1:],mcq['Q23']),pd.crosstab(mcq['Q24_Part_8'][1:],mcq['Q23']),\n                 pd.crosstab(mcq['Q24_Part_9'][1:],mcq['Q23']),pd.crosstab(mcq['Q24_Part_10'][1:],mcq['Q23'])])\nexp_ml=exp_ml[['< 1 years','1-2 years','2-3 years','3-4 years','4-5 years','5-10 years','10-15 years']]\nexp_ml=exp_ml.fillna(0)\n\ntrace1 = go.Bar(\n    x=exp_ml.index,\n    y=exp_ml['< 1 years'].values,\n    name='< 1 years',\n    marker=dict(\n    color=\"#66545e\")\n)\ntrace2 = go.Bar(\n    x=exp_ml.index,\n    y=exp_ml['1-2 years'].values,\n    name='1-2 years',\n    marker=dict(\n    color='#15b2b3')\n)\ntrace3 = go.Bar(\n    x=exp_ml.index,\n    y=exp_ml['2-3 years'].values,\n    name='2-3 years',\n    marker=dict(\n    color='#ff920c')\n)\ntrace4 = go.Bar(\n    x=exp_ml.index,\n    y=exp_ml['3-4 years'].values,\n    name='3-4 years',\n    marker=dict(\n    color='#f05e68')\n)\ntrace5 = go.Bar(\n    x=exp_ml.index,\n    y=exp_ml['4-5 years'].values,\n    name='4-5 years',\n    marker=dict(\n    color='#00a3e0')\n)\ntrace6 = go.Bar(\n    x=exp_ml.index,\n    y=exp_ml['5-10 years'].values,\n    name='5-10 years',\n    marker=dict(\n    color='#4d0e20')\n)\ntrace7 = go.Bar(\n    x=exp_ml.index,\n    y=exp_ml['10-15 years'].values,\n    name='10-15 years',\n    marker=dict(\n    color='#b35a00')\n)\n\n\ndata = [trace1, trace2,trace3,trace4,trace5,trace6,trace7]\nlayout = go.Layout(\n    barmode='group',height=700,width=1000,title='ML Algorithms preference of Experienced Professionals',yaxis_title='Total Usage',xaxis_title='ML Algorithms'\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.offline.iplot(fig, filename='grouped-bar')","73106d37":"labels_frame = [\"Scikit-Learn\",\"Tensorflow \",\"Keras\",\"Random Forest\",\"Xgboost\",\"Pytorch\",\"Caret\",\"LightGBM\", \"Spark MLLib\", \"Fast.ai\"]\nframe_count = [sum(~mcq.Q28_Part_1.isna()), sum(~mcq.Q28_Part_2.isna()), sum(~mcq.Q28_Part_3.isna()), sum(~mcq.Q28_Part_4.isna()), sum(~mcq.Q28_Part_5.isna()),\n             sum(~mcq.Q28_Part_6.isna()), sum(~mcq.Q28_Part_7.isna()), sum(~mcq.Q28_Part_8.isna()), sum(~mcq.Q28_Part_9.isna()), sum(~mcq.Q28_Part_10.isna())]\n\npt_frame = pd.DataFrame({\"ML Frameworks\": labels_frame, \"Percentage Used\": np.array(frame_count) * 100 \/ mcq.shape[0]})\npt_frame.sort_values(\"Percentage Used\", ascending=False, inplace=True)\n\nplt.style.use('classic')\nsns.barplot(pt_frame['ML Frameworks'],pt_frame['Percentage Used'])\nplt.xticks(rotation=90)\nfig=plt.gcf()\nfig.set_size_inches(18,6)\nplt.title('ML Frameworks Popularity')\nplt.show()","244ce9d5":"labels_tools = [\"Auto Data Augment\",\"Auto Feat Engg. \",\"Auto Model Selection\",\"Auto Model Arch Search\",\"Auto Hyperpara Tuning\",\"Auto ML Pipeline\",\"None\"]\ntools_count = [sum(~mcq.Q25_Part_1.isna()), sum(~mcq.Q25_Part_2.isna()), sum(~mcq.Q25_Part_3.isna()), sum(~mcq.Q25_Part_4.isna()), sum(~mcq.Q25_Part_5.isna()),\n             sum(~mcq.Q25_Part_6.isna()), sum(~mcq.Q25_Part_7.isna())]\n\npt_tool = pd.DataFrame({\"ML Tools\": labels_tools, \"Percentage Used\": np.array(tools_count) * 100 \/ mcq.shape[0]})\npt_tool.sort_values(\"Percentage Used\", ascending=False, inplace=True)\n\nplt.style.use('fivethirtyeight')\nsns.barplot(pt_tool['ML Tools'],pt_tool['Percentage Used'],palette=('bright'))\nplt.xticks(rotation=90)\nfig=plt.gcf()\nfig.set_size_inches(18,6)\nplt.title('ML Tools')\nplt.show()\n","f8141f21":"auto_ml = [\"Google_AutoML\", \"Coursera\", \"H20 Driverless AI\", \"Databricks AutoML\", \"DataRobot\", \"TPot\", \"AutoKeras\", \"Auto_ml\", \"Xcessiv\", \"MLbox\"]\nauto_ml_count = [sum(~mcq.Q33_Part_1.isna()), sum(~mcq.Q33_Part_2.isna()), sum(~mcq.Q33_Part_3.isna()), sum(~mcq.Q33_Part_4.isna()), sum(~mcq.Q33_Part_5.isna()),\n                  sum(~mcq.Q33_Part_6.isna()), sum(~mcq.Q33_Part_7.isna()), sum(~mcq.Q33_Part_8.isna()), sum(~mcq.Q33_Part_9.isna()), sum(~mcq.Q33_Part_10.isna())]\n\npt_platform = pd.DataFrame({\"Automl\": auto_ml, \"automl_percentage\": np.array(auto_ml_count) * 100 \/ mcq.shape[0]})\npt_platform.sort_values(\"automl_percentage\", ascending=False, inplace=True)\n\nsns.barplot(pt_platform['automl_percentage'],pt_platform['Automl'],palette=('cubehelix'))\nplt.xticks(rotation=90)\nfig=plt.gcf()\nfig.set_size_inches(18,8)\nplt.title('Automated ML Tools')\nplt.show()","148efd88":"cloud_list = {\n    \"AWS EC2\": sum(~mcq.Q30_Part_1.isna()) * 100 \/ mcq.shape[0],\n    \"GCE\": sum(~mcq.Q30_Part_2.isna()) * 100 \/ mcq.shape[0],\n    \"AWS Lambda\": sum(~mcq.Q30_Part_3.isna()) * 100 \/ mcq.shape[0],\n    \"Azure Virtual Machine\": sum(~mcq.Q30_Part_4.isna()) * 100 \/ mcq.shape[0],\n    \"Google App Engine\": sum(~mcq.Q30_Part_5.isna()) * 100 \/ mcq.shape[0],\n    \"Google Cloud Functions\": sum(~mcq.Q30_Part_6.isna()) * 100 \/ mcq.shape[0],\n     \"AWS Elastic Beanstalk\": sum(~mcq.Q30_Part_6.isna()) * 100 \/ mcq.shape[0],\n     \"Google Kubernetes\": sum(~mcq.Q30_Part_7.isna()) * 100 \/ mcq.shape[0],\n     \"AWS Batch\": sum(~mcq.Q30_Part_8.isna()) * 100 \/ mcq.shape[0],\n     \"Azure Container\": sum(~mcq.Q30_Part_9.isna()) * 100 \/ mcq.shape[0]\n}\n\ndf_cl = pd.DataFrame.from_dict(cloud_list, orient=\"index\", columns=[\"cloud_percentage\"]).reset_index().rename(columns={\"index\": \"Clouds\"})\ndf_cl.sort_values(\"cloud_percentage\", ascending=False, inplace=True)\n\nf = figure(x_range=df_cl.Clouds, plot_width=900, plot_height=400, title=\"Cloud Popularity among Datascientists\")\nf.vbar(x=df_cl.Clouds, top=df_cl.cloud_percentage, width=0.9, color=Spectral11[9], legend=\"% Cloud Computing Usage\")\nf.legend.location=\"top_right\"\nf.legend.click_policy=\"hide\"\nf.xaxis.major_label_orientation=145\n\nshow(f)","4cbc7e77":"mlp = [\"SAS\", \"Cloudera\", \"Azure ML Studio\", \"Google Cloud ML Engine\", \"Google Cloud Vision\", \"Google Cloud Speech to Text\", \n       \"Google Cloud NLP\", \"Rapid Miner\", \"Google Cloud Translation\", \"AWS Sagemaker\"]\nmlp_count = [sum(~mcq.Q32_Part_1.isna()), sum(~mcq.Q32_Part_2.isna()), sum(~mcq.Q32_Part_3.isna()), sum(~mcq.Q32_Part_4.isna()), \n             sum(~mcq.Q32_Part_5.isna()),\n                  sum(~mcq.Q32_Part_6.isna()), sum(~mcq.Q32_Part_7.isna()), sum(~mcq.Q32_Part_8.isna()), sum(~mcq.Q32_Part_9.isna()), \n             sum(~mcq.Q32_Part_10.isna())]\n\npt_mlp = pd.DataFrame({\"MLP\": mlp, \"mlp_count\": np.array(mlp_count) })\npt_mlp.sort_values(\"mlp_count\", ascending=False, inplace=True)\n\n# Get Axis and Figure\nfig, ax = plt.subplots()\n# Our Colormap\ncmap = matplotlib.cm.Pastel1\n# Min and Max Values\nmini = min(pt_mlp['mlp_count'])\nmaxi = max(pt_mlp['mlp_count'])\n# Finding Colors for each tile\nnorm = matplotlib.colors.Normalize(vmin=mini, vmax=maxi)\ncolors = [\"#248af1\", \"#eb5d50\", \"#8bc4f6\", \"#8c5c94\", \"#a170e8\", \"#fba521\", \"#75bc3f\"]\n# Plotting\nsquarify.plot(sizes=pt_mlp['mlp_count'], label=pt_mlp['MLP'], alpha=0.9, color=colors)\n# Removing Axis\nplt.axis('off')\n# Invert Y-Axis\nplt.gca().invert_yaxis()\n# Title\nplt.title(\"Tree map of popular Machine Learning Products\", fontsize=32)\n# Title Positioning\nttl = ax.title\nttl.set_position([.5, 1.05])\n# BG Color\nfig.set_facecolor('#eeffee')\nfig.set_size_inches(18,9)","7ea53f90":"\nbig_data = [\"Google BigQuery\", \"AWS redshift\", \"Databricks\", \"AWS Elastic Mapreduce\", \"Teradata\", \"MS Analysis Services\", \"Google Cloud Dataflow\", \"AWS Athena\", \"AWS Kinesis\", \"Google Clud Pub\/Sub\"]\nbig_data_count = [sum(~mcq.Q31_Part_1.isna()), sum(~mcq.Q31_Part_2.isna()), sum(~mcq.Q31_Part_3.isna()), sum(~mcq.Q31_Part_4.isna()), sum(~mcq.Q31_Part_5.isna()),\n                  sum(~mcq.Q31_Part_6.isna()), sum(~mcq.Q31_Part_7.isna()), sum(~mcq.Q31_Part_8.isna()), sum(~mcq.Q31_Part_9.isna()), sum(~mcq.Q31_Part_10.isna())]\n\npt_bigdata = pd.DataFrame({\"BigData\": big_data, \"bigdata_count\": np.array(big_data_count) })\npt_bigdata.sort_values(\"bigdata_count\", ascending=False, inplace=True)\n\n# Get Axis and Figure\nfig, ax = plt.subplots()\n# Our Colormap\ncmap = matplotlib.cm.gnuplot\n# Min and Max Values\nmini = min(pt_bigdata['bigdata_count'])\nmaxi = max(pt_bigdata['bigdata_count'])\n# Finding Colors for each tile\nnorm = matplotlib.colors.Normalize(vmin=mini, vmax=maxi)\ncolors = [cmap(norm(value)) for value in pt_bigdata['bigdata_count']]\n# Plotting\nsquarify.plot(sizes=pt_bigdata['bigdata_count'], label=pt_bigdata['BigData'], alpha=0.9, color=colors)\n# Removing Axis\nplt.axis('off')\n# Invert Y-Axis\nplt.gca().invert_yaxis()\n# Title\nplt.title(\"Tree map of popular Big Data Analytics Tools\", fontsize=32)\n# Title Positioning\nttl = ax.title\nttl.set_position([.5, 1.05])\n# BG Color\nfig.set_facecolor('#eeffee')\nfig.set_size_inches(18,9)","d985d95b":"  hardware_list = {\n    \"CPUs\": sum(~mcq.Q21_Part_1.isna()) * 100 \/ mcq.shape[0],\n    \"GPUs\": sum(~mcq.Q21_Part_2.isna()) * 100 \/ mcq.shape[0],\n    \"TPUs\": sum(~mcq.Q21_Part_3.isna()) * 100 \/ mcq.shape[0],\n    \"None\": sum(~mcq.Q21_Part_4.isna()) * 100 \/ mcq.shape[0],\n    \"Other\": sum(~mcq.Q21_Part_5.isna()) * 100 \/ mcq.shape[0]\n}\n\ndf_dl_hardware = pd.DataFrame.from_dict(hardware_list, orient=\"index\", columns=[\"hardware_percentage\"]).reset_index().rename(columns={\"index\": \"hardware\"})\ndf_dl_hardware.sort_values(\"hardware_percentage\", ascending=False, inplace=True)\nfig = px.bar(df_dl_hardware, x=\"hardware_percentage\", y=\"hardware\", orientation='h')\nfig.show()","2031a228":"df_hard=pd.concat([pd.crosstab(mcq['Q21_Part_1'][1:],mcq['Q11']),pd.crosstab(mcq['Q21_Part_2'][1:],mcq['Q11']),\n                   pd.crosstab(mcq['Q21_Part_3'][1:],mcq['Q11']),\n                  pd.crosstab(mcq['Q21_Part_4'][1:],mcq['Q11']),pd.crosstab(mcq['Q21_Part_5'][1:],mcq['Q11'])])\ndf_hard=df_hard[['$0 (USD)', '$1-$99','$100-$999','$1000-$9,999','$10,000-$99,999','> $100,000 ($USD)']]\ndf_hard=df_hard.fillna(0)\ntrace1 = go.Bar(\n    x=df_hard.index,\n    y=df_hard['$0 (USD)'].values,\n    name='$0(USD)',\n    marker=dict(\n    color=\"#66545e\")\n)\ntrace2 = go.Bar(\n    x=df_hard.index,\n    y=df_hard['$1-$99'].values,\n    name='$1-99(USD)',\n    marker=dict(\n    color='#15b2b3')\n)\ntrace3 = go.Bar(\n    x=df_hard.index,\n    y=df_hard['$100-$999'].values,\n    name='$100-999(USD)',\n    marker=dict(\n    color='#ff920c')\n)\ntrace4 = go.Bar(\n    x=df_hard.index,\n    y=df_hard['$1000-$9,999'].values,\n    name='$1000-9,999(USD)',\n    marker=dict(\n    color='#f05e68')\n)\ntrace5 = go.Bar(\n    x=df_hard.index,\n    y=df_hard['$10,000-$99,999'].values,\n    name='$10,000-99,999(USD)',\n    marker=dict(\n    color='#00a3e0')\n)\ntrace6 = go.Bar(\n    x=df_hard.index,\n    y=df_hard['> $100,000 ($USD)'].values,\n    name='> 1,00,000(USD)',\n    marker=dict(\n    color='#4d0e20')\n)\n\ndata = [trace1, trace2,trace3,trace4,trace5,trace6]\nlayout = go.Layout(\n    barmode='group',height=700,width=1000,title='Specialized Hardware Usage based on ML expenditure',yaxis_title='Total Usage',xaxis_title='Hardware'\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.offline.iplot(fig, filename='grouped-bar')","d0329c2f":"data_exp=pd.concat([pd.crosstab(mcq['Q21_Part_1'][1:],mcq['Q23']),pd.crosstab(mcq['Q21_Part_2'][1:],mcq['Q23']),\n                   pd.crosstab(mcq['Q21_Part_3'][1:],mcq['Q23']),pd.crosstab(mcq['Q21_Part_4'][1:],mcq['Q23']),\n                    pd.crosstab(mcq['Q21_Part_5'][1:],mcq['Q23'])])\ndata_exp=data_exp[['< 1 years','1-2 years','2-3 years','3-4 years','4-5 years','5-10 years','10-15 years']]\ndata_exp=data_exp.fillna(0)\n\ntrace1 = go.Bar(\n    x=data_exp.index,\n    y=data_exp['< 1 years'].values,\n    name='< 1 years',\n    marker=dict(\n    color=\"#66545e\")\n)\ntrace2 = go.Bar(\n    x=data_exp.index,\n    y=data_exp['1-2 years'].values,\n    name='1-2 years',\n    marker=dict(\n    color='#15b2b3')\n)\ntrace3 = go.Bar(\n    x=data_exp.index,\n    y=data_exp['2-3 years'].values,\n    name='2-3 years',\n    marker=dict(\n    color='#ff920c')\n)\ntrace4 = go.Bar(\n    x=data_exp.index,\n    y=data_exp['3-4 years'].values,\n    name='3-4 years',\n    marker=dict(\n    color='#f05e68')\n)\ntrace5 = go.Bar(\n    x=data_exp.index,\n    y=data_exp['4-5 years'].values,\n    name='4-5 years',\n    marker=dict(\n    color='#00a3e0')\n)\ntrace6 = go.Bar(\n    x=data_exp.index,\n    y=data_exp['5-10 years'].values,\n    name='5-10 years',\n    marker=dict(\n    color='#4d0e20')\n)\ntrace7 = go.Bar(\n    x=data_exp.index,\n    y=data_exp['10-15 years'].values,\n    name='10-15 years',\n    marker=dict(\n    color='#b35a00')\n)\n\n\ndata = [trace1, trace2,trace3,trace4,trace5,trace6,trace7]\nlayout = go.Layout(\n    barmode='group',height=700,width=1000,title='ML Experience in terms of Specialized Hardware Usage',yaxis_title='Total Usage',xaxis_title='Hardware'\n)\n\nfig = go.Figure(data=data, layout=layout)\npy.offline.iplot(fig, filename='grouped-bar')","4b88f198":"Next, we will move towards the most interesting part of this kernel i.e., exploring the **Machine Learning penetration in corporate world **which will answer some of the most sought questions of aspiring data scientists and Machine Learning engineers i.e, \n- **How the current industry actually working in Machine Learning domain ?**\n- **How much is the experience of employees in ML domain ?**\n- **In what Machine Learning activities they are engaged in most ?**\n- **How much they are spending in ML projects ?**\n- **What tools and platforms they are using in current scenario and much more ?**\n\nBefore exploring this section I would like to share some of the important personalities who have significant contribution in Machine Learning domain and we would learn a lot if we follow them.\n![](https:\/\/i.ibb.co\/dW12VjH\/personality.jpg)\n","db2315d6":"Next, we will see **which type of IDE's are most preferred in industries based on Machine Learning experience**?.","0420cc58":"## Top 3 sources of Learning Datascience\n![](https:\/\/i.ibb.co\/cNhPhFC\/learn.jpg)","30830b99":"Next we will move towards the **preferred data analysis tools of the respondents.**","6a829cb6":"## Google Cloud ML Engine, Azure ML Studio and AWS Sagemaker are top Machine Learning Tools","181bc99d":"**Observation:**\n\nMost of the **Males** in this survey are in the age group of **25-29 years** whereas **Females** also have highest presence in this age group only, which suggests that most of the respondents are young professionals or students.","e396a2f1":"**Observation:**\n- As we can see most of the professionals are either **exploring machine learning methods at workplace** or **recently started using ML methods** and this is due to most of the companies have now realized the power of machine learning and thats why they are exploring or just started using machine learning methods currently and the demand for applying machine learning in business use case is incraesing day by day.","c11ea515":"## Most of the DS professionals are not using any ML Tools","709e101a":"**Observation:**\n\n- In the last ML & DS Survey 2018, USA was the leader in terms of number of respondents but in this years survey **India** Leads, while **USA** has second most participants.","66d1ce8a":"## 1. Demographic Distribution - Age, Gender, Education etc.\n\nSo, I am start exploring the data from demographic variables such as age, gender, highest qualification, job profile and income distribution.","b305b405":"Next, we will move towards the countries distribution. Lets see **Top 50 countries of the respondents** partcipated in this survey.","8cedf981":"**Observation:**\n- As we can see from above,Linear regression is the choice of freshers or students community while advanced algorithms such as Gradient boosting, Bayesian, CNN, DNN, RNNs etc. are mostly used by experienced professionals. \n","5ab03566":"![](https:\/\/i.ibb.co\/LxDGzBt\/rich.gif)","f88a2c6d":"Now, we will explore **Coding Experience distribution** among kagglers.","ac59a769":"**Observation:**\n- Mostly larger number of people,**> 20** responsible for datascience workload and this is due to any data science project involves a team of **Datascientists**, **Statisticians**, **Machine learning engineers**, **Devops** and **domain people**  for successfully deploying and making use of datascience in true sense, thats why 20+ professionals are responsible for Datascience workload.\n- Second highest number of people responsible for DS workload are 1-2 and this is due to most of the companies have just now started thinking to use machine learning methods in their business use case and thats why in initial phase only few people around 1-2 are involved in datascience workload.\n","af305f43":"## **Observation:**\n- Coursera tops in terms of platform for learning datascience and this is due to most of the courses are cheap and they have monthly installments that provides ease of usage and higher completion rate of courses.\n- Coursera also has tie-ups with top universities and enrollment in classes is free of charge, but typically cost 29 - 49 USD to earn a certificate; \n- Kaggle is the second most popular platform for datascientists or for any individual aspiring to become a datascientist as they not only learn from the top daat science kernels but they can also hands on and write thier kernel for practice too and above all it is totally free of charge.\n- Udemy is also gaining moemntum as it also provides huge number of course in ML and DS domains and they are extremely cheap (4 to 10 USD)","444f6336":"## Understanding the Data\nUnderstanding of data and its business use case is very crucial step before deep diving into any data analysis project. So, lets start with looking into the first few entries of the dataset to know the basic structure of the data and have a real feel of the dataset.","aee92372":"## Highest Salary goes to Datascientists\n\n![](https:\/\/i.ibb.co\/SXPTr3C\/salary.gif)","1dea6ab8":"**Observation:**\n- Some of the basic machine learning algorithms such as **Linear regression, Logistic Regression, decision tree** and **random forest** are most sought algorithms while **XGboost \/Light GBM and CNNs** are gaining momentum slowly.\n\n## Simple Machine Learning algorithms are winner\n\nNext we will see **which machine learning algorithms are mostly liked by experienced ML professionals ?**","f3707ea4":"## 2. Exploring development skills of Kagglers\nIn this section we will first see which are the top used programming languages in regular basis.","6d11c821":"**Observations:**\n- As we can see **MySQL**, **PostgreSQL** and **MS Sql Server** are the most favorite databases among the respondents and one solid reason for this may be ooen source and ease of use. As we know MYSQL is the open source and user friendly database of all time postgresql is also gaining momentum as it is also the open source database and it has the capacity of handling big data too. ","569af760":"**Observation:**\n- As we can see from above plot, Most of the respondents belongs to **lower income** and **lower middle income group** the reason for this may be the dataset has a **higher percentage of students community** which are generally not working so they will automatically comes in Lower income group which would shoot up the percentage of lower income group respondents.\n\nNext, we will answer most interesting question of all time i.e., **which job profiles are highest paying** ? means we will analyze **Income wise distribution of Job profiles**.","4133c250":"## Most Interesting Question for students community:\n### What is the salary structure of Machine Learning engineers or Datascientists\nwe will answer this question by mapping machine learning experience with income distribution","c248c9ff":"## Most of the professionals having experience between 1-2 yrs are indulged in Analysing data\n","ad5d1f5f":"**Observation:**\n- As we can see from the above plots the top 3 Educational degrees  are **MS**, **Grduation** and **Doctors** while very few are **UnderGrad students**.\n- In the second plot, it is evident that most of the job profile belongs to **Datascientist**,**Students** and **Software Engineers**.","4000ffef":"**Observation:**\n- As we can see most of the respondents have Machine learning experience of less than 1 year this is due to larger presence of students community in the survey.\n- Another important reason for larger percentage of respondents having less than a year experience is that machine learning started grabing attention from 2 to 3 years due to **cheaper availability of cloud solutions** for running machine learning algorithms and **huge availability of data** which are the two favorable points for machine learning to floursih and that is the reason most of the undergrad students and professionals also showing their interest in the machine learning domain which increased the overall percentage of new entrants in this field.  ","791bb323":"Next, we will move towards section of exploring **Top learning sources and platforms** for data science.","e22cac26":"Next we will see **which type of activities takes important part of time at workplace ? **","4882d79f":"## Most of the time is spent in Analysing and understanding data \n![](https:\/\/i.ibb.co\/wNhWQLk\/analysis2.gif)","477e5787":"## Top 3 Platforms for Learning Datascience\n![](https:\/\/i.ibb.co\/HDP0pFR\/t1.jpg)","aa14d744":"**Observation:**\n- **Datascientist community** has varied range of income distribution and it has higher percentage of Lower Middle Income group, Lower Income group, Upper Middle Income group , Higher Income group and very high income group. \n- **Software engineers** and **Data analyst** have larger number of lower income and lower middle income group\n- **Research scientists** and **Business Analyst** have same number of lower income and lower middle income group.","3bdfb5ab":"Next we will see Job profile wise distribution of education to have clear understanding of **which job profiles demands Masters, or higher education while which one relaxed to Graduation level**.","01e6f73e":"Next, we will move towards the **income distribution** but the major problem with this feature is that it has **different income range** which makes it difficult to visualize with other variables so I will segment different income ranges into 5 broader income groups i.e., \n- **Lower Income**,\n- **Lower Middle**, \n- **Upper Middle**, \n- **Higher** and,\n- **Very High Income group **.","5b7ccf9d":"**Observation:**\n- In case of CPU usage,mostly **students** and professionals **having less than 1 years experience **have higher percentage.\n- Where in GPU and TPU usage, professioanls having experience of **1-2 years** have higher percentage as for running algorithms on GPUs require machine learning experience.","cf45b988":"## Google Big Query, Databricks and AWS redshift are top three Big data analytics tools.","e377848d":"## Companies which have invested > 100,000 USD have mostly well established ML methods in place\nThere is a clear pattern visible, as the companies increases their expenditure in Machine Learning, the percentage of well established ML methods increased proportionately.\n\nNext we will see **which IDEs are most popular in terms of percentage of usage among respondents** ?","10a99602":"**Observation:**\n- As we can see most of the companies which are spending **>100,000 USD** are still using CPU and other free resources for running and developing ML applications.\n- While most of the companies which are using GPUs are using **free options($0 USD)**  mainly such as **Google colab** which provides free GPU access for running complex and heavy machine learning algorithms.\n\nNext we will see **what is the distribution of Machine Learning experience in terms of usage of Hardware?**","59a66e22":"Just look at this video which beautifully visualizes the popularity of programming languages from **1965 - 2019**","d6e281f9":"## Most of the people in the survey have less experience in coding\n![](https:\/\/i.ibb.co\/f4KfpNJ\/coding.gif)","60e00db1":"## 3. Favorite Learning sources & Platforms","530be281":"In this kernel I have tried to analyze the **2019 kaggle survey dat**a. This survey received **19,717** usable respondents from **171** countries and territories.This survey is annually conducted by **Kaggle** by asking different questions to kagglers in online form.\n\n\nBefore analyzing the data, I like to highlight some of the  basic skill sets of **Machine Learning Engineers** and **Data Scientists** in corporate world.\n\n### Responsibilities of a Machine Learning Engineer\n* Develop machine learning models\n* Collaborate with data engineers to develop data and model pipelines\n* Apply machine learning and data science techniques and design distributed systems\n* Write production-level code\n* Bring code to production\n* Engage in code reviews\n* Improve existing machine learning models\n* Be in charge of the entire lifecycle (research, design, experimentation, development. deployment, monitoring, and maintenance)\n* Produce project outcomes and isolate issues\n* Implement machine learning algorithms and libraries\n* Communicate complex processes to business leaders\n* Analyze large and complex data sets to derive valuable insights\n* Research and implement best practices to enhance existing machine learning infrastructure\n\n### Responsibilities of a Data Scientists\n* Research and develop statistical models for analysis\n* Better understand company needs and devise possible solutions by collaborating with product management and engineering departments\n* Communicate results and statistical concepts to key business leaders\n* Use appropriate databases and project designs to optimize joint development efforts\n* Develop custom data models and algorithms\n* Build processes and tools to help monitor and analyze performance and data accuracy\n* Use predictive modeling to enhance and optimize customer experiences, revenue generation, ad targeting, and more\n* Develop company A\/B testing framework and test model quality\n\n[reference](https:\/\/www.springboard.com\/blog\/machine-learning-engineer-vs-data-scientist\/)\n\n\nIn short, this picture is enough to have a basic difference between ML and DS in mind.\n![](https:\/\/i.ibb.co\/cYBfb20\/Capture.png)\ncredit:[edureka](https:\/\/www.edureka.co\/)","542b4f0e":"In this section, first we will explore what is the actual Machine Learning Working status in Industries **whether they are exploring innovative Machine Learning methods ?** **Whether they have just started using ML practices ?** **OR they have well established ML methods in place?**\n\nSo, now lets see **Machine Learning working status in Industries**","58849f5f":"## CPUs are still leading the race while GPU usage is on the rise !!!\nThere is a lot of research and active work happening to think of ways to accelerate computing.** Google** is expected to come out with **Tensorflow Processing Units (TPUs)** later this year, which promises an acceleration over and above current GPUs.\nSimilarly **Intel** is working on creating faster FPGAs, which may provide higher flexibility in coming days. In addition, the offerings from Cloud service providers (e.g. AWS) is also increasing. We will see each of them emerge in coming months.\n\nDue to increase in the development of deep learning applications GPU usage is on the rise which will be later take over by TPUs.\n\nNext we will **How much companies are actually spending in $ on hardware resources ? **\n","b1796d22":"Firstly, let's see all the unique questions that were asked during the **ML & DS survey 2019** to have a clear picture in our mind that in which direction we have to proceed our analysis.","09a5b445":"## MS is the most popular Degree !!!!!!!!!\n\n![](https:\/\/i.ibb.co\/TMfQ6xk\/ms.gif)","70245156":"## Indians Leads the Survey !!!!!!!!\n![](https:\/\/i.ibb.co\/jTv6pWV\/INDIA.gif)\n","5cafeda9":"**Observation:**\n\n- As we can see **Male** dominates the survey comprising **82%** of the dataset while Females are of only **16%** presence and around **2%** are those who did'nt disclose their gender by selecting Prefer not to say. This clearly depicts the **Gender Inequality !!!**\n- The bar graph suggests that most of the respondents are in their younger age group of **25-29 years**. \n\nAs we have seen Age and Gender individually now we will analyze **Age wise Gender distribution** ","ab58c39f":"Next, we will move to the section of exploring **Development skills of Datascientists** it includes their favorite **programming languages**, **Coding experience in years**, **favorite databases** etc.  ","ff96c795":"**Observation:**\n- We can see a pattern from above plot,as the Machine Learning experience increases from less than 1 year to more years, percentage of  high income group increases proportionately and when the experience exceeds 5 years, percentage of high income group exceeds from other counterparts which is true because machine learning is very progressive field and Senior machine learning engineers with over a decade of experience are the industry\u2019s unicorns. As a result, they also command the best remuneration packages in the field. If you\u2019re a senior machine learning engineer, you can expect an average salary of $132,500.","587799b0":"**Observation:**\n\n- In almost every Job profile most of the people have **MS Degree** except in case of **Research scientist** where **Doctorate degree** leads which is but obvious, while most of the **Students** are **Graduate**. \n- In case of **software engineers**, **Graduate** is the second most sought degree.","df60de30":"**Observations:**\n- As we can see Kaggle, online blogs such as Medium ,towardsdatascience and KDNuggets and Youtube are most preferred sources for learning Datascience.\n- One valid reason for this is Kaggle is the most comprehensive and shared platform of coding kernels where data scientists from around the world contribute in solving world's most complex problems in current scenario with their data science skills and it is most helpful source for learning data science from the data scientists around the world and it is abolutely free.\n- Whereas Online blogs are also very helpful in learning datascience skills as it shared code samples with details instructions and theory required to understand the concept.","476b9e27":"## Mostly, a team of 20+ professionals are responsible for DS workload.\n\n![](https:\/\/i.ibb.co\/NsyTMqW\/team.png)","4b5943cd":"## Python is the most used and recommended programming language\n![](https:\/\/i.ibb.co\/z5PHGsc\/python.gif)\n","bf01acdb":"**Observation:**\n- Mostly respondents having experience upto 2 years mostly use Jupyter Notebook and Visual Studio.\n- The main reason for this pattern is both Jupyter and Visual studio are free to use and user friendly in comparison to other counterparts.","a00d544b":"**Observation:**\n- Python is most used programming language and it is also top choice for aspiring data scientists.\n- At a time, C, C++ and JAVA were the most popular programming languages which are loosing their sheen now due to advancement in programming and requirements of regular updates in libraries. Whereas MATLAB is the most sophisticated language but it is licensed and it is not that much user friendly as Python and R.","634378b0":"## Scikit Learn, Tensorflow and Keras are top three ML Framework\n![](https:\/\/i.ibb.co\/t46SPFD\/ker1.jpg)","648f1049":"## When developer exceed more than 10 yrs of coding experience they fall into High Income Group","9ef53f1c":"## RStudio and Jupyterlab is the preferred Data Analysis tool\n![](https:\/\/i.ibb.co\/0VbmnXK\/r.png)","d613a293":"## 5. Hardware Requirements\nWith the increasing demand in deep learning, the demand for better as well as sophisticated hardware has also increased. Several Tier I organisations like Intel, Nvidia and Alibaba, among others, are striving hard to bridge the gap between the software and hardware. The only way to build a sophisticated deep learning model is to use better hardware.\nBefore seeing the actual distribution of CPU, TPU and GPU lets see what is the basic difference among them.\n\nThe researchers made a cross-platform comparison in order to choose the most suitable platform based on models of interest. This can also be said as the key takeaways which shows that no single platform is the best for all scenarios. They are mentioned below\n\n- **TPU: Tensor Processing Unit** is highly-optimised for large batches and CNNs and has the highest training throughput.\n- **GPU: Graphics Processing Unit** shows better flexibility and programmability for irregular computations, such as small batches and nonMatMul computations.\n- **CPU: Central Processing Unit** achieves the highest FLOPS utilisation for RNNs and supports the largest model because of large memory capacity.\n\n[source](https:\/\/analyticsindiamag.com\/tpu-vs-gpu-vs-cpu-which-hardware-should-you-choose-for-deep-learning\/)\n\n![](https:\/\/i.ibb.co\/8rQm425\/GPU.png)\n\nlets which hardware usage are on rise !!!","789a3931":"**Observation:**\n- **Matplotlib** emulates **Matlab** like graphs and visualizations. Matlab is not free, is difficult to scale and as a programming language is tedious. So, matplotlib in Python is used as it is a robust, free and easy library for data visualization and thats why it is a most preferred library for visualization.\n- **Seaborn** is the second most preferred library. The seaborn package was developed based on the Matplotlib library. It is used to create more attractive and informative statistical graphics. While seaborn is a different package, it can also be used to develop the attractiveness of matplotlib graphics.\n\n## Matplotlib and Seaborn wins the Race","0807278c":"Next, we will see top 3 country wise distribution of programming language to find out which language is winner in global level.","4c755f87":"1. This survey is heavily dominated by **Young Males** in the age group of **25 to 29 years** which is the reflection of **Gender Inequality**.\n2.  Majority of the respondents have job profile of **Data scientists**, **Students** and **Software Engineers**.\n3. **Master of Science** is the most sought education in current scenario in almost every job profile. The reason for this may be easily availability of MS degree through **Online MOOCs** provided on the platforms of some of the most popular open learning websites such as coursera, edex, upgrad in association with top universities of the world.\n4. Most of the respondents have Lower Income group this is due to higher percentage of student respondents which are not working.\n5. **Datascientist** job profile have larger number of **higher income group** professionals which highlights the notion that \n\"Data scientists jobs are the highest paying jobs of 21st centurty.\"\n6. The survey is dominated by Indian and US respondents.\n7. Programming languages **Python**, **SQL** and **R** is on the rise while **Java**, **C** and **C++** are loosing its sheen now.Where as most of the respondents have **lesser coding experience of upto 2 years**.\n8. **MySQL**, **PostgreSQL** and **SQL Server** are the most preferred databases among majority of the respondents.\n9. **Kaggle**, **Online blogs** and **Youtube videos** are the most preferred media sources where as **Coursera**,**Kaggle** and **Udemy** are the most preferred platforms for learning data science.\n10. Majority of the professionals are **exploring and recently started using Machine Learning methods at workplace** in hope of putting machine learning model into production one day as putting machine learning model into production is the most challenging task and it involves expertise in different domains such as **software engineering**, **Devops**, **Machine Learning** etc.\n11. As **Machine learning experience exceeds 5 years** probability of falling into **high or very high income group** will be increased too.\n12. Most of the time of the professionals is generally spent on **Analysing and understanding data** as it is the most crucial phase of any data science project, it requires proper data mappings and understanding the data fields which also requires close coordination with domain people in the industry thats why it takes most of the time.\n13. Jupyter Notebook & Visual Studio are most preferred IDEs where as RStudio and Jupyterlab are the most popular Data Analysis tools  among respondents.\n14. The Basic machine learning algorithms such as **Linear regression, Logistic Regression, decision tree** and **random forest** are most sought algorithms while **XGboost \/Light GBM** and **CNNs** are gaining momentum slowly.\n15. **AutoKeras, Google AutoML** and **TPOT** are the most popular Automated ML tools as they are fully open source and easy to use. \n16. **AWS EC2, Google Cloud Engine** and **AWS Lambda** are most preferred cloud computing platforms.\n17. **Google Big Query, Databricks and AWS redshift** are top three Big data analytics tools.\n18. In Hardware usage **CPUs** are still leading the race while GPU usage is gaining the momentum.\n\n# Thanks for watching the Kernel.\nI hope you like this kernel. If yes, please upvote for motivation.\n\n![](https:\/\/i.ibb.co\/L9PXc1w\/All-Kind-Africanparadiseflycatcher-small.gif)\n\n\n ","2b10654a":"## AWS EC2, Google Cloud Engine and AWS Lambda are most preferred cloud computing platforms","c7923e26":"## High Income Group leads when ML experience > 5yrs ","122b6aed":"### Programming Language distribution in Top 3 countries (India, USA and China)","cd4c5834":"So, lets start exploring the data.\n![](https:\/\/i.ibb.co\/Dp01nhL\/giphy.gif)\n","a2596e78":"## MySQL,PostgreSQL & Micrososft SQL server are most favorite Databases!!\n![](https:\/\/i.ibb.co\/LQH9FTd\/db.jpg)","9994d0ab":"## Young Male dominates the survey !!!!!!!!!!!\n\n![](https:\/\/i.ibb.co\/swfz8Vy\/male.gif)","ce0b19ff":"## Most of the people having Machine Learning experience within 2 yrs !!! ","9165a39c":"So, let's start our analysis from **Demographic** questions about **Age, Gender, Education, Income distribution** etc.","141d1ae2":"Next comes the top Big Data Analytics Tool","826b95f1":"Next, we will see **Education** as well as **Job Profile** distribution of the respondents ","7aab4962":"Now, we will see how many people are actually responsible for **Handling Datascience workloads  or projects** ","704a8560":"### Majority of the respondents in this survey are Datascientists or Students or Software Engineers.","b99781a7":"# Key Insights of the Kernel","5020aad1":"**Observation:**\n- One interesting observation is visible from the above plot, as coding experience in years increases high income group percentage also increases proportionately and as experience exceeds 10 years high income group professionals leads in comparison to other counterparts.","b4bf4133":"**Observation:**\n\nThe dataset comprises of **19718** responses received as a record and columns of the dataset are the question numbers. As we can see it is a Q-A dataset in which the full question text is available in the first row of the dataset and their answers are available in the  corresponding rows. \n\n**Note:** Most of the columns have Null values so we have to handle them carefully during analysis and some of the questions have different parts which are segregated in separate columns and that's why the number of columns in this dataset is **246**, so we have to combine those columns to extract the answer of that multipart question. ","e4eba9e8":"**Observation:**\n- New observation is popped up from the industrial prospect i.e., more number of professionals started using **Jupyterlab** and **Rstudio** for their data analysis part in comparison to **MS Excel** and **Google sheets** which are most popular analysis tools for over a decade thats a big shift observed in this survey. ","7d0bab3d":"## Auto Keras, Google Auto ML and TPot are the top choices for Auto ML","32098e22":"Next we will find out **Machine Learning engineers usually engaged in which type of activities base don their experience ?**\n","b297aba9":"Next, we will see what are the most favourite **Databases** in use.","8a220e72":"**Observation:**\n- **Jupyter Notebook and Visual Studio** are the most preferred IDEs as they are open source and user friendly whereas MATLAB is least preferred this might be due to licensing issue and not easy to use UI. ","3bd8bb35":"Let's see the shape of the dataset.","b71d1249":"## 4. Machine Learning Penetration in Corporate world","9fb8e04a":"Next important question we will try to answer that **mostly professionals acquires how many years of Machine Learning experience ** ?","92ea8446":"# Welcome to my Kernel !!!\n\nWhen I started my Machine Learning journey I was really excited about the field but at the same time I had lots of doubts poped up in my mind some of them are as follows:\n\n- **How corporates are actually working in Machine Learning domain ?**\n- **Which type of projects they are working on ?**\n- **Which tools they are using for developing and deploying Machine Learning models?**\n- **To become a Machine Learning Engineer what skills should be developed ?**\n- **Which courses should we complete and on which platform ?**\n- **What is the Salary Stucture of Data Scientists\/ Machine Learning engineers ?** \n\nFor honing my Machine Learning skills,I personally use **online blogs** and **youtube videos** . And if you ask me which job title you choose to go for I would say **Machine Learning Engineer (MLE)** rather **Datascientists** because MLE job profile include both the flavors of **Data Analytics and Data Science** and most important part Machine Learning model development cycle which involves different activities from **data collection** to **model deployment** and **maintenance** which is the most interesting part for me.\n\nLets see the below video to know **Why Machine Learning is The Future ?** explained by **Sundar Pichai**\n","713ced95":"## Most of the professionals are exploring Machine Learning methods in workplace !!!\n![](https:\/\/i.ibb.co\/qJ7NxWS\/explore.gif)"}}