{"cell_type":{"2dc370f0":"code","2e37f3db":"code","4fb29bfa":"code","6e68b344":"code","61dc60ef":"code","c4afa25e":"code","d6cffa10":"code","deb9dcec":"code","09f4af3f":"code","4665a203":"code","4b4ab693":"markdown","88803d1b":"markdown","07491125":"markdown","dd87494c":"markdown","7ce3f7f5":"markdown","d8c1dcca":"markdown","175cb064":"markdown"},"source":{"2dc370f0":"!pip install torchvision","2e37f3db":"import torch","4fb29bfa":"t1 = torch.tensor(4.)\nt2 = torch.tensor([3., 2, 1, 8])\nt3 = torch.tensor([[[1, 2, 3, 4],[2,3,4,5]],\n                 [[1, 2, 3, 4],[2,3,4,5]],\n                 [[1, 2, 3, 4],[2,3,4,5]]])\nprint(t1.dtype)\nprint(t2.dtype)\nprint(t3.dtype)\n\nprint(t1.shape)\nprint(t2.shape)\nprint(t3.shape)","6e68b344":"### Creating tensors\nx = torch.tensor(3.)\nw = torch.tensor(4., requires_grad=True)\nb = torch.tensor(5., requires_grad=True)\n## Operation\ny = w * x + b\nprint(y)\n## To compute the derivatives in case were we put required_grad = True can be called using .backward()\ny.backward()\n## Finding derivatives\nprint('dy\/dx', x.grad)   ### No gradient will be there since we didn't mentioned requries_grad = True there\nprint('dy\/dw', w.grad)\nprint('dy\/db', b.grad)\n","61dc60ef":"import numpy as np\n\n## Creating a numpy array\nx = np.array([[1, 2], [2, 3]])\nprint(x)\n## Converting numpy array to torch tensor\ny = torch.from_numpy(x)\nprint(y)\nprint(x.dtype, y.dtype)\n## Converting torch to numpy\nz = y.numpy()\nprint(z)","c4afa25e":"## Intializing inputs and targets\ninputs = np.array([[73, 67, 43],\n                  [91, 88, 64],\n                  [87, 134,  58],\n                  [102, 43, 37],\n                  [69, 96, 70]], dtype='float32')\ntargets = np.array([[56, 70],\n                   [81, 101],\n                   [119, 133],\n                   [22, 37],\n                   [103, 119]], dtype='float32')\n## Getting torch tensors from numpy\ninputs = torch.from_numpy(inputs)\ntargets = torch.from_numpy(targets)\n############################################ Single Epoch ###########################################\n## Intialising random weights and bias\nw = torch.randn(2, 3, requires_grad=True)\nb = torch.randn(2,  requires_grad=True)\n## Equating conditions\ndef model(x):\n    y =  x @ w.t() + b\n    return y\n## MSE (Mean Square Error)\ndef mse(d1, d2):\n    diff = d1 - d2\n    avg_loss = torch.sum(diff*diff)\/diff.numel()\n    return avg_loss\n## Predicting\npredictions = model(inputs)\nloss = mse(predictions, targets)\n## For getting derivative\nloss.backward()\nwith torch.no_grad():\n    w -= w.grad * 1e-5\n    b -= b.grad * 1e-5\n    w.grad.zero_()\n    b.grad.zero_()\npredictions = model(inputs)\nloss = mse(predictions, targets)\n\n\n########################  100 Epochs #####################\nfor i in range(100):\n    preds = model(inputs)\n    loss = mse(preds, targets)\n    loss.backward()\n    with torch.no_grad():\n        w -= w.grad * 1e-5\n        b -= b.grad * 1e-5\n        w.grad.zero_()\n        b.grad.zero_()\n#     print(loss)\npreds = model(inputs)\nloss = mse(preds, targets)\nprint(loss)\nprint(preds)\nprint(targets)\n    ","d6cffa10":"import torch.nn as nn\n\n# Input (temp, rainfall, humidity)\ninputs = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], \n                   [102, 43, 37], [69, 96, 70], [73, 67, 43], \n                   [91, 88, 64], [87, 134, 58], [102, 43, 37], \n                   [69, 96, 70], [73, 67, 43], [91, 88, 64], \n                   [87, 134, 58], [102, 43, 37], [69, 96, 70]], \n                  dtype='float32')\n\n# Targets (apples, oranges)\ntargets = np.array([[56, 70], [81, 101], [119, 133], \n                    [22, 37], [103, 119], [56, 70], \n                    [81, 101], [119, 133], [22, 37], \n                    [103, 119], [56, 70], [81, 101], \n                    [119, 133], [22, 37], [103, 119]], \n                   dtype='float32')\n\n## Convert numpy to torch tensors\ninputs = torch.from_numpy(inputs)\ntargets = torch.from_numpy(targets)","deb9dcec":"from torch.utils.data import TensorDataset, DataLoader\n\n######################################## Single Epoch ###############################################\n\n## Using TensorDataset allows rows from ip and targets to avail as tuples\ntrain_dl = TensorDataset(inputs, targets)\n## Dataloader load data from tensordataset according to a particular batch_size\ndata = DataLoader(train_dl, batch_size=5, shuffle=True)\n\n## Linear Regression model in torch.nn\nmodel = nn.Linear(3, 2)\n# print(list(model.parameters()))\npreds = model(inputs)\n# print(preds)\n\n## Loss Required from Functional library of torch\nimport torch.nn.functional as F\nloss_func = F.mse_loss\n\nprint(loss_func(model(inputs), targets))\n \n","09f4af3f":"# importing optimizer\nopt = torch.optim.SGD(model.parameters(), lr=1e-5)\n\n## Model \ndef fit(num_epochs, model, data, opt, loss_fn):\n    ## 100 epochs\n    for epoch in range(num_epochs):\n        ## Train with dataloader data\n        for xb, yb in data:\n            \n            preds = model(xb)\n            loss = loss_fn(preds, yb)\n            # For taking derivative\n            loss.backward()\n            \n            ## Optimizing parameters using gradients\n            opt.step()\n            \n            ## Setting gradient to zero\n            opt.zero_grad()\n            \n        # Print the progress\n        if (epoch+1) % 10 == 0:\n            print('Epoch [{}\/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n    \n## Fitting the model\nfit(100, model, data, opt, loss_func)\n\npreds = model(inputs)\nloss = loss_func(preds, targets)\nprint(loss)","4665a203":"print(preds)\nprint(targets)","4b4ab693":"# Part-3 - Linear Regression with Builtin Functions","88803d1b":"### Create a torch tensor with an element, vector or a matrix","07491125":"#### Also we can check the data type and shape of the torch tensor.","dd87494c":"# Part-2  -  Linear Regression without builtin functions","7ce3f7f5":"### Operating with NUMPY","d8c1dcca":"# Part - 1 (Basics)","175cb064":"### Let's understand about tensor operations and their gradiesnts."}}