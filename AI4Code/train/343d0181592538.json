{"cell_type":{"27cc566b":"code","367cbed8":"code","17052460":"code","65a667d6":"code","567aace7":"code","46d97279":"code","5322149e":"code","441c4ca9":"code","7082a5d0":"code","a3f7a72b":"code","47be96e6":"code","9e5c90e8":"code","1572d149":"code","dea63e32":"code","85619aab":"code","91009b14":"code","8decb113":"code","c7aa8793":"code","b6223cc4":"code","a0abf61b":"code","61504e1e":"markdown","a22c73e6":"markdown","d2ec6f34":"markdown","04e2d541":"markdown","24a56c35":"markdown","9cc68d28":"markdown","97937677":"markdown","4329fbc7":"markdown","8e8baac9":"markdown","a380fe5c":"markdown","e9bd76f4":"markdown","c9023b4f":"markdown","9d802815":"markdown","df06197d":"markdown"},"source":{"27cc566b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","367cbed8":"#data analysis libraries\nimport numpy as np\nimport pandas as pd\n\n#visualization libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler,RobustScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import model_selection\nfrom sklearn.svm import LinearSVC\nfrom sklearn import metrics\nfrom sklearn.svm import SVC","17052460":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain.head()","65a667d6":"test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest.head()","567aace7":"train.Sex=train.Sex.astype('category').cat.codes\ntest.Sex=test.Sex.astype('category').cat.codes","46d97279":"train.info()","5322149e":"test.info()","441c4ca9":"#checking for total null values\ntrain.isnull().sum() ","7082a5d0":"#checking for total null values\ntest.isnull().sum() ","a3f7a72b":"# drop the Cabin column becuse more the 77% data missing \n# drop Name and Ticket column as well \ntrain.drop(labels = [\"Cabin\",\"Name\",\"Ticket\"], axis=1, inplace=True) \ntest.drop(labels = [\"Cabin\",\"Name\",\"Ticket\"], axis=1, inplace=True) \n\n# fill Age null value with median\ntrain['Age'].fillna(train['Age'].mean(), inplace=True) \ntest['Age'].fillna(train['Age'].mean(), inplace=True) \ntest['Fare'].fillna(test['Fare'].mean(),inplace=True)\n\n# drop any missing value\ntrain=train.dropna()  \n\ntrain.head()","47be96e6":"test.head()","9e5c90e8":"# Visualizations of Feature vs. Target\nfig=plt.figure()\nax1=plt.subplot(321)\nsns.countplot(x = 'Survived', hue = 'Sex', data = train, ax=ax1)\n\nax2=plt.subplot(322)\nsns.countplot(x = 'Survived', hue = 'Pclass', data = train, ax=ax2)\n\nax3=plt.subplot(323)\nsns.countplot(x = 'Survived', hue = 'SibSp', data = train, ax=ax3)\nax3.legend(loc=1, title='Sibling\/Spouse Count', fontsize='x-small')\n\nax4=plt.subplot(324, sharey=ax3)\nsns.countplot(x = 'Survived', hue = 'Parch', data = train, ax=ax4)\nax4.legend(loc=1, title='Parent\/Children Count', fontsize='x-small')\n\nax5=plt.subplot(325)\nsns.countplot(x = 'Survived', hue = 'Embarked', data = train, ax=ax5)\nax5.legend(loc=1, title='Embarked')\n\nfig.set_size_inches(8,12)\nfig.show();","1572d149":"#sort the ages into logical categories\nbins = [0, 5, 12, 18, 24, 35, 60, 80]\nlabels = ['Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\ntrain['AgeGroup'] = pd.cut(train[\"Age\"], bins, labels = labels)\n\n#draw a bar plot of Age vs. survival\nfig1=plt.figure(figsize=(10,5))\nsns.barplot(x=\"AgeGroup\", y=\"Survived\", data=train)\nplt.show();","dea63e32":"tab1=pd.crosstab(train.Pclass,train.Survived,margins=True)\nprint(tab1)\nprint(\"----------------------------------------------\")\ntab2=pd.crosstab(train.Sex,train.Survived,margins=True)\nprint(tab2)","85619aab":"# Encoding Catagorical Values\ntrain.drop(labels = [\"AgeGroup\"], axis=1, inplace=True) \ntrain_df=train.copy()\ntest_df=test.copy()\ntrain_df = pd.get_dummies(train_df, columns=['Embarked', 'Pclass'], drop_first=True)\ntest_df = pd.get_dummies(test_df, columns=['Embarked', 'Pclass'], drop_first=True)\n\ntrain_df.head()","91009b14":"# Correlations\nfig=plt.figure(figsize=(8,8))\nsns.heatmap(train_df.corr(), annot=True, cbar_kws={'label': 'Correlation coeff.'}, cmap=\"RdBu\")\nfig.show();","8decb113":"# Seperating the i\/p features from the target variable\n\nX = train_df.drop('Survived', axis=1)  \ny = train_df['Survived']\n\n# Prepare an array with all the algorithms\nmodels = []\nmodels.append(('LR', LogisticRegression(solver='liblinear'))) # uses default parameters\nmodels.append(('KNC', KNeighborsClassifier())) # uses default parameters\nmodels.append(('NB', GaussianNB())) # uses default parameters\nmodels.append(('SVC', SVC())) # uses default parameters\nmodels.append(('LSVC', LinearSVC())) # uses default parameters\nmodels.append(('RFC', RandomForestClassifier())) # default n_estimators = 100\nmodels.append(('DTC', DecisionTreeClassifier())) # uses default parameters\nmodels.append(('GBC',(GradientBoostingClassifier()))) # uses default parameters\n\nseed = 10\nresults = []  # to cross_validation results of each Model\nnames = []  # to hold the names of the Model\n\n# Scale the i\/p feature_set\nss = StandardScaler()\nX_scaled = ss.fit_transform(X)\n\n# Run all models and print scores\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=10, random_state=seed,  shuffle = True)\n    cv_results = model_selection.cross_val_score(model, X_scaled, y, cv=kfold, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    msg = round(cv_results.mean()* 100,2)\n    print(f'{name} : {msg}')","c7aa8793":"from sklearn.model_selection import cross_val_score\n\nX=train_df.drop('Survived',axis=1)\ny=train_df['Survived']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n\nRFG=RandomForestClassifier()\n\nRFG.fit(X_train,y_train)\nprediction=RFG.predict(X_test)\nscore=cross_val_score(RFG,X_train,y_train,cv=5)\nprint(\"Confusion_matrix:\",confusion_matrix(y_test,prediction))\nacc_log = round(RFG.score(X_train,y_train) * 100, 2)\nacc_log","b6223cc4":"test_df['Survived'] = RFG.predict(test_df)\n\ntest_df.head()","a0abf61b":"#Export as csv\n\nresult = test_df[[\"PassengerId\",\"Survived\"]]\nresult.to_csv('Titanic-results.csv', index=False, header=True)\nprint(\"Your submission was successfully saved!\")","61504e1e":"3) Data Analysis","a22c73e6":"1) Load all necessary Libraries","d2ec6f34":"Babies are more likely to survive than any other age group","04e2d541":"5) Choosing the Best Model","24a56c35":"4) Data Visualization","9cc68d28":"2) Read in and Explore the Train and Test Data","97937677":"encoding object data types to category and further into int data types","4329fbc7":"Some Observations:\n\n1. There are a total of 891 passengers in our training set.\n2. The Age feature is missing approximately 19.8% of its values. I'm guessing that the Age feature is pretty important to survival, so we should probably attempt to fill these gaps.\n3. The Cabin feature is missing approximately 77.1% of its values. Since so much of the feature is missing, it would be hard to fill in the missing values. We'll probably drop these values from our dataset.\n4. The Embarked feature is missing 0.22% of its values, which should be relatively harmless.","8e8baac9":"6) Predection on Test Data","a380fe5c":"congratulations and thank you for reading!","e9bd76f4":"From above table we can see clearly passengers with higher class and women are prioritized during the evacuation process. Also Women survival rate was high then men","c9023b4f":"* Numerical Features: Age, Fare (Continuous), SibSp, Parch, Survived, Pclass (Discrete)\n* Categorical Features: Sex, Embarked\n* Alphanumeric Features: Ticket, Cabin","9d802815":"Some Predictions:\n\n* Sex: Females are more likely to survive.\n* SibSp\/Parch: People traveling alone are more likely to survive.\n* Age: Young children are more likely to survive.\n* Pclass: People of higher socioeconomic class are more likely to survive.","df06197d":"For the above assumptions Random Forest Classifier 82.69% is quite close to the best performing model.  "}}