{"cell_type":{"b02848c5":"code","91b76e8d":"code","6a6558d2":"code","d154ab2e":"code","e93655e1":"code","a13360f1":"code","2af2f623":"code","55010471":"code","73df664b":"code","c953df23":"code","8ae5e274":"code","f9f3f0c6":"code","e35eb4ac":"code","04311ecd":"code","373d8946":"markdown","ce95206a":"markdown","09959ddc":"markdown","68265d75":"markdown","e0d152ac":"markdown","393a0259":"markdown","a06e27b0":"markdown","fe981a3a":"markdown","f7dd3163":"markdown","0de0214b":"markdown","dffc62f8":"markdown"},"source":{"b02848c5":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'            \nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense, BatchNormalization\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","91b76e8d":"train = pd.read_csv('..\/input\/income\/train.csv')\ntest = pd.read_csv('..\/input\/income\/test.csv')","6a6558d2":"train['education'] = train['educational-num']\ntest['education'] = test['educational-num']\ntrain = train.drop(columns=['educational-num'], axis=1)\ntest = test.drop(columns=['educational-num'], axis=1)","d154ab2e":"train_null, test_null = [], []\nprint('=== Train Data ===')\nfor col in train.columns:\n    x = set(train[col])\n    y = train[col].isnull().sum()\n    train_null.append(col) if y != 0 else None\n    print(f'[{col}] unique data: {len(x)}. with {y} nulls')\n\nprint()\nprint('=== Test Data ===')\nfor col in test.columns:\n    x = set(train[col])\n    y = train[col].isnull().sum()\n    test_null.append(col) if y != 0 else None\n    print(f'[{col}] unique data: {len(x)}. with {y} nulls')\n    \nprint()\nprint(f'Train data with null: {train_null}')\nprint(f'Test data with null: {test_null}')","e93655e1":"train=train.dropna()\ntest=test.dropna()\n\ndef dictionarize(data):\n    temp = set(data)\n    return { j:i+1 for i,j in enumerate(temp)}\n\ncolumns_to_classify = ['workclass', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'native-country']\n\nfor col in columns_to_classify:\n    temp_dict = dictionarize(train[col])\n    train[col] = [temp_dict[i] for i in train[col]]\n    test[col] = [temp_dict[i] for i in test[col]]\n\ntrain.rename(columns={'income_>50K':'target'}, inplace=True)\n\ntrain.head()","a13360f1":"x = train.drop(columns=['target'], axis=1)\ny = train['target']\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2)\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=.125)\n\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\nx_val = scaler.transform(x_val)","2af2f623":"print(x_train.shape, y_train.shape)\nprint(x_test.shape, y_train.shape)\nprint(x_val.shape, y_val.shape)","55010471":"base_model = Sequential()\nbase_model.add(Dense(13, input_dim=13, activation='relu'))\nbase_model.add(Dense(2, activation='relu'))\nbase_model.add(Dense(1, activation='sigmoid'))\nbase_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nbase_model.summary()\n\n\ncust_model = Sequential()\ncust_model.add(Dense(13, kernel_initializer='he_uniform', input_dim=13, activation='relu'))\ncust_model.add(BatchNormalization())\ncust_model.add(Dense(32, kernel_initializer='he_uniform', activation='relu'))\ncust_model.add(BatchNormalization())\ncust_model.add(Dense(16, kernel_initializer='he_uniform', activation='relu'))\ncust_model.add(Dense(1, kernel_initializer='glorot_uniform', activation='sigmoid'))\ncust_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\ncust_model.summary()\n\nbase_model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=10, epochs=20)\nprint('='*50)\ncust_model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=10, epochs=20)","73df664b":"train_loss = base_model.history.history['loss']\nvalid_loss = base_model.history.history['val_loss']\ntrain_acc = base_model.history.history['accuracy']\nvalid_acc = base_model.history.history['val_accuracy']\nepochs = len(train_loss)\n\nplt.plot(range(1,epochs+1), train_loss, label='train')\nplt.plot(range(1,epochs+1), valid_loss, label='validation')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.title('Train and Validation Loss Plot (Base Model)')\nplt.legend()\nplt.show()\n\nplt.plot(range(1,epochs+1), train_acc, label='train')\nplt.plot(range(1,epochs+1), valid_acc, label='validation')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.title('Train and Validation Accuracy Plot (Base Model)')\nplt.legend()\nplt.show()","c953df23":"train_loss = cust_model.history.history['loss']\nvalid_loss = cust_model.history.history['val_loss']\ntrain_acc = cust_model.history.history['accuracy']\nvalid_acc = cust_model.history.history['val_accuracy']\nepochs = len(train_loss)\n\nplt.plot(range(1,epochs+1), train_loss, label='train')\nplt.plot(range(1,epochs+1), valid_loss, label='validation')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.title('Train and Validation Loss Plot (Custom Model)')\nplt.legend()\nplt.show()\n\nplt.plot(range(1,epochs+1), train_acc, label='train')\nplt.plot(range(1,epochs+1), valid_acc, label='validation')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.title('Train and Validation Accuracy Plot (Custom Model)')\nplt.legend()\nplt.show()","8ae5e274":"prediction = np.round(base_model.predict(x_test)).flatten().astype(int)\n\nprint(\"=========================================================\\n\")\nprint(\"                    Base Model Result\")\nprint(\"=========================================================\\n\")\n\nacc = accuracy_score(y_test, prediction)\nprint(\"=========================================================\\n\")\nprint(\"Predicted Class (20 Samples):\")\nprint(prediction[:20])\nprint(\"\\nGround Truth (20 Samples):\")\nprint(y_test.values[:20])\n\nprint(\"\\n=========================================================\\n\")\naccuracy = accuracy_score(y_test, prediction)\naccuracy = accuracy*100\nprint(f\"Accuracy: {accuracy}%\")\n\nf1 = f1_score(y_test, prediction, average='macro')\nprint(f\"F1 Score: {f1}\")\n\nauc = roc_auc_score(y_test, prediction, average='macro')\nprint(f\"AUC Score: {auc}\")\n\nprint('\\n\\nClassification Report:')\ncr = classification_report(y_test, prediction)\nprint(cr)","f9f3f0c6":"tmp = pd.DataFrame(confusion_matrix(y_test, prediction), index = ['positive', 'negative'], columns = ['true', 'false'])\nsns.heatmap(tmp, annot=True, fmt='g')","e35eb4ac":"prediction = np.round(cust_model.predict(x_test)).flatten().astype(int)\n\nprint(\"=========================================================\\n\")\nprint(\"                 Custom Model Result\")\nprint(\"=========================================================\\n\")\n\nacc = accuracy_score(y_test, prediction)\nprint(\"=========================================================\\n\")\nprint(\"Predicted Class (20 Samples):\")\nprint(prediction[:20])\nprint(\"\\nGround Truth (20 Samples):\")\nprint(y_test.values[:20])\n\nprint(\"\\n=========================================================\\n\")\naccuracy = accuracy_score(y_test, prediction)\naccuracy = accuracy*100\nprint(f\"Accuracy: {accuracy}%\")\n\nf1 = f1_score(y_test, prediction, average='macro')\nprint(f\"F1 Score: {f1}\")\n\nauc = roc_auc_score(y_test, prediction, average='macro')\nprint(f\"AUC Score: {auc}\")\n\nprint('\\n\\nClassification Report:')\ncr = classification_report(y_test, prediction)\nprint(cr)","04311ecd":"tmp = pd.DataFrame(confusion_matrix(y_test, prediction), index = ['positive', 'negative'], columns = ['true', 'false'])\nsns.heatmap(tmp, annot=True, fmt='g')","373d8946":"### PS: Data Note\nWe will use the splitted train data since the actual test data doesn't have the target column provided so we are not able to perform evaluation later to our prediction data.","ce95206a":"# References\nhttps:\/\/www.tensorflow.org\/guide\/keras\/sequential_model\n\nhttps:\/\/keras.io\/api\/layers\/initializers\/\n\nhttps:\/\/machinelearningmastery.com\/tutorial-first-neural-network-python-keras\/\n\nhttps:\/\/machinelearningmastery.com\/display-deep-learning-model-training-history-in-keras\/","09959ddc":"# Model Evaluation Toward Test Data\nAt this last section, we will predict the actual test data using the model. At this last evaluation, we will show:\n- Prediction and ground truth samples (raw data showcase, up to 20 datas)\n- Accuracy and F1 score\n- Confusion matrix with heatmap","68265d75":"Removing the string version of education column since its redundant","e0d152ac":"# Importing Necessities","393a0259":"# Model Training Evaluation\nAt this section, we will retrieve loss (in both train and validation) from both of our model, as well as the accuracy result to be plotted later. we will see the training metrics plot comparison over epochs.","a06e27b0":"# Inspecting data and perform feature enginnering","fe981a3a":"At this phase, we will drop the row with null value since there is not much and we believe that we will still need the column to increase our model accuracy. Aside from dropping null columns, we will also categorize several columns with 'string-based class data'. After finishing the data transforming, finally we will rename our target column accordingly and inspect our table.","f7dd3163":"# Building and Training Model\nAs for our model, we will use a model base from [tensorflow's keras](https:\/\/www.tensorflow.org\/guide\/keras\/sequential_model) and add layers as listed below:\n- Input dimension with 13 nodes (therefore coming up with also 13 nodes)\n- Hidden layer with 2 nodes\n- Output layer with 1 node (the class output)\n\nfor our custom model architecture, we will make the architecture as listed below:\n- Input dimension with 13 nodes (also coming up with 13 node)\n- Batch Normalization Layer\n- Fully connected layer with 32 nodes\n- Batch Normalization layer\n- Fully connected layer with 16 nodes\n- Output layer with 1 node\n\nWith every layer have `kernel_initializer` parameter that will determine how the initializing weight happened and also activation function (ReLu, ad sigmoid activation for classification output). Also, we have `adam` optimizer, with `binary_crossentropy` loss function and `accuracy` metric.\n\nWith the configuration above, we will running the model training for 20 epochs, with batch size of 10.","0de0214b":"# Loading data","dffc62f8":"# 2301859650 - Cornelius Tantius\n[LO 1, LO 2, LO 3 & LO 4, 30 points] Your task is to implement a backpropagation algorithm on a\nsimple Back Propagation Neural Network (BPNN) architecture [The implementation can use\nTensorflow or PyTorch framework]. The baseline architecture is consisting of n nodes in the input\nlayer, 2 neurons in the hidden layer and 1 neuron in the output layer (n, 2, 1) with n is number of\nused inputs.\nNow, based on that simple BPNN python codes, develop a new model of BPNN capable of tackling\na binary classification task. Please employ datasets taken from Income dataset in Kaggle (\nhttps:\/\/www.kaggle.com\/mastmustu\/income ). You can use all inputs in the dataset or just choose\nthe part of it. The architecture of your new BPNN model must be adjusted accordingly."}}