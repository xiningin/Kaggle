{"cell_type":{"348001e0":"code","1dc78012":"code","6061174a":"code","3a1bcece":"code","02a865b2":"code","3e8cfcc5":"code","1364fc0b":"code","6f72c634":"code","07f22fe1":"code","0b31bc45":"code","70be66f0":"code","ba37a18d":"code","71d0136b":"code","7266506c":"code","a5d6b6eb":"code","34dfa983":"code","69ec43c2":"code","e30f6817":"code","b1a02e89":"code","e66a7bd4":"code","abc5905e":"code","9d8f3d04":"code","ba24cbce":"code","97f70865":"code","88e473d8":"code","bcd2aad5":"code","9a21b68a":"code","73d94e7c":"code","410549b6":"code","641c7190":"code","0771d435":"code","f37670c5":"code","5a8e9c1d":"code","313a609f":"code","732f9bf4":"code","daa6df4b":"code","9119346d":"code","e688bdbd":"code","0166e55d":"code","12d48b2a":"code","9e0f945d":"code","0a2d0af7":"markdown","99259c3b":"markdown","06d6d44f":"markdown","fd483f21":"markdown","565cadb9":"markdown","7ad4faf2":"markdown","052bd306":"markdown","205beef6":"markdown","b666e10b":"markdown","d455aff8":"markdown","609ed8a9":"markdown","e5f5dbc3":"markdown","6e44b23d":"markdown","c79d9f56":"markdown","7e0517aa":"markdown","8043b996":"markdown","fce16362":"markdown"},"source":{"348001e0":"import numpy as np \nimport pandas as pd\nimport seaborn as sns","1dc78012":"sales_train = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\nsales_train.head()","6061174a":"items = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\nitems.head()","3a1bcece":"item_categories = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nitem_categories.head()","02a865b2":"shops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')\nshops.head()","3e8cfcc5":"import string\nshops[\"shop_name\"] = shops[\"shop_name\"].str.replace('[^\\w\\s]','')  # Remove punctuations.\n\n## Uncomment following line to see how we can create city column\n## shops['city'] = shops['shop_name'].apply(lambda shop : shop.split()[0])\n## Since we are creating more than one columns, we will create all of them together.\n\nshops[[\"shop_city\", \"shop_category\", \"shop_name\"]] = shops[\"shop_name\"].str.split(n=2, expand=True)\nshops.drop('shop_name', axis=1, inplace=True)\nshops.head()","1364fc0b":"print('--Cities--')\nprint(shops['shop_city'].value_counts())\nprint('--Categories--')\nprint(shops['shop_category'].value_counts())\n","6f72c634":"item_categories = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv', index_col='item_category_id')\nitem_categories[\"item_category_name\"] = item_categories[\"item_category_name\"].str.replace('[^\\w\\s-]','')  # Remove punctuations\n\nitem_categories[[\"item_main_category_name\", \"item_sub_category_name\"]] = item_categories[\"item_category_name\"].str.split(pat='-', n=1, expand=True)\nitem_categories.drop('item_category_name', axis=1, inplace=True)\n# We fill missing values with category named 'undefined'.\nitem_categories.fillna(value = {'item_sub_category_name' : 'Undefined'}, inplace=True)\n\nitem_categories.head()","07f22fe1":"item_categories['item_sub_category_name'].value_counts()","0b31bc45":"sales_train = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\nsales_train.head()\nsales_train['date'] = pd.to_datetime(sales_train['date'],format='%d.%m.%Y')\nsales_train['day'], sales_train['month'], sales_train['year'], sales_train['weekday'] = sales_train['date'].dt.day, sales_train['date'].dt.month, sales_train['date'].dt.year, sales_train['date'].dt.weekday\n# Monday is 0\nsales_train = sales_train.merge(items.drop('item_name', axis=1), on='item_id')\nsales_train = sales_train.merge(item_categories, on='item_category_id')\nsales_train = sales_train.merge(shops, on='shop_id')\nsales_train.head(25)","70be66f0":"# TODO: Change this plot with something that clearly shows outliers\nsales_train['item_cnt_day'].plot() ","ba37a18d":"sales_train[sales_train['item_cnt_day'] == sales_train['item_cnt_day'].max()]","71d0136b":"sales_train.nlargest(20, 'item_cnt_day', keep='first')","7266506c":"pd.options.display.float_format = '{:.4f}'.format  # Globally suppress sci format\n# Total number of items sold across all shops per day can be found as follows,\n# we are trying to explore on which days sells were maximum.\nprint((sales_train.groupby('date').item_cnt_day).sum().nlargest(50, keep='first'))\n(sales_train.groupby('date').item_cnt_day).sum().plot()","a5d6b6eb":"sales_train = sales_train[(sales_train['item_cnt_day'] < 1001) & (sales_train['item_price'] < 100000) & (sales_train['item_price'] > 0)]","34dfa983":"sales_train.head(20)\nsales_train.shape","69ec43c2":"monthly_group = sales_train.groupby(['date_block_num', 'shop_id', 'item_id'])\nmonthly_count = monthly_group['item_cnt_day'].sum()  # Total number of items sold in month.\nmonthly_avg_price = monthly_group['item_price'].mean()  # Average price of all items sold in month.","e30f6817":"monthly_count.describe()","b1a02e89":"X_train = sales_train.drop(['date', 'item_cnt_day', 'day', 'weekday'], axis=1).set_index(['date_block_num', 'shop_id', 'item_id']).drop_duplicates()\nX_train['item_price'] = monthly_avg_price\nX_train['item_cnt_monthly'] = monthly_count\nX_train.reset_index(inplace=True)\nX_train.head()","e66a7bd4":"y_train = X_train['item_cnt_monthly']\nX_train.drop(['item_cnt_monthly'], axis = 1, inplace = True)","abc5905e":"X_train.head()","9d8f3d04":"X_train.columns","ba24cbce":"from sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import cross_val_score, train_test_split, TimeSeriesSplit\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\ncategorical_cols = ['item_main_category_name', 'item_sub_category_name', 'shop_id', 'shop_city','month', 'year']\nnumerical_cols = ['item_price']\ndrop_cols = ['shop_category', 'date_block_num']\npreprocessor = ColumnTransformer(\ntransformers=[\n    ('categorical', OneHotEncoder(handle_unknown = 'ignore'), categorical_cols),\n    ('numerical', StandardScaler(), numerical_cols)    \n])\n\nlinear_model = SGDRegressor(early_stopping = True, learning_rate='adaptive')\nforest_model = RandomForestRegressor(n_estimators = 4)\nlinear_pipe = Pipeline(steps=[('preprocessor', preprocessor), ('model', linear_model)])\nforest_pipe = Pipeline(steps=[('preprocessor', preprocessor), ('model', forest_model)])","97f70865":"from sklearn.metrics import mean_squared_error\nX_train.drop(drop_cols, axis=1, inplace=True)\nX_train1, X_valid1, y_train1, y_valid1 = X_train[:250000], X_train[250000:], y_train[:250000], y_train[250000:]\ny_valid1_np = np.clip(y_valid1.to_numpy(), 0, 20)","88e473d8":"linear_pipe.fit(X_train1, y_train1)\nans1 = linear_pipe.predict(X_valid1)\nans1 = np.clip(ans1, 0, 20)\nprint(\"RMSE:\",(mean_squared_error(ans1, y_valid1_np)**0.5))","bcd2aad5":"#forest_pipe.fit(X_train1, y_train1)\n#ans2 = forest_pipe.predict(X_valid1)\n#ans2 = np.clip(ans2, 0, 20)\n#print(\"RMSE:\",(mean_squared_error(ans2, y_valid1_np)**0.5))","9a21b68a":"# XGBoost\nfrom xgboost import XGBRegressor\n\nxgb_model = XGBRegressor(n_estimators = 150, max_depth = 15, learning_rate = 0.21, random_state=0)\nxgb_pipe = Pipeline(steps=[('preprocessor', preprocessor), ('model', xgb_model)])\nxgb_pipe.fit(X_train1, y_train1)\nans3 = xgb_pipe.predict(X_valid1)\nans3 = np.clip(ans3, 0, 20)\nprint(\"RMSE:\",(mean_squared_error(ans3, y_valid1_np)**0.5))","73d94e7c":"ans3_tr = xgb_pipe.predict(X_train1)\nans3_tr = np.clip(ans3_tr, 0, 20)\ny_train1_np = np.clip(y_train1.to_numpy(), 0, 20)\nprint(\"RMSE:\",(mean_squared_error(ans3_tr, y_train1_np)**0.5))","410549b6":"pd.Series(abs(ans3-y_valid1_np)).nlargest(n=20)","641c7190":"y_valid1_np[6917]  # Actual sales of (one of the) highest error entry.","0771d435":"pd.Series(ans3).iloc[6917]  # Predicted sales of same entry.","f37670c5":"X_valid1.iloc[7041]  # Let's see what is that entry.","5a8e9c1d":"y_train1[X_train1['item_id'] == 11457] # Have we seen that item_id earlier, what was its sales?","313a609f":"# This item was being sold in less quantities, what caused our model to predict higher sales?\nX_train1[X_train1['item_id'] == 11457]","732f9bf4":"X_train1[X_train1['item_id'] == 12397]","daa6df4b":"print(X_valid1.iloc[7041])\nprint(X_valid1.iloc[6917])","9119346d":"'''Index(['date_block_num', 'shop_id', 'item_id', 'item_price', 'month', 'year',\n       'item_category_id', 'item_main_category_name', 'item_sub_category_name',\n       'shop_city', 'shop_category'],\n      dtype='object')'''\n#todo : item price function\ntest_raw = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/test.csv\", index_col='ID')\ntest_raw['date_block_num'] = 34\ntest_raw['month'] = 11\ntest_raw['year'] = 2015\n\n#test_raw['item_category_id'] = items[items['item_id'] == test_raw['item_id']].item_category_id","e688bdbd":"#test_raw = pd.merge(test_raw, items, how=\"left\", on=\"item_id\")\n#test_raw = pd.merge(test_raw, item_categories, how=\"left\", on=\"item_category_id\")\n#test_raw = pd.merge(test_raw, shops, how=\"left\", on=\"shop_id\")\n\ntest_raw = test_raw.merge(items, on='item_id', suffixes=[None, '_drop'])\ntest_raw = test_raw.merge(item_categories, on='item_category_id', suffixes=[None, '_drop'])\ntest_raw = test_raw.merge(shops, on='shop_id', suffixes=[None, '_drop'])\ntest_raw.drop(test_raw.filter(regex='_drop$').columns.tolist(), axis=1, inplace=True)\n#test_raw.drop(drop_cols, axis=1, inplace=True)\ntest_raw['item_price'] = 0\ntest_raw = test_raw[X_train.columns]","0166e55d":"X_train.head()","12d48b2a":"test_raw.head()","9e0f945d":"len(X_train[(X_train['shop_id'] == 48) & (X_train['item_id'] == 944)]) == 0","0a2d0af7":"## EDA\n\nI had done EDA earlier, but I've removed major portions of it right now, I'll be addding them back again.","99259c3b":"Let's have a look at item_categories.csv","06d6d44f":"For now, I will not extract furthur information from item categories. Now I will prepare training data, for which we will use sales_train.csv and then join appropriate columns, so that we will have all required features in a single dataframe.","fd483f21":"# **Preprocessing and Exploratory Data Analysis**\n\n## Preprocessing\n\nInstead of preprocessing all csv files together, I will be chhecking all csv files one-by-one and see what preprocessing I need to do, I will also see what other features I can extract.\n","565cadb9":"*Note:* This still WIP (work in progress) and it is my first attempt to kaggle competitons from scratch. I'm trying to write this in a manner suitable for those who are just starting with kaggle competitions like me. I will start from basic classical ML models and planning to use LSTM at the end. If this notebook helped you, please give us an upvote!","7ad4faf2":"Our outlier:","052bd306":"Upon translating above into English, I found that some city names and some categories are incorrect, however such entries are less, so I kept things as they are.","205beef6":"Let's explore this data more.","b666e10b":"As you can see, each entry in above dataframe represents non-zero sales of particular item, in a particular shop, on a particular date. If you see our test data, we have been asked to predict *monthly* sales for next one month for given items in a given shop & it's 214k rows. We are not given any information apart from item_id and shop_id, especially we are not given item_price for test set. Other features like item_category, shop_city can be fetched from the training data using item_id and shop_id, however item_price understandabaly varies across different dates even for the same shop & item pair. Getting item_price for our test data, is going to be another challenge we have to deal with.\n\nClearly our current sales_train dataframe and test data are not consistent because our sales_train currently has data for non-zero daily sales only. Following two solutions to this came to my mind, however there are other approaches discussed in other notebooks.\n\nApproach 1. We can make predictions for each day in upcoming month and add total sales for each shop & item combination to get monthly sales prediction. If we follow this approach, we have to make 214k * 30 predictions & then again reprocess them to find sum to get monthly count. Also, on many days sales would be zero and our current training data only has non-zero entries, so we will (probably) need to teach our model somehow - by modifying training data - that most of the days will have zero sales. If to do so we need to add zero rows, our training set, which already has <> rows, will become very large along with large test set. So I haven't used this approach.\n\nApproach 2. Instead, we can restructure our training set to look more like test set. To do so, we create a new dataframe, where each record shows monthly sales of particular item_id and shop_id with average price for the same. To find average price, we first group rows by month, item_id and shop_id and find average price of that group, monthly sales count is found by taking sum of daily sales count of this group.","d455aff8":"#TODO: More EDA later. Like sales vs weekday etc.","609ed8a9":"We do have some outliers, but detecting them would be easy once we've merged things, so I'm not removing them right now.","e5f5dbc3":"Categories like PS2, PS3 can be futher merged into same category - PS. Some notebooks have used that approach, however I'm instead extracting features more generally from the category name. I translated this Russian file to English, after which it seems that if we split category name into two different categories, viz. main category and sub cateogry, it might be more helpful. We would try doing so. But let's check remaining shops.csv before preprocessing everything.","6e44b23d":"Let's remove outliers and preparing final training data.","c79d9f56":"After talking with some people who know Russian (internet is very useful thing), I figured out that most of shop names in fact are in format : CityName TP\/TPK\/ETC ShopOrMallName. TP\/TPK\/TU means Shopping complex, Shopping and Entertainment complex and Shopping center respectively. So we can add another columns, city name, shop category (TP\/TPK\/TU\/Others) and vanila shop name. \n\nNow, we have seen all files and can start prepreocessing, I will start from shops.csv and then see what we can do for item_categories.csv, then we will merge data from all 4 files into a single dataframe.","7e0517aa":"I was very optimistic while I made weekday feature. Because sales heavily depend on which day of week it is in real life. However, we will need to remove daily features from our training set now that we are creating monthly training set. So basically, I created a feature, didn't use it and now deleting it. But I'm leaving all those steps here in this notebook, as a reminder to myself & to emphasise the fact that machine learning is an iterative process.","8043b996":"## Preparing training data\n\nCurrently we have following training data.","fce16362":"This file has item_id, we can add another feature item_category by merging information from the items.csv"}}