{"cell_type":{"e29f32bc":"code","50376c96":"code","b47f1d10":"code","5a835868":"code","1330e4ab":"code","57655dd0":"code","acbe59c7":"code","a279e0f5":"code","dd919b1b":"code","34311573":"code","deba41ce":"code","e142976d":"code","1c888689":"code","dfcdc88b":"code","7ba1bf46":"code","8ad01acb":"code","2fc4b080":"code","f60422ec":"code","6ccce68e":"code","abd5d69b":"code","b4754b3d":"code","b104d928":"code","a746468e":"code","7b2a9418":"code","715aa265":"code","432b7f98":"code","f40ae7ad":"code","15706c6e":"code","b0d7fee6":"code","7946c1f3":"code","e6e004e3":"code","1660d09d":"code","b049d19c":"code","89062b97":"code","2a994a10":"code","65cab8f0":"code","9481cf81":"code","436218e9":"code","17a60df5":"code","e2390936":"code","105ed50a":"code","6ef3a381":"code","3e5fd73d":"code","cbbb7322":"code","93cb5ceb":"code","a9f3c0d5":"code","7b0b5944":"code","2b1bd8b5":"code","169082d2":"code","88eb1c86":"code","89f432b9":"code","ffb0adab":"code","01ef9cfc":"code","b91b039e":"code","5438b3e1":"code","4aecca52":"code","c9c1a044":"code","372bf891":"code","a1eb99f0":"code","0cbb17c1":"code","3ea3093e":"code","4a9f2691":"code","9b485faf":"code","0c874eb1":"code","2e1db423":"code","5ae81616":"code","82653096":"code","aa5f8922":"code","70c8ae5d":"code","49e738f0":"code","f0067c9f":"code","e9606fbd":"code","a7f20682":"code","ec21a7fd":"code","1a84a84d":"code","8083739b":"code","03d551b6":"code","b9073e15":"markdown","2daa8fb9":"markdown","69d578de":"markdown","6bfbc946":"markdown","214dd515":"markdown","cdcedf68":"markdown","89b13065":"markdown","b5d16ec8":"markdown","b672f1c4":"markdown","b8b76f84":"markdown","4ad69f19":"markdown","b7e491fa":"markdown","0035ee7a":"markdown","cc2b73a2":"markdown","7fff9a6d":"markdown","f7c8d5c2":"markdown","3bfa4bf0":"markdown","77ecb3af":"markdown","e124e660":"markdown","5fd8f5c8":"markdown","ebc31b0e":"markdown","f0119768":"markdown","ce99648c":"markdown","5a007b88":"markdown","4c743215":"markdown","f240dc28":"markdown","0c93e847":"markdown","027ed195":"markdown","86748d64":"markdown","03f74ce5":"markdown","9fdc832c":"markdown","9e81f0bf":"markdown","87bbe597":"markdown","fe05d329":"markdown","d685ed3c":"markdown","4b49e47d":"markdown","2ab52c2c":"markdown","2b4c1b07":"markdown","f8db978e":"markdown","e473d8c3":"markdown","61fd1aea":"markdown","0d6dc96f":"markdown","13ba7f2b":"markdown","6425ded1":"markdown","773db060":"markdown","e3c11c17":"markdown","53dd7a21":"markdown","e7c2379e":"markdown","57157d34":"markdown","1fca9054":"markdown"},"source":{"e29f32bc":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport gc\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 150)\npd.set_option('max_rows', 150)","50376c96":"import matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nimport seaborn as sns\nfrom scipy import stats\n#To plot figs on jupyter\n%matplotlib inline\n# figure size in inches\nrcParams['figure.figsize'] = 14,6\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 200)\npd.set_option('max_rows', 200)","b47f1d10":"PATH =   r\"\/kaggle\/input\/features\/FEATURES-2014-2015\/\"\ndf = pd.read_csv(PATH+\"part-067.csv\")\ndf.shape","5a835868":"df.columns","1330e4ab":"df.head()","57655dd0":"df.dtypes","acbe59c7":"df.isAnomaly.value_counts()","a279e0f5":"df[df['isAnomaly']==True].describe()","dd919b1b":"# Plots the disribution of a variable colored by value of the target\ndef kde_target(var_name, df):\n    \n    # Calculate the correlation coefficient between the new variable and the target\n    corr = df['isAnomaly'].corr(df[var_name])\n    \n    # Calculate medians for repaid vs not repaid\n    avg_highr = df.loc[df['isAnomaly'] == 0, var_name].median()\n    avg_lowr = df.loc[df['isAnomaly'] == 1, var_name].median()\n    \n    plt.figure(figsize = (12, 6))\n    \n    # Plot the distribution for target == 0 and target == 1\n    sns.kdeplot(df.loc[df['isAnomaly'] == 0, var_name], label = 'isAnomaly == 0')\n    sns.kdeplot(df.loc[df['isAnomaly'] == 1, var_name], label = 'isAnomaly == 1')\n    \n    # label the plot\n    plt.xlabel(var_name); plt.ylabel('Density'); plt.title('%s Distribution' % var_name)\n    plt.legend();\n    \n    # print out the correlation\n    print('The correlation between %s and the TARGET is %0.4f' % (var_name, corr))\n    # Print out average values\n    print('Median value for request with high runtime value = %0.4f' % avg_highr)\n    print('Median value for request with low runtime value =     %0.4f' % avg_lowr)\n","34311573":"    \nkde_target(r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)', df[[r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)','isAnomaly']].dropna(),)","deba41ce":"kde_target(r'Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed \/ MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))', df[[r'Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed \/ MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))','isAnomaly']].dropna(),)","e142976d":"kde_target('Heap usage activity : (d\/dx (MXBean(java.lang:type=Memory).HeapMemoryUsage.used))', df[['Heap usage activity : (d\/dx (MXBean(java.lang:type=Memory).HeapMemoryUsage.used))','isAnomaly']].dropna(),)","1c888689":"## DATASET 1\nmu, sigma = 0.0, 1.0\nent1 = np.zeros((10000))\nfor i in range(10):\n#     print(mu)\n    for j in range(1000):\n        ent1[1000*i+j] = np.random.normal(mu, sigma)\n    mu = mu + 9 - i\n\na1 = 0.6\na2 = -0.5\nds1 = np.zeros((10000))\nds1[0] = ent1[0]\nds1[1] = ent1[1]\nfor i in range(2,10000):\n    ds1[i] = a1*ds1[i-1] + a2*ds1[i-2] + ent1[i]\n## DATASET 2\nmu = 0.0\nent2 = np.zeros((10000))\nfor i in range(10):\n#     print(mu)\n    for j in range(1000):\n        sigma = 0.1\/(0.01 + (10000 - (i*1000 + j))\/10000)\n        ent2[1000*i+j] = np.random.normal(mu, sigma)\n    mu = mu + 1\n\na1 = 0.6\na2 = -0.5\nds2 = np.zeros((10000))\nds2[0] = ent1[0]\nds2[1] = ent1[1]\nfor i in range(2,10000):\n    ds2[i] = a1*ds2[i-1] + a2*ds2[i-2] + ent2[i]\n\n## DATASET 3\nmu, sigma1, sigma3 = 0.0, 1.0, 3.0\nds3 = np.zeros((10000))\nfor i in range(10):\n    if i in {0,2,4,6,8}:\n        for j in range(1000):\n            ds3[1000*i+j] = np.random.normal(mu, sigma1)\n    else:\n        for j in range(1000):\n            ds3[1000*i+j] = np.random.normal(mu, sigma3) ","dfcdc88b":"plt.figure(figsize=(16,4))\nplt.plot(ent1)\nplt.title('Dataset 1')\nplt.ylabel('Values')\nplt.xlabel('Count')\nplt.legend()\n\nplt.figure(figsize=(16,4))\nplt.plot(ent2)\nplt.title('Dataset 2')\nplt.ylabel('Values')\nplt.xlabel('Count')\nplt.legend()\n\nplt.figure(figsize=(16,4))\nplt.plot(ds1)\nplt.title('Dataset 3')\nplt.ylabel('Values')\nplt.xlabel('Count')\nplt.legend()\n\nplt.figure(figsize=(16,4))\nplt.plot(ds2)\nplt.title('Dataset 4')\nplt.ylabel('Values')\nplt.xlabel('Count')\nplt.legend()\n\nplt.figure(figsize=(16,4))\nplt.plot(ds3)\nplt.title('Dataset 5')\nplt.ylabel('Values')\nplt.xlabel('Count')\nplt.legend()\n\nplt.show()","7ba1bf46":"!pip install luminol\nimport luminol","8ad01acb":"from luminol import anomaly_detector,correlator\n\nfrom luminol.anomaly_detector import AnomalyDetector\nfrom luminol.correlator import Correlator\n","2fc4b080":"#Luminol - only if module 'luminol was installed'\n#data preprocessing for the framework\n\ndata = np.array(ent1)\nts_s = pd.Series(data)\nts_dict = ts_s.to_dict()\n\ndata2 = np.array(ent2)\nts_s2 = pd.Series(data)\nts_dict2 = ts_s.to_dict()","f60422ec":"detector = anomaly_detector.AnomalyDetector(ts_dict)\nanomalies = detector.get_anomalies()","6ccce68e":"if anomalies:\n    time_period = anomalies[0].get_time_window()\n    correlator = correlator.Correlator(ts_dict, ts_dict2, time_period)","abd5d69b":"print(correlator.get_correlation_result().coefficient)","b4754b3d":"# score = detector.get_all_scores()\nfor timestamp, value in score.iteritems():\n    print(timestamp, value)","b104d928":"def scoreLuminolALLData(ts_dict):    \n    data = np.array(ts_dict)\n    ts_s = pd.Series(data)\n    ts_dict = ts_s.to_dict()\n\n\n    detector = anomaly_detector.AnomalyDetector(ts_dict)\n    score = detector.get_all_scores()\n    score_v = []\n    for timestamp, value in score.iteritems():\n        score_v.append(value)\n#         print(timestamp, value)\n    return score_v","a746468e":"dataplot1 = scoreLuminolALLData(ent1)    \ndataplot2 = scoreLuminolALLData(ent2) \ndataplot3 = scoreLuminolALLData(ds1)    \ndataplot4 = scoreLuminolALLData(ds2)        \ndataplot5 = scoreLuminolALLData(ds3) ","7b2a9418":"dataLUMINOL_dataset1 = np.array(dataplot1)\nfrom scipy import stats\ndataLUMINOL_dataset1 = stats.describe(dataplot1)\ndataLUMINOL_dataset1","715aa265":"qt25_ds1 = np.percentile(dataplot1, 25)  # Q1\nqt50_ds1 = np.percentile(dataplot1, 50)  # Q2\nqt75_ds1 = np.percentile(dataplot1, 75)  # Q3\nqt25_ds1,qt50_ds1, qt75_ds1","432b7f98":"dfLUMINOL_dataset1 = pd.DataFrame(dataplot1, columns=['Score'])","f40ae7ad":"def plot_anomaly_score_low_higt(datascore, data):\n    datascore_ = np.array(datascore)\n    from scipy import stats\n    datascore_ = stats.describe(datascore)\n    \n    datascore_ = pd.DataFrame(datascore, columns=['Score'])\n\n    delta = np.percentile(datascore, 75)\n    print('Threashold ',delta)\n\n    plt.figure(figsize=(16,6))\n    plt.plot(data)\n    plt.title(\"data count\")        \n\n    plt.figure(figsize=(16,6))\n    plt.plot(datascore)\n    plt.title(\"data count\")        \n\n    \n    plt.figure(figsize=(16,6))\n    df_high_data_ = datascore_[datascore_ <= delta]\n    df_high_score_ = datascore_[datascore_ > delta]\n    \n    plt.plot(datascore_.index, datascore_.Score.fillna(1), c='gray', alpha=0.4)\n    plt.scatter(df_high_data_.index, df_high_data_.values, label='Inline', s=10)\n    plt.scatter(df_high_score_.index, df_high_score_.values, label='Outlier', c='red', s=10)\n    plt.margins(x=0.01,y=0.2)\n    plt.title('Anomaly Score ')\n    plt.ylabel('Score')\n    plt.xlabel('Data Count')\n    plt.legend()\n    plt.show()","15706c6e":"plot_anomaly_score_low_higt(dataplot1, ent1)","b0d7fee6":"plot_anomaly_score_low_higt(dfLUMINOL_dataset1, ent1)","7946c1f3":"dataLUMINOL_dataset2 = np.array(dataplot2)\nfrom scipy import stats\ndataLUMINOL_dataset2 = stats.describe(dataplot2)\ndataLUMINOL_dataset2","e6e004e3":"qt25_ds2 = np.percentile(dataplot2, 25)  # Q1\nqt50_ds2 = np.percentile(dataplot2, 50)  # Q2\nqt75_ds2 = np.percentile(dataplot2, 75)  # Q3\nqt25_ds2,qt50_ds2, qt75_ds2","1660d09d":"dfLUMINOL_dataset2 = pd.DataFrame(dataplot2, columns=['Score'])","b049d19c":"plot_anomaly_score_low_higt(dfLUMINOL_dataset2, ent2)","89062b97":"dataLUMINOL_dataset3 = np.array(dataplot3)\nfrom scipy import stats\ndataLUMINOL_dataset3 = stats.describe(dataplot3)\ndataLUMINOL_dataset3","2a994a10":"qt25_ds3 = np.percentile(dataplot3, 25)  # Q1\nqt50_ds3 = np.percentile(dataplot3, 50)  # Q2\nqt75_ds3 = np.percentile(dataplot3, 75)  # Q3\nqt25_ds3,qt50_ds3, qt75_ds3","65cab8f0":"dfLUMINOL_dataset3 = pd.DataFrame(dataplot3, columns=['Score'])","9481cf81":"plot_anomaly_score_low_higt(dfLUMINOL_dataset3, ds1)\n","436218e9":"dataLUMINOL_dataset4 = np.array(dataplot4)\nfrom scipy import stats\ndataLUMINOL_dataset4 = stats.describe(dataplot4)\ndataLUMINOL_dataset4","17a60df5":"qt25_ds4 = np.percentile(dataplot4, 25)  # Q1\nqt50_ds4 = np.percentile(dataplot4, 50)  # Q2\nqt75_ds4 = np.percentile(dataplot4, 75)  # Q3\nqt25_ds4, qt50_ds4, qt75_ds4","e2390936":"dfLUMINOL_dataset4 = pd.DataFrame(dataplot4, columns=['Score'])","105ed50a":"plot_anomaly_score_low_higt(dfLUMINOL_dataset4, ds2)","6ef3a381":"\ndataLUMINOL_dataset5 = np.array(dataplot5)\nfrom scipy import stats\ndataLUMINOL_dataset5 = stats.describe(dataplot5)\ndataLUMINOL_dataset5","3e5fd73d":"qt25_ds5 = np.percentile(dataplot5, 25)  # Q1\nqt50_ds5 = np.percentile(dataplot5, 50)  # Q2\nqt75_ds5 = np.percentile(dataplot5, 75)  # Q3\nqt25_ds5, qt50_ds5, qt75_ds5","cbbb7322":"dfLUMINOL_dataset5 = pd.DataFrame(dataplot5, columns=['Score'])\n","93cb5ceb":"plot_anomaly_score_low_higt(dfLUMINOL_dataset5, ds3)","a9f3c0d5":"from scipy.stats import norm, skew","7b0b5944":"(mu, sigma) = norm.fit(df.loc[df['isAnomaly'] == 1, r'Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed \/ MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))'])\nprint(\n    'Memory space usage : anomaly {:.1f} and standard deviation = {:.1f}'.format(mu, sigma))","2b1bd8b5":"(mu, sigma) = norm.fit(df.loc[df['isAnomaly'] == 1, r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)'])\nprint(\n    'Thread CPU time : anomaly {:.1f} and standard deviation = {:.1f}'.format(mu, sigma))","169082d2":"plt.figure(figsize=(15,6))\nplt.style.use('seaborn-colorblind')\nplt.grid(True, alpha=0.5)\nsns.kdeplot(df.loc[df['isAnomaly'] == 0, r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)'], label = 'Inline')\nsns.kdeplot(df.loc[df['isAnomaly'] == 1, r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)'], label = 'Outlier')\nplt.xlabel('Thread CPU time anomaly analysis')\nplt.xlim(left=0)\nplt.ylabel('Density')\nplt.title('Thread CPU time Distribution in Percent by Anomaly Event');","88eb1c86":"plt.figure(figsize=(15,6))\nplt.style.use('seaborn-colorblind')\nplt.grid(True, alpha=0.5)\nsns.kdeplot(df.loc[df['isAnomaly'] == 0, r'Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed \/ MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))'], label = 'Inline')\nsns.kdeplot(df.loc[df['isAnomaly'] == 1, r'Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed \/ MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))'], label = 'Outlier')\nplt.xlabel('Memory space usage anomaly analysis')\nplt.xlim(left=0)\nplt.ylabel('Density')\nplt.title('Memory space usage Distribution in Percent by Anomaly Event');","89f432b9":"\n\ndf[[r'timestamp','isAnomaly', r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)', r'Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed \/ MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))']][df['isAnomaly']==1].head()  \n\n","ffb0adab":"df[r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)'].value_counts()","01ef9cfc":"df['Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed \/ MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))'].value_counts()","b91b039e":"cpu_values = df[r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)'].values\ndataplot_ThreadCPUtime = scoreLuminolALLData(cpu_values)    \n# # dataplot_ThreadCPUtime","5438b3e1":"memory_values = df['Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed \/ MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))'].values\ndataplot_memoryspaceusage = scoreLuminolALLData(memory_values)    \n# dataplot_memoryspaceusage\n","4aecca52":"def correlation_anomaly(ts_dict,ts_dict2,scoreThreshold,period):\n    data = np.array(ts_dict)\n    ts_s = pd.Series(data)\n    ts1 = ts_s.to_dict()\n\n    data2 = np.array(ts_dict2)\n    ts_s2 = pd.Series(data2)\n    ts2 = ts_s2.to_dict()\n#     print('shape ts1, ts2 ',len(ts1), len(ts2))\n    \n    \n    my_detector = AnomalyDetector(ts1)\n    score = my_detector.get_all_scores()\n    anomalies = my_detector.get_anomalies()\n    for a in anomalies[:period]:\n        time_period = a.get_time_window()\n        my_correlator = Correlator(ts1, ts2, time_period)\n        if my_correlator.is_correlated(threshold=scoreThreshold):\n            print(\"ts2 correlate with ts1 at time period (%d, %d)\" % time_period)\n","c9c1a044":"def correlation_anomaly(ts_dict,ts_dict2,scoreThreshold, size):\n    from luminol.correlator import Correlator\n    data = np.array(cpu_values)\n    ts_s = pd.Series(data)\n    ts1 = ts_s.to_dict()\n\n\n    data2 = np.array(memory_values)\n    ts_s2 = pd.Series(data2)\n    ts2 = ts_s2.to_dict()\n\n    detector = anomaly_detector.AnomalyDetector(ts1)\n    anomalies = detector.get_anomalies()\n\n    if anomalies:  \n        for a in range(size):\n            time_period = anomalies[a].get_time_window()\n            my_correlator = Correlator(ts1, ts2, time_period)\n#             print(correlator.get_correlation_result().coefficient)\n#             if my_correlator.is_correlated(threshold=scoreThreshold):\n            print(\"CPU  correlate with  MEMORY at time period (%d, %d)\" % time_period)\n            print('CPU x MEMORY correlation ',correlator.get_correlation_result().coefficient, '\\n')\n                                \n\ncorrelation_anomaly(cpu_values, memory_values,0.10, 11)\n\n","372bf891":"\ndataLUMINOL_memoryspaceusage = np.array(dataplot_memoryspaceusage)\nfrom scipy import stats\ndataLUMINOL_memoryspaceusage = stats.describe(dataplot_memoryspaceusage)\ndataLUMINOL_memoryspaceusage\n","a1eb99f0":"qt25_memoryusage = np.percentile(dataplot_memoryspaceusage, 25)  # Q1\nqt50_memoryusage = np.percentile(dataplot_memoryspaceusage, 50)  # Q2\nqt75_memoryusage = np.percentile(dataplot_memoryspaceusage, 75)  # Q3\nqt25_memoryusage, qt50_memoryusage, qt75_memoryusage\ndataLUMINOL_memoryspaceusage = pd.DataFrame(dataplot_memoryspaceusage, columns=['Score'])\n","0cbb17c1":"plot_anomaly_score_low_higt(dataplot_memoryspaceusage, df[['Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed \/ MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))']].values)","3ea3093e":"plt.figure(figsize=(16,6))\ndf_high_data_ = df[[r'timestamp','isAnomaly', r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)', r'Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed \/ MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))']][df['isAnomaly']==1][ r'Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed \/ MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))']\ndf_high_score_ = df[[r'timestamp','isAnomaly', r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)', r'Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed \/ MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))']][df['isAnomaly']==0][ r'Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed \/ MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))']\n\n    \nplt.plot(df.index, df.isAnomaly.fillna(1), c='gray', alpha=0.4)\nplt.scatter(df_high_data_.index, df_high_data_.values, label='Inline', s=30)\nplt.scatter(df_high_score_.index, df_high_score_.values, label='Outlier', c='red', s=10)\nplt.margins(x=0.01,y=0.2)\nplt.title('Values Grand truth ')\nplt.ylabel('Values')\nplt.xlabel('Data Count')\nplt.legend()\nplt.show()","4a9f2691":"dataLUMINOL_ThreadCPUtime = np.array(dataplot_ThreadCPUtime)\nfrom scipy import stats\ndataLUMINOL_ThreadCPUtime = stats.describe(dataplot_ThreadCPUtime)\ndataLUMINOL_ThreadCPUtime\n","9b485faf":"qt25_ThreadCPUtime = np.percentile(dataplot_ThreadCPUtime, 25)  # Q1\nqt50_ThreadCPUtime = np.percentile(dataplot_ThreadCPUtime, 50)  # Q2\nqt75_ThreadCPUtime = np.percentile(dataplot_ThreadCPUtime, 75)  # Q3\nqt25_ThreadCPUtime, qt50_ThreadCPUtime, qt75_ThreadCPUtime\ndataLUMINOL_ThreadCPUtime = pd.DataFrame(dataplot_ThreadCPUtime, columns=['Score'])\n","0c874eb1":"plot_anomaly_score_low_higt(dataplot_ThreadCPUtime, df[['Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)']].values)","2e1db423":"plt.figure(figsize=(16,6))\ndf_high_data_ = df[[r'timestamp','isAnomaly', r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)', r'Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed \/ MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))']][df['isAnomaly']==1][ r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)']\ndf_high_score_ = df[[r'timestamp','isAnomaly', r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)', r'Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed \/ MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))']][df['isAnomaly']==0][ r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)']\n\n    \nplt.plot(df.index, df.isAnomaly.fillna(1), c='gray', alpha=0.4)\nplt.scatter(df_high_data_.index, df_high_data_.values, label='Inline', s=40)\nplt.scatter(df_high_score_.index, df_high_score_.values, label='Outlier', c='red', s=10)\nplt.margins(x=0.01,y=0.2)\nplt.title('Values Grand truth ')\nplt.ylabel('Values')\nplt.xlabel('Data Count')\nplt.legend()\nplt.show()","5ae81616":"!pip install fbprophet\nfrom fbprophet import Prophet\nimport os","82653096":"# View the data as a table\ndf_ = pd.DataFrame(df, columns=['timestamp', r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)'])\ndf_['ds']=df_['timestamp']\ndf_['y']=df_[r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)'].astype(float)\ndf_=df_.drop(['timestamp',r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)'],axis=1)\ndf_.head()","aa5f8922":"def fit_predict_model(dataframe, interval_width = 0.99, changepoint_range = 0.8):\n    m = Prophet(daily_seasonality = False, yearly_seasonality = False, weekly_seasonality = False,\n#                 seasonality_mode = 'multiplicative', \n                interval_width = interval_width,\n                changepoint_range = changepoint_range)\n    m = m.fit(dataframe)\n    \n    forecast = m.predict(dataframe)\n    forecast['fact'] = dataframe['y'].reset_index(drop = True)\n    print('Displaying Prophet plot')\n    fig1 = m.plot(forecast)\n    return forecast\n    \npred = fit_predict_model(df_)","70c8ae5d":"pred.tail()","49e738f0":"def detect_anomalies(forecast):\n    forecasted = forecast[['ds','trend', 'yhat', 'yhat_lower', 'yhat_upper', 'fact']].copy()\n    #forecast['fact'] = df['y']\n\n    forecasted['anomaly'] = 0\n    forecasted.loc[forecasted['fact'] > forecasted['yhat_upper'], 'anomaly'] = 1\n    forecasted.loc[forecasted['fact'] < forecasted['yhat_lower'], 'anomaly'] = 1 #-1\n\n    #anomaly importances\n    forecasted['importance'] = 0\n    forecasted.loc[forecasted['anomaly'] ==1, 'importance'] = \\\n        (forecasted['fact'] - forecasted['yhat_upper'])\/forecast['fact']\n    forecasted.loc[forecasted['anomaly'] ==-1, 'importance'] = \\\n        (forecasted['yhat_lower'] - forecasted['fact'])\/forecast['fact']\n    \n    return forecasted\n\npred = detect_anomalies(pred)","f0067c9f":"pred.head()","e9606fbd":"pred[ r'anomaly'].value_counts()","a7f20682":"plt.figure(figsize=(16,6))\ndf_outlier_ = pred[[r'ds','anomaly', r'yhat']][pred['anomaly']==1][ r'yhat']\n# df_outlier = pred[[r'ds','anomaly', r'yhat']][pred['anomaly']==-1][ r'yhat']\n\ndf_inline_ = pred[[r'ds','anomaly', r'yhat']][pred['anomaly']==0][ r'yhat']\n\nplt.plot(pred.index, pred.anomaly.fillna(1), c='gray', alpha=0.4)\nplt.scatter(df_inline_.index, df_inline_.values, label='Inline', s=40)\nplt.scatter(df_outlier_.index, df_outlier_.values, label='Outlier', c='red', s=10)\n# plt.scatter(df_outlier.index, df_outlier.values, label='Outlier', c='black', s=10)\nplt.margins(x=0.01,y=0.2)\nplt.title('Values Grand truth ')\nplt.ylabel('Values')\nplt.xlabel('Data Count')\nplt.legend()\nplt.show()","ec21a7fd":"plt.figure(figsize=(16,6))\ndf_high_data_ = df[[r'timestamp','isAnomaly', r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)', r'Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed \/ MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))']][df['isAnomaly']==1][ r'Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed \/ MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))']\ndf_high_score_ = df[[r'timestamp','isAnomaly', r'Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)', r'Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed \/ MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))']][df['isAnomaly']==0][ r'Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed \/ MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))']\n\n    \nplt.plot(df.index, df.isAnomaly.fillna(1), c='gray', alpha=0.4)\nplt.scatter(df_high_data_.index, df_high_data_.values, label='Inline', s=30)\nplt.scatter(df_high_score_.index, df_high_score_.values, label='Outlier', c='red', s=10)\nplt.margins(x=0.01,y=0.2)\nplt.title('Values Grand truth ')\nplt.ylabel('Values')\nplt.xlabel('Data Count')\nplt.legend()\nplt.show()","1a84a84d":"import plotly.graph_objs as go\nimport plotly as py\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected=True)","8083739b":"trace = go.Scatter(\n        name = 'Thread CPU time',\n       mode = 'markers',\n       x = list(df['timestamp']),\n       y = list(df['Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)']),\n       marker=dict(\n              color='#FFBAD2',\n              line=dict(width=1)\n       )\n)\ntrace1 = go.Scatter(\n    name = 'trend',\n       mode = 'lines',\n       x = list(pred['ds']),\n       y = list(pred['yhat']),\n       marker=dict(\n              color='red',\n              line=dict(width=1)\n       )\n)\nupper_band = go.Scatter(\n    name = 'upper band',\n        mode = 'lines',\n        x = list(pred['ds']),\n        y = list(pred['yhat_upper']),\n        line= dict(color='#57b88f'),\n        fill = 'tonexty'\n)\nlower_band = go.Scatter(\n    name= 'lower band',\n        mode = 'lines',\n        x = list(pred['ds']),\n        y = list(pred['yhat_lower']),\n        line= dict(color='#1705ff')\n)\ndata = [trace, trace1, lower_band, upper_band]\nlayout = dict(title='Thread CPU time Estimation',\n             xaxis=dict(title = 'Dates', ticklen=2, zeroline=False))\nfigure=dict(data=data,layout=layout)","03d551b6":"py.offline.iplot(figure)","b9073e15":"Let's create a kernel density estimation (KDE) plot colored by the **Thread CPU time** of the **Anomaly**. A kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. It will allow us to identify if there is a correlation between the **Thread CPU time** and **Anomaly**.","2daa8fb9":"# Anomaly Detection - Thread CPU time, Memory space usage.","69d578de":"Thread CPU time : (MXBean(java.lang:type=Threading).CurrentThreadCpuTime)","6bfbc946":"Plots\/ Threashold in 75%  Quartil","214dd515":"### Visualizations","cdcedf68":"Plot \/ Threashold in 75% Quartil","89b13065":"### df LUMINOL dataset2","b5d16ec8":"# CPU and Memory use correlation","b672f1c4":"### if there is anomaly, correlate the first anomaly period with a secondary time series ts2.","b8b76f84":"Heap usage activity : (d\/dx (MXBean(java.lang:type=Memory).HeapMemoryUsage.used))","4ad69f19":"# Plots\/ Threashold in 75%  Quartil","b7e491fa":"### score example for the ts1","0035ee7a":"Memory space usage","cc2b73a2":"# Introduction\nAnomaly detection has applications in many fields, such as system health monitoring, fraud detection, and intrusion detection.\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F3595464%2F4088133a20318f4e47e1e2d738509d12%2F__results___5_0.png?generation=1590869249365044&alt=media)\n","7fff9a6d":"### Detecting Anomalies:\n- The light blue boundaries in the above graph are yhat_upper and yhat_lower.\n- If y value is greater than yhat_upper and less than yhat lower then it is an anomaly.\n- Also getting the importance of that anomaly based on its distance from yhat_upper and yhat_lower.","f7c8d5c2":"### df LUMINOL TESTING dataset1","3bfa4bf0":"# Remember the upvote button is next to the fork button, and it's free too! ;)\n\n### Don't hesitate to give your suggestions in the comment section","77ecb3af":"### df LUMINOL dataset4","e124e660":"### Raw Operational Data from Enterprise Application\nA dataset for anomaly detection in operations of distributed software systems\n\n### Software Operational Data, Processed and Labeled\nA dataset for anomaly detection in operations of distributed software systems","5fd8f5c8":"# Plots\/ Threashold in 75%  Quartil","ebc31b0e":"Anomaly value counts","f0119768":"# Propeth Anomaly Detection\n\n> see this: (Anomaly detection in time series with Prophet library)[https:\/\/towardsdatascience.com\/anomaly-detection-time-series-4c661f6f165f]","ce99648c":"Propeth Prediction","5a007b88":"- dataset 1, the Gaussian random variable have mean 0 and variance = 1 and we let a1 =0,6 and a2 = 0,5. This data\nset consists of 10,000 records. Change points occur at time * 1000 (x=1,2,3...,9).\n\n- The second data set is a data sequence such that each data between change points was drawn according to the AR model ((6), a1 = 0,6 and a2 = 0,5) in which variance\nof the noise term changes gradually over time.\n\n- The third data set is a data sequence such that at each change point, variance suddenly changes, and each data in\na range between the change points is i.i.d. (independently identically distributed).\n\n","4c743215":"Memory space usage : ((MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.committed \/ MXBean(java.lang:name=Code Cache,type=MemoryPool).Usage.max))","f240dc28":"### df LUMINOL Memory Space Usage \n","0c93e847":"### df LUMINOL dataset 3","027ed195":"### df LUMINOL dataset5","86748d64":"# Linkedin luminol\nThere is a great deal of literature on anomaly detection out in the world \u2014 from open-source packages like Twitter\u2019s [AnomalyDetection](https:\/\/github.com\/twitter\/AnomalyDetection?source=post_page---------------------------) or Linkedin\u2019s [Luminol](https:\/\/github.com\/linkedin\/luminol\/?source=post_page---------------------------), to academic works like Rob Hyndman\u2019s papers on [feature-based anomaly detection](https:\/\/robjhyndman.com\/publications\/oddstream\/?source=post_page---------------------------). \n\nLuminol is a lightweight Python library for time series data analysis. The two major functionalities it supports are anomaly detection and correlation. It can be used to investigate possible causes of anomaly.\nGithub link: https:\/\/github.com\/linkedin\/luminol\nLicense: Apache License 2.0\n\n","03f74ce5":"## End Notebook","9fdc832c":"### Test in the dataset -  Read in data.","9e81f0bf":"# Glipse Data","87bbe597":"Thread CPU time","fe05d329":"### if there is anomaly, correlate the first anomaly period with a secondary time series ts2.\n","d685ed3c":"# KDE - Memory space usage","4b49e47d":"# Generic Data \n> We prepared three kinds of data sets: 1) jumping mean with constant variance, 2) jumping mean with varying variance,\nand 3) jumping variance with constant mean.","2ab52c2c":"Detect Anomalies","2b4c1b07":"Plots\/ Threashold in 75%  Quartil","f8db978e":"# Luminol \n> Detecting Outliers and Change Points from Time Series\n","e473d8c3":"### print the correlation coefficient","61fd1aea":"### conduct anomaly detection on a single time series ts.","0d6dc96f":"### Import Libs","13ba7f2b":"Thread CPU time","6425ded1":"Plots\/ Threashold in 75%  Quartil","773db060":"\n### df LUMINOL Memory Space Usage \n","e3c11c17":"Let's create a kernel density estimation (KDE) plot colored by the **Memory space usage** of the **Anomaly**. A kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. It will allow us to identify if there is a correlation between the **Memory space usage** and **Anomaly**.","53dd7a21":"Memory space usage","e7c2379e":"Plots\/ Threashold in 75%  Quartil","57157d34":"# KDE - Thread CPU time","1fca9054":"Thread CPU time results"}}