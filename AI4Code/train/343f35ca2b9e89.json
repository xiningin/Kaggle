{"cell_type":{"5f0e655c":"code","e15d41b6":"code","669c5d6a":"code","7b2b4e61":"code","79ad6919":"code","7a3e7474":"code","92b3bc54":"code","4de510ca":"code","a789f42f":"code","6f05fdac":"code","7f3333ab":"code","73f7d77d":"code","600ab28b":"code","180805d8":"code","0ee8f356":"code","23e3664e":"code","73ee88cc":"code","865cad51":"code","30d28ca1":"code","f2c7e717":"code","b64a21d5":"code","55ad4a59":"code","0b16b810":"markdown","beb96415":"markdown","6f6640e8":"markdown","f1e97112":"markdown","c0409dc2":"markdown","268fc195":"markdown","c9808875":"markdown","1e547cab":"markdown","9468967d":"markdown","f1f8ea98":"markdown","31a2bb03":"markdown","8522caa0":"markdown","2ef3f885":"markdown","46249642":"markdown","676523aa":"markdown","546f50fa":"markdown","4b871ed1":"markdown"},"source":{"5f0e655c":"import numpy as np # We will be using this for all of the matrix\/tensor operations\nimport pandas as pd # In this notebook, only used for reading\/writing csv files\nimport matplotlib.pyplot as plt # Maybe use this to plot loss function\nimport os","e15d41b6":"data_csv = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest_csv = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\n\nprint(\"data_csv\", data_csv.shape)\nprint(\"test_csv\", test_csv.shape)","669c5d6a":"data = data_csv.values.T \ntest = test_csv.values.T\n\ntrain = data[1:, 2000:]\ndev = data[1:, :2000]\nlabel_train_numeric = data[[0], 2000:]\nlabel_dev_numeric = data[[0], :2000]\n\nprint(\"train\", train.shape)\nprint(\"dev\", dev.shape)\nprint(\"test\", test.shape)\nprint(\"label_train_numeric\", label_train_numeric.shape)\nprint(\"label_dev_numeric\", label_dev_numeric.shape)","7b2b4e61":"label_train_onehot = np.zeros((10, label_train_numeric.shape[1]))    # create 10x40000 zero vector\nfor i in range(label_train_numeric.shape[1]):                        # for each label,\n    label_train_onehot[label_train_numeric[0][i]][i] = 1             # one-hot encoding\n\nlabel_dev_onehot = np.zeros((10, label_dev_numeric.shape[1]))        # same as above\nfor i in range(label_dev_numeric.shape[1]):\n    label_dev_onehot[label_dev_numeric[0][i]][i] = 1\n    \n\ntrain = np.divide(train, 255)\ndev = np.divide(dev, 255)\n\nprint('label_train_numeric', label_train_numeric.shape)\nprint('label_dev_numeric', label_dev_numeric.shape)\nprint('label_train_onehot', label_train_onehot.shape)\nprint('label_dev_onehot', label_dev_onehot.shape)","79ad6919":"train_img = train.T.reshape(-1,28,28)\nplt.imshow(train_img[3000])","7a3e7474":"print(label_train_numeric[0,3000])\nprint(label_train_onehot[:,3000])","92b3bc54":"def sigmoid(matrix):\n    s = np.clip( matrix, -500, 500 )\n    s = 1 \/ (1 + np.exp(-matrix))\n    return s\n\ndef relu(matrix):\n    matrix = np.clip(matrix, -500, 500)\n    matrix = matrix * (matrix > 0)\n    return matrix\n\ndef softmax(z):\n    softmax_matrix = np.exp(z) \/ np.sum(np.exp(z), axis = 0, keepdims=True)\n    return softmax_matrix","4de510ca":"a = np.array([[5, 5],[2, 2],[-1,-1],[3,3]])\nprint('input\\n', a)\nprint('sigmoid\\n',sigmoid(a))\nprint('relu\\n',relu(a))\nprint('softmax\\n',softmax(a))","a789f42f":"def zero_init(dim):\n    zero_vector = np.zeros((dim, 1))\n    return zero_vector\n\ndef he_init(layer_dims):\n    parameters = {}\n    L = len(layer_dims) \n    for i in range(1, L):\n        parameters['w' + str(i)] = np.random.randn(layer_dims[i], layer_dims[i - 1]) * np.sqrt(2 \/ layer_dims[i - 1])\n        parameters['b' + str(i)] = zero_init(layer_dims[i])\n    return parameters\n\ndef rand_init(layer_dims):\n    parameters = {}\n    L = len(layer_dims) \n    for i in range(1, L):\n        parameters['w' + str(i)] = np.random.randn(layer_dims[i], layer_dims[i - 1]) * 0.01\n        parameters['b' + str(i)] = zero_init(layer_dims[i])\n    return parameters\n\ndef init_momentum(layer_dims):\n    momentum = {}\n    L = len(layer_dims)\n    for i in range(1, L):\n        momentum['dw' + str(i)] = np.zeros((layer_dims[i], layer_dims[i - 1]))\n        momentum['db' + str(i)] = np.zeros((layer_dims[i], 1))\n    return momentum","6f05fdac":"# w: weights b: bias a: activation from previous layer\ndef forward_propagation(w, b, a, activation):\n    z = np.dot(w, a) + b\n\n    if activation == 'sigmoid':\n        a = sigmoid(z)\n    elif activation == 'relu':\n        a = relu(z)\n    elif activation == 'softmax': # z.shape = (10, 40000)\n        a = softmax(z)\n    return z, a\n\n\ndef forward_propagation_full(parameters, layer_dim, x):\n    cache = {'a' + str(0): x}\n    L = len(layer_dim)  # L = 3\n    for i in range(1, L):  # looping through each layer\n        w = parameters['w' + str(i)]\n        b = parameters['b' + str(i)]\n        if i == L - 1:  # use softmax activation for the final layer\n            cache['z' + str(i)], cache['a' + str(i)] = forward_propagation(w, b, cache['a' + str(i - 1)], 'softmax')\n        else:  # use relu for the other layers.\n            cache['z' + str(i)], cache['a' + str(i)] = forward_propagation(w, b, cache['a' + str(i - 1)], 'sigmoid')\n\n    aL = cache['a' + str(L - 1)]  # aL : final activation. returned value of softmax\n    return aL, cache","7f3333ab":"layer_dims = [784, 64, 10]\nparameters = he_init(layer_dims)\naL, cache = forward_propagation_full(parameters, layer_dims, train)\nprint(parameters.keys())\nprint(cache.keys())\nprint('aL.shape', aL.shape)\nprint(aL[:,0:4])","73f7d77d":"# cost function for binary classification. not used in this model\n\"\"\"\ndef cost_function(aL, y):  # aL : activation from final layer, y : labels\n    cost = np.zeros((aL.shape[0], 1))\n    for i in range(y.shape[0]):\n        cost[i][0] = -(np.dot(y[i][:], np.log(aL[i][:]).T) + np.dot((1 - y[i][:]), np.log(1 - aL[i][:]).T)) \/ y.shape[1]\n    cost = np.linalg.norm(cost, ord=2)\n    return cost\n\"\"\"\ndef softmax_cost(aL, y):\n    loss = np.sum(np.multiply(y , np.log(aL)), axis=0, keepdims=True)\n    cost = -(1 \/ y.shape[1]) * np.sum(loss, axis=1)\n    return cost","600ab28b":"# da : gradient of a from the next layer\ndef backward_propagation(da, a_prev, z, w, b, activation_method):  \n    if activation_method == 'sigmoid':\n        s = sigmoid(z)\n        dz = da * s * (1 - s)\n\n    elif activation_method == 'relu':\n        dz = np.array(da, copy=True)\n        dz[z <= 0] = 0\n\n\n    elif activation_method == 'softmax':\n        s = softmax(z)\n        dz = s - da\n\n    m = a_prev.shape[1]\n    dw = np.dot(dz, a_prev.T) \/ m\n    da_prev = np.dot(w.T, dz)\n    db = np.sum(dz, axis=1, keepdims=True) \/ m\n    return da_prev, dw, db\n\n\ndef backward_propagation_full(x, y, parameters, cache):\n    grads = {}\n    L = int(len(parameters) \/ 2) # L = 5\n\n    grads['da' + str(L)] = y\n\n    flag = False  # apply gradient descent for softmax only for the first case\n    for i in reversed(range(1, L + 1)):\n        if not flag:\n            activation_method = 'softmax'\n            flag = True\n        else:\n            activation_method = 'sigmoid'\n\n        da_prev, dw, db = backward_propagation(grads['da' + str(i)], cache['a' + str(i - 1)], cache['z' + str(i)],\n                                               parameters['w' + str(i)], parameters['b' + str(i)], activation_method)\n        grads['dw' + str(i)] = dw\n        grads['db' + str(i)] = db\n        grads['da' + str(i - 1)] = da_prev\n        \n    return grads","180805d8":"grads = backward_propagation_full(train, label_train_onehot, parameters, cache)\nprint(grads.keys())","0ee8f356":"def update_parameter(parameters, learning_rate, layer_dims, grads):\n    L = len(layer_dims)\n    for i in range(1, L):\n        parameters['w' + str(i)] = parameters['w' + str(i)] - learning_rate * grads['dw' + str(i)]\n        parameters['b' + str(i)] = parameters['b' + str(i)] - learning_rate * grads['db' + str(i)]\n    return parameters\n\n\ndef update_momentum(momentum, grads, beta1, layer_dims):\n    L = len(layer_dims)\n    for i in range(1, L):\n        momentum['dw' + str(i)] = (beta1 * momentum['dw' + str(i)]) + ((1 - beta1) * grads['dw' + str(i)])\n        momentum['db' + str(i)] = (beta1 * momentum['db' + str(i)]) + ((1 - beta1) * grads['db' + str(i)])\n    return momentum\n\n\ndef update_rms(rms, grads, beta2, layer_dims):\n    L = len(layer_dims)\n    for i in range(1, L):\n        rms['dw' + str(i)] = (beta2 * rms['dw' + str(i)]) + ((1 - beta2) * grads['dw' + str(i)]) ** 2\n        rms['db' + str(i)] = (beta2 * rms['db' + str(i)]) + ((1 - beta2) * grads['db' + str(i)]) ** 2\n    return rms","23e3664e":"def init_minibatch(x, y, minibatch_size):\n    minibatches = []\n    num_mini = y.shape[1] \/\/ minibatch_size\n    \n    for i in range(num_mini):\n        mini_x = x[:, i * minibatch_size : (i + 1) * minibatch_size]\n        mini_y = y[:, i * minibatch_size : (i + 1) * minibatch_size]\n        minibatches.append((mini_x, mini_y))\n\n    if y.shape[1] % minibatch_size != 0:\n        mini_x = x[:, num_mini * minibatch_size:]\n        mini_y = y[:, num_mini * minibatch_size:]\n        minibatches.append((mini_x, mini_y))\n\n    return minibatches","73ee88cc":"print(\"Input Shape\")\nprint(train.shape)\nprint(label_train_onehot.shape)\n\nprint(\"\\nOutput Shape\")\ntest_minibatch = init_minibatch(train, label_train_onehot, 15000)\nfor x,y in test_minibatch:\n    print(\"x\", x.shape)\n    print(\"y\", y.shape)","865cad51":"def model(x, y, layer_dims, num_iter=10, learning_rate=0.0001, minibatch_size=40000, \n          optimizer='gradient_descent', beta1=0.9, beta2=0.9, epsilon=1e-8, show_cost=100):\n    parameters = he_init(layer_dims)\n    momentum = init_momentum(layer_dims)\n    rms = init_momentum(layer_dims)\n    costs = []\n\n    counter = 0\n    for iter in range(num_iter):\n        counter += 1\n        # create random minibatches\n        minibatches = init_minibatch(x, y, minibatch_size)\n        for minibatch in minibatches:\n            mini_x, mini_y = minibatch\n            aL, cache = forward_propagation_full(parameters, layer_dims, mini_x)\n\n            # compute the cost and the gradient\n            cost = softmax_cost(aL, mini_y)\n            grads = backward_propagation_full(mini_x, mini_y, parameters, cache)\n\n            # updating parameters\n            if optimizer == 'momentum':\n                momentum = update_momentum(momentum, grads, beta1, layer_dims)\n                parameters = update_parameter(parameters, learning_rate, layer_dims, momentum)\n\n            elif optimizer == 'gradient_descent':\n                parameters = update_parameter(parameters, learning_rate, layer_dims, grads)\n\n            elif optimizer == 'rms':\n                rms = update_rms(rms, grads, beta2, layer_dims)\n                parameters = update_parameter(parameters, learning_rate, layer_dims, grads, rms, epsilon)\n\n            elif optimizer == 'adam':\n                momentum = update_momentum(momentum, grads, beta1, layer_dims)\n                rms = update_rms(rms, grads, beta2, layer_dims)\n                parameters = update_parameter(parameters, learning_rate, layer_dims, momentum, rms, epsilon)\n        \n        costs.append(cost)\n        # show the cost every 10 iterations    \n        if counter % show_cost == 0:\n            print('Cost After Epoch {} : '.format(iter+1), costs[-1])\n                    \n\n    # plot how my cost is changing\n    plt.plot(costs)\n    plt.ylabel('Cost')\n    plt.xlabel('Epochs')\n    plt.title(\"Learning Rate = \" + str(learning_rate))\n    plt.show()\n\n    return parameters\n\n\ndef predict(parameters, layer_dim, input_data):\n    aL, cache = forward_propagation_full(parameters, layer_dim, input_data)\n    prediction = aL.argmax(0)\n    return prediction","30d28ca1":"def simple_model(x, y, layer_dims, num_iter=10, learning_rate=0.1):\n    parameters = he_init(layer_dims)\n    costs = []\n    \n    for iteration in range(num_iter):\n        aL, cache = forward_propagation_full(parameters, layer_dims, x)\n        cost = softmax_cost(aL, y)\n        costs.append(cost)\n        grads = backward_propagation_full(x, y, parameters, cache)\n        parameters = update_parameter(parameters, learning_rate, layer_dims, grads)\n        print('Epoch',iteration,'cost:', costs[-1])\n    \n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('epochs')\n    plt.title(\"Learning rate = \" + str(learning_rate))\n    plt.show()    \n    return parameters","f2c7e717":"layer_dims1 = [784, 64, 10]\nparam1 = model(train, label_train_onehot, layer_dims1, num_iter=100, \n               learning_rate=0.75, minibatch_size=256, show_cost=10)","b64a21d5":"# predict train set\npred_train = predict(param1, layer_dims1, train)\nimage_id = range(1, pred_train.shape[0] + 1)\nlabel_train = label_train_numeric.reshape(40000,)\npred_df = pd.DataFrame({'ImageId': image_id, \"Prediction\": pred_train, \"Label\": label_train})\npred_df['match'] = np.where(pred_df['Prediction']==pred_df['Label'],True,False)\n\naccuracy = pred_df[pred_df.match==True].shape[0] \/ pred_df.shape[0]\nprint(\"For training set, we got\",pred_df[pred_df.match==True].shape[0],\"out of\",pred_df.shape[0])\nprint(\"accuracy\", accuracy)\n\n# predict dev set\npred_dev = predict(param1, layer_dims1, dev)\nimage_id = range(1, pred_dev.shape[0] + 1)\nlabel_dev = label_dev_numeric.reshape(2000,)\npred_df = pd.DataFrame({'ImageId': image_id, \"Prediction\": pred_dev, \"Label\": label_dev})\npred_df['match'] = np.where(pred_df['Prediction']==pred_df['Label'],True,False)\n\naccuracy = pred_df[pred_df.match==True].shape[0] \/ pred_df.shape[0]\nprint(\"For development set, we got\",pred_df[pred_df.match==True].shape[0],\"out of\",pred_df.shape[0])\nprint(\"accuracy\", accuracy)","55ad4a59":"pred_test = predict(param1, layer_dims1, test)\nimage_id = range(1, pred_test.shape[0] + 1)\nsubmission = pd.DataFrame({'ImageId': image_id, \"Label\": pred_test})\nsubmission.to_csv('digit_recognizer_submission.csv', index=False)","0b16b810":"# INITIALIZE WEIGHTS AND BIASES\nWeights and biases will be stored in a dictionary called `parameters` with keys `w1, w2, w3, ..` and `b1, b2, b3, ..`\n\nInput, `layer_dims`, will take form of `[748, ..., 10]`. 748 for the number of pixels, and 10 for number of classes.\n\nIf first hidden layer has 64 nodes, `w1` will have dimension `(64, 784)` and `b1` will have dimension`(64,1)`","beb96415":"# ACTIVATION FUNCTIONS\n\nI defined sigmoid, relu, and softmax below. For this model, relu function is not needed.","6f6640e8":"# COST FUNCTION","f1e97112":"This is a simplified model I made with just the essentials for debugging purposes","c0409dc2":"Create one-hot encoded version of the label in order to calculate loss with softmax\n\nThen scalie pixel values to range [0,1] by dividing the pixel values by 255","268fc195":"# MODELING","c9808875":"# CREATE PREDICTION\n\nfirst, make a prediction on dev set to see how we are doing","1e547cab":"First, turn the data's datatype from pd.Dataframe to np.array since it is easier to perform numpy operations on ndarray datatype than pd.Dataframe. \n\nHere, `train` and `dev` set are matrices with 784 rows which are pixels, and each columns are pictures.\n\n`label_train_numeric` and `label_dev_numeric` are 1-dimentional vector containing all the labels for the training sets.","9468967d":"# BACKWARD PROPAGATION","f1f8ea98":"# UPDATE PARAMETERS\n\nw := w - learning rate * gradient","31a2bb03":"# Sanity Check","8522caa0":"# INTRODUCTION\nThis notebook was inspired by coursera **deeplearning.ai course**. Here, I will create a softmax regression model for MNIST handwritten digit dataset. Both forward and backward propagation will be done from scratch.","2ef3f885":"Finally, I can train my model. Since this is a relatively simple dataset, I only used one hidden layer of 64 nodes. I used mini batch size of 256, and gradient descent looped around the whole dataset 100 times. ","46249642":"# ALL DONE\nLet's turn in the work!!","676523aa":"# INIT MINIBATCH","546f50fa":"# FORWARD PROPAGATION\nSince this is a rather simple dataset, dimension of the layers we are going to use is going to be `layer_dims = [784, 64, 10]`\n\nThroughout forward propagation process, we will be storing **activation** values and **z** values in `cache` for backward propagation step.\n\n`a0` is the input layer with **784** nodes (pixels of 28X28 picture)\n`aL` is the output layer with **10** nodes (10 digits)","4b871ed1":"# PREPARING THE DATA\n\nRead the dataset as Pandas Dataframe from Kaggle website"}}