{"cell_type":{"f12dddbe":"code","f84ceaf0":"code","49584150":"code","cd4baf0f":"code","7b693014":"code","1a876eb1":"code","f960c3ff":"code","a9aba80b":"code","870eda4d":"code","c8830136":"code","007dddd7":"code","da1152d7":"code","f8bc7223":"code","31ae3704":"code","4a4b9c97":"code","8f7f0ae1":"code","c6679508":"code","0f633a57":"code","a3abf82b":"code","361c1b3d":"code","f8295ff5":"code","88ed93e0":"code","cb2fcc02":"code","d2aed758":"code","1e768f77":"code","1446c3c7":"code","39e7f5e0":"code","95471b00":"code","df53c903":"code","a570c6d0":"code","fbb9f448":"code","1709e5db":"code","1d1b83ab":"code","6172510f":"code","8c3bec2e":"code","a2eb9374":"code","ea2edf2b":"code","70db4fc4":"code","aaf6f8c6":"code","a4901bae":"code","f9d306c7":"code","2579013e":"code","2cfe5eca":"code","a7da9f6d":"code","4e84339c":"code","24c30a82":"code","86a5e723":"code","9f7da714":"code","653ca4c8":"code","e42c6bde":"code","1019af6e":"code","f7f7a903":"code","439b68fb":"code","bcd231be":"code","1040acab":"code","f733217d":"code","93f07f4d":"code","999cdc1d":"code","07c3b291":"code","d6f2da12":"code","abcba413":"markdown","bd5a322e":"markdown","061d1455":"markdown","062037ef":"markdown","34f0047a":"markdown","d2474e7e":"markdown","4021509b":"markdown","f09fa398":"markdown","dccad04d":"markdown","b7535ccc":"markdown","e3fd3e40":"markdown","3007011e":"markdown","9e68a4d1":"markdown","73f00191":"markdown","7cfbbddd":"markdown","b63d364a":"markdown","0238ff34":"markdown","0fdcbf1a":"markdown","410b93a0":"markdown","9f7055ff":"markdown","a9c30273":"markdown","1531a4ad":"markdown","521969cf":"markdown","3ef63f14":"markdown","047dfc98":"markdown"},"source":{"f12dddbe":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns ","f84ceaf0":"train = pd.read_csv('..\/input\/av-hacklive-guided-hackathon\/train.csv')\ntest = pd.read_csv('..\/input\/av-hacklive-guided-hackathon\/test.csv')\ntrain.drop('id', axis=1, inplace=True)\ntest.drop('id', axis=1, inplace=True)","49584150":"print('Total records in train is', train.shape[0], ', and in test is '+ str(test.shape[0])+ '.')","cd4baf0f":"train.isnull().sum()","7b693014":"from sklearn.ensemble import RandomForestClassifier\nX = train.copy()\n\n\ny = train.term_deposit_subscribed\nX = X.drop('term_deposit_subscribed', axis=1)\n\n\nX = X.fillna(-999)\n\n\nfor c in train.columns[train.dtypes == 'object']:\n    X[c] = X[c].factorize()[0]\n    \nrf = RandomForestClassifier()\nrf.fit(X,y)\n\nplt.plot(rf.feature_importances_)\nplt.ylabel('Importance of Feature')\nplt.xticks(np.arange(X.shape[1]), X.columns.tolist(), rotation=90);","1a876eb1":"corr=abs(train.corr())\ncore = corr.term_deposit_subscribed.sort_values(ascending=False)\ncore","f960c3ff":"train.groupby('prev_campaign_outcome')['term_deposit_subscribed'].value_counts()","a9aba80b":"train.groupby('marital')['term_deposit_subscribed'].value_counts()","870eda4d":"train.groupby('job_type')['term_deposit_subscribed'].value_counts()","c8830136":"(train.balance<0).value_counts()","007dddd7":"print(\"Subsctiption breakdown of poeple with negative balance\\n\", \n      train.loc[train.balance<0, 'term_deposit_subscribed'].value_counts(), '\\n')\n\nprint(\"Subsctiption breakdown of poeple with positive balance\\n\", \n      train.loc[train.balance>0, 'term_deposit_subscribed'].value_counts())","da1152d7":"print(\"Subsctiption breakdown of poeple with balancemore than 1000\\n\", \n      train.loc[train.balance>1000, 'term_deposit_subscribed'].value_counts())\n\nprint(\"Subsctiption breakdown of poeple with balancemore than 2000\\n\", \n      train.loc[train.balance>2000, 'term_deposit_subscribed'].value_counts())","f8bc7223":"months = train.month.unique().tolist()\nfor m in months:\n    print(m,'\\t')\n    print(train.loc[train.month==m, 'term_deposit_subscribed'].value_counts(), '\\t')","31ae3704":"train.groupby('communication_type').term_deposit_subscribed.value_counts()","4a4b9c97":"train.groupby('month').communication_type.value_counts()","8f7f0ae1":"plt.figure(figsize=(10,5))\nsns.kdeplot(data=train, x=train.loc[train.last_contact_duration<1200, 'last_contact_duration'], hue='term_deposit_subscribed', multiple = 'fill')\nplt.xlabel('Time in seconds')","c6679508":"plt.figure(figsize=(15,4))\nplt.tight_layout()\n\nplt.subplot(1,3,1)\nplt.hist((train.balance, test.balance), range=(-10000,30000), bins = 10, log = 1)\nplt.title('Balance Distribution')\nplt.xlabel('Balance')\n\nplt.subplot(1,3,2)\nplt.hist((train.customer_age, test.customer_age), bins = 10)\nplt.title('Age Distribution')\nplt.xlabel('Age')\n\nplt.subplot(1,3,3)\nplt.hist((train.month, test.month), bins = 10)\nplt.title('Month Distribution')\nplt.xlabel('Month')\n\nplt.show()","0f633a57":"plt.figure(figsize=(13,4))\nplt.tight_layout()\n\nplt.subplot(1,3,1)\nplt.hist((train.days_since_prev_campaign_contact, test.days_since_prev_campaign_contact), bins = 10)\nplt.title('Days since previous contact distribution')\nplt.xlabel('Days')\n\nplt.subplot(1,3,2)\nplt.hist((train.num_contacts_prev_campaign, test.num_contacts_prev_campaign), bins = 15, log=1)\nplt.title('Number of contacts distribution')\nplt.xlabel('Count')\n\nplt.subplot(1,3,3)\nplt.hist((train.last_contact_duration, test.last_contact_duration), bins = 10, log=1)\nplt.title('Call Duration distribution')\nplt.xlabel('Time in Seconds')\n\nplt.tight_layout()\n\nplt.show()","a3abf82b":"#Checking the distribution between train and test data. This will help us tune the model to get highest possible F1 score\ncolumns = test.columns.to_list()\ncolumns.remove('day_of_month')\ncolumns.remove('month')\ncolumns.remove('balance')\ncolumns.remove('customer_age')\ncolumns.remove('days_since_prev_campaign_contact')\ncolumns.remove('num_contacts_prev_campaign')\ncolumns.remove('last_contact_duration')\nfor c in columns:\n    print(train[c].value_counts().sort_index(),\"\\n\", test[c].value_counts().sort_index(), \"\\n\") ","361c1b3d":"corr = abs(train.corr())\ncore = corr.balance.sort_values(ascending = False)\ncore","f8295ff5":"print(\"Rows where both customer age and account balance are not avaiable in : \",len(train.loc[(train.balance.isnull()==True) & (train.customer_age.isnull()==True)]), \"\\nWe will drop these rows.\")\n#Dropping specified rows \ntrain = train.loc[(train.balance.isnull()!=True) | (train.customer_age.isnull()!=True)]\ntrain.reset_index(drop=True, inplace =True)\nprint('')","88ed93e0":"balances = [-5000, 0, 5000, 10000, 15000, 20000, 25000, 30000, 35000, 40000, 45000, 50000, 60000, 70000, 80000, 90000, 100000, 110000]\nfor b in balances : \n    m1 = train.customer_age.isnull() == True\n    m2 = (train.balance<(b+7500))\n    m3 = (train.balance>(b-7500)) \n    value = round(train.loc[m2 & m3, 'customer_age'].mean(),0) \n    i = train.loc[m1 & m2 & m3,'customer_age'].index\n    train.loc[i,'customer_age'] = value\ndel m1, m2, m3, i \n\n\nfor b in balances : \n    m1 = test.customer_age.isnull() == True\n    m2 = (test.balance<(b+7500))\n    m3 = (test.balance>(b-7500))\n    value = round(test.loc[m2 & m3, 'customer_age'].mean(),0) \n    i = test.loc[m1 & m2 & m3,'customer_age'].index\n    test.loc[i,'customer_age'] = value\ndel m1, m2, m3, i, balances, b","cb2fcc02":"ages = train.customer_age.unique().tolist()\nfor age in ages:\n    value = train.loc[(train.customer_age == age), ['balance']].mean().get('balance')\n    m1 =  train.customer_age == age\n    m2 = train.balance.isnull()==True\n    i = train.loc[m2 & m1, 'balance'].index\n    train.loc[i, 'balance'] = value\ndel ages, m1, m2, i\n\n\n\nages = test.customer_age.unique().tolist()\nfor age in ages:\n    value = test.loc[(test.customer_age == age), ['balance']].mean().get('balance')\n    m1 =  test.customer_age == age\n    m2 = test.balance.isnull()==True\n    i = test.loc[m2 & m1, 'balance'].index\n    test.loc[i, 'balance'] = value\ndel ages, m1, m2, i","d2aed758":"i = test.loc[(test.balance.isnull()==True) & test.customer_age.isnull()==True].index\nmean_balance = test.balance.mean()\nmean_age = round(test.customer_age.mean(), 0)\ntest.loc[i, ['balance', 'customer_age']] = mean_balance, mean_age","1e768f77":"train['personal_loan'] = train['personal_loan'].replace({'no':0, 'yes':1})\ntest['personal_loan'] = test['personal_loan'].replace({'no':0, 'yes':1})\n\nfrom sklearn.impute import KNNImputer\nknn = KNNImputer(n_neighbors=10)\n\ncol_pl = ['balance', 'personal_loan']\nknn.fit(train[col_pl])\n\nr = pd.DataFrame(np.round(knn.transform(train[col_pl]), 0), columns=col_pl)\ntrain['personal_loan'] = r['personal_loan'].astype('int64')\n\nr = pd.DataFrame(np.round(knn.transform(test[col_pl]), 0), columns=col_pl)\ntest['personal_loan'] = r['personal_loan'].astype('int64')","1446c3c7":"train['housing_loan'] = train['housing_loan'].replace({'no':0, 'yes':1})\ntest['housing_loan'] = test['housing_loan'].replace({'no':0, 'yes':1})\n\ntrain['loan'] = (train.personal_loan | train.housing_loan)\ntrain['loan'] = train['loan'].astype('int64')\n\ntest['loan'] = (test.personal_loan | test.housing_loan)\ntest['loan'] = test['loan'].astype('int64')\n\n\ntrain.drop('personal_loan', axis=1, inplace=True)\ntest.drop('personal_loan', axis=1, inplace=True)\n\ntrain.drop('housing_loan', axis=1, inplace=True)\ntest.drop('housing_loan', axis=1, inplace=True)","39e7f5e0":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder","95471b00":"train.last_contact_duration.fillna(value = train.last_contact_duration.mean(), inplace= True)\ntest.last_contact_duration.fillna(value = test.last_contact_duration.mean(), inplace=True)","df53c903":"#Label encoding of education as the data is ordinal \ntrain['education'] = train['education'].replace({'unknown':0, 'primary':1, 'secondary':2, 'tertiary':3})\ntest['education'] = test['education'].replace({'unknown':0, 'primary':1, 'secondary':2, 'tertiary':3})","a570c6d0":"scaler = StandardScaler()\nscaler.fit(train.education.values.reshape(-1,1))\n\nnew_edu = scaler.transform(train.education.values.reshape(-1,1))\ntrain['education'] = new_edu\n\nnew_edu = scaler.transform(test.education.values.reshape(-1,1))\ntest['education'] = new_edu","fbb9f448":"train['last_contact_duration'] = np.clip(train.last_contact_duration,a_min=None, a_max=3000) \ntest['last_contact_duration'] = np.clip(test.last_contact_duration,a_min=None, a_max=3000) \n\nscaler = StandardScaler()\nscaler.fit(train.last_contact_duration.values.reshape(-1,1))\n\nnew_con = scaler.transform(train.last_contact_duration.values.reshape(-1,1))\ntrain['last_contact_duration'] = new_con\n\nnew_con = scaler.transform(test.last_contact_duration.values.reshape(-1,1))\ntest['last_contact_duration'] = new_con","1709e5db":"train['num_contacts_prev_campaign'] = np.clip(train.num_contacts_prev_campaign,a_min=None, a_max=26) \ntest['num_contacts_prev_campaign'] = np.clip(test.num_contacts_prev_campaign,a_min=None, a_max=26) \n\nscaler = StandardScaler()\nscaler.fit(train.num_contacts_prev_campaign.values.reshape(-1,1))\n\nnew_con = scaler.transform(train.num_contacts_prev_campaign.values.reshape(-1,1))\ntrain['num_contacts_prev_campaign'] = new_con\n\nnew_con = scaler.transform(test.num_contacts_prev_campaign.values.reshape(-1,1))\ntest['num_contacts_prev_campaign'] = new_con","1d1b83ab":"train['balance'] = np.clip(train.balance,a_min=None, a_max=40000) \ntest['balance'] = np.clip(train.balance,a_min=None, a_max=40000) \n\nscaler = StandardScaler()\nscaler.fit(train.balance.values.reshape(-1,1))\n\nnew_balance = scaler.transform(train.balance.values.reshape(-1,1))\ntrain['balance'] = new_balance\n\nnew_balance = scaler.transform(test.balance.values.reshape(-1,1))\ntest['balance'] = new_balance","6172510f":"scaler = StandardScaler()\nscaler.fit(train.customer_age.values.reshape(-1,1))\n\nnew_age = scaler.transform(train.customer_age.values.reshape(-1,1))\ntrain['customer_age'] = new_age\n\nnew_age = scaler.transform(test.customer_age.values.reshape(-1,1))\ntest['customer_age'] = new_age","8c3bec2e":"le = LabelEncoder()\nle.fit(train.month)\nnew_m = le.transform(train.month)\ntrain['month'] = new_m\n\nnew_m = le.transform(test.month)\ntest['month'] = new_m\n\nscaler = StandardScaler()\nscaler.fit(train.month.values.reshape(-1,1))\n\nnew_month = scaler.transform(train.month.values.reshape(-1,1))\ntrain['month'] = new_month\n\nnew_month = scaler.transform(test.month.values.reshape(-1,1))\ntest['month'] = new_month","a2eb9374":"le = LabelEncoder()\nle.fit(train.job_type)\nnew_m = le.transform(train.job_type)\ntrain['job_type'] = new_m\n\nnew_m = le.transform(test.job_type)\ntest['job_type'] = new_m\n\nscaler = StandardScaler()\nscaler.fit(train.job_type.values.reshape(-1,1))\n\nnew_jt = scaler.transform(train.job_type.values.reshape(-1,1))\ntrain['job_type'] = new_jt\n\nnew_jt = scaler.transform(test.job_type.values.reshape(-1,1))\ntest['job_type'] = new_jt","ea2edf2b":"scaler = StandardScaler()\nscaler.fit(train.day_of_month.values.reshape(-1,1))\n\nnew_day = scaler.transform(train.day_of_month.values.reshape(-1,1))\ntrain['day_of_month'] = new_day\n\nnew_day = scaler.transform(test.day_of_month.values.reshape(-1,1))\ntest['day_of_month'] = new_day","70db4fc4":"train['marital'] = train['marital'].replace({'single':3, 'married':1, 'divorced':2})\ntest['marital'] = test['marital'].replace({'single':3, 'married':1, 'divorced':2})","aaf6f8c6":"knn = KNNImputer(n_neighbors=10)\n\ncol_marital = ['customer_age', 'education', 'marital']\nknn.fit(train[col_marital])\n\nr = pd.DataFrame(np.round(knn.transform(train[col_marital]), 0), columns=col_marital)\ntrain['marital'] = r['marital']\n\nr = pd.DataFrame(np.round(knn.transform(test[col_marital]), 0), columns=col_marital)\ntest['marital'] = r['marital']","a4901bae":"train = train.join(pd.get_dummies(train.marital, prefix = 'marital'))\ntrain.drop('marital', axis=1, inplace = True)\n\ntest = test.join(pd.get_dummies(test.marital, prefix='marital'))\ntest.drop('marital', axis=1, inplace = True)","f9d306c7":"train = train.join(np.round(pd.get_dummies(train.default, prefix='default'), 0))\ntrain.drop('default', axis=1, inplace = True)\n\ntest = test.join(pd.get_dummies(test.default, prefix = 'default'))\ntest.drop('default', axis=1, inplace = True)","2579013e":"train = train.join(pd.get_dummies(train.communication_type, prefix='communication_type'))\ntrain.drop('communication_type', axis=1, inplace = True)\n\ntest = test.join(pd.get_dummies(test.communication_type, prefix = 'communication_type'))\ntest.drop('communication_type', axis=1, inplace = True)","2cfe5eca":"train = train.join(pd.get_dummies(train.prev_campaign_outcome, prefix='prev_campaign_outcome'))\ntrain.drop('prev_campaign_outcome', axis=1, inplace = True)\n\ntest = test.join(pd.get_dummies(test.prev_campaign_outcome, prefix = 'prev_campaign_outcome'))\ntest.drop('prev_campaign_outcome', axis=1, inplace = True)","a7da9f6d":"knn = KNNImputer(n_neighbors=10)\n\ncol_days = ['month', 'balance', 'customer_age', 'education', 'days_since_prev_campaign_contact']\nknn.fit(train[col_days])\n\nr = pd.DataFrame(knn.transform(train[col_days]), columns=col_days)\ntrain['days_since_prev_campaign_contact'] = r['days_since_prev_campaign_contact']\n\nr =  pd.DataFrame(knn.transform(test[col_days]), columns=col_days)\ntest['days_since_prev_campaign_contact'] = r['days_since_prev_campaign_contact']","4e84339c":"knn = KNNImputer(n_neighbors=10)\ncol_nums = ['day_of_month', 'num_contacts_in_campaign']\nknn.fit(train[col_nums])\nr =  pd.DataFrame(knn.transform(train[col_nums]), columns=col_nums)\ntrain['num_contacts_in_campaign'] = r['num_contacts_in_campaign']\n\n\nr =  pd.DataFrame(knn.transform(test[col_nums]), columns=col_nums)\ntest['num_contacts_in_campaign'] = r['num_contacts_in_campaign']","24c30a82":"train['num_contacts_in_campaign'] = np.clip(train.num_contacts_in_campaign,a_min=None, a_max=30) \ntest['num_contacts_in_campaign'] = np.clip(test.num_contacts_in_campaign,a_min=None, a_max=30) \n\nscaler = StandardScaler()\nscaler.fit(train.num_contacts_in_campaign.values.reshape(-1,1))\n\nnew_con = scaler.transform(train.num_contacts_in_campaign.values.reshape(-1,1))\ntrain['num_contacts_in_campaign'] = new_con\n\nnew_con = scaler.transform(test.num_contacts_in_campaign.values.reshape(-1,1))\ntest['num_contacts_in_campaign'] = new_con","86a5e723":"train['days_since_prev_campaign_contact'] = np.clip(train.days_since_prev_campaign_contact,a_min=None, a_max=600) \ntest['days_since_prev_campaign_contact'] = np.clip(test.days_since_prev_campaign_contact,a_min=None, a_max=600) \n\nscaler = StandardScaler()\nscaler.fit(train.days_since_prev_campaign_contact.values.reshape(-1,1))\n\nnew_con = scaler.transform(train.days_since_prev_campaign_contact.values.reshape(-1,1))\ntrain['days_since_prev_campaign_contact'] = new_con\n\nnew_con = scaler.transform(test.days_since_prev_campaign_contact.values.reshape(-1,1))\ntest['days_since_prev_campaign_contact'] = new_con","9f7da714":"print(train.isnull().sum(axis=0).sum(), test.isnull().sum(axis=0).sum())","653ca4c8":"y = train['term_deposit_subscribed'].values\nX = train.drop('term_deposit_subscribed', axis=1)\nx = X.values\nx.shape","e42c6bde":"print(train.term_deposit_subscribed.value_counts().to_list())\nprint(\"Ration of No to Yes is \" ,train.term_deposit_subscribed.value_counts()[1]\/train.term_deposit_subscribed.value_counts()[0])","1019af6e":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=14)\n\nfor train_index, test_index in sss.split(x, y):\n    xtrain, xval = x[train_index], x[test_index]\n    ytrain, yval = y[train_index], y[test_index]","f7f7a903":"import tensorflow as tf ","439b68fb":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(256, activation='relu', input_shape=(23,)),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(8, activation='relu'),\n    #tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(4, activation='relu'),\n    #tf.keras.layers.Dropout(0.4), \n    tf.keras.layers.Dense(1, activation='sigmoid')\n])","bcd231be":"model.summary()","1040acab":"from keras import backend as K\n\ndef f1(y_true, y_pred):\n    \n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    f1 = 2*(precision*recall)\/(precision+recall+K.epsilon())\n    return f1\n\n\nmodel.compile(loss='binary_crossentropy', optimizer= \"adam\", metrics=[f1,'AUC'])","f733217d":"annealer = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1, min_lr=1e-3)","93f07f4d":"model.fit(xtrain, ytrain, epochs=200, validation_data=(xval, yval), shuffle=True, callbacks=[annealer])","999cdc1d":"xtest = pd.read_csv('..\/input\/av-hacklive-guided-hackathon\/test.csv')\nsample = xtest['id']\nsample = pd.DataFrame(sample)","07c3b291":"p_y = model.predict(test)\np_y = p_y.flatten()\nprint(p_y)\np_y=np.round(p_y,0)\nprint(p_y)\nsample['term_deposit_subscribed'] = p_y","d6f2da12":"sample.to_csv('Predictions.csv', index=False)","abcba413":"## Further, subscription percentage rate gradually increases with increase in balance.\n","bd5a322e":"### For a Neural Network architecture, One-Hot encoding is more effective in recognizing patterns than Label\/Frequency Encoding. \n### Features are scaled about mean=0 and unit variance. Gradients in Neural Networks tend to explode or vanish on unscaled features.\n### Normalising help Neural Networks converge faster to the \"Saddle point\"","061d1455":"## If the last contact duration was high, the chances of the customer subscribing is high, as longer duration implies interest of the customer in services offered by the bank. ","062037ef":"## Customers who subscribed in the previous campaign are most likely to subscribe again. ","34f0047a":"## Month has a significant impact on the **reach** as well as the success of the campaign.\n","d2474e7e":"## Converting dataset to feed into Neural Network","4021509b":"# Data preprocessing \n## Working with Nan values","f09fa398":"## In-depth visualisations, analysis, insights and strong predictive models.\n\n## This dataset was provided in \u201cFactElytics\u201d organized by Drishti, the Annual fest of SIOM Nashik.\n## From a total of 330 participants, 87 teams were shortlisted after a quiz round held on Dare2Compete. Making it to the top 8 Teams, being the only individual participant, based on F1 score on a model for a banking institution\u2019s campaigning dataset.\n#### If this notebook helps you, an upvote would be huge!","dccad04d":"# -----------------------------------------------------------------------------------------------------------","b7535ccc":"## Cellular and telephone communication types are the most efficient with 13% success rate. \"Unknown\" has an efficiency of only 3.5%","e3fd3e40":"__________________________________________________________________________________________________________________________","3007011e":"## Single customers are more inclined towards taking up term deposit subscription.","9e68a4d1":"## Around 3900 bank accounts with balances less than 0, implying customers tried to make payments larger than the amount of money in thier account.","73f00191":"## The distribution of train and test appears to be the same. Thus, working on training set will eventually improve the test set. ","7cfbbddd":"## Customers with job types as management, student and unemployed have a higher chance of taking the subscription. ","b63d364a":"## Because of the skewed dataset (ratio of term deposit not subscribed to term deposit  subscribed is around 0.1), we will use Stratified shuffle split so the distribution of target values are the same in train and validation set. This is a  very significant when working with skewed data.","0238ff34":"## A very peculiar insight here is that \"unknown\" is very prominent on the month of May and June. For the remaning months, it has negligible contribution in terms of campaign reach.\n","0fdcbf1a":"## Few features are very dominant in the model, namely, Last contact duration, month, previous campaign outcome, day of month, balance and customer age. ","410b93a0":"## Only about 5 percent customers with negative bank balance tend to not shy away from subscribing for term deposit, , whereas for customers with positive balances have a subscription percentage of 11%. ","9f7055ff":"## Data scaling and imputing ","a9c30273":"## Building Neural Network model.","1531a4ad":"# -----------------------------------------------------------------------------------------------------------","521969cf":"## Balance and customer age have highest correlation. We will use either of these to fill NaN values of the other. This is more effective and gives higher accuracy than filling the NaN values of any feature with its Mean or Median. \n\n### NOTE : We are not considering the feature \"days_since_prev_campaign_contact\" as 80% NaN values in the dataset, hence the correlation value is not an accurate representation for this particular feature. ","3ef63f14":"# Comparing the distribution of test and training set.\n### If the distribution is different, improving our model in training set will not result in any imporvement in test set. ","047dfc98":"# Feature generation\n## Combining \"personal_loan\" and \"housing_loan\" to generate a new feature \"loan\""}}