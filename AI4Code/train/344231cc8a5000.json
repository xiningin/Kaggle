{"cell_type":{"9a9e94d5":"code","ae13b270":"code","48560bff":"code","148b94b9":"code","b2cac61f":"code","03104c3e":"code","e003d1f5":"code","577ee385":"code","8d318993":"code","6ac1a31e":"code","718d49b3":"code","e6eba055":"code","de5be85e":"code","4648e022":"code","0a7caad7":"code","2dec95fa":"code","3ead25ec":"code","c96aba17":"code","10761e24":"code","978927e2":"markdown"},"source":{"9a9e94d5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# Importing required libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport itertools\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.models import Sequential\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nimport matplotlib.pyplot as plt\n# \uc6b0\ub9ac\uac00 pandas\uc560\ub97c \ub123\uc744\uac70\uc57c(import) \uc774 \ucf54\ub4dc\uc5d0 \uc774\uac70\ub97c pd\ub77c\uace0 \ubd80\ub97c\uac70\uc57c(as)\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\n\n\nimport seaborn as sns\nfrom keras import models\nfrom keras import layers\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import make_column_transformer, make_column_selector\nfrom sklearn.model_selection import train_test_split\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ae13b270":"#\ubbf8\ub9ac \ub9cc\ub4e4\uc5b4\uc838\uc788\ub294 \ud6c8\ub828 \ub370\uc774\ud130\ub791 \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\ub97c \uac01\uac01 train\uacfc test\uac70\uc5d0\ub2e4\uac00 \ub123\uc5c8\uc5b4\uc694\ntrain = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")\n\n\n## Todo: deeper or wider NN, learning rate, epoch, dropout, batchnormalization, early-stopping \ub4f1\uc744 \ub123\uc5b4\uc11c \uac1c\uc120\ud574 \ubcf4\uc138\uc694","48560bff":"#\ucc98\uc74c \ub370\uc774\ud130\uc14b\uc744 \ub9de\ub294 \ub370\uc774\ud130 \ud0c0\uc785\uc5d0 \ub123\uae30 \uc704\ud55c \uacfc\uc815\n\n#train\uc5d0 \ub4e4\uc5b4\uc788\ub294\uac70\ub97c \uccab\ubc88\uc9f8 \uac70\ub97c \uc5c6\uc564\ub2e4: \ub2f5\uc778\uac70 \uac19\uc74c \u3147\u3145\u3147\nX = train.drop(\"label\", axis=1)\n\n#y label\ub9cc\uc744 \uac00\uc838\uc628\uac70 y\uac00 \ub2f5\uc9c0\ub2e4.\ny = train[\"label\"]","148b94b9":"# normalise the input \nX = X\/255.0\n\nx_train , x_test ,y_train , y_test = train_test_split(X,y,test_size = 0.2)\n\ninput_shape = [X.shape[1]]\nprint(\"Input shape: {}\".format(input_shape))","b2cac61f":"X","03104c3e":"x_train","e003d1f5":"#x_test\uc640 x_test\ub97c CNN\uc5d0 \ub9de\uac8c tensor type\uc73c\ub85c reshape\nre_x_train = x_train.values.reshape(-1, 28, 28, 1)\nre_x_test = x_test.values.reshape(-1, 28, 28, 1)","577ee385":"#x_test \uac00 reshape \ub418\uc5c8\ub294\uc9c0 \ud655\uc778\ud558\uae30 \uc704\ud574 print out\nre_x_train","8d318993":"#x_test \uac00 reshape \ub418\uc5c8\ub294\uc9c0 \ud655\uc778\ud558\uae30 \uc704\ud574 print out\nre_x_test","6ac1a31e":"model = tf.keras.Sequential([\n    #inpit layer, we expect to start out with the baseline of 784\n    #dropout: \ud55c\ubc88 \uc5f0\uc0b0\uc744 \ud574 \uc8fc\uace0 \ub098\uc11c \ub123\uc5b4\uc8fc\uba74 \uc88b\ub2e4.\ub808\uc774\uc5b4\ub97c \uac70\uce58\uace0 \ub098\uc11c \ub123\uc5b4\ub77c\n    Dropout(rate= 0.01),\n    \n    #One holistic Block: Conv2D + Dense\n    \n    ## conv2D layers \ucd94\uac00\n    Conv2D(filters = 64, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)),\n    MaxPool2D(pool_size=(2,2)),\n    Dropout(0.25),\n    \n    # add Deep Learning Layers \n    Dense(256,activation = 'relu'),\n    \n    ## 2nd holistic block\n    Conv2D(filters = 128, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'),\n    MaxPool2D(pool_size=(2,2)),\n    Dropout(0.25),\n    Dense(512,activation = 'relu'),\n    \n    ## conv2D layers \ucd94\uac00\n    Conv2D(filters = 256, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'),\n    MaxPool2D(pool_size=(2,2), strides=(2,2)),\n    layers.Dropout(0.25),\n    Flatten(),\n    \n    #Hidden Layer ends \n    Dense(1024,activation = 'relu'), \n    #BatchNormalization: input \ub4e4\uc5b4\uc624\uace0 \ub098\uc11c \ub123\uac70\ub098 layer \ub4e4\uc5b4\uac00\uace0 \ub098\uac04 \ud6c4\uc5d0 \ub123\uc5b4\uc900\ub2e4.\n    BatchNormalization(),\n\n    \n    #this is the output layer - we expect 10 different values\n    Dense(10,activation = 'softmax')\n])\n","718d49b3":"model.compile(optimizer=tf.keras.optimizers.Adam(\n    learning_rate=0.001,\n)\n              ,loss=\"sparse_categorical_crossentropy\",metrics=['accuracy'])","e6eba055":"early_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', min_delta=0, patience=30, verbose=0,\n    #patience\ub294 \"\ubb34\uc870\uac74 \uc774 iteration\uae4c\uc9c0\ub294 \ud574 \ubcf4\uc790\"\ud574\uc11c \uadf8 iteration \ud69f\uc218\ub97c \uc815\uc758\n    mode='auto', baseline=None, restore_best_weights=False\n)","de5be85e":"history = model.fit(\n    re_x_train,y_train,\n    \n    #callbacks argument\uac00 \uc788\uc5b4\uc11c \uadf8\uac78 early_stopping \uc744 \ud558\ub294\uc870\uac74\uc744 \uc704\uc5d0 \uc120\uc5b8\ud55c\uac78 \ub123\uc5b4\uc900\ub2e4.\n    callbacks = early_stopping,\n    epochs=50,\n    validation_data = (re_x_test,y_test)\n)","4648e022":"#history\ub97c history_df\uc5d0 \uc800\uc7a5\ud558\uace0 \uc774\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss', 'accuracy']].plot()","0a7caad7":"re_test = test.values.reshape(-1, 28, 28, 1)","2dec95fa":"re_test","3ead25ec":"#submit code\nsubmit = pd.DataFrame(np.argmax(model.predict(re_test), axis=1), columns=['Label'], \n                      index=pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')['ImageId'])","c96aba17":"submit.index.name = 'ImageId'\nsubmit.to_csv('submission.csv')\n\n#csv\ub97c \uc81c\ucd9c\ud558\ub358 notebook\uc744 \uc81c\ucd9c\ud558\ub358 \ud55c\ub2e4.","10761e24":"submit","978927e2":"# The end"}}