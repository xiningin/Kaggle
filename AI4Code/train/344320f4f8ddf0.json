{"cell_type":{"18348e6e":"code","5a570a91":"code","6989076b":"code","6a5f8885":"code","41a5560e":"code","058aa9e5":"code","dc35dec7":"code","4853975e":"code","d9a92b2d":"code","e2e4f2a8":"code","48f29b23":"code","e1fbff18":"code","855a3f0c":"code","a1dae644":"code","c97fd3d7":"code","f9037a02":"code","b01aa062":"code","d67548d2":"code","21860784":"code","780041db":"code","0127ceed":"code","5d493108":"code","52915c34":"code","9db5999c":"code","2d48b9ad":"code","3b3bff19":"code","d9a325df":"code","fb0af3a5":"code","268b3d0f":"code","ae2d36ec":"code","0519de95":"code","04559c00":"code","6e5a78d2":"code","af5b28f7":"code","85dce996":"markdown","d8db607c":"markdown","84171ea9":"markdown","18936dbf":"markdown","d4d372be":"markdown","a213a57a":"markdown","a9e3b8ea":"markdown","9efa03e8":"markdown","4adc15dc":"markdown","d6186d4f":"markdown","eb7da33d":"markdown","e2c5bff5":"markdown","c7fa7657":"markdown"},"source":{"18348e6e":"# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')","5a570a91":"dataset= pd.read_csv('..\/input\/titanic\/train.csv')","6989076b":"pwd","6a5f8885":"dataset.head()","41a5560e":"%matplotlib inline\nimport seaborn\nseaborn.set() \n\n#-------------------Survived\/Died by Class -------------------------------------\nsurvived_class = dataset[dataset['Survived']==1]['Pclass'].value_counts()\ndead_class = dataset[dataset['Survived']==0]['Pclass'].value_counts()\ndf_class = pd.DataFrame([survived_class,dead_class])\ndf_class.index = ['Survived','Died']\ndf_class.plot(kind='bar',stacked=True, figsize=(5,3), title=\"Survived\/Died by Class\")\n\nClass1_survived= df_class.iloc[0,0]\/df_class.iloc[:,0].sum()*100\nClass2_survived = df_class.iloc[0,1]\/df_class.iloc[:,1].sum()*100\nClass3_survived = df_class.iloc[0,2]\/df_class.iloc[:,2].sum()*100\nprint(\"Percentage of Class 1 that survived:\" ,round(Class1_survived),\"%\")\nprint(\"Percentage of Class 2 that survived:\" ,round(Class2_survived), \"%\")\nprint(\"Percentage of Class 3 that survived:\" ,round(Class3_survived), \"%\")\n\n# display table\nfrom IPython.display import display\ndisplay(df_class)","058aa9e5":"#-------------------Survived\/Died by SEX------------------------------------\n   \nSurvived = dataset[dataset.Survived == 1]['Sex'].value_counts()\nDied = dataset[dataset.Survived == 0]['Sex'].value_counts()\ndf_sex = pd.DataFrame([Survived , Died])\ndf_sex.index = ['Survived','Died']\ndf_sex.plot(kind='bar',stacked=True, figsize=(5,3), title=\"Survived\/Died by Sex\")\n\n\nfemale_survived= df_sex.female[0]\/df_sex.female.sum()*100\nmale_survived = df_sex.male[0]\/df_sex.male.sum()*100\nprint(\"Percentage of female that survived:\" ,round(female_survived), \"%\")\nprint(\"Percentage of male that survived:\" ,round(male_survived), \"%\")\n\n# display table\nfrom IPython.display import display\ndisplay(df_sex) ","dc35dec7":"#-------------------- Survived\/Died by Embarked ----------------------------\n\nsurvived_embark = dataset[dataset['Survived']==1]['Embarked'].value_counts()\ndead_embark = dataset[dataset['Survived']==0]['Embarked'].value_counts()\ndf_embark = pd.DataFrame([survived_embark,dead_embark])\ndf_embark.index = ['Survived','Died']\ndf_embark.plot(kind='bar',stacked=True, figsize=(5,3))\n\nEmbark_S= df_embark.iloc[0,0]\/df_embark.iloc[:,0].sum()*100\nEmbark_C = df_embark.iloc[0,1]\/df_embark.iloc[:,1].sum()*100\nEmbark_Q = df_embark.iloc[0,2]\/df_embark.iloc[:,2].sum()*100\nprint(\"Percentage of Embark S that survived:\", round(Embark_S), \"%\")\nprint(\"Percentage of Embark C that survived:\" ,round(Embark_C), \"%\")\nprint(\"Percentage of Embark Q that survived:\" ,round(Embark_Q), \"%\")\n\nfrom IPython.display import display\ndisplay(df_embark)","4853975e":"X = dataset.drop(['PassengerId','Cabin','Ticket','Fare', 'Parch', 'SibSp'], axis=1)\ny = X.Survived                       # vector of labels (dependent variable)\nX=X.drop(['Survived'], axis=1)       # remove the dependent variable from the dataframe X\n\nX.head(20)","d9a92b2d":"# ----------------- Encoding categorical data -------------------------\n\n# encode \"Sex\"\nfrom sklearn.preprocessing import LabelEncoder\nlabelEncoder_X = LabelEncoder()\nX.Sex=labelEncoder_X.fit_transform(X.Sex)\n\n\n# encode \"Embarked\"\n\n# number of null values in embarked:\nprint ('Number of null values in Embarked:', sum(X.Embarked.isnull()))\n\n# fill the two values with one of the options (S, C or Q)\nrow_index = X.Embarked.isnull()\nX.loc[row_index,'Embarked']='S' \n\nEmbarked  = pd.get_dummies(  X.Embarked , prefix='Embarked'  )\nX = X.drop(['Embarked'], axis=1)\nX= pd.concat([X, Embarked], axis=1)  \n# we should drop one of the columns\nX = X.drop(['Embarked_S'], axis=1)\n\nX.head()","e2e4f2a8":"#-------------- Taking care of missing data  -----------------------------\n\nprint ('Number of null values in Age:', sum(X.Age.isnull()))\n \n\n# -------- Change Name -> Title ----------------------------\ngot= dataset.Name.str.split(',').str[1]\nX.iloc[:,1]=pd.DataFrame(got).Name.str.split('\\s+').str[1]\n# ---------------------------------------------------------- \n\n\n#------------------ Average Age per title -------------------------------------------------------------\nax = plt.subplot()\nax.set_ylabel('Average age')\nX.groupby('Name').mean()['Age'].plot(kind='bar',figsize=(13,8), ax = ax)\n\ntitle_mean_age=[]\ntitle_mean_age.append(list(set(X.Name)))  #set for unique values of the title, and transform into list\ntitle_mean_age.append(X.groupby('Name').Age.mean())\ntitle_mean_age\n#------------------------------------------------------------------------------------------------------\n\n\n#------------------ Fill the missing Ages ---------------------------\nn_traning= dataset.shape[0]   #number of rows\nn_titles= len(title_mean_age[1])\nfor i in range(0, n_traning):\n    if np.isnan(X.Age[i])==True:\n        for j in range(0, n_titles):\n            if X.Name[i] == title_mean_age[0][j]:\n                X.Age[i] = title_mean_age[1][j]\n#--------------------------------------------------------------------    \n\nX=X.drop(['Name'], axis=1)\n\n       ","48f29b23":"for i in range(0, n_traning):\n    if X.Age[i] > 18:\n        X.Age[i]= 0\n    else:\n        X.Age[i]= 1\n\nX.head()","e1fbff18":"y.head()","855a3f0c":"test = pd.read_csv('..\/input\/titanic\/test.csv')\ntest.head()","a1dae644":"X1 = test.drop(['PassengerId','Cabin','Ticket','Fare', 'Parch', 'SibSp'], axis=1)\n\nX1.head(20)","c97fd3d7":"# ----------------- Encoding categorical data -------------------------\n\n# encode \"Sex\"\nfrom sklearn.preprocessing import LabelEncoder\nlabelEncoder_X = LabelEncoder()\nX1.Sex=labelEncoder_X.fit_transform(X1.Sex)\n\n\n# encode \"Embarked\"\n\n# number of null values in embarked:\nprint ('Number of null values in Embarked:', sum(X1.Embarked.isnull()))\n\n# fill the two values with one of the options (S, C or Q)\nrow_index = X1.Embarked.isnull()\nX1.loc[row_index,'Embarked']='S' \n\nEmbarked  = pd.get_dummies(  X1.Embarked , prefix='Embarked'  )\nX1 = X1.drop(['Embarked'], axis=1)\nX1= pd.concat([X1, Embarked], axis=1)  \n# we should drop one of the columns\nX1 = X1.drop(['Embarked_S'], axis=1)\n\nX1.head()","f9037a02":"#-------------- Taking care of missing data  -----------------------------\n\nprint ('Number of null values in Age:', sum(X1.Age.isnull()))\n \n\n# -------- Change Name -> Title ----------------------------\ngot= test.Name.str.split(',').str[1]\nX1.iloc[:,1]=pd.DataFrame(got).Name.str.split('\\s+').str[1]\n# ---------------------------------------------------------- \n\n\n#------------------ Average Age per title -------------------------------------------------------------\nax = plt.subplot()\nax.set_ylabel('Average age')\nX1.groupby('Name').mean()['Age'].plot(kind='bar',figsize=(13,8), ax = ax)\n\ntitle_mean_age=[]\ntitle_mean_age.append(list(set(X1.Name)))  #set for unique values of the title, and transform into list\ntitle_mean_age.append(X1.groupby('Name').Age.mean())\ntitle_mean_age\n#------------------------------------------------------------------------------------------------------\n\n\n#------------------ Fill the missing Ages ---------------------------\nn_traning= test.shape[0]   #number of rows\nn_titles= len(title_mean_age[1])\nfor i in range(0, n_traning):\n    if np.isnan(X1.Age[i])==True:\n        for j in range(0, n_titles):\n            if X1.Name[i] == title_mean_age[0][j]:\n                X1.Age[i] = title_mean_age[1][j]\n#--------------------------------------------------------------------    \n\nX1=X1.drop(['Name'], axis=1)\n\n       ","b01aa062":"for i in range(0, n_traning):\n    if X1.Age[i] > 18:\n        X1.Age[i]= 0\n    else:\n        X1.Age[i]= 1\n\nX1.head()","d67548d2":"\n\nfrom sklearn.model_selection import KFold\n\nntrain = X.shape[0]\nntest = X1.shape[0]\nNFOLDS = 5\nSEED = 0\nkf = KFold(n_splits = NFOLDS, random_state = SEED)\n\n\n# # Some useful parameters which will come in handy later on\n# ntrain = train.shape[0]\n# ntest = test.shape[0]\n# SEED = 0 # for reproducibility\n# NFOLDS = 5 # set folds for out-of-fold prediction\n# kf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\n\n# Class to extend the Sklearn classifier\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y):\n        return self.clf.fit(x,y)\n    \n    def feature_importances(self,x,y):\n        print(self.clf.fit(x,y).feature_importances_)","21860784":"def get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS,ntest))\n    \n    for i, (train_index, test_index) in enumerate(kf.split(x_train)):\n        x_tr = x_train[train_index]\n        x_te = x_train[test_index]\n        y_tr = y_train[train_index]\n        \n        clf.train(x_tr,y_tr)\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i,:] = clf.predict(x_test)\n        \n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1,1),oof_test.reshape(-1,1)\n        ","780041db":"from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC","0127ceed":"# Put in our parameters for said classifiers\n# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators':500,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_estimators': 500,\n     #'max_features': 0.2,\n    'max_depth': 5,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# Support Vector Classifier parameters \nsvc_params = {\n    'kernel' : 'linear',\n    'C' : 0.025\n    }","5d493108":"rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\net = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\nada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\ngb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\nsvc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)","52915c34":"y.head()","9db5999c":"# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\n#X['Survived'].ravel()\ny_train = y\ntrain = X\nx_train = train.values # Creates an array of the train data\nx_test = X1.values # Creats an array of the test data","2d48b9ad":"# Create our OOF train and test predictions. These base results will be used as new features\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n\nprint(\"Training is complete\")","3b3bff19":"rf_feature = rf.feature_importances(x_train,y_train)\net_feature = et.feature_importances(x_train, y_train)\nada_feature = ada.feature_importances(x_train, y_train)\ngb_feature = gb.feature_importances(x_train,y_train)","d9a325df":"rf_features = [0.3116732 , 0.56566957, 0.05057659, 0.05226303, 0.01981761]\net_features = [0.26047002, 0.64797022, 0.03636626, 0.03371078, 0.02148271]\nada_features = [0.29  ,0.106 ,0.076 ,0.066 ,0.102]\ngb_features = [0.24384334 ,0.66760243 ,0.03514145 ,0.0271241 , 0.02628868]","fb0af3a5":"cols = train.columns.values\n# Create a dataframe with features\nfeature_dataframe = pd.DataFrame( {'features': cols,\n     'Random Forest feature importances': rf_features,\n     'Extra Trees  feature importances': et_features,\n      'AdaBoost feature importances': ada_features,\n    'Gradient Boost feature importances': gb_features\n    })","268b3d0f":"# Create the new column containing the average of values\n\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\nfeature_dataframe.head(3)","ae2d36ec":"base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n     'ExtraTrees': et_oof_train.ravel(),\n     'AdaBoost': ada_oof_train.ravel(),\n      'GradientBoost': gb_oof_train.ravel()\n    })\nbase_predictions_train.head()","0519de95":"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)","04559c00":"import xgboost as xgb\n\ngbm = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1).fit(x_train, y_train)\npredictions = gbm.predict(x_test)","6e5a78d2":"stacked_pred=pd.DataFrame(predictions)\nstacked_sub_df=pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nstacked_datasets=pd.concat([stacked_sub_df['PassengerId'],stacked_pred],axis=1)\nstacked_datasets.columns=['PassengerId','Survived']\nstacked_datasets.to_csv('stacked_sample2_submission_titanic.csv',index=False)","af5b28f7":"stacked_datasets.head()","85dce996":"You may wonder why are we still keeping the **\"Name\"** column. In fact the name does not seem to have influence, it does not matter if a person is named Owen or William, however this column has the title located after the Surname and the comma (\"Mr\", \"Mrs\", \"Miss\", etc.) which can be useful.  \n\nIf we take a look at the table X displayed previously we can see many missing values for the **\"Age\"** column. Removing these rows with missing values would involve removing 177 rows (which is quite a lot!) and we would have less information to create the model. In some cases, it is acceptable to take the average of the column and replace the null values, nonetheless in this case, it is possible to estimate the age of the person by their title, present in the **\"Name\"** column.   \n\nTherefore, I will first identify the different titles presented and then average the Age for each title. We can provide this averaged Age found for each title to the people with missing Age values, accordingly to their title in **\"Name\"**. \n\nAfter using the information in **\"Name\"** we can drop this column. ","d8db607c":"We can see, from this displayed DataFrame, that **\"Sex\"** and **\"Embarked\"** are categorical features and have strings instead of numeric values. We need to encode these strings into numeric data, so the algorithm can perform its calculations. \n\nFor the **\"Sex\"** feature we can use the **LabelEncoder** class from  **sklearn.preprocessing** library. \n\nAnother way of doing this is by using the **get_dummies** from **pandas**. We will be using this to encode the **\"Embarked\"** feature. But first, as **\"Embarked\"** has two NaN values we need to take care of these missing values. In this approach, I will provide the 'S' category because it is the most frequent in the data. After this, it is then possible to use the **get_dummies** and get three new columns (Embarked_C,\tEmbarked_Q, Embarked_S) which are called dummy variables (they assign \u20180\u2019 and \u20181\u2019 to indicate membership in a category). The previous **\"Embarked\"** can be dropped from X as it will not be needed anymore and we can now concatenate the X dataframe with the new **\"Embarked\"** which has the three dummy variables. Finally, as the number of dummy variables necessary to represent a single feature is equal to the number of categories in that feature minus one, we can remove one of the dummies created, lets say Embarked_S, for example. This will not remove any information because by having the values from Embarked_C and\tEmbarked_Q the algorithm can easily understand the values from the remaining dummy variable (when Embarked_C and Embarked_Q are '0' Embarked_S will be '1', otherwise it will be '0').  ","84171ea9":"## References:\n1. https:\/\/www.kaggle.com\/rochellesilva\/simple-tutorial-for-beginners\n2. https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python\nshoutout to @rochellesilva and @arthurtok and their work, this notebook could not be possible except them and their notebook. ","18936dbf":"## stacking:","d4d372be":"<font size=\"10\">\n    <div align=\"center\">Titanic Survival Prediction \ud83d\udea2<\/div>\n<\/font>\n\n","a213a57a":"Applying stacking will increase you position in the leader board but the feature engineering which is followed in this notebook in not that much efficient as it should be so, if you try to use the **stacked_sample2_submission_titanic.csv** of this notebook you might end up with top 30-40%. This Kernel is only for learning stacking for classification.","a9e3b8ea":"<center><img src=\"https:\/\/i.imgur.com\/348rrLl.jpg\" width=\"500px\"><\/center>","9efa03e8":"Now let's import the csv file with the training dataset. You can download it from [here](https:\/\/www.kaggle.com\/c\/titanic\/data).  The explanation of the features (each column from the dataset) is also presented in this link. ","4adc15dc":"We can also make feature transformation. For example, we could transform the **\"Age\"** feature in order to simplify it. We could distinguish the youngsters (age less than 18 years) from the adults.  \n\n","d6186d4f":"Now, we can say that we have a quite well clean dataset to provide to our classifier algorithm. \n","eb7da33d":"## 1. Data exploration and visualization  \n\nFor a good start, we should look at the dataset. Analyze the features and think which could be useful to predict the survival rate. The features that probably may have an influence are: the **\"P-class\"** (expect to see more survival for higher class), the **\"Sex\"** and **\"Age\"** (\"women and children first\"), and let's say **\"Embarked\"** also. \n\nWe will now plot some graphs to confirm if these features show some relation with the survival rate. These plots were based in the graphs presented [here](http:\/\/ahmedbesbes.com\/how-to-score-08134-in-titanic-kaggle-challenge.html).  ","e2c5bff5":"### Table of content:\n\n#### 1. Data exploration and visualization  \n   - Explore dataset\n   - Choose important features and visualize them according to survival\/non-survival\n   \n#### 2. Data cleaning, Feature selection and Feature engineering\n   - Null values\n   - Encode categorical data\n   - Transform features\n   \n#### 3. Test different classifiers \n  Stacking of models based on:\n   - Extratrees Classifiers\n   - Adaboost Classifiers\n   - Support Vector Machine\n   - Gradient boosting\n   - Random Forest \n \n <font color=\"red\" size=3>Please upvote  this kernel if you like it. It motivates me to produce more quality content :)<\/font>\n ","c7fa7657":"## 2. Data cleaning, Feature selection and Feature engineering\nThe preprocessing of the data is a quite crucial part. If we just give the dataset without cleaning it, most probably the results will not be good! So, in this step we will preprocess the training dataset and this will involve feature selection, data cleaning, and feature engineering.   \n\nI will start with feature selection. As we saw previously, **\"P-Class\", \"Sex\", \"Age\"** and **\"Embarked\"** showed some relation with Survived rate. Thus, I will drop the remaining features, except **\"Name\"** because it will be useful in a further step of the cleaning process. "}}