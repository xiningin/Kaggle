{"cell_type":{"b376439d":"code","d51c85c5":"code","ba34264e":"code","c4b530c3":"code","7ddae61f":"code","284400f0":"code","e4d19a94":"code","31476832":"code","cadf47f7":"code","6fb8ad0b":"code","f8995596":"code","bb4bc909":"code","a9bc468c":"code","d62c4fe8":"code","ff198d6a":"code","ac6327a9":"code","1f675d17":"code","4625cdda":"code","2439cfd6":"code","fd6987bf":"code","e2833f4d":"code","b952338f":"code","444878c1":"code","2ce6a8e1":"code","79601c71":"code","da06a4d3":"code","81ad6b7e":"code","dfb681d3":"code","9f399432":"code","13dd804e":"code","0740e8b4":"code","8f6696f7":"code","a34f38d6":"code","01f7eda7":"code","0c81a958":"code","9718ee18":"code","eb758920":"code","2320ec19":"code","0af3ccc7":"code","ed0b3159":"code","6305c740":"code","bed43cb4":"code","5b6e8436":"code","3670310f":"code","e184c0d0":"code","4190329c":"code","15c650fe":"code","3fd29a13":"code","231ca1f6":"code","fcaee1e6":"code","d1a00b4b":"code","20d66c5e":"code","86c890a9":"code","cae72ffa":"code","e0b48221":"code","96311948":"code","741dae7b":"code","516fca54":"code","bfb303d9":"code","6ab067a3":"code","e7d6babe":"code","a3dedb07":"code","52a72341":"code","74dae39d":"code","baa460ab":"code","72751c39":"code","e29d1bee":"code","ad47624d":"code","e0285762":"code","139031f2":"code","9105568c":"code","ecfc1cee":"code","309a0493":"code","a50ca240":"code","17914dd6":"code","fb0be017":"code","83c72441":"code","2fd5ee5e":"code","4eac44f0":"code","4727b801":"markdown","facd38a1":"markdown","080e2245":"markdown","556950f6":"markdown","c14d3f74":"markdown","ac99c97a":"markdown","3990b953":"markdown","e936ef70":"markdown","ff0d7476":"markdown","ca351f19":"markdown","fc31f77b":"markdown","6880c377":"markdown","3f39ed73":"markdown","8b7556bb":"markdown","cb24bd26":"markdown","1fb4c551":"markdown","ef63612a":"markdown","6b707daa":"markdown","5a2f8c74":"markdown","58bbd889":"markdown","0e77aff9":"markdown","3fe27e2a":"markdown","3b6b8998":"markdown","c35e6383":"markdown","df5ca0af":"markdown","85b1a529":"markdown","0d97d513":"markdown","6cdf4561":"markdown","d50b5e95":"markdown","e446ec45":"markdown","80d1101a":"markdown","e3db43d6":"markdown"},"source":{"b376439d":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns","d51c85c5":"import warnings\nwarnings.filterwarnings('ignore')","ba34264e":"from statistics import mode","c4b530c3":"import re","7ddae61f":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","284400f0":"train.head()","e4d19a94":"train.isnull().sum()","31476832":"sns.countplot(train['Survived']);","cadf47f7":"sns.countplot(x='Survived', hue='Sex', data=train);","6fb8ad0b":"sns.heatmap(train.isnull(), yticklabels = False, cmap='plasma');","f8995596":"train.describe()","bb4bc909":"sns.countplot(train['Pclass']);","a9bc468c":"train.Name.value_counts().head()","d62c4fe8":"train['Age'].hist(bins=40);","ff198d6a":"train['SibSp'].value_counts()","ac6327a9":"sns.countplot(train['SibSp'])\nplt.title('Count plot for SibSp');","1f675d17":"sns.countplot(train['Parch'])\nplt.title('Count plot for Parch');","4625cdda":"train.Ticket.value_counts(dropna=False, sort=True).head()","2439cfd6":"train['Fare'].hist(bins=50)\nplt.ylabel('Price')\nplt.xlabel('Index')\nplt.title('Fare Price distribution');","fd6987bf":"train.Cabin.value_counts(0)","e2833f4d":"sns.countplot(train['Embarked'])\nplt.title('Count plot for Embarked');","b952338f":"sns.heatmap(train.corr(), annot=True);","444878c1":"sns.countplot(x='Survived', hue='Pclass', data=train)\nplt.title('Count plot for Pclass categorized by Survived');","2ce6a8e1":"age_group = train.groupby('Pclass')['Age']","79601c71":"age_group.median()","da06a4d3":"age_group.mean()","81ad6b7e":"train.loc[train.Age.isnull(), 'Age'] = train.groupby(\"Pclass\").Age.transform('median')\n\ntrain[\"Age\"].isnull().sum()","dfb681d3":"sns.heatmap(train.isnull(), yticklabels = False, cmap='plasma');","9f399432":"train['Sex'][train['Sex'] == 'male'] = 0\ntrain['Sex'][train['Sex'] == 'female'] = 1","13dd804e":"train[\"Embarked\"][train[\"Embarked\"] == \"S\"] = 0\ntrain[\"Embarked\"][train[\"Embarked\"] == \"C\"] = 1\ntrain[\"Embarked\"][train[\"Embarked\"] == \"Q\"] = 2","0740e8b4":"train.head()","8f6696f7":"df = pd.read_csv('..\/input\/titanic\/train.csv')","a34f38d6":"test['Survived'] = np.nan\nfull = pd.concat([df, test])","01f7eda7":"full.isnull().sum()","0c81a958":"full.head()","9718ee18":"full['Embarked'] = full['Embarked'].fillna(mode(full['Embarked']))","eb758920":"# Convert 'Sex' variable to integer form!\nfull[\"Sex\"][full[\"Sex\"] == \"male\"] = 0\nfull[\"Sex\"][full[\"Sex\"] == \"female\"] = 1\n\n# Convert 'Embarked' variable to integer form!\nfull[\"Embarked\"][full[\"Embarked\"] == \"S\"] = 0\nfull[\"Embarked\"][full[\"Embarked\"] == \"C\"] = 1\nfull[\"Embarked\"][full[\"Embarked\"] == \"Q\"] = 2","2320ec19":"sns.heatmap(full.corr(), annot=True);","0af3ccc7":"full['Age'] = full.groupby(\"Pclass\")['Age'].transform(lambda x: x.fillna(x.median()))","ed0b3159":"full.isnull().sum()","6305c740":"full['Fare']  = full.groupby(\"Pclass\")['Fare'].transform(lambda x: x.fillna(x.median()))","bed43cb4":"full['Cabin'] = full['Cabin'].fillna('U')","5b6e8436":"full['Cabin'].unique().tolist()[:20]","3670310f":"full['Cabin'] = full['Cabin'].map(lambda x:re.compile(\"([a-zA-Z])\").search(x).group())","e184c0d0":"full['Cabin'].unique().tolist()","4190329c":"cabin_category = {'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6, 'G':7, 'T':8, 'U':9}\nfull['Cabin'] = full['Cabin'].map(cabin_category)","15c650fe":"full['Cabin'].unique().tolist()","3fd29a13":"full['Name'].head()","231ca1f6":"full['Name'] = full.Name.str.extract(' ([A-Za-z]+)\\.', expand = False)","fcaee1e6":"full['Name'].unique().tolist()","d1a00b4b":"full['Name'].value_counts(normalize = True) * 100","20d66c5e":"full.rename(columns={'Name' : 'Title'}, inplace=True)","86c890a9":"full['Title'] = full['Title'].replace(['Rev', 'Dr', 'Col', 'Ms', 'Mlle', 'Major', 'Countess', \n                                       'Capt', 'Dona', 'Jonkheer', 'Lady', 'Sir', 'Mme', 'Don'], 'Other')","cae72ffa":"full['Title'].value_counts(normalize = True) * 100","e0b48221":"title_category = {'Mr':1, 'Miss':2, 'Mrs':3, 'Master':4, 'Other':5}\nfull['Title'] = full['Title'].map(title_category)\nfull['Title'].unique().tolist()","96311948":"full['familySize'] = full['SibSp'] + full['Parch'] + 1","741dae7b":"# Drop redundant features\nfull = full.drop(['SibSp', 'Parch', 'Ticket'], axis = 1)","516fca54":"full.head()","bfb303d9":"# Recover test dataset\ntest = full[full['Survived'].isna()].drop(['Survived'], axis = 1)","6ab067a3":"test.head()","e7d6babe":"# Recover train dataset\ntrain = full[full['Survived'].notna()]","a3dedb07":"train['Survived'] = train['Survived'].astype(np.int8)","52a72341":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(train.drop(['Survived', 'PassengerId'], axis=1), train['Survived'], test_size = 0.2, random_state=2)","74dae39d":"from sklearn.linear_model import LogisticRegression\nLogisticRegression = LogisticRegression(max_iter=10000)\nLogisticRegression.fit(X_train, y_train)","baa460ab":"predictions = LogisticRegression.predict(X_test)\npredictions","72751c39":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, predictions)","e29d1bee":"acc = (87+54) \/ (87+54+13+25) * 100\nacc","ad47624d":"from sklearn.model_selection import KFold\nkf = KFold(n_splits = 5, random_state=2)","e0285762":"from sklearn.model_selection import cross_val_score\n\ncross_val_score(LogisticRegression, X_test, y_test, cv = kf).mean() * 100","139031f2":"from sklearn.ensemble import RandomForestClassifier\nRandomForest = RandomForestClassifier(random_state=2)","9105568c":"# Set our parameter grid\nparam_grid = { \n    'criterion' : ['gini', 'entropy'],\n    'n_estimators': [100, 300, 500],\n    'max_features': ['auto', 'log2'],\n    'max_depth' : [3, 5, 7]    \n}","ecfc1cee":"from sklearn.model_selection import GridSearchCV\n\nrandomForest_CV = GridSearchCV(estimator = RandomForest, param_grid = param_grid, cv = 5)\nrandomForest_CV.fit(X_train, y_train)","309a0493":"randomForest_CV.best_params_","a50ca240":"randomForestFinalModel = RandomForestClassifier(random_state = 2, criterion = 'gini', max_depth = 7, max_features = 'auto', n_estimators = 300)\n\nrandomForestFinalModel.fit(X_train, y_train)","17914dd6":"predictions = randomForestFinalModel.predict(X_test)","fb0be017":"from sklearn.metrics import accuracy_score\n\naccuracy_score(y_test, predictions) * 100","83c72441":"test['Survived'] = randomForestFinalModel.predict(test.drop(['PassengerId'], axis = 1))","2fd5ee5e":"test[['PassengerId', 'Survived']].to_csv('MySubmission.csv', index = False)","4eac44f0":"test.info()","4727b801":"### You can always use value_counts to check on data, visualization is just another option ","facd38a1":"# Magic Weapon #3: Hyperparameter Tuning","080e2245":"### Didn't Work","556950f6":"### Good practice to check the results","c14d3f74":"### Let's submit our solutions","ac99c97a":"### Wohh that's lot's of title","3990b953":"### You need to do the same changes in test dataset aslo...So lest merge test and train","e936ef70":" Secondly, I would like to introduce one of the most popular algorithms for classification (but also regression, etc), **Random Forest!** In a nutshell, Random Forest is an ensembling learning algorithm which combines **decision trees** in order to increase performance and avoid overfitting.","ff0d7476":"### Did you recognize something? yes, We can get the alphabets(first letter) by running regular expression ","ca351f19":"# Examine Dataset\n## Look In to every column one by one ","fc31f77b":"#### Let's print our optimal hyperparameters set!","6880c377":"### You can also find null values by plotting it on graph","3f39ed73":"### Also, corr(Fare, Pclass) is the highest correlation in absolute numbers for 'Fare', so we'll use Pclass again to impute the missing values!","8b7556bb":" One of the most popular and efficient CV variants is **k-Fold Cross-Validation**, which we will choose to set our strong local validation scheme below. In a nutshell, k is the number of folds, mentioned above!\n\n Nice, now let's apply this key technique ourselves! We will use the basic version of k-Fold with **5 folds** from our friend, Scikit-learn!","cb24bd26":"### Whoops! Apart from Mr, Miss, Mrs, and Master, the rest have percentages close to zero...\n\n### So, let's bundle them!","1fb4c551":"# Let's Fix data","ef63612a":"# Load DataSet","6b707daa":"<center><h1 style=\"color:green\">Don't forget to upvote if you like it! It's free! :)<\/h1><\/center>","5a2f8c74":"### Look in to relationships among dataset","58bbd889":"### Hmmm... but we know from part 2 that Sibsp is the number of siblings \/ spouses aboard the Titanic, and Parch is the number of parents \/ children aboard the Titanic... So, what is another straightforward feature to engineer?\n\n### Yes, it is the size of each family aboard!","0e77aff9":"### Better! let's convert to numeric","3fe27e2a":"### annot argument is mandatory as you also need data value in each cell\n### As you can see that Survived as max relation with Pclass, lets vizualize it in chart","3b6b8998":"### OK, if we look closely, corr(Age, Pclass) is the highest correlation in absolute numbers for 'Age', so we'll use Pclass to impute the missing values:","c35e6383":"# Dateset is completely ready now!","df5ca0af":"# Importing Libraries","85b1a529":"### Pclass and age, as they had max relation in the entire set we are going to replace missing age values with median age calculated per class","0d97d513":"### Wow! 'Sex' looks like a very strong explanatory variable, and it can be our choice for our single feature Logistic Regression model!","6cdf4561":"### You can skip arguments other than x, cmap is styling the heatmap\n","d50b5e95":"# Plz Upvote!","e446ec45":"Below we set the hyperparameter grid of values with 4 lists of values:\n\n- **'criterion'** : A function which measures the quality of a split.\n- **'n_estimators'** : The number of trees of our random forest.\n- **'max_features'** : The number of features to choose when looking for the best way of splitting.\n- **'max_depth'** : the maximum depth of a decision tree.","80d1101a":"# Contents:\n1. Include Libraries\n2. Import DataSet\n3. EDA(Exploratory Data Analysis)\n4. Handle Missing Value\n5. Feature Engineering by OneHotEncoding\n6. Logistic Regression\n7. Hyperparameter Tunning\n8. Train Random Forest Classifier\n9. Final Submittion","e3db43d6":"# Magic Weapon #2: Cross-Validation"}}