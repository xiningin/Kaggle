{"cell_type":{"f9539125":"code","e2e5363d":"code","42d5032e":"code","d412c1bc":"code","689dbe8d":"code","eeca3166":"code","da5714ca":"code","ad3d2f37":"code","b648e77b":"code","d4263bbb":"code","de123dc0":"markdown","25bb063f":"markdown","5a204e73":"markdown","534a3098":"markdown","8f873299":"markdown","d7eebf6d":"markdown","1feb8381":"markdown","3cc81be8":"markdown","7899257c":"markdown","40a3354e":"markdown","bd2ab678":"markdown","7fb92284":"markdown","7d5bc282":"markdown","e9511395":"markdown","2acd3724":"markdown","6751c45e":"markdown"},"source":{"f9539125":"\"\"\" Uncomment and Download data for training (6.1 GB) \"\"\"\n# !wget http:\/\/images.cocodataset.org\/zips\/test2017.zip\n# !mkdir '.\/dataset'\n# !unzip -q .\/test2017.zip -d '.\/dataset'","e2e5363d":"\"\"\" Download the best model weights \"\"\"\n!mkdir .\/checkpoints\n!wget -q -O 'best_model.pth' https:\/\/www.dropbox.com\/s\/7xvmmbn1bx94exz\/best_model.pth?dl=1\n!mv best_model.pth .\/checkpoints","42d5032e":"\"\"\" Download content and style images \"\"\"\n!mkdir .\/content\n!mkdir .\/style\n!wget -q https:\/\/github.com\/myelinfoundry-2019\/challenge\/raw\/master\/japanese_garden.jpg -P '.\/content'\n!wget -q https:\/\/github.com\/myelinfoundry-2019\/challenge\/raw\/master\/picasso_selfportrait.jpg -P '.\/style'","d412c1bc":"import torch\nfrom torch.autograd import Variable\nfrom collections import namedtuple\nfrom torchvision import models\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nimport numpy as np\nimport os\nimport sys\nimport random\nfrom PIL import Image\nimport glob\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torchvision.utils import save_image\nimport matplotlib.pyplot as plt\nimport cv2\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(42) #for reproducibility\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Mean and standard deviation used for training\nmean = np.array([0.485, 0.456, 0.406])\nstd = np.array([0.229, 0.224, 0.225])","689dbe8d":"\"\"\" Pretrained VGG16 Model \"\"\"\nclass VGG16(torch.nn.Module):\n    def __init__(self, requires_grad=False):\n        super(VGG16, self).__init__()\n        vgg_pretrained_features = models.vgg16(pretrained=True).features\n        self.slice1 = torch.nn.Sequential()\n        self.slice2 = torch.nn.Sequential()\n        self.slice3 = torch.nn.Sequential()\n        self.slice4 = torch.nn.Sequential()\n        \n        \n        for x in range(4):\n            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(4, 9):\n            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(9, 16):\n            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(16, 23):\n            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n        if not requires_grad:\n            for param in self.parameters():\n                param.requires_grad = False\n\n    def forward(self, X):\n        h = self.slice1(X)\n        h_relu1_2 = h\n        h = self.slice2(h)\n        h_relu2_2 = h\n        h = self.slice3(h)\n        h_relu3_3 = h\n        h = self.slice4(h)\n        h_relu4_3 = h\n        vgg_outputs = namedtuple(\"VggOutputs\", [\"relu1_2\", \"relu2_2\", \"relu3_3\", \"relu4_3\"])\n        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3)\n        return out\n\n\n\"\"\" Transformer Net \"\"\"\nclass TransformerNet(torch.nn.Module):\n    def __init__(self):\n        super(TransformerNet, self).__init__()\n        self.model = nn.Sequential(\n            ConvBlock(3, 32, kernel_size=9, stride=1),\n            ConvBlock(32, 64, kernel_size=3, stride=2),\n            ConvBlock(64, 128, kernel_size=3, stride=2),\n            ResidualBlock(128),\n            ResidualBlock(128),\n            ResidualBlock(128),\n            ResidualBlock(128),\n            ResidualBlock(128),\n            ConvBlock(128, 64, kernel_size=3, upsample=True),\n            ConvBlock(64, 32, kernel_size=3, upsample=True),\n            ConvBlock(32, 3, kernel_size=9, stride=1, normalize=False, relu=False),\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n\n\"\"\" Components of Transformer Net \"\"\"\nclass ResidualBlock(torch.nn.Module):\n    def __init__(self, channels):\n        super(ResidualBlock, self).__init__()\n        self.block = nn.Sequential(\n            ConvBlock(channels, channels, kernel_size=3, stride=1, normalize=True, relu=True),\n            ConvBlock(channels, channels, kernel_size=3, stride=1, normalize=True, relu=False),\n        )\n\n    def forward(self, x):\n        return self.block(x) + x\n\n\nclass ConvBlock(torch.nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, upsample=False, normalize=True, relu=True):\n        super(ConvBlock, self).__init__()\n        self.upsample = upsample\n        self.block = nn.Sequential(\n            nn.ReflectionPad2d(kernel_size \/\/ 2), nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n        )\n        self.norm = nn.InstanceNorm2d(out_channels, affine=True) if normalize else None\n        self.relu = relu\n\n    def forward(self, x):\n        if self.upsample:\n            x = F.interpolate(x, scale_factor=2)\n        x = self.block(x)\n        if self.norm is not None:\n            x = self.norm(x)\n        if self.relu:\n            x = F.relu(x)\n        return x","eeca3166":"def gram_matrix(y):\n    \"\"\" Returns the gram matrix of y (used to compute style loss) \"\"\"\n    (b, c, h, w) = y.size()\n    features = y.view(b, c, w * h)\n    features_t = features.transpose(1, 2)\n    gram = features.bmm(features_t) \/ (c * h * w)\n    return gram\n\n\ndef train_transform(image_size):\n    \"\"\" Transforms for training images \"\"\"\n    transform = transforms.Compose(\n        [\n            transforms.Resize((int(image_size * 1.15),int(image_size * 1.15))),\n            transforms.RandomCrop(image_size),\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std),\n        ]\n    )\n    return transform\n\n\ndef style_transform(image_size=None):\n    \"\"\" Transforms for style image \"\"\"\n    resize = [transforms.Resize((image_size,image_size))] if image_size else []\n    transform = transforms.Compose(resize + [transforms.ToTensor(), transforms.Normalize(mean, std)])\n    return transform\n\ndef test_transform(image_size=None):\n    \"\"\" Transforms for test image \"\"\"\n    resize = [transforms.Resize(image_size)] if image_size else []\n    transform = transforms.Compose(resize + [transforms.ToTensor(), transforms.Normalize(mean, std)])\n    return transform\n\ndef denormalize(tensors):\n    \"\"\" Denormalizes image tensors using mean and std \"\"\"\n    for c in range(3):\n        tensors[:, c].mul_(std[c]).add_(mean[c])\n    return tensors\n\n\ndef deprocess(image_tensor):\n    \"\"\" Denormalizes and rescales image tensor \"\"\"\n    image_tensor = denormalize(image_tensor)[0]\n    image_tensor *= 255\n    image_np = torch.clamp(image_tensor, 0, 255).cpu().numpy().astype(np.uint8)\n    image_np = image_np.transpose(1, 2, 0)\n    return image_np","da5714ca":"def fast_trainer(style_image,    \n                 style_name,     \n                 dataset_path,   \n                 image_size=256,\n                 style_size=448,\n                 batch_size = 8,\n                 lr = 1e-5,\n                 epochs = 1,\n                 checkpoint_model = None,\n                 checkpoint_interval=200,\n                 sample_interval=200,\n                 lambda_style=10e10,\n                 lambda_content=10e5,):\n    \n    os.makedirs(f\".\/images\/outputs\/{style_name}-training\", exist_ok=True)\n    os.makedirs(f\".\/checkpoints\", exist_ok=True)\n\n\n    \"\"\" Create dataloader for the training data \"\"\"\n    train_dataset = datasets.ImageFolder(dataset_path, train_transform(image_size))\n    dataloader = DataLoader(train_dataset, batch_size=batch_size)\n\n    \"\"\" Define networks \"\"\"\n    transformer = TransformerNet().to(device)\n    vgg = VGG16(requires_grad=False).to(device)\n\n    \"\"\" Load checkpoint model if specified \"\"\"\n    if checkpoint_model:\n        transformer.load_state_dict(torch.load(checkpoint_model))\n\n    \"\"\" Define optimizer and loss \"\"\"\n    optimizer = Adam(transformer.parameters(), lr)\n    l2_loss = torch.nn.MSELoss().to(device)\n\n    \"\"\" Load style image \"\"\"\n    style = style_transform(style_size)(Image.open(style_image))\n    style = style.repeat(batch_size, 1, 1, 1).to(device)\n\n    \"\"\" Extract style features \"\"\"\n    features_style = vgg(style)\n    gram_style = [gram_matrix(y) for y in features_style]\n\n    \"\"\" Sample 8 images for visual evaluation of the model \"\"\"\n    image_samples = []\n    for path in random.sample(glob.glob(f\"{dataset_path}\/*\/*.jpg\"), 8):\n        image_samples += [style_transform(image_size)(Image.open(path))]\n    image_samples = torch.stack(image_samples)\n\n    def save_sample(batches_done):\n        \"\"\" Evaluates the model and saves image samples \"\"\"\n        transformer.eval()\n        with torch.no_grad():\n            output = transformer(image_samples.to(device))\n        image_grid = denormalize(torch.cat((image_samples.cpu(), output.cpu()), 2))\n        save_image(image_grid, f\".\/images\/outputs\/{style_name}-training\/{batches_done}.jpg\", nrow=4)\n        transformer.train()\n    \n    \n    train_metrics = {\"content\": [], \"style\": [], \"total\": []}\n    for epoch in range(epochs):\n        epoch_metrics = {\"content\": [], \"style\": [], \"total\": []}\n        for batch_i, (images, _) in enumerate(dataloader):\n            optimizer.zero_grad()\n\n            images_original = images.to(device)\n            images_transformed = transformer(images_original)\n\n            # Extract features\n            features_original = vgg(images_original)\n            features_transformed = vgg(images_transformed)\n\n            # Compute content loss as MSE between features\n            content_loss = lambda_content * l2_loss(features_transformed.relu2_2, features_original.relu2_2)\n\n            # Compute style loss as MSE between gram matrices\n            style_loss = 0\n            for ft_y, gm_s in zip(features_transformed, gram_style):\n                gm_y = gram_matrix(ft_y)\n                style_loss += l2_loss(gm_y, gm_s[: images.size(0), :, :])\n            style_loss *= lambda_style\n\n            total_loss = content_loss + style_loss\n            total_loss.backward()\n            optimizer.step()\n\n            epoch_metrics[\"content\"] += [content_loss.item()]\n            epoch_metrics[\"style\"] += [style_loss.item()]\n            epoch_metrics[\"total\"] += [total_loss.item()]\n            \n            train_metrics[\"content\"] += [content_loss.item()]\n            train_metrics[\"style\"] += [style_loss.item()]\n            train_metrics[\"total\"] += [total_loss.item()]\n\n            sys.stdout.write(\n                \"\\r[Epoch %d\/%d] [Batch %d\/%d] [Content: %.2f (%.2f) Style: %.2f (%.2f) Total: %.2f (%.2f)]\"\n                % (\n                    epoch + 1,\n                    epochs,\n                    batch_i,\n                    len(train_dataset),\n                    content_loss.item(),\n                    np.mean(epoch_metrics[\"content\"]),\n                    style_loss.item(),\n                    np.mean(epoch_metrics[\"style\"]),\n                    total_loss.item(),\n                    np.mean(epoch_metrics[\"total\"]),\n                )\n            )\n\n            batches_done = epoch * len(dataloader) + batch_i + 1\n            if batches_done % sample_interval == 0:\n                save_sample(batches_done)\n\n            if checkpoint_interval > 0 and batches_done % checkpoint_interval == 0:\n                torch.save(transformer.state_dict(), f\".\/checkpoints\/{style_name}_{batches_done}.pth\")\n\n\n            torch.save(transformer.state_dict(), f\".\/checkpoints\/last_checkpoint.pth\")\n    \n    print(\"Training Completed!\")\n    \n    #printing the loss curve.\n    plt.plot(train_metrics[\"content\"], label = \"Content Loss\")\n    plt.plot(train_metrics[\"style\"], label = \"Style Loss\")\n    plt.plot(train_metrics[\"total\"], label = \"Total Loss\")\n    plt.xlabel('Iteration')\n    plt.ylabel('Loss')\n    plt.title('Training Loss')\n    plt.legend()\n    plt.show()","ad3d2f37":"def test_image(image_path,checkpoint_model,save_path):\n    os.makedirs(os.path.join(save_path,\"results\"), exist_ok=True)\n\n    transform = test_transform()\n\n    # Define model and load model checkpoint\n    transformer = TransformerNet().to(device)\n    transformer.load_state_dict(torch.load(checkpoint_model))\n    transformer.eval()\n\n    # Prepare input\n    image_tensor = Variable(transform(Image.open(image_path))).to(device)\n    image_tensor = image_tensor.unsqueeze(0)\n\n    # Stylize image\n    with torch.no_grad():\n        stylized_image = denormalize(transformer(image_tensor)).cpu()\n    # Save image\n    fn = checkpoint_model.split('\/')[-1].split('.')[0]\n    save_image(stylized_image, os.path.join(save_path,f\"results\/{fn}-output.jpg\"))\n    print(\"Image Saved!\")\n    plt.imshow(cv2.cvtColor(cv2.imread(os.path.join(save_path,f\"results\/{fn}-output.jpg\")), cv2.COLOR_BGR2RGB))","b648e77b":"\"\"\" Run this to train the model \"\"\"\n#[NOTE]: For representation purpose i am using a smaller dataset. Pls use the dataset given at the start of this notebook \n#for better results and change the dataset_path in this function.\n\nfast_trainer(style_image='.\/style\/picasso_selfportrait.jpg',style_name = 'Picasso_Selfportrait',\n             dataset_path='..\/input\/', epochs = 1)","d4263bbb":"test_image(image_path = '.\/content\/japanese_garden.jpg',\n           checkpoint_model = '.\/checkpoints\/best_model.pth',\n           save_path = '.\/')","de123dc0":"### Imports and Setup\nLet's download all the required files and import all modules.","25bb063f":"## The Problem: \nEach new content image will reset the generated image pixels and the process of pixel search needs to be done again. That makes the process very very slow and does not gurantee good results. Due these time and compute constraints, it cannot be implemented in production.\n\n\n## The Solution: \nThe solution is to generalize the approach, using something like a neural net that learns to apply a specific type of style on any input image. Although this approach is also not very good but it is much better than the previous one.\n\n**Advantages**:\n* Much faster than the traditional approach\n* requires us to train the model only once per style\n\n**Disadvantages**:\n* Each style requires its own weights for the model which means it requires a lot of space to save weights for each type of style.","5a204e73":"### To train run this:","534a3098":"## Result\nThe 3 best outputs from my models are:\n\n<img src=\"https:\/\/www.dropbox.com\/s\/8a7i1qufrn2th8i\/best_output1.jpg?raw=1\" width=\"500\" align = \"center\"\/>\n<center>Fig 8. Best Result 1 [More Weight to Style]<\/center>\n<br>\n<img src=\"https:\/\/www.dropbox.com\/s\/mmyikx154whtufj\/best_output2.jpg?raw=1\" width=\"500\" align = \"center\"\/> \n<center>Fig 9. Best Result 2 [Balanced Style and content]<\/center>\n<br>\n<img src=\"https:\/\/www.dropbox.com\/s\/h7nrahjbek3ajq1\/best_output3.jpg?raw=1\" width=\"500\" align = \"center\"\/>\n<center>Fig 10. Best Result 3 [More Weight to Content]<\/center>\n\nPlease find detailed experiment results [here.](https:\/\/drive.google.com\/drive\/folders\/13jTfhQVB2qojOD3cb9EF7-Uy_afYUbDE?usp=sharing)","8f873299":"## Requirements:\nFor smooth working of this notebook please use these settings. <br>\nCreate a new virtual environment and install these dependencies in it.\n1. Python == 3.7.6\n2. Torch == 1.5.1\n3. Torchvision == 0.6.0a0+35d732a\n4. Numpy == 1.18.1\n5. PIL == 5.4.1\n6. tqdm == 4.45.0 \n7. Matplotlib == 3.2.1\n8. OpenCV == 4.2.0.34\n9. CUDA Version == 10.1\n\n## Usage:\nRun the **fast_trainer** function to train your custom model or use the provided pretrained model with the test function, **test_image**, to generate results.","d7eebf6d":"## Training Loop\nThis is our main training loop. Here we follow a specific order of steps to train our neural net. The steps are as follows:\n1. First, the train dataloaders are initialized to provide us with the batches of data that the model will use to train on.\n2. Then the neural nets are initialized for usage.\n3. After that we initialize optimizer which will update the weights of the model and help in training. The optimizer takes a very important hyperparameter called **learning rate** which defines how intensly model weights are updated. A good learning rate marks the balance between slow training and overshooting.\n4. Next, we transform our input images to desired shape and keep a small set of 8 images aside for validation purpose. These 8 images are used to understand how the model training progresses.\n5. After this, the main process starts. The outer loop runs \"epochs\" number of times. The inner loop iterates over the batches provided by the dataloader. Model output is generated for the input image, loss is calculated for the whole batch and model weights are updated using back-propogation. All this runs multiple times in each epoch.\n6. During the training we keep saving the model weights and ouput of the model on the validation set we kept aside earlier.\n\nPlease find all my experiments and there outputs [here.](https:\/\/drive.google.com\/drive\/folders\/13jTfhQVB2qojOD3cb9EF7-Uy_afYUbDE?usp=sharing)","1feb8381":"## Fast Neural Style Transfer\n\n<img src=\"https:\/\/www.fritz.ai\/images\/fast_style_transfer_arch.jpg\" width=\"700\" align = \"center\"\/>\n<center>Fig 6. Working of TransformerNet and VGG16 for fast NST<\/center>\n<br>\n\nTraining a style transfer model requires two networks: a pre-trained feature extractor and a transfer network. The pre-trained feature extractor is used to avoid having to us paired training data. It\u2019s usefulness arises from the curious tendency for individual layers of deep convolutional neural networks trained for image classification to specialize in understanding specific features of an image.\n<br>\n\nThe pre-trained model enables us to compare the content and style of two images, but it doesn't actually help us create the stylized image. That\u2019s the job of a second neural network, which we\u2019ll call the transfer network. The transfer network is an image translation network that takes one image as input and outputs another image. Transfer networks typically have an encode-decoder architecture.\n<br>\n\nAt the beginning of training, one or more style images are run through the pre-trained feature extractor, and the outputs at various style layers are saved for later comparison. Content images are then fed into the system. Each content image passes through the pre-trained feature extractor, where outputs at various content layers are saved. The content image then passes through the transfer network, which outputs a stylized image. The stylized image is also run through the feature extractor, and outputs at both the content and style layers are saved.\n<br>\n\nThe quality of the stylized image is defined by a custom loss function that has terms for both content and style. The extracted content features of the stylized image are compared to the original content image, while the extracted style features are compared to those from the reference style image(s). After each step, only the transfer network is updated. The weights of the pre-trained feature extractor remain fixed throughout. By weighting the different terms of the loss function, we can train models to produce output images with lighter or heavier stylization. ","3cc81be8":"## Content Image\nThis is our content inspiration for final output image. The contents of final image will be similar to this.\n<br>\n\n<img src=\"https:\/\/www.dropbox.com\/s\/rpymcavlpb5iqo4\/japanese_garden.jpg?raw=1\" width=\"500\" align = \"center\"\/>\n<center>Fig 2. Content Image<\/center>\n<br>\n\n## Style Image\nThis is our style inspiration for our final output image. The style of final image will be similar to this.\n<br>\n\n<img src=\"https:\/\/www.dropbox.com\/s\/daqk93nltejhwfi\/picasso_selfportrait.jpg?raw=1\" width=\"300\" align = \"center\"\/>\n<center>Fig 3. Style Image<\/center>\n<br>","7899257c":"## Experiments\nI experimented with different layer formats and style and content weights and these are the results of each experiment.\n\n<img src=\"https:\/\/www.dropbox.com\/s\/ikaq1w8ywmurbk1\/Screenshot_2020-07-04%20yash-choudhary%20Neural-Style-Transfer.png?raw=1\" width=\"500\" align = \"center\"\/>\n<center>Table 1. Experiments<\/center>\n<br>\n<br>\nNow let's look the results of each experiment at different instances.\n<br>\n<br>\n<img src=\"https:\/\/www.dropbox.com\/s\/7plcfdag664z5k5\/grid.png?raw=1\" width=\"900\" align = \"center\"\/>\n<center>Fig 7. Best Result 1 [More Weight to Style]<\/center>","40a3354e":"## **How does NST work?**\n<img src=\"https:\/\/miro.medium.com\/max\/1294\/1*ZgW520SZr1QkGoFd3xqYMw.jpeg\" width=\"700\" align = \"center\"\/>\n<center>Fig 4. Working<\/center>\n<br>\n\nFirst, Let's discuss the traditional approach of neural style transfer first given by Gatys et al. in there paper \"A Neural Algorithm of Artistic Style\".It was built on a very neat idea that, \n\n    It is possible to separate the style representation and content representations in a CNN, learnt during a computer vision task (e.g. image recognition task).\n\nNeural style transfer uses a pretrained convolution neural network. Then to define a loss function which blends two images seamlessly to create visually appealing art, NST defines the following inputs:\n\n    1. A content image (c) \u2014 the image we want to transfer a style to\n    2. A style image (s) \u2014 the image we want to transfer the style from\n    3. An input (generated) image (g) \u2014 the image that contains the final result (the only trainable variable)\n\nThe basic idea behind this approach is that CNN pretrained on large image datasets develop an intuition of how images and objects in those images look in terms of content and style. The shallow layers of these networks are more concerned with content of the image like shapes and structural details. The deeper layers are good at understanding the texture and style of the image.\n<br>\n\n<img src=\"https:\/\/miro.medium.com\/max\/1400\/1*EvBcni8o_O3v4RUl640TZQ@2x.png\" width=\"700\" align = \"center\"\/>\n<center>Fig 5. Features extracted at different levels<\/center>\n<br>\n\nSo, content, style and generated images are passed through the network and the weigts of specific layers are compared using loss fuctions like content loss and style loss. \n<br>\n\n**Content Loss**: The content cost function is making sure that the content present in the content image is captured in the generated image. As CNNs capture information about content in the higher levels, where the lower levels are more focused on individual pixel values, we use the top-most CNN layer to define the content loss function.\n<br>\n\n**Style Loss**:To extract the style information from the VGG network, we use all the layers of the CNN. Furthermore, style information is measured as the amount of correlation present between features maps in a given layer. Next, a loss is defined as the difference of correlation present between the feature maps computed by the generated image and the style image.\n<br>\n\nThen, an optimizer back-propagates and updates the pixel values of the generated image and the process repeats. This process of searching for pixel values is very slow and not at all practical for styling multiple images.","bd2ab678":"# Neural Style Transfer\n\n<img src=\"https:\/\/www.dropbox.com\/s\/ocl42wcx9c6y5w1\/animate.gif?raw=1\" width=\"900\" align = \"center\"\/>\n<center>Fig 1. Neural Style Transfer<\/center>\n<br>\n\nAccording to Wikipedia, Neural Style Transfer (NST) also called Artistic Style Transfer refers to a class of software algorithms that manipulate digital images, or videos, in order to adopt the appearance or visual style of another image. NST algorithms are characterized by their use of deep neural networks for the sake of image transformation.\n<br>\n\n***In simple words, Neural style transfer is the process of creating art using computers. It is the process of painting the contents of one image with the style of another.***","7fb92284":"### To test on your own image run this:","7d5bc282":"## Utility functions\nThese functions help in the training process from preprocessing the input image to calculating the gram-matrix for loss calculation.","e9511395":"## Testing and Inference Loop\nAfter the model has been trained it can be used to generate outputs for desired inputs. Each model is trained on a single style and can produce images with that single style. That means we require multiple model, one model per style, if we want to use this in production. ","2acd3724":"### Important Links\n1. Train Dataset Link: http:\/\/images.cocodataset.org\/zips\/test2017.zip <br>\n2. Style Image: https:\/\/github.com\/myelinfoundry-2019\/challenge\/raw\/master\/picasso_selfportrait.jpg <br>\n3. Content Image: https:\/\/github.com\/myelinfoundry-2019\/challenge\/raw\/master\/japanese_garden.jpg <br>\n4. Best Model: https:\/\/www.dropbox.com\/s\/7xvmmbn1bx94exz\/best_model.pth?dl=1\n<br>\n<br>\n\n### References:\n1. [Style Transfer Guide](https:\/\/www.fritz.ai\/style-transfer\/)\n2. [Breaking Down Leon Gatys\u2019 Neural Style Transfer in PyTorch](https:\/\/towardsdatascience.com\/breaking-down-leon-gatys-neural-style-transfer-in-pytorch-faf9f0eb79db)\n3. [Intuitive Guide to Neural Style Transfer](https:\/\/towardsdatascience.com\/light-on-math-machine-learning-intuitive-guide-to-neural-style-transfer-ef88e46697ee)\n4. [A Neural Algorithm of Artistic Style By\nLeon A. Gatys, Alexander S. Ecker, Matthias Bethge](https:\/\/arxiv.org\/abs\/1508.06576)\n5. [Perceptual Losses for Real-Time Style Transfer and Super-Resolution by Justin Johnson, Alexandre Alahi, Li Fei-Fei](https:\/\/arxiv.org\/abs\/1603.08155)\n6. [Neural Style Transfer on Real Time Video (With Full implementable code)](https:\/\/towardsdatascience.com\/neural-style-transfer-on-real-time-video-with-full-implementable-code-ac2dbc0e9822)\n7. [Classic Neural Style Transfer](https:\/\/github.com\/halahup\/NeuralStyleTransfer)\n8. [Fast Neural Style Transfer using Lua](https:\/\/github.com\/lengstrom\/fast-style-transfer)\n9. [Fast Neural Style Transfer using Python](https:\/\/github.com\/eriklindernoren\/Fast-Neural-Style-Transfer)","6751c45e":"## Defining Models\nHere we have 2 models\n1. **VGG16**: Pre-trained model for feature extraction for loss comparisions.\n2. **TransformerNet**: The main model which acts as an encoder-decoder pair and learns to convert any image to a specific style."}}