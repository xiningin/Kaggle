{"cell_type":{"e1df1364":"code","68cf9b4a":"code","6c2f5e7d":"code","fc687507":"code","297d1818":"code","5c985e9f":"code","c1be9c3c":"code","6aa684b7":"code","b6aca7e1":"code","8ab5a0f5":"code","71ba965a":"code","2a3e5383":"code","14b232f0":"code","9ef78e6d":"code","5da8a9bf":"code","4eb28409":"code","faf36174":"code","573bb314":"code","f1a475ad":"code","be7b402d":"code","a6961dc0":"code","57469339":"code","9ee31b73":"code","1a07cb6c":"code","5495db05":"code","075c6244":"code","a33d230f":"code","306c1b47":"code","f9a2467b":"code","97359abe":"code","4917e2dc":"markdown","b3d2fc23":"markdown","40007d42":"markdown","45e30f82":"markdown","e4e96eca":"markdown","d389a4ce":"markdown","b1a17489":"markdown","ce842711":"markdown","1775b55c":"markdown"},"source":{"e1df1364":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","68cf9b4a":"import io\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom scipy import stats\nfrom sklearn.metrics import accuracy_score","6c2f5e7d":"df = pd.read_csv('\/kaggle\/input\/breast-cancer-prediction-dataset\/Breast_cancer_data.csv')","fc687507":"df.head()","297d1818":"df.columns","5c985e9f":"df.info()","c1be9c3c":"df.describe()","6aa684b7":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndf['diagnosis'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('diagnosis')\nax[0].set_ylabel('')\nsns.countplot('diagnosis',data=df,ax=ax[1])\nax[1].set_title('diagnosis')\nplt.show()\n\n\n# 1 is for malignant, 0 is for benign","b6aca7e1":"def diagnostic_plots(df, variable):## defining a function to plot histogram and Q-Q plot\n    plt.figure(figsize = (15,6))\n    plt.subplot(1,2,1)\n    sns.distplot(df[variable], fit=norm);\n    plt.subplot(1,2,2)\n    stats.probplot(df[variable], dist = 'norm', plot = plt)\n    plt.show()","8ab5a0f5":"diagnostic_plots(df, 'mean_radius')","71ba965a":"#applying log transformation\ndf['mean_radius'] = np.log(df['mean_radius'] + 1)# +1 is added in case there is any 0 input to it which would create issue in taking log\ndiagnostic_plots(df, 'mean_radius')","2a3e5383":"diagnostic_plots(df, 'mean_texture')","14b232f0":"#applying log transformation\ndf['mean_texture'] = np.log(df['mean_texture'] + 1)# +1 is added in case there is any 0 input to it which would create issue in taking log\ndiagnostic_plots(df, 'mean_texture')","9ef78e6d":"diagnostic_plots(df, 'mean_perimeter')","5da8a9bf":"#applying log transformation\ndf['mean_perimeter'] = np.log(df['mean_perimeter'] + 1)# +1 is added in case there is any 0 input to it which would create issue in taking log\ndiagnostic_plots(df, 'mean_perimeter')","4eb28409":"diagnostic_plots(df, 'mean_area')","faf36174":"#applying log transformation\ndf['mean_area'] = np.log(df['mean_area'] + 1)# +1 is added in case there is any 0 input to it which would create issue in taking log\ndiagnostic_plots(df, 'mean_area')","573bb314":"diagnostic_plots(df, 'mean_smoothness')","f1a475ad":"#applying log transformation\ndf['mean_smoothness'] = np.log(df['mean_smoothness'] + 1)# +1 is added in case there is any 0 input to it which would create issue in taking log\ndiagnostic_plots(df, 'mean_smoothness')","be7b402d":"#correlation matrix\ncorrmat = df.corr()\nf, ax = plt.subplots(figsize=(9, 7))\nsns.heatmap(corrmat, vmax=.8, square=True, annot = True);","a6961dc0":"#pairplot\nsns.set()\ncols = ['mean_radius', 'mean_texture', 'mean_perimeter', 'mean_area', 'mean_smoothness']\nsns.pairplot(df[cols], size = 2.5)\nplt.show();","57469339":"y = df['diagnosis']\nX = df.drop(['diagnosis'], axis = True)\ny.head(3)","9ee31b73":"#splitting dataset into train and test sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n","1a07cb6c":"#logistic regression\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nY_pred = logreg.predict(X_test)\naccu_reg = accuracy_score(y_test, Y_pred)\nprint(\"Accuracy score using Random Forest:\", accu_reg*100)","5495db05":"#support vector machine\nfrom sklearn.svm import SVC\nsvc = SVC()\nsvc.fit(X_train, y_train)\nY_pred = svc.predict(X_test)\naccu_svc = accuracy_score(y_test, Y_pred)\nprint(\"Accuracy score using Random Forest:\", accu_svc*100)","075c6244":"#knn\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\nY_pred = svc.predict(X_test)\naccu_knn = accuracy_score(y_test, Y_pred)\nprint(\"Accuracy score using Random Forest:\", accu_knn*100)","a33d230f":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\ngaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = accuracy_score(y_test, Y_pred)\nprint(\"Accuracy score using Random Forest:\", acc_gaussian*100)","306c1b47":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = accuracy_score(y_test, Y_pred)\nprint(\"Accuracy score using Random Forest:\", acc_decision_tree*100)","f9a2467b":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, y_train)\nacc_random_forest = accuracy_score(y_test, Y_pred)\nprint(\"Accuracy score using Random Forest:\", acc_random_forest*100)","97359abe":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', \n              'Decision Tree'],\n    'Score': [accu_svc, accu_knn, accu_reg, \n              acc_random_forest, acc_gaussian, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)","4917e2dc":"# Importing Libraries","b3d2fc23":"Breast cancer is cancer that forms in the cells of the breasts. It can be diagnosed by diagnosing the breast cells with the parameters given in the dataset and decide if it's malignant(1)(cancerous) or benign(0)(non-cancerous).\nWe will use Machine Learning Classification to predict if the cell is cancerous or not.","40007d42":"# Modelling","45e30f82":"# Reading the Dataset","e4e96eca":"Thank You for reading it till the end.","d389a4ce":"**Normalizing all the input variables**","b1a17489":"We have seen that Random Forest gives the maximum accuracy.","ce842711":"# Data Visualization","1775b55c":"Dataset is obtained from the folowing platform -\nhttps:\/\/www.kaggle.com\/merishnasuwal\/breast-cancer-prediction-dataset"}}