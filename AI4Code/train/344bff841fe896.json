{"cell_type":{"8f57d79f":"code","5316071f":"code","8ec67647":"code","b719b4de":"code","95be9bf0":"code","ffa9c093":"code","0c2b050c":"markdown","712d8378":"markdown","7531e727":"markdown","676ab3ac":"markdown","0133cec3":"markdown","b557fb7a":"markdown","9de58fbd":"markdown","7913954d":"markdown","cfec8961":"markdown"},"source":{"8f57d79f":"try:\n    import hugsvision\nexcept:\n    !pip install -q hugsvision\n    import hugsvision\n    \nprint(hugsvision.__version__)","5316071f":"import os\nimport shutil\nimport pandas as pd\nmetadata = pd.read_csv(\"HAM10000_metadata.csv\").set_index('image_id').T.to_dict('list')\nfor current_dir in [\"HAM10000_images_part_1\",\"HAM10000_images_part_2\"]:\n    for image_path in os.listdir(current_dir):\n        label = metadata[image_path.split(\".\")[0]][1]\n        os.makedirs(\".\/HAM10000\/\" + label, exist_ok=True)\n        shutil.copy2(current_dir + \"\/\" + image_path, \".\/HAM10000\/\" + label + \"\/\" + image_path)","8ec67647":"from hugsvision.dataio.VisionDataset import VisionDataset\n\ntrain, test, id2label, label2id = VisionDataset.fromImageFolder(\n\t\"\/users\/ylabrak\/datasets\/HAM10000\/\",\n\ttest_ratio = 0.10,\n\tbalanced = True,\n\ttorch_vision = True,\n)","b719b4de":"from hugsvision.nnet.TorchVisionClassifierTrainer import TorchVisionClassifierTrainer\n\ntrainer = TorchVisionClassifierTrainer(\n\toutput_dir   = \".\/out_torchvision\/HAM10000\/\",\n\tmodel_name   = 'densenet121',\n\ttrain      \t = train,\n\ttest      \t = test,\n\tbatch_size   = 64,\n\tmax_epochs   = 100,\n\tid2label \t = id2label,\n\tlabel2id \t = label2id,\n\tlr=1e-3,\n)","95be9bf0":"hyp, ref = trainer.evaluate_f1_score()","ffa9c093":"from PIL import Image\nfrom hugsvision.inference.TorchVisionClassifierInference import TorchVisionClassifierInference\n\nclassifier = TorchVisionClassifierInference(model_path = \".\/OUT_TORCHVISION\/HAM10000\/\")\n\nprint(\"Predicted class:\", classifier.predict(img_path=\"\/users\/ylabrak\/datasets\/HAM10000\/bcc\/ISIC_0024331.jpg\"))\nprint(\"Predicted class:\", classifier.predict_image(img=Image.open(\"\/users\/ylabrak\/datasets\/HAM10000\/bcc\/ISIC_0024331.jpg\")))","0c2b050c":"## Train the model\n\nSo, once the model choosen, we can start building the `Trainer` and start the fine-tuning.\n\n**Note**: Some architectures need to apply a transformation function to your training data to fit the model requirements. Globally, most of the models are expecting `224x224` resolutions but there's some counter examples in which you will need to add:\n\n```python\ntorch_vision=True,\ntransform=transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n])\n```\n","712d8378":"## Convert Data\n\nOnce it has been downloaded, you need to converted the data to a directory based format:","7531e727":"## Choose a image classifier model on TorchVision\n\nNow we can choose our base model on which we will perform a fine-tuning to make it fit our needs.\n\nSo, to be sure that the model will be compatible with `HugsVision` we need to have a model exported in `PyTorch` and listed in the [TORCHVISION.MODELS](https:\/\/pytorch.org\/vision\/stable\/models.html) section of the `PyTorch` documentation.\n\nYou can also find the list of available models by typing directly in Python:\n\n```python\nimport torchvision.models as models\nprint(models.__dict__.keys())\n```\n\nOutput:\n\n```python\ndict_keys(['alexnet', 'resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'resnext50_32x4d', 'resnext101_32x8d', 'wide_resnet50_2', 'wide_resnet101_2', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19_bn', 'vgg19', 'squeezenet1_0', 'squeezenet1_1', 'inception_v3', 'densenet121', 'densenet169', 'densenet201', 'densenet161', 'googlenet', 'mobilenet_v2', 'mobilenet_v3_large', 'mobilenet_v3_small', 'mnasnet0_5', 'mnasnet0_75', 'mnasnet1_0', 'mnasnet1_3', 'shufflenet_v2_x0_5', 'shufflenet_v2_x1_0'])\n```\n\nOthers models such as ```shufflenet_v2_x1_5 shufflenet_v2_x2_0``` wasn't implemented yet by the PyTorch team and return a ```NotImplementedError```.","676ab3ac":"# Quickstart Guide\n\nThis guide will give a quick intro to training TorchVision models such as VGG16, Inception V3, Resnet50, AlexNet and much more with HugsVision. We'll start by loading in some data and defining a model, then we'll train it for a few epochs and see how well it does.\n\n**Note**: The easiest way to use this tutorial is as a colab notebook, which allows you to dive in with no setup. We recommend you enable a free GPU with\n\n> **Runtime** \u2006\u2006\u2192\u2006\u2006 **Change runtime type** \u2006\u2006\u2192\u2006\u2006 **Hardware Accelerator: GPU**\n\n**Note**: You need to have at least Python 3.6 to run the scripts.\n\n## Install HugsVision\n\nFirst we install HugsVision if needed. ","0133cec3":"## Downloading Data\n\nFirst, we need to download the dataset called `Skin Cancer MNIST: HAM10000` [here](https:\/\/www.kaggle.com\/kmader\/skin-cancer-mnist-ham10000) which weight around ~3.0 GB.","b557fb7a":"## Evaluate F1-Score\n\nUsing the F1-Score metrics will allow us to get a better representation of predictions for all the labels and find out if their are any anomalies wit ha specific label.\n\nBy default, the trainer is running a evaluation after each epoch with the latest model and at the end of training phase with the best model but you can also run it manually if you want to collect the list of predictions after the `argmax` function.","9de58fbd":"## Loading Data\n\nOnce it has been converted, we can start loading the data.","7913954d":"## Make a prediction","cfec8961":"```\n              precision    recall  f1-score   support\n\n       akiec       0.80      0.80      0.80        10\n         bcc       0.80      0.89      0.84         9\n         bkl       0.80      0.80      0.80        10\n          df       1.00      1.00      1.00        15\n         mel       0.75      0.67      0.71         9\n          nv       0.92      0.86      0.89        14\n        vasc       0.93      1.00      0.97        14\n\n    accuracy                           0.88        81\n   macro avg       0.86      0.86      0.86        81\nweighted avg       0.88      0.88      0.88        81\n```"}}