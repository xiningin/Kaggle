{"cell_type":{"f5aad811":"code","92ee1f06":"code","fd58e9e3":"code","4e537a78":"code","1b6100cb":"code","ce63b5d0":"code","5ac9cb6c":"code","25dd8be4":"code","c335435e":"code","f5a42a2e":"code","82b1c7c8":"code","f67dd816":"code","91cfc547":"code","ec95da21":"code","4e27cc7b":"code","393c6228":"code","08621edc":"code","5e339983":"code","96e90b02":"code","a771a2a5":"code","cf1fae74":"code","87dae07a":"code","bb9f4646":"code","f4cc86fb":"code","be993cfc":"code","f285927b":"code","88b2d17d":"code","c4f314a1":"code","6153b750":"markdown","3bca6679":"markdown","aabddb3f":"markdown","16917318":"markdown","bc2cfa42":"markdown","b369da52":"markdown","e43de9e7":"markdown"},"source":{"f5aad811":"# Install pytorch-tabnet\n!pip install pytorch-tabnet","92ee1f06":"#importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pytorch_tabnet.tab_model import TabNetClassifier\nfrom pytorch_tabnet.pretraining import TabNetPretrainer\nimport torch\nfrom sklearn.metrics import roc_auc_score, accuracy_score\nimport phik\nfrom phik.report import plot_correlation_matrix\nfrom phik import report\n\nimport copy","fd58e9e3":"def reduce_memory_usage(df):\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != 'object':\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    pass\n        else:\n            df[col] = df[col].astype('category')\n    \n    return df","4e537a78":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/train.csv')\nreduce_memory_usage(train)\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/test.csv')\nreduce_memory_usage(test)\n# remove the only 5 cover type target\ntrain = train[train.Cover_Type!=5].reset_index(drop=True)\ntrain.loc[train.Cover_Type == 4, 'Cover_Type'] = 3\nprint(train.shape)\nprint(test.shape)","1b6100cb":"train.Cover_Type.value_counts()","ce63b5d0":"train[\"Aspect\"][train[\"Aspect\"] < 0] += 360\ntrain[\"Aspect\"][train[\"Aspect\"] > 359] -= 360\n\ntest[\"Aspect\"][test[\"Aspect\"] < 0] += 360\ntest[\"Aspect\"][test[\"Aspect\"] > 359] -= 360","5ac9cb6c":"train.loc[train[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\ntest.loc[test[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n\ntrain.loc[train[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\ntest.loc[test[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n\ntrain.loc[train[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\ntest.loc[test[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n\ntrain.loc[train[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\ntest.loc[test[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n\ntrain.loc[train[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\ntest.loc[test[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n\ntrain.loc[train[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\ntest.loc[test[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255","25dd8be4":"# a = train.nunique().reset_index(drop=False).rename(columns={\"index\": \"feat_name\", 0: \"count\"})\n# # drop columns with a single value\n# drop_cols = [\"Id\"] + list(a[a[\"count\"] < 2 ].feat_name)\n# target = [\"Cover_Type\"]","c335435e":"# # categorical features are columns with small modalities\n# cat_features = [col for col in list(a[a[\"count\"] < 10 ].feat_name) if col not in drop_cols+target]\n# num_features = [col for col in train.columns if col not in drop_cols+target+cat_features]\n\n# features = cat_features + num_features","f5a42a2e":"# phik_overview_num_features = train[target+num_features].phik_matrix()\n# phik_overview_num_features.round(2)","82b1c7c8":"features_Hillshade = ['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']\nfeatures_for_generation = ['Elevation',\n#                            'Aspect',\n#                            'Slope',\n                           'Horizontal_Distance_To_Hydrology',\n                           'Vertical_Distance_To_Hydrology',\n                           'Horizontal_Distance_To_Roadways',\n#                            'Hillshade_9am',\n#                            'Hillshade_Noon',\n#                            'Hillshade_3pm',\n                           'Horizontal_Distance_To_Fire_Points']\n# soil_features = [x for x in train.columns if x.startswith(\"Soil_Type\")]\nwilderness_features = [x for x in train.columns if x.startswith(\"Wilderness_Area\")]\n\ndef add_num_Feature(X):\n    # Thanks @mpwolke : https:\/\/www.kaggle.com\/mpwolke\/tooezy-where-are-you-no-camping-here\n#     X[\"Soil_Count\"] = X[soil_features].apply(sum, axis=1)\n\n#     # Thanks @yannbarthelemy : https:\/\/www.kaggle.com\/yannbarthelemy\/tps-december-first-simple-feature-engineering\n    X[\"Wilderness_Area_Count\"] = X[wilderness_features].apply(sum, axis=1)\n    X[\"Hillshade_mean\"] = X[features_Hillshade].mean(axis=1)\n    X['amp_Hillshade'] = X[features_Hillshade].max(axis=1) - X[features_Hillshade].min(axis=1)\n    for featr in features_for_generation:\n        X[featr+'^2'] = X[featr]**2\n        X[featr+'^3'] = X[featr]**3\n#         X[featr+'log'] = np.log(X[featr])\n#         for ft in features_for_generation:            \n#             X[featr+'_'+ft] = X[featr] * X[ft]\n    return X.copy()","f67dd816":"add_num_Feature(train)\nreduce_memory_usage(train)","91cfc547":"add_num_Feature(test)\nreduce_memory_usage(train)","ec95da21":"# phik_overview_cat_features = train[target+cat_features].phik_matrix()\n# phik_overview_cat_features.round(2)","4e27cc7b":"# train['Wilderness_Area4+Soil_Type39'] = train['Wilderness_Area4']*train['Soil_Type39']\n# test['Wilderness_Area4+Soil_Type39'] = test['Wilderness_Area4']*test['Soil_Type39']","393c6228":"# cat_features_for_generation = ['Wilderness_Area1',\n#                                'Wilderness_Area3',\n#                                'Wilderness_Area4',\n#                                'Soil_Type10',\n#                                'Soil_Type39']\n\n# def add_cat_Feature(X):\n#     for featr in cat_features_for_generation:        \n#         for ft in cat_features_for_generation:\n#             if ft != featr:\n#                 X[featr+'_'+ft] = (X[featr] * X[ft])\n#     return X.copy()","08621edc":"# add_cat_Feature(train)\n# reduce_memory_usage(train)","5e339983":"# add_cat_Feature(test)\n# reduce_memory_usage(test)","96e90b02":"a = train.nunique().reset_index(drop=False).rename(columns={\"index\": \"feat_name\", 0: \"count\"})\n# drop columns with a single value\ndrop_cols = [\"Id\"] + list(a[a[\"count\"] < 2 ].feat_name)\ntarget = [\"Cover_Type\"]","a771a2a5":"# categorical features are columns with small modalities\ncat_features = [col for col in list(a[a[\"count\"] < 10 ].feat_name) if col not in drop_cols+target]\nnum_features = [col for col in train.columns if col not in drop_cols+target+cat_features]\n\nfeatures = cat_features + num_features","cf1fae74":"from sklearn.preprocessing import RobustScaler\n\nscaler = RobustScaler()\ntrain[num_features] = scaler.fit_transform(train[num_features])\ntest[num_features] = scaler.transform(test[num_features])","87dae07a":"# This is only needed if using embeddings (not used at the moment)\n\nfrom sklearn.preprocessing import LabelEncoder\n\ncategorical_columns = []\ncategorical_dims =  {}\nfor col in cat_features:\n    l_enc = LabelEncoder()\n    train[col] = train[col].fillna(\"VV_likely\")\n    train[col] = l_enc.fit_transform(train[col].values)\n    categorical_columns.append(col)\n    categorical_dims[col] = len(l_enc.classes_)\n    \n    test[col] = l_enc.transform(test[col].values)\n    \ncat_idxs = [i for i, f in enumerate(features) if f in cat_features]\ncat_dims = [categorical_dims[f] for i, f in enumerate(features) if f in cat_features]\n\nX_test = test[features].values","bb9f4646":"BS = 1024*16\nVBS = BS \nmax_epochs=55\n\ntabnet_params = {\"n_d\" : 64,\n                 \"n_a\" : 64,\n                 \"n_steps\" : 5,\n                 \"gamma\" : 1.5,\n                 \"n_independent\" : 2,\n                 \"n_shared\" : 2,\n                 \"cat_idxs\" : cat_idxs,\n                 \"cat_dims\" : cat_dims,\n                 \"cat_emb_dim\" : 1,\n                 \"lambda_sparse\" : 1e-4,\n                 \"momentum\" : 0.3,\n                 \"clip_value\" : 2.,\n                 \"optimizer_fn\" : torch.optim.Adam,\n                 \"optimizer_params\" :dict(lr=2e-2),}\n\n\nparams = copy.deepcopy(tabnet_params)\nparams[\"scheduler_fn\"]=torch.optim.lr_scheduler.StepLR\nparams[\"scheduler_params\"]={\"is_batch_level\":False,\n                            \"gamma\":0.95,\n                            \"step_size\": 1,}","f4cc86fb":"# Pretrain the model on test set\n\nX_unsup_valid = train[features].values[:100000]\nparams = tabnet_params.copy()\n\nunsupervised_model = TabNetPretrainer(**params)\n\nunsupervised_model.fit(\n    X_train=X_test,\n    eval_set=[X_unsup_valid],\n    pretraining_ratio=0.8,\n    max_epochs=25,\n    patience=13,\n    batch_size=1024*4,\n    virtual_batch_size=1024*4\n)\n","be993cfc":"\n# Split for cross validation or single validation\nfrom sklearn.model_selection import StratifiedKFold\n\nN_SPLITS=5\nskf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=0)\n\ncv_preds = np.zeros((X_test.shape[0], N_SPLITS))\n\nfold_idx=0\nfor train_idx, val_idx in skf.split(train, train[target]):\n\n    # Create the numpy datasets\n\n    X_train = train.loc[train_idx, features].values\n    Y_train = train.loc[train_idx, target].values.reshape(-1)\n\n    X_val = train.loc[val_idx, features].values\n    Y_val = train.loc[val_idx, target].values.reshape(-1)\n\n    # Train a tabnet classifier\n\n    params = copy.deepcopy(tabnet_params)\n\n    # Scheduling scheme here is the only part not similar to the original paper\n    # but the dataset is not exactly the same\n\n    # params[\"scheduler_fn\"]=torch.optim.lr_scheduler.StepLR\n    # params[\"scheduler_params\"]={\"is_batch_level\":False,\n    #                             \"gamma\":0.95,\n    #                             \"step_size\": 5,}\n    params[\"scheduler_fn\"]=torch.optim.lr_scheduler.OneCycleLR\n    params[\"scheduler_params\"]={\"is_batch_level\":True,\n                                \"max_lr\":5e-2,\n                                \"steps_per_epoch\":int(X_train.shape[0] \/ BS),\n                                \"epochs\":max_epochs}\n\n    clf = TabNetClassifier(**params)\n\n    clf.fit(\n        X_train,\n        Y_train,\n        eval_set=[(X_train, Y_train), (X_val, Y_val)],\n        eval_name=['train', 'valid'],\n        eval_metric=['accuracy'],\n        max_epochs=max_epochs,\n        patience=20,\n        drop_last=True,\n        batch_size=BS,\n        virtual_batch_size=VBS,\n    #     weights=1,\n        from_unsupervised=unsupervised_model\n    )\n    \n    preds = clf.predict(X_test)\n    cv_preds[:, fold_idx] = preds\n    fold_idx+=1","f285927b":"# Voting ensembling\n\nfrom scipy import stats\nfinal_res, _ = stats.mode(cv_preds, axis=1)","88b2d17d":"df_submission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/sample_submission.csv')\ndf_submission['Cover_Type']= final_res.astype(int)\n# df_submission.loc[df_submission.Cover_Type == 4, 'Cover_Type'] = 3\ndf_submission.to_csv('submission3.csv',index=False)","c4f314a1":"df_submission.Cover_Type.value_counts()","6153b750":"**Phik (\ud835\udf19k) correlation for FE** \n\nexploring num features using phik correlation","3bca6679":"**A quick Google search about Hillshade leads to the following result:**\n\nHillshading computes surface illumination as values from 0 to 255 based on a given compass direction to the sun (azimuth) and a certain altitude above the horizon (altitude). Hillshades are often used to produce maps that are visually appealing.\n\nThus, hillshade is a 3D representation of a terrain which is used to gain insight about its form by measuring luminosity of certain patches of that terrain that results when a source of light is casted at a particular angle.\n\nMore Information about hillshade here\n\nIn both train and test datasets, there are certain rows with hillshade value more than 255 or less than 0. They must be the result of recording error and should be relpaced with an appropriate value. Perhaps, values less than 0 refer to the darkest shade and replacing them with 0 should be fine. Similarly, we can assume that hillshade values more than 255 refer to the brightest shades and a value of 255 should be good replacement.","aabddb3f":"# About this notebook\n\nThis notebook is a simple pipeline using pytorch-tabnet (https:\/\/github.com\/dreamquark-ai\/tabnet) following the original paper's parameters (https:\/\/arxiv.org\/abs\/1908.07442).\n\nIt performs pretraining on test set and standard 5 fold cross validation with voting ensembling of the folds.\n\nAlmost no preprocessing is done (except from removing class 5 row and ignoring trivial columns), no feature engineering is done.\n\nThis is just a very basic starting pipeline.\n","16917318":"exploring cat features using phik correlation","bc2cfa42":"**EDA**\n* Aspect is the compass direction that a terrain faces. Here, It is expressed in degrees. All the values from 0 to 359 are present. Besides, there are some values greater than 359 and some smaller than 0. It will be better If we make all the values in this column lie in the range (0, 359). Moreover, all the values in this column lies in the range (-360, 720) so adding 360 to angles smaller than 0 and subtracting 360 from angles greater than 359 will do the work.","b369da52":"add cat features that correlate with the target","e43de9e7":"add num features that correlate with the target"}}