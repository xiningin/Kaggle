{"cell_type":{"f30aedb2":"code","399e511f":"code","f267f2a4":"code","74f0a0a7":"code","42a43704":"code","190e1082":"code","ac343e65":"code","6a4b53f4":"code","f5019dfd":"code","0dde4869":"code","b4899adb":"code","d91c0b5c":"code","e8c63927":"code","3e653832":"code","188cebcb":"code","71c9c3a6":"code","273063ce":"code","99f73a7d":"code","a86e189d":"code","e0792946":"code","1b7b5176":"code","ac4ed44c":"code","5d857e09":"code","ae9a3ab0":"code","dad043b7":"code","d531b6aa":"code","d2581b51":"code","49aa2a3e":"code","48fe8a4a":"code","00446c4d":"code","c8baf1d9":"code","99466169":"code","f6185766":"code","9b993575":"code","a8c06ea3":"code","215c6489":"code","c83d2561":"code","bc01fc35":"code","3abf50e6":"code","b0fb4363":"code","d3158a22":"code","fee40711":"code","b3b29750":"code","94145d5e":"markdown","fbafd568":"markdown","814986c1":"markdown","b9db3baa":"markdown","4ba5e9c8":"markdown","5cdacf1f":"markdown","432cdec7":"markdown","50bc3dba":"markdown","e56b3857":"markdown","2a27171f":"markdown","70bcdf70":"markdown","dbf2d180":"markdown","649d1213":"markdown","61feee89":"markdown","778e1fdd":"markdown","64c880b8":"markdown","46c6f1e9":"markdown","d50ee805":"markdown","95a88b6b":"markdown","21c343aa":"markdown","d9416e3f":"markdown"},"source":{"f30aedb2":"#refer to these files for the graph\nimport pandas as pd\ndf = pd.read_csv(\"..\/input\/results\/results_1592013931.804742.csv\")\ndf[:5]","399e511f":"RUN_PARTIAL_CACHED_RESULTS = True","f267f2a4":"# Install dependent libraries\n\n!pip install spacy --quiet\n!python -m spacy download en_core_web_sm\n!pip install flashtext --quiet\n!pip install textacy --quiet\n!pip install scispacy --quiet","74f0a0a7":"# Create directories to store results\n\nimport os\noutput_prefix = None\nresults_dir = \"results\" if output_prefix is None else output_prefix + \"results\"\nsearch_results_dir =  results_dir + \"\/search_results\"\nif not os.path.exists(results_dir):\n    os.mkdir(results_dir)\n\nif not os.path.exists(search_results_dir):\n    os.mkdir(search_results_dir)\n    \n#copy cached filtered results into working dir\n!cp \/kaggle\/input\/intermediateresults\/*.txt \/kaggle\/working\/results\/search_results\/","42a43704":"import tqdm\nfrom tqdm import tqdm\nimport json\nimport glob\nfrom flashtext import KeywordProcessor\nimport multiprocessing as mp\nimport os\nimport typing\nfrom typing import List\n\noutput_prefix = \"\/kaggle\/working\/\"\ndataset_path = \"\/kaggle\/input\/CORD-19-research-challenge\/document_parses\/pdf_json\/\"\n\nresults_dir = \"results\" if output_prefix is None else output_prefix + \"results\"\nsearch_results_dir =  results_dir + \"\/search_results\"\ncovid19_words = [ \"covid19\", \"corona virus\", \"covid\", \"corona\"]\n\ncovid19_words_processor = KeywordProcessor()\n\nCOVID_PAPERS_ONLY = True\n\ndef init():\n    '''Creates results dir and search results dir if they dont exist\n    Args: None\n    Returns: None\n    '''\n    if not os.path.exists(results_dir):\n        os.mkdir(results_dir)\n\n    if not os.path.exists(search_results_dir):\n        os.mkdir(search_results_dir)\n\n    if COVID_PAPERS_ONLY:\n        for term in covid19_words:\n            covid19_words_processor.add_keyword(term)\n            \n    return\n\ndef process_file(fname: str, dummy_arg: str)-> str:\n    '''Reads a single file and checks for existense of terms\n    Args:\n        fname(str): name of the file to be processed\n        dummy_arg(str): for future use to enable conditional search\n    Returns: filename(str) if terms exist in the file, otherwise None\n    '''\n    data = {}\n    covid19_keywords = []\n    try:\n        with open(fname,\"r\") as fp:\n            data = json.load(fp)\n        fp.close()\n    except Exception as e:\n        print('Failed to read file:' + str(e))\n        return None\n    \n    #data is a dict, so stringify it\n    data1 = json.dumps(data)\n    \n    #check existence of covid19 papers\n    if COVID_PAPERS_ONLY:\n        covid19_keywords = covid19_words_processor.extract_keywords(data1)\n    \n    #check for existence of keywords\n    keywords_found = keyword_processor.extract_keywords(data1)\n        \n    if COVID_PAPERS_ONLY and covid19_keywords:\n        if keywords_found:\n            return fname\n        \n    if not COVID_PAPERS_ONLY:\n        if keywords_found:\n            return fname\n        \n    return None\n\ndef process_files(files: List):\n    '''Processes all input files\n    Args:\n        files(list): list of foes to be processed\n    Returns:\n        None\n    '''\n    for f in files:\n        op = process_file(f)\n\n    return\n    \ndef get_filelist(op_filename:str) -> List:\n    '''Processes input files to identify files of interest.Dumps the filenames\n        into a flat file for further processing\n    Args:\n        op_filename(str): name of the outputfile where matched files are stored\n    Returns:\n        filenames: list of files which match the terms\n    '''\n\n    #ensure results dir exists\n    init()\n    \n    files = glob.glob(dataset_path + \"\/**\/*.json\", recursive=True)\n    total_files = len(files)\n    print(\"total files \", total_files)\n    files = list(map(lambda x: x.strip(), files))\n    \n      \n    pbar = tqdm(total = total_files)\n    pool = mp.Pool()\n    filenames = []\n    results = []\n        \n    def collect_results(result):\n        if result is not None:\n            filenames.append(result)\n        pbar.update()\n        return\n\n    results = [pool.apply_async(process_file, args=(f,\"\"), callback=collect_results) for f in files]\n    pool.close()\n    pool.join()\n    pbar.close()\n    \n    print(\"total files selected: \", len(filenames))\n    with open(op_filename, \"w\") as fp:\n        for fn in filenames:\n            fp.write(fn + \"\\n\")\n        fp.close()\n        \n    return len(filenames)","190e1082":"import spacy\n#import sent2vec\n\ndef singleton():\n    def singleton_decorator(func):\n        return _singleton_wrapper(func)\n\n    return singleton_decorator\n\n\ndef _singleton_wrapper(func):\n    value = None\n\n    def wrapper(*args, **kwargs):\n        nonlocal value\n\n        if value is None:\n            value = func(*args, **kwargs)\n\n        return value\n\n    return wrapper\n\n@singleton()\ndef load_spacy_model():\n    return spacy.load('en_core_web_sm')\n\n@singleton()\ndef load_scispacy_model():\n    return spacy.load(\"\/kaggle\/input\/scispacymodels\/en_core_sci_md-0.2.4\/en_core_sci_md\/en_core_sci_md-0.2.4\")","ac343e65":"#src.nlp.nlp_ops\n\nimport spacy\nfrom scipy.spatial import distance\nfrom typing import List, Dict, Tuple\nimport textacy\nimport textacy.keyterms\n\nclass do_nlp(object):\n\n    def __init__(self, text=None):\n        self.nlp = load_spacy_model()\n        self.scispacy_nlp = load_scispacy_model()\n        if text is not None:\n            self.doc = self.nlp(text)\n        return\n    \n    def get_phrases_sgrank(self, text: str, num_terms:int = 10) -> List[Tuple[str,float]]:\n        phrases: List[Tuple[str,float]]\n        text = text.lower()\n        doc = self.nlp(text)\n        phrases = textacy.keyterms.sgrank(doc, ngrams=(2, 4), normalize='lower', n_keyterms=num_terms)\n\n        return phrases\n\n\n    def get_entities(self, text=None, union=False, stem=True):\n        \"\"\"\n        Use NER to get entities\n        Args:\n                text (str): text from which entities need to be extracted\n                union (bool): merges(union) of all entities in a single list\n        Returns:\n                dict: list of entities for each entity type\n                list: list of merged(union of) entities\n        \"\"\"\n        #fricles_logger.info('Extracting entities...started')\n        entities = {}\n        union_entities = []\n        if text is not None:\n            self.doc = self.nlp(text)\n\n        for ent in self.doc.ents:\n            if ent.label_ not in entities:\n                entities[ent.label_] = []\n            if ent.text not in entities[ent.label_]:\n                entities[ent.label_].append(ent.text)\n            if union and ent.text not in union_entities:\n                union_entities.append(ent.text)\n\n        #fricles_logger.info('Extracting entities...done!')\n        return entities, union_entities\n\n    def get_noun_chunks(self, text=None):\n        \"\"\"\n        Get noun chunks\n        Args:\n                text (str): text from which noun chunks need to be extracted\n        Returns:\n                dict: list of noun chunks\n        \"\"\"\n        #fricles_logger.info('Extracting noun chunks...started!')\n        noun_chunks = []\n\n        if text is not None:\n            self.doc = self.nlp(text)\n\n        for chunk in self.doc.noun_chunks:\n            if chunk.text not in noun_chunks:\n                noun_chunks.append(chunk.text)\n\n        #fricles_logger.info('Extracting noun chunks...done!')\n        return noun_chunks\n    \n    def get_cosine_sim(self, src:str, target_set:Dict) -> Dict:\n        '''Get sorted cosine sim score for a list of text descriptions against a single source\n        Args:\n            src(str): string to be matched against\n            target_set(Dict): dict of labels and descriptions to be used for scoring\n        Returns:\n            op(Dict): dict of scores against labels in target_set\n        '''\n        op = {}\n        sorted_op = {}\n        try:\n            src = src[:1000000]\n            src_vec = self.scispacy_nlp(src).vector\n            for label, text in target_set.items():\n                tgt_vec = self.scispacy_nlp(text).vector\n                sim_score = 1 - distance.cosine(src_vec, tgt_vec)\n                op[label] = sim_score\n                sorted_op = sorted(op.items(), key=lambda x: x[1], reverse=True)\n\n            final_op = []\n            tolerance = 0.03\n            #get the first value which is the highest score\n            highest_score = sorted_op[0][1]\n            threshold = highest_score - (highest_score * tolerance)\n            for vals in sorted_op:\n                if vals[1] > threshold:\n                    final_op.append(vals[0])\n        \n            return \",\".join(final_op), sorted_op                   \n        except Exception as e:\n            print(\"Failed to get cosine sim scores:\" + str(e))\n            \n        return None, None","6a4b53f4":"#src.nlp.cleanup\nfrom textacy import preprocessing\nimport re\nfrom typing import Dict, Pattern\n\n\ndef cleanup(txt: str) -> str:\n    \n    def norm_whitespace(txt):\n        return \" \".join(txt.split())\n\n    op = norm_whitespace(txt)\n    \n    op = preprocessing.replace.replace_urls(op,\"\")\n    op = preprocessing.replace.replace_emails(op, \"\")\n    \n    txt = \"The copyright holder for this preprint (which was not peer-reviewed) is the\".lower()\n    op = re.sub(txt,'',op)\n\n    pat = re.compile(\"doi\\:?|(arxiv|biorxiv|comm_use_subset|non_comm_use_subset|custom_license|medrxiv)|preprint|table\\s?\\d?\\:?|figure\\s?\\d?\\:?\", re.IGNORECASE)\n    op1 = pat.sub(\"\",op)\n    \n    op = norm_whitespace(op1)\n\n    pat = re.compile(r\"in (image|fig[ure]*)\\.*\\s*\\d+\\s*.*\\w+\\s*(and)*\\s*\\(\\w+\\)\", re.IGNORECASE)\n    op1 = pat.sub(\"\",op)\n    op = norm_whitespace(op1)\n    \n    return op","f5019dfd":"#token_matcher\n\nimport spacy\nfrom spacy.matcher import Matcher\nfrom spacy.tokens import Span\nimport typing\nfrom typing import List\n\n#load nlp spacy model\nnlp = load_spacy_model()\nmatcher_pandemic = Matcher(nlp.vocab)\nmatcher_study_type = Matcher(nlp.vocab)\nmatcher = Matcher(nlp.vocab)\n\nmatched_sents = []\n\ndef collect_sents(matcher, doc:spacy.tokens.doc.Doc, i:int, matches:List[Matcher]) -> List:\n    \"\"\"                                                                                                                                                                                           \n    Callback function to collect spans of interest from sentences in a block of text.                                                                                                             \n                                                                                                                                                                                                  \n    Args:                                                                                                                                                                                         \n        matcher (spacy.Matcher): matcher object set to nlp.vocab                                                                                                                                  \n        doc (spacy.tokens.Doc): spacy Doc object                                                                                                                                                  \n        i (int): index for the match                                                                                                                                                              \n        matches (List): list of matches                                                                                                                                                           \n                                                                                                                                                                                                  \n    Returns:                                                                                                                                                                                      \n        None                                                                                                                                                                                      \n    \"\"\"\n    match_id, start, end = matches[i]\n    # Matched span                                                                                                                                                                                \n    span = doc[start:end]\n    # Sentence containing matched span                                                                                                                                                            \n    sent = span.sent\n\n    # get the match span by ofsetting the start and end of the span with the                                                                                                                      \n    # start and end of the sentence in the doc                                                                                                                                                    \n    string_id = nlp.vocab.strings[match_id]\n    start_span = span.start_char - sent.start_char,\n    end_span =  span.end_char - sent.start_char,\n\n    match_ents = [{\n        \"span\" : str(span),\n        \"label\": string_id\n    }]\n    matched_sents.append({\"text\": sent.text, \"ents\": match_ents})\n\n    return\n\ndef add_patterns(pattern_types:List) -> None:\n    '''                                                                                                                                                                                           \n    Add predefined patterns to the pattern library                                                                                                                                                \n                                                                                                                                                                                                  \n    Args:                                                                                                                                                                                         \n        pattern_types (List): list of the patterns of interest                                                                                                                                    \n                                                                                                                                                                                                  \n    Returns:                                                                                                                                                                                      \n        None                                                                                                                                                                                      \n    '''\n    for pattern_type in pattern_types:\n            #matcher_pandemic.add(pattern_type['LABEL'], collect_sents, pattern_type['pattern'])\n            matcher.add(pattern_type['LABEL'], collect_sents, pattern_type['pattern'])\n    return pattern_types\n\n\ndef find_matches(content:List, matcher_type=\"pandemic\") -> List:\n    \"\"\"                                                                                                                                                                                           \n    Detects patterns of interest from a list of blocks of text                                                                                                                                    \n    Args:                                                                                                                                                                                         \n        content (List): list of text blocks from which patterns need to be detected                                                                                                               \n    Returns:                                                                                                                                                                                      \n        op (List): list of matched patterns with spans and related info                                                                                                                           \n    \"\"\"\n    global matched_sents\n\n    for txt in content:\n        doc = nlp(txt)\n        '''\n        if matcher_type == \"pandemic\":\n            matches = matcher_pandemic(doc)\n        else:\n            matches = matcher_study_type(doc)\n        '''\n        matches = matcher(doc)\n    op = matched_sents\n    matched_sents = []\n    return op","0dde4869":"study_types = {\n    \"Modelling\" : \"Modelling. Mathematical assessment. mathematical model that embodies a set of statistical assumptions concerning the generation of sample data (and similar data from a larger population). A statistical model represents, often in considerably idealized form, the data-generating process\" ,\n    \n    \"Retrospective Cohort Analysis\": \"retrospective cohort analysis. a retrospective cohort study, also called a historic cohort study, is a longitudinal cohort study used \\\nin medical and psychological research. A cohort of individuals that share a common exposure factor is compared with another group of equivalent individuals not exposed to\\\nthat factor, to determine the factor's influence on the incidence of a condition such as disease or death. Retrospective cohort studies have existed for approximately as \\\n    long as prospective cohort studies. We extracted, we compared to.\",\n    \n    \"Prospective Cohort Analysis\": \"prospective cohort analysis. Follow. cohort study that follows over time a group of similar individuals (cohorts) who differ with respect to certain factors under study, to determine how these factors affect rates of a certain outcome. one might follow a cohort of middle-aged truck drivers who vary in\\\n    terms of smoking habits, to test the hypothesis that the 20-year incidence rate of lung cancer will be highest among heavy smokers, followed by moderate smokers, and then nonsmokers. We report findings of five patients in a family cluster. Patients were enrolled in our study \",\n    \n    \"Statistical Analysis\": \"statistical analysis. collection, organization, analysis, interpretation and presentation of data. generating statistics from stored data\\\n    and analyzing the results to deduce or infer meaning about the underlying dataset. \",\n    \n    \"Simulation\": \"simulation. predict. likely. we attempted a prediction of the worldwide spread.  in the coming months. we project that changing temperatures between.\\\n     countries should expect greater transmission in the coming months. approximate imitation of the operation of a process or system. \",\n    \n    \"Time-Series Analysis\": \"time-series analysis. their interaction were included with time trend adjusted. And we found that the associations between meteorological\\\n    factors and the number of COVID-19 daily cases are inconsistent across cities and time.  functions of time in the model.  a time series is a sequence taken at successive equ\\\nally spaced points in time\",\n    \n    \"Systematic Review\":\"systematic review. literature review that uses systematic methods to collect secondary data, critically appraise research studies, and \\\n    synthesize findings qualitatively or quantitatively. synthesize studies that directly relate to the systematic review question. a complete, exhaustive summary of current evidence. \\\n    The aim was to assess. a rapid qualitative evidence synthesis. We searched OVID MEDLINE on 26 March 2020. As we searched only one database due to time constraints, we also \\\n    undertook a rigorous and comprehensive scoping exercise and search of the reference lists of key papers. \",\n    \n    \"Epidemiological Study\": \"epidemiological study. measure the risk of illness or death in an exposed population compared to that risk in an identical, unexposed\\\n    population. study of the distribution of diseases in populations. disease risk factors. Assessing the spread of the COVID-19 across China, in relation to associations between\\\n    cases and ecological factors including population density, temperature, solar radiation and precipitation. We examined climate data from cities with significant community \\\n    spread of COVID-19 using ERA-5 reanalysis, and compared to areas that are either not affected, or do not have significant community spread.The distribution of significant \\\n    community outbreaks along restricted latitude, temperature, and humidity are consistent with the behavior of a seasonal respiratory virus. \",\n    \n    \"Regression\":\"regression.  estimating the relationships between a dependent variable and one or more independent variables. The Correlation Between the Spread of \\\n    COVID-19 Infections and Weather Variables.  This study aimed to explore and interpret the effect of environmental and metrological variables on the spread of coronavirus \\\n    disease. Early insights from laboratory studies of related coronaviruses predicted that COVID-19 would decline at higher temperatures, humidity, and ultraviolet light. We also \\\n    evaluate demographic variables. had the strongest and most significant effect of tested meteorological variables on COVID-19\",\n}","b4899adb":"#src.workflows.get_insights\n\nimport numpy as np\nimport json\nfrom typing import Dict\nimport re\nimport glob\nimport datetime\nimport pandas as pd\nimport tqdm\nfrom tqdm import tqdm\nfrom datetime import datetime\nimport  multiprocessing as mp\nimport typing\nfrom typing import List\n\npd.set_option('display.max_colwidth', -1)\n\nconcurrency = \"MULTI_THREAD\"\ntitle_score = 100\nabstract_score = 70\nbody_text_score = 40\nother_score = 10\n\nresults_dir = \"\/kaggle\/working\/results\/\"\nmetadata_file = \"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\"\n\nclass get_insights(object):\n\n    def __init__(self, concurrency=\"MULTI_THREAD\"):\n        self.insights = []\n        #self.ep = extract_phrases()\n        self.nlp = do_nlp()\n        self.metadata_df = pd.read_csv(metadata_file)\n        self.op_json_file = results_dir +  \"_results_\" + str(datetime.timestamp(datetime.now())) + \".csv\"    \n        return\n\n    def get_metadata(self,sha:str) -> Dict:\n        '''Return metadata fields for a given sha id\n        Args:\n            sha: sha value obtained from the paper\n        Returns:\n            op(Dict): Study, link, journal, DOI, CORDUID and Measure of Evidence\n        '''\n        op = {}\n        try:\n            row_vals = self.metadata_df[self.metadata_df['sha']==sha]\n            row_vals.replace(np.nan, '', regex=True, inplace=True)\n            op['Date'] = row_vals['publish_time'].values[0]\n            op['Study'] = row_vals['title'].values[0]\n            op['Study Link'] = row_vals['url'].values[0]\n            op['Journal'] = row_vals['journal'].values[0]\n            op['DOI'] = row_vals['doi'].values[0]\n            op['CORD_UID'] = row_vals['cord_uid'].values[0]\n            \n        except Exception as e:\n            #print(f'Failed to get metatada for {sha}')\n            return op\n        \n        return op\n\n    def process_file(self, filename: str, search_string:str) -> Dict:\n        '''Parse each file to get content for the summary table\n        Args:\n            filename: path to the file to be parsed\n            search_string: search criteria for future use\n        Returns:\n            insight: contents for summary table\n        '''\n        max_excerpts = 20\n        op = {}\n        data = {}\n        try:\n            with open(filename, \"r\",) as fp:\n                data = json.load(fp)\n                fp.close()\n        except Exception as e:\n            #print(\"failed to process file: \" + str(e))\n            return None\n        \n        #get excerpts and factors from sections in each file\n        insight = self.search_sections(data)\n        if insight is None:\n            return None\n\n        ##get most relevant excerpt\n        insight = self.get_top_n_excerpts(insight,max_excerpts)\n        #insight[\"Date\"] = datetime.now().strftime(\"%m\/%d\/%Y, %H:%M:%S\")\n        \n        #@todo insert logic to determine influential\n        insight['Influential'] = 'Y'\n        \n        #get contents from metadata\n        parts = filename.split(\"\/\")\n        sha = parts[len(parts)-1].split(\".\")[0]\n        metadata = self.get_metadata(sha)\n        \n        #if metadata file does not have info\n        if not metadata:\n            if 'title' in data['metadata']:\n                metadata['Study'] = data['metadata']['title']\n                metadata['Journal'] = filename.split(\"\/\")[1]\n                \n        insights = {**metadata, **insight}\n        #insights['filename'] = filename\n        #write outputs to flatfile, filename has timestamp to ensure\n        #each run will result in a new file\n        with open(self.op_json_file,\"a\") as fp:\n            json.dump(insights, fp)\n            fp.write(\"\\n\")\n        fp.close()\n        \n        return insights\n\n    def get_top_n_excerpts(self, insight: dict, max_excerpts:int = 10) -> dict:\n    \n        processed_spans = []\n        processed_excerpts = []\n        excerpts = insight['excerpts']\n        #if excerpts are not found dont process any further\n        excerpts = list(filter(lambda x: len(x['excerpts']) > 0, excerpts))\n        if len(excerpts) <= 0:\n            return None\n\n        try:\n            processed_excerpts = sorted(excerpts, key=lambda x: x['num_excerpts'], reverse=True)\n            excerpt = processed_excerpts[0]['excerpts']\n            spans = processed_excerpts[0]['span']\n            kp = list(map(lambda x: x[0], excerpt))\n            result = []\n            ind = 1\n            for o in kp:\n                for o1 in kp[ind:]:\n                    if o1.count(o):\n                        result.append(o1)\n                        ind += 1\n                        break\n\n            result = list(set(result))\n            res = list(filter(lambda x: not re.search(r\"[=|\\\\|\\\/]\",x),result))\n            insight['factors'] = \", \".join(res)\n            insight['excerpts'] = spans\n        except Exception as e:\n            print(f\"Failed to process excerpts: \" + str(e))\n            \n        return insight\n    \n    def get_excerpts(self, result, getphrases = True):\n        excerpts = []\n        for res in result:\n            for span in res['ents']:                \n                op = {}\n                kp = self.nlp.get_phrases_sgrank(span['span'],5)\n                np = []\n                #np = self.nlp.get_noun_chunks(span['span'])\n                span['excerpts'] = []\n                if kp:\n                    span['excerpts'].extend(kp)\n                    \n                if np:\n                    np = list(filter(lambda x: len(x.split()) > 3, np))\n                    nps = list(map(lambda x: (x,1), np))\n                    span['excerpts'].extend(nps)\n                    \n                num_excerpts = len(span['excerpts'])\n                span['num_excerpts'] = num_excerpts\n                #detect if a span is a subspan, merge kps present in both the spans\n                \n                excerpts.append(span)\n                \n        return excerpts\n\n    def get_measure_of_evidence(self, txt:str) -> Dict:\n        moe = \"\"\n        doc = nlp(txt)\n        loc = []\n        ts = []\n        for ent in doc.ents:\n            if ent.label_ == \"GPE\" and ent.text not in loc and len(loc) < 3:\n                loc.append(ent.text)\n            if ent.label_ == \"DATE\" and ent.text not in ts and len(ts) < 3:\n                ts.append(ent.text)\n        moe = \",\".join(loc) + \" \" + \",\".join(ts)\n            \n        return moe\n    \n    def search_sections(self, data:Dict) -> Dict:\n        '''Parse sections in a file to find patterns of interest\n        Args:\n            data(Dict): sections of a file represented in a dict\n        Returns:\n            insight(Dict): match results\n        '''\n        content = \"\"\n        insight = {}\n        score = 0\n        op = {}\n        influential_excerpt = []\n        informative_sections = \"\"\n        influential_text = \"\"\n        study_type_content = \"\"\n        \n        op['paper_id'] = data['paper_id']\n        op['matches'] = []\n        match_found = False\n        match_details = []\n        #check of title contains the terms\n        all_excerpts = []\n        title_content = data['metadata']['title'].lower()\n        result = find_matches([title_content])\n        study_type_content = study_type_content + \" \" + str(title_content)\n        \n        if result:\n            match_details.extend(result)\n            match_found = True\n            score += title_score * len(result)\n\n            informative_sections += \"title|\"\n            excerpts = self.get_excerpts(result)\n            all_excerpts.extend(excerpts)\n            \n        #check in abstract\n        abstract = \"\"\n        if 'abstract' in data.keys():\n            if len(data['abstract']):\n                abstract = data['abstract'][0]['text'].lower()\n                abstract = cleanup(abstract)\n                study_type_content = study_type_content + \" \" + str(abstract)\n                result = find_matches([abstract])\n                    \n                if result:\n                    match_found = True\n                    score += abstract_score * len(result)\n\n                    excerpts = self.get_excerpts(result)\n                    all_excerpts.extend(excerpts)\n\n\n        #process body text\n        bt_match_found = False\n        for bt in data['body_text']:\n            body_text = bt['text'].lower()\n            body_text = cleanup(body_text)\n            content = content + \" \" + str(body_text)\n            result = find_matches([body_text])\n            if result:\n                match_found = True\n                bt_match_found = True\n                score += body_text_score * len(result)\n                excerpts = self.get_excerpts(result)\n                all_excerpts.extend(excerpts)\n\n        for key, val in data['ref_entries'].items():\n            ref_text = val['text'].lower()\n            ref_text = cleanup(ref_text)\n            content = content + \" \" + str(ref_text)\n            \n            result = find_matches([ref_text])\n            if result:\n                match_found = True\n                score += other_score * len(result)\n                excerpts = self.get_excerpts(result)\n                all_excerpts.extend(excerpts)\n\n        #back matter\n        bm_match_found = False\n        for bm in data['back_matter']:\n            bm_text = bm['text'].lower()\n            bm_text = cleanup(bm_text)\n            bm_sec_text =  bm['section'].lower()\n            bm_sec_text = cleanup(bm_sec_text)\n            study_type_content = study_type_content + \" \" + str(bm_sec_text)\n            \n            result = find_matches([bm_text,bm_sec_text])\n            if result:\n                match_found = True\n                bm_match_found = True\n                score += other_score * len(result)\n                excerpts = self.get_excerpts(result)\n                all_excerpts.extend(excerpts)\n                \n        if match_found:\n            insight[\"score\"] = score\n            insight[\"excerpts\"] = all_excerpts\n            #get study types\n            matched_study_types, _ = self.nlp.get_cosine_sim(study_type_content, study_types)\n            insight[\"Study Type\"] = matched_study_types\n            #get measure of evidence\n            moe = self.get_measure_of_evidence(content)\n            insight[\"Measure of Evidence\"] = moe\n            return insight\n        else:\n            return None\n\n   \n    def process_files(self, input_filename: str, tag:str, file_limit=100) -> List:\n        '''Parses list of files in parallel to find patterns and build the\n            summary table\n        Args:\n            input_filename(str): file which contains relevant filenames \n            tag(str): tag to indicate which sub task are we processing the files for\n            file_limit(int): upper bound for number of files to be processed\n        Returns:\n            results(List): list of matches \n        '''\n\n        files = []\n        with open(input_filename, \"r\") as fp:\n            files = fp.readlines()\n        fp.close()\n        \n        files = files[:file_limit]\n\n        files = list(map(lambda x: x.strip(), files))\n        total_files = len(files)\n        \n        pbar = tqdm(total = total_files)\n        results = []\n        pool = mp.Pool()\n\n        def collect_results(result):\n            if result is not None:\n                if result is not None:\n                    results.append(result)\n            pbar.update()\n            return\n        results1 = [pool.apply_async(self.process_file, args=(f,\"\"), callback=collect_results) for f in files]\n        pool.close()\n        pool.join()\n        pbar.close()\n    \n        final_df = pd.DataFrame(results)                     \n        print(results)\n        final_df.sort_values(by='score', ascending=False, inplace=True)\n        return final_df\n    \n     ","d91c0b5c":"\nclass patterns(object):\n    \n    def __init__(self):\n        #Effectiveness of case isolation\/isolation of exposed individuals (i.e. quarantine) \n        self.isolation_pattern1 = { \n            \"LABEL\" : \"ISOLATION\",\n            \"pattern\" : [\n                {\"TEXT\" : {  \"REGEX\" : \"effective|effectiveness|power|control|success\" }\n                },\n                {\n                    \"OP\" : \"*\"\n                },\n                {\n                    \"TEXT\" : { \"REGEX\" : \"distancing|isolation|self-quarantine|self quarantine|quarantine\" }\n                }\n            ]\n        }\n\n        self.isolation_pattern2 = {\n            \"LABEL\" : \"ISOLATION\",\n            \"pattern\" : [\n                {\"TEXT\" : {  \"REGEX\" : \"effective|effectiveness|power|control||success\" }\n                },\n                {\n                    \"POS\" : \"ADP\"\n                },\n                {\n                    \"OP\" : \"*\"\n                },\n                {\n                    \"LEMMA\" : {\n                        \"IN\" : [\"expose\",\"quarantine\"] }, \"OP\" : \"*\"\n                },\n                {\n                    \"POS\" : \"VERB\", \"OP\" : \"*\" \n                },\n                {\n                    \"LEMMA\" : { \"IN\" : [ \"individual\", \"people\", \"society\", \"doctor\", \"professional\", \"personnel\", \"patient\" ] }\n                }\n            ]\n        }\n        \n        #Effectiveness of community contact reduction\n        \n        self.community_contact_reduction1 = { \n            \"LABEL\" : \"COMMUNITY\",\n            \"pattern\" : [\n                {\"TEXT\" : {  \"REGEX\" : \"effective|effectiveness|power|control||success\" }\n                },\n                {\n                    \"OP\" : \"*\"\n                },\n                {\n                \"TEXT\" : { \"REGEX\" : \"community\" }\n                }\n            ]\n        }\n        \n        \n        self.community_contact_reduction2 = {\n            \"LABEL\" : \"COMMUNITY\",\n            \"pattern\" : [\n                {\"TEXT\" : {  \"REGEX\" : \"effective|effectiveness|power|control||success\" }\n                },\n                {\n                    \"POS\" : \"ADP\"\n                },\n                {\n                    \"OP\" : \"*\"\n                },\n                {\n                    \"LEMMA\" : {\n                        \"IN\" : [\"contact\", \"physical\", \"social\"] }, \"OP\" : \"*\"\n                },\n                {\n                    \"LEMMA\" : {\n                        \"IN\" : [\"reduction\", \"distancing\"] }, \"OP\" : \"*\"\n                },\n                {\n                    \"POS\" : \"VERB\", \"OP\" : \"*\" \n                },\n                {\n                    \"LEMMA\" : { \"IN\" : [ \"individual\", \"people\", \"society\", \"doctor\", \"professional\", \"personnel\", \"patient\",\"human-to-human\",\"person to person\",\"human to human\", \"handshake\" ] }\n                }\n            ]\n        }\n\n        #Effectiveness of inter\/inner travel restriction \n        \n        self.travel_restriction1 = {\n         \"LABEL\" : \"TRAVEL\",\n        \"pattern\" : [\n            {\"TEXT\" : {  \"REGEX\" : \"effective|effectiveness|power|control|success\" }\n            },\n            {\n                \"OP\" : \"*\"\n            },\n            {\n                \"TEXT\" : { \"REGEX\" : \"travel\" }\n            },\n            {\n                \"OP\" : \"*\"\n            },\n            {\n                \"TEXT\" : { \"REGEX\" : \"restriction|ban|curtailing|reduction|close|limit|cancel|cap\" }\n            }\n        ]\n\n        }\n        \n        self.travel_restriction2 = {\n         \"LABEL\" : \"TRAVEL\",\n        \"pattern\" : [\n            {\"TEXT\" : {  \"REGEX\" : \"effective|effectiveness|power|control|success\"}\n            },\n            {\n                \"OP\" : \"*\"\n            },\n            {\n                \"TEXT\" : { \"REGEX\" : \"travel|mobility|movement|border|national borders|state borders|country|traveller|travellers|trade|transport|flight\" }\n            },\n            {\n                \"OP\" : \"*\"\n            },\n            {\n                \"TEXT\" : { \"REGEX\" : \"restriction|ban|cap|limit\" }\n            },\n            {\n                \"OP\" : \"*\"\n            },\n            {\n                \"TEXT\" : { \"REGEX\" : \"city|country|state|continent\" }\n            }\n        ]\n\n        }\n            \n        self.travel_restriction3 = {\n         \"LABEL\" : \"TRAVEL\",\n        \"pattern\" : [\n            {\"TEXT\" : {  \"REGEX\" : \"travel\" }\n            },\n            {\n                \"OP\" : \"*\"\n            },\n            {\n                \"TEXT\" : { \"REGEX\" : \"agency|company|association\" }\n            }\n        ]\n\n        }\n        \n        #Effectiveness of school distancing\n        \n        self.school_distancing1 = {\n         \"LABEL\" : \"SCHOOL\",\n        \"pattern\" : [\n            {\"TEXT\" : {  \"REGEX\" : \"effectiveness\" }\n            },\n            {\n                \"OP\" : \"*\"\n            },\n            {\n                \"TEXT\" : { \"REGEX\" : \"school|college|university\" }\n            },\n            {\n                \"OP\" : \"*\"\n            },\n            {\n                \"TEXT\" : { \"REGEX\" : \"distancing|closure|cancel\" }\n            }\n        ]\n\n        }\n        \n        self.school_distancing2 = {\n         \"LABEL\" : \"SCHOOL\",\n        \"pattern\" : [\n            {\"TEXT\" : {  \"REGEX\" : \"online|blended|hybrid\" }\n            },\n            {\n                \"OP\" : \"*\"\n            },\n            {\n                \"TEXT\" : { \"REGEX\" : \"teaching|learning|classroom\" }\n            },\n            {\n                \"OP\" : \"*\"\n            },\n            {\n                \"TEXT\" : { \"REGEX\" : \"school|college|university|educational|childcare|stay-at-home|stay at home\" }\n            },\n            {\n                \"OP\" : \"*\"\n            },\n            {\n                \"TEXT\" : { \"REGEX\" : \"2019-nCoV|covid|coronavirun|lockdown\" }\n            }\n        ]\n\n        }\n            \n        self.school_distancing3 = {\n         \"LABEL\" : \"SCHOOL\",\n        \"pattern\" : [\n            {\"TEXT\" : {  \"REGEX\" : \"school|college|university|childcare\" }\n            },\n            {\n                \"OP\" : \"*\"\n            },\n            {\n                \"TEXT\" : { \"REGEX\" : \"student|teacher|children\" }\n            },\n            {\n                \"OP\" : \"*\"\n            },\n            {\n                \"TEXT\" : { \"REGEX\" : \"drop-off|pick-up\" }\n            },\n            {\n                \"OP\" : \"*\"\n            },\n            {\n                \"TEXT\" : { \"REGEX\" : \"2019-nCoV|covid|coronavirun|lockdown\" }\n            }\n        ]\n\n        }\n        \n        #Effectiveness of workplace distancing\n            \n        self.workplace_distancing1 = {\n         \"LABEL\" : \"WORKPLACE\",\n        \"pattern\" : [\n            {\"TEXT\" : {  \"REGEX\" : \"effectiveness\" }\n            },\n            {\n                \"OP\" : \"*\"\n            },\n            {\n                \"TEXT\" : { \"REGEX\" : \"workplace|office|corporate|workforce\" }\n            },\n            {\n                \"OP\" : \"*\"\n            },\n            {\n                \"TEXT\" : { \"REGEX\" : \"distancing|closure\" }\n            }\n        ]\n\n        }\n        \n        self.workplace_distancing2 = {\n         \"LABEL\" : \"WORKPLACE\",\n        \"pattern\" : [\n            {\"TEXT\" : {  \"REGEX\" : \"online|blended|hybrid|virtual\" }\n            },\n            {\n                \"OP\" : \"*\"\n            },\n            {\n                \"TEXT\" : { \"REGEX\" : \"work|working|workforce|office|conference|meeting|working from home\" }\n            },\n            {\n                \"OP\" : \"*\"\n            },\n            {\n                \"TEXT\" : { \"REGEX\" : \"2019-nCoV|covid|coronavirun|lockdown\" }\n            }\n        ]\n\n        }\n        \n        self.workplace_distancing3 = {\n         \"LABEL\" : \"WORKPLACE\",\n        \"pattern\" : [\n            {\n                \"OP\" : \"*\"\n            },\n            {\n                \"TEXT\" : { \"REGEX\" : \"zoom|skype|teams\" }\n            },\n            {\n                \"OP\" : \"*\"\n            },\n            {\n                \"TEXT\" : { \"REGEX\" : \"2019-nCoV|covid|coronavirun|lockdown\" }\n            }\n        ]\n\n        }\n\n        # Effectiveness of a multifactorial strategy prevent secondary transmission\n        self.multifactorial_strategy_1 = {\n        \"LABEL\" : \"STRATEGY\",\n        \"pattern\" : [\n            {\"TEXT\" : {  \"REGEX\" : \"effective|success\" }\n            },\n            {\n                \"OP\" : \"*\"\n            },\n                {\n                    \"TEXT\" : { \"REGEX\" : \"multi-factor|multifactorial|multiple|multifaceted|multitargeted\" }\n                },\n                {\n                    \"OP\" : \"*\"\n                },\n                {\n                    \"TEXT\" : { \"REGEX\" : \"approach|strategy|intervene|intervention|solution|steps|plan|prevent|measure\" }\n                },\n                {\n                    \"OP\" : \"*\"\n                },\n                {\n                    \"TEXT\" : { \"REGEX\" : \"secondary|horizontal|human-to-human|human to human|person to person|person-to-person|community\" }\n                },\n                {\n                    \"OP\" : \"*\"\n                },\n                {\n                    \"TEXT\" : { \"REGEX\" : \"transmit|transmission|spread|transfer\" }\n                }\n            ]\n        }\n        \n        self.multifactorial_strategy_2 = {\n            \"LABEL\" : \"STRATEGY\",\n            \"pattern\" : [\n                {\n                    \"TEXT\" : { \"REGEX\" : \"multi-factor|multifactorial|multiple|multifaceted|multitargeted\" }\n                },\n                {\n                    \"OP\" : \"*\"\n                },\n                {\n                    \"TEXT\" : { \"REGEX\" : \"approach|strategy|intervene|intervention|solution|steps|plan|prevent|measure\" }\n                },\n                {\n                    \"OP\" : \"*\"\n                },\n                {\n                    \"TEXT\" : { \"REGEX\" : \"secondary|horizontal|human-to-human|human to human|person to person|person-to-person|community\" }\n                },\n                {\n                    \"OP\" : \"*\"\n                },\n                {\n                    \"TEXT\" : { \"REGEX\" : \"transmit|transmission|spread|transfer\" }\n                }\n            ]\n        }\n        \n        self.multifactorial_strategy_3 = {\n            \"LABEL\" : \"STRATEGY\",\n            \"pattern\" : [\n                {\n                    \"TEXT\" : { \"REGEX\" : \"approach|strategy|intervene|intervention|solution|steps|plan|prevent|measure\" }\n                },\n                {\n                    \"OP\" : \"*\"\n                },\n                {\n                    \"TEXT\" : { \"REGEX\" : \"secondary|horizontal|human-to-human|human to human|person to person|person-to-person|community\" }\n                },\n                {\n                    \"OP\" : \"*\"\n                },\n                {\n                    \"TEXT\" : { \"REGEX\" : \"transmit|transmission|spread|transfer\" }\n                }\n            ]\n        }\n\n        self.multifactorial_strategy_4 = {\n            \"LABEL\" : \"STRATEGY\",\n            \"pattern\" : [\n                {\n                    \"TEXT\" : { \"REGEX\" : \"secondary|horizontal|human-to-human|human to human|person to person|person-to-person|community\" }\n                },\n                {\n                    \"OP\" : \"*\"\n                },\n                {\n                    \"TEXT\" : { \"REGEX\" : \"transmit|transmission|spread|transfer\" }\n                }\n            ]\n        }\n        \n        # Seasonality of transmission + How does temperature and humidity affect the transmission of 2019-nCoV? + Significant changes in transmissibility in changing seasons?\n        \n        self.seasonality = {\n         \"LABEL\" : \"SEASONALITY\",\n        \"pattern\" : [\n            {\"TEXT\" : {  \"REGEX\" : \"change|season|environmental|material\" }\n            },\n            {\n                \"OP\" : \"*\"\n            },\n            {\"TEXT\" : {  \"REGEX\" : \"climate|condition|conditions|properties|environment|weather|warm|seasonal|season|meterology|atmosphere|seasonality|month|temperature|air pressure|humidity|winter|monsoon|summer\" }\n            },\n            {\n                \"OP\" : \"*\"\n            },\n            {\n                \"TEXT\" : { \"REGEX\" : \"transmission|transfer|transmissability|spead|spreading|contract|contracting|contracted\" }\n            },\n            {\n                \"OP\" : \"*\"\n            },\n            {\n                \"TEXT\" : { \"REGEX\" : \"coronavirus|covid|2019-nCoV\" }\n            },\n            {\n                \"OP\" : \"*\"\n            },\n            {\n                \"TEXT\" : { \"REGEX\" : \"material|copper|cardboard|plastic|stainless steel|coins|banknotes|handshake|handrails|doorknobs\" }\n            }\n        ]\n\n        }\n        \n        # Effectiveness of personal protective equipment (PPE)\n        self.ppe_1 = {\n            \"LABEL\" : \"PPE_TERMS\",\n            \"pattern\" : [\n                {\"TEXT\" : {  \"REGEX\" : \"effective|success|critical\" }\n                },\n                {\n                    \"OP\" : \"*\"\n                },\n                {\"TEXT\" :\n                 {\"REGEX\" : \"mask|shield|gown|glove|protective equipment|PPE|personal protection|gear|cover\"}\n                }\n            ]\n        }\n\n        self.ppe_2 = {\n            \"LABEL\" : \"PPE_TERMS\",\n            \"pattern\" : [\n                {\"TEXT\" : {  \"REGEX\" : \"put on|take off|use|wear|remove|removal|disposal|discard\" }\n                },\n                {\n                    \"OP\" : \"*\"\n                },\n                {\"TEXT\" :\n                 {\"REGEX\" : \"mask|shield|gown|glove|protective equipment|PPE|personal protection|gear|cover\"}\n                }\n            ]\n        }\n\n        self.ppe_3 = {\n            \"LABEL\" : \"PPE_TERMS\",\n            \"pattern\" : [\n                {\"TEXT\" : {  \"REGEX\" : \"mask|shield|gown|glove|protective equipment|PPE|personal protection|gear|cover\" }\n                },\n                {\n                    \"OP\" : \"*\"\n                },\n                {\"TEXT\" :\n                 {\"REGEX\" : \"fluid resistant|air leaks|material|fabric\"}\n            }\n            ]\n        }\n        #temp humidity\n        self.temperature_humidity_1 = {\n            \"LABEL\" : \"TEMP_HUMIDITY\",\n            \"pattern\" : [\n                {\"TEXT\" : {  \"REGEX\" : \"effect|impact\" }\n                },\n                {\n                    \"OP\" : \"*\"\n                },\n                {\"TEXT\" : {  \"REGEX\" : \"temperature|humid\" }\n                },\n                {\n                    \"OP\" : \"*\"\n                },\n                {\n                    \"TEXT\" : { \"REGEX\" : \"transmit|transmission|transmissability|spead|spreading|transfer|contract|contracting|contracted\" }\n                }\n            ]\n        }\n\n        self.temperature_humidity_2 = {\n            \"LABEL\" : \"TEMP_HUMIDITY\",\n            \"pattern\" : [\n                {\"TEXT\" : {  \"REGEX\" : \"transmit|transmission|transmissability|spead|spreading|transfer|contract|contracting|contracted\" }\n                },\n                {\n                    \"OP\" : \"*\"\n                },\n                {\n                    \"TEXT\" : { \"REGEX\" : \"effect|impact|affect\" }\n                },\n                {\n                    \"OP\" : \"*\"\n                },\n                {\"TEXT\" : {  \"REGEX\" : \"temperature|humid\" }\n                }\n            ]\n        }\n        self.temperature_humidity_3 = {\n            \"LABEL\" : \"TEMP_HUMIDITY\",\n            \"pattern\" : [\n                {\"TEXT\" : {  \"REGEX\" : \"temperature|humid\" }\n                },\n                {\n                    \"OP\" : \"*\"\n                },\n                {\n                    \"TEXT\" : { \"REGEX\" : \"effect|impact|affect\" }\n                },\n                {\n                    \"OP\" : \"*\"\n                },\n                {\"TEXT\" : {  \"REGEX\" : \"transmit|transmission|transmissability|spead|spreading|transfer|contract|contracting|contracted\" }\n                }\n            ]\n        }\n\n        self.isolation_patterns = [self.isolation_pattern1, self.isolation_pattern2] #,self.pandemic_pattern1 ] #, pandemic_pattern2 ]\n            \n        self.community_contact_patterns = [self.community_contact_reduction1, self.community_contact_reduction2]\n            \n        self.travel_restriction_patterns = [self.travel_restriction1, self.travel_restriction2, self.travel_restriction3]\n            \n        self.school_distancing_patterns = [self.school_distancing1, self.school_distancing2, self.school_distancing3]\n            \n        self.workplace_distancing_patterns = [self.workplace_distancing1, self.workplace_distancing2, self.workplace_distancing3]\n            \n        self.multifactorial_strategy_patterns = [self.multifactorial_strategy_1,self.multifactorial_strategy_2,self.multifactorial_strategy_3,self.multifactorial_strategy_4]\n        \n        self.temp_humidity_pattern = [ self.temperature_humidity_1, self.temperature_humidity_2, self.temperature_humidity_3 ]\n\n        self.seasonality_pattern = [self.seasonality]\n        \n        self.ppe_pattern = [self.ppe_1, self.ppe_2, self.ppe_3]","e8c63927":"search_results_dir = \"\/kaggle\/working\/results\/search_results\/\"\nsubtask1_terms = [\"case isolation\", \"infected individual\", \"exposed individual\", \"quarantined individuals\"]\nsubtask1_filepath = search_results_dir + \"subtask1_filtered_files.txt\"\n\n#global variable for keywords identification\nkeyword_processor = KeywordProcessor()\n\nfor term in subtask1_terms:\n    keyword_processor.add_keyword(term) \n\nif not os.path.exists(subtask1_filepath) and RUN_PARTIAL_CACHED_RESULTS:\n    op = get_filelist(subtask1_filepath)","3e653832":"## add patterns to spacy's token matcher\npat = patterns()\nadd_patterns(pat.isolation_patterns)\n\n## trigger the workflow to generate summary table\ngrs = get_insights()\n#subtask1_filepath refers to path where list of relevant files are stored\nsubtask1_filepath = \"\/kaggle\/input\/intermediateresults\/subtask1_filtered_files.txt\"\nresults1 = grs.process_files(subtask1_filepath, \"subtask1\", 20)\nop_results_file = \"\/kaggle\/working\/results\/case_isolation_subtask1_results_\" + str(datetime.timestamp(datetime.now())) + \".csv\"    \nresults1.to_csv(op_results_file, index=False)\nresults1[:10]","188cebcb":"# The score column is used a proxy to determine how informative the journals are w.r.t. publishing content related to the subtask\nresults1.groupby('Journal')['score'].mean().sort_values().plot.bar()","71c9c3a6":"subtask2_terms = [\"social distance\", \"physical distance\", \"human to human\", \"human-to-human\",\n                 \"person to person\", \"person-to-person\",\"gathering\"]\n\nsubtask2_filepath = search_results_dir + \"\/community_contact_reduction_subtask2_files.txt\"\nkeyword_processor = KeywordProcessor()\nfor term in subtask2_terms:\n    keyword_processor.add_keyword(term) \n\n#subtask2_filepath refers to path where list of relevant files are stored\nif not os.path.exists(subtask1_filepath) and RUN_PARTIAL_CACHED_RESULTS:\n    op = get_filelist(subtask2_filepath)","273063ce":"## add patterns to spacy's token matcher\npat = patterns()\nadd_patterns(pat.community_contact_patterns)\n#add_patterns(pat.study_type_patterns)\n\n## trigger the workflow to generate summary table\ngrs = get_insights()\nsubtask2_filepath = \"\/kaggle\/working\/results\/search_results\/community_contact_reduction_subtask2_files.txt\"\n#subtask1_filepath refers to path where list of relevant files are stored\nresults2 = grs.process_files(subtask2_filepath, \"subtask2\", 10)\nop_results_file = \"\/kaggle\/working\/results\/community_contact_reduction_subtask2_results_\" + str(datetime.timestamp(datetime.now())) + \".csv\"    \nresults2.to_csv(op_results_file, index=False)\nresults2[:10]","99f73a7d":"# The score column is used a proxy to determine how informative the journals are w.r.t. publishing content related to the subtask\nresults2.groupby('Journal')['score'].mean().sort_values().plot.bar()","a86e189d":"subtask3_terms = [\"travel restriction\", \"ban on travel\", \"transport restriction\", \"flight restriction\",\n                 \"national border\", \"inter-state travel\",\"inter state travel\", \"cross country travel\", \"trade restriction\"]\n\nsubtask3_filepath = search_results_dir + \"\/travel_ban_subtask3_files.txt\"\nkeyword_processor = KeywordProcessor()\nfor term in subtask3_terms:\n    keyword_processor.add_keyword(term) \n\nif not os.path.exists(subtask1_filepath) and RUN_PARTIAL_CACHED_RESULTS:\n    op = get_filelist(subtask3_filepath)","e0792946":"## add patterns to spacy's token matcher\npat = patterns()\nadd_patterns(pat.travel_restriction_patterns)\n\n## trigger the workflow to generate summary table\ngrs = get_insights()\n#subtask3_filepath refers to path where list of relevant files are stored\nsubtask3_filepath = \"\/kaggle\/working\/results\/search_results\/travel_ban_subtask3_files.txt\"\nresults3 = grs.process_files(subtask3_filepath, \"subtask3\", 10)\nop_results_file = \"\/kaggle\/working\/results\/ctravel_ban_subtask3_results_\" + str(datetime.timestamp(datetime.now())) + \".csv\"    \nresults3.to_csv(op_results_file, index=False)\nresults3[:10]","1b7b5176":"# The score column is used a proxy to determine how informative the journals are w.r.t. publishing content related to the subtask\nresults3.groupby('Journal')['score'].mean().sort_values().plot.bar()","ac4ed44c":"subtask4_terms = [\"school\", \"college\", \"university\", \"schooling\", \"learn\", \"educate\", \"education\", \"educational\", \"virtual\", \"remote\", \"home\", \"teaching\", \"classroom\", \"student\", \"teacher\", \"hybrid\", \"blended\", \"zoom\", \"skype\"]\n\nsubtask4_filepath = search_results_dir + \"\/school_distancing_subtask4_files.txt\"\nkeyword_processor = KeywordProcessor()\nfor term in subtask4_terms:\n    keyword_processor.add_keyword(term) \n\nif not os.path.exists(subtask1_filepath) and RUN_PARTIAL_CACHED_RESULTS:\n    op = get_filelist(subtask4_filepath)","5d857e09":"## add patterns to spacy's token matcher\npat = patterns()\nadd_patterns(pat.school_distancing_patterns)\n\n## trigger the workflow to generate summary table\ngrs = get_insights()\nsubtask4_filepath = \"\/kaggle\/working\/results\/search_results\/school_distancing_subtask4_files.txt\"\nresults4 = grs.process_files(subtask4_filepath, \"subtask3\", 10)\nop_results_file = \"\/kaggle\/working\/results\/school_distancing_subtask4_results_\" + str(datetime.timestamp(datetime.now())) + \".csv\"    \nresults4.to_csv(op_results_file, index=False)\nresults4[:10]","ae9a3ab0":"# The score column is used a proxy to determine how informative the journals are w.r.t. publishing content related to the subtask\nresults4.groupby('Journal')['score'].mean().sort_values().plot.bar()","dad043b7":"subtask5_terms = [\"work\", \"working\", \"workplace\", \"meeting\", \"virtual\", \"home\", \"business\", \"businesses\", \"office\", \"occupancy\", \"cap\", \"caps\", \"close\", \"closed\", \"limit\", \"workforce\", \"hybrid\", \"blended\", \"flexible\", \"work-from-home\", \"work from home\", \"zoom\", \"skype\"]\n\nsubtask5_filepath = search_results_dir + \"\/workplace_distancing_subtask5_files.txt\"\nkeyword_processor = KeywordProcessor()\nfor term in subtask5_terms:\n    keyword_processor.add_keyword(term) \n\nif not os.path.exists(subtask1_filepath) and RUN_PARTIAL_CACHED_RESULTS:\n    op = get_filelist(subtask5_filepath)","d531b6aa":"## add patterns to spacy's token matcher\npat = patterns()\nadd_patterns(pat.workplace_distancing_patterns)\n\n## trigger the workflow to generate summary table\ngrs = get_insights()\n#subtask5_filepath refers to path where list of relevant files are stored\nsubtask5_filepath = \"\/kaggle\/working\/results\/search_results\/workplace_distancing_subtask5_files.txt\"\nresults5 = grs.process_files(subtask5_filepath, \"subtask5\", 10)\nop_results_file = \"\/kaggle\/working\/results\/workplace_distancing_subtask5_results_\" + str(datetime.timestamp(datetime.now())) + \".csv\"    \nresults5.to_csv(op_results_file, index=False)\nresults5[:10]","d2581b51":"# The score column is used a proxy to determine how informative the journals are w.r.t. publishing content related to the subtask\nresults5.groupby('Journal')['score'].mean().sort_values().plot.bar()","49aa2a3e":"subtask6_terms = [\"community transmission\", \"community spread\",\"secondary transmission\", \"secondary spread\", \"horizontal spread\", \"horizontal transmission\", \"human-to-human transmission\"]\n\n\nsubtask6_filepath = search_results_dir + \"\/multifact_strategy_subtask6_files.txt\"\nkeyword_processor = KeywordProcessor()\nfor term in subtask6_terms:\n    keyword_processor.add_keyword(term) \n    \nif not os.path.exists(subtask1_filepath)  and RUN_PARTIAL_CACHED_RESULTS:\n    op = get_filelist(subtask6_filepath)","48fe8a4a":"## add patterns to spacy's token matcher\npat = patterns()\nadd_patterns(pat.multifactorial_strategy_patterns)\n\n## trigger the workflow to generate summary table\ngrs = get_insights()\n#subtask5_filepath refers to path where list of relevant files are stored\nsubtask6_filepath = \"\/kaggle\/working\/results\/search_results\/multifact_strategy_subtask6_files.txt\"\nresults6 = grs.process_files(subtask6_filepath, \"subtask6\", 10)\nop_results_file = \"\/kaggle\/working\/results\/multifact_strategy_subtask6_results_\" + str(datetime.timestamp(datetime.now())) + \".csv\"    \nresults6.to_csv(op_results_file, index=False)\nresults6[:10]","00446c4d":"# The score column is used a proxy to determine how informative the journals are w.r.t. publishing content related to the subtask\nresults6.groupby('Journal')['score'].mean().sort_values().plot.bar()","c8baf1d9":"subtask7_terms = [\"seasonality\", \"seasonal\", \"season\", \"seasons\", \"seasonal transmission\"]\n\nsubtask7_filepath = search_results_dir + \"\/seasonality_subtask7_files.txt\"\nkeyword_processor = KeywordProcessor()\nfor term in subtask7_terms:\n    keyword_processor.add_keyword(term) \n\nif not os.path.exists(subtask1_filepath)  and RUN_PARTIAL_CACHED_RESULTS:\n    op = get_filelist(subtask7_filepath)","99466169":"## add patterns to spacy's token matcher\npat = patterns()\nadd_patterns(pat.seasonality_pattern)\n\n## trigger the workflow to generate summary table\ngrs = get_insights()\n#subtask5_filepath refers to path where list of relevant files are stored\nsubtask7_filepath = \"\/kaggle\/working\/results\/search_results\/seasonality_subtask7_files.txt\"\nresults7 = grs.process_files(subtask7_filepath, \"subtask7\", 10)\nop_results_file = \"\/kaggle\/working\/results\/seasonality_subtask7_results_\" + str(datetime.timestamp(datetime.now())) + \".csv\"    \nresults7.to_csv(op_results_file, index=False)\nresults7[:10]","f6185766":"# The score column is used a proxy to determine how informative the journals are w.r.t. publishing content related to the subtask\nresults7.groupby('Journal')['score'].mean().sort_values().plot.bar()","9b993575":"subtask8_terms = [\"temperature\", \"humidity\", \"heat\", \"moisture\"]\n\nsubtask8_filepath = search_results_dir + \"\/temp_humidity_subtask8_files.txt\"\nkeyword_processor = KeywordProcessor()\nfor term in subtask8_terms:\n    keyword_processor.add_keyword(term) \n\nif not os.path.exists(subtask1_filepath)  and RUN_PARTIAL_CACHED_RESULTS:\n    op = get_filelist(subtask8_filepath)","a8c06ea3":"## add patterns to spacy's token matcher\npat = patterns()\nadd_patterns(pat.temp_humidity_pattern)\n\n## trigger the workflow to generate summary table\ngrs = get_insights()\n#subtask5_filepath refers to path where list of relevant files are stored\nsubtask8_filepath = \"\/kaggle\/working\/results\/search_results\/temp_humidity_subtask8_files.txt\"\nresults8 = grs.process_files(subtask8_filepath, \"subtask8\", 10)\nop_results_file = \"\/kaggle\/working\/results\/temp_humidity_subtask8_results_\" + str(datetime.timestamp(datetime.now())) + \".csv\"    \nresults8.to_csv(op_results_file, index=False)\nresults8[:10]","215c6489":"# The score column is used a proxy to determine how informative the journals are w.r.t. publishing content related to the subtask\nresults8.groupby('Journal')['score'].mean().sort_values().plot.bar()","c83d2561":"subtask9_terms = [\"season\", \"seasonal\", \"climate\", \"environment\", \"weather\", \"warm\", \"cold\", \"hot\", \"cool\", \"meterology\", \"meterological\", \"atmosphere\", \"month\", \"temperature\", \"air pressure\", \"humidity\", \"winter\", \"summer\", \"monsoon\", \"spring\", \"summer\"]\nsubtask9_filepath = search_results_dir + \"\/seasons_trans_subtask9_files.txt\"\nkeyword_processor = KeywordProcessor()\nfor term in subtask9_terms:\n    keyword_processor.add_keyword(term) \n\nif not os.path.exists(subtask1_filepath)  and RUN_PARTIAL_CACHED_RESULTS:\n    op = get_filelist(subtask9_filepath)","bc01fc35":"## add patterns to spacy's token matcher\npat = patterns()\nadd_patterns(pat.temp_humidity_pattern)\n\n## trigger the workflow to generate summary table\ngrs = get_insights()\n#subtask5_filepath refers to path where list of relevant files are stored\nsubtask9_filepath = \"\/kaggle\/working\/results\/search_results\/seasons_trans_subtask9_files.txt\"\nresults9 = grs.process_files(subtask9_filepath, \"subtask9\", 10)\nop_results_file = \"\/kaggle\/working\/results\/seasons_trans_subtask9_results_\" + str(datetime.timestamp(datetime.now())) + \".csv\"    \nresults9.to_csv(op_results_file, index=False)\nresults9[:10]","3abf50e6":"# The score column is used a proxy to determine how informative the journals are w.r.t. publishing content related to the subtask\nresults9.groupby('Journal')['score'].mean().sort_values().plot.bar()","b0fb4363":"subtask10_terms = [\"mask\", \"shield\", \"gown\", \"gloves\", \"protective equipment\", \"PPE\", \"personal protection\"]\n\nsubtask10_filepath = search_results_dir + \"\/ppe_subtask10_files.txt\"\nkeyword_processor = KeywordProcessor()\nfor term in subtask10_terms:\n    keyword_processor.add_keyword(term) \n\nif not os.path.exists(subtask1_filepath)  and RUN_PARTIAL_CACHED_RESULTS:\n    op = get_filelist(subtask10_filepath)","d3158a22":"## add patterns to spacy's token matcher\npat = patterns()\nadd_patterns(pat.ppe_pattern)\n\n## trigger the workflow to generate summary table\ngrs = get_insights()\n#subtask5_filepath refers to path where list of relevant files are stored\nsubtask10_filepath = \"\/kaggle\/working\/results\/search_results\/ppe_subtask10_files.txt\"\nresults10 = grs.process_files(subtask10_filepath, \"subtask10\", 10)\nop_results_file = \"\/kaggle\/working\/results\/ppe_subtask10_results_\" + str(datetime.timestamp(datetime.now())) + \".csv\"    \nresults10.to_csv(op_results_file, index=False)\nresults10[:10]","fee40711":"# The score column is used a proxy to determine how informative the journals are w.r.t. publishing content related to the subtask\nresults10.groupby('Journal')['score'].mean().sort_values().plot.bar()","b3b29750":"final_results = pd.concat([results1, results2, results3, results4, results5, results6, results7, results8, results9, results10])\nfinal_results.groupby('Journal')['score'].mean().sort_values()[:10].plot.bar()\n","94145d5e":"# Effectiveness of personal protective equipment (PPE)","fbafd568":"This module is the main workflow which does the following:\n\n1. Processes files relevant to the patterns\n2. Identifies, extracts and ranks excerpts \n3. Extracts factors from excerpts\n4. Creates the final summary table","814986c1":"### Filter input files to limit search space\n\nUse Flashtext to limit the search space from which excerpts, factors are generated along with other fields to create a summary table. Additional Covid19 keywords\/filters are in place to ensure papers surfaced are relevant to covid19\n","b9db3baa":"## Overall mean scors for different Journal Types","4ba5e9c8":"<b>Note<\/b> The graphs below have been generated based off results from an earlier run, partial results of the dataframe showb above. The same can be regenerated with the latest results.\n\nThe first view shows the entire diagram showing connections between the 'Study' column with their corresponding factors, which are comma separated under the 'factors' column. Each Study type is depicted by grey nodes, and all the factors are indicated by purple nodes.The study type nodes also vary in size and the sizes are determined by the values form the 'score' column.The score columns have been normalised using natural log for visualisation purpose.\n\n![Ridiculogram](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F704715%2F1251574%2Fridiculogram.png?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1592463654&Signature=meETaP77Qtk5Q%2BWgFgaihxjvVjd4fQJic8pW%2F%2FabA2mrOXccGxxk5aH3Oh1pMt%2Ff26ouFNP%2F%2B%2BhimLC7Ea5c9Lar1K7tnA8t5IHwTKhli89pPoCydxCnhcmuv5vqDumy%2Fa4WqtdZgDFAaMUr62gzstaC7oli0Zt8bUIYPYu9QE2CHrawvRR2fehE1E6C1xB44o0rB1AMMRCQBpLva%2FdNqan2ERmwK2R7xGDsliH%2F919F43ACsJrR8kjJdmVKzyT0bUpF2hrtp8u6GIIKh6dT1Kn4A8P7wx5GdnS0X1Q6ae%2BZlGSve%2BKUUZGnmn%2BXonTAspLa69PPV7AMUY4Z478V%2Bg%3D%3D)\n\nThis diagram is created using a combination of Fruchterman & Reingold (1991) and ForceAtlas2 methods. Study types that don't have many relevant factors are located at the periphery and the ones that have a lot of coresponding factors tend to be located at the centre of this diagram. These graphs are created using the software called Gephi(the graphml file has been uploaded in under the input folder of the Data pane).\n\n![Annotation](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F704715%2F1251574%2Fislands.png?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1592463677&Signature=jqe41eWPnXM9hctgFJg6fXgXx%2FdTSE0xE1eJkLI8jLCNHPPOT5fSUAPd4saxKAzfcMNrC3qQrQbprtkGSVD9LXsg%2BqmnUkOprLec6AnBjHwYAVo5Af4gwdxovTockhSKJNmUeIGHH98vebXFoBx59Ofeana0BOiXlL%2FD%2FOzd6AYcN7hLNOYXo3DmhLKcaTlS8ImC5ZtUhapaLx4tDkewlV5odTWM4YneuvFs2l9cx8FZ2os7QVE0d0AfnoxaqViS%2Bgn%2BZYG1dAHw2IKKsRDJnI46J6qYNwwk99Zp%2BYiHG8PQEsFI8Nmi60ChE%2BqSSn2fVwOlNDhiztOs55fmq8KMNg%3D%3D)\n\nThe second view shows the zoomed in views to a specific area of this graph. The third and last view shows that hovering over a specific study type and clicking it highlights that study type and its connected factors, while blurring the other nodes in this diagram.\n\n![Islands](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F704715%2F1251574%2Fannotation.png?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1592461955&Signature=MGoVAHWvOGaj4NgIBkcU2IFGAtcM%2Flcf%2BrOIpDzeET6x45sUQ4C8meELkS%2FM5Y7i1R3qrqau418epgFjN4PRAesZ9lTT9NSm1ytdY4kGodTUSbCA5aAMNCjWqzgRFl21saFV%2FRyIDNCzxRt7Cu%2BdIjIQQQjGraTv8OQYeI2WM4ocuN20NUYBySOzpLNcOGt5ndRck%2FE4bo%2FLglmOO7kZnZmxzG6lIYCuE3IHNoWfD%2FLSgDnhIiY1E3uWiYv3NsbUtllE%2Byk%2B4VLNdV%2F0QM%2Fs7lHoKR%2FjcqRiS8jcgiB9VBN%2Bo%2FLY843yhiSGgsTjpwj0E7eZuXAxGOZmFLUlOJyosg%3D%3D)\n\nHow to reproduce this Graph:\n\nAnyone can use the node_link-diagram-file gephi file (covid graph_fnl.graphml.gephi) under images dataset to recreate this result. Before plotting the graphs,the edge and node csv files(also in the images dataset) were created with the sample result view from the above. One file for the nodes and another file for edges are created in the format as per the files here. After uploading them in Gephi's data laboratory, nodes were sized based on the adjusted scores. This adj_score column from covid node.csv is a derived column from the score column of the dataset above, which has been normalised for visualisation purpose using natural log.Also, all the study nodes were coloured grey and factor nodes were coloured purple.The covid edge.csv file used the ID from the covid node.csv file to draw edges among the study type and the relevant factor nodes. This is done to facilitate graphing in Gephi. Using a combination of Fruchterman Reingold and ForceAtlas2 algorithms and iterating a number of times, the desired result was achieved, as shown in the diagrams above.\n\n# Conclusion\n\nThe approach paves ways to further improve the search and retrieve model to get the relevant content from the corpus. The key learnings we got from our implementation are:\n\n1. The linguistic based approach is effective, results relevant. However refinement of patterns will ensure improving the signal to noise ratio \n\n2. Flashtext used for filtering and reducing search scope may have performance limitations if the corpus is too large, therefore an alternate methodology might have to be considered\n\n3. Some of the subtasks may be grouped together based on similarity of patterns and thus simplifying the implementation\n\n4. We inspected data from social media and news feeds and including them in the analysis may throw some light on factors being discussed outside of academia.\n\n5. The linguistics approach and using Spacy's token matcher can be cpu intensive and therefore there is scope to make the pattern matching flow more efficient.\n\n# Future Work\n\nSome of the activities that are currently being worked out are:\n\n1. Refined patterns for all the considerations for this task\n\n2. Visualisation in the form of graphs and charts to depict the results obtained and some key insights.\n\n3. Topic modelling based on the excerpts and factors extracted for each sub task\n\n4. Use of pretrained models such as BioSent2Vec for better matching\n\n# Notes\n\n1. Installing Scispacy's model(en_core_sci_md) via notebook is unreliable. The model has been added as a dataset under scispacymodels. The path to the model is hard coded in the the load_models section.\n\n2. The filtered set of files are placed in \/kaggle\/working\/results\/search_results\/. For faster execution, we recommend use these files to generate patterns. These are essentially cached versions of intermediate results and by default if these files exists, the notebook uses these results. <b> If you want to run the filtering process sett RUN_PARTIAL_CACHED_RESULTS to False. <\/b>\n\n3. The summary tables are also saved as csv files and are placed under \/kaggle\/working\/results\/\n\n4. To run new patterns, define patterns in the patterns definition section and add them. Instantiage get_insights to kickstart workflow to generate summary table\n    > pat = patterns()\n      add_patterns(pat.isolation_patterns)\n\n    > grs = get_insights()\n      subtask1_filepath = \"\/kaggle\/input\/intermediateresults\/subtask1_filtered_files.txt\"\n      results1 = grs.process_files(subtask1_filepath, \"subtask1\", 10)\n","5cdacf1f":"Define patterns\nSearch patterns\nSearch patterns comprising of keywords have been used to find the relevant articles for each subtask. These patterns were constructed for each subtask as follows:\n\nExtract keywords from the subtasks themselves. For example, the keywords \"quarantine\" and \"isolation\" are present in the subtask \"Effectiveness of case isolation\/isolation of exposed individuals (i.e. quarantine)\" itself.\nInclude synonyms of the previously found keywords e.g. \"distancing\", which can be found through news and authoritative sources such as:\n[Australian Government Department of Health (https:\/\/www.health.gov.au\/news\/health-alerts\/novel-coronavirus-2019-ncov-health-alert\/coronavirus-covid-19-advice-for-travellers#travel-within-australia ,\n[US Centers for Disease Control and Prevention (CDC)(https:\/\/www.cdc.gov\/training\/QuickLearns\/epimode\/secondary-spread.html)\n\nAdd structure to the pattern by organising keywords e.g. the following snippet from the Seasonality pattern indicates that we are looking for articles which mention \"transmission\", \"transfer\" etc of \"coronavirus\" without strictly specifying what can be between our two sets of keywords:\n\n>\n{ \"TEXT\" : \n    { \"REGEX\" : \"transmission|transfer|transmissability|spead|spreading|contract|contracting|contracted\" \n    } \n    }, \n    { \"OP\" : \"*\" }, \n    { \"TEXT\" : { \"REGEX\" : \"coronavirus|covid|2019-nCoV\" } \n}\n\nThese patterns are then applied to the articles in the dataset.\n\nSimilarly, patterns are constructed for classifying an article's study type.","432cdec7":"# Seasonality of Transmission","50bc3dba":"# Effectiveness of inter\/inner travel restriction ","e56b3857":"Patterns are identified using Spacy's token matcher feature. This feature enables us to match based on linguistics patterns.","2a27171f":"# How does temperature and humidity affect the transmission of 2019-nCoV?","70bcdf70":"# Effectiveness of social distancing","dbf2d180":"# Effectiveness of community contact reduction\n","649d1213":"Identify study type based on linguistics patterns","61feee89":"Module to perform basic NLP operations such as identifying and extracting entities and noun phrases ","778e1fdd":"## Load Models\n\nLoad necessary spacy models for pattern matching and extracting key phrases. The loading of models are done within a singleton decorator to ensure multiple instances of models are not loaded.","64c880b8":"Module to carry out pre-processing text. The replace url function is picked from Textacy's source as the package could not be installed on the Kaggle platform. ","46c6f1e9":"# Effectiveness of workplace distancing","d50ee805":"# Create summary tables that address relevant factors related to COVID-19\n\n# Background\n\n\nScientists and researchers often have large amount of data to sieve through to help them identify papers, document and articles relevant to their research. This results in time that could have been spent more productively otherwise. In emergency situations such as Covid, it becomes even more critical to ensure research activities can be carried out effectively in a shorter span of time. There is a clear need to have the right technology to help researchers carry out their daily activitites effectively.\n\n# Solution Design\n\n![High Level Solution Design](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets%2F704715%2F1251574%2Fcord19.png?GoogleAccessId=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com&Expires=1592462011&Signature=e3%2BkNAWiP80slxfluUEoDRuqhc6UDZzoDzZzH4j7ylekOOXlZ2%2BzbjzFF56uvgTNyF5UTXK1lRlA6m7S9FYmcBB9wvStF3b8ws347juX%2FBrOrQTthwotaaKANb8CN6yvP%2Fd3KcBgx5UsdL6FN1cHMiO%2F8aFXbidtt8ILy2mC88UYkwXNCyTbf6pIx8qmPBvoE3N3yWmdb2dfJAkSBjmrqDUpcc0oIFoLjddOfAixAqLI5BahR%2BgkzepY4UdD8HHWywOQZXrd92yYrtRbFFN8epUmyhd5AysmdDTFVoDjwTsWYHgjKB80txFhsJBToyx7kqGmBduJIb4BO%2BStQSQYoQ%3D%3D)\nThough the solution implemented is for relevant factors related to Covid-19, it's designed to ensure it can be extended to address other questions or tasks pertaining to Covid-19. Some of the design principles we have followed are:\n\n1. Modularity\n\n  As part of the design, there's a clear demarcation between the capabilities of each functional blocks. Each cell in the notebook would correspond to a different python file or module and can be used for other use cases. \n   \n   \n2. Extensibility\n\nModularity by design paves way for the solution being extensible and thereby provides the capability to cater to other potential use cases or tasks. We've used a config driven approach to ensure workflows are generic and therefore extensible by design.\n\n\n3. Distributed Processing\n\nBy design the solution caters to code being processed in a distributed manner using Python's multiprocessing(mp) library. The modularity of the code is such that if we were to use a cluster, one could substitute mp with a more suitable library such as dask.\n    \n    \n# Implementation Details\n\n\nWe propose a four step approach in generating summary table for the sub tasks.  \n\n## 1. Identify relevant files\n\nThere are numerous options to filter our relevant files for further analysis. Linguistic approach will help us narrow down to a list but is computationally expensive and so is using a regular exprrssion based approach. We instead rely on Flashtext which is high performant and based off [Aho Corasick's algorithm](https:\/\/en.wikipedia.org\/wiki\/Aho%E2%80%93Corasick_algorithm) and the efficient use of trie datastructure. Since we are not dependent on partial match Flashtext is an effective way of filtering out noise. \n\n## 2. Identify relevant excerpts\n\nIn order identify relevant documents, we use linguistic patterns such as phrases and grammatical patterns to reduce the search scope from the shortlisted files to the one's which are truly relevant. \n\nWe use Python's multiprocessing library for distributed processing. At the heart of the solution is the use of Spacy's Token Matcher library as the pattern matching engine. The sub tasks are grouped based on similarity of patterns for different sub tasks \n\n## 3. Scoring, Ranking Excerpts and Extracting Factors\n\nEach pattern represents a sub task or a group of similar sub tasks. The pattern is then used to identify excerpts from each paper. Typically, a pattern would result in more than on match i.e more than one excerpts would be surfaced. Each match or excerpt is scored. The score depends on which part of the paper the excerpt is found. Titles have the highest score followed by abstract and then body text and so on. The scores are configurable. \n\nA paper is then scored based on the cumulative scores of all the excerpts extraced from that paper. The final summary table would include relevant papers sorted based on the paper's score. We use the cumulative score for excerpts as a indicator of how relevant a paper is to the given pattern. \n\nFactors are key phrases which are essentially trigram Noun Phrases extracted from each excerpt. The number of factors indicate how informative the excerpt is. We use this a way to rank excerpts. \n\n\n## 4. Identify Study Types, Measure of Evidence and parameters from Metadata\n\n### Study Types\nThe different study types was chosen based on references, titles and abstract from the research papers. We use a pre-trained model from [Scispacy](https:\/\/allenai.github.io\/scispacy\/) as the basis to identify which study type the papers are most similar to. We extract title, abstract and back matter from papers as the text against which a study type will be assigned. Since we also need descriptions of study type, we handcrafted the study type descriptions based on contents from Wikipedia. We then use Spacy's model trained on biomedical data with a large vocabulary and 50K vectors to get a cosine similarity score. Instead of obtaining top n study types, we instead define a threshold and get all study types which are strongly correlated with the text in the papers. \n\n### Measure of Evidence\nWe use Spacy's NER feature to extract entities which belong to GPE and DATE category and limit the entries upto 3 each. \n\n\n### Parameters from Metadata\nThe remainder of the columns in the summary table are obtained from the metadata file. The 'Influential' column for now defaults to Yes based on the hypothesis that the excerpts are matched against the different sub tasks and therefore influential. However, we are currently evaluating this hypothesis and is bound to change in the coming versions of this notebook.\nPublish date, Cord UID, Study Link and DOI are obtained from the metadata file.\n\n## Defining Patterns \n\nWe use Spacy's [Rule Based Matching Engine ](https:\/\/spacy.io\/usage\/rule-based-matching) to define, manage and use patterns. More specifically, we use Token Matcher as it provides the flexibility to use both regular expression and linguistic patterns. \n\n### Methodology\n\nFor each subtask we developed at least one pattern for searching the dataset and identifying extracting excerpts using the approach outlined below. \n\n1.\tCreate an initial sentence structure based on the subtasks\n     > e.g. [a. subtask keyword] of [b. case isolation] of [c. exposed]  [d. individuals] and [e. qualified]  [f. medical professionals] \n             \n            \n    \n2.\tGeneralise the sentences by including a wildcard character (*) in between to enable for flexibility between two key words\n\n    > \n            \"pattern\" : [\n                {\"TEXT\" : {  \"REGEX\" : \"effective|effectiveness|power|control||success\" }\n                },\n                {\n                    \"POS\" : \"ADP\"\n                },\n                {\n                    \"OP\" : \"*\"\n                },\n                {\n                    \"LEMMA\" : {\n                        \"IN\" : [\"expose\",\"quarantine\"] }, \"OP\" : \"*\"\n                },\n                {\n                    \"POS\" : \"VERB\", \"OP\" : \"*\" \n                },\n                {\n                    \"LEMMA\" : { \"IN\" : [ \"individual\", \"people\", \"society\", \"doctor\", \"professional\", \"personnel\", \"patient\" ] }\n                }\n            ]     \n           \n3.\tExtend on the key words specific to and across each subtask using a broad set of resources (synonym search such as wordnet, authoritative resources such as [Australian Government Department of Health](https:\/\/www.health.gov.au\/news\/health-alerts\/novel-coronavirus-2019-ncov-health-alert\/coronavirus-covid-19-advice-for-travellers#travel-within-australia) and [US Centers for Disease Control and Prevention (CDC)](https:\/\/www.cdc.gov\/training\/QuickLearns\/epimode\/secondary-spread.html), news articles found online, medical journals such as the LANCET, World Medical Association, Bulletin of the WHO, Cambridge University Press, New England Journal of Medicine etc.)\n\n    > e.g. PPE mask, effective, helpful, control, manage, spread, outbreak\n    \n    \n4.\tUse REGEX and LEMMA to account for synonym conjugations\n\n    > eg: Spread - spreading\n    \n\n# Interpreting Results\n\nThe first step of filtering files to reduce the search scope from tens of thousands of files to a few hundreds. Based on the keywords idnetified the distribution of papers covering the topics (sub tasks) are shown in the figure below.\n\nCoverage of sub tasks across research papers based on a small sample set. This may not be a complete representation of the entire dataset![image.png](attachment:image.png)\n\nThe results are obtained for each sub task to help researchers focus based on the task of interest. A sample summary table is shown in Figure 2. The score is a proxy of how informative the paper is with respect to the sub task. Factors are derived from excerpts. \nFor each subtask, we generate the table based off a very sample of the shortlisted papers to ensure completion of the entire flow and to be able to see results.\n\nOne of the ways one could consume the results is building a Knowledge Graph to undersand the relationships between factors, excerpts and papers. It would help researchers navigate through papers having strong tie strengths with other papers. An example of how this can be obtained is shown in the graphs below. ","95a88b6b":"# Effectiveness of a multifactorial strategy prevent secondary transmission","21c343aa":"# Effectiveness of case isolation\/isolation of exposed individuals (i.e. quarantine)","d9416e3f":"# Significant changes in transmissibility in changing seasons?"}}