{"cell_type":{"f6cb9535":"code","ee984112":"code","9b7b4506":"code","3d694483":"code","3b0d49c5":"code","2db947e5":"code","c0378da0":"code","e29baba7":"code","212e755d":"code","035a108f":"code","ea97b566":"code","6c36c6be":"code","7fe97a05":"code","25580461":"code","1476c29f":"code","4d3f07d1":"code","8ad0a05b":"code","5adf9e9f":"code","fd677400":"code","60927cfe":"code","7aaa2149":"code","b4a3946d":"code","cd1a14c6":"code","8832b873":"code","3687a1d1":"code","a4e45d1b":"code","d0b0591b":"code","0e455753":"code","057053c4":"code","df160474":"code","3420f0cf":"code","e559fbee":"code","057fbbdf":"code","55229f13":"code","f4fc0c4a":"code","b3956704":"code","6d1bc233":"code","b5dff9e9":"code","787c518a":"code","cf958a9b":"code","41baabfa":"code","eb4427b6":"code","a62c74d6":"code","0fdd8bbd":"code","4bc639a8":"code","9c6dff80":"code","23badb91":"code","6bc940fe":"code","50d13b84":"code","3121825b":"code","dae34bbc":"code","9fc3a66b":"code","def0326a":"code","3cb7fb26":"code","846081a8":"code","62af7322":"code","d2ac60d5":"code","79e3c7d1":"code","20d13838":"code","7c4053c0":"code","47c519c3":"code","df089843":"code","682586a4":"code","8e5f9984":"code","e2c66c51":"code","1ae235d2":"code","8ca7be94":"code","57b471fe":"code","ca6797d7":"code","f67ff17c":"code","0d0dee7a":"code","20283f87":"code","944774ee":"code","be3822ac":"code","988cd4dc":"code","43f55e1b":"code","714453ff":"code","54301626":"code","9e55ae23":"code","e84cda2d":"code","48032895":"code","0369c6d1":"code","20a0670c":"code","7a98ed40":"code","4724412e":"code","85e56fcb":"code","ec9469bf":"code","b7e2fcef":"code","65e018e1":"code","24d1bdd2":"code","617f0308":"markdown","4ca8cd29":"markdown","189775cb":"markdown","cd54d6a4":"markdown","a1ef6327":"markdown","d3ca6f2e":"markdown","57a4039c":"markdown","9f1d78e4":"markdown","1c522dbe":"markdown","8142f581":"markdown","0eeb8d66":"markdown","d257052d":"markdown","6148f825":"markdown","534ebfe3":"markdown","a21174b6":"markdown","0c7106dd":"markdown","4c20af95":"markdown","a134a2f2":"markdown","91d82599":"markdown","790ce055":"markdown","67c16d2f":"markdown","babfb597":"markdown","863c0265":"markdown","8788321f":"markdown","309c89e4":"markdown","8a3383cb":"markdown","09ca0d8e":"markdown","777c82f3":"markdown","2f588cd2":"markdown","561ea19a":"markdown","ec9ba8ca":"markdown","a602fd6f":"markdown","9f10d500":"markdown","94336dca":"markdown","9926293f":"markdown","9be2966e":"markdown","01feedd7":"markdown","0e33f755":"markdown"},"source":{"f6cb9535":"import pandas as pd\nimport numpy as np\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder","ee984112":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9b7b4506":"test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\nsample_submission = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")","3d694483":"test.head()","3b0d49c5":"train.head()","2db947e5":"sample_submission.head()","c0378da0":"train.dtypes","e29baba7":"train.head()","212e755d":"for i in train.columns:\n    print(train[i].isnull().value_counts())","035a108f":"mean_mr = train[train['Name'].str.find('Mr.')!=-1]['Age'].mean(skipna=True)\nmean_master = train[train['Name'].str.find('Master')!=-1]['Age'].mean(skipna=True)\nmean_miss = train[train['Name'].str.find('Miss.')!=-1]['Age'].mean(skipna=True)\nmean_mrs = train[train['Name'].str.find('Mrs.')!=-1]['Age'].mean(skipna=True)\n\nindex=0 #index_counter\nfor i in train['Age'].isnull():\n    if i==True:\n        name = train.loc[index,'Name']\n        if name.find('Mr.')!=-1:\n            train.loc[index,'Age']=mean_mr\n        elif name.find('Master')!=-1:\n            train.loc[index,'Age']=mean_master\n        elif name.find('Miss.')!=-1:\n            train.loc[index,'Age']=mean_miss\n        elif name.find('Mrs.')!=-1:\n            train.loc[index,'Age']=mean_mrs\n        else :\n            train.loc[index,'Age']= train['Age'].mean(skipna=True)\n    index+=1\n","ea97b566":"index=0 #index_counter\nfor i in test['Age'].isnull():\n    if i==True:\n        name = test.loc[index,'Name']\n        if name.find('Mr.')!=-1:\n            test.loc[index,'Age']=mean_mr\n        elif name.find('Master')!=-1:\n            test.loc[index,'Age']=mean_master\n        elif name.find('Miss.')!=-1:\n            test.loc[index,'Age']=mean_miss\n        elif name.find('Mrs.')!=-1:\n            test.loc[index,'Age']=mean_mrs\n        else :\n            test.loc[index,'Age']= test['Age'].mean(skipna=True)\n    index+=1","6c36c6be":"#Handling Embarking Null\ntrain['Embarked']=train['Embarked'].fillna(\"Unknown\")\ntest['Embarked']=train['Embarked'].fillna(\"Unknown\")","7fe97a05":"#Chaning Cabin Data\ntrain['Cabin'] = train['Cabin'].notnull().astype('int64')","25580461":"train_clean = train.copy()","1476c29f":"train_clean","4d3f07d1":"for i in ['Pclass','Sex','Cabin','SibSp','Parch','Embarked']:\n    plot_data = pd.pivot_table(data=train_clean, index= i,values='Survived')*100\n    plot_data = plot_data.round()\n\n    plt.figure(figsize=(15,8))\n    sns.barplot(data = plot_data,x=plot_data.index,y='Survived')\n\n    plt.xlabel(i)\n    plt.ylabel('Percentage(%)')\n    plt.title('% of Survivors based on {}'.format(i))\n    plt.show()\n    print(\"\\n\\n\\n\")","8ad0a05b":"train_clean['Ticket_prefix'] = train_clean['Ticket'].apply(lambda x: 'None' if(x.split(' ')[0].isdigit()) else x.split(' ')[0][0])\n# train_clean['Ticket_prefix'] = train_clean['Ticket'].apply(lambda x: 0 if(x.split(' ')[0].isdigit()) else 1)","5adf9e9f":"train_clean.head(3)","fd677400":"tic_num=[]\nfor x in train_clean['Ticket']:\n    y=x.split(' ')\n    if y[0]=='LINE':\n        tic_num.append(0)\n    elif y[0].isdigit():\n        tic_num.append(y[0])\n    else:\n        tic_num.append(y[-1])","60927cfe":"train_clean['Ticket_num']=tic_num","7aaa2149":"train_clean.head(3)","b4a3946d":"fig, axs = plt.subplots(2,4,figsize=(20,10))\nfig.subplots_adjust(hspace = .5, wspace=.5)\naxs = axs.ravel()\ncounter=0\n\nfor i in train_clean['Ticket_prefix'].unique():\n    plot_data = train_clean[train_clean['Ticket_prefix']==i][['Ticket_prefix','Survived']]\n    axs[counter].set_title('ticet prefix {}'.format(i))\n    axs[counter].hist(x=plot_data['Survived'])\n    counter+=1","cd1a14c6":"plot_data = pd.pivot_table(data=train_clean, index='Ticket_prefix',values='Survived')*100\nplot_data = plot_data.round()\n\nplt.figure(figsize=(15,8))\nsns.barplot(data = plot_data,x=plot_data.index,y='Survived')\n\nplt.xlabel('Ticket Prefix')\nplt.ylabel('Percentage(%)')\nplt.title('% of Survivors based Ticket Type')\nplt.show()","8832b873":"sns.distplot(train_clean['Age'], kde=False)\nplt.show()","3687a1d1":"bins=[i*9 for i in range(0,10)]\nlabels = ['1-9','10-18','19-27','28-36','37-45','46-54','55-63','64-72','73-81']\ntrain_clean['Age_binned'] =pd.cut(train_clean['Age'],bins,labels=labels)\ntrain_clean.head(3)","a4e45d1b":"fig, axs = plt.subplots(2,5,figsize=(20,8))\nfig.subplots_adjust(hspace = .5, wspace=.5)\naxs = axs.ravel()\ncounter=0\n\nfor i in train_clean['Age_binned'].unique():\n    plot_data = train_clean[train_clean['Age_binned']==i][['Age_binned','Survived']]\n    axs[counter].set_title('Between: {}'.format(i))\n    axs[counter].hist(x=plot_data['Survived'])\n    counter+=1","d0b0591b":"plot_data = pd.pivot_table(data=train_clean, index='Age_binned',values='Survived')*100\nplot_data = plot_data.round()\nsns.barplot(data = plot_data,x=plot_data.index,y='Survived')\nplt.xlabel('Age Bracket')\nplt.ylabel('Percentage(%)')\nplt.title('% of Survivors based on Age')\nplt.xticks(rotation=45)","0e455753":"sns.distplot(train_clean['Fare'], kde=False)\nplt.show()","057053c4":"bins=[i*10 for i in range(0,11)]\nbins.append(520)\nlabels = ['1-10','11-20','21-30','31-40','41-50','51-60','61-70','71-80','81-90','91-100','100+']\ntrain_clean['Fare_binned'] =pd.cut(train_clean['Fare'],bins, labels=labels)\ntrain_clean.head(3)","df160474":"fig, axs = plt.subplots(2,6,figsize=(25,8))\nfig.subplots_adjust(hspace = .5, wspace=.5)\naxs = axs.ravel()\ncounter=0\n\nfor i in train_clean['Fare_binned'].unique():\n    plot_data = train_clean[train_clean['Fare_binned']==i][['Fare_binned','Survived']]\n    axs[counter].set_title('Between: {}'.format(i))\n    axs[counter].hist(x=plot_data['Survived'])\n    counter+=1","3420f0cf":"plot_data = pd.pivot_table(data=train_clean, index='Fare_binned',values='Survived')*100\nplot_data = plot_data.round()\nsns.barplot(data = plot_data,x=plot_data.index,y='Survived')\nplt.xlabel('Fare Bracket')\nplt.ylabel('Percentage(%)')\nplt.title('% of Survivors based on Fare')\nplt.xticks(rotation=45)","e559fbee":"pd.pivot_table(data=train_clean, index='Ticket_prefix', values=['Fare','Survived'], aggfunc={'Survived':['mean'],'Fare':['count','mean','median','min','max']}).sort_values(('Fare','mean'))","057fbbdf":"test['Ticket_prefix'] = test['Ticket'].apply(lambda x: 'None' if(x.split(' ')[0].isdigit()) else x.split(' ')[0][0])","55229f13":"tic_num=[]\nfor x in test['Ticket']:\n    y=x.split(' ')\n    if y[0]=='LINE':\n        tic_num.append(0)\n    elif y[0].isdigit():\n        tic_num.append(y[0])\n    else:\n        tic_num.append(y[-1])\ntest['Ticket_num']=tic_num","f4fc0c4a":"bins=[i*9 for i in range(0,10)]\nlabels = ['1-9','10-18','19-27','28-36','37-45','46-54','55-63','64-72','73-81']\ntest['Age_binned'] =pd.cut(test['Age'],bins,labels=labels)","b3956704":"bins=[i*10 for i in range(0,11)]\nbins.append(520)\nlabels = ['1-10','11-20','21-30','31-40','41-50','51-60','61-70','71-80','81-90','91-100','100+']\ntest['Fare_binned'] =pd.cut(test['Fare'],bins,labels=labels)","6d1bc233":"test['Cabin'] = test['Cabin'].notnull().astype('int64')","b5dff9e9":"train_clean.head(3)","787c518a":"test.head(3)","cf958a9b":"train_clean.to_csv(\"train_clean.csv\")\ntest.to_csv(\"test_clean.csv\")","41baabfa":"# fare and Plass","eb4427b6":"train_clean.head()","a62c74d6":"data_corr=pd.concat([train_clean['Pclass'],pd.get_dummies(train_clean['Fare_binned'])],axis=1)\ndata_corr.corr()['Pclass']","0fdd8bbd":"pd.pivot_table(data=train_clean, index='Pclass',values='Cabin',aggfunc=['mean'])","4bc639a8":"import pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt","9c6dff80":"train_set = pd.read_csv(\"train_clean.csv\")\ntest_set =  pd.read_csv(\"test_clean.csv\")","23badb91":"train = train_set.drop(['PassengerId','Unnamed: 0','Name','Ticket','Ticket_num','Age','Fare'],axis=1)\ntest = test_set.drop(['PassengerId','Unnamed: 0','Name','Ticket','Ticket_num','Age','Fare'],axis=1)","6bc940fe":"print(train.shape)\ntrain.head()","50d13b84":"print(test.shape)\ntest.head()","3121825b":"train.columns","dae34bbc":"train_dum = pd.get_dummies(train)\ntest_dum = pd.get_dummies(test)","9fc3a66b":"from sklearn.model_selection import train_test_split","def0326a":"X = train_dum.drop(['Survived'],axis=1)\ny = train_dum['Survived'].values","3cb7fb26":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=np.random)","846081a8":"from sklearn import metrics","62af7322":"from sklearn.tree import DecisionTreeClassifier\n\ndepths=10\nmean_acc=np.zeros((depths))\n\nfor i in range(1,depths+1):\n    TitanicTree = DecisionTreeClassifier(criterion=\"entropy\", max_depth = i)\n    TitanicTree.fit(X_train,y_train)\n\n    yhat = TitanicTree.predict(X_valid)\n    mean_acc[i-1] = metrics.accuracy_score(y_valid, yhat)\n\nmean_acc","d2ac60d5":"print( \"The best accuracy was with\", mean_acc.max(), \"with k=\", mean_acc.argmax()+1) ","79e3c7d1":"TitanicTree = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 7)","20d13838":"from sklearn.neighbors import KNeighborsClassifier\nKs = 1 #Kstart\nKe = 15 #Kstop\nmean_acc = np.zeros((Ke-Ks+1))\n\nfor n in range(Ks,Ke+1):\n      \n    KNN = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n    yhat=KNN.predict(X_valid)\n    mean_acc[n-1] = metrics.accuracy_score(y_valid, yhat)\n\nmean_acc","7c4053c0":"plt.plot(range(Ks,Ke+1),mean_acc,'g')\n# plt.fill_between(range(1,Ks+1),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\n# plt.legend(('Accuracy ', '+\/- 3xstd'))\nplt.ylabel('Accuracy ')\nplt.xlabel('Number of Nabors (K)')\nplt.tight_layout()\nplt.show()","47c519c3":"print( \"The best accuracy was with\", mean_acc.max(), \"with k=\", mean_acc.argmax()+1) ","df089843":"KNN = KNeighborsClassifier(n_neighbors = 10).fit(X_train,y_train)","682586a4":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import jaccard_score\n\nLR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)\nyhat = LR.predict(X_valid)\nyhat_prob = LR.predict_proba(X_valid)\n\njaccard_score(y_valid, yhat)","8e5f9984":"from sklearn.metrics import log_loss\nlog_loss(y_valid, yhat_prob)","e2c66c51":"from sklearn.metrics import f1_score\nf1_score(y_valid, yhat, average='weighted') ","1ae235d2":"from sklearn import svm\nclf = svm.SVC(kernel='rbf').fit(X_train, y_train) ","8ca7be94":"yhat = clf.predict(X_valid)","57b471fe":"from sklearn.metrics import f1_score\nf1_score(y_valid, yhat, average='weighted') ","ca6797d7":"jaccard_score(y_valid, yhat)","f67ff17c":"metrics.accuracy_score(y_valid,yhat)","0d0dee7a":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(X_train,y_train)","20283f87":"yhat = rf.predict(X_valid)","944774ee":"metrics.accuracy_score(y_valid, yhat)","be3822ac":"from sklearn.model_selection import cross_val_score\nmodel_accuracy={'accuracy_training':{},'accuracy_valid':{}}\nmodels = [TitanicTree,KNN,LR,clf,rf]","988cd4dc":"TitanicTree.fit(X_train,y_train)\nyhat = TitanicTree.predict(X_valid)\nmodel_accuracy['accuracy_training']['DecisionTree']=cross_val_score(TitanicTree,X_train,y_train,scoring='accuracy',cv=5).mean()\nmodel_accuracy['accuracy_valid']['DecisionTree']=metrics.accuracy_score(y_valid, yhat)","43f55e1b":"KNN.fit(X_train,y_train)\nyhat = KNN.predict(X_valid)\nmodel_accuracy['accuracy_training']['KNN']=cross_val_score(KNN,X_train,y_train,scoring='accuracy',cv=5).mean()\nmodel_accuracy['accuracy_valid']['KNN']=metrics.accuracy_score(y_valid, yhat)","714453ff":"LR.fit(X_train,y_train)\nyhat = LR.predict(X_valid)\nmodel_accuracy['accuracy_training']['Logistic R']=cross_val_score(LR,X_train,y_train,scoring='accuracy',cv=5).mean()\nmodel_accuracy['accuracy_valid']['Logistic R']=metrics.accuracy_score(y_valid, yhat)","54301626":"clf.fit(X_train,y_train)\nyhat = clf.predict(X_valid)\nmodel_accuracy['accuracy_training']['Support Vector M']=cross_val_score(clf,X_train,y_train,scoring='accuracy',cv=5).mean()\nmodel_accuracy['accuracy_valid']['Support Vector M']=metrics.accuracy_score(y_valid, yhat)","9e55ae23":"rf.fit(X_train,y_train)\nyhat = rf.predict(X_valid)\nmodel_accuracy['accuracy_training']['Random Forest']=cross_val_score(rf,X_train,y_train,scoring='accuracy',cv=5).mean()\nmodel_accuracy['accuracy_valid']['Random Forest']=metrics.accuracy_score(y_valid, yhat)","e84cda2d":"pd.DataFrame(model_accuracy)","48032895":"from sklearn.metrics import classification_report\npredictions = clf.predict(X_valid) \nprint(classification_report(y_valid, predictions)) ","0369c6d1":"from sklearn.model_selection import GridSearchCV \n  \nparam_grid = {'C': [0.1, 1, 10, 100, 1000],  'gamma': [1, 0.1, 0.01, 0.001, 0.0001],'kernel': ['rbf']}  \n  \ngrid = GridSearchCV(clf, param_grid, refit = True, verbose = 3) \n  \ngrid.fit(X_train, y_train) ","20a0670c":"print(grid.best_params_,\"\\n\") \nprint(grid.best_estimator_) ","7a98ed40":"grid_predictions = grid.predict(X_valid) \nprint(classification_report(y_valid, grid_predictions)) ","4724412e":"prediction_final = grid.predict(test_dum)","85e56fcb":"prediction_final","ec9469bf":"test_set['Survived'] = prediction_final","b7e2fcef":"solution = test_set[['PassengerId','Survived']].set_index('PassengerId')","65e018e1":"solution","24d1bdd2":"solution.to_csv(\"Kaggle Submission.csv\")","617f0308":"#### NOTE: \n\nNow, I will tend to read on Titanic and understand the data - \n\n1. https:\/\/en.wikipedia.org\/wiki\/RMS_Titanic\n1. <a href='https:\/\/www.ultimatetitanic.com\/the-voyage'><i>The route<\/i><\/a>\n\nSomething about the ticket makes me feel that there could feature engineering here... \n\n","4ca8cd29":"__I need to deal with age and embarked__","189775cb":"#### 2. Making Dummy Variables \/OHE","cd54d6a4":"### 8. Ticket Prefix ","a1ef6327":"a score of 0.7 in this competition indicates you predicted Titanic survival correctly for 70% of people.","d3ca6f2e":"__Logistic Regression__\n","57a4039c":"__Decision Tree Classifier__","9f1d78e4":"so I need to reduce my false positives","1c522dbe":"__Question I need answers for ->__ \n\nQ: Difference between sklearn.OHE and pandas.get_dummies ?\n\nA: They are the same \n\nQ: Why does ticket prefix look like a very good predictor...as in it shows promise, a lot of ticket types got 1s or 0s... But the model ignores it ? (Decision Tree importance) \n\nA: frequency, every category should have approximately equal no of values to be considered...so if you specify a category with just 2 ppl in it .. it is considered as overfitting.. algorithm ignores such categories? <- Understand how decision tree algorithm works\n\n<- check this answer..Self proclaimed asnwer","8142f581":"Age has lesser quatifiable impact in terms of regression....other than people above 60 did not survive\n\nA decision tree type model will benefit more from this ","0eeb8d66":"### 9. Age Binning","d257052d":"We should use either of the data or data otherwise we overfit the model","6148f825":"#### 6. Paramter Tuning ","534ebfe3":"### Importing Clean Data","a21174b6":"### Comparing Fare, Ticket_prefix and Survivability","0c7106dd":"# Titanic Predictor","4c20af95":"##### A Kaggle Data Set","a134a2f2":"### Visualization","91d82599":"##### Question Section\nQuestions to study ->\nCan I do half the data under one model and the remaining half of the data under another model and then ensemble the data?\n\nFor example, Age will be a U-curve, so using polynomial modelling for it is accurate, \nwhere as, we have seen fare to be linear, so using MLR for linear \nand using Random forest for Categorical. This would yield best results?? What kind of limitations does this idea have ?","790ce055":"# Checking Correlation :","67c16d2f":"#### 4. Modelling","babfb597":"## Finally","863c0265":"#### 3. Train - Validation Splitting","8788321f":"## Steps :\n\n1. Select relevent columns\n1. Make Dummy Variables (OHE)\n1. train validation split (since we already have test set)\n1. Modelling\n1. Tune model using GridsearchCV\n1. Test ensambles\n\nSelect from the following Models:\n1. decision trees\n1. Naive Bayers\n1. Linear Discriminant Analysis\n1. K-nearest Neighbour\n1. Logistic Regression\n1. Neural Networks \n1. Support Vector Machin\n\n","309c89e4":"Finally adding it to the test set","8a3383cb":"See if I leave p class as labels of 1,2,3 the prediction will get be weight by the number that is meant for class representation.\n[AAh! This is called natural ordered relationships. Explained in the medium article below] \n\nThings I wanna do \n1. One Hot: Passenger Class \n1. One Hot: Sex\n1. One Hot: Spouse and sibling \n1. One Hot: Parent and Child\n1. One Hot: Embarkment\n1. Cabin detail (Present\/Absent)\n1. Embarked\n1. Ticket classification based on pre text\n1. Fare Binning ?\n1. Age Binning ? (5 width)\n\nThing I need to read on is label encoding vs one hot encoding impact on category prediction ML model.\n<a href='https:\/\/medium.com\/@michaeldelsole\/what-is-one-hot-encoding-and-how-to-do-it-f0ae272f1179'>A good Medium Article on this.<\/a>\n","09ca0d8e":"We see amazing result from ticket prefix,  \n* where some prefix result to 100% survivalibility\n* some result to 0%\n* some result to moderate 50%\n* But we get distinctive results which can be used to predict better future :P\n\nLet us furhter visualize","777c82f3":"__Random Forest Classifier__","2f588cd2":"### Handling nan data ","561ea19a":"__KNN - Classifier__","ec9ba8ca":"Therefore we can conclude that Higher the price the lower the Pclass value, which means higher the price the higher the passenger ticket class","a602fd6f":"##### Notice\n* __Ratio of dead:survived for Fare 0-70 > 1 (Anomaly exsists at Fare Bracket 50-60)__\n* __Ratio of dead:survived for Fare 70+ < 1__","9f10d500":"#### 5. Comparing Acuracy of each model using \"train\" data","94336dca":"__Support Vector Machines__","9926293f":"#### Repeat for test set","9be2966e":"Highest accuracy for 6 Neighbours","01feedd7":"### 10. Fare Binning","0e33f755":"#### 1. Selecting Relevent Columns"}}