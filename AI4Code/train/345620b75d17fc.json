{"cell_type":{"5f109198":"code","159a6732":"code","bacdc8e7":"code","69990013":"code","1addafc3":"code","95833faa":"code","9b464a87":"code","25c8a679":"code","43cba00e":"code","26cbef0c":"code","76dd9a47":"code","78bd0648":"code","5dd35df7":"code","9a5f470c":"code","dcfc5085":"code","5c0b117f":"code","e602fa04":"code","e6f39b45":"code","f042d4ea":"code","ba0b1af9":"code","24acbe1f":"code","62458041":"code","940b9836":"code","afbc0b03":"code","76db4b63":"code","bada0533":"code","2e8d2123":"code","cc41ccab":"code","e9df91b6":"code","d1663221":"code","72fa2a4d":"code","c1c47b5e":"code","d680a9a4":"code","e37edced":"code","90b0b838":"code","70d35e55":"code","d67d37a9":"code","1f9dffc8":"code","22d6a4ed":"code","170beb1f":"markdown","6e08482c":"markdown","c7796b2c":"markdown","c9809fde":"markdown","02aa4ad0":"markdown","c83fe70e":"markdown","50085d08":"markdown","24b9d482":"markdown","7d266ad0":"markdown","d55f209b":"markdown","3f9469ef":"markdown","a9b702d5":"markdown","491660a4":"markdown","e95f1872":"markdown","0c4a9b80":"markdown","2049ccf9":"markdown","5835dd89":"markdown","24ec44f8":"markdown","762de7c0":"markdown","68c8062b":"markdown","25e9caad":"markdown","2ef85747":"markdown","f0b2b1b3":"markdown","8f707c17":"markdown","4a269767":"markdown","23794494":"markdown","2e771719":"markdown","73cdb7c7":"markdown","c0e38237":"markdown","1f68a712":"markdown","785061f4":"markdown","d4c53fa8":"markdown","8c5f5f56":"markdown","cbf9c2af":"markdown","0e4c77d0":"markdown","8eb842aa":"markdown","6ea5c443":"markdown","8fa512a9":"markdown","21ff0038":"markdown","36532ce1":"markdown"},"source":{"5f109198":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","159a6732":"#Load the set\ntrain_df=pd.read_csv('..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')\ntrain_df.head()","bacdc8e7":"import numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing,metrics,manifold\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV,cross_val_predict\nfrom imblearn.over_sampling import ADASYN,SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nimport collections\nimport matplotlib.patches as mpatches\nfrom sklearn.metrics import accuracy_score\n%matplotlib inline\nfrom sklearn.preprocessing import RobustScaler\nimport xgboost\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import classification_report,roc_auc_score,roc_curve,r2_score,recall_score,confusion_matrix,precision_recall_curve\nfrom collections import Counter\nfrom sklearn.model_selection import StratifiedKFold,KFold,StratifiedShuffleSplit\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD,SparsePCA\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom nltk.tokenize import word_tokenize\nfrom collections import defaultdict\nfrom collections import Counter\nimport seaborn as sns\nfrom wordcloud import WordCloud,STOPWORDS\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nimport xgboost\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import classification_report,roc_auc_score,roc_curve,r2_score,recall_score,confusion_matrix,precision_recall_curve\nfrom collections import Counter\nfrom sklearn.model_selection import StratifiedKFold,KFold,StratifiedShuffleSplit\nfrom xgboost import XGBClassifier as xg\nfrom lightgbm import LGBMClassifier as lg\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingClassifier,RandomForestClassifier,AdaBoostClassifier,BaggingClassifier,ExtraTreesClassifier","69990013":"%%time\n#Convert the labels into integers (numerics) for reference.\n\ntrain_li=[]\nfor i in range(len(train_df)):\n    if (train_df['sentiment'][i]=='positive'):\n        train_li.append(1)\n    else:\n        train_li.append(0)\ntrain_df['Binary']=train_li\ntrain_df.head()","1addafc3":"%%time\n#Running the Preprocessing and cleaning phase as well as the TFIDF Vectorization\n\nimport re\n#Removes Punctuations\ndef remove_punctuations(data):\n    punct_tag=re.compile(r'[^\\w\\s]')\n    data=punct_tag.sub(r'',data)\n    return data\n\n#Removes HTML syntaxes\ndef remove_html(data):\n    html_tag=re.compile(r'<.*?>')\n    data=html_tag.sub(r'',data)\n    return data\n\n#Removes URL data\ndef remove_url(data):\n    url_clean= re.compile(r\"https:\/\/\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\n\n#Removes Emojis\ndef remove_emoji(data):\n    emoji_clean= re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    data=emoji_clean.sub(r'',data)\n    url_clean= re.compile(r\"https:\/\/\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\n#Lemmatize the corpus\ndef lemma_traincorpus(data):\n    lemmatizer=WordNetLemmatizer()\n    out_data=\"\"\n    for words in data:\n        out_data+= lemmatizer.lemmatize(words)\n    return out_data\n\ndef tfidf(data):\n    tfidfv = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), lowercase=True, max_features=150000)\n    fit_data_tfidf=tfidfv.fit_transform(data)\n    return fit_data_tfidf\n\n\ntrain_df['review']=train_df['review'].apply(lambda z: remove_punctuations(z))\ntrain_df['review']=train_df['review'].apply(lambda z: remove_html(z))\ntrain_df['review']=train_df['review'].apply(lambda z: remove_url(z))\ntrain_df['review']=train_df['review'].apply(lambda z: remove_emoji(z))\ncount_good=train_df[train_df['sentiment']=='positive']\ncount_bad=train_df[train_df['sentiment']=='negative']\ntrain_df['review']=train_df['review'].apply(lambda z: lemma_traincorpus(z))","95833faa":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.layers import LSTM, Dense,Flatten,Conv2D,Conv1D,GlobalMaxPooling1D,GlobalMaxPool1D\nfrom keras.optimizers import Adam\nimport numpy as np  \nimport pandas as pd \nimport keras.backend as k\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional,GRU\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.preprocessing import OneHotEncoder\nfrom keras.utils import to_categorical\nfrom keras.utils.vis_utils import plot_model","9b464a87":"##First Step is to test model performance without pretrained Embeddings\n## Will be using only Keras Embeddings in this case with a minimal neural network model\n\nmaxlen=1000\nmax_features=5000 \nembed_size=300\n\n#clean some null words or use the previously cleaned & lemmatized corpus\n\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['review'],train_y,test_size=0.2,random_state=42)\n\nval_x=test_x\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)","25c8a679":"\nmodel=Sequential()\nmodel.add(Embedding(max_features,embed_size,input_length=maxlen))\nmodel.add(LSTM(60))\nmodel.add(Dense(16,activation='relu'))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"simple_model.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\nmodel.fit(train_x,train_y,batch_size=512,epochs=3,verbose=2)","43cba00e":"## Get to know individual layer sizes and parameters \n\nfrom keras import backend as k\ninputs=model.input\noutputs=[layer.output for layer in model.layers]\nprint(f\"Outputs of the sequential layers{outputs}\")\nfunctions=[k.function([inputs],[outs]) for outs in outputs]\nprint(f'Sequential Model Layers{functions}')\n","26cbef0c":"#Fit and validate\nmodel.fit(train_x,train_y,batch_size=128,epochs=3,verbose=2,validation_data=(val_x,val_y))","76dd9a47":"##Build Static Embedding on top of a Neural Model\n\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow import keras\nfrom keras.preprocessing.text import Tokenizer\nmaxlen=1000\nmax_features=5000 \nembed_size=300\n\ntrain_sample=train_df['review']\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_sample))\ntrain_sample=tokenizer.texts_to_sequences(train_sample)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_sample=pad_sequences(train_sample,maxlen=maxlen)\n\n\n\nEMBEDDING_FILE = '..\/input\/glove-global-vectors-for-word-representation\/glove.6B.50d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","78bd0648":"inp=Input(shape=(maxlen,))\nz=Embedding(max_features,embed_size,weights=[embedding_matrix])(inp)\nz=Bidirectional(LSTM(60,return_sequences='True'))(z)\nz=GlobalMaxPool1D()(z)\nz=Dense(16,activation='relu')(z)\nz=Dense(1,activation='sigmoid')(z)\nmodel=Model(inputs=inp,outputs=z)\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\nplot_model(\n    model,\n    to_file=\"glove_simple_model.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96,\n)\n\nmodel.fit(train_x,train_y,batch_size=128,epochs=3,verbose=2,validation_data=(val_x,val_y))","5dd35df7":"#Make sure ELMO embeddings are working in tf version 1.15\n\n# !pip install tensorflow==1.15\nimport tensorflow as tf\nimport tensorflow_hub as tf_hub\n\nelmo = tf_hub.Module(\"https:\/\/tfhub.dev\/google\/elmo\/2\")\nembeddings = elmo(\n    [\"the cat is on the mat\", \"dogs are in the fog\"],\n    signature=\"default\",\n    as_dict=True)[\"elmo\"]\nembeddings","9a5f470c":"from sklearn.preprocessing import LabelEncoder\nlabel_y= LabelEncoder()\nlabels=label_y.fit_transform(train_df['sentiment'])\nlabels","dcfc5085":"import tensorflow as tf\nimport tensorflow_hub as tf_hub\n\n# from keras.layers import Input, Lambda, Dense\n# from keras import backend as k\nelmo_embed=tf_hub.Module(\"https:\/\/tfhub.dev\/google\/elmo\/2\",trainable=True)\n\n#Creating the elmo embeddings by squeezing the inputs\ndef create_embedding(z):\n    return elmo_embed(tf.squeeze(tf.cast(z,tf.string)),signature='default',as_dict=True)[\"default\"]\n\ntrain_y=labels[:500]\nX=train_df['review'][:500].tolist()\ntrain_x,test_x,train_y,test_y=train_test_split(np.asarray(X),train_y,test_size=0.2,random_state=42)\n#Create the model with ELMO Embeddings and Dense Layers\n\ninp=tf.keras.layers.Input(shape=(1,),dtype=tf.string)\nz=tf.keras.layers.Lambda(create_embedding,output_shape=(1024,))(inp)\nz=tf.keras.layers.Dense(128,activation='relu')(z)\nz=tf.keras.layers.Dense(1,activation='sigmoid')(z)\nmodel=tf.keras.Model(inputs=inp,outputs=z)\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\n# plot_model(\n#     model,to_file=\"seq2seq_elmo_model.png\",\n#     show_shapes=True,\n#     show_layer_names=True,\n#     rankdir=\"TB\",\n#     expand_nested=False,\n#     dpi=96)\n\n\nwith tf.Session() as session:\n#     K.set_session(session)\n    session.run(tf.global_variables_initializer())  \n    session.run(tf.tables_initializer())\n    history = model.fit(train_x, train_y, epochs=1, batch_size=16)\n    model.save_weights('.\/response-elmo-model.h5')\n\n# with tf.Session() as session:\n#     K.set_session(session)\n#     session.run(tf.global_variables_initializer())\n#     session.run(tf.tables_initializer())\n#     model.load_weights('.\/response-elmo-model.h5')  \n#     predicts = model.predict(x_test, batch_size=16)","5c0b117f":"!pip uninstall tensorflow -y\n!pip uninstall tensorflow-cloud -y\n!pip install -U tensorflow==1.15","e602fa04":"!pip uninstall pytorch-lightning -y\n!pip uninstall tensorflow-probability -y","e6f39b45":"maxlen=1000\nmax_features=5000 \nembed_size=300\n\n#clean some null words or use the previously cleaned & lemmatized corpus\n\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['review'],train_y,test_size=0.2,random_state=42)\n\nval_x=test_x\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n#sequence to sequence basic lstm encoder gru decoders\ndef seq2seq_encoder_decoder(maxlen,max_features,embed_size):\n    #Creating LSTM  encoder neural model with no pretrained embeddings\n    encoder_inp=Input(shape=(maxlen,))\n    encoder_embed=Embedding(max_features,embed_size,input_length=maxlen,trainable=True)(encoder_inp)\n    encoder_lstm_cell=LSTM(60,return_state='True')\n    encoder_outputs,encoder_state_lstm_h,encoder_state_lstm_c=encoder_lstm_cell(encoder_embed)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    #Creating LSTM decoder model and feeding the output states (h,c) of lstm of encoders\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(max_features,embed_size,input_length=maxlen,trainable=True)(decoder_inp)\n    decoder_lstm_cell=LSTM(60,return_sequences='True',return_state=True)\n    decoder_outputs,decoder_state_lstm_h,decoder_state_lstm_c=decoder_lstm_cell(decoder_embed,initial_state=[encoder_state_lstm_h,encoder_state_lstm_c])\n    decoder_dense_cell=Dense(16,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2=Dense(1,activation='sigmoid')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    return model\n    \n    \nmodel=seq2seq_encoder_decoder(maxlen,max_features,embed_size)  \nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nplot_model(\n    model,to_file=\"seq2seq_encoder_decoder_model.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n    \nmodel.fit([train_x,train_x],train_y,batch_size=512,epochs=3,verbose=2)\n","f042d4ea":"def seq2seq_encoder_decoder_glove(maxlen,max_features,embedding_matrix):\n    #Creating LSTM  encoder neural model with no pretrained embeddings\n    encoder_inp=Input(shape=(maxlen,))\n    encoder_embed=Embedding(max_features,embed_size,input_length=maxlen)(encoder_inp)\n    encoder_lstm_cell=LSTM(60,return_state='True')\n    encoder_outputs,encoder_state_lstm_h,encoder_state_lstm_c=encoder_lstm_cell(encoder_embed)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    #Creating LSTM decoder model and feeding the output states (h,c) of lstm of encoders\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(max_features,embed_size,input_length=maxlen)(decoder_inp)\n    decoder_lstm_cell=LSTM(60,return_sequences='True',return_state=True)\n    decoder_outputs,decoder_state_lstm_h,decoder_state_lstm_c=decoder_lstm_cell(decoder_embed,initial_state=[encoder_state_lstm_h,encoder_state_lstm_c])\n    decoder_dense_cell=Dense(16,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2=Dense(1,activation='sigmoid')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    return model\n    \n    \nmodel=seq2seq_encoder_decoder_glove(maxlen,max_features,embedding_matrix)  \nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nplot_model(\n    model,to_file=\"seq2seq_encoder_decoder_model_glove.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n    \nmodel.fit([train_x,train_x],train_y,batch_size=512,epochs=2,verbose=2)","ba0b1af9":"#Bidirectional LSTM Encoder-Decoder\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['review'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\ndef seq2seq_encoder_decoder_glove_bilstm(maxlen,max_features,embedding_matrix):\n    #Creating LSTM  encoder neural model with no pretrained embeddings\n    encoder_inp=Input(shape=(maxlen,))\n    encoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(encoder_inp)\n    encoder_lstm_cell=Bidirectional(LSTM(60,return_state='True'))\n    encoder_outputs,encoder_state_flstm_h,encoder_state_flstm_c,encoder_state_blstm_h,encoder_state_blstm_c=encoder_lstm_cell(encoder_embed)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    encoded_states=[encoder_state_flstm_h,encoder_state_flstm_c,encoder_state_blstm_h,encoder_state_blstm_c]\n    #Creating LSTM decoder model and feeding the output states (h,c) of lstm of encoders\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(decoder_inp)\n    \n    decoder_lstm_cell=Bidirectional(LSTM(60,return_sequences='True',return_state=True),merge_mode=\"concat\")\n    decoder_outputs,decoder_state_lstm_h,decoder_state_lstm_c,_,_=decoder_lstm_cell(decoder_embed,initial_state=encoded_states)\n#     decoderoutputs,_,_=decoder_lstm_cell(decoder_embed,initial_state=encoded_states)\n    \n    decoder_dense_cell=Dense(16,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2=Dense(1,activation='sigmoid')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    return model\n    \n    \nmodel=seq2seq_encoder_decoder_glove_bilstm(maxlen,max_features,embedding_matrix)  \nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nplot_model(\n    model,to_file=\"seq2seq_encoder_decoder_model_glove_bilstm.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n    \nmodel.fit([train_x,train_x],train_y,batch_size=512,epochs=2,verbose=2)","24acbe1f":"#Bidirectional LSTM Encoder-Decoder\n# maxlen=1000\n# max_features=5000 \n# embed_size=300\n\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['review'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\ndef seq2seq_encoder_decoder_glove_bilstm_hybrid(maxlen,max_features,embedding_matrix):\n    #Creating LSTM  encoder neural model with no pretrained embeddings\n    encoder_inp=Input(shape=(maxlen,))\n    encoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(encoder_inp)\n    encoder_lstm_cell=Bidirectional(LSTM(60,return_state='True'),merge_mode='sum')\n    encoder_outputs,encoder_state_flstm_h,encoder_state_flstm_c,encoder_state_blstm_h,encoder_state_blstm_c=encoder_lstm_cell(encoder_embed)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    encoded_states=[encoder_state_flstm_h+encoder_state_blstm_h,encoder_state_flstm_c+encoder_state_blstm_c]\n    #Creating LSTM decoder model and feeding the output states (h,c) of lstm of encoders\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(decoder_inp)\n    \n    decoder_lstm_cell=LSTM(60,return_sequences='True',return_state=True)\n    decoder_outputs,decoder_state_lstm_h,decoder_state_lstm_c=decoder_lstm_cell(decoder_embed,initial_state=encoded_states)\n#     decoderoutputs,_,_=decoder_lstm_cell(decoder_embed,initial_state=encoded_states)\n    \n    decoder_dense_cell=Dense(16,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2=Dense(1,activation='sigmoid')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    return model\n    \n    \nmodel=seq2seq_encoder_decoder_glove_bilstm_hybrid(maxlen,max_features,embedding_matrix)  \nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nplot_model(\n    model,to_file=\"seq2seq_encoder_decoder_model_glove_bilstm_hybrid.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n    \nmodel.fit([train_x,train_x],train_y,batch_size=512,epochs=2,verbose=2)\n","62458041":"!pip install MiniAttention\n#Bidirectional LSTM Hybrid Encoder-Decoder with Hierarchical Attention\nimport MiniAttention.MiniAttention as ma\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['review'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\ndef seq2seq_encoder_decoder_glove_bilstm_hybrid_attention(maxlen,max_features,embedding_matrix):\n    #Creating LSTM  encoder neural model with no pretrained embeddings\n    encoder_inp=Input(shape=(maxlen,))\n    encoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(encoder_inp)\n    encoder_embed_attention=ma.MiniAttentionBlock(None,None,None,keras.regularizers.L2(l2=0.02),None,None,None,None,None)(encoder_embed)\n    encoder_lstm_cell=Bidirectional(LSTM(60,return_state='True'),merge_mode=\"sum\")\n    encoder_outputs,encoder_state_flstm_h,encoder_state_flstm_c,encoder_state_blstm_h,encoder_state_blstm_c=encoder_lstm_cell(encoder_embed_attention)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    encoded_states=[encoder_state_flstm_h+encoder_state_blstm_h,encoder_state_flstm_c+encoder_state_blstm_c]\n    #Creating LSTM decoder model and feeding the output states (h,c) of lstm of encoders\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(decoder_inp)\n    \n    decoder_lstm_cell=LSTM(60,return_sequences='True',return_state=True)\n    decoder_outputs,decoder_state_lstm_h,decoder_state_lstm_c=decoder_lstm_cell(decoder_embed,initial_state=encoded_states)\n#     decoderoutputs,_,_=decoder_lstm_cell(decoder_embed,initial_state=encoded_states)\n    \n    decoder_dense_cell=Dense(16,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2=Dense(1,activation='sigmoid')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    return model\n    \n    \nmodel=seq2seq_encoder_decoder_glove_bilstm_hybrid_attention(maxlen,max_features,embedding_matrix)  \nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nplot_model(\n    model,to_file=\"seq2seq_encoder_decoder_model_glove_bilstm_hybrid.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n    \nmodel.fit([train_x,train_x],train_y,batch_size=512,epochs=2,verbose=2)","940b9836":"import MiniAttention.MiniAttention as ma\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['review'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n\n\nclass Simple_Attention(tf.keras.layers.Layer):\n    def __init__(self,units):\n        super(Simple_Attention,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,v):\n        self.q=q\n        self.v=v\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n        score=self.Wv(tf.nn.tanh(self.Wq(self.q)+self.Wk(self.v)))\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n#         context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n    \n\n        \ndef seq2seq_encoder_decoder_glove_bilstm_hybrid_bahdanau(maxlen,max_features,embedding_matrix):\n    #Creating LSTM  encoder neural model with no pretrained embeddings\n    encoder_inp=Input(shape=(maxlen,))\n    encoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(encoder_inp)\n#     encoder_embed_attention=ma.MiniAttentionBlock(None,None,None,keras.regularizers.L2(l2=0.02),None,None,None,None,None)(encoder_embed)\n    encoder_lstm_cell=LSTM(60,return_state='True')\n    encoder_outputs,encoder_state_flstm_h,encoder_state_flstm_c=encoder_lstm_cell(encoder_embed)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    encoded_states=[encoder_state_flstm_h,encoder_state_flstm_c]\n    \n    #Creating LSTM decoder model and feeding the output states (h,c) of lstm of encoders\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(decoder_inp)\n    bahdanau_attention=Simple_Attention(60)\n    \n    decoder_embed_attention_h,decoder_embed_wghts_h=bahdanau_attention(encoder_state_flstm_h,encoder_outputs)\n    decoder_embed_attention_c,decoder_embed_wghts_c=bahdanau_attention(encoder_state_flstm_c,encoder_outputs)\n#     print(decoder_embed_wghts)\n    decoder_lstm_cell=LSTM(60,return_sequences='True',return_state=True)\n    decoder_outputs,decoder_state_lstm_h,decoder_state_lstm_c=decoder_lstm_cell(decoder_embed,initial_state=[decoder_embed_wghts_h,decoder_embed_wghts_c])\n#     decoderoutputs,_,_=decoder_lstm_cell(decoder_embed,initial_state=encoded_states)\n    \n    decoder_dense_cell=Dense(16,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2=Dense(1,activation='sigmoid')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    return model\nmodel=seq2seq_encoder_decoder_glove_bilstm_hybrid_bahdanau(maxlen,max_features,embedding_matrix)  \nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nplot_model(\n    model,to_file=\"seq2seq_encoder_decoder_model_glove_bilstm_bahdanau_attention.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n\nmodel.fit([train_x,train_x],train_y,batch_size=512,epochs=2,verbose=2)\n","afbc0b03":"train_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['review'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n\n\nclass Simple_Attention(tf.keras.layers.Layer):\n    def __init__(self,units):\n        super(Simple_Attention,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,v):\n        self.q=q\n        self.v=v\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n        score=self.Wv(tf.nn.tanh(self.Wq(self.q)+self.Wk(self.v)))\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n#         context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n    \n\n        \ndef seq2seq_encoder_decoder_glove_bilstm_hybrid_bahdanau(maxlen,max_features,embedding_matrix):\n    #Creating LSTM  encoder neural model with no pretrained embeddings\n    encoder_inp=Input(shape=(maxlen,))\n    encoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(encoder_inp)\n#     encoder_embed_attention=ma.MiniAttentionBlock(None,None,None,keras.regularizers.L2(l2=0.02),None,None,None,None,None)(encoder_embed)\n    encoder_lstm_cell=GRU(60,return_state='True')\n    encoder_outputs,encoder_state_flstm_h=encoder_lstm_cell(encoder_embed)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    encoded_states=[encoder_state_flstm_h]\n    \n    #Creating LSTM decoder model and feeding the output states (h,c) of lstm of encoders\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(decoder_inp)\n    bahdanau_attention=Simple_Attention(60)\n    \n    decoder_embed_attention_h,decoder_embed_wghts_h=bahdanau_attention(encoder_state_flstm_h,encoder_outputs)\n#     decoder_embed_attention_c,decoder_embed_wghts_c=bahdanau_attention(encoder_state_flstm_c,encoder_outputs)\n#     print(decoder_embed_wghts)\n    decoder_lstm_cell=GRU(60,return_sequences='True',return_state=True)\n    decoder_outputs,decoder_state_lstm_h=decoder_lstm_cell(decoder_embed,initial_state=[decoder_embed_wghts_h])\n#     decoderoutputs,_,_=decoder_lstm_cell(decoder_embed,initial_state=encoded_states)\n    \n    decoder_dense_cell=Dense(16,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2=Dense(1,activation='sigmoid')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    return model\nmodel=seq2seq_encoder_decoder_glove_bilstm_hybrid_bahdanau(maxlen,max_features,embedding_matrix)  \nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nplot_model(\n    model,to_file=\"seq2seq_encoder_decoder_model_glove_bilstm_bahdanau_attention.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n\nmodel.fit([train_x,train_x],train_y,batch_size=512,epochs=2,verbose=2)\n","76db4b63":"import MiniAttention.MiniAttention as ma\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['review'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n\n\n\nclass Luong_Attention(tf.keras.layers.Layer):\n    def __init__(self,units):\n        super(Luong_Attention,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,v):\n        self.q=q\n        self.v=v\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n#         self.q=tf.transpose(self.q)\n        score=(self.q)*(self.v)\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n#         context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n    \n\n        \ndef seq2seq_encoder_decoder_glove_bilstm_hybrid_luong(maxlen,max_features,embedding_matrix):\n    #Creating LSTM  encoder neural model with no pretrained embeddings\n    encoder_inp=Input(shape=(maxlen,))\n    encoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(encoder_inp)\n#     encoder_embed_attention=ma.MiniAttentionBlock(None,None,None,keras.regularizers.L2(l2=0.02),None,None,None,None,None)(encoder_embed)\n    encoder_lstm_cell=LSTM(60,return_state='True')\n    encoder_outputs,encoder_state_flstm_h,encoder_state_flstm_c=encoder_lstm_cell(encoder_embed)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    encoded_states=[encoder_state_flstm_h,encoder_state_flstm_c]\n    \n    #Creating LSTM decoder model and feeding the output states (h,c) of lstm of encoders\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(decoder_inp)\n    luong_attention=Luong_Attention(128)\n    \n    decoder_embed_attention_h,decoder_embed_wghts_h=luong_attention(encoder_state_flstm_h,encoder_outputs)\n    decoder_embed_attention_c,decoder_embed_wghts_c=luong_attention(encoder_state_flstm_c,encoder_outputs)\n#     print(decoder_embed_wghts)\n    decoder_lstm_cell=LSTM(60,return_sequences='True',return_state=True)\n    decoder_outputs,decoder_state_lstm_h,decoder_state_lstm_c=decoder_lstm_cell(decoder_embed,initial_state=[decoder_embed_wghts_h,decoder_embed_wghts_c])\n#     decoderoutputs,_,_=decoder_lstm_cell(decoder_embed,initial_state=encoded_states)\n    \n    decoder_dense_cell=Dense(16,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2=Dense(1,activation='sigmoid')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    return model\nmodel=seq2seq_encoder_decoder_glove_bilstm_hybrid_luong(maxlen,max_features,embedding_matrix)  \nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nplot_model(\n    model,to_file=\"seq2seq_encoder_decoder_model_glove_bilstm_luong_attention.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n\nmodel.fit([train_x,train_x],train_y,batch_size=512,epochs=2,verbose=2)\n","bada0533":"train_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['review'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n\n\n\nclass Graves_Attention(tf.keras.layers.Layer):\n    def __init__(self,units):\n        super(Graves_Attention,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,v):\n        self.q=q\n        self.v=v\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n#         self.q=tf.transpose(self.q)\n        score=tf.math.cos((self.q)*(self.v))\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n#         context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n    \n\n        \ndef seq2seq_encoder_decoder_glove_bilstm_hybrid_graves(maxlen,max_features,embedding_matrix):\n    #Creating LSTM  encoder neural model with no pretrained embeddings\n    encoder_inp=Input(shape=(maxlen,))\n    encoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(encoder_inp)\n#     encoder_embed_attention=ma.MiniAttentionBlock(None,None,None,keras.regularizers.L2(l2=0.02),None,None,None,None,None)(encoder_embed)\n    encoder_lstm_cell=LSTM(60,return_state='True')\n    encoder_outputs,encoder_state_flstm_h,encoder_state_flstm_c=encoder_lstm_cell(encoder_embed)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    encoded_states=[encoder_state_flstm_h,encoder_state_flstm_c]\n    \n    #Creating LSTM decoder model and feeding the output states (h,c) of lstm of encoders\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(decoder_inp)\n    graves_attention=Graves_Attention(128)\n    \n    decoder_embed_attention_h,decoder_embed_wghts_h=graves_attention(encoder_state_flstm_h,encoder_outputs)\n    decoder_embed_attention_c,decoder_embed_wghts_c=graves_attention(encoder_state_flstm_c,encoder_outputs)\n#     print(decoder_embed_wghts)\n    decoder_lstm_cell=LSTM(60,return_sequences='True',return_state=True)\n    decoder_outputs,decoder_state_lstm_h,decoder_state_lstm_c=decoder_lstm_cell(decoder_embed,initial_state=[decoder_embed_wghts_h,decoder_embed_wghts_c])\n#     decoderoutputs,_,_=decoder_lstm_cell(decoder_embed,initial_state=encoded_states)\n    \n    decoder_dense_cell=Dense(16,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2=Dense(1,activation='sigmoid')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    return model\nmodel=seq2seq_encoder_decoder_glove_bilstm_hybrid_graves(maxlen,max_features,embedding_matrix)  \nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nplot_model(\n    model,to_file=\"seq2seq_encoder_decoder_model_glove_bilstm_graves_attention.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n\nmodel.fit([train_x,train_x],train_y,batch_size=512,epochs=2,verbose=2)\n","2e8d2123":"import math\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['review'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n\n\n\nclass Scaled_Dot_Product_Attention(tf.keras.layers.Layer):\n    def __init__(self,units):\n        super(Scaled_Dot_Product_Attention,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,v,n):\n        self.q=q\n        self.v=v\n        self.n=n\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n#         self.q=tf.transpose(self.q)\n        score=((self.q)*(self.v))\/math.sqrt(self.n)\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n#         context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n    \n\n        \ndef seq2seq_encoder_decoder_glove_bilstm_hybrid_scaled_dot_product(maxlen,max_features,embedding_matrix):\n    #Creating LSTM  encoder neural model with no pretrained embeddings\n    encoder_inp=Input(shape=(maxlen,))\n    encoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(encoder_inp)\n#     encoder_embed_attention=ma.MiniAttentionBlock(None,None,None,keras.regularizers.L2(l2=0.02),None,None,None,None,None)(encoder_embed)\n    encoder_lstm_cell=LSTM(60,return_state='True')\n    encoder_outputs,encoder_state_flstm_h,encoder_state_flstm_c=encoder_lstm_cell(encoder_embed)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    encoded_states=[encoder_state_flstm_h,encoder_state_flstm_c]\n    \n    #Creating LSTM decoder model and feeding the output states (h,c) of lstm of encoders\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(decoder_inp)\n    scaled_dot_product_attention=Scaled_Dot_Product_Attention(128)\n    \n    decoder_embed_attention_h,decoder_embed_wghts_h=scaled_dot_product_attention(encoder_state_flstm_h,encoder_outputs,64)\n    decoder_embed_attention_c,decoder_embed_wghts_c=scaled_dot_product_attention(encoder_state_flstm_c,encoder_outputs,64)\n#     print(decoder_embed_wghts)\n    decoder_lstm_cell=LSTM(60,return_sequences='True',return_state=True)\n    decoder_outputs,decoder_state_lstm_h,decoder_state_lstm_c=decoder_lstm_cell(decoder_embed,initial_state=[decoder_embed_wghts_h,decoder_embed_wghts_c])\n#     decoderoutputs,_,_=decoder_lstm_cell(decoder_embed,initial_state=encoded_states)\n    \n    decoder_dense_cell=Dense(16,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2=Dense(1,activation='sigmoid')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    return model\nmodel=seq2seq_encoder_decoder_glove_bilstm_hybrid_scaled_dot_product(maxlen,max_features,embedding_matrix)  \nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nplot_model(\n    model,to_file=\"seq2seq_encoder_decoder_model_glove_bilstm_scaled_dot_product_attention.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n\nmodel.fit([train_x,train_x],train_y,batch_size=512,epochs=2,verbose=2)","cc41ccab":"import math\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['review'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\n\n\n\nclass Scaled_Dot_Product_Self_Attention(tf.keras.layers.Layer):\n    def __init__(self,units):\n        super(Scaled_Dot_Product_Self_Attention,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,k,v,n):\n        self.q=q\n        self.v=v\n        self.n=n\n        self.k=k\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n#         self.q=tf.transpose(self.q)\n        score=(self.Wq(self.q)*self.Wk(self.k))\/math.sqrt(n)\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n        context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n    \n\n        \ndef seq2seq_encoder_decoder_glove_bilstm_hybrid_scaled_dot_product_self(maxlen,max_features,embedding_matrix):\n    #Creating LSTM  encoder neural model with no pretrained embeddings\n    encoder_inp=Input(shape=(maxlen,))\n    print(embedding_matrix.shape)\n    encoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(encoder_inp)\n#     encoder_embed_attention=ma.MiniAttentionBlock(None,None,None,keras.regularizers.L2(l2=0.02),None,None,None,None,None)(encoder_embed)\n    encoder_lstm_cell=LSTM(60,return_state='True')\n    encoder_outputs,encoder_state_flstm_h,encoder_state_flstm_c=encoder_lstm_cell(encoder_embed)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    encoded_states=[encoder_state_flstm_h,encoder_state_flstm_c]\n    \n    #Creating LSTM decoder model and feeding the output states (h,c) of lstm of encoders\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(max_features,embed_size,weights=[embedding_matrix])(decoder_inp)\n    scaled_dot_product_attention=Scaled_Dot_Product_Self_Attention(60)\n    \n    decoder_embed_attention_h,decoder_embed_wghts_h=scaled_dot_product_attention(encoder_state_flstm_h,encoder_state_flstm_h,encoder_outputs,64)\n    decoder_embed_attention_c,decoder_embed_wghts_c=scaled_dot_product_attention(encoder_state_flstm_c,encoder_state_flstm_c,encoder_outputs,64)\n#     print(decoder_embed_wghts)\n    decoder_lstm_cell=LSTM(60,return_sequences='True',return_state=True)\n    decoder_outputs,decoder_state_lstm_h,decoder_state_lstm_c=decoder_lstm_cell(decoder_embed,initial_state=[decoder_embed_wghts_h,decoder_embed_wghts_c])\n#     decoderoutputs,_,_=decoder_lstm_cell(decoder_embed,initial_state=encoded_states)\n    \n    decoder_dense_cell=Dense(16,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2=Dense(1,activation='sigmoid')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    return model\nmodel=seq2seq_encoder_decoder_glove_bilstm_hybrid_scaled_dot_product_self(maxlen,max_features,embedding_matrix)  \nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nplot_model(\n    model,to_file=\"seq2seq_encoder_decoder_model_glove_bilstm_scaled_dot_self_product_attention.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n\nmodel.fit([train_x,train_x],train_y,batch_size=512,epochs=2,verbose=2)\n","e9df91b6":"import math\nimport transformers\nfrom transformers import AutoTokenizer,AutoModelForQuestionAnswering\n\nmaxlen=1000\nembed_size=768\nmax_features=1000\ntrain_df=train_df[:1000]\nlabels=label_y.fit_transform(train_df['sentiment'])\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['review'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_x))\ntrain_x=tokenizer.texts_to_sequences(train_x)\nval_x=tokenizer.texts_to_sequences(val_x)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_x=pad_sequences(train_x,maxlen=maxlen)\nval_x=pad_sequences(val_x,maxlen=maxlen)\nval_y=test_y\nprint(\"Padded and Tokenized Training Sequence\".format(),train_x.shape)\nprint(\"Target Values Shape\".format(),train_y.shape)\nprint(\"Padded and Tokenized Training Sequence\".format(),val_x.shape)\nprint(\"Target Values Shape\".format(),val_y.shape)\n\ndef build_model(transformer, max_len=maxlen):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    #Replaced from the Embedding+LSTM\/CoNN layers\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    return cls_token,sequence_output\n    \n\n\nclass Scaled_Dot_Product_Self_Attention(tf.keras.layers.Layer):\n    def __init__(self,units):\n        super(Scaled_Dot_Product_Self_Attention,self).__init__()\n        self.units=units\n        self.Wq=tf.keras.layers.Dense(self.units)\n        self.Wk=tf.keras.layers.Dense(self.units)\n        self.Wv=tf.keras.layers.Dense(60)\n        \n    def call(self,q,k,v,n):\n        self.q=q\n        self.v=v\n        self.n=n\n        self.k=k\n#         print(self.q.shape)\n        q_t=tf.expand_dims(self.q,1)\n#         self.q=tf.transpose(self.q)\n        score=(self.Wq(self.q)*self.Wk(self.k))\/math.sqrt(n)\n        attention_wts=tf.nn.softmax(score,axis=1)\n#         print(attention_wts.shape)\n        context_vector=(attention_wts*self.v)\n        context_vector=tf.reduce_sum(context_vector,axis=1)\n#         print(context_vector.shape)\n        return context_vector,attention_wts\n    \n\ndef chunks(l, n):\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n    for i in range(0, len(l), n):\n        yield l[i:i + n]\n        \n        \ndef fetch_vectors(string_list,pretrained_model, batch_size=64):\n    # inspired by https:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/\n    tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n    model = transformers.TFDistilBertModel.from_pretrained(pretrained_model)\n    \n    fin_features = []\n    for data in chunks(string_list, batch_size):\n        tokenized = []\n        for x in data:\n            x = \" \".join(x.strip().split()[:300])\n            tok = tokenizer.encode(x, add_special_tokens=True)\n            tokenized.append(tok[:512])\n\n        max_len = 512\n        #bert variants have attention id, input id and segment id\n        padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized])\n        attention_mask = np.where(padded != 0, 1, 0)\n        input_ids = tf.convert_to_tensor(padded)\n        attention_mask = tf.convert_to_tensor(attention_mask)\n\n        last_hidden_states = model(input_ids, attention_mask=attention_mask)\n\n        features = last_hidden_states[0][:, 0, :].cpu().numpy()\n        fin_features.append(features)\n\n    fin_features = np.vstack(fin_features)\n    return fin_features\n\n        \ndef distilbert_encoder_decoder_attention(maxlen,max_features,distilbert_embeddings):\n    #Creating LSTM  encoder neural model with no pretrained embeddings\n    encoder_inp=Input(shape=(maxlen,))\n    encoder_embed=Embedding(distilbert_embeddings.shape[0],embed_size,weights=[distilbert_embeddings])(encoder_inp)\n    print(encoder_inp.shape)\n#     encoder_embed_attention=ma.MiniAttentionBlock(None,None,None,keras.regularizers.L2(l2=0.02),None,None,None,None,None)(encoder_embed)\n    encoder_lstm_cell=LSTM(60,return_state='True')\n    encoder_outputs,encoder_state_flstm_h,encoder_state_flstm_c=encoder_lstm_cell(encoder_embed)\n    print(f'Encoder Ouputs Shape{encoder_outputs.shape}')\n    encoded_states=[encoder_state_flstm_h,encoder_state_flstm_c]\n    \n    #Creating LSTM decoder model and feeding the output states (h,c) of lstm of encoders\n    decoder_inp=Input(shape=(maxlen,))\n    decoder_embed=Embedding(distilbert_embeddings.shape[0],embed_size,weights=[distilbert_embeddings])(decoder_inp)\n    scaled_dot_product_attention=Scaled_Dot_Product_Self_Attention(60)\n    \n    decoder_embed_attention_h,decoder_embed_wghts_h=scaled_dot_product_attention(encoder_state_flstm_h,encoder_state_flstm_h,encoder_outputs,64)\n    decoder_embed_attention_c,decoder_embed_wghts_c=scaled_dot_product_attention(encoder_state_flstm_c,encoder_state_flstm_c,encoder_outputs,64)\n#     print(decoder_embed_wghts)\n    decoder_lstm_cell=LSTM(60,return_sequences='True',return_state=True)\n    decoder_outputs,decoder_state_lstm_h,decoder_state_lstm_c=decoder_lstm_cell(decoder_embed,initial_state=[decoder_embed_wghts_h,decoder_embed_wghts_c])\n#     decoderoutputs,_,_=decoder_lstm_cell(decoder_embed,initial_state=encoded_states)\n    \n    decoder_dense_cell=Dense(16,activation='relu')\n    decoder_d_output=decoder_dense_cell(decoder_outputs)\n    decoder_dense_cell2=Dense(1,activation='sigmoid')\n    decoder_output=decoder_dense_cell2(decoder_d_output)\n    model=Model([encoder_inp,decoder_inp],decoder_output)\n    model.summary()\n    return model\n\n\n\n# transformer_layer = (\n#         transformers.TFDistilBertModel\n#         .from_pretrained('distilbert-base-multilingual-cased'))\n# embedding_matrix,embedding_vector=build_model(transformer_layer,maxlen)\n# embedding_matrix=tf.keras.layers.Reshape((maxlen, 768))(embedding_matrix)\ndistilbert_embeddings = fetch_vectors(train_df.review.values,'distilbert-base-uncased')\n\nmodel=distilbert_encoder_decoder_attention(maxlen,max_features,distilbert_embeddings)  \nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nplot_model(\n    model,to_file=\"distilbert_encoder_decoder_attention.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n\nmodel.fit([train_x,train_x],train_y,batch_size=512,epochs=2,verbose=2)\n","d1663221":"#Taken from Google-research implementation of transformer\n\nimport random, os, sys\nimport numpy as np\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.callbacks import *\nfrom keras.initializers import *\nimport tensorflow as tf\nfrom keras.engine.topology import Layer\nimport keras.backend as K\ntry:\n    from dataloader import TokenList, pad_to_longest\n    # for transformer\nexcept: pass\n\n#Layer normalization class\nclass LayerNormalization(Layer):\n    def __init__(self, eps=1e-6, **kwargs):\n        self.eps = eps\n        super(LayerNormalization, self).__init__(**kwargs)\n    def build(self, input_shape):\n        #Adding custom weights\n        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n                                     initializer=Ones(), trainable=True)\n        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],\n                                    initializer=Zeros(), trainable=True)\n        super(LayerNormalization, self).build(input_shape)\n    def call(self, x):\n        mean = K.mean(x, axis=-1, keepdims=True)\n        std = K.std(x, axis=-1, keepdims=True)\n        return self.gamma * (x - mean) \/ (std + self.eps) + self.beta\n    def compute_output_shape(self, input_shape):\n        return input_shape\n\n#Division by 8 (q.k\/d^0.5)\nclass ScaledDotProductAttention():\n    def __init__(self, d_model, attn_dropout=0.1):\n        self.temper = np.sqrt(d_model)\n        self.dropout = Dropout(attn_dropout)\n    def __call__(self, q, k, v, mask):\n        attn = Lambda(lambda x:K.batch_dot(x[0],x[1],axes=[2,2])\/self.temper)([q, k])\n        if mask is not None:\n            mmask = Lambda(lambda x:(-1e+10)*(1-x))(mask)\n            attn = Add()([attn, mmask])\n        attn = Activation('softmax')(attn)\n        attn = self.dropout(attn)\n        output = Lambda(lambda x:K.batch_dot(x[0], x[1]))([attn, v])\n        return output, attn\n\nclass MultiHeadAttention():\n    # mode 0 - big martixes, faster; mode 1 - more clear implementation\n    def __init__(self, n_head, d_model, d_k, d_v, dropout, mode=0, use_norm=True):\n        self.mode = mode\n        self.n_head = n_head\n        self.d_k = d_k\n        self.d_v = d_v\n        self.dropout = dropout\n        if mode == 0:\n            self.qs_layer = Dense(n_head*d_k, use_bias=False)\n            self.ks_layer = Dense(n_head*d_k, use_bias=False)\n            self.vs_layer = Dense(n_head*d_v, use_bias=False)\n        elif mode == 1:\n            self.qs_layers = []\n            self.ks_layers = []\n            self.vs_layers = []\n            for _ in range(n_head):\n                self.qs_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n                self.ks_layers.append(TimeDistributed(Dense(d_k, use_bias=False)))\n                self.vs_layers.append(TimeDistributed(Dense(d_v, use_bias=False)))\n        #Joining scaled dot product\n        self.attention = ScaledDotProductAttention(d_model)\n        self.layer_norm = LayerNormalization() if use_norm else None\n        self.w_o = TimeDistributed(Dense(d_model))\n\n    def __call__(self, q, k, v, mask=None):\n        d_k, d_v = self.d_k, self.d_v\n        n_head = self.n_head\n\n        if self.mode == 0:\n            qs = self.qs_layer(q)  # [batch_size, len_q, n_head*d_k]\n            ks = self.ks_layer(k)\n            vs = self.vs_layer(v)\n\n            def reshape1(x):\n                s = tf.shape(x)   # [batch_size, len_q, n_head * d_k]\n                x = tf.reshape(x, [s[0], s[1], n_head, d_k])\n                x = tf.transpose(x, [2, 0, 1, 3])  \n                x = tf.reshape(x, [-1, s[1], d_k])  # [n_head * batch_size, len_q, d_k]\n                return x\n            qs = Lambda(reshape1)(qs)\n            ks = Lambda(reshape1)(ks)\n            vs = Lambda(reshape1)(vs)\n\n            if mask is not None:\n                mask = Lambda(lambda x:K.repeat_elements(x, n_head, 0))(mask)\n            head, attn = self.attention(qs, ks, vs, mask=mask)  \n                \n            def reshape2(x):\n                s = tf.shape(x)   # [n_head * batch_size, len_v, d_v]\n                x = tf.reshape(x, [n_head, -1, s[1], s[2]]) \n                x = tf.transpose(x, [1, 2, 0, 3])\n                x = tf.reshape(x, [-1, s[1], n_head*d_v])  # [batch_size, len_v, n_head * d_v]\n                return x\n            head = Lambda(reshape2)(head)\n        elif self.mode == 1:\n            heads = []; attns = []\n            for i in range(n_head):\n                qs = self.qs_layers[i](q)   \n                ks = self.ks_layers[i](k) \n                vs = self.vs_layers[i](v) \n                head, attn = self.attention(qs, ks, vs, mask)\n                heads.append(head); attns.append(attn)\n            head = Concatenate()(heads) if n_head > 1 else heads[0]\n            attn = Concatenate()(attns) if n_head > 1 else attns[0]\n\n        outputs = self.w_o(head)\n        outputs = Dropout(self.dropout)(outputs)\n        if not self.layer_norm: return outputs, attn\n        outputs = Add()([outputs, q])\n        return self.layer_norm(outputs), attn\n#Feedforward layer using COnv1D and Layer normalization.\nclass PositionwiseFeedForward():\n    def __init__(self, d_hid, d_inner_hid, dropout=0.1):\n        self.w_1 = Conv1D(d_inner_hid, 1, activation='relu')\n        self.w_2 = Conv1D(d_hid, 1)\n        self.layer_norm = LayerNormalization()\n        self.dropout = Dropout(dropout)\n    def __call__(self, x):\n        output = self.w_1(x) \n        output = self.w_2(output)\n        output = self.dropout(output)\n        output = Add()([output, x])\n        return self.layer_norm(output)\n#Encoder layer containing self\/multi head attention with positionwisefeedforward\nclass EncoderLayer():\n    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):\n        self.self_att_layer = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n        self.pos_ffn_layer  = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)\n    def __call__(self, enc_input, mask=None):\n        output, slf_attn = self.self_att_layer(enc_input, enc_input, enc_input, mask=mask)\n        output = self.pos_ffn_layer(output)\n        return output, slf_attn\n#Decoder layer with same architecture as the encoder.\nclass DecoderLayer():\n    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, dropout=0.1):\n        self.self_att_layer = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n        self.enc_att_layer  = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n        self.pos_ffn_layer  = PositionwiseFeedForward(d_model, d_inner_hid, dropout=dropout)\n    def __call__(self, dec_input, enc_output, self_mask=None, enc_mask=None):\n        output, slf_attn = self.self_att_layer(dec_input, dec_input, dec_input, mask=self_mask)\n        output, enc_attn = self.enc_att_layer(output, enc_output, enc_output, mask=enc_mask)\n        output = self.pos_ffn_layer(output)\n        return output, slf_attn, enc_attn\n#This is from the paper \"Attention is all you need\" which hypothesizes sin and cosine for positional encoding\ndef GetPosEncodingMatrix(max_len, d_emb):\n    pos_enc = np.array([\n        [pos \/ np.power(10000, 2 * (j \/\/ 2) \/ d_emb) for j in range(d_emb)] \n        if pos != 0 else np.zeros(d_emb) \n            for pos in range(max_len)\n            ])\n    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2]) # dim 2i\n    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2]) # dim 2i+1\n    return pos_enc\n\n#normal padding class for masking\ndef GetPadMask(q, k):\n    ones = K.expand_dims(K.ones_like(q, 'float32'), -1)\n    mask = K.cast(K.expand_dims(K.not_equal(k, 0), 1), 'float32')\n    mask = K.batch_dot(ones, mask, axes=[2,1])\n    return mask\n\ndef GetSubMask(s):\n    len_s = tf.shape(s)[1]\n    bs = tf.shape(s)[:1]\n    mask = K.cumsum(tf.eye(len_s, batch_shape=bs), 1)\n    return mask\n\nclass Encoder():\n    def __init__(self, d_model, d_inner_hid, n_head, d_k, d_v, \\\n                layers=6, dropout=0.1, word_emb=None, pos_emb=None):\n        self.emb_layer = word_emb\n        self.pos_layer = pos_emb\n        self.emb_dropout = Dropout(dropout)\n        self.layers = [EncoderLayer(d_model, d_inner_hid, n_head, d_k, d_v, dropout) for _ in range(layers)]\n        \n    def __call__(self, src_seq, src_pos, return_att=False, active_layers=999):\n        x = self.emb_layer(src_seq)\n        if src_pos is not None:\n            pos = self.pos_layer(src_pos)\n            x = Add()([x, pos])\n        x = self.emb_dropout(x)\n        if return_att: atts = []\n        mask = Lambda(lambda x:GetPadMask(x, x))(src_seq)\n        for enc_layer in self.layers[:active_layers]:\n            x, att = enc_layer(x, mask)\n            if return_att: atts.append(att)\n        return (x, atts) if return_att else x\n\n\nclass Transformer():\n    def __init__(self, len_limit, d_model=embed_size, \\\n              d_inner_hid=512, n_head=10, d_k=64, d_v=64, layers=2, dropout=0.1, \\\n              share_word_emb=False, **kwargs):\n        self.name = 'Transformer'\n        self.len_limit = len_limit\n        self.src_loc_info = True\n        self.d_model = d_model\n        self.decode_model = None\n        d_emb = d_model\n\n        pos_emb = Embedding(len_limit, d_emb, trainable=False, \\\n                            weights=[GetPosEncodingMatrix(len_limit, d_emb)])\n\n        i_word_emb = Embedding(max_features, d_emb, weights=[embedding_matrix]) # Add Kaggle provided embedding here\n\n        self.encoder = Encoder(d_model, d_inner_hid, n_head, d_k, d_v, layers, dropout, \\\n                               word_emb=i_word_emb, pos_emb=pos_emb)\n\n        \n    def get_pos_seq(self, x):\n        mask = K.cast(K.not_equal(x, 0), 'int32')\n        pos = K.cumsum(K.ones_like(x, 'int32'), 1)\n        return pos * mask\n\n    def compile(self, active_layers=999):\n        src_seq_input = Input(shape=(None,))\n        src_seq = src_seq_input\n        src_pos = Lambda(self.get_pos_seq)(src_seq)\n        if not self.src_loc_info: src_pos = None\n\n        x = self.encoder(src_seq, src_pos, active_layers=active_layers)\n        # x = GlobalMaxPool1D()(x) # Not sure about this layer. Just wanted to reduce dimension\n        x = GlobalAveragePooling1D()(x)\n        outp = Dense(1, activation=\"sigmoid\")(x)\n\n        self.model = Model(inputs=src_seq_input, outputs=outp)\n        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nclassic_transformer = Transformer(maxlen, layers=1)\nclassic_transformer.compile()\nmodel = classic_transformer.model\nmodel.summary()\nplot_model(\n    model,to_file=\"Classic_Transformer.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)","72fa2a4d":"model.fit(train_x[:1000],train_y[:1000],epochs=2,verbose=2,batch_size=512)","c1c47b5e":"import numpy as np\nfrom transformers import AutoTokenizer, pipeline, TFDistilBertModel\nfrom scipy.spatial.distance import cosine\ndef transformer_embedding(name,inp,model_name):\n\n    model = model_name.from_pretrained(name)\n    tokenizer = AutoTokenizer.from_pretrained(name)\n    pipe = pipeline('feature-extraction', model=model, \n                tokenizer=tokenizer)\n    features = pipe(inp)\n    features = np.squeeze(features)\n    return features\nembedding_features1=transformer_embedding('distilbert-base-uncased',z[0],TFDistilBertModel)\nembedding_features2=transformer_embedding('distilbert-base-uncased',z[1],TFDistilBertModel)","d680a9a4":"import os\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\nimport transformers\nfrom tqdm.notebook import tqdm\nfrom tokenizers import BertWordPieceTokenizer\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom transformers import AutoTokenizer, pipeline, TFDistilBertModel\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n#allow experimental tf\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Data access\nGCS_DS_PATH = KaggleDatasets().get_gcs_path('imdb-dataset-of-50k-movie-reviews')\n\n# Configuration of hyperparameters\nEPOCHS = 3\n#batch size denotes the partitioning amongst the cluster replicas.\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMAX_LEN = 192","e37edced":"from kaggle_datasets import KaggleDatasets\nGCS_PATH = KaggleDatasets().get_gcs_path('imdb-dataset-of-50k-movie-reviews')\n!gsutil ls $GCS_PATH","90b0b838":"#Tokenize the data and separate them in chunks of 256 units\n\nmaxlen=512\nchunk_size=256\ndef fast_encode(texts, tokenizer, chunk_size=chunk_size, maxlen=maxlen):\n    tokenizer.enable_truncation(max_length=maxlen)\n#     tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n    #sliding window methodology\n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)\ndef build_model(transformer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    #Replaced from the Embedding+LSTM\/CoNN layers\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    model.summary()\n    return model\n\n# First load the real tokenizer\ntokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n# Save the loaded tokenizer locally\ntokenizer.save_pretrained('.')\n# Reload it with the huggingface tokenizers library\nfast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['review'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\nval_y=test_y\ntrain_x = fast_encode(train_x.astype(str), fast_tokenizer, maxlen=MAX_LEN)\nval_x = fast_encode(test_x.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_x, train_y))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((val_x, val_y))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)","70d35e55":"with strategy.scope():\n    transformer_layer = (\n        transformers.TFDistilBertModel\n        .from_pretrained('distilbert-base-multilingual-cased')\n    )\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()\nplot_model(\n    model,to_file=\"Distilbert_Transformer.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\nn_steps = train_x.shape[0] \/\/ BATCH_SIZE\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS\n)\n","d67d37a9":"\ntokenizer = transformers.AlbertTokenizer.from_pretrained('albert-base-v1')\n# Save the loaded tokenizer locally\ntokenizer.save_pretrained('.')\n# Reload it with the huggingface tokenizers library\nfast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['review'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\nval_y=test_y\ntrain_x = fast_encode(train_x.astype(str), fast_tokenizer, maxlen=MAX_LEN)\nval_x = fast_encode(test_x.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_x, train_y))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((val_x, val_y))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\nwith strategy.scope():\n    transformer_layer = (\n        transformers.TFAlbertModel\n        .from_pretrained('albert-base-v1')\n    )\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()\nplot_model(\n    model,to_file=\"AlbertTransformer.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n\nn_steps = train_x.shape[0] \/\/ BATCH_SIZE\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS\n)","1f9dffc8":"## Testing RobertaTransformer\nimport os\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\nimport transformers\nfrom tqdm.notebook import tqdm\nfrom tokenizers import BertWordPieceTokenizer\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom transformers import AutoTokenizer, pipeline, TFDistilBertModel\n\ntokenizer = transformers.XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n# Save the loaded tokenizer locally\ntokenizer.save_pretrained('.')\n# Reload it with the huggingface tokenizers library\nfast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['review'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\nval_y=test_y\ntrain_x = fast_encode(train_x.astype(str), fast_tokenizer, maxlen=MAX_LEN)\nval_x = fast_encode(test_x.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_x, train_y))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((val_x, val_y))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\nwith strategy.scope():\n    transformer_layer = (\n        transformers.TFRobertaModel\n        .from_pretrained('roberta-base')\n    )\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()\nplot_model(\n    model,to_file=\"Roberta-Transformer.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n\nn_steps = train_x.shape[0] \/\/ BATCH_SIZE\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS\n)","22d6a4ed":"## Testing GPT2Transformer\nimport os\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\nimport transformers\nfrom tqdm.notebook import tqdm\nfrom tokenizers import BertWordPieceTokenizer\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom transformers import AutoTokenizer, pipeline, TFDistilBertModel\n\ntokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2-medium')\n# Save the loaded tokenizer locally\ntokenizer.save_pretrained('.')\n# Reload it with the huggingface tokenizers library\nfast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\ntrain_y=labels\ntrain_x,test_x,train_y,test_y=train_test_split(train_df['review'],train_y,test_size=0.2,random_state=42)\nval_x=test_x\nval_y=test_y\ntrain_x = fast_encode(train_x.astype(str), fast_tokenizer, maxlen=MAX_LEN)\nval_x = fast_encode(test_x.astype(str), fast_tokenizer, maxlen=MAX_LEN)\n\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_x, train_y))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((val_x, val_y))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\nwith strategy.scope():\n    transformer_layer = (\n        transformers.TFGPT2Model\n        .from_pretrained('gpt2-medium')\n    )\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()\nplot_model(\n    model,to_file=\"GPT2-Transformer.png\",\n    show_shapes=True,\n    show_layer_names=True,\n    rankdir=\"TB\",\n    expand_nested=False,\n    dpi=96)\n\nn_steps = train_x.shape[0] \/\/ BATCH_SIZE\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS\n)","170beb1f":"## Inference from the Model\n\nThe Model has almost 1.5 million parameters to be trained even without any embeddings.This is the simplicity of using Keras to train such large parametric mdoels.\n\n<img src=\"https:\/\/media1.tenor.com\/images\/54603c681d37cecb2973e7974dea7f43\/tenor.gif?itemid=16430080\">","6e08482c":"## Conclusion of BERT-base Transformers\n\nThis section concludes the classification models created using all BERT-based transformer models ranging from Bert to Albert \/Roberta. These classes of Bidirectional Encoder Models are based on Discriminatory Transformer architectures and are well suited for classification tasks in general (although they are used for language modelling, question answering).\n\nNow we move forward to some Generative transformers like GPT.\n\n\n<img src=\"https:\/\/i.pinimg.com\/originals\/76\/04\/48\/760448c0de6bed1e9b810b006d264561.gif\">\n\n\n### GPT-Generative Pretraining\n\n\n<img src=\"https:\/\/jalammar.github.io\/images\/gpt2\/gpt2-self-attention-split-attention-heads-1.png\">\n\n\n[GPT-2](https:\/\/cdn.openai.com\/better-language-models\/language_models_are_unsupervised_multitask_learners.pdf) is a large transformer-based language model with 1.5 billion parameters, trained on a dataset[1] of 8 million web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data.\n\nTips:\n\n- GPT-2 is a model with absolute position embeddings so it\u2019s usually advised to pad the inputs on the right rather than the left.\n\n- GPT-2 was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next token in a sequence. Leveraging this feature allows GPT-2 to generate syntactically coherent text as it can be observed in the run_generation.py example script.\n\n- The PyTorch models can take the past as input, which is the previously computed key\/value attention pairs.\n\nResource:\n\n  - [Jay's Blog](http:\/\/jalammar.github.io\/illustrated-gpt2\/)","c7796b2c":"## Encoder Decoder Model With Bahdanau Attention\n\nThe model architecture is as follows:\n\n<img src=\"https:\/\/i.imgur.com\/lIMEt59.png\">","c9809fde":"## Self Attention \n\n\nThis is produced from [Google-research](http:\/\/papers.nips.cc\/paper\/7181-attention-is-all-you-need.pdf).Self-attention is the method the Transformer uses to bake the \u201cunderstanding\u201d of other relevant words into the one we\u2019re currently processing. [Jay's Blog](http:\/\/jalammar.github.io\/illustrated-transformer\/) provide a very good idea of this logic.\n\n<img src=\"http:\/\/jalammar.github.io\/images\/t\/transformer_self-attention_visualization.png\">\n\n\nThree vectors q,k and v (query,key and value) are taken into consideration for computation of the self attention mechanism.The q,k and v are normally of 64 dimensions.\n\n\n<img src=\"http:\/\/jalammar.github.io\/images\/t\/transformer_self_attention_vectors.png\">\n\n\nThe score is calculated by taking the dot product of the query vector with the key vector of the respective word we\u2019re scoring. So if we\u2019re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2.\n\n\n<img  src=\"http:\/\/jalammar.github.io\/images\/t\/transformer_self_attention_score.png\">\n\n\nThe third and forth steps are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper \u2013 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they\u2019re all positive and add up to 1.\n\n\n<img src=\"http:\/\/jalammar.github.io\/images\/t\/self-attention_softmax.png\">\n\n\n\nThis softmax score determines how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it\u2019s useful to attend to another word that is relevant to the current word.\nThe fifth step is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).\nThe sixth step is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).\n\n\n<img  src=\"http:\/\/jalammar.github.io\/images\/t\/self-attention-output.png\">\n\nThe cumulative computation can be thought of like this:\n\n<img src=\"http:\/\/jalammar.github.io\/images\/t\/self-attention-matrix-calculation-2.png\">\n\n","02aa4ad0":"## Encoder Decoder With Luong Attention\n\nIn this case, we are  going to replicate the training process with [Luong Dot Product (Multiplicative Style) Attention Mechanism](https:\/\/arxiv.org\/abs\/1508.04025).\n\n\n### Global Attention\n\n\n<img  src=\"https:\/\/miro.medium.com\/max\/626\/1*LhEapXF1mtaB3rDgIjcceg.png\">\n\n\nLuong, et al., 2015 proposed the \u201cglobal\u201d and \u201clocal\u201d attention. The global attention is similar to the soft attention, while the local one is an interesting blend between hard and soft, an improvement over the hard attention to make it differentiable: the model first predicts a single aligned position for the current target word and a window centered around the source position is then used to compute a context vector.\n\nThe commonality between Global and Local attention\n\n- At each time step t, in the decoding phase, both approaches, global and local attention, first take the hidden state h\u209c at the top layer of a stacking LSTM as an input.\n- The goal of both approaches is to derive a context vector \ud835\udcb8\u209c to capture relevant source-side information to help predict the current target word y\u209c\n- Attentional vectors are fed as inputs to the next time steps to inform the model about past alignment decisions.\n- Global and local attention models differ in how the context vector \ud835\udcb8\u209c is derived\n- Before we discuss the global and local attention, let\u2019s understand the conventions used by Luong\u2019s attention mechanism for any given time t\n  - \ud835\udcb8\u209c : context vector\n  - a\u209c : alignment vector\n  - h\u209c : current target hidden state\n  - h\u209b : current source hidden state\n  - y\u209c: predicted current target word\n  - h\u02dc\u209c : Attentional vectors\n\n- The global attentional model considers all the hidden states of the encoder when calculating the context vector \ud835\udcb8\u209c.\n- A variable-length alignment vector a\u209c equal to the size of the number of time steps in the source sequence is derived by comparing the current target hidden state h\u209c with each of the source hidden state h\u209b\n- The alignment score is referred to as a content-based function for which we consider three different alternatives\n\n\n### Local Attention\n\n\n<img src=\"https:\/\/miro.medium.com\/max\/538\/1*YXjdGl3CnSfHfzYpQiObgg.png\">\n\n\n- Local attention only focuses on a small subset of source positions per target words unlike the entire source sequence as in global attention\n- Computationally less expensive than global attention\n- The local attention model first generates an aligned position P\u209c for each target word at time t.\n- The context vector \ud835\udcb8\u209c is derived as a weighted average over the set of source hidden states within selected the window\n- The aligned position can be monotonically or predictively selected\n\n\n### Formula \n\n<img src=\"https:\/\/miro.medium.com\/max\/875\/1*_Ta67S8_lXTbVzJMztkxKg.png\">\n\n\nSome resources:\n\n- [Lilan's Blog](https:\/\/lilianweng.github.io\/lil-log\/2018\/06\/24\/attention-attention.html#:~:text=Self%2Dattention%2C%20also%20known%20as,summarization%2C%20or%20image%20description%20generation.)\n- [Paper](https:\/\/arxiv.org\/pdf\/1508.04025.pdf)\n- [Paper](https:\/\/arxiv.org\/pdf\/1508.04025.pdf)\n- [Paper](https:\/\/arxiv.org\/pdf\/1508.4025.pdf)\n","c83fe70e":"## Homologous Encoder Decoder With Pretrained Embeddings\n\nIn this context, we will be applying a pretrained static embedding (Glove-embedding matrix) to our Encoder Decoder model (comprising of LSTM units). Then we will visualize the training and performance on our dataset.\n\n","50085d08":"## Hybrid Encoder Decoder With Attention\n\nThis section will comprise of Hybrid Encoder Decoder Architectures with variants of Attention Mechanisms. For an introduction attention refers to allowing certain neural weights to be focussed during training and this in turn assists in model performance.\n\nThe main paper behind this is [Attention is all you need](https:\/\/paperswithcode.com\/paper\/attention-is-all-you-need)\nA preview of this is provided in the images below:\n\n<img src=\"https:\/\/www.tensorflow.org\/images\/seq2seq\/attention_mechanism.jpg\">\n\n<img src=\"https:\/\/www.tensorflow.org\/images\/seq2seq\/attention_equation_0.jpg\">\n\n\nMore details will be explained soon on the different variants . For now, this tf resource should [help](https:\/\/www.tensorflow.org\/tutorials\/text\/nmt_with_attention)","24b9d482":"## Recap from Workshop-1\n\nIn the previous [Notebook](https:\/\/www.kaggle.com\/colearninglounge\/nlp-end-to-end-cll-nlp-workshop), we learned about cleaning,EDA, vectorization and embeddings. We saw how cleaning played a significant role in determining knowledge embeddings from the corpus and also determined the similarity between words and sentences. \n\nWe will be following a few steps from the elaborate Notebook:\n\n- Use the cleaning methods (regex) for redundancy removal\n- Lemmatization\n- Vectorization\n- Embeddings (Static,Dynamic,Transformer)\n\nSince we have already implemented the Regex cleaning method, we can apply the same here. In the first section of this notebook, we will be running statistical classifiers, with 3 different vectorized data.\n\n- Non semantic TFIDF Vectorized Data\n- Static Embedding Vectorized Data\n- Dynamic Embedding Vectorized Data\n\nFor the first use case of statistical models, we will be relying on TFIDF Baseline with Statistical classifiers.\n\n\n<img src=\"https:\/\/i.pinimg.com\/originals\/b0\/ec\/e4\/b0ece436f4244f1f97bab3facf4d6b8a.gif\">\n","7d266ad0":"## Huggingface Transformers\n\nWe will be leveraging the power of Transformers (from [Huggingface](https:\/\/huggingface.co\/)) for training the corpus using any variant of transformer architecture. Some information regarding TPU usage:\n\n### TPU\n\n<img src=\"https:\/\/storage.googleapis.com\/kaggle-media\/tpu\/tpuv3angle.jpg\">\n\nIn this context, we will be using the TPU cluster from the Notebook (Hardware accelerations). TPUs provide a better performance with respect to Tensorflow and Keras computations on tensors against GPUs.But it has to be explicitly called out in the code segment.\n\n[Kaggle Documentation on TPUs provide an excellent starting point for this.Highly recommend to go through it.](https:\/\/www.kaggle.com\/docs\/tpu)\n\nSteps to check and run the TPU cluster:\n\n```python\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver() tf.config.experimental_connect_to_cluster(tpu) tf.tpu.experimental.initialize_tpu_system(tpu)\n\ninstantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\ninstantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope(): model = tf.keras.Sequential( \u2026 ) # define your model normally model.compile( \u2026 )\n\ntrain model normally\nmodel.fit(training_dataset, epochs=EPOCHS, steps_per_epoch=\u2026)\n```\n\nSome points on TPUs:\n\n- TPUs are network-connected accelerators and you must first locate them on the network. This is what TPUClusterResolver() does.\n- To go fast on a TPU, increase the batch size. The rule of thumb is to use batches of 128 elements per core (ex: batch size of 128* 8=1024 for a TPU with 8 cores). At this size, the 128x128 hardware matrix multipliers of the TPU (see hardware section below) are most likely to be kept busy. You start seeing interesting speedups from a batch size of 8 per core though. In the sample above, the batch size is scaled with the core count through this line of code:\n\n```\nBATCH_SIZE = 16 * tpu_strategy.num_replicas_in_sync\n```\n\n<img src=\"https:\/\/storage.googleapis.com\/kaggle-media\/tpu\/tpu_rule_of_thumb.png\">\n\n- With larger batch sizes, TPUs will be crunching through the training data faster. This is only useful if the larger training batches produce more \u201ctraining work\u201d and get your model to the desired accuracy faster. That is why the rule of thumb also calls for increasing the learning rate with the batch size. You can start with a proportional increase but additional tuning may be necessary to find the optimal learning rate schedule for a given model and accelerator\n\n- Because TPUs are very fast, many models ported to TPU end up with a data bottleneck. The TPU is sitting idle, waiting for data for the most part of each training epoch. TPUs read training data exclusively from GCS (Google Cloud Storage). And GCS can sustain a pretty large throughput if it is continuously streaming from multiple files in parallel. Following a couple of best practices will optimize the throughput:For TPU training, organize your data in GCS in a reasonable number (10s to 100s) of reasonably large files (10s to 100s of MB).\n\n- To enable parallel streaming from multiple TFRecord files, we can modify :\n\n   - num_parallel_reads=AUTO instructs the API to read from multiple files if available. It figures out how many automatically.\n   - experimental_deterministic = False disables data order enforcement. We will be shuffling the data anyway so order is not important. With this setting the API can use any TFRecord as soon as it is streamed in.\n\n\n### TPU Hardware\n\n\nAt approximately 20 inches (50 cm), a TPU v3-8 board is a fairly sizeable piece of hardware. It sports 4 dual-core TPU chips for a total of 8 TPU cores.Each TPU core has a traditional vector processing part (VPU) as well as dedicated matrix multiplication hardware capable of processing 128x128 matrices. This is the part that specifically accelerates machine learning workloads.\n\nTPUs are equipped with 128GB of high-speed memory allowing larger batches, larger models and also larger training inputs. In the sample above, you can try using 512x512 px input images, also provided in the dataset, and see the TPU v3-8 handle them easily.\n\n<img src=\"https:\/\/storage.googleapis.com\/kaggle-media\/tpu\/tpu_cores_and_chips.png\">\n\nSome resources:\n\n - [Cloud TPU](https:\/\/cloud.google.com\/tpu\/docs\/tpus)\n - [Tensorflow TPU](https:\/\/www.tensorflow.org\/tfrc)\n \n <img src=\"https:\/\/i.pinimg.com\/originals\/73\/d3\/a1\/73d3a14d212314ab1f7268b71d639c15.gif\">\n ","d55f209b":"## Graves Cosine Attention\n\nHere we apply ,cosine transformation on the Dot product Attention.\n\n<img src=\"https:\/\/theaisummer.com\/assets\/img\/posts\/attention\/attention-calculation.png\">\n\n\nArchitecture\n\n<img src=\"https:\/\/miro.medium.com\/max\/2048\/0*hMbmU5-BjN-i6mZh.jpg\">","3f9469ef":"## XLM Roberta\/Roberta\n\n[XLM](https:\/\/arxiv.org\/pdf\/1907.11692.pdf) builds on BERT and modifies key hyperparameters, removing the next-sentence pretraining objective and training with much larger mini-batches and learning rates.\n\n<img src=\"https:\/\/camo.githubusercontent.com\/f5c0d05eb0635cdd0e17e137265af23fa825b1d4\/68747470733a2f2f646c2e666261697075626c696366696c65732e636f6d2f584c4d2f786c6d5f6669677572652e6a7067\">\n\nTips:\n\nThis implementation is the same as BertModel with a tiny embeddings tweak as well as a setup for Roberta pretrained models.\n\nRoBERTa has the same architecture as BERT, but uses a byte-level BPE as a tokenizer (same as GPT-2) and uses a different pretraining scheme.\n\nRoBERTa doesn\u2019t have token_type_ids, you don\u2019t need to indicate which token belongs to which segment. Just separate your segments with the separation token tokenizer.sep_token (or <\/s>)\n\nCamemBERT is a wrapper around RoBERTa.\n\nResources:\n\n- [FAIR](https:\/\/ai.facebook.com\/blog\/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems\/)\n- [Pytorch](https:\/\/pytorch.org\/hub\/pytorch_fairseq_roberta\/)\n- [Github](https:\/\/github.com\/pytorch\/fairseq\/tree\/master\/examples\/roberta)\n- [Huggingface](https:\/\/huggingface.co\/transformers\/model_doc\/roberta.html)","a9b702d5":"## Standard Neural Networks with Static Semantic Embeddings Baseline\n\n\n<img src=\"https:\/\/miro.medium.com\/max\/688\/1*zR61FG9RUd6ul4ecXA_euQ.jpeg\">\n\n\nIn this context, we will be building a preliminary deep model using sophisticated neural networks and variants of RNNs. We will be building a simple LSTM model for validating the influence of deep models with respect to the statistical ones. In the first case, we will be using the Keras Embedding layer and visualize the results before using the embedding models.\n\n[Keras LSTM](https:\/\/keras.io\/api\/layers\/recurrent_layers\/lstm\/)\n[Keras](https:\/\/keras.io\/)\n[Keras Starter Guides](https:\/\/keras.io\/examples\/nlp\/)\n[Tensorflow Starter](https:\/\/www.tensorflow.org\/tutorials\/keras\/text_classification)\n[Tensorflow Hub](https:\/\/www.tensorflow.org\/tutorials\/keras\/text_classification_with_hub)\n[Jason's Blog-Best practises](https:\/\/machinelearningmastery.com\/best-practices-document-classification-deep-learning\/)\n[Jason's Blog-Convolution Networks](https:\/\/machinelearningmastery.com\/develop-word-embedding-model-predicting-movie-review-sentiment\/)\n\n\nMore resources will be provided, and for now we will be focussing on creating specific  RNN (Recurrent Neural Variants) with\/without Static Semantic Embeddings to create a Neural Model Baseline. \n\n\n<img src=\"https:\/\/miro.medium.com\/max\/875\/1*n-IgHZM5baBUjq0T7RYDBw.gif\">\n\n\n### Recurrent Neural Networks\n\nRecurrent neural networks (RNN) are a class of neural networks that is powerful for modeling sequence data such as time series or natural language.\n\nSchematically, a RNN layer uses a for loop to iterate over the timesteps of a sequence, while maintaining an internal state that encodes information about the timesteps it has seen so far.\n\nThe Keras RNN API is designed with a focus on:\n\n- Ease of use: the built-in keras.layers.RNN, keras.layers.LSTM, keras.layers.GRU layers enable you to quickly build recurrent models without having to make difficult configuration choices.\n\n- Ease of customization: You can also define your own RNN cell layer (the inner part of the for loop) with custom behavior, and use it with the generic keras.layers.RNN layer (the for loop itself). This allows you to quickly prototype different research ideas in a flexible way with minimal code.\n\n\nA classic RNN appears as follows:\n\n<img src=\"https:\/\/miro.medium.com\/max\/627\/1*go8PHsPNbbV6qRiwpUQ5BQ.png\">\n\nThis [video](https:\/\/youtu.be\/8HyCNIVRbSU) provides a good description of how RNNs work.\n\n\nParticulary a RNN works on the logic:\n\n\n<img src=\"https:\/\/miro.medium.com\/max\/875\/1*3mDe6V5DRXqpHYKDfxN4Rg.png\">\n\n\nThere are various kinds of such networks:\n\n\n- Encoding Recurrent Neural Networks are just folds. They\u2019re often used to allow a neural network to take a variable length list as input, for example taking a sentence as input.\n\n\n<img src=\"https:\/\/colah.github.io\/posts\/2015-09-NN-Types-FP\/img\/RNN-encoding.png\">\n\n\n- Generating Recurrent Neural Networks are just unfolds. They\u2019re often used to allow a neural network to produce a list of outputs, such as words in a sentence.\n\n\n<img src=\"https:\/\/colah.github.io\/posts\/2015-09-NN-Types-FP\/img\/RNN-generating.png\">\n\n\n- General Recurrent Neural Networks are accumulating maps. They\u2019re often used when we\u2019re trying to make predictions in a sequence. For example, in voice recognition, we might wish to predict a phenome for every time step in an audio segment, based on past context.\n\n\n<img src=\"https:\/\/colah.github.io\/posts\/2015-09-NN-Types-FP\/img\/RNN-general.png\">\n\n\n- Bidirectional Recursive Neural Networks are a more obscure variant, which I mention primarily for flavor. In functional programming terms, they are a left and a right accumulating map zipped together. They\u2019re used to make predictions over a sequence with both past and future context.\n\n<img src=\"https:\/\/colah.github.io\/posts\/2015-09-NN-Types-FP\/img\/RNN-bidirectional.png\">\n\n \n \nSome resources for understanding the derivatives and optimization inside the RNNs:\n\n- [Maths](https:\/\/www.cs.toronto.edu\/~tingwuwang\/rnn_tutorial.pdf)\n- [Blog](https:\/\/colah.github.io\/posts\/2015-09-NN-Types-FP\/)\n- [Blog](https:\/\/towardsdatascience.com\/under-the-hood-of-neural-networks-part-2-recurrent-af091247ba78)\n- [Kernel](https:\/\/www.kaggle.com\/abhilash1910\/nlp-workshop-ml-india#Neural-Networks)\n\n\nThese are some starter resources for creating preliminary networks for sentiment analysis, text\/intent classifications. There will be some advanced architectures which will be focussed later.\n\n\n### Long Short Term Memory (LSTM)\n\n[Drawbacks of RNNS](https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/): One of the appeals of RNNs is the idea that they might be able to connect previous information to the present task, such as using previous video frames might inform the understanding of the present frame. If RNNs could do this, they\u2019d be extremely useful. But can they? It depends.Sometimes, we only need to look at recent information to perform the present task. For example, consider a language model trying to predict the next word based on the previous ones. If we are trying to predict the last word in \u201cthe clouds are in the sky,\u201d we don\u2019t need any further context \u2013 it\u2019s pretty obvious the next word is going to be sky. In such cases, where the gap between the relevant information and the place that it\u2019s needed is small, RNNs can learn to use the past information.But there are also cases where we need more context. Consider trying to predict the last word in the text \u201cI grew up in France\u2026 I speak fluent French.\u201d Recent information suggests that the next word is probably the name of a language, but if we want to narrow down which language, we need the context of France, from further back. It\u2019s entirely possible for the gap between the relevant information and the point where it is needed to become very large.\nUnfortunately, as that gap grows, RNNs become unable to learn to connect the information.In theory, RNNs are absolutely capable of handling such \u201clong-term dependencies.\u201d A human could carefully pick parameters for them to solve toy problems of this form. Sadly, in practice, RNNs don\u2019t seem to be able to learn them. The problem was explored in depth by Hochreiter (1991) [German] and Bengio, et al. (1994), who found some pretty fundamental reasons why it might be difficult.\nThankfully, LSTMs don\u2019t have this problem!\n\n- LSTMs:\n \n <img src=\"https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/LSTM3-chain.png\">\n \n The first step in our LSTM is to decide what information we\u2019re going to throw away from the cell state. This decision is made by a sigmoid layer called the \u201cforget gate layer.\u201d It looks at ```ht\u22121``` and ```xt```, and outputs a number between 0 and 1 for each number in the cell state ```Ct\u22121```. A 1 represents \u201ccompletely keep this\u201d while a 0 represents \u201ccompletely get rid of this.\u201d\n\nLet\u2019s go back to our example of a language model trying to predict the next word based on all the previous ones. In such a problem, the cell state might include the gender of the present subject, so that the correct pronouns can be used. When we see a new subject, we want to forget the gender of the old subject.\n\n<img src=\"https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/LSTM3-focus-f.png\">\n\n\nThe next step is to decide what new information we\u2019re going to store in the cell state. This has two parts. First, a sigmoid layer called the \u201cinput gate layer\u201d decides which values we\u2019ll update. Next, a tanh layer creates a vector of new candidate values, ```C~t```, that could be added to the state. In the next step, we\u2019ll combine these two to create an update to the state.\n\nIn the example of our language model, we\u2019d want to add the gender of the new subject to the cell state, to replace the old one we\u2019re forgetting.\n\n<img src=\"https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/LSTM3-focus-i.png\">\n\nIt\u2019s now time to update the old cell state, ```Ct\u22121```, into the new cell state ```Ct```. The previous steps already decided what to do, we just need to actually do it.\n\nWe multiply the old state by ```ft```, forgetting the things we decided to forget earlier. Then we add ```it\u2217C~t```. This is the new candidate values, scaled by how much we decided to update each state value.\n\nIn the case of the language model, this is where we\u2019d actually drop the information about the old subject\u2019s gender and add the new information, as we decided in the previous steps.\n\n<img src=\"https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/LSTM3-focus-C.png\">\n\nFinally, we need to decide what we\u2019re going to output. This output will be based on our cell state, but will be a filtered version. First, we run a sigmoid layer which decides what parts of the cell state we\u2019re going to output. Then, we put the cell state through tanh (to push the values to be between \u22121 and 1) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.\n\nFor the language model example, since it just saw a subject, it might want to output information relevant to a verb, in case that\u2019s what is coming next. For example, it might output whether the subject is singular or plural, so that we know what form a verb should be conjugated into if that\u2019s what follows next.\n\n<img src=\"https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/LSTM3-focus-o.png\">\n\n\nAn illustrated working of the LSTM is provided:\n\n\n<img src=\"https:\/\/miro.medium.com\/max\/1900\/1*GjehOa513_BgpDDP6Vkw2Q.gif\">\n\n\nSome blogs:\n\n- [Blog](https:\/\/www.google.com\/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Fillustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21&psig=AOvVaw3GJ2-g9jyCgtlUxlTAmyJ8&ust=1608535825759000&source=images&cd=vfe&ved=0CA0QjhxqFwoTCLjax4eF3O0CFQAAAAAdAAAAABAD)\n- [Blog](https:\/\/www.analyticsvidhya.com\/blog\/2017\/12\/fundamentals-of-deep-learning-introduction-to-lstm\/)\n- [Blog](https:\/\/machinelearningmastery.com\/gentle-introduction-long-short-term-memory-networks-experts\/)\n- [Paper](https:\/\/static.googleusercontent.com\/media\/research.google.com\/en\/\/pubs\/archive\/43905.pdf)\n\nThere are several Variants of LSTMs some of the most famous being Depth GRU \/Gated Recurrent Units:\n\nA slightly more dramatic variation on the LSTM is the Gated Recurrent Unit, or GRU, introduced by Cho, et al. (2014). It combines the forget and input gates into a single \u201cupdate gate.\u201d It also merges the cell state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models, and has been growing increasingly popular.\n\n<img src=\"https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/LSTM3-var-GRU.png\">\n\n\n","491660a4":"## Roberta Base Architecture -768 D\n\nThe model architecture for Albert is as follows:\n\n<img src=\"https:\/\/i.imgur.com\/n6wDjpP.png\">","e95f1872":"## Encoder Decoder Model Architecture with Self Attention\n\nThe architecture is as follows:\n\n<img src=\"https:\/\/i.imgur.com\/3sdKFMW.png\">","0c4a9b80":"## Encoder Decoder Architectures\n\n[Encoder-Decoders](https:\/\/blog.keras.io\/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html) is a classic architecture mostly popular for sequence2sequence learning. Encoder-Decoders are most popularly used for neural machine translation (seq2seq learning with attention). The general workflow revolves around stacks of RNNs (LSTMs\/GRUs\/TimeDistributed Cells) which behaves as an encoder takes as input 3 parameters (max_features,embed_size,maxlen in our example) and returns an output. We then save the 2 output LSTM cell states ,the h and c states. We design the decoder model in a similar manner (if the internal layers are modified it becomes a hybrid decoder). And while passing the inputs of the decoder, we also pass the 2 output LSTM cell states from the encoder output (namely the h and c states). The output of the decoder is then passed through a activation\/distribution function to optimize our target loss function.\n\n\n<img src=\"https:\/\/miro.medium.com\/max\/875\/1*CkeGXClZ5Xs0MhBc7xFqSA.png\">\n\n\n\n### Descriptive overview of NMT with Encoder Decoders:\n\n\n\nIn the context of NMT, the words of one language should be mapped to a different language (machine translation). An example of such an architecture is as follows:\n\n\n\n<img src=\"https:\/\/blog.keras.io\/img\/seq2seq\/seq2seq-teacher-forcing.png\">\n\n\n\nIn the general case, input sequences and output sequences have different lengths (e.g. machine translation) and the entire input sequence is required in order to start predicting the target. This requires a more advanced setup, which is what people commonly refer to when mentioning \"sequence to sequence models\" with no further context. Here's how it works:\n\n- A RNN layer (or stack thereof) acts as \"encoder\": it processes the input sequence and returns its own internal state. Note that we discard the outputs of the encoder RNN, only recovering the state. This state will serve as the \"context\", or \"conditioning\", of the decoder in the next step.\n\n- Another RNN layer (or stack thereof) acts as \"decoder\": it is trained to predict the next characters of the target sequence, given previous characters of the target sequence. Specifically, it is trained to turn the target sequences into the same sequences but offset by one timestep in the future, a training process called \"teacher forcing\" in this context. Importantly, the encoder uses as initial state the state vectors from the encoder, which is how the decoder obtains information about what it is supposed to generate. Effectively, the decoder learns to generate targets[t+1...] given targets[...t], conditioned on the input sequence.\n\n\n\n### Pictorial Representation of Encoder-Decoders for Generative Modelling\n\n\n\n<img src=\"https:\/\/miro.medium.com\/max\/1250\/1*LYGO4IxqUYftFdAccg5fVQ.png\">\n\n\n\nSome resources:\n\n- [TF-Blog](https:\/\/www.tensorflow.org\/tutorials\/text\/nmt_with_attention)\n- [Blog](https:\/\/towardsdatascience.com\/how-to-implement-seq2seq-lstm-model-in-keras-shortcutnlp-6f355f3e5639)\n- [Blog](https:\/\/machinelearningmastery.com\/develop-encoder-decoder-model-sequence-sequence-prediction-keras\/)\n","2049ccf9":"## Run the same model with the Pretrained Embeddings\n\nNow we will run the same model as before with the pretrained static embeddings- Glove in our use case. I have trained it for 2 epochs but this can be made to train on an even larger epoch size.","5835dd89":"## Albert Base Architecture -768 D\n\nThe model architecture for Albert is as follows:\n\n<img src=\"https:\/\/i.imgur.com\/ztcIbsb.png\">","24ec44f8":"## Encoder Decoder Model Architecture with Luong Attention\n\nThe architecture is as follows:\n\n<img src=\"https:\/\/i.imgur.com\/ZAJ2iTH.png\">","762de7c0":"### Creating a Basic LSTM Neural Model without Embeddings\n\nIn this case, we will not be using an y pretrained static\/dynamic embeddings but will be using a simple Neural Network model of LSTM to create our network.The steps are as follows:\n\n\n- Tokenize the input data (Keras.Preprocessing)\n- Creating the limits of Maxlen, Max Features and Embedding Size for our Embedding Matrix\n- Pad the tokenized data to maintain uniformity in length of the input features\n\nA more descriptive overview is found [here](https:\/\/www.kaggle.com\/abhilash1910\/nlp-workshop-ml-india#Neural-Networks) . This also provides an [idea](https:\/\/www.tensorflow.org\/guide\/keras\/rnn)\n\n\n","68c8062b":"## Conclusion on Attention\n\nWe have seen different flavours of attention mechanism and in the next section, we move forward to transformers.\n\n\n<img src=\"https:\/\/i.pinimg.com\/originals\/4a\/2e\/2b\/4a2e2b7a3aabd03daebf11d1a2e970cc.gif\">","25e9caad":"## Creating the Model architecture\n\nHere we creating a [sequential model](https:\/\/keras.io\/api\/models\/sequential\/) and the embedding always has to be the first layer for our use case. In any neural model, Embedding layer always comes first followed by other layers- LSTM\/GRU and others. The heirarchy of the model can be represented as below:\n\n<img src=\"https:\/\/media.springernature.com\/lw685\/springer-static\/image\/art%3A10.1186%2Fs12859-019-3079-8\/MediaObjects\/12859_2019_3079_Fig2_HTML.png\">\n\n\nA proper model overview comprising of LSTMs and Embeddings is provided here:\n\n<img src=\"https:\/\/d3i71xaburhd42.cloudfront.net\/6ac8328113639044d2beb83246b9d07f513ac6c8\/3-Figure1-1.png\">\n\nSome resources:\n\n- [Kernels](https:\/\/www.kaggle.com\/rajmehra03\/a-detailed-explanation-of-keras-embedding-layer)\n- [Kernels](https:\/\/www.kaggle.com\/christofhenkel\/how-to-preprocessing-when-using-embeddings)\n","2ef85747":"## Build a Static Semantic Embedding Neural Network(LSTM) Baseline\n\nIn this case, we will be using pretrained embeddings for ouruse case. For this we will be using the embedding matrix creation  code from our [previous Notebook](https:\/\/www.kaggle.com\/colearninglounge\/nlp-end-to-end-cll-nlp-workshop).\n\nParticularly this lines of code:\n\n```python\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow import keras\nfrom keras.preprocessing.text import Tokenizer\nmaxlen=1000\nmax_features=5000 \nembed_size=300\n\ntrain_sample=train_df['review']\n\n#Tokenizing steps- must be remembered\ntokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_sample))\ntrain_sample=tokenizer.texts_to_sequences(train_sample)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_sample=pad_sequences(train_sample,maxlen=maxlen)\n\n\n\nEMBEDDING_FILE = '..\/input\/wikinews300d1msubwordvec\/wiki-news-300d-1M-subword.vec'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\nplt.plot(embedding_matrix[20])\nplt.show()\n```","f0b2b1b3":"## DistilBERT base Architecture -768 D\n\nThe model architecture for Distilbert for classification is provided here:\n\n<img src=\"https:\/\/i.imgur.com\/uUzt9dk.png\">","8f707c17":"## NLP Model Building: Transformers ,Neural Networks,Attention & more\n\n\nAuthored by [abhilash1910](https:\/\/www.kaggle.com\/abhilash1910)\n\n\n### Movie Reviews !!\n\nThis is an extension of the original Notebook which has been separately provided for a piecewise analysis of the NLP Model building with Transformers and sophiosticated Neural architectures. For more details other kernels are also provided:\n\n- [Kernel](https:\/\/www.kaggle.com\/colearninglounge\/nlp-end-to-end-cll-nlp-workshop)\n- [Kernel](https:\/\/www.kaggle.com\/abhilash1910\/nlp-workshop-ml-india)\n\n\nThe second and most interesting part of the curriculum is building different models - both statistical and deep learning so as to provide a proper classification model for our use case. In this case, we will create an initial baseline with statistical classification algorithms by using both non semantic vectors as well as semantic vectors. Later we will try to improvise on these baselines with standard neural models like Bidirectional LSTMs, Convolution Networks, Gated Recurrecnt Units. The improvements of these traditional neural models over the baselines would be further investigated when we will explore advanced architectures, particularly that of an encoder decoder . Further advancement would be made on attention based encoder-decoder modules like Transformers and using the different flavours from BERT to GPT.\n\nThe following is the workflow of this notebook:\n\n  \n- Traditional NN models\n  - With Static Embeddings\n  - With Dynamic Embeddings\n  - Models: LSTM,CNN,BiLSTM,GRU,Encoder-Decoders\n\n\n- Advanced Architectures\n  - Transformers\n  - Attention\n  - BERT TPU\n  - All BERT variants\n  - GPT TPU\n  - All variants of GPT2\n  - Hybrid Transformer\n  \n\nThis is an in depth approach to analyse the performance of different models on this task.\n\n<img src=\"https:\/\/lumiere-a.akamaihd.net\/v1\/images\/eu_bpan_showcase_hero_v4_m_823e00f3.jpeg?region=0,0,750,668\">\n","4a269767":"## Classic Transformer Architecture -MultiHead Self Attention\n\nThe following is the architecture:\n\n<img src=\"https:\/\/i.imgur.com\/nImDpAf.png\">","23794494":"## Homologous Encoder-Decoder Model Architecture\n\nThe model architecture with pretrained Glove 50d embeddings :\n\n<img src=\"https:\/\/i.imgur.com\/WQww2yU.png\">","2e771719":"## Hybrid Encoder Decoder Models\n\nThese classes of encoder decoders allow different variants of RNNs (LSTM\/Bilstm) which acts as a variational circuit. Hybrid deccoder models generally have a compression decoder which implies that the decoder can be GRU\/LSTM while the encoder can be any Bidirectional version of that. This allows a smooth compression of the tensors by concatenating the hidden and cell state channels.","73cdb7c7":"## Albert Transformer\n\n[Albert](https:\/\/arxiv.org\/abs\/1909.11942) is a lightweight bert which introduces parameter sharing, caching, and intermediate repeated splitting of the embedding matrix for efficient modelling tasks.\n\nAccording to the paper:\n\n'The first one is a factorized embedding parameterization. By decomposing the large vocabulary embedding matrix into two small matrices, we separate the size of the hidden layers from the size of vocabulary embedding. This separation makes it easier to grow the hidden size without significantly increasing the parameter size of the vocabulary embeddings. The second technique is cross-layer parameter sharing. This technique prevents the parameter from growing with the depth of the network. Both techniques significantly reduce the number of parameters for BERT without seriously hurting performance, thus improving parameter-efficiency. An ALBERT configuration similar to BERT-large has 18x fewer parameters and can be trained about 1.7x faster. The parameter reduction techniques also act as a form of regularization that stabilizes the training and helps with generalization. To further improve the performance of ALBERT, we also introduce a self-supervised loss for sentence-order prediction (SOP). SOP primary focuses on inter-sentence coherence and is designed to address the ineffectiveness (Yang et al., 2019; Liu et al., 2019) of the next sentence prediction (NSP) loss proposed in the original BERT.'\n\nResources:\n\n- [Github](https:\/\/github.com\/google-research\/albert)\n- [Huggingface](https:\/\/huggingface.co\/transformers\/model_doc\/albert.html)","c0e38237":"## Model Visualization\n\nThe model with its individual layers can be visualized as follows:\n\n<img src=\"https:\/\/i.imgur.com\/qI6zRqM.png\">\n\n\nWe can also look into the model parameters and the weights of the intermediate layers. We can visualize the sizes of the hidden layers and the output of each sequential layer in the model. Some resources:\n\n- [Keras](https:\/\/keras.io\/getting_started\/faq\/#how-can-i-obtain-the-output-of-an-intermediate-layer-feature-extraction)\n- [Stack Overflow](https:\/\/stackoverflow.com\/questions\/41711190\/keras-how-to-get-the-output-of-each-layer)\n- [Kite](https:\/\/www.kite.com\/python\/answers\/how-to-get-the-output-of-each-layer-of-a-keras-model-in-python#:~:text=A%20Keras%20model%20runs%20data,and%20applies%20the%20layer%20funtion.)\n","1f68a712":"## Encoder Decoder Model Architecture with Graves Attention\n\nThe architecture is as follows:\n\n<img src=\"https:\/\/i.imgur.com\/2AInGw0.png\">","785061f4":"## Conclusion of ELMO Embeddings\n\nWe used ELMO embeddings to see the performance of ELMO on our classification model. For the actual implementation of the ELMO (Peters etal), the resources are provided:\n\n- [Github](https:\/\/github.com\/allenai\/bilm-tf)\n- [Resources](https:\/\/paperswithcode.com\/paper\/deep-contextualized-word-representations)\n\n\n<img src=\"https:\/\/media3.giphy.com\/media\/3o7budMRwZvNGJ3pyE\/giphy.gif\">\n","d4c53fa8":"## Working With ELMO\n\n[ELMO](https:\/\/tfhub.dev\/google\/elmo\/3) is a deep contextual embedding model comprising of stacked dual lstm layers.Salient features of the model:\n\n- Computes contextualized word representations using character-based word representations and bidirectional LSTMs, as described in the paper \"Deep contextualized word representations\" [1].\n\n- This modules supports inputs both in the form of raw text strings or tokenized text strings.\n\n- The module outputs fixed embeddings at each LSTM layer, a learnable aggregation of the 3 layers, and a fixed mean-pooled vector representation of the input.\n\n- The complex architecture achieves state of the art results on several benchmarks.\n\nThe entire architectural model is as follows:\n\n\n\n<img src=\"http:\/\/jalammar.github.io\/images\/Bert-language-modeling.png\">\n\n\nThe deep contextual representations are retained with the help of bidirectional lstm layers, which uses look-ahead and look-back mechanisms combined to provide a correct representation of the context.\n\n\n<img src=\"http:\/\/jalammar.github.io\/images\/elmo-forward-backward-language-model-embedding.png\">\n\n\nSome resources:\n\n- [Paper](https:\/\/arxiv.org\/abs\/1802.05365)\n- [Kernel](https:\/\/www.kaggle.com\/sarthak221995\/textclassification-95-5-accuracy-elmo)\n- [Blog](https:\/\/towardsdatascience.com\/elmo-contextual-language-embedding-335de2268604)\n- [Blog](https:\/\/www.analyticsvidhya.com\/blog\/2019\/03\/learn-to-use-elmo-to-extract-features-from-text\/)\n- [Jay's Blog](http:\/\/jalammar.github.io\/illustrated-bert\/)\n","8c5f5f56":"## Model Architecture\n\nThe model architecture is shown here:\n\n<img src=\"https:\/\/i.imgur.com\/5FAd57t.png\">","cbf9c2af":"## Enter Transformers\n\n\n### Multi Head Self Scaled Dot Product Attention\n\n\nThis improves the performance of the attention layer in two ways:\n\n- It expands the model\u2019s ability to focus on different positions. Yes, in the example above, z1 contains a little bit of every other encoding, but it could be dominated by the the actual word itself. It would be useful if we\u2019re translating a sentence like \u201cThe animal didn\u2019t cross the street because it was too tired\u201d, we would want to know which word \u201cit\u201d refers to.\n\n- It gives the attention layer multiple \u201crepresentation subspaces\u201d. As we\u2019ll see next, with multi-headed attention we have not only one, but multiple sets of Query\/Key\/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder\/decoder). Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders\/decoders) into a different representation subspace.\n\n\n<img src=\"http:\/\/jalammar.github.io\/images\/t\/transformer_attention_heads_qkv.png\">\n\n\nThe entire modelling of multi head attention can be summed up in this image:\n\n\n<img src=\"http:\/\/jalammar.github.io\/images\/t\/transformer_multi-headed_self-attention-recap.png\">\n\n\n### Combining Multi HeadAttention with Encoder Decoders With Layer Normalization\n\n\nIn the classic encoder-decoder model, we will add the Multi head attention mechanism along with some modifications. The Encoder  contains a self attention head along with a Addition and Layer Normalization Layer. [Layer Normalization](https:\/\/arxiv.org\/pdf\/1607.06450.pdf) tries to apply normalization (mean and variance)on the cumulative hidden units present in a particular layer rather than minibatch sampling (as in the case of batch normalization). The \"covariate shift\" issue can be resolved to an extent using this normalization technique.\n\n<img src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcTJyynaajlr9lye4s0p28jOsE2VLZQ1R3l9rw&usqp=CAU\">\n\n\n#### Encoder\n\n\nThe encoder model with this modification appears as follows:\n\n\n<img src=\"http:\/\/jalammar.github.io\/images\/t\/transformer_resideual_layer_norm_2.png\">\n\n \nThe classic transformer consists of 8 stacked encoder decoder units with self attention(multihead) and layer normalization ,along with FFNN Dense Network inside each of them.This leads to a more robust architecture as compared to standard Encoder Decoder models.\n\n\n<img src=\"http:\/\/jalammar.github.io\/images\/t\/transformer_resideual_layer_norm_3.png\">\n\n\n#### Decoder\n\n\nNow that we\u2019ve covered most of the concepts on the encoder side, we basically know how the components of decoders work as well. But let\u2019s take a look at how they work together.\n\nThe encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its \u201cencoder-decoder attention\u201d layer which helps the decoder focus on appropriate places in the input sequence:\n\n\n<img src=\"http:\/\/jalammar.github.io\/images\/t\/transformer_decoding_1.gif\">\n\n\nThe following steps repeat the process until a special symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.\n\n\n<img src=\"http:\/\/jalammar.github.io\/images\/t\/transformer_decoding_2.gif\">\n\n\n#### Masking\n\n\nThe self attention layers in the decoder operate in a slightly different way than the one in the encoder:\n\nIn the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation.\n\nThe \u201cEncoder-Decoder Attention\u201d layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.\n\n\n### Final Linear and Softmax Activation\n\n\nThis is similar to the softmax activated output of the final FFNN layer.The decoder stack outputs a vector of floats. How do we turn that into a word? That\u2019s the job of the final Linear layer which is followed by a Softmax Layer.\n\nThe Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.\n\nLet\u2019s assume that our model knows 10,000 unique English words (our model\u2019s \u201coutput vocabulary\u201d) that it\u2019s learned from its training dataset. This would make the logits vector 10,000 cells wide \u2013 each cell corresponding to the score of a unique word. That is how we interpret the output of the model followed by the Linear layer.\n\nThe softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.\n\n\n<img src=\"http:\/\/jalammar.github.io\/images\/t\/transformer_decoder_output_softmax.png\">\n\n\nSome resources:\n\n- [Research](https:\/\/ai.googleblog.com\/2017\/08\/transformer-novel-neural-network.html)\n- [Repo](https:\/\/github.com\/tensorflow\/tensor2tensor)\n- [Jupyter](https:\/\/colab.research.google.com\/github\/tensorflow\/tensor2tensor\/blob\/master\/tensor2tensor\/notebooks\/hello_t2t.ipynb)\n- [Talk](https:\/\/www.youtube.com\/watch?v=rBCqOTEfxvg)","0e4c77d0":"## Model Visualization\n\nThe Simple LSTM Model with Glove Pretrained embeddings:\n\n<img src=\"https:\/\/i.imgur.com\/7FpjJVP.png\">","8eb842aa":"## Encoder Decoder Model Architecture with Scaled Dot Product Attention\n\nThe architecture is as follows:\n\n<img src=\"https:\/\/i.imgur.com\/pBnshbv.png\">","6ea5c443":"## Conclusion of Notebook\n\n\nThis terminates the notebook -2 for the workshop. The same codebase can be used  with any model from the huggingface repository. In the next [notebook](https:\/\/www.kaggle.com\/colearninglounge\/nlp-end-to-end-cll-nlp-workshop-3), we will breifly look into tensorboard graphs and training a simple model with tensorboard.\n\n\n\n<img src=\"https:\/\/i.pinimg.com\/originals\/1d\/cd\/04\/1dcd045c688cb9b8c85c79ab05834094.gif\">","8fa512a9":"## Transformer Workflow with TPU\n\nIn this case, we will be producing a robust workflow using Tensorflow TPU with Google Cloud Storage bucket data for training any Transformer models. The following are the steps:\n\n- Load the TPU cluster\n- Fast Encode the data with tokenizer from [Huggingface](https:\/\/github.com\/huggingface\/tokenizers).This is done by chunks of window sizes (batches)\n- We will use the Transformer Embeddings (which we created in [Notebook-1](https:\/\/www.kaggle.com\/colearninglounge\/nlp-end-to-end-cll-nlp-workshop))\n- For all transformer models of BERT variants,the standard is to abstract the last hidden layer of the outputs.\n- This particular layer contains the embedding vectors , generally of size (?,768) for BERT base and (?,1024) for BERT large variants.\n- Then the model uses a FFNN Dense Network with a sigmoid\/softmax activation to get the output weights.\n- Then we convert the input data (train data\/validation data) to a [Tensorflow Dataset](https:\/\/www.tensorflow.org\/guide\/data) which can leverage the power of TPU.\n- Then we use a distributed training pattern on TPU using [tf.strategy](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/distribute\/Strategy)\n\n\nIn this case,we are using Huggingface models. The first in this case,is using [DistilBERT model.](https:\/\/huggingface.co\/transformers\/model_doc\/distilbert.html)\n\n<img src=\"https:\/\/cdn.nextjournal.com\/data\/QmNQFSULXLPYnGhHSCxmeGk8oHjfdWnybmZGFztfS26fgZ?filename=2019-05-26%2023-43-43%20%E7%9A%84%E8%9E%A2%E5%B9%95%E6%93%B7%E5%9C%96.png&content-type=image\/png\">\n\n\n## BERT\n\n[BERT](https:\/\/arxiv.org\/abs\/1810.04805) is [bidirectional encoder Transformer model](https:\/\/github.com\/google-research\/bert)\n\n<img src=\"http:\/\/jalammar.github.io\/images\/distilBERT\/bert-output-tensor.png\">\n\n\nThe entire workflow can be designed as follows:\n\nThis image can be used to describe the workflow:\n\n\n<img src=\"http:\/\/jalammar.github.io\/images\/distilBERT\/bert-input-to-output-tensor-recap.png\">\n\nSlicing the important part For sentence classification, we\u2019re only only interested in BERT\u2019s output for the [CLS] token, so we select that slice of the cube and discard everything else.\n\n<img src=\"http:\/\/jalammar.github.io\/images\/distilBERT\/bert-output-tensor-selection.png\">\n\nBERT Model\n\n<img src=\"https:\/\/miro.medium.com\/max\/740\/1*G6PYuBxc7ryP4Pz7nrZJgQ@2x.png\">\n\n## DistilBERT Model\n\nThe distilbert performs better than Bert in most cases owing to continuous feedback of attention weights from the teacher to the student network. Where the weights change by a large extent in case of Bert, this fails to happen in DistilBert.\n\n<img src=\"https:\/\/storage.googleapis.com\/groundai-web-prod\/media%2Fusers%2Fuser_14%2Fproject_391208%2Fimages%2FKD_figures%2Ftransformer_distillation.png\">\n\n\nDistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT\u2019s performances as measured on the GLUE language understanding benchmark. 2 significant benchmarks aspects of this Model:\n\n- Quantization :This leads to approximation of internal weight vectors to a numerically smaller precision\n- Weights Pruning: Removing some connections from the network.\n\nKnowledge distillation (sometimes also referred to as teacher-student learning) is a compression technique in which a small model is trained to reproduce the behavior of a larger model (or an ensemble of models). It was introduced by Bucila et al. and generalized by Hinton et al. a few years later. We will follow the latter method.Rather than training with a cross-entropy over the hard targets (one-hot encoding of the gold class), we transfer the knowledge from the teacher to the student with a cross-entropy over the soft targets (probabilities of the teacher). Our training loss thus becomes:\n\n<img src=\"https:\/\/miro.medium.com\/max\/311\/1*GZkQPjKC_Wqx1F4Uu3FdiQ.png\">\n\nThis loss is a richer training signal since a single example enforces much more constraint than a single hard target. To further expose the mass of the distribution over the classes, Hinton et al. introduce a softmax-temperature:\n\n<img src=\"https:\/\/miro.medium.com\/max\/291\/1*BaVyKMXRWaudFvcI9So8MQ.png\">\n\nWhen T \u2192 0, the distribution becomes a Kronecker (and is equivalent to the one-hot target vector), when T \u2192+\u221e, it becomes a uniform distribution. The same temperature parameter is applied both to the student and the teacher at training time, further revealing more signals for each training example. At inference, T is set to 1 and recover the standard Softmax.\n\nSome resources:\n\n- [Blog](https:\/\/medium.com\/huggingface\/distilbert-8cf3380435b5)\n- [Huggingface](https:\/\/huggingface.co\/transformers\/model_doc\/distilbert.html)\n- [Paper](https:\/\/arxiv.org\/abs\/1910.01108)\n","21ff0038":"## Some Issues with ELMO\n\n\nELMO appears to be working well with TF 1.15 (rather any  TF version <2.0.0) . For using ELMO from Tensorflow [Hub](https:\/\/tfhub.dev\/google\/elmo\/3), we have to follow the steps:\n\n- Restart the Kernel\n- Run the cell containing:\n  ```python\n   !pip install -U tensorflow==1.15\n  ```\n- Check if the older version of tensorflow is installed (the session will automatically get restarted).\n  ```python\n   import tensorflow as tf\n   tf.__version__\n  ```\n- Make sure the ELMO embeddings are loaded and can be used by clicking on the example cell below this markdown.\n- Create the ELMO embeddings and feed it into the classifier.\n\n\nSince ELMO has several benchmarks due to its bidirectionality (LSTMs), the only layers which provide a proper accuracy are Dense layers (when placed after the Embedding Layer).ELMO embeddings generally have a shape of (?,?,1024), and hence compressing these multidimensional embeddings to a Dense Layer (eg.256 units) takes a huge computation time. The program for the ELMO embedding classifier is written in tensorflow.","36532ce1":"## Bahdanau Attention\n\n\n\n<img src=\"https:\/\/miro.medium.com\/max\/639\/1*qhOlQHLdtfZORIXYuoCtaA.png\">\n\nBahdanau et al. proposed an attention mechanism that learns to align and translate jointly. It is also known as Additive attention as it performs a linear combination of encoder states and the decoder states.\n\nlet\u2019s understand the Attention mechanism suggested by Bahdanau\n\n- All hidden states of the encoder(forward and backward) and the decoder are used to generate the context vector, unlike how just the last encoder hidden state is used in seq2seq without attention.\n- The attention mechanism aligns the input and output sequences, with an alignment score parameterized by a feed-forward network. It helps to pay attention to the most relevant information in the source sequence.\n- The model predicts a target word based on the context vectors associated with the source position and the previously generated target words.\n\n\n### Alignment Score\n\nThe alignment score maps how well the inputs around position \u201cj\u201d and the output at position \u201ci\u201d match. The score is based on the previous decoder\u2019s hidden state, s\u208d\u1d62\u208b\u2081\u208e just before predicting the target word and the hidden state, h\u2c7c of the input sentence.\n\n\n<img src=\"https:\/\/miro.medium.com\/max\/535\/1*u2YdTRPjN34Fpr-zxvoJsg.png\">\n\n\nThe decoder decides which part of the source sentence it needs to pay attention to, instead of having encoder encode all the information of the source sentence into a fixed-length vector.\nThe alignment vector that has the same length with the source sequence and is computed at every time step of the decode.\n\n\n### Attention Weights\n\n\nWe apply a softmax activation function to the alignment scores to obtain the attention weights.\n\n<img src=\"https:\/\/miro.medium.com\/max\/685\/1*3aCyU9aSVHvxzOwvQdExdQ.png\">\n \n\nSome resources:\n\n\n- [Paper](https:\/\/arxiv.org\/abs\/1409.0473)\n- [Lilian's Blog](https:\/\/lilianweng.github.io\/lil-log\/2018\/06\/24\/attention-attention.html#:~:text=Self%2Dattention%2C%20also%20known%20as,summarization%2C%20or%20image%20description%20generation.)\n- [Nice Blog](https:\/\/towardsdatascience.com\/sequence-2-sequence-model-with-attention-mechanism-9e9ca2a613a)\n- [Blog](https:\/\/medium.com\/analytics-vidhya\/neural-machine-translation-using-bahdanau-attention-mechanism-d496c9be30c3)"}}