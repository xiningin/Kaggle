{"cell_type":{"bc550b40":"code","7d32d227":"code","6d5ca8dc":"code","c93a485c":"code","30780a0d":"code","b86f1719":"code","ccbe429d":"code","b43654aa":"code","3d047c9e":"code","c5b5c275":"code","4386ce13":"code","a24e58b0":"code","fda50618":"code","e44292c5":"code","bab6cc0d":"code","541cea8e":"code","5941d364":"code","209f1148":"markdown","010bc4cb":"markdown","2dbc4e78":"markdown","730a6c12":"markdown","28207501":"markdown","3c786c15":"markdown","d7d79e85":"markdown","a052a147":"markdown","b61060ce":"markdown","f493bc4a":"markdown","ed5cae28":"markdown","fcb6cd6b":"markdown","e6e2a81d":"markdown","a8a04abe":"markdown","536838a1":"markdown","2210586d":"markdown","de897e82":"markdown"},"source":{"bc550b40":"# upgrade transformers and datasets to latest versions\n!pip install --upgrade transformers\n!pip install --upgrade datasets\n\nimport transformers\nimport datasets\nprint(transformers.__version__)\nprint(datasets.__version__)","7d32d227":"# Make necessary imports\n\n# for array operations \nimport numpy as np \n# TensorFlow framework\nimport tensorflow as tf\n# for modeling and training\nfrom tensorflow import keras\n# for pretty printing\nfrom pprint import pprint\n# plotting\nfrom matplotlib import pyplot as plt\n\n# HuggingFace ecosystem\nfrom transformers import AutoTokenizer, \\\n                        DataCollatorWithPadding, \\\n                        TFAutoModelForSequenceClassification\nfrom datasets import load_dataset\n\n# a seed for reproducibility\nSEED = 42\n# set seed\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n\n# check for GPU device\nprint('Number of CPU Devices: ', len(tf.config.list_physical_devices('CPU')))\nprint('Number of GPU Devices: ', len(tf.config.list_physical_devices('GPU')))","6d5ca8dc":"raw_data = load_dataset(\"glue\", \"sst2\")\n# how does it look like?\nraw_data","c93a485c":"# Sample a data\nraw_data[\"train\"][0]","30780a0d":"raw_data[\"train\"].features","b86f1719":"checkpoint = 'bert-base-uncased'\n# bert tokenizer\ntokenizer = AutoTokenizer.from_pretrained(checkpoint, return_tensors='tf')\n# data collator for dynamic padding as per batch\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors='tf')","ccbe429d":"# define a tokenize function\ndef Tokenize_function(example):\n    return tokenizer(example['sentence'], truncation=True)","b43654aa":"# tokenize entire data\ntokenized_data = raw_data.map(Tokenize_function, batched=True)","3d047c9e":"tokenized_data","c5b5c275":"# set format\ntokenized_data[\"train\"].set_format(type='tf', columns=['attention_mask', 'input_ids', 'token_type_ids', 'label'])\n# rename label as labels, as expected by tf models\ntokenized_data[\"train\"].rename_column_('label', 'labels')\n# set format\ntokenized_data[\"validation\"].set_format(type='tf', columns=['attention_mask', 'input_ids', 'token_type_ids', 'label'])\n# make the renaming\ntokenized_data[\"validation\"].rename_column_('label', 'labels')\n# inspect it\ntokenized_data[\"train\"].format","4386ce13":"# convert to TF dataset\n# train set\ntrain_data = tokenized_data[\"train\"].to_tf_dataset( \n    columns = ['attention_mask', 'input_ids', 'token_type_ids'], \n    label_cols = ['labels'], \n    shuffle = True, \n    collate_fn = data_collator, \n    batch_size = 8 \n)\n# validation set\nval_data = tokenized_data[\"validation\"].to_tf_dataset(\n    columns = ['attention_mask', 'input_ids', 'token_type_ids'],\n    label_cols = ['labels'],\n    shuffle = False,\n    collate_fn = data_collator,\n    batch_size = 8\n)\n# check whether it is a TF's PrefetchDataset\ntrain_data","a24e58b0":"EPOCHS = 3\nTRAINING_STEPS = len(train_data) * EPOCHS\n\n# define a learning rate scheduler\nlr_scheduler = keras.optimizers.schedules.PolynomialDecay(\n    initial_learning_rate=5e-5, \n    end_learning_rate=0.0,\n    decay_steps=TRAINING_STEPS\n)\n\n# define an Adam optimizer\noptimizer = keras.optimizers.Adam(learning_rate=lr_scheduler)\n\n# define loss function\nloss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\n# build model\n\nmodel = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n\n# compile model\nmodel.compile(\n    optimizer=optimizer,\n    loss=loss_fn,\n    metrics=['accuracy']\n)\n\n# What layers, parameters are there in the model\nmodel.summary()","fda50618":"# Train the model\nhist = model.fit(\n    train_data,\n    validation_data = val_data,\n    epochs = EPOCHS\n)","e44292c5":"# plotting accuracy\nhistory = hist.history\nplt.plot(range(1,4), history[\"accuracy\"], label='train')\nplt.plot(range(1,4), history['val_accuracy'], label='val')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Accuracy')\nplt.xticks([1,2,3])\nplt.legend()\nplt.show()","bab6cc0d":"# plotting losses\nhistory = hist.history\nplt.plot(range(1,4), history[\"loss\"], label='train')\nplt.plot(range(1,4), history['val_loss'], label='val')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.xticks([1,2,3])\nplt.title('CrossEntropy Loss')\nplt.legend()\nplt.show()","541cea8e":"# set format\ntokenized_data['test'].set_format(type='tf', \n                                 columns=['input_ids','token_type_ids','attention_mask','label']\n                                 )\n# rename label as labels\ntokenized_data['test'].rename_column_('label','labels')\n# convert to TF Dataset\ntest_data = tokenized_data[\"test\"].to_tf_dataset(\n    columns=['input_ids','token_type_ids','attention_mask'],\n    label_cols=['labels'],\n    shuffle=False,\n    collate_fn=data_collator,\n    batch_size=8\n)","5941d364":"preds = model.predict(test_data)[\"logits\"]\nlabel_preds = np.argmax(preds, axis=1)\nlabel_preds.shape ","209f1148":"# Effortless NLP using HuggingFace's Tranformers Ecosystem","010bc4cb":"How does tokenized data look like?","2dbc4e78":"Data is ready now for efficient data loading and faster training.","730a6c12":"Load the SST-2 Dataset from GLUE benchmark","28207501":"Let's convert the dataset from HF Dataset format to TensorFlow Dataset for convenience.","3c786c15":"We understand that this dataset consists of the supervised task - *Sentiment Analysis* with 2 classes: negative [0] and positive [1] ","d7d79e85":"![Image](https:\/\/raw.githubusercontent.com\/RajkumarGalaxy\/dataset\/master\/Images\/z0001.jpg)\n\n> Image by [Author](https:\/\/raw.githubusercontent.com\/RajkumarGalaxy\/dataset\/master\/Images\/z0001.jpg)\n### How to finetune a model on a custom dataset?","a052a147":"### Thank you for your valuable time!","b61060ce":"# Model Fine-tuning","f493bc4a":"#### ------------------------------------------------ \n#### *Articles So Far In This Series*\n#### -> [[NLP Tutorial] Finish Tasks in Two Lines of Code](https:\/\/www.kaggle.com\/rajkumarl\/nlp-tutorial-finish-tasks-in-two-lines-of-code)\n#### -> [[NLP Tutorial] Unwrapping Transformers Pipeline](https:\/\/www.kaggle.com\/rajkumarl\/nlp-unwrapping-transformers-pipeline)\n#### -> [[NLP Tutorial] Exploring Tokenizers](https:\/\/www.kaggle.com\/rajkumarl\/nlp-tutorial-exploring-tokenizers)\n#### -> [[NLP Tutorial] Fine-Tuning in TensorFlow](https:\/\/www.kaggle.com\/rajkumarl\/nlp-tutorial-fine-tuning-in-tensorflow) \n#### -> [[NLP Tutorail] Fine-Tuning in Pytorch](https:\/\/www.kaggle.com\/rajkumarl\/nlp-tutorial-fine-tuning-in-pytorch) \n#### -> [[NLP Tutorail] Fine-Tuning with Trainer API](https:\/\/www.kaggle.com\/rajkumarl\/nlp-tutorial-fine-tuning-with-trainer-api) \n#### ------------------------------------------------ ","ed5cae28":"# Tokenizer and Data Collator\n\nWe are about to use a pre-trained Bert_base_uncased model for our fine-tuning. A tokenizer function associated with a data collator can ensure efficient memory usage and quick data handling during training.","fcb6cd6b":"# Prepare Environment and Data\n\nIn this article we discuss fine-tuning a BERT model on the famous SST2 dataset using TensorFlow and Keras. This requires a GPU environment for faster training and inference.","e6e2a81d":"# Performance Analysis\n\nPlots are always great in providing insights. Let's have a look at the training curve by plotting accuracies and losses.","a8a04abe":"### That's the end. We got a good understanding of fine-tuning a BERT model on a sentiment analysis dataset in TensorFlow!\n\n##### Key reference: [HuggingFace's NLP Course](https:\/\/huggingface.co\/course)","536838a1":"Each data point contains a sentence, its index and its label. What labels are there? What are their positions?","2210586d":"`attention_mask`, `input_ids`, `token_type_ids` are the necessary input features and `label` is the target. Other features are useless in the view of modeling.","de897e82":"# Prediction \n\nPredict the labels for the test data. Prepare the test set as TF dataset object."}}