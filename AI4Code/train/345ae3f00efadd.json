{"cell_type":{"0a38f11a":"code","456ac1c4":"code","bd9e69e8":"code","9a100f3f":"code","9cdce585":"code","54f7896d":"code","86a71247":"code","89c111e5":"code","8859d4ad":"code","965040a0":"code","b944fe40":"code","22517b1f":"markdown","e41f85a3":"markdown","ec512caa":"markdown","43ab5bcb":"markdown"},"source":{"0a38f11a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gensim\nimport keras\nimport nltk.corpus\n","456ac1c4":"if '\/usr\/share\/nltk_data' in nltk.data.path:\n    nltk.data.path.remove('\/usr\/share\/nltk_data')\nnltk.data.path.append('..\/input\/')\nnltk.data.path","bd9e69e8":"class BrownCorpus(object):\n    def __init__(self):\n        self.brown =  nltk.corpus.LazyCorpusLoader('brown', nltk.corpus.CategorizedTaggedCorpusReader, r'c[a-z]\\d\\d',\n                                                    cat_file='cats.csv', tagset='brown', encoding=\"ascii\",\n                                                    nltk_data_subdir='brown-corpus\/brown')\n    def __iter__(self):\n        for (tag,doc) in enumerate(self.brown.paras()):\n            yield gensim.models.doc2vec.TaggedDocument(sum(doc,[]),[tag])\n            \nmodel = gensim.models.doc2vec.Doc2Vec(BrownCorpus(),\n                                      dm_concat=1)","9a100f3f":"model.vector_size","9cdce585":"def vectorize_cell(cell):\n    return model.infer_vector(list(gensim.utils.tokenize(cell)))\n\ndef vectorize(data):\n    return np.vstack([np.concatenate([vectorize_cell(row[cell])\n                                      for cell in ('question_title','question_body','answer')])\n                     for (i,row) in data.iterrows()])","54f7896d":"training_data = pd.read_csv('..\/input\/google-quest-challenge\/train.csv',\n                           index_col='qa_id')\ntraining_data","86a71247":"training_vectors = vectorize(training_data)\ntraining_vectors","89c111e5":"predictor = keras.models.Sequential([keras.layers.Dense(100,\n                                                       input_shape=(300,),\n                                                       activation='softplus'),\n                                     keras.layers.Dense(100,\n                                                       activation='softplus'),\n                                     keras.layers.Dense(30,\n                                                       activation='sigmoid')])\n\npredictor.compile(optimizer='nadam',\n                  loss='mean_squared_error')\n","8859d4ad":"columns_to_predict = pd.read_csv('..\/input\/google-quest-challenge\/sample_submission.csv',\n                                 index_col='qa_id').columns\npredictor.fit(training_vectors,\n              training_data.loc[:,columns_to_predict].values,\n              epochs=100)","965040a0":"test_data = pd.read_csv('..\/input\/google-quest-challenge\/test.csv',\n                        index_col='qa_id')\ntest_vectors = vectorize(test_data)\nresults = pd.DataFrame(predictor.predict(test_vectors),\n                       index = test_data.index,\n                       columns = columns_to_predict)\nresults","b944fe40":"results.to_csv('submission.csv')","22517b1f":"Now for a predictive model. I'll use a neural net with two dense hidden layers, each with 100 nodes and a softplus activation function. The output layer will have 30 nodes and a sigmoid activation function. Since I'm predicting continuous values, I'll use a Euclidean loss function.","e41f85a3":"Now let's try making some predictions","ec512caa":"I will use the Brown Corpus to train a Doc2Vec model.","43ab5bcb":"For each question\/answer pair, I will infer a vector for each of question_title, question_body, and answer, and concatenate them."}}