{"cell_type":{"4cb78c55":"code","fc7ec28f":"code","56233787":"code","5155e845":"code","0599a004":"code","732803d6":"code","4e9aaa8e":"code","15668c6f":"code","2335802a":"code","d8e08271":"code","bf725e67":"code","d5330d16":"code","3207e342":"code","1806caa7":"code","d25cc49e":"code","cadecd0d":"code","5532ea6d":"code","cda37c65":"code","bf58612b":"code","e3cc1561":"code","5b4e6b20":"code","89eb78d4":"code","6b65f402":"code","9cbd22e9":"code","0a3145f2":"code","2007164e":"code","e640c8ac":"markdown","a80c52cf":"markdown","57079489":"markdown","dcc35ef4":"markdown","be6daa98":"markdown","11dc5258":"markdown","014f4874":"markdown","b4b79fec":"markdown","3b03d0c8":"markdown","a8261fe1":"markdown","a4fd73a0":"markdown","92b5ecbe":"markdown","ebc09c2d":"markdown","b1287d91":"markdown","175e49a1":"markdown","fef50ad8":"markdown","33c90e8b":"markdown","d314ba81":"markdown","ebca33ac":"markdown","96dd86bb":"markdown","cb2bbb00":"markdown","1e5c9884":"markdown","1e006881":"markdown","7b7f5616":"markdown","52e835c7":"markdown","4870bcef":"markdown","a67d4316":"markdown","2388eafc":"markdown","a3330e7b":"markdown","726b9625":"markdown","6ec247d1":"markdown","b6c4ef9e":"markdown","b2ee92c6":"markdown","7cf0916e":"markdown","648343bd":"markdown","1338099f":"markdown","a7bfe5f9":"markdown","c8877172":"markdown","465a7ff7":"markdown"},"source":{"4cb78c55":"import numpy as np\nimport pandas as pd\nimport nltk, pickle\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.utils import Bunch\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay","fc7ec28f":"#bow example without stopwords\n\ntext = [\"hello my name is jonathan because jonathan is my name.\", \n        \"jonathan is his name because his name is jonathan.\",\n        \"is his name jonathan or is it hugo?\"]\n\ncnt_vect = CountVectorizer()\ncnt_matrix = cnt_vect.fit_transform(text)\ndf = pd.DataFrame(data=cnt_matrix.toarray(), columns=cnt_vect.get_feature_names())\ndf","56233787":"my_dict = [{'foo': 1, 'bar': 2, 'nar': 6}, \n           {'foo': 3, 'bar': 1, 'car': 4},\n           {'coo': 4, 'bar': 3, 'tar': 7}]\n\ndict_vect = DictVectorizer(sparse=False)\nmatrix = dict_vect.fit_transform(my_dict)\n\n#convert to dataframe and display\ndf = pd.DataFrame(matrix, index=['dict_1', 'dict_2', 'dict_3'])\ndf","5155e845":"def extract_features(token):    \n    \n    return {'token': token,\n            'is-title': token.istitle(),\n            'is-caps': token.isupper(),\n            'is-lower': token.islower()\n           }","0599a004":"stop = nltk.corpus.stopwords.words('english')\nsent = \"Prime Minister Boris Johnson will fly to Egypt on April 26th to meet the Israeli Prime Minister.\"\ntok_sent = [w for w in nltk.word_tokenize(sent) if w.isalnum() and w not in stop]\n\n#build and vectorize the feature set\nsent_feat = [extract_features(token) for token in tok_sent]\ndict_vect = DictVectorizer(sparse=False)\nsent_matrix = dict_vect.fit_transform(sent_feat)\n\n#convert to dataframe and display\ndf = pd.DataFrame(sent_matrix, index=tok_sent)\ndf","732803d6":"dict_vect = DictVectorizer(sparse=True)\nsent_matrix = dict_vect.fit_transform(sent_feat)\n#return values for 'Prime' are as follows\n#(0, 0) \t0.0 for is_caps (encoded as '0') because 'Prime' is not uppercase\n#(0, 1) \t0.0 for is_lower (encoded as '1') because 'Prime' is not lowercase\n#(0, 2) \t1.0 for is_title (encoded as '2') because 'Prime' is titlecase\n#(0, 10)\t1.0 for token (10 is the unique index number for 'Prime')\nprint(sent_matrix[0])","4e9aaa8e":"with open('..\/input\/movie-reviews-dataset\/movie_reviews_dataset.pkl', mode='rb') as file:\n    data = pickle.load(file)","15668c6f":"data.data[0]","2335802a":"tok_review = lambda x: nltk.word_tokenize(x)\n\n#max_df = ignore terms that have a document frequency higher than this threshold \n#min_df = ignore terms that have a document frequency lower than this threshold.\n#float in range [0.0, 1.0] represents a proportion of documents, integer absolute counts.\n\ntfid_vect = TfidfVectorizer(min_df=20, \n                            max_df=0.95,\n                            max_features=3000,\n                            stop_words='english', \n                            tokenizer=tok_review).fit_transform(data.data)","d8e08271":"#max_df = ignore terms that have a document frequency higher than this threshold \n#min_df = ignore terms that have a document frequency lower than this threshold.\n#float in range [0.0, 1.0] represents a proportion of documents, integer absolute counts.\n\ntfid_vect = TfidfVectorizer(min_df=20,\n                            max_df=0.95,\n                            max_features=3000, \n                            stop_words='english', \n                            token_pattern=r\"\\w+(?:[-']\\w+)*|'|[-.(]+|\\S\\w*\").fit_transform(data.data)","bf725e67":"tfid_vect.shape","d5330d16":"#load the sentiment dictionary\nwith open('..\/input\/sentiment-dictionary\/sub_clues_dict.pkl', mode='rb') as file:\n    sent_dict = pickle.load(file)\n\n#keys are used by document_features()\nsent_keys = set(sent_dict.keys())\n#display an example entry from sent_dict\nsent_dict['awful']","3207e342":"def document_features(rev):\n    features = {}\n    tokens = set(w.lower() for w in rev if w.isalnum() and w not in stop)\n    for token in tokens:\n        if token in sent_keys:\n            sent_entry = sent_dict[token]\n            features['sent_weight ({})'.format(token)] = sent_entry[0]\n            features['sent_type ({})'.format(token)] = sent_entry[1]\n        else:\n            features['sent_weight ({})'.format(token)] = 'N\/A'\n            features['sent_type ({})'.format(token)] = 'N\/A'\n    return features","1806caa7":"stop = nltk.corpus.stopwords.words('english')\ntest_sent = nltk.word_tokenize(\"it was a great movie for those who enjoy bad movies about great movies.\")\ndocument_features(test_sent)","d25cc49e":"stop = nltk.corpus.stopwords.words('english')\n#tokenize each review\ntok_revs = [nltk.word_tokenize(rev) for rev in data.data]\n#build the feature set\nrev_features = [document_features(rev) for rev in tok_revs]\n#vectorize the feature set\ndict_vect = DictVectorizer(sparse=True)\nrev_matrix = dict_vect.fit_transform(rev_features)\nrev_matrix.shape","cadecd0d":"new_data = Bunch(data=rev_matrix, \n                 target=data.target, \n                 target_names=data.target_names, \n                 filenames=data.filenames)","5532ea6d":"x_train, x_test, y_train, y_test, = train_test_split(new_data.data,\n                                                     new_data.target,\n                                                     test_size = 0.20,\n                                                     random_state=42)","cda37c65":"def show_confusion_matrix():\n    disp_matrix = ConfusionMatrixDisplay(confusion_matrix=con_matrix, display_labels=new_data.target_names)\n    disp_matrix.plot(values_format = '')\n    plt.show()","bf58612b":"clf = MultinomialNB().fit(x_train, y_train)\npredicted = clf.predict(x_test)\ncon_matrix = confusion_matrix(y_test, predicted)\nprint(metrics.classification_report(y_test, predicted, target_names=new_data.target_names))","e3cc1561":"show_confusion_matrix()","5b4e6b20":"clf = RandomForestClassifier(n_estimators=200, \n                             criterion='entropy', \n                             max_depth=50, \n                             random_state=42).fit(x_train, y_train)\n\npredicted = clf.predict(x_test)\ncon_matrix = confusion_matrix(y_test, predicted)\nprint(metrics.classification_report(y_test, predicted, target_names=new_data.target_names))","89eb78d4":"show_confusion_matrix()","6b65f402":"clf = SVC(random_state=42).fit(x_train, y_train)\npredicted = clf.predict(x_test)\ncon_matrix = confusion_matrix(y_test, predicted)\nprint(metrics.classification_report(y_test, predicted, target_names=new_data.target_names))","9cbd22e9":"show_confusion_matrix()","0a3145f2":"clf = SGDClassifier(loss='log', alpha=0.1, random_state=42).fit(x_train, y_train)\npredicted = clf.predict(x_test)\ncon_matrix = confusion_matrix(y_test, predicted)\nprint(metrics.classification_report(y_test, predicted, target_names=new_data.target_names))","2007164e":"show_confusion_matrix()","e640c8ac":"<a id='svccm'><\/a>\n**SVC Confusion matrix**","a80c52cf":"*CountVectorizer()* reduces text to a 'bag of words' by assigning an index number to each unique word in the input text and returning a sparse matrix. The matrix contains all the unique words in all the sentences. In the below example, the three input sentences (my 'corpus' so to speak) are converted to three arrays, each containing frequency counts for all the unique tokens in all the sentences. Tokens that appear in the corpus but not in a specific sentence receive a zero count.","57079489":"Finally, I'm going to create a function for displaying a confusion matrix for each estimator.","dcc35ef4":"<a id='nofs'><\/a>\n### **Vectorizing without a featureset**","be6daa98":"<a id='wfs'><\/a>\n### **Vectorizing with a featureset**","11dc5258":"Each review is stored as a lowercase string rather than a tokenized list or list of tokenized sentences.","014f4874":"In the below example 'my_dict' contains three dictionaries with three key:value pairs and six unique keys. \nThe three arrays created by *DictVectorizer()* each contain six elements, one for each unique key. In each case, the absent features (i.e. key:value pair) in each array are represented by a zero. Note that array entries are sorted alphabetically by key.","b4b79fec":"The feature extraction function *document_features()* returns a dictionary indicating whether a word is positive or negative and, if so, whether it is weakly or strongly sentimental. Words that don't appear in the sentiment dictionary are assigned the value 'N\/A'.","3b03d0c8":"<a id='cvex'><\/a>\n## **CountVectorizer Example**","a8261fe1":"Now that's much easier to digest!","a4fd73a0":"<a id='vmr'><\/a>\n## **Vectorizing Movie Reviews**","92b5ecbe":"<a id='rf'><\/a>\n### **Random Forest Estimator**","ebc09c2d":"<a id='lr'><\/a>\n### **Logistic Regressor Estimator**","b1287d91":"<a id='nb'><\/a>\n### **Naive Bayes Estimator**","175e49a1":"<a id='lrcm'><\/a>\n**Logistic Regressor ConfusionMatrix**","fef50ad8":"All I need to do now is tokenize a sentence, remove stopwords, call *extract_features()* on each token, and vectorize the ouput. In this example I've set sparse=False to include zero value entries for unrelated features. Note that for the twelve tokens in the sentence thirteen separate features exist.","33c90e8b":"I'm going to test the feature extractor and view the type of output it'll generate for each review.","d314ba81":"Even if I didn't want to build a featureset I'd still need to tokenize the raw text before creating a bag of words. In the below example I'm going to vectorize the top 3000 words from the corpus. To tokenize the text I can use NLTK's native tokenizer in conjunction with *CountVectorizer()* or *TfidVectorizer()*...","ebca33ac":"Scikit's *CountVectorizer()* or *TfidVectorizer()* can be used to vectorize a document\/corpus (or a subset of it), whereas *DictVectorizer()* allows those more accustomed to the NLTK approach to vectorize a featureset in the form of a list of dictionaries.\n\nAll three vectorizers accept a boolean 'sparse' parameter to determine whether the output is a sparse matrix. Be aware that some classifiers (i.e. the Gaussian Naive Bayes) will *only* accept a sparse matrix as input.\n\n*CountVectorizer()* and *TfidVectorizer()* provide methods that accept a function for preprocessing text, whereas *DictVectorizer()* does not.\n\nIn terms of results, the best accuracy score (87% for the SVC and Logistic Regressor estimators) is significantly higher than what I've achieved with NLTK's native NaiveBayes and Decision Tree classifiers.","96dd86bb":"## **Conclusion**","cb2bbb00":"...and use train_test_split() to split it into train and test sets.","1e5c9884":"The movie reviews corpus contains 2000 documents with an equal split between 'positive' and 'negative'. It was originally sourced from an archive of the rec.arts.movies.reviews newsgroup hosted at IMDB. It appears to be identical to the NLTK movie reviews corpus, except that reviews are returned as a list of strings rather than a list of tokenized lists.\n\nOrdinarily I'd load the data with *load_files()*, which creates a bunch object with the following attributes:\n\n* objName.data contains the actual data\n* objName.target contains vectorized classification labels\n* objName.target_names contains the actual classification labels\n* objName.filenames contains the filenames of the individual files in the data set\n\nHowever, the upload limit on Kaggle is 1000 files, so it's easier to load a pickled version of the bunch object created on my local machine.","1e006881":"<a id='rfcm'><\/a>\n**Random Forest Confusion Matrix**","7b7f5616":"The .shape attribute confirms that we have 2000 arrays (one for each movie review) with 76828 unique features.","52e835c7":"...or exclude both parameters, in which case a default regex pattern (r\"(?u)\\b\\w\\w+\\b\") will be used by the vectorizer. \n\nNote that the shape of the matrix is (2000, 3000), i.e. each individual array in the matrix contains 3000 features, one for each token.","4870bcef":"Scikit-learn's DictVectorizer() does not have the build_analyser() and build_preprocessor() methods available to CountVectorizer() and TfidVectorizer(). That means I can't pass the feature extraction function to the vectorizer, so I need to build my featureset first.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html<br>\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html\n\nIn this example I'm going to build a feature set based on the presence or absence of sentiment words. I'll start by loading a dictionary that contains over 6000 sentiment words. Each word is rated positive or negative and assigned a strength.","a67d4316":"The same principle applies when processing large corpora consisting of hundreds or thousands of documents. Each individual array contains an entry for each unique word within the corpus. The length of these arrays is often in the tens of thousands, the vast majority of array elements being zeros, i.e. because the unique words used in a specific document are usually a small subset of all the unique words in the corpus. Hence the term 'sparse matrix'!","2388eafc":"<a id='nbcm'><\/a>\n**Naive Bayes Confusion Matrix**","a3330e7b":"<a id='dvex'><\/a>\n## **DictVectorizer Examples**","726b9625":"<a id='train'><\/a>\n## **Training the estimators**","6ec247d1":"<a id='svc'><\/a>\n### **SVC Estimator**","b6c4ef9e":"To aid the process of creating train and test sets I'm going to create a new dataset...","b2ee92c6":"**Why choose TfidVectorizer over CountVectorizer?**\n\n\"The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.\"\n\n*CountVectorizer()* returns frequency counts for all words, whereas *TfidVectorizer()* downscales weights for words that occur in many documents.\n\nSee https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfTransformer.html","7cf0916e":"In the below example I've set sparse=True to exclude zero value entries for unrelated features. Note that for each of the twelve tokens only the four features related to that token are included in the matrix. For large datasets  this reduces the amount of memory consumed.","648343bd":"...or define a custom regex pattern...","1338099f":"With the feature extractor function defined and tested I can extract and vectorize the features. As per the above example, the initial feature set returned by *document_features()* will be a list containing one dictionary per review.","a7bfe5f9":"The below example demonstrates how to use *DictVectorizer()* on a featureset extracted from a tokenized sentence. I'll start by defining a simple feature extractor function. It really ought to return the token and a single 'word-shape' feature, but will suffice for demonstration purposes.","c8877172":"<a id='imp'><\/a>\n## **Import Libraries**","465a7ff7":"In this notebook I'm going to demonstrate (mainly for my own benefit) how to use NLTK, *CountVectorizer()* and *TfidVectorizer()* to tokenize and vectorize text. I'm also going to demonstrate how to use NLTK in conjunction with *DictVectorizer()* to:\n\n* vectorize a featureset extracted from a movie reviews corpus\n* convert the vectorized reviews to a dataset\n* train Naives Bayes, Random Forest, SVC and Logistic Regressor estimators on the dataset\n\nBest results were returned by the SVC and Logistic Regressor estimators (87% accuracy).\n\n* [Import Libraries](#imp)\n* [CountVectorizer Example](#cvex)\n* [DictVectorizer Examples](#dvex)\n* [Vectorizing Movie Reviews](#vmr)\n    - [Without a featureset](#nofs)\n    - [With a featureset](#wfs)\n* [Training the Estimators](#train)\n    - [Naive Bayes](#nb)\n    - [Random Forest](#rf)\n    - [Support Vector (SVC)](#svc)\n    - [Logistics Regressor](#lr)\n* [Conclusion](#con)\n    \n"}}