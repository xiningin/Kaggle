{"cell_type":{"4e30a3ed":"code","f89455ba":"code","b8206665":"code","a5871505":"code","bdb0f2bd":"code","ccac819a":"code","58c0957d":"code","f405e037":"code","3ce93374":"code","d91c392b":"code","9317f1ce":"code","dfa8245b":"code","2f1c2128":"code","265e75d5":"code","80b11f08":"code","c7c74846":"code","fae62e73":"code","6ebfd562":"code","7106ac93":"code","17336619":"code","8d0c5e2a":"markdown","9d3b0588":"markdown","8e9637ba":"markdown","cb2d4c67":"markdown","3965b62a":"markdown","8421a275":"markdown","da99de36":"markdown","d857df41":"markdown","ad7efe82":"markdown","7dc56a23":"markdown","4d276049":"markdown","f083bc63":"markdown","07dbab16":"markdown","2cabaaf3":"markdown","5978a578":"markdown","850d83a3":"markdown"},"source":{"4e30a3ed":"import seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport json\nimport nltk\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nfrom wordcloud import WordCloud, STOPWORDS\nimport re","f89455ba":"gb_df = pd.read_csv('..\/input\/youtube-new\/GBvideos.csv')\ngb_df.head(3)","b8206665":"def CatJsonToDF(path):\n    df = pd.read_json(path)\n    df[\"category_id\"] = df[\"items\"].apply(lambda x: x[\"id\"])\n    df[\"Catagory Names\"] = df[\"items\"].apply(lambda x: x[\"snippet\"][\"title\"])\n    df[\"category_id\"] = pd.to_numeric(df[\"category_id\"])\n    df = df.drop([\"kind\", \"etag\", \"items\"], axis=1)\n    return df\n\nallCats = CatJsonToDF('..\/input\/youtube-new\/CA_category_id.json')\n\npd.concat([allCats, CatJsonToDF('..\/input\/youtube-new\/DE_category_id.json')])\npd.concat([allCats, CatJsonToDF('..\/input\/youtube-new\/FR_category_id.json')])\npd.concat([allCats, CatJsonToDF('..\/input\/youtube-new\/GB_category_id.json')])\npd.concat([allCats, CatJsonToDF('..\/input\/youtube-new\/IN_category_id.json')])\npd.concat([allCats, CatJsonToDF('..\/input\/youtube-new\/JP_category_id.json')])\npd.concat([allCats, CatJsonToDF('..\/input\/youtube-new\/KR_category_id.json')])\npd.concat([allCats, CatJsonToDF('..\/input\/youtube-new\/MX_category_id.json')])\npd.concat([allCats, CatJsonToDF('..\/input\/youtube-new\/RU_category_id.json')])\npd.concat([allCats, CatJsonToDF('..\/input\/youtube-new\/US_category_id.json')])\n\n\nCA = pd.read_csv('..\/input\/youtube-new\/CAvideos.csv')\nDE = pd.read_csv('..\/input\/youtube-new\/DEvideos.csv')\nFR = pd.read_csv('..\/input\/youtube-new\/FRvideos.csv')\nGB = pd.read_csv('..\/input\/youtube-new\/GBvideos.csv')\nIN = pd.read_csv('..\/input\/youtube-new\/INvideos.csv')\nJP = pd.read_csv('..\/input\/youtube-new\/JPvideos.csv', encoding='latin')\nMX = pd.read_csv('..\/input\/youtube-new\/MXvideos.csv', encoding='latin')\nRU = pd.read_csv('..\/input\/youtube-new\/RUvideos.csv', encoding='latin')\nKR = pd.read_csv('..\/input\/youtube-new\/KRvideos.csv', encoding='latin')\nUS = pd.read_csv('..\/input\/youtube-new\/USvideos.csv')\n\nCA['country']='Canada'\nDE['country']='Germany'\nFR['country']='France'\nGB['country']='Great Britain'\nIN['country']='India'\nJP['country']='Japan'\nKR['country']='South Korea'\nMX['country']='Mexico'\nRU['country']='Russia'\nUS['country']='United States'\n\n\nallVideos = CA\n\nallVideos = pd.concat([allVideos, DE] )\nallVideos = pd.concat([allVideos, FR] )\nallVideos = pd.concat([allVideos, GB] )\nallVideos = pd.concat([allVideos, IN] )\nallVideos = pd.concat([allVideos, JP] )\nallVideos = pd.concat([allVideos, MX] )\nallVideos = pd.concat([allVideos, RU] )\nallVideos = pd.concat([allVideos, US] )\n\n\nAllData = pd.merge(allVideos, allCats, how=\"inner\", on=\"category_id\")\n\n#convert object type to strings - this is necessary for regex later.\nAllData[\"description\"] = AllData[\"description\"].astype(str) \n\nAllData[\"Number of Interactions\"] = AllData[\"dislikes\"] + AllData[\"likes\"] + AllData[\"comment_count\"]\nAllData[\"Like To Dislike Ratio\"] = AllData[\"likes\"] \/ (AllData[\"dislikes\"] + AllData[\"likes\"])\nAllData[\"Views To Interactions Ratio\"] =  AllData[\"Number of Interactions\"] \/ AllData[\"views\"] \n\nAllData.head(3)","a5871505":"sns.set_context(\"paper\", font_scale=1.5) #Lets set this up for all seaborn figures at a later point.                                                  \n\nfig, ax = plt.subplots(figsize=(10,10))   \nsns.heatmap(AllData[[\"views\", \"likes\", \"dislikes\", \"comment_count\", \"Number of Interactions\", \"Like To Dislike Ratio\", \"Views To Interactions Ratio\"]].corr(), cmap=\"coolwarm\", annot=True)","bdb0f2bd":"yAxes = [\"likes\", \"dislikes\", \"comment_count\"]\nyLabels = [\"Number of likes (Log Scale)\", \"Number of Dislikes (Log Scale)\", \"Number of Comments (Log Scale)\"]\nmarkerColors = ['green', 'red', 'grey']\nfig,axes = plt.subplots(3,1, figsize=(16,16), sharex=True, dpi=80)\nmaxViews = AllData[\"views\"].max()\ni = 0\nfor ax in axes:\n    maxY = AllData[yAxes[i]].max()\n    ax = sns.scatterplot(ax=ax, x='views',y=yAxes[i],data=AllData, s=0.1, color=markerColors[i])\n    ax.set(xlabel=\"Number of Views (Log Scale)\", ylabel=yLabels[i], xscale=\"log\", yscale=\"log\", ylim=(1,maxY), xlim=(100,maxViews))\n    i = i + 1\n","ccac819a":"fig, ax = plt.subplots(figsize=(10,10))   \n\nsns.countplot(y=\"Catagory Names\", data=AllData, palette='rainbow', order = AllData['Catagory Names'].value_counts().index)\n","58c0957d":"#Lets drop the Movies and Trailer data - there is too little data\nAllData = AllData[~AllData[\"Catagory Names\"].isin(['Movies', 'Trailers'])]","f405e037":"top5Cats = AllData[\"Catagory Names\"].value_counts()[:5].index.tolist()\nAllData = AllData[AllData[\"likes\"] > 0] \n\nallSamples = pd.DataFrame()\ni= 0\nfor cat in top5Cats:\n    sample = AllData[(AllData[\"Catagory Names\"] == cat)].sample(n=250, random_state=1)\n    temp = pd.DataFrame()\n    temp[\"views\"] = sample[\"views\"]\n    temp[\"logViews\"] = np.log10(temp[\"views\"])    \n    temp[\"likes\"] = sample[\"likes\"]\n    temp[\"logLikes\"] = np.log10(temp[\"likes\"])    \n    temp[\"dislikes\"] = sample[\"dislikes\"]\n    temp[\"comment_count\"] = sample[\"comment_count\"]\n    temp[\"cat\"] = sample[\"Catagory Names\"]\n    allSamples = allSamples.append(temp)\n\n\nax = sns.jointplot(x='logViews',y='logLikes',data=allSamples, s=8, hue=\"cat\", kind=\"scatter\", height=12, ratio = 4, palette=\"Set1\")\nax.ax_joint.set_ylim(1,allSamples[\"logLikes\"].max() + 1)\nax.ax_joint.set_xlim(1,allSamples[\"logViews\"].max() + 1)\n\nax.ax_joint.set_ylabel('Number of Comments')\nax.ax_joint.set_xlabel('Number of Views')\nax.ax_joint.set_xticks([1,2,3,4,5,6,7,8,9])\nax.ax_joint.set_xticklabels(['1','10','100','1,000','10,000','100,000','1,000,000','10,000,000','100,000,000'])\nax.ax_joint.set_yticks([1,2,3,4,5,6,7,8])\nax.ax_joint.set_yticklabels(['1','10','100','1,000','10,000','100,000','1,000,000','10,000,000'])\n","3ce93374":"AllDataGrpDF =  AllData.groupby(['Catagory Names']).mean()\nAllDataGrpDF['Likes %'] = AllDataGrpDF['likes']\/AllDataGrpDF['Number of Interactions']\nAllDataGrpDF['Dislikes %'] = AllDataGrpDF['dislikes']\/AllDataGrpDF['Number of Interactions']\nAllDataGrpDF['Comment %'] = AllDataGrpDF['comment_count']\/AllDataGrpDF['Number of Interactions']\nAllDataGrpDF['likeDislikeRatioAv'] = AllDataGrpDF['Likes %']\/AllDataGrpDF['Dislikes %']\n\nAllDataGrpDF.sort_values(by=['likeDislikeRatioAv'], inplace=True, ascending=True)\n\nfig, ax = plt.subplots(figsize=(10,10))   \n\nc = ['g','r', 'b']\n\nAllDataGrpDF\nax = AllDataGrpDF[['Likes %', 'Dislikes %', 'Comment %']].plot(kind=\"bar\", color=c, stacked=True, figsize=(13,13), fontsize =16, xlabel='', ax=ax)\n\nax.axes.yaxis.set_ticks([])\nfig.gca().legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=20)","d91c392b":"AllDataLDC= AllData[AllData[\"likes\"]>0]\nAllDataLDC= AllDataLDC[AllDataLDC[\"dislikes\"]>0]\nAllDataLDC= AllDataLDC[AllDataLDC[\"comment_count\"]>0]\nAllDataLDC= AllDataLDC[AllDataLDC[\"views\"]>0]\n\nsample = AllDataLDC.sample(n=400, random_state=0)\n\ntest = sample[sample[\"dislikes\"]== 0]\n\nsample[\"logLikes\"] = np.log10(sample[\"likes\"])    \nsample[\"logDislikes\"] = np.log10(sample[\"dislikes\"])    \nsample[\"logComments\"] = np.log10(sample[\"comment_count\"])    \nsample[\"logViews\"] = np.log10(sample[\"views\"])    \n\nyLabels = [\"Number of likes (Log Scale)\"]\nmarkerColors = ['green']\nfig,ax = plt.subplots(figsize=(10,10), dpi=80)\n\ntest\nComments = sns.regplot(x='logViews',y=\"logComments\",data=sample, color='g',  marker='o', label=\"Number of Comments\",scatter_kws={'s':8} )\nDislikes = sns.regplot(x='logViews',y=\"logDislikes\",data=sample, color='r', marker='+', label=\"Number of Dislikes\", scatter_kws={'s':8})\nLikes = sns.regplot(x='logViews',y=\"logLikes\",data=sample,  color='b', marker='^', label=\"Number of Likes\", scatter_kws={'s':8})\n\nax.set(xlabel=\"Views\", ylabel=\"\",  xticks=[3,4,5,6,7], xticklabels=['1000','10,000', '100,000', '1,000,000', '10,000,000'], xlim=(3,7.5), yticks=[1,2,3,4,5,6], yticklabels=['10', '100', '1,000', '10,000', '100,000', '1,000,000']) \n\nax.legend()  ","9317f1ce":"AllDataGrpCntry =   AllData.groupby(['country'])\nAllDataGrpDFCntry = AllDataGrpCntry.mean()\nAllDataGrpDFCntry[\"views\"] = AllDataGrpDFCntry[\"views\"]\/1000000\nAllDataGrpDFCntry = AllDataGrpDFCntry.reset_index()\n\nAllDataGrpDFCntry.sort_values(by=['views'], inplace=True, ascending=False)\nfig, ax = plt.subplots(figsize=(10,10))   \n\nsns.barplot(x=\"views\", y=\"country\", data=AllDataGrpDFCntry,palette='rainbow')\nax.set(xlabel=\"Views (Millions)\", ylabel=\"Dislikes\") \n","dfa8245b":"#stopWords = nltk.corpus.stopwords.words(\"English\")\n\nwith open('..\/input\/nltk-data\/nltk_data\/corpora\/stopwords\/english') as f:\n    stopWords = f.readlines()\n\nnames = nltk.corpus.names.words()\nAllData = AllData[AllData[\"description\"].notnull()]\n\n#remove some very common words which add little information to the sentiment of the description - i.e. company names\n\nextraStopWords = ['ft', 'feat', 'feature', 'video', 'music', 'twitter', 'instagram', 'facebook', 'channel', 'youtube',\n                 'show', 'subscribe', 'film', 'website', 'google', 'tumblr', 'album', 'movie']\n\n[stopWords.append(w) for w in extraStopWords]\n\nnames = [n.lower() for n in names]","2f1c2128":"Uk_Data = AllData.loc[AllData.country == \"Great Britain\"].copy()\n\nnumRecords = Uk_Data[\"description\"].size\n\ndef CleanDescription(PhraseToClean):\n    CleanWords = [w for w in PhraseToClean.split() if NotUrl(w)]      \n    CleanWords = [RemoveNonAlphaNumeric(w) for w in CleanWords]\n    CleanWords = [w for w in CleanWords if  NotInStopWords(w) and NotInNames(w)]\n    return ' '.join(CleanWords)\n\ndef NotInStopWords(word):\n    return word.lower() not in stopWords\n\ndef NotInNames(word):\n    return word.lower() not in names\n\nurlPattern = \"http[s]*:\\\/\\\/[^ ]*\"\n\ndef NotUrl(word):\n    return not re.match(urlPattern,word)\n\ndef RemoveNonAlphaNumeric(phrase):\n    return re.sub(r'\\W+', '', phrase)\n\nUk_Data[\"Cleaned Description\"] = Uk_Data[\"description\"].apply(lambda x: CleanDescription(x))\n\nUk_Data[\"Cleaned Description\"].head()","265e75d5":"def plot_cloud(wordcloud):\n    # Set figure size\n    plt.figure(figsize=(40, 30))\n    # Display image\n    plt.imshow(wordcloud) \n    # No axis details\n    plt.axis(\"off\");\n\nwordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='salmon', colormap='Pastel1', collocations=False, stopwords = STOPWORDS).generate(' '.join(Uk_Data[\"Cleaned Description\"]))\n\nplot_cloud(wordcloud)","80b11f08":"nltk.download('vader_lexicon')","c7c74846":"sia = SentimentIntensityAnalyzer()\nUk_Data[\"Sentiment Scores\"] = Uk_Data[\"Cleaned Description\"].apply(lambda x: sia.polarity_scores(x)[\"compound\"])\nUk_Data.head(3)","fae62e73":"SentimentScores = Uk_Data[\"Sentiment Scores\"]\nfor col in ['views','likes','dislikes','comment_count','Number of Interactions','Like To Dislike Ratio','Views To Interactions Ratio']:\n    print(f\"Correlation between Sentiment Score and {col} = { SentimentScores.corr(Uk_Data[col])}\")","6ebfd562":"Uk_DataGrp =  Uk_Data.groupby(['Catagory Names'])\nUk_DataGrpDF = Uk_DataGrp.mean()\nUk_DataGrpDF.sort_values(by=['Sentiment Scores'], inplace=True, ascending=False)\n\nUk_DataGrpDF = Uk_DataGrpDF.reset_index()\nfig, ax = plt.subplots(figsize=(10,10))   \nsns.barplot(x=\"Sentiment Scores\", y=\"Catagory Names\", data=Uk_DataGrpDF,palette='rainbow')","7106ac93":"AllDataGrpDF.reset_index(inplace=True)\ncats = AllDataGrpDF[\"Catagory Names\"].tolist()\ncatsDF = pd.DataFrame()\nfor cat in cats:\n    tmpDF = pd.DataFrame([[cat, \n                          str(len(AllDataGrpDF.index) - AllDataGrpDF.index[AllDataGrpDF[\"Catagory Names\"] == cat].tolist()[0]),\n                          (1+ Uk_DataGrpDF.index[Uk_DataGrpDF[\"Catagory Names\"] == cat].tolist()[0])]],\n                         columns=[\"Catagory\", \"Figure 5 Rank\", \"Figure 9 Rank\"]\n                        )\n    catsDF = catsDF.append(tmpDF)\n    \ncatsDF.set_index(\"Catagory\", inplace=True)   \ncatsDF","17336619":"sns.lineplot(data=catsDF.reindex(index=catsDF.index[::-1]), x=\"Figure 5 Rank\", y=\"Figure 9 Rank\")\n","8d0c5e2a":"<b>Fig 5. <\/b><p>This plot shows the relative likes, dislikes and comments of different catagories, ordered by the ratio of Likes\/Dislikes. We can see that whilst News and Politics gains the most dislikes per like, pets and animals gains the fewest. This could be intuitively explained as News and Politics is far more a divisive topic, and is more likely to cause people to react negatively than Pet Videos. Lets investigate a couple more interesting trends between viewership stats. <\/p>","9d3b0588":"<b>Fig 1. <\/b><p>Looking at the above correlation heatmap, it is unsurprising that we find some strong positive corellations between the factors which are understood as markers of a high quality video with a large audience, i.e. the number of views, likes, number of comments and number of interactions.. \n\nThe weaker positive correlations between the number of dislikes and qualities of a good video can be explained intuitively, i.e. a weak positive corellation between dislikes and views is a result which could be a caused by higher numbers of dislikes putting people off from viewing the video, however of course on the flip side it is important to note that there will still be a positive corellation since people need to view the video in order to leave a dislike. \n\nThere are not any particularly note-worthy corellations attained from the like to dislike ratio or view to interaction ratio which can provide much further insight. \n\n\nLets examine a couple of relationships in greater detail, starting with views vs likes, dislikes and number of comments\n<\/p>\n","8e9637ba":"<p>There are no significant correlation between the Sentiment score of the description and any particular video metrics, lastly lets look at whether the sentiment score vary between video catagories.<\/p>","cb2d4c67":"<p>Let's have a look at the GB dataset to begin start. source - https:\/\/www.kaggle.com\/datasnaek\/youtube-new<\/p>","3965b62a":"<b>Fig 10.<\/b>\n<p>Figure showing the rank of each catagory in the orderered column lists in Fiures 5 and 9 respectively. The ranking order of figure 5 represents the like to dislike ratio in descending order and the sentiment of the video description of figure 9. Below is a line plot which shows how these two rankings relate to each other.<\/p>","8421a275":"<p>Lets map out some correlations to see if there are any relationships which are worth a further investigation<\/p>","da99de36":"<b>Fig 6. <\/b><p> We can see that as the number of views increases, the number of dislikes, and likes increase at a similar rate, however it is interesting to note that the comment count seems to increase at a slower rate with views, perhaps there is a lower inclination to comment on videos with more views?<\/p>","d857df41":"<b>Fig 8. <\/b><p>The above word-cloud shows how certain words are dominant in the video descriptions perhaps unsurpisingly i.e. \"New\", \"Official\", \"videos\". The above methodology was used to determine some of the additional stop words which were added manually for removal. It could be interesting to show a word cloud for different video catagories, but this is being left for further imrpovements<\/p>","ad7efe82":"<b>Fig 3. <\/b><p>Just considering the top 5 most popular catagories to reduce noise, to make the comparison more useful in terms of understanding the distributions, lets take random samples with from each of these catagories so each distibution has the same overall volumne. We can then look at the distribution of viewership metrics of each of these catagories<\/p>","7dc56a23":"<b>Fig 7. <\/b><p>Lets have a look at the sentiment of the description field, we will be using a pre-trained sentiment analyser which is used to determine the sentiments of social media posts. The sentiment score is ranked from -1 to +1 to which denotes if a phrase is negative of positive respectiviely - i.e. the phrase \"Today was amazing\" would rate higher than \"Today was quite good\". A negative sentament score could be as follows - \"Today was a bad day :(\". To simplfy things lets just work with the UK dataset<\/p>","4d276049":"<b>Fig 2. <\/b><p>Unsurprisingly it is clear that as the number of views increases, the number of comments,views and dislikes also increases. Now lets see if we can deduce any structure from the catagorical data -  note that the stratification we see along the y-axis in the above figures is caused by the the fact we are looking at discreet low-number quantities<\/p>","f083bc63":"<b>Fig 4. <\/b><p>As we can see the most viewed and most commented videos are dominated by the musical catagory, closely followed by entertainment, however we are only looking at the top 5 catagories by population size in the initial dataset, let's examine the viewership metrics of all catagories now.<\/p>","07dbab16":"<p>Now lets create a dataframe which contains all datasets including catagory names.<\/p>\n<p>Let's also add a few new metrics which could yield some interesting information around viewership.<\/p>\n\n\n- \"Number of Interactions\": Likes + Dislikes + Comments\n- \"Like To Dislike Ratio\": Likes \/ (Likes + Dislikes)\n- \"Views To Interactions Ratio\":  Likes + Dislikes + Comments \/ Views","2cabaaf3":"<h1>Trending Youtube Videos<\/h1>\n<br\/>\n<p>In this Report a dataset containing some exploratory data analysis is performed on some youtube videos. The intention is to find any relationships which could have an intuitative explanation.<\/p>","5978a578":"<b>Fig 9. <\/b>\n\n<p> Descriptions of videos of certain catagories are perhaps unsurprisingly more positive than others, for instance it is intuitive to see why Travel Videos, which often are used to depict a glamourous view of a tourist location would have a more positive description than News and Politics which often describe an story in an unfavourable way dependending on the poltical affiliation of the channel and their respective values allign with the story they are describing.  It is interesting to note that the trends of catagories in Fig 9 and Fig 5 match well - under the assumption both Fig 5 and Fig 9 map to how \"controversial\" a video viewership and description is respectively, it is interesting to note that a majority of the most and least controversial video catagories corroborate approximately between Figures. I.e. \"News and Politics\" is the most and second most controversial catagory according to Fig 5 and 9 respectively, as \"Pets and Animals\" is the Least and Third Least. \n\nBelow are the catagories in order of controversy according to Fig 5 and 9 to desmonstrate the similarity between these two Datasets more visually.\n<\/p>","850d83a3":"<P>Now we have the sentiment score let's see if we can identify any interesting trends between the Sentiment Score and other video metrics or if there are any catagories which have distinct Sentiment Scores.<\/P>"}}