{"cell_type":{"4974cb29":"code","24b0cfab":"code","206abc44":"code","17127dee":"code","a5eed732":"code","3287fb51":"code","b0b4ddf7":"code","b81810f1":"code","601af2b0":"markdown","f6599265":"markdown","37490927":"markdown","91e3baa9":"markdown","13913550":"markdown","a6bd073a":"markdown","5017d3a9":"markdown"},"source":{"4974cb29":"# Generic\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os, gc, re, warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns","24b0cfab":"# Loading Universal Sentence Encoder\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nencoder_url = 'https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4'     # Note that this is 1GB Module\n\nencoder = hub.load(encoder_url)","206abc44":"# Custom Function\ndef embed(input):\n    return encoder(input)","17127dee":"word = \"Tutorial\"\nsentence = \"The input is variable length English text and the output is a 512 dimensional vector\"\nparagraph = (\n    \"The Universal Sentence Encoder encodes text into high-dimensional vectors that can be used for text classification, \"\n    \"semantic similarity, clustering and other natural language tasks. The model is trained and optimized for greater-than-word length text, \"\n    \"such as sentences, phrases or short paragraphs. It is trained on a variety of data sources and a variety of tasks with the aim of dynamically \"\n    \"accommodating a wide variety of natural language understanding tasks\")\n\nmessages = [word, sentence, paragraph]\nmessage_embeddings = embed(messages)\n\nfor i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n    print(\"Message: {}\".format(messages[i]))\n    print(\"Embedding size: {}\".format(len(message_embedding)))\n    message_embedding_snippet = \", \".join((str(x) for x in message_embedding[:3]))\n    print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))","a5eed732":"# Custom Function to Estimate & then plot(heatmap) the text similarity\n\ndef plot_similarity(labels, features, rotation):\n    corr = np.inner(features, features)\n    sns.set(font_scale=1.2)\n    plt.figure(figsize=(10,10))\n    g = sns.heatmap(corr, xticklabels=labels, yticklabels=labels,\n                    vmin=0, vmax=1, cmap='coolwarm', robust=True, cbar=False, square=True, annot=True)\n    g.set_xticklabels(labels, rotation=rotation)\n    g.set_title(\"Semantic Similarity\", fontsize=25)\n\ndef run_and_plot(messages_):\n    message_embeddings_ = embed(messages_)\n    plot_similarity(messages_, message_embeddings_, 90)","3287fb51":"# Some Random Sentences\n\nmessages = [\n    # Shipping\n    \"What has happened to my delivery?\",\n    \"What is wrong with my shipping?\",\n\n    # Weather\n    \"What is the weather like tomorrow?\",\n    \"Will it snow tomorrow?\",\n\n    # Health\n    \"An apple a day, keeps the doctors away\",\n    \"Eating apples is healthy\",\n\n    # Age\n    \"How old are you?\",\n    \"what is your age?\",\n]","b0b4ddf7":"# Plot Similarities\nrun_and_plot(messages)","b81810f1":"# Garbage Collection\ngc.collect()","601af2b0":"# Setup","f6599265":"# Text \/ Semantic Similarity - Tutorial\n\n\n### Premise:\nMore often than not data-geeks\/businesses\/portals need estimate the degree of similarity between two texts. And believe it or not, Google (or other search engines) does this on a regular basis to find the relevance between search-queries.\n\n**In businesses**: \n\nIn Legal Dept. the text\/semantic similarities allow to mitigate risks on a new contract, based on the assumption that if a new contract is similar to a existent one that has been proved to be resilient, the risk of this new contract being the cause of financial loss is minimised\n\nIn Customer Services the model should be able to understand similarity between queries from users and provide a uniform response. The emphasis on text similarity aims to create a model that recognizes language and word patterns to craft responses that are similar to how a human conversation works.\n\n**Example**: If the user asks,\n\n* What is the status of my delivery ?  Or   What is wrong with my shipping ?\n\nThe user will expect the same response. \n\n","37490927":"Now that we have the premise, lets see what exactly is Text \/ Semantic Similarity ?\n\nSimply to put, Text similarity is a process to determine how \u2018close\u2019 two pieces of text are both in surface closeness (lexical similarity) and meaning (semantic similarity)\n\nFor instance, how similar are the phrases \u201chow old are you?\u201d with \u201cwhat is your age?\u201d by just looking at the words.\n\n**Surface closeness** - Not much similarity on the word level\n\n**Meaning** - They have almost the same meaning in terms of the query & the response\n\n\nNow the question arises, how do we go about estimating the similarity between texts? Luckily TensorFlow has the answer to this.\n\nThe TensorFlow Hub provides a [Universal Encoder](https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4) to perform the Semantic Similarity. In this notebook we'll see how to access the **Universal Sentence Encoder** and use it for sentence similarity and sentence classification tasks. The Universal Sentence Encoder encodes text into high dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks. \n\nThe Universal Sentence Encoder makes getting sentence level embeddings as easy as it has historically been to lookup the embeddings for individual words. The sentence embeddings can then be trivially used to compute sentence level meaning similarity as well as to enable better performance on downstream classification tasks using less supervised training data\n\n\nSo, lets get started. :-)","91e3baa9":"# Embedding\n\nLet us see how the Universal Sentence Encoder embeds words , sentences & paragraphs","13913550":"As we can observe, there is a **positive correlation** (approaching 1) between the sentence pairs and this shows that these sentences are semantically similar. ","a6bd073a":"# Visualizing Similarity","5017d3a9":"### I hope this little tutorial was helpful for understanding & estimating Text \/ Semantic Similarity between texts."}}