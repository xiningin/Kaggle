{"cell_type":{"71115b3c":"code","20d1c7a8":"code","4d1c5119":"code","157003bc":"code","1940890d":"code","0f2d484b":"code","82097a41":"code","5bfd20d8":"code","962a2268":"code","4355a130":"code","d0ddf0bb":"code","c8b7e5ce":"code","54810e7f":"code","087a2eb3":"code","9e3c99c6":"code","8d72222a":"code","3b478f15":"code","93fbd376":"code","7b5e033f":"code","4752b369":"code","aad913f3":"code","29cf159b":"code","9793e4db":"code","73d39edf":"code","399c7262":"code","43a6292d":"code","30409456":"code","088954ab":"code","76e29356":"markdown","8ceff32c":"markdown","14c16a80":"markdown","48ea996d":"markdown","c2808411":"markdown","afe5afa0":"markdown","71ba20e3":"markdown","4bd6685e":"markdown","9cedc45c":"markdown","331a3d39":"markdown","f2c97b79":"markdown","d3f5c91b":"markdown","db2d91af":"markdown","3b33a572":"markdown","93e73430":"markdown","06f54ccc":"markdown","0ab544f8":"markdown","cdb8124f":"markdown","b04481f0":"markdown","2641764d":"markdown","8a9547d7":"markdown","e30207b2":"markdown","d75b8000":"markdown"},"source":{"71115b3c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\ndf = pd.read_csv('..\/input\/water-potability\/water_potability.csv')","20d1c7a8":"df.head()","4d1c5119":"df.info()","157003bc":"df.columns","1940890d":"df.describe()","0f2d484b":"fig, ax = plt.subplots(1,2,figsize=(10,8))\ndf.Potability.value_counts().plot(kind ='pie', ax=ax[0])\n# ax[0].pie(x=df['Potability'])\nsns.countplot(data=df,x='Potability',ax=ax[1])\nplt.show()","82097a41":"plt.figure(figsize=(10,10))\n\nfor ax,col in enumerate(df.columns[:9]):\n    plt.subplot(3,3,ax+1)\n    plt.title(f'Distribution of {col}')\n    sns.kdeplot(x=df[col], fill=True, alpha=0.4, hue = df.Potability, multiple='stack')\nplt.tight_layout()","5bfd20d8":"plt.figure(figsize=(10,10))\n\nfor ax,col in enumerate(df.columns[:9]):\n    plt.subplot(3,3,ax+1)\n    plt.title(f'Distribution of {col}')\n    sns.boxplot(data=df, x='Potability',y=df[col])\n    plt.legend(prop=dict(size=10))\nplt.tight_layout()","962a2268":"sns.pairplot(data=df,hue='Potability')","4355a130":"corr = df.corr()\nfig, ax = plt.subplots(figsize=(8, 8))\ncolormap = sns.diverging_palette(220, 10, as_cmap=True)\ndropSelf = np.zeros_like(corr)\ndropSelf[np.triu_indices_from(dropSelf)] = True\ncolormap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, cmap=colormap, linewidths=.5, annot=True, fmt=\".2f\", mask=dropSelf)\nplt.show()","d0ddf0bb":"df['Potability'].value_counts()","c8b7e5ce":"from sklearn.utils import resample\nfrom sklearn.utils import shuffle\n\nnot_potable  = df[df['Potability']==0]   \npotable = df[df['Potability']==1]  \n\nnew = resample(potable, replace = True, n_samples = 1998) \ndf = pd.concat([not_potable, new])\n\ndf = shuffle(df)","54810e7f":"df.Potability.value_counts().plot(kind ='pie')","087a2eb3":"df.isnull().sum()","9e3c99c6":"for cols in ['ph','Sulfate','Trihalomethanes']:\n    df[cols]=df[cols].fillna(df.groupby(['Potability'])[cols].transform('median'))","8d72222a":"df.isnull().sum().any()","3b478f15":"from sklearn.model_selection import train_test_split\nX = df.drop('Potability',axis=1)\ny = df['Potability']\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=101)","93fbd376":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\n\n# FIRST SCALE OUR DATA\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\nlog_model = LogisticRegression(solver='saga', multi_class='ovr', max_iter=5000)\n\npenalty = ['l1','l2','elasticnet']\nl1_ratio = np.linspace(0,1,20)\nC = np.logspace(0,10,20)\nparam_grid = { 'penalty':penalty, 'l1_ratio':l1_ratio, 'C':C }\n\ngrid_model = GridSearchCV(log_model, param_grid=param_grid)\ngrid_model.fit(X_train, y_train)","7b5e033f":"grid_model.best_params_","4752b369":"from sklearn.metrics import accuracy_score, plot_confusion_matrix,classification_report\ny_prediction = grid_model.predict(X_test)\naccuracy_score(y_test,y_prediction)","aad913f3":"plot_confusion_matrix(grid_model, X_test, y_test)","29cf159b":"print(classification_report(y_test,y_prediction))","9793e4db":"from sklearn.neighbors import KNeighborsClassifier\ntest_error_rates = [] \n\nfor k in range(1,30):\n    knn_model = KNeighborsClassifier(n_neighbors=k)\n    knn_model.fit(X_train,y_train)\n    \n    y_pred = knn_model.predict(X_test)\n    test_error = 1 - accuracy_score(y_test, y_pred)\n    \n    test_error_rates.append(test_error)","73d39edf":"plt.plot(range(1,30),test_error_rates)\nplt.ylabel('ERROR RATE')\nplt.xlabel('k Neighbor')\nplt.show()","399c7262":"from sklearn.pipeline import Pipeline\nscaler = StandardScaler()\nknn = KNeighborsClassifier()\noperations = [('scaler',scaler),('knn',knn)]\npipe = Pipeline(operations)\n\nk_values = list(range(1,20))\n\nparam_grid = { 'knn__n_neighbors': k_values }\n\nfull_cv_classifier = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy')\nfull_cv_classifier.fit(X_train, y_train)","43a6292d":"full_cv_classifier.best_estimator_.get_params()","30409456":"y_pred = full_cv_classifier.predict(X_test)\nprint(classification_report(y_test,y_pred))","088954ab":"plot_confusion_matrix(full_cv_classifier, X_test, y_test)","76e29356":"<font size=5><center>Water Quality Prediction<\/center><\/font>\n\"![Potable-Water.jpg](attachment:d4932c33-41ae-4660-ba72-e1d778cd6f41.jpg)","8ceff32c":"### From Above charts we can confirm:\n1. Heatmap, pairplot: there is not any strong corrolation between our features. Also there's not any comultinearity.\n2. Boxplot: There isn't any significant difference between potable and not potable in different feature. There are some outliers too, but i leave them to be in dataset. Removing them again can be a good option depends on the domain knowledge.\n3. kdeplot: some features like Solids might be a little skewed but i guess that's not a big deal in this case.","14c16a80":"### Let's learn some useful stuff about our features first:\n\n1. **ph**: pH of 1. water (0 to 14).\n\n2. **Hardness**: Capacity of water to precipitate soap in mg\/L.\n\n3. **Solids**: Total dissolved solids in ppm.\n\n4. **Chloramines**: Amount of Chloramines in ppm.\n\n5. **Sulfate**: Amount of Sulfates dissolved in mg\/L.\n\n6. **Conductivity**: Electrical conductivity of water in \u03bcS\/cm.\n\n7. **Organic_carbon**: Amount of organic carbon in ppm.\n\n8. **Trihalomethanes**: Amount of Trihalomethanes in \u03bcg\/L.\n\n9. **Turbidity**: Measure of light emiting property of water in NTU.\n\n10. **Potability**: Indicates if water is safe for human consumption. Potable - 1 and Not potable - 0","48ea996d":"**Result: ** KNN gives us a better result and accuracy! ","c2808411":"**Description: ** our data a little bit unbalanced. we need to consider this in our modeling later.","afe5afa0":"### In this part we need to deal with two main problems. First, as we saw befor our labels are imbalanced. So let's try to balance them. shall we? =)","71ba20e3":"<a class=\"anchor\" id=\"toc\"><\/a>\n<div style=\"background: #f9f9f9 none repeat scroll 0 0;border: 1px solid #aaa;display: table;font-size: 95%;margin-bottom: 1em;padding: 20px;width: 600px;\">\n<h1>Contents<\/h1>\n<ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">\n<li style=\"list-style: outside none none !important;font-size:17px\"><a href=\"#1\">1 Introduction<\/a><\/li>\n    \n<li style=\"list-style: outside none none !important;font-size:17px\"><a href=\"#2\">2 Understanding data<\/a><\/li>\n\n<li style=\"list-style: outside none none !important;font-size:17px\"><a href=\"#3\">3 Exploratory Data Analysis<\/a><\/li>\n<li style=\"list-style: outside none none !important;font-size:17px\"><a href=\"#3\">4 Data Prepration and Feature Engineering<\/a><\/li>\n<li style=\"list-style: outside none none !important;font-size:17px\"><a href=\"#4\">5 Modelling<\/a><\/li>\n    <ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">  \n        \n        \n\n<\/ul>\n<\/div>","4bd6685e":"### 5.1 Logistic Regression","9cedc45c":"Let's first use the elbow method to find out the best value for k","331a3d39":"I decided to use Logistic Regression, KNN and Desicion Tree for this dataset. Although there others algorithms to use. So let me know your opinion in the comment section =))","f2c97b79":"Let's try Pipeline and gridsearchCV to find best hyperparameters for our model (I used pipeline for education perpuse. In real life solution it's not necessory for this problem)","d3f5c91b":"## 2. Understanding Data","db2d91af":"## 1. Introduction","3b33a572":"### for the second problem(missing values), we got two options:\n* drop all the NaN values, cause referring to our domain knowledge it's important to have true values in our features so that we don't get more FP(False Posivite)\n* We can describe our TN and FP. Maybe we can use median for our missing data\n#### I will do the second solution for some practise :)) but remember it depends on the domain knowledge to choose the right solution!\n#### As we saw our boxplot of features, median of both potable and not potable data are almost the same in any feature. so we can use the overall median of each feature for NaN values.","93e73430":"Congrats! Our data is now balances. So let's fix our nex problem. Let's fix our missing data ","06f54ccc":"### 5.2 KNN","0ab544f8":"**Result: ** As we saw we use grid search and logistic regression and the result wasn't quite satisfying","cdb8124f":"## 5. Modeling","b04481f0":"## 4. Data Prepration and Feature Engineering","2641764d":"Let's try to know our features better...","8a9547d7":"## 3. Exploratory Data Analysis","e30207b2":"You can balance your data by resampling them. The followings are two different techniques for resampling:\n* Upsampling (increase your minority class)\n* Downsample (decrease your majority class)","d75b8000":"First of all i need to point out what is actually the point of doing full and comprehensive EDA on our data?\nThere are many reasons for that. For start, it's important to mention that in order to do a good prediction we need to know our data based on our domain knowledge.\nthe most import reasons of EDA:\n* It exposes trends, patterns, and relationships that are not readily apparent\n* Getting a \u201cfeel\u201d for this critical information can help you detect mistakes, debunk assumptions, and understand the relationships between different key variables\n* Better insights may eventually lead to do percise feature engineering and better selection of an appropriate predictive model\n\n### Now let's do it step by step. First lets look at our label in order find out see if it's balanced or not."}}