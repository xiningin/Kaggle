{"cell_type":{"7789d0cf":"code","48f621c1":"code","89778d36":"code","7590d733":"code","ca5b7e15":"code","5a68614e":"code","6164b6d9":"code","3d52db13":"code","af8f6de8":"code","62459f3d":"code","490dd635":"code","3df2a71d":"code","fb40d422":"markdown"},"source":{"7789d0cf":"import os\nimport tensorflow as tf\ntf.enable_eager_execution()\nimport pandas as pd\nimport numpy as np\nimport sklearn.model_selection\nimport sklearn.preprocessing","48f621c1":"# Parameters\nimage_dims = (28, 28, 1)\nbatchsize = 64\nnum_labels = 10","89778d36":"data_training, data_validation = sklearn.model_selection.train_test_split(\n    pd.read_csv(\"..\/input\/train.csv\"), test_size=0.2)\ndata_test = pd.read_csv(\"..\/input\/test.csv\")\n\nX_training = data_training.drop(\"label\", axis=1).values\nX_validation = data_validation.drop(\"label\", axis=1).values\nX_test = data_test.values.reshape((-1,) + image_dims)\n\ny_training = data_training[\"label\"]\ny_validation = data_validation[\"label\"]","7590d733":"def preprocess_training_data(image, label):\n    \"\"\"\n    Converts 1D array into 2D matrix and normalized entries\n    \"\"\"\n    image = tf.reshape(image, image_dims)\n    image = tf.dtypes.cast(image, tf.float64)\n    min_val = tf.reduce_min(image)\n    max_val = tf.reduce_max(image)\n    image = (image - min_val) \/ (max_val - min_val)\n    label = tf.one_hot(indices=label, depth=num_labels)\n    return image, label\n\ndef preprocess_testing_data(image):\n    \"\"\"\n    Converts 1D array into 2D matrix and normalized entries\n    \"\"\"\n    image = tf.reshape(image, image_dims)\n    image = tf.dtypes.cast(image, tf.float64)\n    min_val = tf.reduce_min(image)\n    max_val = tf.reduce_max(image)\n    image = (image - min_val) \/ (max_val - min_val)\n    return image","ca5b7e15":"def make_dataset(data, labels=None):\n    if labels is None:\n        dataset = tf.data.Dataset.from_tensor_slices(data)\n        dataset = dataset.map(preprocess_testing_data, num_parallel_calls=-1)\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n        dataset = dataset.shuffle(buffer_size=len(data))\n        dataset = dataset.map(preprocess_training_data, num_parallel_calls=-1)\n    dataset = dataset.batch(batchsize)\n    dataset = dataset.prefetch(1)\n    return dataset","5a68614e":"dataset_training = make_dataset(X_training, y_training)\ndataset_validation = make_dataset(X_validation, y_validation)\ndataset_testing = make_dataset(X_test)","6164b6d9":"def make_model():\n    input_layer = tf.keras.layers.Input(\n        shape=image_dims, name=\"Input\")\n    layer = tf.keras.layers.Conv2D(\n        filters=64, kernel_size=(3, 3), \n        activation=\"relu\", use_bias=True,\n        kernel_initializer=\"glorot_uniform\", \n        bias_initializer=\"glorot_uniform\")(input_layer)\n    layer = tf.keras.layers.Dropout(rate=0.3)(layer)\n    layer = tf.keras.layers.Conv2D(\n        filters=64, kernel_size=(3, 3), \n        activation=\"relu\", use_bias=True,\n        kernel_initializer=\"glorot_uniform\", \n        bias_initializer=\"glorot_uniform\")(layer)\n    layer = tf.keras.layers.Dropout(rate=0.3)(layer)\n    layer = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(layer)\n    layer = tf.keras.layers.Conv2D(\n        filters=128, kernel_size=(3, 3), \n        activation=\"relu\", use_bias=True,\n        kernel_initializer=\"glorot_uniform\", \n        bias_initializer=\"glorot_uniform\")(layer)\n    layer = tf.keras.layers.Dropout(rate=0.3)(layer)\n    layer = tf.keras.layers.Conv2D(\n        filters=128, kernel_size=(3, 3), \n        activation=\"relu\", use_bias=True,\n        kernel_initializer=\"glorot_uniform\", \n        bias_initializer=\"glorot_uniform\")(layer)\n    layer = tf.keras.layers.Dropout(rate=0.3)(layer)\n    layer = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(layer)\n    layer = tf.keras.layers.Flatten()(layer)\n    output_layer = tf.keras.layers.Dense(units=num_labels, activation=\"softmax\")(layer)\n    model = tf.keras.models.Model(inputs=input_layer, outputs=output_layer)\n    model.compile(optimizer=\"adam\", \n        loss=\"categorical_crossentropy\",\n        metrics=[\"accuracy\"])\n    return model","3d52db13":"model = make_model()","af8f6de8":"model.summary()","62459f3d":"model.fit(\n    dataset_training, epochs=5, validation_data=dataset_validation, \n    steps_per_epoch=int(len(data_training)\/batchsize), \n    validation_steps=int(len(data_validation)\/batchsize))","490dd635":"pred = model.predict(dataset_testing)","3df2a71d":"output = pd.DataFrame(\n    data={\n        \"ImageId\": data_test.index+1, \n        \"Label\": np.argmax(pred, axis=1)})\n\noutput.head()\noutput.to_csv(\"TestPrediction.csv\", index=False)","fb40d422":"This notebook creates a tensorflow `tf.data.Dataset` object from images on disk"}}