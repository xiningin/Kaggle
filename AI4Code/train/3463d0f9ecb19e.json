{"cell_type":{"c0b5ba32":"code","9b4b496a":"code","4df8d7b3":"code","f9bedfdb":"code","5221ebee":"code","a7b0d55f":"code","5412887a":"code","62f02629":"code","97a33efe":"code","1f2df4c4":"code","8bab4698":"code","78e97feb":"code","c4d0ab79":"code","37176b18":"code","acb9b069":"code","4a2b3d73":"code","cc4eabb0":"code","cf243b1d":"code","0b08df13":"code","f0513904":"code","83843f6a":"code","8de1cb9f":"code","3d920f8f":"code","323365a9":"code","00f208f7":"code","540e9849":"code","979274d8":"code","aa7deda4":"code","da4f76b7":"code","3e31998e":"code","f9cdc693":"code","5443f8b0":"code","321e757c":"code","66104736":"code","21713f08":"code","898811e0":"code","85ad9902":"code","d2d3dce6":"code","abe8be18":"code","fe6ed570":"code","2b83f5c5":"code","c73ecfd3":"code","78261829":"code","9ce31a66":"code","c3d2d504":"code","1bbef524":"code","33140e4f":"code","855688d7":"code","630791d0":"code","7b0ca68c":"code","1dcdf4ce":"code","320fe235":"code","51032359":"code","5d8aa320":"code","e7e7eee1":"code","0ebb2f50":"code","aaf0573c":"code","4be74581":"code","a7cd078d":"code","449b293a":"code","3b277cbb":"code","e1d3a66d":"code","84cc907d":"code","6fd3a4e1":"code","f4c47391":"code","88d57d87":"code","6322f948":"code","d3a4fdc7":"code","fce102ee":"code","3b62ec39":"code","b490d396":"code","e801c1de":"code","9bd01009":"markdown","4a4a3f0d":"markdown","ffdb4e11":"markdown","3e2343a0":"markdown","d0a38899":"markdown","f78deb2c":"markdown","c3d532f6":"markdown","cf2854a3":"markdown","ca09bb0d":"markdown","4b2e7e6b":"markdown","f043164c":"markdown","b595002b":"markdown","6e7c9cb4":"markdown","a4aaad64":"markdown","8b8984b7":"markdown","1d6dd6a2":"markdown","69b8bfd6":"markdown"},"source":{"c0b5ba32":"# importing the necessary library\n%config Completer.use_jedi = False # if autocompletion doesnot work in kaggle notebook | hit tab\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nsns.set()\nplt.style.use('fivethirtyeight')\n%config InlineBackend.figure_format = 'retina'\nimport warnings\nwarnings.filterwarnings('ignore') # igoring any kind of warning if comes \n","9b4b496a":"df = pd.read_csv('..\/input\/electric-power-consumption-data-set\/household_power_consumption.txt',sep=';', \n                 parse_dates={'date_time' : ['Date', 'Time']}, infer_datetime_format=True, \n                 low_memory=False, na_values=['nan','?'], index_col='date_time')","4df8d7b3":"df.head()","f9bedfdb":"df.info()","5221ebee":"df.shape","a7b0d55f":"np.isnan","5412887a":"df.isna().sum() # these are some missing values ","62f02629":"# filling missing values by the value of one day before \ndef fill_missing(data):\n    one_day = 23*60\n    for row in range(data.shape[0]):\n        for col in range(data.shape[1]):\n            if np.isnan(data[row,col]):\n                data[row,col] = data[row-one_day,col]","97a33efe":"fill_missing(df.values)","1f2df4c4":"df.isna().sum() # no nan values ","8bab4698":"df.to_csv('cleaned_data.csv')","78e97feb":"df = pd.read_csv('cleaned_data.csv',parse_dates=['date_time'], index_col= 'date_time')","c4d0ab79":"#  df index  'datatime64[ns]'\ndata = df.resample('D').sum() #all the units of particular day \ndf.resample('Y').sum()","37176b18":"data # now data is in day frequency\ndata.head()","acb9b069":"fig, ax = plt.subplots(figsize = (18,18))\nfor i in range(len(data.columns)):\n    plt.subplot(len(data.columns),1,i+1)\n    name = data.columns[i]\n    plt.plot(data[name])\n    plt.title(name,y = 0,loc = 'right')\n    plt.yticks([])\nplt.show()\nfig.tight_layout()","4a2b3d73":"# now lets plot the active power per year \nyears = ['2007','2008','2009','2010']\nfig, ax = plt.subplots(figsize = (20,20))\n\nfor i in range(len(years)):\n    plt.subplot(len(data.columns),1,i+1)\n    year = years[i]\n    active_power_data = data[str(year)]['Global_active_power']\n    plt.plot(active_power_data)\n    plt.title(str(year)+\" > \"+str(np.round(active_power_data.sum(),1)),y = 0, loc = 'left')\nplt.show()\nfig.tight_layout()","cc4eabb0":"data['2010']['Global_active_power'].sum()","cf243b1d":"years = ['2007','2008','2009','2010']\nfig, ax = plt.subplots(figsize = (20,20))\n\nfor i in range(len(years)):\n    plt.subplot(len(data.columns),1,i+1)\n    year = years[i]\n    active_power_data = data[str(year)]['Global_active_power']\n    active_power_data.hist(bins = 200)\n    plt.title(str(year)+\" > \"+str(np.round(active_power_data.sum(),1)),y = 0, loc = 'left')\nplt.show()\nfig.tight_layout()","0b08df13":"fig, ax = plt.subplots(figsize = (18,18))\nfor i in range(len(data.columns)):\n    plt.subplot(len(data.columns),1,i+1)\n    name = data.columns[i]\n    data[name].hist(bins = 200)\n    plt.title(name,y = 0,loc = 'right')\n    plt.yticks([])\nplt.show()\nfig.tight_layout()","f0513904":"data","83843f6a":"months = [i for i in range(1,13)]\nfig, ax = plt.subplots(figsize = (20,20))\nfor i in range(len(months)):\n    ax = plt.subplot(len(months),1,i+1)\n    month = '2007-'+str(months[i])\n    active_power = data[month].Global_active_power\n    sns.distplot(active_power, kde = True)\n    plt.title(month+\" \"+str(np.round(active_power.sum(),1)),loc = 'left')\nplt.show()\nfig.tight_layout(pad = 5)\n    ","8de1cb9f":"data['2009'].tail()","3d920f8f":"data_ = data['Global_active_power']\ndata_test  = data_['2010']\ndata_train = data_.loc[:'2009-12-31']\n","323365a9":"data_train","00f208f7":"X_train,y_train =[],[]\n\nfor i in range(21,len(data_train)-7):\n    X_train.append(data_train[i-21:i])\n    y_train.append(data_train[i:i+7])\n    ","540e9849":"y_train[0]","979274d8":"X_train,y_train = np.array(X_train),np.array(y_train)","aa7deda4":"print(X_train.shape,y_train.shape)","da4f76b7":"X_train[0]","3e31998e":"### Scaling down the dataset \nfrom sklearn.preprocessing import MinMaxScaler\nscaler_x = MinMaxScaler()\nX_train = scaler_x.fit_transform(X_train)","f9cdc693":"scaler_y = MinMaxScaler()\ny_train = scaler_y.fit_transform(y_train)","5443f8b0":"# converting into L.S.T.M format\nX_train = X_train.reshape(X_train.shape[0],X_train.shape[1],1)\n\n\n# here our y_train is not in 3D structure \ny_train = y_train.reshape(y_train.shape[0],y_train.shape[1])\n","321e757c":"print(X_train.shape,y_train.shape)","66104736":"## we would use Xval y_val in our training process to check our error over epochs\nX_valid = X_train[:10]\ny_valid = y_train[:10]\nX_train = X_train[10:]\ny_train = y_train[10:]","21713f08":"print(X_valid.shape)","898811e0":"y_train.shape","85ad9902":"# now creating Stacked unidirectional L.S.T.M\nfrom keras import Sequential\nfrom keras.layers import Dense, LSTM\n\nmodel_uni = Sequential()\nmodel_uni.add(LSTM(200, return_sequences= True, activation='relu', input_shape=(X_train.shape[1], 1)))\nmodel_uni.add(LSTM(150))\nmodel_uni.add(Dense(7))\n\nprint(model_uni.summary())\nmodel_uni.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n","d2d3dce6":"# Creating a Bi-directional stacked L.S.T.M\nnos_of_features = 1\nnos_of_timesteps = 21 # X_train.shape[1]\nfrom keras.layers import Bidirectional\nmodel_bi = Sequential()\nmodel_bi.add(Bidirectional(LSTM(200, activation='relu', return_sequences=True), input_shape=(nos_of_timesteps,nos_of_features)))\nmodel_bi.add(LSTM(150))\nmodel_bi.add(Dense(7))\nmodel_bi.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n\nprint(model_bi.summary())","abe8be18":"\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\ncallbacks = [\n    EarlyStopping(patience=20, verbose=1),\n    ReduceLROnPlateau(factor=0.1, patience=3, min_lr=0.00001, verbose=1),\n    ModelCheckpoint('model_uni.h5', verbose=1, save_best_only=True)\n]\n\n\n\nhistory_uni = model_uni.fit(X_train, y_train, epochs=100,validation_data=(X_valid,y_valid), batch_size=32, callbacks=callbacks)\n","fe6ed570":"plt.plot(history_uni.history['val_loss'])\nplt.legend(['val_loss'])\nplt.title('Unidirectional L.S.T.M')\nplt.show()","2b83f5c5":"#### Loading the best model with lowest val_loss\nfrom tensorflow import keras\nmodel_uni = keras.models.load_model('\/kaggle\/working\/model_uni.h5')\nmodel_uni.summary()","c73ecfd3":"callbacks = [\n    EarlyStopping(patience=20, verbose=1),\n    ReduceLROnPlateau(factor=0.1, patience=3, min_lr=0.00001, verbose=1),\n    ModelCheckpoint('model_bi.h5', verbose=1, save_best_only=True)\n]\n\n\n\n\nhistory_bi = model_uni.fit(X_train, y_train, epochs=100,validation_data=(X_valid,y_valid), batch_size=32, callbacks=callbacks)\n","78261829":"plt.plot(history_bi.history['val_loss'])\nplt.title('Bidirectional L.S.T.M')\nplt.show()\n# here we can see our best model we got at epochs =60","9ce31a66":"#### Loading the best model with lowest val_loss\nmodel_bi = keras.models.load_model('\/kaggle\/working\/model_bi.h5')\nmodel_bi.summary()","c3d2d504":"X_test,y_test =[],[]\n\nfor i in range(21,len(data_test)-7):\n    X_test.append(data_test[i-21:i])\n    y_test.append(data_test[i:i+7])\n    ","1bbef524":"X_test,y_test = np.array(X_test),np.array(y_test)","33140e4f":"X_test","855688d7":"X_test = scaler_x.transform(X_test)\n","630791d0":"y_test.shape","7b0ca68c":"X_test = X_test.reshape(X_test.shape[0],X_test.shape[1],1)\n","1dcdf4ce":"X_test.shape","320fe235":"y_train.shape","51032359":"y_pred_uni = model_uni.predict((X_test))\ny_pred_uni.shape","5d8aa320":"y_pred_bi = model_bi.predict((X_test))\ny_pred_bi.shape","e7e7eee1":"prediction_uni = scaler_y.inverse_transform(y_pred_uni)\nprediction_bi = scaler_y.inverse_transform(y_pred_bi)","0ebb2f50":"y_test[:,0]","aaf0573c":"# Plotting our result\nfig, ax = plt.subplots(figsize = (18,10))\nplt.plot(y_test[:,1])\nplt.plot(prediction_uni[:,1])\nplt.title('Unidirectional L.S.T.M')\nplt.show()\n\n","4be74581":"# Plotting our result\nfig, ax = plt.subplots(figsize = (18,10))\nplt.plot(y_test[:,1])\nplt.plot(prediction_bi[:,1])\nplt.title('Bidirectional L.S.T.M')\nplt.show()\n","a7cd078d":"from sklearn import metrics\ndef evaluate_performace(y_true,y_predicted):\n    scores = []\n    # calcualte scores per day\n    for i in range(y_true.shape[1]):\n        mse = metrics.mean_squared_error(y_true[:,1],y_predicted[:,1])\n        rmse = np.sqrt(mse)\n        scores.append(rmse)\n        \n    # calculate score for whole prediction\n    total_score = 0\n    for row in range(y_true.shape[0]):\n        for col in range(y_predicted.shape[1]):\n            total_score = total_score + (y_true[row,col] - y_predicted[row,col])**2\n    total_score = np.sqrt(total_score\/(y_true.shape[0]*y_predicted.shape[1]))\n    return(total_score,scores)        ","449b293a":"print(\"rmse in bidirectional stacked lstm\",evaluate_performace(y_test,prediction_bi)[0])","3b277cbb":"print(\"std deviation in y_true\",np.std(y_test[:,0]))","e1d3a66d":"print(\"rmse in unidirectional stacked lstm\",evaluate_performace(y_test,prediction_uni)[0])","84cc907d":"print(X_train.shape)\nprint(y_train.shape)\n# here we are taking input of 21 days and predicting 7 days\nsteps_in = 21\nsteps_out = 7","6fd3a4e1":"# using sequential keras \nfrom keras.layers import RepeatVector\nfrom keras.layers import TimeDistributed\n\nmodel_endec = Sequential()\n\n# encoder layer\nmodel_endec.add(LSTM(200, activation='relu',return_sequences=True, input_shape=(steps_in, 1)))\nmodel_endec.add(LSTM(200, activation='relu'))\n\n# repeat vector \nmodel_endec.add(RepeatVector(steps_out))\n\n# decoder layer\nmodel_endec.add(LSTM(200, activation='relu',return_sequences=True))\nmodel_endec.add(LSTM(200, activation='relu',return_sequences=True))\nmodel_endec.add(TimeDistributed(Dense(1)))\nmodel_endec.compile(optimizer='adam', loss='mse')\n\nprint(model_endec.summary())","f4c47391":"\n\ncallbacks = [\n    EarlyStopping(patience=20, verbose=1),\n    ReduceLROnPlateau(factor=0.1, patience=3, min_lr=0.00001, verbose=1),\n    ModelCheckpoint('model_endec.h5', verbose=1, save_best_only=True)\n]\n\n\n\nhistory_endec = model_endec.fit(X_train, y_train, epochs=100,validation_data=(X_valid,y_valid), batch_size=32, callbacks=callbacks)\n","88d57d87":"plt.plot(history_endec.history['val_loss'])\nplt.plot(history_endec.history['loss'])\nplt.legend(['val_loss','loss'])\nplt.show()","6322f948":"#### Loading the best model with lowest val_loss\nmodel_endec = keras.models.load_model('\/kaggle\/working\/model_endec.h5')\nmodel_endec.summary()","d3a4fdc7":"y_pred_endec = model_endec.predict((X_test))\n","fce102ee":"prediction_endec = scaler_y.inverse_transform(y_pred_endec.reshape(y_test.shape[0],y_test.shape[1]))","3b62ec39":"# lets do some plotting of result\n# Plotting our result\nfig, ax = plt.subplots(figsize = (18,8))\nplt.plot(y_test[:,1])\nplt.plot(prediction_endec[:,1])\nplt.title('Encoder decoder L.S.T.M')\nplt.show()\n\n\n","b490d396":"print('R.M.S.E of encoder-decoder:',evaluate_performace(y_test,prediction_endec)[0])","e801c1de":"np.std(y_test[:,0])","9bd01009":"#### here we can say that our model is good because the standard deviation is more than the root mean squared error","4a4a3f0d":"### Parsing the index into date-time \n","ffdb4e11":"## Bidirectional L.S.T.M\nOn some sequence prediction problems, it can be beneficial to allow the LSTM model to learn the input sequence both forward and backwards and concatenate both interpretations.z\n\nThis is called a Bidirectional LSTM.\n\nWe can implement a Bidirectional LSTM for univariate time series forecasting by wrapping the first hidden layer in a wrapper layer called Bidirectional.\n\n* Let us take an example of missing word generation in the I am ___ student. \n \n* Unidirectional LSTMs will use only \u2018I am\u2019 to generate next word and based on the example it has seen during training it will generate a new word (it may be \u2018a\u2019, \u2018very\u2019 etc.). But bidirectional LSTMs have information of past (I am) and future (student), so it can easily see that here it has to be a. It\u2019s a very poor example but explain the context clearly. :)","3e2343a0":"## 2.Building Encoder-Decoder model\n model specifically developed for forecasting variable length output sequences is called the Encoder-Decoder LSTM.\n\nThe model was designed for prediction problems where there are both input and output sequences, so-called sequence-to-sequence, or seq2seq problems, such as translating text from one language to another.\n\nThe encoder is a model responsible for reading and interpreting the input sequence. The output of the encoder is a fixed length vector that represents the model\u2019s interpretation of the sequence. The encoder is traditionally a Vanilla LSTM model, although other encoder models can be used such as Stacked, Bidirectional, and CNN models.\n\nThe decoder uses the output of the encoder as an input.\n\n","d0a38899":"#### we always need to shape X_input into a 3D dimension for L.S.T.M and our ouput will be in the shape of (timesteps, nos_of_features)\n\n### this particular architecture is called as Vector Output Model","f78deb2c":"### here takeaway\nEncoder decoder model performs well than vector output model for multivariate time forecasting","c3d532f6":"now we can predict\n* Forecast hourly power consumption for the next day\n* Forecast daily power consumption for the next week \n* Forecast weekly power consumption for the next month\n* Forecast monthly power consumption for the next year\n\ninput-frequency -----> output\n\n[week1]  ------> next week\n\n[monthly] --------> next month\n\n[yearly] ------------> next year\n\n[daily] ---------> next day\n\n\nwe can also do multistep ahead prediction \n[input week1] ----> week2 -----> week3 ---- >","cf2854a3":"#### Defining some callbacks","ca09bb0d":"### Now lets prepare the test Data","4b2e7e6b":"## 1.Building the L.S.T.M model (Vector Output Model)","f043164c":"#### Lets do some EDA ","b595002b":"### Lets prepare the data for L.S.T.M format  (Multivariate lstm model)\ntaking lookback as 14 days we will predict upcoming 5 days\nas the amount of data in our prediction series increases model tends to make more mistakes so we will only predicting 5 days rather than 10 days\n\n* as we increase the amount of data in our training input like 21 days in this case model becomes more good in prediction\n\n* 7 days forecasting will  have more error than 3 days forecasting in future prediction","6e7c9cb4":"### here in this notebook we will do multivariate time series forecasting on HouseHold Electricity Consumption\n## Multivariate LSTM Models & Multi step prediction\n* ### 1.Vector output model & Bidirectional Vector output model\n* ### 2. Encoder Decoder model\n\nMultivariate time series data means data where there is more than one observation for each time step.\n\nThere are two main models that we may require with multivariate time series data; they are:\n\nMultiple Input Series.\nMultiple Parallel Series.\nLet\u2019s take a look at each in turn.\n\nBy-Abhishek Jaiswal\n","a4aaad64":"### Per year data Visualisation (Global active power)","8b8984b7":"#### Repeat Vector\nrepeat vector takes the output from encoder and feeds it repeatedly as input at each time-step to the decoder. For instance, in the output we have three time-steps. To predict each output time-step, the decoder will use the value from the repeat vector, the hidden state from the previous output and the current input.\n#### Time distributed \nWe can use the same output layer or layers to make each one-step prediction in the output sequence. This will predict our output one by one ","1d6dd6a2":"### Plotting power consumption per month in 2007","69b8bfd6":"Abstract:\u00b6\nIn this Notebook, I try to learn and build the Long Short-Term Memory (LSTM) recurrent neural network to fit one third of data and then predict the rest of data.\n\nDatabase information:\n\n(1) date: Date in format dd\/mm\/yyyy\n\n(2) time: time in format hh:mm:ss\n\n(3) global_active_power: household global minute-averaged active power (in kilowatt)\n\n(4) global_reactive_power: household global minute-averaged reactive power (in kilowatt)\n\n(5) voltage: minute-averaged voltage (in volt)\n\n(6) global_intensity: household global minute-averaged current intensity (in ampere)\n\n(7) sub_metering_1: energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the kitchen, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered).\n\n(8) sub_metering_2: energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the laundry room, containing a washing-machine, a tumble-drier, a refrigerator and a light.\n\n(9) sub_metering_3: energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an electric water-heater and an air-conditioner."}}