{"cell_type":{"b4e08a58":"code","fb9f5254":"code","af8313ce":"code","1eb6a7be":"code","79d4bd69":"code","933f3b77":"code","b1eef4d9":"code","f2d7f014":"code","51caeca8":"code","4402f25d":"code","c5e6dcfe":"code","a2a85362":"code","9f8f0181":"code","01df1e1e":"code","d3a672f9":"code","d553d2c3":"code","15dc4882":"code","d0f6802c":"code","4c56e521":"code","a298aede":"code","a3adebfe":"code","349df5d1":"code","d16743ac":"code","8acc2dde":"code","12badad5":"code","2b662a72":"code","584cf4d7":"code","ff962dec":"code","9f26e53e":"code","e1222dce":"code","747c4e7e":"code","2481faec":"code","9441dd24":"code","f75710b6":"code","5c48361d":"code","e33e0dcf":"code","b0f8ad11":"code","23d25d5e":"code","05165fac":"code","bb32f282":"code","ee5b101e":"code","cf2a3953":"code","54f55436":"code","d0155f69":"code","e7b2a255":"code","fe1ade77":"code","3caeb121":"code","39078dc0":"code","a6ac682b":"code","8bf2ce3c":"code","07b04f15":"code","a84106c0":"code","0fb19191":"code","89ae8337":"code","6bca0336":"code","48f577e4":"code","74e90a02":"code","cbc5712b":"code","2cfd48f9":"code","c4dc4f31":"code","736ae86f":"code","78ea14cf":"code","124c0bd9":"code","3b7f2b98":"code","643c0892":"code","f5472ecc":"code","93b714c4":"code","8818aa0a":"code","6537e785":"code","d6689003":"code","0d98311a":"code","d08fced1":"code","9b017073":"code","9a61fa4c":"code","f144937c":"code","c5cd2351":"code","e8c98eab":"code","f711b711":"code","e96b616c":"code","e89d1bcd":"code","14e3a268":"code","dfeaa845":"code","a1c83bb1":"code","123fdc47":"code","f2daaf28":"code","bff2d8a5":"code","f5798acb":"code","c840573a":"code","b4732e2a":"code","ac92359e":"code","03b994f5":"code","dd04e2e9":"code","171d1b25":"code","783d724f":"code","0bc26605":"code","796fdd72":"code","3b2114f8":"code","825763a3":"code","9c180ebb":"code","a90aa45f":"code","fba52310":"code","6edf5405":"code","d5d48f67":"markdown","d6bad14d":"markdown","caa7082a":"markdown","0768290b":"markdown","d6e78010":"markdown","380f005e":"markdown","0607f439":"markdown","a3f2d208":"markdown","186402b6":"markdown","c63a5f40":"markdown","ccb04e13":"markdown","f3603311":"markdown","a87eab0e":"markdown","4378b43a":"markdown","d35e3107":"markdown","6eedc767":"markdown","03dfc88e":"markdown","77eaf233":"markdown","00527431":"markdown","ccb3cba7":"markdown","fe214417":"markdown","eeee8992":"markdown","dc9dd10d":"markdown","08b10877":"markdown","005debff":"markdown","4f64a33d":"markdown","d66daa9d":"markdown","2c68b9be":"markdown","9c98dde0":"markdown","de9eb78c":"markdown","3f6c9458":"markdown","d60eb8bf":"markdown","e9398ca1":"markdown","dd890b71":"markdown","0af137da":"markdown","5cf3df6c":"markdown","eec25e58":"markdown","366c9367":"markdown","05ed4fa6":"markdown","bd24affb":"markdown","ca48402f":"markdown","94b4401a":"markdown","cb7c2a36":"markdown","5053fc82":"markdown","debcdc38":"markdown","b8b2fe7a":"markdown","1407ff2f":"markdown","67789b85":"markdown","408edc07":"markdown","3f6c0539":"markdown","842213df":"markdown","e81180bd":"markdown","3e8f335d":"markdown","8f79d1c9":"markdown","49ee8a7f":"markdown","8d47cc96":"markdown","98f586b0":"markdown","b62dfcab":"markdown","d667748a":"markdown","4d07743b":"markdown","94fdc5ec":"markdown","bb1aa417":"markdown","e4e66d12":"markdown","6a2a8e5b":"markdown","184d19d8":"markdown","2bcae363":"markdown","fc5513f8":"markdown","b512f4e3":"markdown","c78cba3e":"markdown","01b462ef":"markdown","452d68e8":"markdown","b29f5827":"markdown","99d79b73":"markdown","72be69a7":"markdown","2840adaa":"markdown","5d16844c":"markdown"},"source":{"b4e08a58":"import pandas as pd\ndf=pd.read_csv('..\/input\/titanic\/train.csv')\ndf1=pd.read_csv('..\/input\/titanic\/test.csv')\n\ndf","fb9f5254":"df1","af8313ce":"df.isnull().sum()","1eb6a7be":"df1.isnull().sum()","79d4bd69":"for n in df['Name']:\n  if n[-5:] == 'James':\n    print(n)","933f3b77":"for n in df1['Name']:\n  if n[-5:] == 'James':\n    print(n)","b1eef4d9":"first_name=pd.DataFrame()\nlast_name=pd.DataFrame()\n\nfor a in df['Name']:\n  b,c = a.split(',')\n  first_name=first_name.append([b])\n  last_name=last_name.append([c])","f2d7f014":"first_name","51caeca8":"last_name","4402f25d":"first_name1=pd.DataFrame()\nlast_name1=pd.DataFrame()\n\nfor a in df1['Name']:\n  b,c = a.split(',')\n  first_name1=first_name1.append([b])\n  last_name1=last_name1.append([c])","c5e6dcfe":"first_name1","a2a85362":"last_name1","9f8f0181":"for aa in first_name1[0].unique():\n  if (aa in first_name[0].unique()) == True:\n    print(aa)","01df1e1e":"for aa in first_name1[0].unique():\n  if (aa in first_name[0].unique()) == False:\n    print(aa)","d3a672f9":"for aa in last_name1[0].unique():\n  if (aa in last_name[0].unique()) == True:\n    print(aa)","d553d2c3":"for aa in last_name1[0].unique():\n  if (aa in last_name[0].unique()) == False:\n    print(aa)","15dc4882":"last=pd.DataFrame()\nname=pd.DataFrame()\n\nlast1=pd.DataFrame()\nname1=pd.DataFrame()\n\nfor i in last_name[0]:\n  l,n = i.split('.',maxsplit=1)\n  last=last.append([l])\n  name=name.append([n])\n\nfor i in last_name1[0]:\n  l,n = i.split('.',maxsplit=1)\n  last1=last1.append([l])\n  name1=name1.append([n])","d0f6802c":"for aa in last1[0].unique():\n  if (aa in last[0].unique()) == True:\n    print(aa)","4c56e521":"for aa in last1[0].unique():\n  if (aa in last[0].unique()) == False:\n    print(aa)","a298aede":"last[0].unique()","a3adebfe":"last1[0].unique()","349df5d1":"last=last.reset_index().drop(['index'],axis=1)\nlast.columns=['reference']\n\nlast1=last1.reset_index().drop(['index'],axis=1)\nlast1.columns=['reference']","d16743ac":"df['Name'] = last['reference']\ndf","8acc2dde":"df1['Name'] = last1['reference']\ndf1","12badad5":"train_y=df['Survived']\ndf=df.drop(['Survived'],axis=1)\ndf","2b662a72":"train_test=pd.concat([df,df1],axis=0)\ntrain_test","584cf4d7":"train_test.isnull().sum()","ff962dec":"train_test['Ticket'].unique()","9f26e53e":"ticket_number=pd.DataFrame()\nticket_name = pd.DataFrame()\nfor i in train_test['Ticket']:\n  a = i.split(' ',maxsplit=-1)\n  if len(a) == 1:\n    ticket_number=ticket_number.append([a[-1]])\n    ticket_name=ticket_name.append([0])\n  else:\n    ticket_number=ticket_number.append([a[-1]])\n    ticket_name=ticket_name.append([1])","e1222dce":"ticket_number","747c4e7e":"ticket_number[0]=ticket_number.replace('LINE', 0)\nticket_number","2481faec":"ticket_name","9441dd24":"ticket_number=ticket_number.reset_index().drop(['index'],axis=1)\nticket_name=ticket_name.reset_index().drop(['index'],axis=1)","f75710b6":"ticket_number","5c48361d":"ticket_name","e33e0dcf":"train_test","b0f8ad11":"train_test=train_test.reset_index().drop(['index'],axis=1)","23d25d5e":"train_test['Ticket']=ticket_name\ntrain_test['Ticket_number']=ticket_number\ntrain_test","05165fac":"import seaborn as sns\nsns.countplot(last['reference'])","bb32f282":"sns.countplot(last1['reference'])","ee5b101e":"train_test['Name'].value_counts()","cf2a3953":"train_test['Name']=train_test['Name'].replace(' Mr', 1)\ntrain_test['Name']=train_test['Name'].replace(' Mrs', 2)\ntrain_test['Name']=train_test['Name'].replace(' Miss', 3)\ntrain_test['Name']=train_test['Name'].replace(' Master', 4)\ntrain_test['Name']=train_test['Name'].replace(' Ms', 0)\ntrain_test['Name']=train_test['Name'].replace(' Col', 0)\ntrain_test['Name']=train_test['Name'].replace(' Rev', 0)\ntrain_test['Name']=train_test['Name'].replace(' Dr', 0)\ntrain_test['Name']=train_test['Name'].replace(' Dona', 0)\ntrain_test['Name']=train_test['Name'].replace(' Mlle', 0)\ntrain_test['Name']=train_test['Name'].replace(' Major', 0)\ntrain_test['Name']=train_test['Name'].replace(' Lady', 0)\ntrain_test['Name']=train_test['Name'].replace(' Don', 0)\ntrain_test['Name']=train_test['Name'].replace(' Mme', 0)\ntrain_test['Name']=train_test['Name'].replace(' the Countess', 0)\ntrain_test['Name']=train_test['Name'].replace(' Capt', 0)\ntrain_test['Name']=train_test['Name'].replace(' Sir', 0)\ntrain_test['Name']=train_test['Name'].replace(' Jonkheer', 0)","54f55436":"sns.countplot(train_test['Name'])","d0155f69":"train_test","e7b2a255":"sns.countplot(train_test['Sex'])","fe1ade77":"train_test['Sex']=train_test['Sex'].replace('male', 0)\ntrain_test['Sex']=train_test['Sex'].replace('female', 1)\ntrain_test","3caeb121":"train_test['Cabin'].value_counts()","39078dc0":"train_test[train_test['Cabin'] == train_test['Cabin'].value_counts().index[0]]","a6ac682b":"train_test[train_test['Cabin'] == train_test['Cabin'].value_counts().index[1]]","8bf2ce3c":"train_test[train_test['Cabin'] == train_test['Cabin'].value_counts().index[2]]","07b04f15":"train_test=train_test.reset_index().drop(['index'],axis=1)\ntrain_test","a84106c0":"import numpy as np\ntrain_test[train_test['Embarked'].isnull() == True]","0fb19191":"train_test['Embarked'].unique()","89ae8337":"train_test['Embarked']=train_test['Embarked'].replace('S',0)\ntrain_test['Embarked']=train_test['Embarked'].replace('C',1)\ntrain_test['Embarked']=train_test['Embarked'].replace('Q',2)\ntrain_test","6bca0336":"imputation_x=train_test[['Pclass','Name','Sex','SibSp','Parch','Ticket','Embarked','Ticket_number']]\nimputation_x","48f577e4":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nscale_imputation_x=pd.DataFrame(scaler.fit_transform(imputation_x.drop(['Embarked'],axis=1)))\nscale_imputation_x.columns=imputation_x.drop(['Embarked'],axis=1).columns\nscale_imputation_x","74e90a02":"k=3\n\nfrom sklearn.cluster import KMeans\n\nkmeans=KMeans(n_clusters=k,algorithm='full')\nkmean_result=kmeans.fit_transform(scale_imputation_x)\nkmean_result","cbc5712b":"idx_61=np.argmin(kmean_result[61])\nidx_829=np.argmin(kmean_result[829])","2cfd48f9":"print(idx_61)\nprint(idx_829)","c4dc4f31":"pd.DataFrame(np.argmin(kmean_result,axis=1))[pd.DataFrame(np.argmin(kmean_result,axis=1))[0] == 0].index","736ae86f":"sns.countplot(train_test['Embarked'].iloc[pd.DataFrame(np.argmin(kmean_result,axis=1))[pd.DataFrame(np.argmin(kmean_result,axis=1))[0] == 0].index])","78ea14cf":"train_test['Embarked']=train_test['Embarked'].fillna(0)\ntrain_test.isnull().sum()","124c0bd9":"train_test['Age']","3b7f2b98":"from sklearn.linear_model import LinearRegression\n\nage_x = train_test[['Pclass','Name','Sex','SibSp','Parch','Ticket','Embarked','Ticket_number']]\n\nage_y = pd.DataFrame(train_test['Age'])\n\nage_train_y = age_y[age_y['Age'].isnull()==False]\nage_test_y = age_y[age_y['Age'].isnull()==True]\nage_test_x = age_x.iloc[age_test_y.index]\nage_train_x = age_x.iloc[age_train_y.index]\nage_model=LinearRegression()\nage_model.fit(age_train_x, age_train_y)","643c0892":"from sklearn.metrics import r2_score\nr2_score(age_train_y, age_model.predict(age_train_x))","f5472ecc":"train_test","93b714c4":"sns.distplot(train_test['Age'])","8818aa0a":"print(train_test['Age'].min())\nprint(train_test['Age'].max())","6537e785":"age=pd.DataFrame()\n\nfor i in train_test['Age']:\n  i = str(i)[0]\n  age=age.append([i])\nage","d6689003":"age=age.reset_index().drop(['index'],axis=1)\nage.columns=['Age']\nage","0d98311a":"age=age.replace('n',np.NaN)\nage_x = train_test[['Pclass','Name','Sex','SibSp','Parch','Ticket','Embarked','Ticket_number']]\nscaler=StandardScaler()\nage_xx=pd.DataFrame(scaler.fit_transform(age_x))\nage_xx.columns=age_x.columns\n\n\n\nage_y = pd.DataFrame(age['Age'])\n\nage_train_y = age_y[age_y['Age'].isnull()==False]\nage_test_y = age_y[age_y['Age'].isnull()==True]\nage_test_x = age_xx.iloc[age_test_y.index]\nage_train_x = age_xx.iloc[age_train_y.index]","d08fced1":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nmodel = LogisticRegression()\nmodel.fit(age_train_x, age_train_y)\naccuracy_score(age_train_y, model.predict(age_train_x))","9b017073":"train_test['Age']=age['Age']\ntrain_test['Age']=train_test['Age'].fillna(-1)\ntrain_test","9a61fa4c":"sns.distplot(train_test['Fare'])","f144937c":"train_test[train_test['Fare'].isnull()==True]","c5cd2351":"train_test[train_test['Ticket_number'] == '3701']","e8c98eab":"fare_imputation=train_test[(train_test['Pclass'] == 3) & (train_test['Embarked'] == 0)]['Fare'].mean()\nfare_imputation","f711b711":"train_test['Fare']=train_test['Fare'].fillna(fare_imputation)","e96b616c":"train_test['Ticket_number']","e89d1bcd":"train_test[train_test['Ticket_number'] == '21171']","14e3a268":"train_test[train_test['Ticket_number'] == '17599']","dfeaa845":"train_test[train_test['Ticket_number'] == '3101282']","a1c83bb1":"train_test[train_test['Ticket_number'] == '113803']","123fdc47":"train_test[train_test['Cabin'].isnull()== True]['Ticket_number']","f2daaf28":"for i in pd.DataFrame(train_test.groupby(by=['Ticket','Ticket_number']))[1]:\n  print(i)","bff2d8a5":"train_test","f5798acb":"for i in train_test['Cabin']:\n  i = str(i).split(' ')\n  print(i)","c840573a":"cabin=pd.DataFrame()\ntrain_test['Cabin']=train_test['Cabin'].fillna(-1)\nfor i in train_test['Cabin']:\n  if i != -1:\n    i = str(i).split(' ')\n    cabin=cabin.append([i[0][0]])\n  else:\n    cabin=cabin.append([i])\ncabin","b4732e2a":"cabin.value_counts()","ac92359e":"cabin=cabin.replace('C', 0)\ncabin=cabin.replace('B', 1)\ncabin=cabin.replace('D', 2)\ncabin=cabin.replace('E', 3)\ncabin=cabin.replace('A', 4)\ncabin=cabin.replace('F', 5)\ncabin=cabin.replace('G', 6)\ncabin=cabin.replace('T', 7)\ncabin","03b994f5":"cabin.value_counts()","dd04e2e9":"cabin=cabin.reset_index().drop(['index'],axis=1)\ncabin.columns=['Cabin']\ncabin","171d1b25":"train_test['Cabin'] = cabin['Cabin']\ntrain_test","783d724f":"train_test.isnull().sum()","0bc26605":"from sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\n\nscale_data = pd.DataFrame(scaler.fit_transform(train_test.drop(['PassengerId'],axis=1)))\nscale_data.columns=[train_test.drop(['PassengerId'],axis=1).columns]\ntrain_data=scale_data.iloc[:891]\ntest_data=scale_data.iloc[891:]","796fdd72":"train_data","3b2114f8":"test_data","825763a3":"from sklearn.model_selection import train_test_split\n\ntrain_x, val_x, train_y ,val_y = train_test_split(train_data, train_y, test_size=0.4,random_state=2021)\n\ntrain_x=train_x.reset_index().drop(['index'],axis=1)\nval_x=val_x.reset_index().drop(['index'],axis=1)\ntest_data=test_data.reset_index().drop(['index'],axis=1)\n\ntrain_y=pd.DataFrame(train_y).reset_index().drop(['index'],axis=1)\nval_y=pd.DataFrame(val_y).reset_index().drop(['index'],axis=1)","9c180ebb":"train_x","a90aa45f":"val_x","fba52310":"# Load tensorflow for DL.\nimport tensorflow as tf\n\n\n# Set your model with Sequential\nmodel=tf.keras.models.Sequential(\n    # Flatten is used to input your train data.\n    [tf.keras.layers.Flatten(input_shape = [train_x.shape[1]]),\n     # Stack your hidden layers with some neurons.\n     # If you don't know what you should use the activation function, ReLU function will be helpful.\n     # Practice your deep learning by changing some neurons in each layer and try to improve your score iterately.\n     tf.keras.layers.Dense(1500,activation='selu',kernel_initializer='lecun_normal'),\n     tf.keras.layers.Dense(1500, activation='selu',kernel_initializer='lecun_normal'),\n     tf.keras.layers.Dense(1500, activation='selu',kernel_initializer='lecun_normal'),\n     tf.keras.layers.Dense(1500, activation='selu',kernel_initializer='lecun_normal'),\n     tf.keras.layers.Dense(1500, activation='selu',kernel_initializer='lecun_normal'),\n     tf.keras.layers.Dense(1500, activation='selu',kernel_initializer='lecun_normal'),\n     # Dropout is used to regularize and prevent the model from overfitting.\n     tf.keras.layers.Dropout(0.2),\n     # This analysis is for classification. So, you have to use the number of neuron as 1 in output layer.\n     # In this case, the activation function in output layer will be more powerful to use sigmoid than relu.\n     tf.keras.layers.Dense(1,activation='sigmoid')]\n)\n\n\n# # Set your learning schedule for get global optimum point.\n# lr = tf.keras.optimizers.schedules.ExponentialDecay(\n#     initial_learning_rate=2e-3,\n#     decay_steps=10000,\n#     decay_rate=0.95\n# )\n\n# Set your optimizer as Adagrad or SGD or extra...\nopt = tf.keras.optimizers.Adam(learning_rate = 0.003)\n\n# Compile your model with prepared optimizer, loss function for classification and metrics for monitoring.\nmodel.compile(optimizer = opt, loss='binary_crossentropy', metrics = ['accuracy'])\n\n# Fit your train dataset and validation dataset.\n# You can practice this model by changing some hyper parameters such as epochs, batch_size, and so on into another values.\n# EarlyStopping is used for preventing the training from overfitting.\n# patience is related to your epochs and save your best model by setting restore_best_weights.\nmodel.fit(train_x, train_y, batch_size = 8, epochs=200, verbose=8, validation_split=0.2, workers=3, validation_batch_size = 5, \n          callbacks = tf.keras.callbacks.EarlyStopping(monitor= 'val_loss', patience=200 * 0.1, verbose=2, restore_best_weights=True))\n\n# validate your model performance\n# It will be represented loss value and accuracy value, repectively.\nmodel.evaluate(val_x, val_y)","6edf5405":"# Load the submission form.\nsubmission=pd.read_csv('..\/input\/titanic\/gender_submission.csv')\n\n# We will input our prediction into this data frame.\nprediction=pd.DataFrame()\n\n# DL will show the probability about classification. So, we can classify the survival class or no along with the probability (your prediction)\nfor i in model.predict(test_data):\n    # If the probability is greater than 0.5, this observation is regared as survived.\n    # In this case, the decision boundary is 0.5.\n  if i >=0.5:\n    i=1\n    prediction=prediction.append([i])\n    # If it is not that, this observation is regared as dead.\n  else:\n    i=0\n    prediction=prediction.append([i])\nprediction=prediction.reset_index().drop(['index'],axis=1)\nprediction.columns=['Survived']\n\n\n# Finish your submission.\nsubmission['Survived'] = prediction['Survived']\n\n# Submit your result.\nsubmission.to_csv('submission.csv',index=False)","d5d48f67":"## Test dataset contains the non values in Age, Cabin, and Fare, not Embarked.","d6bad14d":"## Now, you are ready to build your model to predict the survival.","caa7082a":"## Most values of this group represent the 0 values, that is, S.\n\nSo, we convet non value into 0.","0768290b":"## Ticket_number and ticket_name reindexing","d6e78010":"# Read me\n\nThis notebook is inline with https:\/\/www.kaggle.com\/pythonash\/how-to-handle-raw-dataset-and-analyze-with-dl.\n\nIf you understand my data handling procedure, you can skip below stages (see directly 'Deep learning for survival classification' in contents).\n\nThe used model consists of selu activation function and several identical neurons in each hidden layer.\n\nFrom this notebook, you can see the performance of identical and deep structure model with selu activation function.\n\nFor using selu as an activation function, you have to prepare as below:\n\n1. The input variables should be standarded (using StandaradScaler in sklearn).\n\n2. The network should be fully-connected structure.\n\n3. The weights in hidden layer should be initialized with Lecun normal distribution (using kernel_initializer = 'lecun_normal')\n\nLet's start!","380f005e":"## We'll devide the ticket value into ticket number and ticket name.\n\nIf ticket name has 1 value, the ticket was combined with stirng and numbering.\n\nOtherwise, if ticket name is 0 value, the ticket was only numeric ticket.","0607f439":"## Find the relationships between train dataset names and test's one.\n\nWhethere the name (first and last) in test dataset is in train dataset or not?\n\n> first_name1 - first name from test dataset\n\n> first_name = first name from train dataset\n\n> last_name1 = last name from test dataset\n\n> last_name = last name from train dataset","a3f2d208":"## We will covert male into 0 and female into 1","186402b6":"## Estimate the null values of Age with logistic regression","c63a5f40":"## Identify the distribution from count plot\n\n> train dataset","ccb04e13":"## Converting the Embarked values into numeric.","f3603311":"## So, we will imputate the null values by deviding the age values into group age.\n\nAnd we try to estimate the null values using with logistic regression ( with softmax).","a87eab0e":"## The min, max values","4378b43a":"## LINE, the value in ticket column, must be coverted another value like 0 which represents only LINE as 0.\n\nSo, we can write the code as in:","d35e3107":"## Insert the pre-processed cabin data into train_test dataset.","6eedc767":"## Name data pre-processing\n\nWe will convert name value such as 'Mr','Miss', ... into 1,2,3... as in:**\uad75\uc740 \ud14d\uc2a4\ud2b8**","03dfc88e":"## Countplot of Name","77eaf233":"## Dona is in test but, not in train.","00527431":"## check the test dataset","ccb3cba7":"## The optimal cluster is 3.","fe214417":"## Same kinds of ticket mean same fare, cabin, and embarked.\n","eeee8992":"## Apply to test dataset with same method","dc9dd10d":"## Cabin has many information and plenty of clues.\n\nIf cabin is same each other, the passengers might be a family or couple.\n\nSo, we could find the clues about the null value.","08b10877":"## There are only a few values except for 'Mr', 'Miss',' Mrs', and 'Master'.\n\nSo, we will convert these four values (Mr, Miss, Mrs, and Master) into numeric and the others will be clustered in one group.","005debff":"# Prepare your submission","4f64a33d":"## Cabin pre-processing.\n\nCabin has high probability that is related to Ticket.","d66daa9d":"## However, many cases show that we couldn't find any clues about null values of Cabin. Accordingly, we will replace the null values to -1.\n\n-1 means that the value is null value of Cabin.\n","2c68b9be":"## Sex pre-processing","9c98dde0":"## The case of Cabin value.\n\n> A passenger has a couple of seats\n\n> Unknown seat type\n\n> Unknown seat number\n\nHence, if we don't know the value, we will replace as -1, otherwise, we will take only the initial of seat as below:","de9eb78c":"## By grouping the ticket, ticket number values, we try to find the clues for null values for Cabin.","3f6c9458":"Fare distribution","d60eb8bf":"## Cabin pre-processing","e9398ca1":"## Age distribution","dd890b71":"## Find whether a reference in test data is in train or not.\n\nMr, Mrs, Miss, Master, Ms, Col, Rev, and Dr are in train and test.","0af137da":"## We will estimate the null values of Embarked by cluster analysis.","5cf3df6c":"## PassengerId 892 is the start point to test dataset.\n","eec25e58":"## First, split the names into fist name, last name.","366c9367":"## Convert the values into 0, 1, 2....","05ed4fa6":"In this case, null value is alone. So, we will replace the null value of Fare to the average of Pclass and Ebarked.","bd24affb":"## But it doesn't work well. because age is discrete value.","ca48402f":"## Age pre-processing","94b4401a":"## We will handle the data with Min-Max scaling and devide the dataset into train, validation, test dataet.\n\nAnd, we will rid of PassengerID because it is unnecessary.","cb7c2a36":"## Both two null values are 0 group.\n\nThen, search what values are. in 0 group.","5053fc82":"## Take the initial character or -1.","debcdc38":"## Find the rule of dataset, what is the clue in name?\n\n> I think there are many James.....\n\nHow can I handle this column to make tidy dataset?","b8b2fe7a":"## Concatenating the pre-processed ticket data into train_test dataset.","1407ff2f":"> test dataset","67789b85":"## Fare pre-processing","408edc07":"# Load pandas to handle the dataset","3f6c0539":"## It is better to use their references.\n\nIn the name data, there are many last names so it will be hard to find the clue.\n\nHowever, the references such as Mr., Miss., and so on are easy to find.\n\nSo, we are going to capture and convert these references into numeric.","842213df":"## Fare imputation","e81180bd":"## Find the clue and relationship from Cabin.","3e8f335d":"## Next, we've got data pre-preocessing in Ticket column","8f79d1c9":"## Make a dataset for imputation.","49ee8a7f":"## We try to imputate the null value of Age with linear regression.","8d47cc96":"## Pre-processing Name values","98f586b0":"# Deep learning for survival classification.","b62dfcab":"## Anyway, insert our name data into train, test dataset respectively.","d667748a":"## Use the fillna method to imputate the null value.","4d07743b":"## Split train and validation from train dataset.","94fdc5ec":"## Train, test dataset reindexing before concatenating pre-processed ticket data into train_test dataset.","bb1aa417":"## Find the clue from same ticket number.","e4e66d12":"## Embarked is split with 3 groups (S, C, and O) so, we determine the number of clusters is 3.","6a2a8e5b":"## In train dataset, there are many references compared with test dataset.","184d19d8":"## Find the cluster of null index.","2bcae363":"## See the value count","fc5513f8":"## Logistic regression doesn't work well, either.\n\nSo, we won't imputate the null values and just keep going.\n\nThat is, non value is just non value and this is a part of data.\n\nThus, we will convert null values into -1 which is not shown in Age.\n\n-1 means this value is non value.\n","b512f4e3":"## Spliting train and test.","c78cba3e":"Sex count plot","01b462ef":"## Min-Max scaling the values before clustering.","452d68e8":"## From now on, we will concatenate train without target value and test dataset. \nBecause data pre-processing will be applied to both train and test at the same time.","b29f5827":"## check the train dataset","99d79b73":"## Train dataset contains some non values in Age, Cabin, and Embarked columns.","72be69a7":"Let's count the values of Cabin.","2840adaa":"## Now, It is end of pre-processing.","5d16844c":"## Total null value?"}}