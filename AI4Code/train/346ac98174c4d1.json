{"cell_type":{"df5b6b57":"code","511c39fd":"code","512d8b3e":"code","e70e4ab1":"code","4ec37a84":"code","06f6102b":"code","45378112":"code","94bd3597":"code","fda15d13":"code","a97f3aab":"code","66da618b":"code","09fd7a16":"code","396a181a":"code","d9e0bc8d":"code","008f2f67":"code","3dd8e078":"code","08bb91cc":"code","c358afcd":"code","a3a87044":"code","31854c0a":"code","7aced7d0":"code","5f2e1bdb":"code","8d066488":"code","f29a265b":"code","ace83dcc":"code","a798b6ca":"code","a2ae3eaa":"code","90ba46ff":"code","17ac335b":"code","8054d29e":"code","1bdc308a":"code","c65bdbd5":"markdown","8c9fba05":"markdown","bb77e6fa":"markdown","9ba7f10a":"markdown","0941ef1e":"markdown","86bcedc1":"markdown","9657ea9f":"markdown","1303b1b0":"markdown","17387f3d":"markdown","7d137e7b":"markdown","1aeb43f8":"markdown","dc2816d7":"markdown","09a0303f":"markdown","f91567c0":"markdown","9d65c3f2":"markdown","ef68a72f":"markdown","610e0637":"markdown","2ccd734a":"markdown","cd14e47b":"markdown","ef01cdf9":"markdown","5baa51ca":"markdown","e4be4dcd":"markdown"},"source":{"df5b6b57":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebrau.\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","511c39fd":"# Import more relevant libraries and modules\nimport scipy\nfrom scipy import stats\nfrom scipy.stats.mstats import winsorize \nfrom sklearn.utils import resample\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Read in the data to a dataframe\n\nheart_failure = pd.read_csv(\"\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\")\n\n# Display the first 5 rows\n\nheart_failure.head(5)","512d8b3e":"# Display information about the dataset\n\nheart_failure.info()","e70e4ab1":"# Obtain a basic statistical description for the dataset\nheart_failure.describe()","4ec37a84":"# Plot a histogram for the `age` variable\n\nplt.figure(figsize = (10,5))\nsns.histplot(heart_failure['age'], bins = 11, kde = True)\nplt.title(\"Distribution of Age\")\n\nplt.savefig('AgeHistogram.pdf', dpi = 300)\nplt.show()","06f6102b":"# Plot pie charts for the following categorical features\nhealth_indicators = ['anaemia', 'diabetes', 'high_blood_pressure', 'sex', 'smoking']\nc = 1\n\nfor var in health_indicators:\n    \n    plt.subplot(2,3,c)\n    ax = heart_failure[var].value_counts(normalize = True).plot.pie(y = var, figsize = (10, 5), autopct='%1.1f%%')\n    \n    c = c + 1\n    \nplt.tight_layout()\nplt.savefig('PieCharts.pdf', dpi = 300)\nplt.show()","45378112":"# Plot histograms of the continuous features\ncontinuous = []\n\nfor col in heart_failure.columns:\n    \n    if heart_failure[col].nunique() > 10:\n        \n        continuous.append(col)\n        \nplt.figure(figsize = (10,6)) \nc = 1\n\nfor col in continuous:\n    \n    plt.subplot(2,4,c)\n    plt.hist(heart_failure[col])\n    plt.title(\"Histogram: \\n{}\".format(col))\n    \n    c = c + 1\n    \nplt.tight_layout()\nplt.show()","94bd3597":"# Output a correlation matrix\ncorr_matrix = heart_failure.corr()\n\nf, ax = plt.subplots(figsize=(15,10))\nheatmap = sns.heatmap(corr_matrix,\n                    square = True,\n                    cmap = 'YlGnBu',\n                    annot = True,\n                    annot_kws = {\"size\": 12})\n\n# Add column names as labels\nax.set_yticklabels(corr_matrix.columns, rotation = 0)\nax.set_xticklabels(corr_matrix.columns, rotation = 80)            \n\nsns.set_style({'xtick.bottom': True}, {'ytick.left': True})","fda15d13":"# Mask the previous correlation matrix to correlation coefficients greater than 0.1\nnew_matrix = corr_matrix[abs(corr_matrix) >= 0.1]\n\ng, ax = plt.subplots(figsize = (15,10))\n\nnew_heatmap = sns.heatmap(new_matrix, square = True,\n                         cmap = 'coolwarm', annot = True,\n                        annot_kws = {\"size\": 12})\n\n# Add column names as labels\nax.set_yticklabels(new_matrix.columns, rotation = 0)\nax.set_xticklabels(new_matrix.columns, rotation = 80)            \n\nsns.set_style({'xtick.bottom': True}, {'ytick.left': True})","a97f3aab":"# Plot barplots\ncategorical = ['anaemia', 'diabetes', 'high_blood_pressure', 'sex', 'smoking']\n\n# Remove the time variable from the list continuous\ncontinuous.remove('time')\n\nplt.figure(figsize = (15,15))\nc = 1 \n\nfor var1 in continuous:\n    \n    for var2 in categorical:\n        \n        plt.subplot(6,5,c)\n        sns.barplot(x = var2, y = var1, data = heart_failure)\n        \n        c = c + 1\n\nplt.tight_layout()\nplt.show()","66da618b":"# Plot scatter plots of the continuous variables with correlation\n# coefficient greater than 0.1\n\nplt.figure(figsize = (10, 5))\n\nplt.subplot(131)\nsns.scatterplot(x = 'ejection_fraction', y = 'serum_creatinine', data = heart_failure)\n\nplt.subplot(132)\nsns.scatterplot(x = 'ejection_fraction', y = 'serum_sodium', data = heart_failure)\n\nplt.subplot(133)\nsns.scatterplot(x = 'serum_creatinine', y = 'serum_sodium', data = heart_failure)\n\nplt.tight_layout()\nplt.savefig('Scatterplots.pdf', dpi = 300)\nplt.show()","09fd7a16":"# Winsorize the aforementioned variables\n# Make a copy of the dataframe to apply modifications to \n\nheart = heart_failure.copy()\n\n# Apply one-way winsorization to the either the 90th or 95th percentile\nheart['winsorized_creatinine_phosphokinase'] = winsorize(heart['creatinine_phosphokinase'], [0, 0.1])\nheart['winsorized_ejection_fraction'] = winsorize(heart['ejection_fraction'], [0, 0.05])\nheart['winsorized_platelets'] = winsorize(heart['platelets'], [0, 0.1])\nheart['winsorized_serum_creatinine'] = winsorize(heart['serum_creatinine'], [0, 0.1])","396a181a":"# Check for outliers in the winsorized variables\nheart[['winsorized_creatinine_phosphokinase', 'winsorized_ejection_fraction', \n       'winsorized_platelets', 'winsorized_serum_creatinine']].describe()","d9e0bc8d":"# Compute the number of unique values for each variable\n\nfor column in heart.columns.drop(['creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine']):\n    \n    print(\"Number of unique values for {}: \".format(column), heart[column].nunique())","008f2f67":"# Check the dataset for class imbalance (or balance)\n\nheart['DEATH_EVENT'].value_counts()","3dd8e078":"# Oversample the minority class\nOnes = heart[heart.DEATH_EVENT == 1]\nZeroes = heart[heart.DEATH_EVENT == 0]\n\nOnes_upsample = resample(Ones, replace = True,\n                          n_samples = len(Zeroes),\n                          random_state = 31)\n\nheart = pd.concat([Zeroes, Ones_upsample])","08bb91cc":"# Re-examine the `DEATH_EVENTS` column\nheart.DEATH_EVENT.value_counts()","c358afcd":"# Import sklearn models and performance metrics\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, mean_absolute_error\nfrom statsmodels.tools.eval_measures import mse, rmse\n\nfrom sklearn import ensemble\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\n# Drop the original columns that we winsorized previously\n\nheart = heart.drop(['creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_creatinine'], axis = 1)","a3a87044":"# Perform Kruskal-Wallis tests\n\nfrom scipy.stats import kruskal\n\ncorr_vars = ['age', 'serum_sodium', 'time', 'winsorized_ejection_fraction', 'winsorized_serum_creatinine']\n\nfor var in corr_vars:\n    \n    stat, p_value = kruskal(heart[var], heart['DEATH_EVENT'])\n    \n    if p_value < 0.05:\n        print(\"P-value for {} and Death_Event:\\n\".format(var), p_value)\n        print('\\n')","31854c0a":"# Split the data into training and test sets, with a test-set size of 30 percent\n\nX = heart.drop('DEATH_EVENT', axis = 1)\nY = heart.DEATH_EVENT\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 131)","7aced7d0":"# Fit the model \nknn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(X_train, y_train)\n\n# Make predictions with the model\ny_pred_knn = knn.predict(X_test)\n\n# Print accuracy, precision, and recall scores for the model \nacc_knn = accuracy_score(y_test, y_pred_knn)\npre_knn = precision_score(y_test, y_pred_knn)\nrecall_knn = recall_score(y_test, y_pred_knn)\nprint(\" Accuracy score: {} \\n\".format(acc_knn), \n      \"Precision score: {} \\n\".format(pre_knn),\n      \"Recall score: {}\".format(recall_knn))","5f2e1bdb":"# Instantiate a Random Forest classifier\nrfc = ensemble.RandomForestClassifier()\nrfc.fit(X_train, y_train)\n\n# Print the cross-validation (5-fold) score \nprint(cross_val_score(rfc, X_train, y_train, cv = 5))\n\n# Make predictions and then print our performance metrics\ny_pred_rfc = rfc.predict(X_test)\n\nacc_rfc = accuracy_score(y_test, y_pred_rfc)\npre_rfc = precision_score(y_test, y_pred_rfc)\nrecall_rfc = recall_score(y_test, y_pred_rfc)\nprint(\" Accuracy score: {} \\n\".format(acc_rfc), \n      \"Precision score: {} \\n\".format(pre_rfc),\n      \"Recall score: {}\".format(recall_rfc))","8d066488":"# Build an SVM classifier\nsvc = SVC()\nsvc.fit(X_train, y_train)\n\n# Print the cross-validation (5-fold) score of the training set\nprint(cross_val_score(svc, X_train, y_train, cv = 5))\n\n# Make predictions and then print performance metrics\ny_pred_svc = svc.predict(X_test)\n\nacc_svc = accuracy_score(y_test, y_pred_svc)\npre_svc = precision_score(y_test, y_pred_svc)\nrecall_svc = recall_score(y_test, y_pred_svc)\nprint(\" Accuracy score: {} \\n\".format(acc_svc), \n      \"Precision score: {} \\n\".format(pre_svc),\n      \"Recall score: {}\".format(recall_svc))","f29a265b":"# Build and fit a Gradient Boosting classifier with default parameters as a baseline model\ngbc = ensemble.GradientBoostingClassifier()\ngbc.fit(X_train, y_train)\n\n# Print cross-validation (5-fold) score for the training set\nprint(cross_val_score(gbc, X_train, y_train, cv = 5))\n\n# Make predictions and then print performance metrics\ny_pred_gb = gbc.predict(X_test)\n\nacc_gb = accuracy_score(y_test, y_pred_gb)\npre_gb = precision_score(y_test, y_pred_gb)\nrecall_gb = recall_score(y_test, y_pred_gb)\nprint(\" Accuracy score: {} \\n\".format(acc_gb), \n      \"Precision score: {} \\n\".format(pre_gb),\n      \"Recall score: {}\".format(recall_gb))","ace83dcc":"# Display confusion matrices for the baseline models\nconfusion_knn = confusion_matrix(y_test, y_pred_knn)\nconfusion_rf = confusion_matrix(y_test, y_pred_rfc)\nconfusion_svc = confusion_matrix(y_test, y_pred_svc)\nconfusion_gb = confusion_matrix(y_test, y_pred_gb)\n\nplt.figure(figsize = (15,10))\n\nplt.subplot(2,2,1)\nsns.heatmap(confusion_knn, annot = True, cmap = \"YlGnBu\")\nplt.title(\"Confusion Matrix: \\nKNN\")\n\nplt.subplot(2,2,2)\nsns.heatmap(confusion_rf, annot = True, cmap =\"YlGnBu\")\nplt.title(\"Confusion Matrix: \\nRandom Forest\")\n\nplt.subplot(2,2,3)\nsns.heatmap(confusion_gb, annot = True, cmap = \"YlGnBu\")\nplt.title(\"Confusion Matrix: \\nGradient Boosting\")\n\nplt.subplot(2,2,4)\nsns.heatmap(confusion_svc, annot = True, cmap = \"YlGnBu\")\nplt.title(\"Confusion Matrix: \\nSupport Vector Machine\")\n\nplt.tight_layout()\nplt.savefig('ConfusionMatrices.pdf', dpi = 300)\nplt.show()","a798b6ca":"from yellowbrick.model_selection import feature_importances\n\n# Display the feature importances\n\nplt.subplot(211)\nviz = feature_importances(rfc, X_train, y_train)\n\nplt.subplot(212)\nviz2 = feature_importances(gbc, X_train, y_train)\n\nplt.tight_layout()\nplt.savefig('Feature_Importances.pdf', dpi = 300)\nplt.show()","a2ae3eaa":"# Use a Random GridSearch with k-fold cross-validation to search for the best hyperparameters\n\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n\n# Set up the parameter grid\n\nparams = {\"n_estimators\": [int(x) for x in np.linspace(start = 100, stop = 2000, num = 10)],\n         \"max_features\": ['auto', 'sqrt'],\n         \"max_depth\": [int(x) for x in np.linspace(start = 10, stop = 110, num = 11)],\n         \"min_samples_split\": [2, 5, 10],\n         \"min_samples_leaf\": [1, 2, 4],\n         \"bootstrap\": [True, False],\n         }\n\n# Iterating through this grid, we will be testing 3960 settings on a base model\nrf = ensemble.RandomForestClassifier()\n\n# Instantiate a randomized GridSearch model with 5-fold cross-validation\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = params, n_iter = 100, cv = 5, verbose = 2, random_state = 43,  n_jobs =-1)\n\n# Fit the random search model to the training set and output the best parameters\nrf_random.fit(X_train, y_train)\nrf_random.best_params_","90ba46ff":"# Create a parameter grid based upon the results from the randomized search\n\nparam_grid = {'bootstrap': [False],\n             'max_features': [2, 3],\n             'max_depth': [110, 120, 130, 140, 150],\n             'min_samples_leaf': [1, 2, 4],\n             'min_samples_split': [2, 3],\n             'n_estimators': [100, 200, 300, 1000]\n             }\n\n# Create our base random forest model\nrf = ensemble.RandomForestClassifier()\n\n# Instantiate the Grid Search model\nGridSearch = GridSearchCV(estimator = rf, param_grid = param_grid, \n                         cv = 5, n_jobs = -1, verbose = 2, error_score = 'raise')","17ac335b":"# Fit the Grid Search to the data and determine the best values for the hyperparameters\nGridSearch.fit(X_train, y_train)\nGridSearch.best_params_","8054d29e":"# Make predictions with the GridSearch parameters\ny_pred_gridcv = GridSearch.predict(X_test)\n\n# Output model evaluation metrics\nacc_gridcv = accuracy_score(y_test, y_pred_gridcv)\npre_gridcv = precision_score(y_test, y_pred_gridcv)\nrecall_gridcv = recall_score(y_test, y_pred_gridcv)\nprint(\" Accuracy score: {} \\n\".format(acc_gridcv), \n      \"Precision score: {} \\n\".format(pre_gridcv),\n      \"Recall score: {}\".format(recall_gridcv))","1bdc308a":"# Display confusion matrices for the original model and the GridSearch model\n\nplt.figure(figsize = (15, 10))\n\nplt.subplot(1,2,1)\nsns.heatmap(confusion_rf, annot = True, cmap = 'YlGnBu')\nplt.title(\"Confusion Matrix: Original Model\")\n\nplt.subplot(1,2,2)\nsns.heatmap(confusion_matrix(y_test, y_pred_gridcv), annot = True, cmap = 'YlGnBu')\nplt.title(\"Confusion Matrix: Grid Search\")\n\nplt.tight_layout()\nplt.savefig('OGvsGS.pdf', dpi = 300)\nplt.show()","c65bdbd5":"The five most important features for the Random Forest and Gradient Boosting classifiers were the following, in respective order:\n\nRF: `time`, `winsorized_serum_creatinine`, `winsorized_ejection_fraction`, `winsorized_platelets`, and `age`\n\nGB: `time`, `winsorized_serum_creatine`, `winsorized_ejection_fraction`, `winsorized_platelets`, and `winsorized_creatinine_phosphokinase`\n\nThe importance of the remaining features decays to zero much more quickly for the Gradient Boosting classifier, indicating that if we opted to proceed with a boosting model, we could reduce the dimensionality further.","8c9fba05":"Now the number of zero and one class equal. We can now train our classification models, compare them to select the most accurate base model, and then tune that model's hyperparameters to increase accuracy and hopefully minimize false negatives as this is a heart failure classification (and risk prediction) problem. ","bb77e6fa":"Now,we can plot barplots for the relationship between the continuous variables and the following categorical variables: `anemia`, `diabetes`, `high_blood_pressure`, `sex`, `smoking`. ","9ba7f10a":"As the above demonstrates, the dataset exhibits class imbalance, which we will need to address. There are two approaches we can utilize:\n\n1. We oversample the minority class by creating more synthetic data (for the minority class) to match the majority class\n\n2. We undersample the majority class by removing data from the majority class to match the minority class\n\nOf the two approaches, we will opt for the former, as our dataset is relatively small; removal of data may affect the performance of any model we build","0941ef1e":"Based off of the standard deviation, $75^{\\text{th}}-$percentile, and the maximum value, the following variables have outliers that we will need to address:\n\n1. `creatinine_phosphokinase`\n2. `ejection_fraction`\n3. `platelets`\n4. `serum_creatinine`\n\nThere are two possible treatments: winsorization or removal by z-score. For sake of completeness of the dataset, we will opt to winsorize the data since it will limit the extreme values. But first, we will generate plots for the data.","86bcedc1":"This is much more workable than previously, so we can proceed further to the feature engineering phase. First, we'll examine the number of unique entries per variable to get a better sense of the dataset.\n","9657ea9f":"This dataset consists of 299 rows and 13 columns, with only numerically valued entries. Furthermore, there don't appear to be any missing values that we would need to address","1303b1b0":"The first model we'll train is a K Nearest-Neighbors model, but first, we must split the data into training and test sets.","17387f3d":"It is evident that none of these features are normally distributed; they all exhibit some skew or aren't even normally distributed.","7d137e7b":"This is a great start to a baseline Random forest model that we can tune even further to improve performance. Next, we can move on to building an SVM classifier","1aeb43f8":"Based on the correlation matrix above, the features most correlated with the target variable are `time`, `winsorized_serum_creatinine`, `age`, `winsorized_ejection_fraction`, and `serum_sodium`. We can now perform non-parametric tests for statistical significance, since our features are not normally distributed. In particular, we will perform the Kruskal-Wallis test.","dc2816d7":"The following variables are binary: `anaemia`, `diabetes`, `high_blood_pressure`, `sex`,`smoking`, and our target variable `DEATH_EVENT`. This implies that we will be utilizing classification models to make predictions. In particular, we will most likely utilize support vector machines and boosting models. There is a possibility of training a random forest classifier, although this would require additional feature engineering to convert the continuous variables into a suitable form for the random forest classifier.","09a0303f":"In each case, since the p-value was significantly smaller than 0.01, we are able to reject the Null hypothesis that, in each instance, the difference or correlation between the feature and the target variable is not statistically significant.","f91567c0":"Similar to the random forest classifier we trained earlier, this too is a great baseline model that we can tune the hyperparameters and improve upon. Going forward, we can compare the random forest and the gradient boosting after tuning their respective hyperparameters. An argument can be made that we should go forward solely with the random forest classifier for this problem, as it pertains to heart failure classification and thus we would prefer higher precision and recall scores to maximize the percentage of correct positive classifications and minimizing the percentage of false negative classifications. On this note, we can print out the confusion matrices to better evaluate which model minimizes false negative classifications.","9d65c3f2":"Based upon the above confusion matrices, the two methods are fairly similar, however, the random forest classifier does possess fewer false negative classifications than the gradient boosting classifier. Hence, we will proceed with the former method for our model and tune its hyperparameters. ","ef68a72f":"# EDA\n","610e0637":"What we can do prior to training our models is to compute a correlation matrix to determine which, if any, features are highly correlated with the target variable and, if so, if that correlation is statistically significant.","2ccd734a":"Let's explore the relationship between variables that possess a correlation coefficent with $ |c| \\geq 0.1.$ This entails masking the correlation matrix above and redisplaying the output","cd14e47b":"This isn't necessarily a great model, but we have a baseline that we can compare to other models going forward. Next, we will build a baseline Random Forest Classifier.","ef01cdf9":"Well, this is somewhat disappointing. We'll proceed to building a boosting classifier.","5baa51ca":"The most intruiging difference here is between `creatinine_phosphokinase` versus `anaemia`, `sex`, and `high_blood_pressure`. Otherwise, there weren't too many other notable differences.","e4be4dcd":"With these hyperparameter values in hand, we can use a more concentrated Grid Search."}}