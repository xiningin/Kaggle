{"cell_type":{"8d77bfa0":"code","924b621a":"code","06becacf":"code","414c3551":"code","7385bd54":"code","773bc2ee":"code","1f67333c":"code","89e317bc":"code","fe62ef33":"code","f2621415":"code","179a64ce":"code","0a6b01b4":"code","b9924619":"code","c325c409":"code","3fcd0dc1":"code","b8349872":"code","0009d3ad":"code","a1ab01fd":"code","713f6de4":"code","b906224a":"code","9a1066ff":"code","2c6d4716":"code","976b96d3":"code","877cb403":"code","d3778946":"code","a69343f5":"code","12ff20b0":"code","14caba88":"code","cdc6bfd9":"code","c22477bf":"code","e3d7b8b3":"code","3cf69b29":"code","7d97b976":"code","aa378cc7":"code","6c149cae":"code","4850515b":"code","cd6feda6":"code","65eb7f4e":"code","ec77fca0":"code","a8caa8b2":"code","0a561a9a":"markdown","fe57c2b1":"markdown","025bda8b":"markdown","8a2b1c5d":"markdown","467eea7e":"markdown","2963f2eb":"markdown","5141804d":"markdown","c11ffcd0":"markdown","1f63a968":"markdown","34b53279":"markdown","8a155de7":"markdown","13d30d47":"markdown","1f97a200":"markdown","05c6e466":"markdown","cd5a17da":"markdown","a2a048a0":"markdown","c815746d":"markdown","1862581f":"markdown","e6b19e09":"markdown","6d1b6bfe":"markdown","f5ca519b":"markdown","455612c4":"markdown","553c5d5f":"markdown","024524db":"markdown","d10b25fd":"markdown","8012f165":"markdown","c4ca7b18":"markdown","bfdda751":"markdown","7a89da96":"markdown","7a93f090":"markdown","ee038a9f":"markdown","3d72fb64":"markdown","97c6d8a3":"markdown","ecae8387":"markdown","0fb1f50c":"markdown","62095913":"markdown","b081809f":"markdown","b09ea06d":"markdown","827534f9":"markdown","dc799d5d":"markdown","812204a4":"markdown","89260dad":"markdown","6e3c54d3":"markdown","003593df":"markdown","5e9e7a5f":"markdown","d159e950":"markdown"},"source":{"8d77bfa0":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nimport tensorflow_addons as tfa\n\nfrom kaggle_datasets import KaggleDatasets\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimport numpy as np\n","924b621a":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n   # strategy = tf.distribute.Strategy.experimental_run_v2(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)\n    \nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\nprint(tf.__version__)","06becacf":"# Google Cloud Storage \nGCS_PATH = KaggleDatasets().get_gcs_path()\nprint(GCS_PATH)\n\nMONET_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '\/monet_tfrec\/*.tfrec'))\nprint('Monet files: ', str(len(MONET_FILENAMES))) \n#print(*MONET_FILENAMES, sep = \"\\n\")\n\nPHOTO_FILENAMES = tf.io.gfile.glob(str(GCS_PATH + '\/photo_tfrec\/*.tfrec'))\nprint('Photo files: ', str(len(PHOTO_FILENAMES)))\n#print(*PHOTO_FILENAMES, sep = \"\\n\")\n","414c3551":"# https:\/\/www.kaggle.com\/dimitreoliveira\/introduction-to-cyclegan-monet-paintings\n    \nimport re #Regular expression operations\n\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\nMONET_DATASET_SIZE = count_data_items(MONET_FILENAMES)\nPHOTO_DATASET_SIZE = count_data_items(PHOTO_FILENAMES)\n\nprint('Monet image files: ', str(MONET_DATASET_SIZE))\nprint('Photo image files: ', str(PHOTO_DATASET_SIZE))","7385bd54":"def read_tfrecord(example):\n    tfrecord_format = {\n        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = tf.image.decode_jpeg(example['image'], channels=3)\n    return image\n\ndef load_dataset(filenames, labeled=True, ordered=False):\n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)\n    return dataset","773bc2ee":"monet_ds = load_dataset(MONET_FILENAMES, labeled=True)\nphoto_ds = load_dataset(PHOTO_FILENAMES, labeled=True)","1f67333c":"plt.figure(figsize=(10,10))\n\nfor i, img in enumerate(monet_ds.take(4)):\n  plt.subplot(2,4,i+1)\n  plt.imshow(img) \n\nfor i, img in enumerate(photo_ds.take(4)):\n  plt.subplot(2,4,i+5)\n  plt.imshow(img) \n    \nplt.show()","89e317bc":"train_size = int(0.7 * PHOTO_DATASET_SIZE)\ntest_size = int(0.15 * PHOTO_DATASET_SIZE)\nval_size = int(0.15 * PHOTO_DATASET_SIZE)\nBUFFER_SIZE = 1000\n\nphoto_ds = photo_ds.shuffle(BUFFER_SIZE)\n\ntrain_photo_3dim_ds = photo_ds.take(train_size)\n\ntest_photo_3dim_ds = photo_ds.skip(train_size)\ntest_photo_3dim_ds = photo_ds.take(test_size)\n\nval_photo_3dim_ds = photo_ds.skip(train_size+test_size)\n","fe62ef33":"train_size = int(0.7 * MONET_DATASET_SIZE)\ntest_size = int(0.15 * MONET_DATASET_SIZE)\nval_size = int(0.15 * MONET_DATASET_SIZE)\nBUFFER_SIZE = 1000\n\nmonet_ds = monet_ds.shuffle(BUFFER_SIZE)\n\ntrain_monet_3dim_ds = monet_ds\n\n#train_monet_3dim_ds = monet_ds.take(train_size)\n\n#test_monet_3dim_ds = monet_ds.skip(train_size)\n#test_monet_3dim_ds = monet_ds.take(test_size)\n\n#val_monet_3dim_ds = monet_ds.skip(train_size+test_size)","f2621415":"IMG_HEIGHT = 256\nIMG_WIDTH = 256\n\n\ndef resize(image):\n  image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])\n  return image\n\n# normalizing the images to [-1, 1]\ndef normalize(image):\n  image = tf.cast(image, tf.float32)\n  image = (image \/ 127.5) - 1\n  return image\n\ndef denormalize(image):\n  image = tf.cast(image, tf.float32)\n  image = (image * 0.5 + 0.5) * 255 # Range 0 to 1 and then to range 0..255\n  image = tf.cast(image, tf.int32)\n  return image\n\ndef random_crop(image):\n  cropped_image = tf.image.random_crop(\n      image, size=[IMG_HEIGHT, IMG_WIDTH, 3])\n\n  return cropped_image\n\ndef random_jitter(image):\n  # resizing to 286 x 286 x 3\n  image = tf.image.resize(image, [286, 286],\n                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n\n  # randomly cropping to 256 x 256 x 3\n  image = random_crop(image)\n\n  # random mirroring\n  image = tf.image.random_flip_left_right(image)\n\n  return image\n\ndef preprocess_image_train(image):\n  image = resize(image)\n  image = random_jitter(image)\n  image = normalize(image)\n  return image\n\ndef preprocess_image_test(image):\n  image = resize(image)\n  image = normalize(image)\n  return image","179a64ce":"train_photo_ds = train_photo_3dim_ds.map(\n    preprocess_image_train, num_parallel_calls=AUTOTUNE).cache().shuffle(\n    BUFFER_SIZE).batch(1)\n\ntest_photo_ds = test_photo_3dim_ds.map(\n    preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(\n    BUFFER_SIZE).batch(1)\n\nval_photo_ds = val_photo_3dim_ds.map(\n    preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(\n    BUFFER_SIZE).batch(1)\n\n\ntrain_monet_ds = train_monet_3dim_ds.map(\n    preprocess_image_train, num_parallel_calls=AUTOTUNE).cache().shuffle(\n    BUFFER_SIZE).batch(1)\n\n# test_monet_ds = test_monet_3dim_ds.map(\n#     preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(\n#     BUFFER_SIZE).batch(1)\n\n# val_monet_ds = val_monet_3dim_ds.map(\n#     preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(\n#     BUFFER_SIZE).batch(1)","0a6b01b4":"plt.figure(figsize=(10,10))\n\nfor i, img in enumerate(train_monet_ds.take(4)):\n  plt.subplot(2,4,i+1)\n  plt.imshow(denormalize(img[0,...]), vmin=0, vmax=255) # first dimension (batch) is eliminated and we denormalize the image \n    \nfor i, img in enumerate(train_photo_ds.take(4)):\n  plt.subplot(2,4,i+5)\n  plt.imshow(denormalize(img[0,...]), vmin=0, vmax=255) # first dimension (batch) is eliminated and we denormalize the image\n    \nplt.show()","b9924619":"example_photo = next(iter(test_photo_ds)) # includes batch dimension and the photo processed according to the pipeline (test set only normalization)\nexample_monet = next(iter(train_monet_ds)) # includes batch dimension and the photo processed according to the pipeline\n\nplt.subplot(1,2,1)\nplt.imshow(denormalize(example_photo[0,...]), vmin=0, vmax=255) # first dimension (batch) is eliminated and we denormalize the image\n\nplt.subplot(1,2,2)\nplt.imshow(denormalize(example_monet[0,...]), vmin=0, vmax=255) # first dimension (batch) is eliminated and we denormalize the image\n\nplt.show()\n","c325c409":"example_monet_origin = denormalize(example_monet[0,...])\n\nplt.figure(figsize=(15,15))\nfor i in range(4):\n  image = random_jitter(example_monet_origin)\n  plt.subplot(1, 4, i+1)\n  plt.imshow(image)\n  plt.axis('off')\nplt.show()\n","3fcd0dc1":"def downsample(filters, apply_norm=True):\n  result = keras.Sequential()\n\n  initializer = tf.random_normal_initializer(0,0.02)\n\n  # Convolutional layer\n  result.add(layers.Conv2D(filters,\n                    kernel_size=4,\n                    strides=2,\n                    padding=\"same\",\n                    kernel_initializer=initializer,\n                    use_bias=not apply_norm)) # when applying Normalization you already have the bias implicit\n\n  # Normalization layer\n  if apply_norm:\n    gamma_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n    result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)) \n\n  # Activation layer\n  result.add(layers.LeakyReLU())\n\n  return result","b8349872":"# A test is performed to verify that it does not give an error\ndown_model = downsample(3)\ndown_result = down_model(example_photo, 0)\nprint (down_result.shape)","0009d3ad":"\ndef upsample(filters, apply_dropout=True):\n  result = keras.Sequential()\n\n  initializer = tf.random_normal_initializer(0,0.02)\n\n  # Transpose convolutional layer\n  result.add(layers.Conv2DTranspose(filters,\n                            kernel_size=4,\n                            strides=2,\n                            padding=\"same\",\n                            kernel_initializer=initializer,\n                            use_bias=False)) # al aplicar BatchNormalization ya tiene implicito el sesgo\n\n  # Normalization layer\n  gamma_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n  result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))  # Cyclegan usa la normalizaci\u00f3n de instancias\n    \n  \n  # Dropout layer\n  if apply_dropout:\n    result.add(layers.Dropout(0.5))\n\n  # Activation layer\n  result.add(layers.ReLU())\n\n  return result","a1ab01fd":"# A test is performed to verify that it does not give an error\nup_model = upsample(3)\nup_result = up_model(down_result)\nprint (up_result.shape)","713f6de4":"def Generator():\n\n  inputs = layers.Input(shape=[256,256,3])\n\n  down_stack = [ \n                downsample(64, apply_norm=False), # [bs, 128, 128, 64] \n                downsample(128), # [bs, 64, 64, 128]\n                downsample(256), # [bs, 32, 32, 256]\n                downsample(512), # [bs, 16, 16, 512]\n                downsample(512), # [bs, 8, 8, 512]\n                downsample(512), # [bs, 4, 4, 512]\n                downsample(512), # [bs, 2, 2, 512]\n                downsample(512), # [bs, 1, 1, 512]\n  ]\n\n  up_stack = [\n                upsample(512), # [bs, 2, 2, 1024] \n                upsample(512), # [bs, 4, 4, 1024]\n                upsample(512), # [bs, 8, 8, 1024]\n                upsample(512, apply_dropout=False), # [bs, 16, 16, 1024]\n                upsample(256, apply_dropout=False), # [bs, 32, 32, 512]\n                upsample(128, apply_dropout=False), # [bs, 64, 64, 256]\n                upsample(64, apply_dropout=False), # [bs, 128, 128, 128]\n  ]  \n\n  # Output layer\n  initializer = tf.random_normal_initializer(0,0.02)\n  last = layers.Conv2DTranspose(filters = 3, # Number of image channels\n                                kernel_size=4,\n                                strides=2,\n                                padding=\"same\",\n                                kernel_initializer=initializer,\n                                activation=\"tanh\" # output -1 to 1\n                                )\n  \n  x = inputs\n  skips = []\n  \n  # We add the Encoder blocks to the model and save the outputs to later perform the Skip Connections\n  for down in down_stack:\n    x = down(x)  \n    skips.append(x)\n  \n\n  # We eliminate the last layer of the Skips Connections, \n  # since it will be a direct input in the first block of the Decoder \n  # and we turn the Skips around since the second layer must connect \n  # with the penultimate output of the Encoder, the third with the penultimate . \n  # and so on successively\n  skips = reversed(skips[:-1]) \n\n  # We add the Decoder blocks to the model and sthe Skip Connections\n  for up, skip in zip(up_stack, skips):\n    x = up(x)\n    x = layers.Concatenate()([x,skip])\n  x = last(x)\n\n  return keras.Model(inputs=inputs, outputs=x)","b906224a":" # We test what creates the model and show architecture\ngenerator_g = Generator()\ntf.keras.utils.plot_model(generator_g, show_shapes=True, dpi=64)\n","9a1066ff":"# We pass the denormalized photo so that some result can be seen, since the model is not trained\nphoto = denormalize(example_photo[0,...])\nexample_gen_output_y = generator_g(photo[tf.newaxis,...], training=False)\n\nplt.subplot(1,2,1)\nplt.imshow(photo, vmin=0, vmax=255) \n\nplt.subplot(1,2,2)\nplt.imshow(example_gen_output_y[0]) \n\nplt.show()","2c6d4716":"def Discriminator():\n  \n  ini = layers.Input(shape=[256, 256, 3], name=\"input_img\") \n\n  down1 = downsample(64, apply_norm=False)(ini) # [bs, 128, 128, 64] \n  down2 = downsample(128)(down1) # [bs, 64, 64, 128]\n  down3 = downsample(256)(down2) # [bs, 32, 32, 256]\n  \n  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n\n  initializer = tf.random_normal_initializer(0., 0.02)\n  conv = tf.keras.layers.Conv2D(512, \n                                kernel_size=4, \n                                strides=1,\n                                kernel_initializer=initializer,\n                                use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n\n  gamma_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n  norm1 = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(conv)\n\n  leaky_relu = tf.keras.layers.LeakyReLU()(norm1)\n\n  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n\n  last = tf.keras.layers.Conv2D(1, \n                                kernel_size=4, \n                                strides=1,\n                                kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n\n  return keras.Model(inputs=ini, outputs=last)\n","976b96d3":" # We test what creates the model and show architecture\ndiscriminator_y = Discriminator()\ntf.keras.utils.plot_model(discriminator_y, show_shapes=True, dpi=64)","877cb403":"# We pass the denormalized photo so that some result can be seen, since the model is not trained\nphoto = denormalize(example_photo[0,...])\nexample_gen_output_y = generator_g(photo[tf.newaxis,...], training=False)\nexample_disc_out = discriminator_y([example_photo, example_gen_output_y], training=False)\n\nprint(example_disc_out.shape)\n\nplt.figure(figsize=(10,10))\n\nplt.subplot(1,3,1)\nplt.imshow(photo, vmin=0, vmax=255) \n\nplt.subplot(1,3,2)\nplt.imshow(example_gen_output_y[0,...]) \n\nplt.subplot(1,3,3)\nm = example_disc_out[0,...,-1].numpy()*1000\nim = plt.imshow(m, vmin=-20, vmax=20, cmap='RdBu_r')\nplt.colorbar(im,fraction=0.046, pad=0.04)\n\nplt.show()","d3778946":"with strategy.scope():\n    \n    generator_monet = Generator()\n    generator_photo = Generator()\n\n    discriminator_monet = Discriminator()\n    discriminator_photo = Discriminator()","a69343f5":"photo = denormalize(example_photo[0,...])\nmonet = denormalize(example_monet[0,...])\n\n# From photo we generate Monet (fake) and regenerate the photo (cycle) again\nexample_gen_output_monet_fake = generator_monet(photo[tf.newaxis,...], training=False)\nexample_gen_output_photo_cycle = generator_photo(example_gen_output_monet_fake, training=False)\n\n# We run the discriminator for Monet (fake)\nexample_disc_out_monet = discriminator_monet(example_gen_output_monet_fake, training=False)\n\n\n# From Monet we generate photo (fake) and regenerate Monet (cycle) again\nexample_gen_output_photo_fake = generator_photo(monet[tf.newaxis,...], training=False)\nexample_gen_output_monet_cycle = generator_monet(example_gen_output_photo_fake, training=False)\n\n# We execute the discriminator for Photo (fake)\nexample_disc_out_photo = discriminator_photo(example_gen_output_photo_fake, training=False)\n\n\n# We present results, as the network is not trained, the outputs are not good, \n# but we modify the scala to be able to have some example images\n\nplt.figure(figsize=(10,10))\n\n# Input Photo\nplt.subplot(2,4,1)\nplt.imshow(photo, vmin=0, vmax=255) \n\n# Fake Monet\nplt.subplot(2,4,2)\n#m = example_gen_output_monet_fake[0,...].numpy()\n#print(np.min(m), np.max(m))\ncontrast = 100 \nplt.imshow(example_gen_output_monet_fake[0,...]*contrast) \n\n# Photo Cycle\nplt.subplot(2,4,3)\n#m = example_gen_output_photo_cycle[0,...].numpy()\n#print(np.min(m), np.max(m))\ncontrast = 100\nplt.imshow(example_gen_output_photo_cycle[0,...]*contrast) \n\n# Monet discriminator result\nplt.subplot(2,4,4)\n#m = example_disc_out_monet[0,...,-1].numpy()\n#print(np.min(m), np.max(m))\ncontrast = 1000\nim = plt.imshow(m*contrast, vmin=-20, vmax=20, cmap='RdBu_r')\nplt.colorbar(im,fraction=0.046, pad=0.04)\n\n\n\n# Input Monet\nplt.subplot(2,4,5)\nplt.imshow(monet, vmin=0, vmax=255) \n\n# Fake Photo\nplt.subplot(2,4,6)\n#m = example_gen_output_photo_fake[0,...].numpy()\n#print(np.min(m), np.max(m))\ncontrast = 100\nplt.imshow(example_gen_output_photo_fake[0,...]*contrast) \n\n# Monet Cycle\nplt.subplot(2,4,7)\n#m = example_gen_output_monet_cycle[0,...].numpy()\n#print(np.min(m), np.max(m))\ncontrast = 100\nplt.imshow(example_gen_output_monet_cycle[0,...]*contrast) \n\n# Photo discriminator result  \nplt.subplot(2,4,8)\n#m = example_disc_out_photo[0,...,-1].numpy()\n#print(np.min(m), np.max(m))\ncontrast = 1000\nim = plt.imshow(m*contrast, vmin=-20, vmax=20, cmap='RdBu_r')\nplt.colorbar(im,fraction=0.046, pad=0.04)\n\n","12ff20b0":"loss_object = keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)","14caba88":"with strategy.scope():\n    def discriminator_loss(disc_real_output, disc_generated_output):\n      #compare the real image with a matrix of 1. (All Ok)\n      real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n\n      #compare the fake image with a matrix of 0 (All Fake)\n      generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)  \n\n      total_disc_loss = (real_loss + generated_loss) \/ 2\n\n      return total_disc_loss","cdc6bfd9":"with strategy.scope():\n    def generator_adversarial_loss(generated):\n      return loss_object(tf.ones_like(generated), generated)","c22477bf":"\nwith strategy.scope():\n    def generator_calc_cycle_loss(real_image, cycled_image, param_lambda):\n      loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n\n      return param_lambda * loss1","e3d7b8b3":"with strategy.scope():\n    def generator_identity_loss(real_image, same_image, param_lambda):\n      loss = tf.reduce_mean(tf.abs(real_image - same_image))\n      return param_lambda * 0.5 * loss","3cf69b29":"with strategy.scope():\n    generator_monet_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    generator_photo_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\n    discriminator_monet_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    discriminator_photo_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","7d97b976":"\nclass CycleGan(keras.Model):\n    def __init__(\n        self,\n        generator_monet,\n        generator_photo,\n        discriminator_monet,\n        discriminator_photo,\n        lambda_cycle=10,\n        lambda_identity=10\n    ):\n        super(CycleGan, self).__init__()\n        self.generator_monet = generator_monet\n        self.generator_photo = generator_photo\n        self.discriminator_monet = discriminator_monet\n        self.discriminator_photo = discriminator_photo\n        self.lambda_cycle = lambda_cycle\n        self.lambda_identity = lambda_identity\n        \n        \n    def compile(\n        self,\n        generator_monet_optimizer,\n        generator_photo_optimizer,\n        discriminator_monet_optimizer,\n        discriminator_photo_optimizer,\n        discriminator_loss,        \n        generator_adversarial_loss,\n        generator_calc_cycle_loss,\n        generator_identity_loss\n    ):\n        super(CycleGan, self).compile()\n        self.generator_monet_optimizer = generator_monet_optimizer\n        self.generator_photo_optimizer = generator_photo_optimizer\n        self.discriminator_monet_optimizer = discriminator_monet_optimizer\n        self.discriminator_photo_optimizer = discriminator_photo_optimizer\n        self.discriminator_loss = discriminator_loss        \n        self.generator_adversarial_loss = generator_adversarial_loss\n        self.generator_calc_cycle_loss = generator_calc_cycle_loss\n        self.generator_identity_loss = generator_identity_loss\n        \n        \n\n    def train_step(self, batch_data):\n\n        monet, photo = batch_data\n        \n        with tf.GradientTape(persistent=True) as tape:\n\n            # Obtener predicciones\n            # desde photo generamos Monet (fake) y regeneramos de nuevo la foto (cycle)\n            gen_output_monet_fake = self.generator_monet(photo, training=False)\n            gen_output_photo_cycle = self.generator_photo(gen_output_monet_fake, training=False)\n\n            # desde Monet generamos photo (fake) y regeneramos de nuevo Monet (cycle) \n            gen_output_photo_fake = self.generator_photo(monet, training=False)\n            gen_output_monet_cycle = self.generator_monet(gen_output_photo_fake, training=False)\n\n            # generating itself are used for identity loss.\n            gen_output_monet_same = self.generator_monet(monet, training=False)\n            gen_output_photo_same = self.generator_photo(photo, training=False)\n\n\n            # Ejecutamos el discriminador para Monet real (entrada) y Monet generado (fake)\n            disc_out_monet_real = self.discriminator_monet(monet, training=False)        \n            disc_out_monet_fake = self.discriminator_monet(gen_output_monet_fake, training=False)\n\n            # Ejecutamos el discriminador para Foto real (entrada) y Foto (fake)\n            disc_out_photo_real = self.discriminator_photo(photo, training=False)\n            disc_out_photo_fake = self.discriminator_photo(gen_output_photo_fake, training=False)\n\n\n            # Calculate the loss\n            \n            gen_monet_adversarial_loss = self.generator_adversarial_loss(disc_out_monet_fake)\n            gen_photo_adversarial_loss = self.generator_adversarial_loss(disc_out_photo_fake)\n\n            # evaluates total cycle consistency loss\n            total_cycle_loss = (self.generator_calc_cycle_loss(monet, gen_output_monet_cycle, self.lambda_cycle) \n                                + self.generator_calc_cycle_loss(photo, gen_output_photo_cycle, self.lambda_cycle))\n\n            gen_monet_identity_loss = self.generator_identity_loss(monet, gen_output_monet_same, self.lambda_identity)\n            gen_photo_identity_loss = self.generator_identity_loss(photo, gen_output_photo_same, self.lambda_identity)\n                \n            # Total generator loss = adversarial loss + cycle loss + identity loss\n            total_gen_monet_loss = (gen_monet_adversarial_loss + total_cycle_loss + gen_monet_identity_loss)\n            total_gen_photo_loss = (gen_photo_adversarial_loss + total_cycle_loss + gen_photo_identity_loss)\n\n            disc_monet_loss = self.discriminator_loss(disc_out_monet_real, disc_out_monet_fake)\n            disc_photo_loss = self.discriminator_loss(disc_out_photo_real, disc_out_photo_fake)\n\n        # Calculate the gradients for generator and discriminator\n        gen_monet_gradients = tape.gradient(total_gen_monet_loss, \n                                            self.generator_monet.trainable_variables)\n        gen_photo_gradients = tape.gradient(total_gen_photo_loss, \n                                            self.generator_photo.trainable_variables)\n\n        disc_monet_gradients = tape.gradient(disc_monet_loss, \n                                                self.discriminator_monet.trainable_variables)\n        disc_photo_gradients = tape.gradient(disc_photo_loss, \n                                               self.discriminator_photo.trainable_variables)\n\n\n        # Apply the gradients to the optimizer\n\n\n        self.generator_monet_optimizer.apply_gradients(zip(gen_monet_gradients,\n                                                      self.generator_monet.trainable_variables))\n        self.generator_photo_optimizer.apply_gradients(zip(gen_photo_gradients,\n                                                     self.generator_photo.trainable_variables)) \n\n        self.discriminator_monet_optimizer.apply_gradients(zip(disc_monet_gradients,\n                                                         self.discriminator_monet.trainable_variables)) \n        self.discriminator_photo_optimizer.apply_gradients(zip(disc_photo_gradients,\n                                                        self.discriminator_photo.trainable_variables)) \n\n        \n        return {\n            \"total_gen_monet_loss\": total_gen_monet_loss,\n            \"total_gen_photo_loss\": total_gen_photo_loss,\n            \"disc_monet_loss\": disc_monet_loss,\n            \"disc_photo_loss\": disc_photo_loss\n        }\n    ","aa378cc7":"with strategy.scope():\n    cycle_gan_model = CycleGan(\n        generator_monet, generator_photo, discriminator_monet, discriminator_photo\n    )\n\n    cycle_gan_model.compile(\n        generator_monet_optimizer = generator_monet_optimizer,\n        generator_photo_optimizer = generator_photo_optimizer,\n        discriminator_monet_optimizer = discriminator_monet_optimizer,\n        discriminator_photo_optimizer = discriminator_photo_optimizer,\n        discriminator_loss = discriminator_loss,        \n        generator_adversarial_loss = generator_adversarial_loss,\n        generator_calc_cycle_loss = generator_calc_cycle_loss,\n        generator_identity_loss = generator_identity_loss\n    )\n","6c149cae":"import time\n\nt1 = time.process_time()\n\ncycle_gan_model.fit(\n    tf.data.Dataset.zip((train_photo_ds, train_monet_ds)),\n    epochs=20\n)\n\nt2 = time.process_time()\n\nprint (\"Accelerator =  ----- Computation time = \" + str(1000*(t2 - t1)) + \"ms\")","4850515b":"def generate_images(model, test_input):\n  prediction = model(test_input)\n\n  plt.figure(figsize=(12, 12))\n\n  plt.subplot(1, 2, 1)\n  plt.imshow(test_input[0] * 0.5 + 0.5)\n  plt.title('Input Image')\n  plt.axis('off')\n    \n  plt.subplot(1, 2, 2)\n  plt.imshow(prediction[0] * 0.5 + 0.5)\n  plt.title('Predicted Image')\n  plt.axis('off')\n\n  plt.show()","cd6feda6":"generate_images(generator_monet, example_photo)","65eb7f4e":"# From photo we generate Monet (fake) and regenerate the photo (cycle) again\nexample_gen_output_monet_fake = generator_monet(example_photo, training=False)\nexample_gen_output_photo_cycle = generator_photo(example_gen_output_monet_fake, training=False)\nexample_gen_output_photo_same = generator_photo(example_photo, training=False)\n\n\n# We execute the discriminator for Photo (real)\nexample_disc_out_photo_real = discriminator_photo(example_photo, training=False)\n\n# We run the discriminator for Monet (fake)\nexample_disc_out_monet_fake = discriminator_monet(example_gen_output_monet_fake, training=False)\n\n\n# from Monet we generate photo (fake) and regenerate Monet (cycle) again\nexample_gen_output_photo_fake = generator_photo(example_monet, training=False)\nexample_gen_output_monet_cycle = generator_monet(example_gen_output_photo_fake, training=False)\nexample_gen_output_monet_same = generator_monet(example_monet, training=False)\n\n# We run the discriminator for Monet (real)\nexample_disc_out_monet_real = discriminator_monet(example_monet, training=False)\n\n# We execute the discriminator for Photo (fake)\nexample_disc_out_photo_fake = discriminator_photo(example_gen_output_photo_fake, training=False)","ec77fca0":"plt.figure(figsize=(10,10))\n\n# Foto entrada\nplt.subplot(4,4,1)\nplt.imshow(example_photo[0] * 0.5 + 0.5) \n\n# Monet generado\nplt.subplot(4,4,2)\nplt.imshow(example_gen_output_monet_fake[0] * 0.5 + 0.5) \n\n# Photo Cycle\nplt.subplot(4,4,3)\nplt.imshow(example_gen_output_photo_cycle[0] * 0.5 + 0.5) \n\n# Photo Same\nplt.subplot(4,4,4)\nplt.imshow(example_gen_output_photo_same[0] * 0.5 + 0.5) \n\n# Discriminador para Photo (real)\nplt.subplot(4,4,5)\nm = example_disc_out_photo_real[0,...,-1].numpy()\nplt.imshow(m, vmin=np.min(m), vmax=np.max(m), cmap='RdBu_r')\nplt.colorbar(im,fraction=0.046, pad=0.04)\n\n# Discriminador para Monet (fake)\nplt.subplot(4,4,7)\nm = example_disc_out_monet_fake[0,...,-1].numpy()\nplt.imshow(m, vmin=np.min(m), vmax=np.max(m), cmap='RdBu_r')\nplt.colorbar(im,fraction=0.046, pad=0.04)\n\n# Monet entrada\nplt.subplot(4,4,9)\nplt.imshow(example_monet[0] * 0.5 + 0.5) \n\n# Foto generado\nplt.subplot(4,4,10)\nplt.imshow(example_gen_output_photo_fake[0] * 0.5 + 0.5) \n\n# Monet Cycle\nplt.subplot(4,4,11)\nplt.imshow(example_gen_output_monet_cycle[0] * 0.5 + 0.5) \n\n# Monet Same\nplt.subplot(4,4,12)\nplt.imshow(example_gen_output_monet_same[0] * 0.5 + 0.5) \n\n# Discriminador para Monet (real)\nplt.subplot(4,4,13)\nm = example_disc_out_monet_real[0,...,-1].numpy()\nplt.imshow(m, vmin=np.min(m), vmax=np.max(m), cmap='RdBu_r')\nplt.colorbar(im,fraction=0.046, pad=0.04)\n\n# Discriminador para Photo (fake)\nplt.subplot(4,4,15)\nm = example_disc_out_photo_fake[0,...,-1].numpy()\nplt.imshow(m, vmin=np.min(m), vmax=np.max(m), cmap='RdBu_r')\nplt.colorbar(im,fraction=0.046, pad=0.04)\n\n\nplt.show()","a8caa8b2":"for img in test_photo_ds.take(5):\n    generate_images(generator_monet, img)","0a561a9a":"Since the calculation of the two terms is the same, then a function is created that allows each of the terms to be calculated, varying only the inputs.","fe57c2b1":"#### Generator loss\n> The generator loss has 3 terms:\n> * Adversary loss\n> * Cycle loss\n> * Identity loss\n&nbsp;\n\n**Generator adversary loss**\n* Takes as input the output of the dicriminator\n    * For the loss of the generator_monet the function will take the output of the discriminator_monet executed with fake monet\n    * For the loss of the generator_photo the function will take the output of the discriminator_photo executed with fake photo\n* The perfect generator will have the discriminator output only ones, all Ok. Therefore, compare the generated image with a matrix of 1 to find the loss.","025bda8b":"Execution example","8a2b1c5d":"### Modelo completo\n\nWe define the CycleGan class that inherits from Keras.model, this will allow overwriting the train_step function that is used in the fit method in such a way that performance can be maximized with the execution in TPU.\n\n","467eea7e":"\nMonet's paints data will all be used for training","2963f2eb":"Using TPUs\n\n* https:\/\/www.kaggle.com\/philculliton\/a-simple-petals-tf-2-2-notebook\n* https:\/\/www.kaggle.com\/c\/tpu-getting-started\n\n\nIt is not necessary to enable the TPUs during the implementation, to carry out the implementation I have left the notebook without any accelerator, and for the first tests of very few epoch and used GPU, only for large trainings I have enabled the TPU.","5141804d":"We define preprocessing functions.","c11ffcd0":"# Introduction to CycleGAN\n\nUnpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks\n\nAdversary Generative Networks (GAN) uses a generator and a discriminator, the generator is not trained to minimize the distance to a specific image but to deceive the discriminator, which allows the model to learn in a generic way.\n* The generator has to generate images that are accepted by the discriminator\n* The discriminator tries to discover the images that are not real and reject the images generated by the generator.\n\nCycleGAN uses a loss of cycle consistency to allow training without the need for paired data. In unpaired dataset, there is no pre-defined meaningful transformation that generator can learn, so will create it.\n\nThe model taking an input image from domain $D_{A}$ which is fed to our first generator  $Generator_{A\u2192B}$  whose job is to transform a given image from domain  $D_{A}$  to an image in target domain $D_{B}$ . This new generated image is then fed to another generator $Generator_{B\u2192A}$ which converts it back into an image, $Cyclic_{A}$, from our original domain $D_{A}$. This output image must be close to original input image to define a meaningful mapping that is absent in unpaired dataset.\n\nSo the goal is to learn:\n\n* G:X\u2192Y mapping such that the image distribution of G(X) is indistinguishable from the Y distribution using a contradictory loss\n* F:Y\u2192X mapping such that the image distribution of F(Y) is indistinguishable from the X distribution using a contradictory loss.\n* The calculation of the total loss of the generators uses:\n    * Adversary losses (from Adversary Generative Networks (GAN)) that allow matching the distribution of images generated with the distribution of data in the target domain\n    * Loss of cycle consistency to enforce F(G(X)) \u2248 X and G(F(Y)) \u2248 Y, prevent the learned assignments G and F from contradicting each other\n    * Loss of identity, allows the generated image to be similar to the original\n\n\nModels\n&nbsp;\n* 2 generators (G_MONET and G_PHOTO) modified U-Net architecture. (will be detailed later https:\/\/lmb.informatik.uni-freiburg.de\/people\/ronneber\/u-net\/)\n    * The G_MONET generator learns how to transform a photograph into a Monet painting \n    * The G_PHOTO generator learns how to transform a Monet painting into a photograph  \n    * As the images are not paired, it is necessary to use two cycles\n        * Input Photo -> G_MONET -> Fake Monet -> G_PHOTO -> Cycle Photo\n        * Input Monet -> G_PHOTO -> Fake Photo -> G_MONET -> Cycle Monet\n&nbsp;\n* 2 discriminators (D_MONET AND D_PHOTO) PatchGAN architecture The PatchGAN discriminator tries to classify if each  patch in an image is real or fake. (will be detailed later https:\/\/arxiv.org\/abs\/1611.07004v3)\n    * The D_MONET discriminator learns to differentiate if a Monet painting is real or fake, serving to calculate the adversary loss and improve the G_MONET generator\n    * The D_PHOTO discriminator learns to differentiate if a photo is real or fake, serving to calculate the adversary loss and improve the G_PHOTO generator\n    * As the images are not paired, to calculate the loss of the discriminators, each one of them is executed twice, one with a real image and another with the fake image generated.\n","1f63a968":"Discriminator initial test","34b53279":"Given that the calculation of the losses is the same for both generators, then a function is created that allows it to be calculated, varying only the inputs.","8a155de7":"Functions are defined to read the images as they are, without any type of transformation","13d30d47":"It is checked again that the upload will be successful","1f97a200":"Generator initial test","05c6e466":"In this Notebook we have tried to carry out the step-by-step implementation of CycleGAN to generate a model that allows a style transfer, on photographs with a painting style like Monet.\n\nIn a next Notebook I will make the adjustment of the model.","cd5a17da":"## Set up the input pipeline\n","a2a048a0":"#### Discriminator loss\n\n* The discriminator loss function takes 2 inputs;\n    * For the discriminator_photo will take as input: \n        * The output of the discriminator_photo whose input is the real photo of the training set \n        * The output of the discriminator_photo whose input is the fake photo generated by the generator_photo\n    * For the discriminator_monet will take as input: \n        * The output of the discriminator_monet whose input is the real Monet of the training set \n        * The output of the discriminator_monet whose input is the fake Monet generated by the generator_monet    \n* The calculation of the loss has two components:    \n    * real_loss compare the real image with a matrix of 1. (All Ok)\n    * generate_loss compare the fake image with a matrix of 0 (All Fake)\n* So the total_loss is 1\/2 of the sum of the real_loss and the generate_loss","c815746d":"## MODEL\n\nTo build the model we will follow the following steps:\n1. Build the Generator\n1. Build the Discriminador\n1. Definition of loss functions\n    * Discriminator loss\n    * Generator loss\n        * Adversary loss\n        * Cycle loss\n        * Identity loss\n1. Define the optimizers\n","1862581f":"Examples are saved for implementation testing","e6b19e09":"For the implementation of the blocks that make up the coding part of the generator, a \"downsample\" function will be created that passing the number of filters to it and if normalization is applied, it will create a keras.Sequential object","6d1b6bfe":"Example\n\nGenerator part:\n\n* Starting from the photo, a simulation of a Monet painting is generated and later from this simulation an attempt is made to generate the original photo\n\n* Starting from the monet, a photo simulation is generated and later from this simulation an attempt is made to generate the original monet\n\nDiscriminator part:\n\n* Discriminator so that the fake photo looks like a real photo\n* Discriminator so that the monet fake looks like a Monet painting","f5ca519b":"Image preprocessing is added to the pipeline, depending on the set, train or test\/validation\n\nThe batch dimension is added","455612c4":"Next, the complete diagram is presented again, which allows to understand the implementation of the train_step function","553c5d5f":"The data is read from the Tfrec files","024524db":"Next, a diagram of the Generator is presented, in which the inputs and outputs of the different blocks can be seen.\n\n* The red arrows represent the execution of a downsample-k block\n* The blue arrows represent the execution of an upsampleD-k block (with Dropout)\n* The green arrows represent the execution of an upsample-k block (without Dropout)\n* Lastly, an output layer, represented by a pink arrow is executed, which is simply Transposed Convolution of 3 filters to convert the output into a 256X256 image by 3 channels\n\n\nAll convolutional layers of downsample, have the parameter strides = 2, which causes the dimensions to be reduced by half, likewise the Transposed Convolution layers of upsample also have the parameter strides = 2 so the dimensions are doubled. In the first two dimensions, not counting the Batch size dimension.\n\nThe size of the 3 dimension will coincide with the number of filters applied.\n\n","d10b25fd":"### Initialize the optimizers \n\nInitialize the optimizers for all the generators and the discriminators.","8012f165":"### Build the complete Model \n\n\nAs seen in the introduction, the complete model consists of:\n&nbsp;\n* 2 generators (G_MONET and G_PHOTO) \n    * The G_MONET generator learns how to transform a photograph into a Monet painting \n    * The G_PHOTO generator learns how to transform a Monet painting into a photograph  \n    * As the images are not paired, it is necessary to use two cycles\n        * Input Photo -> G_MONET -> Fake Monet -> G_PHOTO -> Cycle Photo\n        * Input Monet -> G_PHOTO -> Fake Photo -> G_MONET -> Cycle Monet\n&nbsp;\n* 2 discriminators (D_MONET AND D_PHOTO) \n    * The D_MONET discriminator learns to differentiate if a Monet painting is real or fake, serving to calculate the adversary loss and improve the G_MONET generator\n    * The D_PHOTO discriminator learns to differentiate if a photo is real or fake, serving to calculate the adversary loss and improve the G_PHOTO generator\n    * As the images are not paired, to calculate the loss of the discriminators, each one of them is executed twice, one with a real image and another with the fake image generated.\n    ","c4ca7b18":"**Identity loss**\n\nIt is desired that if the generator_monet is executed with the image of the original monet painting, the result should be a very similar image, and also with the generator_photo and the original photo.\n\nTherefore, the loss of identity forces what the generator generates to resemble the input.\n\n\n* It takes as inputs, 2 images, the original image and the output of running the generator with the original image.\n    * For the loss of the generator_monet the function will take the training monet image and the output of the generator_monet with the same input (same_monet)\n    * For the loss of the generator_photo the function will take the training photo image and the output of the generator_photo with the same input (same_photo)\n   \n\nThe loss will be the mean absolute error between the real image and the generated one.\n","bfdda751":"## CycleGAN architecture\n\nBy way of summary and as a general outline, the following steps will be carried out in each step of the training:\n\n* It starts with two entries:\n    * a Photo (Input Photo)\n    * a Monet paint (Input Monet)\n<br>\n* Photo to Monet conversion is performed, running the following generators:\n    * G_MONET is executed with Input Photo transforming it into a Fake Monet\n    * G_PHOTO is executed with Fake Monet, to try to return to the original photo\n    * G_MONET is executed with Input Photo (for the calculation of identity loss)\n<br>\n* Monet to Photo conversion is performed, running the following generators:\n    * G_PHOTO is executed with Input Monet transforming it into a Fake Photo\n    * G_MONET is executed with Fake Photo, to try to return to the original frame\n    * G_PHOTO is executed with Input Monet (for the calculation of identity loss)\n<br>    \n* The discriminators are executed:\n    * D_PHOTO is executed 2 times to calculate its cross loss:\n        * with Input Photo\n        * with Fake Photo\n    * D_MONET is executed 2 times to calculate its cross loss:\n        * with Input Monet\n        * with Fake Monet\n<br>    \n* The losses of the generators and discriminators are calculated, as a function of:\n    1. Generator loss (adversary loss)\n    1. Discriminator loss (adversary loss)\n    1. Cycle loss\n    1. Identity loss\n<br>    \n* Calculate the gradients using backpropagation.\n<br>\n* Apply the gradients to the optimizer\n","7a89da96":"We check that the upload will be done correctly","7a93f090":"<p>\n<h1><center> Step by Step CycleGAN - Style Transfer (Photos to Monet Paintings) <\/center><\/h1>\n\nThis Notebook aims to follow the development of the CycleGAN architecture to capturing special characteristics of one image collection, in our case Monet paintings, and figuring out how these characteristics could be translated into the other image collection, all in the absence of any paired training examples. \n\n* Compete https:\/\/www.kaggle.com\/c\/gan-getting-started\n\nSources:\n\n* https:\/\/www.kaggle.com\/amyjang\/monet-cyclegan-tutorial (baseline competition)\n* https:\/\/www.kaggle.com\/dimitreoliveira\/introduction-to-cyclegan-monet-paintings\n* https:\/\/arxiv.org\/pdf\/1703.10593.pdf (paper)* \n* https:\/\/junyanz.github.io\/CycleGAN\/\n* https:\/\/hardikbansal.github.io\/CycleGANBlog\/\n* https:\/\/www.tensorflow.org\/tutorials\/generative\/cyclegan\n","ee038a9f":"We visualize example","3d72fb64":"**Generator cycle consistency loss**\n\nAs the training data is not paired, to make the network learn the correct mapping and the result is similar to the original input. The authors of the paper add another term for the calculation of the loss which they call the loss of consistency of the cycle.\n\nThe cycle consists of:\n* Generate a monet style image from a photo, this generated image is passed as input to the second generator, which should generate a photo from a fake monet style image.\n    * Input Photo -> G_MONET -> Fake Monet -> G_PHOTO -> Cycle Photo\n* On the other hand, an image is generated that aims to imitate a real photo from a Monet painting, this generated image is passed as an input to the second generator, which should generate the Monet painting again from the fake photo.\n    * Input Monet -> G_PHOTO -> Fake Photo -> G_MONET -> Cycle Monet  \n\nTo calculate the cycle consistency loss, two terms are calculated\n* Average absolute error for photo, is calculated between Input Photo and Cycle Photo\n* Mean absolute error for monet, is calculated between Input Monet and Cycle Monet\n\nThe cyclo error will be the sum of both terms.\n\nThis term is applied to the total loss calculation for both generators (generator_monet and generator_photo)\n\n","97c6d8a3":"# Implementation","ecae8387":"Model test\n\nWe have tested the model with CPU, GPU and TPU\n\n2 epochs\n* Accelerator = NONE ----- Computation time = 10391658.411232ms\n* Accelerator = GPU ----- Computation time = 270945.03528ms\n* Accelerator = TPU Number of replicas: 8 ----- Computation time = 74406.024123ms\n\n","0fb1f50c":"### IMAGE PRE-PROCESSING\n\n* Resizing image (In this case it would not be necessary to do it, because the images are already in the necessary size. But with this step if we wanted to add new images it would not be necessary to scale them previously)\n\n* Normalizing the images to [-1, 1] \n\n* Random jittering and mirroring to the training dataset. These are some of the image augmentation techniques that avoids overfitting, Random jittering performs:\n    * Resize an image to bigger height and width\n    * Randomly crop to the target size\n    * Randomly flip the image horizontally","62095913":"The photo file is divided into train, test and validation\n\nIn this notebook we will only make a first approach to the architecture, so we will not use the test and validation sets to evaluate the model.","b081809f":"<img src='https:\/\/juancarlossantiagoculebras.github.io\/CycleGAN\/Images\/CycleGAN%20Unet%20generator.jpg'>","b09ea06d":"### Definition of loss functions\n\nSigmoid cross entropy is used to calculate the adversary losses in the discriminator and generator.","827534f9":"Data set size","dc799d5d":"For the implementation of the blocks that make up the decoding part of the generator, an \"upsample\" function will be created that passing the number of filters to it and if dropout is applied, it will create a keras.Sequential object","812204a4":"### Load the dataset","89260dad":"<img src='https:\/\/juancarlossantiagoculebras.github.io\/CycleGAN\/Images\/CycleGAN.jpg'>","6e3c54d3":"### Build the Generator\n\nThe original paper CycleGAN uses a modified based on resnet generator. In this Notebook, a modified unet generator is used for simplicity, as in \n\nhttps:\/\/www.tensorflow.org\/tutorials\/generative\/cyclegan\n\nThe architecture of the generator is a modified U-Net, consisting of an encoder block and a decoder block, each of them is made up of simpler blocks of layers:\n\n* Each block of the encoder, we call it downsample-k where k denotes the number of filters, this block performs an image compression operation (downsample). It consists of the following layers \n  * Convolution \n  * Instance Normalization (not apply to the first block)\n  * Leaky ReLU\n* In the decoder we can find two types of blocks, depending on whether a dropout operation is performed or not, we will call each one of them upsampleD-k and upsample-k, since they are performing a decompression. Each of these blocks is made up of:\n  * Transposed Convolution\n  * Instance Normalization\n  * Dropout (applied to the first 3 blocks) \n  * ReLU      \n* Skip connections exist between encoder and decoder.\n\n![image.png](attachment:image.png)\n\n","003593df":"Test random jitter","5e9e7a5f":"<img src='https:\/\/juancarlossantiagoculebras.github.io\/CycleGAN\/Images\/CycleGAN.jpg'>","d159e950":"### Build the Discriminator\n\n\nThe task of the discriminator is whether an input image is original or fake (the output of a generator)\n\nThe architecture of the discriminator is a convolution network of the PatchGAN type, instead of returning whether the image is real or not, this architecture returns whether pieces of the image can be considered real or false. consisting of an encoder blockmade up of simpler blocks of layers:\n\n\n<img src='https:\/\/juancarlossantiagoculebras.github.io\/CycleGAN\/Images\/PatchGAN discriminator.jpg'>\n\n\nAs in the generator, the encoder is made up of downsample-k blocks, this block performs an image compression operation (downsample). It consists of the following layers \n  * Convolution \n  * Instance Normalization (not apply to the first block)\n  * Leaky ReLU\n\nSo the already implemented function is used \"downsample\", all convolutional layers of downsample, have the parameter strides = 2, which causes the dimensions to be reduced by half.\n\n\nThe shape of the discriminator output layer is (batch_size, 30, 30, 1), each 30x30 patch of the output sorts a 70x70 portion of the input image\n\n"}}