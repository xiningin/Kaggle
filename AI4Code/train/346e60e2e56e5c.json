{"cell_type":{"23abc806":"code","f6b15504":"code","9365d0f1":"code","f9602d84":"code","68a9e46c":"code","7787c411":"code","886611d1":"code","c082ae8d":"code","19a08218":"code","dcb2fb16":"code","169dac44":"code","09da7d0d":"code","ccc70607":"code","ff47d40d":"code","480d4f8e":"code","352fce1e":"code","2c0867d5":"code","bf831817":"code","21eb9045":"code","c464e266":"code","d5b0de92":"code","1fb7b8f4":"code","a1c76d1c":"code","97b55eba":"code","bca633a9":"code","16f4d01e":"code","0c25bc0a":"code","b4f4262f":"code","5bc30154":"code","e83a7441":"markdown","22c2d4bc":"markdown","2b6ff014":"markdown","69f685e2":"markdown","a63ba4cc":"markdown","65495241":"markdown","b78f77eb":"markdown","ba4d9fe5":"markdown","f26950bd":"markdown","4a01a965":"markdown","816a33a7":"markdown","2f955c8c":"markdown","b86eeee7":"markdown","f644b752":"markdown","691c4a89":"markdown","bd41e626":"markdown","0b80d266":"markdown","795a05dc":"markdown"},"source":{"23abc806":"%%time\n# Import the Rapids suite here - takes abot 2 mins\n\nimport sys\n!cp ..\/input\/rapids\/rapids.0.17.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\/site-packages\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path \n!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/","f6b15504":"import os\nimport gc\nimport sys\nimport random\nfrom tqdm.notebook import * \nimport seaborn as sns\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nimport pandas as pd,numpy as np\n\n\nimport cudf\nfrom cuml.linear_model import LogisticRegression\nfrom cuml.svm import SVC, SVR\nfrom cuml.neighbors import KNeighborsClassifier, NearestNeighbors\n\nfrom cuml.metrics import roc_auc_score\nfrom cuml.preprocessing import LabelEncoder\nfrom cuml.experimental.preprocessing import MinMaxScaler\nfrom cuml.linear_model import MBSGDClassifier as cumlMBSGDClassifier\nfrom cuml.naive_bayes import MultinomialNB\n\n\nfrom sklearn.model_selection import StratifiedKFold\n\nimport warnings\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 500)\nwarnings.simplefilter(\"ignore\")\nwarnings.filterwarnings('ignore')","9365d0f1":"DATA_PATH = \"..\/input\/widsdatathon2021\/\"\n\nNA = -9999999999\n\nTARGET = 'diabetes_mellitus'\nKEY = 'encounter_id'\nuseless = ['patient_id', 'hospital_id', 'Train','readmission_status','d1_resprate_max',\n'paco2_apache',\n'paco2_for_ph_apache',\n'ventilated_apache',\n'd1_hco3_max',\n'h1_temp_max',\n'h1_sysbp_invasive_min',\n'resprate_apache',\n'h1_mbp_noninvasive_max',\n'd1_arterial_pco2_max',\n'd1_sysbp_invasive_min',\n'apache_post_operative',\n'h1_resprate_max',\n'h1_heartrate_max',\n'h1_heartrate_min',\n'd1_arterial_pco2_min',\n'd1_lactate_max',\n'elective_surgery',\n'cirrhosis',\n'solid_tumor_with_metastasis',\n'icu_stay_type',\n'gcs_unable_apache',\n'h1_arterial_pco2_min',\n'hepatic_failure',\n'h1_lactate_max',\n'immunosuppression',\n'aids',\n'h1_resprate_min',\n'leukemia',\n'intubated_apache',\n'h1_lactate_min',\n'd1_spo2_min',\n'lymphoma',\n'd1_lactate_min',\n'h1_spo2_min',\n'map_apache',\n'h1_arterial_pco2_max',\n'd1_mbp_invasive_max',\n]","f9602d84":"os.listdir(DATA_PATH)","68a9e46c":"def read(name):\n    df = pd.read_csv(DATA_PATH+f'{name}.csv')\n    try:\n        df = df.drop('Unnamed: 0',axis=1)\n    except:\n        pass\n    return df\n\n\n\ndef converte(df,cols):\n    for c in tqdm_notebook(cols):\n        if df[c].dtypes!='object':\n            df[c] = df[c].fillna(NA)\n            df[c] = df[c].astype(int)\n            df[c] = df[c].astype(str)\n            df.loc[df[c]==str(NA),c] = 'Missing'\n    return df\n\n\ndef cat_na(train,test,cols):\n    \n    for c in tqdm_notebook(cols):\n        diff = [x for x in test[c].unique() if x not in train[c].unique() if x==x]\n        if len(diff)>0:\n            test.loc[test[c].isin(diff),c] = 'Missing'\n        \n        train_val = train[c].value_counts()\n        train_val = train_val[train_val<=10].index.tolist()\n        \n        test_val = test[c].value_counts()\n        test_val = test_val[test_val<=10].index.tolist()\n        \n        diff =  [x for x in test_val if x in train_val]\n        test.loc[test[c].isin(diff),c] = 'Missing'\n        train.loc[train[c].isin(diff),c] = 'Missing' \n            \n    return test   \n\n","7787c411":"train_df = read('TrainingWiDS2021')\ntest_df = read('UnlabeledWiDS2021')","886611d1":"train_df['Train'] = 1\ntest_df['Train'] = 0","c082ae8d":"dico = pd.read_csv(DATA_PATH+'DataDictionaryWiDS2021.csv')","19a08218":"categoricals = dico[dico['Data Type'].isin(['binary','string','integer'])]['Variable Name'].values[3:-1]\ncategoricals = [x for x in categoricals if x not in ['icu_admit_type','readmission_status','aids']+useless]\nnumericals = [x for x in train_df.columns if x not in categoricals+[KEY,TARGET]+useless]","dcb2fb16":"for dv in [train_df,test_df]:\n    dv = converte(dv,categoricals)","169dac44":"test_df = cat_na(train_df,test_df,categoricals)","09da7d0d":"data = pd.concat([train_df,test_df],axis=0).reset_index(drop=True)\ndata[categoricals] = data[categoricals].fillna('Missing')\ndata[numericals] = data[numericals].fillna(data[numericals].median())\ndata[categoricals] = data[categoricals].astype(str) ","ccc70607":"data[categoricals+numericals].isna().sum().sum()","ff47d40d":"data[\"bmi\"]=np.round((data[\"weight\"]\/(data[\"height\"])**2)*10000,6)","480d4f8e":"data = pd.get_dummies(data[[TARGET,'Train','encounter_id']+categoricals+numericals])","352fce1e":"features = [x for x in data.columns if x not in [TARGET,'Train','encounter_id']]","2c0867d5":"for c in tqdm(features):\n    data[c] = (data[c]-data[c].mean())\/data[c].std()","bf831817":"data[features].isna().sum()","21eb9045":"def run_logistic(param,dtrain,dval,test,features,target):\n    \n    X = dtrain[features]\n    y = dtrain[target]\n    \n    reg = LogisticRegression(C=param['C'],penalty=param['penalty'],fit_intercept=param['fit_intercept'],output_type=param['output_type'])\n    reg.fit(X,y)\n    \n    oof = reg.predict_proba(dval[features])[:,1]\n    pred_test = reg.predict_proba(test[features])[:,1]\n    \n    return oof,pred_test\n\ndef run_svm(param,dtrain,dval,test,features,target):\n    \n    X = dtrain[features]\n    y = dtrain[target]\n    \n    reg = SVC(kernel=param['kernel'], degree=param['degree'], C=param['C'],tol = param['tol'],class_weight= param['class_weight'],\n              output_type=param['output_type'],probability=param['output_type'])\n    reg.fit(X,y)\n    \n    oof = reg.predict_proba(dval[features])[:,1]\n    pred_test = reg.predict_proba(test[features])[:,1]\n    \n    return oof,pred_test\n\n\ndef run_knn(param,dtrain,dval,test,features,target):\n    \n    X = dtrain[features]\n    y = dtrain[target]\n    \n    reg = KNeighborsClassifier(n_neighbors=param['n_neighbors'],output_type=param['output_type'])\n    reg.fit(X,y)\n    \n    oof = reg.predict_proba(dval[features])[:,1]\n    pred_test = reg.predict_proba(test[features])[:,1]\n    \n    return oof,pred_test\n\ndef run_xgb(param,dtrain,dval,dtest,features,target,num_round,es):\n    \n    trn_data = xgb.DMatrix(dtrain[features], label=dtrain[target])\n    val_data = xgb.DMatrix(dval[features], label=dval[target])\n    \n    evallist = [(trn_data, 'train'),(val_data, 'validation')]\n    dpred = xgb.DMatrix(dtest[features])\n    \n    bst = xgb.train(param, trn_data, num_round, evallist,\n                            early_stopping_rounds=es,verbose_eval=10)\n    \n    pred_test = bst.predict(dpred)\n    pred_oof = bst.predict(val_data)\n    \n    return pred_oof, pred_test","c464e266":"train_df = data[data.Train==1].reset_index(drop=True)\ntest_df = data[data.Train==0].reset_index(drop=True)","d5b0de92":"param = {'C':1,\n         'penalty':'l2',\n         'fit_intercept':True,\n         'learning_rate':'constant',\n         'eta0':0.001,\n         'epochs':1500,\n         'batch_size':1,\n         'n_iter_no_change':5,\n         'loss':'squared_loss',\n         'shuffle':True,\n         'tol':1e-5,\n         'alpha':0.5,\n         'fit_prior':True,\n         'smoothing':0.1,\n         'output_type':\"numpy\",\n         'probability':True,\n         'class_weight':'balanced',\n         'degree':3,\n         'kernel':'rbf',\n         'n_neighbors':10\n         \n        }\n\nparam_xgb = {\n         'tree_method': 'gpu_hist',\n         'n_gpus': 1, \n         'max_depth' :8,\n         'gamma':1.5,\n         'lambda':5,\n         'colsample_bytree':0.5,\n         'subsample':1,\n         'min_child_weight':8,\n         'learning_rate':0.02,\n         'eval_metric': 'auc',\n         'objective': 'binary:logistic'}\n\n\n# param_xgb2 = {\n#          'tree_method': 'gpu_hist',\n#          'n_gpus': 1, \n#          'max_depth' :3,\n#          'colsample_bytree':0.95,\n#          'subsample':1,\n#          'min_child_weight':5,\n#          'learning_rate':0.02,\n#          'eval_metric': 'auc',\n#          'objective': 'binary:logistic'}\n\n\n\nclass Config:\n    num_round = 10_000\n    es = 100\n    target = TARGET\n    features = features\n    \n    k = 5\n    random_state = 42\n    selected_folds = [0,1,2,3,4,5]\n    \n    name = 'ml_rapids_'\n    param = param\n    param_xgb = param_xgb","1fb7b8f4":"oof, pred_test = run_svm(param,train_df,train_df,test_df,features,TARGET)","a1c76d1c":"def k_fold(config,train,test):\n\n\n    skf = StratifiedKFold(n_splits=config.k, random_state=config.random_state)\n    split = list(skf.split(X=train[config.features], y=train[config.target]))\n\n    param = config.param\n    \n    preds_xgb = []\n#     preds_xgb2 = []\n    preds_svm = []\n    preds_lr = []\n    preds_knn = []\n    \n    for c in ['xgb','svm','knn','lr']: #,'xgb2'\n        train[config.name+c] = 0\n    \n    for i, (train_idx, val_idx) in enumerate(split):\n        print(f\"\\n-------------   Fold {i + 1} \/ {config.k}  -------------\\n\")\n\n        ids = train.iloc[val_idx]['encounter_id'].values\n        \n        dtrain = train.iloc[train_idx]\n        dval = train.iloc[val_idx]\n        \n        # Xgb\n        oof_xgb,pred_xgb = run_xgb(config.param_xgb,dtrain,dval,test,config.features,config.target,config.num_round,config.es)\n        \n        # Xgb\n#         oof_xgb2,pred_xgb2 = run_xgb(param_xgb2,dtrain,dval,test,config.features,config.target,config.num_round,config.es)\n        \n        #SVM\n        oof_svm,pred_svm = run_svm(param,dtrain,dval,test,config.features,config.target)\n        print(f\"\\n-------------   Oof auc SVM : {round(roc_auc_score(dval[config.target].values,oof_svm),5)}-------------\\n\")\n        #KNN\n        oof_knn,pred_knn = run_knn(param,dtrain,dval,test,config.features,config.target)\n        print(f\"\\n-------------   Oof auc KNN : {round(roc_auc_score(dval[config.target].values,oof_knn),5)}-------------\\n\")\n        #logistic\n        oof_lr,pred_lr = run_logistic(param,dtrain,dval,test,config.features,config.target)\n        print(f\"\\n-------------   Oof auc LR : {round(roc_auc_score(dval[config.target].values,oof_lr),5)}-------------\\n\")\n        \n        for c ,v in zip(['xgb','svm','knn','lr'],[oof_xgb,oof_svm,oof_knn,oof_lr]): #,'xgb2',oof_xgb2\n            train.loc[train['encounter_id'].isin(ids),config.name+c]=v\n        \n        preds_xgb.append(pred_xgb)\n#         preds_xgb2.append(pred_xgb2)\n        preds_svm.append(pred_svm)\n        preds_lr.append(pred_lr)\n        preds_knn.append(pred_knn)\n        \n    preds_xgb = np.mean(preds_xgb,axis=0)\n#     preds_xgb2 = np.mean(preds_xgb2,axis=0)\n    preds_svm = np.mean(preds_svm,axis=0)\n    preds_lr = np.mean(preds_lr,axis=0)\n    preds_knn = np.mean(preds_knn,axis=0)\n    \n    for c ,v in zip(['xgb','svm','knn','lr'],[preds_xgb,preds_svm,preds_lr,preds_knn]): #,'xgb2',preds_xgb2\n        print(f\"\\n-------------   Oof auc {c} : {round(roc_auc_score(train[config.target].values,train[config.name+c].values),5)}-------------\\n\") \n        test[config.name+c] = v\n    \n    return train,test","97b55eba":"config = Config","bca633a9":"del data\ngc.collect()","16f4d01e":"train_df,test_df = k_fold(config,train_df,test_df)","0c25bc0a":"version = [f'ml_rapids_{c}' for c in ['xgb','svm','knn','lr']] #,'xgb2'\nweight = [0.7,0.1,0.0125,0.0125,0.15]","b4f4262f":"test_df[TARGET] = 0\nfor w,v in zip(weight,version):\n    test_df[TARGET]+= w*test_df[v]","5bc30154":"test_df[[KEY,TARGET]].to_csv('submission.csv',index=False)","e83a7441":"This is just an indicator to remember which rows are from the training or the test set.","22c2d4bc":"### Woohoo ! Time to run an example test...","2b6ff014":"# Read and Process Data","69f685e2":"## Check my previous notebook on a similar topic : https:\/\/www.kaggle.com\/louise2001\/rapids-feature-importance-is-all-you-need","a63ba4cc":"## In this notebook, I will show you how to run your pipeline end-to-end on GPU, thus benefiting from its amazing calculation performances while simplifying the procedure by running everything on a single device.","65495241":"### Time to save our results !","b78f77eb":"### Hereunder I define the parameters I will be using throughout this notebook.","ba4d9fe5":"### Preprocessing is one of the most crucial parts. Here are some basic functions that will help you read data and fill NaNs.","f26950bd":"# Be fast !","4a01a965":"# Thanks for reading ! Don't forget to leave an upvote... Happy kaggling !","816a33a7":"### I am running a KFold validation to get more accurate, reliable overall results. The good thing with running on GPU is that no matter how many folds you run, it will never take you the night :","2f955c8c":"### Preprocessing of categorical variables is always kind of touchy...","b86eeee7":"## First step : setup your RAPIDS environment. \n\n### RAPIDS are a suite of packages that enable you to run all your favorite procedures, based on pandas, sklearn etc, entirely on GPU.","f644b752":"![logan-mayer-T1FbDy-t7So-unsplash.jpg](attachment:logan-mayer-T1FbDy-t7So-unsplash.jpg)\n<span> <center>Photo by <a href=\"https:\/\/unsplash.com\/@loganmayerr?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Logan Mayer<\/a> on <a href=\"https:\/\/unsplash.com\/s\/photos\/rapids?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash<\/a> <\/center><\/span>","691c4a89":"### It has become so easy now to get amazing NVIDIA GPUs to run your models that you can always try bigger and bigger, achieving amazing performances in no time.\n\n## However, I always find that pre- and post-processing take a huuuuuge amount of the total running time of my pipeline.\n\n### It's not even the hard part ! Nothing like updating millions of weights, very often there are only some basic transformations, filtering and cleaning. What takes long is mostly the copying back and forth between CPU and GPU, and the poor CPU performance compared to GPU even on simple sklearn models.\n\n## Still, it would be a fatal error to neglect those steps... Very often, they create the decisive improvement that can help your model learn loads better and thus lead you towards victory.","bd41e626":"### Now, time to get serious. Here are some essential sklearn analysis I always like to run to get a proper handle of the data, running on GPU so that it's more efficient.","0b80d266":"### Yessss ! Now our data is all clean ! Time to start some feature engineering...","795a05dc":"# Who wants to run a pipeline end-to-end at lightning speed ?"}}