{"cell_type":{"20db3231":"code","12d60172":"code","de20d682":"code","ae09ff76":"code","007681ab":"code","2b7c3e83":"code","67151660":"code","5aff1e75":"code","cc0f18d1":"code","3ecd5816":"code","500fcc53":"code","0f3c7a5e":"code","685877d1":"code","3687f02a":"code","db976d05":"code","6416f6e3":"code","18b51368":"code","d7fe67ed":"markdown","c7f89eb3":"markdown","471d0c45":"markdown","b7c103a8":"markdown","555475ff":"markdown","894f2bbe":"markdown","4e674ea4":"markdown","a72924b6":"markdown","9dfbdf30":"markdown","1cf81c00":"markdown","6fb4bb78":"markdown","dae4ba45":"markdown","d22acc5f":"markdown","81dd9489":"markdown","9eb775eb":"markdown","cdfb542f":"markdown","d74fed98":"markdown","dc807826":"markdown","2bd6ec12":"markdown","b27070f0":"markdown"},"source":{"20db3231":"# import dependencies\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom keras.utils import plot_model\nfrom IPython.display import Image\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\nSEED = 999\nnp.random.seed(SEED)\nsns.set(style='white', context='notebook', palette='pastel')","12d60172":"X_train = pd.read_csv('..\/input\/train.csv')\nX_test = pd.read_csv('..\/input\/test.csv')","de20d682":"Y_train = X_train['label']\nX_train = X_train.drop(labels=['label'], axis=1) \n\nprint(X_train.isnull().any().describe())\n\n# normalize\nX_train = X_train \/ 255.0\nX_test = X_test \/ 255.0\n\n# resize\nX_train = X_train.values.reshape(-1,28,28,1)\nX_test = X_test.values.reshape(-1,28,28,1)","ae09ff76":"# basic frequency plots\ng = sns.countplot(Y_train)\nY_train.value_counts()","007681ab":"# preview an image\nprint('Label:', Y_train[0])\ng = plt.imshow(X_train[0][:,:,0])","2b7c3e83":"# one-hot encoding\nY_train = to_categorical(Y_train, num_classes=10)\n\n# Split the data into training and validation sets\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train,\n                                                  test_size=0.1, random_state=SEED)","67151660":"def create_models(num_nets):\n    models = [0] * num_nets\n\n    for i in range(num_nets):\n        models[i] = Sequential()\n\n        models[i].add(Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu',\n                     input_shape=(28,28,1)))\n        models[i].add(BatchNormalization())\n        models[i].add(Conv2D(filters=32, kernel_size=(5,5), padding='same', activation='relu'))\n        models[i].add(BatchNormalization())\n        models[i].add(MaxPool2D(pool_size=(2,2)))\n        models[i].add(Dropout(0.25))\n\n        models[i].add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))\n        models[i].add(BatchNormalization())\n        models[i].add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))\n        models[i].add(BatchNormalization())\n        models[i].add(MaxPool2D(pool_size=(2,2)))\n        models[i].add(Dropout(0.25))\n\n        models[i].add(Flatten())\n        models[i].add(Dense(256, activation='relu'))\n        models[i].add(Dropout(0.4))\n        models[i].add(Dense(10, activation='softmax'))\n    \n        models[i].compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n        print('Compiled model {}.'.format(i+1))\n    \n    return num_nets, models","5aff1e75":"num_nets, models = create_models(num_nets=7)","cc0f18d1":"plot_model(models[0], to_file='model.png', show_shapes=True, show_layer_names=True)\nImage('model.png')","3ecd5816":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience=3, verbose=1, \n                                            factor=0.5, min_lr=0.00001)","500fcc53":"datagen = ImageDataGenerator(rotation_range=10, zoom_range=0.10, \n                             width_shift_range=0.10, height_shift_range=0.10)","0f3c7a5e":"def train_models(models, epochs):\n    num_nets = len(models)\n    history = [0] * num_nets\n    \n    for i in range(num_nets):\n        X_train2, X_val2, Y_train2, Y_val2 = train_test_split(X_train, Y_train, test_size=0.1)\n        history[i] = models[i].fit_generator(datagen.flow(X_train2, Y_train2, batch_size=64),\n                                            epochs=epochs, steps_per_epoch=X_train2.shape[0]\/\/64,  \n                                            validation_data=(X_val2,Y_val2),\n                                            callbacks=[learning_rate_reduction], verbose=0)\n        \n        print('CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}'.format(\n            i+1, epochs, max(history[i].history['accuracy']), max(history[i].history['val_accuracy'])))\n    \n    return models, history","685877d1":"models, history = train_models(models=models, epochs=18)","3687f02a":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment='center',\n                 color='white' if cm[i, j] > thresh else 'black')\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n\n# Predict the values from the validation dataset \nY_pred = np.zeros((X_val.shape[0], 10)) \n\nfor i in range(num_nets):\n    Y_pred = Y_pred + models[i].predict(X_val)\n    \nY_pred_classes = np.argmax(Y_pred, axis=1)\n\n# Convert validation observations to one hot vectors\nY_true = np.argmax(Y_val, axis=1) \n\n# Compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes)\n\n# Plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes=range(10)) ","db976d05":"# Errors are difference between predicted labels and true labels\nerrors = (Y_pred_classes - Y_true != 0)\n\nY_pred_classes_errors = Y_pred_classes[errors]\nY_pred_errors = Y_pred[errors]\nY_true_errors = Y_true[errors]\nX_val_errors = X_val[errors]\n\ndef display_errors(errors_index,img_errors,pred_errors, obs_errors):\n    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n    n = 0\n    nrows = 2\n    ncols = 3\n    fig, ax = plt.subplots(nrows, ncols, sharex=True, sharey=True)\n    plt.subplots_adjust(hspace=2)\n    for row in range(nrows):\n        for col in range(ncols):\n            error = errors_index[n]\n            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n            n += 1\n\n# Probabilities of the wrong predicted numbers\nY_pred_errors_prob = np.max(Y_pred_errors, axis=1)\n\n# Predicted probabilities of the true values in the error set\ntrue_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n\n# Difference between the probability of the predicted label and the true label\ndelta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n\n# Sorted list of the delta prob errors\nsorted_dela_errors = np.argsort(delta_pred_true_errors)\n\n# Top 6 errors \nmost_important_errors = sorted_dela_errors[-6:]\n\n# Show the top 6 errors\ndisplay_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)","6416f6e3":"results = np.zeros((X_test.shape[0], 10)) \n\nfor i in range(num_nets):\n    results = results + models[i].predict(X_test)\n    \nresults = np.argmax(results, axis=1)\nresults = pd.Series(results, name='Label')","18b51368":"submission = pd.concat([pd.Series(range(1, X_test.shape[0]+1), name='ImageId'), results], axis=1)\nsubmission.to_csv('ensemble_submission.csv', index=False)","d7fe67ed":"### 3.3 Learning rate annealer\n\nThe learning rate is the step by which the optimizer walks through the 'loss landscape'. The higher LR, the bigger are the steps and the quicker is the convergence. However the sampling is very poor with a high learning rate, and the optimizer could fall into a local minimum.\n\nIn order to make the optimizer converge faster and more closely to the global minimum of the loss function, we can use an *annealer*. To keep the advantage of the faster computation time with a higher learning rate, we can decrease it dynamically every X steps (epochs) but only if necessary (when accuracy is not improved).\n\nWith the ReduceLROnPlateau function from keras.callbacks, we reduce the learning rate by half if the validation set accuracy is not improved after 3 epochs.","c7f89eb3":"# 4. Model training","471d0c45":"Train and test images (28px x 28px) are now stored as a pandas DataFrame of 1D vectors with length 784 (=$28^2$). We reshape these vectors to 3D matrices with shape 28x28x1. This third dimension of 1 corresponds to the channel -- MNIST images are grayscale so we need only one channel.","b7c103a8":"In order to avoid overfitting, we need to artificially expand our handwritten digit dataset. The idea is to alter the training data with small transformations to reproduce the variations occuring when someone is writing a digit.\n\nFor example:\n* The number is not centered \n* The scale is not the same (some who write with big\/small numbers)\n* The image is rotated\n\nApproaches that alter the training data in ways that change the array representation while keeping the label the same are known as data augmentation techniques. Some popular augmentations people use are grayscales, horizontal flips, vertical flips, random crops, color jitters, translations, rotations, and much more. \n\nBy applying just a couple of these transformations to our training data, we can easily double or triple the number of training examples and create a very robust model.\n\nFor data augmentation, we will:\n   - Randomly rotate some training images by 10 degrees\n   - Randomly zoom by 10% some training images\n   - Randomly shift horizontally by 10% of the total image width\n   - Randomly shift vertically by 10% of the total image height\n   \nThe parameters `vertical_flip` and `horizontal_flip` don't make sense to apply here because of the symmetrical relationship of some numbers like 6 and 9 -- we don't want to mix them up.","555475ff":"# 1. Introduction\n\nIn this notebook we will use an ensemble of sequential convolutional neural networks for digit recognition on the MNIST dataset. Ensembling, through a simple majority vote, allows us to improve our model's accuracy while greatly reducing the variance of the predictions. The Keras framework presents a very intuitive and user-friendly interface with which we can build and customize our neural networks. Training is done via CPU, but can be much faster by taking advantage of GPU resources.\n\nThis Notebook follows three main parts:\n* Data preparation\n* CNN modeling and evaluation\n* Results prediction and submission\n\n\nA preview of the MNIST dataset:\n\n<img src=\"https:\/\/corochann.com\/wp-content\/uploads\/2017\/02\/mnist_plot.png\" ><\/img>","894f2bbe":"# Convolutional Neural Networks with Keras\n\nStarted with Yassine Ghouzam's kernel, [Introduction to CNN Keras](https:\/\/www.kaggle.com\/yassineghouzam\/introduction-to-cnn-keras-0-997-top-6). Yassine, many thanks and great work! I wanted to build on his work by adding some more commentary (so that I could learn, but hopefully you do too!), and of course to make my own tweaks and experiment a little.\n \nStill need to try:\n - Ranger optimizer\n\n### Table of Contents\n1. Introduction\n1. Data preparation\n1. Model building\n1. Model training\n1. Prediction and submission","4e674ea4":"Now that our models are ready, we train:","a72924b6":"### 2.2 Organize and examine","9dfbdf30":"### 2.4 Prepare for training\n\nWe split the original `X_train` dataset in two parts: a small fraction (10%) for the validation set and the rest (90%) is used to train the model.\n\nSince we have 42,000 training images of balanced labels, a random split of the training set doesn't cause some labels to be over represented in the validation set. Be careful with unbalanced datasets as a simple random split could cause inaccurate evaluation during the validation. To avoid that, you could use the `stratify=True` option of `train_test_split`, or look into the ADASYN and SMOTE over-sampling algorithms. ","1cf81c00":"**Visualize our neural network**\n\nThanks to [Satonio](https:\/\/www.kaggle.com\/acleon\/the-acleon-s-first-competion) for illustrating this - a pretty cool feature!","6fb4bb78":"**Plot top losses**","dae4ba45":"### 3.2 Creating an ensemble of models\n\nChris Deotte has a very good [kernl](https:\/\/www.kaggle.com\/cdeotte\/how-to-choose-cnn-architecture-mnist) that makes use of ensembling, and it is both very informative and powerful. I encourage you to check it out if you haven't already. He also has [another one](https:\/\/www.kaggle.com\/cdeotte\/how-to-choose-cnn-architecture-mnist) with more insights on how to decide on your neural network's architecture. I decided to reduce the number of models from 15 to save training time, and since it seemed a bit excessive to have so many - though, it can obviously achieve some very good results!","d22acc5f":"### 2.3 Preview\n\nWe can get a better sense for one of these examples by visualizing the image and looking at the label:","81dd9489":"# 2. Data preparation\n### 2.1 Load the data","9eb775eb":"### 3.4 Data augmentation ","cdfb542f":"The Keras Sequential API allows us to add one layer at a time, starting from the input layer. Combining convolutional and pooling layers, CNNs are able to learn local features of the image as well as learn more global context information.\n\nA Conv2D layer is like a set of learnable filters, where each filter transforms a part of the image (defined by the kernel size) using a kernel filter matrix. This matrix is applied on the whole image and can be seen as a transformation of the image. The CNN can then isolate features that are useful everywhere from these transformed images (feature maps). I chose to set 32 filters for the two first conv2D layers and 64 filters for the two last ones.\n\nOn the other hand, a pooling (MaxPool2D) layer simply acts as a downsampling filter. It looks at the 2 neighboring pixels and picks the maximal value. These are used to reduce computational cost, and to some extent also reduce overfitting. We specify the pooling size (i.e the area size pooled each time) -- the greater the pooling area, the greater the loss of information. Having pooling area and strides of 2x2 is a very common choice.\n\nDropout is a regularization method where a proportion of nodes in the layer are randomly ignored (setting their weights to zero) for each training sample. This random dropping of nodes in the network forces it to learn features in a more distributed way, improving generalization and reducing overfitting. \n\n`relu`, short for Rectified Linear Unit, is the activation function max(0, x). It is used to add non linearity to the network. \n\nThe Flatten layer is use to convert the final feature maps into a one single 1D vector. This flattening step is needed so that you can make use of fully connected layers after some convolutional\/maxpool layers. It combines all the found local features of the previous convolutional layers.\n\nAn optimizer will iteratively improve parameters (filters kernel values, weights and bias of neurons, etc.) in order to minimize the loss. The Adam optimizer in particular combines the features of RMSProp and momentum, and has proven to be a very popular and efficient option.\n\nIn the end I used the features in two fully-connected (Dense) layers which is just an artificial neural networks (ANN) classifier. In the last layer, `Dense(10, activation='softmax')` uses the softmax activation function to output the probabilities for each class, with `10` indicating the dimension of our output (for the 10 possible digits).\n\nBecause each net's weights are randomly initialized, we can know that adding additional neural networks will have some natural variation in their results and not simply all calculate identical results.","d74fed98":"# 5. Prediction and submission\n### Ensembling our results","dc807826":"### 4.1 Evaluation\n\n**Confusion matrix**","2bd6ec12":"# 3. Model building\n### 3.1 Define the model","b27070f0":"### Submission"}}