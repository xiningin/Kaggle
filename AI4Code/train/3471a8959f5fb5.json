{"cell_type":{"89473230":"code","783fb8d8":"code","154726a0":"code","175473e1":"code","29972d02":"code","02a36a88":"code","c4a81329":"code","0317a5fb":"code","ea2a079a":"code","10a9e25a":"code","d53f8f5d":"code","26d680a5":"code","3334facc":"code","b954d322":"code","5bf54193":"code","b0165142":"code","05d6d70a":"code","deb51371":"code","713fce25":"code","960f2e99":"markdown","489c1b00":"markdown","92f4b274":"markdown","ef9f15cc":"markdown","2ac18cef":"markdown","4f85868c":"markdown","c29c5215":"markdown","a9829b21":"markdown","c41e304d":"markdown","33d1d02b":"markdown","50d02867":"markdown","72b56f92":"markdown","768f7bae":"markdown","01839121":"markdown","457892b8":"markdown","e7d74364":"markdown","1b6c938c":"markdown","d4f95551":"markdown","451b60c0":"markdown","c86ec9e1":"markdown","4e8a7e5a":"markdown","42a986e8":"markdown","88574e55":"markdown","55cc5429":"markdown","28d5126f":"markdown","e183b40d":"markdown","22145184":"markdown","d986e88f":"markdown","4eb98203":"markdown","e5b33873":"markdown","85d1318e":"markdown","816dc991":"markdown","0767c6ce":"markdown"},"source":{"89473230":"import pandas as pd\nimport math\nimport seaborn as sns\nfrom datetime import datetime, timedelta\nimport numpy as np\nimport re\nimport matplotlib.pyplot as plt\nimport os\nfrom random import randint, seed\n\nfrom sklearn.utils import resample\nimport sklearn.model_selection\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn import preprocessing\nfrom tqdm import tqdm_notebook\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, roc_auc_score, auc\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\n\n#utility\nfrom IPython.display import display, HTML\nimport itertools\n\npd.options.display.float_format = '{:,.3f}'.format","783fb8d8":"path='..\/input\/creditcardfraud\/'\ndf = pd.read_csv(path + 'creditcard.csv')\n# create minutes and drop seconds\ndf['Hours'] = df.Time\/3600\ndf.drop(['Time'], axis=1, inplace=True)\ndf.head()","154726a0":"# Barplots\ndef make_bar(var, cutoff, head, colour):\n    p = pd.cut(var, bins = cutoff)\n    p.value_counts(sort=False, normalize= True).plot(kind='bar', color= colour)\n    plt.title(head)\n    plt.xlabel('Categories')\n    plt.ylabel('Frequency')\n    \nf, axes = plt.subplots(figsize = (15, 10))\nplt.subplot(121) \nax = sns.distplot(df.Hours)\nax.set_title('Distribution of Hours')\nplt.subplot(122)\nmake_bar(df['Hours'], [df.Hours.min(), 12, 24, 36, df.Hours.max()], 'Hours Barplot', 'blue')","175473e1":"plt.figure(figsize=(15, 10))\nmake_bar(df['Amount'], [df.Amount.min(), 1, 10, 100, 500, df.Amount.max()], 'Money Amount', 'green')","29972d02":"# correlation between features\n# whitegrid\nsns.set_style('whitegrid')\n#compute correlation matrix...\ncorr_matrix=df.drop('Class', axis=1).corr(method='pearson')\n# delete correlations between components (all the columns starting with V), because by definitions they are uncorrelated (corr=0)\ncmatrix_reduced = corr_matrix.loc[:, ~corr_matrix.columns.str.contains('V.*')]\n\nplt.figure(figsize=(40, 30))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(cmatrix_reduced, cmap=cmap, linewidth=0.5, square=True, vmin=-1, vmax=1, annot=True, cbar_kws={\"shrink\": .5})","02a36a88":"percentage = round(len(df[df.Class == 1])\/len(df)*100, 3)\nprint(f'Percentage of frauds: {percentage}%')","c4a81329":"#separazione dati training e test in modo stratificato\ntest_size = 0.20\n#Balancing parameters for building our bagging datasets\nperc_1 = 0.05\nperc_0 = 1 - perc_1\n\ny = df.Class.to_frame()\ndf_train, df_test = sklearn.model_selection.train_test_split(df, test_size=test_size, random_state=123, stratify=y)\nX_train = df_train.drop('Class', axis=1)\n\nX_test = df_test.drop('Class', axis=1)\ny_train = df_train.Class\ny_test = df_test.Class\n\nprint('Number of cases for X_train: ', len(X_train))\nprint('Number of cases for X_test: ', len(X_test))","0317a5fb":"# undersampling for unbalanced dataset\ndef do_UNDERSAMPLING(df, perc_1 = 0.05, perc_0 = 0.95, random_state=123):\n    df_negative = df[df.Class==0]\n    df_positive = df[df.Class==1]\n    size_1= df_positive.shape[0]\n    size_0 = df_positive.shape[0] * perc_0\/perc_1\n    \n    # Downsample majority class\n    df_negative_downsampled = resample(df_negative, \n                                     replace=False,    # sample without replacement\n                                     n_samples=math.ceil(size_0),     # to match minority class\n                                     random_state=random_state) # for reproducible results\n\n    # Combine minority class with downsampled majority class\n    df_downsampled = pd.concat([df_negative_downsampled, df_positive])\n    X_downsampled = df_downsampled.drop('Class',axis=1)\n    y_downsampled = df_downsampled.Class\n    return df_downsampled, X_downsampled, y_downsampled\n\n\n#variable to choose the balancing method for the test set\ntest_bil = 'Complete' #0.01% fraud out of the total data = completed vs 5% frauds = Balanced \n\n#undersampling if we want to Balance the test set\nif test_bil == 'Balanced':\n    X_test['Class'] = y_test\n    df_temp , X_test, y_test = do_UNDERSAMPLING(X_test, perc_1, perc_0, random_state=123)","ea2a079a":"#scale to unitary variance and zero mean\ndef scale(X_train):\n    numerics=['float64']\n    temp = X_train.copy()\n    numeric_feat=temp.select_dtypes(include=numerics)\n    scaled_cols=numeric_feat.columns.values\n    scaler = preprocessing.StandardScaler().fit(numeric_feat)\n    temp.loc[:,scaled_cols]=scaler.fit_transform(temp.loc[:,scaled_cols])\n    return temp, scaler, scaled_cols\n\n#scaling of our train dataset, mean = 0 and unitary variance\nscaled_df_train, std_scaler, scaled_cols = scale(X_train)\n# scale also x test\nX_test.loc[:,scaled_cols]=std_scaler.transform(X_test.loc[:,scaled_cols])","10a9e25a":"n_models = 6 #n of RF in the ensemble\n\n# vectors containing training datasets for every model\nx_trains = []\ny_trains = []\n\nX_train['Class'] = y_train\n \nsampling = 'UNDERSAMPLING'\n\n# seed selection to get reproducible results\nseed(a=123)\nseeds = [randint(0,999) for i in range(n_models)]\n\n#creating dataset using undersampling\nfor i in range(n_models):\n    df_temp , X_temp, y_temp = do_UNDERSAMPLING(X_train, perc_1, perc_0, random_state=seeds[i])\n    x_trains.append(X_temp)\n    y_trains.append(y_temp)","d53f8f5d":"print('Dummy results if you always predict one:')\nprint('\\tPrecision: {}'.format(precision_score(np.ones(len(y_test)), y_test)))\nprint('\\tRecall: {}'.format(recall_score(np.ones(len(y_test)), y_test)))\nprint('\\tF1-score: {}'.format(recall_score(np.ones(len(y_test)), y_test)))","26d680a5":"imp_thr = 12 #n of variables to keep when applying the RF feature selection\n\ndef rf_importance(X_train, y_train, imp_thr=imp_thr):\n    forest = RandomForestClassifier(n_estimators=250, random_state=123)\n    num_df = X_train.select_dtypes('float64','int')\n    str_df = X_train.select_dtypes('object')\n    forest.fit(num_df, y_train)\n    importances = forest.feature_importances_\n    data = {'variables': num_df.columns.values, 'importance': importances}\n    importance_df = pd.DataFrame.from_dict(data)\n    importance_df.sort_values(by='importance', ascending=False, inplace=True)\n    \n    if imp_thr > 0: \n        importance_df=importance_df.head(imp_thr)\n    \n    num_df = num_df[np.asarray(importance_df.variables)]\n    X_train=pd.concat([str_df, num_df], axis=1)\n    return  X_train, importance_df","3334facc":"# evaluation metrics\nscores_functions = {\n    'precision': precision_score,\n    'recall': recall_score,\n    'f1': f1_score\n}\n\ncorr_thr = 0.80 #correlation threshold\nscores = list(scores_functions.keys())\n\n#configuration parameters\nfixed_params = {\n    'p_test_size': test_size,\n    'p_train_size':  1 - test_size,\n    'p_s_perc_1': perc_1,\n    'p_s_perc_0': perc_0,\n    'p_corr_thr': corr_thr,\n    'p_importance_thr': imp_thr,\n    'p_sampling': sampling,\n    'p_bilanciam_test': test_bil,\n    'p_n_models': n_models\n}\n\n#n of folds for cross-validation\ncv_splits = 5","b954d322":"model_name = 'final_test'\n# the final output will be saved on an Excel File named filename+model_name\nmodel = RandomForestClassifier\n\n#gridsearch configuration for the training set\ntuned_parameters = [{'bootstrap': [False],\n 'max_depth': [10,20],\n 'max_features': ['log2'],\n 'min_samples_leaf': [1, 2, 4],\n 'min_samples_split': [2, 3, 5],\n 'n_estimators': [10,20,25,30]}]\n\ndf_res = pd.DataFrame()\n\n#inside these arrays we will store the results of the cv from the n-models  \nbest_models = []\npredictions = []\npredictions_proba = []\n\nprint('* * * Starting model fitting')\nfor i in range(n_models):\n    \n    #each model reads a dedicated part of the total train dataset\n    x_t = x_trains[i].copy()\n    y_t = y_trains[i].copy()\n    #RF for feature selection on the train set\n    x_train, _ = rf_importance(x_t, y_t, imp_thr=imp_thr)\n    x_train = x_train.select_dtypes('float64','int')\n    y_train = y_trains[i]\n    \n    # corresponding feature selection on the test set\n    x_test = X_test[x_train.columns.tolist()]\n    \n    # gridsearch for the tuning of the parameters \n    print('* * * Starting gridsearch for the {}-th model...'.format(i))\n    clf = GridSearchCV(model(random_state=123), tuned_parameters, cv=StratifiedKFold(n_splits=cv_splits, random_state=123),\n                       scoring=scores, n_jobs=-1, refit='precision', return_train_score=True, verbose=True)\n\n    clf.fit(x_train, y_train)\n    print('* * * Gridsearch end')\n\n    #best model\n    best_est = clf.best_estimator_\n    #best configuration\n    best_par = clf.best_params_\n    \n    #for each model gridsearch identifies the best model, which is then stored in the dedicated array\n    best_models.append(best_est)\n    \n    dict_res = {} #dictionary containing all the configuration parameters\n    dict_res.update(fixed_params)\n    \n    #creating dataframe to save it on excel later on\n    for i in tqdm_notebook(range(len(clf.cv_results_['params']))):\n        params = clf.cv_results_['params'][i]\n        #saving CV results of the best model\n        if params == best_par:\n            \n            #saving all the CV metrics\n            for score in scores:\n                dict_res['cv_mean_test_{}'.format(score)] = clf.cv_results_['mean_test_{}'.format(score)][i]\n                dict_res['cv_std_test_{}'.format(score)] = clf.cv_results_['std_test_{}'.format(score)][i]\n                dict_res['cv_mean_train_{}'.format(score)] = clf.cv_results_['mean_train_{}'.format(score)][i]\n                dict_res['cv_std_train_{}'.format(score)] = clf.cv_results_['std_train_{}'.format(score)][i]\n            #saving model parameters\n            for key in params.keys():\n                dict_res['model_param_{}'.format(key)] = params[key]\n    \n    #prediction using the best classifiers out of the gridsearch\n    y_pred = best_est.predict(x_test)\n    \n    predictions.append(y_pred) #predictions 0\/1\n    predictions_proba.append(best_est.predict_proba(x_test)[:,1]) #probability predictions\n    \n    df_res = df_res.append(dict_res, ignore_index=True)\nprint('* * * Models fitting over')","5bf54193":"# Choose the method of aggregation (majority voting\\mean probability scores)\naggregation_method = 'PROBA' # or 'VOTING' as alternative\n\nth = 0.65 # probability threshold used for the voting section\n\nif aggregation_method == 'VOTING':   \n    df_res['aggregation'] = 'VOTING'\n    y_sum = np.zeros(len(y_test))\n    for i in range(n_models):\n        y_sum = y_sum + predictions[i]\n        \n    final_pred = (y_sum >= np.ceil(n_models\/2)).astype(int)\n    \nelif aggregation_method == 'PROBA':\n    df_res['aggregation'] = 'PROBA_{}'.format(th)\n    mean_prob = np.mean(predictions_proba, axis=0)\n    final_pred = (mean_prob > th).astype(int)\n\n# save test results\nfor score in scores:\n        metric = scores_functions[score]\n        df_res['test_{}'.format(score)] = metric(y_true=y_test, y_pred=final_pred)\n\ndf_res.head()","b0165142":"#confusion matrix function\ndef plot_confusion_matrix(cm, classes,\n                         normalize=False,\n                         title='Confusion matrix',\n                         cmap=plt.cm.Blues):\n    \"\"\"\n   This function prints and plots the confusion matrix.\n   Normalization can be applied by setting `normalize=True`.\n   \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                horizontalalignment=\"center\",\n                color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \n\nplot_confusion_matrix(confusion_matrix(y_true=y_test, y_pred=final_pred), classes=['0','1'])","05d6d70a":"# Print mean values stored in df_res df\nprint('Precision on the test set: ', round(df_res['test_precision'].mean(), 3))\nprint('Recall on the test set: ', round(df_res['test_recall'].mean(), 3))\nprint('F1 on the test set: ', round(df_res['test_f1'].mean(), 3))","deb51371":"fpr, tpr, thresholds = roc_curve(y_test, np.mean(predictions_proba, axis=0))\nauc = roc_auc_score(y_test, np.mean(predictions_proba, axis=0))\n\nplt.plot(fpr, tpr)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.show()\nprint('Area Under Curve: {:.3}'.format(auc))","713fce25":"append_results = False #overwrite results if append_result = False\nfilename = 'output'\n\noutput = '{}_{}.xlsx'.format(filename, model_name)\nif append_results:\n    if(os.path.exists(output)):\n        print('* * * Writing file {}'.format(output))\n        df_file = pd.read_excel(output)\n        df_fin = pd.concat([df_file, df_res])\n        df_fin.to_excel(output)   \n    else:\n        df_res.to_excel(output)\nelse:\n    df_res.to_excel(output)","960f2e99":"**Recap** of the process in the image below:","489c1b00":"### [1. Introduction](#internal_link)\n#### [1.1 Theoretical Background](#internal_link)\n#### [1.2 Preliminary analysis](#link1)\n### [2. Data Preparation](#link2)\n#### [2.1 Train-Test Split](#link2)\n#### [2.2 Undersampling](#link3)\n#### [2.3 Scaling](#link4)\n#### [2.4 Feature Selection](#link5)\n### [3. Ensemble Model](#link6)\n#### [3.1. Fixing Paramenters](link6)\n#### [3.2. Grid Search RF](#link7)\n#### [3.3. Results](#link8)\n### [4. Conclusions](#link9)","92f4b274":"Just two observations regarding this correlation matrix:\n* The 'V' variables are by definition completely uncorrelated (r=0), as they were obtained by a PCA. This is the reason why I masked them in the previous graph;\n* It's very difficult to understand the relationship between variables given we don't know the meaning of most of them. Anyway, it seems that 'Amount' and 'Time' are almost uncorrelated (r= -011), though their relationship is negative. This in turn explains why most 'V'variables are correlated with one but not with the other.   ","ef9f15cc":"## 1. Introduction\n\n### 1.1. Theoretical Background\n\n* This notebook focuses on building an **ensemble algorithm**, to solve the card fraud classification problem. The advantage of putting together many classifiers and weight their result for the final classification it's simply that their collective performance increases over that of a single one. There are two different options for building an ensemble: you can use different types of classifiers, for instance putting together a RF classifier and logistic regressor, or you can use the same algorithm multiple times. I chose to use **multiple RF classifiers**. Please note that the number of RFs used in the ensemble can be changed in my code, as it's one of the ensemble parameters. \n\n* When you use the same type of classifier over the ensemble, you have to train every classifier on different random subsets of the training set. This random extraction of training instances can be applied with or without replacement. In the former we speak of bagging, in the latter pasting. I chose **pasting** for this analysis, but again it's a changeable parameter in the code. \n\n* A second theme is the **aggregation** of the final classification. The simplest way of aggregating the predictions of the different classifiers is to take a vote: the class that gets more votes wins, so to speak. An alternative to this method, which is called *majority vote*, is the *soft voting* option. Soft voting may be applied only by algorithms that predict the probability of belonging to a certain class, so in our case with a RF it's available. What happens is that we average the probabilities obtained by all the individual classifiers to find the class with the highest class probability. Both options are available in my code. \n\n* Last but certainly not least, the **unbalance** of the dataset must be addressed: only a tiny fraction of transactions are frauds (<0.1%). There are different ways of tackling this issue, but I chose *undersampling*. The drawback in this case is that we will lose potentially important information. However, in this case we have 284807 observations, which is quite robust. I tried also an oversample technique (SMOTE), but it was difficult to associate it with a large ensemble because of computational costs. ","2ac18cef":"The bimodal distribution clearly emerging for the Time variable shows that we can differentiate between transactions lasting one day and transactions lasting two days. Working with hours instead of seconds is way simpler: without the transformation from seconds to hours I would have not recognized this pattern. ","4f85868c":"We are in the presence of an markedly unbalanced dataset, with the percentage of frauds below 0.2%. This will need to be adjusted in the model training phase, lest we get an inflated accuracy in the classification due to a very high proportion of non fraudolent transactions.","c29c5215":"Another observation we can make concerns the amount of money in play during these transactions. The most frequent amount of money (more than 40%) is between 10 and 100 dollars, while transactions totalling more than 500 dollars are quite rare (<5%). ","a9829b21":"### 1.2 Preliminary Analysis","c41e304d":"### 3.2 Grid Search Random Forest Classifiers\nCareful: it takes by and large 20 minutes to run this model (7 RFs, 5 folds, features selection) . If you have time, increasing the number of folds and the number of RFs trained, plus dropping the feature selection phase should improve the strength of the classification, but at the cost of significantly increasing the training time. ","33d1d02b":"## 3. Ensemble Model","50d02867":"Quick, always useful revision (in latin 'Repetita iuvant', that is 'Repeating it's a useful practice'):\n* Precision is the probability that a predicted alarm corresponds to a real fraud. With our numbers we have 75\/75+29, that is frauds divided by the number of frauds+false positives. \n* Recall is the probability that a real alarm is captured by the system. If we take the numbers showed in the confusion matrix we have: 75\/(75+23) = 75\/98= 0.765, that is the number of frauds (true negatives), divided by frauds + false negatives (i.e. the system predicted they were frauds, but they weren't). \n\n* F1 is a weighted combination of the two indeces. \n\n---\n\nIn our case recall is slightly higher than precision. In my view, considering the task is to identify potential frauds, this is a good sign: in the precision\/recall trade-off of this case it's ok to get some false alarms (lower precision), as long as we are sure to capture as many frauds as possible (slightly higher precision).","72b56f92":"The receiver operating characteristic (ROC) curve is another common visualization used with binary classifiers. One way to compare classifiers is to measure the area under the curve (AUC). A perfect classifier by definition has a ROC AUC equal to 1, while a random classifier will have a ROC AUC equal to 0.5 (the straight dotted line in the graph). In my case I obtained an honest 0.96","768f7bae":"## 2. Data Preparation","01839121":"The function below simply operates the undersampling of the majority class (i.e. the non fraudolent transactions, class = 0). Its arguments require to specify the dataframe the technique gets applied to, and the proportion of positive vs negative class (in our case I'll choose a 95-5% split). Please note that the downsampling is enforced using the resample() standard pandas function and that I chose a resampling without replacement.\n\nFinally, note that I could apply the undersampling technique also to the test set, but I chose not to. This way I preserve the original proportion of fraudolent transactions and I will see how much the results of my models obtained on an undersampled train set will be generalizable. ","457892b8":"![](http:\/\/)<a id='link6'><\/a>","e7d74364":"## 4.Conclusions\n* The levels of precision, recall and AUC reached by my ensemble are adequate considering the unbalance of the dataset. By increasing the complexity (and the training time) of my model, for example the number of RFs included, it is likely that the performances can be further increased.  \n* The unbalance problem was solved by undersampling. SMOTE is a good alternative, but its higher computational requirement were not very well suited with the kind of analysis I had in mind based on an ensemble of RFs, and the size of the dataset. Anyway, I could extend the analysis in that direction, by comparing the results obtained by the ensemble + downsampling with a simpler model + SMOTE.","1b6c938c":"![](http:\/\/)<a id='link4'><\/a>","d4f95551":"![](http:\/\/)<a id='link5'><\/a>","451b60c0":"#### Optional: export results in an Excel File","c86ec9e1":"![](http:\/\/)<a id='link3'><\/a>","4e8a7e5a":"![ensemble%20image.PNG](attachment:ensemble%20image.PNG)","42a986e8":"### 2.1 Train-Test split\nAs can be seen from the numbers I chose a **80-20 split** between train and test.","88574e55":"![](http:\/\/)<a id='link9'><\/a>","55cc5429":"![](http:\/\/)<a id='link7'><\/a>","28d5126f":"![](http:\/\/)<a id='link2'><\/a>","e183b40d":"### 2.2 Undersampling technique\n","22145184":"![](http:\/\/)<a id='link1'><\/a>","d986e88f":"### 3.3 Results","4eb98203":"### 2.4 Feature Selection\nThe number of feature is considerable in this case, so I decided to put a feature selection step to reduce redundancy and training times of the model. If you want to maximise the predictive power of the model and don't care about decreasing the training time just set a negative number for the imp_thr variable - this way the model will keep all the variables.","e5b33873":"### 3.1. Fixing parameters","85d1318e":"![](http:\/\/)<a id='internal_link'><\/a>","816dc991":"![](http:\/\/)<a id='link8'><\/a>","0767c6ce":"### 2.3 Scaling\n\nScaling is applied both to train and test set. I chose one of the most common scaling method, standardization, which  transforms the data in such a manner that it has 0 as mean and 1 as standard deviation. "}}