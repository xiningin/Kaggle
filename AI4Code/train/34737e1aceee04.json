{"cell_type":{"0b1c8ede":"code","09187802":"code","0e73675e":"code","e189ebe1":"code","d75d79f1":"code","d32e4e9f":"code","96eab1dc":"code","7489413e":"code","2e42185b":"code","303c3965":"code","dc2dd3cb":"code","0a6fe67a":"code","49f1954c":"code","3511f7a9":"code","5ef53c75":"markdown","abe2c764":"markdown","7ccff79e":"markdown","e32e012b":"markdown","3a52e09c":"markdown"},"source":{"0b1c8ede":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\n\nsns.set(rc={'figure.figsize':(11.7,8.27)})\npalette = sns.color_palette(\"bright\", 6)","09187802":"path = '..\/input\/breast-cancer-gene-expression-cumida\/'\n\ndf = pd.read_csv(path + 'Breast_GSE45827.csv', index_col='samples')\ndf.head()","0e73675e":"labels = df['type']\nfeatures = df.drop(columns=['type'])","e189ebe1":"idx_to_category = {k:v for k,v in enumerate(np.unique(labels.values))}\ncategory_to_idx = {v:k for k,v in enumerate(list(idx_to_category.values()))}","d75d79f1":"labels = labels.replace(category_to_idx)","d32e4e9f":"from sklearn.manifold import TSNE","96eab1dc":"tsne = TSNE()\nX_embedded = tsne.fit_transform(features.values)","7489413e":"sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=labels.values, legend='full', palette=palette)","2e42185b":"def Hbeta(D=np.array([]), beta=1.0):\n    \"\"\"\n        Compute the perplexity and the P-row for a specific value of the\n        precision of a Gaussian distribution.\n    \"\"\"\n\n    # Compute P-row and corresponding perplexity\n    P = np.exp(-D.copy() * beta)\n    sumP = sum(P)\n    H = np.log(sumP) + beta * np.sum(D * P) \/ sumP\n    P = P \/ sumP\n    return H, P","303c3965":"def x2p(X=np.array([]), tol=1e-5, perplexity=30.0):\n    \"\"\"\n        Performs a binary search to get P-values in such a way that each\n        conditional Gaussian has the same perplexity.\n    \"\"\"\n\n    # Initialize some variables\n    print(\"Computing pairwise distances...\")\n    (n, d) = X.shape\n    sum_X = np.sum(np.square(X), 1)\n    D = np.add(np.add(-2 * np.dot(X, X.T), sum_X).T, sum_X)\n    P = np.zeros((n, n))\n    beta = np.ones((n, 1))\n    logU = np.log(perplexity)\n\n    # Loop over all datapoints\n    for i in range(n):\n\n        # Print progress\n        if i % 500 == 0:\n            print(\"Computing P-values for point %d of %d...\" % (i, n))\n\n        # Compute the Gaussian kernel and entropy for the current precision\n        betamin = -np.inf\n        betamax = np.inf\n        Di = D[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))]\n        (H, thisP) = Hbeta(Di, beta[i])\n\n        # Evaluate whether the perplexity is within tolerance\n        Hdiff = H - logU\n        tries = 0\n        while np.abs(Hdiff) > tol and tries < 50:\n\n            # If not, increase or decrease precision\n            if Hdiff > 0:\n                betamin = beta[i].copy()\n                if betamax == np.inf or betamax == -np.inf:\n                    beta[i] = beta[i] * 2.\n                else:\n                    beta[i] = (beta[i] + betamax) \/ 2.\n            else:\n                betamax = beta[i].copy()\n                if betamin == np.inf or betamin == -np.inf:\n                    beta[i] = beta[i] \/ 2.\n                else:\n                    beta[i] = (beta[i] + betamin) \/ 2.\n\n            # Recompute the values\n            (H, thisP) = Hbeta(Di, beta[i])\n            Hdiff = H - logU\n            tries += 1\n\n        # Set the final row of P\n        P[i, np.concatenate((np.r_[0:i], np.r_[i+1:n]))] = thisP\n\n    # Return final P-matrix\n    print(\"Mean value of sigma: %f\" % np.mean(np.sqrt(1 \/ beta)))\n    return P","dc2dd3cb":"def pca(X=np.array([]), no_dims=50):\n    \"\"\"\n        Runs PCA on the NxD array X in order to reduce its dimensionality to\n        no_dims dimensions.\n    \"\"\"\n\n    print(\"Preprocessing the data using PCA...\")\n    (n, d) = X.shape\n    X = X - np.tile(np.mean(X, 0), (n, 1))\n    (l, M) = np.linalg.eig(np.dot(X.T, X))\n    Y = np.dot(X, M[:, 0:no_dims])\n    return Y","0a6fe67a":"def tsne(X=np.array([]), no_dims=2, initial_dims=50, perplexity=30.0):\n    \"\"\"\n        Runs t-SNE on the dataset in the NxD array X to reduce its\n        dimensionality to no_dims dimensions. The syntaxis of the function is\n        `Y = tsne.tsne(X, no_dims, perplexity), where X is an NxD NumPy array.\n    \"\"\"\n\n    # Check inputs\n    if isinstance(no_dims, float):\n        print(\"Error: array X should have type float.\")\n        return -1\n    if round(no_dims) != no_dims:\n        print(\"Error: number of dimensions should be an integer.\")\n        return -1\n\n    # Initialize variables\n    X = pca(X, initial_dims).real\n    (n, d) = X.shape\n    max_iter = 200\n    initial_momentum = 0.5\n    final_momentum = 0.8\n    eta = 500\n    min_gain = 0.01\n    Y = np.random.randn(n, no_dims)\n    dY = np.zeros((n, no_dims))\n    iY = np.zeros((n, no_dims))\n    gains = np.ones((n, no_dims))\n\n    # Compute P-values\n    P = x2p(X, 1e-5, perplexity)\n    P = P + np.transpose(P)\n    P = P \/ np.sum(P)\n    P = P * 4.\n    P = np.maximum(P, 1e-12)\n\n    # Run iterations\n    for iter in range(max_iter):\n\n        # Compute pairwise affinities\n        sum_Y = np.sum(np.square(Y), 1)\n        num = -2. * np.dot(Y, Y.T)\n        num = 1. \/ (1. + np.add(np.add(num, sum_Y).T, sum_Y))\n        num[range(n), range(n)] = 0.\n        Q = num \/ np.sum(num)\n        Q = np.maximum(Q, 1e-12)\n\n        # Compute gradient\n        PQ = P - Q\n        for i in range(n):\n            dY[i, :] = np.sum(np.tile(PQ[:, i] * num[:, i], (no_dims, 1)).T * (Y[i, :] - Y), 0)\n\n        # Perform the update\n        if iter < 20:\n            momentum = initial_momentum\n        else:\n            momentum = final_momentum\n        gains = (gains + 0.2) * ((dY > 0.) != (iY > 0.)) + \\\n                (gains * 0.8) * ((dY > 0.) == (iY > 0.))\n        gains[gains < min_gain] = min_gain\n        iY = momentum * iY - eta * (gains * dY)\n        Y = Y + iY\n        Y = Y - np.tile(np.mean(Y, 0), (n, 1))\n\n        # Compute current value of cost function\n        if (iter + 1) % 10 == 0:\n            C = np.sum(P * np.log(P \/ Q))\n            print(\"Iteration %d: error is %f\" % (iter + 1, C))\n\n        # Stop lying about P-values\n        if iter == 100:\n            P = P \/ 4.\n\n    # Return solution\n    return Y","49f1954c":"# use < 2000 feature columns, otherwise we don't have enough RAM to process it\nX_embedded = tsne(features.values[:,:1000], 2, 50, 20.0)","3511f7a9":"sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=labels.values, legend='full', palette=palette)","5ef53c75":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:black; background:yellow; border:0.5px dotted black;\" role=\"tab\" aria-controls=\"home\"><center>Sklearn t-SNE<\/center><\/h3>","abe2c764":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:black; background:yellow; border:0.5px dotted black;\" role=\"tab\" aria-controls=\"home\"><center>Implementation from Scratch<\/center><\/h3>","7ccff79e":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:black; background:yellow; border:0.5px dotted black;\" role=\"tab\" aria-controls=\"home\"><center>Training<\/center><\/h3>","e32e012b":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:black; background:yellow; border:0.5px dotted black;\" role=\"tab\" aria-controls=\"home\"><center>Prepare Data<\/center><\/h3>","3a52e09c":"<div style=\"width:100%;height:200px\">\n    <img src=\"https:\/\/storage.googleapis.com\/kaggle-datasets-images\/491250\/914079\/dbf2b107119670d835aa22ca4014fa47\/dataset-cover.png?t=2020-01-28-11-41-41\"\/>\n<\/div>"}}