{"cell_type":{"d6446b98":"code","28f7da12":"code","f3155482":"code","e66c3d43":"code","cd4c000f":"code","5c8d5f5a":"code","7cae3bb4":"code","c8eb4743":"code","8bc743cb":"code","45283335":"code","c7680283":"code","15b3d890":"code","56287b37":"code","1b36f8c6":"code","bb59e8df":"code","07b41fd5":"code","8b70f881":"code","056ff363":"code","a4b4d363":"code","fa74d90b":"code","0c679257":"code","f0469a52":"code","b54c14ed":"code","8b1ae1bb":"code","b00dc4a0":"code","2c16df65":"code","7de76604":"code","f4413de6":"code","5b468eb5":"code","95fbd7cc":"code","c6a5150a":"code","6a7aca42":"code","099d2f53":"code","576ec7bc":"code","23a82652":"code","d8d4b153":"code","1047ded1":"code","dd410ff3":"code","6616cb32":"code","0cc583ca":"code","9931cc6e":"code","cbfffaff":"code","bec8ab51":"code","262afddc":"markdown","5dcb24c4":"markdown","7726544a":"markdown","6055d796":"markdown","59582567":"markdown","2f3e4aeb":"markdown","9fa11321":"markdown","b3d1d336":"markdown","15b54182":"markdown","789b1776":"markdown","bd6009e1":"markdown","6c4cff76":"markdown","31ca3586":"markdown","24b76f90":"markdown","caa68d8c":"markdown","da8d23c7":"markdown","c99bc8ff":"markdown","7d2c5290":"markdown","a4a4c609":"markdown","fb87634d":"markdown","5aa7c300":"markdown","26238712":"markdown","19bfe807":"markdown","6da027de":"markdown","e81d0f07":"markdown","91ff1e7c":"markdown","33f22f8f":"markdown","3635b101":"markdown","e094253c":"markdown","017bad09":"markdown","7abb4d48":"markdown","2f4856d6":"markdown"},"source":{"d6446b98":"# load libraries\nimport pandas as pd\nimport numpy as np\nimport csv\nimport time\nfrom datetime import datetime, time\nfrom plotnine import *\nfrom mizani.breaks import date_breaks\nfrom dfply import *\nimport seaborn as sns\nimport pprint as p","28f7da12":"# load data set using pandas csv reader\nhotel_data = pd.read_csv('hotel_bookings.csv')","f3155482":"# View first 5 rows of data\nhotel_data.head()","e66c3d43":"# how many rows of data and how many variables?\nvariables = hotel_data.shape[1]\nrows = hotel_data.shape[0]\n\nprint(\"There are {:d} varaibles with {:d} rows in this dataset\\n\".format(variables, rows))","cd4c000f":"# convert reservation_status_date into datetime type\ndate_temp = pd.to_datetime(hotel_data.reservation_status_date, format = '%Y-%m-%d')\n\n#display(date_temp)","5c8d5f5a":"# overwrite date time column to proper data type\nhotel_data['reservation_status_date'] = date_temp\n\n#display(hotel_data.reservation_status_date.dt.strftime(\"%B\"))","7cae3bb4":"# determine date range\nmin_res_date = hotel_data.reservation_status_date.min()\nmax_res_date = hotel_data.reservation_status_date.max()\nprint(\"min date: {}\\nmax date: {}\".format(min_res_date, max_res_date))","c8eb4743":"# visualize results:\n\n# create month list\nmonth_list = ('January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December')\n\n# extract time data out of column and convert to proper format\nhotel_data['res_status_month'] = hotel_data.reservation_status_date.dt.strftime(\"%B\")\nhotel_data['res_status_year'] = hotel_data.reservation_status_date.dt.year\nhotel_data['res_status_year'] = hotel_data['res_status_year'].astype(\"str\")\n\n# create the visualization\nhotel_hist = (ggplot(hotel_data, aes(x='res_status_month', fill='res_status_year'))+\n              geom_bar(stat=\"count\", alpha=0.8)+\n              scale_x_discrete(limits = month_list)+\n              labs(x='Reservation Month', y='Number of Reservations', fill='Year of Res.', title='Reservations by Month')+\n              theme_minimal()+\n              theme(axis_text_x=element_text(rotation=45, hjust=1))\n             )\n\nhotel_hist","8bc743cb":"# normalize height of bars\nhotel_hist_normalized = (ggplot(hotel_data, aes(x='res_status_month', fill='res_status_year'))+\n              geom_bar(stat=\"count\", alpha=0.8, position='fill')+\n              scale_x_discrete(limits = month_list)+\n              labs(x='Reservation Month', y='Percent of Reservations', fill='Year of Res.', title='Reservations by Month')+\n              theme_minimal()+\n              theme(axis_text_x=element_text(rotation=45, hjust=1))\n             )\n\nhotel_hist_normalized","45283335":"# Plot city vs hotel reservation count\nhotel_data['is_canceled'] = hotel_data['is_canceled'].astype('str')\n\nc_vs_h = (ggplot(hotel_data, aes(x='hotel', fill='is_canceled'))+\n          geom_bar(alpha=0.8)+\n          geom_text(aes(label='stat(count)'),\n                        position =position_stack(vjust=0.5),\n                        stat='count',\n                        size=12, \n                        va='top',\n                        format_string='{}')+\n          labs(x='Hotel Type', y='Number of Reservations', fill='Canceled(0=No)', title='Reservations by Hotel Type')+\n          theme_minimal()\n         )\nc_vs_h","c7680283":"# Calculate percent of cancelations for the total, then resort and city hotels\ntotal_cancel = hotel_data['is_canceled'].sum()\nresort_cancel = hotel_data.loc[hotel_data['hotel'] == \"Resort Hotel\"]['is_canceled'].sum()\ncity_cancel = hotel_data.loc[hotel_data['hotel'] == \"City Hotel\"]['is_canceled'].sum()\n\ntotal_canc_percent = (total_cancel\/hotel_data.shape[0])*100\nresort_canc_percent = (resort_cancel\/hotel_data.loc[hotel_data['hotel'] == \"Resort Hotel\"].shape[0])*100\ncity_canc_percent = (city_cancel\/hotel_data.loc[hotel_data['hotel'] == \"City Hotel\"].shape[0])*100\n\nprint(f\"Total bookings canceled: {total_cancel:,} ({total_canc_percent:.0f} %)\")\nprint(f\"Resort hotel bookings canceled: {resort_cancel:,} ({resort_canc_percent:.0f} %)\")\nprint(f\"City hotel bookings canceled: {city_cancel:,} ({city_canc_percent:.0f} %)\")","15b3d890":"# plot of reservation month colored by cancelation\ndodge_text = position_dodge(width=0.9)\nplot3 = (ggplot(hotel_data, aes(x='res_status_month', fill='is_canceled'))+\n              geom_bar(stat=\"count\", alpha=0.8, position='dodge')+\n              scale_x_discrete(limits = month_list)+\n              facet_wrap(\"hotel\")+\n              geom_text(aes(label='stat(count)\/100'),\n                        ha='right',\n                        position=dodge_text, stat='count',\n                        size=8, \n                        format_string='{}%')+\n             coord_flip()+\n             labs(x='Reservation Month', y='Count of Reservations', fill='Canceled(0=No)')+#, title='Reservations by Month')+\n             theme_minimal()\n        )\n\nplot3","56287b37":"# create correlation matrix\ncorr = hotel_data.corr()\nsns.heatmap(corr)","1b36f8c6":"# import more libs\nimport matplotlib.pyplot as plt\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.model_selection import cross_validate\nfrom sklearn import metrics\nimport xgboost as xgb\nfrom xgboost import plot_importance\n%matplotlib inline\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4\nfrom sklearn.metrics import f1_score, precision_score, recall_score","bb59e8df":"def modelfit(alg, x_train, y_train, useTrainCV=True, cv_folds=5, early_stopping_rounds=50, feat_plot=False):\n    '''\n    INPUTS:\n        alg     = Algorithm to pass to function (ex: XGBClassifier\/XGBRegressor)\n        x train = test and training data to pass\n        y train = test and training for predictors\n    '''\n    \n    if useTrainCV:\n        xgb_param = alg.get_xgb_params()\n        xgtrain = xgb.DMatrix(x_train.values, label=y_train) #convert training data into DMatrix\n        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n            metrics='auc', # metric needs to change based on Algorithm passed in\n            early_stopping_rounds=early_stopping_rounds)\n        alg.set_params(n_estimators=cvresult.shape[0])\n    print(\"Optimal estimators for learning rate: \",cvresult.shape[0])\n    \n    #Fit the algorithm on the data\n    alg.fit(x_train, y_train)# ,eval_metric='auc')\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(x_train)\n    dtrain_predprob = alg.predict_proba(x_train)[:,1]\n        \n    #Print model report:\n    print (\"\\nModel Report\")\n    print (\"Accuracy : %.4g\" % metrics.accuracy_score(y_train.values, dtrain_predictions))\n    print (\"AUC Score (Train): %f\" % metrics.roc_auc_score(y_train.values.flatten(), dtrain_predprob))\n                    \n    # plot feature importance\n    if feat_plot:\n        plot_importance(alg, grid=False)\n        plt.show()","07b41fd5":"# create new df with just original data\nhotel_data_original = hotel_data >> select(~X.res_status_month, ~X.res_status_year)\nhotel_data_original.head()","8b70f881":"# #display(hotel_data_original)\n# hotel_data['is_canceled'] = hotel_data['is_canceled'].astype('str')\n\n# #create correlation list\n# cancel_corr = hotel_data_original.corr()['is_canceled']\n\n# display(cancel_corr)\n# cancel_corr.abs().sort_values(ascending=False)","056ff363":"from sklearn.preprocessing import OneHotEncoder\n\n# create numeric features\nnum_feat = [\"lead_time\", \"total_of_special_requests\", \"required_car_parking_spaces\", \"previous_cancellations\", \"is_repeated_guest\",\n            \"agent\", \"adults\", \"previous_bookings_not_canceled\", \"days_in_waiting_list\", \"adr\"]\n\n# create categorical features\ncat_feat = [\"hotel\",\"arrival_date_month\",\"meal\",\"market_segment\",\"distribution_channel\",\"reserved_room_type\",\"deposit_type\",\n            \"customer_type\"]\nhotel_cat = hotel_data_original[cat_feat]\n#display(hotel_cat)\n# # create instance of one-hot encoder\n# enc = OneHotEncoder(handle_unknown='ignore')\n\n# # pass values in\n# enc_df = pd.DataFrame(enc.fit_transform(hotel_cat).toarray())\n# display(enc_df)","a4b4d363":"# Run model again but use pd.get_dummies instead of OneHotEncoder\nhotel_cat2 = pd.get_dummies(hotel_data[cat_feat], dtype='int64')\n\n# merge data frames\nhotel_df2 = pd.concat([hotel_data_original[num_feat], hotel_cat2], axis=1)\ndisplay(hotel_data_original[num_feat])\n#p.pprint(hotel_df2.columns.to_series().groupby(hotel_df2.dtypes).groups)\n\npredictors = hotel_data_original['is_canceled']","fa74d90b":"# split into test train set, this was a 60-40 split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(hotel_df2, predictors, test_size = 0.4,\n                                                    random_state=1)","0c679257":"#list(x_train.columns.values)","f0469a52":"# plot training data\nplot_train = (ggplot(x_train, aes(x='hotel_Resort Hotel'))+\n              geom_bar(stat='count', position='stack')+\n              labs(x='Hotel Type')\n)\nplot_train","b54c14ed":"# build model, note that I left all the default input parameters and will edit them during hyperparameter tuning\nxgb2 = XGBClassifier(seed=2)","8b1ae1bb":"import time","b00dc4a0":"# run model\nstart = time.time() # start timer\n\nmodelfit(xgb2, x_train, y_train) \n\nprint(\"Building time : \" + str(time.time()-start))","2c16df65":"plot_importance(xgb2, grid=False, max_num_features=15)\nplt.show()","7de76604":"# Predict on test set:\npredictions = xgb2.predict(x_test)\npredprob = xgb2.predict_proba(x_test)[:,1]\n\n# Print model report:\nprint (\"\\nModel Report\")\nprint (\"Accuracy: %.4g\" % metrics.accuracy_score(y_test.values, predictions))\nprint (\"AUC Score (Test): %f\" % metrics.roc_auc_score(y_test.values.flatten(), predprob))","f4413de6":"# calculate other classification metrics\n\nf1 = f1_score(y_test.values, predictions)\nprecision =  precision_score(y_test.values, predictions)\nrecall = recall_score(y_test.values, predictions)\n\nprint(\"f1 Score: {:f}\\nPrecision: {:f}\\nRecall: {:f}\".format(f1, precision, recall))","5b468eb5":"hotel_resort = hotel_data_original >> mask(X.hotel=='Resort Hotel')\npredictors_R = hotel_resort['is_canceled']\nhotel_cat_resort = hotel_resort[cat_feat]\n# Run model again but use pd.get_dummies instead of OneHotEncoder\nhotel_cat_resort2 = pd.get_dummies(hotel_resort[cat_feat], dtype='int64')\n# merge data frames\nhotel_resort2 = pd.concat([hotel_resort[num_feat], hotel_cat_resort2], axis=1)\n#display(hotel_resort2)\n\n\nhotel_city = hotel_data_original >> mask(X.hotel=='City Hotel')\npredictors_C = hotel_city['is_canceled']\nhotel_cat_city = hotel_city[cat_feat]\n# Run model again but use pd.get_dummies instead of OneHotEncoder\nhotel_cat_city2 = pd.get_dummies(hotel_city[cat_feat], dtype='int64')\n# merge data frames\nhotel_city2 = pd.concat([hotel_city[num_feat], hotel_cat_city2], axis=1)\n#display(hotel_city2)","95fbd7cc":"import time","c6a5150a":"# split into test train set, this was a 60-40 split\n\nx_trainR, x_testR, y_trainR, y_testR = train_test_split(hotel_resort2, predictors_R, test_size = 0.4,\n                                                    random_state=1)\n# build model, note that I left all the default input parameters and will edit them during hyperparameter tuning\nxgb_R = XGBClassifier(seed=2)\n\n# run model\nstart = time.time() # start timer\n\nmodelfit(xgb_R, x_trainR, y_trainR) \n\nprint(\"Building time : \" + str(time.time()-start))","6a7aca42":"plot_importance(xgb_R, grid=False, max_num_features=15)\nplt.show()","099d2f53":"# Run Test\n# Predict on test set:\npredictions_R = xgb_R.predict(x_testR)\npredprob = xgb_R.predict_proba(x_testR)[:,1]\n\n# Print model report:\nprint (\"\\nModel Report\")\nprint (\"Accuracy: %.4g\" % metrics.accuracy_score(y_testR.values, predictions_R))\nprint (\"AUC Score (Test): %f\" % metrics.roc_auc_score(y_testR.values.flatten(), predprob))\n\nf1 = f1_score(y_testR.values, predictions_R)\nprecision =  precision_score(y_testR.values, predictions_R)\nrecall = recall_score(y_testR.values, predictions_R)\n\nprint(\"f1 Score: {:f}\\nPrecision: {:f}\\nRecall: {:f}\".format(f1, precision, recall))","576ec7bc":"# split into test train set, this was a 60-40 split\n\nx_trainC, x_testC, y_trainC, y_testC = train_test_split(hotel_city2, predictors_C, test_size = 0.4,\n                                                    random_state=1)\n# build model, note that I left all the default input parameters and will edit them during hyperparameter tuning\nxgb_C = XGBClassifier(seed=2)\n\n# run model\nstart = time.time() # start timer\n\nmodelfit(xgb_C, x_trainC, y_trainC) \n\nprint(\"Building time : \" + str(time.time()-start))","23a82652":"plot_importance(xgb_C, grid=False, max_num_features=15)\nplt.show()","d8d4b153":"# test model\n# Predict on test set:\npredictions_C = xgb_C.predict(x_testC)\npredprob = xgb_C.predict_proba(x_testC)[:,1]\n\n# Print model report:\nprint (\"\\nModel Report\")\nprint (\"Accuracy: %.4g\" % metrics.accuracy_score(y_testC.values, predictions_C))\nprint (\"AUC Score (Test): %f\" % metrics.roc_auc_score(y_testC.values.flatten(), predprob))\n\nf1 = f1_score(y_testC.values, predictions_C)\nprecision =  precision_score(y_testC.values, predictions_C)\nrecall = recall_score(y_testC.values, predictions_C)\n\nprint(\"f1 Score: {:f}\\nPrecision: {:f}\\nRecall: {:f}\".format(f1, precision, recall))","1047ded1":"'''\nnote the following format for inputs to xgb:\nxgb1 = XGBClassifier(learning_rate = 0.1,\n                     n_estimators = 1000,\n                     max_depth = 5,          # max depth of tree\n                     min_child_weight = 1,   # min sum of weights of all observ. required in a child\n                     gamma = 0,              # min loss required to make split\n                     subsample = 0.8,        # fraction of observ. to be randomly selected for each tree\n                     colsample_bytree = 0.8, # denotes frac. of col. to be randomly smapled for each tree\n                     scale_pos_weight = 1    # parameter for high class imbalanc,[default=1]\n)\n'''\nfrom sklearn.model_selection import GridSearchCV","dd410ff3":"# Train new model with new hyper-parameters\nxgb3 = XGBClassifier(learning_rate = 0.1,\n                     n_estimators = 439,     # 439 comes from previous iterations of training\n                     max_depth = 5,          # max depth of tree\n                     min_child_weight = 1,   # min sum of weights of all observ. required in a child\n                     gamma = 0,              # min loss required to make split\n                     subsample = 0.8,        # fraction of observ. to be randomly selected for each tree\n                     colsample_bytree = 0.8, # denotes frac. of col. to be randomly smapled for each tree\n                     scale_pos_weight = 1    # parameter for high class imbalanc,[default=1]\n)","6616cb32":"# run model\nstart = time.time() # start timer\n\nmodelfit(xgb3, x_train, y_train) \n\nprint(\"Building time : \" + str(time.time()-start))","0cc583ca":"# Predict on test set:\npredictions = xgb3.predict(x_test)\npredprob = xgb3.predict_proba(x_test)[:,1]\n\n# Print model report:\nprint (\"\\nModel Report\")\nprint (\"Accuracy: %.4g\" % metrics.accuracy_score(y_test.values, predictions))\nprint (\"AUC Score (Test): %f\" % metrics.roc_auc_score(y_test.values.flatten(), predprob))\n\nf1 = f1_score(y_test.values, predictions)\nprecision =  precision_score(y_test.values, predictions)\nrecall = recall_score(y_test.values, predictions)\n\nprint(\"f1 Score: {:f}\\nPrecision: {:f}\\nRecall: {:f}\".format(f1, precision, recall))","9931cc6e":"# Tune max_depth and min_child_weight first\nparam_test1 = {\n    'max_depth':range(3,10,2),\n    'min_child_weight':range(1,6,2)  \n}\ngsearch1 = GridSearchCV(estimator=XGBClassifier(learning_rate=0.1, n_estimators=439,\n                     min_child_weight = 1,   # min sum of weights of all observ. required in a child\n                     gamma = 0,              # min loss required to make split\n                     subsample = 0.8,        # fraction of observ. to be randomly selected for each tree\n                     colsample_bytree = 0.8, # denotes frac. of col. to be randomly smapled for each tree\n                     scale_pos_weight = 1,    # parameter for high class imbalanc,[default=1]\n                     seed=2),\n                        param_grid=param_test1,\n                        scoring='roc_auc',\n                        n_jobs=4,\n                        iid=False,\n                        cv=5\n                       )\ngsearch1.fit(x_train, y_train)\n#gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_\n\nprint('Best parameter set found on development site: ',gsearch1.best_params_,'\\n')\nprint('Best ROC_AUC: ',gsearch1.best_score_,'\\n')\nprint('Mean test score: ',gsearch1.cv_results_['mean_test_score'],'\\n')\nprint('std. on test score: ',gsearch1.cv_results_['std_test_score'])","cbfffaff":"# test tuned model on test set\nxgb_tuned = XGBClassifier(learning_rate = 0.1,\n                     n_estimators = 439,     # 439 comes from previous iterations of training\n                     max_depth = 9,          # max depth of tree\n                     min_child_weight = 1,   # min sum of weights of all observ. required in a child\n                     gamma = 0,              # min loss required to make split\n                     subsample = 0.8,        # fraction of observ. to be randomly selected for each tree\n                     colsample_bytree = 0.8, # denotes frac. of col. to be randomly smapled for each tree\n                     scale_pos_weight = 1    # parameter for high class imbalanc,[default=1]\n)\n\n# run model\nstart = time.time() # start timer\n\nmodelfit(xgb_tuned, x_train, y_train) \n\nprint(\"Building time : \" + str(time.time()-start))","bec8ab51":"# Predict on test set:\npredictions = xgb_tuned.predict(x_test)\npredprob = xgb_tuned.predict_proba(x_test)[:,1]\n\n# Print model report:\nprint (\"\\nModel Report\")\nprint (\"Accuracy: %.4g\" % metrics.accuracy_score(y_test.values, predictions))\nprint (\"AUC Score (Test): %f\" % metrics.roc_auc_score(y_test.values.flatten(), predprob))\n\nf1 = f1_score(y_test.values, predictions)\nprecision =  precision_score(y_test.values, predictions)\nrecall = recall_score(y_test.values, predictions)\n\nprint(\"f1 Score: {:f}\\nPrecision: {:f}\\nRecall: {:f}\".format(f1, precision, recall))","262afddc":"#### City Hotel Model","5dcb24c4":"### Results:\nThe grid search performed above took about 40 minutes.  The best parameters found for the two hyperparameters were max_depth=9 and min_child_weight=1.  Ideally we would continue to perform grid searches on the remaining parameters, intermittently retraining our model to get the output of the optimal number of estimators (ex:\u201d Optimal estimators for learning rate:  100\u201d) as that can change.  For times sake no more grid searches will be performed.\n\nParameters|Value|Description\n---|---|:---\nlearning_rate|0.1|learning rate\nn_estimators|439|number of boosting rounds\nmax_depth|9|max depth of tree\nnin_child_weight|1|min sum of weights of all observ. required in a child\ngamma|0|min loss required to make split\nsubsample|0.8|fraction of observ. to be randomly selected for each tree\ncolsample_bytree|0.8|denotes frac. of col. to be randomly smapled for each tree\nscale_pos_weight|1|parameter for high class imbalance [default=1]\n\nSummary of Model Results:\n\nMetric|Before (defualt)|After (tuning)\n---|---|---\nAccuracy|0.8440|0.8546|\nAUC Score|0.9103|0.9213|\nf1 Score|0.7693|0.7919|\nPrecision|0.8488|0.8408|\nRecall|0.7034|0.7484|\n\n\n\n\n","7726544a":"It appears that this data spans from 2014 to 2017.","6055d796":"---\n#### *Introduction*:\nThe dataset can be found at https:\/\/www.kaggle.com\/jessemostipak\/hotel-booking-demand\n\nThis data set contains booking information for a city hotel and a resort hotel, and includes information such as when the booking was made, length of stay, the number of adults, children, and\/or babies, and the number of available parking spaces, among other things.\n\nPer the description for the dataset listed above, an exploratory data analysis will be performed, then a model will be built to predict whether a reservation will be canceled.  This could be a useful tool for hotels and resorts when predicting or forecasting profits.\n","59582567":"ADD DESRCRIPTION AFTER HYPERPARAMETER TUNING","2f3e4aeb":"The figure above shows the distribution of reservations by hotel type.  We can see that the City Hotel has about twice as many reservations as the Resort Hotel.  It also looks like the City Hotel has a higher percentage of cancelations.","9fa11321":"Looks like accuracy decreased slightly when run on the test set.","b3d1d336":"#### View Basic Attributes of Data:","15b54182":"---\nLets investigate how cancelations are different between the two types of hotels in the data set, City and Resort.","789b1776":"---\n### Build ML Model with XGBoost:\nI chose the XGBoost as my machine learning model mainly due to the fact that I am trying to increase my familiarity with the package and algorithm.  XGBoost is a decision-tree-based ensemble machine leaning algorithm that uses gradient boosting.  Check out this article for more: https:\/\/towardsdatascience.com\/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d","bd6009e1":"By normalizing column height, it is easy to compare the number of reservations across months.","6c4cff76":"This data set has 32 variables with 119390 rows.  It looks like there are a lot of categorical variables in this dataset mixed with dates as well.  An interesting metric they keep track of is number of special requests.  Who knew hotels\/resorts kept track of such things.","31ca3586":"This model produced an accuracy of 85.5%.  This is not bad given that we have default parameters and it only took just under 2 minutes to train.  We can see that the top 3 most important features in the model were *adr*, *lead_time*, and *agent*.  This makes sense as people who put down a lot of money on their reservation are less likely to cancel.","24b76f90":"Select Features for Model:","caa68d8c":"The cancelations at the Resort Hotel appear to be much more consistent across months when compared to the City Hotel.  The reservations that were not canceled seem to follow a parabolic trend peaking in August for both hotels.  ","da8d23c7":"#### Resort Hotel Model","c99bc8ff":"---","7d2c5290":"---\n### Create Correlation Matrix\nBefore we begin modeling, lets examine a correlation matrix for our data set to see if any variables are highly correlated with cancelations. ","a4a4c609":"The plot above shows the number of hotel and resort reservations with color encoded to year of reservation.  By encoding color to year of reservations, we can see that there are almost no reservations made in 2014 and no reservations made from October on in 2017.  There are also not many reservations made in the early months of 2015.  It\u2019s very important to note that the reason for these differences unknown and we can only speculate.  Perhaps the hotels started collecting data in 2014 but didn\u2019t start regularly collecting data till July 2015.  In the next plot, we will normalize the column height to enable more accurate comparisons across months.","fb87634d":"### Build Model for Just Hotel and Just Resort:","5aa7c300":"Check distribution of test data to ensure its been evenly split:","26238712":"To get a better understanding of the performance of our model, the precision, recall, and f1 score will be calculated.  The precision of a model is the ratio of true positives\/(true positives + false positives).  The recall of a model is the true positives\/(true positives + false negatives).  Precision and recall can be easily understood through a fishing example.  Recall is the size of the fishing net cast and precision is how many of whatever you catch are fish.  The best of both worlds is a net that is just the right size that catches only fish.  The F1 score is the harmonic mean of precision and recall. \n\nHere the model has a good combination of precision and recall.  Time to see if it can be improved through hyperparameter tuning.\n","19bfe807":"---\n### EDA:\nFirst we will begin by importing all the necessary libraries needed for the analysis.  Some libraries may be imported later to support packages that were initially thought to be needed.","6da027de":"---","e81d0f07":"Drop variables that will poorly influence the model or don't make sense to include (like reservation status).  Separate numeric and categorical variables.  One-hot encode categorical variables.","91ff1e7c":"### Visualizations:\n\nLets explore the data with a couple of visualizations that may answer some interesting questions.  How does number of reservations trend on a monthly basis? ","33f22f8f":"Looks like this model has managed to produce an accuracy of 85.46% on the test set after some minimal hyperparamter tuning.","3635b101":"---","e094253c":"---\nWhat is the data range for reservations?","017bad09":"We'll begin by defining a function to run XGBoost","7abb4d48":"### Hyper-parameter Tuning","2f4856d6":"## Hotel Booking Model"}}