{"cell_type":{"54f3af1a":"code","c594a773":"code","07985619":"code","c5fb1f8e":"code","dbc46265":"code","d675f128":"code","b4ee3139":"code","d402bfd1":"code","353febf3":"code","32d1aaa3":"code","cf906ea1":"code","ce5cc4ec":"markdown","5cdcea3e":"markdown","6699f6a0":"markdown","48a3fbb1":"markdown","16078f99":"markdown","61d81765":"markdown","a96e35b2":"markdown","5e825838":"markdown","5832dd7c":"markdown","7d210afc":"markdown","235da302":"markdown"},"source":{"54f3af1a":"import tensorflow as tf\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib.image as implt\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3","c594a773":"train_dir = \"\/kaggle\/input\/horses-or-humans-dataset\/horse-or-human\/train\"\ntest_dir = \"\/kaggle\/input\/horses-or-humans-dataset\/horse-or-human\/validation\"\n\ntrain_humans = os.listdir(\"\/kaggle\/input\/horses-or-humans-dataset\/horse-or-human\/train\/humans\")\ntrain_horses = os.listdir(\"\/kaggle\/input\/horses-or-humans-dataset\/horse-or-human\/train\/horses\")\n\ntest_humans = os.listdir(\"\/kaggle\/input\/horses-or-humans-dataset\/horse-or-human\/validation\/humans\")\ntest_horses = os.listdir(\"\/kaggle\/input\/horses-or-humans-dataset\/horse-or-human\/validation\/horses\")","07985619":"print(\"Number of images in the train-set:\", len(train_horses) + len(train_humans))\nprint(\"Number of images in the test-set:\", len(test_horses) + len(test_humans))\n\nprint(\"\\nNumber of humans in the train-set:\", len(train_humans))\nprint(\"Number of horses in the train-set:\", len(train_horses))\n\nprint(\"\\nNumber of humans in the test-set:\", len(test_humans))\nprint(\"Number of horses in the test-set:\", len(test_horses))","c5fb1f8e":"import random\n\nfig, ax = plt.subplots(2,4, figsize=(15, 8))\nfor i in range(4):\n    x = random.randint(0, len(train_horses))\n    ax[0, i].imshow(implt.imread(train_path + '\/humans\/' + train_humans[x]))\n    ax[1, i].imshow(implt.imread(train_path + '\/horses\/' + train_horses[x]))","dbc46265":"fig, ax = plt.subplots(2,4, figsize=(15, 8))\nfor i in range(4):\n    x = random.randint(0, len(test_horses))\n    ax[0, i].imshow(implt.imread(test_path + '\/humans\/' + test_humans[x]))\n    ax[1, i].imshow(implt.imread(test_path + '\/horses\/' + test_horses[x]))","d675f128":"pre_trained_model = InceptionV3(input_shape = (150, 150, 3), \n                                include_top = False, \n                                weights = 'imagenet')\n\nfor layer in pre_trained_model.layers:\n    layer.trainable = False","b4ee3139":"#Commented out model summary because it's output too long. If you wonder uncomment that line and check layers yourself.\n#pre_trained_model.summary()\nlast_layer = pre_trained_model.get_layer('mixed7')\nprint('last layer output shape: ', last_layer.output_shape)\nlast_output = last_layer.output","d402bfd1":"from tensorflow.keras.optimizers import RMSprop\n\nx = tf.keras.layers.Flatten()(last_output)\nx = tf.keras.layers.Dense(1024, activation='relu')(x)\nx = tf.keras.layers.Dropout(0.2)(x)\nx = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = tf.keras.Model(pre_trained_model.input, x)\n\nmodel.compile(optimizer = RMSprop(lr=0.0001),\n              loss = 'binary_crossentropy',\n              metrics = ['accuracy'])","353febf3":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Add our data-augmentation parameters to ImageDataGenerator\ntrain_datagen = ImageDataGenerator(rescale = 1.\/ 255,\n                                  rotation_range = 40,\n                                  width_shift_range = 0.2,\n                                  height_shift_range = 0.2,\n                                  shear_range = 0.2,\n                                  zoom_range = 0.2,\n                                  horizontal_flip = True)\n\n# Validation or test data should not be augmented!\ntest_datagen = ImageDataGenerator(rescale = 1.\/ 255)\n\ntrain_generator = train_datagen.flow_from_directory(train_dir,\n                                                    batch_size = 20,\n                                                    class_mode = 'binary',\n                                                    target_size = (150, 150))\n\nvalidation_generator = test_datagen.flow_from_directory(test_dir,\n                                                  batch_size = 20,\n                                                  class_mode = 'binary',\n                                                  target_size = (150, 150))","32d1aaa3":"history = model.fit(\n    train_generator,\n    validation_data = validation_generator,\n    steps_per_epoch = 50,\n    epochs = 5,\n    validation_steps = 12,\n    verbose = 2)","cf906ea1":"import matplotlib.pyplot as plt\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.ylim(bottom=0.8)\nplt.title('Training and validation accuracy')\nplt.legend(loc=0)\nplt.figure()\n\n\nplt.show()","ce5cc4ec":"# 6. Visualize accuracy scores","5cdcea3e":"<center>\n<img src=\"https:\/\/camo.githubusercontent.com\/200d24b84fb905e680fa1ebaa71af582e3d6e24e\/68747470733a2f2f64327776666f7163396779717a662e636c6f756466726f6e742e6e65742f636f6e74656e742f75706c6f6164732f323031392f30362f576562736974652d5446534465736b746f7042616e6e65722e706e67\" width=800><br><\/center>\n\n\n## I decided to create this notebook while working on [Tensorflow in Practice Specialization](https:\/\/www.coursera.org\/specializations\/tensorflow-in-practice) on Coursera. I highly recommend this course, especially for beginners. Most of the ideas here belong to this course.","6699f6a0":"# 3. Explore the dataset","48a3fbb1":"## 3.2 Sample images in test-set","16078f99":"# 4. Pre-trained model","61d81765":"<img src=\"https:\/\/www.researchgate.net\/profile\/Masoud_Mahdianpari\/publication\/326421398\/figure\/fig6\/AS:649353890889730@1531829440919\/Schematic-diagram-of-InceptionV3-model-compressed-view.png\" width=800>","a96e35b2":"## 3.1 Sample images in train-set","5e825838":"![transfer_learning.png](attachment:transfer_learning.png)","5832dd7c":"# 1. What is transfer learning?\n\n\n## Transfer learning is a methodology where weights from a model trained on one task are taken and either used to construct a fixed feature extractor, as weight initialization and\/or fine-tuning. For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks.","7d210afc":"# 2. Inception V3 Deep Convolutional Architecture\n\n\n## Inception V3 by Google is the 3rd version in a series of Deep Learning Convolutional Architectures. Inception V3 was trained using a dataset of 1,000 classes (See the list of classes [here](https:\/\/gist.github.com\/yrevar\/942d3a0ac09ec9e5eb3a)) from the original ImageNet dataset which was trained with over 1 million training images.","235da302":"# 5. Build new layers on top of the pre-trained model"}}