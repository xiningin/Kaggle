{"cell_type":{"bdfaa92b":"code","93743c03":"code","8e56fe30":"code","09e532fb":"code","2a72de6a":"code","4262753e":"code","f18562ab":"code","c617c96e":"code","170dc8ff":"code","40de4a70":"code","e70d1229":"code","a5df5a8b":"code","f958c8a5":"code","c84f6430":"code","36b55c81":"code","07fa7810":"code","23ca64ff":"code","d5c91810":"code","9a5fd35a":"code","15acbef8":"code","c5305ed9":"code","de009dc3":"code","c165ab82":"code","8ada3c35":"code","4efd1f25":"code","505346fe":"code","634390da":"code","009936b1":"code","831720ec":"code","2cadc118":"code","fe6afaf6":"code","65e29f31":"markdown","9f252a22":"markdown","78ca68d5":"markdown","fdf020c4":"markdown","0ef48786":"markdown","6856dad7":"markdown","04e9f567":"markdown","f2538d70":"markdown","5b9f2dbc":"markdown","8e5c8b0d":"markdown","f2456793":"markdown","14d24ee3":"markdown","dad3d75f":"markdown","352de167":"markdown","9fbb1b7c":"markdown"},"source":{"bdfaa92b":"import numpy as np\nimport pandas as pd \n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom imblearn.over_sampling import SMOTE\n\nimport lightgbm\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\n\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nimport random\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nimport collections\nimport scipy.stats as stat\nfrom scipy.stats import iqr as IQR","93743c03":"read_directory = '\/kaggle\/input\/titanic\/'\ntrain = pd.read_csv(read_directory+'train.csv')\nX_test  = pd.read_csv(read_directory+'test.csv')\ny_test  = pd.read_csv(read_directory+'gender_submission.csv')\nPassengerID = y_test.PassengerId\ntest = pd.concat([X_test,y_test],axis=1,levels='PassangerId')","8e56fe30":"train","09e532fb":"def sd_outlier(x, axis = None, bar = 3, side = 'both'):\n    assert side in ['gt', 'lt', 'both'], 'Side should be `gt`, `lt` or `both`.'\n\n    d_z = stat.zscore(x, axis = axis)\n\n    if side == 'gt':\n        return d_z > bar\n    elif side == 'lt':\n        return d_z < -bar\n    elif side == 'both':\n        return np.abs(d_z) > bar\n\n    \ndef Q1(x, Q, axis = None):\n    return np.percentile(x, Q, axis = axis)\n\ndef Q3(x, Q, axis = None):\n    return np.percentile(x, 100-Q, axis = axis)\n\ndef IQR_outlier(x, axis = None, bar = 1.5, Q = 25, side = 'both'):\n    assert side in ['gt', 'lt', 'both'], 'Side should be `gt`, `lt` or `both`.'\n\n    d_IQR = IQR(x, axis = axis)\n    d_Q1 = Q1(x, Q, axis = axis)\n    d_Q3 = Q3(x, Q, axis = axis)\n    IQR_distance = np.multiply(d_IQR, bar)\n\n    stat_shape = list(x.shape)\n\n    if isinstance(axis, collections.Iterable):\n        for single_axis in axis:\n            stat_shape[single_axis] = 1\n    else:\n        stat_shape[axis] = 1\n\n    if side in ['gt', 'both']:\n        upper_range = d_Q3 + IQR_distance\n        upper_outlier = np.greater(x - upper_range.reshape(stat_shape), 0)\n    if side in ['lt', 'both']:\n        lower_range = d_Q1 - IQR_distance\n        lower_outlier = np.less(x - lower_range.reshape(stat_shape), 0)\n\n    if side == 'gt':\n        return upper_outlier\n    if side == 'lt':\n        return lower_outlier\n    if side == 'both':\n        return np.logical_or(upper_outlier, lower_outlier)","2a72de6a":"outlier_indexes = train[IQR_outlier(np.asarray(train.Fare), axis = 0, bar = 1.5, Q=5,  side = 'both')].index\n    \ntrain.drop(outlier_indexes,inplace=True)\n\nsns.set(style=\"ticks\", rc={'figure.figsize':(14,8)}, palette=\"pastel\")\ns = sns.boxplot(x=\"Survived\", y=\"Fare\",\n            hue=\"Sex\", palette=[\"b\", \"r\"],\n            data=train).set(title=\"Boxplot Before Removing Outliers\")","4262753e":"sns.set(style=\"ticks\", rc={'figure.figsize':(14,8)}, palette=\"pastel\")\ns = sns.boxplot(x=\"Survived\", y=\"Age\",\n            hue=\"Sex\", palette=[\"b\", \"r\"],\n            data=train).set(title=\"Boxplot After Removing Outliers\")","f18562ab":"corr = train.drop('PassengerId',axis=1).corr()\n\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\nf, ax = plt.subplots(figsize=(14, 8))\n\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\ns = sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}).set(title=\"Heatmap\")\n","c617c96e":"s = sns.PairGrid(train, y_vars=\"Survived\",\n                 x_vars=[\"Pclass\", \"Sex\", 'Embarked', 'SibSp', 'Parch'],\n                 height=5, aspect=.6)\n\ns.map(sns.pointplot, scale=1.3, errwidth=4, color=\"xkcd:plum\")\ns.set(ylim=(0, 1))\nsns.despine(fig=s.fig, left=True)","170dc8ff":"s = sns.countplot([x.split(',')[1].split('.')[0].replace(' ','') for x in train.Name]).set(title=\"Titles Count\")","40de4a70":"s = sns.countplot(sorted([str(x)[0] for x in train['Cabin'] if str(x)[0] != 'n'])).set(title=\"Cabins Count\")","e70d1229":"s = sns.countplot(sorted([str(x)[0] for x in train['Embarked'] if str(x)[0] != 'n'])).set(title=\"Embarked Count\")","a5df5a8b":"train_copy=train.copy()\ntest_copy=test.copy()","f958c8a5":"test = test_copy.copy()\ntrain = train_copy.copy()","c84f6430":"all_columns = ['Pclass','Sex','Age','SibSp','Parch','Fare','Cabin','Survived','Name','Embarked','Ticket']\n\ntrain = train.loc[:,all_columns]\ntest  = test.loc[:,all_columns]\n\ntrain.loc[:,'Name'] = [x.split(',')[1].split('.')[0].replace(' ','') for x in train.Name] # Titles of the passangers\ntest.loc[:,'Name'] = [x.split(',')[1].split('.')[0].replace(' ','') for x in test.Name]\n\ntrain.loc[:,'Cabin'] = [str(x)[0] for x in train['Cabin']] # Removing Numbers at the end of cabin types\ntest.loc[:,'Cabin']  = [str(x)[0] for x in test['Cabin']]\n\ntrain.loc[:,'Sex'] = [0 if x == 'male' else 1 for x in train.Sex] # Label encoding of genders\ntest.loc[:,'Sex']  = [0 if x == 'male' else 1 for x in test.Sex]\n\ntrain = train.merge(pd.DataFrame(train.groupby('Ticket')['Ticket'].count()>1),left_on='Ticket',right_index = True)\ntrain['Is_Multi_Ticket'] = [0 if not x else 1 for x in train['Ticket_y']]\ntrain.drop(['Ticket','Ticket_x','Ticket_y'],axis=1,inplace=True)\n\ntest = test.merge(pd.DataFrame(test.groupby('Ticket')['Ticket'].count()>1),left_on='Ticket',right_index = True)\ntest['Is_Multi_Ticket'] = [0 if not x else 1 for x in test['Ticket_y']]\ntest.drop(['Ticket','Ticket_x','Ticket_y'],axis=1,inplace=True)","36b55c81":"train = pd.get_dummies(train, columns=['Cabin'], drop_first=True)  # One Hot Encoding for Cabin column\ntest  = pd.get_dummies(test, columns=['Cabin'], drop_first=True)\n\ntrain = pd.get_dummies(train, columns=['Pclass'], drop_first=True) # One Hot Encoding for Pclass column\ntest  = pd.get_dummies(test, columns=['Pclass'], drop_first=True)\n\ntrain.drop('Cabin_T',axis=1, inplace=True)\n\nconcat   = pd.concat([train,test])  # to use in further processes, creating concatted dataframe\ncolumns  = concat.columns\n\ntop_titles = [x[0] for x in concat[['Name']].value_counts()[concat[['Name']].value_counts()>50].keys()]\ntrain.loc[~train.Name.isin(top_titles),'Name'] = 'Others'\ntest.loc[~test.Name.isin(top_titles),'Name'] = 'Others'\nconcat.loc[~concat.Name.isin(top_titles),'Name'] = 'Others'\n\nenc = OneHotEncoder(drop='first')\nenc_fit = enc.fit(concat[['Name']])  # One Hot Encoding for Name column\ntrain = pd.concat([train.reset_index(drop=True)\n                   ,pd.DataFrame(enc_fit.transform(train[['Name']]).toarray(),\n                                 columns=['Name_'+str(x) for x in range(1,len(top_titles)+1)])],axis=1)\ntest = pd.concat([test.reset_index(drop=True)\n                  ,pd.DataFrame(enc_fit.transform(test[['Name']]).toarray(),\n                                columns=['Name_'+str(x) for x in range(1,len(top_titles)+1)])],axis=1)\n\ntrain.drop('Name',axis=1, inplace=True)\ntest.drop('Name',axis=1, inplace=True)\n\ntrain.loc[train.SibSp>=3,'SibSp'] = 3.0\ntest.loc[test.SibSp>=3,'SibSp'] = 3.0\n\nconcat   = pd.concat([train,test]) # to use in further processes, creating concatted dataframe\ncolumns  = concat.columns\n\nenc = OneHotEncoder(drop='first')\nenc_fit = enc.fit(concat[['SibSp']]) \n\ntrain = pd.concat([train,pd.DataFrame(enc_fit.transform(train[['SibSp']]).toarray(),\n                                      columns=['SibSp_'+str(x) for x in range(1,train.SibSp.nunique())])],axis=1)\ntest = pd.concat([test,pd.DataFrame(enc_fit.transform(test[['SibSp']]).toarray(),\n                                    columns=['SibSp_'+str(x) for x in range(1,train.SibSp.nunique())])],axis=1)\n\ntrain.drop('SibSp',axis=1, inplace=True)\ntest.drop('SibSp',axis=1, inplace=True)\n\ntrain.loc[train.Parch>=3,'Parch'] = 3.0\ntest.loc[test.Parch>=3,'Parch'] = 3.0\n\nconcat   = pd.concat([train,test]) # to use in further processes, creating concatted dataframe\ncolumns  = concat.columns\n\nenc = OneHotEncoder(drop='first')\nenc_fit = enc.fit(concat[['Parch']]) \n\ntrain = pd.concat([train,pd.DataFrame(enc_fit.transform(train[['Parch']]).toarray(),\n                                      columns=['Parch'+str(x) for x in range(1,train.Parch.nunique())])],axis=1)\ntest = pd.concat([test,pd.DataFrame(enc_fit.transform(test[['Parch']]).toarray(),\n                                    columns=['Parch'+str(x) for x in range(1,train.Parch.nunique())])],axis=1)\n\ntrain.drop('Parch',axis=1, inplace=True)\ntest.drop('Parch',axis=1, inplace=True)\n\ntrain = train[~train.Embarked.isna()]\n\nconcat   = pd.concat([train,test]) # to use in further processes, creating concatted dataframe\ncolumns  = concat.columns\n\nenc = OneHotEncoder(drop='first')\nenc_fit = enc.fit(concat[['Embarked']]) \n\ntrain = pd.concat([train,pd.DataFrame(enc_fit.transform(train[['Embarked']]).toarray(),\n                                      columns=['Embarked'+str(x) for x in range(1,train.Embarked.nunique())])],axis=1)\ntest = pd.concat([test,pd.DataFrame(enc_fit.transform(test[['Embarked']]).toarray(),\n                                    columns=['Embarked'+str(x) for x in range(1,train.Embarked.nunique())])],axis=1)\n\ntrain.drop('Embarked',axis=1, inplace=True)\ntest.drop('Embarked',axis=1, inplace=True)\n\nconcat   = pd.concat([train,test]) # to use in further processes, creating concatted dataframe\ncolumns  = concat.columns\n","07fa7810":"train = train[(train.Survived == 0) | (train.Survived == 1)] ","23ca64ff":"imp = SimpleImputer()\nimp_fit = imp.fit(concat)\ntrain = pd.DataFrame(imp_fit.transform(train),columns=columns)\ntest  = pd.DataFrame(imp_fit.transform(test),columns=columns)","d5c91810":"X_train = train.drop('Survived',axis=1)\ny_train = train['Survived']\nX_test  = test.drop('Survived',axis=1)\ny_test  = test['Survived']\n\n\nconcat = pd.concat([X_train,X_test])\ncolumns  = concat.columns","9a5fd35a":"std_scaler = StandardScaler()\nscale_fit = std_scaler.fit(concat) \nX_train = pd.DataFrame(scale_fit.transform(X_train),columns=columns)\nX_test  = pd.DataFrame(scale_fit.transform(X_test),columns=columns)","15acbef8":"pd.DataFrame(y_train,columns=['Survived']).groupby('Survived').Survived.count()","c5305ed9":"oversample = SMOTE() \nX_train, y_train = oversample.fit_resample(X_train, y_train)\npd.DataFrame(y_train,columns=['Survived']).groupby('Survived').Survived.count()","de009dc3":"X_train, X_val, y_train, y_val = train_test_split(X_train,y_train, test_size = 0.1)","c165ab82":"callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n\ninput_dim = X_train.shape[1]\ndropout = 0.2\n\n# Initialising the ANN\nclassifier = Sequential()\nclassifier.add(Dropout(dropout, input_shape=(input_dim,)))\nclassifier.add(Dense(units = round(.5*input_dim), kernel_initializer = 'uniform', activation = 'relu', input_dim = input_dim))\nclassifier.add(Dropout(.8*dropout))\n\n# Adding the output layer\nclassifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n# Compiling the ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['binary_accuracy'])\n\n# Fitting the ANN to the Training set\nprogress = classifier.fit(X_train, y_train, batch_size = 32, epochs = 150,\n               callbacks = [callback], validation_data=(X_val, y_val), verbose=0)\n\nresults = classifier.evaluate(X_test, y_test, batch_size=16)","8ada3c35":"y_pred = classifier.predict(X_test)\nprint(classification_report(y_test, [round(x) for x in [item for sublist in y_pred for item in sublist]]))","4efd1f25":"file_name = \"submission.csv\"\n\ny_pred_series = pd.Series([int(round(x)) for x in y_pred.flatten()], name = 'Survived')\n\nfile = pd.concat([PassengerID, y_pred_series], axis = 1)\n\nfile.to_csv(file_name, index = False)","505346fe":"epochs_range=range(len(progress.history['binary_accuracy']))\nfig = plt.figure(figsize=(16,6))\n \ndef get_graphs():\n    xs =[]\n    ys =[]\n    for i in range(10):\n        xs.append(i)\n        ys.append(random.randrange(10))\n    return xs, ys\n \nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n \nx, y = get_graphs()\nax1.plot(epochs_range, progress.history['binary_accuracy'])\nax1.plot(epochs_range, progress.history['val_binary_accuracy'])\nax1.set_ylabel('Accuracy',fontsize=19)\nax1.set_xlabel('Epochs',fontsize=19)\nax1.legend(('Train','Validation'),  loc='lower right', shadow=True, fontsize=12)\nax1.tick_params(axis ='both', which ='both', length = 0, labelsize=12)\n\nx, y = get_graphs()\nax2.plot(epochs_range, progress.history['loss'])\nax2.plot(epochs_range, progress.history['val_loss'])\nax2.set_ylabel('Loss',fontsize=19)\nax2.set_xlabel('Epochs',fontsize=19)\nax2.legend(('Train','Validation'),  loc='upper right', shadow=True, fontsize=12)\nax2.tick_params(axis ='both', which ='both', length = 0, labelsize=12)\n     \nplt.show()","634390da":"train_data = lightgbm.Dataset(X_train, label=y_train)\nvalid_data = lightgbm.Dataset(X_val, label=y_val)\n\nparams={\n    'learning_rate':0.001,\n    'objective':'binary',\n    'boosting_type':'gbdt',\n    'metric':'binary_logloss'\n}\n\nmodel = lightgbm.train(params,\n                       train_data,\n                       valid_sets=valid_data,\n                       num_boost_round=100000,\n                       verbose_eval=50,\n                       early_stopping_rounds=100\n                       )\n","009936b1":"file_name = \"submission.csv\"\n\ny_pred_series = pd.Series(predictions, name = 'Survived')\n\nfile = pd.concat([PassengerID, y_pred_series], axis = 1)\n\nfile.to_csv(file_name, index = False)","831720ec":"predictions = [round(x) for x in model.predict(X_test,axis=1)]\n\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n\nmetrics = pd.DataFrame([['Accuracy',accuracy_score(list(y_test), predictions)],\n                        ['Precision:',precision_score(list(y_test), predictions, average=\"macro\",zero_division=0)],\n                        ['Recall:',recall_score(list(y_test), predictions, average=\"macro\")],\n                        ['F1 Score:',f1_score(list(y_test), predictions, average=\"macro\")]],columns = ['Metric','Score'])\n\nmetrics","2cadc118":"feature_importances = pd.DataFrame([model.feature_importance(),[x.split('_')[0] for x in X_train.columns]])\nfeature_importances = feature_importances.T\nfeature_importances.columns = ['Importance','Feature']\n\nfeature_importances = feature_importances.groupby('Feature')['Importance'].sum()","fe6afaf6":"warnings.simplefilter(action='ignore', category=FutureWarning)\n\nfeature_imp = pd.DataFrame({'Value':feature_importances.values,'Feature':feature_importances.index})\n\nfeature_imp = feature_imp.groupby('Feature').sum().reset_index()\nplt.figure(figsize=(40, 20))\nsns.set(font_scale = 5)\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", \n                                                    ascending=False)[0:13])\nplt.title('LightGBM Feature Importances')\nplt.tight_layout()\nplt.savefig('lgbm_importances-01.png')\nplt.show()","65e29f31":"### Train Test Split","9f252a22":"## Importing Libraries and Data","78ca68d5":"### Handling Missing Values","fdf020c4":"### Standard Scaling","0ef48786":"To be Continued...","6856dad7":"## Data Wrangling","04e9f567":"### Train Validation Split","f2538d70":"### Over Sampling","5b9f2dbc":"![Titanic****](https:\/\/i.milliyet.com.tr\/MolatikDetayBig\/2020\/04\/14\/fft371_mf33115214.Jpeg)\n\nOn his first journey on the 15th April 1912, Titanic sank after colliding with an iceberg after being generally considered \"sinkable\" RMS. Sadly, the number of lifeboats on board was not adequate for everyone, causing the 1502 deaths of 2224 passengers and crews.\n\nWhile there was some element of chance in survival, some groups of people tend to be more likely than others to survive.\n\nPurpose of this notebook to answer this question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc). To solve the problem, we developed a solution for titanic disaster by using basic feature engineering techniques and two different machine learning algorithms.   ","8e5c8b0d":"## Models","f2456793":"To correct inequality of target column, we used Synthetic Minority Oversampling Technique(SMOTE technique).","14d24ee3":"### 2- LIGHTGBM","dad3d75f":"### 1- Neural Network - Keras","352de167":"### Encodings","9fbb1b7c":"# Titanic: Keras + LightGBM"}}