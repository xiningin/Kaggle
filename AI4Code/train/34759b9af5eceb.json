{"cell_type":{"ae20cf17":"code","0370a2ce":"code","6c6bcd77":"code","49e4e250":"code","d092829a":"code","1411e451":"code","04764586":"code","32f082c1":"code","d86d18b3":"code","d1542a40":"code","54b6191a":"code","23806846":"code","eefb458c":"code","701b78de":"code","6b9b42b9":"code","8377cc2a":"code","b51a11a6":"code","4d0ca45d":"code","9910c5c5":"code","361bfd43":"code","0392af79":"code","f129369f":"code","a2bb6115":"code","ca8a814c":"code","28669d07":"code","27ffd4cf":"code","0a39fe96":"code","69ea2e15":"code","d422e957":"markdown","fc41daf6":"markdown","262d0a56":"markdown","2ded35d9":"markdown","07a2eb31":"markdown","9191bdb1":"markdown","6bda2076":"markdown","5d2f2bc1":"markdown","970b6cd5":"markdown","66bd5632":"markdown","7df6fbe0":"markdown","7e2275d9":"markdown","b2569008":"markdown","5e929152":"markdown","2678e17e":"markdown","1d9e73ee":"markdown","8beb54d1":"markdown","e4eea2b9":"markdown","1658fef7":"markdown","3f647b8b":"markdown","c69a5a1d":"markdown","8587c796":"markdown","f0ee23dc":"markdown","ac99ceca":"markdown","71d9e755":"markdown","b49b6fa0":"markdown","0fcd52d7":"markdown","4049c461":"markdown","0ddca415":"markdown","22498b42":"markdown","3d993612":"markdown","6a821894":"markdown","15b07749":"markdown","969078b2":"markdown","e70d7e89":"markdown","c5b53696":"markdown","24d96d0a":"markdown"},"source":{"ae20cf17":"# Loading packages\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix","0370a2ce":"# import csv\ndf = pd.read_csv('..\/input\/UCI_Credit_Card.csv')\ndf.head()","6c6bcd77":"df.shape","49e4e250":"# Checking missing data\ndf.isnull().sum()","d092829a":"# Check the summary for each feature\ndf.describe().transpose()","1411e451":"df['default.payment.next.month'].value_counts()\nplt.title('Default Payment Next Month - data imbalance check')\nax1 = sns.countplot(x= 'default.payment.next.month', data = df)\nax1.set_xticklabels(['No Default','Default'])\nplt.show()","04764586":"# Education Distribution\nplt.title('Education Distribution')\nax2 = sns.countplot(x= 'EDUCATION', hue = 'default.payment.next.month', data = df)\nax2.set_xticklabels(['Unknown','graduate school','university','high school','others','unknown','unknown'],rotation = 90)\nplt.show()","32f082c1":"# SEX distribution\nplt.title('Sex Distribution')\nax3 = sns.countplot(x= 'SEX', hue = 'default.payment.next.month', data = df)\nax3.set_xticklabels(['Male','Female'])\nplt.show()","d86d18b3":"# Age Distribution\nplt.title('Age Distribution \\n Default(Red) vs. No Default(Grey)')\nagedist0 = df[df['default.payment.next.month']==0]['AGE']\nagedist1 = df[df['default.payment.next.month']==1]['AGE']\nsns.distplot(agedist0, bins = 100, color = 'grey')\nsns.distplot(agedist1, bins = 100, color = 'red')\nplt.show()","d1542a40":"# Credit Amount Distribution\nplt.title('Credit Amount Distribution \\n Default(Red) vs. No Default(Grey)')\ncadist0 = df[df['default.payment.next.month']==0]['LIMIT_BAL']\ncadist1 = df[df['default.payment.next.month']==1]['LIMIT_BAL']\nsns.distplot(cadist0, bins = 100, color = 'grey')\nsns.distplot(cadist1, bins = 100, color = 'red')\nplt.xlabel('Credit Limit')\nplt.show()","54b6191a":"# Define predictor and target variables with X and Y\nX = df.columns[:24]\nY = df.columns[-1]","23806846":"# training and test dataset split, leaving 30% as test set\nx_train, x_test, y_train, y_test = train_test_split(df[X],df[Y], \n                                                    test_size = .3, shuffle = True, random_state = 0)","eefb458c":"# Check splitted data for train and test sets respectively\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","701b78de":"clfLR = LogisticRegression(solver = 'lbfgs',\n                           max_iter = 500,\n                          random_state = 0)\n\nclfLR.fit(x_train,y_train)\n\npredLR = clfLR.predict(x_test)","6b9b42b9":"# Cross Validation\ncross_val_score_LR = cross_val_score(clfLR, x_test, y_test, cv = 10)\nprint('cross_val_score: ',cross_val_score_LR.mean().round(2))\n\n# Precision Score\nprint('precision score is ',precision_score(y_test, predLR).round(2))\n\n# Recall Score\nprint('recall_score is ',recall_score(y_test, predLR).round(4))\n# F1 Score\nprint('f1 score is ',f1_score(y_test, predLR).round(3))\n\n# ROC_AUC\nprint('ROC AUC is ',roc_auc_score(y_test, predLR).round(2))","8377cc2a":"clfSVC = SVC(kernel = 'rbf',\n             gamma = 'scale',\n                random_state = 0)\n\nclfSVC.fit(x_train,y_train)\n\npredSVC = clfSVC.predict(x_test)","b51a11a6":"# Cross Validation\ncross_val_score_SVC = cross_val_score(clfSVC, x_test, y_test, cv = 10)\nprint('cross_val_score: ',cross_val_score_SVC.mean().round(2))\n\n# Precision Score\nprint('precision score is ',precision_score(y_test, predSVC).round(2))\n\n# Recall Score\nprint('recall_score is ',recall_score(y_test, predSVC).round(4))\n# F1 Score\nprint('f1 score is ',f1_score(y_test, predSVC).round(3))\n\n# ROC_AUC\nprint('ROC AUC is ',roc_auc_score(y_test, predSVC).round(2))","4d0ca45d":"clfKNN = KNeighborsClassifier(n_neighbors = 3)\nclfKNN.fit(x_train,y_train)\n\npredKNN = clfKNN.predict(x_test)","9910c5c5":"# Cross Validation\ncross_val_score_KNN = cross_val_score(clfKNN, x_test, y_test, cv = 10)\nprint('cross_val_score: ',cross_val_score_KNN.mean().round(2))\n\n# Precision Score\nprint('precision score is ',precision_score(y_test, predKNN).round(2))\n\n# Recall Score\nprint('recall_score is ',recall_score(y_test, predKNN).round(4))\n# F1 Score\nprint('f1 score is ',f1_score(y_test, predKNN).round(3))\n\n# ROC_AUC\nprint('ROC AUC is ',roc_auc_score(y_test, predKNN).round(2))","361bfd43":"clfRF = RandomForestClassifier(criterion = 'gini',\n                              n_estimators = 100,\n                              verbose = False,\n                              random_state = 0)\n\nclfRF.fit(x_train,y_train)\n\npredRF = clfRF.predict(x_test)","0392af79":"# Cross Validation\ncross_val_score_RF = cross_val_score(clfRF, x_test, y_test, cv = 10)\nprint('cross_val_score: ',cross_val_score_RF.mean().round(2))\n\n# Precision Score\nprint('precision score is ',precision_score(y_test, predRF).round(2))\n\n# Recall Score\nprint('recall_score is ',recall_score(y_test, predRF).round(4))\n# F1 Score\nprint('f1 score is ',f1_score(y_test, predRF).round(3))\n\n# ROC_AUC\nprint('ROC AUC is ',roc_auc_score(y_test, predRF).round(2))","f129369f":"clfXGB = xgb.XGBClassifier()\nclfXGB.fit(x_train,y_train)\npredXGB = clfXGB.predict(x_test)","a2bb6115":"# Cross Validation\ncross_val_score_XGB = cross_val_score(clfXGB, x_test, y_test, cv = 10)\nprint('cross_val_score: ',cross_val_score_XGB.mean().round(2))\n\n# Precision Score\nprint('precision score is ',precision_score(y_test, predXGB).round(2))\n\n# Recall Score\nprint('recall_score is ',recall_score(y_test, predXGB).round(4))\n# F1 Score\nprint('f1 score is ',f1_score(y_test, predXGB).round(3))\n\n# ROC_AUC\nprint('ROC AUC is ',roc_auc_score(y_test, predXGB).round(2))","ca8a814c":"clfLGB = LGBMClassifier(n_estimators = 100,\n                           learning_rate = .2,\n                           random_state = 0)\n\nclfLGB.fit(x_train,y_train)\n\npredLGB = clfLGB.predict(x_test)","28669d07":"# Cross Validation\ncross_val_score_LGB = cross_val_score(clfLGB, x_test, y_test, cv = 10)\nprint('cross_val_score: ',cross_val_score_LGB.mean().round(2))\n\n# Precision Score\nprint('precision score is ',precision_score(y_test, predLGB).round(2))\n\n# Recall Score\nprint('recall_score is ',recall_score(y_test, predLGB).round(4))\n# F1 Score\nprint('f1 score is ',f1_score(y_test, predLGB).round(3))\n\n# ROC_AUC\nprint('ROC AUC is ',roc_auc_score(y_test, predLGB).round(2))","27ffd4cf":"clfCB = CatBoostClassifier(iterations = 100,\n                           learning_rate = .2,\n                           depth = 5,\n                           eval_metric = 'AUC',\n                           random_seed = 0)\n\nclfCB.fit(x_train,y_train)\n\npredCB = clfCB.predict(x_test)","0a39fe96":"# Cross Validation\ncross_val_score_CB = cross_val_score(clfCB, x_test, y_test, cv = 10)\nprint('cross_val_score: ',cross_val_score_CB.mean().round(2))\n\n# Precision Score\nprint('precision score is ',precision_score(y_test, predCB).round(2))\n\n# Recall Score\nprint('recall_score is ',recall_score(y_test, predCB).round(4))\n# F1 Score\nprint('f1 score is ',f1_score(y_test, predCB).round(3))\n\n# ROC_AUC\nprint('ROC AUC is ',roc_auc_score(y_test, predCB).round(2))","69ea2e15":"# Confusion Matrix\ncmLR = confusion_matrix(y_test, predLR)\ncmSVC = confusion_matrix(y_test, predSVC)\ncmKNN = confusion_matrix(y_test, predKNN)\ncmRF = confusion_matrix(y_test, predRF)\ncmXGB = confusion_matrix(y_test, predXGB)\ncmLGB = confusion_matrix(y_test, predLGB)\ncmCB = confusion_matrix(y_test, predCB)\n\n# Confusion Matrix List\ncmList = [cmLR, cmSVC,cmKNN, cmRF, cmXGB, cmLGB, cmCB]\ncmTitle = ['Logistic Regression','Support Vector Machines','K Nearest Neighbors','Random Forest','XGB','LightGB','CatGBM',None]\ni = 0\nplt.figure()\nfig, ax = plt.subplots(2,4, num = 6, figsize = (30,10))\nfor cm in cmList:\n    i += 1\n    plt.subplot(2,4,i)\n    plt.title(cmTitle[i-1])\n    sns.heatmap(cm, annot = True, cmap = 'YlGnBu')\nplt.show();","d422e957":"### Random Forest","fc41daf6":"As the age increases to 30, the probability of default increases. Meanwhile, when clients are over 30, the probability decreases when aging.","262d0a56":"Check whether there is missing data in each columns.","2ded35d9":"# Credit Card Default Predictive Modelling","07a2eb31":"### Logistic Regression","9191bdb1":"The dataset is from the UCI Machine Learning Repository, which contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005.\n\nThere are 25 variables, including 24 predictor variables and 1 target variable, as following:\n* ID: ID of each client\n* LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family\/supplementary credit\n* SEX: Gender (1=male, 2=female)\n* EDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n* MARRIAGE: Marital status (1=married, 2=single, 3=others)\n* AGE: Age in years\n* PAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, ... 8=payment delay for eight months, 9=payment delay for nine months and above)\n* PAY_2: Repayment status in August, 2005 (scale same as above)\n* PAY_3: Repayment status in July, 2005 (scale same as above)\n* PAY_4: Repayment status in June, 2005 (scale same as above)\n* PAY_5: Repayment status in May, 2005 (scale same as above)\n* PAY_6: Repayment status in April, 2005 (scale same as above)\n* BILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)\n* BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\n* BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\n* BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\n* BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\n* BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\n* PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\n* PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\n* PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\n* PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\n* PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\n* PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)\n* <font color = 'blue'>default.payment.next.month: Default payment (1=yes, 0=no)   \u2014 Target Variable<\/font>","6bda2076":"# Conclusions","5d2f2bc1":"Check the summary for each feature (column).","970b6cd5":"Then, let's take a look at how different predictors affect our target.","66bd5632":"### LightGBM","7df6fbe0":"First, divide the features into predictor (X) and target(Y) before fitting the models.","7e2275d9":"### Confusion Matrix","b2569008":"### XGBoost","5e929152":"First, check the data imbalance for the target, \"Default\" and \"No Default\" classes.","2678e17e":"Female has more probability of default than male.","1d9e73ee":"### CatBoostClassifier","8beb54d1":"Basically this is a binary classification problem. The percentage of \"Default\" class is about <font color = 'blue'>22%<\/font>, so the data imbalance is not significant. ","e4eea2b9":"# Exploratory Data Analysis","1658fef7":"Then, split the dataset into train and test sets.","3f647b8b":"From above plot, we can see that most of the defaulters have the degree of graduate\/university\/high school. Among them, clients who have university degree are more likely to default than others.","c69a5a1d":"# Preparation","8587c796":"### K Nearest Neighbors","f0ee23dc":"* Default of credit card clients dataset, https:\/\/archive.ics.uci.edu\/ml\/datasets\/default+of+credit+card+clients\n* Machine Learning Pipeline, https:\/\/towardsdatascience.com\/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b\n* Feature distribution, https:\/\/www.kaggle.com\/gpreda\/default-of-credit-card-clients-predictive-models \n* Logistic Regression, https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html\n* Support Vector Machines, https:\/\/scikit-learn.org\/stable\/modules\/svm.html\n* K Nearest Neighbors, https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html\n* Random Forest Classifier, https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\n* XGBoost, https:\/\/xgboost.readthedocs.io\/en\/latest\/ \n* LightGBM, https:\/\/lightgbm.readthedocs.io\/en\/latest\/Python-API.html\n* CatBoost, https:\/\/tech.yandex.com\/catboost\/doc\/dg\/concepts\/python-reference_catboostclassifier-docpage\/\n* Cross Validation, https:\/\/www.ritchieng.com\/machine-learning-cross-validation\/ \n* Model Evaluation, https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#model-evaluation\n* Accuracy & Precision & Recall & F1, https:\/\/towardsdatascience.com\/accuracy-precision-recall-or-f1-331fb37c5cb9\n* Feature Importance, https:\/\/tech.yandex.com\/catboost\/doc\/dg\/features\/feature-importances-calculation-docpage\/\n* Parameter Tuning, https:\/\/tech.yandex.com\/catboost\/doc\/dg\/concepts\/parameter-tuning-docpage\/\n* Feature Engineering, https:\/\/towardsdatascience.com\/understanding-feature-engineering-part-2-categorical-data-f54324193e63","ac99ceca":"# Content\n* Introduction\n* Preparation\n* Exploratory Data Analysis\n* Modelling & Evaluation\n* Conclusions\n* Reference","71d9e755":"# Introduction","b49b6fa0":"### Support Vector Machine","0fcd52d7":"Check the first 5 rows of the dataset ","4049c461":"This kernel is to practice data analysis and machine learning techniques. The aim is to predict default of credit card clients using several classification models and compare the performances accordingly.\n\nAfter that, the model with best performance will be chosen and optimized with feature engineering and parameter tuning.","0ddca415":"The target variable is \"default.payment.next.month\". There are 24 predictors. ","22498b42":"* Following the machine learning pipeline, we have analyzed selected features (both predictors and target) distributions, built models and evaluated the performances of each model. \n* 7 different models are used, including logistic regression, support vector machines, K nearest neighbors, random forest, XGBoost, LightBoost and CatBoost.\n* The techniques in evaluating the performances of the models are cross validation, precision score, recall score, F1 score, ROC_AUC and confusion matrix.\n* Using the default parameters in all models, Gradient Boosting models outperform others, among which CatBoost has the best performance.\n\n\n\n### FURTHER WORK\n* Feature Engineering is not applied in this kernel. For real business case, it is better to communicate with different teams to figure out the best approach. After all, \"there is no free lunch\u201d.  For example, One-Hot Encoding can be used.\n* The ratio of \"default\" vs \"no default\" is about 1:3 in the dataset. It may affect the accuracy of each model. There are several ways to solve it. For instance, SMOTE.\n* Parameter tuning can be applied as well. For example, to leverage between different learning_rate and n_estimators combos in gradient boosting models.\n* HAVE FUN.","3d993612":"### Modelling prepration","6a821894":"Check the number of rows and columns.","15b07749":"# Modelling & Evaluation","969078b2":"Clients with lower amount tend to default. Especially those with credit amount around 50000 default most.","e70d7e89":"From above confusion matrices, it is observed:\n* Logistic Regression has no false positive, but most false negative. It is overfitting.\n* Ensemble models perform better than others in true positive.\n* It depends on the cost of event ( Cost of False Positive & Cost of False Negative) to further choose which gradient boosting model will be selected for further work","c5b53696":"### About the dataset","24d96d0a":"# Reference"}}