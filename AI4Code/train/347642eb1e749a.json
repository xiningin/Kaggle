{"cell_type":{"6f3fd2ff":"code","c867fb5a":"code","5c6aa63d":"code","7dbd1ec6":"code","666e215c":"code","c970194a":"code","4758ffb7":"code","68cbff99":"code","05d60f08":"code","b0dc1856":"code","4229dec0":"code","0f537457":"code","e27c9e35":"code","0c5039c2":"code","b7835ed0":"code","3d8179e0":"code","0b044538":"code","5e261052":"code","ac571470":"code","44d1a763":"markdown","17a5eb3f":"markdown","87583f04":"markdown","a4b411c6":"markdown","9f1514e8":"markdown","926142cd":"markdown","5e0e8e8d":"markdown","1db57fed":"markdown","3c4a2ab3":"markdown","f21b3879":"markdown","a840f1b3":"markdown","33d30476":"markdown","1c0b945e":"markdown","d035f4d7":"markdown","a7284609":"markdown","c8bc5acd":"markdown","90a2f3f2":"markdown"},"source":{"6f3fd2ff":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imread\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import Model, Sequential","c867fb5a":"from pathlib import Path\nimport zipfile\n\ntrain_zip_path = '..\/input\/carvana-image-masking-challenge\/train.zip'\nmasks_zip_path = '..\/input\/carvana-image-masking-challenge\/train_masks.zip'\ntest_zip_path = '..\/input\/carvana-image-masking-challenge\/test.zip'\n\nif not Path('\/kaggle\/working\/train').exists():\n    with zipfile.ZipFile(train_zip_path,'r') as z:\n        z.extractall('\/kaggle\/working')\nif not Path('\/kaggle\/working\/train_masks').exists():\n    with zipfile.ZipFile(masks_zip_path,'r') as z:\n        z.extractall('\/kaggle\/working')\nif not Path('\/kaggle\/working\/test').exists():\n    pass\n    # with zipfile.ZipFile(test_zip_path,'r') as z:\n    #    z.extractall('\/kaggle\/working')","5c6aa63d":"print(\"train set:  \", len(os.listdir(\"\/kaggle\/working\/train\")))\nprint(\"train masks:\", len(os.listdir(\"\/kaggle\/working\/train_masks\")))\n#print(\"test set:   \", len(os.listdir(\"\/kaggle\/working\/test\")))","7dbd1ec6":"from glob import glob\n\nroot_dir = \"\/kaggle\/working\"\ntrain_path = os.path.join(root_dir, \"train\")\ntrain_masks_path = os.path.join(root_dir, \"train_masks\")\n#test_path = os.path.join(root_dir, \"test\")\n\ntrain_filepaths = glob(os.path.join(train_path, \"*.jpg\"))\ntrain_masks_filepaths = glob(os.path.join(train_masks_path, \"*.gif\"))\n#test_filepaths = glob(os.path.join(test_path, \"*.jpg\"))\n\n# Get unique ids of images\ndef get_root_name(filepaths):\n    file_names = [os.path.basename(filepath) for filepath in filepaths]\n    root_name = [name.split(\"_\")[0] for name in file_names]\n    return root_name\n\nall_train_ids = set(get_root_name(train_filepaths))\nall_train_masks_ids = set(get_root_name(train_masks_filepaths))\n#all_test_ids = set(get_root_name(test_filepaths))","666e215c":"print(\"training set:       \", len(all_train_ids), \" different cars\")\nprint(\"training masks set: \", len(all_train_masks_ids), \"different cars\")\n#print(\"test set:           \", len(all_test_ids), \"different cars\")","c970194a":"def display_images():\n    plt.figure(figsize=(15, 25))\n    title = ['Input Image', 'Mask']\n\n    for i in range(0, 10, 2):\n        plt.subplot(5, 2, i+1)\n        plt.title(title[0])\n        path_img = root_dir + \"\/train\/\" + list(all_train_ids)[i] + f\"_0{i+1}.jpg\"\n        plt.imshow(imread(path_img))\n        plt.axis(\"off\")\n\n        plt.subplot(5, 2, i+2)\n        plt.title(title[1])\n        path_mask_img = root_dir + \"\/train_masks\/\" + list(all_train_ids)[i] + f\"_0{i+1}_mask.gif\"\n        plt.imshow(imread(path_mask_img))\n        plt.axis(\"off\")\n    plt.show()\n\ndisplay_images()","4758ffb7":"def get_image_id(path):\n    return os.path.splitext(os.path.basename(path))[0]\n\ndf = pd.DataFrame(dict(image_path=train_filepaths))\ndf['image_id'] = df['image_path'].map(lambda path: get_image_id(path))\ndf['mask_path'] = df['image_path'].map(\n    lambda x: x.replace('train', 'train_masks').replace('.jpg', '_mask.gif'))\ndf['car_id'] = df['image_id'].map(lambda img_id: img_id.split('_')[0])","68cbff99":"from sklearn.model_selection import train_test_split\n\ndef split_data(ids, col=\"car_id\"):\n    train_ids, valid_ids = train_test_split(ids, random_state=42, test_size=.2)\n    valid_ids, test_ids = train_test_split(valid_ids, random_state=42, test_size=.5)\n    train_df = df[df[col].isin(train_ids)]\n    valid_df = df[df[col].isin(valid_ids)]\n    test_df = df[df[col].isin(test_ids)]\n    return train_df, valid_df, test_df\n\ntrain_df, valid_df, test_df = split_data(list(all_train_ids))\nprint(\"train_df: \", train_df.shape[0])\nprint(\"valid_df: \", valid_df.shape[0])\nprint(\"test_df:  \", test_df.shape[0])","05d60f08":"train_df.head()","b0dc1856":"from tensorflow.image import stateless_random_crop, stateless_random_brightness\n\nIMG_SIZE = [512, 512]\nrng = tf.random.Generator.from_seed(1)\n\ndef decode(path):\n    img = tf.io.read_file(path) \n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, IMG_SIZE)\n    img = img \/ 255.0\n    return img\n\n@tf.function\ndef preprocess(image_path, mask_path):\n    image = decode(image_path)\n    mask = decode(mask_path)\n    mask = mask[:, :, :1] # take one channel\n    return image, mask\n\n@tf.function\ndef data_augmentation(image, mask):\n    if rng.uniform(()) > 0.5: \n        image = tf.image.flip_left_right(image)\n        mask = tf.image.flip_left_right(mask)\n\n    seed = rng.make_seeds(2)[0]\n    image = stateless_random_brightness(image, max_delta=0.1, seed=seed)\n    return image, mask\n\ndef make_dataset(df, shuffle=False, augment=False, batch_size=16, buffer_size=1000):\n    ds = tf.data.Dataset.from_tensor_slices((df[\"image_path\"].values, df[\"mask_path\"].values))\n    ds = ds.map(preprocess, num_parallel_calls=5)\n    if shuffle:\n        ds = ds.shuffle(buffer_size)\n    if augment:\n        ds = ds.map(data_augmentation, num_parallel_calls=tf.data.AUTOTUNE)\n    ds = ds.batch(batch_size)\n    return ds.prefetch(1)\n\ntrain_data = make_dataset(train_df, shuffle=True, augment=True)\nvalid_data = make_dataset(valid_df)\ntest_data = make_dataset(test_df)","4229dec0":"def upsample(filters, size, strides, dropout=None):\n    \"\"\"Upsample the input\"\"\"\n\n    initializer = tf.random_normal_initializer(0., 0.02)\n\n    result = Sequential()\n    result.add(tf.keras.layers.Conv2DTranspose(filters, size, strides=strides,\n                                      padding=\"same\",\n                                      kernel_initializer=initializer,\n                                      use_bias=False))\n    result.add(tf.keras.layers.BatchNormalization())\n    if dropout:\n        result.add(tf.keras.layers.Dropout(dropout))\n    result.add(tf.keras.layers.ReLU())   \n    return result","0f537457":"from tensorflow.keras.applications import VGG19\n\nbase_model = VGG19(input_shape=IMG_SIZE + [3], include_top=False, weights=\"imagenet\")\n\nlayers_names = [\n    \"block2_conv1\",    # 256x256\n    \"block2_conv2\",    # 256x256\n    \"block3_conv1\",    # 128x128\n    \"block3_conv2\",    # 128x128\n    \"block4_conv1\",    # 64x64\n    \"block4_conv2\",    # 64x64\n    \"block5_conv1\",    # 32x32\n]\n\nlayers = [base_model.get_layer(name).output for name in layers_names]\ndown_stack = Model(inputs=base_model.input, outputs=layers)\ndown_stack.trainable = False\n\nup_stack = [\n    upsample(512, 3, 1),   # 32x32 -> 32x32\n    upsample(512, 3, 2),   # 32x32 -> 64x64\n    upsample(256, 3, 1),   # 64x64 -> 64x64 \n    upsample(256, 3, 2),   # 64x64 -> 128x128\n    upsample(128, 3, 1),   # 128x128 -> 128x128\n    upsample(128, 3, 2),   # 128x128 -> 256x256\n]     ","e27c9e35":"keras.backend.clear_session()\n\ndef unet_generator(output_channels=1):\n    inputs = tf.keras.layers.Input(shape=IMG_SIZE + [3])\n    x = inputs\n\n    initializer = tf.random_normal_initializer(0., 0.02)\n    output = tf.keras.layers.Conv2DTranspose(\n        output_channels, 3, strides=2, activation='sigmoid',\n        padding=\"same\", kernel_initializer=initializer\n    )\n    \n    concat = tf.keras.layers.Concatenate()\n\n    # Downsampling \n    skips = down_stack(x)\n    x = skips[-1]\n    skips = reversed(skips[:-1])\n\n    # Upsampling and establishing the skip connection\n    for up, skip in zip (up_stack, skips):\n        x = up(x)\n        if up.layers[0].strides == (2, 2):\n            concat = tf.keras.layers.Concatenate()\n            x = concat([x, skip])\n\n    x = output(x)\n    \n    return tf.keras.Model(inputs=inputs, outputs=x)\n\nmodel = unet_generator()","0c5039c2":"keras.utils.plot_model(model, show_shapes=True)","b7835ed0":"for images, masks in train_data.take(1):\n    for img, mask in zip(images, masks):\n        sample_image = img\n        sample_mask = mask\n        break","3d8179e0":"def visualize(display_list):\n    plt.figure(figsize=(15, 15))\n    title = ['Input Image', 'True Mask', 'Predicted Mask']\n    for i in range(len(display_list)):\n        plt.subplot(1, len(display_list), i+1)\n        plt.title(title[i])\n        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n        plt.axis('off')\n    plt.show()\n\ndef show_predictions(sample_image, sample_mask):\n    pred_mask = model.predict(sample_image[tf.newaxis, ...])\n    pred_mask = pred_mask.reshape(IMG_SIZE[0],IMG_SIZE[1],1)\n    visualize([sample_image, sample_mask, pred_mask])\n    \nshow_predictions(sample_image, sample_mask)","0b044538":"early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=4,\n                                                    restore_best_weights=True)\n\n# Maybe needs to train for more epochs, but kaggle time is limited\nepochs = 10\n\nclass DisplayCallback(tf.keras.callbacks.Callback):\n    def on_epoch_begin(self, epoch, logs=None):\n        if (epoch + 1) % 5 == 0:\n            show_predictions(sample_image, sample_mask)\n    \nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=['accuracy'])\nmodel_history = model.fit(train_data, epochs=epochs,\n                          validation_data=valid_data,\n                          callbacks=[DisplayCallback(), early_stopping_cb])","5e261052":"loss = model_history.history['loss']\nval_loss = model_history.history['val_loss']\n\nacc = model_history.history['accuracy']\nval_acc = model_history.history['val_accuracy']\n\nplt.figure(figsize=(20, 5))\nplt.subplot(1, 2, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(loss, label='Training Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Binary Cross Entropy')\nplt.legend()\nplt.show()","ac571470":"for images, masks in test_data.take(1):\n    for img, mask in zip(images, masks):\n        show_predictions(img, mask)","44d1a763":"# Build the U-Net model","17a5eb3f":"How many different cars in training and test set","87583f04":"How many images do we have in the training and test set","a4b411c6":"# Evaluation on the Test Set","9f1514e8":"# Define the Model\n","926142cd":"Display some cars with their masks","5e0e8e8d":"# Import Libraries","1db57fed":"Here is what the model predict before training","3c4a2ab3":"# Ressources\n\n* [Image segmentation](https:\/\/www.tensorflow.org\/tutorials\/images\/segmentation) tutorial from TensorFlow.\n* [VGG16+U-Net on Carvana](https:\/\/www.kaggle.com\/kmader\/vgg16-u-net-on-carvana) kaggle kernel.\n* [.99 loss - Simple Sol. using u-Net [Keras\/TF] ](https:\/\/www.kaggle.com\/phylake1337\/99-loss-simple-sol-using-u-net-keras-tf) kaggle kernel.\n* [Data visualization](https:\/\/www.kaggle.com\/vfdev5\/data-visualization) kaggle kernel.","f21b3879":"# Training and Validation Split\n\nWe split the data by car to avoid overfitting single images.","a840f1b3":"# Analysis","33d30476":"In this notebook, I only used the training set (the test set is too big), this is why I commented the lines below.","1c0b945e":"In this project, wa are asked to use image segmentation to identify the boundaries of a car in an image.\n\nThe training set contains 5088 of car images. Each car has 16 images taken at different angles, so we have 318 unique cars. We are also provided with the mask for each image in the training set. The test set contains 100 064 car images, again each car has 16 different images, so there are 6254 unique cars in the test set.\n\nYou can found the dataset [here](https:\/\/www.kaggle.com\/c\/carvana-image-masking-challenge\/data?select=train.zip)","d035f4d7":"# Learning Curves","a7284609":"# Losses\n\nI saw in other kaggle kernels that users used the dice + binary cross-entropy loss but since only the binary cross-entropy gives me good results I didn't try the dice loss.","c8bc5acd":"To have a better performance, I used the layers of the VGG19 as the encoder for the U-Net model. The decoder will be a series of the upsample block above.","90a2f3f2":"# Preprocessing the data\n\nHere, we build an input pipline using tf.data. We first load the data from the dataframe above, then we preprocess the image and the mask, and finally, we perform a simple data augmentation."}}