{"cell_type":{"6cad3ce4":"code","a74e62e7":"code","5ba1ed07":"code","516aa4c0":"code","19ac422a":"code","31bb400c":"code","04a68552":"code","c446eb8d":"code","7fa0ddde":"code","9f879fab":"code","b8a14e6f":"code","8ab76df0":"code","07049127":"code","40a84f57":"code","39dddda2":"code","1cfd7b45":"code","69d44aac":"code","96a1efdc":"code","f128a784":"code","d75c0d3a":"code","1877a808":"code","61de84ec":"code","37c42e50":"code","bfbea89a":"code","4683e225":"code","68d13741":"code","7304abb3":"code","b18f5564":"code","78ec6bf6":"code","ab2c3390":"code","ffdb25b0":"code","89e39af8":"code","3232e036":"code","ec929f2a":"code","e57c789b":"code","ebd69983":"code","fc9e69dd":"code","dce08c57":"markdown","e719e69c":"markdown","6ea9402b":"markdown","01215b8f":"markdown","2922c858":"markdown","b0d223e9":"markdown","ea47852a":"markdown","130762b8":"markdown","7a694040":"markdown","66dd5d6a":"markdown","e795b278":"markdown","e8379561":"markdown","92611cde":"markdown"},"source":{"6cad3ce4":"!pip install -q torch==1.9.0","a74e62e7":"import torch\ntorch.__version__","5ba1ed07":"import pandas as pd\npd.set_option('display.max_colwidth', None)\nfrom sklearn import preprocessing as pp\nfrom sklearn.model_selection import train_test_split\nimport scipy.sparse as sp\nimport numpy as np\nimport random\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport time\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm","516aa4c0":"columns_name=['user_id','item_id','rating','timestamp']\ndf = pd.read_csv(\"..\/input\/movielens-100k-dataset\/ml-100k\/u.data\",sep=\"\\t\",names=columns_name)\nprint(len(df))\ndisplay(df.head(5))","19ac422a":"df = df[df['rating']>=3]\nprint(len(df))","31bb400c":"print(\"Rating Distribution\")\ndf.groupby(['rating'])['rating'].count()","04a68552":"train, test = train_test_split(df.values, test_size=0.2, random_state = 16)\ntrain = pd.DataFrame(train, columns = df.columns)\ntest = pd.DataFrame(test, columns = df.columns)","c446eb8d":"print(\"Train Size  : \", len(train))\nprint(\"Test Size : \", len (test))","7fa0ddde":"le_user = pp.LabelEncoder()\nle_item = pp.LabelEncoder()\ntrain['user_id_idx'] = le_user.fit_transform(train['user_id'].values)\ntrain['item_id_idx'] = le_item.fit_transform(train['item_id'].values)","9f879fab":"train_user_ids = train['user_id'].unique()\ntrain_item_ids = train['item_id'].unique()\n\nprint(len(train_user_ids), len(train_item_ids))\n\ntest = test[(test['user_id'].isin(train_user_ids)) & (test['item_id'].isin(train_item_ids))]\nprint(len(test))","b8a14e6f":"test['user_id_idx'] = le_user.transform(test['user_id'].values)\ntest['item_id_idx'] = le_item.transform(test['item_id'].values)","8ab76df0":"n_users = train['user_id_idx'].nunique()\nn_items = train['item_id_idx'].nunique()\nprint(\"Number of Unique Users : \", n_users)\nprint(\"Number of unique Items : \", n_items)","07049127":"latent_dim = 64\nn_layers = 3  ","40a84f57":"def convert_to_sparse_tensor(dok_mtrx):\n    \n    dok_mtrx_coo = dok_mtrx.tocoo().astype(np.float32)\n    values = dok_mtrx_coo.data\n    indices = np.vstack((dok_mtrx_coo.row, dok_mtrx_coo.col))\n\n    i = torch.LongTensor(indices)\n    v = torch.FloatTensor(values)\n    shape = dok_mtrx_coo.shape\n\n    dok_mtrx_sparse_tensor = torch.sparse.FloatTensor(i, v, torch.Size(shape))\n\n    return dok_mtrx_sparse_tensor","39dddda2":"def get_metrics(user_Embed_wts, item_Embed_wts, n_users, n_items, train_data, test_data, K):\n\n    user_Embedding = nn.Embedding(user_Embed_wts.size()[0], user_Embed_wts.size()[1], _weight = user_Embed_wts)\n    item_Embedding = nn.Embedding(item_Embed_wts.size()[0], item_Embed_wts.size()[1], _weight = item_Embed_wts)\n\n    test_user_ids = torch.LongTensor(test_data['user_id_idx'].unique())\n\n    relevance_score = torch.matmul(user_Embed_wts, torch.transpose(item_Embed_wts,0, 1))\n\n    R = sp.dok_matrix((n_users, n_items), dtype = np.float32)\n    R[train_data['user_id_idx'], train_data['item_id_idx']] = 1.0\n\n    R_tensor = convert_to_sparse_tensor(R)\n    R_tensor_dense = R_tensor.to_dense()\n\n    R_tensor_dense = R_tensor_dense*(-np.inf)\n    R_tensor_dense = torch.nan_to_num(R_tensor_dense, nan=0.0)\n\n    relevance_score = relevance_score+R_tensor_dense\n\n    topk_relevance_score = torch.topk(relevance_score, K).values\n    topk_relevance_indices = torch.topk(relevance_score, K).indices\n\n    topk_relevance_indices_df = pd.DataFrame(topk_relevance_indices.numpy(),columns =['top_indx_'+str(x+1) for x in range(K)])\n\n    topk_relevance_indices_df['user_ID'] = topk_relevance_indices_df.index\n \n    topk_relevance_indices_df['top_rlvnt_itm'] = topk_relevance_indices_df[['top_indx_'+str(x+1) for x in range(K)]].values.tolist()\n    topk_relevance_indices_df = topk_relevance_indices_df[['user_ID','top_rlvnt_itm']]\n\n    test_interacted_items = test_data.groupby('user_id_idx')['item_id_idx'].apply(list).reset_index()\n\n    metrics_df = pd.merge(test_interacted_items,topk_relevance_indices_df, how= 'left', left_on = 'user_id_idx',right_on = ['user_ID'])\n    metrics_df['intrsctn_itm'] = [list(set(a).intersection(b)) for a, b in zip(metrics_df.item_id_idx, metrics_df.top_rlvnt_itm)]\n\n\n    metrics_df['recall'] = metrics_df.apply(lambda x : len(x['intrsctn_itm'])\/len(x['item_id_idx']), axis = 1) \n    metrics_df['precision'] = metrics_df.apply(lambda x : len(x['intrsctn_itm'])\/K, axis = 1)\n\n    def get_hit_list(item_id_idx, top_rlvnt_itm):\n        return [1 if x in set(item_id_idx) else 0 for x in top_rlvnt_itm ]\n\n    metrics_df['hit_list'] = metrics_df.apply(lambda x : get_hit_list(x['item_id_idx'], x['top_rlvnt_itm']), axis = 1)\n\n    def get_dcg_idcg(item_id_idx, hit_list):\n        idcg  = sum([1 \/ np.log1p(idx+1) for idx in range(min(len(item_id_idx),len(hit_list)))])\n        dcg =  sum([hit \/ np.log1p(idx+1) for idx, hit in enumerate(hit_list)])\n        return dcg\/idcg\n\n    def get_cumsum(hit_list):\n        return np.cumsum(hit_list)\n\n    def get_map(item_id_idx, hit_list, hit_list_cumsum):\n        return sum([hit_cumsum*hit\/(idx+1) for idx, (hit, hit_cumsum) in enumerate(zip(hit_list, hit_list_cumsum))])\/len(item_id_idx)\n\n    metrics_df['ndcg'] = metrics_df.apply(lambda x : get_dcg_idcg(x['item_id_idx'], x['hit_list']), axis = 1)\n    metrics_df['hit_list_cumsum'] = metrics_df.apply(lambda x : get_cumsum(x['hit_list']), axis = 1)\n\n    metrics_df['map'] = metrics_df.apply(lambda x : get_map(x['item_id_idx'], x['hit_list'], x['hit_list_cumsum']), axis = 1)\n\n    return metrics_df['recall'].mean(), metrics_df['precision'].mean(), metrics_df['ndcg'].mean(), metrics_df['map'].mean() ","1cfd7b45":"class LightGCN(nn.Module):\n    def __init__(self, data, n_users, n_items, n_layers, latent_dim):\n        super(LightGCN, self).__init__()\n        self.data = data\n        self.n_users = n_users\n        self.n_items = n_items\n        self.n_layers = n_layers\n        self.latent_dim = latent_dim\n        self.init_embedding()\n        self.norm_adj_mat_sparse_tensor = self.get_A_tilda()\n\n    def init_embedding(self):\n        self.E0 = nn.Embedding(self.n_users + self.n_items, self.latent_dim)\n        nn.init.xavier_uniform_(self.E0.weight)\n        self.E0.weight = nn.Parameter(self.E0.weight)\n\n    def get_A_tilda(self):\n        R = sp.dok_matrix((self.n_users, self.n_items), dtype = np.float32)\n        R[self.data['user_id_idx'], self.data['item_id_idx']] = 1.0\n\n        adj_mat = sp.dok_matrix(\n                (self.n_users + self.n_items, self.n_users + self.n_items), dtype=np.float32\n            )\n        adj_mat = adj_mat.tolil()\n        R = R.tolil()\n\n        adj_mat[: n_users, n_users :] = R\n        adj_mat[n_users :, : n_users] = R.T\n        adj_mat = adj_mat.todok()\n\n        rowsum = np.array(adj_mat.sum(1))\n        d_inv = np.power(rowsum + 1e-9, -0.5).flatten()\n        d_inv[np.isinf(d_inv)] = 0.0\n        d_mat_inv = sp.diags(d_inv)\n        norm_adj_mat = d_mat_inv.dot(adj_mat)\n        norm_adj_mat = norm_adj_mat.dot(d_mat_inv)\n        \n        # Below Code is toconvert the dok_matrix to sparse tensor.\n        \n        norm_adj_mat_coo = norm_adj_mat.tocoo().astype(np.float32)\n        values = norm_adj_mat_coo.data\n        indices = np.vstack((norm_adj_mat_coo.row, norm_adj_mat_coo.col))\n\n        i = torch.LongTensor(indices)\n        v = torch.FloatTensor(values)\n        shape = norm_adj_mat_coo.shape\n\n        norm_adj_mat_sparse_tensor = torch.sparse.FloatTensor(i, v, torch.Size(shape))\n\n        return norm_adj_mat_sparse_tensor\n    \n    def propagate_through_layers(self):\n        all_layer_embedding = [self.E0.weight]\n        E_lyr = self.E0.weight\n\n        for layer in range(self.n_layers):\n            E_lyr = torch.sparse.mm(self.norm_adj_mat_sparse_tensor, E_lyr)\n            all_layer_embedding.append(E_lyr)\n\n        all_layer_embedding = torch.stack(all_layer_embedding)\n        mean_layer_embedding = torch.mean(all_layer_embedding, axis = 0)\n\n        final_user_Embed, final_item_Embed = torch.split(mean_layer_embedding, [n_users, n_items])\n        initial_user_Embed, initial_item_Embed = torch.split(self.E0.weight, [n_users, n_items])\n\n        return final_user_Embed, final_item_Embed, initial_user_Embed, initial_item_Embed\n\n    def forward(self, users, pos_items, neg_items):\n        final_user_Embed, final_item_Embed, initial_user_Embed, initial_item_Embed = self.propagate_through_layers()\n\n        users_emb, pos_emb, neg_emb = final_user_Embed[users], final_item_Embed[pos_items], final_item_Embed[neg_items]\n        userEmb0,  posEmb0, negEmb0 = initial_user_Embed[users], initial_item_Embed[pos_items], initial_item_Embed[neg_items]\n\n        return users_emb, pos_emb, neg_emb, userEmb0,  posEmb0, negEmb0","69d44aac":"lightGCN = LightGCN(train, n_users, n_items, n_layers, latent_dim)","96a1efdc":"print(\"Size of Learnable Embedding : \", list(lightGCN.parameters())[0].size())","f128a784":"def bpr_loss(users, users_emb, pos_emb, neg_emb, userEmb0,  posEmb0, negEmb0):\n  \n    reg_loss = (1\/2)*(userEmb0.norm().pow(2) + \n                    posEmb0.norm().pow(2)  +\n                    negEmb0.norm().pow(2))\/float(len(users))\n    pos_scores = torch.mul(users_emb, pos_emb)\n    pos_scores = torch.sum(pos_scores, dim=1)\n    neg_scores = torch.mul(users_emb, neg_emb)\n    neg_scores = torch.sum(neg_scores, dim=1)\n        \n    loss = torch.mean(torch.nn.functional.softplus(neg_scores - pos_scores))\n        \n    return loss, reg_loss","d75c0d3a":"def data_loader(data, batch_size, n_usr, n_itm):\n  \n    interected_items_df = data.groupby('user_id_idx')['item_id_idx'].apply(list).reset_index()\n  \n    def sample_neg(x):\n        while True:\n            neg_id = random.randint(0, n_itm - 1)\n            if neg_id not in x:\n                return neg_id\n  \n    indices = [x for x in range(n_usr)]\n    \n    if n_usr < batch_size:\n        users = [random.choice(indices) for _ in range(batch_size)]\n    else:\n        users = random.sample(indices, batch_size)\n\n    users.sort()\n  \n    users_df = pd.DataFrame(users,columns = ['users'])\n\n    interected_items_df = pd.merge(interected_items_df, users_df, how = 'right', left_on = 'user_id_idx', right_on = 'users')\n  \n    pos_items = interected_items_df['item_id_idx'].apply(lambda x : random.choice(x)).values\n\n    neg_items = interected_items_df['item_id_idx'].apply(lambda x: sample_neg(x)).values\n\n    return list(users), list(pos_items), list(neg_items)","1877a808":"optimizer = torch.optim.Adam(lightGCN.parameters(), lr = 0.005)","61de84ec":"EPOCHS = 30\nBATCH_SIZE = 1024 \nDECAY = 0.0001\nK = 10","37c42e50":"loss_list_epoch = []\nMF_loss_list_epoch = []\nreg_loss_list_epoch = []\n\nrecall_list = []\nprecision_list = []\nndcg_list = []\nmap_list = []\n\ntrain_time_list = []\neval_time_list = [] \n\nfor epoch in tqdm(range(EPOCHS)):\n    n_batch = int(len(train)\/BATCH_SIZE)\n  \n    final_loss_list = []\n    MF_loss_list = []\n    reg_loss_list = []\n  \n    best_ndcg = -1\n  \n    train_start_time = time.time()\n    lightGCN.train()\n    for batch_idx in range(n_batch):\n\n        optimizer.zero_grad()\n\n        users, pos_items, neg_items = data_loader(train, BATCH_SIZE, n_users, n_items)\n\n        users_emb, pos_emb, neg_emb, userEmb0,  posEmb0, negEmb0 = lightGCN.forward(users, pos_items, neg_items)\n\n        mf_loss, reg_loss = bpr_loss(users, users_emb, pos_emb, neg_emb, userEmb0,  posEmb0, negEmb0)\n        reg_loss = DECAY * reg_loss\n        final_loss = mf_loss + reg_loss\n\n        final_loss.backward()\n        optimizer.step()\n\n        final_loss_list.append(final_loss.item())\n        MF_loss_list.append(mf_loss.item())\n        reg_loss_list.append(reg_loss.item())\n\n\n    train_end_time = time.time()\n    train_time = train_end_time - train_start_time\n\n    lightGCN.eval()\n    with torch.no_grad():\n    \n        final_user_Embed, final_item_Embed, initial_user_Embed,initial_item_Embed = lightGCN.propagate_through_layers()\n        test_topK_recall,  test_topK_precision, test_topK_ndcg, test_topK_map  = get_metrics(final_user_Embed, final_item_Embed, n_users, n_items, train, test, K)\n\n\n    if test_topK_ndcg > best_ndcg:\n        best_ndcg = test_topK_ndcg\n      \n        torch.save(final_user_Embed, 'final_user_Embed.pt')\n        torch.save(final_item_Embed, 'final_item_Embed.pt')\n        torch.save(initial_user_Embed, 'initial_user_Embed.pt')\n        torch.save(initial_item_Embed, 'initial_item_Embed.pt')\n     \n\n    eval_time = time.time() - train_end_time\n\n    loss_list_epoch.append(round(np.mean(final_loss_list),4))\n    MF_loss_list_epoch.append(round(np.mean(MF_loss_list),4))\n    reg_loss_list_epoch.append(round(np.mean(reg_loss_list),4))\n\n    recall_list.append(round(test_topK_recall,4))\n    precision_list.append(round(test_topK_precision,4))\n    ndcg_list.append(round(test_topK_ndcg,4))\n    map_list.append(round(test_topK_map,4))\n\n    train_time_list.append(train_time)\n    eval_time_list.append(eval_time)  ","bfbea89a":"epoch_list = [(i+1) for i in range(EPOCHS)]","4683e225":"plt.plot(epoch_list, recall_list, label='Recall')\nplt.plot(epoch_list, precision_list, label='Precision')\nplt.plot(epoch_list, ndcg_list, label='NDCG')\nplt.plot(epoch_list, map_list, label='MAP')\nplt.xlabel('Epoch')\nplt.ylabel('Metrics')\nplt.legend()","68d13741":"plt.plot(epoch_list, loss_list_epoch, label='Total Training Loss')\nplt.plot(epoch_list, MF_loss_list_epoch, label='MF Training Loss')\nplt.plot(epoch_list, reg_loss_list_epoch, label='Reg Training Loss')\n\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()","7304abb3":"print(\"Averge time taken to train an epoch -> \", round(np.mean(train_time_list),2), \" seconds\")\nprint(\"Averge time taken to eval an epoch -> \", round(np.mean(eval_time_list),2), \" seconds\")","b18f5564":"print(\"Last Epoch's Test Data Recall -> \", recall_list[-1])\nprint(\"Last Epoch's Test Data Precision -> \", precision_list[-1])\nprint(\"Last Epoch's Test Data NDCG -> \", ndcg_list[-1])\nprint(\"Last Epoch's Test Data MAP -> \", map_list[-1])\n\nprint(\"Last Epoch's Train Data Loss -> \", loss_list_epoch[-1])","78ec6bf6":"!pip install -q tensorflow==1.15 ","ab2c3390":"!git clone https:\/\/github.com\/microsoft\/recommenders.git .\/recommenders_microsoft","ffdb25b0":"import sys\nimport os\nsys.path.insert(0, '.\/recommenders_microsoft')\nsys.path.insert(0, '.\/recommenders_microsoft\/reco_utils')","89e39af8":"import os\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom reco_utils.common.timer import Timer\nfrom reco_utils.recommender.deeprec.models.graphrec.lightgcn import LightGCN\nfrom reco_utils.recommender.deeprec.DataModel.ImplicitCF import ImplicitCF\nfrom reco_utils.dataset import movielens\nfrom reco_utils.dataset.python_splitters import python_stratified_split\nfrom reco_utils.evaluation.python_evaluation import map_at_k, ndcg_at_k, precision_at_k, recall_at_k\nfrom reco_utils.common.constants import SEED as DEFAULT_SEED\nfrom reco_utils.recommender.deeprec.deeprec_utils import prepare_hparams\nfrom reco_utils.recommender.deeprec.deeprec_utils import cal_metric","3232e036":"train = train.rename(columns = {'user_id':'userID', 'item_id':'itemID'})\ntest = test.rename(columns = {'user_id':'userID', 'item_id':'itemID'})","ec929f2a":"data = ImplicitCF(\n    train=train, test=test, seed=0,\n    col_user='userID',\n    col_item='itemID',\n    col_rating='rating'\n)","e57c789b":"yaml_file = '.\/recommenders_microsoft\/examples\/07_tutorials\/KDD2020-tutorial\/lightgcn.yaml'\n\n\nhparams = prepare_hparams(yaml_file,                          \n                          learning_rate=0.005,\n                          eval_epoch=1,\n                          top_k=10,\n                          save_model=False,\n                          epochs=30,\n                          save_epoch=1\n                         )","ebd69983":"model = LightGCN(hparams, data, seed=0)","fc9e69dd":"with Timer() as train_time:\n    model.fit()\n\nprint(\"Took {} seconds for training.\".format(train_time.interval))","dce08c57":"### Please do upvote the notebook if you liked the content. It will motivate me. Thanks !!","e719e69c":"### **BPR Loss**","6ea9402b":"## In the below part, I have used LightGCN original Code from Microsoft's Repository, generated the Metrics, Losses and compared that with my own output.\n#### Steps to run original LightGCN in tensorflow -> https:\/\/github.com\/microsoft\/recommenders\/blob\/main\/examples\/07_tutorials\/KDD2020-tutorial\/step5_run_lightgcn.ipynb","01215b8f":"* Light GCN was originally Developed By Microsoft and was released in Jul'2020. It is the SOTA Algorithm for Recommendation System as of now.\n* Link to the original paper -> https:\/\/paperswithcode.com\/paper\/lightgcn-simplifying-and-powering-graph\/review\/\n* There is also a Github Link to Light GCN implementation using Tensorflow -> https:\/\/github.com\/microsoft\/recommenders\/tree\/efaa3d7742183dee0846877e2dc64977098e1977\n* I have taken help from the above Repository and tried to recreate LightGCN in pytorch.\n* In the 2nd part of the notebook, I have also compared my result with the direct result from Original Tensorflow code. ","2922c858":"## **Metrics**\n### Below Function gets the 4 different metrics out of Test Data -> Recall@K, Precision@K, NDCG@K, MAP@K where K is the top K items we would like to recommend to User. ","b0d223e9":"##  **LightGCN Model**\n### **get_A_tilda** function is used to get A_tilda which will be multiplied with initial user\/item embedding (E0) to create embedding at different layers (n_layers = 3) in **propagate_through_layers** function\n### **forward** function is used to look up for initial(E0) and final embedding of a user\/item","ea47852a":"## In this Notebook, I have implemented Light GCN Model for Recommendation System from scratch using Pytorch.\n","130762b8":"## Please upvote the notebook if you have learned from it. Thanks !!","7a694040":"### As can be seen, Using both my recreated Code (in Pytorch) and original Tensorflow Implementation of Light GCN, results are quite similar. \n### At end of 30th Epoch , Recall@10 -> ~0.21, Precision@10 -> ~0.28, NDCG@10 -> ~0.35, MAP@10 -> ~0.12 and Training Loss -> ~0.12\n### Time Taken to train 1 epoch for original code is ~2.5s while through my code its ~8s and evaluation time is 0.2s and 0.4s respectively.","66dd5d6a":"#### Label Encoding the User and Item IDs","e795b278":"### **Training Loop**","e8379561":"### latent_dim is the length of the user\/item embedding. \n### n_layers is the number of times we want to propagate our initial user\/item embedding through the graph ","92611cde":"### Data Loader - Samples users and for each user it sample 1 postive item - which User interacted with in Training Data and 1 negative item - with which User have not interacted."}}