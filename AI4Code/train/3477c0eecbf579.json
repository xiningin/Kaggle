{"cell_type":{"1633669a":"code","e3388b9f":"code","d8d17266":"code","4870a4d5":"code","a30496b4":"code","28a770ba":"code","15cf7f72":"code","f3e4d51e":"code","4659e6fe":"code","2338ea0f":"code","aa969e6a":"code","76832a99":"code","f1f50bbd":"code","70a76383":"code","73c264d4":"code","29992493":"code","a587ddea":"code","ab2b776e":"code","e88ae714":"code","0df171ca":"markdown","d75a12b3":"markdown","87fa7e0d":"markdown","ff7e5609":"markdown","73cb287e":"markdown","51d02cdc":"markdown","88cd23a9":"markdown","8d2f4c4a":"markdown","10911f2f":"markdown","8f48dd63":"markdown","8cb78fb5":"markdown","fdffe7c7":"markdown"},"source":{"1633669a":"# We use the official tokenization script created by the Google team\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","e3388b9f":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\nimport tokenization\nimport matplotlib.pyplot as plt\nimport seaborn as sns","d8d17266":"def modelConstruct(network_bert, maxlength=512):\n    id_word = Input(shape=(maxlength,), dtype=tf.int32, name=\"input_word_ids\")\n    masked = Input(shape=(maxlength,), dtype=tf.int32, name=\"input_mask\")\n    id_segmented = Input(shape=(maxlength,), dtype=tf.int32, name=\"segment_ids\")\n    _, sequence_output = network_bert([id_word , masked,id_segmented])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)    \n    model = Model(inputs=[id_word , masked,id_segmented], outputs=out)\n    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])    \n    return model","4870a4d5":"def encoderbert(data, tokenizer, maxlength=512):     \n    masked = []\n    segmented = []\n    token = []   \n    for text in data:\n        text = tokenizer.tokenize(text)  \n        text = text[:maxlength-2]  \n        segmented_id = [0] * maxlength\n        sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        tokens = tokenizer.convert_tokens_to_ids(sequence)\n        padlength = maxlength - len(sequence)\n        masked_pad = [1] * len(sequence) + [0] * padlength\n        tokens += [0] * padlength      \n        token.append(tokens)\n        masked.append(masked_pad)\n        segmented.append(segmented_id)\n    return np.array(token), np.array(masked), np.array(segmented)","a30496b4":"%%time\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nnetwork_bert = hub.KerasLayer(module_url, trainable=True)","28a770ba":"df_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv', dtype={'id': np.int16, 'target': np.int8})\ndf_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv', dtype={'id': np.int16})\n\nprint('Training Set Shape = {}'.format(df_train.shape))\nprint('Training Set Memory Usage = {:.2f} MB'.format(df_train.memory_usage().sum() \/ 1024**2))\nprint('Test Set Shape = {}'.format(df_test.shape))\nprint('Test Set Memory Usage = {:.2f} MB'.format(df_test.memory_usage().sum() \/ 1024**2))","15cf7f72":"df_train_fake = df_train[df_train['target'] == 1]\nkeyword_cnt_fake = df_train_fake.keyword.value_counts()\nkeyword_cnt_fake","f3e4d51e":"ax = sns.countplot(x='target',  data=df_train)\nplt.show()","4659e6fe":"keyword_cnt = df_train.keyword.value_counts()\nkeyword_cnt","2338ea0f":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=df_train[df_train['target']==1]['text'].str.len()\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=df_train[df_train['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","aa969e6a":"missing_cols = ['keyword', 'location']\n\ndf_fig, axes = plt.subplots(ncols=2, figsize=(17, 4), dpi=100)\n\nsns.barplot(x=df_train[missing_cols].isnull().sum().index, y=df_train[missing_cols].isnull().sum().values, ax=axes[0])\nsns.barplot(x=df_test[missing_cols].isnull().sum().index, y=df_test[missing_cols].isnull().sum().values, ax=axes[1])\n\naxes[0].set_ylabel('Missing Value Count', size=15, labelpad=20)\naxes[0].tick_params(axis='x', labelsize=15)\naxes[0].tick_params(axis='y', labelsize=15)\naxes[1].tick_params(axis='x', labelsize=15)\naxes[1].tick_params(axis='y', labelsize=15)\n\naxes[0].set_title('Training Set', fontsize=13)\naxes[1].set_title('Test Set', fontsize=13)\n\nplt.show()\n\nfor df in [df_train, df_test]:\n    for col in ['keyword', 'location']:\n        df[col] = df[col].fillna(f'no_{col}')","76832a99":"df_train['target_mean'] = df_train.groupby('keyword')['target'].transform('mean')\n\ndf_fig = plt.figure(figsize=(8, 72), dpi=100)\n\nsns.countplot(y=df_train.sort_values(by='target_mean', ascending=False)['keyword'],\n              hue=df_train.sort_values(by='target_mean', ascending=False)['target'])\n\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=12)\nplt.legend(loc=1)\nplt.title('Target Distribution in Keywords')\n\nplt.show()","f1f50bbd":"Data_train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\nData_test = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","70a76383":"vocab_file = network_bert.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = network_bert.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","73c264d4":"train_input = encoderbert(Data_train.text.values, tokenizer, maxlength=160)\ntest_input = encoderbert(Data_test.text.values, tokenizer, maxlength=160)\ntrain_labels = Data_train.target.values","29992493":"model = modelConstruct(network_bert, maxlength=160)\nmodel.summary()","a587ddea":"train_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=3,\n    batch_size=16\n)\n\nmodel.save('model.h5')","ab2b776e":"test_pred = model.predict(test_input)","e88ae714":"submission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('submission.csv', index=False)","0df171ca":"* We create a csv for the result ","d75a12b3":"# Bert model using Keras","87fa7e0d":"We creat a function to build our model using our bert layer to apply a model to our data","ff7e5609":"* We use our encoder function on our data","73cb287e":"this function modify our data to use them with our Bert model ","51d02cdc":"* Load the data to apply our model","88cd23a9":"* We build our model using the previous function modelConstruct","8d2f4c4a":"* We use our model on the test data","10911f2f":"# Analysis of our data","8f48dd63":"## References\n\n* All pre-trained BERT models from Tensorflow Hub: https:\/\/tfhub.dev\/s?q=bert","8cb78fb5":"# Tools\n* We use the tokenization script develop by the google team from https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.p \n* We import our package \n* We create some usefull function","fdffe7c7":"* We train our model on three epochs"}}