{"cell_type":{"58568722":"code","df845844":"code","44400e34":"code","9ea99de0":"code","78b8fb93":"code","1fd77d14":"code","46a50214":"code","2dcaa7ec":"code","b9476284":"code","b98e1cca":"code","6623a5e0":"code","6de71faf":"code","5e1ad997":"code","36405df3":"code","565172c1":"code","db3387f7":"code","618916e5":"code","19e33253":"code","166629d6":"code","4fd117e0":"code","5a4f5a5d":"code","1170dd53":"code","500655d5":"code","b0083e93":"code","185c6b68":"code","072f9756":"code","dab20ce9":"code","92f71db2":"code","60454673":"code","26016258":"code","3704cf8f":"code","cb5f319b":"code","e27455c3":"code","04242ef8":"code","4c84edac":"code","eb26c796":"code","9f4b18ea":"code","e91e23b7":"code","0b462de4":"code","dcf7bca5":"code","205dccb1":"code","b9ad42c6":"code","b0a4daec":"code","11fc3f13":"code","90cee040":"markdown","fa11b8b8":"markdown","8288328a":"markdown","d3aa8359":"markdown","958851b2":"markdown","249ed243":"markdown","c097c435":"markdown","0f9f003c":"markdown","3d84822a":"markdown","a2589708":"markdown","2b801f7b":"markdown"},"source":{"58568722":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport re\nimport math\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n        \ntrain = pd.read_csv('\/kaggle\/input\/web-traffic-time-series-forecasting\/train_1.csv.zip')\ntrain.shape","df845844":"train.info()","44400e34":"def get_language(page):\n    res = re.search('[a-z][a-z].wikipedia.org',page)\n    if res:\n        return res[0][0:2]\n    return 'na'\n\ntrain['lang'] = train.Page.map(get_language)\n\nfrom collections import Counter\nprint(Counter(train.lang))","9ea99de0":"lang_sets = {}\nlang_sets['en'] = train[train.lang=='en'].iloc[:,0:-1]\nlang_sets['ja'] = train[train.lang=='ja'].iloc[:,0:-1]\nlang_sets['de'] = train[train.lang=='de'].iloc[:,0:-1]\nlang_sets['na'] = train[train.lang=='na'].iloc[:,0:-1]\nlang_sets['fr'] = train[train.lang=='fr'].iloc[:,0:-1]\nlang_sets['zh'] = train[train.lang=='zh'].iloc[:,0:-1]\nlang_sets['ru'] = train[train.lang=='ru'].iloc[:,0:-1]\nlang_sets['es'] = train[train.lang=='es'].iloc[:,0:-1]\n\nsums = {}\nfor key in lang_sets:\n    sums[key] = lang_sets[key].iloc[:,1:].sum(axis=0) \/ lang_sets[key].shape[0]","78b8fb93":"import matplotlib.pyplot as plt\ndays = [r for r in range(sums['en'].shape[0])]\n\nfig = plt.figure(1,figsize=[10,10])\nplt.ylabel('Views per page')\nplt.xlabel('Day')\nplt.title('Pages in Different Languages')\nlabels = {'en' : 'English','ja' : 'Japanese','de':'German',\n         'na' : 'Media','fr': 'French','zh':'Chinese','ru':'Russian','es':'Spanish'}\n\nfor key in sums:\n    plt.plot(days,sums[key],label = labels[key])\n    \n    \nplt.legend()\nplt.show()","1fd77d14":"from scipy.fftpack import fft\n\ndef plot_with_fft(key):\n    \n    fig = plt.figure(1,figsize=[15,5])\n    plt.ylabel('Views per page')\n    plt.xlabel('Day')\n    plt.title(labels[key])\n    plt.plot(days,sums[key], label = labels[key])\n    \n    \n    fig = plt.figure(2,figsize=[15,5])\n    fft_complex = fft(sums[key].values)\n    fft_mag = [np.sqrt(np.real(x)*np.real(x)+np.imag(x)*np.imag(x)) for x in fft_complex]\n    fft_xvals = [day \/ days[-1] for day in days]\n    npts = len(fft_xvals) \/\/ 2 + 1\n    fft_mag = fft_mag[:npts]\n    fft_xvals = fft_xvals[:npts]\n    \n    plt.ylabel('FFT Magnitude')\n    plt.xlabel(f'Frequency {days[-1]}')\n    plt.title('Fourier transform')\n    plt.plot(fft_xvals[1:],fft_mag[1:],label = labels[key])\n    \n    plt.axvline(x=1.\/7,color='red',alpha = 0.3)\n    plt.axvline(x=2.\/7,color='red',alpha = 0.3)\n    plt.axvline(x=3.\/7,color='red',alpha = 0.3)\n    \n    plt.show()\n    \nfor key in sums:\n    plot_with_fft(key)","46a50214":"def plot_entry(key,idx):\n    data = lang_sets[key].iloc[idx,1:]\n    fig = plt.figure(1,figsize=(10,5))\n    plt.plot(days,data)\n    plt.xlabel('day')\n    plt.ylabel('views')\n    plt.title(train.iloc[lang_sets[key].index[idx],0])\n    \n    plt.show()","2dcaa7ec":"idx = [1,5,10,50,100,250,500,750,1000,1500,2000,3000,4000,5000]\n\nfor i in idx:\n    plot_entry('en',i)","b9476284":"idx = [1,5,10,50,100,250,500,750,1000,1500,2000,3000,4000,5000]\n\nfor i in idx:\n    plot_entry('es',i)","b98e1cca":"npages = 5\ntop_pages = {}\nfor key in lang_sets:\n    print(key)\n    sum_set = pd.DataFrame(lang_sets[key]['Page'])\n    sum_set['total'] = lang_sets[key].sum(axis=1)\n    sum_set = sum_set.sort_values('total',ascending=False)\n    print(sum_set.head(10))\n    top_pages[key] = sum_set.index[0]\n    print('\\n\\n')\n    \n    ","6623a5e0":"for key in top_pages:\n    \n    fig=plt.figure(1,figsize=(10,5))\n    cols=train.columns\n    cols=cols[1:-1]\n    data = train.loc[top_pages[key],cols]\n    plt.plot(days,data)\n    plt.xlabel('Days')\n    plt.ylabel('Views')\n    plt.title(train.loc[top_pages[key],'Page'])\n    plt.show()","6de71faf":"from statsmodels.tsa.stattools import pacf\nfrom statsmodels.tsa.stattools import acf\n\nfor key in top_pages:\n    fig = plt.figure(1,figsize=[10,5])\n    ax1 = fig.add_subplot(121)\n    ax2 = fig.add_subplot(122)\n    cols = train.columns[1:-1]\n    data = np.array(train.loc[top_pages[key],cols])\n    data_diff = [data[i] - data[i-1] for i in range(1,len(data))]\n    autocorr = acf(data_diff)\n    pac = pacf(data_diff)\n    \n    x = [x for x in range(len(pac))]\n    ax1.plot(x[1:],autocorr[1:])\n    \n    ax2.plot(x[1:],pac[1:])\n    ax1.set_xlabel('Lag')\n    ax1.set_ylabel('Autocorrelation')\n    ax1.set_title(train.loc[top_pages[key],'Page'])\n    \n    ax2.set_xlabel('Lag')\n    ax2.set_ylabel('Partial Autocorrelation')\n    plt.show()","5e1ad997":"from statsmodels.tsa.arima_model import ARIMA\nimport warnings\n\ncols = train.columns[1:-1]\nfor key in top_pages:\n    data = np.array(train.loc[top_pages[key],cols],'f')\n    result = None\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore')\n        try:\n            arima = ARIMA(data,[2,1,4])\n            result = arima.fit(disp=False)\n        except:\n            try:\n                arima = ARIMA(data,[2,1,2])\n                result = arima.fit(disp=False)\n                \n            except:\n                print(train.loc[top_pages[key],'Page'])\n                print('\\tARIMA failed')\n    pred = result.predict(2,599,typ = 'levels')\n    x = [i for i in range(600)]\n    \n    i=0\n    \n    \n    plt.plot(x[2:len(data)],data[2:],label='Data')\n    plt.plot(x[2:],pred,label='ARIMA Model')\n    plt.title(train.loc[top_pages[key],'Page'])\n    plt.xlabel('Days')\n    plt.ylabel('Views')\n    plt.legend()\n    plt.show()\n    ","36405df3":"page_details = train.Page.str.extract(r'(?P<topic>.*)\\_(?P<lang>.*).wikipedia.org\\_(?P<access>.*)\\_(?P<type>.*)')\n\npage_details[0:10]","565172c1":"unique_topic = page_details['topic'].unique()\nprint(unique_topic)\nprint('Number of Unique Topics: ',len(unique_topic))","db3387f7":"print(page_details['access'].unique())\nprint(page_details['type'].unique())","618916e5":"fig,axs = plt.subplots(3,1,figsize=(12,12))\n\npage_details['lang'].value_counts().sort_index().plot.bar(ax=axs[0])\naxs[0].set_title('Language - Distribution')\n\npage_details['access'].value_counts().sort_index().plot.bar(ax=axs[1])\naxs[1].set_title('access - Distribution')\n\npage_details['type'].value_counts().sort_index().plot.bar(ax=axs[2])\naxs[2].set_title('type - Distribution')","19e33253":"train_df = pd.concat([page_details,train],axis=1)\n\ndef get_train_validate_set(train_df,test_percent):\n    train_end = math.floor((train_df.shape[1]-5) * (1-test_percent))\n    train_ds = train_df.iloc[:,np.r_[0,1,2,3,4,5:train_end]]\n    test_ds = train_df.iloc[:,np.r_[0,1,2,3,4,train_end:train_df.shape[1]]]\n    \n    return train_ds,test_ds\n\nX_train,y_train = get_train_validate_set(train_df,0.1)\nprint(X_train.head())\nprint(y_train.head())","166629d6":"X_train.head()","4fd117e0":"y_train","5a4f5a5d":"import warnings\nimport scipy\nfrom datetime import timedelta\n\n# Forecasting with Decomposable Model\nfrom pylab import rcParams\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import adfuller\n\n# Fore Machine learning Approach\n\nfrom statsmodels.tsa.tsatools import lagmat\nfrom sklearn.linear_model import LinearRegression, RidgeCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\n\n#Visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('fivethirtyeight')\n\nwarnings.filterwarnings('ignore')\n","1170dd53":"train = pd.read_csv('\/kaggle\/input\/web-traffic-time-series-forecasting\/train_1.csv.zip')\ntrain.head()","500655d5":"train_flattened = pd.melt(train[list(train.columns[-50:])+['Page']],id_vars='Page',var_name='date',value_name='Visits')\ntrain_flattened['date'] = train_flattened['date'].astype('datetime64[ns]')\ntrain_flattened['weekend'] = ((train_flattened.date.dt.dayofweek)\/\/5 ==1).astype(float)\ntrain_flattened","b0083e93":"train_flattened.shape","185c6b68":"df_median = pd.DataFrame(train_flattened.groupby(['Page'])['Visits'].median())\ndf_median.columns = ['median']\n\ndf_mean = pd.DataFrame(train_flattened.groupby('Page')['Visits'].mean())\ndf_mean.columns = ['mean']\n\ntrain_flattened = train_flattened.set_index('Page').join(df_mean).join(df_median)","072f9756":"train_flattened.reset_index(drop=False,inplace=True)\ntrain_flattened['weekday'] = train_flattened['date'].apply(lambda x: x.weekday())\ntrain_flattened['year'] = train_flattened.date.dt.year\ntrain_flattened['month'] = train_flattened.date.dt.month\ntrain_flattened['day'] = train_flattened.date.dt.day\ntrain_flattened","dab20ce9":"plt.figure(figsize=(50,8))\nmean_group = train_flattened[['Page','date','Visits']].groupby(['date'])['Visits'].mean()\nplt.plot(mean_group)\nplt.title('Time Series - Average')\nplt.show()","92f71db2":"plt.figure(figsize=(50,8))\nmean_group = train_flattened[['Page','date','Visits']].groupby(['date'])['Visits'].median()\nplt.plot(mean_group,color = 'r')\nplt.title('Time Series - Meidian')\nplt.show()","60454673":"plt.figure(figsize=(50,8))\nstd_group = train_flattened[['Page','date','Visits']].groupby(['date'])['Visits'].std()\nplt.plot(std_group,color = 'g')\nplt.title('Time Series - STD')\nplt.show()","26016258":"times_series_means = pd.DataFrame(mean_group).reset_index(drop=False)\ntimes_series_means['weekday'] = times_series_means['date'].apply(lambda x:x.weekday())\ntimes_series_means['Date_str'] = times_series_means['date'].apply(lambda x:str(x))\ntimes_series_means[['year','month','day']] = pd.DataFrame(times_series_means['Date_str'].str.split('-',2).tolist(),columns=['year','month','day'])\n\ndate_staging = pd.DataFrame(times_series_means['day'].str.split(' ',2).tolist(),columns=['day','other'])\ntimes_series_means['day'] = date_staging['day']*1\ntimes_series_means.drop('Date_str',axis=1,inplace=True)\ntimes_series_means.head()\n\n","3704cf8f":"times_series_means.reset_index(drop=True,inplace=True)\n\ndef lag_func(data,lag):\n    lag = lag\n    X = lagmat(data['diff'],lag)\n    lagged = data.copy()\n    for c in range(1,lag+1):\n        lagged['lag%d' %c] = X[:,c-1]\n        \n    return lagged\n\n\ndef diff_creation(data):\n    \n    data['diff'] = np.nan\n    data.loc[1:,'diff'] = (data.iloc[1:,1].values - data.iloc[:len(data)-1,1].values)\n    return data\n\ndf_count = diff_creation(times_series_means)\n\nlag = 7\nlagged = lag_func(df_count,lag)\nlast_date = lagged['date'].max()","cb5f319b":"lagged.head()\n","e27455c3":"def train_test(data_lag):\n    xc = ['lag%d' % i for i in range(1,lag+1)] + ['weekday'] +['day']\n    split = 0.70\n    xt = data_lag[(lag+1):][xc]\n    yt = data_lag[(lag+1):]['diff']\n    isplit = int(len(xt) * split)\n    x_train,y_train,x_test,y_test = xt[:isplit],yt[:isplit],xt[isplit:],yt[isplit:]\n    return x_train,y_train,x_test,y_test,xt,yt\n\nx_train,y_train,x_test,y_test,xt,yt = train_test(lagged)","04242ef8":"from sklearn.ensemble import ExtraTreesRegressor,GradientBoostingRegressor, BaggingRegressor,AdaBoostRegressor\n\nfrom sklearn.metrics import mean_absolute_error, r2_score\n\ndef modelisation(x_tr,y_tr,x_ts,y_ts,xt,yt,model0,model1):\n    model0.fit(x_tr,y_tr)\n    \n    prediction = model0.predict(x_ts)\n    r2 = r2_score(y_ts.values,model0.predict(x_ts))\n    mae = mean_absolute_error(y_ts.values,model0.predict(x_ts))\n    print('mae with 70% data',mae)\n    \n    model1.fit(xt,yt)#with all data\n    \n    return model1,prediction,model0\n\nmodel0 = AdaBoostRegressor(n_estimators=5000,random_state=42,learning_rate=0.01)\nmodel1 = AdaBoostRegressor(n_estimators=5000,random_state=42,learning_rate=0.01)\n\nclr,prediction,clr0 = modelisation(x_train,y_train,x_test,y_test,xt,yt,model0,model1)","4c84edac":"def pred_df(data,number_of_days):\n    data_pred = pd.DataFrame(pd.Series(data['date'][data.shape[0]-1] + timedelta(days=1)),columns = ['date'])\n    for i in range(number_of_days):\n        inter = pd.DataFrame(pd.Series(data['date'][data.shape[0]-1] + timedelta(days=i+2)),columns=['date'])\n        \n        date_pred = pd.concat([data_pred,inter]).reset_index(drop=True)\n        \n    return data_pred\n\ndata_to_pred = pred_df(df_count,30)\ndata_to_pred","eb26c796":"def initialisation(data_lag,data_pred,model,xtrain,ytrain,number_of_days):\n    \n    model.fit(xtrain,ytrain)\n    \n    for i in range(number_of_days):\n        lag1 = data_lag.tail(1)['diff'].values[0]\n        lag2 = data_lag.tail(1)['lag1'].values[0]\n        lag3 = data_lag.tail(1)['lag2'].values[0]\n        lag4 = data_lag.tail(1)['lag3'].values[0]\n        lag5 = data_lag.tail(1)['lag4'].values[0]\n        lag6 = data_lag.tail(1)['lag5'].values[0]\n        lag7 = data_lag.tail(1)['lag6'].values[0]\n        lag8 = data_lag.tail(1)['lag7'].values[0]\n        \n        data_pred['weekday'] = data_pred['date'].apply(lambda x:x.weekday)\n        weekday = data_pred['weekday'][0]\n        \n        row = pd.Series([lag1,lag2,lag3,lag4,lag5,lag6,lag7,lag8,weekday],\n                       ['lag1','lag2','lag3','lag4','lag5','lag6','lag7','lag8','weekday'])\n        \n        to_predict = pd.DataFrame(columns=['lag1','lag2','lag3','lag4','lag5','lag6','lag7','lag8','weekday'])\n        \n        prediction = pd.DataFrame(columns=['diff'])\n        to_predict = to_predict.append([row])\n        prediction = pd.DataFrame(model.predict(to_predict),columns=['diff'])\n        \n        if i == 0:\n            last_predict = data_lag['Visits'][data_lag.shape[0]-1] + prediction.values[0][0]\n            \n        if i > 0 :\n            \n            last_predict = data_lag['Visits'][data_lag.shape[0]-1] + prediction.values[0][0]\n            \n        data_lag = pd.concat([data_lag,prediction.join(data_pred['date']).join(to_predict)]).reset_index(drop=True)\n        \n        data_lag['Visits'][data_lag.shape[0]-1] = last_predict\n        \n        #test\n        data_pred = data_pred[data_pred['date']>data_pred['date'][0]].reset_index(drop=True)\n        \n    \n    \n    return data_lag\n\n\nmodel_fin = AdaBoostRegressor(n_estimators=5000,random_state=42,learning_rate=0.01)\n\n\n        \n        ","9f4b18ea":"lagged = initialisation(lagged,data_to_pred,model_fin,xt,yt,30)","e91e23b7":"model_fin = AdaBoostRegressor(n_estimators=5000,random_state=42,learning_rate=0.01)","0b462de4":"model_fin.fit(xt,yt)\n    \nfor i in range(30):\n    lag1 = lagged.tail(1)['diff'].values[0]\n    lag2 = lagged.tail(1)['lag1'].values[0]\n    lag3 = lagged.tail(1)['lag2'].values[0]\n    lag4 = lagged.tail(1)['lag3'].values[0]\n    lag5 = lagged.tail(1)['lag4'].values[0]\n    lag6 = lagged.tail(1)['lag5'].values[0]\n    lag7 = lagged.tail(1)['lag6'].values[0]\n    lag8 = lagged.tail(1)['lag7'].values[0]\n        \n    data_to_pred['weekday'] = data_to_pred['date'].apply(lambda x:x.weekday)\n    weekday = data_to_pred['weekday'][0]\n        ","dcf7bca5":"train.shape","205dccb1":"df_eda = train.iloc[:2000,:50]\ndf_eda.head()","b9ad42c6":"df_eda.shape","b0a4daec":"from dataprep import eda as dp_eda","11fc3f13":"report = dp_eda.create_report(df_eda)\nreport","90cee040":"## Aggregated Data Compared to Popular pages","fa11b8b8":"## Individual Entries","8288328a":"## Split into Train and Validation dataset","d3aa8359":"### Importation and Data Cleaning","958851b2":"## Aggregation and Visualization","249ed243":"## ARIMA Models","c097c435":"### Zoupet Predictive analytics with different approaches","0f9f003c":"## 4 Models","3d84822a":"## ML APPROACH","a2589708":"## Periodic Structure and FFTs","2b801f7b":"## More Analysis Tools"}}