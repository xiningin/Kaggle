{"cell_type":{"db7fed1e":"code","ac6168b5":"code","5e2ec5fd":"code","f8a6c1e8":"code","6146b498":"code","80352b1d":"code","4732ad3e":"code","6b7cd624":"code","38cf62f7":"code","9a0c1ab4":"code","331e4871":"code","219e0e45":"code","1890c888":"code","f712f7db":"code","c86ed5bf":"code","d9e78d41":"code","57dcb2fa":"code","77240dd9":"code","fe9935cd":"code","7f9b2d95":"code","a3b26092":"code","caf348fe":"code","51acafc1":"code","1f320838":"code","3a2b776a":"code","2919cb00":"code","b83ca2ab":"code","7c00ee2e":"code","47123bf6":"code","f7b09719":"code","8879247c":"code","23a57d43":"code","f0aa7771":"code","f9e5eac3":"code","dee4895f":"code","e1e58ddf":"code","dc465a9b":"markdown","fd5086a4":"markdown","59a6063b":"markdown","d7c5a117":"markdown","e7dcb42b":"markdown","92f64d3a":"markdown","d40484a7":"markdown","eb25c3cc":"markdown","f750ee89":"markdown","41dbd663":"markdown","e69da6f4":"markdown","5c988152":"markdown","44e177f8":"markdown","9cfe291a":"markdown","8897a2ea":"markdown","5a703632":"markdown","31c75c56":"markdown","e974725b":"markdown","62b893fb":"markdown","98ad54e3":"markdown","1488d4a1":"markdown","7a78eb03":"markdown","432c7161":"markdown","dc0852a7":"markdown","0218bea9":"markdown","24b28ffc":"markdown","efaf97c3":"markdown","6a4a74fb":"markdown","d17b562d":"markdown","9e072084":"markdown","f8f03b4b":"markdown","a59823a0":"markdown","32233e49":"markdown","073dada9":"markdown","ec26c7e7":"markdown","9b0e3c49":"markdown","03662c97":"markdown","61197fba":"markdown","cee35fcd":"markdown","fa4f0ba3":"markdown","fa7cf734":"markdown","682b0094":"markdown","c4775185":"markdown","1b6dad7e":"markdown","514a0d5d":"markdown","7453658d":"markdown","83639364":"markdown","18495398":"markdown","b3e5aaf1":"markdown","fe7ef77b":"markdown","4b814c3a":"markdown","9ad11e9d":"markdown","85ea9e8b":"markdown","dfd5b633":"markdown","169d9ec3":"markdown","7c28fc66":"markdown"},"source":{"db7fed1e":"model_type = 'distilbert'\nmodel_name = 'distilbert-base-uncased'\nwith_kfold = False\nweight = [0.43, 0.57]\ndataset = 'DATA2'  # or 'DATA1'\nn_splits = 1   # if with_kfold then must be n_splits > 1\nseed = 42\nmodel_args =  {'fp16': False,\n               'train_batch_size': 4,\n               'gradient_accumulation_steps': 2,\n               'do_lower_case': True,\n               'learning_rate': 1e-05,\n               'overwrite_output_dir': True,\n               'manual_seed': seed,\n               'num_train_epochs': 2}","ac6168b5":"!pip install --upgrade transformers\n!pip install simpletransformers","5e2ec5fd":"import os, re, string\nimport random\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\n\nfrom wordcloud import WordCloud, STOPWORDS\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\nimport sklearn\nfrom sklearn.decomposition import PCA, TruncatedSVD\n\nimport torch\n\nfrom simpletransformers.classification import ClassificationModel\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split, KFold\n\nimport warnings\nwarnings.simplefilter('ignore')\n\npd.set_option('max_rows', 100)\npd.set_option('max_colwidth', 2000)","f8a6c1e8":"random.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True","6146b498":"if dataset == 'DATA1':\n    # Original dataset of the competition\n    train_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')[['text', 'target']]\n    test_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')[['text']]\n    \nelif (dataset == 'DATA2') or (dataset == 'DATA2b'):\n    # Cleaned dataset from https:\/\/www.kaggle.com\/vbmokin\/nlp-with-disaster-tweets-cleaning-data\n    train_data = pd.read_csv('..\/input\/nlp-with-disaster-tweets-cleaning-data\/train_data_cleaning.csv')[['text', 'target']]\n    test_data = pd.read_csv('..\/input\/nlp-with-disaster-tweets-cleaning-data\/test_data_cleaning.csv')[['text']]\n\n# Original dataset of the competition\nsample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","80352b1d":"train_data['target'].hist()","4732ad3e":"print(\"Weights which I offer for 0 and 1:\", weight)","6b7cd624":"train_data","38cf62f7":"train_data.info()","9a0c1ab4":"test_data['text']","331e4871":"test_data.info()","219e0e45":"def subtext_repeation_in_df(df, col, subtext, num):\n    # Calc statistics as table for character repetition (1...num times) from subtext list in the df[col]\n    \n    text = \"\".join(df[col])\n    result = pd.DataFrame(columns = ['subtext', 'count'])\n    i = 0\n    if (len(df) > 0) and (len(subtext) > 0):\n        for c in subtext:\n            for j in range(num):\n                cs = c*(j+1)\n                result.loc[i,'count'] = text.count(cs)\n                if c == ' ':\n                    cs = cs.replace(' ','<space>')\n                result.loc[i,'subtext'] = cs                \n                i += 1\n    print('Number of all data is', len(df))\n    result = result[result['count'] > 0].reset_index(drop=True)\n    display(result.sort_values(by='subtext'))\n    \n    print('Text examples')\n    problem_examples = pd.DataFrame(columns = ['problem_examples'])\n    problem_examples['problem_examples'] = ''\n    for i in range(len(result)):\n        problem_examples.loc[i,'problem_examples'] = df[df[col].str.find(result.loc[i,'subtext'])>-1].reset_index(drop=True).loc[0, col]\n    problem_examples = problem_examples.drop_duplicates()\n    display(problem_examples)","1890c888":"# Analysis of punctuation marks repetition in training data\nprint('Statistics for punctuation marks repetition in training data')\nsubtext_repeation_in_df(train_data, 'text', list(string.punctuation), 10)","f712f7db":"# Analysis of punctuation marks repetition in test data\nprint('Statistics for punctuation marks repetition in test data')\nsubtext_repeation_in_df(test_data, 'text', list(string.punctuation), 10)","c86ed5bf":"# Model training without KFold\nif not with_kfold:\n    model = ClassificationModel(model_type, model_name, args=model_args, weight=weight) \n    model.train_model(train_data)\n    result, model_outputs, wrong_predictions = model.eval_model(train_data, acc=sklearn.metrics.accuracy_score)\n    y_preds, _, = model.predict(test_data['text'])\n    pred_train, _ = model.predict(train_data['text'])","d9e78d41":"if not with_kfold:\n    acc = result['acc']\n    print('acc =',acc)","57dcb2fa":"# Model training with KFold\nif with_kfold:\n    kf = KFold(n_splits=n_splits, random_state=seed, shuffle=True)\n\n    results = []\n    wrong_predictions = []\n    y_preds = np.zeros(test_data.shape[0])\n    pred_train = np.zeros(train_data.shape[0])\n    \n    first_fold = True\n    for train_index, val_index in kf.split(train_data):\n        train_df = train_data.iloc[train_index]\n        val_df = train_data.iloc[val_index]\n\n        # Model training\n        model = ClassificationModel(model_type, model_name, args=model_args)\n        model.train_model(train_df)\n\n        # Validation data prediction\n        result, model_outputs_fold, wrong_predictions_fold = model.eval_model(val_df, acc=sklearn.metrics.accuracy_score)\n        pred_train[val_index], _ = model.predict(val_df['text'].reset_index(drop=True))\n        \n        # Save fold results\n        if first_fold:\n            model_outputs = model_outputs_fold\n            first_fold = False\n        else: model_outputs = np.vstack((model_outputs,model_outputs_fold))\n        \n        wrong_predictions += wrong_predictions_fold\n        results.append(result['acc'])\n\n        # Test data prediction\n        y_pred, _ = model.predict(test_data['text'])\n        y_preds += y_pred \/ n_splits","77240dd9":"# Thanks to https:\/\/www.kaggle.com\/szelee\/simpletransformers-hyperparam-tuning-k-fold-cv\n# CV accuracy result output\nif with_kfold:\n    for i, result in enumerate(results, 1):\n        print(f\"Fold-{i}: {result}\")\n    \n    acc = np.mean(results)\n\n    print(f\"{n_splits}-fold CV accuracy result: Mean: {acc} Standard deviation:{np.std(results)}\")","fe9935cd":"y_preds[:] = y_preds[:]>=0.5\ny_preds = y_preds.astype(int)\nnp.mean(y_preds)","7f9b2d95":"# Data prediction and submission\nsample_submission[\"target\"] = y_preds\nsample_submission.to_csv(\"submission.csv\", index=False)\ny_preds[:20]","a3b26092":"# Visualization of model outputs for each rows of training data\ndef plot_data_lavel(data, labels):\n    colors = ['orange','blue']\n    plt.scatter(data[:,0], data[:,1], s=8, alpha=.8, c=labels, cmap=matplotlib.colors.ListedColormap(colors))\n    orange_patch = mpatches.Patch(color='orange', label='Not')\n    blue_patch = mpatches.Patch(color='blue', label='Real')\n    plt.legend(handles=[orange_patch, blue_patch], prop={'size': 30})\n\nfig = plt.figure(figsize=(16, 16))          \nplot_data_lavel(model_outputs, train_data['target'].values)\nplt.show()","caf348fe":"stop_words = list(STOPWORDS) + list('0123456789') + ['rt', 'amp', 'us', 'will', 'via', 'dont', 'cant', 'u', 'work', 'im',\n                               'got', 'back', 'first', 'one', 'two', 'know', 'going', 'time', 'go', 'may', 'youtube', 'say', 'day', 'love', \n                               'still', 'see', 'watch', 'make', 'think', 'even', 'right', 'left', 'take', 'want', 'http', 'https', 'co']","51acafc1":"def plot_word_cloud(x, col, num_common_words, stop_words):\n    # Building the WordCloud for the num_common_words most common data in x[col] without words from list stop_words\n    \n    corpus = \" \".join(x[col].str.lower())\n    corpus = corpus.translate(str.maketrans('', '', string.punctuation))\n    corpus_without_stopwords = [word for word in corpus.split() if word not in stop_words]\n    common_words = Counter(corpus_without_stopwords).most_common(num_common_words)\n    \n    plt.figure(figsize=(12,8))\n    word_cloud = WordCloud(stopwords = stop_words,\n                           background_color='black',\n                           max_font_size = 80\n                           ).generate(\" \".join(corpus_without_stopwords))\n    plt.imshow(word_cloud)\n    plt.axis('off')\n    plt.show()\n    return common_words","1f320838":"# Training data visualization as WordCloud\nprint('Word Cloud for training data without stopwords and apostrophes')\nplot_word_cloud(train_data, 'text', 50, stop_words)","3a2b776a":"# Test data visualization as WordCloud\nprint('Word Cloud for test data without stopwords and apostrophes')\nplot_word_cloud(test_data, 'text', 50, stop_words)","2919cb00":"# Form DataFrame with outliers\noutliers = pd.DataFrame(columns = ['text', 'label'])\nfor i in range(len(wrong_predictions)):\n    outliers.loc[i, 'text'] = wrong_predictions[i].text_a\n    outliers.loc[i, 'label'] = wrong_predictions[i].label","b83ca2ab":"outliers","7c00ee2e":"# Outliers visualization as WordCloud\nprint('Word Cloud for outliers without stopwords and apostrophes in the training data predictions')\noutliers_top50 = plot_word_cloud(outliers, 'text', 50, stop_words)","47123bf6":"outliers_top50","f7b09719":"# Analysis of punctuation marks repetition in outliers\nprint('Statistics for punctuation marks repetition in outliers')\nsubtext_repeation_in_df(outliers, 'text', list(string.punctuation), 10)","8879247c":"# Thanks to https:\/\/www.kaggle.com\/marcovasquez\/basic-nlp-with-tensorflow-and-wordcloud and https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert\n# Showing Confusion Matrix\ndef plot_cm(y_true, y_pred, title, figsize=(5,5)):\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)","23a57d43":"# Showing Confusion Matrix for ST Bert model\nplot_cm(pred_train, train_data['target'].values, 'Confusion matrix for ST Bert model', figsize=(7,7))","f0aa7771":"num_outliers_per_cent = round(len(outliers)*100\/len(test_data), 1)","f9e5eac3":"acc_round = round(acc,3)\nprint('acc =', acc, '=', acc_round)","dee4895f":"if n_splits == 1:\n    n_splits_res = \"\"\nelse: n_splits_res = f\"n_splits = {n_splits}, \"","e1e58ddf":"print(f\"Model - {model_type}, {model_name}\")\nprint(f\"* {dataset} - Commit __ (LB = 0._____): lr = {model_args['learning_rate']}, {n_splits_res}num_epochs = {model_args['num_train_epochs']}, seed = {seed}, acc = {acc_round}, num_outliers = {len(outliers)}({num_outliers_per_cent}%), weight = {weight}\")","dc465a9b":"### 1.2.2.1. distilroberta-base <a class=\"anchor\" id=\"1.2.2.1\"><\/a>\n6-layer, 768-hidden, 12-heads, 82M parameters. The DistilRoBERTa model distilled from the RoBERTa model roberta-base checkpoint.\n\n[Back to Table of Contents](#0.1)","fd5086a4":"* DATA2 - Commit 27 (LB = 0.83328): lr = 1e-05, num_epochs = 2, seed = 100, acc = 0.868, num_outliers = 1002(30.7%)","59a6063b":"* DATA1 - Commit 22 (LB = 0.83818): lr = 1e-05, num_epochs = 2, seed = 100, acc = 0.878, num_outliers = 930(28.5%)\n* DATA1 - Commit 21 (LB = 0.83358): lr = 2e-05, num_epochs = 2, seed = 1, acc = 0.892, num_outliers = 819(25.1%)","d7c5a117":"### I plan to research the possibilities and compare different models with different parameters of Simple Transformers models to solve the issue \"Real or Not? NLP with Disaster Tweets\"","e7dcb42b":"### 1.2.4.3. bert-base-multilingual-cased <a class=\"anchor\" id=\"1.2.4.3\"><\/a>\n12-layer, 768-hidden, 12-heads, 110M parameters. Trained on cased text in the top 104 languages with the largest Wikipedias.\n\n[Back to Table of Contents](#0.1)","92f64d3a":"## 6. Submission<a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)","d40484a7":"[Go to Top](#0)","eb25c3cc":"\"**bert-large-uncased**\" get error (see unsuccessful commits 3, 4, 42): \"OSError: [Errno 28] No space left on device\"","f750ee89":"<a class=\"anchor\" id=\"0.1\"><\/a>\n## Table of Contents\n\n1. [All commits](#1)\n    - [Commit now](#1.1)\n    - [Successful and most interesting commits without KFolds](#1.2)\n        - [DistilBERT](#1.2.1)\n            - [distilbert-base-uncased](#1.2.1.1)\n            - [distilbert-base-cased](#1.2.1.2)\n        - [RoBERTa](#1.2.2)\n            - [distilroberta-base](#1.2.2.1)\n            - [roberta-base](#1.2.2.2)\n        - [ALBERT](#1.2.3)\n            - [albert-base-v1](#1.2.3.1)\n            - [albert-xlarge-v2](#1.2.3.2)\n        - [BERT](#1.2.4)\n            - [bert-base-uncased](#1.2.4.1)\n            - [bert-base-cased](#1.2.4.2)\n            - [bert-base-multilingual-cased](#1.2.4.3)            \n    - [Successful commits with KFolds](#1.3)\n        - [DistilBERT](#1.3.1)\n            - [distilbert-base-uncased](#1.3.1.1)            \n1. [Import libraries](#2)\n1. [Download data](#3)\n1. [EDA](#4)\n1. [Model training and prediction](#5)\n    - [Without KFold](#5.1)\n    - [With KFold](#5.2)\n1. [Submission](#6)\n1. [Visualization of model outputs for all training data](#7)\n1. [Outlier Analysis](#8)\n    - [Word Cloud visualization](#8.1)\n    - [Punctuation marks repetition analysis](#8.2)\n1. [Showing Confusion Matrices](#9)\n1. [Resume](#10)","41dbd663":"Your comments and feedback are most welcome.","e69da6f4":"# Acknowledgements\n\nThis kernel uses such good notebooks and resources: \n* libraries [transformers](https:\/\/huggingface.co\/transformers) and [simpletransformers](https:\/\/github.com\/ThilinaRajapakse\/simpletransformers)\n* dataset [NLP with Disaster Tweets - cleaning data](https:\/\/www.kaggle.com\/vbmokin\/nlp-with-disaster-tweets-cleaning-data)\n* notebook [SimpleTransformers + Hyperparam Tuning + k-fold CV](https:\/\/www.kaggle.com\/szelee\/simpletransformers-hyperparam-tuning-k-fold-cv)\n* notebook [NLP with DT cleaning: Simple Transformers predict](https:\/\/www.kaggle.com\/vbmokin\/nlp-with-dt-cleaning-simple-transformers-predict)\n* notebook [NLP with Disaster Tweets - EDA and Cleaning data](https:\/\/www.kaggle.com\/vbmokin\/nlp-with-disaster-tweets-eda-and-cleaning-data)\n* notebook [NLP - EDA, Bag of Words, TF IDF, GloVe, BERT](https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert)\n* notebook [TSE2020 - RoBERTa (CNN) - Outlier Analysis, 3chr](https:\/\/www.kaggle.com\/vbmokin\/tse2020-roberta-cnn-outlier-analysis-3chr)","5c988152":"### 1.2.4.2. bert-base-cased <a class=\"anchor\" id=\"1.2.4.2\"><\/a>\n12-layer, 768-hidden, 12-heads, 110M parameters. Trained on cased English text.\n\n[Back to Table of Contents](#0.1)","44e177f8":"## 1.2.2. RoBERTa <a class=\"anchor\" id=\"1.2.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","9cfe291a":"## 7. Visualization of model outputs for all training data <a class=\"anchor\" id=\"7\"><\/a>\n\n[Back to Table of Contents](#0.1)","8897a2ea":"## 8.1. Word Cloud visualization <a class=\"anchor\" id=\"8.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","5a703632":"## 3. Download data<a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","31c75c56":"### 1.2.2.2. roberta-base <a class=\"anchor\" id=\"1.2.2.2\"><\/a>\n12-layer, 768-hidden, 12-heads, 125M parameters. RoBERTa using the BERT-base architecture\n\n[Back to Table of Contents](#0.1)","e974725b":"### 1.2.3.1. albert-base-v1 <a class=\"anchor\" id=\"1.2.3.1\"><\/a>\n12 repeating layers, 128 embedding, 768-hidden, 12-heads, 11M parameters. ALBERT base model\n\n[Back to Table of Contents](#0.1)","62b893fb":"## 9. Showing Confusion Matrices<a class=\"anchor\" id=\"9\"><\/a>\n\n[Back to Table of Contents](#0.1)","98ad54e3":"This chapter uses notebook [TSE2020 - RoBERTa (CNN) - Outlier Analysis, 3chr](https:\/\/www.kaggle.com\/vbmokin\/tse2020-roberta-cnn-outlier-analysis-3chr)","1488d4a1":"* DATA2 - Commit 36 (LB = 0.84063): lr = 9e-06, num_epochs = 2, seed = 100, acc = 0.898, num_outliers = 780(23.9%)\n* DATA2 - Commit 34 (LB = 0.84033): lr = 1e-05, num_epochs = 2, seed = 100, acc = 0.9, num_outliers = 763(23.4%)\n* DATA2 - Commit 37 (LB = 0.83818): lr = 2e-05, num_epochs = 2, seed = 100, acc = 0.921, num_outliers = 601(18.4%)\n* DATA2 - Commit 38 (LB = 0.83634): lr = 8e-06, num_epochs = 2, seed = 100, acc = 0.893, num_outliers = 812(24.9%)\n* DATA1 - Commit 44 (LB = 0.83512): lr = 9e-06, num_epochs = 2, seed = 100, acc = 0.898, num_outliers = 775(23.8%)\n* DATA1 - Commit 43 (LB = 0.83144): lr = 9e-06, num_epochs = 1, seed = 100, acc = 0.869, num_outliers = 999(30.6%)","7a78eb03":"## 5. Model training and prediction<a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","432c7161":"## 8. Outlier Analysis<a class=\"anchor\" id=\"8\"><\/a>\n\n[Back to Table of Contents](#0.1)","dc0852a7":"## 1.1. Commit now <a class=\"anchor\" id=\"1.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","0218bea9":"## 1.2.1. DistilBERT <a class=\"anchor\" id=\"1.2.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","24b28ffc":"I hope you find this kernel useful and enjoyable.","efaf97c3":"## 1.2. Successful and most interesting commits without KFolds <a class=\"anchor\" id=\"1.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","6a4a74fb":"## 1. All commits<a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","d17b562d":"* DATA2 - Commit 28 (LB = 0.82194): lr = 1e-05, num_epochs = 2, seed = 100, acc = 0.883, num_outliers = 887(27.2%)","9e072084":"### 1.2.3.2. albert-xlarge-v2 <a class=\"anchor\" id=\"1.2.3.2\"><\/a>\n24 repeating layers, 128 embedding, 2048-hidden, 16-heads, 58M parameters. ALBERT xlarge model with no dropout, additional training data and longer training\n\n[Back to Table of Contents](#0.1)","f8f03b4b":"* DATA2 - Commit 33 (LB = 0.83910): lr = 5e-05, n_splits = 5, num_epochs = 3, seed = 1, acc = 0.828, num_outliers = 1313(40.2%)\n* DATA2 - Commit 59 (LB = 0.83879): lr = 1e-05, n_splits = 10, num_epochs = 2, seed = 100, acc = 0.84, num_outliers = 1217(37.3%), weight = [0.4, 0.6]\n* DATA1 - Commit 13 (LB = 0.83450): lr = 9e-06, n_splits = 5, num_epochs = 2, seed = 100, acc = 0.837, num_outliers = 1240(38.0%)","a59823a0":"## 10. Resume<a class=\"anchor\" id=\"10\"><\/a>\n\n[Back to Table of Contents](#0.1)","32233e49":"## 1.3.1. DistilBERT <a class=\"anchor\" id=\"1.3.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","073dada9":"* DATA2 - Commit 26: lr = 1e-05, num_epochs = 2, seed = 100, acc = 0.572, num_outliers = 3259(99.9%)\n\nI don't send this submission file. It's not a good solution.","ec26c7e7":"* DATA2 - Commit 39 (LB = 0.83573): lr = 9e-06, num_epochs = 2, seed = 100, acc = 0.896, num_outliers = 794(24.3%)","9b0e3c49":"## 1.2.4. BERT <a class=\"anchor\" id=\"1.2.4\"><\/a>\n\n[Back to Table of Contents](#0.1)","03662c97":"## 1.3. Successful commits with KFolds <a class=\"anchor\" id=\"1.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","61197fba":"## 1.2.1.1. distilbert-base-uncased <a class=\"anchor\" id=\"1.2.1.1\"><\/a>\n12-layer, 768-hidden, 12-heads, 110M parameters. Trained on lower-cased English text.\n\n[Back to Table of Contents](#0.1)","cee35fcd":"# [Real or Not? NLP with Disaster Tweets](https:\/\/www.kaggle.com\/c\/nlp-getting-started)","fa4f0ba3":"* DATA1 - Commit 60 (LB = 0.82807): lr = 1e-05, num_epochs = 2, seed = 100, acc = 0.885, num_outliers = 878(26.9%), weight = [0.5, 0.5]","fa7cf734":"## 1.2.3. ALBERT <a class=\"anchor\" id=\"1.2.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","682b0094":"## 1.3.1.1. distilbert-base-uncased <a class=\"anchor\" id=\"1.3.1.1\"><\/a>\n12-layer, 768-hidden, 12-heads, 110M parameters. Trained on lower-cased English text.\n\n[Back to Table of Contents](#0.1)","c4775185":"## 5.2. With KFold<a class=\"anchor\" id=\"5.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","1b6dad7e":"I plan to study of **each of Simple Transformers models** with different parameters without K-fold since cross-validation complicates the analysis of the model:\n\n* visualize **embeddings** (I use the function from my notebook in this competition with about 500 forks: [NLP - EDA, Bag of Words, TF IDF, GloVe, BERT](https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert));\n* study **outliers** (forecast errors) (I use the functions of my notebook with more than 700 forks: [TSE2020 - RoBERTa (CNN) - Outlier Analysis, 3chr](https:\/\/www.kaggle.com\/vbmokin\/tse2020-roberta-cnn-outlier-analysis-3chr));\n* build a **confusion matrix** (from the same notebook [NLP - EDA, Bag of Words, TF IDF, GloVe, BERT](https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert));\n\nI commit the result of each prediction and save (parameters and LB) it in a section with successful commits.\n\nThis notebook use my public dataset with cleaning data for this competition [NLP with Disaster Tweets - cleaning data](https:\/\/www.kaggle.com\/vbmokin\/nlp-with-disaster-tweets-cleaning-data):\n* `train_data_cleaning.csv`\n* `test_data_cleaning.csv`\n\nto speed up and increase the accuracy of calculations.\n\nSee all models in list of **transformers** library: https:\/\/huggingface.co\/transformers\/pretrained_models.html","514a0d5d":"* DATA1 - dataset 1 (commits 1-23, 43,...) - original dataset of the competition \n* DATA2 - dataset 2 (commits 24-42, 44,...) - cleaned dataset from [NLP with Disaster Tweets - cleaning data](https:\/\/www.kaggle.com\/vbmokin\/nlp-with-disaster-tweets-cleaning-data)","7453658d":"## 5.1. Without KFold<a class=\"anchor\" id=\"5.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","83639364":"## 1.2.1.2. distilbert-base-cased <a class=\"anchor\" id=\"1.2.1.2\"><\/a>\n6-layer, 768-hidden, 12-heads, 65M parameters. The DistilBERT model distilled from the BERT model bert-base-cased checkpoint\n\n[Back to Table of Contents](#0.1)","18495398":"* DATA2 - Commit 67 (LB = 0.84278): lr = 1e-05, num_epochs = 2, seed = 100, acc = 0.887, num_outliers = 864(26.5%), weight = [0.44, 0.56]\n* DATA2 - Commit 62 (LB = 0.84186): lr = 1e-05, num_epochs = 2, seed = 100, acc = 0.886, num_outliers = 865(26.5%), weight = [0.45, 0.55]\n* DATA2 - Commit 56 (LB = 0.84155): lr = 1e-05, num_epochs = 2, seed = 100, acc = 0.887, num_outliers = 857(26.3%), weight = [0.4, 0.6]\n* DATA1 - Commit 6 (LB = 0.84125): lr = 1e-05, num_epochs = 2, acc = 0.888, num_outliers = 852(26.1%)\n* DATA2 - Commit 55 (LB = 0.84125): lr = 9e-06, num_epochs = 2, seed = 100, acc = 0.884, num_outliers = 884(27.1%), weight = [0.4, 0.6]\n* DATA2 - Commit 32 (LB = 0.84125): lr = 9e-06, num_epochs = 2, seed = 100, acc = 0.883, num_outliers = 893(27.4%)\n* DATA2 - Commit 49 (LB = 0.84033): lr = 1e-05, num_epochs = 2, seed = 100, acc = 0.886, num_outliers = 871(26.7%)\n* DATA2 - Commit 25 (LB = 0.84002): lr = 1e-05, seed = 100, acc = 0.885, num_outliers = 873(26.8%)\n* DATA2 - Commit 54 (LB = 0.83971): lr = 9e-06, num_epochs = 2, seed = 100, acc = 0.882, num_outliers = 895(27.4%), weigh = [0.6, 0.4]\n* DATA2 - Commit 57 (LB = 0.83971): lr = 1e-05, num_epochs = 2, seed = 100, acc = 0.887, num_outliers = 862(26.4%), weight = [0.3, 0.7]\n* DATA2 - Commit 58 (LB = 0.83879): lr = 1.5e-05, num_epochs = 2, seed = 100, acc = 0.9, num_outliers = 763(23.4%), weight = [0.4, 0.6]\n* DATA2 - Commit 45 (LB = 0.83849): lr = 1e-05, num_epochs = 2, seed = 42, acc = 0.887, num_outliers = 857(26.3%)\n* DATA1 - Commit 9 (LB = 0.83726): lr = 2e-05, acc = 0.905, num_outliers = 720(22.1%)\n* DATA2 - Commit 29 (LB = 0.83604): lr = 3e-05, acc = 0.866, num_outliers = 1018(31.2%)\n* DATA1 - Commit 5 (LB = 0.83389): lr = 1e-05, seed = 100, acc = 0.915, num_outliers = 649(19.9%)\n* DATA2 - Commit 30 (LB = 0.83174): lr = 4e-05, num_epochs = 2, seed = 100, acc = 0.917, num_outliers = 632(19.4%)","b3e5aaf1":"* DATA2 - Commit 51 (LB = 0.83512): lr = 1e-05, num_epochs = 2, seed = 100, acc = 0.885, num_outliers = 873(26.8%)\n* DATA1 - Commit 50 (LB = 0.83021): lr = 1e-05, num_epochs = 2, seed = 100, acc = 0.886, num_outliers = 868(26.6%)\n* DATA1 - Commit 52 (LB = 0.82899): lr = 9e-06, num_epochs = 2, seed = 100, acc = 0.887, num_outliers = 862(26.4%)","fe7ef77b":"## 4. EDA<a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","4b814c3a":"### See my posts about this issue \"[Cleaning dataset for this competition](https:\/\/www.kaggle.com\/c\/nlp-getting-started\/discussion\/166426)\"","9ad11e9d":"## 8.2. Analysis of punctuation marks repetition in text <a class=\"anchor\" id=\"8.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","85ea9e8b":"## 2. Import libraries<a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","dfd5b633":"### 1.2.4.1. bert-base-uncased <a class=\"anchor\" id=\"1.2.4.1\"><\/a>\n12-layer, 768-hidden, 12-heads, 110M parameters. Trained on lower-cased English text.\n\n[Back to Table of Contents](#0.1)","169d9ec3":"Thanks to [NLP with Disaster Tweets - EDA and Cleaning data](https:\/\/www.kaggle.com\/vbmokin\/nlp-with-disaster-tweets-eda-and-cleaning-data)","7c28fc66":"### See my posts about this issue \"[Punctuation marks repetition in incorrectly classified text](https:\/\/www.kaggle.com\/c\/nlp-getting-started\/discussion\/166248)\""}}