{"cell_type":{"5c503d00":"code","e55f1338":"code","6fdfb97d":"code","fc0114a0":"code","91c6fd2f":"code","436971c0":"code","bb720086":"code","4247fa32":"code","77f45342":"code","1dab65a2":"code","6611ed6c":"code","c683f8e8":"code","3f1c3488":"code","dae2f7fa":"code","adf16348":"code","f787c50c":"code","7f3d8e12":"code","07379d04":"code","3e738111":"code","a22b50d1":"code","28a73a08":"code","d1fd0aa4":"code","add5b5b9":"code","e079c3f8":"code","f0fc4fc0":"code","47ffddb4":"code","0b7b6bc0":"code","82c4f510":"code","3b894c13":"code","25f706b4":"code","c02f1c75":"code","b64c176b":"code","44ecc734":"code","955dbb62":"code","299b8752":"code","8047426a":"code","d93f8947":"code","5a1cfa04":"code","3ed97077":"code","7d127952":"code","8e1b6747":"code","4c368703":"code","90960783":"code","6b2fbc46":"code","3e620967":"code","ffa06992":"code","adb5bac0":"code","8f40c461":"code","32ebece8":"code","d8948566":"code","6f08b5fc":"code","33b11666":"code","eb69bb0b":"code","290ef180":"code","0cf27447":"code","a5a8aed5":"code","55cb4bb5":"code","9d4ab9e9":"code","b8cec2d0":"code","0d365a17":"code","4c5852c8":"code","d5b8219e":"code","ae863d7b":"code","45211e1b":"code","d3bd64ba":"code","48ea3e1e":"code","4491b6ca":"code","b697c36b":"code","bb101ee9":"code","4f9b39a6":"code","bbca9546":"code","5eab784d":"code","18a6bb04":"code","fa4d903c":"code","2cc28ba6":"code","078df95d":"code","315bf765":"code","4788c9ad":"code","84bb2e52":"code","667e7f90":"code","f8b8a38c":"code","ced14337":"code","4e48d0d3":"code","cc4b143f":"code","cc5f1e84":"code","a6f3f4d9":"code","c9984367":"code","98173170":"code","57219b1c":"code","e4516741":"code","75188bd4":"code","6b19487b":"code","8dfda96b":"code","10454927":"code","9e797bf0":"code","20707395":"code","659bb910":"code","645c9eec":"code","344193fd":"code","8101a909":"code","1dec39ba":"code","3e437e1b":"code","1121c46c":"code","9da59315":"code","b8901286":"code","e5f85a74":"code","e34065f2":"code","c45004df":"code","61d2bc02":"code","01dec8ab":"code","addabd05":"code","3238631d":"code","582f79e4":"code","e9126575":"code","197f6d53":"code","5044e728":"code","1a6234c7":"code","fa2d0666":"code","efc622c1":"code","2335c54f":"code","e27cb2dc":"code","a6f0be16":"code","fd3e875f":"code","b8dcf086":"code","4efc6b69":"code","80e1fb9d":"code","d84503c4":"code","8deeff59":"code","1f133329":"code","cf4f4157":"code","1b805e38":"code","e637a663":"code","d534e1aa":"code","e1e1366e":"code","b29c57cc":"markdown","cfa9f755":"markdown","e21dc79e":"markdown","32947168":"markdown","ad3cc18f":"markdown","503a77ea":"markdown","8fbef84a":"markdown","29031063":"markdown","03154c79":"markdown","9c5ff794":"markdown","2550c5a9":"markdown","02acbc13":"markdown","9d89f8d1":"markdown","844adfbc":"markdown","3f50bde5":"markdown","4bf5f5cb":"markdown","c400f654":"markdown","a6301ce3":"markdown","14f1b961":"markdown","d29ae45d":"markdown","0eab3b10":"markdown","3aa38e7c":"markdown","12651e70":"markdown","e3ae1df4":"markdown","98a2939c":"markdown","0ca1edac":"markdown","a8f7cf19":"markdown","ba5a0fda":"markdown","64d3c8e7":"markdown","a119134f":"markdown","f9242c2f":"markdown","075e5db3":"markdown","f0763971":"markdown","45f373fa":"markdown","e83a00ea":"markdown","cd51fcb8":"markdown","42f366d2":"markdown","7a15ed20":"markdown","34130fa0":"markdown","2ebbcb05":"markdown","908c50b2":"markdown","3d18a91b":"markdown","191ca760":"markdown","266a4e7f":"markdown","9625da84":"markdown","753f05bf":"markdown","5510b132":"markdown","63588677":"markdown","5c9e3600":"markdown","175fc670":"markdown","6cef190b":"markdown","3141fbb9":"markdown","b6477178":"markdown","de3355d5":"markdown","4ffd6eee":"markdown","8fc8e449":"markdown","a4f4a874":"markdown","44d453cd":"markdown","9029f10b":"markdown","0c8e74d4":"markdown","a4c67fbe":"markdown","f8dcf2d3":"markdown","82000160":"markdown","ef4cff81":"markdown","cf2cb3bb":"markdown","2fa02e6c":"markdown","723beded":"markdown","456af4fb":"markdown","7e1c759b":"markdown","3e0bd514":"markdown","ff442f25":"markdown","51de04bf":"markdown","1635ab0f":"markdown","634957f3":"markdown"},"source":{"5c503d00":"# !pip install pyforest\n# !pip install ipython\n# !pip install pyclustertend\n# !pip install xlrd\n# !pip install Autoviz\n# !pip install colorama\n# !pip3 install termcolor\n# !pip install termcolor\n# !pip install pandas-profiling","e55f1338":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n# %matplotlib notebook\nplt.rcParams[\"figure.figsize\"] = (12,6)\n# plt.rcParams['figure.dpi'] = 100\nsns.set_style(\"whitegrid\")\n#pd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.options.display.float_format = '{:,.2f}'.format\n\n# Importing plotly and cufflinks in offline mode\nimport plotly.express as px\nimport cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\n# Ignore Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.warn(\"this will not show\")\n\n# Figure & Display options\nplt.rcParams[\"figure.figsize\"] = (10, 6)\npd.set_option('max_colwidth',200)\npd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 200)\nimport matplotlib.colors as mcolors\n#pd.set_option('display.float_format', lambda x: '%.3f' % x)\n\nfrom termcolor import colored\nimport missingno as msno \n\nimport colorama\nfrom colorama import Fore, Style  # maakes strings colored\nfrom termcolor import colored, cprint\n\n# Import Pandas Profiling\nimport pandas_profiling","6fdfb97d":"###############################################################################\n\ndef missing(df):\n    missing_number = df.isnull().sum().sort_values(ascending=False)\n    missing_percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])\n    return missing_values\n\ndef missing_values(df):\n    return missing(df)[missing(df)['Missing_Number']>0]\n\n###############################################################################\n\ndef first_look(col):\n    print(\"column name    : \", col)\n    print(\"--------------------------------\")\n    print(\"per_of_nulls   : \", \"%\", round(df[col].isnull().sum()\/df.shape[0]*100, 2))\n    print(\"num_of_nulls   : \", df[col].isnull().sum())\n    print(\"num_of_uniques : \", df[col].nunique())\n    print(df[col].value_counts(dropna = False))\n\ndef first_looking(df):\n    print(colored(\"Shape:\", attrs=['bold']), df.shape,'\\n',\n          f\"There is \", df.shape[0], \" observation and \", df.shape[1], \" columns in the dataset.\", '\\n',\n          colored('-'*79, 'red', attrs=['bold']),\n          colored(\"\\nInfo:\\n\", attrs=['bold']), sep='')\n    print(df.info(), '\\n', \n          colored('-'*79, 'red', attrs=['bold']), sep='')\n    print(colored(\"Number of Uniques:\\n\", attrs=['bold']), df.nunique(),'\\n',\n          colored('-'*79, 'red', attrs=['bold']), sep='')\n    print(colored(\"Missing Values:\\n\", attrs=['bold']), missing_values(df),'\\n', \n          colored('-'*79, 'red', attrs=['bold']), sep='')\n    print(colored(\"All Columns:\", attrs=['bold']), list(df.columns),'\\n', \n          colored('-'*79, 'red', attrs=['bold']), sep='')\n\n    df.columns= df.columns.str.lower().str.replace('&', '_').str.replace(' ', '_')\n\n    print(colored(\"Columns after rename:\", attrs=['bold']), list(df.columns),'\\n',\n              colored('-'*79, 'red', attrs=['bold']), sep='')\n\ndef duplicate_values(df):\n    duplicate_values = df.duplicated(subset=None, keep='first').sum()\n    if duplicate_values > 0:\n        df.drop_duplicates(keep='first', inplace=True)\n        print(duplicate_values, colored(\"duplicates were dropped\", attrs=['bold']),'\\n',\n              colored('-'*79, 'red', attrs=['bold']), sep='')\n    else:\n        print(colored(\"No duplicates\", attrs=['bold']),'\\n',\n              colored('-'*79, 'red', attrs=['bold']), sep='')\n        \ndef drop_columns(df, drop_columns):\n    if drop_columns !=[]:\n        df.drop(drop_columns, axis=1, inplace=True)\n        print(drop_columns, 'were dropped')\n    else:\n        print(colored('We will now check the missing values and if necessary drop some columns!!!', attrs=['bold']),'\\n',\n              colored('-'*79, 'red', attrs=['bold']), sep='')\n        \ndef drop_null(df, limit):\n    print('Shape:', df.shape)\n    for i in df.isnull().sum().index:\n        if (df.isnull().sum()[i]\/df.shape[0]*100)>limit:\n            print(df.isnull().sum()[i], 'percent of', i ,'null and were dropped')\n            df.drop(i, axis=1, inplace=True)\n            print('new shape:', df.shape)\n        else:\n            print(df.isnull().sum()[i], '%, percentage of missing values of', i ,'less than limit', limit, '%, so we will keep it.')\n    print('New shape after missing value control:', df.shape)\n\n###############################################################################","fc0114a0":"df0 = pd.read_csv(r\"..\/input\/churn-modelling\/Churn_Modelling.csv\")\ndf = df0.copy()","91c6fd2f":"df","436971c0":"df.profile_report()","bb720086":"first_looking(df)","4247fa32":"df.head(3)","77f45342":"df.tail(3)","1dab65a2":"df.sample(10)","6611ed6c":"df.columns","c683f8e8":"print(\"There is\", df.shape[0], \"observation and\", df.shape[1], \"columns in the dataset\")","3f1c3488":"df.shape","dae2f7fa":"df.info()","adf16348":"df= df.apply(lambda x: x.astype(float) if x.dtype==\"int64\" else x)","f787c50c":"df.info()","7f3d8e12":"df.head(3)","07379d04":"df.drop([\"rownumber\", \"customerid\", \"surname\"], axis=1, inplace=True)","3e738111":"df.head(3)","a22b50d1":"pd.options.display.float_format = '{:}'.format","28a73a08":"df.head(3)","d1fd0aa4":"df.describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='RdPu').format('{:.2f}')","add5b5b9":"df.describe(include=object).T","e079c3f8":"df.nunique()","f0fc4fc0":"# to find how many unique values numerical features have\n\nfor col in df.select_dtypes(include=[np.number]).columns:\n  print(colored(f\"{col}\", 'green', attrs=['bold']), f\"feature has\", colored(f\"{df[col].nunique()}\", 'green', attrs=['bold']), f\"unique values.\")","47ffddb4":"# to find how many unique values object features have\n\nfor col in df.select_dtypes(include=\"object\").columns:\n  print(colored(f\"{col}\", 'green', attrs=['bold']), f\"feature has\", colored(f\"{df[col].nunique()}\", 'green', attrs=['bold']), f\"unique values.\")","0b7b6bc0":"df.duplicated().value_counts()","82c4f510":"df.shape","3b894c13":"missing(df)","25f706b4":"df.isnull().melt(value_name=\"missing\")","c02f1c75":"plt.figure(figsize=(4, 6))\n\nsns.displot(data=df.isnull().melt(value_name=\"missing\"),\n            y=\"variable\",\n            hue=\"missing\",\n            multiple=\"fill\",\n            height=9.25)\n\nplt.axvline(0.3, color=\"r\");","b64c176b":"# Let's take a quick look at Target variable\n\nfirst_look('exited')","44ecc734":"y = df['exited']\n\nprint(f'Percentage of \\033[1m\"being churn\"\\033[0m: % {round(y.value_counts(normalize=True)[1]*100,2)} --> \\\n({y.value_counts()[1]} churn cases out of {len(df)})\\nPercentage of \\033[1m\"NOT being churn\"\\033[0m: % {round(y.value_counts(normalize=True)[0]*100,2)} --> ({y.value_counts()[0]} NOT churn cases out of {len(df)})')\n\nexplode = [0, 0.1]\n\ndf[\"exited\"].value_counts().plot(kind=\"pie\", \n                                 autopct='%1.1f%%', \n                                 figsize=(7, 7), \n                                 explode=explode, \n                                 wedgeprops={'edgecolor': 'black'}, \n                                 shadow=True, \n                                 colors = ['darkorchid', 'turquoise'], \n                                 startangle=140)\nplt.title('Churn Distribution');","955dbb62":"plt.figure(figsize=(4, 2))\ndf['exited'].iplot(kind='hist')","299b8752":"df[df['exited']==0].describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='RdPu').format('{:.2f}')","8047426a":"df[df['exited']==1].describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='RdPu').format('{:.2f}')","d93f8947":"print( f\"Skewness: {df['exited'].skew()}\")","5a1cfa04":"print( f\"Kurtosis: {df['exited'].kurtosis()}\")","3ed97077":"df_skew_temp = df.skew()\ndf_skew_temp = pd.DataFrame(df_skew_temp, columns=[\"skewness_value\"])\ndf_skew_temp","7d127952":"symetric_features = []\nmoderate_skewed_features = []\nhighly_skewed_features = []\n\nfor col, skew in df_skew_temp.iterrows():\n        if -0.5 < skew[0] < 0.5:\n            symetric_features.append(col)\n            print(f\"The skewness value of\", colored(f\"{skew[0]}\", 'green', attrs=['bold']), \"for\", colored(f\"{col}\", 'green', attrs=['bold']), \"feature means that the distribution is approx.\", colored(f\"symmetric\", 'green', attrs=['bold']))\n        elif (-0.5 < skew[0] < -1.0) or (0.5 < skew[0] < 1.0):  \n            moderate_skewed_features.append(col)\n            print(f\"The skewness value of\", colored(f\"{skew[0]}\", 'yellow', attrs=['bold']), \"for\", colored(f\"{col}\", 'yellow', attrs=['bold']), \"feature means that the distribution is approx.\", colored(f\"moderately symmetric\", 'yellow', attrs=['bold']))\n        else:\n            highly_skewed_features.append(col)\n            print(f\"The skewness value of\", colored(f\"{skew[0]}\", 'red', attrs=['bold']), \"for\", colored(f\"{col}\", 'red', attrs=['bold']), \"feature means that the distribution is approx.\", colored(f\"highly symmetric\", 'red', attrs=['bold']))\n\nprint(colored('*'*120, 'cyan', attrs=['bold']))\nprint(\"\\033[1mThe number of symetric features:\\033[0m\", len(symetric_features))\nprint(\"\\033[1mThe number of moderately skewed features:\\033[0m\", len(moderate_skewed_features)) \nprint(\"\\033[1mThe number of highly skewed features:\\033[0m\", len(highly_skewed_features)) ","8e1b6747":"#Calculating Kurtosis \n\nkurtosis_limit = 2 \nkurtosis_vals = df.kurtosis()\nkurtosis_cols = kurtosis_vals[abs(kurtosis_vals) > kurtosis_limit].sort_values(ascending=False)\nkurtosis_cols","4c368703":"plt.figure(figsize=(14, 10))\n\n# Getting the Upper Triangle of the co-relation matrix\nmatrix = np.triu(df.corr())\n\n# using the upper triangle matrix as mask \nsns.heatmap(df.corr(), annot=True, cmap = sns.cubehelix_palette(8), mask=matrix)\n\nplt.xticks(rotation=45);","90960783":"df_temp = df.corr()\n\nfeature =[]\ncollinear=[]\n\nfor col in df_temp.columns:\n    for i in df_temp.index:\n        if (df_temp[col][i]> .85 and df_temp[col][i] < 1) or (df_temp[col][i]< -.85 and df_temp[col][i] > -1) :\n                feature.append(col)\n                collinear.append(i)\n                print(Fore.RED + f\"\\033[1mmulticolinearity alert\\033[0m between {col} - {i}\")\n        else:\n            print(f\"For {col} and {i}, there is \\033[1mNO multicollinearity problem\\033[0m\") \n\nunique_list = list(set(feature+collinear))\n\nprint(colored('*'*80, 'cyan', attrs=['bold']))\nprint(\"\\033[1mThe total number of strong corelated features:\\033[0m\", len(unique_list)) ","6b2fbc46":"plt.figure(figsize = (8, 7))\ndf.corr()['exited'].sort_values().drop(\"exited\").plot(kind = \"barh\");","3e620967":"sns.scatterplot(data=df, x=\"age\", y=\"balance\", hue=\"exited\");","ffa06992":"df.dtypes","adb5bac0":"numerical= df.drop(['exited'], axis=1).select_dtypes('number').columns\n\ncategorical = df.select_dtypes('object').columns\n\nprint(colored(\"Numerical Columns:\", attrs=['bold']), list(df[numerical].columns),'\\n',\n              colored('-'*124, 'red', attrs=['bold']), sep='')\nprint(colored(\"Categorical Columns:\", attrs=['bold']), list(df[categorical].columns),'\\n',\n              colored('-'*124, 'red', attrs=['bold']), sep='')","8f40c461":"df[numerical].head().T","32ebece8":"df[numerical].describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='RdPu').format('{:.2f}')","d8948566":"df[numerical].iplot(kind='hist');","6f08b5fc":"df[numerical].iplot(kind='histogram', subplots=True, bins=50)","33b11666":"df[\"hascrcard\"].value_counts()","eb69bb0b":"df[\"isactivemember\"].value_counts()","290ef180":"for i in df[numerical].drop([\"hascrcard\", \"isactivemember\"], axis=1):\n    df[i].iplot(kind=\"box\", title=i, boxpoints=\"all\", color='lightseagreen')","0cf27447":"index = 0\nplt.figure(figsize=(20,20))\nfor feature in numerical:\n    if feature != \"exited\":\n        index += 1\n        plt.subplot(4, 3, index)\n        sns.boxplot(x='exited', y=feature, data=df)","a5a8aed5":"fig = px.scatter_3d(df, \n                    x='age',\n                    y='estimatedsalary',\n                    z='creditscore',\n                    color='exited')\nfig.show();","55cb4bb5":"sns.pairplot(df, hue=\"exited\", palette=\"inferno\", corner=True);","9d4ab9e9":"df[categorical].head().T","b8cec2d0":"df[categorical].describe().T.style.background_gradient(subset=['unique','freq','count'], cmap='RdPu')","0d365a17":"df[categorical].nunique()","4c5852c8":"for i, col in enumerate(df[categorical].columns):\n    fig = px.histogram(df[col], color=df[\"exited\"], width=800, height=800, title=col, pattern_shape=df[\"hascrcard\"], pattern_shape_sequence=[\"x\", \"+\"])\n    fig.show()","d5b8219e":"for i, col in enumerate(df[categorical].columns):\n    xtab = pd.crosstab(df[col], df[\"exited\"], normalize=True)\n    print(colored('-'*50, 'red', attrs=['bold']), sep='')\n    print(xtab*100)","ae863d7b":"sns.swarmplot(y=\"age\", x=\"geography\", hue=\"exited\", data=df);","45211e1b":"sns.swarmplot(y=\"age\", x=\"gender\", hue=\"exited\", data=df);","d3bd64ba":"sorted_creditscore = df[\"creditscore\"].sort_values()","48ea3e1e":"fig = px.histogram(df, x=df.gender, animation_frame=sorted_creditscore, facet_col=\"exited\")\nfig.show()","4491b6ca":"df.head(3)","b697c36b":"categorical","bb101ee9":"df = pd.get_dummies(df, columns=['geography', 'gender'], drop_first=True)","4f9b39a6":"df.head(3)","bbca9546":"plt.figure(figsize=(14, 10))\n\n# Getting the Upper Triangle of the co-relation matrix\nmatrix = np.triu(df.corr())\n\n# using the upper triangle matrix as mask \nsns.heatmap(df.corr(), annot=True, cmap = sns.cubehelix_palette(8), mask=matrix)\n\nplt.xticks(rotation=45);","5eab784d":"df.describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='RdPu').format('{:.2f}')","18a6bb04":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler","fa4d903c":"df.insert(12, 'churn', df['exited'])","2cc28ba6":"df.drop('exited', axis=1, inplace=True)","078df95d":"df.head(3)","315bf765":"X = df.drop('churn', axis=1)\ny = df['churn'].values","4788c9ad":"X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.10, random_state = 42)","84bb2e52":"scaler = MinMaxScaler()","667e7f90":"X_train= scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","f8b8a38c":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import plot_roc_curve, roc_auc_score, roc_curve\nfrom sklearn.model_selection import cross_val_score, cross_validate\nfrom sklearn.model_selection import GridSearchCV","ced14337":"X_train.shape","4e48d0d3":"X_test.shape","cc4b143f":"model = Sequential()\n\nmodel.add(Dense(32, activation = \"relu\"))  # sigunt ve hiperb. e g\u00f6re \u00f6\u011frenme daha fazla oldu\u011fu i\u00e7in relu kulland\u0131k.\nmodel.add(Dense(16, activation = \"relu\"))\nmodel.add(Dense(1, activation = \"sigmoid\"))  # output binary oldu\u011fu i\u00e7in sigmoid kulland\u0131k. \n\nmodel.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])","cc5f1e84":"early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"auto\", verbose = 1, patience = 20)","a6f3f4d9":"model.fit(x = X_train, y = y_train, validation_split = 0.1, batch_size = 32, epochs = 1000, verbose = 1,\n          callbacks = [early_stop])","c9984367":"model.summary()","98173170":"loss_df = pd.DataFrame(model.history.history)\nloss_df.head()","57219b1c":"loss_df.plot();","e4516741":"model.evaluate(X_test, y_test, verbose=0)","75188bd4":"loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n\nprint(\"loss : \", loss)\nprint(\"accuracy : \", accuracy)","6b19487b":"y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")   \n\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","8dfda96b":"from sklearn.utils import class_weight","10454927":"class_weights = class_weight.compute_class_weight('balanced',\n                                                 np.unique(y_train),\n                                                 y_train)","9e797bf0":"class_weights","20707395":"model_weighted = Sequential()\n\nmodel_weighted.add(Dense(32, activation = \"relu\"))  \nmodel_weighted.add(Dense(16, activation = \"relu\"))\nmodel_weighted.add(Dense(1, activation = \"sigmoid\"))  \n\nopt = Adam(lr = 0.005)\n\nmodel_weighted.compile(optimizer = opt, loss = \"binary_crossentropy\", metrics = [\"accuracy\"])","659bb910":"early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"auto\", verbose = 1, patience = 20)","645c9eec":"class_weight = {0: 0.62787777, 1: 2.45499182}  ","344193fd":"model_weighted.fit(x = X_train, y = y_train, \n                   validation_split = 0.1, \n                   batch_size = 32, \n                   epochs = 1000, \n                   verbose=1, \n                   callbacks = [early_stop], \n                   class_weight=class_weight)","8101a909":"model_weighted.summary()","1dec39ba":"loss_df = pd.DataFrame(model_weighted.history.history)\nloss_df.head()","3e437e1b":"loss_df.plot();","1121c46c":"model_weighted.evaluate(X_test, y_test, verbose=0)","9da59315":"loss, accuracy = model_weighted.evaluate(X_test, y_test, verbose=0)\nprint(\"loss : \", loss)\nprint(\"accuracy : \", accuracy)","b8901286":"y_pred = (model_weighted.predict(X_test) > 0.5).astype(\"int32\")\n#y_pred = model.predict_classes(X_test)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","e5f85a74":"def build_classifier(optimizer):\n    classifier = Sequential()\n    classifier.add(Dense(units = 32, activation = 'relu'))\n    classifier.add(Dense(units = 16, activation = 'relu'))\n    classifier.add(Dense(units = 1, activation = 'sigmoid'))\n    classifier.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return classifier","e34065f2":"early_stop = EarlyStopping(monitor = \"accuracy\", mode = \"auto\", verbose = 1, patience = 20)","c45004df":"class_weight = {0: 0.62787777, 1: 2.45499182}","61d2bc02":"classifier = KerasClassifier(build_fn = build_classifier, epochs = 200)\n\nparameters = {'batch_size': [32, 64],\n              'optimizer': ['adam', 'rmsprop', \"SGD\", \"adagrad\", \"adadelta\"]}\n\ngrid_model = GridSearchCV(estimator = classifier,\n                          param_grid = parameters,\n                          scoring = 'accuracy',\n                          cv = 10,\n                          n_jobs = -1,\n                          verbose = 1)\n\ngrid_model.fit(X_train, y_train, callbacks = [early_stop], class_weight = class_weight)","01dec8ab":"grid_model.best_score_","addabd05":"grid_model.best_params_","3238631d":"y_pred = (grid_model.predict(X_test) > 0.5).astype(\"int32\")\n#y_pred = model.predict_classes(X_test)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","582f79e4":"y_pred_proba = model.predict(X_test)\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('False Positive Rates')\nplt.ylabel('True Positive Rates')\nplt.title('ROC Curve for Keras Model')\nplt.show()","e9126575":"roc_auc_score(y_test, y_pred_proba)","197f6d53":"y_pred_proba = model_weighted.predict(X_test)\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for Keras Model_Weighted')\nplt.show()","5044e728":"roc_auc_score(y_test, y_pred_proba)","1a6234c7":"y_pred_proba = grid_model.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for Grid Model')\nplt.show()","fa2d0666":"roc_auc_score(y_test, y_pred_proba)","efc622c1":"import pickle\n\npickle.dump(scaler, open(\"scaler_exited\", 'wb'))","2335c54f":"final_model = Sequential()\n\nfinal_model.add(Dense(32, activation = \"relu\"))\nfinal_model.add(Dense(16, activation = \"relu\"))\nfinal_model.add(Dense(1, activation = \"sigmoid\"))  \n\nopt = Adam(lr = 0.005)\n\nfinal_model.compile(optimizer = opt, loss = \"binary_crossentropy\", metrics = [\"accuracy\"])","e27cb2dc":"early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"auto\", verbose = 1, patience = 20)","a6f0be16":"class_weight = {0: 0.62787777, 1: 2.45499182}","fd3e875f":"final_model.fit(x = X_train, y = y_train, \n                validation_data = (X_test, y_test), \n                batch_size = 32, \n                epochs = 1000, \n                verbose = 1, \n                callbacks = [early_stop], \n                class_weight = class_weight)","b8dcf086":"loss_df = pd.DataFrame(final_model.history.history)\n\nloss_df.plot();","4efc6b69":"y_pred = (final_model.predict(X_test) > 0.5).astype(\"int32\")\n#y_pred = model.predict_classes(X_test)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","80e1fb9d":"model.save('model_exited.h5')","d84503c4":"from tensorflow.keras.models import load_model","8deeff59":"model_exited = load_model('model_exited.h5')\n\nscaler_exited = pickle.load(open(\"scaler_exited\", \"rb\"))","1f133329":"random_ten_customer = df.sample(n=10, random_state=101).iloc[0:10, :11]\n\nrandom_ten_customer","cf4f4157":"random_ten_customer = scaler_exited.transform(random_ten_customer)\n\nrandom_ten_customer","1b805e38":"prediction = pd.DataFrame(model_exited.predict(random_ten_customer) > 0.5, columns=['prediction']).astype(\"float64\")\nprediction","e637a663":"actual = pd.DataFrame(df.sample(n=10, random_state=101)[\"churn\"]).rename({'churn': 'actual'}, axis=1).astype(\"float64\")\nactual ","d534e1aa":"pred_df = actual.join(prediction.set_index(actual.index))\n\npred_df","e1e1366e":"pred_df['prediction_accuracy'] = pred_df.apply(lambda x: \"TRUE\" if int(x['actual'] == x['prediction']) else \"FALSE\", axis=1)\n\npred_df","b29c57cc":"**It can be concluded that there has been weak correlations between the numerical features and the target variable. So, evenif we accept the level of 0.80 as criteria for multicollinearity, it can be concluded that there is no multi-colliniearity problem among the variables\". On the other hand it can be safely assumed that while our target variable of 'exited' demonstrates a slight negative correlation with the variables of \"creditscore\", \"tenure\", \"numberofproducts\" 'hascrcard' and 'isactivemember', it demonstrates slight positive correlation with the variables of 'age', 'balance' and 'estimatedsalary\".**","cfa9f755":"**In Deep Learning projects, if we actually care about and want a valuable evaluation on the minority classes in our dataset, it\u2019s important to understand that class balancing techniques are really necessary.**\n\n- https:\/\/towardsdatascience.com\/handling-imbalanced-datasets-in-deep-learning-f48407a0e758\n- https:\/\/datascience.stackexchange.com\/questions\/13490\/how-to-set-class-weights-for-imbalanced-classes-in-keras","e21dc79e":"<a id=\"toc\"><\/a>\n\n## <h3 style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\" class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tablist\" aria-controls=\"home\">TABLE OF CONTENTS<\/h3>\n\n* [   PREFACE](#0)\n* [1) LIBRARIES NEEDED IN THE STUDY](#1)\n    * [1.1 User Defined Functions](#1.1)\n* [2) DATA](#2)\n    * [2.1 Context](#2.1)\n    * [2.2 About the Features](#2.2) \n    * [2.3 What the Problem is](#2.3) \n* [3) ANALYSIS](#3)\n    * [3.1) Reading the Data](#3)\n* [4) EXPLORATORY DATA ANALYSIS (EDA) & VISUALIZATION](#4)\n    * [4.1 - A General Looking at the Data](#4.1)\n    * [4.2 - Convert Multi-Index Columns To One Level](#4.2)\n    * [4.3 - Handling with Missing Values](#4.3)\n    * [4.4 The Examination of Target Variable](#4.4)\n    * [4.5 - The Examination of Skewness, Kurtosis & Multicollinearity](#4.5)\n    * [4.6 Numerical vs. Categorical](#4.6)\n        * [4.6.1 Spliting Dataset into Numeric & Categoric Features](#4.6.1)\n        * [4.6.2 The Examination of Numerical Features](#4.6.2)\n        * [4.6.3 The Examination of Categorical Features](#4.6.3)\n        * [4.6.4 Dummy Operation](#4.6.4)  \n    * [4.7 Descriptive Statistics](#4.7)\n* [5) DATA PREPROCESSING](#5)\n    * [5.1 Train|Test Split Operations](#5.1)\n    * [5.2 Scaling Operation](#5.2)\n* [6) MODELING & MODEL PERFORMANCE](#6)        \n    * [6.1 Modelling Without \"class_weigth\"](#6.1)\n        * [6.1.1 The Evaluation of Model with Default Parameters (without class_weigth)](#6.1.1)\n    * [6.2 Modelling With \"class_weigth\"](#6.2)\n        * [6.2.1 The Evaluation of Model With \"class_weigth\"](#6.2.1)\n    * [6.3 Modelling With Best Parameters (GridsearchCV)](#6.3)\n        * [6.3.1 The Evaluation of ROC and AUC for Model without \"class_weigth\"](#6.3.1)\n        * [6.3.2 The Evaluation of ROC and AUC for Model with \"class_weigth\"](#6.3.2)\n        * [6.3.3 The Evaluation of ROC and AUC for GridsearchCV Model](#6.3.3)\n* [7) Final Model & Model Deployment](#7) \n    * [7.1 Saving the Best Model](#7.1)\n    * [7.2 Loading the Model & Scaler](#7.2)\n    * [7.3 Predicting Real Life Case](#7.3)\n","32947168":"<a id=\"3\"><\/a>\n## <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">3 - ANALYSIS<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","ad3cc18f":"<a id=\"6.3.2\"><\/a>\n#### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.3.2 The Evaluation of ROC and AUC for Model with \"class_weigth\"<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","503a77ea":"**First let's import related libraries.**","8fbef84a":"## Enjoy while...","29031063":"<a id=\"6.2.1\"><\/a>\n#### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.2.1 The Evaluation of Model With \"class_weigth\"<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","03154c79":"**Now, let's examine crosstab outputs for each variable.**","9c5ff794":"<a id=\"6\"><\/a>\n## <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">6 - Modelling & Model Performance<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","2550c5a9":"<a id=\"6.1.1\"><\/a>\n#### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.1.1 The Evaluation of Model with Default Parameters (without class_weigth)<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","02acbc13":"**Kurtosis**","9d89f8d1":"<a id=\"7.3\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">7.3 Predicting Real Life Case<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","844adfbc":"Let's check if there is a multicollinearity problem among the features.","3f50bde5":"<a id=\"5.1\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">5.1 Train|Test Split Operations<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","4bf5f5cb":"**Now we can compare each categorical variable with target variable for the bivariate analysis.**","c400f654":"<a id=\"7\"><\/a>\n## <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">7 - Final Model & Model Deployment<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","a6301ce3":"<a id=\"4.6\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">4.6 Numerical vs. Categorical<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","14f1b961":"**The Framework in This Section:**\n\n    1. Implement basic steps to see how is your data looks like\n    2. Check for missing values\n    3. Drop the features that not suitable for modelling\n    4. Implement basic visualization steps such as histogram, countplot, heatmap\n    5. Convert categorical variables to dummy variables","d29ae45d":"**Fortunately we have NO missing values which is NOT case in real life.**","0eab3b10":"<a id=\"5\"><\/a>\n## <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">5 - DATA PREPROCESSING<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","3aa38e7c":"<a id=\"6.1\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.1 Modelling Without \"class_weigth\"<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","12651e70":"**Skewness**","e3ae1df4":"**Multicollinearity**","98a2939c":"<a id=\"1.1\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">1.1 User Defined Functions<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**We have defined some useful user defined functions**","0ca1edac":"<a id=\"4.6.3\"><\/a>\n#### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">4.6.3 The Examination of Categorical Features<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","a8f7cf19":"# <p style=\"background-color:#9c2162;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\">CHURN PREDICTION FOR BANK CUSTOMERS<\/p>\n\n![image-2.png](attachment:image-2.png)\n\n<p style=\"background-color:#9c2162;font-family:newtimeroman;color:#FFF9ED;font-size:140%;text-align:left;border-radius:10px 10px;\">Image credit : https:\/\/www.smartlook.com\/blog\/customer-churn-retention\/<\/p> ","ba5a0fda":"**You can use first_looking(df) user defined function above and profile_report() for getting a general insight before going further in the analysis**\n\nPandas profiling is an open source Python module with which we can quickly do an exploratory data analysis with just a few lines of code. Similarly, for EDA, profile_report() is One-Line Magical Code creating reports in the interactive HTML format which is quite easy to understand and analyze the data. In short, at the first hand, what pandas profiling does is to save us all the work of visualizing and understanding the distribution of each variable.\n\n**For a better understanding and more information, please refer to [external link text](https:\/\/www.analyticsvidhya.com\/blog\/2021\/06\/generate-reports-using-pandas-profiling-deploy-using-streamlit\/) & [external link text](https:\/\/towardsdatascience.com\/exploratory-data-analysis-with-pandas-profiling-de3aae2ddff3)**","64d3c8e7":"<a id=\"2\"><\/a>\n## <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">2 - DATA<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**This is a dataset in which there are details of a bank's customers and the target variable is a binary variable reflecting the fact whether the customer left the bank (closed his account) or he continues to be a customer. Therefore, the study will try to predict behaviors\/attributes to retain bank customers.**\n\n**For a better understanding and more information, please refer to [external link text](https:\/\/www.kaggle.com\/shrutimechlearn\/churn-modelling)**","a119134f":"<a id=\"4.6.2\"><\/a>\n#### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">4.6.2 The Examination of Numerical Features<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","f9242c2f":"<a id=\"3.1\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">3.1 Reading The Data<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**How to read and assign the dataset as df. [external link text](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.read_csv.html) (You can define it as what you want instead of df)**","075e5db3":"<a id=\"6.3\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.3 Modelling With Best Parameters (GridsearchCV)<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","f0763971":"<a id=\"2.3\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:left; border-radius:10px 10px;\">2.3 What The Problem Is<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**- In the given study, we have a classification problem.**\n\n- Based on the data and data dictionary, We have a classification problem.\n- We will try to predict behaviors\/attributes to retain bank customers on the target variable in the given dataset.\n- The Target variable in the given dataset is \"exited\".\n- We will make classification on the target variable \"exited\".\n- Lastly we will build a model to get best classification possible on the target variablevia Deep Learning algorithms.\n- For that we will examine whether the target variable is balanced or imbalanced.\n- As we will see later, our target variable has imbalanced data\n- For that reason we are not going to use Accuracy score,\n- Based on the problem on the hand, we will use Recall score.","45f373fa":"**Swarmplot is the combination of a strip plot and a violin plot. Along with the number of data points, it also provides their respective distribution. Therefore, a swarm plot is a great way of demonstrating and assessing the distribution of an attribute or the joint distribution of a couple of attributes. Now, let's plot it.**","e83a00ea":"<a id=\"4.3\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">4.3 Handling With Missing Values<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**Missing data (or missing values) is defined as the data value that is not stored for a variable in the observation of interest. The problem of missing data is relatively common in almost all research and can have a significant effect on the conclusions that can be drawn from the data. Accordingly, all studies need to focus on handling the missing data, problems caused by missing data, and the methods to avoid or minimize their effects on the analysis. Otherwise, missing data can reduce the statistical power of a study and can produce biased estimates, leading to invalid conclusions.**\n\n**For a better understanding and more information how to handle with missing values in Machine Learning, please refer to [external link text](https:\/\/machinelearningmastery.com\/handle-missing-data-python\/) & [external link text](https:\/\/www.kaggle.com\/kaanboke\/the-most-used-methods-to-deal-with-missing-values)**","cd51fcb8":"<a id=\"1\"><\/a>\n## <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">1 - LIBRARIES NEEDED IN THE STUDY<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**First, if you do NOT have the following libraries in your kernel, just install them since you have to need them to import some libraries.**","42f366d2":"Let's first plot the heatmap and examine the correlations among the features visually. ","7a15ed20":"**Early stopping is a method that allows you to specify an arbitrary large number of training epochs and stop training once the model performance stops improving on a hold out validation dataset.**\n\n**Additionally, for preventing data leakage, \"dropout\" can be used. The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, which helps prevent overfitting.**\n\n**For a better understanding and more information, please refer to [external link text](https:\/\/machinelearningmastery.com\/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping\/#:~:text=Early%20stopping%20is%20a%20method,deep%20learning%20neural%20network%20models.)**","34130fa0":"<a id=\"4\"><\/a>\n## <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">4 - EXPLORATORY DATA ANALYSIS (EDA) & VISUALIZATION<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\n\n**Before performing Machine Learning algorithms, you need to know the data well in order to label the observations correctly. You need to analyze frequency distributions of features, relationships and correlations between the independent variables and the dependent variable. It is recommended to apply data visualization techniques. Observing breakpoints helps you to internalize the data.**\n\n**So, in this Exploratory Data Analysis section the data will be analyzed by summarizing its main characteristics, using statistical graphics and other data visualization methods. As such the reader will be familiar with what the data can tell.**","2ebbcb05":"**Before constructing our model, we should convert categorical features into dummies.**\n\n**A common strategy for encoding categorical variables is \"One-Hot\", \"Label\" or \"Dummy\" encoding, where a new column is created for every N (or N-1) of the possible values, and those columns are given a value of 1 or 0 depending on whether the feature had that value.**","908c50b2":"**A General Look At The Missing Values**","3d18a91b":"**Let's get rid of commas in the column of \"balance\" and \"estimatedsalary\"**","191ca760":"**In our study, we utilized \"Dummy\" encoding; however, instead of applying get_dummies() for conversion, you can also prefer one of other popular categorical encoding processes, Label Encoding or One-Hot Encoding techniques found in scikit-learn library.**","266a4e7f":"<a id=\"7.1\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">7.1 Saving The Best Model<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","9625da84":"**Overall Distribution of Target (\"exited\") Variable.**","753f05bf":"**Interpreting Skewness**\n\n0.5 is our threshold-limit to evaluate skewness. However, overall below abs(1) seems acceptable for the linear models.","5510b132":"**While modelling, we reserved 10% of the data to be used for training. We do not train this reserved part in any way. It will be used for validation.**","63588677":"**To prevent data leakage, while we apply fit_transform() with X_train, we use transform() with X_test for scaling.**","5c9e3600":"<a id=\"5.2\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">5.2 Scaling Operation<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","175fc670":"**As the iterations progressed, while loss and val_loss values decreased, the accuracy and val_accurasy values also increased.**","6cef190b":"**When we examine the distribution of related features in the swarm plot, it's, in general, clear that that bank costemers between the ages of 45 and 65 for male\/female and in geography where they live leave the bank (closed their accounts). We need to take this assumption into consideration while making assessment on modelling.** ","3141fbb9":"Kurtosis are of three types:\n\nMesokurtic: When the tails of the distibution is similar to the normal distribution then it is mesokurtic. The kutosis for normal distibution is 3.\n\nLeptokurtic: If the kurtosis is greater than 3 then it is leptokurtic. In this case, the tails will be heaviour than the normal distribution which means lots of outliers are present in the data. It can be recognized as thin bell shaped distribution with peak higher than normal distribution.\n\nPlatykurtic: Kurtosis will be less than 3 which implies thinner tail or lack of outliers than normal distribution.In case of platykurtic, bell shaped distribution will be broader and peak will be lower than the mesokurtic. Hair et al. (2010) and Bryne (2010) argued that data is considered to be normal if Skewness is between \u20102 to +2 and Kurtosis is between \u20107 to +7.\n\nMulti-normality data tests are performed using leveling asymmetry tests (skewness < 3), (Kurtosis between -2 and 2) and Mardia criterion (< 3). Source Chemingui, H., & Ben lallouna, H. (2013).\n\nSkewness and kurtosis index were used to identify the normality of the data. The result suggested the deviation of data from normality was not severe as the value of skewness and kurtosis index were below 3 and 10 respectively (Kline, 2011). Source Yadav, R., & Pathak, G. S. (2016).","b6477178":"<a id=\"4.4\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">4.4 The Examination of Target Variable<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","de3355d5":"<a id=\"4.5\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">4.5 - The Examination of Skewness, Kurtosis & Multicollinearity<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","4ffd6eee":"**The selection of \"patience\" value is important at this point. While there would be improvement in the future if we cut the iteration early, the score would be lower. Therefore, it is important to choose the appropriate value for patience. Otherwise you may be limiting the score with the value you give.**\n\n**In our analysis, we tried the values of 10, 15 and 20 for \"patience\"; however, we picked up 20 since giving better results.**","8fc8e449":"**Let's plot and examine the \"loss\", \"accuracy\", \"val_loss\" and\t\"val_accuracy\" curves.** ","a4f4a874":"<a id=\"4.7\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">4.7 Descriptive Statistics<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","44d453cd":"<a id=\"7.2\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">7.2 Loading the Model & Scaler<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","9029f10b":"**Let's examine missing values visually.**","0c8e74d4":"<a id=\"6.3.3\"><\/a>\n#### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.3.3 The Evaluation of ROC and AUC for GridsearchCV Model\"<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","a4c67fbe":"<a id=\"2.2\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:left; border-radius:10px 10px;\">2.2 About The Features<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**The features in the given dataset are:**\n\n- **rownumber:** Row Numbers from 1 to 10000.\n\n- **customerid:** A unique ID that identifies each customer.\n\n- **surname:** The customer\u2019s surname.\n\n- **creditscore:** A credit score is a number between 300\u2013850 that depicts a consumer's creditworthiness.\n\n- **geography:** The country from which the customer belongs to.\n\n- **Gender:** The customer\u2019s gender: Male, Female\n\n- **Age:** The customer\u2019s current age, in years, at the time of being customer.\n\n- **tenure:** The number of years for which the customer has been with the bank.\n\n\n- **balance:** Bank balance of the customer.\n\n- **numofproducts:** the number of bank products the customer is utilising.\n\n- **hascrcard:** The number of credit cards given to the customer by the bank.\n\n- **isactivemember:** Binary Flag for indicating if the client is active or not with the bank before the moment where the client exits the company (recorded in the variable \"exited\")\n\n- **exited:** Binary flag 1 if the customer closed account with bank and 0 if the customer is retained.","f8dcf2d3":"**Let's remove the columns of \"rownumber\", \"customerid\", \"surname\" from the given dataset since they do NOT have any contribution to classification in our analysis.**","82000160":"<a id=\"4.6.1\"><\/a>\n#### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">4.6.1 Spliting Dataset into Numeric & Categoric Features<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","ef4cff81":"**Plotly library is very useful for data visualization and understanding the data simply and easily. One of the visualization functions is scatter_3d() which is used to create a 3D scatter plot and can be used with pandas dataframes.** \n\n**3D scatter plots are used to plot data points on three axes in an attempt to show the relationship between three variables. Each row in the data table is represented by a marker whose position depends on its values in the columns set on the X, Y, and Z axes. A fourth variable can be set to correspond to the color or size of the markers; thus, adding yet another dimension to the plot.**","cf2cb3bb":"<a id=\"6.2\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.2 Modelling With \"class_weigth\"<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","2fa02e6c":"<a id=\"0\"><\/a>\n## <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">PREFACE<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**In this Exploratory Data Analysis (EDA) and Deep Learning Analysis, this study will examine the dataset named as \"Churn_Modelling\" which can also be reached via the 'Churn_Modelling.csv.csv' file at Kaggle website [external link text](https:\/\/www.kaggle.com\/shrutimechlearn\/churn-modelling).**\n\n**This study, in general, will cover what any beginner in Deep Learning can do as much as possible for a better understanding with the given dataset not only by examining its various aspects but also visualising it. Later S\/he will be familiar with Clustering algorithms in Deep Learning with Keras.**","723beded":"<a id=\"2.1\"><\/a>\n### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:left; border-radius:10px 10px;\">2.1 Context<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**Each row represents a bank customer, each column contains customer\u2019s attributes described on the column Metadata.**\n\n**The data set includes information about:**\n\n- **Customers who left bank \u2013 the column is called \"Exited\"**\n- **Customer account information that each customer has 'creditscore', 'balance', 'numofproducts', 'hascrcard',  'isactivemember'.**\n- **Demographic info about customers \u2013 surname, geography, gender, age range, tenure and 'estimatedsalary'.**","456af4fb":"**Importing Related Libraries**","7e1c759b":"**\"Sigmoid\" activation function is also a logistic function, compressing data between 0 and 1. Therefore, it would be more correct to use this function in the output layer. Moreover, we used \"binary_crossentropy\" as the loss metric since the output is binary and \"relu\" is used in hidden layers since learning usually happens faster.**","3e0bd514":"**As easily understood from the plots, we have an imbalanced data.**\n\n**% 20.4 of customers, who represent 2037 people out of 10000, didn't show loyalty to the company and got churned while % 79.6 of which representing 7960 people out of 10000 showed loyalty to the company.**","ff442f25":"<a id=\"6.3.1\"><\/a>\n#### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.3.1 The Evaluation of ROC and AUC for Model without \"class_weigth\"<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","51de04bf":"<a id=\"4.6.4\"><\/a>\n#### <p style=\"background-color:#9c2162; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">4.6.4 Dummy Operation<\/p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#e9a1ed\" data-toggle=\"popover\">Table of Contents<\/a>","1635ab0f":"**Interpreting Kurtosis**\n\n2.0 is our threshold-limit to evaluate kurtosis.","634957f3":"**MinMaxScaler is preferred for collecting data in a narrower range and for faster speed. So let's do it.**\n\n**For a better understanding and more information, please refer to [external link text](https:\/\/www.analyticsvidhya.com\/blog\/2020\/04\/feature-scaling-machine-learning-normalization-standardization\/) & [external link text](https:\/\/machinelearningmastery.com\/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling\/)**"}}