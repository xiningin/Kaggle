{"cell_type":{"a1bf93b4":"code","85c42192":"code","ebf68827":"code","eddb55dd":"code","5d24f84f":"code","332d7c97":"code","1012ab41":"code","a7611d58":"code","a7dc5e77":"code","4aa7dace":"code","83494913":"code","9e685ba2":"code","80a53d43":"code","5ac2235c":"code","332228c9":"code","837557c6":"code","d9e7c8ac":"code","24ccbfd6":"code","a4452129":"code","22494f27":"code","a77813b6":"code","f0c5e889":"code","ae234f2d":"code","9793438a":"code","7f55f5af":"code","e4ec3bd8":"code","93437bae":"code","755626f6":"code","66b56786":"code","7344cc63":"code","90fc66aa":"code","da732307":"code","682edd0c":"code","8e506ba6":"code","04e18077":"code","a0ec9f30":"code","bda09e3b":"code","58a5eccc":"code","f724cc70":"markdown","e32b2592":"markdown","2a5c20d1":"markdown","5bb5fd7b":"markdown","415faeaa":"markdown"},"source":{"a1bf93b4":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","85c42192":"df_ti = pd.read_csv(\"..\/input\/travel-insurance\/travel insurance.csv\")\ndf_ti.head()","ebf68827":"df_ti.info()","eddb55dd":"clean_ti = df_ti.copy()\nclean_ti.info()","5d24f84f":"#assign to category to save memory usage from 5+mb to 2+mb\ncategorical_col = ['Agency', 'Agency Type', 'Distribution Channel', 'Product Name', 'Destination','Gender']\n# clean_ti.drop(['Gender'], axis=1, inplace=True)\nclean_ti['Gender'] = clean_ti['Gender'].fillna(\"not_disclosed\")\nclean_ti[categorical_col] = clean_ti[categorical_col].astype('category') \nclean_ti.info()","332d7c97":"#look unique value for each columns\nfor col in categorical_col:\n    uniq = len(clean_ti[col].unique())\n    print(f'{col} :{uniq} Categories')","1012ab41":"from sklearn.preprocessing import LabelEncoder\n# encode the labels, converting them from strings to integers\nle = LabelEncoder()\nlabels = clean_ti['Claim']\nlabels = le.fit_transform(clean_ti['Claim'])","a7611d58":"clean_ti.describe()","a7dc5e77":"clean_ti.hist(figsize=(20,10), grid = False, layout=(3,2), bins = 10);","4aa7dace":"clean_ti[clean_ti[\"Duration\"] <0]","83494913":"clean_ti[clean_ti[\"Age\"] >100]","9e685ba2":"clean_ti.loc[clean_ti['Duration'] < 0, 'Duration'] = 49.317\nclean_ti.loc[clean_ti['Age'] > 100, 'Age'] = 39.97","80a53d43":"clean_ti.describe()","5ac2235c":"#numerical columns\ntest_ti = clean_ti.copy()\ntest_ti['Claim2'] = labels\n\nplt.title(\"Pearson Correlation for Numerical Feature\")\nsns.heatmap(test_ti.corr(), annot=True)","332228c9":"test_ti.corr()","837557c6":"\"\"\"\nsource :https:\/\/towardsdatascience.com\/the-search-for-categorical-correlation-a1cf7f1888c9\n        https:\/\/www.kaggle.com\/ayangupta\/predict-the-claim\n\"\"\"\nimport scipy.stats as ss\nimport numpy as np\n\ndef cramers_v(x, y):\n    confusion_matrix = pd.crosstab(x,y)\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2\/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))\/(n-1))\n    rcorr = r-((r-1)**2)\/(n-1)\n    kcorr = k-((k-1)**2)\/(n-1)\n    return np.sqrt(phi2corr\/min((kcorr-1),(rcorr-1)))\n\ncategorical=['Agency', 'Agency Type', 'Distribution Channel', 'Product Name',  'Destination','Gender','Claim']\ncramers=pd.DataFrame({i:[cramers_v(clean_ti[i],clean_ti[j]) for j in categorical] for i in categorical})\ncramers['column']=[i for i in categorical if i not in ['memberid']]\ncramers.set_index('column',inplace=True)\n\n#categorical correlation heatmap\nplt.figure(figsize=(10,7))\nplt.title(\"Cramer's V Chi-Squared\")\nsns.heatmap(cramers,annot=True)\nplt.show()","d9e7c8ac":"product_claim = pd.crosstab(clean_ti['Product Name'],clean_ti['Claim'],margins=True)\nproduct_claim.drop(index=['All'],inplace=True)\n\nplt.figure(figsize=(10, 7))\nsns.barplot(product_claim.index, product_claim.Yes.values)\nplt.xticks(rotation=90)\nplt.title(\"Claim:Yes Per Product Name\")\nplt.show()","24ccbfd6":"plt.figure(figsize=(10, 7))\nsns.barplot(product_claim.index, product_claim.No.values)\nplt.xticks(rotation=90)\nplt.title(\"Claim:No Per Product Name\")\nplt.show()","a4452129":"#target columns\nsns.countplot(clean_ti['Claim'])\nplt.title(\"Target Label Distribution\")\nplt.grid(axis='y')\nplt.show()","22494f27":"#reducing Target No due to severe imbalance\nrandom_no = clean_ti[clean_ti['Claim']=='No'].sample(frac=1)\nn_to_drop = len(random_no) - 10000\n\nclean_reduce = clean_ti.drop(axis=0, index=random_no.index[:n_to_drop])\n\n#target columns\nsns.countplot(clean_reduce['Claim'])\nplt.title(\"Target Label Distribution After Reducing\")\nplt.grid(axis='y')\nplt.show()","a77813b6":"from sklearn.model_selection import train_test_split\nfrom collections import Counter\n\n\n#split label and features\n#One Hot Encoding for categorical data\nX = clean_reduce.drop(columns=['Claim'])\nX = pd.get_dummies(X, columns=categorical_col).values\n# y = clean_ti['Claim'].replace(labels).values\ny = clean_reduce['Claim'].replace({'No':0, 'Yes':1}).values\nprint(f'Datasets Features Size {X.shape}')\n\n#X, y without Oversampling\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)\nprint('Traininng shape %s' % Counter(y_train))\nprint('Testing shape %s' % Counter(y_test))","f0c5e889":"from imblearn.over_sampling import SMOTE\n\n#with SMOTE\nsm = SMOTE(random_state=42)\nX_smote, y_smote = sm.fit_resample(X_train, y_train)\nprint('Resampled dataset shape %s' % Counter(y_smote))","ae234f2d":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingClassifier \nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBClassifier\n\n\ndef model_check(models, X_train, y_train):\n    for name, model in models.items():\n        score = cross_val_score(model, X_train, y_train, cv=3, scoring='f1', n_jobs=-1)\n        print(f'{name} F1 score : {np.mean(score)}')\n\nmodels = {'random_forest':RandomForestClassifier(), \n          'logistic_reg':LogisticRegression(), \n          'XGB':XGBClassifier(), \n          'GB':GradientBoostingClassifier()}\n\nprint(\"Without SMOTE\")\nmodel_check(models, X_train, y_train)\nprint()\n\nprint(\"With SMOTE\")\nmodel_check(models, X_smote, y_smote)","9793438a":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n\nmodel = RandomForestClassifier(n_jobs=-1, verbose=1)\nparam_dist = {'n_estimators':[300, 400, 500, 600], 'max_depth':[5,6,7,8]}\n\nrandom = RandomizedSearchCV(model, param_dist, random_state=0, scoring='f1', n_jobs=-1, cv=3, verbose=1)\nsearch = random.fit(X_smote, y_smote)\nprint('BEST PARAM', search.best_params_)","7f55f5af":"pd.DataFrame(search.cv_results_).sort_values(by='rank_test_score')","e4ec3bd8":"from sklearn.metrics import f1_score\nmodel = RandomForestClassifier(n_estimators=400, max_depth=8)\nscore = cross_val_score(model,  X_smote, y_smote, cv=5, scoring='f1', n_jobs=-1)\n\nprint(f'Model F1 score : {np.mean(score)}')","93437bae":"model.fit(X_smote, y_smote)","755626f6":"from sklearn.metrics import plot_confusion_matrix, classification_report, f1_score\n\n#using test_set\nplot_confusion_matrix(model, X_test, y_test)\nprint(classification_report(y_test, model.predict(X_test)))\nprint(f1_score(y_test, model.predict(X_test)))","66b56786":"# import joblib\n\n# joblib.dump(model, 'model_insurance_RF.pkl') ","7344cc63":"import matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\nimport keras.backend as K","90fc66aa":"#splitting for validation in training process\nX_smote_train, X_val, y_smote_train, y_val = train_test_split(X_smote, y_smote, \n                                                              test_size=0.2, \n                                                              random_state=0)\n\n#Standardization is very useful for deeplearning model to learn\nscaler = StandardScaler()\nX_smote_scaled = scaler.fit_transform(X_smote_train)\nX_val_scaled = scaler.transform(X_val)\nX_test_scaled = scaler.transform(X_test)","da732307":"batch_size = 1024\n\n#calling dataset\ndef data_to_tensor(X, y, batch_size, shuffle=True):\n  ds = tf.data.Dataset.from_tensor_slices((X, y))\n  if shuffle:\n    ds = ds.shuffle(buffer_size=len(X))\n  ds = ds.batch(batch_size)\n  ds = ds.prefetch(batch_size)\n  return ds\n\n#datasets per batch\ntrain_ds = data_to_tensor(X_smote_train, y_smote_train, batch_size=batch_size)\nval_ds = data_to_tensor(X_val, y_val, batch_size=batch_size)\n\n#model\ndef Model_ANN():\n    model = tf.keras.Sequential([\n            layers.Dense(128, activation='relu', input_shape=(X_smote_train.shape[1],)),\n            layers.Dense(128, activation='relu'),\n            layers.Dropout(.5),\n            layers.Dense(64, activation='relu'),\n            layers.Dropout(.3),\n            layers.Dense(32, activation='relu'),\n            layers.Dropout(.3),\n            layers.Dense(1, activation='sigmoid')])\n    \n    optim =tf.keras.optimizers.Adam(learning_rate=1e-3) \n\n    model.compile(optimizer=optim,\n                  loss=tf.keras.losses.BinaryCrossentropy(),\n                  metrics=['AUC'])\n    \n    return model\n\nmodel_ann = Model_ANN()\nmodel_ann.summary()","682edd0c":"# def get_f1(y_true, y_pred): #taken from old keras source code\n#     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n#     possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n#     predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n#     precision = true_positives \/ (predicted_positives + K.epsilon())\n#     recall = true_positives \/ (possible_positives + K.epsilon())\n#     f1_val = 2*(precision*recall)\/(precision+recall+K.epsilon())\n#     return f1_val","8e506ba6":"tf.keras.utils.plot_model(model_ann, show_shapes=True, rankdir=\"TB\")","04e18077":"EPOCHS = 1000\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_auc', \n    verbose=1,\n    patience=10,\n    mode='max',\n    restore_best_weights=True)\n\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_auc', \n                                                 factor=0.1, patience=10, \n                                                 verbose=0, mode='auto',\n                                                 min_delta=0.0001)\n\nhistory = model_ann.fit(train_ds, epochs=EPOCHS, \n                  validation_data=val_ds, \n                  callbacks=[early_stopping], verbose=1)","a0ec9f30":"plt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], linestyle=\"--\", label='Val Loss')\nplt.legend()\nplt.title(\"Training Loss\")\nplt.show()","bda09e3b":"plt.plot(history.history['auc'], label='Training AUC')\nplt.plot(history.history['val_auc'], linestyle=\"--\", label='Val AUC')\nplt.legend()\nplt.title(\"Training AUC\")\nplt.show()","58a5eccc":"import seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\ny_predict = model_ann.predict(X_test_scaled)>0.5\n\nsns.heatmap(confusion_matrix(y_test, y_predict), annot=True, fmt=\"d\")\nprint(classification_report(y_test, y_predict))","f724cc70":"# Deep Learning Model (ANN)","e32b2592":"# Modelling Machine Learning","2a5c20d1":"## Evaluation","5bb5fd7b":"## Parameter Tuning","415faeaa":"# Preprocessing"}}