{"cell_type":{"870a18fb":"code","60eb7559":"code","3035bf0f":"code","f6d2a7e1":"code","167fb44e":"code","fc6f0b43":"code","19b67dbd":"code","97132c89":"code","4e3fd67e":"code","49f03466":"code","98dd2cc2":"code","bdd9cfbe":"code","6e526498":"code","c8e7984f":"code","067e9d5f":"code","c553cc9b":"code","fd47a7bd":"code","c7eb7002":"code","08022159":"code","035ce1a4":"code","f86c6c0b":"code","2c4abb2a":"code","0e024bb6":"code","4e8c1232":"code","3914b9f8":"code","12e31452":"code","56b21f6b":"code","adf0c3ff":"code","37a28fc4":"code","406c6636":"code","2202c544":"code","c2816ef5":"code","20d6e80a":"code","3c99bfa1":"code","287f95fa":"code","907d5a0a":"code","2ff6c7cb":"code","53c9ea0e":"code","a169c9cb":"code","090bd322":"code","f2ef855d":"code","b2666761":"code","94c91df9":"code","0f9fe7cc":"markdown","e7db4515":"markdown","f6f67e07":"markdown","6ada7151":"markdown","03d85633":"markdown","ba1a5bbb":"markdown","5c6771dd":"markdown","1384f93e":"markdown","3e336286":"markdown","67df9b45":"markdown"},"source":{"870a18fb":"import time\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib\nfrom matplotlib.pyplot import figure\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\n\nimport random\nrandom.seed(42)","60eb7559":"# pretty plots\nsns.set_context('talk')\nmatplotlib.rcParams['font.family'] = 'arial'","3035bf0f":"df = pd.read_csv('..\/input\/uio_clean.csv')\n\n# convert to datetime and strip seconds \ndf['pickup_datetime'] = df['pickup_datetime'].str.slice(0, 16)\ndf['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'], format='%Y-%m-%d %H:%M')\ndf['dropoff_datetime'] = df['dropoff_datetime'].str.slice(0, 16)\ndf['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'], format='%Y-%m-%d %H:%M')\n\ndisplay(df.info())\ndisplay(df.head())","f6d2a7e1":"original_data_size = df.shape[0]\n# creat new feature avg_speed\ndf['avg_speed'] = df['dist_meters'] \/ df['trip_duration']  # m\/sec\ndf['avg_speed'] = 3.6*df['avg_speed'] # km\/hr\ndf['avg_speed'] = np.round(df['avg_speed'])\n\n# Remove observations with missing values\ndf.dropna(how='any', axis='rows', inplace=True)\n\n# Removing observations with erroneous values\nmask = df['pickup_longitude'].between(-80, -77)\nmask &= df['dropoff_longitude'].between(-80, -77)\nmask &= df['pickup_latitude'].between(-4, 1)\nmask &= df['dropoff_latitude'].between(-4, 1)\nmask &= df['trip_duration'].between(30, 2*3600)\nmask &= df['wait_sec'].between(0, 2*3600)\nmask &= df['dist_meters'].between(100, 100*1000)\nmask &= (df['trip_duration'] > df['wait_sec'])\nmask &= df['avg_speed'].between(5, 90)\n\ndf = df[mask]\ncleaned_data_size = df.shape[0]\n\nprint('Original dataset size:', original_data_size,\n     '\\nRemoving erroneous value. Cleaned dataset size:', cleaned_data_size)","167fb44e":"# checking if Ids are unique, \nprint(\"Number of columns and rows and columns are {} and {} respectively.\".format(df.shape[1], df.shape[0]))\nif df.id.nunique() == df.shape[0]:\n    print(\"Dataset ids are unique\")\nprint('\\n')\n\n# check data integrity\n# remove observations if trip_duration is off my more than 2 mins of pickup and dropoff timestamp \ndf['check_trip_duration'] = (df['dropoff_datetime'] - df['pickup_datetime']).map(lambda x: x.total_seconds())\nduration_difference = df[np.abs(df['check_trip_duration'].values  - df['trip_duration'].values) > 2*60]\nprint('Deleting', duration_difference.shape[0], 'rows with trip_duration different from pickup and dropoff times')\ndf = df[np.abs(df['check_trip_duration'].values  - df['trip_duration'].values) <= 2*60]\ndel df['check_trip_duration']\nprint('Dataset:', df.shape[0])\nprint('\\n')\n\n\nprint('store_and_fwd_flag column has only 1 unique value:', df['store_and_fwd_flag'].unique())\ndel df['store_and_fwd_flag']\nprint('Removing feature: store_and_fwd_flag')","fc6f0b43":"# since most trips are attributed to 1 vendor removing vendor id \n# replacing with boolean if vendor in Quito\nvendors = pd.DataFrame(df.groupby('vendor_id')['id'].count())\nvendors = vendors.reset_index()\nvendors['id'] = 100*vendors['id'] \/ vendors['id'].sum()\nvendors = vendors.sort_values('id', ascending=False)\nvendors.columns = ['vendor_id', 'trips']\n\nfig = figure(num=None, figsize=(12, 4), dpi=80, facecolor='w', edgecolor='k')\n\nplt.subplot(1, 1, 1)\nax1 = sns.barplot(x=\"trips\", y=\"vendor_id\", data=vendors, palette=(\"Greys_d\"))\nax1.set_xlabel('Percentage of trips', weight='bold')\nax1.set_ylabel('Vendor', weight = 'bold')\nax1.set_title('Quito vendor breakdown\\n')\n\nsns.despine()\nplt.tight_layout();","19b67dbd":"print(np.round(vendors.trips[0],1),'% of trips have 1 vendor')\ndf['quito'] = 1*(df['vendor_id'] == 'Quito')\n\n# deleting col 'store_and_fwd_flag'\ndel df['vendor_id']\nprint('Removing feature: vendor_id')\nprint('Adding feature: quito')","97132c89":"# Whats the daily number of trips?\ndf['pick_date'] = df['pickup_datetime'].dt.date\ndf['pick_date'] = pd.to_datetime(df['pick_date'])\n\npickups = pd.DataFrame(df.groupby(['pick_date'])['id'].count())\npickups = pickups.reset_index()\npickups = pickups.sort_values('pick_date', ascending=True)\n\n# inital data is very sparse - remove sparse dates\nstart_date = pickups[pickups.id >1]['pick_date'].min()\nend_date = pickups[pickups.id >1]['pick_date'].max()\n#cutoff_date = pd.to_datetime('2017-06-30').date()\n\nprint('Dataset starts from', start_date.date(),'till', end_date.date())\n\n# lets start from '2016-06-22' where we have more than 1 daily trip\npickups = pickups[pickups.pick_date >= start_date]\n\nfig = figure(num=None, figsize=(12, 8), dpi=80, facecolor='w', edgecolor='k')\n\nplt.subplot(1, 1, 1)\nax1 = sns.lineplot(x=\"pick_date\", y=\"id\", data=pickups)\nax1.set_ylabel('No. of trips', weight = 'bold')\nax1.set_xlabel('Pickup date', weight = 'bold')\nax1.set_title('Daily trips\\n')\nhandles, labels = ax1.get_legend_handles_labels()\n\nsns.despine()\nplt.tight_layout();","4e3fd67e":"# quantiles of wait_sec \n# median is around 4 mins\nnp.quantile(df.wait_sec, [.05, .25, .50, .75, .95])","49f03466":"# source https:\/\/www.kaggle.com\/maheshdadhich\/strength-of-visualization-python-visuals-tutorial\n\ndef haversine_(lat1, lng1, lat2, lng2):\n    \"\"\"haversine distance: \n    great-circle distance between two points on a sphere given their longitudes and latitudes.\"\"\"\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    AVG_EARTH_RADIUS = 6371  # in km\n    lat = lat2 - lat1\n    lng = lng2 - lng1\n    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n    return(h)\n\ndef manhattan_distance_pd(lat1, lng1, lat2, lng2):\n    \"\"\"manhatten distance: \n    sum of  horizontal and vertical distances between points on a grid\"\"\"\n    a = haversine_(lat1, lng1, lat1, lng2)\n    b = haversine_(lat1, lng1, lat2, lng1)\n    return a + b\n\nimport math\ndef bearing_array(lat1, lng1, lat2, lng2):\n    \"\"\"bearing:\n    horizontal angle between direction of an object and another object\"\"\"\n    AVG_EARTH_RADIUS = 6371  # in km\n    lng_delta_rad = np.radians(lng2 - lng1)\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    y = np.sin(lng_delta_rad) * np.cos(lat2)\n    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)\n    return np.degrees(np.arctan2(y, x))","98dd2cc2":"# distance related features \n# round to nearest meter\ndf.loc[:,'hvsine_pick_drop'] = haversine_(df['pickup_latitude'].values, \n                                                df['pickup_longitude'].values, \n                                                df['dropoff_latitude'].values, \n                                                df['dropoff_longitude'].values)\ndf.loc[:,'hvsine_pick_drop'] = np.round(df.loc[:,'hvsine_pick_drop'], 3)\ndf.loc[:,'manhtn_pick_drop'] = manhattan_distance_pd(df['pickup_latitude'].values, \n                                                           df['pickup_longitude'].values, \n                                                           df['dropoff_latitude'].values, \n                                                           df['dropoff_longitude'].values)\ndf.loc[:,'manhtn_pick_drop'] = np.round(df.loc[:,'manhtn_pick_drop'], 3)\n# direction of travel\ndf.loc[:,'bearing'] = bearing_array(df['pickup_latitude'].values, \n                                          df['pickup_longitude'].values, \n                                          df['dropoff_latitude'].values, \n                                          df['dropoff_longitude'].values)","bdd9cfbe":"# different speed based on distance measures\n# 1 m\/s = 3.6 km\/hr \ndf.loc[:, 'avg_speed_h'] = 3.6 * 1000 * df['hvsine_pick_drop'] \/ (df['trip_duration'])\ndf.loc[:, 'avg_speed_m'] = 3.6 * 1000 * df['manhtn_pick_drop'] \/ (df['trip_duration'])\n\n# round \ndf.loc[:, 'avg_speed_h'] = np.round(df.loc[:, 'avg_speed_h'])\ndf.loc[:, 'avg_speed_m'] = np.round(df.loc[:, 'avg_speed_m'])","6e526498":"## Equador holiday list\n# https:\/\/www.officeholidays.com\/countries\/ecuador\/2017.php\nholidays = ['2016-01-01', '2016-02-08', '2016-02-09', '2016-03-25', '2016-03-27', \n            '2016-05-01', '2016-05-27', '2016-07-24', '2016-08-10', '2016-10-09', \n            '2016-11-02', '2016-11-03', '2016-12-06', '2016-12-25', \n            '2017-01-01', '2017-02-27', '2017-02-28', '2017-04-14', '2017-04-16', \n            '2017-05-01', '2017-05-24', '2017-07-24', '2017-08-10', '2017-10-09', \n            '2017-11-02', '2017-11-03', '2017-12-06', '2017-12-25']\nholidays = pd.to_datetime(holidays)","c8e7984f":"# was the day a public holiday?\ndf['holiday'] = 1*(df['pick_date'].isin(holidays))","067e9d5f":"# time related features\ndf.loc[:, 'pick_month'] = df['pickup_datetime'].dt.month\ndf.loc[:, 'hour'] = df['pickup_datetime'].dt.hour\ndf.loc[:, 'week_of_year'] = df['pickup_datetime'].dt.weekofyear\ndf.loc[:, 'day_of_year'] = df['pickup_datetime'].dt.dayofyear\ndf.loc[:, 'day_of_week'] = df['pickup_datetime'].dt.dayofweek\n\n# what more features can be added?\n# hourly temp and rainfall would be nice to have - couldnt find a good datasource :( \n# direction api from google - not free but below is the code","c553cc9b":"# import googlemaps\n# from datetime import datetime\n# import pandas as pd\n\n# gmaps = googlemaps.Client(key='HELLO WORLD ')\n\n# orig_lat = 40.767937;orig_lng = -73.982155\n# dest_lat = 40.765602;dest_lng = -73.964630\n# dept_time = pd.to_datetime('2016-09-17 09:32:00')\n\n# gmaps.distance_matrix((orig_lat,orig_lng),(dest_lat,dest_lng), \n#                       departure_time=dept_time, \n#                       mode='driving')[\"rows\"][0][\"elements\"][0][\"duration\"][\"value\"]\n\n# output: travel time in minutes","fd47a7bd":"# transforming continuous features\ndf['trip_duration_log'] = np.round(np.log1p(df['trip_duration']), 5)\ndf['dist_meters_log'] = np.round(np.log1p(df['dist_meters']), 5)\ndf['hvsine_pick_drop_log'] = np.round(np.log1p(df['hvsine_pick_drop']), 5)\ndf['manhtn_pick_drop_log'] = np.round(np.log1p(df['manhtn_pick_drop']), 5)","c7eb7002":"display(df.shape[1])\ndisplay(df.columns)","08022159":"# in past runs coordinates had very high feature importance  \n# and training error was 3 times lower than test error\n# ie- model is memorising\n# binning it to 4 decimals (neighbourhood) from 6\ndf['pickup_longitude'] = np.round(df['pickup_longitude'], 4)\ndf['pickup_latitude'] = np.round(df['pickup_latitude'], 4)\ndf['dropoff_longitude'] = np.round(df['dropoff_longitude'], 4)\ndf['dropoff_latitude'] = np.round(df['dropoff_latitude'], 4)","035ce1a4":"from sklearn.model_selection import train_test_split\n# final feature list\ncols = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude',\n        'trip_duration', 'dist_meters', 'avg_speed', 'quito', \n        'hvsine_pick_drop', 'manhtn_pick_drop', 'bearing',\n        'avg_speed_h', 'avg_speed_m', 'holiday', 'pick_month', 'hour',\n        'week_of_year', 'day_of_year', 'day_of_week', 'trip_duration_log',\n        'dist_meters_log', 'hvsine_pick_drop_log', 'manhtn_pick_drop_log', 'pick_date']\nX = df[cols].copy()\ny = np.log1p(df['wait_sec'].copy())\n\n# create training and testing vars\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=16)\n\nprint('Dataset size\\nTraining:', X_train.shape[0], '\\nTesting:', X_test.shape[0])","f86c6c0b":"fig = figure(num=None, figsize=(12, 8), dpi=80, facecolor='w', edgecolor='k')\n\nplt.plot(X_train.groupby('pick_date').size(), '-', label='train')\nplt.plot(X_test.groupby('pick_date').size(), '-', label='test')\nplt.title('Train and test period complete overlap.')\nplt.legend(loc=0)\nplt.ylabel('No. of trips', weight = 'bold')\nplt.xlabel('Pickup date', weight = 'bold')\nplt.yscale('log')\nsns.despine()\nplt.tight_layout();\n\ndel X_train['pick_date']\ndel X_test['pick_date']","2c4abb2a":"fig = figure(num=None, figsize=(12, 4), dpi=80, facecolor='w', edgecolor='k')\n\nsns.distplot(y_train.values, label='train',)\n             #kde_kws=dict(cumulative=True))\nsns.distplot(y_test.values, label='test',)\n             #kde_kws=dict(cumulative=True))\nplt.title('Similar distribution of waiting time')\nplt.legend(loc=0)\nplt.ylabel('density', weight = 'bold')\nplt.xlabel('log(seconds+1)', weight = 'bold')\nsns.despine()\nplt.tight_layout();","0e024bb6":"fig = figure(num=None, figsize=(12, 4), dpi=80, facecolor='w', edgecolor='k')\n\nplt.subplot(2, 2, 1)\nax1 = sns.distplot(X_train.pickup_longitude, rug=True, kde=True) \nax1.set_xlim([-80, -77])\nax1.set_ylabel('pickup', weight = 'bold')\nax1.set_xlabel('')\nax1.set_yticks([])\nax1.set_xticks([])\n\nplt.subplot(2, 2, 2)\nax2 = sns.distplot(X_train.pickup_latitude, rug=True, kde=True) \nax2.set_xlim([-4, 1])\nax2.set_xlabel('')\nax2.set_yticks([])\nax2.set_xticks([]) \n\nplt.subplot(2, 2, 3)\nax3 = sns.distplot(X_train.dropoff_longitude, rug=True, kde=True) \nax3.set_xlim([-80, -77])\nax3.set_ylabel('dropoff', weight = 'bold')\nax3.set_xlabel('longitude', weight = 'bold')\nax3.set_yticks([])\n\nplt.subplot(2, 2, 4)\nax4 = sns.distplot(X_train.dropoff_latitude, rug=True, kde=True) \nax4.set_xlim([-4, 1])\nax4.set_xlabel('latitude', weight = 'bold')\nax4.set_yticks([]) \n\nsns.despine()\nplt.tight_layout();","4e8c1232":"import datashader as ds\nimport datashader.transfer_functions as tf\nfrom colorcet import fire\n\ncvs = ds.Canvas(plot_width=2*150, plot_height=4*150,\n               x_range=(-78.6,-78.4), y_range=(-0.4,0.0))\n\nagg = cvs.points(X_train, 'pickup_longitude', 'pickup_latitude', ds.count('trip_duration'))\npickup = tf.set_background(tf.shade(agg, cmap=fire),\"black\", name=\"Pickup\")\n\nagg = cvs.points(X_train, 'dropoff_longitude', 'dropoff_latitude', ds.count('trip_duration'))\ndropoff = tf.set_background(tf.shade(agg, cmap=fire),\"black\", name=\"Dropoff\")\n\ntf.Images(pickup, dropoff)","3914b9f8":"fig = figure(num=None, figsize=(12, 6), dpi=80, facecolor='w', edgecolor='k')\nflierprops = dict(markerfacecolor='0.75', markersize=5, linestyle='none')\n\nplt.subplot(2, 1, 1)\nax1 = sns.boxplot(x=X_train['trip_duration']\/60, flierprops=flierprops)\nax1.set_xlabel('', weight = 'bold')\nax1.set_ylabel('trip duration', weight = 'bold')\n\nplt.subplot(2, 1, 2)\nax2 = sns.boxplot(x=np.expm1(y_train.values)\/60, flierprops=flierprops)\nax2.set_xlabel('minutes', weight = 'bold')\nax2.set_ylabel('waiting time', weight = 'bold')\n\nsns.despine()\nplt.tight_layout();","12e31452":"fig = figure(num=None, figsize=(12, 6), dpi=80, facecolor='w', edgecolor='k')\nflierprops = dict(markerfacecolor='0.75', markersize=5, linestyle='none')\n\nplt.subplot(3, 2, 1)\nax1 = sns.boxplot(x=X_train['dist_meters']\/1000, flierprops=flierprops)\nax1.set_ylabel('true', weight = 'bold')\nax1.set_xlabel('', weight = 'bold')\n\nplt.subplot(3, 2, 2)\nax2 = sns.boxplot(x='avg_speed', data=X_train, flierprops=flierprops)\nax2.set_ylabel('', weight = 'bold')\nax2.set_xlabel('', weight = 'bold')\n\nplt.subplot(3, 2, 3)\nax3 = sns.boxplot(x=X_train['hvsine_pick_drop'], flierprops=flierprops)\nax3.set_ylabel('haversine', weight = 'bold')\nax3.set_xlabel('', weight = 'bold')\n\nplt.subplot(3, 2, 4)\nax4 = sns.boxplot(x=X_train['avg_speed_h'], flierprops=flierprops)\nax4.set_ylabel('', weight = 'bold')\nax4.set_xlabel('', weight = 'bold')\n\nplt.subplot(3, 2, 5)\nax5 = sns.boxplot(x=X_train['manhtn_pick_drop'], flierprops=flierprops)\nax5.set_ylabel('manhattan', weight = 'bold')\nax5.set_xlabel('distance (km)', weight = 'bold')\n\nplt.subplot(3, 2, 6)\nax6 = sns.boxplot(x='avg_speed_m', data=X_train, flierprops=flierprops)\nax6.set_ylabel('', weight = 'bold')\nax6.set_xlabel('speed (km\/hr)', weight = 'bold')\n\nsns.despine()\nplt.tight_layout();","56b21f6b":"from sklearn import linear_model\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\n\nreg = linear_model.LinearRegression()\ncv = ShuffleSplit(n_splits=4, test_size=0.3, random_state=0)\ncross_val_score(reg, X_train, y_train, cv=cv)","adf0c3ff":"reg.fit(X_train,y_train)","37a28fc4":"# Predict on testing and training set\nlm_y_pred = reg.predict(X_test)\n\n# Report testing RMSE\nprint(np.sqrt(mean_squared_error(y_test, lm_y_pred)))","406c6636":"import xgboost as xgb\nfrom lightgbm import LGBMRegressor\nimport lightgbm as lgb\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.metrics import mean_squared_error","2202c544":"dtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test)","c2816ef5":"def xgb_evaluate(max_depth, gamma,min_child_weight,max_delta_step,subsample,colsample_bytree):\n    params = {'eval_metric': 'rmse',\n                  'max_depth': int(max_depth),\n                  'subsample': subsample,\n                  'eta': 0.1,\n                  'gamma': gamma,\n                  'colsample_bytree': colsample_bytree,   \n                  'min_child_weight': min_child_weight ,\n                  'max_delta_step':max_delta_step\n                 }\n    # Used around 1000 boosting rounds in the full model\n    cv_result = xgb.cv(params, dtrain, num_boost_round=100, nfold=3)    \n        \n    # Bayesian optimization only knows how to maximize, not minimize, so return the negative RMSE\n    return -1.0 * cv_result['test-rmse-mean'].iloc[-1]","20d6e80a":"%%time\nxgb_bo = BayesianOptimization(xgb_evaluate, {\n    'max_depth': (2, 12),\n    'gamma': (0.001, 10.0),\n    'min_child_weight': (0, 20),\n    'max_delta_step': (0, 10),\n    'subsample': (0.4, 1.0),\n    'colsample_bytree' :(0.4, 1.0)})\n\n# Use the expected improvement acquisition function to handle negative numbers\n# Optimally needs quite a few more initiation points and number of iterations\nxgb_bo.maximize(init_points=3, n_iter=5, acq='ei')","3c99bfa1":"params = xgb_bo.res['max']['max_params']\nprint(params)","287f95fa":"params['max_depth'] = int(params['max_depth'])","907d5a0a":"# Train a new model with the best parameters from the search\nmodel2 = xgb.train(params, dtrain, num_boost_round=250)\n\n# Predict on testing and training set\ny_pred = model2.predict(dtest)\ny_train_pred = model2.predict(dtrain)","2ff6c7cb":"# Report testing and training RMSE\nprint('Test error:', np.sqrt(mean_squared_error(y_test, y_pred)))\nprint('Train error:', np.sqrt(mean_squared_error(y_train, y_train_pred)))\n\n# lm 0.6795364412436091\n# better than lm ","53c9ea0e":"sns.distplot(y_pred, label='y_hat')\nsns.distplot(y_test, label='y')\nplt.legend()\nsns.despine()\nplt.tight_layout();","a169c9cb":"# actual vs predicted \nsns.jointplot(y_test, y_pred, color=\"k\", space=0, kind = 'kde') #\nsns.despine()\nplt.tight_layout();","090bd322":"wait_sec_train = np.expm1(y_train).round()\nnp.quantile(wait_sec_train, [0, 0.05, 0.5, 0.95, 0.99])","f2ef855d":"wait_sec_true = np.expm1(y_test).round()\nwait_sec_predicted = np.expm1(y_pred).round()\n# clip very high values\nwait_sec_predicted = np.clip(wait_sec_predicted, 0, np.quantile(wait_sec_train, 0.95))\n\nresidual = wait_sec_predicted - wait_sec_true\n\nfig = figure(num=None, figsize=(12, 4), dpi=80, facecolor='w', edgecolor='k')\n\nplt.subplot(1, 2, 1)\nax1 = sns.distplot(residual)\nax1.set_title('Distribution of residuals\\n')\nax1.set_xlabel('')\n\nplt.subplot(1, 2, 2)\nax2 = sns.boxplot(residual, showfliers=False)\nax2.set_xlabel('')\n\nsns.despine()\nplt.tight_layout()","b2666761":"print(\"Mean absolute error:\", np.abs(residual).mean())\nprint(\"Median absolute error:\", np.abs(residual).median())","94c91df9":"# feature importance\nfig =  plt.figure(figsize = (12,8))\naxes = fig.add_subplot(111)\nxgb.plot_importance(model2,ax = axes,height =0.5)\nsns.despine()\nplt.tight_layout()","0f9fe7cc":"Let's check the data files! According the data description we should find the following columns:\n\n#### File descriptions\n31348 taxi, Cabify and Uber routes in Quito Ecuador\n\n#### Data fields\n* `id` - a unique identifier for each trip\n* `vendor_id` - the type of fare entered by the user (taxi, Uber, Cabify)\n* `pickup_datetime` - date and time when the meter was engaged\n* `dropoff_datetime` - date and time when the meter was disengaged\n* `pickup_longitude` - the longitude where the meter was engaged\n* `pickup_latitude` - the latitude where the meter was engaged\n* `dropoff_longitude` - the longitude where the meter was disengaged\n* `dropoff_latitude` - the latitude where the meter was disengaged\n* `store_and_fwd_flag` - Set always to N, matching the https:\/\/www.kaggle.com\/c\/nyc-taxi-trip-duration\/data dataset. \n* `trip_duration` - duration of the trip in seconds\n* `dist_meters` - the distance of the trip in meters\n* `wait_sec` - the time the car was completely stopped during the trip or waiting time in seconds","e7db4515":"# Data understanding","f6f67e07":"Let\u2019s assume you are a passenger in the city of **Quito, Ecuador** and you would like to know **how long you will be stuck in traffic jam**.\n\n# Load libraries","6ada7151":"## Bayesian Optimization with XGBoost","03d85633":"# Data Visualization","ba1a5bbb":"# Train\/Test split","5c6771dd":"Extract the parameters of the best model.","1384f93e":"# Feature engineering","3e336286":"# Modelling\n\n## Baseline linear regression","67df9b45":"# Evaluation of results"}}