{"cell_type":{"a0cac097":"code","33c6ea9d":"code","4b475c69":"code","a41edfcd":"code","34c4ce4a":"code","8334423b":"code","0028c027":"code","a9d8ccfd":"code","88ce60a5":"code","ed8bcd1b":"code","0ca3e4eb":"code","0ae67218":"code","88bfefe5":"code","580089a5":"code","71d4f2ca":"code","19c54430":"code","2f4d9969":"code","6b42301a":"code","0ac277ef":"code","27b0d2de":"code","91ea8ac0":"code","ee6077aa":"code","bbb80dce":"code","41d6c3e9":"code","1858403e":"code","7d71a6f0":"code","019a3a72":"code","c9d214e3":"markdown","1b0e0a48":"markdown","704d1105":"markdown","91c2f0fb":"markdown","63dee6a7":"markdown","1e50b88a":"markdown","bbb012c5":"markdown","7fdb4206":"markdown","061b2cd4":"markdown","e84e51c3":"markdown","1d2aa952":"markdown","6104dd9f":"markdown"},"source":{"a0cac097":"!pip install xmltodict --upgrade  # this will install xmltodict library\n\n# importing other libraries\nimport cv2\nimport os\nimport random\nimport xmltodict\nfrom PIL import Image\nimport numpy as np\nimport collections\nimport matplotlib.pyplot as plt\nimport lxml.etree\n\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.layers import AveragePooling2D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications.mobilenet_v2 import preprocess_input\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom tensorflow.keras.models import load_model\nimport seaborn as sns\nfrom sklearn.utils import shuffle","33c6ea9d":"# this will print a sample image from our dataset\nimg =cv2.imread(\"..\/input\/face-mask-detection\/images\/maksssksksss244.png\")   #reading the image \nout_img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)   #converting it from RGB to BGR\n\nplt.figure(figsize=(12,12))    #specifying the size of image to be printed\nplt.imshow(out_img)     # ptinting the image","4b475c69":"#printing the annotations file of the above image\ntree = lxml.etree.parse(\"..\/input\/face-mask-detection\/annotations\/maksssksksss244.xml\")\npretty = lxml.etree.tostring(tree, encoding=\"unicode\", pretty_print=True)\nprint(pretty)","a41edfcd":"# this function helps to extract the coordinates of the rectangles that bound the faces in these images\nlabels=[]\ndef face_exctraction(directory_load_image,directory_load_notation):\n    \n    \n   #opening the annotations file in readable format so that useful data can be extracted from it\n        with open(os.path.join(directory_load_notation,\"maksssksksss244.xml\" ), 'r') as f:\n            data = f.read()\n\n\n            #getting data\n            ann_dict=xmltodict.parse(data)[\"annotation\"]\n            object_list = ann_dict[\"object\"]\n            file=\"maksssksksss244.xml\"\n\n\n            #checking if there are more than 1 face in this image\n            if isinstance(object_list, list): #if yes: itrating through them \n                 for object_ in object_list:\n                        #getting face boundaries\n                        xmin = int(object_[\"bndbox\"][\"xmin\"])\n                        xmax = int(object_[\"bndbox\"][\"xmax\"])\n                        ymin = int(object_[\"bndbox\"][\"ymin\"])\n                        ymax = int(object_[\"bndbox\"][\"ymax\"])\n                        \n                        cv2.rectangle(out_img,(xmin,ymin),(xmax,ymax),(175,200,150),2)#printing a rectangle around the face\n                        labels.append(object_[\"name\"])\n        #printing the  new image\n        \n        plt.figure(figsize=(12,12))\n        plt.imshow(out_img)   \n        return labels\n# executing the function  \nlabels=face_exctraction(\"..\/input\/face-mask-detection\/images\/\",\"..\/input\/face-mask-detection\/annotations\/\")\n\nfor labels in labels:\n    print(labels)","34c4ce4a":"def face_exctraction(directory_load_image,directory_load_notation):\n    labels = []\n    \n    \n  \n    \n    for file in os.listdir(directory_load_notation):\n        with open(os.path.join(directory_load_notation, file), 'r') as f:\n            data = f.read()\n\n\n            #getting data\n            ann_dict=xmltodict.parse(data)[\"annotation\"]\n            object_list = ann_dict[\"object\"]\n\n\n        \n\n            #checking if there are more than 1 face in this image\n            if isinstance(object_list, list): #if yes: itrating through them \n                 for object_ in object_list:\n                        #getting face boundaries\n                        xmin = int(object_[\"bndbox\"][\"xmin\"])\n                        xmax = int(object_[\"bndbox\"][\"xmax\"])\n                        ymin = int(object_[\"bndbox\"][\"ymin\"])\n                        ymax = int(object_[\"bndbox\"][\"ymax\"])\n                        \n                        labels.append(object_[\"name\"])\n                        \n                           \n\n                       \n            else:\n                #same thing on images with one face in them\n                object_ = object_list\n                xmin = int(object_[\"bndbox\"][\"xmin\"])\n                xmax = int(object_[\"bndbox\"][\"xmax\"])\n                ymin = int(object_[\"bndbox\"][\"ymin\"])\n                ymax = int(object_[\"bndbox\"][\"ymax\"])\n\n                labels.append(object_[\"name\"])\n               \n                \n    labels = np.asarray(labels)\n    \n    return labels\nlabels = face_exctraction(\"..\/input\/face-mask-detection\/images\/\",\"..\/input\/face-mask-detection\/annotations\/\")\nprint(labels.shape)\nprint(collections.Counter(labels))\n","8334423b":"# this is a similar function \n#it helps to extract the cropped images of the faces of people in the images\n# we will later pass these cropped images to the model for training it\ndef face_exctraction(directory_load_image,directory_load_notation):\n        faces=[] #this will store the cropped images\n        labels=[]\n   \n        with open(os.path.join(directory_load_notation,\"maksssksksss244.xml\" ), 'r') as f:\n            data = f.read()\n\n\n            #getting data\n            ann_dict=xmltodict.parse(data)[\"annotation\"]\n            object_list = ann_dict[\"object\"]\n            file=\"maksssksksss244.xml\"\n\n            #opening image\n            image_name = file.replace(\"xml\",\"png\")\n            im = Image.open(directory_load_image+image_name)\n\n            #checking if there are more than 1 face in this image\n            if isinstance(object_list, list): #if yes: itrating through them \n                 for object_ in object_list:\n                        #getting face boundaries\n                        xmin = int(object_[\"bndbox\"][\"xmin\"])\n                        xmax = int(object_[\"bndbox\"][\"xmax\"])\n                        ymin = int(object_[\"bndbox\"][\"ymin\"])\n                        ymax = int(object_[\"bndbox\"][\"ymax\"])\n                        crop_rectangle = (xmin, ymin, xmax , ymax) \n                        cropped_im = im.crop(crop_rectangle)  # cropping faces from the given image\n                        faces.append(cropped_im)              # saving cropped images in the list\n                        labels.append(object_[\"name\"])\n        return faces,labels\n# executing the function      \nfaces,labels=face_exctraction(\"..\/input\/face-mask-detection\/images\/\",\"..\/input\/face-mask-detection\/annotations\/\")\n\nprint(plt.imshow(faces[1]))   #printing the images\nprint(labels[1])  ","0028c027":"# this function saves the cropped images of faces in two folders(train_images and test_images)\n# it also saves the labels(\"with_mask\" or \"without_mask\") of these cropped images in two lists y_train and y_test\n# we are splitting our data in to train and test. one for training our model and other for testing our model\n# it will also resize our images to (100,100) as all images are of different sizes so to maintain uniformity in our data it will resize the images\ndef face_exctraction(directory_load_image,directory_load_notation,directory_save_train,directory_save_test):\n    y_train = []\n    y_test = []\n    i=1\n    \n    if not os.path.exists(\".\/\"+directory_save_train):\n        os.makedirs(\".\/\"+directory_save_train)\n    if not os.path.exists(\".\/\"+directory_save_test):\n        os.makedirs('.\/'+directory_save_test)\n    \n    for file in os.listdir(directory_load_notation):\n        with open(os.path.join(directory_load_notation, file), 'r') as f:\n            data = f.read()\n\n\n            #getting data\n            ann_dict=xmltodict.parse(data)[\"annotation\"]\n            object_list = ann_dict[\"object\"]\n\n\n            #opening image\n            image_name = file.replace(\"xml\",\"png\")\n            im = Image.open(directory_load_image+image_name)\n\n            #checking if there are more than 1 face in this image\n            if isinstance(object_list, list): #if yes: itrating through them \n                 for object_ in object_list:\n                        #getting face boundaries\n                        xmin = int(object_[\"bndbox\"][\"xmin\"])\n                        xmax = int(object_[\"bndbox\"][\"xmax\"])\n                        ymin = int(object_[\"bndbox\"][\"ymin\"])\n                        ymax = int(object_[\"bndbox\"][\"ymax\"])\n                        \n                        #cropping the face and saving it\n                        crop_rectangle = (xmin, ymin, xmax , ymax)\n                        cropped_im = im.crop(crop_rectangle)\n                        resized_im = cropped_im.resize((100,100), Image.ANTIALIAS)\n                        \n                        #this part is for sperating train set from test set\n                        if i<3600:\n                            resized_im.save(directory_save_train+str(i)+\".png\")\n                            y_train.append(object_[\"name\"])\n                        else:\n                            resized_im.save(directory_save_test+str(i-3599)+\".png\")\n                            y_test.append(object_[\"name\"])\n\n                        i+=1\n            else:\n                #same thing on images with one face in them\n                object_ = object_list\n                xmin = int(object_[\"bndbox\"][\"xmin\"])\n                xmax = int(object_[\"bndbox\"][\"xmax\"])\n                ymin = int(object_[\"bndbox\"][\"ymin\"])\n                ymax = int(object_[\"bndbox\"][\"ymax\"])\n\n                crop_rectangle = (xmin, ymin, xmax , ymax)\n                cropped_im = im.crop(crop_rectangle)\n                resized_im = cropped_im.resize((100, 100), Image.ANTIALIAS)\n                if i<3600:\n                    resized_im.save(directory_save_train+str(i)+\".png\")\n                    y_train.append(object_[\"name\"])\n                else:\n                    resized_im.save(directory_save_test+str(i-3599)+\".png\")\n                    y_test.append(object_[\"name\"])\n\n                i+=1\n    y_train = np.asarray(y_train)\n    y_test = np.asarray(y_test)\n    return y_train,y_test\n","a9d8ccfd":"# executing the previous function\ny_train,y_test = face_exctraction(\"..\/input\/face-mask-detection\/images\/\",\"..\/input\/face-mask-detection\/annotations\/\",\"train_images\/\",\"test_images\/\")","88ce60a5":"# it will count the number of items in our list\nprint(collections.Counter(y_train))\nprint(collections.Counter(y_test))","ed8bcd1b":"# we can not pass images directly to our model so here we will convert our images into  numpy arrays\ndef images_to_array(directory,num_img):\n    X_nn=[]\n    #iterating through directory and turning 100x100 images to an array with shape of (100,100,3) and stacking them inside a dataset.\n    for i in range(1,num_img+1):\n        im = Image.open(directory+str(i)+\".png\")\n        rgb_im = im.convert('RGB')\n        im = np.asarray(rgb_im,dtype=\"int\")\n        X_nn.append(im)\n    return np.asarray(X_nn)","0ca3e4eb":"#executing the previous function\nX_train = images_to_array(\".\/train_images\/\",3599)\nX_test = images_to_array(\".\/test_images\/\",473)\n#printing the shape of our saved data\nX_train.shape\n#it will print (3599, 100, 100, 3)\n#here 3599 is the number of images in X_train\n# 100x100 is the size of image\n# 3 is the number of colour channels (RGB - RED GREEN BLUE)","0ae67218":"# so now we have arrays of images and their labels\n# example\nprint(\"label= \",y_train[0]) # this prints the label of first image\nprint()\nprint(\"Array = \",X_train[0]) #this prints the numpy array of first image","88bfefe5":"\n# it will generate new images from the provided images to increase the  amount of traning data  \ndef add_data_aug (directory_save_inc , directory_save_no , num_inc , num_no , X_train , y_train):\n    \n    if not os.path.exists(\".\/\"+directory_save_inc):\n        os.makedirs(\".\/\"+directory_save_inc)\n    if not os.path.exists(\".\/\"+directory_save_no):\n        os.makedirs('.\/'+directory_save_no)\n    \n    \n    #the image generator ranges.\n    datagen = ImageDataGenerator(\n        rotation_range=40,\n        width_shift_range = 0.2,\n        height_shift_range = 0.2,\n        shear_range = 0.2,\n        zoom_range = 0.2,\n        horizontal_flip = True,\n        fill_mode = \"nearest\"\n    )\n    \n    #we are trying to upsmaple \"without_mask\" images and \"mask_weared_incorrect\" so we select them\n    without_mask = X_train[y_train==\"without_mask\"]\n    mask_weared_incorrect = X_train[y_train==\"mask_weared_incorrect\"]\n    \n    #creating the desired number of images\n    j=0\n    for batch in datagen.flow(without_mask,batch_size=1,save_to_dir=directory_save_no,save_prefix=\"wo\",save_format='png'):\n        j+=1\n        #if j> 2175:\n        if j>num_no:\n            break\n    j=0\n    for batch in datagen.flow(mask_weared_incorrect,batch_size=1,save_to_dir=directory_save_inc,save_prefix=\"wi\",save_format='png'):\n        j+=1\n        #if j> 2722:\n        if j>num_inc:\n            break\n    X_nn=[]\n    \n    #adding newly made images to trainign set\n    for file in os.listdir(directory_save_no):\n        im = Image.open(os.path.join(directory_save_no, file))\n        rgb_im = im.convert('RGB')\n        im = np.asarray(rgb_im,dtype=\"int\")\n        X_nn.append(im)\n        y_train=np.append(y_train,'without_mask')\n    \n    \n    for file in os.listdir(directory_save_inc):\n        im = Image.open(os.path.join(directory_save_inc, file))\n        rgb_im = im.convert('RGB')\n        im = np.asarray(rgb_im,dtype=\"int\")\n        X_nn.append(im)\n        y_train = np.append(y_train,\"mask_weared_incorrect\")\n        \n    X_train_augmented = np.asarray(X_nn)\n    \n    X_train = np.concatenate((X_train,X_train_augmented  ))\n    \n    X_train, y_train = shuffle(X_train, y_train, random_state=0)\n    \n    return X_train,y_train","580089a5":"#executing the previous function\nX_train,y_train = add_data_aug (\"Augmented_incorrect\/\" , \"Augmented_without\/\", 2722 , 2175 , X_train , y_train)","71d4f2ca":"#see the increased number of images\nprint(collections.Counter(y_train))\nprint(y_train.shape)","19c54430":"def preproccesing(X_train,X_test,y_train,y_test):\n    #nomalizing color values\n    X_train = X_train \/ 255\n    X_test = X_test \/ 255\n    \n    #categorical to numerical values for labels.\n    conditions = [\n                    (y_train == \"without_mask\"),\n                    y_train == (\"mask_weared_incorrect\"),\n                    (y_train == \"with_mask\")]\n    choices = [0, 1, 2]\n    y_train = np.select(conditions, choices)\n    conditions = [\n                    (y_test == \"without_mask\"),\n                    y_test == (\"mask_weared_incorrect\"),\n                    (y_test == \"with_mask\")]\n    choices = [0, 1, 2]\n    y_test = np.select(conditions, choices)\n    return X_train,X_test,y_train,y_test","2f4d9969":"#executing the function\nX_train,X_test,y_train,y_test = preproccesing(X_train,X_test,y_train,y_test)","6b42301a":"collections.Counter(y_train)","0ac277ef":"baseModel=MobileNetV2(weights='imagenet',include_top=False,input_tensor=Input(shape=(100,100,3)))\n#we will remove the top layer of the model \nprint(baseModel.summary())\n","27b0d2de":"headModel=baseModel.output #taking the output of base model\nheadModel=AveragePooling2D(pool_size=(4,4))(headModel) #doing average pooling of 2D 7x7 matrix\nheadModel=Flatten(name='Flatten')(headModel) #flattening the layers\nheadModel=Dense(128,activation='relu')(headModel) # Adding 128 dense layers\nheadModel=Dropout(0.7)(headModel) # adding droupout layers to prevent overfitting\nheadModel=Dense(3,activation='softmax')(headModel) #last dense layers for our 3 classes\n\nmodel=Model(inputs=baseModel.input,outputs=headModel) ","91ea8ac0":"# making the basemodel layers non trainable\nfor layer in baseModel.layers:\n    layer.trainable=False\n\n\nprint(model.summary())","ee6077aa":"#adding checkpoints so that we can save the best model from all\ncheckpoint = ModelCheckpoint('best_model.h5',  # model filename\n                             monitor='val_loss', # quantity to monitor\n                             verbose=0, # verbosity - 0 or 1\n                             save_best_only= True, # The latest best model will not be overwritten\n                             mode='auto') # The decision to overwrite model is made \n                                          # automatically depending on the quantity to monitor","bbb80dce":"learning_rate=0.0001\nEpochs=50\nBS=30\n\nopt=Adam(lr=learning_rate,decay=learning_rate\/Epochs)\n\n#compiling the model\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer=opt,metrics=['accuracy'])","41d6c3e9":"#starting the training of model\nH=model.fit (\n   X_train,y_train,batch_size=BS,\n    steps_per_epoch=len(X_train)\/\/BS,\n    validation_data=(X_test,y_test),\n    validation_steps=len(X_test)\/\/BS,\n    epochs=Epochs,\n    callbacks = [checkpoint],\n    verbose=1\n)","1858403e":"#plotting the accuracy and loss of the model\nN = Epochs\nplt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\nplt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\nplt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\nplt.title(\"Training Loss and Accuracy\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss\/Accuracy\")\nplt.legend(loc=\"lower left\")\nplt.savefig('a.png')","7d71a6f0":"def evaluatuion (model,X_test,y_test):\n    #getting models predcitons on test set\n    y_pred = []\n    y_prob = model.predict(X_test)\n    for i in y_prob:\n        y_pred.append(np.argmax(i))\n        \n                \n    confusion_matrix_array = confusion_matrix(y_test,y_pred)\n    print(\"simple confusion matrix\")\n    print(confusion_matrix_array)\n    print(\"Report:\")\n    print(classification_report(y_test, y_pred,\n\ttarget_names=[\"without mask\",\"mask weared incorrectly\",\"with mask\"]))\n    \n    \n    print(\"Confusion Matrix:\")\n    confusion_matrix_array = confusion_matrix(y_test,y_pred)\n    \n    g= sns.heatmap(confusion_matrix_array, annot=True,yticklabels=[\"Act No Mask\",\"Act mask weared incorrectly\",\"Act with mask\"],xticklabels=[\"Pred No Mask\",\"Pred mask weared incorrectly\",\"Pred with mask\"])\n    g.set_yticklabels(g.get_yticklabels(), rotation = 0)\n    \n\n    \n\n    \n    return y_pred\n    ","019a3a72":"model=load_model(\"best_model.h5\")\ny_pred = evaluatuion (model,X_test,y_test,)","c9d214e3":"**Downloading pre trained model**","1b0e0a48":"# Exploratory Data Analysis\n\nLet us see what data our dataset has\n\n\n\n","704d1105":"**Evaluating the model using classification report and confusion matrix**","91c2f0fb":"# Model","63dee6a7":"**Adding checkpoints to save best model**","1e50b88a":"**Compiling the model**","bbb012c5":"**Plotting the accuracy\/loss vs epochs graph**","7fdb4206":"# Face mask detection\n","061b2cd4":"****So now we have the two training and 2 testing arrays of images and labels****","e84e51c3":"**Adding our own classification layers**","1d2aa952":"**Generating new images**","6104dd9f":"# Data pre-processing\n"}}