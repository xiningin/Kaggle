{"cell_type":{"be2c2fc6":"code","c71b476b":"code","d2035589":"code","87754e13":"code","49855a81":"code","41ff2470":"code","f487c7f4":"code","a320d9f4":"code","54d57419":"code","192b1e52":"code","df197091":"code","0c08747d":"code","0e2b242f":"code","12c22a5d":"code","f84a39e9":"code","7948a3ca":"markdown","ec342899":"markdown","8b73a3e9":"markdown","09351c8c":"markdown","59286507":"markdown","edc4f64a":"markdown","73482d99":"markdown","f55ca166":"markdown","252fb853":"markdown","a8ac8b41":"markdown","e4511c18":"markdown","26b59d4b":"markdown","7a486602":"markdown","28aff930":"markdown","82db04c9":"markdown","056e40f3":"markdown","03c85cc4":"markdown","0af6cf34":"markdown","55f6e4a1":"markdown","8a771d68":"markdown","61169197":"markdown"},"source":{"be2c2fc6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport soundfile as sf\nimport librosa.feature\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport IPython.display as ipd\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c71b476b":"sound,samplerate=librosa.load(\"..\/input\/waka-waka\/Shakira - Waka Waka (This Time for Africa) (The Official 2010 FIFA World Cup Song).wav\")","d2035589":"plt.plot(sound)","87754e13":"np.shape(sound)","49855a81":"window=np.array_split(sound,np.shape(sound)[0]\/\/samplerate)","41ff2470":"features = np.zeros(14)\nfor segundo in window:\n    flatness = librosa.feature.spectral_flatness(y=segundo)\n    rms = librosa.feature.rms(segundo)\n    roll = librosa.feature.spectral_rolloff(segundo)\n    mfcc = librosa.feature.mfcc(y=segundo,sr=samplerate,n_mfcc=5)\n    mfcc = mfcc[1:,:]\n    f = np.hstack([flatness.mean(axis=1), flatness.std(axis=1), rms.mean(axis=1), rms.std(axis=1), roll.mean(axis=1), roll.std(axis=1), mfcc.mean(axis=1), mfcc.std(axis=1)])\n    features = np.vstack([features,  f]) \nfeatures = features[1:,:]\n\nprint(features.shape)\n","f487c7f4":"from sklearn.preprocessing import StandardScaler\nscl = StandardScaler()\nscl.fit(features)\nX = scl.transform(features)","a320d9f4":"from sklearn.datasets import make_blobs\nX_blob, labels_blob = make_blobs(n_samples=500, n_features=2, centers=6, random_state=42)  # Como temos aprendizado n\u00e3o supervisionado, n\u00e3o obteremos as labels\nsns.scatterplot(x=X_blob[:,0],y=X_blob[:,1], hue=labels_blob,legend='full')\nplt.title(\"Podemos ver que, embora 6 seja o n\u00famero de centros pr\u00e9-definidos, 4 e 5 tamb\u00e9m s\u00e3o op\u00e7\u00f5es\")\nplt.show()\n","54d57419":"from sklearn.cluster import KMeans\ninertia = []\nclusters = range(2,15)\nfor n_c in clusters:\n    print(n_c, end=\" \")\n    model = KMeans(n_clusters=n_c, random_state=42)\n    model.fit(X_blob)\n    inertia.append(model.inertia_)\n\nplt.plot(clusters, inertia)","192b1e52":"from sklearn.cluster import KMeans\ninertia = []\nclusters = range(2,15)\nfor n_c in clusters:\n    print(n_c, end=\" \")\n    model = KMeans(n_clusters=n_c, random_state=42)\n    model.fit(X)\n    inertia.append(model.inertia_)\n\nplt.plot(clusters, inertia)","df197091":"n_clusters=8\nmodel = KMeans(n_clusters=n_clusters, random_state=42) # criando um modelo que vai gerar 4 clusters\nmodel.fit(X) # Treinando o modelo ","0c08747d":"plt.plot(model.predict(X))","0e2b242f":"clusters = model.transform(X)\nP = (1\/(10**-6 + clusters)) \/ np.sum(1\/(10**-6 + clusters), axis=1, keepdims=1)\ndef transition_matrix(ndim, p_stay):\n  T = np.ones ( (ndim, ndim)) * ((1-p_stay)\/(ndim-1))\n  T *= 1-np.eye(ndim)\n  T += np.eye(ndim)*p_stay\n  return T\nT = transition_matrix(n_clusters, .9)\nstates = librosa.sequence.viterbi(P.T, T)\nplt.plot(states)","12c22a5d":"changes = np.where(states[:-1] != states[1:])[0]\nprint('\\n'.join([f'{i[0]:02}:{i[1]:02}' for i in zip(changes\/\/60,changes%60)]))","f84a39e9":"ipd.Audio(sound,rate=samplerate)","7948a3ca":"## Obtendo features\nJ\u00e1 que estamos trabalhando com apenas uma m\u00fasica, n\u00e3o iremos trabalhar apenas com um set de features partindo dela, iremos divid\u00ed-la em janelas de tempo e processar as features de cada uma dessas janelas. A divis\u00e3o como j\u00e1 explicada antes pode ser feita com ou sem overlap, nesse caso por motivos de simplifica\u00e7\u00e3o n\u00e3o vamos utilizar nenhum overlap e vamos dividir o som em janelas de aproximadamente um segundo. Dizemos aproximadamente pois nem toda m\u00fasica acaba divis\u00edvel exatamente pelo seu samplerate, portando n\u00e3o fechando em janelas de um segundo. Podemos dividir em janelas de `TAMANHO_DA_MUSICA\/samplerate` segundos, ou em janelas de um segundo e a \u00faltima janela acabando em menos de um segundo e fazendo um _padding_ dessa janela com valores nulos. ","ec342899":"#### Tarefa 02: Plote o \u00e1udio inteiro obtido pela `sound`","8b73a3e9":"#### Tarefa 05: Agora que temos os dados necess\u00e1rios, fa\u00e7a a normaliza\u00e7\u00e3o deles","09351c8c":"#### Tarefa 06: Aplique o m\u00e9todo do cotovelo no range 2-20 com os dados da m\u00fasica (`X`). Lembre do `random_state=42`.\n","59286507":"#### Tarefa 09: Ou\u00e7a a m\u00fasica averiguando a mudan\u00e7a de estados","edc4f64a":"#### Tarefa 03: Divida a m\u00fasica em janelas de aproximadamente um segundo, pode escolher com qual m\u00e9todo voc\u00ea ir\u00e1 divid\u00ed-la. Guarde todas essas janelas numa lista chamada `window`","73482d99":"A aplica\u00e7\u00e3o do m\u00e9dodo do cotovelo \u00e9 simples e n\u00e3o muito otimizada, a ideia \u00e9:\n```\ncriar modelos que aceitam clusters num range entre a e b\npara cada modelo:\n    treinar modelo com X\n    salvar a inertia de cada modelo treinado (inertia \u00e9 o valor que representa qu\u00e3o bem distribu\u00eddos est\u00e3o os centros)\nplota a inertia pelo n\u00famero de clusters\n```\nA cria\u00e7\u00e3o do modelo tamb\u00e9m pode ser feita dentro do for, como no exemplo abaixo:","f55ca166":"# Aula 05 - Classifica\u00e7\u00e3o n\u00e3o supervisiondada\nNessa aula iremos aplicar t\u00e9cnicas de processamento de som e classifica\u00e7\u00e3o n\u00e3o supervisionada (com o algoritmo de KMeans) para dividir uma m\u00fasica de acordo com os seus estados de forma autom\u00e1tica. A m\u00fasica utilizada no exemplo se chama 'Me and your Mama', do artista Childish Gambino, \u00e9 a primeira do \u00e1lbum 'Awaken My Love' lan\u00e7ado em 2016.","252fb853":"#### Tarefa 08: Salve a predi\u00e7\u00e3o do modelo com a base de dados da m\u00fasica na vari\u00e1vel `labels` e plote essa vari\u00e1vel.","a8ac8b41":"### M\u00e9todo do cotovelo\nO m\u00e9todo do cotovelo (Elbow Method) \u00e9 muito utilizado para achar o n\u00famero de clusters mais adequado para o seu dataset. Nem sempre ele estar\u00e1 certo mas \u00e9 um bom come\u00e7o quando voc\u00ea n\u00e3o tem muita ideia dos clusters que voc\u00ea quer. Vamos mostrara a aplica\u00e7\u00e3o dele com um conjunto de dados do `make_blobs`","e4511c18":"## Predi\u00e7\u00e3o do modelo\nAgora que j\u00e1 temos um modelo certo treinado, para termos a classifica\u00e7\u00e3o de cada ponto da base de dados usamos o `model.predict`, que recebe o argumento `X`, que \u00e9 um array de pontos a serem classificados, como se trata de aprendizado puramente n\u00e3o supervisionado podemos usar os dados de treinamento para ver os resultados do modelo.","26b59d4b":"#### Tarefa 04: Para cada janela, calcule as features: flatness, rms, rolloff, mfcc (com 5 coeficientes) e armazene as suas m\u00e9dias e desvios padr\u00f5es (lembrando que features bidimensionais exigem uma m\u00e9dia e um desvio padr\u00e3o para cada coeficiente) e armazene esses valores numa matriz chamada `features`, em que cada linha representa os valores de uma janela","7a486602":"No caso como n\u00e3o queremos adentrar muito na matem\u00e1tica, deixamos a aplica\u00e7\u00e3o do viterbis pr\u00e9 pronta, execute a c\u00e9lula a seguir para ver os estados filtrados.","28aff930":"Como pode ver, temos as classes de cada segundo da m\u00fasica, mas existe uma forte transi\u00e7\u00e3o entre elas,para termos resultados mais coerentes com o que queremos, iremos aplicar o filtro de Viterbi.","82db04c9":"Execute a pr\u00f3xima c\u00e9lula para saber os tempos de mudan\u00e7a de estado do seu modelo:","056e40f3":"A ideia do m\u00e9todo do cotovelo \u00e9 escolher um ponto que esteja no \"cotovelo do gr\u00e1fico\", no caso porque s\u00e3o pontos em que a diferen\u00e7a de inertia entre os n\u00fameros de clusters anteriores \u00e9 muito alta e os n\u00fameros depois tem baixa diferen\u00e7a, logo n\u00e3o seria t\u00e3o relevante em dividir em mais clusters. No caso 4 \u00e9 a nossa melhor op\u00e7\u00e3o nesse caso, mas tamb\u00e9m podemos usar n\u00fameros de clusters maiores. ","03c85cc4":"## Treinamento do modelo\n\nAssim como j\u00e1 fazi\u00e1mos antes, iremos treinar o modelo, por\u00e9m como \u00e9 aprendizado n\u00e3o supervisionado (sem labels) n\u00e3o precisaremos do argumento `y`, na fun\u00e7\u00e3o `fit`. Nesse caso usaremos o algoritmo _KMeans_, da biblioteca `sklearn.cluster`, dentre os argumentos para cria\u00e7\u00e3o do modelo, usaremos `n_clusters` (n\u00famero de clusters que ser\u00e3o criados) e o `random_state=42`. \n\nA sintaxe ser\u00e1 algo como:\n\n```python3 \nmodel = KMeans(n_cluster=4, random_state=42) # criando um modelo que vai gerar 4 clusters\nmodel.fit(X) # Treinando o modelo \n```\nEmbora com isso j\u00e1 possamos ter um modelo treinado, precisamos saber qual o n\u00famero de clusters mais adequados, para isso usamos o *m\u00e9todo do cotovelo*. \n\n","0af6cf34":"## Importando Dados\nPrimeiramente, devemos importar a m\u00fasica como um vetor de intensidades.","55f6e4a1":"# Viterbi\n\nO algoritmo de Viterbi tem como objetivo encontrar a sequ\u00eancia de estados mais prov\u00e1vel que resultaria na medi\u00e7\u00e3o. Para isso, ele recebe uma matriz contendo a probabilidade de mudar de cada estado para outro (transition matrix) e uma matriz contendo a probabilidade do \u00e1udio ser cada estado em cada instante. (Um exemplo simples do algoritmo e do que ele faz pode ser visto aqui: https:\/\/en.wikipedia.org\/wiki\/Viterbi_algorithm#Example)","8a771d68":"#### Tarefa 07: Escolha um n\u00famero de clusters que siga esse m\u00e9todo e crie um modelo `model` com esse n\u00famero de clusters e treine esse modelo para a base de dados da m\u00fasica (`X`). (lembre do `random_state=42`)\n","61169197":"#### Tarefa 01: Importe a m\u00fasica, armazenando o vetor de intensidades na vari\u00e1vel `sound` e o samplerate na vari\u00e1vel `samplerate`, depois fa\u00e7a a sound ter as informa\u00e7\u00f5es de apenas um canal."}}