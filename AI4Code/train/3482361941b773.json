{"cell_type":{"c3ff1b79":"code","a4d551eb":"code","77760abd":"code","39257f07":"code","0210292d":"code","da527377":"code","9194cd53":"code","40e4d7cc":"code","d6d72deb":"code","fe157aa0":"code","020a2687":"code","f91cc184":"code","ca13ffe3":"code","72d0547f":"code","84aa50d4":"code","9c35b948":"code","685233b5":"code","96225f44":"code","e6d1728a":"code","04e3293a":"code","7f3779bf":"code","997d2a42":"code","0eb1aba4":"code","997c300e":"code","46bf1f45":"code","d1539767":"code","f8a9b54a":"code","a10a1b77":"code","a1fe6e93":"code","b2911bc5":"code","d0b44cc4":"code","191fc796":"code","6127575c":"code","e89cab64":"code","3e9c6f88":"code","074e953b":"code","8ffdbd2d":"code","d6efc25e":"code","c647c09e":"code","ea7b0181":"code","e4974042":"code","a0f0a3d6":"code","b56adb87":"code","fcfb54ec":"code","d5503b62":"code","64ed53de":"code","453fb2b0":"code","e087996c":"code","08b29df2":"code","3d4ae54b":"code","ef0e474f":"code","43b7cc1a":"code","86510daf":"code","6cb477a4":"code","72d57dd7":"code","a04ffad3":"code","fcfbda12":"code","0354f845":"code","731f0764":"code","5dfbe0f7":"code","5b303304":"code","e0682dbf":"code","0f59449e":"code","7aa89aaf":"code","43ab6c2f":"code","39dcccee":"code","3d8e9239":"code","46435b99":"code","df52dad7":"code","ea88f4da":"code","74e98287":"code","5974f1d0":"code","1db08e4d":"code","a34ce870":"code","443b0bbb":"code","bfcfa8f5":"code","c52108ed":"code","0b406050":"code","31adfa7f":"code","9afbc572":"code","5dfd03c1":"code","6bf685b2":"code","23e31ff7":"code","155f126c":"code","770123ae":"markdown","7c598122":"markdown","4852d49d":"markdown","ea0d929b":"markdown","2929d346":"markdown","6c28052a":"markdown","3bb905e4":"markdown","cf066ce6":"markdown","8d90f375":"markdown","6625973c":"markdown","a5215160":"markdown","6fff6dc3":"markdown","acbbc44f":"markdown","9514c98b":"markdown","849c0015":"markdown","4e0a1145":"markdown","abb01a7f":"markdown","ecf51b65":"markdown","a931db18":"markdown","d220f972":"markdown","237c2d3c":"markdown","623ff2c1":"markdown","f71589d5":"markdown","5b07b2aa":"markdown","ba269a8d":"markdown","271a6ecc":"markdown","5737eb43":"markdown","5be5e51d":"markdown","59c73913":"markdown","88c75ace":"markdown","32655ad1":"markdown","13ff2ef7":"markdown","320bc789":"markdown"},"source":{"c3ff1b79":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nheaders = ['#000000', '#474747']\nprint(\"Headers Palette\")\nsns.palplot(sns.color_palette(headers));\nplt.show()\n\nsvm = ['#3d5a80', '#98c1d9', '#e0fbfc', '#ee6c4d', '#293241']\nprint(\"Support Vector Machine Palette\")\nsns.palplot(sns.color_palette(svm));\nplt.show()\n\nknn = ['#d9ed92', '#b5e48c', '#99d98c', '#76c893', '#52b69a', '#34a0a4', '#168aad']\nprint(\"K Nearest Neighbour Palette\")\nsns.palplot(sns.color_palette(knn));\nplt.show()\n\n\nlr = ['#590d22', '#800f2f', '#a4133c', '#c9184a', '#ff4d6d', '#ff758f', '#ff8fa3', '#ff8fc4']\nprint(\"Logistic Regression Palette\")\nsns.palplot(sns.color_palette(lr));\nplt.show()\n\n","a4d551eb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns; sns.set()\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","77760abd":"df = pd.read_csv('\/kaggle\/input\/iris\/Iris.csv')\ndf.sample(10)","39257f07":"df.info()","0210292d":"df.describe()","da527377":"df.drop('Id', inplace = True, axis = 1)","9194cd53":"svm_palette = ['#3d5a80', '#98c1d9', '#ee6c4d', '#293241']","40e4d7cc":"plt.figure(1, figsize=(5,5), dpi = 100)\nplt.title(\"Distribution of Species\")\ndf['Species'].value_counts().plot.pie(autopct=\"%1.1f%%\");","d6d72deb":"plt.figure(figsize=(20,15))\nsns.set_style('darkgrid')\nplt.subplot(2,2,1)\nsns.violinplot(x = 'Species', y = 'SepalLengthCm', data = df, palette=svm_palette)\nplt.subplot(2,2,2)\nsns.violinplot(x = 'Species', y = 'SepalWidthCm', data = df, palette=svm_palette)\nplt.subplot(2,2,3)\nsns.violinplot(x = 'Species', y = 'PetalLengthCm', data = df, palette=svm_palette)\nplt.subplot(2,2,4)\nsns.violinplot(x = 'Species', y = 'PetalWidthCm', data = df, palette=svm_palette);","fe157aa0":"plt.figure(figsize=(20,15), dpi = 200)\nsns.set_style('white')\nplt.subplot(2,2,1)\nsns.scatterplot(x = 'SepalLengthCm', y = 'SepalWidthCm', hue = 'Species', data = df, palette=svm_palette[1:])\nplt.subplot(2,2,2)\nsns.scatterplot(x = 'PetalLengthCm', y = 'PetalWidthCm', hue = 'Species', data = df, palette=svm_palette[1:]);","020a2687":"sns.pairplot(df, hue = 'Species', vars = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm'], palette=svm_palette[1:] );","f91cc184":"plt.figure(figsize=(12,8)) \nsns.heatmap(df.corr(), annot=True, cmap=svm_palette, linewidths = 2)\nplt.show()","ca13ffe3":"X = df.iloc[:,0:4].values \ny = df.iloc[:,4:].values ","72d0547f":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.35, random_state = 101)\n\nprint(f'Total # of sample in whole dataset: {len(X)}')\nprint(\"*****\"*10)\nprint(f'Total # of sample in train dataset: {len(X_train)}')\nprint(f'Shape of X_train: {X_train.shape}')\nprint(\"*****\"*10)\nprint(f'Total # of sample in test dataset: {len(X_test)}')\nprint(f'Shape of X_test: {X_test.shape}')","84aa50d4":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","9c35b948":"from sklearn.svm import SVC\n\nmodel = SVC()\nmodel.fit(X_train, y_train)\npred = model.predict(X_test)\n \ntrain_score = model.score(X_train, y_train)\nprint(f'Train score of trained model: {train_score*100}')\n\ntest_score = model.score(X_test, y_test)\nprint(f'Test score of trained model: {test_score*100}')","685233b5":"from sklearn.metrics import confusion_matrix\n\nconf_matrix = confusion_matrix(pred, y_test)\n\nprint(f'Confussion Matrix: \\n{conf_matrix}\\n')\n\nsns.heatmap(conf_matrix, annot=True)","96225f44":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, pred))","e6d1728a":"from sklearn.model_selection import GridSearchCV\n\nparameters = {'C': [0.1, 1, 10, 100, 1000], \n              'gamma': [10, 1, 0.1, 0.01, 0.001, 0.0001],\n              'kernel': ['rbf','linear', 'poly','sigmoid'],\n              'shrinking': [True, False],\n              'degree': [1,2,3],\n              'probability': [True, False],\n              'verbose': [True, False],}\n\ngcv = GridSearchCV(model, parameters, cv=5, verbose = 1, n_jobs = -1).fit(X_train, y_train)","04e3293a":"print(f'GridSearchView Best Score: {gcv.best_score_*100}')\nprint(f'GridSearchView Best Estimator: {gcv.best_estimator_}')\nprint(f'GridSearchView Best Params: {gcv.best_params_}')","7f3779bf":"grid_predictions = gcv.predict(X_test)\n\ncm = confusion_matrix(y_test, grid_predictions)\nsns.heatmap(cm, annot=True)","997d2a42":"print(classification_report(y_test,grid_predictions))","0eb1aba4":"knn_palette = ['#d9ed92', '#b5e48c', '#99d98c', '#76c893', '#52b69a', '#34a0a4', '#168aad']","997c300e":"df = pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndf.sample(10)","46bf1f45":"df.info()","d1539767":"df.describe()","f8a9b54a":"df.drop(\"id\", axis=1, inplace=True)","a10a1b77":"plt.figure(1, figsize=(5,5), dpi = 100)\nplt.title(\"Distribution\")\ndf['stroke'].value_counts().plot.pie(autopct=\"%1.1f%%\");","a1fe6e93":"import seaborn as sns\nsns.set_theme(style=\"white\")\n\nplt.figure(figsize=(20,15))\n\nplt.subplot(2,2,1)\nsns.swarmplot(x=\"stroke\", y=\"age\",hue=\"gender\", data=df, palette=knn_palette)\n\nplt.subplot(2,2,2)\nsns.swarmplot(x=\"stroke\", y=\"age\",hue=\"ever_married\", data=df, palette=knn_palette)\n\nplt.subplot(2,2,3)\nsns.swarmplot(x=\"stroke\", y=\"age\",hue=\"smoking_status\", data=df, palette=knn_palette)\n\nplt.subplot(2,2,4)\nsns.swarmplot(x=\"stroke\", y=\"age\",hue=\"work_type\", data=df, palette=knn_palette)\n\nplt.show()","b2911bc5":"import seaborn as sns\nsns.set_theme(style=\"white\")\n\nplt.figure(figsize=(20,15))\n\nplt.subplot(2,2,1)\nsns.stripplot(x=\"stroke\", y=\"age\",hue=\"gender\", data=df, palette=knn_palette[4:])\n\nplt.subplot(2,2,2)\nsns.stripplot(x=\"stroke\", y=\"age\",hue=\"ever_married\", data=df, palette=knn_palette[4:])\n\nplt.subplot(2,2,3)\nsns.stripplot(x=\"stroke\", y=\"age\",hue=\"smoking_status\", data=df, palette=knn_palette[4:])\n\nplt.subplot(2,2,4)\nsns.stripplot(x=\"stroke\", y=\"age\",hue=\"work_type\", data=df, palette=knn_palette[4:])\n\nplt.show()\n","d0b44cc4":"sns.set_theme(style=\"darkgrid\")\n\n\nplt.figure(figsize=(20,15))\n\nplt.subplot(2,3,1)\nsns.boxenplot(x=df['stroke'], y=df['age'],\n              palette=knn_palette[6:], \n              scale=\"linear\", data=df)\n\nplt.subplot(2,3,2)\nsns.boxenplot(x=df['stroke'], y=df['bmi'],\n              palette=knn_palette[6:], \n              scale=\"linear\", data=df)\n\nplt.subplot(2,3,3)\nsns.boxenplot(x=df['stroke'], y=df['avg_glucose_level'],\n              palette=knn_palette[6:], \n              scale=\"linear\", data=df);","191fc796":"sns.pairplot(df, hue = 'stroke', palette = 'Greens');","6127575c":"plt.figure(figsize=(12,8)) \nsns.heatmap(df.corr(), annot=True, cmap='Greens', linewidths = 2)\nplt.show()","e89cab64":"df[df['bmi'].isnull()]","3e9c6f88":"df.boxplot(column=\"bmi\",by = \"gender\")\nplt.show()","074e953b":"print(\"Mean of BMI value for Females: \", np.mean(df[df['gender'] == 'Female']['bmi']))\nprint(\"Mean of BMI value for Males: \", np.mean(df[df['gender'] == 'Male']['bmi']))\nprint(\"Mean of BMI value: \", np.mean(df['bmi']))","8ffdbd2d":"df['bmi'] = df['bmi'].fillna(0)","d6efc25e":"for i in range(0,5035):\n    if(df['bmi'][i] == 0):\n        if(df['gender'][i] == 'Male'):\n            df['bmi'][i] = 28.594683544303823\n        elif(df['gender'][i] == 'Female'):\n            df['bmi'][i] = 29.035926055109936\n        else:\n            df['bmi'][i] = 28.854652338161664","c647c09e":"df.isnull().sum()","ea7b0181":"categorical = (df.dtypes == \"object\")\ncategorical_list = list(categorical[categorical].index)\n\nprint(\"Categorical variables:\")\nprint(categorical_list)","e4974042":"encoding_list =  ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']\n\none_hot_encoding_list = []\nlabel_encoding_list = []\n\nfor i in range (0, len(encoding_list)):\n    if(len(df[f'{encoding_list[i]}'].unique()) == 2):\n        label_encoding_list.append(encoding_list[i])\n    else:\n        one_hot_encoding_list.append(encoding_list[i])\n    print(f'Unique Values for {encoding_list[i]}', df[f'{encoding_list[i]}'].unique())","a0f0a3d6":"from sklearn.preprocessing import OneHotEncoder\nonehotencoder = OneHotEncoder()","b56adb87":"for i in range(0, len(one_hot_encoding_list)):\n    df[f'{one_hot_encoding_list[i]}'] = pd.Categorical(df[f'{one_hot_encoding_list[i]}'])\n    dummies = pd.get_dummies(df[f'{one_hot_encoding_list[i]}'], prefix = f'{one_hot_encoding_list[i]}_encoded')\n    df.drop([f'{one_hot_encoding_list[i]}'], axis=1, inplace=True)\n    df = pd.concat([df, dummies], axis=1)","fcfb54ec":"ever_married_mapping = {'No': 0, 'Yes': 1}\ndf['ever_married'] = df['ever_married'].map(ever_married_mapping)","d5503b62":"Residence_type_mapping = {'Rural': 0, 'Urban': 1}\ndf['Residence_type'] = df['Residence_type'].map(Residence_type_mapping)","64ed53de":"df","453fb2b0":"from sklearn.model_selection import train_test_split\n\nX = df.drop(['stroke'], axis = 1)\ny = df.stroke\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 101)\n\nprint(f'Total # of sample in whole dataset: {len(X)}')\nprint(\"*****\"*10)\nprint(f'Total # of sample in train dataset: {len(X_train)}')\nprint(f'Shape of X_train: {X_train.shape}')\nprint(\"*****\"*10)\nprint(f'Total # of sample in test dataset: {len(X_test)}')\nprint(f'Shape of X_test: {X_test.shape}')","e087996c":"sc=StandardScaler()\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","08b29df2":"from sklearn.neighbors import KNeighborsClassifier\n\nknn_model = KNeighborsClassifier()\nknn_model.fit(X_train, y_train)\npred = knn_model.predict(X_test)\n\ntrain_score = knn_model.score(X_train, y_train)\nprint(f'Train score of trained model: {train_score*100}')\n\ntest_score = knn_model.score(X_test, y_test)\nprint(f'Test score of trained model: {test_score*100}')","3d4ae54b":"from sklearn.metrics import confusion_matrix\n\nconf_matrix = confusion_matrix(pred, y_test)\n\nprint(f'Confussion Matrix: \\n{conf_matrix}\\n')\n\nsns.heatmap(conf_matrix, annot=True)","ef0e474f":"tn = conf_matrix[0,0]\nfp = conf_matrix[0,1]\ntp = conf_matrix[1,1]\nfn = conf_matrix[1,0]\n\ntotal = tn + fp + tp + fn\nreal_positive = tp + fn\nreal_negative = tn + fp","43b7cc1a":"accuracy  = (tp + tn) \/ total # Accuracy Rate\nprecision = tp \/ (tp + fp) # Positive Predictive Value\nrecall    = tp \/ (tp + fn) # True Positive Rate\nf1score  = 2 * precision * recall \/ (precision + recall)\nspecificity = tn \/ (tn + fp) # True Negative Rate\nerror_rate = (fp + fn) \/ total # Missclassification Rate\nprevalence = real_positive \/ total\nmiss_rate = fn \/ real_positive # False Negative Rate\nfall_out = fp \/ real_negative # False Positive Rate\n\nprint(f'Accuracy    : {accuracy*100}')\nprint(f'Precision   : {precision*100}')\nprint(f'Recall      : {recall*100}')\nprint(f'F1 score    : {f1score*100}')\nprint(f'Specificity : {specificity*100}')\nprint(f'Error Rate  : {error_rate*100}')\nprint(f'Prevalence  : {prevalence*100}')\nprint(f'Miss Rate   : {miss_rate*100}')\nprint(f'Fall Out    : {fall_out*100}')","86510daf":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, pred))","6cb477a4":"from sklearn import metrics\nprint(\"jaccard            :\", metrics.jaccard_score(y_test, pred)*100)","72d57dd7":"print(\"neg_log_loss       :\", metrics.log_loss(y_test, pred)*100)","a04ffad3":"parameters = {'n_neighbors': [5,6,7,8,9,15,20],\n              'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n             'weights': ['uniform', 'distance'],\n             'metric': ['manhattan', 'euclidean', 'minkowski', 'cosine', 'jaccard', 'hamming'],\n             }\n\ngcv = GridSearchCV(knn_model, parameters, cv=5, verbose = 1, n_jobs = -1).fit(X_train, y_train)","fcfbda12":"print(f'GridSearchView Best Score: {gcv.best_score_*100}')\nprint(f'GridSearchView Best Estimator: {gcv.best_estimator_}')\nprint(f'GridSearchView Best Params: {gcv.best_params_}')","0354f845":"grid_predictions = gcv.predict(X_test)\n\ncm = confusion_matrix(y_test, grid_predictions)\nsns.heatmap(cm, annot=True)","731f0764":"print(classification_report(y_test,grid_predictions))","5dfbe0f7":"logreg_palette = ['#590d22', '#800f2f', '#a4133c', '#c9184a', '#ff4d6d', '#ff758f', '#ff8fa3']","5b303304":"df = pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')\ndf.sample(10)","e0682dbf":"df.info()","0f59449e":"df.describe()","7aa89aaf":"df.drop(\"id\", axis=1, inplace=True)\ndf.drop(\"Unnamed: 32\", axis=1, inplace=True)","43ab6c2f":"plt.figure(figsize=(12,8), dpi = 100) \nsns.heatmap(df.corr(), annot=False, color = logreg_palette, linewidths = 2)\nplt.show()","39dcccee":"#This code is retrieved from here: https:\/\/www.kaggle.com\/kanncaa1\/dataiteam-titanic-eda#Introduction\nfrom collections import Counter\n\ndef detect_outliers(df,features):\n    outlier_indices = []\n    \n    for c in features:\n        # 1st quartile\n        Q1 = np.percentile(df[c],25)\n        # 3rd quartile\n        Q3 = np.percentile(df[c],75)\n        # IQR\n        IQR = Q3 - Q1\n        # Outlier step\n        outlier_step = IQR * 1.5\n        # detect outlier and their indeces\n        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n        # store indeces\n        outlier_indices.extend(outlier_list_col)\n    \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n    \n    return multiple_outliers","3d8e9239":"columns = list(df.columns)\ncolumns.remove('diagnosis')","46435b99":"df.loc[detect_outliers(df,columns)]","df52dad7":"# drop outliers\ndf = df.drop(detect_outliers(df,columns),axis = 0).reset_index(drop = True)","ea88f4da":"df.agg(['skew'])","74e98287":"skews = ['area_mean', 'radius_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'symmetry_se', 'fractal_dimension_se', 'area_worst', 'compactness_worst', 'fractal_dimension_worst' ]","5974f1d0":"from scipy.stats import norm, skew, boxcox\nfor i in skews:\n    (mu, sigma) = norm.fit(df[i])\n    print(\"mu {} : {}, sigma {} : {}\".format(i, mu, i, sigma))\n    print()\n    \n    df[i], lam = boxcox(df[i])\n\n    (mu, sigma) = norm.fit(df[i])\n    print(\"mu {} : {}, sigma {} : {}\".format(i, mu, i, sigma))\n    print()","1db08e4d":"df['diagnosis'].unique()","a34ce870":"diagnosis_mapping = {'M': 0, 'B': 1}\ndf['diagnosis'] = df['diagnosis'].map(diagnosis_mapping)","443b0bbb":"features = columns\nlabel = ['diagnosis']\n\nX = df[features]\ny = df[label]","bfcfa8f5":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 101)\n\nprint(f'Total # of sample in whole dataset: {len(X)}')\nprint(\"*****\"*10)\nprint(f'Total # of sample in train dataset: {len(X_train)}')\nprint(f'Shape of X_train: {X_train.shape}')\nprint(\"*****\"*10)\nprint(f'Total # of sample in test dataset: {len(X_test)}')\nprint(f'Shape of X_test: {X_test.shape}')","c52108ed":"from sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","0b406050":"from sklearn.linear_model import LogisticRegression\n\nlogReg_model = LogisticRegression()\nlogReg_model.fit(X_train, y_train)\npred = logReg_model.predict(X_test)\n\ntrain_score = logReg_model.score(X_train, y_train)\nprint(f'Train score of trained model: {train_score*100}')\n\ntest_score = logReg_model.score(X_test, y_test)\nprint(f'Test score of trained model: {test_score*100}')","31adfa7f":"from sklearn.metrics import confusion_matrix\n\nconf_matrix = confusion_matrix(pred, y_test)\n\nprint(f'Confussion Matrix: \\n{conf_matrix}\\n')\n\nsns.heatmap(conf_matrix, annot=True)","9afbc572":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, pred))","5dfd03c1":"from sklearn.model_selection import GridSearchCV\n\nparameters = {'penalty': ['l1', 'l2'],\n               'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n               'solver': ['liblinear', 'saga'],\n             }\n\ngcv = GridSearchCV(logReg_model, parameters, cv=5, verbose = 2, n_jobs = -1).fit(X_train, y_train)","6bf685b2":"print(f'GridSearchView Best Score: {gcv.best_score_*100}')\nprint(f'GridSearchView Best Estimator: {gcv.best_estimator_}')\nprint(f'GridSearchView Best Params: {gcv.best_params_}')","23e31ff7":"grid_predictions = gcv.predict(X_test)\n\ncm = confusion_matrix(y_test, grid_predictions)\nsns.heatmap(cm, annot=True);","155f126c":"print(classification_report(y_test,grid_predictions))","770123ae":"<center><h1 style = \"background:#000000 ;color:white;border:0;font-weight:bold\">Support Vector Machine<\/h1><\/center>\n\nIn machine learning, support vector machines are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training examples, each marked as belonging to one or the other of either category, an SVM training algorithm creates a model that assigns new examples to one category or the other, turning it into a nonprobabilistic binary linear classifier.\n\nBesides performing linear classification, SVMs can efficiently perform nonlinear classification using so-called kernel tricks and implicitly map their inputs to higher-dimensional feature spaces.\n\nWhen data is not labeled, supervised learning is not possible and an unsupervised learning approach is required that attempts to cluster data into groups and then match those groups with new data. The clustering algorithm that provides an improvement to support vector machines is called support vector clustering and is used for industry applications when either data is not flagged or only some data is labeled as a preprocessing for a classification.\n\n\nThe \u201cSupport Vector Machine\u201d is a supervised machine learning algorithm that can be used for classification or regression problems. However, it is mostly used in classification problems. In this algorithm, each data item is plotted as a point in the n-dimensional space (where n is the number of features you have) with the value of each feature being the value of a particular coordinate. Next, the classification is performed by finding the hyperplane that distinguishes the two classes quite well.\n\n![image.png](attachment:59753169-ae02-472e-93e3-e026b971401a.png)\n\n*Source for picture:* https:\/\/medium.com\/@ekrem.hatipoglu\/machine-learning-classification-support-vector-machine-kernel-trick-part-10-7ab928333158\n\nSupport Vectors are just the coordinates of the observation. The Support Vector Machine is a boundary that best separates the two classes (hyperplane\/line).\n\nSVMs are used in many classification problems from facial recognition systems to voice analysis.\n\n### Advantages:\n\n* They are effective when the number of dimensions is greater than the number of samples.\n* They are usefull in high-dimensional spaces.\n* They are versatile. Many different kernel functions can be used for the decision function.\n* A number of training points are used in the decision function. Therefore, memory is used efficiently.","7c598122":"## One-Hot Encoding","4852d49d":"**TP - True Positive:** The model correctly predicted the positive class as a positive class.\n\n**FP - False Positive:** The model predicted the negative class as a false positive class.\n\n**FN - False Negative:** The model predicted the positive class as false, negative class.\n\n**TN - True Negative:** The model predicted the negative class correctly.","ea0d929b":"<center><h1 style = \"background:#99d98c ;color:black;border:0;font-weight:bold\">Train - Test Split<\/h1><\/center>","2929d346":"<center><h1 style = \"background:#c9184a ;color:white;border:0;font-weight:bold\">Skewness<\/h1><\/center>\n\nIt can be said that skewness is the name given to the distortion of symmetry in data distribution in continuous or in other words, non-categorical data sets. In other words, it is the criterion of asymmetry. In summary, it is expected that the distribution of the data sets will show a normal distribution, but if the available data is contrary to this, it can be mentioned that the data is skewed. These distortions are among the reasons that prevent some machine learning models from learning from data, similar to the effect of imbalanced datasets used for categorical data.\n\nIn the image below, there is a graph showing the number of records belonging to three different data sets. In the data set with a symmetrical distribution as in the green graph, mode median and mean values are equal. In other words, the most frequently found number is both the median number and the average. The situation in the orange graph is expressed as positive skewness, and the situation in the blue graph as negative skewness.\n\n![image.png](attachment:21eca5cc-5c72-4d26-98b3-620a21cdfd7a.png)\n\n![image.png](attachment:0fec24ce-0c47-486e-90c3-43d7e8088aa6.png)\n\n* In the formula, n is the number of samples, xm is the arithmetic mean of the array (sample mean), and 's' is its standard deviation.\n\n* As the value of skewness moves towards plus infinity, the force of negative skewness increases as it moves towards positive and minus infinity.\n\n*Source for this explanation:* https:\/\/teachtomachines.com\/2020\/07\/07\/log-donusumu-ile-carpiklik-giderme\/\n","6c28052a":"<center><h1 style = \"background:#ee6c4d ;color:white;border:0;font-weight:bold\">Support Vector Machine<\/h1><\/center>\n\nA simple linear SVM classifier connects two classes by drawing a straight line between them. That is, all of the data points on one side of the line will be assigned to a category, while the data points on the other side of the line will be assigned to a different category. This implies that there may be an unlimited number of lines from which to pick.\n\nWhat distinguishes the linear SVM method from other algorithms, such as k-nearest neighbors, is that it selects the optimal line to categorize your data points. It selects the line that divides the data and is as far away from the closest data points as possible.\n\nA 2-D example can help you understand all of the machine learning terms. In essence, you have some data points on a grid. You're attempting to sort these data points into the appropriate categories, but you don't want any data in the wrong category. That is, you are attempting to identify the line connecting the two closest points that will keep the other data points separated.\n\n## SVM Types\n\nSVMs are classified into two kinds, each of which is used for a different purpose:\n\n* Simple SVM: This type of SVM is commonly used for linear regression and classification tasks.\n\n* Kernel SVM: Has additional flexibility for non-linear data since it can fit a hyperplane rather than a two-dimensional space.\n\n\n\n<html>\n<head>\n<style>\ntable {\n  font-family: arial, sans-serif;\n  border-collapse: collapse;\n  width: 100%;\n}\n\ntd, th {\n  border: 1px solid #dddddd;\n  text-align: left;\n  padding: 8px;\n}\n\ntr:nth-child(even) {\n  background-color: #dddddd;\n}\n<\/style>\n<\/head>\n<body>\n\n<table>\n  <tr>\n    <th>Pros<\/th>\n    <th>Cons<\/th>\n  <\/tr>\n  <tr>\n    <td>Effective on datasets with multiple features, like financial or medical data.<\/td>\n    <td>If the number of features is a lot bigger than the number of data points, avoiding over-fitting when choosing kernel functions and regularization term is crucial.<\/td>\n  <\/tr>\n  <tr>\n    <td>Effective in cases where number of features is greater than the number of data points.<\/td>\n    <td>SVMs don't directly provide probability estimates. Those are calculated using an expensive five-fold cross-validation.<\/td>\n  <\/tr>\n  <tr>\n    <td>Uses a subset of training points in the decision function called support vectors which makes it memory efficient.<\/td>\n    <td>Works best on small sample sets because of its high training time.<\/td>\n  <\/tr>\n  <tr>\n    <td>Different kernel functions can be specified for the decision function. You can use common kernels, but it's also possible to specify custom kernels.<\/td>\n    <td><\/td>\n  <\/tr>\n<\/table>\n\n<\/body>\n<\/html>\n","3bb905e4":"<center><h1 style = \"background:#99d98c ;color:black;border:0;font-weight:bold\">Encoding<\/h1><\/center>","cf066ce6":"<center><h1 style = \"background:#e0fbfc ;color:black;border:0;font-weight:bold\">Train-Test Split<\/h1><\/center>","8d90f375":"<center><h1 style = \"background:#ff8fa3 ;color:white;border:0;font-weight:bold\">Logistic Regression<\/h1><\/center>","6625973c":"<center><h1 style = \"background:#000000 ;color:white;border:0;font-weight:bold\">K Nearest Neighbours<\/h1><\/center>","a5215160":"<center><h1 style = \"background:#99d98c ;color:black;border:0;font-weight:bold\">Missing Values<\/h1><\/center>","6fff6dc3":"**Accuracy Rate:** A measure of how often the classifier predicts correctly.\n\n**Precision:** It shows how many of the values we guess as Positive are actually Positive.\n\n**Recall:** It is a measure of how much the classifier correctly predicts the true positive value. Also known as Sensitivity, Accuracy or Recall. (Sensitivity, Hit Rate or Recall) It should be as high as possible.\n\n**F1 Score:** F1 Score value shows the harmonic mean of Precision and Recall values. The reason why it is a harmonic average instead of a simple average is that we should not ignore extreme cases. If there was a simple average calculation, the F1 Score of a model with a Precision value of 1 and a Recall value of 0 would come as 0.5, and this would mislead us.\n\n**Specificity:** It is a measure of how much the classifier correctly predicted the true negative value.\n\n**Misclassification Rate (Error Rate):** It is a measure of how often the classifier guesses incorrectly. Also known as Error Rate.\n\n**Prevalence:** It is the measure of how often a value of 1 is found at the end of the estimation.\n\n**Miss Rate:** It is the ratio of those predicted to be 0 despite the real value being 1. Also known as loss rate.\n\n**Fall out:** It is the ratio of those predicted to be 1 even though the real value is 0. ","acbbc44f":"## Classification Report","9514c98b":"## Log Loss\n\nLog loss, aka logistic loss or cross-entropy loss.\n\nThis is the loss function used in (multinomial) logistic regression and extensions of it such as neural networks, defined as the negative log-likelihood of a logistic model that returns y_pred probabilities for its training data y_true. The log loss is only defined for two or more labels. For a single sample with true label <math xmlns=\"http:\/\/www.w3.org\/1998\/Math\/MathML\">\n  <mi>y<\/mi>\n  <mo>&#x2208;<\/mo>\n  <mo fence=\"false\" stretchy=\"false\">{<\/mo>\n  <mn>0<\/mn>\n  <mo>,<\/mo>\n  <mn>1<\/mn>\n  <mo fence=\"false\" stretchy=\"false\">}<\/mo>\n<\/math> and a probability estimate <math xmlns=\"http:\/\/www.w3.org\/1998\/Math\/MathML\">\n  <mi>p<\/mi>\n  <mo>=<\/mo>\n  <mi>Pr<\/mi>\n  <mo data-mjx-texclass=\"NONE\">&#x2061;<\/mo>\n  <mo stretchy=\"false\">(<\/mo>\n  <mi>y<\/mi>\n  <mo>=<\/mo>\n  <mn>1<\/mn>\n  <mo stretchy=\"false\">)<\/mo>\n<\/math>, the log loss is:\n\n\n<math xmlns=\"http:\/\/www.w3.org\/1998\/Math\/MathML\">\n  <mi>p<\/mi>\n  <mo>=<\/mo>\n  <mi>Pr<\/mi>\n  <mo data-mjx-texclass=\"NONE\">&#x2061;<\/mo>\n  <mo stretchy=\"false\">(<\/mo>\n  <mi>y<\/mi>\n  <mo>=<\/mo>\n  <mn>1<\/mn>\n  <mo stretchy=\"false\">)<\/mo>\n<\/math>","849c0015":"## Jaccard Similarity Coefficient Score\n\nThe Jaccard index, or Jaccard similarity coefficient, defined as the size of the intersection divided by the size of the union of two label sets, is used to compare set of predicted labels for a sample to the corresponding set of labels in y_true.\n\nThe Jaccard similarity coefficient of the <math xmlns=\"http:\/\/www.w3.org\/1998\/Math\/MathML\">\n  <mi>i<\/mi>\n<\/math>-th samples, with a ground truth label set <math xmlns=\"http:\/\/www.w3.org\/1998\/Math\/MathML\">\n  <msub>\n    <mi>y<\/mi>\n    <mi>i<\/mi>\n  <\/msub>\n<\/math>\n and predicted label set <math xmlns=\"http:\/\/www.w3.org\/1998\/Math\/MathML\">\n  <msub>\n    <mrow>\n      <mover>\n        <mi>y<\/mi>\n        <mo stretchy=\"false\">^<\/mo>\n      <\/mover>\n    <\/mrow>\n    <mi>i<\/mi>\n  <\/msub>\n<\/math>\n, is defined as\n\n<math xmlns=\"http:\/\/www.w3.org\/1998\/Math\/MathML\" display=\"block\">\n  <mi>J<\/mi>\n  <mo stretchy=\"false\">(<\/mo>\n  <msub>\n    <mi>y<\/mi>\n    <mi>i<\/mi>\n  <\/msub>\n  <mo>,<\/mo>\n  <msub>\n    <mrow>\n      <mover>\n        <mi>y<\/mi>\n        <mo stretchy=\"false\">^<\/mo>\n      <\/mover>\n    <\/mrow>\n    <mi>i<\/mi>\n  <\/msub>\n  <mo stretchy=\"false\">)<\/mo>\n  <mo>=<\/mo>\n  <mfrac>\n    <mrow>\n      <mo stretchy=\"false\">|<\/mo>\n      <msub>\n        <mi>y<\/mi>\n        <mi>i<\/mi>\n      <\/msub>\n      <mo>&#x2229;<\/mo>\n      <msub>\n        <mrow>\n          <mover>\n            <mi>y<\/mi>\n            <mo stretchy=\"false\">^<\/mo>\n          <\/mover>\n        <\/mrow>\n        <mi>i<\/mi>\n      <\/msub>\n      <mo stretchy=\"false\">|<\/mo>\n    <\/mrow>\n    <mrow>\n      <mo stretchy=\"false\">|<\/mo>\n      <msub>\n        <mi>y<\/mi>\n        <mi>i<\/mi>\n      <\/msub>\n      <mo>&#x222A;<\/mo>\n      <msub>\n        <mrow>\n          <mover>\n            <mi>y<\/mi>\n            <mo stretchy=\"false\">^<\/mo>\n          <\/mover>\n        <\/mrow>\n        <mi>i<\/mi>\n      <\/msub>\n      <mo stretchy=\"false\">|<\/mo>\n    <\/mrow>\n  <\/mfrac>\n  <mo>.<\/mo>\n<\/math>","4e0a1145":"## Label Encoding","abb01a7f":"<center><h1 style = \"background:#d9ed92 ;color:black;border:0;font-weight:bold\">Information About Dataset<\/h1><\/center>","ecf51b65":"<center><h1 style = \"background:#98c1d9 ;color:black;border:0;font-weight:bold\">Data Visualization<\/h1><\/center>","a931db18":"<center><h1 style = \"background:#800f2f ;color:white;border:0;font-weight:bold\">Data Visualization<\/h1><\/center>","d220f972":"<center><h1 style = \"background:#a4133c ;color:white;border:0;font-weight:bold\">Anomaly Detection<\/h1><\/center>\n\nAnomaly is one that differs \/ deviates significantly from other observations in the same sample. An anomaly detection pattern produces two different results. The first is a categorical tag for whether the observation is abnormal or not; the second is a score or trust value. Score carries more information than the label. Because it also tells us how abnormal the observation is. The tag just tells you if it's abnormal. While labeling is more common in supervised methods, the score is more common in unsupervised and semisupervised methods.","237c2d3c":"<center><h1 style = \"background:#000000 ;color:white;border:0;font-weight:bold\">CLASSIFICATION MASTER NOTEBOOK #1<\/h1><\/center>\n\nHey! In this notebook I have covered three Machine Learning Classification Algorithms. I explained classification algorithms, evaluation metrics, skewness, encoding and hyper-parameter optimization and I performed these techniques in theree different datasets. Three different datasets are used in this notebook:\n\n1. Breast Cancer Wisconsin (Diagnostic) Data Set\n1. Iris Species\n1. Stroke Prediction Dataset\n\n## In each topic I used similar templates such as:\n\n* Information About Dataset\n* Data Visualization\n* Data Preprocessing (fillna, encoding, skew, anomaly detection)\n* Train-Test Split\n* Model\n* Evaluation of Model\n* Hyper-Parameter Optimization\n\n## Examined algorihms are:\n\n#### Support Vector Machines\n#### K Nearest Neighbour\n#### Logistic Regression\n\nYou will find out detailed information about classification evaluation metrics in KNN section.\n\nAll parameters of algorithms have explained in the Hyper-Parameter Optimization section.\n\n\n## Before starting:\n\n* Main headings are indicated with a black background.\n* The following color palettes are used for each main title.","623ff2c1":"<center><h1 style = \"background:#b5e48c ;color:black;border:0;font-weight:bold\">Data Visualization<\/h1><\/center>","f71589d5":"<center><h1 style = \"background:#3d5a80 ;color:white;border:0;font-weight:bold\">Information About Dataset<\/h1><\/center>","5b07b2aa":"<center><h1 style = \"background:#ff4d6d ;color:white;border:0;font-weight:bold\">Encoding<\/h1><\/center>","ba269a8d":"<center><h1 style = \"background:#000000 ;color:white;border:0;font-weight:bold\">Logistic Regression<\/h1><\/center>\n\nLogistic regression is a method used to determine the cause-effect relationship with the explanatory variables when the response variable is observed in categorical and multiple categories. It is a regression method in which the expected values of the response variable according to the explanatory variables are obtained as probabilities. Logistic regression analysis is a regression method that helps classification and assignment. There is no normal distribution assumption, continuity assumption prerequisite. The effects of the explanatory variables on the dependent variable are obtained as probabilities and the risk factors are determined as probabilities.","271a6ecc":"<center><h1 style = \"background:#590d22 ;color:white;border:0;font-weight:bold\">Information About Dataset<\/h1><\/center>","5737eb43":"<center><h1 style = \"background:#ff8fc4 ;color:white;border:0;font-weight:bold\">Hyper-Parameter Optimization<\/h1><\/center>\n\n## Parameters of sklearn.KNeighborsClassifier\n\n<html>\n<head>\n<style>\ntable {\n  font-family: arial, sans-serif;\n  border-collapse: collapse;\n  width: 100%;\n}\n\ntd, th {\n  border: 1px solid #dddddd;\n  text-align: left;\n  padding: 8px;\n}\n\ntr:nth-child(even) {\n  background-color: #dddddd;\n}\n<\/style>\n<\/head>\n<body>\n\n<table>\n  <tr>\n    <th>Parameter<\/th>\n    <th>Explanation<\/th>\n    <th>Default<\/th>\n  <\/tr>\n  <tr>\n    <td>penalty<\/td>\n    <td>Used to indicate the norm used in penalization. It takes 'l1', 'l2', 'elasticnet', 'none' values.<\/td>\n    <td>'l2'<\/td>\n  <\/tr>\n  <tr>\n    <td>C<\/td>\n    <td>Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.<\/td>\n    <td>1.0 (float)<\/td>\n  <\/tr>\n  <tr>\n    <td>solver<\/td>\n    <td><ul class=\"simple\">\n<li><p>For small datasets, \u2018liblinear\u2019 is a good choice, whereas \u2018sag\u2019 and\n\u2018saga\u2019 are faster for large ones.<\/p><\/li>\n<li><p>For multiclass problems, only \u2018newton-cg\u2019, \u2018sag\u2019, \u2018saga\u2019 and \u2018lbfgs\u2019\nhandle multinomial loss; \u2018liblinear\u2019 is limited to one-versus-rest\nschemes.<\/p><\/li>\n<li><p>\u2018newton-cg\u2019, \u2018lbfgs\u2019, \u2018sag\u2019 and \u2018saga\u2019 handle L2 or no penalty<\/p><\/li>\n<li><p>\u2018liblinear\u2019 and \u2018saga\u2019 also handle L1 penalty<\/p><\/li>\n<li><p>\u2018saga\u2019 also supports \u2018elasticnet\u2019 penalty<\/p><\/li>\n<li><p>\u2018liblinear\u2019 does not support setting <code class=\"docutils literal notranslate\"><span class=\"pre\">penalty='none'<\/span><\/code><\/p><\/li>\n<\/ul>\n      <\/td>\n    <td>'lbfgs'<\/td>\n  <\/tr>\n \n<\/table>\n\n<\/body>\n<\/html>","5be5e51d":"<center><h1 style = \"background:#474747 ;color:white;border:0;font-weight:bold\">Classification Evaluation Metrics<\/h1><\/center>\n\nEvaluation metrics are used to measure the quality of the statistical or machine learning model. Evaluating machine learning models or algorithms is essential for any project. There are many different types of evaluation metrics available to test a model. These include classification accuracy, logarithmic loss, confusion matrix, and others. \n\n***Why is this Useful?***\n\nIt is very important to use multiple evaluation metrics to evaluate your model. This is because a model may perform well using one measurement from one evaluation metric, but may perform poorly using another measurement from another evaluation metric. Using evaluation metrics are critical in ensuring that your model is operating correctly and optimally. \n\n***Applications of Evaluation Metrics***\n* Statistical Analysis\n\n* Machine Learning\n\nSource: https:\/\/deepai.org\/machine-learning-glossary-and-terms\/evaluation-metrics\n\n## Confusion Matrix\n\nConfusion matrix is a measurement tool that provides information about the accuracy of predictions. The logic behind it is actually simple, but it is often used especially in classification algorithms as it provides easy to understand information about the accuracy of the measurement.","59c73913":"# Conclusion\n\nWe have come to the end of the notebook. I covered three machine learning classification algorithms in this notebook. I hope you liked it.\n\n* If you have questions, please comment them. I will try to explain if you don't understand.\n* Waiting for your positive and negative comments. :)\n\nThank you for your time.\n\n## In addition:\n\n* You can check my other notebook which is about Regression Techniques. It has same format as this notebook. Click: [REGRESSION MASTER NOTEBOOK](https:\/\/www.kaggle.com\/barisscal\/regression-master-notebook)","88c75ace":"<center><h1 style = \"background:#ff758f ;color:white;border:0;font-weight:bold\">Train-Test Split<\/h1><\/center>","32655ad1":"<center><h1 style = \"background:#76c893 ;color:black;border:0;font-weight:bold\">K Nearest Neighbours<\/h1><\/center>\n\nK-NN algorithm is one of the most used classification algorithm. K-NN is a non-parametric and lazy learning algorithm. Although it is used in the solution of both classification and regression problems, it is mostly used in the solution of classification problems. Unlike Eager learning, lazy learning does not have a training stage. It does not learn the training data but instead memorizes the training data set. When asked to make a forecast, it looks for the nearest neighbors across the entire data set. The distance of the new data to be included in the sample data set is calculated according to the existing data and k number of close neighborhoods are checked. Generally, 3 types of distance functions are used for distance calculations: Euclidean Distance, Manhattan Distance, and Minkowski Distance.\n\n<html>\n\\[Euclidean = \\sqrt{\\sum_{i=1}^{k} (x_i - y_i)^2} \\]\n<\/html>\n\n<html>\n\\[Manhattan = \\sum_{i=1}^{k} |x_i - y_i| \\]\n<\/html>\n\n<html>\n\\[Minkowski = \\sum_{i=1}^{k} (|x_i - y_i|)^4)^{1\/q} \\] \n<\/html>\n\n\n\n\nThere are 3 indicators that are generally used to measure the performance of a model produced with the KNN (K-Nearest Neighbors) Algorithm.\n\n**Jaccard Index:** It is the ratio of the intersection set of the correct prediction set and the true value set to their union set. It takes a value between 1 and 0. 1 means best achievement.\n\n**F1-Score:** It is calculated from the Precission and Recall values calculated over the Confusion Matrix. Pre=TP\/(TP+FP) Rec=TP\/(TP+FN) F1-Score= 2((PreRec)\/(Pre+Rec)) It takes a value between 1 and 0. 1 means best achievement.\n\n**LogLoss:** At the end of Logistic Regression, the LogLoss value is calculated over the probabilities of the predictions. It takes values between 1 and 0. Unlike the two values above, 0 means best performance.","13ff2ef7":"<center><h1 style = \"background:#168aad ;color:black;border:0;font-weight:bold\">Hyper-Parameter Optimization<\/h1><\/center>\n\n\n## Parameters of sklearn.KNeighborsClassifier\n\n<html>\n<head>\n<style>\ntable {\n  font-family: arial, sans-serif;\n  border-collapse: collapse;\n  width: 100%;\n}\n\ntd, th {\n  border: 1px solid #dddddd;\n  text-align: left;\n  padding: 8px;\n}\n\ntr:nth-child(even) {\n  background-color: #dddddd;\n}\n<\/style>\n<\/head>\n<body>\n\n<table>\n  <tr>\n    <th>Parameter<\/th>\n    <th>Explanation<\/th>\n    <th>Default<\/th>\n  <\/tr>\n  <tr>\n    <td>n_neighbors<\/td>\n    <td>This parameter refers to the number of neighbors and used to calculate the between selected number of neighbors and data point.<\/td>\n    <td>5 (int)<\/td>\n  <\/tr>\n  <tr>\n    <td>algorithm<\/td>\n    <td>It takes an algorithm to use for computing the nearest neighbors. Parameters it can take are 'auto', 'ball_tree', 'kd_tree', 'brute'. Default value of algorithms is auto.<\/td>\n    <td>'auto'<\/td>\n  <\/tr>\n  <tr>\n    <td>weights<\/td>\n    <td>Another key parameter for kNN is weights. Both 'uniform' and 'distance' weight functions could be used. In uniform points in each neighbor are weighted the same. In distance, closer neighbors have greater impact than the ones further away.<\/td>\n    <td>'uniform'<\/td>\n  <\/tr>\n  <tr>\n    <td>metric<\/td>\n    <td>Metrics are important parameters for kNN algorithm. These values of metric(Euclidean, Manhattan, Minkowski) are used to calculate the distances.<\/td>\n    <td>'minkowski'<\/td>\n  <\/tr>\n<\/table>\n\n<\/body>\n<\/html>","320bc789":"<center><h1 style = \"background:#474747 ;color:white;border:0;font-weight:bold\">Hyper-Parameter Optimization<\/h1><\/center>\n\nUnlike parameters, hyperparameters are not learned during training the model. They are determined by the data scientist before the modeling phase. For example, KNN algorithm, which is one of the non-parametric classification algorithms, makes classification by looking at the nearest k neighbors to the desired value. Here, the k number (n_neighbors:) and the distance metric (metric:) to be used are the hyperparameters that should be specified by the data scientist before the modeling, which increases the performance of the model.\n\nHyperparameter optimization is the process of finding the most suitable hyperparameter combination according to the success metric specified for a machine learning algorithm.\n\nGiven that there are dozens of hyperparameters for a machine learning algorithm and dozens of values these hyperparameters can take, it's clear how difficult it will be to try all combinations one by one and pick the best combination. For this reason, different methods have been developed for hyperparameter optimization. GridSearcCV and RandomizedSearchCV are among these methods.\n\n## GridSearchCV\n\nFor the hyperparameters and their values that are desired to be tested in the model, a separate model is established with all combinations and the most successful hyperparameter set is determined according to the specified metric.\n\n## Parameters of sklearn.SVC\n\n<html>\n<head>\n<style>\ntable {\n  font-family: arial, sans-serif;\n  border-collapse: collapse;\n  width: 100%;\n}\n\ntd, th {\n  border: 1px solid #dddddd;\n  text-align: left;\n  padding: 8px;\n}\n\ntr:nth-child(even) {\n  background-color: #dddddd;\n}\n<\/style>\n<\/head>\n<body>\n\n<table>\n  <tr>\n    <th>Parameter<\/th>\n    <th>Explanation<\/th>\n    <th>Default<\/th>\n  <\/tr>\n  <tr>\n    <td>Gamma<\/td>\n    <td>The gamma can be thought of as the \"spreading\" of the kernel and hence the decision zone. It is a parameter for nonlinear hyperplanes. The higher the gamma value, the more it tries to fit the training dataset exactly.<\/td>\n    <td>'scale'<\/td>\n  <\/tr>\n  <tr>\n    <td>C<\/td>\n    <td>C is the penalty parameter expressing the misclassification or error rate. Misclassification or error rate determines how many errors SVM optimization can accept. It controls the exchange between the decision boundary and the misclassification term.<\/td>\n    <td>1.0 (float)<\/td>\n  <\/tr>\n  <tr>\n    <td>Kernel<\/td>\n    <td>The main function of the kernel is to take the lower dimensional input space and transform it into a higher dimensional space. It is mostly useful in nonlinear separation problem.<\/td>\n    <td>'rbf'<\/td>\n  <\/tr>\n  <tr>\n    <td>Shrinking<\/td>\n    <td>Determines whether to use the shrinking heuristic. It takes a Boolean value.<\/td>\n    <td>'True' (bool)<\/td>\n  <\/tr>\n  <tr>\n    <td>Probability<\/td>\n    <td>Whether to enable probability estimates.<\/td>\n    <td>'False' (bool)<\/td>\n  <\/tr>\n  <tr>\n    <td>Verbose<\/td>\n    <td>Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context.<\/td>\n    <td>'False' (bool)<\/td>\n  <\/tr>\n   <tr>\n    <td>Degree<\/td>\n    <td>Degree of the polynomial kernel function (\u2018poly\u2019). Ignored by all other kernels.<\/td>\n    <td>3 (int)<\/td>\n  <\/tr>\n<\/table>\n\n<\/body>\n<\/html>\n\n\n## Notes:\n* If the C is higher, the optimization will choose smaller margin hyperplane, so training data miss classification rate will be lower. On the other hand, if the C is low, then the margin will be big, even if there will be miss classified training data examples.\n* High Gamma will consider only points close to the plausible hyperplane and low Gamma will consider points at greater distance."}}