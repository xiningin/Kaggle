{"cell_type":{"1404bbd2":"code","b165dabe":"code","3507ac67":"code","a25c847b":"code","fa4bb852":"code","e5241291":"code","9375d989":"code","94b903f2":"code","78dab3b7":"code","142378c4":"code","dc76fdef":"code","6c87919b":"code","fe32fe9c":"code","b0fb3de3":"code","e9233f3d":"code","f617f8d4":"code","ca943bbd":"code","94ce35b1":"code","d5c31d4a":"code","5591b045":"code","a444441f":"markdown","5a1171c0":"markdown","cacc0ddc":"markdown","8ff16708":"markdown","dbe811f6":"markdown","55293364":"markdown","20b32181":"markdown","1868de76":"markdown","f3423cf5":"markdown","0bac484b":"markdown","0bc952d1":"markdown","636b1123":"markdown","afa36df5":"markdown","bc6f7f2a":"markdown","e476e38b":"markdown","e60c99bb":"markdown","d64bc52e":"markdown","02167f6d":"markdown","ba593762":"markdown","955a1442":"markdown","6bce3f66":"markdown","363dd6da":"markdown","7ffcc11b":"markdown","a46f7651":"markdown","f78ec494":"markdown","867bc259":"markdown","67edc7a2":"markdown","0bb33773":"markdown"},"source":{"1404bbd2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import normalize\nfrom sklearn.model_selection import ParameterGrid\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b165dabe":"train_tweets = pd.read_csv('..\/input\/viral-tweets-prediction-dataset\/Dataset\/Tweets\/train_tweets.csv',index_col=0)\ntrain_tweets_vectorized_text = pd.read_csv('..\/input\/viral-tweets-prediction-dataset\/Dataset\/Tweets\/train_tweets_vectorized_text.csv',index_col=0)\n\n\ntest_tweets = pd.read_csv('..\/input\/viral-tweets-prediction-dataset\/Dataset\/Tweets\/test_tweets.csv',index_col=0)\ntest_tweets_vectorized_text = pd.read_csv('..\/input\/viral-tweets-prediction-dataset\/Dataset\/Tweets\/test_tweets_vectorized_text.csv',index_col=0)\n\nuser_vectorized_descriptions = pd.read_csv('..\/input\/viral-tweets-prediction-dataset\/Dataset\/Users\/user_vectorized_descriptions.csv',index_col=0)\nuser_vectorized_profile_images = pd.read_csv('..\/input\/viral-tweets-prediction-dataset\/Dataset\/Users\/user_vectorized_profile_images.csv',index_col=0)\nusers = pd.read_csv('..\/input\/viral-tweets-prediction-dataset\/Dataset\/Users\/users.csv',index_col=0)","3507ac67":"user_vectorized_descriptions_pc = pd.DataFrame(PCA(n_components=10).fit_transform(normalize(user_vectorized_descriptions)),index=user_vectorized_descriptions.index,columns=user_vectorized_descriptions.columns[0:10])\nuser_vectorized_profile_images_pc = pd.DataFrame(PCA(n_components=5).fit_transform(normalize(user_vectorized_profile_images)),index=user_vectorized_profile_images.index,columns=user_vectorized_profile_images.columns[0:5])\n\npca_tweets = PCA(n_components=25).fit(normalize(train_tweets_vectorized_text))\n\ntrain_tweets_vectorized_text_pc = pd.DataFrame(pca_tweets.transform(normalize(train_tweets_vectorized_text)),index=train_tweets_vectorized_text.index,columns=train_tweets_vectorized_text.columns[0:25])\ntest_tweets_vectorized_text_pc = pd.DataFrame(pca_tweets.transform(normalize(test_tweets_vectorized_text)),index=test_tweets_vectorized_text.index,columns=test_tweets_vectorized_text.columns[0:25])","a25c847b":"users = users.merge(user_vectorized_descriptions_pc, on = \"user_id\", how = \"left\")\nusers = users.merge(user_vectorized_profile_images_pc, on = \"user_id\", how = \"left\")","fa4bb852":"train = train_tweets.merge(train_tweets_vectorized_text_pc, on = \"tweet_id\", how = \"left\")\ntrain = train.merge(users, how='left', left_on='tweet_user_id', right_on='user_id')\n\ntest = test_tweets.merge(test_tweets_vectorized_text_pc, on = \"tweet_id\", how = \"left\")\ntest = test.merge(users, how='left', left_on='tweet_user_id', right_on='user_id')","e5241291":"train.tweet_topic_ids.isna().sum()","9375d989":"train.drop('tweet_topic_ids', axis=1, inplace=True)\ntest.drop('tweet_topic_ids', axis=1, inplace=True)","94b903f2":"train.tweet_has_attachment = train.tweet_has_attachment*1\ntest.tweet_has_attachment = test.tweet_has_attachment*1\n\ntrain.user_has_url = train.user_has_url*1\ntest.user_has_url = test.user_has_url*1\n\ntrain.user_has_location = train.user_has_location*1\ntest.user_has_location = test.user_has_location*1","78dab3b7":"dumm_tweet_attachment_class_train = pd.get_dummies(train['tweet_attachment_class'])\ndumm_tweet_attachment_class_test = pd.get_dummies(test['tweet_attachment_class'])\n\n\ntrain = pd.concat([train,dumm_tweet_attachment_class_train], axis=1)\ntrain.drop(columns=['tweet_attachment_class'], inplace=True)\n\ntest = pd.concat([test,dumm_tweet_attachment_class_test], axis=1)\ntest.drop(columns=['tweet_attachment_class'], inplace=True)","142378c4":"test.isna().sum().sum()","dc76fdef":"train","6c87919b":"(train_model, test_fk) = train_test_split(train, test_size = 0.15,random_state=57)\n\ny_train = train_model['virality']\nX_train = train_model.drop(['virality','tweet_user_id'],1)\n\ny_test_fk = test_fk['virality']\nX_test_fk = test_fk.drop(['virality','tweet_user_id'],1)","fe32fe9c":"#parameters = {'max_depth': [9], 'max_leaf_nodes': [87], 'min_samples_leaf': [0.008889], 'min_samples_split': [0.04737368421052632]}\n\n#parameters = {'max_depth':list(range(2,12)), 'max_leaf_nodes': list(range(2,90))}\n\nparameters = {'max_depth': [11], 'max_leaf_nodes': [60]}\n\n\nbest_params=[]\nbest_score = 0\n\ni=0\n# Treinamento\nparameters_g = shuffle(list(ParameterGrid(parameters)))\nfor param in list(parameters_g):\n    model_dt = tree.DecisionTreeClassifier(max_depth= param['max_depth'],\n                                          max_leaf_nodes = param['max_leaf_nodes'])\n    \n    #model_dt.fit(X_train,y_train)\n    #score = balanced_accuracy_score(y_val, model_dt.predict(X_val))\n\n    # Validacao\n    \n    score_cv = cross_val_score(model_dt, X_train, y_train, cv=10,scoring='accuracy')\n    score = np.mean(score_cv)\n    \n    \n    if score > best_score:\n        best_score = score\n        best_params = param\n        best_model_dt = model_dt\n\n        \n    i+=1\n    if(i%10 == 0):\n        print((i\/len(list(ParameterGrid(parameters))))*100,\"Best score:\", best_score,\"Best params:\", best_params)\n\nbest_model_dt.fit(X_train, y_train)\n\nprint(\"Best params:\", best_params)\nprint(\"Best score:\", best_score)","b0fb3de3":"#parameters = {'max_depth':np.arange(5, 30, 2), \"n_estimators\":[50],  \"criterion\":[\"gini\", 'entropy']}\n\nparameters = {'criterion': ['entropy'], 'max_depth': [29], 'n_estimators': [200]}\n\nbest_params=[]\nbest_score = 0\n\ni=0\n# Treinamento\nparameters_g = shuffle(list(ParameterGrid(parameters)))\nfor param in list(parameters_g):\n    model_rf = RandomForestClassifier( max_depth= param['max_depth'],\n                                      n_estimators= param['n_estimators'],\n                                      criterion = param['criterion'])\n\n    #model_rf.fit(X_train,y_train)\n    #score = balanced_accuracy_score(y_val, model_rf.predict(X_val))\n\n    # Validacao\n    \n    \n    score_cv = cross_val_score(model_rf, X_train, y_train, cv=10,scoring='accuracy',n_jobs=-1)\n    score = np.mean(score_cv)\n    \n    if score > best_score:\n        best_score = score\n        best_params = param\n        best_model_rf = model_rf\n\n        \n    i+=1\n    print((i\/len(list(ParameterGrid(parameters))))*100,\"Best score:\", best_score,\"Best params:\", best_params)\n\nbest_model_rf.fit(X_train, y_train)\n\nprint(\"Best params:\", best_params)\nprint(\"Best score:\", best_score)","e9233f3d":"#model_xgb = XGBClassifier()\n\n#score_cv = cross_val_score(model_xgb, X_train, y_train, cv=3,scoring='accuracy',n_jobs=-1)\n#print(\"CV score:\", np.mean(score_cv))","f617f8d4":"model_bag = BaggingClassifier(n_estimators=50)\nscore_cv = cross_val_score(model_bag, X_train, y_train, cv=3,scoring='accuracy',n_jobs=-1)\nprint(\"CV score:\", np.mean(score_cv))\n\nmodel_bag.fit(X_train, y_train)","ca943bbd":"model_vot = VotingClassifier(estimators=[('rf', best_model_rf),\n                                         ('bag', model_bag)],\n                             voting='soft', n_jobs=-1)\nscore_cv = cross_val_score(model_vot, X_train, y_train, cv=3,scoring='accuracy',n_jobs=-1)\nprint(\"CV score:\", np.mean(score_cv))\n\nmodel_vot.fit(X_train, y_train)","94ce35b1":"melhor_modelo_cv = best_model_rf\n\nprint(\"Test score:\", accuracy_score(y_test_fk,melhor_modelo_cv.predict(X_test_fk)))","d5c31d4a":"X_test = test.drop(['tweet_user_id'],1)\n\npredictions = melhor_modelo_cv.predict(X_test)\n\npredictionsDF = pd.DataFrame({\"tweet_id\":X_test.index, \n                                     \"virality\":predictions})\npredictionsDF.to_csv(\"resposta.csv\",index = False, sep = \",\", decimal = \",\", float_format = str)","5591b045":"predictionsDF","a444441f":" Como algumas bases de dados possuem milhares de colunas, e s\u00e3o colunas que n\u00e3o possuem interpreta\u00e7\u00e3o, \u00e9 o caso perfeito para aplica\u00e7\u00e3o de PCA. Nesse caso foi aplicado PCA em cada base separada para dimunir o numero de features da casa de milhares para casa de dezenas.","5a1171c0":"# Bagging (\u00c1rvore de decis\u00e3o)\n","cacc0ddc":"## Unindo as bases","8ff16708":"## Cria\u00e7\u00e3o de base de teste \"gerada\"","dbe811f6":"# Voting Classifier\n","55293364":"# Pr\u00e9-processamento dos dados","20b32181":"A coluna \"tweet_attachment_class\" foi desmembrada em outras 3 columas por meio de one hot enconding para melhor aplicabilidade nos modelos.","1868de76":"## Removendo colunas","f3423cf5":"## Verificando se h\u00e1 NA","0bac484b":"# Modelagem","0bc952d1":"# Leitura dos dados","636b1123":"Algumas colunas que estavam em formato booleano foram transformadas em numeric para melhor aplicabilidade nos modelos.","afa36df5":"Ap\u00f3s todos os tratamentos foi verificado se as bases de treino e testes continham valores faltantes e foi constatado que n\u00e3o","bc6f7f2a":"Parte da base de treino final.","e476e38b":"## XGBoost","e60c99bb":"Como n\u00e3o consegui submeter a tempo para a competi\u00e7\u00e3o oficial separei uma parte da base de treino para verifica\u00e7\u00e3o da acur\u00e1cia somente no final de tudo. No restante da base de treino normal utilizei Cross Validation para encontrar os melhores hiper-parametros.","d64bc52e":"A coluna \"tweet_topic_ids\" foi removida por apresentar NA e por conter muitas categorias.","02167f6d":"Ap\u00f3s isso a base \"train_tweets_vectorized_text_pc\" foi unida na \"train_tweets\" e a base \"test_tweets_vectorized_text_pc\" foi unida na \"test_tweets\" por meio de um left join.","ba593762":"# Gerar csv submiss\u00e3o","955a1442":"## Florestas Aleat\u00f3rias","6bce3f66":"## Tratamento","363dd6da":"# One hot encoding para vari\u00e1veis categ\u00f3ricas","7ffcc11b":"## Redu\u00e7\u00e3o de dimensionalidade: PCA","a46f7651":"Ap\u00f3s a redu\u00e7\u00e3o de dimensionalidade, \u00e9 necess\u00e1rio juntar as bases para no final ter uma somente uma base de treino e uma de teste. Primeiramente as bases \"user_vectorized_descriptions_pc\" e \"user_vectorized_profile_images_pc\" foram unidas na base \"users\" por meio de um left join.","f78ec494":"## \u00c1rvore de decis\u00e3o","867bc259":"# Simula\u00e7\u00e3o de dados Teste","67edc7a2":"Para o processo de modelogem vamos testar varios modelos e diferentes, e ap\u00f3s isso vamos testar algumas t\u00e9cnicas de ensemble com esses modelos.\n\n","0bb33773":"## Transformando colunas booleanas em inteiras"}}