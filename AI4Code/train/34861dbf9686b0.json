{"cell_type":{"8edc81a4":"code","fe90c3df":"code","704d254a":"code","9ad5f371":"code","8a7dd885":"code","fbcd8f45":"code","39731bbe":"code","bd3636db":"code","9acaf243":"code","19a79547":"code","09dde04f":"code","149c41c9":"code","f496a5de":"code","d9e7a8bb":"code","e6db987d":"code","ef4c637a":"code","6b3824e0":"code","af73ae63":"code","bfb189d5":"code","12183f49":"code","3b412a89":"code","806ef736":"code","1d00e082":"code","36dad234":"code","21264348":"code","0325c381":"code","71fade00":"code","0ead8414":"code","e9775e6d":"code","1cea68b1":"code","569b4fd1":"code","f343da9d":"markdown","46442c7f":"markdown","d7372a45":"markdown","256e4353":"markdown","f743f22e":"markdown","8bf3b42c":"markdown","a024aa73":"markdown","890383f8":"markdown"},"source":{"8edc81a4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\ndf =pd.read_csv('\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndf.head()","fe90c3df":"df['gender'].value_counts()","704d254a":"df = df.drop(df[df['gender']== 'Other'].index)","9ad5f371":"sns.countplot(x='gender' , data = df, palette=\"Set3\")\nxlabel='gender' \nylabel='count'","8a7dd885":"sns.boxplot(x='smoking_status', y='age' , data = df, palette=\"Set3\")\nxlabel='smoking_status'\nylabel='age'","fbcd8f45":"sns.boxplot(x='bmi', y='work_type' , data = df,palette=\"Set3\")\nxlabel='bmi'\nylabel='work_type'","39731bbe":"g = sns.catplot(x=\"gender\", hue=\"work_type\", col=\"stroke\",\n...                 data=df, kind=\"count\",\n...                 height=4, aspect=.7, palette=\"Set2\");","bd3636db":"g = sns.catplot(x=\"gender\", hue=\"ever_married\", col=\"stroke\",\n...                 data=df, kind=\"count\",\n...                 height=4, aspect=.7, palette=\"Set2\");","9acaf243":"g = sns.catplot(x=\"gender\", hue=\"heart_disease\", col=\"stroke\",\n...                 data=df, kind=\"count\",\n...                 height=4, aspect=.7, palette=\"husl\")","19a79547":"g = sns.catplot(x=\"gender\", hue=\"Residence_type\", col=\"stroke\",\n...                 data=df, kind=\"count\",\n...                 height=4, aspect=.7, palette=\"Set3\")","09dde04f":"sns.countplot(x='smoking_status',data=df, hue= 'gender', palette= \"Set3\")\nxlabel='Smoking status' \nylabel='count'","149c41c9":"g = sns.catplot(x=\"gender\", hue=\"smoking_status\", col=\"stroke\",\n...                 data=df, kind=\"count\",\n...                 height=4, aspect=.7, palette=\"Set3\")","f496a5de":"df.select_dtypes(['object']).columns","d9e7a8bb":"dummies = pd.get_dummies(df[['gender','ever_married','work_type','Residence_type','smoking_status']],drop_first=True)\ndf = pd.concat([df,dummies],axis=1)\ndf = df.drop(['gender','ever_married','work_type','Residence_type','smoking_status'], axis=1)\ndf= df.drop(['id'], axis=1)","e6db987d":"df.columns","ef4c637a":"df.isnull().sum()","6b3824e0":"from sklearn.impute import KNNImputer\nimputer = KNNImputer(n_neighbors=5)\ndataset = pd.DataFrame(imputer.fit_transform(df), columns = df.columns)","af73ae63":"dataset.isnull().sum()","bfb189d5":"plt.figure(figsize=(12,8))\nsns.heatmap(dataset.corr(), annot = True )\nxlabel='' \nylabel=''","12183f49":"X = dataset.drop('stroke',axis=1).values\ny = dataset['stroke'].values","3b412a89":"from imblearn import under_sampling, over_sampling\nprint(\"Before OverSampling, counts of label '1': {}\".format(sum(y == 1))) \nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y == 0))) \n  \n# import SMOTE module from imblearn library  \n\nfrom imblearn.combine import SMOTEENN \nsm = SMOTEENN(random_state=10) \nX_over, y_over = sm.fit_resample(X, y) \n  \nprint('After OverSampling, the shape of train_X: {}'.format(X_over.shape)) \nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_over.shape)) \n  \nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_over == 1))) \nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_over == 0)))","806ef736":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\nX_train, X_test, y_train, y_test = train_test_split(X_over, y_over, test_size=0.20, random_state=7)","1d00e082":"from sklearn import preprocessing\nX_train_norm= preprocessing.normalize(X_train)\nX_test_norm =preprocessing.normalize(X_test)","36dad234":"from sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\n\n# prepare configuration for cross validation test harness\nseed = 7\n#MODELS\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))","21264348":"results = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=10)\n    cv_results = model_selection.cross_val_score(model, X_train_norm, y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n# boxplot algorithm comparison\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","0325c381":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier()\ndtree.fit(X_train_norm,y_train)\npredictions = dtree.predict(X_test_norm)","71fade00":"print(classification_report(y_test,predictions))\nprint(confusion_matrix(y_test,predictions))","0ead8414":"from sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier(n_neighbors=5)\nneigh.fit(X_train_norm, y_train)\npred = neigh.predict(X_test_norm)","e9775e6d":"print(classification_report(y_test,pred))\nprint(confusion_matrix(y_test,pred))","1cea68b1":"from sklearn.neural_network import MLPClassifier\nclf = MLPClassifier(hidden_layer_sizes=(150,100,50), max_iter=1000,activation = 'relu',solver='adam',random_state=1)\nclf.fit(X_train_norm, y_train)\npred_mlp= clf.predict(X_test_norm)","569b4fd1":"print(classification_report(y_test,pred_mlp))\nprint(confusion_matrix(y_test,pred_mlp))","f343da9d":"# BEFORE START THE TRAINING I WILL NORMALIZE MY DATA","46442c7f":"#  **DATA PRE-PROCESSING**","d7372a45":"#  DECISION TREE CLASIFICATION","256e4353":"# COMPARTION OF ALGORITHMS","f743f22e":"# CHECKING FOR NULL VALUES AND USING KNNImputer","8bf3b42c":"# KNN","a024aa73":"#  MLPClassifier","890383f8":"# OVERSAMPLING MY DATASET DUE THE DATA IS UNEVEN"}}