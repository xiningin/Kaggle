{"cell_type":{"b6d8ecdc":"code","2b4f733c":"code","62546c12":"code","e4df62fd":"code","ad5ec973":"code","25af9bb1":"markdown","7ad155cd":"markdown"},"source":{"b6d8ecdc":"import numpy as np\nimport pandas as pd","2b4f733c":"df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')","62546c12":"### cabin: delete\ndel df['Cabin']\ndel test_df['Cabin']\n### Ticket: delete\ndel df['Ticket']\ndel test_df['Ticket']\n\n\n### age: knn imputation\nfrom sklearn.impute import KNNImputer\ndf['Age'] = KNNImputer(n_neighbors=5).fit_transform(df[['Age']])\ntest_df['Age'] = KNNImputer(n_neighbors=5).fit_transform(test_df[['Age']])\n\n### fare: treat null data\ntest_df['Fare'] = KNNImputer(n_neighbors=5).fit_transform(test_df[['Fare']])\ndf['Fare'] = np.log(df['Fare'] + np.finfo(float).eps)\ntest_df['Fare'] = np.log(test_df['Fare'] + np.finfo(float).eps)\n\n### name: encoding\ndf['Name'] = df['Name'].str.extract('([A-Za-z]*)\\.')\ntest_df['Name'] = test_df['Name'].str.extract('([A-Za-z]*)\\.')\nname_map = {name: i for i, name in enumerate(set(df['Name']).union(set(test_df['Name'])))}\ndf['Name'] = df['Name'].map(name_map)\ntest_df['Name'] = test_df['Name'].map(name_map)\n\n### sex: encoding\nfrom sklearn.preprocessing import LabelEncoder\nle_sex = LabelEncoder().fit(df['Sex'])\ndf['Sex'] = le_sex.transform(df['Sex'])\ntest_df['Sex'] = le_sex.transform(test_df['Sex'])\n\n### embarked: imputation and encoding\nfrom sklearn.impute import SimpleImputer\ndf['Embarked'] = SimpleImputer(strategy='most_frequent').fit_transform(df[['Embarked']])\nle_embarked = LabelEncoder().fit(df['Embarked'])\ndf['Embarked'] = le_embarked.transform(df['Embarked'])\ntest_df['Embarked'] = le_embarked.transform(test_df['Embarked'])","e4df62fd":"from sklearn.ensemble import VotingClassifier, RandomForestClassifier, BaggingClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.model_selection import cross_val_score\n\nX = df.iloc[:, 2:]\ny = df['Survived']\n\nvc = VotingClassifier(estimators=[\n    ('lgbm', LGBMClassifier(n_estimators=200, max_depth=4)),\n    ('rf', RandomForestClassifier(n_estimators=200, max_depth=4, \n                                  criterion='entropy', bootstrap=False)),\n    ('lr', LogisticRegression()),\n    ('lda', LinearDiscriminantAnalysis()),\n    ('qda', QuadraticDiscriminantAnalysis()),\n    ('bc', BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=10, criterion='entropy'),\n                      n_estimators=100,\n                      bootstrap=False,\n                      max_features=1.0,\n                      bootstrap_features=True,\n                      max_samples=1.0))\n], voting='soft', weights=[1.0, 1.2, 0.5, 0.3, 0.2, 1])\nvc.fit(X, y)\n\nresult = cross_val_score(vc, X, y, scoring='accuracy', cv=10)\nprint(result.mean(), result.std())","ad5ec973":"### \uc81c\ucd9c\ud30c\uc77c\ntestX = test_df.iloc[:, 1:]\n\ny_pred = vc.predict(testX)\nanswer = pd.concat([test_df, pd.Series(y_pred, name='Survived')], axis=1)[['PassengerId','Survived']]\nanswer.to_csv('titanic_answer8.csv', index=False)","25af9bb1":"### Data Preparation","7ad155cd":"1. This notebook does NOT include all the steps (very simple version)\n2. You can get top 13% score"}}