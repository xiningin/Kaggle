{"cell_type":{"bf06108c":"code","5622a216":"code","bd2adf94":"code","de0c636f":"code","1daf62f8":"code","a8f847d3":"code","a7fe363f":"code","34f49cfd":"code","4cc0073b":"code","06de793d":"code","92fad26a":"code","ac3ff876":"code","bcda438a":"code","bfae2d24":"code","ba8626d9":"code","a8f03ba1":"code","f0f69572":"code","09233d2a":"code","4914c16e":"code","117751a0":"code","485e91ff":"code","aa227013":"code","58a0a135":"code","5503f7e8":"code","e5f018c5":"code","165647bf":"code","fdbee3fd":"code","462eab09":"code","c0abecc5":"code","5aef842d":"code","dc397d73":"code","cf369bd6":"code","1d291b72":"code","c7b7765c":"code","360a12f7":"code","14d880e1":"code","30bbfa23":"code","b74def04":"code","3516c42f":"code","d8d634d2":"code","1a6c38bd":"code","67de78be":"code","a2a98c2b":"code","55525295":"code","89bb6014":"code","4533e635":"code","3b5ce824":"code","3fd8161b":"code","a162d560":"code","ec318bf1":"code","ea7bc845":"code","4a4f9e68":"code","565e0e5d":"code","598aae69":"code","6eb37526":"markdown","53c49a3f":"markdown","4e20612e":"markdown","d211945e":"markdown","2347b194":"markdown","fe60bdfc":"markdown","eb60e463":"markdown","d809e858":"markdown","945237da":"markdown","7a64b3a7":"markdown","cd0f4530":"markdown","5d4a02ba":"markdown","47211ad3":"markdown","e1973df4":"markdown","71c05a0b":"markdown","67bf70c9":"markdown","9a065a0f":"markdown","0f983008":"markdown","a329659a":"markdown"},"source":{"bf06108c":"# to load, access, process and dump json files\nimport json\n# regular repression\nimport re\n# to parse HTML contents\nfrom bs4 import BeautifulSoup\n\n# for numerical analysis\nimport numpy as np \n# to store and process in a dataframe\nimport pandas as pd \n\n# for ploting graphs\nimport matplotlib.pyplot as plt\n# advancec ploting\nimport seaborn as sns\n# to create word clouds\nfrom wordcloud import WordCloud, STOPWORDS \n\n# To encode values\nfrom sklearn.preprocessing import LabelEncoder\n# Convert a collection of text documents to a matrix of token counts\nfrom sklearn.feature_extraction.text import CountVectorizer\n# confusion matrix\nfrom sklearn.metrics import confusion_matrix\n# train test split\nfrom sklearn.model_selection import train_test_split\n\n# for deep learning \nimport tensorflow as tf\n# to tokenize text\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n# to pad sequence \nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","5622a216":"def plot_ngram(sentiment, n):\n    \n    temp_df = df[df['sentiment'] == sentiment]\n    \n    word_vectorizer = CountVectorizer(ngram_range=(n, n), analyzer='word')\n    sparse_matrix = word_vectorizer.fit_transform(temp_df['review'])\n    \n    frequencies = sum(sparse_matrix).toarray()[0]\n    \n    return pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])\\\n            .sort_values(by='frequency', ascending=False) \\\n            .reset_index() \\\n            .head(10)","bd2adf94":"def plot_wordcloud(review, cmap):\n    fig, ax = plt.subplots(figsize=(8, 6))\n    wc = WordCloud(max_words = 1000, background_color ='white', stopwords = stopwords, \n                   min_font_size = 10, colormap=cmap)\n    wc = wc.generate(review)\n    plt.axis('off')\n    plt.imshow(wc)","de0c636f":"# to plot model accuracy and loss\n\ndef plot_history(history):\n    \n    plt.figure(figsize=(20, 5))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['accuracy'], label='Training Accuracy', c='green', lw='2')\n    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', c='orangered', lw='2')\n    plt.title('Accuracy', loc='left', fontsize=16)\n    plt.xlabel(\"Epochs\")\n    plt.ylabel('Accuracy')\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'], label='Training Loss', c='green', lw='2')\n    plt.plot(history.history['val_loss'], label='Validation Loss', c='orangered', lw='2')\n    plt.title('Loss', loc='left', fontsize=16)\n    plt.xlabel(\"Epochs\")\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.show()","1daf62f8":"# to plot confusion matrix\n# ========================\n\ndef plot_cm(pred, ticklabels, figsize):\n    \n    pred = pred.ravel()\n    pred = np.round(pred)\n      \n    fig, ax = plt.subplots(1, 1, figsize=(figsize, figsize))\n\n    cm = confusion_matrix(validation_labels, pred)\n    sns.heatmap(cm, annot=True, cbar=False, fmt='1d', cmap='Blues', ax=ax)\n\n    ax.set_xlabel('Predicted')\n    ax.set_ylabel('Actual')\n    ax.set_xticklabels(ticklabels)\n    ax.set_yticklabels(ticklabels, rotation=0)\n\n    plt.show()","a8f847d3":"# read the data\ndf = pd.read_csv(\"..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\")\n\n# shape\nprint('No. of rows and columns :', df.shape)\n\n# first few rows\ndf.head()","a7fe363f":"# Stopwords list from https:\/\/github.com\/Yoast\/YoastSEO.js\/blob\/develop\/src\/config\/stopwords.js\nstopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n\n# specific stopwords\nspecific_sw = ['br', 'movie', 'film']\n\n# all stopwords\nstopwords = stopwords + specific_sw","34f49cfd":"sns.set_style('darkgrid')\nplt.figure(figsize=(4, 5))\nsns.countplot(df['sentiment'], palette=['teal', 'orangered'])\nplt.title('No. of reviews in with each sentiment')\nplt.xlabel(\"\")\nplt.ylabel(\"\")\nplt.show()","4cc0073b":"# word cloud on positve reviews\npos_rev = ' '.join(df[df['sentiment']=='positive']['review'].to_list()[:10000])\nplot_wordcloud(pos_rev, 'Blues')","06de793d":"# word cloud on positve reviews\nneg_rev = ' '.join(df[df['sentiment']=='negative']['review'].to_list()[:10000])\nplot_wordcloud(neg_rev, 'Reds')","92fad26a":"plot_ngram('positive', 1)","ac3ff876":"plot_ngram('negative', 1)","bcda438a":"plot_ngram('positive', 2)","bfae2d24":"plot_ngram('negative', 2)","ba8626d9":"plot_ngram('positive', 3)","a8f03ba1":"plot_ngram('negative', 3)","f0f69572":"# to remove non alphanumeric character\ndef alpha_num(text):\n    return re.sub(r'[^A-Za-z0-9 ]', '', text)\n\n# to remove the stopwords from text\ndef remove_stopwords(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stopwords:\n            final_text.append(i.strip())\n    return \" \".join(final_text)\n\n# to remove URLs\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\n# to remove html tags\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)","09233d2a":"# apply preprocessing steps\n\ndf['review'] = df['review'].apply(remove_URL)\ndf['review'] = df['review'].apply(remove_html)\ndf['review'] = df['review'].str.lower()\ndf['review'] = df['review'].apply(alpha_num)\ndf['review'] = df['review'].apply(remove_stopwords)\n\ndf.head()","4914c16e":"# container for sentences\nreviews = np.array([review for review in df['review']])\n\n# container for labels\nlabels = np.array([label for label in df['sentiment']])","117751a0":"# label encoding labels \n\nenc = LabelEncoder()\nencoded_labels = enc.fit_transform(labels)\n\nprint(enc.classes_)\nprint(labels[:5])\nprint(encoded_labels[:5])","485e91ff":"# train-test split\ntrain_sentences, validation_sentences, train_labels, validation_labels = train_test_split(reviews, encoded_labels, \n                                                                                          test_size=0.33, \n                                                                                          stratify=labels)","aa227013":"# tokenize sentences\ntokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(train_sentences)\nword_index = tokenizer.word_index\n\n# convert train dataset to sequence and pad sequences\ntrain_sequences = tokenizer.texts_to_sequences(train_sentences)\ntrain_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length)\n\n# convert validation dataset to sequence and pad sequences\nvalidation_sequences = tokenizer.texts_to_sequences(validation_sentences)\nvalidation_padded = pad_sequences(validation_sequences, padding=padding_type, maxlen=max_length)","58a0a135":"# model parameters\n\nvocab_size = len(word_index)\nembedding_dim = 100\nmax_length = 120\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"","5503f7e8":"# model initialization\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# compile model\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\n# model summary\nmodel.summary()","e5f018c5":"# fit model\nnum_epochs = 10\nhistory = model.fit(train_padded, train_labels, \n                    epochs=num_epochs, verbose=2, \n                    validation_split=0.3)\n\n# predict values\npred = model.predict(validation_padded)","165647bf":"# plot history\nplot_history(history)","fdbee3fd":"# plot confusion matrix\nplot_cm(pred, enc.classes_, 5)","462eab09":"# reviews on which we need to predict\nsentence = [\"The movie was very touching and heart whelming\", \n            \"I have never seen a terrible movie like this\"]\n\n# convert to a sequence\nsequences = tokenizer.texts_to_sequences(sentence)\n\n# pad the sequence\npadded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\n# preict the label\nprint(model.predict(padded))","c0abecc5":"# model initialization\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# compile model\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\n# model summary\nmodel.summary()","5aef842d":"# fit model\nnum_epochs = 10\nhistory = model.fit(train_padded, train_labels, \n                    epochs=num_epochs, verbose=2, \n                    validation_split=0.33)\n\n# predict values\npred = model.predict(validation_padded)","dc397d73":"# plot history\nplot_history(history)","cf369bd6":"# plot confusion matrix\nplot_cm(pred, enc.classes_, 5)","1d291b72":"# reviews on which we need to predict\nsentence = [\"The movie was very touching and heart whelming\", \n            \"I have never seen a terrible movie like this\"]\n\n# convert to a sequence\nsequences = tokenizer.texts_to_sequences(sentence)\n\n# pad the sequence\npadded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\n# preict the label\nprint(model.predict(padded))","c7b7765c":"# model initialization\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# compile model\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\n# model summary\nmodel.summary()","360a12f7":"# fit model\nnum_epochs = 10\nhistory = model.fit(train_padded, train_labels, \n                    epochs=num_epochs, verbose=2, \n                    validation_split=0.33)\n\n# predict values\npred = model.predict(validation_padded)","14d880e1":"# plot history\nplot_history(history)","30bbfa23":"# plot confusion matrix\nplot_cm(pred, enc.classes_, 5)","b74def04":"# reviews on which we need to predict\nsentence = [\"The movie was very touching and heart whelming\", \n            \"I have never seen a terrible movie like this\"]\n\n# convert to a sequence\nsequences = tokenizer.texts_to_sequences(sentence)\n\n# pad the sequence\npadded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\n# preict the label\nprint(model.predict(padded))","3516c42f":"# model initialization\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64)),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# compile model\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\n# model summary\nmodel.summary()","d8d634d2":"# fit model\nnum_epochs = 10\nhistory = model.fit(train_padded, train_labels, \n                    epochs=num_epochs, verbose=2, \n                    validation_split=0.33)\n\n# predict values\npred = model.predict(validation_padded)","1a6c38bd":"# plot history\nplot_history(history)","67de78be":"# plot confusion matrix\nplot_cm(pred, enc.classes_, 5)","a2a98c2b":"# reviews on which we need to predict\nsentence = [\"The movie was very touching and heart whelming\", \n            \"I have never seen a terrible movie like this\"]\n\n# convert to a sequence\nsequences = tokenizer.texts_to_sequences(sentence)\n\n# pad the sequence\npadded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\n# preict the label\nprint(model.predict(padded))","55525295":"# model initialization\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n    tf.keras.layers.GlobalMaxPooling1D(),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# compile model\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\n# model summary\nmodel.summary()","89bb6014":"# fit model\nnum_epochs = 10\nhistory = model.fit(train_padded, train_labels, \n                    epochs=num_epochs, verbose=2, \n                    validation_split=0.33)\n\n# predict values\npred = model.predict(validation_padded)","4533e635":"# plot history\nplot_history(history)","3b5ce824":"# plot confusion matrix\nplot_cm(pred, enc.classes_, 5)","3fd8161b":"# reviews on which we need to predict\nsentence = [\"The movie was very touching and heart whelming\", \n            \"I have never seen a terrible movie like this\"]\n\n# convert to a sequence\nsequences = tokenizer.texts_to_sequences(sentence)\n\n# pad the sequence\npadded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\n# preict the label\nprint(model.predict(padded))","a162d560":"# Note this is the 100 dimension version of GloVe from Stanford\n# laurencemoroney unzipped and hosted it on his site to make this notebook easier\n\n!wget --no-check-certificate \\\n    https:\/\/storage.googleapis.com\/laurencemoroney-blog.appspot.com\/glove.6B.100d.txt \\\n    -O \/tmp\/glove.6B.100d.txt\n\nembeddings_index = {}\n\nwith open('\/tmp\/glove.6B.100d.txt') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n\nembeddings_matrix = np.zeros((vocab_size+1, embedding_dim))\n\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector","ec318bf1":"# model initialization\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Conv1D(64, 5, activation='relu'),\n    tf.keras.layers.MaxPooling1D(pool_size=4),\n    tf.keras.layers.LSTM(64),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n# compile model\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\n# model summary\nmodel.summary()","ea7bc845":"# fit model\nnum_epochs = 10\nhistory = model.fit(train_padded, train_labels, \n                    epochs=num_epochs, verbose=2, \n                    validation_split=0.33)\n\n# predict values\npred = model.predict(validation_padded)","4a4f9e68":"# plot history\nplot_history(history)","565e0e5d":"# plot confusion matrix\nplot_cm(pred, enc.classes_, 5)","598aae69":"# reviews on which we need to predict\nsentence = [\"The movie was very touching and heart whelming\", \n            \"I have never seen a terrible movie like this\"]\n\n# convert to a sequence\nsequences = tokenizer.texts_to_sequences(sentence)\n\n# pad the sequence\npadded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\n# preict the label\nprint(model.predict(padded))","6eb37526":"## Get review and labels","53c49a3f":"## Utility Function","4e20612e":"## Label encode sentiment","d211945e":"## Plots","2347b194":"## No. of reviews in with each sentiment","fe60bdfc":"## Preprocessing Data","eb60e463":"## Libraries","d809e858":"## Train Test Split","945237da":"# LSTM - 2 Layer","7a64b3a7":"## Tokenize and Sequence text","cd0f4530":"## About the Dataset\nA set of 25,000 highly polar movie reviews for training and 25,000 for testing. \n\n## To Do\nPredict wether the review is positive or negative ","5d4a02ba":"## Dataset","47211ad3":"# With Convolution","e1973df4":"## Stopwords","71c05a0b":"# With Word Embedding","67bf70c9":"## Model parameters","9a065a0f":"# With GloVe","0f983008":"# With GRU","a329659a":"# With LSTM"}}