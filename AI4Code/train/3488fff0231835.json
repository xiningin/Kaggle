{"cell_type":{"0e561a93":"code","d4d5523a":"code","2a942616":"code","8cecff4c":"code","7db46051":"code","a49a942b":"code","0fed3b67":"code","1f03df2c":"code","4d56fd23":"code","7d194bbc":"code","cf273aa6":"code","d50a69e4":"code","0b235ce0":"code","873032b7":"code","44012b91":"code","256061b9":"code","9aff9487":"code","156cf457":"code","3a1004f9":"code","68c6c927":"code","cafa08f7":"code","6bdbe148":"code","42c4acc6":"code","3a156c35":"code","84686300":"code","8d84027a":"code","93a00ba0":"code","0d04a85c":"code","49416605":"code","621b4207":"code","1924a619":"code","755a12f2":"code","f304af4a":"markdown"},"source":{"0e561a93":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os, gc\n\n# Any results you write to the current directory are saved as output.","d4d5523a":"from transformers import *\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow as tf\nfrom tqdm.autonotebook import  tqdm\nfrom ast import literal_eval\nimport tensorflow.keras.layers as L","2a942616":"\n\nAUTO = tf.data.experimental.AUTOTUNE\n\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\nstrategy = tf.distribute.experimental.TPUStrategy(tpu)\n\nGCS_DS_PATH = KaggleDatasets().get_gcs_path('jigsaw-multilingual-toxic-comment-classification')\n\nprint(\"PATH: \", GCS_DS_PATH)\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n","8cecff4c":"EPOCHS = 2\nBATCH_SIZE = 64 * strategy.num_replicas_in_sync","7db46051":"%%time\ntrain_nobias = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-unintended-bias-train.csv\")\n\ntrain_base = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv\")\nvalid_base = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/validation.csv\")\ntest_base = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/test.csv\")\n\ntrain_pre = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train-processed-seqlen128.csv\")\nvalid_pre = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/validation-processed-seqlen128.csv\")\ntest_pre = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/test-processed-seqlen128.csv\")\n\nsub = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/sample_submission.csv\")","a49a942b":"MODEL_TYPE =  \"bert-base-multilingual-cased\"","0fed3b67":"tokenizer = BertTokenizer.from_pretrained(MODEL_TYPE)","1f03df2c":"train_base.shape","4d56fd23":"def tokenize_all(texts, tokenizer, chunk_size=512 ,max_len=512):\n    ids = []\n    \n    for i in tqdm(range(0, len(texts), chunk_size)):\n        chunk_ids = tokenizer.batch_encode_plus(texts[i: i+chunk_size])\n        ids.append(chunk_ids[\"input_ids\"])\n    \n    return ids\n    \n#train_ids = tokenize_all(train_base.comment_text.values, tokenizer)\n#valid_ids = tokenize_all(valid_base.comment_text.values, tokenizer)\n#test_ids = tokenize_all(test_base.comment_text.values, tokenizer)","7d194bbc":"train_pre.head()","cf273aa6":"%%time\ntrain_pre[\"input_word_ids\"] = train_pre[\"input_word_ids\"].apply(literal_eval)\nvalid_pre[\"input_word_ids\"] = valid_pre[\"input_word_ids\"].apply(literal_eval)\ntest_pre[\"input_word_ids\"] = test_pre[\"input_word_ids\"].apply(literal_eval)\n\ntrain_pre[\"input_mask\"] = train_pre[\"input_mask\"].apply(literal_eval)\nvalid_pre[\"input_mask\"] = valid_pre[\"input_mask\"].apply(literal_eval)\ntest_pre[\"input_mask\"] = test_pre[\"input_mask\"].apply(literal_eval)","d50a69e4":"train_ids = train_pre.loc[:, [\"input_word_ids\", \"input_mask\"]]\nvalid_ids = valid_pre.loc[:, [\"input_word_ids\", \"input_mask\"]]\ntest_ids  = test_pre.loc[:, [\"input_word_ids\", \"input_mask\"]]","0b235ce0":"gc.collect()","873032b7":"train_targets = train_pre.loc[:, [\"toxic\"]]#, \"severe_toxic\", \"obscene\", \"threat\",\t\"insult\", \"identity_hate\"]]\nvalid_targets = valid_pre.loc[:, [\"toxic\"]]#, \"severe_toxic\", \"obscene\", \"threat\",\t\"insult\", \"identity_hate\"]]","44012b91":"%%time\ntrain_ids[\"input_word_ids\"] = train_ids[\"input_word_ids\"].apply(np.array, dtype=np.int32)\ntrain_ids[\"input_mask\"] = train_ids[\"input_mask\"].apply(np.array, dtype=np.int32)\n\nvalid_ids[\"input_word_ids\"] = valid_ids[\"input_word_ids\"].apply(np.array, dtype=np.int32)\nvalid_ids[\"input_mask\"] = valid_ids[\"input_mask\"].apply(np.array, dtype=np.int32)\n\ntest_ids[\"input_word_ids\"] = test_ids[\"input_word_ids\"].apply(np.array, dtype=np.int32)\ntest_ids[\"input_mask\"] = test_ids[\"input_mask\"].apply(np.array, dtype=np.int32)","256061b9":"train_data = (tf.convert_to_tensor(train_ids.iloc[:, 0]), tf.convert_to_tensor(train_ids.iloc[:, 1]))\nvalid_data = (tf.convert_to_tensor(valid_ids.iloc[:, 0]), tf.convert_to_tensor(valid_ids.iloc[:, 1]))\ntest_data = (tf.convert_to_tensor(test_ids.iloc[:, 0]), tf.convert_to_tensor(test_ids.iloc[:, 1]))","9aff9487":"gc.collect()","156cf457":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_data[0], train_targets.values))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((valid_data[0], valid_targets.values))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(test_data[0])\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)","3a1004f9":"def get_multi_classifier(base_model, n_classes, max_len=128):\n    input_i = L.Input(shape=(max_len, ), dtype=tf.int32)\n    #input_m = L.Input(shape=(max_len, ), dtype=tf.int32)\n    \n    print(input_i.shape)\n    op1, op2 = base_model(input_i)\n    print(op1.shape)\n    print(op2.shape)\n    cls_token = op1[:, 0, :]\n    \"\"\"\n    cls_token = L.Dense(256, activation=\"relu\")(cls_token)\n    cls_token = L.Dropout(0.1)(cls_token)\n    \"\"\"\n    out = L.Dense(n_classes, activation='sigmoid')(cls_token)\n    \n    model = tf.keras.models.Model(inputs = input_i, outputs = out)\n    model.compile(optimizer=tf.keras.optimizers.Adam(lr=1.5e-5), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n    \n    return model\n    \n    ","68c6c927":"import gc\ngc.collect()","cafa08f7":"%%time\nwith strategy.scope():\n    base_model = TFBertModel.from_pretrained(MODEL_TYPE)\n    model_ = get_multi_classifier(base_model, 1, 128)\n    \n    ","6bdbe148":"model_.summary()","42c4acc6":"from tensorflow.keras.callbacks import Callback","3a156c35":"model_.fit(train_dataset,\n           epochs=15,\n           steps_per_epoch=train_data[0].shape[0]\/\/BATCH_SIZE,\n           validation_data=valid_dataset,\n           validation_steps=valid_data[0].shape[0]\/\/BATCH_SIZE,\n           callbacks = [])","84686300":"ps = model_.predict(test_dataset, verbose=1)","8d84027a":"test_pre.head()","93a00ba0":"ps_s=np.array(ps).squeeze()","0d04a85c":"final = pd.DataFrame({\"id\":test_pre[\"id\"].values, \"toxic\":ps_s})","49416605":"ps_s.mean()","621b4207":"(ps_s>0.5).mean()","1924a619":"final.head()","755a12f2":"final.to_csv(\"submission.csv\", index=False)","f304af4a":"Modelling "}}