{"cell_type":{"ccab4c4c":"code","7565501e":"code","7ba01976":"code","3f77ae1a":"code","5b1d4d05":"code","8de2700c":"code","1f112e3d":"code","b83fc354":"markdown","453780f6":"markdown","b8ecb1ea":"markdown","65cd32d6":"markdown","870aaab8":"markdown","1f6b0dbe":"markdown","2d90a8bb":"markdown","638a8e8b":"markdown","81533181":"markdown","4a745bb6":"markdown"},"source":{"ccab4c4c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport matplotlib.pyplot as plt\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","7565501e":"df2 = pd.read_csv(\"..\/input\/UNIVARIATE polynomial REG.csv\")\n\nplt.scatter(df2['X'], df2['Y'])","7ba01976":"m_tot = df2['X'].count()\n#****************\nteta0_v = -25\nlearning_rate_teta0 = 1.5\nlearning_rate_teta1 = 1.5\n#****************\nY_pred = []\nError = []\nteta1 = []\nteta0 = []\nderivative_teta1 = []\nderivative_teta0 = []\n\n# loop teta0\nfor j in range(0, 50):\n  #*****************\n  teta1_v = -25\n  #*****************\n  \n  # loop teta1\n  for i in range(0, 50):\n    Y_pred = []\n    \n    # loop Xi\n    for m in range(0, m_tot):\n      Y_pred.append(np.sqrt(df2['X'][m])*teta0_v + (df2['X'][m])**7*teta1_v)\n\n    df2['Y_pred'] = Y_pred\n    df2['SE'] = (df2['Y_pred'] - df2['Y'])**2\n    df2['SE_derivative_teta1'] = (df2['Y_pred'] - df2['Y'])*df2['X']\n    df2['SE_derivative_teta0'] = (df2['Y_pred'] - df2['Y'])\n  #  plt.figure()\n    plt.scatter(df2['X'], df2['Y'])\n    plt.scatter(df2['X'], df2['Y_pred'])\n    Error.append(sum(df2['SE'])\/(2*m_tot))\n    derivative_teta1.append(sum(df2['SE_derivative_teta1'])\/(m_tot))\n    derivative_teta0.append(sum(df2['SE_derivative_teta0'])\/(m_tot))\n    teta0.append(teta0_v)\n    teta1.append(teta1_v)\n    teta1_v = teta1_v + learning_rate_teta1\n  \n  teta0_v = teta0_v + learning_rate_teta0\n\nJ = pd.DataFrame()\nJ['teta1'] = teta1\nJ['teta0'] = teta0\nJ['error'] = Error\nJ['derivative_teta1'] = derivative_teta1\nJ['derivative_teta0'] = derivative_teta0\nplt.figure()\nplt.plot(J['teta1'], J['error'])\nplt.plot(J['teta0'], J['error'])","3f77ae1a":"J.round(2)\nplt.figure()\nplt.plot(J['teta1'], J['derivative_teta1'])\nplt.plot(J['teta0'], J['derivative_teta0'])","5b1d4d05":"from mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure()\nax1 = fig.add_subplot(111,projection='3d')\nax1.scatter(J['teta1'],J['teta0'],J['error'])\nplt.xlabel('teta1', fontsize=16)\nplt.ylabel('error', fontsize=16)\nplt.ylabel('teta0', fontsize=16)\nax1.view_init(30, 30)\n\n# rotate the axes and update\n#for angle in range(0, 360):\n#    ax1.view_init(30, angle)\n#    plt.draw()\n#    plt.pause(.001)","8de2700c":"import scipy.interpolate\n\nN = 500 #number of points for plotting\/interpolation    \nx = J['teta0']\ny = J['teta1']\nz = J['error']\nxll = x.min();  xul = x.max();  yll = y.min();  yul = y.max()\n\nxi = np.linspace(xll, xul, N)\nyi = np.linspace(yll, yul, N)\nzi = scipy.interpolate.griddata((x, y), z, (xi[None,:], yi[:,None]), method='cubic')\n\ncontours = plt.contour(xi, yi, zi, 6, colors='white')\nplt.clabel(contours, inline=True, fontsize=13)\nplt.imshow(zi, extent=[xll, xul, yll, yul], origin='lower', cmap=plt.cm.jet, alpha=0.9)\nplt.xlabel(r'$teta0$')\nplt.ylabel(r'$teta1$')\nplt.clim(0, 10)\nplt.colorbar()\nplt.show()","1f112e3d":"Xarray = np.array([np.sqrt(df2['X']), df2['X']**7])\nyarray = np.array([df2['Y']])\n\nX = np.mat(Xarray).T\ny = np.mat(yarray).T\n\n#**********************************NORMAL EQUATION\nfrom numpy.linalg import inv\n\nA = X.T.dot(X)\nAinv = inv(A)\nTHETA = Ainv.dot(X.T).dot(y)\n\nparam = pd.DataFrame(THETA)\nparam.head()","b83fc354":"The above graph is a simple contour plot which is the same than the 3D plot.\n\nNow it is easy to read and we can see that the minima of the function is for **\u03b80 = -22** and **\u03b81 = 14**\n\nFinally we retrieve our parameters which minimized the Cost function.\n\nLet's compute the normal equation to confirm our parameters :\n\n**Normal equation :**  $\\theta=(X^TX)^{-1}X^T\\vec{y}$","453780f6":"The above 2 graphs are showing respectively : \n- All the hypothesis function we computed starting with **\u03b80 = -25** and **\u03b81 = -25** until **\u03b80 = 50** and **\u03b81 = 50**.\n- The error function for each **\u03b80** and **\u03b81** with **\u03b80** in green and **\u03b81** in blue.","b8ecb1ea":"In this notebook, we will compute the hypothesis and the Cost function for severral **\u03b80** and **\u03b81**.\n\nWe will then plot :\n    - The hypothesis in order to find a fit between actual output and our predicted output.\n    - The Cost function in order the find the minima of the error.","65cd32d6":"**The model is a Supervised Learning - Univariate polynomial regression with 2 parameters.**\n\nIn this example the hypothesis function looks like : $$h_\u03b8(x)=\\theta_0\\sqrt{x}+\\theta_1x^7$$ where **\u03b80** and **\u03b81** have to be defined.\n\nThe Cost function to be minimized is the Mean Squared Error :\n\n$$J(\u03b8)=\\frac{1}{2m}\\sum_{i}^{m}\\left[h_\u03b8(x_i)-y_i \\right]^2$$\n\n**m : number of examples in the data set**\n\n**y : value to be predicted**\n\n**xi : the ith example in the data set**\n","870aaab8":"The above function equation is :\n\n---\n\n$y=\\theta_0\\sqrt{x}+\\theta_1x^7$\n\n---\n\nLet's find $\\theta_0$ and $\\theta_1$","1f6b0dbe":"First, let's have a look on the data in order to see the trend and the model of the variable we want to predict.","2d90a8bb":"The above graph is a 3D plot of the Cost function according to the 2 parameters **\u03b80** and **\u03b81**.\n\nWith this graph, it is quite difficult to see the minima of this surface.\n\nWe will have a better view of the minima by plotting the contour plot of the above 3D function.","638a8e8b":"The above graph is showing the derivative of **\u03b80** and **\u03b81**.","81533181":"Following the gradient descent algorithm, we will also compute and plot the gradient of the Cost function.\n\n$$\\frac{dJ}{d\u03b8_1}=\\frac{1}{m}\\sum_{i}^{m}\\left[(h_\u03b8(x_i)-y_i \\right)x_i]$$\n\n$$\\frac{dJ}{d\u03b8_0}=\\frac{1}{m}\\sum_{i}^{m}\\left[h_\u03b8(x_i)-y_i \\right]$$","4a745bb6":"Let's see if we can retrieve this parameters computing a Cost function and then minimized this error.\n\nWe will compute this function for several values of **\u03b80** and **\u03b81** and then plot the graphs."}}