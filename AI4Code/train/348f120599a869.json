{"cell_type":{"ef569fb8":"code","efb9231b":"code","fbeb63b2":"code","4beec93a":"code","be158845":"code","60c42bb3":"code","50abf3ab":"code","c37bce88":"code","6ca4f251":"markdown","16de40bf":"markdown","f9b5ae11":"markdown","92ba7ef0":"markdown","242c1a89":"markdown","639f2ccd":"markdown"},"source":{"ef569fb8":"import os\n\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report","efb9231b":"%%time\n# Only load those columns in order to save space\nkeep_cols = ['event_id', 'game_session', 'installation_id', 'event_count', 'event_code', 'title', 'game_time', 'type', 'world']\n\ntrain = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/train.csv', usecols=keep_cols)\ntest = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/test.csv', usecols=keep_cols)\ntrain_labels = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/train_labels.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/sample_submission.csv')","fbeb63b2":"def group_and_reduce(df):\n    # group1 and group2 are intermediary \"game session\" groups,\n    # which are reduced to one record by game session. group1 takes\n    # the max value of game_time (final game time in a session) and \n    # of event_count (total number of events happened in the session).\n    # group2 takes the total number of event_code of each type\n    group1 = df.drop(columns=['event_id', 'event_code']).groupby(\n        ['game_session', 'installation_id', 'title', 'type', 'world']\n    ).max().reset_index()\n\n    group2 = pd.get_dummies(\n        df[['installation_id', 'event_code']], \n        columns=['event_code']\n    ).groupby(['installation_id']).sum()\n\n    # group3, group4 and group5 are grouped by installation_id \n    # and reduced using summation and other summary stats\n    group3 = pd.get_dummies(\n        group1.drop(columns=['game_session', 'event_count', 'game_time']),\n        columns=['title', 'type', 'world']\n    ).groupby(['installation_id']).sum()\n\n    group4 = group1[\n        ['installation_id', 'event_count', 'game_time']\n    ].groupby(\n        ['installation_id']\n    ).agg([np.sum, np.mean, np.std])\n\n    return group2.join(group3).join(group4)","4beec93a":"%%time\ntrain_small = group_and_reduce(train)\ntest_small = group_and_reduce(test)\n\nprint(train_small.shape)\ntrain_small.head()","be158845":"small_labels = train_labels[['installation_id', 'accuracy_group']].set_index('installation_id')\ntrain_joined = train_small.join(small_labels).dropna()\n\nx_train, x_val, y_train, y_val = train_test_split(\n    train_joined.drop(columns='accuracy_group').values,\n    train_joined['accuracy_group'].values,\n    test_size=0.15, random_state=2019\n)","60c42bb3":"train_set = lgb.Dataset(x_train, y_train)\nval_set = lgb.Dataset(x_val, y_val)\n\nparams = {\n    'learning_rate': 0.01,\n    'bagging_fraction': 0.9,\n    'feature_fraction': 0.9,\n    'num_leaves': 14,\n    'lambda_l1': 0.1,\n    'lambda_l2': 1,\n    'metric': 'multiclass',\n    'objective': 'multiclass',\n    'num_classes': 4,\n    'random_state': 2019\n}\n\nmodel = lgb.train(params, train_set, num_boost_round=10000, early_stopping_rounds=300, valid_sets=[train_set, val_set], verbose_eval=100)","50abf3ab":"val_pred = model.predict(x_val).argmax(axis=1)\nprint(classification_report(y_val, val_pred))","c37bce88":"y_pred = model.predict(test_small).argmax(axis=1)\ntest_small['accuracy_group'] = y_pred\ntest_small[['accuracy_group']].to_csv('submission.csv')","6ca4f251":"# Load Data","16de40bf":"# Training model","f9b5ae11":"# Evaluation","92ba7ef0":"# Group and Reduce","242c1a89":"# Submission","639f2ccd":"# About this notebook\n\nYou might have noticed that the train dataset is composed of over 11M data points, but there are only 17k training labels, and 1000k test labels you are predicting. The reason for that is there are many thousand different entries for each `installation_id`, each representing an `event`. This notebook simply gathers all the events into 17k groups, each group corresponds to an `installation_id`. Then, it takes the aggregation (using sums, counts, mean, std, etc.) of those groups, thus resulting in a dataset of summary statistics of each `installation_id`. After that, it simply fits a model on that dataset."}}