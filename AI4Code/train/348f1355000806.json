{"cell_type":{"fb61b56d":"code","896af495":"code","614e1c45":"code","9452f229":"code","4a7747d9":"code","ca2fd712":"code","c5855ca5":"code","1405672b":"code","8dfe8678":"code","96f974f2":"code","407bc6fe":"code","1ca1fade":"code","ce422c4e":"code","0a3290aa":"code","3cb12d0b":"code","a1abaa6a":"code","ae34f862":"code","10e4ccc9":"code","0f42ef33":"code","01620c3b":"code","72bc8a74":"code","87b80cfe":"markdown","2e1c677d":"markdown","c8e84c31":"markdown","3ffa0096":"markdown","85e23a71":"markdown","7083ac28":"markdown","d4135b48":"markdown"},"source":{"fb61b56d":"import os\nimport time\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom numpy.linalg import norm\nfrom tqdm import tqdm, tqdm_notebook\nfrom keras.preprocessing import image\nfrom keras.applications.resnet50 import ResNet50, preprocess_input","896af495":"#Initialize Model\nmodel = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n#Define Function to extract Features\ndef extract_features(img_path, model):\n    \n    #Preprocessing Input Image\n    input_shape = (224, 224, 3)\n    img = image.load_img(img_path, target_size=(input_shape[0], input_shape[1])) #Reshape input image size into target size\n    img_array = image.img_to_array(img)\n    expanded_img_array = np.expand_dims(img_array, axis=0)\n    preprocessed_img = preprocess_input(expanded_img_array)\n    \n    #Getting features from the Image\n    features = model.predict(preprocessed_img)\n    flattened_features = features.flatten()\n    normalized_features = flattened_features \/ norm(flattened_features)\n    return normalized_features","614e1c45":"#Find Images in the Root Directiry and making list of those Images\nextensions = ['.jpg', '.JPG', '.jpeg', '.JPEG', '.png', '.PNG']\ndef get_file_list(root_dir):\n    file_list = []\n    counter = 1\n    for root, directories, filenames in os.walk(root_dir):\n        for filename in filenames:\n            if any(ext in filename for ext in extensions):\n                file_list.append(os.path.join(root, filename))\n                counter += 1\n    return file_list","9452f229":"# path to the datasets\nroot_dir = '..\/input\/caltech101\/Caltech101\/Caltech101\/train'\nfilenames = sorted(get_file_list(root_dir))","4a7747d9":"#Extracting Features from each Image and saving them in a list\nfeature_list = []\nfor i in tqdm_notebook(range(len(filenames))):\n    feature_list.append(extract_features(filenames[i], model))","ca2fd712":"#Making pickle file of filenames and features of each files for future references\npickle.dump(feature_list, open('.\/features-caltech101-resnet.pickle', 'wb'))\npickle.dump(filenames, open('.\/filenames-caltech101.pickle','wb'))","c5855ca5":"#Getting filenames and features from pickle files\nfilenames = pickle.load(open('.\/filenames-caltech101.pickle', 'rb'))\nfeature_list = pickle.load(open('.\/features-caltech101-resnet.pickle', 'rb'))","1405672b":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\ntrain_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntrain_generator = train_datagen.flow_from_directory('..\/input\/caltech101\/Caltech101\/Caltech101\/train',\n                                                   class_mode='categorical')","8dfe8678":"print(train_generator.filenames[0])","96f974f2":"#Extracting image name and labels of each Image to make a dataframe that can be used later while using annoy\n\nfile_names = []\nlabels = []\nfor files in train_generator.filenames:\n    file = files.split('\/')[1]\n    label = files.split('\/')[0]\n    file_names.append(file)\n    labels.append(label)","407bc6fe":"#Forming dataframe containing Image Id, Features of Images and Labels of the Images\\\ndf = pd.DataFrame({'img_id':file_names, 'img_repr': feature_list, 'label': labels})","1ca1fade":"df.head()","ce422c4e":"len(df['img_repr'][0])","0a3290aa":"from annoy import AnnoyIndex\nimport random\n\nf = len(df['img_repr'][0])\nt = AnnoyIndex(f, metric='euclidean')\n\nfor i in tqdm(range(len(feature_list))):\n    t.add_item(i, feature_list[i])\n    \n_ = t.build(150)","3cb12d0b":"#Defining function to get similar images output in dataframe of the base image index we give as parameter\n\ndef get_similar_images_annoy(img_index):\n    start = time.time()\n    base_img_id, base_vector, base_label  = df.iloc[img_index, [0, 1, 2]]\n    similar_img_ids = t.get_nns_by_item(img_index, 4)\n    end = time.time()\n    # Now we want to get the 5 elements sorted by its euclidean distance relative to the image indexed item\n    print(f'{(end - start) * 1000} ms')\n    return base_img_id, base_label, df.iloc[similar_img_ids[1:]]","a1abaa6a":"#Querying the AnnoyIndex    \nbase_image, base_label, similar_images_df = get_similar_images_annoy(4000)","ae34f862":"print('Base Image Id:', base_image)\nprint('Base Image Label:', base_label)","10e4ccc9":"#Dataframe of similar images\nsimilar_images_df.head()","0f42ef33":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport cv2 as cv","01620c3b":"def show_images(root_dir):\n    plt.figure(figsize = (16,9))\n    \n    plt.subplot(1,4,1)\n    path = os.path.join(root_dir, base_label, base_image)\n    image = mpimg.imread(path)\n    plt.imshow(image)\n    plt.title('Base Image')\n    plt.axis('off')\n    \n    for i in range(len(similar_images_df)):\n        path = os.path.join(root_dir, similar_images_df.iloc[i,2],similar_images_df.iloc[i,0])\n        image = mpimg.imread(path)\n        plt.subplot(1,4,i+2)\n        plt.imshow(image)\n        plt.title('Similar Image')\n        plt.axis('off')","72bc8a74":"root_dir = '..\/input\/caltech101\/Caltech101\/Caltech101\/train'\nshow_images(root_dir)","87b80cfe":"****Explaination of the above code:****\n\n1.  This loads an image and resizes the image to (224, 224): img = image.load_img(img_path, target_size=(224, 224))\n\n2.  The img_to_array() function adds channels: x.shape = (224, 224, 3) for RGB and (224, 224, 1) for gray image:          x = image.img_to_array(img) \n\n3.  expand_dims() is used to add the number of images: x.shape = (1, 224, 224, 3): x = np.expand_dims(x, axis=0)\n\n4.  preprocess_input subtracts the mean RGB channels of the imagenet dataset. This is because the model you are using has     been trained on a different dataset: x.shape is still (1, 224, 224, 3): x = preprocess_input(x)","2e1c677d":"# Plotting Base and Similar Images","c8e84c31":"# Extracting Features from Images","3ffa0096":"# Importing Libraries","85e23a71":"****Explaination of the above code:****\n\n* In this first part, we set the number of dimensions of the space of our net to number of features extracted from image i.e. **100352** and we choose to use an euclidean distance.\n* Then, in the next step, we create a **6162** items i.e. equal to total number of images, each one with its correspondenly **100352** dimensional vector, which can be also understood as a point in a 100352th dimensional space.\n* We build then the corresponding net by using 1000 trees.","7083ac28":"# Using Spotify's Annoy for Similarity Search","d4135b48":"* **numpy.linalg.norm**: This function returns one of the seven matrix norms or one of the infinite vector norms depending upon the value of its parameters.\n* **preprocess_input**: The preprocess_input function is meant to adequate your image to the format the model requires.Some models use images with values ranging from 0 to 1. Others from -1 to +1. Others use the \"caffe\" style, that is not normalized, but is centered."}}