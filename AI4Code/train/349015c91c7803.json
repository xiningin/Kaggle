{"cell_type":{"3a83d66c":"code","97d8fe20":"code","b27bd9a3":"code","e0d161db":"code","4fc73481":"code","ff7521d6":"code","2bbb441f":"code","d595de40":"code","429986dc":"code","ff6ed39b":"code","4067634a":"code","d04b3395":"code","b77074b7":"code","ce6086d2":"code","e7d9fbc4":"code","5bb6945d":"code","dc3f33a3":"code","26f4f48d":"code","cae967cf":"code","0438ca02":"code","59f9f0a9":"code","16d16874":"code","db7af407":"code","de956088":"code","e0333928":"code","5f11a34b":"code","bd97b7fa":"code","874bdeec":"code","bb604c07":"code","dfca843c":"code","4fe7dd11":"code","e24ed195":"code","b231d24b":"code","3bc77b7d":"code","2006cc01":"code","ae1276ce":"code","f6fc7d9c":"code","6b5b98e8":"code","b2544c61":"code","9878f95e":"code","3fb5abae":"code","eb0414ac":"code","5a06d735":"code","516079a7":"code","dcf6168e":"code","6993af7e":"code","117047a2":"code","08ccc336":"code","274a9880":"code","d211e8ac":"code","ce384315":"code","604492cd":"code","9bf099a1":"code","6f9ddf95":"code","56db4f0c":"code","3760de09":"markdown","637f6900":"markdown","82280e64":"markdown","a15fbff9":"markdown","4b7ee1c9":"markdown","6e8a25fd":"markdown","575c69a1":"markdown","e022e279":"markdown","88357e43":"markdown","17a40b64":"markdown","d4f1d81f":"markdown","ac8e411b":"markdown","06f141f4":"markdown","ca26222a":"markdown","213e2e5e":"markdown","ab99474d":"markdown","9a6c31c8":"markdown","e9b9f5f6":"markdown","5c380720":"markdown","b8ce5345":"markdown","f1d7e60a":"markdown","38337be3":"markdown","0a616815":"markdown","182a4789":"markdown","db47447d":"markdown","0972196a":"markdown","c4353444":"markdown"},"source":{"3a83d66c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","97d8fe20":"data = pd.read_csv(\"..\/input\/used-car-dataset-ford-and-mercedes\/audi.csv\")\ndata.head()","b27bd9a3":"print(\"Data has\\033[1m\" , data.shape[0] , \"\\033[0mrows and\\033[1m\", data.shape[1] , \"\\033[0mcolumns\\n\\n\")\nprint(\"Column Name \\t Data Type\\n\")\nprint(data.dtypes) ","e0d161db":"data.isnull().sum()  # no missing values in the dataset","4fc73481":"print(\"There are\\033[1m\", data.duplicated().sum() ,\"\\033[0mduplicated values in the dataset\")","ff7521d6":"data_copy = data.copy() # create a copy of the original data","2bbb441f":"data_copy.drop_duplicates(inplace=True)  # drop the duplicates ","d595de40":"data_copy.shape # 10565 rows remaining","429986dc":"#import the libraries for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","ff6ed39b":"print(\"Types of Transmission:\\n\")\nprint(data_copy['transmission'].value_counts()\/len(data_copy)*100)\nprint(\"\\n\\n\")\nprint(\"Fuel Types:\\n\") \nprint(data_copy['fuelType'].value_counts()\/len(data_copy)*100)","4067634a":"print(\"There are\\033[1m\" , data_copy['model'].nunique(),\"\\033[0munique car models\")","d04b3395":"count_of_model = data_copy['model'].value_counts().sort_values() # series having count of models in asc order\nplt.figure(figsize=(15,8))\nax=sns.countplot(data=data_copy, x='model',order=count_of_model.index)\nplt.title('Frequency of each Model')\nplt.xlabel('Model')\nplt.ylabel('Count')\nfor p in ax.patches:\n        ax.annotate((p.get_height()), (p.get_x()+0.1, p.get_height()+35))","b77074b7":"print(\"Data ranges from the year\\033[1m\",min(data_copy['year']),\"to\",max(data_copy['year']))","ce6086d2":"data_copy.describe() \n#we observe high standard deviations and we will take a closer look at this later","e7d9fbc4":"sns.heatmap(data_copy.corr(),annot=True,cmap='YlOrRd')","5bb6945d":"#before moving on let's remove the two models with only one entry\ncleaned_data = data_copy[(data_copy['model']!=' RS7')&(data_copy['model']!=' A2')]\ncleaned_data.shape","dc3f33a3":"ax=sns.boxplot(data=cleaned_data,x='mileage',color='#FF87CA')","26f4f48d":"cleaned_data = cleaned_data[cleaned_data['mileage']<170000] # we remove our outlier and save the data in new dataframe\ncleaned_data.shape","cae967cf":"sns.boxplot(data=cleaned_data,x='mileage',color='#FF87CA') ","0438ca02":"sns.boxplot(data=cleaned_data,x='tax',color=\"#FFC4E1\")","59f9f0a9":"cleaned_data = cleaned_data[(cleaned_data['tax']<=500)] #removing outliers\ncleaned_data.shape","16d16874":"sns.boxplot(data=cleaned_data,x='tax',color=\"#FFC4E1\") ","db7af407":"sns.boxplot(data=cleaned_data,x='mpg',color='#EAEAEA')","de956088":"cleaned_data = cleaned_data[(cleaned_data['mpg']<100)]","e0333928":"sns.boxplot(data=cleaned_data,x='mpg',color='#EAEAEA')","5f11a34b":"print(\"We removed\", len(data_copy)-len(cleaned_data),\"outliers\")","bd97b7fa":"cleaned_data.describe()","874bdeec":"plt.figure(figsize=(15,8))\nsns.boxplot(data=cleaned_data,x='model',y='price')\nplt.title(\"Price of different models\")","bb604c07":"plt.figure(figsize=(15,8))\nsns.boxplot(data=cleaned_data,x='engineSize',y='price')\nplt.title(\"Price Based on the size of engine\")","dfca843c":"colors = [\"#D47AE8\",\"#FDFF8F\",\"#F4BEEE\",\"#A8ECE7\"]\nsns.displot(data=cleaned_data,x='price',kind='kde',height=6, aspect=10\/6,hue='fuelType',palette=sns.color_palette(colors,2))\nplt.ylabel(\"distribution\")\nplt.title(\"Distribution of price based on Fuel Type\")","4fe7dd11":"colors = [\"#D47AE8\",\"#FDFF8F\",\"#A8ECE7\",\"#F4BEEE\"]\nsns.displot(data=cleaned_data,x='price',kind='kde',height=6, aspect=10\/6,hue='transmission',palette=sns.color_palette(colors,3))\nplt.ylabel(\"distribution\")\nplt.title(\"Distribution of price based on Transmission type\")","e24ed195":"sns.lineplot(x='year',y='price',data=cleaned_data,color=\"#516BEB\")","b231d24b":"sns.displot(data=cleaned_data,x='year',kde=True,height=5, aspect=10\/5,bins=15,color=\"#161853\")\nplt.title(\"Distribution of Year\")","3bc77b7d":"sns.displot(data=cleaned_data,x='mileage',kde=True,height=5, aspect=10\/5,color=\"#6E3CBC\")\nplt.title(\"Distribution of Mileage\")","2006cc01":"colors = [\"#D47AE8\",\"#FDFF8F\",\"#A8ECE7\",\"#F4BEEE\"]\nsns.countplot(data=cleaned_data, x='transmission',palette=sns.color_palette(colors,3))\nplt.title(\"Count of different types of transmission\")","ae1276ce":"colors = [\"#D47AE8\",\"#FDFF8F\",\"#F4BEEE\",\"#A8ECE7\"]\nsns.countplot(data=cleaned_data, x='fuelType',palette=sns.color_palette(colors,2))\nplt.title(\"Count of fuel types\")","f6fc7d9c":"plt.figure(figsize=(15,8))\nsns.barplot(data=cleaned_data,x='model',y='engineSize',palette='Oranges')\nplt.title(\"Average engine Size for each model\")","6b5b98e8":"plt.scatter(x=cleaned_data['price'],y=cleaned_data['mileage'],color=\"#88E0EF\")\nplt.xlabel(\"Price\")\nplt.ylabel(\"Mileage\")\nplt.title(\"Mileage vs Price\")","b2544c61":"data_dummy = pd.get_dummies(cleaned_data,drop_first=True)  #one hot encode all the categorical variable in the dataframe","9878f95e":"data_dummy.head()","3fb5abae":"#import sklearn libraries\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","eb0414ac":"y = data_dummy['price'] #outcome variable\nx = data_dummy.drop('price',axis=1)  #predictors\nse = StandardScaler()  #initializing the standard scalar\nX = se.fit_transform(x) \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) #split the data into training and test sets","5a06d735":"# normalizing outcome variable\nfrom scipy.stats import boxcox\nresult = boxcox(y_train)\ny_train_bc = result[0]\nlam = result[1]","516079a7":"lr = LinearRegression()  \nlr.fit(X_train,y_train_bc) #fit the model\ny_pred_bc = lr.predict(X_test)  #make prediction","dcf6168e":"# we need to convert the predicted variable to their original scale \nfrom scipy.special import inv_boxcox  \ny_pred_tran = inv_boxcox(y_pred_bc,lam)","6993af7e":"pricePredicted = pd.DataFrame({'Actual Price': y_test, 'Predicted Price': np.round(y_pred_tran)})\npricePredicted = pricePredicted.reset_index()\npricePredicted.head(5)","117047a2":"print(\"The root mean squared error for a simple regression model is:\\033[1m\",np.sqrt(np.sum((y_pred_tran-y_test)**2)\/len(y_test)))","08ccc336":"from sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error","274a9880":"X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)","d211e8ac":"from sklearn.model_selection import GridSearchCV\nestimator = Pipeline([(\"scaler\", StandardScaler()),\n        (\"polynomial_features\", PolynomialFeatures()),\n        (\"ridge_regression\", Ridge())])\n\nparams = {\n    'polynomial_features__degree': [1,2, 3],\n    'ridge_regression__alpha': np.geomspace(4, 50, 20)\n}\ngrid = GridSearchCV(estimator, params, cv=3,verbose=True)","ce384315":"grid.fit(X_train, y_train)","604492cd":"y_pred = grid.predict(X_test)","9bf099a1":"pricePredicted = pd.DataFrame({'Actual Price': y_test, 'Predicted Price': y_pred})\npricePredicted = pricePredicted.reset_index()\npricePredicted.head()","6f9ddf95":"print(\"The rmse for a ridge regression model is:\\033[1m\",np.sqrt(mean_squared_error(y_pred,y_test)))","56db4f0c":"grid.best_params_  #best paramteres","3760de09":"Model RS7 and A2 have only 1 row of data and cannot be used for prediction so we drop them later","637f6900":"We note that mileage still has high std dev , this is mainly becuase data has cars from wide range of years and that's y some of them have driven much more compared to others","82280e64":"R8 Models have a much higher price range compared to others while A series is relatively cheaper","a15fbff9":"# Conclusion","4b7ee1c9":"Ridge regression using l2 norms managed to reduce the rmse and yield a better result\n\nOn a side note , i tried Lasso Regression as well but it took a lot of time and did not converge , ended up overfitting and yield sub optimal results and therefore i decided to not include it","6e8a25fd":"Note: Their are only 0.26% of vehicles having hybrid fuel types and all of them turned out to be outliers as we will see soon!!","575c69a1":"While both Petrol and Diesel have similar distribution , Petrol has higher mean and skewness","e022e279":"#### Boxplot shows outliers obve 170k which can skew our results significantly","88357e43":"The data is taken from \"https:\/\/www.kaggle.com\/adityadesai13\/used-car-dataset-ford-and-mercedes\"\n\nThe dataset contains information about used audi cars and their market price \nIt contains columns such as:\n- Model : Model of the car\n- Year : Registration year\n- Price : Price of the car\n- tranmission : Type of gearbox used\n- fuelType : Type of fuel used\n- tax : Tax applied \n- mileage : Distance the car has travlled\n- mpg : Miles per gallon\n- engineSize : Size of the car engine","17a40b64":"# EDA","d4f1d81f":"- To perform EDA to understand the underlying trends \n- Clean the data and prepare it for regression models\n- Fit different regression models and compare their performance at predicting the price of the car","ac8e411b":"#### 100+ values can be easily seen as the outliers ","06f141f4":"- Both Normal and Regularized regression did an above average job at predicting the price with rmse between 2500 to 30000\n- Lasso Regression was not suitable for modelling the above dataset (more feature engineering would be needed which might result in loss of valuable info)\n- Ridge > Linear > Lasso (for this dataset)\n- Will implement more regression techniques as i learn them :)","ca26222a":"\nThe above correlation matrix gives us important information such as:\n    \n    Price and mileage have negative correlation indicating that the car which has travelled more (more used) is cheaper.\n    Price and mpg also have a negative correlation showing that expensive audi generally have lower miles per gallon.\n    Price has positive corr with enginesize as bigger engine provides more power and hence is more expensive.","213e2e5e":"After cleaning and exploratotry data analysis , we next prepare the data to be fed into a regression model\n\n- Firstly we need to convert all the categorical variable into numeric values by one hot encoding them\n- We also need to standardize the predictors using standardScaler or MinMaxScaler, this is essential for regularized regression\n- We also normalize the outcome variable price using boxcox to yield more accurate results","ab99474d":"# Feature Engineering & Linear Regression","9a6c31c8":"### Now let's analyze the outcome variable - price ","e9b9f5f6":"#### Tax is unevenly distributed but does not have a very hight std dev. so it is best to remove values above 500 ","5c380720":"Manual is normally distributed while automatic and semi automatic have higher means but also a lot more skewness.","b8ce5345":"Newer cars are more expensive","f1d7e60a":"# Objective","38337be3":"# Dataset Description","0a616815":"#### Lets analyze the numerical columns now","182a4789":"Price generally increases with increase in engine size","db47447d":"### Lets Inspect each column","0972196a":"# Regularized regression","c4353444":"Majority of data is after 2010"}}