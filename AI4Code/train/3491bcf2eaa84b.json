{"cell_type":{"1b58a23f":"code","9cce79bd":"code","6bbf1377":"code","c8124ded":"code","0d099297":"code","f545c1a0":"code","3c65590e":"code","46d0637f":"code","e733ce28":"code","95480eb8":"code","d112e261":"code","2bb07bf8":"code","2a33f906":"code","28683f4e":"code","34f75419":"code","ba63b163":"code","6276ca00":"code","e8f0f364":"code","08e74c34":"code","5347d1d3":"code","76b39485":"code","8daa7cbe":"code","0fe2e893":"code","d2a40ae0":"code","aa4a5978":"code","d9857437":"code","99761894":"code","9dc2851a":"code","fb51911f":"code","a7493105":"code","330ce896":"code","fc17f097":"code","88ba1b24":"code","93b5c72f":"code","da3a76b3":"code","e1fd1b35":"code","87663a21":"code","61b2cc51":"code","7ca1b71d":"code","b5a092b6":"code","a35b4756":"markdown","d469e08f":"markdown","ed112c6d":"markdown","643dcc58":"markdown"},"source":{"1b58a23f":"# !pip install gensim\n# !pip install pyldavis\n# !python -m spacy download en","9cce79bd":"! pip install --upgrade pip\n! pip install --upgrade ipykernel\n! pip install --upgrade nbdime\n! pip install jupyterlab-git\n! pip install bs4\n! pip install contractions","6bbf1377":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\npd.set_option(\"display.max_colwidth\", 200)\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport os\nimport json\nimport re\nimport gzip","c8124ded":"import nltk\nfrom nltk import FreqDist\nnltk.download('stopwords')\n\nfrom nltk.tokenize import word_tokenize, sent_tokenize  # Import Tokenizer.\nfrom nltk.stem.wordnet import WordNetLemmatizer         # Import Lemmatizer.\n\nimport spacy\nimport gensim\nfrom gensim import corpora\n\nimport contractions\nfrom bs4 import BeautifulSoup    \nimport pyLDAvis\nimport pyLDAvis.gensim","0d099297":"# os.getcwd()\n# os.listdir('\/kaggle\/input\/amazon-automotive-reviews')\n\n## Data Dictionary \n# * **reviewerID \u2013** ID of the reviewer\n# * **asin \u2013** ID of the product\n# * **reviewerName \u2013** name of the reviewer\n# * **helpful \u2013** helpfulness rating of the review, e.g. 2\/3\n# * **reviewText \u2013** text of the review\n# * **overall \u2013** rating of the product\n# * **summary \u2013** summary of the review\n# * **unixReviewTime \u2013** time of the review (unix time)\n# * **reviewTime \u2013** time of the review (raw)\n\n# raw_data = pd.read_json('\/kaggle\/input\/amazon-automotive-reviews\/Automotive_5.json', lines=True)\n# print(raw_data.shape)\n# raw_data.head(3)","f545c1a0":"from sklearn.datasets import fetch_20newsgroups\nnewsgroups = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))\ndocs_raw = newsgroups.data\nprint(len(docs_raw))","3c65590e":"newsgroups.keys()","46d0637f":"print(newsgroups.DESCR)","e733ce28":"cnt=0\nfor text in docs_raw:\n    if cnt==5:\n        break\n    print(text)\n    print(\"-\"*100)\n    print(\"\\n\")\n    cnt+=1 ","95480eb8":"import numpy as np\nlist(zip(np.arange(len(newsgroups.target_names)),[i for i in newsgroups.target_names]))","d112e261":"def freq_words(x, terms = 30):\n    all_words = ' '.join([text for text in x])\n    all_words = all_words.split()\n  \n    fdist = FreqDist(all_words)\n    words_df = pd.DataFrame({'word':list(fdist.keys()), 'count':list(fdist.values())})\n  \n    # selecting top 20 most frequent words\n    d = words_df.nlargest(columns=\"count\", n = terms) \n    plt.figure(figsize=(20,5))\n    ax = sns.barplot(data=d, x= \"word\", y = \"count\")\n    ax.set(ylabel = 'Count')\n    plt.show()","2bb07bf8":"freq_words(docs_raw)","2a33f906":"data=pd.DataFrame(docs_raw)\ndata=data.sample(frac=1) # Shuffling the data\ndata.reset_index(inplace=True)\ndata.columns=['ID','Text']\nprint(data.shape)\ndata.head(3)","28683f4e":"def strip_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()","34f75419":"data['Text'] = data['Text'].apply(lambda x: strip_html(x))\ndata.head()","ba63b163":"def replace_contractions(text):\n    \"\"\"Replace contractions in string of text\"\"\"\n    return contractions.fix(text)","6276ca00":"data['Text'] = data['Text'].apply(lambda x: replace_contractions(x))\ndata.head()","e8f0f364":"def remove_numbers(text):\n    text = re.sub(r'\\d+', '', text)\n    return text","08e74c34":"data['Text'] = data['Text'].apply(lambda x: remove_numbers(x))\ndata.head()","5347d1d3":"data['Text'] = data.apply(lambda row: nltk.word_tokenize(row['Text']), axis=1) # Tokenization of data","76b39485":"data.head()# Look at how tokenized data looks.","8daa7cbe":"from nltk.corpus import stopwords \nstop_words = stopwords.words('english')\nstop_words=list(set(stop_words))\nprint(len(stop_words))\nprint(stop_words)","0fe2e893":"import re, string, unicodedata   \n\nlemmatizer = WordNetLemmatizer()\n\ndef remove_non_ascii(words):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n        new_words.append(new_word)\n    return new_words\n\ndef to_lowercase(words):\n    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = word.lower()\n        new_words.append(new_word)\n    return new_words\n\ndef remove_punctuation(words):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = re.sub(r'[^\\w\\s]', '', word)\n        if new_word != '':\n            new_words.append(new_word)\n    return new_words\n\ndef remove_stopwords(words):\n    \"\"\"Remove stop words from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        if word not in stop_words:\n            new_words.append(word)\n    return new_words\n\ndef lemmatize_list(words):\n    new_words = []\n    for word in words:\n        new_words.append(lemmatizer.lemmatize(word, pos='v'))\n    return new_words","d2a40ae0":"def normalize(words):\n    words = remove_non_ascii(words)\n    words = to_lowercase(words)\n    words = remove_punctuation(words)\n    words = remove_stopwords(words)\n    words = lemmatize_list(words)\n    return ' '.join(words)","aa4a5978":"data['Text'] = data.apply(lambda row: normalize(row['Text']), axis=1)\ndata.head()","d9857437":"freq_words(data['Text'])","99761894":"# remove short words (length < 2)\ndata['Text'] = data['Text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>1]))\ndata.head()","9dc2851a":"freq_words(data['Text'], 35)","fb51911f":"# nlp = spacy.load('en', disable=['parser', 'ner'])","a7493105":"word_tokens = data.apply(lambda row: nltk.word_tokenize(row['Text']), axis=1) # Tokenization of data","330ce896":"word_tokens[:5]","fc17f097":"# Create the term dictionary of our corpus, where every unique term is assigned an index\ndictionary = corpora.Dictionary(word_tokens)","88ba1b24":"cnt=0\nfor i,j in dictionary.items():\n    if(cnt==5):\n        break\n    print(i,j)\n    cnt+=1","93b5c72f":"# Convert list of reviews (reviews_2) into a Document Term Matrix using the dictionary prepared above.\ndoc_term_matrix = [dictionary.doc2bow(rev) for rev in word_tokens]","da3a76b3":"# Creating the object for LDA model using gensim library\nLDA = gensim.models.ldamodel.LdaModel","e1fd1b35":"# ?LDA","87663a21":"# Build LDA model\nlda_model = LDA(corpus=doc_term_matrix,id2word=dictionary,num_topics=10,random_state=11)","61b2cc51":"lda_model.print_topics()","7ca1b71d":"# Visualize the topics\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, doc_term_matrix, dictionary)\nvis","b5a092b6":"# Let\u2019s now put the models to work and transform unseen documents to their topic distribution:\n\n# Each bubble on the left-hand side plot represents a topic. \n# The larger the bubble, the more prevalent is that topic.\n\n# A good topic model will have fairly big, non-overlapping bubbles scattered \n# throughout the chart instead of being clustered in one quadrant.\n\n# A model with too many topics, will typically have many overlaps, small sized \n# bubbles clustered in one region of the chart.","a35b4756":"## Visualizing the models with pyLDAvis\n\n* 1  Relevance is denoted by \u03bb, the weight assigned to the probability of a term in a topic relative to its lift. When \u03bb = 1, the terms are ranked by their probabilities within the topic (the \u2018regular\u2019 method) while when \u03bb = 0, the terms are ranked only by their lif,brt\n* 2 By default, the terms of a topic are ranked in decreasing order according their topic-specific probability ( \u03bb = 1 )\n* 3 Moving the slider allows to adjust the rank of terms based on much discriminatory (or \"relevant\") are for the specific topic\n* 4 The red bars represent the frequency of a term in a given topic, (proportional to p(term | topic))\n* 5 The blue bars represent a term's frequency across the entire corpus, (proportional to p(term))","d469e08f":"### Introduction\nTopic Models, in a nutshell, are a type of statistical language models used for uncovering hidden structure in a collection of texts. In a practical and more intuitively, you can think of it as a task of:\n\n- **Dimensionality Reduction**, where rather than representing a text T in its feature space as {Word_i: count(Word_i, T) for Word_i in Vocabulary}, you can represent it in a topic space as {Topic_i: Weight(Topic_i, T) for Topic_i in Topics}\n- **Unsupervised Learning**, where it can be compared to clustering, as in the case of clustering, the number of topics, like the number of clusters, is an output parameter. By doing topic modeling, we build clusters of words rather than clusters of texts. A text is thus a mixture of all the topics, each having a specific weight\n- **Tagging**, abstract \u201ctopics\u201d that occur in a collection of documents that best represents the information in them.\n\nThere are several existing algorithms you can use to perform the topic modeling. The most common of it are, Latent Semantic Analysis (LSA\/LSI), Probabilistic Latent Semantic Analysis (pLSA), and Latent Dirichlet Allocation (LDA)\n\nIn this tutorial, we\u2019ll take a closer look at LDA, and implement our first topic model using the sklearn implementation in python 2.7\n\n### Theoretical Overview\nLDA is a generative probabilistic model that assumes each topic is a mixture over an underlying set of words, and each document is a mixture of over a set of topic probabilities.\n\n![LDA_Model](https:\/\/github.com\/chdoig\/pytexas2015-topic-modeling\/blob\/master\/images\/lda-4.png?raw=true)\n\nWe can describe the generative process of LDA as, given the M number of documents, N number of words, and prior K number of topics, the model trains to output:\n\n- `psi`, the distribution of words for each topic K\n- `phi`, the distribution of topics for each document i\n\n#### Parameters of LDA\n\n- `Alpha parameter` is Dirichlet prior concentration parameter that represents document-topic density \u2014 with a higher alpha, documents are assumed to be made up of more topics and result in more specific topic distribution per document.\n- `Beta parameter` is the same prior concentration parameter that represents topic-word density \u2014 with high beta, topics are assumed to made of up most of the words and result in a more specific word distribution per topic.","ed112c6d":"## Reference Links - \n1. https:\/\/www.analyticsvidhya.com\/blog\/2016\/08\/beginners-guide-to-topic-modeling-in-python\/\n2. https:\/\/www.analyticsvidhya.com\/blog\/2018\/10\/mining-online-reviews-topic-modeling-lda\/","643dcc58":"### Data Pre-processing:\n\n- Remove html tags.\n- Replace contractions in string. (e.g. replace I'm --> I am) and so on.\\\n- Remove numbers.\n- Tokenization\n- To remove Stopwords.\n- Lemmatized data\n\nWe have used NLTK library to tokenize words , remove stopwords and lemmatize the remaining words."}}