{"cell_type":{"bd79b82a":"code","4a1026fd":"code","72d06cce":"code","47468d5d":"code","2dfb4811":"code","a22f78ae":"code","0e939d6e":"code","af200384":"code","e64b6aba":"code","50c9bc2c":"code","6be93124":"code","68310a74":"code","514837f0":"code","8d6e549c":"code","b93b8123":"code","a13a7b42":"markdown","fb3e19db":"markdown","82942da6":"markdown","66c6ef9a":"markdown","e2b3eff8":"markdown","f3d6c542":"markdown","f673c37a":"markdown","b30ead40":"markdown","3dcbd4c8":"markdown","e4055a06":"markdown","0d027470":"markdown","1a75654c":"markdown","48280572":"markdown","33bd49ad":"markdown","d6fed347":"markdown","837cd047":"markdown","c02d575f":"markdown","1c46be8d":"markdown"},"source":{"bd79b82a":"!pip install -q scanpy\n\nimport scanpy as sc # import scanpy to handle our AnnData \nimport pandas as pd # import pandas to handle dataframes\nimport matplotlib.pyplot as plt # import matplotlib to visualize our qc metrics\n\n# magic incantation to help matplotlib work with our jupyter notebook\n%matplotlib inline ","4a1026fd":"adata = sc.read(\"..\/input\/scrnaseq-analysis-model-outputs\/brain_qc\/brain_qc.h5ad\")","72d06cce":"adata.X.shape","47468d5d":"sc.pp.pca(adata)  # applies pca on adata and adds results of pca in adata in .obsm, .varm, .uns\nsc.pl.pca_overview(adata, color='mouse.id')  # plot the pca results with color as mouse.id ","2dfb4811":"adata.uns\nadata.uns['pca']['variance_ratio'] #- Ratio of explained variance.\n#adata.uns['pca']['variance'] - Explained variance, equivalent to the eigenvalues of the covariance matrix.","a22f78ae":"print(adata.varm['PCs'].shape)\nadata.varm['PCs']                 #.varm['PCs'] - The principal components containing the loadings.","0e939d6e":"print(adata.obsm['X_pca'].shape)\nadata.obsm['X_pca']               # .obsm['X_pca'] - PCA representation of data.","af200384":"adata_cpm = adata.copy()  # apply this to a copy so we can compare methods\nadata_cpm.raw = adata_cpm  # store a copy of the raw values before normalizing\nsc.pp.normalize_per_cell(adata_cpm, \n                         counts_per_cell_after=1e6)\n# Returns or updates adata with normalized version of the original adata_cpm.X","e64b6aba":"sc.pp.pca(adata_cpm)\nsc.pl.pca_overview(adata_cpm, color='mouse.id')","50c9bc2c":"adata_cpm_ex = adata.copy() # make a copy so we can compare results\nsc.pp.normalize_total(adata_cpm_ex, target_sum=1e6, exclude_highly_expressed=True) # normalize\nsc.pp.pca(adata_cpm_ex) # run pca\nsc.pl.pca_overview(adata_cpm_ex, color='mouse.id') # plot pca","6be93124":"not_Rn45s = adata_cpm.var.index != 'Rn45s'\nadata_no_Rn45s = adata_cpm[:, not_Rn45s]\n\nsc.pp.pca(adata_no_Rn45s)\nsc.pl.pca_overview(adata_no_Rn45s, color='mouse.id')","68310a74":"adata_no_Rn45s.shape, adata.shape  # 1 gene is removed","514837f0":"sc.pp.log1p(adata_cpm)  # Returns or updates data, depending on copy. X = log(X + 1) \nsc.pp.scale(adata_cpm)  # Scale data to unit variance and zero mean.\n# updates adata with a scaled adata.X, annotated with 'mean' and 'std' in adata.var.\nsc.pp.pca(adata_cpm)  \nsc.pl.pca_overview(adata_cpm, color='plate.barcode')","8d6e549c":"adata_cpm.write('brain_normalized.h5ad')  # saving the 1 with log1p normalization","b93b8123":"!zip \"brain_normalized.zip\" \".\/brain_normalized.h5ad\"","a13a7b42":"### Saving the Normalized adata","fb3e19db":"The first plot has a strange, very linear first PC (which captures the most variation in the dataset). This suggests that we have outliers in our data.\n\nThe next row of plots shows the loadings, which indicate how strongly each variable in the original data contributes to each principle component. Here, we see that the first PC is strongly determined by the expression of just a small number of genes.\n\nThe bottom plot shows us that the first principle component captures the vast majority of the variation in the raw data, and that a single gene dominates the variation in that component.","82942da6":"This is more reasonable than before: the second component now explains some of the variance.\n\nOther linear approaches to correcting for library size include:\n\n- Downsampling, which randomly samples reads from each cell until a set threshold is reached\n- RPKM and related methods, which correct for transcript length","66c6ef9a":"- A potential drawback of CPM is if your sample contains genes that are both very highly expressed and differentially expressed across the cells. In this case, the total molecules in the cell may depend of whether such genes are on\/off in the cell and normalizing by total molecules may hide the differential expression of those genes and\/or falsely create differential expression for the remaining genes. \n- One way to mitigate this is to exclude highly expressed genes from the size factor estimation.\n\n<b>Normalize with counts per million, excluding highly expressed genes from the size factor calculation<\/b> : ","e2b3eff8":"# Normalizing cell library size \n\n- Major factor that contributes variation to single-cell RNA-sequencing experiments is \"Library size variation\". Library sizes vary for many reasons, including natural differences in cell size, variation of RNA capture, variation in the efficiency of PCR amplification used to generate enough RNA to create the sequencing library. \n\n- In addition, scRNA-seq data is sequenced on highly multiplexed platforms therefore the total reads which are derived from each cell may differ substantially.\n\n- This means that there are lot of technical variations than biological and that is why cell are commonly normalized to have comparable RNA content. \n\n- However, it is important to note that all reasoning about differences between cells after this normalization occurs is restricted to asking question about the relative, not absolute, abundance of RNA in one cell vs another.\n\n- Some quantification methods (eg. Cufflinks, RSEM) incorporate library size when determining gene expression estimates and thus do not require this normalization while other quantification methoda require library size correction.\n\n- There are two main approaches to this correction. Many methods use a **simple linear scaling to adjust counts such that each cell (row) has about the same total library size. Examples include converting to counts per million (CPM) and closely related methods such as scran. While simple, these approaches do a reasonable job of correcting for differences in library size.**\n\n- Other methods are more complex which are useful when there are more complex sources of unwanted variation (e.g., for highly heterogeneous populations of cells with different sizes).\n\n- Here, we'll stick to the simple, **linear scaling methods**. ","f3d6c542":"https:\/\/chanzuckerberg.github.io\/scRNA-python-workshop\/preprocessing\/01-basic-qc.html","f673c37a":"This looks much better: the first plot shows more Gaussian-looking groups of cells. The second row of plots shows well-distributed loadings, indicating that each PC is driven by multiple genes. And the final plot shows that Each of the first ~5-10 components captures some of the variance in the data.\n\nLet's write our normalized data to file for later use.","b30ead40":"Now, applying **PCA** on normalized data.","3dcbd4c8":"<br>\n<h1 style = \"font-size:60px; font-family:Garamond ; font-weight : normal; background-color: #f6f5f5 ; color : #fe346e; text-align: center; border-radius: 100px 100px;\">Summary<\/h1>\n<br>\n<b>Tried various approaches for Normalization and PCA:<\/b>\n\n- Directly performing PCA on quality controlled data\n- Normalizing cell library size using CPM normalization, and then performing PCA\n- Normalizing each cell by total counts over all genes, and then performing PCA\n- Removing \"offending\" genes(very highly expressed genes) such as Rn45s, and then performing PCA\n- Finally, Normalization by log1p, scaling and then PCA.","e4055a06":"**Normalize each cell by total counts over all genes, so that every cell has the same total count after normalization.** If choosing target_sum=1e6, this is CPM normalization.","0d027470":"### Loading the Quality controlled data","1a75654c":"### Installations","48280572":"# Normalizing gene expression \n- As we saw earlier, this dataset is dominated by one or more highly expressed genes. \n- One thing to test is to determine if the **offending gene, Rn45s**, when removed, yields a more reasonable analysis. This can be assessed by removing the gene and re-running PCA.","33bd49ad":"Now that we have a clean expression matrix, we can use PCA to visualize an overview of the data and assess confounding factors. SCANPY provides several very useful functions to simplify visualisation.\n\nLet's first peek at our data before normalization:","d6fed347":"<b>This makes things slightly better.<\/b> \n\n- There are now other genes in PC1 that contribute meaningfully to its loading, and PC1 is no longer the only component that contributes significant variation. \n\n- However, it's obvious from looking at the genes that we would likely have to remove a few more before we'd get more equal contribution from large numbers of genes, and there could be low-expression genes, such as transcription factors, which are really informative of cell state, but are masked by the expression of other, more highly expressed genes.\n\n- Another way to more systematically address this is by centering and scaling the gene expression values (you may remember a \"z-score\" from stats class). Importantly, doing this places an equal weight on each gene for downstream analysis. Depending on your biological question, this may or may not be appropriate. The advantage of doing so, however, is that it de-emphasizes the small handful of genes that are differentially expressed at high levels, which are currently dominating the data.\n\n- <b>First, we take the log(1+x) of each value. The +1 makes sure that 0 values in the original data still map to 0 in log space (and prevents us from trying to take the log of 0). This makes the expression values more closely approximate a Gaussian distribution, which is an assumption inherent to many downstream analysis methods.<\/b>\n\n- <b>Then for each gene, we subtract the mean expression value and divide by the standard deviation.<\/b>","837cd047":"## CPM \nThe simplest way to normalize this data is to convert it to counts per million (CPM) by **dividing each row by a size factor (the sum of all counts in the row), then multiplying by 1,000,000.** Note that this method assumes that each cell originally contained the same amount of RNA.","c02d575f":"![image.png](attachment:e373d76b-7cad-407f-91f2-6121ca13d359.png)","1c46be8d":"<br>\n<h1 style = \"font-size:60px; font-family:Garamond ; font-weight : normal; background-color: #f6f5f5 ; color : #fe346e; text-align: center; border-radius: 100px 100px;\">Normalization & PCA<\/h1>\n<br>\n\n- Single cell data is messy. It often contains noise from technical artefacts, batch effects, and other confounders. Before analyzing our data, we need to assess and correct for as much of this unwanted variation as possible.\n\n- We'll focus on the most fundamental sources of unwanted variation, and simple but effective ways to handle this.\n<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #f6f5f5; color :#fe346e; border-radius: 100px 100px; text-align:center\">Normalization<\/h1>\n\n- Dimensionality reduction methods seek to take a large set of variables and return a smaller set of components that still contain most of the information in the original dataset.\n\n- One of the simplest forms of dimensionality reduction is PCA. Principal component analysis (PCA) is a mathematical procedure that transforms a number of possibly correlated (e.g., expression of genes in a network) variables into a (smaller) number of uncorrelated variables called principal components (\"PCs\").\n\n- Biologically, this type of dimensionality reduction is useful and appropriate because cells respond to their environment by turning on regulatory programs that result in expression of modules of genes. As a result, gene expression displays structured co-expression, and dimnesionality reduction by principle component analysis groups those co-varying genes into principle components, ordered by how much variation they explain."}}