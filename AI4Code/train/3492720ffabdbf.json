{"cell_type":{"c1a56a7c":"code","c5022e1e":"code","e0d74b7c":"code","25c19a49":"code","fe57e88e":"code","b1135b20":"code","a56d1a47":"code","2d0a79b6":"code","d70734c6":"code","d761b3ef":"code","1e3f68bc":"code","32706c27":"code","290f4633":"code","a22772d8":"code","5fa3f389":"code","cd3cee1d":"code","13c3799c":"code","aa6cc8c5":"code","c57c5707":"code","cc20bcbb":"code","253ca79f":"code","5343b51d":"code","cf9e0e6b":"code","d93ab0de":"code","0abd4621":"code","161ba2b5":"code","c7103105":"code","b20f1714":"code","0c9145c4":"code","95f239b4":"code","b9e32d85":"code","e81168da":"code","9a67ab2f":"code","4484fc9a":"code","b71fb61a":"code","35e12483":"code","50b0425a":"code","f6f2a1c4":"code","04e34a15":"code","892c42a7":"code","ebb71965":"code","fe32e5c7":"code","10e28f3e":"code","72559776":"code","c8038e17":"code","21af3c6e":"code","c491afdd":"code","a12369d2":"code","be73a616":"code","65623f75":"code","15fccb58":"code","b351afd0":"code","9f8d247a":"code","64277acb":"code","f7fc9ccf":"code","d8d96085":"code","9f81873e":"code","24da0a19":"code","47f40179":"code","51fdbdd3":"code","7faa978b":"code","17fe1e74":"code","2f09f870":"code","298a9568":"code","7479c252":"code","8032916f":"code","a8883926":"code","f6dc1518":"code","52b40d25":"code","b951402e":"code","db8eb646":"code","ebbfd334":"code","47aef97e":"code","43ea17a4":"code","2d34cf68":"code","52e508b6":"code","9f8abcab":"code","be671c76":"code","3987dff7":"code","36a1f07f":"code","14faaa6f":"code","0be64809":"code","44f4a9c9":"code","67ea36b6":"code","ba1f992b":"code","0fd9975e":"code","8a75f2c0":"code","3e18ba05":"code","be70f417":"code","8626412c":"code","8dee5b40":"code","071a4458":"markdown","1e9a48bb":"markdown","4c262a28":"markdown","8b11669d":"markdown","fd94fb11":"markdown","ca09d1da":"markdown","b404bef2":"markdown","d6700270":"markdown","cff35be2":"markdown","7960dbe9":"markdown","58d0cff4":"markdown"},"source":{"c1a56a7c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re \nimport string\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c5022e1e":"import pandas as pd\nimport numpy as np\nimport spacy\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport nltk\nnltk.download('wordnet')\nnltk.download('stopwords')\nfrom nltk.tokenize import TabTokenizer\ntokenizer = TabTokenizer()\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\nfrom nltk.tokenize import word_tokenize\nnltk.download('punkt')\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer","e0d74b7c":"pip install torch","25c19a49":"pip install transformers","fe57e88e":"from transformers import AutoModel","b1135b20":"import pandas as pd\nimport numpy as np\nimport requests\nfrom transformers import AutoTokenizer\nfrom transformers import pipeline\nimport datasets\nfrom datasets import Dataset\n!pip3 install deberta\n#pip install transformers","a56d1a47":"pip install datasets","2d0a79b6":"import torch\nfrom transformers import DebertaTokenizer, DebertaModel","d70734c6":"#tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft\/deberta-base\")","d761b3ef":"def clean_text(text):\n#replace the html characters with \" \"\n    text=re.sub('<.*?>', ' ', text)  \n#remove the punctuations\n    text = text.translate(str.maketrans(' ',' ',string.punctuation))\n#consider only alphabets and numerics\n    text = re.sub('[^a-zA-Z]',' ',text)  \n#replace newline with space\n    text = re.sub(\"\\n\",\" \",text)\n#convert to lower case\n    text = text.lower()\n#split and join the words\n    text=' '.join(text.split())\n    return text","1e3f68bc":"# url = \"https:\/\/raw.githubusercontent.com\/lobnadoma\/data\/main\/comments_to_score.csv\"\n# df = pd.read_csv(url)\ncts = pd.read_csv(\"\/kaggle\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\nvalidation = pd.read_csv(\"\/kaggle\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\nsample = pd.read_csv(\"\/kaggle\/input\/jigsaw-toxic-severity-rating\/sample_submission.csv\")","32706c27":"df2 = pd.read_csv(\"\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\")\ndf3 = df2[['id','comment_text','toxic','severe_toxic']]","290f4633":"df2.head()","a22772d8":"df2[(df2['insult']==1) & (df2['toxic']==1)]","5fa3f389":"df3['comment_text'] = df3['comment_text'].replace(regex=['<.*?>'],value=' ')\ndf3['comment_text'] = df3['comment_text'].replace(regex=['[^a-zA-Z]'],value=' ')\ndf3['comment_text'] = df3['comment_text'].replace(regex=['\\n'],value=' ')","cd3cee1d":"#df3['comment_text'] = df3['comment_text'].apply(lambda x: ''.join([w for w in clean_text(x)]))\ncts['text'] = cts['text'].apply(lambda x: ''.join([w for w in clean_text(x)]))\n","13c3799c":"df2['other'] = df2['obscene']+df2['threat']+df2['insult']+df2['identity_hate']","aa6cc8c5":"df2[((df2['toxic']==0) & (df2['other']>0))]","c57c5707":"df3['toxic'] = df3['toxic'] + df3['severe_toxic']","cc20bcbb":"df3[df3['toxic']>0]\n\n","253ca79f":"df4 = df3[['toxic','comment_text']]\ndf4.columns = ['label','text']","5343b51d":"df4 = df4.sample(frac=0.1, replace=False, random_state=1)","cf9e0e6b":"X_train, X_test, y_train, y_test = train_test_split(df4['text'], df4['label'], test_size=0.33, random_state=42)","d93ab0de":"d = {'text': X_train.values.tolist(), 'label': y_train.values.tolist()}\ntrain = pd.DataFrame(data=d)\nd = {'text': X_test.values.tolist(), 'label': y_test.values.tolist()}\ntest = pd.DataFrame(data=d)\ntrain['label'] = train.label.astype(\"category\").cat.codes\ntest['label'] = test.label.astype(\"category\").cat.codes","0abd4621":"train_d = Dataset.from_pandas(train)\ntest_d = Dataset.from_pandas(test)","161ba2b5":"test_d","c7103105":"def tokenize_function(examples):\n    return tokenizer(examples[\"text\"],  padding=\"max_length\", truncation=True)\n\n\ntokenized_train = train_d.map(tokenize_function, batched=True)\ntokenized_test= test_d.map(tokenize_function, batched=True)","b20f1714":"from transformers import AutoModelForSequenceClassification\n#DebertaModel\nmodel = AutoModelForSequenceClassification.from_pretrained('microsoft\/deberta-v3-base', problem_type=\"multi_label_classification\")\n#model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=7)","0c9145c4":"from transformers import TrainingArguments\ntraining_args = TrainingArguments(\"test_trainer\")","95f239b4":"from transformers import Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args, \n    train_dataset=tokenized_train, \n    eval_dataset=tokenized_test\n)","b9e32d85":"trainer.train()","e81168da":"model.save_pretrained('deberta-model')","9a67ab2f":"vectorizer = TfidfVectorizer()\nX_train_tfidf = vectorizer.fit_transform(X_train)\nX_test_tfidf = vectorizer.transform(X_test)\nclf = LogisticRegression(max_iter = 200).fit(X_train_tfidf, y_train)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(clf.score(X_test_tfidf, y_test)))","4484fc9a":"y_pred = clf.predict_proba(vectorizer.transform(cts['text']))","b71fb61a":"clf.classes_","35e12483":"probs = pd.DataFrame(y_pred, columns = ['not','toxic','severe'])","50b0425a":"probs['combined'] = probs['toxic'] + probs['severe']","f6f2a1c4":"cts2 = cts.copy()\ncts2['score'] = probs['combined']","04e34a15":"cts2 = cts2[['comment_id','score']]","892c42a7":"cts2['score'] = cts2['score'].round(3)","ebb71965":"cts2.head()","fe32e5c7":"cts2.to_csv('submission.csv', index=False)","10e28f3e":"pip install wandb --upgrade","72559776":"pip install transformers","c8038e17":"import transformers\nfrom transformers import AutoModel, AutoTokenizer\ntransformers.__version__","21af3c6e":"model_path_or_name = '..\/input\/transformers\/roberta-base'\nmodel     = AutoModel.from_pretrained(model_path_or_name)\ntokenizer = AutoTokenizer.from_pretrained(model_path_or_name)","c491afdd":"import pandas as pd\nimport numpy as np\nimport requests\nfrom transformers import AutoTokenizer\nfrom transformers import pipeline\n# import datasets\n# from datasets import Dataset\n!pip3 install deberta\n#pip install transformers","a12369d2":"!curl -s https:\/\/packagecloud.io\/install\/repositories\/github\/git-lfs\/script.deb.sh |  bash\n!apt-get install -y --allow-unauthenticated git-lfs","be73a616":"!git lfs install\n!git clone https:\/\/huggingface.co\/GroNLP\/hateBERT\n# if you want to clone without large files \u2013 just their pointers\n# prepend your git clone with the following env var:\n!GIT_LFS_SKIP_SMUDGE=1","65623f75":"def setup_tensorflow_1_13():\n    \n    # Install `tensorflow-gpu==1.13.1` from pre-downloaded wheels\n    PATH_TO_TF_WHEELS = '\/kaggle\/input\/tensorflow1131-offline-bert\/tensorflow_gpu_1_13_1_with_deps_whl\/tensorflow_gpu_1_13_1_with_deps_whl'\n    # yes, mixing up Python code and bash is ugly. But it's handy \n    !pip install --no-deps $PATH_TO_TF_WHEELS\/*.whl\n","15fccb58":"ruddit = pd.read_csv(\"..\/input\/ruddit-jigsaw-dataset\/Dataset\/ruddit_with_text.csv\")\nprint(ruddit.shape)","b351afd0":"ruddit.head()","9f8d247a":"ruddit = ruddit[['txt', 'offensiveness_score']].rename(columns={'txt': 'text',\n                                                                'offensiveness_score':'y'})","64277acb":"#Rescale values to range between 0 and 1\nruddit['y'] = (ruddit['y'] - ruddit.y.min()) \/ (ruddit.y.max() - ruddit.y.min()) \nruddit.y.hist()","f7fc9ccf":"pip install pytorch_transformers","d8d96085":"import torch\nimport pytorch_transformers\nimport datasets\nfrom datasets import Dataset","9f81873e":"from transformers import AutoConfig\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nfrom transformers import AutoTokenizer,AutoModelForSequenceClassification\nconfig = AutoConfig.from_pretrained(f'..\/input\/hatebert\/')\ntokenizer = AutoTokenizer.from_pretrained(f'..\/input\/hatebert\/')\nmodel = AutoModelForSequenceClassification.from_pretrained(f'..\/input\/hatebert\/', num_labels = 3)","24da0a19":"df4 = df3[['toxic','comment_text']]\ndf4.columns = ['label','text']","47f40179":"df4 = df4.sample(frac=0.25, replace=False, random_state=1)","51fdbdd3":"X_train, X_test, y_train, y_test = train_test_split(df4['text'], df4['label'], test_size=0.33, random_state=42)","7faa978b":"d = {'text': X_train.values.tolist(), 'label': y_train.values.tolist()}\ntrain = pd.DataFrame(data=d)","17fe1e74":"d = {'text': X_train.values.tolist(), 'label': y_train.values.tolist()}\ntrain = pd.DataFrame(data=d)\nd = {'text': X_test.values.tolist(), 'label': y_test.values.tolist()}\ntest = pd.DataFrame(data=d)\ntrain['label'] = train.label.astype(\"category\").cat.codes\ntest['label'] = test.label.astype(\"category\").cat.codes","2f09f870":"train_d['label'][0]","298a9568":"train_d['text'][0]","7479c252":"train_d = Dataset.from_pandas(train)\ntest_d = Dataset.from_pandas(test)","8032916f":"def tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\ntokenized_train = train_d.map(tokenize_function, batched=True)\ntokenized_test= test_d.map(tokenize_function, batched=True)","a8883926":"from transformers import TrainingArguments\ntraining_args = TrainingArguments(\"test_trainer\")","f6dc1518":"from transformers import Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args, \n    train_dataset=tokenized_train, \n    eval_dataset=tokenized_test\n)","52b40d25":"tokenized_train['input_ids'][0]","b951402e":"trainer.train()","db8eb646":"!git lfs install\n!git clone https:\/\/huggingface.co\/GroNLP\/hateBERT","ebbfd334":"tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")","47aef97e":"import os\nimport gc\nimport copy\nimport time\nimport random\nimport string\n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\n\n# Utils\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\n# Sklearn Imports\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\n# For Transformer Models\nfrom transformers import AutoTokenizer, AutoModel, AdamW\n\n# For colored terminal text\nfrom colorama import Fore, Back, Style\nb_ = Fore.BLUE\ny_ = Fore.YELLOW\nsr_ = Style.RESET_ALL\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","43ea17a4":"import wandb\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"wandb_api\")\n    wandb.login(key=api_key)\n    anony = None\nexcept:\n    anony = \"must\"\n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https:\/\/wandb.ai\/authorize')","2d34cf68":"def id_generator(size=12, chars=string.ascii_lowercase + string.digits):\n    return ''.join(random.SystemRandom().choice(chars) for _ in range(size))\n\nHASH_NAME = id_generator(size=12)\nprint(HASH_NAME)","52e508b6":"CONFIG = {\"seed\": 42,\n          \"epochs\": 3,\n          \"model_name\": \"GroNLP\/hateBERT\",\n          \"train_batch_size\": 16,\n          \"valid_batch_size\": 32,\n          'gradient_accumulation_steps' : 2,\n          \"max_length\": 256,\n          \"learning_rate\": 1e-5,\n          \"scheduler\": 'CosineAnnealingLR',\n          \"min_lr\": 1e-6,\n          \"T_max\": 500,\n          \"weight_decay\": 1e-6,\n          \"n_fold\": 5,\n          \"n_accumulate\": 1,\n          \"num_classes\": 1,\n          \"margin\": 0.5,\n          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n          \"hash_name\": HASH_NAME\n          }\n\nCONFIG[\"tokenizer\"] = AutoTokenizer.from_pretrained(CONFIG['model_name'])\nCONFIG['group'] = f'{HASH_NAME}-Baseline'\n","9f8abcab":"def set_seed(seed=42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nset_seed(CONFIG['seed'])","be671c76":"df = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\ndf.head()","3987dff7":"skf = StratifiedKFold(n_splits=CONFIG['n_fold'], shuffle=True, random_state=CONFIG['seed'])\n\nfor fold, ( _, val_) in enumerate(skf.split(X=df, y=df.worker)):\n    df.loc[val_ , \"kfold\"] = int(fold)\n    \ndf[\"kfold\"] = df[\"kfold\"].astype(int)\ndf.head()","36a1f07f":"class Dataset(Dataset):\n    def __init__(self, df, tokenizer, max_length):\n        self.df = df\n        self.max_len = max_lenth\n        self.text = df['text'].values\n        self.toxicity = df['toxicity'].values\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        text = self.text[index]\n        toxicity = self.toxicity[index]\n        inputs_more_toxic = self.tokenizer.encode_plus(\n                                more_toxic,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n        inputs_less_toxic = self.tokenizer.encode_plus(\n                                less_toxic,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n        target = 1\n        \n        more_toxic_ids = inputs_more_toxic['input_ids']\n        more_toxic_mask = inputs_more_toxic['attention_mask']\n        \n        less_toxic_ids = inputs_less_toxic['input_ids']\n        less_toxic_mask = inputs_less_toxic['attention_mask']\n        \n        \n        return {\n            'more_toxic_ids': torch.tensor(more_toxic_ids, dtype=torch.long),\n            'more_toxic_mask': torch.tensor(more_toxic_mask, dtype=torch.long),\n            'less_toxic_ids': torch.tensor(less_toxic_ids, dtype=torch.long),\n            'less_toxic_mask': torch.tensor(less_toxic_mask, dtype=torch.long),\n            'target': torch.tensor(target, dtype=torch.long)\n        }\n        ","14faaa6f":"\nclass Dataset(Dataset):\n    def __init__(self, df, tokenizer, max_length):\n        self.df = df\n        self.max_len = max_lenth\n        self.more_toxic = df['more_toxic'].values\n        self.less_toxic = df['less_toxic'].values\n    \n    def __getitem__(self, index):\n        more_toxic = self.more_toxic[index]\n        less_toxic = self.less_toxic[index]\n        \n        \n    def __getitem__(self, index):\n        more_toxic = self.more_toxic[index]\n        less_toxic = self.less_toxic[index]\n        inputs_more_toxic = self.tokenizer.encode_plus(\n                                more_toxic,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n        inputs_less_toxic = self.tokenizer.encode_plus(\n                                less_toxic,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n        target = 1\n        \n        more_toxic_ids = inputs_more_toxic['input_ids']\n        more_toxic_mask = inputs_more_toxic['attention_mask']\n        \n        less_toxic_ids = inputs_less_toxic['input_ids']\n        less_toxic_mask = inputs_less_toxic['attention_mask']\n        \n        \n        return {\n            'more_toxic_ids': torch.tensor(more_toxic_ids, dtype=torch.long),\n            'more_toxic_mask': torch.tensor(more_toxic_mask, dtype=torch.long),\n            'less_toxic_ids': torch.tensor(less_toxic_ids, dtype=torch.long),\n            'less_toxic_mask': torch.tensor(less_toxic_mask, dtype=torch.long),\n            'target': torch.tensor(target, dtype=torch.long)\n        }\n    def __len__(self):\n        return len(self.df)\n    \n    class JigsawModel(nn.Module):\n    def __init__(self, model_name):\n        super(JigsawModel, self).__init__()\n        self.model = AutoModel.from_pretrained(model_name)\n        self.drop = nn.Dropout(p=0.2)\n        self.fc = nn.Linear(768, CONFIG['num_classes'])\n        \n    def forward(self, ids, mask):        \n        out = self.model(input_ids=ids,attention_mask=mask,\n                         output_hidden_states=False)\n        out = self.drop(out[1])\n        outputs = self.fc(out)\n        return outputs\n    \n    \n    def criterion(outputs1, outputs2, targets):\n    return nn.MarginRankingLoss(margin=CONFIG['margin'])(outputs1, outputs2, targets)\n","0be64809":"class JigsawDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.more_toxic = df['more_toxic'].values\n        self.less_toxic = df['less_toxic'].values\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        more_toxic = self.more_toxic[index]\n        less_toxic = self.less_toxic[index]\n        inputs_more_toxic = self.tokenizer.encode_plus(\n                                more_toxic,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n        inputs_less_toxic = self.tokenizer.encode_plus(\n                                less_toxic,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n        target = 1\n        \n        more_toxic_ids = inputs_more_toxic['input_ids']\n        more_toxic_mask = inputs_more_toxic['attention_mask']\n        \n        less_toxic_ids = inputs_less_toxic['input_ids']\n        less_toxic_mask = inputs_less_toxic['attention_mask']\n        \n        \n        return {\n            'more_toxic_ids': torch.tensor(more_toxic_ids, dtype=torch.long),\n            'more_toxic_mask': torch.tensor(more_toxic_mask, dtype=torch.long),\n            'less_toxic_ids': torch.tensor(less_toxic_ids, dtype=torch.long),\n            'less_toxic_mask': torch.tensor(less_toxic_mask, dtype=torch.long),\n            'target': torch.tensor(target, dtype=torch.long)\n        }","44f4a9c9":"class JigsawModel(nn.Module):\n    def __init__(self, model_name):\n        super(JigsawModel, self).__init__()\n        self.model = AutoModel.from_pretrained(model_name)\n        self.drop = nn.Dropout(p=0.2)\n        self.fc = nn.Linear(768, CONFIG['num_classes'])\n        \n    def forward(self, ids, mask):        \n        out = self.model(input_ids=ids,attention_mask=mask,\n                         output_hidden_states=False)\n        out = self.drop(out[1])\n        outputs = self.fc(out)\n        return outputs","67ea36b6":"def criterion(outputs1, outputs2, targets):\n    return nn.MarginRankingLoss(margin=CONFIG['margin'])(outputs1, outputs2, targets)\n","ba1f992b":"def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n    model.train()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:\n        more_toxic_ids = data['more_toxic_ids'].to(device, dtype = torch.long)\n        more_toxic_mask = data['more_toxic_mask'].to(device, dtype = torch.long)\n        less_toxic_ids = data['less_toxic_ids'].to(device, dtype = torch.long)\n        less_toxic_mask = data['less_toxic_mask'].to(device, dtype = torch.long)\n        targets = data['target'].to(device, dtype=torch.long)\n        \n        batch_size = more_toxic_ids.size(0)\n\n        more_toxic_outputs = model(more_toxic_ids, more_toxic_mask)\n        less_toxic_outputs = model(less_toxic_ids, less_toxic_mask)\n        \n        loss = criterion(more_toxic_outputs, less_toxic_outputs, targets)\n        loss = loss \/ CONFIG['n_accumulate']\n        loss.backward()\n    \n        if (step + 1) % CONFIG['n_accumulate'] == 0:\n            optimizer.step()\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            if scheduler is not None:\n                scheduler.step()\n                \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss \/ dataset_size\n        \n        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])\n    gc.collect()\n    \n    return epoch_loss\n","0fd9975e":"### Modified\ndef train_all_epoch(model, optimizer, scheduler, dataloader, device, epoch, num_epochs):\n    train_loss = []\n    \n    for epoch in range(1, num_epochs + 1): \n        model.train()\n\n        dataset_size = 0\n        running_loss = 0.0\n\n        bar = tqdm(enumerate(dataloader), total=len(dataloader))\n        for step, data in bar:\n            more_toxic_ids = data['more_toxic_ids'].to(device, dtype = torch.long)\n            more_toxic_mask = data['more_toxic_mask'].to(device, dtype = torch.long)\n            less_toxic_ids = data['less_toxic_ids'].to(device, dtype = torch.long)\n            less_toxic_mask = data['less_toxic_mask'].to(device, dtype = torch.long)\n            targets = data['target'].to(device, dtype=torch.long)\n\n            batch_size = more_toxic_ids.size(0)\n\n            more_toxic_outputs = model(more_toxic_ids, more_toxic_mask)\n            less_toxic_outputs = model(less_toxic_ids, less_toxic_mask)\n\n            loss = criterion(more_toxic_outputs, less_toxic_outputs, targets)\n            loss = loss \/ CONFIG['n_accumulate']\n            loss.backward()\n\n            if (step + 1) % CONFIG['n_accumulate'] == 0:\n                optimizer.step()\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                if scheduler is not None:\n                    scheduler.step()\n\n            running_loss += (loss.item() * batch_size)\n            dataset_size += batch_size\n\n            epoch_loss = running_loss \/ dataset_size\n\n            bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n                            LR=optimizer.param_groups[0]['lr'])\n        gc.collect()\n\n        train_loss.append(epoch_loss)\n\n\n\n# def run_training(model, optimizer, scheduler, device, num_epochs, fold):\n#     # To automatically log gradients\n#     wandb.watch(model, log_freq=100)\n    \n#     if torch.cuda.is_available():\n#         print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n    \n#     start = time.time()\n#     best_model_wts = copy.deepcopy(model.state_dict())\n#     best_epoch_loss = np.inf\n#     history = defaultdict(list)\n    \n#     for epoch in range(1, num_epochs + 1): \n#         gc.collect()\n#         train_epoch_loss = train_one_epoch(model, optimizer, scheduler, \n#                                            dataloader=train_loader, \n#                                            device=CONFIG['device'], epoch=epoch)\n        \n#         val_epoch_loss = valid_one_epoch(model, valid_loader, device=CONFIG['device'], \n#                                          epoch=epoch)\n    \n#         history['Train Loss'].append(train_epoch_loss)\n#         history['Valid Loss'].append(val_epoch_loss)\n        \n#         # Log the metrics\n#         wandb.log({\"Train Loss\": train_epoch_loss})\n#         wandb.log({\"Valid Loss\": val_epoch_loss})\n        \n#         # deep copy the model\n#         if val_epoch_loss <= best_epoch_loss:\n#             print(f\"{b_}Validation Loss Improved ({best_epoch_loss} ---> {val_epoch_loss})\")\n#             best_epoch_loss = val_epoch_loss\n#             run.summary[\"Best Loss\"] = best_epoch_loss\n#             best_model_wts = copy.deepcopy(model.state_dict())\n#             PATH = f\"Loss-Fold-{fold}.bin\"\n#             torch.save(model.state_dict(), PATH)\n#             # Save a model file from the current directory\n#             print(f\"Model Saved{sr_}\")\n            \n#         print()\n    \n#     end = time.time()\n#     time_elapsed = end - start\n#     print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n#         time_elapsed \/\/ 3600, (time_elapsed % 3600) \/\/ 60, (time_elapsed % 3600) % 60))\n#     print(\"Best Loss: {:.4f}\".format(best_epoch_loss))\n    \n#     # load best model weights\n#     model.load_state_dict(best_model_wts)\n    \n#     return model, history\n\n","8a75f2c0":"@torch.no_grad()\ndef valid_one_epoch(model, dataloader, device, epoch):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for step, data in bar:        \n        more_toxic_ids = data['more_toxic_ids'].to(device, dtype = torch.long)\n        more_toxic_mask = data['more_toxic_mask'].to(device, dtype = torch.long)\n        less_toxic_ids = data['less_toxic_ids'].to(device, dtype = torch.long)\n        less_toxic_mask = data['less_toxic_mask'].to(device, dtype = torch.long)\n        targets = data['target'].to(device, dtype=torch.long)\n        \n        batch_size = more_toxic_ids.size(0)\n\n        more_toxic_outputs = model(more_toxic_ids, more_toxic_mask)\n        less_toxic_outputs = model(less_toxic_ids, less_toxic_mask)\n        \n        loss = criterion(more_toxic_outputs, less_toxic_outputs, targets)\n        \n        running_loss += (loss.item() * batch_size)\n        dataset_size += batch_size\n        \n        epoch_loss = running_loss \/ dataset_size\n        \n        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n                        LR=optimizer.param_groups[0]['lr'])   \n    \n    gc.collect()\n    \n    return epoch_loss\n","3e18ba05":"def run_training(model, optimizer, scheduler, device, num_epochs, fold):\n    # To automatically log gradients\n    wandb.watch(model, log_freq=100)\n    \n    if torch.cuda.is_available():\n        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n    \n    start = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_epoch_loss = np.inf\n    history = defaultdict(list)\n    \n    for epoch in range(1, num_epochs + 1): \n        gc.collect()\n        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, \n                                           dataloader=train_loader, \n                                           device=CONFIG['device'], epoch=epoch)\n        \n        val_epoch_loss = valid_one_epoch(model, valid_loader, device=CONFIG['device'], \n                                         epoch=epoch)\n    \n        history['Train Loss'].append(train_epoch_loss)\n        history['Valid Loss'].append(val_epoch_loss)\n        \n        # Log the metrics\n        wandb.log({\"Train Loss\": train_epoch_loss})\n        wandb.log({\"Valid Loss\": val_epoch_loss})\n        \n        # deep copy the model\n        if val_epoch_loss <= best_epoch_loss:\n            print(f\"{b_}Validation Loss Improved ({best_epoch_loss} ---> {val_epoch_loss})\")\n            best_epoch_loss = val_epoch_loss\n            run.summary[\"Best Loss\"] = best_epoch_loss\n            best_model_wts = copy.deepcopy(model.state_dict())\n            PATH = f\"Loss-Fold-{fold}.bin\"\n            torch.save(model.state_dict(), PATH)\n            # Save a model file from the current directory\n            print(f\"Model Saved{sr_}\")\n            \n        print()\n    \n    end = time.time()\n    time_elapsed = end - start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 3600, (time_elapsed % 3600) \/\/ 60, (time_elapsed % 3600) % 60))\n    print(\"Best Loss: {:.4f}\".format(best_epoch_loss))\n    \n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    \n    return model, history\n","be70f417":"def prepare_loaders(fold):\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    train_dataset = JigsawDataset(df_train, tokenizer=CONFIG['tokenizer'], max_length=CONFIG['max_length'])\n    valid_dataset = JigsawDataset(df_valid, tokenizer=CONFIG['tokenizer'], max_length=CONFIG['max_length'])\n\n    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], \n                              num_workers=2, shuffle=True, pin_memory=True, drop_last=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], \n                              num_workers=2, shuffle=False, pin_memory=True)\n    \n    return train_loader, valid_loader","8626412c":"def fetch_scheduler(optimizer):\n    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CONFIG['T_max'], \n                                                   eta_min=CONFIG['min_lr'])\n    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CONFIG['T_0'], \n                                                             eta_min=CONFIG['min_lr'])\n    elif CONFIG['scheduler'] == None:\n        return None\n        \n    return scheduler","8dee5b40":"for fold in range(0, CONFIG['n_fold']):\n    print(f\"{y_}====== Fold: {fold} ======{sr_}\")\n    run = wandb.init(project='Jigsaw', \n                     config=CONFIG,\n                     job_type='Train',\n                     group=CONFIG['group'],\n                     tags=['hateBERT', f'{HASH_NAME}', 'margin-loss'],\n                     name=f'{HASH_NAME}-fold-{fold}',\n                     anonymous='must')\n    \n    # Create Dataloaders\n    train_loader, valid_loader = prepare_loaders(fold=fold)\n    \n    model = JigsawModel(CONFIG['model_name'])\n    model.to(CONFIG['device'])\n    \n    # Define Optimizer and Scheduler\n    optimizer = AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n    scheduler = fetch_scheduler(optimizer)\n    \n    model, history = run_training(model, optimizer, scheduler,\n                                  device=CONFIG['device'],\n                                  num_epochs=CONFIG['epochs'],\n                                  fold=fold)\n    \n    run.finish()\n    \n    del model, history, train_loader, valid_loader\n    _ = gc.collect()\n    print()","071a4458":"## Transformer Model","1e9a48bb":"# New Section","4c262a28":"## Training function","8b11669d":"# Training ","fd94fb11":"## Dataset Class","ca09d1da":"## Start Training","b404bef2":"## Model Class","d6700270":"## Run Training","cff35be2":"## Validation Function","7960dbe9":"## Data Cleaning","58d0cff4":"## Loss Function\n"}}