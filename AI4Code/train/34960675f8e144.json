{"cell_type":{"fd9d1650":"code","35da7e75":"code","552d8d7d":"code","d3937df3":"code","a5afe368":"code","a6869221":"code","bcd4564c":"code","afaf4555":"code","6c4267e3":"code","a753922e":"code","a4949611":"code","9b6120d7":"code","4b40960a":"markdown","faaeb8c1":"markdown","c94329e9":"markdown","ce17471f":"markdown","6aeaa9d4":"markdown","0ce3f3ff":"markdown","935ffeb5":"markdown","841feb90":"markdown","169e5f7e":"markdown","4eb16a14":"markdown","cf7423df":"markdown","8c6b5f0c":"markdown","a82cf1a4":"markdown","3a7df7c9":"markdown"},"source":{"fd9d1650":"%%capture\n!pip install wandb --upgrade\n\nimport os\nimport wandb\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom wandb.keras import WandbCallback\nfrom sklearn.model_selection import KFold\nfrom transformers import TFAutoModelForSequenceClassification, TFAutoModel, AutoTokenizer\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\ndef seed_everything(seed=1234):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n\nseed = 42\nseed_everything(seed)\nwarnings.filterwarnings('ignore')\n\ntrain_filepath = '..\/input\/commonlitreadabilityprize\/train.csv'\ntest_filepath = '..\/input\/commonlitreadabilityprize\/test.csv'\n\ntrain = pd.read_csv(train_filepath)\ntest = pd.read_csv(test_filepath)\n\n# removing unused columns\ntrain.drop(['url_legal', 'license'], axis=1, inplace=True)\ntest.drop(['url_legal', 'license'], axis=1, inplace=True)\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\napi_key = user_secrets.get_secret(\"WANDB_API_KEY\")\nwandb.login(key=api_key);","35da7e75":"display(train.head(10))","552d8d7d":"DEVICE = 'GPU'","d3937df3":"if DEVICE == \"TPU\":\n    print(\"connecting to TPU...\")\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        print(\"Could not connect to TPU\")\n        tpu = None\n\n    if tpu:\n        try:\n            print(\"initializing  TPU ...\")\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            \n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n            print(\"TPU initialized\")\n        except _:\n            print(\"failed to initialize TPU\")\n    else:\n        DEVICE = \"GPU\"\n\nif DEVICE == \"GPU\":\n    n_gpu = len(tf.config.experimental.list_physical_devices('GPU'))\n    print(\"Num GPUs Available: \", n_gpu)\n    \n    if n_gpu > 1:\n        print(\"Using strategy for multiple GPU\")\n        strategy = tf.distribute.MirroredStrategy()\n    else:\n        print('Standard strategy for GPU...')\n        strategy = tf.distribute.get_strategy()\n\nAUTO     = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\n\nprint(f'REPLICAS: {REPLICAS}')","a5afe368":"BATCH_SIZE = 8 * REPLICAS\nLEARNING_RATE = 1e-3 * REPLICAS\nEPOCHS = 15\nN_FOLDS = 5\nSEQ_LEN = 300\nBASE_MODEL = '..\/input\/huggingface-roberta-variants\/distilroberta-base\/distilroberta-base'\nNEW_NAME = \"John\"\nproper_names = ['fayre', 'roger', 'blaney']","a6869221":"import string\n\n# must check this list\nstop_words = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ] \n\ntable = str.maketrans('', '', string.punctuation)\n\n# Remove Punctuations\ndef remove_punctuation(orig_str):\n    words = orig_str.split()\n    filtered_sentence = \"\"\n    \n    for word in words:\n        word = word.translate(table)\n        filtered_sentence = filtered_sentence + word + \" \"\n    \n    return filtered_sentence\n\n# Remove all Stopwords\ndef remove_stopwords(orig_str):\n    filtered_sentence = \"\"\n    \n    words = orig_str.split(\" \")\n    \n    for word in words:\n        if word not in stop_words:\n            filtered_sentence = filtered_sentence + word + \" \"\n            \n    return filtered_sentence\n\n# Substitude proper names (all changed with John)\ndef change_proper_names(orig_str):\n    filtered_sentence = \"\"\n    \n    words = orig_str.split(\" \")\n    \n    for word in words:\n        if word not in proper_names:\n            filtered_sentence = filtered_sentence + word + \" \"\n        else:\n            filtered_sentence = filtered_sentence + NEW_NAME + \" \"\n            \n    return filtered_sentence\n\n# A Custom Standardization Function\ndef custom_standardization(text):\n    text = text.lower()\n    text = text.strip()\n    return text","bcd4564c":"# Sampling Function\ndef sample_target(features, target):\n    mean, stddev = target\n    sampled_target = tf.random.normal([], mean=tf.cast(mean, dtype=tf.float32), \n                                      stddev=tf.cast(stddev, dtype=tf.float32), dtype=tf.float32)\n    \n    return (features, sampled_target)\n    \n\n# Convert to tf.data.Dataset\ndef get_dataset(pandas_df, tokenizer, labeled=True, ordered=False, repeated=False, \n                is_sampled=False, batch_size=32, seq_len=128):\n    \n    pandas_df['excerpt'] = pandas_df['excerpt'].apply(remove_punctuation)\n    pandas_df['excerpt'] = pandas_df['excerpt'].apply(remove_stopwords)\n    pandas_df['excerpt'] = pandas_df['excerpt'].apply(change_proper_names)\n    \n    text = [custom_standardization(text) for text in pandas_df['excerpt']]\n    \n    # Tokenize inputs\n    tokenized_inputs = tokenizer(text, max_length=seq_len, truncation=True, \n                                 padding='max_length', return_tensors='tf')\n    \n    if labeled:\n        dataset = tf.data.Dataset.from_tensor_slices(({'input_ids': tokenized_inputs['input_ids'], \n                                                      'attention_mask': tokenized_inputs['attention_mask']}, \n                                                      (pandas_df['target'], pandas_df['standard_error'])))\n        if is_sampled:\n            dataset = dataset.map(sample_target, num_parallel_calls=tf.data.AUTOTUNE)\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices({'input_ids': tokenized_inputs['input_ids'], \n                                                      'attention_mask': tokenized_inputs['attention_mask']})\n        \n    if repeated:\n        dataset = dataset.repeat()\n    if not ordered:\n        dataset = dataset.shuffle(1024)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n    \n    return dataset","afaf4555":"initial_learning_rate = 0.01\n\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate, decay_steps=20, decay_rate=0.96, staircase=True)\n\ndef get_model(encoder, seq_len=256):\n    \n    input_ids = tf.keras.layers.Input(shape=(seq_len), dtype=tf.int32, name='input_ids')\n    \n    input_attention_mask = tf.keras.layers.Input(shape=(seq_len,), dtype=tf.int32, name='attention_mask')\n    \n    output = encoder({'input_ids': input_ids, \n                      'attention_mask': input_attention_mask})\n    \n    model = tf.keras.Model(inputs = [input_ids, input_attention_mask], \n                           outputs = output, name = \"CommonLit_Model\")\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n    \n    model.compile(optimizer=optimizer, \n                  loss=tf.keras.losses.MeanSquaredError(), \n                  metrics=['mse'])\n    \n    return model","6c4267e3":"with strategy.scope():\n    encoder = TFAutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=1)\n    model = get_model(encoder, SEQ_LEN)\n    \nmodel.summary()","a753922e":"tf.keras.utils.plot_model(model,\n    show_shapes=True, show_dtype=False,\n    show_layer_names=False, rankdir='TB', expand_nested=False, dpi=96)","a4949611":"tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n\nskf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=seed)\n\nhistory_list = []\ntest_pred = []\n\nfor fold,(idxT, idxV) in enumerate(skf.split(train)):\n        \n    # Log information\n    print(f'\\nFOLD: {fold+1}')\n\n    # Create Model\n    tf.keras.backend.clear_session()\n    with strategy.scope():\n        encoder = TFAutoModelForSequenceClassification.from_pretrained(BASE_MODEL, num_labels=1)\n        model = get_model(encoder, SEQ_LEN)\n\n    # Callbacks\n    early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n        patience=8, restore_best_weights=True)\n    \n    # Create a W&B run\n    run = wandb.init(project='commonlit', entity='sauravmaheshkar', reinit=True, sync_tensorboard=True)\n\n    # Train\n    history = model.fit(x=get_dataset(train.loc[idxT], tokenizer, repeated=True, is_sampled=True, \n                                      batch_size=BATCH_SIZE, seq_len=SEQ_LEN), \n                        validation_data=get_dataset(train.loc[idxV], tokenizer, ordered=True, \n                                                    batch_size=BATCH_SIZE, seq_len=SEQ_LEN), \n                        steps_per_epoch=100, \n                        callbacks=[early_stopping_cb,WandbCallback(monitor='val_mse', mode='min', \n                                 save_model=False)], \n                        epochs=EPOCHS,  \n                        verbose=1).history\n    \n    run.finish()\n      \n    history_list.append(history)\n    \n    # Test predictions\n    test_ds = get_dataset(test, tokenizer, labeled=False, ordered=True, batch_size=BATCH_SIZE, seq_len=SEQ_LEN)\n    x_test = test_ds.map(lambda sample: sample)\n    test_pred.append(model.predict(x_test)['logits'])","9b6120d7":"submission = test[['id']]\nsubmission['target'] = np.mean(test_pred, axis=0)\nsubmission.to_csv('submission.csv', index=False)\ndisplay(submission.head(10))","4b40960a":"<a id='basic'><\/a>\n# Packages \ud83d\udce6 and Basic Setup","faaeb8c1":"## Basic Hyperparameters \ud83e\udea1","c94329e9":"# Table of Contents\n\n1. [Packages \ud83d\udce6 and Basic Setup](#basic)\n2. [Pre-Processing \ud83d\udc4e\ud83c\udffb -> \ud83d\udc4d](#process)\n3. [The Model \ud83d\udc77\u200d\u2640\ufe0f](#model)\n4. [Training \ud83d\udcaa\ud83c\udffb](#train)","ce17471f":"![](https:\/\/github.com\/SauravMaheshkar\/CommonLit-Readibility\/blob\/main\/assets\/CommonLit%20-%20Big%20Banner.png?raw=true)","6aeaa9d4":"## Disclaimer\n\nThis Kernel builds on top of [@dimitreoliveira](https:\/\/www.kaggle.com\/dimitreoliveira)'s kernel [CommonLit Readability - EDA & RoBERTa TF baseline](https:\/\/www.kaggle.com\/dimitreoliveira\/commonlit-readability-eda-roberta-tf-baseline). Please check out this kernel as well \ud83d\ude0a.","0ce3f3ff":"## LearningRate Schedule and Callbacks\n\n> From a [TowardsDataScience article](https:\/\/towardsdatascience.com\/learning-rate-scheduler-d8a55747dd90)\n\nIn training deep networks, it is helpful to reduce the learning rate as the number of training epochs increases. This is based on the intuition that with a high learning rate, the deep learning model would possess high kinetic energy. As a result, it\u2019s parameter vector bounces around chaotically. Thus, it\u2019s unable to settle down into deeper and narrower parts of the loss function (local minima). If the learning rate, on the other hand, was very small, the system then would have low kinetic energy. Thus, it would settle down into shallow and narrower parts of the loss function (false minima).\n\n<center> <img src = \"https:\/\/miro.medium.com\/max\/668\/1*iYWyu8hemMyaBlK6V-2vqg.png\"> <\/center>\n\n\nThe above figure depicts that a high learning rate will lead to random to and fro moment of the vector around local minima while a slow learning rate results in getting stuck into false minima. Thus, knowing when to decay the learning rate can be hard to find out.\n\nDecreasing the learning rate during training can lead to improved accuracy and (most perplexingly) reduced overfitting of the model. A piecewise decrease of the learning rate whenever progress has plateaued is effective in practice. Essentially this ensures that we converge efficiently to a suitable solution and only then reduce the inherent variance of the parameters by reducing the learning rate.\n\nWe'll also define some callbacks for :-\n\n* `EarlyStopping` ( A callback to stop training when a monitored metric has stopped improving )\n* `WandbCallback` ( Weights and Biases callback to automatically save all the metrics and the loss values tracked in model.fit )","935ffeb5":"<a id='process'><\/a>\n# Pre-Processing \ud83d\udc4e\ud83c\udffb -> \ud83d\udc4d","841feb90":"<a id = 'train'><\/a>\n# Training \ud83d\udcaa","169e5f7e":"# Submission","4eb16a14":"We train the model for `N_FOLDS` and save our model metrics to Weights and Biases for efficient model monitoring and tracking. Visit the [**Weights and Biases Project Page**](https:\/\/wandb.ai\/sauravmaheshkar\/commonlit) to see the metrics\n\n![](https:\/\/raw.githubusercontent.com\/SauravMaheshkar\/CommonLit-Readibility\/a5ce1a982c59f8d3dd86a4b9e4a9b5108ccdfb26\/assets\/Val_MSE_Ex.svg)\n\n![](https:\/\/raw.githubusercontent.com\/SauravMaheshkar\/CommonLit-Readibility\/a5ce1a982c59f8d3dd86a4b9e4a9b5108ccdfb26\/assets\/MSE_Ex.svg)","cf7423df":"Building on top of [@dimitreoliveira](https:\/\/www.kaggle.com\/dimitreoliveira\/commonlit-readability-eda-roberta-tf-baseline)'s work we further process the data by removing punctuations and stopwords (courtesy of [@luigisaetta](https:\/\/www.kaggle.com\/luigisaetta)). \n\n1. Remove Punctuation\n2. Remove Stopwords\n3. Change Proper Names\n4. Convert to Lowercase\n5. Strip spaces\n6. Convert to `tf.data.Dataset` format","8c6b5f0c":"<a id = 'model'><\/a>\n# The Model \ud83d\udc77\u200d\u2640\ufe0f","a82cf1a4":"## Architecture\n\n![](https:\/\/github.com\/SauravMaheshkar\/CommonLit-Readibility\/blob\/main\/assets\/Dataset%20Banner.png?raw=true)\n\nFor the Model Architecture we use `distilroberta-base` from huggingface. Other Variants of Roberta and BERT are worth experimenting with and have been made available in the [**Huggingface Roberta Variants**](https:\/\/www.kaggle.com\/sauravmaheshkar\/huggingface-roberta-variants) Dataset. (BERT Dataset will be uploaded soon). Currently Model Weights for 4 variants are available in the Dataset:-\n\n* **`distilroberta-base`**\n* **`roberta-base`**\n* **`roberta-large`**\n* **`roberta-large-mnli`**","3a7df7c9":"## Device Configuration \ud83d\udd0c"}}