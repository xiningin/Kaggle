{"cell_type":{"67df7f12":"code","47db0b54":"code","dc0ef67c":"code","4203e2b4":"code","84e32f74":"code","3ecdba02":"code","52ae176a":"code","fb315fbb":"code","852a8a7e":"code","fb04540d":"code","abf2955b":"code","67a7e8ca":"code","d5a2b001":"code","b1cdc0f1":"code","1812d531":"code","47190a02":"code","cbf3c2cb":"code","b5347c41":"code","1e77ea2f":"code","f121bf67":"code","a957d29f":"code","4fc0cd41":"code","cd8382e0":"code","fdc1be45":"code","f482e81a":"code","24c383a6":"code","ec575c04":"code","dd5adf02":"code","d322b6c9":"markdown","14b65b7b":"markdown","9a840ece":"markdown","07689fd8":"markdown","6876c0f1":"markdown","3edb6c52":"markdown","7c4304f3":"markdown","0cd20f6c":"markdown","7a37ae70":"markdown","4104a892":"markdown","33237632":"markdown","55dd403d":"markdown","cbad8c04":"markdown","5c6ef35f":"markdown","62d40653":"markdown","b0f3c980":"markdown","a7b32dde":"markdown","2eccfc0d":"markdown","b6d161ff":"markdown","8759d9e6":"markdown","150b58c8":"markdown","699106da":"markdown","c86d854a":"markdown","5748df69":"markdown","7c7f9254":"markdown","8bc86f1d":"markdown","316901b4":"markdown","8ba36a2c":"markdown","012eea10":"markdown","2ad184da":"markdown","42982e25":"markdown","f7e37fc8":"markdown"},"source":{"67df7f12":"%load_ext autoreload\n%autoreload 2\n!pip install solt==0.1.9\n!pip install seaborn==0.11\n\nimport os\nimport math\nfrom collections import OrderedDict\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import softmax\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.model_selection import GroupKFold\nimport pandas as pd\nimport seaborn as sns\nimport cv2\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\nfrom torch.utils.data import Dataset\nfrom torch.optim import Adam\nimport torchvision\n\nfrom tqdm.notebook import tqdm\nimport solt\nimport solt.transforms as slt","47db0b54":"def to_cpu(x: torch.Tensor or torch.cuda.FloatTensor, required_grad=False, use_numpy=True):\n    x_cpu = x\n\n    if isinstance(x, torch.Tensor):\n        if x.is_cuda:\n            if use_numpy:\n                x_cpu = x.detach().cpu().numpy()\n            elif required_grad:\n                x_cpu = x.cpu()\n            else:\n                x_cpu = x.cpu().required_grad_(False)\n        elif use_numpy:\n            x_cpu = x.detach().numpy()\n\n    return x_cpu\n\n\ndef count_data_amounts_per_target(df, target_name):\n    amount_list = []\n    target_list = df[target_name].unique()\n    \n    for target in target_list:\n        if target == np.nan:\n            df_by_target = df[df[target_name].isnull()]\n        else:\n            df_by_target = df[df[target_name] == target]\n        num_samples = len(df_by_target.index)\n        amount_list.append(num_samples)\n        print(f'There are {num_samples} {target}-class samples.')\n    return amount_list\n\n\ndef display_random_samples(df, root_path):\n    \"\"\"Function to display random samples\"\"\"\n    target_names = {1: 'Real', 0: 'Synthesized'}\n    pos = 1\n    n_pairs = 10\n    fig, axs = plt.subplots(2, n_pairs, figsize=(15, 4), sharex=True, sharey=True)\n    \n    # Loop through targets (0, 1)\n    for target in range(2):\n        # Loop through pair indices\n        for pair_i in range(n_pairs):\n            # Select rows by and target\n            df_tmp = df[df['Validity'] == target]\n            # Choose a random index\n            len_df = len(df_tmp.index)\n            random_id = np.random.randint(0, len_df)\n            # Obtain image path corresponding to the selected index\n            image_path = df_tmp.iloc[random_id]['Filename']\n            image_fullname = os.path.join(root_path, image_path)\n            # Read image\n            img = cv2.imread(image_fullname, cv2.IMREAD_GRAYSCALE)\n            # Show image and format its subplot\n            axs[target, pair_i].imshow(img, cmap='gray')\n            axs[target, pair_i].set_axis_off()\n            if pair_i == 0:\n                axs[target, pair_i].set_title(f'{target_names[target]}')\n            pos += 1\n\n    plt.tight_layout()\n    plt.show()\n\n\ndef collect_and_update_metrics(metrics_collector, new_record):\n    # Collect data\n    metrics_collector['loss'].append(new_record['new_loss'])\n    np_preds = to_cpu(torch.argmax(new_record['new_preds'], dim=1))\n    np_probs = to_cpu(torch.softmax(new_record['new_preds'], dim=1))[:, 1]\n    np_labels = to_cpu(new_record['new_labels'])\n    metrics_collector['preds'] += np_preds.tolist()\n    metrics_collector['labels'] += np_labels.tolist()\n    metrics_collector['probs'] += np_probs.tolist()\n\n    # Calculate metrics\n    _loss = np.mean(np.array(metrics_collector['loss']))\n    _ap = average_precision_score(metrics_collector['labels'], metrics_collector['probs'])\n\n    # Put metrics into the dictionary metrics for display in progress bar    \n    metrics_display = OrderedDict()\n    metrics_display['loss'] = _loss\n    metrics_display['average_precision'] = _ap\n    return metrics_collector, metrics_display","dc0ef67c":"root = \"\/kaggle\/input\/yo-medicalai-challenge\/chest_xray_vadility_classification\"\nimg_root = os.path.join(root, \"train_validation\", \"images\")\ninput_meta = os.path.join(root, \"train_validation\", \"challenge_all_meta_final.csv\")","4203e2b4":"df_meta = pd.read_csv(input_meta)\ndf_meta.head(10)","84e32f74":"display_random_samples(df_meta, img_root)","3ecdba02":"validity_targets = df_meta['Validity'].unique()\n\nprint(f'Validity targets: {validity_targets}')","52ae176a":"g1 = sns.displot(df_meta, x='Validity', discrete=True)\ng1.set(xticks=[0, 1])","fb315fbb":"def clean_metadata(df):\n    # TODO:\n    return df\n\ndf_meta = clean_metadata(df_meta)","852a8a7e":"g1 = sns.displot(df_meta, x='Validity', discrete=True)\ng1.set(xticks=[0, 1])","fb04540d":"n_fold = 5\nsplitter = GroupKFold(n_splits=n_fold)\nsplitter_iter = splitter.split(df_meta, df_meta['Validity'], groups=df_meta['ID'])\ncv_folds = [(train_idx, val_idx) for (train_idx, val_idx) in splitter_iter]","abf2955b":"# Training and validation indices\ntrain_idx, val_idx = cv_folds[0]\n\n# Training and validation samples\ntrain_df = df_meta.iloc[train_idx]\nval_df = df_meta.iloc[val_idx]","67a7e8ca":"train_df.head(10)","d5a2b001":"# TODO: implement, for reproducibility of the folds","b1cdc0f1":"class DataFrameDataset(Dataset):\n    \"\"\"Dataset based on ``pandas.DataFrame``.\n        \n    Parameters\n    ----------\n    root : str\n        Path to root directory of input data.\n    meta_data : pandas.DataFrame\n        Meta data of data and labels.\n    transform : callable, optional\n        Transformation applied to row of :attr:`meta_data` (the default is None, which means to do nothing).\n    std_size: tuple\n        The size that input images are standardized to (Default: (224, 224))\n    mean: tuple\n        Mean to normalize image intensities (Default: (0.485, 0.456, 0.406))\n    std: tuple\n        Standard deviation to normalize image intensities (Default: (0.229, 0.224, 0.225))\n    \n    Raises\n    ------\n    TypeError\n        `root` must be `str`.\n    TypeError\n        `meta_data` must be `pandas.DataFrame`.\n    \n    \"\"\"\n\n    def __init__(self, root, meta_data, \n                 transform=None, \n                 std_size=(64, 64), \n                 mean=(0.5, 0.5, 0.5), \n                 std=(0.5, 0.5, 0.5)):\n        if not isinstance(root, str):\n            raise TypeError(\"`root` must be `str`\")\n        if not isinstance(meta_data, pd.DataFrame):\n            raise TypeError(\"`meta_data` must be `pandas.DataFrame`, but found {}\".format(type(meta_data)))\n        self.root = root\n        self.meta_data = meta_data        \n        self.transform = transform        \n        self.std_size = std_size\n        self.mean = mean\n        self.std = std\n        \n    def parse_item(self, root, entry, transform):\n        # Read image from path\n        img_fullname = os.path.join(root, entry['Filename'])\n        img = cv2.imread(img_fullname, cv2.IMREAD_COLOR)        \n                        \n        stats = {'mean': self.mean, 'std': self.std}\n        # Apply transformations into image\n        trf_output = transform({'image': img}, return_torch=True, normalize=True, **stats)\n        \n        if 'Validity' in entry:\n            # Training\/validation regime\n            return {'Filename': entry['Filename'], 'data': trf_output['image'], 'Validity': int(entry['Validity'])}\n        else:\n            # Testing regime (no info on Validity)\n            return {'Filename': entry['Filename'], 'data': trf_output['image']}\n\n    def __getitem__(self, index):\n        \"\"\"Get ``index``-th parsed item of :attr:`meta_data`.\n        \n        Parameters\n        ----------\n        index : int\n            Index of row.\n        \n        Returns\n        -------\n        entry : dict\n            Dictionary of `index`-th parsed item.\n        \"\"\"\n        entry = self.meta_data.iloc[index]\n        entry = self.parse_item(self.root, entry, self.transform)\n        if not isinstance(entry, dict):\n            raise TypeError(\"Output of `parse_item_cb` must be `dict`, but found {}\".format(type(entry)))\n        return entry\n\n    def __len__(self):\n        \"\"\"Get length of `meta_data`\"\"\"\n        return len(self.meta_data.index)","1812d531":"# Standard size\nstd_size = (64, 64)\n\ntrain_transforms = solt.Stream([slt.Pad(std_size)])\nvalid_transforms = solt.Stream([slt.Pad(std_size)])\nprint(f'>>> Training transformations: <<<\\n{train_transforms.to_yaml()}')\nprint(f'>>> Validation transformations: <<<\\n{valid_transforms.to_yaml()}')","47190a02":"# Dataframe to Dataset\ntrain_ds = DataFrameDataset(img_root, train_df, train_transforms, std_size=std_size)\nvalid_ds = DataFrameDataset(img_root, val_df, valid_transforms, std_size=std_size)","cbf3c2cb":"batch_size = 128\ntrain_loader = torch.utils.data.DataLoader(dataset=train_ds, batch_size=batch_size, shuffle=True, num_workers=8)\neval_loader = torch.utils.data.DataLoader(dataset=valid_ds, batch_size=batch_size, shuffle=False, num_workers=8)\n\nprint(f'train_loader is {train_loader}')\nprint(f'eval_loader is {eval_loader}')","b5347c41":"arch_name = 'mobilenet_v2'\nmodel = torch.hub.load('pytorch\/vision', arch_name, pretrained=False)","1e77ea2f":"# Check the depth of `features` and the details of `classifier`\nprint(f'Depth of features: {len(model.features)}.')\nprint(model.classifier)","f121bf67":"maxdepth_channel_map = {16: 160, 17:160, 18: 320, 19: 1280}\nnew_maxdepth = 18\nmodel.features = model.features[:new_maxdepth]\nprint(f'After cropping, depth of features: {len(model.features)}.')","a957d29f":"new_in_features = maxdepth_channel_map[new_maxdepth]\nmodel.classifier = nn.Linear(in_features=new_in_features, out_features=2, bias=True)\nprint(model.classifier)","4fc0cd41":"def main_process(model, train_loader, eval_loader, device='cpu', n_epochs=10):\n    model.to(device)\n    lr = 1e-4\n    wd = 1e-4\n    loss_func = CrossEntropyLoss()\n    optimizer = Adam(params=model.parameters(), lr=lr, weight_decay=wd)    \n    for epoch_id in range(n_epochs):\n        train_loop(model, loss_func, optimizer, train_loader, epoch_id, device)\n        eval_loop(model, loss_func, eval_loader, epoch_id, device)","cd8382e0":"def train_loop(model, loss_func, optimizer, train_loader, epoch_id, device):\n    metrics_collector = {'loss': [], 'probs': [], 'preds': [], 'labels': [], 'average_precision': None}\n    # Tell the model that we are training it\n    model.train(True)    \n    progress_bar = tqdm(train_loader, total=len(train_loader), desc=f\"Epoch [{epoch_id}][Train]:\")\n    for batch_id, batch in enumerate(progress_bar):\n        # Get sampled data and transfer them to the correct device        \n        inputs = batch['data'].to(device)\n        targets = batch['Validity'].to(device)\n        \n        # Forward through the model\n        preds = model(inputs)        \n\n        # Set gradients to 0\n        optimizer.zero_grad()\n\n        # Calculate loss        \n        loss = loss_func(preds, targets)\n\n        # Learn from the loss by applying backpropagation. This function will compute gradients \n        loss.backward()\n        # Update the model's weights based on the computed gradients and other parameters of the optimizer\n        optimizer.step()\n        \n        new_record = {'new_loss': loss.item(), 'new_preds': preds, 'new_labels': targets}\n        metrics_collector, metrics = collect_and_update_metrics(metrics_collector, new_record)\n        # Update metrics to progress bar\n        metrics_display = {k:f'{metrics[k]:.03f}' for k in metrics}\n        progress_bar.set_postfix(metrics_display)","fdc1be45":"def eval_loop(model, loss_func, eval_loader, epoch_id, device, save_model=True, return_df=False):\n    global best_ap\n    metrics_collector = {'loss': [], 'probs': [], 'preds': [], 'labels': [], 'average_precision': None}\n    # Tell the model we are not training but evaluating it\n    model.train(False) # or model.eval()\n    # Init dictionary to store metrics\n    metrics = OrderedDict()\n    \n    if return_df:\n        validity_list = []\n        filename_list = []\n    n_batches = len(eval_loader)\n    \n    progress_bar = tqdm(eval_loader, total=n_batches, desc=f\"Epoch [{epoch_id}][Eval]:\")\n    with torch.no_grad():\n        for batch_id, batch in enumerate(progress_bar):\n            # Get sampled data and transfer them to the correct device\n            filenames = batch['Filename']            \n            inputs = batch['data'].to(device)\n            targets = batch['Validity'].to(device)            \n            \n            # Forward through the model\n            preds = model(inputs)                    \n\n            # Calculate loss        \n            loss = loss_func(preds, targets)\n            \n            new_record = {'new_loss': loss.item(), 'new_preds': preds, 'new_labels': targets}\n            metrics_collector, metrics = collect_and_update_metrics(metrics_collector, new_record)\n            # Update metrics to progress bar\n            metrics_display = {k:f'{metrics[k]:.03f}' for k in metrics}\n            progress_bar.set_postfix(metrics_display)\n            \n            # Collect data for output dataframe \n            if return_df:\n                probs = torch.softmax(preds, 1)[:, 1].cpu().detach().numpy()\n                filename_list.extend(filenames)\n                validity_list.extend(probs)\n\n        # Store model based on balanced accuracy\n        if metrics['average_precision'] > best_ap and save_model:\n            model_filename = \"best_checkpoint.pth\"\n            print(f'Improved average_precision from {best_ap} to {metrics[\"average_precision\"]}. Saved to {model_filename}...')\n            torch.save(model.state_dict(), model_filename)\n            best_ap = metrics['average_precision']\n            \n    if return_df:\n        results = pd.DataFrame(columns=['Filename', 'Validity'])\n        results['Filename'] = filename_list\n        results['Validity'] = validity_list\n        return results    ","f482e81a":"device = \"cuda\"  # TODO: modify for running on `cpu`\/`cuda`\nn_epochs = 10\nbest_ap = -1.0\n\nmain_process(model,\n             train_loader, \n             eval_loader, \n             device=device, \n             n_epochs=n_epochs)","24c383a6":"def test_loop(model, loss_func, eval_loader, epoch_id, device, save_model=True, return_df=False):\n    global best_ap\n    # Tell the model we are not training but evaluating it\n    model.train(False)  # or model.eval()\n    \n    # Init dictionary to store metrics    \n    validity_list = []\n    filename_list = []\n    n_batches = len(eval_loader)\n    \n    progress_bar = tqdm(eval_loader, total=n_batches, desc=f\"Epoch [{epoch_id}][Eval]:\")\n    with torch.no_grad():\n        for batch_id, batch in enumerate(progress_bar):\n            # Get sampled data and transfer them to the correct device\n            filenames = batch['Filename']            \n            inputs = batch['data'].to(device)\n            \n            # Forward through the model\n            preds = model(inputs)                                \n            \n            # Collect data for output dataframe \n            probs = torch.softmax(preds, 1)[:, 1].cpu().detach().numpy()\n            filename_list.extend(filenames)\n            validity_list.extend(probs)        \n            \n    \n    results = pd.DataFrame(columns=['Filename', 'Validity'])\n    results['Filename'] = filename_list\n    results['Validity'] = validity_list\n    return results    ","ec575c04":"# Configurations\ndevice = \"cuda\"  # TODO: modify for running on `cpu`\/`cuda`\nbatch_size = 128\nstd_size = (64, 64)\n\n# Create test dataloader\ntest_img_dir = os.path.join(root, \"test\/images\")\n\noutput_dir = '\/kaggle\/working'\npretrained_model = os.path.join(output_dir, \"best_checkpoint.pth\")\n\ntest_img_fnames = os.listdir(test_img_dir)\ntest_df = pd.DataFrame.from_dict({\"Filename\": test_img_fnames})\n\ntest_transforms = solt.Stream([slt.Pad(std_size)])\ntest_ds = DataFrameDataset(test_img_dir, test_df, test_transforms, std_size=std_size)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n\n# Get architecture\narch_name = 'mobilenet_v2'\nmodel = torch.hub.load('pytorch\/vision', arch_name, pretrained=False)\nmaxdepth_channel_map = {16: 160, 17:160, 18: 320, 19: 1280}\nnew_maxdepth = 18\nmodel.features = model.features[:new_maxdepth]\nprint(f'After cropping, depth of features: {len(model.features)}.')\nnew_in_features = maxdepth_channel_map[new_maxdepth]\nmodel.classifier = nn.Linear(in_features=new_in_features, out_features=2, bias=True)\nprint(model.classifier)\n\n# Load trained weights\nmodel.load_state_dict(torch.load(pretrained_model), strict=True)\nmodel = model.to(device)\n\n# Call main process\nloss_func = CrossEntropyLoss()\nresults = test_loop(model, loss_func, test_loader, 0, device, save_model=False, return_df=True)","dd5adf02":"submission_fullname = os.path.join(output_dir, \"submission.csv\")\nresults.to_csv(submission_fullname, index=None)\ndisplay(results.head(10))","d322b6c9":"### Read metadata file and display","14b65b7b":"The loaded model has 2 main attributes: `features` and `classifier`. Input images are passed though `features` to automatically extract features. The features are then shrunk into `1x1` feature maps by a global pooling layer (i.e., `AdaptiveAvgPool2d`) and flattened into a vector afterwards. Finally, `classifier` is called to perform predictions into a pre-defined number of classes from the flattened features.","9a840ece":"## Import dependencies and utilities","07689fd8":"# Introduction to the Chest Xray dataset\n\nFirstly, read the [GUIDELINES](https:\/\/www.kaggle.com\/c\/yo-medicalai-challenge\/overview\/guidelines).\n\nThe objective is to build a deep learning model to classify **real** chest X-ray images from **synthesized** ones.\n\n<img src=\"https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F832819%2F1ffff2796a8bf6f39a3dc9219be825b7%2Fkaggle_challenge.png?generation=1602446991570042&alt=media\" width=\"50%\" height=\"50%\">\n\n## DIRECTORY STRUCTURE\n```\n.\n\u251c\u2500\u2500 README\n\u251c\u2500\u2500 test\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 images\n\u2514\u2500\u2500 train_validation\n    \u251c\u2500\u2500 challenge_all_meta_final.csv\n    \u2514\u2500\u2500 images\n```\n\nTraining and Validation:\n\n1. Directory images\/ contains real and synthesized images for training and validation\n2. Metadata file challenge_all_meta.csv has 3 columns:\n    - Filename: image filename\n    - Follow_up: follow-up index (start from 0)\n    - Validity: 1 - real image, 0 - synthesized image\n\nTesting:\n* Directory `images\/` contains real and synthesized images for testing\n\n## SUBMISSION\n**ONLY** submit predictions of chest X-ray images in `test\/images\/`","6876c0f1":"Retrieve training and validation data from fold `0`","3edb6c52":"## Create Dataset Objects\n\nUsing `DataframeDataset` class, we create data loaders for training and validation sets.","7c4304f3":"## Architectures\n\nThere are many off-the-shelf models with pretrained weights. You can have a look at https:\/\/github.com\/pytorch\/vision\/blob\/master\/hubconf.py or other repositories.\n\nA model comprises two parts: \n- Architecture: define connections and layers from input to output\n- Weights: values of connections and layers\n\nTo load an available model, you use the command `torch.hub.load`. Below, we load the `mobilenet_v2` architecture with pretrained weights that were trained on the ImageNet dataset. The input image shape of the model is `244x224` (with 3 channels). We recommend to use the mean (`[0.485, 0.456, 0.406]`) and the standard deviation (`[0.229, 0.224, 0.225]`) corresponding to the dataset; however, you are able to replace it by the statistics of the given dataset.\n\n<img src=\"https:\/\/pytorch.org\/assets\/images\/mobilenet_v2_2.png\" width=\"30%\" height=\"30%\">\n","0cd20f6c":"*Important*: Have a quick look through the extracted images to clean duplicate, noisy, or redundant ones (if they are present). Update metadata dataframe accordingly.","7a37ae70":"For computational purposes, we will reduce the depth of `features`, and change `classifier` to be compatible with our problem (2 classes: real\/fake).","4104a892":"### Main loop\n\n- Transfer model to an proper device (CPU or GPU `cuda`)\n- Create a loss function. Here we use cross-entropy loss.\n- Create an optimizer to optimize the model's parameters\n- While looping through epochs, call `train_loop` and `eval_loop`. \n","33237632":"### Clean data","55dd403d":"# Training Pipeline\n\n## Data Loader\n\nWe need to create a `dataloader` to sample batches of data for training and validation. We will rely on the dataframes created above","cbad8c04":"### Save results to CSV for submission","5c6ef35f":"The `DataframeDataset` class requires 4 arguments:\n\n- `root`: a path to image directory\n- `meta_data`: dataframe of metadata\n- `transform`: a set of transformations to apply to data\n- `std_size`: expected standard size of input samples\n- `mean`: mean for normalization\n- `std`: standard deviation for normalization","62d40653":"## Create DataLoader Objects","b0f3c980":"# Evaluation on Test Set\n\nWe use `test_loop` to perform predictions and store them into a dataframe","a7b32dde":"### Visualize distribution of data by target","2eccfc0d":"### Run the training\/validation","b6d161ff":"## Save dataframes to csv files","8759d9e6":"### Training loop\n\nWe present the training loop (in function `train_loop`) that is supposed to do the following things:\n\n1. Sample batches from training data from `train_loader`, and obtain inputs and labels\n2. Forward inputs through the model to produce predictions\n3. Based the predictions and labels, compute loss for learning\n4. Backward the loss to compute gradients of the model\n5. Step the optimizer to officially update weights of the models based the derived gradients.\n\nBesides the main steps above, we put the loss, predictions, and labels to the metric collecting utility.","150b58c8":"List all targets","699106da":"Specify the path of metadata file and image root directory.","c86d854a":"\n# Make Submission\n\nAfter you finish the evaluation above, you need to make a submission to the grading system.\n\n1. In your navigation bar at the top of the screen you will have a tab for Output. This only shows up if you have written an output file (like we did in the Prepare Submission File step). Click on the Output button. This will bring you to a screen with an option to Submit to Competition. Hit that and you will see how your model performed. If you want to go back to improve your model, click the Edit button, which re-opens the kernel. You'll need to re-run all the cells when you re-open the kernel.\n2. Alternatively, you can download the saved `.csv` file in the menu in the upper right corner (\"Data\" -> \"output\" -> \"kaggle\/working\" -> \"submission.csv\" -> \"Download\") and manually submit it via https:\/\/www.kaggle.com\/c\/yo-medicalai-challenge\/submit .","5748df69":"# Split data into subsets","7c7f9254":"### Predict and prepare a dataframe for submission","8bc86f1d":"# Exploratory Data Analysis\n","316901b4":"## Transformations\n\nThe following snipet creates a list of transformations `train_transforms` and `valid_transforms`.\n\nTransformations are \"changes\" applied on input images. With respect to training data, transformations help to increase the data diversity, which prevents a model from memorizing the training data (as it sees the same images over and over again).\n\nSpecifically, the training transformations below include:\n\n- Padding images to fixed shape\n- Convert images to PyTorch tensor and divide image intensity values by 255 to normalize them into the range [0, 1]\n\nWith validation data we assess generalization of the model (~ability to recognize unseen data). It is recommended to use a minimal set of transformations here, making sure that the value range of validation images is consistent with training ones.","8ba36a2c":"Visualize metadata after the performed selection","012eea10":"## Display random X-ray images\n\nYou are provided a function to display random samples. Top and bottom rows include negative and positive samples respectively.","2ad184da":"## Main function\n\nAfter getting the loaders, model, loss, and optimizer ready, we're going to train and validate the model.\n\nHere, we will need to use one utility function to collect metrics and visualize them. It is prepared for you. Feel free to import, modify (if needed), and use it.","42982e25":"### Validation loop\n\nWe present the validation loop (in function `eval_loop`) that is supposed to do the following things:\n\n1. Sample batches from validation data from `eval_loader`, and obtain inputs and labels\n2. Forward inputs through the model to produce predictions\n3. Based the predictions and labels, compute loss (for model selection or visualialization only)\n4. Compute metrics\n5. Save a snapshot of weights if its performance improves","f7e37fc8":"## Setup utilities"}}