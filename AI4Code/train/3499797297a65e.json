{"cell_type":{"c124de78":"code","427db547":"code","564419cf":"code","deb26fe0":"code","bef7ca1e":"code","3772cb35":"code","e24d13a1":"code","55b34eb7":"code","3d9baf14":"code","7917bff7":"code","b7699d68":"code","2d59d1cc":"code","3bf6dfd3":"code","425c3aa9":"code","4bd0d088":"code","565e2baa":"code","39365d7a":"code","7c399cc5":"code","54482477":"code","c7f05790":"code","7568515e":"code","9626b4c6":"code","79315bab":"code","f8f6dc57":"code","c45c7e32":"code","b0c46264":"code","165d039f":"code","1847b08d":"code","453fb2dc":"code","be345908":"markdown","99bfbda1":"markdown","78b63f4c":"markdown","b7a3b1ff":"markdown","ff65ea58":"markdown","1f932255":"markdown","ef8595e0":"markdown","45c37cfa":"markdown","7b349c07":"markdown","67315b88":"markdown","4a5070c3":"markdown","d8772e58":"markdown"},"source":{"c124de78":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","427db547":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","564419cf":"train.isnull().sum()\nprint(\"Train Shape:\",train.shape)\ntest.isnull().sum()\nprint(\"Test Shape:\",test.shape)","deb26fe0":"train.info()","bef7ca1e":"train.head()","3772cb35":"train.describe()","e24d13a1":"train[\"Survived\"].value_counts()","55b34eb7":"train[\"Pclass\"].value_counts()","3d9baf14":"train[\"Sex\"].value_counts()","7917bff7":"train[\"Embarked\"].value_counts()","b7699d68":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names]","2d59d1cc":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\nnum_pipeline = Pipeline([\n        (\"select_numeric\", DataFrameSelector([\"Age\", \"SibSp\", \"Parch\", \"Fare\"])),\n        (\"imputer\", SimpleImputer(strategy=\"median\")),\n    ])","3bf6dfd3":"num_pipeline.fit_transform(train)","425c3aa9":"# Inspired from stackoverflow.com\/questions\/25239958\nclass MostFrequentImputer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        self.most_frequent_ = pd.Series([X[c].value_counts().index[0] for c in X],\n                                        index=X.columns)\n        return self\n    def transform(self, X, y=None):\n        return X.fillna(self.most_frequent_)","4bd0d088":"from sklearn.preprocessing import OneHotEncoder\ncat_pipeline = Pipeline([\n        (\"select_cat\", DataFrameSelector([\"Pclass\", \"Sex\", \"Embarked\"])),\n        (\"imputer\", MostFrequentImputer()),\n        (\"cat_encoder\", OneHotEncoder(sparse=False)),\n    ])","565e2baa":"cat_pipeline.fit_transform(train)","39365d7a":"from sklearn.pipeline import FeatureUnion\npreprocess_pipeline = FeatureUnion(transformer_list=[\n        (\"num_pipeline\", num_pipeline),\n        (\"cat_pipeline\", cat_pipeline),\n    ])","7c399cc5":"X_train = preprocess_pipeline.fit_transform(train)\ny_train = train[\"Survived\"]\nX_test  = preprocess_pipeline.fit_transform(test)\nX_train.shape, y_train.shape, X_test.shape","54482477":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, y_train) * 100, 2)\nacc_log","c7f05790":"# Support Vector Machines\nfrom sklearn.svm import SVC, LinearSVC\nsvc = SVC()\nsvc.fit(X_train, y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, y_train) * 100, 2)\nacc_svc","7568515e":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","9626b4c6":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, y_train) * 100, 2)\nacc_knn","79315bab":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, y_train)\ny_predRF = random_forest.predict(X_test)\nrandom_forest.score(X_train, y_train)\nacc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\nacc_random_forest","f8f6dc57":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, y_train)\ny_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, y_train) * 100, 2)\nacc_decision_tree","c45c7e32":"from sklearn.model_selection import cross_val_score\ncross_val_score(random_forest, X_train, y_train, cv = 3, scoring = \"accuracy\" )","b0c46264":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\n\ny_train_pred =  cross_val_predict(random_forest, X_train, y_train, cv = 3)\nconfusion_matrix(y_train, y_train_pred)","165d039f":"from sklearn.metrics import precision_score, recall_score, f1_score\n\nprecision_score(y_train, y_train_pred)\nrecall_score(y_train, y_train_pred)\nf1_score(y_train, y_train_pred)","1847b08d":"import pandas as pd\nmodels = pd.DataFrame({\n    'Model': ['Logistic Regression','Support Vector Machines', 'KNN',\n              'Random Forest', 'Decision Tree'],\n    'Score': [acc_log, acc_svc, acc_knn, \n              acc_random_forest, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)","453fb2dc":"output = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': y_predRF})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","be345908":"*Let's take a look at the top few rows of the train data*","99bfbda1":"we can see that the target variable is a categorical variable with 2 classes (0,1)\n\nLet's check the other predictor variables","78b63f4c":"* The Embarked variable tells us where the passenger embarked: C=Cherbourg, Q=Queenstown, S=Southampton.\n\n* Note: the code below uses a mix of Pipeline, FeatureUnion and a custom DataFrameSelector to preprocess some columns differently. Since Scikit-Learn 0.20, it is preferable to use a ColumnTransformer.\n\n* Now let's build our preprocessing pipelines. ","b7a3b1ff":"1. From the descriptive statistics above, we can see that only 38% Survived. \n1. The mean Fare was \u00a332.20, which does not seem so expensive (but it was probably a lot of money back then).\n1. The mean Age was less than 30 years old.","ff65ea58":"We will also need an imputer for the scategorical columns (the regular SimpleImputer does not work on those): We will fill the missing values with the most frequent attributes.","1f932255":"*Finally, let's combine(union) the numerical and categorical pipelines:","ef8595e0":"Now, train a classifier or build our first machine learning model","45c37cfa":"From the .info() method we can see that,the Age, Cabin and Embarked attributes have sometimes null (less than 891 non-null), especially the Cabin (77% are null). We will ignore the Cabin for now and focus on the rest. The Age attribute has about 19% null values, so we will need to decide what to do with them. Replacing null values with the median age seems reasonable.","7b349c07":"Let's check that the target variable is indeed 0 and 1 ","67315b88":"Now we can build the pipeline for the categorical variables:","4a5070c3":"Let's build the pipeline for the numerical attributes\n","d8772e58":"**Model Performance**\n* Here we will and evaluate our classifiers to see how they perform"}}