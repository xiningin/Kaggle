{"cell_type":{"5ab155aa":"code","1980467d":"code","49d7830d":"code","03acaa63":"code","09cb7230":"code","61204b29":"code","2fd8da66":"code","04fdcbea":"code","d4a2b400":"code","900e42ef":"code","914da141":"code","dd41584b":"code","682dabd9":"code","a681875a":"code","1d33ea70":"code","535fcdac":"code","152c8a58":"code","f3d3ed85":"code","e108954f":"code","506100f7":"code","46c50c79":"code","01126e6d":"code","0040c4a9":"code","893c0a42":"code","3619f49d":"code","ceb8a933":"code","c9fbff96":"code","30b4a054":"code","b87f4752":"code","eaffa0ce":"code","fc5cd180":"code","1a7ac5b5":"code","e46b9d51":"code","47def9eb":"code","69332bd7":"markdown","763308f7":"markdown","59c5d237":"markdown","492ce1b8":"markdown","ca8e3ddb":"markdown","664a0bbd":"markdown","872a2880":"markdown","799283e3":"markdown","7ccf5f95":"markdown","62e605f7":"markdown"},"source":{"5ab155aa":"%pylab inline\nimport pandas as pd\nimport scipy.sparse as sparse\nimport seaborn as sns\nfrom tqdm import tqdm_notebook\n","1980467d":"data = pd.read_csv(\"..\/input\/BlackFriday.csv\")\ndata.columns","49d7830d":"USER_ID = 'User_ID'\nPRODUCT_ID = 'Product_ID'\nGENDER = 'Gender', \nAGE = 'Age'\nOCCUPATION = 'Occupation'\nCITY_CATEGORY = 'City_Category'\nSTAY_IN_CURRENT = 'Stay_In_Current_City_Years'\nMARITAL_STATUS = 'Marital_Status'\nPRODUCT_CAT_1 = 'Product_Category_1'\nPRODUCT_CAT_2 = 'Product_Category_2'\nPRODUCT_CAT_3 = 'Product_Category_3'\nPURCHASE = 'Purchase'","03acaa63":"sns.distplot(data[PURCHASE]);","09cb7230":"top_product_cats = data[PRODUCT_CAT_1].value_counts()[:5].index\n\nax = plt.figure().add_subplot(111)\nproduct_cat = top_product_cats[0]\nsns.distplot(data[data[PRODUCT_CAT_1] == product_cat][PURCHASE], ax=ax, label=\"Product Category 1 = \" + str(product_cat))\n\nproduct_cat = top_product_cats[1]\nsns.distplot(data[data[PRODUCT_CAT_1] == product_cat][PURCHASE], ax=ax, label=\"Product Category 1 = \" + str(product_cat))\n\nproduct_cat = top_product_cats[2]\nsns.distplot(data[data[PRODUCT_CAT_1] == product_cat][PURCHASE], ax=ax, label=\"Product Category 1 = \" + str(product_cat))\n\nproduct_cat = top_product_cats[3]\nsns.distplot(data[data[PRODUCT_CAT_1] == product_cat][PURCHASE], ax=ax, label=\"Product Category 1 = \" + str(product_cat))\n\nproduct_cat = top_product_cats[4]\nsns.distplot(data[data[PRODUCT_CAT_1] == product_cat][PURCHASE], ax=ax, label=\"Product Category 1 = \" + str(product_cat))\n\n\nplt.legend()","61204b29":"user_id_col = USER_ID\nitem_id_col = PRODUCT_ID\nratings = data[[user_id_col, item_id_col]]\n\nnum_users = ratings[user_id_col].nunique()\nnum_items = ratings[item_id_col].nunique()\npossible_combinations = num_users * num_items\nnnz = len(ratings)\nnnz_percent = nnz \/ possible_combinations\n\nprint(\"Num Users:\", num_users)\nprint(\"Num Items:\", num_items)\nprint(\"Sparsity:\", nnz_percent)\nprint(\"Not very sparse. CF will work wonders here.\")\n\n# average number of hotel_clusters per user\nitem_per_user = ratings.groupby(user_id_col)[item_id_col].nunique()\n\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(211)\ndisplay(item_per_user.describe().to_frame(\"Number of items per user\"))\nsns.distplot(item_per_user, kde=False, ax=ax)\nax.set_title(\"Median number of items per user: {:.2f}\".format(item_per_user.median()))\n\n# # average number of users per hotel\nuser_per_item = ratings.groupby(item_id_col)[user_id_col].nunique()\nax = fig.add_subplot(212)\ndisplay(user_per_item.describe().to_frame(\"Number of users per item\"))\nsns.distplot(user_per_item, kde=False, ax=ax)\nax.set_title(\"Median number of users per item: {:.2f}\".format(user_per_item.median()))\n\nfig.tight_layout()","2fd8da66":"# Create mappings\nmid_to_idx = {}\nidx_to_mid = {}\nfor (idx, mid) in enumerate(data[item_id_col].unique().tolist()):\n    mid_to_idx[mid] = idx\n    idx_to_mid[idx] = mid\n    \nuid_to_idx = {}\nidx_to_uid = {}\nfor (idx, uid) in enumerate(data[user_id_col].unique().tolist()):\n    uid_to_idx[uid] = idx\n    idx_to_uid[idx] = uid\n    \ndef map_ids(row, mapper):\n    return mapper[row]\n\n\nI = data[user_id_col].apply(map_ids, args=[uid_to_idx]).values\nJ = data[item_id_col].apply(map_ids, args=[mid_to_idx]).values\nV = np.ones(I.shape[0])\npurchases = sparse.coo_matrix((V, (I, J)), dtype=np.float64)\npurchases = purchases.tocsr()","04fdcbea":"def train_test_split(ratings, min_count, item_fraction, user_fraction=None):\n    \"\"\"\n    Split recommendation data into train and test sets\n    \n    Params\n    ------\n    ratings : scipy.sparse matrix\n        Interactions between users and items.\n    min_count : int\n        Number of user-item-interactions per user to be considered\n        a part of the test set\n    item_fraction : float\n        Fraction of users' items to go to the test set\n    user_fraction : float\n        Fraction of users to split off some of their\n        interactions into test set. If None, then all \n        users are considered.\n    \"\"\"\n    # Note: likely not the fastest way to do things below.\n    train = ratings.copy().tocoo()\n    test = sparse.lil_matrix(train.shape)\n    \n    if user_fraction:\n        try:\n            user_index = np.random.choice(\n                np.where(np.bincount(train.row) >= min_count)[0], \n                replace=False,\n                size=np.int32(np.floor(user_fraction * train.shape[0]))\n            ).tolist()\n        except:\n            print(('Not enough users with > {} '\n                  'interactions for fraction of {}')\\\n                  .format(min_count, user_fraction))\n            raise\n    else:\n        user_index = range(train.shape[0])\n        \n    train = train.tolil()\n\n    for user in user_index:\n        test_ratings = np.random.choice(ratings.getrow(user).indices, \n                                        size=int(len(ratings.getrow(user).indices) * item_fraction), \n                                        replace=False)\n        train[user, test_ratings] = 0.\n        # These are just 1.0 right now\n        test[user, test_ratings] = ratings[user, test_ratings]\n   \n    # Test and training are truly disjoint\n    assert(train.multiply(test).nnz == 0)\n    return train.tocsr(), test.tocsr(), user_index","d4a2b400":"# 25% of the users with minimum 53 purchases will have 25% of their ratings be part of the test data\n# 53 is the median \ntrain, test, user_index = train_test_split(purchases, 53, 0.25, user_fraction=0.25)","900e42ef":"train","914da141":"test","dd41584b":"# top k\nk = 5\n# random recommendations\nrandom_recommendations = np.random.randint(0, test.shape[1], size=(test.shape[0], k))\n\n# most popular k items\nnp_num_purchases_per_item = train.sum(axis=0)\ntop_recommendations = np_num_purchases_per_item.argsort()[::-1][:k]","682dabd9":"def apk(y, recommendations, k):\n    precs = []\n    for i in range(y.shape[0]):\n        y_i = y[i][0]\n        # skip empty\n        if len(y_i.nonzero()[1]) == 0:\n            continue\n        yhat_i = recommendations[i]\n        hits = len(set(recommendations[7]).intersection(set(y_i.nonzero()[1])))\n        precs.append(hits \/ k)\n\n    return np.mean(precs)\n\ndef coverage_at_k(y, recommendations, k):\n    all_recommended_items = set(recommendations[:, :k].ravel())\n    coverage = len(all_recommended_items) \/ y.shape[1]\n    return coverage","a681875a":"# top k\nk = 5\n\n# bootstrap evaluation\nn_repeats = 10\nlist_random_apk = []\nlist_top_apk = []\nlist_random_covk = []\nlist_top_covk = []\n\nfor i in range(n_repeats):\n    train, test, user_index = train_test_split(purchases, 53, 0.25, user_fraction=0.25)\n    \n    # random recommendations\n    random_recommendations = np.random.randint(0, test.shape[1], size=(test.shape[0], k))\n\n    # most popular k items\n    np_num_purchases_per_item = train.sum(axis=0)\n    top_recommendations = np_num_purchases_per_item.argsort()[::-1][:, :k]\n    top_recos_mat = np.tile(np.asarray(top_recommendations), test.shape[0]).reshape((test.shape[0], k))\n    \n    random_reco_apk5 = apk(test, random_recommendations, 5)\n    top_reco_apk5 = apk(test, top_recos_mat, 5)\n    \n    random_reco_cov5 = coverage_at_k(test, random_recommendations, 5)\n    top_reco_cov_5 = coverage_at_k(test, top_recos_mat, 5)\n    \n    list_random_apk.append(random_reco_apk5)\n    list_top_apk.append(top_reco_apk5)\n    list_random_covk.append(random_reco_apk5)\n    list_top_covk.append(top_reco_cov_5)","1d33ea70":"def compile_result(list_result, algorithm_name, k, column_name):\n    df_result = pd.DataFrame(list_result, columns=[column_name])\n    df_result[\"Algorithm\"] = algorithm_name\n    df_result[\"k\"] = k\n    return df_result\n\ndf_result1 = compile_result(list_random_apk, \"Random\", k, \"AP\")\ndf_result2 = compile_result(list_top_apk, \"Popular\", k, \"AP\")\ndf_baseline_results = df_result1.append(df_result2)\n\ndf_result1_cov = compile_result(list_random_covk, \"Random\", k, \"Coverage\")\ndf_result2_cov = compile_result(list_top_covk, \"Popular\", k, \"Coverage\")\ndf_baseline_results_cov = df_result1_cov.append(df_result2_cov)\n\ndf_baseline_results[\"Coverage\"] = df_baseline_results_cov[\"Coverage\"]\n\ndf_baseline_results.groupby([\"Algorithm\", \"k\"])[[\"AP\", \"Coverage\"]].agg([\"mean\", \"std\"])","535fcdac":"from sklearn.neighbors import NearestNeighbors\nfrom sklearn.impute import SimpleImputer\n\n# there's only three categories. We could do with much more...\ndef create_cbf_recommendations(data, k=5):\n    feature_cols = [\"Product_Category_1\", \"Product_Category_2\", \"Product_Category_3\", ]\n    data_features = data.drop_duplicates(\"Product_ID\").set_index(\"Product_ID\")[feature_cols]\n\n    # impute features with the mode\n    imputer = SimpleImputer(strategy='most_frequent')\n\n    data_features_imputed = imputer.fit_transform(data_features)\n    nbrs = NearestNeighbors(n_neighbors=k+1, algorithm=\"brute\", metric=\"cosine\").fit(data_features_imputed)\n\n    # we average out the item feature vectors of everything a customer bought and then query from our data structure\n    data_imputed_features = pd.DataFrame(imputer.transform(data[feature_cols]), columns=feature_cols, \n                                         index=data[\"User_ID\"])\n    user_bought_item_features = data_imputed_features.groupby(level=0)[feature_cols].mean()\n    distances, indices = nbrs.kneighbors(user_bought_item_features)\n\n    # and here's how you get the items\n    nearest_cols = [\"Item\" + str(v) for v in range(1, k+1)]\n    distance_cols = [\"Distance\" + str(v) for v in range(1, k+1)]\n    return pd.DataFrame(np.hstack((indices[:, 1:k+1], distances[:, 1:k+1])), columns=nearest_cols + distance_cols)","152c8a58":"df_recos = create_cbf_recommendations(data)\ndf_recos[:5]","f3d3ed85":"def convert_sparse_to_df(sp_data, idx_to_uid, idx_to_mid):\n    # convert sparse matrix to pandas so it can be input to our CBF function\n    # (not the best way to do it, but the data is small enough anyway)\n    train_coo = sp_data.tocoo()\n    df_train = pd.DataFrame({\"User_ID\" : train_coo.row, \"Product_ID\" : train_coo.col})\n    # map these indices to their ids\n    df_train[\"User_ID\"] = [idx_to_uid[v] for v in df_train[\"User_ID\"]]\n    df_train[\"Product_ID\"] = [idx_to_mid[v] for v in df_train[\"Product_ID\"]]\n    \n    return df_train","e108954f":"# top k\nk = 5\n\n# bootstrap evaluation\nn_repeats = 10\nlist_cbf_apk = []\nlist_cbf_covk = []\n\nfor i in range(n_repeats):\n    train, test, user_index = train_test_split(purchases, 53, 0.25, user_fraction=0.25)\n    \n    df_train = convert_sparse_to_df(train, idx_to_uid, idx_to_mid)\n    df_train = df_train.merge(data, how='left')\n    cbf_recos = create_cbf_recommendations(df_train)\n    \n    # get the matrix form so apk would work\n    cbf_np_recos = cbf_recos.filter(like=\"Item\").values\n    \n    cbf_reco_apk5 = apk(test, cbf_np_recos, 5)\n    cbf_reco_cov5 = coverage_at_k(test, cbf_np_recos, 5)\n    \n    list_cbf_apk.append(cbf_reco_apk5)\n    list_cbf_covk.append(cbf_reco_cov5)","506100f7":"df_result_cbf_ap = compile_result(list_cbf_apk, \"CBF\", k, \"AP\")\ndf_result_cbf_cov = compile_result(list_cbf_covk, \"CBF\", k, \"Coverage\")\n\ndf_result_cbf_ap[\"Coverage\"] = df_result_cbf_cov[\"Coverage\"]\ndf_result_cbf_ap.groupby([\"Algorithm\", \"k\"])[[\"AP\", \"Coverage\"]].agg([\"mean\", \"std\"])","46c50c79":"import implicit\n\n# top k\nk = 5\n\n# bootstrap evaluation\nn_repeats = 10\nlist_wmf_apk = []\nlist_wmf_covk = []\n\nfor i in range(n_repeats):\n    train, test, user_index = train_test_split(purchases, 53, 0.25, user_fraction=0.25)\n    # train model then create recommendations\n    model = implicit.als.AlternatingLeastSquares(factors=50)\n    model.fit(train.T)\n    \n    list_wmf_recommendation = []\n    # create recommendations for all users\n    for user_id in range(len(idx_to_uid)):\n        recommendations = model.recommend(user_id, train, N=k)\n        wmf_recommendation = [v[0] for v in recommendations]\n        list_wmf_recommendation.append(wmf_recommendation)\n    \n    wmf_recommendation = np.array(list_wmf_recommendation)\n    wmf_reco_apk5 = apk(test, wmf_recommendation, k)\n    wmf_reco_cov5 = coverage_at_k(test, wmf_recommendation, k)\n    \n    list_wmf_apk.append(wmf_reco_apk5)\n    list_wmf_covk.append(wmf_reco_cov5)","01126e6d":"df_result_wmf_ap = compile_result(list_wmf_apk, \"WMF\", k, \"AP\")\ndf_result_wmf_cov = compile_result(list_wmf_covk, \"WMF\", k, \"Coverage\")\n\ndf_result_wmf_ap[\"Coverage\"] = df_result_wmf_cov[\"Coverage\"]\ndf_result_wmf_ap.groupby([\"Algorithm\", \"k\"])[[\"AP\", \"Coverage\"]].agg([\"mean\", \"std\"])","0040c4a9":"user_features_cols = [\"Gender\", \"Age\", \"Occupation\", \"City_Category\", \"Stay_In_Current_City_Years\", \"Marital_Status\"]\nuser_features = data.drop_duplicates(\"User_ID\").set_index(\"User_ID\")[user_features_cols]\n\n# generate new feature - average of what users purchase in their basket\nmean_purchases_per_user = data.groupby(\"User_ID\")[\"Purchase\"].mean()\n\n# numpy array\nnp_user_features = user_features.join(mean_purchases_per_user).values","893c0a42":"# transform occupation and city to dummies form\nuser_features_dummies = pd.get_dummies(user_features[[\"Occupation\", \"City_Category\"]])\n\n# transform gender to binary\nuser_gender = pd.Series(np.where(user_features[\"Gender\"] == \"F\", 1, 0), index=user_features.index, name=\"Gender\")\n\n# transform age to ordinal\nage_to_value = {\"0-17\" : 0, \"18-25\" : 1,\"26-35\" : 2, \"36-45\" : 3, \"46-50\" : 4, \"51-55\" : 5, \"55+\" : 6}\nage_feature = user_features[\"Age\"].replace(age_to_value)\n\n# transform stay to ordinal\nstay_in_city = pd.to_numeric(user_features[\"Stay_In_Current_City_Years\"].str.replace(\"+\", \"\"))\n\nuser_features_processed = user_features_dummies.join(user_gender)\\\n                                                .join(age_feature)\\\n                                                .join(stay_in_city)\\\n                                                .join(user_features[\"Marital_Status\"])\n\nnp_user_features_processed = user_features_processed.values\n\n# concatenate to identity matrix\nnp_user_features = np.hstack((np.eye(len(user_features_dummies)), np_user_features_processed))\nsp_user_features = sparse.csr_matrix(np_user_features)","3619f49d":"from sklearn.impute import SimpleImputer\nitem_feature_cols = [\"Product_Category_1\", \"Product_Category_2\", \"Product_Category_3\", ]\nitem_features = data.drop_duplicates(\"Product_ID\").set_index(\"Product_ID\")[item_feature_cols]\n\n# impute features with the mode\nimputer = SimpleImputer(strategy='most_frequent')\n\nitem_features_imputed = imputer.fit_transform(item_features)\nnp_item_features = np.hstack((np.eye(len(item_features)), item_features_imputed))\nsp_item_features = sparse.csr_matrix(np_item_features)","ceb8a933":"from lightfm import LightFM\n\ndef get_lightfm_top_k_recos(model, train, sp_user_features = None, sp_item_features = None, k =5):\n    users_coo = np.repeat(range(train.shape[0]), train.shape[1])\n    items_coo = np.tile(range(train.shape[1]), train.shape[0])\n    \n    if sp_user_features is not None:\n        recommendations = model.predict(users_coo,\n                                    items_coo,\n                                    user_features=sp_user_features.T,\n                                    item_features=sp_item_features.T)\n    else:\n        recommendations = model.predict(users_coo,items_coo,)\n        \n    recommendations = recommendations.reshape(train.shape[0],-1)\n    \n    # sort top . Since we're getting only the top k, the training set might reduce the recommendations \n    # so we have a multiplier\n    recommendations = np.argsort(-recommendations)\n\n    # remove training instances (already liked items)\n    list_pairwise_reco = []\n    for idx in range(train.shape[0]):\n        list_pairwise_reco.append(recommendations[idx][~np.isin(recommendations[idx], train[idx].indices)][:k].tolist())\n\n    return np.array(list_pairwise_reco)","c9fbff96":"NUM_EPOCHS = 20\nrandom_seed = 1234\n\n# top k\nk = 5\n\n# bootstrap evaluation\nn_repeats = 10\nlist_bpr_apk = []\nlist_bpr_covk = []\nlist_bpr_if_apk = []\nlist_bpr_if_covk = []\nlist_warp_apk = []\nlist_warp_covk = []\nlist_warp_if_apk = []\nlist_warp_if_covk = []\nlist_warp_kos_apk = []\nlist_warp_kos_covk = []\nlist_warp_kos_if_apk = []\nlist_warp_kos_if_covk = []\n\nfor i in tqdm_notebook(range(n_repeats)):\n    train, test, user_index = train_test_split(purchases, 53, 0.25, user_fraction=0.25)\n    # train model\n    bpr_model = LightFM(no_components=30, loss='bpr', random_state=random_seed)\n    bpr_model_if = LightFM(no_components=30, loss='bpr', random_state=random_seed)\n    warp_model = LightFM(no_components=30, loss='warp', random_state=random_seed)\n    warp_model_if = LightFM(no_components=30, loss='warp', random_state=random_seed)\n    kos_model = LightFM(no_components=30, loss='warp-kos', random_state=random_seed)\n    kos_model_if = LightFM(no_components=30, loss='warp-kos', random_state=random_seed)\n    \n    for _ in range(NUM_EPOCHS):\n        bpr_model.fit(train)\n        bpr_model_if.fit(train, sp_user_features, sp_item_features)\n\n        warp_model.fit(train)\n        warp_model_if.fit(train, sp_user_features, sp_item_features)\n\n        kos_model.fit(train)\n        kos_model_if.fit(train, sp_user_features, sp_item_features)\n\n    bpr_model_recos= get_lightfm_top_k_recos(bpr_model, train,)\n    bpr_model_if_recos = get_lightfm_top_k_recos(bpr_model_if, train, sp_user_features, sp_item_features)\n    warp_model_recos  = get_lightfm_top_k_recos(warp_model, train, )\n    warp_model_if_recos = get_lightfm_top_k_recos(warp_model_if, train, sp_user_features, sp_item_features)\n    kos_model_recos = get_lightfm_top_k_recos(kos_model, train, )\n    kos_model_if_recos= get_lightfm_top_k_recos(kos_model_if, train, sp_user_features, sp_item_features)\n    \n    list_bpr_apk.append(apk(test, bpr_model_recos, k))\n    list_bpr_covk.append(coverage_at_k(test, bpr_model_recos, k))\n    \n    list_bpr_if_apk.append(apk(test, bpr_model_if_recos, k))\n    list_bpr_if_covk.append(coverage_at_k(test, bpr_model_if_recos, k))\n    \n    list_warp_apk.append(apk(test, warp_model_recos, k))\n    list_warp_covk.append(coverage_at_k(test, warp_model_recos, k))\n    \n    list_warp_if_apk.append(apk(test, warp_model_if_recos, k))\n    list_warp_if_covk.append(coverage_at_k(test, warp_model_if_recos, k))\n    \n    list_warp_kos_apk.append(apk(test, kos_model_recos, k))\n    list_warp_kos_covk.append(coverage_at_k(test, kos_model_recos, k))\n    \n    list_warp_kos_if_apk.append(apk(test, kos_model_if_recos, k))\n    list_warp_kos_if_covk.append(coverage_at_k(test, kos_model_if_recos, k))","30b4a054":"df_result_bpr_ap = compile_result(list_bpr_apk, \"BPR\", k, \"AP\")\ndf_result_bpr_ap[\"Coverage\"] = compile_result(list_bpr_covk, \"BPR\", k, \"Coverage\")[\"Coverage\"]\n\ndf_result_bpr_if_ap = compile_result(list_bpr_if_covk, \"BPR-IF\", k, \"AP\")\ndf_result_bpr_if_ap[\"Coverage\"] = compile_result(list_bpr_if_covk, \"BPR-IF\", k, \"Coverage\")[\"Coverage\"]\n\ndf_result_warp_ap = compile_result(list_warp_apk, \"WARP\", k, \"AP\")\ndf_result_warp_ap[\"Coverage\"] = compile_result(list_warp_covk, \"WARP\", k, \"Coverage\")[\"Coverage\"]\n\ndf_result_warp_if_ap = compile_result(list_warp_if_apk, \"WARP-IF\", k, \"AP\")\ndf_result_warp_if_ap[\"Coverage\"] = compile_result(list_warp_if_covk, \"WARP-IF\", k, \"Coverage\")[\"Coverage\"]\n\ndf_result_warp_kos_ap = compile_result(list_warp_kos_apk, \"WARP-KOS\", k, \"AP\")\ndf_result_warp_kos_ap[\"Coverage\"] = compile_result(list_warp_kos_covk, \"WARP-KOS\", k, \"Coverage\")[\"Coverage\"]\n\ndf_result_warp_kos_if_ap = compile_result(list_warp_kos_if_apk, \"WARP-KOS-IF\", k, \"AP\")\ndf_result_warp_kos_if_ap[\"Coverage\"] = compile_result(list_warp_kos_if_covk, \"WARP-KOS-IF\", k, \"Coverage\")[\"Coverage\"]","b87f4752":"df_pairwise_results = pd.concat((df_result_bpr_ap,df_result_bpr_if_ap,\n                              df_result_warp_ap, df_result_warp_if_ap,\n                              df_result_warp_kos_ap,df_result_warp_kos_if_ap))\ndf_pairwise_results.groupby([\"Algorithm\", \"k\"])[[\"AP\", \"Coverage\"]].agg([\"mean\", \"std\"])","eaffa0ce":"cm = sns.light_palette(\"green\", as_cmap=True)\n\ndf_all_results = pd.concat((df_baseline_results, df_result_cbf_ap, df_result_wmf_ap, df_pairwise_results))\\\n                            .groupby([\"Algorithm\", \"k\"])[[\"AP\", \"Coverage\"]].agg([\"mean\", \"std\"])\n\ns = df_all_results.style.background_gradient(cmap=cm)\ns","fc5cd180":"NUM_EPOCHS = 1\nrandom_seed = 1234\n\nlist_all_results = []\nfor i in tqdm_notebook(range(n_repeats)):\n    train, test, user_index = train_test_split(purchases, 53, 0.25, user_fraction=0.25)\n    # train model\n    bpr_model = LightFM(no_components=30, loss='bpr', random_state=random_seed)\n    warp_model = LightFM(no_components=30, loss='warp', random_state=random_seed)\n    kos_model = LightFM(no_components=30, loss='warp-kos', random_state=random_seed)\n    \n    bpr_model.fit(train, epochs=NUM_EPOCHS, num_threads=2)\n    warp_model.fit(train, epochs=NUM_EPOCHS, num_threads=2)\n    kos_model.fit(train, epochs=NUM_EPOCHS, num_threads=2)\n    \n    # pairwise recos\n    bpr_model_recos= get_lightfm_top_k_recos(bpr_model, train, k=30)\n    warp_model_recos  = get_lightfm_top_k_recos(warp_model, train, k=30)\n    kos_model_recos = get_lightfm_top_k_recos(kos_model, train, k=30)\n    \n    # cbf recos\n    df_train = convert_sparse_to_df(train, idx_to_uid, idx_to_mid)\n    df_train = df_train.merge(data, how='left')\n    cbf_recos = create_cbf_recommendations(df_train, k=30)\n    \n    for k in np.linspace(5, 30, 6).astype('int'):\n        # random recommendation\n        random_recommendations = np.random.randint(0, test.shape[1], size=(test.shape[0], k))\n        cbf_recos_k = cbf_recos.filter(like=\"Item\").values[:,:k]\n        bpr_model_recos_k = bpr_model_recos[:,:k]\n        warp_model_recos_k = warp_model_recos[:,:k]\n        kos_model_recos_k = kos_model_recos[:,:k]\n        \n        list_results = [{\"Algorithm\" : \"Random\", \"k\" : k, \"AP\": apk(test, random_recommendations, k), \n                         \"Coverage\": coverage_at_k(test, random_recommendations, k)},\n                        {\"Algorithm\" : \"CBF\", \"k\" : k, \"AP\": apk(test, cbf_recos_k, k), \n                         \"Coverage\": coverage_at_k(test, cbf_recos_k, k)},\n                        {\"Algorithm\" : \"BPR\", \"k\" : k, \"AP\": apk(test, bpr_model_recos_k, k), \n                         \"Coverage\": coverage_at_k(test, bpr_model_recos_k, k)},\n                        {\"Algorithm\" : \"WARP\", \"k\" : k, \"AP\": apk(test, warp_model_recos_k, k), \n                         \"Coverage\": coverage_at_k(test, warp_model_recos_k, k)},\n                        {\"Algorithm\" : \"WARP-KOS\", \"k\" : k, \"AP\": apk(test, kos_model_recos_k, k), \n                         \"Coverage\": coverage_at_k(test, kos_model_recos_k, k)},\n                       ]\n        \n        list_all_results.extend(list_results)","1a7ac5b5":"df_all_results = pd.DataFrame(list_all_results)\ndf_results_mean = df_all_results.groupby([\"Algorithm\", \"k\"])[[\"AP\", \"Coverage\"]].mean()","e46b9d51":"ax = plt.figure(figsize=(12, 7)).add_subplot(111)\ndf_results_mean.loc[\"Random\"][\"AP\"].plot(ax=ax, label='Random', style='o-', ms=10)\ndf_results_mean.loc[\"CBF\"][\"AP\"].plot(ax=ax, label=\"CBF\", style='*-', ms=10)\ndf_results_mean.loc[\"BPR\"][\"AP\"].plot(ax=ax, label=\"BPR\", style='x--', ms=10)\ndf_results_mean.loc[\"WARP\"][\"AP\"].plot(ax=ax, label=\"WARP\", style='8--', ms=10)\ndf_results_mean.loc[\"WARP-KOS\"][\"AP\"].plot(ax=ax, label=\"WARP-KOS\", style='D--', ms=10)\n\nplt.legend()\nplt.title(\"Average Precision from k=5 to k=30\", fontsize=14);","47def9eb":"ax = plt.figure(figsize=(12, 7)).add_subplot(111)\ndf_results_mean.loc[\"Random\"][\"Coverage\"].plot(ax=ax, label='Random', style='o-', ms=10)\ndf_results_mean.loc[\"CBF\"][\"Coverage\"].plot(ax=ax, label=\"CBF\", style='*-', ms=10)\ndf_results_mean.loc[\"BPR\"][\"Coverage\"].plot(ax=ax, label=\"BPR\", style='x--', ms=10)\ndf_results_mean.loc[\"WARP\"][\"Coverage\"].plot(ax=ax, label=\"WARP\", style='8--', ms=10)\ndf_results_mean.loc[\"WARP-KOS\"][\"Coverage\"].plot(ax=ax, label=\"WARP-KOS\", style='D--', ms=10)\n\nplt.legend()\nplt.title(\"Coverage from k=5 to k=30\", fontsize=14);","69332bd7":"# Collaborative Filtering - WMF\nWe'll do hyperparameter optimization later, if this algorithm provides compelling results on random hyperparameters.","763308f7":"BPR then WARP seems to perform well. The addition of user and item features seems to worsen the model by a bit. I think a little optimization could go a long way for this one. With respect to coverage, WARP KOS and CBF leads the pack while WMF seems to have a health mix of both AP and Coverage. \n\n# More than k=5\nLet's compare WMF, BPR, WARP and CBF from k=5 to k=30.","59c5d237":"# distribution of purchase. What is purchase anyway?\n- Seems like purchase is how much the product was.\n- It isn't a very good indicator of \"confidence\" for implicit ratings since different products cost different anyway. If we did this, then the costly products would get recommended more often.","492ce1b8":"# Evaluation Methods\nFirst, let's declare baseline methods.\n1. Random recommendations and \n2. top 10 recommendations","ca8e3ddb":"# So in summary:","664a0bbd":"Seems that the random method is significantly better in average precision. This tells us that customers may have their respective niches where the top 5 most popular items just won't do.\n\nNow that we have baselines, we'll set out to do actual algorithms.\n\n# Content-Based Filtering\nFor each product, we get its features and create a features vector. We then use scikit-learn's NearestNeighbors module to create nearest neighbor queries.","872a2880":"## CBF Bootstrap Evaluation","799283e3":"# Pairwise Ranking from LightFM","7ccf5f95":"# Ratings Matrix Stats","62e605f7":"# Would-be algorithms to use\n- We'll start with baselines (popular and random)\n- We'll follow up with a content-based filtering algorithm (just nearest neighbors).\n- We're using implicit data so we could use WMF, Pairwise Ranking or factorization machines.\n- We'll establish a single train-test split of the ratings matrix\n\n## Converting to ratings matrix\n(See Ethan's blog about making this work: https:\/\/www.ethanrosenthal.com\/2016\/10\/19\/implicit-mf-part-1\/)"}}