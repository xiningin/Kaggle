{"cell_type":{"d5b8aec4":"code","3012c8ac":"code","f1533077":"code","b903ba0e":"code","f1823081":"code","1feb0aa6":"code","71f8c170":"code","0370ef06":"code","d4a19ced":"code","116fe8aa":"code","ffc7b495":"code","71d300b9":"code","fd574201":"code","1351191a":"code","d2d7bdc8":"code","737a4cbb":"code","4ec1aa16":"code","72bd4a30":"code","1e122a4f":"code","e621aa67":"code","e4e20b22":"code","9193eea6":"code","8ee62cc8":"code","f15a7cec":"code","bff17a08":"code","57ea5b3a":"code","b95b03eb":"code","fa43ff50":"code","127916b9":"code","e3d822c7":"code","6b9947f3":"code","ac665464":"code","e3e423b1":"code","d0b0dd05":"code","554d7d93":"code","c710bba9":"code","7eaa408e":"code","d32e3527":"code","f0d271b3":"code","2261343f":"code","dcf5e59a":"code","446a5c5d":"code","497579ce":"code","22ebff22":"code","6012caf8":"code","6f7338ee":"code","2de2f1e0":"code","0dbbdaf4":"code","213e3552":"code","6b6cbe4e":"code","03fa6ff2":"code","7bf8e06c":"code","af8a4507":"code","0938ba69":"code","c403a7bf":"code","0f561581":"code","cd40baa2":"code","a2351ff2":"code","ca88fca4":"code","a6e30730":"code","903d8a80":"code","ed8637be":"code","3e85607a":"code","3e3e49d8":"code","165f6917":"code","f9d7a76e":"code","f9a681c9":"code","ebd8f9cd":"code","e519ef3a":"code","ffd95773":"code","5bac62eb":"code","d7e8fa13":"code","8819627f":"code","158107a1":"code","91205b34":"code","3f4bae70":"code","3ceed50e":"code","dae8b5d4":"code","498e13aa":"code","7e242eab":"markdown","61d7e179":"markdown","e8eec249":"markdown","dbe81764":"markdown","21e98bcf":"markdown","a4389d0f":"markdown","3663e1b2":"markdown","e41b8ad6":"markdown","c67d92f0":"markdown","eefb3a1d":"markdown","e1b9f28a":"markdown","cdefe7bf":"markdown","84e85b28":"markdown","718a2734":"markdown","050f5433":"markdown","6ac64b47":"markdown","0faf198b":"markdown","f0056254":"markdown","88f6bb93":"markdown","254461bf":"markdown","30f06743":"markdown","5a164e75":"markdown","7de33b8a":"markdown","4ee88be7":"markdown","29505d21":"markdown","64c063a3":"markdown"},"source":{"d5b8aec4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3012c8ac":"from sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression, VarianceThreshold, SelectFromModel\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\nfrom sklearn.decomposition import PCA\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import GridSearchCV\nimport seaborn as sns\nimport matplotlib.pyplot as plt","f1533077":"pd.set_option('display.max_rows', 500)\npd.set_option('display.float_format', lambda x: '%.5f' % x)","b903ba0e":"train_path = '\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv'\ntest_path = '\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv'\ntrain = pd.read_csv(train_path)\ntest = pd.read_csv(test_path)\ntrain.head()","f1823081":"test.head()","1feb0aa6":"print(f'Training Data shape: {train.shape} \\n Test Data shape: {test.shape}')","71f8c170":"#divide columns in categorical and numerical ones to have a better overview\ncat_columns = train.select_dtypes(include=['object']).columns.tolist()\nnum_columns = train.select_dtypes(exclude=['object']).columns.tolist()\nprint(f'Categorical columns: \\n {cat_columns} \\n Numerical columns: \\n {num_columns}')","0370ef06":"#remove first and last element of the num_columns list as Id and SalePrice are not wanted inside the list\nnum_columns = num_columns[:-1]","d4a19ced":"num_columns","116fe8aa":"#check for missing values\n#train[cat_columns].isna().sum()\n\n#function to only give back the columns with missing values to not have a long list with all other features that don't have missing values\ndef check_missing(df):\n    missing = df.isna().sum()[df.isna().any()==True]\n    df_out = pd.DataFrame({'missing':missing})\n    return df_out\n\ncheck_missing(train[cat_columns])","ffc7b495":"check_missing(train[num_columns])","71d300b9":"X = train.drop(['SalePrice'], axis=1)\ny = train['SalePrice']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.8, random_state=8)\nX_train.head()","fd574201":"X_test.head()","1351191a":"test.head()","d2d7bdc8":"num_imputing = make_pipeline(\n    SimpleImputer(strategy='constant', fill_value=0)\n)","737a4cbb":"cat_imputing = make_pipeline(\n    (SimpleImputer(strategy='constant', fill_value='NA'))\n)","4ec1aa16":"# find all the ordinal features according to the data description and put them in a list\nordinal_features = ['ExterQual','ExterCond','KitchenQual','BsmtQual','BsmtCond','HeatingQC','FireplaceQu','GarageQual','GarageCond','GarageFinish','BsmtExposure','BsmtFinType1','BsmtFinType2','Functional','CentralAir','LandSlope','PavedDrive','Fence','PoolQC','Alley','Street','Utilities']\n\n# the rest of the categorical features goes into the list of nominal categorical features\nnominal_features = list(set(cat_columns) - set(ordinal_features))","72bd4a30":"# lists with values for ordinal feature encoding\nql5 =['None','Po','Fa','TA','Gd','Ex']\nfin=['None','Unf','RFn','Fin']\nexpo=['None','No','Mn','Av','Gd']\nfint=['None','Unf','LwQ','Rec','BLQ','ALQ','GLQ']\nfunc=['None','Sal','Sev','Maj2','Maj1','Mod','Min2','Min1','Typ']\nyn=['Y','N']\nls=['None','Sev','Mod','Gtl']\npad=['N','P','Y']\nfen=['None','MnWw','GdWo','MnPrv','GdPrv']\nql4=['None','Fa','TA','Gd','Ex']\nal=['None','Grvl','Pave']\nst=['None','Grvl','Pave']\nutil=['ELO','NoSeWa','NoSewr','AllPub']\n\n# for each feature in the ordinal_feature list there has to be an encoding category in the following categories list to be passed to the OrdinalEncoder\nordinal_categories = [ql5,ql5,ql5,ql5,ql5,ql5,ql5,ql5,ql5,fin,expo,fint,fint,func,yn,ls,pad,fen,ql4,al,st,util]","1e122a4f":"#check if all columns are in the column lists - should be 79 (81 original columns - 2 (removed Id and SalePrice))\nlen(ordinal_features)+len(nominal_features)+len(num_columns)","e621aa67":"# create ordinal and one-hot encoding Pipeline steps\n\nordinal_enc = Pipeline(steps=[\n    ('ordinal_encoder', OrdinalEncoder(categories=ordinal_categories))\n])\n\none_hot_enc = Pipeline(steps=[\n    ('one_hot_encoder', OneHotEncoder(sparse=False, handle_unknown='ignore')) #drop='first'\n])","e4e20b22":"# use ColumnTransformer to get one single feature space as output - basically relevant when using multiple transformers because of different columns\nimputing = ColumnTransformer(transformers=[\n    ('imp_nums', num_imputing, num_columns),\n    ('imp_cats', cat_imputing, cat_columns)\n])\n\nencoding = ColumnTransformer(transformers=[\n    ('enc_nums', \"passthrough\", num_columns),\n    ('enc_ord', ordinal_enc, ordinal_features),\n    ('enc_nom', one_hot_enc, nominal_features)\n])\n","9193eea6":"scaling = Pipeline(steps=[\n    ('scale', MinMaxScaler())\n])","8ee62cc8":"# try simple version of pipeline\ncat_encoding = ColumnTransformer(transformers=[\n    ('enc_ord', ordinal_enc, ordinal_features),\n    ('enc_nom', one_hot_enc, nominal_features)\n])\n\n\ncats = Pipeline(steps=[\n    ('impute_cats', cat_imputing),\n    #('encode_cats', cat_encoding)\n    ('encode_cats', one_hot_enc)\n])\n\nnums = Pipeline(steps=[\n    ('impute_nums', num_imputing)\n])\n\npreprocess = ColumnTransformer(transformers=[\n    ('cats', cats, cat_columns),\n    ('nums', nums, num_columns)\n])\n\nfull_preprocess2 = Pipeline(steps=[\n    ('preprocess',preprocess),\n    ('scaling',scaling)\n])","f15a7cec":"# full_preprocess =  Pipeline(steps=[\n#      ('imputing', imputing),\n#      ('encoding', encoding),\n#      ('scaling', scaling)   \n# ])","bff17a08":"pd.DataFrame(full_preprocess2.fit_transform(X_train))","57ea5b3a":"lm_pipeline = Pipeline(steps=[\n    ('full_preprocess', full_preprocess2),\n    ('model', LinearRegression())\n])","b95b03eb":"lm_pipeline.fit(X_train, y_train)","fa43ff50":"predictions = lm_pipeline.predict(X_test)","127916b9":"# use logarithm of predictions and true_values for rmse calculation because this will be the evaluation measure in the competition\nrmse = mean_squared_error(y_test, predictions)**0.5\nrmse","e3d822c7":"rmsle = mean_squared_log_error(y_test, abs(predictions))**0.5\nrmsle","6b9947f3":"#evaluate the results\nresults = pd.DataFrame({\n    'predictions':predictions, \n    'true_values':y_test\n})\n\nresults['diff'] = abs(round(results['predictions'] - results['true_values']))\nresults.sort_values(by='diff', ascending = True).tail(5)#head(5)","ac665464":"t = X_test.copy()\nt['Price'] = y_test\nt['preds'] = predictions\nt['diff'] = t['Price'] - t['preds']\nt.sort_values(by='diff', ascending = True).head(5)","e3e423b1":"#remove features that have less then 90% variance (in columns with 0 and 1, 90% are 0 and only 10% are 1)\n\nselect_vt = Pipeline(steps=[\n    ('variance_threshold', VarianceThreshold(threshold=(0.9*(1-0.9))))\n])","d0b0dd05":"vt_pipeline = Pipeline(steps=[\n    ('full_preprocess', full_preprocess2),\n    ('feature_selection',select_vt),\n    ('model', LinearRegression())\n])","554d7d93":"vt_pipeline.fit(X_train, y_train)","c710bba9":"vt_predictions =vt_pipeline.predict(X_test)","7eaa408e":"rmsle = mean_squared_log_error(y_test, abs(vt_predictions))**0.5\nrmsle","d32e3527":"select_kbest = Pipeline(steps=[\n    ('select_kbest', SelectKBest(mutual_info_regression))\n])","f0d271b3":"kbest_pipeline = Pipeline(steps=[\n    ('full_preprocess', full_preprocess2),\n    ('feature_selection',select_kbest),\n    ('model', LinearRegression())\n])","2261343f":"kbest_pipeline.fit(X_train, y_train)","dcf5e59a":"kbest_predictions =kbest_pipeline.predict(X_test)","446a5c5d":"rmsle = mean_squared_log_error(y_test, abs(kbest_predictions))**0.5\nrmsle","497579ce":"from_model_pipeline = Pipeline(steps=[\n    ('full_preprocess', full_preprocess2),\n    ('feature_selection', SelectFromModel(RandomForestRegressor())),\n    ('model', LinearRegression()) #fit_intercept=False - regression lines starts at 0,0 to prevent negative price predictions but also influences rest of predictions that get worse\n])\n","22ebff22":"from_model_pipeline.fit(X_train, y_train)","6012caf8":"from_model_predictions =from_model_pipeline.predict(X_test)","6f7338ee":"sum(i < 0 for i in from_model_predictions)","2de2f1e0":"rmsle = mean_squared_log_error(y_test, abs(from_model_predictions))**0.5\nrmsle","0dbbdaf4":"random_forest_pipeline = Pipeline(steps=[\n    ('full_preprocess', full_preprocess2),\n    ('feature_selection', SelectFromModel(RandomForestRegressor())),\n    ('model', RandomForestRegressor())\n])","213e3552":"random_forest_pipeline.fit(X_train, y_train)","6b6cbe4e":"random_forest_predictions =random_forest_pipeline.predict(X_test)","03fa6ff2":"sum(i < 0 for i in random_forest_predictions)","7bf8e06c":"rmsle = mean_squared_log_error(y_test, abs(random_forest_predictions))**0.5\nrmsle","af8a4507":"test_df = pd.DataFrame(full_preprocess2.transform(test))","0938ba69":"test_df","c403a7bf":"test.head()","0f561581":"X_train.head()","cd40baa2":"test_preds = random_forest_pipeline.predict(test)","a2351ff2":"output = pd.DataFrame({'Id': test.Id,\n                       'SalePrice': test_preds})\noutput.head(2)","ca88fca4":"#output.to_csv('submission.csv', index=False)","a6e30730":"pca = make_pipeline(PCA())","903d8a80":"pca_pipeline = Pipeline(steps=[\n    ('full_preprocess', full_preprocess2),\n    ('pca',pca),\n    ('feature_selection', SelectFromModel(RandomForestRegressor())),\n    ('model', RandomForestRegressor())\n])","ed8637be":"pca_pipeline.fit(X_train, y_train)","3e85607a":"pca_predictions =pca_pipeline.predict(X_test)","3e3e49d8":"rmsle = mean_squared_log_error(y_test, pca_predictions)**0.5\nrmsle","165f6917":"adaboost_pipeline = Pipeline(steps=[\n    ('full_preprocess', full_preprocess2),\n    #('feature_selection', SelectFromModel(RandomForestRegressor())),\n    ('model', AdaBoostRegressor(DecisionTreeRegressor(max_depth=6), n_estimators=450, random_state=42))\n])","f9d7a76e":"adaboost_pipeline.fit(X_train, y_train)","f9a681c9":"adaboost_predictions =adaboost_pipeline.predict(X_test)","ebd8f9cd":"rmsle = mean_squared_log_error(y_test, adaboost_predictions)**0.5\nrmsle","e519ef3a":"test_preds = adaboost_pipeline.predict(test)","ffd95773":"output = pd.DataFrame({'Id': test.Id,\n                       'SalePrice': test_preds})\noutput.head(2)","5bac62eb":"output.to_csv('submission.csv', index=False)","d7e8fa13":"treereg_pipeline = Pipeline(steps=[\n    ('full_preprocess', full_preprocess2),\n    ('feature_selection', SelectFromModel(RandomForestRegressor())),\n    ('model', DecisionTreeRegressor(max_depth = 6, random_state=42))\n])\n\n# param_grid = {\n#     'model__max_depth': range(1, 10),\n#     'model__min_samples_leaf': range(1, 10),\n#     'model__min_samples_split': range(2, 10),\n#    # 'model__criterion':['squared_error','friedman_mse','absolute_error','poisson'],\n#     'model__max_features':['auto', 'sqrt','log2']\n#     }\n# search = GridSearchCV(adaboost_pipeline, param_grid, cv=15, scoring='accuracy', verbose=1, refit=True, n_jobs=-1)\n\n# search.fit(X_train,y_train)\n\ntreereg_pipeline.fit(X_train, y_train)","8819627f":"#search.best_params_","158107a1":"treereg_predictions =treereg_pipeline.predict(X_test)","91205b34":"rmsle = mean_squared_log_error(y_test, treereg_predictions)**0.5\nrmsle","3f4bae70":"df_plot = train.filter(['LotArea','OverallQual','TotalBsmtSF','1stFlrSF','GrLivArea','GarageCars','YrSold','SalePrice'])\ndf_plot.head()","3ceed50e":"g = sns.PairGrid(df_plot)\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\ng.add_legend()","dae8b5d4":"train.SalePrice.describe()","498e13aa":"plt.subplots(figsize=(12, 8))\nsns.scatterplot(x=X_train['LotArea'], y=y_train)\n#plt.axvline(x = 2.5, ymin=0, ymax=18000, c='red')\nplt.show()","7e242eab":"* There are different ways how to impute numerical values. In this case it doesn't make sense to impute with mean or median values as this would probably not reflect what the features tell us about the house. It seems more reasonable to impute with 0 values to say that this feature is not available for the house.","61d7e179":"Using different models does not really improve the predictions. Better try to do some more feature engineering.","e8eec249":"### Missing values in numerical features","dbe81764":"# Random Forest Regressor\n\nThis already worked good as model for Feature Selection - try as predicting Model","21e98bcf":"* Use Ordinal Encoding for ordinal categorical features - features with values that have a meaningful order and thus can be encoded with numbers in one single column instead of splitting up each value of a nominal categorical variable into seperate column\n\n* Use One-Hot Encoding for nominal categorical features - features that do not have a meaningful order - during encoding they are split into seperate columns for each unique value in the feature","a4389d0f":"# Use Grid Search for Parameter Optimization","3663e1b2":"# Try some other Model","e41b8ad6":"# Use a PCA for dimension reduction - with and without additional feature selection","c67d92f0":"### Missing values in categorical features\n\nHave look at the data description to see the available categories and if they are ordinal or not","eefb3a1d":"# Feature Selection\n\nAdd different feature selection approaches to improve predictions\n\n## Remove features with low variance using VarianceThreshold\n\nFeatures with low variance (have lots of the same values) don't add much information and thus could be removed","e1b9f28a":"## Scaling the Data\n\n* scaling prevents features that have bigger\/smaller values to have a bigger or smaller impact on the predictions\n\n* Do the scaling for all features, although for encoded nominal features there are only 0 and 1 in each dummified column so the scaler doesn't do anything\n","cdefe7bf":"# Basic Data Exploration - Missing values","84e85b28":"* Use dummy encoding for nominal categorical features, that means that there will be one column less than values in the variable because this can be explained with the other dummy variables from this feature\n* to achieve this use attributes drop='first' with the One Hot Encoder","718a2734":"* The Linear Regression Model sometimes predicts negative prices for houses. To prevent this I tried to set the beginning of the regression line to 0,0 but this also influences the rest of the predictions a lot and thus is not wanted\n\n* try some other models","050f5433":"## Dealing with the Categorical Features - Imputing - Ordinal Encoding and One Hot Encoding\n","6ac64b47":"## Try SelectKBest for Feature Selection","0faf198b":"* Replacing missing values with some value is important as the sklearn transformers don't work when there are missing values\n* For most of the features a missing value means that this feature is not available for the house, e.g. the house has no Pool or no Fireplace\n* According to the data description there is a \"NA\" category for these features so I'll replace the missing values with \"NA\"\n* there are some few features with only few missign values that do not have an \"NA\" category in the description. However, I'll also encode them with \"NA\" for simplicity.","f0056254":"### Combine Imputing and Encoding into the Pipeline","88f6bb93":"## Full Preprocess of the dataset","254461bf":"Until now there is no feature selection implemented. Maybe this is the problem of the model and the really high root mean squared error for the predictions.","30f06743":"# Preprocessing - Imputing, Encoding and Scaling the data","5a164e75":"PCA doesn't improve the predictions","7de33b8a":"# Splitting the Dataset into Train and Test Data\n\n* This is done to be able to evaluate the model performance","4ee88be7":"## Try also SelectFromModel for Feature Selection","29505d21":"## Dealing with the Numerical Features - Imputing missing Values","64c063a3":"# First Model Prediction\n\n## Using a simple Linear Regression"}}