{"cell_type":{"f616136f":"code","e8603e78":"code","09335886":"code","ee057567":"code","8f1af1ea":"code","2122c9ce":"code","18c1040d":"code","99cb55ff":"code","c7d0b24a":"code","595913cb":"code","a53d1a16":"code","7c2133f4":"code","212c58f1":"code","a8b9980f":"code","d68a6621":"code","d024b3ff":"code","d66f9d64":"code","b021de1a":"code","d82010c7":"code","bda9614c":"code","c2c8fbe0":"code","d4dab738":"markdown","5ed27f45":"markdown","950da64b":"markdown","98cfd296":"markdown","b332ae47":"markdown"},"source":{"f616136f":"import pandas as pd\nfrom ast import literal_eval\nimport sklearn.model_selection as skm\nimport sklearn.model_selection as skp\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords\nimport lightgbm as lgb\nimport numpy as np\nimport itertools as itr\nimport datetime as dt\nimport nltk\n\nimport plotly.offline as pyo\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.figure_factory as ff\nimport plotly.graph_objs as go\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\nimport seaborn as sea\nimport gc\nimport sys\nimport multiprocessing as mp\nimport six\nfrom abc import ABCMeta\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nimport statsmodels.api as sm\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\nimport math\n\ninit_notebook_mode(connected=True)\n%matplotlib inline\n","e8603e78":"from kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()","09335886":"(market_train_df, news_train_df) = env.get_training_data()","ee057567":"#market_train_df = market_train_df[market_train_df[\"assetName\"]==\"Apple Inc\"]\n#news_train_df = news_train_df[news_train_df[\"assetName\"] == \"Apple Inc\"]\nprint(market_train_df.shape)\nprint(news_train_df.shape)","8f1af1ea":"def fix_time_col(in_mkt_df, in_news_df):\n    if \"datetime\" in str(in_mkt_df[\"time\"].dtype):\n        in_mkt_df[\"time\"] = in_mkt_df[\"time\"].dt.date\n        in_news_df[\"time\"] = in_news_df[\"time\"].dt.date\n\ndef clean_assetName_col(in_mkt_df, in_news_df):\n    in_mkt_df[\"assetName\"] = in_mkt_df[\"assetName\"].str.lower()\n    in_mkt_df[\"assetName\"] = in_mkt_df[\"assetName\"].str.strip()\n    in_mkt_df[\"assetName\"] = in_mkt_df[\"assetName\"].str.replace(\"inc\",\"\")\n    in_mkt_df[\"assetName\"] = in_mkt_df[\"assetName\"].str.replace(\"llc\",\"\")\n    in_mkt_df[\"assetName\"] = in_mkt_df[\"assetName\"].str.replace(\"ltd\",\"\")\n    in_mkt_df[\"assetName\"] = in_mkt_df[\"assetName\"].str.strip()\n\n    in_news_df[\"assetName\"] = in_news_df[\"assetName\"].str.lower()\n    in_news_df[\"assetName\"] = in_news_df[\"assetName\"].str.strip()\n    in_news_df[\"assetName\"] = in_news_df[\"assetName\"].str.replace(\"inc\",\"\")\n    in_news_df[\"assetName\"] = in_news_df[\"assetName\"].str.replace(\"llc\",\"\")\n    in_news_df[\"assetName\"] = in_news_df[\"assetName\"].str.replace(\"ltd\",\"\")\n    in_news_df[\"assetName\"] = in_news_df[\"assetName\"].str.strip()","2122c9ce":"def get_final_assetName(row ):\n    min_mkt_assetNameSNo = row[\"min_mkt_assetNameSNo\"]\n    max_mkt_assetNameSNo = row[\"max_mkt_assetNameSNo\"]\n    min_assetNameSNo = row[\"min_assetNameSNo\"]\n    max_assetNameSNo = row[\"max_assetNameSNo\"]\n    if ((not math.isnan(min_mkt_assetNameSNo)) & (math.isnan(max_mkt_assetNameSNo))):\n        return min_assetNameSNo\n    elif ((not math.isnan(max_mkt_assetNameSNo)) & (math.isnan(min_mkt_assetNameSNo))):\n        return max_assetNameSNo\n    else:\n        return 0\n    \n    \ndef get_replace_assetName(row):\n    min_mkt_assetNameSNo = row[\"min_mkt_assetNameSNo\"]\n    max_mkt_assetNameSNo = row[\"max_mkt_assetNameSNo\"]\n    min_assetNameSNo = row[\"min_assetNameSNo\"]\n    max_assetNameSNo = row[\"max_assetNameSNo\"]\n    if ((math.isnan(min_mkt_assetNameSNo)) & (not math.isnan(max_mkt_assetNameSNo))):\n        return min_assetNameSNo\n    elif ((math.isnan(max_mkt_assetNameSNo)) & (not math.isnan(min_mkt_assetNameSNo))):\n        return max_assetNameSNo\n    else:\n        return 0\n\ndic_assetName={}\ndic_assetNameSNo={}\ndic_assetCodes={}\ndef set_assetNameCodes_SNo(in_mkt_df, in_news_df,dic_assetName, dic_assetNameSNo, dic_assetCodes):\n    set_asset = set(in_news_df[\"assetName\"].unique()) | set(in_mkt_df[\"assetName\"].unique())\n    set_codes = set(in_news_df[\"assetCodes\"].unique())\n    list_asset = list(set_asset)\n    list_codes = list(set_codes)\n    \n    if len(dic_assetName) == 0:\n        dic_assetName= {k:v for v,k in enumerate(list_asset)}\n        dic_assetNameSNo= {v:k for v,k in enumerate(list_asset)}\n        dic_assetCodes= {k:v for v,k in enumerate(list_codes)}\n    else: #dictionary already exists, find only for those elemnts that are extra\n        set_asset_new = set(dic_assetName.keys()) - set_asset\n        asset_max_SNo = len(dic_assetName)\n        dic_assetName_new = {k:v+asset_max_SNo for v,k in enumerate(list(set_asset_new))}\n        dic_assetNameSNo_new = {v+asset_max_SNo:k for v,k in enumerate(list(set_asset_new))}\n        dic_assetName.update(dic_assetName_new)\n        dic_assetNameSNo.update(dic_assetNameSNo_new)\n        \n        set_codes_new = set(dic_assetCodes.keys()-set_codes)\n        codes_max_SNo = len(dic_assetCodes)\n        dic_assetCodes_new = {k:v+codes_max_SNo for v,k in enumerate(list(set_codes_new))}\n        dic_assetCodes.update(dic_assetCodes_new)\n        \n    in_news_df[\"assetNameSNo\"] = in_news_df[\"assetName\"].map(dic_assetName)\n    in_mkt_df[\"assetNameSNo\"] = in_mkt_df[\"assetName\"].map(dic_assetName)\n    in_news_df[\"assetCodesSNo\"] = in_news_df[\"assetCodes\"].map(dic_assetCodes)\n    return dic_assetName, dic_assetNameSNo, dic_assetCodes","18c1040d":"def replace_assetName(in_mkt_df, in_news_df):\n    in_news_df[\"num_rows\"] = 1\n    assetName_df = in_news_df[[\"assetCodesSNo\",\"assetNameSNo\",\"num_rows\"]].groupby(\\\n                                                [\"assetCodesSNo\",\"assetNameSNo\"], as_index=False).count()\n    assetCode_df = assetName_df[[\"assetCodesSNo\", \\\n                                 \"assetNameSNo\",\"num_rows\"]].groupby([\"assetCodesSNo\"], \\\n                                 as_index=False).agg({\"assetNameSNo\": [\"max\",\"min\"], \"num_rows\": \"sum\"})\n\n    assetCode_df.columns = [\"assetCodesSNo\", \"max_assetNameSNo\", \"min_assetNameSNo\", \"count_num_rows\"]\n\n    assetCode_df = assetCode_df[assetCode_df[\"max_assetNameSNo\"] != assetCode_df[\"min_assetNameSNo\"]]\n    if assetCode_df.shape[0] > 0:\n        print(assetCode_df.shape)\n        assetCode_df[\"max_assetName\"] = assetCode_df[\"max_assetNameSNo\"].map(dic_assetNameSNo)\n        assetCode_df[\"min_assetName\"] = assetCode_df[\"min_assetNameSNo\"].map(dic_assetNameSNo)\n        list_mkt_asset = list(in_mkt_df[\"assetName\"].unique())\n        dic_mkt_assetName= {k:v for v,k in enumerate(list_mkt_asset)}\n        assetCode_df[\"min_mkt_assetNameSNo\"] = assetCode_df[\"min_assetName\"].map(dic_mkt_assetName)\n        assetCode_df[\"max_mkt_assetNameSNo\"] = assetCode_df[\"max_assetName\"].map(dic_mkt_assetName)\n\n\n        assetCode_df[\"final_assetNameSNo\"] = assetCode_df.apply(get_final_assetName, axis=1)\n        assetCode_df[\"replace_assetNameSNo\"] = assetCode_df.apply(get_replace_assetName, axis=1)\n\n        assetCode_df[\"final_assetName\"] = assetCode_df[\"final_assetNameSNo\"].map(dic_assetNameSNo)\n\n        df_temp = pd.merge(in_news_df.reset_index()[[\"index\",\"assetCodesSNo\",\"assetNameSNo\"]], \\\n                           assetCode_df[[\"replace_assetNameSNo\", \"final_assetNameSNo\",\"assetCodesSNo\",\"final_assetName\"]], \\\n                           how=\"inner\", left_on=[\"assetCodesSNo\",\"assetNameSNo\"], right_on=[\"assetCodesSNo\",\"replace_assetNameSNo\"], left_index=True).set_index(\"index\")\n        in_news_df[\"new_assetNameSNo\"] = -1\n        in_news_df[\"new_assetName\"] = \"\"\n\n        in_news_df.loc[df_temp.index, \"assetNameSNo\"] = list(df_temp[\"final_assetNameSNo\"])\n        in_news_df.loc[df_temp.index, \"assetName\"] = list(df_temp[\"final_assetName\"])","99cb55ff":"dic_rng = {}\ndic_min = {}\ndef scale_col(in_news_df, col):\n    global dic_rng\n    global dic_min\n    if col not in dic_rng.keys(): \n        col_max = in_news_df[col].max()\n        col_min = in_news_df[col].min()\n        col_rng = col_max - col_min\n        dic_rng[col] = col_rng\n        dic_min[col] = col_min\n    else:\n        col_min = dic_min[col] \n        col_rng = dic_rng[col]\n\n    in_news_df[col] = (in_news_df[col]-col_min)\/col_rng\n    if col_min < 0:\n        in_news_df[col] = 2*in_news_df[col] - 1\n    print(col, in_news_df[col].min(), in_news_df[col].max())\n    \n        \ndef drop_extra_news_cols(in_news_df):\n    news_drop_cols = [\"sourceTimestamp\",\"firstCreated\",\"sourceId\",\"headline\",\"urgency\",\"takeSequence\",\"provider\",\"audiences\",\"bodySize\",\\\n                      \"headlineTag\",\"sentenceCount\",\"wordCount\",\"firstMentionSentence\",\"sentimentClass\",\"sentimentWordCount\"]\n    news_drop_cols = [col  for col in news_drop_cols if col in in_news_df.columns ]\n    in_news_df.drop(news_drop_cols, axis=1, inplace=True)\n    \n\ndef drop_irrelevant_news(in_news_df):\n    irrelevant_news = in_news_df[in_news_df[\"relevance\"] < 0.3].index\n    print(\"Num low relevance news:\", len(irrelevant_news))\n    in_news_df.drop(irrelevant_news, axis=0, inplace=True)\n    del irrelevant_news\n    gc.collect()\n \ndef drop_neutral_news(in_news_df):\n    in_news_df[\"netSentimentPositive\"] = (in_news_df[\"sentimentPositive\"] - in_news_df[\"sentimentNegative\"]) \n    neutral_news = in_news_df[in_news_df[\"netSentimentPositive\"].abs() < 0.05].index\n    in_news_df.drop(neutral_news, axis=0, inplace=True)\n    in_news_df[\"netSentimentPositive\"] = in_news_df[\"netSentimentPositive\"] * (1 - in_news_df[\"sentimentNeutral\"])\n    print(\"Num neutral news:\", len(neutral_news))\n    del neutral_news\n    gc.collect()\n\ndef gen_netnovelty_col(in_news_df):\n    # if there is a news in 5 to 7 days then count will be more than 1. This news should have less importance as it is an old news\n    arr_novelty = np.array(in_news_df[[\"noveltyCount12H\",\"noveltyCount24H\",\"noveltyCount3D\",\"noveltyCount5D\",\"noveltyCount7D\"]])\n    \n    in_news_df[\"inv_netNovelty\"] = list(1\/(np.argmax(arr_novelty, axis=1)+2))\n    col = \"inv_netNovelty\"\n    scale_col(in_news_df, col)\n\nsetSector = {\"TECH\",\"ENER\",\"BMAT\",\"INDS\",\"CYCS\",\"NCYC\",\"FINS\",\"HECA\",\"TECH\",\"TCOM\",\"UTIL\"}\ndef get_sector_name(in_news_df):\n    global setSector \n    sectorList = {\"ENER\":\"Energy\", \"BMAT\":\"Basic Materials\", \"INDS\":\"Industrials\", \"CYCS\":\"Cyclical Consumer Goods & Services\", \\\n     \"NCYC\":\"Non Cyclical Consumer Goods & Services\", \"FINS\": \"Financials\", \"HECA\":\"Healthcare\",\"TECH\":\"Technology\", \\\n     \"TCOM\":\"Telecommunication Services\", \"UTIL\":\"Utilities\"}\n\n    in_news_df[\"sector\"] = in_news_df[\"subjects\"].apply(lambda x: str(eval(x) & setSector))\n    for col in setSector:\n        print(col)\n        in_news_df[col] = in_news_df[\"sector\"].apply(lambda x: col in eval(x))\n    lst_sector = list(setSector)\n    dic_sector = {v: k for v,k in enumerate(list(setSector))}\n    lst_sector.append(\"assetNameSNo\")\n    asset_df = in_news_df[lst_sector].groupby([\"assetNameSNo\"]).sum()\n    asset_df1 = asset_df.sum(axis=1)\n    asset_unknown_sector = list(asset_df1[asset_df1==0].index)\n    print(\"Could not find sector of \", len(asset_unknown_sector) , \" assets\")\n    asset_df.drop(asset_unknown_sector, axis=0, inplace=True)\n    asset_df[\"sectorKey\"] = asset_df.values.argmax(axis=1)\n    asset_df[\"sectorName\"] = asset_df[\"sectorKey\"].map(dic_sector)\n    print(asset_df.head(1))\n    asset_df.reset_index(inplace=True)\n    merged_df = pd.merge(in_news_df.reset_index()[[\"assetNameSNo\",\"index\"]], asset_df[[\"assetNameSNo\",\"sectorName\"]], \\\n                         left_on= [\"assetNameSNo\"], right_on= [\"assetNameSNo\"], how=\"left\").set_index(\"index\")\n    in_news_df[\"sectorName\"] = \"\"\n    in_news_df.loc[merged_df.index,[\"sectorName\"]] = merged_df.loc[merged_df.index,[\"sectorName\"]]\n    \n    return asset_df[[\"assetNameSNo\",\"sectorName\"]]\n\ndef merge_mkt_news(in_mkt_df, in_news_df):\n    in_mkt_df[\"mkt_cap\"] = ((in_mkt_df[\"close\"] + in_mkt_df[\"open\"])\/2)  * in_mkt_df[\"volume\"] \n    in_mkt_df_grouped = in_mkt_df[[\"assetNameSNo\",\"time\", \"close\",\"returnsClosePrevRaw10\",\"returnsClosePrevMktres10\",\\\n                                   \"returnsOpenNextMktres10\",\"mkt_cap\"]].groupby([\"assetNameSNo\",\"time\"], as_index=False).sum()\n    merged_df = pd.merge(in_news_df.reset_index()[[\"index\",\"assetNameSNo\",\"time\"]], in_mkt_df_grouped, left_on= [\"assetNameSNo\",\"time\"], \\\n                         right_on= [\"assetNameSNo\",\"time\"], how=\"inner\").set_index(\"index\")\n    in_news_df[\"close\"] = 0\n    in_news_df[\"returnsClosePrevRaw10\"] = 0\n    in_news_df[\"returnsClosePrevMktres10\"] = 0\n    in_news_df[\"returnsOpenNextMktres10\"] = 0\n    in_news_df[\"mkt_cap\"] = 0\n    in_news_df.loc[merged_df.index, [\"close\",\"returnsClosePrevRaw10\",\"returnsClosePrevMktres10\",\"returnsOpenNextMktres10\",\"mkt_cap\"]] = \\\n    merged_df.loc[merged_df.index, [\"close\",\"returnsClosePrevRaw10\",\"returnsClosePrevMktres10\",\"returnsOpenNextMktres10\",\"mkt_cap\"]]\n    \n    in_news_df[\"weighted_SenSR\"] = in_news_df[\"SenSR\"] * in_news_df[\"mkt_cap\"]\n    \n    \ndef get_smooth_asset_sentiment(in_mkt_df, in_news_df):\n    col=\"SenSR\"\n    asset_col = \"asset_SenSR\"\n    rolling_col = \"rolling_\" + asset_col\n    smooth_col = \"smooth_\" + asset_col\n    in_news_df[asset_col] = in_news_df[col]\n    in_news_df[rolling_col] = 0\n    in_news_df[smooth_col] = 0\n    \n    lst_data = []\n    for key, val_df in in_news_df.groupby([\"assetNameSNo\"]):\n        print(\"Generating rolled and smooth col for asset:\" + str(key) + \":\" + dic_assetNameSNo[key])\n        val_df.set_index([\"time\"], inplace=True)\n        val_df[rolling_col] = val_df[asset_col].rolling(window=7, min_periods=1).mean()\n        val_df.dropna(axis=0, inplace=True)\n        y = list(val_df[rolling_col].values)\n        x = range(val_df.shape[0])        \n        smooth = lowess(y,x, 0.02)\n        val_df[smooth_col] = list(smooth[:,1])\n        del smooth, x, y\n        lst_data.append(val_df[[\"assetNameSNo\",asset_col,rolling_col,smooth_col]].reset_index())\n\n    asset_df = pd.concat(lst_data, axis=0)\n    merged_df = pd.merge(in_news_df.reset_index()[[\"index\",\"assetNameSNo\",\"time\"]], asset_df, how=\"left\", \\\n                         left_on=[\"assetNameSNo\", \"time\"], right_on=[\"assetNameSNo\",\"time\"]).set_index(\"index\")\n    in_news_df.loc[merged_df.index, [asset_col,rolling_col,smooth_col]] = merged_df.loc[merged_df.index, [asset_col,rolling_col,smooth_col]]\n    return merged_df\n\ndef get_smooth_sector_sentiment(in_mkt_df, in_news_df): \n    col = \"weighted_SenSR\"\n    sector_col = \"sector_SenSR\"\n    rolling_col = \"rolling_\" + sector_col\n    smooth_col = \"smooth_\" + sector_col\n    in_news_df[sector_col] = 0\n    in_news_df[rolling_col] = 0\n    in_news_df[smooth_col] = 0\n    lst_data = []\n    for key, val_df in in_news_df[[col,\"time\",\"sectorName\",\"mkt_cap\"]].groupby([\"sectorName\"]):\n        print(\"Generating rolled and smooth col for sector:\" + str(key) )\n        val_df = val_df.groupby([\"time\",\"sectorName\"], as_index=False).sum()\n        val_df[sector_col] = val_df[col]\/val_df[\"mkt_cap\"]\n        val_df.set_index([\"time\"], inplace=True)\n        val_df[rolling_col] = val_df[sector_col].rolling(window=7, min_periods=1).mean()\n        val_df.dropna(axis=0, inplace=True)\n        y = list(val_df[rolling_col].values)\n        x = range(val_df.shape[0])        \n        smooth = lowess(y,x, 0.02)\n        val_df[smooth_col] = list(smooth[:,1])\n        del smooth, x, y\n        lst_data.append(val_df[[sector_col,rolling_col,smooth_col, \"sectorName\"]].reset_index())\n    \n    sector_df = pd.concat(lst_data, axis=0)\n    merged_df = pd.merge(in_news_df.reset_index()[[\"index\",\"sectorName\",\"time\"]], sector_df, how=\"left\", \\\n                         left_on=[\"sectorName\", \"time\"], right_on=[\"sectorName\",\"time\"]).set_index(\"index\")\n   \n    in_news_df.loc[merged_df.index, [sector_col,rolling_col,smooth_col]] = merged_df.loc[merged_df.index, [sector_col,rolling_col,smooth_col]]\n    return merged_df\n\ndef get_smooth_overall_sentiment(in_mkt_df, in_news_df): \n    col = \"weighted_SenSR\"\n    all_col = \"all_SenSR\"\n    rolling_col = \"rolling_\" + all_col\n    smooth_col = \"smooth_\" + all_col\n    in_news_df[all_col] = 0\n    in_news_df[rolling_col] = 0\n    in_news_df[smooth_col] = 0\n    lst_data = []\n    val_df = in_news_df[[col,\"time\",\"mkt_cap\"]].groupby([\"time\"]).sum()\n    val_df[all_col] = val_df[col]\/val_df[\"mkt_cap\"]\n    \n    val_df[rolling_col] = val_df[all_col].rolling(window=7, min_periods=1).mean()\n    val_df.dropna(axis=0, inplace=True)\n    y = list(val_df[rolling_col].values)\n    x = range(val_df.shape[0])        \n    smooth = lowess(y,x, 0.02)\n    \n    val_df[smooth_col] = list(smooth[:,1])\n    del smooth, x, y\n    lst_data.append(val_df[[all_col,rolling_col,smooth_col]].reset_index())\n    \n    sector_df = pd.concat(lst_data, axis=0)\n    merged_df = pd.merge(in_news_df.reset_index()[[\"index\",\"time\"]], sector_df, how=\"left\", \\\n                         left_on=[\"time\"], right_on=[\"time\"]).set_index(\"index\")\n    \n    in_news_df.loc[merged_df.index, [all_col,rolling_col,smooth_col]] = merged_df.loc[merged_df.index, [all_col, rolling_col,smooth_col]]\n    return merged_df\n\ndef gen_news_features(in_mkt_df, in_news_df):\n    \n    global setSector\n    drop_extra_news_cols(in_news_df)\n    drop_irrelevant_news(in_news_df)\n    drop_neutral_news(in_news_df)\n\n    gen_netnovelty_col(in_news_df)\n    sector_name_df  = get_sector_name(in_news_df)\n    in_news_df.drop(\"subjects\", axis=1, inplace=True)\n    in_news_df.drop([\"noveltyCount12H\",\"noveltyCount24H\",\"noveltyCount3D\",\"noveltyCount5D\",\"noveltyCount7D\"], axis=1, inplace=True)\n    in_news_df.drop([\"volumeCounts12H\",\"volumeCounts24H\",\"volumeCounts3D\",\"volumeCounts5D\",\"volumeCounts7D\"], axis=1, inplace=True)\n\n    in_news_df[\"SenSR\"] = in_news_df[\"netSentimentPositive\"] * in_news_df[\"relevance\"] * (in_news_df[\"inv_netNovelty\"])\n    in_news_df = in_news_df.groupby([\"assetName\",\"sectorName\",\"time\",\"assetNameSNo\"], as_index=False).sum()\n    merge_mkt_news(in_mkt_df, in_news_df)\n    #in_news_df = in_news_df[((in_news_df[\"returnsClosePrevMktres10\"] < 0) & (in_news_df[\"SenSR\"] > 0)) | \n    #                ((in_news_df[\"returnsClosePrevMktres10\"] < 0) & (in_news_df[\"SenSR\"] > 0))]\n    asset_df = get_smooth_asset_sentiment(in_mkt_df,in_news_df)\n    sector_df = get_smooth_sector_sentiment(in_mkt_df, in_news_df)\n    all_df = get_smooth_overall_sentiment(in_mkt_df, in_news_df)\n    return sector_name_df, asset_df, sector_df, all_df, in_news_df\n    ","c7d0b24a":"news_train_df.head(1)","595913cb":"#news_train_df.drop([\"mkt_cap\",\"weighted_SenSR\",\"sector_SenSR\",\"rolling_sector_SenSR\",\"smooth_sector_SenSR\"], axis=1, inplace=True)\n#get_smooth_sector_sentiment(market_train_df, news_train_df)","a53d1a16":"market_train_df.head(1)\n","7c2133f4":"fix_time_col(market_train_df, news_train_df)\nclean_assetName_col(market_train_df, news_train_df)\ndic_assetName, dic_assetNameSNo, dic_assetCodes = set_assetNameCodes_SNo(market_train_df, news_train_df,dic_assetName, dic_assetNameSNo, dic_assetCodes)\nreplace_assetName(market_train_df, news_train_df)\nsector_name_df, asset_df, sector_df, all_df, news_feature_df = gen_news_features(market_train_df, news_train_df)\n","212c58f1":"drop_cols = list(set([\"companyCount\",\"marketCommentary\",\"assetCodesSNo\",\"num_rows\"]) | setSector)\nnews_feature_df.drop(drop_cols, axis=1, inplace=True)\nnews_feature_df.tail(1)\n","a8b9980f":"drop_cols = [\"sentimentNegative\",\"sentimentPositive\",\"sentimentNeutral\"]\nnews_feature_df.drop(drop_cols, axis=1, inplace=True)","d68a6621":"news_feature_df[\"newsCount\"]=0\ndf_sector = news_feature_df[[\"sectorName\",\"newsCount\"]].groupby([\"sectorName\"]).count()\ndf_sector = df_sector.unstack()\ndf_sector.plot(kind=\"bar\", title=\"Sector Wise News Count\", figsize=(10,4));","d024b3ff":"from pandas.plotting import scatter_matrix\napple_df = news_feature_df[news_feature_df[\"assetName\"]==\"apple\"]\napple_df[\"target\"] = apple_df[\"returnsOpenNextMktres10\"].apply(lambda x: 1 if x>0 else 2 if x<0 else 3)\napple_df[\"asset_sector_SenSR\"] = apple_df[\"smooth_asset_SenSR\"] * apple_df[\"smooth_sector_SenSR\"]\napple_df[\"asset_sector_all_SenSR\"] = apple_df[\"smooth_asset_SenSR\"] * apple_df[\"smooth_sector_SenSR\"] * apple_df[\"smooth_all_SenSR\"]\ncol_list1 = [\"returnsClosePrevMktres10\",\"returnsOpenNextMktres10\",\\\n            \"asset_SenSR\",\"rolling_asset_SenSR\",\"smooth_asset_SenSR\"]\ncol_list2 = [\"returnsClosePrevMktres10\",\"returnsOpenNextMktres10\",\\\n            \"sector_SenSR\",\"rolling_sector_SenSR\",\"smooth_sector_SenSR\"]\ncol_list3 = [\"returnsClosePrevMktres10\",\"returnsOpenNextMktres10\",\\\n            \"asset_sector_SenSR\",\"asset_sector_all_SenSR\"]\nfor col in list(set(col_list1)|set(col_list2)|set(col_list3)):\n    scale_col(apple_df, col)\nscale_col(apple_df, \"smooth_all_SenSR\")\n#apple_df = apple_df[((apple_df[\"returnsClosePrevMktres10\"] < 0) & (apple_df[\"smooth_asset_SenSR\"] > 0)) | \n#                    ((apple_df[\"returnsClosePrevMktres10\"] < 0) & (apple_df[\"smooth_asset_SenSR\"] > 0))]","d66f9d64":"\n\nax = scatter_matrix(apple_df[col_list1],figsize=(15,15))\n","b021de1a":"scatter_matrix(apple_df[col_list2], figsize=(20,20));","d82010c7":"scatter_matrix(apple_df[col_list3], figsize=(20,20));","bda9614c":"#fig, axs = plt.subplots(1,1, figsize=(20,10))\n#apple_df.set_index([\"time\"], inplace=True)\nax1 = apple_df[[\"smooth_sector_SenSR\",\"smooth_asset_SenSR\",\"smooth_all_SenSR\",\"returnsOpenNextMktres10\"]].plot(figsize=(40,10))\nax1.legend()","c2c8fbe0":"for key, val_df in sector_df.groupby([\"sectorName\"]):\n    val_df.set_index([\"time\"], inplace=True)\n    ax1 = val_df[\"smooth_sector_SenSR\"].plot(figsize=(40,10), label=key)\n    ax1.legend()","d4dab738":"# ENERGY Sector\nWe can see that energy sector had a major dip in 2015 as well as around 2016\n\n# FINANCE Sector\nSimilarly we can see the finance sector meltdown in 2009. \n\nHow well these features will impact the model is to be seen.","5ed27f45":"Looks like Asset Sector SenSR has some corelation with next 10 day returns.","950da64b":"Smooth Sector Sentiment has corelation with next 10 day returns.","98cfd296":"# Attempt to generate news features\n1. In V22 mkt cap taken as mean market cap for the asset\n2. In V23 mkt cap taken as mean for asset and time\n3. In V24 \n    a. sen SR mean taken instead of sum as asset and time level and also at the time of rolling in asset function. \n    b. Also for asset sensr also we are multiplying by market cap\n    c. removing news for days that is in the same direction as prevailing sentiment\n 4. In V25 rollback of change in V24\n","b332ae47":"Asset Sentiment has corelation with next 10 days returns in the sentiment and previous returns are in opposite direction."}}