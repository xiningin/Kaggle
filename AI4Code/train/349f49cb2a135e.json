{"cell_type":{"4aad0633":"code","9eb134a2":"code","8fc7f448":"code","57b80526":"code","8d9ceced":"code","ef1ec8dd":"code","750b845b":"code","e64fa94a":"code","b6bfb783":"code","4645c04f":"code","1ade6496":"code","78083c11":"code","4f2f7823":"markdown","1240efcc":"markdown","e93f6c8c":"markdown","aa8f5462":"markdown","c44b66b7":"markdown","39c07d07":"markdown"},"source":{"4aad0633":"!pip install torchviz\n%matplotlib inline\nimport os\nimport cv2\nimport seaborn \nimport numpy as np\nimport pandas as pd \nimport random\nimport tqdm\nfrom tqdm import notebook\nimport albumentations as A\nfrom albumentations import pytorch\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as functional\nimport torchvision\nfrom torchviz import make_dot","9eb134a2":"images_dir = '\/kaggle\/input\/cityscapes-image-pairs\/cityscapes_data'\ntrain_images_dir = os.path.join(images_dir, 'train')\nval_images_dir = os.path.join(images_dir, 'val')\nworking_dir = '\/kaggle\/working'\nweights_path = os.path.join(working_dir, 'unet4_weights.pth')\nlogs_path = os.path.join(working_dir, 'logs')","8fc7f448":"class Generator(object):\n    def __init__(self, images_dir, batch_size, is_augmentation, shuffle = True, rescale = 1.00, target_size = (128, 128)):\n        super(Generator, self).__init__()\n        self.images_dir = images_dir\n        self.batch_size = batch_size\n        self.rescale = rescale\n        self.shuffle = shuffle\n        self.is_augmentation = is_augmentation\n        self.target_size = target_size\n        self.filenames = [os.path.join(self.images_dir, filename) for filename in os.listdir(self.images_dir)]\n        self.current_step = 0\n        self.count_images = len(self.filenames)\n        self.available_steps = int(self.count_images \/\/ self.batch_size)\n        \n        self.transforms = A.Compose([\n            A.Rotate(25), \n            A.OneOf([\n                A.RGBShift(), A.HueSaturationValue()\n            ]),\n            A.OneOf([\n                A.CLAHE(), A.RandomBrightnessContrast(), A.RandomGamma()\n            ]), \n        ])\n    \n    def augmentate(self, batch):\n        batch = batch.astype(np.uint8)\n        batch = [self.transforms(image = image, mask = mask) for (image, mask) in batch]\n        batch = np.array([(transformed['image'], transformed['mask']) for transformed in batch], dtype = np.float32)\n        return batch\n            \n    def generate_batch(self):\n        start = self.current_step * self.batch_size\n        stop = (self.current_step + 1) * self.batch_size\n        filenames_batch = self.filenames[start:stop]\n        \n        # batch of original images from directory\n        images_batch = [cv2.imread(filename) for filename in filenames_batch]\n        \n        # change channeld order to rgb\n        images_batch = np.array([cv2.cvtColor(image, cv2.COLOR_BGR2RGB) for image in images_batch])\n        \n        # split original images by image and mask\n        images_batch = np.array([(image[:, :256,], image[:, 256:]) for image in images_batch]) \n        \n        # resize images \n        images_batch = np.array([(cv2.resize(image, self.target_size), cv2.resize(mask, self.target_size)) for (image, mask) in images_batch], dtype = np.float32)\n        \n        # augmentation\n        if self.is_augmentation:\n            images_batch = self.augmentate(images_batch)\n        \n         # set 'channel_first' order\n        images_batch = np.array([(np.moveaxis(image, -1, 0), np.moveaxis(mask, -1, 0)) for (image, mask) in images_batch])\n        \n        # rescaling \n        images_batch \/= self.rescale\n        #resampling \n        images_batch = np.moveaxis(images_batch, 1, 0)\n        \n        return torch.Tensor(images_batch)\n    \n    def __next__(self):\n        if self.current_step > self.available_steps:\n            self.current_step = 0\n        images, masks = self.generate_batch()\n        self.current_step += 1\n        return masks, images\n    \n    def __len__(self):\n        return self.available_steps","57b80526":"def show_examples(num_cols):\n    stacks = []\n    dataloader = Generator(images_dir = train_images_dir, batch_size = 8, is_augmentation = True, rescale = 255.0)\n    for iteration in range(num_cols):\n        images, masks = next(dataloader)\n        images, masks = images.numpy(), masks.numpy()\n        images, masks = np.concatenate(np.moveaxis(images, 1, -1)), np.concatenate(np.moveaxis(masks, 1, -1))\n        embedded = (images + masks) \/ 2\n        stack = np.hstack([images, masks, embedded])\n        stacks.append(stack)\n    result = np.hstack(stacks)\n    plt.figure(figsize = (30, 30))\n    plt.axis('off')\n    plt.imshow(result)\nshow_examples(2)","8d9ceced":"class DoubleConv(nn.Module):\n    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n\n    def __init__(self, in_channels, out_channels, mid_channels=None):\n        super().__init__()\n        if not mid_channels:\n            mid_channels = out_channels\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(mid_channels),\n            nn.LeakyReLU(0.1),\n            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.LeakyReLU(0.1)\n        )\n\n    def forward(self, x):\n        return self.double_conv(x)\n\n\nclass Down(nn.Module):\n    \"\"\"Downscaling with maxpool then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_channels, out_channels)\n        )\n\n    def forward(self, x):\n        return self.maxpool_conv(x)\n\n\nclass Up(nn.Module):\n    \"\"\"Upscaling then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels, bilinear=True):\n        super().__init__()\n\n        # if bilinear, use the normal convolutions to reduce the number of channels\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n            self.conv = DoubleConv(in_channels, out_channels, in_channels \/\/ 2)\n        else:\n            self.up = nn.ConvTranspose2d(in_channels, in_channels \/\/ 2, kernel_size=2, stride=2)\n            self.conv = DoubleConv(in_channels, out_channels)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        # input is CHW\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n\n        x1 = functional.pad(x1, [diffX \/\/ 2, diffX - diffX \/\/ 2,\n                        diffY \/\/ 2, diffY - diffY \/\/ 2])\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n\n\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(OutConv, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        return self.conv(x)\n","ef1ec8dd":"class UNet(nn.Module):\n    def __init__(self, n_channels, n_classes, bilinear=True):\n        super(UNet, self).__init__()\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.bilinear = bilinear\n\n        self.inc = DoubleConv(n_channels, 64)\n        self.down1 = Down(64, 128)\n        self.down2 = Down(128, 256)\n        self.down3 = Down(256, 512)\n        factor = 2 if bilinear else 1\n        self.down4 = Down(512, 1024 \/\/ factor)\n        self.up1 = Up(1024, 512 \/\/ factor, bilinear)\n        self.up2 = Up(512, 256 \/\/ factor, bilinear)\n        self.up3 = Up(256, 128 \/\/ factor, bilinear)\n        self.up4 = Up(128, 64, bilinear)\n        self.outc = OutConv(64, n_classes)\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        logits = self.outc(x)\n        return logits            ","750b845b":"unet = UNet(n_channels=3, n_classes=3, bilinear=True)\nx = torch.zeros(8, 3, 128, 128, dtype=torch.float, requires_grad=False)\nout = unet(x)\nmake_dot(out, params=dict(list(unet.named_parameters())))","e64fa94a":"!nvidia-smi","b6bfb783":"optimizer = torch.optim.Adam(params = unet.parameters(), \n                             lr=1e-4, \n                             betas=(0.9, 0.999), \n                             eps=1e-08, \n                             weight_decay=0, \n                             amsgrad=False)\n\ncriterion = torch.nn.BCEWithLogitsLoss()\n\nhistory = dict(train_loss = [], \n               train_dice_coeff = [], \n               test_loss = [], \n               test_dice_coeff = [])\n\ndef dice_coeff(pred, target):\n    pred = (pred > 0).float()\n    return 2. * (pred*target).sum() \/ (pred+target).sum()\n            \ndef training(model, epochs, batch_size):\n \n    train_generator = Generator(images_dir = train_images_dir, batch_size = batch_size, is_augmentation = True, rescale = 255.0)\n    test_generator = Generator(images_dir = val_images_dir, batch_size = batch_size, is_augmentation = False, rescale = 255.0)\n    \n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    \n    main_pbar = tqdm.notebook.tqdm(range(epochs))\n    main_pbar.set_description('common progress ')\n    \n    for epoch in main_pbar:\n        running_params = dict(train_loss = [], \n                               train_dice_coeff = [], \n                               test_loss = [], \n                               test_dice_coeff = [])\n        train_pbar = tqdm.notebook.tqdm(range(len(train_generator)))\n        \n        for step in train_pbar:\n            \n            # obtain batches of data \n            train_images, train_masks = next(train_generator)\n            train_images, train_masks = train_images.to(device), train_masks.to(device)\n            \n            # zero optimizer gradients\n            optimizer.zero_grad()\n            \n            # forward + backward + optimize\n            train_predictions = model(train_images)\n            # estimate loss and backward\n            train_loss = criterion(train_predictions, train_masks)\n            train_loss.backward()\n            \n            # estimate dice coeff\n            train_dice_coeff = dice_coeff(pred = train_predictions, target = train_masks)\n            \n            # optimize\n            optimizer.step()\n        \n        \n       \n            with torch.no_grad():\n                # get batches of data\n                test_images, test_masks = next(test_generator)\n                test_images, test_masks = test_images.to(device), test_masks.to(device)\n                # calculate outputs by running images through the network\n                test_predictions = model(test_images)\n                # calculate loss without backpropagation\n                test_loss = criterion(test_predictions, test_masks)\n                # calculate dice coeff\n                test_dice_coeff = dice_coeff(pred = test_predictions, target = test_masks)\n                \n            \n            # set current values in dictionary to store\n            current_metrics = dict(train_loss = [train_loss.item(), ], \n                                   train_dice_coeff = [train_dice_coeff.item(), ], \n                                   test_loss = [test_loss.item(),], \n                                   test_dice_coeff = [test_dice_coeff.item(),])\n            \n            # update dictionary with current epochs\n            running_params.update(current_metrics)\n            # estimate mean values for each metrics\n            mean_metrics = dict(zip(running_params.keys(), [(sum(tensor) \/ (step + 1)) for tensor in running_params.values()]))\n            # update progress bar\n            train_pbar.set_postfix(mean_metrics)\n            # clear gpu memory\n            torch.cuda.empty_cache()\n        \n        history.update(running_params)\n        best_loss = max(history['test_loss'])\n        best_loss_index = history['test_loss'].index(best_loss)\n        current_loss_index = history['test_loss'].index(test_loss.item())\n        if abs(current_loss_index - best_loss_index) >= 5:\n            for param_group in optim.param_groups:\n                if param_group['lr'] * 0.1 > 1e-6:\n                    print('reduce learning rate to', {param_group['lr'] * 0.1})\n                    param_group['lr'] *= 0.1\n\ntraining(model = unet, epochs = 20, batch_size = 32)\ntorch.save(unet.state_dict(), weights_path)","4645c04f":"model = UNet(n_channels=3, n_classes=3, bilinear=True)\nmodel.load_state_dict(torch.load(weights_path))\nmodel.eval()","1ade6496":"def show_final_results(num_cols):\n    generator = Generator(images_dir = val_images_dir, \n                           batch_size = 16, \n                           is_augmentation = True, \n                           rescale = 255.0)\n    result = []\n    for iteration in range(num_cols):\n        masks, images = next(generator)\n        prediction = torch.sigmoid(model(masks))\n        prediction = prediction.cpu().detach().numpy()\n        prediction = np.moveaxis(prediction, 1, -1)\n        prediction = np.concatenate(prediction)\n        \n        masks = np.moveaxis(masks.numpy(), 1, -1)\n        masks = np.concatenate(masks)\n        \n        images = np.moveaxis(images.numpy(), 1, -1)\n        images = np.concatenate(images)\n        \n        outputs = np.hstack([masks, images, prediction])\n        \n        result.append(outputs)\n    result = np.hstack(result)\n    \n    plt.figure(figsize = (30, 60))\n    plt.axis('off')\n    plt.imshow(result)\n\nshow_final_results(num_cols = 1)","78083c11":"model_parameters = filter(lambda p: p.requires_grad, model.parameters())\nparams = sum([np.prod(p.size()) for p in model_parameters])\nprint(f'model parameters: {params}')","4f2f7823":"# Model","1240efcc":"# Conclusion\n# Mission Failed Successfully\n\nI didn\u2019t get the desired result, but nevertheless we see that the simplest unet4 is able to restore an image from a segmentation mask, therefore I will try other architectures and hope that the results are halfway good","e93f6c8c":"# Import dependencies","aa8f5462":"# Define default paths","c44b66b7":"# Custom Generator Compatible with Torch","39c07d07":"![image.png](attachment:b543ce97-8528-4f63-ab50-94f78863fed2.png)"}}