{"cell_type":{"84a410bc":"code","9240acf2":"code","636a451e":"code","185e942f":"code","4109ecd1":"code","b888886a":"code","23a940d7":"code","1403ae13":"code","c016b4da":"code","875c1ba0":"code","09434c6e":"code","65575526":"code","2d2e2589":"code","75d5979a":"code","c95a42d2":"code","960cb566":"code","cab5cefb":"code","6c2fb68c":"code","6e2653aa":"code","ea0e0722":"code","ad8937b2":"markdown","58ad8d07":"markdown","844e1d34":"markdown","e930a675":"markdown","c6d7157c":"markdown","02940e92":"markdown","8cb24e07":"markdown","87fdbd4e":"markdown","749911a9":"markdown","6f109114":"markdown","63c7ba81":"markdown","abf92d0f":"markdown","0fcc8f2f":"markdown","36d76d43":"markdown","53d3ec5b":"markdown","bf74ee8d":"markdown","2386f50e":"markdown","2d86a70e":"markdown","d3831807":"markdown","a2e449aa":"markdown","8cfb147a":"markdown","d420ae56":"markdown","cd3e43b5":"markdown","5ced3a1c":"markdown","d676af82":"markdown","9fe52b12":"markdown","ef2f796e":"markdown","4e664540":"markdown","88c3698d":"markdown","3e75b308":"markdown","a34f921c":"markdown","146e2184":"markdown","756fd1ea":"markdown","7baf289b":"markdown","7c905bea":"markdown","a6825eda":"markdown","8fa754cb":"markdown","b629faf9":"markdown","c7f889cc":"markdown","675df80e":"markdown","79d2eeba":"markdown","5130bf62":"markdown","3441292d":"markdown","b78dba26":"markdown","fdf20093":"markdown","6cc6755f":"markdown","df9d5c78":"markdown","ed008903":"markdown","b19be71f":"markdown","699e9e9e":"markdown","6fd41e98":"markdown","7d72f46f":"markdown","c80e9692":"markdown","6f35a88c":"markdown","4264f1b6":"markdown","3d146cd6":"markdown","15541fa2":"markdown","7bf4414e":"markdown","6d493af1":"markdown","222ac2e2":"markdown","5b50e6ca":"markdown","0b640cee":"markdown"},"source":{"84a410bc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9240acf2":"df=pd.read_csv(\"..\/input\/glass\/glass.csv\")","636a451e":"#Melihat data yang hilang\ndef missing_percentage(df):\n    \"\"\"This function takes a DataFrame(df) as input and returns two columns, total missing values and total missing values percentage\"\"\"\n    total = df.isnull().sum().sort_values(ascending = False)\n    percent = round(df.isnull().sum().sort_values(ascending = False)\/len(df)*100,2)\n    return pd.concat([total, percent], axis=1, keys=['Total','Percent'])\n\nmissing_percentage(df)","185e942f":"df.sample(10)","4109ecd1":"df.shape #Size of Dataset","b888886a":"df.describe()","23a940d7":"for column in df:\n    plt.figure()\n    df.hist([column])","1403ae13":"for column in df:\n    plt.figure()\n    df.boxplot([column])","c016b4da":"# count plot on single categorical variable\nsns.countplot(x ='Type', data = df).set(title='Count of Glass Type in Dataset')\n \n# Show the plot\nplt.show()","875c1ba0":"mask = np.zeros_like(df.corr(), dtype=np.bool)\n## in order to reverse the bar replace \"RdBu\" with \"RdBu_r\"\nplt.subplots(figsize = (10,10))\nsns.heatmap(df.corr(method='kendall'), annot=True,mask = False,cmap = 'OrRd', linewidths=.7, linecolor='black',fmt='.2g',center = 0,square=True)\n\nplt.title(\"Correlations\", y = 1.03,fontsize = 20, fontweight = 'bold', pad = 40);","09434c6e":"# Impor library-library yang dibutuhkan pada cell ini\nfrom urllib import request\nimport csv\nfrom codecs import iterdecode\n\n# Dapatkan file dataset dalam bentuk CSV\ndataset_url = \"https:\/\/gist.github.com\/comfyte\/09c86bb880aa9b151b0bdafa79a89282\/raw\/af12c3fb4a76605e9c79c626910ce63f53b6c16b\/glass-dataset.csv\"\ncsv_object = request.urlopen(dataset_url)\n\n# Lompati baris paling pertama yang hanya berisi header\nnext(csv_object)\n\n# Sesuaikan agar iterable mengeluarkan item-item yang telah didekode\ncsv_rows = iterdecode(csv_object, \"utf-8\")\n\n# Baca objek CSV yang berisi dataset yang akan digunakan\nraw_content = csv.reader(csv_rows, delimiter=',', quoting=csv.QUOTE_NONNUMERIC)\n\n# Ubah semua data menjadi bentuk NumPy Array (n-dimensional array atau ndarray)\nall_data = np.array(list(raw_content))\n\n# Acak urutan seluruh data agar lebih merata dalam training\ndf = all_data\nall_data = np.take(df, np.random.permutation(df.shape[0]), axis=0, out=df)\n\n# Pisahkan keseluruhan data menjadi data input dan data output (bentuk numerik)\ninput_data = all_data[:, :-1]\noutput_data_num = all_data[:, -1].astype(int)","65575526":"def output_num_to_feat(value: int, value_range: range):\n    return [(1 if n == value else 0) for n in value_range]\n\ndef output_feat_to_num(array: np.ndarray):\n    return np.nonzero(array == 1)[1] + 1","2d2e2589":"MIN_NUM = min(output_data_num)\nMAX_NUM = max(output_data_num)\n\noutput_data_feat = np.array([output_num_to_feat(n - MIN_NUM, range(MAX_NUM))\n                             for n in output_data_num])","75d5979a":"def scale_inputs(x, mean_values=None, std_values=None):    \n    feature_length = x.shape[1]\n    \n    compute_own = (mean_values is None) or (std_values is None)\n\n    if compute_own:\n        mean_values = []\n        std_values = []\n    elif len(mean_values) != len(std_values) != feature_length:\n        raise ValueError()\n    \n    for i in range(feature_length):\n        if compute_own:\n            mean_values.append(np.mean(x[:, i]))\n            std_values.append(np.std(x[:, i]))\n        \n        x[:, i] = (x[:, i] - mean_values[i]) \/ std_values[i]\n\n    if compute_own:\n        return mean_values, std_values","c95a42d2":"def sigmoid(x):\n    return 1\/(1+np.exp(-x))\n\ndef sigmoid_der(x):\n    return sigmoid(x) *(1-sigmoid (x))\n\ndef softmax(A):\n    expA = np.exp(A)\n    return expA \/ expA.sum(axis=1, keepdims=True)","960cb566":"# input data & output data dikeluarain\/dipindah jadi ke yang def train(?)\nimport numpy as np\n\nclass Neural:\n    def __init__(self, attributes, output_labels, hidden_nodes_count):\n        \"\"\"\n        Inisialisasikan sebuah instance ANN (model) baru\n        \"\"\"\n        # Berikan seed pada randomizer di NumPy\n        np.random.seed(42)\n\n        # Mulai menginisialisasi parameter-parameter yang dibutuhkan\n        parameters = {}\n\n        parameters[\"wh\"] = np.random.rand(attributes, hidden_nodes_count)\n        parameters[\"bh\"] = np.random.randn(hidden_nodes_count)\n\n        parameters[\"wo\"] = np.random.rand(hidden_nodes_count, output_labels)\n        parameters[\"bo\"] = np.random.randn(output_labels)\n\n        self.parameters = parameters\n\n    def train(self, feature_set, output_data, iter_count, learning_rate):\n        \"\"\"\n        Latih model yang telah dibuat\n        \"\"\"\n        for _ in range(0, iter_count):\n            self.__forward_propagation(feature_set)\n            self.__backward_propagation(feature_set, output_data)\n            self.__update_weights(learning_rate)\n\n    def predict(self, inp, in_num_form = False):\n        \"\"\"\n        Method untuk memprediksi data baru\n        \"\"\"\n        ao = self.__forward_propagation(inp)\n        predictions = np.array([[(1 if n == max(row) else 0) for n in row]\n                                for row in ao])\n        \n        if in_num_form:\n            return output_feat_to_num(predictions)\n\n        return predictions\n\n    # Fungsi-fungsi pembantu (helper functions)\n    def __forward_propagation(self, feature_set):\n        wh = self.parameters['wh']\n        bh = self.parameters['bh']\n        wo = self.parameters['wo']\n        bo = self.parameters['bo']\n\n        zh = np.dot(feature_set, wh) + bh\n        ah = sigmoid(zh)\n\n        # Phase 2\n        zo = np.dot(ah, wo) + bo\n        ao = softmax(zo)\n        self.cache = {\"zh\": zh,\n                      \"ah\": ah,\n                      \"zo\": zo,\n                      \"ao\": ao}\n        return ao\n    \n    def __backward_propagation(self, feature_set, outp):\n        ao = self.cache[\"ao\"]\n        ah = self.cache[\"ah\"]\n        zh = self.cache[\"zh\"]\n    \n        wh = self.parameters['wh']\n        bh = self.parameters['bh']\n        wo = self.parameters['wo']\n        bo = self.parameters['bo']\n    \n        # Phase 1\n        dcost_dzo = ao - outp\n        dzo_dwo = ah\n        dcost_wo = np.dot(dzo_dwo.T, dcost_dzo)\n        dcost_bo = dcost_dzo\n\n        # Phase 2\n        dzo_dah = wo\n        dcost_dah = np.dot(dcost_dzo , dzo_dah.T)\n        dah_dzh = sigmoid_der(zh)\n        dzh_dwh = feature_set\n        dcost_wh = np.dot(dzh_dwh.T, dah_dzh * dcost_dah)\n    \n        dcost_bh = dcost_dah * dah_dzh\n        self.grads = {\"dcost_wh\": dcost_wh,\"dcost_bh\": dcost_bh,\"dcost_wo\":dcost_wo ,\"dcost_bo\": dcost_bo}\n\n    def __update_weights(self, learning_rate):\n        dcost_wh = self.grads[\"dcost_wh\"]\n        dcost_bh = self.grads[\"dcost_bh\"]\n        dcost_wo = self.grads[\"dcost_wo\"]\n        dcost_bo = self.grads[\"dcost_bo\"]\n\n        wh = self.parameters['wh']\n        bh = self.parameters['bh']\n        wo = self.parameters['wo']\n        bo = self.parameters['bo']\n\n        #ADAM\n        beta1=0.9\n        beta2=0.999\n        er=10**(-8)\n\n        mwh=0\n        vwh=0\n        mbh=0\n        vbh=0\n        mwo=0\n        vwo=0\n        mbo=0\n        vbo=0\n\n        #Untuk node hidden layer\n        mwh = beta1*mwh + (1-beta1)*dcost_wh\n        vwh = beta2*vwh + (1-beta2)*(dcost_wh**2)\n        mwh_ = mwh\/(1-beta1)\n        vwh_ = vwh\/(1-beta2)\n        wh -= learning_rate * mwh_\/(np.sqrt(vwh_)+er)\n        \n\n        #untuk bias hidden layer\n        mbh = beta1*mbh + (1-beta1)*dcost_bh.sum(axis=0)\n        vbh = beta2*vbh + (1-beta2)*(dcost_bh.sum(axis=0)**2)\n        mbh_ = mbh\/(1-beta1)\n        vbh_ = vbh\/(1-beta2)\n        bh -= learning_rate * mbh_\/(np.sqrt(vbh_)+er)\n        \n\n        #Untuk node output layer\n        mwo = beta1*mwo + (1-beta1)*dcost_wo\n        vwo = beta2*vwo + (1-beta2)*(dcost_wo**2)\n        mwo_ = mwo\/(1-beta1)\n        vwo_ = vwo\/(1-beta2)\n        wo -= learning_rate * mwo_\/(np.sqrt(vwo_)+er)\n        \n\n        #Untuk bias output layer\n        mbo = beta1*mbo + (1-beta1)*dcost_bo.sum(axis=0)\n        vbo = beta2*vbo + (1-beta2)*(dcost_bo.sum(axis=0)**2)\n        mbo_ = mbo\/(1-beta1)\n        vbo_ = vbo\/(1-beta2)\n        bo -= learning_rate * mbo_\/(np.sqrt(vbo_)+er)\n\n\n        self.parameters = {\"wh\": wh,\n                           \"bh\": bh,\n                           \"wo\": wo,\n                           \"bo\": bo}","cab5cefb":"def check_accuracy(prediction_result, actual_data):\n    if prediction_result.shape != actual_data.shape:\n        raise ValueError(\"Ukuran data input dan output tidak sesuai!\")\n\n    rows, = prediction_result.shape\n    correct_count = 0\n    for i in range(rows):\n        if prediction_result[i] == actual_data[i]:\n            correct_count += 1\n\n    accuracy_rate = correct_count \/ rows\n    return accuracy_rate","6c2fb68c":"input_data_2 = input_data.copy()\nscale_inputs(input_data_2)\n\ntest_same_data = Neural(input_data_2.shape[1], output_data_feat.shape[1], 10)\ntest_same_data.train(input_data_2, output_data_feat, 5000, 0.001)\n\nprediction_result = test_same_data.predict(input_data_2)\nprint(\"Train Accuracy:\",\n      check_accuracy(output_feat_to_num(prediction_result), output_data_num))","6e2653aa":"from math import floor\n\ndef kfold_iterable(k_count: int, inp: np.ndarray, outp: np.ndarray):\n    # Jangan lakukan pass-by-reference agar tidak menimbulkan side-effect\n    inp = inp.copy()\n    outp = outp.copy()\n\n    if len(inp) != len(outp):\n        raise ValueError()\n    \n    data_length = len(inp)\n    chunk_size = floor(data_length \/ k_count)\n \n    def k_split(data, iter):\n        needle = iter * chunk_size\n \n        training = np.concatenate((data[: needle], data[needle + chunk_size :]))\n        testing = data[needle : needle + chunk_size]\n \n        return training, testing\n \n    i = 0\n    while i < k_count:\n        training_data = {}\n        testing_data = {}\n \n        training_data[\"input\"],  testing_data[\"input\"]  = k_split(inp, i)\n        training_data[\"output\"], testing_data[\"output\"] = k_split(outp, i)\n \n        yield training_data, testing_data, i+1\n        i += 1","ea0e0722":"# Selalu gunakan \"otak\" yang sama pada setiap iterasi k-fold\nbrain = Neural(input_data.shape[1], output_data_feat.shape[1],10)\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\nsc = StandardScaler()\n\n# Kumpulkan nilai-nilai akurasi yang didapatkan untuk dapat dihitung reratanya\naccuracies = []\n\n# Lakukan iterasi untuk mengeksekusi metode k-fold cross-validation\nfor training_data, testing_data, i in kfold_iterable(5,\n                                                     input_data,\n                                                     output_data_feat):\n    training_input = training_data[\"input\"]\n    training_output = training_data[\"output\"]\n\n    training_input = sc.fit_transform(training_input)\n    \n\n    testing_input = testing_data[\"input\"]\n    testing_input = sc.transform(testing_input)\n\n    actual_output = output_feat_to_num(testing_data[\"output\"])\n \n    brain.train(training_input, training_output, 5000, 0.001)\n    prediction_result = brain.predict(testing_input, True)\n\n    accuracy = check_accuracy(prediction_result, actual_output)\n    print(f\"Akurasi saat iterasi ke-{i} adalah sebesar {accuracy}\")\n\n    # Tambahkan ke kumpulan nilai-nilai akurasi\n    accuracies.append(accuracy)\n\n# Hitung rata-rata dari keseluruhan nilai-nilai akurasi yang terdapatkan\nprint(\"Rata-rata keseluruhan akurasi adalah sebesar\", np.mean(accuracies))","ad8937b2":"## 3.2. Forward Propagation","58ad8d07":"### 3.4.2. *bo* Parameters","844e1d34":"# 1. Import Library","e930a675":"We will see how far the cost function changes with the parameters we want to change each iteration.\n\nWe do the derivative of the cost function with respect to the parameters we want to change\n\n","c6d7157c":"There is no missing data in the dataset","02940e92":"## 5.5. Define a class that represents a Neural Network instance","8cb24e07":"The following are the steps that occur in the neural network :","87fdbd4e":"We will train our model by dividing the train data as shown above and testing with the test data as shown above. Each iteration is compared with the actual data, then gets an accuracy score, then the score is averaged to get an average validation score","749911a9":"## 5.2. Convert the output data into a feature set and save it in a new variable","6f109114":"We apply this algorithm to different *g* and *w* according to the parameters you want to change. For exmple to update *wh* :\n\n![image.png](attachment:86a3b806-e43b-42ee-b17c-ffdd60cf3c50.png)","63c7ba81":"## 5.8 Define the *iterator* function to separate (*split*) data *input* and *output* with the K-Fold method","abf92d0f":"There are 9 columns that contain the quantity values of the glass constituent elements and 1 column that describes the type of glass","0fcc8f2f":"## 5.3. Define a function to perform *scaling* on input data","36d76d43":"## 2.7 Correlation between Features\/Column","53d3ec5b":"## 5.1. Define a function to convert from numeric data form to feature set","bf74ee8d":"## 5.7. Testing the model that has been made on the same data as the real data","2386f50e":"From the previous point we know :\n\n![image.png](attachment:5344d472-33d8-49d2-98ae-d6924a2e78ed.png)\n\n![image.png](attachment:2064eb8f-4bc8-4aab-830e-b109dd53b7f7.png)\n\nWe get :\n\n![image.png](attachment:d79b7954-cb8f-4231-8dda-1516f37ca6fa.png)","2d86a70e":"## 2.3. See Some Data","d3831807":"Previously, we used a gradient decent algorithm to update the weights and biases, but it takes a lot of iterations, to achieve 80% accuracy it takes 3 million iterations. Therefore we use another optimization method, namely ADAM.\n\nWe initiate each parameters the things below first :\n\nm0 = 0\n\nv0 = 0\n\nbeta1=0.9\n\nbeta2=0.999\n\neta=10**(-8)\n\neta(learning rate) = 0.001","a2e449aa":"## 2.6. Describe the Categorical Feature","8cfb147a":"## 2.2. Identification the Missing Data (NaN Data)","d420ae56":"## 2.1. Import Dataset","cd3e43b5":"# 2. Dataset Identification","5ced3a1c":"## 3.1.  Parameter initialization (*wh, bh, wo, bo*)","d676af82":"# 5. Implementation","9fe52b12":"This is our Artificial Intelligence Assigment in the class, we were instructed to implement one of the machine learning algorithms without using machine learning libraries like *sklearn*, *PyTorch* or *TensorFlow*.\n\n\nThe biggest reason we use Neural Network because **we want to know how the Neural Network algorithm works** and **Neural Network is easy to implement**.\n\nThis notebook is proof of our learning process so it is far from perfect. Therefore, we ask for it to be corrected if you find a mistake.\n\nThanksss :))\n\nHappy Reading!!!\n\n","ef2f796e":"## 2.4. Describe the Numerical Feature","4e664540":"### 3.4.1. *wo* Parameters\n\nWe uses chain rule\n\n![image.png](attachment:1e8f8646-ed73-497e-a6a3-2df3e6653fe7.png)","88c3698d":"## 3.4. Back-Propagation","3e75b308":"### 3.4.4. *bh* Parameters\n\n![image.png](attachment:207b34b8-3c50-43bd-bb0d-310ea24029d0.png)\n\n","a34f921c":"We use the cross entropy function to compare the predicted value and the actual value, for each rows :\n![image.png](attachment:311e6638-f549-43ff-aab7-e43b44788167.png)\n\n*y* is the actual value and *ao* is the predicted value","146e2184":"![image.png](attachment:61822e4c-bf83-4649-96a8-17e1e89cc0c6.png)","756fd1ea":"We get :\n\n![image.png](attachment:07708fe6-c384-4086-925b-9942dcb3ed09.png)\n\n[see https:\/\/deepnotes.io\/softmax-crossentropy]\n\nand : \n\n![image.png](attachment:95454d52-fd9a-4645-b413-b72930c8ca26.png)\n\nso : \n\n![image.png](attachment:bdd34840-b788-454b-821e-f620db00c9d7.png)\n","7baf289b":"The numbers between the features are quite far apart, so we need to do some scaling. The scaling method used is the *standardscaler* ","7c905bea":"![image.png](attachment:df0cc231-5545-42cf-8a36-3606fb4edf84.png)","a6825eda":"This data consists of 10 features and 214 rows. Where 1 feature named 'Type' is a Feature target which is used as a comparison in the model","8fa754cb":"This is a process in the neural network that occurs every data rows (**for example in the first row**)","b629faf9":"We can see in the 'Ba' column there are a lot of data outliers. Ideally, we remove the outliers, but because there are too many outliers in this small dataset, we will not process the outliers. ","c7f889cc":"## 3.3. Cost Function","675df80e":"The image above is the neural network architecture that will be created. There are 9 nodes on the input layer, 10 nodes on the hidden layer, and 7 nodes on the output layer.\n\n*wh* is the weight from the input layer to the hidden layer, *wo* is the weight from the hidden layer to the output layer, *bh* is bias on hidden layer, and *bo* is bias on output layer.","79d2eeba":"# 3. Neural Network\n\n![image.png](attachment:c11b7448-cca2-435c-b6c9-ecc92e473031.png)\n\n**In the picture there are 9 nodes hidden layer, it should be 10**","5130bf62":"# 7. References\n* https:\/\/stackabuse.com\/creating-a-neural-network-from-scratch-in-python-multi-class-classification\n* https:\/\/towardsdatascience.com\/adam-latest-trends-in-deep-learning-optimization-6be9a291375c","3441292d":"# 4. K-fold Cross Validation","b78dba26":"## 5.6. Define a function to measure the accuracy of prediction results against real data","fdf20093":"We can see that the data rows whose target features have values 1 and 2 are more than the others with a significant difference. This dataset is imbalance. To minimize the impact of imbalanced data, **we will randomize the data rows before modeling**.","6cc6755f":"## 5.9 Validation of model implementation results using the K-Fold Cross-Validation method","df9d5c78":"![image.png](attachment:5b82f991-7771-4c51-8a86-43ee724c1927.png)\n\n![image.png](attachment:7eb90926-fa13-49b4-babb-2a985c42a3c1.png)\n\n![image.png](attachment:faa6f886-a1d6-49b5-870c-3b7daff40d1d.png)\n\nWe get :\n\n![image.png](attachment:44f38acd-a56d-4b39-bab9-15aa76f2f8b5.png)","ed008903":"**We will do forward propagation and backpropagation continuously during the iteration, to get the best weight and bias**","b19be71f":"* The model is able to make training accuracy of 93%+\n* The average validation score is 72+%\n* The model is overfitting because when it is validated the score is far below the training accuracy score\n","699e9e9e":"We initialize the parameter matrix sizes *wh, bh, wo, bo* and initialize their values with random numbers.\n* *wh* matrix has size 9 x 10\n* *bh* matrix has size 10 x 1\n* *wo* matrix has size 10 x 7\n* *bo* matriks has size 7 x 1\n\n\n**[Code will be shown in chapter 5 (implementation)]**","6fd41e98":"We use the **Kendall Tau correlation coefficient** for numeric data input and categorical output\n\n'Ca' and 'Ri' have the greatest correlation, which is 0.53.\n\n'Al', 'Ba', and 'Na' have a fairly large positive correlation to the target feature 'Type' with correlations of 0.44, 0.44, and 0.3 respectively.\n\n'Mg' has a high negative correlation to the target feature 'Type' with 0.46.\n\nIn the neural network, we try to include all the features even though the correlation to the target feature has a small value like 'Ca'","7d72f46f":"*x* is the input matrix, its size is like the size of the dataset (214,9)\n\nWe can see that *zh* is the sum of multiplication the rows of the *x* matrix and the columns of the *wh* matrix plus the bias *bh*. \n\n**We can simplify that *zh* matrix is the dot product of *x* matrix  and *wh* matrix plus *bh*** \n\n![image.png](attachment:566e20ca-3254-4f52-bc33-5af4d613c852.png)\n\nNext we will perform an activation function on the Zh matrix,in multi-label classification, the hidden layer uses **sigmoid** and the output layer uses **softmax**. For each value in Zh matrix :\n\n![image.png](attachment:aabc7a4f-14b9-4523-af07-5602dc43f4fb.png)\n\nthen *ah* is processed as *x* in the previous process with *wo* and *bo* parameters (**for example in the first row**)\n\n![image.png](attachment:11917ddc-2cad-4932-a54d-17339a978aaa.png)\n\n**We can simplify that *zo* matrix is the dot product of *ah* matrix  and *wo* matrix plus *bo*** \n\n![image.png](attachment:aa8aef90-8fa6-4ce5-91c1-f410619029d1.png)\n\nFinally, we will implement the softmax activation function for each value in Zo\n\n![image.png](attachment:50b64f02-3f33-40d6-a7e1-e5b76f2e43bb.png)\n\n*ao* is the predicted result of the neural network, which will be compared with the actual value. then we will do **back-propagation** to change the value of weight and bias\n","c80e9692":"### 3.4.3. *wh* Parameters","6f35a88c":"![image.png](attachment:31f708b7-d6e2-4855-8d67-00bd139558e4.png)\n\n![image.png](attachment:e0959d12-d7c5-46ee-81aa-bdd82668c439.png)\n\n![image.png](attachment:c1d8db41-e49e-472e-a5fe-03b11ed6a97e.png)\n\nso :\n\n![image.png](attachment:06737355-c00a-4a3d-9069-65caa8f828f8.png)\n","4264f1b6":"The 'Type' feature is read as numeric data, even though the numbers are the labeling of the type of glass. **We need an action on this feature**\n\nIt can be seen that the mean and median of the 'Type' feature is in the range of 2, this indicates that **the dataset is imbalanced** and leads to type 2 glass.","3d146cd6":"### 2.5 Outliers Data","15541fa2":"In data identification we get several things:\n* The dataset has 214 rows with 9 columns containing the quantity values of the constituent elements and 1 target column named 'Type'.\n* There is no missing data in the dataset.\n* Parse feature 'Type' into 7 columns. For example, if the Feature 'Type' has a value of 3, then it becomes [0 0 1 0 0 0 0].\n* The dataset is imbalanced, we will randomize the data rows before modeling.\n* The numbers between the features are quite far apart, so we need to do some scaling. We will use *Standardscaler*.\n* Too many outliers in this small dataset. Therefore, we will not process the outliers. \n* Some features have a fairly large correlation with the target feature 'Type'","7bf4414e":"# 6. Conclusion ","6d493af1":"## 5.4. Define the mathematical functions that will be needed","222ac2e2":"![image.png](attachment:016b134f-c752-4658-9d82-b0889e667444.png)\n\nt is iteration. g is the derivative of the cost function of the parameters we change such as *wh, wo, bh, and bo*\n\n![image.png](attachment:abcb224a-4c88-42aa-97ec-0882bbbb0443.png)\n\nThen, we update the bias and weight :\n\n![image.png](attachment:724c2c66-b5d1-4c8a-b0a4-afb938e4c09c.png)","5b50e6ca":"### 3.4.5. ADAM Optimization","0b640cee":"![image.png](attachment:f4749035-51c9-43b1-baa5-bbe5fa636e44.png)"}}