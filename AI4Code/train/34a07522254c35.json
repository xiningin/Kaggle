{"cell_type":{"de4d71a3":"code","d9cf90a1":"code","0bb22225":"code","77963968":"code","25b5d7e1":"code","4d6dca8e":"code","48012d5f":"code","58c3c5f0":"code","49b9208f":"code","b4abd4f1":"code","7d0ee0fd":"code","bdb2075d":"code","6bd537b8":"code","dbf2edc3":"code","a4be8fe7":"code","aae9240b":"markdown","b359ac8a":"markdown","4674dbee":"markdown","a8b653ad":"markdown","65e483b0":"markdown","392d31fc":"markdown","d11487ac":"markdown","6b8cc388":"markdown","b41fb6e4":"markdown","956d4e7a":"markdown","85b63402":"markdown","948cf5be":"markdown","75bd1149":"markdown","861c5cd3":"markdown"},"source":{"de4d71a3":"! pip install econml","d9cf90a1":"# If you can't run econml run this block to update your sciket learn\n! pip install -U scikit-learn","0bb22225":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfrom functools import reduce\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.style.use('ggplot')","77963968":"# sklearn\nfrom sklearn.linear_model import Lasso, MultiTaskLasso, LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\n\n# To support SVM and XGB in estimating multi-dimensional outputs\nfrom sklearn.multioutput import MultiOutputRegressor \nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost import XGBRegressor\n\n#EconML\nfrom econml.dml import LinearDMLCateEstimator, SparseLinearDMLCateEstimator","25b5d7e1":"# Load Data\ndf = pd.read_csv('..\/input\/avocado-prices\/avocado.csv')\ndf.drop(columns = ['Unnamed: 0'], inplace = True)\ndf['Date'] = pd.to_datetime(df['Date'])\ndf.sort_values(by = ['Date','region']).head()\n","4d6dca8e":"# Prepare propriate dataset for estimation\n# Use weighted average to calculate the \nwt_avg = lambda x: np.average(x, weights = df.loc[x.index, 'Total Volume'])\n\ndf1 = pd.pivot_table(\n    df, \n    index = ['Date','region'], \n    columns = df.groupby(['Date','region']).cumcount(), \n    values = ['AveragePrice','Total Volume'],\n    aggfunc= {'AveragePrice':wt_avg, 'Total Volume':'sum'}\n).reset_index()\ndf1.head()","48012d5f":"# Rename columns\ndf1.columns = df1.columns.map('{0[0]}{0[1]}'.format) \ndf1 = df1.rename(\n        index = str,\n        columns = {\n            'AveragePrice0':'Price_conventional',\n            'AveragePrice1':'Price_organic',\n            'Total Volume0':'Volume_conventional',\n            'Total Volume1':'Volume_organic'}\n)\n# Fill NA occurs in aggregation\nFillNAValues = df1.groupby('Date')[['Price_organic','Volume_organic']].transform('mean')\ndf1[['Price_organic','Volume_organic']] = df1[['Price_organic','Volume_organic']].fillna(FillNAValues) \ndf1.head()","58c3c5f0":"# Create lag Price and Volume by each type \nlag_variables = ['Price_conventional','Price_organic','Volume_conventional','Volume_organic']\n\ndf1 = df1.sort_values(by = 'Date')\nfor col in lag_variables:\n    for i in [1,2,3]: \n        col_name = f\"Lag{i}_{col}\"\n        df1[col_name] = df1.groupby('region')[col].shift(i)\ndf1 = df1.dropna()\n\n# Separate Aggregated regions to avoid multi-collinearity\nregional_data = df1[~df1['region'].isin(['Northeast','SouthCentral','Southeast','Midsouth','West','TotalUS'])]\nregional_data.head()","49b9208f":"scaler = StandardScaler()\nY = np.log(regional_data[['Volume_conventional','Volume_organic']])\nP = np.log(regional_data[['Price_conventional','Price_organic']])\n\nlag_price =  np.log(\n        regional_data[['Lag1_Price_conventional', 'Lag2_Price_conventional','Lag3_Price_conventional','Lag1_Price_organic', 'Lag2_Price_organic','Lag3_Price_organic']]\n)\nlag_volume = np.log(\n        regional_data[['Lag1_Volume_conventional', 'Lag2_Volume_conventional','Lag3_Volume_conventional','Lag1_Volume_organic', 'Lag2_Volume_organic','Lag3_Volume_organic']]\n)\n\n# Create polynominal features\nPrice_Volume = np.concatenate([lag_price, lag_volume], axis = 1)\npoly = PolynomialFeatures(degree = 2)\nPrice_Volume = poly.fit_transform(Price_Volume)\n\n# Year-Month pairs and region fixed effects\nw_date = pd.get_dummies(regional_data['Date'].dt.to_period('M')).values #.dt.to_period('M')\nw_region= pd.get_dummies(regional_data['region']).values\n\n# Concatenate all the features\nW = reduce(lambda x,y: np.concatenate([x,y], axis = 1), [Price_Volume, w_date, w_region]) #w_region","b4abd4f1":"# OLS\nest_ols = LinearDMLCateEstimator(\n    model_y = LinearRegression(),\n    model_t = LinearRegression(),\n    n_splits = 5\n)\nest_ols.fit(Y, P, None, W, inference = 'statsmodels')\nprint('OLS MSE: {:.4f}'.format(est_ols.score(Y,P,None,W)))\n\n\n# Lasso\nest_lasso = LinearDMLCateEstimator(\n    model_y = Lasso(alpha = 0.001),\n    model_t = Lasso(alpha = 0.01),\n    n_splits = 5\n)\nest_lasso.fit(Y, P, None, W, inference = 'statsmodels')\nprint('Lasso MSE: {:.4f}'.format(est_lasso.score(Y,P,None,W)))\n\n\n# Randomforest\nrf_y = RandomForestRegressor(\n    bootstrap=True, n_estimators=300, n_jobs=-1, oob_score=True, random_state = 23\n)\nrf_t = RandomForestRegressor(\n    bootstrap=True, n_estimators=300, n_jobs=-1, oob_score=True, random_state = 22\n)\nest_rf = LinearDMLCateEstimator(\n    model_y = rf_y,\n    model_t = rf_t,\n    n_splits = 5\n)\nest_rf.fit(Y, P, None, W, inference = 'statsmodels')\nprint('RF MSE: {:.4f}'.format(est_rf.score(Y,P,None,W)))\n\n\n#SVM\nest_svm = LinearDMLCateEstimator(\n    model_y = MultiOutputRegressor(SVR(kernel = 'linear', C = 0.1)),\n    model_t = MultiOutputRegressor(SVR(kernel = 'linear', C = 0.1)), \n)\nest_svm.fit(Y,P,None,W, inference = 'statsmodels')\nprint('SVM MSE: {:.4f}'.format(est_svm.score(Y,P,None,W)))\n\n\n# Gradient Boosting\nest_gbr = LinearDMLCateEstimator(\n    model_y = MultiOutputRegressor(GradientBoostingRegressor()),\n    model_t = MultiOutputRegressor(GradientBoostingRegressor()),\n    n_splits = 5\n)\nest_gbr.fit(Y,P,None,W, inference = 'statsmodels')\nprint('GBR MSE: {:.4f}'.format(est_gbr.score(Y,P,None,W)))\n\n\n# Lasso + Random Forest\nest_mix1 = LinearDMLCateEstimator(\n    model_y = Lasso(alpha = .001),\n    model_t = rf_t,\n    n_splits = 5\n)\nest_mix1.fit(Y, P, None, W, inference = 'statsmodels')\nprint('Mix1 MSE: {:.4f}'.format(est_mix1.score(Y,P,None,W)))\n\n\n# Random Forest + Gradient Boosting\nest_mix2 = LinearDMLCateEstimator(\n    model_y = Lasso(alpha = .001),\n    model_t = MultiOutputRegressor(GradientBoostingRegressor()),\n    n_splits = 5\n)\nest_mix2.fit(Y, P, None, W, inference = 'statsmodels')\nprint('Mix2 MSE: {:.4f}'.format(est_mix2.score(Y,P,None,W)))","7d0ee0fd":"models = [est_ols, est_lasso, est_rf, est_svm, est_gbr, est_mix1, est_mix2]\nlabels = ['OLS','Lasso','RF','SVM', 'GBR','Mix1', 'Mix2']\nxlabels = ['Conventional','Organic']\n\nfig, axes = plt.subplots(ncols=2, nrows=2, figsize = [12,12], dpi = 120)\nfor index, ax in enumerate(axes.flatten()):\n    estmates = [model.const_marginal_effect()[0][index\/\/2][index%2] for model in models]\n    range_ = [model.const_marginal_effect_interval()[1][0][index\/\/2][index%2] - model.const_marginal_effect_interval()[0][0][index\/\/2][index%2] for model in models]\n    tmp = pd.DataFrame({'estimtes':estmates,'range':range_}, index = labels).reset_index()\n    for i in tmp.index:\n        ax.errorbar(\n            i, tmp.loc[i, 'estimtes'],\n            yerr = tmp.loc[i, 'range']\/2,\n            fmt = 'o', ms = 8, mfc = 'white', elinewidth = 3, capsize= 8)\n    ax.set_xticklabels(['','OLS','Lasso','RF','SVM','GBR','Mix1','Mix2'])\n    \nplt.tight_layout()\naxes[0,0].set_ylabel('% of  Conventional Volume Changed\\n')\naxes[0,0].set_title('1% Price Increase of Conventional')\naxes[0,1].set_title('1% Price Increase of Organic')\naxes[1,0].set_ylabel('% of  Organic  Changed\\n')","bdb2075d":"# define preprocessing function\ndef preprocess(data):\n    scaler = StandardScaler()\n    Y = np.log(data[['Volume_conventional','Volume_organic']])\n    P = np.log(data[['Price_conventional','Price_organic']])\n    lag_price = scaler.fit_transform(np.log(data[['Lag1_Price_conventional', 'Lag2_Price_conventional','Lag1_Price_organic', 'Lag2_Price_organic']]))\n    lag_volume = scaler.fit_transform(np.log(data[['Lag1_Volume_conventional', 'Lag2_Volume_conventional','Lag1_Volume_organic', 'Lag2_Volume_organic']]))\n    # Create polynominal features\n    Price_Volume = np.concatenate([lag_price, lag_volume], axis = 1)\n    poly = PolynomialFeatures(degree = 2)\n    Price_Volume = poly.fit_transform(Price_Volume)\n    w_date = pd.get_dummies(data['Date'].dt.to_period('M')).values\n    W = np.concatenate([Price_Volume,w_date],axis = 1)\n    return Y, P, W","6bd537b8":"regions = df1['region'].unique()\n\nest_conventional, conventional_interval_range = [], []\nest_organic, organic_interval_range = [], []\n\n# Estimate  \nfor r in tqdm(regions):\n    data = df1[df1['region'] == r]\n    \n    Y,P,W = preprocess(data)\n    \n    # Use SparseLinearDMLCateEstimator\n    est = LinearDMLCateEstimator(\n        model_y = MultiOutputRegressor(SVR(kernel = 'linear', C = 0.1)),\n        model_t = MultiOutputRegressor(SVR(kernel = 'linear', C = 0.1)), \n        n_splits = 3\n    )\n\n    est.fit(Y,P, None, W, inference = 'statsmodels')\n    est_conventional.append(est.const_marginal_effect()[0,0,0])\n    cont_interval = est.const_marginal_effect_interval()[1][0][0][0] - est.const_marginal_effect_interval()[0][0][0][0]\n    conventional_interval_range.append(cont_interval)\n\n    est_organic.append(est.const_marginal_effect()[0,1,1])\n    organic_interval = est.const_marginal_effect_interval()[1][0][1][1] - est.const_marginal_effect_interval()[0][0][1][1]\n    organic_interval_range.append(organic_interval)","dbf2edc3":"# \nconventional = pd.DataFrame(\n    {'region':regions, 'Elasticity':est_conventional, 'ci_range':conventional_interval_range}\n)\n\norganic = pd.DataFrame(\n     {'region':regions, 'Elasticity':est_organic, 'ci_range':organic_interval_range}\n)","a4be8fe7":"# arrange both dataset by organic avocado's elasticity \norganic  = organic.sort_values(by = 'Elasticity')\nconventional = conventional.set_index('region').loc[organic['region']].reset_index()\n\ncoordinates = np.arange(len(organic['region']))\n\nfig, ax = plt.subplots(figsize = [25,7], dpi = 150)\nax.errorbar(\n    organic['region'],\n    organic['Elasticity'],   #conventional['region']\n    yerr = organic['ci_range']\/2,\n    ecolor = '#461881', mfc = 'white', mec = '#461881',fmt='o', label = 'Organic')\nax.errorbar(coordinates+0.2,\n    conventional['Elasticity'], \n    yerr = conventional['ci_range']\/2,\n    ecolor = '#F9016F', mfc = 'white', mec = '#F9016F',fmt='o', label = 'Conventional')\n\n# Highlight aggregated regions\nax.get_xticklabels()[10].set_weight('bold')\nax.get_xticklabels()[28].set_weight('bold')\nax.get_xticklabels()[30].set_weight('bold')\nax.get_xticklabels()[36].set_weight('bold')\nax.get_xticklabels()[44].set_weight('bold')\nax.get_xticklabels()[51].set_weight('bold')\n\nplt.ylabel('Own-Price Elasticity\\n', fontsize = 20)\nplt.yticks(fontsize = 15)\nplt.yticks(fontsize = 12)\nplt.gca().invert_yaxis()\n\nplt.xticks(rotation = 90, fontsize = 15)\nplt.grid(axis = 'y')\nplt.legend(fontsize = 15)","aae9240b":"# 3. Introduction to DML Modeling\n## Orthogonal\/Double Machine Learning (DML)\nHere I am not going to cover the mathematical detail of the model, just to introduce how the model works and when should you use it. \n\nIf you want to know more about DML, please check [Chernozhukov2016](https:\/\/arxiv.org\/abs\/1608.00060) or [EconML doc](https:\/\/econml.azurewebsites.net\/spec\/estimation\/dml.html).\n\n### What is it?\nThe general model:\n$$Y =T\\cdot\\beta + g(W)+U, \\,\\,\\, \\mathbb{E}[U|T,W]=0$$\n\n$$T = d(W) + V, \\,\\,\\, E[V|W] = 0$$\n\nDouble Machine Learning is a method for estimating (heterogeneous) treatment effects when all potential confounders\/controls (factors that simultaneously had a direct effect on the treatment decision and the observed outcome) are observed.\n\nThe method reduces the problem to first estimating two predictive tasks:\n\n1. predicting the outcome from the controls,\n2. predicting the treatment from the controls;\n\nThen the method combines these two predictive models in a final stage estimation so as to create a model of the heterogeneous treatment efffect. \n\nThe approach allows for arbitrary Machine Learning algorithms to be used for the two predictive tasks, such as random forests, lasso, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. While maintaining many favorable statistical properties related to the final model (e.g. small mean squared error, asymptotic normality, construction of confidence intervals).\n\nThe following figure from [Chernozhukov2016](https:\/\/arxiv.org\/abs\/1608.00060) compares the results of adopting DML(right) and not adopting it(left) in a simulation experiment:\n![fig2.png](attachment:fig2.png)\n### When should you use it?\n\nAssume we have data that are generated from some collection policy. In particular, we assume that we have data of the form: \n$\\{Y_{i}(T_{i}), T_{i}, X_{i}, W_{i}, Z_{i}\\}$, \nwhere \n* $Y_{i}(T_{i})$ is the observed outcome for the chosen treatment, \n* $T_{i}$ is the treatment, \n* $X_{i}$ are the covariates used for heterogeneity, \n* $W_{i}$ are other observable covariates that we believe are affecting the potential outcome $Y_{i}(T_{i})$ and potentially also the treatment $T_{i}$, \n* $Z_{i}$ are variables that affect the treatment $T_{i}$ but do not directly affect the potential outcome. \n\nWe will refer to variables $W_{i}$ as controls and variables $Z_{i}$ as instruments. \nThe variables $X_{i}$ can also be thought of as control variables, but they are special in the sense that they are\na subset of the controls with respect to which we want to measure treatment effect heterogeneity. ","b359ac8a":"# Introduction\n\nHi! I'm Leo, an econ student and data sientist from Taiwan. In this notebook, we're going to explore the demand system, namely price elasticity, of Hass avocado using `Econml`.\n\nThe main issue of estimating treatment effects (price elasticity in this case) is how to solve endogeneity and get the correct, unbiased estimates. The basic problem of endogeneity occurs when the explanans (X) may be influenced by the explanandum (Y) or both may be jointly influenced by an unmeasured third. If we regress such Y on X , we may get a statistically significant but meaningless result.\n\nConventional ML algorithms like Random Forest or Gradient Boosting don't aim to solve this kind of problems, they explore correlations very well but fail on getting unbiased estimates. On the other hand, a lots of traditional econometrics models focus on this kind of problems, like 2SLS, IV, Regression on discontinuity (RD), Difference in Difference (DID), etc. The former two models rely on other variables which affect Y only through X, and the laters rely on some special data generation process. \n\nI encourage you if you think the abovementioned models can be apply on your data to get a causal relationship, however, most of the time we don't get the chance to apply those models by many restrictions. Therefore, we can't get the causal inferences which we want.\n\nToday I want to share with you a very interesting model - Double\/Debiased Machine Learning, which is used to measure the effectiveness of the policy. The original paper was published by Victor Chernozhukov (paper link), and Microsoft introduces the package `EconML` based on the method propused in this paper.\n\n`EconML` offer large flexibility in modeling the effect heterogeneity (via techniques such as random forests, boosting, lasso and neural nets), while at the same time leverage techniques from causal inference and econometrics to preserve the causal interpretation of the learned model and many times also offer statistical validity via the construction of valid confidence intervals.\n\nFor more information please check the [Official Website](https:\/\/econml.azurewebsites.net\/index.html).","4674dbee":"## 4. All Markets Modeling","a8b653ad":"In the modeling section, I will implement technics such as OLS, Lasso, Random Forest, XGBoost and SVM as well as some mixed models(e.g. Random Forest + Lasso).\n\n### Let's begin!","65e483b0":"It is worth noting that the means and confidence intervals in regional estimation are much more larger compare to the overall estimates. It may dut to model can't learn the relatively small data points (which is 166 for each region) very well, so the estimates are upward biased. You can also find that the price elasticity of organic avocado in Northen New England is positive, which is certainly againsts our economic intuition. However, it is still worth to compare price sensitivities between regions.\n ### Key Findings\n1. Demand for conventional avocados appears to be consistent across markets, the price elasticities are centered around -1.3 with small 95% confidence intervals. \n2. Demand for organic avocados seems to be more volatile across markets, that could be result from many reasons. For example, smaller trading volumes, the choose for healthier life style or accessibility to other organic products, etc.","392d31fc":"### Prepare variables and normalize data","d11487ac":"## 2. Preprocessing","6b8cc388":"## Results Visualization","b41fb6e4":"### How to build the model in this case?\nSince the heterogeneity of price elasticities are not of my interest, so I ignore the heterogeneity variable $X_{i}$ and build the model base on the covriates $W$,\nwhere $W$ include Year-Month pairs, regional fixed effects and 3 lagged realizations of the demand system $(Y_{i,t\u2212l}, P_{i,t\u2212l})_{i\u2208[I],\\,\\,l\u2208\\{1,2\\}}$, and interactions to maximize the predictive performance of the first stage ML models.\n\nThe (log) volumes are used as the dependent variable, $Y$ and log prices are used as used treatment variables, $T$.\n\n### What will we get ?\nOur model for demand system:\n$$Log\\,\\,Q_{i,\\,t} = Log\\,\\, P_{own,\\,t}\\cdot\\theta_{own} +Log\\,\\, P_{other,\\,t}\\cdot\\theta_{cross} + g(W_{i,t}) + \\epsilon_{i,t}$$\n\n$$Log\\,\\,P_{own,\\,t} = d(W_{i,\\,t}) + \\mu_{i,t}$$\n\nWhere $i$ and $t$ together denote a unique combination of region and date.\n\nSince the heterogeneity of price elasticities are not of my interest, so I ignore the heterogeneity variable $X_{i}$ and build the model base on the covriates $W$,\nwhere $W$ include Year-Month pairs, regional fixed effects and 3 lagged realizations of the demand system $(Y_{i,t\u2212l}, P_{i,t\u2212l})_{i\u2208[I],\\,\\,l\u2208\\{1,2\\}}$, and interactions to maximize the predictive performance of the first stage ML models.\nThe (log) volumes are used as the dependent variable, $Log\\,\\,Q$ and log prices are used as used treatment variables, $Log\\,\\,P$.\n\nThe own-price elasticities and cross-price elasticities of avocado are $\\theta_{own}$ and $\\theta_{cross}$, respectively.","956d4e7a":"# 1. A Refresher on Price Elasticity\n## What is price elasticity?\n\nThe foundamental assumption in economics is that people will buy the product or service if it\u2019s cheaper and less will buy it if it\u2019s more expensive. But the phenomenon is more quantifiable than that, and price elasticity shows exactly how responsive customer demand is for a product based on its price. \n\n## How is it calculated?\nAssuming that $Q$ is the quantity of demand for a certain commodity, and $P$ is the price of the commodity. Then the formula for price elasticity of demand:\n\n$E_{d} = \\frac{\\Delta Q\/Q}{\\Delta P\/P}$\n\nLet\u2019s look at an example. Say that a clothing company raised the price of one of its coats from \\$100 to \\$120. The price increase is $\\frac{\\$120-\\$100}{\\$100}$ or 20%. Now let\u2019s say that the increase caused a decrease in the quantity sold from 1,000 coats to 900 coats. The percentage decrease in demand is -10%. Plugging those numbers into the formula, you\u2019d get a price elasticity of demand of:\n\n$E_{d} = \\frac{\\Delta Q\/Q}{\\Delta P\/P} = \\frac{-100\/1000}{20\/100} = - 0.5$\n\n## 3. How to interpret price elasticity?\n\nThere are five zones of elasticity. Products and services can be:\n\n* $E_{d}=0$\uff1aPerfectly Inelastic.\n\nWhere the quantity demanded does not change when the price changes. Products in this category are things consumers absolutely need and there are no other options from which to obtain them.\n\n* $0<|E_{d}|<1$: Inelastic.\n\nWhere 1% of changes in price cause small changes in demand (less than 1%). Gasoline is a good example here because most people need it, so even when prices go up, demand doesn\u2019t change greatly. Products with stronger brands tend to be more inelastic.\n\n* $|E_{d}|=1$: Unitarily Elastic.\n\nWhere any 1% change in price is matched by an equal change in quantity (equal to 1%).\n\n* $1<|E_{d}|<\\infty$: Elastic.\n\nWhere 1% changes in price cause large changes in quantity demanded (greater than 1%). Beef is an example of a product that is relatively elastic.\n\n* $E_{d}=\\infty$: Perfectly Elastic.\n\nWhere any very small change in price results in a very large change in the quantity demanded. Products that fall in this category are mostly \u201cpure commodities,\u201d says Avery. \u201cThere\u2019s no brand, no product differentiation, and customers have no meaningful attachment to the product.\u201d\n\n### References\n* [Harvard Business Review](https:\/\/hbr.org\/2015\/08\/a-refresher-on-price-elasticity)\n* [Wiki](https:\/\/en.wikipedia.org\/wiki\/Price_elasticity_of_demand)\n\n## So what do you think the price elasticity of avocado will be like? \n## Let\u2019s go explore it !","85b63402":"# 5. Regional Market Modeling\nTo find out the regional demand system, I am going to run a separate estimation on each region. \n\nUse Random Forest as the base model:","948cf5be":"# Table of contents\n1. A Refresher on Price Elasticity\n2. Loading the data\n3. Introduction to DML \n4. All Markets Modeling\n5. By region Modeling\n6. Conclusion","75bd1149":"Figures represents percentage of change in volume corresponds to 1% of change in the product price. For example the bottom left figure shows that if price of conventional avocado increase by 1%, the volume of organic avocado will increase by roughly 20% ~ 28%, which is the cross elasticity of conventional type to organic type avocado.\n\nThere are three key findings:\n\n* **Price Sensitive**: The diagnal figures shows that the value of own price elasticities are around -1.35 and -1.43, repectly. Which suggest that consumers for both types of avocado are price sensitive. \n\n* **Asymmetric Substitution**: The bottom left figure represents the cross price elasticity of conventional avocado to organic avocado. The effects are consistently centered around 0.23 and statistically significant( 95% intervals didn't intersect with zero). On the other hand, The top right figure represents the cross price elasticity of organic avocados to conventional avocados. These estimated values are very close to zero, and some of them not statistically significant. \nIn summary, consumers may substitude conventional avocados to organic avocados when the price of conventional avocado increases, but not in reverse direction.\nWhich suggest the subsitution effect from conventional to organic avocados  \n\n* **SVM**: Considering mean estimates and confidence intervals, SVM seems to be the stablest model among all algorithms. Therefore, we will use it as base model in the next section.","861c5cd3":"## Conclusion: What do we find ? \n\nKey findings from the avocados demand system analysis:\n* **Picky Consumers**: Consumers are picky to price among markets (elastcity > 1), which make sense because avocados ain't necessity goods. People won't die if they don't eat avocados, they can wait until the price fall.\n* **One way market**: The cross-price elasticity analysis shows that consumers would turn to organic avocados when the price of conventional avocados increases, but would not turn to conventional avocados when the price of organic avocados increases. We can conclude that organic avocado is substitution to conventional avocados, if prices of ordinary avocados become expensive, isn't it great for us to buy organic fruits ?"}}