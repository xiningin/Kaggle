{"cell_type":{"ab4f5187":"code","5cf8550d":"code","aa60c5aa":"code","6dfd645b":"code","f8a8e642":"code","6f1857d7":"code","2ea3952c":"code","98dc5bf3":"code","cab59f3e":"code","40404ba9":"code","84215279":"code","444f6ce9":"code","da323c8d":"code","da429540":"code","e25efa4b":"code","64eb46f0":"code","1612a19a":"code","94b98414":"code","8480a588":"code","e160acca":"code","36bbe21c":"code","c0010ba3":"code","05a94cd9":"code","9da37c08":"code","f2c8f3d1":"code","62b7a7ec":"code","074abd1f":"code","f0a817d0":"code","4cc946ab":"code","cfbbdb18":"code","2baf1fa4":"code","dd60468d":"code","61b15aa2":"code","96ff9c6a":"code","2b94712c":"code","092688c4":"code","e3c45fda":"code","1249fb27":"code","aed0a515":"code","4f42ad7b":"markdown","531b551d":"markdown","e41df62a":"markdown","925c6fd1":"markdown","c6309106":"markdown","27257706":"markdown","d6a49a6a":"markdown","2070d30e":"markdown","3bd0e883":"markdown","16b89fdf":"markdown","522e2ff4":"markdown","100a7fc4":"markdown","2e7a60f8":"markdown","e35c3332":"markdown","980e42be":"markdown","68fe7a45":"markdown","cf2e41a4":"markdown","8ca20dff":"markdown","56733865":"markdown","45e61a7b":"markdown","b909d2e4":"markdown","a899eb61":"markdown","b354f687":"markdown","c8d38899":"markdown","5b714ac9":"markdown","c18d3040":"markdown","5b4f4e5a":"markdown","5b091cb1":"markdown","ce5fa94d":"markdown","72f57260":"markdown"},"source":{"ab4f5187":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom pandas_profiling import ProfileReport\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5cf8550d":"df = pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\nprint('The Dataset contains {} rows and {} columns '.format(df.shape[0], df.shape[1]))","aa60c5aa":"df.head()","6dfd645b":"df.describe()","f8a8e642":"ProfileReport(df)","6f1857d7":"df['quality'].value_counts().index","2ea3952c":"plt.figure(figsize=(18,10))\nsns.heatmap(df.corr(), annot=True, cmap=plt.cm.plasma)","98dc5bf3":"df.isnull().sum().sum()","cab59f3e":"df.info()","40404ba9":"df.hist(bins=40, figsize=(10,15))\nplt.show()","84215279":"df.plot(kind='density', subplots=True, layout=(4,3), sharex=False)\nplt.show()","444f6ce9":"data = df.groupby(by=\"fixed acidity\")[[\"fixed acidity\", \"density\", \"citric acid\"]].first().reset_index(drop=True)\n\n# Figure\nf, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (16, 6))\n\na = sns.distplot(data[\"fixed acidity\"], ax=ax1, hist=False, kde_kws=dict(lw=6, ls=\"--\"))\nb = sns.distplot(data[\"density\"], ax=ax2, hist=False, kde_kws=dict(lw=6, ls=\"--\"))\nc = sns.distplot(data[\"citric acid\"], ax=ax3, hist=False, kde_kws=dict(lw=6, ls=\"--\"))\n\na.set_title(\"Fixed Acidity Distribution\", fontsize=16)\nb.set_title(\"Density Distribution\", fontsize=16)\nc.set_title(\"Citric Acid distribution\", fontsize=16)","da323c8d":"from pandas.plotting import scatter_matrix\n\nsm = scatter_matrix(df, figsize=(16, 10), diagonal='kde')\n\n[s.xaxis.label.set_rotation(40) for s in sm.reshape(-1)]\n[s.yaxis.label.set_rotation(0) for s in sm.reshape(-1)]\n\n#May need to offset label when rotating to prevent overlap of figure\n\n[s.get_yaxis().set_label_coords(-0.6,0.5) for s in sm.reshape(-1)]\n\n#Hide all ticks\n\n[s.set_xticks(()) for s in sm.reshape(-1)]\n[s.set_yticks(()) for s in sm.reshape(-1)]\nplt.show()","da429540":"# Dividing wine as good and bad by giving the limit for the quality\n\nbins = (2, 6, 8)\ngroup_names = ['bad', 'good']\ndf['quality'] = pd.cut(df['quality'], bins = bins, labels = group_names)\n# Now lets assign a labels to our quality variable\n\nlabel_quality = LabelEncoder()\n\n# Bad becomes 0 and good becomes 1\ndf['quality'] = label_quality.fit_transform(df['quality'])\nprint(df['quality'].value_counts())\nsns.countplot(df['quality'])\nplt.show()","e25efa4b":"x = df.drop(['quality'], axis=1)\ny = df['quality']","64eb46f0":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 50)","1612a19a":"sc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.fit_transform(x_test)\n\n\ncols = ['fixed acidity',\n'volatile acidity',\n'citric acid',\n'residual sugar',\n'chlorides',\n'free sulfur dioxide',\n'total sulfur dioxide',\n'density',\n'pH',\n'sulphates',\n'alcohol'\n       ]","94b98414":"dtc = DecisionTreeClassifier(max_depth=200)\ndtc.fit(x_train, y_train)\npreds = dtc.predict(x_test)\nscore = dtc.score(x_test, y_test)\nscore","8480a588":"preds[:5]","e160acca":"y_test[:5]","36bbe21c":"Ks = 100\nmean_acc = np.zeros((Ks-1))\nfor n in range(1,Ks):\n    \n    #Train Model and Predict  \n    dtc = DecisionTreeClassifier(max_depth = n).fit(x_train,y_train)\n    yhat=dtc.predict(x_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n\nmean_acc","c0010ba3":"print( \"The best accuracy was with\", mean_acc.max(), \"with depth =\", mean_acc.argmax()+1)","05a94cd9":"cf = metrics.classification_report(preds,y_test)\nprint(cf)","9da37c08":"rfc = RandomForestClassifier()\nrfc.fit(x_train, y_train)\npreds = rfc.predict(x_test)\nscore = rfc.score(x_test,y_test)\nscore","f2c8f3d1":"preds[:5]","62b7a7ec":"y_test[:5]","074abd1f":"Ks = 100\nmean_acc = np.zeros((Ks-1))\nfor n in range(1,Ks):\n    \n    #Train Model and Predict  \n    rfc = RandomForestClassifier(n_estimators = n).fit(x_train,y_train)\n    yhat=dtc.predict(x_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n\nmean_acc","f0a817d0":"print( \"The best accuracy was with\", mean_acc.max(), \"with n_estimator =\", mean_acc.argmax()+1)","4cc946ab":"cf = metrics.classification_report(preds,y_test)\nprint(cf)","cfbbdb18":"rfc_plot = metrics.plot_roc_curve(rfc, x_test,y_test)","2baf1fa4":"dtc_plot = metrics.plot_roc_curve(dtc, x_test,y_test)","dd60468d":"dtc_eval = cross_val_score(dtc, x_test, y_test, cv=10)\nprint('Cross Val Score accuracy is {:.2f}'.format(dtc_eval.mean()))","61b15aa2":"rfc_eval = cross_val_score(rfc, x_test, y_test, cv=10)\nprint('Cross Val Score accuracy is {:.2f}'.format(rfc_eval.mean()))","96ff9c6a":"tree_para = {'criterion':['gini','entropy'],'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]}\ndtc_cv = GridSearchCV(DecisionTreeClassifier(), tree_para, cv=10)\ndtc_cv.fit(x_test, y_test)","2b94712c":"dtc_cv.best_params_","092688c4":"dtc_new = DecisionTreeClassifier(criterion='entropy', max_depth = 8)\ndtc_new.fit(x_train,y_train)\nnew_score  = dtc_new.score(x_test, y_test)\nnew_score","e3c45fda":"param_grid = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\n\nrfc_cv = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=5)\nrfc_cv.fit(x_test, y_test)","1249fb27":"rfc_cv.best_params_","aed0a515":"rfc_new = RandomForestClassifier(criterion='gini', max_depth = 5, max_features='auto', n_estimators=500)\ndtc_new.fit(x_train,y_train)\nnew_score  = dtc_new.score(x_test, y_test)\nnew_score","4f42ad7b":"# Decision Tree","531b551d":"# Histogram and Density Plots of Columns\n\n**Creating certain visualizations to make understanding of the columns easier**","e41df62a":"# Introduction\n\n**Red Wine classification dataset is a publicly shared [UCI repo](https:\/\/archive.ics.uci.edu\/ml\/datasets\/wine+quality). I have used the available version in Kaggle**\n\n**The two datasets are related to red and white variants of the Portuguese \"Vinho Verde\" wine. For more details, consult: [Web Link](http:\/\/www3.dsi.uminho.pt\/pcortez\/wine\/) or the reference [Cortez et al., 2009](http:\/\/www3.dsi.uminho.pt\/pcortez\/Home.html). Due to privacy and logistic issues, only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types, wine brand, wine selling price, etc.).**\n\n**These datasets can be viewed as classification or regression tasks. The classes are ordered and not balanced (e.g. there are many more normal wines than excellent or poor ones). Outlier detection algorithms could be used to detect the few excellent or poor wines. Also, we are not sure if all input variables are relevant. So it could be interesting to test feature selection methods.**\n\n**For simplicity I aim to approach this as classification problem**\n\n\n**I have made it beginner friendly for someone who is completely new to Machine Learning, I aim to make Machine Learning approach as easier as possible for you guys, so do read and upvote it so that it can reach maximum people**\n\n![red_wine](https:\/\/archive.ics.uci.edu\/ml\/assets\/MLimages\/Large186.jpg)\n\n# Contents\n\n**Input variables (based on physicochemical tests):**\n\n> fixed acidity\n\n> volatile acidity\n\n> citric acid\n\n> residual sugar\n\n> chlorides\n\n> free sulfur dioxide\n\n> total sulfur dioxide\n\n> density\n\n> pH\n\n> sulphates\n\n> alcohol\n\n\n**Output variable (based on sensory data):**\n\n\n> quality (score between 0 and 10)\n\n**HOPE YOU ENJOY MY NOTEBOOK!!**","925c6fd1":"# Random Forest Classifier","c6309106":"# Cross Validation Score Approach\n**Let's check if our metrics get improved using this**","27257706":"# Conclusion\n\n**Using Cross_val_score and GridSearchCV can go a long way in making best scores possible for your developed model so feel free to use them as per your convinience**","d6a49a6a":"# Loading Libraries","2070d30e":"**Let's do the same for RandomForest as well**","3bd0e883":"# Classification Report","16b89fdf":"# ROC curve plot","522e2ff4":"# Model Development","100a7fc4":"**So our dataset is clean**","2e7a60f8":"# GridSearchCV\n\n**For DecisionTree**\n\n**Instead of chosing all possible hyperparameters that can improve the scores, GridSearchCV does that for you selecting the best possible parameter to get best score**","e35c3332":"# Pandas Profiling","980e42be":"**The dataset primarily contains values of float data types**","68fe7a45":"**To check the best hyperparameter**","cf2e41a4":"# Loading Dataset","8ca20dff":"**So this actually works!!**","56733865":"**Analysis**\n\n**Here we can observe positive linear correlation between the higly correlated columns, for instance `fixed acidity` and `density` columns had correlation value of 0.67 and the scatter plot shows the high correlation of it**","45e61a7b":"# Let's explore the Data","b909d2e4":"# Hyperparameter Optimization\n\n**for DecisionTreeClassifier max_depth can be adjusted to increase accuracy, similarly n_estimators can be used for RandomForestClassifiers, However these are completely optional and you may chose to skip them**","a899eb61":"**Let's look for best depth values**","b354f687":"# Scatterplot Analysis\n\n**Since we found out through correlation plots about certain columns having good correlation, let's make a scatter plot matrix that will tell us about the columns that had good correlations**","c8d38899":"# Citric Acid, Fixed Acidity and Density","5b714ac9":"# Missing Values?","c18d3040":"# Correlation","5b4f4e5a":"**What do we Understand?**\n\n**Data distribution for attribute \u201calcohol\u201d is positively skewed, for attribute \u201cdensity\u201d data quite normally distributed. Take attention to the wine quality data distribution. It\u2019s a bimodal distribution and there are more wines with average quality than wines with \u2018good\u2019 or \u2018bad\u2019 quality.**","5b091cb1":"# Decision Tree Classification Report","ce5fa94d":"**Human wine preferences scores varied from 3 to 8, so it\u2019s straightforward to categorize answers into \u2018bad\u2019 or \u2018good\u2019 quality of wines. This allows us to practice with hyperparameter tuning on e.g. decision tree algorithms. Visualizing the graph of the number of values for each category, we could see that there are far many bad answers than good ones. Of course, machine learning algorithms operate digital values, so we assign for categorizes corresponding discrete values 0 or 1.**","72f57260":"**So the ratings are 3,4,5,6,7 and 8 making only 6 values in quality column**"}}