{"cell_type":{"eb032141":"code","df3ac03e":"code","2f27933c":"code","f20dd47b":"code","7b62ec44":"code","1cf3276f":"code","9178d30a":"code","99ecb8bf":"code","dc4ec46e":"code","2ee8d818":"code","6ec2bc95":"code","a932d25f":"code","0cfe1312":"code","d6c20107":"code","f210fb55":"code","a3249223":"code","85356dcd":"code","8b6ab680":"code","f9ea64e4":"code","b656abe4":"code","a0523042":"code","746ff027":"code","a9c85a78":"code","5424fe07":"code","90db6ab7":"code","7234ccf8":"code","20399681":"code","15ea5c9e":"code","3958ac63":"code","3e506930":"code","44480d7a":"code","595a1060":"code","0bcc34c0":"code","3a9808e8":"code","d79e975a":"code","90670a0b":"code","0159d529":"code","e507bbb1":"code","6374ed4c":"code","6b2dd6f8":"code","4623d540":"code","9e0368a0":"code","f2001309":"code","dd847bea":"code","9b23db73":"markdown","159aa6e7":"markdown","c0b3ba57":"markdown","b1b52715":"markdown","49477c75":"markdown","67b49412":"markdown","aa33acb0":"markdown","83a94eef":"markdown","6967db17":"markdown"},"source":{"eb032141":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","df3ac03e":"import matplotlib.pyplot as plt\nimport seaborn as sns","2f27933c":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")","f20dd47b":"train.info()","7b62ec44":"def replace_empty(vec):\n    res = np.array([None if x == \"\" else x for x in vec])\n    return res\n\ndef pre(df):\n    \n    df = df.replace(\"\", np.nan).replace(\" \", np.nan)\n    \n    #variables and observation selected a priori to use in the model\n    if np.isin(\"Survived\", df.columns) == True:\n        df_sel = df[[\"PassengerId\", \"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]]\n        #response\n        y = df_sel[\"Survived\"]\n    else:\n        df_sel = df[[\"PassengerId\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]]\n        \n    #numeric variables\n    pass_id = df_sel[\"PassengerId\"].values.reshape((len(df_sel), 1))\n    df_numeric = df_sel[[\"Age\", \"SibSp\", \"Parch\", \"Fare\"]]\n    #categorical variables\n    df_cat = df_sel[[\"Embarked\", \"Sex\", \"Pclass\"]]\n    df_cat[\"Pclass\"] = df_cat[\"Pclass\"].astype(str)\n    df_cat_d = pd.get_dummies(df_cat, drop_first = True)\n    X = np.hstack((df_numeric.values, df_cat_d.values))\n    \n    if np.isin(\"Survived\", df.columns) == True:\n        return pass_id, X, y\n    else:\n        return pass_id, X","1cf3276f":"train_id, X_train_final, y_train_final = pre(train)\ntest_id, X_test_final = pre(test)","9178d30a":"X = np.vstack((X_train_final, X_test_final))\n\n#knn to fare without age\nX_fare = np.delete(X[np.isnan(X[:, 3]) == False, :],[0, 3], 1)\nX_fare_na = np.delete(X[np.isnan(X[:, 3]) == True, :], [0, 3],1)\n\ny_fare = X[np.isnan(X[:, 3]) == False, 3]","99ecb8bf":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\nX_train, X_test, y_train, y_test = train_test_split(X_fare, y_fare, test_size = 0.8, random_state = 28)\n\nrd = RandomForestRegressor()\ncv = GridSearchCV(\n    estimator = rd,\n    param_grid = {\"max_depth\" : [3, 5, 10, 15]},\n    cv = 30,\n    scoring = \"neg_mean_absolute_error\",\n    refit = True\n)\n\ncv.fit(X_train, y_train)\nprint(cv.best_params_)","dc4ec46e":"pre_na = cv.predict(X_fare_na)\npre_na","2ee8d818":"X[np.isnan(X[:, 3]) == True, 3] = pre_na","6ec2bc95":"#age as response variables\ny_age = X[np.isnan(X[:, 0]) == False, 0]\ny_age_na = X[np.isnan(X[:, 0]) == True, 0]\n\n#covariates\nX_age = X[np.isnan(X[:, 0]) == False, 1:]\nX_age_na = X[np.isnan(X[:, 0]) == True, 1:]","a932d25f":"X_train, y_test, y_train, y_test = train_test_split(X_age, y_age, test_size = 0.2, random_state = 28)\n\nrd = RandomForestRegressor()\ncv = GridSearchCV(\n    estimator = rd,\n    param_grid = {\"max_depth\" : [3, 5, 10, 15]},\n    cv = 30,\n    scoring = \"neg_mean_absolute_error\",\n    refit = True\n)\ncv.fit(X_train, y_train)\ncv.best_params_","0cfe1312":"pred_na = cv.predict(X_age_na)\nX[np.isnan(X[:, 0]) == True, 0] = pred_na","d6c20107":"X_train_final_nna = X[0:len(X_train_final), :]\nX_test_final_nna = X[len(X_train_final):, :]","f210fb55":"(train.shape, X_train_final_nna.shape, test.shape, X_test_final_nna.shape)","a3249223":"X_train, X_test, y_train, y_test = train_test_split(X_train_final_nna, y_train_final, \n                                                    stratify = y_train_final, \n                                                    test_size = 0.2)\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nscaler.fit(X_train)\n\n\nX_train_s = scaler.transform(X_train)\nX_test_s = scaler.transform(X_test)","85356dcd":"from sklearn.linear_model import LogisticRegression","8b6ab680":"def set_grid(est, param, cv = 20):\n    cv = GridSearchCV(\n        estimator = est,\n        param_grid = param,\n        cv = cv,\n        scoring = \"accuracy\",\n        verbose = 0\n    )\n    return cv\n\ndef print_results(model_cv, X_test_s, y_test):\n    return (model_cv.best_score_, model_cv.best_params_, model_cv.score(X_test_s, y_test))","f9ea64e4":"lg = LogisticRegression()\nparam_cv = {\"C\": [0.001, 0.005, 0.01, 0.1, 1, 5]}\n\nlg_cv = set_grid(lg, param_cv)\nlg_cv.fit(X_train_s, y_train)","b656abe4":"print_results(lg_cv, X_test_s, y_test)","a0523042":"cv_df = pd.DataFrame(lg_cv.cv_results_)\nplt.figure()\nsns.lineplot(x = \"param_C\", y = \"mean_test_score\", markers=True, dashes=False, data = cv_df)\nplt.show()","746ff027":"from sklearn.neighbors import KNeighborsClassifier\n","a9c85a78":"knn = KNeighborsClassifier()\nparam_cv = {\"n_neighbors\" : range(1, 25)}\n\nknn_cv = set_grid(knn, param_cv)\nknn_cv.fit(X_train_s, y_train)\n","5424fe07":"print_results(knn_cv, X_test_s, y_test)","90db6ab7":"from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier","7234ccf8":"rf = RandomForestClassifier(n_estimators = 500)\nparam_cv = {\"max_depth\" : [5, 7, 10], \"min_samples_split\" : [30, 40, 45]}\nrf_cv = set_grid(rf, param_cv)\nrf_cv.fit(X_train_s, y_train)","20399681":"print_results(rf_cv, X_test_s, y_test)","15ea5c9e":"gb = GradientBoostingClassifier(n_estimators = 700)\nparam_cv = {\"learning_rate\" : [0.001, 0.01, 1, 5], \"max_depth\": [3, 10, 15]}\ngb_cv = set_grid(gb, param_cv)\ngb_cv.fit(X_train_s, y_train)","3958ac63":"print_results(gb_cv, X_test_s, y_test)","3e506930":"from sklearn.svm import SVC","44480d7a":"svc_linear = SVC(kernel = \"linear\")\nparam_cv = {\"C\": [0.001, 0.01, 1, 3]}\nsvc_linear_cv = set_grid(svc_linear, param_cv)\nsvc_linear_cv.fit(X_train_s, y_train)","595a1060":"print_results(svc_linear_cv, X_test_s, y_test)","0bcc34c0":"svc_rb = SVC(kernel = \"rbf\")\nparam_cv = {\"C\" : [10, 20, 30, 40], \"gamma\" : [0.1, 0.5, 1, 5, 10]}\nsvc_rb_cv = set_grid(svc_rb, param_cv)\nsvc_rb_cv.fit(X_train_s, y_train)","3a9808e8":"print_results(svc_rb_cv, X_test_s, y_test)","d79e975a":"from sklearn.neural_network import MLPClassifier","90670a0b":"nn = MLPClassifier(hidden_layer_sizes = (20, 20, 20, 20), activation = \"relu\", max_iter = 1000, random_state = 28)\n\nparam = {\"alpha\": [0.001, 0.1, 1, 10]}\n\nnn_cv = set_grid(nn, param, cv = 5)\nnn_cv.fit(X_train_s, y_train)","0159d529":"print_results(nn_cv, X_test_s, y_test)","e507bbb1":"from sklearn.ensemble import VotingClassifier\n\nclassifiers = [\n    ('svc_rbf', SVC(kernel = \"rbf\", C = 20, gamma = 0.5)),\n    ('gb', GradientBoostingClassifier(learning_rate = 0.01, max_depth = 10)),\n    ('rv', RandomForestClassifier(n_estimators = 700, max_depth = 7, min_samples_split = 30)),\n    ('lg', LogisticRegression())\n]\n\nclf = VotingClassifier(classifiers, voting = \"hard\")","6374ed4c":"clf.fit(X_train_s, y_train)\nclf.score(X_test_s, y_test)","6b2dd6f8":"scaler = MinMaxScaler()\nscaler.fit(X_train_final_nna)\n\nX_train_s = scaler.transform(X_train_final_nna)\nX_test_s = scaler.transform(X_test_final_nna)","4623d540":"clf.fit(X_train_s, y_train_final)","9e0368a0":"pred = clf.predict(X_test_s)","f2001309":"final = pd.DataFrame({\"PassengerID\" : np.array(test_id).reshape((len(test_id),)), \"Survived\" : pred})","dd847bea":"final.to_csv(\"\/kaggle\/working\/sub.csv\", index = False)","9b23db73":"## SVM","159aa6e7":"### Logistic Regression","c0b3ba57":"### Inputing NA's in Age and Fare for train and test samples","b1b52715":"## Final prediction","49477c75":"## Gradient Boosting","67b49412":"## Combining","aa33acb0":"## NN","83a94eef":"## Random Forests","6967db17":"### KNN"}}