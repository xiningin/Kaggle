{"cell_type":{"feb61d99":"code","6a18f867":"code","bd7944d7":"code","143e9e91":"code","f7c35852":"code","1d79b2ff":"code","5f7bc241":"code","5adbede5":"code","8998a8cd":"code","c5f93482":"code","dcaa38b7":"code","0afad5f3":"code","3e1f84c8":"code","813acd11":"code","e8b4bd2f":"code","d517a06e":"code","21245a1b":"code","5e8903d9":"code","074de1eb":"code","a71ecd8f":"code","78af2586":"code","61e75d83":"code","ffbe41a6":"code","1901fe4a":"code","98336969":"code","066d637b":"code","9780f93e":"code","0b85f2f7":"code","bf2e9bc7":"code","fffa1d3a":"code","0cea29e5":"code","02a6a91b":"code","c00acc9b":"code","6dba6155":"code","790bea59":"code","9800b426":"code","4d4b4f01":"code","7a34d43f":"code","a90d042e":"code","8e207c5d":"code","a75ddd39":"code","809ff67e":"code","e607fa34":"code","85693aa6":"code","9d995986":"code","27570118":"markdown","47ba15bf":"markdown","7033e467":"markdown","6c234b98":"markdown","2d6b5c92":"markdown","19e02c46":"markdown","0e8ce8bb":"markdown","26746889":"markdown","d902ecb1":"markdown","72084a4d":"markdown","b36b616f":"markdown","e9febbbf":"markdown","b508114a":"markdown","5d38c3ad":"markdown","f2866263":"markdown","4b114f79":"markdown","7d1bfab6":"markdown","0f319b2e":"markdown","48362025":"markdown","04909be1":"markdown","f1fe7cd9":"markdown","41d53e2e":"markdown"},"source":{"feb61d99":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/flowers-recognition\"))\n\n# Any results you write to the current directory are saved as output.","6a18f867":"data = \"..\/input\/flowers-recognition\/flowers\/flowers\"\nfolders = os.listdir(data)\nprint(folders)","bd7944d7":"\nimport cv2\nfrom tqdm import tqdm\nimage_names = []\ntrain_labels = []\ntrain_images = []\n\nsize = 120,120\n\nfor folder in folders:\n    for file in tqdm(os.listdir(os.path.join(data,folder))):\n        if file.endswith(\"jpg\"):\n            image_names.append(os.path.join(data,folder,file))\n            train_labels.append(folder)\n            img = cv2.imread(os.path.join(data,folder,file))\n            im = cv2.resize(img,size)\n            train_images.append(im)\n        else:\n            continue","143e9e91":"\n\nX1 = np.array(train_images)\n\nX1.shape","f7c35852":"from sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical\n\nle=LabelEncoder()\nY=le.fit_transform(train_labels)\nY=to_categorical(Y,5)\nX=np.array(X1)\nX=X\/255","1d79b2ff":"print(X.shape)\nprint(Y.shape)","5f7bc241":"from sklearn.model_selection import train_test_split\nX_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 0.20, random_state=42)\nprint(\"x_train shape\",X_train.shape)\nprint(\"x_test shape\",X_val.shape)\nprint(\"y_train shape\",Y_train.shape)\nprint(\"y_test shape\",Y_val.shape)","5adbede5":"\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfor i in range(10,100,20):\n    plt.imshow(X_train[i][:,:,0],cmap='gray')\n    plt.show()","8998a8cd":"del X,Y","c5f93482":"from sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, AveragePooling2D\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters =96, kernel_size = (3,3),padding = 'valid', \n                 activation ='relu', input_shape = (120,120,3)))\nmodel.add(AveragePooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(filters =128, kernel_size = (3,3),padding = 'valid', \n                 activation ='relu'))\nmodel.add(AveragePooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2)) \n\nmodel.add(Conv2D(filters = 160, kernel_size = (3,3),padding = 'valid',\n                 activation ='relu'))\nmodel.add(AveragePooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(filters = 192, kernel_size = (3,3),padding = 'valid',\n                 activation ='relu'))\nmodel.add(AveragePooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2)) \n\nmodel.add(Conv2D(filters = 256, kernel_size = (3,3),padding = 'valid',\n                 activation ='relu'))\nmodel.add(AveragePooling2D(pool_size=(2,2)))\nmodel.add(Dropout(0.2)) \n\nmodel.add(Flatten())\nmodel.add(Dense(512, activation = \"relu\"))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(5, activation = \"softmax\"))","dcaa38b7":"optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)","0afad5f3":"\nmodel.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","3e1f84c8":"epochs = 50\nbatch_size = 256","813acd11":"\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  \n        samplewise_center=False,  \n        featurewise_std_normalization=False,  \n        samplewise_std_normalization=False,  \n        zca_whitening=False, \n        rotation_range=10, \n        zoom_range = 0.8,\n        width_shift_range=0.8,  \n        height_shift_range=0.8,  \n        horizontal_flip=False,  \n        vertical_flip=False)  \n\ndatagen.fit(X_train) \n","e8b4bd2f":"model.summary()","d517a06e":"%%time\nhistory = model.fit(X_train,Y_train, batch_size=batch_size, epochs = epochs, validation_data = (X_val,Y_val))","21245a1b":"import keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.models import model_from_json\n\n\nfrom keras import layers\nfrom keras import models\n\n\n\ndef guardarRNN(model,nombreArchivoModelo,nombreArchivoPesos):\n    print(\"Guardando Red Neuronal en Archivo\")  \n    # serializar modelo a JSON\n\n    # Guardar los Pesos (weights)\n    model.save_weights(nombreArchivoPesos+'.h5')\n\n    # Guardar la Arquitectura del modelo\n    with open(nombreArchivoModelo+'.json', 'w') as f:\n        f.write(model.to_json())\n\n    print(\"Red Neuronal Grabada en Archivo\")   \n    \ndef cargarRNN(nombreArchivoModelo,nombreArchivoPesos):\n        \n    # Cargar la Arquitectura desde el archivo JSON\n    with open(nombreArchivoModelo+'.json', 'r') as f:\n        model = model_from_json(f.read())\n\n    # Cargar Pesos (weights) en el nuevo modelo\n    model.load_weights(nombreArchivoPesos+'.h5')  \n\n    print(\"Red Neuronal Cargada desde Archivo\") \n    return model","5e8903d9":"\nmodel.fit(X_train, Y_train, epochs=100, batch_size=64, verbose=0)\nmodel.summary()\nprint('Resultado en Train:')\nscore = model.evaluate(X_train, Y_train, verbose=0)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n\n#Fase de Testing\nprint('Resultado en Test:')\nscore = model.evaluate(X_val, Y_val, verbose=0)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))\n\nnombreArchivoModelo='arquitectura_base'\nnombreArchivoPesos='pesos_base'\nguardarRNN(model,nombreArchivoModelo,nombreArchivoPesos)","074de1eb":"X=[]\nZ=[]\nIMG_SIZE=150\nFLOWER_DAISY_DIR='..\/input\/flowers-recognition\/flowers\/daisy'\nFLOWER_SUNFLOWER_DIR='..\/input\/flowers-recognition\/flowers\/sunflower'\nFLOWER_TULIP_DIR='..\/input\/flowers-recognition\/flowers\/tulip'\nFLOWER_DANDI_DIR='..\/input\/flowers-recognition\/flowers\/dandelion'\nFLOWER_ROSE_DIR='..\/input\/flowers-recognition\/flowers\/rose'","a71ecd8f":"def assign_label(img,flower_type):\n    return flower_type","78af2586":"#\ndef make_train_data(flower_type,DIR):\n    for img in tqdm(os.listdir(DIR)):\n        label=assign_label(img,flower_type)\n        path = os.path.join(DIR,img)\n        img = cv2.imread(path,cv2.IMREAD_COLOR)\n        img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n        \n        X.append(np.array(img))\n        Z.append(str(label))","61e75d83":"make_train_data('Daisy',FLOWER_DAISY_DIR)\nprint(len(X))","ffbe41a6":"make_train_data('Sunflower',FLOWER_SUNFLOWER_DIR)\nprint(len(X))","1901fe4a":"make_train_data('Tulip',FLOWER_TULIP_DIR)\nprint(len(X))","98336969":"make_train_data('Rose',FLOWER_ROSE_DIR)\nprint(len(X))","066d637b":"fig,ax=plt.subplots(5,2)\nfig.set_size_inches(15,15)\nfor i in range(5):\n    for j in range (2):\n        l=rn.randint(0,len(Z))\n        ax[i,j].imshow(X[l])\n        ax[i,j].set_title('Flower: '+Z[l])\n        \nplt.tight_layout()","9780f93e":"le=LabelEncoder()\nY=le.fit_transform(Z)\nY=to_categorical(Y,5)\nX=np.array(X)\nX=X\/255","0b85f2f7":"import keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.models import model_from_json\n\nfrom keras.applications.resnet import ResNet50\nfrom keras import layers\nfrom keras import models\n\n\n\ndef guardarRNN(model,nombreArchivoModelo,nombreArchivoPesos):\n    print(\"Guardando Red Neuronal en Archivo\")  \n    # serializar modelo a JSON\n\n    # Guardar los Pesos (weights)\n    model.save_weights(nombreArchivoPesos+'.h5')\n\n    # Guardar la Arquitectura del modelo\n    with open(nombreArchivoModelo+'.json', 'w') as f:\n        f.write(model.to_json())\n\n    print(\"Red Neuronal Grabada en Archivo\")   \n    \ndef cargarRNN(nombreArchivoModelo,nombreArchivoPesos):\n        \n    # Cargar la Arquitectura desde el archivo JSON\n    with open(nombreArchivoModelo+'.json', 'r') as f:\n        model = model_from_json(f.read())\n\n    # Cargar Pesos (weights) en el nuevo modelo\n    model.load_weights(nombreArchivoPesos+'.h5')  \n\n    print(\"Red Neuronal Cargada desde Archivo\") \n    return model","bf2e9bc7":"x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.25,random_state=42)","fffa1d3a":"np.random.seed(42)\nrn.seed(42)\ntf.random.set_seed(42)","0cea29e5":"model = Sequential()\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same',activation ='relu', input_shape = (150,150,3)))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same',activation ='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n \n\nmodel.add(Conv2D(filters =96, kernel_size = (3,3),padding = 'Same',activation ='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n\nmodel.add(Conv2D(filters = 96, kernel_size = (3,3),padding = 'Same',activation ='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dense(5, activation = \"softmax\"))","02a6a91b":"batch_size=128\nepochs=50\n\nfrom keras.callbacks import ReduceLROnPlateau\nred_lr= ReduceLROnPlateau(monitor='val_acc',patience=3,verbose=1,factor=0.1)","c00acc9b":"datagen = ImageDataGenerator(\n        featurewise_center=False, # establece la media de entrada a 0 sobre el conjunto de datos\n        samplewise_center=False,  # establece cada media de muestra en 0\n        featurewise_std_normalization=False, # dividir entradas por est\u00e1ndar del conjunto de datos \n        samplewise_std_normalization=False, # divide cada entrada por su est\u00e1ndar\n        zca_whitening=False,  # aplicar blanqueamiento ZCA\n        rotation_range=10,  # rotar im\u00e1genes al azar en el rango (grados, 0 a 180)\n        zoom_range = 0.1, # Ampliar aleatoriamente la imagen\n        width_shift_range=0.2, # mover im\u00e1genes al azar horizontalmente (fracci\u00f3n del ancho total) \n        height_shift_range=0.2, # mover im\u00e1genes al azar verticalmente (fracci\u00f3n de la altura total)\n        horizontal_flip=True, # voltear im\u00e1genes al azar\n        vertical_flip=False) # voltear im\u00e1genes al azar \n\n\ndatagen.fit(x_train)","6dba6155":"model.compile(optimizer=Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])","790bea59":"model.summary()","9800b426":"History = model.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (x_test,y_test),\n                              verbose = 1, steps_per_epoch=x_train.shape[0] \/\/ batch_size)\nmodel.fit(x_train,y_train,epochs=epochs,batch_size=batch_size,validation_data = (x_test,y_test))","4d4b4f01":"#Cargar pesos y la arquitectura\nmodel2=cargarRNN(nombreArchivoModelo,nombreArchivoPesos) \n\nmodel2.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['acc']) #ADADELTA: An Adaptive Learning Rate Method\nscore = model2.evaluate(x_train, y_train, verbose=0)\nprint('Resultado en Train:')\nprint(\"%s: %.2f%%\" % (model2.metrics_names[1], score[1]*100))\n\n#Fase de Testing\nprint('Resultado en Test:')\nscore = model2.evaluate(x_test, y_test, verbose=0)\nprint(\"%s: %.2f%%\" % (model2.metrics_names[1], score[1]*100))\n\n#Guardamos los archivos de modelo de pruebas\nnombreArchivoModelo='arquitectura_prueba'\nnombreArchivoPesos='pesos_prueba'\nguardarRNN(model,nombreArchivoModelo,nombreArchivoPesos)","7a34d43f":"plt.plot(History.history['loss'])\nplt.plot(History.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend(['train', 'test'])\nplt.show()","a90d042e":"plt.plot(History.history['accuracy'])\nplt.plot(History.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['train', 'test'])\nplt.show()","8e207c5d":"# obteniendo predicciones sobre el conjunto de valores.\npred=model.predict(x_test)\npred_digits=np.argmax(pred,axis=1)","a75ddd39":"# ahora almacena algunos \u00edndices correctamente y mal clasificados '.\ni=0\nprop_class=[]\nmis_class=[]\n\nfor i in range(len(y_test)):\n    if(np.argmax(y_test[i])==pred_digits[i]):\n        prop_class.append(i)\n    if(len(prop_class)==8):\n        break\n\ni=0\nfor i in range(len(y_test)):\n    if(not np.argmax(y_test[i])==pred_digits[i]):\n        mis_class.append(i)\n    if(len(mis_class)==8):\n        break","809ff67e":"warnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\ncount=0\nfig,ax=plt.subplots(4,2)\nfig.set_size_inches(15,15)\nfor i in range (4):\n    for j in range (2):\n        ax[i,j].imshow(x_test[prop_class[count]])\n        ax[i,j].set_title(\"Predicted Flower :\"+str(le.inverse_transform([pred_digits[prop_class[count]]]))+\"\\n\"+\"Actual Flower : \"+str(le.inverse_transform([np.argmax(y_test[prop_class[count]])])))\n        plt.tight_layout()\n        count+=1","e607fa34":"warnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\ncount=0\nfig,ax=plt.subplots(4,2)\nfig.set_size_inches(15,15)\nfor i in range (4):\n    for j in range (2):\n        ax[i,j].imshow(x_test[mis_class[count]])\n        ax[i,j].set_title(\"Predicted Flower :\"+str(le.inverse_transform([pred_digits[mis_class[count]]]))+\"\\n\"+\"Actual Flower : \"+str(le.inverse_transform([np.argmax(y_test[mis_class[count]])])))\n        plt.tight_layout()\n        count+=1","85693aa6":"%%time\nhistory = model.fit(x_train,y_train, batch_size=batch_size, epochs = epochs, validation_data = (x_test,y_test))","9d995986":"#1. Compilaci\u00f3n: Prueba de mejores par\u00e1metros batch_size, epochs y optimizer\n#Esto recomiendo probarlo con Google Colab, puesto que se necesita 16GB en RAM y puede llegar a tardar unos 30min.\n\ndef build_model(optimizer):\n    model = Sequential()\n    model.add(Dense(32, input_shape=(x_train.shape[1],), activation='relu'))\n    model.add(Dropout(0.1))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(0.1))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['acc'])\n    return model\n\n\n#par\u00e1metros que queremos probar, y sus valores \n#probaremos con batch_size, epochs, y optimizador, con el fin de encontrar la mejor combinaci\u00f3n entre estos tres par\u00e1metros.\nparameters = parameters = {'batch_size': [16,32],\n             'epochs':[100,500],\n             'optimizer': ['adadelta', 'rmsprop']}\n\nestimator = KerasClassifier(build_fn=build_model, verbose=0)\n#Ahora no le pasamos los par\u00e1metros al KerasClasifier, porque se los pasaremos a trav\u00e9s de GridSearchCV\n#el argumento verbose=0 es para que no muestre salida, si lo dejamos en cero, no mostrar\u00e1 la barra de progreso del entrenamiento\n#GridSearchCV: recibe como par\u00e1metros nuestro modelo, nuestros par\u00e1metros, la medida sobre la que queremos comparar, y la \n#cantidad de veces que lo entrenar\u00e1 para sacar la media de accuracy.\ngrid_search = GridSearchCV(estimator=estimator, param_grid=parameters, scoring='accuracy', cv=10,n_jobs=-1)\ngrid_search.fit(x_train, y_train)\n#grid_search.best_params_\nprint(grid_search.best_params_)\n#Un ejemplo de resultados es: {'batch_size': 16, 'epochs': 100, 'optimizer': 'rmsprop'}\n#Esto indica que el optimizador \"adadelta\" no es adecuado. Y es que este optimizador NO sirve para este tipo de problemas.","27570118":"# CONCLUSIONES","47ba15bf":"Aumento de datos para evitar el sobreajuste","7033e467":"> Nuestro objetivo es llegar a la clasificacion de flores respecto a un conjunto de datos de imagenes de 5 tipos de flores, y a base de entranamiento, que el sistema llegue a predecir el tipo de flor de las imagenes que se le presenten.","6c234b98":"IMPORTACION DEL MODELO****","2d6b5c92":"Divisi\u00f3n en conjuntos de entrenamiento y validaci\u00f3n\n","19e02c46":"Etiqueta de codificaci\u00f3n de la matriz Y (es decir, Daisy-> 0, Rose-> 1, etc.) y luego una codificaci\u00f3n ","0e8ce8bb":"Hacer las funciones para obtener el conjunto de capacitaci\u00f3n y validaci\u00f3n de las im\u00e1genes","26746889":"# Evaluaci\u00f3n del desempe\u00f1o del modelo****","d902ecb1":"Construyendo el modelo ConvNet","72084a4d":"Compilaci\u00f3n del modelo y resumen de Keras","b36b616f":"Encajar en el conjunto de entrenamiento y hacer predicciones en el conjunto de validaci\u00f3n","e9febbbf":"IM\u00c1GENES DESCLASIFICADAS DE FLORES","b508114a":"Usando un recortador LR","5d38c3ad":"# MODELADO****","f2866263":"# Visualizaci\u00f3n de predicciones en el conjunto de validaci\u00f3n****","4b114f79":"IM\u00c1GENES DE FLOR CORRECTAMENTE CLASIFICADAS","7d1bfab6":"# PREPARACION DE LOS DATOS****","0f319b2e":"Visualizando algunas im\u00e1genes aleatorias","48362025":"Establecer las semillas aleatorias","04909be1":"Existen 5 tipos de flores:\n* daisy\n* sunflower\n* tulip\n* rose\n* dandelion'","f1fe7cd9":"# NOMBRE DE TIPOS DE FLORES","41d53e2e":"* El resultado del accuaricy en del 99.88%, por tanto el modelo de prediccion de CNN es bastante efectivo para la prediccion de tipos de flores\n* Como conclusi\u00f3n obtenemos que CNN llega a ser una poderosa herramienta de inteligencia artificial en las clasificaciones de patrones. En esta practica, nos propusimos una arquitectura CNN para poder llegar a clasificar las diferentes clases de im\u00e1genes de flores que existen en nuestro Data Set. En la arquitectura llega a estar dise\u00f1ada con cuatro capas convolucionales. Cada capa utiliza diferentes tama\u00f1os de ventana de filtrado, esto llega a mejorar la velocidad y as\u00ed de la misma manera llega a mejorar la precisi\u00f3n en el reconocimiento de las im\u00e1genes."}}