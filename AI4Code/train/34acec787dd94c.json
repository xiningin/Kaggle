{"cell_type":{"c56fa1b7":"code","a114949b":"code","da5fd815":"code","730712b8":"code","b33aa624":"code","651898f0":"code","3e6fb7bc":"code","33867953":"code","436a2d02":"code","c7ab7dc1":"code","fc668baf":"code","77f9fc6b":"code","b09faf2e":"code","764205a1":"code","27fcfa00":"code","dfd3ca4f":"code","4378926c":"code","5b678d43":"code","e498d25c":"code","aecef2c1":"code","1e4c6bfd":"code","b84bda7a":"code","bb4d699e":"code","d5647643":"code","fe95597e":"code","8f58beb3":"code","5825f6b8":"code","3407f61d":"code","5d2b5f41":"code","5d9bd048":"code","d42a3ab2":"code","2751fc35":"code","28a9e60f":"markdown","170cca89":"markdown","856dec45":"markdown","b5c01d98":"markdown","6a686781":"markdown","a17fff4d":"markdown","3b475204":"markdown"},"source":{"c56fa1b7":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a114949b":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns","da5fd815":"train =  pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest  =  pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","730712b8":"train.head()","b33aa624":"train.shape","651898f0":"train.info()","3e6fb7bc":"men = train[train['Sex'] == 'male']['Survived']\nrate_men = 100 * (sum(men)\/len(men))\nwomen = train.loc[train.Sex == 'female'][\"Survived\"]\nrate_women = 100 * (sum(women)\/len(women))\nprint('The Percentage of Women who survived :',rate_women,'%')\nprint('The Percentage of Men who survived :',rate_men,'%')","33867953":"100 * train.corr()['Survived'].sort_values()","436a2d02":"plt.figure(figsize=(10,10))\nsns.heatmap(data = train.corr(),annot=True)\nplt.show()","c7ab7dc1":"Pclass=['class1','class2','class3']\nax=sns.countplot(data=train,x='Pclass',hue='Survived')\nplt.xticks(ticks = [0,1,2], labels = Pclass)\nplt.legend(['Not Survived', 'Survived'])\nplt.show()","fc668baf":"def plot(df,col):\n    plt.figure(figsize=(4,5))\n    sns.countplot(data=df,x=col,hue='Survived')\n    plt.legend(['Not Survived', 'Survived'])\n    plt.show()\n    \ncols = ['Sex','SibSp','Parch','Embarked']\nfor x in cols:\n    plot(train,x)","77f9fc6b":"plt.figure(figsize=(4,5))\nsns.countplot(x=train['Survived'],hue=pd.cut(train['Age'],5))","b09faf2e":"train.Fare","764205a1":"train['Fare_category'] = pd.cut(train['Fare'],bins=[0,7,14,31,120], labels=['Low','Mid','High_Mid','High'])\ntrain","27fcfa00":"plt.figure(figsize=(4,5))\nsns.countplot(data=train,x='Fare_category',hue='Survived')\nplt.legend(['Not Survived', 'Survived'])\nplt.show()","dfd3ca4f":"train[\"Age\"] = train[\"Age\"].fillna(-1)","4378926c":"dataset = [train,test]\nfor data in dataset:\n    mean = data['Age'].mean()\n    std = data['Age'].std()\n    is_null = data['Age'].isnull().sum()\n    #Compute random values between mean , std and nan values\n    random_age = np.random.randint(mean-std,mean+std,size=is_null)\n    #Fill nan values\n    age_slice = data[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = random_age\n    data[\"Age\"] = age_slice\n    data[\"Age\"] = data[\"Age\"].astype(int)","5b678d43":"dataset = [train,test]\nembarked = train['Embarked'].mode()\nfor data in dataset:\n    data['Embarked'] = data['Embarked'].fillna(embarked)","e498d25c":"train.isnull().sum()","aecef2c1":"features = ['Pclass','Sex','SibSp','Parch']\nX = pd.get_dummies(train[features])\ny = train['Survived']","1e4c6bfd":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)","b84bda7a":"def models(X_train,y_train):\n    \n    # LogisticRegression\n    from sklearn.linear_model import LogisticRegression\n    log = LogisticRegression()\n    log.fit(X_train,y_train)\n    \n    # DecisionTree\n    from sklearn.tree import DecisionTreeClassifier\n    tree = DecisionTreeClassifier()\n    tree.fit(X_train,y_train)\n    \n    # Random Forest\n    from sklearn.ensemble import RandomForestClassifier\n    rnd = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=2)\n    rnd.fit(X_train,y_train)\n    \n    # Gradient Boost\n    from sklearn.ensemble import GradientBoostingClassifier\n    gb = GradientBoostingClassifier()\n    gb.fit(X_train,y_train)\n    \n    # K-Neighbors\n    from sklearn.neighbors import KNeighborsClassifier\n    knn = KNeighborsClassifier(n_neighbors=25)\n    knn.fit(X_train,y_train)\n \n    \n    #Print the model accuracy of training data\n    print('Logistic Regression Training Accuracy : ',log.score(X_train, y_train))\n    print('Decision Tree Training Accuracy : ',tree.score(X_train, y_train))\n    print('Random Forest Training Accuracy : ',rnd.score(X_train, y_train))\n    print('Gradient Boosting Classifier Training Accuracy : ',gb.score(X_train, y_train))\n    print('K-Neighbors Classifier Training Accuracy : ',knn.score(X_train, y_train))\n\n    \n    return log,tree,rnd,gb,knn\n\n#Get the model\nmodel = models(X_train,y_train)\n\n    ","bb4d699e":"#Test model accuracy on test data using confusion matrix and accuracy score\nfrom sklearn.metrics import confusion_matrix , accuracy_score\nfor x in range(len(model)):\n    print('Model :',model[x])\n    cm = confusion_matrix(y_test,model[x].predict(X_test))\n    print(cm)\n    print('Accuracy ',accuracy_score(y_test,model[x].predict(X_test)))\n    print('\\n\\n')","d5647643":"#Another way to get matrix of the models\nfrom sklearn.metrics import classification_report\nfor i in range (len(model)):\n    print('Model :',model[i])\n    print(classification_report(y_test,model[i].predict(X_test)))\n    print('\\n\\n')","fe95597e":"test.head()","8f58beb3":"id = test['PassengerId']","5825f6b8":"test.isnull().sum()","3407f61d":"features = ['Pclass','Sex','SibSp','Parch']\nX_test = pd.get_dummies(test[features])","5d2b5f41":"model[2]","5d9bd048":"prediction = model[2].predict(X_test)","d42a3ab2":"prediction","2751fc35":"submission = pd.DataFrame({'PassengerId':id,'Survived':prediction})\nsubmission.to_csv('submission.csv', index=False)","28a9e60f":"## Selection Model","170cca89":"# Submission","856dec45":"# Import the Libraries ","b5c01d98":"# Import the Dataset and Overview","6a686781":"\nIn the end, we find that ``RandomForestClassifier`` is more accurate than the other","a17fff4d":"# Data Preparation","3b475204":"# Modeling"}}