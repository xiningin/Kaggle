{"cell_type":{"f9494280":"code","6c1024dd":"code","799de1b7":"code","4d6c2d3c":"code","88fe568b":"code","71dc24f0":"code","c5d74635":"code","d201baab":"code","f575ee52":"code","764a1fef":"code","64f0b30a":"code","f0658d77":"code","19b38566":"markdown","8104620f":"markdown","f6cbd61a":"markdown","555dd1d3":"markdown","18b570f5":"markdown","dcc34795":"markdown","859628d1":"markdown","6310e1b7":"markdown"},"source":{"f9494280":"# Will import libraries\nimport numpy as np\nimport pandas as pd\nimport scipy.optimize as sp\nimport xgboost as xgb\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import linear_model, model_selection, metrics, tree, ensemble ","6c1024dd":"#Reading data\ndata = pd.read_csv('..\/input\/just-the-basics-the-after-party\/train.csv')\ndataT = pd.read_csv('..\/input\/just-the-basics-the-after-party\/test.csv')\ny = pd.read_csv('..\/input\/just-the-basics-the-after-party\/train_labels.csv')\ndata.head()","799de1b7":"#Since the dataset has no headers, let's name the columns for further incrimination. \ncolums = list((range(0,100)))\ndata.columns = [colums]\ndataT.columns = [colums]\ndata.info()","4d6c2d3c":"#And let's fill in the missing values with the median\nfor i in colums:\n    data[i,].fillna(data[i,].median(), inplace = True)\n\nfor i in colums:\n    dataT[i,].fillna(dataT[i,].median(), inplace = True)\ndata.info()","88fe568b":"#Let's bring y to the required shape\n\ny_train = np.ravel(y)\nprint(y.shape,type(y), y_train.shape, type(y_train))\n\n#Data is full, no need delete outliers (NEED MORE Explanations)\nX_train = data\nX_test = dataT","71dc24f0":"#For penalty will use Lasso 'l1'. Tune 'C' parameter\nparam_grid = {'C': [0.01, 0.05, 0.1, 0.5, 1, 5, 10]}\n\nestimator = linear_model.LogisticRegression(solver='liblinear', penalty = 'l1', random_state = 1)\noptimizerL = GridSearchCV(estimator, param_grid, scoring = 'roc_auc',cv = 3)                    \noptimizerL.fit(X_train, y_train)\n\nprint('score_train_opt', optimizerL.best_score_)\nprint('param_opt', optimizerL.best_params_)","c5d74635":"param_grid = {'alpha': [0.01, 0.05, 0.1, 0.5, 1, 2, 5]}\n\nestimator = linear_model.RidgeClassifier( random_state = 1)\noptimizerR = GridSearchCV(estimator, param_grid,  scoring = 'roc_auc',cv = 3)                    \noptimizerR.fit(X_train, y_train)\n\nprint('score_train_opt', optimizerR.best_score_)\nprint('param_opt', optimizerR.best_params_)","d201baab":"rf_class = ensemble.RandomForestClassifier(random_state = 1)\ntrain_scores, test_scores = model_selection.validation_curve(rf_class, X_train, y_train, 'max_depth', list(range(1, 11)), cv=3, scoring='roc_auc')\nprint('max_depth=', list(range(1, 10)))\nprint(train_scores.mean(axis = 1))\nprint(test_scores.mean(axis = 1))","f575ee52":"param_grid = {'n_estimators': list(range(20, 100, 5)), 'min_weight_fraction_leaf': [0.001,  0.005, 0.01, 0.05, 0.1, 0.5] } \n\nestimator = ensemble.RandomForestClassifier(max_depth=4, random_state = 1)\noptimizerRF = GridSearchCV(estimator, param_grid, scoring = 'roc_auc',cv = 3)                    \noptimizerRF.fit(X_train, y_train)\n\nprint('score_train_opt', optimizerRF.best_score_)\nprint('param_opt', optimizerRF.best_params_)","764a1fef":"param_grid = {'max_depth': list(range(1, 7)), 'learning_rate': [0.01, 0.05, 0.1, 0.5, 1, 1.5], 'n_estimators': list(range(10, 100, 5)) }\nestimator = xgb.XGBClassifier( random_state = 1, min_child_weight=3)\noptimizer = GridSearchCV(estimator, param_grid, scoring = 'roc_auc',cv = 3)                    \noptimizer.fit(X_train, y_train)\n\nprint('score_train_opt', optimizer.best_score_)\nprint('param_opt', optimizer.best_params_)","64f0b30a":"param_grid = {'n_estimators': list(range(10, 100, 5)), 'min_child_weight': list(range(1, 10)) }\nestimator = xgb.XGBClassifier( max_depth = 3, random_state = 1, learning_rate=0.1)\noptimizer = GridSearchCV(estimator, param_grid, scoring = 'roc_auc',cv = 3)                    \noptimizer.fit(X_train, y_train)\n\nprint('score_train_opt', optimizer.best_score_)\nprint('param_opt', optimizer.best_params_) ","f0658d77":"#Writting answers\n\nans=optimizerRF.predict(X_test)\n\nf=open(\"\/kaggle\/working\/answers.csv\", \"w\")\nf.write(str(ans))\nf.close()\n","19b38566":"## Modeling\n### Will tune hyperparameters using GridSearchCV. For scoring will use area under the ROC curve: 'roc_auc'.","8104620f":"We get the same difference between train and test scores on by  max_depth=4-9\nAnd we have the bigger score ROC AUC by max_depth=4","f6cbd61a":"### LogisticRegression","555dd1d3":"Will use the highest value ROC AUC model - RandomForestClassifier\n","18b570f5":"### Extreme Gradient Boosting","dcc34795":"## SPAM detection task\nThe data contains 100 features extracted from a corpus of emails. Some of the emails are spam and some are normal. The task is to make a spam detector. \ntrain.csv - contains 600 emails x 100 features for use training model(s)\ntrain_labels.csv - contains labels for the 600 training emails (1 = spam, 0 = normal)\ntest.csv - contains 4000 emails x 100 features. Need to detect the spam on them.\n\nPredictions can be continuous numbers or 0\/1 labels. No header is necessary. Submissions are judged on area under the ROC curve. ","859628d1":"### RidgeClassifier","6310e1b7":"### RandomForestClassifier\nWe should have a loose stopping criterion and then use pruning to remove branches that contribute to overfitting. But pruning is a tradeoff between accuracy and generalizability, so our train scores might lower but the difference between train and test scores will also get lower.  This is what we need.  (details - https:\/\/towardsdatascience.com\/how-to-tune-a-decision-tree-f03721801680)"}}