{"cell_type":{"ce402986":"code","974282c4":"code","cb1f5354":"code","c2cc1511":"code","05bb6966":"code","c7372daa":"code","8e92037a":"code","67f8c6be":"code","83207ed4":"code","6cca9d00":"code","9f46b29f":"code","7b2e742b":"code","30a66585":"code","444f2a98":"code","81409150":"code","9faa8875":"code","5844e6a2":"code","cbfa8451":"code","af1e0b44":"code","b64bb1b8":"code","1781ebd2":"code","65253331":"code","1f8891dc":"code","5ff802cc":"code","060e7ded":"code","2ec9ff75":"code","27e8c03f":"code","18fc3c48":"code","27e1927f":"code","cc80a1d1":"code","ea64d971":"code","eba2aeac":"code","df0f07dc":"code","442a5d2d":"code","6db6d246":"code","1cd2c224":"code","1ed31ec6":"code","909e58ed":"code","833ab3f8":"code","bfa5a1ca":"code","0ba9896e":"code","de330384":"code","705cc7ff":"code","aa3dfce3":"code","7b5fe883":"markdown","4b124b25":"markdown","75b54631":"markdown","3668b8e7":"markdown","a4a3cd4e":"markdown","d026f747":"markdown","21d21f6b":"markdown","dc1c2f14":"markdown","fda996a0":"markdown","058bcc73":"markdown","24636623":"markdown","c5563b8a":"markdown","85babe26":"markdown","8cfc5e21":"markdown","c7bec207":"markdown","294504c3":"markdown","d6a0bbd4":"markdown","76bc9295":"markdown","8b908fb6":"markdown","265c83f9":"markdown"},"source":{"ce402986":"!pip3 install ..\/input\/datasketch\/datasketch-1.5.3-py2.py3-none-any.whl","974282c4":"import os\nimport re\nimport json\nimport gc\nfrom itertools import repeat\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom datasketch import MinHash, MinHashLSHForest","cb1f5354":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()","c2cc1511":"source_path = '..\/input\/coleridgeinitiative-show-us-the-data'","05bb6966":"train_df = pd.read_csv(f'{source_path}\/train.csv')\nsample_submission_df = pd.read_csv(f'{source_path}\/sample_submission.csv')","c7372daa":"train_df.info()","8e92037a":"train_df.head()","67f8c6be":"train_df['clean_pub_title'] = train_df.pub_title.apply(lambda x: clean_text(x))","83207ed4":"train_df.head()","6cca9d00":"pub_title_data = ' '.join(i for i in train_df['clean_pub_title'])","9f46b29f":"wordcloud = WordCloud(width = 3000, height = 2000, random_state=1, background_color='black', colormap='Set2', collocations=False, stopwords = STOPWORDS).generate(pub_title_data)\nplt.figure(figsize=(40, 30))\nplt.imshow(wordcloud)  \nplt.axis(\"off\")","7b2e742b":"vectorizer = CountVectorizer(stop_words='english')\ncount_m = vectorizer.fit_transform(train_df['clean_pub_title'])","30a66585":"count_df = pd.DataFrame({'tokens': vectorizer.get_feature_names(), 'count': count_m.toarray().sum(axis=0).tolist()})\ncount_df.sort_values(by='count', ascending=True, inplace=True)","444f2a98":"count_df.head(10)","81409150":"count_df.tail(10)","9faa8875":"plt.figure(figsize = (15, 15))\nsns.pointplot(x=count_df['tokens'][:50], y=count_df['count'][:50], linestyles=\"-\")\nplt.xlabel(\"tokens\")\nplt.ylabel(\"frequency\")\nplt.xticks(rotation=90)\nplt.show()","5844e6a2":"plt.figure(figsize = (15, 15))\nsns.pointplot(x=count_df['tokens'][-50:], y=count_df['count'][-50:], color = \"green\", linestyles=\"-\")\nplt.xlabel(\"tokens\")\nplt.ylabel(\"frequency\")\nplt.xticks(rotation=90)\nplt.show()","cbfa8451":"train_df['pub_title_len'] = train_df.clean_pub_title.apply(lambda x: len(x))","af1e0b44":"print(f'Length: {len(train_df.iloc[train_df.pub_title_len.argmax()].clean_pub_title)}')\nprint(f'Publication title: {train_df.iloc[train_df.pub_title_len.argmax()].clean_pub_title}')","b64bb1b8":"temp_df = train_df[['pub_title_len', 'clean_pub_title']].copy()\ntemp_df.sort_values(by='pub_title_len', ascending=True, inplace=True)","1781ebd2":"plt.figure(figsize = (15, 15))\nsns.pointplot(x=temp_df.index[:50], y=temp_df['pub_title_len'][:50], linestyles=\"-\")\nplt.xlabel(\"row index\")\nplt.ylabel(\"title length\")\nplt.xticks(rotation=90)\nplt.show()","65253331":"plt.figure(figsize = (15, 15))\nsns.pointplot(x=temp_df.index[-50:], y=temp_df['pub_title_len'][-50:], linestyles=\"-\")\nplt.xlabel(\"row index\")\nplt.ylabel(\"title length\")\nplt.xticks(rotation=90)\nplt.show()","1f8891dc":"train_json_source = f'{source_path}\/train'\ntest_json_source = f'{source_path}\/test'","5ff802cc":"%%time\ntemp_json_list = []\ntemp_file_name = []\ntemp_label = []\nfor json_data in os.listdir(train_json_source):\n    temp_json_list.append(pd.read_json(f'{train_json_source}\/{json_data}', orient='records'))\n    with open(f'{train_json_source}\/{json_data}', 'r') as f:\n        temp_data = json.load(f)\n        temp_file_name.extend(repeat(json_data.replace('.json', ''), len(temp_data)))\n        temp_label.extend(repeat(train_df.loc[train_df['Id'] == json_data.replace('.json', '')].cleaned_label.to_list()[0], len(temp_data)))","060e7ded":"train_json = pd.concat(temp_json_list, ignore_index=True)\ntrain_json['file'] = temp_file_name\ntrain_json['cleaned_label'] = temp_label\ntrain_json.shape","2ec9ff75":"del temp_json_list, temp_file_name\ngc.collect()","27e8c03f":"train_groups = train_json.groupby(train_json['file'])\ntrain_groups = train_groups.apply(lambda train_json: train_json.sort_values(by=['file']))\ntrain_groups.drop(['file'], inplace=True, axis=1)\ntrain_groups.to_csv('train_json.csv')","18fc3c48":"train_groups","27e1927f":"del train_groups","cc80a1d1":"%%time\ntemp_json_list = []\ntemp_file_name = []\ntemp_label = []\nfor json_data in os.listdir(test_json_source):\n    temp_json_list.append(pd.read_json(f'{test_json_source}\/{json_data}', orient='records'))\n    with open(f'{test_json_source}\/{json_data}', 'r') as f:\n        temp_data = json.load(f)\n        temp_file_name.extend(repeat(json_data.replace('.json', ''), len(temp_data)))\n        temp_label.extend(repeat(train_df.loc[train_df['Id'] == json_data.replace('.json', '')].cleaned_label.to_list()[0], len(temp_data)))","ea64d971":"len(temp_label), len(temp_file_name)","eba2aeac":"test_json = pd.concat(temp_json_list, ignore_index=True)\ntest_json['file'] = temp_file_name\ntest_json['cleaned_label'] = temp_label\ntest_json.shape","df0f07dc":"del temp_json_list, temp_file_name\ngc.collect()","442a5d2d":"test_json.head()","6db6d246":"test_groups = test_json.groupby(test_json['file'])\ntest_groups = test_groups.apply(lambda test_json: test_json.sort_values(by=['file']))\ntest_groups.drop(['file'], inplace=True, axis=1)\ntest_groups.to_csv('test_json.csv')","1cd2c224":"test_groups","1ed31ec6":"del test_groups","909e58ed":"train_json['clean_text'] = train_json.text.apply(lambda x: clean_text(x))","833ab3f8":"class LSH:\n    def __init__(self, permutations, number_of_recommendations, depth, dataframe):\n        self.permutations = permutations\n        self.number_of_recommendations = number_of_recommendations\n        self.depth = depth\n        self.dataframe = dataframe\n        self.minhash = []\n        self.forest = None\n    \n    def minhash_data(self):\n        for title in self.dataframe['clean_text']:\n            tokens = title.split(' ')\n            min_hash = MinHash(num_perm=self.permutations)\n            for t in tokens:\n                min_hash.update(t.encode('utf-8'))\n            self.minhash.append(min_hash)\n    \n    def prepare_forest(self):\n        self.forest = MinHashLSHForest(num_perm=self.permutations, l=self.depth)\n        for i, j in enumerate(self.minhash):\n            self.forest.add(i, j)\n        self.forest.index()\n        del self.minhash\n        gc.collect()\n    \n    def query_forest(self, query, number_of_results, cosine_sim=False):\n        query_tokens = query.split(' ')\n        min_hash = MinHash(num_perm=self.permutations)\n        for i in query_tokens:\n            min_hash.update(i.encode('utf-8'))\n        result = self.forest.query(min_hash, self.number_of_recommendations)\n        if cosine_sim:\n            # print(\"Cosine Similarity\")\n            result = [(key, self.cosine_similarity(self.dataframe.iloc[key].cleaned_label, query)) for key in result]\n        else:\n            # print(\"Jaccard Similarity\")\n            result = [(key, self.jaccard_similarity(self.dataframe.iloc[key].cleaned_label, query_tokens)) for key in result]\n        result = sorted(result, key=lambda x: x[1], reverse=True)[:number_of_results]\n        iloc = [i[0] for i in result]\n        return '|'.join(set(self.dataframe.iloc[iloc].cleaned_label.to_list()))\n    \n    def jaccard_similarity(self, l1, l2):\n        intersection = len(list(set(l1).intersection(l2)))\n        union = (len(l1) + len(l2)) - intersection\n        return float(intersection) \/ union\n    \n    def cosine_similarity(self, string1, string2):\n        d1 = nlp(string1)\n        d2 = nlp(string2)\n        return d1.similarity(d2)\n    ","bfa5a1ca":"obj = LSH(permutations=128, number_of_recommendations=20, depth=10, dataframe=train_json)","0ba9896e":"%%time\nobj.minhash_data()","de330384":"%%time\nobj.prepare_forest()","705cc7ff":"%%time\nsample_submission_df['PredictionString'] = sample_submission_df.Id.apply(lambda x: obj.query_forest(train_json.loc[train_json['file'] == str(x)].cleaned_label.to_list()[0], 10))","aa3dfce3":"sample_submission_df.to_csv('submission.csv', index=False)\nsample_submission_df","7b5fe883":"### LSH","4b124b25":"## Import libraries","75b54631":"## Submission","3668b8e7":"<h3 align=\"center\" style=\"background-color:#003300;color:white;\">Thanks! More updates to come. WIP<\/h3> ","a4a3cd4e":"### Test - JSON to DataFrame","d026f747":"Cleaning publication title","21d21f6b":"### Train - JSON to DataFrame","dc1c2f14":"## Helper functions","fda996a0":"## Merge JSON files","058bcc73":"Publication title with longest length","24636623":"Reference: http:\/\/ekzhu.com\/datasketch\/","c5563b8a":"#","85babe26":"Top 50 publication title with longest length","8cfc5e21":"Top 50 tokens with high frequency within publication title","c7bec207":"Calculating length of the publication title","294504c3":"* Setting number of permutations\n* Setting number of recommendations to return  \n* Setting depth of LSH Forest\n* Preparing shingles\n* MinHashing all the shingles\n* Preparing MinHashForest of MinHash\n* Indexing forest\n* Querying forest\n* Calculating jaccard similarity or cosine similarity (Post-processing)","d6a0bbd4":"Top 50 publication title with short length","76bc9295":"Top 50 tokens with less frequency within publication title","8b908fb6":"Below clean_text function should be used to clean text as mentioned on the Evaluation page","265c83f9":"## WordCloud"}}