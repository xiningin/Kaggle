{"cell_type":{"5b69a608":"code","46095d37":"code","266a9b78":"code","77caacee":"code","66eaeaf6":"code","2b93a0fe":"code","7608b422":"code","f4d8fce8":"code","51e8c366":"code","5073a115":"code","a453e7b0":"code","868a2ac9":"code","89fc4c5e":"code","1b70c055":"code","0abd7364":"code","9b5946b1":"code","cfeb3982":"code","219aaf1a":"code","79ae1c1f":"code","856c34b6":"code","02865487":"code","ae1b9341":"markdown","a8d97b7e":"markdown","91799c38":"markdown","7756cc79":"markdown","5511ddef":"markdown","355c8b66":"markdown","d0f886eb":"markdown","3438415a":"markdown","17f2b8d2":"markdown","a9c2854e":"markdown","16c898b2":"markdown","fda4ae16":"markdown","dd699ef5":"markdown"},"source":{"5b69a608":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","46095d37":"# Read data\ndf = pd.read_csv('..\/input\/breast-cancer-wisconsin\/data.csv')\ndf = df.drop(['id'], axis = 1) # Irrelevant\n\ndata = df.iloc[:, 1:31]\ndata.head()","266a9b78":"# Standardize data\narr = data.to_numpy()\ncol_means = arr.mean(axis = 0)\nstd = arr.std(axis = 0)\ncol_means.reshape((1,30))\nstd.reshape((1,30))\narr = np.subtract(arr, col_means)\narr = np.divide(arr, std)","77caacee":"# Covariance matrix\ncov_matrix = np.cov(arr.T)\n# Eigenvectors and eigenvalues\neigen_values, eigen_vectors = np.linalg.eig(cov_matrix)\neigen_values.reshape((1,30))\n# print(\"Eigenvector: \\n\",eigen_vectors,\"\\n\")\n# print(\"Eigenvalues: \\n\", eigen_values, \"\\n\")","66eaeaf6":"# Plot eigenvalues\nfrom matplotlib import pyplot as plt\nfig = plt.figure()\nplt.ylabel(\"Eigen Value\")\nplt.bar([i for i in range(30)], eigen_values)\nplt.show()","2b93a0fe":"# Kaiser rule, retain components with only lambda > 1\neigen_vals = np.asarray(eigen_values)\neligible_cnt = (eigen_vals > 1).sum()\nprint(f'The number of components that need be retained based on Kaiser rule = {eligible_cnt}')\n\n# Conditional number rule\nmax_lambda = max(eigen_vals)\neligible_cnt_2 = [(max_lambda \/ eigen_val) < 10 for eigen_val in eigen_vals]\nprint(f'The number of components that need be retained based on conditional number rule = {np.sum(eligible_cnt_2)}')","7608b422":"# Data visualization of first three components in histogram\nnames = list(data)[:3]\nplot_data = data.iloc[:, :3]\n# print(plot_data.shape)\n_ = plt.hist(plot_data, bins = int(plot_data.shape[0]\/25), label=names)\n_ = plt.legend()","f4d8fce8":"# Plotting radius_mean vs texture_mean\nplot_data = data.iloc[:, :2]\nnames = list(data)[:2]\nplt.plot(plot_data, 'o')\n_ = plt.legend(names)","51e8c366":"# Plotting texture_mean vs perimeter_mean\nplot_data = data.iloc[:, 1:3]\nnames = list(data)[1:3]\nplt.plot(plot_data, 'o')\n_ = plt.legend(names)","5073a115":"# Get data after PCA\n# Choose only first 6 components\nprojection_matrix = (eigen_vectors.T[:][:6]).T\ndata_pca = arr.dot(projection_matrix)\n# print(data_pca.shape)","a453e7b0":"import sys\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import davies_bouldin_score\nfrom random import randint","868a2ac9":"k = 2\n# n_init makes k-means run for those many different centroid seeds\nkmeans2 = KMeans(n_clusters=k, n_init = 10, init = 'random', random_state=randint(99, 999)).fit(data_pca)\nlabels = kmeans2.labels_ # Autogenerated labels for clusters\nscore = davies_bouldin_score(data_pca, labels)\nprint(f'The Davies Bouldin Index for k = {k} is {score:.2f}')","89fc4c5e":"y_km2 = kmeans2.predict(data_pca)\nplt.scatter(\n    data_pca[y_km2 == 0, 0], data_pca[y_km2 == 0, 1],\n    s=50, c='red',\n    marker='s',\n    label='Cluster1'\n)\n\nplt.scatter(\n    data_pca[y_km2 == 1, 0], data_pca[y_km2 == 1, 1],\n    s=50, c='blue',\n    marker='o',\n    label='Cluster2'\n)\n\n# plot the centroids\nplt.scatter(\n    kmeans2.cluster_centers_[:, 0], kmeans2.cluster_centers_[:, 1],\n    s=250, marker='*',\n    c='red', edgecolor='black',\n    label='centroids'\n)\nplt.legend(scatterpoints=1)\nplt.show()","1b70c055":"k = 3\n# n_init makes k-means run for those many different centroid seeds\nkmeans3 = KMeans(n_clusters=k, n_init = 10, init = 'random', random_state=randint(99, 999)).fit(data_pca)\nlabels = kmeans3.labels_ # Autogenerated labels for clusters\nscore = davies_bouldin_score(data_pca, labels)\nprint(f'The Davies Bouldin Index for k = {k} is {score:.2f}')","0abd7364":"y_km3 = kmeans3.predict(data_pca)\nplt.scatter(\n    data_pca[y_km3 == 0, 0], data_pca[y_km3 == 0, 1],\n    s=50, c='red',\n    marker='s',\n    label='Cluster1'\n)\n\nplt.scatter(\n    data_pca[y_km3 == 1, 0], data_pca[y_km3 == 1, 1],\n    s=50, c='blue',\n    marker='o',\n    label='Cluster2'\n)\n\nplt.scatter(\n    data_pca[y_km3 == 2, 0], data_pca[y_km3 == 2, 1],\n    s=50, c='green',\n    marker='v',\n    label='Cluster3'\n)\n\n# plot the centroids\nplt.scatter(\n    kmeans3.cluster_centers_[:, 0], kmeans3.cluster_centers_[:, 1],\n    s=250, marker='*',\n    c='red', edgecolor='black',\n    label='centroids'\n)\nplt.legend(scatterpoints=1)\nplt.show()","9b5946b1":"k = 5\n# n_init makes k-means run for those many different centroid seeds\nkmeans5 = KMeans(n_clusters=k, n_init = 10, init = 'random', random_state=randint(99, 999)).fit(data_pca)\nlabels = kmeans5.labels_ # Autogenerated labels for clusters\nscore = davies_bouldin_score(data_pca, labels)\nprint(f'The Davies Bouldin Index for k = {k} is {score:.2f}')","cfeb3982":"y_km5 = kmeans5.predict(data_pca)\nplt.scatter(\n    data_pca[y_km5 == 0, 0], data_pca[y_km5 == 0, 1],\n    s=50, c='red',\n    marker='s',\n    label='Cluster1'\n)\n\nplt.scatter(\n    data_pca[y_km5 == 1, 0], data_pca[y_km5 == 1, 1],\n    s=50, c='blue',\n    marker='o',\n    label='Cluster2'\n)\n\nplt.scatter(\n    data_pca[y_km5 == 2, 0], data_pca[y_km5 == 2, 1],\n    s=50, c='green',\n    marker='v',\n    label='Cluster3'\n)\n\nplt.scatter(\n    data_pca[y_km5 == 3, 0], data_pca[y_km5 == 3, 1],\n    s=50, c='orange',\n    marker='v',\n    label='Cluster4'\n)\n\nplt.scatter(\n    data_pca[y_km5 == 4, 0], data_pca[y_km5 == 4, 1],\n    s=50, c='cyan',\n    marker='v',\n    label='Cluster5'\n)\n\n# plot the centroids\nplt.scatter(\n    kmeans5.cluster_centers_[:, 0], kmeans5.cluster_centers_[:, 1],\n    s=250, marker='*',\n    c='red', edgecolor='black',\n    label='centroids'\n)\nplt.legend(scatterpoints=1)\nplt.show()","219aaf1a":"y_true = df.iloc[:, 0:1].to_numpy()\ny_true = y_true.reshape((y_true.shape[0],))\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny_true = le.fit_transform(y_true)","79ae1c1f":"# Get labels for k = 2\nk_2_labels = kmeans2.labels_\n# print(k_3_labels)\n# Get indices where cluster is same\n# print(clust_true)\n# Get values from true wherever True\nacc_cnt_common_class = 0\nfor clust_idx in range(2):\n    clust_true = (clust_idx == k_2_labels)\n    vals = []\n    for i in range(len(clust_true)):\n        if clust_true[i]:\n            vals.append(y_true[i])\n    # print(vals)\n    cnt_common_class = max(vals.count(0), vals.count(1))\n    #print(cnt_common_class)\n    acc_cnt_common_class += cnt_common_class\nprint(f'Purity of K-Means clustering when k = 2 is {acc_cnt_common_class \/ len(y_true)}')","856c34b6":"# Get labels for k = 3\nk_3_labels = kmeans3.labels_\n# print(k_3_labels)\n# Get indices where cluster is same\n# print(clust_true)\n# Get values from true wherever True\nacc_cnt_common_class = 0\nfor clust_idx in range(3):\n    clust_true = (clust_idx == k_3_labels)\n    vals = []\n    for i in range(len(clust_true)):\n        if clust_true[i]:\n            vals.append(y_true[i])\n    # print(vals)\n    cnt_common_class = max(vals.count(0), vals.count(1))\n    #print(cnt_common_class)\n    acc_cnt_common_class += cnt_common_class\nprint(f'Purity of K-Means clustering when k = 3 is {acc_cnt_common_class \/ len(y_true)}')","02865487":"# Get labels for k = 5\nk_5_labels = kmeans5.labels_\n# print(k_3_labels)\n# Get indices where cluster is same\n# print(clust_true)\n# Get values from true wherever True\nacc_cnt_common_class = 0\nfor clust_idx in range(5):\n    clust_true = (clust_idx == k_5_labels)\n    vals = []\n    for i in range(len(clust_true)):\n        if clust_true[i]:\n            vals.append(y_true[i])\n    # print(vals)\n    cnt_common_class = max(vals.count(0), vals.count(1))\n    #print(cnt_common_class)\n    acc_cnt_common_class += cnt_common_class\nprint(f'Purity of K-Means clustering when k = 5 is {acc_cnt_common_class \/ len(y_true)}')","ae1b9341":"## Standardization of Data","a8d97b7e":"### K = 3","91799c38":"The Purity of K = 2 is very close to 1. So it can classify the cancer as Malignant or Bening based on which cluster the new data point belongs to","7756cc79":"## Visualization of Data","5511ddef":"### K = 2","355c8b66":"Separation of classes is better in perimeter mean as seen above as variance is more\nThe radius mean gives more variance in the covariance matrix \nThe perimeter mean and radius mean are closely related for almost circular breast cancers","d0f886eb":"Visual separation of classes is better in Y-plane as perimeter_mean tends to take higher values than texture_mean","3438415a":"## Purity of clustering and Classification Potential","17f2b8d2":"The Purity when K = 5 is very high. This means that classification based on 5 classes is appropriate as it can correctly identify the data. Possible reasons can include better range of cancer stages that can lead to one or the other (Malignant\/Bening)","a9c2854e":"## K-Means Clustering","16c898b2":"The Purity of K = 3 is also a bit closer to 1 but not that great than K = 2, this means that out of the three clusters, we can identify which stage of cancer it is and make prediction based on three stages rather than 2. Low purity signifies overlap in stages of Malignant and Bening ","fda4ae16":"### K = 5","dd699ef5":"Visual separation of classes is better in Y-plane as texture_mean tends to take higher values than radius_mean"}}