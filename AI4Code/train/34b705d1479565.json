{"cell_type":{"ce6e0b44":"code","0c452035":"code","56ae0f2a":"code","8e94fa7c":"code","f91615d3":"code","9e9639ae":"code","86cfe814":"code","005e4cab":"code","73655636":"code","a6e94c1c":"code","1a91ea84":"code","7e7a9c72":"code","06a6bf9e":"code","a6a2abda":"code","ab1dda28":"code","60b29a77":"code","fb8f064c":"code","53848b2b":"code","d67edc4f":"code","f2afbc12":"code","cbb05688":"code","e2128d0c":"code","1eff1df2":"code","bb7b9384":"code","61076bc8":"code","fb216b52":"code","64e2e5d4":"code","c2ecf878":"code","e430186e":"code","0da99f05":"markdown","f396c6de":"markdown","16c69e2a":"markdown","1881b8d5":"markdown","7bab7f8d":"markdown","d8b61488":"markdown","ff567583":"markdown"},"source":{"ce6e0b44":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0c452035":"data = pd.read_csv(\"..\/input\/suicide-rates-overview-1985-to-2016\/master.csv\",sep=\",\")","56ae0f2a":"# Dropping null and non numeretic columns\ndata.dropna(inplace=True)\ndata.drop([\"age\",\"country\",\"country-year\",\"generation\"],axis=1,inplace=True)","8e94fa7c":"# convert male and femala 1 and 0\ndata.sex = [1 if each == \"male\" else 0 for each in data.sex]","f91615d3":"# deleting spaces\ndata.rename(columns={' gdp_for_year ($) ':'gdp_year'}, inplace=True)\ndata.rename(columns={'HDI for year':'HDI_year'}, inplace=True)\ndata.rename(columns={'suicides\/100k pop':'suicides\/100k_pop'}, inplace=True)\ndata.rename(columns={'gdp_per_capita ($)':'gdp_per_capita_dollar'}, inplace=True)","9e9639ae":"# deleting comas\ndata.gdp_year = data.gdp_year.str.replace(',','')","86cfe814":"# defining x and y\ny = data.sex\nx_data = data.drop([\"sex\"],axis=1)","005e4cab":"# I should convert type of sixth column to float from string.\nx_data.gdp_year = data.gdp_year.apply(lambda x: float(x))","73655636":"# normalization\nx = (x_data - np.min(x_data)) \/ (np.max(x_data) - np.min(x_data))","a6e94c1c":"# train test split\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(x, y, test_size = 0.15, random_state = 42)\n\nx_train = x_train.values.T\nx_test = x_test.values.T\ny_test = y_test.values.reshape(1,y_test.shape[0])\ny_train = y_train.values.reshape(1,y_train.shape[0])","1a91ea84":"# sigmoid function\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head","7e7a9c72":"# intialize parameters and layer sizes\ndef initialize_parameters_and_layer_sizes_NN(x_train, y_train):\n    parameters = {\"weight1\": np.random.randn(3,x_train.shape[0]) * 0.1,\n                  \"bias1\": np.zeros((3,1)),\n                  \"weight2\": np.random.randn(y_train.shape[0],3) * 0.1,\n                  \"bias2\": np.zeros((y_train.shape[0],1))}\n    return parameters","06a6bf9e":"# forward propagation\ndef forward_propagation_NN(x_train, parameters):\n\n    Z1 = np.dot(parameters[\"weight1\"],x_train) +parameters[\"bias1\"]\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(parameters[\"weight2\"],A1) + parameters[\"bias2\"]\n    A2 = sigmoid(Z2)\n\n    cache = {\"Z1\": Z1,\n             \"A1\": A1,\n             \"Z2\": Z2,\n             \"A2\": A2}\n    \n    return A2, cache","a6a2abda":"# Compute cost\ndef compute_cost_NN(A2, Y, parameters):\n    logprobs = np.multiply(np.log(A2),Y)\n    cost = -np.sum(logprobs)\/Y.shape[1]\n    return cost","ab1dda28":"# Backward Propagation\ndef backward_propagation_NN(parameters, cache, X, Y):\n\n    dZ2 = cache[\"A2\"]-Y\n    dW2 = np.dot(dZ2,cache[\"A1\"].T)\/X.shape[1]\n    db2 = np.sum(dZ2,axis =1,keepdims=True)\/X.shape[1]\n    dZ1 = np.dot(parameters[\"weight2\"].T,dZ2)*(1 - np.power(cache[\"A1\"], 2))\n    dW1 = np.dot(dZ1,X.T)\/X.shape[1]\n    db1 = np.sum(dZ1,axis =1,keepdims=True)\/X.shape[1]\n    grads = {\"dweight1\": dW1,\n             \"dbias1\": db1,\n             \"dweight2\": dW2,\n             \"dbias2\": db2}\n    return grads","60b29a77":"# update parameters\ndef update_parameters_NN(parameters, grads, learning_rate = 0.01):\n    parameters = {\"weight1\": parameters[\"weight1\"]-learning_rate*grads[\"dweight1\"],\n                  \"bias1\": parameters[\"bias1\"]-learning_rate*grads[\"dbias1\"],\n                  \"weight2\": parameters[\"weight2\"]-learning_rate*grads[\"dweight2\"],\n                  \"bias2\": parameters[\"bias2\"]-learning_rate*grads[\"dbias2\"]}\n    \n    return parameters","fb8f064c":"# prediction\ndef predict_NN(parameters,x_test):\n    # x_test is a input for forward propagation\n    A2, cache = forward_propagation_NN(x_test,parameters)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(A2.shape[1]):\n        if A2[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","53848b2b":"# 2 - Layer neural network\ndef two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations):\n    cost_list = []\n    index_list = []\n    #initialize parameters and layer sizes\n    parameters = initialize_parameters_and_layer_sizes_NN(x_train, y_train)\n\n    for i in range(0, num_iterations):\n         # forward propagation\n        A2, cache = forward_propagation_NN(x_train,parameters)\n        # compute cost\n        cost = compute_cost_NN(A2, y_train, parameters)\n         # backward propagation\n        grads = backward_propagation_NN(parameters, cache, x_train, y_train)\n         # update parameters\n        parameters = update_parameters_NN(parameters, grads)\n      \n        if i % 100 == 0:\n            cost_list.append(cost)\n            index_list.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))    \n    plt.plot(index_list,cost_list)\n    plt.xticks(index_list,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n    \n    # predict\n    y_prediction_test = predict_NN(parameters,x_test)\n    y_prediction_train = predict_NN(parameters,x_train)\n\n    # Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    return parameters\n\nparameters = two_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations=3000)","d67edc4f":"# reshaping\nx_train, x_test, y_train, y_test = x_train.T, x_test.T, y_train.T, y_test.T","f2afbc12":"# Evaluating the ANN\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential # initialize neural network library \nfrom keras.layers import Dense # build our layers library\ndef build_classifier():\n    classifier = Sequential() # initialize neural network\n    classifier.add(Dense(units = 7, kernel_initializer = 'uniform', activation = 'relu', input_dim = x_train.shape[1])) # we use dimension of x_train as input\n    classifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu')) # we use 4 nodes in first layer\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid')) # if we use sigmoid function it means we add output layer\n    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']) # we will use accuracy as metrics\n    return classifier\n\nclassifier = KerasClassifier(build_fn = build_classifier, epochs = 100) # epochs means that is number of iteration \naccuracies = cross_val_score(estimator = classifier, X = x_train, y = y_train, cv = 3)\nmean = accuracies.mean()\nvariance = accuracies.std()\nprint(\"Accuracy mean: \"+ str(mean))\nprint(\"Accuracy variance: \"+ str(variance))","cbb05688":"dict_of_values = {'ANN_Num': [], 'CV': [],'epochs': [] , 'accuracy': [] }\ndata_temproray = pd.DataFrame.from_dict(dict_of_values)","e2128d0c":"data_temproray.ANN_Num = [2]\ndata_temproray.CV = [3]\ndata_temproray.epochs = [100]\ndata_temproray.accuracy = [0.6558003964712986]","1eff1df2":"data_temproray.head()","bb7b9384":"# Evaluating the ANN V2\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential # initialize neural network library \nfrom keras.layers import Dense # build our layers library\ndef build_classifier2():\n    classifier = Sequential() # initialize neural network\n    classifier.add(Dense(units = 7, kernel_initializer = 'uniform', activation = 'relu', input_dim = x_train.shape[1])) # we use dimension of x_train as input\n    classifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu')) # we use 4 nodes in second layer\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid')) # if we use sigmoid function it means we add output layer\n    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy']) # we will use accuracy as metrics\n    return classifier\n\nclassifier = KerasClassifier(build_fn = build_classifier2, epochs = 150) # epochs means that is number of iteration \naccuracies = cross_val_score(estimator = classifier, X = x_train, y = y_train, cv = 4)\nmean = accuracies.mean()\nvariance = accuracies.std()\nprint(\"Accuracy mean: \"+ str(mean))\nprint(\"Accuracy variance: \"+ str(variance))","61076bc8":"data_temproray.loc[-1] = [2, 4, 150,  0.6640782611279521]  # adding a row\ndata_temproray.index = data_temproray.index + 1  # shifting index\ndata_temproray = data_temproray.sort_index()  # sorting by index\n\ndata_temproray.loc[-1] = [3, 3, 100, 0.492614638565769]  # adding a row\ndata_temproray.index = data_temproray.index + 1  # shifting index\ndata_temproray = data_temproray.sort_index()  # sorting by index\n\ndata_temproray.loc[-1] = [3, 4, 150, 0.49022363005586056]  # adding a row\ndata_temproray.index = data_temproray.index + 1  # shifting index\ndata_temproray = data_temproray.sort_index()  # sorting by index\n\ndata_temproray.loc[-1] = [2, 4, 150, 0.7318890200427076]  # adding a row\ndata_temproray.index = data_temproray.index + 1  # shifting index\ndata_temproray = data_temproray.sort_index()  # sorting by index\n\ndata_temproray.loc[-1] = [2, 5, 150, 0.7341438597594337]  # adding a row\ndata_temproray.index = data_temproray.index + 1  # shifting index\ndata_temproray = data_temproray.sort_index()  # sorting by index\n\ndata_temproray.loc[-1] = [2, 4, 200, 0.7310445842269522]  # adding a row\ndata_temproray.index = data_temproray.index + 1  # shifting index\ndata_temproray = data_temproray.sort_index()  # sorting by index\n\n","fb216b52":"data_temproray","64e2e5d4":"data_temproray.accuracy = data_temproray.accuracy.apply(lambda x: x*100)","c2ecf878":"data_temproray","e430186e":"# plotly\n#import plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\n\ninternational_color = [float(each) for each in data_temproray.epochs]\ndata2 = [\n    {\n        'y':data_temproray.CV,\n        'x': data_temproray.ANN_Num,\n        'mode': 'markers',\n        'marker': {\n            'color': international_color,\n            'size': data_temproray.accuracy,\n            'showscale': True\n        },\n        \"text\" :  data_temproray.accuracy    \n    }\n]\niplot(data2)","0da99f05":"**2. Normalization**","f396c6de":"**1. Data Cleaning**","16c69e2a":"I have changed ANN_Num, CV and epochs but I don't run them again because it takes a long time. I will write results of these algorithms.","1881b8d5":"**3. Train Test Split**","7bab7f8d":"I multiply with 100 to accuracy column if I don't do it, plotly will have be problem.","d8b61488":"**6. Coclusion**\n\nAltough it looks that trying to guess to gender of victims as true is non realistic I have guessed 73.41% as true but I do it to improve myself.\nThere are no doubt that this model can be improved by using other variables whose type is string.\nThe best result of this model is 73.41% thanks to \"Layer of Network\" know as ANN_num = 2, CV=5, epochs=150 ","ff567583":"In this kernel I compare Keras Classifier algorithms for different \"Numbers of ANN\", \"CV\" and \"epochs\" values\".\nI try to guess \"sex\" of people who is die becuase of assassination from only 7 numeric columns. In fact there are a lot of columns and I should have benefit from them by using \"one hot encoder\" and \"label encoder\" techniques but I don't know very well to these techniques. I hope I will use these techniques in other project.\n\nIn fact hoping to gueess to gender of victims as true is not realistic but my results are not bad. I believe they can be better.\n\n**CONTENT**\n1. Data Cleaning\n2. Normalization\n3. Train Test Split\n4. Build to 2 Layer Neural Network\n5. Buil ANN from Keras for different hyperparameters\n6. Conclusion \n"}}