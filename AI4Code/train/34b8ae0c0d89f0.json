{"cell_type":{"3aabde93":"code","f2ec19d7":"code","b59f652f":"code","85ad093b":"code","2a1c3adf":"code","cf7fac2a":"code","f0e7fd59":"code","de827aa2":"code","647caa61":"code","9a8c2f1b":"code","bd55033d":"code","86228205":"code","8dba42f8":"code","14e98cc2":"code","5b9a5f6e":"code","927bb62c":"code","bdff06b2":"code","571ca615":"code","8182a53c":"code","a08afa24":"code","c735d306":"code","dd81538e":"code","84837a0d":"code","b01c90cc":"code","72d3d638":"code","e926199e":"code","1ed58a0f":"code","cf0a2b9c":"code","87bbcd9d":"code","ed7b7d32":"code","53b9c66b":"code","f4a30d8a":"code","d99e7dbf":"code","de13b4b5":"code","0ea1937a":"code","95b5b785":"code","a470f489":"code","049f9167":"code","2a38a3e5":"code","f033afbc":"code","0e1090f3":"code","0df46b4a":"code","1937fa0b":"code","365e96b0":"code","e967cef9":"code","56712092":"code","8cb96999":"code","38f317da":"code","3d0e3df1":"code","6960f894":"code","e1363eee":"code","896e1b85":"code","f9d61ee2":"code","9ed711dd":"code","613a9bed":"code","f62496b4":"code","542a4ba5":"code","5e827fc9":"code","9993eb5b":"code","ccaa20e8":"code","bfb4fa7a":"code","8eaca0cd":"code","2a4e2757":"code","04e1255e":"code","562d27ad":"code","2b471f90":"code","228bace6":"code","dab3b5a8":"code","3391c11f":"code","83fbb997":"code","13130f0c":"code","4391d75e":"code","e1c94998":"code","8ee4dd27":"code","17fbe8d5":"code","9a1d4a9b":"markdown","4646e8b0":"markdown","bb3d24c2":"markdown","f704b918":"markdown","f5bca9f7":"markdown","79c73f73":"markdown","659d0725":"markdown","aa640675":"markdown","3e42f752":"markdown","0be552eb":"markdown","9a0b9753":"markdown","232fab55":"markdown","ddc77241":"markdown","203734d5":"markdown","c71d8ea2":"markdown","2632be8a":"markdown","b03a5895":"markdown","69ca8b58":"markdown","81c96a84":"markdown","4b2cf9dc":"markdown","02a2c7e4":"markdown","57ddd572":"markdown","0c51dc40":"markdown","a6925b8b":"markdown","7a7bc76a":"markdown","c05ec7bc":"markdown","cb65b716":"markdown","85183840":"markdown"},"source":{"3aabde93":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","f2ec19d7":"path = \"..\/input\/\"","b59f652f":"df = pd.read_csv(path+'heart.csv')\ndf.head()","85ad093b":"df.describe()","2a1c3adf":"df.target.value_counts()","cf7fac2a":"sns.countplot(x='target', data=df)\nplt.show()","f0e7fd59":"sns.countplot(x='sex', data=df)\nplt.xlabel('Sex (0=female, 1=male)')\nplt.show()","de827aa2":"sns.countplot(x='sex', hue='target', data=df)\nplt.xlabel('Sex (0=female, 1=male)')\nplt.show()","647caa61":"percentFemale = len(df[df.sex==0])\/len(df.sex)*100\npercentMale = len(df[df.sex==1])\/len(df.sex)*100\nprint(f'Percentage of Female Patients: {percentFemale:.2f}%')\nprint(f'Percentage of Male Patients: {percentMale:.2f}%')","9a8c2f1b":"percentFemaleWithDisease = len(df[(df.sex==0) & (df.target==1)])\/len(df[df.sex==0])*100\npercentMaleWithDisease = len(df[(df.sex==1) & (df.target==1)])\/len(df[df.sex==1])*100\nprint(f'Percentage of Female Patients with Disease: {percentFemaleWithDisease:.2f}%')\nprint(f'Percentage of Male Patients with Disease: {percentMaleWithDisease:.2f}%')","bd55033d":"sns.countplot(x='cp', hue='target', data=df)\nplt.xlabel('Chest pain type')\nplt.show()","86228205":"sns.countplot(x='fbs', hue='target', data=df)\nplt.title('Fasting blood sugar > 120 (0=false, 1=true)')\nplt.show()","8dba42f8":"df.groupby('target').mean()","14e98cc2":"fig, ax = plt.subplots(figsize=(12, 12))\nsns.heatmap(df.corr(), annot=True, ax=ax)","5b9a5f6e":"df.dtypes","927bb62c":"df.nunique()","bdff06b2":"cat_columns = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'thal']","571ca615":"df_cat = df.astype(dict((item, 'object') for item in cat_columns))","8182a53c":"df_cat.dtypes","a08afa24":"df_cat.head()","c735d306":"pd.__version__","dd81538e":"df_cat = pd.get_dummies(df_cat, columns=cat_columns)\ndf_cat.head()","84837a0d":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nimport xgboost as xgb\nimport lightgbm as lgb","b01c90cc":"train = np.array(df.drop('target', axis=1))\ny_train = np.array(df['target'])","72d3d638":"from sklearn.model_selection import KFold, cross_val_score\n\nn_folds = 5\n\ndef get_cv_scores(model, print_scores=True):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train)\n    accuracy = cross_val_score(model, train, y_train, scoring=\"accuracy\", cv = kf)\n    f1_score = cross_val_score(model, train, y_train, scoring=\"f1\", cv = kf)\n    roc_auc_score = cross_val_score(model, train, y_train, scoring=\"roc_auc\", cv = kf)\n    if print_scores:\n        print(f'Accuracy: {accuracy.mean():.3f} ({accuracy.std():.3f})')\n        print(f'f1_score: {f1_score.mean():.3f} ({f1_score.std():.3f})')\n        print(f'roc_auc_score: {roc_auc_score.mean():.3f} ({roc_auc_score.std():.3f})')\n    return [accuracy, f1_score, roc_auc_score]","e926199e":"%%time\nlr = LogisticRegression()\nget_cv_scores(lr);","1ed58a0f":"%%time\nsvm = SVC()\nget_cv_scores(svm);","cf0a2b9c":"%%time\nrf = RandomForestClassifier()\nget_cv_scores(rf);","87bbcd9d":"%%time\ngb = GradientBoostingClassifier()\nget_cv_scores(gb);","ed7b7d32":"%%time\net = ExtraTreesClassifier()\nget_cv_scores(et);","53b9c66b":"%%time\nxgb_model = xgb.XGBClassifier()\nget_cv_scores(xgb_model);","f4a30d8a":"%%time\nlgb_model = lgb.LGBMClassifier()\nget_cv_scores(lgb_model);","d99e7dbf":"train = np.array(df_cat.drop('target', axis=1))\ny_train = np.array(df_cat['target'])","de13b4b5":"%%time\nlr = LogisticRegression()\nget_cv_scores(lr);","0ea1937a":"%%time\nxgb_model = xgb.XGBClassifier()\nget_cv_scores(xgb_model);","95b5b785":"%%time\nlgb_model = lgb.LGBMClassifier()\nget_cv_scores(lgb_model);","a470f489":"train = np.array(df.drop('target', axis=1))\ny_train = np.array(df['target'])","049f9167":"%%time\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparams = {\n    'C': [0.1, 0.3, 1, 3],\n    'max_iter': [50, 100, 200],\n}\n\nclf = RandomizedSearchCV(LogisticRegression(), params, cv=5, scoring='accuracy', random_state=1)\nclf.fit(train, y_train)\nprint(clf.best_params_)","2a38a3e5":"%%time\nlg_tuned = LogisticRegression(**clf.best_params_)\nget_cv_scores(lg_tuned)","f033afbc":"%%time\n\nparams = {\n    'max_depth': [3, 5, 7],\n    'n_estimators': [100, 300, 800, 1100],\n    'colsample_bytree': [0.5, 0.8, 1],\n    'subsample': [0.5, 0.8, 1]\n}\n\nclf = RandomizedSearchCV(xgb.XGBClassifier(), params, cv=5, scoring='roc_auc', random_state=1)\nclf.fit(train, y_train)\nprint(clf.best_params_)","0e1090f3":"%%time\nxgb_model_tuned = xgb.XGBClassifier(**clf.best_params_)\nget_cv_scores(xgb_model_tuned)","0df46b4a":"%%time\nparams = {\n    'max_depth': [3, 5, 7, -1],\n    'n_estimators': [50, 100, 300, 800, 1100],\n    'colsample_bytree': [0.5, 0.8, 1],\n}\n\nclf = RandomizedSearchCV(lgb.LGBMClassifier(), params, cv=5, scoring='roc_auc', random_state=1)\nclf.fit(train, y_train)\nprint(clf.best_params_)","1937fa0b":"%%time\nlgb_model_tuned = xgb.XGBClassifier(**clf.best_params_)\nget_cv_scores(lgb_model_tuned)","365e96b0":"from sklearn.base import BaseEstimator, TransformerMixin, clone, ClassifierMixin\n\n# based on https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\nclass StackingAveragedModels(BaseEstimator, ClassifierMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n        \n    def fit(self, X, y):\n        \"\"\"Fit all the models on the given dataset\"\"\"\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=42)\n        \n        # Train cloned base models and create out-of-fold predictions\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n        \n        # Train meta-model on out-of-fold predicitions\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n    \n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)\n    \n    def predict_proba(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict_proba(meta_features)","e967cef9":"%%time\nstacked_averaged_model_1 = StackingAveragedModels(base_models=[GradientBoostingClassifier(), xgb.XGBClassifier(),\n                                                               lgb.LGBMClassifier(),LogisticRegression()], meta_model=LogisticRegression())\nstacked_averaged_model_1.fit(train, y_train)\nget_cv_scores(stacked_averaged_model_1);","56712092":"%%time\nstacked_averaged_model_2 = StackingAveragedModels(base_models=[xgb_model_tuned, lgb_model_tuned], \n                                                  meta_model=LogisticRegression())\nstacked_averaged_model_2.fit(train, y_train)\nget_cv_scores(stacked_averaged_model_1);","8cb96999":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\ndef get_feature_importance(model, X, y, feature_names):\n    perm = PermutationImportance(model, random_state=42).fit(X, y)\n    return eli5.show_weights(perm, feature_names=feature_names)","38f317da":"from sklearn.model_selection import train_test_split\n\ntrain = np.array(df.drop('target', axis=1))\ny_train = np.array(df['target'])\n\nX_train, X_test, y_train, y_test = train_test_split(train ,y_train , test_size=0.2, random_state=1)","3d0e3df1":"feature_names = df.drop('target', axis=1).columns.tolist()","6960f894":"lr = LogisticRegression(max_iter=50, C=0.3).fit(X_train, y_train)\nget_feature_importance(lr, X_test, y_test, feature_names)","e1363eee":"xgb_model = xgb.XGBClassifier(n_estimators=100, max_depth=5, colsample_bytree=0.8, subsample=0.5).fit(X_train, y_train)\nget_feature_importance(xgb_model, X_test, y_test, feature_names)","896e1b85":"lgb_model = lgb.LGBMClassifier(n_estimators=50, max_depth=3, colsample_bytree=1).fit(X_train, y_train)\nget_feature_importance(lgb_model, X_test, y_test, feature_names)","f9d61ee2":"!pip install git+https:\/\/github.com\/SauceCat\/PDPbox.git","9ed711dd":"from pdpbox import pdp, get_dataset, info_plots\n\nfrom sklearn.model_selection import train_test_split\n\ntrain = df.drop('target', axis=1)\ny_train = df['target']\n\nX_train, X_test, y_train, y_test = train_test_split(train ,y_train , test_size=0.2, random_state=1)","613a9bed":"pdp_sex = pdp.pdp_isolate(model=lr, dataset=X_test, model_features=feature_names, feature='sex')\n\npdp.pdp_plot(pdp_sex, 'Gender', plot_lines=True, frac_to_plot=0.5)\nplt.show()","f62496b4":"pdp_sex = pdp.pdp_isolate(model=lgb_model, dataset=X_test, model_features=feature_names, feature='sex')\n\npdp.pdp_plot(pdp_sex, 'Gender', plot_lines=True, frac_to_plot=0.5)\nplt.show()","542a4ba5":"pdp_thal = pdp.pdp_isolate(model=lr, dataset=X_test, model_features=feature_names, feature='thal')\n\npdp.pdp_plot(pdp_thal, 'Thal', plot_lines=True, frac_to_plot=0.5)\nplt.show()","5e827fc9":"pdp_thal = pdp.pdp_isolate(model=lr, dataset=X_test, model_features=feature_names, feature='ca')\n\npdp.pdp_plot(pdp_thal, 'ca', plot_lines=True, frac_to_plot=0.5)\nplt.show()","9993eb5b":"features_to_plot = ['age', 'sex']\ninter1 = pdp.pdp_interact(model=lr, dataset=X_test, model_features=feature_names, features=features_to_plot)\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour')\nplt.show()","ccaa20e8":"features_to_plot = ['age', 'sex']\ninter1 = pdp.pdp_interact(model=lgb_model, dataset=X_test, model_features=feature_names, features=features_to_plot)\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour')\nplt.show()","bfb4fa7a":"features_to_plot = ['ca', 'sex']\ninter1 = pdp.pdp_interact(model=lr, dataset=X_test, model_features=feature_names, features=features_to_plot)\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour')\nplt.show()","8eaca0cd":"features_to_plot = ['ca', 'sex']\ninter1 = pdp.pdp_interact(model=lgb_model, dataset=X_test, model_features=feature_names, features=features_to_plot)\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour')\nplt.show()","2a4e2757":"X_test.iloc[0:5]","04e1255e":"y_test.iloc[0:5]","562d27ad":"lr.predict_proba(np.array(X_test.iloc[0:5]))","2b471f90":"lgb_model.predict_proba(np.array(X_test.iloc[0:5]))","228bace6":"import shap\n\n# Create a object that can calculate shap values for our logistic regression model\nexplainer = shap.TreeExplainer(lgb_model)\n\n# Calculate Shap values\nshap_values = explainer.shap_values(np.array(X_test))","dab3b5a8":"shap_values[:,0]","3391c11f":"shap.initjs()\nshap.force_plot(explainer.expected_value, shap_values[0,:], X_test.iloc[0,:])","83fbb997":"shap.force_plot(explainer.expected_value, shap_values[1,:], X_test.iloc[1,:])","13130f0c":"shap.force_plot(explainer.expected_value, shap_values[2,:], X_test.iloc[2,:])","4391d75e":"shap.dependence_plot('sex', shap_values, X_test)","e1c94998":"shap.dependence_plot('ca', shap_values, X_test)","8ee4dd27":"shap.dependence_plot('thalach', shap_values, X_test)","17fbe8d5":"shap.summary_plot(shap_values, X_test)","9a1d4a9b":"#### ca ","4646e8b0":"#### thal","bb3d24c2":"### Partial Dependence Plots","f704b918":"#### LightGBM","f5bca9f7":"### Tune LightGBM","79c73f73":"## Explanatory Data Analysis","659d0725":"### Partial Dependence Plots with Shap","aa640675":"#### XGBoost","3e42f752":"# Heart Disease UCI\n\nDetection if someone suffers of a heart disease is an important task where detection the disease as fast as possible could save lives.\n\nIn this kernel we will try to predict if someone has a hearth disease using algorithms like Logistic Regression, XGBosst, ...\n\nBesides creating a good model we will also interprete the model so we can be sure why it makes certain predictions.\n\n## Table of Content\n1. Explanatory data analysis\n2. Creating categorical variables  \n3. Trying models\n4. Trying models on categorical variables\n5. Tuning parameters  \n    5.1 Tune LogisticRegression  \n    5.2 Tune XGBoost  \n    5.3 Tune LightGBM  \n6. Stacking models  \n7. Interpreting models  \n    7.1 Feature Importance  \n    7.2 Partial Dependence Plots  \n    7.3 2d Partial Dependence Plots  \n    7.4 Shap  ","0be552eb":"### Tune XGBoost","9a0b9753":"#### LightGBM Model","232fab55":"#### Sex","ddc77241":"## Trying model on categorical dataframe","203734d5":"### Shap Values","c71d8ea2":"#### LogisticRegression","2632be8a":"## Creating categorical variables","b03a5895":"## Tuning Parameters","69ca8b58":"## Trying models","81c96a84":"## Stacking Models","4b2cf9dc":"## Conclusion\nThis dataset is pretty small for today's standards but is still gives us a pretty good look at different features that cause heart diseases.\n\nThis notebook isn't complete and I will try to approve it in the upcoming weeks. If you have any recommendations please leave a comment down below.","02a2c7e4":"### Summary plot (simular to feature importance but more information)\n","57ddd572":"### 2d Partial Dependence Plots","0c51dc40":"### Feature importance","a6925b8b":"## Interpreting models","7a7bc76a":"#### CA + Gender","c05ec7bc":"#### Age + Gender","cb65b716":"The data contains the following features:\n* <b>age<\/b> age in years \n* <b>sex<\/b> (1=male; 0=female)\n* <b>cp<\/b> chest pain type\n* <b>trestbps<\/b> resting blood pressure (in mm Hg on admission to the hospital)\n* <b>chol<\/b> serum cholestoral in mg\/dl\n* <b>fbs<\/b> (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false) \n* <b>restecg<\/b> resting electrocardiographic results\n* <b>thalach<\/b> maximum heart rate achieved \n* <b>exang<\/b> exercise induced angina (1 = yes; 0 = no)\n* <b>oldpeak<\/b> ST depression induced by exercise relative to rest \n* <b>slope<\/b> the slope of the peak exercise ST segment \n* <b>ca<\/b> number of major vessels (0-3) colored by flourosopy \n* <b>thal<\/b> 3 = normal; 6 = fixed defect; 7 = reversable defect \n* <b>target<\/b> 1 or 0","85183840":"### Tune LogisticRegression"}}