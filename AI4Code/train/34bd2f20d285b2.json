{"cell_type":{"31eb6f97":"code","01d96cf3":"code","297594c8":"code","f32ad5a2":"code","008b0844":"code","6b8d6842":"code","12242b8a":"code","284f8345":"code","5da60376":"code","0cdf11c9":"code","c0d7b5f9":"code","56e18f42":"code","5dd21d9c":"code","881ac032":"code","d1186b32":"code","88a381f0":"code","828f156c":"code","396f1eb6":"code","2b265cf2":"code","7a3ee9c5":"code","332d7f6d":"code","ff567310":"code","b8fb29ad":"code","27da0957":"code","52365652":"code","fe57af94":"code","05d733c4":"code","5d15f843":"code","ee89ca8f":"markdown","0baaeef7":"markdown","b7604063":"markdown","c2148412":"markdown","600731b0":"markdown","7da1fc22":"markdown","6eda8db9":"markdown","f309c693":"markdown","1669cb1f":"markdown","9944470e":"markdown","fd077284":"markdown","bd5ad62a":"markdown","3b4641ac":"markdown"},"source":{"31eb6f97":"# libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nnp.random.seed(32)\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.manifold import TSNE\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import LSTM, Conv1D, MaxPooling1D, Dropout\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder\nfrom keras.callbacks import EarlyStopping\n\n%matplotlib inline","01d96cf3":"data = pd.read_csv(\"..\/input\/arabic-news-texts-corpus\/arabic_categorization_data.csv\")","297594c8":"data.head()","f32ad5a2":"train_text, test_text, train_y, test_y = train_test_split(data['text'],data['type'],test_size = 0.2, random_state=2019)","008b0844":"MAX_NB_WORDS = 20000\n\n# get the raw text data\ntexts_train = train_text.astype(str)\ntexts_test = test_text.astype(str)\n\n# finally, vectorize the text samples into a 2D integer tensor\ntokenizer = Tokenizer(nb_words=MAX_NB_WORDS, char_level=False)\ntokenizer.fit_on_texts(texts_train)\nsequences = tokenizer.texts_to_sequences(texts_train)\nsequences_test = tokenizer.texts_to_sequences(texts_test)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","6b8d6842":"type(tokenizer.word_index), len(tokenizer.word_index)","12242b8a":"index_to_word = dict((i, w) for w, i in tokenizer.word_index.items())","284f8345":"\" \".join([index_to_word[i] for i in sequences[0]])","5da60376":"seq_lens = [len(s) for s in sequences]\nprint(\"average length: %0.1f\" % np.mean(seq_lens))\nprint(\"max length: %d\" % max(seq_lens))","0cdf11c9":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\nplt.hist(seq_lens, bins=50);\n\n","c0d7b5f9":"plt.hist([l for l in seq_lens if l < 400], bins=50);\n\n","56e18f42":"# pad vectors to maximum length\nMAX_SEQUENCE_LENGTH = 300\n\n# pad sequences with 0s\nx_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\nx_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', x_train.shape)\nprint('Shape of data test tensor:', x_test.shape)\n","5dd21d9c":"# encode y data labels\nencoder = LabelEncoder()\nencoder.fit(train_y)\ny_train = encoder.transform(train_y)\ny_test = encoder.transform(test_y)","881ac032":"\n# Converts the labels to a one-hot representation\nN_CLASSES = np.max(y_train) + 1\ny_train = to_categorical(y_train, N_CLASSES)\ny_test = to_categorical(y_test, N_CLASSES)\nprint('Shape of label tensor:', y_train.shape)","d1186b32":"from keras.layers import Dense, Input, Flatten\nfrom keras.layers import GlobalAveragePooling1D, Embedding\nfrom keras.models import Model\n\nEMBEDDING_DIM = 50\n\n# input: a sequence of MAX_SEQUENCE_LENGTH integers\nsequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n\nembedding_layer = Embedding(MAX_NB_WORDS, EMBEDDING_DIM,\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=True)\n\nembedded_sequences = embedding_layer(sequence_input)\n\naverage = GlobalAveragePooling1D()(embedded_sequences)\npredictions = Dense(N_CLASSES, activation='softmax')(average)\n\nmodel = Model(sequence_input, predictions)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam', metrics=['acc'])","88a381f0":"model.fit(x_train, y_train, validation_split=0.2,\n          nb_epoch=40, batch_size=128, callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])","828f156c":"output_test = model.predict(x_test)\nprint(\"test auc:\", roc_auc_score(y_test,output_test))","396f1eb6":"# Evaluate the accuracy of our trained model\nscore = model.evaluate(x_test, y_test,\n                       batch_size=64, verbose=1)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","2b265cf2":"# Here's how to generate a prediction on individual examples\ntext_labels = encoder.classes_ \n\nfor i in range(50,80):\n    prediction = model.predict(np.array([x_test[i]]))\n    predicted_label = text_labels[np.argmax(prediction)]\n    print(texts_test.iloc[i], \"...\")\n    print('Actual label:' + test_y.iloc[i])\n    print(\"Predicted label: \" + predicted_label + \"\\n\")  ","7a3ee9c5":"# input: a sequence of MAX_SEQUENCE_LENGTH integers\nsequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\n\nx = LSTM(128, dropout=0.5, recurrent_dropout=0.2)(embedded_sequences)\npredictions = Dense(N_CLASSES, activation='softmax')(x)\n\n\nmodel = Model(sequence_input, predictions)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['acc'])","332d7f6d":"model.fit(x_train, y_train, validation_split=0.1,\n          nb_epoch=3, batch_size=64)","ff567310":"output_test = model.predict(x_test)\nprint(\"test auc:\", roc_auc_score(y_test,output_test))","b8fb29ad":"# Evaluate the accuracy of our trained model\nscore = model.evaluate(x_test, y_test,\n                       batch_size=64, verbose=1)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","27da0957":"# input: a sequence of MAX_SEQUENCE_LENGTH integers\nsequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\n\n# 1D convolution with 64 output channels\nx = Conv1D(64, 5)(embedded_sequences)\n# MaxPool divides the length of the sequence by 5\nx = MaxPooling1D(5)(x)\nx = Dropout(0.5)(x)\nx = Conv1D(64, 5)(x)\nx = MaxPooling1D(5)(x)\n# LSTM layer with a hidden size of 64\nx = Dropout(0.3)(x)\nx = LSTM(32)(x)\npredictions = Dense(N_CLASSES, activation='softmax')(x)\n\nmodel = Model(sequence_input, predictions)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['acc'])","52365652":"model.fit(x_train, y_train, validation_split=0.1,\n          nb_epoch=3, batch_size=128)","fe57af94":"# Evaluate the accuracy of our trained model\nscore = model.evaluate(x_test, y_test,\n                       batch_size=64, verbose=1)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","05d733c4":"output_test = model.predict(x_test)\nprint(\"test auc:\", roc_auc_score(y_test,output_test))","5d15f843":"# Here's how to generate a prediction on individual examples\ntext_labels = encoder.classes_ \n\nfor i in range(10):\n    prediction = model.predict(np.array([x_test[i]]))\n    predicted_label = text_labels[np.argmax(prediction)]\n    print(texts_test.iloc[i], \"...\")\n    print('Actual label:' + test_y.iloc[i])\n    print(\"Predicted label: \" + predicted_label + \"\\n\")  ","ee89ca8f":"## A more complex model : CNN - LSTM","0baaeef7":"The model seems to overfit. Indeed the train error is really low whereas the test error is really higher. It seems that we have a variance porblem. Adding regularization like drop-out may help to stabilize the performance\n\nEdit : we have added drop-out. It is still not enough. We need to work on regularization technics to stabilize our performance.\n\nEdit2 : With less epochs, we manage to reduce the variance.","b7604063":"## A complex model : LSTM","c2148412":"* ### there are some biases and overfitiing because of different sizes fo data for ech category, so we will try to find some data to make this data balanced","600731b0":"### The dataset\n","7da1fc22":"let's zoom on the distribution of regular sized posts. The vast majority of the posts have less than 400 symbols:","6eda8db9":"![](http:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/LSTM3-chain.png)\n\nimage taken from : http:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/","f309c693":"Big thanks for this kernels:\n    https:\/\/www.kaggle.com\/eliotbarr\/text-classification-using-neural-networks","1669cb1f":"\n\nLet's have a closer look at the tokenized sequences:\n","9944470e":"### Preprocessing text for the (supervised) CBOW model","fd077284":"The tokenizer object stores a mapping (vocabulary) from word strings to token ids that can be inverted to reconstruct the original message (without formatting):","bd5ad62a":"We will implement a simple classification model in Keras. Raw text requires (sometimes a lot of) preprocessing.\n\nThe following cells uses Keras to preprocess text:\n\n    using a tokenizer. You may use different tokenizers (from scikit-learn, NLTK, custom Python function etc.). This converts the texts into sequences of indices representing the 20000 most frequent words\n    sequences have different lengths, so we pad them (add 0s at the end until the sequence is of length 1000)\n    we convert the output classes as 1-hot encodin","3b4641ac":"\nA simple supervised CBOW model in Keras\n\nVector space model is well known in information retrieval where each document is represented as a vector. The vector components represent weights or importance of each word in the document. The similarity between two documents is computed using the cosine similarity measure.\n\n![](https:\/\/iksinc.files.wordpress.com\/2015\/04\/screen-shot-2015-04-12-at-10-58-21-pm.png?w=768&h=740)"}}