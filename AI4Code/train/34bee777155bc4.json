{"cell_type":{"3503d4da":"code","ae59c105":"code","8f2dd284":"code","ee6b73b0":"code","bab209a4":"markdown","4acb7e03":"markdown","511223a9":"markdown","98672270":"markdown","c442b872":"markdown","fb67f616":"markdown","a0371fba":"markdown","b039699b":"markdown"},"source":{"3503d4da":"!pip install kaggle-environments --upgrade","ae59c105":"%%writefile simple_bandit_agent.py\n\nimport random\nimport numpy as np\nfrom kaggle_environments import make\n\nN = None\nQ = None\nR_total = 0\nepsilon = 0.15\n\ndef simple_bandit_agent(observation, configuration):\n    global N, Q, A, R_total, epsilon\n\n    banditCount = configuration.banditCount\n\n    if observation.step == 0:\n        # Initialize, for a=1 to k:\n        N = np.zeros(banditCount)\n        Q = np.zeros(banditCount)\n    else: # observation.step > 0:\n        # R = bandit(A)\n        R = observation.reward - R_total\n        R_total = observation.reward\n        # ...\n        N[A] += 1\n        Q[A] += (R - Q[A]) \/ N[A]\n\n    if np.random.binomial(1, epsilon, 1):\n        A = np.random.randint(0, banditCount)\n    else:\n        A = np.argmax(Q)\n    return int(A)","8f2dd284":"%%writefile random_agent.py\n\nimport random\n\ndef random_agent(observation, configuration):\n    return random.randrange(configuration.banditCount)","ee6b73b0":"from kaggle_environments import make\n\nenv = make(\"mab\", debug=True)\nenv.reset()\nenv.run([\"simple_bandit_agent.py\", \"random_agent.py\"])\nenv.render(mode=\"ipython\", width=800, height=700)","bab209a4":"# Happy New Year!","4acb7e03":"## a simple bandit algorithm\nMAB(multi-armed bandit)\u1102\u1173\u11ab Stationary or Nonstationary MAB\u1105\u1169 \u1100\u116e\u1107\u116e\u11ab\u1103\u116c\u11b8\u1102\u1175\u1103\u1161. Let's start with a Stationary MAB where the probability distribution of the reward value does not change over time. To simplify the problem and make it easy to approach one by one step, let's implement \"a simple bandit algorithm\" and even submit it to the catalog.\n\n\"a simple bandit algorithm\" is shown below.\n\n\nMAB(multi-armed bandit)\ub294 Stationary or Nonstationary MAB\ub85c \uad6c\ubd84\ub429\ub2c8\ub2e4. \ubcf4\uc0c1\uac12\uc758 \ud655\ub960 \ubd84\ud3ec\uac00 \uc2dc\uac04\uc774 \uc9c0\ub098\ub3c4 \ubcc0\ud654\uc9c0 \uc54a\ub294 Stationary MAB\ub85c \uc2dc\uc791\ud558\uaca0\uc2b5\ub2c8\ub2e4. \ubb38\uc81c\ub97c \ub2e8\uc21c\ud654 \ud558\uace0 \ub2e8\uacc4\uc801\uc73c\ub85c \ud558\ub098\ud558\ub098 \uc27d\uac8c \uc811\uadfc\ud558\uae30 \uc704\ud574 \"a simple bandit algorithm\"\uc744 \uad6c\ud604\ud574\ubcf4\uace0 \uce90\uae00\uc5d0 \uc81c\ucd9c\uae4c\uc9c0 \ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\n\"a simple bandit algorithm\"\uc740 \uc544\ub798\uc640 \uac19\uc2b5\ub2c8\ub2e4.\n\n\nReinforcement Learning An Introduction - Chapter 2: Multi-armed Bandits (2.4)\n![Reinforcement Learning An Introduction - Chapter 2: Multi-armed Bandits (2.4)](https:\/\/t1.daumcdn.net\/thumb\/R1280x0.fpng\/?fname=http:\/\/t1.daumcdn.net\/brunch\/service\/user\/aS4g\/image\/gJO0vssuT_UypLVVoJwIcnp9i-o.png)","511223a9":"## Introduction\n\nIn 2020, the morale of Santa Village was greatly reduced due to the corona. To boost morale, I decided to supply candy cane and hold this competition. You need to use the vending machine in the lounge to supply candy cane. There are 100 vending machines and only 2 elves can fit in the lounge by social distance. 100 vending machines can only pull one lever at a time. Pulling the lever will give you a candy cane and you will get a 1 or 0 reward. Each time the point of consideration pulls the lever, the unknown reward activity decreases at a rate of 0.97. The goal is to develop a strategy to maximize rewards by pulling the lever 2000 times.\n\n2020\ub144 \uc0b0\ud0c0\ub9c8\uc744\uc758 \uc0ac\uae30\uac00 \ucf54\ub85c\ub098\ub85c \uc778\ud574 \ub9ce\uc774 \uc800\ud558 \ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uc0ac\uae30\ub97c \uc62c\ub9ac\uae30 \uc704\ud574 \uc0ac\ud0d5 \uc9c0\ud321\uc774\ub97c \uacf5\uae09\ud558\uae30\ub85c \uacb0\uc815\ud558\uace0 \uc774\ubc88 \ub300\ud68c\ub97c \uac1c\ucd5c\ud558\uac8c \ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uc0ac\ud0d5 \uc9c0\ud321\uc774\ub97c \uacf5\uae09\ud558\uae30 \uc704\ud574\uc11c\ub294 \ud734\uac8c\uc2e4\uc758 \uc790\ud310\uae30\ub97c \uc774\uc6a9\ud574\uc57c \ud569\ub2c8\ub2e4. \uc790\ud310\uae30\ub294 100\ub300\uac00 \uc788\uc73c\uba70 \ud734\uac8c\uc2e4\uc5d0\ub294 \uc0ac\ud68c\uc801\uac70\ub9ac\ub450\uae30\ub85c 2\uba85\uc758 \uc5d8\ud504\ub9cc\uc774 \ub4e4\uc5b4\uac08\uc218 \uc788\uc2b5\ub2c8\ub2e4. 100\ub300\uc758 \uc790\ud310\uae30\ub294 \ud55c\ubc88\uc5d0 \ud55c\uac1c\uc758 \ub808\ubc84\ub9cc \ub2f9\uae38\uc218 \uc788\uc73c\uc2b5\ub2c8\ub2e4. \ub808\ubc84\ub97c \ub2f9\uae30\uba74 \uc0ac\ud0d5 \uc9c0\ud321\uc774\uac00 \uc9c0\uae09\ub300\uba70 1 \ub610\ub294 0\uc758 \ubcf4\uc0c1\uc744 \ubc1b\uac8c \ub429\ub2c8\ub2e4. \uace0\ub824\ud560\uc810\uc774 \ub808\ubc84\ub97c \ub2f9\uae38 \ub54c\ub9c8\ub2e4 \uc54c\ub824\uc9c0\uc9c0 \uc54a\uc740 \ubcf4\uc0c1 \ud65c\ub960\uc740 0.97\uc758 \ube44\uc728\ub85c \uac10\uc18c\ud569\ub2c8\ub2e4. \ubaa9\ud45c\ub294 2000\ubc88\uc758 \ub808\ubc84\ub97c \ub2f9\uae40\uc73c\ub85c\uc368 \ubcf4\uc0c1\uc744 \uadf9\ub300\ud654\ud558\ub294 \uc804\ub7b5\uc744 \ub9c8\ub828\ud558\ub294\uac83 \uc785\ub2c8\ub2e4.","98672270":"The above optimization problem is a classic multi-armed bandit (MAB) problem. It is well explained in the first step of reinforcement learning, so I have extracted it as below.\n\nThe simplest form of reinforcement learning is a bandit with n-handles (=arm), or multi-armed bandit. It's easy to think of a bandit as a slot machine with n handles. Each handle provides compensation with a different probability. The goal of the agent is to maximize return compensation over time by finding the handle that offers the highest compensation and always selecting it.\n...\nThe reason why slot machines with n's are a good starting point when first coming down on reinforcement learning is that there is no need to worry about the dependence of time and the dependence of state. All you have to consider in a slot machine with n-handles is what rewards are associated with which actions and to ensure that we choose the best action. (\u110c\u1165\u11bc\u110e\u1162\u11a8=Policy)\nA policy describes a set of actions in which an agent is commissioned in a given environment. The policy in which the agent obtains maximum rewards within a given environment is considered the optimal policy.\n\n\n\uc704 \ucd5c\uc801\ud654 \ubb38\uc81c\ub294 \uace0\uc804\uc801\uc778 MAB (multi-armed bandit) \ubb38\uc81c \uc785\ub2c8\ub2e4. \uac15\ud654\ud559\uc2b5 \uccab\uac78\uc74c\uc5d0 \uc798 \uc124\uba85\ub418\uc5b4\uc788\uc5b4 \uc544\ub798\uc640 \uac19\uc774 \ubc1c\ucdcc\ud574 \uc654\uc2b5\ub2c8\ub2e4.\n\n\uac15\ud654\ud559\uc2b5\uc5d0\uc11c \uac00\uc7a5 \ub2e8\uc21c\ud55c \ud615\ud0dc\uc758 \ubb38\uc81c\ub294 n\uac1c\uc758 \uc190\uc7a1\uc774(=arm)\uac00 \ub2ec\ub9b0 \ubc34\ub527(bandit), \uc989 \uba40\ud2f0\uc554\ub4dc \ubc34\ub527(multi-armed bandit) \uc785\ub2c8\ub2e4. \uc5ec\uae30\uc11c \ubc34\ub527\uc740 '\uc190\uc7a1\uc774\uac00 n\uac1c\uc778 \uc2ac\ub86f\uba38\uc2e0'\uc774\ub77c\uace0 \uc0dd\uac01\ud558\uba74 \uc27d\uc2b5\ub2c8\ub2e4. \uac01\uac01\uc758 \uc190\uc7a1\uc774\ub294 \ub2e4\ub978 \ud655\ub960\ub85c \ubcf4\uc0c1\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. \uc5d0\uc774\uc804\ud2b8\uc758 \ubaa9\ud45c\ub294 \uc2dc\uac04\uc758 \ud750\ub984\uc5d0 \ub530\ub77c \uac00\uc7a5 \ub192\uc740 \ubcf4\uc0c1\uc744 \uc81c\uacf5\ud558\ub294 \uc190\uc7a1\uc774\ub97c \ucc3e\uc544 \ub0b4\uace0 \ud56d\uc0c1 \uc774 \uc190\uc7a1\uc774\ub97c \uc120\ud0dd\ud568\uc73c\ub85c\uc368 \ub3cc\uc544\uc624\ub294 \ubcf4\uc0c1\uc744 \ucd5c\ub300\ud654\ud558\ub294 \uac83\uc785\ub2c8\ub2e4.\n...\n\ucc98\uc74c \uac15\ud654\ud559\uc2b5\uc5d0 \ub300\ud574 \ud558\uc2b5\ud560 \ub54c n\uac1c\uc778 \uc2ac\ub86f\uba38\uc2e0\uc774 \uc88b\uc740 \uc2dc\uc791\uc810\uc774 \ub418\ub294 \uc774\uc720\ub294 \uc2dc\uac04\uc758\uc874\uc131, \uc0c1\ud0dc\uc758\uc874\uc131\uc5d0 \ub300\ud55c \uace0\ubbfc\ud560 \ud544\uc694\uac00 \uc5c6\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4. \uc190\uc7a1\uc774\uac00 n\uac1c\uc778 \uc2ac\ub86f\uba38\uc2e0\uc5d0\uc11c \uace0\ub824\ud574\uc57c \ud558\ub294 \uac83\uc740, \uc5b4\ub5a4 \ubcf4\uc0c1\uc774 \uc5b4\ub5a4 \uc561\uc158\uacfc \uc5f0\uad00\ub418\uc5b4 \uc788\ub294\uc9c0, \uadf8\ub9ac\uace0 \uc6b0\ub9ac\uac00 \ucd5c\uc801\uc758 \uc561\uc158\uc744 \uc120\ud0dd\ud558\ub3c4\ub85d \ubcf4\uc7a5\ud558\ub294 \uac83\uc774 \uc804\ubd80\uc785\ub2c8\ub2e4. (\uc815\ucc45=Policy)\n\uc815\ucc45\uc740 \uc8fc\uc5b4\uc9c4 \ud658\uacbd\uc758 \uc5b4\ub5a4 \uc0c1\ud669\uc5d0\uc11c \uc5b4\ub5a4 \uc5d0\uc774\uc804\ud2b8\uac00 \ucde8\ud559 \ub418\ub294 \uc77c\ub828\uc758 \uc561\uc158\uc744 \uae30\uc220\ud569\ub2c8\ub2e4. \uc5d0\uc774\uc804\ud2b8\uac00 \uc8fc\uc5b4\uc9c4 \ud658\uacbd \ub0b4\uc5d0\uc11c \ucd5c\ub300\uc758 \ubcf4\uc0c1\uc744 \uc5bb\ub294 \uc815\ucc45\uc744 \ucd5c\uc801\uc758 \uc815\ucc45\uc73c\ub85c \uac04\uc8fc\ud558\uac8c \ub429\ub2c8\ub2e4.\n[\ucd9c\ucc98. \uac15\ud654\ud559\uc2b5 \uccab\uac78\uc74c - 2\uc7a5 \ubca4\ub527 \ubb38\uc81c \uc911]","c442b872":"\"Exploration vs. Exploitation Dilemma\"\uc758 Principles\uc740 \uc544\ub798\uc640 \uac19\uc2b5\ub2c8\ub2e4. \n\n\n## Principles\n\n* Naive(or Ramdom) Exploration  : Add noise to greedy policy (e.g. Epsilon-greedy, SoftMax) \n* Optimistic Initialisation : Assume the best until proven otherwise \n* Optimism in the Face of Uncertainty : Prefer actions with uncertain values (e.g. UCB)\n* Probability Matching : Select actions according to probability they are best (e.g. Thompson sampling)\n* Information State Search : Lookahead search incorporating value of information\n","fb67f616":"## submissions\nWhen I submitted it, I got 479 points.\nIf it helped, please recommend(=vote) it.\n\n\uc81c\ucd9c\ud574\ubcf4\ub2c8 479\uc810\uc744 \ubc1b\uc558\uc2b5\ub2c8\ub2e4.\n\n\ub3c4\uc74c\uc774 \ub418\uc168\ub2e4\uba74 up vote \ubd80\ud0c1\ub4dc\ub9bd\ub2c8\ub2e4.\n\uc870\uae08\ub354 \uc790\uc138\ud55c \uc815\ubcf4\ub294 \ube0c\ub7f0\uce58\ub97c \ucc38\uace0 \ubd80\ud0c1\ub4dc\ub9bd\ub2c8\ub2e4 - https:\/\/brunch.co.kr\/@hansungdev\/23","a0371fba":"![](https:\/\/t1.daumcdn.net\/thumb\/R1280x0.fjpg\/?fname=http:\/\/t1.daumcdn.net\/brunch\/service\/user\/aS4g\/image\/7uHg3miGUTx7NCwcfrptfTJ1psA.JPG)\n[ELLA](https:\/\/www.instagram.com\/ella_lina_joe\/?igshid=1n4kwuk0jqh0j)","b039699b":"Epsilon-greedy, Thompson sampling \ub85c \uad6c\ud604 \ud6c4 \uc81c\ucd9c\ud55c \uc810\uc218\ud558\uace0\ub294 \ucc28\uc774\uac00 \uc788\ub124\uc694.\n\n![](https:\/\/t1.daumcdn.net\/thumb\/R1280x0.fpng\/?fname=http:\/\/t1.daumcdn.net\/brunch\/service\/user\/aS4g\/image\/_dQz6VrnWTOu4D__CWOz0d21fn0.png)"}}