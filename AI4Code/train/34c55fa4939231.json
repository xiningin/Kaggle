{"cell_type":{"3f4e6897":"code","39dec0d0":"code","e3df3dfc":"code","0ce004f9":"code","b059cfaa":"code","7661dade":"code","6c3112cc":"code","77b31449":"code","10f2dda3":"code","bf6bd0dd":"code","77af2f76":"code","ea9ceca6":"code","462ed97a":"code","162086bc":"code","48eab9a3":"code","885e9166":"code","07edd872":"code","98e7ee76":"code","89fd56c4":"code","707d954c":"markdown","bbde2d5d":"markdown","ef24bde6":"markdown","a85c6b32":"markdown"},"source":{"3f4e6897":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","39dec0d0":"# read csv (comma separated value) into data\ntrain = pd.read_csv('..\/input\/train.csv', header=None)\ntrainLabel = pd.read_csv('..\/input\/trainLabels.csv', header=None)\ntest = pd.read_csv('..\/input\/test.csv', header=None)\nprint(plt.style.available) # look at available plot styles\nplt.style.use('ggplot')","e3df3dfc":"print('train shape:', train.shape)\nprint('test shape:', test.shape)\nprint('trainLabel shape:', trainLabel.shape)\ntrain.head()","0ce004f9":"train.info()","b059cfaa":"train.describe()","7661dade":"# KNN with cross-validation\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score, train_test_split\n\nX, y = train, np.ravel(trainLabel)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","6c3112cc":"# Model complexity\nneig = np.arange(1, 25)\nkfold = 10\ntrain_accuracy = []\nval_accuracy = []\nbestKnn = None\nbestAcc = 0.0\n# Loop over different values of k\nfor i, k in enumerate(neig):\n    # k from 1 to 25(exclude)\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # Fit with knn\n    knn.fit(X_train,y_train)\n    #train accuracy\n    train_accuracy.append(knn.score(X_train, y_train))\n    # test accuracy\n    val_accuracy.append(np.mean(cross_val_score(knn, X, y, cv=kfold)))\n    if np.mean(cross_val_score(knn, X, y, cv=kfold)) > bestAcc:\n        bestAcc = np.mean(cross_val_score(knn, X, y, cv=10))\n        bestKnn = knn\n\n# Plot\nplt.figure(figsize=[13,8])\nplt.plot(neig, val_accuracy, label = 'Validation Accuracy')\nplt.plot(neig, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.title('k value VS Accuracy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neig)\nplt.show()\n\nprint('Best Accuracy without feature scaling:', bestAcc)\nprint(bestKnn)","77b31449":"# predict test\ntest_fill = np.nan_to_num(test)\nsubmission = pd.DataFrame(bestKnn.predict(test_fill))\nprint(submission.shape)\nsubmission.columns = ['Solution']\nsubmission['Id'] = np.arange(1,submission.shape[0]+1)\nsubmission = submission[['Id', 'Solution']]\nsubmission","10f2dda3":"submission.to_csv('submission_no_normalization.csv', index=False)","bf6bd0dd":"print(check_output([\"ls\", \"..\/working\"]).decode(\"utf8\"))","77af2f76":"from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n\nstd = StandardScaler()\nX_std = std.fit_transform(X)\nmms = MinMaxScaler()\nX_mms = mms.fit_transform(X)\nnorm = Normalizer()\nX_norm = norm.fit_transform(X)","ea9ceca6":"# Model complexity\nneig = np.arange(1, 30)\nkfold = 10\nval_accuracy = {'std':[], 'mms':[], 'norm':[]}\nbestKnn = None\nbestAcc = 0.0\nbestScaling = None\n# Loop over different values of k\nfor i, k in enumerate(neig):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # validation accuracy\n    s1 = np.mean(cross_val_score(knn, X_std, y, cv=kfold))\n    val_accuracy['std'].append(s1)\n    s2 = np.mean(cross_val_score(knn, X_mms, y, cv=kfold))\n    val_accuracy['mms'].append(s2)\n    s3 = np.mean(cross_val_score(knn, X_norm, y, cv=kfold))\n    val_accuracy['norm'].append(s3)\n    if s1 > bestAcc:\n        bestAcc = s1\n        bestKnn = knn\n        bestScaling = 'std'\n    elif s2 > bestAcc:\n        bestAcc = s2\n        bestKnn = knn\n        bestScaling = 'mms'\n    elif s3 > bestAcc:\n        bestAcc = s3\n        bestKnn = knn\n        bestScaling = 'norm'\n\n# Plot\nplt.figure(figsize=[13,8])\nplt.plot(neig, val_accuracy['std'], label = 'CV Accuracy with std')\nplt.plot(neig, val_accuracy['mms'], label = 'CV Accuracy with mms')\nplt.plot(neig, val_accuracy['norm'], label = 'CV Accuracy with norm')\nplt.legend()\nplt.title('k value VS Accuracy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neig)\nplt.show()\n\nprint('Best Accuracy with feature scaling:', bestAcc)\nprint('Best kNN classifier:', bestKnn)\nprint('Best scaling:', bestScaling)","462ed97a":"# predict on test\nbestKnn.fit(X_norm, y)\nsubmission = pd.DataFrame(bestKnn.predict(norm.transform(test_fill)))\nprint(submission.shape)\nsubmission.columns = ['Solution']\nsubmission['Id'] = np.arange(1,submission.shape[0]+1)\nsubmission = submission[['Id', 'Solution']]\nsubmission","162086bc":"submission.to_csv('submission_with_scaling.csv', index=False)","48eab9a3":"print(check_output([\"ls\", \"..\/working\"]).decode(\"utf8\"))","885e9166":"#correlation map\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(pd.DataFrame(X_std).corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","07edd872":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\n# split data train 70 % and val 30 %\nX_train, X_val, y_train, y_val = train_test_split(X_std, y, test_size=0.3, random_state=42)\n\n#random forest classifier with n_estimators=10 (default)\nclf_rf = RandomForestClassifier(random_state=43)      \nclr_rf = clf_rf.fit(X_train,y_train)\n\nac = accuracy_score(y_val,clf_rf.predict(X_val))\nprint('Accuracy is: ',ac)\ncm = confusion_matrix(y_val,clf_rf.predict(X_val))\nsns.heatmap(cm,annot=True,fmt=\"d\")","98e7ee76":"from sklearn.svm import SVC\nfrom sklearn.feature_selection import RFECV\n\nkfold = 10\nbestSVC = None\nbestAcc = 0.0\nval_accuracy = []\ncv_range = np.arange(5, 11)\nn_feature = []\nfor cv in cv_range:\n    # Create the RFE object and compute a cross-validated score.\n    svc = SVC(kernel=\"linear\")\n    # The \"accuracy\" scoring is proportional to the number of correct\n    # classifications\n    rfecv = RFECV(estimator=svc, step=1, cv=cv, scoring='accuracy')\n    rfecv.fit(X_std, y)\n\n    # print(\"Optimal number of features : %d\" % rfecv.n_features_)\n    # print('Best features :', pd.DataFrame(X_train).columns[rfecv.support_])\n\n    # Model complexity\n    val_accuracy += [np.mean(cross_val_score(svc, X_std[:, rfecv.support_], y, cv=kfold))]\n    n_feature.append(rfecv.n_features_)\n    if val_accuracy[-1] > bestAcc:\n        bestAcc = val_accuracy[-1]\n\n# Plot\nplt.figure(figsize=[13,8])\nplt.plot(cv_range, val_accuracy, label = 'CV Accuracy')\nfor i in range(len(cv_range)):\n    plt.annotate(str(n_feature[i]), xy=(cv_range[i],val_accuracy[i]))\nplt.legend()\nplt.title('Cross Validation Accuracy')\nplt.xlabel('k fold')\nplt.ylabel('Accuracy')\nplt.show()\n\nprint('Best Accuracy with feature scaling and RFECV:', bestAcc)","89fd56c4":"import numpy as np\n#import sklearn as sk\n#import matplotlib.pyplot as plt\nimport pandas as pd\n\n#from sklearn.linear_model import LogisticRegression\n#from sklearn.linear_model import Perceptron\n#from sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.ensemble import VotingClassifier\n#from sklearn import svm\n\n#### READING OUR GIVEN DATA INTO PANDAS DATAFRAME ####\nx_train = train\ny_train = trainLabel\nx_test = test\nx_train = np.asarray(x_train)\ny_train = np.asarray(y_train)\nx_test = np.asarray(x_test)\ny_train = y_train.ravel()\nprint('training_x Shape:',x_train.shape,',training_y Shape:',y_train.shape, ',testing_x Shape:',x_test.shape)\n\n#Checking the models\nx_all = np.r_[x_train,x_test]\nprint('x_all shape :',x_all.shape)\n\n#### USING THE GAUSSIAN MIXTURE MODEL ####\nfrom sklearn.mixture import GaussianMixture\nlowest_bic = np.infty\nbic = []\nn_components_range = range(1, 7)\ncv_types = ['spherical', 'tied', 'diag', 'full']\nfor cv_type in cv_types:\n    for n_components in n_components_range:\n        # Fit a mixture of Gaussians with EM\n        gmm = GaussianMixture(n_components=n_components,covariance_type=cv_type)\n        gmm.fit(x_all)\n        bic.append(gmm.aic(x_all))\n        if bic[-1] < lowest_bic:\n            lowest_bic = bic[-1]\n            best_gmm = gmm\n            \nbest_gmm.fit(x_all)\nx_train = best_gmm.predict_proba(x_train)\nx_test = best_gmm.predict_proba(x_test)\n\n\n#### TAKING ONLY TWO MODELS FOR KEEPING IT SIMPLE ####\nknn = KNeighborsClassifier()\nrf = RandomForestClassifier()\n\nparam_grid = dict( )\n#### GRID SEARCH for BEST TUNING PARAMETERS FOR KNN #####\ngrid_search_knn = GridSearchCV(knn,param_grid=param_grid,cv=10,scoring='accuracy').fit(x_train,y_train)\nprint('best estimator KNN:',grid_search_knn.best_estimator_,'Best Score', grid_search_knn.best_estimator_.score(x_train,y_train))\nknn_best = grid_search_knn.best_estimator_\n\n#### GRID SEARCH for BEST TUNING PARAMETERS FOR RandomForest #####\ngrid_search_rf = GridSearchCV(rf, param_grid=dict( ), verbose=3,scoring='accuracy',cv=10).fit(x_train,y_train)\nprint('best estimator RandomForest:',grid_search_rf.best_estimator_,'Best Score', grid_search_rf.best_estimator_.score(x_train,y_train))\nrf_best = grid_search_rf.best_estimator_\n\n\nknn_best.fit(x_train,y_train)\nprint(knn_best.predict(x_test)[0:10])\nrf_best.fit(x_train,y_train)\nprint(rf_best.predict(x_test)[0:10])\n\n#### SCORING THE MODELS ####\nprint('Score for KNN :',cross_val_score(knn_best,x_train,y_train,cv=10,scoring='accuracy').mean())\nprint('Score for Random Forest :',cross_val_score(rf_best,x_train,y_train,cv=10,scoring='accuracy').max())\n\n### IN CASE WE WERE USING MORE THAN ONE CLASSIFIERS THEN VOTING CLASSIFIER CAN BE USEFUL ###\n#clf = VotingClassifier(\n#\t\testimators=[('knn_best',knn_best),('rf_best',rf_best)],\n#\t\t#weights=[871856020222,0.907895269918]\n#\t)\n#clf.fit(x_train,y_train)\n#print clf.predict(x_test)[0:10]\n\n##### FRAMING OUR SOLUTION #####\nknn_best_pred = pd.DataFrame(knn_best.predict(x_test))\nrf_best_pred = pd.DataFrame(rf_best.predict(x_test))\n#voting_clf_pred = pd.DataFrame(clf.predict(x_test))\n\nknn_best_pred.index += 1\nrf_best_pred.index += 1\n#voting_clf_pred.index += 1\n\nrf_best_pred.columns = ['Solution']\nrf_best_pred['Id'] = np.arange(1,rf_best_pred.shape[0]+1)\nrf_best_pred = rf_best_pred[['Id', 'Solution']]\nprint(rf_best_pred)\n\n#knn_best_pred.to_csv('knn_best_pred.csv')\nrf_best_pred.to_csv('Submission_rf.csv', index=False)\n#voting_clf_pred.to_csv('voting_clf_pred.csv')","707d954c":"# **Feature Selection**","bbde2d5d":"# **Add feature scaling**","ef24bde6":"Univariate feature selection is not very accurate because independent variables may correlate with each other.\nBut it can give us a concept. Here we directly use more robust RFECV.","a85c6b32":"# **Use only kNN for classification**"}}