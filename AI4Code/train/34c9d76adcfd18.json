{"cell_type":{"4d3d134c":"code","be81d861":"code","15f1caa9":"code","018c0e00":"code","16137972":"code","0d964af3":"code","8f2bc4cb":"code","0a1663cb":"code","bb844ffb":"code","b4754c1a":"code","a4f54b4e":"code","ac468d7f":"code","751e218b":"code","5b90d623":"code","4f162ef1":"markdown","b998daba":"markdown","34821c10":"markdown","9c2deff8":"markdown","cb10f477":"markdown","651c08a8":"markdown"},"source":{"4d3d134c":"# this library is required if you want to train the roformer tokenizer instead of the bert tokenizer\n!pip install rjieba","be81d861":"import os\nimport random\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GroupKFold\nfrom transformers import TFRoFormerForTokenClassification, RoFormerTokenizer, RoFormerTokenizerFast, AutoTokenizer, BertTokenizerFast\nfrom transformers import RoFormerConfig, AutoConfig, TFAutoModel, TFRoFormerModel, AdamWeightDecay, TFRoFormerForSequenceClassification, TFAutoModelForSequenceClassification\nimport pandas as pd, numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nprint('TF version',tf.__version__)","15f1caa9":"def auto_select_accelerator():\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        print(\"Running on TPU:\", tpu.master())\n    except ValueError:\n        strategy = tf.distribute.get_strategy()\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    \n    return strategy","018c0e00":"train = pd.read_csv('..\/input\/feedback-prize-2021\/train.csv')\nprint( train.shape )\ntrain.head()","16137972":"print('The train labels are:')\ntrain.discourse_type.unique()","0d964af3":"IDS = train.id.unique()\nprint('There are',len(IDS),'train texts.')","8f2bc4cb":"# TOKENIZER\n# Since the English version of RoFormer is under development, and Bert has the same structure, I use the Bert tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking\")","0a1663cb":"# THE TOKENS AND ATTENTION ARRAYS\ntargets = np.load('..\/input\/roformer-tpu\/roformer_tpu\/targets_bert_large_uwwm_1024.npy')\ntrain_tokens = np.load('..\/input\/roformer-tpu\/roformer_tpu\/tokens_bert_large_uwwm_1024.npy')\ntrain_attention = np.load('..\/input\/roformer-tpu\/roformer_tpu\/attention_bert_large_uwwm_1024.npy')\nprint('Loaded NER tokens')","bb844ffb":"from transformers import RoFormerConfig\ndef build_model():\n    \n    tokens = tf.keras.layers.Input(shape=(MAX_LEN,), name = 'tokens', dtype=tf.int32)\n    attention = tf.keras.layers.Input(shape=(MAX_LEN,), name = 'attention', dtype=tf.int32)\n    \n    config = RoFormerConfig('junnyu\/roformer_chinese_base')\n    backbone = TFRoFormerModel.from_pretrained('junnyu\/roformer_chinese_base', config=config)\n    x = backbone(tokens, attention_mask=attention)\n    \n    x = tf.keras.layers.Dense(256, activation='relu')(x[0])\n    x = tf.keras.layers.Dense(15, activation='softmax', dtype='float32')(x)\n    \n    model = tf.keras.Model(inputs=[tokens,attention], outputs=output) #outputs=x)\n    # model.load_weights(f'\/content\/drive\/MyDrive\/feedback-prize\/save\/best_r_bert_notcls_luwwm_v4_bs.h5')\n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-5), #AdamWeightDecay(learning_rate=3e-5, weight_decay_rate=0.5), #tf.keras.optimizers.Nadam(learning_rate = 3e-5), #(lr = 1e-5), #Adam, Nadam, Adamax\n                  loss = [tf.keras.losses.CategoricalCrossentropy()],\n                  metrics = [tf.keras.metrics.CategoricalAccuracy()])\n    \n    return model","b4754c1a":"MAX_LEN = 1024\nstrategy = auto_select_accelerator()\n# BATCH_SIZE = strategy.num_replicas_in_sync\/2 #* 16\nwith strategy.scope():\n    model = build_model()","a4f54b4e":"# TRAIN VALID SPLIT 90% 10%\nnp.random.seed(42)\ntrain_idx = np.random.choice(np.arange(len(IDS)),int(0.9*len(IDS)),replace=False)\nvalid_idx = np.setdiff1d(np.arange(len(IDS)),train_idx)\nnp.random.seed(None)\nprint('Train size',len(train_idx),', Valid size',len(valid_idx))","ac468d7f":"# LEARNING RATE SCHEDULE AND MODEL CHECKPOINT\nEPOCHS = 15\nLRS = [0.25e-4, 0.25e-4, 0.25e-4, 0.25e-4, 0.25e-5, 0.25e-5, 0.25e-5, 0.25e-6, 0.25e-6, 0.25e-6, 0.25e-6, 0.25e-6, 0.25e-6, 0.25e-6, 0.25e-6] \ndef lrfn(epoch):\n    return LRS[epoch]","751e218b":"steps_per_epoch = train_idx.shape[0]\/\/16 #BATCH_SIZE\n\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(\n        f'.\/best_roformer_luwwm.h5', save_best_only=True, monitor='val_categorical_accuracy', mode='max', save_weights_only=True) #'min') 'auto')\n\nlr_reducer = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor=\"val_categorical_accuracy\", patience=3, min_lr=1e-8, mode='max')\n\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n\nmodel.fit(x = [train_tokens[train_idx,], train_attention[train_idx,]],\n      y = targets[train_idx,],\n      validation_data = ([train_tokens[valid_idx,], train_attention[valid_idx,]],\n                         targets[valid_idx,]),\n      callbacks =[checkpoint], #[checkpoint, lr_reducer], #[checkpoint, lr_callback],\n      epochs = 10, #EPOCHS,\n      steps_per_epoch=steps_per_epoch,\n      batch_size = 16, #BATCH_SIZE,\n      verbose = 2)\n\n# SAVE MODEL WEIGHTS\n\n# with open('.\/config.json', 'w') as f1:\n#     json.dump(model.get_config(), f1, indent=4)\n\n# model.save_weights(f'.\/roformer_bert_luwwm_v1_w.h5')\n# tokenizer.save_pretrained('.\/') # \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442\n# model.save(f'.\/roformer_bert_lowwm_v1_m.h5')","5b90d623":"from IPython.display import FileLink\nFileLink(r'.\/best_roformer_luwwm.h5')","4f162ef1":"# Build Model","b998daba":"# Load Libraries","34821c10":"# Load Train","9c2deff8":"### The abstract from the paper is the following:\n\n> Position encoding in transformer architecture provides supervision for dependency modeling between elements at different positions in the sequence. We investigate various methods to encode positional information in transformer-based language models and propose a novel implementation named Rotary Position Embedding(RoPE). The proposed RoPE encodes absolute positional information with rotation matrix and naturally incorporates explicit relative position dependency in self-attention formulation. Notably, RoPE comes with valuable properties such as flexibility of being expand to any sequence lengths, decaying inter-token dependency with increasing relative distances, and capability of equipping the linear self-attention with relative position encoding. As a result, the enhanced transformer with rotary position embedding, or RoFormer, achieves superior performance in tasks with long texts. We release the theoretical analysis along with some preliminary experiment results on Chinese data. The undergoing experiment for English benchmark will soon be updated.","cb10f477":"# Train or Load Model","651c08a8":"#### notebook based on this wonderful work: [https:\/\/www.kaggle.com\/cdeotte\/tensorflow-longformer-ner-cv-0-633](http:\/\/)"}}