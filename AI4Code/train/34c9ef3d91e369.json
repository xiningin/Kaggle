{"cell_type":{"ef113f07":"code","5a7dad4f":"code","674e6c4d":"code","f387e509":"code","9e4a8a56":"code","585a377b":"code","303f4384":"code","6f12fe20":"code","907defed":"code","3f9fe7ed":"code","598bea5a":"code","a747d9a6":"code","5e4e526f":"code","77c9b9b8":"code","ea6c5a1a":"code","47e03cac":"code","a9f3e8bf":"code","fe616698":"code","72d231f8":"code","8ac8e6d6":"code","830f72c7":"code","9ba56d5f":"code","61109483":"code","bf4fe2ea":"code","711ed798":"code","317b4e40":"code","99e7a6c8":"code","f15266fe":"code","05352feb":"code","ff637a73":"code","47c55e70":"code","a4240fd7":"code","2804d648":"code","16efb376":"code","29745a15":"code","19f29aec":"code","0a3d6b83":"code","a109f965":"code","560b0b10":"code","f0896340":"code","1ef4921f":"code","a7a0636d":"code","ab74dcd5":"code","07dca661":"code","f8d907ad":"code","c6951352":"code","1394c993":"code","5cc70f59":"code","0313c5e7":"code","c544ee03":"code","6e6cbf9f":"code","ea854302":"markdown","36f2c6ad":"markdown","c7d115a4":"markdown","a034600b":"markdown","f4e4e2f6":"markdown","523a2e0f":"markdown","60507062":"markdown","cdfe236c":"markdown","2778e4a0":"markdown","2af37f4a":"markdown","6030565e":"markdown","a595366d":"markdown","b9e89bbd":"markdown","251d0c99":"markdown","40e3acc5":"markdown","f9e29463":"markdown","4b30ae66":"markdown","f9ee5c98":"markdown","2607f004":"markdown","11e299ae":"markdown"},"source":{"ef113f07":"import sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')","5a7dad4f":"import pandas as pd\nimport numpy as np\nimport os\nimport tensorflow as tf\nimport sklearn.model_selection as ms\nimport sklearn.preprocessing as prep\nimport seaborn as sns\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nimport sklearn.metrics as metric","674e6c4d":"class CrossValidation:\n    def __init__(self, df, target_cols, shuffle,random_state=0):\n        self.df = df\n        self.target_cols = target_cols\n        self.random_state = 0\n        if shuffle is True:\n            self.df = df.sample(frac=1).reset_index(drop=True)\n\n    def hold_out_split(self,percent,stratify=True):\n        if stratify:\n            y = self.df[self.target_cols]\n            train,val = ms.train_test_split(self.df,test_size=percent\/100,\n                stratify=y, random_state=self.random_state)\n            return train,val\n        size = len(self.df) - int(len(self.df)*(percent\/100))\n        train = self.df.iloc[:size,:]\n        val = self.df.iloc[size:,:]\n        return train,val\n\n    def kfold_split(self, splits, stratify=None):\n        if stratify is not None:\n            if type('stratify')==\"str\":\n                kf = ms.StratifiedKFold(n_splits=splits)\n                y = self.df[stratify].values\n                for train, val in kf.split(X=self.df,y=y):\n                    t = self.df.iloc[train,:]\n                    v = self.df.iloc[val, :]\n                    yield t,v\n            else:\n                kf = MultilabelStratifiedKFold(n_splits=splits)\n                y = self.df[stratify].values\n                for train, val in kf.split(X=self.df,y=y):\n                    t = self.df.iloc[train,:]\n                    v = self.df.iloc[val, :]\n                    yield t,v\n        else:\n            kf = ms.KFold(n_splits=splits)\n            for train, val in kf.split(X=self.df):\n                t = self.df.iloc[train,:]\n                v = self.df.iloc[val, :]\n                yield t,v","f387e509":"class EncodeCategories:\n    def __init__(self, df, encode_cols, encoding_type, \n        handle_na=False, na_placeholder=\"NaN\"):\n        self.df = df\n        self.encode_cols = encode_cols\n        self.encoding_type = encoding_type\n        self.handle_na = handle_na\n\n        self.label_encoders = {}\n        self.binary_encoders = {}\n        self.one_hot_encoder = None\n        self.na_placeholder = na_placeholder\n\n        if self.handle_na:\n            self.df = self.__handle_missing_category(self.df, \n                placeholder=na_placeholder)\n\n    def __handle_missing_category(self, df, placeholder=\"NaN\"):\n        for cat in self.encode_cols:\n            df.loc[:, cat] = df.loc[:, cat].astype(str).fillna(placeholder)\n        return df\n\n    def __label_encoder_fit(self,df, cat):\n        le = prep.LabelEncoder()\n        le.fit(self.df[cat].values)\n        self.label_encoders[cat] = le\n\n    def __label_encoder_transform(self,df, cat):\n        return self.label_encoders[cat].transform(df[cat].values)\n\n    def __binary_encoder_fit(self,df, cat):\n        lbl = prep.LabelBinarizer()\n        lbl.fit(self.df[cat].values)\n        self.binary_encoders[cat] = lbl\n\n    def __binary_encoder_transform(self,df, cat):\n        return self.binary_encoders[cat].transform(df[cat].values)\n\n    def __one_hot_fit(self,df, sparse=False):\n        ohe = prep.OneHotEncoder(sparse=sparse)\n        ohe.fit(self.df[self.encode_cols].values)\n        self.one_hot_encoder = ohe\n\n    def __one_hot_transform(self,df, cat):\n        return self.one_hot_encoder.transform(df[cat].values)\n\n    def __label_encoder(self, df, fit=True):\n        for cat in self.encode_cols:\n            if fit:\n                self.__label_encoder_fit(df,cat)\n            df.loc[:,cat] = self.__label_encoder_transform(df,cat)\n        return df\n\n    def __binary_encoder(self, df, fit=True):\n        for cat in self.encode_cols:\n            if fit:\n                self.__binary_encoder_fit(df,cat)\n            val = self.__binary_encoder_transform(df, cat)\n            df = df.drop(cat, axis=1)\n            for i in range(val.shape[1]):\n                new_col_name = f\"{cat}_bin_{i}\"\n                df[new_col_name] = val[:, i]\n        return df\n\n    def __one_hot_encoder(self, df, sparse=False, fit=True):\n        if fit:\n            self.__one_hot_fit(df, sparse)\n        val = self.__one_hot_transform(df, self.encode_cols)\n        for cat in self.encode_cols:\n            df = df.drop(cat, axis=1)\n            for i in range(val.shape[1]):\n                new_col_name = f\"{cat}_ohe_{i}\"\n                df[new_col_name] = val[:, i]\n        return df\n\n    def fit(self):\n        if self.encoding_type == \"label\":\n            for cat in self.encode_cols:\n                self.__label_encoder_fit(self.df,cat)\n        elif self.encoding_type == \"binary\":\n            for cat in self.encode_cols:\n                self.__binary_encoder_fit(self.df,cat)\n        elif self.encoding_type == \"onehot\":\n            self.__one_hot_fit(self.df, False)\n        elif self.encoding_type == \"onehot_sparse\":\n            self.__one_hot_fit(self.df, True)\n        else:\n            raise Exception(\"specified encoding type not defined\")\n\n    def fit_transform(self):\n        df = self.df.copy(deep=True)\n        if self.encoding_type == \"label\":\n            return self.__label_encoder(df)\n        elif self.encoding_type == \"binary\":\n            return self.__binary_encoder(df)\n        elif self.encoding_type == \"onehot\":\n            return self.__one_hot_encoder(df)\n        elif self.encoding_type == \"onehot_sparse\":\n            return self.__one_hot_encoder(df, True)\n        else:\n            raise Exception(\"specified encoding type not defined\")\n\n    def transform(self,dataframe):\n        if self.handle_na:\n            dataframe = self.__handle_missing_category(dataframe, \n                placeholder=self.na_placeholder)\n        df = dataframe.copy(deep=True)\n        if self.encoding_type == \"label\":\n            return self.__label_encoder(df, fit=False)\n        elif self.encoding_type == \"binary\":\n            return self.__binary_encoder(df, fit=False)\n        elif self.encoding_type == \"onehot\":\n            return self.__one_hot_encoder(df, sparse=False, fit=False)\n        elif self.encoding_type == \"onehot_sparse\":\n            return self.__one_hot_encoder(df, sparse=True, fit=False)\n        else:\n            raise Exception(\"specified encoding type not defined\")","9e4a8a56":"class TFSimpleDataset:\n    def __init__(self,batch_size, repeat,\n        drop_remainder_in_batch=False, \n        num_parallel_calls=tf.data.experimental.AUTOTUNE,\n        buffer_size=tf.data.experimental.AUTOTUNE):\n        self.batch_size = batch_size\n        self.drop_remainder = drop_remainder_in_batch\n        self.num_parallel_calls = num_parallel_calls\n        self.buffer_size = buffer_size\n        self.repeat = repeat\n\n    def create_dataset(self, X, Y=None):\n        datasetX = tf.data.Dataset.from_tensor_slices(X)\n        if Y is not None :\n            datasetY = tf.data.Dataset.from_tensor_slices(Y)\n            dataset = tf.data.Dataset.zip((datasetX,datasetY))\n        else:\n            dataset = datasetX\n        dataset = dataset.batch(self.batch_size, \n            drop_remainder=self.drop_remainder)\n        if self.repeat:\n            dataset = dataset.repeat()\n        dataset = dataset.prefetch(buffer_size=self.buffer_size)\n        return dataset","585a377b":"def write_submission(preds):\n    sub_pred = preds.transpose()\n    submission = pd.read_csv(\"..\/input\/lish-moa\/sample_submission.csv\")\n    for i, col in enumerate(target_cols):\n        submission[col]= sub_pred[i]\n    return submission","303f4384":"train_features = pd.read_csv(\"..\/input\/lish-moa\/train_features.csv\")\ntest_features = pd.read_csv(\"..\/input\/lish-moa\/test_features.csv\")\ntrain_targets = pd.read_csv(\"..\/input\/lish-moa\/train_targets_scored.csv\")","6f12fe20":"train_features.head()","907defed":"sns.countplot('cp_type',data=train_features)","3f9fe7ed":"sns.countplot('cp_dose',data=train_features)","598bea5a":"sns.countplot('cp_time',data=train_features)","a747d9a6":"test_features.head()","5e4e526f":"sns.countplot('cp_type',data=test_features)","77c9b9b8":"sns.countplot('cp_dose',data=test_features)","ea6c5a1a":"sns.countplot('cp_time',data=test_features)","47e03cac":"train_targets.head()","a9f3e8bf":"len(train_targets.columns)","fe616698":"enc = EncodeCategories(train_features, encode_cols=[\"cp_type\", \"cp_dose\", \"cp_time\"], \n                       encoding_type=\"onehot\")\ntrain_features = enc.fit_transform()","72d231f8":"test_df = enc.transform(test_features)","8ac8e6d6":"train_df = train_features.merge(train_targets, on=\"sig_id\")\ntrain_df.head()","830f72c7":"len(train_df.columns)","9ba56d5f":"train_df = train_df.drop(\"sig_id\", axis=1)","61109483":"train_df.head()","bf4fe2ea":"test_df = test_df.drop(\"sig_id\", axis=1)","711ed798":"test_df.head()","317b4e40":"target_cols = [t for t in train_targets.columns if not t==\"sig_id\"]\nprint(len(target_cols))","99e7a6c8":"input_features = [t for t in train_features.columns if not t==\"sig_id\"]\nprint(len(input_features))","f15266fe":"cv = CrossValidation(train_df, target_cols, shuffle=True, random_state=11)","05352feb":"metrices = [\"AUC\"]","ff637a73":"def get_model(hidden_layers_units):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Input(shape=len(input_features)),\n        tf.keras.layers.Dense(len(input_features)),\n        tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.LeakyReLU(),\n    ])\n    for units in hidden_layers_units:\n        model.add(tf.keras.layers.Dense(units))\n        model.add(tf.keras.layers.Dropout(0.2))\n        model.add(tf.keras.layers.LeakyReLU())\n    model.add(tf.keras.layers.Dense(len(target_cols), activation=\"sigmoid\"))\n    model.compile(loss=\"binary_crossentropy\", metrics=metrices, optimizer=\"adam\")\n    return model","47c55e70":"LR_START = 0.00001\nLR_MAX = 0.00005\nLR_MIN = 0.00001\nLR_RAMPUP_EPOCHS = 5\nLR_SUSTAIN_EPOCHS = 0\nLR_EXP_DECAY = .8\n\ndef change_lr(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) \/ LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr","a4240fd7":"def get_callbacks(fold):\n    model_checkpointer = tf.keras.callbacks.ModelCheckpoint(\n        f\"best_model{fold}.h5\",\n        monitor=\"val_auc\",\n        verbose=1,\n        save_best_only=True,\n        mode=\"max\"\n    )\n    early_stop = tf.keras.callbacks.EarlyStopping(\n        monitor=\"val_auc\",\n        min_delta=0,\n        patience=5,\n        verbose=1,\n        mode=\"max\"\n    )\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(change_lr, verbose=True)\n    \n    return [model_checkpointer,early_stop,lr_callback]","2804d648":"batch_size=128\nepochs=100\nhidden_layers=[1200]\nnum_folds=5","16efb376":"tf_dataset_obj = TFSimpleDataset(batch_size=batch_size,\n                                repeat=False)","29745a15":"k_fold_models=[]","19f29aec":"f=1\nval_auc = []\ntrain_auc = []\nfor train, val in cv.kfold_split(splits=num_folds, stratify= target_cols):\n    print(\"Fold: \",f)\n    trainX = train.iloc[:,:len(input_features)].values\n    trainY = train.iloc[:,len(input_features):].values\n    valX = val.iloc[:,:len(input_features)].values\n    valY = val.iloc[:,len(input_features):].values\n    train_dataset = tf_dataset_obj.create_dataset(X=trainX, Y=trainY)\n    val_dataset = tf_dataset_obj.create_dataset(X=valX,Y=valY)\n    model = get_model(hidden_layers)\n    k_fold_models.append(model)\n    callbacks = get_callbacks(f)\n    history = model.fit(train_dataset, epochs=epochs,\n              validation_data = val_dataset,\n              callbacks = callbacks)\n    val_auc.append(np.max(history.history['val_auc']))\n    train_auc.append(np.max(history.history['auc']))\n    f+=1","0a3d6b83":"for val_acc in val_auc:\n    print(val_acc)","a109f965":"for train_acc in train_auc:\n    print(train_acc)","560b0b10":"print(np.mean(val_auc))\nprint(np.mean(train_auc))","f0896340":"for i, model in enumerate(k_fold_models):\n    weights = f\"best_model{i+1}.h5\"\n    model.load_weights(weights)","1ef4921f":"def mean_ensemble(models, dataset):\n    predictions = []\n    for model in models:\n        prediction = model.predict(dataset, verbose=1)\n        predictions.append(prediction)\n    predictions = np.mean(predictions, axis=0)\n    return predictions","a7a0636d":"def max_ensemble(models, dataset):\n    predictions = []\n    for model in models:\n        prediction = model.predict(dataset, verbose=1)\n        predictions.append(prediction)\n    predictions = np.max(predictions, axis=0)\n    return predictions","ab74dcd5":"def ensemble_accuracy(models, ensemble_fn, num_folds=5):\n    f=1\n    val_fold_auc = []\n    train_fold_auc = []\n    for train, val in cv.kfold_split(splits=num_folds, stratify= target_cols):\n        print(\"Fold: \",f)\n        trainX = train.iloc[:,:len(input_features)].values\n        trainY = train.iloc[:,len(input_features):].values\n        valX = val.iloc[:,:len(input_features)].values\n        valY = val.iloc[:,len(input_features):].values\n        train_dataset = tf_dataset_obj.create_dataset(X=trainX)\n        val_dataset = tf_dataset_obj.create_dataset(X=valX)\n        prediction_train = ensemble_fn(models, train_dataset)\n        metric_train = tf.keras.metrics.AUC()\n        metric_train.update_state(trainY, prediction_train)\n        train_fold_auc.append(metric_train.result().numpy())\n\n        prediction_val = ensemble_fn(models, val_dataset)\n        metric_val = tf.keras.metrics.AUC()\n        metric_val.update_state(valY, prediction_val)\n        val_fold_auc.append(metric_val.result().numpy())\n        f+=1\n    return train_fold_auc, val_fold_auc","07dca661":"train_auc, val_auc= ensemble_accuracy(k_fold_models,mean_ensemble, num_folds=5)\nprint(\"\\n======AUC=========\")\nprint(\"Train AUC\", train_auc)\nprint(\"Validation AUC: \",val_auc)","f8d907ad":"train_auc, val_auc= ensemble_accuracy(k_fold_models,max_ensemble, num_folds=5)\nprint(\"\\n======AUC=========\")\nprint(\"Train AUC\", train_auc)\nprint(\"Validation AUC: \",val_auc)","c6951352":"best_val_model = k_fold_models[np.argmax(val_auc)]\ntrain_auc, val_auc= ensemble_accuracy([best_val_model],mean_ensemble, num_folds=5)\nprint(\"\\n======AUC=========\")\nprint(\"Train AUC\", train_auc)\nprint(\"Validation AUC: \",val_auc)","1394c993":"test_dataset = tf_dataset_obj.create_dataset(X=test_df.values)","5cc70f59":"# mean_predictions = mean_ensemble(k_fold_models,test_dataset)\nmax_predictions = max_ensemble(k_fold_models,test_dataset)","0313c5e7":"# print(mean_predictions.shape)\nprint(max_predictions.shape)","c544ee03":"# submission_mean = write_submission(mean_predictions)\n# submission_mean.to_csv(\"submission.csv\", index=False)\n# submission_mean.head()","6e6cbf9f":"submission_max = write_submission(max_predictions)\nsubmission_max.to_csv(\"submission.csv\", index=False)\nsubmission_max.head()","ea854302":"## Helper Classes and Functions\n\n- CrossValidation: Splitting data into training and validation sets\n- EncodeCategories: Encode categorical variables\n- TFSimpleDataset: Create tfdataset input pipeline from data\n- write_submission: Write submission dataframe","36f2c6ad":"Encode Categorical variables into onehot","c7d115a4":"Parameters to tune","a034600b":"Checking up model max validation AUC","f4e4e2f6":"## importing necessary libraries","523a2e0f":"## Loading, visualizing and setting up data","60507062":"list of target columns in dataframe","cdfe236c":"Loading best weights for inferencing","2778e4a0":"## Ensemble Methods\n\n- mean_ensemble: Ensemble models prediction by averaging predictions from different models\n- max_ensemble: Ensemble models prediction by taking max prediction from different models\n- ensemble_accuracy: Check AUC score of ensembled predictions","2af37f4a":"## Test data predictions for submission","6030565e":"Merge target dataframe with training features on basis of their respective id","a595366d":"encoding categories of test data too","b9e89bbd":"Average AUC","251d0c99":"Training model with 5 folds","40e3acc5":"## Defining model, callbacks and metrices","f9e29463":"list of input columns in dataframe","4b30ae66":"## Training the model","f9ee5c98":"## Checking ensembled AUC with respect to best model","2607f004":"# Mechanisms of Action (MoA) Prediction","11e299ae":"Splitting data into folds"}}