{"cell_type":{"1e5f53f2":"code","cea418a5":"code","a899cb9f":"code","11f2e806":"code","d87f009d":"code","b1d5c6f3":"code","9fb017dc":"code","986521fe":"code","f60cf290":"code","e061eded":"code","82bc1102":"code","9f3d5e39":"code","16c631c5":"code","86941a90":"code","3ab1df2a":"code","0096b292":"code","f6f499c5":"code","6fecc001":"code","5a9bb5a1":"code","e2a0e6e3":"code","ccf52862":"markdown","2acfb32c":"markdown","d091c95a":"markdown","aa6c81dc":"markdown"},"source":{"1e5f53f2":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","cea418a5":"#import libraries\n\nimport numpy as np\nfrom collections import Counter\nimport pandas as pd\nimport lightgbm as lgbm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import mean_squared_error,roc_auc_score,precision_score,accuracy_score,log_loss\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_selection import mutual_info_regression\n\nfrom functools import partial\nimport optuna","a899cb9f":"df_train= pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/train.csv',index_col=0)\ndf_test = pd.read_csv('\/kaggle\/input\/tabular-playground-series-nov-2021\/test.csv',index_col=0)","11f2e806":"df_train.head()","d87f009d":"df_train.describe()","b1d5c6f3":"ax =sns.countplot(x='target',data=df_train)\nax.set_xticklabels(['Spam','Ham'],ha=\"right\")\nplt.title(\"Spam or Ham\")\nplt.style.use(\"seaborn-whitegrid\")\ntotal= len(df_train.target)\nfor p in ax.patches:\n    percentage = f'{100 * p.get_height() \/ total:.1f}%\\n'\n    x = p.get_x() + p.get_width() \/ 2\n    y = p.get_height()\n    ax.annotate(percentage, (x, y), ha='center', va='center')","9fb017dc":"#separate targets\n\ny=df_train['target']\nfeatures =df_train.drop(['target'],axis=1)\n","986521fe":"from sklearn import preprocessing\n#scaler = preprocessing.MinMaxScaler()\nscaler = preprocessing.StandardScaler()\n\nfeatures[:]= scaler.fit_transform(features[:])\n\ndf_test[:] =scaler.transform(df_test[:])","f60cf290":"#split the data to train and test.\n\nX_train,X_test,y_train,y_test = train_test_split(features,y,test_size=0.3,random_state=0)\n","e061eded":"#from sklearn.feature_selection import SelectKBest\n#from sklearn.feature_selection import chi2\n# find best scored 75 features\n#select_feature = SelectKBest(chi2, k=75).fit(X_train, y_train)","82bc1102":"#print('Score list:', select_feature.scores_)\n","9f3d5e39":"#X_train_2 = pd.DataFrame(select_feature.transform(X_train[:]),columns= range(75))\n                         \n#X_test_2 = pd.DataFrame(select_feature.transform(X_test[:]),columns=(range(75)))","16c631c5":"def objective(trial,X,y, name='xgb'):\n    params = param = {\n        'tree_method':'gpu_hist',  \n        'lambda': trial.suggest_loguniform(\n            'lambda', 1e-3, 10.0\n        ),\n        'alpha': trial.suggest_loguniform(\n            'alpha', 1e-3, 10.0\n        ),\n        'colsample_bytree': trial.suggest_categorical(\n            'colsample_bytree', [0.5,0.6,0.7,0.8,0.9,1.0]\n        ),\n        'subsample': trial.suggest_categorical(\n            'subsample', [0.6,0.7,0.8,1.0]\n        ),\n        'learning_rate': trial.suggest_categorical(\n            'learning_rate', [0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02]\n        ),\n        'n_estimators': trial.suggest_categorical(\n            \"n_estimators\", [150, 200, 300, 3000]\n        ),\n        'max_depth': trial.suggest_categorical(\n            'max_depth', [4,5,7,9,11,13,15,17]\n        ),\n        'random_state': 42,\n        'min_child_weight': trial.suggest_int(\n            'min_child_weight', 1, 300\n        ),\n    }\n\n    model =  XGBRegressor(**params)\n    model.fit(X_train,y_train,eval_set=[(X_test,y_test)],early_stopping_rounds=50,verbose=False)\n\n\n    train_score = np.round(np.sqrt(mean_squared_error(y_train, model.predict(X_train))), 5)\n    test_score = np.round(np.sqrt(mean_squared_error(y_test, model.predict(X_test))), 5)\n                  \n    print(f'TRAIN RMSE : {train_score} || TEST RMSE : {test_score}')\n                  \n    return test_score","86941a90":"#%%time\n#optimize = partial(objective,X=X_train,y=y_train)\n\n#study_lgbm = optuna.create_study(direction ='minimize')\n#study_lgbm.optimize(optimize,n_trials=50)\n\n\n#[I 2021-11-26 19:19:12,114] Trial 29 finished with value: 0.45579 and parameters: {'lambda': 0.03305191955953313, 'alpha': 0.08282886432419537, 'colsample_bytree': 0.8, 'subsample': 0.6, 'learning_rate': 0.008, 'n_estimators': 3000, 'max_depth': 13, 'min_child_weight': 299}. Best is trial 29 with value: 0.45579.\n#TRAIN RMSE : 0.39394 || TEST RMSE : 0.45579","3ab1df2a":"#print(f\"\\tBest value (rmse): {study_lgbm.best_value:.5f}\")\n#print(f\"\\tBest params:\")\n\n#for key, value in study_lgbm.best_params.items():\n#   print(f\"\\t\\t{key}: {value}\")\n\n#Best value (rmse): 0.45579\n#\tBest params:\n#\t\tlambda: 0.03305191955953313\n#\t\talpha: 0.08282886432419537\n#\t\tcolsample_bytree: 0.8\n#\t\tsubsample: 0.6\n#\t\tlearning_rate: 0.008\n#\t\tn_estimators: 3000\n#\t\tmax_depth: 13\n#\t\tmin_child_weight: 299","0096b292":"params ={'lambda': 0.03305191955953313,\n'alpha': 0.08282886432419537,\n'colsample_bytree': 0.8,\n'subsample': 0.6,\n'learning_rate': 0.008,\n'n_estimators': 3000,\n'max_depth': 13,\n'min_child_weight': 299}","f6f499c5":"model =  XGBRegressor(**params,tree_method='gpu_hist')\nmodel.fit(X_train,y_train)\n\ny_pred = model.predict(X_test)","6fecc001":"mean_squared_error(y_test,y_pred,squared=False)","5a9bb5a1":"#df_test_2=pd.DataFrame(select_feature.transform(df_test[:]),columns=range(75))","e2a0e6e3":"#making submission.csv\npredictions=model.predict(df_test)\noutput =pd.DataFrame({'Id': df_test.index,'target':predictions})\noutput.to_csv('submission.csv',index=False)","ccf52862":"# *2. Explore the Data*\n","2acfb32c":"# *1. Setup*\n\n* Import libraries \n* Import data","d091c95a":"**Reference**\nhttps:\/\/www.kaggle.com\/kanncaa1\/feature-selection-and-data-visualization\nFor Feature Selection.","aa6c81dc":"**Referecne**\n\nMy notebook for tabular-playground-september(I referred many notebooks to use GPU and hyperparameter tuning, and so on.) Needed to predict if people claim the insureance or not.:\nhttps:\/\/www.kaggle.com\/satoshiss\/tabular-playground-september\/notebook"}}