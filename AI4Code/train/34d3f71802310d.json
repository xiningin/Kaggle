{"cell_type":{"04267685":"code","9f369985":"code","25bc66d8":"code","8afaff58":"code","acab3f6e":"code","ddc1af5d":"code","d172bae3":"code","f414726a":"code","c16e2a00":"code","20601657":"code","3ad356d8":"code","eae9ad11":"code","1b900fa3":"code","b3c039ff":"code","b07efe95":"code","9a2d3ba5":"code","ed2974c0":"code","364d49e8":"code","be33398e":"code","081474e4":"code","f04d8d9a":"code","962823c2":"code","77fee40f":"code","8a59ba9a":"code","d6255c74":"code","28d9e158":"code","8d3a057b":"code","c392ae7a":"code","43f179b7":"code","f395789d":"code","9a7b3482":"code","17e82f49":"code","fc367b60":"code","6547b0ac":"code","e5b3acb6":"code","50bbca8e":"code","9d14e4f4":"code","12b5f064":"code","1e49216b":"code","fe1683db":"markdown","6b8ad30f":"markdown","7a001bab":"markdown","0604f1dc":"markdown","9a5ef8ab":"markdown","274e5657":"markdown","e2362fc1":"markdown"},"source":{"04267685":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9f369985":"train_df=pd.read_csv('..\/input\/consumer-reviews-of-amazon-products\/1429_1.csv')\ntrain_df.head()","25bc66d8":"train_df.shape","8afaff58":"from sklearn.metrics import classification_report,roc_auc_score,roc_curve,r2_score,recall_score,confusion_matrix,precision_recall_curve\nfrom collections import Counter\nfrom sklearn.model_selection import StratifiedKFold,KFold,StratifiedShuffleSplit\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD,SparsePCA\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom nltk.tokenize import word_tokenize\nfrom collections import defaultdict\nfrom collections import Counter\nimport seaborn as sns\nfrom wordcloud import WordCloud,STOPWORDS\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go","acab3f6e":"print(\"The Shape of the Dataset\".format(),train_df.shape)","ddc1af5d":"train_df=train_df[[\"reviews.text\",\"reviews.rating\"]]","d172bae3":"train_df[\"reviews.rating\"].value_counts()","f414726a":"train_df.rename(columns={\"reviews.text\":\"reviews\",\"reviews.rating\":\"sentiment\",\"reviews.title\":\"title\"},inplace=True)","c16e2a00":"train_df","20601657":"good_reviews=train_df[train_df['sentiment']>=4]['reviews']\nbad_reviews=train_df[train_df['sentiment']<=2]['reviews']\nprint(\"First 10 samples of good reviews\\n\".format(),good_reviews[:5])\nprint(\"First 10 samples of bad reviews\\n\".format(),bad_reviews[:10])","3ad356d8":"count_good=train_df[train_df['sentiment'] >3]\n\ncount_bad=train_df[train_df['sentiment']<=3]","eae9ad11":"count_bad=count_bad[[\"reviews\",\"sentiment\"]]","1b900fa3":"count_good=count_good[[\"reviews\",\"sentiment\"]]\ncount_good.reviews\ncount_good.reset_index(inplace=True,drop=True)\ncount_bad.reset_index(inplace=True,drop=True)","b3c039ff":"count_bad","b07efe95":"count_good","9a2d3ba5":"z=np.mean([len(w) for w in str(count_good.reviews).split()])\nz","ed2974c0":"#Analyse the count of words in each segment- both positive and negative reviews\n#Function for checking word length\ndef cal_len(data):\n    return len(data)\n\n#Create generic plotter with Seaborn\ndef plot_count(count_ones,count_zeros,title_1,title_2,subtitle):\n    fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\n    sns.distplot(count_zeros,ax=ax1,color='Blue')\n    ax1.set_title(title_1)\n    sns.distplot(count_ones,ax=ax2,color='Red')\n    ax2.set_title(title_2)\n    fig.suptitle(subtitle)\n    plt.show()    \n\n\n\n\ncount_good_words=count_good['reviews'].str.split().apply(lambda z:cal_len(str(z)))\ncount_bad_words=count_bad['reviews'].str.split().apply(lambda z:cal_len(str(z)))\nprint(\"Positive Review Words:\" + str(count_good_words))\nprint(\"Negative Review Words:\" + str(count_bad_words))\nplot_count(count_good_words,count_bad_words,\"Positive Review\",\"Negative Review\",\"Reviews Word Analysis\")\n","364d49e8":"import warnings\nwarnings.filterwarnings('ignore')","be33398e":"def plot_count_1(count_ones,count_zeros,title_1,title_2,subtitle):\n    fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\n    sns.distplot(count_zeros,ax=ax1,color='Blue')\n    ax1.set_title(title_1)\n    sns.distplot(count_ones,ax=ax2,color='Orange')\n    ax2.set_title(title_2)\n    fig.suptitle(subtitle)\n    plt.show()    \n\n\nstops=set(stopwords.words('english'))\ncount_good_stops=train_df['reviews'].apply(lambda z : np.mean([len(w) for w in str(z).split()]))\ncount_bad_stops=train_df['reviews'].apply(lambda z : np.mean([len(w) for w in str(z).split()]))\nplot_count_1(count_good_stops,count_bad_stops,\"Positive Reviews Stopwords\",\"Negative Reviews Stopwords\",\"Reviews Stopwords Analysis\")","081474e4":"count_good_punctuations=count_good['reviews'].apply(lambda z: len([c for c in str(z) if c in string.punctuation]))\ncount_bad_punctuations=count_bad['reviews'].apply(lambda z:len([c for c in str(z) if c in string.punctuation]))\nplot_count(count_good_punctuations,count_bad_punctuations,\"Positive Review Punctuations\",\"Negative Review Punctuations\",\"Reviews Word Punctuation Analysis\")","f04d8d9a":"\ndef plot_count_1(count_ones,count_zeros,title_1,title_2,subtitle):\n    fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\n    sns.distplot(count_zeros,ax=ax1,color='Blue')\n    ax1.set_title(title_1)\n    sns.distplot(count_ones,ax=ax2,color='Orange')\n    ax2.set_title(title_2)\n    fig.suptitle(subtitle)\n    plt.show()    \n\n\nstops=set(stopwords.words('english'))\ncount_good_stops=count_good['reviews'].apply(lambda z : np.mean([len(w) for w in str(z).split()]))\ncount_bad_stops=count_bad['reviews'].apply(lambda z : np.mean([len(w) for w in str(z).split()]))\nplot_count_1(count_good_stops,count_bad_stops,\"Positive Reviews Stopwords\",\"Negative Reviews Stopwords\",\"Reviews Stopwords Analysis\")","962823c2":"count_good_urls=count_good['reviews'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\ncount_bad_urls=count_bad['reviews'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n\nplot_count_1(count_good_stops,count_bad_stops,\"Positive Reviews URLs\",\"Negative Reviews URLs\",\"Reviews URLs Analysis\")","77fee40f":"\n  \ncomment_words = ''\nstopwords = set(STOPWORDS)\n  \n# iterate through the csv file\nfor val in train_df.reviews[:10]:\n      \n    # typecaste each val to string\n    val = str(val)\n  \n    # split the value\n    tokens = val.split()\n      \n    # Converts each token into lowercase\n    for i in range(len(tokens)):\n        tokens[i] = tokens[i].lower()\n      \n    comment_words += \" \".join(tokens)+\" \"\n    \n    \n    wordcloud = WordCloud(width = 800, height = 800,\n                background_color ='white',\n                stopwords = stopwords,\n                min_font_size = 10).generate(comment_words)\n  \n# plot the WordCloud image                       \nplt.figure(figsize = (8, 8), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\n  \nplt.show()\n  ","8a59ba9a":"train_df.head(5)","d6255c74":"sentiment={5:\"positive\",4:\"positive\",3:\"negative\",2:\"negative\",1:\"negative\"}\ntrain_df[\"sentiment\"]=train_df['sentiment'].map(sentiment)\ntrain_df","28d9e158":"def create_corpus(word):\n    corpus=[]\n    \n    for x in train_df[train_df['sentiment']==word]['reviews'][0:2000].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus\n\n\ncorpus=create_corpus('positive')\ncounter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:50]:\n    if (word not in stops) :\n        x.append(word)\n        y.append(count)\nsns.barplot(x=y,y=x)","8d3a057b":"#Gram analysis on Training set- Bigram and Trigram\nimport nltk\nfrom nltk.corpus import stopwords\n#print(stopwords.words('english'))\nstopword=set(stopwords.words('english'))\ndef gram_analysis(data,gram):\n    tokens=[t for t in data.lower().split(\" \") if t!=\"\" if t not in stopword]\n    ngrams=zip(*[tokens[i:] for i in range(gram)])\n    final_tokens=[\" \".join(z) for z in ngrams]\n    return final_tokens\n\n\n#Create frequency grams for analysis\n    \ndef create_dict(data,grams):\n    freq_dict=defaultdict(int)\n    for sentence in data:\n        for tokens in gram_analysis(sentence,grams):\n            freq_dict[tokens]+=1\n    return freq_dict\n\ndef create_new_df(freq_dict,):\n    freq_df=pd.DataFrame(sorted(freq_dict.items(),key=lambda z:z[1])[::-1])\n    freq_df.columns=['n_gram_words','n_gram_frequency']\n    #print(freq_df.head())\n    #plt.barh(freq_df['n_gram_words'][:20],freq_df['n_gram_frequency'][:20],linewidth=0.3)\n    #plt.show()\n    trace=horizontal_bar_chart(freq_df[:20],'orange')\n    return trace\n    \ndef plot_grams(trace_zero,trace_one):\n    fig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words of positive reviews\", \n                                          \"Frequent words of negative reviews\"])\n    fig.append_trace(trace_zero, 1, 1)\n    fig.append_trace(trace_ones, 1, 2)\n    fig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\n    py.iplot(fig, filename='word-plots')\n    \n    \ntrain_df_zero=count_bad['reviews']\ntrain_df_ones=count_good['reviews']\n\n\n\n\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"n_gram_words\"].values[::-1],\n        x=df[\"n_gram_frequency\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\nprint(\"Bi-gram analysis\")\nfreq_train_df_zero=create_dict(train_df_zero[:200],2)\n#print(freq_train_df_zero)\ntrace_zero=create_new_df(freq_train_df_zero)\nfreq_train_df_ones=create_dict(train_df_ones[:200],2)\n#print(freq_train_df_zero)\ntrace_ones=create_new_df(freq_train_df_ones)\nplot_grams(trace_zero,trace_ones)\nprint(\"Tri-gram analysis\")\nfreq_train_df_zero=create_dict(train_df_zero[:200],3)\n#print(freq_train_df_zero)\ntrace_zero=create_new_df(freq_train_df_zero)\nfreq_train_df_ones=create_dict(train_df_ones[:200],3)\n#print(freq_train_df_zero)\ntrace_ones=create_new_df(freq_train_df_ones)\nplot_grams(trace_zero,trace_ones)","c392ae7a":"print(\"Penta-gram analysis\")\nfreq_train_df_zero=create_dict(train_df_zero[:200],5)\n#print(freq_train_df_zero)\ntrace_zero=create_new_df(freq_train_df_zero)\nfreq_train_df_ones=create_dict(train_df_ones[:200],5)\n#print(freq_train_df_zero)\ntrace_ones=create_new_df(freq_train_df_ones)\nplot_grams(trace_zero,trace_ones)","43f179b7":"train_df.shape","f395789d":"%%time\nimport re\n#Removes Punctuations\ndef remove_punctuations(data):\n    punct_tag=re.compile(r'[^\\w\\s]')\n    data=punct_tag.sub(r'',data)\n    return data\n\n#Removes HTML syntaxes\ndef remove_html(data):\n    html_tag=re.compile(r'<.*?>')\n    data=html_tag.sub(r'',data)\n    return data\n\n#Removes URL data\ndef remove_url(data):\n    url_clean= re.compile(r\"https:\/\/\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\n#Removes Emojis\ndef remove_emoji(data):\n    emoji_clean= re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    data=emoji_clean.sub(r'',data)\n    url_clean= re.compile(r\"https:\/\/\\S+|www\\.\\S+\")\n    data=url_clean.sub(r'',data)\n    return data\ntrain_df['reviews']=train_df['reviews'].fillna('').apply(str)\ntrain_df['reviews']=train_df['reviews'].apply(lambda z: remove_punctuations(z))\n\ntrain_df['reviews']=train_df['reviews'].apply(lambda z: remove_html(z))\ntrain_df['reviews']=train_df['reviews'].apply(lambda z: remove_url(z))\ntrain_df['reviews']=train_df['reviews'].apply(lambda z: remove_emoji(z))\n    ","9a7b3482":"train_df","17e82f49":"count_good=train_df[train_df['sentiment']=='positive']\ncount_bad=train_df[train_df['sentiment']=='negative']","fc367b60":"train_df","6547b0ac":"#Apply Gram Analysis\ntrain_df_zero=count_bad['reviews']\ntrain_df_ones=count_good['reviews']\nprint(\"Tri-gram analysis\")\nfreq_train_df_zero=create_dict(train_df_zero[:200].str.lower(),3)\n#print(freq_train_df_zero)\ntrace_zero=create_new_df(freq_train_df_zero)\nfreq_train_df_ones=create_dict(train_df_ones[:200].str.lower(),3)\n#print(freq_train_df_zero)\ntrace_ones=create_new_df(freq_train_df_ones)\nplot_grams(trace_zero,trace_ones)","e5b3acb6":"comment_words = ''\nstopwords = set(STOPWORDS)\n  \n# iterate through the csv file\nfor val in train_df.reviews[:10]:\n      \n    # typecaste each val to string\n    val = str(val)\n  \n    # split the value\n    tokens = val.split()\n      \n    # Converts each token into lowercase\n    for i in range(len(tokens)):\n        tokens[i] = tokens[i].lower()\n      \n    comment_words += \" \".join(tokens)+\" \"\n    \n    \n    wordcloud = WordCloud(width = 800, height = 800,\n                background_color ='white',\n                stopwords = stopwords,\n                min_font_size = 10).generate(comment_words)\n  \n# plot the WordCloud image                       \nplt.figure(figsize = (8, 8), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\n  \nplt.show()","50bbca8e":"#Lemmatize the dataset\nfrom nltk.stem import WordNetLemmatizer\n\n\ndef lemma_traincorpus(data):\n    lemmatizer=WordNetLemmatizer()\n    out_data=\"\"\n    for words in data:\n        out_data+= lemmatizer.lemmatize(words)\n    return out_data\n\ntrain_df['reviews']=train_df['reviews'].apply(lambda z: lemma_traincorpus(z))","9d14e4f4":"#check a sample from the lemmatized dataset\ntrain_df['reviews'][5:10]","12b5f064":"#For example let us try to stem them and check  a sample\n\nfrom nltk.stem import *\ndef stem_traincorpus(data):\n    stemmer = PorterStemmer()\n    out_data=\"\"\n    for words in data:\n        out_data+= stemmer.stem(words)\n    return out_data\n\nsample_train_df=train_df[5:10]\nsample_train_df['reviews']=sample_train_df['reviews'].apply(lambda z: stem_traincorpus(z))\nsample_train_df['reviews']","1e49216b":"%%time\ntrain_li=[]\nfor i in range(len(train_df)):\n    if (train_df['sentiment'][i]=='positive'):\n        train_li.append(1)\n    else:\n        train_li.append(0)\ntrain_df['Binary']=train_li\ntrain_df.head(20)","fe1683db":"Inference from Analysis - II\nIn this section, we have analysed based on positional features of words in a corpus\/sentence\/paragraph. The Gram analysis,particularly the pentagram analysis provides an idea which sentences occur more often in the corpus. And in most of the cases, these bag of words are the ones picked up by any frequency vectorization technique.\n\nThus this provides an outline as to the frequency of the conjuction of words which are occuring at the highest frequency. Another important aspect is that,there is a presence of certain html tags and punctuations which have to be removed as these are adding noise to the review corpus. This will be taken up in the cleaning phase.\n\nTime for some Cleaning!\nBefore we move ahead , let us clean the dataset and remove the redundancies.This includes\n\nHTML codes\nURLs\nEmojis\nStopwords\nPunctuations\nExpanding Abbreviations\nThese will be sufficient for cleaning the corpus!\n\nRegex is a very good tool which will help us to do this cleaning.\nhttps:\/\/docs.python.org\/3\/howto\/regex.html","6b8ad30f":"# **Importance of Data Preprocessing and Cleaning**\nThe afore mentioned phase is one of the most important phase. If the textual data is not properly cleaned or processed, incorrect words\/puncutations\/urls and associated redundancies get added to the data. This impacts the performance when we will be creating static\/dynamic embeddings and analysing the sentence\/word vectors. In the context of embeddings,(and subsequently models), we will find that if we donot remove these inconsistencies, the vectors will not be properly placed. For example, if we apply a SOTA language embedding such as GPT-2 on unclean data containing such redundancies, the tokenizer will create separate tokens for them; which will lead the model to associate certain weights for these. These add to the redundancy and increase complexity of the model. When we will be extracting individual entries (word\/sentence vectors),then these redundancies get added to the vector space and interfere with different metrics such as semantic similarity or classification\/question answers etc.\n\nIn real world, data is much more unclean as in most of the cases, data scientists work with unstructured data coming from MonogoDb or other databases. In some cases, the data may be scraped from websites which picks up a lot of inconsistencies- specially pdf, which when converted to textual format(using beautifulsoup library or similar),may contain some urls\/tags. This paper provides a good analysis.Cleaning is hence really important!","7a001bab":"Data Cleaning is completed!\nWe have completed the cleaning step and gained significant insights about the corpus.\n\n\nTransforming the Corpus!!\nNow at this stage the data is successfully cleaned and all redundant noises are removed. These steps are generic to any NLP pipeline which reduces the dimension of the data. Once the data is cleaned , we can again prune some words to their base form and reduce the sentence lengths. This is important because when we are applying any model (statistical, deep learning, transformers,graphs), 2 different words from the same base word are encoded and tokenized in a different manner. For instance, the word \"watched\" and \"watching\" have the same root word \"watch\", however they are encoded separately with respect to any Tokenizer.\n\nTo alleviate this issue, it is recommended to perform lemmatization on the text corpus so that the words can be reduced to their root semantic word. Morphological transformations such as \"watched\" and \"watching\", are converted to their base form through this method. Stemming , although can be used , is not recommended as it does not take into consideration the semantics of the sentence or the surrounding words which are present around it.Stemming also produces words which are not present in the vocabulary.\n\nFor an in depth study of the same, please refer to the Stanford documentation\n\nFor trying out stemming, you can use my porter stemmer library\n\nBut for now, we will be using NLTK for our lemmatization purposes. So lets, get started!","0604f1dc":"Statistical Analysis-II\nIn this context , we will be exploring further into the analysis part. This would allow us to have a better idea which part of the data requires removal and which part can be transformed before applying any model on it.\n\nHere we will be looking into:\n\nGram Statistics\nGram analysis is an essential tool which forms the base of preparing a common bag of words model containing relevant data. This process implies that we are taking into consideration which words are present in conjunction with other words with a maximum frequency in the dataset. Grams can be n-ary implying that we can have many gram analysis taking n-words together.For example: a Ternary Gram Analysis(Tri-gram) includes analysing sentences which have 3 words occuring together at a higher frequency.\n\nA detailed image of a ternary gram analysis using a famous example is provided:\n\n![image.png](attachment:c7ac5989-fdb7-4272-8b66-4b03d04ee6b1.png)![image.png](attachment:a83bb854-9bfe-40b3-b5f5-a18d954cccde.png)\n\nAnother example is also provided:\n![image.png](attachment:ff2bce70-efa1-4a1c-b5ff-42cd047b11e2.png)![image.png](attachment:19acea16-33e8-4409-a929-ba138a5b21b4.png)","9a5ef8ab":"# End of Dataset Preparation\nAt this stage , we have covered the dataset preparation part of the pipeline. At this stage we have analysed the dataset , got an initial estimate about the words in the corpus. We performed cleaning, statistical analysis as well as lemmatization to prepare the dataset for EDA and successive steps.","274e5657":"Importing libraries\nThe significant aspect is to import libraries for our use statistical analysis cases. Some of these include:\n\n* Sklearn\n* Matplotlib\n* Seaborn\n* NLTK\n* wordcloud\nThese libraries and frameworks are efficient in handling data which can be used for initial analysis. As we progress, we will be including more libraries.","e2362fc1":"# **The data is cleaned!**\nLet us apply the Gram Statistics on the cleaned dataset"}}