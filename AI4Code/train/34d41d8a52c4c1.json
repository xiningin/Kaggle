{"cell_type":{"923de520":"code","75fea755":"code","4469cfa0":"code","be2f58ee":"code","f041d7a0":"code","907f9bf0":"code","a0ee390b":"code","1b2f3b75":"code","5bf45e34":"code","d6e290f7":"code","9f842f23":"code","5a1c4d02":"code","ae48301e":"code","4092300a":"code","f25cf0e7":"code","bb29b744":"code","74cef649":"code","eda070b9":"code","7141b6d1":"code","0bfc7051":"code","5806aea3":"code","de40cb98":"code","94a32a8c":"code","73a980bc":"code","5b1e1cb3":"code","4bc4e991":"code","b45f52a6":"code","2ea680f2":"code","54d28ceb":"code","faeb82d8":"code","2f0aaaaa":"code","d31a378c":"code","49c4e48e":"code","7e129915":"code","a130ba29":"code","74d9c7b8":"code","abfde6a0":"code","3ad674cf":"code","7e55d0a4":"code","fb56362f":"code","123e0fac":"code","8620755b":"code","ebaff230":"code","167c8623":"code","95fd0008":"code","4c41360a":"code","07d7f4a9":"code","ecf4993a":"code","490e12ea":"code","5da8faa0":"code","7668b5f7":"code","38dc1298":"code","88360a46":"code","11b14cd8":"code","2b8bb17e":"code","cbe58be3":"code","c2c099e5":"code","89e3b71c":"code","24214f4c":"code","db6bf1bd":"code","301ec028":"code","f5c3e958":"code","1498a959":"code","b4f0b6c7":"code","27fb749e":"code","249d5de1":"code","a0604411":"code","a489b347":"code","c8058654":"code","ff3c36b0":"markdown","478bfd95":"markdown","26e0814e":"markdown","c71d2aef":"markdown","0eb79d25":"markdown","aa2bb98d":"markdown","22553569":"markdown","12ea05be":"markdown","fa15b575":"markdown","a07fd173":"markdown","78e63a4a":"markdown","2a5bda86":"markdown","0095b01b":"markdown","1b633ce6":"markdown"},"source":{"923de520":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","75fea755":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\n%matplotlib inline","4469cfa0":"train_data = pd.read_csv(\"\/kaggle\/input\/novartis-data\/Train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/novartis-data\/Test.csv\")","be2f58ee":"train_data.head()","f041d7a0":"test_data.head()","907f9bf0":"train_data.info()","a0ee390b":"test_data.info()","1b2f3b75":"train_data.describe()","5bf45e34":"train_data.isnull().sum()","d6e290f7":"test_data.isnull().sum()","9f842f23":"train_data[\"X_12\"] = train_data[\"X_12\"].ffill()\ntest_data[\"X_12\"] = test_data[\"X_12\"].ffill()\ntrain_data[\"X_12\"] = train_data[\"X_12\"].bfill()\ntest_data[\"X_12\"] = test_data[\"X_12\"].bfill()","5a1c4d02":"train_data.isnull().sum()","ae48301e":"test_data.isnull().sum()","4092300a":"train_data.drop([\"INCIDENT_ID\",\"DATE\"],axis=1,inplace=True)","f25cf0e7":"test_data.drop([\"INCIDENT_ID\",\"DATE\"],axis=1,inplace=True)","bb29b744":"train_data.head()","74cef649":"test_data.head()","eda070b9":"sns.set()\ntrain_data.hist(figsize=(20,10),bins=15,color=\"purple\")\nplt.title(\"Distribution of Features\")\nplt.show()","7141b6d1":"sns.set()\nplt.figure(figsize=(20,10))\nsns.boxplot(data= train_data,palette = \"Set3\")\nplt.show()","0bfc7051":"sns.set()\nplt.figure(figsize=(20,10))\nsns.boxplot(data= test_data,palette = \"Set3\")\nplt.show()","5806aea3":"lower_limit = train_data[\"X_8\"].mean() - 3*train_data[\"X_8\"].std()\nupper_limit = train_data[\"X_8\"].mean() + 3*train_data[\"X_8\"].std()","de40cb98":"df_train1 = train_data[(train_data[\"X_8\"] > lower_limit) & (train_data[\"X_8\"] < upper_limit)]","94a32a8c":"train_data.shape[0] - df_train1.shape[0]","73a980bc":"lower_limit = test_data[\"X_8\"].mean() - 3*test_data[\"X_8\"].std()\nupper_limit = test_data[\"X_8\"].mean() + 3*test_data[\"X_8\"].std()","5b1e1cb3":"df_test1 = test_data[(test_data[\"X_8\"] > lower_limit) & (test_data[\"X_8\"] < upper_limit)]","4bc4e991":"test_data.shape[0] - df_test1.shape[0]","b45f52a6":"lower_limit = df_train1[\"X_10\"].mean() - 3*df_train1[\"X_10\"].std()\nupper_limit = df_train1[\"X_10\"].mean() + 3*df_train1[\"X_10\"].std()","2ea680f2":"df_train2 = df_train1[(df_train1[\"X_10\"] > lower_limit) & (df_train1[\"X_10\"] < upper_limit)]","54d28ceb":"df_train1.shape[0] - df_train2.shape[0]","faeb82d8":"lower_limit = df_test1[\"X_10\"].mean() - 3*df_test1[\"X_10\"].std()\nupper_limit = df_test1[\"X_10\"].mean() + 3*df_test1[\"X_10\"].std()","2f0aaaaa":"df_test2 = df_test1[(df_test1[\"X_8\"] > lower_limit) & (df_test1[\"X_8\"] < upper_limit)]","d31a378c":"df_test1.shape[0] - df_test2.shape[0]","49c4e48e":"lower_limit = df_train2[\"X_11\"].mean() - 3*df_train2[\"X_11\"].std()\nupper_limit = df_train2[\"X_11\"].mean() + 3*df_train2[\"X_11\"].std()","7e129915":"df_train3 = df_train2[(df_train2[\"X_11\"] > lower_limit) & (df_train2[\"X_11\"] < upper_limit)]","a130ba29":"df_train2.shape[0] - df_train3.shape[0]","74d9c7b8":"lower_limit = df_test2[\"X_11\"].mean() - 3*df_test2[\"X_11\"].std()\nupper_limit = df_test2[\"X_11\"].mean() + 3*df_test2[\"X_11\"].std()","abfde6a0":"df_test3 = df_test2[(df_test2[\"X_11\"] > lower_limit) & (df_test2[\"X_11\"] < upper_limit)]","3ad674cf":"df_test2.shape[0] - df_test3.shape[0]","7e55d0a4":"lower_limit = df_train3[\"X_12\"].mean() - 3*df_train3[\"X_12\"].std()\nupper_limit = df_train3[\"X_12\"].mean() + 3*df_train3[\"X_12\"].std()","fb56362f":"df_train4 = df_train3[(df_train3[\"X_12\"] > lower_limit) & (df_train3[\"X_12\"] < upper_limit)]","123e0fac":"df_train3.shape[0] - df_train4.shape[0]","8620755b":"lower_limit = df_test3[\"X_12\"].mean() - 3*df_test3[\"X_12\"].std()\nupper_limit = df_test3[\"X_12\"].mean() + 3*df_test3[\"X_12\"].std()","ebaff230":"df_test4 = df_test3[(df_test3[\"X_12\"] > lower_limit) & (df_test3[\"X_12\"] < upper_limit)]","167c8623":"df_test3.shape[0] - df_test4.shape[0]","95fd0008":"lower_limit = df_train4[\"X_13\"].mean() - 3*df_train4[\"X_13\"].std()\nupper_limit = df_train4[\"X_13\"].mean() + 3*df_train4[\"X_13\"].std()","4c41360a":"df_train5 = df_train4[(df_train4[\"X_13\"] > lower_limit) & (df_train4[\"X_13\"] < upper_limit)]","07d7f4a9":"df_train4.shape[0] - df_train5.shape[0]","ecf4993a":"lower_limit = df_test4[\"X_13\"].mean() - 3*df_test4[\"X_13\"].std()\nupper_limit = df_test4[\"X_13\"].mean() + 3*df_test4[\"X_13\"].std()","490e12ea":"df_test5 = df_test4[(df_test4[\"X_13\"] > lower_limit) & (df_test4[\"X_13\"] < upper_limit)]","5da8faa0":"df_test4.shape[0] - df_test5.shape[0]","7668b5f7":"df_train5.head()","38dc1298":"df_test5.head()","88360a46":"df_train5.info()","11b14cd8":"df_test5.info()","2b8bb17e":"n = msno.bar(df_train5,color=\"purple\")","cbe58be3":"x = df_train5.drop(\"MULTIPLE_OFFENSE\",axis=1)\ny = df_train5[\"MULTIPLE_OFFENSE\"]\n","c2c099e5":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nx = scaler.fit_transform(x)","89e3b71c":"x","24214f4c":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression","db6bf1bd":"model_params ={\n    \"svm\": {\n        \"model\" : SVC(gamma=\"auto\"),\n        \"params\": {\n            \"C\" : [1,10,20],\n            \"kernel\": [\"rbf\"],\n            \"random_state\":[0,10,100]\n        }\n    },\n    \n    \"decision_tree\":{\n        \"model\": DecisionTreeClassifier(),\n        \"params\":{\n            \"criterion\": [\"entropy\",\"gini\"],\n            \"max_depth\": [5,8,9],\n            \"random_state\":[0,10,100]\n        }\n    },\n    \"random_forest\":{\n        \"model\": RandomForestClassifier(),\n        \"params\": {\n            \"n_estimators\" : [1,5,10],\n            \"max_depth\" : [5,8,9],\n            \"random_state\":[0,10,100]\n        }\n    },\n    \"naive_bayes\":{\n        \"model\": GaussianNB(),\n        \"params\": {}\n    },\n    \n    \"logistic_regression\":{\n        \"model\" : LogisticRegression(solver='liblinear',multi_class = 'auto'),\n        \"params\":{\n            \"C\" : [1,5,10],\n            \"random_state\":[0,10,100]\n        }\n    },\n    \"knn\" : {\n        \"model\" : KNeighborsClassifier(),\n        \"params\": {\n            \"n_neighbors\" : [5,12,13]\n        }\n    }\n    \n    \n}","301ec028":"scores =[]\nfor model_name,mp in model_params.items():\n    clf = GridSearchCV(mp[\"model\"],mp[\"params\"],cv=12,return_train_score=False)\n    clf.fit(x,y)\n    scores.append({\n        \"Model\" : model_name,\n        \"Best_Score\": clf.best_score_,\n        \"Best_Params\": clf.best_params_\n    })","f5c3e958":"result_score = pd.DataFrame(scores, columns = [\"Model\",\"Best_Score\",\"Best_Params\"])","1498a959":"result_score","b4f0b6c7":"from sklearn.model_selection import  train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25,random_state=0)","27fb749e":"clf_dt = DecisionTreeClassifier(criterion= \"gini\",max_depth = 9,random_state=0)","249d5de1":"clf_dt.fit(x_train,y_train)","a0604411":"y_pred = clf_dt.predict(x_test)","a489b347":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test,y_pred)","c8058654":"result = pd.DataFrame({\"Actual_Value\": y_test, \"Predicted_Value\": y_pred})\nresult","ff3c36b0":"## So ,let's now scale our Dataset","478bfd95":"## We can see that we have some null values .So let's remove these null values","26e0814e":"## Now the precious time has come to see which parameter is best to create a Good Model","c71d2aef":"## Wow, we can see that \"Random Forest\" & \"Decision Tree\" gives us almost 99% Accuracy","0eb79d25":"## Now let's drop the first two column(INCIDENT_ID & DATE) because we don't need these two column","aa2bb98d":"## Now we can see that, we have no Null Values.So we have successfully remove all the Null Values","22553569":"## So we can see that our Dataset has no Null Values","12ea05be":"## Here we can see that we have totally removed (23856 - 22628) = 1228 outliers from train data","fa15b575":"## Let's see our Actual vs Predicted Values,Which says,if a server will be Hack or Not","a07fd173":"## *** --------If you like it,then please do UpVote-------- ***","78e63a4a":"## Here we can see that our dataset has a lot of outliers,Let's remove some of the outliers, becouse outliers are gonna make some trouble for us to create a Good Model","2a5bda86":"## Now Let's see  our Dataset has any null values or not","0095b01b":"## Here we can see that we have totally removed (15903  - 14981 ) = 922 outliers from test data","1b633ce6":"## Let's remove these Outliers using Standard Deviation"}}