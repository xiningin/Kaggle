{"cell_type":{"420afa02":"code","930b922b":"code","873e90d0":"code","50f086d8":"code","74679f38":"code","59538546":"code","12e6ea41":"code","31225e4a":"code","69ffefb8":"code","2937a847":"code","6ad64967":"code","dc3d7457":"code","7a0fc905":"code","28403697":"code","ef8282e5":"code","083ecf35":"code","941f821c":"code","7329b500":"code","a8ee684c":"code","8e3e358b":"code","30fd82a9":"markdown","625124b0":"markdown","c628411f":"markdown","fa46704f":"markdown","ecefabed":"markdown"},"source":{"420afa02":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nimport os\nfrom tqdm.notebook import tqdm","930b922b":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","873e90d0":"path = \"..\/input\/m5-forecasting-accuracy\"\n\ncalendar = pd.read_csv(os.path.join(path, \"calendar.csv\"))\nselling_prices = pd.read_csv(os.path.join(path, \"sell_prices.csv\"))\nsample_submission = pd.read_csv(os.path.join(path, \"sample_submission.csv\"))","50f086d8":"sales = pd.read_csv(os.path.join(path, \"sales_train_validation.csv\"))","74679f38":"from sklearn.preprocessing import OrdinalEncoder\n\ndef prep_calendar(df):\n    df = df.drop([\"date\", \"weekday\"], axis=1)\n    df = df.assign(d = df.d.str[2:].astype(int))\n    df = df.fillna(\"missing\")\n    cols = list(set(df.columns) - {\"wm_yr_wk\", \"d\"})\n    df[cols] = OrdinalEncoder(dtype=\"int\").fit_transform(df[cols])\n    df = reduce_mem_usage(df)\n    return df\n\ncalendar = prep_calendar(calendar)","59538546":"def prep_selling_prices(df):\n    gr = df.groupby([\"store_id\", \"item_id\"])[\"sell_price\"]\n    df[\"sell_price_rel_diff\"] = gr.pct_change()\n    df[\"sell_price_roll_sd7\"] = gr.transform(lambda x: x.rolling(7).std())\n    df[\"sell_price_cumrel\"] = (gr.shift(0) - gr.cummin()) \/ (1 + gr.cummax() - gr.cummin())\n    df = reduce_mem_usage(df)\n    return df\n\nselling_prices = prep_selling_prices(selling_prices)","12e6ea41":"def reshape_sales(df, drop_d = None):\n    if drop_d is not None:\n        df = df.drop([\"d_\" + str(i + 1) for i in range(drop_d)], axis=1)\n    df = df.assign(id=df.id.str.replace(\"_validation\", \"\"))\n    df = df.reindex(columns=df.columns.tolist() + [\"d_\" + str(1913 + i + 1) for i in range(2 * 28)])\n    df = df.melt(id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n                 var_name='d', value_name='demand')\n    df = df.assign(d=df.d.str[2:].astype(\"int16\"))\n    return df\n\nsales = reshape_sales(sales, 1000)","31225e4a":"def prep_sales(df):\n    df['lag_t28'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28))\n    df['rolling_mean_t7'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).mean())\n    df['rolling_mean_t30'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).mean())\n    df['rolling_mean_t60'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(60).mean())\n    df['rolling_mean_t90'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(90).mean())\n    df['rolling_mean_t180'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(180).mean())\n    df['rolling_std_t7'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).std())\n    df['rolling_std_t30'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).std())\n\n    # Remove rows with NAs except for submission rows. rolling_mean_t180 was selected as it produces most missings\n    df = df[(df.d >= 1914) | (pd.notna(df.rolling_mean_t180))]\n    df = reduce_mem_usage(df)\n\n    return df\n\nsales = prep_sales(sales)","69ffefb8":"sales = sales.merge(calendar, how=\"left\", on=\"d\")\ngc.collect()","2937a847":"sales = sales.merge(selling_prices, how=\"left\", on=[\"wm_yr_wk\", \"store_id\", \"item_id\"])\nsales.drop([\"wm_yr_wk\"], axis=1, inplace=True)\ngc.collect()","6ad64967":"del selling_prices","dc3d7457":"cat_id_cols = [\"item_id\", \"dept_id\", \"store_id\", \"cat_id\", \"state_id\"]\ncat_cols = cat_id_cols + [\"wday\", \"month\", \"year\", \"event_name_1\", \n                          \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n\n# In loop to minimize memory use\nfor i, v in tqdm(enumerate(cat_id_cols)):\n    sales[v] = OrdinalEncoder(dtype=\"int\").fit_transform(sales[[v]])\n\nsales = reduce_mem_usage(sales)\nsales.head()\ngc.collect()","7a0fc905":"num_cols = [\"sell_price\", \"sell_price_rel_diff\", \"sell_price_roll_sd7\", \"sell_price_cumrel\",\n#            \"lag_t28\", \"rolling_mean_t7\", \"rolling_mean_t30\", \"rolling_mean_t60\", \n#            \"rolling_mean_t90\", \"rolling_mean_t180\", \"rolling_std_t7\", \"rolling_std_t30\"]\n           ]\nbool_cols = [\"snap_CA\", \"snap_TX\", \"snap_WI\"]\ndense_cols = num_cols + bool_cols\n\n# Need to do column by column due to memory constraints\nfor i, v in tqdm(enumerate(num_cols)):\n    sales[v] = sales[v].fillna(sales[v].median())\n    \nsales.head()","28403697":"test = sales[sales.d >= 1914]\ntest = test.assign(id=test.id + \"_\" + np.where(test.d <= 1941, \"validation\", \"evaluation\"),\n                   F=\"F\" + (test.d - 1913 - 28 * (test.d > 1941)).astype(\"str\"))\ntest.head()\ngc.collect()","ef8282e5":"from sklearn.model_selection import train_test_split\n# Submission data\n#X_test = make_X(test)\n\n# One month of validation data\nflag = (sales.d < 1914) & (sales.d >= 1914 - 28)\n#valid = (make_X(sales[flag]),\n#         sales[\"demand\"][flag])\n\n# Rest is used for training\nflag = sales.d < 1914 - 28\n\nsales = sales.sample(n=10000, random_state=1)\nX_train = sales[flag].drop('id', axis=1)\ny_train = sales[\"demand\"][flag]\n\n#X_train, X_test, y_train, y_test = train_test_split(\n#         X, y, test_size=0.33, random_state=42)\n                             \ndel sales, flag\ngc.collect()","083ecf35":"import tensorflow as tf\nimport tensorflow.keras as keras\n\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.layers import Dense, Input, Embedding, Dropout, concatenate, Flatten\nfrom tensorflow.keras.models import Model","941f821c":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation, LSTM\nfrom keras.callbacks import TensorBoard\nimport keras.backend as K\nEarlyStopping = tf.keras.callbacks.EarlyStopping()\n\n\ndef rmse(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred -y_true)))\n\nepochs= 12\nbatch_size = 28\nverbose = 1\nvalidation_split = 0.3\ninput_dim = X_train.shape[1]\nn_out = 1\nlr = 0.5\n\nmodel = Sequential()\nmodel.add(Dense(1024, input_shape=(input_dim,)))\nmodel.add(Activation('relu'))\nmodel.add(Dense(1024))\nmodel.add(Activation('relu'))\n\n\nmodel.add(Dense(n_out))\n#model.add(Activation('tanh'))\nmodel.summary()\n          \nmodel.compile(loss=keras.losses.mean_squared_error,\n                  metrics=[\"mse\"],\n                  optimizer='adam')\n\nhist = model.fit(X_train, y_train,\n                         batch_size = batch_size, epochs = epochs,\n                         callbacks = [EarlyStopping],\n                         verbose=verbose, validation_split=validation_split)","7329b500":"plt.clf()\nplt.figsize=(15, 10)\nloss = hist.history['loss']\nval_loss = hist.history['val_loss']\nepochs = range(1, len(loss) +1)\n\nplt.plot(epochs, loss, 'bo', label = 'Training loss')\nplt.plot(epochs, val_loss, 'b', label = 'Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","a8ee684c":"#It takes so long a time!\nimport shap\nsummary = shap.kmeans(X_train.values, 25)\n\nexplainer = shap.KernelExplainer(model.predict, summary)\nshap_values = explainer.shap_values(X_train.values)","8e3e358b":"shap.summary_plot(shap_values[0], X_train)","30fd82a9":"![shap](https:\/\/shap.readthedocs.io\/en\/latest\/_images\/shap_header.png)","625124b0":"# model","c628411f":"# [SHAP]feature importances plot of NN models  \nContrary to GBDT models, NN model is hard to look into its feature importances.  \nIn this kernel, we try to visualise the NN model's feature importances by using Shap Explainers.  \n\nAbout shap: https:\/\/shap.readthedocs.io\/en\/latest\/  \nOn loading and FE parts, we owe to this great kernel: https:\/\/www.kaggle.com\/mayer79\/m5-forecast-keras-with-categorical-embeddings-v2  \nBig thanks to all kagglers!!","fa46704f":"The feature importances were visualized like GBDT models","ecefabed":"# Visualisation"}}