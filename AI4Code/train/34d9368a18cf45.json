{"cell_type":{"a88c73c6":"code","d791112a":"code","51470a04":"code","b95eb527":"code","942cf0c3":"code","24cbc4bc":"code","54eae700":"code","5bfb68c8":"code","e92639a3":"code","88914754":"code","c06ec7b6":"code","002b0270":"code","e996eecd":"code","092b6bb9":"code","9db545c0":"code","5e66d608":"code","bbcfa347":"code","70acff2c":"code","bb232e81":"code","b24a7a04":"code","82e9069c":"code","ca6dbfc8":"code","c9a9135b":"markdown","5cb58173":"markdown","acf734d6":"markdown","0bfa2210":"markdown","f5a7ff9f":"markdown","709ca44f":"markdown","68ec7a74":"markdown","a128ac03":"markdown","5d0187da":"markdown","3c4dcb22":"markdown","c66ab276":"markdown","673cc063":"markdown","cefb3164":"markdown","18c72b03":"markdown","108c28f0":"markdown","b8fe5e31":"markdown","a074fb0e":"markdown","d0642d47":"markdown","70307518":"markdown","1b083a60":"markdown","3cf1f7bc":"markdown","8148fd4c":"markdown","f15b31d5":"markdown","b09a9ac7":"markdown","36f68504":"markdown","bcb8dae1":"markdown","6ce5adc4":"markdown"},"source":{"a88c73c6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport matplotlib.pyplot as plt\nimport seaborn as sns","d791112a":"dataset = pd.read_csv(\"\/kaggle\/input\/bank-churn\/Bank_churn_modelling.csv\")\ndataset","51470a04":"X = dataset.iloc[:, 3:13].values\nX","b95eb527":"y = dataset.iloc[:, 13].values\ny","942cf0c3":"from sklearn.preprocessing import LabelEncoder","24cbc4bc":"labelencoder_X_2 = LabelEncoder()","54eae700":"X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])","5bfb68c8":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\ntransformer = ColumnTransformer(transformers=[(\"OneHot\",OneHotEncoder(),[1])],remainder='passthrough')\nX = transformer.fit_transform(X.tolist())\nX = X.astype('float64')","e92639a3":"X = X[:, 1:]","88914754":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","c06ec7b6":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","002b0270":"import keras","e996eecd":"from keras.models import Sequential","092b6bb9":"from keras.layers import Dense","9db545c0":"classifier = Sequential()","5e66d608":"classifier.add(Dense(output_dim = 6, init = 'uniform' , activation = 'relu', input_dim = 11))","bbcfa347":"classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))","70acff2c":"classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))","bb232e81":"classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","b24a7a04":"classifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)","82e9069c":"y_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.5) #if y_pred is larger than 0.5 it returns true(1) else false(2)\n\nprint(y_pred)","ca6dbfc8":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)","c9a9135b":"### Feature Scaling\n\nFeature Scaling or Standardization  is applied to independent variables or features of data. It basically helps to normalise the data within a particular range. Sometimes, it also helps in speeding up the calculations in an algorithm.","5cb58173":"Now, After the first look on the dataset we can conclude that the dependent varaiables() for the prediction of bank churning is from the columns 3 to 12.\n\nSo we drop them from X which contains the features Indexes from 3 to 12.","acf734d6":"Obtained from Confusion Matrix.You may change values as per what is obtained in your confusion matrix.\n\n\n**Congratulations! you just wrote your own Neural Network for theBank which had given you this task.**","0bfa2210":"## Part 1 - Data Preprocessing\n\n### Importing the Libraries\n","f5a7ff9f":"**This Model when trained on the train data and when tested on the test data gives us an accuracy of around 84% in both of the cases**.Which from our point of view is Great!!!\n\n### Making the Confusion Matrix","709ca44f":"### Dummy Variable Trap\n\nBy including dummy variable in a regression model however, one should be careful of the Dummy Variable Trap. The Dummy Variable trap is a scenario in which the independent variables are multicollinear - a scenario in which two or more variables are highly correlated; in simple terms one variable can be predicted from the others.\n\nWe can avoid the dummy variable trap by using one less varaible from all the variable. \n\nFor example , There are three dummies created for the feature \"Geography\". Now if we remove any one of the dummy then the we will avoid the trap.So, In this case we will remove the first column which has index 0.","68ec7a74":"## Part 2 - ANN\n\nListing out the steps involved in training the ANN with Stochastic Gradient Descent:-\n\n1)Randomly initialize the weights to small numbers close to 0(But not 0).\n\n2)Input the 1st observation of your dataset in the Input Layer, each Feature in one Input Node.\n\n3)Forward-Propagation from Left to Right, the neurons are activated in a way that the impact of each neuron\u2019s activation.\nis limited by the weights.Propagate the activations until getting the predicted result y.\n\n4)Compare the predicted result with the actual result. Measure the generated error.\n\n5)Back-Propagation: From Right to Left, Error is back propagated.Update the weights according to how much they are\nresponsible for the error.The Learning Rate tells us by how much such we update the weights.\n\n6)Repeat Steps 1 to 5 and update the weights after each observation(Reinforcement Learning).\nOr: Repeat Steps 1 to 5 but update the weights only after a batch of observations(Batch Learning).\n\n7)When the whole training set is passed through the ANN.That completes an Epoch. Redo more Epochs.","a128ac03":"### Adding the second hidden layer\n\n* Set Output Dim=6\n* Init will initialize the Hidden Layer weights uniformly\n* Activation Function is Rectifier Activation Function(Relu)\n* No need for Input Dim.\n","5d0187da":"### Adding the input layer and the first hidden layer\n\nThere is no thumb rule but you can set the number of nodes in Hidden Layers as an Average of the number of Nodes in Input and Output Layer Respectively.\n\n* So set Output Dim=6\n* Init will initialize the Hidden Layer weights uniformly\n* Activation Function is Rectifier Activation Function(Relu)\n* Input dim tells us the number of nodes in the Input Layer.This is done only once and wont be specified in further layers.","3c4dcb22":"### Dummy Variables\n\nA dummy variable (aka, an indicator variable) is a numeric variable that represents categorical data, such as gender, race, political affiliation, etc. Technically, dummy variables are dichotomous, quantitative variables. Their range of values is small; they can take on only two quantitative values.\n\nNow creating Dummy variables.","c66ab276":"You can also have a look at [ANN for Bank Churn Modeling](http:\/\/https:\/\/towardsdatascience.com\/building-your-own-artificial-neural-network-from-scratch-on-churn-modeling-dataset-using-keras-in-690782f7d051)","673cc063":"### Importing The Dataset","cefb3164":"As you can see the part 1 has a lot of work in it and we have not even started our neural network.\n\nNow it is important to know that one of the **big part of being a data scientist is data preprocessing** so that we can have a  usable data to apply to our models or neural network.\n\nSo, now lets start making our Artificial Neural Network","18c72b03":"To randomly initialize the weights to small numbers close to 0(But not 0)","108c28f0":"Encoding Gender from string to just 2 numbers i.e. 0,1(male,female) respectively","b8fe5e31":"### Compiling the ANN","a074fb0e":"### Fitting the ANN to the Training set","d0642d47":"### Importing Keras Packages\n\nFor building the Neural Network layer by layer","70307518":"### Splitting the dataset into the Training set and Test set\n\nIn Machine Learning, we make a model which is nothing but an algorithm where some parameters needs to be modified such that it is able to perform good at the application i.e it is able to predict values of one wants to.\n\nHow can we modify those parameters such that it can do well ?\nWe can train the model using data which we call as training data or training set. The training data is the one which already has the actual value that the model should have predicted and thus the algorithm changes the value of parameters to account for the data in the training set.\n\nBut how do we know after training the model is overall good ?\nFor that, we have test data\/test set which is basically a different data for which we know the values but this data was never shown to the model before. Thus if the model after training is performing good on test set as well then we can say that the Machine Learning model is good.\n\nIf the model is not tested and is made such that it just perform good on training data then parameters will be such that they are only good enough to predict the value for data which was in training set. That is not general. This is called overfitting.\n\nSo we don\u2019t land making a useless model which is only good for the training set and not general enough.\n\nThus test set and training set is important to make Machine Learning model better.","1b083a60":"### Initializing the ANN\n\nSo there are actually 2 ways of initializing a deep learning model\n\n1. Defining each layer one by one\n2. Defining a Graph\n\n\nWe did not put any parameter in the Sequential object as we will be defining the Layers manually","3cf1f7bc":"## Part 3 \u2014 Making the predictions and evaluating the model\n\n### Predicting the Test set results","8148fd4c":"## What is Churn Rate?\n\n**Churn rate** (sometimes called attrition rate), in its broadest sense, is a measure of the number of individuals or items moving out of a collective group over a specific period. It is one of two primary factors that determine the steady-state level of customers a business will support.\n","f15b31d5":"We store the **Dependent value\/predicted value** in y by storing the 13th index in the variable **y**.","b09a9ac7":"Creating label encoder object no. 2 to encode Gender name(index 2 in features)","36f68504":"### Importing the Keras libraries ","bcb8dae1":"### Converting Categorical Variable\n\nNow, We need to convert our categorical dependent variables into numeric dependent variables.\n\nCategorical variables are known to hide and mask lots of interesting information in a data set. It\u2019s crucial to learn the methods of dealing with such variables.\n\nThe only 2 values are Gender and Region which need to converted into numerical data.","6ce5adc4":"### Adding the output layer\n\n* Set Output Dim=1\n* Init will initialize the Hidden Layer weights uniformly\n* Activation Function is Sigmoid Activation Function(sigmoid)\n\n**Sigmoid activation function** is used whenever we need Probabilities of 2 categories or less(Similar to Logistic Regression)\n\n\nSwitch to **Softmax** when the dependent variable has more than 2 categories."}}