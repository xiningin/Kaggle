{"cell_type":{"69153af3":"code","f8ada749":"code","f854f501":"code","815090d4":"code","e1569512":"code","e7b5c67a":"code","5e712a76":"code","274a7da4":"code","39f77b3c":"code","656d6c89":"code","7bcdb926":"code","886d3df4":"code","9c61b66d":"code","5cffba4d":"code","4a90b4d7":"code","b26226de":"code","d8ee5ff6":"markdown","63824d61":"markdown","35a4181b":"markdown","88403702":"markdown","c2a472e2":"markdown","7502b4e6":"markdown","08d92f2f":"markdown","35b51d35":"markdown","f8df68fd":"markdown","a4dd9e13":"markdown","c71d9719":"markdown","b3d84bbc":"markdown","0afb1118":"markdown","61e1cb9a":"markdown","c0544f91":"markdown","b948e804":"markdown","7ac4d4b5":"markdown"},"source":{"69153af3":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.svm import SVC","f8ada749":"pd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\npd.options.mode.chained_assignment = None \n\ndf_train = pd.read_csv(\"..\/input\/houe-prices-train-and-test\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/houe-prices-train-and-test\/test.csv\")\n\nprint(df_train.shape)\nprint(df_test.shape)","f854f501":"df_all_raw = pd.concat([df_train, df_test], axis=0, sort=False)\ndf_all = df_all_raw.drop([\"Id\", \"SalePrice\"], axis=1)\ndf_all_withPrice = df_all_raw.drop(\"Id\", axis=1)\ntotal = df_all.isnull().sum().sort_values(ascending=False)\npercent = (df_all.isnull().sum()\/df_all.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(25)\nprint(missing_data)","815090d4":"df_all = df_all.drop([\"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\"], axis=1)","e1569512":"corr_withPrice = df_all_withPrice.corr()\nhighest_corr_features = corr_withPrice.index[abs(corr_withPrice[\"SalePrice\"])>0.5]\ng = sns.heatmap(df_all_withPrice[highest_corr_features].corr(),annot=True,cmap=\"coolwarm\")\nplt.figure(figsize=(15,15))\nplt.show()","e7b5c67a":"corr = df_all.corr()\ncorr_pairs = corr.unstack()\nsorted_pairs = corr_pairs.sort_values(kind=\"quicksort\")\nprint(sorted_pairs[sorted_pairs > 0.7])","5e712a76":"corr_withPrice = df_all_withPrice.corr()\ncorr_Price = corr_withPrice[\"SalePrice\"].sort_values(ascending=False)\nredundant_candidates = [\"1stFlrSF\", \"TotalBsmtSF\", \"GrLivArea\", \"TotRmsAbvGrd\", \"YearBuilt\", \"GarageYrBlt\", \"GarageCars\", \"GarageArea\"]\nfor candidate in redundant_candidates:\n    print(\"{} 's correlation with SalePrice is {}\".format(candidate, corr_Price[candidate]))","274a7da4":"redundant_features = [\"1stFlrSF\", \"TotRmsAbvGrd\", \"GarageYrBlt\", \"GarageArea\"]\nfeatures_lowCorr_SalePrice = corr_Price.loc[lambda x: abs(x) < 0.1].index.tolist()\ndf_all = df_all.drop(features_lowCorr_SalePrice, axis=1)\ndf_all = df_all.drop(redundant_features, axis=1)","39f77b3c":"# find features with missing value\nfeatures_missing = df_all.columns[df_all.isna().any()].tolist()\n# identify numeric and categorial features in feature_missing\nnumeric_features_missing = df_all[features_missing].select_dtypes(include=[\"int64\", \"float64\"]).columns\ncategorical_features_missing = [i for i in features_missing if i not in numeric_features_missing]\n\n# Fill LotFrontage with mean value\ndf_all[\"LotFrontage\"].fillna((df_all[\"LotFrontage\"].mean()), inplace=True)\n\n# Fill some numeric cols with 0 since missing means not having\nfor col in [\"GarageCars\", \"MasVnrArea\", \"BsmtFinSF1\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"BsmtFullBath\"]:\n    df_all[col] = df_all[col].fillna(0)\n# Fill some cols with most frequent value\nfor col in ['OverallQual', 'MSZoning', 'Utilities', 'Exterior1st', 'Exterior2nd', 'Electrical', 'KitchenQual', \"Functional\", 'SaleType']:\n    df_all[col] = df_all[col].fillna(df_all[col].mode().iloc[0])\n# Fill some categorical cols with None since missing means not having\nfor col in ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'MasVnrType', 'FireplaceQu']:\n    df_all[col] = df_all[col].fillna('None')\n\n# Check for missing values\nprint(df_all.isnull().values.any())","656d6c89":"# get numeric part of df_all, and visualize outliners\ndf_train_missingFilled = df_all.iloc[:len(df_train), :]\ndf_train_missingFilled[\"SalePrice\"] = df_train[\"SalePrice\"]\nnumeric_features = df_train_missingFilled.select_dtypes(include=[\"int64\", \"float64\"]).columns\ndf_numeric = df_train_missingFilled.loc[:, numeric_features]\nclf = IsolationForest(max_samples=200, random_state=42, contamination=0.01)\nclf.fit(df_numeric)\npred = clf.predict(df_numeric)\n\nmarkers = {1: \".\", -1:\"X\", -2:\"X\"}\nsns.set(style='ticks', font_scale=0.6)\nfig, axs = plt.subplots(12, 2, num=\"outliers\", figsize=(16, 25), sharey=True)\naxs = axs.flatten()\n\nfor i, feature in enumerate(numeric_features):\n        ax = axs[i]\n        sns.scatterplot(x=df_numeric[feature], y=df_numeric['SalePrice'],\n                        hue=pred, style=pred, palette=\"tab10\",\n                        markers=markers, hue_order=[1,-1,-2], ax=ax, legend=False)\n        xlabel = ax.get_xlabel()\n        ax.text(0.95, 0.85, xlabel, transform=ax.transAxes, ha='right', fontsize=10)\n\nplt.show()","7bcdb926":"numeric_features_list = numeric_features.to_list()\nnumeric_features_list.remove(\"SalePrice\")\ndf_numeric = df_all.loc[:, numeric_features_list]\n# define helper function to get outliner info\ndef get_outliner_info(df, feature):\n    clf = IsolationForest(max_samples=200, random_state=42, contamination=0.01)\n    clf.fit(df[[feature]])\n    pred = clf.predict(df[[feature]])\n    scores = clf.decision_function(df[[feature]])\n    stats = pd.DataFrame()\n    stats['val'] = df[feature]\n    stats['score'] = scores\n    stats['outlier'] = pred\n    stats['min'] = df[feature].min()\n    stats['max'] = df[feature].max()\n    stats['mean'] = df[feature].mean()\n    stats['feature'] = [feature] * len(df)\n    return stats\n\ndef print_outliers(df, feature, n):\n    print(feature)\n    print(df[feature].head(n).to_string(), \"\\n\")\n    \nresult = pd.DataFrame()\nfor feature in numeric_features_list:\n    stats = get_outliner_info(df_numeric, feature)\n    result = pd.concat([result, stats])\noutliers = {numeric_feature:sub_result.drop('feature', axis=1)\n       for numeric_feature, sub_result in result.sort_values(by='score').groupby('feature')}\n# get first 10 outliners from each numeric feature\nfor feature in numeric_features_list:\n    print_outliers(outliers, feature, 10)","886d3df4":"# trim outliners based on observation\ndf_all.LotArea = df_all.LotArea.clip(1300,70000)\ndf_all.YearBuilt = df_all.YearBuilt.clip(1880,2010)\ndf_all.BsmtFinSF1 = df_all.BsmtFinSF1.clip(0,2000)\ndf_all.BsmtUnfSF = df_all.BsmtUnfSF.clip(0,2000)\ndf_all.TotalBsmtSF = df_all.TotalBsmtSF.clip(0,4000)\ndf_all[\"2ndFlrSF\"] = df_all[\"2ndFlrSF\"].clip(0,1500)\ndf_all.GrLivArea = df_all.GrLivArea.clip(334,3800)\ndf_all.WoodDeckSF = df_all.WoodDeckSF.clip(0, 1000)\ndf_all.OpenPorchSF = df_all.OpenPorchSF.clip(0,500)\ndf_all.EnclosedPorch = df_all.EnclosedPorch.clip(0,600)\ndf_all.ScreenPorch = df_all.ScreenPorch.clip(0,400)","9c61b66d":"df_numeric_trimmed = df_all.loc[:, numeric_features_list]\nresult_trimmed = pd.DataFrame()\n\nfor feature in numeric_features_list:\n    stats = get_outliner_info(df_numeric_trimmed, feature)\n    result_trimmed = pd.concat([result_trimmed, stats])\n\noutliers_trimmed = {numeric_feature:sub_result.drop('feature', axis=1)\n       for numeric_feature, sub_result in result_trimmed.sort_values(by='score').groupby('feature')}\n\n# get first 10 outliners from each numeric feature\nfor feature in numeric_features_list:\n    print_outliers(outliers_trimmed, feature, 10)","5cffba4d":"# split df_all to X_train and X_text\nX_train = df_all.iloc[:len(df_train), :]\nY_train = df_train[\"SalePrice\"]\nX_test = df_all.iloc[len(df_train):, :]","4a90b4d7":"# set up data transformation in preprocessor and the model\n\nnumeric_transformer = Pipeline(steps=[\n    ('scaler', StandardScaler())])\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\nnumeric_features = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns\ncategorical_features = X_train.select_dtypes(include=['object']).columns\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)])\nrf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', GradientBoostingRegressor())])\n\nrf.fit(X_train, Y_train)\nY_pred = rf.predict(X_train)\nR_squared_2 = r2_score(Y_train, Y_pred)\nRMSE_2 = mean_squared_error(Y_train, Y_pred)\n\nprint(\"R2: \", R_squared_2)\nprint(\"RMSE: \", RMSE_2)","b26226de":"# Make pridiction with the trained model for X_text\ntest_pred = rf.predict(X_test)\ntest_pred_frame = pd.DataFrame(data=test_pred.flatten(),columns=['Values'])\nsubmission_df = pd.DataFrame(columns= [\"Id\", \"SalePrice\"])\nsubmission_df[\"Id\"] = df_test[\"Id\"]\nsubmission_df[\"SalePrice\"] = test_pred_frame\nsubmission_df.to_csv('submissions.csv', header=True, index=False)","d8ee5ff6":"**Import Libraries**","63824d61":"**Get Hign Correlated Pairs of Features**","35a4181b":"Accordingly, OverallQual and GrLivArea have strong correlations with SalePrice.","88403702":"Therefore I decided to drop 1stFlrSF, TotRmsAbvGrd, GarageYrBlt and GarageArea.","c2a472e2":"**Drop these four sparse cols:**\n![image.png](attachment:image.png)","7502b4e6":"**Concatenate Two Datasets and Check Missing Value Count & Percentage**","08d92f2f":"**Then visualize outliners in training dataset and trim outliners in df_all:**","35b51d35":"I also decide to drop features whose correlation with SalePrice is lower than 0.1:","f8df68fd":"Based on the score and the value of outliners, use pandas's clip function to remove outliners for features.","a4dd9e13":"After removing outliners, train IsolationForest again:","c71d9719":"**Draw Correlation Matrix for Features whose Correlation with SalePrice is over 0.5**","b3d84bbc":"**Set Display Options and Load Data**","0afb1118":"Here comes to the step 3, fill null values,","61e1cb9a":"Here are notebooks and other websites that I find help a lot, really appreciate people sharing their wonderful insight in this field:\n* https:\/\/www.kaggle.com\/adamml\/how-to-be-in-top-10-for-beginner\n* https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial\n* https:\/\/www.kaggle.com\/shuiyaohuang\/house-price-i-comprehensive-eda\n* https:\/\/www.kaggle.com\/shuiyaohuang\/house-price-from-50-to-top-4\n* https:\/\/www.kaggle.com\/christianlillelund\/tutorial-use-isolationforest-to-detect-outliers\n* https:\/\/www.kaggle.com\/pouryaayria\/a-complete-ml-pipeline-tutorial-acu-86\n* https:\/\/d2l.ai\/chapter_multilayer-perceptrons\/kaggle-house-price.html?highlight=kaggle\n* https:\/\/towardsdatascience.com\/outlier-detection-with-isolation-forest-3d190448d45e\n* https:\/\/towardsdatascience.com\/polynomial-regression-with-a-machine-learning-pipeline-7e27d2dedc87\n* https:\/\/medium.com\/vickdata\/a-simple-guide-to-scikit-learn-pipelines-4ac0d974bdcf\n\nFrom my point of view, here are some points that I gonna cover to get higher rank in this competition:\n* Do the Feature Engineering in a more detailed way.\n* Learn how to test and validate the model, in this notebook, I train the model with all the training data, which is not a good practice. However, it takes time for me to understand the test and validation process.\n* Explore the Multilayer Perceptron, and I find the nice tutorial from D2L:\nhttps:\/\/d2l.ai\/chapter_multilayer-perceptrons\/kaggle-house-price.html?highlight=kaggle, really helpful, but also takes time to digest.\n\nPlease comment below if you have any idea that I can improve, or questions, thanks.","c0544f91":"As a beginner, I acquired basic knowledge from many notebooks in this competition, and tried to combine these together. Currently my score is 0.14188, still a long way to go.\n\n**Here are main steps in my workflow:**\n1. Check Missing Value in Each Feature and Drop Sparse Columns\n2. Check Redundant Columns and Remove Them\n3. Fill Missing Values\n4. Trim Outliners\n5. Feature Transformation\n6. Train the Model and Make the Pridiction","b948e804":"From the second training output, it can be perceived that trimmed features have lower score than before.\nThe dataset is set up now, let's go to the trainsformation and training step:","7ac4d4b5":"In each pair, I will remove the one which has lower correlation with SalePrice when compared with another."}}