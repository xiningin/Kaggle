{"cell_type":{"e7e4332d":"code","0205ee6b":"code","5cb6163c":"code","e213fff9":"code","c4d427ac":"code","ec29e607":"code","ea4ba53f":"code","3045ef07":"code","2c00f2f3":"code","abff92a2":"code","10251aa3":"code","239385e0":"code","95de8e9c":"code","a620af9d":"code","f210b908":"code","8c4a5240":"code","0bee3b22":"code","9b5d715b":"code","ecd4f577":"code","f23c02dd":"code","551954e8":"code","2263659d":"code","8b01dcd4":"code","c83d19f2":"code","3214f562":"code","53c4bfe6":"code","945e4949":"code","76e84760":"code","7e65e968":"code","d28b4f9b":"code","7431700f":"code","48216465":"code","f742dd3b":"code","c39390aa":"code","688e1606":"code","24dd33a7":"code","95d2a22d":"code","eafc2041":"code","97007377":"code","2d3cb3cb":"code","3ca6b689":"code","52cbf1b4":"code","73e4cafb":"code","ff22ed24":"code","da6f9e2e":"markdown","3fe8623d":"markdown","52e62445":"markdown","5e47fa89":"markdown","da34cdc5":"markdown","499db719":"markdown","df3663bd":"markdown","d14dc2c3":"markdown","8b484e84":"markdown","a4991b91":"markdown","073a67e1":"markdown","96b1aa3b":"markdown","f47bd196":"markdown","63099bf2":"markdown","ff467c10":"markdown","882f3a8c":"markdown","c1efe405":"markdown"},"source":{"e7e4332d":"from IPython.core.magic import register_cell_magic\n\n@register_cell_magic\ndef write_and_run(line, cell):\n    argz = line.split()\n    file = argz[-1]\n    mode = 'w'\n    if len(argz) == 2 and argz[0] == '-a':\n        mode = 'a'\n    with open(file, mode) as f:\n        f.write(cell)\n    get_ipython().run_cell(cell)","0205ee6b":"%%write_and_run main.py \nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os, time, datetime\nfrom IPython.display import clear_output\nfrom os.path import join","5cb6163c":"%%write_and_run -a main.py \nfrom PIL import Image\nfrom PIL import ImageDraw\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms\ndef location(loc):\n    row = 7\n    column = 11\n    return loc % column, loc \/\/ column\n\ndef custom_env_render(obs, row=7, column=11):\n    txt = Image.new(\"RGBA\", (column * 30,row * 30), (255, 255, 255))\n    draw = ImageDraw.Draw(txt)\n    grid_size = 30\n    #draw grid\n    for i in range(column):\n        for j in range(row):\n            x1 = i*grid_size\n            y1 = j*grid_size\n            x2 = (i+1)*grid_size\n            y2 = (j+1)*grid_size\n            draw.rectangle(((x1, y1), (x2, y2)), outline='black', width=3)\n    \n    for food_location in obs['food']:\n        # draw foods\n        x,y = location(food_location)\n        draw.ellipse((x*grid_size, y*grid_size, (x+1)*grid_size, (y+1)*grid_size), fill = 'blue', outline ='blue')\n        \n    color_list = ['red', 'yellow', 'green', 'purple']\n    for i, geese in enumerate(obs['geese']):\n        for j, body in enumerate(geese):\n            x,y = location(body)\n            if j==0:\n                #draw head\n                draw.polygon([((x+1\/2)*grid_size,y*grid_size),(x*grid_size,(y+1\/2)*grid_size),\n                              ((x+1\/2)*grid_size,(y+1)*grid_size), ((x+1)*grid_size,(y+1\/2)*grid_size)], fill = color_list[i])\n            else:\n                #draw body\n                draw.rectangle(((x*grid_size, y*grid_size), ((x+1)*grid_size, (y+1)*grid_size)), fill=color_list[i], outline=color_list[i])\n\n    txt = txt.resize((64, 64)).convert('RGB')\n    numpy_image = np.array(txt)\n    plt.axis(\"off\")\n    trans1 = transforms.ToTensor()\n    tensor_image = trans1(numpy_image)\n    \n    return tensor_image\n\ndef custom_reward(obs):\n    #reward = steps survived * (configuration.max_length + 1) + goose length\n    \n    # scaling reward to 0 ~ 1\n    max_reward = 200 * (99 + 1) + 99\n    steps_survived = obs['step']\n    max_length = 99\n    goose_length = len(obs['geese'][0])\n    \n    reward = (steps_survived * (max_length + 1) + goose_length) \/ max_reward\n    \n    return reward","e213fff9":"def defensive_random_action(prev_action, obs):\n    # Agent that try to avoid death as much as possible and move randomly\n    prohibition = set([])\n    action_list = ['NORTH','SOUTH','WEST','EAST']\n    num_action = len(action_list)\n    row = 7\n    column = 11\n    geese = obs['geese']\n    g_list = []\n    for gs in geese:\n        g_list.extend(gs)\n    my_head = g_list.pop(0)\n    myhead = location(my_head)\n    \n    for g in g_list:\n        g = location(g)\n        if g[0] == myhead[0] and (g[1] - myhead[1] == 1 or (g[1] == 0 and myhead[1] == row)):\n            prohibition.add('SOUTH')\n        if g[0] == myhead[0] and (g[1] - myhead[1] == -1 or (g[1] == row and myhead[1] == 0)):\n            prohibition.add('NORTH')\n        if g[1] == myhead[1] and (g[0] - myhead[0] == 1 or (g[0] == 0 and myhead[0] == column)):\n            prohibition.add('EAST')\n        if g[1] == myhead[1] and (g[0] - myhead[0] == -1 or (g[0] == column and myhead[0] == 0)):\n            prohibition.add('WEST')\n    if prev_action is not None:\n        opposite_action = {'NORTH':'SOUTH' , 'SOUTH':'NORTH', 'EAST':'WEST', 'WEST':'EAST' }\n        prohibition.add(opposite_action[prev_action])\n    \n    \n    if len(prohibition) < num_action:\n        action = random.choice([x for x in action_list if x not in prohibition])\n    else:\n        # no way to survive\n        action = random.choice(action_list)\n    return action","c4d427ac":"from kaggle_environments import make\nimport random\nenv = make(\"hungry_geese\", debug=False)\n\n# Training agent in first position (player 1) against the default random agent.\ntrainer = env.train([None, \"greedy\", \"greedy\", \"greedy\"])\n\nobs = trainer.reset()\nimg = custom_env_render(obs, True)\nprint(obs)\nprev_action = None\nfor _ in range(3):\n    action = defensive_random_action(prev_action, obs)\n    obs, reward, done, info = trainer.step(action)\n    \n    # change reward to custom reward\n    reward = custom_reward(obs)\n    prev_action = action\n    img_tensor = custom_env_render(obs)\n    tf = transforms.ToPILImage()\n    img_t = tf(img_tensor)\n    plt.imshow(img_t)\n    plt.show()\n    plt.close()\n    print('obs :{}'.format(obs))\n    print('reward :{}'.format(reward))\n    print('done :{}'.format(done))\n    if done:\n        obs = trainer.reset()\n        print('episode ended')\n        break","ec29e607":"total_episodes = 250\ntime_steps = 200","ea4ba53f":"class Rollout():\n    def __init__(self, data_dic, dir_name):\n        super().__init__()\n        self.data_dic = data_dic\n        self.dir_name = dir_name\n        \n    def make_rollout(self):\n        if not os.path.exists(self.dir_name):\n            os.makedirs(self.dir_name)\n\n        env = make(\"hungry_geese\", debug=False)\n\n        trainer = env.train([None, \"greedy\", \"greedy\", \"greedy\"])\n        s = 0\n        start_time = time.time()\n        while s < total_episodes:\n            obs_sequence = []\n            action_sequence = []\n            reward_sequence= []\n            done_sequence = []\n            img_tensor_sequence = []\n            obs = trainer.reset()\n            img = custom_env_render(obs, True)\n            prev_action = None\n            for t in range(time_steps):\n                env.render()\n                action = defensive_random_action(prev_action, obs)\n                obs, reward, done, info = trainer.step(action)\n                reward = custom_reward(obs)\n                prev_action = action\n                img_tensor = custom_env_render(obs)\n                img_tensor_sequence.append(img_tensor)   \n                obs_sequence.append(obs)\n                action_sequence.append(action)\n                reward_sequence.append(reward)\n                done_sequence.append(done)\n\n                t+=1\n                if done:\n                    clear_output(wait = True)\n                    print(\"Episode [{}\/{}] finished after {} timesteps\".format(s + 1,total_episodes, t), flush=True)\n                    obs = trainer.reset()\n                    break\n            self.data_dic[s] = {\"img_sequence\": img_tensor_sequence, \"obs_sequence\":obs_sequence, \"action_sequence\":action_sequence, \n                        \"reward_sequence\":reward_sequence, \"done_sequence\":done_sequence}        \n            s+=1\n\n        torch.save(self.data_dic, self.dir_name  + \"save_rollout.pt\")\n        \n        end_time = time.time()-start_time\n        times = str(datetime.timedelta(seconds=end_time)).split(\".\")\n        print('Finished in {0}'.format(times[0]))\n        \n    def pad_tensor(self, tensor, pad):\n        pad_size = pad - tensor.size(0)\n        return torch.cat([tensor.to(device), torch.zeros([pad_size, tensor.size(1)]).to(device)], dim=0)\n    \n    def fit_dataset_to_rnn(self, vae, hiddens):\n        dir2code = {\"EAST\":torch.Tensor([1,0,0,0]), \"NORTH\": torch.Tensor([0,1,0,0]), \"WEST\":torch.Tensor([0,0,1,0]), \"SOUTH\": torch.Tensor([0,0,0,1])}\n        \n        for episode_idx, episode_data in enumerate(self.data_dic.values()):\n            mu_sequence = []\n            log_var_sequence = []\n            \n            img_sequence = episode_data['img_sequence']\n            action_sequence = episode_data['action_sequence']\n            reward_sequence = episode_data['reward_sequence']\n            done_sequence = episode_data['done_sequence']\n            \n            for img in img_sequence:\n                img = img.to(device)\n                mu, log_var = vae.Encoder(img.unsqueeze(0))\n                mu_sequence.append(mu)\n                log_var_sequence.append(log_var)\n            mu_sequence = torch.stack(mu_sequence, dim=0).squeeze(1)\n            log_var_sequence = torch.stack(log_var_sequence, dim=0).squeeze(1)\n            \n            done = [int(d) for d in done_sequence]\n            done = torch.tensor(done).unsqueeze(-1)\n            done_sequence = self.pad_tensor(done, pad=time_steps)\n            \n            action = [dir2code[a] for a in action_sequence]\n            action = torch.stack(action, dim=0)\n            action_sequence = self.pad_tensor(action, pad=time_steps)\n            \n            reward = torch.tensor(reward_sequence).unsqueeze(-1)\n            reward_sequence = self.pad_tensor(reward, pad=time_steps)\n            #reward = self.pad_tensor(reward, pad=time_steps)\n            #reward_sequence = torch.where(reward > 0 , 1, 0) * torch.where(done_sequence==0, 1, 0)\n            \n            episode_data['mu_sequence'] = mu_sequence\n            episode_data['log_var_sequence'] = log_var_sequence\n            episode_data['action_sequence'] = action_sequence\n            episode_data['done_sequence'] = done_sequence\n            episode_data['reward_sequence'] = reward_sequence\n            \n        torch.save(self.data_dic, self.dir_name + 'save_rollout_rnn.pt')","3045ef07":"rollout_dic = {}\nrollout_dir = 'data\/'\nro = Rollout(rollout_dic, rollout_dir)\nro.make_rollout()","2c00f2f3":"# ro.data_dic[episode index][data type][step index]\nprint(ro.data_dic[0]['img_sequence'][0].size())\nprint(ro.data_dic[0]['obs_sequence'][0])\nprint(ro.data_dic[0]['action_sequence'][0])\nprint(ro.data_dic[0]['reward_sequence'][0])\nprint(ro.data_dic[0]['done_sequence'][0])","abff92a2":"%%write_and_run -a main.py \n\nclass Encoder(nn.Module):\n    def __init__(self, latent_size):\n        super().__init__()\n        \n        # Encoder\n        self.enc = nn.Sequential(\n            nn.Conv2d(3, 32, 4, stride=2),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, 4, stride=2),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, 4, stride=2),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, 4, stride=2),\n            nn.ReLU(),\n            nn.Flatten()\n        )\n                \n        self.log_var = nn.Linear(512, latent_size)\n        self.mu = nn.Linear(512,latent_size)\n        \n    def forward(self, x):\n        x = self.enc(x)\n        mu = self.mu(x)\n        log_var = self.log_var(x)\n        \n        return mu, log_var\n\nclass Decoder(nn.Module):\n    def __init__(self, latent_size):\n        super().__init__()\n        # Decoder\n        self.dense_layer = nn.Sequential(\n            nn.Linear(latent_size, 1024),\n            nn.ReLU()\n        )\n        \n        self.dec_reshaped = nn.Sequential(\n            nn.ConvTranspose2d(1024, 64, 5, stride=2),\n            nn.ReLU(),\n            nn.ConvTranspose2d(64, 64, 5, stride=2),\n            nn.ReLU(),\n            nn.ConvTranspose2d(64, 32, 6, stride=2),\n            nn.ReLU(),\n            nn.ConvTranspose2d(32, 3, 6, stride=2),\n            nn.Sigmoid(),\n        )\n    def forward(self, z):\n        densed = self.dense_layer(z)\n        densed_reshaped = densed.unsqueeze(-1).unsqueeze(-1)\n        \n        recon = self.dec_reshaped(densed_reshaped)\n        \n        return recon\n\nclass VAE(nn.Module):\n    def __init__(self, latent_size):\n        super().__init__()\n        \n        self.Encoder = Encoder(latent_size)\n        self.Decoder = Decoder(latent_size)\n        \n        \n    def forward(self, x):\n        mu, log_var = self.Encoder(x)\n        \n        sigma = log_var.exp()\n        eps = torch.randn_like(sigma)\n        z = eps.mul(sigma).add_(mu)\n        \n        recon = self.Decoder(z)\n        \n        return recon, mu, log_var\n        ","10251aa3":"class VAE_Dataset(torch.utils.data.Dataset):\n    def __init__(self, obs_data):\n        self.obs_data = obs_data\n        \n    def __len__(self):\n        return len(obs_data)\n\n    def __getitem__(self, idx):\n        data = obs_data[idx]\n        return data","239385e0":"def flating_obs_data(data):\n    imgs = []\n    for episode_data in data.values():\n        imgs = imgs + episode_data['img_sequence']\n    print('num_of_images: {}'.format(len(imgs)))\n    imgs = torch.stack(imgs, dim=0)\n    print('obs_dataset.size :', imgs.size())\n    return imgs","95de8e9c":"obs_data = flating_obs_data(ro.data_dic)\nprint('image size :',obs_data[0].size())\nplt.imshow(np.transpose(obs_data[0].detach().numpy().squeeze(), (1,2,0)))\n\ntrain_dataset = VAE_Dataset(obs_data)\n\nbatch_size = 32\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, \n                                               batch_size=batch_size, shuffle=True)","a620af9d":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nlatents = 32\nactions = 4\nhiddens = 256\ngaussians = 5\n\nepochs = 100\n\nvae = VAE(latents).to(device)\n\nlr = 2e-4\noptim_vae = torch.optim.Adam(vae.parameters(), lr=lr)","f210b908":"def vae_r_loss(x, recon_x):\n    BCE = F.mse_loss(recon_x, x, size_average=False)\n    return BCE\n\n#kullback-Leibler divergence \ndef vae_kl_loss(mu, log_var):\n    kl_loss = -0.5 * torch.sum(1 + 2 * log_var - mu.pow(2) - (2 * log_var).exp())\n    return kl_loss\n\ndef vae_loss(x, recon_x, mu, log_var):\n    r_loss = vae_r_loss(x, recon_x)\n    kl_loss = vae_kl_loss(mu, log_var)\n    \n    return r_loss + kl_loss\n\n\nvae.train()\nstart_time = time.time()\nbest_loss = float(\"inf\")\n\nepoch_ = []\nepoch_train_loss = []\nfor epoch in range(1, epochs + 1):\n    train_loss = 0\n    for batch_idx, data in enumerate(train_dataloader):\n        obs = data.to(device)\n        \n        recon_x , mu, log_var = vae(obs)\n        optim_vae.zero_grad()\n        loss = vae_loss(obs, recon_x, mu, log_var)\n        loss.backward()\n        train_loss += loss.item()\n        optim_vae.step()\n    train_loss = train_loss\/ len(train_dataset)\n    \n    if train_loss <= best_loss:\n        if not os.path.exists('model'):\n            os.makedirs('model')\n        torch.save(vae.state_dict(), 'vae.pt')\n        best_loss = train_loss\n    clear_output(wait = True)\n    epoch_.append(epoch)\n    epoch_train_loss.append(train_loss)\n    \n    print('EPOCH : {} Average_loss : {}'.format(epoch, train_loss))\n\nfig = plt.figure(figsize=(8,8))\nfig.set_facecolor('white')\nax = fig.add_subplot()\n \nax.plot(epoch_,epoch_train_loss, label='Average loss')\nax.legend()\nax.set_xlabel('epoch')\nax.set_ylabel('loss')\n\nplt.show()\n\nend_time = time.time() - start_time\ntimes = str(datetime.timedelta(seconds=end_time)).split(\".\")\nprint('Finished in {0}'.format(times[0]))","8c4a5240":"f = plt.figure()\n\nimg = ro.data_dic[0]['img_sequence'][-1].unsqueeze(0)\nimg_sample = np.transpose(img.squeeze(), (1,2,0))\np1 = f.add_subplot(1, 2, 1)\np1.imshow(img_sample)\np1.set_title('VAE input')\np1.axis(\"off\")\nimg = img.to(device)\nrecon, mu, log_var = vae(img)\nrecon = recon.to('cpu')\nrecon = np.transpose(recon.detach().numpy().squeeze(), (1,2,0))\np2 = f.add_subplot(1, 2, 2)\np2.imshow(recon)\np2.set_title('VAE output')\np2.axis(\"off\")\nplt.show(block=True)","0bee3b22":"ro.fit_dataset_to_rnn(vae, hiddens)\nprint(ro.data_dic[0]['reward_sequence'].size())\nprint(ro.data_dic[0]['done_sequence'].size())\nprint(ro.data_dic[0]['mu_sequence'].size())\nprint(ro.data_dic[0]['log_var_sequence'].size())","9b5d715b":"%%write_and_run -a main.py \nclass _MDRNNBase(nn.Module):\n    def __init__(self, latents, actions, hiddens, gaussians):\n        super().__init__()\n        self.latents = latents\n        self.actions = actions\n        self.hiddens = hiddens\n        self.gaussians = gaussians\n\n        self.gmm_linear = nn.Linear(\n            hiddens, gaussians * 3 * latents + 1)\n\n    def forward(self, *inputs):\n        pass\n\n\nclass MDN_RNN(_MDRNNBase):\n    def __init__(self, latents, actions, hiddens, gaussians):\n        super().__init__(latents, actions, hiddens, gaussians)\n        self.latents = latents\n        self.actions = actions\n        self.hiddens = hiddens\n        self.gaussians = gaussians\n        self.rnn = nn.LSTM(1 + actions + latents, hiddens)\n        self.gmm_linear = nn.Linear(hiddens, gaussians * 3 * latents + 1)\n    def forward(self, ins):\n        outs, _ = self.rnn(ins)\n        gmm_outs = self.gmm_linear(outs)\n        \n        return gmm_outs\n    \nclass MDRNNCell(_MDRNNBase):\n    \"\"\" MDRNN model for one step forward \"\"\"\n    def __init__(self, latents, actions, hiddens, gaussians):\n        super().__init__(latents, actions, hiddens, gaussians)\n        self.rnn = nn.LSTMCell(1 + latents + actions, hiddens)\n\n    def forward(self, input, hidden): # pylint: disable=arguments-differ\n        next_hidden = self.rnn(input, hidden)\n        out_rnn = next_hidden[0]\n        out_full = self.gmm_linear(out_rnn)\n\n        return out_full, next_hidden","ecd4f577":"class MDN_Dataset(torch.utils.data.Dataset):\n    def __init__(self, MDN_data):\n        self.MDN_data = MDN_data\n    def __len__(self):\n        return len(self.MDN_data)\n\n    def __getitem__(self, idx):\n        data = self.MDN_data[idx]\n        mu_sequence = data['mu_sequence']\n        log_var_sequence = data['log_var_sequence']\n        mu_sequence = ro.pad_tensor(data['mu_sequence'], pad=time_steps)\n        log_var_sequence = ro.pad_tensor(data['log_var_sequence'], pad=time_steps)\n        sigma = log_var_sequence.exp()\n        eps = torch.randn_like(sigma)\n        z = eps.mul(sigma).add_(mu_sequence)\n        action = data['action_sequence']\n        reward = data['reward_sequence']\n        return (reward, action, z)","f23c02dd":"train_dataset = MDN_Dataset(ro.data_dic)\n\nbatch_size = 32\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, \n                                               batch_size=batch_size, shuffle=True)","551954e8":"rnn = MDN_RNN(latents, actions, hiddens, gaussians).to(device)\nepochs = 150\nlr = 2e-4\noptim_rnn = torch.optim.Adam(rnn.parameters(), lr=lr)","2263659d":"Z_FACTOR = 1\nREWARD_FACTOR = 5\n\ndef get_responses(y_true):\n\n    z_true = y_true[:,:,:latents]\n    rew_true = y_true[:,:,-1]\n\n    return z_true, rew_true\n\n\ndef get_mixture_coef(z_pred):\n    log_pi, mu, log_sigma = torch.split(z_pred, 5, 1)\n    log_pi = log_pi - torch.log(torch.sum(log_pi.exp(), axis = 1, keepdims = True)) # axis 1 is the mixture axis\n\n    return log_pi, mu, log_sigma\n\n\ndef lognormal(z_true, mu, log_sigma):\n\n    logSqrtTwoPI = np.log(np.sqrt(2.0 * np.pi))\n    return -0.5 * ((z_true - mu) \/ log_sigma.exp()) ** 2 - log_sigma - logSqrtTwoPI\n\n\n\ndef rnn_z_loss(y_true, y_pred):\n    z_true, rew_true = get_responses(y_true) \n    d = gaussians * latents\n    z_pred = y_pred[:,:,:(3*d)]\n    z_pred = torch.reshape(z_pred, [-1, gaussians * 3])\n\n    log_pi, mu, log_sigma = get_mixture_coef(z_pred)\n\n    flat_z_true = torch.reshape(z_true,[-1, 1])\n\n    z_loss = log_pi + lognormal(flat_z_true, mu, log_sigma)\n    z_loss = -torch.log(torch.sum(z_loss.exp(), 1, keepdims=True))\n\n    z_loss = torch.mean(z_loss) \n\n    return z_loss\n\ndef rnn_rew_loss(y_true, y_pred):\n    z_true, rew_true = get_responses(y_true) #, done_true\n    reward_pred = y_pred[:,:,-1]\n    rew_loss = F.binary_cross_entropy(F.sigmoid(reward_pred), rew_true, reduce=False)\n    rew_loss = torch.mean(rew_loss)\n\n    return rew_loss\n\ndef rnn_loss(y_true, y_pred):\n\n    z_loss = rnn_z_loss(y_true, y_pred)\n    rew_loss = rnn_rew_loss(y_true, y_pred)\n\n    return (Z_FACTOR * z_loss + REWARD_FACTOR * rew_loss) \/ (Z_FACTOR + REWARD_FACTOR)\n","8b01dcd4":"start=time.time()\nbest_loss = float(\"inf\")\nepoch_ = []\nepoch_train_loss = []\nrnn.train()\nfor epoch in range(1, epochs + 1):\n    train_loss = 0\n    for batch_idx, (reward, action, z) in enumerate(train_dataloader):\n        rnn_input = torch.cat((z[:, :-1, :], action[:, :-1, :], reward[:,  :-1, :]), -1).detach().to(device)\n        with torch.no_grad():\n            rnn_output = torch.cat((z[:, 1:, :], reward[:, 1:, :]), -1).to(device)\n        out = rnn(rnn_input)\n        \n        optim_rnn.zero_grad()\n        loss_rnn = rnn_loss(rnn_output, out)\n        loss_rnn.backward(retain_graph=True)\n        train_loss += loss_rnn.item()\n\n        optim_rnn.step()\n        \n    train_loss = train_loss\/ len(train_dataset)\n    \n    if train_loss <= best_loss:\n        if not os.path.exists('model'):\n            os.makedirs('model')\n        torch.save(rnn.state_dict(), 'MDN_RNN.pt')\n        best_loss = train_loss\n    clear_output(wait = True)\n    epoch_.append(epoch)\n    epoch_train_loss.append(train_loss)\n\n    print('EPOCH : {} Average_loss : {}'.format(epoch, train_loss))\n\nfig = plt.figure(figsize=(8,8))\nfig.set_facecolor('white')\nax = fig.add_subplot()\n \nax.plot(epoch_,epoch_train_loss, label='Average loss')\n\n\nax.legend()\nax.set_xlabel('epoch')\nax.set_ylabel('loss')\n\nplt.show()\nend_time = time.time() - start_time\ntimes = str(datetime.timedelta(seconds=end_time)).split(\".\")\nprint('Finished in {0}'.format(times[0]))","c83d19f2":"rnn.load_state_dict(torch.load('MDN_RNN.pt'))\nreward, action, z = train_dataset[0]\nprint('reward :', reward.size(), 'action :', action.size(), 'z :', z.size())\nrnn_input = torch.cat((z[:-1, :], action[:-1, :], reward[:-1, :]), -1).unsqueeze(0).to(device)\nout = rnn(rnn_input)\nprint('MDN_RNN Output  :', out.size())","3214f562":"%%write_and_run -a main.py \nclass Controller(nn.Module):\n    \"\"\" Controller \"\"\"\n    def __init__(self, latents, hiddens, actions):\n        super().__init__()\n        self.fc = nn.Linear(latents + hiddens, actions)\n\n    def forward(self, inputs):\n        return self.fc(inputs)","53c4bfe6":"controller = Controller(latents, hiddens, actions).to(device)","945e4949":"!pip install cma\nimport cma","76e84760":"%%write_and_run -a main.py \ndef prevent_opposite_action(action, prev_action):\n    opposite_action = {'NORTH':'SOUTH' , 'SOUTH':'NORTH', 'EAST':'WEST', 'WEST':'EAST' }\n    if prev_action is not None:\n        if opposite_action[prev_action] == \"NORTH\":\n            action[:,0] = float(\"-Inf\")\n        if opposite_action[prev_action] == \"SOUTH\":\n            action[:,1] = float(\"-Inf\")\n        if opposite_action[prev_action] == \"WEST\":\n            action[:,2] = float(\"-Inf\")\n        if opposite_action[prev_action] == \"EAST\":\n            action[:,3] = float(\"-Inf\")\n    return action","7e65e968":"def evaluate_control_model(vae, rnn, controller, device):\n    dir2code = {\"NORTH\":torch.Tensor([1,0,0,0]), \"SOUTH\": torch.Tensor([0,1,0,0]), \n                \"WEST\":torch.Tensor([0,0,1,0]), \"EAST\": torch.Tensor([0,0,0,1])}\n    code2dir = [\"NORTH\", \"SOUTH\", \"WEST\", \"EAST\"]\n    env = make(\"hungry_geese\", debug=False) \n    trainer = env.train([None, \"greedy\", \"greedy\", \"greedy\"])\n    total_episodes = 20\n    time_steps = 200\n    s = 0\n    cumulative = 0\n    cumulative_ = 0\n    mdrnn = MDRNNCell(latents, actions, hiddens, gaussians).to(device)\n    \n    while s < total_episodes:\n        obs = trainer.reset()\n        img = custom_env_render(obs, True)\n        action = torch.zeros(1, actions).to(device)\n        reward_ = torch.zeros(1, 1).to(device)\n        hidden = [torch.zeros(1, hiddens).to(device) for _ in range(2)]\n        \n        rnn_state = torch.load('MDN_RNN.pt')\n        prev_action = None\n        for t in range(time_steps): \n            _, mu, log_var = vae(img.unsqueeze(0).to(device))\n            sigma = log_var.exp()\n            eps = torch.randn_like(sigma)\n            z = eps.mul(sigma).add_(mu)\n            mdrnn.load_state_dict({k.strip('_l0'): v for k, v in rnn_state.items()})\n\n            rnn_input = torch.cat((z, action, reward_), -1)\n            out_full, hidden = mdrnn(rnn_input, hidden)\n            \n            c_in = torch.cat((z, hidden[1]),-1)\n            controller.to(device)\n            action = controller(c_in)\n            action = action.detach().to('cpu')\n            action = prevent_opposite_action(action, prev_action)\n            prob = F.softmax(action.squeeze(0)\/0.7, dim=0).numpy()\n            action_dir = np.random.choice(code2dir, p=prob)\n            \n            obs, reward, done, info = trainer.step(action_dir)\n            reward = custom_reward(obs)\n            prev_action = action_dir\n            action = dir2code[action_dir]\n            img = custom_env_render(obs)\n            reward = torch.Tensor([[reward * (1-int(done))]])\n            #reward = torch.where(reward > 0 , 1, 0)\n            action = action.unsqueeze(0).to(device)\n            cumulative += reward\n            if done:\n                obs = trainer.reset()\n                break\n        \n        cumulative_ += cumulative\n        s+=1\n    cumulative_ = cumulative \/ s\n    return float(cumulative_)","d28b4f9b":"def get_mixture_coef(z_pred):\n    log_pi, mu, log_sigma = torch.split(z_pred, 5, 1)\n    log_pi = log_pi - torch.log(torch.sum(torch.exp(log_pi), axis = 1, keepdims = True))\n\n    return log_pi, mu, log_sigma\n\ndef get_pi_idx(x, pdf):\n    # samples from a categorial distribution\n    N = pdf.size()\n    accumulate = 0\n    for i in range(0, N[0]):\n        accumulate += pdf[i]\n        if (accumulate >= x):\n            return i\n    random_value = np.random.randint(N)\n    #print('error with sampling ensemble, returning random', random_value)\n    return random_value\n\ndef sample_z(mu, log_sigma):\n    z =  mu + (torch.exp(log_sigma)) * torch.randn(*log_sigma.shape) \n    return z\n\n\ndef get_z_from_rnn_output(y_pred):\n    HIDDEN_UNITS = 256\n    GAUSSIAN_MIXTURES = 5\n    Z_DIM = 32\n    d = GAUSSIAN_MIXTURES * Z_DIM\n\n    z_pred = y_pred[:,:(3*d)]\n    rew_pred = y_pred[:,-1]\n    z_pred = torch.reshape(z_pred, [-1, GAUSSIAN_MIXTURES * 3])\n\n    log_pi, mu, log_sigma = get_mixture_coef(z_pred)\n\n    chosen_log_pi = torch.zeros(Z_DIM)\n    chosen_mu = torch.zeros(Z_DIM)\n    chosen_log_sigma = torch.zeros(Z_DIM)\n\n    # adjust temperatures\n    logmix2 = log_pi\n    logmix2 -= logmix2.max()\n    logmix2 = torch.exp(logmix2)\n    logmix2 \/= logmix2.sum(axis=1).reshape(Z_DIM, 1)\n\n\n    for j in range(Z_DIM):\n        idx = get_pi_idx(np.random.rand(), logmix2[j])\n        chosen_log_pi[j] = idx\n        chosen_mu[j] = mu[j, idx]\n        chosen_log_sigma[j] = log_sigma[j,idx]\n\n    next_z = sample_z(chosen_mu, chosen_log_sigma)\n    \n    # custom reward output for CMA-es\n    next_reward = F.sigmoid(rew_pred)\n    \"\"\"\n    if rew_pred > 0:\n        next_reward = 1\n    else:\n        next_reward = 0\n    \"\"\"    \n    return next_z, next_reward, chosen_mu","7431700f":"def evaluate_control_model_dream(vae, rnn, controller, device):\n    dir2code = {\"NORTH\":torch.Tensor([1,0,0,0]), \"SOUTH\": torch.Tensor([0,1,0,0]), \n                \"WEST\":torch.Tensor([0,0,1,0]), \"EAST\": torch.Tensor([0,0,0,1])}\n    code2dir = [\"NORTH\", \"SOUTH\", \"WEST\", \"EAST\"]\n    total_episodes = 10\n    time_steps = 200\n    s = 0\n    cumulative = 0\n    cumulative_ = 0\n    mdrnn = MDRNNCell(latents, actions, hiddens, gaussians).to(device)\n    env = make(\"hungry_geese\", debug=False) \n    trainer = env.train([None, \"greedy\", \"greedy\", \"greedy\"])\n    while s < total_episodes:\n        # make initiate observation\n        obs = trainer.reset()\n        img = custom_env_render(obs, True)\n        _, mu, log_var = vae(img.unsqueeze(0).to(device))\n        sigma = log_var.exp()\n        eps = torch.randn_like(sigma)\n        z = eps.mul(sigma).add_(mu)\n        obs = trainer.reset()\n        \n        action = torch.zeros(1, actions).to(device)\n        reward_ = torch.zeros(1, 1).to(device)\n        \n        hidden = [torch.zeros(1, hiddens).to(device) for _ in range(2)]\n        prev_action = None\n        rnn_state = torch.load('MDN_RNN.pt')\n        for t in range(time_steps): \n            mdrnn.load_state_dict({k.strip('_l0'): v for k, v in rnn_state.items()})\n            rnn_input = torch.cat((z, action, reward_), -1)\n            out, hidden = mdrnn(rnn_input, hidden)\n            \n            z, reward, mu = get_z_from_rnn_output(out)\n            z = z.to(device)\n            c_in = torch.cat((z, hidden[1].squeeze(0)),-1)\n            action = controller(c_in)\n            action = action.unsqueeze(0).detach().to('cpu')\n            action = prevent_opposite_action(action, prev_action)\n            prob = F.softmax(action.squeeze(0)\/0.7, dim=0).numpy()\n            action_dir = np.random.choice(code2dir, p=prob)\n            prev_action = action_dir\n            action = dir2code[action_dir]\n            z = z.unsqueeze(0)\n            action = action.unsqueeze(0).to(device)\n            if reward_==0 and t>0:\n                print('break in step:{}'.format(t))\n                break\n            cumulative += reward_\n            reward_ = torch.Tensor([[reward]]).to(device)\n        \n        cumulative_ += cumulative\n        s+=1\n    cumulative_ = cumulative \/ s\n    \n    return float(cumulative_)\n","48216465":"def flatten_parameters(params):\n    return torch.cat([p.detach().view(-1) for p in params], dim=0).to('cpu').numpy()","f742dd3b":"def unflatten_parameters(params, example, device):\n\n    params = torch.Tensor(params).to(device)\n    idx = 0\n    unflattened = []\n    for e_p in example:\n        unflattened += [params[idx:idx + e_p.numel()].view(e_p.size())]\n        idx += e_p.numel()\n    return unflattened\n\ndef load_parameters(params, controller):\n\n    proto = next(controller.parameters())\n    params = unflatten_parameters(\n        params, controller.parameters(), proto.device)\n\n    for p, p_0 in zip(controller.parameters(), params):\n        p.data.copy_(p_0)","c39390aa":"def train_controller(controller, vae, rnn,  mode='real'):\n    parameters = controller.parameters()\n    es = cma.CMAEvolutionStrategy(flatten_parameters(parameters), 0.1,\n                                  {'popsize': 32})\n\n    vae = vae.to(device)\n    rnn = rnn.to(device)\n    start_time = time.time()\n    epoch = 0\n    best = 0.0\n    cur_best = None\n    epochs = 8\n\n    while not es.stop():\n        print('epoch : {}'.format(epoch))\n        solutions = es.ask()\n        reward_list = []\n        for s_idx, s in enumerate(solutions):\n            load_parameters(s, controller)\n            if mode == 'real':\n                reward = evaluate_control_model(vae, rnn, controller, device)\n            elif mode == 'dream':\n                reward = evaluate_control_model_dream(vae, rnn, controller, device)\n\n            reward_list.append(reward)\n        es.tell(solutions, reward_list)\n        es.disp()\n\n        cur_best = max(reward_list)\n        best_index = np.argmax(reward_list)\n        best_params = solutions[best_index]\n        print('current best reward : {}'.format(cur_best))\n        if not best or cur_best >= best:\n            best = cur_best\n            print(\"Saving new best with value {}...\".format(cur_best))\n            load_parameters(best_params, controller)\n            if mode == 'real':\n                torch.save(controller.state_dict(), 'controller.pt')\n            elif mode == 'dream':\n                torch.save(controller.state_dict(), 'controller_dream.pt')\n\n        epoch += 1\n        if epoch > epochs:\n            break\n\n\n    es.result_pretty()","688e1606":"#train controller in real env\ntrain_controller(controller, vae, rnn, 'real')","24dd33a7":"#train controller in dream\ntrain_controller(controller, vae, rnn, 'dream')","95d2a22d":"import shutil\nshutil.copy('main.py', 'main_nodream.py')","eafc2041":"%%writefile -a main.py\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Action\nACTIONS = [a for a in Action]\nimport sys\nimport os\n\nsys.path.append(\"\/kaggle_simulations\/agent\")\n\nprev_action = None\n\naction_ = torch.zeros(1, 4)\nreward_ = torch.zeros(1, 1)\nhidden = [torch.zeros(1, 256) for _ in range(2)]\n\n\n\ndef agent(obs, config):\n    global action_, reward_, hidden, prev_action\n    dir2code = {\"NORTH\":torch.Tensor([1,0,0,0]), \"SOUTH\": torch.Tensor([0,1,0,0]), \n                \"WEST\":torch.Tensor([0,0,1,0]), \"EAST\": torch.Tensor([0,0,0,1])}\n    code2dir = [\"NORTH\", \"SOUTH\", \"WEST\", \"EAST\"]\n    dir2action = {\"NORTH\":ACTIONS[0].name, \"EAST\":ACTIONS[1].name, \"SOUTH\":ACTIONS[2].name, \"WEST\":ACTIONS[3].name}\n\n    device = torch.device('cpu') \n    latents = 32\n    actions = 4\n    hiddens = 256\n    gaussians = 5\n    \n    vae = VAE(latents)\n    controller = Controller(latents, hiddens, actions)\n    mdrnn = MDRNNCell(latents, actions, hiddens, gaussians)\n    if os.path.exists('\/kaggle_simulations\/agent\/vae.pt'):\n        vae.load_state_dict(torch.load('\/kaggle_simulations\/agent\/vae.pt',map_location=torch.device('cpu')))\n        rnn_state = torch.load('\/kaggle_simulations\/agent\/MDN_RNN.pt',map_location=torch.device('cpu'))    \n        #use controller model trained in dream environment\n        controller.load_state_dict(torch.load('\/kaggle_simulations\/agent\/controller_dream.pt',map_location=torch.device('cpu')))\n        \n    else:\n        vae.load_state_dict(torch.load('vae.pt'))\n        rnn_state = torch.load('MDN_RNN.pt')    \n        #use controller model trained in dream environment\n        controller.load_state_dict(torch.load('controller_dream.pt'))    \n    \n    mdrnn.load_state_dict({k.strip('_l0'): v for k, v in rnn_state.items()})\n\n    img = custom_env_render(obs, True)\n    reward_[0][0] = custom_reward(obs)\n    _, mu, log_var = vae(img.unsqueeze(0))\n    sigma = log_var.exp()\n    eps = torch.randn_like(sigma)\n    z = eps.mul(sigma).add_(mu)\n    rnn_input = torch.cat((z, action_, reward_), -1)\n    \n    out_full, hidden = mdrnn(rnn_input, hidden)\n    \n    c_in = torch.cat((z, hidden[1]),-1)\n    action_ = controller(c_in)\n    action_ = prevent_opposite_action(action_.detach(), prev_action)\n    prob = F.softmax(action_.squeeze(0)\/0.7, dim=0).numpy()\n    action_dir = np.random.choice(code2dir, p=prob)\n    action_ = dir2code[action_dir].unsqueeze(0)\n    prev_action = action_dir\n    return dir2action[action_dir]","97007377":"env = make(\"hungry_geese\", debug=True)\n\nenv.reset()\nenv.run([\"main.py\", \"main.py\", \"main.py\", \"main.py\"])\nenv.render(mode=\"ipython\", width=640, height=480)","2d3cb3cb":"%%writefile -a main_nodream.py\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Action\nimport sys\nimport os\n\nsys.path.append(\"\/kaggle_simulations\/agent\")\n\nACTIONS = [a for a in Action]\nprev_action = None\naction_ = torch.zeros(1, 4)\nreward_ = torch.zeros(1, 1)\nhidden = [torch.zeros(1, 256) for _ in range(2)]\ndef agent(obs, config):\n    \n    global action_, reward_, hidden, prev_action\n    dir2code = {\"NORTH\":torch.Tensor([1,0,0,0]), \"SOUTH\": torch.Tensor([0,1,0,0]), \n                \"WEST\":torch.Tensor([0,0,1,0]), \"EAST\": torch.Tensor([0,0,0,1])}\n    code2dir = [\"NORTH\", \"SOUTH\", \"WEST\", \"EAST\"]\n    dir2action = {\"NORTH\":ACTIONS[0].name, \"EAST\":ACTIONS[1].name, \"SOUTH\":ACTIONS[2].name, \"WEST\":ACTIONS[3].name}\n    \n    device = torch.device('cpu') \n    latents = 32\n    actions = 4\n    hiddens = 256\n    gaussians = 5\n    vae = VAE(latents)\n    controller = Controller(latents, hiddens, actions)\n    \n    mdrnn = MDRNNCell(latents, actions, hiddens, gaussians)\n    if os.path.exists('\/kaggle_simulations\/agent\/vae.pt'):\n        vae.load_state_dict(torch.load('\/kaggle_simulations\/agent\/vae.pt',map_location=torch.device('cpu')))\n        rnn_state = torch.load('\/kaggle_simulations\/agent\/MDN_RNN.pt',map_location=torch.device('cpu'))\n        #use controller model trained in dream environment\n        controller.load_state_dict(torch.load('\/kaggle_simulations\/agent\/controller.pt',map_location=torch.device('cpu')))\n    else:\n        vae.load_state_dict(torch.load('vae.pt'))\n        rnn_state = torch.load('MDN_RNN.pt')\n        #use controller model trained in dream environment\n        controller.load_state_dict(torch.load('controller.pt'))\n        \n    mdrnn.load_state_dict({k.strip('_l0'): v for k, v in rnn_state.items()})\n\n    img = custom_env_render(obs, True)\n    reward_[0][0] = custom_reward(obs)\n    _, mu, log_var = vae(img.unsqueeze(0))\n    sigma = log_var.exp()\n    eps = torch.randn_like(sigma)\n    z = eps.mul(sigma).add_(mu)\n    rnn_input = torch.cat((z, action_, reward_), -1)\n    \n    out_full, hidden = mdrnn(rnn_input, hidden)\n    \n    c_in = torch.cat((z, hidden[1]),-1)\n    action_ = controller(c_in)\n    action_ = prevent_opposite_action(action_, prev_action)\n    prob = F.softmax(action_.squeeze(0)\/0.7, dim=0).detach().numpy()\n    action_dir = np.random.choice(code2dir, p=prob)\n    action_ = dir2code[action_dir].unsqueeze(0)\n    prev_action = action_dir\n    return dir2action[action_dir]","3ca6b689":"env = make(\"hungry_geese\", debug=True)\n\nenv.reset()\nenv.run([\"main_nodream.py\", \"main_nodream.py\", \"main_nodream.py\", \"main_nodream.py\"])\nenv.render(mode=\"ipython\", width=640, height=480)","52cbf1b4":"!tar cvfz sub.tar.gz main.py 'vae.pt' 'controller_dream.pt' 'MDN_RNN.pt'","73e4cafb":"from shutil import move\nmove('main_nodream.py', 'main.py')","ff22ed24":"!tar cvfz sub_nodream.tar.gz main.py 'vae.pt' 'controller.pt' 'MDN_RNN.pt'","da6f9e2e":"# Function for Rendering observation Image(3x64x64) of each step","3fe8623d":"# Collecting RNN training data","52e62445":"Try to solve Hungry Geese Problem by Worldmodels model.\n\nhttps:\/\/arxiv.org\/abs\/1803.10122 , Worldmodel paper arxiv, David Ha, J\u00fcrgen Schmidhuber 9 May 2018.\n\n1. Make custom environment renderer to plot observation image of each steps for kaggle_environment\n\n\n2. Make Rollout data for training VAE and MDN-RNN model. \n\n Rollout data :{obs_img, reward, done, action} and z vector made by trained VAE\n \n Rollout data is made by 250 episode of simulation log with defensive_random_action agent \n \n that moves randomly but try not to do actions that directly lead to death\n \n - scaling reward by (reward \/ max_reward)\n \n\n3. Train VAE(Variational AutoEncoder) model by Rollout data. \n\n VAE compresses the high-dimensional input image into a random variable in the latent space that follows a multivariate standard normal distribution.\n \n\n\n4. Train MDN_RNN model by Rollout data with z vector for each steps.\n\n RNN part receives each step's Z vector, action, reward(sequential situation) to predict next step's z vector's distribution\n \n The last hidden state of RNN is used for training controller in real kaggle environment \n \n MDN part receives RNN part's output and generate 5 mixed distribution of log_pi, mu, log_sigma to sample z vector, and reward\n \n The z vector sampled by MDN's output is used to train controller in dream (The real world observation replaced by MDN's z vector and reward)\n \n  \n5. Train Controller model in real world mode and dream mode\n\n Train controller by CMA evolution strategy in real world mode and dream mode.\n \n - The model's performance improvement could not be observed through the training of the evolutionary strategy.\n \n \n6. Agent is composed of VAE, MDN-RNN, and Controller(trained by dream mode simulation) to return Action according to Observation.\n\n - The opposite action was forcibly prevented to keep minimum performance of agent.\n","5e47fa89":"Sample of reconstructed image by VAE","da34cdc5":"# Train Controller","499db719":"references : https:\/\/github.com\/AppliedDataSciencePartners\/WorldModels","df3663bd":"# Make Dataset by Rollout","d14dc2c3":"# Controller evaluation in real env","8b484e84":"# Controller Evaluation in Dream env ","a4991b91":"# Train Contoller with CMA Evolution Strategy","073a67e1":"# Testing Image Rendering function","96b1aa3b":"![cma-es.PNG](attachment:cma-es.PNG)","f47bd196":"# Train MDN_RNN","63099bf2":"![worldmodels.PNG](attachment:worldmodels.PNG)","ff467c10":"# Training VAE","882f3a8c":"Blue Circle : Food\n\nRhombus : Geese's Head\n\nRectangle : Geese's Body\n\nRed Geese : Player Geese\n\nYello, Green, Purple Geese : Rival Geese","c1efe405":"![worldmodels_dream.PNG](attachment:worldmodels_dream.PNG)\n\nTraining controller in environment made by MDN-RNN\n\nobservation image and reward from kaggle environment is replaced by MDN's output (z vector and reward)"}}