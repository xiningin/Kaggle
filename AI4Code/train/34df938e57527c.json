{"cell_type":{"c31da795":"code","076e4126":"code","b2697ed2":"code","daa1fce5":"code","36be3af0":"code","6362536f":"code","2c84491d":"code","71e41c83":"code","ec15420d":"code","2aecafa7":"code","1000c99d":"code","22a0c5d7":"code","5c843b2e":"code","f6d4ca0e":"code","0f115366":"code","05c6b9f9":"code","02fa2250":"code","511df1a8":"code","e6c2b679":"code","8522a9e9":"code","9ec22dc0":"code","d90481eb":"code","f8668feb":"code","9d39b2ae":"code","5fd30638":"code","a0b5f0d5":"code","e62e8cc5":"code","49696cf8":"code","75288053":"code","2e5e0f9a":"code","64cdcffe":"code","9635519b":"code","ebba5734":"code","1cdc50e9":"markdown","7b54da47":"markdown","dd08be04":"markdown","62b21873":"markdown","b4303951":"markdown","845e2c1a":"markdown","2fd3ac03":"markdown","5af1df82":"markdown","e72ef110":"markdown"},"source":{"c31da795":"import os\nimport seaborn as sns\nfrom pandas_profiling import ProfileReport\nimport pydicom\nfrom glob import glob\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport matplotlib.pyplot as plt\nfrom skimage import exposure\nimport cv2\nimport warnings\nwarnings.filterwarnings('ignore')","076e4126":"!pip install pylibjpeg ","b2697ed2":"dataset_dir = '..\/input\/siim-covid19-detection'\ndef dicom2array(path, voi_lut=True, fix_monochrome=True):\n    dicom = pydicom.read_file(path)\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    data = (data * 255).astype(np.uint8)\n    return data\n        \n    \ndef plot_img(img, size=(7, 7), is_rgb=True, title=\"\", cmap='gray'):\n    plt.figure(figsize=size)\n    plt.imshow(img, cmap=cmap)\n    plt.suptitle(title)\n    plt.show()\n\ndef plot_imgs(imgs, cols=5, size=5, is_rgb=True, title=\"\", cmap='gray', img_size=(500,500)):\n    rows = len(imgs)\/\/cols + 1\n    fig = plt.figure(figsize=(cols*size, rows*size))\n    for i, img in enumerate(imgs):\n        if img_size is not None:\n            img = cv2.resize(img, img_size)\n        fig.add_subplot(rows, cols, i+1)\n        plt.imshow(img, cmap=cmap)\n    plt.suptitle(title)\n    plt.show()","daa1fce5":"#imgs = [exposure.equalize_hist(img) for img in imgs]\n#plot_imgs(imgs)","36be3af0":"train = pd.read_csv(f'{dataset_dir}\/train_image_level.csv')\ntrain_study = pd.read_csv(f'{dataset_dir}\/train_study_level.csv')","6362536f":"train.head(5)","2c84491d":"train_study.head(5)","71e41c83":"train_study['Negative for Pneumonia'].sum()+train_study['Typical Appearance'].sum()+train_study['Indeterminate Appearance'].sum()+train_study['Atypical Appearance'].sum()","ec15420d":"len(train_study)","2aecafa7":"train_study['StudyInstanceUID'] = train_study['id'].apply(lambda x: x.replace('_study', ''))\ndel train_study['id']\ntrain = train.merge(train_study, on='StudyInstanceUID')","1000c99d":"train","22a0c5d7":"#train = train[~train.boxes.isnull()] \nclass_names = ['Typical Appearance','Negative for Pneumonia', 'Indeterminate Appearance', 'Atypical Appearance']\nunique_classes = np.unique(train[class_names].values, axis=0)\nimgs = []\nlabel2color = {\n    '[1, 0, 0, 0]': [255,0,0], # Typical Appearance\n    '[0, 1, 0, 0]': [0,255,0], # Indeterminate Appearance\n    '[0, 0, 1, 0]': [0,0,255], # Atypical Appearance\n    '[0, 0, 0, 1]': [255,255,0], # Negative for Pneumonia\n}","5c843b2e":"#!pip install GDCM\n#!pip install pylibjpeg\n#!python -m pip install pydicom\n#!pip install pylibjpeg pylibjpeg-libjpeg pylibjpeg-openjpeg\n#!pip install git+https:\/\/github.com\/pydicom\/pydicom.git","f6d4ca0e":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport pydicom\n\n#path ='..\/input\/siim-covid19-detection\/train\/8f668881cb15\/84baf9c3b2be\/7bc56e205e08.dcm'\n#ds = pydicom.dcmread(path)\n#plt.imshow(ds.pixel_array,cmap=None)\n\n#img = dicom2array(path=path)\n#plt.imshow(img, cmap=cmap)\n#plt.show()","0f115366":"!pip install '..\/input\/k\/tensorchoko\/torch-text\/torch-1.2.0-cp37-cp37m-manylinux1_x86_64.whl'\n!pip install '..\/input\/k\/tensorchoko\/torch-text\/torch_stable.html'\n!pip install '..\/input\/k\/tensorchoko\/torch-text\/torchtext-0.5.0-py3-none-any.whl'\n!pip install '..\/input\/k\/tensorchoko\/torch-text\/torchvision-0.4.0-cp37-cp37m-manylinux1_x86_64.whl'","05c6b9f9":"import warnings\nwarnings.simplefilter('ignore')\nimport os\nimport glob\nimport os.path as osp\nimport random\nimport numpy as np\nimport json\nimport cv2\nimport time\nfrom PIL import Image\nfrom tqdm import tqdm\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom collections import OrderedDict #\nimport pathlib\nimport matplotlib as mpl\nimport numpy as np\nimport urllib.request\n \nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\nimport torchvision\nfrom torchvision import models,transforms\nfrom itertools import product\nfrom math import sqrt\nimport torch\nfrom torch.autograd import Function\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.init as init\nimport datetime\nimport pytz","02fa2250":"import os\nimport sys\n\nsys.path.append(os.path.join('..\/input\/ssdutiilty\/'))","511df1a8":"from data_augumentation import Compose, ConvertFromInts, ToAbsoluteCoords, PhotometricDistort, Expand, RandomSampleCrop, RandomMirror, ToPercentCoords, Resize, SubtractMeans\n\nfrom ssd_model import make_datapath_list, od_collate_fn  # VOCDataset,DataTransform, Anno_xml2list,\nfrom ssd_model import SSD,MultiBoxLoss\nclass DataTransform():\n    \"\"\"\n    \u753b\u50cf\u3068\u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3\u306e\u524d\u51e6\u7406\u30af\u30e9\u30b9\u3002\u8a13\u7df4\u3068\u63a8\u8ad6\u3067\u7570\u306a\u308b\u52d5\u4f5c\u3092\u3059\u308b\u3002\n    \u753b\u50cf\u306e\u30b5\u30a4\u30ba\u3092300x300\u306b\u3059\u308b\u3002\n    \u5b66\u7fd2\u6642\u306f\u30c7\u30fc\u30bf\u30aa\u30fc\u30ae\u30e5\u30e1\u30f3\u30c6\u30fc\u30b7\u30e7\u30f3\u3059\u308b\u3002\n\n\n    Attributes\n    ----------\n    input_size : int\n        \u30ea\u30b5\u30a4\u30ba\u5148\u306e\u753b\u50cf\u306e\u5927\u304d\u3055\u3002\n    color_mean : (B, G, R)\n        \u5404\u8272\u30c1\u30e3\u30cd\u30eb\u306e\u5e73\u5747\u5024\u3002\n    \"\"\"\n\n    def __init__(self, input_size, color_mean):\n        self.data_transform = {\n            'train': Compose([\n                ConvertFromInts(),  # int\u3092float32\u306b\u5909\u63db\n                ToAbsoluteCoords(),  # \u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u306e\u898f\u683c\u5316\u3092\u623b\u3059\n                PhotometricDistort(),  # \u753b\u50cf\u306e\u8272\u8abf\u306a\u3069\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u5909\u5316\n                #Expand(color_mean),  # \u753b\u50cf\u306e\u30ad\u30e3\u30f3\u30d0\u30b9\u3092\u5e83\u3052\u308b\n                #RandomSampleCrop(),  # \u753b\u50cf\u5185\u306e\u90e8\u5206\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u629c\u304d\u51fa\u3059\n                RandomMirror(),  # \u753b\u50cf\u3092\u53cd\u8ee2\u3055\u305b\u308b\n                ToPercentCoords(),  # \u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u30920-1\u306b\u898f\u683c\u5316\n                Resize(input_size),  # \u753b\u50cf\u30b5\u30a4\u30ba\u3092input_size\u00d7input_size\u306b\u5909\u5f62\n                SubtractMeans(color_mean)  # BGR\u306e\u8272\u306e\u5e73\u5747\u5024\u3092\u5f15\u304d\u7b97\n            ]),\n            'val': Compose([\n                ConvertFromInts(),  # int\u3092float\u306b\u5909\u63db\n                Resize(input_size),  # \u753b\u50cf\u30b5\u30a4\u30ba\u3092input_size\u00d7input_size\u306b\u5909\u5f62\n                SubtractMeans(color_mean)  # BGR\u306e\u8272\u306e\u5e73\u5747\u5024\u3092\u5f15\u304d\u7b97\n            ])\n        }\n\n    def __call__(self, img, phase, boxes, labels):\n        \"\"\"\n        Parameters\n        ----------\n        phase : 'train' or 'val'\n            \u524d\u51e6\u7406\u306e\u30e2\u30fc\u30c9\u3092\u6307\u5b9a\u3002\n        \"\"\"\n        return self.data_transform[phase](img, boxes, labels)","e6c2b679":"LABEL_CODE1 = {'[0 0 0 1]':0,'[0 0 1 0]': 1, '[0 1 0 0]': 2, '[1 0 0 0]':3}\nLABEL_CODE2 = {0:'[0 0 0 1]',1:'[0 0 1 0]',2: '[0 1 0 0]',3: '[1 0 0 0]'}\nclass_names = ['Typical Appearance','Negative for Pneumonia', 'Indeterminate Appearance', 'Atypical Appearance']","8522a9e9":"unique_classes = np.unique(train[class_names].values, axis=0)\nunique_classes","9ec22dc0":"train.iloc[0]","d90481eb":"class Anno_xml2list(): \n  \n  def __init__(self,classes): \n    self.clases = classes \n  \n  def __call__(self,index,width,height):\n      bboxes = []\n      row = train.iloc[index]\n      bbox = []\n      for i, l in enumerate(row['label'].split(' ')):\n        if (i % 6 == 0) | (i % 6 == 1):\n            continue\n        if (i % 6 == 2) | (i % 6 == 4):\n            bbox.append(float(l)\/width)\n        if (i % 6 == 3) | (i % 6 == 5):\n            bbox.append(float(l)\/height)\n        if i % 6 == 5:\n            bbox.append(LABEL_CODE1[str(row[class_names].values)])\n            bboxes.append(bbox)\n            bbox = []    \n            \n      return np.array(bboxes) #","f8668feb":"dataset_dir = '..\/input\/siim-covid19-detection'\n#train = train[~train.boxes.isnull()] \nclass_names = ['Typical Appearance','Negative for Pneumonia', 'Indeterminate Appearance', 'Atypical Appearance']\nunique_classes = np.unique(train[class_names].values, axis=0)\nimgs = []\nthickness = 4\nscale = 5\nfor a, row in train.iloc[:3].iterrows():\n    study_id = row['StudyInstanceUID']\n    file = '..\/input\/smmsize300\/train\/train\/{}.jpg'.format(study_id)\n    print(file)\n    #img_path = glob.glob(f'{dataset_dir}\/train\/{study_id}\/*\/*')[0]\n    img = cv2.imread(file)\n    #img = cv2.resize(img, None, fx=1\/scale, fy=1\/scale)\n    #img = np.stack([img, img, img], axis=-1)\n    #img = exposure.equalize_hist(img)\n        \n    claz = row[class_names].values\n    color = label2color[str(claz.tolist())]\n\n    annotation = '..\/input\/smmsize300\/annotation'\n    bboxes = annotation[a]\n\n    plt.imshow(img)\n   \n    plt.show()","9d39b2ae":"i=0\nimage_file_path = train.iloc[i]\n\nstudy_id = train.iloc[i]['StudyInstanceUID']\nfile ='..\/input\/smmsize300\/train\/train\/{}.jpg'.format(study_id)\nimg = cv2.imread(file)\nheight,width,h = img.shape\n\ntransform_anno =Anno_xml2list(LABEL_CODE1)\nanno_list = transform_anno(i,height,width)\nplt.imshow(img, cmap='gray')\nplt.show()\n\ninput_size=300\ncolor_mean=(104,117,123)\n\ntransform = DataTransform(input_size,color_mean)\n                     \nphase = 'train'\nimg_transformed,boxes,labels = transform(img, phase,  anno_list[:,:4] ,anno_list[:,4])\nplt.imshow(img_transformed,cmap='gray')\nplt.show()\nLABEL_CODE1[str(row[class_names].values)]","5fd30638":"class VOCDataset(data.Dataset):\n \n    def __init__(self, img_list, anno_list, phase, transform, transform_anno):\n        self.img_list = img_list\n        self.anno_list = anno_list\n        self.phase = phase  # train \u3082\u3057\u304f\u306f val\u3092\u6307\u5b9a\n        self.transform = transform  # \u753b\u50cf\u306e\u5909\u5f62\n        self.transform_anno = transform_anno  # \u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u3092xml\u304b\u3089\u30ea\u30b9\u30c8\u3078\n \n    def __len__(self):\n        '''\u753b\u50cf\u306e\u679a\u6570\u3092\u8fd4\u3059'''\n        return len(self.img_list)\n \n    def __getitem__(self, index):\n        '''\n        \u524d\u51e6\u7406\u3092\u3057\u305f\u753b\u50cf\u306e\u30c6\u30f3\u30bd\u30eb\u5f62\u5f0f\u306e\u30c7\u30fc\u30bf\u3068\u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3\u3092\u53d6\u5f97\n        '''\n        im, gt, h, w = self.pull_item(index)\n        return im, gt,h,w\n \n    def pull_item(self, index):\n        '''\u524d\u51e6\u7406\u3092\u3057\u305f\u753b\u50cf\u306e\u30c6\u30f3\u30bd\u30eb\u5f62\u5f0f\u306e\u30c7\u30fc\u30bf\u3001\u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3\u3001\u753b\u50cf\u306e\u9ad8\u3055\u3001\u5e45\u3092\u53d6\u5f97\u3059\u308b'''\n \n        # 1. \u753b\u50cf\u8aad\u307f\u8fbc\u307f\n        #\u4eee\u5bfe\u5fdc   \n        #if index==3215:\n        #    index =1\n            \n        study_id = train.iloc[index]['StudyInstanceUID']\n        #img_path = glob.glob(f'{dataset_dir}\/train\/{study_id}\/*\/*')[0]\n               \n       \n        flag=0\n        img_path = '..\/input\/smmsize300\/train\/train\/{}.jpg'.format(study_id)   \n        img = cv2.imread(img_path)\n        \n        #print(len(img.shape))\n        if len(img.shape) ==2:\n          height,width, = img.shape\n        if len(img.shape) ==3:\n          height,width,channel = img.shape\n        \n        #img = np.stack([img, img, img], axis=-1) =>#shape\u304c4\u3068\u306a\u308a\u52d5\u304b\u306a\u3044\n        #img = exposure.equalize_hist(img)\n        \n        #2. xml\u5f62\u5f0f\u306e\u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3\u60c5\u5831\u3092\u30ea\u30b9\u30c8\u306b\n        anno_list = transform_anno(index,width,height)\n        \n        # 3. \u524d\u51e6\u7406\u3092\u5b9f\u65bd\n        img, boxes, labels = transform(\n            img, self.phase, anno_list[:, :4], anno_list[:, 4])\n \n        # \u8272\u30c1\u30e3\u30cd\u30eb\u306e\u9806\u756a\u304cBGR\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u3067\u3001RGB\u306b\u9806\u756a\u5909\u66f4\n        # \u3055\u3089\u306b\uff08\u9ad8\u3055\u3001\u5e45\u3001\u8272\u30c1\u30e3\u30cd\u30eb\uff09\u306e\u9806\u3092\uff08\u8272\u30c1\u30e3\u30cd\u30eb\u3001\u9ad8\u3055\u3001\u5e45\uff09\u306b\u5909\u63db\n        img = torch.from_numpy(img[:, :, (2, 1, 0)]).permute(2, 0, 1)\n \n        # BBox\u3068\u30e9\u30d9\u30eb\u3092\u30bb\u30c3\u30c8\u306b\u3057\u305fnp.array\u3092\u4f5c\u6210\u3001\u5909\u6570\u540d\u300cgt\u300d\u306fground truth\uff08\u7b54\u3048\uff09\u306e\u7565\u79f0\n        #print(labels)\n        gt = np.hstack((boxes, np.expand_dims(labels, axis=1)))\n \n        return img, gt, height, width #img\u304ctensor,\u305d\u308c\u4ee5\u5916\u304carray\u3068\u306a\n\n","a0b5f0d5":"# \u52d5\u4f5c\u78ba\u8a8d\n\ninput_size = 300  # \u753b\u50cf\u306einput\u30b5\u30a4\u30ba\u3092300\u00d7300\u306b\u3059\u308b\n\ntrain_dataset = VOCDataset(train.iloc[:6000]['StudyInstanceUID'], anno_list[:6000], phase=\"train\", transform=DataTransform(\n    input_size, color_mean), transform_anno=Anno_xml2list(LABEL_CODE1))\n\nval_dataset = VOCDataset(train.iloc[6000:]['StudyInstanceUID'], anno_list[6000:], phase=\"val\", transform=DataTransform(\n    input_size, color_mean), transform_anno=Anno_xml2list(LABEL_CODE1))\n\n\nbatch_size = 32 #4\n\ntrain_dataloader = data.DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=od_collate_fn)\n\nval_dataloader = data.DataLoader(\n    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=od_collate_fn)\n\n# \u8f9e\u66f8\u578b\u5909\u6570\u306b\u307e\u3068\u3081\u308b\ndataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n\n# \u30c7\u30fc\u30bf\u306e\u53d6\u308a\u51fa\u3057\u4f8b\ntrain_dataset.__getitem__(0)\n","e62e8cc5":"# \u52d5\u4f5c\u306e\u78ba\u8a8d\nbatch_iterator = iter(dataloaders_dict[\"train\"])  # \u30a4\u30bf\u30ec\u30fc\u30bf\u306b\u5909\u63db\nimages, targets = next(batch_iterator)  # 1\u756a\u76ee\u306e\u8981\u7d20\u3092\u53d6\u308a\u51fa\u3059\nprint(images.size())  # torch.Size([4, 3, 300, 300])\nprint(len(targets))\nprint(targets[0].size())  # \u30df\u30cb\u30d0\u30c3\u30c1\u306e\u30b5\u30a4\u30ba\u306e\u30ea\u30b9\u30c8\u3001\u5404\u8981\u7d20\u306f[n, 5]\u3001n\u306f\u7269\u4f53\u6570","49696cf8":"# SSD300\u306e\u8a2d\u5b9a\nssd_cfg = {\n    'num_classes': 5,  # \u80cc\u666f\u30af\u30e9\u30b9\u3092\u542b\u3081\u305f\u5408\u8a08\u30af\u30e9\u30b9\u6570\n    'input_size': 300,  # \u753b\u50cf\u306e\u5165\u529b\u30b5\u30a4\u30ba\n    'bbox_aspect_num': [4, 6, 6, 6, 4, 4],  # \u51fa\u529b\u3059\u308bDBox\u306e\u30a2\u30b9\u30da\u30af\u30c8\u6bd4\u306e\u7a2e\u985e\n    'feature_maps': [38, 19, 10, 5, 3, 1],  # \u5404source\u306e\u753b\u50cf\u30b5\u30a4\u30ba\n    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n    'steps': [8, 16, 32, 64, 100, 300],  # DBOX\u306e\u5927\u304d\u3055\u3092\u6c7a\u3081\u308b\n    'max_sizes': [60, 111, 162, 213, 264, 315],  # DBOX\u306e\u5927\u304d\u3055\u3092\u6c7a\u3081\u308b\n    'min_sizes': [30, 60, 111, 162, 213, 264],  # DBOX\u306e\u5927\u304d\u3055\u3092\u6c7a\u3081\u308b\n    \n    #'steps': [8, 16, 32, 40, 64, 80],  # DBOX\u306e\u5927\u304d\u3055\u3092\u6c7a\u3081\u308b\n    #'min_sizes': [4,8,16,32,40,64],  # DBOX\u306e\u5927\u304d\u3055\u3092\u6c7a\u3081\u308b\n    #'max_sizes': [8,16,32,40,64,80],  # DBOX\u306e\u5927\u304d\u3055\u3092\u6c7a\u3081\u308b\n}\n \n# SSD\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30e2\u30c7\u30eb\nnet = SSD(phase=\"train\", cfg=ssd_cfg)\n \n \nweights_dir = \".\/weights\/\"\nif not os.path.exists(weights_dir):\n    os.mkdir(weights_dir)\n \ndef weights_init(m):\n    if isinstance(m, nn.Conv2d):\n        init.kaiming_normal_(m.weight.data)\n        if m.bias is not None:  # \u30d0\u30a4\u30a2\u30b9\u9805\u304c\u3042\u308b\u5834\u5408\n            nn.init.constant_(m.bias, 0.0)\n \n##################### \u65b0\u3057\u3044\u5b66\u7fd2\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3059\u308b\nurl = \"https:\/\/s3.amazonaws.com\/amdegroot-models\/vgg16_reducedfc.pth\"\ntarget_path = os.path.join(weights_dir, \"vgg16_reducedfc.pth\") \nif not os.path.exists(target_path):\n    urllib.request.urlretrieve(url, target_path)\n    \n# SSD\u306e\u521d\u671f\u306e\u91cd\u307f\u3092\u8a2d\u5b9a\n# ssd\u306evgg\u90e8\u5206\u306b\u91cd\u307f\u3092\u30ed\u30fc\u30c9\u3059\u308b\nvgg_weights = torch.load('..\/input\/k\/tensorchoko\/torch-text\/vgg16_reducedfc.pth')\nnet.vgg.load_state_dict(vgg_weights)\n#He\u306e\u521d\u671f\u5024\u3092\u9069\u7528\nnet.extras.apply(weights_init)\nnet.loc.apply(weights_init)\nnet.conf.apply(weights_init)\n \n######################  \u5b66\u7fd2\u6e08\u30e2\u30c7\u30eb\u3092\u4f7f\u3046\n#model_path =(''.\/ssdmodel_save.pth'')\n#net.load_state_dict(torch.load(model_path))\n###################################################################\n# GPU\u304c\u4f7f\u3048\u308b\u304b\u3092\u78ba\u8a8d\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"\u4f7f\u7528\u30c7\u30d0\u30a4\u30b9\uff1a\", device)\n \nprint('\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u8a2d\u5b9a\u5b8c\u4e86\uff1a\u5b66\u7fd2\u6e08\u307f\u306e\u91cd\u307f\u3092\u30ed\u30fc\u30c9\u3057\u307e\u3057\u305f')","75288053":"# \u4e71\u6570\u306e\u30b7\u30fc\u30c9\u3092\u8a2d\u5b9a\ntorch.manual_seed(1234)\nnp.random.seed(1234)\nrandom.seed(1234)\n \ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"\u4f7f\u7528\u30c7\u30d0\u30a4\u30b9\uff1a\", device)","2e5e0f9a":"# GPU\u304c\u4f7f\u3048\u308b\u304b\u3092\u78ba\u8a8d\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"\u4f7f\u7528\u30c7\u30d0\u30a4\u30b9\uff1a\", device)\n\n# \u640d\u5931\u95a2\u6570\u306e\u8a2d\u5b9a\ncriterion = MultiBoxLoss(jaccard_thresh=0.5, neg_pos=3, device=device)\n\n# \u6700\u9069\u5316\u624b\u6cd5\u306e\u8a2d\u5b9a\n#optimizer = optim.Adam(net.parameters(), lr=1e-4, \n#                      betas=(0.9, 0.999), weight_decay=5e-4, amsgrad=False)\noptimizer = optim.SGD(net.parameters(), lr=1e-4, # lr=1e-3,\n                     momentum=0.9, weight_decay=5e-4)  ","64cdcffe":"def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n\n    # GPU\u304c\u4f7f\u3048\u308b\u304b\u3092\u78ba\u8a8d\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    print(\"\u4f7f\u7528\u30c7\u30d0\u30a4\u30b9\uff1a\", device)\n\n    # \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092GPU\u3078\n    net.to(device)\n\n    # \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u3042\u308b\u7a0b\u5ea6\u56fa\u5b9a\u3067\u3042\u308c\u3070\u3001\u9ad8\u901f\u5316\u3055\u305b\u308b\n    torch.backends.cudnn.benchmark = True\n\n    # \u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u30ab\u30a6\u30f3\u30bf\u3092\u30bb\u30c3\u30c8\n    iteration = 1\n    epoch_train_loss = 0.0  # epoch\u306e\u640d\u5931\u548c\n    epoch_val_loss = 0.0  # epoch\u306e\u640d\u5931\u548c\n    logs = []\n\n    # epoch\u306e\u30eb\u30fc\u30d7\n    for epoch in range(num_epochs+1):\n\n        # \u958b\u59cb\u6642\u523b\u3092\u4fdd\u5b58\n        t_epoch_start = time.time()\n        t_iter_start = time.time()\n\n        print('-------------')\n        print('Epoch {}\/{}'.format(epoch+1, num_epochs))\n        print('-------------')\n\n        # epoch\u3054\u3068\u306e\u8a13\u7df4\u3068\u691c\u8a3c\u306e\u30eb\u30fc\u30d7\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                net.train()  # \u30e2\u30c7\u30eb\u3092\u8a13\u7df4\u30e2\u30fc\u30c9\u306b\n                print('\uff08train\uff09')\n            else:\n                if((epoch+1) % 10 == 0):\n                    net.eval()   # \u30e2\u30c7\u30eb\u3092\u691c\u8a3c\u30e2\u30fc\u30c9\u306b\n                    print('-------------')\n                    print('\uff08val\uff09')\n                else:\n                    # \u691c\u8a3c\u306f10\u56de\u306b1\u56de\u3060\u3051\u884c\u3046\n                    continue\n\n            # \u30c7\u30fc\u30bf\u30ed\u30fc\u30c0\u30fc\u304b\u3089minibatch\u305a\u3064\u53d6\u308a\u51fa\u3059\u30eb\u30fc\u30d7\n            for images, targets in dataloaders_dict[phase]:\n\n                # GPU\u304c\u4f7f\u3048\u308b\u306a\u3089GPU\u306b\u30c7\u30fc\u30bf\u3092\u9001\u308b\n                images = images.to(device)\n                targets = [ann.to(device)\n                           for ann in targets]  # \u30ea\u30b9\u30c8\u306e\u5404\u8981\u7d20\u306e\u30c6\u30f3\u30bd\u30eb\u3092GPU\u3078\n\n                # optimizer\u3092\u521d\u671f\u5316\n                optimizer.zero_grad()\n\n                # \u9806\u4f1d\u642c\uff08forward\uff09\u8a08\u7b97\n                with torch.set_grad_enabled(phase == 'train'):\n                    # \u9806\u4f1d\u642c\uff08forward\uff09\u8a08\u7b97\n                    outputs = net(images)\n\n                    # \u640d\u5931\u306e\u8a08\u7b97\n                    loss_l, loss_c = criterion(outputs, targets)\n                    loss = loss_l + loss_c\n                    #print('loss=',loss,' targets=',targets)\n\n                    # \u8a13\u7df4\u6642\u306f\u30d0\u30c3\u30af\u30d7\u30ed\u30d1\u30b2\u30fc\u30b7\u30e7\u30f3\n                    if phase == 'train':\n                        loss.backward()  # \u52fe\u914d\u306e\u8a08\u7b97\n\n                        # \u52fe\u914d\u304c\u5927\u304d\u304f\u306a\u308a\u3059\u304e\u308b\u3068\u8a08\u7b97\u304c\u4e0d\u5b89\u5b9a\u306b\u306a\u308b\u306e\u3067\u3001clip\u3067\u6700\u5927\u3067\u3082\u52fe\u914d2.0\u306b\u7559\u3081\u308b\n                        nn.utils.clip_grad_value_(\n                            net.parameters(), clip_value=2.0)\n\n                        optimizer.step()  # \u30d1\u30e9\u30e1\u30fc\u30bf\u66f4\u65b0\n\n                        if (iteration % 20 == 0):  # 20iter\u306b1\u5ea6\u3001loss\u3092\u8868\u793a\n                            t_iter_finish = time.time()\n                            dt_now =  datetime.datetime.now(pytz.timezone('Asia\/Tokyo'))\n                            duration = t_iter_finish - t_iter_start\n                            print('iteration  {} || Loss: {:.4f} || 10iter: {:.4f} sec.'.format(\n                                iteration, loss.item(), duration),\" \",dt_now)\n                            t_iter_start = time.time()\n                            #print('loss=',loss,' targets=',targets,' outputs=',outputs) #######\n\n                        epoch_train_loss += loss.item()\n                        iteration += 1\n\n                    # \u691c\u8a3c\u6642\n                    else:\n                        epoch_val_loss += loss.item()\n\n        # epoch\u306ephase\u3054\u3068\u306eloss\u3068\u6b63\u89e3\u7387\n        t_epoch_finish = time.time()\n        print('-------------')\n        print('epoch {} || Epoch_TRAIN_Loss:{:.4f} ||Epoch_VAL_Loss:{:.4f}'.format(\n            epoch+1, epoch_train_loss, epoch_val_loss))\n        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n        t_epoch_start = time.time()\n\n        epoch_val_loss = 0.0  # epoch\u306e\u640d\u5931\u548c\n\n        # \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u4fdd\u5b58\u3059\u308b\n        if ((epoch+1) % 10 == 0):\n           torch.save(net.state_dict(), '.\/ssdmodel_save.pth')","9635519b":"# \u5b66\u7fd2\u30fb\u691c\u8a3c\u3092\u5b9f\u884c\u3059\u308b\nimport datetime\nnum_epochs= 5\ntrain_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs) ","ebba5734":"torch.save(net.state_dict(), '.\/ssdmodel_save.pth')","1cdc50e9":" \u3053\u306eexposure.equalize_hist\u3092\u65bd\u3059\u3068\u30af\u30ea\u30a2\u306b\u8868\u793a\u3055\u308c\u308b","7b54da47":"1. Typical Appearance: Multifocal bilateral, peripheral opacities with rounded morphology, lower lung\u2013predominant distribution\n\n2. Indeterminate Appearance: Absence of typical findings AND unilateral, central or upper lung predominant distribution\n\n3. Atypical Appearance: Pneumothorax, pleural effusion, pulmonary edema, lobar consolidation, solitary lung nodule or mass, diffuse tiny nodules, cavity\n\n4. Negative for Pneumonia: No lung opacities","dd08be04":"![image.png](attachment:b45cc6e9-1faa-4a78-b2ce-a9e6fd5aaa09.png)","62b21873":"\u7686\u3055\u3093\u30c8\u30ec\u30a4\u30f3\u30c7\u30fc\u30bf\u306e\u5199\u771f\u307f\u307e\u3057\u305f\uff1f\u3069\u3046\u3084\u3063\u3066\u307f\u3066\u5224\u5b9a\u3059\u308c\u3070\u3044\u3044\u304b\u3001\u3044\u308d\u3044\u308d\u8abf\u3079\u307e\u3057\u305f<br>\n\u30ab\u30b0\u30eb\u30de\u30b9\u30bf\u30fc\u306b\u805e\u3044\u305f\u3089\u3001\u533b\u8005\u3058\u3083\u306a\u304d\u3083\u7121\u7406\u3068\u306e\u3053\u3068\u3002\u30e2\u30c7\u30eb\u4f5c\u308c\u3068\u306e\u3053\u3068\u3002<br>\n\nSSD\u306a\u3089\u306a\u3093\u304b\u5224\u5b9a\u3067\u304d\u3061\u3083\u3046\u3093\u3058\u3083\u306a\u3044\u304b\u3068\u601d\u3063\u3066\u3084\u3063\u3066\u307f\u307e\u3057\u305f\u304c\u3001\u307e\u3063\u305f\u304f\u5b66\u7fd2\u3067\u304d\u307e\u305b\u3093\u3002<br>\n\u4eba\u9593\u898b\u3066\u308f\u304b\u3089\u306a\u3044\u304b\u3089\u5f53\u305f\u308a\u524d\u3067\u3059\u304b\u306d\u3002<br>\n\u3055\u3066\u3001\u3069\u3046\u3059\u308c\u3070\u3044\u3044\u3093\u3067\u3057\u3087\u3046\u3002","b4303951":"![image.png](attachment:45f044ab-9a14-48db-baee-b215908a2f15.png)","845e2c1a":"\u5b66\u7fd2\u3059\u308b\u305f\u3073\u306bloss\u304c\u5897\u3048\u308b\u3001\u5b66\u3079\u306a\u3044\u30e2\u30c7\u30eb\u304c\u3067\u304d\u3066\u3057\u307e\u3044\u307e\u3057\u305f\u3002\u3069\u3046\u3057\u305f\u3089\u3044\u3044\u3093\u3067\u3057\u3087\u3046\uff1f","2fd3ac03":"![image.png](attachment:3caadce3-9c9c-4e0d-8a61-c6bbaf8ddb38.png)","5af1df82":"![image.png](attachment:a14ab8b2-a863-46a1-8e30-af887c581050.png)","e72ef110":"6054\u4ef6\u3002\u3068\u3044\u3046\u3053\u3068\u306f\u30c8\u30ec\u30a4\u30f3\u30c7\u30fc\u30bf\u304b\u3089\u306f\u3001\u5404\u9805\u76ee\u306e\u3046\u3061\u3069\u308c\u304b\u304c\uff11\u3064\u3060\u3051\u30d5\u30e9\u30b0\u304c\u305f\u3063\u3066\u3044\u308b\u3053\u3068\u306b\u306a\u308b\u3002\uff12\u3064\u306e\u9805\u76ee\u306b\u30d5\u30e9\u30b0\u304c\u7acb\u3064\u3053\u3068\u306f\u306a\u3044\u306f\u305a\u3002"}}