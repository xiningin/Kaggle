{"cell_type":{"02e872d8":"code","e95d9f8c":"code","03fe69b9":"code","156444b6":"code","afb8b03a":"code","421c6f3f":"code","74d437e3":"code","2b5373e0":"code","0f06c94b":"code","927b1281":"code","198ad21c":"code","6b0ce266":"code","2f983df6":"code","00c4834d":"code","6b9efa1f":"code","cba0548c":"code","bb128cfb":"code","09168a43":"code","2fcc4009":"code","7e2aba27":"code","965952a1":"code","d0622fea":"code","52a56e04":"code","48291f91":"code","7d296744":"code","2baa00ac":"code","685dc084":"code","08d654b7":"code","af49a627":"code","40f32df0":"code","9e523615":"code","22d98672":"code","9d449068":"code","fa8a545f":"code","76d1da55":"code","04994703":"code","fef7e9e8":"code","1bae54f3":"code","5fd4d9f2":"code","c091a46e":"code","90961de7":"code","6fe0bc79":"code","563c23bd":"code","24306e00":"code","3c86af02":"code","d4e59e4b":"code","306f98d7":"code","8efc7b99":"code","83762da2":"code","f59edb21":"code","8ceb4315":"code","7a22159b":"markdown","23995cf6":"markdown","42e62bc2":"markdown","a7537cd1":"markdown","2c4dc4cf":"markdown","f6c569dd":"markdown","a8a012e5":"markdown","203af5e3":"markdown","0cab7707":"markdown","cd401fde":"markdown","804b8c23":"markdown","819497f4":"markdown","e483437e":"markdown","184d0b2e":"markdown","8b0ff709":"markdown","bc58b144":"markdown","a5d33e98":"markdown","ec2f0362":"markdown","12aeeeaf":"markdown","e3105cc7":"markdown","aa6f31dc":"markdown","802fe4a9":"markdown","3c050097":"markdown","abd056a6":"markdown","925f0c65":"markdown","eabbd1e8":"markdown","2d7c46e7":"markdown","c618c03d":"markdown","beb22d9e":"markdown","69be170c":"markdown","6e5bcb53":"markdown","3446355f":"markdown"},"source":{"02e872d8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e95d9f8c":"data = pd.read_csv('\/kaggle\/input\/indian-liver-patient-dataset\/Indian Liver Patient Dataset (ILPD).csv')","03fe69b9":"#Looking at Data in order to have a look and feel\ndata.head()","156444b6":"#Looking at the tail of it \ndata.tail()","afb8b03a":"#Looking at the information contained within the data\ndata.info()","421c6f3f":"data.isnull().sum()","74d437e3":"from sklearn.impute import SimpleImputer\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')","2b5373e0":"#Imputing values\ndata['alkphos']=imp.fit_transform(data[['alkphos']])","0f06c94b":"data.isnull().sum()","927b1281":"data.describe()","198ad21c":"#Replacing Male and Female with 1 and 0\ndata['gender'] = data['gender'].apply(lambda x:1 if x == 'Male' else 0)","6b0ce266":"#Now looking at the info\ndata.info()","2f983df6":"data.isnull().sum()","00c4834d":"data.nunique()","6b9efa1f":"#Now it's time to see the correlation\ndata.corr().style.background_gradient(cmap='coolwarm')","cba0548c":"import seaborn as sns\n#Implementing Pairplot to see the various mixtures\nsns.set()\nsns.pairplot(data, hue= 'is_patient', kind = 'reg')\n\n#sns.pairplot(df_liver, hue='Dataset', kind='reg')","bb128cfb":"#Importing variance inflation factor from statsmodels\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as vif","09168a43":"data.head()","2fcc4009":"#Making New Data Sets for Computation\n#Select all rows except for the Target variable\nTest_Set_Var = data.iloc[:,:10]","7e2aba27":"df = pd.DataFrame()\ndf[\"vif_index\"] = [vif(Test_Set_Var.values, i) for i in range(Test_Set_Var.shape[1])]\ndf[\"features\"] = Test_Set_Var.columns\ndf.sort_values(by = 'vif_index', ascending=False)","965952a1":"#Over here we can see that sgot, sgpt, and alkphos have the highest amount for the VIF Index.\n#Normally, a value above 10 would mean that we should slash it out however,\n#This is a medical data set, we should still consider these variables as it seems to have some\n#form of correlation according to my minimal industry knowledge. Any input would be appreciated! ","d0622fea":"gender_data = data[['gender', 'is_patient']].groupby('gender', as_index = False).agg(np.sum)\ngender_data","52a56e04":"#Importing MatplotLib\nimport matplotlib.pyplot as plt","48291f91":"plt.figure(figsize= (12,8))\nplt.title('Men and Womnens Ratio with respect to Having Disease or Not')\nsns.barplot(x = 'gender', y = 'is_patient', data = data)","7d296744":"import matplotlib.pyplot as plt\nplt.figure(figsize= (12,8))\nplt.title('Men and Womnens Numbers with respect to Having Disease or Not')\nsns.barplot(x = 'gender', y = 'is_patient', data = gender_data)","2baa00ac":"df_liver_TP = data[['gender', 'tot_proteins', 'is_patient']].groupby(['gender'], as_index=False).agg(np.sum)\ndf_liver_TP","685dc084":"#Plotting it on a graph for better visualization\nplt.figure(figsize = (12,8))\nsns.barplot(x = 'gender', y = 'tot_proteins', hue= 'is_patient', data = df_liver_TP)","08d654b7":"#Before moving forward we can try and see the different stats of the whole data set\n#This is without the NaN's meaning we haven't includes the empty variables\ndata.describe()","af49a627":"data.shape","40f32df0":"#Importing the Necessary Modules\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_curve, roc_auc_score","9e523615":"#Defining X and y as we never defined it\nX = data.iloc[:, :10]\ny = data['is_patient']","22d98672":"#Using Scaling Methods to scale the entire data set as I'm running into errors\nscaler=MinMaxScaler()\nscaled_values=scaler.fit_transform(X)\nX.loc[:,:]=scaled_values","9d449068":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size =0.3, random_state = 123)","fa8a545f":"#Checking to see isnull again, I'm good so far :) \nprint(X_train.isnull().sum())\nprint(y_train.isnull().sum())\nprint(X_test.isnull().sum())\nprint(y_test.isnull().sum())","76d1da55":"print(f'Shape of X:{X_train.shape}, Shape of y: {y_train.shape}')","04994703":"print(f'Shape of X:{X_test.shape}, Shape of y: {y_test.shape}')","fef7e9e8":"#Instantiating 3 Classification Models\ndt = DecisionTreeClassifier(max_depth=4,min_samples_leaf=0.14,random_state=1)\nlr = LogisticRegression(solver='lbfgs', multi_class='ovr')\nKNN_1 = KNN(n_neighbors = 10)\nrf = RandomForestClassifier(n_estimators=25, random_state=2)","1bae54f3":"classifiers = [('Decision Tree',dt),('Logistic Regression',lr),('KNN',KNN_1), ('Random Forest', rf)]","5fd4d9f2":"dt.fit(X_train, y_train)\ndt_y_pred = dt.predict(X_test)\ndt_acc_test = accuracy_score(y_test, dt_y_pred)\nprint('Test set accuracy of dt: {:.2f}'.format(dt_acc_test)) ","c091a46e":"#Logistic Regression Accuracy\nlr.fit(X_train, y_train)\nlr_y_pred = lr.predict(X_test)\nlr_acc_test = accuracy_score(y_test, lr_y_pred)\nprint('Test set accuracy of dt: {:.2f}'.format(lr_acc_test)) ","90961de7":"#KNN Accuracy\nKNN_1.fit(X_train, y_train)\nknn_y_pred = KNN_1.predict(X_test)\nknn_acc_test = accuracy_score(y_test, knn_y_pred)\nprint('Test set accuracy of dt: {:.2f}'.format(knn_acc_test)) ","6fe0bc79":"#Random Forest Accuracy\nrf.fit(X_train, y_train)\nrf_y_pred = rf.predict(X_test)\nrf_acc_test = accuracy_score(y_test, rf_y_pred)\nprint('Test set accuracy of dt: {:.2f}'.format(rf_acc_test)) ","563c23bd":"#Using RF to see Best Features\n# Create a pd.Series of features importances\nimportances = pd.Series(data=rf.feature_importances_,\n                        index= X_train.columns)\n\n# Sort importances\nimportances_sorted = importances.sort_values()\n\n# Draw a horizontal barplot of importances_sorted\nimportances_sorted.plot(kind='barh', color='lightgreen')\nplt.title('Features Importances')\nplt.show()","24306e00":"#Decided to run them all together and print them out in a for loop for Automation\nfor clf_name, clf in classifiers:\n    clf.fit(X_train,y_train)\n    y_pred = clf.predict(X_test)\n    print('{:s} : {:.3f}'.format(clf_name, accuracy_score(y_test, y_pred)))","3c86af02":"#Predict Proba Scores\ny_score1 = dt.predict_proba(X_test)[:,1]\ny_score2 = lr.predict_proba(X_test)[:,1]\ny_score3 = KNN_1.predict_proba(X_test)[:,1]\ny_score4 = rf.predict_proba(X_test)[:,1]","d4e59e4b":"print('roc_auc_score for DecisionTree: ', roc_auc_score(y_test, y_score1))\nprint('roc_auc_score for Logistic Regression: ', roc_auc_score(y_test, y_score2))\nprint('roc_auc_score for KNearest Neighbors: ', roc_auc_score(y_test, y_score3))\nprint('roc_auc_score for Random Forest: ', roc_auc_score(y_test, y_score4))","306f98d7":"#define metrics\ny_pred_proba_rf_auc = rf.predict_proba(X_test)[::,1]\nfpr_rf, tpr_rf, _ = roc_curve(y_test,  y_pred_proba_rf_auc, pos_label = 2)\nrf_auc = roc_auc_score(y_test, y_score4)\n\n\n#create ROC curve\nplt.plot(fpr_rf,tpr_rf,label=\"AUC=\"+str(rf_auc))\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.legend(loc=4)\nplt.show()","8efc7b99":"#define metrics\ny_pred_proba_auc_lr = lr.predict_proba(X_test)[::,1]\nfpr_lr, tpr_lr, _ = roc_curve(y_test,  y_pred_proba_auc_lr, pos_label = 2)\nlr_auc = roc_auc_score(y_test, y_score2)\n\n\n#create ROC curve\nplt.plot(fpr_lr,tpr_lr,label=\"AUC=\"+str(lr_auc))\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.legend(loc =4)\nplt.show()","83762da2":"#define metrics\ny_pred_proba_auc_knn = KNN_1.predict_proba(X_test)[::,1]\nfpr_knn, tpr_knn, _ = roc_curve(y_test,  y_pred_proba_auc_knn, pos_label = 2)\nknn_auc = roc_auc_score(y_test, y_score3)\n\n#create ROC curve\nplt.plot(fpr_knn,tpr_knn,label=\"AUC=\"+str(knn_auc))\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.legend(loc =4)\nplt.show()","f59edb21":"#define metrics\ny_pred_proba_auc_dt = dt.predict_proba(X_test)[::,1]\nfpr_dt, tpr_dt, _ = roc_curve(y_test,  y_pred_proba_auc_dt, pos_label = 2)\ndt_auc = roc_auc_score(y_test, y_score1)\n\n\n#create ROC curve\nplt.plot(fpr_dt,tpr_dt,label=\"AUC=\"+str(dt_auc))\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.legend(loc =4)\nplt.show()","8ceb4315":"results = pd.DataFrame({'Model': ['Decision Tree','Logistic Regression', 'KNN','Random Forest Classifier'],\n                        'Roc_Score': [roc_auc_score(y_test, y_score1), roc_auc_score(y_test, y_score2),roc_auc_score(y_test, y_score3), roc_auc_score(y_test, y_score4)],\n                       'Accuracy_Score': [dt_acc_test, lr_acc_test,knn_acc_test, rf_acc_test]})\ndf_results = results.sort_values(by='Roc_Score', ascending=False)\ndf_results","7a22159b":"# Any input would be highly appreciated :D","23995cf6":"## We can see that Men have a higher chance of having Liver Disease, but how\/why ? ","42e62bc2":"# Plotting the Decision Tree's ROC Curve","a7537cd1":"# Making the Test Set, excluding the Target, i.e \"y\" variable in order to meet up with Variance Inflations (vif's) syntax","2c4dc4cf":"# Plotting the Random Forests ROC Curve","f6c569dd":"Checking to see Shape Again","a8a012e5":"# Receiver Operating Characteristic (ROC) and Area under the curve (AUC) Score","203af5e3":"# Results in a Data Frame","0cab7707":"# Plotting the Logistic Regressions ROC Curve","cd401fde":"# Using Hold Out Method.\n## I was a bit skeptical to use the Cross-Validation method but I would maybe give it a try","804b8c23":"# Seaborns PairPlot","819497f4":"# Seeing Variations in outputs with respect to Gender","e483437e":"##### Trying to see a relationship between Proteins and Gender. I'm not a Bio-med Major,\n##### We can see a relationship between total_proteins, gender and their final outcome","184d0b2e":"# Instantiating Classifiers","8b0ff709":"# Random Forest's Feature Importance","bc58b144":"# Machine Learning Section (MLOps)","a5d33e98":"# Imputation using SkLearns Simple Imputer for Missing Values","ec2f0362":"# Scaling using MinMaxScaler","12aeeeaf":"# Age has been excluded as it is a String Value and not a Number","e3105cc7":"# Predict Proba Scores \n## For those who don't know, Predict Proba basically means the probability of having the correct output i.e 1 or 0","aa6f31dc":"## Indian Liver Disease Classification Machine Learning Ops (MLOps) \n### Hi Everybody, My name is Abdullah and I'm a Junior Data Scientist\/Engineer. Just trying to put up some code on Kaggle in order to see how I am with coding and what not. Would love to hear your input! \n\n## A few Pointers:\n* Everythings coded in Python\n* Everythings coded in Kaggles version of Jupyter Notebook \n* I'm just on the Intermediate level, so I would appreciate any constructive criticism from the experts on Kaggle :) \n* Add me on LinkedIn maybe ? www.linkedin.com\/in\/abanwar","802fe4a9":"# Data Analysis","3c050097":"# Running the Automation Method :)","abd056a6":"# Decision Tree Classifier","925f0c65":"#### The plots are quite small hence, I've decided to plot certain plots individually and print out a few Data Frames","eabbd1e8":"# Logisitic Regression","2d7c46e7":"# Variance Inflation to see the best models\n## Later on, in the notebook, there will be a chance to see the best features by using Random Forests *best features* method","c618c03d":"# Random Forest Classifier","beb22d9e":"# Making a List of Tuples for Automating the results\n## First I would like to individually hard code them however, both methods shall be used :)","69be170c":"# Correlation Plot Map","6e5bcb53":"# Plotting the KNearestNeighbors ROC Curve","3446355f":"# K-Neighbors Classifier"}}