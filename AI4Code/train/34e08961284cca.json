{"cell_type":{"af156971":"code","014f97b3":"code","f7838804":"code","964e8a63":"code","9fcca151":"code","eae699ab":"code","8d1c48ac":"code","72416f74":"code","a647bffd":"code","e6c0b1c0":"code","5822bc76":"code","9c524158":"code","3f549ee8":"code","13f5e38b":"code","b399890b":"code","33172b44":"code","e6a3f99d":"code","4df98b68":"code","b35b36b4":"code","194c81a1":"code","eec30ab4":"code","b08a1b88":"code","f882cae1":"code","1cf33038":"code","0510516e":"code","bf805c7d":"code","7f75147f":"code","007b4ae1":"code","80dfbd44":"code","cde584e8":"code","64b11fd1":"code","acc248ce":"code","e95bdc78":"code","840f6a7b":"code","bdaf424c":"code","33251065":"code","00892f63":"code","4ec6497f":"code","3e2660bd":"code","c2cbdca4":"code","97299bce":"code","3e25d9a8":"code","f8f79e66":"code","185a60a7":"code","1aa74330":"code","804d76e7":"code","fd752da2":"code","6e9c0fae":"code","11e4822e":"code","81ebc8b0":"code","39a4d794":"code","8adf17a7":"code","d4175832":"code","be566cb9":"code","792f1b34":"code","19fcb2c0":"code","3955ac95":"code","dcfc58b2":"code","39c60970":"markdown","eeb3e9d2":"markdown","62a16089":"markdown","2c3786d5":"markdown","0231d372":"markdown","ee1c730f":"markdown","0f3c927b":"markdown","271dceb5":"markdown","7df0aab0":"markdown"},"source":{"af156971":"import string\nimport numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\nimport wordcloud\nfrom PIL import Image\nfrom collections import Counter\n\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize \n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression","014f97b3":"\ndf = pd.read_csv(\"..\/input\/sms-spam-collection-dataset\/spam.csv\")","f7838804":"df.shape","964e8a63":"df.head()","9fcca151":"df = df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\ndf = df.rename(columns={\"v1\":\"label\", \"v2\":\"message\"})\ndf.head()","eae699ab":"df.info()","8d1c48ac":"df.groupby(\"label\").describe()","72416f74":"df.label.value_counts().plot.bar()","a647bffd":"df['spam'] = df['label'].map( {'spam': 1, 'ham': 0} ).astype(int)\ndf.head()","e6c0b1c0":"df[\"length\"] = df[\"message\"].apply(len)\ndf.head()","5822bc76":"ham_df = df[df[\"spam\"]==0]\nspam_df = df[df[\"spam\"]==1]","9c524158":"def word_cloud(df, title):\n    \"\"\"\n    `word_cloud` is function which helps to generate word_cloud of text\n    \"\"\"\n    text = ' '.join(df['message'].astype(str).tolist())\n    stopwords = set(wordcloud.STOPWORDS)\n    facecolor = '#353b47'\n    fig_wordcloud = wordcloud.WordCloud(width=1000, height=600, background_color=\"black\",stopwords=stopwords,\n            max_font_size = 160, margin=0).generate(text)\n    \n    plt.figure(figsize=(10,6), facecolor=facecolor)\n    plt.imshow(fig_wordcloud)\n    plt.axis('off') \n    plt.title(title, fontsize=35,color=\"cadetblue\" )\n    plt.tight_layout(pad=2)","3f549ee8":"word_cloud(ham_df, \"Ham Messages\")","13f5e38b":"word_cloud(spam_df,\"Spam Messages\")","b399890b":"def ps_remover(input_data):\n    \"\"\"\n    ps_remover is a function which helps to remove the punctuation and stopwords\n    from the text.\n    \"\"\"\n    list_of_text_without_punctuation = [ch for ch in input_data if ch not in string.punctuation]\n    text_without_punctuation = \"\".join(list_of_text_without_punctuation).split()\n    \n    text_without_puctuation_and_stopword = \\\n            [word.lower() for word in text_without_punctuation if word.lower() not in stopwords.words(\"english\")]\n        \n    return text_without_puctuation_and_stopword\n","33172b44":"ham_df.loc[:, 'message'] = ham_df['message'].apply(ps_remover)\nham_words = ham_df['message'].tolist()\nspam_df.loc[:, 'message'] = spam_df['message'].apply(ps_remover)\nspam_words = spam_df['message'].tolist()","e6a3f99d":"ham_words_list = []\nfor sublist in ham_words:\n    for item in sublist:\n        ham_words_list.append(item)\nc_ham  = Counter(ham_words_list)\ntop30_ham_words  = pd.DataFrame(c_ham.most_common(30),  columns=['word', 'count'])\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.barplot(x='word', y='count', data=top30_ham_words, ax=ax)\nplt.title(\"Top 30 Ham words\")\nplt.xticks(rotation='vertical')","4df98b68":"spam_words_list = []\nfor sublist in spam_words:\n    for item in sublist:\n        spam_words_list.append(item)\nc_spam = Counter(spam_words_list)\ntop30_spam_words= pd.DataFrame(c_spam.most_common(30), columns=['word', 'count'])\nfig, ax = plt.subplots(figsize=(10, 6))\nsns.barplot(x='word', y='count', \n            data=top30_spam_words, ax=ax)\nplt.title(\"Top 30 Spam words\")\nplt.xticks(rotation='vertical');","b35b36b4":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,10))\nword=df[df['spam']==1]['message'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\nax1.set_title('Spam')\nword=df[df['spam']==0]['message'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')\nax2.set_title('Ham')\nfig.suptitle('Average word length in each text')","194c81a1":"from sklearn.feature_extraction.text import CountVectorizer\nbow_transformer = CountVectorizer(analyzer = ps_remover).fit(df['message'])","eec30ab4":"vocab = bow_transformer.vocabulary_","b08a1b88":"len(vocab)","f882cae1":"bow_data = bow_transformer.transform(df['message'])","1cf33038":"bow_data.shape","0510516e":"bow_data.nnz","bf805c7d":"print( bow_data.nnz \/ (bow_data.shape[0] * bow_data.shape[1]) *100 )","7f75147f":"from sklearn.feature_extraction.text import TfidfTransformer","007b4ae1":"tfidf_transformer = TfidfTransformer().fit(bow_data)","80dfbd44":"sample_ham = df[\"message\"][4]\nbow_sample_ham = bow_transformer.transform([sample_ham])\ntfidf_sample_ham = tfidf_transformer.transform(bow_sample_ham)\nprint(tfidf_sample_ham)","cde584e8":"sample_spam = df[\"message\"][8]\nbow_sample_spam = bow_transformer.transform([sample_spam])\ntfidf_sample_spam = tfidf_transformer.transform(bow_sample_spam)\nprint(tfidf_sample_spam)","64b11fd1":"data_tfidf = tfidf_transformer.transform(bow_data)","acc248ce":"data_tfidf","e95bdc78":"from sklearn.model_selection import train_test_split","840f6a7b":"tfidf_train_data, tfidf_test_data, label_train, label_test = \\\n    train_test_split(data_tfidf, df[\"spam\"], test_size=0.25, random_state=5)","bdaf424c":"tfidf_train_data\n","33251065":"tfidf_test_data","00892f63":"from scipy.sparse import  hstack\nfrom sklearn.preprocessing import MinMaxScaler","4ec6497f":"X2 = hstack((data_tfidf ,np.array(df['length'])[:,None])).A\nX2_train, X2_test, y2_train, y2_test = \\\n    train_test_split(X2, df[\"spam\"], test_size=0.25, random_state=5)\n\ntfidf_train_data = tfidf_train_data.A\ntfidf_test_data = tfidf_test_data.A","3e2660bd":"spam_detect_model = MultinomialNB().fit(tfidf_train_data, label_train)\npred_test_MNB = spam_detect_model.predict(tfidf_test_data)\nacc_MNB = accuracy_score(label_test, pred_test_MNB)\nprint(acc_MNB)","c2cbdca4":"import pickle\npickle.dump(spam_detect_model,open('model.pkl','wb'))","97299bce":"tfidf_test_data1 = tfidf_test_data[:1]","3e25d9a8":"len(tfidf_test_data[0])","f8f79e66":"tfidf_test_data1 = tfidf_test_data1[0]","185a60a7":"len(tfidf_test_data1)","1aa74330":"len(deta[0])","804d76e7":"tfidf_test_data=tfidf_test_data.reshape(-1, 1)","fd752da2":"type(label_test)","6e9c0fae":"\n\nscaler = MinMaxScaler()\ntfidf_train_sc_data = scaler.fit_transform(tfidf_train_data)\ntfidf_test_sc_data  = scaler.transform(tfidf_test_data)\n\nX2_tfidf_train = X2_train[:,0:9400]\nX2_tfidf_test  = X2_test[:,0:9400]\nX2_length_train = X2_train[:,9400]\nX2_length_test  = X2_test[:,9400]\n\nscaler = MinMaxScaler()\nX2_tfidf_train = scaler.fit_transform(X2_tfidf_train)\nX2_tfidf_test  = scaler.transform(X2_tfidf_test)\n\nscaler = MinMaxScaler()\nX2_length_train = scaler.fit_transform(X2_length_train.reshape(-1, 1))\nX2_length_test  = scaler.transform(X2_length_test.reshape(-1, 1))\n\nX2_train = np.hstack((X2_tfidf_train, X2_length_train))\nX2_test  = np.hstack((X2_tfidf_test,  X2_length_test))","11e4822e":"models = []\nmodels.append((\"LR\",LogisticRegression()))\nmodels.append((\"NB\",GaussianNB()))\nmodels.append((\"MNB\",MultinomialNB()))\nmodels.append((\"Dtree\",DecisionTreeClassifier()))\nmodels.append((\"KNN\",KNeighborsClassifier()))","81ebc8b0":"from sklearn.model_selection import KFold, cross_val_score, train_test_split","39a4d794":"for name,model in models:\n    kfold = KFold(n_splits=5)\n    cv_result = cross_val_score(model,X2_train, y2_train, cv = kfold,scoring =\"accuracy\")\n    print(name, cv_result)","8adf17a7":"model_MNB = MultinomialNB().fit(X2_train, y2_train)","d4175832":"import string\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom scipy.sparse import  hstack\nfrom sklearn.preprocessing import MinMaxScaler\n\n\n\ndef ps_remover(input_data):\n    \"\"\"\n    ps_remover is a function which helps to remove the punctuation and stopwords\n    from the text.\n    \"\"\"\n    list_of_text_without_punctuation = [ch for ch in input_data if ch not in string.punctuation]\n    text_without_punctuation = \"\".join(list_of_text_without_punctuation).split()\n    \n    text_without_puctuation_and_stopword = \\\n            [word.lower() for word in text_without_punctuation if word.lower() not in stopwords.words(\"english\")]\n        \n    return text_without_puctuation_and_stopword\n\ndef encoder(text):\n\n    bow_transformer = CountVectorizer(analyzer = ps_remover).fit([text])\n    bow_transformer.vocabulary_ = vocab\n    bow_data= bow_transformer.transform([text])\n    tfidf_transformer = TfidfTransformer().fit(bow_data)\n    tfidf_data = tfidf_transformer.transform(bow_data)\n    encoded_data = tfidf_data.A\n    \n    \n\n    return encoded_data","be566cb9":"deta = encoder(\"I don't know\")","792f1b34":"len(deta[0])","19fcb2c0":"type(deta)","3955ac95":"result = spam_detect_model.predict(encoder(\"I don't know\"))","dcfc58b2":"import pickle\npickle.dump(model_MNB,open('model.pkl','wb'))","39c60970":"<a id=\"3\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:black; background:#D4E6E5; border:0' role=\"tab\" aria-controls=\"home\"><center>Encoding<\/center><\/h3>","eeb3e9d2":"![](https:\/\/raw.githubusercontent.com\/divyanshugit\/spam-analyzer\/master\/static\/temp.png)","62a16089":"<a id=\"2\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:black; background:#D4E6E5;border:0' role=\"tab\" aria-controls=\"home\"><center>Data Preprocessing<\/center><\/h3>","2c3786d5":"<a id=\"4\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:black; background:#D4E6E5; border:0' role=\"tab\" aria-controls=\"home\"><center>Model Selection<\/center><\/h3>","0231d372":"<a id=\"7\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=' color:black; background:#D4E6E5; border:0' role=\"tab\" aria-controls=\"home\"><center>Saving Models<\/center><\/h3>","ee1c730f":"<h4>From this we are able to identify this that the dataset contains 4825 ham and 747 spam messages and for both classes, some messages appear more than once.<h4>","0f3c927b":"<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:black; background:#D4E6E5; border:0' role=\"tab\" aria-controls=\"home\"><center>Quick Navigation<\/center><\/h3>\n\n* [1. About](#0)  \n    \n* [2. Data Preprcocessing ](#2)\n\n* [3. Encoding ](#3)\n\n* [4. Model Selection ](#4)\n    \n* [5. Saving Model](#5)\n    \n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#112; border:0' role=\"tab\" aria-controls=\"home\"><center>Do Upvote it if you find it interesting\/useful \ud83e\udd17<\/center><\/h3>","271dceb5":"<a id=\"0\"><\/a>\n\n<font color='cadetblue'>About:<\/font>\n---\nIn this EDA Walkthrough you will get to know about how to preprocess and visualize the data. This kernel covers basics of building a **spam analyzer(filter) model**.\n\nYou can checkout the original source of the dataset from this [link](https:\/\/www.kaggle.com\/uciml\/sms-spam-collection-dataset)\n\n<font color='red'>Hoping that you will love it.<\/font>\n---","7df0aab0":"Labeling for Classifiers:\n---\nMachine Learning Algorithms are functional operator they execute fuctional mathematical operations on their inputs and generate numerical oputputs as a result. So, We need to provide them a label to interpret the data for classification of messages.\n"}}