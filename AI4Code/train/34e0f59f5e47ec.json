{"cell_type":{"2ad8d912":"code","ff71da42":"code","2050da6f":"code","9caecc69":"code","b2ea0529":"code","00dc039c":"code","6046cf88":"code","a16586d8":"code","318b8f0c":"code","b4a614ae":"code","c7641dbb":"code","94215570":"code","ac5ff998":"code","8c415fd5":"code","0d6e3bc6":"code","3aed5520":"code","05872abe":"code","a4202cd6":"code","cfdc7819":"code","9444652f":"code","7a89d08d":"code","119a2468":"code","7143d480":"code","7340290d":"code","88e1fac2":"code","38bef807":"code","cfdda2f1":"code","3eea073c":"code","483c2996":"code","52318d82":"code","dddd783b":"code","38999f47":"code","1d1239c0":"code","19d9eca7":"code","c82599ba":"code","755c8a03":"code","7a444b2c":"code","91805653":"code","383578ef":"code","7cc97f61":"code","3e773763":"code","c4bf4e24":"code","8fc49bff":"code","7187bf35":"code","3b240b73":"code","b747086e":"code","2f801704":"code","0f2e17a2":"code","0430ee8e":"code","36e61764":"code","edee0033":"code","c6fd3b1d":"code","6ca9ccf1":"code","fa190bc7":"code","3326aa6d":"code","616fe614":"code","fb4adb87":"code","f67d6c4c":"code","e8a50e8b":"code","c6b06d05":"code","bd12ba17":"code","5e2da199":"code","db00f8b4":"code","096e9047":"code","a3eb6d5c":"code","3348f877":"code","4418821d":"code","c4e349aa":"code","6879b7ce":"code","823d3cb5":"code","e476a949":"code","976c8ce0":"code","0b5359c1":"code","bdd9c1d3":"code","063efde7":"code","666ee892":"code","14580432":"code","dfcbe457":"code","7b3c5c4f":"code","37ec7e2e":"code","397dfe83":"code","503e7bf4":"code","82a7bd8e":"markdown","77afc395":"markdown","2c67fa64":"markdown","86719f5e":"markdown","51cad041":"markdown","cc21e3a6":"markdown","aa394764":"markdown","114deb65":"markdown","2c00e690":"markdown","227655cf":"markdown","81a6b79f":"markdown","83293c85":"markdown","7e7d1446":"markdown","a0ecf531":"markdown","6eab671c":"markdown","f2fe1c6b":"markdown","dd3bce37":"markdown","152d7505":"markdown","64ef9bf6":"markdown","27984915":"markdown","729eeeec":"markdown","40d21643":"markdown","70ec80e4":"markdown","65002f9c":"markdown","e68affe9":"markdown","49be5bed":"markdown","cba75b64":"markdown","d346bfc1":"markdown","b5af7f86":"markdown","38bbe994":"markdown","4eb86902":"markdown","bf69dae9":"markdown","cfc017e4":"markdown","19108592":"markdown","67755482":"markdown","9e5487b0":"markdown","2a68db58":"markdown","d01c457f":"markdown","f91a35dc":"markdown","247888c2":"markdown","a0b13082":"markdown","5d0ad62e":"markdown","1716954c":"markdown","3b939b1c":"markdown","aaa09a1b":"markdown","7b05f974":"markdown","c033d6ce":"markdown","29409da7":"markdown","089aea95":"markdown","782752c6":"markdown","885e2903":"markdown","511fabdb":"markdown","74cd59d1":"markdown","79edd37b":"markdown","671f51eb":"markdown","0602fd21":"markdown","618421ed":"markdown","27ae5f6b":"markdown","84c8ad7b":"markdown"},"source":{"2ad8d912":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n% matplotlib inline\nimport seaborn as sns\nsns.set(style=\"whitegrid\", color_codes=True)\nsns.set(font_scale=1)\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore') # hiding warning messages for readability\n# warnings.filterwarnings(\"ignore\", category=DeprecationWarning)","ff71da42":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","2050da6f":"train.head()","9caecc69":"test.head()","b2ea0529":"train.shape, test.shape","00dc039c":"train.columns","6046cf88":"train.get_dtype_counts()","a16586d8":"train.describe()","318b8f0c":"train.info()","b4a614ae":"corr = train.corr()[\"SalePrice\"]\ncorr[np.argsort(corr, axis=0)[::-1]]","c7641dbb":"plt.figure(figsize=(20,20))\ncorr = corr[1:-1] # removing 1st (SalePrice) and last (Id) row from dataframe\ncorr.plot(kind='barh') # using pandas plot\nplt.title('Correlation coefficients w.r.t. Sale Price')","94215570":"# taking high correlated variables having positive correlation of 45% and above\nhigh_positive_correlated_variables = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea', \\\n                               'TotalBsmtSF', '1stFlrSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt', \\\n                               'YearRemodAdd', 'GarageYrBlt', 'MasVnrArea', 'Fireplaces']\n\ncorrMatrix = train[high_positive_correlated_variables].corr()\n\nsns.set(font_scale=1.10)\nplt.figure(figsize=(15, 15))\n\nsns.heatmap(corrMatrix, vmax=.8, linewidths=0.01,\n            square=True, annot=True, cmap='viridis', linecolor=\"white\")\n\nplt.title('Correlation between features');","ac5ff998":"feature_variable = 'OverallQual'\ntarget_variable = 'SalePrice'\ntrain[[feature_variable, target_variable]].groupby([feature_variable], as_index=False).mean().sort_values(by=feature_variable, ascending=False)","8c415fd5":"feature_variable = 'GarageCars'\ntarget_variable = 'SalePrice'\ntrain[[feature_variable, target_variable]].groupby([feature_variable], as_index=False).mean().sort_values(by=feature_variable, ascending=False)","0d6e3bc6":"cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'YearBuilt']\nsns.pairplot(train[cols], size = 2.5)","3aed5520":"# box plot overallqual\/saleprice\nplt.figure(figsize=[10,5])\nsns.boxplot(x='OverallQual', y=\"SalePrice\", data=train)","05872abe":"train['SalePrice'].describe()","a4202cd6":"# histogram to graphically show skewness and kurtosis\nplt.figure(figsize=[15,5])\nsns.distplot(train['SalePrice'])\nplt.title('Distribution of Sale Price')\nplt.xlabel('Sale Price')\nplt.ylabel('Number of Occurences')","cfdc7819":"# normal probability plot\nplt.figure(figsize=[8,6])\nstats.probplot(train['SalePrice'], plot=plt)","9444652f":"# skewness and kurtosis\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","7a89d08d":"plt.figure(figsize=[8,6])\nplt.scatter(train[\"SalePrice\"].values, range(train.shape[0]))\nplt.title(\"Distribution of Sale Price\")\nplt.xlabel(\"Sale Price\");\nplt.ylabel(\"Number of Occurences\")","119a2468":"# removing outliers\nupperlimit = np.percentile(train.SalePrice.values, 99.5)\ntrain['SalePrice'].loc[train['SalePrice']>upperlimit] = upperlimit # slicing dataframe upto the uppperlimit","7143d480":"# plotting again the graph after removing outliers\nplt.figure(figsize=[8,6])\nplt.scatter(train[\"SalePrice\"].values, range(train.shape[0]))\nplt.title(\"Distribution of Sale Price\")\nplt.xlabel(\"Sale Price\");\nplt.ylabel(\"Number of Occurences\")","7340290d":"# applying log transformation\ntrain['SalePrice'] = np.log(train['SalePrice'])","88e1fac2":"# histogram to graphically show skewness and kurtosis\nplt.figure(figsize=[15,5])\nsns.distplot(train['SalePrice'])\nplt.title('Distribution of Sale Price')\nplt.xlabel('Sale Price')\nplt.ylabel('Number of Occurences')\n\n# normal probability plot\nplt.figure(figsize=[8,6])\nstats.probplot(train['SalePrice'], plot=plt)","38bef807":"# skewness and kurtosis\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","cfdda2f1":"sns.factorplot(x=\"PoolArea\",y=\"SalePrice\",data=train,hue=\"PoolQC\",kind='bar')\nplt.title(\"Pool Area , Pool quality and SalePrice \")\nplt.ylabel(\"SalePrice\")\nplt.xlabel(\"Pool Area in sq feet\");","3eea073c":"sns.factorplot(\"Fireplaces\",\"SalePrice\",data=train,hue=\"FireplaceQu\");","483c2996":"pd.crosstab(train.Fireplaces, train.FireplaceQu)","52318d82":"# scatter plot grlivarea\/saleprice\nplt.figure(figsize=[8,6])\nplt.scatter(x=train['GrLivArea'], y=train['SalePrice'])\nplt.xlabel('GrLivArea', fontsize=13)\nplt.ylabel('SalePrice', fontsize=13)","dddd783b":"# Deleting outliers\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)","38999f47":"# Plot the graph again\n\n# scatter plot grlivarea\/saleprice\nplt.figure(figsize=[8,6])\nplt.scatter(x=train['GrLivArea'], y=train['SalePrice'])\nplt.xlabel('GrLivArea', fontsize=13)\nplt.ylabel('SalePrice', fontsize=13)","1d1239c0":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nall_data.shape","19d9eca7":"null_columns = all_data.columns[all_data.isnull().any()]\ntotal_null_columns = all_data[null_columns].isnull().sum()\npercent_null_columns = ( all_data[null_columns].isnull().sum() \/ all_data[null_columns].isnull().count() )\nmissing_data = pd.concat([total_null_columns, percent_null_columns], axis=1, keys=['Total', 'Percent']).sort_values(by=['Percent'], ascending=False)\n#missing_data.head()\nmissing_data","c82599ba":"plt.figure(figsize=[20,5])\nplt.xticks(rotation='90', fontsize=14)\nsns.barplot(x=missing_data.index, y=missing_data.Percent)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","755c8a03":"# get unique values of the column data\nall_data['PoolQC'].unique()","7a444b2c":"# replace null values with 'None'\nall_data['PoolQC'].fillna('None', inplace=True)","91805653":"# get unique values of the column data\nall_data['PoolQC'].unique()","383578ef":"# get unique values of the column data\nall_data['MiscFeature'].unique()","7cc97f61":"# replace null values with 'None'\nall_data['MiscFeature'].fillna('None', inplace=True)","3e773763":"# get unique values of the column data\nall_data['Alley'].unique()","c4bf4e24":"# replace null values with 'None'\nall_data['Alley'].fillna('None', inplace=True)","8fc49bff":"# get unique values of the column data\nall_data['Fence'].unique()","7187bf35":"# replace null values with 'None'\nall_data['Fence'].fillna('None', inplace=True)","3b240b73":"# get unique values of the column data\nall_data['FireplaceQu'].unique()","b747086e":"# replace null values with 'None'\nall_data['FireplaceQu'].fillna('None', inplace=True)","2f801704":"# barplot of median of LotFrontage with respect to Neighborhood\nsns.barplot(data=train,x='Neighborhood',y='LotFrontage', estimator=np.median)\nplt.xticks(rotation=90)","0f2e17a2":"# get unique values of the column data\nall_data['LotFrontage'].unique()","0430ee8e":"# replace null values with median LotFrontage of all the Neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","36e61764":"all_data['LotFrontage'].unique()","edee0033":"# get unique values of the column data\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    print (all_data[col].unique())","c6fd3b1d":"# replace null values with 'None'\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col].fillna('None', inplace=True)","6ca9ccf1":"# get unique values of the column data\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    print (all_data[col].unique())","fa190bc7":"# replace null values with 0\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col].fillna(0, inplace=True)","3326aa6d":"# get unique values of the column data\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    print (all_data[col].unique())","616fe614":"# replace null values with 'None'\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col].fillna('None', inplace=True)","fb4adb87":"# replace null values with 0\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col].fillna(0, inplace=True)","f67d6c4c":"all_data[\"MasVnrType\"].fillna(\"None\", inplace=True)\nall_data[\"MasVnrArea\"].fillna(0, inplace=True)","e8a50e8b":"for col in ('MSZoning', 'Utilities', 'Functional', 'Exterior2nd', 'Exterior1st', 'KitchenQual', 'Electrical', 'SaleType'):\n    all_data[col].fillna(all_data[col].mode()[0], inplace=True)","c6b06d05":"null_columns = all_data.columns[all_data.isnull().any()]\nprint (null_columns)","bd12ba17":"numeric_features = all_data.dtypes[all_data.dtypes != 'object'].index\n#print (numeric_features)\n\nskewness = []\nfor col in numeric_features:\n    skewness.append( (col, all_data[col].skew()) )\n    \npd.DataFrame(skewness, columns=('Feature', 'Skewness')).sort_values(by='Skewness', ascending=False)","5e2da199":"all_data.head()","db00f8b4":"positively_skewed_features = all_data[numeric_features].columns[abs(all_data[numeric_features].skew()) > 1]\n#print (positively_skewed_features)\n\n# applying log transformation\nfor col in positively_skewed_features:\n    all_data[col] = np.log(np.ma.array(all_data[col], mask=(all_data[col]<=0))) # using masked array to ignore log transformation of 0 values as (log 0) is undefined","096e9047":"all_data.head()","a3eb6d5c":"%%HTML\n<style>\n  table {margin-left: 0 !important;}\n<\/style>","3348f877":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","4418821d":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","c4e349aa":"train.head()","6879b7ce":"test.head()","823d3cb5":"# importing model libraries\nfrom sklearn.linear_model import ElasticNet, Lasso\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","e476a949":"X_train = train.drop(['Id'], axis=1)\n# y_train has been defined above where we combined train and test data to create all_data\nX_test = test.drop(['Id'], axis=1)","976c8ce0":"#lasso = Lasso(alpha =0.0005, random_state=1)\n#lasso = Lasso()\nmodel_lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005))\n# y_train is defined above where we combined train and test data to create all_data\n# np.sqrt() function is used to create square root of MSE returned by cross_val_score function\ncv_score = np.sqrt( -cross_val_score(model_lasso, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=5) )\nprint (cv_score)\nprint (\"SCORE (mean: %f , std: %f)\" % (np.mean(cv_score), np.std(cv_score)))","0b5359c1":"model_elastic_net = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005))\n# y_train is defined above where we combined train and test data to create all_data\n# np.sqrt() function is used to create square root of MSE returned by cross_val_score function\ncv_score = np.sqrt( -cross_val_score(model_elastic_net, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=5) )\nprint (cv_score)\nprint (\"SCORE (mean: %f , std: %f)\" % (np.mean(cv_score), np.std(cv_score)))","bdd9c1d3":"model_kernel_ridge = KernelRidge(alpha=0.6)\n# y_train is defined above where we combined train and test data to create all_data\n# np.sqrt() function is used to create square root of MSE returned by cross_val_score function\ncv_score = np.sqrt( -cross_val_score(model_kernel_ridge, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=5) )\nprint (cv_score)\nprint (\"SCORE (mean: %f , std: %f)\" % (np.mean(cv_score), np.std(cv_score)))","063efde7":"model_gboost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state=5)\n\n# y_train is defined above where we combined train and test data to create all_data\n# np.sqrt() function is used to create square root of MSE returned by cross_val_score function\ncv_score = np.sqrt( -cross_val_score(model_gboost, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=5) )\nprint (cv_score)\nprint (\"SCORE (mean: %f , std: %f)\" % (np.mean(cv_score), np.std(cv_score)))","666ee892":"model_xgboost = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=True, nthread = -1)\n\n# y_train is defined above where we combined train and test data to create all_data\n# np.sqrt() function is used to create square root of MSE returned by cross_val_score function\ncv_score = np.sqrt( -cross_val_score(model_xgboost, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=5) )\nprint (cv_score)\nprint (\"SCORE (mean: %f , std: %f)\" % (np.mean(cv_score), np.std(cv_score)))","14580432":"model_lgbm = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n\n# y_train is defined above where we combined train and test data to create all_data\n# np.sqrt() function is used to create square root of MSE returned by cross_val_score function\ncv_score = np.sqrt( -cross_val_score(model_lgbm, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=5) )\nprint (cv_score)\nprint (\"SCORE (mean: %f , std: %f)\" % (np.mean(cv_score), np.std(cv_score)))","dfcbe457":"model_lasso.fit(X_train, y_train)\nmodel_elastic_net.fit(X_train, y_train)\nmodel_kernel_ridge.fit(X_train, y_train)\nmodel_gboost.fit(X_train, y_train)\nmodel_xgboost.fit(X_train, y_train)\nmodel_lgbm.fit(X_train, y_train)","7b3c5c4f":"dict_models = {'lasso':model_lasso, 'elastic_net':model_elastic_net, \n               'kernel_ridge':model_kernel_ridge, \n               'gboost':model_gboost, 'xgboost':model_xgboost, 'lgbm':model_lgbm}\n\nfor key, value in dict_models.items():\n    pred_train = value.predict(X_train)\n    rmse = np.sqrt(mean_squared_error(y_train, pred_train))\n    print (\"%s: %f\" % (key, rmse))","37ec7e2e":"prediction_lasso = np.expm1(model_lasso.predict(X_test))\nprediction_elastic_net = np.expm1(model_elastic_net.predict(X_test))\nprediction_kernel_ridge = np.expm1(model_kernel_ridge.predict(X_test))\nprediction_gboost = np.expm1(model_gboost.predict(X_test))\nprediction_xgboost = np.expm1(model_xgboost.predict(X_test))\nprediction_lgbm = np.expm1(model_lgbm.predict(X_test))","397dfe83":"# kaggle score: 0.12346\n#prediction = prediction_gboost\n\n# kaggle score: 0.12053\n#prediction = (prediction_lasso + prediction_xgboost) \/ float(2) \n\n# kaggle score: 0.11960\n#prediction = prediction_lasso \n\n# kaggle score: 0.11937\nprediction = (prediction_lasso + prediction_elastic_net) \/ float(2) \n\n#print prediction","503e7bf4":"submission = pd.DataFrame({\n        \"Id\": test[\"Id\"],\n        \"SalePrice\": prediction\n    })\n\n#submission.to_csv('submission.csv', index=False)","82a7bd8e":"### BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath, BsmtHalfBath\nThese are ordinal\/numeric variables related to Basement. We replace their missing values with \"0\" (zero). Zero means no Basement in the house.","77afc395":"### MSZoning, Utilities, Functional, Exterior2nd, Exterior1st, KitchenQual, Electrical, SaleType\nAll of these features are nominal\/categorical. Each of them has less than 5 missing values. We replace the missing values of each feature by the most common value for that particular feature.","2c67fa64":"### PoolQC\n\n+99% values are missing for *PoolQC*. This means majority of houses have \"no Pool\" in them. We replace NULL values with \"None\".","86719f5e":"# House Price Prediction (Regression Problem)\n## A Beginner's Guide\n\n- Exploratory Data Analysis (EDA) with Visualization\n- Feature Extraction\n- Data Modelling\n- Model Evaluation","51cad041":"## Get data information","cc21e3a6":"### Gradient Boosting Regression\n\n*loss='huber'* is added as parameter to make the model less sensitive to outliers.","aa394764":"### Defining train and test data to train model","114deb65":"## MultiCollinearity\n\nFrom the above **heatmap**, we can see that some features (other than our target variable *SalePrice*) are highly correlated among themselves. Note the yellow blocks in the above heatmap. The following features are intercorrelated:\n\n>TotRmsAbvGrd <> GrLivArea = 0.83\n>\n>GarageYrBlt <> YearBuilt = 0.83\n>\n>1stFlrSF <> TotalBsmtSF = 0.82\n>\n>GarageArea <> GarageCars = 0.88\n\n*OverallQual* is the other feature which is highly correlated with our target variable *SalePrice*.\n\n> SalePrice <> OverallQual = 0.79\n\nThis type of scenario results in **multicollinearity**. [Multicollinearity](https:\/\/en.wikipedia.org\/wiki\/Multicollinearity) occurs when there is moderate or high intercorrelation between independent variables. This can result in high standard error.\n\nThere are different ways to reduce multicollinearity like: \n   - removing the interrelated features \n   - creating a new feature by combining the interrelated features.\n","2c00e690":"### GarageYrBlt, GarageArea and GarageCars\nThese are ordinal\/numeric variables related to Garage. We replace their missing values with \"0\" (zero). Zero means no Garage in the house, so no Cars in Garage.","227655cf":"### Fireplaces, FireplaceQu vs SalePrice\nLet's analyze number of Fireplaces, Fireplace Quality and Sale Price's relationship. \n\n**Note:** Sale price is not displayed in thousand value because it is log-transformed above.\n\nFigure below shows that having 2 fireplaces increases sale price of the house. Excellent quality of Fireplace increases the sale price significantly.","81a6b79f":"### MasVnrArea, MasVnrType\nNA for *MasVnrArea* and *MasVnrType* means there is \"no masonry veneer\" for the house. We replace the NA value of nominal\/categorical feature *MasVnrType* with \"None\" and the NA value of ordinal feature *MasVnrArea* with 0 (zero).","83293c85":"From the above computation and also from the above histogram, we can say that *SalePrice*: \n- is positively skewed or right skewed \n- have high kurtosis\n\nHigh Kurtosis means that *SalePrice* has some outliners. We need to remove them so that they don't affect our prediction result.","7e7d1446":"### XGBoost (eXtreme Gradient Boosting)\n\n[XGBoost](https:\/\/machinelearningmastery.com\/gentle-introduction-xgboost-applied-machine-learning\/) is an implementation of gradient boosted decision trees designed for speed and performance.","a0ecf531":"Target variable *SalePrice* is more in train dataset. It's not present in test dataset. We have to predict *SalePrice* for test dataset.","6eab671c":"### BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2\nThese are categorical (nominal) variables related to Basement. We replace their missing values with \"None\". None means no Basement in the house.","f2fe1c6b":"Note at the bottom right of the above plot. This shows that two very large *GrLivArea* are having low *SalePrice*. These values are outliers for *GrLivArea*. \n\nLet's remove these outliers.","dd3bce37":"### LightGBM (Light Gradient Boosting)\n\n[Light GBM](https:\/\/www.analyticsvidhya.com\/blog\/2017\/06\/which-algorithm-takes-the-crown-light-gbm-vs-xgboost\/) is a fast, distributed, high-performance gradient boosting framework based on decision tree algorithm, used for ranking, classification and many other machine learning tasks.\n\nSince it is based on decision tree algorithms, it splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms. Also, it is surprisingly very fast, hence the word \u2018Light\u2019.","152d7505":"### Generate Predictions on Test dataset\n\nWe use **np.expm1()** function. This calculates $exp(x) - 1$ for all elements in the array. This is needed here because we have log transformed the *SalePrice* earlier to reduce the Skewness of *SalePrice* data distribution.","64ef9bf6":"### Unskewing Data\nWe will use **Log Transformation** to reduce the Skewness of the positively skewed features.","27984915":"### FireplaceQu\n+48% values are missing for *FireplaceQu*. Null value or NA means \"no fireplace\" in the house.","729eeeec":"## Modelling\n\nHere, we create different regression models and evaluate the **Root Mean Square Error (RMSE)** of predictions done by those models. The [root-mean-square error (RMSE)](https:\/\/en.wikipedia.org\/wiki\/Root-mean-square_deviation) is a frequently used measure of the differences between values predicted by a model or an estimator and the values actually observed.\n\n**Note: **\n\n> Scikit-Learn cross-validation features expect a utility function (greater is better) rather than a cost function (lower is better).\n\n> Mean Square Error (MSE) ranges from 0 to 1. Generally, low error means better model. But, in case of scikit-learn, high MSE means better model. So, if our MSE value is 0.9 then we can say that our model is performing better as compared to MSE value 0.2.\n\n> To revert this behavior of scikit-learn, we can use \"scoring\" parameter in \"cross_val_scores\" function like this: \n\n> cv_score = cross_val_score(lasso, train.drop(['Id'], axis=1), y_train, scoring=\"neg_mean_squared_error\", cv=5) \n\nWe will be testing the following Regression Models for this House Price problem:\n\n- Lasso\n- Elastic Net\n- Kernel Ridge\n- Gradient Boost\n- XGBoost\n- LightGBM\n\nLet's first import the model libraries.","40d21643":"## Recheck Columns for Missing Values\nNow, there are no columns with missing values.","70ec80e4":"## Analyzing Predictors \/ Independent Variables Distribution\n\nLet's analyze the distribution of predictors or independent variables. \n\nHere, we do **MULTIVARIATE ANALYSIS**. It's a kind of data observation and analysis which involves two or more variables at a time.","65002f9c":"### Heatmap of highly correlated features with respect to SalePrice","e68affe9":"Let's remove the extreme outliers as seen in the above figure.","49be5bed":"### PoolArea, PoolQC vs SalePrice\nLet's analyze Pool Area, Pool Quality and Sale Price's relationship.\n\n**Note:** Sale price is not displayed in thousand value because it is log-transformed above.","cba75b64":"### Fence\n+80% values are missing for *Fence*. Null value or NA means \"no fence\" in the house.","d346bfc1":"Let's see these features relation to *SalePrice* in overall data:","b5af7f86":"## Import Modules\/Libraries","38bbe994":"### Generating Prediction on Training data\n\nAbove, we have trained our model with the training dataset. Here, use those trained models to generate predition on the training data itself. And then calculate the Root Mean Squre Error (RMSE) of those predictions. \n\nThis can show how accurately the model predict the data that it has already seen before. The result below shows that ***Gradient Boosting*** model has the most accurate predictions for already learned data.","4eb86902":"## Generate Predictions","bf69dae9":"We have removed the extreme outliers from *GrLivArea* variable. Outliers can be present in other variables as well. But, removing outliers from all other variables may adversly affect our model because there can be outliers in test dataset as well. Solution to this will be to make the model more robust.","cfc017e4":"## Getting new Train and Test dataset\n\nWe are done with Feature Engineering part. We will not split ***all_data*** into train and test dataset.","19108592":"## Creating Dummy Categorical Features\n\nDummy variables are used to convert categorical\/nominal features into quantitative one. A new column is created for each unique category of a nominal\/categorical column. Values in that newly created column will be either 1 or 0.\n\nLet's take an example of a column named \"Sex\" which has two values \"male\" and \"female\". If we create dummy variables for this column then two new columns will be added with name \"male\" and \"female\". For any row, if the \"Sex\" value is 'male' then the \"male\" column will have value 1 and \"female\" column will have value 0. Similary, if the \"Sex\" value is 'female' then the \"male\" column will have value 0 and \"female\" column will have value 1.\n\n**BEFORE**\n\n| Row | Sex     |\n| ------------- |\n| 1   | male    |\n| 2   | female  |\n| 3   | female  |\n| 4   | male    |\n\n**AFTER CREATING DUMMY VARIABLES**\n\n| Row | Sex     | male | female |\n| ------------- | ------------- |\n| 1   | male    | 1    | 0      |\n| 2   | female  | 0    | 1      |\n| 3   | female  | 0    | 1      |\n| 4   | male    | 1    | 0      |\n\nWe will now create dummy variables for all our categorical\/nominal features.","67755482":"## Log Transformation\n\nAnother way of reducing skewness is by using **Log Transformation** method so that the data distribution become more linear. The logarithm function squeezes the larger values in your dataset and stretches out the smaller values.\n\nOriginal value = $x$\n\nNew value after log-transformation = $log_{10}(x)$ = $x'$\n\n$x = 1$ then $log_{10}(1) = 0 $\n\n$x = 10$ then $log_{10}(10) = 1$\n\n$x = 100$ then $log_{10}(100) = 2$\n\nLet's **log transform** our target variable ***SalePrice*** values:","9e5487b0":"### Kernel Ridge Regression","2a68db58":"## Analyzing Response \/ Dependent Variable (*SalePrice*) distribution\n\nLet's analyze the distribution of *SalePrice* across our train dataset.\n\nHere, we do **UNIVARIATE ANALYSIS**. It's a kind of data observation and analysis which involves only one variable at a time.\n\nWe analyze [Skewness](https:\/\/en.wikipedia.org\/wiki\/Skewness) and [Kurtosis](https:\/\/en.wikipedia.org\/wiki\/Kurtosis) of *SalePrice*.\n\n**Skewness** is a measure of symmetry, or more precisely, the lack of symmetry. A distribution, or data set, is symmetric if it looks the same to the left and right of the center point.\n\n- ***negative skew:*** The left tail is longer; the mass of the distribution is concentrated on the right of the figure. The distribution is said to be *left-skewed*, *left-tailed*, or *skewed to the left*, despite the fact that the curve itself appears to be skewed or leaning to the right; left instead refers to the left tail being drawn out and, often, the mean being skewed to the left of a typical center of the data. \n\n\n- ***positive skew:*** The right tail is longer; the mass of the distribution is concentrated on the left of the figure. The distribution is said to be *right-skewed*, *right-tailed*, or *skewed to the right*, despite the fact that the curve itself appears to be skewed or leaning to the left; right instead refers to the right tail being drawn out and, often, the mean being skewed to the right of a typical center of the data.\n\n[**Kurtosis**](http:\/\/www.itl.nist.gov\/div898\/handbook\/eda\/section3\/eda35b.htm) is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution. That is, data sets with high kurtosis tend to have heavy tails, or outliers. Data sets with low kurtosis tend to have light tails, or lack of outliers.\n\nGraphical representation of data distribution for ***SalePrice***:\n- **Histogram** - For viewing Skewness and Kurtosis.\n- **Normal Probability Plot** - For viewing how linearly the data is distribute. Data distribution should closely follow the diagonal that represents the normal distribution.\n\n","d01c457f":"Multicollinearity among independent variables as stated above are:\n\n>TotRmsAbvGrd <> GrLivArea = 0.83\n>\n>GarageYrBlt <> YearBuilt = 0.83\n>\n>1stFlrSF <> TotalBsmtSF = 0.82\n>\n>GarageArea <> GarageCars = 0.88\n\nLet's draw a **scatter plot** between *SalePrice* and some of the high correlated variables having positive correlation with respect to *SalePrice*. We take the following independent variables:\n\n- **OverallQual**\n- *TotRmsAbvGrd* and *GrLivArea* are correlated as stated above with 83%. Hence, we only take **GrLivArea** because it has higher correlation with *SalePrice* as compared to *TotalRmsAbvGrid*.\n- *GarageArea* and *GarageCars* are correlated as stated above with 88%. Hence, we only take **GarageCars** because it has a bit higher correlation with *SalePrice* as compared to *GarageArea*.\n- *1stFlrSF* and *TotalBsmtSF* are correlated with 82%. We keep **TotalBsmtSF** because it has a bit higher correlation with *SalePrice* as compared to *1stFlrSF*. \n- *GarageYrBlt* and *YearBuilt* are correlated as stated above with 83%. We keep **YearBuilt** because it has higher correlation with *SalePrice* as compared to *GarageYrBlt*.","f91a35dc":"### ElasticNet Regression\n\n*RobustScaler()* method is added to the pipeline to make the model less sensitive to outliers.","247888c2":"### Alley\n+93% values are missing for *Alley*. Null value or NA means \"no alley access\" in the house.","a0b13082":"### GarageType, GarageFinish, GarageQual and GarageCond\nThese are categorical (nominal) variables related to Garage. We replace their missing values with \"None\". None means no Garage in the house.","5d0ad62e":"## Imputing Missing Values\n\nImputation\/Imputing = Replacing missing data with substituted values.","1716954c":"### Training our regression models\n\nWe have already done Cross Validation before but Cross Validation fits the classifier on different subsets of dataset, and then averages their scores. It is a common practice to train\/fit classifier on full dataset after it has shown sufficient score in Cross Validation.\n\nHence, here we train our models with fit method, i.e. we fit our models with the predictors (X_train) and outcome (y_train) so that it can learn and predict the outcome in future.","3b939b1c":"### MiscFeature\n+96% values are missing for *MiscFeature*. Null value or NA means \"no misc feature\" in the house.","aaa09a1b":"## Getting Missing Values\n\nLet's first concatenate train and test dataset into a single dataframe named ***all_data***.","7b05f974":"## Reducing Skewness of Predictors (Independent Variables)\nEarlier in this notebook, we have reduced the Skewness of our target variable *SalePrice*. We did it through **Log Transformation**. We will apply the same for all other *numeric* dependent variables having high skewness.\n\nLet's check the Skewness of numeric dependent variables:","c033d6ce":"### GrLivArea vs SalePrice\n\nLet's analyze ***GrLivArea*** variable with respect to our target\/response variable ***SalePrice***.","29409da7":"### Different combinations of Predictions\n\nWe can try different prediction combination before generating the Kaggle submission file. We can try single prediction model or an average of two or more prediction model.\n\nI got better result on Kaggle score while combining prediction *Lasso* and *Elastic Net* models and taking the average of their prediction.","089aea95":"### LotFrontage\nLotFrontage: Linear feet of street connected to property\n\n16.67% values are missing for *LotFrontage*. We can assume that the distance of the street connected to the property (*LotFrontage*) will be same as that of that particular property's neighbor property (*Neighborhood*). \n\nWe can fill the missing value by the median *LotFrontage* of all the *Neighborhood*.","782752c6":"List variables with missing data with total number of missing rows along with the missing percentage.","885e2903":"## Correlation of Features with Target Variable\n\nMost features (*OverallQual*, *GrLivArea*, *GarageCars*, etc.) have positive correlation with our target variable **SalesPrice** but some features (*BsmtFinSF2*, *BsmtHalfBath*, *YrSold*, etc.) have negative correlation as well.","511fabdb":"## Load train and test dataset","74cd59d1":"### CSS to change table alignment to left\nTo left-align the categorical tables shown below.","79edd37b":"### Lasso Regression\n*RobustScaler()* method is added to the pipeline to make the model less sensitive to outliers.","671f51eb":"After applying log transformation, let's see the **histogram** and **normal probability** plot to see how has this affected *Skewness* and *Kurtosis* of the data. And, how normal and linear does the data distribution becomes.","0602fd21":"Great! We can see that log transformation has worked well and data distribution of *SalePrice* has been changed from right skewed to normal.","618421ed":"## Create Submission File to Kaggle","27ae5f6b":"From above scatter plot, we can see that: \n- *GrLivArea* and *TotalBsmtSF* are linearly related with *SalePrice*. The variables are positively related. When value of *GrLivArea* or *TotalBsmtSF* increases then *SalePrice* increases.\n- *OverallQual* and *YearBuilt* are also positively related with *SalePrice*. \n\nLet's draw a box plot of *OverallQual* with respect to *SalePrice*.","84c8ad7b":"## References\n\nThis notebook is created by learning from these awesome notebooks:\n\n- [House Prices - Data Exploration and Visualisation](https:\/\/www.kaggle.com\/poonaml\/house-prices-data-exploration-and-visualisation)\n- [Stacked Regressions : Top 4% on LeaderBoard](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard)\n- [Comprehensive data exploration with Python](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python)\n"}}