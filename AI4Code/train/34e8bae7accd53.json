{"cell_type":{"6ea6316a":"code","c558009a":"code","f56c09cf":"code","294595ba":"code","1563d141":"code","fda53a5f":"code","673f89a4":"code","07502704":"code","d96f6ffa":"code","f22414b4":"code","a36f8d4b":"code","820e363c":"code","36ac832c":"code","99261d85":"code","4991e26b":"code","d4a3b206":"code","d57300f9":"code","c3891527":"code","c18e64d2":"code","bf31cb71":"code","10f03322":"code","49bd153f":"code","b1d6cf4f":"code","31c6a913":"code","1780d6ac":"code","0d2367cf":"code","db81825f":"code","0174c0b6":"code","4da13462":"code","0e3c8fcb":"code","db8fc942":"code","1a91088a":"code","e4c10db2":"code","6e20b90c":"code","ad535d49":"code","a5062a63":"code","daae605b":"code","e9c5233d":"code","f9a914a2":"code","c17c8ea3":"code","5c0d70e8":"code","07f7c723":"code","b8764275":"code","1586b098":"code","4af0b21a":"code","f981d1ce":"code","8201920c":"code","685d9248":"code","ec8140aa":"code","c922da5c":"code","b107d5ae":"code","a15a6b8b":"code","f59974ce":"code","d94f20c2":"code","bf0166da":"code","376d9a88":"code","04410eeb":"code","244cbff0":"code","7d2ef299":"code","8e324b7f":"code","0dbe1eaf":"code","48a8933d":"code","183bcda9":"code","7d780a28":"code","f069c520":"code","013893f0":"code","75ffcf34":"code","7de44b64":"code","d4c1440a":"code","b071e291":"code","a8424b81":"code","ff2a65ef":"code","69d3d84c":"code","6b97003a":"code","2ab6db64":"code","4dc61954":"code","1c55a082":"code","5bdff04c":"code","e4b880d7":"code","56bdccc1":"code","c9dacea6":"code","8c14a670":"code","fe753a44":"code","992733c0":"code","7ab4d7db":"code","cd144c76":"code","28ad9def":"code","9236c0d1":"code","0652c8e5":"code","4822f582":"code","814e00d6":"code","f4ffd601":"code","39ef24b1":"code","7abf1eaa":"code","4d882430":"code","7b9d9988":"code","05d95bf9":"code","a1b1f21f":"code","4e3ca250":"code","0f8d3b7e":"code","9fd23fb5":"code","eced3629":"code","678949f8":"code","7595e85a":"code","77955525":"markdown","c35f458c":"markdown","9ef84f45":"markdown","2331f927":"markdown","0a923ba3":"markdown","e2c14f59":"markdown","f5af4d03":"markdown","7fe91bb9":"markdown","50d3c9a1":"markdown","2d3c465b":"markdown","1dceca3f":"markdown","500466f2":"markdown","7e528fe6":"markdown","fce49302":"markdown","12658d20":"markdown","729aaabd":"markdown","c426c8f1":"markdown","e18520ae":"markdown","c04a57ed":"markdown"},"source":{"6ea6316a":"TF Record Basics :https:\/\/www.kaggle.com\/ryanholbrook\/tfrecords-basics","c558009a":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, BatchNormalization, Conv2D\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import models\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom functools import partial\n\nimport numpy as np\nfrom os import path\nimport os\n\nimport tensorflow_addons as tfa\nfrom functools import partial\nfrom kaggle_datasets import KaggleDatasets","f56c09cf":"# setting ploting background \nsns.set_style(\"dark\")\nTPU = True","294595ba":"train_csv = \"..\/input\/seti-breakthrough-listen\/train_labels.csv\"\ntrain_df_master = pd.read_csv( train_csv)\ntrain_df_master.head()","1563d141":"train_df_master.shape","fda53a5f":"train_df_master[\"path\"] = train_df_master[\"id\"].apply( lambda x: \"..\/input\/seti-breakthrough-listen\/train\/\"+ str(x[0]) +\"\/\"+x +\".npy\" )\ntrain_df_master.head()","673f89a4":"for x in range( 1, 5 ):    \n    file_to_check = train_df_master[\"path\"] [ np.random.randint ( 0, train_df_master.shape[0] ) ]\n    \n    if( path.exists ( file_to_check ) ):\n        print ( \"{} file exists...!\".format( file_to_check.split(\"\/\")[-1] ) )\n    else:\n        print ( \"{} file Does not exists...!\".format( file_to_check.split(\"\/\")[-1] ) )","07502704":"def create_heatmap(file_to_load,target ):\n    data= np.load( file_to_load ).astype( np.float32 )\n    fig = plt.figure( figsize = (5,5) , dpi = 80 )\n    \n    fig.suptitle( file_to_load +\"\\b target ==\" +str (target) )\n    \n    print ( file_to_load,target  )\n    data = np.vstack ( (data[1], data[3], data[5] ))\n    print ( \"Shape after vertical stack = {}\".format ( data.shape))\n    #data = np.resize( data, (256,256,3))\n    #for i in range( 0, 6 ):\n    plt.subplot( 1, 1,1 )\n    plt.imshow( data, interpolation='nearest', aspect='auto' )\n    \n    plt.tight_layout()\n    plt.show()","d96f6ffa":"for i in range(0,4):\n    \n    if i % 2== 0 :\n        \n        file_name, target = train_df_master[ [\"path\",\"target\"] ].iloc[  np.random.randint(train_df_master.shape[0] ) ].values \n        \n    else:\n        target,file_name = train_df_master[ train_df_master[\"target\"]== 1] [[\"target\",\"path\"]] .iloc[  np.random.randint(train_df_master[ train_df_master[\"target\"]== 1] [[\"target\",\"path\"]].shape[0] ) ].values \n        \n    create_heatmap ( file_name, target)    ","f22414b4":"   \ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n   \n    \nexcept:\n    strategy = tf.distribute.get_strategy()\n    \nprint('Number of replicas:', strategy.num_replicas_in_sync)\n\n\n\nclass CONFIG():\n    \n    SAMPLE_SIZE= 10000\n    IMAGE_WIDTH = 819\n    IMAGE_HEIGHT= 256\n    BATCH_SIZE = 100 \n    RANDOM_STATE = 200\n    CHANNEL = 3\n    \n    \n\ntrain_df_2 = train_df_master.head( CONFIG.SAMPLE_SIZE )\ntrain_df,test_df  = train_test_split( train_df_2, test_size = 0.2,random_state= CONFIG.RANDOM_STATE, shuffle= True ,stratify= train_df_2[\"target\"] )\nvaldation_df,test_df = train_test_split( test_df, test_size = 0.1, random_state= CONFIG.RANDOM_STATE, shuffle= True ,stratify= test_df[\"target\"] )\n\nclass_weights = compute_class_weight('balanced', \n                                    classes=np.unique(train_df['target'].values),\n                                    y=train_df['target'].values)\n\nclass_weights_dict = {key: val for key, val in zip(np.unique(train_df['target'].values), class_weights)}\nclass_weights_dict     ","a36f8d4b":"train_df.shape","820e363c":"fig, ax  = plt.subplots( nrows = 1, ncols = 3, figsize = ( 15, 5 ), dpi = 90  )\nax[0].set_title( label = \"Train Sample Distribution\")\nsns.countplot( train_df[\"target\"]  ,ax = ax[0])\nax[1].set_title( label = \"Test Sample Distribution\")\nsns.countplot( test_df[\"target\"]  ,ax = ax[1])\nax[2].set_title( label = \"Validation Sample Distribution\")\nsns.countplot( valdation_df[\"target\"]  ,ax = ax[2])\nplt.show()","36ac832c":"'''\ndef Read_numpy_image( file_name_list, traget_list, train = False ):\n    \n    def gen():\n        for each_file, target in zip( file_name_list,traget_list ):\n            \n            data = np.load( file_name )\n            data = np.dstack( (data[0],data[2], data[3]) )\n            data = np.resize(data, ( CONFIG.IMAGE_WIDTH,CONFIG.IMAGE_HEIGHT, 3 ) )\n        \n            yield  data, target \n            \n    return ( gen )\n\n\ndef map_func ( x, y  ):\n    return ( x, y )\n\n\n\ndef DATA_SET_GENERATOR ( df ):\n    \n        AUTOTUNE = tf.data.AUTOTUNE\n        train_dataset = tf.data.Dataset.from_generator(generator=Read_numpy_image( file_name_list= df[\"path\"], \n                                                                                   traget_list = df[\"target\"] \n                                                                                 ),\n                                                      output_types = ( np.float32,np.float32),\n                                                      output_shapes= ( ( CONFIG.IMAGE_WIDTH, CONFIG.IMAGE_HEIGHT,3 ), 1)\n                                                      )\n        train_dataset.map( map_func=map_func,\n                            num_parallel_calls=AUTOTUNE,\n                            # Order does not matter.\n                            deterministic=False#,\n                            #prefetch(AUTO)\n                         )\n        train_dataset = train_dataset.batch( CONFIG.BATCH_SIZE, drop_remainder=False)\n        # Prefetch some batches.\n        train_dataset = train_dataset.prefetch(AUTOTUNE)\n    \n        return ( train_dataset )\n'''","99261d85":"\ndef read_numpy_data( file_name_result ):\n\n    file_name=file_name_result\n    data = np.load( file_name.numpy() ).astype( np.float32 )    \n    data = np.vstack( ( data[1], data[3], data[5] ) )\n    return  np.dstack ( [ data, data, data] ) \n\n\n\ndef resize_image( df_dict ):\n\n    [image, ] =  tf.py_function( read_numpy_data,  [ df_dict[\"path\"] ], [tf.float32] )\n    image.set_shape( ( CONFIG.IMAGE_HEIGHT,CONFIG.IMAGE_WIDTH, CONFIG.CHANNEL ) )\n    image = tf.image.resize( image , ( CONFIG.IMAGE_HEIGHT,CONFIG.IMAGE_WIDTH ) )\n    label = df_dict[\"target\"]\n    label = tf.cast( label, tf.int16 )\n\n    return  image, label \n\n\n\n\ndef create_tf_dataset( dataframe ):\n\n    tf_dataset = tf.data.Dataset.from_tensor_slices ( dict ( dataframe[ [\"path\",\"target\"] ] ) ) \n    tf_dataset = ( tf_dataset\n                  .shuffle (1024)\n                  .map( resize_image,num_parallel_calls= tf.data.AUTOTUNE)\n                  .batch( CONFIG.BATCH_SIZE )\n                  .prefetch( tf.data.AUTOTUNE)\n                 )\n\n    return  tf_dataset  \n\ntrain_dataset = create_tf_dataset ( train_df )\ntest_dataset = create_tf_dataset ( test_df )\n\nval_dataset = create_tf_dataset ( valdation_df )\n\n\n\n\nTRAIN_STEPS_PER_EPOCH= int ( train_df.shape[0]\/CONFIG.BATCH_SIZE )\nVALIDATION_STEPS_PER_EPOCH = int ( valdation_df.shape[0]\/CONFIG.BATCH_SIZE )\n\nif ( TRAIN_STEPS_PER_EPOCH * CONFIG.BATCH_SIZE ) !=  train_df.shape[0]: TRAIN_STEPS_PER_EPOCH= TRAIN_STEPS_PER_EPOCH+1\nif ( VALIDATION_STEPS_PER_EPOCH * CONFIG.BATCH_SIZE ) !=  valdation_df.shape[0]: VALIDATION_STEPS_PER_EPOCH= VALIDATION_STEPS_PER_EPOCH+1\n\n\n    ","4991e26b":"'''\ntrain_dataset = DATA_SET_GENERATOR ( train_df )\ntest_dataset = DATA_SET_GENERATOR ( test_df )\nval_dataset = DATA_SET_GENERATOR ( valdation_df )\n\nTRAIN_STEPS_PER_EPOCH= int ( train_df.shape[0]\/CONFIG.BATCH_SIZE )\nVALIDATION_STEPS_PER_EPOCH = int ( valdation_df.shape[0]\/CONFIG.BATCH_SIZE )\nif ( TRAIN_STEPS_PER_EPOCH * CONFIG.BATCH_SIZE ) !=  train_df.shape[0]: TRAIN_STEPS_PER_EPOCH= TRAIN_STEPS_PER_EPOCH+1\nif ( VALIDATION_STEPS_PER_EPOCH * CONFIG.BATCH_SIZE ) !=  valdation_df.shape[0]: VALIDATION_STEPS_PER_EPOCH= VALIDATION_STEPS_PER_EPOCH+1\n'''","d4a3b206":"def build_lrfn(lr_start=0.00001, lr_max=0.000075, lr_min=0.000001, lr_rampup_epochs=20, lr_sustain_epochs=0, lr_exp_decay=.8):\n    lr_max = lr_max * strategy.num_replicas_in_sync\n    \n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) \/ lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_exp_decay ** (epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n        return lr\n    \n    return lrfn","d57300f9":"def create_train_val_loss_plot( history):\n    \n    key = list ( history.history.keys() )\n    \n    key = [ key[1],key[3] ]\n    sns.lineplot (x = range(0, len(history.history[key[0]] )), y = history.history[key[0]] ) \n    sns.lineplot (x = range(0, len(history.history[key[1]] )), y = history.history[key[1]] ) \n    plt.legend( key, loc =\"upper right\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(key[0])\n    plt.show()","c3891527":"optimizer = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0001)\n\nnew_eff_model.compile( optimizer=optimizer, loss=\"binary_crossentropy\",metrics=['accuracy'])","c18e64d2":"lrfn = build_lrfn()\n\nmodel_history = new_eff_model.fit_generator(train_dataset,\n                                             class_weight= class_weights_dict,\n                                             steps_per_epoch= TRAIN_STEPS_PER_EPOCH, \n                                             epochs = 10, \n                                             validation_data= val_dataset,\n                                             validation_steps = VALIDATION_STEPS_PER_EPOCH,\n                                            callbacks=[ tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1),\n                                                      tf.keras.callbacks.ModelCheckpoint( os.path.join(\".\/model.h5\"), \n                                                                                         monitor='train_loss', \n                                                                                         verbose=0,\n                                                                                         save_best_only=True, \n                                                                                         save_weights_only=False,\n                                                                                         mode='auto', \n                                                                                         save_freq='epoch'\n                                                                                        )\n                                                      ]\n                                        )","bf31cb71":"efff_net.summary()","10f03322":"create_train_val_loss_plot ( model_history )","49bd153f":"horizontal_flip = tf.keras.layers.experimental.preprocessing.RandomFlip( \"horizontal_and_vertical\", seed=CONFIG.RANDOM_STATE )\nrandom_rotation = tf.keras.layers.experimental.preprocessing.RandomRotation ( 0.2 )\ncontrast = tf.keras.layers.experimental.preprocessing.RandomContrast( factor = 0.2,  seed=CONFIG.RANDOM_STATE  )\nnoise = tf.keras.layers.GaussianNoise( stddev= 0.2 )","b1d6cf4f":"\ninput_mo = tf.keras.layers.Input(shape= ( CONFIG.IMAGE_HEIGHT, CONFIG.IMAGE_WIDTH, CONFIG.CHANNEL) )\n\nefff_net =tf.keras.applications.EfficientNetB7(include_top = False, \n                                               weights =\"imagenet\" , \n                                               input_shape = ( CONFIG.IMAGE_HEIGHT, CONFIG.IMAGE_WIDTH, CONFIG.CHANNEL) ,\n                                               input_tensor = input_mo,\n                                               classes=2\n                                              )\n\nfor i ,layer in enumerate ( efff_net.layers ) :\n    if i> 700:\n        layer.trainable = False\n    else:\n        layer.trainable = False\n        #horizontal_flip,random_rotation,contrast,\n  \nmodel = tf.keras.Sequential( [horizontal_flip,random_rotation,contrast,noise,efff_net,\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(512, activation= 'tanh'), \n        tf.keras.layers.Dropout(0.25),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(200, activation= 'tanh'),\n        tf.keras.layers.Dropout(0.25),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dense(100, activation= 'tanh'),\n        tf.keras.layers.Dense(1, activation='tanh')\n        ])    \n        \n        \n#efff_net( inputs = input_mo)\n#eff_net_output = tf.keras.layers.Flatten()( efff_net.layers[-1].output )\n\n\n#x = layers.GlobalAveragePooling2D()( efff_net.layers[-1].output )\n#x = layers.Dropout(0.5)(x)\n    \n#outputs = layers.Dense(1 )(x)\n#outputs = layers.Activation('sigmoid', dtype='float32', name='predictions')(outputs)\n\n#layer_1 = tf.keras.layers.Dense( units =500,activation = \"relu\" , use_bias = True ) (eff_net_output) \n#layer_2 = tf.keras.layers.Dropout( rate= 0.05 )  ( layer_1 )\n#layer_3 =  tf.keras.layers.Dense( units =100,activation = \"relu\" , use_bias = True )  ( layer_2 )\n#layer_4 =  tf.keras.layers.Dropout( rate= 0.05 )  ( layer_3 )\n#layer_5 =  tf.keras.layers.Dense( units =1,activation = \"sigmoid\" , use_bias = True ) ( layer_4 ) \n#layer_6 =  tf.keras.layers.Dropout( rate= 0.25 )  ( layer_5 )\n#layer_7 =  tf.keras.layers.Dense( units =1,activation = \"sigmoid\" , use_bias = True )  ( layer_6 )\n\n#new_eff_model_2 = models.Model( inputs = input_mo, outputs = outputs )\n\noptimizer = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0001)\n","31c6a913":"model.compile( optimizer= \"adam\", #optimizer\n                        loss=\"binary_crossentropy\",\n                        metrics=[tf.keras.metrics.AUC(curve='ROC')])\n","1780d6ac":"\n\n#import neural_structured_learning as nsl\n\nlrfn = build_lrfn()\n\n# Wrap the model with adversarial regularization.\n#adv_config = nsl.configs.make_adv_reg_config(multiplier=0.2, adv_step_size=0.05)\n#adv_model = nsl.keras.AdversarialRegularization(model, adv_config=adv_config)\n\n#adv_model.compile( optimizer= \"adam\", #optimizer\n#                        loss=\"binary_crossentropy\",\n#                        metrics=[tf.keras.metrics.AUC(curve='ROC')])\n\n\nmodel_history2 = model.fit_generator(train_dataset,\n                                              class_weight= class_weights_dict,\n                                             steps_per_epoch= TRAIN_STEPS_PER_EPOCH, \n                                             epochs =10, \n                                             validation_data= val_dataset,\n                                             validation_steps = VALIDATION_STEPS_PER_EPOCH,\n                                            callbacks=[ tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1),\n                                  tf.keras.callbacks.ModelCheckpoint( \".\/model2.h5\", \n                                                                     monitor='train_loss', \n                                                                     verbose=0,\n                                                                     save_best_only=True, \n                                                                     save_weights_only=True,\n                                                                     mode='auto', \n                                                                     save_freq='epoch'\n                                                                    )\n                                  ]\n                        )","0d2367cf":"create_train_val_loss_plot ( model_history2 )","db81825f":" predict = model.predict( test_dataset ) ","0174c0b6":"def confusion_matrix ( model, test_data, ):\n    \n    predicted_data = model.predict( test_data )","4da13462":"## Thanks to UPGRAD, from one othere project I got this code, hoping it will work for Auto encoder architecture neural network \n\ndef data_generator_Rev_01( file_list , image_width,image_length, num_channel):\n    \n    def generator():\n        \n\n            #batch_data = np.zeros( (  image_width, image_length, num_channel ) )\n            #batch_file_list = np.random.permutation ( file_list )\n\n            #for count , each_file in enumerate( batch_file_list) :\n            for each_file in file_list:\n                \n                np_data = np.load( each_file ).astype( np.float32 )    \n                np_data = np.dstack( ( np_data[0], np_data[2], np_data[4] ) ) #( np_data[0], np_data[2], np_data[4] )\n\n                np_data = np.resize( np_data,( image_width,image_length, num_channel ) )\n\n                yield np_data,np_data\n            \n    return ( generator )\n\n\n\ndef map_func( x ):\n    return x","0e3c8fcb":"AUTOTUNE = tf.data.AUTOTUNE\n#file_list = train_df[train_df[\"target\"] == 0][\"path\"].values()\ngen = data_generator_Rev_01 ( file_list = file_list,\n                                #batch_size = CONFIG[\"BATCH_SIZE\"],\n                                image_width = CONFIG[\"image_width\"], \n                                image_length = CONFIG[\"image_height\"], \n                                num_channel = CONFIG[\"num_channel\"]\n                               )\ndataset = tf.data.Dataset.from_generator(\n    generator=gen, \n    output_types=(np.float32, np.int32), \n    output_shapes=( ( CONFIG[\"image_width\"],CONFIG[\"image_height\"],3 ), ( CONFIG[\"image_width\"],CONFIG[\"image_height\"],3 ) )\n)\n\n# Parallelize the augmentation.\ndataset = dataset.map(\n    map_func, \n    num_parallel_calls=AUTOTUNE,\n    # Order does not matter.\n    deterministic=False#,\n    #prefetch(AUTO)\n)\ndataset = dataset.batch( CONFIG[\"BATCH_SIZE\"], drop_remainder=False)\n# Prefetch some batches.\ndataset = dataset.prefetch(AUTOTUNE)","db8fc942":"## Thanks to UPGRAD, from one othere project I got this code, hoping it will work for Auto encoder architecture neural network \n\ndef data_generator( file_list, batch_size,image_width, image_length, num_channel = 3 ):\n    \n    \n    number_of_batches = int ( len ( file_list )\/ batch_size )\n    \n    leftoved_file_count = 0\n    \n    if (number_of_batches * batch_size) == len( file_list ):\n        leftover= False\n    else:\n        leftover = True\n        leftoved_file_count = len ( file_list ) -  (number_of_batches * batch_size)\n    \n    \n    while True:\n        \n        for each_batch in range( 0, number_of_batches ):\n            \n            if  leftover & ( each_batch == number_of_batches -1 )  :\n                \n                batch_data = np.zeros( ( leftoved_file_count, image_width, image_length, num_channel ) ) # [ Batch number]\n                batch_file_list = np.random.permutation ( file_list[ : - leftoved_file_count ] )\n                \n                \n            else:\n                \n                batch_data = np.zeros( ( batch_size, image_width, image_length, num_channel ) )\n                batch_file_list = np.random.permutation ( file_list[ ( each_batch * batch_size ) : (batch_size * (each_batch+ 1 ) ) ] )\n            \n            for count , each_file in enumerate( batch_file_list) :\n                \n                np_data = np.load( each_file ).astype( np.float32 )    \n                np_data = np.dstack( ( np_data[0], np_data[2], np_data[4] ) )\n                \n                batch_data[count] = resize( np_data,( image_width,image_length, num_channel ) )\n        \n        yield batch_data","1a91088a":"CONFIG = { \"BATCH_SIZE\" : 50, \"image_width\": 224, \"image_height\": 224,\"num_channel\" : 3, \"sample_size\" : 1000, }","e4c10db2":"train_data_x = data_generator ( file_list = np.random.permutation (train_df[\"path\"]) [: CONFIG[\"sample_size\"]],\n                                batch_size = CONFIG[\"BATCH_SIZE\"],\n                                image_width = CONFIG[\"image_width\"], \n                                image_length = CONFIG[\"image_height\"], \n                                num_channel = CONFIG[\"num_channel\"]\n                               )\n\nif CONFIG[\"sample_size\"] % CONFIG[\"BATCH_SIZE\"] == 0 :\n    num_steps = int( CONFIG[\"sample_size\"]\/CONFIG[\"BATCH_SIZE\"]) \nelse:\n    num_steps = int( CONFIG[\"sample_size\"] \/CONFIG[\"BATCH_SIZE\"])  + 1","6e20b90c":"#next ( train_data_x )","ad535d49":"model_input  =   ( CONFIG[\"image_height\"], CONFIG[\"image_width\"], CONFIG[\"num_channel\"])\nencoder = models.Sequential()\nencoder.add( layers.Conv2D ( filters = 224 , kernel_size = 2, padding =\"same\",strides = 1, activation = \"tanh\",use_bias = True, input_shape = model_input  ) )\n#encoder.add( layers.BatchNormalization ( ) )\nencoder.add( layers.Conv2D ( filters = 128 , kernel_size = 3, padding =\"same\",strides = 2,activation = \"tanh\",use_bias = True ) )#, name=\"layer_02\" )\n#encoder.add( layers.BatchNormalization ( ) )\nencoder.add( layers.Conv2D ( filters = 64 , kernel_size = 4, padding =\"same\",strides = 2,activation = \"tanh\",use_bias = True ) )#, name=\"layer_03\")\n#encoder.add( layers.BatchNormalization ( ) si\nencoder.add( layers.Conv2D ( filters = 32 , kernel_size = 4, padding =\"same\",strides = 2,activation = \"tanh\",use_bias = True ) )#, name=\"layer_04\")\n#encoder.add( layers.BatchNormalization ( ) )\nencoder.add( layers.Conv2D ( filters = 16 , kernel_size = 4, padding =\"same\",strides = 2,activation = \"tanh\",use_bias = True ) )#, name=\"layer_04\")\nencoder.add( layers.Conv2D ( filters = 8 , kernel_size = 4, padding =\"same\",strides = 2,activation = \"tanh\",use_bias = True ) )#, name=\"layer_04\")\n#decoder = models.Sequential()\nencoder.add( layers.Conv2DTranspose ( filters = 8 , kernel_size =4 , padding =\"same\",strides = 2,activation = \"tanh\",use_bias = True ) )#, name=\"layer_04\")                    \n#encoder.add( layers.BatchNormalization ( ) )\nencoder.add( layers.Conv2DTranspose ( filters = 16 , kernel_size = 4, padding =\"same\",strides = 2,activation = \"tanh\",use_bias = True ) )#, name=\"layer_03\")\n#encoder.add( layers.BatchNormalization ( ) )\nencoder.add( layers.Conv2DTranspose ( filters = 32 , kernel_size = 4, padding =\"same\",strides = 2,activation = \"tanh\",use_bias = True ) )#, name=\"layer_02\" )\n#encoder.add( layers.BatchNormalization ( ) )\nencoder.add( layers.Conv2DTranspose ( filters = 64 , kernel_size = 4, padding =\"same\",strides = 2,activation = \"tanh\",use_bias = True) )\n#encoder.add( layers.BatchNormalization ( ) )\nencoder.add( layers.Conv2DTranspose ( filters = 128 , kernel_size =3, padding =\"same\",strides = 2,activation = \"tanh\",use_bias = True) )\nencoder.add( layers.Conv2DTranspose ( filters = 3 , kernel_size = 2, padding =\"same\",strides = 1,activation = \"tanh\",use_bias = True) )\n\n\nencoder.compile ( optimizer= \"adam\", \n               loss=\"mse\"\n               )\n\nencoder.summary()","a5062a63":"#encoder.fit( train_data_x,train_data_x,epochs = 10  ,steps_per_epoch = num_steps ,verbose=1)","daae605b":"## Thanks to UPGRAD, from one othere project I got this code, hoping it will work for Auto encoder architecture neural network \n\ndef data_generator_Rev_01( file_list , image_width,image_length, num_channel):\n    \n    def generator():\n        \n\n            #batch_data = np.zeros( (  image_width, image_length, num_channel ) )\n            #batch_file_list = np.random.permutation ( file_list )\n\n            #for count , each_file in enumerate( batch_file_list) :\n            for each_file in file_list:\n                \n                np_data = np.load( each_file ).astype( np.float32 )    \n                np_data = np.dstack( ( np_data[0], np_data[2], np_data[4] ) ) #( np_data[0], np_data[2], np_data[4] )\n\n                np_data = np.resize( np_data,( image_width,image_length, num_channel ) )\n\n                yield np_data,np_data\n            \n    return ( generator )\n\n\n\ndef map_func( x, y):\n    return x, y","e9c5233d":"file_list = train_df[train_df[\"target\"] == 0][\"path\"].values [: CONFIG[\"sample_size\"]]","f9a914a2":"AUTOTUNE = tf.data.AUTOTUNE\n#file_list = train_df[train_df[\"target\"] == 0][\"path\"].values()\ngen = data_generator_Rev_01 ( file_list = file_list,\n                                #batch_size = CONFIG[\"BATCH_SIZE\"],\n                                image_width = CONFIG[\"image_width\"], \n                                image_length = CONFIG[\"image_height\"], \n                                num_channel = CONFIG[\"num_channel\"]\n                               )\ndataset = tf.data.Dataset.from_generator(\n    generator=gen, \n    output_types=(np.float32, np.int32), \n    output_shapes=( ( CONFIG[\"image_width\"],CONFIG[\"image_height\"],3 ), ( CONFIG[\"image_width\"],CONFIG[\"image_height\"],3 ) )\n)\n\n# Parallelize the augmentation.\ndataset = dataset.map(\n    map_func, \n    num_parallel_calls=AUTOTUNE,\n    # Order does not matter.\n    deterministic=False#,\n    #prefetch(AUTO)\n)\ndataset = dataset.batch( CONFIG[\"BATCH_SIZE\"], drop_remainder=False)\n# Prefetch some batches.\ndataset = dataset.prefetch(AUTOTUNE)","c17c8ea3":"lrfn = build_lrfn()\nAuto_Encoder_History = encoder.fit_generator( dataset ,\n                                             epochs = 30 ,\n                                             callbacks=[ tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)] )","5c0d70e8":"# Reference :https:\/\/www.kaggle.com\/akhileshdkapse\/seti-cnn-encoder-decoder-model-tpu\n#https:\/\/www.kaggle.com\/awsaf49\/seti-bl-tf-starter-tpu\n#https:\/\/machinelearningmastery.com\/learning-rate-for-deep-learning-neural-networks\/","07f7c723":"loss_value = Auto_Encoder_History.history[\"loss\"]\nsns.lineplot( y = loss_value ,x = range( 0, len(loss_value) )  )","b8764275":"def data_generator_for_predictor( file_list,image_width, image_length, num_channel = 3 ):\n    \n        #batch_data = np.zeros( (  image_width, image_length, num_channel ) )\n\n        np_data = np.load( file_list ).astype( np.float32 )    \n        np_data = np.dstack( ( np_data[1], np_data[3], np_data[5] ) )\n\n        batch_data = np.resize( np_data,( 1 ,image_width,image_length, num_channel ) )\n\n        return ( batch_data )","1586b098":"file_path_name = train_df [\"path\"].iloc [ np.random.randint( 1000) ]\nfile_path_name = train_df[train_df[\"target\"] == 0 ] [\"path\"].iloc [10  ]\n\nd = data_generator_for_predictor( file_path_name,\n                                image_width = CONFIG[\"image_width\"], \n                                image_length = CONFIG[\"image_height\"], \n                                num_channel = CONFIG[\"num_channel\"]\n                                )\ndecode_data = encoder.predict( d )\nmean_absoulte_error = tf.keras.losses.MeanAbsoluteError()\nmean_absoulte_error( decode_data[0] , d[ 0 ] )","4af0b21a":"len( list ( train_df[\"path\"][:100] ) )","f981d1ce":"mean_absoulte_error = tf.keras.losses.MeanAbsoluteError()\nloss_value = []\ntrain_df_2 = train_df.head( 100 )\nfor each_file in train_df[\"path\"][:100]:\n    d = data_generator_for_predictor( each_file,\n                                image_width = CONFIG[\"image_width\"], \n                                image_length = CONFIG[\"image_height\"], \n                                num_channel = CONFIG[\"num_channel\"]\n                                )\n    decode_data = encoder.predict( d )\n\n    loss_value.append( mean_absoulte_error( decode_data[0] , d[ 0 ] ).numpy() )\n\ntrain_df_2[\"MAE\"] = loss_value","8201920c":"train_df_2.head()","685d9248":"fig = plt.figure(figsize=( 10,10))\nsns.lineplot (data = train_df_2, y=\"MAE\",x =\"id\" , hue = \"target\" )\nplt.show()","ec8140aa":"train_df_2[ ( train_df_2[\"MAE\"]>0.6) & (train_df_2[\"taget\"] == 1 ) ]","c922da5c":"target = train_df [ train_df[\"path\"] ==file_path_name ][\"target\"].values[0]\ncreate_heatmap ( file_path_name, target)","b107d5ae":"train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n  \"..\/input\/seti-breakthrough-listen\/train\/\",\n  validation_split=0.2,\n  subset=\"training\",\n  seed=123,\n  #image_size=(img_height, img_width),\n  batch_size=30)","a15a6b8b":"a.shape","f59974ce":"a[1] = np.zeros( (2,2,3))","d94f20c2":"data_path = \"..\/input\/seti-breakthrough-listen\/train_labels.csv\"\ndf = pd.read_csv( data_path, sep = \",\")","bf0166da":"df.head()","376d9a88":"print (\"number of signal with target 1 = {}\".format( df[ df[\"target\"] == 1 ].shape) )\nprint (\"number of signal with target 0 = {}\".format( df[ df[\"target\"] == 0 ].shape) )","04410eeb":"sns.set_style(\"darkgrid\")","244cbff0":"fig = plt.figure( figsize = ( 8,10) , dpi =80 )\nax = fig.add_subplot ( 111)\nplt.title( \" Count of Anomally & non Anamolly data \")\nsns.countplot( df[\"target\"], ax = ax  )\nax.xaxis.set_tick_params( labelsize = 20 )\nax.yaxis.set_tick_params( labelsize = 20 )\nax.yaxis.label.set_fontsize( 20 )\nax.xaxis.label.set_fontsize( 20 )","7d2ef299":"df.tail()","8e324b7f":"train_path =\"..\/input\/seti-breakthrough-listen\/train\/\"\ndf[\"path\"]= df[\"id\"].apply( lambda x: train_path+x[0]+\"\/\"+ x +\".npy\" )","0dbe1eaf":"df.head()","48a8933d":"np.load( df[\"path\"].iloc[ np.random.randint(df.shape[0] )] ).shape","183bcda9":"def create_heatmap():\n    data= np.load( df[\"path\"].iloc[ np.random.randint(df.shape[0] )] )\n    fig, axis = plt.subplots( nrows = 1, ncols = 6 , dpi = 80 , figsize  =(20,5))\n    axis = axis.flatten()\n    for i,each_axis in enumerate( axis) :    sns.heatmap( data[i], ax = each_axis )","7d780a28":"create_heatmap()\ncreate_heatmap()\ncreate_heatmap()","f069c520":"## IMPORTING REQUIRED TENSOR FLOW \n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom  tensorflow.keras import layers,models\nfrom sklearn.model_selection import train_test_split\nimport datetime\nimport os","013893f0":"CONFIG = {\"image_height\" : 224,\n          \"image_width\"  : 224,\n          \"batch_size\" :100,\n          \"random_state\" :  100\n         }","75ffcf34":"test_df,train_df  = train_test_split ( train_df[ [\"path\",\"target\" ] ], \n                             test_size= 0.8, \n                             random_state= CONFIG[\"random_state\"], \n                             shuffle =True )","7de44b64":"def creat_plot(path):\n    data= np.load( path )\n    fig, axis = plt.subplots( nrows = 1, ncols = 6 , dpi = 80 , figsize  =(20,5))\n    axis = axis.flatten()\n    for i,each_axis in enumerate( axis) :    sns.heatmap( data[i], ax = each_axis )","d4c1440a":"creat_plot(train_df[\"path\"].iloc [10] )\ncreat_plot(train_df[\"path\"].iloc[20] )","b071e291":"print ( \"Number of file for training = {}\".format( train_df.shape ))\nprint ( \"Number of file has anamoly  = {}\".format( test_df.shape ))","a8424b81":"fig = plt.figure( figsize = ( 15,8) , dpi =80 )\nax = fig.add_subplot ( 121)\nplt.title( \" Count of Anomally & non Anamolly data in train_data  \")\nsns.countplot( train_df[\"target\"], ax = ax  )\nax.xaxis.set_tick_params( labelsize = 20 )\nax.yaxis.set_tick_params( labelsize = 20 )\nax.yaxis.label.set_fontsize( 20 )\nax.xaxis.label.set_fontsize( 20 )\n\nax = fig.add_subplot ( 122)\nplt.title( \" Count of Anomally & non Anamolly data in test data  \")\nsns.countplot( test_df[\"target\"], ax = ax  )\nax.xaxis.set_tick_params( labelsize = 20 )\nax.yaxis.set_tick_params( labelsize = 20 )\nax.yaxis.label.set_fontsize( 20 )\nax.xaxis.label.set_fontsize( 20 )","ff2a65ef":"def read_numpy_data( file_name_result ):\n    \n    file_name=file_name_result\n    data = np.load( file_name.numpy() ).astype( np.float32 )    \n    data = np.dstack( ( data[0], data[2], data[4] ) )\n    return ( data )\n    ","69d3d84c":"def resize_image( df_dict ):\n    \n    [image, ] =  tf.py_function( read_numpy_data,  [ df_dict[\"path\"] ], [tf.float32] )\n    image.set_shape(( 273, 256, 3 ) )\n    image = tf.image.resize( image , ( CONFIG[\"image_height\"],  CONFIG[\"image_width\"] ) )\n    label =  df_dict[\"target\"]#.reshape((-1,1))\n    label = tf.cast( label, tf.float32 ), (-1,1) \n  \n    return ( image, label )","6b97003a":"def create_tf_dataset( dataframe ):\n    \n    tf_dataset = tf.data.Dataset.from_tensor_slices ( dict ( dataframe[ [\"path\",\"target\"] ] ) ) \n    tf_dataset = ( tf_dataset\n                  .shuffle (1024)\n                  .map( resize_image,num_parallel_calls= tf.data.AUTOTUNE)\n                  .batch( CONFIG[\"batch_size\"] )\n                  .prefetch(tf.data.AUTOTUNE)\n                 )\n    \n    return ( tf_dataset )","2ab6db64":"train_tf_dataset = create_tf_dataset ( train_df )\ntest_tf_dataset = create_tf_dataset ( test_df )","4dc61954":"CONFIG[\"model_name\"] = \"model_1\"\nCONFIG[\"group\"] = \"model_1\"\nCONFIG[\"run_name\"] = \"model_1_baseline\"","1c55a082":"DEFAULT_MODEL = False\nif DEFAULT_MODEL: ## bulding model \n    model_input  = (  CONFIG[\"image_height\"], CONFIG[\"image_width\"], 3 )\n    model = models.Sequential()\n    model.add( layers.Conv2D ( filters = 32 , kernel_size = ( 3,3), padding =\"valid\",activation = \"relu\",use_bias = True, input_shape = model_input  ) )\n    model.add( layers.Conv2D ( filters = 32 , kernel_size = ( 4,4), padding =\"valid\",activation = \"relu\",use_bias = True ) )#, name=\"layer_02\" )\n    #model.add( layers.Conv2D ( filters = 32 , kernel_size = ( 5,5), padding =\"valid\",activation = \"relu\",use_bias = True ) )#, name=\"layer_03\")\n    #model.add( layers.Conv2D ( filters = 32 , kernel_size = ( 6,6), padding =\"valid\",activation = \"relu\",use_bias = True ) )#, name=\"layer_04\")\n    #model.add( layers.Conv2D ( filters = 32 , kernel_size = ( 7,7), padding =\"valid\",activation = \"relu\",use_bias = True ) )#, name=\"layer_04\")\n    #model.add( layers.Conv2D ( filters = 32 , kernel_size = ( 8,8), padding =\"valid\",activation = \"relu\",use_bias = True ) )#, name=\"layer_04\")\n    #model.add( layers.Conv2D ( filters = 32 , kernel_size = ( 9,9), padding =\"valid\",activation = \"relu\",use_bias = True ) )#, name=\"layer_05\" )\n    #model.add( layers.Conv2D ( filters = 32 , kernel_size = ( 12,12), padding =\"valid\",activation = \"relu\",use_bias = True ) )#, name=\"layer_05\" )\n    #model.add( layers.Conv2D ( filters = 32 , kernel_size = ( 15,15), padding =\"valid\",activation = \"relu\",use_bias = True ) )#, name=\"layer_05\" )\n    #model.add( layers.Conv2D ( filters = 32 , kernel_size = ( 15,15), padding =\"valid\",activation = \"relu\",use_bias = True ) )#, name=\"layer_05\" )\n    #model.add( layers.Conv2D ( filters = 32 , kernel_size = ( 20,20), padding =\"valid\",activation = \"relu\",use_bias = True ) )#, name=\"layer_05\" )\n    #model.add( layers.Conv2D ( filters = 32 , kernel_size = ( 25,25), padding =\"valid\",activation = \"relu\",use_bias = True ) )#, name=\"layer_05\" )\n    #model.add( layers.Conv2D ( filters = 32 , kernel_size = ( 20,20), padding =\"valid\",activation = \"relu\",use_bias = True ) )#, name=\"layer_05\" )\n    #model.add( layers.Conv2D ( filters = 32 , kernel_size = ( 25,25), padding =\"valid\",activation = \"relu\",use_bias = True ) )#, name=\"layer_05\" )\n    #model.add( layers.Conv2D ( filters = 32 , kernel_size = ( 25,25), padding =\"valid\",activation = \"relu\",use_bias = True ) )#, name=\"layer_05\" )\n    model.add( layers.Flatten () )\n    model.add( layers.Dense  ( units = 32 , use_bias = True, activation = \"relu\"))#,name =\"dense_01\")\n    model.add( layers.Dense  ( units = 16 , use_bias = True, activation = \"sigmoid\") )#,name =\"dense_02\" )\n    model.add( layers.Dense  ( units = 8 , use_bias = True, activation = \"sigmoid\") )#,name =\"dense_03\" )\n    model.add( layers.Dense  ( units = 1 , use_bias = True, activation = \"sigmoid\") )#,name =\"dense_04\" )\n\n\n\n    model.compile ( optimizer= \"adam\", \n                   loss= \"binary_crossentropy\",\n                   metrics=[tf.keras.metrics.AUC(curve='ROC')])\n\n    model.summary()","5bdff04c":"'''\n## initilisize WandB\nif DEFAULT_MODEL: ## bulding model \n    wandb_store = wandb.init( project = \"SETI_Breakthrough_Listening\",\n                            config = CONFIG,\n                            group =CONFIG[\"group\"],\n                            job_type=\"train\",\n                            name=CONFIG[\"run_name\"]\n                            )\n'''","e4b880d7":"'''\nif DEFAULT_MODEL: ## bulding model \n    model.fit( train_tf_dataset ,epochs = 10 , callbacks= [ WandbCallback(log_weights=True) ])\n\n    # Saving model\n\n    model_path = \".\/WandB\/\"+CONFIG[\"model_name\"]\n    os.makedirs(model_path,exist_ok = True )\n    model_count = len ( os.listdir( model_path )) +1\n    model.save(model_path +\"model_count\" +\".h5\" )\n\n    wandb_store.finish()\n'''","56bdccc1":"CONFIG[\"model_name\"] = \"encoder_decoder\"\nCONFIG[\"group\"] = \"encoder_decoder\"\nCONFIG[\"run_name\"] = \"encoder_decore_structure\"","c9dacea6":"class Auto_Encoder (tf.keras.Model ):\n    \n    def __init__(self, image_height , image_width, dimension ):\n        \n        super( Auto_Encoder, self ).__init__( )\n        self.model_input  = (  image_height, image_width, dimension )\n                             \n        self.encoder_model=  self.encoder_func()\n        \n        #self.encoder_out_put_shape = self.encoder_model.output_shape\n        self.decoder_model = self.decoder_func()\n    \n    def encoder_func(self ):\n        \n        model_input  = self.model_input #(  CONFIG[\"image_height\"], CONFIG[\"image_width\"], 3 )\n        encoder = models.Sequential()\n        encoder.add( layers.Conv2D ( filters = 128 , kernel_size = 2, padding =\"same\",strides = 2, activation = \"relu\",use_bias = True, input_shape = model_input  ) )\n        encoder.add( layers.Conv2D ( filters = 64 , kernel_size = 2, padding =\"same\",strides = 2,activation = \"relu\",use_bias = True ) )#, name=\"layer_02\" )\n        encoder.add( layers.Conv2D ( filters = 32 , kernel_size = 2, padding =\"same\",strides = 2,activation = \"relu\",use_bias = True ) )#, name=\"layer_03\")\n        encoder.add( layers.Conv2D ( filters = 16 , kernel_size = 2, padding =\"same\",strides = 2,activation = \"relu\",use_bias = True ) )#, name=\"layer_04\")\n        encoder.add( layers.Conv2D ( filters = 8 , kernel_size = 2, padding =\"same\",strides = 2,activation = \"relu\",use_bias = True ) )#, name=\"layer_04\")\n\n        return ( encoder )\n                             \n    def decoder_func ( self):\n         \n        decoder = models.Sequential() \n        decoder.add( layers.Conv2DTranspose ( filters = 8 , kernel_size = 2 , padding =\"valid\",activation = \"relu\",use_bias = True, input_shape = self.encoder_model.output_shape[1:]  ) )#, name=\"layer_04\")                    \n        decoder.add( layers.Conv2DTranspose ( filters = 16 , kernel_size = 2, padding =\"valid\",activation = \"relu\",use_bias = True ) )#, name=\"layer_03\")\n        decoder.add( layers.Conv2DTranspose ( filters = 32 , kernel_size = 2, padding =\"valid\",activation = \"relu\",use_bias = True ) )#, name=\"layer_02\" )\n        decoder.add( layers.Conv2DTranspose ( filters = 64 , kernel_size = 2, padding =\"valid\",activation = \"relu\",use_bias = True) )\n        decoder.add( layers.Conv2DTranspose ( filters = 128 , kernel_size =2, padding =\"valid\",activation = \"relu\",use_bias = True) )\n        decoder.add( layers.Conv2DTranspose ( filters = 3 , kernel_size = 223, padding =\"valid\",activation = \"relu\",use_bias = True) )\n        \n        return( decoder )\n    \n    def call(self, x):\n        encoded = self.encoder_model(x)\n        decoded = self.decoder_model(encoded)\n        return decoded","8c14a670":"model_input  =   ( CONFIG[\"image_height\"], CONFIG[\"image_width\"], 6) \nencoder = models.Sequential()\nencoder.add( layers.Conv2D ( filters = 128 , kernel_size = 2, padding =\"same\",strides = 1, activation = \"relu\",use_bias = True, input_shape = model_input  ) )\nencoder.add( layers.Conv2D ( filters = 64 , kernel_size = 3, padding =\"same\",strides = 2,activation = \"relu\",use_bias = True ) )#, name=\"layer_02\" )\nencoder.add( layers.Conv2D ( filters = 32 , kernel_size = 4, padding =\"same\",strides = 2,activation = \"relu\",use_bias = True ) )#, name=\"layer_03\")\nencoder.add( layers.Conv2D ( filters = 16 , kernel_size = 5, padding =\"same\",strides = 2,activation = \"relu\",use_bias = True ) )#, name=\"layer_04\")\nencoder.add( layers.Conv2D ( filters = 8 , kernel_size = 6, padding =\"same\",strides = 2,activation = \"relu\",use_bias = True ) )#, name=\"layer_04\")\n\n#decoder = models.Sequential()\nencoder.add( layers.Conv2DTranspose ( filters = 8 , kernel_size = 6 , padding =\"same\",strides = 2,activation = \"relu\",use_bias = True ) )#, name=\"layer_04\")                    \nencoder.add( layers.Conv2DTranspose ( filters = 16 , kernel_size = 5, padding =\"same\",strides = 2,activation = \"relu\",use_bias = True ) )#, name=\"layer_03\")\nencoder.add( layers.Conv2DTranspose ( filters = 32 , kernel_size = 4, padding =\"same\",strides = 2,activation = \"relu\",use_bias = True ) )#, name=\"layer_02\" )\nencoder.add( layers.Conv2DTranspose ( filters = 64 , kernel_size = 3, padding =\"same\",strides = 2,activation = \"relu\",use_bias = True) )\nencoder.add( layers.Conv2DTranspose ( filters = 128 , kernel_size =2, padding =\"same\",strides = 1,activation = \"relu\",use_bias = True) )\nencoder.add( layers.Conv2DTranspose ( filters = 6 , kernel_size = 2, padding =\"same\",strides = 1,activation = \"relu\",use_bias = True) )\n        \n\nencoder.compile ( optimizer= \"adam\", \n               loss= \"binary_crossentropy\",\n               metrics=[tf.keras.metrics.AUC(curve='ROC')])\n\nencoder.summary()","fe753a44":"'''\nmodel_input  =   ( CONFIG[\"image_height\"], CONFIG[\"image_width\"], 3 )\nencoder = models.Sequential()\nencoder.add( layers.Conv2D ( filters = 128 , kernel_size = 2, padding =\"same\",strides = 1, activation = \"relu\",use_bias = True, input_shape = model_input  ) )\nencoder.add( layers.Conv2D ( filters = 64 , kernel_size = 3, padding =\"same\",strides = 2,activation = \"relu\",use_bias = True ) )#, name=\"layer_02\" )\nencoder.add( layers.Conv2D ( filters = 32 , kernel_size = 4, padding =\"same\",strides = 2,activation = \"relu\",use_bias = True ) )#, name=\"layer_03\")\nencoder.add( layers.Conv2D ( filters = 16 , kernel_size = 5, padding =\"same\",strides = 2,activation = \"relu\",use_bias = True ) )#, name=\"layer_04\")\nencoder.add( layers.Conv2D ( filters = 8 , kernel_size = 6, padding =\"same\",strides = 2,activation = \"relu\",use_bias = True ) )#, name=\"layer_04\")\n\n#decoder = models.Sequential()\nencoder.add( layers.Conv2DTranspose ( filters = 8 , kernel_size = 6 , padding =\"same\",strides = 2,activation = \"relu\",use_bias = True ) )#, name=\"layer_04\")                    \nencoder.add( layers.Conv2DTranspose ( filters = 16 , kernel_size = 5, padding =\"same\",strides = 2,activation = \"relu\",use_bias = True ) )#, name=\"layer_03\")\nencoder.add( layers.Conv2DTranspose ( filters = 32 , kernel_size = 4, padding =\"same\",strides = 2,activation = \"relu\",use_bias = True ) )#, name=\"layer_02\" )\nencoder.add( layers.Conv2DTranspose ( filters = 64 , kernel_size = 3, padding =\"same\",strides = 2,activation = \"relu\",use_bias = True) )\nencoder.add( layers.Conv2DTranspose ( filters = 128 , kernel_size =2, padding =\"same\",strides = 1,activation = \"relu\",use_bias = True) )\nencoder.add( layers.Conv2DTranspose ( filters = 3 , kernel_size = 10, padding =\"same\",strides = 1,activation = \"relu\",use_bias = True) )\n        \n\nencoder.compile ( optimizer= \"adam\", \n               loss= \"binary_crossentropy\",\n               metrics=[tf.keras.metrics.AUC(curve='ROC')])\n\nencoder.summary()\n'''","992733c0":"CONFIG[\"batch_size\"] = 50\ndef resize_image( df_dict ):\n    \n    [image, ] =  tf.py_function( read_numpy_data,  [ df_dict[\"path\"] ], [tf.float32] )\n    image.set_shape(( 273, 256, 3 ) )\n    image = tf.image.resize( image , ( CONFIG[\"image_height\"],  CONFIG[\"image_width\"] ) )\n    label =  df_dict[\"target\"]#.reshape((-1,1))\n    label = tf.cast( label, tf.float32 ), (-1,1) \n  \n    return  (image,image)\n            \ndef create_tf_dataset( dataframe ):\n    \n    tf_dataset = tf.data.Dataset.from_tensor_slices ( dict ( dataframe[ [\"path\",\"target\"] ] ) ) \n    tf_dataset = ( tf_dataset\n                  .shuffle (1024)\n                  .map( resize_image,num_parallel_calls= tf.data.AUTOTUNE)\n                  .batch( CONFIG[\"batch_size\"] )\n                  .prefetch(tf.data.AUTOTUNE)\n                  \n                 )\n    \n    return ( tf_dataset )\n\n            \ntrain_tf_dataset = create_tf_dataset ( train_df.head(200) )\ntest_tf_dataset = create_tf_dataset ( test_df.head(50) )","7ab4d7db":"train_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\ntrain_generator1 = train_datagen.flow_from_directory(\n        \"..\/input\/seti-breakthrough-listen\/train\/\",\n        target_size=(224, 224),\n        batch_size=32,\n        class_mode=None,\n        shuffle=True)","cd144c76":"'''\n\nCONFIG[\"model_name\"] = \"encoder_decoder\"\nCONFIG[\"group\"] = \"encoder_decoder\"\nCONFIG[\"run_name\"] = \"encoder_decore_structure\"\n\nwandb_store = wandb.init( project = \"SETI_Breakthrough_Listening\",\n                            config = CONFIG,\n                            group =CONFIG[\"group\"],\n                            job_type=\"train\",\n                            name=CONFIG[\"run_name\"]\n                            )\n\nencoder.fit( train_generator1,train_generator1 ,epochs = 10 , callbacks= [ WandbCallback(log_weights=True) ])\n\n# Saving model\n\nmodel_path = \".\/WandB\/\"+CONFIG[\"model_name\"]\nos.makedirs(model_path,exist_ok = True )\nmodel_count = len ( os.listdir( model_path )) +1\nmodel.save(model_path +\"model_count\" +\".h5\" )\n\nwandb_store.finish()\n'''","28ad9def":"from tensorflow.keras import utils\n#import numpy as np","9236c0d1":"class DataGenerator ( utils.Sequence) :\n    \n    def __init__ (self, data_list, batch_size, #required_channels = [1, 3, 5 ] , \n                  image_height = 224, image_width = 224, \n                  shuffle = True,num_channel = 3  ):\n        \n        self.data_list = data_list\n        self.batch_size = batch_size\n        #self.channels_required = required_channels\n        self.image_width = image_width\n        self.image_height = image_height\n        self.shuffle = True \n        self.num_channel =num_channel \n        self.is_there_left_over = False # this to check is there any left images after creating batch slices \n        \n        self.number_of_batches = 0\n        \n        \n    def on_epoch_end(self):\n        self.indices = np.arrange( len(self.data_list ))\n        \n        if self.shuffle :\n            np.random.shuffle( self.indices )\n    \n    def __len__(self ):\n        \n        number_of_batch =int ( len( self.data_list)\/ self.batch_size )\n        \n        ''' below if loop to check for remaining data after slicing with respectto batch'''\n        if number_of_batch * self.batch_size !=len( self.data_list):\n            number_of_batch = number_of_batch +1\n            self.is_there_left_over = True\n        \n        self.number_of_batches = number_of_batch\n        \n        return ( number_of_batch  )\n    \n    def __getitem__(self, idx = 0  ):\n        \n        x = np.empty( ( self.batch_size, self.image, self.num_channel ) )\n        \n        if self.is_there_left_over & (idx == (self.number_of_batches - 1 ) ) :\n            \n            indices = self.indices [ idx * self.batch_size :  ]\n            \n        else:\n            \n            indices = self.indices [ idx * self.batch_size : ( idx +1 )* self.batch_size ]\n        \n        for i, path in enumerate ( indices):\n            x[i] = self.read_image( path )\n        \n        return x \n            \n    def read_image ( self, path ):\n        \n        file_name=file_name_result\n        data = np.load( path ).astype( np.float32 )    \n        data = np.dstack( ( data[0], data[2], data[4] ) )\n        \n        return ( data )","0652c8e5":"from tensorflow.keras.preprocessing.image import ImageDataGenerator","4822f582":"def read_stack_image (  path ):\n        \n        file_name=file_name_result\n        data = np.load( path ).astype( np.float32 )    \n        data = np.dstack( ( data[0], data[2], data[4] ) )\n        \n        return ( data )","814e00d6":"data_gen = ImageDataGenerator( preprocessing_function =  read_stack_image )\ntrain_x = data_gen.flow_from_directory ( \"..\/input\/seti-breakthrough-listen\/train\/\", target_size = ( 224,224), \n                                         batch_size = 20, #color_mode =\"rgb\",\n                                        shuffle = True, classes =[\"test\"],\n                                       )","f4ffd601":"train_x.next()","39ef24b1":"train_x = DataGenerator( data_list = train_df[\"path\"], batch_size = 20 )\ntest_x = DataGenerator( data_list = test_df[\"path\"], batch_size = 20 )","7abf1eaa":"'''\n\nCONFIG[\"model_name\"] = \"encoder_decoder\"\nCONFIG[\"group\"] = \"encoder_decoder\"\nCONFIG[\"run_name\"] = \"encoder_decore_structure\"\n\nwandb_store = wandb.init( project = \"SETI_Breakthrough_Listening\",\n                            config = CONFIG,\n                            group =CONFIG[\"group\"],\n                            job_type=\"train\",\n                            name=CONFIG[\"run_name\"]\n                            )\n\nencoder.fit( train_x ,epochs = 10 , callbacks= [ WandbCallback(log_weights=True) ])\n\n# Saving model\n\nmodel_path = \".\/WandB\/\"+CONFIG[\"model_name\"]\nos.makedirs(model_path,exist_ok = True )\nmodel_count = len ( os.listdir( model_path )) +1\nmodel.save(model_path +\"model_count\" +\".h5\" )\n\nwandb_store.finish()\n'''","4d882430":"#train_df[\"path\"]\nimport tensorflow as tf","7b9d9988":"AUTO = tf.data.experimental.AUTOTUNE\ntrain_dataset = tf.data.TFRecordDataset ( list (train_df[\"path\"] ), num_parallel_reads= AUTO )","05d95bf9":"list ( train_df[\"path\"] )[:10]","a1b1f21f":"def read_stack_image (  path ):\n        \n        #file_name=file_name_result\n        #data = np.load( path ).astype( np.float32 )    \n        #data = np.dstack( ( data[0], data[2], data[4] ) )\n        \n        return ( []  )","4e3ca250":"for raw_record in train_dataset.take(10):\n  print(repr(raw_record))","0f8d3b7e":"train_dataset = train_dataset.map( read_stack_image, num_parallel_calls= AUTO )","9fd23fb5":"from keras.preprocessing.image import ImageDataGenerator\n\n\n\ntrain_datagen = ImageDataGenerator()","eced3629":"train_datagen.flow_from_directory( \"..\/input\/seti-breakthrough-listen\/train\/\",  )","678949f8":"from tf.keras.","7595e85a":"class DataGenerator ( utils.Sequence) :\n    \n    def __init__ (self, data_list, batch_size, #required_channels = [1, 3, 5 ] , \n                  image_height = 224, image_width = 224, \n                  shuffle = True,num_channel = 3  ):\n        \n        self.data_list = data_list\n        self.batch_size = batch_size\n        #self.channels_required = required_channels\n        self.image_width = image_width\n        self.image_height = image_height\n        self.shuffle = True \n        self.num_channel =num_channel \n        self.is_there_left_over = False # this to check is there any left images after creating batch slices \n        \n        self.number_of_batches = 0\n        \n        \n    def on_epoch_end(self):\n        self.indices = np.arrange( len(self.data_list ))\n        \n        if self.shuffle :\n            np.random.shuffle( self.indices )\n    \n    def __len__(self ):\n        \n        number_of_batch =int ( len( self.data_list)\/ self.batch_size )\n        \n        ''' below if loop to check for remaining data after slicing with respectto batch'''\n        if number_of_batch * self.batch_size !=len( self.data_list):\n            number_of_batch = number_of_batch +1\n            self.is_there_left_over = True\n        \n        self.number_of_batches = number_of_batch\n        \n        return ( number_of_batch  )\n    \n    def __getitem__(self, idx = 0  ):\n        \n        x = np.empty( ( self.batch_size, self.image, self.num_channel ) )\n        \n        if self.is_there_left_over & (idx == (self.number_of_batches - 1 ) ) :\n            \n            indices = self.indices [ idx * self.batch_size :  ]\n            \n        else:\n            \n            indices = self.indices [ idx * self.batch_size : ( idx +1 )* self.batch_size ]\n        \n        for i, path in enumerate ( indices):\n            x[i] = self.read_image( path )\n        \n        return x \n            \n    def read_image ( self, path ):\n        \n        file_name=file_name_result\n        data = np.load( path ).astype( np.float32 )    \n        data = np.dstack( ( data[0], data[2], data[4] ) )\n        \n        return ( data )","77955525":"channels ABACAD. select only cannel B,C &D","c35f458c":"# UseEfficient net to start classification ","9ef84f45":"* Creating Tes, Train & validation Tensor data ","2331f927":"# Checking distribution of Target variable between train , test & validation split ","0a923ba3":"*  Reading data set to pandas dataframe, and updating file path ","e2c14f59":"# Got this facinating link to for data generator.\nhttps:\/\/stackoverflow.com\/questions\/64356769\/tensorflow2-x-custom-data-generator-with-multiprocessing\n\nmy method was taking 160s, were as method in the above link takes 37s. close 6 time faster wow.\n\nFunction implemented with above method is with \"data_generator_Rev_01\"","f5af4d03":"# * Creating generator for efficient net classification ","7fe91bb9":"# In this Notebook, you have 3 section.\n# 1. with Efficient Net archtecture --> to learn tranfer learning\n*     1. With Transfer Learning I am able to achieve only 90% Accuracy.\n*     2. Try to handle imbalanced data using class weight\n\n# 2. Tried to build own model --> I couldn't get all things good. have tryafter efficient net\n# 3. Tried to create Encoder + Decoder Archtecture. But result is very bad, have to work on model building \n# Note, code doesn't have submission part yet ","50d3c9a1":"# 1. Using Efficient Net to classifcation","2d3c465b":"# Data is already normailsed to have mean of 0, not doing any preprocessing on the each images.\n# Will start wriing generator function for Encoders. Because of below reason\n* Avalable generator wont work for Autoencoder, tried few they didn't work. Found one code f using TFrecod, it was to heay for me.\n* So i took one of generator code I got from Upgrad assignment,","1dceca3f":"* 1. All required libraries ","500466f2":"* Each numpy file has 6 channels, not all channels are use full found few discussion thread mentioning using only 3 channels.\n* I'll be exploring with 3 channels first, later I can expand to 6 channels\n* discussion therad : \n* https:\/\/www.kaggle.com\/c\/seti-breakthrough-listen\/discussion\/239173\n* https:\/\/www.kaggle.com\/c\/seti-breakthrough-listen\/discussion\/238241","7e528fe6":"* Reading Data & Visualising  images ","fce49302":"# Creating Data generator & takingonly axis 0,2,4 from numpy arrya of 6  channels.\n# Channel 1,3,5 are signal coming from man made sources","12658d20":"***** random file exists which confirms file path constructio correct we can preceeed to other operations","729aaabd":"* Upading each image path","c426c8f1":"#  Lets visualise random images ","e18520ae":"* Visualizing the data for random samples after resizingdata","c04a57ed":"***** Checking is file path constructed is correct or not by verifying file path using pathexist function"}}