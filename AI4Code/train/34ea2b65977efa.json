{"cell_type":{"277f895c":"code","4ab14f08":"code","9e709492":"code","ca906b4f":"code","5898b72a":"code","a1ec7ed0":"code","24c3a047":"code","a12b98a5":"code","ae56deb8":"code","4ec96312":"code","3aba25bc":"code","d3dbffaa":"code","19227298":"code","63087e7c":"markdown","c85eafe4":"markdown","ba9003fe":"markdown","cadf01fe":"markdown","9a255a01":"markdown","2a2b1493":"markdown","6f26afe0":"markdown","34d25a26":"markdown","9704745f":"markdown","7d7e9f6a":"markdown","bc9f8742":"markdown","89e0c121":"markdown","7fcdbb7c":"markdown","13854ecc":"markdown"},"source":{"277f895c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","4ab14f08":"# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","9e709492":"dataset = pd.read_csv(\"..\/input\/Churn_Modelling.csv\",encoding = \"ISO-8859-1\")\ndataset.head()","ca906b4f":"## Remove columns \nX = dataset.iloc[:, 3:13].values\ny = dataset.iloc[:, 13].values\nX","5898b72a":"# Encoding categorical data\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nlabelencoder_X_1 = LabelEncoder()\nX[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\nlabelencoder_X_2 = LabelEncoder()\nX[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\nonehotencoder = OneHotEncoder(categorical_features = [1])\nX = onehotencoder.fit_transform(X).toarray()\nX = X[:, 1:]","a1ec7ed0":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n","24c3a047":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n","a12b98a5":"# Importing the Keras libraries and packages\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n","ae56deb8":"# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n\n# Adding the second hidden layer\nclassifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n\n# Adding the output layer\nclassifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n# Compiling the ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n","4ec96312":"# Fitting the ANN to the Training set\nclassifier.fit(X_train, y_train, batch_size = 10, epochs = 100)\n","3aba25bc":"# Predicting the Test set results\ny_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.5)\ny_pred\n","d3dbffaa":"# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)","19227298":"cm","63087e7c":"With these notations, the activation** alj of the jth neuron in the lth layer is related to the activations in the (l\u22121)th** layer by the equation \n<h1>**alj=\u03c3(\u2211kwljkal\u22121k+blj)**<h1>\n\n**where the sum is over all neurons k in the (l\u22121)th layer. To rewrite this expression in a matrix form we define a weight matrix wl for each layer, l. The entries of the weight matrix wl are just the weights connecting to the lth layer of neurons, that is, the entry in the jth row and kth column is wljk. Similarly, for each layer l we define a bias vector, bl. You can probably guess how this works - the components of the bias vector are just the values blj, one component for each neuron in the lth layer. And finally, we define an activation vector al whose components are the activations alj**.","c85eafe4":"Remove columns which are not used full for us.\nremove first three columns surname, Rownumber, CustoomerId these are the unnecessary data","ba9003fe":"**<h1> Implemantation <\/h1>**","cadf01fe":"### Keras is a minimalist Python library for deep learning that can run on top of Theano or TensorFlow.","9a255a01":"## How ANN Works\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/0*0mia7BQKjUAuXeqZ.jpeg\" width=\"900\"><\/img>\n### Look carefully at the diagram given above.Information (data) is travelling or going from input layer to hidden layers and then to output layer.","2a2b1493":"<h1>Backpropogation<\/h1>\nThe backpropagation algorithm was originally introduced in the 1970s, but its importance wasn't fully appreciated until a famous 1986 paper by David Rumelhart, Geoffrey Hinton, and Ronald Williams. That paper describes several neural networks where backpropagation works far faster than earlier approaches to learning, making it possible to use neural nets to solve problems which had previously been insoluble. Today, the backpropagation algorithm is the workhorse of learning in neural networks.\n\nAt the heart of backpropagation is an expression for the partial derivative \u2202C\/\u2202w of the cost function C with respect to any weight w (or bias b) in the network. The expression tells us how quickly the cost changes when we change the weights and biases. And while the expression is somewhat complex, it also has a beauty to it, with each element having a natural, intuitive interpretation. And so backpropagation isn't just a fast algorithm for learning. It actually gives us detailed insights into how changing the weights and biases changes the overall behaviour of the network. That's well worth studying in detail.\n\nLet's begin with a notation which lets us refer to weights in the network in an unambiguous way. We'll use wljk to denote the weight for the connection from the **kth** neuron in the** (l\u22121)th** layer to the** jth **neuron in the** lth** layer. So, for example, the diagram below shows the weight on a connection from the fourth neuron in the second layer to the second neuron in the third layer of a network: \n<img src=\"http:\/\/neuralnetworksanddeeplearning.com\/images\/tikz16.png\" width=\"500\"><\/img>\n\nThis notation is cumbersome at first, and it does take some work to master. But with a little effort you'll find the notation becomes easy and natural. One quirk of the notation is the ordering of the** j **and** k** indices. You might think that it makes more sense to use **j **to refer to the input neuron, and** k **to the output neuron, not vice versa, as is actually done. I'll explain the reason for this quirk below.\n\nWe use a similar notation for the network's biases and activations. Explicitly, we use blj\nfor the bias of the **jth neuron in the lth **layer. And we use alj for the activation of the **jth neuron in the lth** layer. The following diagram shows examples of these notations in use: \n<img src=\"http:\/\/neuralnetworksanddeeplearning.com\/images\/tikz17.png\">","6f26afe0":"## Task Overview\nthis dataset is the data of a bank, who want to detect which are more likely to live the bank and which customer will set in bank.\n","34d25a26":"### After data preprocessing it's time for build ANN(Artificial Neaural Network)","9704745f":"<h1>Most popular types of Activation functions<h1>\n<h3>1.Sigmoid or Logistic<\/h3>\n<h3>2.Tanh\u200a\u2014\u200aHyperbolic tangent<\/h3>\n<h3>3.ReLu -Rectified linear unit<\/h3>","7d7e9f6a":"## Artificial Neural networks\nArtificial Neural networks (ANN) or neural networks are computational algorithms.\nA neural network is a machine learning algorithm based on the model of a human neuron. The human brain consists of millions of neurons. It sends and process signals in the form of electrical and chemical signals. These neurons are connected with a special structure known as synapses. Synapses allow neurons to pass signals. From large numbers of simulated neurons neural networks forms.\nAn Artificial Neural Network is an information processing technique. It works like the way human brain processes information. ANN includes a large number of connected processing units that work together to process information. They also generate meaningful results from it.\nWe can apply Neural network not only for classification. It can also apply for regression of continuous target attributes.\nNeural networks find great application in data mining used in sectors. For example economics, forensics, etc and for pattern recognition. It can be also used for data classification in a large amount of data after careful training.","bc9f8742":"## Three types of layers in Artificial Neural network\n\nArtificial Neural network is typically organized in layers. Layers are being made up of many interconnected \u2018nodes\u2019 which contain an \u2018activation function\u2019. A neural network may contain the following 3 layers:\n\n### a. Input layer\nThe purpose of the input layer is to receive as input the values of the explanatory attributes for each observation. Usually, the number of input nodes in an input layer is equal to the number of explanatory variables. \u2018input layer\u2019 presents the patterns to the network, which communicates to one or more \u2018hidden layers\u2019.\nThe nodes of the input layer are passive, meaning they do not change the data. They receive a single value on their input and duplicate the value to their many outputs. From the input layer, it duplicates each value and sent to all the hidden nodes.\n\n### b. Hidden layer\nThe Hidden layers apply given transformations to the input values inside the network. In this, incoming arcs that go from other hidden nodes or from input nodes connected to each node. It connects with outgoing arcs to output nodes or to other hidden nodes. In hidden layer, the actual processing is done via a system of weighted \u2018connections\u2019. There may be one or more hidden layers. The values entering a hidden node multiplied by weights, a set of predetermined numbers stored in the program. The weighted inputs are then added to produce a single number.\n\n### c. Output layer\nThe hidden layers then link to an \u2018output layer\u2018. Output layer receives connections from hidden layers or from input layer. It returns an output value that corresponds to the prediction of the response variable. In classification problems, there is usually only one output node. The active nodes of the output layer combine and change the data to produce the output values.\n\nThe ability of the neural network to provide useful data manipulation lies in the proper selection of the weights. This is different from conventional information processing.","89e0c121":"Deal with categorical data \nColumns \"Geography\" and Gender Contain categorical data ","7fcdbb7c":"for more study about Activation function visit this link\nhttps:\/\/towardsdatascience.com\/activation-functions-and-its-types-which-is-better-a9a5310cc8f","13854ecc":"<h1>Activation Function<\/h1>\nActivation functions are really important for a Artificial Neural Network to learn and make sense of something really complicated and Non-linear complex functional mappings between the inputs and response variable.They introduce non-linear properties to our Network.Their main purpose is to convert a input signal of a node in a A-NN to an output signal. That output signal now is used as a input in the next layer in the stack."}}