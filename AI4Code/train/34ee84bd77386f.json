{"cell_type":{"c7559140":"code","63582e23":"code","b9105ec3":"code","39714eaa":"code","ade43a7b":"code","77afc8f7":"code","eba13aaa":"code","4616ef47":"code","bdb46e3f":"code","688ee7ae":"code","30e5d8e2":"code","70ed15e2":"code","98ce7036":"code","1a47f05b":"code","126d4907":"code","f79f32e5":"code","b2b4f56e":"code","eadd315e":"markdown","851ac649":"markdown","db4d4e7b":"markdown","f57ef055":"markdown","80dac9a3":"markdown","bd33b7a1":"markdown","902f79d0":"markdown","8c17045c":"markdown","69b5801c":"markdown"},"source":{"c7559140":"import torch\nimport numpy as np\nimport random\nimport torchvision.datasets\nimport torchvision.transforms\nimport torchvision.transforms.functional\nimport matplotlib.pyplot as plt\nimport copy\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n#random seed between 0 and 1000\nseed = int(random.random()*1000)\n\nseed=116\n\n#here we set up our seed\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\n#to reproduce same result over and over\n#also we lose some performance in gpu if it set to true\ntorch.backends.cudnn.deterministic = True\n\nseed","63582e23":"transforms_train = torchvision.transforms.Compose([\n    #torchvision.transforms.RandomResizedCrop(size(28, 28), , scale=(0.75, 1.0)),\n    torchvision.transforms.RandomPerspective(distortion_scale=0.6, p=0.6),\n    #torchvision.transforms.RandomRotation(degrees=15),\n    torchvision.transforms.ToTensor()\n])\n\ntransforms_val = torchvision.transforms.Compose([\n    #torchvision.transforms.RandomResizedCrop(size(28, 28), , scale=(0.75, 1.0)),\n    #torchvision.transforms.RandomPerspective(distortion_scale=0.6, p=0.6),\n    #torchvision.transforms.RandomRotation(degrees=15),\n    torchvision.transforms.ToTensor()\n])\n\nQMNIST_train = torchvision.datasets.QMNIST('.\/', what='train', download=True, compat=True, transform=transforms_train)\nQMNIST_val = torchvision.datasets.QMNIST('..\/', what='test10k', download=True, compat=True, transform=transforms_val, train=False)","b9105ec3":"batch_size = 100\n\ntrain_dataloader = torch.utils.data.DataLoader(\n    QMNIST_train, batch_size=batch_size, shuffle=True, num_workers=batch_size, drop_last=True\n)\n\nval_dataloader = torch.utils.data.DataLoader(\n    QMNIST_val, batch_size=batch_size, shuffle=True, num_workers=batch_size, drop_last=True\n)","39714eaa":"import matplotlib.pyplot as plt\n\ndef show_line_images_with_labels(images, labels):\n    fig, axs = plt.subplots(1, len(images), figsize=(15, 5), constrained_layout=True)\n    for i in range(len(images)):\n        img = images[i].reshape(28, 28).numpy()\n        axs[i].imshow(img)\n        axs[i].text(0.95, 0.01, f\"{int(labels[i])}\",\n            verticalalignment='bottom', horizontalalignment='right',\n            transform=axs[i].transAxes,\n            color='white', fontsize=25)\n        axs[i].axis('off')\ndef show_grid_images_with_labels(images, labels):\n    counter = 10\n    for step in range(0, len(images), 10):\n        show_line_images_with_labels(images[step:step+counter], labels[step:step+counter])","ade43a7b":"inputs, labels = next(iter(train_dataloader))\nshow_grid_images_with_labels(inputs, labels)","77afc8f7":"class MNISTnet(torch.nn.Module):\n    def __init__(self, n_hidden_neurons=32, p=0.22):\n        super(MNISTnet, self).__init__()\n        self.seq0 = torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels=1, out_channels=n_hidden_neurons, kernel_size=3, padding=1),\n            torch.nn.LeakyReLU(),\n            torch.nn.BatchNorm2d(n_hidden_neurons),\n            torch.nn.Dropout(p),\n            torch.nn.Conv2d(in_channels=n_hidden_neurons, out_channels=n_hidden_neurons, kernel_size=3, padding=1),\n            torch.nn.LeakyReLU(),\n            torch.nn.BatchNorm2d(n_hidden_neurons),\n        )\n        self.maxpool1 = torch.nn.MaxPool2d(2, 2)\n        self.seq1 = torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels=n_hidden_neurons, out_channels=2*n_hidden_neurons, kernel_size=3, padding=0),\n            torch.nn.LeakyReLU(),\n            torch.nn.BatchNorm2d(2*n_hidden_neurons),\n            torch.nn.Conv2d(in_channels=2*n_hidden_neurons, out_channels=4*n_hidden_neurons, kernel_size=3, padding=0),\n            torch.nn.LeakyReLU(),\n            torch.nn.BatchNorm2d(4*n_hidden_neurons),\n        )\n        self.maxpool2 = torch.nn.MaxPool2d(2, 2)\n        self.seq2 = torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels=4*n_hidden_neurons, out_channels=2*n_hidden_neurons, kernel_size=3, padding=1),\n            torch.nn.LeakyReLU(),\n            torch.nn.BatchNorm2d(2*n_hidden_neurons),\n            torch.nn.Dropout(p),\n            torch.nn.Conv2d(in_channels=2*n_hidden_neurons, out_channels=2*n_hidden_neurons, kernel_size=3, padding=1),\n            torch.nn.LeakyReLU(),\n            torch.nn.BatchNorm2d(2*n_hidden_neurons),\n        )\n        self.avgpool1 = torch.nn.AvgPool2d(kernel_size=(5, 5))\n        \n        self.fc1 = torch.nn.Linear(2*n_hidden_neurons, 10)\n    def forward(self, x):\n        x = self.seq0(x)\n        x = self.maxpool1(x)\n        x = self.seq1(x)\n        x = self.maxpool2(x)\n        x = self.seq2(x)\n        x = self.avgpool1(x)\n        x = x.reshape(x.shape[0], x.shape[1])\n        x = self.fc1(x)\n        return x","eba13aaa":"def acc_n_loss_func(preds, labels):\n    f = torch.nn.CrossEntropyLoss()\n    loss = f(preds, labels)\n    accuracy = (preds.argmax(dim=1) == labels).float().mean()\n    return accuracy, loss","4616ef47":"def train_model(model, accnlossfunc, optimizer, scheduler, num_epochs):\n    try:\n        best_model_wts = copy.deepcopy(model.state_dict())\n        best_acc = 0.0\n        \n        val_accuracy_history = []\n        val_loss_history = []\n        \n        for epoch in range(num_epochs):\n            for phase in ['train', 'val']:\n                if phase == 'train':\n                    dataloader = train_dataloader\n                    model.train()  # Set model to training mode\n                else:\n                    dataloader = val_dataloader\n                    model.eval()   # Set model to evaluate mode\n                running_loss = 0.\n                running_acc = 0\n                # Iterate over data.\n                for i, (inputs, labels) in enumerate(dataloader):\n                    inputs = inputs.to(device)\n                    labels = labels.type(torch.LongTensor).to(device)\n                    labels = labels.flatten()\n                    optimizer.zero_grad()\n                    # forward and backward\n                    with torch.set_grad_enabled(phase == 'train'):\n                        preds = model(inputs)\n                        acc, loss_value = accnlossfunc(preds, labels)\n                        # backward + optimize only if in training phase\n                        if phase == 'train':\n                            loss_value.backward()\n                            optimizer.step() \n                            scheduler.step()\n\n                    # statistics\n                    running_loss += loss_value.item()\n                    running_acc += acc.item()\n                    print(\"epoch: {}\/{} nested {}\/{} {}  - loss: {:.4f} - acc: {:.4f}\".format(epoch, num_epochs, (i+1), len(dataloader), phase, running_loss\/(i+1), running_acc\/(i+1)), end='\\r')\n                epoch_loss = running_loss \/ len(dataloader)\n                epoch_acc = running_acc \/ len(dataloader)\n                if phase == 'val' and epoch_acc > best_acc:\n                    best_acc = epoch_acc\n                    best_model_wts = copy.deepcopy(model.state_dict())\n                    val_loss_history.append(epoch_loss)\n                    val_accuracy_history.append(epoch_acc)\n                print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc), flush=True)\n    except (KeyboardInterrupt or RuntimeError) as e:\n        model.load_state_dict(best_model_wts)\n        print(f\"Returning model saved with best accuracy:{best_acc}\")\n        return model, val_loss_history, val_accuracy_history\n        \n    model.load_state_dict(best_model_wts)\n    print(f\"Returning model saved with best accuracy:{best_acc}\")\n    return model, val_loss_history, val_accuracy_history","bdb46e3f":"model = MNISTnet(n_hidden_neurons=64)\nmodel = model.to(device)\nloss = acc_n_loss_func\noptimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n# Decay LR by a factor of 0.1 every 88 epochs\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2240, gamma=0.7)","688ee7ae":"model, val_loss_history, val_accuracy_history = train_model(model, loss, optimizer, scheduler, num_epochs=40)","30e5d8e2":"from tqdm import tqdm\nimport pandas as pd\n\ntest_csv = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\ntest_data = torch.zeros([len(test_csv), 1, 28, 28])\n\nfor i in range(len(test_csv)):\n   test_data[i, 0] = (torch.tensor(test_csv.iloc[i, :]).reshape(28, 28))\/255.0","70ed15e2":"plt.plot(val_loss_history)\nplt.xlabel('epoch')\nplt.ylabel('loss_value')","98ce7036":"plt.plot(val_accuracy_history)\nplt.xlabel('epoch')\nplt.ylabel('accuracy')","1a47f05b":"test_preds = model(test_data[:100].to(device))\nlabels = test_preds.argmax(dim=1)\nshow_grid_images_with_labels(test_data[:100], labels)","126d4907":"result = torch.zeros(len(test_data)).to(device)\n\nfor index in range(0, len(test_data), batch_size):\n    test_preds = model(test_data[index:index+batch_size].to(device))\n    result[index:index+batch_size] = test_preds.argmax(dim=1)\nresult = result.type(torch.LongTensor)","f79f32e5":"submission = pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')\nsubmission['Label'] = result.data.cpu()","b2b4f56e":"submission.to_csv('submission.csv', index=False)","eadd315e":"# LET'S HAVE A LOOK AT ACCURACY AND LOSS HISTORY","851ac649":"# TEST PREDICTION AND SUBMISSION","db4d4e7b":"# DATA PREPARATION","f57ef055":"![image.png](attachment:51b481cb-4640-40bd-9c55-b2586fa37a0b.png)","80dac9a3":"# TRAINING SECTION","bd33b7a1":"# IMPLEMENTATION OF NEURAL NETWORK","902f79d0":"# PREVIEW OUR DATASETS","8c17045c":"# LOSS AND ACCURACY FUNCTION","69b5801c":"# TEST DATA PREPARATION"}}