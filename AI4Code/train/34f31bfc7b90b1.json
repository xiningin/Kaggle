{"cell_type":{"24dd7cd2":"code","ec640f32":"code","80070aeb":"code","d7a3aa51":"code","9e0ef479":"code","0719eede":"code","67d3de6d":"code","2656df2c":"code","0dc85da8":"code","d9fae862":"code","8cf91d34":"code","39a874f0":"code","680e6c39":"code","db2d0d36":"code","ed613d9b":"code","60224698":"code","e089934e":"code","b677c45f":"code","f3281f7a":"code","757043f8":"code","a702414b":"code","29932291":"code","20a17059":"code","8a883ec5":"code","79cd32e1":"code","48b8e9c3":"code","3be9b5a7":"code","eb166284":"code","1c5e1118":"code","65bcbf95":"code","f7fd32b4":"code","40907c74":"code","3fc48abe":"code","2d2b41f4":"code","eca889ea":"code","94018422":"code","6a70a759":"code","588c4d3f":"code","a4494004":"code","b3691b11":"code","8e26e36f":"code","82bf337b":"code","a9cfc36f":"code","06de4947":"code","14d2daf2":"code","b0c234a0":"code","cb1c2830":"code","d4cdbd69":"markdown","443bb6a0":"markdown"},"source":{"24dd7cd2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ec640f32":"train = pd.read_csv('\/kaggle\/input\/learn-together\/train.csv')","80070aeb":"train.info()","d7a3aa51":"from sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier","9e0ef479":"train.head(5)","0719eede":"test=pd.read_csv('\/kaggle\/input\/learn-together\/test.csv')","67d3de6d":"test.describe().T","2656df2c":"print(test.shape)\nprint(train.shape)","0dc85da8":"train['Cover_Type'].value_counts()","d9fae862":"train.isnull().sum().sum()\ntest.isnull().sum().sum()","8cf91d34":"train.nunique()","39a874f0":"X=train.drop(['Id','Soil_Type15','Soil_Type7','Cover_Type'],axis=1)","680e6c39":"y=train['Cover_Type']","db2d0d36":"from matplotlib import pyplot\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC","ed613d9b":"models = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))","60224698":"results = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n  kfold = KFold(n_splits=10, random_state=7)\n  cv_results = cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n  results.append(cv_results)\n  names.append(name)\n  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n  print(msg)\n# boxplot algorithm comparison\nfig = pyplot.figure() \nfig.suptitle('Algorithm Comparison') \nax = fig.add_subplot(111) \npyplot.boxplot(results) \nax.set_xticklabels(names) \npyplot.show()","e089934e":"from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, \n                              ExtraTreesClassifier)\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier","b677c45f":"models = []\nmodels.append(('RFC', RandomForestClassifier(n_jobs =  -1, n_estimators = 500, max_features = 12, max_depth = 35, random_state = 1)))\nmodels.append(('ADA', AdaBoostClassifier()))\nmodels.append(('GrB', GradientBoostingClassifier()))\nmodels.append(('EXTree', ExtraTreesClassifier()))\nmodels.append(('Bag', BaggingClassifier()))\nmodels.append(('LGBM', LGBMClassifier()))","f3281f7a":"results = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n  kfold = KFold(n_splits=10, random_state=7)\n  cv_results = cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n  results.append(cv_results)\n  names.append(name)\n  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n  print(msg)\n# boxplot algorithm comparison\nfig = pyplot.figure() \nfig.suptitle('Algorithm Comparison') \nax = fig.add_subplot(111) \npyplot.boxplot(results) \nax.set_xticklabels(names) \npyplot.show()","757043f8":"clf = XGBClassifier(n_estimators=500,colsample_bytree=0.9,max_depth=9,random_state=1,eta=0.2)\n#clf.fit(x,y)","a702414b":"#kfold = KFold(n_splits=10, random_state=7)\ncv_results = cross_val_score(clf, X, y, cv=3, scoring=scoring,n_jobs=3)","29932291":"print(cv_results)","20a17059":"test.info()","8a883ec5":"ID = test['Id']\nX_test = test.drop(['Id','Soil_Type7','Soil_Type15'],axis=1)","79cd32e1":"n_jobs = 8\nseed=1\nestimator=500\nbag_clf = BaggingClassifier(n_estimators=estimator,\n                            max_features = 15, max_depth = 35,\n                            random_state=seed)\n\nlg_clf = LGBMClassifier(n_estimators=estimator,\n                        num_leaves=100,\n                        verbosity=0,\n                        random_state=seed,\n                        n_jobs=n_jobs)\n\nrf_clf = RandomForestClassifier(n_estimators=estimator,\n                                min_samples_leaf=1,\n                                verbose=0,\n                                random_state=seed,\n                                n_jobs=n_jobs)\net_clf = ExtraTreesClassifier(n_estimators=estimator,\n                                min_samples_leaf=1,\n                                verbose=0,\n                                random_state=seed,\n                                n_jobs=n_jobs)\n\n\nfinal_model=[]\nfinal_model.append(('RFC',RandomForestClassifier(n_jobs = n_jobs, n_estimators = estimator, max_features = 12, max_depth = 35, random_state = seed)))\nfinal_model.append(('LGBM',LGBMClassifier(n_estimators=estimator,random_state=seed,n_jobs=n_jobs)))\nfinal_model.append(('Bag', BaggingClassifier(n_estimators=estimator,random_state=seed,n_jobs=n_jobs)))","48b8e9c3":"for name,model in final_model:\n    model.fit(X,y)\n    predict = model.predict(X_test)\n    output = pd.DataFrame({'Id': ID,\n                       'Cover_Type': predict})\n    output.to_csv('submission_' + name + '.csv', index=False)","3be9b5a7":"## Things to do\n#Stacking classifier\n#GridSearchCV to set the parameters\nmodels=[]\nscoring='accuracy'\nmodels.append(('RFC', RandomForestClassifier(random_state = seed)))\nmodels.append(('EXTree', ExtraTreesClassifier(random_state = seed)))\nmodels.append(('Bag', BaggingClassifier(random_state = seed)))\nmodels.append(('LGBM', LGBMClassifier(random_state = seed)))\nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110,150],\n    'max_features': [10,15,20,30,50],\n    #'min_samples_leaf': [3, 4, 5],\n    #'min_samples_split': [8, 10, 12],\n    'n_estimators': [ 200, 300, 500,1000]\n}\nRFC=RandomForestClassifier(random_state = seed)\ngrid = GridSearchCV(estimator=RFC,n_jobs=16,param_grid =param_grid, scoring=scoring, cv=3,verbose=3)\ngrid_result = grid.fit(X, y)\n\n#for name,model in models:\n#    grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=3)\n#    grid_result = grid.fit(X, y)\n#    print(\"%s -- Best: %f using %s\" % (name,grid_result.best_score_, grid_result.best_params_))\n","eb166284":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","1c5e1118":"grid_result.best_estimator_","65bcbf95":"predict_grid_search = grid_result.best_estimator_.predict(X_test)","f7fd32b4":"output = pd.DataFrame({'Id': ID,\n                       'Cover_Type': predict_grid_search})\noutput.to_csv('submission_predict_grid_search.csv', index=False)","40907c74":"from IPython.display import FileLink\n#FileLink(r'df_name.csv')","3fc48abe":"FileLink('submission_predict_grid_search.csv')","2d2b41f4":"## Things to do\n#Stacking classifier\n#GridSearchCV to set the parameters\nscoring = 'accuracy'\ngridParams = {\n    'learning_rate': [0.005,0.01],\n    'n_estimators': [40,80,120,400,800],\n    'num_leaves': [6,8,12,16],\n    'boosting_type' : ['gbdt'],\n    'objective' : ['binary'],\n  #  'random_state' : [501], # Updated from 'seed'\n    'colsample_bytree' : [0.65, 0.66],\n    'subsample' : [0.7,0.75],\n    'reg_alpha' : [1,1.2],\n    'reg_lambda' : [1,1.2,1.4],\n    }\n\nLGBM=LGBMClassifier(random_state = seed)\ngrid = GridSearchCV(estimator=LGBM,n_jobs=16,param_grid =gridParams, scoring=scoring, cv=3,verbose=3)\ngrid_result = grid.fit(X, y)\n\n#for name,model in models:\n#    grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=3)\n#    grid_result = grid.fit(X, y)\n#    print(\"%s -- Best: %f using %s\" % (name,grid_result.best_score_, grid_result.best_params_))\n","eca889ea":"predict_grid_search = grid_result.best_estimator_.predict(X_test)\noutput = pd.DataFrame({'Id': ID,\n                       'Cover_Type': predict_grid_search})\noutput.to_csv('submission_predict_grid_search.csv', index=False)\nfrom IPython.display import FileLink\nFileLink(r'submission_predict_grid_search.csv')","94018422":"grid_result.best_estimator_","6a70a759":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","588c4d3f":"LGBM=LGBMClassifier(n_estimators=estimator,random_state=seed,n_jobs=n_jobs)","a4494004":"LGBM_new = LGBMClassifier(n_estimators=800,random_state=seed,n_jobs=16,reg_alpha=1, reg_lambda=1.2, subsample=0.7,\n                          colsample_bytree= 0.66, learning_rate= 0.11, num_leaves= 16)","b3691b11":"LGBM_new.fit(X,y)","8e26e36f":"predict_LGBM_new = LGBM_new.predict(X_test)\noutput = pd.DataFrame({'Id': ID,\n                       'Cover_Type': predict_LGBM_new})\noutput.to_csv('submission_predict_LGBM_new.csv', index=False)\nfrom IPython.display import FileLink\nFileLink(r'submission_predict_LGBM_new.csv')","82bf337b":"pip install mlxtend","a9cfc36f":"from mlxtend.classifier import StackingCVClassifier","06de4947":"n_jobs = 8\nseed=1\nestimator=500\nbag_clf = BaggingClassifier(n_estimators=estimator,\n                            random_state=seed)\n\nlg_clf = LGBMClassifier(n_estimators=estimator,\n                        num_leaves=100,\n                        verbosity=0,\n                        random_state=seed,\n                        n_jobs=n_jobs)\n\nrf_clf = RandomForestClassifier(n_estimators=estimator,\n                                min_samples_leaf=1,\n                                verbose=0,\n                                random_state=seed,\n                                n_jobs=n_jobs,max_features = 15, max_depth = 35,)\n\net_clf = ExtraTreesClassifier(n_estimators=estimator,\n                                min_samples_leaf=1,\n                                verbose=0,\n                                random_state=seed,\n                                n_jobs=n_jobs)\n","14d2daf2":"sclf = StackingCVClassifier(classifiers=[bag_clf, lg_clf, rf_clf],\n                            meta_classifier=et_clf,\n                            random_state=seed,cv=3,verbose=2,n_jobs=n_jobs)","b0c234a0":"sclf.fit(X,y)","cb1c2830":"predict_SCLF = sclf.predict(X_test)\noutput = pd.DataFrame({'Id': ID,\n                       'Cover_Type': predict_SCLF})\noutput.to_csv('submission_predict_SCLF.csv', index=False)\nfrom IPython.display import FileLink\nFileLink(r'submission_predict_SCLF.csv')","d4cdbd69":"'''doesnt work \n\n\n\nseed=1\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y)\n#def baseline_model():\n  # create model\nmodel = Sequential()\nmodel.add(Dense(100, input_dim=52, kernel_initializer='normal', activation='relu')) \nmodel.add(Dense(units=16, activation=\"relu\"))\nmodel.add(Dense(8, kernel_initializer='normal', activation='sigmoid'))\n# Compile model\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n#    return model\n#estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)\n#kfold = KFold(n_splits=3, shuffle=True, random_state=seed)\nmodel.fit(X,y_binary,epochs=10,batch_size=100)\n#print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n'''","443bb6a0":"***\nInspired by https:\/\/www.kaggle.com\/phsheth\/forestml-part-6-stacking-eval-selected-fets-2\nand\nhttps:\/\/www.kaggle.com\/kwabenantim\/forest-cover-stacking-multiple-classifiers\n***"}}