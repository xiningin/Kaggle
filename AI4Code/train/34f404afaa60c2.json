{"cell_type":{"1d5b507c":"code","2c4f46c0":"code","cd3b978b":"code","608b8fc9":"code","68696a1f":"code","22b35d77":"code","cea2a003":"code","7a6f203f":"code","0f10fdb3":"code","6cd5fd51":"code","67fdbfd4":"code","aaef3172":"code","59b9e79d":"code","c282ad0d":"markdown","516e4f90":"markdown"},"source":{"1d5b507c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2c4f46c0":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Dropout\nfrom tensorflow.keras.layers import Flatten,MaxPooling2D,Conv2D\nfrom tensorflow.keras.optimizers import Adam\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom tensorflow.distribute import MirroredStrategy\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","cd3b978b":"dataset = '..\/input\/chest-xray-pneumonia\/chest_xray\/'\n\ntrain_dir = os.path.join(dataset,\"train\")\nvalid_dir = os.path.join(dataset,\"val\")\ntest_dir = os.path.join(dataset,\"test\")","608b8fc9":"dataset = '..\/input\/chest-xray-pneumonia\/chest_xray\/'\n\ntrain_dir = os.path.join(dataset,\"train\")\nvalid_dir = os.path.join(dataset,\"val\")\ntest_dir = os.path.join(dataset,\"test\")","68696a1f":"# \u0e17\u0e33\u0e44\u0e21\u0e16\u0e36\u0e07\u0e44\u0e21\u0e48\u0e43\u0e0a\u0e49 numpy \u0e40\u0e1e\u0e23\u0e32\u0e30\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e40\u0e1b\u0e47\u0e19 big data \u0e01\u0e32\u0e23\u0e17\u0e35\u0e48\u0e08\u0e30\u0e42\u0e2b\u0e25\u0e14\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25\u0e08\u0e33\u0e19\u0e27\u0e19\u0e21\u0e32\u0e01\u0e40\u0e02\u0e49\u0e32\u0e2a\u0e39\u0e48 memory \u0e2d\u0e32\u0e08\u0e08\u0e30\u0e17\u0e33\u0e43\u0e2b\u0e49\u0e40\u0e04\u0e23\u0e37\u0e48\u0e2d\u0e07\u0e04\u0e2d\u0e21\u0e41\u0e2e\u0e07\u0e44\u0e14\u0e49\u0e40\u0e19\u0e37\u0e48\u0e2d\u0e07\u0e21\u0e32\u0e01\u0e21\u0e35 data \u0e21\u0e2b\u0e32\u0e28\u0e32\u0e25\n# \u0e43\u0e02\u0e49 ImageDataGenerater \u0e43\u0e0a\u0e49\u0e42\u0e2b\u0e25\u0e14\u0e02\u0e49\u0e2d\u0e21\u0e39\u0e25 \u0e42\u0e14\u0e22\u0e08\u0e30\u0e2d\u0e48\u0e32\u0e19 data \u0e40\u0e1b\u0e47\u0e19\u0e0a\u0e38\u0e14\u0e46 \u0e08\u0e32\u0e01 disk \u0e40\u0e1e\u0e37\u0e48\u0e2d feed \u0e40\u0e02\u0e49\u0e32\u0e44\u0e1b\u0e22\u0e31\u0e07 neural network \u0e17\u0e33\u0e43\u0e2b\u0e49\u0e21\u0e35\u0e01\u0e32\u0e23\u0e01\u0e34\u0e19 memory \u0e19\u0e49\u0e2d\u0e22\u0e01\u0e27\u0e48\u0e32\ndata_gen_train = ImageDataGenerator(rescale=1\/255.)\ndata_gen_valid = ImageDataGenerator(rescale=1\/255.)\ndata_gen_test = ImageDataGenerator(rescale=1\/255.)","22b35d77":"train_generator = data_gen_train.flow_from_directory(train_dir,target_size=(200,200),batch_size=128,class_mode='binary')\nvalid_generator = data_gen_valid.flow_from_directory(valid_dir,target_size=(200,200),batch_size=128,class_mode='binary')\ntest_generator = data_gen_test.flow_from_directory(test_dir,target_size=(200,200),batch_size=128,class_mode='binary')","cea2a003":"model = Sequential()\nmodel.add(Conv2D(32, (3, 3),activation=('relu'),input_shape=(200,200,3)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(64, (3, 3),activation=('relu')))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\n\nmodel.add(Flatten())\nmodel.add(Dense(64,activation=('relu')))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1,activation=('sigmoid')))","7a6f203f":"model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","0f10fdb3":"mirrored_strategy = MirroredStrategy(devices=[\"\/gpu:0\"])","6cd5fd51":"model.summary()","67fdbfd4":"history = model.fit_generator(train_generator,epochs=10,validation_data=valid_generator)","aaef3172":"# acc_valid = model.evaluate_generator(valid_generator,verbose=0)\nacc_test = model.evaluate_generator(test_generator,verbose=0)\n# print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], acc_valid[1]*100))\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1], acc_test[1]*100))","59b9e79d":"def summarize_diagnostics(history):\n    # plot loss\n    plt.subplot(211)\n    plt.title('Cross entropy Loss')\n    plt.plot(history.history['loss'],color='blue',label='train_generator')\n    plt.plot(history.history['val_loss'],color='orange',label='test_generator')\n    # plot accuracy\n    plt.subplot(212)\n    plt.title('Classification Accuracy')\n    plt.plot(history.history['accuracy'],color='blue',label='train_generator')\n    plt.plot(history.history['val_accuracy'],color='orange',label='test_generator')\n\nsummarize_diagnostics(history)","c282ad0d":"## preprocessing image","516e4f90":"## model"}}