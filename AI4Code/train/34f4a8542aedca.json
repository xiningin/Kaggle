{"cell_type":{"d0f80e18":"code","f32387c9":"code","b34d0775":"code","7685a5b6":"code","eda49f36":"code","cbbb04ee":"code","504add10":"code","2ab7bfda":"code","e4785494":"code","8726de46":"code","b278b2ab":"code","d2c3d5b9":"code","a741f20a":"code","51f103e6":"code","cc758b22":"code","59e1c13d":"code","2a7c869f":"code","c0c41707":"code","ad8b4ebf":"code","d801a9e8":"code","9331e9b1":"markdown","635493e3":"markdown","332b2fef":"markdown","21f6cebe":"markdown","60137fca":"markdown","93878234":"markdown","0829c483":"markdown","d833c0ed":"markdown","0ba632c8":"markdown","99767836":"markdown","34d5aeb1":"markdown","27c6ee66":"markdown","9468306f":"markdown","c4e20c06":"markdown"},"source":{"d0f80e18":"from __future__ import print_function\nfrom keras.callbacks import LambdaCallback\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.layers import LSTM\nfrom keras.optimizers import RMSprop\nfrom keras.utils.data_utils import get_file\nimport numpy as np\nimport random\nimport sys\nimport io","f32387c9":"text = open('..\/input\/sherlock_homes.txt', 'r').read().lower()\nprint('text length', len(text))","b34d0775":"print(text[:300])","7685a5b6":"chars = sorted(list(set(text)))\nprint('total chars: ', len(chars))","eda49f36":"char_indices = dict((c, i) for i, c in enumerate(chars))\nindices_char = dict((i, c) for i, c in enumerate(chars))","cbbb04ee":"maxlen = 40\nstep = 3\nsentences = []\nnext_chars = []\nfor i in range(0, len(text) - maxlen, step):\n    sentences.append(text[i: i + maxlen])\n    next_chars.append(text[i + maxlen])\nprint('nb sequences:', len(sentences))","504add10":"print(sentences[:3])\nprint(next_chars[:3])","2ab7bfda":"# Print length\nprint(len(sentences))","e4785494":"x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\ny = np.zeros((len(sentences), len(chars)), dtype=np.bool)\nfor i, sentence in enumerate(sentences):\n    for t, char in enumerate(sentence):\n        x[i, t, char_indices[char]] = 1\n    y[i, char_indices[next_chars[i]]] = 1","8726de46":"print(x[:3])\nprint(y[:3])","b278b2ab":"model = Sequential()\nmodel.add(LSTM(128, input_shape=(maxlen, len(chars))))\nmodel.add(Dense(len(chars)))\nmodel.add(Activation('softmax'))","d2c3d5b9":"optimizer = RMSprop(lr=0.01)\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer)","a741f20a":"def sample(preds, temperature=1.0):\n    # helper function to sample an index from a probability array\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) \/ temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds \/ np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds, 1)\n    return np.argmax(probas)","51f103e6":"def on_epoch_end(epoch, logs):\n    # Function invoked at end of each epoch. Prints generated text.\n    print()\n    print('----- Generating text after Epoch: %d' % epoch)\n\n    start_index = random.randint(0, len(text) - maxlen - 1)\n    for diversity in [0.2, 0.5, 1.0, 1.2]:\n        print('----- diversity:', diversity)\n\n        generated = ''\n        sentence = text[start_index: start_index + maxlen]\n        generated += sentence\n        print('----- Generating with seed: \"' + sentence + '\"')\n        sys.stdout.write(generated)\n\n        for i in range(400):\n            x_pred = np.zeros((1, maxlen, len(chars)))\n            for t, char in enumerate(sentence):\n                x_pred[0, t, char_indices[char]] = 1.\n\n            preds = model.predict(x_pred, verbose=0)[0]\n            next_index = sample(preds, diversity)\n            next_char = indices_char[next_index]\n\n            generated += next_char\n            sentence = sentence[1:] + next_char\n\n            sys.stdout.write(next_char)\n            sys.stdout.flush()\n        print()\nprint_callback = LambdaCallback(on_epoch_end=on_epoch_end)","cc758b22":"from keras.callbacks import ModelCheckpoint\n\nfilepath = \"weights.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='loss',\n                             verbose=1, save_best_only=True,\n                             mode='min')","59e1c13d":"from keras.callbacks import ReduceLROnPlateau\nreduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2,\n                              patience=1, min_lr=0.001)","2a7c869f":"callbacks = [print_callback, checkpoint, reduce_lr]","c0c41707":"model.fit(x, y, batch_size=128, epochs=5, callbacks=callbacks)","ad8b4ebf":"def generate_text(length, diversity):\n    # Get random starting text\n    start_index = random.randint(0, len(text) - maxlen - 1)\n    generated = ''\n    sentence = text[start_index: start_index + maxlen]\n    generated += sentence\n    for i in range(length):\n            x_pred = np.zeros((1, maxlen, len(chars)))\n            for t, char in enumerate(sentence):\n                x_pred[0, t, char_indices[char]] = 1.\n\n            preds = model.predict(x_pred, verbose=0)[0]\n            next_index = sample(preds, diversity)\n            next_char = indices_char[next_index]\n\n            generated += next_char\n            sentence = sentence[1:] + next_char\n    return generated","d801a9e8":"print(generate_text(500, 0.2))","9331e9b1":"<a id=\"4.1\"><\/a> \n### 4.1 Helper Functions\n\nI got this function from the lstm_text_generation example from keras. [https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/lstm_text_generation.py](https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/lstm_text_generation.py)\n","635493e3":"<a id=\"1\"><\/a> \n## 1. Introduction\nBecause the sequence in an text is important we will recurrent neural network which can remember its previous inputs.","332b2fef":"Samples an index from a probability array with some temperature.","21f6cebe":"<a id=\"4.2\"><\/a> \n### 4.2 Defining callbacks and training the model","60137fca":"<a id=\"3.2\"><\/a> \n### 3.2 Split up into subsequences\nCreates an array of sentence data with the length maxlen as well as an array with the next character.","93878234":"Callback function to print predicted text generated by our LSTM. It prints generated text with 5 different temperatures [0.2, 0.5, 1.0, 1.2]. 0.2 will generate text with more ordinary word. 1.2 will generate wilder guesses.","0829c483":"<a id=\"6\"><\/a> \n## 6. Conclusion\n\n\nAfter 5 epochs our LSTM performed a ok job and I'm more than satisfied with the result.\n\nHere are a few things you can change to get better results\n\n1. Add more LSTM Layers.\n2. Use more LSTM Cells.\n3. Train for more than 5 epochs. (25+)\n4. Add dropout Layer.\n5. Play around with the batch-size\n\n","d833c0ed":"<a id=\"4\"><\/a> \n## 4. Building model\nFor this kernel I will use a really small LSTM network but if you want to get better results feel free to replace it with a bigger network.","0ba632c8":"# Text Generation using an LSTM in Keras\nIn this kernel you we will go over how to let a network create text in the style of sir arthur conan doyle. This kernel is heavily based on the [official keras text generation example](https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/lstm_text_generation.py).  I also made [a video](https:\/\/youtu.be\/QtQt1CUEE3w) on text generation using an LSTM network.\nContent:\n1. [Introduction](#1)\n2. [Loading in data](#2)\n3. [Preprocessing](#3)  \n    3.1 [Map chars to integers](#3.1)  \n    3.2 [Split up into subsequences](#3.2)\n4. [Building model](#4)  \n    4.1 [Helper Functions](#4.1)  \n    4.2 [Defining callbacks and training the model](#4.2)\n5. [Generate new text](#5)  \n6. [Conclusion](#6)","99767836":"<a id=\"3\"><\/a> \n## 3. Preprocessing","34d5aeb1":"<a id=\"2\"><\/a> \n## 2. Loading in data","27c6ee66":"<a id=\"5\"><\/a> \n## 5. Generate new text\n\n\nWe can generate text using the same approach as in the on_epoch_end helper function create by Keras.","9468306f":"We need to reshape our data in a format we can pass to the Keras LSTM The shape look like [samples, time steps, features]","c4e20c06":"<a id=\"3.1\"><\/a> \n### 3.1 Map chars to integers\n\nBecause we will be training on a character level we need to relate each unique character with a number.\nWe are going to create two dicts one from character to integer and one to transform back to character"}}