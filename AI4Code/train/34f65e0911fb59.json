{"cell_type":{"6f900b0c":"code","c776129d":"code","9683c6d9":"code","2226491e":"code","3caea884":"code","42ab011f":"code","cbea4de2":"code","e8d3df78":"code","1e02ab8e":"code","20952291":"code","d7deb390":"code","64361150":"code","5e02c9e1":"code","18b4e2f6":"code","31be7250":"code","4ec69f94":"code","127d42b7":"code","e4f53335":"code","a7dc1ef8":"code","c20bc7a6":"code","a273f731":"code","7031911b":"code","7bfd1345":"code","f3a2f8d9":"code","57c31e42":"code","96c1806d":"code","3c546a58":"code","4b80f060":"code","f4d99020":"code","ef71a14b":"markdown","8dadbb7d":"markdown","a1352cac":"markdown","b67c16b2":"markdown","f64b4f0f":"markdown","28091df5":"markdown","570496cb":"markdown","612d4316":"markdown","7036af9e":"markdown","778d6153":"markdown","41f9f29b":"markdown","a4df8241":"markdown","4c7f2e4a":"markdown","74f776bf":"markdown","de68dd2f":"markdown","52eaeefb":"markdown","a7a02c54":"markdown","4755d974":"markdown","232491ba":"markdown","a84a3269":"markdown"},"source":{"6f900b0c":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfont1 = {'family':'serif','color':'brown','size':15}","c776129d":"train = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/test.csv')\nsample = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/sample_submission.csv')\n\n# ID is of no use to me \ntrain.drop(columns = 'id', inplace = True)\ntest.drop(columns = 'id', inplace = True)","9683c6d9":"print('Train Shape', train.shape, '\\n', 'Test Shape', test.shape)\ntrain.describe().T","2226491e":"# Simple exploration\nprint('Null in Train', train.isnull().sum().sum())\nprint('Null in Test', test.isnull().sum().sum())\nprint('Train Duplicate data', train.duplicated().sum())\nprint('Test Duplicate data', test.duplicated().sum())","3caea884":"hist = train.hist(figsize = (25, 12), bins=50, grid = False, \n                  xlabelsize=8, ylabelsize=8, layout = (3,5))","42ab011f":"box = train.boxplot(figsize = (18,8), rot = 20 )","cbea4de2":"box = train.drop(columns = ['target']).boxplot(figsize = (18,8), rot = 20 )","e8d3df78":"train.columns","1e02ab8e":"cols = ['cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7',\n       'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14', 'target']\nsns.pairplot(train[cols], size = 4)","20952291":"plt.figure(figsize=(18,12))\nplt.title(\"Correlation Plot\", fontdict = font1)\nsns.heatmap(train.corr(), annot=True, cmap=\"RdBu\", fmt='.2f', \n            center = 0, linewidths=0.1, cbar_kws={\"shrink\": .8}) ","d7deb390":"# Splitting\nx = train.iloc[:,:-1]\ny = train.iloc[:,-1]\n\n\n# PCA is scale sensitive and thus need scaling before\n# Standard scaling\nfrom sklearn.preprocessing import StandardScaler\n\nss = StandardScaler()\nx_train = pd.DataFrame(ss.fit_transform(x))","64361150":"# PCA\nfrom sklearn.decomposition import PCA\npca = PCA()\npca_train = pca.fit_transform(x_train)\n\n# Converting PC Datasets for model training\npca_train = pd.DataFrame(pca_train)\n\nlist_name = [f'pc{i}' for i in range(14)]\n\npca_train.columns = list_name","5e02c9e1":"# NO of PC's\nexplained_variance = pca.explained_variance_ratio_\n\nplt.plot(explained_variance)\nplt.grid()\nplt.xlabel('Number of components')\nplt.ylabel('Explained variance')\nplt.title(\"Scree Plot\");","18b4e2f6":"hist = pca_train.hist(figsize = (25, 12), bins=50, \n                      grid = False, xlabelsize=8, ylabelsize=8, \n                      layout = (3,5))\nplt.suptitle('PCA-train set', fontsize = 20)","31be7250":"# x = pca_train\n# y ","4ec69f94":"# prepare the cross-validation procedure\ncv = KFold(n_splits = 3, random_state = 2, shuffle=True)\n\n# Import the models we are using\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(verbose = 1)","127d42b7":"# evaluate model for Random Forest with original Dataset\nscores = cross_val_score(model, x, y, cv = cv, scoring = \"neg_mean_absolute_error\")\nprint('Random Forest baseline score without PCA' , -scores.mean())","e4f53335":"# evaluate model For Random Forest with PCA Dataset\nscores = cross_val_score(model, pca_train, y, cv = cv, scoring = \"neg_mean_absolute_error\")\nprint('Random Forest baseline score with PCA' , -scores.mean())","a7dc1ef8":"x = train.iloc[:,:-1]\ny = train.iloc[:,-1]","c20bc7a6":"# prepare the cross-validation procedure\ncv = KFold(n_splits=3, random_state=2, shuffle=True)\n\n# Import the models we are using\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = lgb.LGBMRegressor()","a273f731":"# evaluate model\nscores = cross_val_score(model, x, y, cv = cv, scoring = \"neg_mean_absolute_error\")\nprint('LGBM baseline score without PCA' , -scores.mean())","7031911b":"# evaluate model\nscores = cross_val_score(model, pca_train, y, cv = cv, scoring = \"neg_mean_absolute_error\")\nprint('LGBM baseline score with PCA' , -scores.mean())","7bfd1345":"# Parameters\n\nn_estimators = [100, 300, 500, 700, 900]\nnum_leaves = [30, 50, 80]\nrandom_state = [2, 5]\n# max_depth = [-1, 30, 50, 100]\n# learning_rate = [0.1, 0.05, 0.01]","f3a2f8d9":"# Dictionary of parameters\nparams = {'n_estimators': n_estimators,\n          'num_leaves': num_leaves,\n          'random_state': random_state\n          } ","57c31e42":"# create model\nmodel = lgb.LGBMRegressor()\n\nr_search = RandomizedSearchCV(estimator = model, param_distributions = params, \n                              cv = 3, scoring ='neg_mean_absolute_error', \n                              n_iter = 10, verbose=1, return_train_score=True)\n\nr_search.fit(x,y)","96c1806d":"print('Best parameters', r_search.best_params_)\nprint('Best score', -r_search.best_score_)","3c546a58":"rcv_results = pd.DataFrame(r_search.cv_results_)\nrcv_results","4b80f060":"model = lgb.LGBMRegressor(n_estimator = 900, num_leaves = 30,  )\nmodel.fit(x, y)","f4d99020":"sample['target'] = model.predict(test)\nsample.to_csv('submission.csv', index = False)","ef71a14b":"Well the PCA looks promising and PC's 6-7 looks good, that can explain >80% of variance","8dadbb7d":"**Inference** :\n\nNot a bad correlation but the data points are all over the place. I'll perform PCA to reduce the feature number. Lets see how PCA will work out with my model(s).","a1352cac":"### **Base Summary**\n\n* Error - Random forest with PCA -> 0.6005048129803532\n* Error - Random forest without PCA -> 0.5926412679654879\n\n* Error - LGBM with PCA -> 0.6022344194486661\n* Error - LGBM with PCA -> 0.5900531147333651\n\nWell few things that are clear that, modelling with PCA was futile, since the error in the base model was higher in case of PCA.\n\nAlso I am finalising LGBM over Random forest, since the runtime was too much for random forest compared to LGBM. Next step will be hypertuning the LGBM Parameters.\n","b67c16b2":"**Inference** : \n\nNot Many outliers. The Scale of the variables is pretty same, still will check if the model accuracy increases with scaling.","f64b4f0f":"Well I will plot this again without Target, as the scales are clearly different","28091df5":"# Data Exploration","570496cb":"## Univarite analysis\n Box plot and Histogram","612d4316":"## LGBM","7036af9e":"Another TPS, weird set, but better than TPS-August. My method is simple and straight forward, something that will help any beginner. \n\nP.S - Would appreciate constructive criticism","778d6153":"Not much of change in the data distribution, now becoming bit skeptic of PCA in model building.\n\nWhat I can do is to run a baseline model on PCA and Non PCA data and check accuracy with CV","41f9f29b":"## Hyperparameter Tuning","a4df8241":"# PCA","4c7f2e4a":"..\/input\/tabular-playground-series-jan-2021\/train.csv","74f776bf":"# Baseline models - PCA vs Normal Dataset\n\nHere I am evaluating the modle type and the dataset to work on for the final model building. \n\nSince its a regression problem with not that good correlation a random forest vs LGBM makes sense for now. \n\nP.S - Any suggestions what other models can be applied, comment down please\n","de68dd2f":"## Final Showdown","52eaeefb":"PCA is scale sensitive and thus need scaling before","a7a02c54":"# Importing Data and Libraries","4755d974":"# Final Model","232491ba":"*I have commented out some parameters, since the run time was getting too high exceeding the RAM capacity. \nIf anyone has a suggestion for a better way to compute multiple parameters without running out of RAM, please do comment out.*","a84a3269":"## Random Forest"}}