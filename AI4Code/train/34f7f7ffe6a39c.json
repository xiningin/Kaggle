{"cell_type":{"f1383f58":"code","bd41b515":"code","83b89058":"code","c8dc253b":"code","f6c12011":"code","aba5ee8e":"code","7c99ddb3":"code","6f2de6ab":"code","6f3ca1b7":"code","929118e4":"code","6f78c12c":"code","79f54d17":"code","edafdf86":"code","4c7b4b00":"code","07b575e9":"code","f83201de":"code","09e37457":"code","bb3d69bd":"markdown","569ee3c0":"markdown","459a3155":"markdown","6a7f76ea":"markdown","91ef81b1":"markdown","678a2cd4":"markdown"},"source":{"f1383f58":"%%capture\ntry:\n    from pytorch_tabnet.tab_model import TabNetRegressor\nexcept:\n    !pip install ..\/input\/officialpytorchtabnet\/pytorch_tabnet-3.0.0-py3-none-any.whl","bd41b515":"import pandas as pd\nimport numpy as np\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.model_selection import KFold\n\ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/test.csv\")\nsub = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/sample_submission.csv\")\ntrain","83b89058":"train = train.drop([\"id\"], axis=1)\nfeatures = [c for c in train.columns if \"cont\" in c]\ntest = test.drop(\"id\", axis=1)\ntest","c8dc253b":"fe = dict(\n    rankgauss = False,\n    stats = True,\n    gaussmix = True,\n    pca = True,\n    tsne = True,\n    umap = True,\n    drop_original = True,\n)","f6c12011":"all_data = pd.concat([train, test], axis=0, ignore_index=True)\ntargets = all_data.target[:300000]\nall_data = all_data.drop(\"target\", axis=1)\nCOLS = [c for c in all_data.columns if \"cont\" in c]\nall_data","aba5ee8e":"import tqdm\n\nif fe[\"stats\"]:\n    for stats in tqdm.tqdm([\"sum\", \"mean\", \"std\", \"kurt\", \"skew\"]):\n        all_data[\"cont_\" + stats] = getattr(all_data[COLS], stats)(axis = 1)\n        \nall_data","7c99ddb3":"import sys\nsys.path.append(\"..\/input\/rank-gauss\")\nfrom gauss_rank_scaler import GaussRankScaler\n\nif fe[\"rankgauss\"]:\n    scaler = GaussRankScaler()\n    rankgauss_feat = scaler.fit_transform(all_data[COLS])\n    rankgauss_df = pd.DataFrame(rankgauss_feat, columns=[f\"rankgauss_{i}\" for i in range(rankgauss_feat.shape[1])])\n    all_data = pd.concat([all_data, rankgauss_df], axis=1)\nall_data","6f2de6ab":"from sklearn.mixture import GaussianMixture\n\nif fe[\"gaussmix\"]:\n    def get_gmm_class_feature(feat, n):\n        gmm = GaussianMixture(n_components=n, random_state=42)\n\n        gmm.fit(all_data[feat].values.reshape(-1, 1))\n\n        all_data[f'{feat}_class'] = gmm.predict(all_data[feat].values.reshape(-1, 1))\n\n    get_gmm_class_feature('cont1', 4)\n    get_gmm_class_feature('cont2', 10)\n    get_gmm_class_feature('cont3', 6)\n    get_gmm_class_feature('cont4', 4)\n    get_gmm_class_feature('cont5', 3)\n    get_gmm_class_feature('cont6', 2)\n    get_gmm_class_feature('cont7', 3)\n    get_gmm_class_feature('cont8', 4)\n    get_gmm_class_feature('cont9', 4)\n    get_gmm_class_feature('cont10', 8)\n    get_gmm_class_feature('cont11', 5)\n    get_gmm_class_feature('cont12', 4)\n    get_gmm_class_feature('cont13', 6)\n    get_gmm_class_feature('cont14', 6)\n    CLASS_COLS = [c for c in all_data.columns if \"_class\" in c]\n    CLASS_COLS_IDX = []\n    for c in CLASS_COLS:\n        CLASS_COLS_IDX.append(all_data.columns.get_loc(c))\n    assert len(CLASS_COLS) > 0\nall_data","6f3ca1b7":"from sklearn.decomposition import PCA\n\nif fe[\"pca\"]:\n    pca = PCA(n_components = 0.8, random_state = 42).fit(all_data[COLS])\n    pca_feat = pca.transform(all_data[COLS])\n    pca_df = pd.DataFrame(pca_feat, columns = [f\"pca_cont{i}\" for i in range(pca.n_components_)])\n    all_data = pd.concat([all_data, pca_df], axis=1)\n    PCA_COLS = [c for c in all_data.columns if \"pca\" in c]\n    assert len(PCA_COLS) > 0\n\nall_data","929118e4":"from cuml import TSNE\n\nif fe[\"tsne\"]:\n    tsne_components = 2\n    \n    perplexity = [10, 20, 30, 40, 50]\n    for per in perplexity:\n        tsne = TSNE(n_components = tsne_components, perplexity = per, n_neighbors = 3.01 * per)\n        tsne_feat = tsne.fit_transform(all_data[COLS])\n        tsne_df = pd.DataFrame(tsne_feat, columns=[f\"tsne_{per}_{i}\" for i in range(tsne_components)])\n        all_data = pd.concat([all_data, tsne_df], axis = 1)\n    TSNE_COLS = [c for c in all_data.columns if \"tsne\" in c]\nall_data","6f78c12c":"from cuml import UMAP\n\nif fe[\"umap\"]:\n    umap_components = 10\n    umap = UMAP(n_components = umap_components)\n    umap_feat = umap.fit_transform(all_data[COLS])\n    umap_df = pd.DataFrame(umap_feat, columns=[f\"umap{i}\" for i in range(umap_components)])\n    all_data = pd.concat([all_data, umap_df], axis=1)\n    UMAP_COLS = [c for c in all_data.columns if \"umap\" in c]\n    assert len(UMAP_COLS) > 0\nall_data","79f54d17":"if fe[\"drop_original\"]:\n    all_data = all_data.drop(COLS, axis=1)","edafdf86":"train = all_data[:300000]\ntest = all_data[300000:]\nfeatures = list(all_data.columns)","4c7b4b00":"all_data","07b575e9":"MAX_EPOCHS = 200\nBATCH_SIZE = 512\nVIRTUAL_BS = 32\nSEED = 421789\nN_SPLITS = 5\ntabnet_params = dict(\n    n_d = 16,\n    n_a = 16,\n    n_steps = 3,\n    gamma = 1.2,\n    lambda_sparse = 1e-5,\n    optimizer_fn = optim.RMSprop,\n    optimizer_params = dict(lr = 2e-2, weight_decay=1e-5),\n    mask_type = \"entmax\",\n    scheduler_params = dict(\n        mode = \"min\", patience = 5, min_lr = 1e-5, factor = 0.9),\n    scheduler_fn = ReduceLROnPlateau,\n    seed = SEED,\n    verbose = 1,\n    cat_idxs = CLASS_COLS_IDX if fe[\"gaussmix\"] else None\n)\n\n\npredictions = np.zeros((N_SPLITS, len(test), 1))\nfor fold, (tr_idx, val_idx) in enumerate(KFold(n_splits=N_SPLITS, shuffle=True).split(train, targets)):\n    print(f\"FOLD: {fold}\")\n    X_tr, y_tr = train.loc[tr_idx, features].values, targets[tr_idx].values.reshape(-1, 1)\n    X_val, y_val = train.loc[val_idx, features].values, targets[val_idx].values.reshape(-1, 1)\n    \n    model = TabNetRegressor(**tabnet_params)\n    model.fit(\n        X_train = X_tr,\n        y_train = y_tr,\n        eval_set = [(X_val, y_val)],\n        eval_metric = [\"rmse\"],\n        max_epochs = MAX_EPOCHS,\n        batch_size = BATCH_SIZE,\n        virtual_batch_size = VIRTUAL_BS,\n        num_workers = 1,\n        drop_last=False,\n        patience = 20\n    )\n    predictions[fold] = model.predict(test.values)\n    ","f83201de":"sub[\"target\"] = predictions.mean(axis=0)\nsub","09e37457":"sub.to_csv(\"submission.csv\", index=False)","bb3d69bd":"# Drop the ID column","569ee3c0":"# TabNet for Tabular data\nTabNet is a Transformer-based Deep Learning model which is developed by Google researchers. A PyTorch implementation that is sklearn-friendly can be found here: https:\/\/github.com\/dreamquark-ai\/tabnet\n\nHere is the original paper: https:\/\/arxiv.org\/pdf\/1908.07442.pdf","459a3155":"# Get mean of predictions and submit!","6a7f76ea":"# Install TabNet and load data","91ef81b1":"# Train the model\nWe will not use any Feature Engineering technique to test the model's power!","678a2cd4":"# Feature Engineering"}}