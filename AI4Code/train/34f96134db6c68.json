{"cell_type":{"c87cdabf":"code","e7a8d362":"code","20838ee0":"code","b57ca6df":"code","23ec994a":"code","0ad4d79d":"code","9db9b5f1":"code","3aa0a2c8":"code","d8751df1":"code","b6a29779":"code","6a094f85":"code","09b7efd8":"code","381e3775":"code","75e9fa5c":"code","36e9358c":"code","805ee05d":"code","35821f22":"code","30643f81":"code","18cb8135":"code","739ffda9":"code","61696ec6":"code","e64f9445":"code","a4c09806":"code","c210db49":"code","9bf62f9f":"markdown","62a79023":"markdown","6c6189f4":"markdown","a3fe0df0":"markdown","ad7e539b":"markdown","acf8dbc5":"markdown","daada7c0":"markdown"},"source":{"c87cdabf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.linear_model import HuberRegressor\nfrom sklearn import metrics\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e7a8d362":"trainSet = pd.read_csv('..\/input\/train.csv')\ndisplay(trainSet.head())","20838ee0":"testSet = pd.read_csv('..\/input\/test.csv')\ndisplay(testSet.head())","b57ca6df":"structures = pd.read_csv('..\/input\/structures.csv')\ndisplay(structures.head())","23ec994a":"# Map the atom structure data into train and test files\n\ndef map_atom_info(df, atom_idx):\n    df = pd.merge(df, structures, how = 'left',\n                  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],\n                  right_on = ['molecule_name',  'atom_index'])\n    \n    df = df.drop('atom_index', axis=1)\n    df = df.rename(columns={'atom': f'atom_{atom_idx}',\n                            'x': f'x_{atom_idx}',\n                            'y': f'y_{atom_idx}',\n                            'z': f'z_{atom_idx}'})\n    return df\n\ntrainSet = map_atom_info(trainSet, 0)\ntrainSet = map_atom_info(trainSet, 1)\n\ntestSet = map_atom_info(testSet, 0)\ntestSet = map_atom_info(testSet, 1)\n","0ad4d79d":"display(trainSet.head())\ndisplay(testSet.head())","9db9b5f1":"# https:\/\/www.kaggle.com\/jazivxt\/all-this-over-a-dog\n# https:\/\/www.kaggle.com\/artgor\/molecular-properties-eda-and-models\ntrain_p0 = trainSet[['x_0', 'y_0', 'z_0']].values\ntrain_p1 = trainSet[['x_1', 'y_1', 'z_1']].values\ntest_p0 = testSet[['x_0', 'y_0', 'z_0']].values\ntest_p1 = testSet[['x_1', 'y_1', 'z_1']].values\n\ntrainSet['dist'] = np.linalg.norm(train_p0 - train_p1, axis=1)\ntestSet['dist'] = np.linalg.norm(test_p0 - test_p1, axis=1)\n\ntrainSet['dist_to_type_mean'] = trainSet['dist'] \/ trainSet.groupby('type')['dist'].transform('mean')\ntestSet['dist_to_type_mean'] = testSet['dist'] \/ testSet.groupby('type')['dist'].transform('mean')","3aa0a2c8":"# All atom_0 are hydrogens\nassert all(trainSet[\"atom_0\"].astype('category').cat.categories == ['H'])\nassert all(testSet[\"atom_0\"].astype('category').cat.categories == ['H'])","d8751df1":"# atom_1 are carbon, hydrogen or nitrogen\nprint(trainSet[\"atom_1\"].astype('category').cat.categories)\nprint(testSet[\"atom_1\"].astype('category').cat.categories)","b6a29779":"# We use the interaction types, that already include the type of atoms involved\nprint(testSet[\"type\"].astype('category').cat.categories)\nprint(trainSet[\"type\"].astype('category').cat.categories)","6a094f85":"for i in trainSet[\"type\"].astype('category').cat.categories.values:\n    trainSet['type_'+str(i)] = (trainSet['type'] == i)\n    testSet['type_'+str(i)] = (testSet['type'] == i)","09b7efd8":"model = HuberRegressor()","381e3775":"# Features to include (regressors)\nregressors = ['type_1JHC', 'type_1JHN', 'type_2JHC', 'type_2JHH', 'type_2JHN', \n                                       'type_3JHC', 'type_3JHH', 'dist', 'dist_to_type_mean']","75e9fa5c":"# Add bias, interaction term and quadratic and cubic terms\npolyFeat = PolynomialFeatures(degree=3, interaction_only=False, include_bias=True)","36e9358c":"trainX = polyFeat.fit_transform(np.array(trainSet[regressors]))","805ee05d":"# Some features are uninformative:\n# Interaction type features don't (statistically) interact as they are mutually exclusive\nusefulFeatures = [i for i,x in enumerate(np.abs(np.sum(trainX, axis = 0))) if x > 0]\ntrainX = trainX[:,usefulFeatures]\ntrainX.shape","35821f22":"# NB: no need to include type_3JHN as this is redundant: this is always true when all other types are false\nfitDist = model.fit(trainX, \n                    trainSet['scalar_coupling_constant'])","30643f81":"# Display factors to learn what is important for the prediction\nfitDist.coef_","18cb8135":"# See https:\/\/www.kaggle.com\/uberkinder\/efficient-metric\ndef group_mean_log_mae(y_true, y_pred, types, floor=1e-9):\n    maes = (y_true-y_pred).abs().groupby(types).mean()\n    return np.log(maes.map(lambda x: max(x, floor))).mean()","739ffda9":"group_mean_log_mae(trainSet['scalar_coupling_constant'], \n                   model.predict(trainX), trainSet['type'])","61696ec6":"# Control: this should perform better than outputing the same overfitted value for all interactions\nprint(group_mean_log_mae(trainSet['scalar_coupling_constant'], trainSet['scalar_coupling_constant'].median(), trainSet['type']))\nprint(group_mean_log_mae(trainSet['scalar_coupling_constant'], 0.85, trainSet['type']))","e64f9445":"testX = polyFeat.transform(np.array(testSet[regressors]))[:,usefulFeatures]\nresultSet = pd.DataFrame( { \"id\" : testSet['id'],\n                            \"scalar_coupling_constant\" : model.predict(testX)} )","a4c09806":"resultSet.to_csv(\"results.csv\", index = False, header = True)","c210db49":"# Check content of the output file\nwith open(\"results.csv\", \"r\") as f:\n    for i, line in enumerate(f):\n        print(line)\n        if i > 5:\n            break","9bf62f9f":"### Atom types","62a79023":"### Evaluate performance\n","6c6189f4":"### Atomic distance\nhttps:\/\/www.kaggle.com\/inversion\/atomic-distance-benchmark\/","a3fe0df0":"## Huber regression\nRobust linear regression (tries to ignore outliers)","ad7e539b":"## Load data sets","acf8dbc5":"## Export results","daada7c0":"# Naive Kernel for Magnetic Interaction Prediction\nThis is a work-in-progress kernel."}}