{"cell_type":{"c75c04b3":"code","fd2ea18a":"code","de994393":"code","1c4272ab":"code","727d4f6a":"code","31f868f7":"code","a3909fce":"code","1fd50239":"code","ca706173":"code","faadb9d6":"code","f6f21db2":"code","391fa222":"code","c6d84fcb":"code","cd05b003":"code","1dae2215":"markdown","63e5f8db":"markdown","02633053":"markdown","8a213c8e":"markdown"},"source":{"c75c04b3":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport sys\nsys.path.append(\"..\/input\/timm-pytorch-image-models\/pytorch-image-models-master\/\")\nimport timm\nimport cv2\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms as T\nfrom sklearn.metrics import accuracy_score","fd2ea18a":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True","de994393":"def get_img(path):\n    im_bgr = cv2.imread(path)\n    im_rgb = im_bgr[:, :, ::-1]\n    #print(im_rgb)\n    return im_rgb","1c4272ab":"img = get_img('..\/input\/plants114514\/MK\/D1\/train\/Class (1)\/R_0Class1 (10).jpg')\nplt.imshow(img)\nplt.show()","727d4f6a":"img = get_img('..\/input\/plants114514\/MK\/D1\/train\/Class (10)\/R_0Class10 (1).jpg')\nplt.imshow(img)\nplt.show()","31f868f7":"img = get_img('..\/input\/plants114514\/MK\/D1\/train\/Class (11)\/R_0Class11 (6).jpg')\nplt.imshow(img)\nplt.show()","a3909fce":"import os\nimport math\nimport time\nimport random\nimport shutil\nfrom pathlib import Path\nfrom contextlib import contextmanager\nfrom collections import defaultdict, Counter\n\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom tqdm.auto import tqdm\nfrom functools import partial\n\nimport cv2\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD\nimport torchvision.models as models\nfrom torch.nn.parameter import Parameter\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n\nfrom albumentations import (\n    Compose, OneOf, Normalize, Resize, RandomResizedCrop, RandomCrop, HorizontalFlip, VerticalFlip, \n    RandomBrightness, RandomContrast, RandomBrightnessContrast, Rotate, ShiftScaleRotate, Cutout, \n    IAAAdditiveGaussianNoise, Transpose\n    )\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import ImageOnlyTransform\n\nimport timm\nfrom pathlib import Path\nimport warnings \nwarnings.filterwarnings('ignore')\n\n\nclass CFG:\n    debug=False\n    apex=False\n    print_freq=100\n    num_workers=4\n    model_name='resnext50_32x4d'\n    size=256\n    scheduler='CosineAnnealingWarmRestarts' # ['ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']\n    epochs=10\n    #factor=0.2 # ReduceLROnPlateau\n    #patience=4 # ReduceLROnPlateau\n    #eps=1e-6 # ReduceLROnPlateau\n    #T_max=10 # CosineAnnealingLR\n    T_0=10 # CosineAnnealingWarmRestarts\n    lr=1e-4\n    min_lr=1e-6\n    batch_size=32\n    weight_decay=1e-6\n    gradient_accumulation_steps=1\n    max_grad_norm=1000\n    seed=42\n    target_size=5\n    target_col='label'\n    n_fold=5\n    trn_fold=[0, 1, 2, 3, 4]\n    train=True\n    inference=False\n\nif CFG.apex:\n    from apex import amp\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","1fd50239":"\n    \ndef get_transforms(mode):\n    \n    if mode == 'train':\n        return Compose([\n            Transpose(p=0.5),\n            HorizontalFlip(p=0.5),\n            VerticalFlip(p=0.5),\n            ShiftScaleRotate(p=0.5),\n            Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n            ToTensorV2(),\n        ])\n\n    elif mode == 'val':\n        return Compose([\n            Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n            ToTensorV2(),\n        ])\n    \n\nclass PLantDataSet(Dataset):\n    \n    def __init__(self, images, device, transform=None):\n        self.images = images\n        self.transform = transform\n    \n    def __len__(self):\n        return self.images.shape[0]\n    \n    def __getitem__(self, item):\n        img = cv2.imread(self.images.iloc[item, 0])\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        if self.transform is not None:\n            img = self.transform(image=img)\n        label = self.images.iloc[item, 1]\n        img = torch.tensor(img[\"image\"]).float().to(device)\n        label = torch.tensor(label).long().to(device)\n        return img, label","ca706173":"class Flatten(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n    \n    def forward(self, x):\n        return x.view(x.shape[0], -1)\n\n\nclass VGGNet(nn.Module):\n    \n    def __init__(self, num_classification, freeze=True):\n        super().__init__()\n        self.vgg = timm.create_model(\"vgg16\", pretrained=True)\n        self.vgg.head = nn.Identity()\n        if freeze:\n            for param in self.vgg.parameters():\n                param.requires_grad = False\n        self.classify = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)),\n            Flatten(),\n            nn.Linear(4096, num_classification))\n    \n    def forward(self, x):\n        x = self.vgg(x)\n        return self.classify(x)\n    \n    \nclass VGG16Bilinear(nn.Module):\n    def __init__(self, num_classification, freeze=True):\n        super().__init__()\n        vgg = timm.create_model(\"vgg16\", pretrained=True)\n        vgg.head = nn.Identity()\n        if freeze:\n            for param in vgg.parameters():\n                param.requires_grad = False\n        \n        self.cnn = nn.Sequential(vgg, nn.AdaptiveAvgPool2d((1, 1)), nn.Conv2d(4096, 64, 1), nn.ReLU())\n        x = torch.rand(1, 3, 256, 256)\n        _, c, h, w = self.cnn(x).shape\n        \n        if freeze:\n            for param in self.cnn.parameters():\n                param.requires_grad = False\n        \n        self.classifier = nn.Linear(c ** 2, num_classification)\n        \n    def forward(self, x):\n        x = self.cnn(x)\n        b, c, h, w = x.shape\n        x = torch.bmm(x.view(b, c, h * w), x.view(b, c, h * w).transpose(1, 2)).view(b, -1)\n        x = torch.nn.functional.normalize(torch.sign(x) * torch.sqrt(torch.abs(x) + 1e-10))\n        x = self.classifier(x)\n        return x\n    \n\n","faadb9d6":"from pathlib import Path    \nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold","f6f21db2":"train_images = []\nd1_train_path = Path(\"..\/input\/plants114514\/MK\/D1\/train\")\nfor d in d1_train_path.iterdir():\n    for f in d.glob(\"*.jpg\"):\n        train_images.append((str(f.absolute()), d.name))\ntrain_images = pd.DataFrame(train_images, columns=[\"path\", \"label\"])\n\ntest_images = []\nd1_test_path = Path(\"..\/input\/plants114514\/MK\/D1\/test\")\nfor d in d1_test_path.iterdir():\n    for f in d.glob(\"*.jpg\"):\n        test_images.append((str(f.absolute()), d.name))\ntest_images = pd.DataFrame(test_images, columns=[\"path\", \"label\"])","391fa222":"label_enc = LabelEncoder().fit(train_images.label)\ntrain_images[\"label\"] = label_enc.transform(train_images[\"label\"])\ntest_images[\"label\"] = label_enc.transform(test_images[\"label\"])","c6d84fcb":"def train_epoch(model, optimizer, loss_fn, data_loader):\n    losses = []\n    model.train()\n    for imgs, labels in data_loader:\n        preds = model(imgs)\n        loss = loss_fn(preds, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        losses.append(loss.item())\n    return np.mean(losses)\n\n\ndef eval_epoch(model, loss_fn, data_loader):\n    losses = []\n    acces = []\n    model.eval()\n    with torch.no_grad():\n        for imgs, labels in data_loader:\n            preds = model(imgs)\n            loss = loss_fn(preds, labels)\n            pred_labels = torch.argmax(preds, dim=1).cpu().numpy()\n            labels = labels.cpu().numpy()\n            acc = accuracy_score(pred_labels, labels)\n            losses.append(loss.item())\n            acces.append(acc)\n    return np.mean(losses), np.mean(acces)","cd05b003":"device = \"cuda\"\nbatch_size = 128\nlr = 0.003\nnum_classifications = 44\ntrain_transform = get_transforms(\"train\")\nvalid_transform = get_transforms(\"val\")\nloss_fn = nn.CrossEntropyLoss()\nnum_epochs = 3\nearly_stopping = 5\n\nfold = StratifiedKFold(5)\ntest_set = PLantDataSet(test_images, device, valid_transform)\ntest_dl = DataLoader(test_set, batch_size=batch_size, shuffle=False)\nfor cv, (trn_idx, val_idx) in enumerate(fold.split(train_images.index, train_images.label)):\n    \n    # model = VGGNet(num_classifications)\n    model = VGG16Bilinear(num_classifications)\n    model.to(device)\n    optimizer = Adam(filter(lambda x: x.requires_grad, model.parameters()), lr)\n    best_acc = -float(\"inf\")\n    best_acc_epoch = 0\n    \n    trn = train_images.iloc[trn_idx].reset_index(drop=True)\n    val = train_images.iloc[val_idx].reset_index(drop=True)\n    trn_set = PLantDataSet(trn, device, train_transform)\n    val_set = PLantDataSet(val, device, valid_transform)\n    trn_dl = DataLoader(trn_set, batch_size=batch_size, shuffle=True)\n    val_dl = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n    \n    for epoch in range(num_epochs):\n        train_loss = train_epoch(model, optimizer, loss_fn, trn_dl)\n        val_loss, val_acc = eval_epoch(model, loss_fn, val_dl)\n        print(f\"CV {cv+1} epoch {epoch} train loss {train_loss:.3f}, val loss {val_loss:.3f} acc {val_acc:.3f}\")\n        if val_acc > best_acc:\n            torch.save(model.state_dict(), f\"cv_{cv+1}_best.pth\")\n    print(f\"stop training, load best model epoch {epoch+1}\")\n    model.load_state_dict(torch.load(f\"cv_{cv+1}_best.pth\"))\n    test_loss, test_acc = eval_epoch(model, loss_fn, test_dl)\n    print(f\"cv {cv+1} test acc {test_acc:.3f}\")","1dae2215":"## Data Prepare","63e5f8db":"## Utils","02633053":"## Model Train","8a213c8e":"## Models"}}