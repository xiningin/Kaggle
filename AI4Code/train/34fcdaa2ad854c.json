{"cell_type":{"926e2e0a":"code","e34018bd":"code","f5ba901e":"code","d6eb9e0d":"code","c63efbfa":"code","a9f71f39":"code","96e94866":"code","2886f3e1":"code","3f5efeb8":"code","2231e295":"code","c8587b63":"code","c3c19d50":"code","be34fffc":"code","b61b0890":"code","e5eab70e":"code","03a46fa5":"code","bbbb1c4d":"code","6704db02":"code","cf4d24ec":"code","00d531f1":"code","a53d7470":"code","965a3110":"code","5ed0289a":"code","fda9db2e":"code","1526a3ca":"code","28230c8f":"code","a89f6347":"code","412366c3":"code","e67cdf83":"code","b52d8d4d":"code","06c40ce4":"code","3f8f157a":"code","80b4f586":"code","800f208a":"markdown","e528bac2":"markdown","34320e25":"markdown","d0c9fb27":"markdown","f83c4054":"markdown","e4cd501b":"markdown","b4d8216c":"markdown","e103f1ab":"markdown","48ea343f":"markdown","a485a829":"markdown","a307ede7":"markdown","f3d77499":"markdown","2bb165ee":"markdown","896157a9":"markdown","8f027496":"markdown","3f2eff3a":"markdown","62510aec":"markdown","2b735f5f":"markdown","d49a6643":"markdown"},"source":{"926e2e0a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style('whitegrid')\nplt.style.use('seaborn-deep')\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.serif'] = 'Ubuntu'\nplt.rcParams['font.monospace'] = 'Ubuntu Mono'\nplt.rcParams['font.size'] = 10\nplt.rcParams['axes.labelsize'] = 12\nplt.rcParams['axes.titlesize'] = 12\nplt.rcParams['xtick.labelsize'] = 8\nplt.rcParams['ytick.labelsize'] = 8\nplt.rcParams['legend.fontsize'] = 12\nplt.rcParams['figure.titlesize'] = 14\nplt.rcParams['figure.figsize'] = (12, 8)\n\npd.options.mode.chained_assignment = None\npd.options.display.float_format = '{:.2f}'.format\npd.set_option('display.max_columns', 200)\npd.set_option('display.width', 400)\nimport warnings\nwarnings.filterwarnings('ignore')\nimport sklearn.metrics as skm\nimport sklearn.model_selection as skms\nimport sklearn.preprocessing as skp\nimport random\nseed = 12\nnp.random.seed(seed)\n\nfrom datetime import date","e34018bd":"# important funtions\ndef datasetShape(df):\n    rows, cols = df.shape\n    print(\"The dataframe has\",rows,\"rows and\",cols,\"columns.\")\n    \n# select numerical and categorical features\ndef divideFeatures(df):\n    numerical_features = df.select_dtypes(include=[np.number])\n    categorical_features = df.select_dtypes(include=[np.object])\n    return numerical_features, categorical_features","f5ba901e":"base = '\/kaggle\/input\/tabular-playground-series-jan-2021\/'\ndata_file = base + \"train.csv\"\ndf = pd.read_csv(data_file)\ndf.head()","d6eb9e0d":"data_file = base + \"test.csv\"\ndf_test = pd.read_csv(data_file)\ndf_test.head()","c63efbfa":"# check dataset shape\ndatasetShape(df)","a9f71f39":"df.drop('id', inplace=True, axis=1)","96e94866":"# check for duplicates\nprint(df.shape)\ndf.drop_duplicates(inplace=True)\nprint(df.shape)","2886f3e1":"# boxplots of numerical features for outlier detection\n\nfig = plt.figure(figsize=(16,20))\nfor i in range(len(df.columns)):\n    fig.add_subplot(3, 5, i+1)\n    sns.boxplot(y=df.iloc[:,i])\nplt.tight_layout()\nplt.show()","3f5efeb8":"# check for missing values\ndf.isna().any().sum()","2231e295":"import matplotlib.gridspec as gridspec\nfig = plt.figure(constrained_layout=True, figsize=(16,6))\ngrid = gridspec.GridSpec(ncols=2, nrows=1, figure=fig)\nax1 = fig.add_subplot(grid[0, :2])\nax1.set_title('Histogram')\nsns.distplot(df.loc[:,'target'], norm_hist=True, ax = ax1)\nplt.show()","c8587b63":"sns.pairplot(df)\nplt.show()\n\n# correlation heatmap for all features\ncorr = df.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr, mask = mask, annot=True)\nplt.show()","c3c19d50":"# plot sample skewed feature\nplt.figure(figsize=(10,4))\nsns.distplot(df['cont1'])\nplt.show()","be34fffc":"skewed_features = df.apply(lambda x: x.skew()).sort_values(ascending=False)\nskewed_features","b61b0890":"# # transform skewed features\n# for feat in skewed_features.index:\n#     if abs(skewed_features.loc[feat]) > 0.0005:\n#         df[feat] = np.log1p(df[feat])\n#         if 'Close' not in feat:\n#             df_test[feat] = np.log1p(df_test[feat])","e5eab70e":"# plot sample treated feature\nplt.figure(figsize=(10,4))\nsns.distplot(df['cont1'])\nplt.show()","03a46fa5":"# shuffle samples\ndf_shuffle = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n\ndf_y = df_shuffle.pop('target')\ndf_X = df_shuffle\n\n# split into train dev and test\nX_train, X_test, y_train, y_test = skms.train_test_split(df_X, df_y, train_size=0.9, random_state=seed)\nprint(f\"Train set has {X_train.shape[0]} records out of {len(df_shuffle)} which is {round(X_train.shape[0]\/len(df_shuffle)*100)}%\")\nprint(f\"Test set has {X_test.shape[0]} records out of {len(df_shuffle)} which is {round(X_test.shape[0]\/len(df_shuffle)*100)}%\")","bbbb1c4d":"import sklearn.linear_model as sklm","6704db02":"# scaler = skp.RobustScaler()\nscaler = skp.MinMaxScaler()\n# scaler = skp.StandardScaler()\n\n# apply scaling to all numerical variables except dummy variables as they are already between 0 and 1\nX_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n\n# scale test data with transform()\nX_test = pd.DataFrame(scaler.transform(X_test), columns=X_train.columns)\n\n# view sample data\nX_train.describe()","cf4d24ec":"def expm1(x):\n    return np.expm1(x)\ndef getRmse(y_train, y_train_pred):\n#     print(skm.mean_squared_error(expm1(y_train), expm1(y_train_pred)))\n    print(skm.mean_squared_error(y_train, y_train_pred))","00d531f1":"lmr = sklm.Ridge(alpha=0.001)\nlmr.fit(X_train, y_train)\n\n# predict\ny_train_pred = lmr.predict(X_train)\ny_test_pred = lmr.predict(X_test)\ngetRmse(y_train, y_train_pred)\ngetRmse(y_test, y_test_pred)","a53d7470":"# list of alphas to tune\nparams = {'alpha': [0.0001, 0.001, 0.005, 0.01, 0.03, 0.05, 0.1, 0.5, 1.0, 5.0, 10]}\nridge = sklm.Ridge()\n\n# cross validation\nmodel_cv_ridge = skms.GridSearchCV(estimator = ridge, n_jobs=-1, param_grid = params, \n                             scoring= 'neg_mean_squared_error', cv = 5, \n                             return_train_score=True, verbose = 3)            \nmodel_cv_ridge.fit(X_train, y_train)\nprint(model_cv_ridge.best_estimator_)\ny_train_pred = model_cv_ridge.predict(X_train)\ny_test_pred = model_cv_ridge.predict(X_test)\ngetRmse(y_train, y_train_pred)\ngetRmse(y_test, y_test_pred)","965a3110":"import catboost as cb\n\ncbr = cb.CatBoostRegressor(loss_function='RMSE', verbose=0)\ncbr.fit(X_train, y_train, eval_set=(X_test, y_test))\nprint(cbr.best_score_)\n\ny_train_pred = cbr.predict(X_train)\ny_test_pred = cbr.predict(X_test)\ngetRmse(y_train, y_train_pred)\ngetRmse(y_test, y_test_pred)","5ed0289a":"import sklearn.ensemble as ske\n\nxgb = ske.GradientBoostingRegressor(criterion='mse', random_state=1)\nxgb.fit(X_train, y_train)\n\n# predict\ny_train_pred = xgb.predict(X_train)\ny_test_pred = xgb.predict(X_test)\ngetRmse(y_train, y_train_pred)\ngetRmse(y_test, y_test_pred)","fda9db2e":"xgb = ske.ExtraTreesRegressor(criterion='mse', random_state=1)\nxgb.fit(X_train, y_train)\n\n# predict\ny_train_pred = xgb.predict(X_train)\ny_test_pred = xgb.predict(X_test)\ngetRmse(y_train, y_train_pred)\ngetRmse(y_test, y_test_pred)","1526a3ca":"xgb = ske.RandomForestRegressor(criterion='mse', random_state=1)\nxgb.fit(X_train, y_train)\n\n# predict\ny_train_pred = xgb.predict(X_train)\ny_test_pred = xgb.predict(X_test)\ngetRmse(y_train, y_train_pred)\ngetRmse(y_test, y_test_pred)","28230c8f":"import xgboost as xg\nxgb = xg.XGBRegressor(objective ='reg:squarederror', random_state=1)\nxgb.fit(X_train, y_train)\n\n# predict\ny_train_pred = xgb.predict(X_train)\ny_test_pred = xgb.predict(X_test)\ngetRmse(y_train, y_train_pred)\ngetRmse(y_test, y_test_pred)","a89f6347":"import tensorflow as tf\nprint(\"TF version:-\", tf.__version__)\nimport keras as k\ntf.random.set_seed(seed)","412366c3":"THRESHOLD = 0\nbestModelPath = '.\/best_model.hdf5'\n\nclass myCallback(k.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('mse') < THRESHOLD):\n            print(\"\\n\\nStopping training as we have reached our goal.\")   \n            self.model.stop_training = True\n\nmycb = myCallback()\ncheckpoint = k.callbacks.ModelCheckpoint(filepath=bestModelPath, monitor='val_loss', verbose=1, save_best_only=True)\n\ncallbacks_list = [mycb,\n                  checkpoint\n                 ]\n            \ndef plotHistory(history):\n    print(\"Min. Validation MSE\",min(history.history[\"val_mse\"]))\n    pd.DataFrame(history.history).plot(figsize=(12,6))\n    plt.show()","e67cdf83":"epochs = 40\n\nmodel_1 = k.models.Sequential([\n    k.layers.Dense(512, activation='relu', input_shape=(X_train.shape[1],)),\n    k.layers.Dropout(0.2),\n    \n#     k.layers.Dense(4096, activation='relu'),\n#     k.layers.Dropout(0.2),\n\n    k.layers.Dense(256, activation='relu'),\n    k.layers.Dropout(0.2),\n\n    k.layers.Dense(1, activation='linear'),\n])\nprint(model_1.summary())\n\nmodel_1.compile(optimizer='adam',\n              loss='mse',\n              metrics='mse'\n)\nhistory = model_1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs,\n                 callbacks=[callbacks_list])","b52d8d4d":"plotHistory(history)","06c40ce4":"def getTestResults(m=None):\n    df_final = df.sample(frac=1, random_state=1).reset_index(drop=True)\n    test_cols = [x for x in df.columns if 'target' not in x]\n    df_final_test = df_test[test_cols]\n    df_y = df_final.pop('target')\n    df_X = df_final\n\n#     scaler = skp.RobustScaler()\n    scaler = skp.MinMaxScaler()\n#     scaler = skp.StandardScaler()\n\n    df_X = pd.DataFrame(scaler.fit_transform(df_X), columns=df_X.columns)\n\n    X_test = pd.DataFrame(scaler.transform(df_final_test), columns=df_X.columns)\n    \n    if m is None:\n\n#         lmr = sklm.Ridge(alpha=0.0001)\n#         lmr.fit(df_X, df_y)\n\n        lmr = cb.CatBoostRegressor(loss_function='RMSE', verbose=0)\n        lmr.fit(df_X, df_y)\n\n#         lmr = ske.ExtraTreesRegressor(criterion='mse', random_state=1)\n#         lmr.fit(df_X, df_y)\n\n#         lmr = ske.RandomForestRegressor(criterion='mse', random_state=1)\n#         lmr.fit(df_X, df_y)\n\n#         lmr = xg.XGBRegressor(objective ='reg:squarederror', random_state=1)\n#         lmr.fit(df_X, df_y)\n\n    else:\n        lmr = m\n\n    # predict\n    y_train_pred = lmr.predict(df_X)\n    y_test_pred = lmr.predict(X_test)\n    if m is not None:\n        y_test_pred = [y[0] for y in y_test_pred]\n    getRmse(df_y, y_train_pred)\n    return y_test_pred\n\n# ML models\nresults = getTestResults()\n\n# Neural Network model\n# results = getTestResults(k.models.load_model(bestModelPath))","3f8f157a":"submission = pd.DataFrame({\n    'id': df_test['id'],\n    'target': results,\n})\nsubmission.head()","80b4f586":"submission.to_csv('.\/submission_Catboost.csv', index=False)","800f208a":"### Gradient Boosting","e528bac2":"# Step 3: Data Preparation\n\n### Outlier Treatment\n\nWe will take log of the feature values using np.log1p()","34320e25":"Metrics - RMSE\n\n1 - NN - .71704\n\n2 - Ridge(0.0001) - .72782\n\n3 - CatBoost - .70001\n\n4 - ExtraTrees - .70887\n\n5 - RF - .70985\n\n6 - XGB - .70463\n\n#### CatBoost performed best.","d0c9fb27":"# Step 4: Data Modelling\n\n### Split Train-Test Data","f83c4054":"Not applying skewness.","e4cd501b":"# Tabular Playground Challenge","b4d8216c":"### Feature Scaling","e103f1ab":"### Extra Trees","48ea343f":"### XGBoost","a485a829":"## Model Building","a307ede7":"# Step 1: Reading and Understanding the Data","f3d77499":"## Deep Learning Model","2bb165ee":"# Test Evaluation & Submission","896157a9":"### CatBoost","8f027496":"### RandomForest","3f2eff3a":"No Data Cleaning required. Lets check for data skewness further.","62510aec":"### Univariate Analysis","2b735f5f":"### Ridge","d49a6643":"# Step 2: EDA"}}