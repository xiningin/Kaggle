{"cell_type":{"e8fae5c1":"code","6fb70b2f":"code","65f18838":"code","b2a46056":"code","cb068e79":"code","509f5ddb":"code","d690610f":"code","0cca3e06":"code","f54b3e01":"code","e3794046":"code","01eb3e85":"code","6e99156e":"code","946603b0":"code","abac8a67":"code","367dbfd4":"code","f88b81a1":"code","abed1feb":"code","66707a31":"markdown","bd34dcfe":"markdown","2db12023":"markdown","b2c020b8":"markdown","160c98e7":"markdown","64b600d9":"markdown","1765ef32":"markdown","fd3e61e9":"markdown","b1951f9a":"markdown","8f5b156e":"markdown","5b03abe4":"markdown","acd66b35":"markdown","13bfe3c8":"markdown","0a66dada":"markdown","df0d3e70":"markdown","b5e66ec5":"markdown","3746e97f":"markdown","26c5d716":"markdown"},"source":{"e8fae5c1":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport tensorflow as tf\nimport keras\nimport os\nimport glob as gb\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten ,Conv2D, MaxPooling2D","6fb70b2f":"all_letters = os.listdir('..\/input\/notmnist\/notMNIST_large\/notMNIST_large')\n\nprint(f'We have {len(all_letters)} letters , which are : {all_letters}')","65f18838":"total_images = 0\nfor letter in all_letters : \n    available_images = gb.glob(pathname= f'..\/input\/notmnist\/notMNIST_large\/notMNIST_large\/{letter}\/*.png')\n    total_images+=len(available_images)\n    print(f'for letter {letter} we have  {len(available_images)} available images')\nprint('-----------------------')    \nprint(f'Total Images are {total_images} images')","b2a46056":"X = list(np.zeros(shape=(total_images , 28,28)))\ny = list(np.zeros(shape=(total_images)))","cb068e79":"i=0\ny_value = 0\nfor letter in all_letters : \n    available_images = gb.glob(pathname= f'..\/input\/notmnist\/notMNIST_large\/notMNIST_large\/{letter}\/*.png')\n    for image in available_images : \n        try : \n            x = plt.imread(image)\n            X[i] = x\n            y[i] = y_value\n            i+=1\n        except : \n            pass\n    y_value+=1","509f5ddb":"ohe  = OneHotEncoder()\ny = np.array(y)\ny = y.reshape(len(y), 1)\nohe.fit(y)\ny = ohe.transform(y).toarray()","d690610f":"y[10000]","0cca3e06":"X = np.expand_dims(X, -1).astype('float32')\/255.0","f54b3e01":"X.shape","e3794046":"X_part, X_cv, y_part, y_cv = train_test_split(X, y, test_size=0.15, random_state=44, shuffle =True)\n\nprint('X_train shape is ' , X_part.shape)\nprint('X_test shape is ' , X_cv.shape)\nprint('y_train shape is ' , y_part.shape)\nprint('y_test shape is ' , y_cv.shape)","01eb3e85":"X_train, X_test, y_train, y_test = train_test_split(X_part, y_part, test_size=0.25, random_state=44, shuffle =True)\n\nprint('X_train shape is ' , X_train.shape)\nprint('X_test shape is ' , X_test.shape)\nprint('y_train shape is ' , y_train.shape)\nprint('y_test shape is ' , y_test.shape)","6e99156e":"KerasModel = keras.models.Sequential([\n        keras.layers.Conv2D(filters = 32, kernel_size = 4,  activation = tf.nn.relu , padding = 'same'),\n        keras.layers.MaxPool2D(pool_size=(3,3), strides=None, padding='valid'),\n        keras.layers.BatchNormalization(),\n        keras.layers.Conv2D(filters=32, kernel_size=4,activation = tf.nn.relu , padding='same'),\n        keras.layers.MaxPool2D(),\n        keras.layers.BatchNormalization(),\n        keras.layers.Conv2D(filters=64, kernel_size=5,activation = tf.nn.relu , padding='same'),\n        keras.layers.MaxPool2D(),\n        keras.layers.Flatten(),    \n        keras.layers.Dropout(0.5),        \n        keras.layers.Dense(64),    \n        keras.layers.Dropout(0.3),            \n        keras.layers.Dense(units= 10,activation = tf.nn.softmax ),                \n\n    ])\n    \n\nKerasModel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])","946603b0":"#Train\nKerasModel.fit(X_train,y_train,validation_data=(X_cv, y_cv),epochs=3,batch_size=64,verbose=1)","abac8a67":"KerasModel.summary()","367dbfd4":"y_pred = KerasModel.predict(X_test)\n\nprint('Prediction Shape is {}'.format(y_pred.shape))","f88b81a1":"Letters ={0:'A', 1:'B' , 2:'C' ,3:'D' ,4:'E' ,5:'F' ,6:'G' ,7:'H' ,8:'I' ,9:'J' }\n\nfor i in list(np.random.randint(0,len(X_test) ,size= 10)) : \n    print(f'for sample  {i}  the predicted value is   {Letters[np.argmax(y_pred[i])]}   , while the actual letter is {Letters[np.argmax(y_test[i])]}')\n","abed1feb":"ModelLoss, ModelAccuracy = KerasModel.evaluate(X_test, y_test)\n\nprint('Test Loss is {}'.format(ModelLoss))\nprint('Test Accuracy is {}'.format(ModelAccuracy ))","66707a31":"now how the model looks like ? ","bd34dcfe":"then we need to check the folders to know what letters available . ","2db12023":"total 529 thousand images for all 10 letters , now let's create X & y variables , so we can fill them with read data\n","b2c020b8":"& to measure the loss & accuracy","160c98e7":"now to open each file & read it using plt.imread , then fill it in its place in X & y data","64b600d9":"and we can check random samples from X_test","1765ef32":"____\n\n\n# Read Data . \n\nwe'll use glob library to collect all png pictures to know how many pictures we have for each letter . ","fd3e61e9":"then to train it , using few number of epochs to avoid OF","b1951f9a":"# CNN for classifying letters \nBy: Hesham Asem\n\n______\n\nhere we'll build a Conv2d to be used in reading & classifying about half million pictures of  first 10 alphabetic letters . . \n\nyou can find data file  here :  https:\/\/www.kaggle.com\/jwjohnson314\/notmnist\n\nlet;s first import libraries \n","8f5b156e":"then to split X_part & y_part into train & test","5b03abe4":"# Predicting\n\nthen we'll predict X_test","acd66b35":"____\n\n# Forming Dimensions\n\nsince (y) data now is a single number vary from 0 to 9 , we'll need to categorize it using OneHotEncoder from sklearn , so it be ready for the softmax activation functing in CNN","13bfe3c8":"now X shape should be : sample size * 28 * 28 * 1","0a66dada":"# Splitting Data\n\nnow lets split our dat to Train , Cross-Validation & Test sets . . \n\nfirst to create X_part & y_part which is 85% of data , also X_test & y_test which is 15%\n","df0d3e70":"great , with 91% accuracy we achieved good result without moving to OF","b5e66ec5":"now we can check a random y value","3746e97f":"then we have to expand X dimension to be suitable with the CNN dimensions","26c5d716":"# Build the Model\n\nnow let's build the model with Keras , using Conv2d & Maxpooling tools , & not to forget to dropout some cells to avoid OF"}}