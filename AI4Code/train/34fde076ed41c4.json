{"cell_type":{"263e145f":"code","2cfe620b":"code","9ec64b67":"code","102e9b29":"code","d75b2c19":"code","199260f2":"code","a5fca36e":"code","c1f5e9c2":"code","f4c240da":"code","9f8eff37":"code","389afdf2":"code","ceec6092":"code","2a621338":"code","02c83819":"code","03263fc9":"code","f8f206a9":"code","86c45eec":"code","99369890":"code","7fafa980":"code","a2f2dbe1":"code","618eb42e":"code","4fe82cf3":"markdown","59338e56":"markdown","e2614d51":"markdown","bece3e23":"markdown","e4c05c4d":"markdown","38cef25d":"markdown","d069b3de":"markdown","661223d1":"markdown","9739d885":"markdown","fa6bbab3":"markdown","d33adedc":"markdown","2bd2f45f":"markdown","13eb575d":"markdown","987e6770":"markdown","b6a4a2b7":"markdown","a522e13a":"markdown","20f2af70":"markdown","9a171728":"markdown"},"source":{"263e145f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport re\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nimport collections\nimport itertools","2cfe620b":"df = pd.read_csv(\"..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\")\n\ndf.head()","9ec64b67":"sns.countplot(df[\"sentiment\"])","102e9b29":"df.isna().sum()","d75b2c19":"def preprocessing_text(texts):\n    texts = re.sub(r'<.*?>', '', texts)\n    texts = re.sub(r'[^a-zA-Z]', ' ', texts)\n    return ' '.join(x.lower() for x in texts.split())","199260f2":"df['review_cleaned'] = df['review'].apply(lambda x : preprocessing_text(x))","a5fca36e":"X_train, X_test, y_train, y_test = train_test_split(df['review_cleaned'], df['sentiment'].map({'negative':0, 'positive':1}))\npipeline = Pipeline([('tfidf', TfidfVectorizer()),\n                    ('lr_clf', LogisticRegression())])\npipeline.fit(X_train, y_train)\n\ny_pred = pipeline.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\n\nsns.heatmap(confusion_matrix(y_test, y_pred), \n            annot=True, fmt='.0f', \n            xticklabels=['Predicted negative', 'Predicted positive'], \n            yticklabels=['Negative', 'Positive'])","c1f5e9c2":"df['review_cleaned_500'] = df['review_cleaned'].apply(lambda x:x[:500])\n\ndf['review_cleaned_500'][2]","f4c240da":"tokenizer = Tokenizer(num_words=10000)","9f8eff37":"tokenizer.fit_on_texts(df['review_cleaned_500'])\nseq = tokenizer.texts_to_sequences(df['review_cleaned_500'])\nX = pad_sequences(seq, padding='post')\n\nprint(f'X_shape: {X.shape}, X_min: {np.min(X)}, X_max: {np.max(X)}')","389afdf2":"y = df['sentiment'].map({'negative' : 0, 'positive' : 1}).values","ceec6092":"X_train_valid, X_test, y_train_valid, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size=0.2, random_state=42)\nprint(X_train.shape, X_valid.shape, X_test.shape, y_train.shape, y_valid.shape, y_test.shape)","2a621338":"y_train","02c83819":"embed_size = 64\nmodel = keras.models.Sequential([\n    keras.layers.Embedding(input_dim=10000, output_dim=embed_size, input_shape=[None], mask_zero=True),\n    keras.layers.LSTM(64),\n    keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.summary()","03263fc9":"optimizer =keras.optimizers.Adam(learning_rate=0.001)\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, batch_size=32, epochs=5, validation_data=(X_valid, y_valid))","f8f206a9":"pd.DataFrame(history.history).plot()","86c45eec":"y_pred = model.predict_classes(X_test)\n\nprint(classification_report(y_test, y_pred))\n\nsns.heatmap(confusion_matrix(y_test, y_pred), \n            annot=True, fmt='.0f', \n            xticklabels=['Predicted negative', 'Predicted positive'], \n            yticklabels=['Negative', 'Positive'])","99369890":"embed_size = 64\nmodel_v2 = keras.models.Sequential([\n    keras.layers.Embedding(input_dim=10000, output_dim=embed_size, input_shape=[None], mask_zero=True),\n    keras.layers.SpatialDropout1D(0.2),\n    keras.layers.LSTM(64, return_sequences=False, dropout=0.2, recurrent_dropout=0.2),\n    keras.layers.Dense(1, activation='sigmoid')\n])\nmodel_v2.summary()","7fafa980":"early_stopping_cb = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\noptimizer = keras.optimizers.Adam(learning_rate=0.0001)\nmodel_v2.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\nhistory = model_v2.fit(X_train, y_train, epochs=20, batch_size=64, validation_data=(X_valid, y_valid), callbacks=[early_stopping_cb])","a2f2dbe1":"pd.DataFrame(history.history).plot()","618eb42e":"y_pred = model_v2.predict_classes(X_test)\n\nprint(classification_report(y_test, y_pred))\n\nsns.heatmap(confusion_matrix(y_test, y_pred), \n            annot=True, fmt='.0f', \n            xticklabels=['Predicted negative', 'Predicted positive'], \n            yticklabels=['Negative', 'Positive'])","4fe82cf3":"Sentiment is converted to 0 (negative) or 1 (positive).","59338e56":"We added Dropout layers to prevent overfitting.","e2614d51":"Before we build time-consuming neural network model, simple classifier is tried.\nTfidf method is used for text vectorization.","bece3e23":"To quickly explore LSTM model, we firstly take only 500 characters. That is much faster way than taking full sentences.","e4c05c4d":"Looks better than first model.","38cef25d":"### Text cleaning","d069b3de":"## Modified LSTM model","661223d1":"This simple model is overfitted with training data as the validation loss increases.","9739d885":"# 2. Baseline linear model","fa6bbab3":"Logistic regression provides 90% accuracy for test set, which is fast and adapted for such large datasets.","d33adedc":"There is no null data.","2bd2f45f":"# 3. LSTM model","13eb575d":"Use of full-sentences would increase accurcy. But it is time-consuming and seems difficult to overwhelm simple Logistic regression model.","987e6770":"Let's build the simple LSTM model using first 500 characters datasets.","b6a4a2b7":"# 1. EDA & cleaning","a522e13a":"Let's convert tokens to ID. We will use keras Tokenizer which can filter punctuations and take only most-counted 10000 words.","20f2af70":"### Tokenize & Padding","9a171728":"We will perform sentiment analysis with IMDb. The workflow is as follows:\n1. EDA & cleaning\n2. Baseline linear model\n3. LSTM model"}}