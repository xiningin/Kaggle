{"cell_type":{"9756085b":"code","f5ac0f58":"code","5664adf8":"code","8387df56":"code","4f060f03":"code","7778f8bb":"code","05b0372c":"code","8c95f50f":"code","6ebde9ca":"markdown","b6244b66":"markdown","7c2b80eb":"markdown","0f4028ca":"markdown","b172604e":"markdown","1732c0f6":"markdown","6050c796":"markdown"},"source":{"9756085b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import datasets\nfrom torchvision import transforms as T\nfrom torch.utils.data import ConcatDataset,DataLoader\nfrom torchvision.models import densenet201\n\nfrom PIL import Image,ImageEnhance\ndevice='cuda' if torch.cuda.is_available() else 'cpu'","f5ac0f58":"model=densenet201(pretrained=True)\nclassifier_in=model.classifier.in_features\nmodel.classifier=nn.Linear(classifier_in,10)\nmodel=model.to(device)","5664adf8":"'''\ntransform=T.Compose([T.Pad(padding=1,padding_mode='constant',fill=0),T.ToTensor()])\nbatch=1000\ntraining_data=datasets.MNIST(root='.\/',train=True,download=True,transform=transform)\ntest_data=datasets.MNIST(root='.\/',train=False,download=True,transform=transform)\ntraining_data=ConcatDataset([training_data,test_data])\ntrain_loader=DataLoader(training_data,batch_size=batch)\n\ncriterion=nn.CrossEntropyLoss()\noptimiser=torch.optim.Adam(model.parameters(),lr=1e-4)\nfor epoch in range(5):\n    epoch_loss=0\n    model.train()\n    for index,sample in enumerate(train_loader):\n        image,label=sample\n        image=image.repeat(1,3,1,1)\n        image=image.to(device)\n        label=label.to(device)\n        optimiser.zero_grad()\n        \n        output=model(image)        \n        loss=criterion(output,label)\n        loss.backward()\n        optimiser.step()\n        \n        epoch_loss+=loss.item()     \n    print('Epoch:{} Training Loss:{}'.format(epoch+1,epoch_loss))\nmodel.eval()\ntorch.save(model.state_dict(),'.\/MNIST.pth')\n'''","8387df56":"weights=torch.load('..\/input\/densenet\/MNIST.pth',map_location=device)\nmodel.load_state_dict(weights)\nmodel.eval()\nprint('Weights Loaded Successfully')","4f060f03":"from urllib import request\nrequest.urlretrieve('https:\/\/i.ibb.co\/dMBzV4y\/Whats-App-Image-2021-12-31-at-7-24-37-PM.jpg','number.png')\nprint('Download Successful')","7778f8bb":"def sort_contours(cnts):\n    boundingBoxes=[cv2.boundingRect(contour) for contour in contours]\n    (cnts, boundingboxes) = zip(*sorted(zip(cnts, boundingBoxes),key=lambda b:b[1][0]))\n    return cnts,boundingboxes\ndef predict(image):\n    img=cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    img=cv2.resize(img,dsize=(16,16))\n    transform=T.Compose([T.ToPILImage(),T.Pad(padding=7,padding_mode='constant',fill=255),T.ToTensor()])    \n    img=1-transform(img)\n    #plt.imshow(img[0])\n    #plt.show()\n    img=img.repeat(1,3,1,1).to(device)\n    output=model(img)\n    _,predicted=torch.max(output,1)\n    return predicted.item()","05b0372c":"image=cv2.imread('.\/number.png')\ngray=cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)\nedged=cv2.Canny(gray,100,200)\ncontours,heirarchy = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\ncontours,boundingboxes=sort_contours(contours)\nplt.imshow(image)\nplt.show()","8c95f50f":"digits=[]\nfor box in boundingboxes:\n    x,y,w,h=box   \n    if (w >= 20 and h >= 150):\n        number=image[y:y+h,x:x+w]\n        digits.append(predict(number))\nnumber=''\nfor digit in digits:\n    number+=str(digit)\nprint(\"Predicted Number is {}\".format(number))","6ebde9ca":"For Downloading the Image on which I will be doing the OCR","b6244b66":"**Downloading the Model**\n\nHere we import the Densenet from torchvision models, last layer of densenet has been modified to predict only 10 classes, because original Densenet is trained on 1000 classes.","7c2b80eb":"**Training model on MNIST**\n\nHere, I trained the model on the MNIST Dataset for a few epochs. I have already stored the weights of the model after training and hence I will be skipping over to the next step.","0f4028ca":"If the bounding box is of appropriate size, the corresponding region is fed into Densenet for prediction.","b172604e":"**Loading Weights**\n\nHere, I loaded the already trained weights. This step will not be necessary if we had trained the model in the above step.","1732c0f6":"This is the main step where we break the image into single digits, first we convert the image to a binary one using cv2.Canny. After that, we get the contours of the image using the binary image. Once we have the contours, sort_contours sorts the contours from left to right and then draw a bounding box enclosing each contour.","6050c796":"In this Notebook, I have done optical character recognition(handwriting recognition) for numbers using Densenet transfer learning and cv2 contours.\n\n**Dataset**\n\nWhile training the model, I have used MNIST Dataset which contains 70k handwritten images of digits 0-9. \n\n**Result**\n\nI was able to recognize some of my own handwritten examples, in the end I give a demo with a three digit number but I had successfully predicted my phone number(10 digits) using this notebook.\n\n**Explanation**\n\nTo train an OCR model, we will need to do the following steps:\n1. Train a model that can recognise a single digit, I have achieved that using Densenet and MNIST dataset.\n2. Separating the given image into single digit image using cv2 contours, in the shown example, 339 will be broken down into 3,3,9.\n3. Individual digits are fed into the densenet model for classification."}}