{"cell_type":{"66589a43":"code","0c562aca":"code","7cccc6fc":"code","91be9867":"code","3bccd47d":"code","0757b458":"code","7a7f9b76":"code","6e6482c2":"code","6f2ed0ed":"code","7ef4568b":"code","e096721b":"code","bbb52ca5":"code","0209af2c":"code","8d9035ab":"code","7c32e8ed":"code","8c0ab1a0":"code","5f74a5fc":"code","a4b25286":"code","9b3f5a35":"code","c3fd9343":"code","2ba249c4":"code","7b9f4da4":"code","c188ed5b":"code","3dc3f1ab":"code","3d7aa710":"code","1080c928":"code","a9c7ea15":"code","6c952cd7":"code","ebdb6bf7":"code","2a82b87d":"code","c60d40f5":"code","5cbf7733":"code","36bf47bb":"code","7d8472d9":"code","31c910ef":"code","4fa338cc":"code","f2fcb5b8":"code","5202e57d":"code","cea0db0d":"code","bf4a9427":"markdown","f3bc202a":"markdown","b541bdf1":"markdown","63452857":"markdown","6dfe41c8":"markdown","0967fe97":"markdown","1343326a":"markdown","0fccfb79":"markdown","2aa84512":"markdown","e2c75bb1":"markdown","9c134e97":"markdown","9e69ecbc":"markdown","58cad289":"markdown","f6dfde3a":"markdown","4e3f6544":"markdown","f0c52b48":"markdown","8aa902c5":"markdown","1186dafa":"markdown","2eabebfd":"markdown","ce061be9":"markdown","02e2e7f1":"markdown","d6d70df3":"markdown","c5d702d2":"markdown","7bdd5f19":"markdown","cc72a59b":"markdown","016e3b48":"markdown","719539d1":"markdown","4158145d":"markdown","f4d9cb52":"markdown","1aa1c0d4":"markdown","286e9a4a":"markdown","684823a7":"markdown"},"source":{"66589a43":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.feature_selection import SelectKBest\nprint(\"Setup Complete\")","0c562aca":"from sklearn import linear_model","7cccc6fc":"filepath=\"..\/input\/bigmart-sales-data\/Train.csv\"","91be9867":"data=pd.read_csv(filepath)","3bccd47d":"data.shape","0757b458":"data.head()","7a7f9b76":"data.head()","6e6482c2":"data.Item_Fat_Content.unique()","6f2ed0ed":"len(data.Item_Identifier.unique())","7ef4568b":"data.describe()","e096721b":"target='Item_Outlet_Sales'\nfeatures=[col for col in data.columns if col!=target]\nprint(len(features), features)","bbb52ca5":"cate_features=list(data[features].select_dtypes(include='object').columns)\nnum_features=list(set(features)-set(cate_features))\nprint('categorical features',len(cate_features),cate_features)\nprint('num features',len(num_features),num_features)","0209af2c":"#the function which get the columns which contains the null values\ndef get_null_cols(X,cols):\n    null_cols=list(X[cols].isnull().sum()[X[cols].isnull().sum()>0].index)\n    print(\"columns which contains null\",null_cols)\n    for col in null_cols :\n        percent=(X[col].isnull().sum()\/len(X[cols]))*100\n        print(\"We have \",round(percent,2),\" % to nulls values for \",col,\"column\")\ncols = features + [target]\nget_null_cols(data,cols)","8d9035ab":"plt.figure(figsize=(14,7))\ncorr = data[num_features+[target]].corr()\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(240,10,as_cmap=True),\n            square=True)","7c32e8ed":"plt.figure()\nplt.scatter(x=data['Item_MRP'], y=data[target])\nplt.xlabel(\"Item_MRP\")\nplt.ylabel(\"Item_Ou<tlet_Sales\")","8c0ab1a0":"sns.scatterplot(x=data['Item_MRP'], y=data[target])\nplt.xlabel(\"Item_MRP\")\nplt.ylabel(\"Item_Outlet_Sales\")","5f74a5fc":"plt.figure()\nsns.displot(data[target],color=\"purple\")\nplt.show()\nprint(\"the skew (is positive):\",data[target].skew())","a4b25286":"for col in cate_features:\n    uniq_values=data[col].unique();\n    print(\"unique values for columns \",col, \" is : \",len(uniq_values),uniq_values)","9b3f5a35":"plt.figure()\ndata[['Outlet_Type', target]].groupby('Outlet_Type').median().plot(kind='bar')\nplt.show()","c3fd9343":"plt.figure()\ndata[['Item_Fat_Content',target]].groupby('Item_Fat_Content').median().plot(kind='bar')\nplt.show()","2ba249c4":"plt.figure()\ndata[['Outlet_Size',target]].groupby('Outlet_Size').median().plot(kind='bar')\nplt.show()","7b9f4da4":"plt.figure()\n#data[['Outlet_Location_Type',target]].groupby('Outlet_Location_Type').median().plot(kind='bar')\ndata[['Outlet_Location_Type',target]].groupby('Outlet_Location_Type').median().plot.barh()\nplt.show()","c188ed5b":"data.pivot_table(values='Item_Outlet_Sales', index='Item_Type').sort_values(by='Item_Outlet_Sales').plot(kind=\"barh\")","3dc3f1ab":"data.head()","3d7aa710":"for type in data['Outlet_Type'].unique():\n    print(\"\\n\",type)\n    print(data[data['Outlet_Type']==type]['Outlet_Size'].value_counts(dropna=False))","1080c928":"data[['Outlet_Identifier', target]].groupby('Outlet_Identifier').median().plot(kind='bar')\nplt.show()","a9c7ea15":"items_weight_mean = data[['Item_Identifier', 'Item_Weight']].groupby('Item_Identifier').mean()\nprint(items_weight_mean[items_weight_mean['Item_Weight'].isnull()])\nitems_weight_mean[items_weight_mean['Item_Weight'].isnull()] = data['Item_Weight'].mean()\nprint(items_weight_mean[items_weight_mean['Item_Weight'].isnull()])","6c952cd7":"def add_item_weight(row):\n    item_id = row['Item_Identifier']\n    item_weight = row['Item_Weight']\n    \n    if not pd.isnull(item_weight):\n        return item_weight\n    # else\n    return items_weight_mean['Item_Weight'][items_weight_mean.index==item_id]\n    \n# impute item_weight\ndata['Item_Weight'] = data.apply(add_item_weight, axis=1).astype(float)\nget_null_cols(data, features)","ebdb6bf7":"most_outlet_size_by_type = data.pivot_table(values='Outlet_Size', columns='Outlet_Type', aggfunc=(lambda x: x.mode()[0]))\nmost_outlet_size_by_type","2a82b87d":"def add_outlet_size(row):\n    outlet_type = row['Outlet_Type']\n    outlet_size = row['Outlet_Size']\n    \n    if not pd.isnull(outlet_size):\n        return outlet_size\n    return most_outlet_size_by_type.loc['Outlet_Size'][most_outlet_size_by_type.columns==outlet_type][0]\n\n# impute outlet_size\n\ndata['Outlet_Size'] = data.apply(add_outlet_size, axis=1)\nget_null_cols(data, features)","c60d40f5":"data.replace({'Item_Fat_Content':{'low fat':'Low Fat','LF':'Low Fat','reg':'Regular'}},inplace=True)","5cbf7733":"for col in cate_features:\n    encoder=LabelEncoder()\n    data[col]=encoder.fit_transform(data[col])","36bf47bb":"data.head()","7d8472d9":"X_train, X_valid, y_train, y_valid = train_test_split(data[features], data[target], test_size=0.3, random_state=1)","31c910ef":"# model = linear_model.LinearRegression()\n# model.fit(X_train, y_train)\n# preds = model.predict(X_valid)\n# score = r2_score(y_valid, preds)\n# print('R2:', score)","4fa338cc":"model = RandomForestRegressor(n_estimators=1000, random_state=0)\nmodel.fit(X_train, y_train)\npreds = model.predict(X_valid)\nscore = r2_score(y_valid, preds)\nprint('R2:', score)","f2fcb5b8":"model = RandomForestRegressor(random_state=0)\n\nmy_pipeline = Pipeline(steps=[\n                              ('rfr', model)\n                             ])\n\nparam_grid = [\n{'rfr__n_estimators': [100, 1000], 'rfr__max_features': [\"auto\", \"log2\"], \n 'rfr__max_depth': [None, 25]}\n]\n\ngrid_search_rfr = GridSearchCV(my_pipeline, param_grid, cv=4, scoring='r2', n_jobs=-1)\ngrid_search_rfr.fit(X_train, y_train)","5202e57d":"print(\"Best parameter (CV score=%0.3f):\" % grid_search_rfr.best_score_)\nprint(grid_search_rfr.best_params_)","cea0db0d":"model = RandomForestRegressor(n_estimators=1000, random_state=0, max_depth=None, max_features='log2')\nmodel.fit(X_train, y_train)\npreds = model.predict(X_valid)\nscore = r2_score(y_valid, preds)\nprint('R2:', score)","bf4a9427":"Let's manage categorical features with labelencoder","f3bc202a":"The correlation between \"Item_Visibility\" and \"Item_Outlet_Sales\" is less high than what i get , i thought that more item is visible more it will be sold .But the graph show the contrary , i think that it is because we have 0.0% visibility for some items .\nI guess it's probably missing data and they set 0.0% by defaul","b541bdf1":"Among the categorical characteristics, we can notice that there are ordinal characteristics. An ordinal characteristic is first of all a categorical characteristic which contains values which can be ordered. In our dataset we have:\n- 'Item_Fat_Content'\n- 'Outlet_Size'\n- 'Outlet_Type'\n- 'Outlet_Location_Type'\n\nFollowing anlysis is based on median value. When you have a skewed distribution, the median is a better measure of central tendency than the ","63452857":"Our dataset contains null values in two columns: Item_Weight and Outlet_Size. By making exploratory we found some idea about how we can deal with NaN values in these columns. \n<br>\nFor 'Item_Weight' column, we have items which weights are known except 4 items . So for these 4 items we will use the mean value of 'Item_Weight'\n<br>","6dfe41c8":"The values of the categorical features .","0967fe97":"Let's try to improve our model","1343326a":"# Eploartory Data Analysis","0fccfb79":"You almost have to transform categorigal features to numerical ? with labelencoder","2aa84512":"we make two list,one for categorical features and the second for numerical features .","e2c75bb1":"# Explore Data analysis","9c134e97":"1. Dataset(Information about Data)\n2. Exploratory Data Analysis\n3. Preproccessing\n4.Train Model","9e69ecbc":"supermarket Type1 is alone which have 'High' as Outlet Size. Supermarket Type2 and Type3 have only a Medium size for its outlets. This analysis will be helpful in preprocessing step to do imputation in order to give value which will have sense for Outlet_Size column","58cad289":"# Data Preprocessing","f6dfde3a":"Let's impute Outlet_Size by taking most frequent Size based on Outlet type.Above we found the relation between Outlet_Size and Outlet_Type .","4e3f6544":"* Observations :\nWe have 7 categorigals features, 4 numericals features ; 2 features 'Item_Weight'and 'Outlet_Size' which contains Null values .","f0c52b48":"#  **BigMart Sales Predictions**","8aa902c5":"Let's make the values of the column \"Item_Fat_Content\" uniform","1186dafa":"The main goal of this mini-projet is to predict the sales of BigMart for its various points of sales . We have in our possession the data of approximately 8323 products .\nBeyond the fact of designing a model which predicts the number of sales of a product X in a point of sale P for example, we must here try to find the determining elements which allow a product to be more sold than other","2eabebfd":"# Numerical features exploratory","ce061be9":"We have 12 coloumns which are:\n* Item_Identifier :identifier of item\n* Item_Weight : the weight of item\n* Item_Fat_Content: item fat content\n* Item_Visibility : visibility of item\n* Item_Type: type of item(meat, fruits,...)\n* Item_MRP: Maximum retail price of item\n* Outlet_Identifier : identifier of the outlet\n* Outlet_Establishment_Year : Outlet Establishment Year\n* Outlet_Size: Size of the outlet\n* Outlet_Location_Type: Location of the outlet\n* Outlet_Type : type of outlet\n* Item_Outlet_Sales: the sales of each items in the different BigMart Outlet\n","02e2e7f1":"# Model Training and parameters tuning","d6d70df3":"'Item_Outlet_Sales' is the target we have to predict with our future model .","c5d702d2":"Relashionship between the price(Item_MRP) and sales(Item_Outlet_Sales) ","7bdd5f19":"We have 12 columns and 8523 rows . The main goal is to make analysis of data and to build Machine learning model to predict the sales of each of each item at a particular outlet","cc72a59b":"the scatterplot above suggests that the price'Item_MRP' and the 'Item_Outlet_Sales' are strong correlated .","016e3b48":"# Information About Data","719539d1":"In this part we will try to analyze our data and to see the features which affects more the Item_Outlet_Sales and the relationships beetween different features .And try to see what element can increase or decrease the items sales of outlet .","4158145d":"our target is skewed , we can make a transformation(log,...) to reduce the skewness .","f4d9cb52":"# Categorical features exploratory","1aa1c0d4":"get the name of columns which contains null values","286e9a4a":"* Item_Type: The type of items allow to see her usefulness ,this feature could help to estimate how the item is loved by costomers.\n* Item_MRP: the price of product can influence her sale . we will check the correlation between Item_MRP and Item_Outlet_Sales\n* Outlet_Location_Type: The place where the supermarket is estabilished can influence her price(the price of a same item can be different for another store)  and her sales if the store is in a big city or if it is easily accesible .\n* Item_Visibility: if the product is clearly visible in the supermarket, it will be easily accessible to customers and its can be influence her sales. \n","684823a7":"# Steps"}}