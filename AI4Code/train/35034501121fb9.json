{"cell_type":{"5cdfc334":"code","fae9d45d":"code","ab470959":"code","c54cdeb5":"code","e5b6a065":"code","a605c47a":"code","48def37f":"code","a70f2ac7":"code","c7bcb7c0":"code","457d353e":"code","3831a030":"code","e7c07808":"code","033b3dae":"code","cd01f76d":"code","294fd4aa":"code","fb40e1a6":"code","c05121cd":"code","79b784e9":"code","1211e0f3":"code","0296d87e":"code","08fb487c":"code","446f0d8c":"code","dd5b84a4":"code","e008e9a0":"code","81d95975":"code","d5701a52":"code","1bd43a3a":"code","bdecffee":"code","198c00f6":"code","41587f15":"code","81a3394b":"code","8dbe1701":"code","69bf17ef":"code","f75cce11":"code","0105ce54":"code","0365312a":"code","c7683935":"code","a2ce011d":"code","6a2618a1":"code","bbcbf106":"code","5357ce5f":"code","81c0ae96":"code","00a9bffc":"code","c48075ac":"code","2901b806":"code","4d7ae359":"code","a737cf00":"code","ad9ad135":"code","3f223a09":"code","a306f902":"code","b8b82e48":"code","b673361f":"code","618b7f03":"code","f7086460":"code","71181d81":"code","c7db2b40":"code","3ee2c35d":"code","b52d90b9":"code","94f9be6f":"code","1931766d":"code","899f95ee":"code","5156f837":"code","be976a0d":"code","40c8d908":"code","6e7292ad":"code","0ada65b7":"code","845bc41c":"code","609ae24d":"code","196972b5":"code","a69da140":"code","d9dac567":"code","ac172f43":"code","02db79f9":"code","b193c086":"code","5fc73d4e":"code","8a4803a9":"code","55483c82":"code","0dda9c7f":"code","1155e17d":"code","6713ee5d":"code","c1239949":"code","cd6e0cd9":"code","1fc94836":"code","b562d995":"code","eda73751":"code","06b8ee0c":"code","917cf4af":"code","bc548ebc":"code","130c3fca":"code","5d2f927e":"code","a2341633":"code","cc9f02ca":"code","345455f1":"code","b63e5f52":"code","6dd539c2":"code","0b398cc5":"code","5b7cf2fb":"code","1829badd":"code","89a80aa6":"code","bf7f33d1":"code","69ed462c":"code","6740133f":"code","fdc1ca53":"code","2bf3516d":"markdown","132d26c7":"markdown","482ce30f":"markdown","00fd494d":"markdown","c2464b85":"markdown","36f118da":"markdown","21b66803":"markdown","d19e845e":"markdown","a9e2e9cc":"markdown","88ff9ac7":"markdown","7abb1093":"markdown","c7522ef6":"markdown","1eb6e1c9":"markdown","d8be8cb9":"markdown","2e6a194e":"markdown","562695ca":"markdown","e1e26d41":"markdown","cd81bff8":"markdown","a95ec72f":"markdown","86f4f6f5":"markdown","1e3c05ad":"markdown","6b128031":"markdown","a664b904":"markdown","d0671937":"markdown","55e0163f":"markdown","6d743053":"markdown","171b269d":"markdown","c018de11":"markdown","37214340":"markdown","f30dff69":"markdown","772c508a":"markdown","a7b8b9d8":"markdown","44bcada2":"markdown","12c0c429":"markdown","599c2d18":"markdown","c5cbf452":"markdown","470dfd54":"markdown","88079b8d":"markdown","5e8e71eb":"markdown","82134031":"markdown","1bfb33c6":"markdown","bff603eb":"markdown","c3e436ee":"markdown","8203c116":"markdown","75f825c4":"markdown","b9675cc7":"markdown","c30b6c90":"markdown","50459731":"markdown","cdc4e788":"markdown","7d3a0fe6":"markdown","cbf975b2":"markdown","09818457":"markdown","a513cc3b":"markdown","76d4102a":"markdown","7e3ebe64":"markdown","54424f1f":"markdown","74aa87d5":"markdown","a3e70057":"markdown","19b3cf38":"markdown","85d06e44":"markdown","13f9a3b2":"markdown","eee3e920":"markdown","e0942760":"markdown","07544def":"markdown","950bb136":"markdown","9dbcae56":"markdown","17d1d72a":"markdown","25234bdc":"markdown","52b87650":"markdown","4eb81514":"markdown","8ee814ed":"markdown","8bc2ec78":"markdown","67394be3":"markdown","6550e9d4":"markdown","20051e2a":"markdown","1fcfc27a":"markdown","c8e37cc8":"markdown","5fc4110a":"markdown","a5899003":"markdown"},"source":{"5cdfc334":"!pip install imblearn","fae9d45d":"### IMPORT: ------------------------------------\nimport scipy.stats as stats \nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport warnings\n\nimport statsmodels.api as sm\n#--Sklearn library--\nfrom sklearn.model_selection import train_test_split,StratifiedKFold, cross_val_score # Sklearn package's randomized data splitting function\n\nfrom sklearn import metrics\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import (\n    AdaBoostClassifier,\n    GradientBoostingClassifier,\n    RandomForestClassifier,\n    BaggingClassifier,\n    StackingClassifier\n)\n\nfrom xgboost import XGBClassifier\nfrom sklearn import tree\nfrom sklearn.linear_model import LogisticRegression\n\n# To impute missing values\nfrom sklearn.impute import KNNImputer\n# Libtune to tune model, get different metric scores\n\nfrom sklearn.metrics import  classification_report, accuracy_score, precision_score, recall_score,f1_score\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay,plot_confusion_matrix #to plot confusion matric\n\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\n\npd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('display.max_rows', 300)\npd.set_option('display.max_columns', 25)\npd.set_option('display.max_colwidth',200)\n# To supress numerical display in scientific notations\npd.set_option('display.float_format', lambda x: '%.5f' % x) \nwarnings.filterwarnings('ignore') # To supress warnings\n # set the background for the graphs\nplt.style.use('ggplot')\n# For pandas profiling\nfrom pandas_profiling import ProfileReport\nprint('Load Libraries-Done')\n","ab470959":"#Reading the Excel file  used tourism.xlsx \ndata_path='..\/input\/bankcurners\/BankChurners.csv'\n\ndf=pd.read_csv(data_path)\n\n\ndf_credit=df.copy()\nprint(f'There are {df_credit.shape[0]} rows and {df_credit.shape[1]} columns') # fstring ","c54cdeb5":"# View the first  5 rows of the dataset.\ndf_credit.head()","e5b6a065":"# last 5 rows\ndf_credit.tail()","a605c47a":"#Understand the  dataset.\n#get the size of dataframe\nprint (\"Rows     : \" , df_credit.shape[0])  #get number of rows\/observations\nprint (\"Columns  : \" , df_credit.shape[1]) #get number of columns\nprint (\"#\"*40,\"\\n\",\"Features : \\n\\n\", df_credit.columns.tolist()) #get name of columns\/features\nmissing_df = pd.DataFrame({\n    \"Missing\": df_credit.isnull().sum(),\n    \"Missing %\": round((df_credit.isnull().sum()\/ df_credit.isna().count()*100), 2)\n})\ndisplay(missing_df.sort_values(by='Missing', ascending=False))","48def37f":"#### Check the data types of the columns for the dataset.\ndf_credit.info()","a70f2ac7":"df_credit.describe().T","c7bcb7c0":"df_credit.drop(['CLIENTNUM'],axis=1,inplace=True)","457d353e":"\ncat_cols = ['Attrition_Flag','Gender', 'Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category','Dependent_count','Total_Relationship_Count','Months_Inactive_12_mon','Contacts_Count_12_mon']\n","3831a030":"\nfor col in cat_cols:\n    print(f\"Feature: {col}\")\n    print(\"-\"*40)\n    display(pd.DataFrame({\"Counts\": df_credit[col].value_counts(dropna=False)}).sort_values(by='Counts', ascending=False))","e7c07808":"## Converting the data type of categorical features to 'category'\n\ncat_cols = ['Attrition_Flag','Gender', 'Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category','Dependent_count','Total_Relationship_Count','Months_Inactive_12_mon','Contacts_Count_12_mon']\n\ndf_credit[cat_cols] = df_credit[cat_cols].astype('category')\ndf_credit.info()","033b3dae":"df_credit.describe(include=['category']).T","cd01f76d":"df_credit.Customer_Age.describe()","294fd4aa":"df_credit['Agebin'] = pd.cut(df_credit['Customer_Age'], bins = [25, 35,45,55,65, 75], labels = ['25-35', '36-45', '46-55', '56-65','66-75'])","fb40e1a6":"df_credit.Agebin.value_counts()","c05121cd":"def dist_box(data):\n # function plots a combined graph for univariate analysis of continous variable \n #to check spread, central tendency , dispersion and outliers  \n    Name=data.name.upper()\n    fig,(ax_box,ax_dis)  =plt.subplots(nrows=2,sharex=True,gridspec_kw = {\"height_ratios\": (.25, .75)},figsize=(8, 5))\n    mean=data.mean()\n    median=data.median()\n    mode=data.mode().tolist()[0]\n    sns.set_theme(style=\"white\")\n    fig.suptitle(\"SPREAD OF DATA FOR \"+ Name  , fontsize=18, fontweight='bold')\n    sns.boxplot(x=data,showmeans=True, orient='h',color=\"tan\",ax=ax_box)\n    ax_box.set(xlabel='')\n     # just trying to make visualisation better. This will set background to white\n    sns.despine(top=True,right=True,left=True) # to remove side line from graph\n    sns.distplot(data,kde=False,color='red',ax=ax_dis)\n    ax_dis.axvline(mean, color='r', linestyle='--',linewidth=2)\n    ax_dis.axvline(median, color='g', linestyle='-',linewidth=2)\n    plt.legend({'Mean':mean,'Median':median})\n                    ","79b784e9":"#select all quantitative columns for checking the spread\nlist_col=  df_credit.select_dtypes(include='number').columns.to_list()\nfor i in range(len(list_col)):\n    dist_box(df_credit[list_col[i]])","1211e0f3":"# Making a list of all categorical variables\n\nplt.figure(figsize=(15,20))\n\nsns.set_theme(style=\"white\") \nfor i, variable in enumerate(cat_cols):\n                     plt.subplot(9,2,i+1)\n                     order = df_credit[variable].value_counts(ascending=False).index   \n                     #sns.set_palette(list_palette[i]) # to set the palette\n                     sns.set_palette('twilight_shifted')\n                     ax=sns.countplot(x=df_credit[variable], data=df_credit )\n                     sns.despine(top=True,right=True,left=True) # to remove side line from graph\n                     for p in ax.patches:\n                           percentage = '{:.1f}%'.format(100 * p.get_height()\/len(df_credit[variable]))\n                           x = p.get_x() + p.get_width() \/ 2 - 0.05\n                           y = p.get_y() + p.get_height()\n                           plt.annotate(percentage, (x, y),ha='center')\n                     plt.tight_layout()\n                     plt.title(cat_cols[i].upper())\n                                     \n","0296d87e":"sns.set_palette(sns.color_palette(\"Set2\", 8))\nplt.figure(figsize=(15,12))\nsns.heatmap(df_credit.corr(),annot=True)\nplt.show()","08fb487c":"sns.set_palette(sns.color_palette(\"Set1\", 8))\nsns.pairplot(df_credit, hue=\"Attrition_Flag\",corner=True)\nplt.show()","446f0d8c":"### Function to plot distributions and Boxplots of customers\ndef plot(x,target='Attrition_Flag'):\n    fig,axs = plt.subplots(2,2,figsize=(12,10))\n    axs[0, 0].set_title(f'Distribution of {x} \\n of a existing customer',fontsize=12,fontweight='bold')\n    sns.distplot(df_credit[(df_credit[target] == 'Existing Customer')][x],ax=axs[0,0],color='teal')\n    axs[0, 1].set_title(f\"Distribution of {x}\\n of a  attrited customer \",fontsize=12,fontweight='bold')\n    sns.distplot(df_credit[(df_credit[target] == 'Attrited Customer')][x],ax=axs[0,1],color='orange')\n    axs[1,0].set_title(f'Boxplot of {x} w.r.t attrited customer',fontsize=12,fontweight='bold')\n    \n    line = plt.Line2D((.1,.9),(.5,.5), color='grey', linewidth=1.5,linestyle='--')\n    fig.add_artist(line)\n   \n    sns.boxplot(df_credit[target],df_credit[x],ax=axs[1,0],palette='gist_rainbow',showmeans=True)\n    axs[1,1].set_title(f'Boxplot of {x} w.r.t Attrited customer - Without outliers',fontsize=12,fontweight='bold')\n    sns.boxplot(df_credit[target],df_credit[x],ax=axs[1,1],showfliers=False,palette='gist_rainbow',showmeans=True) #turning off outliers from boxplot\n    sns.despine(top=True,right=True,left=True) # to remove side line from graph\n    plt.tight_layout(pad=4)\n    plt.show()","dd5b84a4":"#select all quantitative columns for checking the spread\n#list_col=  ['Age','DurationOfPitch','MonthlyIncome']\nlist_col=df_credit.select_dtypes(include='number').columns.to_list()\n#print(list_col)\n#plt.figure(figsize=(14,23))\nfor j in range(len(list_col)):\n   plot(list_col[j])\n   ","e008e9a0":"plt.figure(figsize=(10,5)) \nsns.set_palette(sns.color_palette(\"tab20\", 8))\n\nsns.barplot(y='Credit_Limit',x='Income_Category',hue='Attrition_Flag',data=df_credit)\nsns.despine(top=True,right=True,left=True) # to remove side line from graph\nplt.legend(bbox_to_anchor=(1.00, 1))\nplt.title('Income vs credit')","81d95975":"plt.figure(figsize=(10,5)) \nsns.set_palette(sns.color_palette(\"tab20\", 9))\nsns.barplot(y='Credit_Limit',x='Education_Level',hue='Attrition_Flag',data=df_credit)\nsns.despine(top=True,right=True,left=True) # to remove side line from graph\nplt.legend(bbox_to_anchor=(1.00, 1))\nplt.title('CustomerAge  vs Education')\n","d5701a52":"plt.figure(figsize=(10,5)) \nsns.set_palette(sns.color_palette(\"tab20\", 9))\nsns.barplot(x='Agebin',y='Credit_Limit',hue='Attrition_Flag',data=df_credit)\nsns.despine(top=True,right=True,left=True) # to remove side line from graph\nplt.legend(bbox_to_anchor=(1.00, 1))\nplt.title('CustomerAge  vs Credit limit')\n","1bd43a3a":"plt.figure(figsize=(10,5)) \nsns.set_palette(sns.color_palette(\"tab20\", 9))\nsns.barplot(x='Agebin',y='Total_Revolving_Bal',hue='Attrition_Flag',data=df_credit)\nsns.despine(top=True,right=True,left=True) # to remove side line from graph\nplt.legend(bbox_to_anchor=(1.00, 1))\nplt.title('CustomerAge  vs Total Revolving Balance')","bdecffee":"plt.figure(figsize=(10,5)) \nsns.set_palette(sns.color_palette(\"tab20\", 9))\nsns.barplot(x='Agebin',y='Total_Trans_Amt',hue='Attrition_Flag',data=df_credit)\nsns.despine(top=True,right=True,left=True) # to remove side line from graph\nplt.legend(bbox_to_anchor=(1.00, 1))\nplt.title('CustomerAge  vs Total Transcational Amount')","198c00f6":"\nplt.figure(figsize=(10,5)) \nsns.barplot(y='Credit_Limit',x='Gender',hue='Attrition_Flag',data=df_credit) \nsns.despine(top=True,right=True,left=True) # to remove side line from graph\nplt.legend(bbox_to_anchor=(1.00, 1))\nplt.title('Credit limit  vs Gender')","41587f15":"\nplt.figure(figsize=(10,5))\nsns.barplot(y='Credit_Limit',x='Card_Category',hue='Attrition_Flag',data=df_credit) \nsns.despine(top=True,right=True,left=True) # to remove side line from graph\nplt.legend(bbox_to_anchor=(1.00, 1))\nplt.title('Credit Limit  vs Card Category')","81a3394b":"plt.figure(figsize=(10,5))\nsns.barplot(y='Total_Trans_Amt',x='Card_Category',hue='Attrition_Flag',data=df_credit) \nsns.despine(top=True,right=True,left=True) # to remove side line from graph\nplt.legend(bbox_to_anchor=(1.00, 1))\nplt.title('Total Transcation Amount  vs Card')","8dbe1701":"plt.figure(figsize=(10,5))\nsns.barplot(y='Total_Trans_Ct',x='Card_Category',hue='Attrition_Flag',data=df_credit) \nsns.despine(top=True,right=True,left=True) # to remove side line from graph\nplt.legend(bbox_to_anchor=(1.00, 1))\nplt.title('Total Transcation Count  vs Card Category')","69bf17ef":"## Function to plot stacked bar chart\ndef stacked_plot(x):\n    sns.set_palette(sns.color_palette(\"tab20\", 8))\n    tab1 = pd.crosstab(x,df_credit['Attrition_Flag'],margins=True)\n    display(tab1)\n    tab = pd.crosstab(x,df_credit['Attrition_Flag'],normalize='index')\n    tab.plot(kind='bar',stacked=True,figsize=(9,5))\n    plt.xticks(rotation=360)\n    #labels=[\"No\",\"Yes\"]\n    plt.legend(loc='lower left', frameon=False,)\n    plt.legend(loc=\"upper left\",title=\" \",bbox_to_anchor=(1,1))\n    sns.despine(top=True,right=True,left=True) # to remove side line from graph\n    #plt.legend(labels)\n    plt.show()","f75cce11":"cat_cols.append(\"Agebin\")\nfor i, variable in enumerate(cat_cols):\n       stacked_plot(df_credit[variable])","0105ce54":"#Profile of Attrited Customer with Blue Card \ndf_credit[(df_credit['Card_Category']=='Blue') & (df_credit['Attrition_Flag']=='Attrited Customer')].describe(include='all').T","0365312a":"#Profile of Attrited Customer with gold Card \ndf_credit[(df_credit['Card_Category']=='Gold') & (df_credit['Attrition_Flag']=='Attrited Customer')].describe(include='all').T","c7683935":"#Profile of Attrited Customer with silver  Card \ndf_credit[(df_credit['Card_Category']=='Silver') & (df_credit['Attrition_Flag']=='Attrited Customer')].describe(include='all').T","a2ce011d":"#Profile of Attrited Customer with platinum Card \ndf_credit[(df_credit['Card_Category']=='Platinum') & (df_credit['Attrition_Flag']=='Attrited Customer')].describe(include='all').T","6a2618a1":"Q1 = df_credit.quantile(0.25)             #To find the 25th percentile and 75th percentile.\nQ3 = df_credit.quantile(0.75)\n\nIQR = Q3 - Q1                           #Inter Quantile Range (75th perentile - 25th percentile)\n\nlower=Q1-1.5*IQR                        #Finding lower and upper bounds for all values. All values outside these bounds are outliers\nupper=Q3+1.5*IQR","bbcbf106":"((df_credit.select_dtypes(include=['float64','int64'])<lower) | (df_credit.select_dtypes(include=['float64','int64'])>upper)).sum()\/len(df_credit)*100","5357ce5f":"numeric_columns = df_credit.select_dtypes('number').columns.to_list()\n# outlier detection using boxplot\nplt.figure(figsize=(20,30))\n\nfor i, variable in enumerate(numeric_columns):\n                     plt.subplot(4,4,i+1)\n                     plt.boxplot(df_credit[variable],whis=1.5)\n                     plt.tight_layout()\n                     plt.title(variable)\n\nplt.show()","81c0ae96":"print(upper)","00a9bffc":"df_credit[df_credit['Credit_Limit'] > upper.Credit_Limit].sort_values(by='Credit_Limit',ascending=False ).count()","c48075ac":"df_credit[df_credit['Credit_Limit']== 34516.00000].count() # had seen this number during EDA so verifying\n","2901b806":"df_credit[df_credit['Total_Trans_Amt'] > upper.Total_Trans_Amt].sort_values(by='Total_Trans_Amt',ascending=False ).head(10)","4d7ae359":"df_credit[df_credit['Avg_Open_To_Buy'] > upper.Avg_Open_To_Buy].sort_values(by='Avg_Open_To_Buy',ascending=False ).head(10)","a737cf00":"df_credit = df_credit.replace({'Unknown': None})\n","ad9ad135":"df_credit.isnull().sum()","3f223a09":"df_credit.info()","a306f902":"# Label Encode categorical variables  \nattrition = {'Existing Customer':0, 'Attrited Customer':1}\ndf_credit['Attrition_Flag']=df_credit['Attrition_Flag'].map(attrition)\n\nmarital_status = {'Married':1,'Single':2, 'Divorced':3}\ndf_credit['Marital_Status']=df_credit['Marital_Status'].map(marital_status)\n\n\neducation = {'Uneducated':1,'High School':2, 'Graduate':3, 'College':4, 'Post-Graduate':5, 'Doctorate':6}\ndf_credit['Education_Level']=df_credit['Education_Level'].map(education)\n\nincome = {'Less than $40K':1,'$40K - $60K':2, '$60K - $80K':3, '$80K - $120K':4, '$120K +':5}\ndf_credit['Income_Category']=df_credit['Income_Category'].map(income)\n\n","b8b82e48":"imputer = KNNImputer(n_neighbors=5)","b673361f":"reqd_col_for_impute = ['Income_Category','Education_Level','Marital_Status']","618b7f03":"# Separating target column\nX = df_credit.drop(['Agebin','Attrition_Flag','Avg_Open_To_Buy'],axis=1)\n#X = pd.get_dummies(X,drop_first=True)\ny = df_credit['Attrition_Flag']","f7086460":"# Splitting the data into train and test sets in 70:30 ratio\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1,stratify=y)\nX_train.shape, X_test.shape","71181d81":"#Fit and transform the train data\nX_train[reqd_col_for_impute]=imputer.fit_transform(X_train[reqd_col_for_impute])\n\n#Transform the test data \nX_test[reqd_col_for_impute]=imputer.transform(X_test[reqd_col_for_impute])","c7db2b40":"#Checking that no column has missing values in train or test sets\nprint(X_train.isnull().sum())\nprint('-'*30)\nprint(X_test.isnull().sum())","3ee2c35d":"## Function to inverse the encoding\ndef inverse_mapping(x,y):\n    inv_dict = {v: k for k, v in x.items()}\n    X_train[y] = np.round(X_train[y]).map(inv_dict).astype('category')\n    X_test[y] = np.round(X_test[y]).map(inv_dict).astype('category')","b52d90b9":"\ninverse_mapping(education,'Education_Level')\ninverse_mapping(marital_status,'Marital_Status')\ninverse_mapping(income,'Income_Category')\n\n","94f9be6f":"X_train=pd.get_dummies(X_train,drop_first=True)\nX_test=pd.get_dummies(X_test,drop_first=True)\nprint(X_train.shape, X_test.shape)","1931766d":"X_train","899f95ee":"# # defining empty lists to add train and test results \nmodel_name=[]\nacc_train = []\nacc_test = []\nrecall_train = []\nrecall_test = []\nprecision_train = []\nprecision_test = []\nf1_train = []\nf1_test = []\n\ndef make_confusion_matrix(y_actual,y_predict,title):\n    '''Plot confusion matrix'''\n    fig, ax = plt.subplots(1, 1)\n    \n    cm = confusion_matrix(y_actual, y_predict, labels=[0,1])\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                               display_labels=[\"No\",\"Yes\"])\n    disp.plot(cmap='Blues',ax=ax)\n    \n    ax.set_title(title)\n    plt.tick_params(axis=u'both', which=u'both',length=0)\n    plt.grid(b=None,axis='both',which='both',visible=False)\n    plt.show()","5156f837":"def get_metrics_score(model,modelname,X_train_pass,X_test_df_pass,y_train_pass,y_test_pass):\n    '''\n    Function to calculate different metric scores of the model - Accuracy, Recall, Precision, and F1 score\n    model: classifier to predict values of X\n    train, test: Independent features\n    train_y,test_y: Dependent variable\n    threshold: thresold for classifiying the observation as 1\n    '''\n    # defining an empty list to store train and test results\n    score_list=[]\n    \n    pred_train = model.predict(X_train_pass)\n    pred_test = model.predict(X_test_df_pass)\n    pred_train = np.round(pred_train)\n    pred_test = np.round(pred_test)\n    train_acc = accuracy_score(y_train_pass,pred_train)\n    test_acc = accuracy_score(y_test_pass,pred_test)\n    train_recall = recall_score(y_train_pass,pred_train)\n    test_recall = recall_score(y_test_pass,pred_test)\n    train_precision = precision_score(y_train_pass,pred_train)\n    test_precision = precision_score(y_test_pass,pred_test)\n    train_f1 = f1_score(y_train_pass,pred_train)\n    test_f1 = f1_score(y_test_pass,pred_test)\n    score_list.extend((train_acc,test_acc,train_recall,test_recall,train_precision,test_precision,train_f1,test_f1))\n    model_name.append(modelname)  \n    acc_train.append(score_list[0])\n    acc_test.append(score_list[1])\n    recall_train.append(score_list[2])\n    recall_test.append(score_list[3])\n    precision_train.append(score_list[4])\n    precision_test.append(score_list[5])\n    f1_train.append(score_list[6])\n    f1_test.append(score_list[7])\n    metric_names = ['Train_Accuracy', 'Test_Accuracy', 'Train_Recall', 'Test_Recall','Train_Precision',\n                          'Test_Precision', 'Train_F1-Score', 'Test_F1-Score']\n    cols = ['Metric', 'Score']\n    records = [(name, score) for name, score in zip(metric_names, score_list)]\n    display(pd.DataFrame.from_records(records, columns=cols, index='Metric').T)\n    # display confusion matrix\n    make_confusion_matrix(y_train_pass,pred_train,\"Confusion Matrix for Train\")     \n    make_confusion_matrix(y_test_pass,pred_test,\"Confusion Matrix for Test\") \n    return score_list # returning the list with train and test scores","be976a0d":"#Initialize model using pipeline\npipe_lr = make_pipeline( StandardScaler(), (LogisticRegression(random_state=1)))\n\n#Fit on train data\npipe_lr.fit(X_train,y_train)","40c8d908":"lr_score=get_metrics_score(pipe_lr,'LogisticRegression',X_train,X_test,y_train,y_test)","6e7292ad":"#Evaluate the model performance by using KFold and cross_val_score\nscoring='recall'\nkfold=StratifiedKFold(n_splits=5,shuffle=True,random_state=1)     #Setting number of splits equal to 5\nlr_cv_result=cross_val_score(estimator=pipe_lr, X=X_train, y=y_train, scoring=scoring, cv=kfold)\n\n#Plotting boxplots for CV scores of model defined above\nplt.boxplot(lr_cv_result)\nplt.show()","0ada65b7":"print(f\"Before UpSampling, counts of label attrited customer: {sum(y_train==1)}\")\nprint(f\"Before UpSampling, counts of label existing customer: {sum(y_train==0)} \\n\")\n\nsm = SMOTE(sampling_strategy = 1 ,k_neighbors = 5, random_state=1)   #Synthetic Minority Over Sampling Technique\nX_train_over, y_train_over = sm.fit_resample(X_train, y_train.ravel())\n\nprint(f\"After UpSampling, counts of label attrited customer: {sum(y_train_over==1)}\")\nprint(f\"After UpSampling, counts of label existing customer: {sum(y_train_over==0)} \\n\")\n\nprint(f'After UpSampling, the shape of train_X: {X_train_over.shape}')\nprint(f'After UpSampling, the shape of train_y: {y_train_over.shape} \\n')","845bc41c":"lr_over = LogisticRegression(solver='liblinear')\nlr_over.fit(X_train_over, y_train_over)","609ae24d":"lr_score_over=get_metrics_score(lr_over,'LogisticRegression with over sampling',X_train_over,X_test,y_train_over,y_test)","196972b5":"# Choose the type of classifier. \npipe_lr_reg = make_pipeline( StandardScaler(), (LogisticRegression(random_state=1)))\n\n# Grid of parameters to choose from\nparameters = {'logisticregression__C': np.arange(0.007,0.5,0.01),\n              'logisticregression__solver' : ['liblinear','newton-cg','lbfgs','sag','saga'],\n              'logisticregression__penalty': ['l1','l2']\n             }\n\n# Run the grid search\ngrid_obj = RandomizedSearchCV(pipe_lr_reg, parameters, scoring='recall',n_jobs=-1)\ngrid_obj = grid_obj.fit(X_train_over, y_train_over)\n\n# Set the clf to the best combination of parameters\npipe_lr_reg = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \npipe_lr_reg.fit(X_train_over, y_train_over)","a69da140":"lr_score_under=get_metrics_score(pipe_lr_reg,'LogisticRegression with Regularization on Over sampling',X_train_over,X_test,y_train_over,y_test)","d9dac567":"rus = RandomUnderSampler(random_state = 1) # Undersample dependent variable\nX_train_under, y_train_under = rus.fit_resample(X_train, y_train)\n#Undersample to balance classes\nprint(\"Before Under Sampling, counts of label 'Attrited': {}\".format(sum(y_train==1)))\nprint(\"Before Under Sampling, counts of label 'Existing': {} \\n\".format(sum(y_train==0)))\n\nprint(\"After Under Sampling, counts of label 'Attrited': {}\".format(sum(y_train_under==1)))\nprint(\"After Under Sampling, counts of label 'Existing': {} \\n\".format(sum(y_train_under==0)))\n\nprint('After Under Sampling, the shape of train_X: {}'.format(X_train_under.shape))\nprint('After Under Sampling, the shape of train_y: {} \\n'.format(y_train_under.shape))\n                                          ","ac172f43":"# Initialize model using pipeline\npipe_lr_under = make_pipeline( StandardScaler(), (LogisticRegression(random_state=1)))\n\n# Training the basic logistic regression model with training set \npipe_lr_under.fit(X_train_under,y_train_under)","02db79f9":"#Evaluate the model performance by using KFold and cross_val_score\nscoring='recall'\nkfold=StratifiedKFold(n_splits=5,shuffle=True,random_state=1)     #Setting number of splits equal to 5\ncv_result_under=cross_val_score(estimator=pipe_lr_under, X=X_train_under, y=y_train_under, scoring=scoring, cv=kfold)\n\n#Plotting boxplots for CV scores of model defined above\nplt.boxplot(cv_result_under)\nplt.show()","b193c086":"lr_score_under=get_metrics_score(pipe_lr_under,'LogisticRegression with under sampling',X_train_under,X_test,y_train_under,y_test)\n","5fc73d4e":"# Choose the type of classifier. \npipe_lr_reg_under = make_pipeline( StandardScaler(), (LogisticRegression(random_state=1)))\n\n# Grid of parameters to choose from\nparameters = {'logisticregression__C': np.arange(0.007,0.5,0.01),\n              'logisticregression__solver' : ['liblinear','newton-cg','lbfgs','sag','saga'],\n              'logisticregression__penalty': ['l1','l2']\n             }\n\n# Run the grid search\ngrid_obj = RandomizedSearchCV(pipe_lr_reg_under, parameters, scoring='recall',n_jobs=-1)\ngrid_obj = grid_obj.fit(X_train_under, y_train_under)\n\n# Set the clf to the best combination of parameters\npipe_lr_reg_under = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \npipe_lr_reg_under.fit(X_train_under, y_train_under)","8a4803a9":"lr_score_reg=get_metrics_score(pipe_lr_reg_under,'LogisticRegression with Regularization on Undersampled',X_train_under,X_test,y_train_under,y_test)\n","55483c82":"comparison_frame = pd.DataFrame({'Model':model_name,\n                                          'Train_Accuracy': acc_train,'Test_Accuracy': acc_test,\n                                          'Train_Recall':recall_train,'Test_Recall':recall_test,\n                                          'Train_Precision':precision_train,'Test_Precision':precision_test,\n                                          'Train_F1':f1_train,\n                                          'Test_F1':f1_test  }) \n\n#Sorting models in decreasing order of test recall\ncomparison_frame.sort_values(by='Test_Recall',ascending=False)","0dda9c7f":"models = []  # Empty list to store all the models\n\n# Appending pipelines for each model into the list\nmodels.append(\n    (\n        \"DTREE\",\n        Pipeline(\n            steps=[\n                (\"scaler\", StandardScaler()),\n                (\"decision_tree\", DecisionTreeClassifier(random_state=1)),\n            ]\n        ),\n    )\n)\n\nmodels.append(\n    (\n        \"RF\",\n        Pipeline(\n            steps=[\n                (\"scaler\", StandardScaler()),\n                (\"random_forest\", RandomForestClassifier(random_state=1)),\n            ]\n        ),\n    )\n)\n\nmodels.append(\n    (\n        \"BG\",\n        Pipeline(\n            steps=[\n                (\"scaler\", StandardScaler()),\n                (\"bagging\", BaggingClassifier(random_state=1)),\n            ]\n        ),\n    )\n)\nmodels.append(\n    (\n        \"GBM\",\n        Pipeline(\n            steps=[\n                (\"scaler\", StandardScaler()),\n                (\"gradient_boosting\", GradientBoostingClassifier(random_state=1)),\n            ]\n        ),\n    )\n)\nmodels.append(\n    (\n        \"ADB\",\n        Pipeline(\n            steps=[\n                (\"scaler\", StandardScaler()),\n                (\"adaboost\", AdaBoostClassifier(random_state=1)),\n            ]\n        ),\n    )\n)\nmodels.append(\n    (\n        \"XGB\",\n        Pipeline(\n            steps=[\n                (\"scaler\", StandardScaler()),\n                (\"xgboost\", XGBClassifier(random_state=1,eval_metric='logloss')),\n            ]\n        ),\n    )\n)\n\n\nresults = []  # Empty list to store all model's CV scores\nnames = []  # Empty list to store name of the models\n\n# loop through all models to get the mean cross validated score\nfor name, model in models:\n    scoring = \"recall\"\n    kfold = StratifiedKFold(\n        n_splits=5, shuffle=True, random_state=1\n    )  # Setting number of splits equal to 5\n    cv_result = cross_val_score(\n        estimator=model, X=X_train, y=y_train, scoring=scoring, cv=kfold\n    )\n    results.append(cv_result)\n    names.append(name)\n    print(\"{}: {}\".format(name, cv_result.mean() * 100))","1155e17d":"# Plotting boxplots for CV scores of all models defined above\nfig = plt.figure(figsize=(10, 7))\n\nfig.suptitle(\"Algorithm Comparison\")\nax = fig.add_subplot(111)\n\nplt.boxplot(results)\nax.set_xticklabels(names)\n\nplt.show()","6713ee5d":"%%time\n# Creating pipeline\npipe_ada_grid = make_pipeline(StandardScaler(), AdaBoostClassifier(random_state=1))\n\n# Parameter grid to pass in GridSearchCV\nparam_grid = {\n    \"adaboostclassifier__n_estimators\": np.arange(10, 110, 10),\n    \"adaboostclassifier__learning_rate\": [0.1, 0.01, 0.2, 0.05, 1],\n    \"adaboostclassifier__base_estimator\": [\n        DecisionTreeClassifier(max_depth=1, random_state=1),\n        DecisionTreeClassifier(max_depth=2, random_state=1),\n        DecisionTreeClassifier(max_depth=3, random_state=1),\n    ],\n}\n# Type of scoring used to compare parameter combinations\nscorer = metrics.make_scorer(metrics.recall_score)\n\n# Calling GridSearchCV\npipe_ada_grid = GridSearchCV(estimator=pipe_ada_grid, param_grid=param_grid, scoring=scorer, cv=5,n_jobs = -1)\n\n# Fitting parameters with undersampled train data in GridSeachCV\npipe_ada_grid.fit(X_train, y_train)\n                              \nprint(\"Best parameters are {} with CV score={}:\" .format(pipe_ada_grid.best_params_,pipe_ada_grid.best_score_))","c1239949":"# Creating new pipeline with best parameters\nabc_tuned_grid = make_pipeline(\n    StandardScaler(),AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2, \n                                                                              random_state=1),\n                                        learning_rate=1, n_estimators=70))\n\n# Fit the model on undersampled training data\nabc_tuned_grid.fit(X_train, y_train)","cd6e0cd9":"abc_tuned_score=get_metrics_score(abc_tuned_grid,' Adaboost with Grid Search',X_train,X_test,y_train,y_test)\n","1fc94836":"feature_names = X_train.columns\nimportances = abc_tuned_grid[1].feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(12, 12))\nplt.title(\"Feature Importances\")\nplt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel(\"Relative Importance\")\nplt.show()","b562d995":"%%time\n\n# Creating pipeline\npipe_ada_ran = make_pipeline(StandardScaler(), AdaBoostClassifier(random_state=1))\n\n# Parameter grid to pass in GridSearchCV\nparam_grid = {\n    \"adaboostclassifier__n_estimators\": np.arange(10, 110, 10),\n    \"adaboostclassifier__learning_rate\": [0.1, 0.01, 0.2, 0.05, 1],\n    \"adaboostclassifier__base_estimator\": [\n        DecisionTreeClassifier(max_depth=1, random_state=1),\n        DecisionTreeClassifier(max_depth=2, random_state=1),\n        DecisionTreeClassifier(max_depth=3, random_state=1),\n    ],\n}\n# Type of scoring used to compare parameter combinations\nscorer = metrics.make_scorer(metrics.recall_score)\n\n#Calling RandomizedSearchCV\nabc_rand_cv = RandomizedSearchCV(estimator=pipe_ada_ran, param_distributions=param_grid, n_iter=10,n_jobs = -1, scoring=scorer, cv=5, random_state=1)\n\n#Fitting parameters in RandomizedSearchCV\nabc_rand_cv.fit(X_train,y_train)\n\n\nprint(\"Best parameters are {} with CV score={}:\" .format(abc_rand_cv.best_params_,abc_rand_cv.best_score_))","eda73751":"# Creating new pipeline with best parameters\nabc_tuned_rand = make_pipeline(\n    StandardScaler(),AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=2,\n                                                                              random_state=1),\n                                        learning_rate=1, n_estimators=90))\n\n# Fit the model on training data\nabc_tuned_rand.fit(X_train, y_train)\n","06b8ee0c":"abc_rand_tuned_score=get_metrics_score(abc_tuned_rand,' Adaboost with Random Search',X_train,X_test,y_train,y_test)\n","917cf4af":"feature_names = X_train.columns\nimportances = abc_tuned_rand[1].feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(12, 12))\nplt.title(\"Feature Importances\")\nplt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel(\"Relative Importance\")\nplt.show()","bc548ebc":"%%time\n# Creating pipeline\npipe_gb_grid = make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state=1))\n\n# Grid of parameters to choose from\nparam_grid = {'gradientboostingclassifier__n_estimators':[100,200],\n              'gradientboostingclassifier__max_depth':[10,20],\n              'gradientboostingclassifier__min_samples_leaf': [10,20],\n              'gradientboostingclassifier__min_samples_split': [25,35]\n              }\n# Type of scoring used to compare parameter combinations\nscorer = metrics.make_scorer(metrics.recall_score)\n\n# Run the grid search\ngrid_cv = GridSearchCV(pipe_gb_grid, param_grid, scoring=scorer,cv=5,n_jobs = -1)\n\n# Fitting parameters in GridSeachCV\npipe_gb_grid = grid_cv.fit(X_train, y_train)\n\n\nprint(\"Best parameters are {} with CV score={}:\" .format(pipe_gb_grid.best_params_,grid_cv.best_score_))","130c3fca":"# Creating new pipeline with best parameters\ngb_tuned_grid = make_pipeline(\n    StandardScaler(),GradientBoostingClassifier(max_depth=20,\n                                            min_samples_leaf=20,\n                                            min_samples_split=25,\n                                            n_estimators=200, random_state=1\n                                            ))\n\n# Fit the model on training data\ngb_tuned_grid.fit(X_train, y_train)","5d2f927e":"gb_tuned_score=get_metrics_score(gb_tuned_grid,' Gradient with Grid Search',X_train,X_test,y_train,y_test)","a2341633":"feature_names = X_train.columns\nimportances = gb_tuned_grid[1].feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(12, 12))\nplt.title(\"Feature Importances\")\nplt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel(\"Relative Importance\")\nplt.show()","cc9f02ca":"%%time \npipe_gb_rand = make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state=1))\n\nparam_grid = {'gradientboostingclassifier__n_estimators':[100,200],\n              'gradientboostingclassifier__max_depth':[10,20],\n              'gradientboostingclassifier__min_samples_leaf': [10,20],\n              'gradientboostingclassifier__min_samples_split': [25,35]\n              }\n\n\nscorer = metrics.make_scorer(metrics.recall_score)\n\n#Calling RandomizedSearchCV\npipe_gb_rand = RandomizedSearchCV(estimator=pipe_gb_rand, param_distributions=param_grid,n_jobs = -1, n_iter=10, scoring=scorer, cv=5, random_state=1)\n\n#Fitting parameters in RandomizedSearchCV\npipe_gb_rand.fit(X_train,y_train)\n\nprint(\"Best parameters are {} with CV score={}:\" .format(pipe_gb_rand.best_params_,pipe_gb_rand.best_score_))","345455f1":"gb_tuned_rand = make_pipeline(\n    StandardScaler(),GradientBoostingClassifier(max_depth=20, min_samples_leaf=20,\n                                            min_samples_split=25,\n                                            n_estimators=200,\n                                            random_state=1))\n\n# Fit the model on training data\ngb_tuned_rand.fit(X_train, y_train)","b63e5f52":"gb_rand_tuned_score=get_metrics_score(gb_tuned_rand,' Gradient boosting with Random Search',X_train,X_test,y_train,y_test)","6dd539c2":"feature_names = X_train.columns\nimportances = gb_tuned_rand[1].feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(12, 12))\nplt.title(\"Feature Importances\")\nplt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel(\"Relative Importance\")\nplt.show()","0b398cc5":"%%time \n\n#Creating pipeline\n#Creating pipeline\npipe_xgboost=make_pipeline(StandardScaler(), XGBClassifier(random_state=1,eval_metric='logloss'))\n\n#Parameter grid to pass in GridSearchCV\nparam_grid={'xgbclassifier__n_estimators':np.arange(50,300,50),'xgbclassifier__scale_pos_weight':[2,10],\n            'xgbclassifier__learning_rate':[0.01,0.1,0.2], \n            'xgbclassifier__subsample':[0.7,1]}\n\n\n# Type of scoring used to compare parameter combinations\nscorer = metrics.make_scorer(metrics.recall_score)\n\n#Calling GridSearchCV\nXgboost_grid_cv = GridSearchCV(estimator=pipe_xgboost, param_grid=param_grid, scoring=scorer, cv=5, n_jobs = -1)\n\n#Fitting parameters in GridSeachCV\nXgboost_grid_cv.fit(X_train,y_train)\n\n\nprint(\"Best parameters are {} with CV score={}:\" .format(Xgboost_grid_cv.best_params_,Xgboost_grid_cv.best_score_))","5b7cf2fb":"# Creating new pipeline with best parameters\nxgb_tuned_grid = make_pipeline(\n    StandardScaler(),\n    XGBClassifier(\n        random_state=1,\n        n_estimators=150,\n        scale_pos_weight=10,\n        subsample=1,\n        learning_rate=0.01,\n        eval_metric='logloss',\n    ),\n)\n\n# Fit the model on training data\nxgb_tuned_grid.fit(X_train, y_train)","1829badd":"xgb_tuned_score_grid=get_metrics_score(xgb_tuned_grid,' XGboost with Grid Search',X_train,X_test,y_train,y_test)","89a80aa6":"%%time \n#Creating pipeline\n\npipe_xgboost_ran=make_pipeline(StandardScaler(), XGBClassifier(random_state=1,eval_metric='logloss'))\n\n#Parameter grid to pass in random\nparam_grid={'xgbclassifier__n_estimators':np.arange(50,300,50),'xgbclassifier__scale_pos_weight':[2,10],\n            'xgbclassifier__learning_rate':[0.01,0.1,0.2], \n            'xgbclassifier__subsample':[0.7,1]}\n\n\n# Type of scoring used to compare parameter combinations\nscorer = metrics.make_scorer(metrics.recall_score)\n\n#Calling RandomizedSearchCV\nrandomized_cv = RandomizedSearchCV(estimator=pipe_xgboost_ran, param_distributions=param_grid,n_jobs = -1, n_iter=10, scoring=scorer, cv=5, random_state=1)\n\n#Fitting parameters in RandomizedSearchCV\nrandomized_cv.fit(X_train,y_train)\n\nprint(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))","bf7f33d1":"# Creating new pipeline with best parameters\nxgb_rand = make_pipeline(\n    StandardScaler(),\n    XGBClassifier(\n        random_state=1,\n        n_estimators=50,\n        scale_pos_weight=10,\n        subsample=0.7,\n        learning_rate=0.01,\n        eval_metric='logloss',\n    ),\n)\n\n# Fit the model on training data\nxgb_rand.fit(X_train, y_train)","69ed462c":"randomized_cv_tuned_score=get_metrics_score(randomized_cv,'XG boosting with Random Search',X_train,X_test,y_train,y_test)\n","6740133f":"comparison_frame = pd.DataFrame({'Model':model_name,\n                                          'Train_Accuracy': acc_train,'Test_Accuracy': acc_test,\n                                          'Train_Recall':recall_train,'Test_Recall':recall_test,\n                                          'Train_Precision':precision_train,'Test_Precision':precision_test,\n                                          'Train_F1':f1_train,\n                                          'Test_F1':f1_test  }) \n\n#Sorting models in decreasing order of test recall\ncomparison_frame.sort_values(by='Test_Recall',ascending=False)","fdc1ca53":"\nfeature_names = X_train.columns\nimportances = xgb_tuned_grid[1].feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(12, 12))\nplt.title(\"Feature Importances\")\nplt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel(\"Relative Importance\")\nplt.show()","2bf3516d":"**Observations**\n- ~16% of credit card customers attrited.\n-  ~ 52 % are female customers who have credit cards.\n- ~ 30 % customers are graduate. There are very few post graduate and doctorate customers.\n- ~46 % are married customers. 7.4 % unknown status needs to be imputed.\n- ~ 35% earn less than 40 k.\n- ~ 93 % have blue card. Very less customers have a plantinum card.\n- ~22 % have more than 3 bank products \n- ~38 % are inactive from 3 months. Customers who are inactive from 4,5,6 month should be investigated more to see if there is any relationship with attrition\n- ~60 % for contacted 2-3 times in 12 month.","132d26c7":"* Lower transcation count on credit card , less revolving balance , less transcational amount are an indication that customer will attrite. Lower transcation indicate customer is not using this credit card , bank should offer more rewards or cashback  or some other offers to customer to use the credit card more.\n* As per the EDA if customer hold more product with the bank he\/she is less likely to attrite.Bank can offer more product to  such customers so they buy  more products which will help retain such customers\n* Customers who have been inactive for a month show high chances of attrition.Bank should focus on such customers as well.\n* Avg utilization ratio is lower amongst attrited customers.\n* As per EDA Customer in age range 36-55 ,who were doctorate or postgraduate ,or Female attrited more. One of the reasons can be some competitive bank is offering them better deals leading to lesser user of this banks credit card.\n* As per the EDA Customers who have had high number of contacts with the bank in the last 12 months have attrited. This needs to be investigated whether there were any  issues of customers which were not resolved leading into customer leaving the bank.\n\n\n","482ce30f":"**Observations**\n- Customer age and number of books are highly correlated.\n- credit limit and Avg utlization ration has some negative correlation.\n- Total revolving balance and average utlization are positively correlated.\n- Average opening balance is negatively correlated to avg utlization ratio.\n- There is very little correlation between total transfer amount and credit limit\n- As expected there is very high correlation total transfer amount and total transfer count.\n- Credit limit and Average open to buy is fully correlated, we can drop one of them.\n- It is also logical that Total_Trans_Amt  is correlated  to Total_Amt_Chng_Q4_Q1,total ct_change_q4_Q1 . These features seems to be derived from Total_Trans_Amt. May be we can drop one of these columns.","00fd494d":"### GradientBoosting with Grid Search","c2464b85":"### Regularization on Oversampled dataset","36f118da":"**Profile of customer who attrited most based on there card type**\n- #### Blue Card\n    - Most likely  Female who were married ,  age group 46-55  and earning less than 40 k, Education level graduate and dependent member 3 , total bank product 3 and were inactive for 3 months. There average utilzation ratio was very low\n    \n- #### Gold Card\n    - Most likely Male who are single , between age group 36-45  earning 60- 80k, education level graduate and inactive for 3 months\n- #### Silver Card\n    - Most likely Male who are single , between age group 46-55 , earned between 80 k -120 k ,education level graduate and inactive for 3 months\n- #### Platinum card\n    - Most likely Female who were single , age group 46-55 ,earning less than 40 k , education level graduate and inactive for 3 months\n\n","21b66803":"**Let's evaluate the model performance by using KFold and cross_val_score**\n\nK-Folds cross-validation provides dataset indices to split data into train\/validation sets. Split dataset into k consecutive stratified folds (without shuffling by default). Each fold is then used once as validation while the k - 1 remaining folds form the training set.","d19e845e":"The Thera bank recently saw a steep decline in the number of users of their credit card, credit cards are a good source of income for banks because of different kinds of fees charged by the banks like annual fees, balance transfer fees, and cash advance fees, late payment fees, foreign transaction fees, and others. Some fees are charged to every user irrespective of usage, while others are charged under specified circumstances.\n\nCustomers\u2019 leaving credit cards services would lead bank to loss, so the bank wants to analyze the data of customers and identify the customers who will leave their credit card services and reason for same \u2013 so that bank could improve upon those areas\n\n**Objective**\n- Explore and visualize the dataset.\n- Build a classification model to predict if the customer is going to churn or not\n- Optimize the model using appropriate techniques\n- Generate a set of insights and recommendations that will help the bank","a9e2e9cc":"### GradientBoosting with Random Search","88ff9ac7":"### Adaboost Using Random Search","7abb1093":"<h2 style = \"font-family:TimesNewRoman;color:black;font-weight:bold\">Table of Contents<\/h2>\n\n- [Context](#Context) \n- [Data Dictionary](#Data-Dictionary)\n- [Problem](#Problem)\n- [Libraries](#Libraries)\n- [Read and Understand Data](#Read-and-Understand-data)\n- [Data Preprocessing](#Data-Preprocessing)\n- [Exploratory Data Analysis](#Exploratory-Data-Analysis) \n- [Insights based on EDA](#Insights-based-on-EDA)\n- [Missing value Detection and Treatment](#Missing-value-Detection-and-Treatment)   \n- [Outlier Detection](#Outlier-Detection)\n- [Model Building Logistic Regression](#Model-Building-Logistic-Regression)\n- [HyperParameter Tuning](#Hyperparameter-Tuning) \n- [Conclusion](#Conclusion) \n- [Business Recommendations & Insights](#Business-Recommendations-&-Insights)\n \n","c7522ef6":"**Observation**\n- There is no difference in Age, months on book,credit limit,average open to buy, of attrited and existing customers. it doesnt seem to have any relation with attrition. \n- It seems existing customers have a higher Total Revolving Balance than customers who attrited.\n- Customers with lesser transaction amount spend and low change in transaction_spend_Q1_Q4 were more likely to attrite. \n- The customers with low number of transactions and low change in number of transactions between Q1 and Q4 attrited.\n- On average, customers  with less utlization attrited.\n","1eb6e1c9":"The recall on test data has improved let see if undersampling can improve the recall","d8be8cb9":"508 customer have credit limit at 34516, it seems to be some default value.","2e6a194e":"The recall on test data is only 0.48 ,and model is overfitting there is lot of discrepancy between test score and train score. let try regularization","562695ca":"<h3 style = \"font-size:40px; font-family:Garamond ; font-weight : normal; background-color: #0667BB; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\">Exploratory Data Analysis<\/h3>","e1e26d41":"**Observations**\n-  Customer Age is almost Normally disturbuted, with some outlier on higher end.\n- Month on book has maximum distrubution around ~35-36.Most customer have credit card for this long. It has many outliers on lower and higher end.\n- Credit card limit is right skewed , with a sudden pick at 35000, as seen before this is maxiumum limit and seems to be some kind of default value.There are lot of outliers on higher end. Customers above 25000 need to beinvestigated further.\n- Total Revolving bal seems to have different  disturbution with many customers with ~0 revolving balance and then it follows almost normal distrubution and then  sudden peak at 2500.\n- Average open to buy has same distribution as Credit card limit.\n- Total Amt change  has lot of outliers on lower and upper end. There are some 3.5 ratio of  total amount change from Q4 to Q1,this  customers which needs to be investigated further.\n- Total trans amt also has very different distrubution  with data between 0 -2500 , then 2500-5000, and then 750-10000 and then 12500-17500. It has lot of outliers on higher end.\n- Total_trans_ct also has 3 modal with outliers on higher end.\n- Total ct change q4_q1 has normal like disturbtion with lot of outliers on higher and lower end.\n-  Avg_utlization ration is measure of  how much of the given credit limit is the customer actually using. it ranges from 0.0 to 1\n","cd81bff8":"- Logistic Regression with oversampling performed very poorly on test data. The recall was only 0.48.\n- The xgboost model tuned using Grid search is giving the best test recall of 0.95. The model can 93% time accuractely predict customers who will attrite.Precision is very low for this model.\n- Time take by Random search was less compared to time taken by Grid search, but that doesn't necessarily mean that the performance was better .  Since not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions random search is faster. The number of parameter settings that are tried is given by n_iter. All set of hyperparameters is not searched sequentially.Random search doesn\u2019t guarantee finding the best set of hyperparameters.Grid search did slightly better in case of XGboost and Adaboost.\n- The performance can vary with range of hyperparmeters selected. With more number of hyperparemter grid search will probably take more time.\n- Let's see the feature importance from the tuned xgboost model.","a95ec72f":"<h2 style = \"font-size:40px; font-family:Garamond ; font-weight : normal; background-color: #0667BB; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\">Credit Card Users Churn Prediction<\/h2>","86f4f6f5":"<h3 style = \"font-size:40px; font-family:Garamond ; font-weight : normal; background-color: #0667BB; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\">Insights based on EDA<\/h3>\n\n- ~16% customer attrited .\n- Female customer attrited more compared to male.\n- Customers who were single attrited more.\n- Customers who earned more than 120k and less than 40k.\n- Customers with plantinum card attrited more but there are only 20 samples so this is inclusive. Customers with gold attrited more compared to blue and silver card.\n- Customer in age range 36-55 attrited more.\n- Customers who were doctorate or postgraduate attrited most. \n- Surprising  Attrition has been higher when there is higher number of contacts with the Bank in the last 12 months.\n","1e3c05ad":"[Top](#Table-of-Contents)","6b128031":"#### Age\n\nAge can be a vital factor in tourism, converting ages to bin to explore if there is any pattern","a664b904":"896 customers has transcational amount greater than 8619.25000.With number of transcation count this data seems to be correct.","d0671937":"### Observation\n\n-  Model after undersampling is  generalized well on training and test set . Our recall after undersampling on test was better than our recall after oversampling on test.Let try regularization and see. Trying to use all the solver and different penality","55e0163f":"## Conclusion\n- Random Grid search takes less time and tries to choose best parameters , but that doesnt necessarily mean it will perform well.\n- Different hyperparamters can be tried to improve some of the model.\n- XGbooost with grid search performed best.\n- Total Transcation count is most important features followed by Total Revolving balance and Total Transacational amount.\n- Customers lower transcation , lower revolving balance , lower transcational amount are an indication that customer will attrite.","6d743053":"\n<h3 style = \"font-size:40px; font-family:Garamond ; font-weight : normal; background-color: #0667BB; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\">Read and Understand data<\/h3>","171b269d":"- Random search took less time but recall was not better than Grid search","c018de11":"<h3 style = \"font-size:40px; font-family:Garamond ; font-weight : normal; background-color: #0667BB; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\">Libraries<\/h3>","37214340":"Logistic Regression with Under sampling is giving a generalized model and best recall with 0.857. ","f30dff69":"### Over Sampling\nSince dataset is imbalanced  let try oversampling using SMOTE and see if performance can be improved.","772c508a":"- We can see that XGBoost is giving the highest cross-validated recall  with just one outlier followed by Gradient Boost, Adaboost. Bagging classifier had maxiumum recall ~ 84 but the minimum was 74 resulting into mean being only ~79. therefore I didn't choose bagging classfier.\n- Best performing three models are XGBoost model, Gradient Boost, Adaboost.\n- We will tune our 3 best models  to see if the performance improves after tuning","a7b8b9d8":"### Model evaluation criterion:\n\n#### Model can make wrong predictions as:\n1. Predicting a customer will churn  but he does not - Loss of resources\n2. Predicting a customer will not churn the services but he does - Loss of income\n\n#### Which case is more important? \n* Predicting that customer will not churn but he does i.e. losing on a potential source of income for the bank  . Bank can  taken actions to stop these customer from churning.\n\n#### How to reduce this loss i.e need to reduce False Negatives?\n* Banks wants Recall to be maximized, greater the Recall lesser the chances of false negatives  means lesser chances of predicting customers will not churn when in reality they do.","44bcada2":"<h2 style = \"font-family:TimesNewRoman;color:black;font-weight:bold\">Business Recommendations & Insights<\/h2>","12c0c429":"### Encoding categorical variables","599c2d18":"<h1 style = \"font-family:TimesNewRoman;color:black;font-weight:bold\"> Model Performance Evaluation and Improvement-Logistic Regression<\/h1>","c5cbf452":"- Grid search took a significantly longer time than random search. \n- Both the model perform the same , both have same recall and are overfitting","470dfd54":"<h3 style = \"font-size:40px; font-family:Garamond ; font-weight : normal; background-color: #0667BB; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\">Missing Value Detection & Treatment<\/h3>\n","88079b8d":"**Observation**\n- Customer with 35-55 were given more credit limit.\n- Plantinum card holder had higher credit limit\n- Customer earning more than 120 k had higher credit limit\n- Male customer were given more credit limit than female","5e8e71eb":"#### Droping CLIENTNUM","82134031":"Not treating outliers here, and want alogorthims to learn about this outliers.","1bfb33c6":"### Split the dataset","bff603eb":"**Observations**\n- Average customer age is ~46 and max customer age is 73.\n- Average period of relationship with the bank is ~35 months with minimum of 13 and max as 56.\n- Maximum Total number of product  held by customer is 6 and on average is ~4.\n- Mean Credit_limit 8631 while median is 4549 , indicates data has outliers and right skewed.\n- Total_Revolving_Bal has mean as 1162.\n- Avg_Open_To_Buy has mean 7469 and max as 34516 .This number had appeared before in credit limit. This seems to be some default value. Distrubution is right skewed with some outliers on higher end.\n- Total_Amt_Chng_Q4_Q1 median is 0.73600 and mean is 0.75994.\n- Total_Trans_Amt has an average of 4404 and median of 3899. This indicate data is right skewed with outliers on higher end \n- Total_Trans_Ct has an average value of 64.8 and median of 67. This ndicates slight skewness to the right.\n- Total_Ct_Chng_Q4_Q1 has an average of 0.71 and median value of 0.702.\n- Avg_Utilization_Ratio is right skewed with an average of 0.27 and median at 0.176.\n","c3e436ee":"**Observations**\n- Female customer attrited more compared to male.\n- Customers who were doctorate or postgraduate attrited most.\n- Customers who were single attrited more.\n- Customers who earned more than 120k and less than 40k.\n- Customers with plantinum card attrited more but there are only 20 samples so this is inclusive. Customers with gold attrited more compared to blue and silver card. May be analyzing profile of customers with different card help us in identfying some pattern here.\n- Custimer with 3 dependent attrited more.\n- Customer having 1 or 2 bank product attrited more compared to customers with more bank products.\n- Customers who were never inactive attrited most.we can't be sure about this we have only 29 samples.Customers who were inactive for 4 months attrited most followed by 3 month and 5 month.\n- This is surpising , customer who were contacted most in last 12 month attrited.Did bank had any information about there attrition which was a reason bank was contacting those customers so many times.? or so much of contact from bank lead to attrition\n- Customer in age range 66-75 attrited most , but this is inclusive since we have only 18 samples.Customer in age range 36-55 attrited more.\n","8203c116":"<h1 style = \"font-family:TimesNewRoman;color:black;font-weight:bold\">Model Building <\/h1> ","75f825c4":"### XGBclassifier with Grid Search","b9675cc7":"### Missing-Value Treatment\n\n* We will use KNN imputer to impute missing values.\n k-Nearest Neighbours (kNN)  identifies the neighboring points through a measure of distance and the missing values can be estimated using completed values of neighboring observations.\n* `KNNImputer`: Each sample's missing values are imputed by looking at the n_neighbors nearest neighbors found in the training set. Default value for n_neighbors=5.\n* KNN imputer replaces missing values using the average of k nearest non-missing feature values.\n* Nearest points are found based on euclidean distance.","c30b6c90":"[Top](#Table-of-Contents)","50459731":"\n<h3 style = \"font-size:40px; font-family:Garamond ; font-weight : normal; background-color: #0667BB; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\">Data Dictionary<\/h3>\n\n- CLIENTNUM: Client number. Unique identifier for the customer holding the account\n- Attrition_Flag: Internal event (customer activity) variable - if the account is closed then \"Attrited Customer\" else \"Existing Customer\"\n- Customer_Age: Age in Years\n- Gender: Gender of the account holder\n- Dependent_count: Number of dependents\n- Education_Level:  Educational Qualification of the account holder - Graduate, High School, Unknown, Uneducated, College(refers to a college student), Post-Graduate, Doctorate.\n- Marital_Status: Marital Status of the account holder\n- Income_Category: Annual Income Category of the account holder\n- Card_Category: Type of Card\n- Months_on_book: Period of relationship with the bank\n- Total_Relationship_Count: Total no. of products held by the customer\n- Months_Inactive_12_mon: No. of months inactive in the last 12 months\n- Contacts_Count_12_mon: No. of Contacts between the customer and bank in the last 12 months\n- Credit_Limit: Credit Limit on the Credit Card\n- Total_Revolving_Bal: The balance that carries over from one month to the next is the revolving balance\n- Avg_Open_To_Buy: Open to Buy refers to the amount left on the credit card to use (Average of last 12 months)\n- Total_Trans_Amt: Total Transaction Amount (Last 12 months)\n- Total_Trans_Ct: Total Transaction Count (Last 12 months)\n- Total_Ct_Chng_Q4_Q1: Ratio of the total transaction count in 4th quarter and the total transaction count in 1st quarter\n- Total_Amt_Chng_Q4_Q1: Ratio of the total transaction amount in 4th quarter and the total transaction amount in 1st quarter\n- Avg_Utilization_Ratio: Represents how much of the available credit the customer spent\n\n\n**What Is a Revolving Balance?**\n\nIf we don't pay the balance of the revolving credit account in full every month, the unpaid portion carries over to the next month. That's called a revolving balance\n\n**What is the Average Open to buy?**\n\n'Open to Buy' means the amount left on your credit card to use. Now, this column represents the average of this value for the last 12 months.\n\n**What is the Average utilization Ratio?**\n\nThe Avg_Utilization_Ratio represents how much of the available credit the customer spent. This is useful for calculating credit scores.\n\n**Relation b\/w Avg_Open_To_Buy, Credit_Limit and Avg_Utilization_Ratio:**\n\n( Avg_Open_To_Buy \/ Credit_Limit ) + Avg_Utilization_Ratio = 1\n","cdc4e788":"Total Transcation count is most important features followed by Total Revolving balance and Total Transacational amount.","7d3a0fe6":"<h3 style = \"font-size:40px; font-family:Garamond ; font-weight : normal; background-color: #0667BB; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\">Outlier Detection<\/h3>\n","cbf975b2":"**Observations**\n- 1657 customers has attrited.\n-  Education level,Income,martial status has `Unknown` category , this will have to be treated as missing value and will have to be imputed.\n- Blue card has maxiumum customers.","09818457":"<h1 style = \"font-family:TimesNewRoman;color:black;font-weight:bold\">  Model building Decision Tree ,Bagging and Boosting<\/h1>\n\nHere I am building different models using KFold and cross_val_score with pipelines and  will tune the best model 3 models using GridSearchCV and RandomizedSearchCV\n\nStratified K-Folds cross-validation provides dataset indices to split data into train\/validation sets. Split dataset into k consecutive folds (without shuffling by default) keeping the distribution of both classes in each fold the same as the target variable. Each fold is then used once as validation while the k - 1 remaining folds form the training set.","a513cc3b":"### Undersampling \nLet see try undersampling and see if performance is different.","76d4102a":"[Top](#Table-of-Contents)","7e3ebe64":"There are Unknown values for the columns Education_Level,Marital_Status & Income_Category which can be treated as missing values. Replacing Unknown with nan","54424f1f":"<h2 style = \"font-family:TimesNewRoman;color:black;font-weight:bold\">Problem<\/h2> \n\n- Does Income has any effect on Attrition .?\n- Does Sex has any relation on Attrition.?\n- What are the signs of attrition .?\n\n","74aa87d5":"### Logistic Regression on undersampled data","a3e70057":"**What is Regularization ?**\n\nLinear regression algorithm works by selecting coefficients for each independent variable that minimizes a loss function. However, if the coefficients are large, they can lead to over-fitting on the training dataset, and such a model will not generalize well on the unseen test data.This is where regularization helps. Regularization is the process which regularizes or shrinks the coefficients towards zero. In simple words, regularization discourages learning a more complex or flexible model, to prevent overfitting.\n\n**Main  Regularization Techniques**\n\n**Ridge Regression (L2 Regularization)**\n\n`Ridge regression` adds \u201csquared magnitude\u201d of coefficient as penalty term to the loss function.\n\n\n**Lasso Regression (L1 Regularizaion)**\n\n`Lasso` adds  \"absolute values of magnitude  of coefficient  as penalty term to the loss function\n\n**Elastic Net Regression**\n\n`Elastic net regression` combines the properties of ridge and lasso regression. It works by penalizing the model using both the 1l2-norm1 and the 1l1-norm1. \n\nElastic Net Formula: Ridge + Lasso\n","19b3cf38":"\n<h3 style = \"font-size:40px; font-family:Garamond ; font-weight : normal; background-color: #0667BB; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\">Context<\/h3>","85d06e44":"**Handling Imbalanced dataset**\n\nThis is an Imbalanced dataset .A problem with imbalanced classification is that there are too few examples of the minority class for a model to effectively learn the decision boundary.\n\nOne way to solve this problem is to oversample the examples in the minority class. This can be achieved by simply duplicating examples from the minority class in the training dataset prior to fitting a model. This can balance the class distribution but does not provide any additional information to the model.One approach to addressing imbalanced datasets is to oversample the minority class. The simplest approach involves duplicating examples in the minority class, although these examples don\u2019t add any new information to the model. Instead, new examples can be synthesized from the existing examples. This is a type of data augmentation for the minority class and is referred to as the **`Synthetic Minority Oversampling Technique`**, or SMOTE for short.","13f9a3b2":"<h3 style = \"font-size:40px; font-family:Garamond ; font-weight : normal; background-color: #0667BB; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\">Data Preprocessing<\/h3>","eee3e920":"What is Customer Churn?\nCustomer churn means a customer\u2019s ending their relationship with a bank\/company for any reason. Although churn is inevitable at a certain level, a high customer churn rate is a reason for failing to reach the business goals. So identifying customers who would churn is very important for business","e0942760":"- Recall has improved by ~9% using grid search and hyperparameter.","07544def":"[Top](#Table-of-Contents)","950bb136":"- Here Random  search took a less  time , but the recall has improved with the  random search.False negative cases have reduced.\n- Grid search took a significantly longer time than random search. ","9dbcae56":"* Since we have a significant imbalance in the distribution of the target classes, we will use stratified sampling to ensure that relative class frequencies are approximately preserved in train and test sets. \n* For that we will use the `stratify` parameter in the train_test_split function.\n* Dropping Avg_open_to_buy has it is highly correlated with credit limit","17d1d72a":"### XGboost using Random Search","25234bdc":"[Top](#Table-of-Contents)","52b87650":"[Top](#Table-of-Contents)","4eb81514":"# Model Building Logistic Regression","8ee814ed":"- Grid search gave a better recall than cross validation. \n- Model is overfitting. Let see how randomized search perform.","8bc2ec78":"* Performance on training set is in range  between 0.58 to 0.66 recall with the average recall being 0.61","67394be3":"- The test recall has increased by ~6% as compared to cross-validated recall\n- Model is generalized , let see if random search give different result","6550e9d4":"### Adaboost Using Grid Search","20051e2a":"**Observations**\n\n- Customer_Age,Credit_Limit,Total_Revolving_Bal,Avg_Open_To_Buy,Total_Amt_Chng_Q4_Q1,Total_Trans_Amt,Total_Trans_Ct, Total_Ct_Chng_Q4_Q1,Avg_Utilization_Ratio, are all continous varaibles, rest are categorical variables\n- Attrition_Flag is the Target variable.\n- There are no missing values.","1fcfc27a":"### Comparing all models","c8e37cc8":"[Top](#Table-of-Contents)","5fc4110a":"#### Summary of the dataset.","a5899003":"# Hyperparameter Tuning\n\nWe will use pipelines with StandardScaler and classifiers model and tune the model using GridSearchCV and RandomizedSearchCV. We will also compare the performance and time taken by these two methods - grid search and randomized search.\n\n**Random Search** . Define a search space as a bounded domain of hyperparameter values and randomly sample points in that domain.\n\n**Grid Search** Define a search space as a grid of hyperparameter values and evaluate every position in the grid.\n\nWe can also use the make_pipeline function instead of Pipeline to create a pipeline.\n\n**`make_pipeline`: This is a shorthand for the Pipeline constructor; it does not require and does not permit, naming the estimators. Instead, their names will be set to the lowercase of their types automatically.**"}}