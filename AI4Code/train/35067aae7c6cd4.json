{"cell_type":{"37052068":"code","29628e61":"code","cee2f148":"code","3fed4e5b":"code","488eaf04":"code","62fd24cb":"code","312cfc6b":"code","2d1fd9e6":"code","00dd2161":"code","4b96d712":"code","a2b8f063":"code","87126fd4":"code","c25af55b":"code","c1ca42cb":"code","d7975784":"code","b417d7d4":"code","05840d1a":"code","30589657":"code","0c1395b6":"code","334e36ba":"code","d0780b90":"code","32481956":"code","fc2be1f8":"code","f1d971df":"code","7504cd3b":"code","de1e8af3":"code","2ab5c1f7":"code","40a501c6":"code","fd8d3dd3":"code","897d4350":"code","1f768a93":"code","55cab826":"code","14920d0a":"code","0fcba8d4":"code","eb53dc8a":"markdown","9698b623":"markdown","621ce4a9":"markdown","8ff52860":"markdown","9a55165c":"markdown","31f11247":"markdown","0a5858cc":"markdown","3ea4e4de":"markdown","b64d6c09":"markdown","8fe6305c":"markdown","a2218773":"markdown","5fbaa172":"markdown","120e9241":"markdown","e7824810":"markdown","e05b4a64":"markdown","78c6de74":"markdown","8efb8cdb":"markdown","4797cdf5":"markdown","084faf07":"markdown","e75a9b0d":"markdown","d9ba731e":"markdown","f500dbf0":"markdown","3c27cfd8":"markdown","d7a4335d":"markdown","f83f8356":"markdown","fa0063a5":"markdown","a07adc50":"markdown","99fd903a":"markdown","b5b1d833":"markdown","4877ba32":"markdown","4b0482dc":"markdown","bdf9308a":"markdown"},"source":{"37052068":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","29628e61":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom itertools import product\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.svm import LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import StackingClassifier\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","cee2f148":"# read the data \nks_project = pd.read_csv('\/kaggle\/input\/kickstarter-projects\/ks-projects-201801.csv')\nks_project.head()","3fed4e5b":"ks_project.info()","488eaf04":"# firstly we change deadline and launched to datetime\nks_project['deadline'] = pd.to_datetime(ks_project['deadline'])\nks_project['launched'] = pd.to_datetime(ks_project['launched'])\nks_project.info()","62fd24cb":"# search how many null there are and how the rate is ?\nprint('Number of null')\nprint(ks_project.isnull().sum())\nprint('-'*100)\nprint('Rate of null in each cols')\nprint(ks_project.isnull().sum() \/ len(ks_project) * 100)","312cfc6b":"ks_project_dropna = ks_project.dropna()\n# check how many null there are and how the rate is ?\nprint('Number of null')\nprint(ks_project_dropna.isnull().sum())\nprint('-'*100)\nprint('Rate of null in each cols')\nprint(ks_project_dropna.isnull().sum() \/ len(ks_project_dropna) * 100)","2d1fd9e6":"each_rate = ks_project_dropna['state'].value_counts() \/ len(ks_project_dropna['state']) * 100\nprint(f'{each_rate}%')\nprint()\n\nfigure = plt.figure(figsize=(10, 10))\nplt.pie(ks_project_dropna['state'].value_counts(), labels=ks_project_dropna['state'].unique(), autopct=\"%1.1f %%\", textprops={\"size\": 20})\nplt.title('Check the rate of each state value', fontsize=25, color='blue')\nplt.tight_layout()","00dd2161":"ks_project_dropna= ks_project_dropna.replace({'state': {'canceled': 'failed', 'suspended': 'failed'}})\nks_project_dropna = ks_project_dropna[ks_project_dropna.state.isin(['failed', 'successful'])]\n# check the data\nks_project_dropna['state'].value_counts()","4b96d712":"# copy to save for the moment\nks_project_copy = ks_project_dropna.copy()","a2b8f063":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\nsns.violinplot(np.log(ks_project_copy['usd pledged'] + 1e-9), ax=ax1)\nsns.violinplot(np.log(ks_project_copy['usd_pledged_real'] + 1e-9), ax=ax2)","87126fd4":"usd_df = ks_project_copy[['pledged', 'usd pledged', 'usd_pledged_real', 'usd_goal_real']]\nprint(usd_df.corr())\nprint()\n\nfig, axes = plt.subplots(2, 2, figsize=(15, 15), facecolor='lightblue')\nsns.scatterplot(x='usd pledged', y='pledged', data=usd_df, ax=axes[0, 0])\nsns.scatterplot(x='usd pledged', y='usd_pledged_real', data=usd_df, ax=axes[0, 1])\nsns.scatterplot(x='usd pledged', y='usd_goal_real', data=usd_df, ax=axes[1, 0])\nsns.scatterplot(x='usd_pledged_real', y='usd_goal_real', data=usd_df, ax=axes[1, 1])\n\nplt.suptitle('USD Co-relation!!', fontsize=25)\naxes[0, 0].set_title('usd pledged\\n&\\npledged', fontsize=20, color='red')\naxes[0, 1].set_title('usd pledged\\n&\\nusd_pledged_real', fontsize=20, color='green')\naxes[1, 0].set_title('usd pledged\\n&\\nusd_goal_real', fontsize=20, color='blue')\naxes[1, 1].set_title('usd_pledged_real\\n&\\nusd_goal_real', fontsize=20, color='gray')\nplt.tight_layout()","c25af55b":"ks_project_drop_real = ks_project_copy.drop('usd_pledged_real', axis=1)","c1ca42cb":"sns.histplot(np.log(ks_project_drop_real['goal'] + 1e-9), bins=20, kde=True)","d7975784":"# seek the corr goal and usd_pledged\nprint(ks_project_drop_real[['goal', 'usd_goal_real']].corr())\nprint()\nplt.scatter(ks_project_drop_real['goal'], ks_project_drop_real['usd_goal_real'])\nplt.title('Check the corr goal and usd_pledged')\nplt.show()","b417d7d4":"# so let's drop 'goal' and other unimportant columns\nks_project_without_goal = ks_project_drop_real.drop(['goal', 'ID', 'name', 'country', 'deadline'], axis=1)\n# create date col\nks_project_without_goal['year'] = ks_project_without_goal['launched'].dt.year\nks_project_without_goal['month'] = ks_project_without_goal['launched'].dt.month\nks_project_without_goal = ks_project_without_goal.drop('launched', axis=1)\nks_project_without_goal.head()","05840d1a":"print(ks_project_without_goal['backers'].value_counts().sort_values(ascending=False)[:20])\n\nfig, axes = plt.subplots(1, 2, figsize=(15, 5))\nsns.histplot(ks_project_without_goal['backers'], bins=20, kde=True, ax=axes[0])\nsns.histplot(np.log(ks_project_without_goal['backers'] + 1e-9), bins=25, kde=True, ax=axes[1])\naxes[0].set_title('Backers Hist', fontsize=20)\naxes[1].set_title('Log Backers Hist', fontsize=20)\nplt.tight_layout()\nplt.show()","30589657":"backers_df = ks_project_without_goal[['pledged', 'backers', 'usd pledged', 'usd_goal_real']]\nprint(backers_df.corr())\nprint()\nfig, axes = plt.subplots(1, 3, figsize=(17, 8), facecolor='lightblue')\nsns.scatterplot(x='backers', y='pledged', data=backers_df, ax=axes[0])\nsns.scatterplot(x='backers', y='usd pledged', data=backers_df, ax=axes[1])\nsns.scatterplot(x='backers', y='usd_goal_real', data=backers_df, ax=axes[2])\n\nplt.suptitle('backers correlation!', fontsize=25)\naxes[0].set_title('pledged', fontsize=20)\naxes[1].set_title('usd pledged', fontsize=20)\naxes[2].set_title('usd_goal_real', fontsize=20)\nplt.tight_layout()\nplt.show()","0c1395b6":"print('Main_category')\nprint(ks_project_without_goal.main_category.unique())\nprint(len(ks_project_without_goal.main_category.unique()))\nprint('-'*10)\nprint('category')\nprint(ks_project_without_goal.category.unique()[:10])\nprint(len(ks_project_without_goal.category.unique()))","334e36ba":"plt.figure(figsize=(20, 5))\nsns.countplot(x='main_category', data=ks_project_without_goal, hue='state')","d0780b90":"cols = ['Publishing', 'Film & Video', 'Food', 'Design', 'Games', 'Fashion', 'Technology']\nindices = product([0, 1, 2], [0, 1])\nfig, axes = plt.subplots(3, 2, figsize=(30, 50), sharex=True, facecolor='lightgray')\n\nfor idx, var in zip(indices, cols):\n    var_data = ks_project_without_goal.query('main_category==@var')\n    sns.countplot(y='category', data=var_data, hue='state', ax=axes[idx[0], idx[1]])\n    axes[idx[0], idx[1]].set_title(f\"{var}'s Category\\n\", fontsize=35)\n    axes[idx[0], idx[1]].tick_params(axis='y', labelsize=20)\n\nplt.tight_layout()\nplt.show()","32481956":"# first, we delete the time data\nks_project_without_goal['year'].value_counts()","fc2be1f8":"ks_project_drop_dt = ks_project_without_goal.query('year != 1970 and year != 2018')\nks_project_drop_dt['year'].value_counts()","f1d971df":"fig, axes = plt.subplots(2, 2, figsize=(15, 10), facecolor='lightgray')\ngrouby_df = ks_project_drop_dt.groupby('year').sum()\ncols = ['pledged', 'backers', 'usd pledged', 'usd_goal_real']\nindices = product([0, 1], [0, 1])\n\nfor idx, var in zip(indices, cols):\n    axes[idx[0], idx[1]].bar(grouby_df.index, grouby_df[var])\n    axes[idx[0], idx[1]].set_title(f'{var}', fontsize=20)\n    axes[idx[0], idx[1]].set_xticks(grouby_df.index)\n    axes[idx[0], idx[1]].tick_params(axis='x', labelsize=10)\n\nplt.tight_layout()\nplt.show()","7504cd3b":"fig, axes = plt.subplots(2, 2, figsize=(15, 10), sharey=True, facecolor='lightgray')\ncols = ['pledged', 'backers', 'usd pledged', 'usd_goal_real']\nindices = product([0, 1], [0, 1])\n\nfor idx, var in zip(indices, cols):\n    sns.barplot(x='year', y=var, hue='state', data=ks_project_drop_dt, ax=axes[idx[0], idx[1]])\n    axes[idx[0], idx[1]].set_title(f'{var}', fontsize=20)\n    axes[idx[0], idx[1]].tick_params(axis='x', labelsize=10)\n    \nplt.tight_layout()\nplt.show()","de1e8af3":"data = ks_project_drop_dt.copy()\n# separate the data \nX = data.drop('state', axis=1)\nX = pd.get_dummies(X, drop_first=True) # change the categorical columns to numeric\ny = data['state'].replace({'failed': 0, 'successful': 1})\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)","2ab5c1f7":"X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y, random_state=123)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=123)","40a501c6":"# make pipeline which have standardscaler and decidion\ndecision_tree = DecisionTreeClassifier(max_depth=5, random_state=123)\ndecision_tree.fit(X_train, y_train)\nscores = cross_val_score(estimator=decision_tree, X=X_train, y=y_train, cv=10)\nprint(np.mean(scores))\nprint(np.mean(decision_tree.score(X_val, y_val)))","fd8d3dd3":"#ROC curve\npred = decision_tree.predict(X_val)\nfpr, tpr, thresholds = roc_curve(y_val, pred)\nauc_score = roc_auc_score(y_val, pred)\nplt.plot(fpr, tpr, label='AUC = %.3f' % (auc_score))\nplt.legend()\nplt.title('ROC curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.grid(True)","897d4350":"# seek the best param\nparam_dist = {'C': np.arange(0.01, 10, 0.5)}\n\nrs = RandomizedSearchCV(estimator=LogisticRegression(), param_distributions=param_dist, cv=5, random_state=123)\nrs.fit(X_train, y_train)\nprint(rs.score(X_val, y_val))\nprint(rs.best_params_)","1f768a93":"rf = RandomForestClassifier(random_state=123)\nrf.fit(X_train, y_train)\nprint(rf.score(X_val, y_val))","55cab826":"top5_importance_indices = np.argpartition(rf.feature_importances_, -10)[-10:]\nprint(top5_importance_indices)\nprint(X.columns[top5_importance_indices])","14920d0a":"estimators = [\n     ('rf', RandomForestClassifier(n_estimators=10, random_state=123)),\n     ('svr', LinearSVC(random_state=123))\n]\n\nstack = StackingClassifier(estimators=estimators, \n                          final_estimator=LogisticRegression(C=0.01, max_iter=30)\n                         )\n\nstack.fit(X_train, y_train)\ny_pred = stack.predict(X_val)\n\nprint(stack.score(X_val, y_val))\nprint(accuracy_score(y_true=y_val, y_pred=y_pred))","0fcba8d4":"accuracy_score(y_test, rf.predict(X_test))","eb53dc8a":"### Seek the 'category' which has the far distance between failed and success\n### That's Film&video, Publishing, Food, Design, Games, Tech","9698b623":"# <h2 style=\"text-align:center; background-color:lightblue; padding:18px\">Last, We try Ensemble model<\/h2>","621ce4a9":"#### That's fine!\n\n# <h2 style=\"background-color:lightgreen;padding:18px; text-align:center\">Firstly, We have to know what distribution the state is !<\/h2>","8ff52860":"<div style=\"text-align:center; background-color:lightyellow; padding:18px\">\n    <h1>Conclusion<\/h1>\n    <h2>1st step: We seek the data<\/h2>\n    <h3>We find some nulls but number of them is so tiny that I thought we can remove null because removing doesn't influence to data<\/h3>\n    <h2>2nd step:EDA and Visualize<\/h2>\n    <h3>we seek the some values then we figure out each distributions and how each cols relate other. <\/h3>\n    <h3>We could get to know some categories are tend to fail but some specific sub-cattegories in main-cate tends to be success!<\/h3>\n    <h2>Last: Machine Learning<\/h2>\n    <h3>We tried various approach. And Finally we could get the good model<\/h3>\n    <h3>And this model shoed some important features when you decide to invest<\/h3>\n<\/div>\n<h2 style=\"color:red;\">Thank you for visiting my notebook and feel free to vote, comment, or something if you like mine!!!<\/h2>","9a55165c":"### OK, This model didn't work well. but We get best param of Log if we use ensemble method!","31f11247":"<h2 style=\"text-align:center;\"> Next, check 'goal' distribution<\/h2>","0a5858cc":"# <h2 style=\"text-align:center; background-color:lightgreen; padding:18px\"> Again, we think the <span style=\"color:red;\">'goal'<\/span> can be dropped because of the corr<\/h2>","3ea4e4de":"# <h2 style=\"text-align:center; background-color:lightgreen; padding:18px\">So let's move the machine learning<\/h2>","b64d6c09":"<h3 style=\"text-align:center;\"> This data's null is about <span style=\"color:red; font-size:25px;\">only 1%<\/span> in all<\/h3>\n<h2 style=\"text-align:center; color:red;\"> So I drop all of null rows because I think those null's row can't influence the analysis<\/h2>","8fe6305c":"## Finaly, we try ensemble and try to get higher score!!\n### This time I try only stacking but if you have some interests, let's try adaboost, lightgbm, xgboost, etc. !!","a2218773":"# <h2 style=\"text-align:center; background-color:lightblue; padding:18px\">Second, We try simple Logistic<\/h2>","5fbaa172":"### backers looks necessary to analyze.","120e9241":"### Mostly, number of backes is tiny but some has huge number","e7824810":"<h2 style=\"text-align:center;\">We think we use only <span style=\"color:red\">'failed', 'successfull' data <\/span>this time<\/h2>\n\n<h2 style=\"text-align:center;\">Moreover, we regard 'canceled' and 'suspended' as 'failed' by meaning<\/h2>","e05b4a64":"<div style=\"text-align:center; background-color:lightyellow; padding:18px\">\n    <h3>This model is better than DesicionTree! and get the top 10 features.<\/h3>\n    <h3>As we do EDA before modeling, Projects which is tend to fail are key to predict!<\/h3>\n<\/div>","78c6de74":"### what's backers??","8efb8cdb":"<div style=\"text-align:center; background-color:lightyellow; padding:18px\">\n    <h2>This graph shows Publishing, Film&Video, Food, Design, Games, Fashion, Tech's project <br><br>\n        are tend to be failed!\n        <br><br>(if you are investors or something, Be careful haha.)\n    <\/h2>\n    <h2>I think those project is easy to start nowadays, so it's really hard to compete in those industries<\/h2>\n    <h2>But Music, Comics, Theater, Dance have possibility to success!!<\/h2>\n<\/div>","4797cdf5":"# <h2 style=\"text-align:center; background-color:lightgreen; padding:18px\"> We can drop the 'usd_pledged_real' because the corr is 90% that is almost similar to 'usd pledged'<\/h2>","084faf07":"<div style=\"text-align:center; background-color:lightyellow; padding:18px\">\n    <h3>This model is better than DesicionTree but it a little worse than rf.<\/h3>\n    <h3>So, we use rf model for test data!!<\/h3>\n<\/div>","e75a9b0d":"# <h2 style=\"text-align:center; background-color:lightgreen; padding:18px\"> Next, we care about datetime and try to think the datetime is important for this data??<\/h2>","d9ba731e":"### OK, this model looks enough model!","f500dbf0":"# <h2 style=\"text-align:center; background-color:lightblue; padding:18px\">First, We try DecisionTree<\/h2>","3c27cfd8":"#### It looks good\n### let's get information about correlations","d7a4335d":"### pledged is increasing the year goes by, but other cols don't have any relations ?","f83f8356":"<div style=\"text-align:center; background-color:lightyellow; padding:18px\">\n    <h2>If you success, You have to care about pledged!!<\/h2>\n    <h3>We got to know why 'pledged' and some params' amount of 'successful' is so high but totaly most of state is failed!!<\/h3>\n    <h3>the usd_goal_real influences seriously!!<\/h3>\n<\/div>","fa0063a5":"<h2 style=\"text-align:center\">We have to change Dtypes and fill the nulls<\/h2>","a07adc50":"## Look Good!!\n## But, I have to think this is over fitting or not\n### let's see ROC curve","99fd903a":"# <h2 style=\"text-align:center; background-color:lightblue; padding:18px\">Third, We try RandomForest<\/h2>","b5b1d833":"# <h2 style=\"text-align:center; background-color:lightgreen; padding:18px\"> Let's move categorical columns and seek information <\/h2>","4877ba32":"### We find 1970s, those data is filled by default value and 2018 is too small\n### So, we drop them","4b0482dc":"<div style=\"text-align:center; background-color:lightyellow; padding:18px\">\n    <h2>This graph shows we knew those main categories are tend to fail <br><br>but \n        some sub-categories are good to invest in each main-categories!!\n    <\/h2>\n    <h2>If you are asked to be backers or invest, you have to see the sub categories!!<\/h2>\n<\/div>","bdf9308a":"### well, We regard this as normal distriibution."}}