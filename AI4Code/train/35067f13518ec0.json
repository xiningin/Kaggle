{"cell_type":{"bc292b39":"code","9f51dbce":"code","aab871b4":"code","1d618ffc":"code","962a4a45":"code","b229173a":"code","2ac38b76":"code","2bc1fac5":"code","d341f03b":"code","41dc2e59":"code","f4fbf88d":"code","f2c0b560":"code","2751f4e0":"code","5ef38ddd":"code","19b4093c":"code","9977d399":"code","258dd0b0":"code","784fc354":"code","9cf431a7":"code","820d6df0":"code","879904a9":"code","368eceab":"code","ea81283a":"code","e542f43d":"code","89ae2ad7":"code","28f97349":"code","74ac8637":"code","c51f150e":"code","4ba96715":"code","62dd6c77":"code","cda30cbf":"code","1337ebbd":"code","3105530a":"code","62bc45a2":"code","d2d6573e":"code","0aebf27b":"code","86a40186":"code","ab80eca3":"code","7adea3b8":"code","f13f452f":"code","a3043e24":"code","e1bf74a6":"code","06fd5528":"code","b7fcdd6a":"code","2caf3c7b":"code","69e3be66":"code","45d5c182":"code","8ab74f46":"code","3bcc094f":"code","e90a0b95":"code","93a8f21c":"code","bf1e7f7c":"code","8aab0bcc":"code","d7eb47e8":"code","f502a36b":"code","b10d0e03":"code","1bc97664":"markdown","92536bf9":"markdown","078a16d6":"markdown","fbfde3c0":"markdown","1c0ad47a":"markdown","7a845022":"markdown","b9af9ea8":"markdown","9b2c89b2":"markdown","4ca18d85":"markdown","3a11c6bb":"markdown","6e7b6023":"markdown","7528e48a":"markdown","a86b6472":"markdown","94ebac53":"markdown","bb5100b7":"markdown","f0700cb5":"markdown","669aa5a6":"markdown","2b607fd3":"markdown","e5f6a99d":"markdown","e55dbb0d":"markdown","10d397ae":"markdown","366e25c1":"markdown","1d3dd8e8":"markdown","e8142b5e":"markdown","f8ef4ad0":"markdown","91d54fa8":"markdown","703c8cf1":"markdown","4ac0c0b1":"markdown","b4589947":"markdown","0f68b164":"markdown","d4fbcffb":"markdown","ee8ad554":"markdown","4ba1c0e8":"markdown"},"source":{"bc292b39":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.transforms as transforms\nimport seaborn as sns\nimport sklearn.metrics as metrics\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score","9f51dbce":"df = pd.read_csv(\"..\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv\")","aab871b4":"df.head()","1d618ffc":"df.shape","962a4a45":"df.info()","b229173a":"df.isnull().sum()","2ac38b76":"df.describe()","2bc1fac5":"labels = df[\"class\"].unique().tolist()\nsizes = df[\"class\"].value_counts().tolist()\ncolors = [\"#C95555\", \"#D8AFAF\"]\nexplode = (0, 0)\nfig, ax = plt.subplots(1,2, figsize=(14,6))\nsns.countplot(df[\"class\"], palette=\"Oranges\", ax=ax[0])\nax[0].set_title(\"Distribution of Patients\", size=28, fontweight=\"bold\")\nax[0].set_xlabel(\"Class\", size=18, fontweight=\"bold\")\nax[0].set_ylabel(\"Count\", size=18, fontweight=\"bold\")\nplt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90, textprops={'fontsize': 14, \"fontweight\" : \"bold\"}, colors=colors)\nplt.title(\"Distribution of Patients\", size=28, fontweight=\"bold\")","d341f03b":"plt.figure(figsize=(14,6))\ndf_corr = df.corr()\nsns.heatmap(df_corr, annot=True, cmap=\"YlGn\")\nplt.tight_layout()","41dc2e59":"plt.figure(figsize=(16,8))\nsns.scatterplot(df[\"pelvic_incidence\"], df[\"sacral_slope\"], hue=df[\"class\"], palette=\"deep\", s=100)\nplt.ylabel(\"Sacral Slope\", fontsize=15, fontweight=\"bold\")\nplt.xlabel(\"Pelvic Incidence\", fontsize=15, fontweight=\"bold\")\nplt.title(\"Distribution of Patients with Respect to Pelvic Incidence \\nAnd Sacral Slope\", fontsize=22, fontweight=\"bold\")\nplt.legend(prop={\"size\":15})","f4fbf88d":"plt.figure(figsize=(16,8))\nsns.scatterplot(df[\"pelvic_radius\"], df[\"sacral_slope\"], hue=df[\"class\"], palette=\"cubehelix\", s=100)\nplt.ylabel(\"Sacral Slope\", fontsize=15, fontweight=\"bold\")\nplt.xlabel(\"Pelvic Radius\", fontsize=15, fontweight=\"bold\")\nplt.title(\"Distribution of Patients with Respect to Pelvic Radius \\nAnd Sacral Slope\", fontsize=22, fontweight=\"bold\")\nplt.legend(prop={\"size\":15})","f2c0b560":"sns.pairplot(df, hue='class', height=2)","2751f4e0":"df_grouped = df.groupby(\"class\").agg(\"mean\")\ndf_grouped","5ef38ddd":"df_grouped.plot.bar(rot=0, figsize=(16,8))\nplt.title(\"Comparison of Abnormal and Normal Patients \\n(Average Values)\", fontsize=22, fontweight=\"bold\")\nplt.xlabel(\"Patient\", fontsize=15, fontweight=\"bold\")\nplt.grid()\nplt.legend(prop={\"size\":12})","19b4093c":"df_grouped.plot.bar(rot=0, figsize=(16,8), subplots=True, layout=(3,2))","9977d399":"df_normal = df[df[\"class\"] == \"Normal\"]\ndf_abnormal = df[df[\"class\"] == \"Abnormal\"]\nnormal_df = df_normal.drop(\"class\", axis=1)\nabnormal_df = df_abnormal.drop(\"class\", axis=1)\nnormal_columns = normal_df.columns.tolist()\nabnormal_columns = abnormal_df.columns.tolist()","258dd0b0":"fig, axs = plt.subplots(3,2, figsize=(20,12))\nfig.suptitle('Distribution Plots of Biomechanical Attributes for Normal Patients', \n             fontsize=25, fontweight=\"bold\")\nax_iter = iter(axs.flat)\nfor columns in normal_columns:\n    ax = next(ax_iter)\n    sns.distplot(df_normal[columns], ax=ax)","784fc354":"fig, axs = plt.subplots(3,2, figsize=(20,12))\nfig.suptitle('Distribution Plots of Biomechanical Attributes for Abnormal Patients', \n             fontsize=25, fontweight=\"bold\")\nax_iter = iter(axs.flat)\nfor columns in abnormal_columns:\n    ax = next(ax_iter)\n    sns.distplot(df_abnormal[columns], ax=ax)","9cf431a7":"score_dict = {}\ndf[\"class\"].replace({\"Abnormal\": 1, \"Normal\": 0}, inplace=True)\nx = df.drop(\"class\", axis=1)\ny = df[\"class\"]","820d6df0":"X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.33, random_state=10)","879904a9":"neighbors_list = list(range(3,20,2))\nknn_score_list = []\n\nfor number in neighbors_list:\n    knn = KNeighborsClassifier(n_neighbors=number)\n    knn.fit(X_train, y_train)\n    y_predict_knn = knn.predict(X_test)\n    knn_score_list.append(accuracy_score(y_test, y_predict_knn))\n    \nfig, ax = plt.subplots(1,1, figsize=(10,6))\nplt.plot(neighbors_list, knn_score_list, marker=\"o\", markerfacecolor=\"red\", markersize=8)\nplt.xticks(np.arange(3, 20, 2))\nplt.xlabel(\"k value\", size=12)\nplt.ylabel(\"Accuracy Score\", size=12)\nax.axhline(y = max(knn_score_list) , linewidth = 1.5, color = \"red\", linestyle=\"dashed\")\ntrans = transforms.blended_transform_factory(\n    ax.get_yticklabels()[0].get_transform(), ax.transData)\nax.text(0, max(knn_score_list), \"{:.4f}\".format(max(knn_score_list)), color=\"red\", transform=trans, \n        ha=\"right\", va=\"center\")","368eceab":"knn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(X_train, y_train)","ea81283a":"score_dict[\"KNN\"] = knn.score(X_test, y_test)\ny_predict_knn = knn.predict(X_test)\nknn.score(X_test, y_test)","e542f43d":"cm_knn = confusion_matrix(y_test, y_predict_knn)\nplt.figure(figsize=(10,6))\nsns.heatmap(cm_knn, annot=True, cmap=\"Blues\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")","89ae2ad7":"print(classification_report(y_test, y_predict_knn))","28f97349":"cross_val_score(estimator=knn, X = X_train, y = y_train, cv=5).mean()","74ac8637":"param_grid = {'n_neighbors': np.arange(1,20)}\nknn_gscv = GridSearchCV(knn, param_grid, cv=5)\nknn_gscv.fit(X_train, y_train)\nprint(\"Tuned hyperparameter: {}\".format(knn_gscv.best_params_)) \nprint(\"Best score: {}\".format(knn_gscv.best_score_))","c51f150e":"lr = LogisticRegression(C = 0.1)\nlr.fit(X_train, y_train)","4ba96715":"score_dict[\"Logistic Regression\"] = lr.score(X_test, y_test)\ny_predict_lr = lr.predict(X_test)\nlr.score(X_test, y_test)","62dd6c77":"cm_lr = confusion_matrix(y_test, y_predict_lr)\nplt.figure(figsize=(10,6))\nsns.heatmap(cm_lr, annot=True, cmap=\"Blues\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")","cda30cbf":"print(classification_report(y_test, y_predict_lr))","1337ebbd":"cross_val_score(estimator=lr, X = X_train, y = y_train, cv=5).mean()","3105530a":"param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\nlr_gscv = GridSearchCV(lr, param_grid, cv=5)\nlr_gscv.fit(X_train, y_train)\nprint(\"Tuned hyperparameters {}\".format(lr_gscv.best_params_)) \nprint(\"Best score: {}\".format(lr_gscv.best_score_))","62bc45a2":"dtc = DecisionTreeClassifier(criterion = 'gini', max_depth = 4, min_samples_split = 50, random_state=10)\ndtc.fit(X_train, y_train)","d2d6573e":"score_dict[\"Decision Tree Classifier\"] = dtc.score(X_test, y_test)\ny_predict_dtc = dtc.predict(X_test)\ndtc.score(X_test, y_test)","0aebf27b":"cm_dtc = confusion_matrix(y_test, y_predict_dtc)\nplt.figure(figsize=(10,6))\nsns.heatmap(cm_dtc, annot=True, cmap=\"Blues\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")","86a40186":"print(classification_report(y_test, y_predict_dtc))","ab80eca3":"cross_val_score(estimator=dtc, X = X_train, y = y_train, cv=5).mean()","7adea3b8":"param_grid = {\"criterion\" : ['gini', 'entropy'], \"max_depth\" : np.arange(2,21,2), \n              'min_samples_split' : np.arange(10,200,10)}\ndtc_gscv = GridSearchCV(dtc, param_grid, cv=5)\ndtc_gscv.fit(X_train, y_train)\nprint(\"Tuned hyperparameters {}\".format(dtc_gscv.best_params_)) \nprint(\"Best score: {}\".format(dtc_gscv.best_score_))","f13f452f":"rfc = RandomForestClassifier(n_estimators=100, criterion='gini', random_state=10)\nrfc.fit(X_train, y_train)","a3043e24":"rfc.score(X_test, y_test)","e1bf74a6":"cross_val_score(estimator=rfc, X = X_train, y = y_train, cv=5).mean()","06fd5528":"param_grid = {'n_estimators': np.arange(100,500,50), 'max_depth' : [4,5,6,7,8], \n              'criterion' :['gini', 'entropy']}\nrfc_gscv = GridSearchCV(rfc, param_grid, cv=5)\nrfc_gscv.fit(X_train,y_train)\nprint(\"Tuned hyperparameters {}\".format(rfc_gscv.best_params_)) \nprint(\"Best score: {}\".format(rfc_gscv.best_score_))","b7fcdd6a":"rfc = RandomForestClassifier(criterion = 'gini', max_depth = 4, n_estimators = 200, random_state=10)\nrfc.fit(X_train, y_train)","2caf3c7b":"score_dict[\"Random Forest Classifier\"] = rfc.score(X_test, y_test)\ny_predict_rfc = rfc.predict(X_test)\nrfc.score(X_test, y_test)","69e3be66":"cross_val_score(estimator=rfc, X = X_train, y = y_train, cv=5).mean()","45d5c182":"cm_rfc = confusion_matrix(y_test, y_predict_rfc)\nplt.figure(figsize=(10,6))\nsns.heatmap(cm_rfc, annot=True, cmap=\"Blues\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")","8ab74f46":"print(classification_report(y_test, y_predict_rfc))","3bcc094f":"svm = SVC(kernel=\"linear\", probability=True)\nsvm.fit(X_train, y_train)","e90a0b95":"score_dict[\"SVM\"] = svm.score(X_test, y_test)\ny_predict_svm = svm.predict(X_test)\nsvm.score(X_test, y_test)","93a8f21c":"cross_val_score(estimator=svm, X = X_train, y = y_train, cv=5).mean()","bf1e7f7c":"cm_svm = confusion_matrix(y_test, y_predict_svm)\nplt.figure(figsize=(10,6))\nsns.heatmap(cm_svm, annot=True, cmap=\"Blues\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")","8aab0bcc":"print(classification_report(y_test, y_predict_svm))","d7eb47e8":"score_dict","f502a36b":"models = score_dict.keys()\nscores = score_dict.values()\n\nplt.figure(figsize=(16,6))\nplt.bar(models, scores, color=\"#A67EB0\")\nplt.yticks(np.arange(0, 1.05, 0.05))\nplt.xlabel(\"Model\", fontsize=15)\nplt.ylabel(\"Accuracy Score\", fontsize=15)\nfor i, v in enumerate(score_dict.values()):\n    plt.text(i-0.1, v+0.03, \"{:.4f}\".format(v), color='black', va='center', fontweight='bold')","b10d0e03":"prob_knn = knn.predict_proba(X_test)[:,1]\nprob_lr = lr.predict_proba(X_test)[:,1]\nprob_dtc = dtc.predict_proba(X_test)[:,1]\nprob_rfc = rfc.predict_proba(X_test)[:,1]\nprob_svm = svm.predict_proba(X_test)[:,1]\n\nprob_dict = {\"ROC KNN\": prob_knn, \"ROC LR\": prob_lr, \"ROC DTC\": prob_dtc, \n             \"ROC RFC\": prob_rfc, \"ROC SVM\": prob_svm}\n\nfor model, prob in prob_dict.items():\n    fpr, tpr, threshold = metrics.roc_curve(y_test, prob)\n    roc_auc = metrics.auc(fpr, tpr)\n    plt.figure(figsize=(10,6))\n    plt.plot(fpr, tpr, color = \"b\", label = \"AUC = %0.2f\" %roc_auc)\n    plt.legend(loc=\"lower right\", prop={\"size\":15})\n    #plt.xlim([-0.005,1])\n    #plt.ylim([0,1.015])\n    plt.xlabel(\"False Positive Rate\", size=12)\n    plt.ylabel(\"True Positive Rate\", size=12)\n    plt.plot([0,1], [0,1], \"r--\")\n    plt.title(str(model), size=20)","1bc97664":"<h2>We can see that the average degree spondylolisthesis value in abnormal patients is pretty higher than average the degree spondylolisthesis value in normal patients.<h2>","92536bf9":"<h2>Necessary libraries<h2>","078a16d6":"# *Comparison*","fbfde3c0":"# *Decision Tree Classifier*","1c0ad47a":"<h2>Now, we can create a new model with tuned hyperparameters.<h2>","7a845022":"<h2>Let's create a random forest model with all randomly selected hyperparameters.<h2>","b9af9ea8":"# ***Machine Learning Algorithms***","9b2c89b2":"<h2>Now, let's get some visualizations about target column which is class. <h2>","4ca18d85":"<h2>Let's try to understand the relations between feature columns by visualizing a correlation matrix.<h2>","3a11c6bb":"<h2> Let's visualize some of those correlations! <h2>","6e7b6023":"<h2> Let's check the cross validation score. <h2>","7528e48a":"<h2> From the matrix, we can see that there are highly positive correlations between Pelvic Incidence and Sacral Slope,  as well as, between Pelvic Incidence and Lumbar Lordosis Angle.<h2>","a86b6472":"<h2> In this section, I am going to create some machine learning models by using KNN (K-Nearest Neighbors), Logistic Regression, Decision Tree Classifier, Random Forest Classifier and SVM (Support Vector Machines) algorithms. <h2>  ","94ebac53":"# *Random Forest Classifier*","bb5100b7":"<h2> Now, let's take a glance at the confusion matrix. <h2>","f0700cb5":"<h2>Now, let's compare these models.<h2>","669aa5a6":"<h2>It's 80.62 %. Not bad. But, is it possible to increase the cross validation score?<h2>","2b607fd3":"<h2> Let's check if there are null values or not. <h2>","e5f6a99d":"# *KNN (K-Nearest Neighbors)*","e55dbb0d":"<h2>There is no any null value.<h2>","10d397ae":"<h2>As it's seen, n_neighbors = 15 gives the best CV score which is 83.05%.<h2>","366e25c1":"<h2>Cross validation score has been increased from 80.61% to 83.04%. That's good! <h2>","1d3dd8e8":"<h2>It's clearly seen that we have 67.7% abnormal patients and 32.3% normal Patients.<h2>","e8142b5e":"<h2>Firstly, let's create a loop and try to find the best k value to reach the highest accuracy score.<h2>","f8ef4ad0":"<h2>Let's take a glance at the distribution of each feature for each class.<h2>","91d54fa8":"<h2>We can use GridSearchCV method to tune hyperparameters for the best CV score. In the end, we can select the best parameters from the listed hyperparameters. For this model, my hyperparameter for tuning is n_neighbors number.<h2>","703c8cf1":"<h2>It looks like we get the highest accuracy score for k value = 5 which is 90.29%.<h2>","4ac0c0b1":"# *SVM*","b4589947":"<h2> To understand more about the data and compare the patients with respect to their orthopedic features, let's get the average values for both abnormal patients and normal patients. <h2>","0f68b164":"<h2> Classification <h2>","d4fbcffb":"# *Logistic Regression*","ee8ad554":"<h2>Checking the ROC curves and AUCs for each model. Higher the AUC, better the model is at predicting normals as normals and abnormals as abnormals.<h2>","4ba1c0e8":"# *Data Analysis and Visualizations*"}}