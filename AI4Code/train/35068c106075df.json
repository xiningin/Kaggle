{"cell_type":{"ee533d43":"code","37d5b8ab":"code","98d6504e":"code","6303cd21":"code","5065078d":"code","d9faec2f":"code","dbe2e354":"code","2094837c":"code","5282552c":"code","5aeba8a8":"code","0dfb6ecb":"code","211bf9e6":"code","a1110311":"code","55a99e15":"code","792b2258":"code","91f8a9ff":"code","edde1111":"code","fbe089c1":"code","f06d8ed8":"code","d16f5d95":"code","c982c2d4":"code","02a6a1f0":"code","f8fdde13":"code","5e470daa":"code","2a6e7f11":"code","98084268":"code","c87e9b4c":"code","e68668e9":"code","0b246516":"code","488efaae":"code","98bc8b22":"code","2e8f23b3":"code","adca3176":"code","c15b338c":"code","845abd83":"code","85ddb7cd":"code","a8d5c1c7":"code","325da182":"code","48c90c21":"code","e3ba41ed":"code","215b8328":"code","53a79fce":"code","8155da7b":"code","515041f2":"code","39c6f27f":"code","9d177f28":"code","01b14f96":"code","e58e39a6":"code","d898dfc4":"code","976e5aea":"code","2c136fcd":"code","9ec650c6":"code","90b05212":"code","f35ea1dd":"code","bb7310a1":"code","0146e34b":"code","3ad946a4":"code","fc00ea9c":"code","2786b794":"code","d76a0d0c":"code","8ad0e4a9":"code","e94df2c2":"code","31db5e2a":"code","5713f0af":"code","4753a09d":"code","5fc6cb1c":"code","af2451f2":"code","b3fe8e01":"code","24834828":"code","0e7ca8b5":"code","be81d5db":"code","edee8581":"code","8ad62708":"code","7fd057bb":"code","f3062367":"code","c0d3f9ce":"code","6e2acbd2":"code","bfc4ee7f":"markdown","8aaba014":"markdown","6002059d":"markdown","89df53b7":"markdown","d6752ec3":"markdown","3702bafc":"markdown","f783e0f3":"markdown","964f1c64":"markdown","625ab41e":"markdown","af283951":"markdown","0daf5f54":"markdown","588dd55b":"markdown","5cf706cf":"markdown","df89d03e":"markdown","214a37ae":"markdown","eabc7ac8":"markdown","8aa78f59":"markdown","5cf116bf":"markdown","c576a7aa":"markdown","e32e75e6":"markdown","e776dd2d":"markdown","63eb08d2":"markdown","23cbb0af":"markdown","9b01844f":"markdown"},"source":{"ee533d43":"import tarfile\nimport numpy as np\nimport pandas as pd\nimport json\n \nimport nltk \nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize, sent_tokenize \nimport re, string, timeit\nimport os\nfrom math import sqrt\nimport pickle\nfrom scipy import spatial\nfrom sklearn.feature_extraction.text import TfidfVectorizer","37d5b8ab":"covidTerms = ['sars-cov-2','covid-19', 'coronavirus']\nincubationTerms = ['incubation period', 'incubation','incubations','incubation time']\npunctuationTerms = ['!','(',')','-','[',']','{','}',';',':','\\ ', '<', '>','\/','?','@','#','$','%','^','&','*','_','~','+','=']\n\nmonthTerms = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december']\n\n\n","98d6504e":"countries = [\"Afghanistan\",\"Albania\",\"Algeria\",\"Andorra\",\"Angola\",\"Antigua & Deps\",\"Argentina\",\"Armenia\",\"Australia\",\"Austria\",\"Azerbaijan\"\n,\"Bahamas\"\n\"Bahrain\",\n\"Bangladesh\",\n\"Barbados\",\n\"Belarus\",\n\"Belgium\",\n\"Belize\",\n\"Benin\",\n\"Bhutan\",\n\"Bolivia\",\n\"Bosnia Herzegovina\",\n\"Botswana\",\n\"Brazil\",\n\"Brunei\",\n\"Bulgaria\",\n\"Burkina\",\n\"Burundi\",\n\"Cambodia\",\n\"Cameroon\",\n\"Canada\",\n\"Cape Verde\",\n\"Central African Rep\",\n\"Chad\",\n\"Chile\",\n\"China\",\n\"Colombia\",\n\"Comoros\",\n\"Congo\",\n\"Congo {Democratic Rep}\",\n\"DRC\",\n\"Costa Rica\",\n\"Croatia\",\n\"Cuba\",\n\"Cyprus\",\n\"Czech Republic\",\n\"Denmark\",\n\"Djibouti\",\n\"Dominica\",\n\"Dominican Republic\",\n\"East Timor\",\n\"Ecuador\",\n\"Egypt\",\n\"El Salvador\",\n\"Equatorial Guinea\",\n\"Eritrea\",\n\"Estonia\",\n\"Ethiopia\",\n\"Fiji\",\n\"Finland\",\n\"France\",\n\"Gabon\",\n\"Gambia\",\n\"Georgia\",\n\"Germany\",\n\"Ghana\",\n\"Greece\",\n\"Grenada\",\n\"Guatemala\",\n\"Guinea\",\n\"Guinea-Bissau\",\n\"Guyana\",\n\"Haiti\",\n\"Honduras\",\n\"Hungary\",\n\"Iceland\",\n\"India\",\n\"Indonesia\",\n\"Iran\",\n\"Iraq\",\n\"Ireland\",\n\"Israel\",\n\"Italy\",\n\"Ivory Coast\",\n\"Jamaica\",\n\"Japan\",\n\"Jordan\",\n\"Kazakhstan\",\n\"Kenya\",\n\"Kiribati\",\n\"Korea North\",\n\"Korea South\",\n\"Korea\",\n\"Kosovo\",\n\"Kuwait\",\n\"Kyrgyzstan\",\n\"Laos\",\n\"Latvia\",\n\"Lebanon\",\n\"Lesotho\",\n\"Liberia\",\n\"Libya\",\n\"Liechtenstein\",\n\"Lithuania\",\n\"Luxembourg\",\n\"Macedonia\",\n\"Madagascar\",\n\"Malawi\",\n\"Malaysia\",\n\"Maldives\",\n\"Mali\",\n\"Malta\",\n\"Marshall Islands\",\n\"Mauritania\",\n\"Mauritius\",\n\"Mexico\",\n\"Micronesia\",\n\"Moldova\",\n\"Monaco\",\n\"Mongolia\",\n\"Montenegro\",\n\"Morocco\",\n\"Mozambique\",\n\"Myanmar\",\n\"Burma\",\n\"Namibia\",\n\"Nauru\",\n\"Nepal\",\n\"Netherlands\",\n\"New Zealand\",\n\"Nicaragua\",\n\"Niger\",\n\"Nigeria\",\n\"Norway\",\n\"Oman\",\"Pakistan\",\"Palau\",\"Panama\",\"Papua New Guinea\",\"Paraguay\",\"Peru\",\"Philippines\",\"Poland\",\"Portugal\",\"Qatar\",\"Romania\",\"Russian Federation\",\"Russia\",\"Rwanda\",\"St Kitts & Nevis\",\"St Lucia\",\"Saint Vincent & the Grenadines\",\"Samoa\",\"San Marino\",\"Sao Tome & Principe\",\"Saudi Arabia\",\"Senegal\",\"Serbia\",\"Seychelles\",\"Sierra Leone\",\"Singapore\",\"Slovakia\",\"Slovenia\",\"Solomon Islands\",\"Somalia\",\"South Africa\",\"South Sudan\",\"Spain\",\"Sri Lanka\",\"Sudan\",\"Suriname\",\"Swaziland\",\"Sweden\",\"Switzerland\",\"Syria\",\"Taiwan\",\"Tajikistan\",\"Tanzania\",\"Thailand\",\"Togo\",\"Tonga\",\"Trinidad & Tobago\",\"Tunisia\",\"Turkey\",\"Turkmenistan\",\"Tuvalu\",\"Uganda\",\"Ukraine\",\"United Arab Emirates\",\"UAE\",\"United Kingdom\",\"UK\",\"United States\",\"US\",\"Uruguay\",\"Uzbekistan\",\"Vanuatu\",\"Vatican City\",\"Venezuela\",\"Vietnam\",\"Yemen\",\"Zambia\",\"Zimbabwe\"]","6303cd21":"#load files\ndef loadZip(file):\n    json_files = [pos_json for pos_json in os.listdir(file) if pos_json.endswith('.json')]\n\n    \n    \n    return json_files\ncontentList = loadZip('\/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset')","5065078d":"contentListNonCom = loadZip('\/kaggle\/input\/CORD-19-research-challenge\/noncomm_use_subset\/noncomm_use_subset')","d9faec2f":"#biorxiv_medrxiv dataset + PMCCusomt\ncontentListBio = loadZip('\/kaggle\/input\/CORD-19-research-challenge\/biorxiv_medrxiv\/biorxiv_medrxiv')\ncontentListPmc = loadZip('\/kaggle\/input\/CORD-19-research-challenge\/custom_license\/custom_license')","dbe2e354":"#combine articles into single string for tokenization\ndef text2pandas(ls,base):\n    \n\n    articles = []\n    uniqueArticles = []\n    for x in ls:\n        dataList = [\"\",\"\",'']\n        with open(base+x) as f:\n          \n            d = json.loads(f.read())\n            dataList[0] = d['paper_id']\n            dataList[1] = d['metadata']['title']\n            string = \"\"\n            for n in d['body_text']:\n                string += n['text']\n            dataList[2] = string\n            uniqueArticles.append(string)\n            articles.append(dataList)\n        \n    dataFrame = pd.DataFrame(articles, columns = ['ID','Title', 'Corpus']) \n        \n    \n    return uniqueArticles,dataFrame","2094837c":"#turn data into panda for bio and pmc data\narticlesBio,dfBio = text2pandas(contentListBio,\"\/kaggle\/input\/CORD-19-research-challenge\/biorxiv_medrxiv\/biorxiv_medrxiv\/\")\narticlesPmc, dfPmc = text2pandas(contentListPmc,\"\/kaggle\/input\/CORD-19-research-challenge\/custom_license\/custom_license\/\")","5282552c":"#turn data into panda for com data\narticles,df = text2pandas(contentList,\"\/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\/\")\n","5aeba8a8":"#turn data into panda for noncom data\narticlesNonCom,dfNonCom = text2pandas(contentListNonCom,\"\/kaggle\/input\/CORD-19-research-challenge\/noncomm_use_subset\/noncomm_use_subset\/\")","0dfb6ecb":"del dfNonCom\ndel df\ndel dfBio\ndel dfPmc","211bf9e6":"#create tokens from article corpus\ndef createTokenList(articles):\n    articleTokens = []\n    for x in articles:\n        tokens1 = nltk.word_tokenize(x)\n        \n        articleTokens.append(tokens1)\n    return articleTokens\n","a1110311":"#coprus com zip tokens\narticleTokens = createTokenList(articles)","55a99e15":"#coprus non-com zip tokens\narticleTokensNonCom = createTokenList(articlesNonCom)","792b2258":"#corpus bio and pmc tokens\narticleTokensBio = createTokenList(articlesBio)\narticleTokensPmc = createTokenList(articlesPmc)","91f8a9ff":"#flatten token list\n\nflatten = lambda l: [item for sublist in l for item in sublist]\n#flattenTokenList = (flatten(articleTokens))","edde1111":"flattenTokenList = (flatten(articleTokens))","fbe089c1":"#flatten Non Com token list\nflattenTokenListNonCom = (flatten(articleTokensNonCom))","f06d8ed8":"#flatten bio and pmc\nflattenTokenListBio = (flatten(articleTokensBio))\nflattenTokenListPmc = (flatten(articleTokensPmc))","d16f5d95":"del articleTokens\ndel articleTokensNonCom\ndel articleTokensBio\ndel articleTokensPmc","c982c2d4":"#word tag list\ndef dateCheck(string):\n    listofDates = 0\n    for i in range(len(string)):\n        if(bool(re.match('^[0-3][0-9][0-9][0-9]', string[i]))) == True:\n            listofDates= listofDates + 1\n            \n            string[i] = \"<<year>>\"\n        \n    return string;\ndef stopwordCheck(string):\n    \"\"\"Remove stop words from list of tokenized words\"\"\"\n    newStopWords =0\n    for i in range(len(string)):\n        if string[i] in stopwords.words('english'):\n            newStopWords = newStopWords + 1\n            string[i] = \"<<stopWord>>\"\n    return string\ndef numberCheck(string):\n    \"\"\"Remove numbers from list of tokenized words\"\"\"\n    punctuations = '''!()-[]{};:'\"\\,<>.\/?@#$%^&*_~'''\n    new_words = 0\n    for i in range(len(string)):\n        finalSt = \"\"\n        for charac in string[i]:\n            if charac not in punctuations:\n                finalSt = finalSt + charac\n\n        if bool(re.match('^[0-9]', finalSt)) == True:\n            new_words = new_words + 1\n            string[i] = \"<<realNumber>>\"\n    return string;\ndef countryCheck(string):\n\n    lss = []\n    for i in range(len(string)):\n        if string[i] in countries:\n           lss.append(string[i])\n           string[i] = \"<<country>>\"\n            \n\n    return lss\ndef covidCheck(string):\n\n    lss = []\n    for i in range(len(string)):\n        if string[i].lower() in covidTerms:\n           lss.append(string[i])\n           string[i] = \"<<COVID-19>>\"\n            \n\n    return string\ndef punctuationCheck(string):\n\n    lss = []\n    for i in range(len(string)):\n        if string[i].lower() in punctuationTerms:\n           lss.append(string[i])\n           string[i] = \"<<Punctuation>>\"\n            \n\n    return string\ndef monthCheck(string):\n\n    lss = []\n    for i in range(len(string)):\n        if string[i].lower() in monthTerms:\n           lss.append(string[i])\n           string[i] = \"<<Month>>\"\n            \n\n    return string\n\n\n#replace list of words to list of tokens\ndef integer_representations(vocab_dict,token_list):\n    integer_rep = []\n    for token in token_list:\n        try:\n          \n          integer_rep.append((vocab_dict[token])[0])\n        except:\n          integer_rep.append(999999999)\n    return integer_rep","02a6a1f0":"#gets rid of words that do not appear more than the set frequency threshold\ndef useless_words(freq_dict, freq_threshold):\n    copy = freq_dict.copy()\n    for word in copy:\n        if freq_dict[word][1] < freq_threshold:\n            del freq_dict[word]\n    return freq_dict\n","f8fdde13":"#Com Corpus\nnewStopsList = stopwordCheck(flattenTokenList)","5e470daa":"newList = punctuationCheck(newStopsList)","2a6e7f11":"newList = monthCheck(newList)","98084268":"newList = covidCheck(newList)","c87e9b4c":"newList = dateCheck(newList)","e68668e9":"newList = numberCheck(newList)","0b246516":"#non com corpus\nnewStopsListNon = stopwordCheck(flattenTokenListNonCom)\nnewListNon = punctuationCheck(newStopsListNon)\nnewListNon = monthCheck(newListNon)\nnewListNon = covidCheck(newListNon)\nnewListNon = dateCheck(newListNon)\nnewListNon = numberCheck(newListNon)","488efaae":"#bio corpus\nnewStopsListBio = stopwordCheck(flattenTokenListBio)\nnewListBio = punctuationCheck(newStopsListBio)\nnewListBio = monthCheck(newListBio)\nnewListBio = covidCheck(newListBio)\nnewListBio = dateCheck(newListBio)\nnewListBio = numberCheck(newListBio)","98bc8b22":"#pmc corpus\nnewStopsListPmc = stopwordCheck(flattenTokenListPmc)\nnewListPmc = punctuationCheck(newStopsListPmc)\nnewListPmc = monthCheck(newListPmc)\nnewListPmc = covidCheck(newListPmc)\nnewListPmc = dateCheck(newListPmc)\nnewListPmc = numberCheck(newListPmc)","2e8f23b3":"def frequency2(newList):\n    d = {}\n    wID = 0\n    for t in newList:\n        try:\n            elem = d[t]\n        except:\n            elem = [wID,0]\n            wID = wID + 1\n        elem[1] = elem[1] + 1\n        d[t] = elem\n    return d","adca3176":"def frequencyUpdate(newList,d):\n    wID = 0\n    for t in newList:\n        try:\n            elem = d[t]\n        except:\n            elem = [wID,0]\n            wID = wID + 1\n        elem[1] = elem[1] + 1\n        d[t] = elem\n    return d","c15b338c":"freqDictionary = frequency2(newList)","845abd83":"newDictionary = frequencyUpdate(newListNon,freqDictionary)","85ddb7cd":"newDictionary2 = frequencyUpdate(newListBio,newDictionary)","a8d5c1c7":"finalDictionary = frequencyUpdate(newListPmc,newDictionary2)","325da182":"output = open('comboDictionary.pkl', 'wb')\npickle.dump(finalDictionary, output)\noutput.close()\n","48c90c21":"finalDictionary = pd.read_pickle(\"\/kaggle\/input\/covidsaved\/comboDictionary.pkl\")","e3ba41ed":"freq_threshold = 3\nnewDictValues = useless_words(finalDictionary, freq_threshold)","215b8328":"def finalDictionary(dictionary):\n    copy = dictionary.copy()\n    for word in copy:\n        del dictionary[word][1]\n    return dictionary","53a79fce":"final_dict = finalDictionary(newDictValues)","8155da7b":"#turn articles in tokens\ndef newPDList(articles,dictionary):\n    \n    tokenFinalList = []\n    for x in articles:\n        tokens1 = nltk.word_tokenize(x)\n        copyCheck = tokens1.copy()\n        punct = punctuationCheck(copyCheck)\n        covid = covidCheck(copyCheck)\n        dateC = dateCheck(copyCheck)\n        ls = stopwordCheck(copyCheck)\n        newW = numberCheck(copyCheck)\n        countryL = countryCheck(copyCheck)\n        month = monthCheck(copyCheck)\n        \n        #used dictionary to alter string tokens to integers\n        newList = integer_representations(dictionary,copyCheck)\n        tokenFinalList.append(newList)\n    return tokenFinalList","515041f2":"#create com corpus token list from full dictionary\nnewArticleList = newPDList(articles,final_dict)","39c6f27f":"#create non com corpus token list from full dictionary\nnewArticleListNonCom = newPDList(articlesNonCom,final_dict)","9d177f28":"#create pmc corpus token list from full dictionary\nnewArticleListPmc = newPDList(articlesPmc,final_dict)\n","01b14f96":"#create bio corpus token list from full dictionary\nnewArticleListBio = newPDList(articlesBio,final_dict)","e58e39a6":"#save com article tokens\noutput = open('trainingTokensCom.pkl', 'wb')\npickle.dump(newArticleList, output)\noutput.close()","d898dfc4":"#save non com article tokens\noutput = open('trainingTokensNonCom.pkl', 'wb')\npickle.dump(newArticleListNonCom, output)\noutput.close()","976e5aea":"#save pmc article tokens\noutput = open('trainingTokensPmc.pkl', 'wb')\npickle.dump(newArticleListPmc, output)\noutput.close()","2c136fcd":"#save bio article tokens\noutput = open('trainingTokensBio.pkl', 'wb')\npickle.dump(newArticleListBio, output)\noutput.close()","9ec650c6":"def addToPanda(text,newArticleList,base):\n    \n\n    articles = []\n    count = 0\n    for x in text:\n        dataList = [\"\",\"\",'']\n        with open(base+x) as f:\n          \n            d = json.loads(f.read())\n            dataList[0] = d['paper_id']\n            dataList[1] = d['metadata']['title']\n            string = \"\"\n            for n in d['body_text']:\n                string += n['text']\n            dataList[2] = newArticleList[count]\n            dataList[3] = string\n            articles.append(dataList)\n            count = count + 1\n        \n    dataFrame = pd.DataFrame(articles, columns = ['ID','Title', 'Corpus', 'CorpusText']) \n    length = []\n    [length.append(len(str(text))) for text in dataFrame['Corpus']]\n    dataFrame['Length'] = length\n    return dataFrame","90b05212":"#data frame of first zip\nnewDf = addToPanda(contentList,newArticleList,\"\/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\/\")","f35ea1dd":"#data frame of pmc zip\nnewDfPmc = addToPanda(contentListPmc, newArticleListPmc,\"\/kaggle\/input\/CORD-19-research-challenge\/custom_license\/custom_license\/\")","bb7310a1":"#data frame if bio zip\nnewDfBio = addToPanda(contentListBio, newArticleListBio,\"\/kaggle\/input\/CORD-19-research-challenge\/biorxiv_medrxiv\/biorxiv_medrxiv\/\")","0146e34b":"newDfNonCom = addToPanda(contentListNonCom, newArticleListNonCom,\"\/kaggle\/input\/CORD-19-research-challenge\/noncomm_use_subset\/noncomm_use_subset\/\")","3ad946a4":"#concatenate tables to form one table\nframes = [newDf, newDfPmc, newDfBio, newDfNonCom]\n\nresult = pd.concat(frames)","fc00ea9c":"#check panda frame to ensure corpus of integers\nresult.head()","2786b794":"#check smallest and largest article lenghts\nmin(result['Length']), max(result['Length']), round(sum(result['Length'])\/len(result['Length']))","d76a0d0c":"#remove articles with lengths less than 5000 tokens\nresult2 = result.copy()\nresult2 = result2.drop(result2['Corpus'][result2['Length'] < 5000].index, axis = 0)\n\n","8ad0e4a9":"#check again for any discrepencies in legnth\nmin(result2['Length']), max(result2['Length']), round(sum(result2['Length'])\/len(result2['Length']))","e94df2c2":"#save final table\noutput = open('trainingFinal.pkl', 'wb')\npickle.dump(result2, output)\noutput.close()","31db5e2a":"result2 = pd.read_pickle(\"\/kaggle\/input\/covidsaved\/trainingFinal (1).pkl\")\n","5713f0af":"from sklearn.metrics.pairwise import cosine_similarity\ndef tfIdfSearch(docs,docTitles,title,num_neighbors,df4):\n  try:\n    test_row = (df4.loc[df4['Title'] == title]['CorpusText'].tolist())[0]\n  except:\n    test_row = title\n    title = \"your text\"\n  \n  docs= docs + (test_row,)\n  docTitles= docTitles + (title,)\n  tfidf_vectorizer = TfidfVectorizer()\n  tfidf_matrix = tfidf_vectorizer.fit_transform(docs)\n  l = cosine_similarity(tfidf_matrix[-1:], tfidf_matrix)\n  newDt = {}\n  counting = 0\n\n  for x in l[0]:\n      newDt[x] = {docTitles[counting]:docs[counting]}\n      counting = counting + 1\n  neighborList = []\n\n  neighCount = 0\n  for i in sorted (newDt,reverse=True): \n          if neighCount != num_neighbors:\n              neighborList.append((\"distance: \" + str(i),newDt[i]))\n              neighCount = neighCount + 1\n          elif neighCount >= neighCount:\n              return neighborList\n\n#Euclidean distance between two vectors\ndef euclidean_distance(row1, row2):\n    distance = 0.0\n    #pad empty columns\n    \n    if len(row1) < len(row2):\n      N = len(row2) - len(row1)\n      for x in range(N):\n        row1.append(0)\n    \n    elif len(row1) > len(row2):\n      N = len(row1) - len(row2)\n      for x in range(N):\n        row2.append(0)\n    for i in range(len(row2)-1):\n        \n       \n        distance += (row1[i] - row2[i])**2\n    return sqrt(distance)\ndef cosineSim(row1,row2):\n  #pad empty columns\n  \n  if len(row1) < len(row2):\n    N = len(row2) - len(row1)\n    for x in range(N):\n      row1.append(0)\n    \n  elif len(row1) > len(row2):\n    N = len(row1) - len(row2)\n    for x in range(N):\n      row2.append(0)\n  \n  result = 1 - spatial.distance.cosine(row1, row2)\n  \n  return result\n  \n#Find the closest neighbors\ndef get_neighborsTitle(title,train, test_row, num_neighbors,trainTxt,df4,cosEu):\n    distances = list()\n    distDict = {}\n    newCount = 0\n    test_row = (df4.loc[df4['Title'] == test_row]['Corpus'].tolist())[0]\n    if cosEu == \"cosine\":\n      for train_row in (train):\n        if train_row != test_row:\n            dist = cosineSim(test_row, train_row)\n            distances.append((train_row, \"distance \"+ str(dist)))\n            distDict[dist] = {title.iloc[newCount]:trainTxt.iloc[newCount]}\n        newCount = newCount + 1\n    else:\n      for train_row in (train):\n        if train_row != test_row:\n            dist = euclidean_distance(test_row, train_row)\n            distances.append((train_row, \"distance \"+ str(dist)))\n            distDict[dist] = {title.iloc[newCount]:trainTxt.iloc[newCount]}\n        newCount = newCount + 1\n    \n        \n    distances.sort(key=lambda tup: tup[1])\n    neighborList = []\n    neighCount = 0\n    for i in sorted (distDict): \n        if neighCount != num_neighbors:\n            neighborList.append((distDict[i],\"distance: \" + str(i)))\n            neighCount = neighCount + 1\n        elif neighCount >= neighCount:\n            return neighborList\ndef transformText(text,dictionary):\n  tokens1 = nltk.word_tokenize(text)\n  copyCheck = tokens1.copy()\n  punct = punctuationCheck(copyCheck)\n  covid = covidCheck(copyCheck)\n  dateC = dateCheck(copyCheck)\n  ls = stopwordCheck(copyCheck)\n  newW = numberCheck(copyCheck)\n  countryL = countryCheck(copyCheck)\n  month = monthCheck(copyCheck)\n  #retrive unique vocab list\n  #uniqueLs = get_vocabs(copyCheck)\n  #vocabDict = dictionaryM(uniqueLs)\n  #used dictionary to alter string tokens to integers\n  newList = integer_representations(dictionary,copyCheck)\n  return newList\ndef get_neighborsText(title,train, test_row, num_neighbors,dictionary,trainTxt,cosEu):\n    distances = list()\n    distDict = {}\n    newCount = 0\n    test_row = transformText(test_row,dictionary)\n    if cosEu == \"cosine\":\n      for train_row in (train):\n        if train_row != test_row:\n            dist = cosineSim(test_row, train_row)\n            distances.append((train_row, \"distance \"+ str(dist)))\n            distDict[dist] = {title.iloc[newCount]:trainTxt.iloc[newCount]}\n        newCount = newCount + 1\n    else:\n      for train_row in (train):\n        if train_row != test_row:\n            dist = euclidean_distance(test_row, train_row)\n            distances.append((train_row, \"distance \"+ str(dist)))\n            distDict[dist] = {title.iloc[newCount]:trainTxt.iloc[newCount]}\n        newCount = newCount + 1\n        \n    distances.sort(key=lambda tup: tup[1])\n    neighborList = []\n    neighCount = 0\n    for i in sorted (distDict): \n        if neighCount != num_neighbors:\n            neighborList.append((\"distance: \" + str(i),distDict[i]))\n            neighCount = neighCount + 1\n        elif neighCount >= neighCount:\n            return neighborList","4753a09d":"#build corpus text tuple and title text tuple for tf idf calculations\ndocuments = (\n\n)\ndocumentTitles = ()\nfor x in range(len(result2)):\n  documents = documents + (result2.iloc[x]['CorpusText'],)\n  documentTitles = documentTitles + (result2.iloc[x]['Title'],)\n\n","5fc6cb1c":"#save save document tuples\noutput = open('trainingDocTuples.pkl', 'wb')\npickle.dump(documents, output)\noutput.close()","af2451f2":"#save save document tuples\noutput = open('trainingDocTitleTuples.pkl', 'wb')\npickle.dump(documentTitles, output)\noutput.close()","b3fe8e01":"documents = pd.read_pickle(\"\/kaggle\/input\/covidsaved\/trainingDocTuples (1).pkl\")\ndocumentTitles = pd.read_pickle(\"\/kaggle\/input\/covidsaved\/trainingDocTitleTuples (1).pkl\")\n\n","24834828":"txt1 = \"Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery.\"\ntxt2 = \"Prevalence of asymptomatic shedding and transmission (particularly children).\"\ntxt3 = \"Seasonality of transmission.\"\ntxt4 = \"Physical science of the coronavirus ( charge distribution, adhesion to hydrophilic or phobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding).\"\ntxt5 = \"Persistence and stability on a multitude of substrates and sources (nasal discharge, sputum, urine, fecal matter, blood).\"\ntxt6 = \"Persistence of virus on surfaces of different materials (copper, stainless steel, plastic).\"\ntxt7 = \"Natural history of the virus and shedding of it from an infected person\"\ntxt8 = \"Implementation of diagnostics and products to improve clinical processes\"\ntxt9 = \"Disease models, including animal models for infection, disease and transmission\"\ntxt10 = \"Tools and studies to monitor phenotypic change and potential adaptation of the virus\"\ntxt11 = \"Immune response and immunity\"\ntxt12 = \"Effectiveness of movement control strategies to prevent secondary transmission in health care and community settings\"\ntxt13 = \"Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings\"\ntxt14 = \"Role of the environment in transmission\"","0e7ca8b5":"riskFactors = [\"Data on potential risks factors\",\"Smoking, pre-existing pulmonary disease\",\"Co-infections (determine whether co-existing respiratory or viral infections make the virus more transmissible or virulent) and other co-morbidities Neonates and pregnant women\",\"Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences.\",\"Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors\",\"Severity of disease, including risk of fatality among symptomatic hospitalized patients, and high-risk patient groups\",\"Susceptibility of populations\",\"Public health mitigation measures that could be effective for control\"]","be81d5db":"infoShare1 = \"Methods for coordinating data-gathering with standardized nomenclature.\"\ninfoShare2 = \"Sharing response information among planners, providers, and others.\"\ninfoShare3 = \"Understanding and mitigating barriers to information-sharing.\"\ninfoShare4 = \"How to recruit, support, and coordinate local (non-Federal) expertise and capacity relevant to public health emergency response (public, private, commercial and non-profit, including academic).\"\ninfoShare5 = \"Integration of federal\/state\/local public health surveillance systems.\"\ninfoShare6 = \"Value of investments in baseline public health response infrastructure preparedness\"\ninfoShare7 = \"Modes of communicating with target high-risk populations (elderly, health care workers).\"\ninfoShare8 = \"Risk communication and guidelines that are easy to understand and follow (include targeting at risk populations\u2019 families too).\"\ninfoShare9 = \"Communication that indicates potential risk of disease to all population groups.\"\ninfoShare10 = \"Misunderstanding around containment and mitigation.\"\ninfoShare11 = \"Action plan to mitigate gaps and problems of inequity in the Nation\u2019s public health capability, capacity, and funding to ensure all citizens in need are supported and can access information, surveillance, and treatment.\"\ninfoShare12 = \"Measures to reach marginalized and disadvantaged populations.\"\ninfoShare13 = \"Data systems and research priorities and agendas incorporate attention to the needs and circumstances of disadvantaged populations and underrepresented minorities.\"\ninfoShare14 = \"Mitigating threats to incarcerated people from COVID-19, assuring access to information, prevention, diagnosis, and treatment.\"\ninfoShare15 = \"Understanding coverage policies (barriers and opportunities) related to testing, treatment, and care\"","edee8581":"considerations1 = \"Efforts to articulate and translate existing ethical principles and standards to salient issues in COVID-2019\"\nconsiderations2 = \"Efforts to embed ethics across all thematic areas, engage with novel ethical issues that arise and coordinate to minimize duplication of oversight\"\nconsiderations3 = \"Efforts to support sustained education, access, and capacity building in the area of ethics\"\nconsiderations4 = \"Efforts to establish a team at WHO that will be integrated within multidisciplinary research and operational platforms and that will connect with existing and expanded global networks of social sciences.\"\nconsiderations5 = \"Efforts to develop qualitative assessment frameworks to systematically collect information related to local barriers and enablers for the uptake and adherence to public health measures for prevention and control. This includes the rapid identification of the secondary impacts of these measures. (e.g. use of surgical masks, modification of health seeking behaviors for SRH, school closures)\"\nconsiderations6 = \"Efforts to identify how the burden of responding to the outbreak and implementing public health measures affects the physical and psychological health of those providing care for Covid-19 patients and identify the immediate needs that must be addressed.\"\nconsiderations7 = \"Efforts to identify the underlying drivers of fear, anxiety and stigma that fuel misinformation and rumor, particularly through social media.\"","8ad62708":"surveil1 = \"How widespread current exposure is to be able to make immediate policy recommendations on mitigation measures. Denominators for testing and a mechanism for rapidly sharing that information, including demographics, to the extent possible. Sampling methods to determine asymptomatic disease (e.g., use of serosurveys (such as convalescent samples) and early detection of disease (e.g., use of screening of neutralizing antibodies such as ELISAs).\"\nsurveil2 = \"Efforts to increase capacity on existing diagnostic platforms and tap into existing surveillance platforms.\"\nsurveil3 = \"Recruitment, support, and coordination of local expertise and capacity (public, private\u2014commercial, and non-profit, including academic), including legal, ethical, communications, and operational issues.\"\nsurveil4 = \"National guidance and guidelines about best practices to states (e.g., how states might leverage universities and private laboratories for testing purposes, communications to public health officials and the public).\"\nsurveil5 = \"Development of a point-of-care test (like a rapid influenza test) and rapid bed-side tests, recognizing the tradeoffs between speed, accessibility, and accuracy.\"\nsurveil6 = \"Rapid design and execution of targeted surveillance experiments calling for all potential testers using PCR in a defined area to start testing and report to a specific entity. These experiments could aid in collecting longitudinal samples, which are critical to understanding the impact of ad hoc local interventions (which also need to be recorded).\"\nsurveil7 = \"Separation of assay development issues from instruments, and the role of the private sector to help quickly migrate assays onto those devices.\"\nsurveil8 = \"Efforts to track the evolution of the virus (i.e., genetic drift or mutations) and avoid locking into specific reagents and surveillance\/detection schemes.\"\nsurveil9 = \"Latency issues and when there is sufficient viral load to detect the pathogen, and understanding of what is needed in terms of biological and environmental sampling.\"\nsurveil10 = \"Use of diagnostics such as host response markers (e.g., cytokines) to detect early disease or predict severe disease progression, which would be important to understanding best clinical practice and efficacy of therapeutic interventions.\"\nsurveil11 = \"Policies and protocols for screening and testing.\"\nsurveil12 = \"Policies to mitigate the effects on supplies associated with mass testing, including swabs and reagents.\"\nsurveil13 = \"Technology roadmap for diagnostics.\"\nsurveil14 = \"Barriers to developing and scaling up new diagnostic tests (e.g., market forces), how future coalition and accelerator models (e.g., Coalition for Epidemic Preparedness Innovations) could provide critical funding for diagnostics, and opportunities for a streamlined regulatory environment.\"\nsurveil15 = \"New platforms and technology (e.g., CRISPR) to improve response times and employ more holistic approaches to COVID-19 and future diseases.\"\nsurveil16 = \"Coupling genomics and diagnostic testing on a large scale.\"\nsurveil17 = \"Enhance capabilities for rapid sequencing and bioinformatics to target regions of the genome that will allow specificity for a particular variant.\"\nsurveil18 = \"Enhance capacity (people, technology, data) for sequencing with advanced analytics for unknown pathogens, and explore capabilities for distinguishing naturally-occurring pathogens from intentional.\"\nsurveil19 = \"One Health surveillance of humans and potential sources of future spillover or ongoing exposure for this organism and future pathogens, including both evolutionary hosts (e.g., bats) and transmission hosts (e.g., heavily trafficked and farmed wildlife and domestic food and companion species), inclusive of environmental, demographic, and occupational risk factors.\"","7fd057bb":"txt = \"Your Text\"\ntitle = \"Late viral or bacterial respiratory infections in lung transplanted patients: impact on respiratory function\"\n#here you can replace txt13 with your text in txt or by an existing title\n\nneighborCount = 4\nneighborList = tfIdfSearch(documents,documentTitles,txt13,neighborCount,result2)","f3062367":"neighborList","c0d3f9ce":"#enter in what you want to search for in text\ntext2 = \"Your Text\"\ntext = txt1\nneighborCount = 4\nneighbors = get_neighborsText(result2['Title'],result2['Corpus'], text, neighborCount,final_dict,result2['CorpusText'],\"euclidean\")\nprint(\"original: \")\nprint(text)\n\n#Each new neighbor represents a similar article\n\nfor neighbor in neighbors:\n    print('newNeighbor')\n    print(neighbor)","6e2acbd2":"#enter in article title \ntitle = \"A Mini-Review on the Epidemiology of Canine Parvovirus in China\"\nneighborCount = 4\n\nneighbors = get_neighborsTitle(result2['Title'],result2['Corpus'], title, neigborCount,result2['CorpusText'],result2,'cosine')\nprint(\"original title: \")\nprint(title)\n\nfor neighbor in neighbors:\n    print('newNeighbor')\n    print(neighbor)","bfc4ee7f":"## What has been published about ethical and social science considerations?","8aaba014":"**Save dictionary in case for later use**","6002059d":"**Helper function to load Zip files from comm_use, Noncomm_use, biorxiv, and pmc**","89df53b7":"**Flatten list of lists from articles in order to develop comprehensive dictionary**","d6752ec3":"**Helper functions to create dictionary of term frequencies in order to create integer representations for text corpora**","3702bafc":"**Update article word tokens with integer represenations from dictionary**","f783e0f3":"**Queries to Search**\n\nPrompt\n- \"What is known about transmission, incubation, and environmental stability?\"","964f1c64":"## Clean and Tag Text from corpora\n","625ab41e":"##  What do we know about COVID-19 risk factors?","af283951":"## What do we know about diagnostics and surveillance?","0daf5f54":"**Helper Function**\n- Create new pandas dataframe for corpus text\n- Include column with corpus of integer representations","588dd55b":"**Create basic terms lists in order to tag text corpora to eliminate similar words from document comparisons**","5cf706cf":"## Open Research Dataset Challenge (CORD-19)\n\n### What is known about transmission, incubation, and environmental stability?\n\n* Note - running this notebook in a personal jupyter notebook, free google colab, or aws environment will be quicker than running with the limited computational capacity in Kaggle\n\n* To speed things up, I have added the developed pkl files of the data so that you can just load in the information instead of going through each step, the four pkl files that are needed to run these operations without doing all of the data processing are \/kaggle\/input\/covidsaved\/comboDictionary.pkl ,\/kaggle\/input\/covidsaved\/trainingFinal (1).pkl, \/kaggle\/input\/covidsaved\/trainingDocTuples (1).pkl, and\/kaggle\/input\/covidsaved\/trainingDocTitleTuples (1).pkl\n\n* I have created lines of code to load these files in within this document to save time","df89d03e":"## What has been published about information sharing and inter-sectoral collaboration?","214a37ae":"**Data cleaning**","eabc7ac8":"***Remove words below frequency threshold***","8aa78f59":"What has been published about information sharing and inter-sectoral collaboration? What has been published about data standards and nomenclature? What has been published about governmental public health? What do we know about risk communication? What has been published about communicating with high-risk populations? What has been published to clarify community measures? What has been published about equity considerations and problems of inequity?","5cf116bf":"**Tokenize article corpus for eventual word tagging**","c576a7aa":"## Search by Term frequency and inverse document frequency (TF-IDF)\n\n***Overall Tf-idf by title is the most effective search however free text is also effective***\n\n***TF-IDF is also the least computational taxing, which means it is faster to run than the k-nearest neighbor approach***\n\n\n- Search by an existing query\n    - Search by replacing the title variable with any of the queries above(eg, txt1,txt2,infoShare1,riskFactors[1], surveil1,considerations1,etc) into tfIdfSearch\n- Search by article title\n    - These searches produce the best results because they include many more words to compare term frequencies across mutliple documents\n- Search by free text\n    - Searches with free text will produce results less comprehensive than by title search because there is less text to compare term frequencies across documents\n\n\n- The data is returned as a list of (distance of the article to the text\/title,{article title: article text})\n- the amount of articles are determined by the neighborCount","e32e75e6":"## Helper Functions\n\n**The Objective of these helper functions are eliminate variance in the word tokens to improve overall prediction**\n\n- dateCheck\n    - Removing date entries like 2009,2020,etc and replacing these terms with year to prevent\n    - Replaced with << year >>\n    \n    \n- stopwordCheck\n    - Remove stop words like and,but,or,etc\n    - Replaced with << stopword >>\n    \n    \n- numberCheck\n    - Remove numbers to be replace with << realnumber >>\n   \n    \n- countryCheck\n    - Remove countries and replace with << country >>\n    \n \n- covidCheck\n    - Remove coronavirus terms to replace with << COVID-19 >>\n    \n   \n- punctuationCheck\n    - Remove punction terms to replace with << Punctuation >>\n    \n    \n- monthCheck\n    - Remove month terms to replace with << Month >>\n    \n    \n- integer_representations\n    - Takes the dictionary of the corpus where words are represented as integers\n    - Uses dictionary to change corpus word tokens with dictionary integer tokens","e776dd2d":"## Pandas Helper function \n\n\n- Turn the json files into a pandas data frame for view\n\n- Helper function also combines article segments into single article text for each document -- Output of list of articles is \"uniqueArticles\"","63eb08d2":"## K- Nearest Neighbor and TF-IDF\n**Used to return similar articles**\n\n**Four different methods are used**\n\n\n- **K-Nearest:** Title search for similarity based on distance\n  - Similarity based on Euclidean Distance\n  - Or Simalrity based on Cosine Distance\n- **K-Nearest:** Text search for similarity based on distance\n  - Similarity based on Euclidean Distance\n  - Or Simalrity based on Cosine Distance\n- **TF-IDF:** Title search for similarity based on term frequency\n- **TF-IDF:** Text search for similarity based on term frequency\n","23cbb0af":"What has been published concerning ethical considerations for research? What has been published concerning social sciences at the outbreak response?","9b01844f":"## Search using cosine or euclidean distance\n***Overall search by cosine and euclidean is the most effective when searching by article title***\n\n***K-Nearest neighbor search by euclidean distance\/cosine distance is computatinally more expensive than searching by, TF-IDF, this means it requires more processing power to complete than the standard kaggle environment, try running this in your own jupyter environment, aws jupyter environemt, or google colab python environment***\n\n***K-nearest approach (ie. integer tokenization and distance search) was primarily developed in addition to tf-idf for future use of NLP tasks (eg. information extraction, text generation, etc)***\n\n\n- Search by an existing query\n    - Search by replacing the text variable with any of the queries above(eg, txt1,txt2,infoShare1,riskFactors[1],surveil1,considerations1,etc) into get_neighborsText\n- Search by article title\n    - These searches produce the best results because the include many more words to compare word distances in vector space across mutliple documents\n    - Title search is completed by get_neighborsTitle\n- Search by free text\n    - Searches with free text will produce results less comprehensive than by title search because there are less words tokenize and compare with other documents in vector space\n    - Free text search is completed by get_neighborsText\n    \n    \n**These functions return the articles similar to the text or title**\n\n- The data is returned as a list of (distance of the article to the text\/title,{article title: article text})\n- the amount of articles are determined by the neighborCount"}}