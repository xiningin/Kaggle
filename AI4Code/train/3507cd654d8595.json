{"cell_type":{"36262eef":"code","0d26f4a4":"code","15f982b2":"code","f9e4b9d6":"code","1114aa6b":"code","ed3896f3":"code","077d6749":"code","530fe795":"code","c33a66b0":"markdown","69a0e54d":"markdown","66351905":"markdown","3c01cae3":"markdown","656efc51":"markdown"},"source":{"36262eef":"import numpy as np, pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.compose import ColumnTransformer\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_log_error  ####USE param squared=False to evaluate using RMSE\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n##### TEST\ntest_dataset=pd.read_csv(os.path.join(dirname,'test.csv'),index_col='Id')","0d26f4a4":"def read_datasets():\n    ''' READ THE DATA FROM train.csv FILE AND RETURN DATASETS '''\n    train_dataset=pd.read_csv(os.path.join(dirname,'train.csv'),index_col='Id')\n    fulltrain_y=train_dataset.SalePrice\n    fulltrain_X=train_dataset.drop(columns=['SalePrice'])\n    \n    #''' DROP COLUMNS WITH 200+ N\/A VALUES AFTER MEANINGFULLY VERIFYING DESCRIPTION AND RELEVANCE TO PRICE PREDICTION '''\n    x=train_dataset.isna().sum()\n    verysparse_columns=list(x[x>200].index)\n    print(\"Columns with 200+ N\/A values= \",verysparse_columns)\n    #print(\"I think we can drop off 'PoolQC' and 'Alley'.\")\n    fulltrain_X=fulltrain_X[set(fulltrain_X.columns)-set(['PoolQC','Alley','FireplaceQu','Fence'])]\n    \n    #''' REPLACE N\/A IN REST CATEGORICAL VARIABLES WITH \"Other\" VALUE '''\n    #rest_categorical_sparse_columns=['MiscFeature','Fence','FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','BsmtExposure','BsmtFinType2','BsmtQual','BsmtCond','BsmtFinType1','MasVnrType','Electrical']\n    #for col in rest_categorical_sparse_columns:\n    #    fulltrain_X[col].fillna(value='Other',inplace=True)\n    \n    train_X,val_X, train_y,val_y=train_test_split(fulltrain_X,fulltrain_y,random_state=0)\n    return (train_X,train_y,val_X,val_y)\n\ntrain_X,train_y,val_X,val_y=read_datasets()\n\n\n''' EXTRACT THE NUMERICAL FIELDS AND CATEGORICAL FIELDS WITH LOW CARDINALITY '''\nnumerical_cols=list(train_X.select_dtypes(exclude=['object']).columns)\ncategorical_cols=list(train_X.select_dtypes(include=['object']).columns)\nlowcardinality_categorical_cols = [col for col in categorical_cols if train_X[col].nunique()<10]\n\nrequired_cols=numerical_cols+lowcardinality_categorical_cols\ntrain_X=train_X[required_cols]\nval_X=val_X[required_cols]\n\nprint(\"\\nTRAIN_X:\")\ntrain_X.describe()","15f982b2":"def exec_pipeline(xgb_estimators, xgb_max_depth, xgb_learning_rate):\n    extremeGradientBoost_RegressorModel=XGBRegressor(n_estimators=xgb_estimators,max_depth=xgb_max_depth,learning_rate=xgb_learning_rate,random_state=0)\n\n    numerical_transformer=SimpleImputer(strategy='mean')\n    categorical_transformer=Pipeline(steps=[\n        #Commenting out this line since we handled NULL values in first function itself\n        ('impute_categ_vals',SimpleImputer(strategy='most_frequent')),\n        ('onehotenc',OneHotEncoder(sparse=False,handle_unknown='ignore'))\n    ])\n\n    preprocessor=ColumnTransformer(transformers=[\n        ('numerical_preprocess', numerical_transformer, numerical_cols),\n        ('categorical_preprocess', categorical_transformer, lowcardinality_categorical_cols)\n    ])\n\n    model_pipeline=Pipeline(steps=[\n        ('preprocess_fields',preprocessor),\n        ('xgbregr_model',extremeGradientBoost_RegressorModel)\n    ])\n\n    model_pipeline.fit(train_X,train_y)\n    predictions=model_pipeline.predict(val_X)\n    \n    rmse=np.sqrt(mean_squared_log_error(predictions,val_y))\n    print('N_ESTIMATORS={} , MAX_DEPTH={} , LEARNING_RATE={} ==> RMSE={}'.format(xgb_estimators, xgb_max_depth, xgb_learning_rate,rmse))\n    return rmse","f9e4b9d6":"'''losses=[]\n#1500: 0.123142\n#1600: 0.1218009\nestimators=[1200,1400,1600,1800,2000,2100,2200,2300,2400,2500]\n\n#0.0084:   0.124657\n#0.008372: 0.1218009\n#learning_rates=[0.008372, 0.0083718, 0.0083722, 0.0083716, 0.00837224, 0.0083714, 0.0083728]\nlearning_rates=[0.001,0.002,0.003,0.004,0.005,0.006,0.007,0.008,0.0089,0.009]\n\n#6: 0.1234141\nmax_depths=[6,8,10,12,14,16,18,20]\nfor lr in learning_rates:\n    for e in estimators:\n        for d in max_depths:\n            losses.append((e,d,lr,exec_pipeline(e,d,lr)))\n        print(\"\")\n    print(\"-\")\n\nprint(losses)\nexp_losses=pd.DataFrame(losses,columns=['N_ESTIMATORS','MAX_DEPTH','LEARNING_RATE','RMSE'])\nexp_losses.to_csv('exp_losses.csv')\n'''","1114aa6b":"def exec_pipeline_with_kcross_val(xgb_estimators, xgb_max_depth, xgb_learning_rate,kfolds=4):\n    extremeGradientBoost_RegressorModel = XGBRegressor(n_estimators = xgb_estimators, max_depth = xgb_max_depth, learning_rate = xgb_learning_rate,\n                                                       random_state=0)\n\n    numerical_transformer=SimpleImputer(strategy='mean')\n    categorical_transformer=Pipeline(steps=[\n        ('impute_categ_vals',SimpleImputer(strategy='most_frequent')),\n        ('onehotenc',OneHotEncoder(sparse=False,handle_unknown='ignore'))\n    ])\n\n    preprocessor=ColumnTransformer(transformers=[\n        ('numerical_preprocess', numerical_transformer, numerical_cols),\n        ('categorical_preprocess', categorical_transformer, lowcardinality_categorical_cols)\n    ])\n\n    model_pipeline=Pipeline(steps=[\n        ('preprocess_fields',preprocessor),\n        ('xgbregr_model',extremeGradientBoost_RegressorModel)\n    ])\n\n    kf=KFold(shuffle=True,n_splits=kfolds,random_state=0)\n    fullX=pd.concat([train_X,val_X],axis=0)\n    fully=pd.concat([train_y,val_y],axis=0)\n    model_pipeline.fit(train_X,train_y)\n    rmse=np.sqrt(-1*cross_val_score(model_pipeline, fullX, fully, cv=kf, scoring='neg_mean_squared_log_error'))\n    \n    print('N_ESTIMATORS={} , MAX_DEPTH={} , LEARNING_RATE={} ==> RMSE={}'.format(xgb_estimators, xgb_max_depth, xgb_learning_rate,rmse))\n    return (np.mean(rmse),model_pipeline)\n","ed3896f3":"'''estimators=[1200,1400,1600,1800,2000,2100,2200,2300,2400,2500]\nkfold_losses=[]\nfor e in estimators:\n    kfold_losses.append([e,14,0.0089,4,(exec_pipeline_with_kcross_val(e,14,0.0089,4))])'''\n#0.1381 => 0.133829\nrmse,model_pipeline=exec_pipeline_with_kcross_val(1400,6,0.0081,4)\nprint(rmse)","077d6749":"test_dataset=test_dataset[set(test_dataset.columns)-set(['PoolQC','Alley','FireplaceQu','Fence'])]\npredictions=model_pipeline.predict(test_dataset)\npredictions\nsubmission_df=pd.DataFrame()\nsubmission_df['Id']=test_dataset.index\nsubmission_df['SalePrice']=predictions\n\nsubmission_df.to_csv('submission.csv',index=False)","530fe795":"for i in submission_df.SalePrice:\n    print(i)","c33a66b0":"# KFold Cross validation enhancement\nAdded in a Kfold CV strategy to ensure different subsets of validation data give appropriate results.\n","69a0e54d":"# Notebook Outline\nThis notebook aims to explore the [Ames Housing Dataset](http:\/\/www.amstat.org\/publications\/jse\/v19n3\/decock.pdf)\n\n1) Start by importing the datasets\n\n2) Dig deeper into meta-information strictly from a data-distribution perspective\n\n3) Identify numerical columns = 36\n\n4) Identify categorical columns with less than 10 cardinality (these 'n' will be converted to 'n' One-hot columns) = 40","66351905":"# Hyperparameters Experimentation\nJust used 3 nested-for loops to explore effect of arbitrary hyperparams.\nCan be substituted by the GridSearchCV module from sklearn","3c01cae3":"# Pipelined Architecture\nA pipelined ML architecture will help create a generic flow for the data to move from raw format to a cleaner form that's ready to be pushed into the model to start making predictions\n\nI've used a function to create a pipeline and return the RMSE because I wanted to experiment with hyperparams without changing much of the other code.","656efc51":"# Hyperparameters Experimentation with KFold CV\nUsed a nested-for loop to explore effect of arbitrary hyperparams.\nCan be substituted by the GridSearchCV module from sklearn"}}