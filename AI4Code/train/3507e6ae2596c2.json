{"cell_type":{"eed285b7":"code","c1906cf5":"code","63384017":"code","aa10cfed":"code","9a206a9c":"code","c5db400b":"code","a5551175":"code","b6de85bb":"code","1e666be3":"code","d57b1665":"code","7b4e5c13":"code","cefc540d":"code","62ed2b12":"code","9ed60aae":"code","1189ab37":"code","16de7c6f":"code","4dda7c3e":"code","9d3788f1":"code","d2b98a2e":"code","e3a6a344":"code","78d6d519":"code","6a5503db":"code","6b36af4e":"code","3cf65c2c":"code","f5833626":"markdown"},"source":{"eed285b7":"#We shall import some stuff here\n\nimport pandas as pd\nimport numpy as np\nimport re\nimport string\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tag import pos_tag\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D","c1906cf5":"%matplotlib inline","63384017":"# Initializing a lemmatizer\nlemmatizer = WordNetLemmatizer()","aa10cfed":"# Lets open the file and and extract all the content line by line into a list\nwith open('..\/input\/bbchealth.txt') as file:\n    contents = file.readlines()\n    \n# Fancy regex for extracting data. Because I'm awesome and I refuse to split it.\nregex = re.compile(r'(?P<tweet_id>.*)\\|(?P<date>.*)\\|(?P<news>.*)\\s?(?P<link>http:\/\/.*)')","9a206a9c":"# I hope the regex words\nre.search(regex, contents[0]).groupdict()\n\n# And it worked. Cool","c5db400b":"# You all know this. Don't pretend that you don't.\ndf = pd.DataFrame(columns=['tweet_id', 'date', 'news', 'link'])\n\n# Self explanatory\nfor line in contents:\n    line_data = re.search(regex, line).groupdict()\n    df = df.append(line_data, ignore_index=True)","a5551175":"# Our dataset looks like this. Not too shabby.\ndf.head()","b6de85bb":"# Lets define some function to normalize, lemmatize the line and getting POS tags. \n# Normalizing, the following methods in order -- lower, strip white spaces, remove punctiations and \n# digits, and finally word tokenizing\n\ndef normalize(line):\n    line = line.lower().strip()\n    line = ''.join([char for char in line if char not in string.punctuation+string.digits])\n    return word_tokenize(line)\n\n# Lemmatizing each word in each line\ndef lemmatize_sent(line_tokens):\n    return list(map(lemmatizer.lemmatize, line_tokens))\n\n# POS tagging, This one returns just the POS tags in order.\ndef tokens(line_tokens):\n    word_tags = pos_tag(line_tokens)\n    return list(zip(*word_tags))[1]","1e666be3":"# Let's apply all those functions to the columns.\ndf['news_tokens'] = df['news'].map(normalize)\ndf['corresponding_tags'] = df['news_tokens'].map(tokens)\ndf['lemmatized_news_tokens'] = df['news_tokens'].map(lemmatize_sent)","d57b1665":"# Dropping unnecessary columns, that I think I don't need.\ndf.drop(labels=['link'], inplace=True, axis=1)\ndf.drop(labels=['tweet_id'], inplace=True, axis=1)","7b4e5c13":"# Let's see what our dataset looks like again\ndf.head()","cefc540d":"# Now that we've cleaned and prepared our dataset, let's do some transformation\n# We'll create a tfidf matrix of all the documents. \n# Tfidf is generally used in document similarity. Because the dot product followed by division by the their\n# normals give you their cosine similarity.\ntfidf = TfidfVectorizer()\ntfidf_matrix = tfidf.fit_transform(df['lemmatized_news_tokens'].map(lambda x: ' '.join(x)).tolist())","62ed2b12":"# Let's do some actual machine learning now. We'll be doing KMeans for now.\n# Let's take the default value for number of clusters(n_clusters=8).\n# because we don't know any crap.\n# And later use elbow method and silhoutte score to find the right number of \n# clusters\nkmeans = KMeans()\nkmeans.fit(tfidf_matrix)","9ed60aae":"# Yup, right number of labels.\nnp.unique(kmeans.labels_)","1189ab37":"tfidf_matrix","16de7c6f":"# Now, plotting time. But we can't just plot a sparse matrix of size 3929x3955.\n# That's stupid. So, let's trim down the dimensions to 2.\n# This is going to lose some information, but do we have a choice?\npca = PCA(n_components=2)\npca_matrix = pca.fit_transform(tfidf_matrix.A)","4dda7c3e":"# Let's plot the cluster centers\nplt.figure(figsize=(5, 5))\ncluster_centers = pca.fit_transform(kmeans.cluster_centers_)\nsns.scatterplot(x=cluster_centers[:,0], y=cluster_centers[:,1], hue=np.unique(kmeans.labels_))\n    \nplt.xlim((-0.4, 0.4))\nplt.ylim((-0.4, 0.4))\n\n# Oh well. Horrible.","9d3788f1":"# Plotting the whole data, we get the following graph.\nplt.figure(figsize=(10, 10))\nsns.scatterplot(x=pca_matrix[:,0], y=pca_matrix[:,1], hue=kmeans.labels_)","d2b98a2e":"# Did you just think we're gonna stop at 2D plots?\n# If you did, you're wrong buddy. Behold 3D plots.\npca_3d = PCA(n_components=3)\npca_matrix_3d = pca_3d.fit_transform(tfidf_matrix.A)","e3a6a344":"fig = plt.figure(figsize=(5, 5))\nax = fig.add_subplot(111, projection='3d')\nfor p, l in zip(pca_matrix_3d, kmeans.labels_):\n    plt.scatter(*p, 'bgrcmykw'[l])","78d6d519":"# Now, for elbow method. We'll try clustering from k=1 to k=15 and see how well the curve is displayed\nx, y = [], []\nfor k in range(1, 16):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(tfidf_matrix)\n    x.append(k)\n    y.append(kmeans.inertia_)\n    \nplt.plot(x, y)","6a5503db":"# That suck. Where's the elbow?\n# Let's see how sihoutte score goes.\n# For those who doesn't know what silhoutte score is,\n# The silhoutte score is a value of measure of closeness\n# to it's own cluster(cohesion) compared to other clusters(seperation)\n# it ranges from [-1, 1]. A high value indicates how well matched is\n# an object is to it's cluster.\n# For more info, wikipedia is your guy.\n\n# We'll calculate silhoutte scores for the same range as before excpet 1\n# Because it'll throw an error if k = 1. Because you know,\n# how well a point is matched to it's cluster and there exists one cluster.\n# Make sense to you. It doesn't to me.","6b36af4e":"for k in range(2, 16):\n    kmeans = KMeans(n_clusters=k).fit(tfidf_matrix)\n    sil_score = silhouette_score(tfidf_matrix, kmeans.labels_)\n    print('Number of clusters: {}, Silhoutte Score: {}'.format(k, sil_score))","3cf65c2c":"# That's not nice, the silhoutte score is still going up.\n# No matters, now this is where you come in visitor. If you find\n# something, please post it in the comment section. It can be \n# suggestions, imrpovements, or errors. ","f5833626":"## Hello visitor,\n\nThe following analysis and clustering is done on a text dataset of news headlines extracted from twitter. I found this dataset on UCI Machine Learning repository and though I'd give a try doing something with it. The following is in accordance with the tutorial i found in kdnuggets though I \"might\" have missed some parts, haha. Now, behold my #### Failed attempt in doing something with this dataset. You may go through this, and you will definitely find mistakes and improvements. Please post them in the comment section if you find any. So me and anyone who visits this can benefit from it. "}}