{"cell_type":{"2942cf00":"code","03b19cb4":"code","98480ba0":"code","303477c4":"code","e45ed28d":"code","20454599":"code","edfdcdf8":"code","2a848e83":"code","fdaa090e":"code","9f0050d2":"code","b836e62d":"code","ee82e94a":"code","9284c9d3":"code","83e29266":"code","c643d01b":"code","82f467b0":"code","9410259a":"code","6384fbb7":"code","4d45792b":"code","18e9094f":"code","35b7d29d":"code","c8408f8e":"code","3f3feed0":"code","314108d8":"code","8f0b1bbe":"code","4b57d436":"code","e28501ee":"code","331594be":"code","55550cd8":"code","86e352e7":"code","1188487b":"code","a6db65df":"code","9ebb3bf4":"code","5edbe818":"code","c5eb8c4e":"code","e20b336e":"code","b07ecde9":"code","4211b508":"code","8467435f":"code","432c8511":"markdown","d13c4606":"markdown","adb8632e":"markdown","83b65a57":"markdown","3e4f5311":"markdown","547603b6":"markdown","79a5ff8a":"markdown","ddf9a3ae":"markdown"},"source":{"2942cf00":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy             as np \nimport pandas            as pd \nimport matplotlib.pyplot as plt\nimport seaborn           as sns\nimport statsmodels.api   as sm\n%matplotlib inline\n\nfrom   sklearn.model_selection   import train_test_split\nfrom   sklearn.linear_model      import LinearRegression\nfrom   sklearn.preprocessing     import MinMaxScaler\nfrom   sklearn.metrics           import confusion_matrix\nfrom   sklearn.metrics           import r2_score,accuracy_score\nfrom   sklearn.feature_selection import RFE\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","03b19cb4":"heart_attack = pd.read_csv(r'..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')\nheart_attack.head()","98480ba0":"#Analyzing the dataset \nheart_attack.info()\n#There are no null values in the dataset & all the datatypes are correctly assigned","303477c4":"#Checking for outliers if any \nfig, axes = plt.subplots(1, 4, figsize=(14,5))\naxes[0].boxplot(heart_attack['age']);\naxes[1].boxplot(heart_attack['trtbps']);\naxes[2].boxplot(heart_attack['chol']);\naxes[3].boxplot(heart_attack['fbs']);","e45ed28d":"#Function to analyse the columns with outliers \ndef outliers(df,x):\n    return df[x].quantile([0.25,0.50,0.75,0.90,0.95,0.96,0.97,0.98,0.99,1.00])","20454599":"#Analysing the suspected columns\nprint(outliers(heart_attack,'trtbps'))\nprint(outliers(heart_attack,'chol'))","edfdcdf8":"#dropping the outliers from chol column\nprint(heart_attack[heart_attack['chol']>400].count())\nheart_attack.drop(heart_attack[(heart_attack['chol']>400)].index,axis=0,inplace=True)","2a848e83":"#dropping the outliers from \nprint(heart_attack[heart_attack['trtbps']>175].count())\nheart_attack.drop(heart_attack[(heart_attack['trtbps']>175)].index,axis=0,inplace=True)","fdaa090e":" #Analysing the suspected columns\nprint(outliers(heart_attack,'fbs'))","9f0050d2":"#dropping the outliers from fbs column\nprint(heart_attack[heart_attack['fbs']==1].count())\nheart_attack.drop(heart_attack[(heart_attack['fbs']==1)].index,axis=0,inplace=True)","b836e62d":"#Checking for outliers if any \nfig, axes = plt.subplots(1, 4, figsize=(14,5))\naxes[0].boxplot(heart_attack['age']);\naxes[1].boxplot(heart_attack['trtbps']);\naxes[2].boxplot(heart_attack['chol']);\naxes[3].boxplot(heart_attack['fbs']);\n\n#We can observe from the figure that all the outliers have been treated","ee82e94a":"#Checking for outliers if any \nfig, axes = plt.subplots(1, 4, figsize=(14,5))\naxes[1].boxplot(heart_attack['thalachh']);\naxes[2].boxplot(heart_attack['oldpeak']);\naxes[3].boxplot(heart_attack['slp']);","9284c9d3":"#dropping the outliers from oldpeak column\nprint(heart_attack[heart_attack['oldpeak']>6].count())\nheart_attack.drop(heart_attack[(heart_attack['oldpeak']>6)].index,axis=0,inplace=True)","83e29266":"sns.boxplot(heart_attack['oldpeak']);","c643d01b":"#Analysing the correlation between the features \nplt.figure(figsize=(20,10))\nsns.heatmap(heart_attack.corr(),annot=True)","82f467b0":"#Checking if the data is balanced or imbalanced \nprint(heart_attack.shape)\nprint(heart_attack['output'].value_counts(normalize=True)*100)","9410259a":"#Splitting the data into train test split \nY = heart_attack['output']\nX = heart_attack.drop('output',axis=1).copy()\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)","6384fbb7":"#converting Y into dataframe \ny_train = y_train.values.reshape(-1,1)\ny_test = y_test.values.reshape(-1,1)","4d45792b":"X_train.head()","18e9094f":"#Preprocessing the data \nscaler = MinMaxScaler()\nscaling_list = ['age','trtbps','chol','thalachh','oldpeak']\nX_train[scaling_list] = scaler.fit_transform(X_train[scaling_list])","35b7d29d":"X_test[scaling_list] = scaler.transform(X_test[scaling_list])","c8408f8e":"#Training a model using top-down approach \nlr_model1 = sm.GLM(y_train,sm.add_constant(X_train),families=sm.families.Binomial())\nlr_model  = lr_model1.fit()\nprint(lr_model.summary())","3f3feed0":"# Running RFE with the output number of the variable equal to 10\nlm = LinearRegression()\nlm.fit(X_train, y_train)\nrfe = RFE(lm, 8)             # running RFE\nrfe = rfe.fit(X_train, y_train)\nlist(zip(X_train.columns,rfe.support_,rfe.ranking_))","314108d8":"#Fetching the list of best 8 columns \ncol = X_train.columns[rfe.support_]\ncol","8f0b1bbe":"#Building a model using the above features \nX_train_rfe = X_train[col].copy()\nlr_model2   = sm.GLM(y_train,sm.add_constant(X_train_rfe),family=sm.families.Binomial())\nlr_model    = lr_model2.fit()\nprint(lr_model.summary())","4b57d436":"#Function to predict using latest model & printing the confusion matrix \ndef testing(model_name,test_set,thres_value,test_op):\n    y_pred_test = model_name.predict(test_set).values.reshape(-1,1)\n    y_train_pred_final = pd.DataFrame({'output':test_op, 'op_Prob':y_pred_test})\n    y_train_pred_final['index'] = test_set.index\n    return y_train_pred_final.head()","e28501ee":"#Creating new_testing df\nX_train_rfe = sm.add_constant(X_train_rfe)","331594be":"#Function to create a table with pred values for logistic regression \ndef prediction(model_name,x_test,y_test):\n    y_pred                        = model_name.predict(x_test)\n    y_pred_final                  = pd.DataFrame({'op_train_Prob':y_pred})\n    y_pred_final['train_op']      = y_test\n    y_pred_final['op_train_pred'] = y_pred_final['op_train_Prob'].apply(lambda x:1 if x>0.5 else 0)\n    return y_pred_final","55550cd8":"prediction(lr_model,X_train_rfe,y_train)","86e352e7":"#function to test the logistic Regression model \ndef validating_lr(y_real,y_pred):\n    from sklearn.metrics import confusion_matrix, accuracy_score\n    print('Confusion Matrix')\n    confusion = confusion_matrix(y_pred,y_real)\n    print(confusion)\n    print('Accuracy Score')\n    print(accuracy_score(y_pred,y_real)*100)\n    TP = confusion[1,1] # true positive \n    TN = confusion[0,0] # true negatives\n    FP = confusion[0,1] # false positives\n    FN = confusion[1,0] # false negatives\n    print('Sensitivity')\n    print((TP \/ float(TP+FN))*100)\n    print('specificity')\n    print((TN \/ float(TN+FP))*100)\n    print('false postive rate - predicting 1 when its 0')\n    print((FP\/ float(TN+FP))*100)\n    print('Positive predictive value')\n    print((TP \/ float(TP+FP))*100)\n    print('Negative predictive value')\n    print((TN \/ float(TN+ FN))*100)","1188487b":"validating_lr(y_pred_final['train_op'],y_pred_final['op_train_pred'])","a6db65df":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_pred_final[i]= y_pred_final['op_train_pred'].map(lambda x: 1 if x > i else 0)\npd.set_option('display.max_rows',None)\ny_pred_final","9ebb3bf4":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = confusion_matrix(y_pred_final[i],y_pred_final['train_op'])\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","5edbe818":"cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","c5eb8c4e":"X_test_rfe = X_test[col]\nX_test_rfe = sm.add_constant(X_test_rfe)","e20b336e":"prediction(lr_model,X_test_rfe,y_test)","b07ecde9":"#Checking the accuracy of the model\nvalidating_lr(y_pred_final['train_op'],y_pred_final['op_train_pred'])","4211b508":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = confusion_matrix(y_pred_final[i],y_pred_final['train_op'])\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","8467435f":"#This is my final model as it is giving ","432c8511":"**We can observe that all the outliers have been eliminated & all our data is clean & ready to use**","d13c4606":"**4. Model Building **","adb8632e":"#The most correlated values to the Target values are \n1. slp\n2. thalachh\n3. restecg\n4. sex\n5. cp\n6. exng","83b65a57":"****2. Data cleaning ****","3e4f5311":"**Description of columns in the DATASET**\n* Age : Age of the patient\n* Sex : Sex of the patient\n* exang: exercise induced angina (1 = yes; 0 = no)\n* ca: number of major vessels (0-3)\n* cp : Chest Pain type chest pain type\n*     Value 1: typical angina\n*     Value 2: atypical angina\n*     Value 3: non-anginal pain\n*     Value 4: asymptomatic\n* trtbps : resting blood pressure (in mm Hg)\n* chol : cholestoral in mg\/dl fetched via BMI sensor\n* fbs : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n* rest_ecg : resting electrocardiographic results\n*     Value 0: normal\n*     Value 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n*     Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n* thalach : maximum heart rate achieved\n* target : 0= less chance of heart attack 1= more chance of heart attack","547603b6":"1. **Importing Data & Analyzing the Data ****","79a5ff8a":"#As we can see there is no variation in the accuracy even if we change the threshold \n#that means our model is stable & gives 85% accuracy on train data now lets check with test data ","ddf9a3ae":"3. Exploratory Data analysis"}}