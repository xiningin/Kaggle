{"cell_type":{"39b7321d":"code","0ecfdc22":"code","ee670a40":"code","f59c86dc":"code","fe40681f":"code","5f8cfb4b":"code","7d6ee0f5":"code","9e8c8623":"code","ada42b02":"code","7c177361":"code","3de5f693":"code","3e8da0b4":"code","1883a71f":"code","e74c862e":"code","c0392138":"code","096fd6a7":"code","8706b176":"code","cf8b9707":"code","f8860e6a":"code","373f0860":"code","576a97cb":"code","09f86e06":"code","ac79ac24":"code","002d5024":"markdown","aebf5002":"markdown","fdfa6092":"markdown","01a42022":"markdown","5aeaa95d":"markdown","b2697683":"markdown","b0b99444":"markdown","5f81ab47":"markdown","3fa87298":"markdown","bddc4b5d":"markdown","3158dbf1":"markdown","abdcc786":"markdown","82c59a0b":"markdown","dd78ea61":"markdown","553bda76":"markdown","6a128b1a":"markdown","451a459b":"markdown","30331cbe":"markdown"},"source":{"39b7321d":"import sys\nimport warnings\n\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport random\nimport seaborn as sns\n\nfrom mlxtend.regressor import StackingCVRegressor\nfrom scipy.stats import skew\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import ElasticNetCV\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\n\nfrom keras.layers import Activation\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\n\n\nimport optuna\n\nprint(\"Imports have been set\")\nrandom.seed(42)\n\n# Disabling warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")","0ecfdc22":"# Reading the training\/val data and the test data\nX = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv', index_col='Id')\nX_test = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv', index_col='Id')\n\n# Rows before:\nrows_before = X.shape[0]\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\nrows_after = X.shape[0]\nprint(\"Rows containing NaN in SalePrice were dropped: \" + str(rows_before - rows_after))\n\n# Let's look at the target variable distribution\n# sns.distplot(a=X['SalePrice'], label=\"Target variable distribution\", kde=False)\n# sns.barplot(x=X.index, y=X['SalePrice'])\n# plt.show()\n# print(\"Well, looks like it's shuffled properly\")","ee670a40":"# Logarithming target variable in order to make distribution better\nX['SalePrice'] = np.log1p(X['SalePrice'])\ny = X['SalePrice'].reset_index(drop=True)\ntrain_features = X.drop(['SalePrice'], axis=1)\n\n# Let's see what happens after\nsns.distplot(a=y, label=\"Target variable distribution\", kde=False)\nplt.show()","f59c86dc":"# concatenate the train and the test set as features for tranformation to avoid mismatch\nfeatures = pd.concat([train_features, X_test]).reset_index(drop=True)\nprint('Features size:', features.shape)","fe40681f":"nan_count_table = (features.isnull().sum())\nnan_count_table = nan_count_table[nan_count_table > 0].sort_values(ascending=False)\nprint(\"\\nColums containig NaN: \")\nprint(nan_count_table)\n\ncolumns_containig_nan = nan_count_table.index.to_list()\nprint(\"\\nWhat values they contain: \")\nprint(features[columns_containig_nan])","5f8cfb4b":"for column in columns_containig_nan:\n\n    # populating with 0\n    if column in ['GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n                  'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'TotalBsmtSF',\n                  'Fireplaces', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold',\n                  'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea']:\n        features[column] = features[column].fillna(0)\n\n    # populate with 'None'\n    if column in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', \"PoolQC\", 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n                  'BsmtFinType2', 'Neighborhood', 'BldgType', 'HouseStyle', 'MasVnrType', 'FireplaceQu', 'Fence', 'MiscFeature']:\n        features[column] = features[column].fillna('None')\n\n    # populate with most frequent value for cateforic\n    if column in ['Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'RoofStyle',\n                  'Electrical', 'Functional', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType', 'RoofMatl', 'ExterQual', 'ExterCond',\n                  'Foundation', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'PavedDrive', 'SaleType', 'SaleCondition']:\n        features[column] = features[column].fillna(features[column].mode()[0])\n\n# MSSubClass: Numeric feature. Identifies the type of dwelling involved in the sale.\n#     20  1-STORY 1946 & NEWER ALL STYLES\n#     30  1-STORY 1945 & OLDER\n#     40  1-STORY W\/FINISHED ATTIC ALL AGES\n#     45  1-1\/2 STORY - UNFINISHED ALL AGES\n#     50  1-1\/2 STORY FINISHED ALL AGES\n#     60  2-STORY 1946 & NEWER\n#     70  2-STORY 1945 & OLDER\n#     75  2-1\/2 STORY ALL AGES\n#     80  SPLIT OR MULTI-LEVEL\n#     85  SPLIT FOYER\n#     90  DUPLEX - ALL STYLES AND AGES\n#    120  1-STORY PUD (Planned Unit Development) - 1946 & NEWER\n#    150  1-1\/2 STORY PUD - ALL AGES\n#    160  2-STORY PUD - 1946 & NEWER\n#    180  PUD - MULTILEVEL - INCL SPLIT LEV\/FOYER\n#    190  2 FAMILY CONVERSION - ALL STYLES AND AGES\n\n# Stored as number so converted to string.\nfeatures['MSSubClass'] = features['MSSubClass'].apply(str)\nfeatures[\"MSSubClass\"] = features[\"MSSubClass\"].fillna(\"Unknown\")\n# MSZoning: Identifies the general zoning classification of the sale.\n#    A    Agriculture\n#    C    Commercial\n#    FV   Floating Village Residential\n#    I    Industrial\n#    RH   Residential High Density\n#    RL   Residential Low Density\n#    RP   Residential Low Density Park\n#    RM   Residential Medium Density\n\n# 'RL' is by far the most common value. So we can fill in missing values with 'RL'\nfeatures['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n# LotFrontage: Linear feet of street connected to property\n# Groupped by neighborhood and filled in missing value by the median LotFrontage of all the neighborhood\n# TODO may be 0 would perform better than median?\nfeatures['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n# LotArea: Lot size in square feet.\n# Stored as string so converted to int.\nfeatures['LotArea'] = features['LotArea'].astype(np.int64)\n# Alley: Type of alley access to property\n#    Grvl Gravel\n#    Pave Paved\n#    NA   No alley access\n\n# So. If 'Street' made of 'Pave', so it would be reasonable to assume that 'Alley' might be 'Pave' as well.\nfeatures['Alley'] = features['Alley'].fillna('Pave')\n# MasVnrArea: Masonry veneer area in square feet\n# Stored as string so converted to int.\nfeatures['MasVnrArea'] = features['MasVnrArea'].astype(np.int64)","7d6ee0f5":"features['YrBltAndRemod'] = features['YearBuilt'] + features['YearRemodAdd']\nfeatures['TotalSF'] = features['TotalBsmtSF'] + features['1stFlrSF'] + features['2ndFlrSF']\n\nfeatures['Total_sqr_footage'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +\n                                 features['1stFlrSF'] + features['2ndFlrSF'])\n\nfeatures['Total_Bathrooms'] = (features['FullBath'] + (0.5 * features['HalfBath']) +\n                               features['BsmtFullBath'] + (0.5 * features['BsmtHalfBath']))\n\nfeatures['Total_porch_sf'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n                              features['EnclosedPorch'] + features['ScreenPorch'] +\n                              features['WoodDeckSF'])\n\n# If area is not 0 so creating new feature looks reasonable\nfeatures['haspool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['has2ndfloor'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasgarage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasbsmt'] = features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['hasfireplace'] = features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\nprint('Features size:', features.shape)","9e8c8623":"nan_count_train_table = (features.isnull().sum())\nnan_count_train_table = nan_count_train_table[nan_count_train_table > 0].sort_values(ascending=False)\nprint(\"\\nAre no NaN here now: \" + str(nan_count_train_table.size == 0))","ada42b02":"numeric_columns = [cname for cname in features.columns if features[cname].dtype in ['int64', 'float64']]\nprint(\"\\nColumns which are numeric: \" + str(len(numeric_columns)) + \" out of \" + str(features.shape[1]))\nprint(numeric_columns)\n\ncategoric_columns = [cname for cname in features.columns if features[cname].dtype == \"object\"]\nprint(\"\\nColumns whice are categoric: \" + str(len(categoric_columns)) + \" out of \" + str(features.shape[1]))\nprint(categoric_columns)\n\nskewness = features[numeric_columns].apply(lambda x: skew(x))\nprint(skewness.sort_values(ascending=False))\n\nskewness = skewness[abs(skewness) > 0.5]\nfeatures[skewness.index] = np.log1p(features[skewness.index])\nprint(\"\\nSkewed values: \" + str(skewness.index))","7c177361":"# Kind of One-Hot encoding\nfinal_features = pd.get_dummies(features).reset_index(drop=True)\n\n# Spliting the data back to train(X,y) and test(X_sub)\nX = final_features.iloc[:len(y), :]\nX_test = final_features.iloc[len(X):, :]\n\n# Spltting X and y to train and validation sets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=42)\n\nprint(\"Shape of X_train: \" + str(X_train.shape) + \", shape of y_train: \" + str(y_train.shape))\nprint(\"Shape of X_valid: \" + str(X_valid.shape) + \", shape of y_valid: \" + str(y_valid.shape))","3de5f693":"e_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n\n# check maybe 10 kfolds would be better\nkfolds = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# Kernel Ridge Regression : made robust to outliers\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))\n\n# LASSO Regression : made robust to outliers\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas=alphas2, random_state=14, cv=kfolds))\n\n# Elastic Net Regression : made robust to outliers\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))\n\n# Gradient Boosting for regression\ngboost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10,\n                                   loss='huber', random_state=5)\n\n# LightGBM regressor\ndtrain = lgb.Dataset(X_train, label=y_train)\nlgbm_params = {\n    'objective': 'regression',\n    'metric': 'mean_absolute_error',\n    'lambda_l1': 0.009917563046305308,\n    'lambda_l2': 0.0005854111105267089,\n    'num_leaves': 179,\n    'learning_rate': 0.08994549293068982,\n    'n_estimators': 1780,\n    'feature_fraction': 0.6669586810450638,\n    'bagging_fraction': 0.6225238656510562,\n    'bagging_freq': 4,\n    'min_child_samples': 5}\n\n\n# optimal parameters, received from CV\nc_grid = {\"n_estimators\": [1000],\n          \"early_stopping_rounds\": [1],\n          \"learning_rate\": [0.1]}\nxgb_regressor = XGBRegressor(objective='reg:squarederror', eval_metric='mae')\nxgb_r = GridSearchCV(estimator=xgb_regressor,\n                     param_grid=c_grid,\n                     cv=kfolds)\n\n# stack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, lgbm, gboost),\n#                                 meta_regressor=elasticnet,\n#                                 use_features_in_secondary=True)\n\nsvr = make_pipeline(RobustScaler(), SVR(C=20, epsilon=0.008, gamma=0.0003))","3e8da0b4":"def mae(y_actual, y_pred):\n    return mean_absolute_error(np.expm1(y_actual), np.expm1(y_pred))","1883a71f":"# sampler = TPESampler(seed=10) # for reproducibility\n# def objective(trial):\n#     dtrain = lgb.Dataset(X_train, label=y_train)\n    \n#     param = {\n#         'objective': 'regression',\n#         'metric': 'mean_absolute_error',\n#         'verbosity': -1,\n#         'boosting_type': 'gbdt',\n#         'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n#         'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n#         'num_leaves': trial.suggest_int('num_leaves', 2, 512),\n#         'learning_rate': trial.suggest_loguniform('learning_rate', 1e-8, 1.0),\n#         'n_estimators': trial.suggest_int('n_estimators', 700, 3000),\n#         'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n#         'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n#         'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n#         'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n#     }\n\n#     gbm = lgb.train(param, dtrain)\n#     return mean_absolute_error(np.expm1(y_valid), np.expm1(gbm.predict(X_valid)))\n\n# study = optuna.create_study(direction='minimize', sampler=sampler)\n# study.optimize(objective, n_trials=100)","e74c862e":"# THIS VERSION SUCKS\nimport lightgbm as lgbm\nfrom optuna.samplers import TPESampler\n\nsampler = TPESampler(seed=10) # for reproducibility\ndef objective(trial):\n    \n    # dtrain = lgb.Dataset(X_train, label=y_train)\n    \n    param = {\n        'objective': 'regression',\n        'metric': 'mean_absolute_error',\n        'verbosity': -1,\n        'boosting_type': 'gbdt',\n        # 'early_stopping_rounds': trial.suggest_int('early_stopping_rounds', 10, 2000),\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 512),\n        # 'learning_rate': trial.suggest_loguniform('learning_rate', 1e-8, 1.0),\n        'learning_rate': 0.01,\n        'n_estimators': trial.suggest_int('n_estimators', 700, 3000),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n    }\n    lgbm_regr = lgbm.LGBMRegressor(**param)\n    gbm_2 = lgbm_regr.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n    return mean_absolute_error(np.expm1(y_valid), np.expm1(gbm_2.predict(X_valid)))\n\nstudy = optuna.create_study(direction='minimize', sampler=sampler)\nstudy.optimize(objective, n_trials=100)","c0392138":"is_NN_on = False\n\nif is_NN_on:\n    model_nn = Sequential()\n\n    model_nn.add(Dense(1028, input_dim=X_train.shape[1], init='he_normal'))\n    model_nn.add(BatchNormalization())\n    model_nn.add(Activation('relu'))\n    model_nn.add(Dropout(0.2))\n\n    model_nn.add(Dense(128, init='he_normal'))\n    model_nn.add(BatchNormalization())\n    model_nn.add(Activation('relu'))\n    model_nn.add(Dropout(0.2))\n\n    model_nn.add(Dense(1, init='he_normal'))\n    #model_nn.add(BatchNormalization())\n    #model_nn.add(Activation('sigmoid'))\n\n    opt = Adam(lr=0.00001)\n    model_nn.compile(loss='mean_absolute_error', optimizer=opt, metrics=['mae'])\n\n    print('Neural Network is fitting now...')\n\n    nn_history = model_nn.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=10, epochs=600)\n\n    val_acc = nn_history.history['val_mae']\n    acc = nn_history.history['mae']\n\n    epochs = range(1, len(val_acc) + 1)\n\n    plt.figure(figsize=(15,10))\n    plt.ylim(0, 1.0)\n    plt.plot(epochs, val_acc, label='Validation mae')\n    plt.plot(epochs, acc, label='Train mae')\n    plt.xlabel('Epochs')\n    plt.ylabel('MAE')\n    plt.legend()\n\n    plt.show()","096fd6a7":"# print('lgbm is fitting now...')\n# lgbm = lgb.train(lgbm_params, dtrain)","8706b176":"print('lgbm is fitting now...')\nimport lightgbm as lgbm\nparam = {'lambda_l1': 0.001040845213184379, 'lambda_l2': 1.0320042952640715e-08, 'num_leaves': 450, 'n_estimators': 992, 'feature_fraction': 0.5307283632444058, 'bagging_fraction': 0.48228480181276895, 'bagging_freq': 3, 'min_child_samples': 5}\nlgbm_regr = lgbm.LGBMRegressor(**param)\nlgbm = lgbm_regr.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], eval_metric= 'mean_absolute_error', verbose=False)","cf8b9707":"MODELS = {\n    # 'stack_gen': (stack_gen, 0.20),\n    'svr': (svr, 0.21),\n    'elastic': (elasticnet, 0.20),\n    'gboost': (gboost, 0.20),\n    'lgbm': (lgbm, 0.20),\n    'lasso': (lasso, 0.19),\n    #'ridge': (ridge, 0.16638505996865854),\n    # 'xgb_r': (xgb_r, 0.10)\n}","f8860e6a":"excluding_list = [lgbm]\n\nprint('Fitting our models ensemble: ')\nfor modelname, model in MODELS.items():\n    if model[0] not in excluding_list:\n        print(str(modelname), 'is fitting now...')\n        model[0].fit(X_train, y_train)","373f0860":"print('Models evaluating: ')\nscores = {}\nfor modelname, model in MODELS.items():\n    score = mae(y_valid, model[0].predict(X_valid))\n    print(modelname, \"score: {:.4f}\".format(score))\n    scores[modelname] = 1\/score","576a97cb":"print('Optimal coefficients based on score: \\n')\nscores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1])}\n\nvals = np.fromiter(scores.values(), dtype=float)\n\nfor key in scores:\n    print(key, ': ', scores[key] \/ sum(vals))","09f86e06":"#  Last successful: 0.11663\ndef blend_models(models, x):\n    output = np.zeros(x.shape[0])\n    for blend_modelname, blend_model in models.items():\n        output = np.add(output, blend_model[1] * blend_model[0].predict(x))\n    return output\n\nprint('MAE score on validation data:')\nprint('Ensemble: ', mae(y_valid, blend_models(MODELS, X_valid)))\n\nif is_NN_on:\n    print('Neural network: ', mae(y_valid, pd.Series(model_nn.predict(X_valid).reshape(1, X_valid.shape[0])[0]).values))","ac79ac24":"submission = pd.read_csv('..\/input\/home-data-for-ml-course\/sample_submission.csv')\nsubmission.iloc[:, 1] = np.expm1(blend_models(MODELS, X_test))\nsubmission.to_csv(\"submission.csv\", index=False)\n\nprint(\"Submission file is formed\")","002d5024":"<h1>\n    Categoric features encoding and splitting to train and test data\n<\/h1>\n<span> Briefly:\n    <ul>\n        <li> I used pd.get_dummies(features) which returns kind of One-Hot encoded categoric features<\/li>\n        <li> Splitted to X and X_test by y length<\/li>\n    <\/ul>\n<\/span>","aebf5002":"<h1>\n    Input data handling\n<\/h1>\n<span> Briefly:\n    <ul>\n        <li> removing rows with NaN in Sale Price <\/li>\n        <li> logarithm SalePrice (and when model predicts I use np.expm1 function to return value)  <\/li>\n        <li> splitting X to target variable y and train_features <\/li>\n        <li> joining X_test and train_features to process all features together <\/li>\n    <\/ul>\n<\/span>","fdfa6092":"<h1>\n    Feature engineering\n<\/h1>\n<span> Briefly:\n    <ul>\n        <li> Filling with 0 numeric columns <\/li>\n        <li> Filling with 'None' categoric columns where 'NA' meant 'other' value<\/li>\n        <li> Filling with the most frequent values categoric columns where 'NA' meant 'nothing is here'<\/li>\n        <li> Turning to 'str' columns which are actually categoric <\/li>\n        <li> Turning to 'int' columns which are actually numeric <\/li>\n    <\/ul>\n<\/span>","01a42022":"<h1>\n    ML part (models ensembling)\n<\/h1>\n<span> Briefly:\n    <ul>\n        <li> The weighted sum of models on the basis of which the solution is assembled<\/li>\n        <li> There is score in comment to each row which explains coefficient to model<\/li>\n    <\/ul>\n<\/span>","5aeaa95d":"<h1>\n    Models fitting\n<\/h1>","b2697683":"<h1>\n    ML part (models fitting)\n<\/h1>\n<span> Briefly:\n    <ul>\n        <li> One-by-one all models fitting<\/li>\n        <li> Printing models scores (might be commented for quicker work) <\/li>\n    <\/ul>\n<\/span>","b0b99444":"<h1>\n    Checking for NaNs and printing them\n<\/h1>\n<span> Briefly:\n    <ul>\n        <li> printing NaN-containing columns names <\/li>\n        <li> printing NaN-containing columns values for clarity<\/li>\n    <\/ul>\n<\/span>","5f81ab47":"<h1>\n    Introducing MAE metrics:\n<\/h1>","3fa87298":"<h1>\n    Let's check if we filled all the gaps\n<\/h1>\n<span> Briefly:\n    <ul>\n        <li> Just printing True or False if all the gaps are filled <\/li>\n    <\/ul>\n<\/span>","bddc4b5d":"<h1>\n    Adding new features\n<\/h1>\n<span> Briefly:\n    <ul>\n        <li> YrBltAndRemod means overall sum of years <\/li>\n        <li> Separating to the other features overall squares<\/li>\n        <li> Separating to the other features presence\/absence of a garage and so on<\/li>\n    <\/ul>\n<\/span>","3158dbf1":"<h1>TODO-list<\/h1>\n<ul>\n    <li>Add more new features<\/li>\n    <li>Add extra feature indicating something was absent<\/li>\n    <li>Remove outliers<\/li>\n<\/ul>","abdcc786":"<h1>\n    Fixing skewed values\n<\/h1>\n<span> Briefly:\n    <ul>\n        <li> Checking skewness of all the numeric features and logarithm it if more than 0.5 <\/li>\n    <\/ul>\n<\/span>","82c59a0b":"<h1>\n    Models evaluating\n<\/h1>","dd78ea61":"<h1>\n    Models tuning with Optuna\n<\/h1>","553bda76":"<h1>\n    Imports\n<\/h1>","6a128b1a":"<h1>\n    Printing optimal ensemble coefficients\n<\/h1>","451a459b":"Current best value is 14793.129634228912 with parameters:\n{'lambda_l1': 2.105459136785425e-06, 'lambda_l2': 1.5056384903072508e-05, 'num_leaves': 161, 'learning_rate': 0.018421617801600454, 'n_estimators': 2918, 'feature_fraction': 0.47097269927768537, 'bagging_fraction': 0.8176017124007523, 'bagging_freq': 4, 'min_child_samples': 11}.\n\nCurrent best value is 14564.808632104898 with parameters: {'lambda_l1': 0.001040845213184379, 'lambda_l2': 1.0320042952640715e-08, 'num_leaves': 450, 'n_estimators': 992, 'feature_fraction': 0.5307283632444058, 'bagging_fraction': 0.48228480181276895, 'bagging_freq': 3, 'min_child_samples': 5}.","30331cbe":"<h1>\n    ML part (models initialization)\n<\/h1>\n<span> Briefly:\n    <ul>\n        <li> I used pd.get_dummies(features) as encoder which returns kind of One-Hot encoded categoric features<\/li>\n        <li> Splitted to X and X_test by y length<\/li>\n    <\/ul>\n<\/span>"}}