{"cell_type":{"c8d7676e":"code","47e8b8f7":"code","1a08eaf0":"code","78c755ae":"code","c70ae774":"code","5370b1a1":"code","ddb433ae":"code","9e32afb2":"code","69a46c6c":"code","ab77bc06":"code","85f08858":"code","18e29314":"code","b4e467fc":"code","573d6380":"code","caa01194":"code","f884b299":"code","4d274c67":"code","be9ea523":"markdown","153db3dd":"markdown","0922b188":"markdown","44801258":"markdown","afc4c30c":"markdown","a0bd5881":"markdown","a25d7d2d":"markdown","0873cced":"markdown","d9a7e2e4":"markdown"},"source":{"c8d7676e":"# Import the necessary libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport time\nimport warnings\nimport os\nfrom six.moves import urllib\nimport matplotlib\nimport matplotlib.pyplot as plt\nwarnings.filterwarnings('ignore')","47e8b8f7":"#Add All the Models Libraries\n\n# Scalers\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import shuffle\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\n\n# Models\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn.svm import SVC # Support Vector Classifier\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.ensemble import ExtraTreesClassifier \nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.ensemble import BaggingClassifier\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\nfrom scipy.stats import reciprocal, uniform\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\n\n# Cross-validation\nfrom sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nfrom sklearn.model_selection import cross_validate\n\n# GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n#Common data processors\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import check_array\nfrom scipy import sparse\n\n#Accuracy Score\nfrom sklearn.metrics import accuracy_score\n\nfrom tqdm import tqdm_notebook as tqdm","1a08eaf0":"# to make this notebook's output stable across runs\nnp.random.seed(123)\n\n# To plot pretty figures\n%matplotlib inline\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12","78c755ae":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","c70ae774":"TrainFile = reduce_mem_usage(pd.read_csv(\"..\/input\/3253-34\/train.csv\")) #read the data from the csv file.\nTestFile = reduce_mem_usage(pd.read_csv(\"..\/input\/3253-34\/test.csv\"))","5370b1a1":"print(TrainFile.shape)\nprint(TestFile.shape)","ddb433ae":"features = [c for c in TrainFile.columns if c not in ['ID_code', 'target']]\ny = TrainFile['target']\nX = TrainFile[features]\nX_test = TestFile[features]","9e32afb2":"import seaborn as sns\nsns.set_style('whitegrid')\nsns.countplot(y)","69a46c6c":"import scipy.ndimage\n\nsigma_fac = 0.001\nsigma_base = 4\n\neps = 0.00000001\n\ndef get_count(X):\n    features_count = np.zeros((X.shape[0], len(features)))\n    features_density = np.zeros((X.shape[0], len(features)))\n    features_deviation = np.zeros((X.shape[0], len(features)))  \n    sigmas = []\n\n    for i,var in enumerate(tqdm(features)):\n        X_var_int = (X[var].values * 10000).round().astype(int)\n        lo = X_var_int.min()\n        X_var_int -= lo\n        hi = X_var_int.max()+1\n        counts_all = np.bincount(X_var_int, minlength=hi).astype(float)\n        zeros = (counts_all == 0).astype(int)\n        before_zeros = np.concatenate([zeros[1:],[0]])\n        indices_all = np.arange(counts_all.shape[0])\n        \n        # Geometric mean of twice sigma_base and a sigma_scaled which is scaled to the length of array \n        sigma_scaled = counts_all.shape[0]*sigma_fac\n        sigma = np.power(sigma_base * sigma_base * sigma_scaled, 1\/3)\n        sigmas.append(sigma)\n        counts_all_smooth = scipy.ndimage.filters.gaussian_filter1d(counts_all, sigma)\n        deviation = counts_all \/ (counts_all_smooth+eps)\n        indices = X_var_int\n        features_count[:,i] = counts_all[indices]\n        features_density[:,i] = counts_all_smooth[indices]\n        features_deviation[:,i] = deviation[indices]\n        \n    features_count_names = [var+'_count' for var in features]\n    features_density_names = [var+'_density' for var in features]\n    features_deviation_names = [var+'_deviation' for var in features]\n\n    X_count = pd.DataFrame(columns=features_count_names, data = features_count)\n    X_count.index = X.index\n    X_density = pd.DataFrame(columns=features_density_names, data = features_density)\n    X_density.index = X.index\n    X_deviation = pd.DataFrame(columns=features_deviation_names, data = features_deviation)\n    X_deviation.index = X.index\n    X_transform = pd.concat([X,X_count, X_density, X_deviation], axis=1)\n    \n    features_count = features_count_names\n    features_density = features_density_names\n    features_deviation = features_deviation_names\n    return X_transform, features_count, features_density, features_deviation\n\nX_transform, features_count, features_density, features_deviation = get_count(X)\nX_test_transform, test_feature_count, test_feature_density, test_feature_deviation = get_count(X_test)\nprint(X_transform.shape)\nprint(X_test_transform.shape)","ab77bc06":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per row in the train and test set\")\nsns.distplot(X_transform[features].mean(axis=1),color=\"black\", kde=True,bins=120, label='train')\nsns.distplot(X_test_transform[features].mean(axis=1),color=\"red\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","85f08858":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of std values per rows in the train and test set\")\nsns.distplot(X_transform[features].std(axis=1),color=\"blue\",kde=True,bins=120, label='train')\nsns.distplot(X_test_transform[features].std(axis=1),color=\"green\", kde=True,bins=120, label='test')\nplt.legend(); plt.show()","18e29314":"#Create numerical pipeline to transform numerical values\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer \nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder,StandardScaler \n\n#Convert the numerical to standard scaler.\nColumns_train_X = list(X_transform)\n\npipeline_numerical = Pipeline([\n  ('scaler', StandardScaler()),\n])\n\npipeline_full = ColumnTransformer([\n  (\"numerical\", pipeline_numerical, Columns_train_X)\n])","b4e467fc":"pipeline_full.fit(X_transform)\nX_train_transformed = pipeline_full.transform(X_transform)\nX_test_transformed = pipeline_full.transform(X_test_transform)\n\nprint(f\"X_train_transformed.shape: {X_train_transformed.shape}\")\nprint(f\"X_test_transformed.shape: {X_test_transformed.shape}\")","573d6380":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n       \nPCA_train = PCA(2).fit_transform(X_train_transformed)\nplt.scatter(PCA_train[:, 0], PCA_train[:, 1], c=y, cmap=\"copper_r\")\nplt.axis('off')\nplt.colorbar()\nplt.show()","caa01194":"from sklearn.decomposition import KernelPCA\n\nlin_pca = KernelPCA(n_components = 2, kernel=\"linear\", fit_inverse_transform=True)\nrbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433, fit_inverse_transform=True)\nsig_pca = KernelPCA(n_components = 2, kernel=\"sigmoid\", gamma=0.001, coef0=1, fit_inverse_transform=True)\n\n\nplt.figure(figsize=(11, 4))\nfor subplot, pca, title in ((131, lin_pca, \"Linear kernel\"), (132, rbf_pca, \"RBF kernel, $\\gamma=0.04$\"), \n                            (133, sig_pca, \"Sigmoid kernel, $\\gamma=10^{-3}, r=1$\")):\n       \n    PCA_train = PCA(2).fit_transform(X_train_transformed)\n    plt.subplot(subplot)\n    plt.title(title, fontsize=14)\n    plt.scatter(PCA_train[:, 0], PCA_train[:, 1], c=y, cmap=\"nipy_spectral_r\")\n    plt.xlabel(\"$z_1$\", fontsize=18)\n    if subplot == 131:\n        plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n    plt.grid(True)\n\nplt.show()","f884b299":"def augment(x,y,t=2):\n    xs,xn = [],[]\n    for i in range(t):\n        mask = y>0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xs.append(x1)\n\n    for i in range(t\/\/2):\n        mask = y==0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xn.append(x1)\n\n    xs = np.vstack(xs)\n    xn = np.vstack(xn)\n    ys = np.ones(xs.shape[0])\n    yn = np.zeros(xn.shape[0])\n    x = np.vstack([x,xs,xn])\n    y = np.concatenate([y,ys,yn])\n    return x,y","4d274c67":"import pickle\n\nwith open('X_train_transaction.pkl','wb') as f:\n    pickle.dump(X_train_transformed, f)\n    \nwith open('X_test_transaction.pkl','wb') as f:\n    pickle.dump(X_test_transformed, f)\n    \nwith open('y_train_transaction.pkl','wb') as f:\n    pickle.dump(y, f)","be9ea523":"### Visualization : Mean Distribution","153db3dd":"### Kernel PCA","0922b188":"### Feature Engineering","44801258":"### Is this a curse of dimentionality?","afc4c30c":"### Visualization : Class Imbalance","a0bd5881":"### Visualization : Standard Distribution","a25d7d2d":"With new features - Good luck with the competition","0873cced":"### Data Augmentation Function","d9a7e2e4":"### Creating Pipeline"}}