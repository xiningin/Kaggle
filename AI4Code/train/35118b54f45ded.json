{"cell_type":{"daf3e577":"code","46938caf":"code","2b5727db":"code","a620185b":"code","dc09b766":"code","8c5704a9":"code","512f5089":"code","1b7510a5":"code","a9e743f6":"code","96ba38b2":"code","043b6c62":"code","b6957807":"code","5dd771bb":"code","9d9d2e4d":"code","90ce695b":"code","52525e7c":"code","9096f940":"code","18905242":"code","07b515f4":"code","b69ee780":"markdown","c6d7155f":"markdown","d510234e":"markdown","d7773651":"markdown","58a91b92":"markdown","ba95a760":"markdown","0b67c368":"markdown","6db7711f":"markdown"},"source":{"daf3e577":"import os\nimport numpy as np\nimport pandas as pd","46938caf":"train = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/test.csv\")\n\ntrain.drop([170514], axis=0, inplace=True)\n\nX = np.array(train.drop([\"id\", \"target\"], axis=1))\nX_test = np.array(test.drop(\"id\", axis=1))\ny = np.array(train[\"target\"])","2b5727db":"import random\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error as mse\n\ndef RMSE(pred, true):\n    return np.sqrt(mse(pred, true))\n\nSEED = 2039","a620185b":"# \u3053\u306e\u30bb\u30eb\u3067\u30b9\u30bf\u30c3\u30ad\u30f3\u30b0\u306b\u4f7f\u3046\u95a2\u6570\u3092\u5b9a\u7fa9\u3057\u3066\u3044\u308b\u3002\n# Define the function to be used for stacking in this cell.\n\n# stack_xgb, stack_lgb\u306f\u6b21\u306e\u5c64\u306b\u5165\u529b\u3059\u308b\u5b66\u7fd2\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u8fd4\u3059\u95a2\u6570\u3002\n# stack_xgb and stack_lgb are functions that return the training and test data to be input to the next layer.\ndef stack_xgb(X, X_test, y, params):\n    \n    SEED = random.randint(0, 100)\n    kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n    \n    # \u6b21\u306e\u5c64\u306e\u5b66\u7fd2\u30c7\u30fc\u30bf\u3068\u306a\u308b\u3082\u306e\u3092\u683c\u7d0d\u3059\u308b\u30ea\u30b9\u30c8\n    # A list to store what will be the training data for the next layer\n    next_train = []\n    \n    # \u6b21\u306e\u5c64\u306e\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3068\u306a\u308b\u3082\u306e\u3092\u683c\u7d0d\u3059\u308b\u30ea\u30b9\u30c8\n    # A list to store what will be the test data for the next layer\n    next_test = []\n    \n    # validation\u30c7\u30fc\u30bf\u306b\u6307\u5b9a\u3055\u308c\u305f\u30c7\u30fc\u30bf\u306e\u884c\u756a\u53f7\u3092\u683c\u7d0d\u3059\u308b\u30ea\u30b9\u30c8\n    # A list that stores the indices of the data specified in the validation data.\n    vl_ids = []\n    \n    d_test = xgb.DMatrix(X_test)\n    \n    for tr_id, vl_id in kf.split(X, y):\n        \n        X_train, X_val = X[tr_id, :], X[vl_id, :]\n        y_train, y_val = y[tr_id], y[vl_id]\n        \n        d_train = xgb.DMatrix(X_train, label=y_train)\n        d_val = xgb.DMatrix(X_val, label=y_val)\n\n        model = xgb.train(params=params,\n                          dtrain=d_train,\n                          num_boost_round=10000,\n                          early_stopping_rounds=100,\n                          verbose_eval=0,\n                          evals=[(d_val, \"val\")])\n        \n        # \u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u4e88\u6e2c\u3092\u884c\u3046\u3002\u3053\u308c\u304c\u6b21\u306e\u5c64\u306e\u5165\u529b\u30c7\u30fc\u30bf\uff08\u6b21\u306e\u5c64\u306b\u3068\u3063\u3066\u306e\u5b66\u7fd2\u30c7\u30fc\u30bf\uff09\u3068\u306a\u308b\n        # Make predictions on the validation data. This will be the input data for the next layer (training data for the next layer).\n        pred_train = model.predict(d_val, ntree_limit=model.best_ntree_limit)\n        \n        # \u4e88\u6e2c\u3057\u305f\u3082\u306e\u3092next_train\u306b\u8ffd\u52a0\u3057\u3066\u3044\u304f\n        # Add the predictions to the next_train\n        next_train.append(pred_train)\n        \n        # \u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u306b\u9078\u3070\u308c\u305f\u30c7\u30fc\u30bf\u306eindex\u756a\u53f7\u3092\u30ea\u30b9\u30c8\u306b\u8ffd\u52a0\u3057\u3066\u3044\u304f\n        # Add the index number of the data selected for verification to the list.\n        vl_ids.append(vl_id)\n        \n        # \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3057\u3066\u4e88\u6e2c\u3092\u884c\u3046\u3002\u3053\u308c\u304c\u6b21\u306e\u5c64\u306e\u5165\u529b\u30c7\u30fc\u30bf\uff08\u6b21\u306e\u5c64\u306b\u3068\u3063\u3066\u306e\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\uff09\u3068\u306a\u308b\n        # Make predictions on the test data. This will be the input data for the next layer (the test data for the next layer).\n        pred_test = model.predict(d_test, ntree_limit=model.best_ntree_limit)\n        \n        # \u4e88\u6e2c\u3057\u305f\u3082\u306e\u3092next_test\u306b\u8ffd\u52a0\u3057\u3066\u3044\u304f\u3002\n        # Add the predictions to the next_test.\n        next_test.append(pred_test)\n        \n    # \u3068\u308a\u3042\u3048\u305a\u3053\u306e\u64cd\u4f5c\u3092\u3057\u3066\u304a\u304f\u3002\n    # I'll do this operation for now.\n    vl_ids = np.concatenate(vl_ids)\n    next_train = np.concatenate(next_train, axis=0)\n    \n    # KFold\u306eshuffle=True\u306b\u3088\u3063\u3066\u884c\u756a\u53f7\u304c\u30d0\u30e9\u30d0\u30e9\u306b\u306a\u3063\u3066\u3044\u308b\u306e\u3067\u3001\u305d\u308c\u3092\u3053\u3053\u3067\u4fee\u6b63\u3059\u308b\u3002\n    # KFold's shuffle=True is causing the line numbers to be broken up, so we'll fix that here.\n    order = np.argsort(vl_ids)\n    next_train = next_train[order]\n    \n    # KFold\u3092\u4f7f\u3063\u305f\u3053\u3068\u306b\u3088\u3063\u3066\u3001\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u4e88\u6e2c\u304cn_splits=10\u56de\u5206\u884c\u308f\u308c\u3066\u3044\u308b\u306e\u3067\u3001\u5e73\u5747\u3092\u3068\u3063\u30661\u3064\u306b\u3059\u308b\u3002\n    # By using KFold, we have made \"n_splits=10\" predictions for the test data, so we will average them into one.\n    next_test = np.mean(next_test, axis=0)\n    \n    # pd.Series\u578b\u3067\u8fd4\u3059\n    # Return as pd.Series type\n    return pd.Series(next_train), pd.Series(next_test)\n\n\n# \u4e0a\u8a18\u306estack_xgb\u3068\u540c\u69d8\n# Same as stack_xgb above\ndef stack_lgb(X, X_test, y, params):\n    \n    SEED = random.randint(0, 100)\n    kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n    \n    next_train = []\n    next_test = []\n    vl_ids = []\n    \n    for tr_id, vl_id in kf.split(X, y):\n        \n        X_train, X_val = X[tr_id, :], X[vl_id, :]\n        y_train, y_val = y[tr_id], y[vl_id]\n\n        lgb_train = lgb.Dataset(X_train, label=y_train)\n        lgb_val = lgb.Dataset(X_val, label=y_val, reference=lgb_train)\n\n        model = lgb.train(params=params,\n                          train_set=lgb_train,\n                          valid_sets=(lgb_train, lgb_val),\n                          num_boost_round=10000,\n                          early_stopping_rounds=100,\n                          verbose_eval=0)\n        \n        pred_train = model.predict(X_val, num_iteration=model.best_iteration)\n        next_train.append(pred_train)\n        \n        pred_test = model.predict(X_test, num_iteration=model.best_iteration)\n        next_test.append(pred_test)\n        \n        vl_ids.append(vl_id)\n        \n    vl_ids = np.concatenate(vl_ids)\n    next_train = np.concatenate(next_train, axis=0)\n    order = np.argsort(vl_ids)\n    next_train = next_train[order]\n    \n    next_test = np.mean(next_test, axis=0)\n    \n    return pd.Series(next_train), pd.Series(next_test)","dc09b766":"params_xgb11 = {\n    \"booster\": \"gbtree\",\n    \"objective\": \"reg:squarederror\",\n    \"eval_metric\": \"rmse\",\n    \"max_depth\": 15,\n    \"eta\": 0.02,\n    \"gamma\": 0.005346636874993822,\n    \"colsample_bytree\": 0.5,\n    \"subsample\": 0.7,\n    \"min_child_weight\": 257,\n    \"alpha\": 0.01563,\n    \"lambda\": 0.003,\n    \"tree_method\": \"gpu_hist\",\n    \"predictor\": \"gpu_predictor\",\n    \"seed\": SEED\n}\n\nparams_xgb12 = {\n    \"booster\": \"gbtree\",\n    \"objective\": \"reg:squarederror\",\n    \"eval_metric\": \"rmse\",\n    \"max_depth\": 20,\n    \"eta\": 0.02,\n    \"colsample_bytree\": 1.0,\n    \"subsample\": 0.5,\n    \"min_child_weight\": 100,\n    \"tree_method\": \"gpu_hist\",\n    \"predictor\": \"gpu_predictor\",\n    \"seed\": SEED\n}\n\nparams_xgb13 = {\n    \"booster\": \"gbtree\",\n    \"objective\": \"reg:squarederror\",\n    \"eval_metric\": \"rmse\",\n    \"max_depth\": 10,\n    \"eta\": 0.02,\n    \"colsample_bytree\": 0.5,\n    \"subsample\": 0.8,\n    \"min_child_weight\": 20,\n    \"tree_method\": \"gpu_hist\",\n    \"predictor\": \"gpu_predictor\",\n    \"seed\": SEED\n}\n\nparams_xgb14 = {\n    \"booster\": \"gbtree\",\n    \"objective\": \"reg:squarederror\",\n    \"eval_metric\": \"rmse\",\n    \"max_depth\": 6,\n    \"eta\": 0.01,\n    \"colsample_bytree\": 1.0,\n    \"subsample\": 0.8,\n    \"tree_method\": \"gpu_hist\",\n    \"predictor\": \"gpu_predictor\",\n    \"seed\": SEED\n}\n\nparams_lgb11 = {\n    \"task\": \"train\",\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    \"learning_rate\": 0.02,\n    \"num_leaves\": 256,\n    \"bagging_fraction\": 0.8206341150202605,\n    \"feature_fraction\": 0.5,\n    \"min_data_in_leaf\": 100,\n    \"lambda_l1\": 1.074622455507616e-05,\n    \"lambda_l2\": 2.0521330798729704e-06,\n    \"min_data_per_group\": 5,\n    \"max_depth\": -1,\n    \"subsample_for_bin\": 200000,\n    \"cat_smooth\": 1.0,\n    \"importance_type\": \"split\",\n    \"min_sum_hessian_in_leaf\": 0.001,\n    \"bagging_freq\": 6,\n    \"min_gain_to_split\": 0.0,\n    \"random_state\": SEED\n}\n\nparams_lgb12 = {\n    \"task\": \"train\",\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    \"learning_rate\": 0.02,\n    \"num_leaves\": 256,\n    \"max_depth\": 15,\n    \"bagging_fraction\": 0.8206341150202605,\n    \"feature_fraction\": 0.5,\n    \"min_data_in_leaf\": 100,\n    \"lambda_l1\": 1.074622455507616e-05,\n    \"lambda_l2\": 2.0521330798729704e-06,\n    \"min_data_per_group\": 5,\n    \"max_depth\": -1,\n    \"subsample_for_bin\": 200000,\n    \"cat_smooth\": 1.0,\n    \"importance_type\": \"split\",\n    \"min_sum_hessian_in_leaf\": 0.001,\n    \"bagging_freq\": 6,\n    \"min_gain_to_split\": 0.0,\n    \"random_state\": SEED\n}\n\nparams_lgb13 = {\n    \"task\": \"train\",\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    \"learning_rate\": 0.02,\n    \"num_leaves\": 50,\n    \"bagging_fraction\": 0.5,\n    \"feature_fraction\": 0.8,\n    \"random_state\": SEED\n}\n\nparams_lgb14 = {\n    \"task\": \"train\",\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    \"learning_rate\": 0.02,\n    \"num_leaves\": 100,\n    \"max_depth\": 7,\n    \"bagging_fraction\": 0.8,\n    \"feature_fraction\": 1.0,\n    \"random_state\": SEED\n}","8c5704a9":"next_train_11, next_test_11 = stack_xgb(X, X_test, y, params_xgb11)\nnext_train_12, next_test_12 = stack_xgb(X, X_test, y, params_xgb12)\nnext_train_13, next_test_13 = stack_xgb(X, X_test, y, params_xgb13)\nnext_train_14, next_test_14 = stack_xgb(X, X_test, y, params_xgb14)\nnext_train_15, next_test_15 = stack_lgb(X, X_test, y, params_lgb11)\nnext_train_16, next_test_16 = stack_lgb(X, X_test, y, params_lgb12)\nnext_train_17, next_test_17 = stack_lgb(X, X_test, y, params_lgb13)\nnext_train_18, next_test_18 = stack_lgb(X, X_test, y, params_lgb14)","512f5089":"next_train_1 = pd.concat([next_train_11, next_train_12,\n                          next_train_13, next_train_14,\n                          next_train_15, next_train_16,\n                          next_train_17, next_train_18], axis=1)\n\nnext_test_1 = pd.concat([next_test_11, next_test_12,\n                         next_test_13, next_test_14,\n                         next_test_15, next_test_16,\n                         next_test_17, next_test_18], axis=1)\n\nnext_train_1 = np.array(next_train_1)\nnext_test_1 = np.array(next_test_1)","1b7510a5":"params_xgb21 = {\n    \"booster\": \"gbtree\",\n    \"objective\": \"reg:squarederror\",\n    \"eval_metric\": \"rmse\",\n    \"max_depth\": 15,\n    \"eta\": 0.01,\n    \"gamma\": 0.004,\n    \"colsample_bytree\": 0.9,\n    \"subsample\": 0.7,\n    \"min_child_weight\": 200,\n    \"alpha\": 0.005,\n    \"lambda\": 0.001,\n    \"tree_method\": \"gpu_hist\",\n    \"predictor\": \"gpu_predictor\",\n    \"seed\": SEED\n}\n\nparams_xgb22 = {\n    \"booster\": \"gbtree\",\n    \"objective\": \"reg:squarederror\",\n    \"eval_metric\": \"rmse\",\n    \"max_depth\": 8,\n    \"eta\": 0.01,\n    \"gamma\": 0.0006,\n    \"colsample_bytree\": 0.9,\n    \"subsample\": 0.5,\n    \"min_child_weight\": 50,\n    \"alpha\": 0.0004,\n    \"lambda\": 0.00002,\n    \"tree_method\": \"gpu_hist\",\n    \"predictor\": \"gpu_predictor\",\n    \"seed\": SEED\n}\n\nparams_xgb23 = {\n    \"booster\": \"gbtree\",\n    \"objective\": \"reg:squarederror\",\n    \"eval_metric\": \"rmse\",\n    \"max_depth\": 6,\n    \"eta\": 0.01,\n    \"colsample_bytree\": 1.0,\n    \"subsample\": 0.95,\n    \"min_child_weight\": 10,\n    \"tree_method\": \"gpu_hist\",\n    \"predictor\": \"gpu_predictor\",\n    \"seed\": SEED\n}\n\nparams_lgb21 = {\n    \"task\": \"train\",\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    \"learning_rate\": 0.02,\n    \"num_leaves\": 256,\n    \"max_depth\": 15,\n    \"bagging_fraction\": 0.8,\n    \"feature_fraction\": 0.8,\n    \"min_data_in_leaf\": 10,\n    \"lambda_l1\": 0.00003,\n    \"lambda_l2\": 0.000035,\n    \"random_state\": SEED\n}\n\nparams_lgb22 = {\n    \"task\": \"train\",\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    \"learning_rate\": 0.01,\n    \"num_leaves\": 50,\n    \"max_depth\": 20,\n    \"bagging_fraction\": 0.8,\n    \"feature_fraction\": 1.0,\n    \"random_state\": SEED\n}","a9e743f6":"next_train_21, next_test_21 = stack_xgb(next_train_1, next_test_1, y, params_xgb21)\nnext_train_22, next_test_22 = stack_xgb(next_train_1, next_test_1, y, params_xgb22)\nnext_train_23, next_test_23 = stack_xgb(next_train_1, next_test_1, y, params_xgb23)\nnext_train_24, next_test_24 = stack_lgb(next_train_1, next_test_1, y, params_lgb21)\nnext_train_25, next_test_25 = stack_lgb(next_train_1, next_test_1, y, params_lgb22)","96ba38b2":"next_train_2 = pd.concat([next_train_21, next_train_22, next_train_23,\n                          next_train_24, next_train_25], axis=1)\nnext_test_2 = pd.concat([next_test_21, next_test_22, next_test_23,\n                         next_test_24, next_test_25], axis=1)\n\nnext_train_2 = np.array(next_train_2)\nnext_test_2 = np.array(next_test_2)","043b6c62":"params_xgb = {\n    \"booster\": \"gbtree\",\n    \"objective\": \"reg:squarederror\",\n    \"eval_metric\": \"rmse\",\n    \"tree_method\": \"gpu_hist\",\n    \"predictor\": \"gpu_predictor\",\n    \"eta\": 0.01,\n    \"max_depth\": 4,\n    \"gamma\": 0.09192773718666183, \n    \"colsample_bytree\": 0.6961218462820887,\n    \"subsample\": 0.9987425774067743,\n    \"min_child_weight\": 0.1137086502328514,\n    \"alpha\": 0.9245596765233609,\n    \"lambda\": 6.230027264411933e-06,\n    \"seed\": SEED\n}\n\nparams_lgb = {\n    \"task\": \"train\",\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"regression\",\n    \"metric\": \"rmse\",\n    \"learning_rate\": 0.01,\n    \"num_leaves\": 164,\n    \"max_depth\": 4,\n    \"bagging_fraction\": 0.8498649731960014,\n    \"feature_fraction\": 0.5016568140931941,\n    \"min_data_in_leaf\": 200,\n    \"lambda_l1\": 0.00023282363705273031,\n    \"lambda_l2\": 4.693349803533469e-08,\n    \"random_state\": SEED\n}","b6957807":"kf = KFold(n_splits=10, shuffle=True, random_state=SEED)","5dd771bb":"pred_xgb = pd.DataFrame()\nd_test = xgb.DMatrix(next_test_2)\n\nfor tr_id, vl_id in kf.split(next_train_2, y):\n    \n    X_train, X_val = next_train_2[tr_id, :], next_train_2[vl_id, :]\n    y_train, y_val = y[tr_id], y[vl_id]\n    \n    d_train = xgb.DMatrix(X_train, label=y_train)\n    d_val = xgb.DMatrix(X_val, label=y_val)\n    \n    model = xgb.train(params=params_xgb,\n                      dtrain=d_train,\n                      num_boost_round=10000,\n                      early_stopping_rounds=100,\n                      verbose_eval=0,\n                      evals=[(d_val, \"val\")])\n    \n    pred = model.predict(d_test, ntree_limit=model.best_ntree_limit)\n    pred = pd.Series(pred)\n    pred_xgb = pd.concat([pred_xgb, pred], axis=1)","9d9d2e4d":"pred_xgb.head()","90ce695b":"pred_lgb = pd.DataFrame()\n\nfor tr_id, vl_id in kf.split(next_train_2, y):\n    \n    X_train, X_val = next_train_2[tr_id, :], next_train_2[vl_id, :]\n    y_train, y_val = y[tr_id], y[vl_id]\n    \n    lgb_train = lgb.Dataset(X_train, label=y_train)\n    lgb_val = lgb.Dataset(X_val, label=y_val, reference=lgb_train)\n\n    model = lgb.train(params=params_lgb,\n                      train_set=lgb_train,\n                      valid_sets=(lgb_train, lgb_val),\n                      num_boost_round=10000,\n                      early_stopping_rounds=100,\n                      verbose_eval=0)\n    \n    pred = model.predict(next_test_2, num_iteration=model.best_iteration)\n    pred = pd.Series(pred)\n    pred_lgb = pd.concat([pred_lgb, pred], axis=1)","52525e7c":"pred_lgb.head()","9096f940":"pred = pd.concat([pred_xgb, pred_lgb], axis=1)\npred = pred.mean(axis=1)","18905242":"sample_sub = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/sample_submission.csv\")\nsub = sample_sub.copy()\n\nsub[\"target\"] = pred\nsub","07b515f4":"sub.to_csv(\"submission_stackingGBDTs.csv\", index=False)","b69ee780":"# Submission","c6d7155f":"# This is a Notebook that implements how to do stacking for serious beginners.\nI've only recently started Kaggle, so I can't write code well. For an expert user, I think the code is written in a very beginner-like way. But on the other hand, I think it makes the code easy to understand for beginners.","d510234e":"# Last Layer \/ \u6700\u5f8c\u306e\u5c64\nThe inputs of the last layer are the outputs of the second layer, next_train_2 and next_test_2. In the last layer, we use KFold to predict as usual.  \n\u6700\u5f8c\u306e\u5c64\u306e\u5165\u529b\u306f2\u5c64\u76ee\u306e\u51fa\u529b\u3067\u3042\u308bnext_train_2, next_test_2\u305f\u3061\u3002\u6700\u5f8c\u306e\u5c64\u3067\u306fKFold\u3092\u4f7f\u3063\u3066\u666e\u901a\u306b\u4e88\u6e2c\u3059\u308b\u3002","d7773651":"# \u8d85\u521d\u5fc3\u8005\u5411\u3051\u306b\u30b9\u30bf\u30c3\u30ad\u30f3\u30b0\u306e\u3084\u308a\u65b9\u3092\u5b9f\u88c5\u3057\u305fNotebook\u3067\u3059\u3002\n\u79c1\u81ea\u8eab\u304c\u6700\u8fd1Kaggle\u3092\u59cb\u3081\u305f\u3070\u304b\u308a\u306a\u306e\u3067\u3001\u30b3\u30fc\u30c9\u3092\u304d\u308c\u3044\u306b\u66f8\u304f\u3053\u3068\u304c\u3067\u304d\u307e\u305b\u3093\u3002\u7384\u4eba\u306e\u65b9\u304b\u3089\u3057\u305f\u3089\u3001\u3044\u304b\u306b\u3082\u521d\u5fc3\u8005\u306a\u66f8\u304d\u65b9\u3092\u3057\u3066\u3044\u308b\u3068\u601d\u3044\u307e\u3059\u3002\u3057\u304b\u3057\u3001\u9006\u306b\u305d\u306e\u5206\u3001\u521d\u5fc3\u8005\u306e\u65b9\u306b\u306f\u8aad\u307f\u3084\u3059\u3044\u30b3\u30fc\u30c9\u306b\u306a\u3063\u3066\u3044\u308b\u3068\u601d\u3044\u307e\u3059\u3002","58a91b92":"# Modeling\n2\u5c64\u306e\u30b9\u30bf\u30c3\u30ad\u30f3\u30b0\u3092\u884c\u3046\u3002two-layer stacking.","ba95a760":"# Layer 2 \/ 2\u5c64\u76ee\nThe input of the second layer is the output of the first layer, next_train_1, next_test_1, and so on.  \n2\u5c64\u76ee\u306e\u5165\u529b\u306f1\u5c64\u76ee\u306e\u51fa\u529b\u3067\u3042\u308bnext_train_1, next_test_1\u305f\u3061\u3002","0b67c368":"# Layer 1 \/ 1\u5c64\u76ee\nThe input for the first layer is the original training data X, y, X_test.  \n1\u5c64\u76ee\u306e\u5165\u529b\u306f\u3082\u3068\u3082\u3068\u306e\u5b66\u7fd2\u30c7\u30fc\u30bf\u306eX, y, X_test\u3002","6db7711f":"### If you find it helpful, I would appreciate it if you could upvote it...  \n### \u53c2\u8003\u306b\u306a\u308a\u307e\u3057\u305f\u3089\u3001Upvote\u3057\u3066\u3044\u305f\u3060\u3051\u308b\u3068\u5e78\u3044\u3067\u3059\u3001\u3001\u3001"}}