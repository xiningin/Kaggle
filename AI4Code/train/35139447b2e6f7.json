{"cell_type":{"4a2575be":"code","90a13b2e":"code","f0b76211":"code","d04e816d":"code","741d0d62":"code","adc75daf":"code","56231362":"code","335fe99a":"code","1c6cc475":"code","b0ee46ed":"code","8d7c0b13":"code","87c23763":"code","451a8b93":"code","c2cb0f6c":"code","514ac4d7":"code","c5063eee":"code","88b0ed9b":"code","1486f232":"code","df1c5bb4":"code","7c211f36":"code","6c776c67":"code","8240c1c2":"code","a9158488":"code","7e9d0b5b":"code","41e9e51c":"code","138d26a6":"code","3a19ba92":"code","eabf93c1":"code","af7287ee":"code","079e91c9":"code","897726ee":"code","5cdddd10":"code","c88cbc36":"code","fe59ddd9":"code","aff9b8df":"code","dc7f7ab9":"code","17f69d22":"code","12242927":"code","17dd11a8":"code","6a12fc8a":"code","e8ccd568":"code","55a3dd38":"code","a86f4c6a":"code","1261ac54":"code","763fede2":"code","663d54fd":"code","0f378cbe":"code","6e2cee77":"code","65d47d1e":"markdown","b589b295":"markdown","2fb544af":"markdown","0a1d1282":"markdown","0358b57d":"markdown","6d2eb12c":"markdown","0664e239":"markdown","3f583c04":"markdown","402b92d8":"markdown","4d9d098d":"markdown","326a5114":"markdown","06358f55":"markdown","67c22965":"markdown","ee17a0ac":"markdown","4ff69555":"markdown","b5af1f63":"markdown","c3494755":"markdown","fa31bf66":"markdown","99c32378":"markdown","af507c80":"markdown","28d140af":"markdown","529242b1":"markdown"},"source":{"4a2575be":"# imports and essentials\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd \nimport random\nfrom matplotlib import pyplot as plt\nfrom IPython.display import display \n%matplotlib inline","90a13b2e":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\ndef rmse(y_actual,y_predicted):\n    return sqrt(mean_squared_error(y_actual, y_predicted))","f0b76211":"# read the data into pandas DataFrames\ntrain_data = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\nitems = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\ncategories = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nshops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')\ntest_data = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv').drop('ID',axis = 1)","d04e816d":"print(\"train_data shape: \" + str(train_data.shape))\nprint(\"items shape: \" + str(items.shape))\nprint(\"categories shape: \" + str(categories.shape))\nprint(\"shops shape: \" + str(shops.shape))\nprint(\"test_data shape: \" + str(test_data.shape))","741d0d62":"# aggregate train_data monthly\ntrain_data = train_data[train_data['item_cnt_day'] > 0]\ntrain_data['daily_rev'] = train_data['item_price'] * train_data['item_cnt_day']\ntrain_data = train_data.groupby(['date_block_num','shop_id','item_id'],axis=0).agg({'item_cnt_day':'sum','daily_rev':'sum'}).reset_index()\ntrain_data.columns = ['date_block_num','shop_id','item_id','item_shop_cnt','item_shop_rev']","adc75daf":"train_data.head(15)","56231362":"def expand_train_data(train_data):\n    from itertools import product\n    num_months = 34\n\n    expanded = []\n    cols = ['date_block_num','shop_id','item_id']\n    for i in range(num_months):\n        sales = train_data[train_data['date_block_num'] == i]\n        expanded.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique()))))    \n    expanded = pd.DataFrame(np.vstack(expanded), columns=cols)\n    train_data = pd.merge(expanded, train_data, on=cols, how='left')\n    return train_data","335fe99a":"train_data = expand_train_data(train_data)\ntrain_data","1c6cc475":"# plotting revenue per month for the store 24\nshop_id = 24\nplt.figure()\ntrain_data[train_data['shop_id'] == shop_id].fillna(0).groupby(train_data['date_block_num']).agg({'item_shop_rev': 'sum'}).reset_index()['item_shop_rev'].plot(kind=\"bar\", figsize=(20,10), color=plt.cm.nipy_spectral(np.linspace(0, 1, 34)))\nplt.xlabel(\"Month ID\",fontsize=20)\nplt.ylabel(\"Revenue Per Month(10^6)\",fontsize=20)\nplt.title(\"Revenue Per Month - Store ID: 24\",fontsize=24)\nplt.show()","b0ee46ed":"# plotting items bought per month for the store 24\nshop_id = 24\nplt.figure()\ntrain_data[train_data['shop_id'] == shop_id].fillna(0).groupby(train_data['date_block_num']).agg({'item_shop_cnt': 'sum'}).reset_index()['item_shop_cnt'].plot(kind=\"bar\", figsize=(20,10), color=plt.cm.brg(np.linspace(0, 1, 34)))\nplt.xlabel(\"Month ID\",fontsize=20)\nplt.ylabel(\"Items Bought Per Month\",fontsize=20)\nplt.title(\"Items Bought Per Month - Store ID: 24\",fontsize=24)\nplt.show()","8d7c0b13":"# adding the test data to our data\ntest_data['date_block_num'] = 34\nkeys = ['date_block_num','shop_id','item_id']\ntrain_data = pd.concat([train_data, test_data], ignore_index=True, sort=False,keys=keys)","87c23763":"train_data['item_shop_cnt'] = train_data['item_shop_cnt'].clip(0,20)\ntrain_data['cnt_per_month'] = train_data['item_shop_cnt']\ntrain_data = train_data.fillna(0)","451a8b93":"# We are going to merge our data with previous month salse\nnext_month_features = ['date_block_num','shop_id','item_id',\n                      'item_shop_cnt','item_shop_rev']\nmerge_month_features = ['date_block_num','shop_id','item_id']\n\nnext_month = train_data[next_month_features].copy()\nnext_month['date_block_num'] += 1\ntrain_data = train_data.drop(['item_shop_cnt','item_shop_rev'], axis=1)\ntrain_data = pd.merge(train_data, next_month,\n                     on=merge_month_features,\n                     how='left').fillna(0)","c2cb0f6c":"\ntrain_data  = train_data.rename(columns={'item_shop_cnt' : 'item_shop_cnt_prev_mo',\n                                        'item_shop_rev': 'item_shop_rev_prev_mo'})\ntrain_data","514ac4d7":"# adding category feature\ntrain_data = pd.merge(train_data, items[['item_id','item_category_id']],on=['item_id'])\ntrain_data","c5063eee":"# adding actual month\ntrain_data['month'] = train_data['date_block_num'] % 12\ntrain_data","88b0ed9b":"# adding previous month sales of each item(how many times the item was purchased in the last month)\nlast_month_item_count = train_data.groupby(['date_block_num','item_id'],axis=0).agg({'item_shop_cnt_prev_mo': 'sum'}).reset_index()\nlast_month_item_count = last_month_item_count.rename(columns={'item_shop_cnt_prev_mo': 'sales_item_prev_month'})\ntrain_data = pd.merge(train_data,last_month_item_count)\ntrain_data","1486f232":"# add all sales in a month\nall_sales = train_data.groupby(['date_block_num'],axis=0).agg({'item_shop_cnt_prev_mo': 'sum'}).reset_index()\nall_sales = all_sales.rename(columns={'item_shop_cnt_prev_mo': 'all_sales_perv_month'})\ntrain_data = pd.merge(train_data,all_sales)\ntrain_data","df1c5bb4":"# adding previous month sales of each shop\nlast_month_item_count = train_data.groupby(['date_block_num','shop_id'],axis=0).agg({'item_shop_cnt_prev_mo': 'sum'}).reset_index()\nlast_month_item_count = last_month_item_count.rename(columns={'item_shop_cnt_prev_mo': 'sales_shop_prev_month'})\ntrain_data = pd.merge(train_data,last_month_item_count)\ntrain_data","7c211f36":"# saving our final data in right formats\n\ntypes = {'date_block_num' : np.int8,\n        'shop_id' : np.int8,\n        'item_id' : np.int16,\n        'cnt_per_month' : np.int32,\n        'item_shop_cnt_prev_mo' : np.int32,\n        'item_shop_rev_prev_mo' : np.float64,\n        'item_category_id' : np.int8,\n        'month' : np.int8,\n        'sales_item_prev_month' : np.int32,\n        'all_sales_perv_month' : np.int32,\n        'sales_shop_prev_month' : np.int32}\n\ntrain_data = train_data.fillna(0)\ntrain_data = train_data.astype(types)","6c776c67":"import pickle\ntrain_data.to_pickle('data.pkl')\ntest_data = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv').drop('ID',axis = 1)","8240c1c2":"# creating our train data\nX_train = train_data[train_data['date_block_num'] < 33]\nX_train = X_train.drop(['cnt_per_month'],axis=1)\ny_train = train_data[train_data['date_block_num'] < 33]['cnt_per_month']\n\n# creating our validation data\nX_val = train_data[train_data['date_block_num'] == 33]\nX_val = X_val.drop(['cnt_per_month'],axis=1)\ny_val = train_data[train_data['date_block_num'] == 33]['cnt_per_month']\n\n# creating our test data\nX_test = train_data[train_data['date_block_num'] == 34]\nX_test = X_test.drop(['cnt_per_month'],axis=1)\nX_test = pd.merge(test_data,X_test, on=['item_id','shop_id'],right_index=True).sort_index()\ncols = X_train.columns\nX_test = X_test[cols]","a9158488":"# our benchmark model - DecisionTreeRegressor\nfrom sklearn.tree import DecisionTreeRegressor\ndt = DecisionTreeRegressor()\n# fit the model on the train set\ndt.fit(X_train,y_train)","7e9d0b5b":"# calculate RMSE for our validation data\ny_pred_val = dt.predict(X_val)\nprint(\"RMSE for validation set is: \"+str(rmse(y_val,y_pred_val)))","41e9e51c":"def submit_to_kaggle(index, y_test,title):\n    submission = pd.DataFrame({\n        \"ID\": index, \n        \"item_cnt_month\": y_test\n    })\n    submission.to_csv(title, index=False)","138d26a6":"# submitting scores to kaggle\ny_test = dt.predict(X_test).clip(0,20)\nindex = X_test.index\nsubmit_to_kaggle(index,y_test,\"decision_tree_benchmark.csv\")","3a19ba92":"from keras.models import Model\nfrom keras.layers import Input, Embedding, concatenate, Dense, Flatten, Dropout, BatchNormalization\nfrom keras.regularizers import l2\nfrom keras.metrics import RootMeanSquaredError","eabf93c1":"def plot_accuracy_vs_loss(history):\n  # from lecture no.2\n\n  fig, ax = plt.subplots(1,2,figsize=(12,4))\n  ax[0].plot(history.history['root_mean_squared_error'])\n  ax[0].plot(history.history['val_root_mean_squared_error'])\n  ax[0].set_title('Model accuracy')\n  ax[0].set_ylabel('Accuracy')\n  ax[0].set_xlabel('Epoch')\n  ax[0].legend(['Train', 'Test'], loc='upper left')\n\n  # Plot training & validation loss values\n  ax[1].plot(history.history['loss'])\n  ax[1].plot(history.history['val_loss'])\n  ax[1].set_title('Model loss')\n  ax[1].set_ylabel('Loss')\n  ax[1].set_xlabel('Epoch')\n  ax[1].legend(['Train', 'Test'], loc='upper left')\n  plt.show()","af7287ee":"def get_embedding_layer(shape,inp_len,reg,is_fe=False,weights=[]):\n    emb = (Embedding(shape[0],shape[1],input_length=inp_len,embeddings_regularizer=l2(reg))) if not is_fe else (Embedding(shape[0],shape[1],input_length=inp_len,embeddings_regularizer=l2(reg),weights=weights))\n    return emb\ndef get_shape_for_embedding(X):\n    X_shp = X.shape[0]\n    return (X_shp,int(np.sqrt(X_shp)))","079e91c9":"input_month = Input(shape=(1,), dtype='int64')\ninput_shop = Input(shape=(1,), dtype='int64')\ninput_category = Input(shape=(1,), dtype='int64')\ninput_item = Input(shape=(1,), dtype='int64')","897726ee":"cat_features = ['month','shop_id','item_category_id','item_id']\nX_train_cat = X_train[cat_features]\nX_val_cat = X_val[cat_features]\nX_test_cat = X_test[cat_features]","5cdddd10":"months_shape = (12,4)\nshops_shape = get_shape_for_embedding(shops)\ncats_shape = get_shape_for_embedding(categories)\nitems_shape = get_shape_for_embedding(items)\n\nmonths_emb_cat = get_embedding_layer(months_shape,1,1e-5)(input_month)\nshops_emb_cat = get_embedding_layer(shops_shape,1,1e-5)(input_shop)\ncats_emb_cat = get_embedding_layer(cats_shape,1,1e-5)(input_category)\nitems_emb_cat = get_embedding_layer(items_shape,1,1e-5)(input_item)","c88cbc36":"x =  concatenate([months_emb_cat,shops_emb_cat,cats_emb_cat,items_emb_cat])\nx = Flatten()(x)\nx = Dense(1,activation='relu')(x)\n\nmodel_embedding_cat = Model([input_month,input_shop,input_category,input_item],x)\nmodel_embedding_cat.compile(loss = 'mse',optimizer='RMSProp', metrics=[RootMeanSquaredError()])\nmodel_embedding_cat.summary()","fe59ddd9":"hist_emb = model_embedding_cat.fit([X_train_cat['month'],X_train_cat['shop_id'],X_train_cat['item_category_id'],X_train_cat['item_id']],\n                       y_train,\n                       epochs=6,\n                       validation_data=[[X_val_cat['month'],X_val_cat['shop_id'],X_val_cat['item_category_id'],X_val_cat['item_id']],y_val],\n                       batch_size = 4096,\n                       shuffle=True)\nplot_accuracy_vs_loss(hist_emb)","aff9b8df":"# submitting scores to kaggle\ny_test_cat = model_embedding_cat.predict([X_test_cat['month'],X_test_cat['shop_id'],X_test_cat['item_category_id'],X_test_cat['item_id']]).clip(0,20)\nindex = X_test_cat.index\nsubmit_to_kaggle(index,y_test_cat.ravel(),\"model_embedding_0.csv\")","dc7f7ab9":"x =  concatenate([months_emb_cat,shops_emb_cat,cats_emb_cat,items_emb_cat])\nx = Flatten()(x)\nx = BatchNormalization()(x)\n\nx = Dense(64, activation='relu')(x)\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.3)(x)\nx = Dense(128, activation='relu')(x)\nx = Dense(256, activation='relu')(x)\nx = Dropout(0.2)(x)\n\nx = Dense(1,activation='relu')(x)\n\nmodel_embedding_cat_1 = Model([input_month,input_shop,input_category,input_item],x)\nmodel_embedding_cat_1.compile(loss = 'mse',optimizer='RMSProp', metrics=[RootMeanSquaredError()])\nmodel_embedding_cat_1.summary()","17f69d22":"hist_emb_1 = model_embedding_cat_1.fit([X_train_cat['month'],X_train_cat['shop_id'],X_train_cat['item_category_id'],X_train_cat['item_id']],\n                       y_train,\n                       epochs=6,\n                       validation_data=[[X_val_cat['month'],X_val_cat['shop_id'],X_val_cat['item_category_id'],X_val_cat['item_id']],y_val],\n                       batch_size = 4096,\n                       shuffle=True)\nplot_accuracy_vs_loss(hist_emb_1)","12242927":"# submitting scores to kaggle\ny_test_cat = model_embedding_cat_1.predict([X_test_cat['month'],X_test_cat['shop_id'],X_test_cat['item_category_id'],X_test_cat['item_id']]).clip(0,20)\nindex = X_test_cat.index\nsubmit_to_kaggle(index,y_test_cat.ravel(),\"model_embedding_1.csv\")","17dd11a8":"input_all_items = Input(shape=(1,), dtype='int64')\ninput_all_shops = Input(shape=(1,), dtype='int64')\n\nshops_shape = get_shape_for_embedding(shops)\nitems_shape = get_shape_for_embedding(items)\n\nall_items_emb = get_embedding_layer(items_shape,1,1e-5)(input_all_items)\nall_shops_emb = get_embedding_layer(shops_shape,1,1e-5)(input_all_shops)\n","6a12fc8a":"x =  concatenate([months_emb_cat,shops_emb_cat,cats_emb_cat,items_emb_cat,all_items_emb,all_shops_emb])\nx = Flatten()(x)\nx = BatchNormalization()(x)\n\nx = Dense(64, activation='relu')(x)\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.3)(x)\nx = Dense(128, activation='relu')(x)\nx = Dense(256, activation='relu')(x)\nx = Dropout(0.2)(x)\n\nx = Dense(1,activation='relu')(x)\n\nmodel_embedding_cat_2 = Model([input_month,input_shop,input_category,input_item,input_all_items,input_all_shops],x)\nmodel_embedding_cat_2.compile(loss = 'mse',optimizer='RMSProp', metrics=[RootMeanSquaredError()])\nmodel_embedding_cat_2.summary()","e8ccd568":"hist_emb_2 = model_embedding_cat_2.fit([X_train_cat['month'],X_train_cat['shop_id'],X_train_cat['item_category_id'],X_train_cat['item_id'],X_train['sales_item_prev_month'],X_train['sales_shop_prev_month']],\n                       y_train,\n                       epochs=6,\n                       validation_data=[[X_val_cat['month'],X_val_cat['shop_id'],X_val_cat['item_category_id'],X_val_cat['item_id'],X_val['sales_item_prev_month'],X_val['sales_shop_prev_month']],y_val],\n                       batch_size = 4096,\n                       shuffle=True)","55a3dd38":"plot_accuracy_vs_loss(hist_emb_2)\n# submitting scores to kaggle\ny_test_cat = model_embedding_cat_2.predict([X_test_cat['month'],X_test_cat['shop_id'],X_test_cat['item_category_id'],X_test_cat['item_id'],X_test['sales_item_prev_month'],X_test['sales_shop_prev_month']]).clip(0,20)\nindex = X_test_cat.index\nsubmit_to_kaggle(index,y_test_cat.ravel(),\"model_embedding_2.csv\")","a86f4c6a":"def get_euclidean_distance(a,b):\n    from scipy.spatial import distance\n    return distance.euclidean(a, b)\n\ndef get_similar_vectors(vectors,thresh):\n    vec_dict = {}\n    i=0\n    for v1 in vectors:\n        similar = []\n        j=0\n        for v2 in vectors:\n            if (v1 != v2).all() and (get_euclidean_distance(v1,v2) < thresh):\n                similar.append(j)\n            j += 1\n        vec_dict[i] = similar\n        i += 1\n    return vec_dict","1261ac54":"# gets the weights for the items from the embedding layer\nitem_weights = model_embedding_cat_2.layers[9].get_weights()\n# checks which weights are similar\nsim = get_similar_vectors(item_weights[0][:1000],0.0001)","763fede2":"print(items.iloc[0].item_name)\nfor i in sim[4]:\n    print(items.iloc[i].item_name)","663d54fd":"# creating a model using the weights of the previous model\nmonths_emb_cat = get_embedding_layer(months_shape,1,1e-5,is_fe=True,weights=model_embedding_cat_2.layers[6].get_weights())(input_month)\nshops_emb_cat = get_embedding_layer(shops_shape,1,1e-5,is_fe=True,weights=model_embedding_cat_2.layers[7].get_weights())(input_shop)\ncats_emb_cat = get_embedding_layer(cats_shape,1,1e-5,is_fe=True,weights=model_embedding_cat_2.layers[8].get_weights())(input_category)\nitems_emb_cat = get_embedding_layer(items_shape,1,1e-5,is_fe=True,weights=model_embedding_cat_2.layers[9].get_weights())(input_item)\nall_items_emb = get_embedding_layer(items_shape,1,1e-5,is_fe=True,weights=model_embedding_cat_2.layers[10].get_weights())(input_all_items)\nall_shops_emb = get_embedding_layer(shops_shape,1,1e-5,is_fe=True,weights=model_embedding_cat_2.layers[11].get_weights())(input_all_shops)\n\nx =  concatenate([months_emb_cat,shops_emb_cat,cats_emb_cat,items_emb_cat,all_items_emb,all_shops_emb],weights=model_embedding_cat_2.layers[12].get_weights())\nx = Flatten(weights=model_embedding_cat_2.layers[13].get_weights())(x)\nx = BatchNormalization(weights=model_embedding_cat_2.layers[14].get_weights())(x)\n\nx = Dense(64, activation='relu',weights=model_embedding_cat_2.layers[15].get_weights())(x)\nx = Dense(128, activation='relu',weights=model_embedding_cat_2.layers[16].get_weights())(x)\nx = Dropout(0.3,weights=model_embedding_cat_2.layers[17].get_weights())(x)\nx = Dense(128, activation='relu',weights=model_embedding_cat_2.layers[18].get_weights())(x)\nx = Dense(256, activation='relu',weights=model_embedding_cat_2.layers[19].get_weights())(x)\nx = Dropout(0.2,weights=model_embedding_cat_2.layers[20].get_weights())(x)\n\nmodel_fe = Model([input_month,input_shop,input_category,input_item,input_all_items,input_all_shops],x)\nmodel_fe.compile(loss = 'mse',optimizer='RMSProp', metrics=[RootMeanSquaredError()])\nmodel_fe.summary()","0f378cbe":"X_fe_emb_train = model_fe.predict([X_train_cat['month'],X_train_cat['shop_id'],X_train_cat['item_category_id'],X_train_cat['item_id'],X_train['sales_item_prev_month'],X_train['sales_shop_prev_month']])\nX_fe_emb_val = model_fe.predict([X_val_cat['month'],X_val_cat['shop_id'],X_val_cat['item_category_id'],X_val_cat['item_id'],X_val['sales_item_prev_month'],X_val['sales_shop_prev_month']])\n\nX_fe_emb_train = np.squeeze(X_fe_emb_train)\nX_fe_emb_val = np.squeeze(X_fe_emb_val)","6e2cee77":"dt = DecisionTreeRegressor()\n# fit the model on the train set\ndt.fit(X_fe_emb_train,y_train)\n\n# calculate RMSE for our validation data\ny_pred_val = dt.predict(X_fe_emb_val)\nprint(\"RMSE for validation set is: \"+str(rmse(y_val,y_pred_val)))\n\n# submitting scores to kaggle\ny_test = dt.predict(X_test).clip(0,20)\nindex = X_test.index\nsubmit_to_kaggle(index,y_test,\"decision_tree_feature_extraction.csv\")","65d47d1e":"# Insights about the Embeddings\n----\n### In this section we will look on the weigths of our embedding layers and see some interesting facts about what we got","b589b295":"![image.png](attachment:image.png)","2fb544af":"## Building Classic ML benchmark - DecisionTreeRegressor\n----\n### In the following code we are going to pass our data into a DecisionTreeRegressor in order to get a solid benchmark score for our predictions.","0a1d1282":"### *We got a RMSE score of : 1.58954*\n![image.png](attachment:image.png)","0358b57d":"## Building Better Categorical Embedding Model\n----\n### In this chapter we will try to improve our previous embedding model.\n* We will make our network even deeper by adding extra layers and we will try to use normalization and dropouts in order to reach better results.\n* In the next attempt we will add the non-categorical features we added previously and create another model.","6d2eb12c":"# Conclusions\n----\n1. We used a DecisionTreeRegressor in order to get a solic benchmark for predicting the sales for next month\n2. We improved the results by creating a categoricl embedding model\n3. We saw that we can improve our predictions by adding a non-categorical features and creating an extended embedding model\n4. We saw that the embeddings tries to minimize the distance between items that are related\n5. We used the notion of Feature-Extraction in order to get a solid benchmark of our best embedding model","0664e239":"### *We got a RMSE score of : 1.60811*\n![image.png](attachment:image.png)","3f583c04":"### On of the runs of the previous code yeilded the following results:<br>\nsim = {0: [4,\n  25,\n  83,\n  104,\n  110,\n  114,\n  124,\n  131,\n  138,\n  139,\n  140,\n  146,\n  147,\n  159,\n  160,\n  162,\n  167]}<br><br>\n  item_names:<br>! \u0412\u041e \u0412\u041b\u0410\u0421\u0422\u0418 \u041d\u0410\u0412\u0410\u0416\u0414\u0415\u041d\u0418\u042f (\u041f\u041b\u0410\u0421\u0422.)         D<br>\n\/\u0422\u042b  - \u0422\u0420\u0423\u041f<br>\n12\/\u0414\u0412\u0415\u041d\u0410\u0414\u0426\u0410\u0422\u042c (\u0421\u0420) (\u0420\u0435\u0433\u0438\u043e\u043d)<br>\n1\u0421-\u0411\u0438\u0442\u0440\u0438\u043a\u0441: \u0423\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0441\u0430\u0439\u0442\u043e\u043c - \u0421\u0442\u0430\u0440\u0442 (Bitrix) [PC, \u0426\u0438\u0444\u0440\u043e\u0432\u0430\u044f \u0432\u0435\u0440\u0441\u0438\u044f]<br>\n1\u0421:\u0410\u0443\u0434\u0438\u043e. \u041c\u0443\u0437\u044b\u043a\u0430 \u0432\u043e\u0441\u0441\u0442\u0430\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u044f \u0440\u0430\u0432\u043d\u043e\u0432\u0435\u0441\u0438\u044f \u0434\u0443\u0448\u0438 \u0438 \u0442\u0435\u043b\u0430 [\u0426\u0438\u0444\u0440\u043e\u0432\u0430\u044f \u0432\u0435\u0440\u0441\u0438\u044f]<br>\n1\u0421:\u0410\u0443\u0434\u0438\u043e. \u041c\u0443\u0437\u044b\u043a\u0430 \u0434\u043b\u044f \u0441\u043d\u044f\u0442\u0438\u044f \u0441\u0442\u0440\u0435\u0441\u0441\u0430 [\u0426\u0438\u0444\u0440\u043e\u0432\u0430\u044f \u0432\u0435\u0440\u0441\u0438\u044f]<br>\n1\u0421:\u0410\u0443\u0434\u0438\u043e\u043a\u043d\u0438\u0433\u0438 \"\u0421\u0435\u0440\u0430\u044f \u0428\u0435\u0439\u043a\u0430\" [\u0426\u0438\u0444\u0440\u043e\u0432\u0430\u044f \u0432\u0435\u0440\u0441\u0438\u044f]<br>\n1\u0421:\u0410\u0443\u0434\u0438\u043e\u043a\u043d\u0438\u0433\u0438 \"\u0421\u043a\u0430\u0437\u043a\u0438. \u0412\u044b\u043f\u0443\u0441\u043a 6\" [\u0426\u0438\u0444\u0440\u043e\u0432\u0430\u044f \u0432\u0435\u0440\u0441\u0438\u044f]<br>\n1\u0421:\u0410\u0443\u0434\u0438\u043e\u043a\u043d\u0438\u0433\u0438. \u0410. \u041d\u0435\u043a\u0440\u0430\u0441\u043e\u0432. \u041f\u0440\u0438\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u044f \u043a\u0430\u043f\u0438\u0442\u0430\u043d\u0430 \u0412\u0440\u0443\u043d\u0433\u0435\u043b\u044f (Digipack)<br>\n1\u0421:\u0410\u0443\u0434\u0438\u043e\u043a\u043d\u0438\u0433\u0438. \u0410.\u041c. \u0412\u043e\u043b\u043a\u043e\u0432. \u0412\u043e\u043b\u0448\u0435\u0431\u043d\u0438\u043a \u0438\u0437\u0443\u043c\u0440\u0443\u0434\u043d\u043e\u0433\u043e \u0433\u043e\u0440\u043e\u0434\u0430 (Digipack)<br>\n1\u0421:\u0410\u0443\u0434\u0438\u043e\u043a\u043d\u0438\u0433\u0438. \u0410\u043b\u0430\u0434\u0434\u0438\u043d \u0438 \u0434\u0440\u0443\u0433\u0438\u0435 \u0432\u043e\u0441\u0442\u043e\u0447\u043d\u044b\u0435 \u0441\u043a\u0430\u0437\u043a\u0438 (Jewel)<br>\n1\u0421:\u0410\u0443\u0434\u0438\u043e\u043a\u043d\u0438\u0433\u0438. \u0410\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u0438\u0435 \u043d\u0435\u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u044b\u0435 \u0433\u043b\u0430\u0433\u043e\u043b\u044b. \u0422\u0440\u0435\u043d\u0430\u0436\u0435\u0440   [PC, \u0426\u0438\u0444\u0440\u043e\u0432\u0430\u044f \u0432\u0435\u0440\u0441\u0438\u044f]<br>\n1\u0421:\u0410\u0443\u0434\u0438\u043e\u043a\u043d\u0438\u0433\u0438. \u0410\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u0438\u0435 \u043f\u043e\u0441\u043b\u043e\u0432\u0438\u0446\u044b, \u043f\u043e\u0433\u043e\u0432\u043e\u0440\u043a\u0438 \u0438 \u0443\u0441\u0442\u043e\u0439\u0447\u0438\u0432\u044b\u0435 \u0432\u044b\u0440\u0430\u0436\u0435\u043d\u0438\u044f. \u0421\u0430\u043c\u043e\u0443\u0447\u0438\u0442\u0435\u043b\u044c  [PC, \u0426\u0438\u0444\u0440\u043e\u0432\u0430\u044f \u0432\u0435\u0440\u0441\u0438\u044f]<br>\n1\u0421:\u0410\u0443\u0434\u0438\u043e\u043a\u043d\u0438\u0433\u0438. \u0410\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u0438\u0439 \u044f\u0437\u044b\u043a \u0437\u0430 12 \u0434\u043d\u0435\u0439: \u0421\u0430\u043c\u043e\u0443\u0447\u0438\u0442\u0435\u043b\u044c. \u0414\u0435\u043b\u043e\u0432\u043e\u0439 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u0438\u0439  [PC, \u0426\u0438\u0444\u0440\u043e\u0432\u0430\u044f \u0432\u0435\u0440\u0441\u0438\u044f]<br>\n1\u0421:\u0410\u0443\u0434\u0438\u043e\u043a\u043d\u0438\u0433\u0438. \u0410\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u0438\u0439 \u044f\u0437\u044b\u043a \u0437\u0430 12 \u0434\u043d\u0435\u0439: \u0421\u0430\u043c\u043e\u0443\u0447\u0438\u0442\u0435\u043b\u044c. \u041e\u0442\u0434\u044b\u0445 \u0437\u0430 \u0440\u0443\u0431\u0435\u0436\u043e\u043c [PC, \u0426\u0438\u0444\u0440\u043e\u0432\u0430\u044f \u0432\u0435\u0440\u0441\u0438\u044f]<br>\n1\u0421:\u0410\u0443\u0434\u0438\u043e\u043a\u043d\u0438\u0433\u0438. \u0410\u043d\u0434\u0435\u0440\u0441\u0435\u043d \u0413.\u0425. \u0414\u044e\u0439\u043c\u043e\u0432\u043e\u0447\u043a\u0430<br><br>\n### Since the labels are in russian an explanation is needed here.<br><br>The first item is a DVD movie, all the other item are CD's and most of them are from the same company label.<br>We can see that our embedding layer gives minmizing distance of weights for items with the same or related categories(see below picture of item 0 and some close item related).\n\n### *Note:<br> 1. We took only 1000 items since this mapping took a long time to run <br> 2. We can do the same procedure for shops and other features of the model to see how are they corrolating*\n","402b92d8":"### Improved Categorical Embedding Model - version 1\n----","4d9d098d":"### *We got a RMSE score of : 1.09923*\n![image.png](attachment:image.png)","326a5114":"### We can see that months 11 and 23 which corrolates for month 'November'(in different years) has a peak in the sales.<br>We assume that because it is a russian sales company that during this time of year the new year and the holidays are coming so more people purchase items for gifts.","06358f55":"### Improved Categorical Embedding Model - version 0\n----","67c22965":"### *We got a RMSE score of : 1.49274*","ee17a0ac":"## Building Categorical Embedding Model\n----\n### In this chapter we are going to use categorical embeddings in order to improve our model.<br> We will build a NN model with Embeddings layers.","4ff69555":"## Loading the Data\n----","b5af1f63":"## Loading the Data & Data Exploration\n----\n### First we will start by giving the reader a basic knowledge about the data we are dealing with.<br>The following statements show how the data is stored in the given files by the kaggle competition:\n![image.png](attachment:image.png)\n### We can see that a-lot of features were given to us, this allows us to perform different approches to build our models.<br><br> Let's start by loading our data!","c3494755":"## Data preprocessing and preperation\n----","fa31bf66":"# Practical Deep Learning Workshop - Assignment 2 - Predict Future Sales\n----\n### *Roy Levy - 313577611 & Yuval Sabag - 205712151<br> Group_Name - BGU-DL-TheRegressors*\n\n----\n### In the following notebook we will represent few models for solving the problem of the kaggle [Predict Future Sales](https:\/\/www.kaggle.com\/c\/competitive-data-science-predict-future-sales\/overview) competition.<br><br>We are given a daily historical data about sales in several store.<br>Our goal is to predict on a test dataset what will be the amount of purchases of a product in every store.","99c32378":"# Using Feature Extraction \n---\n### In this section we will use feature extraction from our last model and we will try to use theses features in a classic ML algorithm","af507c80":"![image.png](attachment:image.png)","28d140af":"### *We got a RMSE score of : 1.52793*\n\n![image.png](attachment:image.png)\n<br> \n### *Note: We will show leaderboard only for best result since that's the way kaggle saves the results*","529242b1":"### We are going to add the following non-categorical features:\n1. total sales in previous month based on item_id.\n2. total sales in previous month based on shop_id.\n3. total sales in previous month.\n\n### this features will become handy in next chapter when we will try to improve our embedding model."}}