{"cell_type":{"10d05e33":"code","ed132385":"code","a5a38b37":"code","63ebc9ba":"code","451af2b6":"code","056860af":"code","ed22b716":"code","47ae6279":"code","6fc40ed6":"code","d6a31e5e":"code","aa549311":"code","6b4d8669":"code","2ade7275":"code","13b834f5":"code","914f6dcb":"code","ed403dd6":"code","b2206aa3":"code","5da976e2":"code","67d1220b":"code","5599f772":"code","6443edc1":"code","f9542940":"markdown","9e0b00e2":"markdown","52e608f6":"markdown","1db91bb3":"markdown","5599297d":"markdown","5c315cc7":"markdown"},"source":{"10d05e33":"# import library\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn import preprocessing, svm, model_selection, neighbors, ensemble\nfrom sklearn.tree import plot_tree\nfrom sklearn import tree\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier","ed132385":"# print file available\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a5a38b37":"# dataset read\ndf = pd.read_csv('\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')","63ebc9ba":"# print first lines dataset\ndf.head()","451af2b6":"# print info\ndf.info()","056860af":"# looking for correlation between variable\nsns.pairplot(df, hue=\"DEATH_EVENT\", palette=[\"g\", \"r\"])\nplt.title('Correlation graphs between variables');","ed22b716":"# looking for correlation between variable\nplt.figure(figsize=(12,12))\nsns.heatmap(data=df.corr(), square=True, annot=True, cmap=\"YlGnBu\");","47ae6279":"# target and feats selection\ntarget = df['DEATH_EVENT']\nfeats = df[['age', 'ejection_fraction', 'serum_creatinine', 'time']]\n\n# Separation of data sets for training\nX_train, X_test, y_train, y_test = train_test_split(feats, target, test_size=0.2, random_state=101)\n\n# Normalization\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","6fc40ed6":"# searching best parameters\n\nscore_minko = []\nscore_man = []\nscore_cheb = []\n\nfor k in range(1, 41):\n    knn = neighbors.KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    score_minko.append(knn.score(X_test, y_test))\n\nfor k in range(1, 41):\n    knn = neighbors.KNeighborsClassifier(n_neighbors=k, metric='manhattan')\n    knn.fit(X_train, y_train)\n    score_man.append(knn.score(X_test, y_test))\n    \nfor k in range(1, 41):\n    knn = neighbors.KNeighborsClassifier(n_neighbors=k, metric='chebyshev')\n    knn.fit(X_train, y_train)\n    score_cheb.append(knn.score(X_test, y_test))\n    \n\n# Plot parameters Accuracy vs K\nplt.figure(figsize=(16,8))\nplt.plot(range(1, 41), score_minko, color='blue', linestyle='dashed', lw=2, label='Minkowski')\nplt.plot(range(1, 41), score_man, color='orange', linestyle='dashed', lw=2, label='Manhattan')\nplt.plot(range(1, 41), score_cheb, color='red', linestyle='dashed', lw=2, label='Chebyshev')\nplt.title('Score - K value')  \nplt.xlabel('Value of K')  \nplt.ylabel('Accuracy') \nplt.legend();","d6a31e5e":"# Accuracy score in function of K\npd.DataFrame({'K_value' : range(1, 41),\n              'score_minko' : score_minko,\n              'score_man' : score_man,\n              'score_cheb' : score_cheb}).style.background_gradient(cmap='Greens').set_precision(3).highlight_max(axis=0, color='blue')","aa549311":"# Prediction of test features and creation of the confusion matrix\nknn = neighbors.KNeighborsClassifier(n_neighbors=19, metric='manhattan')\nknn.fit(X_train, y_train)\n\n# Confusion matrix\ny_pred = knn.predict(X_test)\npd.crosstab(y_test, y_pred, rownames=['Real class'], colnames=['Predicted class']).style.background_gradient(cmap='Blues')","6b4d8669":"# Creating a clf classifier without parameters\nclf = svm.SVC()\n\n# Algorithm training on the training set\nclf.fit(X_train, y_train)\n\n# Finding the best hyperparameters\nparametres = {'C':[0.1,1,10,50], 'kernel':['rbf','linear']}\ngrid_clf = model_selection.GridSearchCV(estimator=clf, param_grid=parametres)\ngrille = grid_clf.fit(X_train,y_train)\n\nprint(pd.DataFrame.from_dict(grille.cv_results_).loc[:,['params', 'mean_test_score']])\nprint('The model that provides the best prediction has for parameters :',grid_clf.best_params_)","2ade7275":"# Prediction of test features and creation of the confusion matrix\nclf = svm.SVC(C=50, kernel='linear')\nclf.fit(X_train, y_train)\n\n# Confusion matrix\ny_pred = clf.predict(X_test)\npd.crosstab(y_test, y_pred, rownames=['Real class'], colnames=['Predicted class']).style.background_gradient(cmap='Blues')","13b834f5":"# Creating a clf classifier without parameters\nclf = ensemble.RandomForestClassifier()\nclf.fit(X_train, y_train)\n\n# Finding the best hyperparameters\nparametres = {'max_features':[None, \"sqrt\", \"log2\"], 'min_samples_split':list(range(2, 32, 2))}\ngrid_clf = model_selection.GridSearchCV(estimator=clf, param_grid=parametres)\ngrille = grid_clf.fit(X_train,y_train)\n\nprint(pd.DataFrame.from_dict(grille.cv_results_).loc[:,['params', 'mean_test_score']])\nprint('The model that provides the best prediction has for parameters :',grid_clf.best_params_)","914f6dcb":"# Prediction of test features and creation of the confusion matrix\nclf = ensemble.RandomForestClassifier(min_samples_split=20, max_features='log2')\nclf.fit(X_train, y_train)\n\n# Confusion matrix\ny_pred = clf.predict(X_test)\npd.crosstab(y_test, y_pred, rownames=['Real class'], colnames=['Predicted class']).style.background_gradient(cmap='Blues')","ed403dd6":"# features importance\nfeatures_importance = {}\n\nfor feature, importance in zip(feats.columns, clf.feature_importances_):\n    features_importance[feature] = importance\n    \nimportances = pd.DataFrame.from_dict(features_importance, orient='index').rename(columns={0:'importance'})\nimportances = importances.sort_values(by='importance', ascending=False)\n\nplt.figure(figsize=(16,8))\nplt.title('Importance features for Random Forest Model')\nsns.barplot(data=importances, y=importances.index.to_list(), x='importance');","b2206aa3":"# tree Random Forest display\nfig = plt.figure(figsize=(15, 10))\nplot_tree(clf.estimators_[0], \n          feature_names=feats.columns.to_list(),\n          class_names=None, \n          filled=True, impurity=True, \n          rounded=True);","5da976e2":"# Creating a clf classifier without parameters\ndt_clf = DecisionTreeClassifier()\ndt_clf.fit(X_train, y_train)\n\n# Finding the best hyperparameters\nparametres = {'criterion':[\"gini\", \"entropy\"], 'max_depth':list(range(2, 32, 2))}\n\ngrid_dt_clf = model_selection.GridSearchCV(estimator=dt_clf, param_grid=parametres)\n\ngrille = grid_dt_clf.fit(X_train,y_train)\n\nprint(pd.DataFrame.from_dict(grille.cv_results_).loc[:,['params', 'mean_test_score']])\nprint('The model that provides the best prediction has for parameters :',grid_dt_clf.best_params_)","67d1220b":"# Prediction of test features and creation of the confusion matrix\ndt_clf = DecisionTreeClassifier(criterion='gini', max_depth=2)\ndt_clf.fit(X_train, y_train)\n\n# Confusion matrix\ny_pred = dt_clf.predict(X_test)\npd.crosstab(y_test, y_pred, rownames=['Real class'], colnames=['Predicted class']).style.background_gradient(cmap='Blues')","5599f772":"# features importance\nfeatures_importance = {}\n\nfor feature, importance in zip(feats.columns, dt_clf.feature_importances_):\n    features_importance[feature] = importance\n    \nimportances = pd.DataFrame.from_dict(features_importance, orient='index').rename(columns={0:'importance'})\nimportances = importances.sort_values(by='importance', ascending=False)\n\nplt.figure(figsize=(16,8))\nplt.title('Importance features for Decision Tree Model')\nsns.barplot(data=importances, y=importances.index.to_list(), x='importance');","6443edc1":"# tree Decision Tree display\nplt.figure(figsize=(16,8))\ntree.plot_tree(dt_clf);","f9542940":"# Preprocessing","9e0b00e2":"# Decision Tree model","52e608f6":"# KNN model","1db91bb3":"# Random Forest model","5599297d":"# Datas load and vizualisation","5c315cc7":"# SVM model"}}