{"cell_type":{"7d959f80":"code","4cbf66ab":"code","a3e50c68":"code","11b5f28f":"code","6e0ca5d0":"code","094553f8":"code","aa9a4071":"code","5e46619a":"code","8c3af277":"code","5b9093d8":"code","fb075646":"code","5d211109":"code","8024a0dd":"code","14218195":"code","6a702caa":"code","5de855b2":"code","13749bba":"code","11bdafee":"code","a30775cf":"code","0e61f254":"code","42fa8672":"code","95232a2b":"code","30221685":"code","bf9655d9":"markdown","25f5d0d2":"markdown","abb4bbd8":"markdown","ab2710b8":"markdown","54c1a28a":"markdown","a1baeb20":"markdown","be775dda":"markdown","7082ba8c":"markdown"},"source":{"7d959f80":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, precision_recall_curve, confusion_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import svm\n\nimport re\nfrom wordcloud import WordCloud\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nlemma = WordNetLemmatizer()\n\nfrom keras.models import Model\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Dense, Bidirectional, LSTM, Dropout, BatchNormalization\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence","4cbf66ab":"embeddings_index = dict()\nf = open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.50d.txt')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()","a3e50c68":"raw_data = pd.read_csv('..\/input\/sms-spam-collection-dataset\/spam.csv',delimiter=',',encoding='latin-1')\nraw_data.head()","11b5f28f":"raw_data.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'],axis=1,inplace=True)\nle = LabelEncoder()\nraw_data.v1 = le.fit_transform(raw_data.v1)\nraw_data.head()","6e0ca5d0":"num_spam = raw_data.v1.sum()\nnum_ham = len(raw_data) - num_spam\n\nplt.pie([num_ham, num_spam],labels=[\"Ham\", \"Spam\"],explode=(0,0.2),autopct='%1.2f%%',startangle=45)\nplt.show()","094553f8":"total_stopwords = set([word.replace(\"'\",'') for word in stopwords.words('english')])\n\ndef preprocess_text(text):\n    text = text.lower()\n    text = text.replace(\"'\",'')\n    text = re.sub('[^a-zA-Z]',' ',text)\n    words = text.split()\n    words = [lemma.lemmatize(word) for word in words if (word not in total_stopwords) and (len(word)>1)] # Remove stop words\n    text = \" \".join(words)\n    return text","aa9a4071":"raw_data.v2 = raw_data.v2.apply(preprocess_text)","5e46619a":"wordcloud = WordCloud(height=2000, width=2000, stopwords=set(stopwords.words('english')), background_color='white')\nwordcloud = wordcloud.generate(' '.join(raw_data[raw_data.v1==1].v2))\nplt.imshow(wordcloud)\nplt.title(\"Most common words in spam SMS\")\nplt.axis('off')\nplt.show()","8c3af277":"x_train, x_test, y_train, y_test = train_test_split(raw_data.v2, raw_data.v1, test_size=0.15, stratify=raw_data.v1)","5b9093d8":"max_words = x_train.apply(lambda str: len(str.split())).max()","fb075646":"tok = Tokenizer()\ntok.fit_on_texts(x_train)\nsequences = tok.texts_to_sequences(x_train)\nsequences_matrix = sequence.pad_sequences(sequences, maxlen=max_words, padding='post')\nvocab_size = len(tok.word_index) + 1","5d211109":"embedding_matrix = np.zeros((vocab_size, 50))\nfor word, i in tok.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","8024a0dd":"model_glove = Sequential()\nmodel_glove.add(Embedding(vocab_size, 50, input_length=max_words, weights=[embedding_matrix], trainable=True))\nmodel_glove.add(Bidirectional(LSTM(20, return_sequences=True)))\nmodel_glove.add(Dropout(0.2))\nmodel_glove.add(BatchNormalization())\nmodel_glove.add(Bidirectional(LSTM(20, return_sequences=True)))\nmodel_glove.add(Dropout(0.2))\nmodel_glove.add(BatchNormalization())\nmodel_glove.add(Bidirectional(LSTM(20)))\nmodel_glove.add(Dropout(0.2))\nmodel_glove.add(BatchNormalization())\nmodel_glove.add(Dense(64, activation='relu'))\nmodel_glove.add(Dense(64, activation='relu'))\nmodel_glove.add(Dense(1, activation='sigmoid'))\nmodel_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","14218195":"model_glove.fit(sequences_matrix, y_train, epochs = 10)","6a702caa":"test_sequences = tok.texts_to_sequences(x_test)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_words)","5de855b2":"y_pred = model_glove.predict(test_sequences_matrix)","13749bba":"pr, rc, thresholds = precision_recall_curve(y_test, y_pred)\nplt.plot(thresholds, pr[1:])\nplt.plot(thresholds, rc[1:])\nplt.show()\ncrossover_index = np.max(np.where(pr <= rc))\ncrossover_cutoff = thresholds[crossover_index]\ncrossover_recall = rc[crossover_index]\n\nprint(classification_report(y_test, y_pred > crossover_cutoff))\n\nm_confusion_test = confusion_matrix(y_test, y_pred > crossover_cutoff)\ndisplay(pd.DataFrame(data = m_confusion_test, columns = ['Predicted 0', 'Predicted 1'],\n            index = ['Actual 0', 'Actual 1']))\n\nprint(\"Terrible! This model misses all the spam SMS.\")","11bdafee":"vectorizer = TfidfVectorizer()\nvectorizer.fit(x_train)","a30775cf":"x_train_vec = vectorizer.transform(x_train).toarray()","0e61f254":"model_tdif = svm.SVC(gamma='scale')\nmodel_tdif.fit(x_train_vec, y_train)","42fa8672":"x_test_vec = vectorizer.transform(x_test).toarray()","95232a2b":"y_pred_tdif = model_tdif.predict(x_test_vec)","30221685":"print(classification_report(y_test, y_pred_tdif))\n\nm_confusion_test = confusion_matrix(y_test, y_pred_tdif)\ndisplay(pd.DataFrame(data = m_confusion_test, columns = ['Predicted 0', 'Predicted 1'],\n            index = ['Actual 0', 'Actual 1']))\n\nprint(\"This model misclassifies {c} genuine SMS as spam and misses only {d} SPAM.\".format(c = m_confusion_test[0,1], d = m_confusion_test[1,0]))","bf9655d9":"We have an unbalanced dataset. Only 13.41% of the data is spam. So, let's use recall as a metric to evaluate our model.","25f5d0d2":"With a significantly higher recall value and fewer false positives, the TF-IDF based vectorization is a more superior choice as compared to word embeddings. Since the corpus has a lot of shorthand and mispelt words, the word embeddings rendered do not hold any analogical meanings (the main reason why we use embeddings).","abb4bbd8":"# Preprocessing","ab2710b8":"# Word Embedding approach\nWe use a pre-trained GloVe embedding because we have a very small training set. This will serve as a starting point for our trainable embedding layer.","54c1a28a":"# TF-IDF approach","a1baeb20":"We are going to try lemmatization and stopword removal. However, conventional processing techniques are not going to work well with a SMS corpus. The texts have a lot of shortened words and abbreviations. Ideally, we have to implement a customized normalization of text.","be775dda":"# Spam or Ham ?\n* We will see two approaches to vectorize text - word embedding and TF-IDF\n* We will transfer learning from the GloVe word embeddings dataset.\n* We use recall to evaluate the model because the dataset is highly unbalanced.\n* We also see the False Positives because we do not want genuine SMS's to be classified as spam.\n* TF-IDF works supremely better than word embeddings because the texts have a lot of shorthand and misspelt words.","7082ba8c":"Taking a look at the most common words in spam SMS. We also observe a lot of shorthand which is going to mislead our classifiers."}}