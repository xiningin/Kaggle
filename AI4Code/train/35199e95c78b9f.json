{"cell_type":{"1d495dce":"code","db46f81d":"code","868fc162":"code","a7ac19e4":"code","b66861c0":"code","2e6dc03f":"code","bd5981c0":"code","72a339d3":"code","3123bceb":"code","81bce084":"code","73e5bf90":"code","be9437a3":"code","f515d21b":"code","2006f485":"code","4227a29a":"code","77abde63":"code","864f7554":"code","52ff97df":"code","298427fa":"code","0944ce5a":"code","e9e5e442":"code","7c7562e4":"code","caa0d17b":"code","c14720fc":"code","04a32c3b":"code","dee29ab3":"code","ee2c7ac5":"code","064ffff3":"code","994bc632":"code","498d1da2":"code","af1a915b":"code","e72572de":"code","027049a1":"code","32c58961":"code","5e72fad7":"code","e9346424":"code","be637733":"code","85c7d886":"code","57520377":"code","85aad910":"code","2e316058":"code","f97f51e3":"code","66f16514":"code","a0f21b07":"code","c6d053d3":"code","10c1826d":"code","e642774f":"code","15e14f32":"code","b3b5f1e5":"code","e45d52e5":"markdown","11079d57":"markdown","c0cf923a":"markdown","6d3ef9e0":"markdown","63cdb5b0":"markdown","506838ac":"markdown","efda6250":"markdown"},"source":{"1d495dce":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport re\nimport string\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nfrom numpy import array\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Flatten\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing.text import Tokenizer\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","db46f81d":"#Get character encoding of the files\nfile = [\"..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv\", \"..\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv\"]\nfor single in file:\n    with open(single) as f:\n        print(f.encoding)","868fc162":"train = pd.read_csv('\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv', encoding='latin-1')\ntest = pd.read_csv('\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv', encoding='latin-1')","a7ac19e4":"#Get summary of the training data\ntrain.info()","b66861c0":"#display first rows of the training dataset\ntrain.head()","2e6dc03f":"#Get summary of the testing data\ntest.info()","bd5981c0":"#display first rows of the testing dataset\ntest.head()","72a339d3":"#combine the training and testing data t\nframes = [train, test]\ndata = pd.concat(frames)\ndata.info()","3123bceb":"#check for missing data \nmissing = data['Location'].isnull()\n\n\nprint(data[missing])","81bce084":"#replace missing data \ndata['Location'] = data['Location'].replace(np.nan, \"Unknown\")\n#test['Location'] = test['Location'].replace(np.nan, \"Unknown\")","73e5bf90":"#check again for missing data\ndata[data['Location'].isnull()]","be9437a3":"#check for duplicates in the data\ndups = data.duplicated()\ndata[dups]","f515d21b":"# Check for Locations distribution\nloc_dist = data['Location'].unique()\nprint(len(loc_dist))","2006f485":"#summary of the whole data\ndata.info()","4227a29a":"#sentiment Distribution","77abde63":"#plot total negative vs total positive\npos  = len(data[data['Sentiment'] == 'Positive'])\next_pos = len(data[data['Sentiment'] == 'Extremely Positive'])\n\nneut =len(data[data['Sentiment'] == 'Neutral'])\n\nneg = len(data[data['Sentiment'] == 'Negative'])\next_neg = len(data[data['Sentiment'] == 'Extremely Negative'])\n\ntotal_positive = pos + ext_pos\ntotal_negative = neg + ext_neg\n\ntt_label = [\"Total Positive\", \"Neutral\", \"Total Negative\"]\ntt = [total_positive, neut, total_negative]","864f7554":"plt.bar(tt_label, tt)\nplt.show()","52ff97df":"#plot\nplt.pie(tt, labels=tt_label, autopct='%1.1f%%')\nplt.show()","298427fa":"sentiment_count = data['Sentiment'].value_counts()\nsentiment_count","0944ce5a":"#plot\nplt.bar(sentiment_count.index,sentiment_count)\nplt.xticks(rotation=90)\nplt.show()","e9e5e442":"#plot\nplt.pie(sentiment_count, labels=sentiment_count.index, autopct='%1.1f%%')\n#plt.legend('upper right')\nplt.show()","7c7562e4":"#time series - start of date and end date - line plot frequency of positive and negative\nday_total = data['TweetAt'].unique()\nprint(day_total)","caa0d17b":"#vectorizer:\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')\ntrain_matrix = vectorizer.fit_transform(train['OriginalTweet'])\ntest_matrix = vectorizer.transform(test['OriginalTweet'])","c14720fc":"X_train1 = train_matrix\nX_test1 = test_matrix\n#y_train = train['sentiment']\n#y_test = test['sentiment']","04a32c3b":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n\nlab = train['Sentiment']\ny_train1 = le.fit_transform(lab)\ny_test1 = le.fit_transform(test['Sentiment'])","dee29ab3":"#The model\nlr = LogisticRegression(max_iter=10000)","ee2c7ac5":"#Training the model\nlr.fit(X_train1,y_train1)","064ffff3":"#Make predictions\npredictions1 = lr.predict(X_test1)\n","994bc632":"print(predictions1[:10])","498d1da2":"#Show the results of the predictions\nlab_names = test['Sentiment'].unique()\nlab_names[predictions1[:10]]","af1a915b":"# find accuracy, precision, recall:\nfrom sklearn.metrics import confusion_matrix,classification_report\nnew = np.asarray(y_test1)\nconfusion_matrix(predictions1,y_test1)","e72572de":"print(classification_report(predictions1,y_test1))","027049a1":"#Get Text data from the Tweet Column\ncorpus = data['OriginalTweet']\ncorpus","32c58961":"#One-Hot Encoding of the labels\nsentiment = pd.get_dummies(data['Sentiment'])\nprint(sentiment)","5e72fad7":"print(len(corpus))\nprint(len(sentiment))","e9346424":"#Words Tokenization\nfrom nltk.tokenize import word_tokenize\n\nall_words = []\nfor sent in corpus:\n    tokenize_word = word_tokenize(sent)\n    for word in tokenize_word:\n        all_words.append(word)","be637733":"#Extract each word while ignoring duplicates\nunique_words = set(all_words)\nprint(len(unique_words))","85c7d886":"\nvocab_length = 101948\nembedded_sentences = [one_hot(sent, vocab_length) for sent in corpus]\n#print(embedded_sentences )","57520377":"#count number of words\nword_count = lambda sentence: len(word_tokenize(sentence))\nlongest_sentence = max(corpus, key=word_count)\nlength_long_sentence = len(word_tokenize(longest_sentence))","85aad910":"#Fill the end of each sentence with '0' so that they all have same lenght\npadded_sentences = pad_sequences(embedded_sentences, length_long_sentence, padding='post')\nprint(padded_sentences)","2e316058":"len(padded_sentences)","f97f51e3":"#divide the data into Training and Testing\n\nX_train,X_test, y_train, y_test = train_test_split(padded_sentences, sentiment, train_size=0.9, random_state=42)","66f16514":"#Build the Model \nmodel = Sequential()\nmodel.add(Embedding(vocab_length, 20, input_length=length_long_sentence))\nmodel.add(LSTM(20, return_sequences=True))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Flatten())\nmodel.add(Dense(5, activation='softmax'))","a0f21b07":"#compile model and show summary\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\nprint(model.summary())","c6d053d3":"#train the model\n\nmodel.fit(X_train, y_train, epochs=15, steps_per_epoch=200, verbose=1)","10c1826d":"#Evaluate model performance\nloss, accuracy = model.evaluate(padded_sentences, sentiment, verbose=0)\nprint('Accuracy: %f' % (accuracy*100))","e642774f":"#make predictions\npredictions = model.predict(X_test)","15e14f32":"#Store Predictions result\npred_result = (np.argmax(predictions[:20], axis=1))","b3b5f1e5":"#Show result of predictions\nprint(lab_names[pred_result])","e45d52e5":"**Logistic Regresson**","11079d57":"**LOAD DATA**","c0cf923a":"The charts above gives the summary of the whole data categorized into **3 classes**. \n\nThe **positive** column comprises of the *Positive* and *Extremely positive* sentiments which acounts for **43.6%** of the total data. \n\nThe **Negative** column, with **37.9%**, represents the total of the *Negative* and *Extremely Negative* sentiments, while the **Neutral** sentiments representing **18.5%** of the total data.","6d3ef9e0":"# MODELS","63cdb5b0":"**LSTM**","506838ac":"The plots above shows that Positive sentiments has the highest count at 27.5%, closely followed by the Negative sentiments data with 24.4% of the whole data. Neutral opininions occupy the center with about 18.5% of the records. Extremely Positive and Extremely Negative sentiments have values of 16.1% and 13.5% respectively.","efda6250":"**EDA + VISUALIZATIONS**"}}