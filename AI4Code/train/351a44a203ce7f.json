{"cell_type":{"bee752a3":"code","60265b60":"code","e0bf1a92":"code","73eeea79":"code","7960a3fd":"code","72a57c80":"code","ef7fad89":"code","843889e7":"code","87244548":"code","5319e9ae":"code","4d695a18":"code","350f1c37":"code","b0738549":"code","f79be2f9":"code","3e32c3fb":"code","f007f259":"code","be528fe5":"code","1b78f650":"code","bbafaed9":"code","080530f8":"code","240a159f":"code","0fefa613":"code","653c3e64":"code","6477cff6":"code","f3b6631e":"code","2c059452":"code","8f7fe6c3":"code","12a00dce":"code","d546658a":"code","7454842e":"code","16033d02":"code","785926b9":"code","92926598":"code","a09d4736":"code","ac6554ee":"code","d5e780f0":"code","7c7d470b":"code","b4433a4b":"code","e052cd9b":"code","14ed6435":"code","c3810d37":"code","cfdf0ed0":"code","f35b8ca5":"code","77e4432d":"code","9e92229b":"code","98c8c535":"code","e6b939e0":"code","0d6e220c":"code","97d3bb30":"code","579c2ddf":"code","360f080c":"code","07d0418a":"code","f238e0f6":"code","d704ae01":"code","5c5f0bf4":"code","91ed5ff8":"code","03cb2e47":"code","9b1d1d27":"code","cd8c5455":"code","d3273f36":"code","f660c893":"code","eae7cd70":"code","74d24c60":"code","a1732537":"code","221eb2e8":"code","dadc30e5":"code","bec4be82":"code","800a8f4e":"code","f1ed4dae":"code","17ddeedb":"code","eb594c97":"code","7816e580":"code","3d8c090d":"code","3cedd346":"code","38931962":"code","677047aa":"code","b1cfe835":"code","b922dd51":"code","a8b11693":"code","fd055cfc":"code","2f7ec2e9":"code","bc8d9199":"code","6e416927":"code","c858b432":"code","575b6a93":"code","82581b5b":"code","c08c0843":"code","1a188724":"code","b7a75ad5":"code","90c0e5e0":"code","5725e9bf":"code","c713a5cd":"code","05bd6a15":"code","f7ba095b":"code","686fb2ab":"code","3d58e421":"code","b12f117c":"code","40847779":"code","47a3a91d":"code","7fef8e05":"code","4ebbed92":"code","7982360b":"code","412701e4":"markdown","2e09f27a":"markdown","d8357b82":"markdown","65a15e83":"markdown","98d736ad":"markdown","c581f1d7":"markdown","fd9c2a35":"markdown","b1e3255b":"markdown","031b4cbb":"markdown","3281583c":"markdown","606b3528":"markdown","90aefb54":"markdown","bad32a53":"markdown","7f34cc52":"markdown","30ff0241":"markdown","b74af74b":"markdown","9001fb80":"markdown","18c3371f":"markdown","26b6de8f":"markdown","47876be9":"markdown","f0f8c288":"markdown","b43df7f5":"markdown","d006cba2":"markdown","fcc9e2d4":"markdown","7ea53adc":"markdown","0c8f3dae":"markdown","9e95f35d":"markdown","f810147a":"markdown","9d908d88":"markdown","25ad572f":"markdown"},"source":{"bee752a3":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nimport os\nimport random\nrandom.seed(1234)\nimport gc\nimport os\nimport re\nfrom itertools import product\nimport pickle\nimport pandas as pd\nimport numpy as np\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\nimport matplotlib.pylab as plt\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4\nimport warnings\nwarnings.filterwarnings('ignore')","60265b60":"os.listdir(\"..\/input\")","e0bf1a92":"%pwd","73eeea79":"\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7960a3fd":"items = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/items.csv\")\ncats = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")\nshops = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/shops.csv\")\ntrain = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\ntest = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/test.csv\")","72a57c80":"train.isnull().any()","ef7fad89":"plt.figure(figsize = (10,4))\nplt.xlim(-100, 3000)\nsns.boxplot( x= train.item_cnt_day )\nplt.figure( figsize = (10,4) )\nplt.xlim(train.item_price.min(), train.item_price.max())\nsns.boxplot( x = train.item_price )\nplt.show()","843889e7":"print(train.shape[0])\nprint(len( train[train.item_cnt_day >999 ] ))\nprint( len(train[ train.item_cnt_day > 500  ]) )\nprint(len(train[train.item_price >100000 ]))\nprint(train.shape[0])","87244548":"train = train[(train.item_price < 100000 )& (train.item_cnt_day < 1000)]","5319e9ae":"plt.figure(figsize = (10,4))\nplt.xlim( -100, 1000 )\nsns.boxplot( x= train.item_cnt_day )\nplt.figure( figsize = (10,3) )\nplt.xlim( train.item_price.min(), train.item_price.max()*1.1 )\nsns.boxplot( x = train.item_price )\nplt.show()\n","4d695a18":"print(len( train[train.item_price < 0 ] ))\n# delete that petty 1 row\n\nprint(len( train[train.item_cnt_day < 1 ] ))\n# restore value with 0 instead of deleting","350f1c37":"train = train[train.item_price > 0].reset_index(drop = True)\ntrain.loc[train.item_cnt_day < 1, \"item_cnt_day\"] = 0","b0738549":"ts=train.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\nts.astype('float')\nplt.figure(figsize=(16,8))\nplt.title('Total Sales of the company')\nplt.xlabel('Time')\nplt.ylabel('Sales')\nplt.plot(ts);","f79be2f9":"plt.figure(figsize=(16,6))\nplt.plot(ts.rolling(window=12,center=False).mean(),label='Rolling Mean');\nplt.plot(ts.rolling(window=12,center=False).std(),label='Rolling sd');\nplt.legend();","3e32c3fb":"shops","f007f259":"train.loc[train.shop_id == 0, \"shop_id\"] = 57\ntest.loc[test.shop_id == 0 , \"shop_id\"] = 57\ntrain.loc[train.shop_id == 1, \"shop_id\"] = 58\ntest.loc[test.shop_id == 1 , \"shop_id\"] = 58\ntrain.loc[train.shop_id == 11, \"shop_id\"] = 10\ntest.loc[test.shop_id == 11, \"shop_id\"] = 10\ntrain.loc[train.shop_id == 40, \"shop_id\"] = 39\ntest.loc[test.shop_id == 40, \"shop_id\"] = 39","be528fe5":"# renaming\nshops.loc[ shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"',\"shop_name\" ] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\n# split the column into 2 subcategories\nshops[\"city\"] = shops.shop_name.str.split(\" \").map( lambda x: x[0] )\nshops[\"category\"] = shops.shop_name.str.split(\" \").map( lambda x: x[1] )\nshops.loc[shops.city == \"!\u042f\u043a\u0443\u0442\u0441\u043a\", \"city\"] = \"\u042f\u043a\u0443\u0442\u0441\u043a\"\n","1b78f650":"category = []\nfor cat in shops.category.unique():\n    print(cat, len(shops[shops.category == cat]) )\n    if len(shops[shops.category == cat]) > 4:\n        category.append(cat)","bbafaed9":"# keep the name if the unique value is greater than 4, otherwise rename the rest of them as \"etc\"\nshops.category = shops.category.apply( lambda x: x if (x in category) else \"etc\" )","080530f8":"# now see the unique value\nfor cat in shops.category.unique():\n    print(cat, len(shops[shops.category == cat]) )","240a159f":"# Labelling the unique names\n\nshops[\"shop_category\"] = LabelEncoder().fit_transform( shops.category )\nshops[\"shop_city\"] = LabelEncoder().fit_transform( shops.city )","0fefa613":"shops = shops[[\"shop_id\", \"shop_category\", \"shop_city\"]]","653c3e64":"shops.head()","6477cff6":"cats.head()","f3b6631e":"# rename the names which seems familiar\ncats[\"type_code\"] = cats.item_category_name.apply( lambda x: x.split(\" \")[0] ).astype(str)\ncats.loc[ (cats.type_code == \"\u0418\u0433\u0440\u043e\u0432\u044b\u0435\")| (cats.type_code == \"\u0410\u043a\u0441\u0435\u0441\u0441\u0443\u0430\u0440\u044b\"), \"category\" ] = \"\u0418\u0433\u0440\u044b\"","2c059452":"cats.shape","8f7fe6c3":"category = []\nfor cat in cats.type_code.unique():\n    print(cat, len(cats[cats.type_code == cat]))\n    if len(cats[cats.type_code == cat]) > 4: \n        category.append( cat )","12a00dce":"cats.type_code = cats.type_code.apply(lambda x: x if (x in category) else \"etc\")","d546658a":"for cat in cats.type_code.unique():\n    print(cat, len(cats[cats.type_code == cat]))","7454842e":"cats.type_code = LabelEncoder().fit_transform(cats.type_code)\n","16033d02":"\ncats[\"split\"] = cats.item_category_name.apply(lambda x: x.split(\"-\"))\n# if there is something after '-' strip it, otherwise strip the remaining first chunk of words\ncats[\"subtype\"] = cats.split.apply(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ncats.subtype","785926b9":"cats[\"subtype_code\"] = LabelEncoder().fit_transform( cats[\"subtype\"] )\ncats.subtype_code\ncats = cats[[\"item_category_id\", \"type_code\", \"subtype_code\"]]","92926598":"cats.head()","a09d4736":"\ndef name_correction(x):\n    x = x.lower()\n    x = x.partition('[')[0]\n    x = x.partition('(')[0]\n    x = re.sub('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', ' ', x)\n    x = x.replace('  ', ' ')\n    x = x.strip()\n    return x","ac6554ee":"items.item_name","d5e780f0":"# include item_name in items dataframe\nitems[\"name1\"], items[\"name2\"] = items.item_name.str.split(\"[\", 1).str\nitems[\"name1\"], items[\"name3\"] = items.item_name.str.split(\"(\", 1).str\n\nitems[\"name2\"] = items.name2.str.replace('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', \" \").str.lower()\nitems[\"name3\"] = items.name3.str.replace('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', \" \").str.lower()\nitems = items.fillna('0')\n\nitems[\"item_name\"] = items[\"item_name\"].apply(lambda x: name_correction(x))\n# exclude the last letter if it's not zero to remove square bracket.\nitems.name2 = items.name2.apply( lambda x: x[:-1] if x !=\"0\" else \"0\")","7c7d470b":"items.head()","b4433a4b":"items.name2.value_counts()","e052cd9b":"# I'm limiting the range of xbox into 8 letters.\nitems[\"type\"] = items.name2.apply(lambda x: x[0:8] if x.split(\" \")[0] == \"xbox\" else x.split(\" \")[0] )","14ed6435":"items.type.value_counts()","c3810d37":"items.loc[(items.type == \"x360\") | (items.type == \"xbox360\") | \n          (items.type == \"xbox 360\") ,\"type\"] = \"xbox 360\"\nitems.loc[ items.type == \"\", \"type\"] = \"mac\"\nitems.type = items.type.apply( lambda x: x.replace(\" \", \"\") )\nitems.loc[ (items.type == 'pc' )| (items.type == 'p\u0441') | (items.type == \"pc\"), \"type\" ] = \"pc\"\nitems.loc[ items.type == '\u0440s3' , \"type\"] = \"ps3\"\n","cfdf0ed0":"items.head()","f35b8ca5":"group_sum = items.groupby([\"type\"]).agg({\"item_id\": \"count\"})\ngroup_sum = group_sum.reset_index()\n\nprint(group_sum)","77e4432d":"drop_cols = []\nfor cat in group_sum.type.unique():\n#     print(group_sum.loc[(group_sum.type == cat), \"item_id\"].values[0])\n    if group_sum.loc[(group_sum.type == cat), \"item_id\"].values[0] <40:\n        drop_cols.append(cat)","9e92229b":"drop_cols","98c8c535":"items.head()","e6b939e0":"items.shape","0d6e220c":"items.name1.value_counts()","97d3bb30":"items.name1.value_counts()","579c2ddf":"items.name2.value_counts()","360f080c":"items.name3.value_counts()","07d0418a":"items.name2 = items.name2.apply( lambda x: \"etc\" if (x in drop_cols) else x )\nitems = items.drop([\"type\"], axis = 1)","f238e0f6":"\nitems.name2 = LabelEncoder().fit_transform(items.name2)\nitems.name3 = LabelEncoder().fit_transform(items.name3)\n\nitems.drop([\"item_name\", \"name1\"],axis = 1, inplace= True)\nitems.head()","d704ae01":"items.describe()","5c5f0bf4":"train['date_block_num'].nunique()","91ed5ff8":"%%time\n# product acts like nested for loop.\nmatrix = []\n\n# id doesn't mean anything, so I'm gonna put the unique shop and product of a particular date like 2.1.2013\ncols  = [\"date_block_num\", \"shop_id\", \"item_id\"]\ncols  = [\"date_block_num\", \"shop_id\", \"item_id\"]\nfor i in range(34):\n    sales = train[train.date_block_num == i]\n    matrix.append( np.array(list( product( [i], sales.shop_id.unique(), sales.item_id.unique() ) ), dtype = np.int16) )\n\nmatrix = pd.DataFrame( np.vstack(matrix), columns = cols )\nmatrix[\"date_block_num\"] = matrix[\"date_block_num\"].astype(np.int8)\nmatrix[\"shop_id\"] = matrix[\"shop_id\"].astype(np.int8)\nmatrix[\"item_id\"] = matrix[\"item_id\"].astype(np.int16)\nmatrix.sort_values( cols, inplace = True )","03cb2e47":"matrix.head()","9b1d1d27":"train[\"revenue\"] = train[\"item_cnt_day\"] * train[\"item_price\"]","cd8c5455":"group = train.groupby( [\"date_block_num\", \"shop_id\", \"item_id\"] ).agg( {\"item_cnt_day\": [\"sum\"]} )\ngroup.head()","d3273f36":"group.columns = [\"item_cnt_month\"]\ngroup.reset_index( inplace = True)\ngroup.head()","f660c893":"group.isnull().sum()","eae7cd70":"%time\n\n# here \"cols\" reflects the three column stated before and those columns should be on the left side.\nmatrix = pd.merge( matrix, group, on = cols, how = \"left\" )\n# as mention in, the range of the output should be in between 0 and 20\nmatrix[\"item_cnt_month\"] = matrix[\"item_cnt_month\"].fillna(0).clip(0,20).astype(np.float16)\n","74d24c60":"matrix.head()","a1732537":"test[\"date_block_num\"] = 34\ntest[\"date_block_num\"] = test[\"date_block_num\"].astype(np.int8)\ntest[\"shop_id\"] = test.shop_id.astype(np.int8)\ntest[\"item_id\"] = test.item_id.astype(np.int16)\ntest.head()","221eb2e8":"matrix.head()","dadc30e5":"%%time\n\nmatrix = pd.concat([matrix, test.drop([\"ID\"],axis = 1)], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna( 0, inplace = True )\n","bec4be82":"matrix.head()","800a8f4e":"%%time\nmatrix = pd.merge( matrix, shops, on = [\"shop_id\"], how = \"left\" )\nmatrix = pd.merge(matrix, items, on = [\"item_id\"], how = \"left\")\nmatrix = pd.merge( matrix, cats, on = [\"item_category_id\"], how = \"left\" )\nmatrix[\"shop_city\"] = matrix[\"shop_city\"].astype(np.int8)\nmatrix[\"shop_category\"] = matrix[\"shop_category\"].astype(np.int8)\nmatrix[\"item_category_id\"] = matrix[\"item_category_id\"].astype(np.int8)\nmatrix[\"subtype_code\"] = matrix[\"subtype_code\"].astype(np.int8)\nmatrix[\"name2\"] = matrix[\"name2\"].astype(np.int8)\nmatrix[\"name3\"] = matrix[\"name3\"].astype(np.int16)\nmatrix[\"type_code\"] = matrix[\"type_code\"].astype(np.int8)\n","f1ed4dae":"matrix.head()","17ddeedb":"del items\ndel cats\ndel shops\n\ngc.collect()","eb594c97":"def lag_feature( df,lags, cols ):\n    for col in cols:\n        print(col)\n        tmp = df[[\"date_block_num\", \"shop_id\",\"item_id\",col ]]\n        for i in lags:\n            shifted = tmp.copy()\n            shifted.columns = [\"date_block_num\", \"shop_id\", \"item_id\", col + \"_lag_\"+str(i)]\n            shifted.date_block_num = shifted.date_block_num + i\n            df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","7816e580":"%%time\nmatrix = lag_feature( matrix, [1,2,3], [\"item_cnt_month\"] )","3d8c090d":"%%time\ngroup = matrix.groupby( [\"date_block_num\"] ).agg({\"item_cnt_month\" : [\"mean\"]})\ngroup.columns = [\"date_avg_item_cnt\"]\ngroup.reset_index(inplace = True)\n\nmatrix = pd.merge(matrix, group, on = [\"date_block_num\"], how = \"left\")\nmatrix.date_avg_item_cnt = matrix[\"date_avg_item_cnt\"].astype(np.float16)\nmatrix = lag_feature( matrix, [1], [\"date_avg_item_cnt\"] )\nmatrix.drop( [\"date_avg_item_cnt\"], axis = 1, inplace = True )","3cedd346":"%%time\ngroup = matrix.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix.date_item_avg_item_cnt = matrix['date_item_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3], ['date_item_avg_item_cnt'])\nmatrix.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)\n","38931962":"matrix.head()","677047aa":"%%time\ngroup = matrix.groupby( [\"date_block_num\",\"shop_id\"] ).agg({\"item_cnt_month\" : [\"mean\"]})\ngroup.columns = [\"date_shop_avg_item_cnt\"]\ngroup.reset_index(inplace = True)\n\nmatrix = pd.merge(matrix, group, on = [\"date_block_num\",\"shop_id\"], how = \"left\")\nmatrix.date_avg_item_cnt = matrix[\"date_shop_avg_item_cnt\"].astype(np.float16)\nmatrix = lag_feature( matrix, [1,2,3], [\"date_shop_avg_item_cnt\"] )\nmatrix.drop( [\"date_shop_avg_item_cnt\"], axis = 1, inplace = True )","b1cfe835":"%%time\ngroup = matrix.groupby( [\"date_block_num\",\"shop_id\",\"item_id\"] ).agg({\"item_cnt_month\" : [\"mean\"]})\ngroup.columns = [\"date_shop_item_avg_item_cnt\"]\ngroup.reset_index(inplace = True)\n\nmatrix = pd.merge(matrix, group, on = [\"date_block_num\",\"shop_id\",\"item_id\"], how = \"left\")\nmatrix.date_avg_item_cnt = matrix[\"date_shop_item_avg_item_cnt\"].astype(np.float16)\nmatrix = lag_feature( matrix, [1,2,3], [\"date_shop_item_avg_item_cnt\"] )\nmatrix.drop( [\"date_shop_item_avg_item_cnt\"], axis = 1, inplace = True )\n","b922dd51":"%%time\ngroup = matrix.groupby(['date_block_num', 'shop_id', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_subtype_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'subtype_code'], how='left')\nmatrix.date_shop_subtype_avg_item_cnt = matrix['date_shop_subtype_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], ['date_shop_subtype_avg_item_cnt'])\nmatrix.drop(['date_shop_subtype_avg_item_cnt'], axis=1, inplace=True)\n","a8b11693":"%%time\ngroup = matrix.groupby(['date_block_num', 'shop_city']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_city_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', \"shop_city\"], how='left')\nmatrix.date_city_avg_item_cnt = matrix['date_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], ['date_city_avg_item_cnt'])\nmatrix.drop(['date_city_avg_item_cnt'], axis=1, inplace=True)","fd055cfc":"%%time\ngroup = matrix.groupby(['date_block_num', 'item_id', 'shop_city']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'item_id', 'shop_city'], how='left')\nmatrix.date_item_city_avg_item_cnt = matrix['date_item_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], ['date_item_city_avg_item_cnt'])\nmatrix.drop(['date_item_city_avg_item_cnt'], axis=1, inplace=True)\n","2f7ec2e9":"%%time\ngroup = train.groupby( [\"item_id\"] ).agg({\"item_price\": [\"mean\"]})\ngroup.columns = [\"item_avg_item_price\"]\ngroup.reset_index(inplace = True)\n\nmatrix = matrix.merge( group, on = [\"item_id\"], how = \"left\" )\nmatrix[\"item_avg_item_price\"] = matrix.item_avg_item_price.astype(np.float16)\n\n\ngroup = train.groupby( [\"date_block_num\",\"item_id\"] ).agg( {\"item_price\": [\"mean\"]} )\ngroup.columns = [\"date_item_avg_item_price\"]\ngroup.reset_index(inplace = True)\n\nmatrix = matrix.merge(group, on = [\"date_block_num\",\"item_id\"], how = \"left\")\nmatrix[\"date_item_avg_item_price\"] = matrix.date_item_avg_item_price.astype(np.float16)\nlags = [1, 2, 3]\nmatrix = lag_feature( matrix, lags, [\"date_item_avg_item_price\"] )\nfor i in lags:\n    matrix[\"delta_price_lag_\" + str(i) ] = (matrix[\"date_item_avg_item_price_lag_\" + str(i)]- matrix[\"item_avg_item_price\"] )\/ matrix[\"item_avg_item_price\"]\n\ndef select_trends(row) :\n    for i in lags:\n        if row[\"delta_price_lag_\" + str(i)]:\n            return row[\"delta_price_lag_\" + str(i)]\n    return 0\n\nmatrix[\"delta_price_lag\"] = matrix.apply(select_trends, axis = 1)\nmatrix[\"delta_price_lag\"] = matrix.delta_price_lag.astype( np.float16 )\nmatrix[\"delta_price_lag\"].fillna( 0 ,inplace = True)\n\nfeatures_to_drop = [\"item_avg_item_price\", \"date_item_avg_item_price\"]\nfor i in lags:\n    features_to_drop.append(\"date_item_avg_item_price_lag_\" + str(i) )\n    features_to_drop.append(\"delta_price_lag_\" + str(i) )\nmatrix.drop(features_to_drop, axis = 1, inplace = True)","bc8d9199":"matrix.head()","6e416927":"%%time\ngroup = train.groupby( [\"date_block_num\",\"shop_id\"] ).agg({\"revenue\": [\"sum\"] })\ngroup.columns = [\"date_shop_revenue\"]\ngroup.reset_index(inplace = True)\n\nmatrix = matrix.merge( group , on = [\"date_block_num\", \"shop_id\"], how = \"left\" )\nmatrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n\ngroup = group.groupby([\"shop_id\"]).agg({ \"date_block_num\":[\"mean\"] })\ngroup.columns = [\"shop_avg_revenue\"]\ngroup.reset_index(inplace = True )\n\nmatrix = matrix.merge( group, on = [\"shop_id\"], how = \"left\" )\nmatrix[\"shop_avg_revenue\"] = matrix.shop_avg_revenue.astype(np.float32)\nmatrix[\"delta_revenue\"] = (matrix['date_shop_revenue'] - matrix['shop_avg_revenue']) \/ matrix['shop_avg_revenue']\nmatrix[\"delta_revenue\"] = matrix[\"delta_revenue\"]. astype(np.float32)\n\nmatrix = lag_feature(matrix, [1], [\"delta_revenue\"])\nmatrix[\"delta_revenue_lag_1\"] = matrix[\"delta_revenue_lag_1\"].astype(np.float32)\nmatrix.drop( [\"date_shop_revenue\", \"shop_avg_revenue\", \"delta_revenue\"] ,axis = 1, inplace = True)\n\n","c858b432":"matrix","575b6a93":"matrix.head().T","82581b5b":"matrix.describe()","c08c0843":"matrix[\"month\"] = matrix[\"date_block_num\"] % 12","1a188724":"days = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\nmatrix[\"days\"] = matrix[\"month\"].map(days).astype(np.int8)\n","b7a75ad5":"%%time\nmatrix[\"item_shop_first_sale\"] = matrix[\"date_block_num\"] - matrix.groupby([\"item_id\",\"shop_id\"])[\"date_block_num\"].transform('min')\nmatrix[\"item_first_sale\"] = matrix[\"date_block_num\"] - matrix.groupby([\"item_id\"])[\"date_block_num\"].transform('min')\n","90c0e5e0":"%%time\nmatrix = matrix[matrix[\"date_block_num\"] > 3]\n","5725e9bf":"matrix.head()","c713a5cd":"data = matrix.copy()\ndel matrix\ngc.collect()","05bd6a15":"data[data[\"date_block_num\"]==34].shape","f7ba095b":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)","686fb2ab":"X_train.head()","3d58e421":"Y_train.head()","b12f117c":"X_test.shape","40847779":"del data\ngc.collect();","47a3a91d":"%%time\n\nmodel = XGBRegressor(\n    max_depth=10,\n    n_estimators=1000,\n    min_child_weight=0.5, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    tree_method='gpu_hist', gpu_id=0,\n    seed=42)\n\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose= True, \n    early_stopping_rounds = 5)\n","7fef8e05":"X_test.shape","4ebbed92":"Y_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('xgb_submission.csv', index=False)\n\n# save predictions for an ensemble\npickle.dump(Y_pred, open('xgb_train.pickle', 'wb'))\npickle.dump(Y_test, open('xgb_test.pickle', 'wb'))","7982360b":"\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nplot_features(model, (10,14))","412701e4":"We have to deal with monthly item count, and the total month is 34.","2e09f27a":"There is too many unique value in number1(201611 out of 22170), so I will drop it.","d8357b82":"Price trend for the last 3 months.","65a15e83":"# SHOPS","98d736ad":"Find out missing data","c581f1d7":"Number of days in a month. There are no leap years.","fd9c2a35":"Quick observations: There is an obvious \"seasonality\" (Eg: peak sales around a time of year) and a decreasing \"Trend\".","b1e3255b":"# TREND FEATURES","031b4cbb":"There are items with strange prices and sales. After detailed exploration I decided to remove items with price > 100000 and sales > 1001 (1000 is ok).","3281583c":"Because of the using 3(highest) as lag value, I have to drop first 3 months. Also drop all the columns with this month calculated values (other words which can not be calcucated for the test set).","606b3528":"# Outliers","90aefb54":"To use time tricks append test pairs to the matrix. The plan is to get the result for the 34th month in case of test dataframe.","bad32a53":"Check out to know more about \"how\" and \"on\" here : https:\/\/jakevdp.github.io\/PythonDataScienceHandbook\/03.07-merge-and-join.html","7f34cc52":"Extreme Gradient Boost (XGBoost) is a powerful optimized decision tree-based grident boost algorithm, and it is possible to accelerate by GPU. Grident boost uses large numbers of relative small and weak decision trees to model the residues for each iteration. In each iteration, it gives a 'direction', then update the predictions and residues until stop criteria are reached.\n\n* Load Dataset and Setting\n* Model Hyperparameter Tuning and Training","30ff0241":"Validation strategy is 34th month for the test set, 33th month for the validation set and (4-33)th months for the train.","b74af74b":"# XGBoost","9001fb80":"Our memory is almost full, let's do some garbage collecting stuffs.","18c3371f":"Welcome to Predict Future Sales Challenge. In this kernel, I will focus on doing some illustrative data visualizations and then use XGBoost to predict November 2015's sale","26b6de8f":"I can count on them(name2 and name3).","47876be9":"# SALES TRAIN","f0f8c288":"# MEAN ENCODED FEATURES","b43df7f5":"* type_code is the unique chronology of names\n* subtype_code is the ","d006cba2":"# TIME SERIES LAG FEATURE","fcc9e2d4":"Assign the number of days according to the month","7ea53adc":"Aggregation usage in dataframe and series :\n* https:\/\/www.shanelynn.ie\/summarising-aggregation-and-grouping-data-in-python-pandas\/\n* https:\/\/towardsdatascience.com\/why-and-how-to-use-merge-with-pandas-in-python-548600f7e738","0c8f3dae":"add test dataframe with matrix(train dataframe)","9e95f35d":"why","f810147a":"# **ITEMS**","9d908d88":"# CATEGORIES","25ad572f":"Last month shop revenue trend."}}