{"cell_type":{"8b164c1e":"code","1ad380ad":"code","bc356d64":"code","efde2547":"code","fd1a9d79":"code","7ff356dc":"code","f2e93eeb":"code","9f57db68":"code","6b1b948b":"code","0f2d4840":"code","01c253a8":"code","a364e22e":"code","0e303043":"code","63b03ef5":"code","18f54679":"code","312ce42a":"code","023d5774":"code","aad37af5":"code","f7cf559a":"code","e77c4d6d":"code","17718835":"code","56539c2c":"code","55cba755":"code","d62ebb3f":"code","6c026fbf":"code","71eff389":"code","5834bb1b":"code","fb1200c8":"code","0d6a33da":"code","be54d381":"code","0d802e7f":"code","010fa6d5":"code","43f53646":"code","a240a813":"code","7c20fe1e":"markdown","a4a8cc00":"markdown","1271aadf":"markdown","32842c2a":"markdown","dd51f9a7":"markdown","69167143":"markdown","a53eeca4":"markdown","5af9c502":"markdown","53babbd6":"markdown"},"source":{"8b164c1e":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL.Image import fromarray","1ad380ad":"samples = ['..\/input\/anime-faces\/data\/10001.png',\n           '..\/input\/anime-faces\/data\/10004.png',\n           '..\/input\/anime-faces\/data\/10009.png',\n           '..\/input\/anime-faces\/data\/10021.png']","bc356d64":"plt.figure(figsize = (20,20))\n\ni = 0\n\nwhile i < 4 :\n    \n    img = cv2.cvtColor(cv2.imread(samples[i]), cv2.COLOR_BGR2RGB)\n    plt.subplot(2,2,i+1)\n    plt.imshow(img)\n    plt.axis('off')\n    i += 1\nplt.title('Anime faces')\nplt.show()","efde2547":"import os\nfrom keras.preprocessing.image import img_to_array","fd1a9d79":"path = '..\/input\/anime-faces\/data\/'","7ff356dc":"X = []\n\nfor img_path in os.listdir(path) :\n    if img_path[-3:]   ==  'png' :\n        X.append(img_to_array(fromarray(cv2.cvtColor(cv2.imread(os.path.join(path, img_path)), cv2.COLOR_BGR2RGB))))\nX = np.array(X)","f2e93eeb":"print(X.shape)","9f57db68":"X = (X-127.5)\/127.5","6b1b948b":"import tensorflow as tf","0f2d4840":"BUFFER_SIZE = 1000\nBATCH_SIZE  = 128\nNOISE_DIM   = 100","01c253a8":"train_data = tf.data.Dataset.from_tensor_slices(X).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)","a364e22e":"for trainX in train_data.take(1) :\n    pass","0e303043":"print(trainX.shape)","63b03ef5":"plt.figure(figsize = (20,20))\ni = 0\n\nwhile i < 16 :\n    \n    plt.subplot(4,4, i+1)\n    plt.imshow(trainX[i])\n    plt.axis('off')\n    i += 1","18f54679":"from keras.initializers import RandomNormal\nfrom keras.layers import Conv2DTranspose\nfrom keras.models import Sequential\nfrom keras.layers import Reshape\nfrom keras.layers import Dense\nfrom keras.layers import ReLU\nfrom keras.layers import Input\nfrom keras.layers import Conv2D\nfrom keras.layers import Dropout\nfrom keras.layers import LeakyReLU\nfrom keras.layers import BatchNormalization","312ce42a":"init = RandomNormal(mean = 0.0, stddev = 0.02)","023d5774":"def conv_layer(filterx,pad,isbatchnorm) :\n    \n    model = Sequential()\n    model.add(Conv2D(filterx, (4,4), strides = 2, padding = pad,\n                     use_bias = False, kernel_initializer=init))\n    \n    if isbatchnorm :\n        model.add(BatchNormalization())\n    model.add(Dropout(.2))\n    model.add(LeakyReLU())\n    \n    return model","aad37af5":"def dis () :\n    \n    model = Sequential([\n        \n        Input((64, 64, 3,)),            # 64x64\n        conv_layer(2**7, 'same',False), # 32x32\n        conv_layer(2**8, 'same', True), # 16x16\n        conv_layer(2**9, 'same', True), # 08x08\n        conv_layer(2**10,'same', True), # 04x04\n        \n        Conv2D(1, (4,4), strides = 2, padding = 'valid', activation = 'sigmoid') # 01x01\n    ])\n    \n    return model","f7cf559a":"d_model = dis()","e77c4d6d":"from keras.utils import plot_model","17718835":"plot_model(d_model, '.\/dis.png', show_shapes = True)","56539c2c":"def tran_layer(filterx,pad,isbatchnorm) :\n    \n    model = Sequential()\n    model.add(Conv2DTranspose(filterx, (4,4), strides = 2, padding = pad,\n                              use_bias = False, kernel_initializer=init))\n    \n    if isbatchnorm :\n        model.add(BatchNormalization())\n    model.add(Dropout(.2))\n    model.add(ReLU())\n    \n    return model","55cba755":"def gen () :\n    \n    model = Sequential([\n        \n        Input((NOISE_DIM,)),            # 01x01\n        \n        Dense(1024*4*4, kernel_initializer = init),\n        Reshape((4,4,1024)),            # 04x04\n        \n        tran_layer(2**9, 'same', True), # 08x08\n        tran_layer(2**8, 'same', True), # 16x16\n        tran_layer(2**7, 'same', True), # 32x32\n        Conv2DTranspose(3**1, (4,4), strides = 2, padding = 'same', activation = 'tanh') # 64x64\n    ])\n    \n    return model","d62ebb3f":"g_model = gen()","6c026fbf":"plot_model(g_model, '.\/gen.png', show_shapes = True)","71eff389":"bin_entropy = tf.keras.losses.BinaryCrossentropy(from_logits = True)","5834bb1b":"'''\nDiscriminator loss\n'''\ndef d_loss (real_output, fake_output) :\n    \n    real_loss = bin_entropy(tf.ones_like (real_output), real_output)\n    fake_loss = bin_entropy(tf.zeros_like(fake_output), fake_output)\n    \n    return real_loss + fake_loss\n\n'''\nGenerator loss\n'''\ndef g_loss (fake_output) :\n    \n    return bin_entropy(tf.ones_like(fake_output), fake_output)","fb1200c8":"g_optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0002, beta_1 = 0.5)\nd_optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0002, beta_1 = 0.5)","0d6a33da":"@tf.function\ndef train_batch (image) :\n    \n    noise = tf.random.normal([BATCH_SIZE,NOISE_DIM])\n    with tf.GradientTape(persistent = True) as tape :\n        \n        g_img = g_model(noise, training = True)\n        real_output = d_model(image, training=True)\n        fake_output = d_model(g_img, training=True)\n        \n        gen_loss = g_loss(fake_output)\n        dis_loss = d_loss(real_output, fake_output)\n    \n    gen_grad = tape.gradient(gen_loss, g_model.trainable_variables)\n    dis_grad = tape.gradient(dis_loss, d_model.trainable_variables)\n    \n    g_optimizer.apply_gradients(zip(gen_grad, g_model.trainable_variables))\n    d_optimizer.apply_gradients(zip(dis_grad, d_model.trainable_variables))","be54d381":"def fig_plot (noise) :\n    \n    gen_img = g_model(noise , training = True)\n    \n    plt.figure(figsize = (20,20))\n    i = 0\n    \n    plt.subplot(1, 3, i+1)\n    plt.imshow(gen_img[0])\n    plt.axis('off')\n    \n    plt.subplot(1, 3, i+2)\n    plt.imshow(gen_img[1])\n    plt.axis('off')\n    \n    plt.subplot(1, 3, i+3)\n    plt.imshow(gen_img[2])\n    plt.axis('off')\n    \n    plt.show()\n\ndef fit (epochs) :\n    \n    for epoch in range(epochs) :\n        \n        print(f'{epoch} done out of {epochs}')\n        for n, image in train_data.enumerate() :\n            if n%4 == 0 :\n                print('#', end='')\n            train_batch(image)\n        print()\n        \n        if epoch%3 == 0 :\n            \n            noise = tf.random.normal([3,NOISE_DIM])\n            fig_plot(noise)","0d802e7f":"EPOCHS = 35\nfit(EPOCHS)","010fa6d5":"gen_img = g_model(tf.random.normal([16, NOISE_DIM]), training = False)","43f53646":"plt.figure(figsize = (20,20))\n\ni = 0\n\nwhile i < 16 :\n    \n    plt.subplot(4, 4, i+1)\n    plt.imshow(gen_img[i])\n    plt.axis('off')\n    i += 1","a240a813":"tf.keras.models.save_model(g_model, '.\/trained_gen.h5')\ntf.keras.models.save_model(d_model, '.\/trained_dis.h5')","7c20fe1e":"# Model Training","a4a8cc00":"# Model Architecture (DCGANs)\n<div style = \"text-align: justify\">GANs or <b>Generative Adversarial Networks<\/b> comprise of two components : a Generator and a Discriminator. These two models are locked in <b>a zero-sum game, with the aim to achieve Nash Equilibrium.<\/b> The Discriminator aims to distinguish the generated images from real one (from dataset). Generator, on the other hand, aims to trick the Discriminator by creating realistic images.<\/div>\n\n![image.png](attachment:a253169f-0e1b-4617-bb71-1afa8a197adf.png)\n![image.png](attachment:5417fabe-ea13-4cf7-a7fc-29cc70275398.png)\n\n<div style = \"text-align: justify\"><a href = \"https:\/\/arxiv.org\/pdf\/1511.06434.pdf\">DCGAN<\/a> or <b>Deep Convolutional Generative Adversarial Network<\/b> will be implemented in this notebook. The Generator first upsamples the noise vector to the image size, and then Discriminator downsamples it to a single 1x1 output. Important architectural implemeatations in DCGANs - <b>Instead of Pooling layers, Convolutional and Transpose-Convolutional layers were used<\/b>, <b>Batch-Normalization was applied in both Discriminator and Generator, except for the first and last layer, respectively<\/b> and <b>LeakyRelu was used in all layers of Discriminator (except for the last one) and Relu was used in all layers of Generator (except for the last one).<\/b><\/div>\n\n![image.png](attachment:332f4c3e-a4db-4874-b53f-39aad567d118.png)","1271aadf":"# Create the dataset","32842c2a":"# Sample Visualization","dd51f9a7":"# Stand-alone Generator Perfromance","69167143":"# Discriminator","a53eeca4":"# AIM\n<div style = \"text-align: justify\">So, I was searching for datasets to implement GANs, when I came accross this dataset. The aim of this notebook is to implement <b>DCGAN for anime face generation.<\/b><\/div>\n\n# Dataset - [Link](https:\/\/www.kaggle.com\/soumikrakshit\/anime-faces)\n<div style = \"text-align: justify\">The dataset consists of 21551 images and all the images are cropped to size 64x64. Some outliers are present in the dataset, eg. bad cropping results and non-human faces.<\/div>\n\n# Sources\n<div style = \"text-align: justify\">The images were scraped from this <a href = \"http:\/\/www.getchu.com\/\">link<\/a>, and the cropping was done using anime face detection algorithm whose implemetation can be found on <a href = \"https:\/\/github.com\/nagadomi\/lbpcascade_animeface\">github.<\/a><\/div>\n\n#### *Few samples from the dataset,*","5af9c502":"# Generator","53babbd6":"#### *Running for about 100-150 epochs should give even more reasonable results. We can get an idea though. I could only run it for 1 hr beacuse my GPU qouta was going to end :-)*"}}