{"cell_type":{"dd5f5439":"code","f08f1451":"code","cfeaf912":"code","8f28de9f":"code","b7d758ff":"code","729cd54f":"code","73595f39":"code","ad01f03c":"code","ee7299e6":"code","8824ac2d":"code","c3f19acd":"code","dc723024":"code","f91a47dc":"code","2180b4fa":"code","f261fe84":"code","580eeb7e":"code","7af805fa":"code","51d469bc":"code","26e43ff1":"code","8d4eaac4":"code","85680987":"code","9b5dfff5":"code","f17d19bd":"code","6173c7b8":"code","ad04fd88":"code","4405f0bb":"code","f394be8e":"code","d3ef38d8":"code","611cddc1":"code","94b13221":"code","3eeb3f8e":"code","eae07021":"code","ca08d45f":"code","211b2a6a":"code","aa43f664":"code","d99c319f":"code","1786807b":"code","171644c9":"code","54dae750":"code","06add116":"code","baa8b54c":"code","0e0fe05f":"code","93d5cd6a":"code","cdcb5659":"code","490abe4a":"code","98d4883c":"code","533f897d":"code","4b4cb9c8":"code","dc2a6256":"code","0a8ef8a0":"code","9fa3c2b7":"code","2a925fbb":"code","3324d2ef":"code","5c1d6f56":"code","42828df7":"code","f9626dcd":"code","2725f281":"code","9e20f540":"code","79d0465e":"code","da399902":"code","ef044b70":"code","9a0bc0c8":"code","2d63f43f":"code","b6564639":"code","be573e65":"code","5e3efee3":"code","8d4c178d":"code","dca2c2bd":"code","c67ec3fe":"code","a74f3dec":"code","766790d0":"code","e52a2f09":"code","59324515":"code","da0f37bf":"code","a5deb4f9":"code","d64b05cd":"code","08cdca8b":"code","81dcc609":"code","0d262ed1":"code","0e8f0a9b":"code","1eb995c5":"code","24bfa985":"code","bb3f1d46":"code","ac6ea370":"code","7dd6aaca":"code","da2627ff":"code","4c6cd115":"code","897415f7":"code","6aa05be9":"code","40584dfc":"code","832da1a7":"code","aa7db4ce":"code","a0705a79":"code","fef4bac7":"code","dcfe6bdc":"code","e79482f9":"code","052f1921":"code","1fc9592d":"code","81192a00":"code","1e7689db":"code","4344f504":"code","80917b74":"code","8d13854d":"markdown","43b8913a":"markdown","3c7704ed":"markdown","f7b14009":"markdown","372194f1":"markdown","60212442":"markdown","94cdcc66":"markdown","60d9b937":"markdown","e7fda20e":"markdown","42b8ceac":"markdown","b40eac03":"markdown","de2fdfab":"markdown","a12d7315":"markdown","ddd0c4ae":"markdown","d1f27c2f":"markdown","e1f3d333":"markdown","218cd4f2":"markdown","28680e3e":"markdown","6d43e776":"markdown","3cd33c5c":"markdown","53181404":"markdown","7ece0538":"markdown","a961681c":"markdown","daa8ad74":"markdown","ac1da6b0":"markdown","36bf10b1":"markdown","6dc98792":"markdown"},"source":{"dd5f5439":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f08f1451":"import numpy as np\nimport pandas as pd\n\nimport random\nrandom.seed(28)\nnp.random.seed(28)\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n                             roc_curve, recall_score, classification_report, f1_score,\n                             precision_recall_fscore_support)\nimport os\nimport copy\nfrom sklearn.metrics import mean_absolute_error\npd.options.display.precision = 15\nfrom collections import defaultdict\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cat\nimport time\nfrom collections import Counter\nimport datetime\nfrom catboost import CatBoostRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit, RepeatedStratifiedKFold\nfrom sklearn import metrics\nimport gc\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom bayes_opt import BayesianOptimization\nimport eli5\nimport shap\nfrom IPython.display import HTML\nimport json\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nimport time\nimport datetime\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import metrics\npd.set_option('max_rows', 500)\nimport re\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\npd.set_option('display.max_columns', 1000)\nnp.random.seed(566)\npd.set_option('display.max_rows', 500)\npd.set_option('display.width', 1000)\npd.set_option('display.float_format', '{:20,.2f}'.format)\npd.set_option('display.max_colwidth', -1)","cfeaf912":"test=pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntest.set_index(\"PassengerId\",inplace=True)\ntest['Data']=\"Test\"\ntrain=pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntrain.set_index(\"PassengerId\",inplace=True)\ntrain['Data']=\"Train\"\nsamplesubmission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")","8f28de9f":"print('train ' , train.shape)\nprint('test ' , test.shape)\nprint('samplesubmission ' , samplesubmission.shape)","b7d758ff":"df=train.append(test)\ndf.head(2)","729cd54f":"print('Merged Dataframe' , df.shape)","73595f39":"import missingno as msno\n%matplotlib inline\nmsno.matrix(train,figsize=(35, 60), \n            width_ratios=(10, 1), color=(.0, 0.5, 0.5),\n            fontsize=16)","ad01f03c":"# Define a function to visulize the features with missing values, and % of total values, & datatype\ndef missing_values_table(df):\n     # Total missing values\n    mis_val = df.isnull().sum()\n    # Percentage of missing values\n    mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n    mis_val_type = df.dtypes\n    # Make a table with the results\n    mis_val_table = pd.concat([mis_val, mis_val_percent, mis_val_type], axis=1)\n        \n     # Rename the columns\n    mis_val_table_ren_columns = mis_val_table.rename(columns = {0 : 'Missing Values', 1 : '% of Total Values', 2: 'type'})\n        \n    # Sort the table by percentage of missing descending\n    mis_val_table_ren_columns = mis_val_table_ren_columns[ mis_val_table_ren_columns.iloc[:,1] != 0].sort_values('% of Total Values', ascending=False).round(1)\n        \n    # Print some summary information\n    print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\" \"There are \" + str(mis_val_table_ren_columns.shape[0]) + \" columns that have missing values.\")\n        \n    # Return the dataframe with missing information\n    return mis_val_table_ren_columns","ee7299e6":"missing_values_table(df)","8824ac2d":"#pct_null = df.isnull().sum() \/ len(df) \n#missing_features = pct_null[pct_null > 0.50].index \n#df.drop(missing_features, axis=1, inplace=True)","c3f19acd":"df['Fare'] = df['Fare'].fillna(0)\ndf[\"Embarked\"] = df[\"Embarked\"].fillna('C')\nmissing_values_table(df)","dc723024":"#Fill missing values in Fare\ndf.loc[(df['Fare'] == 0) & (df['Pclass'] == 1), 'Fare'] = df[df['Pclass'] == 1]['Fare'].mean()\ndf.loc[(df['Fare'] == 0) & (df['Pclass'] == 2), 'Fare'] = df[df['Pclass'] == 2]['Fare'].mean()\ndf.loc[(df['Fare'] == 0) & (df['Pclass'] == 3), 'Fare'] = df[df['Pclass'] == 3]['Fare'].mean()","f91a47dc":"bins = np.linspace(min(df['Fare']), max(df['Fare']),6)\nGroup_name=['Very Low','Low','Medium','High', 'Very High']\ndf['FareCat']=pd.cut(df['Fare'], bins, labels=Group_name, include_lowest=True)\ndf.head()","2180b4fa":"df['Cabin'] = df['Cabin'].fillna(\"Missing\")\n#df['Age'] = df['Age'].fillna(df['Age'].mean())\ndf['CabinType'] = df['Cabin'].str[0]\ndf.drop('Cabin', axis=1, inplace=True)","f261fe84":"missing_values_table(df)","580eeb7e":"import re\ndf['Title'] = df['Name'].apply(lambda x: re.search(' ([A-Z][a-z]+)\\.', x).group(1))\ndf.head()","7af805fa":"df['Title'] = df['Title'].replace('Mlle', 'Miss')","51d469bc":"df['Title'] = df['Title'].replace('Mme', 'Mrs')","26e43ff1":"i = df[~df['Title'].isin(['Mr', 'Mrs', 'Miss', 'Master'])].index\ndf.loc[i, 'Title'] = 'Rare Title'\ndf['Title'].unique()","8d4eaac4":"for t in df['Title'].unique():\n    for p in df['Pclass'].unique():\n        df.loc[(df['Title'] == t) & (df['Pclass'] == p) & (df['Age'].isnull()), 'Age'] = df.loc[(df['Title'] == t) & (df['Pclass'] == p), 'Age'].median()","85680987":"bins = np.linspace(min(df['Age']), max(df['Age']),6)\nGroup_name=['Very Low','Low','Medium','High', 'Very High']\ndf['AgeCat']=pd.cut(df['Age'], bins, labels=Group_name, include_lowest=True)\ndf.head(2)","9b5dfff5":"df_dummies = pd.get_dummies(df.AgeCat, prefix=\"AgeCat\", drop_first=True)\ndf=pd.concat([df,df_dummies], axis=1)\ndf.drop(\"AgeCat\", axis=1, inplace=True)\ndf.head(2)","f17d19bd":"df_dummies = pd.get_dummies(df.Title, prefix=\"Title\", drop_first=True)\ndf=pd.concat([df,df_dummies], axis=1)\ndf.drop(\"Title\", axis=1, inplace=True)\ndf.head()","6173c7b8":"df_dummies = pd.get_dummies(df.CabinType, prefix=\"CabinType\", drop_first=True)\ndf=pd.concat([df,df_dummies], axis=1)\ndf.drop(\"CabinType\", axis=1, inplace=True)\ndf.head()","ad04fd88":"df_dummies = pd.get_dummies(df.Embarked, prefix=\"Embarked\", drop_first=True)\ndf=pd.concat([df,df_dummies], axis=1)\ndf.drop(\"Embarked\", axis=1, inplace=True)\ndf.head()","4405f0bb":"df_dummies = pd.get_dummies(df.FareCat, prefix=\"FareCat\", drop_first=True)\ndf=pd.concat([df,df_dummies], axis=1)\ndf.drop(\"FareCat\", axis=1, inplace=True)\n#df.drop(\"Fare\", axis=1, inplace=True)\ndf.head()","f394be8e":"df_dummies = pd.get_dummies(df.Sex, prefix=\"Sex\", drop_first=True)\ndf=pd.concat([df,df_dummies], axis=1)\ndf.drop(\"Sex\", axis=1, inplace=True)\ndf.head()","d3ef38d8":"df_dummies = pd.get_dummies(df.Pclass, prefix=\"Pclass\", drop_first=True)\ndf=pd.concat([df,df_dummies], axis=1)\ndf.drop(\"Pclass\", axis=1, inplace=True)\ndf.head()","611cddc1":"df['Fsize'] = df['SibSp'] + df['Parch']+1\ndf.head(2)","94b13221":"df=df.reset_index()\ndf.head(2)","3eeb3f8e":"temp = df['Ticket'].value_counts().reset_index(name='Tsize') \ntemp.rename({'index': 'Ticket'}, axis=1, inplace=True) \ndf=df.merge(temp, left_on='Ticket', right_on='Ticket',how='inner')\ndf.set_index(\"PassengerId\",inplace=True)\ndf.head(2)","eae07021":"df['Group'] = df[['Tsize', 'Fsize']].max(axis=1)","ca08d45f":"df['GrpSize'] = ''\ndf.loc[df['Group']==1, 'GrpSize'] = df.loc[df['Group']==1, 'GrpSize'].replace('', 'solo')\ndf.loc[df['Group']==2, 'GrpSize'] = df.loc[df['Group']==2, 'GrpSize'].replace('', 'couple')\ndf.loc[(df['Group']<=4) & (df['Group']>=3), 'GrpSize'] = df.loc[(df['Group']<=4) & (df['Group']>=3), 'GrpSize'].replace('', 'group')\ndf.loc[df['Group']>4, 'GrpSize'] = df.loc[df['Group']>4, 'GrpSize'].replace('', 'large group')\ndf.head(2)","211b2a6a":"df_dummies = pd.get_dummies(df.Fsize, prefix=\"Fsize\", drop_first=True)\ndf=pd.concat([df,df_dummies], axis=1)\ndf.drop(\"Fsize\", axis=1, inplace=True)\ndf.head(2)","aa43f664":"df_dummies = pd.get_dummies(df.Tsize, prefix=\"Tsize\", drop_first=True)\ndf=pd.concat([df,df_dummies], axis=1)\ndf.drop(\"Tsize\", axis=1, inplace=True)\ndf.head(2)","d99c319f":"df['FarePP'] = df['Fare']\/df['Group']","1786807b":"#Classification of FarePP with catagories\nbins = np.linspace(min(df['FarePP']), max(df['FarePP']),6)\nGroup_name=['Very Low','Low','Medium','High', 'Very High']\ndf['FarePPCat']=pd.cut(df['FarePP'], bins, labels=Group_name, include_lowest=True)\ndf.head(2)\ndf_dummies = pd.get_dummies(df.FarePPCat, prefix=\"FarePPCat\", drop_first=True)\ndf=pd.concat([df,df_dummies], axis=1)\ndf.drop(\"FarePPCat\", axis=1, inplace=True)\ndf.head(2)","171644c9":"df_dummies = pd.get_dummies(df.GrpSize, prefix=\"GrpSize\", drop_first=True)\ndf=pd.concat([df,df_dummies], axis=1)\ndf.drop(\"GrpSize\", axis=1, inplace=True)\ndf.head(2)","54dae750":"df_dummies = pd.get_dummies(df.Group, prefix=\"Group\", drop_first=True)\ndf=pd.concat([df,df_dummies], axis=1)\ndf.drop(\"Group\", axis=1, inplace=True)\ndf.head(2)","06add116":"df_dummies = pd.get_dummies(df.Data, prefix=\"Data\", drop_first=True)\ndf=pd.concat([df,df_dummies], axis=1)\ndf.drop(\"Data\", axis=1, inplace=True)\ndf.head()","baa8b54c":"unit = list(df.select_dtypes(include=['uint8']).columns)\ndf[unit]=df[unit].astype('int')\ndf.head()","0e0fe05f":"#df.drop(\"Fare\", axis=1, inplace=True)\n#df.drop(\"FarePP\", axis=1, inplace=True)","93d5cd6a":"unit = list(df.select_dtypes(include=['uint8']).columns)\ndf[unit]=df[unit].astype('int')\ndf.head()","cdcb5659":"# Fill Missing Values with median values of given variable \ndf=df.fillna(df.median())","490abe4a":"#Listing Numerical Variables\nnumerical_df = list(df.select_dtypes(include=['float', 'int']).columns)\nnumerical_df","98d4883c":"df1=df[numerical_df]\ndf1.head()","533f897d":"train=df1[df1.Data_Train == 1]\ntrain.drop(\"Data_Train\", axis=1, inplace=True)\ntrain[\"Survived\"]=train[\"Survived\"].astype('int')\ntrain.head()","4b4cb9c8":"test=df1[df1.Data_Train == 0]\ntest.drop(\"Data_Train\", axis=1, inplace=True)\ntest.drop(\"Survived\", axis=1, inplace=True)\ntest.head()","dc2a6256":"#sns.pairplot(data=train,diag_kind='kde',vars=numerical_df,hue='Survived')\n#plt.show()","0a8ef8a0":"X = train.drop('Survived', axis=1)\ny = train.Survived.values","9fa3c2b7":"# Train\/Test Split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","2a925fbb":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_train = pd.DataFrame(scaler.fit_transform(X_train.values), columns=X_train.columns)\nX_test = pd.DataFrame(scaler.transform(X_test.values), columns=X_test.columns)\n\nprint(\"Feature space holds %d observations and %d features\" % X_train.shape)\nprint(\"Unique target labels:\", np.unique(y_train))","3324d2ef":"from collections import Counter\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nfrom xgboost import  XGBClassifier","5c1d6f56":"# Cross validate model with Kfold stratified cross val\nkfold = StratifiedKFold(n_splits=5)","42828df7":"# Test differents algorithms \nrandom_state = 42\nclassifiers = []\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(LinearDiscriminantAnalysis())\nclassifiers.append(XGBClassifier(random_state=random_state))\n\n\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, X_train, y = y_train, scoring = \"roc_auc\", cv = kfold, n_jobs=-1))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"KNeighboors\",\"LogisticRegression\",\"LinearDiscriminantAnalysis\",\"XGBoost\"]})\ncv_res = cv_res.sort_values(by=['CrossValMeans'], ascending=False)\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean roc_auc\")\ng = g.set_title(\"Cross validation scores\")\n","f9626dcd":"cv_res = cv_res.sort_values(by=['CrossValMeans'], ascending=False)\ncv_res","2725f281":"### SVC classifier\nsvm = SVC(probability=True)\nsvm_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.0001, 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300,500,700,1000]}\n\ngrid_svm = GridSearchCV(svm, param_grid = svm_param_grid, cv=kfold, scoring=\"roc_auc\", n_jobs=-1, verbose = 1)\ngrid_svm.fit(X_train,y_train)","9e20f540":"svm_best = grid_svm.best_estimator_\n# Best score\nprint('Best Score:', grid_svm.best_score_)\nprint('Best parameters set: \\n', grid_svm.best_params_)","79d0465e":"y_pred_svm = svm_best.predict(X_test)\ny_prob_svm = svm_best.predict_proba(X_test)[:,1]","da399902":"### SVC classifier\nrf = RandomForestClassifier(n_estimators=20, criterion=\"entropy\", random_state=42)\nsvm_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\nrf_param_grid = {\n    \"max_features\": range(1,10),\n    \"min_samples_split\": range(1,10),\n    \"min_samples_leaf\": range(1,10),\n}\n\ngrid_rf = GridSearchCV(rf, param_grid = rf_param_grid, cv=kfold, scoring=\"roc_auc\", n_jobs=-1, verbose = 1)\ngrid_rf.fit(X_train,y_train)","ef044b70":"rf_best = grid_rf.best_estimator_\n# Best score\nprint('Best Score:', grid_rf.best_score_)\nprint('Best parameters set: \\n', grid_rf.best_params_)","9a0bc0c8":"y_pred_rf = rf_best.predict(X_test)\ny_prob_rf = rf_best.predict_proba(X_test)[:,1]","2d63f43f":"### KNN classifier\nknn = KNeighborsClassifier()\nknn_param_grid = {\n    'n_neighbors': range(1,50),\n    'weights': ['uniform','distance']\n}\n\ngrid_knn = GridSearchCV(knn, param_grid = knn_param_grid, cv=kfold, scoring=\"roc_auc\", n_jobs=-1, verbose = 1)\ngrid_knn.fit(X_train,y_train)","b6564639":"knn_best = grid_knn.best_estimator_\n# Best score\nprint('Best Score:', grid_knn.best_score_)\nprint('Best parameters set: \\n', grid_knn.best_params_)","be573e65":"y_pred_knn = knn_best.predict(X_test)\ny_prob_knn = knn_best.predict_proba(X_test)[:,1]","5e3efee3":"### Logistic Regression classifier\n\nlr = LogisticRegression()\nC_grid = 0.001*10**(np.arange(0,1.01,0.01)*3)\nlr_param_grid  = {\n    'penalty': ['l1', 'l2'], \n    'C': C_grid\n}\n\ngrid_lr = GridSearchCV(lr, lr_param_grid, scoring='roc_auc')\ngrid_lr.fit(X_train, y_train)","8d4c178d":"lr_best = grid_lr.best_estimator_\n# Best score\nprint('Best Score:', grid_lr.best_score_)\nprint('Best parameters set: \\n', grid_lr.best_params_)","dca2c2bd":"y_pred_lr = lr_best.predict(X_test)\ny_prob_lr = lr_best.predict_proba(X_test)[:,1]","c67ec3fe":"# Gradient boosting tunning\ngb = GradientBoostingClassifier(random_state=random_state)\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [5, 10,20,50],\n              'learning_rate': [0.5, 0.4, 0.3, 0.2, 0.1],\n              'max_depth': range(1,10, 2),\n              'min_samples_leaf': [5, 10, 25, 50],\n              'max_features': [0.5, 0.25, 0.3, 0.1] \n              }\n\ngrid_gb = GridSearchCV(gb,param_grid = gb_param_grid, cv=kfold, scoring=\"roc_auc\", n_jobs= 4, verbose = 1)\n\ngrid_gb.fit(X_train,y_train)","a74f3dec":"gb_best = grid_gb.best_estimator_\n# Best score\nprint('Best Score:', grid_gb.best_score_)\nprint('Best parameters set: \\n', grid_gb.best_params_)","766790d0":"y_pred_gb = gb_best.predict(X_test)\ny_prob_gb = gb_best.predict_proba(X_test)[:,1]","e52a2f09":"# XGboost tunning\nxgb = GradientBoostingClassifier(random_state=random_state)\n\n#xgb.get_params().keys()\nxgb_param_grid = {\n    'learning_rate': [0.001, 0.01, 0.025,0.05,0.1], \n    'max_depth': range(1,10),\n    'subsample': [0.01,0.05, 0.1, 0.25,0.5,1.0, 1.5, 2]\n}\ngrid_xgb = GridSearchCV(xgb, param_grid = xgb_param_grid, cv=kfold, scoring=\"roc_auc\", n_jobs= 4, verbose = 1)\n\ngrid_xgb.fit(X_train,y_train)","59324515":"xgb_best = grid_xgb.best_estimator_\n# Best score\nprint('Best Score:', grid_xgb.best_score_)\nprint('Best parameters set: \\n', grid_xgb.best_params_)","da0f37bf":"y_pred_xgb = xgb_best.predict(X_test)\ny_prob_xgb = xgb_best.predict_proba(X_test)[:,1]","a5deb4f9":"# LDA tunning\nLDA = LinearDiscriminantAnalysis()\n#lda.get_params().keys()\n## Search grid for optimal parameters\nlda_param_grid = {\"solver\" : [\"svd\"],\n              \"tol\" : [0.00001,0.0001,0.0002,0.0003]}\n\n\ngrid_lda = GridSearchCV(LDA, param_grid = lda_param_grid, cv=kfold, scoring=\"roc_auc\", n_jobs= 4, verbose = 1)\ngrid_lda.fit(X_train,y_train)","d64b05cd":"lda_best = grid_lda.best_estimator_\n# Best score\nprint('Best Score:', grid_lda.best_score_)\nprint('Best parameters set: \\n', grid_lda.best_params_)","08cdca8b":"y_pred_lda = lda_best.predict(X_test)\ny_prob_lda = lda_best.predict_proba(X_test)[:,1]","81dcc609":"# print a summary of the scores\ndef print_grid_search_metrics(gs):\n    print(\"Best score: %0.3f\" % gs.best_score_)\n    print(\"Best parameters set:\")\n    best_parameters = gs.best_params_\n    for param_name in sorted(parameters.keys()):\n        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))","0d262ed1":"\"\"\"\nprint_grid_search_metrics(grid_svm)\nprint_grid_search_metrics(grid_rf)\nprint_grid_search_metrics(grid_gb)\nprint_grid_search_metrics(grid_knn)\nprint_grid_search_metrics(grid_lr)\nprint_grid_search_metrics(grid_xgb)\nprint_grid_search_metrics(grid_lda)\n\"\"\"","0e8f0a9b":"# Plot learning curves\n#grid_svm, grid_rf, grid_gb, grid_knn, grid_lr, grid_xgb\n\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ng = plot_learning_curve(grid_svm.best_estimator_,\"SVM learning curves\",X_train,y_train,cv=kfold)\ng = plot_learning_curve(grid_rf.best_estimator_,\"Random Forest learning curves\",X_train,y_train,cv=kfold)\ng = plot_learning_curve(grid_gb.best_estimator_,\"Gradient Boosting learning curves\",X_train,y_train,cv=kfold)\ng = plot_learning_curve(grid_knn.best_estimator_,\"KNN learning curves\",X_train,y_train,cv=kfold)\ng = plot_learning_curve(grid_lr.best_estimator_,\"Logistic Regression learning curves\",X_train,y_train,cv=kfold)\ng = plot_learning_curve(grid_xgb.best_estimator_,\"XGBoost learning curves\",X_train,y_train,cv=kfold)\ng = plot_learning_curve(grid_lda.best_estimator_,\"LDA learning curves\",X_train,y_train,cv=kfold)","1eb995c5":"## Calculate Confusion Matrix (Precision, Recall, Accuracy)","24bfa985":"from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score, accuracy_score, precision_score, recall_score\n\ndef cal_evaluation(classifier, cm, auc):\n    tn = cm[0][0]\n    fp = cm[0][1]\n    fn = cm[1][0]\n    tp = cm[1][1]\n    accuracy  = (tp + tn) \/ (tp + fp + fn + tn + 0.0)\n    precision = tp \/ (tp + fp + 0.0)\n    recall = tp \/ (tp + fn + 0.0)\n    f1 = 2 * precision * recall \/ (precision + recall)\n    print(classifier)\n    print(\"Accuracy is \" + str(accuracy))\n    print(\"Precision is \" + str(precision))\n    print(\"Recall is \" + str(recall))\n    print(\"F1 score is \" + str(f1))\n    print(\"ROC AUC is \" + str(auc))\n\ndef draw_confusion_matrices(confusion_matricies):\n    class_names = ['Not Survived','Survived']\n    for x in confusion_matrices:\n        classifier, cm, auc = x[0], x[1], x[2]\n        cal_evaluation(classifier, cm, auc)\n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n        cax = ax.matshow(cm, interpolation='nearest',cmap=plt.get_cmap('Reds'))\n        plt.title('Confusion matrix for {}'.format(classifier))\n        fig.colorbar(cax)\n        ax.set_xticklabels([''] + class_names)\n        ax.set_yticklabels([''] + class_names)\n        plt.xlabel('Predicted')\n        plt.ylabel('True')\n        plt.show()","bb3f1d46":"%matplotlib inline\n\ny = np.array(y)\nclass_names = np.unique(y)\nprint(class_names)\n\nconfusion_matrices = [\n    (\"Gradient Boosting\", confusion_matrix(y_test, y_pred_gb), roc_auc_score(y_test, y_prob_gb)),\n    (\"Logisitic Regression\", confusion_matrix(y_test, y_pred_lr), roc_auc_score(y_test, y_prob_lr)),\n    (\"K-Nearest-Neighbors\", confusion_matrix(y_test, y_pred_knn), roc_auc_score(y_test, y_prob_knn)),\n    (\"Random Forest\", confusion_matrix(y_test, y_pred_rf), roc_auc_score(y_test, y_prob_rf)),\n    (\"Support Vector Machine\", confusion_matrix(y_test, y_pred_svm), roc_auc_score(y_test, y_prob_svm)),\n    (\"XGBoost Classifier\", confusion_matrix(y_test, y_pred_xgb), roc_auc_score(y_test, y_prob_xgb)),\n    (\"LDA Classifier\", confusion_matrix(y_test, y_pred_lda), roc_auc_score(y_test, y_prob_lda))\n]\n\ndraw_confusion_matrices(confusion_matrices)","ac6ea370":"\"\"\"\nforest = RandomForestClassifier()\nforest.fit(X_train, y_train)\nimportances = forest.feature_importances_\n# Print the feature ranking\nprint(\"Feature importance ranking by Random Forest Model:\")\nfor k,v in sorted(zip(map(lambda x: round(x, 4), importances), X_train.columns), reverse=True):\n    print(v + \": \" + str(k))\n\"\"\"","7dd6aaca":"#nrows = ncols = 2\nnrows = 3\nncols = 1\nfig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(5,15))\n\nnames_classifiers = [(\"XGBoost\", xgb_best),(\"RandomForest\",rf_best),(\"GradientBoosting\",gb_best),(\"GradientBoosting\",gb_best)]\n\nnclassifier = 0\nfor row in range(nrows):\n    name = names_classifiers[nclassifier][0]\n    classifier = names_classifiers[nclassifier][1]\n    indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n    g = sns.barplot(y=X_train.columns[indices][:40],x = classifier.feature_importances_[indices][:40] , orient='h',ax=axes[row])\n    g.set_xlabel(\"Relative importance\",fontsize=12)\n    g.set_ylabel(\"Features\",fontsize=12)\n    g.tick_params(labelsize=9)\n    g.set_title(name + \" feature importance\")\n    nclassifier += 1","da2627ff":"# Concatenate all classifier results\n\ntest_rf = pd.Series(rf_best.predict(X_test), name=\"Random Forest\")\ntest_lr = pd.Series(lr_best.predict(X_test), name=\"Logistic Regression\")\ntest_gb = pd.Series(gb_best.predict(X_test), name=\"Gradient Boosting\")\ntest_knn = pd.Series(knn_best.predict(X_test), name=\"KNN\")\ntest_svm = pd.Series(svm_best.predict(X_test), name=\"SVM\")\ntest_xgb = pd.Series(xgb_best.predict(X_test), name=\"XGBoost\")\ntest_lda = pd.Series(lda_best.predict(X_test), name=\"LDA\")\n\n\nensemble_results = pd.concat([test_rf, test_lr,test_gb,test_knn, test_svm, test_xgb, test_lda],axis=1)\n\ng= sns.heatmap(ensemble_results.corr(),annot=True)","4c6cd115":"votingC = VotingClassifier(estimators=[('rf', rf_best), ('lr',lr_best),('knn', knn_best),\n('svm', svm_best), ('gb',gb_best),('xgb',xgb_best),('lda',lda_best)], voting='soft', n_jobs=4)\nvotingC = votingC.fit(X_train, y_train)","897415f7":"y_pred_voting = votingC.predict(X_test)\ny_prob_voting = votingC.predict_proba(X_test)[:,1]","6aa05be9":"confusion_matrices = [\n    (\"Ensemble modeling\", confusion_matrix(y_test, y_pred_voting), roc_auc_score(y_test, y_prob_voting))   \n]\ndraw_confusion_matrices(confusion_matrices)","40584dfc":"confusion_matrices = [\n    (\"LDA\", confusion_matrix(y_test, y_pred_lda), roc_auc_score(y_test, y_prob_lda))\n]\n\ndraw_confusion_matrices(confusion_matrices)","832da1a7":"# make the ROC curve\nfpr, tpr, thresh = roc_curve(y_test, y_prob_lda, pos_label=1)\nroc_auc = roc_auc_score(y_test, y_prob_lda)\n\n# These are the points at threshold = 0.1~0.5\nx1 = fpr[(thresh <= 0.5) & (thresh >= 0.1)] \nx2 = tpr[(thresh <= 0.5) & (thresh >= 0.1)]\n\nfig = plt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\nplt.plot([0, 1], [0, 1], color='b', lw=2, linestyle='--')\nplt.plot(x1, x2, color='k', lw=3, label='threshold = 0.1 ~ 0.5')\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve for LDA')\nplt.legend(loc=\"lower right\")\nplt.show()","aa7db4ce":"acc_grid = np.zeros(thresh.shape)\nprec_grid = np.zeros(thresh.shape)\nrecall_grid = np.zeros(thresh.shape)\nTP_grid = np.zeros(thresh.shape)\nFP_grid = np.zeros(thresh.shape)\nFN_grid = np.zeros(thresh.shape)\n\nfor i in range(thresh.shape[0]):\n    cm = confusion_matrix(y_test, y_prob_rf >= thresh[i])\n    acc_grid[i] = accuracy_score(y_test, y_prob_rf >= thresh[i])\n    prec_grid[i] = precision_score(y_test, y_prob_rf >= thresh[i])\n    recall_grid[i] = recall_score(y_test, y_prob_rf >= thresh[i])\n    TP_grid[i] = cm[1][1]\n    FP_grid[i] = cm[0][1]\n    FN_grid[i] = cm[1][0]","a0705a79":"fig = plt.figure()\nplt.plot(thresh, acc_grid, color='k', lw=2, label='Accuracy')\nplt.plot(thresh, prec_grid, color='b', lw=2, label='Precision')\nplt.plot(thresh, recall_grid, color='r', lw=2, label='Recall')\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel('Threshold')\nplt.ylabel('Score')\nplt.legend(loc=\"lower center\")\nplt.show()","fef4bac7":"P = 0.5 # The chance a customer who was going to drop would take the special offer\nloss = TP_grid*P*(-0.1) + TP_grid*(1-P)*(-1) + FN_grid*(-1) + FP_grid*(-0.1)\nthresh_best = thresh[loss == max(loss)][0]\n    \nfig = plt.figure()\nplt.plot(thresh, loss, color='b', lw=2)\nplt.scatter([thresh_best], [max(loss)], color='r', s=50, label='Threshold = {:.2f}'.format(thresh_best))\nplt.xlim([-0.05, 1.05])\nplt.xlabel('Threshold')\nplt.ylabel('Loss of Profitability')\nplt.legend(loc=\"lower right\")\nplt.show()","dcfe6bdc":"y_pred_lda_new = y_prob_lda > thresh_best\n\nconfusion_matrices = [\n    (\"LDA\", confusion_matrix(y_test, y_pred_lda_new), roc_auc_score(y_test, y_prob_lda))\n]\n\ndraw_confusion_matrices(confusion_matrices)","e79482f9":"#test1=test.copy()\ntest=test.sort_values(by='PassengerId', ascending=False)\ntest.head()","052f1921":"from sklearn.preprocessing import StandardScaler\nscaled_test = StandardScaler().fit_transform(test.values)\nscaled_test = pd.DataFrame(scaled_test, index=test.index, columns=test.columns)\nscaled_test.head()","1fc9592d":"y_pred_lda = lda_best.predict(scaled_test)\ny_prob_lda = lda_best.predict_proba(scaled_test)[:,1]","81192a00":"scaled_test['Survived']=y_pred_lda\nscaled_test.head()","1e7689db":"scaled_test=scaled_test.reset_index()\nscaled_test.head()","4344f504":"PassengerId = scaled_test.PassengerId.values\ntest_preds = scaled_test.Survived.values\nsubmission = pd.DataFrame.from_dict({\n    'PassengerId':PassengerId,\n    'Survived':test_preds,\n})\nsubmission.to_csv('gender_submission.csv', index=False)","80917b74":"submission = pd.read_csv('gender_submission.csv')\nsubmission.head()","8d13854d":"### Hyperparameter tunning for best models\ngrid search optimization for the classifiers","43b8913a":"### K-fold Cross-Validation\n\nI compare 11 of the most popular classifiers and evaluate their performance using a stratified kfold cross validation procedure.\n\n- Logistic Regression\n- Linear Discriminant Analysis\n- SVC\n- KNN\n- Random Forest (RFC)\n- Decision Tree\n- Extra Trees (ExtC)\n- AdaBoost (Ada)\n- Gradient Boosting (GBC)\n- Multiple layer perceptron (neural network)\n- XGBoost","3c7704ed":"Next, I will try to determine the optimal threshold. As true positive rate and recall are actually equal, therefore, one can use a lower threshold(<0.5) to achieve higher recall rate.","f7b14009":"## 2. Loading Data & Basic Analysis","372194f1":"### Ensemble modeling\n\nI chosed a voting classifier to combine the predictions coming from the 7 classifiers.\n\nI preferred to pass the argument \"soft\" to the voting parameter to take into account the probability of each vote.","60212442":"The term *Mademoiselle* is a French familiar title, abbreviated **Mlle**, traditionally given to an unmarried woman. The equivalent in English is \"Miss\". However, the courtesy title \"Madame\" is accorded women where their marital status is unknown.","94cdcc66":"# Part 3: Model Training and Result Evaluation\nI will implement machine learning pipelines consisting of one or more of the following steps, depending on the particular model:\n\n- Mean imputation of missing values\n- Dimension reduction using linear discriminant analysis (LDA)\n- Data standardization: rescaling to zero mean and unit variance\n- The chosen model\n\nI will evaluate and compare the following models using a cross-validated Area Under the Receiver Operating Characteristic Curve (AUROC)** score on the training set\n\nI'll perform some hyperparameter tuning for each model to choose the most promising model, then more carefully tune the hyperparameters of the best-performing model.\n\n** For the metrics, both precision and recall of the result are important, as we care about true positives as well as false positives. ","60d9b937":"#### Plot learning curves\nLearning curves are a good way to see the overfitting effect on the training set and the effect of the training size on the accuracy.","e7fda20e":"#### KNN","42b8ceac":"In France, one traditionally calls a young, unmarried woman Mademoiselle \u2013 Mlle for short \u2013 and an older, married woman _Madame_, whose abbreviation is **Mme**.","b40eac03":"## Have a look at missing value","de2fdfab":"## Feature importance of tree based classifiers\nIn order to see the most informative features for the prediction of passengers survival, i displayed the feature importance for the tree based classifiers","a12d7315":"___Dividing Titles into bigger categories___","ddd0c4ae":"## LDA for predictions\nAgain, the goal is to predict survival. \n\nI will display the LDA results below, as a reminder. ","d1f27c2f":"I decide to choose: SVC, RandomForest, GradientBoosting, KNN and logistic Regression, XGBoost, for further fine-tunning and the ensemble modeling","e1f3d333":"### Comparing the best models\nI will take a closer look of the selected models: logistic Regression, Random Forest, Gradient Boosting, KNN, XGBoost and SVM. \n\n- Learning Curve\n- Confusion Matrix","218cd4f2":"## Extracting & Analyzing Titles","28680e3e":"## 1. Loading Packages","6d43e776":"#### SVM","3cd33c5c":"## Travelling Alone vs. Travelling in groups\n* Family Size","53181404":"#### XGBoost","7ece0538":"# EDA","a961681c":"#### Gradient Boosting","daa8ad74":"The AUROC score of is smaller than the XGBoost score. Therefore, I will use XGboost for the final prediction. ","ac1da6b0":"#### Random Forest","36bf10b1":"#### Linear Discriminant Analysis","6dc98792":"#### Logistic Regression "}}