{"cell_type":{"d1737124":"code","53bd9d15":"code","30cb1eef":"code","e916c022":"code","cd442182":"code","2d3d626d":"code","10927fff":"code","18d1b5e1":"code","646ca161":"code","3536db2a":"code","8ee1a3db":"code","a5212450":"code","172d49af":"code","02f03daf":"code","e2926fca":"code","a536c4c1":"code","5a0f87f6":"code","218bc072":"code","d9977baa":"code","81501d7c":"code","e1eb81c5":"code","ff62986b":"code","0f7bb283":"code","1825055c":"code","1c893746":"code","3c9b6d98":"code","8ea79cdf":"code","87f13723":"code","dc2cc1ea":"code","eb847f4d":"code","fb051ccf":"code","fb7fe119":"code","d12da116":"markdown","80a316d7":"markdown","d3d3e965":"markdown","a68af57c":"markdown","5adeed0a":"markdown","9e2c1c80":"markdown","bd04986e":"markdown","9c49ff3c":"markdown","41e9c9f0":"markdown","443aad09":"markdown","d218b658":"markdown","f504a6cb":"markdown","3d4ddbb8":"markdown","a8210f7e":"markdown","74300e97":"markdown","52ebceda":"markdown","916d4910":"markdown","ca3a8d80":"markdown","3af6927b":"markdown","16accc44":"markdown","ff539d8f":"markdown","35d027ae":"markdown","8578b1b9":"markdown","064829d2":"markdown","a72393f2":"markdown","d8101cf5":"markdown","7cdce7cc":"markdown","e77ac1e0":"markdown","ca01ff0a":"markdown","bf99eaa5":"markdown","c4710deb":"markdown","b6a897fb":"markdown","9235401a":"markdown","27b9d632":"markdown","0bc15f07":"markdown","f8f12bd0":"markdown","b5b64d0c":"markdown","c8ff2e76":"markdown","00db432c":"markdown","ee0abb35":"markdown"},"source":{"d1737124":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\n\n, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","53bd9d15":"Train_dir = '\/kaggle\/input\/cat-and-dog\/training_set\/training_set'\nTest_dir = '\/kaggle\/input\/cat-and-dog\/test_set\/test_set'\ntrain_total =len(os.listdir(Train_dir))\ntest_total =len(os.listdir(Test_dir))\nprint(\"Training data: {} and testing data: {}\".format(train_total,test_total))\n                                                                                               ","30cb1eef":"# load the dataset\n\ncats_locate = []\ndogs_locate = []\n\ncat_train = os.path.join(Train_dir, 'cats')\ndog_train = os.path.join(Train_dir, 'dogs')\n\nfor path in os.listdir(cat_train):\n    if 'jpg' in path:\n        cats_locate.append(os.path.join(cat_train, path))\n    \nfor path in os.listdir(dog_train):\n    if 'jpg' in path:\n        dogs_locate.append(os.path.join(dog_train, path))    ","e916c022":"print(\"Length: Cats: {} and Dogs: {}\".format(len(cats_locate), len(dogs_locate)))","cd442182":"import numpy as np\nfrom keras import preprocessing\n\ntraining_set_orig = np.zeros((6000, 32, 32, 3))\n\n\ndef train_data_generate(save_image, total_size, divide_size):\n    for i in range(total_size):\n        # save the first 3000 as the cats images\n        if i < divide_size:\n            path = dogs_locate[i]\n            img = preprocessing.image.load_img(path, target_size=(32,32))\n            save_image[i] = preprocessing.image.img_to_array(img)\n        else:\n            path = cats_locate[i - divide_size]\n            img = preprocessing.image.load_img(path, target_size=(32,32))\n            save_image[i] = preprocessing.image.img_to_array(img)\n\n\ndef test_data_generate(save_image, total_size, divide_size):\n    for i in range(total_size):\n        # save the first 3000 as the cats images\n        if i < divide_size:\n            path = dogs_locate[i + 3000]\n            img = preprocessing.image.load_img(path, target_size=(32,32))\n            save_image[i] = preprocessing.image.img_to_array(img)\n        else:\n            path = cats_locate[i + 2000]\n            img = preprocessing.image.load_img(path, target_size=(32,32))\n            save_image[i] = preprocessing.image.img_to_array(img)\n\n    ","2d3d626d":"train_data_generate(training_set_orig, 6000, 3000)\n\nprint(\"Training Set: \", len(training_set_orig))","10927fff":"# do the same with the test data\ntesting_set_orig = np.zeros((2000,32,32,3), dtype='float32')\ntest_data_generate(testing_set_orig, 2000,1000)\nprint(\"Testing data: \", len(testing_set_orig))","18d1b5e1":"training_set_orig.shape","646ca161":"testing_set_orig.shape","3536db2a":"# create the labels dataset too\n# we have 3000 as dogs image and remaining as cats images\n# 0 for dogs and 1 for cats\ny_train = np.zeros((3000,))\ny_train = np.concatenate((y_train, np.ones((3000,))))","8ee1a3db":"y_train = y_train.reshape(1,-1)\ny_train","a5212450":"y_test = np.zeros((1000,))\ny_test = np.concatenate((y_test, np.ones((1000,))))\ny_test = y_test.reshape(1,-1)\n","172d49af":"y_test\nprint(\"Training set labels\" +str(y_train.shape)+ \"  Test set labels\" + str(y_test.shape))","02f03daf":"\nm_train = training_set_orig.shape[0]\nm_test = testing_set_orig.shape[0]\nnum_px = training_set_orig.shape[1]\n\n\nprint(\"Number of Training examples: {}\".format(m_train))\nprint(\"Number of Test examples: {}\".format(m_test))\nprint(\"Height\/Width of each images: {}\".format(num_px))\nprint(\"Each images size: {} X {}\".format(num_px, num_px))\nprint(\"Training shape: {}\".format(training_set_orig.shape))\nprint(\"Testing shape: {}\".format(testing_set_orig.shape))\nprint(\"Training label: {}\".format(y_train.shape))\nprint(\"Test label: {}\".format(y_test.shape))","e2926fca":"num_test = training_set_orig.shape[1]\nprint(num_test)\ntrain_set_x_flatten = training_set_orig.reshape(training_set_orig.shape[0], -1).T\nprint(\"Train set \", train_set_x_flatten.shape)\ntest_set_x_flatten = testing_set_orig.reshape(testing_set_orig.shape[0],-1).T\nprint(\"Test set :- \", test_set_x_flatten.shape)\nprint(\"Test label: {}\".format(y_test.shape))\nprint(\"Train label: {}\".format(y_train.shape))\nprint (\"sanity check after reshaping: \" + str(train_set_x_flatten[0:5,0]))\n","a536c4c1":"train_set_x = train_set_x_flatten\/255.\ntest_set_x = test_set_x_flatten\/255.","5a0f87f6":"# Now create the basic sigmoid and relu functions\nimport math\n\ndef sigmoid(x):\n    \"\"\"\n    :param x: value scalar\n    :return s: sigmoid value: sigmoid(x)\n    \n    \"\"\"\n    s = 1\/(1+np.exp(-x))\n    return s\n","218bc072":"# initialize parameters\n\n\ndef initialize_with_zeros(dim):\n    \"\"\"\n    Initiliaze w and b with the dimension\n    \"\"\"\n    w = np.zeros(shape=(dim, 1))\n    b = 0\n    return w, b","d9977baa":"# let check\nw1, b1 = initialize_with_zeros(2)\nprint(w1, b1)","81501d7c":"def propogate( w, b, X, Y):\n    # calculate m\n    \n    m = X.shape[1]\n    \n    # FORWARD PROPAGATION (FROM X TO COST)\n    A = sigmoid(np.dot(w.T, X) + b)                                # compute activation\n    \n    cost = (-1\/m)* np.sum(Y * np.log(A) + (1-Y) * (np.log(1-A)))                         # compute cost\n\n   \n    # BACKWARD PROPAGATION (TO FIND GRAD)\n    ### START CODE HERE ### (\u2248 2 lines of code)\n    dw = (1\/m) * np.dot(X , (A-Y).T)\n    \n    db = (1\/m) *np.sum(A-Y)\n    ### END CODE HERE ###\n\n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    cost = np.squeeze(cost)\n    assert(cost.shape == ())\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return grads, cost","e1eb81c5":"w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\ngrads, cost = propogate( w, b,X, Y)\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))\nprint (\"cost = \" + str(cost))","ff62986b":"\ndef optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = True):\n    \"\"\"\n    This function optimizes w and b by running a gradient descent algorithm\n    \n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (num_px * num_px * 3, number of examples)\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n\n    \"\"\"\n    \n    costs = []\n    \n    for i in range(num_iterations):\n        \n        \n        # Cost and gradient calculation (\u2248 1-4 lines of code)\n        grads, cost =  propogate(w, b, X, Y)\n        \n        # Retrieve derivatives from grads\n        dw = grads[\"dw\"]\n        db = grads[\"db\"]\n        \n        # update rule (\u2248 2 lines of code)\n        w = w - (learning_rate * dw)\n        b = b - (learning_rate * db)\n        \n        # Record the costs\n        if i % 50 == 0:\n            costs.append(cost)\n        \n        # Print the cost every 100 training iterations\n        if print_cost and i % 50 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\": w,\n              \"b\": b}\n    \n    grads = {\"dw\": dw,\n             \"db\": db}\n    \n    return params, grads, costs","0f7bb283":"params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = True)\n\nprint (\"w = \" + str(params[\"w\"]))\nprint (\"b = \" + str(params[\"b\"]))\nprint (\"dw = \" + str(grads[\"dw\"]))\nprint (\"db = \" + str(grads[\"db\"]))","1825055c":"def predict(w, b, X):\n    m = X.shape[1]\n    Y_prediction = np.zeros((1,m))\n    w = w.reshape(X.shape[0],1)\n    \n    A = sigmoid(np.dot(w.T,X) + b)\n    \n    for i in range(A.shape[1]):\n        if A[0,i] > 0.5:\n            Y_prediction[0,i] = 1\n        else:\n            Y_prediction[0,i] = 0\n        pass\n    return Y_prediction","1c893746":"w = np.array([[0.1124579],[0.23106775]])\nb = -0.3\nX = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])\nprint (\"predictions = \" + str(predict(w, b, X)))","3c9b6d98":"\n\ndef model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = True):\n    \"\"\"\n    :param X_train:   Training images\n    :param Y_train: Training labels\n    :param X_test: Testing images\n    :param Y_test: Testing label\n    :return d: dictionary containing all the values\n    \n    \"\"\"\n    \n    ### START CODE HERE ###\n    \n    # initialize parameters with zeros (\u2248 1 line of code)\n    w, b = initialize_with_zeros(X_train.shape[0])\n\n    # Gradient descent (\u2248 1 line of code)\n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = print_cost)\n    \n    # Retrieve parameters w and b from dictionary \"parameters\"\n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n    \n    # Predict test\/train set examples (\u2248 2 lines of code)\n    Y_prediction_test = predict(w,b, X_test)\n    Y_prediction_train = predict(w,b,X_train)\n\n    ### END CODE HERE ###\n\n    # Print train\/test Errors\n    train_acc = 100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100\n    test_acc = 100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100\n    print(\"train accuracy: {} %\".format(train_acc))\n    print(\"test accuracy: {} %\".format(test_acc))\n\n    \n    d = {\"costs\": costs,\n         \"Y_prediction_test\": Y_prediction_test, \n         \"Y_prediction_train\" : Y_prediction_train, \n         \"w\" : w, \n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_iterations\": num_iterations,\n        'train_Accuracy': train_acc,\n        'test_accuracy': test_acc}\n    \n    return d","8ea79cdf":"print(train_set_x.shape)\n# print(y_train_flatten.shape)\nprint(test_set_x.shape)\n# print(y_test_flatten.shape)\n","87f13723":"d = model(train_set_x, y_train, test_set_x, y_test,\n          num_iterations=2000, learning_rate=0.005)\n","dc2cc1ea":"# predict  the images\nindex = [1,100,1000, 500]\n\nimport matplotlib.pyplot as plt\nfor i in index:\n    plt.imshow(test_set_x[:, i].reshape(32,32,3))\n    \n    current = (y_test[:,i])\n    predicted = (d['Y_prediction_test'][:,i])\n    plt.title(\"current:{} and expected: {}\".format(current, predicted))\n    plt.show()","eb847f4d":"learning_rates=[0.01,0.001,0.00001]\n\nmodel_info = {}\n\nfor i in learning_rates:\n    print(\"learning rate: {}\".format(i))\n    data =  model(train_set_x, y_train, test_set_x, y_test,num_iterations=2000, learning_rate=i)\n    model_info[i] = data\n    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n\n          \n\n","fb051ccf":"model_info\n","fb7fe119":"for val in model_info:\n    print(\"\\n learning rate: {}\\n Test Accuracy:{}\\nTrain Accuracy:{}\".format(val, model_info[val]['test_accuracy'], model_info[val]['train_Accuracy'] ))","d12da116":"### Convert the Data into proper format for Neural Network","80a316d7":"![out.png](attachment:out.png)\n\n\n\nSigmoid is used in the final layer","d3d3e965":"### Content:\n\n* Read the Data\n* Convert the Data into proper format for Neural Network\n* Flatten the images\n* Develop method for (1) sigmoid (2) propogation (3) parameter updation (4) predict\n* Merge all the above function together\n* Iterate with multiple learning rate","a68af57c":"Now, we need to flatten each training images into one columns\n","5adeed0a":"Let test with diff learning rate","9e2c1c80":"We need to normalize the data by dividing 255\n","bd04986e":"We can read from the tensorflow function using some builtin function. But, we want to do something different here :)","9c49ff3c":"The below function will fetch the parameters we get from the forward propogation","41e9c9f0":"we have created target tensor for train and test datasets","443aad09":"### Our main Model:\n\n\n![sig.png](attachment:sig.png)","d218b658":"\n**Mathematical expression of the algorithm**:\n\nFor one example $x^{(i)}$:\n$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n\nThe cost is then computed by summing over all training examples:\n$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{6}$$","f504a6cb":"So, scratch knowledge is important. SO, lets start","3d4ddbb8":"Please upvote this kernel.\n\n\nTHanks \n\n\n![upvote.jpg](attachment:upvote.jpg)","a8210f7e":"We will use cats and dogs datasets for the predictions.\n\n","74300e97":"we finally achieved .","52ebceda":"This kernel is specially for the one layer Neural Network from scratch implementation\n\n\nWe have not used any tensorflow inbuilt function for this.\n\nGoal is to see what happens behind regression from calculus point of view.\n\nThis kernel is inspired from the Deeplearning.ai course and kernel: # https:\/\/www.kaggle.com\/benanakca\/logistic-regression-adagrad-from-scratch","916d4910":"###  Forward and Backward propagation\n\nNow that your parameters are initialized, you can do the \"forward\" and \"backward\" propagation steps for learning the parameters.\n\n\nLink: https:\/\/www.coursera.org\/lecture\/neural-networks-deep-learning\/forward-and-backward-propagation-znwiG\n\n**Hints**:\n\nForward Propagation:\n- You get X\n- You compute $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n- You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n\nHere are the two formulas you will be using: \n\n$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$\"","ca3a8d80":"We will convert 3(RGB layers) into one single layers using np.reshape","3af6927b":"![1.jpg](attachment:1.jpg)","16accc44":"### Prediction\n\n\nWe will predict from calculated values and expected values.\n\nTHis is the last neuron.","ff539d8f":"### sigmoid function, np.exp() ###\n\n\n\n\n**Reminder**:\n$sigmoid(x) = \\frac{1}{1+e^{-x}}$ is sometimes also known as the logistic function. It is a non-linear function used not only in Machine Learning (Logistic Regression), but also in Deep Learning.\n\n","35d027ae":"## Logistic Regression using Calculus","8578b1b9":"## Goal\n\nWe will predict Logistic Regression using the pure mathematical expression ( Calculus ). It is very important for us to understand the maths behind the neural network and regression technologies.\n\n\nI will use numpy module to calculate the mathematical equations for relu, sigmoid, forward and backward propogations.\n\n","064829d2":"There are total 8005 training images including cats and dogs.\n\nWe will create the 6000 for the training and 2000 for the validation\n\nAnd in 6000, we will have 3000 for the cats and dogs simultaneoulsy images.\n\n","a72393f2":"\n1. Calculate $\\hat{Y} = A = \\sigma(w^T X + b)$\n\n2. Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector `Y_prediction`. If you wish, you can use an `if`\/`else` statement in a `for` loop (though there is also a way to vectorize this). ","d8101cf5":" The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate.","7cdce7cc":"Initialize all the variables like w and b","e77ac1e0":"So, we can improve the model by tuning the learning rate and epochs_time.  For 0.01 learning rate, we got 61 percent accuracy from single layer neural network","ca01ff0a":"![1.gif](attachment:1.gif)","bf99eaa5":"![fla.png](attachment:fla.png)","c4710deb":"![diff.png](attachment:diff.png)","b6a897fb":"![si1.png](attachment:si1.png)","9235401a":"### Update the parameters for optimization","27b9d632":"Merge all the function together\n","0bc15f07":"Following methods should be created:-\n\n(1) sigmoid\n\n(2) update_parameters\n\n(3) propogate (Forward and Backward)\n\n\n\nSteps:-\n    (1) Initialize the parameters\n\n   (2) Compute loss\n    Calculate gradient\n    Update parameters\n    ","f8f12bd0":"### Flatten the images","b5b64d0c":"## Read the data","c8ff2e76":"![nn.jpg](attachment:nn.jpg)","00db432c":"Accuracy is upto 60 percent which is not that bad","ee0abb35":"if the diff is more than 0.5, then we will declare value as 1 , otherwise 0"}}