{"cell_type":{"7079da39":"code","c64101e9":"code","b2e0c7a2":"code","f7382bc2":"code","a6fe0fbc":"code","a28d6367":"code","8c585a8b":"code","61330fb5":"code","da5c2a30":"code","8ea01d25":"code","c3dd0bd0":"code","96ce486a":"code","f8c4c095":"code","b7a42d47":"code","833f9e8e":"code","b0b9d572":"markdown","f73cd14f":"markdown","24c8a781":"markdown","8d389faa":"markdown","33a95e89":"markdown","df136193":"markdown","1670530e":"markdown"},"source":{"7079da39":"import numpy as np\nimport pandas as pd\nimport glob # Read File Routes\nfrom tqdm import tqdm\nimport sys, os","c64101e9":"reorganize_data_by_time(\"test\",\"trade\",\"0\")","b2e0c7a2":"DATA_ROOT = \"\/kaggle\/input\/optiver-realized-volatility-prediction\"\nDATA_FEATURE_COUNT = 10\nBATCH_SIZE = 1\nFEATURES_COUNT = 14","f7382bc2":"def log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() ","a6fe0fbc":"def reorganize_data_by_time(mode,data_type,stock=\"*\",path=DATA_ROOT):\n    # data_type = trade \/ book\n    # mode = test \/ train\n    book_paths = glob.glob(f'{path}\/{data_type}_{mode}.parquet\/stock_id={stock}\/*')\n    for path in tqdm(book_paths,desc = \"Reading From parquet:\"):\n        stock_id = int(path.split(\"=\")[1].split(\"\/\")[0])\n        book_df = pd.read_parquet(path)\n        books_by_time = dict()\n\n        for time_id in book_df.time_id.unique():\n            books_by_time[time_id] = book_df[book_df[\"time_id\"] == time_id].reset_index(drop=True).drop(\"time_id\",axis=1)\n\n    return books_by_time\n    \n\ndef load_predict_data(mode, path=DATA_ROOT):\n    if mode != \"train\" and mode != \"test\":\n        raise OSError\n    file_name = f'{path}\/{mode}.csv'\n    return pd.read_csv(file_name)","a28d6367":"\nfor idx , time_id in tqdm(enumerate(trade_data),desc = \"Organizing Data by time\",total = len(trade_data)):\n    stock_data = pd.merge(trade_data[time_id],book_data[time_id],how = 'outer').sort_values(by=['seconds_in_bucket']).fillna(0)\n    stock_data['wap'] = (stock_data['bid_price1'] * stock_data['ask_size1'] +\n                            stock_data['ask_price1'] * stock_data['bid_size1']) \/ (\n                                   stock_data['bid_size1']+ stock_data['ask_size1'])\n    stock_data[\"log_return\"] = log_return(stock_data['wap'])\n    stock_data.fillna(0,inplace = True)\n    # \u5f00\u59cb\u586b\u5145\u6570\u636e\n    pad_data = np.zeros(shape=(800, FEATURES_COUNT))\n    print(stock_data.shape)\n    for index,data in stock_data.iterrows():\n        pad_data[index] = np.array(data)\n    # print(pad_data.shape)\n    stock_all_time_data[idx] = pad_data","8c585a8b":"from torch.utils.data import Dataset\nfrom torch import tensor\n\nclass TrainDataset(Dataset):\n    def __init__(self,mode = \"train\"):\n        self.mode = mode\n        self.target = load_predict_data(self.mode)\n        self.length = self.target.max()[\"stock_id\"]\n        self.max_seq_len = 800\n        self.max_stock_len = 8000\n#         self.trade_data = reorganize_data_by_time(self.mode,'trade')\n#         self.book_data = reorganize_data_by_time(self.mode,'book')\n        # \u8fd9\u4e24\u4e2a\u4e0d\u80fd\u9884\u5148\u52a0\u8f7d \u56e0\u4e3a\u592a\u5927\u4e86\uff0c\u5230\u65f6\u5019\u6839\u636estockid\u52a0\u8f7d\n    # TODO: Logger\n    \n    def __getitem__(self,stock_id):\n        trade_data = reorganize_data_by_time(self.mode,\"trade\",str(stock_id))\n        book_data = reorganize_data_by_time(self.mode,\"book\",str(stock_id))\n        stock_data_lengths = []\n        label = self.target.loc[stock_id, \"target\"]\n        \n        stock_all_time_data = np.zeros(shape=(self.max_stock_len,self.max_seq_len,FEATURES_COUNT))\n        for idx , time_id in tqdm(enumerate(trade_data),desc = \"Organizing Data by time\",total = len(trade_data)):\n            stock_data = pd.merge(trade_data[time_id],book_data[time_id],how = 'outer').sort_values(by=['seconds_in_bucket']).fillna(0)\n            stock_data['wap'] = (stock_data['bid_price1'] * stock_data['ask_size1'] +\n                                    stock_data['ask_price1'] * stock_data['bid_size1']) \/ (\n                                           stock_data['bid_size1']+ stock_data['ask_size1'])\n            stock_data[\"log_return\"] = log_return(stock_data['wap'])\n            stock_data.fillna(0,inplace = True)\n            stock_data_lengths.append(stock_data.shape[0])\n            # \u5f00\u59cb\u586b\u5145\u6570\u636e\n            pad_data = np.zeros(shape=(self.max_seq_len, FEATURES_COUNT))\n            for index,data in stock_data.iterrows():\n                pad_data[index] = np.array(data)\n            # print(pad_data.shape)\n            stock_all_time_data[idx] = pad_data\n        return {\"data\":tensor(stock_all_time_data), \"label\":tensor(label) ,'seq_len': tensor(stock_data_lengths)} # data : (time,seconds,data)\n        # return [tensor(stock_all_time_data), tensor(label) ,len(trade_data)]# data : (time,seconds,data)\n     \n    def __len__(self):\n        return self.length.astype(np.int16)","61330fb5":"from torch.utils.data import DataLoader\ntrain_data = DataLoader(TrainDataset(\"train\"),batch_size = BATCH_SIZE ,shuffle = False)","da5c2a30":"aa = DataLoader(TrainDataset(mode=\"test\"),batch_size = 1 ,shuffle = False)","8ea01d25":"for i in aa:\n    print(i)\n    break","c3dd0bd0":"np.array([1, 4,2,13])[np.argsort(np.array([1,4, 2,13]))[::-1]]","96ce486a":"i[\"seq_len\"][0][order_idx[0]]","f8c4c095":"order_idx[0]","b7a42d47":"from torch.nn.utils.rnn import pack_padded_sequence\nfor batch in i['data']:\n    order_idx = np.array(np.argsort(i[\"seq_len\"]))[::-1]\n    print('order_idx:', str(order_idx))\n    order_x = batch[order_idx.tolist()]\n    order_seq = np.array(i[\"seq_len\"][0][order_idx[0]])\n    # Pack it\n    pack = pack_padded_sequence(order_x, order_seq, batch_first=True ,enforce_sorted=False)\n    i['data '] = pack","833f9e8e":"from torch.nn import RNN\nrnn = RNN(input_size=14, hidden_size=1, num_layers=20)\ncriterion = nn.CrossEntropyLoss()\noptimzier = torch.optim.Adadelta(net.parameters(), 1e-1)\n\ndef get_acc(output, label):\n    total = output.shape[0]\n    _, pred_label = output.max(1)\n    num_correct = (pred_label == label).sum().data\n    # print(num_correct, total)\n    return num_correct\n\ndef train(net, train_data, valid_data, num_epochs, optimizer, criterion):\n    if torch.cuda.is_available():\n        net = net.cuda()\n    for i in range(num_epochs):\n        train_loss = 0\n        train_acc = 0\n        net = net.train()\n        for im, label in train_data:\n            if torch.cuda.is_available():\n                im = Variable(im.cuda())\n                label = Variable(label.cuda())\n            else:\n                im = Variable(im)\n                label = Variable(label)\n            # forward\n            output = net(im)\n            total = output.shape[0]\n            loss = criterion(output, label)\n            # backward\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.data.cpu().numpy()\/float(total)\n            train_acc += get_acc(output, label).cpu().numpy()\/float(total)\n        if valid_data is not None:\n            valid_loss = 0\n            valid_acc = 0\n            net = net.eval()\n            for im, label in valid_data:\n                if torch.cuda.is_available():\n                    im = Variable(im.cuda(), volatile=True)\n                    label = Variable(label.cuda(), volatile=True)\n                else:\n                    im = Variable(im, volatile=True)\n                    label = Variable(label, volatile=True)\n                output = net(im)\n                total = output.shape[0]\n                loss = criterion(output, label)\n                valid_loss += loss.data.cpu().numpy()\/float(total)\n                valid_acc += get_acc(output, label).cpu().numpy()\/float(total)\n            print(\"epoch: %d, train_loss: %f, train_acc: %f, valid_loss: %f, valid_acc:%f\"\n                  % (i, train_loss\/len(train_data),  train_acc\/len(train_data),\n                  valid_loss\/len(valid_data),  valid_acc\/len(valid_data)))\n\n        else:\n            print(\"epoch= \", i, \"train_loss= \", train_loss\/len(train_data), \"train_acc= \", train_acc\/len(train_data))\n# \u5f00\u59cb\u8bad\u7ec3\ntrain(net, train_data, test_data, 10, optimzier, criterion)","b0b9d572":"### \u5bfc\u5165\u9700\u8981\u7684\u5e93","f73cd14f":"## \u7ec4\u5efaDataLoader","24c8a781":"### \u7ec4\u5efaDataLoader","8d389faa":"### \u5b9a\u4e49\u5e38\u91cf","33a95e89":"### \u7ec4\u5efaPackedSequence","df136193":"### \u5b9e\u4f8b\u5316RNN\u7f51\u7edc","1670530e":"### \u5bfc\u5165\u6570\u636e\u5e76\u8fdb\u884c\u9884\u5904\u7406"}}