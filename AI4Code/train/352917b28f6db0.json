{"cell_type":{"0da54077":"code","9d35c48e":"code","a9b1f44a":"code","c6db3928":"code","632248e2":"code","6812fcf2":"code","d59887dc":"code","85d88fc0":"code","2f12cf01":"code","aa8e0541":"code","4af87ead":"code","6e7025ff":"code","8144e5bd":"markdown","ea6b8960":"markdown","1fa28d3f":"markdown","79563645":"markdown","13e6d51f":"markdown","cee524ea":"markdown","7ef423cc":"markdown"},"source":{"0da54077":"!pip install -q mtcnn","9d35c48e":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport random\n\nfrom tqdm import tqdm_notebook as tqdm\ntqdm().pandas()\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.svm import SVC\n\nfrom mtcnn.mtcnn import MTCNN","a9b1f44a":"PATH = '\/kaggle\/input\/5-celebrity-faces-dataset\/train\/ben_afflek\/'\n\nROOT_PATH = '\/kaggle\/input\/5-celebrity-faces-dataset\/'\n\nTRAIN_PATH = '\/kaggle\/input\/5-celebrity-faces-dataset\/train\/'\n\nVAL_PATH = '\/kaggle\/input\/5-celebrity-faces-dataset\/val\/'","c6db3928":"plt.figure(figsize=(10,5))\nfor i, filename in enumerate(os.listdir(PATH)):\n    path = PATH + filename\n    image = plt.imread(path)\n    \n    plt.subplot(2, 7, i+1)\n    \n    plt.axis('off')\n    plt.imshow(image)\n    \nplt.show()","632248e2":"# extract a single face from a given photograph\ndef extract_face(filename, required_size=(160, 160)):\n    image = Image.open(filename)\n    image = image.convert('RGB')\n    pixels = np.asarray(image)\n    \n    detector = MTCNN()\n    results = detector.detect_faces(pixels)\n    \n    # extract the bounding box from the first face\n    x1, y1, width, height = results[0]['box']\n    # bug fix\n    x1, y1 = abs(x1), abs(y1)\n    \n    x2, y2 = x1 + width, y1 + height\n    \n    face = pixels[y1:y2, x1:x2]\n    \n    image = Image.fromarray(face)\n    \n    image = image.resize(required_size)\n    face_array = np.asarray(image)\n    \n    return face_array\n\nplt.figure(figsize=(10,5))\nfor i, filename in tqdm(enumerate(os.listdir(PATH))):\n    path = PATH + filename\n    \n    face = extract_face(path)\n    \n    #print(i+1, face.shape)\n    \n    plt.subplot(2, 7, i+1)\n    \n    plt.axis('off')\n    plt.imshow(face)\n    \nplt.show()","6812fcf2":"def load_faces(directory):\n    faces = list()\n    for file_name in tqdm(os.listdir(directory)):\n        path = directory + file_name\n        face = extract_face(path)\n        \n        faces.append(face)\n    return faces\n\ndef load_dataset(directory):\n    images, labels = list(), list()\n    for folder in tqdm(os.listdir(directory)):\n        path = directory + folder + '\/'\n        \n        if not os.path.isdir(path):\n            continue\n        \n        faces = load_faces(path)\n\n        print(f'Celebrity : {folder}, Faces : {len(faces)}')\n\n        label = [folder for _ in range(len(faces))]\n\n        images.extend(faces)\n        labels.extend(label)\n        \n    return np.asarray(images), np.asarray(labels)","d59887dc":"X_train, y_train = load_dataset(TRAIN_PATH)\n\nX_test, y_test = load_dataset(VAL_PATH)\n","85d88fc0":"face_pixels = X_train[0]\n\nprint(face_pixels.shape)\n\nface_pixels = np.expand_dims(face_pixels, axis=0)\nprint(face_pixels.shape)","2f12cf01":"from keras.models import load_model\n\ndef get_embedding(model, face_pixels):\n    face_pixels = face_pixels.astype('float32')\n    \n    # standardize pixel values across channels (global) \n    mean, std = face_pixels.mean(), face_pixels.std() \n    face_pixels = (face_pixels - mean) \/ std\n    \n    samples = np.expand_dims(face_pixels, axis=0)\n    \n    yhat = model.predict(samples)\n    \n    return yhat[0]\n\nmodel = load_model('\/kaggle\/input\/keras-facenet\/facenet_keras.h5')\n\n\ntrainX = list()\n\nfor pixels in tqdm(X_train):\n    embedding = get_embedding(model, pixels)\n    trainX.append(embedding)\ntrainX = np.asarray(trainX)\n\nprint('Train X :', trainX.shape)\n\n\ntestX = list()\n\nfor pixels in tqdm(X_test):\n    embedding = get_embedding(model, pixels)\n    testX.append(embedding)\ntestX = np.asarray(testX)\n\nprint('test X :', testX.shape)","aa8e0541":"norm = Normalizer(norm='l2')\n\ntrainX = norm.transform(trainX)\ntestX = norm.transform(testX)\n\nlabel = LabelEncoder()\n\ntrainy = label.fit_transform(y_train)\ntesty = label.fit_transform(y_test)\n\nmodel = SVC(kernel='linear', probability=True)\nmodel.fit(trainX, trainy)\n\nyhat_train = model.predict(trainX)\nyhat_test = model.predict(testX)\n\nscore_train = accuracy_score(trainy, yhat_train)\nscore_test = accuracy_score(testy, yhat_test)\n\nprint('Accuracy: train=%.3f, test=%.3f' % (score_train*100, score_test*100))","4af87ead":"# Choose a random index\nrandom.seed(100)\nselection = random.choice([i for i in range(testX.shape[0])])\n\nface = X_test[selection]\n\nyhat_prob = model.predict_proba(testX[selection].reshape(1,-1))\nyhat = model.predict(testX[selection].reshape(1,-1))\n\nceleb = label.inverse_transform(yhat)\n\nplt.imshow(face)\nplt.axis('off')\n\nprint(f'Probability : {np.max(yhat_prob)*100}, \\nCelebrity - Predicted : {celeb}, Acutal : {y_test[selection]}')","6e7025ff":"from sklearn.manifold import TSNE\nimport seaborn as sns\n\ntsne = TSNE(learning_rate=100)\n\ntsne_features = tsne.fit_transform(trainX)\n\nX = tsne_features[:,0]\ny = tsne_features[:,1]\n\ndataset = pd.DataFrame(data=y_train, columns=['label'])\ndataset['X'] = X\ndataset['y'] = y\n\nplt.figure(figsize=(13,8))\nsns.scatterplot(data=dataset, x='X', y='y', hue='label', s=120)","8144e5bd":"<h3><center>4. Create Face Embeddings<\/center><\/h3>\n\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\n    A face embedding is a vector that represents the features extracted from the face.<br>\n    For example, another vector that is close (by some measure) may be the same person, whereas another vector that is far (by some measure) may be a different person. The classifier model that we want to develop will take a face embedding as input and predict the identity of the face. The FaceNet model will generate this embedding for a given image of a face.<\/div>","ea6b8960":"<h3><center>2. Detect Faces using MTCNN<\/center><\/h3>","1fa28d3f":"<h3><center>3. Load Faces<\/center><\/h3>","79563645":"<h3><center>5. Perform Face Classification<\/center><\/h3>","13e6d51f":"<h3><center>6. T-SNE Plot <\/center><\/h3>","cee524ea":"<h3><center>1. Importing Libraries<\/center><\/h3>","7ef423cc":"<h3><center>Introduction<\/center><\/h3>\n\n![image.png](attachment:image.png)\n\n<div style=\"font-family:verdana; word-spacing:1.7px;\">\nFaceNet is a face recognition system developed in 2015 by researchers at Google that achieved then state-of-the-art results on a range of face recognition benchmark datasets.<br><br>    \nThe FaceNet system can be used to extract high-quality features from faces(128 element vector representation), called face embeddings, that can then be used to train a face identification system. In this kernel, we will  develop a face detection system using FaceNet and an SVM classifier to identify people from photographs.\n<\/div>"}}