{"cell_type":{"29a01252":"code","f890fb37":"code","00a574dc":"code","419d2bb4":"code","1833b64c":"code","158a19b1":"code","e324f023":"code","feffab30":"code","23533cec":"code","e4f428d1":"code","7a5b7851":"code","af063bfd":"code","bf0d7821":"code","1d2daf78":"code","062ddd03":"code","1a535513":"code","cb2b7dec":"code","2933752d":"code","2dfdcaf9":"code","eb74a947":"code","775cbac9":"code","18ab6c55":"code","5ca2681c":"code","17aab592":"code","981ba664":"code","71e56b3b":"code","c5dd14b4":"code","c6e651af":"code","a5877636":"code","16a48084":"code","3551078a":"code","01e8d96a":"code","e46a8e06":"code","8724a947":"code","268e47f0":"code","d04d328b":"code","281dccb7":"code","ee2fbbf5":"code","8082dc32":"code","37ec5f1d":"code","da737d88":"code","b6e1f370":"code","9ebc269d":"code","3d15313e":"code","b287961c":"code","33f0978b":"code","b33c25f5":"code","baf3b9a2":"code","ab46929d":"code","6bf1b53f":"code","f5191a69":"code","bb15f684":"code","5d410f40":"code","e03c07d3":"code","584345d1":"code","ba2003d2":"markdown","ceaed4c6":"markdown","c9b8cb98":"markdown","891687bf":"markdown","b407c69f":"markdown","17768b29":"markdown","4fa653ac":"markdown","4f9a578c":"markdown","362f463b":"markdown","0fcd1e5b":"markdown","4fcdf936":"markdown","8c424e6a":"markdown","83342e9c":"markdown","5a5a3f3c":"markdown","25e174c2":"markdown","1365d3e7":"markdown","4d88d843":"markdown","3685e594":"markdown","d330ae5d":"markdown","d0501ee8":"markdown","0ea5b2e1":"markdown","d472e55a":"markdown","a1df1bc0":"markdown","0d5ffc08":"markdown","d5621292":"markdown","84107a9f":"markdown","2bf162fb":"markdown","495d80ed":"markdown","6501a555":"markdown","6bb421af":"markdown","aa449d22":"markdown","af0c5353":"markdown","305d41ca":"markdown","229c3960":"markdown","d8c95a4a":"markdown","02f248ae":"markdown","552f20f0":"markdown","117babd9":"markdown","80eab571":"markdown","a9cd1aee":"markdown","d77a6aea":"markdown","50a7eb9b":"markdown","ba9b004f":"markdown","b844b04f":"markdown","a44e005e":"markdown","be189eab":"markdown","3371dd27":"markdown","bc736832":"markdown","6256d643":"markdown","3100006b":"markdown","aa101af5":"markdown","7a814ac5":"markdown","edd24966":"markdown","2c62e3b5":"markdown"},"source":{"29a01252":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","f890fb37":"path_train = '..\/input\/train.csv'\npath_test = '..\/input\/train.csv'","00a574dc":"df_train = pd.read_csv(path_train)\ndf_test = pd.read_csv(path_test)","419d2bb4":"df = df_train","1833b64c":"df.head()","158a19b1":"df.info()","e324f023":"null_columns=df.columns[df.isnull().any()]\ndf[null_columns].isnull().sum()","feffab30":"def lotfrontage_cleaner():\n    neighborhoods = df['Neighborhood'].drop_duplicates().reset_index(drop=True)\n\n    for neighborhood in neighborhoods:\n        median = df['LotFrontage'].loc[df['Neighborhood'] == neighborhood].median()\n        df['LotFrontage'].loc[df['Neighborhood'] == neighborhood] = df['LotFrontage'].loc[df['Neighborhood'] == neighborhood].fillna(median)","23533cec":"#define function to filter df by only the null columns in a chosen column\ndef null_rowsincolumn(df, null_columns, chosen_column):\n    \n    filtered_df = df[null_columns][df[null_columns][chosen_column].isnull()]\n    return(filtered_df)","e4f428d1":"df['Alley'].value_counts()","7a5b7851":"#define function to replace non null values with Alley & null values with no_alley\ndef alley_cleaner():\n    df.loc[df['Alley'].notnull(), 'Alley'] = 'alley'\n    df['Alley'].fillna('no_alley', inplace=True)","af063bfd":"null_rowsincolumn(df, null_columns, 'MasVnrType')","bf0d7821":"#define function to replace NA in MasVnrType with none & in MasVnrArea with 0\ndef MasVnr_cleaner():\n    df['MasVnrType'].fillna('None', inplace=True)\n    df['MasVnrArea'].fillna(0 , inplace=True)","1d2daf78":"null_rowsincolumn(df, null_columns, 'BsmtExposure')","062ddd03":"#define function to drop any row where not all bsmt attributes are NA \ndef bsmt_cleaner():\n    columns = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\n    df.drop(df[(df[columns].isnull().sum(axis=1) < len(columns)) & (df[columns].isnull().sum(axis=1) > 0)].index, inplace=True)\n    df[columns] = df[columns].fillna(value = 'no_basement')","1a535513":"def fireplace_cleaner():\n    df['FireplaceQu'][(df['FireplaceQu'].isnull()) & (df['Fireplaces'] == 0)] = df['FireplaceQu'][(df['FireplaceQu'].isnull()) & (df['Fireplaces'] == 0)].fillna('None')","cb2b7dec":"#define function to drop any row where not all bsmt attributes are NA \ndef garage_cleaner():\n    columns = ['GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual', 'GarageCond']\n    #fill all garage attributes (in above column array) with none for rows where Garage Area = 0\n    df[columns][df['GarageArea'] == 0] = df[columns][df['GarageArea'] == 0].fillna(value = 'none')\n    df.drop(df[(df[columns].isnull().sum(axis=1) < len(columns)) & (df[columns].isnull().sum(axis=1) > 0)].index, inplace=True)\n    df[columns] = df[columns].fillna(value = 'none')","2933752d":"def pool_cleaner():\n    \n    df['PoolQC'][df['PoolArea'] == 0] = df['PoolQC'][df['PoolArea'] == 0].fillna(value = 'None')\n    df['PoolQC'].fillna(value='None', inplace=True)","2dfdcaf9":"def fence_cleaner():\n    \n    df['Fence'].fillna('none', inplace=True)","eb74a947":"def misc_cleaner():\n    \n    df['MiscFeature'].fillna('none', inplace=True)","775cbac9":"def final_cleaner():\n    \n    df.fillna(value=0, inplace=True)","18ab6c55":"lotfrontage_cleaner()\nalley_cleaner()\nMasVnr_cleaner()\nbsmt_cleaner()\nfireplace_cleaner()\npool_cleaner()\nfence_cleaner()\ngarage_cleaner()\nmisc_cleaner()\n\nfinal_cleaner()","5ca2681c":"df[null_columns].isnull().sum()","17aab592":"df.hist(bins=30, figsize=(20,15));\nplt.tight_layout()","981ba664":"# We will get all numerical columns and assign them to df_columns\ndf_columns = df._get_numeric_data()\ncolumns = df_columns.columns\ncolumns","71e56b3b":"for col in columns:\n    \n    sns.jointplot(x=col, y='SalePrice', kind='reg', data=df);\n    plt.show()\n","c5dd14b4":"#drop irrelevant columns:\ncolumns_drop = ['PoolArea', 'MiscVal', 'MoSold', 'YrSold', '3SsnPorch', 'EnclosedPorch', 'BsmtHalfBath', \n                'LowQualFinSF', 'BsmtFinSF2']\ndf.drop(columns = columns_drop, inplace=True)","c6e651af":"df['2ndFlrSF'][df['2ndFlrSF'] > 0] = 1\ndf['2ndFlrSF'][df['2ndFlrSF'] == 0] = 0","a5877636":"df['OpenPorchSF'][df['OpenPorchSF'] > 0] = 1\ndf['OpenPorchSF'][df['OpenPorchSF'] == 0] = 0","16a48084":"df['ScreenPorch'][df['ScreenPorch'] > 0] = 1\ndf['ScreenPorch'][df['ScreenPorch'] == 0] = 0","3551078a":"df['WoodDeckSF'][df['WoodDeckSF'] > 0] = 1\ndf['WoodDeckSF'][df['WoodDeckSF'] == 0] = 0","01e8d96a":"df.set_index('Id', inplace=True)","e46a8e06":"df.head()","8724a947":"#Create a dictionary with the values & their interpretations:\nsubclass_dict = {\n    20: '1-STORY 1946 & NEWER ALL STYLES',\n    30: '1-STORY 1945 & OLDER',\n    40: '1-STORY W\/FINISHED ATTIC ALL AGES',\n    45: '1-1\/2 STORY - UNFINISHED ALL AGES',\n    50: '1-1\/2 STORY FINISHED ALL AGES',\n    60: '2-STORY 1946 & NEWER',\n    70: '2-STORY 1945 & OLDER',\n    75: '2-1\/2 STORY ALL AGES',\n    80: 'SPLIT OR MULTI-LEVEL',\n    85: 'SPLIT FOYER',\n    90: 'DUPLEX - ALL STYLES AND AGES',\n    120: '1-STORY PUD (Planned Unit Development) - 1946 & NEWER',\n    150: '1-1\/2 STORY PUD - ALL AGES',\n    160: '2-STORY PUD - 1946 & NEWER',\n    180: 'PUD - MULTILEVEL - INCL SPLIT LEV\/FOYER',\n    190: '2 FAMILY CONVERSION - ALL STYLES AND AGES',\n}","268e47f0":"# Replace the values with their interpretations\ndf['MSSubClass'].replace(subclass_dict, inplace=True)","d04d328b":"df['MasVnrArea'][df['MasVnrArea'] < 50] = 50\ndf['MasVnrArea'][(df['MasVnrArea'] > 50) & (df['MasVnrArea'] <200)] = 200\ndf['MasVnrArea'][(df['MasVnrArea'] > 200) & (df['MasVnrArea'] <500)] = 500\ndf['MasVnrArea'][df['MasVnrArea'] > 500] = 1600\n\nmasvnrarea_dict = {\n    50: '50',\n    200: '50-200',\n    500: '200-500',\n    1600: '500+'\n}\ndf['MasVnrArea'].replace(masvnrarea_dict, inplace=True)","281dccb7":"df['BsmtUnfSF'][df['BsmtUnfSF'] < 500] = 500\ndf['BsmtUnfSF'][(df['BsmtUnfSF'] > 500) & (df['BsmtUnfSF'] <1000)] = 1000\ndf['BsmtUnfSF'][(df['BsmtUnfSF'] > 1000) & (df['BsmtUnfSF'] <1500)] = 1500\ndf['BsmtUnfSF'][df['BsmtUnfSF'] > 1500] = 2500\n\nbsmtunfsf_dict = {\n    500: '500',\n    1000: '500-1000',\n    1500: '1000-1500',\n    2500: '1500+'\n}\ndf['BsmtUnfSF'].replace(bsmtunfsf_dict, inplace=True)","ee2fbbf5":"cat_columns = df.loc[:, df.dtypes == object].columns","8082dc32":"df = pd.get_dummies(df, columns = cat_columns, drop_first=True)","37ec5f1d":"columns_transform = ['1stFlrSF', 'GarageArea', 'GrLivArea', 'LotArea', 'LotFrontage', 'TotalBsmtSF', 'SalePrice']","da737d88":"for col in columns_transform:\n    df[col] = np.log(df[col]+1)\n    df[col].plot(kind='hist', bins=50, fontsize = 20, figsize= (20, 10), edgecolor='black', linewidth=1.2)\n    plt.xlabel(col, fontsize=20)\n    plt.ylabel('Frequency', fontsize=20)\n    plt.show()","b6e1f370":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.externals import joblib","9ebc269d":"X = df.drop(columns=['SalePrice'])\ny = df['SalePrice']","3d15313e":"scaler = MinMaxScaler()","b287961c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","33f0978b":"ridge = Ridge()\nridge.fit(X_train, y_train)\npreds_train = ridge.predict(X_train)\npreds_test = ridge.predict(X_test)\nrmse_train = np.sqrt(mean_squared_error(y_train, preds_train))\nrmse_test = np.sqrt(mean_squared_error(y_test, preds_test))\nprint(\"RMSE: %f\" % (rmse_train))\nprint(\"RMSE: %f\" % (rmse_test))","b33c25f5":"lasso = Lasso()\nlasso.fit(X_train, y_train)\npreds_train = lasso.predict(X_train)\npreds_test = lasso.predict(X_test)\nrmse_train = np.sqrt(mean_squared_error(y_train, preds_train))\nrmse_test = np.sqrt(mean_squared_error(y_test, preds_test))\nprint(\"RMSE: %f\" % (rmse_train))\nprint(\"RMSE: %f\" % (rmse_test))","baf3b9a2":"rfr = RandomForestRegressor()\nrfr.fit(X_train, y_train)\npreds_train = rfr.predict(X_train)\npreds_test = rfr.predict(X_test)\nrmse_train = np.sqrt(mean_squared_error(y_train, preds_train))\nrmse_test = np.sqrt(mean_squared_error(y_test, preds_test))\nprint(\"RMSE: %f\" % (rmse_train))\nprint(\"RMSE: %f\" % (rmse_test))","ab46929d":"xg = xgb.XGBRegressor(learning_rate=0.08, n_estimators=1000)\nxg.fit(X_train, y_train, eval_metric='rmse')\npreds_train = xg.predict(X_train)\npreds_test = xg.predict(X_test)\nrmse_train = np.sqrt(mean_squared_error(y_train, preds_train))\nrmse_test = np.sqrt(mean_squared_error(y_test, preds_test))\nprint(\"RMSE: %f\" % (rmse_train))\nprint(\"RMSE: %f\" % (rmse_test))","6bf1b53f":"params = {\n    'learning_rate': [0.01, 0.1, 0.5],\n    'n_estimators': [500, 800, 1000],\n}\n\ncv_xg = GridSearchCV(xg, params, cv=5, scoring='neg_mean_squared_error')\ncv_xg.fit(X_train, y_train)\n\n\nprint(\"Tuned parameters {}\".format(cv_xg.best_params_))","f5191a69":"params = {\n    'n_estimators': [10,25],\n    'max_depth': [10,50, None],\n    'max_features': [5,10,15,20,25],\n    'bootstrap': [True, False]\n}\n\ncv_rfr = GridSearchCV(rfr, params, cv=10, scoring='neg_mean_squared_error')\ncv_rfr.fit(X_train, y_train)\n\n\nprint(\"Tuned parameters {}\".format(cv_rfr.best_params_))","bb15f684":"#Getting the importances from grid search\nimportances = cv_rfr.best_estimator_.feature_importances_\n\n#getting feature names\nfeatures = list(X.columns)\n\nfeature_importance = sorted(zip(importances, features), reverse=True)\n\ndf_importances = pd.DataFrame(feature_importance, columns = ['importances', 'features'])\n\ndf_importances.head(30)","5d410f40":"rfr_tuned = RandomForestRegressor(n_estimators=50, max_features=125, max_depth=None, min_samples_leaf=3, bootstrap=False)\n\nrfr_tuned.fit(X_train,y_train)\npreds_train = rfr_tuned.predict(X_train)\npreds_test = rfr_tuned.predict(X_test)\n\nrmse_train = np.sqrt(mean_squared_error(y_train, preds_train))\nrmse_test = np.sqrt(mean_squared_error(y_test, preds_test))\n\nprint(\"RMSE: %f\" % (rmse_train))\nprint(\"RMSE: %f\" % (rmse_test))","e03c07d3":"preds_train = cv_xg.predict(X_train)\npreds_test = cv_xg.predict(X_test)\n\nrmse_train = np.sqrt(mean_squared_error(y_train, preds_train))\nrmse_test = np.sqrt(mean_squared_error(y_test, preds_test))\n\nprint(\"RMSE: %f\" % (rmse_train))\nprint(\"RMSE: %f\" % (rmse_test))","584345d1":"X = scaler.fit_transform(X)\nxg.fit(X, y)\ncv_xg.fit(X, y)\nrfr.fit(X, y)\nrfr_tuned.fit(X,y)\nridge.fit(X,y)\nlasso.fit(X,y)","ba2003d2":"### Now we will deal with the columns we will change to yes\/no (0,1):\n   * 2ndFlrSF\n   * OpenPorchSF\n   * ScreenPorch\n   * WoodDeckSF","ceaed4c6":"### Next we try Random Forest Regressor","c9b8cb98":"### Let's try some hyper parameter tuning on Random Forrest regressor","891687bf":"### Split the data with an 70\/30 ratio to train on part of the data set & test on the other","b407c69f":"### First let's replace MSSubClass with their actual descriptions:\n\n        20\t1-STORY 1946 & NEWER ALL STYLES\n        30\t1-STORY 1945 & OLDER\n        40\t1-STORY W\/FINISHED ATTIC ALL AGES\n        45\t1-1\/2 STORY - UNFINISHED ALL AGES\n        50\t1-1\/2 STORY FINISHED ALL AGES\n        60\t2-STORY 1946 & NEWER\n        70\t2-STORY 1945 & OLDER\n        75\t2-1\/2 STORY ALL AGES\n        80\tSPLIT OR MULTI-LEVEL\n        85\tSPLIT FOYER\n        90\tDUPLEX - ALL STYLES AND AGES\n       120\t1-STORY PUD (Planned Unit Development) - 1946 & NEWER\n       150\t1-1\/2 STORY PUD - ALL AGES\n       160\t2-STORY PUD - 1946 & NEWER\n       180\tPUD - MULTILEVEL - INCL SPLIT LEV\/FOYER\n       190\t2 FAMILY CONVERSION - ALL STYLES AND AGES\n\n","17768b29":"### Now lets check which values are categorical and convert all categorical values to dummies","4fa653ac":"### Now we will transform columns that require to be normalized:\n\n  * 1stFlrSF\n  * GarageArea\n  * GrLivArea\n  * LotArea\n  * LotFrontage\n  * TotalBsmtSF\n  * SalePrice (need to do np.exp() on sales values after ML to return to original values)","4f9a578c":"### We will start by dropping the irrelevant features","362f463b":"### Both MasVnrType & Area have N\/A in the same indicies, so we can safely assume that N\/A genuinely means None (the data is not missing it is actually None)\n### we will define a function to replace NA with None","0fcd1e5b":"### Let's check directly by Basement Exposure since we saw previously that there was one extra value misisng in Bsmt Exposure & also finish type 2 when we checked for missing values in the whole df","4fcdf936":"### For Garagetype attributes, we will write a function to:\n   1. fill all na rows in garage attribetus where Garage Area = 0 with no garage since those genuinely mean there is no garage\n   2. consider the row as a genuinely misisng if N\/A is not in all garage attributes","8c424e6a":"# Machine Learning","83342e9c":"### Now we will deal with the remaining categorical columns from initial analysis after dropping the rest that we need to change to categories then dummies:\n* MSSubClass (can convert to text values from description first if needed for better readability)\n* MasVnrArea (convert to ranges of values first, ex: from 0 to 100 sf, then 100 to 500sf... etc.)\n* BsmtUnfSF (convert to ranges of values first, ex: from 0 to 100 sf, then 100 to 500sf... etc.)\n* BedroomAbvGr\n* BsmtFullBath\n* Fireplaces\n* FullBath\n* GarageCars\n* HalfBath\n* OverallCond\n* OverallQual\n* TotRmsAbvGrd","5a5a3f3c":"### Let's try some hyper parameter tuning on XGBoost","25e174c2":"### For PoolQC we will assume N\/A means no pool if Pool Area attribute is = 0 else we will drop the values","1365d3e7":"## From the Above plots we can deduce the below insights:\n\n### ID: Irrelevant corr. with Sales (will be changed to become index later)\n\n### Features considered mostly relevant (or can be made relevant by making them categorical or yes\/no):\n   * MSSubClass: slightly downward trend (r= -0.084), but considered irrelevant\n   * LotFrontage: upward trend (r= 0.35), but variability is very high\n   * LotArea: slightly upward trend (r= 0.26), variability very high\n   * OverallQual: strong clear upward trend (r= 0.79)\n   * OverallCond: no clear trend, high variability\n   * YearBuilt: upward clear trend (r= 0.52) with some variability\n   * YearRemodAdd: upward clear trend (r= 0.51) with some variability (very similar to YearBuilt)\n   * MasVnrArea: upward trend (r= 0.47) with high variability\n   * BsmtFinSF1: upward trend (r= 0.32) with very high variability\n   * TotalBsmtSF: clear upward trend (r= 0.62)\n   * 1stFlrSF: clear upward trend (r= 0.61)\n   * 2ndFlrSF: slightly upward trend (r=0.32), very high variability\n   * GrLivArea: very strong upward trend (r= 0.71)\n   * FullBath: slightly upward trend (r= 0.56)\n   * HalfBath: slightly upward but irrelevant trend\n   * BedroomAbvGr: slightly upward but irrelevant trend\n   * TotRmsAbvGrd: slightly upward trend (r= 0.53)\n   * Fireplaces: slightly upward trend (r= 0.47)\n   * GarageCars: slightly upward trend (r= 0.64)\n   * GarageArea: clear upward trend (r= 0.62)\n   * WoodDeckSF: slightly upward trend (r= 0.33), high variability\n   * OpenPorchSF: slightly upward trend (r= 0.32), high variability\n\n\n\n### Features I am not sure about so we'll later try ML with & without to see which gives better results:\n   * ScreenPorch: slightly upward but irrelevant trend\n   * BsmtFullBath: no clear trend (try ML with & without)\n   * BsmtUnfSF: slightly upward trend (r= 0.21), high variability (doesnt make sense as price goes up with unfinished SF)\n   * KitchenAbvGr: slightly upward but irrelevant trend\n    \n    \n### Features that are considered irrelevant (to be dropped):\n   * PoolArea: no trend\n   * MiscVal: no trend\n   * MoSold: no trend\n   * YrSold: no trend\n   * 3SsnPorch: no trend\n   * EnclosedPorch: slightly downward but irrelevant trend\n   * BsmtHalfBath: no clear trend\n   * LowQualFinSF: no trend\n   * BsmtFinSF2: no trend","4d88d843":"### lets examine the alley column","3685e594":"### Next we try Lasso regression","d330ae5d":"### It is clear that Bsmt attributes (quality, condition, exposure, basement finish type 1 & type 2) have missing values in the same indicies, but there is a row where N\/A value is only present in BsmtExposure & finish type 2 in index 948, so we will have to write a function to dropna when not all bsmt attributes have N\/A values","d0501ee8":"### First let's deal with N\/A in LotFrontage\n * We will replace the median of lot frontage for each neighborhood","0ea5b2e1":"### For numerical values like LotFrontage it is obvious that N\/A is actually missing.\n### For categorical values like alley, Pool... etc.:\n   * The dataset description mentions that N\/A as the feature not being present (for example no pool, no alley... etc.) \n   * This needs to be checked to make sure the value is not genuinely missing","d472e55a":"### For Alley we will just change it to Alley or No Alley (consider N\/A) to be no alley","a1df1bc0":"### Second let's deal with N\/A in categorical values ","0d5ffc08":"## From the above histograms we can conclude some insights:\n\n   ### 1. Some attributes are skewed and will probably require some transformation like log transformation:\n   * 1stFlrSF\n   * GarageArea\n   * GrLivArea\n   * LotArea\n   * LotFrontage\n   * TotalBsmtSF\n   * SalePrice (remember to return to pre-transformation values after predictions on test set)\n   \n   ### 2. Some data will make more sense if we change it to yes\/no:\n   * 2ndFlrSF\n   * 3SsnPorch\n   * EnclosedPorch\n   * OpenPorchSF\n   * LowQualFinSF\n   * ScreenPorch\n   * WoodDeckSF\n   ### 3. Some data will make more sense when we change it to be categorical binary data using pd_dummies:\n   \n   * BedroomAbvGr\n   * BsmtFullBath\n   * BsmtHalfBath\n   * BsmtUnfSF (convert to ranges of values first, ex: from 0 to 100 sf, then 100 to 500sf... etc.)\n   * Fireplaces\n   * FullBath\n   * GarageCars\n   * HalfBath\n   * KitchenAbvGr\n   * MSSubClass (can convert to text values from description first if needed for better readability)\n   * MasVnrArea (convert to ranges of values first, ex: from 0 to 100 sf, then 100 to 500sf... etc.)\n   * OverallCond\n   * OverallQual\n   * TotRmsAbvGrd\n   \n   ### 4. Some data are better off just dropped since they are referenced in another attribute implicitly:\n   * BsmtFinSF1\n   * BsmtFinSF2\n   * MiscVal\n   * PoolArea\n   \n   ### 5. Some data are better off just dropped as they are just meaningless in the analysis:\n   * MoSold\n        \n   ### 6. Some attributes will be more clear after correlating to Sales Price:\n   * YearBuilt\n   * YearRemodAdd\n   * YrSold\n        \n   ### For the ID column we will set it to be the index","d5621292":"### We will change ID to be the index","84107a9f":"### Now let's fit the XGBoost with the tuned parameters and check the RMSE\nNote: I took the values from GridSearch as a base line then I did some tuning manually to try to reduce the difference between RMSE train & RMSE test to reduce overfitting","2bf162fb":"Data fields\n\nHere's a brief version of what you'll find in the data description file.\n\n    SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\n    MSSubClass: The building class\n    MSZoning: The general zoning classification\n    LotFrontage: Linear feet of street connected to property\n    LotArea: Lot size in square feet\n    Street: Type of road access\n    Alley: Type of alley access\n    LotShape: General shape of property\n    LandContour: Flatness of the property\n    Utilities: Type of utilities available\n    LotConfig: Lot configuration\n    LandSlope: Slope of property\n    Neighborhood: Physical locations within Ames city limits\n    Condition1: Proximity to main road or railroad\n    Condition2: Proximity to main road or railroad (if a second is present)\n    BldgType: Type of dwelling\n    HouseStyle: Style of dwelling\n    OverallQual: Overall material and finish quality\n    OverallCond: Overall condition rating\n    YearBuilt: Original construction date\n    YearRemodAdd: Remodel date\n    RoofStyle: Type of roof\n    RoofMatl: Roof material\n    Exterior1st: Exterior covering on house\n    Exterior2nd: Exterior covering on house (if more than one material)\n    MasVnrType: Masonry veneer type\n    MasVnrArea: Masonry veneer area in square feet\n    ExterQual: Exterior material quality\n    ExterCond: Present condition of the material on the exterior\n    Foundation: Type of foundation\n    BsmtQual: Height of the basement\n    BsmtCond: General condition of the basement\n    BsmtExposure: Walkout or garden level basement walls\n    BsmtFinType1: Quality of basement finished area\n    BsmtFinSF1: Type 1 finished square feet\n    BsmtFinType2: Quality of second finished area (if present)\n    BsmtFinSF2: Type 2 finished square feet\n    BsmtUnfSF: Unfinished square feet of basement area\n    TotalBsmtSF: Total square feet of basement area\n    Heating: Type of heating\n    HeatingQC: Heating quality and condition\n    CentralAir: Central air conditioning\n    Electrical: Electrical system\n    1stFlrSF: First Floor square feet\n    2ndFlrSF: Second floor square feet\n    LowQualFinSF: Low quality finished square feet (all floors)\n    GrLivArea: Above grade (ground) living area square feet\n    BsmtFullBath: Basement full bathrooms\n    BsmtHalfBath: Basement half bathrooms\n    FullBath: Full bathrooms above grade\n    HalfBath: Half baths above grade\n    Bedroom: Number of bedrooms above basement level\n    Kitchen: Number of kitchens\n    KitchenQual: Kitchen quality\n    TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n    Functional: Home functionality rating\n    Fireplaces: Number of fireplaces\n    FireplaceQu: Fireplace quality\n    GarageType: Garage location\n    GarageYrBlt: Year garage was built\n    GarageFinish: Interior finish of the garage\n    GarageCars: Size of garage in car capacity\n    GarageArea: Size of garage in square feet\n    GarageQual: Garage quality\n    GarageCond: Garage condition\n    PavedDrive: Paved driveway\n    WoodDeckSF: Wood deck area in square feet\n    OpenPorchSF: Open porch area in square feet\n    EnclosedPorch: Enclosed porch area in square feet\n    3SsnPorch: Three season porch area in square feet\n    ScreenPorch: Screen porch area in square feet\n    PoolArea: Pool area in square feet\n    PoolQC: Pool quality\n    Fence: Fence quality\n    MiscFeature: Miscellaneous feature not covered in other categories\n    MiscVal: $Value of miscellaneous feature\n    MoSold: Month Sold\n    YrSold: Year Sold\n    SaleType: Type of sale\n    SaleCondition: Condition of sale\n","495d80ed":"### Now let's fit the random forest regressor with the tuned parameters and check the RMSE\nNote: I took the values from GridSearch as a base line then I did some tuning manually to try to reduce the difference between RMSE train & RMSE test to reduce overfitting","6501a555":"### Final notes:\n* Further improvement can probably be achieved by dropping some of the irrelevant feautres\n* Dropping outliers most likely will also improve the score","6bb421af":"### Let's confirm that there are no null values anymore","aa449d22":" ### For Fireplace Quality we will write a function to check FireplaceQu against number of fire places (if the N\/A values are adjacent to the 0 number of fireplaces then N\/A actually means no fireplace else we will drop the value)","af0c5353":"### For  Fence we will just assume N\/A means no fence (at the moment I don't find a reasonable way to check if the value is actually missing)","305d41ca":"## Let's try Several regression algorithms and see which gives the best results","229c3960":"### Now lets convert MasVnrArea to four ranges as follows:\n\n#### 0-50, 50-200, 200-500, 500+","d8c95a4a":"### For MiscFeature we will also consider N\/A to mean no misc feature","02f248ae":"### Now let's examine the MasVnrType & MaxVnrArea","552f20f0":"### Finally lets do the same to BsmtUnfSF:\n\n#### 500, 500-1000, 1000-1500, 1500+","117babd9":"### Looking at the RMSE values above it is obvious that all models (except maybe Lasso) are over fitting the data, that's because there is a big difference between RMSE Train & RMSE Test","80eab571":"### Finally we will define a function to drop all remaining N\/A values","a9cd1aee":"### We will use cubic root transformation below","d77a6aea":"### Import Libraries","50a7eb9b":"### Finall we will try XGboost regressor","ba9b004f":"# Exploratory data analysis\n## Lets start by checking the distribution of the data","b844b04f":"### Let's check feature importances","a44e005e":"### Create X df for features & y for target variable","be189eab":"### We will start by trying Ridge Regression","3371dd27":"### For the electrical attribute we only have 1 missing row so we will just drop that since it doesn't make sense to actually be \"None\" (we will just dropna for all columns anyway after manually imputing values in columns where needed)","bc736832":"### Let's see the correlation between all numerical columns vs. Sales Price","6256d643":"### Now let's run all the functions","3100006b":"### Import Libraries","aa101af5":"# Data Clearning\n## Let's deal with the missing values","7a814ac5":"### Now let's fit the models to the full dataset","edd24966":"### Next let's examine BsmtQual, Cond & Exposure","2c62e3b5":"### Load csv files"}}