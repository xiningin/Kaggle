{"cell_type":{"ddb455f9":"code","6854c78a":"code","06e5704d":"code","4f69e407":"code","b349ddff":"code","621033a3":"code","0283d9d7":"code","cff205b4":"code","60a116d1":"code","ed6117da":"code","345511d6":"code","94ba1631":"markdown","4cdb3395":"markdown","787e6cf6":"markdown","01310c9c":"markdown","cb70186a":"markdown","2af94d66":"markdown","64d09857":"markdown","d16b943a":"markdown","26978b7a":"markdown","b70f4bfa":"markdown"},"source":{"ddb455f9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6854c78a":"# Import Packages\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nfrom IPython.html.widgets import interact\nimport seaborn as sns\nsns.set(color_codes=True,rc={'figure.figsize':(11.7,8.27)})","06e5704d":"data=pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\ndata.head()","4f69e407":"print(\"Dimensions of Data => \", data.shape)","b349ddff":"data.isnull().sum()","621033a3":"data.columns","0283d9d7":"X=data.drop([\"label\"],axis=1)\ny=data.label","cff205b4":"# project from 784 to 10 dimensions (just to look on to how good 10 dimensions can classify data..)\npca = PCA(10)  \nXproj = pca.fit_transform(X)\nprint(X.shape)\nprint(Xproj.shape)","60a116d1":"# Creating a scatter plot of the datapoints\nplt.scatter(Xproj[:, 0], Xproj[:, 1], c=y, edgecolor='none', alpha=0.5,\n            cmap=plt.cm.get_cmap(\"nipy_spectral\",10))\nplt.colorbar();","ed6117da":"pca=PCA().fit(X)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('No. of principal components')\nplt.ylabel('cumulative explained variance')","345511d6":"# project from 784 to 200 dimensions (as 200 is the optimal number of principal components,\n#                                                           found through Elbow Method in Scree Plot)\npca = PCA(200)  \nXproj = pca.fit_transform(X)\nprint(\"Shape of new data =>\",Xproj.shape)\n\n# Creating a scatter plot of the datapoints\nplt.scatter(Xproj[:, 0], Xproj[:, 1], c=y, edgecolor='none', alpha=0.5,\n            cmap=plt.cm.get_cmap(\"nipy_spectral\",10))\nplt.colorbar();","94ba1631":"## AIM :-\nThe main aim of this notebook is to implement and classify data using principal component analysis (PCA), therefore data preprocessing is not given high priority in this notebook.","4cdb3395":"### Observations :-\n\nSeeing this colormap we can visualize that only using 10 dimensions we are able to classify the datapoints to some extent.<br>\nSo, we thereby need to find the optimal value of number of dimensions (principal components) by which data can be easily classified. <br>\nbut what this really means is\n\n$$\nimage(x) = x_1 \\cdot{\\rm (pixel~1)} + x_2 \\cdot{\\rm (pixel~2)} + x_3 \\cdot{\\rm (pixel~3)} \\cdots\n$$\n\nIf we reduce the dimensionality in the pixel space to (say) 10, we recover only a partial image. <br>\nHere we see that with only ten PCA components, we recover a reasonable approximation of the input!","787e6cf6":"### Observation ..\n\nFrom the Scree plot, it can be seen that **200 components** are required to explain about **95% of the variance** which is still better than computing using all the 784 features. <br>\nThe explained variance threshold can be choosen based on the doamin and business requirements.","01310c9c":"### Observation :-\nAs we can see data has large dimensions (785 features), there we can try using PCA and see if we can get the nearly same results with reduced dimensioned data or not.","cb70186a":"**Pros of PCA:**\n\n- Correlated features are removed.\n- Model training time is reduced.\n- Overfitting is reduced.\n- Helps in better visualizations\n- Ability to handle noise\n\n**Cons of PCA**\n- The resultant principal components are less interpretable than the original data\n- Can lead to information loss if the explained variance threshold is not considered appropriately.\n","2af94d66":"## Implementation of PCA on Digits Dataset\n\nThe dimensionality reduction might seem a bit abstract in two dimensions, but the projection and dimensionality reduction can be extremely useful when visualizing high-dimensional data. Let's implement PCA to the digits data. This data consists of a collection of different points in the plane to represent a digit.","64d09857":"### PCA for data compression\n\nAs mentioned, PCA can be used for a sort of data compression as well. Using a smaller value of ``n_components`` allows you to represent a higher dimensional point as a sum of just a few principal component vectors.\n","d16b943a":"## Principal Component Analysis: \nThe principal component analysis is an unsupervised machine learning algorithm used for feature selection using dimensionality reduction techniques. As the name suggests, it finds out the principal components from the data. PCA transforms and fits the data from a higher-dimensional space to a new, lower-dimensional subspace This results into an entirely new coordinate system of the points where the first axis corresponds to the first principal component that explains the most variance in the data.\n\n**What are the principal components?**\nPrincipal components are the derived features which explain the maximum variance in the data. The first principal component explains the most variance, the 2nd a bit less and so on. Each of the new dimensions found using PCA is a linear combination of the old features.\n\nLet's take the following example where the data is distributed like the diagram on the left:\n![PCA_intro1.png](attachment:431eebf0-b991-48f8-946a-c9208415a148.png)\nIn the diagram above, we are considering 3 orthogonal(_C3  is in the third dimension_) axes to show the distribution of data. If you notice the diagram on the right, the first two axes **C1** and **C2** successfully explain the maximum variation in the data whereas the axes **C3** only consists of a fewer number of points. Hence, while considering the principal components C1 and C2 will be our choices.","26978b7a":"### Conclusion\nFrom all the explanations above, we can conclude that PCA is a very powerful technique for reducing the dimensions of the data, projecting the data from a higher dimension to a lower dimension, helps in data visualization, helps in data compression and most of all increases the model training speed drastically by decreasing the number of variables involved in computation.","b70f4bfa":"### Choosing the Number of Components\n\nBut how much information have we thrown away? <br> We can figure this out by looking at the **explained variance** as a function of the components:\n"}}