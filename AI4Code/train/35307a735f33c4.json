{"cell_type":{"1d96673b":"code","e74c05a4":"code","a06cc9e9":"code","6cbdc172":"code","ee7437b5":"code","b7460a2a":"code","00446ecb":"code","87dd1c79":"code","d14d720f":"code","04440986":"code","31bb7f2a":"code","0781fcb8":"code","caab69f9":"code","8e547674":"code","8c75b4e0":"code","192a5891":"code","4d7760ff":"code","c7754bff":"code","0bb09161":"code","19bb78a3":"code","7c1c2281":"code","acae7927":"code","4d224c11":"code","dae2be8a":"code","a9fd818e":"code","0e1bdbc1":"code","039ad8f2":"code","80532be4":"code","1e4f9766":"code","0740614e":"code","285a1129":"code","df0c72cd":"code","f9779d79":"code","c000563c":"code","bc74e199":"code","4ea92897":"markdown","8ddda2fd":"markdown","299495a0":"markdown","14d220e1":"markdown","95dab179":"markdown"},"source":{"1d96673b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e74c05a4":"#Lets import some libraries that we will use\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow.keras as tf\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n# To shift lists\nfrom collections import deque","a06cc9e9":"# Load single data-file\ndf_raw = pd.read_csv('..\/input\/netflix-prize-data\/combined_data_2.txt', header=None, names=['User', 'Rating', 'Date'], usecols=[0, 1, 2])\n\n\n# Find empty rows to slice dataframe for each movie\ntmp_movies = df_raw[df_raw['Rating'].isna()]['User'].reset_index()\nmovie_indices = [[index, int(movie[:-1])] for index, movie in tmp_movies.values]\n\n# Shift the movie_indices by one to get start and endpoints of all movies\nshifted_movie_indices = deque(movie_indices)\nshifted_movie_indices.rotate(-1)\n\n\n# Gather all dataframes\nuser_data = []\n\n# Iterate over all movies\nfor [df_id_1, movie_id], [df_id_2, next_movie_id] in zip(movie_indices, shifted_movie_indices):\n    \n    # Check if it is the last movie in the file\n    if df_id_1<df_id_2:\n        tmp_df = df_raw.loc[df_id_1+1:df_id_2-1].copy()\n    else:\n        tmp_df = df_raw.loc[df_id_1+1:].copy()\n        \n    # Create movie_id column\n    tmp_df['Movie_Id'] = movie_id\n    \n    # Append dataframe to list\n    user_data.append(tmp_df)\n\n# Combine all dataframes\nmovie_info = pd.concat(user_data)\ndel user_data, df_raw, tmp_movies, tmp_df, shifted_movie_indices, movie_indices, df_id_1, movie_id, df_id_2, next_movie_id\nprint('Shape User-Ratings:\\t{}'.format(movie_info.shape))\nmovie_info.sample(5)","6cbdc172":"#load titles of movies\nmovie_title = pd.read_csv('..\/input\/netflix-prize-data\/movie_titles.csv', encoding = \"ISO-8859-1\", header = None, names = ['Movie_Id', 'Year', 'Name'])\nmovie_title.set_index('Movie_Id', inplace = True)\nmovie_title.head()","ee7437b5":"#put all together\ndata_merge = pd.merge(movie_info, movie_title, on='Movie_Id')","b7460a2a":"data_merge.head()","00446ecb":"data_merge.sample(5)","87dd1c79":"data_merge.groupby('Name')['Rating'].mean().sort_values(ascending=False).head()\n","d14d720f":"data_merge.groupby('Name')['Rating'].count().sort_values(ascending=False).head()","04440986":"#Data frame for ratings \nratings = pd.DataFrame(data_merge.groupby('Name')['Rating'].mean())\nratings.head()","31bb7f2a":"ratings['num of ratings'] = pd.DataFrame(data_merge.groupby('Name')['Rating'].count().sort_values(ascending=False))\nratings.head()","0781fcb8":"plt.figure(figsize=(10,4))\nratings['num of ratings'].hist()\n","caab69f9":"plt.figure(figsize=(10,4))\nratings['Rating'].hist(bins=100)","8e547674":"\nsns.jointplot(x='Rating',y='num of ratings',data=ratings,alpha=0.5)\n","8c75b4e0":"#drop columns that we won't use\ndata_model = data_merge.drop(columns=['Date','Year','Name'])\ndata_model.head()","192a5891":"print(data_model.shape)\nprint(data_model.User.nunique())\nprint(data_model.Movie_Id.nunique())\ndata_model.isna().sum()","4d7760ff":"data_model.dtypes","c7754bff":"data_model['User']=data_model['User'].astype(int)\ndata_model.dtypes","0bb09161":"columns_titles = [\"Movie_Id\",'User',\"Rating\"]\ndata_model=data_model.reindex(columns=columns_titles)\ndata_model.head()","19bb78a3":"#here we slice our data to save time for testing our model\ndata_model=data_model.sample(20000)","7c1c2281":"data_model.head()","acae7927":"#split our data\nfrom sklearn.model_selection import train_test_split\nXtrain, Xtest = train_test_split(data_model, test_size=0.3, random_state=1)\nprint(f\"Shape of train data: {Xtrain.shape}\")\nprint(f\"Shape of test data: {Xtest.shape}\")","4d224c11":"#Get the number of unique entities in movies and users columns\nnmovies_id = data_model.Movie_Id.nunique()\nnuser_id = data_model.User.nunique()","dae2be8a":"\n#Movie input network\ninput_movies = tf.layers.Input(shape=[1])\nembed_movies = tf.layers.Embedding(2700000 + 1,15)(input_movies)\nmovies_out = tf.layers.Flatten()(embed_movies)\n\n#user input network\ninput_users = tf.layers.Input(shape=[1])\nembed_users = tf.layers.Embedding(2700000 + 1,15)(input_users)\nusers_out = tf.layers.Flatten()(embed_users)\n\nconc_layer = tf.layers.Concatenate()([movies_out, users_out])\nx = tf.layers.Dense(4, activation='relu')(conc_layer)\nx_out = x = tf.layers.Dense(1, activation='relu')(x)\nmodel = tf.Model([input_movies, input_users], x_out)","a9fd818e":"opt = tf.optimizers.Adam(learning_rate=0.001)\nmodel.compile(optimizer=opt, loss='mean_squared_error')\nmodel.summary()","0e1bdbc1":"hist = model.fit([Xtrain.Movie_Id, Xtrain.User], Xtrain.Rating, \n                 batch_size=64, \n                 epochs=10, \n                 verbose=1,\n                 validation_data=([Xtest.Movie_Id, Xtest.User], Xtest.Rating))","039ad8f2":"train_loss = hist.history['loss']\nval_loss = hist.history['val_loss']\nplt.plot(train_loss, color='r', label='Train Loss')\nplt.plot(val_loss, color='b', label='Validation Loss')\nplt.title(\"Train and Validation Loss Curve\")\nplt.legend()\nplt.show()","80532be4":"#save the model\nmodel.save('model')","1e4f9766":"# Extract embeddings\nmovie_em = model.get_layer('embedding')\nmovie_em_weights = movie_em.get_weights()[0]\nmovie_em_weights.shape","0740614e":"data_copy = data_merge.copy()\ndata_copy = data_copy.set_index(\"Movie_Id\")","285a1129":"m_id =list(data_merge.Movie_Id.unique())","df0c72cd":"\n# dict_map = {}\n# for i in m_id:\n#     dict_map[i] = data_copy.iloc[i]['Name']\n    \n# out_v = open('vecs.tsv', 'w')\n# out_m = open('meta.tsv', 'w')\n# for i in m_id:\n#     book = dict_map[i]\n#     embeddings = movie_em_weights[i]\n#     out_m.write(book + \"\\n\")\n#     out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n    \n# out_v.close()\n# out_m.close()","f9779d79":"#Making recommendations for user 50\nmovie_arr = np.array(m_id) #get all Movie IDs\nuser = np.array([50 for i in range(len(m_id))])\npred = model.predict([movie_arr, user])\npred","c000563c":"pred = pred.reshape(-1) #reshape to single dimension\npred_ids = (-pred).argsort()[0:7]\npred_ids\n","bc74e199":"data_merge.iloc[pred_ids]","4ea92897":"**Bellow, We get 20 predictions for the user number 100\nFrom the table, we notice that .. first User_Id is the user number 100 and the other Users_Ids indicates that the user have the same behavior as the first User_Id**","8ddda2fd":"# **About Project**\n**This is the final project of An End TO End Deep Learning training in Electro Pi for AI, This project is a recommender system for Netflix for movies using Machine Learning and Deep Learning tools**","299495a0":"**To visualize the data on embedding projector of Tensorflow >> \n[Embedding Projector](https:\/\/projector.tensorflow.org\/)**","14d220e1":"let's get some information from data","95dab179":"# **Let's prepair data for our model**"}}