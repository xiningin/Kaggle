{"cell_type":{"347b4ca0":"code","0bf48a8f":"code","56e74a3f":"code","54250b0d":"code","7e148858":"code","0b1c2d39":"code","7ee6fbbd":"code","73deb4fc":"code","f1503281":"code","067d4ed1":"code","afeb59a8":"code","f064af7b":"code","4b67b500":"code","cd19170e":"code","fc69c242":"code","c897528b":"code","b34f8fd4":"code","a16ef6e3":"code","eea19aaf":"code","8e27c2ad":"code","f40bdc22":"code","6794c73b":"code","f27a6842":"code","158c5fe6":"code","075341d5":"code","12bbfced":"code","d12cfbb9":"code","b5fe9800":"code","b10b96df":"code","4dcbd28a":"code","f2fc9d17":"code","edd286da":"code","e6e7d66d":"code","ac857682":"code","de1945e0":"code","03c79810":"code","a00f2b37":"code","a21f10a9":"code","143bc3a9":"code","f3f425b1":"code","1e459371":"code","e353347c":"code","03fe9fdd":"code","6ba0dc01":"code","40e2c487":"code","daf1d606":"code","a03fb757":"code","69e4bd49":"code","9c03d841":"code","aad578bf":"code","819aed2e":"code","d07ebee0":"code","98bc8853":"code","ba6d3103":"code","fa8baf13":"code","bb1faf50":"code","74804389":"code","fa88aebc":"code","2b5fea79":"code","e7168237":"code","3cf6f59e":"code","1f18a8b1":"code","5f502118":"code","48f214a4":"code","1e3a7986":"code","da0e98cc":"code","8c550d46":"markdown","a2e82464":"markdown","f450b34b":"markdown","43042ead":"markdown","309f4af1":"markdown","f762a95d":"markdown","aa78d4f7":"markdown","ac426f27":"markdown","ed04e481":"markdown","8bdfbcb5":"markdown","7dd0fc8f":"markdown","d8700aa5":"markdown","bc7f73a1":"markdown","5f7097bd":"markdown","7f1af80d":"markdown","4dc52ad4":"markdown","e5e679e1":"markdown","5507cbc5":"markdown","98357fbb":"markdown","fd424afc":"markdown","56801d07":"markdown","f26e2257":"markdown","d88105f1":"markdown","bbe20d7e":"markdown","131fb22e":"markdown","175cdf28":"markdown","8688c76b":"markdown","70ca70b2":"markdown","df6a79ba":"markdown","6ed427a5":"markdown","079b3e47":"markdown","8d1d83f5":"markdown","f40dc524":"markdown","f29c8a41":"markdown","a5f6c307":"markdown","3ea1d9a7":"markdown","7b1e2b81":"markdown","a40b059a":"markdown","f8059708":"markdown","f5a08f71":"markdown","05a64288":"markdown","0114e62e":"markdown","783c05d3":"markdown"},"source":{"347b4ca0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom xgboost import XGBRegressor","0bf48a8f":"# Score function\n\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    \"\"\"Calculate the score for a specific approach.\n    used model = RandomForestRegressor(n_estimators=100, random_state=0).\n\n    OUTPUT: \n    mean_absolute_error: Total absolute error \/ n. Sum of total absolute error divided by number of samples.\n    \"\"\"\n    model = RandomForestRegressor(n_estimators=100, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)\n\ndef get_score(n_estimators, X, y):\n    \"\"\"Return the average MAE over 3 CV folds of random forest model.\n    \n    Keyword argument:\n    n_estimators -- the number of trees in the forest\n    \"\"\"\n    # Replace this body with your own code\n    my_pipeline = Pipeline(steps=[\n    ('preprocessor', SimpleImputer()),\n    ('model', RandomForestRegressor(n_estimators=n_estimators, random_state=0))\n])\n    \n    from sklearn.model_selection import cross_val_score\n\n    # Multiply by -1 since sklearn calculates *negative* MAE\n    scores = -1 * cross_val_score(my_pipeline, X, y,\n                              cv=3,\n                              scoring='neg_mean_absolute_error')\n\n    return scores.mean()\n\n# Predict function\n\ndef predict (df1, df2, df3, estimators=100):\n    \"\"\"Predict for a specific approach.\n    used model = RandomForestRegressor(n_estimators=100, random_state=0).\n\n    INPUT:\n    df1: training data\n    df2: target\n    df3: test data\n    \n    OUTPUT: \n    predictions for test data\n    \"\"\"\n    # Define and fit model\n    my_model = RandomForestRegressor(n_estimators=estimators, random_state=0)\n    my_model.fit(df1, df2)\n\n    # Get test predictions\n    print (\"Submission data have been calculated\")\n    return my_model.predict(df3)\n\n# Save function\n\ndef save_file (predictions):\n    \"\"\"Save submission file.\"\"\"\n    # Save test predictions to file\n    output = pd.DataFrame({'Id': sample_submission_file.Id,\n                       'SalePrice': predictions})\n    output.to_csv('submission.csv', index=False)\n    print (\"Submission file is saved\")\n    \ndef impute_numerical(df1, df2):\n    \"\"\"Impute to 2 dataframes that have only numerical values.\"\"\"\n    num_imputer = SimpleImputer(strategy='mean')\n\n    num_X = pd.DataFrame(num_imputer.fit_transform(df1))\n    num_test = pd.DataFrame(num_imputer.transform(df2))\n\n    # Imputation removed column names; put them back\n    num_X.columns = df1.columns\n    num_test.columns = df2.columns\n    return num_X, num_test\n\n\ndef impute_categorical(df1, df2):\n    \"\"\"Impute to 2 dataframes that have only categorical values.\"\"\"\n    \n    cat_imputer = SimpleImputer(strategy='most_frequent')\n\n    cat_X = pd.DataFrame(cat_imputer.fit_transform(df1))\n    cat_test = pd.DataFrame(cat_imputer.transform(df2))\n\n    # Imputation removed column names; put them back\n    cat_X.columns = df1.columns\n    cat_test.columns = df2.columns\n    return cat_X, cat_test\n\nprint(\"Functions have been loaded!\")","56e74a3f":"# For displaying all the columns of the data frame\npd.set_option('display.max_columns', None)\n\n# get data\ntrain_data = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv', index_col='Id')\n\ntest_data = pd.read_csv(\"\/kaggle\/input\/home-data-for-ml-course\/test.csv\", index_col='Id')\n\nsample_submission_file = pd.read_csv(\"\/kaggle\/input\/home-data-for-ml-course\/sample_submission.csv\")\n\nwith open('\/kaggle\/input\/home-data-for-ml-course\/data_description.txt', 'r') as f:\n    description = f.read() \n\n# Create a dictionary of scores\nscores_dict = {} # will be used for storing the scores of each approach.\nsubmission_dict = {} # will be used for storing the submission scores of each approach.\n\nprint(\"Data have been loaded!\")","54250b0d":"train_data.head()","7e148858":"test_data.head(5)","0b1c2d39":"sample_submission_file.head()","7ee6fbbd":"print (\"Shape of train data: {}\".format(train_data.shape))\nprint (\"Shape of test data: {}\".format(test_data.shape))\nprint (\"Shape of submission file: {}\".format(sample_submission_file.shape))","73deb4fc":"# Data descrition\nprint(description)","f1503281":"# Process Data\n\n# Select target\ny = train_data.SalePrice\n\n# Just select numerical features!\nX = train_data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object'])\nX_test = test_data.select_dtypes(exclude=['object']) # will be used for submision later...\n\n# Divide data into training and validation subsets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                      random_state=0)\nprint (\"Shapes:\")\nprint (\"X_train: {}\".format(X_train.shape))\nprint (\"X_valid: {}\".format(X_valid.shape))\nprint (\"y_train: {}\".format(y_train.shape))\nprint (\"y_valid: {}\\n\".format(y_valid.shape))\n\n# Number of missing values in each column of training data\nmissing_val_count_by_column_train = (X_train.isnull().sum())\n\nprint('Missing value counts for numerical columns:')\nprint(missing_val_count_by_column_train[missing_val_count_by_column_train > 0])","067d4ed1":"# Get names of columns with missing values\ncols_with_missing = [col for col in X_train.columns\n                     if X_train[col].isnull().any()]\n\n# Drop columns in training and validation data\nreduced_X_train = X_train.drop(cols_with_missing, axis=1)\nreduced_X_valid = X_valid.drop(cols_with_missing, axis=1)\n\nprint (\"Shapes:\")\nprint (\"reduced_X_train: {}\".format(reduced_X_train.shape))\nprint (\"reduced_X_valid: {}\".format(reduced_X_valid.shape))","afeb59a8":"print(\"MAE from Approach 1 (Drop columns with missing values):\")\nscores_dict['A1'] = score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid) # Store the score in the dictioanary\nprint(scores_dict['A1'])","f064af7b":"# Get names of columns with missing values of training data\ncols_with_missing_X = [col for col in X.columns\n                     if X[col].isnull().any()]\nprint (\"Columns with missing values in training data: {}\".format(cols_with_missing_X) + \"\\n\")\n\n# Get names of columns with missing values of test data\ncols_with_missing_X_test = [col for col in X_test.columns\n                     if X_test[col].isnull().any()]\nprint (\"Columns with missing values in test data: {}\".format(cols_with_missing_X_test) + \"\\n\")\n\n# Combine all missing columns\ncols_with_missing = list(set(cols_with_missing_X).union(set(cols_with_missing_X_test)))\n\nprint (\"Columns with missing values combined: {}\".format(cols_with_missing))","4b67b500":"# Drop missing values\nX_a1 = X.drop(cols_with_missing, axis=1)\nX_test_a1 = X_test.drop(cols_with_missing, axis=1)\n\nprint (\"Shape of X_a1: {}\".format(X_a1.shape))\nprint (\"Shape of test_data_a1: {}\".format(X_test_a1.shape))","cd19170e":"# Get test predictions\npreds_a1 = predict(X_a1, y, X_test_a1)\n\n# Save test predictions to file\nsave_file(preds_a1)","fc69c242":"submission_dict['A1'] = 17688.42490","c897528b":"# Process Data\n\n# Select target\ny = train_data.SalePrice\n\n# Just select numerical features!\nX = train_data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object'])\nX_test = test_data.select_dtypes(exclude=['object']) # will be used for submision later...\n\n# Divide data into training and validation subsets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                      random_state=0)\nprint (\"Shapes:\")\nprint (\"X_train: {}\".format(X_train.shape))\nprint (\"X_valid: {}\".format(X_valid.shape))\nprint (\"y_train: {}\".format(y_train.shape))\nprint (\"y_valid: {}\\n\".format(y_valid.shape))\n\n# Number of missing values in each column of training data\nmissing_val_count_by_column_train = (X_train.isnull().sum())\nprint('Missing value counts for numerical columns:')\nprint(missing_val_count_by_column_train[missing_val_count_by_column_train > 0])","b34f8fd4":"# Imputation\nimputed_X_train, imputed_X_valid = impute_numerical(X_train, X_valid)\n\nprint(\"MAE from Approach 2 (Imputation):\")\nscores_dict['A2'] = score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid) # Store the score in the dictioanary\nprint(scores_dict['A2'])","a16ef6e3":"# Imputation \nimputed_X, imputed_X_test = impute_numerical(X, X_test)\n\nprint (\"Shape of imputed_X: {}\".format(imputed_X.shape))\nprint (\"Shape of imputed_test_data: {}\".format(imputed_X_test.shape))","eea19aaf":"# Get test predictions\npreds_a2 = predict(imputed_X, y, imputed_X_test)\n\n# Save test predictions to file\nsave_file(preds_a2)","8e27c2ad":"submission_dict['A2'] = 16546.14937","f40bdc22":"# Process Data\n\n# Select target\ny = train_data.SalePrice\n\n# Just select numerical features!\nX = train_data.drop(['SalePrice'], axis=1).select_dtypes(exclude=['object'])\nX_test = test_data.select_dtypes(exclude=['object']) # will be used for submision later...\n\n# Divide data into training and validation subsets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                      random_state=0)\nprint (\"Shapes:\")\nprint (\"X_train: {}\".format(X_train.shape))\nprint (\"X_valid: {}\".format(X_valid.shape))\nprint (\"y_train: {}\".format(y_train.shape))\nprint (\"y_valid: {}\\n\".format(y_valid.shape))\n\n# Number of missing values in each column of training data\nmissing_val_count_by_column_train = (X_train.isnull().sum())\nprint('Missing value counts for numerical columns:')\nprint(missing_val_count_by_column_train[missing_val_count_by_column_train > 0])","6794c73b":"# Make copy to avoid changing original data (when imputing)\nX_train_plus = X_train.copy()\nX_valid_plus = X_valid.copy()\n\n# Make new columns indicating what will be imputed\nfor col in cols_with_missing:\n    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n\n# Imputation \nimputed_X_train_plus, imputed_X_valid_plus = impute_numerical(X_train_plus, X_valid_plus)\n\nprint(\"MAE from Approach 3 (An Extension to Imputation):\")\nscores_dict['A3'] = score_dataset(imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid) # Store the score in the dictioanary\nprint(scores_dict['A3'])","f27a6842":"# Make copy to avoid changing original data (when imputing)\nX_plus = X.copy()\nreduced_X_test_plus = X_test.copy()\n\n# Make new columns indicating what will be imputed\nfor col in cols_with_missing:\n    X_plus[col + '_was_missing'] = X_plus[col].isnull()\n    reduced_X_test_plus[col + '_was_missing'] = reduced_X_test_plus[col].isnull()\n\n# Imputation \nimputed_X_plus, imputed_reduced_X_test_plus = impute_numerical(X_plus, reduced_X_test_plus)\n\nprint (\"Shape of imputed_X_plus: {}\".format(imputed_X_plus.shape))\nprint (\"Shape of imputed_reduced_X_test_plus: {}\".format(imputed_reduced_X_test_plus.shape))","158c5fe6":"# Get test predictions\npreds_a3 = predict(imputed_X_plus, y, imputed_reduced_X_test_plus)\n\n# Save test predictions to file\nsave_file(preds_a3)","075341d5":"submission_dict['A3'] = 16423.75248","12bbfced":"# Prepare data\n\n# Separate target from predictors\ny = train_data.SalePrice\nX = train_data.drop(['SalePrice'], axis=1)\n\n# Get training and validation subsets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\nprint (\"Shape of X_train: {}\".format(X_train.shape))\nprint (\"Shape of X_valid: {}\".format(X_valid.shape))\nprint (\"Shape of y_train: {}\".format(y_train.shape))\nprint (\"Shape of y_valid: {}\".format(y_valid.shape))\n\n# Drop columns with missing values ()\ncols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()] \nprint()\nprint (\"Columns with missing values in training data: {}\".format(cols_with_missing) + \"\\n\")\n\nX_train.drop(cols_with_missing, axis=1, inplace=True)\nX_valid.drop(cols_with_missing, axis=1, inplace=True)\n\nprint (\"Shape of X_train after dropping missing values: {}\".format(X_train.shape))\nprint (\"Shape of X_valid after dropping missing values: {}\".format(X_valid.shape))\n\n# Get list of categorical variables\ns = (X_train.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint()\nprint(\"Categorical variables: {}\\n\".format(object_cols))\n\ndrop_X_train = X_train.select_dtypes(exclude=['object'])\ndrop_X_valid = X_valid.select_dtypes(exclude=['object'])\n\nprint (\"Shape of drop_X_train after dropping object values: {}\".format(drop_X_train.shape))\nprint (\"Shape of drop_X_valid after dropping object values: {}\".format(drop_X_valid.shape))","d12cfbb9":"print(\"MAE from Approach 4 (Drop categorical variables):\")\nscores_dict['A4'] = score_dataset(drop_X_train, drop_X_valid, y_train, y_valid) # Store the score in the dictioanary\nprint(scores_dict['A4'])","b5fe9800":"# Prepare data\n\n# Separate target from predictors\ny = train_data.SalePrice\nX = train_data.drop(['SalePrice'], axis=1)\nX_test = test_data.copy()\nprint (\"Shape of X: {}\".format(X.shape))\nprint (\"Shape of X_test: {}\".format(X_test.shape))\nprint (\"Shape of y: {}\".format(y.shape))\n\n# Drop columns with missing values ()\ncols_with_missing_X = [col for col in X.columns if X[col].isnull().any()]\ncols_with_missing_X_test = [col for col in X_test.columns if X_test[col].isnull().any()]\ntotal_missing=set(cols_with_missing_X).union(set(cols_with_missing_X_test))\ncols_with_missing=list(total_missing)\n\nprint()\nprint (\"Columns with missing values in training data: {}\".format(cols_with_missing) + \"\\n\")\n\nX.drop(cols_with_missing, axis=1, inplace=True)\nX_test.drop(cols_with_missing, axis=1, inplace=True)\n\nprint (\"Shape of X after dropping missing values: {}\".format(X.shape))\nprint (\"Shape of X_test after dropping missing values: {}\".format(X_test.shape))\n\n# Get list of categorical variables\ns = (X.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint()\nprint(\"Categorical variables: {}\\n\".format(object_cols))\n\ndrop_X = X.select_dtypes(exclude=['object'])\ndrop_X_test = X_test.select_dtypes(exclude=['object'])\n\nprint (\"Shape of drop_X after dropping object values: {}\".format(drop_X.shape))\nprint (\"Shape of drop_X_test after dropping object values: {}\".format(drop_X_test.shape))","b10b96df":"# Get test predictions\npreds_a4 = predict(drop_X, y, drop_X_test)\n\n# Save test predictions to file\nsave_file(preds_a4)","4dcbd28a":"submission_dict['A4'] = 17688.42490","f2fc9d17":"# Prepare data\n\n# Separate target from predictors\ny = train_data.SalePrice\nX = train_data.drop(['SalePrice'], axis=1)\nX_test = test_data.copy()\n\nprint (\"Shape of X: {}\".format(X.shape))\nprint (\"Shape of X_test: {}\".format(X_test.shape))\nprint (\"Shape of y: {}\\n\".format(y.shape))\n\n# Get training and validation subsets\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\nprint (\"Shape of X_train_full: {}\".format(X_train_full.shape))\nprint (\"Shape of X_valid_full: {}\".format(X_valid_full.shape))\nprint (\"Shape of y_train: {}\".format(y_train.shape))\nprint (\"Shape of y_valid: {}\\n\".format(y_valid.shape))\n\n# Check Condition2 column\nprint(\"Unique values in 'Condition2' column in training data:\", X_train['Condition2'].unique())\nprint(\"Unique values in 'Condition2' column in validation data:\", X_valid['Condition2'].unique())\n\n# All categorical columns\nobject_cols = [col for col in X_train_full.columns if X_train_full[col].dtype == \"object\"]\n\n# Columns that can be safely label encoded\ngood_label_cols = [col for col in object_cols if \n                   set(X_train_full[col]) == set(X_valid_full[col])]\n        \n# Problematic columns that will be dropped from the dataset\nbad_label_cols = list(set(object_cols)-set(good_label_cols))\n        \nprint('\\nCategorical columns that will be label encoded:', good_label_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)\n\n# Drop categorical columns that will not be encoded\nlabel_X_train = X_train_full.drop(bad_label_cols, axis=1)\nlabel_X_valid = X_valid_full.drop(bad_label_cols, axis=1)\n\nprint (\"\\nShape of label_X_train after dropping bad labels: {}\".format(label_X_train.shape))\nprint (\"Shape of label_X_valid after dropping bad labels: {}\".format(label_X_valid.shape))","edd286da":"# Apply label encoder \n\n# https:\/\/stackoverflow.com\/questions\/46406720\/labelencoder-typeerror-not-supported-between-instances-of-float-and-str\n# Thanks to: @pceccon and @sgDysregulation\nlabel_encoder = LabelEncoder()\nfor col in set(good_label_cols):\n    label_X_train[col] = label_encoder.fit_transform(X_train_full[col].astype(str))\n    label_X_valid[col] = label_encoder.transform(X_valid_full[col].astype(str))\n\n# Imputation to numerical columns\ni_label_X_train, i_label_X_valid = impute_numerical(label_X_train, label_X_valid)\n\nprint(\"MAE from Approach 5 (Label Encoding):\") \nscores_dict['A5'] = score_dataset(i_label_X_train, i_label_X_valid, y_train, y_valid) # Store the score in the dictioanary\nprint(scores_dict['A5'])  ","e6e7d66d":"# Prepare data\n\n# Separate target from predictors\ny = train_data.SalePrice\nX = train_data.drop(['SalePrice'], axis=1)\nX_test = test_data.copy()\n\nprint (\"Shape of X: {}\".format(X.shape))\nprint (\"Shape of X_test: {}\".format(X_test.shape))\nprint (\"Shape of y: {}\\n\".format(y.shape))\n\n# Find numerical columns\nnum_X = X.select_dtypes(exclude=['object']).copy()\nnum_test = test_data.select_dtypes(exclude=['object']).copy()\n\nprint (\"Shape of num_X: {}\".format(num_X.shape))\nprint (\"Shape of num_test: {}\".format(num_test.shape))\n\n# All categorical columns\nobject_cols = X.columns\n\n# Columns that can be safely label encoded\ngood_label_cols = [col for col in object_cols if \n                   set(X[col]) == set(X_test[col])]\n        \n# Problematic columns that will be dropped from the dataset\nbad_label_cols = list(set(object_cols)-set(good_label_cols))\n        \nprint('\\nCategorical columns that will be label encoded:', good_label_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)\n\n# Find categorical columns\ncat_X = X[object_cols].copy()\ncat_test = test_data[object_cols].copy()\n\nprint (\"\\nShape of cat_X: {}\".format(cat_X.shape))\nprint (\"Shape of cat_test: {}\".format(cat_test.shape))\n\n# Drop categorical columns that will not be encoded\ncat_X = cat_X.drop(bad_label_cols, axis=1)\ncat_test = cat_test.drop(bad_label_cols, axis=1)\n\nprint (\"\\nShape of cat_X after dropping bad label columns: {}\".format(cat_X.shape))\nprint (\"Shape of cat_test after dropping bad label columns: {}\".format(cat_test.shape))","ac857682":"# Imputation and label encoding\n\n# Imputation to numerical columns\n\ni_num_X, i_num_test = impute_numerical(num_X, num_test)\n\nprint (\"Shape of i_num_X: {}\".format(i_num_X.shape))\nprint (\"Shape of i_num_test: {}\".format(i_num_test.shape))\n\n# Imputation to categorical columns\n\ni_cat_X, i_cat_test = impute_categorical(cat_X, cat_test)\n\nprint (\"\\nShape of i_cat_X: {}\".format(i_cat_X.shape))\nprint (\"Shape of i_cat_test: {}\".format(i_cat_test.shape))\n\n# Label encoding to categorical columns\n\nlabel_encoder = LabelEncoder()\nfor col in set(good_label_cols):\n    i_cat_X[col] = label_encoder.fit_transform(i_cat_X[col])\n    i_cat_test[col] = label_encoder.transform(i_cat_test[col])\n\nprint (\"\\nShape of i_cat_X: {}\".format(i_cat_X.shape))\nprint (\"Shape of i_cat_test: {}\".format(i_cat_test.shape))\n\n# merge datasets\n\nlabel_X = pd.concat([i_num_X, i_cat_X], axis=1)\nlabel_test = pd.concat([i_num_test, i_cat_test], axis=1)\n\nprint (\"\\nShape of label_X after merge: {}\".format(label_X.shape))\nprint (\"Shape of label_test after merge: {}\".format(label_test.shape))","de1945e0":"# Get test predictions\npreds_a5 = predict(label_X, y, label_test)\n\n# Save test predictions to file\nsave_file(preds_a5)","03c79810":"submission_dict['A5'] = 16004.32251","a00f2b37":"# Prepare data\n\n# Separate target from predictors\ny = train_data.SalePrice\nX = train_data.drop(['SalePrice'], axis=1)\nX_test = test_data.copy()\n\nprint (\"Shape of X: {}\".format(X.shape))\nprint (\"Shape of X_test: {}\".format(X_test.shape))\nprint (\"Shape of y: {}\\n\".format(y.shape))\n\n# Get training and validation subsets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\nprint (\"Shape of X_train: {}\".format(X_train.shape))\nprint (\"Shape of X_valid: {}\".format(X_valid.shape))\nprint (\"Shape of y_train: {}\".format(y_train.shape))\nprint (\"Shape of y_valid: {}\".format(y_valid.shape))\n\nobject_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n\n# Columns that will be one-hot encoded\nlow_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]\n\n# Columns that will be dropped from the dataset\nhigh_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n\nprint('\\nCategorical columns that will be one-hot encoded:', low_cardinality_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)\n\n# Imputation to categorical columns\n\nX_train_onehot, X_valid_onehot = impute_categorical(X_train[low_cardinality_cols], X_valid[low_cardinality_cols])\n\nprint (\"\\nShape of X_train_onehot: {}\".format(X_train_onehot.shape))\nprint (\"Shape of X_valid_onehot: {}\".format(X_valid_onehot.shape))\n\n# Apply one-hot encoder to each column with low_cardinality categorical data\nOH_encoder = OneHotEncoder(handle_unknown = 'ignore', sparse = False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train_onehot))\nOH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid_onehot))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train_onehot.index\nOH_cols_valid.index = X_valid_onehot.index\n\n# Remove categorical columns (will replace with one-hot encoding) and high_cardinality_cols\nnum_X_train = X_train.drop(object_cols, axis=1)\nnum_X_valid = X_valid.drop(object_cols, axis=1)\n\ni_num_X_train, i_num_X_valid = impute_numerical(num_X_train, num_X_valid)\n\nOH_X_train = pd.concat([i_num_X_train, OH_cols_train], axis=1)\nOH_X_valid = pd.concat([i_num_X_valid, OH_cols_valid], axis=1)\n\nprint (\"\\nShape of OH_X_train after merge: {}\".format(OH_X_train.shape))\nprint (\"Shape of OH_X_valid after merge: {}\".format(OH_X_valid.shape))","a21f10a9":"print(\"MAE from Approach 6 (One-Hot Encoding):\") \nscores_dict['A6'] = score_dataset(OH_X_train, OH_X_valid, y_train, y_valid) # Store the score in the dictioanary\nprint(scores_dict['A6'])","143bc3a9":"# Prepare data\n\n# Separate target from predictors\ny = train_data.SalePrice\nX = train_data.drop(['SalePrice'], axis=1)\nX_test = test_data.copy()\n\nprint (\"Shape of X: {}\".format(X.shape))\nprint (\"Shape of X_test: {}\".format(X_test.shape))\nprint (\"Shape of y: {}\\n\".format(y.shape))\n\nobject_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n\n# Columns that will be one-hot encoded\nlow_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]\n\n# Columns that will be dropped from the dataset\nhigh_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n\nprint('\\nCategorical columns that will be one-hot encoded:', low_cardinality_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)\n\n# Imputation to categorical columns\n\nX_onehot, test_data_onehot = impute_categorical(X[low_cardinality_cols], X_test[low_cardinality_cols])\n\nprint (\"\\nShape of X_onehot: {}\".format(X_onehot.shape))\nprint (\"Shape of test_data_onehot: {}\".format(test_data_onehot.shape))\n\n# Apply one-hot encoder to each column with low_cardinality categorical data\nOH_encoder = OneHotEncoder(handle_unknown = 'ignore', sparse = False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_onehot))\nOH_cols_test = pd.DataFrame(OH_encoder.transform(test_data_onehot))\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_onehot.index\nOH_cols_test.index = test_data_onehot.index\n\n# Remove categorical columns (will replace with one-hot encoding) and high_cardinality_cols\nnum_X = X.drop(object_cols, axis=1)\nnum_test = test_data.drop(object_cols, axis=1)\n\ni_num_X, i_num_test = impute_numerical(num_X, num_test)\n\nOH_X_train = pd.concat([i_num_X, OH_cols_train], axis=1)\nOH_X_test = pd.concat([i_num_test, OH_cols_test], axis=1)\n\nprint (\"\\nShape of OH_X_train: {}\".format(OH_X_train.shape))\nprint (\"Shape of OH_X_test: {}\".format(OH_X_test.shape))","f3f425b1":"# Get test predictions\npreds_a6 = predict(OH_X_train, y, OH_X_test)\n\n# Save test predictions to file\nsave_file(preds_a6)","1e459371":"submission_dict['A6'] = 16073.17499","e353347c":"# Prepare data\n\n# Separate target from predictors\ny = train_data.SalePrice\nX = train_data.drop(['SalePrice'], axis=1)\nX_test = test_data.copy()\n\nprint (\"Shape of X: {}\".format(X.shape))\nprint (\"Shape of X_test: {}\".format(X_test.shape))\nprint (\"Shape of y: {}\\n\".format(y.shape))\n\n\n# Get training and validation subsets\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\nprint (\"Shape of X_train_full: {}\".format(X_train_full.shape))\nprint (\"Shape of X_valid_full: {}\".format(X_valid_full.shape))\nprint (\"Shape of y_train: {}\".format(y_train.shape))\nprint (\"Shape of y_valid: {}\".format(y_valid.shape))\n\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n                        X_train_full[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\n\nprint (\"\\nShape of X_train: {}\".format(X_train.shape))\nprint (\"Shape of X_valid: {}\".format(X_valid.shape))","03fe9fdd":"# Define transformers\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='mean')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])","6ba0dc01":"# Define the Model\nmodel = RandomForestRegressor(n_estimators=100, random_state=0)","40e2c487":"# Create and Evaluate the Pipeline\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])\n\n# Preprocessing of training data, fit model \nmy_pipeline.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = my_pipeline.predict(X_valid)\n\n# Evaluate the model\nscores_dict['A7'] = mean_absolute_error(y_valid, preds) # Store the score in the dictioanary\n\nprint('MAE from Approach 7 (pipelines):')\nprint(scores_dict['A7'])\n","daf1d606":"# Prepare data\n\n# Separate target from predictors\ny = train_data.SalePrice\nX = train_data.drop(['SalePrice'], axis=1)\nX_test = test_data.copy()\n\nprint (\"Shape of X: {}\".format(X.shape))\nprint (\"Shape of X_test: {}\".format(X_test.shape))\nprint (\"Shape of y: {}\\n\".format(y.shape))\n\n\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_cols = [cname for cname in X.columns if X[cname].nunique() < 10 and \n                        X[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols\n\n# Preprocessing of training data \npipeline_X = X[my_cols].copy()\npipeline_X_test = X_test[my_cols].copy()\n\nprint (\"\\nShape of X: {}\".format(pipeline_X.shape))\nprint (\"Shape of X_test: {}\\n\".format(pipeline_X_test.shape))\n\n# Fit model\nmy_pipeline.fit(pipeline_X, y)\n\n# Get predictions\npreds_a7 = my_pipeline.predict(pipeline_X_test)\n\n# Save test predictions to file\nsave_file(preds_a7)\n","a03fb757":"submission_dict['A7'] = 16073.17499","69e4bd49":"# Prepare data\n\n# Separate target from predictors\ny = train_data.SalePrice\nX = train_data.drop(['SalePrice'], axis=1)\nX_test = test_data.copy()\n\nprint (\"Shape of X: {}\".format(X.shape))\nprint (\"Shape of X_test: {}\".format(X_test.shape))\nprint (\"Shape of y: {}\\n\".format(y.shape))\n\n\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n                        X_train_full[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n\n\ncv_X = X[numerical_cols].copy()\ncv_test = X_test[numerical_cols].copy()\n\nprint (\"\\nShape of cv_X: {}\".format(cv_X.shape))\nprint (\"Shape of cv_test: {}\".format(cv_test.shape))","9c03d841":"results = {}\nmy_list = [50, 100, 150, 200, 250, 300, 350, 400]\nfor num in my_list:\n    results.update({num : get_score(num,cv_X, y)})\n    print (results.get(num))","aad578bf":"scores_dict['A8']=min(results.values()) # Store the score in the dictioanary\n\n# https:\/\/stackoverflow.com\/questions\/43431347\/python-dictionary-plot-matplotlib\/43431522\n# Thanks to: @tanaka, @LucG\nfig = plt.figure(figsize=[20, 10])\nplt.plot(list(results.keys()),list(results.values()))","819aed2e":"# Prepare data\n\n# Separate target from predictors\ny = train_data.SalePrice\nX = train_data.drop(['SalePrice'], axis=1)\nX_test = test_data.copy()\n\nprint (\"Shape of X: {}\".format(X.shape))\nprint (\"Shape of X_test: {}\".format(X_test.shape))\nprint (\"Shape of y: {}\\n\".format(y.shape))\n\ncategorical_cols = [cname for cname in X.columns if X[cname].nunique() < 10 and \n                        X[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols","d07ebee0":"# Define transformers\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='mean')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])","98bc8853":"# Define the Model\nmodel = RandomForestRegressor(n_estimators=200, random_state=0)","ba6d3103":"# Preprocessing of training data, fit model \ncv_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])\ncv_X = X[my_cols].copy()\ncv_X_test = X_test[my_cols].copy()\ncv_pipeline.fit(cv_X, y)\n\n# Preprocessing of validation data, get predictions\npreds_a8 = cv_pipeline.predict(cv_X_test)\n\n# Save test predictions to file\nsave_file(preds_a8)","fa8baf13":"submission_dict['A8'] = 15950.53953","bb1faf50":"# Prepare data\n\n# Separate target from predictors\ny = train_data.SalePrice\nX = train_data.drop(['SalePrice'], axis=1)\n\n\nprint (\"Shape of X: {}\".format(X.shape))\nprint (\"Shape of y: {}\\n\".format(y.shape))\n\n\n# Get training and validation subsets\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\nprint (\"Shape of X_train_full: {}\".format(X_train_full.shape))\nprint (\"Shape of X_valid_full: {}\".format(X_valid_full.shape))\nprint (\"Shape of y_train: {}\".format(y_train.shape))\nprint (\"Shape of y_valid: {}\".format(y_valid.shape))\n\n\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\nlow_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n                        X_train_full[cname].dtype == \"object\"]\n\n# Select numeric columns\nnumeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = low_cardinality_cols + numeric_cols\n\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\n\n\n# One-hot encode the data (to shorten the code, we use pandas)\nX_train = pd.get_dummies(X_train)\nX_valid = pd.get_dummies(X_valid)\nX_train, X_valid = X_train.align(X_valid, join='left', axis=1)\n\nprint (\"\\nShape of X_train after encoding: {}\".format(X_train.shape))\nprint (\"Shape of y_train after encoding: {}\".format(y_train.shape))\nprint (\"Shape of X_valid after encoding: {}\".format(X_valid.shape))\nprint (\"Shape of y_valid after encoding: {}\".format(y_valid.shape))","74804389":"# Train model\nmy_model = XGBRegressor(random_state=0)\nmy_model.fit(X_train, y_train)\n# Predict\nprediction_1 = my_model.predict(X_valid)\n\n# Calculate MAE\nscores_dict['A9'] = mean_absolute_error(prediction_1, y_valid) # Store the score in the dictioanary\n\n# print MAE\nprint(\"Mean Absolute Error:\\n\" , scores_dict['A9'])","fa88aebc":"# Prepare data\n\n# Separate target from predictors\ny = train_data.SalePrice\nX = train_data.drop(['SalePrice'], axis=1)\nX_test = test_data.copy()\n\nprint (\"Shape of X: {}\".format(X.shape))\nprint (\"Shape of X_test: {}\".format(X_test.shape))\nprint (\"Shape of y: {}\\n\".format(y.shape))\n\n\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\nlow_cardinality_cols = [cname for cname in X.columns if X[cname].nunique() < 10 and \n                        X[cname].dtype == \"object\"]\n\n# Select numeric columns\nnumeric_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = low_cardinality_cols + numeric_cols\nX = X[my_cols]\nX_test = X_test[my_cols]\n\n# One-hot encode the data (to shorten the code, we use pandas)\nX = pd.get_dummies(X)\nX_test = pd.get_dummies(X_test)\nX, X_test = X.align(X_test, join='left', axis=1)\n\nprint (\"\\nShape of X after encoding: {}\".format(X.shape))\nprint (\"Shape of X_test after encodin: {}\".format(X_test.shape))\n\n\nmy_model = XGBRegressor(random_state=0)\nmy_model.fit(X, y)\npreds_a9 = my_model.predict(X_test)\n\n# Save test predictions to file\nsave_file(preds_a9)","2b5fea79":"submission_dict['A9'] = 16011.84256","e7168237":"# Prepare data\n\n# Separate target from predictors\ny = train_data.SalePrice\nX = train_data.drop(['SalePrice'], axis=1)\n\n\nprint (\"Shape of X: {}\".format(X.shape))\nprint (\"Shape of y: {}\\n\".format(y.shape))\n\n\n# Get training and validation subsets\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\nprint (\"Shape of X_train_full: {}\".format(X_train_full.shape))\nprint (\"Shape of X_valid_full: {}\".format(X_valid_full.shape))\nprint (\"Shape of y_train: {}\".format(y_train.shape))\nprint (\"Shape of y_valid: {}\".format(y_valid.shape))\n\n\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\nlow_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n                        X_train_full[cname].dtype == \"object\"]\n\n# Select numeric columns\nnumeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = low_cardinality_cols + numeric_cols\n\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\n\n\n# One-hot encode the data (to shorten the code, we use pandas)\nX_train = pd.get_dummies(X_train)\nX_valid = pd.get_dummies(X_valid)\nX_train, X_valid = X_train.align(X_valid, join='left', axis=1)\n\nprint (\"\\nShape of X_train after encoding: {}\".format(X_train.shape))\nprint (\"Shape of y_train after encoding: {}\".format(y_train.shape))\nprint (\"Shape of X_valid after encoding: {}\".format(X_valid.shape))\nprint (\"Shape of y_valid after encoding: {}\".format(y_valid.shape))\n","3cf6f59e":"# Define the model\nmy_model_2 = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=2)\n\n# Fit the model\nmy_model_2.fit(X_train, y_train,\n               early_stopping_rounds=5,\n               eval_set=[(X_valid, y_valid)],\n               verbose=False)\n               \n\n# Get predictions\npredictions_2 = my_model_2.predict(X_valid)\n\n# Calculate MAE\nscores_dict['A10'] = mean_absolute_error(predictions_2, y_valid) # Store the score in the dictioanary\n\n# Uncomment to print MAE\nprint(\"Mean Absolute Error:\\n\" , scores_dict['A10'])","1f18a8b1":"# Prepare data\n\n# Separate target from predictors\ny = train_data.SalePrice\nX = train_data.drop(['SalePrice'], axis=1)\nX_test_full = test_data.copy()\n\n\nprint (\"Shape of X: {}\".format(X.shape))\nprint (\"Shape of y: {}\".format(y.shape))\nprint (\"Shape of X_test_full: {}\\n\".format(X_test_full.shape))\n\n# Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\nprint (\"Shape of X_train_full: {}\".format(X_train_full.shape))\nprint (\"Shape of X_valid_full: {}\".format(X_valid_full.shape))\nprint (\"Shape of y_train: {}\".format(y_train.shape))\nprint (\"Shape of y_valid: {}\".format(y_valid.shape))\n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\nlow_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n                        X_train_full[cname].dtype == \"object\"]\n\n# Select numeric columns\nnumeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = low_cardinality_cols + numeric_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()\n\n\n# One-hot encode the data (to shorten the code, we use pandas)\nX_train = pd.get_dummies(X_train)\nX_valid = pd.get_dummies(X_valid)\nX_test = pd.get_dummies(X_test)\nX_train, X_valid = X_train.align(X_valid, join='left', axis=1)\nX_train, X_test = X_train.align(X_test, join='left', axis=1)\n\nprint (\"\\nShape of X_train after encoding: {}\".format(X_train.shape))\nprint (\"Shape of y_train after encoding: {}\".format(y_train.shape))\nprint (\"Shape of X_valid after encoding: {}\".format(X_valid.shape))\nprint (\"Shape of y_valid after encoding: {}\".format(y_valid.shape))\nprint (\"Shape of X_test after encoding: {}\".format(X_test.shape))","5f502118":"# Define the model\nmy_model_2 = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=2)\n\n# Fit the model\nmy_model_2.fit(X_train, y_train,\n               early_stopping_rounds=5,\n               eval_set=[(X_valid, y_valid)],\n               verbose=False)\n               \n\nprediction_2 = my_model_2.predict(X_test)\n\n# Save test predictions to file\nsave_file(prediction_2)\n","48f214a4":"submission_dict['A10'] = 14810.12828","1e3a7986":"print(scores_dict)\nprint(submission_dict)","da0e98cc":"# Plot Validation Score ve Actual Results figure\nY1=[x for x in scores_dict.values()]\nY2=[x for x in submission_dict.values()]\n\nX1=[x for x in scores_dict.keys()]\nX2=[x for x in submission_dict.keys()]\n\nfig = plt.figure(figsize=[20, 10])\nplt.xlabel('Approaches')\nplt.ylabel('Score')\nplt.title('Validation ve Actual Scores')\nplt.plot(X1,Y1, color='tab:blue')\nplt.plot(X2,Y2, color='tab:orange')\n\nplt.legend([\"Validation Score\", \"Actual Result\"])","8c550d46":"# **Lesson 3 - Categorical Variables**  <a id='Lesson3'><\/a>","a2e82464":"## **Approach 4 (A4):**  <a id='L3A4'><\/a>\nSimplest approach: Include only numerical columns, drop all the categorical data","f450b34b":"# **Lesson 2 - Missing Values**  <a id='Lesson2'><\/a>","43042ead":"### Create Submission File for approach 2","309f4af1":"Score for A2 = 16546.14937 ","f762a95d":"### Create Submission File for approach 10","aa78d4f7":"## **Approach 5 (A5):**  <a id='L3A5'><\/a>\nInclude numerical columns, Label Encoding","ac426f27":"# Helper functions   <a id='functions'><\/a>   ","ed04e481":"## **Approach 6 (A6):**  <a id='L3A6'><\/a>\nInclude numerical columns, One-hot Encoding","8bdfbcb5":"### Create Submission File for approach 7","7dd0fc8f":"### Create Submission File for approach 1\n\nWhile calculating MAE for approach 1 we are dealing with training data only. However if we want a model to predict the test data with approach 1 we must also consider missing values in the test data.","d8700aa5":"### Create Submission File for approach 9","bc7f73a1":"## **Approach 3 (A3):**  <a id='L2A3'><\/a>\nInclude only numerical columns, impute columns with missing values plus an extension to imputation","5f7097bd":"## **Approach 2 (A2):**  <a id='L2A2'><\/a>\nInclude only numerical columns and impute columns with missing values.","7f1af80d":"As can be seen while we are calculating MAE with Approach 1 we use 33 features. On the other hand to send a submission with Approach 1 we use 25 features instead.","4dc52ad4":"## **Approach 9 (A9):**  <a id='L6A9'><\/a>\nUsing Gradient boosting","e5e679e1":"### Create Submission File for approach 8","5507cbc5":"As can be seen easily this is the same solution with A1. So no need to submit an answer. But I will submit for consistency.","98357fbb":"Score for A3 = 16423.75248","fd424afc":"### Create Submission File for approach 4","56801d07":"# **Lesson 6 - XGBoost**  <a id='Lesson6'><\/a>","f26e2257":"# **Introduction**    <a id='introduction'><\/a>\n\nThis notebook is greatly inspired by [Intermediate Machine Learning (IML)](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning) Course given by [Alexis Cook](https:\/\/www.kaggle.com\/alexisbcook) at [Kaggle Learn](https:\/\/www.kaggle.com\/learn\/overview). Approximately all the code in this notebook is taken from Intermediate Machine Learning Course notebooks. On the other hand, all the mistakes (if any) are done by me.\n\nThis notebook is neither an alternative nor a complementary work for Intermediate Machine Learning Course. I strongly suggest to the Kaggle Learn users to take the course and solve the exercise notebooks by themself. However, I observe that from time to time students got stuck on some parts of the course. This notebook is intended for those students who got stuck and cannot move forward.\n\nIntermediate Machine Learning consists of 7 chapters. In some chapters students use the data from [Housing Prices Competition for Kaggle Learn Users](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course) to learn new approaches, they even make submissions for that very competition. You can find those different approaches below. You can also find how to submit your predictions using those approaches to [Housing Prices Competition for Kaggle Learn Users](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course).\n\nIf you face any problem or cannot implement the code just let me know via the comments. I will try to answer as soon as possible. Thank you for reading.\n\n# Table of Contents\n* [Introduction](#introduction)\n* [Helper Functions](#functions)\n* [Lesson 2 - Missing Values](#Lesson2)\n    * [Lesson 2 - Missing Values Approach 1](#L2A1)\n    * [Lesson 2 - Missing Values Approach 2](#L2A2)\n    * [Lesson 2 - Missing Values Approach 3](#L2A3)\n* [Lesson 3 - Categorical Variables](#Lesson3)\n    * [Lesson 3 - Categorical Variables Approach 4](#L3A4)\n    * [Lesson 3 - Categorical Variables Approach 5](#L3A5)\n    * [Lesson 3 - Categorical Variables Approach 6](#L3A6)\n* [Lesson 4 - Pipelines](#Lesson4)\n    * [Lesson 4 - Pipelines Approach 7](#L4A7)\n* [Lesson 5 - Cross-Validation](#Lesson5)\n    * [Lesson 5 - Cross-Validation Approach 8](#L5A8)\n* [Lesson 6 - XGBoost](#Lesson6)\n    * [Lesson 6 - XGBoost Approach 9](#L6A9)\n    * [Lesson 6 - XGBoost Approach 10](#L6A10)\n* [Conclusion](#conclusion)\n* [References](#references)\n","d88105f1":"**Getting the data**","bbe20d7e":"# **References**  <a id='references'><\/a>\n* [@alexisbcook](https:\/\/www.kaggle.com\/alexisbcook)\n* [Kaggle Learn](https:\/\/www.kaggle.com\/learn\/overview)\n* [Intermediate Machine Learning Course](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning)\n* [10-simple-hacks-to-speed-up-your-data-analysis - Parul Pandey](https:\/\/www.kaggle.com\/parulpandey\/10-simple-hacks-to-speed-up-your-data-analysis)","131fb22e":"## **Approach 7 (A7):**  <a id='L4A7'><\/a>\nUsing Pipelines","175cdf28":"# **Lesson 5 - Cross-Validation**  <a id='Lesson5'><\/a>","8688c76b":"### Create Submission File for approach 3","70ca70b2":"## **Approach 10 (A10):**  <a id='L6A10'><\/a>\nGradient boosting - Early stopping rounds","df6a79ba":"Score for A8 (n_estimators=200) = 15950.53953","6ed427a5":"Score for A6 = 16073.17499","079b3e47":"Score for A1 = 17688.42490","8d1d83f5":"Score for A4 = 17688.42490","f40dc524":"# **Conclusion**  <a id='conclusion'><\/a>\nBelow you can see how validation and corresponding actual scores behaves for each score. ","f29c8a41":"# **Lesson 4 - Pipelines**  <a id='Lesson4'><\/a>","a5f6c307":"### Create Submission File for approach 6","3ea1d9a7":"### Create Submission File for approach 5","7b1e2b81":"Score for A7 = 16073.17499","a40b059a":"Score for A9 = 16011.84256","f8059708":"## **Approach 8 (A8):**  <a id='L5A8'><\/a>\nUsing Cross-validation","f5a08f71":"Score for A5 = 16004.32251","05a64288":"<div class=\"alert alert-block alert-info\">\n<b>Tip:<\/b> We will use some utility functions throughout the notebook. Collecting them in one place is a good idea. It makes the code more organized.\n<\/div>","0114e62e":"This is the simplest approach. We will just include the numerical columns without missing values. All the remaining columns will be dropped!","783c05d3":"Score for A10= 14810.12828"}}