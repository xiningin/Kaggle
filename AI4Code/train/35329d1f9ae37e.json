{"cell_type":{"4cb5334f":"code","6ba73410":"code","9d7934e9":"code","afe39cf7":"code","50a15c74":"code","737e538a":"code","2a230622":"code","9f70544a":"code","8edd58bd":"code","6b0b04a2":"code","df1c36b2":"code","6e1c1105":"code","9d49bb53":"code","265ac411":"code","a9e04905":"code","4d37ed44":"code","931abddc":"code","45278809":"code","b219122e":"code","9e06f5ce":"code","ce169276":"code","097f0a2b":"code","da3bf2c7":"code","20c7578b":"code","1efab6c0":"code","f5113440":"code","65f69dff":"code","43ebf18a":"code","0603c39d":"code","ff9d57cb":"code","0dba7043":"code","2dd3a45c":"code","cefb8c1c":"code","f6f2ac4e":"code","27459549":"code","db867f0a":"code","fa30b124":"code","fa6a976f":"code","ee1fc0fa":"code","5362e1ed":"code","956aa6a2":"code","02e49503":"code","87204fc8":"code","7feac4a7":"code","44809f29":"code","24084254":"code","21b4c1a7":"code","88a36e8a":"code","f838ce91":"code","0f46f2a3":"code","bc3852fd":"code","946e3c62":"code","f8b82705":"code","73410787":"markdown","e3b867c5":"markdown","7fef09a0":"markdown","f6d67287":"markdown","df84de5b":"markdown","afbd827d":"markdown","41368712":"markdown","5730f00b":"markdown","34e037e8":"markdown","5bff31dc":"markdown","cd24e256":"markdown","de89ee5f":"markdown","a6245695":"markdown"},"source":{"4cb5334f":"import gc\nimport os\nfrom pathlib import Path\nimport random\nimport sys\n\nfrom tqdm import tqdm_notebook as tqdm\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom IPython.core.display import display, HTML\n\n# --- plotly ---\nfrom plotly import tools, subplots\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\n\n# --- models ---\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\n","6ba73410":"# Original code from https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage by @gemartin\n# Modified to support timestamp type, categorical type\n# Modified to add option to use float16 or not. feather format does not support float16.\nfrom pandas.api.types import is_datetime64_any_dtype as is_datetime\nfrom pandas.api.types import is_categorical_dtype\n\ndef reduce_mem_usage(df, use_float16=False):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n            # skip datetime type or categorical type\n            continue\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","9d7934e9":"!ls ..\/input","afe39cf7":"%%time\nroot = Path('..\/input\/ashrae-feather-format-for-fast-loading')\n\ntrain_df = pd.read_feather(root\/'train.feather')\nweather_train_df = pd.read_feather(root\/'weather_train.feather')\nbuilding_meta_df = pd.read_feather(root\/'building_metadata.feather')","50a15c74":"train_df['date'] = train_df['timestamp'].dt.date\ntrain_df['meter_reading_log1p'] = np.log1p(train_df['meter_reading'])","737e538a":"def plot_date_usage(train_df, meter=0, building_id=0):\n    train_temp_df = train_df[train_df['meter'] == meter]\n    train_temp_df = train_temp_df[train_temp_df['building_id'] == building_id]    \n    train_temp_df_meter = train_temp_df.groupby('date')['meter_reading_log1p'].sum()\n    train_temp_df_meter = train_temp_df_meter.to_frame().reset_index()\n    fig = px.line(train_temp_df_meter, x='date', y='meter_reading_log1p')\n    fig.show()","2a230622":"plot_date_usage(train_df, meter=0, building_id=0)","9f70544a":"building_meta_df[building_meta_df.site_id == 0]","8edd58bd":"train_df = train_df.query('not (building_id <= 104 & meter == 0 & timestamp <= \"2016-05-20\")')","6b0b04a2":"debug = False","df1c36b2":"def preprocess(df):\n    df[\"hour\"] = df[\"timestamp\"].dt.hour\n#     df[\"day\"] = df[\"timestamp\"].dt.day\n    df[\"weekend\"] = df[\"timestamp\"].dt.weekday\n    df[\"month\"] = df[\"timestamp\"].dt.month\n    df[\"dayofweek\"] = df[\"timestamp\"].dt.dayofweek\n\n#     hour_rad = df[\"hour\"].values \/ 24. * 2 * np.pi\n#     df[\"hour_sin\"] = np.sin(hour_rad)\n#     df[\"hour_cos\"] = np.cos(hour_rad)","6e1c1105":"preprocess(train_df)","9d49bb53":"df_group = train_df.groupby('building_id')['meter_reading_log1p']\nbuilding_mean = df_group.mean().astype(np.float16)\nbuilding_median = df_group.median().astype(np.float16)\nbuilding_min = df_group.min().astype(np.float16)\nbuilding_max = df_group.max().astype(np.float16)\nbuilding_std = df_group.std().astype(np.float16)\n\ntrain_df['building_mean'] = train_df['building_id'].map(building_mean)\ntrain_df['building_median'] = train_df['building_id'].map(building_median)\ntrain_df['building_min'] = train_df['building_id'].map(building_min)\ntrain_df['building_max'] = train_df['building_id'].map(building_max)\ntrain_df['building_std'] = train_df['building_id'].map(building_std)","265ac411":"building_mean.head()","a9e04905":"weather_train_df.head()","4d37ed44":"# weather_train_df.describe()","931abddc":"weather_train_df.isna().sum()","45278809":"weather_train_df.shape","b219122e":"weather_train_df.groupby('site_id').apply(lambda group: group.isna().sum())","9e06f5ce":"weather_train_df = weather_train_df.groupby('site_id').apply(lambda group: group.interpolate(limit_direction='both'))","ce169276":"weather_train_df.groupby('site_id').apply(lambda group: group.isna().sum())","097f0a2b":"def add_lag_feature(weather_df, window=3):\n    group_df = weather_df.groupby('site_id')\n    cols = ['air_temperature', 'cloud_coverage', 'dew_temperature', 'precip_depth_1_hr', 'sea_level_pressure', 'wind_direction', 'wind_speed']\n    rolled = group_df[cols].rolling(window=window, min_periods=0)\n    lag_mean = rolled.mean().reset_index().astype(np.float16)\n    lag_max = rolled.max().reset_index().astype(np.float16)\n    lag_min = rolled.min().reset_index().astype(np.float16)\n    lag_std = rolled.std().reset_index().astype(np.float16)\n    for col in cols:\n        weather_df[f'{col}_mean_lag{window}'] = lag_mean[col]\n        weather_df[f'{col}_max_lag{window}'] = lag_max[col]\n        weather_df[f'{col}_min_lag{window}'] = lag_min[col]\n        weather_df[f'{col}_std_lag{window}'] = lag_std[col]","da3bf2c7":"add_lag_feature(weather_train_df, window=3)\nadd_lag_feature(weather_train_df, window=72)","20c7578b":"weather_train_df.head()","1efab6c0":"weather_train_df.columns","f5113440":"# categorize primary_use column to reduce memory on merge...\n\nprimary_use_list = building_meta_df['primary_use'].unique()\nprimary_use_dict = {key: value for value, key in enumerate(primary_use_list)} \nprint('primary_use_dict: ', primary_use_dict)\nbuilding_meta_df['primary_use'] = building_meta_df['primary_use'].map(primary_use_dict)\n\ngc.collect()","65f69dff":"reduce_mem_usage(train_df, use_float16=True)\nreduce_mem_usage(building_meta_df, use_float16=True)\nreduce_mem_usage(weather_train_df, use_float16=True)","43ebf18a":"building_meta_df.head()","0603c39d":"category_cols = ['building_id', 'site_id', 'primary_use']  # , 'meter'\nfeature_cols = ['square_feet', 'year_built'] + [\n    'hour', 'weekend', # 'month' , 'dayofweek'\n    'building_median'] + [\n    'air_temperature', 'cloud_coverage',\n    'dew_temperature', 'precip_depth_1_hr', 'sea_level_pressure',\n    'wind_direction', 'wind_speed', 'air_temperature_mean_lag72',\n    'air_temperature_max_lag72', 'air_temperature_min_lag72',\n    'air_temperature_std_lag72', 'cloud_coverage_mean_lag72',\n    'dew_temperature_mean_lag72', 'precip_depth_1_hr_mean_lag72',\n    'sea_level_pressure_mean_lag72', 'wind_direction_mean_lag72',\n    'wind_speed_mean_lag72', 'air_temperature_mean_lag3',\n    'air_temperature_max_lag3',\n    'air_temperature_min_lag3', 'cloud_coverage_mean_lag3',\n    'dew_temperature_mean_lag3',\n    'precip_depth_1_hr_mean_lag3', 'sea_level_pressure_mean_lag3',\n    'wind_direction_mean_lag3', 'wind_speed_mean_lag3']","ff9d57cb":"def create_X_y(train_df, target_meter):\n    target_train_df = train_df[train_df['meter'] == target_meter]\n    target_train_df = target_train_df.merge(building_meta_df, on='building_id', how='left')\n    target_train_df = target_train_df.merge(weather_train_df, on=['site_id', 'timestamp'], how='left')\n    X_train = target_train_df[feature_cols + category_cols]\n    y_train = target_train_df['meter_reading_log1p'].values\n\n    del target_train_df\n    return X_train, y_train","0dba7043":"def fit_lgbm(train, val, devices=(-1,), seed=None, cat_features=None, num_rounds=1500, lr=0.1, bf=0.1):\n    \"\"\"Train Light GBM model\"\"\"\n    X_train, y_train = train\n    X_valid, y_valid = val\n    metric = 'l2'\n    params = {'num_leaves': 31,\n              'objective': 'regression',\n#               'max_depth': -1,\n              'learning_rate': lr,\n              \"boosting\": \"gbdt\",\n              \"bagging_freq\": 5,\n              \"bagging_fraction\": bf,\n              \"feature_fraction\": 0.9,\n              \"metric\": metric,\n#               \"verbosity\": -1,\n#               'reg_alpha': 0.1,\n#               'reg_lambda': 0.3\n              }\n    device = devices[0]\n    if device == -1:\n        # use cpu\n        pass\n    else:\n        # use gpu\n        print(f'using gpu device_id {device}...')\n        params.update({'device': 'gpu', 'gpu_device_id': device})\n\n    params['seed'] = seed\n\n    early_stop = 20\n    verbose_eval = 20\n\n    d_train = lgb.Dataset(X_train, label=y_train, categorical_feature=cat_features)\n    d_valid = lgb.Dataset(X_valid, label=y_valid, categorical_feature=cat_features)\n    watchlist = [d_train, d_valid]\n\n    print('training LGB:')\n    model = lgb.train(params,\n                      train_set=d_train,\n                      num_boost_round=num_rounds,\n                      valid_sets=watchlist,\n                      verbose_eval=verbose_eval,\n                      early_stopping_rounds=early_stop)\n\n    # predictions\n    y_pred_valid = model.predict(X_valid, num_iteration=model.best_iteration)\n    \n    print('best_score', model.best_score)\n    log = {'train\/mae': model.best_score['training']['l2'],\n           'valid\/mae': model.best_score['valid_1']['l2']}\n    return model, y_pred_valid, log","2dd3a45c":"folds = 5\nseed = 666\nshuffle = False\nkf = KFold(n_splits=folds, shuffle=shuffle, random_state=seed)","cefb8c1c":"target_meter = 0\nX_train, y_train = create_X_y(train_df, target_meter=target_meter)\ny_valid_pred_total = np.zeros(X_train.shape[0])\ngc.collect()\nprint('target_meter', target_meter, X_train.shape)\n\ncat_features = [X_train.columns.get_loc(cat_col) for cat_col in category_cols]\nprint('cat_features', cat_features)\n\nmodels0 = []\nfor train_idx, valid_idx in kf.split(X_train, y_train):\n    train_data = X_train.iloc[train_idx,:], y_train[train_idx]\n    valid_data = X_train.iloc[valid_idx,:], y_train[valid_idx]\n\n    print('train', len(train_idx), 'valid', len(valid_idx))\n#     model, y_pred_valid, log = fit_cb(train_data, valid_data, cat_features=cat_features, devices=[0,])\n    model, y_pred_valid, log = fit_lgbm(train_data, valid_data, cat_features=category_cols,\n                                        num_rounds=1000, lr=0.05, bf=0.7)\n    y_valid_pred_total[valid_idx] = y_pred_valid\n    models0.append(model)\n    gc.collect()\n    if debug:\n        break\n\nsns.distplot(y_train)\ndel X_train, y_train\ngc.collect()","f6f2ac4e":"def plot_feature_importance(model):\n    importance_df = pd.DataFrame(model.feature_importance(),\n                                 index=feature_cols + category_cols,\n                                 columns=['importance']).sort_values('importance')\n    fig, ax = plt.subplots(figsize=(8, 8))\n    importance_df.plot.barh(ax=ax)\n    fig.show()","27459549":"target_meter = 1\nX_train, y_train = create_X_y(train_df, target_meter=target_meter)\ny_valid_pred_total = np.zeros(X_train.shape[0])\ngc.collect()\nprint('target_meter', target_meter, X_train.shape)\n\ncat_features = [X_train.columns.get_loc(cat_col) for cat_col in category_cols]\nprint('cat_features', cat_features)\n\nmodels1 = []\nfor train_idx, valid_idx in kf.split(X_train, y_train):\n    train_data = X_train.iloc[train_idx,:], y_train[train_idx]\n    valid_data = X_train.iloc[valid_idx,:], y_train[valid_idx]\n\n    print('train', len(train_idx), 'valid', len(valid_idx))\n#     model, y_pred_valid, log = fit_cb(train_data, valid_data, cat_features=cat_features, devices=[0,])\n    model, y_pred_valid, log = fit_lgbm(train_data, valid_data, cat_features=category_cols, num_rounds=1000,\n                                       lr=0.05, bf=0.5)\n    y_valid_pred_total[valid_idx] = y_pred_valid\n    models1.append(model)\n    gc.collect()\n    if debug:\n        break\n\nsns.distplot(y_train)\ndel X_train, y_train\ngc.collect()","db867f0a":"target_meter = 2\nX_train, y_train = create_X_y(train_df, target_meter=target_meter)\ny_valid_pred_total = np.zeros(X_train.shape[0])\n\ngc.collect()\nprint('target_meter', target_meter, X_train.shape)\n\ncat_features = [X_train.columns.get_loc(cat_col) for cat_col in category_cols]\nprint('cat_features', cat_features)\n\nmodels2 = []\nfor train_idx, valid_idx in kf.split(X_train, y_train):\n    train_data = X_train.iloc[train_idx,:], y_train[train_idx]\n    valid_data = X_train.iloc[valid_idx,:], y_train[valid_idx]\n\n    print('train', len(train_idx), 'valid', len(valid_idx))\n#     model, y_pred_valid, log = fit_cb(train_data, valid_data, cat_features=cat_features, devices=[0,])\n    model, y_pred_valid, log = fit_lgbm(train_data, valid_data, cat_features=category_cols,\n                                        num_rounds=1000, lr=0.05, bf=0.8)\n    y_valid_pred_total[valid_idx] = y_pred_valid\n    models2.append(model)\n    gc.collect()\n    if debug:\n        break\n\nsns.distplot(y_train)\ndel X_train, y_train\ngc.collect()","fa30b124":"target_meter = 3\nX_train, y_train = create_X_y(train_df, target_meter=target_meter)\ny_valid_pred_total = np.zeros(X_train.shape[0])\n\ngc.collect()\nprint('target_meter', target_meter, X_train.shape)\n\ncat_features = [X_train.columns.get_loc(cat_col) for cat_col in category_cols]\nprint('cat_features', cat_features)\n\nmodels3 = []\nfor train_idx, valid_idx in kf.split(X_train, y_train):\n    train_data = X_train.iloc[train_idx,:], y_train[train_idx]\n    valid_data = X_train.iloc[valid_idx,:], y_train[valid_idx]\n\n    print('train', len(train_idx), 'valid', len(valid_idx))\n#     model, y_pred_valid, log = fit_cb(train_data, valid_data, cat_features=cat_features, devices=[0,])\n    model, y_pred_valid, log = fit_lgbm(train_data, valid_data, cat_features=category_cols, num_rounds=1000,\n                                       lr=0.03, bf=0.9)\n    y_valid_pred_total[valid_idx] = y_pred_valid\n    models3.append(model)\n    gc.collect()\n    if debug:\n        break\n\nsns.distplot(y_train)\ndel X_train, y_train\ngc.collect()","fa6a976f":"print('loading...')\ntest_df = pd.read_feather(root\/'test.feather')\nweather_test_df = pd.read_feather(root\/'weather_test.feather')\n\nprint('preprocessing building...')\ntest_df['date'] = test_df['timestamp'].dt.date\npreprocess(test_df)\ntest_df['building_mean'] = test_df['building_id'].map(building_mean)\ntest_df['building_median'] = test_df['building_id'].map(building_median)\ntest_df['building_min'] = test_df['building_id'].map(building_min)\ntest_df['building_max'] = test_df['building_id'].map(building_max)\ntest_df['building_std'] = test_df['building_id'].map(building_std)\n\nprint('preprocessing weather...')\nweather_test_df = weather_test_df.groupby('site_id').apply(lambda group: group.interpolate(limit_direction='both'))\nweather_test_df.groupby('site_id').apply(lambda group: group.isna().sum())\n\nadd_lag_feature(weather_test_df, window=3)\nadd_lag_feature(weather_test_df, window=72)\n\nprint('reduce mem usage...')\nreduce_mem_usage(test_df, use_float16=True)\nreduce_mem_usage(weather_test_df, use_float16=True)\n\ngc.collect()","ee1fc0fa":"sample_submission = pd.read_feather(os.path.join(root, 'sample_submission.feather'))\nreduce_mem_usage(sample_submission)","5362e1ed":"def create_X(test_df, target_meter):\n    target_test_df = test_df[test_df['meter'] == target_meter]\n    target_test_df = target_test_df.merge(building_meta_df, on='building_id', how='left')\n    target_test_df = target_test_df.merge(weather_test_df, on=['site_id', 'timestamp'], how='left')\n    X_test = target_test_df[feature_cols + category_cols]\n    return X_test","956aa6a2":"def pred(X_test, models, batch_size=1000000):\n    iterations = (X_test.shape[0] + batch_size -1) \/\/ batch_size\n    print('iterations', iterations)\n\n    y_test_pred_total = np.zeros(X_test.shape[0])\n    for i, model in enumerate(models):\n        print(f'predicting {i}-th model')\n        for k in tqdm(range(iterations)):\n            y_pred_test = model.predict(X_test[k*batch_size:(k+1)*batch_size], num_iteration=model.best_iteration)\n            y_test_pred_total[k*batch_size:(k+1)*batch_size] += y_pred_test\n\n    y_test_pred_total \/= len(models)\n    return y_test_pred_total\n","02e49503":"%%time\nX_test = create_X(test_df, target_meter=0)\ngc.collect()\n\ny_test0 = pred(X_test, models0)\n\nsns.distplot(y_test0)\n\ndel X_test\ngc.collect()","87204fc8":"%%time\nX_test = create_X(test_df, target_meter=1)\ngc.collect()\n\ny_test1 = pred(X_test, models1)\nsns.distplot(y_test1)\n\ndel X_test\ngc.collect()","7feac4a7":"%%time\nX_test = create_X(test_df, target_meter=2)\ngc.collect()\n\ny_test2 = pred(X_test, models2)\nsns.distplot(y_test2)\n\ndel X_test\ngc.collect()","44809f29":"X_test = create_X(test_df, target_meter=3)\ngc.collect()\n\ny_test3 = pred(X_test, models3)\nsns.distplot(y_test3)\n\ndel X_test\ngc.collect()","24084254":"sample_submission.loc[test_df['meter'] == 0, 'meter_reading'] = np.expm1(y_test0)\nsample_submission.loc[test_df['meter'] == 1, 'meter_reading'] = np.expm1(y_test1)\nsample_submission.loc[test_df['meter'] == 2, 'meter_reading'] = np.expm1(y_test2)\nsample_submission.loc[test_df['meter'] == 3, 'meter_reading'] = np.expm1(y_test3)","21b4c1a7":"sample_submission.to_csv('submission.csv', index=False, float_format='%.4f')","88a36e8a":"sample_submission.head()","f838ce91":"np.log1p(sample_submission['meter_reading']).hist()","0f46f2a3":"plot_feature_importance(models0[1])","bc3852fd":"plot_feature_importance(models1[1])","946e3c62":"plot_feature_importance(models2[1])","f8b82705":"plot_feature_importance(models3[1])","73410787":"Some features introduced in https:\/\/www.kaggle.com\/ryches\/simple-lgbm-solution by @ryches\n\nFeatures that are likely predictive:\n\n#### Weather\n\n- time of day\n- holiday\n- weekend\n- cloud_coverage + lags\n- dew_temperature + lags\n- precip_depth + lags\n- sea_level_pressure + lags\n- wind_direction + lags\n- wind_speed + lags\n\n#### Train\n\n- max, mean, min, std of the specific building historically\n\n\n\nHowever we should be careful of putting time feature, since we have only 1 year data in training,\nincluding `date` makes overfiting to training data.\n\nHow about `month`? It may be better to check performance by cross validation.\nI go not using this data in this kernel for robust modeling.","e3b867c5":"# Train model\n\nTo win in kaggle competition, how to evaluate your model is important.\nWhat kind of cross validation strategy is suitable for this competition? This is time series data, so it is better to consider time-splitting.\n\nHowever this notebook is for simple tutorial, so I will proceed with KFold splitting without shuffling, so that at least near-term data is not included in validation.","7fef09a0":"# ASHRAE - Great Energy Predictor III\n\n\nOur aim in this competition is to predict energy consumption of buildings.\n\nThere are 4 types of energy to predict:\n\n - 0: electricity\n - 1: chilledwater\n - 2: steam\n - 3: hotwater\n\nElectricity and water consumption may have different behavior!\nSo I tried to separately train & predict the model.\n\nI moved previous [ASHRAE: Simple LGBM submission](https:\/\/www.kaggle.com\/corochann\/ashrae-simple-lgbm-submission) kernel.\n\n**[Update] I published \"[Optuna tutorial for hyperparameter optimization](https:\/\/www.kaggle.com\/corochann\/optuna-tutorial-for-hyperparameter-optimization)\" notebook.\nPlease also check it :)**","f6d67287":"# References\n\nThese kernels inspired me to write this kernel, thank you for sharing!\n\n - https:\/\/www.kaggle.com\/rishabhiitbhu\/ashrae-simple-eda\n - https:\/\/www.kaggle.com\/isaienkov\/simple-lightgbm\n - https:\/\/www.kaggle.com\/ryches\/simple-lgbm-solution","df84de5b":"## Removing weired data on site_id 0\n\nAs you can see above, this data looks weired until May 20. It is reported in [this discussion](https:\/\/www.kaggle.com\/c\/ashrae-energy-prediction\/discussion\/113054#656588) by @barnwellguy that **All electricity meter is 0 until May 20 for site_id == 0**. I will remove these data from training data.\n\nIt corresponds to `building_id <= 104`.","afbd827d":"# Fast data loading\n\nThis kernel uses the preprocessed data from my previous kernel, [\nASHRAE: feather format for fast loading](https:\/\/www.kaggle.com\/corochann\/ashrae-feather-format-for-fast-loading), to accelerate data loading!","41368712":"# Data preprocessing\n\nNow, Let's try building GBDT (Gradient Boost Decision Tree) model to predict `meter_reading_log1p`. I will try using LightGBM in this notebook.","5730f00b":"# Prediction on test data","34e037e8":"# Fill Nan value in weather dataframe by interpolation\n\n\nweather data has a lot of NaNs!!\n\n![](http:\/\/)I tried to fill these values by **interpolating** data.","5bff31dc":"Seems number of nan has reduced by `interpolate` but some property has never appear in specific `site_id`, and nan remains for these features.","cd24e256":"## lags\n\nAdding some lag feature","de89ee5f":"# Train model by each meter type","a6245695":"# Add time feature"}}