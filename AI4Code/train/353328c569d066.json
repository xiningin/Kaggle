{"cell_type":{"38aca310":"code","6da3feb9":"code","2692e697":"code","206a4a55":"code","3f7cd39a":"code","2a12d7bc":"code","f9d1dc98":"code","74a55f08":"code","0766a402":"code","46998a15":"code","4a4ac30e":"code","69bf1a7a":"code","400a0ddb":"code","9251e9d7":"code","69429ee5":"code","7f5a1c04":"code","26c3929d":"code","50eb930a":"code","1ab123b3":"code","c7ad427d":"code","2592db92":"code","72443c6a":"code","44dc793a":"code","9ab8906a":"code","caa4e4dc":"code","27fe1f37":"code","f81be930":"code","52c9817f":"code","eca0780e":"code","36252720":"code","1858b057":"code","fc2037ba":"code","c0e0393c":"code","789cfc11":"code","c30b4011":"code","21c11750":"code","c77de9ce":"code","91185422":"markdown","5c5178c3":"markdown","42a44b73":"markdown","0a1c0c19":"markdown","4d2cb874":"markdown","3c7b2235":"markdown","6369075f":"markdown","9319182e":"markdown","07ecdd3d":"markdown","8ec37846":"markdown","2fd6cce8":"markdown","585cd961":"markdown","7104f36a":"markdown","02eba36f":"markdown","369d6191":"markdown"},"source":{"38aca310":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport tensorflow as tf","6da3feb9":"# Check our GPU Availability\n!nvidia-smi","2692e697":"# load the fake and real news datasets\nfake_news = pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/Fake.csv\")\nfake_news.head()","206a4a55":"true_news = pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/True.csv\")\ntrue_news.head()","3f7cd39a":"# Create a column with fake=1 in fake_news dataset\nfake_news['fake']=1\nfake_news.head()","2a12d7bc":"# Create a column with fake=0 in true_news dataset\ntrue_news['fake'] = 0\ntrue_news.head()","f9d1dc98":"# Concat two fake and true news\nnews = pd.concat([fake_news, true_news])\nnews.sample(5)","74a55f08":"# Check for any null values\nnews.isna().sum()","0766a402":"# Check the info\nnews.info()","46998a15":"# Explore the target variable\nsns.countplot(x='fake', data=news)\n","4a4ac30e":"# Explore 2 text for the fake dataset\nnews[news['fake']==1]['text'].head(2)","69bf1a7a":"# Explore 2 text for true news\nnews[news['fake']==0]['text'].head(2)","400a0ddb":"# Explore the subject column\nplt.figure(figsize=(10,5))\nsns.countplot(x='subject', data=news, hue='fake')","9251e9d7":"news['date'] = pd.to_datetime(news['date'], errors='coerce')\nnews['Year'] = news['date'].dt.year\nnews['Month'] = news['date'].dt.month\n\nnews.head()","69429ee5":"# check the impact of yead on tha target\nsns.countplot(x='Year', data=news, hue='fake')","7f5a1c04":"# Check the impact of Month on the target variable\nsns.countplot(x='Month', data=news, hue='fake')","26c3929d":"news['text'] = news['title'] + news['text']\nnews.drop(labels=['title'], axis=1,inplace=True)\nnews.head()","50eb930a":"news.drop(labels=['subject','date', 'Year','Month'], axis=1, inplace=True)\nnews.head()","1ab123b3":"# We will shuffle the dataframe and extract the feature and label\nnews = news.sample(frac=1)\nnews.head()","c7ad427d":"# Split the dataset into training and testing\nfrom sklearn.model_selection import train_test_split\n\ntrain_sentences, val_sentences, train_labels, val_labels=train_test_split(news['text'].to_numpy(),\n                                                                            news['fake'].to_numpy(),\n                                                                            test_size=0.2,\n                                                                            random_state=42)\n","2592db92":"len(train_sentences),len(val_sentences),len(train_labels),len(val_labels)","72443c6a":"# Check the first 10 samples\ntrain_sentences[:2], train_labels[:10]","44dc793a":"# find the average number of tokens (words) in the training tweets\nround(sum([len(i.split()) for i in train_sentences])\/len(train_sentences))","9ab8906a":"# Setup text vectorization variables\nmax_vocab_length = 10000\nmax_length = 418\n\n\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\ntext_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n                                   output_mode='int',\n                                   output_sequence_length=max_length)","caa4e4dc":"# Fit the text vectorizer to the training text\ntext_vectorizer.adapt(train_sentences)","27fe1f37":"# Create a sample sentences and tekenize it\nsample_sentence = \"Please Do Not Forget To Upvoted\"\ntext_vectorizer([sample_sentence])","f81be930":"# choose a random sentence from the training dataset and tokeize it\nimport random\nrandom_sentence = random.choice(train_sentences)\nprint(f\"Original text;\\n{random_sentence}\\\n\\n\\n Vectorized Version:\")\ntext_vectorizer([random_sentence])","52c9817f":"words = text_vectorizer.get_vocabulary()\nlen(words)","eca0780e":"from tensorflow.keras import layers\n\nembedding = layers.Embedding(input_dim=max_vocab_length,\n                            output_dim=128,\n                            embeddings_initializer='uniform',\n                            input_length=max_length)\nembedding","36252720":"# Get a random sentence from the training set\nrandom_sentenc = random.choice(train_sentences)\nprint(f\"Original text:\\n{random_sentence}\\\n      \\n\\nEmbedd version: \")\nembedding(text_vectorizer([random_sentence]))","1858b057":"# Create an LSTM model\nfrom tensorflow.keras import layers\ninputs = layers.Input(shape=(1,),dtype='string')\n\n# Pass inputs to text_vectorizer(convert text into numbers)\nx = text_vectorizer(inputs) \n\n# Convert text_vectorizer layer into embedding layer\nx = embedding(x)\n\n# Model \nx = layers.LSTM(64)(x)\n\n\n# output\noutputs = layers.Dense(1, activation='sigmoid')(x)\n\n# Pass inputs and outputs to our model\nmodel = tf.keras.Model(inputs, outputs, name='model_LSTM')","fc2037ba":"# Get a summary\nmodel.summary()","c0e0393c":"# Compile the model\nmodel.compile(loss='binary_crossentropy',\n             optimizer=tf.keras.optimizers.Adam(),\n             metrics=['accuracy'])","789cfc11":"# Fit the model\nmodel_history = model.fit(train_sentences,\n                         train_labels,\n                          epochs=5,\n                         validation_data=(val_sentences, val_labels))","c30b4011":"# Make predictions \nmodel_prediction = model.predict(val_sentences)\nmodel_prediction[:10]","21c11750":"# Convert model prediction to our val_labels\nmodel_preds = tf.squeeze(tf.round(model_prediction))\nmodel_preds[:10]","c77de9ce":"# Evaluatinon metrics\nfrom sklearn.metrics import accuracy_score, recall_score,precision_score, f1_score\n\nprint(f\"Accuracy Score: {accuracy_score(val_labels,model_preds)}\")\nprint(f\"Recall Score : {recall_score(val_labels, model_preds)}\")\nprint(f\"Precsion Score : {precision_score(val_labels, model_preds)}\")\nprint(f\"f1 Score : {f1_score(val_labels, model_preds)}\")","91185422":"**We got 99% accuracy on valid data**","5c5178c3":"## Creating an Embedding using an Embedding Layer\nTo make our embedding we're going to use TensorFlow's embedding layer\n\nThe parameters we care most about for our embedding layer:\n\n* input_dim = the size of our vocabulary\n* output_dim = the size of output embedding vector, for example, a value of 100 would mean each token gets represented by a vector 100 long\n* input_length = length of the sequences being passed to be embedding layer","42a44b73":"## Import Required Libraries","0a1c0c19":"## Text Vectorization (tokenization)","4d2cb874":"We will be using LSTM(long-short term memory) neural network.","3c7b2235":"## Exploratory Data Analysis and Data Visualizations","6369075f":"## Converting text into numbers\nWhen dealing with a text problem, one of the first things you'll have to do before you cna build a model is to covert your text to numbers.\n\nThere are a few ways to do this, namely:\n\n* Tokenization -direct mapping of token (a token could be a word or a character ) to a number.\n* Embedding - create a matrix of feature vector for each token (the size of the feature vector can be defined and this embedding can be learned).\n","9319182e":"We will load the individual dataset, create a target attribute which will indicate '1' if the news is fake. Combine both the dataframes and create the combine dataframe for modelling","07ecdd3d":"## Load the dataset","8ec37846":"## Modelling","2fd6cce8":"## Preparing the final data\n\nWe will remove the subject attribute - Since it perfectly distributes the target variable We will remove the Year attribute - This also has a clear division for the target variable We will remove the Month Attribute - This also has a very clear approach of demarcating the target variable\n\nFor now we will just go ahead with the \"text attribute\"","585cd961":"**We will combine the title and text column**","7104f36a":"## Split the dataset into training and testing","02eba36f":"## Feature Engineering\n\nWe will create a new columns calld Month and Year from Date and Analyse whether fake or true news has some correlation with Month or Year in the timeline","369d6191":"**References for Feature Engineering and EDA**: https:\/\/www.kaggle.com\/suvofalcon\/fake-real-news-tensorflow-hub-99-accuracy"}}