{"cell_type":{"18e0cc3d":"code","9c915039":"code","eeee7e1d":"code","9d049265":"code","5e4b7182":"code","7e3d603f":"code","708a5f64":"code","605d0854":"code","f9c3f0b8":"code","287149ec":"code","b21bf34f":"code","c0bfc167":"code","97a73224":"code","7dcae462":"code","95d49a72":"code","b5380a7b":"code","ceed3090":"code","1140f41e":"code","6bcc8c9f":"code","a1056dc6":"code","c7d2875a":"code","9deb3bb4":"code","88690365":"code","dbb69533":"code","c424063c":"code","46c05b15":"code","41a1f14d":"code","6cd3c9e5":"code","188e13d6":"code","745359f5":"code","0cfc1313":"code","d9074fbf":"code","850bcd6c":"code","4f6aef8e":"code","c2e55e84":"code","bd7c02c9":"code","baf0fcab":"code","27c49846":"code","79cdc5c6":"code","91c0dd69":"code","6ed7fca3":"code","64d6a42a":"code","27fa8265":"code","60e17d21":"code","5774c790":"code","2108c301":"code","26eecd4d":"markdown"},"source":{"18e0cc3d":"!pip install scikit-learn==0.24.2\n!pip install talib-binary","9c915039":"TEST_DAY = 3 * 30\n# train_day = 6 * 30\nTRAIN_DAY = -1\nGAP_DAY = 15\nN_SPLIT = 6\nCKPT = \"ckpt\"\nSKIPS = []\nEPS = 1e-18\n\nMODEL_PARAMS = {\n    \"n_estimators\": 1000,\n    \"early_stopping_round\": 50,\n    \"max_depth\": 4,  # choose a very shallow depth to ovoid overfitting.\n    \"random_seed\": 2021,\n    \"learning_rate\": 1e-3,\n    \"colsample_bytree\": 0.3,  # For the most of the time, trader only looks at <= 5 features to make decision. Accordingly, we limite the feature-wise sample size.\n    \"subsample\": 0.3,\n    \"metric\": \"custom\",\n    \"verbosity\": -1,\n    \"min_data_in_leaf\": 100,\n    \"device\": \"gpu\"\n}","eeee7e1d":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb \nimport sklearn\nimport re\nimport os\nimport json\nimport talib\nimport gc\nimport pickle\nfrom tqdm.notebook import tqdm\nfrom scipy.stats import pearsonr\nimport logging\npd.set_option('display.max_rows', 200)\ndef pearson_eval(preds, train_data):\n    \"\"\"customized lgb evaluation method \"\"\"\n    labels = np.nan_to_num(train_data.get_label())\n    return 'corr', pearsonr(labels, np.nan_to_num(preds))[0], True\n\n# logger = logging.getLogger()\n# logger.setLevel(logging.INFO)\n# lgb.register_logger(logger)\ndef weighted_correlation(a, b, weights):\n    w = np.ravel(weights)\n    a = np.ravel(a)\n    b = np.ravel(b)\n\n    sum_w = np.sum(w)\n    mean_a = np.sum(a * w) \/ sum_w\n    mean_b = np.sum(b * w) \/ sum_w\n    var_a = np.sum(w * np.square(a - mean_a)) \/ sum_w\n    var_b = np.sum(w * np.square(b - mean_b)) \/ sum_w\n\n    cov = np.sum((a * b * w)) \/ np.sum(w) - mean_a * mean_b\n    corr = cov \/ (np.sqrt(var_a * var_b) + 1e-12)\n    return corr\n\ndef validate_one_symble(model, features, label):\n    pred = model.predict(features)\n    dummy_weights = np.ones_like(pred)\n    corr = weighted_correlation(label, pred, dummy_weights)\n    return corr\n\ndef neutralize_series(series : pd.Series, by : pd.Series, proportion=1.0):\n    \"\"\"\n    neutralize pandas series (originally from the Numerai Tournament)\n    \"\"\"\n    scores = np.nan_to_num(series.values).reshape(-1, 1)\n    exposures = np.nan_to_num(by.values).reshape(-1, 1)\n    exposures = np.hstack((exposures, np.array([np.mean(np.nan_to_num(series.values))] * len(exposures)).reshape(-1, 1)))\n    correction = proportion * (exposures.dot(np.linalg.lstsq(exposures, scores)[0]))\n    corrected_scores = scores - correction\n    neutralized = pd.Series(corrected_scores.ravel(), index=series.index)\n    return neutralized\n\ndef feature_exposures(df, prediction_name = 'Target'):\n    feature_names = features\n    exposures = []\n    for f in feature_names:\n        fe = np.corrcoef(np.nan_to_num(df[prediction_name].values), np.nan_to_num(df[f].values))[0, 1]\n        exposures.append(fe)\n    return np.array(exposures)\n\ndef max_feature_exposure(df): return np.max(np.abs(feature_exposures(df)))\ndef feature_exposure(df): return np.sqrt(np.mean(np.square(feature_exposures(df))))","9d049265":"def get_feature_name(func_name, meta):\n    if \"timeperiod\" in meta:\n        if isinstance(meta[\"timeperiod\"], str):\n            name = func_name + \"_\" + \"_\".join([str(i) for i in eval(meta[\"timeperiod\"])])\n        else:\n            name = func_name + \"_\" + str(meta[\"timeperiod\"])\n    else:\n        name = func_name\n    return name\n\ndef read_feature_config(symbol):\n    with open(os.path.join(\"..\/input\/bestparameter\", \"best_period_{}.json\".format(symbol))) as f:\n        config = json.load(f)\n    new_config = {}\n    for func_name, setting in config.items():\n        setting[\"func_name\"] = func_name\n        feature_name = get_feature_name(func_name, setting)\n        new_config[feature_name] = setting\n    return new_config","5e4b7182":"def RET(df, n):\n    return df['Close'].pct_change(n)\n\ndef RET_C(df, n):\n    return df['Close'].pct_change(n)\n\ndef RET_H(df, n):\n    return df['High'].pct_change(n)\n    \ndef RET_L(df, n):\n    return df['Low'].pct_change(n)\n    \ndef RET_O(df, n):\n    return df['Open'].pct_change(n)\n    \ndef RET_V(df, n):\n    return df['Volume'].pct_change(n)\n    \ndef RET_VWAP(df, n):\n    return df['VWAP'].pct_change(n)\n    \ndef RET_Cnt(df, n):\n    return df['Count'].pct_change(n)\n\ndef STD(df, n):\n    return df['Close'].pct_change(1).rolling(n).std()\n\ndef RET_STD(df, n):\n    return RET(df, n) * STD(df, n)\n\ndef RSI(df, n):\n    return talib.RSI(df['Close'], n)\n\ndef ATR(df, n):\n    return talib.ATR(df[\"High\"], df.Low, df.Close, n)\n\ndef MFI(df, n):\n    return talib.MFI(df['High'], df['Low'], df['Close'], df['Volume'], n)\n\ndef VOL(df, n):\n    ret = df['Close'].pct_change(1)\n    return np.sqrt((ret ** 2).rolling(n).mean())\n\ndef TRIX(df, n):\n    return talib.TRIX(df['Close'], n)\n\ndef MACD(df, fast, slow):\n    return talib.MACD(df.Close, fast, slow)[0]\n\ndef MACD_HIST(df, fast, slow):\n    return talib.MACD(df.Close, fast, slow)[2]\n\ndef DEMA(df, n1, n2):\n    return np.log(df['Close'].rolling(n1).mean() \/ (df['Close'].rolling(n2).mean() + EPS))\n\ndef EFFICIENCY(df, n):\n    speed = (df.Close - df.Close.shift(n))\n    volatility = (df.Close - df.Close.shift(1)).abs().rolling(n).sum()\n    return speed \/ (volatility+ EPS)\n\ndef EI(df, n):\n    neg = df.Close - df.Low.rolling(n).min()\n    high = df.High.rolling(n).max() - df.Close\n    ei = high \/ (neg + EPS)\n    ei = np.clip(ei, -100, 100)\n    return ei\n\ndef EVEMT_BIGGER_VOLUME(df):\n    return (df.Volume > df.Volume.shift(1)).astype(float)\n\ndef EVENT_MACROSS(df, fast, slow):\n    fast_s = df.Close.rolling(fast).mean()\n    slow_s = df.Close.rolling(slow).mean()\n    return (fast_s > slow_s).astype(float)\n\ndef upper_shadow(df):\n    return df['High'] - np.maximum(df['Close'], df['Open'])\n\ndef lower_shadow(df):\n    return np.minimum(df['Close'], df['Open']) - df['Low']","7e3d603f":"def RET_VEC(df, n):\n    return df['Close'].pct_change(n)\n\ndef RET_C_VEC(df, n):\n    return df['Close'].pct_change(n)\n\ndef RET_H_VEC(df, n):\n    return df['High'].pct_change(n)\n    \ndef RET_L_VEC(df, n):\n    return df['Low'].pct_change(n)\n    \ndef RET_O_VEC(df, n):\n    return df['Open'].pct_change(n)\n    \ndef RET_V_VEC(df, n):\n    return df['Volume'].pct_change(n)\n    \ndef RET_VWAP_VEC(df, n):\n    return df['VWAP'].pct_change(n)\n    \ndef RET_Cnt_VEC(df, n):\n    return df['Count'].pct_change(n)\n\ndef STD_VEC(df, n):\n    return df['Close'].pct_change(1).rolling(n).std()\n\ndef RET_STD_VEC(df, n):\n    return RET_VEC(df, n) * STD_VEC(df, n)\n\ndef RSI_VEC(df, n):\n    close = df[\"Close\"]\n    feat = pd.DataFrame(index=close.index, columns=close.columns)\n    for c in close.columns:\n        feat[c] = talib.RSI(close[c], n)\n    return feat\n\ndef ATR_VEC(df, n):\n    close = df[\"Close\"]\n    feat = pd.DataFrame(index=close.index, columns=close.columns)\n    for c in close.columns:\n        feat[c] = talib.ATR(df[\"High\"][c], df[\"Low\"][c], df[\"Close\"][c], n)\n    return feat\n\ndef MFI_VEC(df, n):\n    close = df[\"Close\"]\n    feat = pd.DataFrame(index=close.index, columns=close.columns)\n    for c in close.columns:\n        feat[c] = talib.MFI(df[\"High\"][c], df[\"Low\"][c], df[\"Close\"][c], df[\"Volume\"][c], n)\n    return feat\n\ndef VOL_VEC(df, n):\n    ret = df['Close'].pct_change(1)\n    return np.sqrt((ret ** 2).rolling(n).mean())\n\ndef TRIX_VEC(df, n):\n    close = df[\"Close\"]\n    feat = pd.DataFrame(index=close.index, columns=close.columns)\n    for c in close.columns:\n        feat[c] = talib.TRIX(close[c], n)\n    return feat\n\ndef MACD_VEC(df, fast, slow):\n    close = df[\"Close\"]\n    feat = pd.DataFrame(index=close.index, columns=close.columns)\n    for c in close.columns:\n        feat[c] = talib.MACD(close[c], fast, slow)[0]\n    return feat\n\ndef MACD_HIST_VEC(df, fast, slow):\n    close = df[\"Close\"]\n    feat = pd.DataFrame(index=close.index, columns=close.columns)\n    for c in close.columns:\n        feat[c] = talib.MACD(close[c], fast, slow)[2]\n    return feat\n\ndef DEMA_VEC(df, n1, n2):\n    return np.log(df['Close'].rolling(n1).mean() \/ (df['Close'].rolling(n2).mean() + EPS))\n\ndef EFFICIENCY_VEC(df, n):\n    speed = (df[\"Close\"] - df[\"Close\"].shift(n))\n    volatility = (df[\"Close\"] - df[\"Close\"].shift(1)).abs().rolling(n).sum()\n    return speed \/ (volatility+ EPS)\n\ndef EI_VEC(df, n):\n    neg = df[\"Close\"] - df[\"Low\"].rolling(n).min()\n    high = df[\"High\"].rolling(n).max() - df[\"Close\"]\n    ei = high \/ (neg + EPS)\n    ei = np.clip(ei, -100, 100)\n    return ei\n\ndef EVEMT_BIGGER_VOLUME_VEC(df):\n    return (df[\"Volume\"] > df[\"Volume\"].shift(1)).astype(float)\n\ndef EVENT_MACROSS_VEC(df, fast, slow):\n    fast_s = df[\"Close\"].rolling(fast).mean()\n    slow_s = df[\"Close\"].rolling(slow).mean()\n    return (fast_s > slow_s).astype(float)\n\ndef upper_shadow_VEC(df):\n    return df['High'] - np.maximum(df['Close'], df['Open'])\n\ndef lower_shadow_VEC(df):\n    return np.minimum(df['Close'], df['Open']) - df['Low']","708a5f64":"def get_features(df, row=False, config=None):\n    df_feat = df[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP']].copy()\n    d = {\"Count\": \"RET_Cnt\", \n         \"Open\": \"RET_O\", \n         \"Close\": \"RET_C\", \n         \"Volume\": \"RET_V\", \n         \"VWAP\": \"RET_VWAP\",\n         \"High\": \"RET_H\",\n         \"Cnt\": \"RET_Cnt\",\n         \"Low\": \"RET_L\",\n        }\n    if config is None:\n        config = {}\n    for col in ['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP']:\n        config[\"RET60_{}\".format(col)] = {\"func_name\": d[col], \"timeperiod\": 60}\n    \n    df_feat['Upper_Shadow'] = upper_shadow(df_feat)\n    df_feat['target_lag'] = df[\"Target\"].shift(16)\n    df_feat['Lower_Shadow'] = lower_shadow(df_feat)\n    df_feat[\"MoM15\"] = df_feat[\"VWAP\"] - df_feat[\"VWAP\"].shift(15)\n    \n    ## possible seasonality, datetime  features (unlikely to me meaningful, given very short time-frames)\n    ### to do: add cyclical features for seasonality\n    times = pd.to_datetime(df[\"timestamp\"],unit=\"s\",infer_datetime_format=True)\n    if row:\n        df_feat[\"hour\"] = times.hour  # .dt\n    else:\n        df_feat[\"hour\"] = times.dt.hour  # .dt\n\n    # self-define feature engineering\n    if config is not None:\n        for feature_name, setting in config.items():\n            func_name = setting[\"func_name\"]\n            func = eval(func_name)\n            if \"timeperiod\" not in setting:\n                # no argument\n                feature: float = func(df_feat)\n            elif isinstance(setting.get(\"timeperiod\"), str):\n                args = eval(setting.get(\"timeperiod\"))\n                feature: float = func(df_feat, *args)\n            else:\n                feature: float = func(df_feat, setting.get(\"timeperiod\"))\n            df_feat[feature_name] = feature\n            \n    df_feat.pop(\"Close\")\n    df_feat.pop(\"High\")\n    df_feat.pop(\"Low\")\n    df_feat.pop(\"Open\")\n    df_feat.pop(\"VWAP\")\n    df_feat.pop(\"Volume\")\n    return df_feat","605d0854":"def get_features_vec(df, config=None):\n    removed_features = [\n    'Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', \"VOL_(.+?)\",\n    \"RET_6\",\"RET_5\",\"RET_24\",\"RET_9\",\"RET_90\",\"RET_92\",\"RET_81\",\"RET_96\",\"RET_40\",\n    \"STD_41\",\"STD_82\",\"STD_43\",\"STD_30\",\"STD_11\",\"STD_6\",\"STD_5\",\"STD_8\",\n    \"RET_STD_86\",\"RET_STD_69\",\"RET_STD_80\",\"RET_STD_10\",\"RET_STD_72\",\"RET_STD_45\",\"RET_STD_48\",\"RET_STD_26\",\"RET_STD_29\",\"RET_STD_25\",\"RET_STD_6\",\n    \"RSI_11\",\"RSI_32\",\"RSI_37\",\"RSI_38\",\"RSI_8\",\"RSI_42\",\"RSI_6\",\"RSI_5\",\"RSI_94\",\n    \"ATR_(.+?)\",\n    \"MFI_21\",\"MFI_13\",\"MFI_48\",\"MFI_38\",\"MFI_10\",\"MFI_69\",\"MFI_8\",\"MFI_6\",\"MFI_96\",\"MFI_99\",\"MFI_5\",\n    \"VOL_34\",\"VOL_82\",\"VOL_42\",\"VOL_96\",\"VOL_14\",\"VOL_10\",\"VOL_6\",\"VOL_5\",\"VOL_2\",\"VOL_4\",\n    \"TRIX_5\",\"TRIX_9\",\"TRIX_12\",\"TRIX_14\",\"TRIX_32\",\"TRIX_34\",\"TRIX_41\",\"TRIX_44\",\"TRIX_49\",\n    \"EFFICIENCY_15\",\"EFFICIENCY_14\",\"EFFICIENCY_13\",\"EFFICIENCY_72\",\"EFFICIENCY_81\",\"EFFICIENCY_92\",\"EFFICIENCY_96\",\"EFFICIENCY_97\",\n    \"EI_11\",\"EI_19\",\"EI_23\",\"EI_35\",\"EI_97\",\"EI_98\",\"EI_96\",\"EI_79\",\"EI_83\",\"EI_64\",\n    \"MACD_HIST_2_133\",\"MACD_HIST_2_183\",\"MACD_HIST_22_53\",\"MACD_HIST_2_53\",\"MACD_HIST_32_63\",\"MACD_HIST_22_183\",\"MACD_HIST_2_33\",\"MACD_HIST_32_183\",\"MACD_HIST_32_193\",\"MACD_HIST_42_163\",\"MACD_HIST_42_183\",\n    \"DEMA_2_28\",\"DEMA_2_63\",\"DEMA_2_108\",\"DEMA_2_138\",\"DEMA_2_143\",\"DEMA_2_153\",\"DEMA_8_13\",\"DEMA_16_158\",\"DEMA_42_98\",\"DEMA_32_98\",\"DEMA_48_133\",\"DEMA_48_143\",\"DEMA_44_63\",\n    \"MACD_4_8\",\"MACD_6_18\",\"MACD_12_13\",\"MACD_2_78\",\"MACD_2_83\",\"MACD_2_98\",\"MACD_2_113\",\"MACD_2_138\",\"MACD_2_153\",\"MACD_8_143\",\"MACD_48_118\",\n    \"EVENT_MACROSS_1_98\",\"EVENT_MACROSS_1_93\",\"EVENT_MACROSS_1_133\",\"EVENT_MACROSS_3_48\",\"EVENT_MACROSS_1_178\",\"EVENT_MACROSS_27_53\",\"EVENT_MACROSS_23_83\",\"EVENT_MACROSS_49_68\",\"EVENT_MACROSS_49_103\",\"EVENT_MACROSS_47_123\",\"EVENT_MACROSS_41_143\",\"EVENT_MACROSS_43_163\",\"EVENT_MACROSS_49_183\",\n    \"RET1_Count\",\"RET1_Open\",\"RET1_High\",\"RET1_Low\",\"RET1_Close\",\"RET1_Volume\",\"RET1_VWAP\"]\n    df_feat = {}\n    d = {\"Count\": \"RET_Cnt\", \n         \"Open\": \"RET_O\", \n         \"Close\": \"RET_C\", \n         \"Volume\": \"RET_V\", \n         \"VWAP\": \"RET_VWAP\",\n         \"High\": \"RET_H\",\n         \"Cnt\": \"RET_Cnt\",\n         \"Low\": \"RET_L\",\n        }\n    if config is None:\n        config = {}\n    for col in ['Count', 'Volume', 'VWAP']:\n        config[\"RET60_{}\".format(col)] = {\"func_name\": d[col], \"timeperiod\": 60}\n        config[\"RET240_{}\".format(col)] = {\"func_name\": d[col], \"timeperiod\": 240}\n    \n    df_feat['Upper_Shadow'] = upper_shadow_VEC(df)\n    df_feat['target_lag'] = df[\"Target\"].shift(16)\n    df_feat[\"MoM1\"] = df[\"Close\"] - df[\"Close\"].shift(1)\n    ## possible seasonality, datetime  features (unlikely to me meaningful, given very short time-frames)\n    ### to do: add cyclical features for seasonality\n    times = pd.to_datetime(df[\"timestamp\"].iloc[:, 0],unit=\"s\",infer_datetime_format=True)\n    df_feat[\"hour\"] = pd.DataFrame(np.broadcast_to(times.dt.hour.values.reshape(-1, 1), df[\"timestamp\"].shape), index=df[\"timestamp\"].index, columns=df[\"timestamp\"].columns)  # .dt\n\n    # self-define feature engineering\n    if config is not None:\n        for feature_name, setting in tqdm(config.items()):\n            remove = False\n            for feat in removed_features:\n                if re.match(feat, feature_name):\n                    remove = True\n                    break\n            if remove:\n                continue\n            args = ()\n            if \"timeperiod\" not in setting:\n                # no argument\n                pass\n            elif isinstance(setting.get(\"timeperiod\"), str):\n                args = eval(setting.get(\"timeperiod\"))\n            else:\n                args = (setting.get(\"timeperiod\"), )\n                \n            vec_func_name = setting[\"func_name\"] + \"_VEC\"\n            func = eval(vec_func_name)\n            df_feat[feature_name] = func(df, *args)\n\n    for feat in removed_features:\n        keys = list(df_feat.keys())\n        for k in keys:\n            if re.match(feat, k):\n                df_feat.pop(k)\n                print(\"remove \", k)\n    return df_feat","f9c3f0b8":"df = pd.read_feather(\"..\/input\/filleddataset\/train.feather\")\ndf['datetime'] = pd.to_datetime(df['timestamp'], unit='s')\ndf = df.set_index([\"Asset_ID\", \"datetime\"], drop=False)","287149ec":"asset_df = pd.read_csv(\"..\/input\/c\/c\/g-research-crypto-forecasting\/asset_details.csv\", index_col=\"Asset_Name\")\nassets = list(asset_df.index)","b21bf34f":"weights = asset_df[\"Weight\"]\nweights = weights \/ weights.sum()","c0bfc167":"period1 = slice(\"2018-1-1\", \"2018-11-1\")  # 178560\n# weighted_ret.loc[period1].plot()\nperiod2 = slice(\"2019-1-1\", \"2019-7-1\")\n# weighted_ret.loc[period2].plot()\nperiod3 = slice(\"2019-8-1\", \"2019-12-1\")\n# weighted_ret.loc[period3].plot()\nperiod4 = slice(\"2020-1-1\", \"2021-5-1\")\n# weighted_ret.loc[period4].plot()\nperiod5 = slice(\"2021-6-1\", \"2021-10-1\")\n# weighted_ret.loc[period5].plot()\nperiod6 = slice(\"2021-10-1\", \"2022-1-25\")\n# weighted_ret.loc[period6].plot()","97a73224":"# preprocess the data\n# 1) delete the unused data\n# 2) forward fill\n# 3) normalize some features\nnew_dfs = {}\nfor asset_name in assets:\n    sub_df = df.loc[asset_name]\n    if asset_name == \"Maker\":\n        sub_df = sub_df.loc[\"2020-08-04\":]\n    elif asset_name == \"Monero\":\n        sub_df = sub_df.loc[\"2018-11-05\":]\n    elif asset_name == \"Stellar\":\n        sub_df = sub_df.loc[\"2018-07-14\":]\n#     sub_df = sub_df.loc[\"2020-1-1\":]\n    index = sub_df.index\n    sub_df = sub_df.fillna(method=\"ffill\").dropna()\n    if sub_df.size == 0:\n        raise ValueError()\n    for c in sub_df.columns:\n        if c in [\"Close\", \"High\", \"Low\", \"Open\", \"VWAP\"]:\n            sub_df[c] = sub_df[c] \/ sub_df[\"Close\"].iloc[0]\n        elif c in [\"Volume\", \"Count\"]:\n            sub_df[c] = sub_df[c] \/ sub_df[c].iloc[0]\n    new_dfs[asset_name] = sub_df\n    \ndf = pd.concat(new_dfs, axis=0, names=[\"Asset_ID\"])\ndel new_dfs","7dcae462":"all_configs = {\n#  \"CHuWhiteCUMSUM_1200\": {\"func_name\": \"CHuWhiteCUMSUM\", \"timeperiod\": \"(1200, 3600)\"},\n#  \"CHuWhiteCUMSUM_240\": {\"func_name\": \"CHuWhiteCUMSUM\", \"timeperiod\": \"(240, 3600)\"},\n#  \"CHuWhiteCUMSUM_90\": {\"func_name\": \"CHuWhiteCUMSUM\", \"timeperiod\": \"(90, 3600)\"},\n#  \"HighLowVolatilityEstimator_240\": {\"func_name\": \"HighLowVolatilityEstimator\", \"timeperiod\": 240},\n#  \"HighLowVolatilityEstimator_90\": {\"func_name\": \"HighLowVolatilityEstimator\", \"timeperiod\": 90},\n#  \"HigLowLiquidityEstimator_240\": {\"func_name\": \"HigLowLiquidityEstimator\", \"timeperiod\": 240},\n#  \"HigLowLiquidityEstimator_90\": {\"func_name\": \"HigLowLiquidityEstimator\", \"timeperiod\": 90},\n    \n 'MFI_23': {'timeperiod': 23, 'correlation': 0, 'func_name': 'MFI'},\n 'DEMA_2_23': {'timeperiod': '(2, 23)', 'correlation': 0, 'func_name': 'DEMA'},\n 'RSI_16': {'timeperiod': 16, 'correlation': 0, 'func_name': 'RSI'},\n 'MACD_2_18': {'timeperiod': '(2, 18)', 'correlation': 0, 'func_name': 'MACD'},\n 'STD_7': {'timeperiod': 7, 'correlation': 0, 'func_name': 'STD'},\n 'EFFICIENCY_84': {'timeperiod': 84, 'correlation': 0,'func_name': 'EFFICIENCY'},\n 'MACD_HIST_2_163': {'timeperiod': '(2, 163)','correlation': 0,'func_name': 'MACD_HIST'},\n 'EVENT_MACROSS_1_83': {'timeperiod': '(1, 83)','correlation': 0,'func_name': 'EVENT_MACROSS'},\n 'RET_STD_90': {'timeperiod': 90, 'correlation': 0, 'func_name': 'RET_STD'},\n 'RSI_96': {'timeperiod': 96, 'correlation': 0, 'func_name': 'RSI'},\n 'TRIX_81': {'timeperiod': 81, 'correlation': 0, 'func_name': 'TRIX'},\n 'EI_20': {'timeperiod': 20, 'correlation': 0, 'func_name': 'EI'},\n 'RET_STD_23': {'timeperiod': 23, 'correlation': 0, 'func_name': 'RET_STD'},\n 'MFI_74': {'timeperiod': 74, 'correlation': 0, 'func_name': 'MFI'},\n 'EI_77': {'timeperiod': 77, 'correlation': 0, 'func_name': 'EI'},\n 'RET_15': {'timeperiod': 15, 'correlation': 0, 'func_name': 'RET'},\n 'STD_96': {'timeperiod': 96, 'correlation': 0, 'func_name': 'STD'},\n 'TRIX_4': {'timeperiod': 4, 'correlation': 0, 'func_name': 'TRIX'},\n 'EFFICIENCY_16': {'timeperiod': 16,'correlation': 0,'func_name': 'EFFICIENCY'},\n}","95d49a72":"# store all data in a dictionary\ndv = {}\nfor feature in tqdm(df.columns):\n    dv[feature] = df.pivot(index=\"timestamp\", columns=\"Asset_ID\", values=feature)\n    dv[feature] = dv[feature].fillna(method=\"ffill\")","b5380a7b":"del df","ceed3090":"dv_features = get_features_vec(dv, all_configs)","1140f41e":"feature_names = list(dv_features)","6bcc8c9f":"target = dv[\"Target\"]\ndel dv","a1056dc6":"import gc; gc.collect()","c7d2875a":"averaged_feature = [\"DEMA_2_23\", \"RET60_VWAP\", \"RSI_16\", \"RET_15\", \n                    \"RSI_96\", \"RET_STD_23\", \"RET_STD_90\", \"TRIX_4\",\n                   \"RET240_VWAP\", \"EFFICIENCY_84\", \"EFFICIENCY_16\", \n                    \"MACD_2_18\",\n                   ]","9deb3bb4":"for feat in dv_features.keys():\n    path = os.path.join(\"{}.parquet\".format(feat))\n    dv_features[feat].to_parquet(path)\n#     if feat not in averaged_feature:\n#         del dv_features[feat]  # save memory","88690365":"rev = list(set(dv_features.keys()) - set(averaged_feature))\nfor feat in rev:\n    print(\"delete \", feat)\n    if feat in dv_features:\n        del dv_features[feat]  # save memory","dbb69533":"gc.collect()","c424063c":"drop_original = False\nfor feat in tqdm(averaged_feature):\n    new_name = feat + \"_residualized\"\n    feature_names.append(new_name)\n    # dv_features[feat] \u662f\u4e00\u4e2a(T, K)\u7684\u77e9\u9635\uff0c\u4ee3\u8868\u6240\u6709\u7684asset\u7684\u6240\u6709\u65f6\u95f4\u6bb5\u7684\u67d0\u4e2afeature\uff0c\u7136\u540e\u51cf\u53bbmean\n    dv_features[new_name] = dv_features[feat].subtract(dv_features[feat].multiply(weights, axis=\"columns\").sum(axis=\"columns\"), axis=\"index\")\n    path = os.path.join(\"{}.parquet\".format(new_name))\n    dv_features[feat].to_parquet(path)\n    if drop_original:\n        feature_names.remove(feat)\n        \n    del dv_features[new_name]\n    if feat in dv_features:\n        del dv_features[feat]","46c05b15":"TEST_SCORE_DF = pd.DataFrame(index=asset_df.index, columns=[period1.start, period2.start, period3.start, period4.start, period5.start, period6.start])\nTRAIN_SCORE_DF = pd.DataFrame(index=asset_df.index, columns=[period1.start, period2.start, period3.start, period4.start, period5.start, period6.start])","41a1f14d":"FEAT_SPLIT_DF = pd.DataFrame()\nFEAT_GAIN_DF = pd.DataFrame()","6cd3c9e5":"def get_score_for_one_symbol_new_cv(all_df, asset_id, feature_names, dry_run=False, model_params={}, dump_root=\"ckpt\"):\n    symbol_df = all_df\n    train_score_by_cv = [0] * N_SPLIT\n    test_score_by_cv = [0] * N_SPLIT\n    train_size_by_cv = [0] * N_SPLIT\n    test_size_by_cv = [0] * N_SPLIT\n    test_period_by_cv = [0] * N_SPLIT\n    test_type_by_cv = [0] * N_SPLIT\n    iter_by_cv = [0] * N_SPLIT\n    df_proc = symbol_df[feature_names]\n    bulls = [period4, period2]\n    bears = [period3]\n    neutral = [period1, period5, period6]\n    for i, period in enumerate([period1, period2, period3, period4, period5, period6]):\n        print(period)\n        train_features, train_target = df_proc.loc[period], symbol_df[\"Target\"].loc[period]\n        test_features, test_target = df_proc.loc[period], symbol_df[\"Target\"].loc[period]\n        if test_features.size == 0:\n            continue\n        part1 = pd.Timestamp(period.start) - pd.Timedelta(\"30D\")\n        part2 = pd.Timestamp(period.stop) + pd.Timedelta(\"30D\")\n        dfs = []\n        targets = []\n        _df1 = df_proc.loc[:part1]\n        _df2 = df_proc.loc[part2:]\n        if _df1.size > 0:\n            dfs.append(_df1)\n            targets.append(symbol_df[\"Target\"].loc[:part1])\n        if _df2.size > 0:\n            dfs.append(_df2)\n            targets.append(symbol_df[\"Target\"].loc[part2:])\n        if len(dfs) == 2:\n            train_features = pd.concat(dfs)\n            train_target = pd.concat(targets)\n        elif len(dfs) == 1:\n            train_features = dfs[0]\n            train_target = targets[0]\n        else:\n            continue\n        train_size = len(train_features)\n        test_size = len(test_features)\n        train_features = train_features.replace([np.inf, -np.inf], np.nan)\n        test_features = test_features.replace([np.inf, -np.inf], np.nan)\n        train_nan_mask = train_features.isnull().any(axis=1)\n        test_nan_mask = test_features.isnull().any(axis=1)\n        train_features, train_target = train_features.loc[~train_nan_mask], train_target.loc[~train_nan_mask]\n        test_features, test_target = test_features.loc[~test_nan_mask], test_target.loc[~test_nan_mask]\n        \n        train_set = lgb.Dataset(train_features, label=train_target, feature_name=feature_names)\n        test_set = lgb.Dataset(test_features, label=test_target, feature_name=feature_names)\n        # continuous\n        \n        assert len(train_features) == len(train_target), \"{}_{}\".format(len(train_features), len(train_target))\n        assert len(test_features) == len(test_target), \"{}_{}\".format(len(train_features), len(train_target))\n        booster = lgb.train(train_set=train_set, params=model_params, valid_sets=[test_set], feval=pearson_eval)\n        corr_train = validate_one_symble(booster, train_features, train_target)\n        corr_test = validate_one_symble(booster, test_features, test_target)\n        TEST_SCORE_DF.loc[asset_id, period.start] = float(corr_test)\n        TRAIN_SCORE_DF.loc[asset_id, period.start] = float(corr_train)\n        for name in feature_names:\n            FEAT_SPLIT_DF.loc[name, \"{}_{}\".format(asset_id, period.start)] = dict(zip(feature_names, booster.feature_importance(\"split\")))[name]\n            FEAT_GAIN_DF.loc[name, \"{}_{}\".format(asset_id, period.start)] = dict(zip(feature_names, booster.feature_importance(\"gain\")))[name]\n\n        train_score_by_cv[i] = float(corr_train)\n        test_score_by_cv[i] = float(corr_test)\n        train_size_by_cv[i] = int(train_size)\n        test_size_by_cv[i] = int(test_size)\n        test_period_by_cv[i] = [period.start, period.stop]\n        if period in bulls:\n            test_type_by_cv[i] = \"bull\"\n        elif period in bears:\n            test_type_by_cv[i] = \"bear\"\n        else:\n            test_type_by_cv[i] = \"neutral\"\n        iter_by_cv[i] = booster.best_iteration\n        str_path = os.path.join(os.getcwd(), dump_root, asset_id, str(i))\n        os.makedirs(str_path, exist_ok=True)\n        model_str = booster.model_to_string()\n        with open(os.path.join(str_path, \"lgb.ckpt\"), \"w\") as f:\n            f.write(model_str)\n        with open(os.path.join(str_path, \"used_features.pickle\"), \"wb\") as f:\n            pickle.dump(feature_names, f)\n        if dry_run:\n            break\n    avg_train_score = sum(train_score_by_cv) \/ N_SPLIT\n    avg_test_score = sum(test_score_by_cv) \/ N_SPLIT\n    best_iteration = booster.best_iteration\n    meta = {\n            \"train_score\": train_score_by_cv,\n            \"test_score\": test_score_by_cv,\n            \"train_size_by_cv\": train_size_by_cv,\n            \"test_size_by_cv\": test_size_by_cv,\n            \"test_type_by_cv\": test_type_by_cv,\n            \"test_period_by_cv\": test_period_by_cv,\n            \"model_params\": model_params,\n            \"avg_train_score\": avg_train_score,\n            \"avg_test_score\": avg_test_score,\n            \"iter_by_cv\": iter_by_cv\n        }\n        \n    meta_path = os.path.join(os.getcwd(), dump_root, asset_id, \"lgb_meta.json\")\n    with open(meta_path, \"w\") as f:\n        f.write(json.dumps(meta, indent=2))\n    return avg_train_score, avg_test_score, meta","188e13d6":"print(feature_names)","745359f5":"print(len(feature_names))","0cfc1313":"train_score_by_symbol = {}\ntest_score_by_symbol = {}\n\nfor asset_id in assets:\n    df = {\"Target\": target[asset_id]}\n    for feat in tqdm(feature_names):\n        feature_df = pd.read_parquet(os.path.join(\"{}.parquet\".format(feat)), columns=[asset_id])[asset_id]\n        df[feat] = feature_df\n        del feature_df\n    df = pd.concat(df, axis=1)\n    df.index = pd.to_datetime(df.index, unit='s')\n    df.dropna(subset=feature_names+[\"Target\"], inplace=True)\n    print(asset_id + \"\\n***\")\n    train_score, test_score, meta = get_score_for_one_symbol_new_cv(df, asset_id, feature_names, dry_run=True, model_params=MODEL_PARAMS, dump_root=CKPT)\n    train_score_by_symbol[asset_id] = train_score\n    test_score_by_symbol[asset_id] = test_score\n    \n    print(meta)\n    print(\"\\n\")","d9074fbf":"TRAIN_SCORE_DF","850bcd6c":"TEST_SCORE_DF","4f6aef8e":"TRAIN_SCORE_DF.to_csv(os.path.join(os.getcwd(), CKPT, \"train_score_df.csv\"))\nTEST_SCORE_DF.to_csv(os.path.join(os.getcwd(), CKPT, \"test_score_df.csv\"))\nFEAT_SPLIT_DF.to_csv(os.path.join(os.getcwd(), CKPT, \"FEAT_SPLIT_DF.csv\"))\nFEAT_GAIN_DF.to_csv(os.path.join(os.getcwd(), CKPT, \"FEAT_GAIN_DF.csv\"))","c2e55e84":"final_train_score = sum([score * weights[s] for s, score in train_score_by_symbol.items()])\nfinal_test_score = sum([score * weights[s] for s, score in test_score_by_symbol.items()])\nprint(\"avg. model score on train: {:.4f}\".format(final_train_score))\nprint(\"avg. model score on test: {:.4f}\".format(final_test_score))","bd7c02c9":"score_by_symbol = pd.DataFrame({\"train_score\": train_score_by_symbol, \"test_score\": test_score_by_symbol}).sort_values(by=\"train_score\")","baf0fcab":"# read baseline model\nnew_meta_root = CKPT\nold_scores = pd.read_csv(os.path.join(\"..\/input\/baselineresult\/ckpt\", \"test_score_df.csv\"), index_col=\"Asset_Name\")\nnew_scores = pd.read_csv(os.path.join(CKPT, \"test_score_df.csv\"), index_col=\"Asset_Name\")\nscore_dif = new_scores - old_scores\n# show_dif = pd.DataFrame(index=new_scores.index, columns=new_scores.columns, dtype=float)\n# for i in new_scores.index:\n#     for j in new_scores.columns:\n#         diff = new_scores.loc[i, j] - old_scores.loc[i, j]\n#         if diff > 0:\n#             show_dif.loc[i,j] = \"{:.2%}(+{:.2%})\".format(new_scores.loc[i, j], diff)\n#         else:\n#             show_dif.loc[i,j] = \"{:.2%}({:.2%})\".format(new_scores.loc[i, j], diff)\n# show_dif","27c49846":"print(\"new fold result\")\nprint(new_scores.multiply(weights, axis=\"index\").sum(axis=0))\nprint(\"baseline fold result\")\nprint(old_scores.multiply(weights, axis=\"index\").sum(axis=0))\n\nprint(\"new fold std result\")\nprint(new_scores.multiply(weights, axis=\"index\").sum(axis=0).std())\nprint(\"baseline fold std result\")\nprint(old_scores.multiply(weights, axis=\"index\").sum(axis=0).std())\n\nprint(\"new fold mean result\")\nprint(new_scores.multiply(weights, axis=\"index\").sum(axis=0).mean())\nprint(\"baseline fold mean result\")\nprint(old_scores.multiply(weights, axis=\"index\").sum(axis=0).mean())","79cdc5c6":"from scipy.stats import ttest_1samp\nscore_dif = score_dif.astype(float)\ndif = np.ravel(score_dif.values)\ndif = dif[~np.isnan(dif)]\nttest_1samp(dif, 0)","91c0dd69":"score_dif.mean(axis=1)","6ed7fca3":"score_dif.mean(axis=0)","64d6a42a":"np.nanmean(score_dif)","27fa8265":"FEAT_GAIN_DF.rank().std(axis=1).sort_values()","60e17d21":"FEAT_SPLIT_DF.rank().std(axis=1).sort_values()","5774c790":"FEAT_GAIN_DF.rank().mean(axis=1).sort_values()","2108c301":"FEAT_SPLIT_DF.rank().mean(axis=1).sort_values()","26eecd4d":"## CrossValidation Method\n- In TimeSeries, TimeSeriesSplit usually is used for cross-validation. However, this will limit the number of training data used for training.\n- According to \"M\u00f6rke, Mathis. \"Marcos L\u00f3pez de Prado: Advances in financial machine learning.\" (2019): 491-493.\", we can also use PurgedKFold to do the cross-validation. In this schema, the purpose is to find the model performance under different market temperature. So we need to mannually find a split of market and test the model performance. Also, in order to prevent look-ahead bias, we use one month gap before and after the test data size.\n\n## Feature Engineering\n- The engineering techniques are described here https:\/\/www.kaggle.com\/axzhang\/pipeline-building-featureengineering-stage1"}}