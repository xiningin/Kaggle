{"cell_type":{"098052fa":"code","57968772":"code","f68d63b8":"code","ea89f099":"code","0d32cad0":"code","f645979e":"code","c5d6f3f1":"code","248d3b53":"markdown","10ac8e26":"markdown","9dc8ab1b":"markdown","f377032d":"markdown","60639a71":"markdown","10554219":"markdown"},"source":{"098052fa":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n# LOAD THE DATA\ntrain = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\n\nprint (train.shape)\nprint (test.shape)\n","57968772":"# PREPARE DATA FOR FEEDING YOUR CNN\nfrom keras.utils.np_utils import to_categorical\nY_train = train[\"label\"]\nX_train = train.drop(labels = [\"label\"],axis = 1)\nX_train = X_train \/ 255.0\nX_test = test \/ 255.0\nX_train = X_train.values.reshape(-1,28,28,1)\nX_test = X_test.values.reshape(-1,28,28,1)\nY_train = to_categorical(Y_train, num_classes = 10)\n\nprint(X_train.shape)\nprint(Y_train.shape)\nprint(X_test.shape)","f68d63b8":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, BatchNormalization\nfrom keras.initializers import he_normal\nfrom keras.optimizers import Adam\n\nnets = 15\nmodel = [0] *nets\n\n#Conv layer kernel sizes for the top 6 layers\nks1=3\nks2=3\nks3=5\nks4=5\nks5=7\nks6=7\ninit = he_normal(seed=82)\n\nfor j in range(nets):\n    model[j] = Sequential()\n\n    model[j].add(Conv2D(32, kernel_size=ks1, activation='relu', kernel_initializer = init, input_shape = (28, 28, 1)))\n    model[j].add(BatchNormalization())\n    #model[j].add(Dropout(0.4))\n    model[j].add(Conv2D(32, kernel_size=ks2, activation='relu', kernel_initializer = init ))\n    model[j].add(BatchNormalization())\n    #model[j].add(Dropout(0.4))\n    model[j].add(Conv2D(32, kernel_size=ks3, activation='relu', kernel_initializer = init ))\n    model[j].add(BatchNormalization())\n    model[j].add(Dropout(0.4))\n\n    model[j].add(Conv2D(64, kernel_size=ks4, activation='relu', kernel_initializer = init ))\n    model[j].add(BatchNormalization())\n    #model[j].add(Dropout(0.4))\n    model[j].add(Conv2D(64, kernel_size=ks5, activation='relu', kernel_initializer = init ))\n    model[j].add(BatchNormalization())\n    #model[j].add(Dropout(0.4))\n    model[j].add(Conv2D(64, kernel_size=ks6, activation='relu', kernel_initializer = init ))\n    model[j].add(BatchNormalization())\n    model[j].add(Dropout(0.4))\n\n    model[j].add(Conv2D(128, kernel_size=4, activation='relu', kernel_initializer = init ))\n    model[j].add(BatchNormalization())\n    model[j].add(Flatten())\n    model[j].add(Dropout(0.4))\n    model[j].add(Dense(10, activation='softmax'))\n\n    optA = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n    model[j].compile(optimizer=optA, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\nmodel[0].summary()\n","ea89f099":"from keras.preprocessing.image import ImageDataGenerator\n\ndatagen = ImageDataGenerator(rotation_range=10, zoom_range = 0.10, width_shift_range=0.1, height_shift_range=0.1)","0d32cad0":"from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience=5, verbose=1, factor=0.31623, min_delta=1e-4)\nearly_stopping = EarlyStopping(monitor='val_acc', min_delta=1e-5, patience=10, verbose=1, mode='max', restore_best_weights=True)\n\n","f645979e":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nhistory = [0] * nets\nepochs = 50\nfor j in range(nets):\n    rs = 10*j + 1\n    X_train2, X_val2, Y_train2, Y_val2 = train_test_split(X_train, Y_train, train_size = 0.9, random_state = rs)\n    history[j] = model[j].fit_generator(datagen.flow(X_train2,Y_train2, batch_size=64), epochs = epochs, steps_per_epoch = X_train2.shape[0]\/\/64, validation_data = (X_val2,Y_val2), callbacks=[learning_rate_reduction, early_stopping], verbose=0)\n    #pred = model[j].predict_classes(X_val2)\n    #Y_val0 = np.argmax(Y_val2,axis=1) \n    maxpos = history[j].history['val_acc'].index(max(history[j].history['val_acc']))\n    print(\"N{0:d}:: Max val_acc={1:.5f} at Epoch {2:d} Min_tr_loss={3:.5f} Min_val_loss={4:.5f}\".format(j+1,max(history[j].history['val_acc']),maxpos+1, min(history[j].history['loss']), min(history[j].history['val_loss']) ))\n","c5d6f3f1":"results = np.zeros( (X_test.shape[0],10) ) \nfor j in range(nets):\n    results = results + model[j].predict(X_test)\n\nresults = np.argmax(results,axis = 1)\n\nresults = pd.Series(results,name=\"Label\")\n\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"MNIST_32C3C3C564C5C7C7128C4FC10_50e_LR10_RLROP_ES_BN_DA_DO40_TR90_N15_ADAM_xx.csv\",index=False)\n\nprint(submission.shape)\n","248d3b53":"**The model** \n\nThere are many structures you may find out. This model below is very much inspired by [Chris Deotte](https:\/\/www.kaggle.com\/cdeotte)'s [25 Million Images! MNIST kernel](https:\/\/www.kaggle.com\/cdeotte\/25-million-images-0-99757-mnist). I highly recommend reading his kernels and also those ones he refers to.\n\nThis model below contains 7 convolutional layers and 1 dense layer in one net\nand it is an ensemble-of-nets type solution which I found very useful to improve accuracy in MNIST competition.\n\nI kept C4 and dense layers fixed and played a bit with the order of top 6 layers and found C3-C3-C5-C5-C7-C7-C4-FC10 structure the best. Feel free to permutate top 6 layers by changing kernel sizes below and see changes in Trainable parameters printed below. For kernel sizes you may use other than 3,3,5,5,7,7 of course but you may need to change other parameters in the model.","10ac8e26":"**Summarize ensemble results and create submission file**","9dc8ab1b":"And here comes the training.\nBefore the actual training there is a further split of the MNIST training dataset into the actual training set and a validation set. This split is randomly done for each network. So very probably we use up all images for training.","f377032d":"**Regularization and optimizatio**n\n\nIn the model there are hundreds of thousands of parameters. \nTo find the best set ... or one of the best sets of parameters in a reasonable time - there are several tricks you may apply.\nWithout going into details these tricks are dropout, batch normalization, ADAM. These are already there, defined in the model.\n\nThere is one more trick in this kernel which is data augmentation. This is a very useful trick and it does a little change in the original image. Using this trick you alter the original image just a little and you do this change in a random way. So while training you take the original image then alter a bit and you do the training. \nTwo things which I learned from this.\n1. Original image never used as input for the network during training.\n2. Practically you never train with same exact image.","60639a71":"**Controlling the process of training**\n","10554219":"**INTRO**\n\nMNIST dataset has 70000 28x28 grayscale images and a single label for each image. \n\nIn this kaggle competition the original MNIST dataset splitted into a train dataset with 42000 images and labels and a test dataset with 28000 images only. \nThere are no labels in kaggle's test dataset. \n\nSo in this competition for training you should use kaggle's train dataset with 42000 images and not the original dataset with 60000 or even 70000 images as some competitors do.\nIf you find submission with kaggle score higher than 0.998 in the Leaderboard, it is very likely that more than 42k images were used for training. \nSee details about it in section 'How much more accuracy is possible?' in [this fine kernel](http:\/\/www.kaggle.com\/cdeotte\/25-million-images-0-99757-mnist) by Chris Deotte.\n"}}