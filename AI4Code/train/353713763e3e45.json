{"cell_type":{"cc82db93":"code","9d5a2c9a":"code","c24082a7":"code","4d039145":"code","ebafad49":"code","589b2ffc":"code","128d0f55":"code","2144d11d":"code","0a95720e":"code","17324e9b":"code","f33fac30":"code","5d452b78":"code","4a67439f":"code","f138ef1f":"code","636dd722":"code","08003399":"code","03fbb839":"code","3c48ac47":"code","423139c5":"code","95a521ac":"code","5314565e":"code","ba426ec3":"code","b99d85ce":"code","8452ef95":"code","9577ef02":"code","9f95924e":"code","87744b2e":"code","9fdaa833":"markdown","c9b5fbb0":"markdown","24bfcc6d":"markdown","ff9d1f8b":"markdown","3ffbff37":"markdown","a70f477f":"markdown","e6e401af":"markdown","23290d9c":"markdown","07a308c4":"markdown","b4e5445e":"markdown","a929b036":"markdown","9860d895":"markdown"},"source":{"cc82db93":"import numpy as np\nimport pandas as pd\nimport scipy as sp\nimport matplotlib.pyplot as plt\nimport torch\nimport copy\nimport math\nimport gc\nfrom tqdm import tqdm\nimport torch.utils.data as D\nimport random\nimport os\nfrom transformers import AutoModelWithLMHead, AutoTokenizer,RobertaConfig, RobertaModel,AutoModelForSequenceClassification,AutoModelForMaskedLM\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch import nn\nfrom torch import optim\nimport time\nimport torch.nn.functional as F\nfrom transformers import (\n    AutoModel,\n    CONFIG_MAPPING,\n    MODEL_MAPPING,\n    AdamW,\n    AutoConfig,\n    AutoModelForMaskedLM,\n    AutoTokenizer,\n    DataCollatorForLanguageModeling,\n    SchedulerType,\n    get_scheduler,\n    set_seed,\n)","9d5a2c9a":"class CFG:\n    seed=75111\n    path='funnel-transformer\/large'\n    checkpoint=None\n    max_len=256\n    batch_size=8\n    grad_avg_n=2\n    lr=3e-5\n    betas=(0.9, 0.999)\n    lr_diff_rate=1\n    weight_decay=0.01\n    dropout_p=0.1\n    initializer=None\n    re_init_n=0\n    epochs=5\n    folds=5\n    cv_shuffle=False\n    val_freq=10\n    patience=1\n    lr_factor=0.1\n    score_avg_n=1\n    pad_token_id=1\n    early_stop_epoch=1000\n    device=torch.device('cuda:0')\n    dtype=torch.float32","c24082a7":"if CFG.dtype==torch.float64:\n    torch.set_default_tensor_type(torch.DoubleTensor)\nelse:\n    torch.set_default_tensor_type(torch.FloatTensor)\ntorch.set_default_dtype(CFG.dtype)","4d039145":"random.seed(CFG.seed)\nos.environ['PYTHONHASHSEED'] = str(CFG.seed)\nnp.random.seed(CFG.seed)\ntorch.manual_seed(CFG.seed)\ntorch.cuda.manual_seed(CFG.seed)\ntorch.cuda.manual_seed_all(CFG.seed)\ntorch.backends.cudnn.deterministic = True","ebafad49":"train_df=pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/train.csv')\ntest_df=pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/test.csv')\nres_df=pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/sample_submission.csv')","589b2ffc":"def CV_split(m,k=5,shuffle=False,seed=7):\n    index=np.arange(m)\n    if shuffle:\n        np.random.seed(seed)\n        np.random.shuffle(index)\n    test_size=math.ceil(m\/k)\n    split_indices=[]\n    for i in range(k):\n        bool_index=np.zeros(m)\n        bool_index[test_size*i:test_size*(i+1)]=1\n        bool_index=bool_index.astype('bool')\n        val_index=index[bool_index]\n        train_index=index[~bool_index]\n        split_indices.append((train_index,val_index))\n    return split_indices\n\ndef score_test(model,test_ldr):\n    print('start eval')\n    model.eval()\n    preds=[]\n    for texts, attns, idx in tqdm(test_ldr):\n        with torch.no_grad():\n            pred = model(texts,attns)\n            preds.append(pred)\n    preds=torch.cat(preds,axis=0)\n    preds=preds.to('cpu').numpy().reshape(-1)\n    return preds\n    \ndef rmse(y1,y2):\n    score=np.sqrt(((y1-y2)**2).mean())\n    return score","128d0f55":"class RobertaDataset(D.Dataset):\n    def __init__(self, token, target):\n        self.token = token\n        self.target = target\n        \n    def __len__(self):\n        return self.token.shape[0]\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.token[idx].input_ids), \\\n                torch.tensor(self.token[idx].attention_mask), self.target[idx]\n    \ndef collate(batch):\n    ids, attns, targets = zip(*batch)\n    ids = pad_sequence(ids, batch_first=True,padding_value=CFG.pad_token_id).to(CFG.device)\n    attns = pad_sequence(attns, batch_first=True,padding_value=CFG.pad_token_id).to(CFG.device)\n    targets = torch.tensor(targets).float().to(CFG.device)\n    return ids, attns, targets\n","2144d11d":"path=CFG.path\nconfig = AutoConfig.from_pretrained(path, output_hidden_states=True,attention_probs_dropout_prob=CFG.dropout_p,hidden_dropout_prob=CFG.dropout_p)\ntokenizer = AutoTokenizer.from_pretrained(path,model_max_length=CFG.max_len)\nCFG.pad_token_id=tokenizer.pad_token_id\nmodel = AutoModel.from_pretrained(path,config=config)","0a95720e":"if CFG.checkpoint is not None:\n    checkpoint=torch.load('..\/input\/'+CFG.checkpoint+'\/ITPT_state_dict',map_location=CFG.device)\n    model.load_state_dict(checkpoint['model_state_dict'])","17324e9b":"config.save_pretrained('model_init')\nmodel.save_pretrained('model_init')\ntokenizer.save_pretrained('model_init')","f33fac30":"model","5d452b78":"config","4a67439f":"def tokenize(tokenizer,texts):\n    tokens=[]\n    for text in texts:\n        token=tokenizer(text,max_length=CFG.max_len,truncation=True, padding='max_length',add_special_tokens=True)\n        #print(len(token['input_ids']))\n        tokens.append(token)\n    return tokens","f138ef1f":"train_df['token'] = tokenize(tokenizer,train_df.excerpt)\ntest_df['token'] = tokenize(tokenizer,test_df.excerpt)\n\nds1 = RobertaDataset(train_df.token, train_df.target)\nds2 = RobertaDataset(test_df.token, test_df.index)\n\ntest_ldr = D.DataLoader(ds2, batch_size=CFG.batch_size,\n                        shuffle=False, collate_fn = collate,num_workers=0)","636dd722":"class MyModel(nn.Module):\n    def __init__(self, model):\n        super(MyModel, self).__init__()\n        self.model = model\n        self.linear1=nn.Linear(1024,1)\n\n    def forward(self, text,attention_mask):\n        x1 = self.model(text,attention_mask)\n        last_hidden_state = x1['hidden_states'][-1]\n        text_emb=last_hidden_state.mean(axis=1)\n        x2=self.linear1(text_emb)\n        return x2","08003399":"model=MyModel(model)\ninit_state=copy.deepcopy(model.state_dict())","03fbb839":"@torch.no_grad()\ndef val_rmse(model,loader, f_loss,mode='train'):\n    loss_seq = []\n    pred_seq=[]\n    target_seq=[]\n    if mode=='eval':\n        model.eval()\n    elif mode=='train':\n        model.train()\n    for texts, attns, target in loader:\n        pred = model(texts, attention_mask=attns).reshape(-1)\n        loss = f_loss(pred, target).item()\n        loss_seq.append(loss)\n        pred_seq.append(pred.to('cpu').numpy())\n        target_seq.append(target.to('cpu').numpy())\n    loss = np.sqrt(np.array(loss_seq).mean())\n    pred=np.concatenate(pred_seq)\n    target=np.concatenate(target_seq)\n    return loss,pred,target","3c48ac47":"class StateRecorder:\n    def __init__(self):\n        self.best_score=float('inf')\n        self.best_state_dict=None\n        self.best_epoch=0\n        self.stop=False\n        self.scores=[]\n        self.cv_scores=[]\n        return\n    \n    def record(self,score,epoch,model):\n        self.scores.append(score)\n        avg_score=np.mean(self.scores[-CFG.score_avg_n:])\n        self.cv_scores.append(avg_score)\n        print(f'average({CFG.score_avg_n}) validation (train) rmse:{avg_score.round(6)}')\n        if avg_score<self.best_score:\n            self.best_score=avg_score\n            self.best_state_dict={}\n            for k,v in model.state_dict().items():\n                self.best_state_dict[k] = v.cpu()\n            self.best_epoch=epoch\n        else:\n            if (epoch-self.best_epoch)>CFG.early_stop_epoch:\n                self.stop=True\n        return","423139c5":"def get_group_parameters(model,lr,lr_diff_rate=1,weight_decay=0.01):\n    init_lr=1\n    last_layer=-1\n    opt_params=[]\n    for name,matrix in model.named_parameters():\n        info=name.split('.')\n        params_dict={'params':matrix}\n        ###############################\n#         if info[-1]=='bias':\n#             params_dict['weight_decay']=0\n#         else:\n        params_dict['weight_decay']=weight_decay\n#         ###############################\n        if len(info)>=4 and info[1]=='encoder' and info[2]=='layer':\n            layer=int(info[3])\n            if layer!=last_layer:\n                last_layer=layer\n                init_lr\/=lr_diff_rate\n        params_dict['lr']=init_lr\n        opt_params.append(params_dict)\n    scale=lr\/init_lr\n    for params_dict in opt_params:\n        params_dict['lr']*=scale\n    return opt_params","95a521ac":"def re_init(model,n=4,initializer=None):\n    # plz modify name here based on model\n    if n==0:\n        return\n    if initializer is None:\n        initializer=nn.init.kaiming_normal_\n    for layer in model.model.encoder.layer[-n:]:\n        for module in layer.modules(): \n            if isinstance(module, nn.Linear):\n                initializer(module.weight)\n                if module.bias is not None:\n                    nn.init.zeros_(module.bias)\n            elif isinstance(module, nn.LayerNorm):\n                nn.init.zeros_(module.bias)\n                nn.init.ones_(module.weight)\n    return","5314565e":"def train(model,train_ldr,val_ldr,fold):\n    epoch_res=[]\n    val_epoch=1\n    epoch_L=len(train_ldr)\n    val_freq=CFG.val_freq\n    val_gap=epoch_L\/\/val_freq\n    print(f'validation gap iteration is:{val_gap}')\n    \n    mse= torch.nn.MSELoss()\n    \n    recorder=StateRecorder()\n    #re_init(model,n=CFG.re_init_n,initializer=CFG.initializer)\n    #opt_params=get_group_parameters(model,CFG.lr,CFG.lr_diff_rate,CFG.weight_decay)\n    optimizer = AdamW(model.parameters(),lr=CFG.lr,weight_decay=CFG.weight_decay,correct_bias=True)\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(CFG.epochs*len(train_ldr))\/\/CFG.grad_avg_n,eta_min=0)\n    total_iter=0\n    for i in range(CFG.epochs):      \n        start_time = time.time()\n        loss_seq = []          \n        for itr,(texts, attns, target) in enumerate(train_ldr):\n            model.train()\n            outputs = model(texts, attention_mask=attns)\n            loss = mse(outputs.reshape(-1), target)\/CFG.grad_avg_n\n            loss_seq.append(loss.item()*CFG.grad_avg_n)\n            loss.backward()\n            total_iter+=1\n            if total_iter%CFG.grad_avg_n==0:\n                optimizer.step()\n                optimizer.zero_grad()\n                scheduler.step()\n                if total_iter%val_gap==0:\n                    val_loss,pred,target = val_rmse(model, val_ldr,mse,mode='train')\n                    val_loss2,_,_ = 0,None,None#val_rmse(model, val_ldr,mse,mode='eval')\n\n                    res=pd.DataFrame()\n                    res['target']=target\n                    res['pred']=pred\n                    res['fold']=fold\n                    res['epoch']=val_epoch\n                    epoch_res.append(res)\n                    print(f'################  epoch {i+1}   val epoch {val_epoch} #################################')\n                    print(f'training rmse:{np.sqrt(np.mean(loss_seq)).round(6)}')\n                    print(f'validation rmse (train):{val_loss.round(6)}')#  (eval):{val_loss2.round(6)}')\n                    recorder.record(val_loss,val_epoch,model)\n\n                    val_epoch+=1\n                #scheduler.step(recorder.cv_scores[-1])\n    torch.cuda.empty_cache()\n    fold_res=pd.concat(epoch_res,axis=0).reset_index(drop=True)\n    fold_res['is_best_epoch']=False\n    fold_res.loc[fold_res[fold_res.epoch==recorder.best_epoch].index,'is_best_epoch']=True\n    return fold_res,recorder","ba426ec3":"split_indices=CV_split(len(train_df),k=CFG.folds,shuffle=CFG.cv_shuffle,seed=7)\nmodels=[]\ncv_res=[]\nfor fold in range(1,CFG.folds+1):\n    print(f'fold {fold}')\n    train_index,val_index=split_indices[fold-1]\n    train_ds=D.Subset(ds1, train_index)\n    valid_ds = D.Subset(ds1, val_index)\n    ################\n    random.seed(CFG.seed)\n    os.environ['PYTHONHASHSEED'] = str(CFG.seed)\n    np.random.seed(CFG.seed)\n    torch.manual_seed(CFG.seed)\n    torch.cuda.manual_seed(CFG.seed)\n    torch.cuda.manual_seed_all(CFG.seed)\n    torch.backends.cudnn.deterministic = True\n    ##################\n    train_ldr = D.DataLoader(train_ds, batch_size=CFG.batch_size,\n                             shuffle=True, collate_fn = collate,num_workers=0)\n    val_ldr = D.DataLoader(valid_ds, batch_size=CFG.batch_size,\n                           shuffle=False, collate_fn = collate,num_workers=0)\n    \n    model.load_state_dict(init_state)\n    model.to(CFG.device)\n    fold_res,recorder=train(model,train_ldr,val_ldr,fold)\n    print(f'best epoch: {recorder.best_epoch} with best rmse:{recorder.best_score}')\n    cv_res.append(fold_res)\n    preds=score_test(model,test_ldr)\n    test_df[f'fold {fold} preds']=preds\n    torch.save({'model_state_dict':recorder.best_state_dict},f'fold_{fold}_model')\n    del recorder\n    gc.collect()\ncv_res=pd.concat(cv_res,axis=0).reset_index(drop=True)","b99d85ce":"cv_res['ae']=np.abs(cv_res.target-cv_res.pred)","8452ef95":"rmse_curve=[]\nfor epoch in range(1,cv_res.epoch.max()+1):\n    epoch_data=cv_res[cv_res.epoch==epoch].reset_index(drop=True)\n    score=rmse(epoch_data.pred,epoch_data.target)\n    rmse_curve.append(score)\n    print(f'abs error auto correlation: {epoch_data.ae.autocorr()}')\n    print(f'epoch {epoch} rmse: {score} oof pred error std:{np.std(epoch_data.ae)\/np.sqrt(len(epoch_data))}')\nplt.plot(rmse_curve)","9577ef02":"epoch_data=cv_res[cv_res.is_best_epoch==True].reset_index(drop=True)\nrmse(epoch_data.pred,epoch_data.target)","9f95924e":"final_preds=0\nfor i in range(CFG.folds):\n    final_preds+=test_df[f'fold {i+1} preds']\/CFG.folds\nres_df['target']=final_preds","87744b2e":"res_df","9fdaa833":"# LOAD state dict with IPTP","c9b5fbb0":"# parameters for this notebook","24bfcc6d":"# My Model with additional layers","ff9d1f8b":"# Load pretrained model","3ffbff37":"# save init model","a70f477f":"# deeper Look into oof_pred","e6e401af":"# Load Dataset","23290d9c":"# Helpers ","07a308c4":"* best oof score","b4e5445e":"* oof score for each epoch","a929b036":"# Cross Validation","9860d895":"# Version changes:\n* funnel large\n* seed 75111\n* 1 linear layer\n* reinit 5\n* further fineturn F36\n* bs 16\n* lr 3e-5"}}