{"cell_type":{"3b360b1a":"code","566a22bd":"code","542609e3":"code","bbbea5d6":"code","f7cd6100":"code","18f4f4eb":"code","841decab":"code","cae71719":"code","d5df4682":"code","1b371c2b":"code","94bfeadd":"code","a7508c2e":"code","d5122ff6":"code","5361af48":"code","cc96b84e":"code","0db86b3f":"code","541e7b8b":"code","9f0aefe2":"code","db9a75c6":"code","e6e801c5":"code","0598af04":"code","b8c74701":"code","448baa8a":"code","e57fd011":"code","2885dbe6":"code","30c6f719":"code","7d0bb4c1":"markdown","027dae60":"markdown","3d574ed7":"markdown","8277d355":"markdown","6ee65005":"markdown","c4ac9dc1":"markdown","68400c7b":"markdown","6de22814":"markdown","69e235fc":"markdown","f15ca53b":"markdown","d882f1eb":"markdown"},"source":{"3b360b1a":"%matplotlib inline\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#pd.set_option('display.max_columns', None)\n#pd.set_option('display.max_rows', None)\n\nfrom tqdm import tqdm\nfrom glob import glob\nimport gc\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import classification_report, roc_auc_score, f1_score\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n\nfrom sklearn.linear_model import LogisticRegression\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\n\nimport shap\n\nplt.rcParams[\"figure.figsize\"] = (12,8)\nplt.rcParams['axes.titlesize'] = 16\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom time import time, strftime, gmtime\nstart = time()\nimport datetime\nprint(str(datetime.datetime.now()))","566a22bd":"base_dir = '\/kaggle\/input\/insurance-churn-prediction-weekend-hackathon\/Insurance_Churn_ParticipantsData\/'","542609e3":"train = pd.read_csv(base_dir + 'Train.csv')\nprint(f'Number of rows in trainset: {train.shape[0]} \\nNumber of columns in trainset: {train.shape[1]}')\ntrain.head()","bbbea5d6":"test = pd.read_csv(base_dir + 'Test.csv')\nprint(f'Number of rows in testset: {test.shape[0]} \\nNumber of columns in testset: {test.shape[1]}')\ntest.head()","f7cd6100":"!pip install -q openpyxl","18f4f4eb":"sub = pd.read_excel(base_dir + 'sample_submission.xlsx')\nsub.head()","841decab":"train.info()","cae71719":"train.describe().T","d5df4682":"train.isna().sum(), test.isna().sum()","1b371c2b":"ax = sns.countplot(data = train, x = 'labels', palette = 'Set3')\n\nfor p in ax.patches:\n        ax.annotate('{:.1f}%'.format(100 * p.get_height() \/ len(train)), (p.get_x() + 0.1, p.get_height() + 5))","94bfeadd":"features = list(train.columns)\nfeatures.remove('labels')\nfeatures","a7508c2e":"numerical_features = [c for c in train.columns if train[c].dtype == 'float64']\nnumerical_features","d5122ff6":"fig, ax = plt.subplots(4, 2, figsize = (15, 10))\nax = ax.flatten()\nfor i, c in enumerate(numerical_features):\n    sns.boxplot(x = train[c], ax = ax[i], palette = 'Set3')\nplt.suptitle('Box Plot', fontsize = 25)\nfig.tight_layout()","5361af48":"fig, ax = plt.subplots(4, 2, figsize = (20, 15))\nax = ax.flatten()\nfor i, c in enumerate(numerical_features):\n    sns.histplot(x = train[c], ax = ax[i], kde = True)\nplt.suptitle('Histogram Plot', fontsize = 25)\nfig.tight_layout()","cc96b84e":"int64_cols = [c for c in train.columns if train[c].dtype == 'int64']\nprint(f'There are {len(int64_cols)} features with int64 dtype: \\n{int64_cols}')\n\n#Check their unique values\n\nprint('Unique number of values in int64 features:')\nfor c in int64_cols:\n    if c != 'labels':\n        print(f'{c.upper()}: {train[c].nunique()}, {test[c].nunique()}')","0db86b3f":"fig, ax = plt.subplots(5, 2, figsize = (10, 20))\nax = ax.flatten()\nfor i, c in enumerate(int64_cols):\n    a = sns.countplot(x = train[c], ax = ax[i], palette = 'Set3', hue = train['labels'])\n    for p in a.patches:\n        a.annotate('{:.1f}%'.format(100 * p.get_height() \/ len(train)), (p.get_x() + 0.1, p.get_height() + 5))\nplt.suptitle('Count Plot of Categorical Features', fontsize = 20)\nfig.tight_layout()","541e7b8b":"scl = StandardScaler()\ntrain[numerical_features] = scl.fit_transform(train[numerical_features])\ntest[numerical_features] = scl.transform(test[numerical_features])","9f0aefe2":"int64_cols.remove('labels')\nfor c in int64_cols: \n    lbl = LabelEncoder() \n    lbl.fit(list(train[c].astype(str).values) + list(test[c].astype(str).values)) #Takes care of cardinality mismatch\n    train[c] = lbl.transform(list(train[c].astype(str).values))\n    test[c] = lbl.transform(list(test[c].astype(str).values))","db9a75c6":"X = train.drop('labels', axis = 1)\ny = train['labels']\n\nsfs = SFS(LogisticRegression(class_weight = 'balanced'),\n           k_features = 10,\n           forward = True,\n           floating = False,\n           scoring = 'f1',\n           cv = 2)\n\nsfs.fit(X,y)\n\nprint(f'Top 10 features selected using Forward Propagation: \\n{sfs.k_feature_names_}')\nprint(f'Score: {sfs.k_score_}')\n\nselected_features = list(sfs.k_feature_names_)","e6e801c5":"fig = plot_sfs(sfs.get_metric_dict(), kind = 'std_dev')\nplt.title('Sequential Forward Selection')\nplt.grid()\nplt.show()","0598af04":"train_df, valid_df = train_test_split(train, test_size = 0.2, random_state = 2021, stratify = train['labels'])\n\nXtrain = train_df[selected_features]\nytrain = train_df['labels']\nXvalid = valid_df[selected_features]\nyvalid = valid_df['labels']\nprint(Xtrain.shape, ytrain.shape, Xvalid.shape, yvalid.shape)","b8c74701":"num_pos_samples = train['labels'].value_counts().values[1]\ntotal_samples = len(train['labels'])\nscale_pos_weight = 100 - ( (num_pos_samples \/ total_samples) * 100 )\nscale_pos_weight","448baa8a":"import lightgbm as lgbm\n\nparams = {'num_leaves': 7,  # 2^max_depth - 1\n          'min_child_samples': 100,\n          'objective': 'binary',\n          #'scale_pos_weight': scale_pos_weight, #99,\n          'is_unbalance': 'true',\n          'max_depth': 3,\n          'learning_rate': 0.01,\n          \"boosting_type\": \"gbdt\",\n          \"subsample_freq\": 3,\n          \"subsample\": 0.7,\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.3,\n          'reg_lambda': 0.3,\n          'colsample_bytree': 0.9,\n          'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n          'seed': 2021\n         }","e57fd011":"print(f'Training LightGBM..')\nltrain = lgbm.Dataset(Xtrain, label = ytrain)\nlvalid = lgbm.Dataset(Xvalid, label = yvalid)\n\nnum_rounds = 10000\nclf = lgbm.train(params, ltrain, num_rounds, valid_sets = [ltrain, lvalid], verbose_eval = 50, \n                    early_stopping_rounds = 100)\n\ntrain_preds = clf.predict(Xtrain, num_iteration = clf.best_iteration)\nprint(f'Training ROC_AUC_SCORE: {roc_auc_score((train_preds > 0.5), ytrain)}')\nprint(f'Training F1 SCORE: {f1_score((train_preds > 0.5), ytrain)}')\n\nvalid_preds = clf.predict(Xvalid, num_iteration = clf.best_iteration)\nprint(f'Validation ROC_AUC_SCORE: {roc_auc_score((valid_preds > 0.5), yvalid)}')\nprint(f'Validation F1 SCORE: {f1_score((valid_preds > 0.5), yvalid)}')\n\ntest_preds = clf.predict(test[selected_features], num_iteration = clf.best_iteration)\nprint(test_preds[:10])","2885dbe6":"sub['labels'] = (test_preds > 0.5).astype(int)\nax = sns.countplot(data = sub, x = 'labels', palette = 'Set3')\n\nfor p in ax.patches:\n        ax.annotate('{:.1f}%'.format(100 * p.get_height() \/ len(train)), (p.get_x() + 0.1, p.get_height() + 5))","30c6f719":"finish = time()\nprint(strftime(\"%H:%M:%S\", gmtime(finish - start)))","7d0bb4c1":"__Boxplots of numerical features__","027dae60":"__Value counts of int64 features__","3d574ed7":"- Feature0 to feature6 are numerical features with dtype float64\n- Feature7 to feature15 seems to be categorical with dtype int64","8277d355":"__Feature Selection using Forward Propagation__","6ee65005":"- There are no NaNs in the dataset","c4ac9dc1":"- Numerical features seems to have lots of outliers.","68400c7b":"- From the above plot f1 score tend to flatten after 8th feature","6de22814":"__Standardize Num Features and Label Encode Cat Features__","69e235fc":"- From the countplot its clear that dataset is unbalanced.","f15ca53b":"__Count plot of target__","d882f1eb":"__Check for NaNs in train and test__"}}