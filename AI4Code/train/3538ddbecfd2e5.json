{"cell_type":{"a8d51efd":"code","54e4190c":"code","df24dd3b":"code","1c7ffca1":"code","32bf479b":"code","122ebf60":"code","7c03c807":"code","b020813e":"code","849675e1":"code","fe0be602":"code","7cc0d45d":"code","b24d7053":"code","f2024b9c":"code","22122a89":"code","36675b76":"code","118aa1b6":"code","ecd65ea9":"code","2f24274a":"code","6d71be8a":"code","0a6d72fb":"code","4637dc78":"code","12edda97":"markdown"},"source":{"a8d51efd":"import time\nstart_time = time.time()\nfrom sklearn.model_selection import train_test_split\nimport sys, os, re, csv, codecs, numpy as np, pandas as pd\nnp.random.seed(32)\nos.environ[\"OMP_NUM_THREADS\"] = \"4\"\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten, CuDNNGRU,CuDNNLSTM\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\nfrom keras.engine import InputSpec, Layer\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom keras.layers import GRU, BatchNormalization, Conv1D, MaxPooling1D\n\nimport logging\nfrom sklearn.metrics import roc_auc_score\nfrom keras.callbacks import Callback\n\nimport warnings\nwarnings.filterwarnings('ignore')","54e4190c":"class RocAucEvaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=0)\n            score = roc_auc_score(self.y_val, y_pred)\n            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))","df24dd3b":"class AttentionWeightedAverage(Layer):\n    \"\"\"\n    Computes a weighted average of the different channels across timesteps.\n    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n    \"\"\"\n\n    def __init__(self, return_attention=False, **kwargs):\n        self.init = initializers.get('uniform')\n        self.supports_masking = True\n        self.return_attention = return_attention\n        super(AttentionWeightedAverage, self).__init__(** kwargs)\n\n    def build(self, input_shape):\n        self.input_spec = [InputSpec(ndim=3)]\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight(shape=(input_shape[2], 1),\n                                 name='{}_W'.format(self.name),\n                                 initializer=self.init)\n        self.trainable_weights = [self.W]\n        super(AttentionWeightedAverage, self).build(input_shape)\n\n    def call(self, x, mask=None):\n        # computes a probability distribution over the timesteps\n        # uses 'max trick' for numerical stability\n        # reshape is done to avoid issue with Tensorflow\n        # and 1-dimensional weights\n        logits = K.dot(x, self.W)\n        x_shape = K.shape(x)\n        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n\n        # masked timesteps have zero weight\n        if mask is not None:\n            mask = K.cast(mask, K.floatx())\n            ai = ai * mask\n        att_weights = ai \/ K.sum(ai, axis=1, keepdims=True)\n        weighted_input = x * K.expand_dims(att_weights)\n        result = K.sum(weighted_input, axis=1)\n        if self.return_attention:\n            return [result, att_weights]\n        return result\n\n    def get_output_shape_for(self, input_shape):\n        return self.compute_output_shape(input_shape)\n\n    def compute_output_shape(self, input_shape):\n        output_len = input_shape[2]\n        if self.return_attention:\n            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n        return (input_shape[0], output_len)\n\n    def compute_mask(self, input, input_mask=None):\n        if isinstance(input_mask, list):\n            return [None] * len(input_mask)\n        else:\n            return None","1c7ffca1":"os.listdir('..\/input\/')","32bf479b":"train = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv\")\ntest = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv\")\nembedding_path = \"..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec\"","122ebf60":"train.head(3)","7c03c807":"test.head(3)","b020813e":"## Since we have only 13GB RAM get rid of the other columns in train data set\ntrain = train[['id','target','comment_text']]\ntrain.head(2)","849675e1":"# Since the target column is in probability. we will convert this to first 0 and 1 based on 0.5 threshold\ntrain['target'] = np.where(train['target'] >= 0.5, 1, 0)","fe0be602":"embed_size = 300\nmax_features = 130000\nmax_len = 220\n","7cc0d45d":"y = train['target'].values","b24d7053":"X_train, X_valid, Y_train, Y_valid = train_test_split(train[['comment_text']], y, test_size = 0.1)","f2024b9c":"raw_text_train = X_train[\"comment_text\"].str.lower()\nraw_text_valid = X_valid[\"comment_text\"].str.lower()\nraw_text_test = test[\"comment_text\"].str.lower()","22122a89":"%%time\ntk = Tokenizer(num_words = max_features, lower = True)\ntk.fit_on_texts(raw_text_train)\nX_train[\"comment_seq\"] = tk.texts_to_sequences(raw_text_train)\nX_valid[\"comment_seq\"] = tk.texts_to_sequences(raw_text_valid)\ntest[\"comment_seq\"] = tk.texts_to_sequences(raw_text_test)","36675b76":"%%time\nX_train = pad_sequences(X_train.comment_seq, maxlen = max_len)\nX_valid = pad_sequences(X_valid.comment_seq, maxlen = max_len)\ntest = pad_sequences(test.comment_seq, maxlen = max_len)","118aa1b6":"def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n\nword_index = tk.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.zeros((nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","ecd65ea9":"file_path = \"best_model.hdf5\"\ncheck_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n                              save_best_only = True, mode = \"min\")\nra_val = RocAucEvaluation(validation_data=(X_valid, Y_valid), interval = 1)\nearly_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 5)","2f24274a":"def build_model(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0):\n    inp = Input(shape = (max_len,))\n    x = Embedding(max_features, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n    x = SpatialDropout1D(dr)(x)\n    x = Bidirectional(CuDNNGRU(units, return_sequences=True))(x)\n    x = Bidirectional(CuDNNGRU(units, return_sequences=True))(x)  \n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x) \n    att = AttentionWeightedAverage()(x)\n    conc = concatenate([att,avg_pool, max_pool])\n    output = Dropout(0.7)(conc)\n    output = Dense(units=144)(output)\n    output = Activation('relu')(output)\n    prediction = Dense(1, activation = \"sigmoid\")(conc)\n    model = Model(inputs = inp, outputs = prediction)\n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    history = model.fit(X_train, Y_train, batch_size = 128, epochs = 3, validation_data = (X_valid, Y_valid), \n                        verbose = 1, callbacks = [ra_val, check_point, early_stop])\n    model = load_model(file_path)\n    return model","6d71be8a":"%%time\nmodel = build_model(lr = 1e-3, lr_d = 0, units = 112, dr = 0.3)","0a6d72fb":"pred = model.predict(test, batch_size = 1024, verbose = 1)","4637dc78":"submission = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/sample_submission.csv\")\nsubmission['prediction'] = pred\nsubmission.to_csv(\"submission.csv\", index = False)\nsubmission.head(10)","12edda97":"Thats it"}}