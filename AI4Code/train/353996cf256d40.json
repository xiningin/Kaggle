{"cell_type":{"9bc1887a":"code","84650e94":"code","6443e763":"code","4d0f9e8e":"code","485ddafd":"code","89b09125":"code","530aa800":"code","c2ef60a1":"code","5cf78a82":"code","e8ccd8fc":"code","49fe55d1":"code","7919c4f5":"code","a9611ea8":"code","fdefa2a3":"code","a3f8dbea":"code","6f380780":"code","7a1a1460":"code","a5f6f523":"code","9fbad3ee":"code","cdcb8535":"markdown","3d72d214":"markdown","cd6c83ac":"markdown"},"source":{"9bc1887a":"# Import our dependencies\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler,OneHotEncoder\nimport pandas as pd\nimport tensorflow as tf\n\n#  Import and read the charity_data.csv.\nimport pandas as pd \napplication_df = pd.read_csv(\"\/kaggle\/input\/charity-data\/charity_data.csv\")\napplication_df.head()","84650e94":"# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.\napplication_df=application_df.drop(['EIN', 'NAME'], axis=1)\napplication_df","6443e763":"# Determine the number of unique values in each column.\napplication_df.nunique()","4d0f9e8e":"# Look at APPLICATION_TYPE value counts for binning\napplication_df['APPLICATION_TYPE'].value_counts()","485ddafd":"# Determine which values to replace if counts are less than ...?\nreplace_application = list(application_df['APPLICATION_TYPE'].value_counts().loc[application_df['APPLICATION_TYPE'].value_counts() < 525].index)\n\n# Replace in dataframe\nfor app in replace_application:\n    application_df.APPLICATION_TYPE = application_df.APPLICATION_TYPE.replace(app,\"Other\")\n    \n# Check to make sure binning was successful\napplication_df.APPLICATION_TYPE.value_counts()","89b09125":"# Look at CLASSIFICATION value counts for binning\napplication_df['CLASSIFICATION'].value_counts()","530aa800":"# Determine which values to replace if counts are less than ..?\nreplace_class = list(application_df['CLASSIFICATION'].value_counts().loc[application_df['CLASSIFICATION'].value_counts() < 1880].index)\n\n# Replace in dataframe\nfor cls in replace_class:\n    application_df.CLASSIFICATION = application_df.CLASSIFICATION.replace(cls,\"Other\")\n    \n# Check to make sure binning was successful\napplication_df.CLASSIFICATION.value_counts()","c2ef60a1":"# Determine the number of unique values in each column.\napplication_df.nunique()","5cf78a82":"# Generate our categorical variable lists\n\ncat_vars = ['APPLICATION_TYPE','AFFILIATION','CLASSIFICATION','USE_CASE','ORGANIZATION','INCOME_AMT','SPECIAL_CONSIDERATIONS']\n","e8ccd8fc":"# Create a OneHotEncoder instance\nenc = OneHotEncoder(sparse=False)\n\n# Fit and transform the OneHotEncoder using the categorical variable list\nenc.fit(application_df[cat_vars])\nencode_df = pd.DataFrame(enc.transform(application_df[cat_vars]),columns=enc.get_feature_names(cat_vars))\n\n# Add the encoded variable names to the dataframe\nencode_df","49fe55d1":"reg_vars = ['STATUS','ASK_AMT','IS_SUCCESSFUL']\n\napplication_df[reg_vars]","7919c4f5":"# Merge one-hot encoded features and drop the originals\nencode_df = application_df[reg_vars].join(encode_df)\nencode_df","a9611ea8":"# Split our preprocessed data into our features and target arrays\nX = encode_df[[x for x in encode_df.columns if x != 'IS_SUCCESSFUL']].to_numpy()\ny = encode_df['IS_SUCCESSFUL'].to_numpy()\n\n# Split the preprocessed data into a training and testing dataset\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.33, random_state = 40)","fdefa2a3":"# Create a StandardScaler instances\nscaler = StandardScaler()\n\n# Fit the StandardScaler\nX_scaler = scaler.fit(X_train)\n\n# Scale the data\nX_train_scaled = X_scaler.transform(X_train)\nX_test_scaled = X_scaler.transform(X_test)","a3f8dbea":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","6f380780":"# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n#  YOUR CODE GOES HERE\n\nnn_model = tf.keras.models.Sequential()\n\n\n# First hidden layer\nnn_model.add(tf.keras.layers.Dense(units=80, activation=\"relu\", input_dim=43))\n\n# Second hidden layer\nnn_model.add(tf.keras.layers.Dense(units=30, activation=\"sigmoid\"))\n\n# Output layer\nnn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n\n# Check the structure of the model\nnn_model.summary()","7a1a1460":"# Compile the model\nnn_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])","a5f6f523":"# Train the model\nfit_model = nn_model.fit(X_train_scaled, y_train, epochs=100)","9fbad3ee":"# Evaluate the model using the test data\nmodel_loss, model_accuracy = nn_model.evaluate(X_test_scaled,y_test,verbose=2)\nprint(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")","cdcb8535":"### Deliverable 1: Preprocessing the Data for a Neural Network","3d72d214":"### Deliverable 2: Compile, Train and Evaluate the Model","cd6c83ac":"# Neural Network Charity Analysis\n\nThe objective of this activity is to practice making a neural network. We are using a dataset to predict whether or not funding applicants will be successful if funded by Alphabet Soup. \n\nThe data we have is a dataframe containing information about organizations that have received funding from Alphabet Soup. The feature columns have metadata about the organizations, and the target column indicates whether or not the investment was successful and the money was used effectively.\n\n## Preprocessing\n\nFirst, we imported and preprocessed the data. This involved several steps:\n\n- We dropped informationless columns such as the company name and id number\n- We binned a few of the categorical variables that had too many options.  These variables were the application type and the classification, which initially had 17 and 71 different options, which we trimmed down significantly\n- We used one-hot encoding on the categorical variables and left the numeric variables as they were. \n- We scaled the feature data\n\n## Model\n\n- We used a layered neural network with two hidden layers and one output layer. The hidden layers had 80 and 30 neurons, and the output layer had one.\n- We used a relu activation function\n- We trained with 100 epochs\n\n## Results\n\nI had a hard time getting above 75 percent. I'm not sure why, but most of the changes I made didn't really move the accuracy. It remained largely constant at 73%. \n\nI tried a few options to improve the model.  Among them were:\n\n- convert the income from a categorical variable to a numeric variable\n- make a new feature, which was the ratio of the income to the ask amount\n- change to fewer neurons in the hidden layer\n- change the activation function to a sigmoid\n- scale the income amount and ask amount by the maximums, but this probably didn't do anything because we apply scaling later anyway\n- dropped the ask amount and income amount\n\n## Recommendation\n\nAs with any deep learning research, there is probably a ton more that I could do to improve this model. I feel like at the moment, I've just given the problem the brute force treatment and just crossed my fingers and hit run, but haven't really thought my way through potential improvements. I think I would in the long run try to determine which variables are not helping and I would remove them. I'd probably also look at the accuracy as a function of the epochs. \n\nI also think that we could get somewhere with tree classification. This data is highly categorical, so I think a tree or forrest might be our best bet. "}}