{"cell_type":{"e237bd43":"code","6b9b38aa":"code","fb0839cd":"code","8f7aba0c":"code","26de5ebc":"code","83fa1cd5":"code","e34b8fe3":"code","92c7c156":"code","a2eb8d8d":"markdown","372730d5":"markdown","e5a7aa53":"markdown","861dee79":"markdown"},"source":{"e237bd43":"import numpy as np\nimport pandas as pd\nimport spacy\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        if filename.endswith(\".json\"):\n            break\n        print(os.path.join(dirname, filename))\n","6b9b38aa":"#You can change these terms for your own paper search:\nterms = [\"support\", \"care facilities\", \"nurse facilities\",]","fb0839cd":"meta = pd.read_csv(\"\/kaggle\/input\/CORD-19-research-challenge\/2020-03-13\/all_sources_metadata_2020-03-13.csv\")\nmeta.head()","8f7aba0c":"#Looking for NaN's in meta[\"title\"]:\nprint(f'There are {meta[\"title\"].isna().sum()} NaN values in meta[title]')\n\n#Looking for NaN's in meta[\"abstract\"]:\nprint(f'There are {meta[\"abstract\"].isna().sum()} NaN values in meta[abstract]')\nprint( \"-\" * 50)\n\n#Check for rows where \"title\" and \"abstract\" is NaN:\nna = meta[\"title\"].isna() & meta[\"abstract\"].isna() \n\n#select only the rows where at least the title is there:\n   \nmeta_clean = meta[~na]\n\nprint(\"\\n\\nData after cleaning:\")\nprint( \"-\" * 50)\nprint(f'There are {meta_clean[\"title\"].isna().sum()} NaN values in meta_clean[title]')\nprint(f'There are {meta_clean[\"abstract\"].isna().sum()} NaN values in meta_clean[abstract]')","26de5ebc":"#Import NLP librarie spacy\nimport spacy\nfrom spacy.matcher import PhraseMatcher\nfrom collections import defaultdict\n\n#Generate model\nnlp = spacy.load('en')\nmatcher = PhraseMatcher(nlp.vocab, attr='LOWER')\n\nterm_search = defaultdict(list)","83fa1cd5":"#iterate over doc titels in meta data:\n\npatterns = [nlp(text) for text in terms]\nmatcher.add(\"SearchTerms\", None, *patterns)\n\nfor idx, meta in meta_clean.iterrows():\n    doc = nlp(meta.title)\n    matches = matcher(doc)\n\n    found_items = set([doc[match[1]:match[2]] for match in matches])\n\n    for item in found_items:\n        term_search[str(item).lower()].append(meta.title)","e34b8fe3":"for term in terms:\n    print(f'{len(term_search[term])} papers found for term -{term}-')","92c7c156":"term_search[\"care facilities\"]","a2eb8d8d":"There are no doc's found for the term \"nurse facilities\" but 12 for \"care facilities\":","372730d5":"**Task: What has been published about medical care?**\n\nThis is my first NLP kernel. I try to help fighting the corona virus even with my little skill. Hope I can help out by working on some basic tasks:\n\nSearch relevant papers based on meta data title \/ abstract:","e5a7aa53":"Task Details\n\nWhat has been published about medical care? What has been published concerning surge capacity and nursing homes? What has been published concerning efforts to inform allocation of scarce resources? What do we know about personal protective equipment? What has been published concerning alternative methods to advise on disease management? What has been published concerning processes of care? What do we know about the clinical characterization and management of the virus?\n\nSpecifically, we want to know what the literature reports about:\n\n*     **Resources to support skilled nursing facilities and long term care facilities.** -->First Task atm\n    \n*     **Mobilization of surge medical staff to address shortages in overwhelmed communities** --> Second Task\n\n\nCurrently working on:\n\n1. Looking for papers with relevant tokens in meta data title: [\"support\", \"care facilities\", \"nurse facilities\",]\n\n    --> I search the \"title\" and \"abstract\" column for specific terms to collect relevant publications. After that I could have a deeper look at the found papers and make some suggestions on how to support nursing \/ care facilities\n\n","861dee79":"I will continuously update the kernel. Please comment if you have any suggestions on how to make the search more efficient."}}