{"cell_type":{"e02c4586":"code","6cc21a7d":"code","ca173c55":"code","a02f7755":"code","c50ab039":"code","3ec463c8":"code","c94320e5":"code","58f452e1":"code","1efdeb39":"code","a98b5c41":"code","5a7efd4b":"code","ac456688":"code","73b5d63f":"code","ef67b05e":"code","70dea3e1":"code","b71eb2f8":"code","ba9c3ae9":"code","f310c704":"code","34f50cdd":"code","4c3b48f7":"code","40eb2867":"code","85d83650":"code","9c1aeb66":"code","278bdf11":"code","cb6344ae":"code","14c60199":"code","3fcf0698":"code","3e36bc38":"code","c6dc0be8":"code","3a353a07":"markdown","afdc5ab5":"markdown","acddd7d2":"markdown","f3f1c925":"markdown","7b463a0c":"markdown","0426e75f":"markdown","c27e0d85":"markdown","e8f1489e":"markdown","65d8b9fb":"markdown","c2789f43":"markdown","f34abc11":"markdown","f9d0c0f2":"markdown","70f93c69":"markdown"},"source":{"e02c4586":"# Use pandas to read data as a dataframe \nimport pandas as pd \nfeatures = pd.read_csv('..\/input\/temperates\/temps_extended.csv')\nfeatures.head(5)","6cc21a7d":"# Import matplotlib for plotting and use magic command for Jupyter Notebooks; Import datetime\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Set the style\nplt.style.use('fivethirtyeight')\nimport datetime\n\n# Get years, months, and days\nyears = features['year']\nmonths = features['month']\ndays = features['day']\n\n# List and then convert to datetime object\ndates = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) for year, month, day in zip(years, months, days)]\ndates = [datetime.datetime.strptime(date, '%Y-%m-%d') for date in dates]\n\n# Set up the plotting layout\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize = (15,10))\nfig.autofmt_xdate(rotation = 45)\n\n# Actual max temperature measurement\nax1.plot(dates, features['actual'])\nax1.set_xlabel(''); ax1.set_ylabel('Temperature (F)'); ax1.set_title('Max Temp')\n\n# Temperature from 1 day ago\nax2.plot(dates, features['temp_1'])\nax2.set_xlabel(''); ax2.set_ylabel('Temperature (F)'); ax2.set_title('Prior Max Temp')\n\n# Temperature from 2 days ago\nax3.plot(dates, features['temp_2'])\nax3.set_xlabel('Date'); ax3.set_ylabel('Temperature (F)'); ax3.set_title('Two Days Prior Max Temp')\n\n# Friend Estimate\nax4.plot(dates, features['friend'])\nax4.set_xlabel('Date'); ax4.set_ylabel('Temperature (F)'); ax4.set_title('Friend Estimate')\n\nplt.tight_layout(pad=2)","ca173c55":"# Set up the plotting layout\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize = (15,10))\nfig.autofmt_xdate(rotation = 45)\n\n# Historical Average Max Temp\nax1.plot(dates, features['average'])\nax1.set_xlabel(''); ax1.set_ylabel('Temperature (F)'); ax1.set_title('Historical Avg Max Temp')\n\n# Prior Avg Wind Speed \nax2.plot(dates, features['ws_1'], 'r-')\nax2.set_xlabel(''); ax2.set_ylabel('Wind Speed (mph)'); ax2.set_title('Prior Wind Speed')\n\n# Prior Precipitation\nax3.plot(dates, features['prcp_1'], 'r-')\nax3.set_xlabel('Date'); ax3.set_ylabel('Precipitation (in)'); ax3.set_title('Prior Precipitation')\n\n# Prior Snowdepth\nax4.plot(dates, features['snwd_1'], 'ro')\nax4.set_xlabel('Date'); ax4.set_ylabel('Snow Depth (in)'); ax4.set_title('Prior Snow Depth')\n\nplt.tight_layout(pad=2)","a02f7755":"# Create columns of seasons for pair plotting colors\nseasons = []\nfor month in features['month']:\n    if month in [1, 2, 12]:\n        seasons.append('winter')\n    elif month in [3, 4, 5]:\n        seasons.append('spring')\n    elif month in [6, 7, 8]:\n        seasons.append('summer')\n    elif month in [9, 10, 11]:\n        seasons.append('fall')\n# Will only use six variables for plotting pairs\nreduced_features = features[['temp_1', 'prcp_1', 'ws_1', 'average', 'friend', 'actual']]\nreduced_features['season'] = seasons\n# Use seaborn for pair plots\nimport seaborn as sns\nsns.set(style=\"ticks\", color_codes=True);\n# Create a custom color palete\npalette = sns.xkcd_palette(['dark blue', 'dark green', 'gold', 'orange'])\n# Make the pair plot with a some aesthetic changes\nsns.pairplot(reduced_features, hue = 'season', diag_kind = 'kde', palette= palette, plot_kws=dict(alpha = 0.7),\n                   diag_kws=dict(shade=True))","c50ab039":"# One Hot Encoding \nfeatures = pd.get_dummies(features)\n\n# Extract labels and features \nlabels = features['actual']\nfeatures = features.drop('actual', axis=1)\n\nfeature_list = ['temp_1', 'average', 'ws_1', 'temp_2', 'friend', 'year']\n# feature_list = important_feature_names[:]\nfeatures = features[feature_list]\n\n# Convert to numpy arrays \nimport numpy as np \nfeatures = np.array(features)\nlabels = np.array(labels)","3ec463c8":"# Using Skicit-learn to split data into training and testing sets\nfrom sklearn.model_selection import train_test_split\n\n# Split the data into training and testing sets\noriginal_features = pd.read_csv('..\/input\/tempscsv\/temps.csv')\noriginal_features = pd.get_dummies(original_features)\noriginal_labels = np.array(original_features['actual'])\noriginal_features= original_features.drop('actual', axis = 1)\noriginal_feature_list = list(original_features.columns)\noriginal_features = np.array(original_features)\noriginal_train_features, original_test_features, original_train_labels, original_test_labels = train_test_split(original_features, original_labels, test_size = 0.25, random_state = 42)\n\n# The baseline predictions are the historical averages\nbaseline_preds = original_test_features[:, original_feature_list.index('average')]\n\n# Baseline errors, and display average baseline error\nbaseline_errors = abs(baseline_preds - original_test_labels)\nprint('Average baseline error: ', round(np.mean(baseline_errors), 2), 'degrees.')\n\n# Import the model we are using\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Instantiate model \nrf = RandomForestRegressor(n_estimators= 1000, random_state=42)\n\n# Train the model on training data\nrf.fit(original_train_features, original_train_labels);\n\n# Use the forest's predict method on the test data\npredictions = rf.predict(original_test_features)\n\n# Calculate the absolute errors\nerrors = abs(predictions - original_test_labels)\n\n# Print out the mean absolute error (mae)\nprint('Average model error:', round(np.mean(errors), 2), 'degrees.')\n\n# Compare to baseline\nimprovement_baseline = 100 * abs(np.mean(errors) - np.mean(baseline_errors)) \/ np.mean(baseline_errors)\nprint('Improvement over baseline:', round(improvement_baseline, 2), '%.')\n\n# Calculate mean absolute percentage error (MAPE)\nmape = 100 * (errors \/ original_test_labels)\n\n# Calculate and display accuracy\naccuracy = 100 - np.mean(mape)\nprint('Accuracy:', round(accuracy, 2), '%.')","c94320e5":"train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\nprint('Training Features Shape:', train_features.shape)\nprint('Training Labels Shape:', train_labels.shape)\nprint('Testing Features Shape:', test_features.shape)\nprint('Testing Labels Shape:', test_labels.shape)","58f452e1":"# Find the original feature indices \noriginal_feature_indices = [feature_list.index(feature) for feature in\n                                      feature_list if feature not in\n                                      ['ws_1', 'prcp_1', 'snwd_1']]\n\n# Create a test set of the original features\noriginal_test_features = test_features[:, original_feature_indices]   # rf.predict(original_test_features) and to compare original_test_labels\n\n# Make predictions on test data using the model trained on original data\nbaseline_predictions = predictions\n\n# Performance metrics\nbaseline_errors = abs(baseline_predictions - original_test_labels)\n\nprint('Metrics for Random Forest Trained on Original Data')\nprint('Average absolute error:', round(np.mean(baseline_errors), 2), 'degrees.')\n\n# Calculate mean absolute percentage error (MAPE)\nbaseline_mape = 100 * np.mean((baseline_errors \/ original_test_labels))\n\n# Calculate and display accuracy\nbaseline_accuracy = 100 - baseline_mape\nprint('Accuracy:', round(baseline_accuracy, 2), '%.')","1efdeb39":"rf_exp = RandomForestRegressor(n_estimators= 1000, random_state=42)\nrf_exp.fit(train_features, train_labels);\n\n# Make predictions on test data\npredictions = rf_exp.predict(test_features)\n\n# Performance metrics\nerrors = abs(predictions - test_labels)\n\nprint('Metrics for Random Forest Trained on Expanded Data')\nprint('Average absolute error:', round(np.mean(errors), 4), 'degrees.')\n\n# Calculate mean absolute percentage error (MAPE)\nmape = np.mean(100 * (errors \/ test_labels))\n\n# Compare to baseline\nimprovement_baseline = 100 * abs(mape - baseline_mape) \/ baseline_mape\nprint('Improvement over baseline:', round(improvement_baseline, 2), '%.')\n\n# Calculate and display accuracy\naccuracy = 100 - mape\nprint('Accuracy:', round(accuracy, 2), '%.')","a98b5c41":"# Get numerical feature importances\nimportances = list(rf_exp.feature_importances_)\n\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];","5a7efd4b":"# Reset style \nplt.style.use('fivethirtyeight')\n\n# list of x locations for plotting\nx_values = list(range(len(importances)))\n\n# Make a bar chart\nplt.bar(x_values, importances, orientation = 'vertical', color = 'r', edgecolor = 'k', linewidth = 1.2)\n\n# Tick labels for x axis\nplt.xticks(x_values, feature_list, rotation='vertical')\n\n# Axis labels and title\nplt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');","ac456688":"# List of features sorted from most to least important\nsorted_importances = [importance[1] for importance in feature_importances]\nsorted_features = [importance[0] for importance in feature_importances]\n\n# Cumulative importances\ncumulative_importances = np.cumsum(sorted_importances)\n\n# Make a line graph\nplt.plot(x_values, cumulative_importances, 'g-')\n\n# Draw line at 95% of importance retained\nplt.hlines(y = 0.95, xmin=0, xmax=len(sorted_importances), color = 'r', linestyles = 'dashed')\n\n# Format x ticks and labels\nplt.xticks(x_values, sorted_features, rotation = 'vertical')\n\n# Axis labels and title\nplt.xlabel('Variable'); plt.ylabel('Cumulative Importance'); plt.title('Cumulative Importances');","73b5d63f":"# Find number of features for cumulative importance of 95%\n# Add 1 because Python is zero-indexed\nprint('Number of features for 95% importance:', np.where(cumulative_importances > 0.95)[0][0] + 1)\n\n# Extract the names of the most important features\nimportant_feature_names = [feature[0] for feature in feature_importances[0:6]]\n# Find the columns of the most important features\nimportant_indices = [feature_list.index(feature) for feature in important_feature_names]\n\n# Create training and testing sets with only the important features\nimportant_train_features = train_features[:, important_indices]\nimportant_test_features = test_features[:, important_indices]\n\n# Sanity check on operations\nprint('Important train features shape:', important_train_features.shape)\nprint('Important test features shape:', important_test_features.shape)","ef67b05e":"rf_exp.fit(important_train_features, train_labels);\n# Make predictions on test data\nexp_predictions = rf_exp.predict(important_test_features)\n\n# Performance metrics\nexp_errors = abs(exp_predictions - test_labels)\n\nprint('Average absolute error:', round(np.mean(errors), 4), 'degrees.')\n\n# Calculate mean absolute percentage error (MAPE)\nexp_mape = 100 * (exp_errors \/ test_labels)\n\n# Calculate and display accuracy\nexp_accuracy = 100 - np.mean(exp_mape)\nprint('Accuracy:', round(exp_accuracy, 2), '%.')","70dea3e1":"rf = RandomForestRegressor(random_state=42)\nfrom pprint import pprint \n# Initial parameters\nprint('Parameters currently used: \\n')\npprint(rf.get_params())","b71eb2f8":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random parameters grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\npprint(random_grid)","ba9c3ae9":"from sklearn.model_selection import RandomizedSearchCV\n# Random search of parameters, using 3 fold cross validation, search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,\n                              n_iter = 100, scoring='neg_mean_absolute_error', \n                              cv = 3, verbose=2, random_state=42, n_jobs=-1)\n# Fit the random search model\nrf_random.fit(train_features, train_labels)","f310c704":"rf_random.best_params_","34f50cdd":"def evaluate_models(model, test_features, test_labels): \n    predictions = model.predict(test_features)\n    erros = abs(predictions - test_labels)\n    mape = 100 * np.mean(errors \/ test_labels)\n    accuracy = 100 - mape\n    print('Model Performance')\n    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n    print('Accuracy = {:0.2f}%.'.format(accuracy))\n    \n    return accuracy\n# Split into training and testing sets \n# evaluate_models(rf, features, labels, param_grid=random_grid)","4c3b48f7":"base_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\nbase_model.fit(train_features, train_labels)\nbase_accuracy = evaluate_models(base_model, test_features, test_labels)","40eb2867":"best_random = rf_random.best_estimator_\nrandom_accuracy = evaluate_models(best_random, test_features, test_labels)\nprint('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) \/ base_accuracy))","85d83650":"from sklearn.model_selection import GridSearchCV\n\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110, 120, None],\n    'max_features': [2, 3, 4],\n    'min_samples_leaf': [3, 4, 5, 6, 7],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [70, 100, 125, 200, 300, 1000]\n}\n\n# Create a based model\nrf = RandomForestRegressor()\n\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                           scoring = 'neg_mean_absolute_error', cv = 3, \n                           n_jobs = -1, verbose = 2)\n\n# Fit the grid search to the data\ngrid_search.fit(train_features, train_labels)","9c1aeb66":"grid_search.best_params_","278bdf11":"best_grid = grid_search.best_estimator_\nevaluate_models(best_grid, test_features, test_labels)","cb6344ae":"\"\"\"\nprint('Model Parameters:\\n')\npprint(best_grid.get_params())\nprint('\\n')\nevaluate(best_grid, test_features, test_labels)\n\"\"\"\n\nfinal_model = grid_search\nprint('Final Model Parameters: \\n')\npprint(final_model.get_params())\ngrid_final_accuracy = evaluate_models(final_model, test_features, test_labels)","14c60199":"base_model.feature_importances_","3fcf0698":"# Evaluate run time and prediction accuracy\nimport time\ndef evaluate_model(model, x_train, y_train, x_test, y_test):\n    n_trees = model.get_params()['n_estimators']\n    n_features = x_train.shape[1]\n    \n    # Train and predict 10 times to evaluate time and accuracy\n    predictions = []\n    run_times = []\n    for _ in range(10):\n        start_time = time.time()\n        model.fit(x_train, y_train)\n        predictions.append(model.predict(x_test))\n    \n        end_time = time.time()\n        run_times.append(end_time - start_time)\n    \n    # Run time and predictions need to be averaged\n    run_time = np.mean(run_times)\n    predictions = np.mean(np.array(predictions), axis = 0)\n    \n    # Calculate performance metrics\n    errors = abs(predictions - y_test)\n    mean_error = np.mean(errors)\n    mape = 100 * np.mean(errors \/ y_test)\n    accuracy = 100 - mape\n    \n    # Return results in a dictionary\n    results = {'time': run_time, 'error': mean_error, 'accuracy': accuracy, 'n_trees': n_trees, 'n_features': n_features}\n    \n    return results","3e36bc38":"# Export the tree using sklearn \nfrom sklearn.tree import export_graphviz \n\n# Write the decision tree as a dot file \nvisual_tree = final_model.best_estimator_[12]\nexport_graphviz(visual_tree, out_file='best_tree.dot', feature_names = feature_list, \n               precision=2, filled=True, rounded=True, max_depth=None)\n\n# Converting to image using pydot \nimport pydot \n# Import the dot file to a graph and then convert to a png \n(graph, ) = pydot.graph_from_dot_file('best_tree.dot')\ngraph.write_png('best_tree.png')","c6dc0be8":"# Calculate mean absolute error for each model\noriginal_mae = np.mean(abs(baseline_predictions - original_test_labels))\nexp_all_mae = np.mean(abs(exp_predictions - test_labels))\nexp_reduced_mae = np.mean(abs(predictions - test_labels))\n\n# Calculate accuracy for model trained on 1 year of data\noriginal_accuracy = improvement_baseline # 100 * (1 - np.mean(abs(original_features_predictions - test_labels) \/ test_labels))\n\n# Create a dataframe for comparison\nmodel_comparison = pd.DataFrame({'model': ['original', 'exp_all', 'exp_reduced'], \n                                 'error (degrees)':  [original_mae, exp_all_mae, exp_reduced_mae],\n                                 'accuracy': [original_accuracy, exp_accuracy, base_accuracy],})\n# Order the dataframe\nmodel_comparison = model_comparison[['model', 'error (degrees)', 'accuracy']]\n\n# Make plots \n# Set up the plotting layout\nfig, (ax1, ax2, ax3) = plt.subplots(nrows=3, ncols=1, figsize = (8,16), sharex = True)\n\n# Set up x-axis\nx_values = [0, 1, 2]\nlabels = list(model_comparison['model'])\nplt.xticks(x_values, labels)\n\n# Set up fonts\nfontdict = {'fontsize': 18}\nfontdict_yaxis = {'fontsize': 14}\n\n# Error Comparison\nax1.bar(x_values, model_comparison['error (degrees)'], color = ['b', 'r', 'g'], edgecolor = 'k', linewidth = 1.5)\nax1.set_ylim(bottom = 3.5, top = 4.5)\nax1.set_ylabel('Error (degrees) (F)', fontdict = fontdict_yaxis); \nax1.set_title('Model Error Comparison', fontdict= fontdict)\n\n# Accuracy Comparison\nax2.bar(x_values, model_comparison['accuracy'], color = ['b', 'r', 'g'], edgecolor = 'k', linewidth = 1.5)\nax2.set_ylim(bottom = 92, top = 94)\nax2.set_ylabel('Accuracy (%)', fontdict = fontdict_yaxis); \nax2.set_title('Model Accuracy Comparison', fontdict= fontdict)\n\n# Run Time Comparison\n# ax3.bar(x_values, model_comparison['run_time (s)'], color = ['b', 'r', 'g'], edgecolor = 'k', linewidth = 1.5)\n# ax3.set_ylim(bottom = 2, top = 12)\n# ax3.set_ylabel('Run Time (sec)', fontdict = fontdict_yaxis); \n# ax3.set_title('Model Run-Time Comparison', fontdict= fontdict);","3a353a07":"## Visualize Tree in the Forest ","afdc5ab5":"### Random Search with Cross Validation","acddd7d2":"### Check Feature Importance","f3f1c925":"## Data Preparation ","7b463a0c":"### Establish on Expanded Data and Features\u00b6","0426e75f":"### Evaluation Function ","c27e0d85":"#### Evaluate the best random search model","e8f1489e":"## Examine Grid Search with ML Models","65d8b9fb":"### Feature Reduction\nFrom previous experience and the graphs produced at the beginning, we know that some features are not useful for our temperature prediction problem. To reduce the number of features, which will reduce runtime, hopefully without significantly reducing performance, we can examine the feature importances from the random forest.","c2789f43":"## Grid Search\nWe can now perform grid search building on the result from the random search. We will test a range of hyperparameters around the best values returend by random search. ","f34abc11":"### Establish on New Baseline","f9d0c0f2":"## Load Data","70f93c69":"#### Evaluate the default model "}}