{"cell_type":{"fca42e38":"code","a5493c33":"code","e0de3d93":"code","ada22248":"code","b86ebc3b":"code","d3c4d8d3":"code","762ae783":"code","6c5c9f11":"code","d92b1922":"code","2e91b840":"code","52a52a9f":"code","0e52563e":"code","c735e2c1":"code","4afae601":"code","c570f8b6":"code","620eebc4":"code","50f40f6f":"code","1285e076":"code","87534a7e":"code","2b51f6f1":"code","1295a669":"code","2707989c":"code","141eec88":"code","2383579c":"code","88e9f2f2":"code","3e629b61":"code","3969422b":"code","ff86a85e":"code","eda4952f":"code","e498825c":"code","cd3a8f2e":"code","81906bad":"code","211f28bd":"code","6c59c535":"code","fb6e6325":"code","9963caf2":"code","1b326b39":"code","3127824d":"code","666f5582":"code","ce0a417c":"code","9bbc95de":"code","6bd9c7dd":"code","8e06f4d4":"code","3db6cae4":"code","fd995471":"code","8cc1c4ab":"code","f83acad6":"code","0e3c89e2":"code","c3a1feda":"code","031c2e63":"code","13a7c7ce":"code","9bee635a":"markdown","ee52c04d":"markdown","d127878c":"markdown","86c4a204":"markdown","ba55f8cf":"markdown","c4167981":"markdown","25dc5cbc":"markdown","173cc1aa":"markdown","6b1e807f":"markdown","9876a1d6":"markdown","3bfb4a36":"markdown","09936d79":"markdown","914bfa40":"markdown","f9f2d2bb":"markdown","72205eb6":"markdown","5d9ab876":"markdown","98ce4574":"markdown","e4a7594e":"markdown","66dc5dbf":"markdown","c7729226":"markdown","86eed240":"markdown","801e3b84":"markdown","161a6bd0":"markdown","9e05e6cf":"markdown","d420d686":"markdown","7af5baba":"markdown","13ece100":"markdown","4ed47a6c":"markdown","6bbbf9c8":"markdown","ddc67826":"markdown","5abf70c5":"markdown","907f2d0d":"markdown","1be7b905":"markdown","ea0e71f9":"markdown","41d8d8b7":"markdown"},"source":{"fca42e38":"import pandas as pd\nimport numpy as np\n\nfrom itertools import product\nfrom sklearn.preprocessing import LabelEncoder\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nimport time\nimport sys\nimport gc\nimport pickle\nsys.version_info","a5493c33":"shops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')\nshops.describe()","e0de3d93":"categories = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\ncategories.describe()","ada22248":"items = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\nitems.describe()","b86ebc3b":"training_set = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\ntraining_set.describe()","d3c4d8d3":"test = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')\ntest.describe()","762ae783":"fig = plt.figure(figsize=(18,9))\nplt.subplots_adjust(hspace=.5)\n\nplt.subplot2grid((3,3), (0,0), colspan = 3)\ntraining_set['shop_id'].value_counts(normalize=True).plot(kind='bar', alpha=0.7)\nplt.title('Shop ID Values in the Training Set (Normalized)')\n\nplt.subplot2grid((3,3), (1,0))\ntraining_set['item_id'].plot(kind='hist', alpha=0.7)\nplt.title('Item ID Histogram')\n\nplt.subplot2grid((3,3), (1,1))\ntraining_set['item_price'].plot(kind='hist', alpha=0.7, color='orange')\nplt.title('Item Price Histogram')\n\nplt.subplot2grid((3,3), (1,2))\ntraining_set['item_cnt_day'].plot(kind='hist', alpha=0.7, color='green')\nplt.title('Item Count Day Histogram')\n\nplt.subplot2grid((3,3), (2,0), colspan = 3)\ntraining_set['date_block_num'].value_counts(normalize=True).plot(kind='bar', alpha=0.7)\nplt.title('Month (date_block_num) Values in the Training Set (Normalized)')\n\nplt.show()","6c5c9f11":"plt.figure(figsize=(10,4))\nplt.xlim(-100, 3000)\nsns.boxplot(x=training_set['item_cnt_day'])\nprint('Sale volume outliers:',training_set['item_id'][training_set['item_cnt_day']>=1000].unique())\n\nplt.figure(figsize=(10,4))\nplt.xlim(training_set['item_price'].min(), training_set['item_price'].max())\nsns.boxplot(x=training_set['item_price'])\nprint('Item price outliers:',training_set['item_id'][training_set['item_price']>=100000].unique())","d92b1922":"training_set = training_set[training_set['item_price']<100000]\ntraining_set = training_set[training_set['item_cnt_day']<1001]\n\nmedian = training_set[(training_set['shop_id']==32)&(training_set['item_id']==2973)&(training_set['date_block_num']==4)&(training_set['item_price']>0)]['item_price'].median()\ntraining_set.loc[training_set['item_price']<0, 'item_price'] = median","2e91b840":"training_set['revenue'] = training_set['item_price'] *  training_set['item_cnt_day']","52a52a9f":"shops.head()","0e52563e":"training_set.loc[training_set.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n\ntraining_set.loc[training_set.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n\ntraining_set.loc[training_set.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11","c735e2c1":"shops.loc[shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"', 'shop_name'] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\nshops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops.loc[shops.city == '!\u042f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u042f\u043a\u0443\u0442\u0441\u043a'\nshops['city_code'] = LabelEncoder().fit_transform(shops['city'])\nshops = shops[['shop_id','city_code']]\n\nshops.head()","4afae601":"categories['split'] = categories['item_category_name'].str.split('-')\ncategories['type'] = categories['split'].map(lambda x: x[0].strip())\ncategories['type_code'] = LabelEncoder().fit_transform(categories['type'])\n\n# if subtype is nan then type\ncategories['subtype'] = categories['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ncategories['subtype_code'] = LabelEncoder().fit_transform(categories['subtype'])\ncategories = categories[['item_category_id','type_code', 'subtype_code']]\n\ncategories.head()","c570f8b6":"items.drop(['item_name'], axis=1, inplace=True)\nitems.head()","620eebc4":"test['date_block_num'] = 34\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\ntest['shop_id'] = test['shop_id'].astype(np.int8)\ntest['item_id'] = test['item_id'].astype(np.int16)","50f40f6f":"## Creation of Data structure for feauture engineering\nmatrix = []","1285e076":"## Generating the pairs\ncols = ['date_block_num','shop_id','item_id']\nfor i in range(34):\n    sales = training_set[training_set['date_block_num']==i]\n    matrix.append(np.array(list(product([i], sales['shop_id'].unique(), sales['item_id'].unique())), dtype='int16'))\n    \nmatrix = pd.DataFrame(np.vstack(matrix), columns=cols)\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix.sort_values(cols,inplace=True)","87534a7e":"temp_group = training_set.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\ntemp_group.columns = ['item_cnt_month']\ntemp_group.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, temp_group, on=cols, how='left')\nmatrix['item_cnt_month'] = (matrix['item_cnt_month'].fillna(0).clip(0,20).astype(np.float16))","2b51f6f1":"matrix = pd.concat([matrix, test], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna(0, inplace=True)","1295a669":"matrix = pd.merge(matrix, shops, on=['shop_id'], how='left')\nmatrix = pd.merge(matrix, items, on=['item_id'], how='left')\nmatrix = pd.merge(matrix, categories, on=['item_category_id'], how='left')","2707989c":"## Changing data types of the data structure to ease future processing\nmatrix['city_code'] = matrix['city_code'].astype(np.int8)\nmatrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\nmatrix['type_code'] = matrix['type_code'].astype(np.int8)\nmatrix['subtype_code'] = matrix['subtype_code'].astype(np.int8)","141eec88":"def lags(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","2383579c":"matrix = lags(matrix, [1,2,3,6,12], 'item_cnt_month')","88e9f2f2":"# Mean number of sales per month\ntemp_group = matrix.groupby(['date_block_num']).agg({'item_cnt_month': ['mean']})\ntemp_group.columns = [ 'date_avg_item_cnt' ]\ntemp_group.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, temp_group, on=['date_block_num'], how='left')\nmatrix['date_avg_item_cnt'] = matrix['date_avg_item_cnt'].astype(np.float16)\n\n## Additional lags for the monthly average in count\nmatrix = lags(matrix, [1], 'date_avg_item_cnt')\nmatrix.drop(['date_avg_item_cnt'], axis=1, inplace=True)\nmatrix.head()","3e629b61":"# Mean quantities grouped by month and item\ntemp_group = matrix.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\ntemp_group.columns = [ 'date_item_avg_item_cnt' ]\ntemp_group.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, temp_group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_cnt'] = matrix['date_item_avg_item_cnt'].astype(np.float16)\n\n## Additional Lags for the means that were created\nmatrix = lags(matrix, [1,2,3,6,12], 'date_item_avg_item_cnt')\nmatrix.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)\nmatrix.head()","3969422b":"## Mean quantities by month and shop\ntemp_group = matrix.groupby(['date_block_num', 'shop_id']).agg({'item_cnt_month': ['mean']})\ntemp_group.columns = [ 'date_shop_avg_item_cnt' ]\ntemp_group.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, temp_group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_avg_item_cnt'] = matrix['date_shop_avg_item_cnt'].astype(np.float16)\n\n#As before, additional lags for the means are created\nmatrix = lags(matrix, [1,2,3,6,12], 'date_shop_avg_item_cnt')\nmatrix.drop(['date_shop_avg_item_cnt'], axis=1, inplace=True)\nmatrix.head()","ff86a85e":"## Means by month and the category Id\ntemp_group = matrix.groupby(['date_block_num', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ntemp_group.columns = [ 'date_cat_avg_item_cnt' ]\ntemp_group.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, temp_group, on=['date_block_num','item_category_id'], how='left')\nmatrix['date_cat_avg_item_cnt'] = matrix['date_cat_avg_item_cnt'].astype(np.float16)\n\n## Same, additional Lags are created for the month-category means\nmatrix = lags(matrix, [1], 'date_cat_avg_item_cnt')\nmatrix.drop(['date_cat_avg_item_cnt'], axis=1, inplace=True)\nmatrix.head()","eda4952f":"# Means by month, item and shop\ntemp_group = matrix.groupby(['date_block_num', 'shop_id', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ntemp_group.columns = ['date_shop_cat_avg_item_cnt']\ntemp_group.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, temp_group, on=['date_block_num', 'shop_id', 'item_category_id'], how='left')\nmatrix['date_shop_cat_avg_item_cnt'] = matrix['date_shop_cat_avg_item_cnt'].astype(np.float16)\n\n#Creation of lags for the means of month, item and shop\nmatrix = lags(matrix, [1], 'date_shop_cat_avg_item_cnt')\nmatrix.drop(['date_shop_cat_avg_item_cnt'], axis=1, inplace=True)\nmatrix.head()","e498825c":"# Means using the month, shop, and shop type feature created before\ntemp_group = matrix.groupby(['date_block_num', 'shop_id', 'type_code']).agg({'item_cnt_month': ['mean']})\ntemp_group.columns = ['date_shop_type_avg_item_cnt']\ntemp_group.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, temp_group, on=['date_block_num', 'shop_id', 'type_code'], how='left')\nmatrix['date_shop_type_avg_item_cnt'] = matrix['date_shop_type_avg_item_cnt'].astype(np.float16)\n\n#Creating lags for... month, shop and the type\nmatrix = lags(matrix, [1], 'date_shop_type_avg_item_cnt')\nmatrix.drop(['date_shop_type_avg_item_cnt'], axis=1, inplace=True)\nmatrix.head()","cd3a8f2e":"# Likewise, creating of means with month, shop and the subtypes\ntemp_group = matrix.groupby(['date_block_num', 'shop_id', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ntemp_group.columns = ['date_shop_subtype_avg_item_cnt']\ntemp_group.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, temp_group, on=['date_block_num', 'shop_id', 'subtype_code'], how='left')\nmatrix['date_shop_subtype_avg_item_cnt'] = matrix['date_shop_subtype_avg_item_cnt'].astype(np.float16)\n\n#Creating lags for... month, shop and the subtypes\nmatrix = lags(matrix, [1], 'date_shop_subtype_avg_item_cnt')\nmatrix.drop(['date_shop_subtype_avg_item_cnt'], axis=1, inplace=True)\nmatrix.head()","81906bad":"temp_group = matrix.groupby(['date_block_num', 'city_code']).agg({'item_cnt_month': ['mean']})\ntemp_group.columns = [ 'date_city_avg_item_cnt' ]\ntemp_group.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, temp_group, on=['date_block_num', 'city_code'], how='left')\nmatrix['date_city_avg_item_cnt'] = matrix['date_city_avg_item_cnt'].astype(np.float16)\nmatrix = lags(matrix, [1], 'date_city_avg_item_cnt')\nmatrix.drop(['date_city_avg_item_cnt'], axis=1, inplace=True)\nmatrix.head()","211f28bd":"# Means using month, item and the city encoding that was done previously\ntemp_group = matrix.groupby(['date_block_num', 'item_id', 'city_code']).agg({'item_cnt_month': ['mean']})\ntemp_group.columns = [ 'date_item_city_avg_item_cnt' ]\ntemp_group.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, temp_group, on=['date_block_num', 'item_id', 'city_code'], how='left')\nmatrix['date_item_city_avg_item_cnt'] = matrix['date_item_city_avg_item_cnt'].astype(np.float16)\n\n# Lags with the month, item and encoded cities\nmatrix = lags(matrix, [1], 'date_item_city_avg_item_cnt')\nmatrix.drop(['date_item_city_avg_item_cnt'], axis=1, inplace=True)\nmatrix.head()","6c59c535":"# Means with the month, and the type of category encodings done previously\ntemp_group = matrix.groupby(['date_block_num', 'type_code']).agg({'item_cnt_month': ['mean']})\ntemp_group.columns = [ 'date_type_avg_item_cnt' ]\ntemp_group.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, temp_group, on=['date_block_num', 'type_code'], how='left')\nmatrix['date_type_avg_item_cnt'] = matrix['date_type_avg_item_cnt'].astype(np.float16)\n\n# Lags for the month and the encoded category types\nmatrix = lags(matrix, [1], 'date_type_avg_item_cnt')\nmatrix.drop(['date_type_avg_item_cnt'], axis=1, inplace=True)\nmatrix.head()","fb6e6325":"#Just as before (what a surprise!), the means with month and the encoded subtypes of the categories\ntemp_group = matrix.groupby(['date_block_num', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ntemp_group.columns = [ 'date_subtype_avg_item_cnt' ]\ntemp_group.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, temp_group, on=['date_block_num', 'subtype_code'], how='left')\nmatrix['date_subtype_avg_item_cnt'] = matrix['date_subtype_avg_item_cnt'].astype(np.float16)\n\n# Lags for the months with encoded category subtypes\nmatrix = lags(matrix, [1], 'date_subtype_avg_item_cnt')\nmatrix.drop(['date_subtype_avg_item_cnt'], axis=1, inplace=True)\nmatrix.head()","9963caf2":"## Feature to relate the months along the dataset's years\nmatrix['month'] = matrix['date_block_num'] % 12","1b326b39":"## Feature to indicate the number of days per month\ndays = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\nmatrix['days'] = matrix['month'].map(days).astype(np.int8)","3127824d":"matrix = matrix[matrix.date_block_num > 11]","666f5582":"## Filling NAs from the lags\ndef process_nas(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df\n\nmatrix = process_nas(matrix)","ce0a417c":"matrix.to_pickle('data.pkl')\ndel matrix\ndel group\ndel items\ndel shops\ndel cats\ndel train\n\ngc.collect();","9bbc95de":"data = pd.read_pickle('data.pkl')\ndata = data[[\n    'date_block_num',\n    'shop_id',\n    'item_id',\n    'item_cnt_month',\n    'city_code',\n    'item_category_id',\n    'type_code',\n    'subtype_code',\n    'item_cnt_month_lag_1',\n    'item_cnt_month_lag_2',\n    'item_cnt_month_lag_3',\n    'item_cnt_month_lag_6',\n    'item_cnt_month_lag_12',\n    'date_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_2',\n    'date_item_avg_item_cnt_lag_3',\n    'date_item_avg_item_cnt_lag_6',\n    'date_item_avg_item_cnt_lag_12',\n    'date_shop_avg_item_cnt_lag_1',\n    'date_shop_avg_item_cnt_lag_2',\n    'date_shop_avg_item_cnt_lag_3',\n    'date_shop_avg_item_cnt_lag_6',\n    'date_shop_avg_item_cnt_lag_12',\n    'date_cat_avg_item_cnt_lag_1',\n    'date_shop_cat_avg_item_cnt_lag_1',\n    'date_city_avg_item_cnt_lag_1',\n    'date_item_city_avg_item_cnt_lag_1',\n    'month',\n    'days'\n]]","6bd9c7dd":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)\n\n## Remove unnecessary variables to allocate space\ndel data\ngc.collect();","8e06f4d4":"## First Model\n\nmodel = XGBRegressor(\n    max_depth=8,\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,    \n    seed=42)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds = 10)","3db6cae4":"## Second Moder\nrf_model = RandomForestRegressor(n_estimators=50, \n                                 max_depth=7, \n                                 random_state=0, \n                                 n_jobs=-1, \n                                 verbose=1)\n\nrf_model.fit(X_train, Y_train)","fd995471":"## Predctions from XGB\nxgb_val_pred = model.predict(X_valid)\nxgb_test_pred = model.predict(X_test)\n\n## Predictions from RF\nrf_val_pred = rf_model.predict(X_valid)\nrf_test_pred = rf_model.predict(X_test)","8cc1c4ab":"# Feauture Importance\nfig, ax = plt.subplots(1,1,figsize=(10, 14))\nplot_importance(booster=model, ax=ax)","f83acad6":"# Dataset that will be the train set of the ensemble model.\nfirst_level = pd.DataFrame(xgb_val_pred, columns=['xgbm'])\nfirst_level['random_forest'] = rf_val_pred\nfirst_level['label'] = Y_valid.values\nfirst_level.head(20)","0e3c89e2":"# Dataset that will be the test set of the ensemble model.\nfirst_level_test = pd.DataFrame(xgb_test_pred, columns=['xgbm'])\nfirst_level_test['random_forest'] = rf_test_pred","c3a1feda":"meta_model = LinearRegression(n_jobs=-1)\nfirst_level.drop('label', axis=1, inplace=True)\nmeta_model.fit(first_level, Y_valid)","031c2e63":"ensemble_pred = meta_model.predict(first_level)\nfinal_predictions = meta_model.predict(first_level_test)\nprint('Train rmse:', np.sqrt(mean_squared_error(ensemble_pred, Y_valid)))","13a7c7ce":"submission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": final_predictions.clip(0., 20.)\n})\nsubmission.to_csv('submission.csv', index=False)\n\n# save predictions for an ensemble\npickle.dump(xgb_val_pred, open('xgb_val.pickle', 'wb'))\npickle.dump(xgb_test_pred, open('xgb_test.pickle', 'wb'))\n\npickle.dump(rf_val_pred, open('rf_val.pickle', 'wb'))\npickle.dump(rf_test_pred, open('rf_test.pickle', 'wb'))\n\npickle.dump(final_predictions, open('ensemble_pred.pickle', 'wb'))","9bee635a":"I just modify the types of the test data for future convenience. Additionally, I add the *date block* number from the month that the test data will be predcting","ee52c04d":"Ready to play! But firts, remove unnecesary information to avoid running out of space (happened a couple of times, unfortunately)","d127878c":"Afterwards, using the EDA done to the other datasets, I proceed to join them with the feature-engineered datastructure","86c4a204":"# Ensembling Models","ba55f8cf":"Likewise, I use the previous modification in the test dataset to add the month to predict to the matrix.","c4167981":"With the matrix generated, I proceed to generate new aggregated features","25dc5cbc":"## **Ensembling Architecture**\n\n1st level:\n* XGBM\n* Random forest\n\n2nd level:\n* Linear Regression","173cc1aa":"Because of the using 12 as lag value, it is necessary to drop those months. likewise, I remove the columns with this month's calculated values","6b1e807f":"### Additional Features","9876a1d6":"# Submission","3bfb4a36":"## Training Dataset","09936d79":"## Feauture Engineering\nSince we have to alculate monthly sales, the creation and modification of feature consists in using and extending the information with have from each unique pair (item, shop) within the month (item, shop, month). This way train data will be similar to test data.","914bfa40":"## Items Dataset\nFor now, I didn't find any good or relevant use to the items names, so I proceeded to remove them from the data set","f9f2d2bb":"## Revenue\nI believe that an interesting feature to have would be the revenue (total amount of money) from each transaction\/sale","72205eb6":"From the graphs above,\n\n* From the 60 different shop IDs, there is an uneven distribtution of these in the dataset. Four of these shops make around 25 percent of this dataset. These are shops.\n* The Item IDs seem to have variations in frequency, but it is no possible to make any further assumptions yet.\n* From the vast empty spaces in the histograms of 'item_price' and 'item_cnt_day', it is possible to argue that there are outliers in their distribution.\n* Plotting the individual months from January 2013 to October 2015, it is possible to see that the December months are the ones with a hgher amount of sales","5d9ab876":"From that, I proceed to remove outliers from the training data set. Additionally, there is one price below zero (as seen when describing the data), so I change the value with the median","98ce4574":"### Mean Encodings","e4a7594e":"For the training, the metaparameters where adjusted using the insights gathered during the course and documentation","66dc5dbf":"### Final Preprocessing before training","c7729226":"## Data Splitting\nValidation strategy is \n* 34 month for the test set\n* 33 month for the validation set \n* 13-33 months for the train.","86eed240":"Additionally, by looking at the actual names and reading the community forums and notebooks, it was possible to determine that some shops have duplicated id\/name.\n* 11 and 10\n* 1 and 58\n* 0 and 57","801e3b84":"## Categories Dataset","161a6bd0":"# Loading Data","9e05e6cf":"# Data Explorationg & EDA","d420d686":"# Training the model","7af5baba":"### Outliers by price and sales volume","13ece100":"## Test Dataset","4ed47a6c":"To combine the 1st level model predictions, I'll use a simple linear regression (As I'm only feeding the model with predictions, it is not necessary a complex model)","6bbbf9c8":"From the previous intuition regarding the outliers, using some boxplots it is possible to see that there are quite a few! Because of that, an empirical estimation is mabe (by looking at the boxplot) to identify the outliers","ddc67826":"## Models Results","5abf70c5":"### Shop Names\nAfter using Google (and Google Maps) for a couple of minutes\/hours, it was possible to understand that the structure of the sho_name is \"City\" - \"Type\" - \"Name\". With that, I use **Label Encoders** for encoding the city of each store","907f2d0d":"I create some additional aggregates for their posterior encoding","1be7b905":"This is the model that will combine the other ones to hopefully make an overall better prediction.","ea0e71f9":"As done with the shops, I encode information about the categories, such as the name, type and subtype","41d8d8b7":"## Shops Dataset"}}