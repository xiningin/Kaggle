{"cell_type":{"12c2cf08":"code","9e84df66":"code","f95447b3":"code","84b69122":"code","0ce4f917":"code","238deecc":"code","2034cd3e":"code","0b87858a":"code","62683738":"code","96c25d68":"code","eb1f0282":"code","9184910f":"code","d0bbba9b":"code","d1cefc7b":"code","81f750aa":"code","10a45834":"code","b21c6f6b":"code","1af94064":"code","8312f0b7":"code","d43ee5e2":"code","05ad1348":"code","d2c415b5":"code","a464c3d1":"code","4ba0b81d":"code","17967038":"code","591c2545":"code","1bb2a1a4":"code","ea0c2c74":"code","f2ff1c14":"code","6d11e5f3":"code","f1ae6354":"code","8e5fb55e":"code","1265bf5d":"code","1f9b0ca9":"code","df8454ed":"code","d15431e6":"code","0ef0dab0":"code","0c644df9":"code","85d7cfbf":"code","738c4be6":"code","604274b5":"code","98544911":"code","2088e243":"code","306340bf":"code","fa61bd71":"code","8efb0bef":"code","66b074a9":"code","9d0d20d3":"code","fee3de69":"code","a621650b":"code","2573c0ba":"code","0b6ff588":"code","c5f1a2c6":"code","7436fed4":"code","21889559":"code","da290f05":"code","3c0a2f2d":"code","e3cb5093":"code","ca032204":"code","97a1debf":"code","dafd4cda":"code","44fee457":"code","5135890d":"code","30d8e73c":"code","041f15dc":"code","9be2c3dd":"code","8806233d":"code","bd97e3e2":"code","f3b44af4":"code","41eddfdf":"code","69951f37":"code","a15e27c8":"code","ba0994af":"code","035303f9":"code","9b3e78e6":"code","77cdcbb5":"code","b00f08bb":"code","21aa6f52":"markdown","fc9db322":"markdown","e203cf52":"markdown","3bf36085":"markdown","d6e9a4fd":"markdown","9bcc1a56":"markdown","aaef15c4":"markdown","e04eb888":"markdown","d3a33e56":"markdown","a8dba9d6":"markdown","f1744dcf":"markdown","39643b18":"markdown","6e4e86b8":"markdown","abe3b3d4":"markdown","20e440c2":"markdown","7246003a":"markdown","1d3fc8a0":"markdown","e214253e":"markdown","3ac91ab3":"markdown","1166c3e1":"markdown","dc3834ed":"markdown","c54df9f3":"markdown","5b292898":"markdown","9d309457":"markdown","9e924205":"markdown","d92402d7":"markdown","d37a92bd":"markdown","9bbc63f5":"markdown","aac8328c":"markdown","1a11e6d9":"markdown","e8353d05":"markdown","38e53467":"markdown","3fc64299":"markdown","4a3c7f08":"markdown","1dfcc419":"markdown","05424180":"markdown","ad49dee0":"markdown","ce4fdf44":"markdown","729e7ba5":"markdown","8c181fd5":"markdown","1c3cd290":"markdown","387bdf44":"markdown","afc4b667":"markdown","dcf019fc":"markdown","ff2413f8":"markdown","ce503689":"markdown","4e1a07e5":"markdown","dc75717a":"markdown","e173559a":"markdown","961ffaa5":"markdown","356a559e":"markdown","809bf2bc":"markdown","c70d52c1":"markdown","945b743f":"markdown","f4cd4d2a":"markdown","78833779":"markdown"},"source":{"12c2cf08":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%config InlineBackend.figure_format='retina'\nimport warnings\nwarnings.filterwarnings('ignore')","9e84df66":"train_df=pd.read_csv('..\/input\/titanic\/train.csv',index_col='PassengerId')\ntest_df=pd.read_csv('..\/input\/titanic\/test.csv',index_col='PassengerId')","f95447b3":"train_df.head()","84b69122":"train_df.info()","0ce4f917":"train_df.describe()","238deecc":"train_df.shape","2034cd3e":"train_df['Sex']=train_df['Sex'].map({'female':0,'male':1})","0b87858a":"plt.rcParams['figure.figsize']=(16,12)\ntrain_df.drop(['Name','Cabin','Embarked','Ticket','Survived'], axis=1).hist();","62683738":"plt.figure(figsize=(10,8))\nsns.heatmap(train_df.corr(method='pearson'));","96c25d68":"plt.figure(figsize=(8,4))\nsns.boxplot(x='Age',data=train_df);","eb1f0282":"train_df['Pclass'].value_counts(normalize=True)","9184910f":"train_df['SibSp'].value_counts()","d0bbba9b":"train_df['Parch'].value_counts()","d1cefc7b":"pd.crosstab(train_df['Survived'],train_df['Pclass'])","81f750aa":"plt.figure(figsize=(8,5))\nsns.countplot(x='Pclass',hue='Survived',data=train_df);","10a45834":"plt.figure(figsize=(6,6))\nsns.regplot(x=train_df['Pclass'],y=train_df['Survived']);","b21c6f6b":"train_df.groupby('Survived')['Pclass'].agg(np.mean)","1af94064":"pd.crosstab(train_df['Survived'], train_df['Sex'])","8312f0b7":"plt.figure(figsize=(8,5))\nsns.countplot(x='Sex',hue='Survived',data=train_df);","d43ee5e2":"train_df.groupby('Survived')['Sex'].agg(np.mean)","05ad1348":"pd.crosstab(train_df['Survived'], train_df['SibSp'])","d2c415b5":"plt.figure(figsize=(8,5))\nsns.countplot(x='SibSp',hue='Survived',data=train_df);","a464c3d1":"pd.crosstab(train_df['Survived'], train_df['Parch'])","4ba0b81d":"plt.figure(figsize=(8,5))\nsns.countplot(x='Parch',hue='Survived',data=train_df);","17967038":"train_df.groupby('Survived')['Parch','SibSp'].agg(np.mean)","591c2545":"train_df['Ticket'].value_counts()","1bb2a1a4":"train_df.drop(['Ticket','Name'],axis=1,inplace=True)","ea0c2c74":"train_df.info()","f2ff1c14":"plt.figure(figsize=(8,5))\nsns.regplot(x=train_df['Fare'],y=train_df['Survived']);","6d11e5f3":"train_df.groupby('Survived')['Fare'].agg([np.mean,np.median])","f1ae6354":"plt.figure(figsize=(6,5))\nsns.boxplot(x='Survived',y='Fare',data=train_df);","8e5fb55e":"plt.figure(figsize=(8,5))\nsns.regplot(x=train_df['Age'],y=train_df['Survived']);","1265bf5d":"train_df.groupby('Survived')['Age'].agg([np.mean,np.median])","1f9b0ca9":"train_df['Age'].mean()","df8454ed":"plt.figure(figsize=(6,5))\nsns.boxplot(x='Survived',y='Age',data=train_df);","d15431e6":"age_mean=train_df['Age'].mean()","0ef0dab0":"train_df['Age']=train_df['Age'].fillna(age_mean)","0c644df9":"train_df['Age'].mean()","85d7cfbf":"train_df['Cabin'].value_counts()","738c4be6":"train_df=train_df.drop('Cabin',axis=1)","604274b5":"train_df['Embarked'].value_counts()","98544911":"pd.crosstab(train_df['Survived'], train_df['Embarked'])","2088e243":"plt.figure(figsize=(8,5))\nsns.countplot(x=train_df['Embarked'], hue='Survived',data=train_df);","306340bf":"train_df['Embarked']=train_df['Embarked'].replace(to_replace=np.nan,value='S')","fa61bd71":"train_df['Embarked'].unique()","8efb0bef":"train_df['Embarked']=train_df['Embarked'].map({'S':1,'C':2,'Q':3})","66b074a9":"train_df.head()","9d0d20d3":"X,y=train_df.drop('Survived',axis=1),train_df['Survived']","fee3de69":"from sklearn.model_selection import train_test_split","a621650b":"X_train, X_holdout, y_train, y_holdout= \\\n        train_test_split(X,y,test_size=.3,random_state=67)","2573c0ba":"X_train.shape, X_holdout.shape  #Checking the size of split (just for visualization).","0b6ff588":"from sklearn.tree import DecisionTreeClassifier","c5f1a2c6":"tree=DecisionTreeClassifier(random_state=67)  #create DecisionTree","7436fed4":"tree.fit(X_train,y_train) #fit","21889559":"from sklearn.metrics import accuracy_score #It calculates accuracy","da290f05":"pred_holdout=tree.predict(X_holdout) #predict","3c0a2f2d":"accuracy_score(y_holdout,pred_holdout) #same as i got in Kaggle training course","e3cb5093":"y.value_counts(normalize=True)   #doing this I can compare my result with baseline (0.62 vs 0.78) good result","ca032204":"from sklearn.model_selection import GridSearchCV, StratifiedKFold","97a1debf":"params = {'max_depth': np.arange(2,11), 'min_samples_leaf': np.arange(2,11)}","dafd4cda":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=67) #with this object you can specify your CV","44fee457":"best_tree = GridSearchCV(estimator=tree, param_grid=params, cv=skf, n_jobs=-1) #to get the result we need to fit data to it","5135890d":"best_tree.fit(X_train,y_train)","30d8e73c":"best_tree.best_params_ #best params shows the best configuration of hyperparams","041f15dc":"best_tree.best_score_ #CV assessment #Now with tuned hyperparams we have a better tree. And we calculate accuracy with it.","9be2c3dd":"pred_holdout_better=best_tree.predict(X_holdout)","8806233d":"accuracy_score(y_holdout,pred_holdout_better) #holdout assessment #result improved (0.779 vs 0.795) slightly better","bd97e3e2":"test_df=test_df.drop(['Name','Ticket','Cabin'],axis=1)","f3b44af4":"test_df['Sex']=test_df['Sex'].map({'female':0,'male':1})","41eddfdf":"test_df['Embarked']=test_df['Embarked'].map({'S':1, 'C':2, 'Q':3})","69951f37":"test_df['Age'] = test_df['Age'].fillna(test_df['Age'].mean())","a15e27c8":"test_df['Fare'] = test_df['Fare'].fillna(test_df['Fare'].mean())","ba0994af":"test_df.info()    #now test_df is preprocessed: have no null values, all objects are numeric","035303f9":"X_test = test_df","9b3e78e6":"predictions = best_tree.predict(X_test)","77cdcbb5":"output = pd.DataFrame({'PassengerId':test_df.index,'Survived':predictions})","b00f08bb":"output.to_csv('decision_tree.csv',index=False)","21aa6f52":"#### Problems that i encountered during this step:\n1)Map() function for 'Sex' returned 'Nan' for all values. The solution was: -write map function for 'sex' and plotting code in 2 different cells.(i will be glad if somebody explains to me why i cant do it in one cell. As far as i know code is run sequentially from top to bottom) <br>\n2)After i solved problems with 'Sex' feature plot was refusing to plot itself. I failed to find solution anywhere and finally restart&run kernel. After this everything worked fine.","fc9db322":"*Conclusion*: i will definetely add this feature to the future model.","e203cf52":"Split the training data into two subframes 70% and 30%","3bf36085":"Not sure if this is the best way, but i replaced 2 nan values in 'Embarked' with the largest value 'S'. In order to be able to use this feature more I will also turn it into integer where S=1, C=2, Q=3.","d6e9a4fd":"It seems like the only feature that can be successfully turned into numeric is 'Sex'. Maybe 'Embarked' can also be transformed but i think i'll be able to explore it using other methods(without plots).","9bcc1a56":"For now i just drop 'Cabin' maybe when I learn how to deal with it otherwhise.","aaef15c4":"# PART 1. EDA","e04eb888":"## 1.2. Correlation metrics","d3a33e56":"### 'Pclass'","a8dba9d6":"First of all i set the notebook and import all the libraries needed for EDA (Exploratory Data Analysis).\nThen i import both train and test csv files given in the competition.","f1744dcf":"sns.boxplot turns out to be not very useful for binary features and numeric features with small number of unique values.\nIn Titanic case only 'Age' feature works well with this plot. We see that most of the passengers are 20-40 years old. However I needn't build the boxplot to find this out.","39643b18":"### *Now i will tune the hyperparameters of the tree*\n(to see if it can do better)","6e4e86b8":"Now i need to try the model on test_df. And it seems i have to preprocess it to in order to try the model.","abe3b3d4":"First i will explore each feature in pare with the target feature. plt.scatter doesn't work fine here, and i can't really interpret something with it (if somebody knows the way, please tell me).","20e440c2":"## Decision Trees\nThis is the first model i will build.","7246003a":"After preprocessing the Data i have no nan values and no categorical falues.","1d3fc8a0":"## Linear models","e214253e":"*describe() shows results only for numeric features","3ac91ab3":"### 'Name'\nNot sure what to do with this one yet, but i don't think it has any relation to the surviving probability.","1166c3e1":"### 'Cabin'","dc3834ed":"First of all i create a heatmap for all of the feature to see the correlation between them.","c54df9f3":"*Best params should be somewhere in the middle of the range i given, if they are near borders: (2,11) this is a bad sign.*","5b292898":"I want to see 'Sex' feature among other histograms so i turn it into binary using map() function.","9d309457":"Mean of 'Age' and age of those who survived are alomst equal. So i think this parameter isn't really useful for the model.","9e924205":"I assume that tickets with several values are for people from one family, maybe they all have tickets with the same number. But i don't think i will use it in the model.","d92402d7":"# 2. EDA of single features","d37a92bd":"*Higher the 'Fare'* bigger the chance to survive","9bbc63f5":"The plot above shows that people who had 1<SibSp<3 had much bigger chances of surviving than others.","aac8328c":"### 'Fare'","1a11e6d9":"*older people have more chances to die*","e8353d05":"# 3. Feature interractions","38e53467":"### 'Parch'","3fc64299":"Actually, 'Embarked' shows that more than a half of people who boarded in C survived, and 70 percent of those who boarded in S died. ","4a3c7f08":"### 'Sex'","1dfcc419":"### 'Age'","05424180":"# Intro\nHello! I started learning CS, programming and Data Science nearly 2 months ago and i think it's time to start my first Kaggle competition analysis project. My main goal is to practise main EDA technics, build and compare a couple of models and beat my fitst submission from the 'Titanic learning micro-course'.\n\nI will be glad to hear your feedback!","ad49dee0":"I replace nan values with the mean age.","ce4fdf44":"The \"Age' feature isn't really showing anything. Chances for surviving were equal among passengers of all ages.","729e7ba5":"# 1. EDA of the whole dataset.\n## 1.1. Histograms of all features \n\nI need to drop all non-numeric features or turn them into numeric in order to build histogram.","8c181fd5":"where X-DataFrame of features used to train the model, y-target feature.","1c3cd290":"The biggest percentage of survived is people with class 1 tickets. People with class 3 tickets weren't very lucky.","387bdf44":"### 'Embarked'","afc4b667":"### *Fit-Predict*","dcf019fc":"Results here are pretty similar to 'SibSp' feature.\nThe plot above shows that people who had 0<Parch<4 had much bigger chances of surviving than others.","ff2413f8":"During this step i will use corr() function on different features and build a couple of heatmap plots for further exploration of features.","ce503689":"#### 3 features have null values: 'Age', 'Cabin' and 'Embarked'.\n\n*'Age'* feature to my mind is going to be very correlated with the target feature 'Survived'\nthat's why i can't drop this column and will have to find the best way to replace missing values.\n\n*'Embarked'* feature has only 2 missing values which i will easily replace with maybe mean values of the column.\nAnd i think that the port of embarkment isn't really tied with the taget feature, but i'll check it later to make sure.\n\n*'Cabin'* feature has more than half missing values and it will be very difficult to replace them. I'll check if there's any\nstrong relations for this feature but as for now i think i will drop this column.\n","4e1a07e5":"### 'Ticket'","dc75717a":"I'll leave it here for now.","e173559a":"#### Ideas that the histograms of numeric fetures gives: \n1)there are ~90 kids on board a ship <br>\n2)most of the passengers travelled alone <br>\n3)most of the passengers had level-3 tickets","961ffaa5":"With Decision trees the maximum accuracy I achieved is 0.75 which is even worth than the default Kaggle mini-course submission which uses only 'Sex' feature... <br>\nOk, I think I get it. The Kaggle variant uses Random Forest - very simple, without tuning or even using all avaliable features. I guess my scores is lower because Decision Tree is simply worse in this particular case.","356a559e":"Men are so sacrificial...","809bf2bc":"The next step is to explore the whole dataset using general functions and methods.","c70d52c1":"Crosstab and countplot aren't the nest idea with this one. Here we need to build some plot that can show the relation with a lot of values (same situation for 'Age')","945b743f":"### 'SibSp'","f4cd4d2a":"From the plot we can conclude that the biggest corellation metrics are between: <br>\n*Negative*: 1) Pclass - Fare 2) Sex - Survived <br>\n*Positive*: 1) SibSp - Parch  <br>\nThese indicators are very natural.","78833779":"# PART 2. MODEL"}}