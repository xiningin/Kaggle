{"cell_type":{"d1589a41":"code","db1d7e81":"code","d81ab82b":"code","6b1c4e2a":"code","d971554b":"code","24d71c6e":"code","b37e72f7":"code","251d68f8":"code","01e61a3d":"code","5bcaf949":"code","13f0d20e":"code","b06bf341":"code","5da7fc03":"code","42a6537d":"code","f24e97d6":"code","ae078aa8":"code","67f9acdc":"code","8369e6d1":"code","1a394c0d":"code","9c378218":"code","71f02e7d":"code","1309fd62":"code","ce80aa29":"code","58a9de31":"code","06c8b5bd":"code","8cda115a":"code","f1c0bab4":"code","2bda7a18":"code","bede63d8":"code","a3c3dc9e":"code","734f8fe3":"code","c1aaef95":"code","b79bf181":"code","5d33e279":"code","b135f21b":"code","bc4d3408":"code","1b6ae38c":"code","233f285c":"code","8ca7bf0b":"code","e3eef915":"code","3331fecc":"code","6b98f8be":"code","5e8cc675":"code","7ac8a8bd":"code","1dbc69b5":"markdown","a01380c7":"markdown","2373a5ea":"markdown","3321453b":"markdown","b6675d4b":"markdown","dcd38aee":"markdown","d56633f4":"markdown","ee726045":"markdown","c7c19a80":"markdown","bc764362":"markdown","b9c24d74":"markdown","ac0702f1":"markdown","13cfa1ff":"markdown","756d37a5":"markdown","395225d2":"markdown","ec806348":"markdown","247d2933":"markdown","186bc8ef":"markdown","bb9d473e":"markdown","148791cc":"markdown","2b50fd4b":"markdown","d30c3aa5":"markdown","38f7720c":"markdown","279d6b95":"markdown","ddf64588":"markdown","304aec47":"markdown","81ca5dff":"markdown","7f77a40c":"markdown","7d0ee70e":"markdown","c66eac12":"markdown","0f444d33":"markdown","e25712bd":"markdown","4003921f":"markdown","db6490f9":"markdown","b9a5b303":"markdown","039c7ba0":"markdown","7e47f4e3":"markdown","ce1961ff":"markdown","f8d4d1db":"markdown","a16584ef":"markdown","383e11f4":"markdown","4d9b9a45":"markdown","7b876e3a":"markdown","0495266d":"markdown","602858fc":"markdown","bf6bef33":"markdown","1adeb5fa":"markdown"},"source":{"d1589a41":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_color_codes()\n%matplotlib inline\nfrom sklearn import metrics\n\n#setting up for customized printing\nfrom IPython.display import Markdown, display\nfrom IPython.display import HTML\ndef printmd(string, color=None):\n    colorstr = \"<span style='color:{}'>{}<\/span>\".format(color, string)\n    display(Markdown(colorstr))\n    \n#function to display dataframes side by side    \nfrom IPython.display import display_html\ndef display_side_by_side(*args):\n    html_str=''\n    for df in args:\n        html_str+=df.to_html()\n    display_html(html_str.replace('table','table style=\"display:inline;margin-left:50px !important;margin-right: 40px !important\"'),raw=True)","db1d7e81":"bankLoan = pd.read_csv(\"..\/input\/Bank_Personal_Loan_Modelling.csv\")\nbankLoan.head()","d81ab82b":"print('The total number of rows :', bankLoan.shape[0])\nprint('The total number of columns :', bankLoan.shape[1])","6b1c4e2a":"bankLoan.info()","d971554b":"bankLoan.drop('ID', axis = 1, inplace=True)\nbankLoan.drop('ZIP Code', axis = 1, inplace=True)","24d71c6e":"print(bankLoan.isna().sum())\nprint('=============================')\nprint(bankLoan.isnull().sum())\nprint('=============================')\nprintmd('**CONCLUSION**: As seen from the data above, we conclude there are **\"NO Missing\"** values in the data', color=\"blue\")","b37e72f7":"printmd('**Find the names of columns having negative values**', color=\"brown\")\nprint([bankLoan[bankLoan[col] < 0].Experience.value_counts() for col in bankLoan.columns if any(bankLoan[col] < 0)])\nprint('=============================')\nprintmd('**CONCLUSION**: There are **\"NEGATIVE\"** values in the **\"Experience\"** column', color=\"blue\")","251d68f8":"printmd('**Replace the negative values with nan**', color=\"brown\")\nbankLoan = bankLoan.assign(Experience = lambda x: x.Experience.where(x.Experience.ge(0)))\nprint(bankLoan.Experience.isna().sum())\n\nprint('=============================')\n\nprintmd('**Since the imputation needs to be done in only 1 columns (Experience), we will use fillna imputation method**', color=\"brown\")\n\nprint(f'Median of Experience Column is {bankLoan.Experience.median()}')\n\nprint('=============================')\n\nprint(f'Mean of Experience Column before imputation is {bankLoan.Experience.mean()}')\nbankLoan.Experience = bankLoan.Experience.fillna(bankLoan.Experience.median())\nprint(f'Mean of Experience Column after imputation is {bankLoan.Experience.mean()}')\n","01e61a3d":"bankLoan.describe().transpose()","5bcaf949":"bankLoan[bankLoan['Personal Loan'] == 0]['Age']","13f0d20e":"f, axes = plt.subplots(1, 3, figsize=(20, 8))\nage = sns.distplot(bankLoan['Age'], color=\"red\", ax=axes[0], kde=True, hist_kws={\"edgecolor\":\"k\"})\nage.set_xlabel(\"Age\",fontsize=20)\n\nexp = sns.distplot(bankLoan['Experience'], color='green', ax = axes[1], kde=True, hist_kws={\"edgecolor\":\"k\"})\nexp.set_xlabel(\"Experience\",fontsize=20)\n\nincome = sns.distplot(bankLoan['Income'], color='blue', ax = axes[2], kde=True, hist_kws={\"edgecolor\":\"k\"})\nincome.set_xlabel(\"Income\",fontsize=20)\n\nf, axes = plt.subplots(1, 2, figsize=(11, 6))\n\nccavg = sns.distplot(bankLoan['CCAvg'], color=\"brown\", ax=axes[0], kde=True, hist_kws={\"edgecolor\":\"k\"})\nccavg.set_xlabel(\"CCAvg\",fontsize=20)\n\nmort = sns.distplot(bankLoan['Mortgage'], color=\"teal\", ax=axes[1], kde=True, hist_kws={\"edgecolor\":\"k\"})\nmort.set_xlabel(\"Mortgage\",fontsize=20)\n","b06bf341":"pd.DataFrame.from_dict(dict(\n    {\n        'Age':bankLoan.Age.skew(), \n        'Experience': bankLoan.Experience.skew(), \n        'Income': bankLoan.Income.skew(),\n        'CCAvg': bankLoan.CCAvg.skew(),\n        'Mortgage': bankLoan.Mortgage.skew()        \n    }), orient='index', columns=['Skewness'])","5da7fc03":"f, axes = plt.subplots(1, 3, figsize=(10, 8))\nincome = sns.boxplot(bankLoan['Income'], color=\"olive\", ax=axes[0], orient='v')\nincome.set_xlabel(\"Income\",fontsize=20)\n\nccavg = sns.boxplot(bankLoan['CCAvg'], color='lightgreen', ax=axes[1], orient='v')\nccavg.set_xlabel(\"CCAvg\",fontsize=20)\n\nmort = sns.boxplot(bankLoan['Mortgage'], color='lightblue', ax=axes[2], orient='v')\nmort.set_xlabel(\"Mortgage\",fontsize=20)\n","42a6537d":"plData = pd.DataFrame(bankLoan['Personal Loan'].value_counts(), columns=['Personal Loan'])\nsaData = pd.DataFrame(bankLoan['Securities Account'].value_counts(), columns=['Securities Account'])\ncdacctData = pd.DataFrame(bankLoan['CD Account'].value_counts(), columns=['CD Account'])\nonlineData = pd.DataFrame(bankLoan['Online'].value_counts(), columns=['Online'])\nccData = pd.DataFrame(bankLoan['CreditCard'].value_counts(), columns=['CreditCard'])\n\ncat = pd.concat([plData,saData,onlineData,cdacctData,ccData], axis=1)\ndisplay(cat)\nprint('=============================')\nedu = pd.DataFrame(bankLoan.Education.value_counts(), columns=['Education'])\ndisplay(edu.sort_index())\nprint('=============================')\nfam = pd.DataFrame(bankLoan.Family.value_counts(), columns=['Family'])\ndisplay(fam.sort_index())","f24e97d6":"f, axes = plt.subplots(1, 5, figsize=(20, 6))\npl = sns.countplot(bankLoan['Personal Loan'], color=\"orange\", ax=axes[0])\npl.set_xlabel(\"Personal Loan\",fontsize=20)\n\nsecacct = sns.countplot(bankLoan['Securities Account'], color='lightgreen', ax = axes[1])\nsecacct.set_xlabel(\"Securities Account\",fontsize=20)\n\ncdacct = sns.countplot(bankLoan['CD Account'], color='lightblue', ax = axes[2])\ncdacct.set_xlabel(\"CD Account\",fontsize=20)\n\nonline = sns.countplot(bankLoan['Online'], color='silver', ax = axes[3])\nonline.set_xlabel(\"Online\",fontsize=20)\n\ncc = sns.countplot(bankLoan['CreditCard'], color='teal', ax = axes[4])\ncc.set_xlabel(\"CreditCard\",fontsize=20)\n\nf, axes = plt.subplots(1, 2, figsize=(10, 6))\nfamily = sns.countplot('Family',data=bankLoan, color='darkgreen', ax=axes[0])\nfamily.set_xlabel(\"Family\",fontsize=20)\n\nedu = sns.countplot('Education',data=bankLoan, color='tomato', ax = axes[1])\nedu.set_xlabel(\"Education\",fontsize=20)","ae078aa8":"f, axes = plt.subplots(1, 6, figsize=(20, 6))\n\nsecacct = sns.countplot('Securities Account', data=bankLoan, hue='Personal Loan', palette='Accent', ax = axes[0])\nsecacct.set_xlabel(\"Securities Account\",fontsize=20)\n\ncdacct = sns.countplot(bankLoan['CD Account'], data=bankLoan, hue='Personal Loan', palette='Accent_r', ax = axes[1])\ncdacct.set_xlabel(\"CD Account\",fontsize=20)\n\nonline = sns.countplot(bankLoan['Online'], data=bankLoan, hue='Personal Loan', palette='BuGn_r', ax = axes[2])\nonline.set_xlabel(\"Online\",fontsize=20)\n\ncc = sns.countplot(bankLoan['CreditCard'], data=bankLoan, hue='Personal Loan', palette='BuPu_r', ax = axes[3])\ncc.set_xlabel(\"CreditCard\",fontsize=20)\n\n#f, axes = plt.subplots(1, 2, figsize=(10, 6))\nfamily = sns.countplot('Family',data=bankLoan, palette='Dark2', hue='Personal Loan', ax=axes[4])\nfamily.set_xlabel(\"Family\",fontsize=20)\n\nedu = sns.countplot('Education',data=bankLoan, palette='OrRd_r', hue='Personal Loan', ax = axes[5])\nedu.set_xlabel(\"Education\",fontsize=20)\n\n\nprintmd('**Count of Personal Loans availed**', color='brown')\nsecacct = pd.DataFrame(bankLoan[bankLoan['Personal Loan'] == 1]['Securities Account'].value_counts(), columns=['Securities Account'])\ncdacct = pd.DataFrame(bankLoan[bankLoan['Personal Loan'] == 1]['CD Account'].value_counts(), columns=['CD Account'])\nonline = pd.DataFrame(bankLoan[bankLoan['Personal Loan'] == 1]['Online'].value_counts(), columns=['Online'])\ncc = pd.DataFrame(bankLoan[bankLoan['Personal Loan'] == 1]['CreditCard'].value_counts(), columns=['CreditCard'])\n\ncat = pd.concat([secacct,cdacct,online,cc], axis=1)\ndisplay(cat)","67f9acdc":"f, axes = plt.subplots(1, 3, figsize=(20, 6))\n\nplt1 = sns.boxplot('Family', 'Income', data=bankLoan, hue='Personal Loan', palette='Set1', ax=axes[0])\nplt1.set_xlabel(\"Family\",fontsize=20)\nplt1.set_ylabel(\"Income\",fontsize=20)\n\nplt2 = sns.boxplot('Family', 'CCAvg', data=bankLoan, hue='Personal Loan', palette='YlOrBr_r', ax=axes[1])\nplt2.set_xlabel(\"Family\",fontsize=20)\nplt2.set_ylabel(\"CCAvg\",fontsize=20)\n\nplt3 = sns.boxplot('Family', 'Mortgage', data=bankLoan, hue='Personal Loan', palette='viridis', ax=axes[2])\nplt3.set_xlabel(\"Family\",fontsize=20)\nplt3.set_ylabel(\"Mortgage\",fontsize=20)","8369e6d1":"f, axes = plt.subplots(1, 2, figsize=(14, 6))\n\nplt1 = sns.boxplot('Education', 'Income', data=bankLoan, hue='Personal Loan', palette='Set1', ax=axes[0])\nplt1.set_xlabel(\"Education\",fontsize=20)\nplt1.set_ylabel(\"Income\",fontsize=20)\n\nplt2 = sns.boxplot('Education', 'CCAvg', data=bankLoan, hue='Personal Loan', palette='YlOrBr_r', ax=axes[1])\nplt2.set_xlabel(\"Education\",fontsize=20)\nplt2.set_ylabel(\"CCAvg\",fontsize=20)","1a394c0d":"sns.catplot('Family', 'Income', data=bankLoan, hue='Personal Loan', col='Education', kind='box', palette='Set1')\n\nsns.catplot('Family', 'CCAvg', data=bankLoan, hue='Personal Loan', col='Education', kind='box', palette='YlOrBr_r')\n\nsns.catplot('Family', 'Mortgage', data=bankLoan, hue='Personal Loan', col='Education', kind='box', palette='viridis')","9c378218":"sns.pairplot(bankLoan[['Age','Experience','Income','CCAvg', 'Mortgage', 'Family', 'Personal Loan']], hue='Personal Loan', diag_kind = 'kde', palette='rocket')","71f02e7d":"corrData = bankLoan.corr()\nf, axes = plt.subplots(1, 1, figsize=(10, 8))\nsns.heatmap(corrData,cmap='YlGnBu', ax=axes, annot=True, fmt=\".2f\", linecolor='white', linewidths=0.3, square=True)\nplt.xticks(rotation=60)","1309fd62":"bankLoan['Education'] = bankLoan['Education'].astype(dtype='category')\nbankLoan['Family'] = bankLoan['Family'].astype(dtype='category')","ce80aa29":"# Create correlation matrix\ncorr_matrix = bankLoan.corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nf, axes = plt.subplots(1, 1, figsize=(10, 8))\nsns.heatmap(upper,cmap='YlGnBu', annot=True, fmt=\".2f\", ax=axes, linecolor='white', linewidths=0.3, square=True)\nplt.xticks(rotation=60)","58a9de31":"# Find index of feature columns with correlation greater than 0.98\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\nprintmd('List of columns with correlation higher than 0.95', color='brown')\ndisplay(to_drop)\n\nprintmd('Removing **\"Experience\"** column due to **Multicollinearity**', color='brown')\n\nbankLoanNew = bankLoan.drop('Experience', axis = 1)\nbankLoanNew.info()","06c8b5bd":"f, axes = plt.subplots(1, 3, figsize=(20, 6))\n\nmort = sns.distplot(bankLoanNew['Mortgage'], color=\"darkgreen\", ax=axes[0], kde=True, hist_kws={\"edgecolor\":\"k\"})\nmort.set_xlabel(\"Mortgage\",fontsize=20)\nmort.set_title('Without Log Tranformation')\n\nfirstLogTransform = np.log1p(bankLoanNew['Mortgage'])\nmort = sns.distplot(firstLogTransform, color=\"darkgreen\", kde=True, ax=axes[1], hist_kws={\"edgecolor\":\"k\"})\nmort.set_xlabel(\"Mortgage\",fontsize=20)\nmort.set_title('After First Log Tranformation')\n\nsecondLogTransform = np.log1p(firstLogTransform)\nmort = sns.distplot(secondLogTransform, color=\"darkgreen\", kde=True,ax=axes[2], hist_kws={\"edgecolor\":\"k\"})\nmort.set_xlabel(\"Mortgage\",fontsize=20)\nmort.set_title('After Second Log Tranformation')\n\nmortSkew = pd.DataFrame.from_dict(dict(\n    {\n        'Mortgage Without Log Transformation':bankLoan.Mortgage.skew(), \n        'Mortgage After First Log Transformation': firstLogTransform.skew(), \n        'Mortgage After Second Log Transformation': secondLogTransform.skew()\n    }), orient='index', columns=['Skewness'])\n\ndisplay(mortSkew)\n\nbankLoanNew['Mortgage'] = secondLogTransform","8cda115a":"f, axes = plt.subplots(1, 2, figsize=(15, 6))\n\nccavg = sns.distplot(bankLoanNew['CCAvg'], color=\"darkorange\", ax=axes[0], kde=True, hist_kws={\"edgecolor\":\"k\"})\nccavg.set_xlabel(\"CCAvg\",fontsize=20)\nccavg.set_title('Without Log Tranformation')\n\nfirstLogTransform = np.log1p(bankLoanNew['CCAvg'])\nccavg = sns.distplot(firstLogTransform, color=\"darkorange\", kde=True, ax=axes[1], hist_kws={\"edgecolor\":\"k\"})\nccavg.set_xlabel(\"CCAvg\",fontsize=20)\nccavg.set_title('After First Log Tranformation')\n\nccavgSkew = pd.DataFrame.from_dict(dict(\n    {\n        'CCAvg Without Log Transformation':bankLoan.CCAvg.skew(), \n        'CCAvg After First Log Transformation': firstLogTransform.skew()\n    }), orient='index', columns=['Skewness'])\n\ndisplay(ccavgSkew)\n\nbankLoanNew['CCAvg'] = firstLogTransform","f1c0bab4":"from sklearn.preprocessing import OneHotEncoder\n\nonehotencoder = OneHotEncoder(categories='auto')\nencodedData = onehotencoder.fit_transform(bankLoanNew[['Family','Education']]).toarray() \nencodedFeatures = pd.DataFrame(encodedData, columns= onehotencoder.get_feature_names(['Family','Education']))\nencodedFeatures.head(2)","2bda7a18":"printmd('''Dropping last encoded feature in each attribute i.e **Family_4**, **Education_3** can be **DROPPED** as information for these features\ncan be obtained from others''', color='brown')\n\nencodedFeatures.drop(['Family_4', 'Education_3'], axis=1, inplace=True)\nbankLoanNew.drop(['Family', 'Education'], axis=1, inplace=True)","bede63d8":"encodedFeatures.head(2)","a3c3dc9e":"bankLoanNew = pd.concat([bankLoanNew,encodedFeatures],axis=1)\nbankLoanNew.head(2)","734f8fe3":"printmd('**As \"Personal Loan\" attribute is imbalanced, STRATIFYING the same to maintain the same percentage of distribution**', color='brown')\nX = bankLoanNew.loc[:, bankLoanNew.columns != 'Personal Loan']\ny = bankLoanNew['Personal Loan']\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y, test_size =.30, random_state=1)\n\nprintmd('**Training and Testing Set Distribution**', color='brown')\n\nprint(f'Training set has {X_train.shape[0]} rows and {X_train.shape[1]} columns')\nprint(f'Testing set has {X_test.shape[0]} rows and {X_test.shape[1]} columns')\n\nprintmd('**Original Set Target Value Distribution**', color='brown')\n\nprint(\"Original Personal Loan '1' Values    : {0} ({1:0.2f}%)\".format(len(bankLoanNew.loc[bankLoanNew['Personal Loan'] == 1]), (len(bankLoanNew.loc[bankLoanNew['Personal Loan'] == 1])\/len(bankLoanNew.index)) * 100))\nprint(\"Original Personal Loan '0' Values   : {0} ({1:0.2f}%)\".format(len(bankLoanNew.loc[bankLoanNew['Personal Loan'] == 0]), (len(bankLoanNew.loc[bankLoanNew['Personal Loan'] == 0])\/len(bankLoanNew.index)) * 100))\n\nprintmd('**Training Set Target Value Distribution**', color='brown')\n\nprint(\"Training Personal Loan '1' Values    : {0} ({1:0.2f}%)\".format(len(y_train[y_train[:] == 1]), (len(y_train[y_train[:] == 1])\/len(y_train)) * 100))\nprint(\"Training Personal Loan '0' Values   : {0} ({1:0.2f}%)\".format(len(y_train[y_train[:] == 0]), (len(y_train[y_train[:] == 0])\/len(y_train)) * 100))\n\nprintmd('**Testing Set Target Value Distribution**', color='brown')\nprint(\"Test Personal Loan '1' Values        : {0} ({1:0.2f}%)\".format(len(y_test[y_test[:] == 1]), (len(y_test[y_test[:] == 1])\/len(y_test)) * 100))\nprint(\"Test Personal Loan '0' Values       : {0} ({1:0.2f}%)\".format(len(y_test[y_test[:] == 0]), (len(y_test[y_test[:] == 0])\/len(y_test)) * 100))\n","c1aaef95":"from sklearn.preprocessing import StandardScaler\nscalar = StandardScaler()\nX_train_scaled = scalar.fit_transform(X_train)\nX_test_scaled = scalar.fit_transform(X_test)","b79bf181":"f, axes = plt.subplots(1, 1, figsize=(10, 8))\nsns.heatmap(pd.DataFrame(X_train_scaled).corr().abs(),cmap='YlGnBu', ax=axes, annot=True, fmt=\".2f\",xticklabels=X_train.columns, yticklabels=X_train.columns, linecolor='white', linewidths=0.3, square=True)\nplt.xticks(rotation=60)","5d33e279":"# function for model fitting, prediction and calculating different scores\ndef Modelling_Prediction_Scores(model, algoName):\n    model.fit(X_train_scaled, y_train)\n    #predict on train and test\n    y_train_pred = model.predict(X_train_scaled)\n    y_test_pred = model.predict(X_test_scaled)\n\n    #predict the probabilities on train and test\n    y_train_pred_proba = model.predict_proba(X_train_scaled) \n    y_test_pred_proba = model.predict_proba(X_test_scaled)\n\n    #get Accuracy Score for train and test\n    accuracy_train = metrics.accuracy_score(y_train, y_train_pred)\n    accuracy_test = metrics.accuracy_score(y_test, y_test_pred)\n    accdf = pd.DataFrame([[accuracy_train, accuracy_test, ]], columns=['Training', 'Testing'], index=['Accuracy'])\n\n    #get Mean Squared Error on train and test\n    mse_train = metrics.mean_squared_error(y_train, y_train_pred)\n    mse_test = metrics.mean_squared_error(y_test, y_test_pred)\n    msedf = pd.DataFrame([[mse_train, mse_test, ]], columns=['Training', 'Testing'], index=['Mean Squared Error'])\n\n    #get Precision Score on train and test\n    precision_train = metrics.precision_score(y_train, y_train_pred)\n    precision_test = metrics.precision_score(y_test, y_test_pred)\n    precdf = pd.DataFrame([[precision_train, precision_test, ]], columns=['Training', 'Testing'], index=['Precision'])\n\n    #get Recall Score on train and test\n    recall_train = metrics.recall_score(y_train, y_train_pred)\n    recall_test = metrics.recall_score(y_test, y_test_pred)\n    recdf = pd.DataFrame([[recall_train, recall_test, ]], columns=['Training', 'Testing'], index=['Recall'])\n\n    #get F1-Score on train and test\n    f1_score_train = metrics.f1_score(y_train, y_train_pred)\n    f1_score_test = metrics.f1_score(y_test, y_test_pred)\n    f1sdf = pd.DataFrame([[f1_score_train, f1_score_test, ]], columns=['Training', 'Testing'], index=['F1 Score'])\n\n    #get Area Under the Curve (AUC) for ROC Curve on train and test\n    roc_auc_score_train = metrics.roc_auc_score(y_train, y_train_pred)\n    roc_auc_score_test = metrics.roc_auc_score(y_test, y_test_pred)\n    rocaucsdf = pd.DataFrame([[roc_auc_score_train, roc_auc_score_test, ]], columns=['Training', 'Testing'], index=['ROC AUC Score'])\n\n    #get Area Under the Curve (AUC) for Precision-Recall Curve on train and test\n    precision_train, recall_train, thresholds_train = metrics.precision_recall_curve(y_train, y_train_pred_proba[:,1])\n    precision_recall_auc_score_train = metrics.auc(recall_train, precision_train)\n    precision_test, recall_test, thresholds_test = metrics.precision_recall_curve(y_test,y_test_pred_proba[:,1])\n    precision_recall_auc_score_test = metrics.auc(recall_test, precision_test)\n    precrecaucsdf = pd.DataFrame([[precision_recall_auc_score_train, precision_recall_auc_score_test]], columns=['Training', 'Testing'], index=['Precision Recall AUC Score'])\n\n    #calculate the confusion matrix \n    #print('tn, fp, fn, tp')\n    confusion_matrix_test = pd.crosstab(y_test, y_test_pred, rownames=['Actual'], colnames=['Predicted'])\n\n    #display confusion matrix in a heatmap\n    f, axes = plt.subplots(1, 2, figsize=(20, 8))\n    hmap = sns.heatmap(confusion_matrix_test, cmap='YlGnBu', annot=True, fmt=\".0f\", ax=axes[0], )\n    hmap.set_xlabel('Predicted', fontsize=15)\n    hmap.set_ylabel('Actual', fontsize=15)\n\n    #plotting the ROC Curve and Precision-Recall Curve\n    fpr, tpr, threshold = metrics.roc_curve(y_test,y_test_pred_proba[:,1])\n    plt.plot(fpr, tpr, marker='.', label='ROC Curve')\n    plt.plot(recall_test, precision_test, marker='.', label='Precision Recall Curve')\n    plt.axes(axes[1])\n    plt.title(algoName, fontsize=15)\n    # axis labels\n    plt.xlabel('ROC Curve - False Positive Rate \\n Precision Recall Curve - Recall', fontsize=15)    \n    plt.ylabel('ROC Curve - True Positive Rate \\n Precision Recall Curve - Precision', fontsize=15)\n    # show the legend\n    plt.legend()\n    # show the plot\n    plt.show()\n\n    #concatenating all the scores and displaying as single dataframe\n    consolidatedDF= pd.concat([accdf, msedf,precdf,recdf,f1sdf, rocaucsdf, precrecaucsdf])\n\n    printmd('**Confusion Matrix**', color='brown')\n    display_side_by_side(confusion_matrix_test, consolidatedDF)\n    \n    return confusion_matrix_test, consolidatedDF","b135f21b":"from sklearn.linear_model import LogisticRegression\n\n# Fit the model on train\nlogRegModel = LogisticRegression()\ncmLR, dfLR = Modelling_Prediction_Scores(logRegModel, 'Logistic Regression')","bc4d3408":"# Fit the model on train\n# C is inverse of lambda (Regularization Parameter). Hence lower the C value will strenthen the lambda parameter.\nlogRegModel = LogisticRegression(solver = 'liblinear', C=0.2)\ncmLR, dfLR = Modelling_Prediction_Scores(logRegModel, 'Logistic Regression')","1b6ae38c":"from sklearn.naive_bayes import GaussianNB\n\n# Fit the model on train\ngnb = GaussianNB()\ncmNB, dfNB = Modelling_Prediction_Scores(gnb, 'Gaussian Naive Bayes Classifier')","233f285c":"#plot the f1-scores for different values of k for a model and see which is optimal\ndef Optimal_k_Plot(model):\n    # creating odd list of K for KNN\n    myList = list(range(3,20))\n\n    # subsetting just the odd ones\n    klist = list(filter(lambda x: x % 2 != 0, myList))\n    # empty list that will hold accuracy scores\n    scores = []\n\n    # perform accuracy metrics for values from 3,5....19\n    for k in klist:        \n        model.n_neighbors = k\n        model.fit(X_train_scaled, y_train)\n        # predict the response\n        y_test_pred = knn.predict(X_test_scaled)        \n        test_score= metrics.f1_score(y_test, y_test_pred)\n        scores.append(test_score)\n\n    # determining best k\n    optimal_k = klist[scores.index(max(scores))]\n    print(\"The optimal number of neighbors is %d\" % optimal_k)\n\n    import matplotlib.pyplot as plt\n    # plot misclassification error vs k\n    plt.plot(klist, scores)\n    plt.xlabel('Number of Neighbors K')\n    plt.ylabel('AUC Score')\n    plt.show()","8ca7bf0b":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nOptimal_k_Plot(knn)","e3eef915":"knn = KNeighborsClassifier(n_neighbors=3)\ncmKNN, dfKNN = Modelling_Prediction_Scores(knn, 'KNN Classifier')","3331fecc":"knn = KNeighborsClassifier(weights='distance',p=2)\nOptimal_k_Plot(knn)","6b98f8be":"knn = KNeighborsClassifier(n_neighbors=3,weights='distance',p=2)\ncmKNN, dfKNN = Modelling_Prediction_Scores(knn, 'KNN Classifier')","5e8cc675":"knn = KNeighborsClassifier(weights='distance',p=1)\nOptimal_k_Plot(knn)","7ac8a8bd":"from sklearn.neighbors import KNeighborsClassifier\n\n# Fit the model on train\nknn = KNeighborsClassifier(n_neighbors=5, weights='distance', p=1)\ncmKNN, dfKNN = Modelling_Prediction_Scores(knn, 'KNN Classifier')","1dbc69b5":"#### Observations\n\n1. Higher the level of **Education**, higher is the **Income**. \n2. Average spend using **Credit Card** gradually increases with levels of **Education**.","a01380c7":"#### Observations\n\n1. Type-I Error (FP) = 15\n2. Type-II Error (FN) = 52\n3. **Accuracy** is high, above 95%. \n4. Though **Accuracy** is higher, we need to look at Precision, Recall and AUC under Precision-Recall Curve since the target variable is imbalaced.\n5. The metrics on Training and Testing sets are closer.\n6. ROC Curve as well as Precision-Recall curve looks good in the plot \n7. Looking at the values of Precision(0.85), Recall(0.63) and AUC(0.83), this looks like a good model. \n","2373a5ea":"## Logistic Regression","3321453b":"### Let us initially try with default value of weights as Uniform","b6675d4b":"### Let us try to see if the model can be improved with Manhattan distance","dcd38aee":"### Conclusion\n\nBased on the observations for all the models, it is decided that **K-Nearest-Neighbours** Algorithm and the model with scores of Precision(0.95), Recall(0.63) and AUC(0.85) is the best. ","d56633f4":"## Multivariate Analysis","ee726045":"#### Observation\n1. The above plots confirm the presence of several outliers in **Income**, **CCAvg**, **Mortgage** columns.\n2. There is no bottom whisker or Q1(25%) for **Mortgage** as Q2(50%) starts at '0'. This means data is heavily skewed towards right with most of the values being zeros and others being large values. It can also seen that there lot more outliers than any other attribute.\n","c7c19a80":"## Data type of each attribute","bc764362":"## Removing ID and ZIP Code columns as they are redundant","b9c24d74":"## Different values of each categorical feature and their distributions","ac0702f1":"## Checking the presence of invalid values","13cfa1ff":"#### Using k value as 5 though the optimal value is 3","756d37a5":"#### Observations\n\n1. **Personal Loan**: This is the **TARGET** column. Many customers did not accept the personal loan offered and hence look very imbalanced.\n2. **Securities Account**: Many customers do not have a securities account with the bank. Looks imbalanced.\n3. **CD Account**: Many customers do not have a certificate of deposit (CD) account with the bank. Looks imbalanced.\n4. **Online**: Customers usage of internet banking facilities is at 60% and no-usage is at 40%. This is fairly distributed.\n5. **CreditCard**: Many customers (70%) do not use a credit card issued by UniversalBank.\n6. **Family**: Family size is fairly distributed with size of '3' being a bit low.\n7. **Education**: Education levels (Undergrads, Graduates, Professionals) are failrly distributed with undergrads being higher than others.","395225d2":"## Applying Log Transformation to the 'Mortgage' feature to reduce skewness","ec806348":"# Read the data as a data frame","247d2933":"### Using Gaussian Naive Bayes as some of the features are continuous","186bc8ef":"## Applying Log Transformation to the 'CCAvg' feature to reduce skewness","bb9d473e":"## Details of the Attributes and Banking Terms\n\n1. **ID**: Customer's ID.\n2. **Age** : Customer's age in completed years.\n3. **Experience** : Number of years of professional experience.\n4. **Income** : Annual income of the customer in thousand dollars (\\$000)\n5. **ZIP Code** : Home Address ZIP code.\n6. **Family** : Family size of the customer.\n7. **CCAvg** : Average spending on credit cards per month in thousand dollars (\\$000).\n8. **Education** : Level of Education,\n                1: Undergrad\n                2: Graduate\n                3: Advanced\/Professional\n9. **Mortgage** : A mortgage is a loan in which property or real estate is used as collateral. The borrower enters into an agreement with the lender (usually a bank) wherein the borrower receives cash upfront then makes payments over a set time span until he pays back the lender in full. This feature specified value of house mortgage if any, in thousand dollars (\\$000).\n10. **Personal Loan** : This feature specified whether customers accepted the personal loan offered in the last campaign.\n11. **Securities Account** : A securities account sometimes known as a **Brokerage Account** is an account that holds financial assets such as securities on behalf of an investor with a bank, broker or custodian. Investors and traders typically have a securities account with the broker or bank they use to buy and sell securities. This feature specified whether customers have a securities account with the bank. There is a possiblity that customers can avail loan against Securities. **Loans Against Securities** is available in the form of an overdraft facility which is pledged against financial securities like shares, units and bonds. Loan Against Shares\/Bonds\/Mutual Funds is basically a loan wherein you pledge the securities you have invested in as collateral against the loan amount.\n12. **CD Account** : Certificates of deposit are a secure form of time deposit, where money must stay in the bank for a certain length of time to earn a promised return. A CD, also called a \u201c**Share Certificate**\u201d at credit unions, almost always earns more interest than a regular savings account. This feature specifies whether customers have a certificate of deposit (CD) account with the bank or not. There is a possiblity that customers can avail loan against CD Accounts called a **CD loan**, which is a type of personal loan you obtain by putting up a certificate of deposit as collateral.\n13. **Online** : This feature specifies whether customers use internet banking facilities.\n14. **Credit card** : This feature specifies whether customers use a credit card issued by UniversalBank.","148791cc":"1. **Age** : Value is is close to 0, hence uniformly distributed as shown in the plot above.\n2. **Experience** : Values is close to 0, hence uniformly distributed as shown in the plot above.\n3. **Income**: Value is slightly high and hence right skewed as shown in the plot above.\n4. **CCAvg**: Value is high and hence right skewed as shown in the plot above.\n5. **Mortgage**: Value is high and hence right skewed as shown in the plot above.","2b50fd4b":"## Checking the presence of missing values","d30c3aa5":"#### Observations\n\n1. **Age** and **Experience** are evenly distributed and looks almost the same\n2. **Income** is slightly right skewed with few outliers\n3. **CCAvg** is right skewed with few outliers\n4. **Mortgage** contains many zeroes and lot of high values","38f7720c":"## One-Hot Encoding for \u2018Family\u2019, \u2018Education\u2019 columns","279d6b95":"## 5 point summary of numerical attributes","ddf64588":"## Checking the presence of outliers in \u2018Income\u2019, \u2018CCAvg\u2019 and \u2018Mortgage\u2019 columns","304aec47":"#### Observations\n\n1. Type-I Error (FP) = 58\n2. Type-II Error (FN) = 57\n3. Total number of errors are high compared to Logistic Regression.\n4. **Accuracy** is high, above 92%. \n5. Though **Accuracy** is higher, we need to look at Precision, Recall and AUC under Precision-Recall Curve since the target variable is imbalaced. These metrics are lower in value. \n6. ROC Curve looks good however Precision-Recall curve does not look good.\n6. Looking at the values of Precision(0.60), Recall(0.60) and AUC(0.67), this does not look like a better model.\n","81ca5dff":"#### Observations\n\n1. Type-I Error (FP) = 6,\n2. Type-II Error (FN) = 49. This is 3 less than previous model. \n3. However since the scores of **Accuracy**, **Precision**, **Recall**, **F1 Score**, **ROC Curve** and **Precision-Recall Curve** are all 1, we have the issue of overfitting. Hence this is not a better model.","7f77a40c":"#### Observations\n\n1. **Age** and **Experience** have very high correlation (0.98) and the relationship is close to linearly perfect. This is shown is above pairplot and heatmap. Either one can be removed. \n2. **Income** and **CCAvg** have good correlation (0.65)\n3. Target attribute **Personal Loan** has a good correlation with **Income**, **CCAvg** and **CD Account**","7d0ee70e":"#### Setting the k value as 3 as per above graph","c66eac12":"#### Setting the k value as 3 as per above graph","0f444d33":"## Naive Bayes Classification","e25712bd":"### Details of Classification Metrics\n\n#### Terms used\n\n**TP**: True Positive - Both Predicted and Actual are True\n\n**TN**: True Negative - Both Predicted and Actual are False\n\n**FP**: False Positive - Predicted True but Acutal is False\n\n**FN**: False Negative - Predicted False but Actual is True\n\n**Accuracy**: The accuracy of the model is basically the total number of correct predictions divided by total number of predictions. This is a very useful metrics when the target variable is balanced.\n\n    **Accuracy Score = (TP + TN) \/ (TP + TN + FP + FN)**\n\n**Mean Squared Error**: This is one minues the accuracy score. \n\n    **Mean Squared Error = 1 - Accuracy Score**\n\n**Precision**: The precision of a class defines how much we can trust the result when the model predicts that a point belongs to a class. \n\n    **Precision = TP \/ TP + FP**\n\n**Recall**: The recall of a class defines how good the model is able to predict a class. \n\n    **Recall = TP \/ TP + FN**\n\n**Recall** and **Precision** are useful metrics when the target variable is **Imbalanced**.\n\n**F1 Score**: The F1 score of a class is given by the harmonic mean of precision and recall.\n    \n    **F1 Score = (2 * Precision * Recall) \/ (Precision + Recall)** \n    \n**ROC AUC Score**: Area under the curve of a ROC (Receiver Operating Characteristic) curve which plots False Positive Rate on X-axis and True Positive Rate on Y-axis. This metric is useful when the target variable is balanced. Higher the area covered, better is the model.\n\n**Precision Recall AUC Score**: Area under the curve of a Precision-Recall curve which plots Recall on X-axis and Precision on Y-axis. This metric is useful when the target variable is imbalanced. Higher the area covered, better is the model.","4003921f":"## Distributions of Continuous attributes","db6490f9":"#### Observations\n\nTotal number of errors are reduced from 67 to 65 and thereby a very marginal increase in values of Precision, Recall and AUC Score.","b9a5b303":"#### Observations\n\n1. **Personal Loan** is availed for customers with higher **Income** for families of all sizes.\n2. **Personal Loan** is availed for customers with higher **CCAvg** i.e spending average using Credit Card for families of all sizes.\n4. **Personal Loan** availed is more for families of sizes 2 and 4 and with higher **Mortgage**. However the count of not availing is almost same for families of all sizes. ","039c7ba0":"### Let us try to see if the model can be improved by setting 'C' (Default is 1) value to a lower value and setting the algorithm as liblinear. ","7e47f4e3":"## KNN Classification","ce1961ff":"## Check Correlation Matrix and Heatmap for Dimensionality Reduction","f8d4d1db":"#### Observations\n\n1. Type-I Error (FP) = 5, a good reduction from both Logistic Regression and Naive Bayes classifier.\n2. Type-II Error (FN) = 52, looks almost same for all models built so far. \n3. **Accuracy** is high, above 96%. \n4. Though **Accuracy** is higher, we need to look at Precision, Recall and AUC under Precision-Recall Curve since the target variable is imbalaced.\n5. The metrics on Training and Testing sets are closer.\n6. **ROC Curve** as well as **Precision-Recall Curve** looks good in the plot \n7. **Precision** is higher compared to other models at 0.94.\n8. Looking at the values of Precision(0.95), Recall(0.63) and AUC(0.85), this looks like a good model. ","a16584ef":"## Imputation of Experience Column","383e11f4":"# Import the necessary libraries","4d9b9a45":"#### Observations\n1. **Age**: It is uniformly distributed with mean and median being almost equal\n\n2. **Experience**: It is uniformly distributed with mean and median being almost equal\n\n3. **Income**: It looks a bit skewed towards right side as mean (73.77) is greater than median 50% (64.0) and there may be few outliers at the top end of values.\n\n4. **Family**: Can be considered as a categorical variable with values of 1,2,3,4.\n\n5. **CCAvg**: It looks a bit skewed towards right side as mean (1.93) is greater than median 50% (1.5) and there may be few outliers at the top end of values.\n\n6. **Education**: It is a categorical variable with values of 1,2,3.\n\n7. **Mortgage**: It is unevenly distributed with zero median and mean of 56.49. There are lot of zeroes and few high values. \n\n8. **Personal Loan**, **Securities Account**, **CD Account**, **Online**, **CreditCard**: These are categorical variables with values of 0,1.","7b876e3a":"## Split the data into training and test set in the ratio of 70:30 respectively","0495266d":"## Shape of the data","602858fc":"#### Observations\n\n1. Type-I Error (FP) = 3. This is lowest we got so far. \n2. Type-II Error (FN) = 52. \n3. However since the scores of **Accuracy**, **Precision**, **Recall**, **F1 Score**, **ROC Curve** and **Precision-Recall Curve** are all 1, we have the issue of overfitting. Hence this is not a better model.","bf6bef33":"##### Observations\n\n1. As per the data, customers who did not accept the **Personal Loan** are very high. The same is shown across different values of the categorical attributes **Securities Account**, **CD Account**, **Online**, **Family**, **Education**.\n2. **Securities Account**: Number of Personal Loans availed (460) is slightly more for customers with **No Securities Account**. \n3. **CD Account**: Number of Personal Loans availed (340) is slightly more for customers with **No CD Account**.\n4. **Online**: Number of Personal Loans availed (291) is slightly more for customers who use internet banking facilities.\n5. **CreditCard**: Number of Personal Loans availed (337) is slightly more for customers with **No Credit Card**.\n6. **Family**: Availing of **Personal Loan** gradually increases with increase in the family size.\n7. **Education**: Availing of **Personal Loan** gradually increases with levels of education i.e. from Undergrads to Professionals.","1adeb5fa":"### Let us try to see if the model can be improved with weights as 'distance' and Euclidean distance"}}