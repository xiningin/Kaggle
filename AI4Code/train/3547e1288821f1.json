{"cell_type":{"c48f9a04":"code","b180a2c7":"code","86f7977d":"code","fceb4f76":"code","aad8cc10":"code","6968e8db":"code","dc30e9c5":"code","04ffec67":"code","5a6ba796":"code","86ddfbb6":"code","e33e081a":"code","9018e33c":"code","7f1ba33c":"code","8bc41081":"code","6535c5a9":"code","09fd0585":"code","ca1dc9dc":"code","0d9f55f7":"code","24017433":"code","2960006c":"code","29e1dad8":"code","0c04f6bf":"code","8e29eef5":"code","a4dac6c4":"code","dcc3c6cc":"markdown"},"source":{"c48f9a04":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b180a2c7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns","86f7977d":"import os\nprint(os.listdir('..\/input'))","fceb4f76":"data_train = pd.read_csv(\"..\/input\/learn-together\/train.csv\")\ndata_test = pd.read_csv(\"..\/input\/learn-together\/test.csv\")","aad8cc10":"features_soil = ['Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n       'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8',\n       'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n       'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16',\n       'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n       'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n       'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n       'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n       'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n       'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']\ndata_train[\"Soil_Count\"] = data_train[features_soil].apply(sum, axis=1)\ndata_train.head()","6968e8db":"features_wilderness = ['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3','Wilderness_Area4']\ndata_train[\"Wilderness_Area\"] = data_train[features_wilderness].apply(sum, axis=1)\ndata_train.Wilderness_Area.describe()","dc30e9c5":"data_train[\"Wilderness_Area\"] = data_train[features_wilderness].apply(np.argmax, axis=1)\ndata_train[\"Wilderness_Area\"] = data_train[\"Wilderness_Area\"].apply(lambda x: x.split(\"Wilderness_Area\")[-1])\ndata_train.Wilderness_Area.head()","04ffec67":"features = ['Elevation', 'Aspect', 'Slope','Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n       'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', \"Cover_Type\"]\nsns.heatmap(data=data_train[features].corr(), annot=True, linecolor=\"w\", fmt=\".1\")\nplt.show()","5a6ba796":"sns.distplot(data_train[\"Hillshade_Noon\"].apply(lambda x: x**4))\nplt.show()","86ddfbb6":"def clear_dataset(dataset):\n    features_soil = ['Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n       'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8',\n       'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n       'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16',\n       'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n       'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n       'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n       'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n       'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n       'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']\n    features_wilderness = ['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3','Wilderness_Area4']\n    dataset[\"Soil_Type\"] = dataset[features_soil].apply(np.argmax, axis=1)\n    dataset[\"Soil_Type\"] = dataset[\"Soil_Type\"].apply(lambda x: x.split(\"Soil_Type\")[-1]).astype(int)\n    dataset = dataset.drop([\"Soil_Type15\", \"Soil_Type7\"], axis=1)\n    dataset[\"Wilderness_Area\"] = dataset[features_wilderness].apply(np.argmax, axis=1)\n    dataset[\"Wilderness_Area\"] = dataset[\"Wilderness_Area\"].apply(lambda x: x.split(\"Wilderness_Area\")[-1]).astype(int)\n    #dataset = dataset.drop(features_wilderness, axis=1)\n    dataset[\"Hillshade_1\"] = (dataset.Hillshade_Noon * dataset.Hillshade_9am)\n    dataset[\"Hillshade_1_sqrt\"] = np.sqrt(dataset[\"Hillshade_1\"])\n    dataset[\"Hillshade_2\"] = (dataset.Hillshade_3pm * dataset.Hillshade_9am)\n    dataset[\"Hillshade_2_sqrt\"] = np.sqrt(dataset[\"Hillshade_2\"])\n    dataset[\"Hillshade_3\"] = (dataset.Hillshade_3pm * dataset.Hillshade_Noon * dataset.Hillshade_9am)\n    dataset[\"Hillshade_3_sqrt\"] = np.sqrt(dataset[\"Hillshade_3\"])\n    dataset.Hillshade_1 = dataset.Hillshade_1.astype(float)\n    dataset[\"DistanceToHydrology\"] = np.sqrt(dataset.Horizontal_Distance_To_Hydrology ** 2 + dataset.Vertical_Distance_To_Hydrology ** 2)\n    dataset[\"Horizontal_Distance_To_Roadways_sqrt\"]= np.sqrt(dataset[\"Horizontal_Distance_To_Roadways\"])\n    dataset[\"Horizontal_Distance_To_Fire_Points_sqrt\"]= np.sqrt(dataset[\"Horizontal_Distance_To_Fire_Points\"])\n    dataset[\"Slope_sqrt\"] = np.sqrt(dataset[\"Slope\"])\n    dataset[\"Hillshade_9am_cube\"] = dataset[\"Hillshade_9am\"].apply(lambda x: x**3)\n    dataset[\"Hillshade_Noon_cube\"] = dataset[\"Hillshade_Noon\"].apply(lambda x: x**3)\n    dataset[\"Aspect_Slope_cbrt\"] = np.cbrt(dataset.Aspect * dataset.Slope)\n    dataset[\"Elevation_Slope_sqrt\"] = np.sqrt(dataset.Elevation * dataset.Slope)\n    dataset[\"Elevation_Aspect_sqrt\"] = np.sqrt(dataset.Elevation * dataset.Aspect)\n    dataset[\"Elevation_sqrt\"] = np.sqrt(dataset.Elevation)\n    return dataset","e33e081a":"data_test = pd.read_csv(\"..\/input\/learn-together\/test.csv\")\nfinal_test = clear_dataset(data_test)","9018e33c":"data_train = pd.read_csv(\"..\/input\/learn-together\/train.csv\")\nfinal_train = clear_dataset(data_train)","7f1ba33c":"x_data = final_train.drop([\"Cover_Type\", \"Id\"], axis=1)\ny_data = final_train[\"Cover_Type\"]","8bc41081":"x_data = final_train.drop([\"Cover_Type\", \"Id\"], axis=1)\ny_data = final_train[\"Cover_Type\"]","6535c5a9":"from sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.25, random_state=42)","09fd0585":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(n_estimators=100, max_depth=19, max_features=11,n_jobs=-1, random_state=42)\nclf.fit(x_train, y_train)\n\ny_predicted = clf.predict(x_val)","ca1dc9dc":"from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n\ndef get_metrics(y_test, y_predicted):  \n    # true positives \/ (true positives+false positives)\n    precision = precision_score(y_test, y_predicted, pos_label=None,\n                                    average='weighted')             \n    # true positives \/ (true positives + false negatives)\n    recall = recall_score(y_test, y_predicted, pos_label=None,\n                              average='weighted')\n    \n    # harmonic mean of precision and recall\n    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n    \n    # true positives + true negatives\/ total\n    accuracy = accuracy_score(y_test, y_predicted)\n    return accuracy, precision, recall, f1","0d9f55f7":"accuracy, precision, recall, f1 = get_metrics(y_val, y_predicted)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","24017433":"def plot_feature_importances(clf, X_train, y_train=None, \n                             top_n=10, figsize=(8,8), print_table=False, title=\"Feature Importances\"):\n    '''\n    plot feature importances of a tree-based sklearn estimator\n    \n    Note: X_train and y_train are pandas DataFrames\n    \n    Note: Scikit-plot is a lovely package but I sometimes have issues\n              1. flexibility\/extendibility\n              2. complicated models\/datasets\n          But for many situations Scikit-plot is the way to go\n          see https:\/\/scikit-plot.readthedocs.io\/en\/latest\/Quickstart.html\n    \n    Parameters\n    ----------\n        clf         (sklearn estimator) if not fitted, this routine will fit it\n        \n        X_train     (pandas DataFrame)\n        \n        y_train     (pandas DataFrame)  optional\n                                        required only if clf has not already been fitted \n        \n        top_n       (int)               Plot the top_n most-important features\n                                        Default: 10\n                                        \n        figsize     ((int,int))         The physical size of the plot\n                                        Default: (8,8)\n        \n        print_table (boolean)           If True, print out the table of feature importances\n                                        Default: False\n        \n    Returns\n    -------\n        the pandas dataframe with the features and their importance\n        \n    Author\n    ------\n        George Fisher\n    '''\n    \n    __name__ = \"plot_feature_importances\"\n    \n    import pandas as pd\n    import numpy  as np\n    import matplotlib.pyplot as plt\n    \n    from xgboost.core     import XGBoostError\n    from lightgbm.sklearn import LightGBMError\n    \n    try: \n        if not hasattr(clf, 'feature_importances_'):\n            clf.fit(X_train.values, y_train.values.ravel())\n\n            if not hasattr(clf, 'feature_importances_'):\n                raise AttributeError(\"{} does not have feature_importances_ attribute\".\n                                    format(clf.__class__.__name__))\n                \n    except (XGBoostError, LightGBMError, ValueError):\n        clf.fit(X_train.values, y_train.values.ravel())\n            \n    feat_imp = pd.DataFrame({'importance':clf.feature_importances_})    \n    feat_imp['feature'] = X_train.columns\n    feat_imp.sort_values(by='importance', ascending=False, inplace=True)\n    feat_imp = feat_imp.iloc[:top_n]\n    \n    feat_imp.sort_values(by='importance', inplace=True)\n    feat_imp = feat_imp.set_index('feature', drop=True)\n    feat_imp.plot.barh(title=title, figsize=figsize)\n    plt.xlabel('Feature Importance Score')\n    plt.show()\n    \n    if print_table:\n        from IPython.display import display\n        print(\"Top {} features in descending order of importance\".format(top_n))\n        display(feat_imp.sort_values(by='importance', ascending=False))\n        \n    return feat_imp","2960006c":"a = plot_feature_importances(clf, x_train, y_train, top_n=x_train.shape[1], title=clf.__class__.__name__)","29e1dad8":"data_test = pd.read_csv(\"..\/input\/learn-together\/test.csv\")\nfinal_test = clear_dataset(data_test)","0c04f6bf":"clf = RandomForestClassifier(n_estimators=100,max_depth=11, max_features=21,min_samples_leaf=0.001,criterion=\"entropy\",n_jobs=-1, random_state=42)\nclf.fit(x_train, y_train)\ny_predicted = clf.predict(x_val)\naccuracy, precision, recall, f1 = get_metrics(y_val, y_predicted)\nprint(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","8e29eef5":"y_predicted = clf.predict(x_train)\naccuracy, precision, recall, f1 = get_metrics(y_train, y_predicted)\nprint(\"train accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))","a4dac6c4":"clf = RandomForestClassifier(n_estimators=100,max_depth=11, max_features=21,min_samples_leaf=0.001,criterion=\"entropy\",n_jobs=-1, random_state=42)\nclf.fit(x_data, y_data)\ntest_preds = clf.predict(final_test.drop([\"Id\"], axis=1))\noutput = pd.DataFrame({'Id': data_test.Id,\n                       'Cover_Type': test_preds})\noutput.to_csv('rf_submission.csv', index=False)","dcc3c6cc":"Thank's \"tooeasy\" user for having published your kernel in that same 2015 competition! And also Sharmasanthosh. He published two awesomes inspiring kernels.Thank's to my team. Fellows, publish your kernels and share your knowledge. That's our goal, that's our legacy . Learn Together 2019.  "}}