{"cell_type":{"c5c9efb1":"code","43ff0ffe":"code","22ac961a":"code","137643bf":"code","03ec3948":"code","f53abd15":"code","8b063f0f":"code","85687eb5":"code","336d695e":"code","7b269b7d":"code","31bff348":"code","d63c3616":"code","81b7488a":"code","77d6e9f7":"code","60bb60bd":"code","6cd68d4f":"code","331b8f8d":"code","5d65159c":"code","7e45c50f":"code","cf1a41bf":"code","fd72ca01":"code","7a739009":"code","5a2b9e86":"code","56fb79f4":"code","82aa631b":"code","234499db":"code","fabd6782":"code","201883db":"code","195b4117":"code","bf26d35f":"code","b6e6ce3b":"code","ecb535c2":"code","873c868a":"code","6e723458":"code","e8a7b739":"code","3ebe0dd6":"code","9b2e7806":"code","b7439f48":"code","aa58b0dd":"code","03422257":"code","a11e30ed":"code","6addaa98":"code","7bd43481":"code","1830cd4e":"code","2991c3cc":"code","0192afc0":"code","8b6e69b7":"code","2fd63ee4":"code","4dbf83ea":"code","c9bb2717":"code","a6a496fa":"code","332d4178":"code","ba0cfe36":"code","4ae03ed0":"code","081d74da":"code","dca3c358":"code","d7425ced":"code","19610f97":"code","d1466a39":"code","ed86734f":"code","2569b862":"code","2bd540e0":"code","094a7fcd":"code","8ef1ca03":"code","f793f99b":"code","d89bcb15":"code","4e326c05":"code","4db1dccc":"code","24f9a085":"code","7a24dcd1":"code","80fb32d1":"code","b7cda2c4":"code","d3539fc3":"markdown","0084ac9c":"markdown","7ce1554c":"markdown","dc17e8d6":"markdown","23338fa6":"markdown","0e7de2f0":"markdown","5ef89013":"markdown","08504aab":"markdown","ec2164d7":"markdown","50fb5b38":"markdown","ee683cf2":"markdown","10a08a19":"markdown","a7422b60":"markdown","da739d48":"markdown","a3cf06b2":"markdown","8841a333":"markdown","2ede8374":"markdown","2faf2420":"markdown","2e7bc5c0":"markdown","85158082":"markdown","43530a7e":"markdown","d8ae503f":"markdown","f117ebc8":"markdown"},"source":{"c5c9efb1":"%load_ext autoreload\n%autoreload 2","43ff0ffe":"# Linear algebra library\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (8, 5) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'","22ac961a":"def checkConvergence(X, w, y):\n    m = X.shape[0]\n    sum_node = X @ w\n    # step function\n    activation_node = sign_function(sum_node)\n    # check convergence\n    error = error_function(y, activation_node, X, w)\n    if error < 0.001:\n        return True, error\n    else:\n        return False, error\n\ndef error_function(y, f, X, w):\n    incorrect = (y != f).astype(int)\n    indx = get_indx_incorrect(incorrect)\n    error = - np.sum(X[indx]@w * y[indx])\n    return error\n\ndef get_indx_incorrect(incorrect):\n    indx = []\n    for i in range(len(incorrect)):\n        if incorrect[i] == 1:\n            indx.append(i)\n    return indx\n    \ndef sign_function(z):\n    return (z>0).astype(int) - (z<=0).astype(int)\n\ndef predict(X, w):\n    sum_node = X @ w\n    activation_node = sign_function(sum_node)\n    return activation_node","137643bf":"from tqdm.autonotebook import tqdm\nimport time\nconvergence = False\ncont = 0\n# data\nX = np.array([[1,1],\n              [-1,1],\n              [1,-1],\n              [-1,-1]])\n\ny = np.array([1,-1,-1,-1])\n# add bias term\nX = np.hstack((np.ones(4).reshape(-1,1),X))\n# random initialization of weigths\nw = np.random.rand(3)\nm = X.shape[0]\n\nP = [] # positive set\nN = [] # negative set\nfor i in range(len(y)):\n    if y[i] == 1:\n        P.append(i)\n    else:\n        N.append(i)\n        \nwith tqdm() as pbar:\n    while convergence != True:\n        indx = np.random.choice(4)\n        x = X[indx]\n        l = y[indx]\n        if indx in P and int(x.dot(w))<=0:\n            w = w + x * l\n        if indx in N and int(x.dot(w))>=0:\n            w = w + x * l\n        convergence, dif = checkConvergence(X, w, y)\n        pbar.set_description(f'dif {dif}')\n        cont += 1\n        if cont > 500:\n            print('Stuck Method')\n            break\n        pbar.update(1)","03ec3948":"f = predict(X,w)\nf","f53abd15":"from mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d.proj3d import proj_transform\nfrom matplotlib.text import Annotation\n\nX = np.array([[1,1],\n              [-1,1],\n              [1,-1],\n              [-1,-1]])\n\ny = np.array([1,-1,-1,-1])\n\n# w = np.array([-0.52558756,  1.4177699 ,  1.17914055])\n\nfig = plt.figure(figsize=(15,7))\nax0 = fig.add_subplot(1,2,1)\nax0.set_title('real')\ncross, = ax0.plot([],[],'rx')\ngood, = ax0.plot([],[],'go')\nfor i in range(len(y)):\n    if y[i] == -1:\n        ax0.plot(X[i,0], X[i,1], 'rx')\n    else:\n        ax0.plot(X[i,0],X[i,1], 'go')\nax0.legend(handles = [cross, good], labels = ['False', 'True'],bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\nx_var = np.linspace(-1.5,1.5,10)\ny_var = -x_var * (w[1]\/w[2]) - w[0]\/w[2]\nax0.set_ylim(-1.5,1.5)\nax0.set_xlim(-1.5,1.5)\nax0.plot(x_var, y_var, 'm-')\nplt.tight_layout()\n\nax = fig.add_subplot(1,2,2, projection = '3d')\nax.set_title('3D hyperplane decision boundary')\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\nax.set_zlim(-1.5,1.5)\n\nx1 = np.linspace(-1.5,1.5,101)\ny1 = np.linspace(-1.5,1.5,101)\n\nX1,Y1 = np.meshgrid(x1, y1)\n\nZ = -X1 * (w[1]\/w[2]) - w[0]\/w[2] - Y1\n\nnorm = plt.Normalize(Z.min(), Z.max())\ncolors = cm.viridis(norm(Z))\nrcount, ccount, _ = colors.shape\nsurf = ax.plot_surface(X1, Y1, Z,rcount=20, ccount=20,\n                    facecolors=colors, shade=False )\nsurf.set_facecolor((0,0,0,0))\ncross, = ax.plot([],[],'rx')\ngood, = ax.plot([],[],'go')\nfor i in range(len(y)):\n    if y[i] == -1:\n        ax.scatter(X[i,0], X[i,1], 0, marker ='x', c='red')\n    else:\n        ax.scatter(X[i,0],X[i,1], 0, marker='o', c='green')\nax.legend(handles = [cross, good], \n           labels = ['False', 'True'],bbox_to_anchor=(1.05, 1),\n           loc='upper left', borderaxespad=0)\nplt.tight_layout()\nplt.show()\n","8b063f0f":"X = np.array([[1,1,1],\n              [1,0,1],\n              [1,1,0],\n              [1,0,0]])\n\ny = np.array([0,1,1,0])\n\nw1 = np.array([[-0.5, 1.5],\n               [1, -1],\n               [1, -1]])\n\nw2 = np.array([-1.5, 1, 1])\n\nP = []\nN = []\nfor i in range(len(y)):\n    if y[i] == 1:\n        P.append(i)\n    else:\n        N.append(i)\n        \ndef checkConvergence(X, w1, w2, y):\n    fcl1 = np.hstack((np.ones((4,1)),X.dot(w1)))\n    fcl2 = fcl1.dot(w2)\n    d = ((fcl2) > 0).astype(int)\n    if np.linalg.norm(d-y) <= 0.1:\n        return True, 0\n    else:\n        return False, np.linalg.norm(d-y)\n    \ndef predict(X,w1, w2):\n    fcl1 = np.hstack((np.ones((4,1)),X.dot(w1)))\n#     print(fcl1)\n    fcl1 = step(fcl1)\n#     print(fcl1)\n    fcl2 = fcl1.dot(w2)\n#     print(fcl2)\n    d = ((fcl2) > 0).astype(int)\n    return d\n\ndef step(z):\n    h = (z > 0).astype(int) - (z <= 0).astype(int)\n    return h","85687eb5":"y_pred = predict(X, w1, w2)\ny_pred","336d695e":"from mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d.proj3d import proj_transform\nfrom matplotlib.text import Annotation\n\nX = np.array([[1,1],\n              [0,1],\n              [1,0],\n              [0,0]])\n\ny = np.array([0,1,1,0])\nfig = plt.figure(figsize=(15,7))\nax0 = fig.add_subplot(1,2,1)\nax0.set_title('real')\nax0.set_xlabel('x')\nax0.set_ylabel('y')\nax0.set_xlim(-0.5,1.5)\nax0.set_ylim(-0.5,1.5)\ncross, = ax0.plot([],[],'rx')\ngood, = ax0.plot([],[],'go')\nfor i in range(len(y)):\n    if y[i] == 0:\n        ax0.plot(X[i,0], X[i,1], 'rx')\n    else:\n        ax0.plot(X[i,0],X[i,1], 'go')\nax0.legend(handles = [cross, good], \n           labels = ['False', 'True'],bbox_to_anchor=(1.05, 1),\n           loc='upper left', borderaxespad=0)\nplt.tight_layout()\n\nax = fig.add_subplot(1,2,2, projection = '3d')\nax.set_title('3D HyperPlane decision boundary')\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\nax.set_zlim(-1.5,1.5)\n\nx1 = np.linspace(-0.5,1.5,101)\ny1 = np.linspace(-0.5,1.5,101)\nw1 = np.array([[-0.5, 1.5],\n               [1, -1],\n               [1, -1]])\n\nw2 = np.array([-1.5, 1, 1])\n\nX1,Y1 = np.meshgrid(x1, y1)\n\nZ = ((w1[1,0]*X1+w1[2,0]*Y1+w1[0,0]) > 0).astype(int) *w2[1] + ((w1[1,1]*X1+w1[2,1]*Y1+w1[0,1]) > 0).astype(int)*w2[2] + w2[0]\n\nnorm = plt.Normalize(Z.min(), Z.max())\ncolors = cm.viridis(norm(Z))\nrcount, ccount, _ = colors.shape\nsurf = ax.plot_surface(X1, Y1, Z,rcount=20, ccount=20,\n                    facecolors=colors, shade=False )\nsurf.set_facecolor((0,0,0,0))\ncross, = ax.plot([],[],'rx')\ngood, = ax.plot([],[],'go')\nfor i in range(len(y)):\n    if y[i] == 0:\n        ax.scatter(X[i,0], X[i,1], 0, marker ='x', c='red')\n    else:\n        ax.scatter(X[i,0],X[i,1], 0, marker='o', c='green')\nax.legend(handles = [cross, good], \n           labels = ['False', 'True'],bbox_to_anchor=(1.05, 1),\n           loc='upper left', borderaxespad=0)\nplt.tight_layout()\nplt.show()","7b269b7d":"from mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d.proj3d import proj_transform\nfrom matplotlib.text import Annotation\n\ndef f(x):\n    return x**2\n\ndef df(x):\n    return 2*x\n\nx = np.linspace(-10, 10, 101)\ny = lambda x: f(x)\n\nfig = plt.figure(figsize=(5,5))\nplt.plot(x, y(x))\nplt.title('2D gradient descent')\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\n\nx_init = -10\nX = [x_init]\nY = [y(x_init)]\n# Begin Gradient descent with N steps and a learning reate\nN = 10\nalpha = 0.3\n\nfor j in range(N):\n    last_X = X[-1]\n    this_X = last_X - alpha * df(last_X)\n    X.append(this_X)\n    Y.append(y(this_X))\n\n\ntheta = []\nfor j in range(len(X)):\n    val_X = X[j]\n    val_Y = Y[j]\n    theta.append(np.array((val_X, val_Y)))\n\nfor j in range(N):\n    if j>0:\n        plt.annotate('',xy=theta[j], xytext=theta[j-1],\n                    arrowprops={'arrowstyle': '->', 'color': 'r', 'lw': 1},\n                    va='center', ha='center')\n    plt.scatter(X[j], Y[j], s=40, lw=0)\n\n\nplt.show()","31bff348":"from mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d.proj3d import proj_transform\nfrom matplotlib.text import Annotation\n\ndef f(x,y):\n    return x**2 + y**2\n\ndef dfdy(x,y):\n    return 2*y\n\ndef dfdx(x,y):\n    return 2*x\n\nfig =  plt.figure(figsize=(10,5))\nax0 =  fig.add_subplot(1,2,1)\nax0.set_title('contour plot')\nax0.set_xlabel('x')\nax0.set_ylabel('y')\n\nax = fig.add_subplot(1,2,2, projection = '3d')\nax.set_title('3D gradient descent plot')\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\n\nx = np.linspace(-10,10,101)\ny = np.linspace(-10,10,101)\n\nX,Y = np.meshgrid(x, y)\nZ = f(X,Y)\nnorm = plt.Normalize(Z.min(), Z.max())\ncolors = cm.viridis(norm(Z))\nrcount, ccount, _ = colors.shape\nsurf = ax.plot_surface(X, Y, Z,rcount=20, ccount=20,\n                    facecolors=colors, shade=False )\nsurf.set_facecolor((0,0,0,0))\ncontours = ax0.contour(X, Y, Z, 30)\nax0.clabel(contours)\n\n# Take N steps with learning rate alpha down the steepest gradient, starting at (0,0)\nN = 10\nalpha = 0.2\np_init = (-5,10)\nP = [np.array(p_init)]\nF = [f(*P[0])]\n\nfor i in range(N):\n    last_P = P[-1]\n    new_P = np.zeros(2)\n    new_P[0] = last_P[0] - alpha*dfdx(last_P[0], last_P[1])\n    new_P[1] = last_P[1] - alpha*dfdy(last_P[0], last_P[1])\n    P.append(new_P)\n    F.append(f(new_P[0], new_P[1]))\n\ncolors = cm.rainbow(np.linspace(0, 1, N+1))\n\n\nfor j in range(0,N):\n    if j>0:\n        ax0.annotate('',xy=P[j], xytext=P[j-1],\n                    arrowprops={'arrowstyle': '->', 'color': 'r', 'lw': 1},\n                    va='center', ha='center')\n    ax0.scatter(P[j][0], P[j][1], s=40, lw=0)\n    ax.scatter(P[j][0], P[j][1], F[j], s=40, lw = 0)\n\nplt.show()","d63c3616":"from mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d.proj3d import proj_transform\nfrom matplotlib.text import Annotation\n\n# The data to fit\nm = 20\ntheta0_true = 2\ntheta1_true = 0.5\nx = np.linspace(-1,1,m)\ny = theta0_true + theta1_true * x\n\n# Figure\nfig =  plt.figure(figsize=(14,5))\n\n# First plot\nax = fig.add_subplot(1,3,1)\n\n# Scatter plot on the real data\nax.scatter(x, y, marker = 'x', s = 40, color = 'k')\n\ndef cost_func(theta0, theta1):\n    \"\"\"The cost function, J(theta0, theta1) describing the goodness of fit.\"\"\"\n    theta0 = np.atleast_3d(np.asarray(theta0))\n    theta1 = np.atleast_3d(np.asarray(theta1))\n    return np.average((y-hypothesis(x, theta0, theta1))**2, axis=2)\/2\n\ndef hypothesis(x, theta0, theta1):\n    \"\"\"Our \"hypothesis function\", a straight line.\"\"\"\n    return theta0 + theta1*x\n\n# First construct a grid of (theta0, theta1) parameter pairs and their\n# corresponding cost function values.\ntheta0_grid = np.linspace(-1,4,101)\ntheta1_grid = np.linspace(-5,5,101)\nJ_grid = cost_func(theta0_grid[np.newaxis,:,np.newaxis],\n                theta1_grid[:,np.newaxis,np.newaxis])\n\n# Setup for 3d plot or surface plot\nX, Y = np.meshgrid(theta0_grid, theta1_grid)\n\n# Contour plot\nax1 = fig.add_subplot(1,3,2)\nax1.set_title('Contour plot')\ncontours = ax1.contour(X, Y, J_grid, 30)\nax1.clabel(contours)\n# The target parameter values indicated on the cost function contour plot\nax1.scatter([theta0_true]*2,[theta1_true]*2,s=[50,10], color=['k','w'])\n\n# Surface Plot\nax2 = fig.add_subplot(1,3,3, projection = '3d')\n# ax2.plot_wireframe(X, Y, J_grid, rstride=10, cstride=10,\n#                     linewidth=1, antialiased=False, cmap=cm.coolwarm)\nnorm = plt.Normalize(J_grid.min(), J_grid.max())\ncolors = cm.viridis(norm(J_grid))\nrcount, ccount, _ = colors.shape\n\nsurf = ax2.plot_surface(X, Y, J_grid, rcount=10, ccount=10,\n                    facecolors=colors, shade=False)\nsurf.set_facecolor((0,0,0,0))\n# plot the real point\nax2.scatter([theta0_true]*2,[theta1_true]*2, cost_func(theta0_true, theta1_true), s=50 ,color=['r'])\n\n# Take N steps with learning rate alpha down the steepest gradient, starting at (0,0)\nN = 10\nalpha = 0.5\n\n# Initial theta\ntheta = [np.array((0,0))]\nJ = [cost_func(*theta[0])[0]]\nfor j in range(N):\n    last_theta = theta[-1]\n    this_theta = np.empty((2,))\n    this_theta[0] = last_theta[0] - alpha \/ m * np.sum(\n                                    (hypothesis(x, *last_theta) - y))\n    this_theta[1] = last_theta[1] - alpha \/ m * np.sum(\n                                    (hypothesis(x, *last_theta) - y) * x)\n    theta.append(this_theta)\n    J.append(cost_func(*this_theta))\n\ncolors = cm.rainbow(np.linspace(0, 1, N+1))\nax.plot(x, hypothesis(x, *theta[0]), color = colors[0], lw = 2, label=r'$\\theta_0 = {:.3f}, \\theta_1 = {:.3f}$'.format(*theta[j]))\n\nfor j in range(0,N):\n    if j>0:\n        ax1.annotate('',xy=theta[j], xytext=theta[j-1],\n                    arrowprops={'arrowstyle': '->', 'color': 'r', 'lw': 1},\n                    va='center', ha='center')\n    ax.plot(x, hypothesis(x, *theta[j]), color=colors[j], lw=2,\n            label=r'$\\theta_0 = {:.3f}, \\theta_1 = {:.3f}$'.format(*theta[j]))\n    ax1.scatter(theta[j][0], theta[j][1], s=40, lw=0)\n    ax2.scatter(theta[j][0], theta[j][1], J[j], s=40, lw = 0)\n\n# Labels, titles and a legend.\nax1.set_xlabel(r'$\\theta_0$')\nax1.set_ylabel(r'$\\theta_1$')\nax1.set_title('Cost function')\nax2.set_xlabel(r'$\\theta_0$')\nax2.set_ylabel(r'$\\theta_1$')\nax2.set_zlabel(r'cost')\nax2.set_title('3D plot')\nax.set_xlabel(r'$x$')\nax.set_ylabel(r'$y$')\nax.set_title('Data and fit')\naxbox = ax.get_position()\n# Position the legend by hand so that it doesn't cover up any of the lines.\nax.legend(loc=(axbox.x0+0.5*axbox.width, axbox.y0+0.1*axbox.height),\n            fontsize='small')\nplt.tight_layout()\nplt.show()","81b7488a":"# My neural net model\nfrom neural_net import neural_net\nfrom vis_utils import plotStats, plotData\nfrom gradient_utils import gradient_difference","77d6e9f7":"X = np.array([[1,1],\n              [0,1],\n              [1,0],\n              [0,0]])\n\ny = np.array([1,0,0,1])\n\nnet_params = {'il':2,'hl1':2,'ol':1}\nactiv_function = 'Sigmoid'\ncost_function = 'Entropy_Loss'\nnet = neural_net(net_params, activation_function= activ_function,\n                 cost_function= cost_function, std= 1)\nresults = net.train_sgd_momentum(X, y, X, y, learning_rate=2.1, \n                                 learning_rate_decay=1, reg = 0.0,\n                                max_iter = 400, batch_size = 2, verbose =  True, rho=0.7,\n                                 stochastic = False, epoch = 20)\nplotStats(results)\n\ny_pred = net.predict(X)\nplotData(X, y, y_pred)","60bb60bd":"from mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d.proj3d import proj_transform\nfrom matplotlib.text import Annotation\n\nX = np.array([[1,1],\n              [0,1],\n              [1,0],\n              [0,0]])\n\ny = np.array([1,0,0,1])\n\n# params = {'W1': np.array([[ 6.03878347, -6.70829504],\n#         [ 6.03877163, -6.70825261]]),\n#  'b1': np.array([ 9.33092666, -2.72624356]),\n#  'W2': np.array([[11.97407601],\n#         [12.07598502]]),\n#  'b2': np.array([5.8718201])}\n\nparams = net.function_params\n\ndef sigmoid( z):\n    \"\"\"\n    Return the sigmoid function evaluated at z\n    \"\"\"\n    g = 1\/(1 + np.exp(-z))\n    return g\n\nw = params\nx1 = np.linspace(-1.5,1.5,101)\ny1 = np.linspace(-1.5,1.5,101)\n\nw1 = w['W1']\nb1 = w['b1']\nw2 = w['W2']\nb2 = w['b2']\n\n\nX1,Y1 = np.meshgrid(x1, y1)\n\nZ = sigmoid(sigmoid(X1*w1[0,0]+ Y1*w1[1,0] - b1[0])*w2[0,0] + sigmoid(X1*w1[0,1]+ Y1*w1[1,1] - b1[1])*w2[1,0] - b2[0])\n\n\nfig = plt.figure(figsize=(10,7))\nax = fig.add_subplot(1,1,1, projection = '3d')\nax.set_title('3D HyperPlane decision boundary')\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\n\n\nnorm = plt.Normalize(Z.min(), Z.max())\ncolors = cm.viridis(norm(Z))\nrcount, ccount, _ = colors.shape\nsurf = ax.plot_surface(X1, Y1, Z,rcount=20, ccount=20,\n                    facecolors=colors, shade=False )\nsurf.set_facecolor((0,0,0,0))\ncross, = ax.plot([],[],'rx')\ngood, = ax.plot([],[],'go')\nfor i in range(len(y)):\n    if y[i] == 0:\n        ax.scatter(X[i,0], X[i,1], 0, marker ='x', c='red')\n    else:\n        ax.scatter(X[i,0],X[i,1], 0, marker='o', c='green')\nax.legend(handles = [cross, good], \n           labels = ['False', 'True'],bbox_to_anchor=(1.05, 1),\n           loc='upper left', borderaxespad=0)\nplt.tight_layout()\nplt.show()","6cd68d4f":"# My neural net model\nfrom neural_net import neural_net\nfrom vis_utils import plotStats, plotData\nfrom gradient_utils import gradient_difference\n# from tuning_utils import tuning_hyper_parameter\nfrom vis_utils import visualize_grid_withoutRGB, visualize_grid\n\n# Linear algebra library\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (8, 5) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\n%load_ext autoreload\n%autoreload 2","331b8f8d":"from vis_utils import visualize_grid_withoutRGB, visualize_grid\ndef show_net_weights(net, Wkey):\n    plt.figure(figsize=(7,7))\n    W1 = net.function_params[Wkey]\n    Z, N = W1.shape\n    size = int(np.sqrt(Z))\n    W1 = W1.reshape(size, size,-1).transpose(2,0,1)\n    print(W1.shape)\n    plt.imshow(visualize_grid_withoutRGB(W1, padding=3).astype('uint8'))\n    plt.gca().axis('off')\n    plt.show()","5d65159c":"def plot_image(i, predictions_array, true_label, img):\n    predictions_array, true_label, img = predictions_array, true_label[i], img[i]\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])\n\n    plt.imshow(img, cmap=plt.cm.binary)\n\n    predicted_label = np.argmax(predictions_array)\n    if predicted_label == true_label:\n        color = 'blue'\n    else:\n        color = 'red'\n\n    plt.xlabel(\"{} {:2.0f}% ({})\".format(predicted_label,100*np.max(predictions_array),true_label),color=color)\n\ndef plot_value_array(i, predictions_array, true_label):\n    predictions_array, true_label = predictions_array, true_label[i]\n    plt.grid(False)\n    plt.xticks(range(10))\n    plt.yticks([])\n    thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n    plt.ylim([0, 1])\n    predicted_label = np.argmax(predictions_array)\n\n    thisplot[predicted_label].set_color('red')\n    thisplot[true_label].set_color('blue')","7e45c50f":"# Loading digits dataset\nfrom scipy.io import loadmat\nfrom mlxtend.data import loadlocal_mnist\nseminario_path = '\/kaggle\/input\/seminariodataset\/'\ndigits_image_path = seminario_path+'data\/mnist-dataset\/train_images'\ndigits_labels_path = seminario_path+'data\/mnist-dataset\/train_labels'\ntest_image_path = seminario_path+'data\/mnist-dataset\/test_images'\ntest_labels_path = seminario_path+'data\/mnist-dataset\/test_labels'\ntrain_images, train_labels = loadlocal_mnist(images_path= digits_image_path, \n                                 labels_path = digits_labels_path)\ntest_images, test_labels = loadlocal_mnist(images_path = test_image_path, \n                                           labels_path = test_labels_path)","cf1a41bf":"# shape of the images\nprint(f'shape of train_images dataset : {train_images.shape}')\nprint(f'shape of train_labels dataset : {train_labels.shape} ')\nprint(f'shape of test_images dataset : {test_images.shape}')\nprint(f'shape of test_labels dataset : {test_labels.shape} ')","fd72ca01":"num_samples, pixels_square = train_images.shape\npixel_size = int(np.sqrt(pixels_square))\ntrain_images = train_images.reshape(num_samples, pixel_size, pixel_size)\nprint(train_images.shape)\ntest_images = test_images.reshape(test_images.shape[0], 28, 28)\nprint(test_images.shape)","7a739009":"plt.figure(figsize=(10,10))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(train_images[i], cmap=plt.cm.binary)\n    plt.xlabel(train_labels[i])\nplt.show()","5a2b9e86":"train_images = train_images.reshape(num_samples, pixel_size, pixel_size)\nprint(train_images.shape)\ntest_images = test_images.reshape(test_images.shape[0], 28, 28)\nprint(test_images.shape)\nmean_image = np.mean(train_images, axis = 0) \n#Normalize the data is very important for fastest converge\n#X = train_images - mean_image \n#X_val = test_images - mean_image\nX = train_images\nX_val = test_images\n# X = train_images\n# X_val = test_images\nX_val = X_val.reshape(X_val.shape[0], -1)\nX = X.reshape(num_samples, -1)","56fb79f4":"y = train_labels\ny_val = test_labels","82aa631b":"reg = 0\nnet_params = {'il': 784, 'hl1': 25, 'ol': 10}\nactiv_function = 'Sigmoid'\ncost_function = 'Entropy_Loss'\nlr = 0.09\n# activ_function = 'ReLu'\n# cost_function = 'SoftMax'\n#lr = 0.0005\n# max_iter = 10000\nmax_iter = 2000\nnet = neural_net(net_params=net_params, activation_function=activ_function,\n                cost_function=cost_function, std = 1)\n\nresults = net.train_stochastic_gradient_descent(X, y, X_val, y_val,\n                                                learning_rate = lr, reg = reg,\n                                               max_iter = max_iter, batch_size = 100,\n                                               verbose = True, epoch = 200)","234499db":"plotStats(results)\ny_pred = net.predict(X)\nprint(f'{(y_pred == y).mean() * 100}')\ny_pred_val, y_score_val = net.predict(X_val, with_score=True)\nprint(f'{(y_pred_val == y_val).mean() * 100}')\nshow_net_weights(net, 'W1')","fabd6782":"reg = 0\nnet_params = {'il': 784, 'hl1': 25, 'ol': 10}\n# activ_function = 'Sigmoid'\n# cost_function = 'Entropy_Loss'\n# lr = 0.5\nactiv_function = 'ReLu'\ncost_function = 'SoftMax'\nlr = 0.0005\nmax_iter = 2000\nnet = neural_net(net_params=net_params, activation_function=activ_function,\n                cost_function=cost_function, std = 1e-4)\nresults = net.train_stochastic_gradient_descent(X, y, X_val, y_val,\n                                                learning_rate = lr, reg = reg,\n                                               max_iter = max_iter, batch_size = 100,\n                                               verbose = True, epoch = 200)","201883db":"plotStats(results)\ny_pred = net.predict(X)\nprint(f'{(y_pred == y).mean() * 100}')\ny_pred_val, y_score_val = net.predict(X_val, with_score=True)\nprint(f'{(y_pred_val == y_val).mean() * 100}')\nshow_net_weights(net, 'W1')","195b4117":"num_rows = 26\nnum_cols = 3\nnum_images = num_rows*num_cols\nplt.figure(figsize=(2*2*num_cols, 2*num_rows))\nfor i in range(num_images):\n    plt.subplot(num_rows, 2*num_cols, 2*i+1)\n    plot_image(i, y_score_val[i], y_val, X_val.reshape(len(y_val), 28, 28))\n    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n    plot_value_array(i, y_score_val[i], y_val)\nplt.tight_layout()\nplt.show()","bf26d35f":"reg = 0\nnet_params = {'il': 784, 'hl1': 225, 'ol': 10}\nactiv_function = 'Sigmoid'\ncost_function = 'Entropy_Loss'\nlr = 0.09\n# activ_function = 'ReLu'\n# cost_function = 'SoftMax'\n# lr = 0.0005\nmax_iter = 2000\nnet = neural_net(net_params=net_params, activation_function=activ_function,\n                cost_function=cost_function, std = 1)\nresults = net.train_stochastic_gradient_descent(X, y, X_val, y_val,\n                                                learning_rate = lr, reg = reg,\n                                               max_iter = max_iter, batch_size = 100,\n                                               verbose = True, epoch = 200)","b6e6ce3b":"plotStats(results)\ny_pred = net.predict(X)\nprint(f'{(y_pred == y).mean() * 100}')\ny_pred_val, y_score_val = net.predict(X_val, with_score=True)\nprint(f'{(y_pred_val == y_val).mean() * 100}')\nshow_net_weights(net, 'W1')","ecb535c2":"reg = 0\nnet_params = {'il': 784, 'hl1': 225, 'ol': 10}\n#activ_function = 'Sigmoid'\n#cost_function = 'Entropy_Loss'\n#lr = 0.5\nactiv_function = 'ReLu'\ncost_function = 'SoftMax'\nlr = 0.0005\nmax_iter = 2000\nnet = neural_net(net_params=net_params, activation_function=activ_function,\n                cost_function=cost_function, std = 1e-4)\nresults = net.train_stochastic_gradient_descent(X, y, X_val, y_val,\n                                                learning_rate = lr, reg = reg,\n                                               max_iter = max_iter, batch_size = 100,\n                                               verbose = True, epoch = 200)","873c868a":"plotStats(results)\ny_pred = net.predict(X)\nprint(f'{(y_pred == y).mean() * 100}')\ny_pred_val, y_score_val = net.predict(X_val, with_score=True)\nprint(f'{(y_pred_val == y_val).mean() * 100}')\nshow_net_weights(net, 'W1')","6e723458":"num_rows = 26\nnum_cols = 3\nnum_images = num_rows*num_cols\nplt.figure(figsize=(2*2*num_cols, 2*num_rows))\nfor i in range(num_images):\n    plt.subplot(num_rows, 2*num_cols, 2*i+1)\n    plot_image(i, y_score_val[i], y_val, X_val.reshape(len(y_val), 28, 28))\n    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n    plot_value_array(i, y_score_val[i], y_val)\nplt.tight_layout()\nplt.show()","e8a7b739":"# My neural net model\nfrom neural_net import neural_net\nfrom vis_utils import plotStats, plotData\nfrom gradient_utils import gradient_difference\n# from tuning_utils import tuning_hyper_parameter\nfrom vis_utils import visualize_grid_withoutRGB, visualize_grid\n\n# Linear algebra library\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (8, 5) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\n%load_ext autoreload\n%autoreload 2","3ebe0dd6":"from vis_utils import visualize_grid\n\n# Visualize the weights of the network\n\ndef show_net_weights(net):\n    plt.rcParams['figure.figsize'] = (8, 8) # set default size of plots\n    W1 = net.function_params['W1']\n    W1 = W1.reshape(32, 32, 3, -1).transpose(3, 0, 1, 2)\n    plt.imshow(visualize_grid(W1, padding=3).astype('uint8'))\n    plt.gca().axis('off')\n    plt.show()\n    \n    \ndef plot_image(i, predictions_array, true_label, img):\n    classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', \n               'frog', 'horse', 'ship', 'truck']\n    predictions_array, true_label, img = predictions_array, true_label[i], img[i]\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])\n\n    plt.imshow(img.astype('uint8'))\n\n    predicted_label = np.argmax(predictions_array)\n    if predicted_label == true_label:\n        color = 'blue'\n    else:\n        color = 'red'\n\n    plt.xlabel(\"{} {:2.0f}% ({})\".format(classes[predicted_label],\n                                         100*np.max(predictions_array),true_label),\n                                           color=color)\n\ndef plot_value_array(i, predictions_array, true_label):\n    predictions_array, true_label = predictions_array, true_label[i]\n    plt.grid(False)\n    plt.xticks(range(10))\n    plt.yticks([])\n    thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n    plt.ylim([0, 1])\n    predicted_label = np.argmax(predictions_array)\n\n    thisplot[predicted_label].set_color('red')\n    thisplot[true_label].set_color('blue')","9b2e7806":"from features import color_histogram_hsv, hog_feature\nfrom data_utils import load_CIFAR10\n\ndef get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n    seminario_path = '\/kaggle\/input\/seminariodataset\/'\n    # Load the raw CIFAR-10 data\n    cifar10_dir = seminario_path+'data\/cifar-10-batches-py'\n\n    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n    \n    # Subsample the data\n    mask = list(range(num_training, num_training + num_validation))\n    X_val = X_train[mask]\n    y_val = y_train[mask]\n    mask = list(range(num_training))\n    X_train = X_train[mask]\n    y_train = y_train[mask]\n    mask = list(range(num_test))\n    X_test = X_test[mask]\n    y_test = y_test[mask]\n    \n    mean_image = np.mean(X_train, axis=0)\n    \n    return X_train, y_train, X_val, y_val, X_test, y_test, mean_image\n\n# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\ntry:\n   del X_train, y_train\n   del X_test, y_test\n   print('Clear previously loaded data.')\nexcept:\n   pass\n\nX_train, y_train, X_val, y_val, X_test, y_test, mean_image = get_CIFAR10_data()\nprint('Finish Loading Datasets')","b7439f48":"print('Train data shape: ', X_train.shape)\nprint('Train labels shape: ', y_train.shape)\nprint('Validation data shape: ', X_val.shape)\nprint('Validation labels shape: ', y_val.shape)\nprint('Test data shape: ', X_test.shape)\nprint('Test labels shape: ', y_test.shape)","aa58b0dd":"plt.rcParams['figure.figsize'] = (25, 25) # set default size of plots\n#plt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\nexamples_per_class = 10\nclasses = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', \n           'horse', 'ship', 'truck']\nfor cls, cls_name in enumerate(classes):\n    idxs = np.where((y_test == cls))[0]\n    idxs = np.random.choice(idxs, examples_per_class, replace=False)\n    for i, idx in enumerate(idxs):\n        plt.subplot(examples_per_class, len(classes), i * len(classes) + cls + 1)\n        plt.imshow(X_test[idx].astype('uint8'))\n        plt.axis('off')\n        if i == 0:\n            plt.title(cls_name)\nplt.show()","03422257":"num_training=49000\nnum_validation=1000\nnum_test=1000\n# Reshape data to rows\nX_train = X_train.reshape(num_training, -1)\nX_val = X_val.reshape(num_validation, -1)\nX_test = X_test.reshape(num_test, -1)","a11e30ed":"net_params = {'il':X_train.shape[1],'hl1':50,'ol':10}\nactiv_function = 'Sigmoid'\ncost_function = 'Entropy_Loss'\n#activ_function = 'ReLu'\n#cost_function = 'SoftMax'\nnet = neural_net(net_params, activ_function, cost_function, std = 1)\nlr = 0.01\nreg = 0\nmax_iter = 2000\n\nresults = net.train_stochastic_gradient_descent(X_train, y_train, X_val, y_val, \n                                        learning_rate=lr, reg = reg,\n                                 max_iter = max_iter, batch_size = 500, verbose =  True, \n                                                epoch = 500)","6addaa98":"plotStats(results)\ny_pred, train_score = net.predict(X_train, with_score=True)\nprint(f'accuracy in training set : {(y_pred == y_train).mean() *100}')\ny_pred_val, val_score = net.predict(X_val, with_score=True)\nprint(f'accuracy in validation set : {(y_pred_val == y_val).mean() *100}')\ny_pred_test, test_score = net.predict(X_test, with_score=True)\nprint(f'accuracy in validation set : {(y_pred_test == y_test).mean() *100}')\nshow_net_weights(net)","7bd43481":"net_params = {'il':X_train.shape[1],'hl1':50,'ol':10}\n# activ_function = 'Sigmoid'\n# cost_function = 'Entropy_Loss'\nactiv_function = 'ReLu'\ncost_function = 'SoftMax'\nnet = neural_net(net_params, activ_function, cost_function, std = 1e-5)\nlr = 2.0e-04\nreg = 4.789351e-04\nmax_iter = 2000\n# results = net.train_stochastic_gradient_descent(X_train, y_train, X_val, y_val, \n#                                         learning_rate=3.9e-04, reg = 4.789351e-04,\n#                                  max_iter = 10000, batch_size = 200, verbose =  True, \n#                                                 epoch = 500)\nresults = net.train_stochastic_gradient_descent(X_train, y_train, X_val, y_val, \n                                        learning_rate=lr, reg = reg,\n                                 max_iter = max_iter, batch_size = 500, verbose =  True, \n                                                epoch = 500)","1830cd4e":"plotStats(results)\ny_pred, train_score = net.predict(X_train, with_score=True)\nprint(f'accuracy in training set : {(y_pred == y_train).mean() *100}')\ny_pred_val, val_score = net.predict(X_val, with_score=True)\nprint(f'accuracy in validation set : {(y_pred_val == y_val).mean() *100}')\ny_pred_test, test_score = net.predict(X_test, with_score=True)\nprint(f'accuracy in validation set : {(y_pred_test == y_test).mean() *100}')\nshow_net_weights(net)","2991c3cc":"X_train, y_train, X_val, y_val, X_test, y_test, mean_image = get_CIFAR10_data()\nprint('Finish Loading Datasets')\nprint('Train data shape: ', X_train.shape)\nprint('Train labels shape: ', y_train.shape)\nprint('Validation data shape: ', X_val.shape)\nprint('Validation labels shape: ', y_val.shape)\nprint('Test data shape: ', X_test.shape)\nprint('Test labels shape: ', y_test.shape)","0192afc0":"# Normalize the data: subtract the mean image\nmean_image = np.mean(X_train, axis=0)\nX_train -= mean_image\nX_val -= mean_image\nX_test -= mean_image\n\n# Reshape data to rows\nX_train = X_train.reshape(num_training, -1)\nX_val = X_val.reshape(num_validation, -1)\nX_test = X_test.reshape(num_test, -1)","8b6e69b7":"net_params = {'il':X_train.shape[1],'hl1':416,'ol':10}\nactiv_function = 'Sigmoid'\ncost_function = 'Entropy_Loss'\n# activ_function = 'ReLu'\n# cost_function = 'SoftMax'\nnet = neural_net(net_params, activ_function, cost_function, std = 1)\nlr = 0.01\nreg = 4.789351e-07\nmax_iter = 2000\n\n\nresults = net.train_stochastic_gradient_descent(X_train, y_train, X_val, y_val, \n                                        learning_rate=lr, reg = reg,\n                                 max_iter = max_iter, batch_size = 500, verbose =  True, \n                                                epoch = 500)","2fd63ee4":"plotStats(results)\ny_pred, train_score = net.predict(X_train, with_score=True)\nprint(f'accuracy in training set : {(y_pred == y_train).mean() *100}')\ny_pred_val, val_score = net.predict(X_val, with_score=True)\nprint(f'accuracy in validation set : {(y_pred_val == y_val).mean() *100}')\ny_pred_test, test_score = net.predict(X_test, with_score=True)\nprint(f'accuracy in validation set : {(y_pred_test == y_test).mean() *100}')\nshow_net_weights(net)","4dbf83ea":"net_params = {'il':X_train.shape[1],'hl1':416,'ol':10}\n#activ_function = 'Sigmoid'\n#cost_function = 'Entropy_Loss'\nactiv_function = 'ReLu'\ncost_function = 'SoftMax'\nnet = neural_net(net_params, activ_function, cost_function, std = 1e-4)\nlr =5.0e-04\nreg = 4.789351e-07\nmax_iter = 2000\n\n\nresults = net.train_stochastic_gradient_descent(X_train, y_train, X_val, y_val, \n                                        learning_rate=lr, reg = reg,\n                                 max_iter = max_iter, batch_size = 500, verbose =  True, \n                                                epoch = 500)","c9bb2717":"plotStats(results)\ny_pred, train_score = net.predict(X_train, with_score=True)\nprint(f'accuracy in training set : {(y_pred == y_train).mean() *100}')\ny_pred_val, val_score = net.predict(X_val, with_score=True)\nprint(f'accuracy in validation set : {(y_pred_val == y_val).mean() *100}')\ny_pred_test, test_score = net.predict(X_test, with_score=True)\nprint(f'accuracy in validation set : {(y_pred_test == y_test).mean() *100}')\nshow_net_weights(net)","a6a496fa":"X_train, y_train, X_val, y_val, X_test, y_test, mean_image = get_CIFAR10_data()\nprint('Finish Loading Datasets')\nprint('Train data shape: ', X_train.shape)\nprint('Train labels shape: ', y_train.shape)\nprint('Validation data shape: ', X_val.shape)\nprint('Validation labels shape: ', y_val.shape)\nprint('Test data shape: ', X_test.shape)\nprint('Test labels shape: ', y_test.shape)","332d4178":"import cv2\nfrom tqdm.autonotebook import tqdm\nX_train_hist = np.zeros(shape=X_train.shape)\nX_val_hist = np.zeros(shape=X_val.shape)\nX_test_hist = np.zeros(shape=X_test.shape)\n\nwith tqdm(total=X_train.shape[0]) as pbar:\n    for i in range(X_train.shape[0]):\n        img = np.float32(X_train[i])\/255\n        # Calculate gradient \n        gx = cv2.Sobel(img, cv2.CV_32F, 1, 0, ksize=1)\n        gy = cv2.Sobel(img, cv2.CV_32F, 0, 1, ksize=1)\n        mag, angle = cv2.cartToPolar(gx, gy, angleInDegrees=True)\n        X_train_hist[i] = mag\n        pbar.set_description(f'{i}\/{X_train.shape[0]}')\n        pbar.update(1)\n        \nwith tqdm(total=X_val.shape[0]) as pbar:\n    for i in range(X_val.shape[0]):\n        img = np.float32(X_val[i])\/255\n        # Calculate gradient \n        gx = cv2.Sobel(img, cv2.CV_32F, 1, 0, ksize=1)\n        gy = cv2.Sobel(img, cv2.CV_32F, 0, 1, ksize=1)\n        mag, angle = cv2.cartToPolar(gx, gy, angleInDegrees=True)\n        X_val_hist[i] = mag\n        pbar.set_description(f'{i}\/{X_val.shape[0]}')\n        pbar.update(1)\n        \nwith tqdm(total=X_test.shape[0]) as pbar:\n    for i in range(X_test.shape[0]):\n        img = np.float32(X_test[i])\/255\n        # Calculate gradient \n        gx = cv2.Sobel(img, cv2.CV_32F, 1, 0, ksize=1)\n        gy = cv2.Sobel(img, cv2.CV_32F, 0, 1, ksize=1)\n        mag, angle = cv2.cartToPolar(gx, gy, angleInDegrees=True)\n        X_test_hist[i] = mag\n        pbar.set_description(f'{i}\/{X_test.shape[0]}')\n        pbar.update(1)","ba0cfe36":"plt.subplot(1,2,1)\nplt.imshow(X_train[1].astype('uint8'))\nplt.axis('off')\nplt.subplot(1,2,2)\nplt.imshow(X_train_hist[1])\nplt.axis('off')\nplt.show()","4ae03ed0":"num_training=49000\nnum_validation=1000\nnum_test=1000\n# Reshape data to rows\nX_train_hist = X_train_hist.reshape(num_training, -1)\nX_val_hist = X_val_hist.reshape(num_validation, -1)\nX_test_hist = X_test_hist.reshape(num_test, -1)","081d74da":"net_params = {'il':X_train_hist.shape[1],'hl1':416,'ol':10}\nactiv_function = 'Sigmoid'\ncost_function = 'Entropy_Loss'\n# activ_function = 'ReLu'\n# cost_function = 'SoftMax'\nnet = neural_net(net_params, activ_function, cost_function, std = 1)\nlr = 0.09\nreg = 4.789351e-07\nmax_iter = 2000\n\nresults = net.train_stochastic_gradient_descent(X_train_hist, y_train, X_val_hist, y_val, \n                                        learning_rate=lr, reg = reg,\n                                 max_iter = max_iter, batch_size = 500, verbose =  True, \n                                                epoch = 500)","dca3c358":"plotStats(results)\ny_pred, train_score = net.predict(X_train_hist, with_score=True)\nprint(f'accuracy in training set : {(y_pred == y_train).mean() *100}')\ny_pred_val, val_score = net.predict(X_val_hist, with_score=True)\nprint(f'accuracy in validation set : {(y_pred_val == y_val).mean() *100}')\ny_pred_test, test_score = net.predict(X_test_hist, with_score=True)\nprint(f'accuracy in validation set : {(y_pred_test == y_test).mean() *100}')\nshow_net_weights(net)","d7425ced":"net_params = {'il':X_train_hist.shape[1],'hl1':416,'ol':10}\n#activ_function = 'Sigmoid'\n#cost_function = 'Entropy_Loss'\nactiv_function = 'ReLu'\ncost_function = 'SoftMax'\nnet = neural_net(net_params, activ_function, cost_function, std = 1)\nlr = 5e-01\nreg = 4.789351e-07\nmax_iter = 2000\n\nresults = net.train_stochastic_gradient_descent(X_train_hist, y_train, X_val_hist, y_val, \n                                        learning_rate=lr, reg = reg,\n                                 max_iter = max_iter, batch_size = 500, verbose =  True, \n                                                epoch = 500)","19610f97":"plotStats(results)\ny_pred, train_score = net.predict(X_train_hist, with_score=True)\nprint(f'accuracy in training set : {(y_pred == y_train).mean() *100}')\ny_pred_val, val_score = net.predict(X_val_hist, with_score=True)\nprint(f'accuracy in validation set : {(y_pred_val == y_val).mean() *100}')\ny_pred_test, test_score = net.predict(X_test_hist, with_score=True)\nprint(f'accuracy in validation set : {(y_pred_test == y_test).mean() *100}')\nshow_net_weights(net)","d1466a39":"# Train on histogram Images\nX_train, y_train, X_val, y_val, X_test, y_test, mean_image = get_CIFAR10_data()\nprint('Finish Loading Datasets')\nprint('Train data shape: ', X_train.shape)\nprint('Train labels shape: ', y_train.shape)\nprint('Validation data shape: ', X_val.shape)\nprint('Validation labels shape: ', y_val.shape)\nprint('Test data shape: ', X_test.shape)\nprint('Test labels shape: ', y_test.shape)\n\nfrom features import *\n\nnum_color_bins = 10 # Number of bins in the color histogram\nfeature_fns = [hog_feature, lambda img: color_histogram_hsv(img, nbin=num_color_bins)]\nX_train_feats = extract_features(X_train, feature_fns, verbose=True)\nX_val_feats = extract_features(X_val, feature_fns, verbose = True)\nX_test_feats = extract_features(X_test, feature_fns, verbose = True)\n\n# Preprocessing: Subtract the mean feature\nmean_feat = np.mean(X_train_feats, axis=0, keepdims=True)\nX_train_feats -= mean_feat\nX_val_feats -= mean_feat\nX_test_feats -= mean_feat\n\n# Preprocessing: Divide by standard deviation. This ensures that each feature\n# has roughly the same scale.\nstd_feat = np.std(X_train_feats, axis=0, keepdims=True)\nX_train_feats \/= std_feat\nX_val_feats \/= std_feat\nX_test_feats \/= std_feat\n\n# Preprocessing: Add a bias dimension\nX_train_feats = np.hstack([X_train_feats, np.ones((X_train_feats.shape[0], 1))])\nX_val_feats = np.hstack([X_val_feats, np.ones((X_val_feats.shape[0], 1))])\nX_test_feats = np.hstack([X_test_feats, np.ones((X_test_feats.shape[0], 1))])\nprint('finish')","ed86734f":"print(X_train_feats.shape)\nX_train_feats = X_train_feats[:, :-1]\nX_val_feats = X_val_feats[:, :-1]\nX_test_feats = X_test_feats[:, :-1]\n\nprint(f'train data feats shape {X_train_feats.shape}')\nprint(f'val data feats shape {X_val_feats.shape}')\nprint(f'test data feats shape {X_test_feats.shape}')","2569b862":"net_params = {'il':X_train_feats.shape[1],'hl1':416,'ol':10}\nactiv_function = 'Sigmoid'\ncost_function = 'Entropy_Loss'\n# activ_function = 'ReLu'\n# cost_function = 'SoftMax'\nnet = neural_net(net_params, activ_function, cost_function, std = 1)\nlr = 0.9\nreg = 4.789351e-07\nmax_iter = 2000\n\nresults = net.train_stochastic_gradient_descent(X_train_feats, y_train, X_val_feats, y_val, \n                                        learning_rate=lr, reg = reg,\n                                 max_iter = max_iter, batch_size = 500, verbose =  True, \n                                                epoch = 500)","2bd540e0":"plotStats(results)\ny_pred, train_score = net.predict(X_train_feats, with_score=True)\nprint(f'accuracy in training set : {(y_pred == y_train).mean() *100}')\ny_pred_val, val_score = net.predict(X_val_feats, with_score=True)\nprint(f'accuracy in validation set : {(y_pred_val == y_val).mean() *100}')\ny_pred_test, test_score = net.predict(X_test_feats, with_score=True)\nprint(f'accuracy in validation set : {(y_pred_test == y_test).mean() *100}')","094a7fcd":"net_params = {'il':X_train_feats.shape[1],'hl1':416,'ol':10}\n#activ_function = 'Sigmoid'\n#cost_function = 'Entropy_Loss'\nactiv_function = 'ReLu'\ncost_function = 'SoftMax'\nnet = neural_net(net_params, activ_function, cost_function, std = 1)\nlr = 5e-01\nreg = 4.789351e-07\nmax_iter = 2000\n\nresults = net.train_stochastic_gradient_descent(X_train_feats, y_train, X_val_feats, y_val, \n                                        learning_rate=lr, reg = reg,\n                                 max_iter = max_iter, batch_size = 500, verbose =  True, \n                                                epoch = 500)","8ef1ca03":"plotStats(results)\ny_pred, train_score = net.predict(X_train_feats, with_score=True)\nprint(f'accuracy in training set : {(y_pred == y_train).mean() *100}')\ny_pred_val, val_score = net.predict(X_val_feats, with_score=True)\nprint(f'accuracy in validation set : {(y_pred_val == y_val).mean() *100}')\ny_pred_test, test_score = net.predict(X_test_feats, with_score=True)\nprint(f'accuracy in validation set : {(y_pred_test == y_test).mean() *100}')","f793f99b":"plt.rcParams['figure.figsize'] = (20, 20) # set default size of plots\n#plt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\nexamples_per_class = 10\nclasses = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\nfor cls, cls_name in enumerate(classes):\n    idxs = np.where((y_train != cls) & (y_pred == cls))[0]\n    idxs = np.random.choice(idxs, examples_per_class, replace=False)\n    for i, idx in enumerate(idxs):\n        plt.subplot(examples_per_class, len(classes), i * len(classes) + cls + 1)\n        plt.imshow(X_train[idx].astype('uint8'))\n        plt.axis('off')\n        if i == 0:\n            plt.title(cls_name)\nplt.show()","d89bcb15":"classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', \n               'frog', 'horse', 'ship', 'truck']\nstri = ''\nfor i, clas in enumerate(classes):\n    stri = stri+','+str(i)+': '+clas","4e326c05":"print(stri[1:])\nnum_rows = 3\nnum_cols = 4\nnum_images = num_rows*num_cols\nplt.figure(figsize=(2*2*num_cols, 2*num_rows))\nfor i in range(num_images):\n    plt.subplot(num_rows, 2*num_cols, 2*i+1)\n    plot_image(i, train_score[i], y_train, X_train)\n    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n    plot_value_array(i, train_score[i], y_train)\nplt.tight_layout()\nplt.show()","4db1dccc":"from __future__ import absolute_import, division, print_function, unicode_literals\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import datasets, layers, models\nimport matplotlib.pyplot as plt\n\nclass_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n               'dog', 'frog', 'horse', 'ship', 'truck']\n\ntrain_images = X_train\/255\ntest_images = X_test\/255\nval_images = X_val\/255\nplt.figure(figsize=(10,10))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(train_images[i])\n    # The CIFAR labels happen to be arrays, \n    # which is why you need the extra index\n    plt.xlabel(class_names[y_train[i]])\nplt.show()","24f9a085":"model = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(10, activation='softmax'))\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])","7a24dcd1":"weights_history = []\n\n# A custom callback\n# https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks\/Callback\nclass MyCallback(keras.callbacks.Callback):\n    def on_batch_end(self, batch, logs):\n        weights_history.append(model.get_weights())\n\n\ncallback = MyCallback()","80fb32d1":"history = model.fit(train_images, y_train, epochs=10, \n                    validation_data=(val_images, y_val), callbacks=[callback])","b7cda2c4":"plt.rcParams['figure.figsize'] = (4, 4)\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\n\ntest_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)","d3539fc3":"## Non-Linearly separable data\n\n\u00bfQu\u00e9 ocurre cuando los datos no son linealmente separables?\n\n1.  Soluci\u00f3n: Unir perceptrones y crear una red de perceptrones <br>\n2.  Problema: Entre m\u00e1s amplia y profunda sea la red neuronal con una funci\u00f3n activaci\u00f3n no diferenciable con la del perceptron (signo o paso) no ser\u00e1 posible encontrar los valores optimos de $\\vec{w}$, adem\u00e1s no es posible hacer backprop y la funci\u00f3n no es convexa (bad-loca-minima) \n\nEjemplo:","0084ac9c":"# Single perceptron","7ce1554c":"## Algoritmos de gradiente descendente\n- Stochastic gradient descent\n- Stochastic gradient descent with momemtum\n- Adam \n\n## Problema de la convexidad de la funci\u00f3n error de una red neuronal\n![nonconvexity.png](attachment:nonconvexity.png)\n\nbibliograf\u00eda: https:\/\/papers.nips.cc\/paper\/7875-visualizing-the-loss-landscape-of-neural-nets.pdf","dc17e8d6":"## State of the art technique\n1. Pytorch","23338fa6":"## Deep model","0e7de2f0":"# 2D gradient descent","5ef89013":"## Small model","08504aab":"# MNIST DATASET\n- Conjunto de imagenes de digitos","ec2164d7":"## Histogram\n1. Extraer el histograma de la Imagen","50fb5b38":"## Loading dataset","ee683cf2":"## Soluci\u00f3n:\nla soluci\u00f3n dada al problema presentado anteriormente fue la idea de backpropagation (derivadas parciales de manera recursiva), y la propuesta de otras funciones activaci\u00f3n no lineales suaves (funci\u00f3n continuamente diferenciable). Las principales son:\n<br>\n**Sigmoid**:\n\\begin{equation}\n    \\sigma(x) = \\frac{1}{1+e^{-x}}\n\\end{equation}\n\n![](http:\/\/cs231n.github.io\/assets\/nn1\/sigmoid.jpeg)\n\n**Tanh**: \n\\begin{equation}\n    \\tanh(x) = 2\\sigma(2x)-1\n\\end{equation}\n![](http:\/\/cs231n.github.io\/assets\/nn1\/tanh.jpeg)\n\n**ReLu**: The rectified Linear Unit.\n\\begin{equation}\n    f(x) = max(0,x) = \\left\\{\n                            \\begin{array}{ll}\n                                 x &, x \\geq 0\\\\\n                                 0 &, x \\leq 0\\\\\n                            \\end{array}\n                      \\right.\n\\end{equation}\n![](http:\/\/cs231n.github.io\/assets\/nn1\/relu.jpeg)\nEsta es la \u00fanica que no es suave en todo $R$ pues no es diferenciable en $x = 0$, en la pr\u00e1ctica se le suele dar cualquier valor porque $f(0)$ no suele presentarse. Una posibilidad es $\\epsilon = 0.000000001$ el cual es 0, se considera 1 el gradiente a este valor.\n\nLa otra posibilidad encontrar una aproximaci\u00f3n de la funci\u00f3n ReLu que sea suave. Una familia de aproximaciones ser\u00eda:\n\\begin{equation}\n    f_k(x) := \\frac{1}{2k}log(1+e^{2kx})\n\\end{equation}\nPara $k$ mayores, mejor $f_k$ aproxima la funci\u00f3n ReLu, como se puede ver en la siguiente imagen:\n\n![aproximation.png](attachment:aproximation.png)\n\nEl motivo por el que ReLu es la que m\u00e1s se usa es debido a que es la m\u00e1s r\u00e1pida y tiene menos gasto computacional.\n","10a08a19":"## Regla para actualizar los parametros\nEl algoritmo que se usar\u00e1 es del gradiente descendiente con el proceso es el siguiente:\n\n1. Obtener el gradiente usando el backprop.\n2. El gradiente es la direcci\u00f3n del m\u00e1ximo crecimiento de la funci\u00f3n, luego el gradiente descendiente es actualizar los parametros $w$ de la funci\u00f3n error $L$ para minimizarla en direcci\u00f3n contraria al gradiente.\n\n\\begin{equation}\n    w = w - \\alpha\\nabla L(x,w)\n\\end{equation}","a7422b60":"# Sigmoid multi-layer neuron","da739d48":"## Prepare the data","a3cf06b2":"## Estructura del perceptron (Una sola neurona)\n\n<img src=\"https:\/\/s3.amazonaws.com\/stackabuse\/media\/intro-to-neural-networks-scikit-learn-2.png\"  style=\"width: 1200px;\"\/>\n\nFunci\u00f3n activaci\u00f3n: funci\u00f3n signo\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/4\/4f\/Signum_function.svg\/1024px-Signum_function.svg.png\"  style=\"width: 300px;\"\/>\n\nMatem\u00e1ticamente se puede ver cada se\u00f1al entrante, $x_i$ $\\forall i \\in \\{1,..,n\\}$, como un vector $\\vec{x}$ de $n$ entradas. Los pesos, $w_i$ $\\forall i \\in \\{1,..,n\\}$, un vector $\\vec{w}$ de $n$ entradas y $T$ el umbral. De esta manera el modelo es el siguiente:\n\n\\begin{equation}\n    f = \\left\\{\n    \\begin{array}{ll}\n         1 &, \\sum_{i=1}^{n}x_i w_i > T\\\\\n         -1 &, \\sum_{i=1}^{n}x_i w_i \\leq T\\\\\n    \\end{array}\n    \\right.\n\\end{equation}\n\nReescribiendo la ecuaci\u00f3n anterior para solo trabajar con $w$ definamos, $x_0 = 1$, $w_0 = -T$ y $\\sum_{i=0}^{n}x_i w_i = \\vec{w}\\cdot\\vec{x}$, se tiene:\n\n\\begin{equation}\n    f = \\left\\{\n    \\begin{array}{ll}\n         1 &, \\vec{w}\\cdot\\vec{x} > 0\\\\\n         -1 &, \\vec{w}\\cdot\\vec{x} \\leq 0\\\\\n    \\end{array}\n    \\right.\n\\end{equation}\n\nLuego el problema se resume en encontrar los valores $\\vec{w}$ que optimizan el error entre la predicci\u00f3n $f$ y el resultado real $y$. Para el Perceptron se tiene la siguiente funci\u00f3n error, llamada el **criterio del perceptron**:\n\n\\begin{equation}\n    E(w) = -\\sum_{x \\in M} (\\vec{w}\\cdot\\vec{x})*y\n\\end{equation}\n\ndonde $M$ es el conjunto de todos los $x$ que no se clasificaron correctamente por los pesos $w$. Como $x$ no est\u00e1 correctamente clasificada, el producto punto tendr\u00e1 un signo opuesto al valor deseado, $y$, de $x$. Entonces multiplicando por el valor correcto resultar\u00e1 en un valor negativo, forzando el error $E(w)$ a ser positivo\n\nPara minimizar $E(w)$, se usar\u00e1 el algoritmo \"stochastic gradient descent\" el cu\u00e1l actualiza el peso de la siguiente manera:\n\n\\begin{equation}\n    w = w - \\nabla_{w}E(w) = w + x_{k}y_{k}\n\\end{equation}\n\nDonde $\\nabla_{w}E(w)$ es el gradiente de $E(w)$ para cada $x_{k}$ que no se encuentre correctamente clasificado.\n\n## Teorema de convergencia del perceptron\nEstablece que si el conjunto de datos $T$ es linealmente separable, esto es, $\\;\\exists \\vec{w^{*}},\\; \\forall(x,y) \\in T.\\ y = sign(\\vec{w^{*}}^{T}x)$. Entonces para cualquiera iniciaci\u00f3n del vector $\\vec{w}$, la regla de aprendizaje del perceptron converger\u00e1 a un vector peso (no necesariamente \u00fanico y no necesariamente $\\vec{w^{*}}$) que nos de la correcta clasificaci\u00f3n para todos los datos de entrada, y esto se har\u00e1 en un n\u00famero finito de pasos.\n\nSe muestra una implementaci\u00f3n:","8841a333":"# 3D gradient descent","2ede8374":"## Showing some Images","2faf2420":"## Preprocesando las Imagenes\n1. Normalizamos los datos: restando la media de la imagen","2e7bc5c0":"# Linear Regression example","85158082":"## Work in raw Images","43530a7e":"## Extracci\u00f3n total de caracter\u00edsticas de los datos\n1. Histogram + color selected\n2. Normalization\n3. standard deviation\n","d8ae503f":"# CIFAR10 Dataset\n","f117ebc8":"## BackPropagation method\n\n![back.png](attachment:back.png)\n\nBackPropagation es la manera de calcular gradientes aplicando regla de la cadena de manera recursiva. El problema principal es el siguiente, tenemos una funci\u00f3n $f(x)$  donde $x$ es un vector de entradas y estamos interesados en calcular el gradiente de $f$ en $x$ ($\\nabla f(x)$). <br>\n\nEn el caso espec\u00edfico de una red neuronal, $f$ corresponde a la funci\u00f3n error o perdida ($L$) y la entrada $x$ consiste en los datos de entrenamiento y los pesos de la red neuronal. Por ejemplo, la funci\u00f3n error puede ser la norma de la diferencia al cuadrado o la funci\u00f3n error de support vector machines, los datos de entrada ser\u00edan los datos de entrenamiento $(x_i,y_i), \\; i=1...N$, los pesos y los biases $W, \\; b$. Los datos de entrenamiento son fijos, y los pesos son las variables que se optimizar\u00e1n. Luego usaremos backpropagation para calcular el gradiente de las variables $W, \\; b$ y as\u00ed poder usarlos para actualizar los parametros. Sin embargo clacular el gradiente en $(x_i,y_i)$ puede ser usado para visualizar e interpretar que es lo que la red neuronal puede estar haciendo. <br>\n\nVeamos el ejemplo de $f(x,y,z) = (x+y)z$. Esta expresi\u00f3n es bastante simple para diferenciar directamente, pero veamos como funciona backpropagation. La expresi\u00f3n se puede dividir en dos: $q=x+y$ y $f=qz$. Calculando tenemos: \n\n\\begin{equation}\n    \\frac{\\partial{f}}{\\partial{q}} = z \\\\\n    \\frac{\\partial{f}}{\\partial{z}} = q \\\\\n    \\frac{\\partial{q}}{\\partial{x}} = 1 \\\\\n    \\frac{\\partial{q}}{\\partial{y}} = 1 \\\\\n\\end{equation}\n\nAplicando regla de la cadena tenemos para hallar $\\frac{\\partial{f}}{\\partial{z}}$, $\\frac{\\partial{f}}{\\partial{x}}$, $\\frac{\\partial{f}}{\\partial{y}}$:\n\n\\begin{equation}\n    \\frac{\\partial{f}}{\\partial{y}} = \\frac{\\partial{f}}{\\partial{q}}\\frac{\\partial{q}}{\\partial{y}} = z * 1\\\\\n    \\frac{\\partial{f}}{\\partial{x}} = \\frac{\\partial{f}}{\\partial{q}}\\frac{\\partial{q}}{\\partial{x}} = z * 1\\\\\n    \\frac{\\partial{f}}{\\partial{z}} = q \\\\\n\\end{equation}\n\nSi le damos valores $x = -2$, $y = 5$, $z = -4$ que se muestra en el siguiente grafo computacional, donde los valores en verde es el forwardpropagation del grafo y en rojo es el backpropagation mostrando los valores del gradiente de $f$ con respecto a cada variable:\n![backProp1.png](attachment:backProp1.png)\n\nEs decir, en cada nodo del grafo computacional recibe ciertos datos de entrada y se puede computar dos cosas: \n1. Su valor de salida.\n2. Su gradiente local en ese nodo del valor de salida con respecto al dato entrante.\n\nNote que los nodos realizan este proceso totalmente independiente sin necesidad de tener en cuenta todo el grafo. Sin embargo, una vez completado el forward propagation, durante el backprop los nodos eventualmente aprender\u00e1n sobre el gradiente de su valor de salida del grafo entero. Regla de la cadena dice que el nodo debe tomar ese gradiente y multiplicarlo por cada gradiente local que se comput\u00f3 anteriormente. Veamos el siguiente ejemplo con una red neuronal que usa la funci\u00f3n activaci\u00f3n sigmoid.<br>\n\n**Ejemplo:**\n\n\\begin{equation}\n    f(w,x) = \\frac{1}{1+e^{-(w_0x_0+w_1x_1+w_2)}}\n\\end{equation}\n\n![sigmoidexample.png](attachment:sigmoidexample.png)\n\nLa derivada de la funci\u00f3n sigmoid es:\n\n\\begin{equation}\n    \\sigma(x) = \\frac{1}{1+e^{-x}}\\\\\n    \\frac{d\\sigma(x)}{dx} = \\frac{e^{-x}}{(1+e^{-x})} = \\left(\\frac{1+e^{-x}-1}{1+e^{-x}}\\right) \\left(\\frac{1}{1+e^{-x}}\\right) = (1-\\sigma(x))\\sigma(x)\n\\end{equation}"}}