{"cell_type":{"8cd01f93":"code","bcd3ef68":"code","3eac9fd0":"code","97bc291f":"code","bd1a9cae":"code","aa3f1a10":"code","048653e3":"code","5109471f":"code","8f871aef":"code","4b5d0100":"code","67cc79cc":"code","c278803a":"code","ea8281ed":"code","1ded152b":"code","66c94c56":"code","13daf117":"code","b641f950":"code","d2814b6d":"code","40b988dd":"code","902bcce1":"code","22ef6f39":"code","df654074":"code","7a32447d":"code","7a24602e":"code","f952f1d6":"code","38d41c2a":"code","e615b1f5":"code","217f3e36":"code","e6816626":"code","104b24b3":"code","8c2eeee0":"markdown","d52f99c1":"markdown","bfb0f0d2":"markdown","d8442de6":"markdown","f896220e":"markdown","92b50c37":"markdown","e38f7aa4":"markdown","499354b6":"markdown","32573ada":"markdown","dec704a0":"markdown","48b45503":"markdown","d6ef335e":"markdown","74b4dca2":"markdown","f11e7d7e":"markdown","57958287":"markdown","784658f1":"markdown","99f21b4f":"markdown","51d3deb0":"markdown","aac502cc":"markdown","bf0e4cd2":"markdown","924b5759":"markdown","b6abc50d":"markdown","7f22ed18":"markdown","04bf76c8":"markdown","7259471d":"markdown","e71e5102":"markdown"},"source":{"8cd01f93":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import LinearRegression, Ridge\nimport plotly.express as px\nimport plotly.figure_factory as ff\nimport xgboost as xgb\n\nsns.set()\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n%matplotlib inline","bcd3ef68":"dataset = pd.read_csv('..\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv')\ndataset.sample(5)","3eac9fd0":"dataset.describe().T","97bc291f":"# Dropping Serial No. as it doesn't give any additional data.\n\ndataset = dataset.drop('Serial No.', axis=1)\ndataset.columns = ['GRE Score', 'TOEFL Score', 'University Rating', 'SOP', 'LOR', 'CGPA', 'Research','Chance of admission']","bd1a9cae":"print(pd.isnull(dataset).sum())","aa3f1a10":"sns.distplot(dataset['GRE Score'], bins = 10, color = 'orange', label = 'KDE')\nplt.legend()\nplt.gcf().set_size_inches(12, 5)","048653e3":"sns.boxplot(x=dataset['GRE Score'], color = 'orange')\nplt.title('GRE boxplot', fontsize = 20)\nplt.show()","5109471f":"sns.distplot(dataset['TOEFL Score'], bins = 10, color = 'blue', label = 'KDE')\nplt.legend()\nplt.gcf().set_size_inches(12, 5)","8f871aef":"sns.boxplot(x=dataset['TOEFL Score'], color = 'blue')\nplt.title('TOEFL boxplot', fontsize = 20)\nplt.show()","4b5d0100":"hist_data = [dataset['SOP'], dataset['LOR'], dataset['CGPA']]\ngroup_labels = [ 'SOP','LOR','CGPA']\n\nfig = ff.create_distplot(hist_data, group_labels, bin_size=[0.5, 0.5,0.1], colors = ['#D4323E', 'turquoise','#177415'])\nfig.update_layout(title_text='SOP, LOR and CGPA')\nfig.show()","67cc79cc":"sns.boxplot(x=dataset['SOP'], color = 'pink')\nplt.title('SOP boxplot', fontsize = 20)\nplt.show()","c278803a":"sns.boxplot(x=dataset['CGPA'], color = 'green')\nplt.title('CGPA boxplot', fontsize = 20)\nplt.show()","ea8281ed":"sns.boxplot(x=dataset['LOR'], color = 'turquoise')\nplt.title('LOR boxplot', fontsize = 20)\nplt.show()","1ded152b":"LOR = dataset['LOR'] # Variable data\nLOR_Q1 = LOR.quantile(0.25) # Q1 inf limit\nLOR_Q3 = LOR.quantile(0.75) # Q3 sup limit\nLOR_IQR = LOR_Q3 - LOR_Q1 # IQR\nLOR_lowerend = LOR_Q1 - (1.5 * LOR_IQR) # q1 - 1.5 * q1\nLOR_upperend = LOR_Q3 + (1.5 * LOR_IQR) # q3 + 1.5 * q3\n\nLOR_outliers = LOR[(LOR < LOR_lowerend) | (LOR > LOR_upperend)] # Outlier index\nLOR_outliers","66c94c56":"sns.distplot(dataset['Chance of admission'], bins = 10, color = 'red', label = 'KDE')\nplt.legend()\nplt.gcf().set_size_inches(12, 5)","13daf117":"sns.boxplot(x=dataset['Chance of admission'], color = 'red')\nplt.title('Chance of admission', fontsize = 20)\nplt.show()","b641f950":"Adm = dataset['Chance of admission'] # Variable data\nAdm_Q1 = Adm.quantile(0.25) # Q1 inf limit\nAdm_Q3 = Adm.quantile(0.75) # Q3 sup limit\nAdm_IQR = Adm_Q3 - Adm_Q1 # IQR\nAdm_lowerend = Adm_Q1 - (1.5 * Adm_IQR) # q1 - 1.5 * q1\nAdm_upperend = Adm_Q3 + (1.5 * Adm_IQR) # q3 + 1.5 * q3\n\nAdm_outliers = Adm[(Adm < Adm_lowerend) | (Adm > Adm_upperend)] # Outlier index\nAdm_outliers","d2814b6d":"Research = pd.DataFrame(dataset['Research'].value_counts()).reset_index()\nResearch.columns = ['Research','Total']\nfig = px.pie(Research, values = 'Total', names = 'Research', title='Research', hole=.4, color = 'Research',width=800, height=400)\nfig.show()","40b988dd":"sns.heatmap(dataset.corr(), annot = True, linewidths=.5, cmap= 'YlGnBu')\nplt.title('Correlations', fontsize = 20)\nplt.gcf().set_size_inches(12, 7)\nplt.show()","902bcce1":"correlaciones = sns.pairplot(dataset, corner = True)","22ef6f39":"X = dataset.iloc[:,0:7].values \n\ny = dataset.iloc[:,-1].values","df654074":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0) # Set Splitting\n\n# K fold definition\n\nkfold = KFold(n_splits = 10)","7a32447d":"def regression_model(model):\n    \"\"\"\n    Will fit the regression model passed and will return the regressor object and the score\n    \"\"\"\n    regressor = model\n    regressor.fit(X_train, y_train)\n    score = regressor.score(X_test, y_test) # R2\n    cv_results = cross_val_score(model, X_train, y_train, cv = kfold)\n    cv_results = cv_results.mean()\n    return regressor, score, cv_results","7a24602e":"model_performance = pd.DataFrame(columns = [\"Model\", \"Score\",\"10 - fold cross val\"])\n\nmodels_to_evaluate = [xgb.XGBRegressor(),\n                      LinearRegression(), Ridge(), SVR(kernel = 'linear'), RandomForestRegressor()]\n\nfor model in models_to_evaluate:\n    regressor, score, cv_results = regression_model(model)\n    model_performance = model_performance.append({\"Model\": model, \"Score\": score, \"10 - fold cross val\":cv_results},\n                                                 ignore_index=True)\n\nmodel_performance","f952f1d6":"ridge_parameters = {'alpha': [1,0.1,0.01,0.001,0.0001,0]}\n\nridge_Regressor = Ridge()\ngrid = GridSearchCV(ridge_Regressor, ridge_parameters , cv = 10)\ngrid.fit(X_train,y_train)\nprint('The parameters combination that would give best accuracy is : ')\nprint(grid.best_params_)\nprint('The best accuracy achieved after parameter tuning via grid search is : ', grid.best_score_)","38d41c2a":"XGB_parameters = {'n_estimators': [50, 100, 200],\n        'subsample': [ 0.6, 0.8, 1.0],\n        'max_depth': [1,2,3,4],\n        'learning_rate': [0.1,0.2, 0.3, 0.4, 0.5]}\n\nXGB_Regressor = xgb.XGBRegressor()\ngrid = GridSearchCV(XGB_Regressor, XGB_parameters , cv = 10)\ngrid.fit(X_train,y_train)\nprint('The parameters combination that would give best accuracy is : ')\nprint(grid.best_params_)\nprint('The best accuracy achieved after parameter tuning via grid search is : ', grid.best_score_)","e615b1f5":"regressor = Ridge(alpha=1)\nregressor.fit(X_train, y_train)\ny_pred = regressor.predict(X_test)","217f3e36":"plt.scatter(y_test, y_pred, color = 'red')\nplt.xlabel('True admission chance',fontsize = 15)\nplt.ylabel('Predicted admission chance', fontsize = 15)\nplt.title('Ridge regressor', fontsize = 20)\nplt.grid(True)","e6816626":"features = {'GRE Score': [300, 295, 314], 'TOEFL Score': [105, 94, 101],\n                          'University Rating': [3, 3, 4], 'SOP':[3.4, 4.2, 3.1],\n                          'LOR':[3.3, 2.5, 2.9], 'CGPA':[7.12, 7.99, 9],\n                            'Research':[0, 1, 1]}\nfeaturesDF = pd.DataFrame.from_dict(features)\nfeaturesDF","104b24b3":"Admission = regressor.predict(featuresDF)\n\nAdmission_DF = pd.DataFrame({'Admission':Admission})\njoined = featuresDF.join(Admission_DF)\njoined","8c2eeee0":"The objective of the analysis is to see wheter someone will be admitted or not to a further academic instance given numerous grades received in other instances.","d52f99c1":"## Modeling and checking with K-Cross Validation","bfb0f0d2":"**TOEFL Score**","d8442de6":"**Research**","f896220e":"## GridSearch","92b50c37":"Data will be divided into k subsets (the method is repeated k times).\n\nEach time, one of the k-subsets is used as test set and the others k-1 subsets are going to be the training set. This will reduce bias and variance.\n\nFirst I'm going to evaluate the score using just train-test split, and will compare with the mean score of the cross validation with 10 folds for each model.\n\nWe will see if there is any remarkable conclussion to make.","e38f7aa4":"**GRE Score**","499354b6":"Almost every feature is a good one to predict Chance of admission!\n\nGRE score and TOEFL score are really correlated, also GRE score and CGPA.","32573ada":"## Correlations","dec704a0":"**XGBoost**","48b45503":"## EDA and visualizations","d6ef335e":"As I used the default parameters for the models above, I'll check what parameters could improve some of them. I'll try improving Ridge regression and will try to see if a more robust model as XGBoost could be tuned to surpass the Ridge accuracy.\n\nThis can be made with GridSearch. It'll methodically build and evaluate a model for each combination of parameters that I specified in a grid.\n\nWe could've used Random Search also. This other model will sample parameters from a random distribution.","74b4dca2":"There are no missing values in the set","f11e7d7e":"**The dataset has:**\n- 8 numerical columns ('Serial No.', six grades and wheter a student has made any research).\n- Chance of admission","57958287":"Even using GridSearch we couldn't find any combination of parameters for XGBoost that surpasses Ridge regression in this case.","784658f1":"**Ridge regression**","99f21b4f":"**Chance of admission**","51d3deb0":"- There are 500 students evaluated.\n- GRE and TOEFL score have different scales as University rating, SOP, LOR and CGP.\n\nThis means that if we don't use a robust model, we should scale the data to have a better accuracy.","aac502cc":"## Thanks for reaching the end!! Upvote if you liked it or found it useful! Glad to receive feedback.","bf0e4cd2":"**SOP, LOR, GCPA**","924b5759":"The following function will evaluate the score and the cross validation score for each input model.","b6abc50d":"**Loading modules and packages**","7f22ed18":"**Conclussions:**\n- All features follow a normal distribution.\n- Outliers may represent really bad candidates!","04bf76c8":"**Now that we see that Ridge is still a more accurate model than XGBoost, let's do some predictions!**","7259471d":"## Admission Prediction","e71e5102":"Now that we have the results, we can see that if we had just evaluated the accuracy of the algorithms with train-test split, we'd probably stayed with a tree based regressor, but seeing the cross val score, undoubtedly Ridge model for example is more accurate."}}