{"cell_type":{"b27c3ec4":"code","0041e1a0":"code","2acad664":"code","ce3cdc56":"code","fc48ed9d":"code","8c2ff02d":"code","2089c22b":"code","d90bc21c":"code","56e8d0a5":"code","523d514f":"code","fa38cffc":"code","bd67c503":"code","3a54d651":"code","91d8f1ac":"code","af3d1a53":"code","9a249f1d":"code","ccbe3ba2":"code","8e67164c":"code","5befb685":"code","505eed4d":"code","2eab03c9":"code","c62c47ce":"code","8a42f647":"markdown","cec037ec":"markdown"},"source":{"b27c3ec4":"import numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nimport warnings\nfrom pandas.core.common import SettingWithCopyWarning\nfrom warnings import simplefilter\n\nwarnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\nsimplefilter(action='ignore', category=FutureWarning)\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","0041e1a0":"df = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/test.csv\")\n\ndf.head(3)","2acad664":"del df[\"id\"]\ntest.index = test[\"id\"]\nid = test[\"id\"]\ntest.drop(\"id\",axis=1, inplace=True)","ce3cdc56":"df[\"class1\"] = np.where(df[\"target\"]==\"Class_1\", 1, 0)\n\ndf[\"class2\"] = np.where(df[\"target\"]==\"Class_2\", 1, 0)\n\ndf[\"class3\"] = np.where(df[\"target\"]==\"Class_3\", 1, 0)\n\ndf[\"class4\"] = np.where(df[\"target\"]==\"Class_4\", 1, 0)","fc48ed9d":"df.info()","8c2ff02d":"df1 = df.drop([\"class2\", \"class3\",\"class4\",\"target\"], axis=1)\ndf2 = df.drop([\"class1\", \"class3\",\"class4\",\"target\"], axis=1)\ndf3 = df.drop([\"class1\", \"class2\",\"class4\",\"target\"], axis=1)\ndf4 = df.drop([\"class1\", \"class2\",\"class3\",\"target\"], axis=1)","2089c22b":"X = df1.drop(\"class1\", axis=1)\ny = df1[\"class1\"]\n\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X, \n                                                    y, \n                                                    test_size=0.1,\n                                                    random_state=13,\n                                                    shuffle=True)\n\n","d90bc21c":"X = df2.drop(\"class2\", axis=1)\ny = df2[\"class2\"]\n\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X, \n                                                    y, \n                                                    test_size=0.1,\n                                                    random_state=13,\n                                                    shuffle=True)","56e8d0a5":"X = df3.drop(\"class3\", axis=1)\ny = df3[\"class3\"]\n\nX_train3, X_test3, y_train3, y_test3 = train_test_split(X, \n                                                    y, \n                                                    test_size=0.1,\n                                                    random_state=13,\n                                                    shuffle=True)","523d514f":"X = df4.drop(\"class4\", axis=1)\ny = df4[\"class4\"]\n\nX_train4, X_test4, y_train4, y_test4 = train_test_split(X, \n                                                    y, \n                                                    test_size=0.1,\n                                                    random_state=13,\n                                                    shuffle=True)","fa38cffc":"test1 = test.copy()\ntest2 = test.copy()\ntest3 = test.copy()\ntest4 = test.copy()","bd67c503":"eval_set1 = [(X_train1, y_train1), (X_test1, y_test1)]\neval_set2 = [(X_train2, y_train2), (X_test2, y_test2)]\neval_set3 = [(X_train3, y_train3), (X_test3, y_test3)]\neval_set4 = [(X_train4, y_train4), (X_test4, y_test4)]","3a54d651":"xgb = XGBClassifier(objective=\"binary:logistic\", \n                    n_estimators=1000, \n                    max_depth=4, \n                    learning_rate=0.1, subsample=0.9, colsample_bytree=0.6)\n\n\nxgb.fit(X_train1, y_train1, early_stopping_rounds=10, eval_metric=\"logloss\", eval_set=eval_set1, verbose=False)\ny_pred_train = xgb.predict_proba(X_train1)\ny_pred_test = xgb.predict_proba(X_test1)\nprint(\"Train: \", log_loss(y_train1, y_pred_train))\nprint(\"Test: \", log_loss(y_test1, y_pred_test))","91d8f1ac":"class1 = xgb.predict_proba(test)[:, 1]\nclass1[0:5]","af3d1a53":"xgb = XGBClassifier(objective=\"binary:logistic\", \n                    n_estimators=1000, \n                    max_depth=5, \n                    learning_rate=0.1, subsample=0.9, colsample_bytree=0.8)\n\n\nxgb.fit(X_train2, y_train2, early_stopping_rounds=10, eval_metric=\"logloss\", eval_set=eval_set2, verbose=False)\ny_pred_train = xgb.predict_proba(X_train2)\ny_pred_test = xgb.predict_proba(X_test2)\nprint(\"Train: \", log_loss(y_train2, y_pred_train))\nprint(\"Test: \", log_loss(y_test2, y_pred_test))","9a249f1d":"class2 = xgb.predict_proba(test2)[:,1]\nclass2[0:5]","ccbe3ba2":"xgb = XGBClassifier(objective=\"binary:logistic\", \n                    n_estimators=1000, \n                    max_depth=5, \n                    learning_rate=0.1, subsample=0.9, colsample_bytree=0.6)\n\n\nxgb.fit(X_train3, y_train3, early_stopping_rounds=10, eval_metric=\"logloss\", eval_set=eval_set3, verbose=False)\n\ny_pred_train = xgb.predict_proba(X_train3)\ny_pred_test = xgb.predict_proba(X_test3)\nprint(\"Train: \", log_loss(y_train3, y_pred_train))\nprint(\"Test: \", log_loss(y_test3, y_pred_test))","8e67164c":"class3 = xgb.predict_proba(test3)[:,1]\nclass3[0:5]","5befb685":"xgb = XGBClassifier(objective=\"binary:logistic\", \n                    n_estimators=1000, \n                    max_depth=5, \n                    learning_rate=0.1, subsample=0.9, colsample_bytree=0.7)\n\n\nxgb.fit(X_train4, y_train4, early_stopping_rounds=10, eval_metric=\"logloss\", eval_set=eval_set4, verbose=False)\ny_pred_train = xgb.predict_proba(X_train4)\ny_pred_test = xgb.predict_proba(X_test4)\nprint(\"Train: \", log_loss(y_train4, y_pred_train))\nprint(\"Test: \", log_loss(y_test4, y_pred_test))","505eed4d":"class4 = xgb.predict_proba(test4)[:,1]\nclass4[0:5]","2eab03c9":"sub = pd.DataFrame(class1, columns=[\"Class_1\"])\nsub.index = test.index\nsub[\"Class_2\"] = class2\nsub[\"Class_3\"] = class3\nsub[\"Class_4\"] = class4\nsub.head(3)","c62c47ce":"sub.to_csv('.\/sub.csv')\n","8a42f647":"## Even though I haven't used feature engineering, the submission result is not bad. Moreover, I haven't applied anything for an imbalance of datasets in which 2 of 4 datasets are severely imbalanced. Also, the hyperparameters of models are almost arbitrary because I haven't used Grid or Random search. \n\n","cec037ec":"\n## I have wondered what if I use four binary classification models instead of one multiclassification model. So, I've decided to build a baseline depends on this idea. \n\n\n\n"}}