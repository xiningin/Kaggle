{"cell_type":{"d9e54aee":"code","94293ed5":"code","210e5824":"code","b0300ba2":"code","338e1aa3":"code","929667a0":"markdown"},"source":{"d9e54aee":"import numpy as np\nimport matplotlib.pylab as plt\nimport nltk\nimport re\nimport pandas as pd\nfrom sklearn import metrics \nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom string import punctuation","94293ed5":"training = pd.read_csv('..\/input\/imdb-dataset-sentiment-analysis-in-csv-format\/Train.csv')\nvalidation = pd.read_csv('..\/input\/imdb-dataset-sentiment-analysis-in-csv-format\/Valid.csv')\ntesting = pd.read_csv('..\/input\/imdb-dataset-sentiment-analysis-in-csv-format\/Test.csv')\n\n# combine files\nsentiments = pd.concat([training, validation, testing], axis=0)\n\n# shuffle rows\nsentiments = sentiments.sample(frac=1)","210e5824":"#build the functions\n\ndef sig(z):\n    return 1\/(1+np.exp(-z))\n\ndef costFunction(y, X, theta, lamda):\n    m = X.shape[0]\n    z = X @ theta\n    h=sig(z)\n    regCost = 1\/m * np.sum((-y * np.log(h)) - ((1-y) * np.log(1-h)))+lamda\/(2*m)*np.square(np.linalg.norm(theta))\n    fPrimeConst = 1\/m * (X.T @ (h-y)) [0]\n    fPrimeReg = 1\/m * (X.T @ (h-y))[1:] + (lamda\/m)*theta[1:]\n    gradient = np.vstack((fPrimeConst,fPrimeReg))\n    return regCost, gradient\n    \ndef stochasticGradientDescent(y, X, theta, lamda, eta, rounds, batch):\n    sgd = []\n    z = np.c_[y.reshape(len(y),-1), X.reshape(len(X),-1)]\n    for i in range(rounds):\n        np.random.shuffle(z)\n        z=z[:batch]\n        cost, gradient = costFunction(z[:,:1],z[:,1:],theta,lamda)\n        theta = theta - (eta * gradient)\n        sgd.append(cost)\n    sgd = np.array(sgd)\n    return sgd, theta   \n\ndef logitClassifer(theta, X):\n    z = X @ theta\n    h=sig(z)\n    outcome = (h>=.5)*1\n    return outcome\n\n\ndef textCleaner(pattern, corpus):\n    clean = [re.sub(pattern,' ', c) for c in corpus]\n    return clean\n\ndef LowerExcept(data):\n    p_strip = lambda x: \"\".join(w for w in x if w not in punctuation)\n    allcaps = re.findall(r\"\\b[A-Z][A-Z]+\\b\",data)\n    to_lower = lambda l: \" \".join( a if p_strip(a) in allcaps else a.lower() for a in l.split())\n    return to_lower(data)","b0300ba2":"# clean text\nsentiments['text']=textCleaner('<br\\s\/>|\\(|\\)|\\\/|\\*',sentiments['text'])\nsentiments['text']=[LowerExcept(w) for w in sentiments['text']]\n\n# sample data\nsentiments = sentiments.sample(n=1000)","338e1aa3":"#convert to list\nx = sentiments['text'].values.tolist()\ny = sentiments['label'].values.tolist()\n\n\nvec = CountVectorizer(ngram_range=(2,2),\n                      tokenizer=nltk.word_tokenize)\n\ndf = vec.fit_transform(x)\ndf = pd.DataFrame(df.toarray(), columns=vec.get_feature_names())\n\nX = np.array(df)\ny = np.array(y)\ny = np.expand_dims(y, axis=1)\n\nX_train, X_test, y_train, y_test=train_test_split(X,y,random_state=1,test_size=0.5,shuffle=False)\nX_train, X_val, y_train, y_val=train_test_split(X_train,y_train,random_state=1,test_size=0.2,shuffle=False)\n\n\n# Validation & confusion matrix\ntheta0 = np.random.randn(X_train.shape[1],1) * np.sqrt(1. \/ X_train.shape[1])\ngd,theta =  stochasticGradientDescent(y_train,X_train,theta0,0,.1,100,50)\n\ny_pred_val = logitClassifer(theta, X_val)\nprint('Validation Accuracy:', np.sum((y_pred_val == y_val)*1)\/df.shape[0])\n\nconfmat = metrics.confusion_matrix(y_val, y_pred_val) \nfig, ax = plt.subplots(figsize=(2.5, 2.5)) \nax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3) \nfor i in range(confmat.shape[0]): \n    for j in range(confmat.shape[1]): \n        ax.text(x=j, y=i, \n            s=confmat[i, j], \n                     va= 'center', ha='center') \nplt.xlabel('predicted label') \nplt.ylabel('true label') \nplt.title('Validation')\nplt.show() ","929667a0":"# **Logistic Regression Classifier**\n\nFor sentiment classification using stochastic gradient ascent as the optimization algorithm. Also creates a confusion matrix for the classifier. This was a learning exercise with the objective of building the classifier rather than use a package.[](http:\/\/)"}}