{"cell_type":{"ad075b8e":"code","aa9fb67d":"code","39dc9473":"code","4c1263ef":"code","d25dcae4":"code","cb337dc9":"code","d386bbf8":"code","bc100081":"code","1e258b1a":"code","9d055681":"code","464fafca":"code","cdf3175a":"code","fdc1c234":"code","05795c1b":"code","ad0f30cb":"code","720c3024":"code","dc305ba3":"code","93a6efe2":"code","c3d1c587":"code","3694fe09":"code","2b814e08":"code","b704b48f":"code","c42843ed":"code","138563a3":"code","49a7e010":"code","947606e2":"code","6cb8f2ff":"code","b9b7e6ec":"code","f23215e8":"code","8cdcfc88":"code","06af7276":"code","d8efa16d":"code","d1c74e98":"code","f985d027":"code","356bfea9":"code","096c9ca6":"code","eb3e1c1f":"code","dd6d8f4c":"code","27831ed3":"code","431d21c0":"code","87e8a4c8":"code","019e8870":"code","f5b39a6c":"code","966462bb":"code","2f1ddea0":"code","c75ecdc4":"code","56a6d008":"code","3d90647b":"markdown","391b722c":"markdown","7cdc5064":"markdown","69f62e51":"markdown","798fed5d":"markdown","94fc6e88":"markdown","95a63791":"markdown","a6c7b8ec":"markdown"},"source":{"ad075b8e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport tensorflow as tf\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","aa9fb67d":"insurance = pd.read_csv(\"\/kaggle\/input\/insurance\/insurance.csv\")\ninsurance","39dc9473":"# 7 columns and 1338 rows\ninsurance.shape ","4c1263ef":"# check for empty value\ninsurance.isnull().sum()","d25dcae4":"# check data types\ninsurance.dtypes","cb337dc9":"# turn the object datatypes to numerical \ninsurance_one_hot = pd.get_dummies(insurance)\ninsurance_one_hot.head()","d386bbf8":"# checking columns in insurance_one_hot increases from 7 to 12 after get_dummies \ninsurance_one_hot.shape","bc100081":"# checking data types\ninsurance_one_hot.dtypes","1e258b1a":"# seperate X for feature and y for target\nX = insurance_one_hot.drop(\"charges\", axis=1)\ny = insurance_one_hot[\"charges\"]","9d055681":"# split the data from training and testing \nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X, \n                                                 y, \n                                                 test_size=0.2,\n                                                 random_state=42)","464fafca":"model = tf.keras.Sequential([\n tf.keras.layers.Dense(100),\n tf.keras.layers.Dense(10),\n tf.keras.layers.Dense(1)\n])\n\nmodel.compile(loss=\"mae\",\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=[\"mae\"])\n\nhistory = model.fit(X_train,\n                    y_train,\n                    epochs =150)","cdf3175a":"from tensorflow.keras.utils import plot_model\nplot_model(model,show_shapes=True)","fdc1c234":"model.summary()","05795c1b":"eva_1 = model.evaluate(X_test,y_test)","ad0f30cb":"# visualize \npd.DataFrame(history.history).plot()\nplt.xlabel(\"mae\")\nplt.ylabel(\"loss\");","720c3024":"# i pick this code from https:\/\/github.com\/mrdbourke\n\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n\n# Create column transformer (this will help us normalize\/preprocess our data)\nct = make_column_transformer(\n    (MinMaxScaler(), [\"age\", \"bmi\", \"children\"]), # get all values between 0 and 1\n    (OneHotEncoder(handle_unknown=\"ignore\"), [\"sex\", \"smoker\", \"region\"])\n)","dc305ba3":"# Create X & y\nX = insurance.drop(\"charges\", axis=1)\ny = insurance[\"charges\"]\n","93a6efe2":"# Build our train and test sets (use random state to ensure same split as before)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","c3d1c587":"# Fit column transformer on the training data only (doing so on test data would result in data leakage)\nct.fit(X_train)\n\n# Transform training and test data with normalization (MinMaxScalar) and one hot encoding (OneHotEncoder)\nX_train_normal = ct.transform(X_train)\nX_test_normal = ct.transform(X_test)","3694fe09":"# actual data\nX_train.loc[0]","2b814e08":"# normalized data\nX_train_normal[0]","b704b48f":"tf.random.set_seed(42)\nmodel_1 = tf.keras.Sequential([\n  tf.keras.layers.Dense(100,activation=\"relu\"),\n  tf.keras.layers.Dense(10,activation=\"relu\"),\n  tf.keras.layers.Dense(1)\n])\n","c42843ed":"model_1.compile(loss=tf.keras.losses.mean_absolute_error,\n                optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n                metrics=[\"mae\"])","138563a3":"history_1 = model_1.fit(X_train_normal,\n                        y_train,\n                        epochs=150,\n                        batch_size=32,\n                        verbose=1)","49a7e010":"# This is awesome\nmodel_1_evaluate = model_1.evaluate(X_test_normal, y_test)\nmodel_1_evaluate","947606e2":"pd.DataFrame(history_1.history).plot();","6cb8f2ff":"model_2 = tf.keras.Sequential([\n  tf.keras.layers.Dense(128, activation='relu'),\n  tf.keras.layers.Dense(64, activation='relu'),\n  tf.keras.layers.Dense(32, activation='relu'),\n  tf.keras.layers.Dense(16, activation='relu'),\n  tf.keras.layers.Dense(1)  \n])\n\nmodel_2.compile(loss=tf.keras.losses.mae,\n                          optimizer=tf.keras.optimizers.Adam(lr=0.001),\n                          metrics=[\"mae\"])\n\nhistory_2 = model_2.fit(X_train_normal,\n                        y_train,\n                        batch_size=32,\n                        epochs=500)","b9b7e6ec":"model_2_evaluate = model_2.evaluate(X_test_normal,y_test)\nmodel_2_evaluate","f23215e8":"pd.DataFrame(history_2.history).plot()","8cdcfc88":"model_2_preds = model_2.predict(X_test_normal)\nmodel_2_preds[:10]","06af7276":"# compare the prediction with model_2_preds its so close \ny_test[:10]","d8efa16d":"model_3 = tf.keras.Sequential([\n  tf.keras.layers.Dense(128, activation='relu'),                             \n  tf.keras.layers.Dense(128, activation='relu'),\n  tf.keras.layers.Dense(64, activation='relu'),\n  tf.keras.layers.Dense(32, activation='relu'),\n  tf.keras.layers.Dense(16, activation='relu'),\n  tf.keras.layers.Dense(1)  \n])\n\nmodel_3.compile(loss=tf.keras.losses.mae,\n                          optimizer=tf.keras.optimizers.Adam(lr=0.01),\n                          metrics=[\"mae\"])\n\nhistory_3 = model_3.fit(X_train_normal,\n                        y_train,\n                        batch_size=32,\n                        epochs=1000)","d1c74e98":"model_3_evaluate = model_3.evaluate(X_test_normal,y_test)","f985d027":"model_4 = tf.keras.Sequential([\n  tf.keras.layers.Dense(128, activation='relu'),\n  tf.keras.layers.Dense(64, activation='relu'),\n  tf.keras.layers.Dense(32, activation='relu'),\n  tf.keras.layers.Dense(16, activation='relu'),\n  tf.keras.layers.Dense(1)  \n])\n\nmodel_4.compile(loss=tf.keras.losses.mae,\n                          optimizer=tf.keras.optimizers.Adam(lr=0.001),\n                          metrics=[\"mae\"])\n\nhistory_4 = model_4.fit(X_train_normal,\n                        y_train,\n                        batch_size=32,\n                        epochs=2000)","356bfea9":"model_4_evaluate = model_4.evaluate(X_test_normal,y_test)","096c9ca6":"model_5 = tf.keras.Sequential([\n  tf.keras.layers.Dense(64, activation='relu'),\n  tf.keras.layers.Dense(64, activation='relu'),\n  tf.keras.layers.Dense(32, activation='relu'),\n  tf.keras.layers.Dense(16, activation='relu'),\n  tf.keras.layers.Dense(1)\n])\n\nmodel_5.compile(loss=tf.keras.losses.mae,\n                          optimizer=tf.keras.optimizers.Adam(lr=0.001),\n                          metrics=[\"mae\"])\n\nhistory_5 = model_5.fit(X_train_normal,\n                        y_train,\n                        batch_size=32,\n                        epochs=2000)","eb3e1c1f":"model_5_evaluate = model_4.evaluate(X_test_normal,y_test)","dd6d8f4c":"model_6 = tf.keras.Sequential([\n  tf.keras.layers.Dense(96, activation='relu'),\n  tf.keras.layers.Dense(64, activation='relu'),\n  tf.keras.layers.Dense(32, activation='relu'),\n  tf.keras.layers.Dense(32, activation='relu'),\n tf.keras.layers.Dense(16, activation='relu'),\n  tf.keras.layers.Dense(1)  \n])\n\nmodel_6.compile(loss=tf.keras.losses.mae,\n                          optimizer=tf.keras.optimizers.Adam(lr=0.001),\n                          metrics=[\"mae\"])\n\nhistory_6 = model_6.fit(X_train_normal,\n                        y_train,\n                        batch_size=32,\n                        epochs=2000)","27831ed3":"model_6_evaluate = model_6.evaluate(X_test_normal,y_test)","431d21c0":"print(\"Model_1\",model_1_evaluate)\nprint(\"Model_2\",model_2_evaluate)\nprint(\"Model_3\",model_3_evaluate)\nprint(\"Model_4\",model_4_evaluate)\nprint(\"Model_5\",model_5_evaluate)\nprint(\"Model_6\",model_6_evaluate)","87e8a4c8":"model_5.save(\"Final_Model_For_Prediction\")","019e8870":"# list of folder\n!ls Final_Model_For_Prediction","f5b39a6c":"loading_model = tf.keras.models.load_model(\"Final_Model_For_Prediction\")","966462bb":"loading_model.summary()","2f1ddea0":"model_5_preds = model_5.predict(X_test_normal)\nmodel_5_preds[:10]","c75ecdc4":"loading_model_preds = loading_model.predict(X_test_normal)\nloading_model_preds[:10]","56a6d008":"y_test[:10]","3d90647b":"**LETS BUILD A NEW MODEL WITH**\n\n\n\n1.   input activation=relu\n2.   Adam(learning_rate=0.001)\n3.   Batch_size = 32  \n\n\n\n\n","391b722c":"# **Comparing Model**","7cdc5064":"**ABOVE MODEL IS GREAT** \n\n> LETS EXPERIMENT MORE BY ADDING EXTRA LAYERS AND INCREASE THE NUMBER OF EPOCHS TO 500\n\n","69f62e51":"**comparing first 10 values with actual data with *1264.52990* loss value**\n\n","798fed5d":"Well seems like bad effect of adding more layers in model_6 \nof **EarlyStoppingcallback** where your model stop performing at some level of epochs and it doesn't matter how much epoch you run on a model to improve it more and more","94fc6e88":"Seems like model 4 and 5 are best model among all","95a63791":"# **model_5 is our Final Model**\n\n> SAVING MODEL 5 FOR DEPLOYMENT\n\n","a6c7b8ec":"# **LOADING MODEL FOR PREDICTION**"}}