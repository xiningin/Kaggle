{"cell_type":{"1332febf":"code","e4efb44e":"code","8a69bb33":"code","a111dc3a":"code","0ad460e7":"code","abafcfe4":"code","cd4fa097":"code","1ac75f81":"code","5b329f1a":"code","ec05a461":"code","20477717":"code","c792b5cd":"code","79fba833":"code","4af34249":"markdown","d36b8724":"markdown","729b3efb":"markdown","1da44c61":"markdown","26eb85d7":"markdown","6acedd94":"markdown","c65087b3":"markdown","3d23c36e":"markdown","fd424f4a":"markdown","d094c11e":"markdown","e13df913":"markdown","55a8a060":"markdown","8484a274":"markdown","644dfaf0":"markdown"},"source":{"1332febf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e4efb44e":"import string\nimport re","8a69bb33":"df = pd.read_csv('..\/input\/all-trumps-twitter-insults-20152021\/trump_insult_tweets_2014_to_2021.csv')\ntweets = df.tweet.tolist()\ndf.head(200)","a111dc3a":"print(f\"Number of tweets before removing dupplicates: {len(tweets)}\")\ntweets = list(set(tweets)) \nprint(f\"Number of tweets after removing dupplicates: {len(tweets)}\")\ntweets[0:10]\n\n","0ad460e7":"tweets = tweets[4000:]\n\nfor i in range(len(tweets)):\n    tweets[i] = tweets[i].lower()\n    tweets[i] = re.sub(r'http\\S+', '', tweets[i])\n    tweets[i] = re.sub(r'[^\\w\\s]', '', tweets[i]) \ntweets[0:20]","abafcfe4":"from tensorflow.keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(tweets)\ntotal_no_words = len(tokenizer.word_index) + 1\ntotal_no_words","cd4fa097":"input_tweet_sequences = []\nfor tweet in tweets:\n    tokenlist = tokenizer.texts_to_sequences([tweet])[0]\n    for i in range(1, len(tokenlist)):\n        ngram_sequence = tokenlist[:i+1]\n        input_tweet_sequences.append(ngram_sequence)\n        \nprint(input_tweet_sequences)","1ac75f81":"from tensorflow.keras.preprocessing.sequence import pad_sequences\n\nmax_input_sequence_len = max([len(x) for x in input_tweet_sequences])\ninput_tweet_sequences = np.array(pad_sequences(input_tweet_sequences, maxlen=max_input_sequence_len, padding='pre'))\n\nprint(max_input_sequence_len)\nprint(input_tweet_sequences[0:10])","5b329f1a":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nes = EarlyStopping(\n    monitor='val_loss', min_delta=0, patience=0, verbose=0,\n    mode='auto', baseline=None, restore_best_weights=False\n)\n\nmodel = Sequential()\nmodel.add(Embedding(total_no_words, 160, input_length=max_input_sequence_len-1))\nmodel.add(Bidirectional(LSTM(200, return_sequences=True)))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(100))\nmodel.add(Dense(total_no_words\/2, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\nmodel.add(Dense(total_no_words, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","ec05a461":"import tensorflow.keras.utils as ku \n\nmodel_predictors, output_label = input_tweet_sequences[:,:-1],input_tweet_sequences[:,-1]\noutput_label = ku.to_categorical(output_label, num_classes=total_no_words)\n\nhistory = model.fit(model_predictors, output_label, epochs=100, verbose=1, callbacks=[es])","20477717":"def Song_Generate(text, next_words):\n    for _ in range(next_words):\n        tokenlist = tokenizer.texts_to_sequences([text])[0]\n        tokenlist = pad_sequences([tokenlist],\n                     maxlen=max_input_sequence_len-1,padding='pre')\n        predicted = model.predict_classes(tokenlist, verbose=0)\n        output= \"\"\n        for word, index in tokenizer.word_index.items():\n            if index == predicted:\n                output = word\n                break\n        text += \" \" + output\n    print(text)","c792b5cd":"next_words = 30 \ntext = \"Democrats and Obama have developed\"\nSong_Generate(text, next_words)","79fba833":"import matplotlib.pyplot as plt\ndef plot_graph(history, string):\n    plt.plot(history.history[string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.show()\nplot_graph(history, 'accuracy')","4af34249":"# Preprocessing of data","d36b8724":"Providing all the sequence the same length.","729b3efb":"Arranging the words to be represented by numbers instead of string representations.","1da44c61":"For the preprocessing, i have used regular expression to remove puntuations and url tags. I have also changed tweets to lowercase. Taken about just last tweets to speeden up computations.","26eb85d7":"# Tokenization","6acedd94":"# Modeling","c65087b3":"Importing string and regular expression library","3d23c36e":"It's one of my first notebooks. Feel free to comment and leave some suggestion how could i improve it :D","fd424f4a":"Removing duplicates using set operation in pyhton.","d094c11e":"# Input Sequence","e13df913":"# Read data and put it to dataframe","55a8a060":"# Visualizing history of epochs","8484a274":"# Remove duplicate tweets","644dfaf0":"# Add padding"}}