{"cell_type":{"6c5b7dee":"code","7955280f":"code","1d3c21f3":"code","4547818f":"code","fab6852c":"code","7bc135b9":"code","9ca538f7":"code","8b1f11e4":"code","2b136e99":"code","10367a5a":"code","f64e8fbd":"code","30a6dd83":"code","72d27094":"code","9a943d0f":"code","dfdb78e3":"code","dfde6d84":"code","6400e8cb":"code","93ee211e":"code","a8c369df":"code","2ff9227b":"code","96447680":"code","234660d9":"markdown","9a5dae7c":"markdown","fa6bc578":"markdown","99b591a2":"markdown","b6ecf8fc":"markdown","a38e4461":"markdown","c593d5d4":"markdown","10a96174":"markdown","c561030b":"markdown","5704ccd1":"markdown"},"source":{"6c5b7dee":"# Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import models\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport os\nimport shutil\nimport sys","7955280f":"# Load dataset\ndataset_dir = '..\/input\/dog-breed-identification\/train'\nlabels = pd.read_csv('..\/input\/dog-breed-identification\/labels.csv')","1d3c21f3":"# Number of classes\nn_class = len(labels.breed.unique())\nn_class","4547818f":"# Distribution of classes in the train dataset\nimport seaborn as sns\ndata = labels.breed.value_counts()\n\nplt.figure(figsize=(17,7))\nsns.barplot(x=data.index, y=data.values, color='royalblue')\nplt.xticks(rotation=90)\nplt.show()","fab6852c":"import os\n\ndef make_dir(x):\n    if os.path.exists(x)==False:\n        os.makedirs(x)\n        \nbase_dir = '.\/subset'\nmake_dir(base_dir)","7bc135b9":"# Set up directories for the training data\ntrain_dir = os.path.join(base_dir, 'train')\nmake_dir(train_dir)","9ca538f7":"# Loop through images\nbreeds = labels.breed.unique()\nfor breed in breeds:\n    # Make folder for each breed\n    _ = os.path.join(train_dir, breed)\n    make_dir(_)\n    \n    # Copy images to the corresponding folders\n    images = labels[labels.breed == breed]['id']\n    for image in images:\n        source = os.path.join(dataset_dir, f'{image}.jpg')\n        destination = os.path.join(train_dir, breed,f'{image}.jpg')                             \n        shutil.copyfile(source, destination)","8b1f11e4":"# Set batch size 64 as global variable\nbatch_size = 64","2b136e99":"from tensorflow.keras.utils import image_dataset_from_directory\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndatagen = ImageDataGenerator(rescale=1.\/255) # rescale pixel values to [0,1] to reduce memory usage\n\ntrain_generator = datagen.flow_from_directory(\n    directory=train_dir,\n    target_size=(299, 299),\n    batch_size=batch_size,\n    class_mode='sparse',\n    seed=123)","10367a5a":"from tensorflow.keras.applications import InceptionResNetV2\n\ninception_bottleneck = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(299, 299, 3))","f64e8fbd":"# The shape of the features\nfeature_shape = inception_bottleneck.layers[-1].output_shape[1:]\nprint(f'The shape of each feature tensor is: {feature_shape}')\n\n# Height, width and depth of the tensor\nh = inception_bottleneck.layers[-1].output_shape[1]\nw = inception_bottleneck.layers[-1].output_shape[2]\nd = inception_bottleneck.layers[-1].output_shape[3]","30a6dd83":"# Number of training samples\ntrain_samples = len(labels)\n\n# Initialize tensors with zeros\nX_train = np.zeros(shape=(train_samples, h, w, d), dtype=np.float32) # specify dtype as float32\ny_train = np.zeros(shape=(train_samples))\n\n# Update the tensors with the features outputed by the bottleneck model\nlen_ = 0\nfor input_batch, label_batch in train_generator:\n    features_batch = inception_bottleneck.predict(input_batch)\n    X_train[len_:len_+len(features_batch)] = features_batch\n    y_train[len_:len_+len(features_batch)] = label_batch\n    len_+=len(features_batch)\n    if len_ == train_samples:\n        break","72d27094":"# Shape of the feature tensors -- 10222 samples, each with shape (8,8,1536)\nsize = sys.getsizeof(X_train)\/(1024*1024)\nprint(f'Shape: {X_train.shape}\\nSize: {size:.3g}MB')","9a943d0f":"# Flatten the feature tensors to 1D vectors \nX_train = np.reshape(X_train, (train_samples, h*w*d)) \nshape = X_train.shape\nprint(f'Shape: {shape}')","dfdb78e3":"# Build the final fully connected dense layers for classification\nmodel = models.Sequential()\nmodel.add(layers.Dense(512, activation='relu', input_dim=h*w*d))\nmodel.add(layers.Dropout(0.2))\nmodel.add(layers.Dense(512, activation='relu'))\nmodel.add(layers.Dropout(0.2))\nmodel.add(layers.Dense(n_class, activation='softmax')) # using softmax, the result could be interpreted in probability distribution\n\n# Compile model using the popular'adam' optimizer\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Print model summary\nmodel.summary()","dfde6d84":"# Train the model\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\nepochs = 50\n\nhistory = model.fit(X_train, y_train,\n         epochs=epochs,\n         batch_size=batch_size,\n         validation_split=0.1,\n         callbacks=[early_stop],\n         verbose=1)","6400e8cb":"# Plot result\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\n\n# Accuracy\nplt.plot(epochs, acc, label='Train accuracy')\nplt.plot(epochs, val_acc, label='Val accuracy')\nplt.title('Training & validation accuracy')\nplt.legend()\n\n# Loss\nplt.figure()\nplt.plot(epochs, loss, label='Train loss')\nplt.plot(epochs, val_loss, label='Val loss')\nplt.title('Training & validation loss')\nplt.legend()\nplt.show()","93ee211e":"import cv2\nfrom PIL import Image","a8c369df":"# Dictionary of breeds for decoding the one-hot-encoded prediction\nbreed_label = {j:i for i,j in train_generator.class_indices.items()}\n\n# Crop the image to square\ndef crop_center(pil_img):\n    img_w, img_h = pil_img.size\n    hw = min(pil_img.size)\n    return pil_img.crop(((img_w - hw) \/\/ 2,\n                         (img_h - hw) \/\/ 2,\n                         (img_w + hw) \/\/ 2,\n                         (img_h + hw) \/\/ 2))\n\n# Resize & reshape image array, extract features and return prediction\ndef predict(pil_img):\n    img_array = np.array(crop_center(pil_img))\n    image_array = cv2.resize(img_array, (299,299))\/255\n    image_array = np.reshape(image_array,(1,299,299,3))\n    features = inception_bottleneck.predict(image_array)\n    X = features.reshape((1,-1))\n    prediction = model.predict(X)\n    return breed_label[prediction.argmax()]","2ff9227b":"from PIL import Image\nimage = Image.open('..\/input\/dog-breed-identification\/test\/0012a730dfa437f5f3613fb75efcd4ce.jpg')","96447680":"# Predict\npredict(image)","234660d9":"## Model building with InceptionResNetV2","9a5dae7c":"First, we need to rearrange the images such that images of the same class are put in the same folder like the following. After the directory is set and done, we then can use Kera's flow_from_directory() method.","fa6bc578":"## Test the model","99b591a2":"I also made a simple Streamlit app to showcase the model!","b6ecf8fc":"![Screenshot 2021-11-07 at 8.02.37 PM.png](attachment:f019e80e-ab9b-4232-b5a1-6282ac583a52.png)","a38e4461":"Extract features by processing the images with the pretrained model. Since the last dense layer(s) of the pretrained model is not used, the output will be a set of intermediate feature tensors (hence the name bottleneck,) which will be put into our own dense layers for final output.","c593d5d4":"Loop through the photos and arrange them in the right directory structure","10a96174":"## Preprocessing","c561030b":"To begin with, let's create some helper functions","5704ccd1":"![image.png](attachment:fcc625a8-230f-4d31-9629-cf9a22d4ad10.png)"}}