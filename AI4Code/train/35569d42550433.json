{"cell_type":{"5b9559d1":"code","7ba9821f":"code","1264d3ed":"code","09865b63":"code","81eef56c":"code","9d982ccc":"code","394b82cd":"code","8a43cdad":"code","ee60effc":"code","ce4921fc":"code","db8676f1":"code","27a409ef":"code","ddd6519a":"code","b2ecbfb7":"code","a9adca2a":"code","9f2de34d":"code","22de0afd":"code","70d8d9a9":"code","3ea12d5c":"code","370fc2fb":"code","f1e3ad72":"code","2b3ae4a0":"code","9038bcb5":"code","0a727d88":"code","e629e246":"markdown","770a7689":"markdown","7599053e":"markdown","bed89861":"markdown","94730e54":"markdown","cc0959cf":"markdown","c26d6ced":"markdown","e63ab616":"markdown"},"source":{"5b9559d1":"import gc\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom datetime import datetime\nimport math\nimport mpmath\nimport torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nimport torch.nn as nn\nfrom copy import deepcopy\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom imblearn.under_sampling import NeighbourhoodCleaningRule","7ba9821f":"raw_train = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv')\nraw_test = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/test.csv')\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv')","1264d3ed":"device = \"cuda:0\" if torch.cuda.is_available() else 'cpu'\nBATCH_SIZE = 256\nEPOCHS = 15\nprint(device)","09865b63":"raw_train.describe()","81eef56c":"raw_test.describe()","9d982ccc":"categorical_cols = raw_train.select_dtypes(include=['object'])\nprint(f'Categorical Columns: {len(categorical_cols.columns)}')\nfor col in categorical_cols.columns:\n    print(f'{col}: {len(categorical_cols[col].unique())} unique labels')\n\nnumerical_cols = raw_train.select_dtypes(include=['number'])\nprint(f'\\nNumerical Columns: {len(numerical_cols.columns)}')\nfor col in numerical_cols.columns:\n    print(f'{col}: {len(numerical_cols[col].unique())}')","394b82cd":"# Memory saving function credit to https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    return df","8a43cdad":"def target_dist(df):\n    sns.set_style(\"whitegrid\")\n    palette = sns.color_palette(\"flare\")\n    plt.figure(figsize=(12, 7))\n    splot = sns.countplot(x=df.Cover_Type, palette=palette, orient=\"h\")\n    for p in splot.patches:\n        splot.annotate(format(p.get_height(), '.2f'), (p.get_x() + p.get_width() \/ 2., p.get_height()),\n                       ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n    plt.show()\n    \ntarget_dist(raw_train)","ee60effc":"raw_train = reduce_mem_usage(raw_train)\nraw_test = reduce_mem_usage(raw_test)","ce4921fc":"def feature_engineering_copied(train, test):\n    # Manhattan distance to Hydrology\n    train[\"mht_dist_hydrlgy\"] = np.abs(train.iloc[:, 3]) + np.abs(train.iloc[:, 4])\n    test[\"mht_dist_hydrlgy\"] = np.abs(test.iloc[:, 3]) + np.abs(test.iloc[:, 4])\n\n    # Clips hillshades 0 to 255 index.\n    hillshades = [col for col in train.columns if col.startswith('Hill')]\n    train[hillshades] = train[hillshades].clip(0, 255)\n    test[hillshades] = test[hillshades].clip(0, 255)\n\n    # Clips 'Aspect' 0 to 360 degrees.\n    col = 'Aspect'\n    train.loc[train[col] < 0, col] += 360\n    train.loc[train[col] > 359, col] -= 360\n    test.loc[test[col] < 0, col] += 360\n    test.loc[test[col] > 359, col] -= 360\n\n    return train, test","db8676f1":"gc.collect()\ny = raw_train.drop(index = int(np.where(raw_train[\"Cover_Type\"] == 5 )[0]))['Cover_Type']\nFEATURES = [col for col in raw_train.columns if col not in ['Id','Cover_Type']]\n\nuseless_feat = [col for col in raw_train.columns if len(raw_train[col].unique())<2]\nfor col in useless_feat:\n    if col in FEATURES:\n        FEATURES.remove(str(col))\n        \ntrain_df = pd.DataFrame(data=raw_train.drop(index = int(np.where(raw_train[\"Cover_Type\"] == 5 )[0]))[FEATURES],\n                        columns=FEATURES)\ntest_df = pd.DataFrame(data=raw_test[FEATURES], columns=FEATURES)\n\n# feature eng.\ntrain_df, test_df = feature_engineering_copied(train_df, test_df)\n# reduction \ntrain_df = reduce_mem_usage(train_df)\ntest_df = reduce_mem_usage(test_df)","27a409ef":"print(f'Count samples WRT classes: {dict(y.value_counts())}')\n\nncr_ = NeighbourhoodCleaningRule(sampling_strategy=[1,2], kind_sel=\"all\", n_jobs=-1)\ntrain_df_res, y_res = ncr_.fit_resample(train_df, y)\nprint(f'resampled data shape: ({train_df_res.shape}) ({y_res.shape})')\nprint(f'reduction from: {train_df.shape[0]} to: {train_df_res.shape[0]}, difference: {y.shape[0] - y_res.shape[0]}')","ddd6519a":"temp_df = deepcopy(train_df_res)\ntemp_df['target'] = y_res\ntemp_df.to_csv('cleaned.csv', index=False)","b2ecbfb7":"train_df_res = pd.read_csv('..\/input\/tpsdeccleanedfeateng\/cleaned.csv')\ny_res = train_df_res.loc[:,'target']\ntrain_df_res.drop(['target'], axis=1, inplace=True)\ntrain_df_res = reduce_mem_usage(train_df_res)","a9adca2a":"print(f'Count samples WRT resampled classes: {dict(y_res.value_counts())}')\n\ngc.collect()\nencoder_ = LabelEncoder()\nscaler_ = StandardScaler()\ny_res = encoder_.fit_transform(y_res)\nFEATURES = [col for col in train_df_res.columns]\ntrain_df_res[FEATURES] = scaler_.fit_transform(train_df_res[FEATURES])\ntest_df[FEATURES] = scaler_.transform(test_df[FEATURES])\n\nprint(f'{\"-\"*30}')\ntrain_df_res = reduce_mem_usage(train_df_res)\ntest_df = reduce_mem_usage(test_df)","9f2de34d":"class CustomDataset:\n    def __init__(self, X, y=None):\n        self.X = X\n        self.y = y\n    def __len__(self):\n        return len(self.X)\n    def __getitem__(self, idx):\n        if self.y is None:\n            return self.X[idx]\n        else:\n            return self.X[idx], self.y[idx]","22de0afd":"# takes in a module and applies the specified weight initialization\ndef weights_init_uniform_rule(m):\n    classname = m.__class__.__name__\n    if classname.find('Linear') != -1:\n        n = m.in_features\n        y = 1.0\/np.sqrt(n)\n        m.weight.data.uniform_(-y, y)\n        m.bias.data.fill_(0)","70d8d9a9":"def fc_block(in_f, out_f):\n        return nn.Sequential(\n            nn.Linear(in_f, out_f),\n            nn.ReLU(),\n            nn.BatchNorm1d(out_f),\n        )    \nclass Net(nn.Module):\n    def __init__(self, num_features, output_classes):\n        super(Net, self).__init__()\n        self.flatten = nn.Flatten()\n        self.fc1 = fc_block(num_features, 156)\n        self.fc2 = fc_block(156, 108)\n        self.fc3 = fc_block(108, 64)\n        self.fc4 = fc_block(64, 48)\n        self.out = nn.Linear(48, output_classes)\n        \n    def forward(self, x):\n        x = self.flatten(x)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        x = self.fc3(x)\n        x = self.fc4(x)\n        x = self.out(x)\n        return x\n\nnet_model = Net(len(FEATURES), len(set(y))).to(device)\nnet_model.apply(weights_init_uniform_rule)","3ea12d5c":"def multi_acc(y_pred, y_test):\n    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n    correct_pred = (y_pred_tags == y_test).float()\n    acc = correct_pred.sum() \/ len(correct_pred)\n    return acc*100","370fc2fb":"def batch_gd(model, train_loader, test_loader, epochs, val_score_best, lr_scheduler):\n    train_losses = np.zeros(epochs)\n    test_losses = np.zeros(epochs)\n    epochs_no_improve = 0\n    for it in range(epochs):\n        t0 = datetime.now()\n        model.train()\n        train_loss = []\n        train_acc = []\n        for batch_idx, (inputs, targets) in enumerate(train_loader):\n            # move data to GPU\n            inputs, targets = inputs.to(device), targets.to(device)\n            # zero the parameter gradients\n            optimizer.zero_grad()\n            # Forward pass\n            outputs = model(inputs)\n            # accuracy and loss\n            loss = criterion(outputs, targets)\n            acc_ = multi_acc(outputs, targets)\n            # Backward and optimize\n            loss.backward()\n            optimizer.step()\n            train_loss.append(loss.item())\n            train_acc.append(acc_)\n            \n        else:\n            model.eval()\n            test_loss = []\n            test_acc = []\n            for inputs, targets in test_loader:\n                inputs, targets = inputs.to(device), targets.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n                acc_ = multi_acc(outputs, targets)\n                test_loss.append(loss.item())\n                test_acc.append(acc_)\n            #get train and test loss\n            test_loss = np.mean(test_loss)\n            train_loss = np.mean(train_loss)\n            lr_scheduler.step(test_loss)\n            ###    \n            print('learning_rate: {}'.format(optimizer.state_dict()['param_groups'][0]['lr']))\n            # Save losses\n            train_losses[it] = train_loss\n            test_losses[it] = test_loss\n            test_acc = torch.FloatTensor(test_acc)\n            test_accuracy = torch.mean(test_acc)\n            train_acc = torch.FloatTensor(train_acc)\n            train_accuracy = torch.mean(train_acc)\n            # saving best weights\n            if test_loss < val_score_best:\n                epochs_no_improve = 0\n                val_score_best = test_loss\n                print(f'--- saving best weights ---')\n                torch.save(model.state_dict(), 'best_weights.pth')\n            else:\n                epochs_no_improve += 1\n            # getting the duration\n            dt = datetime.now() - t0\n            print(f'Epoch {it+1}\/{epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \\\n                    Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, Improvement: {epochs_no_improve}, Duration: {dt}')\n            if epochs_no_improve == 6:\n                print(f'Early Stopping..\\n')\n                break\n    return train_losses, test_losses","f1e3ad72":"# garbage collection\ngc.collect()\n# creating and loading test data\ntest_dataset = CustomDataset(torch.from_numpy(test_df.to_numpy()).to(torch.float32))\ntest_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE)\n# defining folds dictionary\nfolds_train_losses = {}\nfolds_test_losses = {}\n# test data predictions\ntest_predictions = []\n# defining skfolds\nskf = StratifiedKFold(n_splits=5, random_state=47, shuffle=True)\nfor fold, (train_idx, val_idx) in tqdm(enumerate(skf.split(train_df_res, y_res)), leave=False):\n    X_train, y_train = train_df_res.iloc[train_idx], y_res[train_idx]\n    X_val, y_val = train_df_res.iloc[val_idx], y_res[val_idx]\n    train_dataset = CustomDataset(X=torch.from_numpy(X_train.to_numpy()).to(torch.float32), y=torch.from_numpy(y_train).to(torch.long))\n    val_dataset = CustomDataset(X=torch.from_numpy(X_val.to_numpy()).to(torch.float32), y=torch.from_numpy(y_val).to(torch.long))\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n        \n    criterion = nn.CrossEntropyLoss(ignore_index=-1)    \n    optimizer = torch.optim.AdamW(net_model.parameters(), lr=1e-3)\n    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=1, \n                                                          verbose=True, min_lr=1e-7, mode='min')    \n    # training and validation\n    val_score_best = math.inf\n    train_losses, test_losses = batch_gd(net_model, train_loader, val_loader, EPOCHS, val_score_best, lr_scheduler)\n    folds_train_losses[fold] = train_losses\n    folds_test_losses[fold] = test_losses\n    \n    # loading best weights\n    print(f'--- loading best weights ---')\n    net_model.load_state_dict(torch.load('best_weights.pth'))\n    \n    # prediction on test data\n    test_preds = []\n    net_model.eval()\n    with torch.no_grad():\n        for idx, batch_tensor in enumerate(test_loader):\n            batch_tensor = batch_tensor.to(device)\n            y_test_pred = net_model(batch_tensor)\n            _, y_pred_tags = torch.max(y_test_pred, dim = 1)\n            test_preds.extend((y_pred_tags.cpu().numpy()))\n    test_predictions.append(test_preds)\n    gc.collect()","2b3ae4a0":"predictions_df = pd.DataFrame(data=(np.array(test_predictions).astype(np.float32)).T, columns=['p1','p2','p3','p4','p5'])\npredictions_df.to_csv('predictions_5fold.csv', index=None)","9038bcb5":"predictions_df['stack'] = predictions_df.mean(axis=1)\npredictions_df['stack'] = predictions_df['stack'].round(0)\nsubmission['Cover_Type'] = encoder_.inverse_transform(predictions_df['stack'].to_numpy().astype(np.int32))","0a727d88":"submission['Cover_Type'] = encoder_.inverse_transform(predictions_df['p4'].to_numpy().astype(np.int16))\nsubmission.to_csv('submission.csv',index=None)","e629e246":"weight initialization based on uniform-rule.","770a7689":"# Imports","7599053e":"# Model and Training","bed89861":"Credits:\n* @chryzal [https:\/\/www.kaggle.com\/chryzal\/features-engineering-for-you\/notebook](http:\/\/)\n* @gulshanmishra [https:\/\/www.kaggle.com\/gulshanmishra\/tps-dec-21-tensorflow-nn-feature-engineering](http:\/\/)\n* @sergiosaharovskiy [https:\/\/www.kaggle.com\/sergiosaharovskiy\/tps-dec-2021-a-complete-guide-eda-pytorch#5.-Feature-engineering](http:\/\/)","94730e54":"saving and loading the feature engineered data into a csv, since kaggle kernel keeps restarting due to memory issues.","cc0959cf":"# Basic EDA and Feature Engineering","c26d6ced":"# Submission","e63ab616":"This approach is one the combined-undersampling approaches, and uses ENN and kNN to for removal of noisy samples. I've been experimenting with sampling strategies and haven't found any robust approach. The strategies I've used (including this one), more or less give me the same test-score."}}