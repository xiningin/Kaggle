{"cell_type":{"79185b61":"code","d340b649":"code","fb1c7d73":"code","f672fb35":"code","f0713aaf":"code","c9370ae4":"code","e145bbd0":"code","303b7ce4":"code","e3b696eb":"code","fbef3d42":"code","e2ea72fb":"code","3dcd3207":"code","d51c512f":"code","a71e9dee":"code","32f0a38e":"code","d490df78":"code","8bcaa547":"code","b118fffe":"code","9b580de0":"code","0b5de69a":"code","0671e159":"code","78c041bd":"code","203a06d0":"code","70cfaacd":"code","6781474d":"code","0c31dd65":"markdown","91ed0d47":"markdown","ddbc07d1":"markdown","847e592b":"markdown","cfbb1643":"markdown","f951a4b1":"markdown","543e547c":"markdown","22fadbe4":"markdown","7d2d6790":"markdown","20e669c6":"markdown"},"source":{"79185b61":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d340b649":"# Importando a base\n\ndf = pd.read_csv('..\/input\/churn-modelling\/Churn_Modelling.csv')\n                 \ndf.shape","fb1c7d73":"# Visualizando os dados\n\ndf.head()","f672fb35":"# Visualizando tipos de dados\n\ndf.info()","f0713aaf":"# An\u00e1lise de valores \u00fanicos das vari\u00e1veis object\n\nprint('Surname: ',df['Surname'].nunique(),'\\nGeography: ',df['Geography'].nunique(),'\\nGender: ',df['Gender'].nunique())","c9370ae4":"# Drop de vari\u00e1veis com alta cardinalidade ou vari\u00e1veis de contagem\ndf.drop(columns=['RowNumber', 'CustomerId', 'Surname'],inplace=True)\ndf.head()","e145bbd0":"# Visualiza\u00e7\u00e3o das variaveis no dataset\nax = df.hist(bins=25, grid=False, figsize=(18,18), color='#1DB954', zorder=2, rwidth=0.9)","303b7ce4":"# Verificando a feature Estimated Salary\ndf.EstimatedSalary.describe()","e3b696eb":"# Boxplot Estimated Salary\n\nplt.figure(figsize=(8,8))\nax = sns.boxplot(y=df[\"EstimatedSalary\"],color='#1DB954')\nplt.title(\"Estimated Salary\")\nplt.show()","fbef3d42":"# Criar variaveis Dummy para Gender e Geography\n\ndata = pd.concat([pd.get_dummies(df[['Gender', 'Geography']]), df[['CreditScore', 'Age', 'Tenure', 'Balance',\n       'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary','Exited']]], axis=1)","e2ea72fb":"# Visualizando dados de forma transposta\ndata.head().T","3dcd3207":"# Treinamento do modelo\n\n# Separando o dataframe\n\n# Importando o train_test_split\nfrom sklearn.model_selection import train_test_split\n\n# Separando treino e teste\ntrain, test = train_test_split(data, test_size=0.20, random_state=42)\n\n# Separando treino e valida\u00e7\u00e3o\ntrain, valid = train_test_split(train, test_size=0.20, random_state=42)\n\ntrain.shape, valid.shape, test.shape","d51c512f":"# definindo colunas de entrada\nfeats = [c for c in data.columns if c not in ['Exited']]\n\nfeats","a71e9dee":"# treinar o modelo\n\n# Importando o modelo\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Instanciar o modelo\nrf = RandomForestClassifier(n_estimators=200, random_state=42)","32f0a38e":"# treinar o modelo RandomForestClassifier\nrf.fit(train[feats], train['Exited'])","d490df78":"# Prevendo os dados de valida\u00e7\u00e3o\npreds_val = rf.predict(valid[feats])\n\npreds_val","8bcaa547":"# Avaliando o desempenho do modelo\n\n# Importando a metrica\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nimport scikitplot as skplt","b118fffe":"# Medindo a acur\u00e1cia nos dados de teste\npreds_test = rf.predict(test[feats])\n\nprint(\"Acur\u00e1cia:\",accuracy_score(test['Exited'], preds_test))","9b580de0":"# Matriz de Confus\u00e3o com os dados de teste\nprint(\"Matriz de Confus\u00e3o:\",confusion_matrix(test['Exited'], preds_test))","0b5de69a":"# Matriz de Confus\u00e3o com os dados de teste de forma gr\u00e1fica\nskplt.metrics.plot_confusion_matrix(test['Exited'], preds_test)","0671e159":"# Olhando para vari\u00e1vel target no dataset\ntest['Exited'].value_counts(normalize=True)","78c041bd":"test['Exited'].value_counts()","203a06d0":"# Importando outros modelos\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score","70cfaacd":"def find_best(x,y,cv):\n    \n    ## instanciando modelos\n    log = LogisticRegression()\n    dt = DecisionTreeClassifier()\n    \n    ## gerando lista de modelos\n    list_of_models = [log,dt,rf]\n    \n    ## gerando lista de scores\n    dc_of_scores = {}\n    \n    ## iterando sobre modelos e executando a valida\u00e7\u00e3o\n    for idx,model in enumerate(list_of_models):\n        dc_of_scores[idx] = cross_val_score(model,x,y,cv=cv,scoring='precision').mean()\n    \n    best_model = max(dc_of_scores,key=dc_of_scores.get)\n    \n    print('xxx Encontrando melhor modelo xxx\\n')\n    if best_model == 0:\n        print(f'Melhor modelo: LogisticRegression\\nPrecis\u00e3o : {dc_of_scores[0]}')\n    \n    elif best_model == 1:\n        print(f'Melhor modelo: DecisionTreeClassifier\\nPrecis\u00e3o : {dc_of_scores[1]}')\n        \n    elif best_model == 2:\n        print(f'Melhor modelo: RandomForestClassifier\\nPrecis\u00e3o : {dc_of_scores[2]}')\n\n    print(f'\\nLogisticRegression\\nPrecis\u00e3o : {dc_of_scores[0]}')\n    print(f'DecisionTreeClassifier\\nPrecis\u00e3o : {dc_of_scores[1]}')\n    print(f'RandomForestClassifier\\nPrecis\u00e3o : {dc_of_scores[2]}')","6781474d":"find_best(data[feats],data['Exited'],10)","0c31dd65":"# Vari\u00e1veis Object","91ed0d47":"O dataset cont\u00e9m caracter\u00edsticas dos clientes de um banco. O nosso objetivo \u00e9 desenvolver um modelo para predi\u00e7\u00e3o de quando o cliente deixar\u00e1 o banco. A vari\u00e1vel target se chama \" Exited \", \u00e9 bin\u00e1ria : 0 quando o cliente ainda permanece no banco e 1 quando ele deixou de ser cliente.","ddbc07d1":"# M\u00e9tricas do modelo Random Forest Classifier","847e592b":"Dos 393 clientes que deixaram o banco, o nosso modelo conseguiu detectar 179 ( na parti\u00e7\u00e3o teste )","cfbb1643":"# Modelo de Classifica\u00e7\u00e3o - Vari\u00e1vel Target Bin\u00e1ria","f951a4b1":"# Problema de Gest\u00e3o","543e547c":"# Base","22fadbe4":"# Random Forest Classifier","7d2d6790":"# Compara\u00e7\u00e3o de modelos de classifica\u00e7\u00e3o - Cross Val Score","20e669c6":"No caso desse problema de gest\u00e3o, a m\u00e9trica usada para avialar qual melhor modelo foi PRECIS\u00c3O.\nPor qu\u00ea? Porque essa m\u00e9trica mostra qu\u00e3o bem o modelo consegue captar os verdadeiros positivos. \nO objetivo \u00e9 encontrar quem est\u00e1 propenso a sair do banco (Exited == 1). \nA acur\u00e1cia mede uma perfomance geral. Nesse caso, foi necess\u00e1rio analisar precis\u00e3o tamb\u00e9m.\n\nPara sele\u00e7\u00e3o de melhor modelo foi usado o cross_val_score, que mostrou que o Random Forest Classifier \u00e9 que possui a maior precis\u00e3o entre Random Forest, Decision Tree e Logistic Regression."}}