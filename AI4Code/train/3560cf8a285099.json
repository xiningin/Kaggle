{"cell_type":{"f5bd79f6":"code","392874c3":"code","c245ed99":"code","c1f2ab7b":"code","a3ffd2cd":"code","d68da9b5":"code","186352d4":"code","500cccb3":"code","dd7342b2":"code","7b500282":"code","5afbbf55":"code","698e3cfd":"code","7fc35d39":"code","f3bb5528":"code","f755d620":"code","e1b6079b":"code","806d8308":"code","8f0e67ba":"code","c246994f":"code","696ce252":"code","2a6eabeb":"code","af39baba":"code","41a04290":"code","26d93e93":"markdown","1472e7ff":"markdown","80e14157":"markdown","edb0ccca":"markdown","8237cb80":"markdown","39b928cf":"markdown","2af81f0c":"markdown","5e776d1b":"markdown","4ecb5635":"markdown","d7580451":"markdown","1806f278":"markdown","a07c9668":"markdown","b73a9168":"markdown","520e5b3d":"markdown","876c72c4":"markdown","1eeb079c":"markdown","0fd0c7e4":"markdown"},"source":{"f5bd79f6":"import numpy as np\nimport pandas as pd\n\n\nfrom os.path import join\n\nfrom collections import defaultdict\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport cv2\n\nfrom tqdm import tqdm\n\nimport plotly\nfrom plotly import graph_objects as go\nimport plotly.express as px\n","392874c3":"DATA_DIR = join('..', 'input')\n\nTRAIN_IMG_DIR = join(DATA_DIR, 'train_images')\nTEST_IMG_DIR = join(DATA_DIR, 'test_images')\nSAMPLE_SUB = join(DATA_DIR, 'sample_submission.csv')\nTRAIN_DATA = join(DATA_DIR, 'train.csv')\nmodel_save_path = join('.', 'ResUNetSteel_z.h5')\npretrained_model_path = join('..', 'input', 'severstal-pretrained-model', 'ResUNetSteel_z.h5')\n\n\ntrain_df = pd.read_csv(TRAIN_DATA)\ntrain_df['ImageId_ClassId'] = train_df.apply(lambda x: '{}_{}'.format(x.ImageId, x.ClassId), axis=1)\nsub_df = pd.read_csv(SAMPLE_SUB)\nsave_model = True\n\n\n# Kernel Configurations\nmake_submission = False # used to turn off lengthy model analysis so a submission version doesn't run into memory error\nload_pretrained_model = True # load a pre-trained model","c245ed99":"train_df.head()","c1f2ab7b":"class_counts = {}\nclass_to_images = {}\nfor name, group in train_df.groupby(by='ClassId'):\n    class_to_images[name] = group['ImageId'].tolist()\n    class_counts[name] = len(group)\n\nfig, ax = plt.subplots()\nsns.barplot(x=list(class_counts.keys()), y=list(class_counts.values()), ax=ax)\nax.set_title(\"Number of images for each class\")\nax.set_xlabel(\"class\")\nclass_counts","a3ffd2cd":"import os\n\nunique_labeled_imgs = set(train_df.ImageId.unique())\nunique_imgs = set(os.listdir(TRAIN_IMG_DIR))\n\nclass_to_images['no_damage'] = list(unique_imgs.difference(unique_labeled_imgs))\n\nprint('Number of all images:', len(unique_imgs))\nprint('Number of labeled images:', len(unique_labeled_imgs))\nprint('Number of unlabeled images:', len(unique_imgs) - len(unique_labeled_imgs))\nprint('Number of entries in the dataset that does not have an image', len(unique_labeled_imgs.difference(unique_imgs)))\nprint('Number of images that does not have an entry', len(unique_imgs.difference(unique_labeled_imgs)))\nprint('Number of test images:', len(sub_df))","d68da9b5":"def load_img(img_id):\n    img = cv2.imread(join(TRAIN_IMG_DIR, img_id))\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n    return img","186352d4":"rgb_for_label = {i:v for i, v in enumerate([(249, 192, 12), (0, 185, 241), (114, 0, 218), (249,50,12)], start=1)}\n\n    \nfig, ax = plt.subplots(1, 4, figsize=(15, 5))\nfor i in range(0, 4):\n    ax[i].axis('off')\n    ax[i].imshow(np.ones((50, 50, 3), dtype=np.uint8) * rgb_for_label[i+1])\n    ax[i].set_title(\"class color: {}\".format(i+1))\nfig.suptitle(\"Colors for the classes\")\n\nplt.show()","500cccb3":"# a more elaborate version of kaggle.com\/paulorzp\/rle-functions-run-lenght-encode-decode\n# note that we will transpose the incoming array outside of the function, \n# as I find this a clearer illustration\n\ndef mask_to_rle(mask):\n    \"\"\"\n    params:  mask - numpy array\n    returns: run-length encoding string (pairs of start & length of encoding)\n    \"\"\"\n    \n    # turn a n-dimensional array into a 1-dimensional series of pixels\n    # for example:\n    #     [[1. 1. 0.]\n    #      [0. 0. 0.]   --> [1. 1. 0. 0. 0. 0. 1. 0. 0.]\n    #      [1. 0. 0.]]\n    flat = mask.flatten()\n    \n    # we find consecutive sequences by overlaying the mask\n    # on a version of itself that is displaced by 1 pixel\n    # for that, we add some padding before slicing\n    padded = np.concatenate([[0], flat, [0]])\n    \n    # this returns the indices where the sliced arrays differ\n    runs = np.where(padded[1:] != padded[:-1])[0] \n    # indexes start at 0, pixel numbers start at 1\n    runs += 1\n\n    # every uneven element represents the start of a new sequence\n    # every even element is where the run comes to a stop\n    # subtract the former from the latter to get the length of the run\n    runs[1::2] -= runs[0::2]\n \n    # convert the array to a string\n    return ' '.join(str(x) for x in runs)","dd7342b2":"def rle_to_mask(lre, shape=(1600, 256)):\n    '''\n    params:  rle   - run-length encoding string (pairs of start & length of encoding)\n             shape - (width,height) of numpy array to return \n    \n    returns: numpy array with dimensions of shape parameter\n    '''    \n    # the incoming string is space-delimited\n    runs = np.asarray([int(run) for run in lre.split(' ')])\n    \n    # we do the same operation with the even and uneven elements, but this time with addition\n    runs[1::2] += runs[0::2]\n    # pixel numbers start at 1, indexes start at 0\n    runs -= 1\n    \n    # extract the starting and ending indeces at even and uneven intervals, respectively\n    run_starts, run_ends = runs[0::2], runs[1::2]\n    \n    # build the mask\n    h, w = shape\n    mask = np.zeros(h*w, dtype=np.uint8)\n    for start, end in zip(run_starts, run_ends):\n        mask[start:end] = 1\n    \n    # transform the numpy array from flat to the original image shape\n    return mask.reshape(shape).T","7b500282":"def show_masked_image(img_id, ax=None, thickness=2):\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(15, 5))\n    \n    img = load_img(img_id)\n    for i, col in train_df[train_df['ImageId'] == img_id].iterrows():\n        encoded_pixels = col['EncodedPixels']\n        label = col['ClassId']\n        mask = rle_to_mask(encoded_pixels, shape=(1600, 256))\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n        img = cv2.drawContours(img, contours, -1, rgb_for_label[label], thickness=thickness)\n    ax.imshow(img)\n    return ax\n\ndef show_masked_images_per_class(label, num_images=5):\n\n    num_imgs = 5\n    fig, axs = plt.subplots(nrows=num_imgs, ncols=1, figsize=(15, 15))\n    axs = axs.ravel()\n    \n    image_ids = np.asarray(class_to_images[label])\n    random_ids = list(np.random.randint(len(image_ids), size=num_imgs))\n    \n    for i, img_id in enumerate(image_ids[random_ids]):\n        show_masked_image(img_id, ax=axs[i])\n        ","5afbbf55":"show_masked_images_per_class(1, num_images=5)","698e3cfd":"show_masked_images_per_class(2, num_images=5)","7fc35d39":"show_masked_images_per_class(3, num_images=5)","f3bb5528":"show_masked_images_per_class(4, num_images=5)","f755d620":"show_masked_images_per_class('no_damage', num_images=5)","e1b6079b":"# calculate sum of the pixels for the mask per class id\ntrain_df['ClassId_str'] = train_df['ClassId'].astype(str)\n\ntrain_df['mask_pixel_sum'] = train_df.apply(lambda x: rle_to_mask(x['EncodedPixels']).sum(), axis=1)\n\nclass_ids = [str(i) for i in range(1, 5)]\nmask_count_per_class = [train_df[(train_df['ClassId_str']==class_id)&(train_df['mask_pixel_sum']!=0)]['mask_pixel_sum'].count() for class_id in class_ids]\npixel_sum_per_class = [train_df[(train_df['ClassId_str']==class_id)&(train_df['mask_pixel_sum']!=0)]['mask_pixel_sum'].sum() for class_id in class_ids]","806d8308":"# Create subplots: use 'domain' type for Pie subplot\nfig = plotly.subplots.make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])\n\nfig.add_trace(go.Pie(labels=class_ids, values=mask_count_per_class, name=\"Mask Count\"), 1, 1)\nfig.add_trace(go.Pie(labels=class_ids, values=pixel_sum_per_class, name=\"Pixel Count\"), 1, 2)\n# Use `hole` to create a donut-like pie chart\nfig.update_traces(hole=.4, hoverinfo=\"label+percent+name\")\n\nfig.update_layout(\n    title_text=\"Steel Defect Mask & Pixel Count\",\n    # Add annotations in the center of the donut pies.\n    annotations=[dict(text='Count', x=0.18, y=0.5, font_size=20, showarrow=False),\n                 dict(text='Sum', x=0.80, y=0.5, font_size=20, showarrow=False)])\nfig['layout'].update(height=400, width=900, title='Pixel count and sum per class mask', legend={'traceorder':'normal'})\nfig.show()","8f0e67ba":"# plot a histogram and boxplot combined of the mask pixel sum per class Id\nfig = px.histogram(train_df[train_df['mask_pixel_sum']!=0][['ClassId','mask_pixel_sum']], \n                   x=\"mask_pixel_sum\", y=\"ClassId\", color=\"ClassId\", marginal=\"box\")\n\nfig['layout'].update(height=400, width=900,title='Histogram and Boxplot of Sum of Mask Pixels Per Class')\nfig.show()","c246994f":"def count_segments(mask):\n    \"\"\"Given a mask, count the number of regions.\n\n    Parameters:\n    mask (numpy.array): numpy array of the mask\n\n    Returns:\n    int: number of segments\n    \"\"\"\n    # if the mask is empty return zero\n    if mask.sum() == 0:\n        return 0\n    else:\n        # use open cv and threshold mechanism to calculate contours\n        _, contours = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n        \n        # get the segment count\n        for c in contours:\n            segments_count = len(c)\n\n        return segments_count","696ce252":"# use the count_segments function to conver encoded mask to mask and count the number of segments per defect\ntrain_df['segments'] = train_df.apply(lambda r: count_segments(rle_to_mask(r['EncodedPixels'])),axis=1)\n\n# use the count_segments function to conver encoded mask to mask and count the number of segments per defect\ntrain_df['avg_mask_per_seg'] = (train_df['mask_pixel_sum'] \/ train_df['segments']).fillna(0)","2a6eabeb":"\nfig = px.scatter(train_df[train_df['mask_pixel_sum']!=0], x=\"mask_pixel_sum\", y=\"segments\", color='ClassId_str', size=\"avg_mask_per_seg\", hover_data=[\"avg_mask_per_seg\"])\nfig['layout'].update(height=800, width=800,title='')\nfig.show()","af39baba":"fig = px.scatter(train_df[train_df['mask_pixel_sum']!=0], x=\"mask_pixel_sum\", y=\"segments\", color=\"ClassId_str\", marginal_y=\"rug\", marginal_x=\"histogram\")\nfig['layout'].update(height=800, width=800,title='')\nfig.show()","41a04290":"sample_size = 100\nimage_shapes = train_df.iloc[np.random.randint(len(train_df), size=sample_size), :].ImageId.apply(lambda x: load_img(x).shape)\nprint(len(image_shapes.unique()))\nprint(image_shapes.iloc[0])\n\nimage_shape = IMAGE_X, IMAGE_Y, IMAGE_CHANNELS = image_shapes.iloc[0]","26d93e93":"### Class 1","1472e7ff":"### Class 4","80e14157":"You can clearly notice a class imballance present in the dataset. Lets check how many of the images does not have labels at all.","edb0ccca":"\n## Run-Length Encoding\n> In order to reduce the submission file size, our metric uses run-length encoding on the pixel values. Instead of submitting an exhaustive list of indices for your segmentation, you will submit **pairs of values** that contain a **start position and a run length**. E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).\n>\n>The competition format requires a **space delimited list of pairs**. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The metric checks that the pairs are **sorted, positive, and the decoded pixel values are not duplicated**. The pixels are numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.\n\nSo, if we were to encode something like our example above, we would have to write it as follows:","8237cb80":"# Summary\nTaking into the account the class imballances and the size of the mask patches it seems like the regular crossentropy loss will not do a good job on this dataset.\n\n![image.png](https:\/\/i.ibb.co\/bHPQrmR\/image-2021-01-15-150827.png)\nSource: https:\/\/arxiv.org\/pdf\/2006.14822.pdf\n","39b928cf":"# Introduction\nThis competition is about identifying steel defects by the means of *semantic segmentation*. \n\nIn this notebook we are going to perform an Exploratory Data Analysis (EDA).","2af81f0c":"### Segments Per Defect Type\nWhen we visualize the defects, we can see that per defect we can have multiple regions in our image with the same kind of defect. In this section we find out the number of segments behave for different class of defects.","5e776d1b":"You might notice that these defects differ a lot in the size and number.","4ecb5635":"There are a lot of images without any defect masks. We are not sure if these images just missing labels or these are examples of instances without any defect. We visualize couple of images per each class to get a better understanding what kind of data is there. We are going to use different collors for each of the deffect classes.","d7580451":"# Main Configuration","1806f278":"<a id=\"1\"><\/a> <br>\n\n## Prediction Output Format\nThe following explanation is taken from here: https:\/\/www.kaggle.com\/robinteuwens\/mask-rcnn-detailed-starter-code\n\nFrom the competition's [data](https:\/\/www.kaggle.com\/c\/severstal-steel-defect-detection\/data) page:\n> Each image may have no defects, a defect of a single class, or defects of multiple classes. For each image you must segment defects of each class ```(ClassId = [1, 2, 3, 4])```.\n\nThe submission format requires us to make the classifications for each respective class on a separate row, adding the *_class* to the imageId:\n![format](https:\/\/i.imgur.com\/uEeoOQg.png)\n\n## Loss Function\n\n### Dice Coefficient\nFrom the [evaluation](https:\/\/www.kaggle.com\/c\/severstal-steel-defect-detection\/overview\/evaluation) page:\n\n> This competition is evaluated on the mean Dice coefficient. The Dice coefficient can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by:\n>\n>$$Dice(X,Y) = \\frac{2\u2217|X\u2229Y|}{|X|+|Y|}$$\n>\n>\n>where X is the predicted set of pixels and Y is the ground truth. The Dice coefficient is defined to be 1 when both X and Y are empty. The leaderboard score is the mean of the Dice coefficients for each ```<ImageId, ClassId>``` pair in the test set.\n\nVisual illustration of the Dice Coefficient:\n![dice_viz](https:\/\/i.imgur.com\/zl2W0xQ.png)\n\n","a07c9668":"### Class 2","b73a9168":"### Class 3","520e5b3d":"# Exploratory Data Analysis\nThe training data is presented in the following format:","876c72c4":"### Images with no defect","1eeb079c":"From the box plot we can reconfirm our previous observation of class 4 are generally larger in size than class 3, and of course class 1 and 2. Defect class 3 has a lot of outliers. Even though class 4 is generally bigger in size, the outlier values in class 3 can be a lot larger than the ones in class 4!\n\n","0fd0c7e4":"### Mask Size Per Defect Class\nSince we have binary mask, we will count the number of pixels we have in our mask to get some sort of approximation for the size of defect per class, and look how this varies from class to class."}}