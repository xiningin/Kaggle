{"cell_type":{"25087912":"code","97bb3d00":"code","766f3d27":"code","ef1f8e06":"code","3537d6ef":"code","5bda70ed":"code","a7c1b99d":"code","c6d998ac":"code","3d460d59":"code","0fed16b4":"code","63656405":"code","0d4ff190":"code","6241079c":"code","8d022be1":"code","a9cd96e1":"code","cd4e446a":"code","554dd768":"code","a57932c6":"code","4639331f":"code","41a90311":"code","a86fed48":"code","e57524cf":"code","709da251":"code","023e0ba0":"code","f93006df":"code","3ab79f55":"code","18c5a2e8":"code","fa3bb558":"code","1c829776":"code","78a6dc67":"code","5036b033":"code","db303417":"code","43437ca6":"code","9ee462b9":"code","cc8da82f":"code","70b4f709":"code","28ab9f1e":"code","55a06e15":"code","43cd5a13":"code","e9d13d91":"code","3ec5113c":"code","2b8be6f4":"code","825005c7":"code","48ef14fc":"code","92b2d38c":"code","ce773af7":"code","1e654107":"code","ee3def67":"code","77f44fb0":"code","5e5ba15f":"code","f6eb9229":"markdown","5ab20a1d":"markdown","57e1933c":"markdown","240950e8":"markdown","1602b770":"markdown","ff819b18":"markdown","441fa782":"markdown","c11310f4":"markdown","378fdac9":"markdown","e51bcfb4":"markdown","43c49202":"markdown","022eba52":"markdown","df3fc766":"markdown","b4f09b5e":"markdown","28f1932c":"markdown","caf89460":"markdown","3d3ae9d5":"markdown","6f29c1f5":"markdown","798f16ed":"markdown","708d0a26":"markdown","02a6989f":"markdown","e31e8536":"markdown","44c3a12e":"markdown","84d86575":"markdown","77e54d55":"markdown","ec7a0d60":"markdown","b4b8a2ba":"markdown","cc29d6c3":"markdown","e6749531":"markdown","22faa645":"markdown","2d1ffe3b":"markdown"},"source":{"25087912":"# Importing all the required Packages\nimport os\nimport pickle\nimport re\nimport string\nfrom tqdm import tqdm\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import sparse\nfrom gensim.models import Word2Vec\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.corpus import stopwords\nfrom textblob import TextBlob\nfrom wordcloud import WordCloud\nimport plotly.offline as py\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.figure_factory as ff\ninit_notebook_mode(connected=True)\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.manifold import TSNE\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport warnings\nwarnings.filterwarnings('ignore')","97bb3d00":"# Reading the text data present in the directories. Each review is present as text file.\nif not (os.path.isfile('..\/input\/end-to-end-text-processing-for-beginners\/train.csv' and \n                       '..\/input\/end-to-end-text-processing-for-beginners\/test.csv')):\n    path = '..\/input\/imdb-movie-reviews-dataset\/aclimdb\/aclImdb\/'\n    train_text = []\n    train_label = []\n    test_text = []\n    test_label = []\n    train_data_path_pos = os.path.join(path,'train\/pos\/')\n    train_data_path_neg = os.path.join(path,'train\/neg\/')\n\n    for data in ['train','test']:\n        for label in ['pos','neg']:\n            for file in sorted(os.listdir(os.path.join(path,data,label))):\n                if file.endswith('.txt'):\n                    with open(os.path.join(path,data,label,file)) as file_data:\n                        if data=='train':\n                            train_text.append(file_data.read())\n                            train_label.append( 1 if label== 'pos' else 0)\n                        else :\n                            test_text.append(file_data.read())\n                            test_label.append( 1 if label== 'pos' else 0)\n\n    train_df = pd.DataFrame({'Review': train_text, 'Label': train_label})\n    test_df = pd.DataFrame({'Review': test_text, 'Label': test_label})\n    train_df = train_df.sample(frac=1).reset_index(drop=True)\n    test_df = test_df.sample(frac=1).reset_index(drop=True)\n    \n    train_df.to_csv('train.csv')\n    test_df.to_csv('test.csv')\n    \nelse:\n    train_df = pd.read_csv('..\/input\/end-to-end-text-processing-for-beginners\/train.csv',index_col=0)\n    test_df = pd.read_csv('..\/input\/end-to-end-text-processing-for-beginners\/test.csv',index_col=0)\n    \nprint('The shape of train data:',train_df.shape)\nprint('The shape of test data:', test_df.shape)    ","766f3d27":"print('The first 5 rows of data:')\ntrain_df.head()","ef1f8e06":"# Basic info\ntrain_df.info()","3537d6ef":"# Removing the duplicate rows\ntrain_df_nodup = train_df.drop_duplicates(keep='first',inplace=False)\ntest_df_nodup = test_df.drop_duplicates(keep='first',inplace=False)\nprint('No of duplicate train rows that are dropped:',len(train_df)-len(train_df_nodup))\nprint('No of duplicate test rows that are dropped:',len(test_df)-len(test_df_nodup))","5bda70ed":"# Defining Functions for Polarity, Subjectivity and Parts of Speech counts\nstop_words = stopwords.words('english')\n\ndef get_polarity(text):\n    try:\n        tb = TextBlob(str(text))\n        polarity = tb.sentiment.polarity\n    except:    \n        polarity = 0.0\n    return polarity    \n\ndef get_subjectivity(text):\n    try:\n        tb = TextBlob(str(text))\n        subjectivity = tb.sentiment.subjectivity\n    except:    \n        subjectivity = 0.0\n    return subjectivity\n\n\npos_dict = {\n    'noun' : ['NN','NNS','NNP','NNPS'],\n    'pronoun' : ['PRP','PRP$'],\n    'adjective' : ['JJ','JJR','JJS'],\n    'adverb' : ['RB','RBR','RBS','WRB'],\n    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ']\n           }\n\n\ndef get_pos(text,pos):\n    count = 0\n    try:\n        tokens = nltk.word_tokenize(text)\n        for tag in nltk.pos_tag(tokens):\n            if tag[1] in pos_dict[pos]:\n                count += 1\n        return count   \n    except:\n        pass","a7c1b99d":"# Function for extracting features\ndef extract_text_features(df):\n    df['Word_Count'] = df['Review'].apply(lambda text : len(str(text).split()))\n    df['Length'] = df['Review'].apply(len)\n    df['Word_Density'] = df['Length']\/df['Word_Count']\n    df['Stop_Word_Count'] = df['Review'].apply(lambda text: len([word for word in str(text).split() \n                                                                     if word in stop_words]))\n    df['Upper_Case_Word_Count'] = df['Review'].apply(lambda text: len([word for word in str(text).split()\n                                                                    if word.isupper()]))\n    df['Title_Case_Word_Count'] = df['Review'].apply(lambda text: len([word for word in str(text).split()\n                                                                    if word.istitle()]))\n    df['Numeric_Cnt'] = df['Review'].apply(lambda text:len([word for word in str(text).split()\n                                                                    if word.isdigit()]))\n    df['Punctuation_Cnt'] = df['Review'].apply(lambda text:len([word for word in str(text).split()\n                                                                    if word in string.punctuation]))\n    df['Noun_Cnt'] = df['Review'].apply(lambda text: get_pos(str(text),'noun'))\n    df['Pronoun_Cnt'] = df['Review'].apply(lambda text: get_pos(str(text),'pronoun'))\n    df['Adjective_Cnt'] = df['Review'].apply(lambda text: get_pos(str(text),'adjective'))\n    df['Adverb_Cnt'] = df['Review'].apply(lambda text: get_pos(str(text),'adverb'))\n    df['Verb_Cnt'] = df['Review'].apply(lambda text: get_pos(str(text),'verb'))\n    df['Polarity'] = df['Review'].apply(get_polarity)\n    df['Subjectivity'] = df['Review'].apply(get_subjectivity)\n\n    return df","c6d998ac":"if not (os.path.isfile('..\/input\/end-to-end-text-processing-for-beginners\/train_feat.csv' \n                       and '..\/input\/end-to-end-text-processing-for-beginners\/test_feat.csv')):\n    train_feat = extract_text_features(train_df_nodup)\n    test_feat = extract_text_features(test_df_nodup)\n    train_feat.to_csv('train_feat.csv')\n    test_feat.to_csv('test_feat.csv')\nelse:    \n    train_feat = pd.read_csv('..\/input\/end-to-end-text-processing-for-beginners\/train_feat.csv',index_col=0)\n    test_feat = pd.read_csv('..\/input\/end-to-end-text-processing-for-beginners\/test_feat.csv',index_col=0)","3d460d59":"train_feat.head()","0fed16b4":"pol_1 = train_feat[train_feat['Label']==1]['Polarity'].values\npol_0 = train_feat[train_feat['Label']==0]['Polarity'].values\n\npol_data=[pol_1,pol_0]\ngroup_labels=['Positive','Negative']\ncolors = ['#A56CC1', '#63F5EF']\n\n\nfig = ff.create_distplot(pol_data, group_labels, colors=colors,\n                         bin_size=.05, show_rug=False)\n\nfig.layout.margin.update({'t':50, 'l':100})\nfig['layout'].update(title='Polarity Distplot')\nfig.layout.template = 'plotly_dark'\n\npy.iplot(fig, filename='Polarity')","63656405":"print('The average polarity of all Positive reviews:',round(np.mean(train_feat[train_feat['Label']==1]['Polarity'].values),2))\nprint('The average polarity of all Negative reviews:',round(np.mean(train_feat[train_feat['Label']==0]['Polarity'].values),2))","0d4ff190":"pol_1 = train_feat[train_feat['Label']==1]['Subjectivity'].values\npol_0 = train_feat[train_feat['Label']==0]['Subjectivity'].values\n\npol_data=[pol_1,pol_0]\ngroup_labels=['Positive','Negative']\ncolors = ['#A56CC1', '#63F5EF']\n\nfig = ff.create_distplot(pol_data, group_labels, colors=colors,\n                         bin_size=.05, show_rug=False)\n\nfig['layout'].update(title='Subjectivity Distplot')\nfig.layout.template = 'plotly_dark'\n\npy.iplot(fig, filename='Subjectivity Distplot')","6241079c":"pol_1 = train_feat[train_feat['Label']==1]['Word_Count'].values\npol_0 = train_feat[train_feat['Label']==0]['Word_Count'].values\n\npol_data=[pol_1,pol_0]\ngroup_labels=['Positive','Negative']\ncolors = ['#A56CC1', '#63F5EF']\n\nfig = ff.create_distplot(pol_data, group_labels, colors=colors,\n                         bin_size=50, show_rug=False)\n\nfig['layout'].update(title='Word Count Distplot')\nfig.layout.template = 'plotly_dark'\n\npy.iplot(fig, filename='Word Count Distplot')","8d022be1":"print('The average word count of all Positive reviews:',round(np.mean(train_feat[train_feat['Label']==1]['Word_Count'].values),2))\nprint('The average word count of all Negative reviews:',round(np.mean(train_feat[train_feat['Label']==0]['Word_Count'].values),2))","a9cd96e1":"# Preparing the data for applying to T-SNE\nscaler = MinMaxScaler()\ncol_filter = [col for col in train_feat.columns if col not in ['Review', 'Label'] ]\ntrain_viz = scaler.fit_transform(train_feat[col_filter][:10000])\ny_label = train_feat['Label'][:10000].values","cd4e446a":"# Using TSNE for Dimentionality reduction for 15 Features to 2 dimentions\ntsne2d = TSNE(n_components=2,init='random',random_state=101,n_iter=1000,verbose=2,angle=0.5).fit_transform(train_viz)","554dd768":"#2D Plot\ntrace1 = go.Scatter(\n    x=tsne2d[:,0],\n    y=tsne2d[:,1],\n    mode='markers',\n    marker=dict(\n        sizemode='diameter',\n        color = y_label,\n        colorscale = 'Viridis',\n        colorbar = dict(title = 'Label'),\n        line=dict(color='rgb(255, 255, 255)'),\n        opacity=0.75\n    )\n)\n\ndata=[trace1]\nlayout=dict(height=600, width=600, title='2d embedding with engineered features')\nfig=dict(data=data, layout=layout)\npy.iplot(fig, filename='2d embedding with engineered features')","a57932c6":"# Using TSNE for Dimentionality reduction for 15 Features to 3 dimentions\ntsne3d = TSNE(n_components=3,init='random',random_state=101,n_iter=1000,verbose=2,angle=0.5).fit_transform(train_viz)","4639331f":"#3D plot\ntrace1 = go.Scatter3d(\n    x=tsne3d[:,0],\n    y=tsne3d[:,1],\n    z=tsne3d[:,2],\n    mode='markers',\n    marker=dict(\n        sizemode='diameter',\n        color = y_label,\n        colorscale = 'Viridis',\n        colorbar = dict(title = 'Label'),\n        line=dict(color='rgb(255, 255, 255)'),\n        opacity=0.75\n    )\n)\n\ndata=[trace1]\nlayout=dict(height=600, width=600, title='3d embedding with engineered features')\nfig=dict(data=data, layout=layout)\npy.iplot(fig, filename='3DBubble')","41a90311":"# Defining function to preprocess the text\nlemmatizer = WordNetLemmatizer()\nstop_words.append('br')\n\ndef preprocess(text):\n    text = str(text.encode('utf-8')).lower()\n    text = re.sub(r'[^A-Za-z1-9]+',' ',text)\n    text_tokens = word_tokenize(text)\n    \n    text_process = []\n    for word in text_tokens:\n        if word not in stop_words and len(word)>1:\n            text_process.append(str(lemmatizer.lemmatize(word)))\n    text = ' '.join(text_process)       \n            \n    return text","a86fed48":"if not (os.path.isfile('..\/input\/end-to-end-text-processing-for-beginners\/train_feat_clean.csv' \n                       and '..\/input\/end-to-end-text-processing-for-beginners\/test_feat_clean.csv')):\n    train_feat['Review_Clean'] = train_feat['Review'].apply(preprocess)\n    test_feat['Review_Clean'] = test_feat['Review'].apply(preprocess)\n    train_feat.to_csv('train_feat_clean.csv')\n    test_feat.to_csv('test_feat_clean.csv')\nelse:    \n    train_feat = pd.read_csv('..\/input\/end-to-end-text-processing-for-beginners\/train_feat_clean.csv',index_col=0)\n    test_feat = pd.read_csv('..\/input\/end-to-end-text-processing-for-beginners\/test_feat_clean.csv',index_col=0)","e57524cf":"train_feat.head()","709da251":"print('Review before Text preprocessing:\\n',train_feat['Review'][1])\nprint('\\nReview after Text preprocessing:\\n',train_feat['Review_Clean'][1])","023e0ba0":"# Preparing the data in the format accepted by WordCloud\npositive_reviews = ' '.join(train_feat[train_feat['Label']==1]['Review_Clean'])\nnegative_reviews = ' '.join(train_feat[train_feat['Label']==0]['Review_Clean'])","f93006df":"# Positive reviews word cloud\nstart = datetime.now()\nwordcloud = WordCloud(    background_color='black',\n                          width=1600,\n                          height=800,\n                    ).generate(positive_reviews)\n\nfig = plt.figure(figsize=(30,20))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.tight_layout(pad=0)\nprint('Word Cloud generated with Positive reviews.')\nfig.savefig(\"positive_wc.png\")\nplt.show()\nprint(\"Time taken to run this cell :\", datetime.now() - start)","3ab79f55":"# Negative reviews word cloud\nstart = datetime.now()\nwordcloud = WordCloud(    background_color='black',\n                          width=1600,\n                          height=800,\n                    ).generate(negative_reviews)\n\nfig = plt.figure(figsize=(30,20))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.tight_layout(pad=0)\nprint('Word Cloud generated with Negative reviews.')\nfig.savefig(\"negative_wc.png\")\nplt.show()\nprint(\"Time taken to run this cell :\", datetime.now() - start)","18c5a2e8":"# Extracting 15 Engineered features as a seperate Dataframe\nfeat_15_cols = [col for col in train_feat.columns if col not in ['Review','Label','Review_Clean']]\ntrain_feat_15 = train_feat[feat_15_cols]\ntest_feat_15 = test_feat[feat_15_cols]\n\ntrain_feat_15.reset_index(drop=True, inplace=True)\ntest_feat_15.reset_index(drop=True, inplace=True)","fa3bb558":"train_feat_15.head()","1c829776":"# Applying BOW text featurization on the feature 'Review_Clean'\nbow_vect = CountVectorizer(min_df=5, analyzer='word', dtype=np.float32)\n                             \nbow_train = bow_vect.fit_transform(train_feat['Review_Clean'].values)\nbow_test = bow_vect.transform(test_feat['Review_Clean'].values)\n\nprint(\"The shape of train BOW vectorizer \",bow_train.get_shape())\nprint(\"the number of unique words \", bow_train.get_shape()[1])","78a6dc67":"# Combining Review clean text featurization with 15 engineered features so as to get final dataset.\nfinal_bow_train = sparse.hstack((bow_train, train_feat_15)).tocsr()\nfinal_bow_test = sparse.hstack((bow_test, test_feat_15)).tocsr()\n\nprint(\"The shape of final train BOW dataset:\",final_bow_train.get_shape())\nprint(\"The shape of final test BOW dataset:\",final_bow_test.get_shape())","5036b033":"bow = pd.DataFrame(\n    {\n      'BOW': list(bow_train.sum(axis=0).A1)\n                    \n    }, index=bow_vect.get_feature_names())\n\nprint('The important features based on BOW values:')\nbow.sort_values('BOW',ascending=False).head(10)","db303417":"bow_vect_ngrams = CountVectorizer(min_df=5, analyzer='word', ngram_range=(1,3), dtype=np.float32)\n                             \nbow_train_ngrams = bow_vect_ngrams.fit_transform(train_feat['Review_Clean'].values)\nbow_test_ngrams = bow_vect_ngrams.transform(test_feat['Review_Clean'].values)\n\nprint(\"The shape of train BOW vectorizer with ngrams \",bow_train_ngrams.get_shape())\nprint(\"the number of unique words \", bow_train_ngrams.get_shape()[1])","43437ca6":"# Combining Review clean text featurization with 15 engineered features so as to get final dataset.\nfinal_bow_train_ngrams = sparse.hstack((bow_train_ngrams, train_feat_15)).tocsr()\nfinal_bow_test_ngrams = sparse.hstack((bow_test_ngrams, test_feat_15)).tocsr()\n\nprint(\"The shape of final train BOW with ngrams dataset:\",final_bow_train_ngrams.get_shape())\nprint(\"The shape of final test BOW with ngrams dataset:\",final_bow_test_ngrams.get_shape())","9ee462b9":"tfidf_vect = TfidfVectorizer(min_df=5, analyzer='word',dtype=np.float32) \ntfidf_train = tfidf_vect.fit_transform(train_feat['Review_Clean'].values)\ntfidf_test = tfidf_vect.transform(test_feat['Review_Clean'].values)\n\nprint(\"The shape of train Tf-Idf vectorizer \",tfidf_train.get_shape())\nprint(\"the number of unique words \", tfidf_train.get_shape()[1])","cc8da82f":"# Combining Review clean text featurization with 15 engineered features so as to get final dataset.\nfinal_tfidf_train = sparse.hstack((tfidf_train, train_feat_15)).tocsr()\nfinal_tfidf_test = sparse.hstack((tfidf_test, test_feat_15)).tocsr()\n\nprint(\"The shape of final train Tf-Idf dataset:\",final_tfidf_train.get_shape())\nprint(\"The shape of final test Tf-Idf dataset:\",final_tfidf_test.get_shape())","70b4f709":"tfidf = pd.DataFrame(\n    {\n      'Tf-Idf': list(tfidf_train.sum(axis=0).A1)\n                    \n    }, index=tfidf_vect.get_feature_names())\n\nprint('The important features based on TF-IDF values:')\ntfidf.sort_values('Tf-Idf',ascending=False).head(10)","28ab9f1e":"tfidf_vect_ngrams = TfidfVectorizer(min_df=5, analyzer='word',ngram_range=(1,3),dtype=np.float32) \ntfidf_train_ngrams = tfidf_vect_ngrams.fit_transform(train_feat['Review_Clean'].values)\ntfidf_test_ngrams = tfidf_vect_ngrams.transform(test_feat['Review_Clean'].values)\n\nprint(\"The shape of train Tf-Idf vectorizer with ngrams \",tfidf_train_ngrams.get_shape())\nprint(\"the number of unique words \", tfidf_train_ngrams.get_shape()[1])","55a06e15":"# Combining Review clean text featurization with 15 engineered features so as to get final dataset.\nfinal_tfidf_train_ngrams = sparse.hstack((tfidf_train_ngrams, train_feat_15)).tocsr()\nfinal_tfidf_test_ngrams = sparse.hstack((tfidf_test_ngrams, test_feat_15)).tocsr()\n\nprint(\"The shape of final train Tf-Idf with ngrams dataset:\",final_tfidf_train_ngrams.get_shape())\nprint(\"The shape of final test Tf-Idf with ngrams dataset:\",final_tfidf_test_ngrams.get_shape())","43cd5a13":"# Preparing the data in the format required for Word2Vec\nlist_of_sentence_train=[]\nfor sentence in train_feat['Review_Clean']:\n    list_of_sentence_train.append(sentence.split())\n    \nlist_of_sentence_test=[]\nfor sentence in test_feat['Review_Clean']:\n    list_of_sentence_test.append(sentence.split())","e9d13d91":"# Training the Word2Vec model on the own corpus\nw2v_model=Word2Vec(list_of_sentence_train, size=50, min_count=5, workers=4)\nw2v_words = list(w2v_model.wv.vocab)\n\nwith open('w2v_model.pkl', 'wb') as f:\n    pickle.dump(w2v_model, f)","3ec5113c":"# Word2Vec representation for the word 'pleasure'\nw2v_model.wv['pleasure']","2b8be6f4":"print('Few words trained by Word2Vec:',w2v_words[:10])\nprint('The no of words trained by Word2Vec:',len(w2v_words))","825005c7":"# compute average word2vec for each review.\nsent_vectors_train = []; \nfor sent in tqdm(list_of_sentence_train): \n    sent_vec = np.zeros(50) \n    cnt_words =0; \n    for word in sent: \n        if word in w2v_words:\n            vec = w2v_model.wv[word]\n            sent_vec += vec\n            cnt_words += 1\n    if cnt_words != 0:\n        sent_vec \/= cnt_words\n    sent_vectors_train.append(sent_vec)\nprint(len(sent_vectors_train))\nprint(len(sent_vectors_train[0]))","48ef14fc":"sent_vectors_test = [];\nfor sent in tqdm(list_of_sentence_test): \n    sent_vec = np.zeros(50)\n    cnt_words =0; \n    for word in sent:\n        if word in w2v_words:\n            vec = w2v_model.wv[word]\n            sent_vec += vec\n            cnt_words += 1\n    if cnt_words != 0:\n        sent_vec \/= cnt_words\n    sent_vectors_test.append(sent_vec)\nprint(len(sent_vectors_test))\nprint(len(sent_vectors_test[0]))","92b2d38c":"# Combining Review clean text featurization with 15 engineered features so as to get final dataset.\navg_w2v_train = pd.DataFrame(sent_vectors_train)\navg_w2v_test = pd.DataFrame(sent_vectors_test)\n\nfinal_avg_w2v_train = pd.concat([avg_w2v_train,train_feat_15], axis=1)\nfinal_avg_w2v_test = pd.concat([avg_w2v_test,test_feat_15], axis=1)\n\nprint('The shape of final train avg word2vec dataset:',final_avg_w2v_train.shape)\nprint('The shape of final test avg word2vec dataset:',final_avg_w2v_test.shape)","ce773af7":"final_avg_w2v_train.head()","1e654107":"model = TfidfVectorizer(min_df=5, analyzer='word',dtype=np.float32) \ntfidf_matrix = model.fit_transform(train_feat['Review_Clean'].values)\ndictionary = dict(zip(model.get_feature_names(), list(model.idf_)))","ee3def67":"# TF-IDF weighted Word2Vec\ntfidf_feat = model.get_feature_names()\ntfidf_sent_vectors_train = [];\nrow=0;\nfor sent in tqdm(list_of_sentence_train):\n    sent_vec = np.zeros(50)\n    weight_sum =0; \n    for word in sent: \n        if word in w2v_words and word in tfidf_feat:\n            vec = w2v_model.wv[word]\n            tf_idf = dictionary[word]*(sent.count(word)\/len(sent))\n            sent_vec += (vec * tf_idf)\n            weight_sum += tf_idf\n    if weight_sum != 0:\n        sent_vec \/= weight_sum\n    tfidf_sent_vectors_train.append(sent_vec)\n    row += 1","77f44fb0":"tfidf_feat = model.get_feature_names()\ntfidf_sent_vectors_test = []; \nrow=0;\nfor sent in tqdm(list_of_sentence_test):\n    sent_vec = np.zeros(50) \n    weight_sum =0; \n    for word in sent:\n        if word in w2v_words and word in tfidf_feat:\n            vec = w2v_model.wv[word]\n            tf_idf = dictionary[word]*(sent.count(word)\/len(sent))\n            sent_vec += (vec * tf_idf)\n            weight_sum += tf_idf\n    if weight_sum != 0:\n        sent_vec \/= weight_sum\n    tfidf_sent_vectors_test.append(sent_vec)\n    row += 1","5e5ba15f":"# Combining Review clean text featurization with 15 engineered features so as to get final dataset.\ntfidf_w2v_train = pd.DataFrame(tfidf_sent_vectors_train)\ntfidf_w2v_test = pd.DataFrame(tfidf_sent_vectors_test)\n\nfinal_tfidf_w2v_train = pd.concat([tfidf_w2v_train,train_feat_15], axis=1)\nfinal_tfidf_w2v_test = pd.concat([tfidf_w2v_test,test_feat_15], axis=1)\n\nprint('The shape of final train avg word2vec dataset:',final_tfidf_w2v_train.shape)\nprint('The shape of final test avg word2vec dataset:',final_tfidf_w2v_test.shape)","f6eb9229":"### 2.4.1 Word Clouds generated from Positive and Negative reviews.\n**Word Cloud:** It is a visual representation of text data where if a specific word appears more times in the text, the bigger and bolder it appears in the word cloud. <br\/>\n<br\/>\nIt is used to highlight the important words in the given text corpus.","5ab20a1d":"#### Observations:\n- The Average word count of Positive reviews is greater than that of Negative reviews. \n- Positive reviews are having higher tendency of having higher word counts, we could also see the same from the above plot.","57e1933c":"### Text Based Features:\n<ul>\n    <li><b>Word_Count:<\/b> No. of words in each review.<\/li>\n    <li><b>Length:<\/b> Length of each review including spaces.<\/li>\n    <li><b>Word_Density:<\/b> Average length of each word.<\/li>\n    <li><b>Stop_Word_Count:<\/b> No. of stop words in each review.<\/li>\n    <li><b>Upper_Case_Word_Count:<\/b> No. of Upper case words in each review.<\/li>\n    <li><b>Title_Case_Word_Count:<\/b> No. of Title case words in each review.<\/li>\n    <li><b>Numeric_Cnt:<\/b> No. of digits in each review.<\/li>\n    <li><b>Punctuation_Cnt:<\/b> No. of punctuations in each review.<\/li> \n<\/ul>\n\n### NLP Based Features:\n<ul>\n    <li><b>Noun_Cnt:<\/b> No. of nouns in each review.<\/li>\n    <li><b>Pronoun_Cnt:<\/b> No. of pronouns in each review.<\/li>\n    <li><b>Adjective_Cnt:<\/b> No. of Adjectives in each review.<\/li>\n    <li><b>Adverb_Cnt:<\/b> No. of Adverbs in each review.<\/li>\n    <li><b>Verb_Cnt:<\/b> No. of Verbs in each review.<\/li>\n    <li><b>Polarity:<\/b> Polarity of each review. [-1,1] are the range of values that can take, -1 being negative and 1 being positive.<\/li>\n    <li><b>Subjectivity:<\/b> Subjectivity of each review.It refers to personal opinion, emotion or judgment which lies in the range of [0,1]<\/li>\n<\/ul>","240950e8":"#### 2.5.3.2 Tf-Idf Word2Vec:\nInstead of taking just the average of all the word vector representations in the sentence. In Tf-Idf Word2Vec, we will multiply Tf-Idf values with word vector representations and divide by the sum of all Tf-Idf values.","1602b770":"### 2.3 Feature Engineering","ff819b18":"#### Observations:\n- From the above 2d,3d plots, we could see there are few seperations between Positive and Negative reviews. So the 15 extracted features are helpful to seperate Reviews.","441fa782":"# Complete Text Processing on IMDB Movie Reviews with Feature Engineering.","c11310f4":"#### Observations:\n- Subjectivity is not able to seperate Positive and Negative reviews. Both the distributions are overlapping each other.","378fdac9":"### 2.2 Dropping Duplicates","e51bcfb4":"## 2.Exploratory Data Analysis:","43c49202":"#### Observations:\n- Words like Movie, Film are common in all both type of reviews.This is expected since the reviews are about Movies.\n- We could see words like Good, Best, Interesting in Positive reviews word cloud and words like Bad, Stupid, Boring in Negative reviews word cloud.","022eba52":"The final datasets prepared using BOW, Tf-Idf, Word2Vec can be used with Machine Learning models directly.","df3fc766":"### 1.1 Description:","b4f09b5e":"#### 2.5.3.1 Average Word2Vec:\nSince Word2Vec gives vector representation for each word, we need some method to get vector representation for each sentence. In Average Word2Vec, we take the average of the all the word vector representations in the sentence.","28f1932c":"### 2.3.1 Analysis of some of the extracted features","caf89460":"### 1.2 Objective:\nGiven a Movie review, we need to predict whether the review is Positive or Negative.\nIn this notebook, I am going to cover only the text feature engineering, text preprocessing and text featurization techniques.","3d3ae9d5":"Folllowing are the steps followed for text preprocessing:\n1. Converting the text to Lower case.\n2. Removing all the characters other than alpha numeric characters.\n3. Tokenizing the sentence.\n4. Removing stopwords and words with character length one.\n5. Lemmatization of each word.","6f29c1f5":"### 2.5.2 TF-IDF featurization:\nThe tf-idf weight is composed by two terms: the first term (TF) is the ratio of  the number of times a word appears in a document and the total number of words in that document; the second term (IDF) is logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.<br\/>\n<br\/>\nGenerally tf-idf values are high when a word is present more in the document and less often in the corpus. \n\n- TF(t) = (Number of times term t appears in a document) \/ (Total number of terms in the document)\n- IDF(t) = log_e(Total number of documents \/ Number of documents with term t in it)","798f16ed":"## 2.4 Text Preprocessing:\nTo preprocess your text simply means to bring your text into a form that is predictable and analyzable for your task. Depending on the task in hand, we generally use different preprocessing techniques. <br\/>\nHere we don't want to remove numbers because some people will give rating of the movie in the Review, but in some tasks numbers were not needed.","708d0a26":"## 1.Business Problem:","02a6989f":"### 2.3.2 Dimensionality reduction of 15 features and Visualization in 2d, 3d:\nHere we have used T-SNE which is a nonlinear dimensionality reduction technique for embedding 15 dimensional data to two or three dimensions.<br\/>\n<br\/>\nWonderful resource to understand T-SNE in detail: https:\/\/distill.pub\/2016\/misread-tsne\/","e31e8536":"### BOW featurization with N-grams:\nIn the above Bag-of-words model only the counts of words mattered and the order is discarded. For instance, in the example \"Ram likes to watch movies. Krish likes movies too\", the bag-of-words representation will not reveal that the verb \"likes\" always follows a person's name in this text. <br\/>\n<br\/>\nAs an alternative, the n-gram model can store this spatial information.","44c3a12e":"### 2.5.1 BOW featurization:\nThe bag-of-words model is a simple method to featurize the text where the (frequency of) occurrence of each word is used as a feature and the order of words is discarded. <br\/>\n<br\/>\nRefer : https:\/\/medium.com\/greyatom\/an-introduction-to-bag-of-words-in-nlp-ac967d43b428\n","84d86575":"IMDb is the world's most popular and authoritative source for movie, TV and celebrity content. <br\/>\nIt is an online database of information related to films, television programs, home videos and video games, and streaming content online -- including cast, production crew and personnel biographies, plot summaries, trivia, and fan reviews and ratings. \n","77e54d55":"### 2.5.3 Word Embeddings featurization (Word2Vec):\nWord embedding is one of the most popular representation of document vocabulary. It is capable of capturing context of a word in a document, semantic and syntactic similarity relation with other words. <br\/> \n<br\/>\nWord2Vec is one of the most popular technique to learn word embeddings using shallow neural network. It generates a vector of size 'd' for each word. <br\/>\n<br\/>\nFolllow this link to understand in great detail: https:\/\/www.analyticsvidhya.com\/blog\/2017\/06\/word-embeddings-count-word2veec\/","ec7a0d60":"### TF-IDF featurization with N-grams:","b4b8a2ba":"### 2.1 Reading data and basic stats","cc29d6c3":"#### Observations:\n- From the above plot, we could clearly see 'Polarity' feature is able to seperate Positive and Negative reviews. Positive reviews tending to +1 and negative reviews tending to -1.\n- The average polarity of Positive reviews is greater than that of Negative reviews.","e6749531":"Thanks for reading!!!<br\/>\nPlease upvote the kernel, if you liked it.","22faa645":"### 1.3 Source:\nSource : https:\/\/www.kaggle.com\/iarunava\/imdb-movie-reviews-dataset","2d1ffe3b":"## 2.5 Text Featurization Techniques:\nSince machines can only understand numbers, we cannot use text directly with Machine Learning algorithms.  So we need to convert the text to numbers.<br\/>\n<br\/>\nFollowing are the techniques that we can use for text featurization:\n\n- Bag Of Words (BOW) featurization\n- Term Frequency-Inverse Document Frequency (TF-IDF) featurization\n- Word Embeddings (Word2Vec) featurization\n  - Average Word2Vec\n  - TF-IDF Word2Vec"}}