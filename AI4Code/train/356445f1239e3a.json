{"cell_type":{"62ce54b0":"code","f5a81ddf":"code","f4c994ad":"code","4c90007f":"code","a0f2c29d":"code","5b6ea24f":"code","dcae27f1":"code","5dfb4b56":"code","fe444229":"code","b2652766":"code","2d5ecff7":"code","a74d997c":"code","a556e4fb":"code","f62ebcd2":"code","d5c22992":"code","dd09f7f6":"code","6eff6177":"code","30820d1a":"code","bafa900e":"code","67344254":"code","4f2d1048":"code","16931867":"code","6d58c4ae":"code","bb8f0c5e":"code","9d4eb508":"code","b798f19f":"code","7d2f74a2":"code","ceb32415":"code","4ff8a2dc":"code","ad3a641f":"code","7e7ce86f":"code","c2c62a52":"code","60c912e8":"code","a646cf8f":"code","74d884e6":"code","8bf92c22":"code","b3c17a96":"code","8229e885":"code","7968e32d":"code","d9413efa":"code","05d93048":"code","83475c41":"code","5b5b0ebd":"code","98f21888":"code","e08bc4c4":"code","4b03a2c8":"code","9b6fdac1":"code","b0168be1":"code","30ee626a":"code","2c8672a8":"code","c7285868":"code","d1f043be":"code","9077a1be":"code","ef2d83ac":"code","59c6e46a":"code","daa07b0c":"code","22cf81af":"code","44172fc4":"code","1a2db0f0":"code","ee32e26c":"code","ae02989e":"code","552f038e":"code","ec959072":"code","52d6ab43":"code","33afc4c6":"code","b2381862":"code","593611c9":"code","5bc884a1":"code","a2f42d48":"code","e3f22d04":"code","55dd11b7":"code","a178562c":"code","9ec38e01":"code","70ce4325":"code","6a5ad0a7":"code","5756edb5":"code","54ab635a":"code","27998dc6":"code","f13c575b":"code","d98b93b2":"code","d840a575":"code","48e03964":"markdown","14236b77":"markdown","79f5326f":"markdown","cddb5582":"markdown","e1e79848":"markdown","abfc5822":"markdown","1cf20d8b":"markdown","c3f9e025":"markdown","a84a04a6":"markdown","282ca232":"markdown","012ac6a3":"markdown","008e1b31":"markdown","5af2dac3":"markdown","9162343b":"markdown","178eea9b":"markdown","8d5b4769":"markdown","0998ff07":"markdown","00086973":"markdown","11a22d55":"markdown","a31483e8":"markdown"},"source":{"62ce54b0":"## Import basic packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","f5a81ddf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f4c994ad":"## Read data\ntrain = pd.read_csv('\/kaggle\/input\/sentiment-analysis-on-movie-reviews\/train.tsv.zip',sep=\"\\t\") \ntest = pd.read_csv('\/kaggle\/input\/sentiment-analysis-on-movie-reviews\/test.tsv.zip',sep=\"\\t\") ","4c90007f":"train.head()","a0f2c29d":"train.info()","5b6ea24f":"## Show the number of class distributed\nplt.figure(figsize=(10,5))\nax=plt.axes()\nax.set_title('Number of sentiment class')\nsns.countplot(x=train.Sentiment,data=train)","dcae27f1":"train.Phrase[:10]","5dfb4b56":"import string\nstring.punctuation","fe444229":"train.Phrase=train.Phrase.apply(lambda x: x.translate(str.maketrans('','',string.punctuation)).lower())","b2652766":"train.Phrase[:10]","2d5ecff7":"train.Phrase=train.Phrase.str.split(' ')","a74d997c":"train.Phrase[:10]","a556e4fb":"from nltk.corpus import stopwords\nstopwords_e=stopwords.words('english')","f62ebcd2":"stopwords_e=stopwords.words('english')","d5c22992":"train.Phrase=[w for w in train.Phrase if w not in stopwords_e]\ntrain.Phrase.head()","dd09f7f6":"import nltk\n##nltk.download()","6eff6177":"from nltk.stem import WordNetLemmatizer\nlemmar=WordNetLemmatizer()","30820d1a":"train.Phrase=train.Phrase.apply(lambda x: [lemmar.lemmatize(w) for w in x])","bafa900e":"## Method1:\nfrom nltk.stem import PorterStemmer\nporter=PorterStemmer()","67344254":"train.Phrase=train.Phrase.apply(lambda x: [porter.stem(w) for w in x])","4f2d1048":"## Method2:\nfrom nltk.stem import SnowballStemmer\nsnow=SnowballStemmer('english')","16931867":"train.Phrase=train.Phrase.apply(lambda x: [snow.stem(w) for w in x])","6d58c4ae":"from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,TfidfTransformer\nvector=TfidfVectorizer(stop_words='english')","bb8f0c5e":"train.Phrase=train.Phrase.apply(lambda x: ' '.join(x))","9d4eb508":"vector1=vector.fit(train.Phrase)","b798f19f":"train_feature=vector1.transform(train.Phrase)","7d2f74a2":"train_feature.toarray()","ceb32415":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix\nlr=LogisticRegression(multi_class='ovr')","4ff8a2dc":"train.head()","ad3a641f":"train.info()","7e7ce86f":"lr=lr.fit(train_feature,train.Sentiment)","c2c62a52":"## Coefficient\nlr.coef_","60c912e8":"## Get the model performance on train dataset since we don't have test response data\ntrain_predict=lr.predict(train_feature)","a646cf8f":"## the number of data in each class\ntrain.Sentiment.value_counts().sort_index()","74d884e6":"## number of data in predict result\nnp.unique(train_predict,return_counts=True)","8bf92c22":"## Plot predict result\nplt.figure(figsize=(10,5))\nax=plt.axes()\nax.set_title('Number of sentiment class')\nsns.countplot(train_predict)","b3c17a96":"print(classification_report(train_predict, train.Sentiment))","8229e885":"from sklearn import svm","7968e32d":"svm1=svm.SVC(decision_function_shape='ovo')","d9413efa":"svm1.fit(train_feature, train.Sentiment)","05d93048":"svm_train_pred=svm1.predict(train_feature)","83475c41":"## Number of predict class\nnp.unique(svm_train_pred,return_counts=True)","5b5b0ebd":"plt.figure(figsize=(10,5))\nax=plt.axes()\nax.set_title('Number of sentiment class')\nsns.countplot(svm_train_pred)","98f21888":"print(classification_report(svm_train_pred, train.Sentiment))","e08bc4c4":"from sklearn.tree import DecisionTreeClassifier","4b03a2c8":"ds=DecisionTreeClassifier()\nds.fit(train_feature, train.Sentiment)","9b6fdac1":"print(ds.feature_importances_)","b0168be1":"ds_train_pred=ds.predict(train_feature)","30ee626a":"train.Sentiment.value_counts().sort_index()","2c8672a8":"## Number of predict class\nnp.unique(ds_train_pred,return_counts=True)","c7285868":"plt.figure(figsize=(10,5))\nax=plt.axes()\nax.set_title('Number of sentiment class')\nsns.countplot(ds_train_pred)","d1f043be":"print(classification_report(ds_train_pred, train.Sentiment))","9077a1be":"from sklearn.ensemble import RandomForestClassifier","ef2d83ac":"rf=RandomForestClassifier()\nrf.fit(train_feature, train.Sentiment)","59c6e46a":"print(rf.feature_importances_)","daa07b0c":"rf_train_pred=rf.predict(train_feature)","22cf81af":"plt.figure(figsize=(10,5))\nax=plt.axes()\nax.set_title('Number of sentiment class')\nsns.countplot(rf_train_pred)","44172fc4":"print(classification_report(rf_train_pred, train.Sentiment))","1a2db0f0":"def data_preprocess(text):\n    text_nonpunc=[w.lower() for w in text if w not in string.punctuation]\n    text_nonpunc=''.join(text_nonpunc)\n    text_rmstop=[x for x in text_nonpunc.split(' ') if x not in stopwords_e]\n    text_stem=[snow.stem(w) for w in text_rmstop]\n    text1=' '.join(text_stem)\n    return (text1)","ee32e26c":"from sklearn.pipeline import Pipeline","ae02989e":"# Can't use TfidVecterizer() because line: \n# https:\/\/stackoverflow.com\/questions\/50192763\/python-sklearn-pipiline-fit-attributeerror-lower-not-found\n# TfidTransformer should combine with countVectorizer()\nlrpipeline=Pipeline([('preprocess',CountVectorizer(analyzer=data_preprocess)),\n                  ('Tfidf',TfidfTransformer()),\n                  ('classify',LogisticRegression())])","552f038e":"lrpipeline.fit(train.Phrase,train.Sentiment)","ec959072":"## have to saved the vocabulary\nresult=lrpipeline.predict(test['Phrase'])","52d6ab43":"np.unique(result)","33afc4c6":"plt.figure(figsize=(10,5))\nax=plt.axes()\nax.set_title('Number of sentiment class')\nsns.countplot(result)","b2381862":"## Import every packages\nfrom scipy import stats\nimport string\nfrom nltk.corpus import stopwords\nstopwords_e=stopwords.words('english')\nfrom nltk.stem import SnowballStemmer\nsnow=SnowballStemmer('english')\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import  RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,TfidfTransformer\nvector=TfidfVectorizer(stop_words='english')","593611c9":"## Preprocess function\ndef data_preprocess(text):\n    text_nonpunc=[w.lower() for w in text if w not in string.punctuation]\n    text_nonpunc=''.join(text_nonpunc)\n    text_rmstop=[x for x in text_nonpunc.split(' ') if x not in stopwords_e]\n    text_stem=[snow.stem(w) for w in text_rmstop]\n    text1=' '.join(text_stem)\n    return (text1)","5bc884a1":"## OOP Class \n## Notice: Class name and the first def should have a blank line\nclass EstimatorSelection:\n    \n    def __init__(self, models):\n        self.models=models\n        self.keys=models.keys()\n        self.results={}\n        self.modelfit={}\n        self.modelpredict={}\n    def fit(self, x, y):\n        x1=x.apply(lambda i: data_preprocess(i))\n        x_feature1=vector.fit_transform(x1)\n        for key in self.keys:\n            model=self.models[key]\n            self.modelfit[key]=model.fit(x_feature1,y)\n            y_pred=model.predict(x_feature1)\n            self.results[key]=classification_report(y, y_pred,output_dict=True)\n    def predict(self,test_x):\n        test_x1=test_x.apply(lambda i: data_preprocess(i))\n        test_feature1=vector.transform(test_x1)\n        test_frames=[]\n        for key in self.keys:\n            modelfit=self.modelfit[key]\n            test_y=modelfit.predict(test_feature1)\n            test_frame=pd.DataFrame(test_y,columns=[key])\n            test_frames.append(test_frame)\n        predict_frame=pd.concat(test_frames,axis=1)            \n        return(predict_frame)     \n    def summary(self):\n        Frames=[]\n        for key in self.keys:\n            result=self.results[key]\n            Frame=pd.DataFrame(result['macro avg'], index=[key])\n            Frames.append(Frame)\n        result_sum=pd.concat(Frames)\n        return result_sum.iloc[:,:3]","a2f42d48":"## Models want to predict on test data\nmodels = { \n    'LogisticClassifier': LogisticRegression(multi_class='ovr'),\n    'RandomforestClassifier':RandomForestClassifier(),\n    'DecisionTreeClassifier':DecisionTreeClassifier()\n}","e3f22d04":"model_compare=EstimatorSelection(models)","55dd11b7":"model_compare.fit(train.Phrase, train.Sentiment)","a178562c":"summary=model_compare.summary()\nsummary","9ec38e01":"predict_result=model_compare.predict(test.Phrase)\npredict_result","70ce4325":"predict_result1=predict_result.reset_index().rename(columns={'index':'case'})\npredict_result2=pd.melt(predict_result1,id_vars='case', value_vars=['LogisticClassifier', 'RandomforestClassifier', 'DecisionTreeClassifier'])","6a5ad0a7":"predict_result2=pd.melt(predict_result1,id_vars='case', value_vars=['LogisticClassifier', 'RandomforestClassifier', 'DecisionTreeClassifier'])\npredict_result2","5756edb5":"predict_result3=predict_result2.groupby(['variable','value']).size().reset_index().rename(columns={0:'count'})\npredict_result3","54ab635a":"plt.figure(figsize=(10,5))\nax=plt.axes()\nax.set_title('Number of class for each methods')\nsns.barplot(x='value', y='count', hue='variable', data=predict_result3)","27998dc6":"Final_results=[]\nfor i in range(predict_result1.shape[0]):\n    Final_result=stats.mode(predict_result1.iloc[i,]).mode.item()\n    Final_results.append(Final_result)","f13c575b":"predict_result1['Final_result']=Final_results\npredict_result1","d98b93b2":"test['Sentiment']=Final_results\ntest","d840a575":"#make the predictions with trained model and submit the predictions.\nsub_file = pd.read_csv('\/kaggle\/input\/sentiment-analysis-on-movie-reviews\/sampleSubmission.csv',sep=',')\nsub_file.Sentiment=Final_results\nsub_file.to_csv('Submission.csv',index=False)","48e03964":"### Multi_class logistic regression","14236b77":"## Reshape result dataframe to plot\nMethod: Melt() and Pivottable()","79f5326f":"### Random forest model","cddb5582":"## Compare ML models predict results","e1e79848":"### Muti-class SVM","abfc5822":"# Build ML models step by step\n## Step1:  Exploring Data Analysis","1cf20d8b":"### Remove stopwords","c3f9e025":"## Submission","a84a04a6":"### Tokenize sentence","282ca232":"## Method 2: OOP to built class perform all models\n\nPerform the TOP 3 models (based on accuracy on train data) in functions. ","012ac6a3":"### Decision tree model","008e1b31":"### Lemmatize words","5af2dac3":"## Step2: NLP process (step by step)\n### Remove punctuation and lowercase","9162343b":"# Stremline the process \n\n## Method1: Pipeline \nonly use LR model as an example","178eea9b":"### Compare model performance","8d5b4769":"## Step3: Build ML models on train dataset","0998ff07":"# Movie sentiment analysis\n\n**Data:**\n\nThe data source is Movie Sentiment project from Kaggle, including train and test datasets; In trainning data, there is Phrase column and Sentiment column as the result score from 0 to 4 (negative to positve); In test data, there is Phrase column for us to analyze the sentiment of each phrase.\n\n**Goal:**\n\n\nPerform NLP analysis on trainning data and use different Machine learning models to compare model performances, and predict Phrase sentiment on test data. \n\n**Highlight:** \n\n- **Show NLP process step by step**\n- **Stremline NLP process with ML Pipeline**\n- **Using Object_oriented programming build class to perfrom mutiple ML models efficiently**\n\n**NLP process steps:**\n\n- Remove punctuation\n- Tokenize sentence\n- Remove stopwords\n- Stem or lemmatize words:\n  - Both methods aim to change the words to original form (if using both: better lemmartize first and then stem)\n     - Stemming change words based on rules on string: e.g.: delted 's' at the end of noun. While it has serious limitations on change the actual meaning of words. Since the algorithm is change based on rules for strings, it runs faster and it's a good choice if time is a concern in NLP process\n          - There are three stemmer: porter, snowball(porter2), lancaster; porter is the orginal and most gental one, while it's the most computationally intensive. snowball is a litter intensive than porter and it improves from porter (common option); lancaster is the most aggresive one, the faster one while the final words might obscure\n     - Lemmatization change words based on the dictionary from different algorithms, such as \"went\" to \"go\". Based on the differnt type of the word (verb, noun), it can change to differnt meaning of word which solve the disambiguation problem. While it demands more computaional power. (It can be used if you want to build a dictionary world: NLP system)\n- Calculate TFIDF \n- Train ML models\n- Compare models results and test model\n\n\n\n**Reference:**\n\n\nNLP process: \nhttps:\/\/towardsdatascience.com\/your-guide-to-natural-language-processing-nlp-48ea2511f6e1\n\nTFIDF:\nhttps:\/\/towardsdatascience.com\/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089\n\nHyperparameter on machine learning models:  https:\/\/github.com\/davidsbatista\/machine-learning-notebooks\/blob\/master\/hyperparameter-across-models.ipynb\n\n","00086973":"### TFIDF vectorize\n\nTFIDF: Term frequency inverse document frequency\n\n\n**formula:** \nTFIDF=Term frequency* Inverse Document frequency\n\n\n\n- Term frequency: count of same word w in a documents\/ the total number of words in documents\n\n- Document frequency: number of documents have the word\/the total number of documents\n\n- To avoid the number of documents too big, we take log of the IDF: if word not shows up, log(IDF)=0, and 0 cannot be divide, we add 1, so formula becomes: TF*log(N\/DF+1) [More info in references]\n","11a22d55":"## Get the Final result from the mode of three classification results","a31483e8":"### Stemming words"}}