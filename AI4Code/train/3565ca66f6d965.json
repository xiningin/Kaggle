{"cell_type":{"8ccf8529":"code","d00a0722":"code","92a0d9b1":"code","2c2eb14d":"code","2658762c":"code","b4f80c84":"code","db029708":"code","8ad7b46a":"code","f80373e0":"code","c1c2ad6a":"markdown","8bc60862":"markdown","f16ebb80":"markdown"},"source":{"8ccf8529":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d00a0722":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mlxtend.frequent_patterns import apriori, association_rules\nimport statsmodels.stats.api as sms\nfrom scipy.stats import ttest_1samp, shapiro, levene, ttest_ind, mannwhitneyu, pearsonr, spearmanr, kendalltau, \\\n    f_oneway, kruskal\nfrom statsmodels.stats.proportion import proportions_ztest\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n# !pip install missingno\nimport missingno as msno\nfrom datetime import date\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler, RobustScaler\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mlxtend.frequent_patterns import apriori, association_rules\nimport statsmodels.stats.api as sms\nfrom scipy.stats import ttest_1samp, shapiro, levene, ttest_ind, mannwhitneyu, pearsonr, spearmanr, kendalltau, \\\n    f_oneway, kruskal\nfrom statsmodels.stats.proportion import proportions_ztest\nfrom sklearn.model_selection import train_test_split\nimport sqlite3\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import scale, StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error, r2_score, roc_auc_score, roc_curve, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport numpy as np\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn import neighbors\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.model_selection import cross_val_score,train_test_split,GridSearchCV\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import scale\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, cross_validate\n\nimport random\nimport warnings\n\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('display.width', 500)\n\nmodels = [LGBMRegressor,\n          XGBRegressor,\n          GradientBoostingRegressor,\n          RandomForestRegressor,\n          DecisionTreeRegressor,\n          MLPRegressor,\n          KNeighborsRegressor,\n          SVR]\nlbe=LabelEncoder()\n\n","92a0d9b1":"df=pd.read_csv(\"..\/input\/hitters-baseball-data\/Hitters.csv\")","2c2eb14d":"def degisken_tiplerine_ayirma(data,cat_th,car_th):\n   \"\"\"\n   Veri:data parametresi ili fonksiyona girilen verinin de\u011fi\u015fkenlerin s\u0131n\u0131fland\u0131r\u0131lmas\u0131.\n   Parameters\n   ----------\n   data: pandas.DataFrame\n   \u0130\u015flem yap\u0131lacak veri seti\n\n   cat_th:int\n   categoric de\u011fi\u015fken threshold de\u011feri\n\n   car_th:int\n   Cardinal de\u011fi\u015fkenler i\u00e7in threshold de\u011feri\n\n   Returns\n   -------\n    cat_deg:list\n    categorik de\u011fi\u015fken listesi\n    num_deg:list\n    numeric de\u011fi\u015fken listesi\n    car_deg:list\n    categoric ama cardinal de\u011fi\u015fken listesi\n\n   Examples\n   -------\n    df = dataset_yukle(\"breast_cancer\")\n    cat,num,car=degisken_tiplerine_ayirma(df,10,20)\n   Notes\n   -------\n    cat_deg + num_deg + car_deg = toplam de\u011fi\u015fken say\u0131s\u0131\n\n   \"\"\"\n\n\n   num_but_cat=[i for i in data.columns if data[i].dtypes !=\"O\" and data[i].nunique() < cat_th]\n\n   car_deg=[i for i in data.columns if data[i].dtypes == \"O\" and data[i].nunique() > car_th]\n\n   num_deg=[i for i in data.columns if data[i].dtypes !=\"O\" and i not in num_but_cat]\n\n   cat_deg = [i for i in data.columns if data[i].dtypes == \"O\" and i not in car_deg]\n\n   cat_deg = cat_deg+num_but_cat\n\n   print(f\"Dataset kolon\/de\u011fi\u015fken say\u0131s\u0131: {data.shape[1]}\")\n   print(f\"Dataset sat\u0131r\/veri say\u0131s\u0131: {data.shape[0]}\")\n   print(\"********************************************\")\n   print(f\"Datasetin numeric de\u011fi\u015fken say\u0131s\u0131: {len(num_deg)}\")\n   print(f\"Datasetin numeric de\u011fi\u015fkenler: {num_deg}\")\n   print(\"********************************************\")\n   print(f\"Datasetin categoric de\u011fi\u015fken say\u0131s\u0131: {len(cat_deg)}\")\n   print(f\"Datasetin categoric de\u011fi\u015fkenler: {cat_deg}\")\n   print(\"********************************************\")\n   print(f\"Datasetin cardinal de\u011fi\u015fken say\u0131s\u0131: {len(car_deg)}\")\n   print(f\"Datasetin cardinal de\u011fi\u015fkenler: {car_deg}\")\n   print(\"********************************************\")\n\n   return cat_deg,num_deg,car_deg\ndef dataset_ozet(data, head=5):\n    print(\"##################### Shape #####################\")\n    print(f\"Sat\u0131r say\u0131s\u0131: {data.shape[0]}\")\n    print(f\"Kolon say\u0131s\u0131: {data.shape[1]}\")\n\n    print(\"##################### Types #####################\")\n    print(data.dtypes)\n\n    print(\"##################### Head #####################\")\n    print(data.head(head))\n\n    print(\"##################### Tail #####################\")\n    print(data.tail(head))\n\n    print(\"##################### NA Kontrol\u00fc #####################\")\n    print(data.isnull().sum())\n\n    print(\"##################### Quantiles #####################\")\n    print(data.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n\n    print(\"##################### Describe Tablosu #####################\")\n    print(data.describe().T)\ndef outlier_thresholds(dataframe, col_name, q1=0.25, q3=0.75):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\ndef check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n    else:\n        return False\ndef categoric_ozet(data,degisken,plot=False,null_control=False):\n    \"\"\"\n    Task\n    ----------\n    Datasetinde bulunan categoric de\u011fi\u015fkenlerin de\u011fi\u015fken tiplerinin say\u0131s\u0131n\u0131 ve totale kar\u015f\u0131 oran\u0131n\u0131 bulur.\n    Ayr\u0131ca iste\u011fe ba\u011fl\u0131 olarak de\u011fi\u015fken da\u011f\u0131l\u0131m\u0131n\u0131n grafi\u011fini ve de\u011fi\u015fken i\u00e7inde bulunan null say\u0131s\u0131n\u0131 \u00e7\u0131kart\u0131r.\n\n    Parameters\n    ----------\n    data:pandas.DataFrame\n    categoric de\u011fi\u015fkenin bulundu\u011fu dataset.\n    degisken:String\n    Categoric de\u011fi\u015fken ismi.\n    plot:bool\n    Fonksiyonda categoric de\u011fi\u015fken da\u011f\u0131l\u0131m\u0131n\u0131n grafi\u011fini \u00e7izdirmek i\u00e7in opsiyonel \u00f6zellik.\n    null_control:bool\n    Fonksiyonda de\u011fi\u015fken i\u00e7inde null de\u011fer kontol\u00fc i\u00e7in opsiyonel \u00f6zellik\n\n    Returns\n    -------\n    tablo:pandas.DataFrame\n    Unique de\u011fi\u015fkenlerin ratio olarak oran tablosu\n    Examples\n    -------\n    df=dataset_yukle(\"titanic\")\n    cat_deg,num_deg,car_deg=degisken_tiplerine_ayirma(df,10,20)\n    for i in cat_deg:\n        tablo=categoric_ozet(df,i,True,True)\n    \"\"\"\n\n    print(pd.DataFrame({degisken: data[degisken].value_counts(),\n                        \"Ratio\": 100 * data[degisken].value_counts() \/ len(data)}))\n    tablo=pd.DataFrame({degisken: data[degisken].value_counts(),\n                        \"Ratio\": 100 * data[degisken].value_counts() \/ len(data)})\n    print(\"##########################################\")\n    if plot:\n        sns.countplot(x=data[degisken], data=data)\n        plt.show()\n    if null_control:\n        print(f\"Null veri say\u0131s\u0131: {data[degisken].isnull().sum()}\")\n\n    return tablo\ndef numeric_ozet(data,degisken,plot=False,null_control=False):\n    \"\"\"\n    Task\n    ----------\n    Datasetinde bulunan numeric de\u011fi\u015fkenlerin de\u011fi\u015fken tiplerinin say\u0131s\u0131n\u0131 ve totale kar\u015f\u0131 oran\u0131n\u0131 bulur.\n    Ayr\u0131ca iste\u011fe ba\u011fl\u0131 olarak de\u011fi\u015fken da\u011f\u0131l\u0131m\u0131n\u0131n grafi\u011fini ve de\u011fi\u015fken i\u00e7inde bulunan null say\u0131s\u0131n\u0131 \u00e7\u0131kart\u0131r.\n\n    Parameters\n    ----------\n    data:pandas.DataFrame\n    categoric de\u011fi\u015fkenin bulundu\u011fu dataset.\n    degisken:String\n    Categoric de\u011fi\u015fken ismi.\n    plot:bool\n    Fonksiyonda categoric de\u011fi\u015fken da\u011f\u0131l\u0131m\u0131n\u0131n grafi\u011fini \u00e7izdirmek i\u00e7in opsiyonel \u00f6zellik.\n    null_control:bool\n    Fonksiyonda de\u011fi\u015fken i\u00e7inde null de\u011fer kontol\u00fc i\u00e7in opsiyonel \u00f6zellik\n\n    Returns\n    -------\n    tablo:pandas.DataFrame\n    Unique de\u011fi\u015fkenlerin ratio olarak oran tablosu\n    Examples\n    -------\n    df=dataset_yukle(\"titanic\")\n    cat_deg,num_deg,car_deg=degisken_tiplerine_ayirma(df,10,20)\n    for i in cat_deg:\n        tablo=categoric_ozet(df,i,True,True)\n    \"\"\"\n    quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]\n    print(data[degisken].describe(quantiles).T)\n    \n    if plot:\n        data[degisken].hist(bins=20)\n        plt.xlabel(degisken)\n        plt.title(degisken)\n        plt.show(block=True)\n    print(\"##########################################\")\n   \n    if null_control:\n        print(f\"Null veri say\u0131s\u0131: {data[degisken].isnull().sum()}\")\n\n    \ndef target_analyser(dataframe, target,num_deg,cat_deg):\n    \n    for degisken in dataframe.columns:\n        if degisken in cat_deg:\n            print(degisken, \":\", len(dataframe[degisken].value_counts()))\n            print(pd.DataFrame({\"COUNT\": dataframe[degisken].value_counts(),\n                            \"RATIO\": dataframe[degisken].value_counts() \/ len(dataframe),\n                            \"TARGET_MEAN\": dataframe.groupby(degisken)[target].mean()}), end=\"\\n\\n\\n\")\n        if degisken in num_deg:\n            print(pd.DataFrame({\n                            \"TARGET_MEAN\": dataframe.groupby(target)[degisken].mean()}), end=\"\\n\\n\\n\")\ndef outlier_threshold(data,degisken):\n    Q1=data[degisken].quantile(0.25)\n    Q3=data[degisken].quantile(0.75)\n    Q_Inter_Range=Q3-Q1\n    alt_limit=Q1-1.5*Q_Inter_Range\n    ust_limit=Q3+1.5*Q_Inter_Range\n    return alt_limit,ust_limit\ndef threshold_degisimi(data,degisken):\n    alt_limit,ust_limit=outlier_threshold(data,degisken)\n    data[data[degisken]<alt_limit]=alt_limit\n    data[data[degisken]>ust_limit]=ust_limit\n    return data\ndef missing_values_table(dataframe, na_name=False):\n    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n\n    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[na_columns].isnull().sum() \/ dataframe.shape[0] * 100).sort_values(ascending=False)\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n    print(missing_df, end=\"\\n\")\n\n    if na_name:\n        return na_columns\n\ndef one_hot_encoder(dataframe, categorical_cols, drop_first=True):\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n    return dataframe\n\ndef model_olusturma(dff,model,target):\n    X = dff.drop(columns=target)\n    y = dff[target]\n\n\n    X_train, X_test, y_train, y_test = train_test_split(X,\n                                                        y,\n                                                        test_size=0.25,\n                                                        random_state=42)\n\n    Model=model().fit(X_train,y_train)\n    y_pred=Model.predict(X_test)\n    hata_score=np.sqrt(mean_squared_error(y_test,y_pred))\n\n    print(str(model),\"Hata de\u011feri :\",hata_score, \" RMSE: \", mean_squared_error(y_test,y_pred))\n    return hata_score,df","2658762c":"def hitter_feature_engineering(df,target):\n   \n    dataset_ozet(df)\n   \n    cat_deg,num_deg,car_deg=degisken_tiplerine_ayirma(df,10,20)\n    \n    \n    for i in cat_deg:\n        categoric_ozet(df,i,True,True)\n    \n    for i in num_deg:\n        numeric_ozet(df,i,True,True)\n\n    for i in num_deg:\n        s=check_outlier(df,i)\n        print(f\"{i} i\u00e7in outlier de\u011fer var m\u0131: {s}\")\n\n    for i in num_deg:\n        df=threshold_degisimi(df,i)\n\n  \n    df.loc[(df[\"AtBat\"] < 250), 'NEW_ATBAT_CAT'] = 'Low'\n    df.loc[(df[\"AtBat\"] >= 250) & (df[\"AtBat\"] < 380), 'NEW_ATBAT_CAT'] = 'Not Bad'\n    df.loc[(df[\"AtBat\"] >= 380) & (df[\"AtBat\"] < 520), 'NEW_ATBAT_CAT'] = 'Good'\n    df.loc[(df[\"AtBat\"] >= 520), 'NEW_ATBAT_CAT'] = 'Excellent'\n    df.groupby(\"NEW_ATBAT_CAT\")[\"Salary\"].mean()\n    \n    df.loc[(df[\"Hits\"] < 65), 'NEW_HITS_CAT'] = 'Low'\n    df.loc[(df[\"Hits\"] >= 65) & (df[\"Hits\"] < 100), 'NEW_HITS_CAT'] = 'Not Bad'\n    df.loc[(df[\"Hits\"] >= 100) & (df[\"Hits\"] < 140), 'NEW_HITS_CAT'] = 'Good'\n    df.loc[(df[\"Hits\"] >= 140), 'NEW_HITS_CAT'] = 'Excellent'\n    df.groupby('NEW_HITS_CAT')[\"Salary\"].mean()\n              \n    df.loc[(df[\"HmRun\"] < 5), 'NEW_HMRUN_CAT'] = 'Low'\n    df.loc[(df[\"HmRun\"] >= 5) & (df[\"HmRun\"] < 10), 'NEW_HMRUN_CAT'] = 'Not Bad'\n    df.loc[(df[\"HmRun\"] >= 10) & (df[\"HmRun\"] < 17), 'NEW_HMRUN_CAT'] = 'Good'\n    df.loc[(df[\"HmRun\"] >= 17), 'NEW_HMRUN_CAT'] = 'Excellent'\n    df.groupby('NEW_HMRUN_CAT')[\"Salary\"].mean()\n\n    df.loc[(df[\"Runs\"] < 30), 'NEW_RUNS_CAT'] = 'Low'\n    df.loc[(df[\"Runs\"] >= 30) & (df[\"Runs\"] < 50), 'NEW_RUNS_CAT'] = 'Not Bad'\n    df.loc[(df[\"Runs\"] >= 50) & (df[\"Runs\"] < 70), 'NEW_RUNS_CAT'] = 'Good'\n    df.loc[(df[\"Runs\"] >= 70), 'NEW_RUNS_CAT'] = 'Excellent'\n    df.groupby('NEW_RUNS_CAT')[\"Salary\"].mean()\n\n    df.loc[(df[\"RBI\"] < 30), 'NEW_RBI_CAT'] = 'Low'\n    df.loc[(df[\"RBI\"] >= 30) & (df[\"RBI\"] < 45), 'NEW_RBI_CAT'] = 'Not Bad'\n    df.loc[(df[\"RBI\"] >= 45) & (df[\"RBI\"] < 65), 'NEW_RBI_CAT'] = 'Good'\n    df.loc[(df[\"RBI\"] >= 65), 'NEW_RBI_CAT'] = 'Excellent'\n    df.groupby('NEW_RBI_CAT')[\"Salary\"].mean()\n\n    df.loc[(df[\"Walks\"] < 30), 'NEW_WALK_CAT'] = 'Low'\n    df.loc[(df[\"Walks\"] >= 30) & (df[\"Walks\"] < 45), 'NEW_WALK_CAT'] = 'Not Bad'\n    df.loc[(df[\"Walks\"] >= 45) & (df[\"Walks\"] < 65), 'NEW_WALK_CAT'] = 'Good'\n    df.loc[(df[\"Walks\"] >= 65), 'NEW_WALK_CAT'] = 'Excellent'\n    df.groupby('NEW_WALK_CAT')[\"Salary\"].mean()\n              \n    df.loc[(df[\"Years\"] < 4), 'NEW_YEAR_CAT'] = 'Rookie'\n    df.loc[(df[\"Years\"] >= 4) & (df[\"Years\"] < 6), 'NEW_YEAR_CAT'] = 'Beginner'\n    df.loc[(df[\"Years\"] >= 6) & (df[\"Years\"] < 11), 'NEW_YEAR_CAT'] = 'Ideal'\n    df.loc[(df[\"Years\"] >= 11), 'NEW_YEAR_CAT'] = 'Expert'\n    df.groupby('NEW_YEAR_CAT')[\"Salary\"].mean()        \n              \n    df.loc[(df[\"CAtBat\"] < 850), 'NEW_CATBAT_CAT'] = 'Low'\n    df.loc[(df[\"CAtBat\"] >= 850) & (df[\"CAtBat\"] < 1950), 'NEW_CATBAT_CAT'] = 'Not Bad'\n    df.loc[(df[\"CAtBat\"] >= 1950) & (df[\"CAtBat\"] < 3950), 'NEW_CATBAT_CAT'] = 'Good'\n    df.loc[(df[\"CAtBat\"] >= 3950), 'NEW_CATBAT_CAT'] = 'Excelent'\n    df.groupby('NEW_CATBAT_CAT')[\"Salary\"].mean()\n              \n    df.loc[(df[\"CHits\"] < 250), 'NEW_CHITS_CAT'] = 'Low'\n    df.loc[(df[\"CHits\"] >= 250) & (df[\"CHits\"] < 550), 'NEW_CHITS_CAT'] = 'Not Bad'\n    df.loc[(df[\"CHits\"] >= 550) & (df[\"CHits\"] < 1100), 'NEW_CHITS_CAT'] = 'Good'\n    df.loc[(df[\"CHits\"] >= 1100), 'NEW_CHITS_CAT'] = 'Excellent'\n    df.groupby('NEW_CHITS_CAT')[\"Salary\"].mean()        \n              \n    df.loc[(df[\"CHmRun\"] < 15), 'NEW_CHMRUN_CAT'] = 'Low'\n    df.loc[(df[\"CHmRun\"] >= 15) & (df[\"CHmRun\"] < 40), 'NEW_CHMRUN_CAT'] = 'Not Bad'\n    df.loc[(df[\"CHmRun\"] >= 40) & (df[\"CHmRun\"] < 95), 'NEW_CHMRUN_CAT'] = 'Good'\n    df.loc[(df[\"CHmRun\"] >= 95), 'NEW_CHMRUN_CAT'] = 'Excellent'\n    df.groupby('NEW_CHMRUN_CAT')[\"Salary\"].mean()          \n              \n    df.loc[(df[\"CRuns\"] < 100), 'NEW_CRUNS_CAT'] = 'Low'\n    df.loc[(df[\"CRuns\"] >= 100) & (df[\"CRuns\"] < 250), 'NEW_CRUNS_CAT'] = 'Not Bad'\n    df.loc[(df[\"CRuns\"] >= 250) & (df[\"CRuns\"] < 530), 'NEW_CRUNS_CAT'] = 'Good'\n    df.loc[(df[\"CRuns\"] >= 530), 'NEW_CRUNS_CAT'] = 'Excellent'\n    df.groupby('NEW_CRUNS_CAT')[\"Salary\"].mean()\n              \n    df.loc[(df[\"CRBI\"] < 90), 'NEW_CRBI_CAT'] = 'Low'\n    df.loc[(df[\"CRBI\"] >= 90) & (df[\"CRBI\"] < 230), 'NEW_CRBI_CAT'] = 'Not Bad'\n    df.loc[(df[\"CRBI\"] >= 230) & (df[\"CRBI\"] < 430), 'NEW_CRBI_CAT'] = 'Good'\n    df.loc[(df[\"CRBI\"] >= 430), 'NEW_CRBI_CAT'] = 'Excellent'\n    df.groupby('NEW_CRBI_CAT')[\"Salary\"].mean()          \n              \n              \n    df.loc[(df[\"CWalks\"] < 70), 'NEW_CWALK_CAT'] = 'Low'\n    df.loc[(df[\"CWalks\"] >= 70) & (df[\"CWalks\"] < 175), 'NEW_CWALK_CAT'] = 'Not Bad'\n    df.loc[(df[\"CWalks\"] >= 175) & (df[\"CWalks\"] < 340), 'NEW_CWALK_CAT'] = 'Good'\n    df.loc[(df[\"CWalks\"] >= 340), 'NEW_CWALK_CAT'] = 'Excellent'\n    df.groupby('NEW_CWALK_CAT')[\"Salary\"].mean()\n              \n              \n    df.loc[(df[\"PutOuts\"] < 120), 'NEW_POUT_CAT'] = 'Low'\n    df.loc[(df[\"PutOuts\"] >= 120) & (df[\"PutOuts\"] < 220), 'NEW_POUT_CAT'] = 'Not Bad'\n    df.loc[(df[\"PutOuts\"] >= 220) & (df[\"PutOuts\"] < 340), 'NEW_POUT_CAT'] = 'Good'\n    df.loc[(df[\"PutOuts\"] >= 340), 'NEW_POUT_CAT'] = 'Excellent'\n    df.groupby('NEW_POUT_CAT')[\"Salary\"].mean()          \n              \n    df.loc[(df[\"Assists\"] < 10), 'NEW_ASST_CAT'] = 'Low'\n    df.loc[(df[\"Assists\"] >= 10) & (df[\"Assists\"] < 45), 'NEW_ASST_CAT'] = 'Not Bad'\n    df.loc[(df[\"Assists\"] >= 45) & (df[\"Assists\"] < 170), 'NEW_ASST_CAT'] = 'Good'\n    df.loc[(df[\"Assists\"] >= 170), 'NEW_ASST_CAT'] = 'Excellent'\n    df.groupby('NEW_ASST_CAT')[\"Salary\"].mean()\n              \n    df.loc[(df[\"Errors\"] < 3), 'NEW_ERR_CAT'] = 'Excellent'\n    df.loc[(df[\"Errors\"] >= 3) & (df[\"Errors\"] < 6), 'NEW_ERR_CAT'] = 'Good'\n    df.loc[(df[\"Errors\"] >= 6) & (df[\"Errors\"] < 11), 'NEW_ERR_CAT'] = 'Not Bad'\n    df.loc[(df[\"Errors\"] >= 11), 'NEW_ERR_CAT'] = 'Low'\n    df.groupby('NEW_ERR_CAT')[\"Salary\"].mean()           \n              \n              \n    cat_deg,num_deg,car_deg=degisken_tiplerine_ayirma(df,10,20)\n    df.Salary.fillna(df.groupby(['NEW_HITS_CAT','NEW_CHMRUN_CAT'])[\"Salary\"].transform(\"mean\"),inplace=True)\n    \n    df=one_hot_encoder(df,cat_deg)\n    \"\"\"cat_deg=[i for i in cat_deg if i !=\"Survived\"]\n    for i in cat_deg:\n        l=0\n        while True:\n            a=df.groupby(i).mean()[target].index.tolist()\n            b=df.groupby(i).mean()[target].values.tolist()\n            r=pd.DataFrame(a,b)\n            r=r.reset_index()\n            r=pd.DataFrame(r)\n            r.columns=[\"a\",\"b\"]\n\n            sozluk={}\n            for j in r.index:\n                sozluk[r[\"b\"][j]]=r[\"a\"][j]*100\n            df[i].replace(sozluk,inplace=True)\n            l+=1\n            if l==len(df[i]):\n               break\n\"\"\"\n   \n    scaler = StandardScaler()\n    num_deg=[i for i in num_deg if i !=\"Salary\"]\n    df[num_deg] = scaler.fit_transform(df[num_deg])\n    \n\n    for mod in models:\n        model_olusturma(df,mod,target)\n    return df","b4f80c84":"df=hitter_feature_engineering(df,\"Salary\")","db029708":"X = df.drop(columns=\"Salary\")\ny = df[\"Salary\"]\nmodels = [('LR', LinearRegression()),\n          (\"Ridge\", Ridge()),\n          (\"Lasso\", Lasso()),\n          (\"ElasticNet\", ElasticNet()),\n          ('KNN', KNeighborsRegressor()),\n          ('CART', DecisionTreeRegressor()),\n          ('RF', RandomForestRegressor()),\n          ('SVR', SVR()),\n          ('GBM', GradientBoostingRegressor()),\n          (\"XGBoost\", XGBRegressor(objective='reg:squarederror')),\n          (\"LightGBM\", LGBMRegressor()),\n          (\"CatBoost\", CatBoostRegressor(verbose=False))]\n\nfor name, regressor in models:\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {round(rmse, 4)} ({name}) \")","8ad7b46a":"rf_params = {\"max_depth\": [5, 8, 15, None],\n             \"max_features\": [5, 7, \"auto\"],\n             \"min_samples_split\": [8, 15, 20],\n             \"n_estimators\": [200, 500, 1000]}\n\nxgboost_params = {\"learning_rate\": [0.1, 0.01, 0.01],\n                  \"max_depth\": [5, 8, 12, 20],\n                  \"n_estimators\": [100, 200, 300, 500],\n                  \"colsample_bytree\": [0.5, 0.8, 1]}\n\nlightgbm_params = {\"learning_rate\": [0.01, 0.1, 0.001],\n                   \"n_estimators\": [300, 500,700, 1500],\n                   \"colsample_bytree\": [0.5, 0.7, 1]}\n\nregressors = [(\"RF\", RandomForestRegressor(), rf_params),\n              ('XGBoost', XGBRegressor(objective='reg:squarederror'), xgboost_params),\n              ('LightGBM', LGBMRegressor(), lightgbm_params)]\n\nbest_models = {}\n\nfor name, regressor, params in regressors:\n    print(f\"########## {name} ##########\")\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {round(rmse, 4)} ({name}) \")\n\n    gs_best = GridSearchCV(regressor, params, cv=3, n_jobs=-1, verbose=False).fit(X, y)\n\n    final_model = regressor.set_params(**gs_best.best_params_)\n    rmse = np.mean(np.sqrt(-cross_val_score(final_model, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE (After): {round(rmse, 4)} ({name}) \")\n\n    print(f\"{name} best params: {gs_best.best_params_}\", end=\"\\n\\n\")\n\n    best_models[name] = final_model","f80373e0":"voting_reg = VotingRegressor(estimators=[('RF', best_models[\"RF\"]),\n                                         ('LightGBM', best_models[\"LightGBM\"])])\n\nvoting_reg.fit(X, y)\n\n\nnp.mean(np.sqrt(-cross_val_score(voting_reg, X, y, cv=10, scoring=\"neg_mean_squared_error\")))","c1c2ad6a":"# Loading Dataset","8bc60862":"# Loading Sub-Functions","f16ebb80":"![This is an image](https:\/\/previews.123rf.com\/images\/chromaco\/chromaco1112\/chromaco111200002\/11375504-baseball-hitter-swinging-at-a-fast-pitch-vector-illustration.jpg)"}}