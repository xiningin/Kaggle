{"cell_type":{"6e8ef0ce":"code","99d48ab4":"code","8655b940":"code","1dc98aae":"code","81ff92cd":"code","b2f0d025":"code","006e3f1a":"code","c6e61a9d":"code","6214c6f4":"code","25de8e90":"markdown","412fa024":"markdown","073bd0fe":"markdown","bdd4fa21":"markdown","eca1b318":"markdown","988a1262":"markdown","3b9086ff":"markdown"},"source":{"6e8ef0ce":"import torch\nfrom torchvision import datasets, transforms, models\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom torch import nn, optim\nimport torch.nn.functional as F\nimport pandas\nfrom sklearn.model_selection import train_test_split\nimport os","99d48ab4":"train_transforms = [\n    transforms.RandomRotation(degrees=90),\n    transforms.ColorJitter(),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    transforms.Resize(size=[224,224]),\n    transforms.ToTensor(),\n    transforms.Normalize([0.1306],[0.3081])\n]\n\ndataset = datasets.ImageFolder(root='..\/input\/dataset', transform=transforms.Compose(train_transforms))\n\ndataset_size = len(dataset) \nindices = list(range(dataset_size))[:dataset_size] \nsplit = int(np.floor(0.3 * dataset_size)) \nnp.random.shuffle(indices) \n\ntrain_indices, valid_indices = indices[split:], indices[:split] \n\ndataset_size_hold = len(train_indices)\nindices_2 = list(range(dataset_size_hold))\nholdout_split = int(np.floor(dataset_size_hold*0.35))\n\nnp.random.seed(123)\nnp.random.shuffle(indices_2)\ntrain_indices, holdout_indices = indices_2[holdout_split:], indices_2[:holdout_split]\n\ntrain = torch.utils.data.SubsetRandomSampler(train_indices) \nvalid = torch.utils.data.SubsetRandomSampler(valid_indices)\nholdout = torch.utils.data.SubsetRandomSampler(holdout_indices)","8655b940":"print(\"The size of valid dataset \" + str(len(train_indices)))\nprint(\"The size of training dataset \" + str(len(valid_indices)))\nprint(\"The size of holdout dataset \" + str(len(holdout_indices)))","1dc98aae":"train_loader = torch.utils.data.DataLoader(dataset, batch_size=64, \n                                           sampler=train, drop_last=False)\nvalidation_loader = torch.utils.data.DataLoader(dataset, batch_size=64,\n                                                sampler=valid, drop_last=False)\nholdout_loader = torch.utils.data.DataLoader(dataset, batch_size=64,\n                                                sampler=holdout, drop_last=False)","81ff92cd":"model = models.densenet161(pretrained=True)\nFREEZE = True\nif FREEZE:\n    for param in model.parameters():\n        param.requires_grad = False\n\nfeature_num = model.classifier.in_features\nmodel.classifier = nn.Sequential(nn.Linear(feature_num, 3), nn.Softmax(dim=1), nn.Dropout(p=0.2))","b2f0d025":"runtime = torch.device('cuda') if torch.cuda.is_available() else \"cpu\"\nmodel.to(runtime)","006e3f1a":"optimizer = optim.Adam(model.parameters(), lr=0.005)\nloss_criterion = nn.CrossEntropyLoss()\n\ntrain_losses = []\ntraining_acc = []\nvalid_losses = []\nvalidation_acc = []\n\nepochs = 27\nfor e in range(epochs):\n    \n    print(\"Epoch\", (e+1))\n\n    running_loss = 0\n    running_acc = 0\n    \n    model.train()\n    for images, labels in train_loader: \n        images, labels = images.to(runtime), labels.to(runtime) \n        predictions = model(images) \n        loss = loss_criterion(predictions, labels) \n\n        optimizer.zero_grad()\n        loss.backward() \n        optimizer.step()\n        running_loss += loss.item()\n        running_acc += torch.sum(torch.argmax(predictions, dim=1) == labels.data) \/ len(labels)\n    valid_loss = 0\n    valid_acc = 0    \n    model.eval()\n    with torch.no_grad():\n        for data in validation_loader:\n            images, labels = images.to(runtime), labels.to(runtime)\n            predictions = model(images)\n            loss = loss_criterion(predictions, labels) \n\n            valid_loss += loss.item()\n        \n    train_loss = running_loss\/len(train_loader)\n    valid_loss = valid_loss\/len(validation_loader)\n    valid_acc += torch.sum(torch.argmax(predictions, dim=1) == labels.data) \/ len(labels)\n    \n    print(f\"\\tTraining loss: {train_loss}\")\n    train_losses.append(train_loss)\n    print(f\"\\tValidation loss: {valid_loss}\")\n    valid_losses.append(valid_loss)  \n    print(f\"\\tTraining accuracy: {running_acc\/len(train_loader)}\")\n    training_acc.append(running_acc\/len(train_loader))\n    print(f\"\\tValidation accuracy: {valid_acc\/len(validation_loader)}\")\n    validation_acc.append(valid_acc\/len(validation_loader))","c6e61a9d":"plt.plot(train_losses, label='Training loss')\nplt.plot(valid_losses, label='Validation loss')\nplt.legend(frameon=False)\nplt.show()","6214c6f4":"plt.plot(training_acc, label='Training accuracy')\nplt.plot(validation_acc, label='Validation accuracy')\nplt.legend(frameon=False)\nplt.show()","25de8e90":"The optimizer chosen is ADAM. Mainly this comes from the previous homeworks where you've suggested to use that one. \n\nThe loss function is Cross Entropy Loss cause it's good with multi-class classification. ","412fa024":"There's visible overfitting issues with the model.\nI assume one of the reasons why it overfit was due to little dataset. Shuffling is random therefore each train may give different results. Possibly future attempts of re-training will give better or worse results. Adding more data would definitely help. And after writing this markdown I'll reduce the size of holdout and validation datasets to see if it will affect the results.","073bd0fe":"## Novel Dataset for Fine-Grained Image Categorization: Stanford Dogs\n\nI assume I need to discuss this paper (there was 2nd one too)\n\nFrom what I've learned using CNN model tries to find shapes and curves based on which it can categorize the images. In this case, some dogs have really similar looks and because of that model can't always correctly predict what breed it is. That's what the authors of the paper also mention. 120 different breeds is a lot for a relatively small data of 22000 images in my opinion. The dataset they used contains not only dogs inside, but with various backgrounds and additional presence of humans in some. This in my opinion created diversity in the data but also makes it more difficult to generalize. What I found interesting is their technique of using different training sets but keeping the fixed test set. I assume this helps with finding the best set for the training of the model. Though, they mentioned that model was giving average performance. ","bdd4fa21":"Graph shows the up curve which usually shows generalization of the model. However, it is also broken instead of more of a straight line which shows that there are issues with the model not allowing it to generalize well.","eca1b318":"The model used is Densenet (161). The only reason why this model is when I was searching for models for dog classification one of the examples was this one so I've decided to go with it.\n\nThe activation function used is Softmax cause that's what we have mainly mentioned in classes. Which essentially will give the % of the likelihood of the end predictions. \n\nI've decided to add dropout cause I think overfitting is possible in the training cause the data is too little.\n\nDidn't add batch normalization cause there were no issues with the computational speed requiring assistance.","988a1262":"I've decided to manually take the three folders you've mentioned and used it as dataset. For the case of what holdout set I've used, it is a randomly shuffled data so that in there not only one breed of dogs shown. Or otherwise we would've had very low results on holdout run.","3b9086ff":"The batch_size is chosen 64. The data isn't that huge so bigger bataches shouldn't hurt the model.\n\nSamplers are self explanatory.\n\nDrop_last is False so that in the case if there are non-full batches they are not dropped and data is not lost."}}