{"cell_type":{"92546601":"code","b147b3ed":"code","85fcc431":"code","c2f9ad0b":"code","4632bae2":"code","d558848e":"code","4ef8e32f":"code","8c2b12c1":"code","24897910":"code","65886c68":"code","92263b0e":"code","b864838e":"code","dcea230c":"code","79715e9e":"markdown","5860ec24":"markdown","64f7e074":"markdown","96a47268":"markdown","af43f18c":"markdown","185d8684":"markdown"},"source":{"92546601":"from pathlib import Path\nimport math\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm","b147b3ed":"DATA_ROOT = Path('\/kaggle\/input\/graduate-admissions\/')\nTRAIN_CSV = DATA_ROOT \/ 'Admission_Predict.csv'\n\nLR = 3e-2\nEPOCHS = 50000","85fcc431":"train_df = pd.read_csv(TRAIN_CSV)\ntrain_df.head()","c2f9ad0b":"data = train_df[['CGPA', 'Research']]\ndata = data.dropna()\nX = data.iloc[:, 0].tolist()\ny = data.iloc[:, 1].tolist()","4632bae2":"fig, ax = plt.subplots(figsize=(16, 8))\n\nax.scatter(X, y)\nplt.show()","d558848e":"min_X = min(X)\nmax_X = max(X)\n\nnumerator = [X_i - min_X for X_i in X]\ndenominator = max_X - min_X\n\nX = [item \/ denominator for item in numerator]","4ef8e32f":"def sigmoid(z):\n    return 1 \/ (1 + math.e ** -z)\n\ndef infer(X, W):\n    y_hat = sigmoid(W[0] * X + W[1])\n    \n    return y_hat\n\n\ndef compute_loss(X, y, W):\n    bs = len(X)\n    E = 0\n    \n    for X_i, y_i in zip(X, y):\n        y_hat_i = infer(X_i, W)\n        \n        err = (y_hat_i - y_i) ** 2\n        err = y_i * math.log(y_hat_i) + (1 - y_i) * math.log(1 - y_hat_i)\n        E += err\n\n    L = -(1 \/ bs) * E\n\n    return L\n\n\ndef update_weights(X, y, W, lr, epochs):\n    bs = len(X)\n    L_history = []\n\n    for epoch in tqdm(range(epochs)):\n        grad_w0 = 0\n        grad_w1 = 0\n        \n        for X_i, y_i in zip(X, y):\n            y_hat_i = infer(X_i, W)\n            grad_w0_i = (y_hat_i * (1 - y_i) - y_i * (1 - y_hat_i)) * X_i\n            grad_w1_i = y_hat_i * (1 - y_i) - y_i * (1 - y_hat_i)\n            \n            grad_w0 += grad_w0_i\n            grad_w1 += grad_w1_i\n            \n        W[0] -= lr * (1 \/ bs) * grad_w0\n        W[1] -= lr * (1 \/ bs) * grad_w1\n        \n        L = compute_loss(X, y, W)\n        \n        L_history.append(L)\n        \n    return W, L_history\n\n\ndef calculate_prec_recall_f1(preds, labels, threshold=0.5, epsilon=1e-7):\n    labels = np.array(labels, dtype=np.uint8)\n    preds = (np.array(preds) >= threshold).astype(np.uint8)\n    tp = np.count_nonzero(np.logical_and(labels, preds))\n    tn = np.count_nonzero(np.logical_not(np.logical_or(labels, preds)))\n    fp = np.count_nonzero(np.logical_not(labels)) - tn\n    fn = np.count_nonzero(labels) - tp\n    precision = tp \/ (tp + fp + epsilon)\n    recall = tp \/ (tp + fn + epsilon)\n    f1 = (2 * precision * recall) \/ (precision + recall + epsilon)\n    \n    return precision, recall, f1\n\n\ndef evaluate(X, y, W):\n    y_hat = []\n    for X_i, y_i in zip(X, y):\n        y_hat_i = infer(X_i, W)\n        y_hat.append(y_hat_i)\n    \n    prec, recall, f1 = calculate_prec_recall_f1(y_hat, y)\n    print(f'Precision: {prec}')\n    print(f'Recall: {recall}')\n    print(f'F1: {f1}')","8c2b12c1":"W = [0, 0]","24897910":"W, L_history = update_weights(X, y, W, LR, EPOCHS)","65886c68":"print(f'New weights: {W}')","92263b0e":"# Compute metrics for training set\nevaluate(X, y, W)","b864838e":"fig, ax = plt.subplots(figsize=(16, 8))\n\nax.plot(L_history)\nplt.show()","dcea230c":"fig, ax = plt.subplots(figsize=(16, 8))\n\nax.scatter(X, y)\ntmp_X = [item \/ 100 for item in range(101)]\ntmp_y = [infer(item, W) for item in tmp_X]\nax.plot(tmp_X, tmp_y, 'r--')\nplt.show()","79715e9e":"# Define helper-functions","5860ec24":"# Load data","64f7e074":"# Configure hyper-parameters","96a47268":"# Initialize weights","af43f18c":"# Preprocess data","185d8684":"# Import libraries"}}