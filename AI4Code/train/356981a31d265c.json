{"cell_type":{"0fabbabd":"code","4a836809":"code","71359a32":"code","4ce9f7cd":"code","2fdf3e53":"code","c5becadf":"code","9145148e":"code","c354f4fa":"code","f35fb8fb":"code","a600647e":"code","89a64cdd":"code","65e251c8":"code","a25627eb":"code","0cd14240":"code","ce6b446c":"code","7622fdeb":"code","20a1fd04":"code","e4889525":"code","aa625bec":"code","e59617c9":"code","eaabafad":"code","7bdf429e":"markdown","77d7c48b":"markdown","1b13669c":"markdown","570c850d":"markdown","4530b896":"markdown","47a4924a":"markdown"},"source":{"0fabbabd":"%matplotlib inline\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport json\nfrom glob import glob\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.rcParams[\"figure.figsize\"] = (8, 8)\nplt.rcParams[\"figure.dpi\"] = 150\nplt.rcParams[\"font.size\"] = 14\nplt.style.use('ggplot')\nsns.set_style(\"whitegrid\", {'axes.grid': False})\ntop_row_dict = lambda in_df: list(in_df.head(1).T.to_dict().values())[0]\nbase_dir = os.path.join('..', 'input', 'quickdraw_simplified')","4a836809":"obj_files = glob(os.path.join(base_dir, '*.ndjson'))\nprint(len(obj_files), 'categories found!', 'first is:', obj_files[0])","71359a32":"c_json = pd.read_json(obj_files[0], lines = True, chunksize = 1)\nf_row = next(c_json)\nf_dict = top_row_dict(f_row)\nf_row","4ce9f7cd":"def draw_dict(in_dict, in_ax, legend = True):\n    for i, (x_coord, y_coord) in enumerate(in_dict['drawing']):\n        in_ax.plot(x_coord, y_coord, '.-', label = 'stroke {}'.format(i))\n    if legend:\n        in_ax.legend()\n    in_ax.set_title('A {word} from {countrycode}\\n Guessed Correctly: {recognized}'.format(**in_dict))\n    in_ax.axis('off')\nfig, ax1 = plt.subplots(1, 1, figsize = (8,8))\ndraw_dict(f_dict, ax1)","2fdf3e53":"def multi_ndjson_gen(in_paths, shuffle = True):\n    json_readers = [pd.read_json(c_path, lines = True, chunksize = 1) for c_path in in_paths]\n    while True:\n        if shuffle:\n            np.random.shuffle(json_readers)\n        for c_reader in json_readers:\n            yield top_row_dict(next(c_reader))\nnd_gen = multi_ndjson_gen(obj_files)","c5becadf":"fig, m_axs = plt.subplots(3, 3, figsize = (20,20))\nfor f_dict, c_ax in zip(nd_gen, m_axs.flatten()):\n    draw_dict(f_dict, c_ax)","9145148e":"from PIL import Image\ndef strokes_to_mat(in_strokes, out_dims = (256, 256), rescale_dims = None, resample_points = 500):\n    base_img = np.zeros(out_dims, dtype = np.float32)\n    # TODO: 1d interpolation is a bad strategy here, should be improved to something that makes more sense\n    rs_points = lambda x_pts, out_dim: np.interp(np.linspace(0, 1, resample_points),\n                                        np.linspace(0, 1, len(x_pts)), \n                                        np.array(x_pts)\/256.0*out_dim).astype(int)\n    for (x_coord, y_coord) in in_strokes:\n        rx_coord = rs_points(x_coord, out_dims[1])\n        ry_coord = out_dims[0]-1-rs_points(y_coord, out_dims[0])\n        base_img[ry_coord, rx_coord] = 1.0\n    if rescale_dims is not None:\n        base_img = np.array(Image.fromarray(base_img).resize(rescale_dims, \n                                                             resample = Image.BICUBIC))\n    return base_img\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (12,8))\ndraw_dict(f_dict, ax1)\nax2.imshow(strokes_to_mat(f_dict['drawing']))\nax2.set_title('High Resolution')\nax3.matshow(strokes_to_mat(f_dict['drawing'], rescale_dims = (28, 28)), vmin = 0, cmap = 'bone')\nax3.set_title('Low Resolution')","c354f4fa":"fig, m_axs = plt.subplots(9, 9, figsize = (20,20))\nfor f_dict, c_ax in zip(nd_gen, m_axs.flatten()):\n    c_ax.imshow(strokes_to_mat(f_dict['drawing'], rescale_dims = (28, 28)))\n    c_ax.set_title(f_dict['word'])\n    c_ax.axis('off')\nfig.savefig('tiles.jpg', figdpi = 300)","f35fb8fb":"fig, m_axs = plt.subplots(9, 9, figsize = (20,20))\nfor f_dict, c_ax in zip(nd_gen, m_axs.flatten()):\n    draw_dict(f_dict, c_ax, legend = False)\n    c_ax.set_title('')\nfig.savefig('overview.jpg', figdpi = 300)","a600647e":"n_strokes = []\nstroke_length = []\ntotal_length = []\nfor f_dict, _ in zip(nd_gen, range(512)):\n    n_strokes += [len(f_dict['drawing'])]\n    stroke_length += [len(x) for x,y in f_dict['drawing']]\n    total_length += [sum([len(x) for x,y in f_dict['drawing']])]","89a64cdd":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (12, 4))\nax1.hist(n_strokes, np.arange(20))\nax1.set_title('Number of Strokes')\nax2.hist(stroke_length, np.arange(64))\nax2.set_title('Stroke Length')\nax3.hist(total_length, np.arange(200))\nax3.set_title('Total Length')\nprint('Max Strokes', np.max(n_strokes))\nprint('Max Total Length', np.max(total_length))\nprint('Total Length (99.59 percentile)', np.percentile(total_length, 99.5))","65e251c8":"def drawing_to_array(in_drawing, max_length = 100):\n    out_arr = np.zeros((max_length, 3), dtype = np.uint8) # x, y, indicator if it is a new stroke\n    c_idx = 0\n    for seg_label, (x_coord, y_coord) in enumerate(in_drawing):\n        last_idx = min(c_idx + len(x_coord), max_length)\n        seq_len = last_idx - c_idx\n        out_arr[c_idx:last_idx, 0] = x_coord[:seq_len]\n        out_arr[c_idx:last_idx, 1] = y_coord[:seq_len]\n        out_arr[c_idx, 2] = 1 # indicate a new stroke\n        c_idx = last_idx\n        if last_idx>=max_length:\n            break\n    out_arr[:last_idx, 2] += 1\n    return out_arr","a25627eb":"test_arr = drawing_to_array(f_dict['drawing'])\ntest_arr = test_arr[test_arr[:,2]>0, :] # only keep valid points\nfig, (ax1, ax2) = plt.subplots(1,2, figsize = (10, 5))\ndraw_dict(f_dict, ax1, legend = False)\nlab_idx = np.cumsum(test_arr[:,2]-1)\nfor i in np.unique(lab_idx):\n    ax2.plot(test_arr[lab_idx==i,0], \n                test_arr[lab_idx==i,1], '.-')\nax2.axis('off')\nax2.set_title('Single Array Reconstruction');","0cd14240":"!wc ..\/input\/*.ndjson","ce6b446c":"from tqdm import tqdm_notebook\nout_blocks = []\nfor c_path in tqdm_notebook(obj_files, desc = 'File Progress'):\n    for c_block in pd.read_json(c_path, lines = True, chunksize = 2100): # only take 1000\n        # export as NHWC\n        c_block['thumbnail'] = c_block['drawing'].map(lambda x: np.expand_dims(strokes_to_mat(x, rescale_dims = (28, 28)), -1))\n        c_block['strokes'] = c_block['drawing'].map(lambda x: drawing_to_array(x))\n        c_block.drop('drawing', inplace = True, axis = 1)\n        out_blocks += [c_block]\n        break","7622fdeb":"# we don't want the blocks in order otherwise batches (KerasHDF5 loader) will be very boring\nnp.random.shuffle(out_blocks)\nbig_df = pd.concat(out_blocks, ignore_index=True) #index seems to c\ndel out_blocks\nprint(big_df.shape[0], 'rows')\nbig_df.sample(2)","20a1fd04":"import h5py\nfrom tqdm import tqdm\ndef write_df_as_hdf(out_path,\n                    out_df,\n                    compression='gzip'):\n    with h5py.File(out_path, 'w') as h:\n        for k, arr_dict in tqdm(out_df.to_dict().items()):\n            try:\n                s_data = np.stack(arr_dict.values(), 0)\n                try:\n                    h.create_dataset(k, data=s_data, compression=\n                    compression)\n                except TypeError as e:\n                    try:\n                        h.create_dataset(k, data=s_data.astype(np.string_),\n                                         compression=compression)\n                    except TypeError as e2:\n                        print('%s could not be added to hdf5, %s' % (\n                            k, repr(e), repr(e2)))\n            except ValueError as e:\n                print('%s could not be created, %s' % (k, repr(e)))\n                all_shape = [np.shape(x) for x in arr_dict.values()]\n                warn('Input shapes: {}'.format(all_shape))","e4889525":"from sklearn.model_selection import train_test_split\ntrain_df, test_df = train_test_split(big_df, random_state = 2018, test_size = 0.20, stratify = big_df['word'])\ndel big_df\ntrain_df, valid_df = train_test_split(train_df, random_state = 2018, test_size = 0.33, stratify = train_df['word'])\nprint('Training', train_df.shape[0], \n      'Valid', valid_df.shape[0], \n      'Test', test_df.shape[0])","aa625bec":"write_df_as_hdf('quickdraw_train.h5', train_df)\nwrite_df_as_hdf('quickdraw_valid.h5', valid_df)\nwrite_df_as_hdf('quickdraw_test.h5', test_df)","e59617c9":"# show what is inside\nwith h5py.File('quickdraw_train.h5', 'r') as h5_data:\n    for c_key in h5_data.keys():\n        print(c_key, h5_data[c_key].shape, h5_data[c_key].dtype)","eaabafad":"!ls -lh *.h5","7bdf429e":"# Standardization and Export\nNDJSON requires a fair amount of preprocessing to feed into models, let's try to preprocess it a bit here so we can build models later. Here we just pack all the strokes together into a single fixed sized array and add stroke id to keep them seperate. We can also include a low resolution rendering and the category \/ country for reference.\n\n## Statistics\nWe calculate some statistics first to determine the appropriate size and dimensions for the data. If the number of strokes is <256 we can use an 8-bit unsigned integer for the index, and the total length is the zero-padded size of the output","77d7c48b":"# Export to HDF5\nWhile TFRecords are great for somethings, HDF5 is a lot easier for every tool but tensorflow. In addition we can open HDF5 in R or ImageJ in case there is some analysis that makes more sense in those tools","1b13669c":"# All Classes\nHere we make a nice simple round-robin generator to give us examples from each category so we can start to possibly build models","570c850d":"# Nice Figures\nHere we make a nice overview figure for the dataset description","4530b896":"# Overvew\nThe notebook shows how to load the data and visualize the results easily","47a4924a":"# Generating Images\nFor some tasks (offline classification), the strokes will need to be rasterized into an image for classification. We show a few basic examples here\n"}}