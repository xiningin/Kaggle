{"cell_type":{"2e3c6d5e":"code","a35734c6":"code","b0c659b9":"code","f3e03f1c":"code","ed3f1fc6":"code","5155e25a":"code","f21dc518":"code","3ad6161d":"code","ad23b95e":"code","73ecfcab":"code","0b9b65a7":"code","cfcfd8aa":"code","86041d14":"code","56bf5f16":"code","8ffb1843":"code","234f6641":"code","b47d3952":"code","7fe6d56c":"code","15e80e0a":"code","5d20f56d":"code","a206d0aa":"code","972422c5":"code","9a5209c8":"code","9c39e4c4":"code","8574dc0d":"code","126f87d8":"code","39bfd413":"code","1768df4b":"markdown","24becde1":"markdown","131bcac2":"markdown","5a6c5fb1":"markdown","62b8bd14":"markdown","58c2757c":"markdown","50b74864":"markdown","4dde8ef6":"markdown","de6c6b7d":"markdown","af1fda4c":"markdown","ed9a5c68":"markdown","201f41c8":"markdown","ce2579f9":"markdown","353dcebf":"markdown","223baa47":"markdown","f00d6083":"markdown","48d381bb":"markdown","eddfa30e":"markdown"},"source":{"2e3c6d5e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nData = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nTestData = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\n## Split data in Targets and Train\nTrainTargets = Data['SalePrice']\nData = Data.drop(['SalePrice'], axis=1)\nData = pd.concat([Data.assign(), TestData.assign()])","a35734c6":"Data.head(5)","b0c659b9":"# There are 81 columns, of which 79 are independent variables, 1 dependent varaible and 1 categorycal id which is not usable for this project.\nData.info()","f3e03f1c":"Data.describe().transpose()","ed3f1fc6":"# I\u00b4ll drop the ID column\nData = Data.drop('Id', axis=1)\n\n#identiy which variables are useful to the model and what will be converted to categorycal and dummie variables to enter the model\nData['MSSubClass'] = Data['MSSubClass'].astype('object')\nData['YrSold'] = Data['YrSold'].astype('object')\nData['MoSold'] = Data['MoSold'].astype('object')","5155e25a":"#Remolated or no dummie variable: year built and year remodelated\nData['Remodelated'] = np.where(Data['YearBuilt'] != Data['YearRemodAdd'], 1, 0)\nData = Data.drop(['YearBuilt', 'YearRemodAdd'],axis=1)\nData['CentralAir'] = Data['CentralAir'].map({'Y':1, 'N':0})","f21dc518":"# Mssing data heat map\nsns.set(rc={'figure.figsize':(30,8)})\nsns.heatmap(Data.isnull(), cmap=sns.color_palette(['#000099', '#ffff00']))","3ad6161d":"## Missiing data % for each feature\nToDrop = []\nfor col in Data.columns:\n    missing = np.mean(Data[col].isnull())\n    pct = round(missing * 100)\n    if pct > 35:\n        ToDrop.append(col)\n    print('{} - {}%'.format(col, round(missing * 100)))","ad23b95e":"# i\u00b4ll drop all features over 25% of missing data\nData = Data.drop(ToDrop, axis=1)","73ecfcab":"#I have to fill out the missing data with mean in case of numeric column or NO in case of categorical variables\nfor index, value in enumerate(Data.columns):\n    if Data[value].dtype != 'object':\n        med = Data[value].median()\n        Data[value].fillna(med, inplace = True)\n        #print('{} - {}'.format(med, value))\n    else:\n        Data[value].fillna('NO', inplace = True)\nData.head(5)","0b9b65a7":"# first, i\u00b4ll show possible outliers on numeric data\n# We can detect outliers with descriptive statitis watching the 75% quartile vs maximum value and 25% quartiles vs minimun value\n\nData['MSSubClass'] = Data['MSSubClass'].astype('object')\nData['YrSold'] = Data['YrSold'].astype('object')\nData['MoSold'] = Data['MoSold'].astype('object')\nData.describe().transpose()","cfcfd8aa":"# i only detected that GarageYrBuilt has a max of 2207 and a min of 1895, so i think that column is unnecessary and i\u00b4ll drop it.\nData = Data.drop(['GarageYrBlt'], axis=1)","86041d14":"#Now, i\u00b4ll show outliers on categorical data\nfor col in Data.select_dtypes(exclude=[\"number\"]).columns:\n    info = Data[col].value_counts(dropna=False)\n    print(info)\n    print()\n# i didn\u00b4t detect any outliers, so after i\u00b4ll drop unrelevant data ","56bf5f16":"#Now, i\u00b4ll show unnecessary columns, which have too many rows beign the same value. These columns don\u00b4t add value.\nnum_rows = len(Data.index)\nlow_information_cols = [] #\n\nfor col in Data.columns:\n    cnts = Data[col].value_counts(dropna=False)\n    top_pct = (cnts\/num_rows).iloc[0]\n    \n    if top_pct > 0.95:\n        low_information_cols.append(col)\n        print('{0}: {1:.5f}%'.format(col, top_pct*100))\n        print(cnts)\n        print()\nData = Data.drop(low_information_cols, axis=1)","8ffb1843":"Data['LotShape'] = Data['LotShape'].map({'Reg':4, 'IR1':3, 'IR2':2, 'IR3':1, 'NO': 0})\nData['LotShape'] = Data['LotShape'].astype('int')\n\nData['ExterQual'] = Data['ExterQual'].map({'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po': 1, 'NO': 0})\nData['ExterQual'] = Data['ExterQual'].astype('int')\n\nData['ExterCond'] = Data['ExterCond'].map({'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po': 1, 'NO': 0})\nData['ExterCond'] = Data['ExterCond'].astype('int')\n\nData['BsmtQual'] = Data['BsmtQual'].map({'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po': 1, 'NO': 0})\nData['BsmtQual'] = Data['BsmtQual'].astype('int')\n\nData['BsmtCond'] = Data['BsmtCond'].map({'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po': 1, 'NO': 0})\nData['BsmtCond'] = Data['BsmtCond'].astype('int')\n\nData['HeatingQC'] = Data['HeatingQC'].map({'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po': 1, 'NO': 0})\nData['HeatingQC'] = Data['HeatingQC'].astype('int')\n\nData['KitchenQual'] = Data['KitchenQual'].map({'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po': 1, 'NO': 0})\nData['KitchenQual'] = Data['KitchenQual'].astype('int')","234f6641":"#HeadMap using Pearson Correlation. It shows us a matrix with correlation among variables\n\nData['SalePrice'] = TrainTargets #I need SalePrice in my data set to check the correlation \n\nDataCor = Data[0:len(TrainTargets)]\nplt.figure(figsize=(30,20))\nsns.set(font_scale=1)\nCorrelation = DataCor.select_dtypes(exclude=[\"object\"])\nCorrelation = Correlation.corr(method='pearson')\nsns.heatmap(Correlation, annot=True, cmap='Greens')\nplt.show()\n\nData = Data.drop(['SalePrice'], axis=1)# I don\u00b4t need SalePrice in my dataset anymore.","b47d3952":"CuantiColumns = Data.select_dtypes(exclude=['object']).columns\nCatColumns = Data.select_dtypes(exclude=[\"number\"]).columns\nData.reindex()","7fe6d56c":"#Histogram sale price\n#We have 180k as mean and std of 79k\n#The min value es larger than zero, exellenten, i don\u00b4t have outliers which drestroy my model\n#We have a appreciable positive skewness and normal distribution\nsns.distplot(TrainTargets, color='Orange', label='Sale price distribution')\nplt.show()\nTrainTargets.describe()","15e80e0a":"#Exploration: SalePrice vs Cuantitative variables\n#There might be outliers in some variables but how the data has too many columns the outliers in each variables could be due to all of others variables.\nsns.set(font_scale=1.5)\nfig, axs = plt.subplots(9,4,figsize=(40,60))\naxes = np.array(axs).reshape(-1).tolist()\nfor index, value in enumerate(CuantiColumns):\n    sns.scatterplot(x=value, y=TrainTargets, data=Data[0:len(TrainTargets)], ax=axes[index])\n","5d20f56d":"#Check for cualitatives variables\n#The most important variable which i\u00b4ve detected was \"Overall Qualitie\" where it tell us the general rate from each house, we see that it\u00b4s exponecial, if we have a expensive house, we have a large general rate\nfig, axs = plt.subplots(7, 4, figsize=(40,40))\naxes = np.array(axs).reshape(-1,).tolist()\nfor index, value in enumerate(CatColumns):\n    figura = sns.stripplot(x=value, y=TrainTargets, data=Data[0:len(TrainTargets)], ax=axes[index])\n","a206d0aa":"#I\u00b4ll get dummies on cualitative columns before introduce it to the regression\nData = pd.get_dummies(Data)\n#Split the two data\nTestData = Data[len(TrainTargets):]\nData = Data[0:len(TrainTargets)]","972422c5":"from sklearn import preprocessing\n\n#Data to scale\nCuantiColumns = Data.select_dtypes(exclude=['object']).columns\n\nTrainScaled = Data\nTestScaled = TestData\nCuantiColumns = CuantiColumns.drop(['Remodelated', 'CentralAir'])# I won\u00b4t scale dummy variables\nTrainScaled[CuantiColumns] = preprocessing.scale(TrainScaled[CuantiColumns])\nTestScaled[CuantiColumns] = preprocessing.scale(TestData[CuantiColumns])","9a5209c8":"#i will select all cuantitative columns again, including Dummy variables\nCuantiColumns = Data.select_dtypes(exclude=[\"object\"]).columns","9c39e4c4":"import statsmodels.api as sm\ny = TrainTargets\nx1 = TrainScaled[CuantiColumns]\n\nx = sm.add_constant(x1)\nresults = sm.OLS(y, x).fit()\nresults.summary()","8574dc0d":"x = sm.add_constant(TestScaled[CuantiColumns])\nOls_Prediction = results.predict(x) \nOls_Prediction","126f87d8":"from sklearn.tree import DecisionTreeRegressor\n\nTreeRegressor = DecisionTreeRegressor(random_state=0).fit(x1, y)\n\nTreePrediction = TreeRegressor.predict(TestScaled[CuantiColumns])\nTreePrediction","39bfd413":"\nToSave = pd.DataFrame(range(1461, 2920, 1))\nPredictions = np.array(Ols_Prediction)\nToSave = ToSave.rename(columns= {0: 'Id'})\nToSave['Saleprice'] = Predictions\n\n#ToSave.to_csv('Files\/Predictions_House_Submission.csv', index=False, header=True)","1768df4b":"## Unnecessary data","24becde1":"## Standarize variables","131bcac2":"## Decision tree, machine learning","5a6c5fb1":"# Exploration and analysis of data","62b8bd14":"## Missing data","58c2757c":"## Outliers","50b74864":"### Using the Ols regressioin I've obtained a good result on r2 adjusted with almost 0.8 score. I think there were a lot of useful variables however as well there were  variables which were redundant and I decided to remove them. How the data doesn\u2019t have too many rows, there were no outliers that broken the regression. We can understand the most important values with the visualizations, especially characteristics which are no numerical and understand how the price increment with mayor characteristics, for instance a good heat or a pool.","4dde8ef6":"## Split data, Train and Test - Get Dummies","de6c6b7d":"## Pearson correlation","af1fda4c":"<h1>House sales prediction by python and some regression models<\/h1>\n<p>This kernel is going to solve a house pricing dataset, prediction the sales prices of houses with 79 idenpendet variables.<\/p>\n<p>Data from: <a href=\"https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\">Link<\/a> it contains train data and test data.<\/p>\n<h2>Goals<\/h2>\n<ul>\n<li>Visualize data and prove some insights<\/li>\n<li>Clean and preprocess data<\/li>\n<li>Compare dependent variable vs independent variables<\/li>\n<li>Find most important variables<\/li>\n<li>Try to explain easily the insights for a bussines enviorement<\/li>\n<li>Implement some regressions models and predict sale prices<\/li>\n<\/ul>\n\n<h2>Content<\/h2>\n<ol>\n<li>Data first check<\/li>\n<li>Data clean and preprocess<\/li>\n<li>Exploration and analysis of data <\/li>\n<li>Implementation models<\/li>\n<li>Conclusions<\/li>\n<\/ol>","ed9a5c68":"# Data clean","201f41c8":"# Data first check","ce2579f9":"## Save predictions","353dcebf":"# Conclusions","223baa47":"## Multiple Linear regression with statsmodels OLS","f00d6083":"# Implementation models","48d381bb":"## Select columns cualitative and cuantitative","eddfa30e":"## Map ordinal data"}}