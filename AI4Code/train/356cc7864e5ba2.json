{"cell_type":{"b552d42e":"code","7809c79c":"code","4f480f6e":"code","8b630b7e":"code","44681dd5":"code","04fb456f":"code","a64e47ae":"code","ee111e20":"code","f4146066":"code","10d25d4a":"code","68e15443":"code","b2ac90af":"code","7990350b":"code","4bb42f96":"code","25da62a5":"code","057fde98":"code","cfbdc56f":"code","82f4073c":"code","a8367a7f":"code","96379e5c":"code","387b8fc9":"code","aa477b92":"code","4da5568f":"code","9109174e":"code","2c458841":"markdown","bb190edd":"markdown","e6aae572":"markdown","5dd86a56":"markdown","10ca8fcd":"markdown","b627b61c":"markdown","35e1ce5c":"markdown","4decec4a":"markdown","93abcfe1":"markdown","ee41f9d7":"markdown","efc5ba70":"markdown","52f7ae8d":"markdown","0283b81b":"markdown","49748ea8":"markdown","587cb1a2":"markdown","d300cafc":"markdown","93b952fe":"markdown","8fec8aa8":"markdown","f5d4f0c3":"markdown","19e74d1a":"markdown","5bbfdbe0":"markdown","7a483cb1":"markdown","566cdaac":"markdown","7dc32af1":"markdown"},"source":{"b552d42e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport math, time\nimport itertools\nimport datetime\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nimport torch\nimport torch.utils.data\nimport torch.nn as nn\nimport seaborn as sns\ntry:\n   import cPickle as pickle\nexcept:\n   import pickle\n\nsns.set_style('whitegrid')\nplt.style.use(\"fivethirtyeight\")","7809c79c":"dates = pd.date_range('1981-01-01','1991-01-01',freq='B')\ndf1=pd.DataFrame(index=dates)\ndf_temp=pd.read_csv(\"..\/input\/daily-min-temperatures\/daily-min-temperatures.csv\", parse_dates=True, index_col=0)\nprint(df_temp.describe())\n\n\ndf_temp=df1.join(df_temp)\n\n# extract month series from index\ndf_temp['Month'] = df1.index.month\n\n\ndf_temp[['Temp']].plot(figsize=(15, 8))\nplt.ylabel(\"Temperature\")\nplt.title(\"10-years temperatures\")\n\ndf_temp[['Month']].plot(figsize=(15, 8))\nplt.ylabel(\"Month of the year\")","4f480f6e":"print(df_temp[df_temp['Temp'].isnull()])\n\ndf_temp.info()\n\n# df_temp=df_temp.fillna(method='ffill')\ndf_temp = df_temp.interpolate(method='linear')\n\ndf_temp.info()","8b630b7e":"fig = plt.figure(figsize=(15,8))\n\nplt.subplot(2, 1, 1)\ndiff = df_temp['Temp'].diff()\ndiff.plot(linestyle='--', marker='o', title='Daily temperature difference (\u00b0C)')\n\nplt.subplot(2, 1, 2)\npct_diff = df_temp['Temp'].pct_change()\npct_diff.plot(linestyle='--', marker='o', title='Daily temperature difference (%)')\nfig.tight_layout()","44681dd5":"plt.figure(figsize=(12, 12))\n\nsns.distplot(df_temp['Temp'], bins=100, color='purple')\nplt.ylabel('Temperature')\nplt.title(f'Temperature across 10 years')","04fb456f":"ma_day = [2, 15, 30]\naverages = pd.DataFrame()\naverages[\"Original temperature\"] = df_temp[\"Temp\"]\nfor ma in ma_day:\n    column_name = f\"MA for {ma} days\"\n    averages[column_name] = df_temp['Temp'].rolling(ma).mean()\n    \ndf_temp['Temp'] = df_temp['Temp'].rolling(2).mean()  \ndf_temp['Temp'][0] = df_temp['Temp'][1]\n    \naverages.plot(figsize=(15, 8))","a64e47ae":"scaler = MinMaxScaler(feature_range=(-1, 1))\ndf_temp['Temp'] = scaler.fit_transform(df_temp['Temp'].values.reshape(-1,1))\n\nscaler2 = MinMaxScaler(feature_range=(0, 1))\ndf_temp['Month'] = scaler2.fit_transform(df_temp['Month'].values.reshape(-1,1))\n\nprint(df_temp.head())\nprint(df_temp.tail())","ee111e20":"# function to create train, test data given tenerature data and sequence length\ndef load_data(data, look_back, test_set_ratio):\n    data_raw = data.to_numpy() # convert to numpy array\n    data = []\n    \n    # create all possible sequences of length seq_len\n    for index in range(len(data_raw) - look_back): \n        data.append(data_raw[index: index + look_back])\n    \n    data = np.array(data);\n    test_set_size = int(np.round(test_set_ratio*data.shape[0]));\n    train_set_size = data.shape[0] - (test_set_size);\n    \n    x_train = data[:train_set_size,:-1,:]\n    y_train = np.squeeze(data[:train_set_size,-1,0])\n    \n    x_test = data[train_set_size:,:-1]\n    y_test = np.squeeze(data[train_set_size:,-1,0])\n    \n    return [x_train, y_train, x_test, y_test]\n\nlook_back = 30 # 30 # choose sequence length\nx_train, y_train, x_test, y_test = load_data(df_temp, look_back, test_set_ratio=0.1)\nprint('x_train.shape = ',x_train.shape)\nprint('y_train.shape = ',y_train.shape)\nprint('x_test.shape = ',x_test.shape)\nprint('y_test.shape = ',y_test.shape)","f4146066":"# make training and test sets in torch\nx_train = torch.from_numpy(x_train).type(torch.Tensor)\nx_test = torch.from_numpy(x_test).type(torch.Tensor)\ny_train = torch.from_numpy(y_train).type(torch.Tensor)\ny_test = torch.from_numpy(y_test).type(torch.Tensor)","10d25d4a":"y_train.size(),x_train.size()","68e15443":"def create_data_loaders(x_train, y_train, x_test, y_test, batch_size):\n\n    train = torch.utils.data.TensorDataset(x_train, y_train)\n    test = torch.utils.data.TensorDataset(x_test, y_test)\n\n    train_loader = torch.utils.data.DataLoader(\n        dataset=train, \n        batch_size=batch_size, \n        shuffle=True\n    )\n\n    test_loader = torch.utils.data.DataLoader(\n        dataset=test, \n        batch_size=batch_size, \n        shuffle=True\n    )\n    \n    return (train_loader, test_loader)","b2ac90af":"class EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement. \n                            Default: False\n            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n                            Default: 0\n            path (str): Path for the checkpoint to be saved to.\n                            Default: 'checkpoint.pt'\n            trace_func (function): trace print function.\n                            Default: print            \n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n        self.path = path\n        self.trace_func = trace_func\n    def __call__(self, val_loss, model):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            if self.verbose:\n              self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        '''Saves model when validation loss decrease.'''\n        if self.verbose:\n            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), self.path)\n        self.val_loss_min = val_loss","7990350b":"class LSTM(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n        super(LSTM, self).__init__()\n        # Hidden dimensions\n        self.hidden_dim = hidden_dim\n\n        # Number of hidden layers\n        self.num_layers = num_layers\n\n        # Building your LSTM\n        # batch_first=True causes input\/output tensors to be of shape\n        # (batch_dim, seq_dim, feature_dim)\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, dropout = 0.25, batch_first=True)\n\n        self.fc1 = nn.Linear(self.hidden_dim, self.hidden_dim)\n        self.fc1.weight.data.normal_()\n        self.fc2 = nn.Linear(self.hidden_dim, 10)\n        self.fc3 = nn.Linear(10, 1)\n        self.relu = nn.ReLU()\n        self.T = 10\n\n    def forward(self, x):\n        # Initialize hidden state with zeros\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n\n        # Initialize cell state\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n\n        # One time step\n        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n        # If we don't, we'll backprop all the way to the start even after going through another batch\n        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n\n        out = out[:, -1, :]  # last prediction\n        out = self.fc1(out)\n        out = self.fc2(out)\n        out = self.relu(out)\n        out = self.fc3(out)\n        return out","4bb42f96":"hidden_dim = 32 #32\nnum_layers = 1 #1\ninitial_lr=0.01 #0.01\nnum_epochs = 1000    \ninput_dim = x_train.size()[2]\n    \nmodel = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=1, num_layers=num_layers)\n\nprint(model)\nprint(len(list(model.parameters())))\nfor i in range(len(list(model.parameters()))):\n    print(list(model.parameters())[i].size())","25da62a5":"def train(model, optimiser, scheduler, loss_fn, num_epochs, train_loader, test_loader, early_stopping_patience = False, early_stopping_delta=10e-6, gradient_clipping = 10e-0):\n    \"\"\"Trains the model given training and test dataset\"\"\"\n    \"\"\"\n        Args:\n            model (NN): The pytorch neural network model used for training\n            optimiser (Optimizer): pytorch optimizer object\n            scheduler (Scheduler): pytorch optimizer scheduler\n            loss_fn (Loss): pytorch loss function object\n            num_epochs (int): Number of epochs to traing the model with\n            train_loader (DataLoader): training loader\n            test_loader (DataLoader): test loader\n            early_stopping_patience (int|bool|None): if a value is provided it is used as EarlyStopping patience parameter\n            early_stopping_delta(decimal): minimum value considered as improvement\n            gradient_clipping: maximum gradient allowed\n        \"\"\"\n    \n#     def gradient_clipping_hook(grad):\n#         torch.clamp(grad, -gradient_clipping, gradient_clipping)\n    \n#     for p in model.parameters():\n#         p.register_hook(gradient_clipping_hook)\n    \n    train_losses = []\n    validation_losses = []\n    \n    if early_stopping_patience:\n        early_stopping = EarlyStopping(patience=early_stopping_patience, verbose=True, delta=early_stopping_delta)\n        \n    for t in range(num_epochs):\n        training_loss_sum = 0\n        \n        model.train()\n        for x_train, y_train in train_loader:\n            y_train_pred = model(x_train)\n            train_loss = loss_fn(torch.squeeze(y_train_pred), torch.squeeze(y_train))\n            \n            \n            # Backward pass\n            train_loss.backward()\n            training_loss_sum += train_loss.item()\n\n            \n            # Clip gradient\n            if gradient_clipping:\n                torch.nn.utils.clip_grad_value_(model.parameters(), gradient_clipping)\n\n            # Update parameters            \n            optimiser.step()\n            \n        train_losses.append(training_loss_sum)\n            \n        if t % 10 == 0 and t !=0:\n            print(\"Epoch \", t, \"Training MSE: \", training_loss_sum)\n\n        # Zero out gradient, else they will accumulate between epochs\n        optimiser.zero_grad()\n\n        \n\n        # Validation\n        with torch.no_grad():\n            model.eval()\n            \n            validation_loss_sum = 0  \n            for x_test, y_test in test_loader:            \n                y_test_pred = model(x_test)\n                validation_loss = loss_fn(torch.squeeze(y_test_pred), torch.squeeze(y_test))    \n                \n                validation_loss_sum += validation_loss.item()\n\n            validation_losses.append(validation_loss_sum)\n\n        if early_stopping_patience:    \n            if early_stopping.early_stop:\n                print(\"Early stopping\")\n                model.load_state_dict(torch.load('.\/checkpoint.pt'))     \n                break # we could break here, but want to see how much training was saved\n            else:\n                early_stopping(validation_loss_sum, model)\n                \n        if scheduler is not None:\n            scheduler.step()\n                \n    return  (model, train_losses, validation_losses)\n      ","057fde98":"def training_results(model, x_train, train_losses, validation_losses):\n    y_train_pred = model(x_train)\n\n    plt.figure(figsize=(15,8))\n    plt.plot(train_losses, label=\"Training loss\")\n    plt.plot(validation_losses, label=\"Validation loss\")\n    # find position of lowest validation loss\n    minloss = validation_losses.index(min(validation_losses))\n    plt.axvline(minloss, linestyle='--', color='r',label='Early Stopping Checkpoint')\n    plt.legend()\n    plt.show()\n    \n    return y_train_pred","cfbdc56f":"def validation_results(model, x_test, y_test, y_train, y_train_pred):\n    # make predictions\n    y_test_pred = model(x_test)\n\n    # invert predictions\n    y_train_pred = scaler.inverse_transform(y_train_pred.detach().numpy())\n    y_train = scaler.inverse_transform(y_train.detach().numpy().reshape(-1, 1))\n    y_test_pred = scaler.inverse_transform(y_test_pred.detach().numpy())\n    y_test = scaler.inverse_transform(y_test.detach().numpy().reshape(-1, 1))\n\n    # calculate root mean squared error\n    train_score = math.sqrt(mean_squared_error(y_train[:,0], y_train_pred[:,0]))\n    print('Train Score: %.2f RMSE' % (train_score))\n    test_score = math.sqrt(mean_squared_error(y_test[:,0], y_test_pred[:,0]))\n    print('Test Score: %.2f RMSE' % (test_score))\n    \n    train_error = abs(y_train[:,0] - y_train_pred[:,0])\n\n    mean_train_error = sum(train_error) \/ len(train_error) \n    print('Mean Train Error: %.2f (\u00b0C)' % (mean_train_error))\n    min_train_error = min(train_error)\n    print('Min Train Error: %.2f (\u00b0C)' % (min_train_error))\n    max_train_error = max(train_error)\n    print('Max Train Error: %.2f (\u00b0C)' % (max_train_error))\n\n    test_error = abs(y_test[:,0] - y_test_pred[:,0])\n\n    mean_test_error = sum(test_error) \/ len(test_error) \n    print('Mean Test Error: %.2f (\u00b0C)' % (mean_test_error))\n    min_test_error = min(test_error)\n    print('Min Test Error: %.2f (\u00b0C)' % (min_test_error))\n    max_test_error = max(test_error)\n    print('Max Test Error: %.2f (\u00b0C)' % (max_test_error))\n    \n    return (y_train_pred, y_test_pred, train_error, test_error)\n","82f4073c":"def prediction_plots(df, y_train_pred, y_test_pred, train_error, test_error, look_back, scaler):\n    data = scaler.inverse_transform(df[\"Temp\"].values.reshape(-1,1))\n    \n    # shift train predictions for plotting\n    train_predict_plot = np.empty_like(df)\n    train_predict_plot[:, :] = np.nan\n    train_predict_plot[look_back:len(y_train_pred)+look_back, :] = y_train_pred\n       \n\n    # shift train error for plotting\n    train_error_plot = np.empty_like(df[\"Temp\"]).ravel()\n    train_error_plot[:] = np.nan\n    train_error_plot[look_back:len(train_error)+look_back] = train_error\n    \n    max_train_error_pos = np.nanargmax(train_error_plot)\n    min_train_error_pos = np.nanargmin(train_error_plot)\n\n\n    # shift test predictions for plotting\n    test_predict_plot = np.empty_like(df)\n    test_predict_plot[:, :] = np.nan\n    test_predict_plot[len(y_train_pred)+look_back-1:len(df)-1, :] = y_test_pred\n\n    # shift test error for plotting\n    test_error_plot = np.empty_like(df[\"Temp\"]).ravel()\n    test_error_plot[:] = np.nan\n    test_error_plot[len(df)-1-len(test_error):len(df)-1] = test_error\n    \n    max_test_error_pos = np.nanargmax(test_error_plot)\n    min_test_error_pos = np.nanargmin(test_error_plot)\n\n    # plot baseline and predictionstrainError\n\n    plt.figure(figsize=(15,24))\n    \n    plt.subplot(3, 1, 1)\n    plt.plot(data, label = 'Actual temperatures')\n    plt.plot(train_predict_plot, 'c', label = 'Trained temperatures', alpha=0.6)\n    plt.plot(test_predict_plot, 'y', label = 'Predicted temperatures', alpha=0.6)\n    plt.ylim((-2,30))\n    plt.legend()\n    \n    \n    plt.subplot(3, 1, 2)\n    plt.vlines(range(1,len(train_error_plot)), 0, train_error_plot, 'c', alpha=0.25, label = \"Training error\")\n    plt.vlines(range(1,len(test_error_plot)), 0, test_error_plot, 'y', alpha=0.25, label = \"Prediction error\") \n    plt.ylim((-2,30))\n    plt.legend()\n    \n    \n    plt.subplot(3, 1, 3)\n    test_plot = test_predict_plot[len(y_train_pred)+look_back-1:len(df)-1, :]\n    plt.plot(data[len(df)-1-len(test_error):len(df)-1:], label = 'Actual temperatures')\n    plt.plot(y_test_pred, label = 'Predicted temperatures')\n    plt.legend()\n    \n    plt.show()\n","a8367a7f":"loss_fn = torch.nn.MSELoss(reduction='mean')\n# loss_fn = torch.nn.L1Loss(reduction='mean')\n\noptimiser = torch.optim.Adam(model.parameters(), lr=initial_lr)\n# optimiser = torch.optim.SGD(model.parameters(), lr=lr)\n\nscheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=30, gamma=0.9, verbose=True)\n# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimiser, [30, 100, 200], gamma=0.1, last_epoch=-1, verbose=True)\n# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimiser, verbose = True)\n# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimiser, T_0 = 1, verbose = True)\n\n(train_loader, test_loader) = create_data_loaders(x_train, y_train, x_test, y_test, batch_size = 1024)  ","96379e5c":"(model, train_losses, validation_losses) = train(model, optimiser, scheduler, loss_fn, num_epochs, train_loader, test_loader, early_stopping_patience = 70, early_stopping_delta=10e-6, gradient_clipping = 10e-3)","387b8fc9":"y_train_pred_scaled = training_results(model, x_train, train_losses, validation_losses)\n\n(y_train_pred, y_test_pred, train_error, test_error) = validation_results(model, x_test, y_test, y_train, y_train_pred_scaled)\n\nprediction_plots(df_temp, y_train_pred, y_test_pred, train_error, test_error, look_back, scaler)","aa477b92":"(model, train_losses, validation_losses) = train(model, optimiser, None, loss_fn, num_epochs, train_loader, test_loader, early_stopping_patience = False, gradient_clipping = 10e-2)","4da5568f":"y_train_pred_scaled = training_results(model, x_train, train_losses, validation_losses)\n\n(y_train_pred, y_test_pred, train_error, test_error) = validation_results(model, x_test, y_test, y_train, y_train_pred_scaled)\n\nprediction_plots(df_temp, y_train_pred, y_test_pred, train_error, test_error, look_back, scaler)","9109174e":"def run_simulation(bs, log_learning_rates):\n    (train_loader, test_loader) = create_data_loaders(x_train, y_train, x_test, y_test, batch_size = bs)    \n    losses = []\n    for log_lr in log_learning_rates:\n        print(f\"Running simulation for batch_size {bs} and learning rate {10**log_lr}.\")\n        model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=1, num_layers=num_layers)\n        optimiser = torch.optim.Adam(model.parameters(), lr=10**log_lr)\n        (model, train_losses, validation_losses) = train(model, optimiser, None, loss_fn, num_epochs, train_loader, test_loader, early_stopping_patience = 50, early_stopping_delta=10e-6, gradient_clipping = False)\n        losses.append(min(validation_losses))\n        \n    return losses\n\n\ndef was_precomputed(filename):\n    try:\n        f = open(filename)\n        f.close()\n        return True        \n    except IOError:\n        return False\n    \n\ndef batch_size_vs_learning_rate_plot():\n    plt.figure(figsize=(15,8))\n    \n    batch_sizes = (32, 64, 128, 256, 512, 1024,)\n    log_learning_rates = np.linspace(-5,0,50)\n\n    for bs in batch_sizes:\n        filename = f'..\/input\/temperature-losses\/losses_bs_{bs}_s.pkl'\n        if was_precomputed(filename): \n            losses = pickle.load(open(filename, \"rb\" ))\n        else:\n            losses = run_simulation(bs, log_learning_rates)\n\n            afile = open(f'.\/losses_bs_{bs}_s.pkl', 'wb')\n            pickle.dump(losses, afile)\n            afile.close()\n        plt.plot(log_learning_rates, losses, label=f\"Batch size = {bs}\")  \n        \n    plt.ylim((0, 1.0))\n    plt.xlabel(\"log(learning_rate)\")\n    plt.ylabel(\"loss [MSE]\")\n    plt.legend()\n    plt.show()\n                 \n                 \nbatch_size_vs_learning_rate_plot()    \n    ","2c458841":"## 6. Main execution\n\n### 6.1 Training\n\nIn this phase training is actually executed. \n\nUser must choose:\n- A loss function (MSE, L1)\n- An optimiser must be chosen (Adam, SGD, ...)\n- Optionally, an optimiser scheduler can be defined to change the learning rate during the train process according to the implemented strategy ()\n\nIt can be ran with or without EarlyStopping. In the first case, the proecdure stops when the test metric cannot be improved and best generalization is achieved. In the latter, the procedure stops after `n_epochs`. Note that this allows a better fit of training data but leads to overfitting.\n\n**It is suggested to run the notebook in both cases to see the difference**.","bb190edd":"### 5.3. Prediction and error plots\n\nThe following code shows the plots of the train and test predictions (orange and green lines), superimpsed on the actual data (blue line), along with the training and test prediction errors (semi-transparent bottom lines).","e6aae572":"## 1. Libraries and settings\n\nSome libraries are loaded, some are standard libraries, while other are related to pytorch (for neural networks related tasks) and sklearn (for metrics and preprocessing)","5dd86a56":"### 2.4. Input normalization\n\nLinear rescaling of the input variables. Data is normalized in the range [-1, 1]. \n\nThis is required for avoiding gradient explosion or getting stuck in local optima.","10ca8fcd":"\nThe plot shows the resulting loss in terms of MSE as a function of learning rate (in x axis, logarithmic scale) and a given batch size (y_axis). The results show that the minimum batch size to achieve convergence to a minimum is 256. For such value and higher ones, learning rate could be set as 10^-3.\n\nIncreasing the batch size, a broader range of learning rate value could be used while still allowing convergence to minimum loss. In such cases, choosing the larger  learning rate value would determine a faster convergence (i.e. a lower number of epochs required).","b627b61c":"### Figures\nFrom top to bottom:\n- Training phase: Loss function value vs epoch. Training data (in blue) and test data (in red)\n- Training results: fitting of training data (in cyan) and predicted temperatures (in yellow), overimposed on training and test actual data (in blue)\n- Plot of the absolute error for training data (in cyan) and for test data (in yellow)","35e1ce5c":"### Figures\nFrom top to bottom:\n- Training phase: Loss function value vs epoch. Training data (in blue) and test data (in red)\n- Training results: fitting of training data (in cyan) and predicted temperatures (in yellow), overimposed on training and test actual data (in blue)\n- Plot of the absolute error for training data (in cyan) and for test data (in yellow)","4decec4a":"- **Discrete distribution** (histogram) and pdf estimation","93abcfe1":"### 2.2. Removing \"holes\"\nChecking for null values and showing some information about the dataset.\n\nNull values are then filled using linear interpolation strategy.","ee41f9d7":"### 2.6. Create Data Loaders\n\nSince neural network will be built using mini-batches, training and test data are organized using data loaders with `batch_size` provided as argument.\n\nThis function will be called before the training phase.","efc5ba70":"# LSTM for predicting Temperatures\nIn this notebook we will be building and training LSTM to predict Temperatures. We will use PyTorch.\n\n\nThe project consists of the following sections:\n1. Libraries and settings\n2. Data Loading, analysis and Pre-processing\n3. EarlyStopping class definition\n4. LSTM model definition\n5. Evaluation functions\n6. Main execution\n","52f7ae8d":"## 2. Data loading and pre-processing\n\n### 2.1. Data is loaded from a csv file containing two columns\n\nThe loaded columns are\n\nDate | Temp\n\nindicating the measured temperature for a given day of the year.\n\nSince some data may be missing, a continuous empty time series dataframe is initially built, then the dataframe containing data is merged. This generates `null` values that must be filled\n","0283b81b":"## 5. Evaluation functions\n\n### 5.1. Training vs validation performance evaluation\n\nThe following plot shows how the model fits training and test data. The red vertical link marks the best point found using EarlyStopping, i.e. the actual final model.","49748ea8":"## 4. LSTM Model definition\n\n### 4.1 Network architecture definition\n\nThe following class defines the architecture used for the addressed problem. \n\n![image.png](attachment:image.png)\n\n\nIt consists of two layers:\n\na **Long Short Term Memory (LSTM)** network configurable using the following arguments:\n- input size\n- hidden layer size\n- number of hidden layers\n- output size\n\nthree fully connected layear of decreasing size\n1. hidden_dim * hidden_dim\n2. hidden_dim * 10 \n3. 10 * 1\n4. ReLU output activtion","587cb1a2":"### 4.2. Model instantiation and configuration\n\nThe previously designed model is instantiated and parameters are shown.\n\nThe following parameters must be defined:\n- number of hidden layers\n- hidden layers size\n- learning rate\n- number of epochs\n\nFinally, model parameters are shown.","d300cafc":"## 2.3. Perform some analysis\n\n- **Daily difference**, in absoult and percentage terms.","93b952fe":"## 4. Training function\n\n### 4.1. Training function definition\n\nA training function for the LSTM network is defined. It uses the following arguments:\n- Neural network model\n- Optimiser\n- Loss function\n- Number of epochs\n- Training data loader\n- Testing data loader\n- Whether to use early stopping or not (using False, None or integer patience value)\n- Early stopping minimum amount of improvement\n- Whether to use gradient clipping or not (using False, None or float gradient clip value)\n\nThe training runs for `num_epochs` epochs. For each epoch each minibatch is used to train the model using stochastic gradient descent. Then, model is evaluated using the test set. Loss function is evaluated for both training and test set to keep track of how the process progresses in terms of fitting. These data will be plotted at the the end of the process.\n\nIf EarlyStopping is used, training history is checked to see if the training should be stopped early. In this case, the model is recovered from the last checkpoint and the training is stopped.","8fec8aa8":"### 2.5. Splitting data into training and test set\n\nData is distributed into training and test set according to a `test_set_ratio` ratio, which for this purpose is set to `0.1` (i.e. 90% of data is assigned to train set, 10% to validation set).\n\nFinally, data is converted to pytorch tensors.","f5d4f0c3":"### 6.2. Results with Early Stopping\n\n- Plot of training performance across epoches\n- Plot of training and test data vs prediction fit and errors","19e74d1a":"### 6.2. Results without Early Stopping (overfitting)\n\n- Plot of training performance across epoches\n- Plot of training and test data vs prediction fit and errors","5bbfdbe0":"### 5.2. Error evaluation\n\nThe following code measure RMSE, min and max absolute error measured in degrees (\u00b0C)","7a483cb1":"- **Moving average** for 2, 15 and 30 days, aimed to noise reduction and underlying pattern revealing","566cdaac":"## 3. EarlyStopping class definition\n\nThis class serves as utility for early stopping the training process when there is not any validation gain over a given number of training steps (`patience` parameter).\n\nEvery time a better validation is measured a model checkpoint is saved and should be reloaded once the training ends before running the model in production.\n\nIn addition to saving useless training time, it prevents the model from overfitting to the training data, which would result in worst validation performance.","7dc32af1":"### 6.3. Batch size vs learning rate benchmark\n\nA benchmark was run to find the best hyperparameter value for learning rate against a given batch size.\n\nSince this computation would require several hours to complete, results are precomputed and saved to pickle data file. At each step, if the precomputed file is found, it is used for displaying, otherwise it is computed and cached for the next run.\n\n![image.png](attachment:image.png)\n"}}