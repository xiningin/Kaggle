{"cell_type":{"b658673d":"code","5813e99e":"code","61e2eb17":"code","b8b121da":"code","b99b366b":"code","8566273d":"code","164d6b0c":"code","3e3f156d":"code","f3f3e6b7":"code","a56f343a":"code","d7042dac":"code","04f1c452":"code","08566f43":"code","0a91577c":"code","302a79e3":"code","6e7ea7ee":"code","08c606b5":"code","df046d05":"code","010ee826":"code","80280878":"code","d4a8046c":"code","7c65e7ea":"code","52c10e65":"code","4152338a":"code","1a510a15":"code","2eb13dd7":"code","a26ed37f":"code","3a038681":"code","9f80d7d2":"code","4aa997d6":"code","7f7240c8":"code","84aad068":"code","2a54e442":"code","e2ef04ac":"code","67fc1960":"code","ddb33816":"code","01be162b":"code","42a98370":"code","3bf25656":"code","8f43203d":"code","d80a336b":"code","102c341f":"code","b77f3ffc":"code","adfcee02":"code","c89d27fc":"code","5ad82081":"code","198f39ee":"code","b11a51cc":"code","ad743112":"code","5b50e239":"code","2e21e323":"code","6f3d6256":"code","25265644":"code","16f87541":"code","5f924086":"code","8462df52":"code","b89db5ab":"code","4fe0b9de":"code","3c197c4b":"code","f479ba3b":"code","355489dd":"code","bf56315d":"code","51a93a23":"code","d76a1054":"code","492957ce":"code","44fd5828":"code","04db3e6b":"code","4fadd5c3":"code","346cb4a2":"code","2c4d6fd7":"code","bdfde99b":"code","ff49f3a9":"code","7ad98e75":"code","f594a78c":"code","681d1ba2":"code","a7cb41ef":"code","23b96a3f":"code","e16de76a":"code","075a6045":"code","c31f2691":"code","1d5d64e4":"code","e4361b80":"code","8f93f669":"code","a21312be":"code","32ee901b":"code","78586a77":"code","b329d69b":"code","8f110fed":"code","bab8d652":"code","7cf3a92b":"code","08f4868b":"code","2003af06":"code","6303fc65":"markdown","b5b38526":"markdown","a3625c22":"markdown","0077ff95":"markdown","ff7853c4":"markdown","9198a2a0":"markdown","80259dbf":"markdown","6cd875c3":"markdown","cdd50ae8":"markdown","5bba2bf7":"markdown","60d86e1d":"markdown","bb46714c":"markdown","d245e998":"markdown","5254116c":"markdown","bb9ca7b2":"markdown","4ee8ff06":"markdown","51327dca":"markdown","1188dcc0":"markdown","2dc7c32c":"markdown","668ca0de":"markdown","5ad3a78d":"markdown","b267c803":"markdown","56853fa8":"markdown","1ddabde0":"markdown","3d6e4d98":"markdown","9da32334":"markdown","3e8c68c5":"markdown","84bcae89":"markdown","13434ffc":"markdown","8d15a029":"markdown","ee81e24c":"markdown","d809d772":"markdown","bfdc5e08":"markdown","98e174d2":"markdown","58e37965":"markdown","0ee3149c":"markdown","10b0fef8":"markdown","36c15388":"markdown","8f2013f9":"markdown","bd323bca":"markdown","cf012ac2":"markdown","74758a35":"markdown","71760490":"markdown","4269f047":"markdown","68eb719c":"markdown","cc0b663c":"markdown","54645ae4":"markdown","3345914f":"markdown","136f7ae1":"markdown","1161e2ec":"markdown","acba3137":"markdown","7645e1a8":"markdown","b27954d2":"markdown","1b6f73e5":"markdown","31874f74":"markdown","2cfca138":"markdown","18ea1623":"markdown","17261ba5":"markdown","efb9a093":"markdown","ae5089a2":"markdown","65b9d05a":"markdown"},"source":{"b658673d":"# Pacotes para instalar\n!pip install prince;","5813e99e":"# Geral\/Dados\nimport numpy as np\nimport pandas as pd\nfrom time import sleep\n\n# Graficos\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Valores faltantes\nimport missingno as msno\n\n# Testes estat\u00edsticos\nfrom scipy import stats\n\n# Pre-processamento\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\n\n# Imputa\u00e7\u00e3o\nfrom sklearn.experimental import enable_iterative_imputer \nfrom sklearn.impute import IterativeImputer as MICE\n\n# Sele\u00e7\u00e3o de Features\nfrom boruta import BorutaPy\n\n# Modelagem com AI\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as XGBoost\nfrom xgboost.sklearn import XGBClassifier\n\n# Analise de resultados\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n\n# Explicabilidade de Modelos\/Resultados\nimport shap\nimport prince\nimport eli5\nimport lime\nimport lime.lime_tabular\n\n# Para sumir mensagens chatas do pandas (mesmo eu usando .loc)\npd.set_option('mode.chained_assignment', None)\n%matplotlib inline","61e2eb17":"def construir_distribuicoes(data, variaveis, objetivo=None, usar_objetivo=False):\n    # Definindo figura para construir gr\u00e1ficos\n    fig, axes = plt.subplots(nrows=int(len(variaveis)\/2), ncols=2, figsize=(20,15))\n    \n    # Indexadores\n    r = 0      # Linha\n    c = 0      # Coluna\n\n    # Loop em todas as variaveis\n    for v in variaveis:\n        # Realizar gr\u00e1fico de distribuicao\n        if usar_objetivo is False:\n            sns.kdeplot(data[v], shade=True, color='#50c878', ax=axes[r][c])\n        else:\n            sns.kdeplot(data.loc[data[objetivo] == 0, v], shade=True, color='#386796', ax=axes[r][c], label='Negative')\n            sns.kdeplot(data.loc[data[objetivo] == 1, v], shade=True, color='#F06C61', ax=axes[r][c], label='Positive')\n            # Gerar legenda\n            axes[r][c].legend(title=objetivo)\n            pass\n        \n        # Aplicar configuracao do gr\u00e1fico - gerais\n        axes[r][c].set_title('Vari\u00e1vel - ' + v)   \n        axes[r][c].set_xlabel('')\n        \n        # Controle dos indexadores\n        c += 1\n        if c > 1:\n            c = 0\n            r += 1\n    \n    # Configuracao b\u00e1sica de gr\u00e1fico\n    plt.tight_layout()","b8b121da":"def verificar_zero(data, variaveis):\n    for v in variaveis:\n        print('Vari\u00e1vel',v,'possui',sum(data[v] == 0),'valores zerados.')    ","b99b366b":"def substituir_zero(data, variaveis):\n    for v in variaveis:\n        data[v].replace({0:np.nan}, inplace=True)\n    return data","8566273d":"def grafico_faltantes(data, variaveis, titulo):\n    msno.matrix(data[variaveis], sort='ascending')\n    plt.title(titulo, weight='bold', size=35)","164d6b0c":"def crosstab(data, col1, col2):\n    return round(100*pd.crosstab(data[col1], data[col2])\/len(data), 2)","3e3f156d":"def calcular_correlacao_numerica(data, variaveis, corr_tipo='spearman'):\n    return data[variaveis].corr(corr_tipo)","f3f3e6b7":"def fazer_mapa_calor_corr(corr_matrix, titulo):    \n    # Fazer mascara para evitar correla\u00e7\u00e3o espelho\n    mascara = np.triu(corr_matrix)\n    # Fazer grafico\n    plt.figure(figsize=(20, 10), dpi= 80, facecolor='w', edgecolor='k')\n    sns.heatmap(corr_matrix, annot = True, mask=mascara, fmt='.2f', vmin=-1, vmax=1)\n    plt.title(titulo, fontsize=25, fontweight='bold')\n    plt.tight_layout()    ","a56f343a":"def fazer_scatter_plot(data, col1, col2, color_by=None):\n    # Definir figura\n    plt.figure(figsize=(20,10))\n    # Escolher de acordo a vari\u00e1vel\n    if color_by is None:   \n        sns.scatterplot(data=data, x=col1, y=col2)\n    else:\n        sns.scatterplot(data=data, x=col1, y=col2, hue=color_by)","d7042dac":"def fazer_kruskal_wallis_test(variavel1, variavel2, alpha=0.05):\n    # Realizar teste\n    p_valor, kw_res_stats = stats.kruskal(variavel1, variavel2)\n    \n    # Verificar resultado\n    if p_valor > alpha:\n        print('Nenhuma diferen\u00e7a significativa entre as distribui\u00e7\u00f5es (falhou em rejeitar H0)')\n    else:\n        print('Distribui\u00e7\u00f5es distintas (rejeito H0)')","04f1c452":"# Fun\u00e7\u00e3o bem gen\u00e9rica para anotar valores em gr\u00e1ficos\ndef annotate_bars_value(eixo, coord_texto_xy, barra_eixo_y=True):\n    # Loop em cada barra que existe no gr\u00e1fico\n    for barra in eixo.patches:\n        if barra_eixo_y:\n            tamanho_barra = barra.get_width() # Pegar o tamanho da barra\n            texto_barra = str(round(tamanho_barra, 2))+'%'  # Texto que sera anotado referente a barra\n            coord_anotacao_xy = (barra.get_x() + tamanho_barra, barra.get_y()) # Coordenada de onde sera a anotacao no grafico\n            eixo.annotate(texto_barra, coord_anotacao_xy, xytext=coord_texto_xy, fontsize=12, color='black',\n                          textcoords='offset points', horizontalalignment='right')\n        else:\n            tamanho_barra = barra.get_height() # Pegar o tamanho da barra\n            texto_barra = str(round(tamanho_barra, 2))+'%' # Texto que sera anotado referente a barra\n            coord_anotacao_xy = (barra.get_x(), barra.get_y() + tamanho_barra) # Coordenada de onde sera a anotacao no grafico\n            eixo.annotate(texto_barra, coord_anotacao_xy, xytext=coord_texto_xy, fontsize=12,color='black',\n                          textcoords='offset points', horizontalalignment='right')","08566f43":"# Base de dados\ndf = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')","0a91577c":"# Definindo grupos de vari\u00e1veis (features) e objetivo\nfeatures = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\nobjetivo = 'Outcome'","302a79e3":"construir_distribuicoes(df, features)","6e7ea7ee":"verificar_zero(df, features)","08c606b5":"construir_distribuicoes(df, features, objetivo, True)","df046d05":"# Trocar zeros por valores faltantes\ndf = substituir_zero(df, features[1:])","010ee826":"for v in features:\n    # Mostrar nome da variavel\n    print('########################################## ',v)\n    # Realizar teste e verificar resultado\n    fazer_kruskal_wallis_test(df.loc[df[objetivo]==0, v],df.loc[df[objetivo]==1, v])","80280878":"grafico_faltantes(df, features, 'Vari\u00e1veis')","d4a8046c":"# Criar vari\u00e1veis bin\u00e1rias referente aos valores faltantes para colunas mais faltantes\ndf['NAN_Insulin'] = pd.isnull(df['Insulin']).replace({True:1, False:0})\ndf['NAN_SkinThickness'] = pd.isnull(df['SkinThickness']).replace({True:1, False:0})","7c65e7ea":"# Verificar crosstab\ncrosstab(df, 'NAN_Insulin', objetivo)","52c10e65":"# Verificar crosstab\ncrosstab(df, 'NAN_SkinThickness', objetivo)","4152338a":"# Verificar proporcao de variavel objetivo\ndf[objetivo].value_counts()","1a510a15":"# Verificar proporcao de variavel objetivo apenas com amostras completas\ndf.dropna()[objetivo].value_counts()","2eb13dd7":"# Criar dataframe de apenas amostras completas\ndf_full = df.dropna()","a26ed37f":"# Calcular correlacao\ncorr_matrix = calcular_correlacao_numerica(df_full, features, 'spearman')\n\n# Gerar mapa de calor\nfazer_mapa_calor_corr(corr_matrix, 'Correla\u00e7\u00e3o Spearman')","3a038681":"fazer_scatter_plot(df_full, 'Age', 'Pregnancies', objetivo)","9f80d7d2":"fazer_scatter_plot(df_full, 'Insulin', 'Glucose', objetivo)","4aa997d6":"# Age X Pregnancies\nfazer_scatter_plot(df_full, 'BMI', 'SkinThickness', objetivo)","7f7240c8":"# Definindo conjunto X e Y\nx = df.drop(axis=1, columns=[objetivo]).copy()\ny = df[objetivo].copy()","84aad068":"# Fazendo a divis\u00e3o estratificada 80\/20\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state=1206, stratify=y)","2a54e442":"# Corrigir indexes para X\nx_train.reset_index(inplace=True, drop=True)\nx_test.reset_index(inplace=True, drop=True)","e2ef04ac":"# Corrigir indexes para Y\ny_train.reset_index(inplace=True, drop=True)\ny_test.reset_index(inplace=True, drop=True)","67fc1960":"# Verificando conjunto de treino das features\nx_train.head(5)","ddb33816":"# Definindo subgrupos de features\nfeatures_int = ['Pregnancies', 'Age']\nfeatures_flo = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction']\nfeatures_bin = ['NAN_Insulin', 'NAN_SkinThickness']","01be162b":"# Criar objetos de transforma\u00e7\u00e3o\ntransf_mm = MinMaxScaler()\ntransf_ss = StandardScaler()","42a98370":"# Aplicar transforma\u00e7\u00e3o para inteiras\nx_train.loc[:,features_int] = transf_mm.fit_transform(x_train.loc[:,features_int]).round(3)\nx_test.loc[:,features_int] = transf_mm.transform(x_test.loc[:,features_int]).round(3)","3bf25656":"# Aplicar transforma\u00e7\u00e3o para floats\nx_train.loc[:,features_flo] = transf_mm.fit_transform(x_train.loc[:,features_flo]).round(3)\nx_test.loc[:,features_flo] = transf_mm.transform(x_test.loc[:,features_flo]).round(3)","8f43203d":"# Visualizando conjunto de treino\nx_train.head()","d80a336b":"# Visualizando o balanceamento da vari\u00e1vel objetivo para conjunto de treino\ny_train.value_counts()","102c341f":"# Definir objeto de imputa\u00e7\u00e3o\nimputer = MICE(max_iter=10000, verbose=1, random_state=1206)","b77f3ffc":"# Realizar imputa\u00e7\u00e3o no conjunto de treino\nx_train = pd.DataFrame(imputer.fit_transform(x_train.values), columns=x_train.columns)","adfcee02":"# Realizar imputa\u00e7\u00e3o no conjunto de teste\nx_test = pd.DataFrame(imputer.transform(x_test.values), columns=x_test.columns)","c89d27fc":"# Criar objeto de SMOTE\nsmt = SMOTE(k_neighbors=5, random_state=1206)","5ad82081":"# Realizar processo de oversampling\nx_train, y_train = smt.fit_resample(x_train, y_train)","198f39ee":"# Verificar balanceamento da classe objetivo\ny_train.value_counts()","b11a51cc":"# Definir um estimador para o m\u00e9todo\nrfe = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_features='log2', max_depth=5)\n\n# Definir parametros do m\u00e9todo boruta\nfeat_selector = BorutaPy(rfe, n_estimators='auto', verbose=0, random_state=1206, max_iter=100)","ad743112":"# Buscar boas preditoras\nfeat_selector.fit(x_train.values, y_train.values.ravel());","5b50e239":"# Verificar vari\u00e1veis selecionadas\nlist(x_train.columns[feat_selector.support_])","2e21e323":"# Verificar n\u00e3o vari\u00e1veis selecionadas\n[i for i in x_train.columns if i not in list(x_train.columns[feat_selector.support_])]","6f3d6256":"# Definir apenas as vari\u00e1veis selecionadas para conjunto de treino\nx_train = x_train[list(x_train.columns[feat_selector.support_])]","25265644":"# Definir apenas as vari\u00e1veis selecionadas para conjunto de teste\nx_test = x_test[list(x_test.columns[feat_selector.support_])]","16f87541":"def mostrarResultadosFinaisOtimizacao(modelo, hp, score, metrica_avaliada):\n    print('Model:', type(modelo).__name__)\n    print('Metric: ', metrica_avaliada, ' | Score (%): ', 100*round(score,5))\n    print('Best Parameters: ', hp)","5f924086":"def optimizacaoHP(modelo, params, metrica, cv_gerador, n_iter, features, objetivo):\n    # Definir Objeto de busca com Bayes Optmization\n    hp_search = RandomizedSearchCV(estimator=modelo,\n                                   param_distributions=params, \n                                   n_iter=n_iter,\n                                   cv=cv_gerador,\n                                   scoring=metrica,\n                                   verbose=0,\n                                   random_state=1206,\n                                   n_jobs=-1)\n    # Buscar\n    hp_search.fit(features, objetivo)\n    # Retornar resultado\n    return (hp_search.best_estimator_, hp_search.best_params_, hp_search.best_score_, hp_search.cv_results_)","8462df52":"def PipelineDeOtimizacaoHP(modelo,params,x_train,y_train,metrica,cv_gerador,n_iter=1000):\n    # 1 - Buscar\n    best_model,best_params,best_score,cv_results = optimizacaoHP(modelo,params,metrica,cv_gerador,n_iter,x_train,y_train)\n    # Aguardar ...\n    sleep(1.5)    \n    # 2 - Mostrar resultados finais\n    mostrarResultadosFinaisOtimizacao(best_model, best_params, best_score, metrica)\n    # 3 - Retornar resultados\n    returned_dict = {'best_model':best_model,'best_params':best_params,'best_score':best_score,\n                     'cv_res':pd.DataFrame(cv_results),'metric_used':metrica}\n    return returned_dict","b89db5ab":"# Objeto de valida\u00e7\u00e3o cruzada (manter divisao padr\u00e3o para todos os tipos de modelo)\nstrat_cv_folds = StratifiedKFold(n_splits=5, random_state=101, shuffle=True)\n# Metrica para otimizar\nmetrica = 'roc_auc'","4fe0b9de":"# Define estimador\nestimador = LogisticRegression(random_state=1206,penalty='elasticnet', solver='saga', max_iter=5000)\n\n# Define parametros de busca (grid)\nparams = {'C': [0.001, 0.01, 0.025, 0.1, 1, 5, 10],\n          'l1_ratio': [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n          'class_weight': ['balanced',None]}\n\n# Otimizar\nlre = PipelineDeOtimizacaoHP(estimador, params, x_train, y_train, metrica, strat_cv_folds, 100)","3c197c4b":"# Define estimador\nestimador = RandomForestClassifier(random_state=1206, criterion='gini')\n\n# Define parametros de busca (grid)\nparams = {'n_estimators': [100, 250, 500, 1000],\n          'max_depth': [2, 4, 8, 16, 32],\n          'min_samples_split': [2, 4, 8, 10, 16],\n          'min_samples_leaf': [1, 3, 4, 6, 8],\n          'class_weight': ['balanced','balanced_subsample',None],\n          'max_features': ['sqrt','log2']}\n# Otimizar\nrfc = PipelineDeOtimizacaoHP(estimador, params, x_train, y_train, metrica, strat_cv_folds, 100)","f479ba3b":"# Define estimador\nestimador = GradientBoostingClassifier(random_state=1206, loss='deviance', criterion='friedman_mse')\n\n# Define parametros de busca (grid)\nparams = {'n_estimators': [100, 250, 500, 1000],\n          'min_samples_leaf': [1, 3, 4, 6, 8],\n          'min_samples_split': [2, 4, 8, 10, 16],\n          'max_depth': [2, 4, 8, 16, 32],\n          'subsample': [0.75, 0.80, 0.85, 0.9, 0.95, 1.0],\n          'learning_rate': [0.0001, 0.001, 0.01, 0.05, 0.1, 0.5, 1, 5, 10],\n          'max_features': ['sqrt','log2']}\n\n# Otimizar\nbst = PipelineDeOtimizacaoHP(estimador, params, x_train, y_train, metrica, strat_cv_folds, 100)","355489dd":"# Fazer predicao\ny_pred = lre['best_model'].predict(x_test)\n\n# Mostrar Classification Report\nprint(classification_report(y_test, y_pred))\n\n# Mostrar Matriz de confus\u00e3o\npd.DataFrame(confusion_matrix(y_test, y_pred, labels=np.unique(y_test)), index=['Real - 0', 'Real - 1'], columns=['Predito - 0', 'Predito - 1'])","bf56315d":"# Fazer predicao\ny_pred = rfc['best_model'].predict(x_test)\n\n# Mostrar Classification Report\nprint(classification_report(y_test, y_pred))\n\n# Mostrar Matriz de confus\u00e3o\npd.DataFrame(confusion_matrix(y_test, y_pred, labels=np.unique(y_test)), index=['Real - 0', 'Real - 1'], columns=['Predito - 0', 'Predito - 1'])","51a93a23":"# Fazer predicao\ny_pred = bst['best_model'].predict(x_test)\n\n# Mostrar Classification Report\nprint(classification_report(y_test, y_pred))\n\n# Mostrar Matriz de confus\u00e3o\npd.DataFrame(confusion_matrix(y_test, y_pred, labels=np.unique(y_test)), index=['Real - 0', 'Real - 1'], columns=['Predito - 0', 'Predito - 1'])","d76a1054":"# melhor modelo selecionado\nmodel = rfc['best_model']","492957ce":"# C\u00e1lculo do SHAP - Definindo explainer com caracter\u00edsticas desejadas\nexplainer = shap.TreeExplainer(model=model)","44fd5828":"# C\u00e1lculo do SHAP\nshap_values_train = explainer.shap_values(x_train, y_train)","04db3e6b":"# Vamos calcular o resultado de probabilidade de predi\u00e7\u00e3o do conjunto de treino\ny_pred_train_proba = model.predict_proba(x_train)","4fadd5c3":"# Vamos agora selecionar um resultado que previu como positivo\nprint('Probabilidade do modelo prever negativo -',100*y_pred_train_proba[3][0].round(2),'%.')\nprint('Probabilidade do modelo prever positivo -',100*y_pred_train_proba[3][1].round(2),'%.')","346cb4a2":"# Valores de SHAP para essa amostra na classe positiva\nshap_values_train[1][3]","2c4d6fd7":"# Valores de SHAP para essa amostra na classe negativa\nshap_values_train[0][3]","bdfde99b":"# Somat\u00f3rio do valor SHAP - Classe positiva\nshap_values_train[1][3].sum().round(2)","ff49f3a9":"# Somat\u00f3rio do valor SHAP - Classe positiva\nshap_values_train[0][3].sum().round(2)","7ad98e75":"# Adquirir valor de base de classifica\u00e7\u00e3o do modelo\nexpected_value = explainer.expected_value\n\n# Apresentar em tela\nprint('Valor de base para classe negativa -',100*expected_value[0].round(2))\nprint('Valor de base para classe positiva -',100*expected_value[1].round(2))","f594a78c":"# Somat\u00f3rio dos valores SHAP para classe positiva\nprint('Somat\u00f3rio SHAP para classe negativa nesta amostra:',100*y_pred_train_proba[3][0].round(2)-100*expected_value[0].round(2))\nprint('Somat\u00f3rio SHAP para classe positiva nesta amostra:',100*y_pred_train_proba[3][1].round(2)-100*expected_value[1].round(2))","681d1ba2":"for col, vShap in zip(x_train.columns, shap_values_train[1][3]):\n    print('###################', col)\n    print('Valor SHAP associado:',100*vShap.round(2))","a7cb41ef":"for col, vShap in zip(x_train.columns, shap_values_train[1][3]):\n    print('###################', col)\n    print('Valor SHAP associado:',100*(100*vShap.round(2)\/50).round(2),'%')","23b96a3f":"# Gr\u00e1fico 1 - Contribu\u00e7\u00e3o das vari\u00e1veis\nshap.summary_plot(shap_values_train[1], x_train, plot_type=\"dot\", plot_size=(20,15));","e16de76a":"# Gr\u00e1fico 2 - Contribu\u00e7\u00e3o de Import\u00e2ncia das vari\u00e1veis\nshap.summary_plot(shap_values_train[1], x_train, plot_type=\"bar\", plot_size=(20,15));","075a6045":"# Gr\u00e1fico 3 - Impacto das vari\u00e1veis em uma predi\u00e7\u00e3o espec\u00edfica do modelo vers\u00e3o Waterfall Plot\nshap.plots._waterfall.waterfall_legacy(expected_value=expected_value[1], shap_values=shap_values_train[1][3].reshape(-1), feature_names=x_train.columns, show=True)","c31f2691":"# Gr\u00e1fico 4 - Impacto das vari\u00e1veis em uma predi\u00e7\u00e3o espec\u00edfica do modelo vers\u00e3o Line Plot\nshap.decision_plot(base_value=expected_value[1], shap_values=shap_values_train[1][3], features=x_train.iloc[3,:],highlight=0)","1d5d64e4":"# Gr\u00e1fico 2 - Gr\u00e1fico de Depend\u00eancia\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20,10))\nshap.dependence_plot('Glucose', shap_values_train[1], x_train.values, feature_names=x_train.columns, interaction_index='Insulin', ax=ax)","e4361b80":"# Gr\u00e1fico 2 - Gr\u00e1fico de Depend\u00eancia\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20,10))\nshap.dependence_plot('BMI', shap_values_train[1], x_train.values, feature_names=x_train.columns, interaction_index='Glucose', ax=ax)","8f93f669":"# Calcular import\u00e2ncia de permuta\u00e7\u00e3o para conjunto de teste\nmodel_pi = eli5.sklearn.PermutationImportance(model, scoring='roc_auc', random_state=1206).fit(x_test, y_test)\n\n# Apresentar resultados para o conjunto de teste\neli5.show_weights(model_pi, feature_names =x_test.columns.tolist())","a21312be":"# Definindo XAI do modelo com LIME\nexplainer_lime = lime.lime_tabular.LimeTabularExplainer(training_data=x_train.values, \n                                                        feature_names=list(x_train.columns),\n                                                        class_names=['Sem Diabetes', 'Com Diabetes'],\n                                                        discretize_continuous=False,\n                                                        random_state=1206)","32ee901b":"# Calculando LIME\nexplained_sample = explainer_lime.explain_instance(x_train.values[3], model.predict_proba)","78586a77":"# Mostrando o resultado final\nexplained_sample.show_in_notebook(show_table=True, show_all=False)","b329d69b":"# Amostra definida - 83% de certeza de ser positivo\nsample = x_train.iloc[3,:].copy()\n\n# 1 - Predi\u00e7\u00e3o original\nprint('Probabilidade original de apresentar diabetes - ', 100*round(model.predict_proba(np.array(sample).reshape(1, -1))[0, 1] ,2), '%.')\n\n# 2 - Aumentando o valor de glucose, qual ser\u00e1 a porcentagem?\nsample['Glucose'] = sample['Glucose'] + (sample['Glucose']\/2)\nprint('Probabilidade com aumento de glucose de apresentar diabetes - ', 100*round(model.predict_proba(np.array(sample).reshape(1, -1))[0, 1] ,2), '%.')\n\n# 3 - Diminuindo o valor de glucose drasticamente, qual ser\u00e1 a porcentagem?\nsample = x_train.iloc[3,:].copy()\nsample['Glucose'] = sample['Glucose'] - (sample['Glucose']\/2)\nprint('Probabilidade com diminui\u00e7\u00e3o de glucose de apresentar diabetes - ', 100*round(model.predict_proba(np.array(sample).reshape(1, -1))[0, 1] ,2), '%.')","8f110fed":"# Construir um objeto para realizar a PCA\npca_obj = prince.PCA(n_components=len(x_train), random_state=1206, n_iter=100, engine='sklearn')\n\n# Aplicar a PCA\npcs = pca_obj.fit_transform(x_train)","bab8d652":"# Construir varia\u00e7\u00e3o explicada aculumada\ninertial_explained = 100*pca_obj.explained_inertia_.round(4)\n\n# Definindo a lista acumulada\ninertial_expl_acc = [inertial_explained[0]]\n\n# Conseguindo os valores acumulados\nfor i in range(1, len(inertial_explained)):\n    inertial_expl_acc.append(inertial_expl_acc[i-1] + inertial_explained[i])","7cf3a92b":"# Definir tamanho da figura\nplt.figure(figsize=(20,10));\n# Realizar grafico\nax = sns.barplot(x=['PC '+ str(i) for i in range(0, len(inertial_explained))], y=inertial_expl_acc, color='blue');\n# Aplicar configuracoes esteticas no grafico\nplt.ylabel('');\nplt.yticks([]);\nax.set_title('Vari\u00e2ncia Explicada', fontsize=25, weight='bold');\nax.spines['top'].set_visible(False);\nax.spines['left'].set_visible(False);\nax.spines['right'].set_visible(False);\nax.spines['bottom'].set_visible(False);\nannotate_bars_value(ax, (85,10), barra_eixo_y=False);","08f4868b":"# Visualizando o espa\u00e7o transformado da PCA\npca_obj.plot_row_coordinates(x_train, figsize=(20, 15), x_component=0, y_component=1, color_labels=y_train);","2003af06":"# Calcular correlacao\npca_corr = pca_obj.column_correlations(x_train)\n\n# Correla\u00e7\u00e3o das vari\u00e1veis com os componentes principais\nplt.figure(figsize=(20, 10), dpi= 80, facecolor='w', edgecolor='k')\nsns.heatmap(pca_corr, annot = True, fmt='.2f', vmin=-1, vmax=1)\nplt.title('Correla\u00e7\u00e3o dos PCs com as vari\u00e1veis originais', fontsize=25, fontweight='bold')\nplt.tight_layout()","6303fc65":"Bom, at\u00e9 o momento para ser um vetor de n\u00fameros estranhos n\u00e3o \u00e9?\n\nPor\u00e9m, se voc\u00ea olhar com mais calma vai perceber que os valores s\u00e3o iguais em m\u00f3dulo! O que diferencia \u00e9 o sinal sendo negativo ou positivo.\n\nSe a gente realizar a soma desses vetores, o que iremos encontrar?","b5b38526":"### Boosting Gradient","a3625c22":"### Conclus\u00f5es\n\nNote que deixei as vari\u00e1veis referente as presen\u00e7as de valores faltantes. A ideia \u00e9 avaliar atrav\u00e9s de uma sele\u00e7\u00e3o de features se essas vari\u00e1veis podem ser boas preditoras.\n\n### (2) Transforma\u00e7\u00f5es\nPara vari\u00e1veis inteiras vamos utilizar `MinMax` e para *floats* vamos utilizar `StandardScaler`. Para bin\u00e1rias n\u00e3o ser\u00e1 feito nada.","0077ff95":"Note que eu defini um `TreeExplainer`. Isso ocorreu, pois o meu modelo \u00e9 um modelo baseado em \u00e1rvore, logo a biblioteca possui um *explainer* espec\u00edfico para. Al\u00e9m disso, at\u00e9 o momento o que fizemos foi:\n- Definir um *explainer* com os par\u00e2metros desejados (existem uma diversidade de par\u00e2metros para `TreeExplainer`, recomendo checar futuramente)\n- Calcular os valores SHAP para o conjuntos de treino\n\n## O que s\u00e3o os valores SHAP?\n\nCom o conjunto de valores SHAP j\u00e1 definidos para o nosso conjunto de treinamento, podemos avaliar como cada valor de cada vari\u00e1vel influenciou no resultado alcan\u00e7ado pelo modelo preditivo.\n\nEm nosso caso, estaremos avaliando os resultados dos modelos em termos de probabilidade, ou seja, a porcentagem X que o modelo apresentou para dizer se a classe correta \u00e9 0 ou 1.\n\nVale ressaltar que isso pode variar de modelo para modelo: Caso voc\u00ea use um modelo XGBoost, provavelmente seu resultado padr\u00e3o n\u00e3o ser\u00e1 em termos de probabilidade como \u00e9 o da random forest do pacote sklearn. Para tornar o valor em termos de probabilidade voc\u00ea pode defini-lo atrav\u00e9s do `TreeExplainer`.\n\nMas chega de papo e vamos l\u00e1: **Como posso interpretar os valores de SHAP**?","ff7853c4":"### Conclus\u00f5es\nAvaliando estes resultados no momento, percebemos que o modelo de **Random Forest** como o melhor preditor.","9198a2a0":"### Avalia\u00e7\u00e3o do resultado\nA esquerda voc\u00ea encontra as probabilidades de predi\u00e7\u00e3o e na extrema direita os valores da amostra para cada vari\u00e1vel. No meio est\u00e1 a magia do LIME: O impacto de cada vari\u00e1vel por ordem de grandeza no resultado final.\n\nPara ter mais informa\u00e7\u00f5es sobre o m\u00e9todo recomendo acessar fortemente esta publica\u00e7\u00e3o do [medium](https:\/\/towardsdatascience.com\/understanding-model-predictions-with-lime-a582fdff3a3b) ou acessar o reposit\u00f3rio [aqui](https:\/\/github.com\/marcotcr\/lime).\n\nComo este m\u00e9todo tem poucas op\u00e7\u00f5es em compara\u00e7\u00e3o ao SHAP, acredito que n\u00e3o vale a pena me estender neste momento.","80259dbf":"## An\u00e1lise de valores faltantes inicial\n\nPrimeiro vamos verificar os valores faltantes distribuidos pela tabela.","6cd875c3":"# Constru\u00e7\u00e3o de modelo\n## Importando os dados","cdd50ae8":"## An\u00e1lise explorat\u00f3ria das vari\u00e1veis I\n\n### (1) Distribui\u00e7\u00f5es\n\nInicialmente vamos visualizar a distribui\u00e7\u00e3o das vari\u00e1veis para identificar alguma anomalia nos dados.","5bba2bf7":"### Conclus\u00f5es\nVamos avaliar algumas correla\u00e7\u00f5es usando apenas o conjunto completo de vari\u00e1veis. Al\u00e9m disso, vamos manter as vari\u00e1veis referentes \u00e0 presen\u00e7a de valores faltantes para testar futuramente.\n\n## An\u00e1lise explorat\u00f3ria das vari\u00e1veis II\n\n### (1) Correla\u00e7\u00f5es de Spearman","60d86e1d":"# Solu\u00e7\u00e3o para predi\u00e7\u00e3o de diabetes\n![DIABETS](https:\/\/user-images.githubusercontent.com\/32513366\/113214255-98e2bc80-924f-11eb-9e89-01510207e43d.png)\n\nEste notebook busca apresentar uma solu\u00e7\u00e3o um pipeline padr\u00e3o para solu\u00e7\u00e3o de problemas de data science e uma explica\u00e7\u00e3o rebuscada a respeito de explica\u00e7\u00e3o de modelos.\n\nCaso este notebook te ajude, d\u00ea um **UPVOTE**!\n\n# M\u00f3dulos","bb46714c":"### (3) Teste de hip\u00f3teses\nA ideia desta etapa \u00e9 avaliar a seguinte hip\u00f3tese:\n\n**Existe diferen\u00e7a entre a distribui\u00e7\u00e3o dos valores de uma vari\u00e1vel X para casos positivos e negativos?**\n\nPara as vari\u00e1veis que confirmarem essa hip\u00f3tese, podemos esperar que a mesma seja uma boa preditora para o nosso problema.","d245e998":"Note que para ambas as classes, o valor de base \u00e9 igual afinal este \u00e9 o valor m\u00ednimo necess\u00e1rio para um modelo definir aquela classe como correta. Nenhum modelo de classifica\u00e7\u00e3o bin\u00e1ria escolheria (por padr\u00e3o) que a classe com 49% seria correta em compara\u00e7\u00e3o com a classe que possui 51%.\n\n**OBS**: Vale ressaltar que \u00e9 poss\u00edvel o cientista de dados mudar esses valores de base nos modelos para garantir que uma classe tenha mais for\u00e7a para ser prevista. Essa t\u00e1tica pode ser usada por exemplo em problemas de imbalanceamento onde voc\u00ea precisa dar algum tipo de vantagem para classe minorit\u00e1ria ser prevista com sucesso. Por\u00e9m, considerando o formato padr\u00e3o, o valor de base em termos probabil\u00edsticos \u00e9 definido como 100% dividido pela quantidade de categorias.\n\nFinalmente, o somat\u00f3rio dos valores SHAP para uma amostra \u00e9 definido como:\n![shapForm](https:\/\/user-images.githubusercontent.com\/32513366\/112762502-5793a800-8fd6-11eb-9bbb-dca6b3fb4896.png)\n\nOnde *i* \u00e9 referente a categoria que aqueles valores representam (em nosso caso, categoria 0 ou 1).\n\nVamos verificar isso em c\u00f3digo:","5254116c":"# Interpreta\u00e7\u00e3o de Modelo I - SHAP\n\n![shapLogo](https:\/\/user-images.githubusercontent.com\/32513366\/112763297-3e8cf600-8fda-11eb-998a-ec470e168b77.png)\n\n**SH**apley **A**dditive ex**P**lanations (SHAP) \u00e9 um m\u00e9todo relativamente recente (menos de 10 anos) que busca explicar as decis\u00f5es dos modelos de intelig\u00eancia artificial de uma forma mais direta e intuitiva, fugindo de solu\u00e7\u00f5es \"caixa preta\".\n\nSeu conceito \u00e9 baseado na teoria dos jogos com uma matem\u00e1tica bem robusta. Por\u00e9m, para utilizar esta metodologia no nosso dia a dia de cientista de dados, n\u00e3o \u00e9 necess\u00e1rio o seu completo entendimento. Para quem deseja aprender de forma mais profunda como \u00e9 constru\u00edda a teoria recomendo a leitura desta [publica\u00e7\u00e3o](https:\/\/towardsdatascience.com\/one-feature-attribution-method-to-supposedly-rule-them-all-shapley-values-f3e04534983d).\n\nPara este notebook, irei buscar demonstrar interpreta\u00e7\u00f5es mais pr\u00e1ticas sobre o SHAP assim como compreender seus resultados.\n\nPara constru\u00e7\u00e3o de toda esta an\u00e1lise, foi utilizada a biblioteca [shap](https:\/\/github.com\/slundberg\/shap), mantida inicialmente pelo autor do artigo que originou o m\u00e9todo e pela comunidade.\n\nPrimeiramente vamos calcular os valores SHAP:\n","bb9ca7b2":"### Avalia\u00e7\u00e3o de resultado\nNote que adicionando metade do valor da glucose, aumentamos a probabilidade para 93%, aumentando assim a confian\u00e7a do modelo naquela resposta. Por\u00e9m ao remover metade do valor da Glucose, o modelo perde toda confian\u00e7a na classe positiva e responde a classe negativa como correta com 61%.","4ee8ff06":"### (1) Constru\u00e7\u00e3o dos modelos","51327dca":"### Avalia\u00e7\u00e3o do gr\u00e1fico\nEste gr\u00e1fico antes de mais nada mostra a **vari\u00e2ncia explicada** do seu conjunto de treino pelos componentes principais gerados atrav\u00e9s da PCA. Atrav\u00e9s dele voc\u00ea consegue perceber que as primeiras componentes possuem uma explicabilidade maior em compara\u00e7\u00e3o aos \u00faltimos. Isso ocorre, pois na PCA existe um ranqueamento, logo o componente principal 0 possuir\u00e1 uma explicabilidade maior que o 1 e assim vai...\n\nPor meio desse gr\u00e1fico voc\u00ea consegue ter uma ideia da explicabilidade do seu conjunto inicial com N vari\u00e1veis para um conjunto constru\u00eddo a partir dela com uma quantidade menor.\n\nPara para construir nosso mapa, iremos trabalhar com os dois primeiros componentes principais que possuem junto uma explicabilidade de 52% da vari\u00e2ncia do conjunto original. Iremos utiliza-los como eixos de um mapa 2D","1188dcc0":"Ao realizar a PCA em conjunto de 8 vari\u00e1veis, geramos 8 componentes principais independentes construidos linearmente atrav\u00e9s do conjunto original.","2dc7c32c":"### Conclus\u00f5es\nNote que para ambas as categorias, os valores de zero est\u00e3o presentes. Situa\u00e7\u00f5es interessantes poderiam ter emergidos como os valores zerados est\u00e3o apenas para uma das categorias, o que indicaria uma boa correla\u00e7\u00e3o entre valores faltantes com nosso objeto de predi\u00e7\u00e3o.\n\nVamos ent\u00e3o solucionar essa quest\u00e3o para ent\u00e3o analisar outros fatores referentes a essas vari\u00e1veis.","668ca0de":"### Avalia\u00e7\u00e3o do gr\u00e1fico\nJ\u00e1 neste caso, vemos inicialmente a rela\u00e7\u00e3o entre os valores SHAP e a vari\u00e1vel **BMI** apresenta uma curvatura em S, diferente da rela\u00e7\u00e3o linear que achamos anteriormente. Al\u00e9m disso, valores maiores de **Glucose** se concentram mais na parte do superior do gr\u00e1fico com valores maiores de SHAP.\n\nVisualiza\u00e7\u00f5es como essa podem gerar outros resultados como: **A correla\u00e7\u00e3o dos valores SHAP VS os valores da vari\u00e1vel pode ser significante?** Assim encontrar\u00edamos por exemplo uma clara correla\u00e7\u00e3o simples entre o resultado do modelo e o valor da vari\u00e1vel.","5ad3a78d":"## Interpreta\u00e7\u00e3o de predi\u00e7\u00e3o da amostra\nAl\u00e9m de visualiza\u00e7\u00f5es gerais, o SHAP proporciona an\u00e1lises mais individuais por amostras. Gr\u00e1ficos como esses s\u00e3o interessantes para apresentar algum resultado em especial. \n\nPor exemplo digamos que esteja trabalhando em um problema de churn de clientes e voc\u00ea quer mostrar como seu modelo compreendeu a sa\u00edda do maior cliente de sua companhia. Atrav\u00e9s dos gr\u00e1ficos que ser\u00e3o apresentados aqui, voc\u00ea consegue mostrar em uma apresenta\u00e7\u00e3o de forma bem did\u00e1tica o que aconteceu atrav\u00e9s do Machine Learning e discutir sobre aquele case.\n\nO primeiro gr\u00e1fico a ser mostrar \u00e9 o Waterfall em rela\u00e7\u00e3o a categoria positiva:","b267c803":"### Random Forest","56853fa8":"### Boosting","1ddabde0":"# Interpreta\u00e7\u00e3o Gr\u00e1fica I - PCA\n\nDiferentes dos m\u00e9todos passados anteriormente, PCA \u00e9 um m\u00e9todo de redu\u00e7\u00e3o de dimensionalidade, ou seja, reduzir seu conjunto de vari\u00e1veis. N\u00e3o irei entrar em detalhes matem\u00e1ticos de como funciona por debaixo dos panos o algoritmo de PCA, para saber mais recomendo fortemente este v\u00eddeo do [StatsQuest](https:\/\/www.youtube.com\/watch?v=FgakZw6K1QQ) em ingl\u00eas ou este [v\u00eddeo por Leandro Souto](https:\/\/www.youtube.com\/watch?v=KqZAC4jyJKc) para entender mais. A PCA pode ser usada para lidar com conjuntos com muitas correla\u00e7\u00f5es entre suas vari\u00e1veis, visto que os componentes principais gerados pelo m\u00e9todo n\u00e3o apresentam essa caracter\u00edstica! Vale lembrar que v\u00e1rios modelos de ML sofrem de alguma forma com vari\u00e1veis correlacionadas.\n\nAqui irei apresentar uma utiliza\u00e7\u00e3o diferente da mais utilizada para este m\u00e9todo: **constru\u00e7\u00e3o de mapas explic\u00e1veis do seu conjunto de vari\u00e1veis em rela\u00e7\u00e3o a sua vari\u00e1vel objetivo**.\n\nVamos entender isso mais para frente, agora vamos preparar a PCA:","3d6e4d98":"# Interpreta\u00e7\u00e3o de Modelo III - Permutation Importance\nEste m\u00e9todo busca avaliar a import\u00e2ncia das vari\u00e1veis no resultado do modelo em rela\u00e7\u00e3o \u00e0 alguma m\u00e9trica. Ou seja, diferente dos m\u00e9todos anteriores onde avaliavamos a **resposta do modelo**, com este m\u00e9todo avaliamos o **desempenho do modelo**.\n\nSua ideia \u00e9 bem simples: entender a import\u00e2ncia daquela vari\u00e1vel em uma m\u00e9trica de desempenho quando a mesma n\u00e3o est\u00e1 presente. Por\u00e9m, nos modelos que n\u00e3o conhecemos isso n\u00e3o \u00e9 poss\u00edvel, afinal para fazer uma predi\u00e7\u00e3o como uma regress\u00e3o log\u00edstica ou XGBoost, vamos precisar de uma amostra completa sem valor faltante. Em vista disso, podemos fazer o seguinte:\n- Treinar o modelo sem a vari\u00e1vel X\n- Avaliar o resultado do modelo em rela\u00e7\u00e3o a m\u00e9trica\n- Calcular a import\u00e2ncia\n\nPor\u00e9m, isso n\u00e3o \u00e9 uma boa t\u00e9cnica: \u00e9 algo caro computacionalmente e nada garante que n\u00e3o estamos fazendo essa an\u00e1lise em rela\u00e7\u00e3o aquele o conjunto de teste e n\u00e3o generalizando.\n\nSem mais rodeios, a **Permutation Importance** \u00e9 feita inserindo ru\u00eddo em uma vari\u00e1vel X e calculando sua import\u00e2ncia. Para funcionar esse m\u00e9todo, voc\u00ea pode adicionar ru\u00eddos extra\u00eddos de uma distribui\u00e7\u00e3o equivalente ao da vari\u00e1vel OU embaralhar os valores daquela vari\u00e1vel. Por exemplo, para a amostra 5 vamos utilizar o valor da vari\u00e1vel X da amostra 10. Assim conseguimos verificar com qualidade a import\u00e2ncia daquela vari\u00e1vel. A imagem abaixo exemplifica o que acontece:\n![PI](https:\/\/user-images.githubusercontent.com\/32513366\/112769785-4e680280-8ff9-11eb-99fd-c79fcee7b61d.png)\n\nEm resumo voc\u00ea bagun\u00e7a os valores da vari\u00e1vel que voc\u00ea quer medir.","9da32334":"### Avalia\u00e7\u00e3o do gr\u00e1fico\nNeste gr\u00e1fico, voc\u00ea consegue ver que sua predi\u00e7\u00e3o parte debaixo para cima a partir do valor de base. Cada vari\u00e1vel contribui de forma positiva (modelo prever categoria positiva) e de forma negativa (modelo prever outra classe). Neste exemplo vemos por exemplo que a contribui\u00e7\u00e3o de **SkinThickness** \u00e9 anulada pela **Age**.\n\nAinda neste gr\u00e1fico, o eixo X representa os valores de SHAP e os valores das setas indicam as contribui\u00e7\u00f5es dessas vari\u00e1veis.","3e8c68c5":"### Conclus\u00f5es\nConseguimos ver que as vari\u00e1veis `Insulin` e `SkinThickness` possuem uma grande quantidade de valores faltantes. Ser\u00e1 que isso possui alguma correla\u00e7\u00e3o com a vari\u00e1vel objetivo?","84bcae89":"### Conclus\u00f5es\nPodemos verificar que nenhuma das colunas criadas a partir de valores faltantes passaram neste filtro. Nenhuma das vari\u00e1veis que perderam no teste foi eliminada aqui. Ao final, todas as vari\u00e1veis s\u00e3o eleg\u00edveis para constru\u00e7\u00e3o de modelo.\n\n## Constru\u00e7\u00e3o de modelos\nIremos utilizar *RandomSearch* para buscar os melhores hiper par\u00e2metros.\n\n### Fun\u00e7\u00f5es espec\u00edficas","13434ffc":"Aqui conseguimos verificar que **Insulin** e **SkinThickness** e **BMI** tiveram juntas uma influ\u00eancia de 62%. Podemos perceber tamb\u00e9m que a vari\u00e1vel Age consegue anular o impacto de SkinThickness ou Insulin neste amostra.","8d15a029":"### Regress\u00e3o Log\u00edstica","ee81e24c":"## Visualiza\u00e7\u00e3o geral\n\nAgora que vimos muitos n\u00fameros vamos para as visualiza\u00e7\u00f5es. Em minha percep\u00e7\u00e3o um dos motivos do SHAP ter sido t\u00e3o difundido \u00e9 a qualidade de suas visualiza\u00e7\u00f5es, que em minha opini\u00e3o, superam as do LIME (iremos v\u00ea-lo futuramente).\n\nVamos fazer uma avalia\u00e7\u00e3o geral do conjunto de treino em rela\u00e7\u00e3o a predi\u00e7\u00e3o do nosso modelo para compreender o que est\u00e1 acontecendo no meio de tantas \u00e1rvores:","d809d772":"### Avalia\u00e7\u00e3o do gr\u00e1fico\nEste gr\u00e1fico \u00e9 equivalente ao anterior. Como nossa categoria de refer\u00eancia \u00e9 a positiva, o resultado do modelo seguir para tons mais avermelhados (na direita) indica uma predi\u00e7\u00e3o para classe positiva e para a esquerda, uma predi\u00e7\u00e3o para a classe negativa.\n\nNeste gr\u00e1fico, os valores pr\u00f3ximos da seta indicam os valores das vari\u00e1veis (referente a amostra) e n\u00e3o aos valores SHAP.","bfdc5e08":"### Random Forest","98e174d2":"### Conclus\u00f5es\nBase de treinamento balanceada, vamos selecionar as vari\u00e1veis para construir o modelo.","58e37965":"Abaixo apresentamos o resultado encontrado pelo LIME para a amostra 3:","0ee3149c":"### Avalia\u00e7\u00e3o do gr\u00e1fico\nAntes de analisar este gr\u00e1fico, precisamos entender suas caracter\u00edsticas:\n- O eixo X s\u00e3o os valores da vari\u00e1vel **Glucose**\n- O eixo Y s\u00e3o os valores SHAP calculados para vari\u00e1vel **Glucose**\n- As cores dos pontos s\u00e3o os valores da segunda vari\u00e1vel: **Insulin**. Note novamente que da cada ponto representa uma amostra, totalizando assim 800 pontos no gr\u00e1fico.\n\nDe in\u00edcio conseguimos perceber uma certa tend\u00eancia linear: a medida que a vari\u00e1vel **Glucose** aumenta, os valores SHAP aumentam tamb\u00e9m. Cores mais avermelhadas (indicam maiores valores de **Insulin**), logo conseguimos perceber que rela\u00e7\u00f5es onde temos alto valor de **Glucose** e **Insulin** podem indicar uma maior probabilidade do modelo responder como categoria positiva. Este tipo de resultado \u00e9 bem interessante de encontrar. \n\nComo estamos avaliando um modelo com base m\u00e9dica, rela\u00e7\u00f5es como essas podem ser validadas na litetura m\u00e9dica e fortalecer ainda mais a confian\u00e7a no seu modelo.\n","10b0fef8":"# Interpreta\u00e7\u00e3o de Modelo II - *Partial Dependence Plots*\nApesar de estar contido no pacote do SHAP, esta t\u00e9cnica \u00e9 independente desta teoria. Por\u00e9m, sua visualiza\u00e7\u00e3o est\u00e1 bem estabelecida dentro deste pacote.\n\n**Partial Dependence Plots** (PDP) s\u00e3o uma forma de voc\u00ea avaliar o impacto da resposta do modelo a partir da varia\u00e7\u00e3o de duas vari\u00e1veis. Nesta modalidade, voc\u00ea deixa de avaliar apenas as vari\u00e1veis de forma separadas como foi visto anteriormente e consegue verificar rela\u00e7\u00f5es em pares.\n\nAn\u00e1lises como essas s\u00e3o interessantes quando voc\u00ea analisa problemas que possuem um dom\u00ednio t\u00e9cnico muito forte.\n\nPor exemplo, considere um modelo que no seu conjunto de vari\u00e1veis tenha o pre\u00e7o e a demanda de determinado produto, \u00e9 de se esperar que exista algum tipo de rela\u00e7\u00e3o entre essas vari\u00e1veis e esse tipo de rela\u00e7\u00e3o consegue ser avaliado por meio de PDPs.\n\nNo gr\u00e1fico abaixo, vamos avaliar a rela\u00e7\u00e3o entre as duas vari\u00e1veis mais importantes do modelo usando com refer\u00eancia a categoria positiva:","36c15388":"Aqui avaliamos para a amostra 3 os valores SHAP referentes a classe positiva. **Valores SHAP positivos como da Glucose, BloodPressure, SkinThickness, BMI e DiabetesPedigreeFunction influenciaram o modelo \u00e0 prever a classe positiva como correta.** Ou seja, valores positivos implicam uma tend\u00eancia para a categoria de refer\u00eancia.\n\nJ\u00e1 os valores negativos como Age e Pregnancies, eles buscam indicar que a classe verdadeira \u00e9 a negativa (a oposta). Neste exemplo, se ambas fossem tamb\u00e9m positivas o nosso modelo resultaria em uma predi\u00e7\u00e3o de 100% para a classe positiva, por\u00e9m como isso n\u00e3o aconteceu eles representam o 17% que \u00e9 contra a escolha da classe positiva.\n\nEm resumo, voc\u00ea pode pensar no SHAP em contribui\u00e7\u00f5es para o modelo definir entre uma das classes. Assim:\n- Neste caso, o somat\u00f3rio dos valores SHAP n\u00e3o podem ultrapassar de 50%\n- Valores positivos considerando uma classe de refer\u00eancia indicam ser favor\u00e1veis aquela classe na predi\u00e7\u00e3o.\n- Valores negativos indicam que a classe correta n\u00e3o \u00e9 aquela de refer\u00eancia, mas sim outra classe.\n\nAl\u00e9m disso, podemos quantificar em termos de porcentagem a contribui\u00e7\u00e3o de cada vari\u00e1vel a resposta final daquele modelo divindindo pelo m\u00e1ximo de contribui\u00e7\u00e3o poss\u00edvel que neste caso \u00e9 50%:","8f2013f9":"### Conclus\u00f5es\nImputa\u00e7\u00e3o realizada com sucesso em um conjunto desbalanceado.","bd323bca":"## Refer\u00eancias de leitura\/estudo\n- [Canal Stats Quest - Muito conte\u00fado explicativo de DS](https:\/\/www.youtube.com\/watch?v=FgakZw6K1QQ)\n- [Documenta\u00e7\u00e3o SHAP - Muito explicativo e principalmente com novas ideias de gr\u00e1ficos para utilizar](https:\/\/shap.readthedocs.io\/en\/latest\/example_notebooks\/overviews\/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html#linear_regression)\n- [Documenta\u00e7\u00e3o Permutation Importance](https:\/\/eli5.readthedocs.io\/en\/latest\/blackbox\/permutation_importance.html)\n- [Tutorial Kaggle XAI](https:\/\/www.kaggle.com\/learn\/machine-learning-explainability)","cf012ac2":"# Fun\u00e7\u00f5es","74758a35":"### (3) Imputa\u00e7\u00e3o de Dados\n\nPara isso, vou desconsiderar toda a **an\u00e1lise necess\u00e1ria para realizar uma imputa\u00e7\u00e3o de qualidade**. Caso queira saber mais os requisitos para uma imputa\u00e7\u00e3o, acesse este [notebook](https:\/\/www.kaggle.com\/kaikewreis\/a-second-end-to-end-solution-for-covid-19#Impute-or-Not-Impute?-that-is-the-question) na se\u00e7\u00e3o de imputa\u00e7\u00e3o.","71760490":"Note que os valores SHAP bateram com o resultado apresentado anteriormente. **Mas e os valores SHAP individualmente, o que eles representam?**\n\nPara isso vamos usar mais c\u00f3digo, usando como refer\u00eancia classe positiva:","4269f047":"### Avalia\u00e7\u00e3o do resultado\nO resultado apresentado mostra duas colunas: **Weight** (Impacto da vari\u00e1vel no desempenho) e **Feature** (Nome da coluna). As vari\u00e1veis est\u00e3o ordenadas de acordo a import\u00e2ncia dela, logo as vari\u00e1veis do topo foram as mais impactantes no desempenho, e as debaixo as que menos importam.\n\nO primeiro n\u00famero em cada linha mostra o quanto o desempenho do modelo diminuiu com um embaralhamento aleat\u00f3rio. Como a maioria das coisas na ci\u00eancia de dados, h\u00e1 alguma aleatoriedade na mudan\u00e7a exata de desempenho de uma coluna embaralhada. Medimos a quantidade de aleatoriedade em nosso c\u00e1lculo de import\u00e2ncia de permuta\u00e7\u00e3o repetindo o processo com embaralhamento. O n\u00famero ap\u00f3s \u00b1 mede como o desempenho variou de uma remodela\u00e7\u00e3o para a pr\u00f3xima.\n\nOcasionalmente, voc\u00ea ver\u00e1 valores negativos para import\u00e2ncias de permuta\u00e7\u00e3o como no caso os valores de **SkinThickness**, **Pregnancies**, **BloodPressure**. Nesses casos, as previs\u00f5es sobre os dados embaralhados (ou ruidosos) eram mais precisas do que os dados reais. \n\nIsso acontece quando a vari\u00e1vel n\u00e3o importava (deveria ter uma import\u00e2ncia pr\u00f3xima a 0), mas a chance aleat\u00f3ria fazia com que as previs\u00f5es nos dados embaralhados fossem mais precisas.","68eb719c":"### Avalia\u00e7\u00e3o do resultado\nFinalmente, este gr\u00e1fico final mostra a correla\u00e7\u00e3o entre as vari\u00e1veis do conjunto original e os componentes principais. Por ser um m\u00e9todo linear, voc\u00ea consegue avaliar o peso de cada vari\u00e1vel em cada componente e assim conseguir explicar o resultado do mapa anterior. \n\nUm exemplo seria a vari\u00e1vel `DiabetesPedigreeFunction` que contribuiu fortemente para a constru\u00e7\u00e3o do PC3.\n\n# Conclus\u00e3o\nEste notebook tem o intuito de trazer mais conte\u00fado para a l\u00edngua portuguesa sobre a \u00e1rea de Ci\u00eancia de dados. \n\nCaso tenha ajudo d\u00e1 um UP Vote!\n\nRecomendo fortemente a leitura das refer\u00eancias...","cc0b663c":"### Conclus\u00f5es\nNeste caso podemos esperar que `BMI`, `Insulin`, `Glicose` e `SkinThickness` podem ser boas candidatas para predi\u00e7\u00e3o de vari\u00e1veis.","54645ae4":"Vamos visualizar os valores de SHAP para aquela amostra de acordo as classes poss\u00edveis (0 e 1):","3345914f":"### Conclus\u00f5es\nPodemos verificar inicialmente `Insulin`, `BMI` e `SkinThickness` possuem picos em zero. Vamos buscar as defini\u00e7\u00f5es te\u00f3ricas dessas vari\u00e1veis:\n- `Insulin`: \u00c9 um horm\u00f4nio secretado pelo p\u00e2ncreas que controla o n\u00edvel de glicose no sangue. A insulina funciona como uma chave para a glicose entrar nas c\u00e9lulas e ser utilizada como fonte de energia.\n- `BMI`: O \u00edndice de massa corporal \u00e9 uma medida internacional usada para calcular se uma pessoa est\u00e1 no peso ideal.\n- `SkinThickness`: Espessura da pele.\n\nConsiderando a defini\u00e7\u00e3o biol\u00f3gica, essas vari\u00e1veis n\u00e3o podem ter valor igual a zero, logo o mais prov\u00e1vel \u00e9 que o valor zero neste caso seja valor faltante.","136f7ae1":"### Conclus\u00f5es\nDe acordo a m\u00e9trica escolhida, o melhor modelo foi a **random forest**. Por\u00e9m vamos avaliar estes modelos no conjunto de teste.\n\n\n## Analise de resultados finais em conjunto de teste\n\n### Regress\u00e3o Log\u00edstica","1161e2ec":"### Conclus\u00f5es\n\nPercebemos que existe uma certa correla\u00e7\u00e3o: enquanto uma aumenta a outra aumenta tamb\u00e9m para todos os casos. Inclusive casos positivos de diabetes pegam valores maiores para as vari\u00e1veis dos dois \u00faltimos gr\u00e1ficos citados.\n\nPor\u00e9m, como n\u00e3o foi uma correla\u00e7\u00e3o forte, iremos mant\u00ea-las no conjunto de vari\u00e1veis.\n\n## Pr\u00e9-processamento\n\n### (1) Divis\u00e3o treino-teste","acba3137":"Com exce\u00e7\u00e3o de `Pregancies`, esses valores zerados devem ser tratados, pois n\u00e3o fazem sentido biol\u00f3gico serem zerados. Por\u00e9m antes, vamos verificar a mesma distribui\u00e7\u00e3o estratificada pela vari\u00e1vel objetivo.\n\n### (2) Distribui\u00e7\u00f5es estratificadas","7645e1a8":"# Extra - Situa\u00e7\u00f5es de \"What if Analysis\"\n\n**What If Analysis** s\u00e3o estudos mais aprofundados de situa\u00e7\u00f5es \"E se...\".\n\nSe mantermos todas as vari\u00e1veis com o mesmo valor e alterarmos uma delas, como o modelo ir\u00e1 responder aquele resultado?\n\nVamos usar a amostra 3 para verificar um cen\u00e1rio onde modificamos a vari\u00e1vel mais importante Glucose:","b27954d2":"# Interpreta\u00e7\u00e3o de Modelo IV - LIME\nO LIME funciona de forma equivalente ao SHAP, ou seja, possui a mesma fun\u00e7\u00e3o e por isso n\u00e3o irei explorar tanto.\n\nPara quem fez Engenharia de Controle e Automa\u00e7\u00e3o ou El\u00e9trica, vai entender como funciona a ideia por tr\u00e1s deste m\u00e9todo: Ele assume que a predi\u00e7\u00e3o em um n\u00edvel local consegue ser aproximada por um modelo linear (regress\u00e3o linear), assim com uma an\u00e1lise local (e n\u00e3o global como \u00e9 feito no SHAP) voc\u00ea consegue explicar qualquer modelo, sendo assim um m\u00e9todo agn\u00f3stico.\n\nEsse conceito \u00e9 bem parecido com a lineariza\u00e7\u00e3o usada em an\u00e1lise de sistemas.\n\nPara utilizar esse m\u00e9todo, usei a biblioteca `lime` constru\u00edda pelos autores do paper que o originou (inclusive, o autor principal \u00e9 brasileiro).\n\nNeste exemplo, definimos a inst\u00e2ncia do LIME e ent\u00e3o calculamos a explica\u00e7\u00e3o para uma amostra em espec\u00edfico:","1b6f73e5":"Como havia dito, os valores se diferenciam de acordo ao sinal. Podemos dizer que os valores SHAP para classe positiva s\u00e3o o espelho da classe negativa.\n\nSegura esse resultado na sua mem\u00f3ria DDR3 que vamos avaliar outro fator importante para nos ajudar a entender os valores SHAP: **O valor de base**.\n\nO valor de base \u00e9 extra\u00eddo do pr\u00f3prio explainer e de modo geral, consiste no limite de predi\u00e7\u00e3o do seu modelo para ele escolher uma classe como correta. \n\nA classifica\u00e7\u00e3o bin\u00e1ria (nosso caso) apresenta este valor de forma intuitiva: Caso nosso modelo tenha uma probabilidade maior que 50% para uma classe X, ent\u00e3o ele ir\u00e1 dizer que aquela amostra deve ser classificada como tal, pois ela tem sua maior confian\u00e7a naquela classe.\n\nCaso fosse uma situa\u00e7\u00e3o com tr\u00eas classes para prever, o valor de base seria 33%. Note que isso n\u00e3o significa que apenas uma confian\u00e7a acima de 33% iria garantir que modelo selecione aquela classe, mas o modelo precisa ter a maioria do **e** ser acima de 33%.\n\nVamos pegar os valores de base do nosso `Explainer`:","31874f74":"## Sele\u00e7\u00e3o de Features\n\nPara isso vamos usar o m\u00e9todo **Boruta**.","2cfca138":"### Conclus\u00f5es\nConsiderando um limite de 0.8 para definir uma correla\u00e7\u00e3o forte, vemos que neste caso isto n\u00e3o ocorre. Por\u00e9m, iremos avaliar alguns gr\u00e1ficos *scatter* para as duplas com correla\u00e7\u00f5es acima de 0.6 para entender melhor essas rela\u00e7\u00f5es.\n\n### (2) Gr\u00e1ficos de *Scatter*","18ea1623":"### (4) *Oversampling*\nNeste trabalho, vamos optar por realizar a aplica\u00e7\u00e3o de uma t\u00e9cnica para remover o desbalanceamento no conjunto de treinamento. Essa parte poderia ser ignorada (trata-se de uma decis\u00e3o do cientista de dados). Irei realiza-la, dado os bons resultados que j\u00e1 obtive com este m\u00e9todo e para evitar o treinamento com uma base desbalanceada.","17261ba5":"### Conclus\u00f5es\n\nPercebemos que aproximadamente metade dos valores faltantes para a vari\u00e1vel `SkinThickness` ocorrem para casos negativos de diabetes. Essa informa\u00e7\u00e3o pode ser interessante para constru\u00e7\u00e3o de modelos.","efb9a093":"### Avalia\u00e7\u00e3o do gr\u00e1fico\nO gr\u00e1fico apresentado \u00e9 um mapa gerado atrav\u00e9s de uma transforma\u00e7\u00e3o do nosso conjunto de treinamento segmentado por nossa vari\u00e1vel objetivo (aquela que usamos para classficar). \n\nSeu resultado n\u00e3o \u00e9 ideal, visto que existe uma grande intersec\u00e7\u00e3o entre ambos os dois conjuntos. O resultado ideal seria aquele onde ambos os grupos estivessem distantes. Resultados como o ideal podem identificar que de fato seu grupo de features \u00e9 capaz de identificar bem aquela sua vari\u00e1vel objetivo.\n\nAl\u00e9m disso, fica muito mais f\u00e1cil convencer em algum projeto de DS utilizando visualiza\u00e7\u00f5es como essa, pois o resultado estaria claro o que favoreceria sua narrativa.","ae5089a2":"### Avalia\u00e7\u00e3o do gr\u00e1fico\nAntes de avaliar o que este gr\u00e1fico quer nos dizer sobre o nosso problema, precisamos entender cada caracter\u00edstica nele presente:\n- O eixo Y s\u00e3o as vari\u00e1veis do nosso modelo em ordem de import\u00e2ncia (o SHAP ordena isso de forma padr\u00e3o, voc\u00ea pode escolher outra ordem atrav\u00e9s dos par\u00e2metros)\n- O eixo X s\u00e3o os valores SHAP. Como a nossa refer\u00eancia \u00e9 a categoria positiva ent\u00e3o valores positivos indicam um suporte para a categoria de refer\u00eancia (contribui pro modelo responder categoria positiva no final) e valores negativos indicam um suporte \u00e0 categoria oposta (neste caso de classifica\u00e7\u00e3o bin\u00e1ria, seria a classe negativa)\n- Cada ponto no gr\u00e1fico representa uma amostra. Cada vari\u00e1vel possui 800 pontos distribu\u00eddos horizontalmente (visto que temos 800 amostras, logo cada amostra tem um valor para aquela vari\u00e1vel). Note que essas nuvens de pontos em algum momento se expande verticalmente. Isso ocorre dado a densidade de valores daquela vari\u00e1vel em rela\u00e7\u00e3o aos valores SHAP.\n- Finalmente, as cores representam o aumento\/diminui\u00e7\u00e3o do valor da vari\u00e1vel, onde tons mais vermelhos s\u00e3o valores altos e tons azulados s\u00e3o valores mais baixos.\n\nDe forma geral, iremos buscar vari\u00e1veis que:\n- Tenham uma divis\u00e3o bem clara de cores, ou seja, vermelho e azul em lugares opostos. Essa informa\u00e7\u00e3o mostra que elas s\u00e3o boas preditoras, afinal apenas ao mudar seu valor o modelo consegue verificar de forma mais f\u00e1cil qual categoria \u00e9 a correta.\n- Associado a isso, quanto maior o intervalo de alcance de valores SHAP, melhor ser\u00e1 aquela vari\u00e1vel para o modelo. Vamos considerar **Glucose**, que apresenta em algumas situa\u00e7\u00f5es valores de SHAP em torno de 0,3, ou seja, 30% de contribui\u00e7\u00e3o para o resultado do modelo (isso porque o m\u00e1ximo que qualquer vari\u00e1vel pode atingir \u00e9 50%).\n\nA vari\u00e1vel **Glucose** e **Insulin** apresenta essas duas caracter\u00edsticas mencionadas. Agora note a vari\u00e1vel **BloodPressure**: No geral, ela \u00e9 uma vari\u00e1vel confusa visto que seus valores SHAP ficam em torno de 0 (contribui\u00e7\u00f5es fracas) e com uma clara mistura de cores, visto que voc\u00ea n\u00e3o consegue ver uma tend\u00eancia do aumento\/diminui\u00e7\u00e3o dessa vari\u00e1vel na resposta final.\n\nVale destacar tamb\u00e9m a vari\u00e1vel **Pregnancies** que n\u00e3o possui um intervalo t\u00e3o grande como **Glucose**, por\u00e9m demonstra uma divis\u00e3o clara de cores.\n\n**Por meio desse gr\u00e1fico, voc\u00ea consegue tirar um panorama geral de como seu modelo chega a suas conclus\u00f5es a partir do conjunto de treinamento**.\n\nO gr\u00e1fico seguinte mostra uma contribui\u00e7\u00e3o m\u00e9dia do gr\u00e1fico que apresentamos anteriormente:","65b9d05a":"### Avalia\u00e7\u00e3o do gr\u00e1fico\nBasicamente como o pr\u00f3prio t\u00edtulo do eixo X demonstra, cada barra representa a m\u00e9dia dos valores SHAP em m\u00f3dulo, assim avaliamos a contribui\u00e7\u00e3o m\u00e9dia das vari\u00e1veis nas respostas do modelo. Considerando a **Glucose**, vemos que sua contribui\u00e7\u00e3o m\u00e9dia gira em torno de 12% para a categoria positiva.\n\nEste gr\u00e1fico pode ser feito em rela\u00e7\u00e3o a qualquer uma das categorias (optei pela positiva) ou em rela\u00e7\u00e3o \u00e0 ambas. Ela configura como um \u00f3timo gr\u00e1fico para substituir o primeiro em explica\u00e7\u00f5es para gestores ou pessoas mais ligadas na \u00e1rea de neg\u00f3cio dado sua simplicidade."}}