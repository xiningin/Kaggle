{"cell_type":{"fdbcdb35":"code","bab3fc2b":"code","8aacc95d":"code","8716382c":"code","61e57601":"code","580c85c5":"code","e2157be0":"code","947e972f":"code","c912b1ae":"code","d377442a":"code","884e36db":"code","34210dd2":"code","f52a0076":"code","e0d84007":"code","d2aa4c85":"code","1c41cb89":"code","6322645d":"code","8d8a1685":"code","9e9ecd44":"code","eb27bf89":"code","a3ac13fd":"code","a0d6de11":"code","b126a939":"code","ce7785df":"code","fc1cf502":"code","177cdb98":"code","174aedb6":"code","326d71ca":"code","e39f7a99":"code","351d8ec1":"code","89a4e11e":"code","31d29873":"code","051bd538":"code","a9f8a4f2":"code","e75953ee":"code","ca4b09a0":"code","357ab114":"code","3b67e85a":"code","173429f9":"code","6d360685":"code","adcd97ca":"code","84f99d21":"code","bec68db0":"code","c254e9db":"code","9bfc7e25":"code","99d2a32d":"code","bdb925ac":"code","5fd40232":"code","7d33e673":"code","7f0f9b33":"code","c80784f1":"code","d3cc287f":"code","6e49f43f":"code","3155039b":"code","8a02588f":"code","520f950e":"code","8eaa23de":"code","7ddb5b69":"code","04c71eae":"code","47d6120b":"code","eca8df8b":"code","d9b31c84":"code","2a6ddd1d":"markdown","16bebb1f":"markdown","bd8b2db9":"markdown","0fd0ec8e":"markdown","b181b337":"markdown","b896fed1":"markdown","b4a55a45":"markdown","9853d19f":"markdown","ccc52e35":"markdown","529bb296":"markdown","2eb9f9ce":"markdown","edafda1c":"markdown","4a12f0b0":"markdown","e5028212":"markdown","f5c7878c":"markdown","2caa1f61":"markdown","0a934ed4":"markdown","0e7fe982":"markdown","79a69ece":"markdown","496dade8":"markdown","2512758d":"markdown","079e790c":"markdown","84c96b4c":"markdown","26abfd2e":"markdown","b33c89e8":"markdown","7c0d3855":"markdown","3b372452":"markdown","3d537b96":"markdown","e4b71e77":"markdown","8ee4d12a":"markdown","f73b98af":"markdown","682bab2f":"markdown","6e91dd4b":"markdown","47cb23ab":"markdown","ba22aa11":"markdown","c924f87c":"markdown","bb4b6a76":"markdown","cf981bfa":"markdown","f21e2106":"markdown","ecd7bae6":"markdown","2a0fe117":"markdown","231d77e3":"markdown","0e05e88d":"markdown","583ae5f7":"markdown","5c8c4d10":"markdown","2f2c4132":"markdown","22f0d10a":"markdown","39e48e12":"markdown","35c10f2c":"markdown","b1a9c73d":"markdown","cdb6e6c9":"markdown","67be2953":"markdown","5457820a":"markdown","6c40ad8c":"markdown","9932ac8e":"markdown","a17744f2":"markdown","e62ca830":"markdown"},"source":{"fdbcdb35":"import pandas as pd\nfrom plotly.offline import init_notebook_mode, iplot_mpl, download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly_express as px\nimport plotly.figure_factory as ff\nfrom sklearn.linear_model import LinearRegression as lm\nfrom sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport statsmodels.formula.api as sm\nfrom statsmodels.compat import lzip\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import r2_score\nimport statsmodels.api as sm\nfrom plotly.subplots import make_subplots\nimport warnings\nwarnings.filterwarnings('ignore')\nimport plotly.figure_factory as ff\n# Import plotting modules\nimport matplotlib.pyplot as plt\ninit_notebook_mode(connected=True)\nimport pandas_profiling","bab3fc2b":"df = pd.read_csv('..\/input\/insurance.csv')","8aacc95d":"pandas_profiling.ProfileReport(df)","8716382c":"df.head()","61e57601":"df.describe()","580c85c5":"import plotly.graph_objects as go\nlabels1 = ['Male', 'Female']\nlabels2= ['Smoker','Non Smoker']\nlabels3= ['southeast','northwest','southwest','northeast']\nlabels4= ['0 child','1  child','2 children','3 children','4 children','5 children']\n\n# Create subplots, using 'domain' type for pie charts\nspecs = [[{'type':'domain'}, {'type':'domain'}], [{'type':'domain'}, {'type':'domain'}]]\nfig = make_subplots(rows=2, cols=2, specs=specs)\n\n# Define pie charts\nfig.add_trace(go.Pie(labels=labels1, values=[676,662], name='Sex'), 1, 1)\nfig.add_trace(go.Pie(labels=labels2, values=[274,1064], name='Smoker'), 1, 2)\nfig.add_trace(go.Pie(labels=labels3, values=[364,325,325,324], name='Region'), 2, 1)\nfig.add_trace(go.Pie(labels=labels4, values=[574,324,240,157,25,18], name='Children'), 2, 2)\n\n# Tune layout and hover info\nfig.update_traces(hoverinfo='label+percent+name', textinfo='value', textfont_size=20,\n                  marker=dict(line=dict(color='#000000', width=2)))\n\nfig.update(layout_title_text='Categorical variables counts 1338 n-samples distribution')\nfig = go.Figure(fig)\nfig.show()","e2157be0":"df1=df.copy()\nle = LabelEncoder()\nle.fit(df1.sex.drop_duplicates()) \ndf1.sex = le.transform(df1.sex)\n# smoker or not\nle.fit(df1.smoker.drop_duplicates()) \ndf1.smoker = le.transform(df1.smoker)\n#region\nle.fit(df1.region.drop_duplicates()) \ndf1.region = le.transform(df1.region)\n\n\ncorr = df1.corr()\n#Plot figsize\nfig, ax = plt.subplots(figsize=(12, 10))\n#Generate Heat Map, allow annotations and place floats in map\nsns.heatmap(corr, linewidths=.5, cmap='Reds', annot=True, fmt=\".2f\")\n#Apply xticks\n#show plot\nplt.show()\n","947e972f":"px.histogram(df, x=\"charges\",nbins=int(np.sqrt(len(df.charges))),\n             marginal=\"box\",title='Histogram & Box-Plot: Charges',\n             template='ggplot2').update_traces(dict(marker_line_width=1, marker_line_color=\"black\"))","c912b1ae":"px.histogram(df, x=\"bmi\",nbins=int(np.sqrt(len(df.charges))),\n             marginal=\"box\",title='Histogram & Box-Plot: BMI',\n             template='ggplot2').update_traces(dict(marker_line_width=1, marker_line_color=\"black\", \n                                                    marker_color='darkcyan'))","d377442a":"px.histogram(df, x=\"age\",nbins=int(np.sqrt(len(df.charges))),\n             marginal=\"box\",title='Histogram & Box-Plot: Age',\n             template='ggplot2').update_traces(dict(marker_line_width=1,marker_line_color=\"black\", \n                                                    marker_color='violet'))","884e36db":"fig, ax = plt.subplots(figsize=(15,5))\nfig = sm.graphics.interaction_plot(x=df.sex, response=df.charges, trace=df.smoker,\n                                   ax=ax, plottype='both', colors=['red','blue'],markers=['D','^'], ms=8)","34210dd2":"px.histogram(df, x=\"charges\",nbins=int(np.sqrt(len(df.charges))),color='smoker',\n             template='ggplot2',facet_col='smoker', \n             facet_row='sex').update_traces(dict(marker_line_width=1,marker_line_color=\"black\"))","f52a0076":"fig, ax = plt.subplots(figsize=(15,5))\nfig = sm.graphics.interaction_plot(x=df.age, response=df.charges, trace=df.smoker,\n                                   ax=ax, plottype='both', colors=['blue','red'],markers=['D','^'], ms=8)","e0d84007":"fig, ax = plt.subplots(figsize=(15,5))\nfig = sm.graphics.interaction_plot(x=df.bmi, response=df.charges, trace=df.smoker,\n                                   ax=ax, plottype='both', colors=['blue','red'],markers=['D','^'], ms=8)","d2aa4c85":"px.scatter(df, x=\"bmi\", y=\"charges\", color=\"smoker\",trendline='ols',\n           template='ggplot2',facet_col='sex').update_traces(dict(marker_line_width=1, marker_line_color=\"black\"))","1c41cb89":"fig, ax = plt.subplots(figsize=(15,5))\nfig = sm.graphics.interaction_plot(x=df.children, response=df.charges, trace=df.smoker,\n                                   ax=ax, plottype='both', colors=['blue','red'],markers=['D','^'], ms=8)","6322645d":"fig, ax = plt.subplots(figsize=(15,5))\nfig = sm.graphics.interaction_plot(x=df.children, response=df.charges, trace=df.sex,\n                                   ax=ax, plottype='both', colors=['blue','red'],markers=['D','^'], ms=8)","8d8a1685":"fig, ax = plt.subplots(figsize=(15,5))\nfig = sm.graphics.interaction_plot(x=df.region, response=df.charges, trace=df.sex,\n                                   ax=ax, plottype='both', colors=['blue','red'],markers=['D','^'], ms=8)","9e9ecd44":"fig, ax = plt.subplots(figsize=(15,5))\nfig = sm.graphics.interaction_plot(x=df.region, response=df.charges, trace=df.smoker,\n                                   ax=ax, plottype='both', colors=['blue','red'],markers=['D','^'], ms=8)","eb27bf89":"df2=df.copy()\nX=df2.iloc[:,0:5]\ny=df2.iloc[:,6]\nX=pd.get_dummies(X)","a3ac13fd":"X.head()","a0d6de11":"X['bmi_bins_smoker_yes'] = pd.cut(x=X['bmi']*X.smoker_yes, bins=X['bmi'].quantile([0, .125, .250, .375, .5, .625, .75, .875, 1.]))","b126a939":"X['age_bins_smoker_no'] = pd.cut(x=X['age']*X.smoker_no, bins=X['age'].quantile([0, .25, .5, .75, 1.]))","ce7785df":"X['+4children_smoker_no'] = pd.cut(x=X['children']*X.smoker_no, bins=[0,3,6])","fc1cf502":"X=pd.get_dummies(X)","177cdb98":"X['age*bmi']=X.age*X.bmi\nX['age*smoker_yes']=X.age*X.smoker_no\nX['bmi*smoker_no']=X.bmi*X.smoker_yes\nX['bmi**2']=X.bmi**2\nX['log_age']=np.log(X.age)\nX['log_bmi']=np.log(X.bmi)","174aedb6":"X.head()","326d71ca":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold","e39f7a99":"#cross-validation function for N-K(2,110,10) fold cross-validation\ndef cross_val(model,features,predictor):\n    RMSE=[]\n    model = lm()\n    for KFold in range (2,110,10):\n        scores = cross_val_score(model, X=features, y=predictor, cv=KFold, scoring='neg_mean_squared_error')\n        scores=-scores\n        rsme_score=np.sqrt(scores).mean()\n        RMSE.append(rsme_score)\n        return np.mean(RMSE)","351d8ec1":"RMSE=[]","89a4e11e":"oX=X.iloc[:,0:6]\nRMSE.append(cross_val(lm,oX,y))","31d29873":"RMSE.append(cross_val(lm,X,y))","051bd538":"for i in range (2,6):\n    RMSE.append(cross_val(lm,PolynomialFeatures(i,interaction_only=True).fit_transform(oX),y))","a9f8a4f2":"for i in range (2,6):\n    RMSE.append(cross_val(lm,PolynomialFeatures(i,interaction_only=False).fit_transform(oX),y))","e75953ee":"fig = go.Figure(data=go.Scatter(x=['Original X','Feature engi X','Inter. d2 X','Inter. d3 X ',\n                           'Inter. d4 X','Inter. d5 X','PolynomialF.d2','PolynomialF.d3',\n                           'PolynomialF.d4','PolynomialF.d5'], y=RMSE))\n# Edit the layout\nfig.update_layout(title='Root Mean Square error (RMSE) with different Matrix features',\n                   xaxis_title='Feature Matrix',\n                   yaxis_title='RMSE')\nfig.show()","ca4b09a0":"model = lm().fit(X,y)\nresiduals=y-model.predict(X)\ndf['residuals']=residuals\ndf['prediction']=model.predict(X)\ndf['index']=df.index","357ab114":"# Fit and summarize OLS model\nmod = sm.OLS(y, X)\nresult = mod.fit()\nprint(result.summary())","3b67e85a":"prediction_summary=result.get_prediction(X).summary_frame()","173429f9":"\n# Residual Plots\ndef regression_residual_plots(model_fit, dependent_var, data, size = [10,10]):\n    \"\"\"\n    This function requires:\n        import matplotlib.pyplot as plt\n        import statsmodels.api as sm\n    \n    Arguments:\n    model_fit: It takes a fitted model as input.\n        Obtainable through Statsmodels regression: \n            model_fit = sm.OLS(endog= DEPENDENT VARIABLE, exog= INDEPENDENT VARIABLE).fit()\n    dependent_var: string of the pandas column used as the model dependent variable.\n    data: pandas dataset where the dependent variable is located. The model data.\n    size: default [10,10]. Updates the [width, height], inputed in matplotlibs figsize = [10,10]\n        \n    Ive only run it on simple, non-robust, ordinary least squares models,\n    but these metrics are standard for linear models.\n    \"\"\"\n    \n    # Extract relevant regression output for plotting\n    # fitted values (need a constant term for intercept)\n    model_fitted_y = model_fit.fittedvalues\n    # model residuals\n    model_residuals = model_fit.resid\n    # normalized residuals\n    model_norm_residuals = model_fit.get_influence().resid_studentized_internal\n    # absolute squared normalized residuals\n    model_norm_residuals_abs_sqrt = np.sqrt(np.abs(model_norm_residuals))\n    # absolute residuals\n    model_abs_resid = np.abs(model_residuals)\n    # leverage, from statsmodels internals\n    model_leverage = model_fit.get_influence().hat_matrix_diag\n    # cook's distance, from statsmodels internals\n    model_cooks = model_fit.get_influence().cooks_distance[0]\n\n    ########################################################################\n    # Plot Size\n    fig = plt.figure(figsize=size)\n    \n    # Residual vs. Fitted\n    ax = fig.add_subplot(2, 2, 1) # Top Left\n    sns.residplot(model_fitted_y, dependent_var, data=data, \n                              lowess=True, \n                              scatter_kws={'alpha': 0.5}, \n                              line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8},\n                 ax=ax)\n    ax.set_title('Residuals vs Fitted')\n    ax.set_xlabel('Fitted values')\n    ax.set_ylabel('Residuals')\n\n    # Annotations of Outliers\n    abs_resid = model_abs_resid.sort_values(ascending=False)\n    abs_resid_top_3 = abs_resid[:3]\n    for i in abs_resid_top_3.index:\n        ax.annotate(i, xy=(model_fitted_y[i], model_residuals[i]));\n\n    ########################################################################\n    # Normal Q-Q\n    ax = fig.add_subplot(2, 2, 2) # Top Right\n    QQ = sm.ProbPlot(model_norm_residuals)\n    QQ.qqplot(line='45', alpha=0.5, color='#4C72B0', lw=1, ax=ax)\n    ax.set_title('Normal Q-Q')\n    ax.set_xlabel('Theoretical Quantiles')\n    ax.set_ylabel('Standardized Residuals')\n\n    # Annotations of Outliers\n    abs_norm_resid = np.flip(np.argsort(np.abs(model_norm_residuals)), 0)\n    abs_norm_resid_top_3 = abs_norm_resid[:3]\n    for r, i in enumerate(abs_norm_resid_top_3):\n        ax.annotate(i, xy=(np.flip(QQ.theoretical_quantiles, 0)[r],\n                                model_norm_residuals[i]));\n\n    ########################################################################\n    # Scale-Location Plot\n    ax = fig.add_subplot(2, 2, 3) # Bottom Left\n    plt.scatter(model_fitted_y, model_norm_residuals_abs_sqrt, alpha=0.5)\n    sns.regplot(model_fitted_y, model_norm_residuals_abs_sqrt, \n                scatter=False, \n                ci=False, \n                lowess=True,\n                line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8}, ax=ax)\n    ax.set_title('Scale-Location')\n    ax.set_xlabel('Fitted values')\n    ax.set_ylabel('$\\sqrt{|Standardized Residuals|}$');\n    # Annotations of Outliers\n    abs_sq_norm_resid = np.flip(np.argsort(model_norm_residuals_abs_sqrt), 0)\n    abs_sq_norm_resid_top_3 = abs_sq_norm_resid[:3]\n    for i in abs_norm_resid_top_3:\n        ax.annotate(i, \n                                   xy=(model_fitted_y[i], \n                                       model_norm_residuals_abs_sqrt[i]));\n\n    ########################################################################  \n    # Cook's Distance Plot\n    ax = fig.add_subplot(2, 2, 4) # Bottom Right\n    plt.scatter(model_leverage, model_norm_residuals, alpha=0.5)\n    sns.regplot(model_leverage, model_norm_residuals, \n                scatter=False, \n                ci=False, \n                lowess=True,\n                line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8},\n               ax=ax)\n    ax.set_xlim(0, 0.20)\n    ax.set_ylim(-3, 5)\n    ax.set_title('Residuals vs Leverage')\n    ax.set_xlabel('Leverage')\n    ax.set_ylabel('Standardized Residuals')\n\n    # Annotations\n    leverage_top_3 = np.flip(np.argsort(model_cooks), 0)[:3]\n    for i in leverage_top_3:\n        ax.annotate(i, xy=(model_leverage[i],model_norm_residuals[i]))\n\n    # Shenanigans for Cook's distance contours\n    def graph(formula, x_range, label=None):\n        x = x_range\n        y = formula(x)\n        plt.plot(x, y, label=label, lw=1, ls='--', color='red')\n    p = len(model_fit.params) # number of model parameters\n    graph(lambda x: np.sqrt((0.5 * p * (1 - x)) \/ x), \n          np.linspace(0.001, 0.200, 50), \n          'Cook\\'s distance') # 0.5 line\n    graph(lambda x: np.sqrt((1 * p * (1 - x)) \/ x), \n          np.linspace(0.001, 0.200, 50)) # 1 line\n    plt.legend(loc='upper right')\n    plt.savefig('residual_plots.png',bbox_inches='tight')\n    plt.show()\n\nprint(\"Residual Plots Function Ready\")","6d360685":"regression_residual_plots(result, dependent_var=\"charges\",data=df)","adcd97ca":"px.scatter(df, x='index', y='residuals',title=\"Model Residuals\",trendline='lowess').update_traces(dict(marker_line_width=1,\n                                                                                    marker_line_color=\"pink\"))","84f99d21":"px.histogram(df, x=\"residuals\", marginal=\"box\",template='ggplot2',nbins=int(np.sqrt(len(df.residuals))),\n             title=\"Model Residuals histogram and box-plot\").update_traces(dict(marker_line_width=1, marker_line_color=\"black\"))","bec68db0":"px.scatter(df, x='index', y='residuals',color='smoker',\n           title=\"Model Residuals with Smoker yes\/no\").update_traces(dict(marker_line_width=1,marker_line_color=\"black\"))","c254e9db":"px.scatter(df, x='prediction',y='charges',trendline='ols',\n           title=\"Predicted values vs Actual Values \").update_traces(dict(marker_line_width=1,marker_line_color=\"black\"))","9bfc7e25":"df['squared_pearson_residuals']=result.resid_pearson**2\npx.scatter(df, x='prediction',y='charges',trendline='lowess',\n           title=\"Predicted values vs Actual Values - Bubble Size = Squared person residuals\",\n           color='smoker',size='squared_pearson_residuals').update_traces(dict(marker_line_width=1,marker_line_color=\"black\"))","99d2a32d":"influence= result.get_influence().summary_frame()\ndf['cook_d']=influence.cooks_d","bdb925ac":"influence = result.get_influence()\n#c is the distance and p is p-value\n(c, p) = influence.cooks_distance\nfig, ax = plt.subplots(figsize=(15,5))\nfig=plt.stem(np.arange(len(c)), c, markerfmt=\",\")","5fd40232":"df['cook_d']=df['cook_d'].fillna(0)","7d33e673":"px.scatter(df, x='prediction',y='charges',trendline='ols',\n           title=\"Predicted values vs Actual Values bubble size = cook's distance\",\n           size='cook_d',color='smoker').update_traces(dict(marker_line_width=1,marker_line_color=\"black\"))","7f0f9b33":"df_o=df[df['cook_d'] < 0.005]","c80784f1":"len(df)-len(df_o)","d3cc287f":"cook_d=df[df['cook_d'] > 0.005]\npx.scatter(cook_d, x='prediction',y='charges',trendline='lowess',\n           title=\"Predicted values vs Actual Values PRUNED OBSERVATIONS bubble size = cook's distance\",\n           size='cook_d',color='smoker').update_traces(dict(marker_line_width=1,marker_line_color=\"black\"))","6e49f43f":"X=df_o.iloc[:,0:6]\ny=df_o.iloc[:,6]\nX=pd.get_dummies(X)","3155039b":"X['bmi_bins_smoker_yes'] = pd.cut(x=X['bmi']*X.smoker_yes, bins=[0,20,30,40,50,60])","8a02588f":"X['age_bins_smoker_no'] = pd.cut(x=X['age']*X.smoker_no, bins=[0,42,70])","520f950e":"X['+4children_smoker_no'] = pd.cut(x=X['children']*X.smoker_no, bins=[0,3,6])","8eaa23de":"X=pd.get_dummies(X)","7ddb5b69":"# Fit and summarize OLS model\nmod = sm.OLS(y, X)\nresult = mod.fit()\nprint(result.summary())","04c71eae":"df_o['residuals']=y-result.predict(X)\ndf_o['prediction']=result.predict(X)\ndf_o['squared_residuals']=df_o['residuals']**2","47d6120b":"(cross_val(lm,X,y))","eca8df8b":"px.scatter(df_o, x='prediction',y='charges',trendline='ols',\n           title=\"Predicted values vs Actual Values model without obvervations with Cook's d. > 0.005 - bubble size = squared residuals\",size='squared_residuals').update_traces(dict(marker_line_width=1,marker_line_color=\"black\",marker_color='violet'))","d9b31c84":"px.scatter(df_o, x='prediction',y='charges',trendline='ols',color='smoker',\n           title=\"Predicted values vs Actual Values model without obvervations with Cook's d. > 0.005 - bubble size = squared residuals\",\n           size='squared_residuals').update_traces(dict(marker_line_width=1,marker_line_color=\"black\"))","2a6ddd1d":"### 1.1 DataSet Descriptive statistics","16bebb1f":"**Cook's distance of our observations**","bd8b2db9":"Our model seem to underpredict many outlier observations.","0fd0ec8e":"**Interaction: Sex-smoker**","b181b337":"**3.2 Interactions & higher order polynomials**\n\nInteractions only -> If true, only interaction features are produced: \nfeatures that are products of at most ``degree`` *distinct* input features (so not ``x[1] ** 2``, ``x[0] * x[2] ** 3``,etc.).\n\nInteractions only -> If False, generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form\n``d[a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].``","b896fed1":"**Residual Plots**","b4a55a45":"Source code for Residual Plots, Authored by Emre Can here:\n\nhttps:\/\/medium.com\/@emredjan\/emulating-r-regression-plots-in-python-43741952c034","9853d19f":"**Interactions, Polynomials and logarithms.**","ccc52e35":"Our feature Engineered feature Matrix reproduces a smaller RMSE than all the other cross-validated models. ","529bb296":"We proceed to build a simple feature Matrix :","2eb9f9ce":"### 3.4 Model Summary","edafda1c":"Thanks to the omission of the 49 observations with the highest Cook's distance, we have been able to increase the accuracy of oure cross-validated linear regression model to an R-squared of 0.905. and RMSE to 3233.18. This means that our algorithm can make reasonably good predictions. \n\nPruning data is that it's a complex process and should be executed with caution. One must be careful about being too aggressive while pruning outliers. ","4a12f0b0":"We engineer new dummy variables for the interaction of BMI and smoking. (5 bins)","e5028212":"### Conclusions","f5c7878c":"# 2. Feature Engineering\n\nFeature engineering is the process of transforming data into features to act as inputs for machine learning models such that good quality features help in improving the overall model performance. Using the insights derived in the previous chapter we can generate new features to reproduce the significant interactions detected and make our linear model more flexible and powerful. \n\nAs we saw from our EDA analysis, the interactions between our features seem to have a lot of explanatory to our target. We didn't explore the dependencies of our higher order polynomials, however with Polynomial Features will test the model accuracy up to the 5th Polynomial with cross-validation.","2caa1f61":"We engineer a new dummy variable for the interaction of having 4 children or more and not smoking.","0a934ed4":"### 1.3 Continuous variables distributions","0e7fe982":"### 3.1 Root Mean square error RMSE of original features, feature engineered matrix and interactions & higher order polynomials.","79a69ece":"**Interaction Plot : Bmi - Smoker**","496dade8":"- We see that smoking has a huge positive relationship with medical costs. \n- On the other hand the effect of sex seems almost non-existent, for both smokers and non smokers. ","2512758d":"**Interaction: Age - Smoker**","079e790c":"The DataSet has a very symmetric distribution regarding sex and region. However, smokers make up only 20.5% of the sample data.","84c96b4c":"We can generate a second linear regression model omiting the values with a cook's distance higher than a certain treshhold.","26abfd2e":"### 2.2 Our feature engineered Matrix X","b33c89e8":"Seems that most extreme residuals are of some non smokers with very high medical costs. Our dataset doesn't bring enough explanatory power to predict more accurately these cases. ","7c0d3855":"We engineer new dummy variables for the interaction of age and not smoking. (3 bins)","3b372452":"**Quantile-based Binning and Discretization**\n\nIn order to make linear models more powerful on continuous data we can use binning (also known as discretization) of the feature to split it up into multiple new features which compile significant interactions and increase the expressiveness of our feature matrix.\n\nThe problem of only using continuous features in linear regression is that often the distribution of values in these features is skewed. Binning, also known as quantization is used for transforming continuous numeric features into categories. Each bin represents a specific intensity and hence a specific range of continuous numeric values fall into it. \n\nWe can use quantile-based binning to avoind ending up with irregular bins which, not uniform based on the number of values which fall in each bin.","3d537b96":"- 18\/19 year old are the most represented age group with 137 observations.\n- Only 22 observations fall to the 64-65 age group. This would be a problem for generalization.","e4b71e77":"### 1.4 EDA conclusions\n\n- Smoking seems to be by far the most significant feature to determine the charges. There is a very small difference between sexes and smoking. \n\n- Age has a positive relationship with charges. The relationships of non smokers is more consistent than of non smokers.\n\n- There is a very strong positive relationship between bmi-smoker(yes) and higher medical charges. (more in the case of females than males).\n\n- For smokers, with bmi > 30; the medical costs seem to skyrocket. \n      \n- For non smokers over 42 years old, there is a strong consistency in having higher medical costs. \n     \n- Parents of 4 children or more and no smokers tend to have much lower medical costs on average than otherwhise. (very few of those observations though).","8ee4d12a":"- We see a very strong relationship between smokers with higher bmi with medical charges. (The R-squred is 0.71 in the case of females smokers.!)\n- For smokers, when their bmi > 30; their medical costs seem to skyrocket. \n- The relationship between non smokers, bmi and medical charges seems to follow a white gaussian noise.\n- We can use this insight to engineer binned discrete dummy variables to integratates the interaction of having a high BMI levels and being a smoker.","f73b98af":"**Interaction: Region-Sex-Smoker**","682bab2f":"### 1.2 DataSet Correlation Matrix \nWe can explore the correlation matrix of our features.","6e91dd4b":"We fit the model with the new \"pruned\" data :","47cb23ab":"- Age seems to have a positive relationship with medical costs, for both smokers and non-smokers alike, however for smokers the relationship is less evident.","ba22aa11":"- The distribution of the medical costs is very asymetric, with a very long right tail. \n- 75% of the values are under 16.650$ and many outliers fall toward the right extreme. \n- A logarytmic transformation could make the variable less skewed. ","c924f87c":"- We see that the BMI distribution resembles a normal distribution.","bb4b6a76":"**Interaction: Sex-Children**","cf981bfa":"- The smoker distribution is very spread out for both males and females.","f21e2106":"**Interaction: Children - Smoker**","ecd7bae6":"### 3.5 Influential Observations: Outlier pruining to derive better generalizability ?","2a0fe117":"As we have identified, most extreme residuals seem to be non smokers with very high medical costs. (red bubbles)","231d77e3":"#  Linear regression with feature engineering, error analysis and outlier pruning","0e05e88d":"# 1. Dataset EDA","583ae5f7":"Cook's distance tries to identify the points that have more influence than other points. Such influential points tend to have a considerable impact on the inclination of the regression line. In other words, adding or removing those points from the model can completely change the model's statistics. Cook's Distance depends on both, the residuals and the leverage, hat value. It summarizes directly how much all the fitted values change when the i-th observation is eliminated. A data point that has a large D. of Cook indicates that the data point strongly influences the fitted values.","5c8c4d10":"### 3.5 Error Analysis\n\nWhen trying to solve a new machine learning problem, one common approach is to build a quick basic learning model and iterate on it. Identifying patterns in the errors and keep on fixing them. Manually examining the errors our model is systematically making, can give you insights into what to do next. This process is called error analysis. ","2f2c4132":"- We see that being a smoker seems has a strong positive correlation with higher medical costs.\n- Age and bmi also have a positive correlation, altough less strong.","22f0d10a":"- From up to to the third child, charges tend to grow for both sexes, however from the 4th children on, average charges drop drastically. (We have have to take these results with a grain of salt, though, because we have very few observations with 4 or 5 children.","39e48e12":"**RMSE with our feature engineered Matrix :**","35c10f2c":"# 3. Linear Regression Cross Validation and model Selection","b1a9c73d":"FEATURES :\n\n- **sex:** insurance contractor gender, female, male\n\n- **bmi:** body mass index, providing an understanding of body, weights that are relatively high or low relative to height, objective index of body weight (kg \/ m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9\n\n- **children:** Number of children covered by health insurance \/ Number of dependents\n\n- **smoker:** Smoking\n\n- **region:** the beneficiary's residential area in the US, northeast, southeast, southwest, northwest.\n\nTARGET:\n\n- **charges:** Individual medical costs billed by health insurance","cdb6e6c9":"**We have \"pruned\" the 49 observations with the highest Cook's distance.**","67be2953":"- We don't see any clear regional trend with medical costs, however, males from the southeast seem to be higher charged and females from the shouthwest less. ","5457820a":"We see a clear separation in the the \"pruned\" observations between smokers and non smokers.","6c40ad8c":"**One-Hot-Encoding (Dummy Variables)**\n\nWe can represent our categorical variables using the one-hotencoding or one-out-of-N encoding, also known as dummy variables. The idea behind dummy variables is to replace a categorical variable with one or more new features\nthat can have the values 0 and 1. ","9932ac8e":"**RMSE with Original feature Matrix :**","a17744f2":"### 1.3 Dependencies and interactions between features\n\nInteractions occur when variables act together to impact the output of the process. We can use interaction plots are used to understand the behavior of how one variable (discrete or continuous) depends on the value of another variable. Interaction plots plot both variables together on the same graph. \n\nWhile these plots can help us interpret the interaction effects, we would normally use hypothesis tests to determine whether the effect is statistically significant. We also have to be aware that these plots can display random sample error rather than an actual effect, specially in small samples. \n\n- Parallel lines: No interaction occurs.\n- Nonparallel lines: An interaction occurs. The more nonparallel the lines are, the greater the strength of the interaction.","e62ca830":"### 3.3 Root Mean Square error (RMSE) with different Matrix features"}}