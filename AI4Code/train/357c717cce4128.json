{"cell_type":{"9549ea89":"code","8d96b2a3":"code","e2643243":"code","a21f4909":"code","87817f0f":"code","5a56bb6d":"code","ce1728e8":"code","e412a20a":"code","3ef25f57":"code","b702986f":"code","2e6cd654":"code","f573ea26":"code","2eb8706d":"code","be8e8e47":"code","daeccb72":"code","56e54c4c":"code","605aaa3e":"code","acd69045":"code","a40170c8":"code","b835d352":"code","a5d3aead":"code","45379091":"code","a6218a3e":"code","66cccb6d":"code","e7aebaac":"code","1bd1d8e3":"code","3f4f0ba3":"code","31e2be57":"code","dfd5be0c":"code","d39e4b0a":"code","3afa260d":"code","c231cde6":"code","596c0193":"code","b32c6415":"code","e34ac4a6":"code","d5920c93":"code","21e86e7a":"code","432da80d":"code","223ff2c7":"code","75035368":"code","f8332ce4":"code","326427ef":"code","f01db1f8":"code","264c18de":"code","1425ebec":"code","8a51c2e2":"code","9adfaa7e":"code","ca895c8d":"code","b8bf0406":"code","c84772a9":"code","2b483e8e":"markdown","8f763ffb":"markdown","67bceab9":"markdown","6688f474":"markdown","b6c1fa67":"markdown","c2beb616":"markdown","664c4b51":"markdown","acbcb0a6":"markdown","97938e79":"markdown","1712d02d":"markdown","f73dccc9":"markdown","c8e45bf9":"markdown","b601958b":"markdown","502fecc9":"markdown"},"source":{"9549ea89":"import pandas as pd\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm \nfrom matplotlib import cm\nimport seaborn as sns","8d96b2a3":"import os\nprint(os.listdir(\"\/kaggle\/input\/forest-cover-type-kernels-only\"))","e2643243":"import zipfile\ntrain_zip = zipfile.ZipFile('\/kaggle\/input\/forest-cover-type-kernels-only\/train.csv.zip')\ntest_zip = zipfile.ZipFile('\/kaggle\/input\/forest-cover-type-kernels-only\/test.csv.zip')\n\ntrain = pd.read_csv(train_zip.open('train.csv'))\ntest = pd.read_csv(test_zip.open('test.csv'))\n\nId = test['Id']","a21f4909":"train.head()","87817f0f":"test.head()","5a56bb6d":"train.info()","ce1728e8":"print(\"The number of traning examples(data points) = %i \" % train.shape[0])\nprint(\"The number of features we have = %i \" % train.shape[1])","e412a20a":"print(\"The number of traning examples(data points) = %i \" % test.shape[0])\nprint(\"The number of features we have = %i \" % test.shape[1])","3ef25f57":"train.describe()","b702986f":"train.isnull().sum()","2e6cd654":"f,ax = plt.subplots(figsize=(25, 25))\nsns.heatmap(train.corr(), annot=True, linewidths=.5, fmt= '.3f',ax=ax)\nplt.show()","f573ea26":"train.corr()","2eb8706d":"#train.drop(['Id'], inplace = True, axis = 1 )\ntrain.drop(['Id','Soil_Type15' , \"Soil_Type7\"], inplace = True, axis = 1 )\ntest.drop(['Id','Soil_Type15' , \"Soil_Type7\"], inplace = True, axis = 1 )","be8e8e47":"train['HorizontalHydrology_HorizontalFire'] = (train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Fire_Points'])\ntrain['Neg_HorizontalHydrology_HorizontalFire'] = (train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Fire_Points'])\ntrain['HorizontalHydrology_HorizontalRoadways'] = (train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Roadways'])\ntrain['Neg_HorizontalHydrology_HorizontalRoadways'] = (train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Roadways'])\ntrain['HorizontalFire_Points_HorizontalRoadways'] = (train['Horizontal_Distance_To_Fire_Points']+train['Horizontal_Distance_To_Roadways'])\ntrain['Neg_HorizontalFire_Points_HorizontalRoadways'] = (train['Horizontal_Distance_To_Fire_Points']-train['Horizontal_Distance_To_Roadways'])\n\ntrain['Neg_Elevation_Vertical'] = train['Elevation']-train['Vertical_Distance_To_Hydrology']\ntrain['Elevation_Vertical'] = train['Elevation']+train['Vertical_Distance_To_Hydrology']\n\ntrain['mean_hillshade'] =  (train['Hillshade_9am']  + train['Hillshade_Noon'] + train['Hillshade_3pm'] ) \/ 3\n\ntrain['Mean_HorizontalHydrology_HorizontalFire'] = (train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Fire_Points'])\/2\ntrain['Mean_HorizontalHydrology_HorizontalRoadways'] = (train['Horizontal_Distance_To_Hydrology']+train['Horizontal_Distance_To_Roadways'])\/2\ntrain['Mean_HorizontalFire_Points_HorizontalRoadways'] = (train['Horizontal_Distance_To_Fire_Points']+train['Horizontal_Distance_To_Roadways'])\/2\n\ntrain['MeanNeg_Mean_HorizontalHydrology_HorizontalFire'] = (train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Fire_Points'])\/2\ntrain['MeanNeg_HorizontalHydrology_HorizontalRoadways'] = (train['Horizontal_Distance_To_Hydrology']-train['Horizontal_Distance_To_Roadways'])\/2\ntrain['MeanNeg_HorizontalFire_Points_HorizontalRoadways'] = (train['Horizontal_Distance_To_Fire_Points']-train['Horizontal_Distance_To_Roadways'])\/2\n\ntrain['Slope2'] = np.sqrt(train['Horizontal_Distance_To_Hydrology']**2+train['Vertical_Distance_To_Hydrology']**2)\ntrain['Mean_Fire_Hydrology_Roadways']=(train['Horizontal_Distance_To_Fire_Points'] + train['Horizontal_Distance_To_Hydrology'] + train['Horizontal_Distance_To_Roadways']) \/ 3\ntrain['Mean_Fire_Hyd']=(train['Horizontal_Distance_To_Fire_Points'] + train['Horizontal_Distance_To_Hydrology']) \/ 2 \n\ntrain[\"Vertical_Distance_To_Hydrology\"] = abs(train['Vertical_Distance_To_Hydrology'])\n\ntrain['Neg_EHyd'] = train.Elevation-train.Horizontal_Distance_To_Hydrology*0.2\n\n\ntest['HorizontalHydrology_HorizontalFire'] = (test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Fire_Points'])\ntest['Neg_HorizontalHydrology_HorizontalFire'] = (test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Fire_Points'])\ntest['HorizontalHydrology_HorizontalRoadways'] = (test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Roadways'])\ntest['Neg_HorizontalHydrology_HorizontalRoadways'] = (test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Roadways'])\ntest['HorizontalFire_Points_HorizontalRoadways'] = (test['Horizontal_Distance_To_Fire_Points']+test['Horizontal_Distance_To_Roadways'])\ntest['Neg_HorizontalFire_Points_HorizontalRoadways'] = (test['Horizontal_Distance_To_Fire_Points']-test['Horizontal_Distance_To_Roadways'])\n\ntest['Neg_Elevation_Vertical'] = test['Elevation']-test['Vertical_Distance_To_Hydrology']\ntest['Elevation_Vertical'] = test['Elevation'] + test['Vertical_Distance_To_Hydrology']\n\ntest['mean_hillshade'] = (test['Hillshade_9am']  + test['Hillshade_Noon']  + test['Hillshade_3pm'] ) \/ 3\n\ntest['Mean_HorizontalHydrology_HorizontalFire'] = (test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Fire_Points'])\/2\ntest['Mean_HorizontalHydrology_HorizontalRoadways'] = (test['Horizontal_Distance_To_Hydrology']+test['Horizontal_Distance_To_Roadways'])\/2\ntest['Mean_HorizontalFire_Points_HorizontalRoadways'] = (test['Horizontal_Distance_To_Fire_Points']+test['Horizontal_Distance_To_Roadways'])\/2\n\ntest['MeanNeg_Mean_HorizontalHydrology_HorizontalFire'] = (test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Fire_Points'])\/2\ntest['MeanNeg_HorizontalHydrology_HorizontalRoadways'] = (test['Horizontal_Distance_To_Hydrology']-test['Horizontal_Distance_To_Roadways'])\/2\ntest['MeanNeg_HorizontalFire_Points_HorizontalRoadways'] = (test['Horizontal_Distance_To_Fire_Points']-test['Horizontal_Distance_To_Roadways'])\/2\n\ntest['Slope2'] = np.sqrt(test['Horizontal_Distance_To_Hydrology']**2+test['Vertical_Distance_To_Hydrology']**2)\ntest['Mean_Fire_Hydrology_Roadways']=(test['Horizontal_Distance_To_Fire_Points'] + test['Horizontal_Distance_To_Hydrology'] + test['Horizontal_Distance_To_Roadways']) \/ 3 \ntest['Mean_Fire_Hyd']=(test['Horizontal_Distance_To_Fire_Points'] + test['Horizontal_Distance_To_Hydrology']) \/ 2\n\n\ntest['Vertical_Distance_To_Hydrology'] = abs(test[\"Vertical_Distance_To_Hydrology\"])\n\ntest['Neg_EHyd'] = test.Elevation-test.Horizontal_Distance_To_Hydrology*0.2","daeccb72":"train.head()","56e54c4c":"test.head()","605aaa3e":"train.shape","acd69045":"test.shape","a40170c8":"from sklearn.model_selection import train_test_split\nx = train.drop(['Cover_Type'], axis = 1)\ny = train['Cover_Type']\n\n\nx_train, x_val, y_train, y_val = train_test_split( x.values, y.values, test_size=0.2, random_state=42 )\nprint(x_train.shape)\nprint(x_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)","b835d352":"unique, count= np.unique(y_train, return_counts=True)\nprint(\"The number of occurances of each class in the dataset = %s \" % dict (zip(unique, count) ), \"\\n\" )","a5d3aead":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(x_train)\nx_train = scaler.transform(x_train)\nx_val = scaler.transform(x_val)\n\ntest = scaler.transform(test)","45379091":"from sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, RandomizedSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier , ExtraTreesClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression","a6218a3e":"rf_1 = RandomForestClassifier(n_estimators = 200,criterion = 'entropy',random_state = 0)\nrf_1.fit(X=x_train, y=y_train)\n\ny_pred_train_rf_1 = rf_1.predict(x_train)\ny_pred_val_rf_1 = rf_1.predict(x_val)\n\ny_pred_test_rf_1 = rf_1.predict(test)","66cccb6d":"rf_2 = RandomForestClassifier(n_estimators = 200,criterion = 'gini',random_state = 0)\nrf_2.fit(X=x_train, y=y_train)\n\ny_pred_train_rf_2 = rf_2.predict(x_train)\ny_pred_val_rf_2 = rf_2.predict(x_val)\n\ny_pred_test_rf_2 = rf_2.predict(test)","e7aebaac":"et_1 = ExtraTreesClassifier(n_estimators = 200,criterion = 'entropy',random_state = 0)\net_1.fit(X=x_train, y=y_train)\n\ny_pred_train_et_1 = et_1.predict(x_train)\ny_pred_val_et_1 = et_1.predict(x_val)\n\ny_pred_test_et_1 = et_1.predict(test)","1bd1d8e3":"et_2 = ExtraTreesClassifier(n_estimators = 200,criterion = 'gini',random_state = 0)\net_2.fit(X=x_train, y=y_train)\n\ny_pred_train_et_2 = et_2.predict(x_train)\ny_pred_val_et_2 = et_2.predict(x_val)\n\ny_pred_test_et_2 = et_2.predict(test)","3f4f0ba3":"lgb = LGBMClassifier(n_estimators = 200,learning_rate = 0.1)\nlgb.fit(X=x_train, y=y_train)\n\ny_pred_train_lgb = lgb.predict(x_train)\ny_pred_val_lgb = lgb.predict(x_val)\n\ny_pred_test_lgb = lgb.predict(test)","31e2be57":"lr_1 = LogisticRegression(solver = 'liblinear',multi_class = 'ovr',C = 1,random_state = 0)\nlr_1.fit(X=x_train, y=y_train)\n\ny_pred_train_lr_1 = lr_1.predict(x_train)\ny_pred_val_lr_1 = lr_1.predict(x_val)\n\ny_pred_test_lr_1 = lr_1.predict(test)","dfd5be0c":"xgb_1 = XGBClassifier(seed = 0,colsample_bytree = 0.7, silent = 1, subsample = 0.7, learning_rate = 0.1, objective = 'multi:softprob',\n                      num_class = 7,max_depth = 4, min_child_weight = 1, eval_metric = 'mlogloss', nrounds = 200)\nxgb_1.fit(X=x_train, y=y_train)\n\ny_pred_train_xgb_1 = xgb_1.predict(x_train)\ny_pred_val_xgb_1 = xgb_1.predict(x_val)\n\ny_pred_test_xgb_1 = xgb_1.predict(test)","d39e4b0a":"knn = KNeighborsClassifier(n_neighbors=5)  \n\nknn.fit(x_train, y_train)\n\ny_pred_train_knn = knn.predict(x_train)\ny_pred_val_knn = knn.predict(x_val)\n\ny_pred_test_knn = knn.predict(test)","3afa260d":"## Creating DF from Predections\n\nstack_train = pd.DataFrame([y_pred_train_rf_1,y_pred_train_rf_2,y_pred_train_et_1,y_pred_train_et_2,y_pred_train_lgb,\n                            y_pred_train_lr_1,y_pred_train_xgb_1,y_pred_train_knn])\n\nstack_val = pd.DataFrame([y_pred_val_rf_1,y_pred_val_rf_2,y_pred_val_et_1,y_pred_val_et_2,y_pred_val_lgb,\n                            y_pred_val_lr_1,y_pred_val_xgb_1,y_pred_val_knn])\n\nstack_test = pd.DataFrame([y_pred_test_rf_1,y_pred_test_rf_2,y_pred_test_et_1,y_pred_test_et_2,y_pred_test_lgb,\n                            y_pred_test_lr_1,y_pred_test_xgb_1,y_pred_test_knn])","c231cde6":"print(stack_train.head())\nprint(stack_val.head())\n\nprint(stack_test.head())","596c0193":"## Transpose - it will change row into columns and columns into rows\n\nstack_train = stack_train.T\nstack_val = stack_val.T\n\nstack_test = stack_test.T","b32c6415":"print(stack_train.head())\nprint(stack_val.head())\nprint(stack_test.head())","e34ac4a6":"print(stack_train.shape)\nprint(stack_val.shape)\nprint(stack_test.shape)","d5920c93":"stack_test.isnull().sum()","21e86e7a":"lr_2 = LogisticRegression(solver = 'liblinear',multi_class = 'ovr',C = 5,random_state = 0)\nlr_2.fit(X=stack_train, y=y_train)\n\nstacked_pred_train = lr_2.predict(stack_train)\nstacked_pred_val = lr_2.predict(stack_val)\n\nstacked_pred_test = lr_2.predict(stack_test)","432da80d":"#Id = test['Id']\n#test.drop(['Id'], inplace = True, axis = 1 )\n#-final_pred = lr_2.predict(test)\n\n\nsubmission_1 = pd.DataFrame()\nsubmission_1['Id'] = Id\nsubmission_1['Cover_Type'] = stacked_pred_test\nsubmission_1.to_csv('submission_stack.csv', index=False)\nsubmission_1.head(5)","223ff2c7":"from catboost import Pool, CatBoostClassifier\n\ncat = CatBoostClassifier()\n\ncat.fit(x_train, y_train)","75035368":"print('Accuracy of classifier on training set: {:.2f}'.format(cat.score(x_train, y_train) * 100))\nprint('Accuracy of classifier on test set: {:.2f}'.format(cat.score(x_val, y_val) * 100))","f8332ce4":"cat_predictions = cat.predict(test)","326427ef":"submission_2 = pd.DataFrame()\nsubmission_2['Id'] = Id\nsubmission_2['Cover_Type'] = cat_predictions\nsubmission_2.to_csv('submission.csv', index=False)\nsubmission_2.head(5)","f01db1f8":"XGB = XGBClassifier()\n\nXGB.fit(x_train, y_train)","264c18de":"print('Accuracy of classifier on training set: {:.2f}'.format(XGB.score(x_train, y_train) * 100))\nprint('Accuracy of classifier on test set: {:.2f}'.format(XGB.score(x_val, y_val) * 100))","1425ebec":"XGB_predictions = XGB.predict(test)","8a51c2e2":"submission_3 = pd.DataFrame()\nsubmission_3['Id'] = Id\nsubmission_3['Cover_Type'] = XGB_predictions\nsubmission_3.to_csv('submission_XGB.csv', index=False)\nsubmission_3.head(5)","9adfaa7e":"RFC = RandomForestClassifier()\n\nRFC.fit(x_train, y_train)","ca895c8d":"print('Accuracy of classifier on training set: {:.2f}'.format(RFC.score(x_train, y_train) * 100))\nprint('Accuracy of classifier on test set: {:.2f}'.format(RFC.score(x_val, y_val) * 100))","b8bf0406":"RFC_predictions = RFC.predict(test)","c84772a9":"submission_4 = pd.DataFrame()\nsubmission_4['Id'] = Id\nsubmission_4['Cover_Type'] = RFC_predictions\nsubmission_4.to_csv('submission_RFC.csv', index=False)\nsubmission_4.head(5)","2b483e8e":"## XGBoostClassifier","8f763ffb":"Now we should seperate the training set from the labels and name them x and y then we will split them into training and test sets to be able to see how well it would do on unseen data which will give anestimate on how well it will do when testing on Kaggle test data. I will use the convention of using 80% of the data as training set and 20% for the test set.","67bceab9":"## Select and Initialize Classifiers\n\nI have tried to select various classifiers. The key is a good validation score and if possible the use of a diffferent method\/classifier for the ensembling.","6688f474":"## RandomForestClassifier","b6c1fa67":"It seems the data points in each class are almost balanced so it will be okay to use accuracy as a metric to measure how well the ML model performs","c2beb616":"From the above results it seems that soil_Type7 and soil_Type15 doesn't haveany correlation with the output cover_Type so we can easily drop them from the data we have. Also Soil_Type9, Soil_Type36, Soil_Type27, Soil_Type25, Soil_Type8 have weak correlation, but when a feature has a weak correlation tht doesn't mean it is useful cuz combined with other feature it may make a good impact. I choose those columns after experimenting many times with the data i have from the Extratrees, correlation matrix and the heatmap.","664c4b51":"It is important to know if the number of points in the classes are balanced. If the data is skewed then we will not be able to use accuracy as a performance metric since it will be misleading but if it is skewed we may use F-beta score or precision and recall. Precision or recall or F1 score. the choice depends on the problem itself. Where high recall means low number of false negatives , High precision means low number of false positives and F1 score is a trade off between them. You can refere to this article for more about precision and recall http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_precision_recall.html","acbcb0a6":"Let's check if any of the columns contains NaNs or Nulls so that we can fill those values if they are insignificant or drop them. We may drop a whole column if most of its values are NaNs or fill its value according to its relation with other columns in the dataframe. Nones can also be 0 in some datasets and that is why i am going to use the describe of the train to see if the range of numbers is not reasonable or not. if you are dropping rows with NaNs and you notice that you need to drop a large portion of your dataset then you should think about filling the NaN values or drop a column that has most of its values missing.","97938e79":"Data Exploration and analysis","1712d02d":"The given dataset cantains 56 features including the target variable Cover_Type, along with 15120 observations and following are the features\n\n* Elevation - Elevation in meters\n* Aspect - Aspect in degrees azimuth\n* Slope - Slope in degrees\n* Horizontal_Distance_To_Hydrology - Horz Dist to nearest surface water features\n* Vertical_Distance_To_Hydrology - Vert Dist to nearest surface water features\n* Horizontal_Distance_To_Roadways - Horz Dist to nearest roadway\n* Hillshade_9am (0 to 255 index) - Hillshade index at 9am, summer solstice\n* Hillshade_Noon (0 to 255 index) - Hillshade index at noon, summer solstice\n* Hillshade_3pm (0 to 255 index) - Hillshade index at 3pm, summer solstice\n* Horizontal_Distance_To_Fire_Points - Horz Dist to nearest wildfire ignition points\n* Wilderness_Area (4 binary columns, 0 = absence or 1 = presence) - Wilderness area designation\n* Soil_Type (40 binary columns, 0 = absence or 1 = presence) - Soil Type designation\n* Cover_Type (7 types, integers 1 to 7) - Forest Cover Type designation\n\nThe wilderness areas are:\n\n* 1 - Rawah Wilderness Area\n* 2 - Neota Wilderness Area\n* 3 - Comanche Peak Wilderness Area\n* 4 - Cache la Poudre Wilderness Area\n\nThe soil types are:\n\n* 1 Cathedral family - Rock outcrop complex, extremely stony.\n* 2 Vanet - Ratake families complex, very stony.\n* 3 Haploborolis - Rock outcrop complex, rubbly.\n* 4 Ratake family - Rock outcrop complex, rubbly.\n* 5 Vanet family - Rock outcrop complex complex, rubbly.\n* 6 Vanet - Wetmore families - Rock outcrop complex, stony.\n* 7 Gothic family.\n* 8 Supervisor - Limber families complex.\n* 9 Troutville family, very stony.\n* 10 Bullwark - Catamount families - Rock outcrop complex, rubbly.\n* 11 Bullwark - Catamount families - Rock land complex, rubbly.\n* 12 Legault family - Rock land complex, stony.\n* 13 Catamount family - Rock land - Bullwark family complex, rubbly.\n* 14 Pachic Argiborolis - Aquolis complex.\n* 15 unspecified in the USFS Soil and ELU Survey.\n* 16 Cryaquolis - Cryoborolis complex.\n* 17 Gateview family - Cryaquolis complex.\n* 18 Rogert family, very stony.\n* 19 Typic Cryaquolis - Borohemists complex.\n* 20 Typic Cryaquepts - Typic Cryaquolls complex.\n* 21 Typic Cryaquolls - Leighcan family, till substratum complex.\n* 22 Leighcan family, till substratum, extremely bouldery.\n* 23 Leighcan family, till substratum - Typic Cryaquolls complex.\n* 24 Leighcan family, extremely stony.\n* 25 Leighcan family, warm, extremely stony.\n* 26 Granile - Catamount families complex, very stony.\n* 27 Leighcan family, warm - Rock outcrop complex, extremely stony.\n* 28 Leighcan family - Rock outcrop complex, extremely stony.\n* 29 Como - Legault families complex, extremely stony.\n* 30 Como family - Rock land - Legault family complex, extremely stony.\n* 31 Leighcan - Catamount families complex, extremely stony.\n* 32 Catamount family - Rock outcrop - Leighcan family complex, extremely stony.\n* 33 Leighcan - Catamount families - Rock outcrop complex, extremely stony.\n* 34 Cryorthents - Rock land complex, extremely stony.\n* 35 Cryumbrepts - Rock outcrop - Cryaquepts complex.\n* 36 Bross family - Rock land - Cryumbrepts complex, extremely stony.\n* 37 Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.\n* 38 Leighcan - Moran families - Cryaquolls complex, extremely stony.\n* 39 Moran family - Cryorthents - Leighcan family complex, extremely stony.\n* 40 Moran family - Cryorthents - Rock land complex, extremely stony.","f73dccc9":"## CatBoostClassifier","c8e45bf9":"It seems we don't have any NaN or Null value among the dataset we are trying to classify. Let's now discover the correlation matrix for this dataset and see if we can combine features or drop some according to its correlation with the output labels.","b601958b":"## Problem Statement","502fecc9":"Predict the forest cover type from the given cartographic variables. This study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. These areas represent forests with minimal human-caused disturbances, so that existing forest cover types are more a result of ecological processes rather than forest management practices."}}