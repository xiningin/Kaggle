{"cell_type":{"6b148d43":"code","49c3bfd2":"code","0248e8c8":"code","52311ad9":"code","79333a17":"code","52b6b65d":"code","d79bc0d0":"code","3250ba5f":"code","1283ad73":"code","79afaea7":"code","9903b46c":"code","6a684c12":"code","7e732561":"code","efca5df9":"code","2ed593d7":"code","d53efe2a":"code","310e1fa6":"code","08bdd19b":"code","90adcf6c":"code","d6789bf0":"code","69c74e33":"code","6b7873e1":"code","8faf5dd9":"code","ed31472c":"code","caa98643":"code","cb557685":"code","7db5e8ff":"code","f5e48d86":"code","874a5835":"code","bc8a0ed2":"code","50dc83ae":"code","6228fd28":"code","a31a5945":"code","b147203e":"code","5841cf70":"code","0e33c803":"code","29949a36":"code","2727958c":"code","47e23ad6":"code","866b2fdd":"code","d9224b0e":"code","c13a5bc8":"code","b2609ae8":"code","df3c65d7":"code","af24594b":"code","bd0f0434":"code","772fcf8f":"code","d81ea00b":"code","87139703":"code","b9f5b716":"code","f32ff55c":"code","b08a2d51":"code","8d245b2e":"code","76810314":"code","42a60caa":"code","69d5f8cd":"code","44ccaa5c":"code","b92fd104":"code","01bb56ce":"code","9a0c037f":"code","1679d14f":"code","e81ed283":"code","b958c7bc":"code","fb4b0b87":"code","11b46af7":"code","87db72b7":"code","556484f5":"code","601b38b5":"code","c2a431b1":"code","38d696a6":"code","67c8b1df":"code","9dfece24":"code","d6472428":"code","0ac419a7":"code","5878c366":"code","28f11b7a":"code","fcd746ed":"code","c15ab238":"code","85ad33de":"code","564c81a0":"code","4be594f4":"code","d9d6b873":"code","c11269f5":"code","7a817fb8":"code","87fa2e61":"code","f8a1824b":"code","f11708ac":"code","5ffcdc7c":"code","4d543d63":"code","47e8ae41":"code","11bbd85f":"code","f8dd394b":"code","6b43cb6a":"code","82f921b8":"markdown","a9f0bc36":"markdown","c8e24d2d":"markdown","4aff5230":"markdown","30266cb2":"markdown","8a9ba28f":"markdown","1cf08a25":"markdown","eb63e06c":"markdown","ded74d63":"markdown","2c9e701d":"markdown","44afa19a":"markdown","4d76f508":"markdown","75ad9f4f":"markdown","54c0c9e7":"markdown","c51f462c":"markdown","8f7916bc":"markdown","084542bb":"markdown","338b87fb":"markdown","eeb19265":"markdown","9e0b03b0":"markdown","79eaf1ee":"markdown"},"source":{"6b148d43":"import torch\nimport torch.nn as nn\n\nimport pprint\npp = pprint.PrettyPrinter()","49c3bfd2":"data =[\n    [0,1],\n    [2,3],\n    [4,5]\n]\nx_python = torch.tensor(data)\nx_python","0248e8c8":"x_float = torch.tensor(data, dtype=torch.float)\nx_float","52311ad9":"x_bool = torch.tensor(data, dtype=torch.bool)\nx_bool","79333a17":"x_python.float()","52b6b65d":"x = torch.Tensor(data)\n# \u5927\u5199\u7684Tensor\u548c\u5c0f\u5199\u7684\u4e0d\u4e00\u6837\nx","d79bc0d0":"import numpy as np\nndarray = np.array(data)\nx_numpy = torch.from_numpy(ndarray)\nx_numpy","3250ba5f":"x = torch.tensor([[1., 2.],[3., 4.]])\nx","1283ad73":"x_zeros = torch.zeros_like(x)\n# \u5965\u5965\u76f4\u63a5\u6284\u7ed3\u6784\nx_zeros","79afaea7":"x_ones = torch.ones_like(x)\nx_ones","9903b46c":"# 0-1\u4e4b\u95f4\u7684\u968f\u673a\u6570\nx_rand = torch.rand_like(x)\nx_rand","6a684c12":"# \u6b63\u6001\u5206\u5e03\nx_randn = torch.randn_like(x)\nx_randn","7e732561":"shape = (4,2,2)\nx_zeros = torch.zeros(shape)\nx_zeros","efca5df9":"x = torch.arange(10)\nx","2ed593d7":"x = torch.ones(3,2)\nx.dtype","d53efe2a":"x.shape","310e1fa6":"x.size(0)","08bdd19b":"x = torch.Tensor([[1,2],[3,4],[5,6]])\nx_view = x.view(3,2)\nx_view","90adcf6c":"x_view = x.view(-1,3)\n# \u4e0ex_view = x.view(2,3)\u540c, -1\u76f8\u5f53\u4e8e\u8ba9\u5b83\u81ea\u5df1\u7b97\u8981\u591a\u5c11\u884c\u6765\u5339\u914d\u4e00\u884c\u4e09\u4e2a\u6570\u5b57\nx_view","d6789bf0":"x_reshaped = torch.reshape(x,(2,3))\nx_reshaped","69c74e33":"x = torch.arange(10).reshape(5,2)\nx","6b7873e1":"# Add a new dimension of size 1 at the 1st dimension\nx = x.unsqueeze(1)\nx.shape","8faf5dd9":"x","ed31472c":"# Squeeze the dimensions of x by getting rid of all the dimensions with 1 element\nx = x.squeeze()\nx.shape","caa98643":"x","cb557685":"# Get the number of elements in tensor.\nx.numel()","7db5e8ff":"x = torch.Tensor([[1,2],[3,4]])\nx","f5e48d86":"x.device","874a5835":"if torch.cuda.is_available():\n    x.to('cuda')","bc8a0ed2":"# Initialize an example tensor\nx = torch.Tensor([\n                  [[1, 2], [3, 4]],\n                  [[5, 6], [7, 8]], \n                  [[9, 10], [11, 12]] \n                 ])\nx","50dc83ae":"x.shape","6228fd28":"x[0]","a31a5945":"x[:,0,0]","b147203e":"x","5841cf70":"# Let's access the 0th and 1st elements, each twice\ni = torch.tensor([0,0,1,1])\nx[i]","0e33c803":"# Let's access the 0th elements of the 1st and 2nd elements\ni = torch.tensor([1,2])\nj = torch.tensor([0])\nx[i,j]","29949a36":"# We can get a Python scalar value from a tensor with item().\nx[0,0,0]","2727958c":"x[0,0,0].item()","47e23ad6":"x = torch.ones(3,2,2)\nx","866b2fdd":"x+2","d9224b0e":"x*2","c13a5bc8":"a = torch.ones((4,3))*6\n# a = torch.ones(4,3)*6 \u4e5f\u4e00\u6837\na","b2609ae8":"b = torch.ones(3)*2\nb","df3c65d7":"a\/b","af24594b":"b = torch.Tensor([1,2,3])\na\/b\n# \u5bf9\u5e94\u884c\u548c\u5217\u76f8\u9664","bd0f0434":"# Alternative to a.matmul(b)\n# a @ b.T returns the same result since b is 1D tensor and the 2nd dimension\n# is inferred\na @ b ","772fcf8f":"pp.pprint(a.shape)\npp.pprint(a.T.shape)","d81ea00b":"m = torch.tensor([\n    [1.,1.],\n    [2.,2.],\n    [3.,3.],\n    [4.,4.]\n])\n\npp.pprint(\"Mean: {}\".format(m.mean()))\npp.pprint(\"Mean to the 0th dimension: {}\".format(m.mean(0)))\npp.pprint(\"Mean to the 1th dimension: {}\".format(m.mean(1)))","87139703":"a_cat0 = torch.cat([a,a,a], dim=0)\na_cat1 = torch.cat([a,a,a], dim=1)\n\nprint(\"Initial shape: {}\".format(a.shape))\nprint(\"Shape after concatentation in dimension 0: {}\".format(a_cat0.shape))\nprint(\"Shape after concatentation in dimension 1: {}\".format(a_cat1.shape))","b9f5b716":"a","f32ff55c":"a_cat0","b08a2d51":"a_cat1","8d245b2e":"# add() is not in place\na.add(a)\na","76810314":"#add_() is in place\na.add_(a)\na","42a60caa":"x = torch.tensor([2.], requires_grad=True)\n\npp.pprint(x.grad)","69d5f8cd":"y = x*x*3\ny.backward()\npp.pprint(x.grad)","44ccaa5c":"z = x*x*2\nz.backward()\npp.pprint(x.grad)","b92fd104":"import torch.nn as nn","01bb56ce":"input = torch.ones(2,3,4)\n\nlinear = nn.Linear(4,2)\nlinear_output = linear(input)\nlinear_output","9a0c037f":"sigmoid = nn.Sigmoid()\noutput = sigmoid(linear_output)\noutput","1679d14f":"block = nn.Sequential(\n    nn.Linear(4,2),\n    nn.Sigmoid()\n)\ninput = torch.ones(2,3,4)\noutput = block(input)\noutput","e81ed283":"class MultilayerPerceptron(nn.Module):\n\n  def __init__(self, input_size, hidden_size):\n    # Call to the __init__ function of the super class\n    super(MultilayerPerceptron, self).__init__()\n\n    # Bookkeeping: Saving the initialization parameters\n    self.input_size = input_size \n    self.hidden_size = hidden_size \n\n    # Defining of our model\n    # There isn't anything specific about the naming of `self.model`. It could\n    # be something arbitrary.\n    self.model = nn.Sequential(\n        nn.Linear(self.input_size, self.hidden_size),\n        nn.ReLU(),\n        nn.Linear(self.hidden_size, self.input_size),\n        nn.Sigmoid()\n    )\n    \n  def forward(self, x):\n    output = self.model(x)\n    return output","b958c7bc":"class MultilayerPerceptron(nn.Module):\n\n  def __init__(self, input_size, hidden_size):\n    # Call to the __init__ function of the super class\n    super(MultilayerPerceptron, self).__init__()\n\n    # Bookkeeping: Saving the initialization parameters\n    self.input_size = input_size \n    self.hidden_size = hidden_size \n\n    # Defining of our layers\n    self.linear = nn.Linear(self.input_size, self.hidden_size)\n    self.relu = nn.ReLU()\n    self.linear2 = nn.Linear(self.hidden_size, self.input_size)\n    self.sigmoid = nn.Sigmoid()\n    \n  def forward(self, x):\n    linear = self.linear(x)\n    relu = self.relu(linear)\n    linear2 = self.linear2(relu)\n    output = self.sigmoid(linear2)\n    return output","fb4b0b87":"# Make a sample input\ninput = torch.randn(2, 5)\n\n# Create our model\nmodel = MultilayerPerceptron(5, 3)\n\n# Pass our input through our model\nmodel(input)","11b46af7":"list(model.named_parameters())","87db72b7":"import torch.optim as optim","556484f5":"# Create the y data\ny = torch.ones(10, 5)\n\n# Add some noise to our goal y to generate our x\n# We want out model to predict our original data, albeit the noise\nx = y + torch.randn_like(y)\nx","601b38b5":"# Instantiate the model\nmodel = MultilayerPerceptron(5, 3)\n\n# Define the optimizer\nadam = optim.Adam(model.parameters(), lr=1e-1)\n\n# Define loss using a predefined loss function\nloss_function = nn.BCELoss()\n\n# Calculate how our model is doing now\ny_pred = model(x)\nloss_function(y_pred, y).item()","c2a431b1":"# Set the number of epoch, which determines the number of training iterations\nn_epoch = 10 \n\nfor epoch in range(n_epoch):\n  # Set the gradients to 0\n  adam.zero_grad()\n\n  # Get the model predictions\n  y_pred = model(x)\n\n  # Get the loss\n  loss = loss_function(y_pred, y)\n\n  # Print stats\n  print(f\"Epoch {epoch}: traing loss: {loss}\")\n\n  # Compute the gradients\n  loss.backward()\n\n  # Take a step to optimize the weights\n  adam.step()","38d696a6":"# See how our model performs on the training data\ny_pred = model(x)\ny_pred","67c8b1df":"# Create test data and check how our model performs on it\nx2 = y + torch.randn_like(y)\ny_pred = model(x2)\ny_pred","9dfece24":"# Our raw data, which consists of sentences\ncorpus = [\n          \"We always come to Paris\",\n          \"The professor is from Australia\",\n          \"I live in Stanford\",\n          \"He comes from Taiwan\",\n          \"The capital of Turkey is Ankara\"\n         ]","d6472428":"# The preprocessing function we will use to generate our training examples\n# Our function is a simple one, we lowercase the letters\n# and then tokenize the words.\ndef preprocess_sentence(sentence):\n  return sentence.lower().split()\n\n# Create our training set\ntrain_sentences = [sent.lower().split() for sent in corpus]\ntrain_sentences","0ac419a7":"# Set of locations that appear in our corpus\nlocations = set([\"australia\", \"ankara\", \"paris\", \"stanford\", \"taiwan\", \"turkey\"])\n\n# Our train labels\ntrain_labels = [[1 if word in locations else 0 for word in sent] for sent in train_sentences]\ntrain_labels","5878c366":"# Find all the unique words in our corpus \nvocabulary = set(w for s in train_sentences for w in s)\nvocabulary","28f11b7a":"# Add the unknown token to our vocabulary\nvocabulary.add(\"<unk>\")","fcd746ed":"# Add the <pad> token to our vocabulary\nvocabulary.add(\"<pad>\")\n\n# Function that pads the given sentence\n# We are introducing this function here as an example\n# We will be utilizing it later in the tutorial\ndef pad_window(sentence, window_size, pad_token=\"<pad>\"):\n  window = [pad_token] * window_size\n  return window + sentence + window\n\n# Show padding example\nwindow_size = 2\npad_window(train_sentences[0], window_size=window_size)","c15ab238":"# We are just converting our vocabularly to a list to be able to index into it\n# Sorting is not necessary, we sort to show an ordered word_to_ind dictionary\n# That being said, we will see that having the index for the padding token\n# be 0 is convenient as some PyTorch functions use it as a default value\n# such as nn.utils.rnn.pad_sequence, which we will cover in a bit\nix_to_word = sorted(list(vocabulary))\n\n# Creating a dictionary to find the index of a given word\nword_to_ix = {word: ind for ind, word in enumerate(ix_to_word)}\nword_to_ix","85ad33de":"# Given a sentence of tokens, return the corresponding indices\ndef convert_token_to_indices(sentence, word_to_ix):\n  indices = []\n  for token in sentence:\n    # Check if the token is in our vocabularly. If it is, get it's index. \n    # If not, get the index for the unknown token.\n    if token in word_to_ix:\n      index = word_to_ix[token]\n    else:\n      index = word_to_ix[\"<unk>\"]\n    indices.append(index)\n  return indices\n\n# More compact version of the same function\ndef _convert_token_to_indices(sentence, word_to_ix):\n  return [word_to_ind.get(token, word_to_ix[\"<unk>\"]) for token in sentence]\n\n# Show an example\nexample_sentence = [\"we\", \"always\", \"come\", \"to\", \"kuwait\"]\nexample_indices = convert_token_to_indices(example_sentence, word_to_ix)\nrestored_example = [ix_to_word[ind] for ind in example_indices]\n\nprint(f\"Original sentence is: {example_sentence}\")\nprint(f\"Going from words to indices: {example_indices}\")\nprint(f\"Going from indices to words: {restored_example}\")","564c81a0":"# Converting our sentences to indices\nexample_padded_indices = [convert_token_to_indices(s, word_to_ix) for s in train_sentences]\nexample_padded_indices","4be594f4":"# Creating an embedding table for our words\nembedding_dim = 5\nembeds = nn.Embedding(len(vocabulary), embedding_dim)\n\n# Printing the parameters in our embedding table\nlist(embeds.parameters())","d9d6b873":"# Get the embedding for the word Paris\nindex = word_to_ix[\"paris\"]\nindex_tensor = torch.tensor(index, dtype=torch.long)\nparis_embed = embeds(index_tensor)\nparis_embed","c11269f5":"# We can also get multiple embeddings at once\nindex_paris = word_to_ix[\"paris\"]\nindex_ankara = word_to_ix[\"ankara\"]\nindices = [index_paris, index_ankara]\nindices_tensor = torch.tensor(indices, dtype=torch.long)\nembeddings = embeds(indices_tensor)\nembeddings","7a817fb8":"from torch.utils.data import DataLoader\nfrom functools import partial\n\ndef custom_collate_fn(batch, window_size, word_to_ix):\n  # Break our batch into the training examples (x) and labels (y)\n  # We are turning our x and y into tensors because nn.utils.rnn.pad_sequence\n  # method expects tensors. This is also useful since our model will be\n  # expecting tensor inputs. \n  x, y = zip(*batch)\n\n  # Now we need to window pad our training examples. We have already defined a \n  # function to handle window padding. We are including it here again so that\n  # everything is in one place.\n  def pad_window(sentence, window_size, pad_token=\"<pad>\"):\n    window = [pad_token] * window_size\n    return window + sentence + window\n\n  # Pad the train examples.\n  x = [pad_window(s, window_size=window_size) for s in x]\n\n  # Now we need to turn words in our training examples to indices. We are\n  # copying the function defined earlier for the same reason as above.\n  def convert_tokens_to_indices(sentence, word_to_ix):\n    return [word_to_ix.get(token, word_to_ix[\"<unk>\"]) for token in sentence]\n\n  # Convert the train examples into indices.\n  x = [convert_tokens_to_indices(s, word_to_ix) for s in x]\n\n  # We will now pad the examples so that the lengths of all the example in \n  # one batch are the same, making it possible to do matrix operations. \n  # We set the batch_first parameter to True so that the returned matrix has \n  # the batch as the first dimension.\n  pad_token_ix = word_to_ix[\"<pad>\"]\n\n  # pad_sequence function expects the input to be a tensor, so we turn x into one\n  x = [torch.LongTensor(x_i) for x_i in x]\n  x_padded = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=pad_token_ix)\n\n  # We will also pad the labels. Before doing so, we will record the number \n  # of labels so that we know how many words existed in each example. \n  lengths = [len(label) for label in y]\n  lenghts = torch.LongTensor(lengths)\n\n  y = [torch.LongTensor(y_i) for y_i in y]\n  y_padded = nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=0)\n\n  # We are now ready to return our variables. The order we return our variables\n  # here will match the order we read them in our training loop.\n  return x_padded, y_padded, lenghts  ","87fa2e61":"def _custom_collate_fn(batch, window_size, word_to_ix):\n  # Prepare the datapoints\n  x, y = zip(*batch)  \n  x = [pad_window(s, window_size=window_size) for s in x]\n  x = [convert_tokens_to_indices(s, word_to_ix) for s in x]\n\n  # Pad x so that all the examples in the batch have the same size\n  pad_token_ix = word_to_ix[\"<pad>\"]\n  x = [torch.LongTensor(x_i) for x_i in x]\n  x_padded = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=pad_token_ix)\n\n  # Pad y and record the length\n  lengths = [len(label) for label in y]\n  lenghts = torch.LongTensor(lengths)\n  y = [torch.LongTensor(y_i) for y_i in y]\n  y_padded = nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=0)\n\n  return x_padded, y_padded, lenghts  ","f8a1824b":"# Parameters to be passed to the DataLoader\ndata = list(zip(train_sentences, train_labels))\nbatch_size = 2\nshuffle = True\nwindow_size = 2\ncollate_fn = partial(custom_collate_fn, window_size=window_size, word_to_ix=word_to_ix)\n\n# Instantiate the DataLoader\nloader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n\n# Go through one loop\ncounter = 0\nfor batched_x, batched_y, batched_lengths in loader:\n  print(f\"Iteration {counter}\")\n  print(\"Batched Input:\")\n  print(batched_x)\n  print(\"Batched Labels:\")\n  print(batched_y)\n  print(\"Batched Lengths:\")\n  print(batched_lengths)\n  print(\"\")\n  counter += 1","f11708ac":"# Print the original tensor\nprint(f\"Original Tensor: \")\nprint(batched_x)\nprint(\"\")\n\n# Create the 2 * 2 + 1 chunks\nchunk = batched_x.unfold(1, window_size*2 + 1, 1)\nprint(f\"Windows: \")\nprint(chunk)","5ffcdc7c":"class WordWindowClassifier(nn.Module):\n\n  def __init__(self, hyperparameters, vocab_size, pad_ix=0):\n    super(WordWindowClassifier, self).__init__()\n    \n    \"\"\" Instance variables \"\"\"\n    self.window_size = hyperparameters[\"window_size\"]\n    self.embed_dim = hyperparameters[\"embed_dim\"]\n    self.hidden_dim = hyperparameters[\"hidden_dim\"]\n    self.freeze_embeddings = hyperparameters[\"freeze_embeddings\"]\n\n    \"\"\" Embedding Layer \n    Takes in a tensor containing embedding indices, and returns the \n    corresponding embeddings. The output is of dim \n    (number_of_indices * embedding_dim).\n\n    If freeze_embeddings is True, set the embedding layer parameters to be\n    non-trainable. This is useful if we only want the parameters other than the\n    embeddings parameters to change. \n\n    \"\"\"\n    self.embeds = nn.Embedding(vocab_size, self.embed_dim, padding_idx=pad_ix)\n    if self.freeze_embeddings:\n      self.embed_layer.weight.requires_grad = False\n\n    \"\"\" Hidden Layer\n    \"\"\"\n    full_window_size = 2 * window_size + 1\n    self.hidden_layer = nn.Sequential(\n      nn.Linear(full_window_size * self.embed_dim, self.hidden_dim), \n      nn.Tanh()\n    )\n\n    \"\"\" Output Layer\n    \"\"\"\n    self.output_layer = nn.Linear(self.hidden_dim, 1)\n\n    \"\"\" Probabilities \n    \"\"\"\n    self.probabilities = nn.Sigmoid()\n\n  def forward(self, inputs):\n    \"\"\"\n    Let B:= batch_size\n        L:= window-padded sentence length\n        D:= self.embed_dim\n        S:= self.window_size\n        H:= self.hidden_dim\n        \n    inputs: a (B, L) tensor of token indices\n    \"\"\"\n    B, L = inputs.size()\n\n    \"\"\"\n    Reshaping.\n    Takes in a (B, L) LongTensor\n    Outputs a (B, L~, S) LongTensor\n    \"\"\"\n    # Fist, get our word windows for each word in our input.\n    token_windows = inputs.unfold(1, 2 * self.window_size + 1, 1)\n    _, adjusted_length, _ = token_windows.size()\n\n    # Good idea to do internal tensor-size sanity checks, at the least in comments!\n    assert token_windows.size() == (B, adjusted_length, 2 * self.window_size + 1)\n\n    \"\"\"\n    Embedding.\n    Takes in a torch.LongTensor of size (B, L~, S) \n    Outputs a (B, L~, S, D) FloatTensor.\n    \"\"\"\n    embedded_windows = self.embeds(token_windows)\n\n    \"\"\"\n    Reshaping.\n    Takes in a (B, L~, S, D) FloatTensor.\n    Resizes it into a (B, L~, S*D) FloatTensor.\n    -1 argument \"infers\" what the last dimension should be based on leftover axes.\n    \"\"\"\n    embedded_windows = embedded_windows.view(B, adjusted_length, -1)\n\n    \"\"\"\n    Layer 1.\n    Takes in a (B, L~, S*D) FloatTensor.\n    Resizes it into a (B, L~, H) FloatTensor\n    \"\"\"\n    layer_1 = self.hidden_layer(embedded_windows)\n\n    \"\"\"\n    Layer 2\n    Takes in a (B, L~, H) FloatTensor.\n    Resizes it into a (B, L~, 1) FloatTensor.\n    \"\"\"\n    output = self.output_layer(layer_1)\n\n    \"\"\"\n    Softmax.\n    Takes in a (B, L~, 1) FloatTensor of unnormalized class scores.\n    Outputs a (B, L~, 1) FloatTensor of (log-)normalized class scores.\n    \"\"\"\n    output = self.probabilities(output)\n    output = output.view(B, -1)\n\n    return output","4d543d63":"# Prepare the data\ndata = list(zip(train_sentences, train_labels))\nbatch_size = 2\nshuffle = True\nwindow_size = 2\ncollate_fn = partial(custom_collate_fn, window_size=window_size, word_to_ix=word_to_ix)\n\n# Instantiate a DataLoader\nloader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n\n# Initialize a model\n# It is useful to put all the model hyperparameters in a dictionary\nmodel_hyperparameters = {\n    \"batch_size\": 4,\n    \"window_size\": 2,\n    \"embed_dim\": 25,\n    \"hidden_dim\": 25,\n    \"freeze_embeddings\": False,\n}\n\nvocab_size = len(word_to_ix)\nmodel = WordWindowClassifier(model_hyperparameters, vocab_size)\n\n# Define an optimizer\nlearning_rate = 0.01\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\n# Define a loss function, which computes to binary cross entropy loss\ndef loss_function(batch_outputs, batch_labels, batch_lengths):   \n    # Calculate the loss for the whole batch\n    bceloss = nn.BCELoss()\n    loss = bceloss(batch_outputs, batch_labels.float())\n\n    # Rescale the loss. Remember that we have used lengths to store the \n    # number of words in each training example\n    loss = loss \/ batch_lengths.sum().float()\n\n    return loss","47e8ae41":"# Function that will be called in every epoch\ndef train_epoch(loss_function, optimizer, model, loader):\n  \n  # Keep track of the total loss for the batch\n  total_loss = 0\n  for batch_inputs, batch_labels, batch_lengths in loader:\n    # Clear the gradients\n    optimizer.zero_grad()\n    # Run a forward pass\n    outputs = model.forward(batch_inputs)\n    # Compute the batch loss\n    loss = loss_function(outputs, batch_labels, batch_lengths)\n    # Calculate the gradients\n    loss.backward()\n    # Update the parameteres\n    optimizer.step()\n    total_loss += loss.item()\n\n  return total_loss\n\n\n# Function containing our main training loop\ndef train(loss_function, optimizer, model, loader, num_epochs=10000):\n\n  # Iterate through each epoch and call our train_epoch function\n  for epoch in range(num_epochs):\n    epoch_loss = train_epoch(loss_function, optimizer, model, loader)\n    if epoch % 100 == 0: print(epoch_loss)","11bbd85f":"num_epochs = 1000\ntrain(loss_function, optimizer, model, loader, num_epochs=num_epochs)","f8dd394b":"# Create test sentences\ntest_corpus = [\"She comes from Paris\"]\ntest_sentences = [s.lower().split() for s in test_corpus]\ntest_labels = [[0, 0, 0, 1]]\n\n# Create a test loader\ntest_data = list(zip(test_sentences, test_labels))\nbatch_size = 1\nshuffle = False\nwindow_size = 2\ncollate_fn = partial(custom_collate_fn, window_size=2, word_to_ix=word_to_ix)\ntest_loader = torch.utils.data.DataLoader(test_data, \n                                           batch_size=1, \n                                           shuffle=False, \n                                           collate_fn=collate_fn)","6b43cb6a":"for test_instance, labels, _ in test_loader:\n  outputs = model.forward(test_instance)\n  print(labels)\n  print(outputs)","82f921b8":"**by specifying a Shape**\n* torch.zeros()\n* torch.ones()\n* torch.rand()\n* torch.randn()","a9f0bc36":"# Neural Network Module","c8e24d2d":"**Custom Modules**","4aff5230":"**Device**","30266cb2":"# Optimization","8a9ba28f":"**Tensor Indexing**","1cf08a25":"We can see that the x.grad is updated to be the sum of the gradients calculated so far. When we run backprop in a neural network, we sum up all the gradients for a particular neuron before making an update. This is exactly what is happening here! This is also the reason why we need to run zero_grad() in every training iteration (more on this later). Otherwise our gradients would keep building up from one training iteration to the other, which would cause our updates to be wrong.","eb63e06c":"**from a Tensor**","ded74d63":"# Tensors","2c9e701d":"# Demo: Word Window Classification","44afa19a":"**from a numpy array**","4d76f508":"# Autograd","75ad9f4f":"**Other Module Layers**\nThere are several other preconfigured layers in the nn module. Some commonly used examples are nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm1d, nn.BatchNorm2d, nn.Upsample and nn.MaxPool2d among many others. We will learn more about these as we progress in the course. For now, the only important thing to remember is that we can treat each of these layers as plug and play components: we will be providing the required dimensions and PyTorch will take care of setting them up.","54c0c9e7":"**Activation Function Layer**\nnn.ReLU(), nn.Sigmoid() and nn.LeakyReLU()","c51f462c":"**Putting the Layers Together**","8f7916bc":"**operations**","084542bb":"**With torch.arange()**\n* We can also create a tensor with torch.arange(end), which returns a 1-D tensor with elements ranging from 0 to end-1. We can use the optional start and step parameters to create tensors with different ranges.","338b87fb":"**Linear Layer**","eeb19265":"**Preprocessing**","9e0b03b0":"http:\/\/web.stanford.edu\/class\/cs224n\/materials\/CS224N_PyTorch_Tutorial.html","79eaf1ee":"**Data type**"}}