{"cell_type":{"c5375402":"code","6a797eeb":"code","7e2abb13":"code","6cbb9e61":"code","0ee0a09f":"code","a3c705c0":"code","1b996a07":"code","516810c6":"code","f20cc8da":"code","dacce626":"code","9ee236cc":"code","74545a9b":"code","95fb1a60":"code","844cf484":"code","a9bd3a10":"code","ae6d3362":"code","fe973dd4":"code","bd6a2f27":"code","dbf0aa22":"code","65ab5466":"code","319e3e3c":"code","c74a9d7c":"code","72956d44":"code","cb0fcbfb":"code","a4d97702":"code","810ec3a6":"code","aaa84ee4":"code","cbcb91a9":"code","6b492bca":"code","9ea31038":"code","fc931a9d":"code","fff6dce0":"code","a7f81f58":"code","6ed3ae17":"code","0afa77a0":"markdown","1a7f9644":"markdown","f08094f0":"markdown","7de160fc":"markdown","ee3de125":"markdown","f9237128":"markdown","6e5f613f":"markdown","90c596da":"markdown","83b39d83":"markdown","7f4a8238":"markdown","d280648c":"markdown","9b88a14a":"markdown","b112193d":"markdown","dc78ed39":"markdown","081b1519":"markdown"},"source":{"c5375402":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6a797eeb":"from matplotlib import pyplot as plt\nimport seaborn as sns\n\ndf_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntarget_column = \"Survived\"  # \u76ee\u7684\u5909\u6570\nrandom_seed = 1234  # \u4e71\u6570\u56fa\u5b9a\u7528","7e2abb13":"# train\nprint(len(df_train))\nprint(df_train.isnull().sum())","6cbb9e61":"# Embarked 2 \u3092\u898b\u3066\u307f\u308b\nprint(df_train[df_train[\"Embarked\"].isnull()])","0ee0a09f":"# test\nprint(len(df_test))\nprint(df_test.isnull().sum())","a3c705c0":"# Fare \u3092\u898b\u3066\u307f\u308b\nprint(df_test[df_test[\"Fare\"].isnull()])","1b996a07":"df_train.describe(include='all')","516810c6":"df_test.describe(include='all')","f20cc8da":"for column in df_train.columns:\n    # uniq\u60c5\u5831\u3092\u53d6\u5f97\n    uniq = df_train[column].unique()\n\n    # \u8868\u793a\n    print(\"{:20} unique:{:5} {}\".format(\n        column,\n        len(uniq),\n        uniq[:5],  # \u591a\u3059\u304e\u308b\u5834\u5408\u304c\u3042\u308b\u306e\u30675\u4ef6\u306b\u6291\u3048\u308b\n    ))","dacce626":"print(df_train['Survived'].value_counts())  # \u6570\nprint(df_train['Survived'].value_counts(normalize=True))  # \u5272\u5408\nsns.countplot(x=\"Survived\", data=df_train)","9ee236cc":"def plot_category(df, column, target_column):\n    # \u30ab\u30a6\u30f3\u30c8\u60c5\u5831\n    print(pd.crosstab(df[column],df[target_column]))\n\n    print(\"\u5404\u30af\u30e9\u30b9\u6bce\u306e\u751f\u5b58\u7387\")\n    print(pd.crosstab(df[column],df[target_column], normalize='index'))\n\n    print(\"\u751f\u5b58\u7387\u306b\u5bfe\u3059\u308b\u5404\u30af\u30e9\u30b9\u306e\u5272\u5408\")\n    print(pd.crosstab(df[column],df[target_column], normalize='columns'))\n\n    # plot\n    sns.countplot(df[column], hue=df[target_column])\n    plt.show()","74545a9b":"plot_category(df_train, 'Pclass', target_column)","95fb1a60":"plot_category(df_train, 'Sex', target_column)","844cf484":"plot_category(df_train, 'SibSp', target_column)","a9bd3a10":"plot_category(df_train, 'Parch', target_column)","ae6d3362":"plot_category(df_train, 'Embarked', target_column)","fe973dd4":"def plot_float(df, column, target_column, bins=20):\n    # \u5168\u4f53plot\n    sns.distplot(df[column], kde=True, rug=False, bins=bins)\n    plt.show()\n    \n    # \u76ee\u7684\u5909\u6570\u6bce\u306eplot\n    sns.distplot(df[df[target_column]==1][column], kde=True, rug=False, bins=bins, label=1)\n    sns.distplot(df[df[target_column]==0][column], kde=True, rug=False, bins=bins, label=0)\n    plt.legend()\n    plt.show()","bd6a2f27":"plot_float(df_train, \"Age\", \"Survived\")","dbf0aa22":"plot_float(df_train, \"Fare\", \"Survived\")","65ab5466":"#-----------\n# Age\n#-----------\ndef missing_value(df):\n    # \u6b20\u640d\u5024\u30d5\u30e9\u30b0\n    df[\"Age_na\"] = df[\"Age\"].isnull().astype(np.int64)\n    # \u6b20\u640d\u5024\u3092\u4e2d\u592e\u5024\u3067\u57cb\u3081\u308b\n    df[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\n\nmissing_value(df_train)  # train\u30c7\u30fc\u30bf\nmissing_value(df_test)    # test\u30c7\u30fc\u30bf","319e3e3c":"#-----------\n# Embarked\n#-----------\ndf_train[\"Embarked\"].fillna(\"S\", inplace=True)","c74a9d7c":"#-----------\n# Fare\n#-----------\ndf_test[\"Fare\"].fillna(df_test['Fare'].median(), inplace=True)","72956d44":"def normalization(df, name):\n    # \u6b63\u898f\u5316(\u4e0d\u504f\u5206\u6563)\n    df[name] = (df[name] - df[name].mean()) \/ df[name].std()\n\n#normalization(df_train, \"Age\")\n#normalization(df_train, \"Fare\")\n#normalization(df_test, \"Age\")\n#normalization(df_test, \"Fare\")","cb0fcbfb":"def dummy(df):\n    df = pd.get_dummies(df, columns=[\n        \"Pclass\", \n        \"Sex\", \n        #\"SibSp\",\n        #\"Parch\",\n        \"Embarked\",\n    ])\n    return df\n\ndf_train = dummy(df_train)\ndf_test = dummy(df_test)\n","a4d97702":"print(list(df_train.columns))","810ec3a6":"select_columns = [\n    \"Age\",\n    \"Age_na\",\n    \"SibSp\",\n    \"Parch\",\n    \"Fare\", \n    \"Pclass_1\",\n    \"Pclass_2\",\n    #\"Pclass_3\",  # dummy\u9664\u5916\n    \"Sex_male\",\n    #\"Sex_female\",  # dummy\u9664\u5916\n    \"Embarked_C\",\n    \"Embarked_Q\",\n    #\"Embarked_S\",  # dummy\u9664\u5916\n]","aaa84ee4":"import sklearn.ensemble\nimport sklearn.gaussian_process\nimport sklearn.naive_bayes\nimport sklearn.linear_model\nimport sklearn.neighbors\nimport sklearn.tree\nimport sklearn.discriminant_analysis\nimport xgboost as xgb\nimport lightgbm as lgb\ndef create_models(random_seed):\n    models = [\n        #Ensemble Methods\n        sklearn.ensemble.AdaBoostClassifier(random_state=random_seed),\n        sklearn.ensemble.BaggingClassifier(random_state=random_seed),\n        sklearn.ensemble.ExtraTreesClassifier(random_state=random_seed),\n        sklearn.ensemble.GradientBoostingClassifier(random_state=random_seed),\n        sklearn.ensemble.RandomForestClassifier(random_state=random_seed),\n\n        #Gaussian Processes\n        sklearn.gaussian_process.GaussianProcessClassifier(random_state=random_seed),\n\n        #GLM\n        sklearn.linear_model.LogisticRegressionCV(random_state=random_seed),\n        sklearn.linear_model.RidgeClassifierCV(),\n\n        #Navies Bayes\n        sklearn.naive_bayes.BernoulliNB(),\n        sklearn.naive_bayes.GaussianNB(),\n\n        #Nearest Neighbor\n        sklearn.neighbors.KNeighborsClassifier(),\n\n        #Trees\n        sklearn.tree.DecisionTreeClassifier(random_state=random_seed),\n        sklearn.tree.ExtraTreeClassifier(random_state=random_seed),\n\n        #Discriminant Analysis\n        sklearn.discriminant_analysis.LinearDiscriminantAnalysis(),\n        sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n        #xgboost\n        xgb.XGBClassifier(eval_metric=\"logloss\", use_label_encoder=False, random_state=random_seed),\n\n        # light bgm\n        lgb.LGBMClassifier(random_state=random_seed),\n    ]\n    return models","cbcb91a9":"def fit(df, columns, target_column, random_seed):\n    # \u6559\u5e2b\u30c7\u30fc\u30bf\u3092\u4f5c\u6210\n    x = df[columns].to_numpy()\n    y = df[target_column].to_numpy()\n\n    # \u4ea4\u53c9\u691c\u8a3c\n    model_scores = {}\n    kf = sklearn.model_selection.KFold(n_splits=3, shuffle=True, random_state=random_seed)\n    for train_idx, true_idx in kf.split(x, y):\n        # \u5404\u5b66\u7fd2\u30c7\u30fc\u30bf\u3068\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\n        x_train = x[train_idx]\n        y_train = y[train_idx]\n        x_true = x[true_idx]\n        y_true = y[true_idx]\n\n        # \u5404\u30e2\u30c7\u30eb\u6bce\u306b\u5b66\u7fd2\n        for model in create_models(random_seed):\n            name = model.__class__.__name__\n            if name not in model_scores:\n                model_scores[name] = []\n            \n            # \u30e2\u30c7\u30eb\u306e\u5b66\u7fd2\u3068\u8a55\u4fa1\n            model.fit(x_train, y_train)\n            pred_y = model.predict(x_true)\n\n            # \u7d50\u679c\u3092\u8a55\u4fa1\n            model_scores[name].append((\n                sklearn.metrics.accuracy_score(y_true, pred_y),\n                sklearn.metrics.precision_score(y_true, pred_y),\n                sklearn.metrics.recall_score(y_true, pred_y),\n                sklearn.metrics.f1_score(y_true, pred_y),\n            ))\n\n    accs = []\n    for k, scores in model_scores.items():\n        scores = np.mean(scores, axis=0)\n\n        # \u30e2\u30c7\u30eb\u6bce\u306e\u5e73\u5747\n        print(\"\u6b63\u89e3\u7387 {:.3f}, \u9069\u5408\u7387 {:.3f}, \u518d\u73fe\u7387 {:.3f}, F\u5024 {:.3f} : {}\".format(\n            scores[0],\n            scores[1],\n            scores[2],\n            scores[3],\n            k,\n        ))\n        accs.append(scores)\n    \n    # \u5168\u30e2\u30c7\u30eb\u306e\u4e2d\u592e\u5024\n    accs = np.median(accs, axis=0)  # \u4e2d\u592e\u5024\n    print(\"\u6b63\u89e3\u7387 {:.3f}, \u9069\u5408\u7387 {:.3f}, \u518d\u73fe\u7387 {:.3f}, F\u5024 {:.3f}\".format(accs[0], accs[1], accs[2], accs[3]))","6b492bca":"fit(df_train, select_columns, target_column, random_seed)","9ea31038":"# \u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u4f5c\u6210\nx = df_train[select_columns].to_numpy()\ny = df_train[target_column].to_numpy()\n\n# \u51fa\u529b\u7528\u30c7\u30fc\u30bf\nx_test = df_test[select_columns].to_numpy()\n\n# \u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u3067\u5b66\u7fd2\u3057\u3001\u8a55\u4fa1\u3059\u308b\nmodel = sklearn.ensemble.RandomForestClassifier(random_state=random_seed)\nmodel.fit(x, y)\npred_y = model.predict(x_test)\n\n# \u63d0\u51fa\u7528\u306b\u30c7\u30fc\u30bf\u52a0\u5de5\noutput = pd.DataFrame({'PassengerId': df_test[\"PassengerId\"], 'Survived': pred_y})\noutput.to_csv(\"result.csv\", header=True, index=False)\nprint(\"Your submission was successfully saved!\")","fc931a9d":"def print_feature_importance(df, columns, target_column, random_seed):\n    x = df[columns]\n    y = df[target_column]\n\n    print(\"--- RandomForestClassifier\")\n    model = sklearn.ensemble.RandomForestClassifier(random_state=random_seed)\n    model.fit(x, y)\n    fti1 = model.feature_importances_\n    for i, column in enumerate(columns):\n        print('{:20s} : {:>.6f}'.format(column, fti1[i]))\n\n\n    print(\"--- XGBClassifier\")\n    model = xgb.XGBClassifier(eval_metric=\"logloss\", use_label_encoder=False)\n    model.fit(x, y)\n    fti2 = model.feature_importances_\n    for i, column in enumerate(columns):\n        print('{:20s} : {:>.6f}'.format(column, fti2[i]))\n\n\n    print(\"--- LGBMClassifier\")\n    model = lgb.LGBMClassifier(random_state=random_seed)\n    model.fit(x, y)\n    fti3 = model.feature_importances_   \n    for i, column in enumerate(columns):\n        print('{:20s} : {:>.2f}'.format(column, fti3[i]))\n\n    #--- \u7d50\u679c\u3092plot\n    fig = plt.figure()\n    ax1 = fig.add_subplot(3, 1, 1, title=\"RandomForestClassifier(Feature Importance)\")\n    ax2 = fig.add_subplot(3, 1, 2, title=\"XGBClassifier(Feature Importance)\")\n    ax3 = fig.add_subplot(3, 1, 3, title=\"LGBMClassifier(Feature Importance)\")\n    ax1.barh(columns, fti1)\n    ax2.barh(columns, fti2)\n    ax3.barh(columns, fti3)\n    fig.tight_layout()\n    plt.show()\nprint_feature_importance(df_train, select_columns, target_column, random_seed)","fff6dce0":"from statsmodels.stats.outliers_influence import OLSInfluence\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport statsmodels.api as sm\ndef print_statsmodels(df, columns, target_column):\n    # \u91cd\u56de\u5e30\u5206\u6790\n    X1_train = sm.add_constant(df[columns])\n    y = df[target_column]\n    model = sm.OLS(y, X1_train)\n    fitted = model.fit()\n\n    # summary\u898b\u65b9\u306e\u53c2\u8003\n    # https:\/\/self-development.info\/%E3%80%90%E5%88%9D%E5%BF%83%E8%80%85%E8%84%B1%E5%87%BA%E3%80%91statsmodels%E3%81%AB%E3%82%88%E3%82%8B%E9%87%8D%E5%9B%9E%E5%B8%B0%E5%88%86%E6%9E%90%E7%B5%90%E6%9E%9C%E3%81%AE%E8%A6%8B%E6%96%B9\/\n    #print('summary = \\n', fitted.summary())\n    \n    print(\"--- \u91cd\u56de\u5e30\u5206\u6790\u306e\u6c7a\u5b9a\u4fc2\u6570\")\n    for i, column in enumerate(columns):\n        print('\\t{:15s} : {:7.4f}(coef) {:5.1f}%(P>|t|)'.format(\n            column, \n            fitted.params[i+1],\n            fitted.pvalues[i]*100\n        ))\n    print(\"\")\n\n    # \u5404column\u306b\u304a\u3051\u308b\u30af\u30c3\u30af\u8ddd\u96e2\u3092\u3060\u3059\n    print(\"--- \u5916\u308c\u5024(cook_distance threshold:0.5)\")\n    for column in columns:\n        # \u5358\u56de\u5e30\u5206\u6790\n        X1_train = sm.add_constant(df[column])\n        model = sm.OLS(y, X1_train)\n        fitted = model.fit()\n\n        cook_distance, p_value = OLSInfluence(fitted).cooks_distance\n        kouho = np.where(cook_distance > 0.5)[0]\n        if len(kouho) == 0:\n            print(\"{:20s} cook_distance is 0(max: {:.4f})\".format(column, np.max(cook_distance)))\n        else:\n            for index in kouho:\n                print(\"{:20s} cook_distance: {}, index: {}\".format(column, cook_distance[index], index))\n\n    print(\"\")\n\n# \u5b9f\u884c\nprint_statsmodels(df_train, select_columns, target_column)","a7f81f58":"def print_correlation(df, columns):\n\n    # \u76f8\u95a2\u4fc2\u65701:1\n    print(\"--- \u76f8\u95a2\u4fc2\u65701:1 (threshold: 0.5)\")\n    cor = df[columns].corr()\n    count = 0\n    for i in range(len(columns)):\n        for j in range(i+1, len(columns)):\n            val = cor[columns[i]][j]\n            if abs(val) > 0.5:\n                print(\"{} {}: {:.2f}\".format(columns[i], columns[j], val))\n                count += 1\n    if count == 0:\n        print(\"empty\")\n    print(\"\")\n\n    # heatmap\n    plt.figure(figsize=(12,9))\n    sns.heatmap(df[columns].corr(), annot=True, vmax=1, vmin=-1, fmt='.1f', cmap='RdBu')\n    plt.show()\n\n\n    # \u76f8\u95a2\u4fc2\u65701:\u591a\n    # 5\u4ee5\u4e0a\u3060\u3068\u3042\u3084\u3057\u3044\u300110\u3060\u3068\u304b\u306a\u308a\u78ba\u5b9a\n    print(\"--- VIF(5\u4ee5\u4e0a\u3060\u3068\u602a\u3057\u3044)\")\n    vif = pd.DataFrame()\n    x = df[columns]\n    vif[\"VIF Factor\"] = [\n        variance_inflation_factor(x.values, i) for i in range(x.shape[1])\n    ]\n    vif[\"features\"] = columns\n    print(vif)\n    plt.barh(columns, vif[\"VIF Factor\"])\n    plt.vlines([5], 0, len(columns), \"blue\", linestyles='dashed')\n    plt.vlines([10], 0, len(columns), \"red\", linestyles='dashed')\n    plt.title(\"VIF\")\n    plt.tight_layout()\n    plt.show()\nprint_statsmodels(df_train, select_columns, target_column)","6ed3ae17":"def print_correlation(df, columns):\n\n    # \u76f8\u95a2\u4fc2\u65701:1\n    print(\"--- \u76f8\u95a2\u4fc2\u65701:1 (threshold: 0.5)\")\n    cor = df[columns].corr()\n    count = 0\n    for i in range(len(columns)):\n        for j in range(i+1, len(columns)):\n            val = cor[columns[i]][j]\n            if abs(val) > 0.5:\n                print(\"{} {}: {:.2f}\".format(columns[i], columns[j], val))\n                count += 1\n    if count == 0:\n        print(\"empty\")\n    print(\"\")\n\n    # heatmap\n    plt.figure(figsize=(12,9))\n    sns.heatmap(df[columns].corr(), annot=True, vmax=1, vmin=-1, fmt='.1f', cmap='RdBu')\n    plt.show()\n\n\n    # \u76f8\u95a2\u4fc2\u65701:\u591a\n    # 5\u4ee5\u4e0a\u3060\u3068\u3042\u3084\u3057\u3044\u300110\u3060\u3068\u304b\u306a\u308a\u78ba\u5b9a\n    print(\"--- VIF(5\u4ee5\u4e0a\u3060\u3068\u602a\u3057\u3044)\")\n    vif = pd.DataFrame()\n    x = df[columns]\n    vif[\"VIF Factor\"] = [\n        variance_inflation_factor(x.values, i) for i in range(x.shape[1])\n    ]\n    vif[\"features\"] = columns\n    print(vif)\n    plt.barh(columns, vif[\"VIF Factor\"])\n    plt.vlines([5], 0, len(columns), \"blue\", linestyles='dashed')\n    plt.vlines([10], 0, len(columns), \"red\", linestyles='dashed')\n    plt.title(\"VIF\")\n    plt.tight_layout()\n    plt.show()\n\n# \u5b9f\u884c\nprint_correlation(df_train, select_columns)","0afa77a0":"# 3.\u30ed\u30fc\u30ab\u30eb\u3067\u5b66\u7fd2","1a7f9644":"# 5.\u305d\u306e\u4ed6\u306e\u5206\u6790","f08094f0":"## 2-2.(\u6b63\u898f\u5316)","7de160fc":"## 1-1.\u30ab\u30e9\u30e0\u4e00\u89a7\u3068\u6b20\u640d\u5024\u306e\u78ba\u8a8d","ee3de125":"## 5-1.\u8aac\u660e\u5909\u6570\u306e\u5f71\u97ff\u5ea6","f9237128":"## 5-2.\u56de\u5e30\u5206\u6790","6e5f613f":"## 2-3.\u30c0\u30df\u30fc\u5316","90c596da":"## 2-4.\u8aac\u660e\u5909\u6570\u306e\u9078\u629e","83b39d83":"# 1.\u30c7\u30fc\u30bf\u78ba\u8a8d","7f4a8238":"# 2.\u5b66\u7fd2\u30c7\u30fc\u30bf\u306e\u4f5c\u6210\uff08\u524d\u51e6\u7406\uff09","d280648c":"## 5-3.\u8aac\u660e\u5909\u6570\u540c\u58eb\u306e\u76f8\u95a2","9b88a14a":"## 1-4.\u5404\u30ab\u30e9\u30e0\u3068\u76ee\u7684\u5909\u6570\u3068\u306e\u95a2\u4fc2","b112193d":"# 4.\u63d0\u51fa\u30c7\u30fc\u30bf\u4f5c\u6210","dc78ed39":"1-2.\u5404\u30ab\u30e9\u30e0\u306e\u8981\u7d04\u60c5\u5831\u306e\u78ba\u8a8d","081b1519":"## 2-1.\u6b20\u640d\u5024\u306e\u88dc\u586b"}}