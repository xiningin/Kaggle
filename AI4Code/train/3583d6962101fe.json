{"cell_type":{"ddeeeef4":"code","03916647":"code","92cdd985":"code","9a73bd35":"code","e70d29b1":"code","80de4f9b":"code","cf001240":"code","0cd17712":"code","4e9f3c4e":"code","39680087":"code","0c432580":"code","9f7150ad":"code","caa641a2":"code","127b6077":"code","dd5a3d11":"code","339da113":"code","4818b4d4":"code","6e088fe6":"code","c8858513":"code","e8238751":"code","809c003e":"code","2b580ac0":"code","fd81b046":"code","24e6ef0d":"code","008b33d5":"code","484bd942":"code","611c8f2b":"code","f53bf5a0":"markdown","38a0d05f":"markdown","0072e4b8":"markdown","3d1dfdae":"markdown","a98e7dd8":"markdown","db2448f3":"markdown","5da285b1":"markdown","a40b04a0":"markdown","8ce4b35f":"markdown","85d7afe4":"markdown","39cc115a":"markdown","aedb7d06":"markdown","f0eb73ac":"markdown","b9553a13":"markdown"},"source":{"ddeeeef4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","03916647":"#import modules\nfrom sklearn.metrics import r2_score\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\n\nimport os\nimport glob\n\nfrom multiprocessing import Pool\n\nfrom pandarallel import pandarallel\npandarallel.initialize()\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\n\nimport time\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\npath_to_files = \"..\/input\/optiver-realized-volatility-prediction\"\n\nbook_train_files =  path_to_files + '\/book_train.parquet\/stock_id={}'\ntrade_train_files =  path_to_files + '\/trade_train.parquet\/stock_id={}'\n\nbook_test_files =  path_to_files + '\/book_test.parquet\/stock_id={}'\ntrade_test_files =  path_to_files + '\/trade_test.parquet\/stock_id={}'\n\nSMALL_F = 0.00000001\n\ntf.random.set_seed(111)\nnp.random.seed(111)","92cdd985":"#Configuration\ncfg = dict(\n    isCollectDataOnly = True,\n    isStockIdUsed = False,\n    isTFModelUsed = False,\n    trainNotUsedCols = ['row_id', 'target', 'time_id', 'stock_id'],\n    predictNotUsedCols = ['row_id', 'time_id', 'stock_id'],\n    useHyperOpt = False,\n    useLabelTransformation = False,\n    volumeBarThreshold = 500.0\n)\n\n\ncfg","9a73bd35":"# Function to count unique elements of a series\ndef count_unique(series):\n    return len(np.unique(series))\n\ndef log_return(prob):\n    prob += SMALL_F\n    return np.log(prob).diff()\n\ndef log_return2(x):\n    return np.log1p(x.pct_change())\n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\ndef rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n\n\ndef getOscStoch(x):\n    return (x[-1] - np.min(x))\/(np.max(x)-np.min(x)+SMALL_F)\n\ndef getNormVal(x):\n    return (x[-1])\/(np.mean(x)+SMALL_F)\n\ndef getBreath(ret, size):\n    f = ret > 0\n    upside = size[f]\n    downside = size[~f]\n    \n    di = np.sum(upside)\/(np.sum(downside)+SMALL_F)\n    ado = np.sum(upside) - np.sum(downside)\n    \n    return di, ado\n    \ndef rateLastFirst(x):\n    return np.mean(x)\/(np.sum(x)+SMALL_F)\n\ndef rolling_windows_vectorized(array, sub_window_size=2):\n    start = 0\n    max_time = len(array)-sub_window_size\n    sub_windows = (\n        start +\n        # expand_dims are used to convert a 1D array to 2D array.\n        np.expand_dims(np.arange(sub_window_size), 0) +\n        np.expand_dims(np.arange(max_time + 1), 0).T\n    )\n    \n    return array[sub_windows]\n\ndef rolling_windows_vectorized_v2(array, sub_window_size, stride_size):\n    start = 0\n    max_time = len(array)-sub_window_size\n    sub_windows = (\n        start + \n        np.expand_dims(np.arange(sub_window_size), 0) +\n        # Create a rightmost vector as [0, V, 2V, ...].\n        np.expand_dims(np.arange(max_time + 1, step=stride_size), 0).T\n    )\n    \n    return array[sub_windows]    \n\ndef getVolumaBars(i_data, threshold=1000.0):\n    o, h, l, c, v  = 0.0, 0.0, 1000000.0, 0.0, 0.0\n    res_array = []\n    isNewBar = True\n    bar_index = 0.0\n    cum_volume = 0.0\n    data_len = i_data.shape[0]\n    for i in range(data_len):\n        #print(i_data[i])\n        cur_price = i_data[i][0]\n        #print(cur_price)\n        if True == isNewBar:\n            bar_index = i\n            o = cur_price\n            c = 0.0\n            h = 0.0\n            l = 10000000.0\n            v = 0.0\n            \n            isNewBar = False\n\n        if cur_price > h:\n            h = cur_price\n        if cur_price < l:\n            l = cur_price\n\n        v += i_data[i][1]\n        \n        if (v >= threshold) or (i == data_len-1):\n            isNewBar = True\n            #bar_index = i\n            c = cur_price\n            res_array.append([bar_index, o, h, l, c, v])\n\n\n    return pd.DataFrame(res_array, columns=['bar_index', 'open', 'high', 'low', 'close', 'volume'])\n\n\ndef _get_beta(high, low, window):\n\n    beta_r = np.empty(high.shape)\n    beta_r[:] = np.NaN\n\n    ret = np.log(high \/ low)\n    high_low_ret = ret ** 2\n    beta = rolling_windows_vectorized(high_low_ret, 2).sum(axis=1)\n    beta = rolling_windows_vectorized(beta, window).mean(axis=1)\n    beta_r[len(beta_r)-len(beta):] = beta    \n\n    return beta_r\n\n\ndef _get_gamma(high, low):\n    gamma_r = np.empty(high.shape)\n    gamma_r[:] = np.NaN\n\n    high_max = rolling_windows_vectorized(high, 2).max(axis=1)\n    low_min = rolling_windows_vectorized(low, 2).min(axis=1)\n    gamma = np.log(high_max \/ low_min) ** 2\n    gamma_r[len(gamma_r)-len(gamma):] = gamma\n\n    return gamma_r    \n\ndef get_bekker_parkinson_vol2(high, low, window: int = 20):\n\n    beta = _get_beta(high, low, window)\n    gamma = _get_gamma(high, low)\n\n    k2 = (8 \/ np.pi) ** 0.5\n    den = 3 - 2 * 2 ** .5\n    sigma = (2 ** -0.5 - 1) * beta ** 0.5 \/ (k2 * den)\n    sigma += (gamma \/ (k2 ** 2 * den)) ** 0.5\n    sigma[sigma < 0] = 0\n\n    return sigma\n\ndef get_garman_class_vol2(open, high, low, close, window):\n    ret_value = np.empty(high.shape)\n    ret_value[:] = np.NaN\n    ret = np.log(high \/ low)  # High\/Low return\n    close_open_ret = np.log(close \/ open)  # Close\/Open return\n    estimator = 0.5 * ret ** 2 - (2 * np.log(2) - 1) * close_open_ret ** 2\n    ret_v = rolling_windows_vectorized(estimator, window).mean(axis=1)\n    ret_value[len(ret_value)-len(ret_v):] = np.sqrt(ret_v)\n    return ret_value\n\ndef getWindows(n_bars, isSpecialWindow=False, min_bar_length=3):\n    if(True==isSpecialWindow):\n        window_size = n_bars\/\/2\n    else:\n        window_size = n_bars-(min_bar_length-1)\n\n    if(window_size<=0):\n        if(n_bars-1 > 0):\n            window_size = n_bars - 1\n        else:\n            window_size=1\n\n    return window_size\n\ndef getMicrostructuralFeatures(input_df, output_df, col_prefix = '', col_postfix = '', min_bar_length = 3, volumeThreshold = cfg['volumeBarThreshold'], isParkinson=True, isGarman=True, isYyang=True, isBekker=True, isMicro=True, isSpecialWindow=False, micro_cols=[]):\n    \n    v_sum = np.sum(input_df.loc[:, 'size']).astype(np.float64)\n    thres = volumeThreshold if v_sum >= volumeThreshold else v_sum\n    volume_bars = getVolumaBars(input_df.loc[:,['price','size']].to_numpy(), thres)\n\n    window_size = getWindows(len(volume_bars), isSpecialWindow=isSpecialWindow, min_bar_length=min_bar_length)\n\n    if(True == isGarman):\n        col_name = col_prefix+'garman_class_vol'+col_postfix\n        pv = get_garman_class_vol2(volume_bars.open.to_numpy(), volume_bars.high.to_numpy(), volume_bars.low.to_numpy(), volume_bars.close.to_numpy(), window=window_size)\n        pv = pv[~np.isnan(pv)]\n        if(len(pv)>0):\n            output_df.loc[:,col_name] = np.median(pv)\n        else:\n            output_df.loc[:,col_name] = 0.0\n\n    if(True == isBekker):\n        col_name = col_prefix+'bekker_parkinson_vol'+col_postfix\n        pv = get_bekker_parkinson_vol2(volume_bars.high.to_numpy(), volume_bars.low.to_numpy(), window=window_size)\n        pv = pv[~np.isnan(pv)]\n        if(len(pv)>0):\n            output_df.loc[:,col_name] = np.median(pv)\n        else:\n            output_df.loc[:,col_name] = 0.0\n\n    return output_df\n","e70d29b1":"def getDataFromBidAsk_numpy(df, ci):\n    a = 0\n    b = 0\n    spread  = {}\n    for k in [1,2]:\n        #k = i+1\n        bidp = 'bid_price{}'.format(k)\n        askp = 'ask_price{}'.format(k)\n        bids = 'bid_size{}'.format(k)\n        asks = 'ask_size{}'.format(k)\n        #calculate comulative wap\n        a += (df[:,ci[bidp]] * df[:,ci[asks]] + df[:,ci[askp]] * df[:,ci[bids]])\n        b += df[:,ci[bids]] + df[:,ci[asks]]\n\n        #wap 1 and 2\n        spread[f'fb_w_{k}'] = (df[:,ci[bidp]] * df[:,ci[asks]] + df[:,ci[askp]] * df[:,ci[bids]] ) \/ (df[:,ci[bids]] + df[:,ci[asks]] + SMALL_F)\n        spread[f'fb_mid_point_{k}'] = (df[:,ci[askp]]) + (df[:,ci[bidp]]) \/ 2\n        spread[f'fb_volume_total_{k}'] = (df[:,ci[asks]]) + (df[:,ci[bids]])\n\n    \n    # mean wap\n    spread['fb_w'] = (a\/(b+SMALL_F))\n    # rates\n    spread['fb_w_rate'] = (spread['fb_w_1']) \/ (spread['fb_w_2']+SMALL_F) \n    spread['fb_mid_point_rate'] = (spread['fb_mid_point_1']) \/ (spread['fb_mid_point_2']+SMALL_F)\n    #sum volume\n    spread['fb_volume_total'] = spread['fb_volume_total_1'] + spread['fb_volume_total_2']\n\n    \n    ################# test ##################\n    spread['ask_1'] = df[:,ci['ask_price1']]\n    spread['bid_1'] = df[:,ci['bid_price1']]\n    spread['ask_2'] = df[:,ci['ask_price2']]\n    spread['bid_2'] = df[:,ci['bid_price2']]\n    #########################################\n    \n    return spread\n\n\n\ndef Fx(group, stock_id=0, n=10):\n    new_df = pd.DataFrame()\n    name = int(group.time_id.unique()[0])\n    tmp = pd.DataFrame()\n\n    #calculate log return from the following features:\n    cols = [\n        'fb_w', \n        'fb_w_1', \n        'fb_w_2',\n        'fb_mid_point_1',\n        'fb_mid_point_rate',\n        'fb_w_rate',\n    ]\n\n    new_cols = [s + '_lr' for s in cols]\n    group.loc[:,new_cols] = log_return2(group[cols]).to_numpy()\n    group = group[~group['fb_w'].isnull()]\n\n    #calculate realized volatility\n    cols = new_cols\n    new_cols = [s + '_vola' for s in cols]\n    tmp = pd.concat([tmp, pd.DataFrame(realized_volatility(group.loc[:,cols]).to_numpy().reshape(1,-1), columns=new_cols)], axis=1)\n\n    #calculate sum of log return\n    cols = [\n        'fb_w_2_lr',\n    ]\n    new_cols = [s + '_sum' for s in cols]\n    tmp = pd.concat([tmp, pd.DataFrame(np.sum(group.loc[:,cols]).to_numpy().reshape(1,-1), columns=new_cols)], axis=1)\n\n    #calculate sqsum    \n    cols = [\n        'fb_w_lr', \n        'fb_w_1_lr', \n        'fb_w_2_lr',\n        'fb_mid_point_1_lr',\n    ]\n    new_cols = [s + '_sqsum' for s in cols]\n    tmp = pd.concat([tmp, pd.DataFrame(np.sum((group.loc[:,cols])**2).to_numpy().reshape(1,-1), columns=new_cols)], axis=1)\n    \n    #calculate book length\n    tmp.loc[:,'book_length'] = [group.shape[0]]\n\n    #calclulate market microstructural features\n    c = '1'\n    cols_1 = []\n    new_df = pd.DataFrame({'price': group.loc[:,'fb_mid_point_'+c].to_numpy().flatten(), 'size': group.loc[:,'fb_volume_total_'+c].to_numpy().flatten()}).reset_index()\n    tmp = getMicrostructuralFeatures(new_df, tmp, col_prefix = 'fb_', col_postfix = '_'+c, micro_cols=cols_1)\n    c = '2'\n    cols_2 = []  \n    new_df = pd.DataFrame({'price': group.loc[:,'fb_mid_point_'+c].to_numpy().flatten(), 'size': group.loc[:,'fb_volume_total_'+c].to_numpy().flatten()}).reset_index()\n    tmp = getMicrostructuralFeatures(new_df, tmp, col_prefix = 'fb_', col_postfix = '_'+c, micro_cols=cols_2)\n    \n    ############ test idea ################\n    col_name = \"Test_1\"\n    window_size = getWindows(len(group), isSpecialWindow=False, min_bar_length=3)\n    pv = get_bekker_parkinson_vol2(group.ask_1.to_numpy(), group.bid_1.to_numpy(), window=window_size)\n    pv = pv[~np.isnan(pv)]\n    if(len(pv)>0):\n        tmp.loc[:,col_name] = np.median(pv)\n    else:\n        tmp.loc[:,col_name] = 0.0\n\n    col_name = \"Test_2\"\n    window_size = getWindows(len(group), isSpecialWindow=False, min_bar_length=3)\n    pv = get_bekker_parkinson_vol2(group.ask_2.to_numpy(), group.bid_2.to_numpy(), window=window_size)\n    pv = pv[~np.isnan(pv)]\n    if(len(pv)>0):\n        tmp.loc[:,col_name] = np.median(pv)\n    else:\n        tmp.loc[:,col_name] = 0.0\n    #######################################\n    \n    tmp.loc[:,'row_id'] = str(stock_id) + '-' + str(name)\n    tmp.loc[:,'time_id'] = int(name)\n    return tmp\n\ndef getFeaturesFromBookData(df, stock_id, n=10):\n    results = df.groupby(['time_id']).parallel_apply(Fx, stock_id=stock_id, n=n).reset_index(drop=True)\n    return results","80de4f9b":"def getDataFromTrade(df):\n    log_ret = log_return(df.price).dropna()\n    rz_vol = realized_volatility(log_ret)\n    \n    tmp = pd.DataFrame()\n\n    \n    tmp.loc[:,'p_vwap_my'] = [np.sum(df['price'].values*df['size'].values)\/(np.sum(df['size'].values+SMALL_F))]\n    tmp.loc[:,'p_rz_vol'] = rz_vol\n    tmp.loc[:,'p_sqsum'] = np.sum(log_ret**2)\n    tmp.loc[:,'p_sum'] = np.sum(log_ret)\n    \n    tmp.loc[:,'p_lr_rate'] = rateLastFirst(log_ret)\n    \n    tmp.loc[:,'p_price_count'] = count_unique(df['price'].to_numpy())\n    tmp.loc[:,'p_sec_count'] = count_unique(df['seconds_in_bucket'].to_numpy())\n\n    cols_p = []  \n    tmp = getMicrostructuralFeatures(df.loc[:, ['price', 'size']], tmp, col_prefix = 'p_', col_postfix = '', isParkinson=False, isGarman=False, isYyang=False, micro_cols=cols_p)\n\n    tmp.loc[:,'p_size_mean'] = np.mean(df['size']) \n\n    time_id = df.time_id.unique()[0]\n    tmp.loc[:,'time_id'] = time_id\n    return tmp\n\ndef getFeaturesFromTradeData(df):\n    return df.groupby(['time_id']).parallel_apply(getDataFromTrade).reset_index(drop=True)","cf001240":"def constructPreprocessedDataFrame(file_path, isTrain):\n    stock_id = file_path.split('=')[1]\n    df_book_data = pd.read_parquet(file_path)\n    if True == isTrain:\n        df_trade_data =  pd.read_parquet(trade_train_files.format(stock_id))\n    else:\n        df_trade_data =  pd.read_parquet(trade_test_files.format(stock_id))\n\n    print('Processing stock id:', stock_id)\n    #display(df_book_data.time_id.unique())\n    #preprocess book\n    a = time.time()\n    spread = getDataFromBidAsk_numpy(df_book_data.to_numpy(),{k: v for v, k in enumerate(df_book_data.columns.values)})\n    df_book_data = pd.concat([df_book_data,pd.DataFrame(spread)], axis=1)\n    df_book_datar = getFeaturesFromBookData(df_book_data, stock_id, 10)\n    b = time.time()\n    #print(f'preprocess book: {b-a}')\n    \n    #preprocess trade\n    df_trade_datar = getFeaturesFromTradeData(df_trade_data)\n    df_book_datar = df_book_datar.merge(df_trade_datar, on = ['time_id'], how = 'left')\n    c = time.time()\n    #print(f'preprocess trade: {c-b}')\n\n    df_book_datar.loc[:,'stock_id'] = stock_id\n    df_book_datar = df_book_datar.fillna(0.0)\n    return df_book_datar\n\ndef constructBookDataDataFrame(list_file, isTrain=True):\n    df_book = pd.DataFrame()\n    for file in list_file:\n        df_book = pd.concat([df_book, constructPreprocessedDataFrame(file, isTrain=isTrain)])\n    return df_book","0cd17712":"list_order_book_file_train = glob.glob(path_to_files + '\/book_train.parquet\/*')\nlist_order_book_file_train[0:1]","4e9f3c4e":"%%time\nret_df = constructBookDataDataFrame(list_order_book_file_train)\ndisplay(ret_df.shape)\nret_df.head()","39680087":"%%time\ndef getDataFromTransformedDataFx(df, prefix=''):\n    cs = ['row_id', 'stock_id', 'time_id']\n    used_cols = list(set(df.columns.to_list()) - set(cs))\n    for c in used_cols:\n        df.loc[:, prefix+c+'_rate'] = df.loc[:,c] \/ (np.mean(df.loc[:,c]) + SMALL_F)\n        df.loc[:, prefix+c+'_diff'] = df.loc[:,c] - (np.mean(df.loc[:,c]) )\n\n    return df\n\ndef getDataFromTransformedData(df):\n    tmp1 = df.groupby(['stock_id']).parallel_apply(getDataFromTransformedDataFx, prefix='stock_id_').reset_index(drop=True)\n    cols_tmp1 = [x for x in tmp1.columns if 'stock_id_' in x]\n    cols_tmp1.append('stock_id')\n    cols_tmp1.append('time_id')\n    tmp2 = df.groupby(['time_id']).parallel_apply(getDataFromTransformedDataFx, prefix='time_id_').reset_index(drop=True)\n    cols_tmp2 = [x for x in tmp2.columns if 'time_id_' in x]\n    cols_tmp2.append('stock_id')\n    cols_tmp2.append('time_id')\n\n    df = df.merge(tmp1.loc[:,cols_tmp1], on = ['stock_id', 'time_id'], how = 'left')\n    df = df.merge(tmp2.loc[:,cols_tmp2], on = ['stock_id', 'time_id'], how = 'left')\n    \n    print(df.shape)\n\n    return df\n\nret_df = getDataFromTransformedData(ret_df)\nret_df.head()","0c432580":"cs = ['Unnamed: 0', 'row_id', 'stock_id', 'time_id']\nused_cols = list(set(ret_df.columns.to_list()) - set(cs))\ny_col = 'target'\n\nALL_STOCKS = {k: v for v, k in enumerate(ret_df.stock_id.unique())}","9f7150ad":"_used_cols = ['fb_w_1_lr_sqsum',\n 'fb_w_1_lr_vola',\n 'fb_w_2_lr_sqsum',\n 'p_rz_vol',\n 'fb_w_lr_vola',\n 'fb_bekker_parkinson_vol_2',\n 'fb_w_2_lr_vola',\n 'time_id_p_lr_rate_diff',\n 'fb_bekker_parkinson_vol_1',\n 'time_id_p_sec_count_diff',\n 'book_length',\n 'time_id_fb_bekker_parkinson_vol_1_diff',\n 'p_price_count',\n 'stock_id_fb_w_1_lr_vola_rate',\n 'time_id_fb_bekker_parkinson_vol_2_diff',\n 'stock_id_book_length_diff',\n 'fb_w_rate_lr_vola',\n 'p_sqsum',\n 'p_lr_rate',\n 'p_bekker_parkinson_vol',\n 'p_sec_count',\n #'time_id_fb_avg_tick_size_2_diff',\n 'stock_id_fb_w_1_lr_vola_diff',\n 'stock_id_p_sqsum_diff',\n 'time_id_fb_w_1_lr_vola_rate',\n 'stock_id_fb_w_rate_lr_vola_rate',\n 'time_id_fb_bekker_parkinson_vol_1_rate',\n 'time_id_p_sqsum_diff',\n 'time_id_book_length_rate',\n 'time_id_book_length_diff',\n 'time_id_fb_bekker_parkinson_vol_2_rate',\n 'time_id_p_bekker_parkinson_vol_diff',\n 'fb_garman_class_vol_1',\n 'fb_mid_point_rate_lr_vola',\n'time_id_p_lr_rate_rate',\n 'stock_id_fb_w_lr_vola_rate',\n 'stock_id_fb_mid_point_1_lr_sqsum_diff',\n 'time_id_fb_w_1_lr_vola_diff',\n 'stock_id_fb_bekker_parkinson_vol_2_diff',\n 'stock_id_fb_mid_point_rate_lr_vola_diff',\n 'time_id_fb_w_lr_vola_diff',\n 'stock_id_fb_mid_point_1_lr_vola_diff',\n 'stock_id_fb_bekker_parkinson_vol_1_diff',\n 'stock_id_fb_w_2_lr_sum_diff',\n 'time_id_fb_w_lr_sqsum_rate',\n 'stock_id_fb_mid_point_1_lr_vola_rate',\n 'stock_id_fb_garman_class_vol_1_rate',\n #'time_id_fb_avg_tick_size_1_diff',\n 'stock_id_fb_w_rate_lr_vola_diff',\n 'time_id_fb_garman_class_vol_2_diff',\n 'stock_id_fb_w_lr_vola_diff',\n 'stock_id_fb_mid_point_rate_lr_vola_rate',\n             \n ###########################################\n 'stock_id_Test_1_diff',\n 'stock_id_Test_1_rate',\n 'Test_1',\n 'time_id_Test_1_diff',\n 'time_id_Test_1_rate',\n 'stock_id_Test_2_diff',\n 'stock_id_Test_2_rate',\n 'Test_2',\n 'time_id_Test_2_diff',\n 'time_id_Test_2_rate'\n##########################################             \n ]","caa641a2":"#list(set(used_cols2) - set(used_cols))","127b6077":"ret_df.loc[:, used_cols].head()","dd5a3d11":"train = pd.read_csv(path_to_files + '\/train.csv')\ntrain.head()","339da113":"scaler_target = MinMaxScaler()\ndef getTrainData(ret_df, seed = 42):\n    train = pd.read_csv(path_to_files + '\/train.csv')\n    #convert stock_id to the same time as in train data\n    ret_df.stock_id = ret_df.stock_id.astype(int)\n    #merge\n    data_df = ret_df.merge(train, on = ['stock_id', 'time_id'], how = 'left')\n    data_df.loc[:,'target_orig'] = data_df.loc[:,'target'] \n\n    if True == cfg['useLabelTransformation']:\n        data_df.loc[:,'target'] = data_df.loc[:,'target'] * 100\n        scaler_target.fit(data_df.loc[:,'target'].to_numpy().reshape(-1,1))\n        data_df.loc[:,'target'] = scaler_target.transform(data_df.loc[:,'target'].to_numpy().reshape(-1,1)).flatten()\n\n    #get train test index \n    all_time_ids = data_df.time_id.unique()\n\n    train_ids, val_ids = train_test_split(all_time_ids, test_size=0.05, random_state=seed)\n    test_ids, val_ids = train_test_split(val_ids, test_size=0.5, random_state=seed)\n\n    f = data_df.time_id.isin(train_ids)\n    train_df = data_df.loc[f].reset_index(drop=True)\n\n    f = data_df.time_id.isin(val_ids)\n    val_df = data_df.loc[f].reset_index(drop=True)\n\n    f = data_df.time_id.isin(test_ids)\n    test_df = data_df.loc[f].reset_index(drop=True)    \n    \n    return train_df, val_df, test_df\n\ndef predictFromModel(model, df, used_cols=used_cols, prediction_column_name='target'):\n    predict = model.predict(df.loc[:, used_cols].values).flatten()\n    df_ret = pd.DataFrame()\n    df_ret[prediction_column_name] = predict\n    df_ret['row_id'] = df['row_id'].values\n    return df_ret[['row_id', prediction_column_name]].reset_index(drop=True)","4818b4d4":"from sklearn.model_selection import train_test_split\nimport lightgbm as lgbm\n\ndef feval_RMSPE(preds, train_data):\n    labels = train_data.get_label()\n    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False\n\ndef printAndReturnModelDescrAndErrors(model, model_id, y_test, predict_y):\n    R2 = round(r2_score(y_true = y_test, y_pred = predict_y),5)\n    RMSPE = round(rmspe(y_true = y_test, y_pred = predict_y),5)\n    print(f'Model {model_id} Performance of the prediction: R2 score: {R2}, RMSPE: {RMSPE}')\n    \n    return  {'model':model, 'R2':R2, 'RMSPE':RMSPE}\n\ndef getTrainDats(df, df_target, test_size):\n    df_target['row_id'] = df_target['stock_id'].astype(str) + '-' + df_target['time_id'].astype(str)\n    df_joined = df.merge(df_target[['row_id','target']], on = ['row_id'], how = 'left')\n    \n    cols_no_used = cfg['trainNotUsedCols']\n    \n    X_train, X_test, y_train, y_test = train_test_split(df_joined.loc[:, ~df_joined.columns.isin(cols_no_used)], \n                                                        df_joined.target,\n                                                        test_size=test_size, random_state=42)\n    return X_train, X_test, y_train, y_test\n\ndef train_model_lgbm(df_train, df_val, df_test, X_col, y_col, cat_feats, model_id):\n    \n    for c in cat_feats:\n        df_train[c] = df_train[c].astype(int)\n        df_val[c] = df_val[c].astype(int)\n        df_test[c] = df_test[c].astype(int)\n    #print(X_train.columns)\n    train = lgbm.Dataset(df_train.loc[:,X_col], label=df_train.loc[:,y_col], categorical_feature=cat_feats, weight=1\/np.power(df_train.loc[:,y_col],2))\n    val = lgbm.Dataset(df_val.loc[:,X_col], label=df_val.loc[:,y_col], categorical_feature=cat_feats, weight=1\/np.power(df_val.loc[:,y_col],2))\n    test = lgbm.Dataset(df_test.loc[:,X_col], label=df_test.loc[:,y_col], categorical_feature=cat_feats, weight=1\/np.power(df_test.loc[:,y_col],2))\n    \n\n    lgbm_params = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'learning_rate': 0.15, #0.01,\n        'objective': 'regression',\n        'metric': 'None',\n        'max_depth': -1,\n        'n_jobs': -1,\n        'feature_fraction': 0.7,\n        'bagging_fraction': 0.75,\n         'lambda_l1': 1, # L1 regularization\n         'lambda_l2': 1, # L2 Regularization\n         'bagging_seed': 100, # random seed, default in light 100\n        \n    }\n\n    model = lgbm.train(lgbm_params, \n                          train, \n                          50000, \n                          valid_sets=test, \n                          feval=feval_RMSPE,\n                          early_stopping_rounds=1500,\n                          verbose_eval=False,\n                          categorical_feature=cat_feats,\n                         )\n\n    predict_y = model.predict(df_test.loc[:,X_col])\n    \n    model_descr = printAndReturnModelDescrAndErrors(model, model_id, df_test.loc[:,y_col], predict_y)\n\n    return model_descr\n","6e088fe6":"def trainModel(df, n_models=1, seeds=[]):\n    models_stat = {}\n    for i in range(n_models):\n        if len(seeds) == 0:\n            rnd_seed = np.random.randint(1000)\n        else:\n            rnd_seed = seeds[i]\n        \n        train_df, val_df, test_df = getTrainData(ret_df, seed=rnd_seed)\n        used_cols_lgbm = used_cols.copy()\n        #used_cols_lgbm.append('stock_id')\n        models_stat['model'+str(i)] = train_model_lgbm(train_df, val_df, test_df, used_cols_lgbm, 'target_orig', [], \"lgbm_model\"+str(i))\n    return models_stat","c8858513":"models = trainModel(ret_df, n_models=5, seeds=[42,456,555,333,444])","e8238751":"def predictFromModels(models_stat, df, used_cols=used_cols, weight=[]):\n    predictions = None\n    if(len(models_stat.keys()) == 1):\n        predictions = predictFromModel(models_stat['model0']['model'], df)\n    else:\n        isFirst=True\n        w = 1\/len(models_stat.keys())\n        for k in models_stat.keys():\n            if(isFirst==True):\n                isFirst=False\n                predictions = predictFromModel(models_stat[k]['model'], df)\n                predictions.loc[:,'target'] = predictions.loc[:,'target'] * w\n            else:\n                p = predictFromModel(models_stat[k]['model'], df).loc[:, 'target']\n                predictions.loc[:,'target'] += p * w\n                \n    return predictions","809c003e":"train_df, val_df, test_df = getTrainData(ret_df, seed=777)\ntt = predictFromModels(models, test_df)\npredict_y = tt.loc[:,'target']\ntest_y = test_df.loc[:,y_col]\n_ = printAndReturnModelDescrAndErrors(None, \"lgbms\", test_y, predict_y)","2b580ac0":"list_order_book_file_train = glob.glob(path_to_files + '\/book_test.parquet\/*')\nlist_order_book_file_train","fd81b046":"%%time\nt_df = constructBookDataDataFrame(list_order_book_file_train, isTrain=False)\nt_df = getDataFromTransformedData(t_df)\ndisplay(t_df.shape)\nt_df.head()","24e6ef0d":"tmp = predictFromModels(models, t_df)\n\ntmp.to_csv('submission.csv',index = False)\ntmp.head()","008b33d5":"#models\nfor k,v in models.items():\n    print(f\"{k} - R2: {v['R2']} RMSPE: {v['RMSPE']}\")\n    lgbm.plot_importance(v['model'], figsize=(10,25), importance_type='gain')","484bd942":"#models\nfor k,v in models.items():\n    print(f\"{k} - R2: {v['R2']} RMSPE: {v['RMSPE']}\")\n    lgbm.plot_importance(v['model'], figsize=(10,25), importance_type='split')","611c8f2b":"import shap\n\nseeds=[42,456,555,333,444]\nfor (k,v),s in zip(models.items(), seeds):\n    train_df, val_df, test_df = getTrainData(ret_df, seed=s)\n    X_train = train_df.loc[:,used_cols]\n    explainer = shap.TreeExplainer(v['model'])\n    shap_values = explainer.shap_values(X_train)\n    shap.summary_plot(shap_values, X_train, max_display=40)","f53bf5a0":"\nThere are two ideas behind this notebook:\n\n- not to use all these min,max,std... features, but instead of it try to find something that can be more interpreteble. I use only 60 features, but i think we can leave half of them without any significant drop. \n- elemenate stock_id from the inputs\n- added feature importance for all calculated features\n\n","38a0d05f":"## Prepare test data","0072e4b8":"## Train Models","3d1dfdae":"## Constract \\[stock_id,time_id\\] features\n","a98e7dd8":"## Model Definition","db2448f3":"## Test Prediction","5da285b1":"## Feature Importance","a40b04a0":"## Useful functions","8ce4b35f":"## Predict test","85d7afe4":"## construct features across dataset","39cc115a":"## collect and preprocess data","aedb7d06":"## Preprocessing trade data","f0eb73ac":"## SHAP Feature Importance","b9553a13":"## Preprocessing book data"}}