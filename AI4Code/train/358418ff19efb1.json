{"cell_type":{"c74b6bc3":"code","fb5c4643":"code","2d575266":"code","4ad3bee9":"code","366284d4":"code","4020e00e":"code","3f62f960":"code","4ff90fd9":"code","885610f4":"code","91daeab1":"code","d9e3468a":"code","1e7ad5a9":"code","cd5174f3":"code","751aa7e9":"code","fd1079f8":"code","e3952f9a":"code","1f365d11":"code","ae2d265b":"code","878908b4":"code","efcb7061":"code","ce41f367":"code","fc6e0337":"code","52855480":"code","2a48f310":"code","138640c7":"code","ff38c13a":"code","af21eab2":"code","3383f56f":"code","41d25ef6":"code","47893cdc":"code","6f1715dc":"code","3685f60d":"code","69a6188a":"code","cd2d2f82":"code","de50a1e9":"code","a15f9919":"code","e7ccd3a4":"code","cdbfa2a2":"code","709ae4e5":"code","49afec02":"code","c43502b0":"code","90c33678":"code","bbf0230a":"code","0f34b43c":"code","df82d34e":"code","11c36e33":"code","ef2d9e3b":"code","fb9efa3e":"code","a3dcb9a2":"code","ecdb50d2":"code","2edb5731":"code","5791d66f":"code","57b5eeaf":"code","0bb843ba":"code","a7fbee02":"code","be4c8222":"code","34e699c7":"code","b902e0ed":"code","d30c401b":"code","b6f0b6ca":"code","b9bb8871":"code","0b5d9660":"code","dae4ed8e":"code","55b2d185":"code","faf779c8":"code","57ae2d0e":"code","a6272bfd":"code","9afb8389":"code","291c0221":"code","f08431a8":"code","c334a959":"code","744f81a2":"code","9a4e63af":"code","afe60bff":"markdown","2badc2af":"markdown","1b7a7d30":"markdown","ce9cca14":"markdown","e10d7456":"markdown","e2ec0b4c":"markdown","afefdce1":"markdown","0df36473":"markdown","4cedf688":"markdown","8d32d00e":"markdown","18d37857":"markdown","6f13469f":"markdown","5066e567":"markdown","4b5d05d5":"markdown","d793dd6b":"markdown","20b003b6":"markdown","8aecc40f":"markdown","7fffdece":"markdown","ddc04e89":"markdown","c04d06bf":"markdown","b197aa14":"markdown","d3cb74f8":"markdown","d1a3a413":"markdown","e70ac60a":"markdown","4212ca4d":"markdown","f9d1d9d1":"markdown","55ef4e44":"markdown","7979e626":"markdown","047d7989":"markdown","65bc6340":"markdown","3922a186":"markdown","3e4430de":"markdown","7d892c61":"markdown","01cf3e9c":"markdown","b32b6085":"markdown","f3bca538":"markdown","a6f8c37e":"markdown","a5806089":"markdown","40ac9464":"markdown","690749a8":"markdown","2a12220a":"markdown","b4f556e9":"markdown","46f6ac97":"markdown","e12656b1":"markdown","752ef7aa":"markdown","deeead16":"markdown","c40b32d8":"markdown","d0d33ae2":"markdown","cb4d8839":"markdown","36a7007a":"markdown"},"source":{"c74b6bc3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","fb5c4643":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.preprocessing import StandardScaler\n\n%matplotlib inline","2d575266":"stores = pd.read_csv('\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/stores.csv', dtype={\"Type\": \"category\"})\nfeatures = pd.read_csv('\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/features.csv.zip')\ntrain = pd.read_csv('\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/train.csv.zip')","4ad3bee9":"stores.head()","366284d4":"stores.info()","4020e00e":"features.head()","3f62f960":"features.info()","4ff90fd9":"train.head()","885610f4":"train.info()","91daeab1":"# Set Date as Datetime\ndef set_datetime_column(df):\n    df[\"Datetime\"] = pd.to_datetime(df.Date, format='%Y-%m-%d')\n    df.drop(\"Date\", axis=1, inplace=True)\n    df.rename(columns={\"Datetime\" : \"Date\"}, inplace=True)\n    return df\n\n# Split Date to Year, Month, Day\ndef split_datetime_info(df):\n    df[\"Year\"] = df.Date.dt.year\n    df[\"Month\"] = df.Date.dt.month\n    df[\"Day\"] = df.Date.dt.day\n    return df\n\n# Create Numerical Ordinal Type column\ndef create_num_ordinary_type_column(df):\n    di = {\"A\": 3, \"B\": 2, \"C\": 1}\n    df[\"TypeInt\"] = df.Type.map(di)\n    return df\n\n# Join Dataframes informations\ndef join_dataframe_columns(base_df, how=\"inner\"):\n    df = base_df.copy()\n    df = df.merge(right=stores, how=how, on=\"Store\")\n    df = df.merge(right=features.drop(\"IsHoliday\", axis=1), how=how, on=[\"Date\", \"Store\"])\n    return df\n\ndef set_information_lag(df, lag_range= 7):\n    for i in range(1, (lag_range + 1)):\n        df[\"lag_{}\".format(i)] = df.Weekly_Sales.shift(i)\n    return df\n\ndef get_train_test_size(df_len, test_size=0.3):\n    test = round(df_len * test_size)\n    train = df_len - test\n    return train, test\n\ndef get_train_test_ndarray(df, train_len):\n    train = df.iloc[0:train_len, :].values\n    test = df.iloc[train_len:, :].values\n    return train, test\n\ndef get_train_test_df(df, train_len):\n    train = df.iloc[0:train_len, :]\n    test = df.iloc[train_len:, :]\n    return train, test\n\ndef train_test_split_time_series(df, test_size=0.3, ndarrayType=True):\n    train_len, _ = get_train_test_size(len(df), test_size)\n    if ndarrayType == True:\n        return get_train_test_ndarray(df, train_len)\n    else:\n        return get_train_test_df(df, train_len)\n\ndef split_X_y_ndarray(ndarray):\n    X = ndarray[:, 2:]\n    y = ndarray[:, 1]\n    return X, y\n\ndef split_X_y_df(df):\n    try:\n        df = df.set_index(\"Date\")\n    except:\n        pass\n    X = df.drop(\"Weekly_Sales\",axis=1).iloc[:, 1:]\n    y = df[\"Weekly_Sales\"].values\n    return X, y\n\ndef rmse(y_test, y_hat):\n    result = mse(y_test, y_hat)**0.5\n    print(\"RMSE\", result)\n    return result\n\ndef add_sqr_foot_sales(df):\n    df[\"SquareFoot_Sales\"] = df.Weekly_Sales \/ df.Size\n    return df\n\ndef drop_sqr_foot_sales(df):\n    df.drop(columns=[\"SquareFoot_Sales\"], inplace=True)\n    \ndef drop_type(df):\n    df.drop(columns=[\"Type\"], inplace=True)\n    \ndef plot_lag(df, lag_num=0):\n    if (lag_num == 0):\n        plt.plot(df.Date, df.Weekly_Sales)\n    else:\n        plt.plot(df.Date, df[F\"lag_{lag_num}\"])\n        \ndef lag_graph(df, lag_range= 3):\n    plt.figure(figsize=(20, 10))\n    plot_lag(df_sales)\n    for i in range(1, (lag_range + 1)):\n        plot_lag(df_sales, i)\n        \ndef plot_prediction_result(df, y_test, y_hat):\n    plt.figure(figsize=(20, 7))\n    offset = len(df) - len(y_test)\n    plt.plot(df_sales.Date[offset:], y_test)\n    plt.plot(df_sales.Date[offset:], y_hat)\n    \ndef update_dataframe_lag(df, value, lag_range=7, offset=0):\n    for i in range(0, lag_range + 1):\n        try:\n            df.iloc[i + offset, i] = value\n        except:\n            pass   \n        \ndef predict_df_sales(df, model):\n    for i in range(len(df)):   \n        y_hat = model.predict([df_test.iloc[i, 1:].to_numpy()])\n        update_dataframe_lag(df_test, y_hat, offset=i)\n        \ndef plot_dow_boxplot(df, side: int, title: str):\n    sns.set(style=\"whitegrid\")\n    plt.figure(figsize=(10,3))\n    plt.title(title)\n    ax = sns.boxplot(x=df[\"Weekly_Sales\"])\n    ax.axis(xmin=4*10000000,xmax=6.5*10000000)\n    \ndef apply_dickey_fuller_stationary_test(df):\n    print('Results of the Dickey Fuller Test')\n    dftest = adfuller(x = df['Weekly_Sales'], autolag= 'AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    print(dfoutput)\n    for key,value in dftest[4].items():\n        print('Critical Value ({}) = {}'.format(key,value))\n\ndef remove_variance_df(df):\n    try:\n        df = df.set_index(\"Date\")\n    except:\n        pass\n    df_log = np.log(df) # Penalizes the positive heteroscedasticity\n    plt.figure(figsize=(18, 7))\n    plt.plot(df_log['Weekly_Sales'], linewidth = 3)\n    return df_log\n\ndef remove_tendency_df(df):\n    try:\n        df = df.set_index(\"Date\")\n    except:\n        pass\n    dfTimeShift = df.shift()\n    dfDiffShift = df - dfTimeShift \n    plt.figure(figsize=(18, 7))\n    plt.plot(dfDiffShift, linewidth = 3)\n    return dfDiffShift\n\ndef apply_holiday_progression(df, regression_len=4, _type=\"exp\", column_name='Holiday_Progression'):\n    holidays = df[df.IsHoliday == True].Date.unique()\n    df[column_name] = 0\n    days = df.Date.unique()\n    for date in holidays:\n        idx = np.where(days == date)[0][0]\n        try:\n            for i in range(0, regression_len):\n                df.loc[df.Date == days[idx - i], column_name] = regression_len - i\n        except:\n            pass\n    if (_type == \"exp\"):\n        df['Exp_' + column_name] = df[column_name] ** 2\n        df.drop(columns=[column_name], inplace=True)\n        df.rename(columns={F'Exp_{column_name}' : column_name}, inplace=True)\n        \n    return df\n\ndef drop_holiday_progression(df, column_name='Holiday_Progression'):\n    df.drop(columns=[column_name], inplace=True)\n\ndef plot_scatter(df, independent, dependent=\"Weekly_Sales\"):\n    plt.figure(figsize=(18, 10))\n    _ = df.groupby(independent)[dependent].sum().reset_index()\n    sns.scatterplot(_[independent], _[dependent], alpha=0.7)\n    del _\n    \ndef plot_hist(df, independent, dependent=\"Weekly_Sales\", bins=20):\n    plt.figure(figsize=(18, 10))\n    sns.distplot(df.groupby(independent)[dependent].sum().reset_index(), bins=bins)\n    \ndef drop_markdowns(df):\n    for i in range(1, 6):\n        df.drop(columns=F\"MarkDown{i}\", inplace=True)\n        \ndef set_prophet_requirements(df):\n    df = set_datetime_column(df)\n    df.rename(columns={\"Date\": \"ds\", \"Weekly_Sales\": \"y\"}, inplace=True)\n    #df = df.set_index('ds')\n    return df\n\ndef fill_markdowns(df, how=\"zero\"):\n    for i in range(1, 6):\n        if how == \"zero\":\n            df[F\"MarkDown{i}\"].fillna(0)\n        elif how == \"median\":\n            df[F\"MarkDown{i}\"].fillna(df[F\"MarkDown{i}\"].median())\n        elif how == \"mean\":\n            df[F\"MarkDown{i}\"].fillna(df[F\"MarkDown{i}\"].mean())\n    return df","d9e3468a":"stores.describe().T","1e7ad5a9":"features.describe().T","cd5174f3":"df = join_dataframe_columns(train)\ndf.head()","751aa7e9":"plt.figure(figsize=(15, 15))\ncorr = df.corr()\nsns.heatmap(corr, annot=True, cmap='Blues',\n        xticklabels=corr,\n        yticklabels=corr)","fd1079f8":"# How Store Type matters to Weekly Sales\n_type = df.groupby(\"Store\").Weekly_Sales.sum().reset_index().merge(stores, on=\"Store\")\n_type.head()\nplt.title(\"Type X Weekly_Sales\")\nplt.scatter(_type.Type, _type.Weekly_Sales, alpha=0.5)","e3952f9a":"# How Store Size matters to define Store's Type\nplt.title(\"Type X Size\")\nplt.scatter(_type.Type, _type.Size, alpha=0.5)","1f365d11":"# How Store Size matters to define Store's Type\nplt.figure(figsize=(17, 10))\nplt.title(\"Type X Log(Size)\")\nsns.violinplot(_type.Type, np.log(_type.Size))\ndel _type","ae2d265b":"# How Size matters to Weekly Sales\nplt.figure(figsize=(17, 10))\n\n_ = df.groupby('Size').Weekly_Sales.median().reset_index()\n\nm, b = np.polyfit(np.log(_.Size),_.Weekly_Sales, 1) # slope and intercept of best fit line\nplt.plot(np.log(_.Size), m*np.log(_.Size) + b, color=\"g\") # plot best fit line\n\nplt.scatter(np.log(_.Size), _.Weekly_Sales) # plot scatter","878908b4":"# Sales per square foot is a good indicator of sales performance?\n\ndf = add_sqr_foot_sales(df)\n_sf_sales = df.groupby(['Date', 'Store']).SquareFoot_Sales.sum().reset_index()\n_sf_sales = _sf_sales.merge(df.groupby(['Date', 'Store']).Weekly_Sales.sum().reset_index(), on=[\"Store\", \"Date\"])\n_sf_sales.head()\nplt.scatter(_sf_sales.SquareFoot_Sales, _sf_sales.Weekly_Sales, alpha=0.7)\ndrop_sqr_foot_sales(df)\ndel _sf_sales, _","efcb7061":"# Plotting ACF (Auto Correlation Function) test result, to determinate Lags \n\n_temp = df.groupby([\"Date\"]).Weekly_Sales.sum().reset_index()\n_temp = set_datetime_column(_temp)\n_temp = _temp.set_index('Date')\n\nfig, ax = plt.subplots(figsize=(15,10)) # Increase plot size\nfig = sm.graphics.tsa.plot_acf(_temp.values.squeeze(), lags=60, ax=ax) # shows ACF test result\nax.set_xticks(range(0,60, 2)) # change X axis ticks to show every 2 numbers\nfig.show() # show figure","ce41f367":"# Plotting PACF (Auto Correlation Function) test result, to determinate Sazonal Lags \n\nfig, ax = plt.subplots(figsize=(15,10)) # Increase plot size\nfig = sm.graphics.tsa.plot_pacf(_temp.values.squeeze(), lags=60, ax=ax) # shows ACF test result\nax.set_xticks(range(0,60, 2)) # change X axis ticks to show every 2 numbers\nfig.show() # show figure\n","fc6e0337":"_temp = set_information_lag(_temp, 60)\nplt.figure(figsize=(15, 15))\nsns.heatmap(_temp.corr(), cmap=\"Blues\"); # Manual ACF\ndel _temp","52855480":"features.info()","2a48f310":"_holi = df[df.IsHoliday == True].groupby([\"Date\"]).Weekly_Sales.sum().reset_index()\n_common = df[df.IsHoliday == False].groupby([\"Date\"]).Weekly_Sales.sum().reset_index()","138640c7":"# Plotting Holiday Weekly_Sales agains Common Days Weekly Sales\n\nplot_dow_boxplot(_holi, 1, \"Holidays\") # Plot boxplot (holidays)\nplot_dow_boxplot(_common, 2, \"Common Days\") # Plot boxplot (Common days)","ff38c13a":"_holi.Date.unique()","af21eab2":"df = apply_holiday_progression(df)\ndf.head()","3383f56f":"# Analysing Holiday Progression with Weekly_Sales\n_temp = df.groupby([\"Date\", \"Holiday_Progression\"]).Weekly_Sales.sum().reset_index()\n_temp.head()\nplt.figure(figsize=(18, 10))\nplt.title(\"Holiday Progression (4 Weeks)\")\nsns.scatterplot(_temp.Holiday_Progression, _temp.Weekly_Sales, alpha=0.5)","41d25ef6":"# Analysing Holiday Progression with Weekly_Sales\ndf = apply_holiday_progression(df, 3)\ndf.head()\n_temp = df.groupby([\"Date\", \"Holiday_Progression\"]).Weekly_Sales.sum().reset_index()\n_temp.head()\nplt.figure(figsize=(18, 10))\nplt.title(\"Holiday Progression (3 Weeks)\")\nsns.scatterplot(_temp.Holiday_Progression, _temp.Weekly_Sales, alpha=0.5)","47893cdc":"# Cleaning data\ndrop_holiday_progression(df)\ndel _holi, _common, _temp","6f1715dc":"# grouping all week sales on dataframe\n_ = train.groupby(\"Date\").Weekly_Sales.sum().reset_index()\nplt.figure(figsize=(20, 10))\nplt.plot(_.Date, _.Weekly_Sales)","3685f60d":"# Rolling Statistics\nrolmean = _.rolling(window=4).mean() # Gives a series of means of the number of previous values equals the window size.\nrolstd = _.rolling(window=4).std()\n\nplt.figure(figsize=(18, 7))\nplt.plot(_['Weekly_Sales'], linewidth = 2, label = 'Weekly_Sales')\nplt.plot(rolmean, linewidth = 2, label = 'Rolling Mean', color = 'r')\nplt.plot(rolstd, linewidth = 2, label = 'Rolling Std Dev', color = 'k')\nplt.legend(loc = 'best')\nplt.title('Rolling Mean and Standard Deviation')","69a6188a":"apply_dickey_fuller_stationary_test(_)","cd2d2f82":"# Applying variance correction to time series\n_ = remove_variance_df(_)","de50a1e9":"# Applying tendency correction to time series\n_ = remove_tendency_df(_)","a15f9919":"apply_dickey_fuller_stationary_test(_.dropna())\ndel _","e7ccd3a4":"_ = df.groupby(\"Fuel_Price\").Weekly_Sales.sum().reset_index()\nsns.scatterplot(_.Fuel_Price, _.Weekly_Sales, alpha=0.7)","cdbfa2a2":"sns.distplot(_.Fuel_Price, bins=20)","709ae4e5":"sns.distplot(np.log(_.Fuel_Price), bins=20)\ndel _","49afec02":"_ = df.groupby(\"Temperature\").Weekly_Sales.sum().reset_index()\nsns.scatterplot(_.Temperature, _.Weekly_Sales, alpha=0.7)","c43502b0":"sns.distplot(_.Temperature, bins=20)\ndel _","90c33678":"plot_scatter(df, \"CPI\")","bbf0230a":"plot_hist(df, \"CPI\")","0f34b43c":"plot_scatter(df, \"Unemployment\")","df82d34e":"plot_hist(df, \"Unemployment\")","11c36e33":"!pip install fbprophet","ef2d9e3b":"# Super Bowl: 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13\n# Labor Day: 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13\n# Thanksgiving: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13\n# Christmas: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13\n\nlaborday = pd.DataFrame({\n  'holiday': 'laborday',\n  'ds': pd.to_datetime(['2010-09-10', '2011-09-09', '2012-09-07', '2013-09-06']),\n  'lower_window': 0,\n  'upper_window': 1,\n})\nsuperbowls = pd.DataFrame({\n  'holiday': 'superbowl',\n  'ds': pd.to_datetime(['2010-02-12', '2011-02-11', '2012-02-10', '2013-02-08']),\n  'lower_window': 0,\n  'upper_window': 1,\n})\nthanksgiving = pd.DataFrame({\n  'holiday': 'thanksgiving',\n  'ds': pd.to_datetime(['2010-11-26', '2011-11-25', '2012-11-23', '2013-11-29']),\n  'lower_window': 0,\n  'upper_window': 1,\n})\nxmas = pd.DataFrame({\n  'holiday': 'christmas',\n  'ds': pd.to_datetime(['2010-12-31', '2011-12-30', '2012-12-28', '2013-12-27']),\n  'lower_window': 0,\n  'upper_window': 1,\n})\nholidays = pd.concat((laborday, superbowls, thanksgiving, xmas))","fb9efa3e":"from fbprophet import Prophet\n\nm = Prophet(holidays=holidays, interval_width=0.95)\n_temp = df.groupby(\"Date\").Weekly_Sales.sum().reset_index()\n_temp = set_prophet_requirements(_temp)\nm.fit(_temp)\n_temp.head()","a3dcb9a2":"future = m.make_future_dataframe(periods=365, freq='d', include_history = True)\nforecast = m.predict(future)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].head()","ecdb50d2":"fig1 = m.plot(forecast)","2edb5731":"fig2 = m.plot_components(forecast)","5791d66f":"sample = pd.read_csv('\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/sampleSubmission.csv.zip')\nsample['Date'] = sample.Id.apply(lambda x: x.split('_')[2])\nsample = set_datetime_column(sample)\nfinal = sample.drop('Weekly_Sales', axis=1).merge(forecast[['ds', 'yhat']], how='left',left_on='Date', right_on='ds').drop('Date', axis=1)\nfinal.drop(columns='ds', inplace=True)\nfinal.rename(columns={'yhat': 'Weekly_Sales'}, inplace=True)\nfinal.to_csv('walmart-submission.csv', index=False)\nfinal.tail()","57b5eeaf":"df = set_datetime_column(df)\ndf = split_datetime_info(df)\ndf = create_num_ordinary_type_column(df)\ndf.head()","0bb843ba":"# grouping all week sales on dataframe\ndf_sales = train.groupby(\"Date\").Weekly_Sales.sum().reset_index()\ndf_sales.head()","a7fbee02":"plt.figure(figsize=(20, 10))\nplt.plot(df_sales.Date, df_sales.Weekly_Sales)","be4c8222":"# Applying Information Lag\ndf_sales = set_information_lag(df_sales)\ndf_sales.head(10)","34e699c7":"# Lag Visualization\nlag_graph(df_sales)","b902e0ed":"# Random Forest\nmodel = RandomForestRegressor()\n\ndf_sales.set_index('Date')\nfirst_weekly_sales = df_sales.iloc[0, 1]\ndf_sales.fillna(first_weekly_sales, inplace=True) # fill NaN with Sales Mean()\nnd_train, nd_test = train_test_split_time_series(df_sales)\nX_train, y_train = split_X_y_ndarray(nd_train)\n\nmodel.fit(X_train, y_train)","d30c401b":"X_test, y_test = split_X_y_ndarray(nd_test)\ny_hat = model.predict(X_test)\nrmse(y_test, y_hat) # RMSE\nplt.scatter(y_test, y_hat)","b6f0b6ca":"# Benchmark baseline result for comparison\nplot_prediction_result(df_sales, y_test, y_hat)","b9bb8871":"sample = pd.read_csv('\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/sampleSubmission.csv.zip')\ntest = pd.read_csv('\/kaggle\/input\/walmart-recruiting-store-sales-forecasting\/test.csv.zip')","0b5d9660":"test.head()","dae4ed8e":"sample.head()","55b2d185":"# Setting initial lag on test dataframe\n\ntest = join_dataframe_columns(test)\ntest[\"Weekly_Sales\"] = 0\ndf_test = test.groupby(\"Date\").Weekly_Sales.sum().reset_index().set_index(\"Date\")\ndf_test = set_information_lag(df_test)\ndf_test = pd.concat([df_sales.iloc[-7:, :].set_index(\"Date\"), df_test])\ndf_test = pd.DataFrame(df_test.iloc[:, :1])\ndf_test = set_information_lag(df_test)\ndf_test = df_test.iloc[7:, :]\ndf_test.head(10)","faf779c8":"y_hat = model.predict([df_test.iloc[0, 1:].to_numpy()])","57ae2d0e":"predict_df_sales(df_test, model)\nsample['Date'] = sample.Id.apply(lambda x: x.split('_')[2])\nfinal = sample.drop('Weekly_Sales', axis=1).merge(df_test.iloc[:,0], how='left', on='Date').drop('Date', axis=1)","a6272bfd":"final.head()","9afb8389":"final.to_csv('walmart-submission.csv', index=False)","291c0221":"# grouping all week sales on dataframe\ndf_sales = train.groupby([\"Date\", \"Store\"]).Weekly_Sales.sum().reset_index()\ndf_sales = join_dataframe_columns(df_sales)\ndf_sales = create_num_ordinary_type_column(df_sales)\ndrop_type(df_sales)\ndf_sales = df_sales.set_index('Date')\ndrop_markdowns(df_sales)\ndf_sales.head()","f08431a8":"# Random Forest Ensambles for Regression\nmodel = RandomForestRegressor()\ndf_train, df_test = train_test_split_time_series(df_sales, ndarrayType=False)\nX_train, y_train = split_X_y_df(df_train)\nmodel.fit(X_train, y_train)","c334a959":"# Testing\nX_test, y_test = split_X_y_df(df_test)\ny_hat = model.predict(X_test)\nrmse(y_test, y_hat) # RMSE","744f81a2":"plt.figure(figsize=(20, 7))\ndf_sales = train.groupby([\"Date\", \"Store\"]).Weekly_Sales.sum().reset_index()\noffset = len(df_sales) - len(y_test)\n_ = df_sales.iloc[offset:, :]\n_[\"y_test\"] = y_test\n_[\"y_hat\"] = y_hat\nplt.plot(_.groupby(\"Date\").Weekly_Sales.sum().reset_index().Date.values, _.groupby(\"Date\").y_test.sum().reset_index().y_test)\nplt.plot(_.groupby(\"Date\").Weekly_Sales.sum().reset_index().Date.values, _.groupby(\"Date\").y_hat.sum().reset_index().y_hat)\n","9a4e63af":"df.head()","afe60bff":"### <span style=\"color: blue\">Random Forest Regressor<\/span>","2badc2af":"___","1b7a7d30":"**Stationarity Conclusion**\n\nWe can conclude by the Augmented Dickey Fuller stationarity test that our time series is not stationary. Besides it shows no tendency and no relevance variance, it didn't passed on fullers test even with stationary correction. ","ce9cca14":"______","e10d7456":"<img src=\"https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/3816\/logos\/front_page.png\">\n## Walmart Recruiting - Store Sales Forecasting\nhttps:\/\/www.kaggle.com\/c\/walmart-recruiting-store-sales-forecasting\n\n### Challange\nOne challenge of modeling retail data is the need to make decisions based on limited history. If Christmas comes but once a year, so does the chance to see how strategic decisions impacted the bottom line. \n\nThis challange uses historical sales data for 45 Walmart stores located in different regions. Each store contains many departments, and participants **must project the sales for each department in each store**. To add to the challenge, selected holiday markdown events are included in the dataset. These markdowns are known to affect sales, but it is challenging to predict which departments are affected and the extent of the impact.\n\n\n### Promotional Markdowns\n\nThese are discounts that derive from any type of promotional sale such as a temporary price reduction, circular promotion, coupons, endcap promotions and more.\n\n### Consumer Price Index \u2013 [CPI](https:\/\/www.investopedia.com\/terms\/c\/consumerpriceindex.asp)\n\n#### What Is the Consumer Price Index \u2013 CPI?\nThe Consumer Price Index (CPI) is a measure that examines the weighted average of prices of a basket of consumer goods and services, such as transportation, food, and medical care. It is calculated by taking price changes for each item in the predetermined basket of goods and averaging them. Changes in the CPI are used to assess price changes associated with the cost of living; the CPI is one of the most frequently used statistics for identifying periods of inflation or deflation.\n\n#### How the CPI Is Used\nCPI is widely used as an economic indicator. It is the most widely used measure of inflation and, by proxy, of the effectiveness of the government\u2019s economic policy. The CPI gives the government, businesses, and citizens an idea about prices changes in the economy, and can act as a guide in order to make informed decisions about the economy.\n\n\n### Dataset\n\nIn addition, Walmart runs several promotional markdown events throughout the year. These markdowns precede prominent holidays, the four largest of which are the Super Bowl, Labor Day, Thanksgiving, and Christmas. The weeks including these holidays are weighted five times higher in the evaluation than non-holiday weeks. Part of the challenge presented by this competition is modeling the effects of markdowns on these holiday weeks in the absence of complete\/ideal historical data.\n\n- features.csv\n- sampleSubmission.csv\n- stores.csv\n- test.csv\n- train.csv\n\n#### **stores.csv**\n\nThis file contains anonymized information about the 45 stores, indicating the type and size of store.\n\n#### **train.csv**\n\nThis is the historical training data, which covers to 2010-02-05 to 2012-11-01. Within this file you will find the following fields:\n\n- Store - the store number\n- Dept - the department number\n- Date - the week\n- Weekly_Sales -  sales for the given department in the given store\n- IsHoliday - whether the week is a special holiday week\n- test.csv\n\nThis file is identical to train.csv, except we have withheld the weekly sales. You must predict the sales for each triplet of store, department, and date in this file.\n\n##### **features.csv**\n\nThis file contains additional data related to the store, department, and regional activity for the given dates. It contains the following fields:\n\n- Store - the store number\n- Date - the week\n- Temperature - average temperature in the region\n- Fuel_Price - cost of fuel in the region\n- MarkDown1-5 - anonymized data related to promotional markdowns that Walmart is running. MarkDown data is only available after Nov 2011, and is not available for all stores all the time. Any missing value is marked with an NA.\n- CPI - the consumer price index\n- Unemployment - the unemployment rate\n- IsHoliday - whether the week is a special holiday week\n\n#### **Holidays**\n\nFor convenience, the four holidays fall within the following weeks in the dataset (not all holidays are in the data):\n\n- Super Bowl: 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13\n- Labor Day: 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13\n- Thanksgiving: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13\n- Christmas: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13\n\n#### Support\n- https:\/\/www.kaggle.com\/abefukasawa\/walmart-recruiting-draft\n","e2ec0b4c":"### <span style=\"color: blue\" >Benchmark Baseline (Lag Based - Random Forest Regressor)<\/span>","afefdce1":"### <span style=\"color: blue\">Simple Random Forest Regressor<\/span>","0df36473":"___","4cedf688":"**Temperature Conclusion**\n\nThe graphs shows that when the temperature increases, the sales also increase. It's an heterocedastic graph, but shows an correlation between the two variables","8d32d00e":"### Unemployment","18d37857":"#### Current Holidays\n\n- Super Bowl: 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13\n- Labor Day: 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13\n- Thanksgiving: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13\n- Christmas: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13\n\nBy analysing the box-plot graphs of holidays and common days, we can see that holidays have a bigger mean of sales, so it's relevant feature. By creating a holiday progression, we can see that the increase on sales are not centered on holiday specific day. Depending on holiday type, the summit of sales will be centered on holiday or previous weeks.","6f13469f":"### <span style=\"color: blue\">Importing Datasets<\/span>","5066e567":"___","4b5d05d5":"**Fuel Conclusion**\n\nThe graphs shows that fuel price seems not to correlate with weeekly sales directly. The graph shows a random dispersion of weekly sales points over fuel prices. The distribution is not an gaussian, but seems to be composed by 2 gaussians distributions.","d793dd6b":"**Markdown Conclusion**\n\nBy analysing the information of Markdowns, we can see that are too many values missing. My first decision is to drop markdown values on first hand, and than come back to analyse it on a second time for deeper conclusions.","20b003b6":"Submission and Description\n\nPrivate Score 60776366.55718\n\nPublic Score 61069388.70502","8aecc40f":"#### Stores","7fffdece":"### Holidays","ddc04e89":"___","c04d06bf":"_____","b197aa14":"### Lag","d3cb74f8":"___","d1a3a413":"___","e70ac60a":"**Size Conclusion**\n\nSize has an linear correlation with Weekly Sales and Square foot Sales  has an linear \"custered\" correlation, besides it's heteroscedasticity, seems to have an positive tendency on separeted clusters.","4212ca4d":"### CPI","f9d1d9d1":"___","55ef4e44":"**Type Conclusion**\n\nType seems to matter for Weekly_Sales, but there is some intersections points on Store Size and Weekly Sales. Some outliers needs to be threaten before encoding as ordinal. ","7979e626":"### <span style=\"color: blue\">Auxiliar Functions<\/span>","047d7989":"### Stationarity","65bc6340":"Basic Time Series (Lag-Based) prediction for benchmark baseline","3922a186":"### Prediction","3e4430de":"___","7d892c61":"### Prophet","01cf3e9c":"___","b32b6085":"____","f3bca538":"### Markdown (1 - 5)","a6f8c37e":"### Clusters","a5806089":"### Temperature","40ac9464":"**Unemployment Conclusion**\n\nUnemployment seems to have a weak correlation with Weekly_Sales. Even when unemployment is high, the weekly sales not shows a decrease tendency","690749a8":"### Fuel","2a12220a":"### Type","b4f556e9":"**CPI Conclusion**\n\nLower CPI values are correlated with higher Weekly_Sales. We can see that are some clusters on CPI prices, that can be separeted easily.","46f6ac97":"#### Dept","e12656b1":"### <span style=\"color: blue\">Explaratory Data Analysis (EDA)<\/span>\n\n#### Possibilities\n\n- I can rank (encode) store type as ordinals \n- sales per square foot as an indicator of sales performance\n- lags. How many?\n- holidays\n- Holidays progression\n- DOW (Day of Week)\n- sales last month (window of 4)\n- get year-month-day\n- get sales same week last year\n- get statistical data from windows\n- what is more important: raw size or log size?\n- how does fuel price impacts weekly sales?\n- how does temperature impacts weekly sales?\n- what is the relation between CPI and sales?\n- cluster CPI","752ef7aa":"### <span style=\"color:blue\">Feature Engeneering<\/span>","deeead16":"### Size\n","c40b32d8":"___","d0d33ae2":"### <span style=\"color:blue\">Training<\/span>","cb4d8839":"**Lag Conclusion**\n\nThe most relevants lags were \n- ACF (1, 2, 5, 52)\n- PACF (1, 5, 37, 47, 48, 49, 50 ,51, 52) \n- Pearson's Lag Corr. (1, 2, 3, 4, 48 ~ 56)\n\nThe annual sazonality is really strong as we can see on the 52\u00ba week of the last year.","36a7007a":"___"}}