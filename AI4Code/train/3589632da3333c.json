{"cell_type":{"22b8188d":"code","ac885fce":"code","1266cad6":"code","3828086f":"code","e8e04097":"code","1d2d859b":"code","1e02be39":"code","d6a2ccee":"markdown","07fbf418":"markdown","691e1d70":"markdown","5ff61a30":"markdown","3c7a3402":"markdown","19016399":"markdown","203bcc5d":"markdown","3e3ef1d0":"markdown","fec65b6d":"markdown","cffdc99d":"markdown","1a76a883":"markdown"},"source":{"22b8188d":"import pandas as pd\n\n# Load data\ndata = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\ntarget = data.SalePrice\npredictors = data.drop(['SalePrice','Id'], axis='columns')\n\n# For the sake of keeping the example simple, we'll use only numeric predictors. \nnumeric_predictors = predictors.select_dtypes(exclude=['object'])\n","ac885fce":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(numeric_predictors, \n                                                    target,\n                                                    train_size=0.7, \n                                                    test_size=0.3, \n                                                    random_state=0)\n\ndef score_dataset(X_train, X_test, y_train, y_test):\n    model = RandomForestRegressor()\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return mean_absolute_error(y_test, preds)","1266cad6":"missing_val_count_by_column = (numeric_predictors.isnull().sum())\n\nn_rows = len(numeric_predictors)\nmissing_val_percent_by_column = missing_val_count_by_column \/ n_rows\n\nstat = pd.DataFrame(data={'Count': missing_val_count_by_column , \n                          'Percent': missing_val_percent_by_column})\nprint(stat[stat['Count'] > 0])\n\n","3828086f":"cols_with_missing = [col for col in X_train.columns \n                                 if X_train[col].isnull().any()]\nreduced_X_train = X_train.drop(cols_with_missing, axis='columns')\nreduced_X_test  = X_test.drop(cols_with_missing, axis='columns')\nprint(\"Mean Absolute Error from dropping columns with Missing Values:\")\nprint(score_dataset(reduced_X_train, reduced_X_test, y_train, y_test))","e8e04097":"from sklearn.impute import SimpleImputer\n\nmy_imputer = SimpleImputer()\nimputed_X_train = my_imputer.fit_transform(X_train)\n# this will replace null values in the test data with the mean values in the train data\nimputed_X_test = my_imputer.transform(X_test)\nprint(\"Mean Absolute Error from Imputation:\")\nprint(score_dataset(imputed_X_train, imputed_X_test, y_train, y_test))","1d2d859b":"cols_with_missing = [col for col in X_train.columns \n                                 if X_train[col].isnull().any()]\nprint('my_imputer replaced Null values with mean of each column:')\nprint(X_train[cols_with_missing].mean())","1e02be39":"imputed_X_train_plus = X_train.copy()\nimputed_X_test_plus = X_test.copy()\n\ncols_with_missing = (col for col in X_train.columns \n                                 if X_train[col].isnull().any())\nfor col in cols_with_missing:\n    imputed_X_train_plus[col + '_was_missing'] = imputed_X_train_plus[col].isnull()\n    imputed_X_test_plus[col + '_was_missing'] = imputed_X_test_plus[col].isnull()\n\n# Imputation\nmy_imputer = SimpleImputer()\nimputed_X_train_plus = my_imputer.fit_transform(imputed_X_train_plus)\nimputed_X_test_plus = my_imputer.transform(imputed_X_test_plus)\n\nprint(\"Mean Absolute Error from Imputation while Track What Was Imputed:\")\nprint(score_dataset(imputed_X_train_plus, imputed_X_test_plus, y_train, y_test))","d6a2ccee":"# Your Turn\n1) Find some columns with missing values in your dataset.\n\n2) Use the Imputer class so you can impute missing values\n\n3) Add columns with missing values to your predictors. \n\nIf you find the right columns, you may see an improvement in model scores. That said, the Iowa data doesn't have a lot of columns with missing values.  So, whether you see an improvement at this point depends on some other details of your model.\n\nOnce you've added the Imputer, keep using those columns for future steps.  In the end, it will improve your model (and in most other datasets, it is a big improvement). \n\n# Keep Going\nOnce you've added the Imputer and included columns with missing values, you are ready to [add categorical variables](https:\/\/www.kaggle.com\/dansbecker\/using-categorical-data-with-one-hot-encoding), which is non-numeric data representing categories (like the name of the neighborhood a house is in).\n\n---\n\nPart of the **[Learn Machine Learning](https:\/\/www.kaggle.com\/learn\/machine-learning)** track.","07fbf418":"# Conclusion\nIn general it seems all methods perfroms well since a mean error of 20,000$ of houses prices is no bad. comparing to melbourne which the error is x10.\nImputing missing values allowed us to improve our model compared to dropping those columns a bit. Since there not a lot of missing values I would think the difference should be greater. I guess that the features having missing values don't affect the prediction a lot.\nFor some reason Imputation with Extra Columns perfroms worse than other two.  TBD: how can I explain this?","691e1d70":"## Solutions\n\n\n## 1) A Simple Option: Drop Columns with Missing Values\nIf your data is in a DataFrame called `original_data`, you can drop columns with missing values. One way to do that is\n```\ndata_without_missing_values = original_data.dropna(axis='columns')\n```\n\nIn many cases, you'll have both a training dataset and a test dataset.  You will want to drop the same columns in both DataFrames. In that case, you would write\n\n```\ncols_with_missing = [col for col in original_data.columns \n                                 if original_data[col].isnull().any()]\nredued_original_data = original_data.drop(cols_with_missing, axis='columns')\nreduced_test_data = test_data.drop(cols_with_missing, axis='columns')\n```\nIf those columns had useful information (in the places that were not missing), your model loses access to this information when the column is dropped. Also, if your test data has missing values in places where your training data did not, this will result in an error.  \n\nSo, it's somewhat usually not the best solution. However, it can be useful when most values in a column are missing.\n\n\n\n## 2) A Better Option: Imputation\nImputation fills in the missing value with some number. The imputed value won't be exactly right in most cases, but it usually gives more accurate models than dropping the column entirely.\n\nThis is done with\n```\nfrom sklearn.impute import SimpleImputer\nmy_imputer = SimpleImputer()\ndata_with_imputed_values = my_imputer.fit_transform(original_data)\n```\nThe default behavior fills in the **mean** value for imputation.  Statisticians have researched more complex strategies, but those complex strategies typically give no benefit once you plug the results into sophisticated machine learning models.\n\nOne (of many) nice things about Imputation is that it can be included in a scikit-learn Pipeline. Pipelines simplify model building, model validation and model deployment.\n\n## 3) An Extension To Imputation\nImputation is the standard approach, and it usually works well.  However, imputed values may by systematically above or below their actual values (which weren't collected in the dataset). Or rows with missing values may be unique in some other way. **In that case, your model would make better predictions by considering which values were originally missing**.  Here's how it might look:\n```\n# make copy to avoid changing original data (when Imputing)\nnew_data = original_data.copy()\n\n# make new columns indicating what will be imputed\ncols_with_missing = (col for col in new_data.columns \n                                 if new_data[col].isnull().any())\n\n# so the model knows if the value is imputed or original\nfor col in cols_with_missing:\n    new_data[col + '_was_missing'] = new_data[col].isnull()\n\n# Imputation\nmy_imputer = SimpleImputer() \/\/mean\nnew_data = pd.DataFrame(my_imputer.fit_transform(new_data))\nnew_data.columns = original_data.columns\n```\n\nIn some cases this approach will meaningfully improve results. In other cases, it doesn't help at all.\n\n---\n# Example (Comparing All Solutions)\n\nWe will see am example predicting housing prices from the Melbourne Housing data.  To master missing value handling, fork this notebook and repeat the same steps with the Iowa Housing data.  Find information about both in the **Data** section of the header menu.\nFor keeping the example simple,  we compare  applying each approch to **all** columns containing missing value. But different columns may benefit from  different approches.\n\n\n### Basic Problem Set-up","5ff61a30":"*This tutorial is part Level 2 in the [Learn Machine Learning](https:\/\/www.kaggle.com\/learn\/machine-learning) curriculum. This tutorial picks up where Level 1 finished, so you will get the most out of it if you've done the exercise from Level 1.*\n\nIn this step, you will learn three approaches to dealing with missing values. You will then learn to compare the effectiveness of these approaches on any given dataset.* \n\n# Introduction\n\nThere are many ways data can end up with missing values. For example\n- A 2 bedroom house wouldn't include an answer for _How large is the third bedroom_\n- Someone being surveyed may choose not to share their income\n\nPython libraries represent missing numbers as **nan** which is short for \"not a number\".  You can detect which cells have missing values, and then count how many there are in each column with the command:\n```\nmissing_val_count_by_column = (data.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])\n```\n\nMost libraries (including scikit-learn) will give you an **error** if you try to build or test a model using data with missing values. So **you'll need to choose one** of the strategies below **for each column**..\n\n---\n","3c7a3402":"### Get Score from Imputation with Extra Columns Showing What Was Imputed","19016399":"### Create Function to Measure Quality of An Approach\nWe divide our data into **training** and **test**. If the reason for this is unfamiliar, review [Welcome to Data Science](https:\/\/www.kaggle.com\/dansbecker\/welcome-to-data-science-1).\n\nWe've loaded a function `score_dataset(X_train, X_test, y_train, y_test)` to compare the quality of diffrent approaches to missing values. This function reports the out-of-sample MAE score from a RandomForest.","203bcc5d":"We can see there are  not  a lot of missing values.\nGarageYrBlt  will be null if the house don't contain a garage.\nMasVnrArea will be null if the house don't contain a Masonry veneer.","3e3ef1d0":"### Get Model Score from Imputation","fec65b6d":"GarageYrBlt are **integers** but we filled the missing values with **float** numbers.  I wonder how this affect the model.","cffdc99d":"### Explore missing values","1a76a883":"### Get Model Score from Dropping Columns with Missing Values"}}