{"cell_type":{"e492e5cd":"code","a149ff42":"code","d381846b":"code","00ac2567":"code","f9c0be2e":"code","10317b40":"code","7955d071":"code","b61c6b50":"code","0f278b22":"code","eb2dc7aa":"code","eda8cab0":"code","ad1c4e12":"code","66723029":"markdown","93302752":"markdown","07b251cf":"markdown","3e005c80":"markdown","de810bba":"markdown","126613f2":"markdown","9363d9bf":"markdown","e8354e44":"markdown"},"source":{"e492e5cd":"# Asthetics\nimport warnings\nimport sklearn.exceptions\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n\n# General\nfrom kaggle_datasets import KaggleDatasets\nfrom glob import glob\nimport pandas as pd\nimport numpy as np\nimport os\nimport time\nimport cv2\nimport random\nimport shutil\nimport math\nimport re\npd.set_option('display.max_columns', None)\n\n# Visualizations\nfrom PIL import Image\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import iplot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nimport plotly.express as px\n%matplotlib inline\nsns.set(style=\"whitegrid\")\n\n# Machine Learning\n# Pre Procesing\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n# Models\nfrom sklearn.model_selection import train_test_split, KFold\n# Deep Learning\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\n#import efficientnet.tfkeras as efn\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n#from tensorflow.keras.applications import EfficientNetB4\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, BatchNormalization, GlobalAveragePooling2D\nfrom tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nimport tensorflow_addons as tfa\nfrom tensorflow_addons.metrics import F1Score, FBetaScore\nfrom tensorflow_addons.callbacks import TQDMProgressBar\nfrom tensorflow.keras.utils import plot_model\n\n#Metrics\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n\nprint('TF',tf.__version__)\n\n# Random Seed Fixing\nRANDOM_SEED = 42\n\ndef seed_everything(seed=RANDOM_SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n\nseed_everything()","a149ff42":"# From https:\/\/www.kaggle.com\/xhlulu\/ranzcr-efficientnet-tpu-training\ndef auto_select_accelerator():\n    TPU_DETECTED = False\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        print(\"Running on TPU:\", tpu.master())\n        TPU_DETECTED =True\n    except ValueError:\n        strategy = tf.distribute.get_strategy()\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    \n    return strategy, TPU_DETECTED","d381846b":"# Model Params\nKFOLDS = 4\nIMG_SIZES = [256]*KFOLDS\nBATCH_SIZES = [64]*KFOLDS\nEPOCHS = [15]*KFOLDS\nEFF_NETS = [0]*KFOLDS # WHICH EFFICIENTNET B? TO USE\n\n# Model Eval Params\nDISPLAY_PLOT = True\n\n# Inference Params\nWGTS = [1\/KFOLDS]*KFOLDS","00ac2567":"strategy, TPU_DETECTED = auto_select_accelerator()\nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync","f9c0be2e":"from tqdm.notebook import tqdm\n\nfiles_test_g = []\nfor i,k in tqdm([(0, 1), (2, 3), (4, 5), (6, 7)]):\n    GCS_PATH = KaggleDatasets().get_gcs_path(f'cqt-g2net-test-{i}-{k}')\n    files_test_g.extend(np.sort(np.array(tf.io.gfile.glob(GCS_PATH + '\/test*.tfrec'))).tolist())\nnum_train_files = len(files_test_g)\nprint('test_files:',num_train_files)","10317b40":"def read_labeled_tfrecord(example):\n    tfrec_format = {\n        'image'                        : tf.io.FixedLenFeature([], tf.string),\n        'image_id'                     : tf.io.FixedLenFeature([], tf.string),\n        'target'                       : tf.io.FixedLenFeature([], tf.int64)\n    }           \n    example = tf.io.parse_single_example(example, tfrec_format)\n    return prepare_image(example['image']), tf.reshape(tf.cast(example['target'], tf.float32), [1])\n\n\ndef read_unlabeled_tfrecord(example, return_image_id):\n    tfrec_format = {\n        'image'                        : tf.io.FixedLenFeature([], tf.string),\n        'image_id'                     : tf.io.FixedLenFeature([], tf.string),\n    }\n    example = tf.io.parse_single_example(example, tfrec_format)\n    return prepare_image(example['image']), example['image_id'] if return_image_id else 0\n\n \ndef prepare_image(img, dim=IMG_SIZES[0]):    \n    img = tf.image.resize(tf.image.decode_png(img, channels=3), size=(dim, dim))\n    img = tf.cast(img, tf.float32) \/ 255.0\n    img = tf.reshape(img, [dim,dim, 3])\n            \n    return img\n\ndef count_data_items(fileids):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(fileid).group(1)) \n         for fileid in fileids]\n    return np.sum(n)","7955d071":"def get_dataset(files, shuffle = False, repeat = False, \n                labeled=True, return_image_ids=True, batch_size=16, dim=IMG_SIZES[0]):\n    \n    ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO)\n    ds = ds.cache()\n    \n    if repeat:\n        ds = ds.repeat()\n    \n    if shuffle: \n        ds = ds.shuffle(1024*2)\n        opt = tf.data.Options()\n        opt.experimental_deterministic = False\n        ds = ds.with_options(opt)\n        \n    if labeled: \n        ds = ds.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n    else:\n        ds = ds.map(lambda example: read_unlabeled_tfrecord(example, return_image_ids), \n                    num_parallel_calls=AUTO)      \n    \n    ds = ds.batch(batch_size * REPLICAS)\n    ds = ds.prefetch(AUTO)\n    return ds","b61c6b50":"\"\"\"\nCreates a EfficientNetV2 Model as defined in:\nMingxing Tan, Quoc V. Le. (2021).\nEfficientNetV2: Smaller Models and Faster Training\narXiv preprint arXiv:2104.00298.\n\"\"\"\nimport os\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import (\n    Activation,\n    Add,\n    BatchNormalization,\n    Conv2D,\n    Dense,\n    DepthwiseConv2D,\n    Dropout,\n    GlobalAveragePooling2D,\n    Input,\n    PReLU,\n    Reshape,\n    Multiply,\n)\n\nBATCH_NORM_DECAY = 0.9\nBATCH_NORM_EPSILON = 0.001\nCONV_KERNEL_INITIALIZER = keras.initializers.VarianceScaling(scale=2.0, mode=\"fan_out\", distribution=\"truncated_normal\")\n# CONV_KERNEL_INITIALIZER = 'glorot_uniform'\n\nBLOCK_CONFIGS = {\n    \"b0\": {  # width 1.0, depth 1.0\n        \"first_conv_filter\": 32,\n        \"expands\": [1, 4, 4, 4, 6, 6],\n        \"out_channels\": [16, 32, 48, 96, 112, 192],\n        \"depthes\": [1, 2, 2, 3, 5, 8],\n        \"strides\": [1, 2, 2, 2, 1, 2],\n        \"use_ses\": [0, 0, 0, 1, 1, 1],\n    },\n    \"b1\": {  # width 1.0, depth 1.1\n        \"first_conv_filter\": 32,\n        \"expands\": [1, 4, 4, 4, 6, 6],\n        \"out_channels\": [16, 32, 48, 96, 112, 192],\n        \"depthes\": [2, 3, 3, 4, 6, 9],\n        \"strides\": [1, 2, 2, 2, 1, 2],\n        \"use_ses\": [0, 0, 0, 1, 1, 1],\n    },\n    \"b2\": {  # width 1.1, depth 1.2\n        \"first_conv_filter\": 32,\n        \"output_conv_filter\": 1408,\n        \"expands\": [1, 4, 4, 4, 6, 6],\n        \"out_channels\": [16, 32, 56, 104, 120, 208],\n        \"depthes\": [2, 3, 3, 4, 6, 10],\n        \"strides\": [1, 2, 2, 2, 1, 2],\n        \"use_ses\": [0, 0, 0, 1, 1, 1],\n    },\n    \"b3\": {  # width 1.2, depth 1.4\n        \"first_conv_filter\": 40,\n        \"output_conv_filter\": 1536,\n        \"expands\": [1, 4, 4, 4, 6, 6],\n        \"out_channels\": [16, 40, 56, 112, 136, 232],\n        \"depthes\": [2, 3, 3, 5, 7, 12],\n        \"strides\": [1, 2, 2, 2, 1, 2],\n        \"use_ses\": [0, 0, 0, 1, 1, 1],\n    },\n    \"s\": {  # width 1.4, depth 1.8\n        \"first_conv_filter\": 24,\n        \"output_conv_filter\": 1280,\n        \"expands\": [1, 4, 4, 4, 6, 6],\n        \"out_channels\": [24, 48, 64, 128, 160, 256],\n        \"depthes\": [2, 4, 4, 6, 9, 15],\n        \"strides\": [1, 2, 2, 2, 1, 2],\n        \"use_ses\": [0, 0, 0, 1, 1, 1],\n    },\n    \"m\": {  # width 1.6, depth 2.2\n        \"first_conv_filter\": 24,\n        \"output_conv_filter\": 1280,\n        \"expands\": [1, 4, 4, 4, 6, 6, 6],\n        \"out_channels\": [24, 48, 80, 160, 176, 304, 512],\n        \"depthes\": [3, 5, 5, 7, 14, 18, 5],\n        \"strides\": [1, 2, 2, 2, 1, 2, 1],\n        \"use_ses\": [0, 0, 0, 1, 1, 1, 1],\n    },\n    \"l\": {  # width 2.0, depth 3.1\n        \"first_conv_filter\": 32,\n        \"output_conv_filter\": 1280,\n        \"expands\": [1, 4, 4, 4, 6, 6, 6],\n        \"out_channels\": [32, 64, 96, 192, 224, 384, 640],\n        \"depthes\": [4, 7, 7, 10, 19, 25, 7],\n        \"strides\": [1, 2, 2, 2, 1, 2, 1],\n        \"use_ses\": [0, 0, 0, 1, 1, 1, 1],\n    },\n    \"xl\": {\n        \"first_conv_filter\": 32,\n        \"output_conv_filter\": 1280,\n        \"expands\": [1, 4, 4, 4, 6, 6, 6],\n        \"out_channels\": [32, 64, 96, 192, 256, 512, 640],\n        \"depthes\": [4, 8, 8, 16, 24, 32, 8],\n        \"strides\": [1, 2, 2, 2, 1, 2, 1],\n        \"use_ses\": [0, 0, 0, 1, 1, 1, 1],\n    },\n}\n\n\ndef _make_divisible(v, divisor=4, min_value=None):\n    \"\"\"\n    This function is taken from the original tf repo.\n    It ensures that all layers have a channel number that is divisible by 8\n    It can be seen here:\n    https:\/\/github.com\/tensorflow\/models\/blob\/master\/research\/slim\/nets\/mobilenet\/mobilenet.py\n    \"\"\"\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor \/ 2) \/\/ divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\ndef conv2d_no_bias(inputs, filters, kernel_size, strides=1, padding=\"VALID\", name=\"\"):\n    return Conv2D(\n        filters, kernel_size, strides=strides, padding=padding, use_bias=False, kernel_initializer=CONV_KERNEL_INITIALIZER, name=name + \"conv\"\n    )(inputs)\n\ndef batchnorm_with_activation(inputs, activation=\"swish\", name=\"\"):\n    \"\"\"Performs a batch normalization followed by an activation. \"\"\"\n    bn_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n    nn = BatchNormalization(\n        axis=bn_axis,\n        momentum=BATCH_NORM_DECAY,\n        epsilon=BATCH_NORM_EPSILON,\n        name=name + \"bn\",\n    )(inputs)\n    if activation:\n        nn = Activation(activation=activation, name=name + activation)(nn)\n        # nn = PReLU(shared_axes=[1, 2], alpha_initializer=tf.initializers.Constant(0.25), name=name + \"PReLU\")(nn)\n    return nn\n\ndef se_module(inputs, se_ratio=4, name=\"\"):\n    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n    h_axis, w_axis = [2, 3] if K.image_data_format() == \"channels_first\" else [1, 2]\n\n    filters = inputs.shape[channel_axis]\n    # reduction = _make_divisible(filters \/\/ se_ratio, 8)\n    reduction = filters \/\/ se_ratio\n    # se = GlobalAveragePooling2D()(inputs)\n    # se = Reshape((1, 1, filters))(se)\n    se = tf.reduce_mean(inputs, [h_axis, w_axis], keepdims=True)\n    se = Conv2D(reduction, kernel_size=1, use_bias=True, kernel_initializer=CONV_KERNEL_INITIALIZER, name=name + \"1_conv\")(se)\n    # se = PReLU(shared_axes=[1, 2])(se)\n    se = Activation(\"swish\")(se)\n    se = Conv2D(filters, kernel_size=1, use_bias=True, kernel_initializer=CONV_KERNEL_INITIALIZER, name=name + \"2_conv\")(se)\n    se = Activation(\"sigmoid\")(se)\n    return Multiply()([inputs, se])\n\n\ndef MBConv(inputs, output_channel, stride, expand_ratio, shortcut, survival=None, use_se=0, is_fused=False, name=\"\"):\n    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n    input_channel = inputs.shape[channel_axis]\n\n    if is_fused and expand_ratio != 1:\n        nn = conv2d_no_bias(inputs, input_channel * expand_ratio, (3, 3), strides=stride, padding=\"same\", name=name + \"sortcut_\")\n        nn = batchnorm_with_activation(nn, name=name + \"sortcut_\")\n    elif expand_ratio != 1:\n        nn = conv2d_no_bias(inputs, input_channel * expand_ratio, (1, 1), strides=(1, 1), padding=\"same\", name=name + \"sortcut_\")\n        nn = batchnorm_with_activation(nn, name=name + \"sortcut_\")\n    else:\n        nn = inputs\n\n    if not is_fused:\n        nn = DepthwiseConv2D(\n            (3, 3), padding=\"same\", strides=stride, use_bias=False, depthwise_initializer=CONV_KERNEL_INITIALIZER, name=name + \"MB_dw_\"\n        )(nn)\n        nn = batchnorm_with_activation(nn, name=name + \"MB_dw_\")\n\n    if use_se:\n        nn = se_module(nn, se_ratio=4 * expand_ratio, name=name + \"se_\")\n\n    # pw-linear\n    if is_fused and expand_ratio == 1:\n        nn = conv2d_no_bias(nn, output_channel, (3, 3), strides=stride, padding=\"same\", name=name + \"fu_\")\n        nn = batchnorm_with_activation(nn, name=name + \"fu_\")\n    else:\n        nn = conv2d_no_bias(nn, output_channel, (1, 1), strides=(1, 1), padding=\"same\", name=name + \"MB_pw_\")\n        nn = batchnorm_with_activation(nn, activation=None, name=name + \"MB_pw_\")\n\n    if shortcut:\n        if survival is not None and survival < 1:\n            from tensorflow_addons.layers import StochasticDepth\n\n            return StochasticDepth(float(survival))([inputs, nn])\n        else:\n            return Add()([inputs, nn])\n    else:\n        return nn\n\n\ndef EfficientNetV2(\n    model_type,\n    input_shape=(None, None, 3),\n    classes=1000,\n    dropout=0.2,\n    first_strides=2,\n    survivals=None,\n    classifier_activation=\"softmax\",\n    pretrained=\"imagenet21k-ft1k\",\n    name=\"EfficientNetV2\",\n):\n    \"\"\"\n    model_type: is the pre-defined model, value in [\"s\", \"m\", \"l\", \"b0\", \"b1\", \"b2\", \"b3\"].\n    classes: Output classes number, 0 to exclude top layers.\n    first_strides: is used in the first Conv2D layer.\n    survivals: is used for [Deep Networks with Stochastic Depth](https:\/\/arxiv.org\/abs\/1603.09382).\n        Can be a constant value like `0.5` or `0.8`,\n        or a tuple value like `(1, 0.8)` indicates the survival probability linearly changes from `1 --> 0.8` for `top --> bottom` layers.\n        A higher value means a higher probability will keep the conv branch.\n        or `None` to disable.\n    pretrained: value in [None, \"imagenet\", \"imagenet21k\", \"imagenet21k-ft1k\"]. Save path is `~\/.keras\/models\/efficientnetv2\/`.\n    \"\"\"\n    blocks_config = BLOCK_CONFIGS.get(model_type.lower(), BLOCK_CONFIGS[\"s\"])\n    expands = blocks_config[\"expands\"]\n    out_channels = blocks_config[\"out_channels\"]\n    depthes = blocks_config[\"depthes\"]\n    strides = blocks_config[\"strides\"]\n    use_ses = blocks_config[\"use_ses\"]\n    first_conv_filter = blocks_config.get(\"first_conv_filter\", out_channels[0])\n    output_conv_filter = blocks_config.get(\"output_conv_filter\", 1280)\n\n    inputs = Input(shape=input_shape)\n    out_channel = _make_divisible(first_conv_filter, 8)\n    nn = conv2d_no_bias(inputs, out_channel, (3, 3), strides=first_strides, padding=\"same\", name=\"stem_\")\n    nn = batchnorm_with_activation(nn, name=\"stem_\")\n\n    # StochasticDepth survival_probability values\n    total_layers = sum(depthes)\n    if isinstance(survivals, float):\n        survivals = [survivals] * total_layers\n    elif isinstance(survivals, (list, tuple)) and len(survivals) == 2:\n        start, end = survivals\n        survivals = [start - (1 - end) * float(ii) \/ total_layers for ii in range(total_layers)]\n    else:\n        survivals = [None] * total_layers\n    survivals = [survivals[int(sum(depthes[:id])) : sum(depthes[: id + 1])] for id in range(len(depthes))]\n\n    pre_out = out_channel\n    for id, (expand, out_channel, depth, survival, stride, se) in enumerate(zip(expands, out_channels, depthes, survivals, strides, use_ses)):\n        out = _make_divisible(out_channel, 8)\n        is_fused = True if se == 0 else False\n        for block_id in range(depth):\n            stride = stride if block_id == 0 else 1\n            shortcut = True if out == pre_out and stride == 1 else False\n            name = \"stack_{}_block{}_\".format(id, block_id)\n            nn = MBConv(nn, out, stride, expand, shortcut, survival[block_id], se, is_fused, name=name)\n            pre_out = out\n\n    output_conv_filter = _make_divisible(output_conv_filter, 8)\n    nn = conv2d_no_bias(nn, output_conv_filter, (1, 1), strides=(1, 1), padding=\"valid\", name=\"post_\")\n    nn = batchnorm_with_activation(nn, name=\"post_\")\n\n    if classes > 0:\n        nn = GlobalAveragePooling2D(name=\"avg_pool\")(nn)\n        if dropout > 0 and dropout < 1:\n            nn = Dropout(dropout)(nn)\n        nn = Dense(classes, activation=classifier_activation, name=\"predictions\")(nn)\n    model = Model(inputs=inputs, outputs=nn, name=name)\n\n    pretrained_dd = {\"imagenet\": \"imagenet\", \"imagenet21k\": \"21k\", \"imagenet21k-ft1k\": \"21k-ft1k\"}\n    if pretrained in pretrained_dd:\n        pre_url = \"https:\/\/github.com\/leondgarse\/Keras_efficientnet_v2\/releases\/download\/v1.0.0\/efficientnetv2-{}-{}.h5\"\n        url = pre_url.format(model_type, pretrained_dd[pretrained])\n        file_name = os.path.basename(url)\n        pretrained_model = keras.utils.get_file(file_name, url, cache_subdir=\"models\/efficientnetv2\")\n        model.load_weights(pretrained_model, by_name=True, skip_mismatch=True)\n    return model\n\ndef EfficientNetV2B0(\n    input_shape=(None, None, 3),\n    classes=1000,\n    dropout=0.2,\n    first_strides=2,\n    survivals=None,\n    classifier_activation=\"softmax\",\n    pretrained=\"imagenet21k\",\n    name=\"EfficientNetV2B0\",\n):\n    return EfficientNetV2(model_type=\"b0\", **locals())\n\ndef EfficientNetV2B1(\n    input_shape=(None, None, 3),\n    classes=1000,\n    dropout=0.2,\n    first_strides=2,\n    survivals=None,\n    classifier_activation=\"softmax\",\n    pretrained=\"imagenet21k\",\n    name=\"EfficientNetV2B1\",\n):\n    return EfficientNetV2(model_type=\"b1\", **locals())\n\ndef EfficientNetV2B2(\n    input_shape=(None, None, 3),\n    classes=1000,\n    dropout=0.2,\n    first_strides=2,\n    survivals=None,\n    classifier_activation=\"softmax\",\n    pretrained=\"imagenet21k\",\n    name=\"EfficientNetV2B2\",\n):\n    return EfficientNetV2(model_type=\"b2\", **locals())\n\ndef EfficientNetV2B3(\n    input_shape=(None, None, 3),\n    classes=1000,\n    dropout=0.2,\n    first_strides=2,\n    survivals=None,\n    classifier_activation=\"softmax\",\n    pretrained=\"imagenet21k\",\n    name=\"EfficientNetV2B3\",\n):\n    return EfficientNetV2(model_type=\"b3\", **locals())\n\ndef EfficientNetV2S(\n    input_shape=(None, None, 3),\n    classes=1000,\n    dropout=0.2,\n    first_strides=2,\n    survivals=None,\n    classifier_activation=\"softmax\",\n    pretrained=\"imagenet21k\",\n    name=\"EfficientNetV2S\",\n):\n    return EfficientNetV2(model_type=\"s\", **locals())\n\n\ndef EfficientNetV2M(\n    input_shape=(None, None, 3),\n    classes=1000,\n    dropout=0.3,\n    first_strides=2,\n    survivals=None,\n    classifier_activation=\"softmax\",\n    pretrained=\"imagenet21k\",\n    name=\"EfficientNetV2M\",\n):\n    return EfficientNetV2(model_type=\"m\", **locals())\n\n\ndef EfficientNetV2L(\n    input_shape=(None, None, 3),\n    classes=1000,\n    dropout=0.4,\n    first_strides=2,\n    survivals=None,\n    classifier_activation=\"softmax\",\n    pretrained=\"imagenet21k\",\n    name=\"EfficientNetV2L\",\n):\n    return EfficientNetV2(model_type=\"l\", **locals())\n\n\ndef EfficientNetV2XL(\n    input_shape=(None, None, 3),\n    classes=1000,\n    dropout=0.4,\n    first_strides=2,\n    survivals=None,\n    classifier_activation=\"softmax\",\n    pretrained=\"imagenet21k\",\n    name=\"EfficientNetV2XL\",\n):\n    return EfficientNetV2(model_type=\"xl\", **locals())\n\ndef get_actual_survival_probabilities(model):\n    from tensorflow_addons.layers import StochasticDepth\n    return [ii.survival_probability for ii in model.layers if isinstance(ii, StochasticDepth)]\n","0f278b22":"EFNS = [EfficientNetV2B0, EfficientNetV2B1, EfficientNetV2B2, EfficientNetV2B3, \n        EfficientNetV2S, EfficientNetV2M, EfficientNetV2L, EfficientNetV2XL]\n\ndef build_model(size, ef=0, count=820):\n    inp = tf.keras.layers.Input(shape=(size, size,3))\n    base = EFNS[ef](input_shape=(size, size, 3), survivals=None, dropout=0.5, classes=0, pretrained=None)\n    #base.load_weights(\"..\/input\/efficientnetv2-pretrained-weights\/efficientnetv2-b0-21k-notop.h5\")\n    \n    x = base(inp)\n    \n    x = tf.keras.layers.GlobalAvgPool2D()(x)\n    \n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.Dense(1,activation='sigmoid')(x)\n    model = tf.keras.Model(inputs=inp, outputs=x)\n    lr_decayed_fn = tf.keras.experimental.CosineDecay(\n                              1e-3,\n                              count,\n    )\n\n    opt = tfa.optimizers.AdamW(lr_decayed_fn, learning_rate=1e-4)\n    loss = tf.keras.losses.BinaryCrossentropy() \n    model.compile(optimizer=opt,loss=loss,metrics=['AUC'])\n    return model","eb2dc7aa":"skf = KFold(n_splits=KFOLDS,shuffle=True,random_state=RANDOM_SEED)\noof_pred = []; oof_tar = []; oof_val = []; oof_f1 = []; oof_ids = []; oof_folds = [] \n\nfiles_test_g = np.array(files_test_g)\n\nfor fold in range(0, KFOLDS):\n    \n    print('#'*25); print('#### FOLD',fold+1)\n    # BUILD MODEL\n    K.clear_session()\n    \n    with strategy.scope():\n        model = build_model(IMG_SIZES[fold], ef=EFF_NETS[fold])\n    print('\\tLoading model...')\n    \n    model.load_weights(f'..\/input\/cqt-g2net-efficientnetv2-training\/fold-{fold}.h5')  \n    \n    print('\\tPredict...')\n    ds_test = get_dataset(files_test_g,labeled=False,return_image_ids=True,\n            repeat=False,shuffle=False,dim=IMG_SIZES[fold],batch_size=BATCH_SIZES[fold]*2)\n    \n    _oof_pred = []\n    _oof_tar = []\n    for img, target in tqdm(iter(ds_test)):\n        p = model.predict(img).flatten()\n        t = target.numpy().flatten()\n        _oof_pred.extend(p.tolist())\n        _oof_tar.extend(t.tolist())\n    oof_pred.append(np.array(_oof_pred).flatten())\n    oof_ids.append(np.array(_oof_tar).flatten())\n    sns.distplot(oof_pred[-1])\n    plt.show()               \n    print('\\tFinished...')","eda8cab0":"sub = pd.read_csv('..\/input\/g2net-gravitational-wave-detection\/sample_submission.csv')\nsub['id'] = [t.decode(\"utf-8\") for t in oof_ids[-1]]\nsub['target'] = np.mean(oof_pred, axis=0)\nsub = sub.sort_values('id') \nsub.head()","ad1c4e12":"sub.to_csv('submission.csv', index=False)","66723029":"# Evaluate","93302752":"# References:\n\n### [CQT G2Net EfficientNetB1[TPU Inference]](https:\/\/www.kaggle.com\/miklgr500\/cqt-g2net-efficientnetb7-tpu-inference?scriptVersionId=67501537) by [Welf Crozzo](https:\/\/www.kaggle.com\/miklgr500)\n### [Keras_efficientnet_v2](https:\/\/github.com\/leondgarse\/Keras_efficientnet_v2) by [leondgarse](https:\/\/github.com\/leondgarse)","07b251cf":"# Dataset Creation","3e005c80":"# CFG","de810bba":"# Train Noteboook\n* [CQT G2Net EfficientNetB1[TPU Training]](https:\/\/www.kaggle.com\/miklgr500\/cqt-g2net-efficientnetb1-tpu-training)\n\n# Train Datasets\n* [Q-Transform TFRecords](https:\/\/www.kaggle.com\/miklgr500\/q-transform-tfrecords)\n    * [CQT G2Net V2 [0 - 1]](https:\/\/www.kaggle.com\/miklgr500\/cqt-g2net-v2-0-1)\n    * [CQT G2Net V2 [2 - 3]](https:\/\/www.kaggle.com\/miklgr500\/cqt-g2net-v2-2-3)\n    * [CQT G2Net V2 [4 - 5]](https:\/\/www.kaggle.com\/miklgr500\/cqt-g2net-v2-4-5)\n    * [CQT G2Net V2 [6 - 7]](https:\/\/www.kaggle.com\/miklgr500\/cqt-g2net-v2-6-7)\n    * [CQT G2Net V2 [8 - 9]](https:\/\/www.kaggle.com\/miklgr500\/cqt-g2net-v2-8-9)\n    * [CQT G2Net V2 [10 - 11]](https:\/\/www.kaggle.com\/miklgr500\/cqt-g2net-v2-10-11)\n    * [CQT G2Net V2 [12 - 13]](https:\/\/www.kaggle.com\/miklgr500\/cqt-g2net-v2-12-13)\n    * [CQT G2Net V2 [14 - 15]](https:\/\/www.kaggle.com\/miklgr500\/cqt-g2net-v2-14-15)\n    \n# Test Datasets\n* [CQT G2Net Test [0 - 1]](https:\/\/www.kaggle.com\/miklgr500\/cqt-g2net-test-0-1)\n* [CQT G2Net Test [2 - 3]](https:\/\/www.kaggle.com\/miklgr500\/cqt-g2net-test-2-3)\n* [CQT G2Net Test [4 - 5]](https:\/\/www.kaggle.com\/miklgr500\/cqt-g2net-test-4-5)\n* [CQT G2Net Test [6 - 7]](https:\/\/www.kaggle.com\/miklgr500\/cqt-g2net-test-6-7)","126613f2":"# Next steps\n* Add augmentation\n* Add TTA Inference","9363d9bf":"# Build Model","e8354e44":"# Reading Tfrecords"}}