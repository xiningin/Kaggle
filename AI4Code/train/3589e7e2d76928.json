{"cell_type":{"bc5f8e24":"code","f57edbae":"code","f400d1c2":"code","ff3852a8":"code","e503e7fe":"code","695a9afe":"code","e88ec14d":"code","9ed7ca5f":"code","008bc664":"code","07ea6ff5":"code","56a379a6":"code","91d438a9":"code","585e8291":"code","c935b324":"code","c3af3ac0":"code","12c3c69e":"code","919e6b1a":"code","33e43bac":"code","348ee252":"code","c300b0e6":"code","34ea39ef":"code","67574f45":"code","bdb14a1c":"code","c5cdce2a":"code","9525fc6d":"code","b9894473":"code","ef9451a9":"code","7f5f7730":"code","7091b244":"code","aaacda2d":"code","647c7bb3":"code","8183931e":"code","c5bfd684":"code","44266f87":"code","ff367e45":"code","d008569b":"code","ef934574":"code","5af248bf":"code","c26addbf":"code","a0abb9c3":"code","563b924c":"code","cda5d0a0":"code","46291e01":"code","aae17aec":"code","a5aedca8":"code","e9292a37":"code","65aed5cc":"code","3e380139":"markdown","a2077617":"markdown","1c65b939":"markdown","c215f3f5":"markdown","8e4adbcd":"markdown","a643fe30":"markdown","2dc3ccd3":"markdown","d3c0a21d":"markdown","fbb41b67":"markdown","fc9b6018":"markdown","59416634":"markdown","f0163d3f":"markdown","6e65e1f6":"markdown","7263ea70":"markdown","40f667b8":"markdown","5c52c0ce":"markdown","cece7400":"markdown","787a9fd1":"markdown","bf3ac723":"markdown","883655e9":"markdown","ad0d5d3d":"markdown","5e2805f8":"markdown","077ee0a3":"markdown","78efa022":"markdown","d9d0adb0":"markdown","9d602272":"markdown","f700e213":"markdown","985b9e93":"markdown","a2074a9b":"markdown","08d628a6":"markdown","b8872cdd":"markdown"},"source":{"bc5f8e24":"from copy import deepcopy\nimport random","f57edbae":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import Lasso, ElasticNet\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom xgboost import XGBRegressor\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import skew","f400d1c2":"%matplotlib inline","ff3852a8":"pd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","e503e7fe":"df_train_raw = pd.read_csv('..\/input\/train.csv')\ndf_train_raw.head(3)","695a9afe":"df_train_raw.shape","e88ec14d":"df_test_raw = pd.read_csv('..\/input\/test.csv')\ndf_test_raw.head()\nids_test = df_test_raw[\"Id\"]","9ed7ca5f":"y_name = 'SalePrice'","008bc664":"df_train_raw[y_name] = np.log(df_train_raw[y_name])","07ea6ff5":"cont_cols = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', \n             'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', \n             'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', \n            '3SsnPorch', 'ScreenPorch', 'PoolArea']","56a379a6":"scaler = StandardScaler()\ndf_num = df_train_raw[cont_cols].copy()\ndf_num[cont_cols] = scaler.fit_transform(df_num[cont_cols])","91d438a9":"df_num[cont_cols[:10]].boxplot(figsize=(14, 10))","585e8291":"df_num[cont_cols[10:]].boxplot(figsize=(14, 10))","c935b324":"cut = {'LotFrontage': 5, 'LotArea': 5.5, 'BsmtFinSF1': 10, 'BsmtFinSF2': 5.5, \n       'TotalBsmtSF': 10, '1stFlrSF': 5.5, 'GrLivArea': 5, 'WoodDeckSF': 5.1, \n       'OpenPorchSF': 5, 'EnclosedPorch': 5.1}\n\nindexes = list()\nfor col in cut:\n    indexes += list(df_num[df_num[col]>cut[col]].index)\nindexes = set(indexes)\nprint(df_train_raw.shape)\ndf_train_raw = df_train_raw.drop(indexes)\nprint(df_train_raw.shape)","c3af3ac0":"len_train = df_train_raw.shape[0]\ndf_raw = pd.concat([df_train_raw, df_test_raw], sort=False)\ndf_raw = df_raw.reset_index(drop=True)\ndf_raw = df_raw.drop('Id', 1)","12c3c69e":"(df_raw.isnull().sum() \/ df_raw.shape[0]).sort_values(ascending=False).head(10)","919e6b1a":"df = df_raw.copy()","33e43bac":"df['MSSubClass'] = df['MSSubClass'].astype(str)\ndf['MoSold'] = df['MoSold'].astype(str)","348ee252":"nanvars = {'Alley': 'None', 'BsmtQual': 'None', 'BsmtCond': 'None', 'BsmtExposure': 'None', \n           'BsmtFinType1': 'None', 'BsmtFinType2': 'None', 'FireplaceQu': 'None', \n           'GarageType': 'None', 'GarageFinish': 'None', 'GarageQual': 'None', \n           'GarageCond': 'None', 'PoolQC': 'None', 'Fence': 'None', 'MiscFeature': 'None',\n           'Utilities': 'ELO', 'MasVnrType': 'None', 'MasVnrArea': 0, 'Functional': 'Typ',\n           'BsmtFinSF1': 0, 'BsmtFinSF2': 0, 'BsmtUnfSF': 0,'TotalBsmtSF': 0, 'BsmtFullBath': 0, \n           'BsmtHalfBath': 0, 'MSSubClass': 'None', 'GarageCars': 0, 'GarageArea': 0\n}\n\nnanmode = ['MSZoning', 'Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType', \n           'GarageYrBlt']\nfor c in df.columns:\n    if df[c].isnull().sum() == 0 or c == y_name: \n        continue\n    df[c+'_na'] = df[c].isnull()\n    if c in nanvars:\n        df[c] = df[c].fillna(nanvars[c])\n    elif c in nanmode:\n        df[c] = df[c].fillna(df[c].mode()[0])\n\nfor c in ['LotFrontage']:\n    df[c+'_na'] = df[c].isnull()\ndf['LotFrontage'] = df['LotFrontage'].fillna(df['LotFrontage'].mean())","c300b0e6":"df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\ndf['TotalSF2'] = (df['BsmtFinSF1'] + df['BsmtFinSF2'] + \n                  df['1stFlrSF'] + df['2ndFlrSF'])\ndf['TotalBathrooms'] = (df['FullBath'] + (0.5 * df['HalfBath']) + \n                        df['BsmtFullBath'] + (0.5 * df['BsmtHalfBath']))\ndf['TotalPorchSF'] = (df['OpenPorchSF'] + df['3SsnPorch'] + \n                      df['EnclosedPorch'] + df['ScreenPorch'] + \n                      df['WoodDeckSF'])\n\ndf['HasPool'] = df['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ndf['Has2ndFloor'] = df['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ndf['HasGarage'] = df['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ndf['HasBsmt'] = df['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ndf['HasFireplace'] = df['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","34ea39ef":"def get_high_corr(df, threshold):\n    corr = df.corr()\n    showcols = list()\n    for y_col, row in corr.iterrows():\n        for x_col, val in zip(row.index, row):\n            if x_col != y_col and val >= threshold:\n                showcols.append(x_col)\n                showcols.append(y_col)\n    return list(set(showcols))","67574f45":"showcols = get_high_corr(df, 0.99999)\nsns.set()\nplt.figure(figsize=(16, 12))\nsns.heatmap(df[showcols].corr())","bdb14a1c":"todel = ['Exterior1st_na', 'BsmtFinSF1_na', 'BsmtFinSF2_na', 'BsmtUnfSF_na', \n         'BsmtHalfBath_na', 'Exterior2nd_na', 'GarageYrBlt_na', \n         'GarageFinish_na', 'GarageCond_na', 'GarageCars_na']\ndf = df.drop(todel, 1)","c5cdce2a":"plt.figure(figsize=(16, 10))\nsns.heatmap(df[[c for c in showcols if c not in todel]].corr())","9525fc6d":"gradcats = {\n    'LotShape': ['Reg', 'IR1', 'IR2', 'IR3'], \n    'LandContour': ['Lvl', 'Bnk', 'HLS', 'Low'],\n    'Utilities': ['AllPub', 'NoSewr', 'NoSeWa', 'ELO'], \n    'LandSlope': ['Gtl', 'Mod', 'Sev'],\n    'ExterQual': ['Ex', 'Gd', 'TA', 'Fa', 'Po'], \n    'ExterCond': ['Ex', 'Gd', 'TA', 'Fa', 'Po'],\n    'BsmtQual': ['Ex', 'Gd', 'TA', 'Fa', 'Po', 'None'], \n    'BsmtCond': ['Ex', 'Gd', 'TA', 'Fa', 'Po', 'None'],\n    'BsmtExposure': ['Gd', 'Av', 'Mn', 'No', 'None'], \n    'BsmtFinType1': ['GLQ', 'ALQ', 'BLQ','Rec','LwQ', 'Unf', 'None'],\n    'BsmtFinType2': ['GLQ', 'ALQ', 'BLQ','Rec','LwQ', 'Unf', 'None'], \n    'HeatingQC': ['Ex', 'Gd', 'TA', 'Fa', 'Po'],\n    'KitchenQual': ['Ex', 'Gd', 'TA', 'Fa', 'Po'], \n    'FireplaceQu': ['Ex', 'Gd', 'TA', 'Fa', 'Po', 'None'],\n    'GarageFinish': ['Fin', 'RFn', 'Unf', 'None'], \n    'GarageQual': ['Ex', 'Gd', 'TA', 'Fa', 'Po', 'None'],\n    'GarageCond': ['Ex', 'Gd', 'TA', 'Fa', 'Po', 'None'], \n    'PoolQC': ['Ex', 'Gd', 'TA', 'Fa', 'None'],\n    'Fence': ['GdPrv', 'MnPrv', 'GdWo', 'MnWw', 'None'],\n    'Functional': ['Typ', 'Min1', 'Min2', 'Mod', 'Maj1', 'Maj2', 'Sev', 'Sal']\n}\nfor c in df.columns:\n    if df[c].dtype == 'object':\n        if c in gradcats:\n            subs = list()\n            for val in gradcats[c]:\n                if val in df[c].values:\n                    subs.append(val)\n            df[c] = df[c].astype('category')\n            df[c] = df[c].cat.reorder_categories(subs)\n            df[c] = df[c].cat.codes\n","b9894473":"for c in df.columns:\n    if c == y_name or df[c].dtype == 'object':\n        continue\n    df[c + '_sqrt'] = np.sqrt(df[c])\n    df[c + '_times_2'] = df[c] ** 2","ef9451a9":"df = pd.get_dummies(df) ","7f5f7730":"numeric_feats = df.dtypes[df.dtypes != 'object'].index\nnumeric_feats = [c for c in numeric_feats if df[c].nunique() > 2]\nskewed_feats = df[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nprint(skewness.head())\nprint()\nprint(skewness.tail())","7091b244":"skew_val = 1.35\nprint(f'Number of columns to transform: {skewness[abs(skewness) > skew_val].notnull().sum()[0]}')","aaacda2d":"for c in skewness.index:\n    if c != y_name and abs(skewness.loc[c][0]) > skew_val:\n        df[c + '_log'] = np.log1p(df[c])\n#         df[c] = np.log1p(df[c])","647c7bb3":"def print_cv_result(cv: list):\n    print(f'CV mean: {round(np.mean(cv), 6)}, CV std: {round(np.std(cv), 6)}')\n\ndef print_feature_importance(columns, values, abs_threshold=0.0):\n    fi = {name: val for name, val in zip(columns, values)}\n    for k in sorted(fi, key=fi.get, reverse=True):\n        if abs(fi[k]) >= abs_threshold:\n            print(k.rjust(30), fi[k])","8183931e":"X_train = df[:len_train].drop(y_name, 1)\ny_train = df[:len_train][y_name]","c5bfd684":"def scorer(estimator, X, y):\n    return np.sqrt(mean_squared_error(y, estimator.predict(X)))","44266f87":"xgb = XGBRegressor(\n    random_state=1, n_estimators=100, verbosity=1, objective='reg:squarederror', \n    **{'subsample': 0.5308, 'reg_lambda': 1.051, 'min_child_weight': 1, 'max_depth': 3, \n       'colsample_bytree': 0.4, 'eta': 0.3, 'gamma': 0})\ncv = cross_val_score(xgb, X_train, y_train, cv=4, scoring=scorer)\nprint_cv_result(cv)","ff367e45":"xgb.fit(X_train, y_train)\nprint_feature_importance(X_train.columns, xgb.feature_importances_, abs_threshold=0.01)","d008569b":"en = make_pipeline(RobustScaler(), ElasticNet(random_state=1, alpha=0.00065, max_iter=3000, l1_ratio=0.7))\ncv = cross_val_score(en, X_train, y_train, cv=4, scoring=scorer)\nprint_cv_result(cv)","ef934574":"en.fit(X_train, y_train)\nprint_feature_importance(X_train.columns, en.steps[1][1].coef_, abs_threshold=0.05)","5af248bf":"class BlendingModel():\n    def __init__(self, weights: dict, models: dict):\n        if list(weights.keys()) != list(models.keys()):\n            raise ValueError('Weights and models must have the same keys')\n        self.weights = deepcopy(weights)\n        self.models = deepcopy(models)\n        \n    def fit(self, X, y, verbose=True):\n        for m_name in self.models:\n            if verbose:\n                print(f'fitting model {m_name}')\n            self.models[m_name].fit(X, y)\n    \n    def predict(self, X):\n        return sum(self.weights[m_name] * self.models[m_name].predict(X) \n                   for m_name in self.models)\n        ","c26addbf":"def find_weights_step(models: dict, df, y_name, step=0.05):\n    if len(models.keys()) != 2:\n        raise ValueError('find_weights_step() Only works if there are two models in dict')\n    best_weights = float('inf')\n    best_score = float('inf')\n    \n    df_tmp = df.sample(random_state=5, frac=1)\n    df_train = df_tmp[:1100]\n    df_val = df_tmp[1100:]\n    X_train, y_train = df_train.drop(y_name, 1), df_train[y_name]\n    X_val, y_val = df_val.drop(y_name, 1), df_val[y_name]\n        \n    \n    weights = dict()\n    models_keys = list(models.keys())\n    weights[models_keys[0]] = 1.\n    weights[models_keys[1]] = 0.\n    \n    models_ = deepcopy(models)\n    for m in models_:\n        models_[m].fit(X_train, y_train)\n    \n    while weights[models_keys[0]] > 0:\n        pred_stack = sum(weights[m_name] * models_[m_name].predict(X_val) \n                         for m_name in models_)\n        score = np.sqrt(mean_squared_error(y_val, pred_stack))\n        \n        print('score:', score, 'at', weights)\n        if score < best_score:\n            best_weights = deepcopy(weights)\n            best_score = score\n            \n        weights[models_keys[0]] -= step\n        weights[models_keys[1]] += step\n    print('best validation score:', best_score,  'at', best_weights)\n    return best_weights","a0abb9c3":"models = {'xgb': xgb, 'en': en}","563b924c":"weights = find_weights_step(models, df[:len_train], y_name, step=0.1)","cda5d0a0":"weights = {'xgb': 0.3, 'en': 0.7}","46291e01":"X = df[:len_train].drop(y_name, 1)\ny = df[:len_train][y_name]\nX_test = df[len_train:].drop('SalePrice', 1)","aae17aec":"xgb_final = XGBRegressor(\n    random_state=1, n_estimators=2100, verbosity=1, objective='reg:squarederror', nthread=-1,\n    **{'subsample': 0.5308, 'reg_lambda': 1.051, 'min_child_weight': 1, 'max_depth': 3,\n       'colsample_bytree': 0.5, 'eta': 0.4, 'gamma': 0})\nxgb_final.fit(X, y)","a5aedca8":"en_final = make_pipeline(RobustScaler(), \n                         ElasticNet(random_state=1, alpha=0.00065, max_iter=3000))\nen_final.fit(X, y)","e9292a37":"models = {'xgb': xgb_final, 'en': en_final}\nblending_final = BlendingModel(\n    weights=weights,\n    models=models\n)","65aed5cc":"SalePrice = np.exp(blending_final.predict(X_test))\n\nout_df = pd.DataFrame({'Id': ids_test, \"SalePrice\": SalePrice})\nout_df.to_csv('submission.csv', index=False)","3e380139":"Percent of null values","a2077617":"Mostly inspired by https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard","1c65b939":"### Filling NaN values\nFor most columns  we understand from data description what we should it fill with.  \nOther columns, if they are categorical, we fill with feature mode. If they are numerical, we could choose mean or median.  \nThe only one not filled numeric variable is LotFrontage, and since area feels more like continious variable rather than dicrete, we use mean.","c215f3f5":"We create new variables by summing corresponding areas and making boolean values","8e4adbcd":"Combine datasets, so transformations (such as creating dummy variables) won't break on test set","a643fe30":"Makes sense to transform only variables with skewness greater than certain threshold (it shows better validation accuracy)","2dc3ccd3":"# Feature engineering","d3c0a21d":"Converting categorical variables that are presented as numbers","fbb41b67":"### First model: ensemble method (XGB)","fc9b6018":"The dataset is rather small, so it makes sense to use cross-validation instead of splitting data into train and validation once.\nWe use the whole dataset to look at feature importance.","59416634":"### Correlation","f0163d3f":"XGB features with feature importance >= 0.01","6e65e1f6":"Elastic net features with absolute value of coefficient >= 0.05","7263ea70":"Now we can also create sqrt and sqaure of features for the linear model","40f667b8":"### Processing categorical variables\nThere are a lot of columns that show quality or condition, so the values are gradual. We should use label encoding for them,\nconsidering the order of categories.\nFor other categorical variables we construct dummy variables.","5c52c0ce":"For '_na' columns we randomly choose a column to remove, for other we remove the ones with lesser feat. importance","cece7400":"### Skewness\nWe use log transformation to deal with skewed data","787a9fd1":"# Reading data","bf3ac723":"Scale the data so we can use one plot for all columns","883655e9":"# Outliers removal ","ad0d5d3d":"# Modeling","5e2805f8":"best validation score: 0.11025080137123029 at {'xgb': 0.30000000000000016, 'en': 0.7}","077ee0a3":"Since there are only two models, we could just try every pair of values as weights of models with a certain step.  \nWe're not using cross-validation for the sake of simplicity, so scores might be far higher or lower than ones from single models CV earlier. Though we will see that blending is better than single models (when one of coefficients is 0)","78efa022":"And finally transform remaining categroical variables to dummies","d9d0adb0":"### Ensembling model\nElastic net seems to perform far better than ensemble methods, but we might get even better results with blending models","9d602272":"Let's look only on those columns that have correlation >= 0.99999 with other columns","f700e213":"### Second model: linear model (Elastic net)","985b9e93":"To display plots inline, at the same cell as code","a2074a9b":"# Final models and submission","08d628a6":"We have to remove outliers by hand, as opposed to some methods like three sigma, \nbecause if we remove everything beyond three sigmas, it would be a lot of rows, since some columns have most rows very distant from the median (such as 3SsnPorch)","b8872cdd":"Log transformation of y, so higher prices won't affect result more than low ones"}}