{"cell_type":{"1c81f830":"code","a076b969":"code","59be7787":"code","857d175a":"code","52287dc3":"code","d44f45f5":"code","8ceaa0fd":"code","ef368d20":"code","d840df4e":"code","dc5c8097":"code","5d9a4c08":"code","90e89100":"code","c2e00ed3":"code","fa9f829a":"code","e927caa6":"code","f0c8b37e":"code","f40fb3b0":"code","2474dac0":"code","92991eb0":"code","00625548":"code","003e5e1b":"code","48df7969":"code","a168d8e9":"code","5d2ae52e":"markdown","acc563b5":"markdown","7a368194":"markdown","34d698ee":"markdown","fa41669a":"markdown","19746922":"markdown","bdc4bfba":"markdown","67323204":"markdown","13ffb570":"markdown","b9533e72":"markdown","07c29645":"markdown","5dfa3cbf":"markdown","b6b8aff1":"markdown","297fab10":"markdown","c4abd3ca":"markdown","163ca59e":"markdown","fd6095cf":"markdown","f1af9071":"markdown","b9f4cfae":"markdown","a8534f8e":"markdown","8c05c8f3":"markdown","a4b6de0b":"markdown","3b857816":"markdown","b748c1a3":"markdown"},"source":{"1c81f830":"# data manipulations\nimport numpy as np\nimport pandas as pd \n\n# plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nsns.set()\n\n# ML models\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# automated preprocessing and evaluation\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\n# additional\nfrom functools import partial\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a076b969":"titanic = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntitanic.shape","59be7787":"titanic","857d175a":"titanic.info()","52287dc3":"cat_featutes = ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'SibSp', 'Parch', 'Ticket', 'Cabin', 'Embarked']\nnum_featutes = ['Age', 'Fare'] ","d44f45f5":"fig, ax = plt.subplots(1, 2, figsize=(12, 5))\nfor axi, feature in zip(ax.flat, num_featutes):\n    sns.distplot(titanic[feature], ax=axi)","8ceaa0fd":"for feature in cat_featutes:\n    x = titanic[feature].dropna()\n    if np.unique(x).shape[0] < 10:\n        fig, ax = plt.subplots(figsize=(8, 5))\n        sns.countplot(x, ax=ax)","ef368d20":"for feature in cat_featutes:\n    f = titanic[feature].dropna()\n    if np.unique(f).shape[0] > 10:\n        print(feature)\n        print('total count:', f.shape[0])\n        print('unique count:', f.unique().shape[0], end='\\n\\n')","d840df4e":"catplot = partial(sns.catplot, data=titanic, height=5, aspect=1.5)\ncatplot(x=\"Pclass\", y=\"Survived\", kind=\"bar\")\ncatplot(x=\"Embarked\", y=\"Survived\", kind=\"bar\")\ncatplot(x=\"Pclass\", y=\"Age\", hue='Survived', kind=\"violin\", split=True).set(ylim=0)\ncatplot(x=\"Pclass\", y=\"Fare\", hue='Survived', kind=\"violin\", split=True).set(ylim=(0, 300))\ncatplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", kind=\"point\");","dc5c8097":"useful_featues = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']  # excluding target\nuseful_num_featues = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']  \nuseful_cat_featues = ['Sex', 'Embarked']","5d9a4c08":"def get_preprocessing_pipeline():\n    num_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('scaler', StandardScaler())\n    ])\n\n    cat_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n        ('encode', OneHotEncoder())\n    ])\n\n    columns_pipeline = ColumnTransformer([\n        (\"numeric\", num_pipeline, useful_num_featues),\n        (\"categorical\", cat_pipeline, useful_cat_featues)\n    ])\n    final_pipeline = Pipeline([\n        ('columns', columns_pipeline),\n        ('pca', PCA(0.99, random_state=0))\n    ])\n    return final_pipeline","90e89100":"X = titanic.drop('Survived', axis=1)\ny = titanic['Survived']","c2e00ed3":"def chain_pipelines(**kwargs):\n    pipelines = list(kwargs.items())\n    return Pipeline(pipelines)\n\nmodel = chain_pipelines(preprocessing=get_preprocessing_pipeline(),\n                        model=None)","fa9f829a":"param_grid = [\n    {\n        'model': [LogisticRegression(n_jobs=-1)],\n        'model__C': np.linspace(0.5, 5, 10)\n    },\n    {\n        'model': [GaussianNB()]\n    },\n    {\n        'model': [KNeighborsClassifier(n_jobs=-1)],\n        'model__n_neighbors': np.arange(5, 11),\n        'model__weights': ['uniform', 'distance']\n    },\n    {\n        'model': [SVC(random_state=0)],\n        'model__C': np.linspace(1, 10, 19)\n    },\n    {\n        'model': [RandomForestClassifier(random_state=0, n_jobs=-1)],\n        'model__n_estimators': [20, 25, 30, 35, 40],\n        'model__max_depth': [5, 10, 15],\n        'model__min_samples_leaf': [1, 2],\n        'model__class_weight': ['balanced_subsample', 'balanced', None]\n    }\n]\ngrid = GridSearchCV(model, param_grid, scoring='accuracy', cv=3, n_jobs=-1)","e927caa6":"%%time\ncross_val_score(grid, X, y, cv=3, n_jobs=-1).mean()","f0c8b37e":"best_ML = grid.fit(X, y).best_estimator_","f40fb3b0":"best_ML.named_steps['model']","2474dac0":"best_ML.score(X, y)","92991eb0":"test_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntest_data.shape","00625548":"example_sub = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\nexample_sub.head()","003e5e1b":"predictions = best_ML.predict(test_data)\ntest_data['Survived'] = predictions\npredictions_frame = test_data[['PassengerId', 'Survived']]\npredictions_frame.head()","48df7969":"predictions_frame.to_csv('predictions.csv', index=False)","a168d8e9":"prepr = get_preprocessing_pipeline()\nX_trans = prepr.fit_transform(X)\n\nfig, ax = plt.subplots(2, 3, \n                       figsize=(15, 10),\n                       subplot_kw=dict(xticks=[], yticks=[]),\n                       gridspec_kw=dict(hspace=0.01, wspace=0.01))\n\nfor axi, perplexity in zip(ax.flat, [5, 10, 15, 20, 25, 30]):\n    X_2D = TSNE(perplexity=perplexity).fit_transform(X_trans)\n    sns.scatterplot(X_2D[:, 0], X_2D[:, 1], hue=y, ax=axi)\nfig.suptitle(\"t-SNE dimensionality reduction (with different perplexities)\", y=0.92);","5d2ae52e":"## <a name=\"id6\"><\/a> **Can We Do Better?**\n**In order to get a better accuracy we can try to build a more complex model.  \nTherefore, before getting started we should get more insights about our data. **\n\n**Let's implement one technique to look at our data in a high-dimensional space.  \nOur feature matrix consists of 8 columns, so instances live in a 8D space.  \nWe will project these points (instances) on a 2D plane using t-SNE.** ","acc563b5":"**Super! The gap between training and validation accuracy is not significant**","7a368194":"**To easily and correctly perform a cross-validation we can create a preprocessing pipeline.  \nOtherwise, it will be very hard to avoid data leakage.  \nThe pipeline consists of steps:** \n1. Select only relevant (to use in training) features\n2. For numeric features:  \n      * Impute missing values with a median value  \n      * Standardize features (scale to zero mean and unit variance)   \n3. For categorical features:  \n      * Impute missing values with a most frequent value  \n      * Convert to a one-hot representation  \n4. Concatenate (2) and (3)\n5. Apply PCA (this will reduce features from 10 to 8 almost without any  \n   information lost; it turned out it boosts a performance)","34d698ee":"**Estimate the performance. Note that here we implemented 2-level cross-validation:  \nOuter one is for performance estimation and inner one is for the best parameter search**","fa41669a":"**Split the data into the feature matrix (still a dataframe) and a target vertor (Series object)**","19746922":"**Assuming that we are satisfied, let's get the best ready-to-predict model**","bdc4bfba":"**Define a wide range of different models and for each specify parameter space to search for the best model**","67323204":"## <a name=\"id1\"><\/a> **Download Data**","13ffb570":"**Chain preprocessing and prediction into a one pipeline**","b9533e72":"## <a name=\"id5\"><\/a> **Making Predictions**","07c29645":"## <a name=\"id2\"><\/a> **Explore Data**","5dfa3cbf":"**Let's look at the distribution of the numeric variables**","b6b8aff1":"**Download the test dataset**","297fab10":"**It's also worth to look at the accuracy score across the whole dataset.  \nIt gives us a measure of overfit**","c4abd3ca":"**Let's look at the distribution of the categorical variables (almost categorical :))**","163ca59e":"**Some of the categorical featues have too wide range of values (sometimes looks like unique within a dataset)**","fd6095cf":"## <a name=\"id4\"><\/a> **Find The Best ML Model**","f1af9071":"## <a name=\"id3\"><\/a> **Data Preprocessing**\n**Firstly, a good practice is to split a dataset into a validation and train ones, but... wait!  \nThis dataset is quite small enough, so performing cross-validation will not be time consuming, let's implement it!  **\n\n**Now, define features that we'll use in a predictive model**","b9f4cfae":"**It's also useful to visualize distributions across multiple variables.**","a8534f8e":"**And... what we can conclude about these scatter plots?  \nWe see, that all the charts are similar in context of relations between points.  \nAnd these relatinons show us that classes strongly overlaps each other.  \nProbably, it's impossible to draw a simple boundary to split the classes.   \nThat's why we've achieved only ~82% accuracy trying a bunch of ML models  \nwith many different hyperparameters.**    \n\n**So, we may not expect a significant boost in a performance if trying a more complex model.  \nThe reason is we have little data and have already tried several powerful models.  \nWe should rather come back to a feature engineering step and extract better features**","8c05c8f3":"[Download Data](#id1)  \n[Explore data](#id2)  \n[Data Preprocessing](#id3)  \n[Find The Best ML Model](#id4)  \n[Making Predictions](#id5)    \n[Can We Do Better?](#id6)    ","a4b6de0b":"**Who is winner? Check it out**","3b857816":"**Create a submission file**","b748c1a3":"**Look at how our submission have to looks like**"}}