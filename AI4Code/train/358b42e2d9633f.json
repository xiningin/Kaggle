{"cell_type":{"bf45d331":"code","d9948716":"code","18750115":"code","df4c61aa":"code","6a443a9a":"code","24760652":"code","f831b10b":"code","07c4cc2f":"code","5a2f3f6e":"code","744fafab":"code","bc61b2f6":"code","837743d1":"code","79bd6c13":"code","7e4634e2":"code","b159f55f":"code","2887450c":"code","896997e5":"code","8d3ca52c":"code","e3090351":"code","7883976e":"code","319c74fd":"code","3b39cce4":"code","34378107":"code","6016b8fb":"code","ab5cbf54":"code","2a89ca2b":"code","f06d289b":"code","7b75b145":"code","ba168bf8":"code","a4d6ae09":"code","6867af81":"code","9413e7c0":"code","2b648421":"code","f452eb74":"code","b381a408":"code","8aaf8262":"markdown","5b23b43c":"markdown"},"source":{"bf45d331":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d9948716":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch import nn, optim\nimport torch.utils.data\nimport torch.nn.functional as F\nfrom sklearn.preprocessing import StandardScaler\n\n%matplotlib inline","18750115":"from sklearn import model_selection\nfrom sklearn.metrics import mean_absolute_error, r2_score\nfrom sklearn.linear_model import LogisticRegression, ARDRegression, OrthogonalMatchingPursuitCV, OrthogonalMatchingPursuit\nfrom sklearn.linear_model import SGDRegressor, TheilSenRegressor, RANSACRegressor, PassiveAggressiveRegressor, HuberRegressor\nfrom sklearn.linear_model import RidgeCV, SGDClassifier, PassiveAggressiveClassifier, Perceptron, BayesianRidge, MultiTaskElasticNetCV\nfrom sklearn.linear_model import MultiTaskElasticNet, ElasticNetCV, MultiTaskLassoCV, LassoLars, Lasso\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVR, SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom collections import Counter\nfrom scipy import interp\nfrom sklearn import grid_search","df4c61aa":"treino = pd.read_csv('dataset_treino.csv')\nteste = pd.read_csv(\"dataset_teste.csv\")","6a443a9a":"treino.info()","24760652":"sns.countplot(x=\"ENERGY STAR Score\", data=treino)","f831b10b":"sns.distplot(treino['ENERGY STAR Score'])","07c4cc2f":"sns.heatmap(treino.corr(), annot=True)","5a2f3f6e":"def tratamento1(x):\n    x = str(x)\n    aux = x.split(\";\")\n    if len(aux[len(aux)-1]) <= 10:\n        return aux[len(aux)-2].strip(\" \").replace(\"\\u200b\", \"\").replace(\"nan\", \"0000000000\")\n    else:\n        return aux[len(aux)-1].strip(\" \").replace(\"\\u200b\", \"\").replace(\"nan\", \"0000000000\")\n\ntreino['BBL'] = treino[\"BBL - 10 digits\"].apply(lambda x: tratamento1(x))\nteste['BBL'] = teste[\"BBL - 10 digits\"].apply(lambda x: tratamento1(x))","744fafab":"treino['Borough'] = treino['BBL'].apply(lambda x: int(x[:1]))\nteste['Borough'] = teste['BBL'].apply(lambda x: int(x[:1]))","bc61b2f6":"treino['TaxBlock'] = treino['BBL'].apply(lambda x: int(x[1:6]))\nteste['TaxBlock'] = teste['BBL'].apply(lambda x: int(x[1:6]))","837743d1":"treino['TaxLotNumber'] = treino['BBL'].apply(lambda x: int(x[6:]))\nteste['TaxLotNumber'] = teste['BBL'].apply(lambda x: int(x[6:]))","79bd6c13":"treino['Water Required?'] = treino['Water Required?'].map({'No': 0, 'Yes': 1})\nteste['Water Required?'] = teste['Water Required?'].map({'No': 0, 'Yes': 1})","7e4634e2":"y = treino['ENERGY STAR Score']\nX = treino.drop(['Order', 'Property Name', 'Postal Code', 'Parent Property Id', 'Parent Property Name', 'BBL - 10 digits', \n                 'NYC Borough, Block and Lot (BBL) self-reported', 'NYC Building Identification Number (BIN)', \n                 'Address 1 (self-reported)', 'Address 2', 'Street Number', 'Street Name', \n                 'Primary Property Type - Self Selected', 'List of All Property Use Types at Property',\n                 'Largest Property Use Type', 'Largest Property Use Type - Gross Floor Area (ft\u00b2)',\n                 '2nd Largest Property Use Type', '2nd Largest Property Use - Gross Floor Area (ft\u00b2)', \n                 '3rd Largest Property Use Type', '3rd Largest Property Use Type - Gross Floor Area (ft\u00b2)',\n                 'Metered Areas (Energy)', 'Metered Areas  (Water)', 'Release Date', 'DOF Benchmarking Submission Status',\n                 'Latitude', 'Longitude', 'NTA', 'BBL', 'ENERGY STAR Score', 'Fuel Oil #1 Use (kBtu)', \n                 'Fuel Oil #2 Use (kBtu)', 'Fuel Oil #4 Use (kBtu)', 'Fuel Oil #5 & 6 Use (kBtu)', \n                 'Diesel #2 Use (kBtu)', 'District Steam Use (kBtu)', 'Property Id'], axis=1)","b159f55f":"X[X.columns[22:]].head(2)","2887450c":"X2 = teste.drop(['OrderId', 'Property Name', 'Postal Code', 'Parent Property Id', 'Parent Property Name', 'BBL - 10 digits', \n                 'NYC Borough, Block and Lot (BBL) self-reported', 'NYC Building Identification Number (BIN)', \n                 'Address 1 (self-reported)', 'Address 2', 'Street Number', 'Street Name', \n                 'Primary Property Type - Self Selected', 'List of All Property Use Types at Property',\n                 'Largest Property Use Type', 'Largest Property Use Type - Gross Floor Area (ft\u00b2)',\n                 '2nd Largest Property Use Type', '2nd Largest Property Use - Gross Floor Area (ft\u00b2)', \n                 '3rd Largest Property Use Type', '3rd Largest Property Use Type - Gross Floor Area (ft\u00b2)',\n                 'Metered Areas (Energy)', 'Metered Areas  (Water)', 'Release Date', 'DOF Benchmarking Submission Status',\n                 'Latitude', 'Longitude', 'NTA', 'BBL', 'Fuel Oil #1 Use (kBtu)', \n                 'Fuel Oil #2 Use (kBtu)', 'Fuel Oil #4 Use (kBtu)', 'Fuel Oil #5 & 6 Use (kBtu)', \n                 'Diesel #2 Use (kBtu)', 'District Steam Use (kBtu)', 'Property Id'], axis=1)","896997e5":"X.head(10)","8d3ca52c":"X.replace('Not Available', np.nan, inplace=True)\nX2.replace('Not Available', np.nan, inplace=True)","e3090351":"columns = ['Weather Normalized Site EUI (kBtu\/ft\u00b2)', 'Weather Normalized Site Electricity Intensity (kWh\/ft\u00b2)',\n           'Weather Normalized Site Natural Gas Intensity (therms\/ft\u00b2)', 'Weather Normalized Source EUI (kBtu\/ft\u00b2)', \n           'Natural Gas Use (kBtu)', 'Weather Normalized Site Natural Gas Use (therms)', \n           'Electricity Use - Grid Purchase (kBtu)',\n           'Weather Normalized Site Electricity (kWh)', 'Total GHG Emissions (Metric Tons CO2e)',\n           'Direct GHG Emissions (Metric Tons CO2e)', 'Indirect GHG Emissions (Metric Tons CO2e)', \n           'Water Use (All Water Sources) (kgal)', 'Water Intensity (All Water Sources) (gal\/ft\u00b2)']","7883976e":"for i in columns:\n    X[i] = X[i].astype(float)\n    X2[i] = X2[i].astype(float)","319c74fd":"X2.info()","3b39cce4":"X.info()","34378107":"X = X.apply(lambda x: x.fillna(x.mean()),axis=0) \nX2 = X2.apply(lambda x: x.fillna(x.mean()),axis=0)\nX2","6016b8fb":"validation_size = 0.25\nseed = 7\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=validation_size, random_state=seed)","ab5cbf54":"scaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\nX2 = scaler.transform(X2)\n\n# how many samples per batch to load\nbatch_size = 64\n\n#y_train = pd.get_dummies(y_train).values\n#y_test = pd.get_dummies(y_test).values\n\ny_train = y_train.values\ny_test = y_test.values\n\n\n\ntrain_target = torch.tensor(y_train.astype(np.float32))\ntrain = torch.tensor(X_train.astype(np.float32)) \ntrain_tensor = torch.utils.data.TensorDataset(train, train_target) \ntrainloader = torch.utils.data.DataLoader(dataset = train_tensor, batch_size = batch_size, shuffle = True)\n\ntest_target = torch.tensor(y_test.astype(np.float32))\ntest = torch.tensor(X_test.astype(np.float32)) \ntest_tensor = torch.utils.data.TensorDataset(test, test_target) \ntestloader = torch.utils.data.DataLoader(dataset = test_tensor, batch_size = batch_size, shuffle = False)\n\nvalid_target = torch.tensor(np.zeros(len(X2)))\nvalid = torch.tensor(X2.astype(np.float32)) \nvalid_tensor = torch.utils.data.TensorDataset(valid, valid_target) \nvalidloader = torch.utils.data.DataLoader(dataset = valid_tensor, batch_size = 1, shuffle = False)","2a89ca2b":"def init_weights(m):\n    if type(m) == nn.Linear:\n        torch.nn.init.kaiming_normal_(m.weight)\n\nclass Network(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        # Inputs to hidden layer linear transformation\n        self.h1 = nn.Linear(27, 128)\n        # Hidden layer linear transformation\n        self.bn1 = nn.BatchNorm1d(128)\n        self.h2 = nn.Linear(128, 64)\n        self.bn2 = nn.BatchNorm1d(64)\n        # Hidden layer linear transformation\n        self.h3 = nn.Linear(64, 32)\n        self.bn3 = nn.BatchNorm1d(32)\n        # Hidden layer linear transformation\n        self.h4 = nn.Linear(32, 16)\n        self.bn4 = nn.BatchNorm1d(16)\n        # Hidden layer linear transformation\n        self.h5 = nn.Linear(16, 8)\n        self.bn5 = nn.BatchNorm1d(8)\n        # Output layer, 1 classes - one for each category\n        self.output = nn.Linear(8, 1)\n        #Dropout p=0.3\n        self.dropout = nn.Dropout(0.3)\n        \n    def forward(self, x):\n        # Pass the input tensor through each of our operations\n        x = F.relu(self.bn1(self.h1(x)))\n        x = self.dropout(x)\n        x = F.relu(self.bn2(self.h2(x)))\n        x = self.dropout(x)\n        x = F.relu(self.bn3(self.h3(x)))\n        x = self.dropout(x)\n        x = F.relu(self.bn4(self.h4(x)))\n        x = self.dropout(x)\n        x = F.relu(self.bn5(self.h5(x)))\n        x = self.output(x)\n        \n        return x","f06d289b":"model = Network()\nmodel.apply(init_weights)\ncriterion = nn.L1Loss()\n\n# Only train the classifier parameters, feature parameters are frozen\noptimizer = optim.ASGD(model.parameters(), lr=0.001)","7b75b145":"epochs = 300\n\nvalid_loss_min = np.Inf # track change in validation loss\n\nfor epoch in range(1, epochs+1):\n\n    # keep track of training and validation loss\n    train_loss = 0.0\n    valid_loss = 0.0\n    \n    ###################\n    # train the model #\n    ###################\n    model.train()\n    for data, target in trainloader:\n        # clear the gradients of all optimized variables\n        optimizer.zero_grad()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)\n        # calculate the batch loss\n        target = np.reshape(target, (*target.shape, 1))\n        loss = criterion(output, target)\n        # backward pass: compute gradient of the loss with respect to model parameters\n        loss.backward()\n        # perform a single optimization step (parameter update)\n        optimizer.step()\n        # update training loss\n        train_loss += loss.item()*data.size(0)\n        \n    ######################    \n    # validate the model #\n    ######################\n    \n    predictions = []\n    model.eval()\n    for data, target in testloader:\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)\n        # calculate the batch loss\n        target = np.reshape(target, (*target.shape, 1))\n        loss = criterion(output, target)\n        # update average validation loss \n        valid_loss += loss.item()*data.size(0)\n        predictions.append(int(output.data.numpy()[0]))\n    \n    # calculate average losses\n    train_loss = train_loss\/len(trainloader.dataset)\n    valid_loss = valid_loss\/len(testloader.dataset)\n        \n    # print training\/validation statistics \n    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n        epoch, train_loss, valid_loss))\n    \n    # save model if validation loss has decreased\n    if valid_loss <= valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        valid_loss))\n        torch.save(model.state_dict(), 'model_cifar.pt')\n        valid_loss_min = valid_loss","ba168bf8":"y_train = y\nX_train = X\n\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\n\ntrain_target = torch.tensor(y_train.values.astype(np.float32))\ntrain = torch.tensor(X_train.astype(np.float32)) \ntrain_tensor = torch.utils.data.TensorDataset(train, train_target) \ntrainloader = torch.utils.data.DataLoader(dataset = train_tensor, batch_size = batch_size, shuffle = True)","a4d6ae09":"epochs = 200\n\ntrain_loss_min = np.Inf\n\nfor epoch in range(1, epochs+1):\n\n    # keep track of training and validation loss\n    train_loss = 0.0\n    \n    ###################\n    # train the model #\n    ###################\n    model.train()\n    for data, target in trainloader:\n        # clear the gradients of all optimized variables\n        optimizer.zero_grad()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)\n        # calculate the batch loss\n        target = np.reshape(target, (*target.shape, 1))\n        loss = criterion(output, target)\n        # backward pass: compute gradient of the loss with respect to model parameters\n        loss.backward()\n        # perform a single optimization step (parameter update)\n        optimizer.step()\n        # update training loss\n        train_loss += loss.item()*data.size(0)\n     \n    # calculate average losses\n    train_loss = train_loss\/len(trainloader.dataset)\n        \n    # print training\/validation statistics \n    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n        epoch, train_loss))\n    \n    # save model if validation loss has decreased\n    if train_loss <= train_loss_min:\n        print('train loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        train_loss_min,\n        train_loss))\n        torch.save(model.state_dict(), 'model_cifar.pt')\n        train_loss_min = train_loss","6867af81":"model.load_state_dict(torch.load('model_cifar.pt'))","9413e7c0":"validation_size = 0.25\nseed = 7\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=validation_size, random_state=seed)","2b648421":"rf = RandomForestRegressor(bootstrap=True, max_depth=90, max_features=3, min_samples_leaf=3, \n                              min_impurity_split=8, n_estimators=1000) #SGDRegressor(penalty='l2',alpha=0.0001, l1_ratio=0.5, shuffle=False)\n\nrf.fit(X_train, y_train)\npred_rf = rf.predict(X_test)\nprint('RF Tunning')\nprint('MAE:', mean_absolute_error(y_test, pred_rf))\nprint('R^2:', r2_score(y_test, pred_rf))","f452eb74":"predictions = []\n\nmodel.eval()\n# iterate over test data\nfor data, target in validloader:\n    # forward pass: compute predicted outputs by passing inputs to the model\n    output = model(data)\n    pred = output\n    predictions.append(int(pred.data.numpy()[0]))\n\n\nrf.fit(X, y)\npred = rf.predict(X2)","b381a408":"final_pred = [int((x1 + x2) \/\/ 2) for x1, x2 in zip(list(pred), predictions)]\n\ndf_out = pd.DataFrame({'Property Id': teste['Property Id'].values, 'score': final_pred}, columns=['Property Id', 'score'])\n\ndf_out.to_csv(\"sampleSubmission.csv\", sep=',', index=False)\nprint(\"Done!\")","8aaf8262":"Importando bibliotecas","5b23b43c":"Leitura dos dados de treino e teste"}}