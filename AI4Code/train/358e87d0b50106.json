{"cell_type":{"76127ff7":"code","9a3d99c6":"code","123d3c2e":"code","b4869572":"code","2f60cef0":"code","2e6d3ca9":"code","9cf6aef6":"code","9168e4b9":"code","dce8ac71":"code","53e6811d":"code","d0e4e522":"code","cec7cad9":"code","b93fdcd4":"code","8b767ca1":"code","1c5ca2b2":"code","2fac6758":"code","499b5cae":"markdown","18cb63ca":"markdown","8d0eef61":"markdown","3611e353":"markdown","409f261a":"markdown","02972eea":"markdown","54cd9652":"markdown"},"source":{"76127ff7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9a3d99c6":"data= pd.read_csv(\"\/kaggle\/input\/weather-dataset-rattle-package\/weatherAUS.csv\")\ndisplay(data.head(n=10))\ndisplay(data.tail(n=10))","123d3c2e":"#Total number of days\nn_days = len(data)\n\n#The number of days with atleast 1mm or more of rain\nn_atleast_1mm = data[data['Rainfall']>=1.0].count()[4]\n\n# The number of days with less than 1mm of rain\nn_lessthan_1mm = data[data['Rainfall']<1.0].count()[4]\n\n#percentage of days with 1mm or more than 1mm of rain\nrainy_days_percentage= (float(n_atleast_1mm)*100\/float(n_days))\n\nprint(n_days,n_atleast_1mm,n_lessthan_1mm,rainy_days_percentage)","b4869572":"data.drop([\"Evaporation\",\"Sunshine\",\"Cloud9am\",\"Cloud3pm\",\"Location\"], inplace= True, axis =1)\ndisplay(data.head(n=10))","2f60cef0":"#deleting the rows with null values\ndata1= data.dropna(axis=0,how=\"any\",thresh = None,subset=None, inplace=False)\ndata1_len= len(data1)\ndisplay(data1.head(n=100))","2e6d3ca9":"#Normalizing numerical features\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nnumerical = ['MinTemp','MaxTemp','Rainfall','WindGustSpeed','WindSpeed9am','WindSpeed3pm','Humidity9am','Humidity3pm','Pressure9am','Pressure3pm','Temp9am','Temp3pm']\nfeatures_log_minmax_transform = pd.DataFrame(data = data1)\nfeatures_log_minmax_transform[numerical] = scaler.fit_transform(data1[numerical])\n\n# Show an example of a record with scaling applied\ndisplay(features_log_minmax_transform.head(n = 5))","9cf6aef6":"#splitting the data into features and target label\nrain_tomorrow_raw= data1['RainTomorrow']\nfeatures_raw = data1.drop('RainTomorrow',axis=1)","9168e4b9":"#Converting non numerical values to numerical values using one hot encoding\nfrom sklearn.preprocessing import LabelEncoder\n\nfeatures_final = pd.get_dummies(features_raw)\nle=LabelEncoder()\nle.fit(rain_tomorrow_raw)\n\nrain_tomorrow_final= le.transform(rain_tomorrow_raw)\nencoded = list(features_final.columns)\ndisplay(features_final.head(n=100))\n","dce8ac71":"features_final = features_final.dropna(how='any',axis=0) \nfeatures_final.isnull().sum().sum()\n#print(len(features_final))","53e6811d":"#Shuffle and split data\n# Import train_test_split\nfrom sklearn.model_selection import train_test_split\n\n\n# Split the 'features' and 'income' data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(features_final, \n                                                    rain_tomorrow_final, \n                                                    test_size = 0.2, \n                                                    random_state = 0)\n\n# Show the results of the split\nprint(\"Training set has {} samples.\".format(X_train.shape[0]))\nprint(\"Testing set has {} samples.\".format(X_test.shape[0]))","d0e4e522":"#implementing random forset classifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nmodel_rf= RandomForestClassifier()\nmodel_rf.fit(X_train,y_train)\n# Make predictions for the test set\ny_pred_test = model_rf.predict(X_test)\n# View accuracy score\nmetrics.accuracy_score(y_test, y_pred_test)","cec7cad9":"from sklearn.ensemble import AdaBoostClassifier\nmodel = AdaBoostClassifier().fit(X_train,y_train)\n#Extract the important features using .features_importances_\nimportances = model.feature_importances_\n# Plot\ncolumn_importances = pd.Series(model.feature_importances_, index=features_final.columns)\ncolumn_importances.nlargest(10).plot(kind='barh')","b93fdcd4":"#testing the accuracy of adaboost classifier\ny_pred = model.predict(X_test)\nmetrics.accuracy_score(y_test, y_pred)\n","8b767ca1":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import fbeta_score\nfrom xgboost import XGBClassifier\n\n#Initialize the classifier\nclf= XGBClassifier()\n\n#Parameters\nparameters =  {'nthread':[4], #when use hyperthread, xgboost may become slower\n              'objective':['binary:logistic'],\n              'learning_rate': [0.05], #so called `eta` value\n              'max_depth': [6],\n              'min_child_weight': [11],\n              'silent': [1],\n              'subsample': [0.8],\n              'colsample_bytree': [0.7],\n              'n_estimators': [5], #number of trees, change it to 1000 for better results\n              'missing':[-999],\n              'seed': [1337]}\n\n\n\nscorer = make_scorer(fbeta_score,beta=0.5)\n\ngrid_obj = GridSearchCV(clf, parameters,scorer)\n\ngrid_fit = grid_obj.fit(X_train,y_train)\n\n","1c5ca2b2":"\n# Get the estimator\nbest_clf = grid_fit.best_estimator_\n\n# Make predictions using unoptimized and optimized model\npredictions = (clf.fit(X_train, y_train)).predict(X_test)\nbest_predictions = best_clf.predict(X_test)\n\n\n\n","2fac6758":"from sklearn.metrics import accuracy_score\n# Report the before-and-afterscores\nprint(\"Unoptimized model\\n------\")\nprint(\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_test, predictions)))\nprint(\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, predictions, beta = 0.5)))\nprint(\"\\nOptimized Model\\n------\")\nprint(\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\nprint(\"Final F-score on the testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 0.5)))","499b5cae":"Handling missing values by deleting columns with mostly null values such as Evaporation, Sunshine, Cloud9am , Cloud3pm","18cb63ca":"* Total number of days recorded = 145460\n* Number of days with atleast 1mm or more of rain = 33639\n* Number of days with atleast less than 1mm of rain = 108560\n* rainy_days_percentage= 23.1259","8d0eef61":"**Implementation:Data Exploration**\nWe need to determine whether it will rain tomorrow or not. It's a yes for rain tomorrow if it is more than 1mm for that day. We need to compute the following:\n\n* The total number of days, 'n_days'\n* The number of days with atleast 1mm or more of rain, 'n_atleast_1mm'\n* The number of days with leass than 1mm of rain, 'n_lessthan_1mm'\n* Percentage of days with 1mm or more than 1mm of rain, 'rainy_days_percentage'","3611e353":"**Created model to do rain prediction for tommorrow.Highest model accuracy is achieved by using XGBoost classifier. For unoptimized XGboost model accuray is 0.8576 and for optimized(using Grid Search CV) it is 0.8449. **","409f261a":"Good enough amout of data left, so we can afford to remove the rows with null values.\nToDo: Check for skewed columns in the data and transform them if the accuracy is not good.","02972eea":"Implementing Adaboost classifier with feature importance","54cd9652":"Implementing model tuning using GridSearch CV. I will be using RandomForestClassifer for this purpose"}}