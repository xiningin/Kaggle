{"cell_type":{"dc28ef28":"code","ef981ac0":"code","3267454c":"code","a94ecc39":"code","d002f51c":"code","3048dbe0":"code","893ed085":"code","6820af2d":"code","8d3049c0":"code","0427687c":"code","47500aea":"code","4d11a2fc":"code","c301c00c":"code","a55e8d23":"code","0daa040d":"code","bb19343f":"code","3bde6928":"markdown","f047c884":"markdown","30f0af00":"markdown","27498d71":"markdown","e80dd53e":"markdown","103a98f7":"markdown","dd657097":"markdown","f1777b2c":"markdown"},"source":{"dc28ef28":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        URL = os.path.join(dirname, filename)\n        print(URL)","ef981ac0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport spacy\n\nimport datetime\nimport random\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","3267454c":"spacy.util.fix_random_seed(0)\nnp.random.seed(0)\nrandom.seed(0)","a94ecc39":"data = pd.read_csv(URL)\nprint(len(data))\ndata.head()","d002f51c":"data.isnull().sum()","3048dbe0":"data = (data.drop(data.columns[0], axis=1)).dropna()\nprint(len(data))\ndata.head()","893ed085":"tweet_text = \" \".join(data.tweet)\n\nwordcloud = WordCloud(width=1500, height=500).generate(tweet_text)\n\nplt.figure( figsize=(20,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","6820af2d":"target_text = \" \".join(data.target)\n\nwordcloud = WordCloud(width=1500, height=500).generate(target_text)\n\nplt.figure( figsize=(20,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","8d3049c0":"(data.groupby(\"target\").target.count())\\\n    .nlargest(10)\\\n    .plot(kind=\"barh\")","0427687c":"data[\"date\"] = pd.to_datetime(data[\"date\"])\nprint(data[\"date\"].dtype)\ndata.head()","47500aea":"plt.title(\"Insults over Years\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Percentage%\")\ndata[\"year\"] = data[\"date\"].dt.year\ninsults_over_year = data[\"year\"].value_counts().drop(2021) # 2021 is not completed as of yet\nsns.lineplot(insults_over_year.index, insults_over_year.values)","4d11a2fc":"nlp = spacy.blank(\"en\")\n\ntextcat = nlp.create_pipe(\n              \"textcat\",\n              config={\n                \"exclusive_classes\": True,\n                \"architecture\": \"bow\"})\n\nnlp.add_pipe(textcat)","c301c00c":"labels = np.unique(data[\"year\"].values).tolist()\nlabels = [textcat.add_label(str(label)) for label in labels]","a55e8d23":"train_text = data[\"tweet\"].values\ntrain_label = [{'cats': {'2014': label == '2014',\n                          '2015': label == '2015',\n                          '2016': label == '2016',\n                          '2017': label == '2017',\n                          '2018': label == '2018',\n                          '2019': label == '2019',\n                          '2020': label == '2020',\n                          '2021': label == '2021',\n                          '2022': label == '2022'}} for label in data[\"year\"]]\n\ntrain_data = list(zip(train_text, train_label))\ntrain_data[0]","0daa040d":"optimizer = nlp.begin_training()\n\nlosses = {}\nfor epoch in range(10):\n    random.shuffle(train_data)\n    batches = spacy.util.minibatch(train_data, size=4)\n    for batch in batches:\n        texts, labels = zip(*batch)\n        nlp.update(texts, labels, sgd=optimizer, losses=losses)\n    print(losses)","bb19343f":"texts = [\"GET SLEEPY JOE\", \n         \"GET HILLARY CLINTON\",]\ndocs = [nlp.tokenizer(text) for text in texts]\n\ntextcat = nlp.get_pipe('textcat')\nscores, _ = textcat.predict(docs)\n\nprint(scores)\n\npredicted_labels = scores.argmax(axis=1)\nprint([textcat.labels[label] for label in predicted_labels])","3bde6928":"# Imports","f047c884":"# Load Data","30f0af00":"# Drop Columns\/NA values","27498d71":"# Insults per Target","e80dd53e":"# WordCloud of Insults & Targets","103a98f7":"## Making Predictions","dd657097":"# Text Classification to Year Tweeted","f1777b2c":"# Sorting by Date"}}