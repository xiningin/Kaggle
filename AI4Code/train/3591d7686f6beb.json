{"cell_type":{"1bb07569":"code","7a0e6e4e":"code","22a878f9":"code","38fbdb3d":"code","1de99561":"code","c6156077":"code","4857c968":"code","a1d5ee30":"code","eaf3d699":"code","b6f9ac02":"code","a44a5d30":"code","3a59e49f":"code","c175e95c":"code","c0f7828e":"code","d6b2234e":"code","9c6cd182":"code","daa6676b":"code","ad43a59c":"code","327dfb60":"code","e536c076":"code","1ea2ed5c":"code","03e5f6ca":"code","895f41c1":"code","f45abeb5":"code","f45618b4":"code","cf174a7b":"code","7e198fe6":"code","18fd47f2":"code","708a1d57":"code","9d7fba53":"markdown","294bce51":"markdown","6fcb1012":"markdown","45901e2d":"markdown","53144ec7":"markdown","ece1e924":"markdown","21cc2e06":"markdown","1fab2e27":"markdown","b3bb61d2":"markdown","1aec009c":"markdown","e817cf9c":"markdown","f32f2967":"markdown","dbe12ca5":"markdown","c82f5d75":"markdown","3a7cdee8":"markdown","fbd29e15":"markdown","0d51996a":"markdown","f044c80a":"markdown"},"source":{"1bb07569":"import os\nimport random\nimport pandas as pd\nimport numpy as np\nimport torch\nimport math\nfrom lightgbm import LGBMRegressor\nimport lightgbm as lgb\nimport optuna \nimport optuna.integration.lightgbm as lgbo\noptuna.logging.disable_default_handler()\nfrom sklearn.metrics import mean_absolute_error\nimport gresearch_crypto\nimport datetime\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import mean_absolute_error as MSE\nimport warnings\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.decomposition import PCA\n\nwarnings.filterwarnings('ignore')\n\nTRAIN_CSV = '\/kaggle\/input\/g-research-crypto-forecasting\/train.csv'\nASSET_DETAILS_CSV = '\/kaggle\/input\/g-research-crypto-forecasting\/asset_details.csv'\n\nSEED = 20\n\nREMOVE_LB_TEST_OVERLAPPING_DATA = True","7a0e6e4e":"%%capture\n'''\n!cp ..\/input\/talibinstall\/ta-lib-0.4.0-src.tar.gzh  .\/ta-lib-0.4.0-src.tar.gz\n!tar -xzvf ta-lib-0.4.0-src.tar.gz > null\n!cd ta-lib && .\/configure --prefix=\/usr > null && make  > null && make install > null\n!cp ..\/input\/talibinstall\/TA-Lib-0.4.21.tar.gzh TA-Lib-0.4.21.tar.gz\n!pip install TA-Lib-0.4.21.tar.gz > null\n!pip install ..\/input\/talibinstall\/numpy-1.21.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl >null\nimport talib as ta\n'''","22a878f9":"def fix_all_seeds(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nfix_all_seeds(SEED)","38fbdb3d":"df_train = pd.read_csv(TRAIN_CSV)\ndf_train","1de99561":"df_test = pd.read_csv('..\/input\/g-research-crypto-forecasting\/example_test.csv')\ndf_test.head()","c6156077":"%%capture\n'''\nl =len(df_train)\nl = l\/\/3\ndf_train = df_train[l:].reset_index(drop=True)\ndf_train\n#'''","4857c968":"# Remove the future\nif REMOVE_LB_TEST_OVERLAPPING_DATA:\n    df_train['datetime'] = pd.to_datetime(df_train['timestamp'], unit='s')\n    df_valid = df_train[(df_train['datetime'] > '2021-06-13 00:00:00')].reset_index(drop=True)\n    df_train = df_train[(df_train['datetime'] < '2021-06-13 00:00:00')].reset_index(drop=True)\n    #df_valid = df_train[(df_train['datetime'] > '2021-06-13 00:00:00') | (df_train['datetime'] < '2019-01-01 00:00:00')]\n    #df_train = df_train[(df_train['datetime'] < '2021-06-13 00:00:00') | (df_train['datetime'] > '2019-01-01 00:00:00')]\n    #print('delete data  ','train=',len(df_train),'  valid=',len(df_valid))\n    df_train  = df_train.drop(['datetime'],axis=1)\n    df_valid  = df_valid.drop(['datetime'],axis=1)\nelse:\n    df_train['datetime'] = pd.to_datetime(df_train['timestamp'], unit='s')\n    df_valid = df_train[(df_train['datetime'] > '2021-06-13 00:00:00')].reset_index(drop=True)\n    df_train  = df_train.drop(['datetime'],axis=1)\n    df_valid  = df_valid.drop(['datetime'],axis=1)\n","a1d5ee30":"df_train = df_train.dropna(subset=['Target']).reset_index(drop=True)\ndf_valid = df_valid.dropna(subset=['Target']).reset_index(drop=True)\n#bkup =df_valid.copy()","eaf3d699":"#df_train = df_train.sample(n=len(df_train), random_state=0)\ndf_valid = df_valid.sample(n=len(df_valid), random_state=0)","b6f9ac02":"df_valid","a44a5d30":"df_asset_details = pd.read_csv(ASSET_DETAILS_CSV).sort_values(\"Asset_ID\")\ndf_asset_details","3a59e49f":"#asset_weight_dict =df_asset_details[['Asset_ID','Weight']].to_dict()\n#df_train['Weight'] = df_train['Asset_ID'].map(asset_weight_dict['Weight'])\n#df_valid['Weight'] = df_valid['Asset_ID'].map(asset_weight_dict['Weight'])","c175e95c":"df_train","c0f7828e":"from sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nfrom sklearn.metrics import accuracy_score\n#!pip install optuna\nimport optuna \nimport optuna.integration.lightgbm as lgbo","d6b2234e":"def log_return(series, periods=5):\n    return np.log(series).diff(periods=periods).fillna(0)\n\n# Two features from the competition tutorial\ndef upper_shadow(df): return df['High'] - np.maximum(df['Close'], df['Open'])\ndef lower_shadow(df): return np.minimum(df['Close'], df['Open']) - df['Low']\ndef sigmoid(a):\n    s = 1 \/ (1 + math.e**-a)\n    return s","9c6cd182":"df_train","daa6676b":"df_train.isnull().sum()","ad43a59c":"#%%capture\n#'''\ndf_train = df_train.fillna(-1)\ndf_train.VWAP = df_train.VWAP.replace(np.inf,0)\ndf_train.VWAP = df_train.VWAP.replace(-np.inf,0)\n\ndf_valid = df_valid.fillna(-1)\ndf_valid.VWAP = df_valid.VWAP.replace(np.inf,0)\ndf_valid.VWAP = df_valid.VWAP.replace(-np.inf,0)\n\ndfs = df_train.iloc[:, 2:-1].apply(lambda x: (x-x.mean())\/x.std(), axis=0)\ndf_train.iloc[:, 2:-1] = dfs \n\ndfs = df_valid.iloc[:, 2:-1].apply(lambda x: (x-x.mean())\/x.std(), axis=0)\ndf_valid.iloc[:, 2:-1] = dfs \n#'''","327dfb60":"df_train","e536c076":"%%capture\n'''\n#\u4e3b\u6210\u5206\u5206\u6790\u306e\u5b9f\u884c\npca = PCA()\npca.fit(dfs)\n# \u30c7\u30fc\u30bf\u3092\u4e3b\u6210\u5206\u7a7a\u9593\u306b\u5199\u50cf\nfeature = pca.transform(dfs)\n# \u4e3b\u6210\u5206\u5f97\u70b9\ndata = pd.DataFrame(feature, columns=[\"PC{}\".format(x + 1) for x in range(len(dfs.columns))])\ndel dfs\ndel feature\ndf_train = pd.concat([df_train,data],axis=1)\ndel data\ndf_train\n'''\n#dfs = df_train.iloc[:, 2:-1].apply(lambda x: (x-x.mean())\/x.std(), axis=0)\n#df_train.iloc[:, 2:-1] = dfs \n","1ea2ed5c":"%%capture\n'''\npca = PCA()\npca.fit(dfs)\n\nfeature = pca.transform(dfs)\ndata = pd.DataFrame(feature, columns=[\"PC{}\".format(x + 1) for x in range(len(dfs.columns))])\ndel dfs\ndel feature\ndf_valid = pd.concat([df_valid,data],axis=1)\ndel data\n'''\n#dfs = df_valid.iloc[:, 2:-1].apply(lambda x: (x-x.mean())\/x.std(), axis=0)\n","03e5f6ca":"%%capture\n'''\ndf_train = df_train.fillna(-1)\ndf_train.VWAP = df_train.VWAP.replace(np.inf,0)\ndf_train.VWAP = df_train.VWAP.replace(-np.inf,0)\n\ndf_valid = df_valid.fillna(-1)\ndf_valid.VWAP = df_valid.VWAP.replace(np.inf,0)\ndf_valid.VWAP = df_valid.VWAP.replace(-np.inf,0)\n\nrs = RobustScaler().fit(df_train.iloc[:,2:-1])\ndts = rs.transform(df_train.iloc[:,2:-1])\ndf_train.iloc[:,2:-1] = dts\n\nrs = RobustScaler().fit(df_valid.iloc[:,2:-1])\ndts = rs.transform(df_valid.iloc[:,2:-1])\ndf_valid.iloc[:,2:-1] = dts\n'''","895f41c1":"def hlco_ratio(df): \n    return (df['High'] - df['Low'])\/(df['Close']-df['Open'])\ndef upper_shadow(df):\n    return df['High'] - np.maximum(df['Close'], df['Open'])\ndef lower_shadow(df):\n    return np.minimum(df['Close'], df['Open']) - df['Low']\n","f45abeb5":"def add_features2(df):\n    df['Upper_Shadow'] = upper_shadow(df)\n    df['hlco_ratio'] = hlco_ratio(df)\n    df['Lower_Shadow'] = lower_shadow(df)\n    times = pd.to_datetime(df.index,unit=\"s\",infer_datetime_format=True)\n    \n    #df[\"year\"] = times.year\n    #df[\"month\"] = times.month\n    \n    df[\"hour\"] = times.hour  \n    df[\"dayofweek\"] = times.dayofweek \n    df[\"day\"] = times.day \n    if 1==1:\n        \n        df['open2close'] = df['Close'] \/ df['Open']\n        df['high2low'] = df['High'] \/ df['Low']\n        mean_price = df[['Open', 'High', 'Low', 'Close']].mean(axis=1)\n        median_price = df[['Open', 'High', 'Low', 'Close']].median(axis=1)\n        df['high2mean'] = df['High'] \/ mean_price\n        df['low2mean'] = df['Low'] \/ mean_price\n        df['high2median'] = df['High'] \/ median_price\n        df['low2median'] = df['Low'] \/ median_price\n        df['volume2count'] = df['Volume'] \/ (df['Count'] + 1)\n    \n        df[\"high_div_low\"] = df[\"High\"] \/ df[\"Low\"]\n        df[\"open_sub_close\"] = df[\"Open\"] - df[\"Close\"]\n        \n    return df","f45618b4":"def add_features(df):\n    df['Upper_Shadow'] = upper_shadow(df)\n    df['hlco_ratio'] = hlco_ratio(df)\n    df['Lower_Shadow'] = lower_shadow(df)\n    times = pd.to_datetime(df.index,unit=\"s\",infer_datetime_format=True)\n    \n    df[\"year\"] = times.year\n    df[\"month\"] = times.month\n    \n    df[\"hour\"] = times.hour  \n    df[\"dayofweek\"] = times.dayofweek \n    df[\"day\"] = times.day \n    if 1==1:\n        \n        df['open2close'] = df['Close'] \/ df['Open']\n        df['high2low'] = df['High'] \/ df['Low']\n        mean_price = df[['Open', 'High', 'Low', 'Close']].mean(axis=1)\n        median_price = df[['Open', 'High', 'Low', 'Close']].median(axis=1)\n        df['high2mean'] = df['High'] \/ mean_price\n        df['low2mean'] = df['Low'] \/ mean_price\n        df['high2median'] = df['High'] \/ median_price\n        df['low2median'] = df['Low'] \/ median_price\n        df['volume2count'] = df['Volume'] \/ (df['Count'] + 1)\n    \n        df[\"high_div_low\"] = df[\"High\"] \/ df[\"Low\"]\n        df[\"open_sub_close\"] = df[\"Open\"] - df[\"Close\"]\n        \n        df['time_open'] = df['Open'] * (df['timestamp'] \/1000)\n        df['time_close'] = df['Close']* (df['timestamp'] \/1000)\n        df['time_high'] = df['High']* (df['timestamp'] \/1000)\n        df['time_low'] = df['Low']* (df['timestamp'] \/1000)\n        df['time_vwap'] = df['VWAP']* (df['timestamp'] \/1000)\n        df['time_count'] = df['Count']* (df['timestamp'] \/1000)\n        df['time_volume'] = df['Volume']* (df['timestamp'] \/1000)\n        \n        df['time_open'] = df['Open'].diff().fillna(0)\n        df['time_close'] = df['Close'].diff().fillna(0)\n        df['time_high'] = df['High'].diff().fillna(0)\n        df['time_low'] = df['Low'].diff().fillna(0)\n        df['time_vwap'] = df['VWAP'].diff().fillna(0)\n        df['time_count'] = df['Count'].diff().fillna(0)\n        df['time_volume'] = df['Volume'].diff().fillna(0)\n        \n        #df['time_open_close'] = np.log1p(df['Open'] + df['Close']) * (df['timestamp'] \/1000)\n        \n        ####\n        df['open*2'] = df['Open']**2 * (df['timestamp'] \/1000)#\n        df['close*2'] = df['Close']**2 * (df['timestamp'] \/1000)# \n        df['high*2'] = df['High']**2 * (df['timestamp'] \/1000)#\n        df['low*2'] = df['Low']**2 * (df['timestamp'] \/1000)#\n        df['vwap*2'] = df['VWAP']**2 * (df['timestamp'] \/1000)#\n        df['count*2'] = df['Count']**2 * (df['timestamp'] \/1000)#\n        df['volume*2'] = df['Volume']**2 * (df['timestamp'] \/1000)#\n        \n        \n\n    \n        df['open_close'] = df['Open']*df['Close']* (df['timestamp'] \/1000)\n        df['open_high'] = df['Open']* df['High']* (df['timestamp'] \/1000)\n        df['open_low'] = df['Open']* df['Low']* (df['timestamp'] \/1000)\n        df['open_vwap'] = df['Open']* df['VWAP']* (df['timestamp'] \/1000)\n        df['open_count'] = df['Open']* df['Count'] * (df['timestamp'] \/1000)#\n        df['open_volume'] = df['Open']* df['Volume']* (df['timestamp'] \/1000) #\n        \n        df['close_high'] = df['Close']* df['High']* (df['timestamp'] \/1000)\n        df['close_low'] = df['Close']* df['Low']* (df['timestamp'] \/1000)\n        df['close_vwap'] = df['Close']* df['VWAP']* (df['timestamp'] \/1000)\n        df['close_count'] = df['Close']* df['Count']* (df['timestamp'] \/1000) #\n        df['high_open'] = df['High']* df['Open']* (df['timestamp'] \/1000) #\n        df['high_close'] = df['High']* df['Close']* (df['timestamp'] \/1000) #\n        df['high_low'] = df['High']* df['Low']* (df['timestamp'] \/1000)\n        df['high_vwap'] = df['High']* df['VWAP']* (df['timestamp'] \/1000)\n        df['high_count'] = df['High']* df['Count']* (df['timestamp'] \/1000) #\n        df['high_volume'] = df['High']* df['Volume']* (df['timestamp'] \/1000) #\n        \n        df['low_open'] = df['Low']* df['Open']* (df['timestamp'] \/1000) #\n        df['low_close'] = df['Low']* df['Close']* (df['timestamp'] \/1000) #\n        df['low_vwap'] = df['Low']* df['VWAP']* (df['timestamp'] \/1000)\n        df['low_count'] = df['Low']* df['Count']* (df['timestamp'] \/1000) #\n        df['low_volume'] = df['Low']* df['Volume']* (df['timestamp'] \/1000) #\n        \n        df['count_open'] = df['Count']* df['Open']* (df['timestamp'] \/1000)\n        df['count_close'] = df['Count']* df['Close']* (df['timestamp'] \/1000) #\n        df['count_low'] = df['Count']* df['Low']* (df['timestamp'] \/1000)\n        df['count_high'] = df['Count']* df['High']* (df['timestamp'] \/1000) #\n        df['count_vwap'] = df['Count']* df['VWAP']* (df['timestamp'] \/1000)\n        df['count_volume'] = df['Count']* df['Volume']* (df['timestamp'] \/1000)\n        \n        df['volume_open'] = df['Volume']* df['Open']* (df['timestamp'] \/1000)\n        df['volume_close'] = df['Volume']* df['Close']* (df['timestamp'] \/1000) #\n        df['volume_low'] = df['Volume']* df['Low']* (df['timestamp'] \/1000)\n        df['volume_high'] = df['Volume']* df['High']* (df['timestamp'] \/1000) #\n        df['volume_vwap'] = df['Volume']* df['VWAP']* (df['timestamp'] \/1000)\n        df['volume_count'] = df['Volume']* df['Count']* (df['timestamp'] \/1000) #\n        ###\n        \n        df['open_sigmoid'] = sigmoid(df['Open'])* (df['timestamp'] \/1000)\n        df['close_sigmoid'] = sigmoid(df['Close'])* (df['timestamp'] \/1000)\n        df['high_sigmoid'] = sigmoid(df['High'])* (df['timestamp'] \/1000)\n        df['low_sigmoid'] = sigmoid(df['Low'])* (df['timestamp'] \/1000)\n        df['vwap_sigmoid'] = sigmoid(df['VWAP'])* (df['timestamp'] \/1000)\n        df['count_sigmoid'] = sigmoid(df['Count'])* (df['timestamp'] \/1000)\n        df['volum_sigmoid'] = sigmoid(df['Volume'])* (df['timestamp'] \/1000)\n        \n        df['open_log'] = np.log1p(df['Open'])* (df['timestamp'] \/1000)\n        df['close_log'] = np.log1p(df['Close'])* (df['timestamp'] \/1000)\n        df['high_log'] = np.log1p(df['High'])* (df['timestamp'] \/1000)\n        df['low_log'] = np.log1p(df['Low'])* (df['timestamp'] \/1000)\n        df['vwap_log'] = np.log1p(df['VWAP'])* (df['timestamp'] \/1000)\n        df['count_log'] = np.log1p(df['Count'])* (df['timestamp'] \/1000)\n        df['volum_log'] = np.log1p(df['Volume'])* (df['timestamp'] \/1000)\n        \n        #df['open_log_'] = np.sqrt(df['Open'])* (df['timestamp'] \/1000)\n        #df['close_log_'] = np.sqrt(df['Close'])* (df['timestamp'] \/1000)\n        #df['high_log_'] = np.sqrt(df['High'])* (df['timestamp'] \/1000)\n        #df['low_log_'] = np.sqrt(df['Low'])* (df['timestamp'] \/1000)\n        #df['vwap_log_'] = np.sqrt(df['VWAP'])* (df['timestamp'] \/1000)\n        #df['count_log_'] = np.sqrt(df['Count'])* (df['timestamp'] \/1000)\n        #df['volum_log_'] = np.sqrt(df['Volume'])* (df['timestamp'] \/1000)\n        \n        #df[f'Openlog_10']  = df[f'Open'] \/ ((np.log(np.abs(df[f'Open'])) **10)+(np.log(np.abs(df[f'Close']))**10))#need\n        #df[f'Closelog_10'] = df[f'Close'] \/((np.log(np.abs(df[f'Open'])) **10)+(np.log(np.abs(df[f'Close']))**10))\n        #df[f'Highlog_10'] = df[f'High'] \/ ((np.log(np.abs(df[f'High']))**10)+(np.log(np.abs(df[f'Low']))**10))\n        #df[f'Lowlog_10'] = df[f'Low'] \/ ((np.log(np.abs(df[f'High']))**10)+(np.log(np.abs(df[f'Low']))**10))\n        \n        #df[f'Openlog_2']  = df[f'Open'] \/ ((np.log(np.abs(df[f'Open'])) **2)+(np.log(np.abs(df[f'Close']))**2)) #need\n        #df[f'Closelog_2'] = df[f'Close'] \/((np.log(np.abs(df[f'Open'])) **2)+(np.log(np.abs(df[f'Close']))**2))\n        #df[f'Highlog_2'] = df[f'High'] \/ ((np.log(np.abs(df[f'High']))**2)+(np.log(np.abs(df[f'Low']))**2))\n        #df[f'Lowlog_2'] = df[f'Low'] \/((np.log(np.abs(df[f'High']))**2)+(np.log(np.abs(df[f'Low']))**2))\n        \n        \n        #df['return1'] = df[f'High'] - np.maximum(df[f'Close'], df[f'Open']) #no need\n        #df['return2'] = np.minimum(df[f'Close'], df[f'Open']) - df[f'Low']\n        \n        #df['open_log1'] = log_return(df['Open'],periods=1)\n        #df['open_log4'] = log_return(df['Open'],periods=4)\n        \n        #df['close_log1'] = log_return(df['Close'],periods=1)\n        #df['close_log4'] = log_return(df['Close'],periods=4)\n        \n        #df['high_log1'] = log_return(df['High'],periods=1)\n        #df['high_log4'] = log_return(df['High'],periods=4)\n        \n        #df['low_log1'] = log_return(df['Low'],periods=1)\n        #df['low_log4'] = log_return(df['Low'],periods=4)\n        \n        #df['vwap_log1'] = log_return(df['VWAP'],periods=1)\n        #df['vwap_log4'] = log_return(df['VWAP'],periods=4)     \n        #df['count_log1'] = log_return(df['Count'],periods=1)\n        #df['count_log4'] = log_return(df['Count'],periods=4)\n        \n        #df['volume_log1'] = log_return(df['Volume'],periods=1) #no need@\n        #df['volume_log4'] = log_return(df['Volume'],periods=4)\n        #df[f'Open-change'] = (df[f'Open'] + df[f'High']) \/ (df[f'Close'] +df[f'Low'])  #no need@\n        \n        #df[f'Close-Open-vmap'] = (df[f'Open'] + df[f'Close']) \/df[f'VWAP']\n        #df['volume_Count'] = df['Count'] \/(df['Count']+df['Volume'])   #no need@\n        #df['vwap_open'] = df['VWAP']* df['Open']\n        #df['vwap_close'] = df['VWAP']* df['Close']\n        #df['vwap_low'] = df['VWAP']* df['Low']\n        #df['vwap_high'] = df['VWAP']* df['High']\n        #df['vwap_count'] = df['VWAP']* df['Count']\n        #df['vwap_volume'] = df['VWAP']* df['Volume']\n        \n        \n        #df[f'Openlog']  = np.log1p(df[f'Open'])\n        #df[f'Closelog'] = np.log1p(df[f'Close'])\n        #df[f'Highlog'] = np.log1p(df[f'High'])\n        #df[f'Lowlog'] = np.log1p(df[f'Low'])\n        #df[f'VWAPlog'] = np.log1p(df[f'VWAP'])\n        #df[f'Countlog'] = np.log1p(df[f'Count'])\n        \n        #df[f'Openlog_4']  = df[f'Open'] \/ ((np.log(df[f'Open']) **4)+(np.log(df[f'Close'])**4)) #no need @\n        #df[f'Closelog_4'] = df[f'Close'] \/((np.log(df[f'Open']) **4)+(np.log(df[f'Close'])**4))\n        #df[f'Highlog_4'] = df[f'High'] \/ ((np.log(df[f'High'])**4)+(np.log(df[f'Low'])**4))\n        #df[f'Lowlog_4'] = df[f'Low'] \/((np.log(df[f'High'])**4)+(np.log(df[f'Low'])**4))\n        #df[f'VWAPlog_4'] = df[f'VWAP'] \/ ((np.log(df[f'VWAP'])**4)+(np.log(df[f'Count'])**4))\n        \n        #df = df.drop(['Openlog','Closelog','Highlog','Lowlog','VWAPlog','Countlog'],axis=1)# no need\n        #df['RSI'] = ta.RSI(df['Close'], timeperiod=14) #no need\n        #df['MACD'], df['macdsignal'], df['MACD_HIST'] = ta.MACD(df['Close'], fastperiod=12, slowperiod=26, signalperiod=9)\n        #df['adx'] = ta.ADX(df['High'], df['Low'],np.array(df.loc[:, 'Close']),timeperiod=14)\n        #df['DI_plus'] = ta.PLUS_DI(df['High'], df['Low'],np.array(df.loc[:, 'Close']), timeperiod=14)\n        #df['DI_minus'] = ta.MINUS_DI(df['High'], df['Low'],np.array(df.loc[:, 'Close']), timeperiod=14)\n        \n    return df \n","cf174a7b":"def get_Xy_and_model_for_asset(df_train,asset_id):\n    df = df_train[df_train[\"Asset_ID\"] == asset_id]\n    df = df.dropna(subset=['Target'])\n    y = df['Target'] \n    #y= np.log1p(y)\n    \n    df      = df.drop(['Target','Asset_ID'],axis=1)\n    #df_proc = get_features(df)\n    df_proc = add_features(df)\n    df_proc = df_proc.fillna(-1)\n    X= df_proc #.drop(\"y\", axis=1)\n    \n    \n    '''\n    params = {'objective': 'regression',  'metric': 'rmse' } #'objective': 'mean_squared_error',\n    \n\n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    lgb_train = lgb.Dataset(x_train, y_train)\n    lgb_valid = lgb.Dataset(x_test, y_test)\n    optuna.logging.disable_default_handler()\n    model = lgbo.train(params, lgb_train, valid_sets=[lgb_valid], verbose_eval=False, num_boost_round=100, early_stopping_rounds=5)\n    #model.params\n    \n    #best_lgb_params ={'objective': 'regression',\n    #'metric': 'rmse',\n    #'feature_pre_filter': False,\n    #'lambda_l1': 0.010565309968664168,\n    #'lambda_l2': 0.3120057367604998,\n    #'num_leaves': 255,\n    #'feature_fraction': 1.0,\n    #'bagging_fraction': 1.0,\n    #'bagging_freq': 0,\n    #'min_child_samples': 5}\n\n    best_lgb_params =model.params\n    best_lgb_params[\"learning_rate\"] = 0.01\n\n    best_lgb_params[\"bagging_freq\"] = 0\n    #best_lgb_params[\"device\"] = 'gpu'\n    best_lgb_params[\"early_stopping_round\"] = 100\n    best_lgb_params[\"num_iterations\"] = 2000\n    lgb_train = lgb.Dataset(x_train, y_train)\n    lgb_valid = lgb.Dataset(x_test, y_test)\n     \n    model = lgb.train(best_lgb_params,lgb_train, valid_sets=[lgb_valid], verbose_eval=100)\n    '''\n    best_params = {\n    \"objective\": \"regression\",\n    \"n_estimators\" : 5000,     # <-- (9) change from 200 to 500\n    \"num_leaves\" : 300,       # <-- (10) Added parameter\n    \"learning_rate\" : 0.09,   # <-- (10) Added parameter\n    \"random_seed\" : 1234}\n    #model = LGBMRegressor(n_estimators=10)\n    model = LGBMRegressor(n_estimators=1500,num_leaves=700,learning_rate=0.1,silent=True)\n    #model = LGBMRegressor(**best_params)\n    model.fit(X, y)\n    \n    #print( 'step3',datetime.datetime.now().strftime('%Y\u5e74%m\u6708%d\u65e5 %H:%M:%S'))\n    fi =model.feature_importances_\n    fi_df = pd.DataFrame({'feature': list(X.columns),\n         'feature importance': fi[:]}).sort_values('feature importance', ascending = False)\n    if asset_id ==0:\n        display(fi_df)\n    return model","7e198fe6":"Xs = {}\nys = {}\nmodels = {}\n\nfor asset_id, asset_name in zip(df_asset_details['Asset_ID'], df_asset_details['Asset_Name']):\n    print(f\"Training model for {asset_name:<16} (ID={asset_id:<2})   \",datetime.datetime.now().strftime('%Y\/%m\/%d %H:%M:%S'))\n            \n    models[asset_id]= get_Xy_and_model_for_asset(df_train, asset_id)    \n    \n    #validation\n   \n    record = df_valid[df_valid.Asset_ID == asset_id]   \n    target = record.Target \n    #target = np.log1p(target)\n    \n    x_test = record.drop(['Target','Asset_ID'],axis=1)\n    model = models[asset_id]\n    #x_test = get_features(record)\n    x_test = add_features(x_test) \n    #x_test = x_test.drop(['Asset_ID','Target'],axis=1)\n        \n    x_pred = model.predict(x_test)\n    print('Test score for LR baseline: ', f\"{np.corrcoef(x_pred, target)[0,1]:.5f}\")\n    #print('MSE=',MSE(x_pred.x, target))\n    #print(x_pred.x[:10])\n    #print(target[:10])\n    \n    \n    del record\n    del x_pred\n    del x_test\n    del target\n    \n   ","18fd47f2":"import pickle\nwith open('single_models','wb') as web:\n    pickle.dump(models,web)","708a1d57":"    x_pred = pd.DataFrame()\n    for x in range(len(df_valid.Asset_ID.unique())):\n        print( 'Asset_ID=',x,' ',datetime.datetime.now().strftime('%Y\/%m\/%d\/ %H:%M:%S'))\n        record = df_valid[df_valid.Asset_ID == x]     \n        x_test = record.drop(['Target','Asset_ID'],axis=1)\n        model = models[x]\n        #x_test = get_features(record)\n        x_test = add_features(pd.DataFrame(x_test)) #.values\n        #x_test = x_test.drop(['Asset_ID','Target'],axis=1)\n\n        x_test['y_pred'] = model.predict(x_test)\n        x_pred = pd.concat([x_test,x_pred])    \n    x_pred = x_pred.sort_index()    \n    print(datetime.datetime.now().strftime('%Y\/%m\/%d %H:%M:%S'),'Test score for LR baseline: ', f\"{np.corrcoef(x_pred.y_pred,df_valid.Target)[0,1]:.5f}\")","9d7fba53":"---------------","294bce51":"## Feature Function \u3092\u4f5c\u308b\n\n## Make Feature Function","6fcb1012":"### Target\u304cNan\u306e\u3082\u306e\u3092\u524a\u9664","45901e2d":"\ud83d\ude3a\ud83d\ude05\u3299\ud83d\udd30\ud83d\uddd1\u2b1b\ud83d\udfe5\ud83d\udfe8\ud83d\udfe9","53144ec7":"# RobustScaler","ece1e924":"--------------------------------","21cc2e06":"## \u5b66\u7fd2\u30c7\u30fc\u30bf\u306e\u975e\u9023\u7d9a\u5316\n\n## Discontinuity of training data","1fab2e27":"![image.png](attachment:11d7f4d8-3b08-4b4c-a077-6661dcad5152.png)","b3bb61d2":"# model save","1aec009c":"# Validation","e817cf9c":"## Training & Validation","f32f2967":"### \u30c7\u30fc\u30bf\u3092\u76f4\u8fd1\u306e\u3082\u306e\u3060\u3051\u306b\u3059\u308b\u3000\u4eca\u306f\u7121\u52b9\u5316\n\n### Make the data only the latest one. Now disabled","dbe12ca5":"# In Progress","c82f5d75":"# \u884c\u5217\u306e\u6a19\u6e96\u5316,(PCA)","3a7cdee8":"<pre>\nTest score for LR baseline:  0.01367\nTraining model for Bitcoin          (ID=1 )    2022\/01\/01 03:12:43\nTest score for LR baseline:  0.02114\nTraining model for Bitcoin Cash     (ID=2 )    2022\/01\/01 03:18:24","fbd29e15":"2022\/01\/01 04:40:14 Test score for LR baseline:  0.00101","0d51996a":"### \u30b3\u30a4\u30f3\u6bce\u3067\u306f\u306a\u304f\u3001\u30c8\u30fc\u30bf\u30eb\u306eValidation\u30b9\u30b3\u30a2\u3092\u8a08\u7b97\u3057\u307e\u3059\u3002\n\n### Calculate the total Validation score, not per coin.","f044c80a":"## Leak\u306b\u306a\u308b\u30c7\u30fc\u30bf\u3092\u5916\u3059\u3002valid\u30c7\u30fc\u30bf\u3092\u4f5c\u308b\u3002\n\n## Remove the data that becomes Leak. Create valid data."}}