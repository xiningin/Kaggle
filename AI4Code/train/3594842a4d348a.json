{"cell_type":{"69efcb0d":"code","5d76f1ed":"code","4527a76b":"code","15b3603f":"code","f20308ef":"code","1f426c79":"code","ba7f9aeb":"code","c92630e1":"code","29ae5b58":"code","c714ec9a":"code","f8c3b8d0":"code","3a27662d":"code","c3a811a2":"code","4ffb2e71":"code","814f09af":"code","06aa4a5f":"code","21fda605":"code","82d2d07f":"code","73d9ad62":"code","b03cd4df":"code","b57fcd76":"code","dec0b5d0":"code","a82d1763":"code","50013cd9":"code","2f228496":"code","90f9b7e1":"code","132895b3":"code","a1fca410":"code","0a5cedb9":"code","174539a6":"code","c2334fe0":"code","216a2ee4":"markdown","2a57a8a7":"markdown","f9072db2":"markdown","93074b33":"markdown","1d655268":"markdown","6ca1fbb9":"markdown","d051a1f7":"markdown","0addbb08":"markdown","5b75167a":"markdown","4fa14020":"markdown"},"source":{"69efcb0d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","5d76f1ed":"# Data loading\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ngender_submission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")","4527a76b":"# Number of rows and columns\nprint('Train', train.shape)\nprint('Test', test.shape)","15b3603f":"train.head()","f20308ef":"train.describe()","1f426c79":"test.head()","ba7f9aeb":"test.describe()","c92630e1":"# Quantity of null values\ntrain_missing1 = train.isnull().sum()\ntest_missing1 = test.isnull().sum()\n\n# Propotion of null values\ntrain_missing2 = round(train.isnull().sum() \/ len(train) * 100)\ntest_missing2 = round(test.isnull().sum() \/ len(test) * 100)\n\n# Show as a table\nmissing_values = pd.DataFrame({'Num of null (train)' : train_missing1, '% of null (train)' : train_missing2, 'Num of null (test)' : test_missing1, '% of null (test)' : test_missing2})\nmissing_values","29ae5b58":"# Distribution of survivers in age\ntrain['Age'].fillna(train.median(), inplace=True)\nx1 = train['Age'][train['Survived'] == 1]\nx2 = train['Age'][train['Survived'] == 0]\n\nplt.figure(figsize=(10,6))\nplt.hist(x1, bins=10, color='red', alpha=0.4)\nplt.hist(x2, bins=10, color='blue',alpha=0.4)\nplt.title('Histogram of age: survived vs. not survived group')\nplt.xlabel('Age')\nplt.legend(['Survived', 'Not survived'])\nplt.show()","c714ec9a":"# Comparison of gender between survived and not survived groups\nfrom statsmodels.graphics.mosaicplot import mosaic\nplt.rcParams[\"figure.figsize\"]=(10, 6)\ngender_colors = lambda key: {'color': 'chocolate' if 'female' in key else 'lightblue'}\nmosaic(train, ['Sex', 'Survived'], title='Proportion of gender: survived vs. not survived\\n1: Survived, 0: Not survived', \n       properties = gender_colors, gap = 0.02)\nplt.show()","f8c3b8d0":"# Comparison of ticket class between survived and not survived groups\ntrain['Pclass2'] = train['Pclass'].replace(1, '1st class').replace(2, '2nd class').replace(3, '3rd class')\n\nplt.rcParams[\"figure.figsize\"]=(10, 6)\nclass_colors = lambda key: {'color': 'lightblue' if '1st class' in key else ('chocolate' if '2nd class' in key else 'lightgreen')}\nmosaic(train.sort_values('Pclass2', ascending=True).sort_values('Survived'), ['Pclass2', 'Survived'], title='Proportion of passenger class: survived vs. not survived\\n1: Survived, 0: Not survived', \n       properties = class_colors, gap = 0.02)\nplt.show()","3a27662d":"# Age: Fill null values with median\ntrain['Age'].fillna(train['Age'].median(), inplace=True)\ntest['Age'].fillna(test['Age'].median(), inplace=True)\n\n# Cast data type from float to int\ntrain['Age'] = train['Age'].astype(int)\ntest['Age'] = test['Age'].astype(int)","c3a811a2":"# Embarked: Fill null values with mode\ntrain['Embarked'].fillna(train['Embarked'].mode()[0], inplace=True)","4ffb2e71":"# Fare: Fill null values with median\ntest['Fare'].fillna(test['Fare'].median(), inplace=True)","814f09af":"# Transform categorical variables in Sex and Embarked columns to numerical variables\ntrain['Sex'] = train['Sex'].replace('male', 0).replace('female', 1)\ntrain['Embarked'] = train['Embarked'].replace('S', 1).replace('C', 2).replace('Q', 3)\n\ntest['Sex'] = test['Sex'].replace('male', 0).replace('female', 1)\ntest['Embarked'] = test['Embarked'].replace('S', 1).replace('C', 2).replace('Q', 3)\n\ntrain.head()","06aa4a5f":"# Create Accompanying feature (1 if passengers was accompanied by siblings or child)\ntrain['Accompanying'] = 1\ntrain.loc[(train['SibSp'] == 0) & (train['Parch'] == 0), 'Accompanying'] = 0\n\ntest['Accompanying'] = 1\ntest.loc[(test['SibSp'] == 0) & (test['Parch'] == 0), 'Accompanying'] = 0","21fda605":"# Create Family feature (sum of sibling and child) and Family_group feature (binning)\n\n# Binning family size\ndef f_group(x):\n  if x == 0:\n    return 0\n  elif x >= 1 and x < 4:\n    return 1\n  elif x >= 4 and x < 7:\n    return 2\n  elif x >= 7 and x < 9:\n    return 3\n  else:\n    return 4\n\ntrain['Family'] = train['SibSp'] + train['Parch']\ntrain['Family_group'] = train['Family'].map(f_group)\n\ntest['Family'] = test['SibSp'] + test['Parch']\ntest['Family_group'] = test['Family'].map(f_group)","82d2d07f":"# Create Age_group feature (Binning)\n\n# Binning age group\ndef a_group(x):\n  if x < 10:\n    return 0\n  elif x >= 10 and x < 30:\n    return 1\n  elif x >= 20 and x < 30:\n    return 2\n  elif x >= 30 and x < 40:\n    return 3\n  elif x >= 40 and x < 50:\n    return 4\n  elif x >= 50 and x < 60:\n    return 5\n  elif x >= 60 and x < 70:\n    return 6\n  elif x >= 70 and x < 80:\n    return 7\n  elif x >= 80 and x < 90:\n    return 8\n  else:\n    return 9\n\ntrain['Age_group'] = train['Age'].map(a_group)\ntest['Age_group'] = test['Age'].map(a_group)","73d9ad62":"train.head()","b03cd4df":"# Use only numerical attributes\ntrain2 = train[['Survived', 'Pclass', 'Sex', 'Age', 'Age_group', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Accompanying', 'Family', 'Family_group']].copy()\ntest2 = test[['Pclass', 'Sex', 'Age', 'Age_group', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Accompanying', 'Family', 'Family_group']].copy()","b57fcd76":"# Check the correlation coefficients of the attributes\nplt.subplots(figsize=(8, 8))\nmask = np.triu(train2.corr())\nsns.heatmap(train2.corr(), annot=True, mask=mask, vmin=-1, vmax=1, cmap='coolwarm', square=True)\nplt.show()","dec0b5d0":"# Check potentially impactful attributes using Random Forest Classifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\ndata_std = train2.drop('Survived', axis=1)\ntarget = np.array(train['Survived'])\n\n# Split dataset 70 : 30\nX_train, X_test, y_train, y_test = train_test_split(data_std, target, random_state=0, train_size=0.7)\n\n# Set Random Forest Classifier\nrfc = RandomForestClassifier(random_state=0)\nrfc.fit(X_train, y_train)\n\n# Visualize the importance ranking\nplt.figure(figsize=(8,5))\nplt.barh(\n    X_train.columns[np.argsort(rfc.feature_importances_)],\n    rfc.feature_importances_[np.argsort(rfc.feature_importances_)],\n    label='RandomForestClassifier'\n)\nplt.title('Random Forest Classifier feature importance')\nplt.show()","a82d1763":"# Combinations of attribute to train\n# Combination 1 \nc1 = train2[['Sex', 'Fare', 'Age']] \n\n# Combination 2\nc2 = train2[['Sex', 'Fare', 'Age', 'Pclass']] \n\ncombinations = [c1, c2]\ntarget = np.array(train['Survived'])\n","50013cd9":"# model selection\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\n\n# models\nfrom sklearn.tree import DecisionTreeClassifier # Decision tree\nfrom sklearn import tree # Decision tree\n\n# performance indicators\nfrom sklearn.metrics import classification_report","2f228496":"# Set different patterns of featrure combinations\ntree_features = combinations \n\n# Hyper parameter search using grid search\nparam_grid = {\n    \"max_depth\": [3, 5, 10, 15, 20, None],\n    \"max_leaf_nodes\":  [2, 4, 6, 8, 10]\n}\n\nfor i in range(len(tree_features)):\n  # set a combination\n  attempt = tree_features[i]\n  print(\"\u25a0\u25a0\u25a0 Combination \" + str(i+1) + \": \" + str(', '.join(map(str, attempt.columns))))\n\n  # Split training dataset\n  X_train, X_test, y_train, y_test = train_test_split(attempt, target, random_state=0, train_size=0.7)\n\n  # Model fitting using grid search\n  cross_val = 10\n  dtree = DecisionTreeClassifier()\n  grid_cv = GridSearchCV(dtree, param_grid, scoring=\"roc_auc\", n_jobs=-1, cv=cross_val).fit(X_train, y_train) \n  grid_cv_accuracy = GridSearchCV(dtree, param_grid, scoring=\"accuracy\", n_jobs=-1, cv=cross_val).fit(X_train, y_train) \n\n  print(\"Selected parameters by Grid Search\", grid_cv.best_params_)\n  print(str(cross_val) + \"-fold Cross-Validation: Mean ROC AUC of the best estimator=\" + str(round(grid_cv.best_score_, 2)) + \", SD=\" + str(round(grid_cv.cv_results_['std_test_score'][grid_cv.best_index_], 2)))\n  print(str(cross_val) + \"-fold Cross-Validation: Mean Accuracy of the best estimator=\" + str(round(grid_cv_accuracy.best_score_, 2)) + \", SD=\" + str(round(grid_cv_accuracy.cv_results_['std_test_score'][grid_cv_accuracy.best_index_], 2)))\n  print('\\n')","90f9b7e1":"pip install pydotplus","132895b3":"import six\nimport sys\nsys.modules['sklearn.externals.six'] = six","a1fca410":"# Draw a tree with the best combination\nimport pydotplus\nfrom IPython.display import Image\nfrom graphviz import Digraph\nfrom sklearn.externals.six import StringIO\n\n# Set the second combination that showed better accuracy and ROC AUC\nattempt = c2\n\n# Split training dataset\nX_train, X_test, y_train, y_test = train_test_split(attempt, target, random_state=42, train_size=0.7)\n\ndt_pre = tree.DecisionTreeClassifier(max_depth=5, max_leaf_nodes=10)\ndt_pre = dt_pre.fit(X_train, y_train)\npredict_tree = dt_pre.predict(X_test)\n\n# Visualization\ndot_data = StringIO()\ntree.export_graphviz(dt_pre, out_file=dot_data, feature_names=X_train.columns)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())\nImage(graph.create_png())","0a5cedb9":"# Search an optimal ccp_alpha, a hyper parameter for post pruning\n\nfor i in range(len(combinations)):\n  # set a combination\n  attempt = tree_features[i]\n  print(\"\u25a0\u25a0\u25a0 Combination \" + str(i+1) + \": \" + str(', '.join(map(str, attempt.columns))))\n\n  # Split training dataset\n  X_train, X_test, y_train, y_test = train_test_split(attempt, target, random_state=0, train_size=0.7)\n\n  # Allow a decision tree to grow to its full depth\n  dt = DecisionTreeClassifier()\n  dt.fit(X_train, y_train)\n\n  # Compute ccp_alpha values\n  path = dt.cost_complexity_pruning_path(X_train, y_train)\n  ccp_alphas, impurities = path.ccp_alphas, path.impurities\n\n  # Train Decision Tree classifier for each ccp_alpha value\n  clfs = []\n  for ccp_alpha in ccp_alphas:\n    dt2 = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n    dt2.fit(X_train, y_train)\n    clfs.append(dt2)\n\n  # Plot train and test score for each of the above trained model    \n  clfs = clfs[:-1]\n  ccp_alphas = ccp_alphas[:-1]\n\n  train_scores = [clf.score(X_train, y_train) for clf in clfs]\n  test_scores = [clf.score(X_test, y_test) for clf in clfs]\n\n  fig, ax = plt.subplots()\n  ax.set_xlabel(\"alpha\")\n  ax.set_ylabel(\"accuracy\")\n  ax.set_title(\"Accuracy vs alpha\")\n  ax.plot(ccp_alphas, train_scores, marker='o', label=\"train\", drawstyle=\"steps-post\")\n  ax.plot(ccp_alphas, test_scores, marker='o', label=\"evaluation\", drawstyle=\"steps-post\")\n  ax.legend()\n  #plt.xlim(0, 0.02)\n  plt.ylim(0.7, 1)\n  plt.show()","174539a6":"# Simulate each combination using the optimal ccp_alpha\n\n# Set the best ccp_alphas for each combination\nselected_ccp_alphas = [0.0022, 0.004] \n\nfor i in range(len(combinations)):\n  # set a combination\n  attempt = combinations[i]\n  print(\"\u25a0\u25a0\u25a0 Combination \" + str(i+1) + \": \" + str(', '.join(map(str, attempt.columns))))\n\n  # Split training dataset\n  X_train, X_test, y_train, y_test = train_test_split(attempt, target, random_state=0, train_size=0.7)\n\n  # Allow a decision tree to grow to its full depth\n  dt_post = DecisionTreeClassifier()\n  dt_post.fit(X_train, y_train)\n\n  # Apply the best ccp_alpha for decision tree\n  cross_val = 10\n  dt_post2 = DecisionTreeClassifier(ccp_alpha=selected_ccp_alphas[i])\n  cv_score1 = cross_val_score(dt_post2, X_train, y_train, cv=cross_val, scoring=\"roc_auc\")\n  cv_score2 = cross_val_score(dt_post2, X_train, y_train, cv=cross_val, scoring=\"accuracy\")\n\n  print(str(cross_val) + \"-fold Cross-Validation: Mean of the ROC AUC score=\", str(round(cv_score1.mean(), 2)) + \" *SD=\" + str(round(cv_score1.std(), 2)))\n  print(str(cross_val) + \"-fold Cross-Validation: Mean of the accuracy score=\", str(round(cv_score2.mean(), 2)) + \" *SD=\" + str(round(cv_score2.std(), 2)))\n  print('\\n')\n","c2334fe0":"# Visualization of post pruned tree\n# Set the second combination that showed better accuracy and ROC AUC\nattempt = c2\n\n# Split training dataset\nX_train, X_test, y_train, y_test = train_test_split(attempt, target, random_state=0, train_size=0.7)\n\ndt_post_visu = tree.DecisionTreeClassifier(ccp_alpha=0.004) # set the best ccp_alphas for each combination\ndt_post_visu = dt_post_visu.fit(X_train, y_train)\npredict_tree = dt_post_visu.predict(X_test)\n\n# Visualization\ndot_data = StringIO()\ntree.export_graphviz(dt_post_visu, out_file=dot_data, feature_names=X_train.columns)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())\nImage(graph.create_png())","216a2ee4":"## Visualization","2a57a8a7":"# Feature Selection","f9072db2":"## Pre pruning","93074b33":"## Post pruning","1d655268":"## Deal with null values","6ca1fbb9":"# Decision Tree (CART)","d051a1f7":"# Preprocessing","0addbb08":"# Data checking","5b75167a":"## Feature engineering","4fa14020":"## Dummy coding"}}