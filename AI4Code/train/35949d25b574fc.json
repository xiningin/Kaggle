{"cell_type":{"2b808378":"code","a3b6ea7a":"code","ebb8360d":"code","12a9eecf":"code","4142a912":"code","fb9bceda":"code","76a86750":"code","59547c9f":"code","c347c38d":"code","1e75a4df":"code","8e0f6133":"code","0a2bfbe5":"code","d10a2d6e":"code","ee6ca55e":"code","64f6d59d":"code","93e54904":"code","71346db5":"code","bb559b12":"code","0754642c":"code","a05a8575":"code","d28d7135":"code","78db1e69":"code","6c8549a7":"code","967b7cfc":"code","e38d7cc2":"code","3f53a186":"code","d535bf48":"code","603b1e72":"code","2686070b":"code","5d6d8b09":"code","3d0421a1":"code","27ddaa2f":"code","a1ae781d":"code","11abff6f":"code","01339a6b":"code","18efc8dd":"code","99b83bb2":"code","28fe14fe":"code","eaf6fe13":"code","e3321b2d":"code","df62d1eb":"code","3eb45d7b":"code","a2bbfdfa":"code","2f255e14":"code","d6b8ab2d":"code","210e4465":"code","cb92064e":"code","ad9cd257":"code","3f3cee26":"code","ec6d902f":"code","4db75716":"code","cdf097e0":"code","3c728b02":"code","77bf19a6":"code","d1246f47":"code","7b9f55c5":"code","79d50b87":"code","91650e80":"code","6f3c3f14":"code","6be3849a":"code","7b8900ff":"code","13f6cd4b":"code","92613cf9":"code","a0c55bba":"code","f125df52":"code","3ad36e49":"code","7ced339e":"code","6822ea38":"code","f2504f0f":"code","a140519c":"code","61f3b5af":"code","2931b095":"code","cb76e44e":"code","7bedb112":"code","1fe8b688":"code","add84d47":"markdown","b9d0c5eb":"markdown","949efbbf":"markdown","5f192d5c":"markdown","5275d417":"markdown","0fd71750":"markdown","a1285b5f":"markdown","7abe0248":"markdown","0a4c1ca4":"markdown","9d475a71":"markdown","aad9ddf9":"markdown","a1c3e086":"markdown","5418aa84":"markdown","77509cc7":"markdown","a74c6b3c":"markdown","ee4a1486":"markdown","3d9032ff":"markdown"},"source":{"2b808378":"# Use the official tokenization script created by the Google team\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","a3b6ea7a":"import tensorflow as tf\nimport timeit\n\ndevice_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","ebb8360d":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\n!pip install sentencepiece\nimport tokenization\n\n# text processing libraries\nimport re\nimport string\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\n# sklearn \nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n \nimport warnings\nwarnings.filterwarnings('ignore')","12a9eecf":"train_aug = pd.read_csv('..\/input\/augmented-data-bdc\/train_aug.csv')\ntrain_aug.drop('Unnamed: 0', axis=1, inplace=True)\ntrain_aug.head()","4142a912":"#Training data\ntrain = pd.read_excel('..\/input\/fixed-bdc\/Data BDC - Satria Data 2020\/Data Latih\/Data Latih BDC.xlsx')\nprint('Training data shape: ', train.shape)\ntrain.head()","fb9bceda":"# Testing data \ntest = pd.read_excel('..\/input\/fixed-bdc\/Data BDC - Satria Data 2020\/Data Uji\/Data Uji BDC.xlsx')\nprint('Testing data shape: ', test.shape)\ntest.head()","76a86750":"train['text'] = train['judul'] + ' ' + train['narasi']\ntest['text'] = test['judul'] + ' ' + test['narasi']","59547c9f":"train = train.drop(columns=['ID', 'tanggal', 'judul', 'narasi', 'nama file gambar'])\ntest = test.drop(columns=['ID', 'tanggal', 'judul', 'narasi', 'nama file gambar'])\n\nprint(train.head())\nprint(test.head())","c347c38d":"#Missing values in training set\ntrain.isnull().sum()","1e75a4df":"#Missing values in test set\ntest.isnull().sum()","8e0f6133":"train['label'].value_counts()","0a2bfbe5":"# take copies of the data to leave the originals for BERT\ntrain1 = train.copy()\ntest1 = test.copy()","d10a2d6e":"# Applying a first round of text cleaning techniques\n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower() # make text lower case\n    text = re.sub('\\[.*?\\]', '', text) # remove text in square brackets\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text) # remove URLs\n    text = re.sub('<.*?>+', '', text) # remove html tags\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) # remove punctuation\n    text = re.sub('\\n', '', text) # remove words conatinaing numbers\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = re.sub('[\u2018\u2019\u201c\u201d\u2026]', '', text)\n\n    return text","ee6ca55e":"# emoji removal\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n# Applying the de=emojifying function to both test and training datasets\ntrain1['text'] = train1['text'].apply(lambda x: remove_emoji(x))\ntest1['text'] = test1['text'].apply(lambda x: remove_emoji(x))","64f6d59d":"# text preprocessing function\ndef text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenizer_reg = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    \n    nopunc = clean_text(text)\n    tokenized_text = tokenizer_reg.tokenize(nopunc)\n    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = ' '.join(remove_stopwords)\n    return combined_text\n\n# Applying the cleaning function to both test and training datasets\ntrain1['text'] = train1['text'].apply(lambda x: text_preprocessing(x))\ntest1['text'] = test1['text'].apply(lambda x: text_preprocessing(x))\n\n# Let's take a look at the updated text\ntrain1['text'].head()","93e54904":"train_full = pd.concat([train1, train_aug])","71346db5":"train_full.head()","bb559b12":"#count_vectorizer = CountVectorizer()\ncount_vectorizer = CountVectorizer(ngram_range = (1,2), min_df = 1)\ntrain_vectors = count_vectorizer.fit_transform(train1['text'])\ntest_vectors = count_vectorizer.transform(test1[\"text\"])\ntrain_aug_vectors = count_vectorizer.transform(train_full['text'])\n\n## Keeping only non-zero elements to preserve space \ntrain_vectors.shape","0754642c":"train_vectors","a05a8575":"tfidf = TfidfVectorizer(ngram_range=(1, 2), min_df = 2, max_df = 0.5)\ntrain_tfidf = tfidf.fit_transform(train1['text'])\ntest_tfidf = tfidf.transform(test1[\"text\"])\ntrain_aug_tfidf = tfidf.fit_transform(train_full['text'])\n\ntrain_tfidf.shape","d28d7135":"# Fitting a simple Logistic Regression on BoW\nlogreg_bow = LogisticRegression(C=1.0)\nlogreg_bow.fit(train_vectors, train[\"label\"])\nscores = model_selection.cross_val_score(logreg_bow, train_vectors, train[\"label\"], cv=5, scoring=\"f1\")\nscores.mean()","78db1e69":"# Fitting a simple Logistic Regression on TFIDF\nlogreg_tfidf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(logreg_tfidf, train_tfidf, train[\"label\"], cv=5, scoring=\"f1\")\nscores.mean()","6c8549a7":"# Fitting a simple Naive Bayes on BoW\nNB_bow = MultinomialNB()\nNB_bow.fit(train_vectors, train[\"label\"])\nscores = model_selection.cross_val_score(NB_bow, train_vectors, train[\"label\"], cv=5, scoring=\"f1\")\nscores.mean()","967b7cfc":"# Fitting a simple Naive Bayes on TFIDF\nNB_tfidf = MultinomialNB()\nscores = model_selection.cross_val_score(NB_tfidf, train_tfidf, train[\"label\"], cv=5, scoring=\"f1\")\nscores.mean()","e38d7cc2":"# Fitting a simple Random Forest on BoW\nRF_bow = RandomForestClassifier()\nRF_bow.fit(train_vectors, train[\"label\"])\nscores = model_selection.cross_val_score(RF_bow, train_vectors, train[\"label\"], cv=5, scoring=\"f1\")\nscores.mean()","3f53a186":"# Fitting a simple Random Forest on TFIDF\nRF_tfidf = RandomForestClassifier()\nscores = model_selection.cross_val_score(RF_tfidf, train_tfidf, train[\"label\"], cv=5, scoring=\"f1\")\nscores.mean()","d535bf48":"# Fitting a simple SVC on BoW\nSVC_bow = SVC()\nSVC_bow.fit(train_vectors, train[\"label\"])\nscores = model_selection.cross_val_score(SVC_bow, train_vectors, train[\"label\"], cv=5, scoring=\"f1\")\nscores.mean()","603b1e72":"# Fitting a simple SVC on TFIDF\nSVC_tfidf = SVC()\nscores = model_selection.cross_val_score(SVC_tfidf, train_tfidf, train[\"label\"], cv=5, scoring=\"f1\")\nscores.mean()","2686070b":"# Fitting a simple XGB on BoW\nXGB_bow = XGBClassifier()\nXGB_bow.fit(train_vectors, train[\"label\"])\nscores = model_selection.cross_val_score(XGB_bow, train_vectors, train[\"label\"], cv=5, scoring=\"f1\")\nscores.mean()","5d6d8b09":"# Fitting a simple XGB on TFIDF\nXGB_tfidf = XGBClassifier()\nscores = model_selection.cross_val_score(XGB_tfidf, train_tfidf, train[\"label\"], cv=5, scoring=\"f1\")\nscores.mean()","3d0421a1":"# Fitting a simple AdaBoost on BoW\nada_bow = AdaBoostClassifier()\nada_bow.fit(train_vectors, train[\"label\"])\nscores = model_selection.cross_val_score(ada_bow, train_vectors, train[\"label\"], cv=5, scoring=\"f1\")\nscores.mean()","27ddaa2f":"# Fitting a simple AdaBoost on TFIDF\nada_tfidf = AdaBoostClassifier()\nscores = model_selection.cross_val_score(ada_tfidf, train_tfidf, train[\"label\"], cv=5, scoring=\"f1\")\nscores.mean()","a1ae781d":"# Fitting a simple Extra on BoW\next_bow = ExtraTreesClassifier()\next_bow.fit(train_vectors, train[\"label\"])\nscores = model_selection.cross_val_score(ext_bow, train_vectors, train[\"label\"], cv=5, scoring=\"f1\")\nscores.mean()","11abff6f":"# Fitting a simple AdaBoost on TFIDF\next_tfidf = ExtraTreesClassifier()\nscores = model_selection.cross_val_score(ext_tfidf, train_tfidf, train[\"label\"], cv=5, scoring=\"f1\")\nscores.mean()","01339a6b":"\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom kaggle_datasets import KaggleDatasets\n\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tqdm.notebook import tqdm\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n\nprint('Using Tensorflow version:', tf.__version__)\n\n","18efc8dd":"def regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n             texts, \n             return_attention_masks=False, \n             return_token_type_ids=False,\n             pad_to_max_length=True,\n             max_length=maxlen)\n    \n    return np.array(enc_di['input_ids'])","99b83bb2":"def build_model(transformer, max_len=512):\n    \n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(2, activation='softmax')(cls_token) \n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","28fe14fe":"try:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","eaf6fe13":"# For tf.dataset\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Configuration\nEPOCHS = 4\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMODEL = 'jplu\/tf-xlm-roberta-large' # bert-base-multilingual-uncased","e3321b2d":"from tensorflow.keras.utils import to_categorical\n\n# convert to one-hot-encoding-labels\ntrain_full_labels = to_categorical(train_full['label'], num_classes=2)\ntrain1_labels = to_categorical(train1['label'], num_classes=2)","df62d1eb":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(train1['text'],\n                                                  train1_labels,\n                                                  stratify=train1_labels,\n                                                  test_size=0.1,\n                                                  random_state=2020)\n\nX_train.shape, X_val.shape, y_train.shape, y_val.shape\n\n\n# from sklearn.model_selection import StratifiedKFold\n\n# tra_fold_df = []\n# val_fold_df = []\n# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=88)\n# for tra_idx, val_idx in skf.split(train1, train1[['label']]):\n#     X_tra = train1.iloc[tra_idx]\n#     X_val = train1.iloc[val_idx]\n#     tra_fold_df.append(X_tra)\n#     val_fold_df.append(X_val)","3eb45d7b":"# load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL)","a2bbfdfa":"MAX_LEN = 192\n\nX_train = regular_encode(X_train.values, tokenizer, maxlen=MAX_LEN)\n\n# X_train = regular_encode(X_train.values, tokenizer, maxlen=MAX_LEN)\nX_val = regular_encode(X_val.values, tokenizer, maxlen=MAX_LEN)\nX_test = regular_encode(test1['text'].values, tokenizer, maxlen=MAX_LEN)\n# X_val = regular_encode(X_val.values, tokenizer, maxlen=MAX_LEN)\n# X_test = regular_encode(test1['text'].values, tokenizer, maxlen=MAX_LEN)","2f255e14":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_train, y_train))\n    .repeat()\n    .shuffle(1024)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_val, y_val))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(X_test)\n    .batch(BATCH_SIZE)\n)","d6b8ab2d":"%%time\n\nwith strategy.scope():\n    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()","210e4465":"n_steps = X_train.shape[0] \/\/ BATCH_SIZE\n\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=1\n)","cb92064e":"ext_test_pred = ext_bow.predict_proba(test_vectors)[:,1]\nforest_test_pred = RF_bow.predict_proba(test_vectors)[:,1]\nxgb_test_pred = XGB_bow.predict_proba(test_vectors)[:,1]\nlg_test_pred = logreg_bow.predict_proba(test_vectors)[:,1]\nada_test_pred = ada_bow.predict_proba(test_vectors)[:,1]\nroberta_test_pred = model.predict(test_dataset)[:,1]","ad9cd257":"X_train_predict = regular_encode(train1['text'].values, tokenizer, maxlen=MAX_LEN)\n\n\ntrain_dataset_predict = (\n    tf.data.Dataset\n    .from_tensor_slices(X_train_predict)\n    .batch(BATCH_SIZE)\n)\n\n# ext_train_pred = ext_bow.predict_proba(train_vectors)[:,1]\nforest_train_pred = RF_bow.predict_proba(train_vectors)[:,1]\nxgb_train_pred = XGB_bow.predict_proba(train_vectors)[:,1]\nlg_train_pred = logreg_bow.predict_proba(train_vectors)[:,1]\nada_train_pred = ada_bow.predict_proba(train_vectors)[:,1]\nroberta_train_pred = model.predict(train_dataset_predict)[:,1]\n","3f3cee26":"base_pred = pd.DataFrame({\n    'ext':ext_train_pred.ravel(),\n    'xgb':xgb_train_pred.ravel(), \n    'lg':lg_train_pred.ravel(),\n    'ada':ada_train_pred.ravel(),\n})\n\ntest_pred = pd.DataFrame({\n    'ext':ext_test_pred.ravel(),\n    'xgb':xgb_test_pred.ravel(), \n    'lg':lg_test_pred.ravel(),\n    'ada':ada_test_pred.ravel(),\n    \n})","ec6d902f":"base_pred.head()","4db75716":"# Display numerical correlations between features on heatmap\nsns.set(font_scale=1.1)\ncorrelation_train = base_pred.corr()\nmask = np.triu(correlation_train.corr())\nplt.figure(figsize=(10, 10))\nsns.heatmap(correlation_train,\n            annot=True,\n            fmt='.1f',\n            cmap='coolwarm',\n            square=True,\n            mask=mask,\n            linewidths=1)\n\nplt.show()","cdf097e0":"from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nparameters = {\n    'kernel': ['linear', 'poly', 'rbf'],\n    'C': [0.1, 0.5, 1,10,100,1000], \n    'gamma': [1, 0.1, 0.001, 0.0001, 'auto'],\n    'degree': [3, 4, 5]\n}\n\nfinal_model = GridSearchCV(SVC(), parameters, cv=5)\nfinal_model.fit(base_pred, train1[\"label\"])\n\nscores = model_selection.cross_val_score(final_model, base_pred, train1[\"label\"], cv=5, scoring=\"f1\")\nprint('Cross Validation :', scores.mean())\n\n# make prediction using our test data and model\ny_pred = final_model.predict(base_pred)\nprint('')\nprint('###### SVC Classifier ######')\n\n# evaluating the model\nprint(\"Testing Accuracy :\", accuracy_score(train1[\"label\"], y_pred))\nprint(\"Best Score :\", final_model.best_score_)\nprint('F1-Score :', metrics.f1_score(train1[\"label\"], y_pred, average = 'macro'))\nprint('Best Params :', final_model.best_params_)\nprint('Best Estimator', final_model.best_estimator_)","3c728b02":"test_read = pd.read_excel('..\/input\/fixed-bdc\/Data BDC - Satria Data 2020\/Data Uji\/Data Uji BDC.xlsx')\nfinal_pred = final_model.predict(test_pred)\nsubmission_svc = pd.DataFrame()\nsubmission_svc[\"ID\"] = test_read[\"ID\"]\nsubmission_svc[\"prediksi\"] = final_pred\nsubmission_svc.to_csv(\"submission_stacking_proba_roberta.csv\", index=False)","77bf19a6":"template = pd.read_csv('..\/input\/groundtruth-bestlb-bdc\/groundtruth.csv')\nfor aidi in template['ID']:\n        template.loc[template['ID'] == aidi, 'prediksi'] = int(submission_svc.loc[submission_svc['ID'] == aidi]['prediksi'])","d1246f47":"groundtruth = pd.read_csv('..\/input\/groundtruth-bestlb-bdc\/groundtruth.csv')","7b9f55c5":"\n\n## CEK SCORE SUBMISSION\n## MEMBANDINGKAN HASIL PREDIKSI DENGAN GROUNDTRUTH\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\ny_true = groundtruth.prediksi\ny_pred = template.prediksi\n\nprint(f'accuracy :',accuracy_score(y_true,y_pred))\nprint(f'f1_score_avg_none :',f1_score(y_true, y_pred, average=None))\nprint(f'f1_score_avg_macro :',f1_score(y_true, y_pred, average='macro'))\nprint(f'f1_score_avg_micro :',f1_score(y_true, y_pred, average='micro'))\nprint(f'f1_score_avg_weighted :',f1_score(y_true, y_pred, average='weighted'))\n\n","79d50b87":"XGB_bow.fit(train_vectors, train[\"label\"])","91650e80":"test_read = pd.read_excel('..\/input\/fixed-bdc\/Data BDC - Satria Data 2020\/Data Uji\/Data Uji BDC.xlsx')\nsample_submission = pd.DataFrame()\nsample_submission[\"ID\"] = test_read[\"ID\"]\nsample_submission[\"prediksi\"] = logreg_bow.predict(test_vectors) \nsample_submission.to_csv(\"submission_logreg.csv\", index=False)","6f3c3f14":"print(sample_submission.shape)\nprint(sample_submission.head())","6be3849a":"# The Encoding function takes the text column from train or test dataframe, the tokenizer,\n# and the maximum length of text string as input.\n\n# Outputs:\n# Tokens\n# Pad masks - BERT learns by masking certain tokens in each sequence.\n# Segment id\n\ndef bert_encode(texts, tokenizer, max_len = 512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","7b8900ff":"# Build and compile the model\n\ndef build_model(bert_layer, max_len = 512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","13f6cd4b":"#Training data\ntrain = pd.read_excel('..\/input\/fixed-bdc\/Data BDC - Satria Data 2020\/Data Latih\/Data Latih BDC.xlsx')\nprint('Training data shape: ', train.shape)\nprint(train.head())\n\n# Testing data \ntest = pd.read_excel('..\/input\/fixed-bdc\/Data BDC - Satria Data 2020\/Data Uji\/Data Uji BDC.xlsx')\nprint('Testing data shape: ', test.shape)\nprint(test.head())","92613cf9":"train['text'] = train['judul'] + ' ' + train['narasi']\ntest['text'] = test['judul'] + ' ' + test['narasi']","a0c55bba":"train = train.drop(columns=['ID', 'tanggal', 'judul', 'narasi', 'nama file gambar'])\ntest = test.drop(columns=['ID', 'tanggal', 'judul', 'narasi', 'nama file gambar'])\n\nprint(train.head())\nprint(test.head())","f125df52":"# def decontracted(phrase):\n#     # specific\n#     phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n#     phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n#     # general\n#     phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n#     phrase = re.sub(r\"\\'re\", \" are\", phrase)\n#     phrase = re.sub(r\"\\'s\", \" is\", phrase)\n#     phrase = re.sub(r\"\\'d\", \" would\", phrase)\n#     phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n#     phrase = re.sub(r\"\\'t\", \" not\", phrase)\n#     phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n#     phrase = re.sub(r\"\\'m\", \" am\", phrase)\n#     return phrase","3ad36e49":"# import spacy\n# import re\n# nlp = spacy.load('en')\n# def preprocessing(text):\n#   text = text.replace('#','')\n#   text = decontracted(text)\n#   text = re.sub('\\S*@\\S*\\s?','',text)\n#   text = re.sub('http[s]?:(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',text)\n\n#   token=[]\n#   result=''\n#   text = re.sub('[^A-z]', ' ',text.lower())\n  \n#   text = nlp(text)\n#   for t in text:\n#     if not t.is_stop and len(t)>2:  \n#       token.append(t.lemma_)\n#   result = ' '.join([i for i in token])\n\n#   return result.strip()","7ced339e":"# train.text = train.text.apply(lambda x : preprocessing(x))\n# test.text = test.text.apply(lambda x : preprocessing(x))","6822ea38":"#train.head()","f2504f0f":"# Download BERT architecture\n# BERT-Large uncased: 24-layer, 1024-hidden-nodes, 16-attention-heads, 340M parameters\n\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","a140519c":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","61f3b5af":"train_input = bert_encode(train.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(test.text.values, tokenizer, max_len=160)\ntrain_labels = train.label.values","2931b095":"model = build_model(bert_layer, max_len=160)\nmodel.summary()","cb76e44e":"checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=3,\n    callbacks=[checkpoint],\n    batch_size=8\n)","7bedb112":"model.load_weights('model.h5')\ntest_pred = model.predict(test_input)","1fe8b688":"test_read = pd.read_excel('..\/input\/fixed-bdc\/Data BDC - Satria Data 2020\/Data Uji\/Data Uji BDC.xlsx')\nsubmission = pd.DataFrame()\nsubmission[\"ID\"] = test_read[\"ID\"]\nsubmission['prediksi'] = test_pred.round().astype(int)\nsubmission.to_csv(\"submission2.csv\", index=False)","add84d47":"# Helper Functions","b9d0c5eb":"# Load and Prepare Data","949efbbf":"# Models\n\nFit Logistic Regression and Multinomial Naive Bayes models with BoW and TF-IDF, giving four models in total. This is not an extensive list of vectorization options and models and I won't tune any of the models. It's more of an example framework for linear models in NLP and (spoiler) BERT is going to beat whatever linear model we can come up with.","5f192d5c":"## DEEPLEARNING (ROBERTA) ADDING FOR STACKING\n","5275d417":"# BERT Model","0fd71750":"# Data Preprocessing","a1285b5f":"# EDA","7abe0248":"## Output","0a4c1ca4":"**Data Cleaning** - I've found that a relatively quick and generic data cleaning like I did with the BoW + linear model does not improve the result. The best results seem to be achieved with a painstakingly detailed clean up of train and test text which isn't particularly realistic irl. Some simple data cleaning code in hidden cells below.","9d475a71":"## Stacking SVC","aad9ddf9":"# Predictions and Submission","a1c3e086":"# Bag of Words Vectorizer\n\nHere we're only going to use uni-grams and add any word that appears to the vocabulary. Adding 2- and 3- grams didn't improve the score, surprisingly.","5418aa84":"# Model 1: Traditional NLP - Bag of Words + Linear Model","77509cc7":"# Modelling","a74c6b3c":"**Data cleaning:** In summary, we want to tokenize our text then send it through a round of cleaning where we turn all characters to lower case, remove brackets, URLs, html tags, punctuation, numbers, etc. We'll also remove emojis from the text and remove common stopwords. This is a vital step in the Bag-of-words + linear model","ee4a1486":"# TF-IDF Vectorizer\n\nHere we use 1- and 2-grams where each terms has to appear at least twice and we ignore terms appearing in over 50% of text examples.","3d9032ff":"# Load and Prepare Data"}}