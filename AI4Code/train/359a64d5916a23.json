{"cell_type":{"8bf7e290":"code","e3444dfa":"code","74030029":"code","1006d7ad":"code","df020941":"code","154df908":"code","53fc8276":"code","303837ad":"code","56df355d":"code","3e64ba55":"code","481e19b4":"code","79f488a8":"code","7886a987":"code","53abb8ae":"code","afe28298":"code","11bf02fb":"code","88c21ddd":"code","d81ca876":"markdown","54788212":"markdown","d1050398":"markdown","d351f24e":"markdown","f8b6cf4a":"markdown","da4ede57":"markdown","5e0c9ff9":"markdown","95969b84":"markdown","a7e52ff3":"markdown","f1c8bca5":"markdown","af2d2b51":"markdown","7fb509f3":"markdown","defb3d96":"markdown","cc7b433d":"markdown","d51d4b90":"markdown","589d7fe0":"markdown","4d18b1d1":"markdown","5a7dfa83":"markdown","68081203":"markdown"},"source":{"8bf7e290":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import timedelta\nimport numpy as np\nimport seaborn as sns\nsns.set()","e3444dfa":"df = pd.read_csv(\"..\/input\/ecommerce-bookings-data\/ecommerce_data.csv\")\ndf.head(10)","74030029":"df.dtypes","1006d7ad":"df.isnull().sum()","df020941":"df.hist(figsize = (10,6))\nplt.subplots_adjust(hspace = 1, wspace = 1, top = 0.9)\nplt.tight_layout()\nplt.suptitle('Histograms of Features')","154df908":"fig, ax = plt.subplots(2,2, figsize = (10,6))\nfig.tight_layout()\nfig.suptitle('Comparison of Distributions below Specific Quantiles of `order`')\nfig.subplots_adjust(top = 0.85, hspace = 0.3, wspace = 0.3)\nj = 0\ni = 0\nquantile_list = [0.8, 0.9, 0.95, 0.99]\nfor q in quantile_list:\n    ax[i,j].hist(df['orders'][df['orders']<df['orders'].quantile(q)])\n    ax[i,j].set_title(\"Below {}$^{{th}}$ Percentile\".format(int(q*100)))\n    if j < 1:\n        j+=1\n    else:\n        i+=1\n        j-=1","53fc8276":"df['orders'].describe().round(1)","303837ad":"cutoff = df['orders'].quantile(0.95)\n# Create a dummy indicating if the number of orders was greater than 95th percentile\ndf['high_volume'] = (df['orders']>cutoff)*1\ndf.loc[(df['orders']>cutoff), 'orders'] = cutoff\ndf['high_volume'].value_counts(normalize = True)","56df355d":"df['date'] = pd.to_datetime(df['date'])","3e64ba55":"pd.crosstab(df['date'], df['city_id']).head(20)","481e19b4":"# Defines a function to identify which dates are missing for each city\ndef missing_date_info(city, data):\n    city_df = data.loc[data['city_id']==city, :] #create a temporary dataframe for a given city\n\n    base = city_df['date'].min() #identifies the earliest date of observation\n    maxdate = city_df['date'].max() #identifies the latest date of observation\n    full_date_list = [base + timedelta(days=x) for x in range((maxdate-base).days)] # creates a list of dates spanning from earliest to latest that includes every day in between\n\n    date_vals = sorted(set(city_df['date'])) #create a list of the date values currently in the dataframe for a given city\n    date_missing = [i for i in full_date_list if i not in date_vals] #identifies the dates that are not currently represented in the dataframe\n    return date_missing","79f488a8":"missing_dict = {i : [] for i in df.columns}\nfor x in range(df['city_id'].max()+1):\n    dates = missing_date_info(x, df) #gets the missing dates for each city   \n    #Appends information to the new dictionary\n    for d in dates:\n        missing_dict['date'].append(d)\n        missing_dict['product_id'].append(np.nan)\n        missing_dict['city_id'].append(x)\n        missing_dict['orders'].append(0)\n        missing_dict['high_volume'].append(0)\n\n# Creates a new dataframe with only the missing date rows + other columns    \nmissing_rows = pd.DataFrame(missing_dict)","7886a987":"missing_rows","53abb8ae":"# Appends missing rows to the original dataframe and sorts the dataframe by city and date\ndf2 = df.append(missing_rows)\ndf2 = df2.sort_values(['city_id', 'date']).reset_index(drop = True)","afe28298":"print('df had {} rows.\\nAnd df2 now has {} rows.'.format(len(df), len(df2)))","11bf02fb":"test_list = []\nfor x in range(df2['city_id'].max()+1):\n    test_list.append(missing_date_info(x, df2) == [])\nprint('Are all of the missing date lists empty?\\nResponse: {}'.format(all(test_list)))","88c21ddd":"df2.to_csv(\"ecommerce_clean.csv\", index = False)","d81ca876":"Now, I will append the missing dates to the original dataframe and sort the values of the dataframe by `city_id` and `date`.","54788212":"First, I will need to set the `date` column to pandas datetime format.","d1050398":"# Wrapping Up\nSo that is how far I got with the data cleaning task. I am not sure if there were other problems that I may have missed, but I would love to get feedback on this. Thanks a lot for reading!","d351f24e":"For example, city 25 has purchases on between July 10 - 13, but the other cities in this view do not. Likewise, further down the table we can see that cities 28 and 29 had no purchases on July 25-26, while all of the others did. Thus, if we want to create a panel time series from this data, we will need to add observations into the dataframe to fill those gaps, even if those rows are filled with zeroes. ","f8b6cf4a":"It is clear that this column has a strong positive skew, which can often be expected when we look at something like purchases: most people will only buy a small number of items. Although the skew is inherent to this kind of data, we still have some substantial outliers. The largest order value was nearly 127,000! But as we saw above, the large majority of our data is was below even just 200 purchases.","da4ede57":"Based on the above findings, I am going to do two things that make our data easier to work with ***and*** preserve our ability to investigate high-volume purchases in the future. First, I will create a dummy variable that indicates if this particular city and date had a high-volume of purchases (i.e. greater than the 95th percentile).  Second, I will 'cap' the orders column at the 95th percentile. That is, I will replace all values above the 95th percentile with the value occuring at that point in the distribution. And we can see at the end that just under 5% of the rows have an indicator for high-volume purchases.","5e0c9ff9":"We can see below that the new dataframe consists of the missing dates and the rest of the columns from the original dataframe.","95969b84":"We can see that there are appear to be some significant outliers in the `orders` column of our dataframe. Let's look a bit closer at the distribution of that column to get an idea where the outliers are located:","a7e52ff3":"# Introduction\nThis kernel will do some basic data cleaning on the Ecommerce Daily Orders Data. Not sure if I caught everything, but would be glad to have feedback!","f1c8bca5":"It seems like it was a success. Now, I will just export the cleaned dataframe as a csv for future work.","af2d2b51":"With a simple crosstab we can already see that not all cities have purchases on the same dates in the dataframe.","7fb509f3":"Loading the data:","defb3d96":"Checking the dataframe for missing data: ","cc7b433d":"As a final check to ensure that there are no longer any missing dates in the dataframe, I will make use of the function I created once again. This time, I will create a list called `test_list`, which will indicate, for each city, if there were any dates in the dataframe that were missing from the theoretical list of dates ranging from the minimum to the maximum (as I defined in the function earlier). If there were no missing dates, the function should have returned an empty list for each city. I will then test if this is the case by using `all(test_list)`, which will indicate if all of the items in the test_list were `True` (i.e. empty lists).","d51d4b90":"# Dealing with Outliers","589d7fe0":"Now, I will create a dictionary that will be eventually filled with the information on the missing dates for each city, as well as information for the other columns of the dataframe. I will loop over `city_id`  and, for each city, identify the missing dates using the function defined above. Then, I will iterate through those dates and add new values to the dictionary. Ultimately, I will end up having a dictionary with five keys (representing the five columns of the dataframe) and each key will be paired with the corresponding values for the missing dates. Here, I assumed that if a date was missing from the dataframe, there were 0 orders. I then used this dictionary to create a new pandas dataframe.","4d18b1d1":"Importing the relevant libraries:","5a7dfa83":"# Completing Date Information\n","68081203":"To figure out which dates are missing from each city, I will create a function that identifies the minimum and maximum sale date observed in each city. Then, the function will compare the set of dates in the actual dataframe to another set of dates that includes all of the possible dates between the minimum and maximum. It will then store the dates without a match in a list."}}