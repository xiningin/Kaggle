{"cell_type":{"2a212a02":"code","08c7511b":"code","c464f226":"code","cb6a9dcc":"code","ae2353fc":"code","c431d100":"code","dc4a7800":"code","f2f66ca4":"code","b140de2c":"code","d6f5d22f":"code","526edf98":"code","49732449":"code","2a082425":"code","e9484ac2":"code","3b7c18f1":"code","a11a26bb":"code","56916037":"code","207bee88":"code","3fd82be7":"code","b1694218":"code","a604b04c":"code","3bd82b46":"code","0dd54e40":"code","0aca921c":"code","2a72cec7":"code","932b79a6":"code","a490d7d6":"code","2ddb369a":"code","fb67bb58":"code","2e323b61":"code","5628c59a":"code","110a4c9f":"code","f2519c47":"code","de58bd89":"code","b71c4424":"code","87d40c6c":"code","6daa98c0":"code","6f35a9fa":"code","92a5a115":"code","5305e561":"code","750c1f48":"code","45ea7610":"code","7542a0a2":"code","cc51130d":"code","7cf67ddf":"code","55dd12d6":"code","79bb7b75":"code","e3a35d90":"code","7aab10d0":"code","4623ce19":"code","c4886ef8":"code","585ebb8a":"code","68765928":"code","4de6ea03":"code","3c1d62df":"code","d7891c6d":"code","deeece2f":"code","9ac35093":"code","65132a74":"code","d600dd51":"code","86dce50f":"code","7c3ce2c0":"code","b097be49":"code","c466d5c7":"code","142254f7":"code","e8e68b91":"code","5548e787":"code","51b25faf":"code","b5bf546d":"code","2a5057e6":"markdown","886d37f1":"markdown","e4049bf0":"markdown","40268968":"markdown","241131d9":"markdown","0a12e0b3":"markdown","cd542787":"markdown","a7a26ce5":"markdown","422a0c11":"markdown","778739af":"markdown","ad41c04e":"markdown","91c13c7d":"markdown","8801fdee":"markdown","cdca1af9":"markdown","0a8b14c6":"markdown","99fbf757":"markdown","a594f985":"markdown","609ef37f":"markdown","ad075aa7":"markdown","7c320211":"markdown","fc786268":"markdown","2e7cc6c9":"markdown","7fb183ba":"markdown","8a3cd09f":"markdown","23c57a40":"markdown","ddcc2533":"markdown","9602ac07":"markdown","59eb012d":"markdown","d04da424":"markdown","b331e6ac":"markdown","7727b184":"markdown","1f625cdd":"markdown","f295853e":"markdown","5a1bb9be":"markdown","4acd535d":"markdown","de37e5f7":"markdown","b01e963a":"markdown","feb61bb1":"markdown","42798318":"markdown","ef2d7779":"markdown","869c94d7":"markdown","a9ba9664":"markdown","8ca91149":"markdown","83c0dd27":"markdown","e0b70f90":"markdown","55ecae16":"markdown","e1db889d":"markdown","4596f385":"markdown","738be858":"markdown","5e7d7c61":"markdown","ae703f69":"markdown","aeefed3d":"markdown","ec7e5930":"markdown","e0c46567":"markdown","85711952":"markdown","4d4ad34f":"markdown","2aeca55c":"markdown"},"source":{"2a212a02":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","08c7511b":"import os\nimport tarfile\nimport urllib\n\nDOWNLOAD_ROOT = \"https:\/\/raw.githubusercontent.com\/ageron\/handson-ml2\/master\/\"\nHOUSING_PATH = os.path.join(\"datasets\", \"housing\")\nHOUSING_URL = DOWNLOAD_ROOT + \"datasets\/housing\/housing.tgz\"\n\ndef fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n    if not os.path.isdir(housing_path):\n        os.makedirs(housing_path)\n    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n    urllib.request.urlretrieve(housing_url, tgz_path)\n    housing_tgz = tarfile.open(tgz_path)\n    housing_tgz.extractall(path=housing_path)\n    housing_tgz.close()","c464f226":"fetch_housing_data()","cb6a9dcc":"import pandas as pd\n\ndef load_housing_data(housing_path=HOUSING_PATH):\n    csv_path = os.path.join(housing_path, \"housing.csv\")\n    return pd.read_csv(csv_path)","ae2353fc":"housing = load_housing_data()\nhousing.head()","c431d100":"housing.info()","dc4a7800":"housing[\"ocean_proximity\"].value_counts()","f2f66ca4":"housing.describe()","b140de2c":"import matplotlib.pyplot as plt\nhousing.hist(bins=50, figsize=(20, 15))\nplt.show()","d6f5d22f":"from sklearn.model_selection import train_test_split\ntrain_set,test_set = train_test_split(housing,test_size=0.2,random_state=42)","526edf98":"len(train_set)","49732449":"len(test_set)","2a082425":"housing = train_set.copy()","e9484ac2":"housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")","3b7c18f1":"housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1)","a11a26bb":"housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4, s=housing[\"population\"]\/100, label=\"population\",\n             figsize=(10, 7), c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True)\nplt.legend()","56916037":"corr_matrix = housing.corr()","207bee88":"corr_matrix[\"median_house_value\"].sort_values(ascending=False)","3fd82be7":"housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.1)","b1694218":"housing[\"rooms_per_household\"] = housing[\"total_rooms\"]\/housing[\"households\"]\nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]\/housing[\"total_rooms\"]\nhousing[\"population_per_household\"] = housing[\"population\"]\/housing[\"households\"]","a604b04c":"corr_matrix = housing.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","3bd82b46":"housing = train_set.drop(\"median_house_value\", axis=1)\nhousing_labels = train_set[\"median_house_value\"].copy()","0dd54e40":"#housing.dropna(subset=[\"total_bedrooms\"])\n#housing.drop(\"total_bedrooms\", axis=1)\n#median = housing[\"total_rooms\"].median()\n#housing[\"total_bedrooms\"].fillna(median, inplace=True)\n","0aca921c":"from sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy=\"median\")","2a72cec7":"housing.info()","932b79a6":"housing_num = housing.drop(\"ocean_proximity\", axis=1)","a490d7d6":"imputer.fit(housing_num)","2ddb369a":"imputer.statistics_","fb67bb58":"housing_num.median().values","2e323b61":"X = imputer.transform(housing_num)","5628c59a":"housing_cat = housing[[\"ocean_proximity\"]]\nhousing_cat.head(10)","110a4c9f":"from sklearn.preprocessing import OrdinalEncoder\nordinal_encoder = OrdinalEncoder()","f2519c47":"housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\nhousing_cat_encoded[:10]","de58bd89":"ordinal_encoder.categories_","b71c4424":"from sklearn.preprocessing import OneHotEncoder\ncat_encoder = OneHotEncoder()","87d40c6c":"housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\nhousing_cat_1hot","6daa98c0":"housing_cat_1hot.toarray()","6f35a9fa":"cat_encoder.categories_","92a5a115":"from sklearn.base import BaseEstimator, TransformerMixin\n\n# column index\nrooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room=True): # no *args or **kargs\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    def fit(self, X, y=None):\n        return self  # nothing else to do\n    def transform(self, X):\n        rooms_per_household = X[:, rooms_ix] \/ X[:, households_ix]\n        population_per_household = X[:, population_ix] \/ X[:, households_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] \/ X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household,\n                         bedrooms_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\n\nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\nhousing_extra_attribs = attr_adder.transform(housing.values)","5305e561":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler","750c1f48":"num_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\n\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('attribs_adder', CombinedAttributesAdder()),\n        ('std_scaler', StandardScaler()),\n    ])\n\nhousing_num_tr = num_pipeline.fit_transform(housing_num)","45ea7610":"housing_num_tr","7542a0a2":"from sklearn.compose import ColumnTransformer\n\nnum_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\n\nfull_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, num_attribs),\n        (\"cat\", OneHotEncoder(), cat_attribs),\n    ])\n\nhousing_prepared = full_pipeline.fit_transform(housing)","cc51130d":"housing_prepared","7cf67ddf":"from sklearn.linear_model import LinearRegression","55dd12d6":"lin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)","79bb7b75":"some_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)","e3a35d90":"predictions = lin_reg.predict(some_data_prepared)\nprint(\"Predictions: \", predictions)","7aab10d0":"print(\"Labels: \", list(some_labels))","4623ce19":"from sklearn.metrics import mean_squared_error\n\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)","c4886ef8":"lin_rmse","585ebb8a":"from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(housing_prepared, housing_labels)","68765928":"housing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)","4de6ea03":"tree_rmse","3c1d62df":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)","d7891c6d":"def display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n\ndisplay_scores(tree_rmse_scores)","deeece2f":"from sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor()\nforest_reg.fit(housing_prepared, housing_labels)","9ac35093":"housing_predictions = forest_reg.predict(housing_prepared)\nforest_mse = mean_squared_error(housing_labels, housing_predictions)\nforest_rmse = np.sqrt(forest_mse)","65132a74":"forest_rmse","d600dd51":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    # try 12 (3\u00d74) combinations of hyperparameters\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n    # then try 6 (2\u00d73) combinations with bootstrap set as False\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n  ]\n\nforest_reg = RandomForestRegressor(random_state=42)\n# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\ngrid_search.fit(housing_prepared, housing_labels)","86dce50f":"grid_search.best_params_","7c3ce2c0":"grid_search.best_estimator_","b097be49":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","c466d5c7":"feature_importances = grid_search.best_estimator_.feature_importances_","142254f7":"final_rmse","e8e68b91":"extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n#cat_encoder = cat_pipeline.named_steps[\"cat_encoder\"] # old solution\ncat_encoder = full_pipeline.named_transformers_[\"cat\"]\ncat_one_hot_attribs = list(cat_encoder.categories_[0])\nattributes = num_attribs + extra_attribs + cat_one_hot_attribs\nsorted(zip(feature_importances, attributes), reverse=True)","5548e787":"final_model = grid_search.best_estimator_\n\nX_test = test_set.drop(\"median_house_value\", axis=1)\ny_test = test_set[\"median_house_value\"].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)","51b25faf":"final_rmse","b5bf546d":"from scipy import stats\n\nconfidence = 0.95\nsquared_errors = (final_predictions - y_test) ** 2\nnp.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n                         loc=squared_errors.mean(),\n                         scale=stats.sem(squared_errors)))","2a5057e6":"* Now, we can plot a histogram for each numerical attribute. In a histogram, the values on the vertical axis shows the number of instances and the values on the horizontal axis shows given values. ","886d37f1":"## Grid Search","e4049bf0":"Now, we can use this \"trained\" imputer to transform the training set by replacing missing values.","40268968":"As you can see, \"ocean_proximity\" attribute is not numerical. So let's drop it.","241131d9":"Let's train a Decision Tree algorithm","0a12e0b3":"So, let's revert to a clean training set and also drop our target labels which is median house value.","cd542787":"Let's find what is our error:","a7a26ce5":"We have done a working Linear Regression model. So, let's try to predict with 5 examples.","422a0c11":"* When you call fetch_housing_data() function, it creates  datasets\/housing directory in your workspace, downloads the housing.tgz file, and extracts the housing.csv file.","778739af":"## Let's get the fetch Data:","ad41c04e":"This is how California looks like. But it is hard to see any pattern. The default value of alpha is None. It's typically means opaque. If you set alpha value to 0, it means transparent. So let's set alpha option to 0.1 in order to see easily any pattern.","91c13c7d":"We can use the describe() method to get a summary of the numerical attributes.","8801fdee":"* As you can see, there are 20,640 instances in the dataset. And all attributes are numerical, except the ocean_proximity. ","cdca1af9":"## Data Cleaning\n\nThe main reason for Data Cleaning is prepare the data set to apply ML algorithm. Because, most Machine Learning algorithms can not work with missing values. So let's solve.","0a8b14c6":"Until now, we handled the numerical columns and categorical columns. Let's combine these to single transformer:","99fbf757":"Now, we should distinguish which category is 1 when others not 1. Basically, This process calling One Hot Encoding.","a594f985":"* This is our train data length.","609ef37f":"## Looking for Correlations","ad075aa7":"So let's zoom in the most correlated attributes which median income and median house value.","7c320211":"After these transformation, we get a SciPy sparce matrix whit thousands of columns and the matrix is full of 0s except for a single per row. \n\nBut output of this method using a lot of memory. We may want to convert the matrix to a Numpy array. ","fc786268":"* Now, let's create our test and train sets.","2e7cc6c9":"## Feature Scaling","7fb183ba":"So, let's look at the results:","8a3cd09f":"## Discover and Visualize Data\n\n    * In this section, you should put the test set aside and only exploring the training set.\n    So, let's create a copy so that you can play with iy without harming the training set.\n   ","23c57a40":"This result is not great. Because our model is underfitting. ","ddcc2533":"## Train Linear Regression","9602ac07":"* The info() is a useful to get a quick description.","59eb012d":"* Let's look at ocean_proximity attribute.","d04da424":"Our Decision Tree model has a mean which is 68025. Lastly let's try Random Forest Regressor.","b331e6ac":"You can use one of these three option. But Scikit-Learn provide a useful class for missing values: SimpleImputer. \nLet's use it.","7727b184":"# Introdution\n\n    * In this notebook i have been working on California Housing Price dataset.\n    * This notebook contains preprocessing, cleaning, visualize, feature scaling and training model.\n    * In this notebook i have got referance Aur\u00e9lien G\u00e9ron's \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition\" book.\n","1f625cdd":"Each of these outputs represent a category. So this attribute is categorical. But most Machine Learning algorithms prefer to work with number not categorical attributes. Then, we should convert categories from text to number. Of course Scikit-Learn will help us to make this.","f295853e":"The correlation coefficient ranges from -1 to 1. When it is close to 1, it means that there is a strong positive correlation. For example, the median house value tends to go up when the median income goes up. \n\nWhen coefficient is close to -1, it means that there is a strong negative correlation. Lastly, coefficients close to 0 mean that there is no linear correlation.","5a1bb9be":"Let's look closely. The radius of each circle represents the district's population and the color represents the price. We used color map(cmap) called jet which ranges from blue(low prices) to red(high prices).\n\nThis plot tells use that the housing prices are related to the location. For example, house's prices increase when close to ocean. But in some district housing prices are not too high. So it is not a simple rule. \n\nTheni let's look for correlation.","4acd535d":"Let's look at how much each attribute correlates with the median house value:","de37e5f7":"We can get lists of categories using categories_ instance variable.","b01e963a":"Until now, we dealt with numerical attributes. Let's look at text attriubes. In our dataset, only ocean_proximity attribute is text. Let's examine:","feb61bb1":"# Train Model","42798318":"Our geographical information is latitude and longitude. So we can create a scatterplot. ","ef2d7779":"* As you can see, there are 10 attributes in our data: longitude, latitude, housing_median_age,total_rooms, total_bedrooms, population, households, median_income,median_house_value, and ocean_proximity.","869c94d7":"Before scaling, we want to create a custom transform to add extra attributes:","a9ba9664":"That's ok. But what does imputer.fit() do?\n\nThe imputer computes the median of each attribute and stored the result in its statistics_ instance variable. Let's see statistics_ :","8ca91149":"As you can see, our new **bedrooms_per_room** attribute is much more correlated with the median house value than the total number of rooms. And the number of rooms per household is more informative than the total number of rooms. Because the larger the houses, the more expensive they are.","83c0dd27":"Now, we can clearly see high density areas. So let's look more deatils.","e0b70f90":"But before keep going, median can only be computed on numerical attributes. So have to drop text attributes.","55ecae16":"* Now we should write a fuction that load the data.","e1db889d":"In Machine Learning we don't want our attributes have very different scales. So we should make them either standardisation or normalization. Normalization means to make attributes values between 0-1. ","4596f385":"* There are five different districts in ocean_proximity. We can see how many districts belong to each category.","738be858":"Now, let's look at the correlation matrix:","5e7d7c61":"As you can see, all values are equal.","ae703f69":"## Preprocess For Machine Learning\n\nBefore prepare the data for ML algorithms, we may want to this process via functions. Because:\n    * To reproduce the transformations easily on any dataset\n    * To use this library of transformation functions in future projects again.\n    * This will make easy to see combination of various transformations ","aeefed3d":"It seeming our error is zero. Wait but why? Because our model is overfitting. ","ec7e5930":"## Experimenting with Attribute Combinations\n\nTo implement a Machine Learning algorithm, we should find correlations between attributes, in particularly with the target attribute. \n\nBefore preparing the data for Machine Learning algorithms, we may want to try out various attribute combinations. For example, the total number of rooms in a district is not very useful if we don't know how many households are there. What we really want is the number of rooms per household. Similarly, the same goes for other features. So we want to look at the rooms per household, the bedrooms per rooms and the population per household.","e0c46567":"* Let's take a look at the top five rows.","85711952":"Now, we can fit the imputer instance.","4d4ad34f":"To solve missing values problem, we have three options:\n * Get rid of the corresponding districts.\n * Get rid of the whole attribute.\n * Set the values to some value.","2aeca55c":"Let's use cross-validation to better evaluation"}}