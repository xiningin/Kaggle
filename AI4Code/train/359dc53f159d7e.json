{"cell_type":{"4f27b119":"code","beccdf7c":"code","ab55ad9d":"code","5950b62d":"code","a68678fd":"code","328844b1":"code","0e70b3c4":"code","ca66cbcf":"code","0d17de91":"code","00dc0433":"code","73269f19":"code","9863aa52":"code","c48086ec":"code","d3a6112e":"code","2e8491c1":"code","6ff1e74c":"code","9033cad5":"code","df8eaf80":"code","72a9c784":"code","dd52695f":"code","5fdd9a15":"code","12da9393":"code","ffde73f1":"code","e163ff36":"code","01427c0e":"code","fb869e98":"code","0de6b9b1":"code","c3c8b358":"code","752f4649":"code","66d02be1":"code","36998199":"code","2afc2da7":"code","bc68254b":"code","ede114a1":"code","f15686b3":"code","0c557b62":"code","467018f7":"markdown","bfebbdec":"markdown","e030e99c":"markdown","798cab0f":"markdown","af98fefd":"markdown","eda61200":"markdown","556541c7":"markdown","9a03722e":"markdown","fe24f8aa":"markdown","1f96dbf2":"markdown","d6bfd7cf":"markdown","805cde70":"markdown","71cea8fc":"markdown","36da8268":"markdown","32d83026":"markdown","beac6c33":"markdown","cf7442b7":"markdown","8d9e8b29":"markdown","8b40cd3e":"markdown","7ae7a34b":"markdown","9c4cc0bf":"markdown"},"source":{"4f27b119":"import os\nimport numpy as np\nimport pandas as pd\n\nimport cv2\nimport matplotlib.pyplot as plt\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications import VGG16","beccdf7c":"path = '..\/input\/'\nos.listdir(path)","ab55ad9d":"train_data = pd.read_csv(path+'labels.csv')\nsamp_subm = pd.read_csv(path+'sample_submission.csv')","5950b62d":"print('Number train samples:', len(train_data))\nprint('Number train images:', len(os.listdir(path+'train\/')))\nprint('Number test images:', len(os.listdir(path+'test\/')))","a68678fd":"train_data.head()","328844b1":"train_data['breed'].value_counts()[0:10]","0e70b3c4":"id_ = train_data.loc[0, 'id']\nbreed = train_data.loc[0, 'breed']\nfile = id_+'.jpg'\nimg = cv2.imread(path+'train\/'+file)\nprint('Shape:', img.shape)","ca66cbcf":"fig, ax = plt.subplots(1, 1, figsize=(7, 7))\nax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\nax.set_xticklabels([])\nax.set_yticklabels([])\nax.set_title(breed)\nplt.show()","0d17de91":"def plot_examples(category = 'scottish_deerhound'):\n    \"\"\" Plot 5 images of a given category \"\"\"\n    \n    fig, axs = plt.subplots(1, 5, figsize=(25, 20))\n    fig.subplots_adjust(hspace = .1, wspace=.1)\n    axs = axs.ravel()\n    temp = train_data[train_data['breed']==category].copy()\n    temp.index = range(len(temp.index))\n    for i in range(5):\n        id_ = temp.loc[i, 'id']\n        breed = temp.loc[i, 'breed']\n        file = id_+'.jpg'\n        img = cv2.imread(path+'train\/'+file)\n        axs[i].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n        axs[i].set_title(breed)\n        axs[i].set_xticklabels([])\n        axs[i].set_yticklabels([])\n    plt.show()","00dc0433":"plot_examples()","73269f19":"plot_examples('maltese_dog')","9863aa52":"plot_examples('afghan_hound')","c48086ec":"def image_preprocessing(image, image_size):\n    \"\"\" Image Preprocessing \"\"\"\n\n    # Load Image\n    readFlag=cv2.COLOR_BGR2GRAY\n    #image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n    #image = cv2.imdecode(image, readFlag)\n    image_gray = cv2.cvtColor(image, readFlag)\n    \n    # Crop Image\n    mid_row = int(image_gray.shape[0]\/2)\n    mid_col = int(image_gray.shape[1]\/2)\n    if image_gray.shape[0]>image_gray.shape[1]:\n        image_cropped = image_gray[mid_row-mid_col:mid_row+mid_col,\n                                   0:image_gray.shape[1]]\n    else:\n        image_cropped = image_gray[0:image_gray.shape[0],\n                                   mid_col-mid_row:mid_col+mid_row]\n    \n    # Rescale Image\n    image_rescale = cv2.resize(image_cropped,\n                               dsize=(image_size, image_size),\n                               interpolation=cv2.INTER_AREA)\n    return image_rescale\n\ndef plot_befor_after(image):\n    \"\"\" Compare original and prepared image \"\"\"\n    \n    fig, axs = plt.subplots(1, 2, figsize=(15, 10))\n    fig.subplots_adjust(hspace = .1, wspace=.1)\n    axs = axs.ravel()\n    # Plot Original Image\n    axs[0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    axs[0].set_title('original shape: '+str(image.shape))\n    # Image Preprocessing\n    image_rescale = image_preprocessing(image, image_size)\n    # Plot Prepared Image\n    axs[1].imshow(image_rescale, cmap='gray')\n    axs[1].set_title('rescaled shape: '+str(image_rescale.shape))\n    for i in range(2):\n        axs[i].set_xticklabels([])\n        axs[i].set_yticklabels([])\n    plt.show()","d3a6112e":"image_size = 128","2e8491c1":"row = 0\nid_ = train_data.loc[row, 'id']\nbreed = train_data.loc[row, 'breed']\nfile = id_+'.jpg'\nimage = cv2.imread(path+'train\/'+file)\nprint('Shape:', image.shape)","6ff1e74c":"plot_befor_after(image)","9033cad5":"row = 10\nid_ = train_data.loc[row, 'id']\nbreed = train_data.loc[row, 'breed']\nfile = id_+'.jpg'\nimage = cv2.imread(path+'train\/'+file)\nprint('Shape:', image.shape)","df8eaf80":"plot_befor_after(image)","72a9c784":"image_size = 128","dd52695f":"def prepare_data(path, data, image_size):\n    \"\"\" Read all images into a numpy array \"\"\"\n    \n    X = np.empty((len(data), image_size, image_size), dtype=np.uint8)\n    for row in data.index:\n        id_ = data.loc[row, 'id']\n        file = id_ + '.jpg'\n        image = cv2.imread(path+file)\n        image_rescaled = image_preprocessing(image, image_size)\n        X[row, :, :] = image_rescaled\n    X = X.astype('float32')\/255\n    return X","5fdd9a15":"X_train = prepare_data(path+'train\/', train_data, image_size)\nX_test = prepare_data(path+'test\/', samp_subm, image_size)","12da9393":"y_train = train_data['breed']\ny_train = pd.get_dummies(y_train)","ffde73f1":"X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state=2021)","e163ff36":"print('Shape train data:', X_train.shape)\nprint('Shape val data:', X_val.shape)","01427c0e":"X_train = X_train.reshape(-1,image_size,image_size,1)\nX_val = X_val.reshape(-1,image_size,image_size,1)\nX_test = X_test.reshape(-1,image_size,image_size,1)","fb869e98":"model = Sequential()\n\nmodel.add(Conv2D(filters=32, kernel_size = (3, 3), activation='relu', input_shape=(image_size, image_size, 1)))\nmodel.add(MaxPool2D((2, 2)))\n\nmodel.add(Conv2D(32, (3, 3),activation='relu'))\nmodel.add(MaxPool2D((2, 2)))\n\nmodel.add(Conv2D(64, (3, 3),activation='relu'))\nmodel.add(MaxPool2D((2, 2)))\n\n#model.add(Conv2D(128, (3, 3),activation='relu'))\n#model.add(MaxPool2D((2, 2)))\n\n#model.add(Conv2D(128, (3, 3),activation='relu'))\n#model.add(MaxPool2D((2, 2)))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(120, activation = 'softmax'))","0de6b9b1":"model = Sequential()\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (image_size, image_size, 1)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.15))\n\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.15))\n\n\nmodel.add(Flatten())\n#model.add(Dense(256, activation = \"relu\"))\n#model.add(Dropout(0.3))\nmodel.add(Dense(120, activation = \"softmax\"))","c3c8b358":"#model.compile(optimizer = RMSprop(lr=0.0001), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\nmodel.compile(optimizer=Adam(lr=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])","752f4649":"model.summary()","66d02be1":"epochs = 5\nbatch_size = 128","36998199":"#history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\n                              #epochs = epochs, validation_data = (X_val,y_val), steps_per_epoch=X_train.shape[0] \/ batch_size)\n\nhistory = model.fit(X_train, y_train,\n                    epochs=epochs,\n                    batch_size=batch_size,\n                    validation_data=(X_val, y_val))","2afc2da7":"loss = history.history['loss']\nloss_val = history.history['val_loss']\nepochs = range(1, len(loss)+1)\nplt.plot(epochs, loss, 'bo', label='loss_train')\nplt.plot(epochs, loss_val, 'b', label='loss_val')\nplt.title('value of the loss function')\nplt.xlabel('epochs')\nplt.ylabel('value of the loss function')\nplt.legend()\nplt.grid()\nplt.show()","bc68254b":"acc = history.history['acc']\nacc_val = history.history['val_acc']\nepochs = range(1, len(loss)+1)\nplt.plot(epochs, acc, 'bo', label='accuracy_train')\nplt.plot(epochs, acc_val, 'b', label='accuracy_val')\nplt.title('accuracy')\nplt.xlabel('epochs')\nplt.ylabel('value of accuracy')\nplt.legend()\nplt.grid()\nplt.show()","ede114a1":"y_test = model.predict(X_test)","f15686b3":"samp_subm[samp_subm.columns[1:]] = y_test","0c557b62":"samp_subm.to_csv('submission.csv', index=False)","467018f7":"Encode the labels","bfebbdec":"# Analyse Training","e030e99c":"# Define Model","798cab0f":"Example Portrait:","af98fefd":"# EDA","eda61200":"# Overview","556541c7":"We set the image size:","9a03722e":"# Split Train Data ","fe24f8aa":"# Load Data","1f96dbf2":"# Plot Examples\nWe plot example images of the breed top 10.","d6bfd7cf":"# Intro\nWelcome to the [Dog Breed Identification](https:\/\/www.kaggle.com\/c\/dog-breed-identification\/overview) competition.\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/7327\/logos\/header.png)\n\n<font size=\"4\"><span style=\"color: royalblue;\">Please vote the notebook up if it helps you. Feel free to leave a comment above the notebook. Thank you. <\/span><\/font>","805cde70":"# Predict Test Data","71cea8fc":"# Reshape Data","36da8268":"Example Landscape:","32d83026":"# Image Preprocessing\nAs we can see the images have different format: landscape or portrait. For the neural network we need a standard size. So we have to prepare the data. ","beac6c33":"# Read All Images","cf7442b7":"There are 120 breeds. Here you can see the distribution of the top 10: ","8d9e8b29":"# Libraries","8b40cd3e":"# Export Data","7ae7a34b":"# Path","9c4cc0bf":"# Load Single Image\nWe plot the first image of of the train data."}}