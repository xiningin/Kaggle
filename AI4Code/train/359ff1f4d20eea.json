{"cell_type":{"587646af":"code","972ff1df":"code","a6b0ab61":"code","83f0a66b":"code","ca03b306":"code","48536e01":"code","e513af08":"code","ab1ca987":"code","4244f2ad":"code","6a469a64":"code","7bd3f684":"code","65c13227":"code","4290a075":"code","5e3d94f4":"code","b3f77860":"code","cc513a69":"code","35c068f5":"code","11000cf8":"code","73a45c0d":"code","8a1a7fd8":"code","cf451ab9":"code","56a7cea7":"code","5cc2c255":"code","bef34612":"code","186b1422":"code","6c8d2e0c":"code","f820c542":"code","8bda2b9f":"code","31fd506f":"code","17b86ff4":"code","d403d46d":"code","08de7774":"code","3fd80f2b":"code","8ae5680c":"code","8f0d06e2":"code","8dd13b71":"code","69e9392e":"markdown","9d09c58a":"markdown","2fa2e8f7":"markdown","e529ccee":"markdown","a448d66d":"markdown","151193ab":"markdown","6b8bb2d6":"markdown","41338362":"markdown","75ce5eee":"markdown","460972a6":"markdown","84a20818":"markdown","df6df20d":"markdown","f46c6375":"markdown","34d114dc":"markdown","86b3c8c7":"markdown","b96a51d9":"markdown","284573e2":"markdown","3b4fcaeb":"markdown","a0b957d9":"markdown","a7a54867":"markdown","ba180fc7":"markdown","502bef14":"markdown","5ed91398":"markdown"},"source":{"587646af":"## Importing Libraries\nimport os\nimport pandas as pd\nimport numpy as np\nimport datatable as dt  # pip install datatable\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport math \n%matplotlib inline\nimport seaborn as sns\nsns.set_style('ticks')\nimport plotly.offline as py\npy.init_notebook_mode(connected = True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\nprint(\"Packages Imported\")","972ff1df":"train_data = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/train.csv')\ntest_data = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/test.csv')\nsample     = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv')","a6b0ab61":"train_data.head(5)","83f0a66b":"print(f'Number of rows: {train_data.shape[0]}; \\n  Number of columns: {train_data.shape[1]}; \\n No of missing values: {sum(train_data.isna().sum())};')","ca03b306":"print('there is no missing values.')\ntrain_data.isna().sum(axis = 0).sort_values(ascending = True)","48536e01":"print(\"Info about the train data: \")\nCounter(train_data.dtypes.values)","e513af08":"train_data.describe().style.background_gradient(cmap = 'coolwarm')","ab1ca987":"## Correlationmatrix\ncorrMatrix = train_data.corr(method = 'pearson', min_periods = 1)\ncorrMatrix","4244f2ad":"ax = sns.heatmap(\n    corrMatrix, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);","6a469a64":"corr_targ = train_data.corrwith(train_data['target'])\n\ncorr_targ.abs().sort_values(ascending = False)[1:11].plot.bar(title = 'Top 10 abs corr features')","7bd3f684":"print(\"Top 10 abs corr features : {}\".format(corr_targ.abs().sort_values(ascending = False)[1:11].index))","65c13227":"print('percentage of target values: ')\npercent_value = pd.DataFrame(train_data['target'].value_counts()\/len(train_data))\npercent_value.T","4290a075":"# visualization\ncountplt, ax = plt.subplots(figsize = (8, 5))\nax = sns.countplot(train_data['target'], palette = 'husl')","5e3d94f4":"test_data.head(5)","b3f77860":"print(f'Number of rows: {test_data.shape[0]}; \\n  Number of columns: {test_data.shape[1]}; \\n No of missing values: {sum(test_data.isna().sum())};')","cc513a69":"print('there is no missing values.')\ntrain_data.isna().sum(axis = 0).sort_values(ascending = True)","35c068f5":"test_data.describe().style.background_gradient(cmap = 'coolwarm')","11000cf8":"train_data.iloc[:, 1:286].shape[1]\/5","73a45c0d":"\nfig, ax = plt.subplots(2,2, figsize = (12, 8))\n\ntrain_data.iloc[:, 1:286].mean(axis = 0).plot(ax = ax[0,0],\n                                              title = 'the train data distribution in a view of feature means')\ntrain_data.iloc[:, 1:286].std(axis = 0).plot(ax = ax[0,1],\n                                             title = 'the train data distribution in a view of of feature stds')\ntest_data.iloc[:, 1:286].mean(axis = 0).plot(ax = ax[1,0],\n                                             title = 'the test data distribution in a view of feature means')\ntest_data.iloc[:, 1:286].std(axis = 0).plot(ax = ax[1,1],\n                                            title = 'the test data  distribution in a view of feature stds')\n\nplt.tight_layout()","8a1a7fd8":"features = train_data.iloc[:, 1: 286] \ni = 1\nplt.figure()\nfig, ax = plt.subplots(9, 6, figsize = (28, 28))\nfor feature in features: # feature \ub97c \ucd9c\ub825\ud558\uba74 \uceec\ub7fc\uba85\uc774 \ub098\uc634\n    plt.subplot(57, 5, i)\n    sns.distplot(train_data[feature], color='blue', kde=True, bins = 120, label = 'train')\n    sns.distplot(test_data[feature], color='orange', kde=True, bins = 120, label = 'test')\n    i += 1\nplt.show()","cf451ab9":"# train a basic RF classifier\nrf = RandomForestClassifier(n_estimators = 100, max_depth = 5, min_samples_leaf = 4, max_features = 0.2, n_jobs = -1, random_state = 1234)\nrf.fit(train_data.drop(['id', 'target'], axis = 1), train_data.target)\nprint(\"Training Done\")","56a7cea7":"#this snapCode comes from this notbook (https:\/\/www.kaggle.com\/arthurtok\/interactive-porto-insights-a-plot-ly-tutorial)\nfeatures = train_data.drop(['id', 'target'],axis=1).columns.values\ntrace = go.Scatter(\n    y = rf.feature_importances_,\n    x = features,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 13,\n        color = rf.feature_importances_,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = features\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Random Forest Feature Importance',\n    hovermode= 'closest',\n     xaxis= dict(\n         ticklen= 5,\n         showgrid=False,\n        zeroline=False,\n        showline=False\n     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        showgrid=False,\n        zeroline=False,\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')","5cc2c255":"sample_size = int(train_data.shape[0]\/10)\nreduced_train = train_data.groupby('target', group_keys=False).apply(lambda x: x.sample(sample_size, random_state = 1234))","bef34612":"# install packages\nimport h2o\nfrom h2o.automl import H2OAutoML\nh2o.init() # h2o initialization","186b1422":"train = h2o.H2OFrame(reduced_train)\n","6c8d2e0c":"test = h2o.H2OFrame(test_data)","f820c542":"x = train.columns \ny = \"target\" # target \nx.remove(y) # # X_train \ntrain[y] = train[y].asfactor() #binary classification ","8bda2b9f":"%%time\n\nauto_ml = H2OAutoML( \n    nfolds=5, # use 5 folds \n    seed = 1234,\n    max_models = 10,\n    include_algos = [\"XGBoost\" ,\"StackedEnsemble\",\"GBM\"],\n    max_runtime_secs=3600*2,  #time in sec , if set to much high value may give high score \n    stopping_metric='AUC'\n    )\nauto_ml.train(x=x, y=y, training_frame=train)","31fd506f":"# check leaderboard\nleader = auto_ml.leaderboard\nleader","17b86ff4":"model = h2o.get_model(leader[7,\"model_id\"]) # get gbm model \nmodel.varimp_plot()","d403d46d":"mc_plot = auto_ml.model_correlation_heatmap(train)","08de7774":"learning_curve_plot = model.learning_curve_plot()","3fd80f2b":"preds = auto_ml.leader.predict(test)","8ae5680c":"print(preds.head())","8f0d06e2":"## create submission\nsubmission = pd.DataFrame({\n    'id': test['id'].as_data_frame().id,\n    'target': preds.as_data_frame().p1\n})\nsubmission.head()","8dd13b71":"# save submission\nsubmission.to_csv('h2o_submission.csv', index=False)","69e9392e":"### Quick look at the Train data","9d09c58a":"### Basic summary statistics","2fa2e8f7":"### Generate Prediction","e529ccee":"### Correlation with target","a448d66d":"### Feature Importance","151193ab":"- Data size is too big. So, I try to 1:10 sample a data to be efficient.","6b8bb2d6":"### Learning Curve Plot\n","41338362":"### Model Correlation Heatmap","75ce5eee":"- it seems that there is no significant relation b\/w target and exploratory variables.","460972a6":"- Result is similar with former correlation analysis.\n- f22 is significantly important to predict target variable. and f179 is second important but is is not significant. but other is not. \n","84a20818":"## Load the data","df6df20d":"- min-max scaling is already employed.","f46c6375":"### Basic summary statistics for test data","34d114dc":"## Quick look at the Test dataset","86b3c8c7":"## Data reduction\n","b96a51d9":"- Most part of this notebook is based on a article, 'Simple EDA + H2OAutoML' in the reference above.","284573e2":"- It seems that train data is similar with test data.","3b4fcaeb":"- Top 10 features : 'f22', 'f179', 'f69', 'f156', 'f58', 'f136', 'f214', 'f78', 'f8'","a0b957d9":"### Traget columns","a7a54867":"- reference : \n    - [Simple EDA + H2OAutoML](https:\/\/www.kaggle.com\/mhslearner\/simple-eda-h2oautoml)","ba180fc7":"## Feature Selection\n### Train a basic RF classifier","502bef14":"## ","5ed91398":"## AutoML and Submission"}}