{"cell_type":{"8d302c92":"code","914d76d4":"code","4487beef":"code","52c309fc":"code","535b577d":"code","fa2b811e":"code","3e9e8301":"code","cc05a1cb":"code","1abb29e2":"code","873c4825":"code","82aeb117":"code","d6dfe0e7":"code","224d0f9f":"code","ad494f43":"code","47269b95":"code","21ad9f22":"code","18a5b346":"code","1c8c0ac7":"markdown","d46360b5":"markdown","aa75aa3a":"markdown","1a03ee51":"markdown","0d0dbaac":"markdown","0d494b57":"markdown","54289a5a":"markdown","26a2b349":"markdown","7e57057c":"markdown","0c7cf3a1":"markdown","9cba8c1d":"markdown","292ab6c4":"markdown","5912359f":"markdown","51617ac5":"markdown","1d4f4a64":"markdown","84e09aa4":"markdown"},"source":{"8d302c92":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pickle\nimport time\nfrom scipy.stats import laplace\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, initializers","914d76d4":"\ndef create_teacher_FCN(input_dim=10, output_dim=1, nn_depth=4, nn_width=3, model_name='FCN_DxW_4x3_default', leaky_relu_alpha=0.4, initializer='glorot_uniform'):\n    \n    # input\n    inputs = keras.Input(shape=(input_dim), name='input')\n    \n    # first hiddent layer\n    x = layers.Dense(nn_width, activation=layers.LeakyReLU(alpha=leaky_relu_alpha), kernel_initializer=initializer, name='FC_LReLU_1')(inputs)\n    \n    # rest of hidden layers\n    for k in range(nn_depth-1):\n        x = layers.Dense(nn_width, activation=layers.LeakyReLU(alpha=leaky_relu_alpha), kernel_initializer=initializer, name='FC_LReLU_%d' %(k+2))(x)\n\n    # output\n    outputs = layers.Dense(output_dim, kernel_initializer=initializer, name='output')(x)\n\n    # assemble the model\n    FCN_model = keras.Model(inputs=inputs, outputs=outputs, name=model_name)\n    \n    return FCN_model\n\n\ndef create_student_FCN(input_dim=10, output_dim=1, nn_depth=4, nn_width=3, model_name='FCN_DxW_4x3_default', leaky_relu_alpha=0.4, initializer='glorot_uniform'):\n    \n    # input\n    inputs = keras.Input(shape=(input_dim), name='input')\n    \n    # first hiddent layer\n    x = layers.Dense(nn_width, activation=layers.LeakyReLU(alpha=leaky_relu_alpha), kernel_initializer=initializer, name='FC_LReLU_1')(inputs)\n    x = layers.BatchNormalization(momentum=0.9, name='BN_1')(x)\n\n    # rest of hidden layers\n    for k in range(nn_depth-1):\n        x = layers.Dense(nn_width, activation=layers.LeakyReLU(alpha=leaky_relu_alpha), kernel_initializer=initializer, name='FC_LReLU_%d' %(k+2))(x)\n        x = layers.BatchNormalization(momentum=0.9, name='BN_%d' %(k+2))(x)\n        \n    # output\n    outputs = layers.Dense(output_dim, kernel_initializer=initializer, name='output')(x)\n\n    # assemble the model\n    FCN_model = keras.Model(inputs=inputs, outputs=outputs, name=model_name)\n    \n    return FCN_model\n\n\nclass GaussianDataGenerator(keras.utils.Sequence):\n    def __init__(self, teacher_model, input_dim=10, num_batches_per_epoch=1, batch_size=64, stdev=1.0):\n        self.teacher_model = teacher_model\n        self.input_dim = input_dim\n        self.batch_size = batch_size\n        self.num_batches_per_epoch = num_batches_per_epoch\n        self.stdev = stdev\n\n    def __len__(self):\n        return self.num_batches_per_epoch\n\n    def __getitem__(self, idx):\n        batch_x = self.stdev * np.random.randn(self.batch_size, self.input_dim)\n        batch_y = self.teacher_model.predict(batch_x)\n        return batch_x, batch_y\n    \n    \nclass UniformDataGenerator(keras.utils.Sequence):\n    def __init__(self, teacher_model, input_dim=10, num_batches_per_epoch=1, batch_size=64, stdev=1.0):\n        self.teacher_model = teacher_model\n        self.input_dim = input_dim\n        self.batch_size = batch_size\n        self.num_batches_per_epoch = num_batches_per_epoch\n        self.stdev = stdev\n\n    def __len__(self):\n        return self.num_batches_per_epoch\n\n    def __getitem__(self, idx):\n        batch_x = self.stdev * np.sqrt(12) * (np.random.rand(self.batch_size, self.input_dim) - 0.5)\n        batch_y = self.teacher_model.predict(batch_x)\n        return batch_x, batch_y\n        \n    \nclass LaplaceDataGenerator(keras.utils.Sequence):\n    def __init__(self, teacher_model, input_dim=10, num_batches_per_epoch=1, batch_size=64, stdev=1.0):\n        self.teacher_model = teacher_model\n        self.input_dim = input_dim\n        self.batch_size = batch_size\n        self.num_batches_per_epoch = num_batches_per_epoch\n        self.stdev = stdev\n\n    def __len__(self):\n        return self.num_batches_per_epoch\n\n    def __getitem__(self, idx):\n        batch_x = (self.stdev \/ np.sqrt(2)) * laplace.rvs(size=[self.batch_size, self.input_dim])\n        batch_y = self.teacher_model.predict(batch_x)\n        return batch_x, batch_y\n\n    \ndef generate_initializer(init_method='truncated_normal', fanint_normal_scale=1.0, truncated_normal_stddev=0.6):\n\n    if init_method == 'glorot_uniform':\n        initializer = initializers.GlorotUniform()\n    elif init_method == 'truncated_normal':\n        initializer = initializers.TruncatedNormal(mean=0.0, stddev=truncated_normal_stddev)\n    elif init_method == 'orthogonal':\n        initializer = initializers.Orthogonal()\n    elif init_method == 'fanin_normal':\n        initializer = initializers.VarianceScaling(scale=fanint_normal_scale, mode='fan_in', distribution='truncated_normal')\n\n    return initializer\n\n","4487beef":"random_seed = 1234\n#random_seed = 4321\n#random_seed = 1111\n#random_seed = 1357\nnp.random.seed(random_seed)\n\ninput_dim = 10\nnn_depth = 3\nnn_width = 8\noutput_dim = 1\n\nleaky_relu_alpha = 0.5\n\nsave_student_weights = False\n\napply_activation_normalization_teacher = True\n#apply_activation_normalization_teacher = False\nactivation_normalization_method = 'full'\n#activation_normalization_method = 'input_output'\n#activation_normalization_method = 'output'\n\n#teacher_init_method = 'glorot_uniform'\n#teacher_init_method = 'truncated_normal'\nteacher_init_method = 'orthogonal'\n#teacher_init_method = 'fanin_normal'\n\n\napply_activation_normalization_student = False\n\nstudent_init_method = 'glorot_uniform'\n#student_init_method = 'truncated_normal'\n#student_init_method = 'orthogonal'\n#student_init_method = 'fanin_normal'\n\nfanint_normal_scale = 1.0\ntruncated_normal_stddev = 0.5\n\n# generate the initializer for teacher and student\nteacher_initializer = generate_initializer(init_method=teacher_init_method, fanint_normal_scale=fanint_normal_scale, truncated_normal_stddev=truncated_normal_stddev)\nstudent_initializer = generate_initializer(init_method=student_init_method, fanint_normal_scale=fanint_normal_scale, truncated_normal_stddev=truncated_normal_stddev)\n\nmodel_name = 'FCN_DxW_%dx%d_GT' %(nn_depth, nn_width)\n\nGT_model = create_teacher_FCN(input_dim, output_dim, nn_depth, nn_width, model_name, leaky_relu_alpha, teacher_initializer)\nGT_model.summary()\nGT_model.compile(optimizer=\"adam\", loss=\"mse\")","52c309fc":"init_params_dict = {}\ninit_params_dict['teacher_init_method'] = teacher_init_method\ninit_params_dict['student_init_method'] = student_init_method\ninit_params_dict['fanint_normal_scale'] = fanint_normal_scale\ninit_params_dict['truncated_normal_stddev'] = truncated_normal_stddev\ninit_params_dict['apply_activation_normalization_teacher'] = apply_activation_normalization_teacher\ninit_params_dict['apply_activation_normalization_student'] = apply_activation_normalization_student\ninit_params_dict['activation_normalization_method'] = activation_normalization_method\ninit_params_dict","535b577d":"def extract_weights_dict(model, show_prints=False):\n    \n    weights_dict = {}\n\n    if show_prints:\n        # collect and display the weights of the GT teacher model\n        for k, layer in enumerate(model.layers): \n            print('----------------------------------------------------------------')\n            print('k = %d:' %(k))\n            print('--------')\n            print(layer.get_config())\n            if (layer.get_config()['name'].find('FC') != -1) or (layer.get_config()['name'].find('output') != -1):\n                print('------------------------------')\n                curr_layer_weights = layer.get_weights()[0]\n                print('\"%s\" weights (weights.shape = %s):' %(layer.get_config()['name'], str(curr_layer_weights.shape)))\n                print('------------------------------')\n                print(curr_layer_weights)\n                weights_dict[layer.get_config()['name']] = {}\n                weights_dict[layer.get_config()['name']]['weights'] = curr_layer_weights\n                print('------------------------------')\n                curr_layer_biases = layer.get_weights()[1]\n                print('\"%s\" biases (biases.shape = %s):' %(layer.get_config()['name'], str(curr_layer_biases.shape)))\n                print('------------------------------')\n                print(curr_layer_biases)\n                weights_dict[layer.get_config()['name']]['biases'] = curr_layer_biases\n                print('------------------------------')\n            print('----------------------------------------------------------------')\n    else:\n        # collect and display the weights of the GT teacher model\n        for k, layer in enumerate(model.layers): \n            if (layer.get_config()['name'].find('FC') != -1) or (layer.get_config()['name'].find('output') != -1):\n                curr_layer_weights = layer.get_weights()[0]\n                weights_dict[layer.get_config()['name']] = {}\n                weights_dict[layer.get_config()['name']]['weights'] = curr_layer_weights\n                curr_layer_biases = layer.get_weights()[1]\n                weights_dict[layer.get_config()['name']]['biases'] = curr_layer_biases\n\n    return weights_dict\n\nGT_weights_dict = extract_weights_dict(GT_model, show_prints=True)","fa2b811e":"apply_activation_normalization_teacher = True\n\nif apply_activation_normalization_teacher:\n\n    temp_X = 1.0 * np.random.randn(4096, input_dim)\n\n    desired_layers = [layer for layer in GT_model.layers if ((layer.name.find('LReLU') != -1) or (layer.name.find('output') != -1))]\n    layer_names = [layer.name for layer in desired_layers]\n    outputs = [layer.output for layer in desired_layers]\n\n    if activation_normalization_method == 'full':\n    \n        num_update_iterations = nn_depth + 5\n        learning_rate = 0.67\n    \n        for update_iter in range(num_update_iterations):\n            print('---------------------------')\n\n            all_activations_model = keras.Model(inputs=GT_model.input, outputs=outputs, name='GT_activations')\n            all_activations = all_activations_model.predict(temp_X)\n\n            new_weights = []\n            # calculate the necessary update\n            for layer_ind, activations_layer in enumerate(all_activations):\n                num_units = activations_layer.shape[1]\n\n                curr_layer_weights = GT_model.get_layer(layer_names[layer_ind]).get_weights()[0]\n                curr_layer_biases = GT_model.get_layer(layer_names[layer_ind]).get_weights()[1]\n\n                for unit_ind in range(num_units):\n                    unit_mean = activations_layer[:,unit_ind].mean()\n                    unit_std  = activations_layer[:,unit_ind].std()\n\n                    print('layer %s, units %d: (mu, sigma) = (%5.3f, %5.3f)' %(layer_ind, unit_ind, unit_mean, unit_std))\n\n                    if layer_ind in range(update_iter+1):\n                        curr_layer_weights[:,unit_ind] \/= unit_std\n                        curr_layer_biases[unit_ind]    -= unit_mean\n\n                new_weights.append([curr_layer_weights, curr_layer_biases])\n\n            # update the weights\n            for layer_ind in range(len(layer_names)):\n                GT_model.get_layer(layer_names[layer_ind]).set_weights(new_weights[layer_ind])\n\n                \n                \n    elif activation_normalization_method == 'input_output':\n        \n        print('------------------------------------------------------------------------------------------')\n        # make first pass - correct variance of first layer only\n        all_activations_model = keras.Model(inputs=GT_model.input, outputs=outputs, name='GT_activations')\n        all_activations = all_activations_model.predict(temp_X)\n\n        new_weights = []\n        # calculate the necessary update\n        for layer_ind, activations_layer in enumerate(all_activations):\n            num_units = activations_layer.shape[1]\n\n            curr_layer_weights = GT_model.get_layer(layer_names[layer_ind]).get_weights()[0]\n            curr_layer_biases = GT_model.get_layer(layer_names[layer_ind]).get_weights()[1]\n\n            for unit_ind in range(num_units):\n                unit_mean = activations_layer[:,unit_ind].mean()\n                unit_std  = activations_layer[:,unit_ind].std()\n\n                print('layer %s, units %d: (mu, sigma) = (%5.3f, %5.3f)' %(layer_ind, unit_ind, unit_mean, unit_std))\n\n                if layer_names[layer_ind] == 'FC_LReLU_1':\n                    curr_layer_weights[:,unit_ind] \/= unit_std\n                    #curr_layer_biases[unit_ind] -= unit_mean\n                    \n            new_weights.append([curr_layer_weights, curr_layer_biases])\n\n        # update the weights\n        for layer_ind in range(len(layer_names)):\n            GT_model.get_layer(layer_names[layer_ind]).set_weights(new_weights[layer_ind])\n\n        print('------------------------------------------------------------------------------------------')\n\n        \n        \n        print('------------------------------------------------------------------------------------------')\n        # make second pass - correct bias of first layer only\n        all_activations_model = keras.Model(inputs=GT_model.input, outputs=outputs, name='GT_activations')\n        all_activations = all_activations_model.predict(temp_X)\n\n        new_weights = []\n        # calculate the necessary update\n        for layer_ind, activations_layer in enumerate(all_activations):\n            num_units = activations_layer.shape[1]\n\n            curr_layer_weights = GT_model.get_layer(layer_names[layer_ind]).get_weights()[0]\n            curr_layer_biases = GT_model.get_layer(layer_names[layer_ind]).get_weights()[1]\n\n            for unit_ind in range(num_units):\n                unit_mean = activations_layer[:,unit_ind].mean()\n                unit_std  = activations_layer[:,unit_ind].std()\n\n                print('layer %s, units %d: (mu, sigma) = (%5.3f, %5.3f)' %(layer_ind, unit_ind, unit_mean, unit_std))\n\n                if layer_names[layer_ind] == 'FC_LReLU_1':\n                    #curr_layer_weights[:,unit_ind] \/= unit_std\n                    curr_layer_biases[unit_ind] -= unit_mean\n                    \n            new_weights.append([curr_layer_weights, curr_layer_biases])\n\n        # update the weights\n        for layer_ind in range(len(layer_names)):\n            GT_model.get_layer(layer_names[layer_ind]).set_weights(new_weights[layer_ind])\n                    \n        print('------------------------------------------------------------------------------------------')\n                \n            \n        print('------------------------------------------------------------------------------------------')\n        # make additional input correction pass\n        all_activations_model = keras.Model(inputs=GT_model.input, outputs=outputs, name='GT_activations')\n        all_activations = all_activations_model.predict(temp_X)\n\n        new_weights = []\n        # calculate the necessary update\n        for layer_ind, activations_layer in enumerate(all_activations):\n            num_units = activations_layer.shape[1]\n\n            curr_layer_weights = GT_model.get_layer(layer_names[layer_ind]).get_weights()[0]\n            curr_layer_biases = GT_model.get_layer(layer_names[layer_ind]).get_weights()[1]\n\n            for unit_ind in range(num_units):\n                unit_mean = activations_layer[:,unit_ind].mean()\n                unit_std  = activations_layer[:,unit_ind].std()\n\n                print('layer %s, units %d: (mu, sigma) = (%5.3f, %5.3f)' %(layer_ind, unit_ind, unit_mean, unit_std))\n\n                if layer_names[layer_ind] == 'FC_LReLU_1':\n                    curr_layer_weights[:,unit_ind] \/= unit_std\n                    curr_layer_biases[unit_ind] -= unit_mean\n                    \n            new_weights.append([curr_layer_weights, curr_layer_biases])\n\n        # update the weights\n        for layer_ind in range(len(layer_names)):\n            GT_model.get_layer(layer_names[layer_ind]).set_weights(new_weights[layer_ind])\n                    \n        print('------------------------------------------------------------------------------------------')\n            \n            \n        print('------------------------------------------------------------------------------------------')\n        # make third pass - correct variance of output layer only\n        all_activations_model = keras.Model(inputs=GT_model.input, outputs=outputs, name='GT_activations')\n        all_activations = all_activations_model.predict(temp_X)\n\n        new_weights = []\n        # calculate the necessary update\n        for layer_ind, activations_layer in enumerate(all_activations):\n            num_units = activations_layer.shape[1]\n\n            curr_layer_weights = GT_model.get_layer(layer_names[layer_ind]).get_weights()[0]\n            curr_layer_biases = GT_model.get_layer(layer_names[layer_ind]).get_weights()[1]\n\n            for unit_ind in range(num_units):\n                unit_mean = activations_layer[:,unit_ind].mean()\n                unit_std  = activations_layer[:,unit_ind].std()\n\n                print('layer %s, units %d: (mu, sigma) = (%5.3f, %5.3f)' %(layer_ind, unit_ind, unit_mean, unit_std))\n\n                if layer_names[layer_ind] == 'output':\n                    curr_layer_weights[:,unit_ind] \/= unit_std\n                    #curr_layer_biases[unit_ind] -= unit_mean\n                    \n            new_weights.append([curr_layer_weights, curr_layer_biases])\n\n        # update the weights\n        for layer_ind in range(len(layer_names)):\n            GT_model.get_layer(layer_names[layer_ind]).set_weights(new_weights[layer_ind])\n        print('------------------------------------------------------------------------------------------')\n\n        \n        print('------------------------------------------------------------------------------------------')\n        # make forth pass - correct bias of output layer only\n        all_activations_model = keras.Model(inputs=GT_model.input, outputs=outputs, name='GT_activations')\n        all_activations = all_activations_model.predict(temp_X)\n\n        new_weights = []\n        # calculate the necessary update\n        for layer_ind, activations_layer in enumerate(all_activations):\n            num_units = activations_layer.shape[1]\n\n            curr_layer_weights = GT_model.get_layer(layer_names[layer_ind]).get_weights()[0]\n            curr_layer_biases = GT_model.get_layer(layer_names[layer_ind]).get_weights()[1]\n\n            for unit_ind in range(num_units):\n                unit_mean = activations_layer[:,unit_ind].mean()\n                unit_std  = activations_layer[:,unit_ind].std()\n\n                print('layer %s, units %d: (mu, sigma) = (%5.3f, %5.3f)' %(layer_ind, unit_ind, unit_mean, unit_std))\n\n                if layer_names[layer_ind] == 'output':\n                    #curr_layer_weights[:,unit_ind] \/= unit_std\n                    curr_layer_biases[unit_ind] -= unit_mean    \n\n            new_weights.append([curr_layer_weights, curr_layer_biases])\n\n        # update the weights\n        for layer_ind in range(len(layer_names)):\n            GT_model.get_layer(layer_names[layer_ind]).set_weights(new_weights[layer_ind])\n        print('------------------------------------------------------------------------------------------')\n\n        \n        print('------------------------------------------------------------------------------------------')\n        # make last pass - just validate\n        all_activations_model = keras.Model(inputs=GT_model.input, outputs=outputs, name='GT_activations')\n        all_activations = all_activations_model.predict(temp_X)\n\n        for layer_ind, activations_layer in enumerate(all_activations):\n            num_units = activations_layer.shape[1]\n            for unit_ind in range(num_units):\n                unit_mean = activations_layer[:,unit_ind].mean()\n                unit_std  = activations_layer[:,unit_ind].std()\n                print('layer %s, units %d: (mu, sigma) = (%5.3f, %5.3f)' %(layer_ind, unit_ind, unit_mean, unit_std))\n        print('------------------------------------------------------------------------------------------')\n\n        \n        \n    # after normalization, extract and show the weights again\n    GT_weights_dict = extract_weights_dict(GT_model, show_prints=True)\n","3e9e8301":"temp_X = 1.0 * np.random.randn(4096, input_dim)\n\noutputs = [layer.output for layer in GT_model.layers if ((layer.name.find('LReLU') != -1) or (layer.name.find('output') != -1))]\n\nall_activations_model = keras.Model(inputs=GT_model.input, outputs=outputs, name='GT_activations')\nall_activations = all_activations_model.predict(temp_X)\n\nfor activations_layer in all_activations:\n    num_units = activations_layer.shape[1]\n    plt.figure(figsize=(22,4))\n    plt.subplots_adjust(left=0.02, right=0.98, top=0.95, bottom=0.03, wspace=0.12)\n    for k in range(num_units):\n        unit_mean = activations_layer[:,k].mean()\n        unit_std  = activations_layer[:,k].std()\n        plt.subplot(1, num_units, k+1); plt.hist(activations_layer[:,k], bins=100); \n        plt.title('$\\mu$ = %.3f, $\\sigma$ = %.3f' %(unit_mean,unit_std), fontsize=16); plt.xlabel('unit %d' %(k+1), fontsize=16)","cc05a1cb":"batch_size = 32\npre_generated_train_batches = 12000\npre_generated_valid_batches = 125\nX_truncation_limits = [-4.5, 4.5]\ntrain_datagen = GaussianDataGenerator(GT_model, input_dim=input_dim, num_batches_per_epoch=1, batch_size=batch_size, stdev=1.0)\n\nvalid_datagen_1 = GaussianDataGenerator(GT_model, input_dim=input_dim, num_batches_per_epoch=1, batch_size=batch_size, stdev=1.0)\nvalid_datagen_2 = GaussianDataGenerator(GT_model, input_dim=input_dim, num_batches_per_epoch=1, batch_size=batch_size, stdev=1.4)\nvalid_datagen_3 = UniformDataGenerator(GT_model, input_dim=input_dim, num_batches_per_epoch=1, batch_size=batch_size, stdev=1.4)\nvalid_datagen_4 = LaplaceDataGenerator(GT_model, input_dim=input_dim, num_batches_per_epoch=1, batch_size=batch_size, stdev=1.4)\n\ndataset_generation_start_time = time.time()\n\n# create train, valid datasets\nx_batch, y_batch = train_datagen[0]\n\nX_train = np.zeros((pre_generated_train_batches * batch_size, x_batch.shape[1]))\ny_train = np.zeros((pre_generated_train_batches * batch_size, y_batch.shape[1]))\n\nX_valid_1 = np.zeros((pre_generated_valid_batches * batch_size, x_batch.shape[1]))\ny_valid_1 = np.zeros((pre_generated_valid_batches * batch_size, y_batch.shape[1]))\n\nX_valid_2 = np.zeros((pre_generated_valid_batches * batch_size, x_batch.shape[1]))\ny_valid_2 = np.zeros((pre_generated_valid_batches * batch_size, y_batch.shape[1]))\n\nX_valid_3 = np.zeros((pre_generated_valid_batches * batch_size, x_batch.shape[1]))\ny_valid_3 = np.zeros((pre_generated_valid_batches * batch_size, y_batch.shape[1]))\n\nX_valid_4 = np.zeros((pre_generated_valid_batches * batch_size, x_batch.shape[1]))\ny_valid_4 = np.zeros((pre_generated_valid_batches * batch_size, y_batch.shape[1]))\n\n# populate the datasets\nend_ind = 0\nfor k in range(pre_generated_train_batches):\n    x_batch, y_batch = train_datagen[k]\n\n    start_ind = end_ind\n    end_ind = start_ind + batch_size\n    # print('%d: [%d,%d]' %(k,start_ind,end_ind))\n\n    X_train[start_ind:end_ind,:] = x_batch\n    y_train[start_ind:end_ind,:] = y_batch\n\n    if k < pre_generated_valid_batches:\n        x_batch, y_batch = valid_datagen_1[k]\n        X_valid_1[start_ind:end_ind,:] = x_batch\n        y_valid_1[start_ind:end_ind,:] = y_batch\n        \n        x_batch, y_batch = valid_datagen_2[k]\n        X_valid_2[start_ind:end_ind,:] = x_batch\n        y_valid_2[start_ind:end_ind,:] = y_batch\n        \n        x_batch, y_batch = valid_datagen_3[k]\n        X_valid_3[start_ind:end_ind,:] = x_batch\n        y_valid_3[start_ind:end_ind,:] = y_batch\n        \n        x_batch, y_batch = valid_datagen_4[k]\n        X_valid_4[start_ind:end_ind,:] = x_batch\n        y_valid_4[start_ind:end_ind,:] = y_batch\n\n# truncate all the points\nX_train[X_train < X_truncation_limits[0]] = X_truncation_limits[0]\nX_train[X_train > X_truncation_limits[1]] = X_truncation_limits[1]\n\nX_valid_1[X_valid_1 < X_truncation_limits[0]] = X_truncation_limits[0]\nX_valid_2[X_valid_2 < X_truncation_limits[0]] = X_truncation_limits[0]\nX_valid_3[X_valid_3 < X_truncation_limits[0]] = X_truncation_limits[0]\nX_valid_4[X_valid_4 < X_truncation_limits[0]] = X_truncation_limits[0]\n\nX_valid_1[X_valid_1 > X_truncation_limits[1]] = X_truncation_limits[1]\nX_valid_2[X_valid_2 > X_truncation_limits[1]] = X_truncation_limits[1]\nX_valid_3[X_valid_3 > X_truncation_limits[1]] = X_truncation_limits[1]\nX_valid_4[X_valid_4 > X_truncation_limits[1]] = X_truncation_limits[1]\n\ndataset_generation_duration_min = (time.time() - dataset_generation_start_time) \/ 60\nprint('finished generating train and valid datasets. took %.1f minutes' %(dataset_generation_duration_min))","1abb29e2":"X_train_std = X_train.std()\nX_valid_1_std = X_valid_1.std()\nX_valid_2_std = X_valid_2.std()\nX_valid_3_std = X_valid_3.std()\nX_valid_4_std = X_valid_4.std()\n\nnum_valid_samples = X_valid_1.shape[0]\n\nbins = np.linspace(-4.5,4.5, 100)\n\nplt.figure(figsize=(22,12))\nplt.subplots_adjust(bottom=0.05, top=0.80, left=0.05, right=0.95, hspace=0.07, wspace=0.1)\n\nplt.subplot(2,4,1)\nplt.hist(X_train[:num_valid_samples,0], bins=bins, color='g', alpha=0.5, label='train (gaussian, $\\sigma$ = %.1f)' %(X_train_std))\nplt.hist(X_valid_1[:,0], bins=bins, color='b', alpha=0.5, label='valid 1 (gaussian, $\\sigma$ = %.1f)' %(X_valid_1_std))\nplt.legend(fontsize=16, loc='upper center', bbox_to_anchor=(0.5, 1.22))\nplt.ylabel('counts', fontsize=20)\n\nplt.subplot(2,4,5); \nplt.hist(X_train[:num_valid_samples,0], bins=bins, log=True, color='g', alpha=0.5, label='train (gaussian, $\\sigma$ = %.1f)' %(X_train_std))\nplt.hist(X_valid_1[:,0], bins=bins, log=True, color='b', alpha=0.5, label='valid 1 (gaussian, $\\sigma$ = %.1f)' %(X_valid_1_std));\nplt.xlabel('$X_1$', fontsize=20)\nplt.ylabel('counts (log scale)', fontsize=20)\n\nplt.subplot(2,4,2)\nplt.hist(X_train[:num_valid_samples,0], bins=bins, color='g', alpha=0.5, label='train (gaussian, $\\sigma$ = %.1f)' %(X_train_std))\nplt.hist(X_valid_2[:,0], bins=bins, color='b', alpha=0.5, label='valid 2 (wider gaussian, $\\sigma$ = %.1f)' %(X_valid_2_std))\nplt.legend(fontsize=16, loc='upper center', bbox_to_anchor=(0.5, 1.22))\n\nplt.subplot(2,4,6); \nplt.hist(X_train[:num_valid_samples,0], bins=bins, log=True, color='g', alpha=0.5, label='train (gaussian, $\\sigma$ = %.1f)' %(X_train_std))\nplt.hist(X_valid_2[:,0], bins=bins, log=True, color='b', alpha=0.5, label='valid 2 (wider gaussian, $\\sigma$ = %.1f)' %(X_valid_2_std));\nplt.xlabel('$X_1$', fontsize=20)\n\nplt.subplot(2,4,3)\nplt.hist(X_train[:num_valid_samples,0], bins=bins, color='g', alpha=0.5, label='train (gaussian, $\\sigma$ = %.1f)' %(X_train_std))\nplt.hist(X_valid_3[:,0], bins=bins, color='b', alpha=0.5, label='valid 3 (uniform, $\\sigma$ = %.1f)' %(X_valid_3_std))\nplt.legend(fontsize=16, loc='upper center', bbox_to_anchor=(0.5, 1.22))\n\nplt.subplot(2,4,7); \nplt.hist(X_train[:num_valid_samples,0], bins=bins, log=True, color='g', alpha=0.5, label='train (gaussian, $\\sigma$ = %.1f)' %(X_train_std))\nplt.hist(X_valid_3[:,0], bins=bins, log=True, color='b', alpha=0.5, label='valid 3 (uniform, $\\sigma$ = %.1f)' %(X_valid_3_std));\nplt.xlabel('$X_1$', fontsize=20)\n\nplt.subplot(2,4,4)\nplt.hist(X_train[:num_valid_samples,0], bins=bins, color='g', alpha=0.5, label='train (gaussian, $\\sigma$ = %.1f)' %(X_train_std))\nplt.hist(X_valid_4[:,0], bins=bins, color='b', alpha=0.5, label='valid 4 (laplace, $\\sigma$ = %.1f)' %(X_valid_4_std))\nplt.legend(fontsize=16, loc='upper center', bbox_to_anchor=(0.5, 1.22))\n\nplt.subplot(2,4,8); \nplt.hist(X_train[:num_valid_samples,0], bins=bins, log=True, color='g', alpha=0.5, label='train (gaussian, $\\sigma$ = %.1f)' %(X_train_std))\nplt.hist(X_valid_4[:,0], bins=bins, log=True, color='b', alpha=0.5, label='valid 4 (laplace, $\\sigma$ = %.1f)' %(X_valid_4_std));\nplt.xlabel('$X_1$', fontsize=20);\n","873c4825":"y_train_std = y_train.std()\ny_valid_1_std = y_valid_1.std()\ny_valid_2_std = y_valid_2.std()\ny_valid_3_std = y_valid_3.std()\ny_valid_4_std = y_valid_4.std()\n\n# determine the bins to display histograms\ny_concat = np.concatenate((y_valid_1, y_valid_2, y_valid_3, y_valid_4))\ny_min = np.percentile(y_concat,  0.1)\ny_max = np.percentile(y_concat, 99.9)\nbins = np.linspace(y_min,y_max, 100)\n\nnum_valid_samples = y_valid_1.shape[0]\n\nplt.figure(figsize=(22,12))\nplt.subplots_adjust(bottom=0.05, top=0.8, left=0.05, right=0.95, hspace=0.07, wspace=0.1)\n\nplt.subplot(2,4,1)\nplt.hist(y_train[:num_valid_samples], bins=bins, color='g', alpha=0.5, label='train ($\\sigma$ = %.3f)' %(y_train_std))\nplt.hist(y_valid_1[:], bins=bins, color='b', alpha=0.5, label='valid 1 ($\\sigma$ = %.3f)' %(y_valid_1_std))\nplt.legend(fontsize=16, loc='upper center', bbox_to_anchor=(0.5, 1.22))\nplt.ylabel('counts', fontsize=20)\n\nplt.subplot(2,4,5); \nplt.hist(y_train[:num_valid_samples], bins=bins, log=True, color='g', alpha=0.5, label='train ($\\sigma$ = %.3f)' %(y_train_std))\nplt.hist(y_valid_1[:], bins=bins, log=True, color='b', alpha=0.5, label='valid 1 ($\\sigma$ = %.3f)' %(y_valid_1_std));\nplt.xlabel('$y$', fontsize=20)\nplt.ylabel('counts (log scale)', fontsize=20)\n\nplt.subplot(2,4,2)\nplt.hist(y_train[:num_valid_samples,0], bins=bins, color='g', alpha=0.5, label='train ($\\sigma$ = %.3f)' %(y_train_std))\nplt.hist(y_valid_2[:,0], bins=bins, color='b', alpha=0.5, label='valid 2 ($\\sigma$ = %.3f)' %(y_valid_2_std))\nplt.legend(fontsize=16, loc='upper center', bbox_to_anchor=(0.5, 1.22))\n\nplt.subplot(2,4,6); \nplt.hist(y_train[:num_valid_samples,0], bins=bins, log=True, color='g', alpha=0.5, label='train ($\\sigma$ = %.3f)' %(y_train_std))\nplt.hist(y_valid_2[:,0], bins=bins, log=True, color='b', alpha=0.5, label='valid 2 ($\\sigma$ = %.3f)' %(y_valid_2_std));\nplt.xlabel('$y$', fontsize=20)\n\nplt.subplot(2,4,3)\nplt.hist(y_train[:num_valid_samples,0], bins=bins, color='g', alpha=0.5, label='train ($\\sigma$ = %.3f)' %(y_train_std))\nplt.hist(y_valid_3[:,0], bins=bins, color='b', alpha=0.5, label='valid 3 ($\\sigma$ = %.3f)' %(y_valid_3_std))\nplt.legend(fontsize=16, loc='upper center', bbox_to_anchor=(0.5, 1.22))\n\nplt.subplot(2,4,7); \nplt.hist(y_train[:num_valid_samples], bins=bins, log=True, color='g', alpha=0.5, label='train ($\\sigma$ = %.3f)' %(y_train_std))\nplt.hist(y_valid_3[:], bins=bins, log=True, color='b', alpha=0.5, label='valid 3 ($\\sigma$ = %.3f)' %(y_valid_3_std));\nplt.xlabel('$y$', fontsize=20)\n\nplt.subplot(2,4,4)\nplt.hist(y_train[:num_valid_samples], bins=bins, color='g', alpha=0.5, label='train ($\\sigma$ = %.3f)' %(y_train_std))\nplt.hist(y_valid_4[:], bins=bins, color='b', alpha=0.5, label='valid 4 ($\\sigma$ = %.3f)' %(y_valid_4_std))\nplt.legend(fontsize=16, loc='upper center', bbox_to_anchor=(0.5, 1.22))\n\nplt.subplot(2,4,8); \nplt.hist(y_train[:num_valid_samples], bins=bins, log=True, color='g', alpha=0.5, label='train ($\\sigma$ = %.3f)' %(y_train_std))\nplt.hist(y_valid_4[:], bins=bins, log=True, color='b', alpha=0.5, label='valid 4 ($\\sigma$ = %.3f)' %(y_valid_4_std));\nplt.xlabel('$y$', fontsize=20);\n","82aeb117":"# gather y distribution main statsistics\ny_GT_stats_dict = {}\ny_GT_stats_dict['valid_1'] = {}\ny_GT_stats_dict['valid_1']['mean']  = y_valid_1.mean()\ny_GT_stats_dict['valid_1']['std']   = y_valid_1.std()\ny_GT_stats_dict['valid_1']['mse_0'] = y_valid_1.var()\n\ny_GT_stats_dict['valid_2'] = {}\ny_GT_stats_dict['valid_2']['mean']  = y_valid_2.mean()\ny_GT_stats_dict['valid_2']['std']   = y_valid_2.std()\ny_GT_stats_dict['valid_2']['mse_0'] = y_valid_2.var()\n\ny_GT_stats_dict['valid_3'] = {}\ny_GT_stats_dict['valid_3']['mean']  = y_valid_3.mean()\ny_GT_stats_dict['valid_3']['std']   = y_valid_3.std()\ny_GT_stats_dict['valid_3']['mse_0'] = y_valid_3.var()\n\ny_GT_stats_dict['valid_4'] = {}\ny_GT_stats_dict['valid_4']['mean']  = y_valid_4.mean()\ny_GT_stats_dict['valid_4']['std']   = y_valid_4.std()\ny_GT_stats_dict['valid_4']['mse_0'] = y_valid_4.var()\n\n[print(x, y_GT_stats_dict[x]) for x in y_GT_stats_dict.keys()];","d6dfe0e7":"plt.figure(figsize=(18,18))\nplt.subplots_adjust(bottom=0.05, top=0.95, left=0.05, right=0.95, hspace=0.1, wspace=0.1)\nnum_samples_to_draw = 1000\nlimits = [-4.6,4.6]\n\nplt.subplot(2,2,1); \nplt.scatter(X_train[:num_samples_to_draw,0], X_train[:num_samples_to_draw,1], color='g', alpha=0.8, label='train (gaussian)')\nplt.scatter(X_valid_1[:num_samples_to_draw,0], X_valid_1[:num_samples_to_draw,1], color='b', alpha=0.8, label='valid 1 (gaussian)')\nplt.xlim(limits); plt.ylim(limits); plt.legend(fontsize=16, loc=2)\nplt.ylabel('$X_2$', fontsize=20)\n\nplt.subplot(2,2,2); \nplt.scatter(X_train[:num_samples_to_draw,0], X_train[:num_samples_to_draw,1], color='g', alpha=0.8, label='train (gaussian)')\nplt.scatter(X_valid_2[:num_samples_to_draw,0], X_valid_2[:num_samples_to_draw,1], color='b', alpha=0.8, label='valid 2 (wider gaussian)')\nplt.xlim(limits); plt.ylim(limits); plt.legend(fontsize=16, loc=2)\n\nplt.subplot(2,2,3); \nplt.scatter(X_train[:num_samples_to_draw,0], X_train[:num_samples_to_draw,1], color='g', alpha=0.8, label='train (gaussian)')\nplt.scatter(X_valid_3[:num_samples_to_draw,0], X_valid_3[:num_samples_to_draw,1], color='b', alpha=0.8, label='valid 3 (uniform)')\nplt.xlim(limits); plt.ylim(limits); plt.legend(fontsize=16, loc=2)\nplt.xlabel('$X_1$', fontsize=20); plt.ylabel('$X_2$', fontsize=20)\n\nplt.subplot(2,2,4); \nplt.scatter(X_train[:num_samples_to_draw,0], X_train[:num_samples_to_draw,1], color='g', alpha=0.8, label='train (gaussian)')\nplt.scatter(X_valid_4[:num_samples_to_draw,0], X_valid_4[:num_samples_to_draw,1], color='b', alpha=0.8, label='valid 4 (laplace)')\nplt.xlim(limits); plt.ylim(limits); plt.legend(fontsize=16, loc=2)\nplt.xlabel('$X_1$', fontsize=20);","224d0f9f":"class LossHistoryCallback(keras.callbacks.Callback):\n    def on_train_begin(self, logs):\n        self.per_batch_train_losses = []\n        self.train_losses_batch_ind = []\n        \n        self.per_batch_valid_losses_1 = []\n        self.per_batch_valid_losses_2 = []\n        self.per_batch_valid_losses_3 = []\n        self.per_batch_valid_losses_4 = []\n        self.valid_losses_batch_ind = []\n\n    def on_batch_end(self, batch, logs):\n        # make sure we keep track of the full training curves\n        self.train_losses_batch_ind.append(batch + 1) \n        self.per_batch_train_losses.append(logs.get(\"loss\")) \n\n        # every once in a while, perform full evaluation against several data distributions\n        if batch in [0, 9, 19, 29, 49, 79, 109, 139, 169, 199, 249, 299, 399, 599, 1199]:\n            valid_loss_1 = self.model.evaluate(X_valid_1, y_valid_1, verbose=0)\n            valid_loss_2 = self.model.evaluate(X_valid_2, y_valid_2, verbose=0)\n            valid_loss_3 = self.model.evaluate(X_valid_3, y_valid_3, verbose=0)\n            valid_loss_4 = self.model.evaluate(X_valid_4, y_valid_4, verbose=0)\n            \n            self.valid_losses_batch_ind.append(batch + 1)\n            self.per_batch_valid_losses_1.append(valid_loss_1)\n            self.per_batch_valid_losses_2.append(valid_loss_2)\n            self.per_batch_valid_losses_3.append(valid_loss_3)\n            self.per_batch_valid_losses_4.append(valid_loss_4)","ad494f43":"student_nn_depth = 6\nstudent_nn_width = 6\nmodel_name = 'FCN_DxW_%dx%d_student' %(student_nn_depth, student_nn_width)\n\nnum_train_iterations = 1200\n\nprint('currently training \"%s\":' %(model_name))\n\n# create model\nstudent_model = create_student_FCN(input_dim, output_dim, student_nn_depth, student_nn_width, model_name, leaky_relu_alpha, student_initializer)\n#student_model.summary()\nstudent_model.compile(optimizer=\"adam\", loss=\"mse\")\n\nmodel_training_start_time = time.time()\n\n# train model\nhistory = LossHistoryCallback()\nsubset_inds = np.random.permutation(X_train.shape[0])[:(num_train_iterations * batch_size)]\nstudent_model.fit(X_train[subset_inds], y_train[subset_inds], epochs=1, callbacks=[history])\n\ncurr_model_duration_sec = (time.time() - model_training_start_time)\nprint('finished all random inits for current model. took %.2f seconds' %(curr_model_duration_sec))","47269b95":"plt.figure(figsize=(16,14)); \n\nplt.subplot(3,1,1); plt.title('NOTE: train loss is a mean that is reset after every eval', fontsize=14)\nplt.plot(history.train_losses_batch_ind, history.per_batch_train_losses, label='train (gaussian)')\nplt.legend(fontsize=18, loc=1)\nplt.xlim(0,1200); plt.ylabel('MSE', fontsize=16)\n\nplt.subplot(3,1,2)\nplt.plot(history.valid_losses_batch_ind, history.per_batch_valid_losses_1, alpha=0.7, label='valid 1 (gaussian)')\nplt.plot(history.valid_losses_batch_ind, history.per_batch_valid_losses_2, alpha=0.7, label='valid 2 (wide gaussian)')\nplt.plot(history.valid_losses_batch_ind, history.per_batch_valid_losses_3, alpha=0.7, label='valid 3 (unifrom)')\nplt.plot(history.valid_losses_batch_ind, history.per_batch_valid_losses_4, alpha=0.7, label='valid 4 (laplace)')\nplt.legend(fontsize=18, loc=1, ncol=2)\nplt.xlim(0,1200); plt.ylabel('MSE', fontsize=16)\n\nplt.subplot(3,1,3)\nplt.semilogy(history.valid_losses_batch_ind, history.per_batch_valid_losses_1, alpha=0.7, label='valid 1 (gaussian)')\nplt.semilogy(history.valid_losses_batch_ind, history.per_batch_valid_losses_2, alpha=0.7, label='valid 2 (wide gaussian)')\nplt.semilogy(history.valid_losses_batch_ind, history.per_batch_valid_losses_3, alpha=0.7, label='valid 3 (unifrom)')\nplt.semilogy(history.valid_losses_batch_ind, history.per_batch_valid_losses_4, alpha=0.7, label='valid 4 (laplace)')\nplt.legend(fontsize=18, loc=1, ncol=2)\nplt.xlim(0,1200); plt.ylabel('MSE (log scale)', fontsize=16); plt.xlabel('training steps', fontsize=16);","21ad9f22":"num_random_inits = 30\nnum_train_iterations = 1200\n\n# collect a dictionay with models to fit\ndepths_to_try = np.array([1,2,4,8,16,32]).astype(int)\nwidths_to_try = np.array([4,8,16,32,64,128,256]).astype(int)\n\nmodel_hyperparams_list = []\nfor student_nn_depth in depths_to_try:\n    for student_nn_width in widths_to_try:\n        model_name = 'FCN_DxW_%dx%d_student' %(student_nn_depth, student_nn_width)\n\n        model_hyperparams_dict = {}\n        model_hyperparams_dict['model_name'] = model_name\n        model_hyperparams_dict['student_nn_depth'] = student_nn_depth\n        model_hyperparams_dict['student_nn_width'] = student_nn_width\n        model_hyperparams_dict['leaky_relu_alpha'] = leaky_relu_alpha\n        \n        model_hyperparams_dict['init_params_dict'] = init_params_dict\n        model_hyperparams_dict['y_GT_stats_dict']  = y_GT_stats_dict\n        \n        model_hyperparams_list.append(model_hyperparams_dict)\n\nexperiment_start_time = time.time()\n\ntraining_results = {}\nfor model_hyperparams_dict in model_hyperparams_list:\n    print('--------------------------------------------------------------------------------------------')\n    for key, value in model_hyperparams_dict.items():\n        print('  %s: %s' %(str(key), str(value)))\n    model_training_start_time = time.time()\n    \n    model_name = model_hyperparams_dict['model_name']\n    student_nn_depth = model_hyperparams_dict['student_nn_depth']\n    student_nn_width = model_hyperparams_dict['student_nn_width']\n    \n    training_results[model_name] = {}\n    training_results[model_name]['model_hyperparams_dict'] = model_hyperparams_dict\n    training_results[model_name]['GT_weights_dict'] = GT_weights_dict\n    training_results[model_name]['all learning curves'] = []\n    training_results[model_name]['all final losses'] = []\n    if save_student_weights:\n        training_results[model_name]['all start weight dicts'] = []\n        training_results[model_name]['all final weight dicts'] = []\n    \n    # train several student networks with different random weight inits\n    for k in range(num_random_inits):\n        curr_model_name = '%s_%d' %(model_name, k+1)\n        print('currently training \"%s\":' %(curr_model_name))\n\n        # create model\n        student_model = create_student_FCN(input_dim, output_dim, student_nn_depth, student_nn_width, curr_model_name, leaky_relu_alpha, student_initializer)\n        student_model.compile(optimizer=\"adam\", loss=\"mse\")\n\n        # extract and store the weights of the randomly initialized model\n        if save_student_weights:\n            student_init_weights_dict = extract_weights_dict(student_model, show_prints=False)\n            training_results[model_name]['all start weight dicts'].append(student_init_weights_dict)\n        \n        # train model\n        history = LossHistoryCallback()\n        subset_inds = np.random.permutation(X_train.shape[0])[:(num_train_iterations * batch_size)]\n        student_model.fit(X_train[subset_inds], y_train[subset_inds], epochs=1, callbacks=[history], verbose=0)\n        \n        # store learning curves and final weights dicts\n        if save_student_weights:\n            student_final_weights_dict = extract_weights_dict(student_model, show_prints=False)\n            training_results[model_name]['all final weight dicts'].append(student_final_weights_dict)\n        \n        curr_learning_curves = {}\n        curr_learning_curves['num_batches'] = np.array(history.valid_losses_batch_ind)\n        curr_learning_curves['num_samples'] = batch_size * curr_learning_curves['num_batches']\n        curr_learning_curves['valid_1_loss']  = np.array(history.per_batch_valid_losses_1)\n        curr_learning_curves['valid_2_loss']  = np.array(history.per_batch_valid_losses_2)\n        curr_learning_curves['valid_3_loss']  = np.array(history.per_batch_valid_losses_3)\n        curr_learning_curves['valid_4_loss']  = np.array(history.per_batch_valid_losses_4)\n\n        training_results[model_name]['all learning curves'].append(curr_learning_curves)\n        \n        # store some aggregate properties and display some of the progress\n        curr_final_point = curr_learning_curves['valid_1_loss'][-1]\n        training_results[model_name]['all final losses'].append(curr_final_point)\n        \n        print('                final loss: %.5f' %(curr_final_point))\n        \n    # collect key outcomes\n    training_results[model_name]['key outcomes'] = {}\n    training_results[model_name]['key outcomes']['best final point'] = np.array([x for x in training_results[model_name]['all final losses']]).min()\n    training_results[model_name]['key outcomes']['mean final point'] = np.array([x for x in training_results[model_name]['all final losses']]).mean()\n    training_results[model_name]['key outcomes']['std final point']  = np.array([x for x in training_results[model_name]['all final losses']]).std()\n    \n    curr_model_duration_min = (time.time() - model_training_start_time) \/ 60\n    print('finished all random inits for current model. took %.1f minutes' %(curr_model_duration_min))\n    print('some aggregate summary results:')\n    print('---------------------------------------------')\n    print('  mean final loss: %.5f)' %(training_results[model_name]['key outcomes']['mean final point']))\n    print('  std  final loss: %.5f)' %(training_results[model_name]['key outcomes']['std final point']))\n    print('  best final loss: %.5f)' %(training_results[model_name]['key outcomes']['best final point']))\n    print('---------------------------------------------')\n    print('--------------------------------------------------------------------------------------------')\n\n    \nexperiment_duration_hour = (time.time() - experiment_start_time) \/ 3600\nprint('finished collecting all data for current experiment. took %.2f hours' %(experiment_duration_hour))","18a5b346":"num_models = len(list(training_results.keys()))\n\nlong_to_short_init_map = {}\nlong_to_short_init_map['glorot_uniform']   = 'glorot'\nlong_to_short_init_map['truncated_normal'] = 'gauss'\nlong_to_short_init_map['orthogonal']       = 'ortho'\nlong_to_short_init_map['fanin_normal']     = 'fanin'\n\nteacher_short_init_method = long_to_short_init_map[teacher_init_method]\nstudent_short_init_method = long_to_short_init_map[student_init_method]\n\nif apply_activation_normalization_teacher:\n    if activation_normalization_method == 'full':\n        teacher_short_init_method += '_F_norm'\n    elif activation_normalization_method == 'input_output':\n        teacher_short_init_method += '_IO_norm'\n    elif activation_normalization_method == 'input':\n        teacher_short_init_method += '_I_norm'\n    elif activation_normalization_method == 'output':\n        teacher_short_init_method += '_O_norm'\n\noutput_filename = 'FCN_LReLU_0%d_%s_init__I-DxW-O__%d-%dx%d-%d__n_models_%d__n_inits_%d_%s__n_iter_%d__seed_%d.pickle' %(10 * leaky_relu_alpha, teacher_short_init_method, \n                                                                                                                         input_dim, nn_depth, nn_width, output_dim, num_models, num_random_inits, \n                                                                                                                         student_short_init_method, num_train_iterations, random_seed)\n\nprint('saving \"%s\"' %(output_filename))\nwith open(output_filename, 'wb') as handle:\n    pickle.dump(training_results, handle, protocol=pickle.HIGHEST_PROTOCOL)","1c8c0ac7":"# Save the initialization parameters","d46360b5":"# 1D view of inputs ($X$)","aa75aa3a":"# Train a single model as an example","1a03ee51":"# Define special callback to evaluate results on 4 different valid sets","0d0dbaac":"# Display the activation histograms for all units in GT teacher network","0d494b57":"# Create a GT Teacher network","54289a5a":"# 1D view of outputs ($y$)","26a2b349":"# Create a dataset\n\n","7e57057c":"# For illustration, have a look at the different distributions","0c7cf3a1":"# Save the results","9cba8c1d":"# Helper functions","292ab6c4":"# Fix model weights if requested and show it again","5912359f":"# Look at learning curves ","51617ac5":"# Train many student networks","1d4f4a64":"# 2D view of inputs ($X$)","84e09aa4":"# Collect and display GT teacher model weights"}}