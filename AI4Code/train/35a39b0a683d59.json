{"cell_type":{"f89d5a6e":"code","f533caeb":"code","89f18c37":"code","3c6b1444":"code","f5ffe854":"code","5d6809da":"code","eabac295":"code","830f356a":"code","e99a452b":"code","957ca29c":"code","9f8cc1cc":"code","8ca2036f":"code","d19ebc4a":"code","d5990198":"code","7ba501fe":"code","d28f56b0":"code","b69e2115":"code","a750dbac":"code","85c2b8ad":"code","de856ba6":"markdown","85642e6c":"markdown","13fe0b27":"markdown","c2dfec2d":"markdown"},"source":{"f89d5a6e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f533caeb":"import optuna\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.ensemble import StackingRegressor\n\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import PowerTransformer\n# from feature_engine.encoding import OrdinalEncoder\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# !pip install feature_engine --quiet","89f18c37":"# train = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/train.csv\")\n# test = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/test.csv\")\n# submission = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/sample_submission.csv\")\n\n# X = train.drop(columns=['id', 'target'], axis=1)\n# y = train['target']\n\n# df_test = test.drop(columns=['id'], axis=1)\n\n# # Select categorical columns\n# categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n\n# # Select numerical columns\n# numerical_cols = X.select_dtypes(include=['float64']).columns.tolist()","3c6b1444":"# # Rare label encoding\n# from feature_engine.encoding import RareLabelEncoder\n\n# rare_encoder = RareLabelEncoder(tol=0.041, n_categories=1, replace_with='Z')\n# rare_encoder.fit(X_train[categorical_cols])\n\n# X_train_rare = rare_encoder.transform(X_train[categorical_cols]).add_prefix('rare_')\n# X_test_rare = rare_encoder.transform(X_test[categorical_cols]).add_prefix('rare_')\n\n# from feature_engine.encoding import OrdinalEncoder\n\n# ordinal_encoder = OrdinalEncoder(encoding_method='ordered')\n# ordinal_encoder.fit(X_train_rare, y)\n\n# X_train_ordinal = ordinal_encoder.transform(X_train_rare).add_prefix('ordinal_')\n# X_test_ordinal = ordinal_encoder.transform(X_test_rare).add_prefix('ordinal_')\n\n# # Categorical Encoding Frequency Encoding\n# from feature_engine.encoding import CountFrequencyEncoder\n\n# count_enc = CountFrequencyEncoder(encoding_method='frequency')\n# count_enc.fit(X_train_rare)\n\n# X_train_freq = count_enc.transform(X_train_rare).add_prefix('freq_')\n# X_test_freq= count_enc.transform(X_test_rare).add_prefix('freq_')\n\n# from sklearn.preprocessing import PowerTransformer\n\n# scaler = PowerTransformer()\n# scaler.fit(X_train[numerical_cols])\n\n# X_train_power = scaler.transform(X_train[numerical_cols])\n# X_test_power = scaler.transform(X_test[numerical_cols])\n\n# X_train_power = pd.DataFrame(X_train_power, columns=numerical_cols).add_prefix('power_')\n# X_test_power = pd.DataFrame(X_test_power, columns=numerical_cols).add_prefix('power_')\n\n# from sklearn.preprocessing import MinMaxScaler\n\n# scaler = MinMaxScaler()\n# scaler.fit(X_train[numerical_cols])\n\n# X_train_minmax = scaler.transform(X_train[numerical_cols])\n# X_test_minmax = scaler.transform(X_test[numerical_cols])\n\n# X_train_minmax = pd.DataFrame(X_train_minmax, columns=numerical_cols).add_prefix('minmax_')\n# X_test_minmax = pd.DataFrame(X_test_minmax, columns=numerical_cols).add_prefix('minmax_')\n\n# X = pd.concat([X_train_freq, X_train_ordinal, \n#                X_train_power, X_train_minmax], axis=1)\n\n# X_test = pd.concat([X_test_freq, X_test_ordinal, \n#                     X_test_power, X_test_minmax], axis=1)","f5ffe854":"# ordinal_encoder = OrdinalEncoder(encoding_method='ordered')\n# ordinal_encoder.fit(X_train[categorical_cols], y)\n\n# X_train_ordinal = ordinal_encoder.transform(X_train[categorical_cols])\n# X_test_ordinal = ordinal_encoder.transform(X_test[categorical_cols])\n\n# X = pd.concat([X_train_ordinal, X_train[numerical_cols]], axis=1)\n\n# X_test = pd.concat([X_test_ordinal, X_test[numerical_cols]], axis=1)","5d6809da":"# params = {\n#     'random_state': 42,\n#     'tree_method':'gpu_hist',  # Use GPU acceleration\n#     'gamma': trial.suggest_float(\n#         'gamma', 0.0, 20.0\n#     ),\n#     'reg_lambda': trial.suggest_float(\n#         'reg_lambda', 0.0, 100\n#     ),\n#     'reg_alpha': trial.suggest_float(\n#         'reg_alpha', 0.0, 100.0\n#     ),\n#     'learning_rate': trial.suggest_float(\n#         'learning_rate', 0.2, 0.4\n#     ),\n#     'n_estimators': trial.suggest_int(\n#         \"n_estimators\", 5000, 10000\n#     ),\n#     'max_depth': trial.suggest_int(\n#         \"max_depth\", 3, 10\n#     ),\n#     'min_child_weight': trial.suggest_int(\n#         'min_child_weight', 2, 20\n#     ),\n#     'subsample': trial.suggest_float(\n#         'subsample', 0.5, 0.9\n#     ),\n#     'colsample_bytree': trial.suggest_float(\n#         'colsample_bytree', 0.5, 0.9\n#     ),\n#     'colsample_bylevel': trial.suggest_float(\n#         'colsample_bylevel', 0.5, 0.9\n#     ),\n#     'colsample_bynode': trial.suggest_float(\n#         'colsample_bynode', 0.5, 0.9\n#     ),\n# }","eabac295":"# # Uncomment this\n# def objective(trial):\n#     splits = 10\n#     kfold = KFold(splits, shuffle=True, random_state=42)\n    \n#     # To select which parameters to optimize, please look at the XGBoost documentation:\n#     # https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\n#     params = {\n#         'random_state': 42,\n#         'tree_method':'gpu_hist',  # Use GPU acceleration\n#         'gamma': trial.suggest_float(\n#             'gamma', 0.0, 0.2\n#         ),\n#         'reg_lambda': trial.suggest_float(\n#             'reg_lambda', 50.0, 70.0\n#         ),\n#         'reg_alpha': trial.suggest_float(\n#             'reg_alpha', 60.0, 80.0\n#         ),\n#         'learning_rate': trial.suggest_float(\n#             'learning_rate', 0.2, 0.3\n#         ),\n#         'n_estimators': trial.suggest_int(\n#             \"n_estimators\", 7000, 10000\n#         ),\n#         'max_depth': trial.suggest_int(\n#             \"max_depth\", 3, 5\n#         ),\n#         'min_child_weight': trial.suggest_int(\n#             'min_child_weight', 50, 100\n#         ),\n#         'subsample': trial.suggest_float(\n#             'subsample', 0.9, 1.0\n#         ),\n#         'colsample_bytree': trial.suggest_float(\n#             'colsample_bytree', 0.0, 0.1\n#         ),\n#         'colsample_bylevel': trial.suggest_float(\n#             'colsample_bylevel', 0.7, 0.9\n#         ),\n#         'colsample_bynode': trial.suggest_float(\n#             'colsample_bynode', 0.7, 0.9\n#         )\n#     }\n    \n#     # Creating an array of zeros for storing \"out of fold\" predictions\n#     final_predictions = []\n#     scores = []\n    \n#     # Generating folds and making training and prediction for each of 10 folds\n#     for num, (train_idx, valid_idx) in enumerate(kfold.split(X)):\n#         X_train, X_valid = X.loc[train_idx], X.loc[valid_idx]\n#         y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n#         X_test = df_test.copy()\n        \n#         ordinal_encoder = OrdinalEncoder()\n#         X_train[categorical_cols] = ordinal_encoder.fit_transform(X_train[categorical_cols])\n#         X_valid[categorical_cols] = ordinal_encoder.transform(X_valid[categorical_cols])\n#         X_test[categorical_cols] = ordinal_encoder.transform(X_test[categorical_cols])\n\n#         scaler = PowerTransformer()\n#         X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n#         X_valid[numerical_cols] = scaler.transform(X_valid[numerical_cols])\n#         X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n        \n#         model = XGBRegressor(**params)\n#         model.fit(X_train, y_train,\n#                   verbose=False,\n#                   # These three parameters will stop training before a model starts overfitting \n#                   eval_set=[(X_train, y_train), (X_valid, y_valid)],\n#                   eval_metric=\"rmse\",\n#                   early_stopping_rounds=300,\n#                   )\n\n#         # Getting mean test data predictions (i.e. divided by number of splits)\n#         preds_test = model.predict(X_test)\n#         preds_valid = model.predict(X_valid)\n#         final_predictions.append(preds_test)\n\n#         # Getting score for a fold model\n#         rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n# #         print(f'VotingRegressor: Fold {num+1}, RMSE: {rmse}')\n#         scores.append(rmse)\n\n#     return np.mean(scores)\n\n# study = optuna.create_study(direction='minimize', study_name='xgb_parameter_opt')\n# study.optimize(objective, n_trials=2)\n# print('Number of finished trials:', len(study.trials))\n# print('Best trial:', study.best_trial.params)\n\n# xgb_params = study.best_trial.params\n# xgb_params # 0.7177683244544724","830f356a":"# def objective(trial, data=X, target=y):\n#     splits = 10\n#     kfold = KFold(splits, shuffle=True, random_state=42)\n    \n#     params = {\n#         'device_type':'gpu',  # Use GPU acceleration\n#         'num_leaves': trial.suggest_int(\n#             'num_leaves', 40, 50\n#         ),\n#         'learning_rate': trial.suggest_float(\n#             'learning_rate', 0.0, 0.2\n#         ),\n#         'n_estimators': trial.suggest_int(\n#             \"n_estimators\", 5000, 10000\n#         ),\n#         'max_depth': trial.suggest_int(\n#             'max_depth', 1, 10\n#         ),\n#         'reg_alpha': trial.suggest_float(\n#             'reg_alpha', 20.0, 30.0\n#         ),\n#         'seed': 42,\n#         'min_child_weight': trial.suggest_loguniform(\n#             'min_child_weight', 1e-1, 1\n#         ),\n#     }\n    \n#     # Creating an array of zeros for storing \"out of fold\" predictions\n#     final_predictions = []\n#     scores = []\n    \n#     # Generating folds and making training and prediction for each of 10 folds\n#     for num, (train_idx, valid_idx) in enumerate(kfold.split(X)):\n#         X_train, X_valid = X.loc[train_idx], X.loc[valid_idx]\n#         y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n\n#         model = LGBMRegressor(**params)\n#         model.fit(X_train, y_train,\n#                   verbose=False,\n#                   # These three parameters will stop training before a model starts overfitting \n#                   eval_set=[(X_train, y_train), (X_valid, y_valid)],\n#                   eval_metric=\"rmse\",\n#                   early_stopping_rounds=300,\n#                   )\n\n#         # Getting mean test data predictions (i.e. divided by number of splits)\n#         preds_test = model.predict(X_test)\n#         preds_valid = model.predict(X_valid)\n#         final_predictions.append(preds_test)\n\n#         # Getting score for a fold model\n#         rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n# #         print(f'VotingRegressor: Fold {num+1}, RMSE: {rmse}')\n#         scores.append(rmse)\n\n#     return np.mean(scores)\n\n# study = optuna.create_study(direction='minimize', study_name='lgbm_parameter_opt')\n\n# study.optimize(objective, n_trials=2)\n# print('Number of finished trials:', len(study.trials))\n# print('Best trial:', study.best_trial.params)\n\n# lgbm_params = study.best_trial.params\n# lgbm_params # 0.7181305083440424","e99a452b":"# def objective(trial, data=X, target=y):\n#     splits = 10\n#     kfold = KFold(splits, shuffle=True, random_state=42)\n    \n#     # To select which parameters to optimize, please look at the XGBoost documentation:\n#     # https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\n#     params = {\n#         'task_type':'GPU',  # Use GPU acceleration\n#         'learning_rate': trial.suggest_float(\n#             'learning_rate', 0.0, 1.0\n#         ),\n#         'random_strength': trial.suggest_loguniform(\n#             'random_strength', 1e-9, 10\n#         ),\n#         'bagging_temperature':trial.suggest_float(\n#             'bagging_temperature', 0.0, 1.0\n#         ),\n#         'n_estimators': trial.suggest_int(\n#             \"n_estimators\", 1000, 7000\n#         ),\n#         'depth': trial.suggest_int(\n#             'max_depth', 1, 15\n#         ),\n#         'reg_lambda': trial.suggest_int(\n#             'reg_alpha', 0.0, 50\n#         ),\n#         'random_state': 42,\n# #         'silent': True\n#         'verbose': 0\n#     }\n    \n#     # Creating an array of zeros for storing \"out of fold\" predictions\n#     final_predictions = []\n#     scores = []\n    \n#     # Generating folds and making training and prediction for each of 10 folds\n#     for num, (train_idx, valid_idx) in enumerate(kfold.split(X)):\n#         X_train, X_valid = X.loc[train_idx], X.loc[valid_idx]\n#         y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n\n#         model = CatBoostRegressor(**params)\n#         model.fit(X_train, y_train,\n#                   verbose=False,\n#                   # These three parameters will stop training before a model starts overfitting \n#                   eval_set=(X_valid, y_valid),\n# #                   cat_features=categorical_cols,\n#                   early_stopping_rounds=300,\n#                   )\n\n#         # Getting mean test data predictions (i.e. divided by number of splits)\n#         preds_test = model.predict(X_test)\n#         preds_valid = model.predict(X_valid)\n#         final_predictions.append(preds_test)\n\n#         # Getting score for a fold model\n#         rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n# #         print(f'VotingRegressor: Fold {num+1}, RMSE: {rmse}')\n#         scores.append(rmse)\n\n#     return np.mean(scores)\n\n# study = optuna.create_study(direction='minimize', study_name='cb_parameter_opt')\n# study.optimize(objective, n_trials=30)\n# print('Number of finished trials:', len(study.trials))\n# print('Best trial:', study.best_trial.params)\n\n# cb_params = study.best_trial.params\n# cb_params","957ca29c":"lgbm_params = {\n#     'device_type':'gpu',\n#              'gpu_platform_id': 0,\n#              'gpu_device_id': 0,\n             'seed': 42,\n             'num_leaves': 47,\n             'learning_rate': 0.09369959637621708,\n             'n_estimators': 5135,\n             'max_depth': 2,\n             'reg_alpha': 26.928438150963935,\n             'min_child_weight': 0.37004853205139515\n            }\n\nxgb_params = {\n#     'tree_method': 'gpu_hist',\n#             'gpu_id':0,\n#             'predictor':'gpu_predictor',\n             'seed':42,\n             'lambda': 64.68740340138797,\n             'alpha': 40.27766966065833,\n             'colsample_bytree': 0.031521737831702715,\n             'subsample': 0.9970842659217856,\n             'learning_rate': 0.34087533404234777,\n             'n_estimators': 8390,\n             'max_depth': 6,\n             'min_child_weight': 112\n            }\n\ncb_params = {\n#     'task_type':'GPU',\n                  'random_state':42,\n                  'learning_rate': 0.8058733338962497,\n                 'n_estimators': 4536,\n                 'max_depth': 1,\n                 'reg_lambda': 6.82720069348649\n                }","9f8cc1cc":"xgb = XGBRegressor(**xgb_params)\n# xgb.fit(X, y)\n\nlgbm = LGBMRegressor(**lgbm_params)\n# lgbm.fit(X, y)\n\ncb = CatBoostRegressor(**cb_params, silent=True)\n# cb.fit(X, y, logging_level='Silent')\n\nvote_regressor = VotingRegressor([('xgboost', xgb), ('lightGBM', lgbm), ('catboost',cb)], \n                        weights=[0.6,0.35,0.05])","8ca2036f":"train = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/sample_submission.csv\")\n\nX = train.drop(columns=['id', 'target'], axis=1)\ny = train['target']\n\ndf_test = test.drop(columns=['id'], axis=1)\n\n# Select categorical columns\ncategorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n\n# Select numerical columns\nnumerical_cols = X.select_dtypes(include=['float64']).columns.tolist()","d19ebc4a":"%%time\nsplits = 10\nkfold = KFold(splits, shuffle=True, random_state=42)\n\n# Creating an array of zeros for storing \"out of fold\" predictions\nfinal_predictions = []\nscores = []\n    \n# Generating folds and making training and prediction for each of 10 folds\nfor num, (train_idx, valid_idx) in enumerate(kfold.split(X)):\n    X_train, X_valid = X.loc[train_idx], X.loc[valid_idx]\n    y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n    X_test = df_test.copy()\n\n    ordinal_encoder = OrdinalEncoder()\n    X_train[categorical_cols] = ordinal_encoder.fit_transform(X_train[categorical_cols])\n    X_valid[categorical_cols] = ordinal_encoder.transform(X_valid[categorical_cols])\n    X_test[categorical_cols] = ordinal_encoder.transform(X_test[categorical_cols])\n\n    scaler = PowerTransformer()\n    X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n    X_valid[numerical_cols] = scaler.transform(X_valid[numerical_cols])\n    X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n\n    model = VotingRegressor([('xgboost', xgb), \n                             ('lightGBM', lgbm), \n                             ('voting_regression',vote_regressor)], \n                            weights=[0.2,0.1,0.7])\n\n#     model = StackingRegressor([('xgboost', xgb), ('lightGBM', lgbm), ('catboost',cb)])\n    \n    model.fit(X_train, y_train)\n\n    # Getting mean test data predictions (i.e. divided by number of splits)\n    preds_test = model.predict(X_test)\n    preds_valid = model.predict(X_valid)\n    final_predictions.append(preds_test)\n    \n    # Getting score for a fold model\n    rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n#     print(f'VotingRegressor: Fold {num+1}, RMSE: {rmse}')\n    scores.append(rmse)\n    \nprint(np.mean(scores), np.std(scores))","d5990198":"%%time\nvote_regressor = VotingRegressor([('xgboost', xgb), ('lightGBM', lgbm), ('catboost',cb)], \n                        weights=[0.6,0.35,0.05])\n\ncv_results = cross_val_score(vote_regressor, X, y, cv=5, scoring='neg_mean_squared_error')\nnp.sqrt(np.abs(cv_results.mean()))","7ba501fe":"%%time\nvote_regressor2 = VotingRegressor([('xgboost', xgb), \n                                  ('lightGBM', lgbm), \n                                  ('voting_regression',vote_regressor)], \n                        weights=[0.2,0.1,0.7])\n\ncv_results = cross_val_score(vote_regressor2, X, y, cv=5, scoring='neg_mean_squared_error')\nnp.sqrt(np.abs(cv_results.mean()))","d28f56b0":"y_pred = np.mean(np.column_stack(final_predictions), axis=1)\n\nsubmission['target'] = y_pred\n\n# Save submission_df to csv and upload to Kaggle\nsubmission.to_csv(r'submission_oof_stackregressor.csv', index=False)","b69e2115":"import numpy as np\nimport pandas as pd\nfrom sklearn import model_selection\nfrom sklearn.preprocessing import OrdinalEncoder, PowerTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor","a750dbac":"df_train = pd.read_csv(r'..\/input\/30-days-of-ml\/train.csv')\nkf = model_selection.KFold(n_splits=10, shuffle=True, random_state=42)\n\ndf_train['kfold'] = -1\n\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(X=df_train)):\n    df_train.loc[valid_idx, 'kfold'] = fold\n\ndf_train.to_csv('train_folds.csv', index=False)","85c2b8ad":"\n\ndf = pd.read_csv('train_folds.csv')\ndf_test = pd.read_csv('..\/input\/30-days-of-ml\/test.csv')\nsubmission = pd.read_csv('..\/input\/30-days-of-ml\/sample_submission.csv')\n\nuseful_features = [c for c in df.columns if c not in ['id', 'target', 'kfold']]\ndf_test = df_test[useful_features]\n\n# Select categorical columns\ncategorical_cols = df[useful_features].select_dtypes(include=['object']).columns.tolist()\n\n# Select numerical columns\nnumerical_cols = df[useful_features].select_dtypes(include=['float64']).columns.tolist()\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\n\nfor fold in range(10):\n    xtrain = df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n    \n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ordinal_encoder = OrdinalEncoder()\n    xtrain[categorical_cols] = ordinal_encoder.fit_transform(xtrain[categorical_cols])\n    xvalid[categorical_cols] = ordinal_encoder.transform(xvalid[categorical_cols])\n    xtest[categorical_cols] = ordinal_encoder.transform(xtest[categorical_cols])\n\n    scaler = PowerTransformer()\n    xtrain[numerical_cols] = scaler.fit_transform(xtrain[numerical_cols])\n    xvalid[numerical_cols] = scaler.transform(xvalid[numerical_cols])\n    xtest[numerical_cols] = scaler.transform(xtest[numerical_cols])\n    \n    xgb_params = {\n#             'tree_method': 'gpu_hist',\n#             'gpu_id':0,\n#             'predictor':'gpu_predictor',\n             'seed':42,\n             'lambda': 64.68740340138797,\n             'alpha': 40.27766966065833,\n             'colsample_bytree': 0.031521737831702715,\n             'subsample': 0.9970842659217856,\n             'learning_rate': 0.34087533404234777,\n             'n_estimators': 8390,\n             'max_depth': 6,\n             'min_child_weight': 112\n            }\n    \n    # Define the model \n    model = XGBRegressor(random_state=fold, n_jobs=4, **xgb_params)\n\n    # Train the model (will take about 10 minutes to run)\n    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores)) # 0.7177641712350958 0.002237195974190003\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_1\"]\nfinal_valid_predictions.to_csv(\"train_pred_1.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_1\"]\nsample_submission.to_csv(\"test_pred_1.csv\", index=False)\n\npreds = np.mean(np.column_stack(final_predictions), axis=1)\npreds\n\nsubmission['target'] = preds\nsubmission.to_csv('submission.csv', index=False)","de856ba6":"## LightGBM","85642e6c":"## CatBoost","13fe0b27":"# Modeling\n\n## XGBoost","c2dfec2d":"## Feature Engineering\n\nFor numeric columns, I've used power transform. For categorical column, I've used Frequency Encoding. Please refer to [this](https:\/\/feature-engine.readthedocs.io\/en\/latest\/index.html) for more information on other ways to feature engineer your training variables "}}