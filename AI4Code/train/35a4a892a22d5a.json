{"cell_type":{"362bb0f0":"code","f0ef3106":"code","695952d5":"code","f55773f0":"code","9d3ef0ad":"code","9f451507":"code","7b9ffef0":"code","b34442fc":"code","dd6fdd30":"code","6de42597":"code","c872ed12":"code","4a174210":"code","c8189562":"code","4998c725":"code","061cd870":"code","089f577e":"code","0774a40c":"code","11e8bd1a":"code","86093bf7":"code","2ada478d":"code","7f577ca4":"code","60bd9c39":"code","75db6c41":"code","022b7863":"code","ac623df3":"code","827eccc3":"code","a2c20cd8":"code","ac7bfed4":"code","2c8e5e9c":"code","b615f37d":"code","deb22cb5":"code","ddeb1ba8":"code","31909615":"code","866f2b74":"code","1636a1ed":"code","e818d93f":"code","a80e353a":"code","0662cd7b":"code","7dd39e60":"code","474f25b6":"code","acb825fd":"markdown","2e5ec51f":"markdown","d9734787":"markdown","d0fff669":"markdown","53b9eefb":"markdown","4fd789bc":"markdown","03a08530":"markdown","7bf0dfa3":"markdown","158c7527":"markdown","520b545b":"markdown","d5925551":"markdown","f90ca266":"markdown","d2968be3":"markdown"},"source":{"362bb0f0":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"whitegrid\")\nsns.set_palette('hot')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, plot_roc_curve\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nimport sys, glob, copy, warnings, time\nwarnings.simplefilter('ignore') # once | error | always | default | module\n\ninp = '\/kaggle\/input\/tabular-playground-series-mar-2021\/'","f0ef3106":"df, features = {}, {}\nprint('{:18s}{:>10s}{:>5s}{:>5s}'.format('FILE', 'ROWS', 'COLS', 'NULL'))\nfor file in glob.glob(f'{inp}\/*.csv'):\n    label = file.split('\/')[-1].split('.')[0]\n    df[label] = pd.read_csv(file, index_col='id')\n    features[label] = set(df[label].columns.to_list())\n    print('{:18s}{:10,d}{:5d}{:5d}'.format(label, *df[label].shape, df[label].isna().any().sum()))","695952d5":"(df['sample_submission'].index == df['test'].index).all()\n# Straightforward if True.","f55773f0":"features['train'] == features['test'].union(features['sample_submission'])\n# Straightforward if True.","9d3ef0ad":"df['train'].sample(5)","9f451507":"sr = pd.DataFrame(df['train'].dtypes, columns=['dtype'])\nfor dtype, dtype_data in sr.groupby('dtype'):\n    print('{:2d} columns of dtype {}\\n{}'.format(len(dtype_data), dtype, '='*10))\n    print(dtype_data.index.to_list(), '\\n')","7b9ffef0":"df['train']['target'].unique()\n# target is in fact categorical, not continuous.","b34442fc":"df['train'].describe(include='float')","dd6fdd30":"df['train'].describe(include='object')","6de42597":"features = {'cat': df['train'].columns[ df['train'].columns.str.startswith('cat') ].to_list(),\n            'con': df['train'].columns[ df['train'].columns.str.startswith('con') ].to_list(),\n            'num': df['train'].select_dtypes(include=[float, int]).columns.to_list()}\n# We are going to use these over and over; save us from having to do dot-columns again and again.\nfeatures","c872ed12":"unik = {'train'    : {}, # to hold unique categorical values from train\n        'test'     : {}} # to hold unique categorical values from test\nprint('{:<8s}{} {}'.format('FEATURE', 'NUNIQUE', 'UNIQUE VALUES IN TRAIN'))\n# Print list of unique values starting from the lowest nunique, in that order. Features near the bottom are the troublesome ones.\nfor feature in df['train'][features['cat']].nunique().sort_values().index:\n    unik['train'].update({feature: set(sorted(df['train'][feature].unique()))})\n    unik['test'].update({feature: set(sorted(df['test'][feature].unique()))})\n    print('{:<8s}{:7d} {}'.format(feature, len(unik['train'][feature]), str(unik['train'][feature])))","4a174210":"print('{:<8s}{:76s}'.format('FEATURE', 'UNIQUE VALUES IN TRAIN'))\nfor feature in features['cat']:\n    if unik['train'][feature]!=unik['test'][feature]:\n        print('in train but not in test:', feature, unik['train'][feature].difference(unik['test'][feature]))\n        print('in test but not in train:', feature, unik['test'][feature].difference(unik['train'][feature]))","c8189562":"ncoda = OrdinalEncoder().fit(pd.concat([ df['train'][features['cat']], \n                             df['test'][features['cat']] ]))\n# For sanity check only; will be deleted real soon:\norig = copy.deepcopy(df)\nfor dataset in ['train', 'test']:\n    df[dataset][features['cat']] = ncoda.transform(df[dataset][features['cat']])\n    df[dataset][features['cat']] = df[dataset][features['cat']].astype(int)# .astype('category')\nncoda.categories_","4998c725":"# Just a pedantic sanity check.\nassert (ncoda.inverse_transform(df['train'][features['cat']]) == orig['train'][features['cat']]).all().all()\nassert (ncoda.inverse_transform(df['test'][features['cat']]) == orig['test'][features['cat']]).all().all()\ndel orig   # Deleted as promised.","061cd870":"df['train'].info()","089f577e":"valuecount2D = pd.DataFrame()\nfor nfeature, feature in enumerate(features['cat']):\n    tis = {'feature': feature}\n    for group, group_data in df['train'].groupby(feature):\n        tis['feature_category'] = group\n        if group_data['target'].value_counts().nunique()==1:\n            print(feature, group)\n        for tis['target'], tis['count'] in group_data['target'].value_counts().iteritems():\n            valuecount2D = pd.concat([valuecount2D, pd.DataFrame(tis, index=[f'{feature}_{group}_{tis[\"target\"]}'])])\n# valuecount2D.reset_index(drop=True, inplace=True)\nvaluecount2D.rename(columns={0: 'target=0', 1: 'target=1'}, inplace=True)\nvaluecount2D","0774a40c":"# sanity\nauto = valuecount2D.loc['cat18_3_1', 'count']\nmanual = len(df['train'].query('target==1 and cat18==3'))\nif auto==manual:\n    print('sane')\nelse:\n    print('insane')","11e8bd1a":"cols = 3\nrows = int(np.ceil(len(features['cat'])\/cols))\nfig, ax = plt.subplots(rows, cols, figsize=(15, 7*rows), sharex=True)\n# As before, start with well-behaved features, with the problematic ones at the end, in that order.\nfor nfeature, feature in enumerate(df['train'][features['cat']].nunique().sort_values().index):\n    tis_ax = ax[nfeature\/\/cols][nfeature%cols]\n    sns.barplot(data=valuecount2D.loc[valuecount2D['feature']==feature], \n                x='count', y='feature_category', hue='target', orient='h', ax=tis_ax, palette='hot')\n    tis_ax.set_title(feature)\n# As warned by earlier text output we find cat5 and cat10 screaming for attention.","86093bf7":"%%time\nplt.figure(figsize=(15, 5))\nsns.violinplot(data=df['train'][ features['con'] ])","2ada478d":"tmp = df['train'][features['con']]\nxx = tmp.mean()\nyy = tmp.median()\nplt.figure(figsize=(10, 10))\nplt.plot([xx.min(), xx.max()], [yy.min(), yy.max()], 'y-.')\nplt.plot(xx, yy, '.r')\nfor x, y, z in zip(xx, yy, tmp):\n    plt.text(x+.005, y, z)\n_ = plt.axis('equal'); plt.xlabel('feature mean'); plt.ylabel('feature median')","7f577ca4":"cols = 2\nrows = int(np.ceil(len(features['con'])\/cols))\nfig, ax = plt.subplots(rows, cols, figsize= (15, 5*rows))\nfor nfeature, feature in enumerate(features['con']):\n    sns.histplot(data=df['train'], y=feature, hue='target', stat='density', ax=ax[nfeature\/\/cols, nfeature%cols], palette='hot')","60bd9c39":"traintest = pd.concat([df['train'], df['test']])\nbinned = traintest[features['con']].apply(lambda x: pd.cut(x, bins=32, labels=False))\nplt.figure(figsize=(15, 15))\nnfeatures = len(features['con'])\nfor aa in range(1, nfeatures):\n    for bb in range(aa):\n        plt.subplot(nfeatures, nfeatures, aa*nfeatures + bb + 1)\n        sns.heatmap(binned.groupby(features['con'][aa]).apply(lambda x: x[features['con'][bb]].value_counts()).unstack(), \n                    square=True, cmap='hot', cbar=False, xticklabels=False, yticklabels=False)\n        plt.axis('off')\nfor tmp in range(1, nfeatures):\n    plt.subplot(nfeatures, nfeatures, nfeatures*tmp+1)\n    plt.axis('on'); plt.ylabel(features['con'][tmp])\nfor tmp in range(nfeatures-1):\n    plt.subplot(nfeatures, nfeatures, nfeatures*(nfeatures-1)+tmp+1)\n    plt.axis('on'); plt.xlabel(features['con'][tmp])\nfor tmp in range(1, nfeatures-1):\n    plt.subplot(nfeatures, nfeatures, nfeatures*(nfeatures-1)+tmp+1)\n    plt.ylabel('')","75db6c41":"%%time\ncorr = traintest.corr()\ncorr.to_csv('corr.csv')\nplt.figure(figsize=(15, 15))\nsns.heatmap(corr, mask=np.triu(np.ones_like(corr, dtype=bool)), annot=True, fmt='.1f', linewidths=.5, square=True, cmap='hot', annot_kws={'size': 10}, cbar_kws={\"shrink\": .5})","022b7863":"slimcorr = pd.Series(dtype=float)\nfor feature in corr.columns:\n    slimcorr.loc[feature] = corr[feature].sort_values()[-2]\nslimcorr.sort_values(ascending=False)\n# output reports no correlation too high; therefore too premature to drop any feature","ac623df3":"dataX = df['train'].copy()\ndatay = dataX.pop('target')\ntrainX, validX, trainy, validy = train_test_split(dataX, datay)\n\ndef trainNpredict(model):\n    tic = time.time()\n    pred = model.fit(trainX, trainy).predict_proba(validX)[:, 1]\n    roc_auc = roc_auc_score(validy, pred, average='micro')\n    print(\"roc_auc_score(validy, pred, average='micro') =\", roc_auc)\n    print(\"roc_auc_score(validy, pred) =\", roc_auc_score(validy, pred))\n#   plot ROC curve\n    return model, time.time()-tic, roc_auc\n\nmodel, tictoc, roc_auc = {}, pd.Series(dtype=float), pd.Series(dtype=float)","827eccc3":"label = 'rf'\nmodel[label], tictoc[label], roc_auc.loc[label] = trainNpredict(RandomForestClassifier(n_estimators=200, max_depth=7))","a2c20cd8":"label = 'lgb'\nmodel[label], tictoc[label], roc_auc.loc[label] = trainNpredict(LGBMClassifier(**{'is_unbalance': True}))","ac7bfed4":"label = 'xgb'\nscale_pos_weight = (df['train']['target']==0).sum() \/ (df['train']['target']==1).sum()\nmodel[label], tictoc[label], roc_auc.loc[label] = trainNpredict(XGBClassifier(**{'scale_pos_weight': scale_pos_weight}))","2c8e5e9c":"label = 'cat'\nmodel[label], tictoc[label], roc_auc.loc[label] = trainNpredict(CatBoostClassifier(**{'scale_pos_weight': scale_pos_weight}))","b615f37d":"roc_auc.sort_values(ascending=False)","deb22cb5":"tictoc.sort_values()","ddeb1ba8":"# manual back-of-envelop calculation\nplt.figure(figsize=(7, 5))\nkolor = {'rf': 'r',\n         'lgb': 'g',\n         'xgb': 'b',\n         'cat': 'k'}\nfor k, v in model.items():\n    df['train'][k] = v.predict_proba(dataX)[:, 1]\n    for threshold in np.linspace(df['train'][k].min(), df['train'][k].max(), 100):\n        positive = df['train'][k]>threshold\n        true_positive = positive & (datay==1)\n        false_positive = positive & (datay==0)\n        plt.plot(false_positive.sum()\/len(df['train']), true_positive.sum()\/len(df['train']), '.', color=kolor[k])\n        plt.xlabel('false positives'); plt.ylabel('true ppsitives')","31909615":"# auto: sklearn.metrics.plot_roc_curve\nfor k in model.keys():\n    plot_roc_curve(model[k], dataX, datay, color=kolor[k])","866f2b74":"df['sample_submission']['target'] = model[roc_auc.idxmax()].predict_proba(df['test'])[:, 1]\ndf['sample_submission'].to_csv('submission.csv')","1636a1ed":"gain = pd.DataFrame(index=trainX.columns)\nfor treetype in model.keys():\n    gain[treetype] = model[treetype].feature_importances_\ngain.rank().astype(int).sort_values(by='lgb')","e818d93f":"if 'BorutaShap' not in sys.modules:\n    !pip install BorutaShap\nfrom BorutaShap import BorutaShap","a80e353a":"Feature_Selector = BorutaShap(model=XGBClassifier(**{'tree_method':'gpu_hist'}), importance_measure='shap')   # importance_measure='gini'\n# Feature_Selector = BorutaShap(model=LGBMClassifier(), importance_measure='shap')\nFeature_Selector.fit(X=dataX, y=datay, n_trials=1000, verbose=False) # sample=False, train_or_test = 'test', normalize=True, verbose=True)","0662cd7b":"Feature_Selector.results_to_csv(filename='borutashap.csv')\nFeature_Selector.plot(which_features='all')  # X_size=15, figsize=(12,8), y_scale='log'","7dd39e60":"Feature_Selector.accepted","474f25b6":"Feature_Selector.features_to_remove","acb825fd":"## 4 baselines before tuning","2e5ec51f":"## Distribution: categorical features by target","d9734787":"## BorutaShap","d0fff669":"## Datatypes","53b9eefb":"![](https:\/\/marychin.org\/download\/kaggle\/tabmar.png)","4fd789bc":"## Pick the best baseline, submit and see","03a08530":"## Gain","7bf0dfa3":"## Distribution: continuous features by target","158c7527":"## Categories & encoding","520b545b":"## 2D flood maps: how features pair cross-talk\nSeaborn has one-liners for this; but runs till eternity without returning. Here is therefore a dirty hack.","d5925551":"Update 10th March:\n* Plot ROC curves: \n* - manual back-of-envelop calculation (excellent refresher);\n* - sklearn.metrics.plot_roc_curve.\n* ```roc_auc_score``` with and without ```average='micro'``` option: no difference found.\n\nThis is the first walkthrough of the March Playground:\n* identifying troublesome features such as ```cat10```, ```cat5```, ```cat8```, ```cat7``` and others, which require handling;\n* running quick-and-dirty baselines (without parameters tweaking) using LightGBM, XGBoost, CatBoost and Random Forests;\n* looking at gains and feature rankings from LightGBM, XGBoost, CatBoost and Random Forests;\n* running BorutaShap, which reports each feature as either confirmed important, unimportant or tentative.","f90ca266":"## Distribution: continuous features","d2968be3":"## Plot ROC curves"}}