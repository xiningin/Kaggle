{"cell_type":{"8ed33134":"code","b149598c":"code","e805da23":"code","d831d6e4":"code","9c0a65d5":"code","9fd24bfe":"code","9d10140e":"code","f44cb5ce":"code","207f0620":"code","a00846d8":"code","7e359be6":"code","ff31bfb2":"code","f05e8205":"code","96c384cd":"code","d66caf68":"code","864d6926":"code","a70a3deb":"code","38d713d5":"code","50855316":"code","cbb89a72":"code","39c546ad":"code","858c47c9":"code","db1fa6bf":"code","d331b101":"code","eff525a1":"code","008dd7ab":"code","cb971729":"code","3603bdb3":"code","6252220f":"code","a3e21f28":"code","433df8f4":"code","d2800862":"code","c6eb3881":"code","c9ba599c":"code","4c80bd84":"code","7076aabe":"code","749dfda9":"code","30cc4cf2":"code","93b62dba":"code","862b1055":"code","54023452":"code","34191274":"code","bc7955ae":"code","f649715e":"code","18e57d1c":"code","4a5a016b":"code","2e181d25":"markdown","c7ede3f9":"markdown","a88d37f5":"markdown","7bbad8cd":"markdown","537d4f38":"markdown","0d4fd235":"markdown","384983ea":"markdown","ba1c0298":"markdown","c6e1bfb3":"markdown","6087ccc9":"markdown","10d3696f":"markdown","20c7a40e":"markdown","4963af35":"markdown","f76a81f9":"markdown","25410969":"markdown","2753648b":"markdown","c47719d2":"markdown","106a65fc":"markdown","20fd8a0c":"markdown","caad824f":"markdown","fe7209dd":"markdown","567c35e6":"markdown","852a0bda":"markdown"},"source":{"8ed33134":"import pandas as pd\nimport numpy as np\nimport multiprocessing\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn import metrics\nimport gc\nfrom time import time\nimport datetime\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold,GroupKFold\nfrom sklearn.metrics import roc_auc_score\nwarnings.simplefilter('ignore')\nsns.set()\n%matplotlib inline","b149598c":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","e805da23":"%%time\nwarnings.simplefilter('ignore')\nfiles = ['..\/input\/ieee-fraud-detection\/test_identity.csv', \n         '..\/input\/ieee-fraud-detection\/test_transaction.csv',\n         '..\/input\/ieee-fraud-detection\/train_identity.csv',\n         '..\/input\/ieee-fraud-detection\/train_transaction.csv',\n         '..\/input\/ieee-fraud-detection\/sample_submission.csv']\n\ndef load_data(file):\n    return reduce_mem_usage(pd.read_csv(file))\n\nwith multiprocessing.Pool() as pool:\n    test_identity, test_transaction, train_identity, train_transaction, sample_submission = pool.map(load_data, files)","d831d6e4":"train_transaction['TransactionAmt'] = train_transaction['TransactionAmt'].astype(float)\ntotal = len(train_transaction)\ntotal_amt = train_transaction.groupby(['isFraud'])['TransactionAmt'].sum().sum()\nplt.figure(figsize=(12,5))\n\nplt.subplot(121)\nplot_tr = sns.countplot(x='isFraud', data=train_transaction)\nplot_tr.set_title(\"Fraud Transactions Distribution \\n 0: No Fraud | 1: Fraud\", fontsize=18)\nplot_tr.set_xlabel(\"Is fraud?\", fontsize=16)\nplot_tr.set_ylabel('Count', fontsize=16)\nfor p in plot_tr.patches:\n    height = p.get_height()\n    plot_tr.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/total*100),\n            ha=\"center\", fontsize=15) \n    \npercent_amt = (train_transaction.groupby(['isFraud'])['TransactionAmt'].sum())\npercent_amt = percent_amt.reset_index()\nplt.subplot(122)\nplot_tr_2 = sns.barplot(x='isFraud', y='TransactionAmt',  dodge=True, data=percent_amt)\nplot_tr_2.set_title(\"% Total Amount in Transaction Amt \\n 0: No Fraud | 1: Fraud\", fontsize=18)\nplot_tr_2.set_xlabel(\"Is fraud?\", fontsize=16)\nplot_tr_2.set_ylabel('Total Transaction Amount Scalar', fontsize=16)\nfor p in plot_tr_2.patches:\n    height = p.get_height()\n    plot_tr_2.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/total_amt * 100),\n            ha=\"center\", fontsize=15) ","9c0a65d5":"plt.figure(figsize=(16,12))\nplt.suptitle('Transaction Values Distribution', fontsize=22)\nplt.subplot(221)\nsub_plot_1 = sns.distplot(train_transaction[train_transaction['TransactionAmt'] <= 1000]['TransactionAmt'])\nsub_plot_1.set_title(\"Transaction Amount Distribuition <= 1000\", fontsize=18)\nsub_plot_1.set_xlabel(\"\")\nsub_plot_1.set_ylabel(\"Probability\", fontsize=15)\n\nplt.subplot(222)\nsub_plot_2 = sns.distplot(np.log(train_transaction['TransactionAmt']))\nsub_plot_2.set_title(\"Transaction Amount (Log) Distribuition\", fontsize=18)\nsub_plot_2.set_xlabel(\"\")\nsub_plot_2.set_ylabel(\"Probability\", fontsize=15)\n\nplt.figure(figsize=(16,12))\n\n\nplt.subplot(212)\nsub_plot_3 = plt.scatter(range(train_transaction[train_transaction['isFraud'] == 0].shape[0]),\n                 np.sort(train_transaction[train_transaction['isFraud'] == 0]['TransactionAmt'].values), \n                 label='NoFraud', alpha=.2)\nsub_plot_3 = plt.scatter(range(train_transaction[train_transaction['isFraud'] == 1].shape[0]),\n                 np.sort(train_transaction[train_transaction['isFraud'] == 1]['TransactionAmt'].values), \n                 label='Fraud', alpha=.2)\nsub_plot_3= plt.title(\"ECDF \\nFRAUD and NO FRAUD Transaction Amount Distribution\", fontsize=18)\nsub_plot_3 = plt.xlabel(\"Index\")\nsub_plot_3 = plt.ylabel(\"Amount Distribution\", fontsize=15)\nsub_plot_3 = plt.legend()\n\nplt.figure(figsize=(16,12))\n\nplt.subplot(321)\nsub_plot_4 = plt.scatter(range(train_transaction[train_transaction['isFraud'] == 1].shape[0]), \n                 np.sort(train_transaction[train_transaction['isFraud'] == 1]['TransactionAmt'].values), \n                label='isFraud', alpha=.4)\nplt.title(\"FRAUD - Transaction Amount ECDF\", fontsize=18)\nplt.xlabel(\"Index\")\nplt.ylabel(\"Amount Distribution\", fontsize=12)\n\nplt.subplot(322)\nsub_plot_5 = plt.scatter(range(train_transaction[train_transaction['isFraud'] == 0].shape[0]),\n                 np.sort(train_transaction[train_transaction['isFraud'] == 0]['TransactionAmt'].values), \n                 label='NoFraud', alpha=.2)\nsub_plot_5 = plt.title(\"NO FRAUD - Transaction Amount ECDF\", fontsize=18)\nsub_plot_5 = plt.xlabel(\"Index\")\nsub_plot_5 = plt.ylabel(\"Amount Distribution\", fontsize=15)\n\nplt.suptitle('Individual ECDF Distribution', fontsize=20)\n\nplt.show()","9fd24bfe":"tmp = pd.crosstab(train_transaction['ProductCD'], train_transaction['isFraud'], normalize='index') * 100\ntmp = tmp.reset_index()\ntmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\nplt.figure(figsize=(14,10))\nplt.suptitle('ProductCD Distributions', fontsize=22)\n\nplt.subplot(221)\nplot_1 = sns.countplot(x='ProductCD', data=train_transaction)\n# plt.legend(title='Fraud', loc='upper center', labels=['No', 'Yes'])\n\nplot_1.set_title(\"ProductCD Distribution\", fontsize=18)\nplot_1.set_xlabel(\"ProductCD Name\", fontsize=16)\nplot_1.set_ylabel(\"Count\", fontsize=17)\nplot_1.set_ylim(0,500000)\nfor p in plot_1.patches:\n    height = p.get_height()\n    plot_1.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/total*100),\n            ha=\"center\", fontsize=14) \n\nplt.subplot(222)\nplot_2 = sns.countplot(x='ProductCD', hue='isFraud', data=train_transaction)\nplt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])\nplot_2_2 = plot_2.twinx()\nplot_2_2 = sns.pointplot(x='ProductCD', y='Fraud', data=tmp, color='black', order=['W', 'H',\"C\", \"S\", \"R\"], legend=False)\nplot_2_2.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\n\nplot_2.set_title(\"Product CD by Target(isFraud)\", fontsize=18)\nplot_2.set_xlabel(\"ProductCD Name\", fontsize=16)\nplot_2.set_ylabel(\"Count\", fontsize=16)\n\nplt.subplot(212)\nplot_3 = sns.boxenplot(x='ProductCD', y='TransactionAmt', hue='isFraud', \n              data=train_transaction[train_transaction['TransactionAmt'] <= 2000] )\nplot_3.set_title(\"Transaction Amount Distribuition by ProductCD and Target\", fontsize=18)\nplot_3.set_xlabel(\"ProductCD Name\", fontsize=16)\nplot_3.set_ylabel(\"Transaction Values\", fontsize=16)\n\nplt.subplots_adjust(hspace = 0.6, top = 0.85)\n\nplt.show()","9d10140e":"train_transaction.loc[train_transaction.card3.isin(train_transaction.card3.value_counts()[train_transaction.card3.value_counts() < 200].index), 'card3'] = \"Others\"\ntrain_transaction.loc[train_transaction.card5.isin(train_transaction.card5.value_counts()[train_transaction.card5.value_counts() < 300].index), 'card5'] = \"Others\"","f44cb5ce":"tmp = pd.crosstab(train_transaction['card3'], train_transaction['isFraud'], normalize='index') * 100\ntmp = tmp.reset_index()\ntmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\ntmp2 = pd.crosstab(train_transaction['card5'], train_transaction['isFraud'], normalize='index') * 100\ntmp2 = tmp2.reset_index()\ntmp2.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\nplt.figure(figsize=(14,22))\n\nplt.subplot(411)\nplot_1 = sns.distplot(train_transaction[train_transaction['isFraud'] == 1]['card1'], label='Fraud')\nplot_1 = sns.distplot(train_transaction[train_transaction['isFraud'] == 0]['card1'], label='NoFraud')\nplot_1.legend()\nplot_1.set_title(\"Card 1 Values Distribution by Target\", fontsize=20)\nplot_1.set_xlabel(\"Card 1 Values\", fontsize=18)\nplot_1.set_ylabel(\"Probability\", fontsize=18)\n\nplt.subplot(412)\nplot_2 = sns.distplot(train_transaction[train_transaction['isFraud'] == 1]['card2'].dropna(), label='Fraud')\nplot_2 = sns.distplot(train_transaction[train_transaction['isFraud'] == 0]['card2'].dropna(), label='NoFraud')\nplot_2.legend()\nplot_2.set_title(\"Card 2 Values Distribution by Target\", fontsize=20)\nplot_2.set_xlabel(\"Card 2 Values\", fontsize=18)\nplot_2.set_ylabel(\"Probability\", fontsize=18)\n\nplt.subplot(413)\nplot_3 = sns.countplot(x='card3', data=train_transaction, order=list(tmp.card3.values))\nplot_3_2 = plot_3.twinx()\nplot_3_2 = sns.pointplot(x='card3', y='Fraud', data=tmp, \n                    color='black', order=list(tmp.card3.values))\nplot_3_2.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\nplot_3.set_title(\"Card 3 Values Distribution and % of Transaction Frauds\", fontsize=20)\nplot_3.set_xlabel(\"Card 3 Values\", fontsize=18)\nplot_3.set_ylabel(\"Count\", fontsize=18)\nfor p in plot_3.patches:\n    height = p.get_height()\n    plot_3.text(p.get_x()+p.get_width()\/2.,\n            height + 25,\n            '{:1.2f}%'.format(height\/total*100),\n            ha=\"center\") \n\nplt.subplot(414)\nplot_4 = sns.countplot(x='card5', data=train_transaction, order=list(tmp2.card5.values))\nplot_4_2 = plot_4.twinx()\nplot_4_2 = sns.pointplot(x='card5', y='Fraud', data=tmp2, \n                    color='black', order=list(tmp2.card5.values))\nplot_4_2.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\nplot_4.set_title(\"Card 5 Values Distribution and % of Transaction Frauds\", fontsize=20)\nplot_4.set_xticklabels(plot_4.get_xticklabels(),rotation=90)\nplot_4.set_xlabel(\"Card 5 Values\", fontsize=18)\nplot_4.set_ylabel(\"Count\", fontsize=18)\nfor p in plot_4.patches:\n    height = p.get_height()\n    plot_4.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/total*100),\n            ha=\"center\",fontsize=11) \n    \nplt.subplots_adjust(hspace = 0.6, top = 0.85)\n\n\nplt.show()","207f0620":"tmp = pd.crosstab(train_transaction['card4'], train_transaction['isFraud'], normalize='index') * 100\ntmp = tmp.reset_index()\ntmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\nplt.figure(figsize=(14,10))\nplt.suptitle('Card 4 Distributions', fontsize=22)\n\nplt.subplot(221)\nplot_1 = sns.countplot(x='card4', data=train_transaction)\n# plt.legend(title='Fraud', loc='upper center', labels=['No', 'Yes'])\nplot_1.set_title(\"Card4 Distribution\", fontsize=19)\nplot_1.set_ylim(0,420000)\nplot_1.set_xlabel(\"Card4 Category Names\", fontsize=17)\nplot_1.set_ylabel(\"Count\", fontsize=17)\nfor p in plot_1.patches:\n    height = p.get_height()\n    plot_1.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/total*100),\n            ha=\"center\",fontsize=14) \n\n\nplt.subplot(222)\nplot_2 = sns.countplot(x='card4', hue='isFraud', data=train_transaction)\nplt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])\nplot_2_2 = plot_1.twinx()\nplot_2_2 = sns.pointplot(x='card4', y='Fraud', data=tmp, \n                   color='black', legend=False, \n                   order=['discover', 'mastercard', 'visa', 'american express'])\nplot_2_2.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\nplot_2.set_title(\"Card4 by Target(isFraud)\", fontsize=19)\nplot_2.set_xlabel(\"Card4 Category Names\", fontsize=17)\nplot_2.set_ylabel(\"Count\", fontsize=17)\n\nplt.subplot(212)\nplot_3 = sns.boxenplot(x='card4', y='TransactionAmt', hue='isFraud', \n              data=train_transaction[train_transaction['TransactionAmt'] <= 2000] )\nplot_3.set_title(\"Card 4 Distribuition by ProductCD and Target\", fontsize=20)\nplot_3.set_xlabel(\"Card4 Category Names\", fontsize=17)\nplot_3.set_ylabel(\"Transaction Values\", fontsize=17)\n\nplt.subplots_adjust(hspace = 0.6, top = 0.85)\n\nplt.show()","a00846d8":"tmp = pd.crosstab(train_transaction['card6'], train_transaction['isFraud'], normalize='index') * 100\ntmp = tmp.reset_index()\ntmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\nplt.figure(figsize=(14,10))\nplt.suptitle('Card 6 Distributions', fontsize=22)\n\nplt.subplot(221)\nplot_1 = sns.countplot(x='card6', data=train_transaction, order=list(tmp.card6.values))\n# plt.legend(title='Fraud', loc='upper center', labels=['No', 'Yes'])\nplot_1.set_title(\"Card6 Distribution\", fontsize=19)\nplot_1.set_ylim(0,480000)\nplot_1.set_xlabel(\"Card6 Category Names\", fontsize=17)\nplot_1.set_ylabel(\"Count\", fontsize=17)\nfor p in plot_1.patches:\n    height = p.get_height()\n    plot_1.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/total*100),\n            ha=\"center\",fontsize=14) \n\nplt.subplot(222)\nplot_2 = sns.countplot(x='card6', hue='isFraud', data=train_transaction, order=list(tmp.card6.values))\nplt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])\nplot_2_2 = plot_2.twinx()\nplot_2_2 = sns.pointplot(x='card6', y='Fraud', data=tmp, order=list(tmp.card6.values),\n                   color='black', legend=False, )\nplot_2_2.set_ylim(0,20)\nplot_2_2.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\nplot_2.set_title(\"Card6 by Target(isFraud)\", fontsize=19)\nplot_2.set_xlabel(\"Card6 Category Names\", fontsize=17)\nplot_2.set_ylabel(\"Count\", fontsize=17)\n\nplt.subplot(212)\nplot_3 = sns.boxenplot(x='card6', y='TransactionAmt', hue='isFraud', order=list(tmp.card6.values),\n              data=train_transaction[train_transaction['TransactionAmt'] <= 2000] )\nplot_3.set_title(\"Card 6 Distribuition by ProductCD and Target\", fontsize=20)\nplot_3.set_xlabel(\"Card6 Category Names\", fontsize=17)\nplot_3.set_ylabel(\"Transaction Values\", fontsize=17)\n\nplt.subplots_adjust(hspace = 0.6, top = 0.85)\n\nplt.show()","7e359be6":"for columns in ['M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']:\n    train_transaction[columns] = train_transaction[columns].fillna(\"Miss\")\n    \ndef ploting_dist_ratio(DataFile, Column, lim=2000):\n    tmp = pd.crosstab(DataFile[Column], DataFile['isFraud'], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\n    plt.figure(figsize=(20,5))\n    plt.suptitle(f'{Column} Distributions ', fontsize=22)\n\n    plt.subplot(121)\n    plot_1 = sns.countplot(x=Column, data=DataFile, order=list(tmp[Column].values))\n    # plt.legend(title='Fraud', loc='upper center', labels=['No', 'Yes'])\n    plot_1.set_title(f\"{Column} Distribution\\nCound and %Fraud by each category\", fontsize=18)\n    plot_1.set_ylim(0,400000)\n    plot_1_2 = plot_1.twinx()\n    plot_1_2 = sns.pointplot(x=Column, y='Fraud', data=tmp, order=list(tmp[Column].values),\n                       color='black', legend=False, )\n    plot_1_2.set_ylim(0,20)\n    plot_1_2.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\n    plot_1.set_xlabel(f\"{Column} Category Names\", fontsize=16)\n    plot_1.set_ylabel(\"Count\", fontsize=17)\n    for p in plot_1_2.patches:\n        height = p.get_height()\n        plot_1_2.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(height\/total*100),\n                ha=\"center\",fontsize=14) \n        \n    perc_amt = (train_transaction.groupby(['isFraud',Column])['TransactionAmt'].sum() \/ total_amt * 100).unstack('isFraud')\n    perc_amt = perc_amt.reset_index()\n    perc_amt.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\n    plt.subplot(122)\n    plot_2 = sns.boxplot(x=Column, y='TransactionAmt', hue='isFraud', \n                     data=DataFile[DataFile['TransactionAmt'] <= lim], order=list(tmp[Column].values))\n    plot_2_2 = plot_2.twinx()\n    plot_2_2 = sns.pointplot(x=Column, y='Fraud', data=perc_amt, order=list(tmp[Column].values),\n                       color='black', legend=False, )\n    plot_2_2.set_ylim(0,5)\n    plot_2_2.set_ylabel(\"%Fraud Total Amount\", fontsize=16)\n    plot_2.set_title(f\"{Column} by Transactions dist\", fontsize=18)\n    plot_2.set_xlabel(f\"{Column} Category Names\", fontsize=16)\n    plot_2.set_ylabel(\"Transaction Amount(U$)\", fontsize=16)\n        \n    plt.subplots_adjust(hspace=.4, wspace = 0.35, top = 0.80)\n    \n    plt.show()","ff31bfb2":"for columns in ['M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']:\n    ploting_dist_ratio(train_transaction, columns, lim=2500)","f05e8205":"train_transaction.loc[train_transaction.addr1.isin(train_transaction.addr1.value_counts()[train_transaction.addr1.value_counts() <= 5000 ].index), 'addr1'] = \"Others\"\ntrain_transaction.loc[train_transaction.addr2.isin(train_transaction.addr2.value_counts()[train_transaction.addr2.value_counts() <= 50 ].index), 'addr2'] = \"Others\"","96c384cd":"def ploting_cnt_amt(DataFile, Column, lim=2000):\n    tmp = pd.crosstab(DataFile[Column], DataFile['isFraud'], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n    \n    plt.figure(figsize=(16,14))    \n    plt.suptitle(f'{Column} Distributions ', fontsize=24)\n    \n    plt.subplot(211)\n    plot_1 = sns.countplot(x=Column,data=DataFile,order=list(tmp[Column].values))\n    plot_1_2 = plot_1.twinx()\n    plot_1_2 = sns.pointplot(x=Column, y='Fraud', data=tmp, order=list(tmp[Column].values),\n                       color='black', legend=False)\n    plot_1_2.set_ylim(0,tmp['Fraud'].max()*1.1)\n    plot_1_2.set_ylabel(\"%Fraud Transactions\", fontsize=16)\n    plot_1.set_title(f\"Most Frequent {Column} values and % Fraud Transactions\", fontsize=20)\n    plot_1.set_xlabel(f\"{Column} Category Names\", fontsize=16)\n    plot_1.set_ylabel(\"Count\", fontsize=17)\n    plot_1.set_xticklabels(plot_1.get_xticklabels(),rotation=45)\n    sizes = []\n    for p in plot_1.patches:\n        height = p.get_height()\n        sizes.append(height)\n        plot_1.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(height\/total*100),\n                ha=\"center\",fontsize=12) \n        \n    plot_1.set_ylim(0,max(sizes)*1.15)\n    \n    #########################################################################\n    perc_amt = (DataFile.groupby(['isFraud',Column])['TransactionAmt'].sum() \\\n                \/ DataFile.groupby([Column])['TransactionAmt'].sum() * 100).unstack('isFraud')\n    perc_amt = perc_amt.reset_index()\n    perc_amt.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n    amt = DataFile.groupby([Column])['TransactionAmt'].sum().reset_index()\n    perc_amt = perc_amt.fillna(0)\n    plt.subplot(212)\n    plot_2 = sns.barplot(x=Column, y='TransactionAmt', \n                       data=amt, \n                       order=list(tmp[Column].values))\n    plot_2_2 = plot_2.twinx()\n    plot_2_2 = sns.pointplot(x=Column, y='Fraud', data=perc_amt, \n                        order=list(tmp[Column].values),\n                       color='black', legend=False, )\n    plot_2_2.set_ylim(0,perc_amt['Fraud'].max()*1.1)\n    plot_2_2.set_ylabel(\"%Fraud Total Amount\", fontsize=16)\n    plot_2.set_xticklabels(plot_2.get_xticklabels(),rotation=45)\n    plot_2.set_title(f\"{Column} by Transactions Total + %of total and %Fraud Transactions\", fontsize=20)\n    plot_2.set_xlabel(f\"{Column} Category Names\", fontsize=16)\n    plot_2.set_ylabel(\"Transaction Total Amount(U$)\", fontsize=16)\n    plot_2.set_xticklabels(plot_2.get_xticklabels(),rotation=45)    \n    \n    for p in plot_2.patches:\n        height = p.get_height()\n        plot_2.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(height\/total_amt*100),\n                ha=\"center\",fontsize=12) \n        \n    plt.subplots_adjust(hspace=.4, top = 0.9)\n    plt.show()\n    \nploting_cnt_amt(train_transaction, 'addr1')","d66caf68":"ploting_cnt_amt(train_transaction, 'addr2')","864d6926":"train_transaction.loc[train_transaction['P_emaildomain'].isin(['gmail.com', 'gmail']),'P_emaildomain'] = 'Google'\n\ntrain_transaction.loc[train_transaction['P_emaildomain'].isin(['yahoo.com', 'yahoo.com.mx',  'yahoo.co.uk',\n                                         'yahoo.co.jp', 'yahoo.de', 'yahoo.fr',\n                                         'yahoo.es']), 'P_emaildomain'] = 'Yahoo Mail'\ntrain_transaction.loc[train_transaction['P_emaildomain'].isin(['hotmail.com','outlook.com','msn.com', 'live.com.mx', \n                                         'hotmail.es','hotmail.co.uk', 'hotmail.de',\n                                         'outlook.es', 'live.com', 'live.fr',\n                                         'hotmail.fr']), 'P_emaildomain'] = 'Microsoft'\ntrain_transaction.loc[train_transaction.P_emaildomain.isin(train_transaction.P_emaildomain\\\n                                         .value_counts()[train_transaction.P_emaildomain.value_counts() <= 500 ]\\\n                                         .index), 'P_emaildomain'] = \"Others\"\ntrain_transaction.P_emaildomain.fillna(\"NoInf\", inplace=True)","a70a3deb":"ploting_cnt_amt(train_transaction, 'R_emaildomain')","38d713d5":"train_transaction.loc[train_transaction.C1.isin(train_transaction.C1\\\n                              .value_counts()[train_transaction.C1.value_counts() <= 400 ]\\\n                              .index), 'C1'] = \"Others\"","50855316":"ploting_cnt_amt(train_transaction, 'C1')","cbb89a72":"train_transaction.loc[train_transaction.C2.isin(train_transaction.C2\\\n                              .value_counts()[train_transaction.C2.value_counts() <= 350 ]\\\n                              .index), 'C2'] = \"Others\"","39c546ad":"ploting_cnt_amt(train_transaction, 'C2')","858c47c9":"# https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/100400#latest-579480\nimport datetime\n\nSTART_DATE = '2017-12-01'\nstartdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\ntrain_transaction[\"Date\"] = train_transaction['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\ntrain_transaction['_Weekdays'] = train_transaction['Date'].dt.dayofweek\ntrain_transaction['_Hours'] = train_transaction['Date'].dt.hour\ntrain_transaction['_Days'] = train_transaction['Date'].dt.day\n\ntest_transaction[\"Date\"] = test_transaction['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\ntest_transaction['_Weekdays'] = test_transaction['Date'].dt.dayofweek\ntest_transaction['_Hours'] = test_transaction['Date'].dt.hour\ntest_transaction['_Days'] = test_transaction['Date'].dt.day","db1fa6bf":"ploting_cnt_amt(train_transaction, '_Days')","d331b101":"ploting_cnt_amt(train_transaction, '_Weekdays')","eff525a1":"ploting_cnt_amt(train_transaction, '_Hours')","008dd7ab":"import plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.offline import iplot, init_notebook_mode\n#import cufflinks\n#import cufflinks as cf\nimport plotly.figure_factory as ff\n# Calling the function to transform the date column in datetime pandas object\n\n#seting some static color options\ncolor_op = ['#5527A0', '#BB93D7', '#834CF7', '#6C941E', '#93EAEA', '#7425FF', '#F2098A', '#7E87AC', \n            '#EBE36F', '#7FD394', '#49C35D', '#3058EE', '#44FDCF', '#A38F85', '#C4CEE0', '#B63A05', \n            '#4856BF', '#F0DB1B', '#9FDBD9', '#B123AC']\n\n\ndates_temp = train_transaction.groupby(train_transaction.Date.dt.date)['TransactionAmt'].count().reset_index()\n# renaming the columns to apropriate names\n\n# creating the first trace with the necessary parameters\ntrace = go.Scatter(x=dates_temp['Date'], y=dates_temp.TransactionAmt,\n                    opacity = 0.8, line = dict(color = color_op[7]), name= 'Total Transactions')\n\n# Below we will get the total amount sold\ndates_temp_sum = train_transaction.groupby(train_transaction.Date.dt.date)['TransactionAmt'].sum().reset_index()\n\n# using the new dates_temp_sum we will create the second trace\ntrace1 = go.Scatter(x=dates_temp_sum.Date, line = dict(color = color_op[1]), name=\"Total Amount\",\n                        y=dates_temp_sum['TransactionAmt'], opacity = 0.8, yaxis='y2')\n\n#creating the layout the will allow us to give an title and \n# give us some interesting options to handle with the outputs of graphs\nlayout = dict(\n    title= \"Total Transactions and Fraud Informations by Date\",\n    xaxis=dict(\n        rangeselector=dict(\n            buttons=list([\n                dict(count=1, label='1m', step='month', stepmode='backward'),\n                dict(count=3, label='3m', step='month', stepmode='backward'),\n                dict(count=6, label='6m', step='month', stepmode='backward'),\n                dict(step='all')\n            ])\n        ),\n        rangeslider=dict(visible = True),\n        type='date' ),\n    yaxis=dict(title='Total Transactions'),\n    yaxis2=dict(overlaying='y',\n                anchor='x', side='right',\n                zeroline=False, showgrid=False,\n                title='Total Transaction Amount')\n)\n\n# creating figure with the both traces and layout\nfig = dict(data= [trace, trace1,], layout=layout)\n\n#rendering the graphs\niplot(fig) #it's an equivalent to plt.show()","cb971729":"# Calling the function to transform the date column in datetime pandas object\n\n#seting some static color options\ncolor_op = ['#5527A0', '#BB93D7', '#834CF7', '#6C941E', '#93EAEA', '#7425FF', '#F2098A', '#7E87AC', \n            '#EBE36F', '#7FD394', '#49C35D', '#3058EE', '#44FDCF', '#A38F85', '#C4CEE0', '#B63A05', \n            '#4856BF', '#F0DB1B', '#9FDBD9', '#B123AC']\n\ntmp_amt = train_transaction.groupby([train_transaction.Date.dt.date, 'isFraud'])['TransactionAmt'].sum().reset_index()\ntmp_trans = train_transaction.groupby([train_transaction.Date.dt.date, 'isFraud'])['TransactionAmt'].count().reset_index()\n\ntmp_trans_fraud = tmp_trans[tmp_trans['isFraud'] == 1]\ntmp_amt_fraud = tmp_amt[tmp_amt['isFraud'] == 1]\n\ndates_temp = train_transaction.groupby(train_transaction.Date.dt.date)['TransactionAmt'].count().reset_index()\n# renaming the columns to apropriate names\n\n# creating the first trace with the necessary parameters\ntrace = go.Scatter(x=tmp_trans_fraud['Date'], y=tmp_trans_fraud.TransactionAmt,\n                    opacity = 0.8, line = dict(color = color_op[1]), name= 'Fraud Transactions')\n\n# using the new dates_temp_sum we will create the second trace\ntrace1 = go.Scatter(x=tmp_amt_fraud.Date, line = dict(color = color_op[7]), name=\"Fraud Amount\",\n                    y=tmp_amt_fraud['TransactionAmt'], opacity = 0.8, yaxis='y2')\n\n#creating the layout the will allow us to give an title and \n# give us some interesting options to handle with the outputs of graphs\nlayout = dict(\n    title= \"FRAUD TRANSACTIONS - Total Transactions and Fraud Informations by Date\",\n    xaxis=dict(\n        rangeselector=dict(\n            buttons=list([\n                dict(count=1, label='1m', step='month', stepmode='backward'),\n                dict(count=3, label='3m', step='month', stepmode='backward'),\n                dict(count=6, label='6m', step='month', stepmode='backward'),\n                dict(step='all')\n            ])\n        ),\n        rangeslider=dict(visible = True),\n        type='date' ),\n    yaxis=dict(title='Total Transactions'),\n    yaxis2=dict(overlaying='y',\n                anchor='x', side='right',\n                zeroline=False, showgrid=False,\n                title='Total Transaction Amount')\n)\n\n# creating figure with the both traces and layout\nfig = dict(data= [trace, trace1], layout=layout)\n\n#rendering the graphs\niplot(fig) #it's an equivalent to plt.show()","3603bdb3":"from sklearn.model_selection import train_test_split, StratifiedKFold, GroupKFold\ndef seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)","6252220f":"import lightgbm as lgb\n\ndef make_predictions(tr_df, tt_df, features_columns, target, lgb_params, NFOLDS=6):\n    \n    folds = GroupKFold(n_splits=NFOLDS)\n\n    X,y = tr_df[features_columns], tr_df[target]    \n    P,P_y = tt_df[features_columns], tt_df[target]  \n    split_groups = tr_df['DT_M']\n\n    tt_df = tt_df[['TransactionID',target]]    \n    predictions = np.zeros(len(tt_df))\n    oof = np.zeros(len(tr_df))\n    \n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y, groups=split_groups)):\n        print('Fold:',fold_)\n        tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]\n        vl_x, vl_y = X.iloc[val_idx,:], y[val_idx]\n            \n        print(len(tr_x),len(vl_x))\n        tr_data = lgb.Dataset(tr_x, label=tr_y)\n        vl_data = lgb.Dataset(vl_x, label=vl_y)  \n\n        estimator = lgb.train(\n            lgb_params,\n            tr_data,\n            valid_sets = [tr_data, vl_data],\n            verbose_eval = 200,\n        )   \n        \n        pp_p = estimator.predict(P)\n        predictions += pp_p\/NFOLDS\n        \n        oof_preds = estimator.predict(vl_x)\n        oof[val_idx] = (oof_preds - oof_preds.min())\/(oof_preds.max() - oof_preds.min())\n\n        if LOCAL_TEST:\n            feature_imp = pd.DataFrame(sorted(zip(estimator.feature_importance(),X.columns)), columns=['Value','Feature'])\n            print(feature_imp)\n        \n        del tr_x, tr_y, vl_x, vl_y, tr_data, vl_data\n        gc.collect()\n        \n    tt_df['prediction'] = predictions\n    print('OOF AUC:', metrics.roc_auc_score(y, oof))\n    if LOCAL_TEST:\n        print('Holdout AUC:', metrics.roc_auc_score(tt_df[TARGET], tt_df['prediction']))\n    \n    return tt_df","a3e21f28":"import os, sys, gc, warnings, random, datetime\nSEED = 42\nseed_everything(SEED)\nLOCAL_TEST = False\nTARGET = 'isFraud'\nSTART_DATE = datetime.datetime.strptime('2017-11-30', '%Y-%m-%d')","433df8f4":"print('Load Data')\ntrain_df = pd.read_pickle('..\/input\/ieee-data-minification\/train_transaction.pkl')\n\nif LOCAL_TEST:\n   \n    train_df['DT_M'] = train_df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n    train_df['DT_M'] = (train_df['DT_M'].dt.year-2017)*12 + train_df['DT_M'].dt.month \n    test_df = train_df[train_df['DT_M']==train_df['DT_M'].max()].reset_index(drop=True)\n    train_df = train_df[train_df['DT_M']<(train_df['DT_M'].max()-1)].reset_index(drop=True)\n    \n    train_identity = pd.read_pickle('..\/input\/ieee-data-minification\/train_identity.pkl')\n    test_identity  = train_identity[train_identity['TransactionID'].isin(\n                                    test_df['TransactionID'])].reset_index(drop=True)\n    train_identity = train_identity[train_identity['TransactionID'].isin(\n                                    train_df['TransactionID'])].reset_index(drop=True)\n    del train_df['DT_M'], test_df['DT_M']\n    \nelse:\n    test_df = pd.read_pickle('..\/input\/ieee-data-minification\/test_transaction.pkl')\n    train_identity = pd.read_pickle('..\/input\/ieee-data-minification\/train_identity.pkl')\n    test_identity = pd.read_pickle('..\/input\/ieee-data-minification\/test_identity.pkl')\n    \nbase_columns = list(train_df) + list(train_identity)\nprint('Shape control:', train_df.shape, test_df.shape)","d2800862":"for df in [train_df, test_df]:\n\n    df['DT'] = df['TransactionDT'].apply(lambda x: (START_DATE + datetime.timedelta(seconds = x)))\n    df['DT_M'] = (df['DT'].dt.year-2017)*12 + df['DT'].dt.month\n    df['DT_W'] = (df['DT'].dt.year-2017)*52 + df['DT'].dt.weekofyear\n    df['DT_D'] = (df['DT'].dt.year-2017)*365 + df['DT'].dt.dayofyear\n    \n    df['DT_hour'] = df['DT'].dt.hour\n    df['DT_day_week'] = df['DT'].dt.dayofweek\n    df['DT_day'] = df['DT'].dt.day\n    \n    df['D9'] = np.where(df['D9'].isna(),0,1)","c6eb3881":"i_cols = ['card1']\n\nfor col in i_cols: \n    valid_card = pd.concat([train_df[[col]], test_df[[col]]])\n    valid_card = valid_card[col].value_counts()\n    valid_card = valid_card[valid_card>2]\n    valid_card = list(valid_card.index)\n\n    train_df[col] = np.where(train_df[col].isin(test_df[col]), train_df[col], np.nan)\n    test_df[col]  = np.where(test_df[col].isin(train_df[col]), test_df[col], np.nan)\n\n    train_df[col] = np.where(train_df[col].isin(valid_card), train_df[col], np.nan)\n    test_df[col]  = np.where(test_df[col].isin(valid_card), test_df[col], np.nan)","c9ba599c":"i_cols = ['M1','M2','M3','M5','M6','M7','M8','M9']\n\nfor df in [train_df, test_df]:\n    df['M_sum'] = df[i_cols].sum(axis=1).astype(np.int8)\n    df['M_na'] = df[i_cols].isna().sum(axis=1).astype(np.int8)","4c80bd84":"train_df['uid'] = train_df['card1'].astype(str)+'_'+train_df['card2'].astype(str)\ntest_df['uid'] = test_df['card1'].astype(str)+'_'+test_df['card2'].astype(str)\n\ntrain_df['uid2'] = train_df['uid'].astype(str)+'_'+train_df['card3'].astype(str)+'_'+train_df['card5'].astype(str)\ntest_df['uid2'] = test_df['uid'].astype(str)+'_'+test_df['card3'].astype(str)+'_'+test_df['card5'].astype(str)\n\ntrain_df['uid3'] = train_df['uid2'].astype(str)+'_'+train_df['addr1'].astype(str)+'_'+train_df['addr2'].astype(str)\ntest_df['uid3'] = test_df['uid2'].astype(str)+'_'+test_df['addr1'].astype(str)+'_'+test_df['addr2'].astype(str)\n\ntrain_df['TransactionAmt_check'] = np.where(train_df['TransactionAmt'].isin(test_df['TransactionAmt']), 1, 0)\ntest_df['TransactionAmt_check']  = np.where(test_df['TransactionAmt'].isin(train_df['TransactionAmt']), 1, 0)\n\ni_cols = ['card1','card2','card3','card5','uid','uid2','uid3']\n\nfor col in i_cols:\n    for agg_type in ['mean','std']:\n        new_col_name = col+'_TransactionAmt_'+agg_type\n        temp_df = pd.concat([train_df[[col, 'TransactionAmt']], test_df[[col,'TransactionAmt']]])\n        temp_df = temp_df.groupby([col])['TransactionAmt'].agg([agg_type]).reset_index().rename(\n                                                columns={agg_type: new_col_name})\n        \n        temp_df.index = list(temp_df[col])\n        temp_df = temp_df[new_col_name].to_dict()   \n    \n        train_df[new_col_name] = train_df[col].map(temp_df)\n        test_df[new_col_name]  = test_df[col].map(temp_df)\n           \ntrain_df['TransactionAmt'] = np.log1p(train_df['TransactionAmt'])\ntest_df['TransactionAmt'] = np.log1p(test_df['TransactionAmt'])  ","7076aabe":"p = 'P_emaildomain'\nr = 'R_emaildomain'\nuknown = 'email_not_provided'\n\nfor df in [train_df, test_df]:\n    df[p] = df[p].fillna(uknown)\n    df[r] = df[r].fillna(uknown)\n    \n    df['email_check'] = np.where((df[p]==df[r])&(df[p]!=uknown),1,0)\n\n    df[p+'_prefix'] = df[p].apply(lambda x: x.split('.')[0])\n    df[r+'_prefix'] = df[r].apply(lambda x: x.split('.')[0])","749dfda9":"for df in [train_identity, test_identity]:\n\n    df['DeviceInfo'] = df['DeviceInfo'].fillna('unknown_device').str.lower()\n    df['DeviceInfo_device'] = df['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n    df['DeviceInfo_version'] = df['DeviceInfo'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))\n    \n    df['id_30'] = df['id_30'].fillna('unknown_device').str.lower()\n    df['id_30_device'] = df['id_30'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))\n    df['id_30_version'] = df['id_30'].apply(lambda x: ''.join([i for i in x if i.isnumeric()]))\n    \n    df['id_31'] = df['id_31'].fillna('unknown_device').str.lower()\n    df['id_31_device'] = df['id_31'].apply(lambda x: ''.join([i for i in x if i.isalpha()]))","30cc4cf2":"temp_df = train_df[['TransactionID']]\ntemp_df = temp_df.merge(train_identity, on=['TransactionID'], how='left')\ndel temp_df['TransactionID']\ntrain_df = pd.concat([train_df,temp_df], axis=1)\n    \ntemp_df = test_df[['TransactionID']]\ntemp_df = temp_df.merge(test_identity, on=['TransactionID'], how='left')\ndel temp_df['TransactionID']\ntest_df = pd.concat([test_df,temp_df], axis=1)","93b62dba":"i_cols = ['card1','card2','card3','card5',\n          'C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14',\n          'D1','D2','D3','D4','D5','D6','D7','D8',\n          'addr1','addr2',\n          'dist1','dist2',\n          'P_emaildomain', 'R_emaildomain',\n          'DeviceInfo','DeviceInfo_device','DeviceInfo_version',\n          'id_30','id_30_device','id_30_version',\n          'id_31_device',\n          'id_33',\n          'uid','uid2','uid3',\n         ]\n\nfor col in i_cols:\n    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n    fq_encode = temp_df[col].value_counts(dropna=False).to_dict()   \n    train_df[col+'_fq_enc'] = train_df[col].map(fq_encode)\n    test_df[col+'_fq_enc']  = test_df[col].map(fq_encode)\n\n\nfor col in ['DT_M','DT_W','DT_D']:\n    temp_df = pd.concat([train_df[[col]], test_df[[col]]])\n    fq_encode = temp_df[col].value_counts().to_dict()\n            \n    train_df[col+'_total'] = train_df[col].map(fq_encode)\n    test_df[col+'_total']  = test_df[col].map(fq_encode)\n        \n\nperiods = ['DT_M','DT_W','DT_D']\ni_cols = ['uid']\nfor period in periods:\n    for col in i_cols:\n        new_column = col + '_' + period\n            \n        temp_df = pd.concat([train_df[[col,period]], test_df[[col,period]]])\n        temp_df[new_column] = temp_df[col].astype(str) + '_' + (temp_df[period]).astype(str)\n        fq_encode = temp_df[new_column].value_counts().to_dict()\n            \n        train_df[new_column] = (train_df[col].astype(str) + '_' + train_df[period].astype(str)).map(fq_encode)\n        test_df[new_column]  = (test_df[col].astype(str) + '_' + test_df[period].astype(str)).map(fq_encode)\n        \n        train_df[new_column] \/= train_df[period+'_total']\n        test_df[new_column]  \/= test_df[period+'_total']","862b1055":"for col in ['ProductCD','M4']:\n    temp_dict = train_df.groupby([col])[TARGET].agg(['mean']).reset_index().rename(\n                                                        columns={'mean': col+'_target_mean'})\n    temp_dict.index = temp_dict[col].values\n    temp_dict = temp_dict[col+'_target_mean'].to_dict()\n\n    train_df[col] = train_df[col].map(temp_dict)\n    test_df[col]  = test_df[col].map(temp_dict)","54023452":"for col in list(train_df):\n    if train_df[col].dtype=='O':\n        print(col)\n        train_df[col] = train_df[col].fillna('unseen_before_label')\n        test_df[col]  = test_df[col].fillna('unseen_before_label')\n        \n        train_df[col] = train_df[col].astype(str)\n        test_df[col] = test_df[col].astype(str)\n        \n        le = LabelEncoder()\n        le.fit(list(train_df[col])+list(test_df[col]))\n        train_df[col] = le.transform(train_df[col])\n        test_df[col]  = le.transform(test_df[col])\n        \n        train_df[col] = train_df[col].astype('category')\n        test_df[col] = test_df[col].astype('category')","34191274":"rm_cols = [\n    'TransactionID','TransactionDT', # These columns are pure noise right now\n    TARGET,                          # Not target in features))\n    'uid','uid2','uid3',             # Our new client uID -> very noisy data\n    'bank_type',                     # Victims bank could differ by time\n    'DT','DT_M','DT_W','DT_D',       # Temporary Variables\n    'DT_hour','DT_day_week','DT_day',\n    'DT_D_total','DT_W_total','DT_M_total',\n    'id_30','id_31','id_33',\n]","bc7955ae":"########################### Features elimination \nfrom scipy.stats import ks_2samp\nfeatures_check = []\ncolumns_to_check = set(list(train_df)).difference(base_columns+rm_cols)\nfor i in columns_to_check:\n    features_check.append(ks_2samp(test_df[i], train_df[i])[1])\n\nfeatures_check = pd.Series(features_check, index=columns_to_check).sort_values() \nfeatures_discard = list(features_check[features_check==0].index)\nprint(features_discard)\n\nfeatures_discard = [] \n\n# Final features list\nfeatures_columns = [col for col in list(train_df) if col not in rm_cols + features_discard]","f649715e":"lgb_params = {\n                'objective':'binary',\n                'boosting_type':'gbdt',\n                'metric':'auc',\n                'n_jobs':-1,\n                'learning_rate':0.01,\n                'num_leaves': 2**8,\n                'max_depth':-1,\n                'tree_learner':'serial',\n                'colsample_bytree': 0.85,\n                'subsample_freq':1,\n                'subsample':0.85,\n                'n_estimators':2**9,\n                'max_bin':255,\n                'verbose':-1,\n                'seed': SEED,\n                'early_stopping_rounds':100,\n                'reg_alpha':0.3,\n                'reg_lamdba':0.243\n            } ","18e57d1c":"if LOCAL_TEST:\n    lgb_params['learning_rate'] = 0.01\n    lgb_params['n_estimators'] = 20000\n    lgb_params['early_stopping_rounds'] = 100\n    test_predictions = make_predictions(train_df, test_df, features_columns, TARGET, lgb_params)\n    print(metrics.roc_auc_score(test_predictions[TARGET], test_predictions['prediction']))\nelse:\n    lgb_params['learning_rate'] = 0.007\n    lgb_params['n_estimators'] = 1800\n    lgb_params['early_stopping_rounds'] = 100    \n    test_predictions = make_predictions(train_df, test_df, features_columns, TARGET, lgb_params, NFOLDS=6)","4a5a016b":"if not LOCAL_TEST:\n    test_predictions['isFraud'] = test_predictions['prediction']\n    test_predictions[['TransactionID','isFraud']].to_csv('submission.csv', index=False)","2e181d25":"<pre><b>Ploting P-Email Domain<\/b><\/pre>","c7ede3f9":"<pre><b>The Product Feature<\/b><\/pre>","a88d37f5":"<pre><b>Card 6 - Categorical<\/b><\/pre>","7bbad8cd":"<pre><b>Ploting WeekDays Distributions<\/b><\/pre>","537d4f38":"IEEE-CIS works across a variety of AI and machine learning areas, including deep neural networks, fuzzy systems, evolutionary computation, and swarm intelligence. Today they\u2019re partnering with the world\u2019s leading payment service company, Vesta Corporation, seeking the best solutions for fraud prevention industry","0d4fd235":"<pre><b> ADDR2 Distributions <\/b><\/pre>","384983ea":"![https:\/\/cis.ieee.org\/images\/files\/template\/cis-logo.png](https:\/\/cis.ieee.org\/images\/files\/template\/cis-logo.png)","ba1c0298":"# Visualize the Dataset","c6e1bfb3":"<pre><b>Fraud Transactions by Date<\/b><\/pre>","6087ccc9":"<pre><b>Top Days with highest Total Transaction Amount<\/b><\/pre>","10d3696f":"TimeDelta Feature to check if frauds have some specific hour that has highest % of frauds<\/b><\/pre>","20c7a40e":"<pre><b>Credits to Leonardo's Kernel : \nhttps:\/\/www.kaggle.com\/kabure\/extensive-eda-and-modeling-xgb-hyperopt<\/b><\/pre>","4963af35":"<pre><b>Exploring M1-M9 Features<\/b><\/pre>","f76a81f9":"<pre><b>Transactions and Total Amount per day<\/b><\/pre>","25410969":"<pre><b>Ploting Transaction Amount Values Distribution<\/b><\/pre>","2753648b":"<pre><b>Card 4 - Categorical Feature<\/b><\/pre>","c47719d2":"<pre><b>Credits to Konstantin Yakovlev's Kernel\nhttps:\/\/www.kaggle.com\/kyakovlev\/ieee-gb-2-make-amount-useful-again<\/b><\/pre> ","106a65fc":"<pre><b>Visualizing Card 1, Card 2 and Card 3 Distributions<\/b><\/pre>","20fd8a0c":"# Feature Engineering","caad824f":"<pre><b>Addr1 Distributions<\/b><\/pre>","fe7209dd":"# Import Libraries","567c35e6":"<pre><b>Exploring C1-C14 features<\/b><\/pre>","852a0bda":"<pre><b>Ploting Hours Distributions<\/b><\/pre>"}}