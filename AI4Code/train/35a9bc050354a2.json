{"cell_type":{"1c50b7ee":"code","8c557220":"code","dd38db73":"code","6077f4c7":"code","64a6399a":"code","e82d643e":"code","27277bc9":"code","546dafaf":"code","c9379fe6":"code","a884aac6":"code","690beeb9":"code","2c5e01d6":"code","1160c924":"code","ff759f38":"code","f8ec5fa2":"code","16f436f3":"code","00643e08":"code","b8429d0c":"code","834e4cd0":"code","605e960a":"code","0a050539":"code","0040e239":"code","dab7d571":"code","cd0e3eeb":"code","f0dd0d88":"code","63ef472c":"code","2f0eccc9":"code","977ad3d6":"code","21e53aea":"code","c0deb352":"code","ce94dcc9":"code","b016aae1":"code","f9825a67":"code","70f5fa0b":"code","123c8150":"code","51e3c89f":"code","2ff50cb7":"code","d321f124":"code","475abca2":"code","b4b423a5":"code","e6b20f37":"code","906ddecb":"code","738c7383":"code","6b5cfd10":"code","d99e85a8":"code","73a311fb":"code","b816ec7e":"code","eebc3759":"code","d0fe362c":"code","9ffb9694":"code","055c2956":"code","eb38aa36":"code","73949d55":"code","3cc8f770":"code","f4aa0393":"code","8237040a":"code","0a40e28e":"code","cea7ac7b":"markdown","159943aa":"markdown","6bd5681a":"markdown","68916701":"markdown","6270d142":"markdown","86babdd2":"markdown","4cf0e135":"markdown","01ade425":"markdown","d88a64fe":"markdown","3cbda497":"markdown","4dbda8cc":"markdown","4f1b3bfd":"markdown","10795e83":"markdown","2e71d216":"markdown","51eaebc3":"markdown","3d264fec":"markdown","5b838c47":"markdown","0b56997b":"markdown","f5b4fafa":"markdown","0d13398a":"markdown","624705e8":"markdown","d44b3656":"markdown","0fee9640":"markdown","a1e531f9":"markdown","609e2382":"markdown","4b09f4f0":"markdown","74ecd6e1":"markdown","82da3e58":"markdown","88221c69":"markdown","c939fc32":"markdown","90cbd87c":"markdown","b1dce013":"markdown","a10ed977":"markdown","8940650e":"markdown","7eea028c":"markdown","14ca9984":"markdown","f2b7464a":"markdown","686974b2":"markdown","56cae5c5":"markdown","6c5f26ec":"markdown","8201f937":"markdown","93427f03":"markdown","0b27b1fb":"markdown","ec0a15f2":"markdown","b79178c2":"markdown","2bfea23c":"markdown","5d4f4881":"markdown","f1db7bed":"markdown","7130b99d":"markdown","6e83c78c":"markdown","284b2114":"markdown","60894a48":"markdown","391199bc":"markdown","d4b374a6":"markdown","12ea842e":"markdown","e08b3562":"markdown","81e50c3e":"markdown","730e1e63":"markdown","49888527":"markdown","d6d8617b":"markdown","9851c560":"markdown"},"source":{"1c50b7ee":"# Libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","8c557220":"# Reading Dataset\ndf = pd.read_csv('..\/input\/bank-marketing-dataset\/bank.csv')\ndf.head()","dd38db73":"df.describe()","6077f4c7":"# Looking for NaNs\ndf.isnull().sum()","64a6399a":"fig, ax = plt.subplots()\nsns.countplot(x ='deposit', data = df, palette = 'viridis')\n\nplt.title('Deposit Distribution of Bank Customers', fontsize = 16)\nplt.xlabel('Deposit', fontsize = 14)\nplt.ylabel('Total Customers', fontsize = 14)\nplt.xticks(fontsize = 12)\n\n# Show the plot\nplt.show()","e82d643e":"from matplotlib.patches import Patch\nfig, ax = plt.subplots(1, 2, figsize=(15, 5))\nfig.suptitle(\"Balance vs Deposits of Customers\", size = 16)\n\n# Subplot 1\nax[0].hist(df[df[\"deposit\"]=='no'][\"balance\"], bins=30, alpha=0.5, color=\"green\", label=\"Non-Depositors\")\nax[0].hist(df[df[\"deposit\"]=='yes'][\"balance\"], bins=30, alpha=0.5, color=\"blue\", label=\"Depositors\")\n\nax[0].set_xlabel(\"Balance\", fontsize=14)\nax[0].set_ylabel(\"Total Customers\", fontsize=14)\nax[0].legend(fontsize = 11);\n\n# Subplot 2\nsns.boxplot(x=\"balance\", y=\"deposit\", data=df, orient=\"h\", palette={ 'no':\"#80e880\", 'yes':\"#2626ff\"}, ax = ax[1])\nax[1].get_yaxis().set_visible(False)\nax[1].set_xlabel('Balance', fontsize=14)\n\ncolor_patches = [\n    Patch(facecolor=\"#80e880\", label=\"Non-Depositors\"),\n    Patch(facecolor=\"#2626ff\", label=\"Depositors\")\n]\nax[1].legend(handles=color_patches, fontsize=11);","27277bc9":"fig, ax = plt.subplots(1, 2, figsize=(15, 5))\nfig.suptitle(\"Age vs Deposits of Customers\", size = 16)\n\n# Subplot 1\nax[0].hist(df[df[\"deposit\"]=='no'][\"age\"], bins=7, alpha=0.5, color=\"green\", label=\"Non-Depositors\")\nax[0].hist(df[df[\"deposit\"]=='yes'][\"age\"], bins=7, alpha=0.5, color=\"blue\", label=\"Depositors\")\n\nax[0].set_xlabel(\"Age\", fontsize=14)\nax[0].set_ylabel(\"Total Customers\", fontsize=14)\nax[0].legend(fontsize = 11);\n\n# Subplot 2\nsns.boxplot(x=\"age\", y=\"deposit\", data=df, orient=\"h\", palette={ 'no':\"#80e880\", 'yes':\"#2626ff\"}, ax = ax[1])\nax[1].get_yaxis().set_visible(False)\nax[1].set_xlabel('Age', fontsize=14)\n\ncolor_patches = [\n    Patch(facecolor=\"#80e880\", label=\"Non-Depositors\"),\n    Patch(facecolor=\"#2626ff\", label=\"Depositors\")\n]\nax[1].legend(handles=color_patches, fontsize=11);","546dafaf":"fig, ax = plt.subplots()\n\nsns.catplot(\"job\", hue = 'deposit', data=df, kind=\"count\", palette={'no':\"#80e880\", 'yes':\"#2626ff\"}, legend = False)\n\ncolor_patches = [\n    Patch(facecolor=\"#80e880\", label=\"Non-Depositors\"),\n    Patch(facecolor=\"#2626ff\", label=\"Depositors\")\n]\n\nplt.title(\"Job vs Deposit of Customers\", size = 18, y=1.08) \nplt.xlabel(\"Education\", size = 14)\nplt.ylabel(\"Count\", size = 14)\nplt.xticks(size = 12, rotation = 'vertical')\nplt.legend(handles = color_patches, fontsize = 12,  bbox_to_anchor=(1.4,1.05))\n\nplt.close(1) ","c9379fe6":"fig, ax = plt.subplots()\n\nsns.catplot(\"marital\", hue = 'deposit', data=df, kind=\"count\", palette={'no':\"#80e880\", 'yes':\"#2626ff\"}, legend = False)\n\ncolor_patches = [\n    Patch(facecolor=\"#80e880\", label=\"Non-Depositors\"),\n    Patch(facecolor=\"#2626ff\", label=\"Depositors\")\n]\n\nplt.title(\"Marital vs Deposit of Customers\", size = 18, y=1.08) \nplt.xlabel(\"Marital\", size = 14)\nplt.ylabel(\"Count\", size = 14)\nplt.xticks(size = 12)\nplt.legend(handles = color_patches, fontsize = 12,  bbox_to_anchor=(1.4,1.05))\n\nplt.close(1) ","a884aac6":"fig, ax = plt.subplots(figsize = (15,5))\n\nsns.catplot(\"education\", hue = 'deposit', data=df, kind=\"count\", palette={'no':\"#80e880\", 'yes':\"#2626ff\"}, legend = False)\n\ncolor_patches = [\n    Patch(facecolor=\"#80e880\", label=\"Non-Depositors\"),\n    Patch(facecolor=\"#2626ff\", label=\"Depositors\")\n]\n\nplt.title(\"Education vs Deposit of Customers\", size = 18, y=1.08) \nplt.xlabel(\"Education\", size = 14)\nplt.ylabel(\"Count\", size = 14)\nplt.xticks(size = 14)\nplt.legend(handles = color_patches, fontsize = 12,  bbox_to_anchor=(1.4,1.05))\n\nplt.close(1) ","690beeb9":"col_list = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'month', 'poutcome', 'deposit', 'contact']\n\nfor col in col_list:\n    df[col] = df[col].astype('category')\n    df[col] = df[col].cat.codes\n\ndf.head()","2c5e01d6":"%matplotlib inline\ndf.hist(bins = 50, figsize=(20,16), color = '#00A86B') \nplt.show()\n","1160c924":"import scipy\nimport scipy.stats as stats\n\ndef outlier_cols(x): \n    n = len(x)\n    mean_x = np.mean(x)\n    sd_x = np.std(x)\n    numerator = max(abs(x-mean_x))\n    g_calculated = numerator\/sd_x\n    t_value = stats.t.ppf(1 - 0.05 \/ (2 * n), n - 2)\n    g_critical = ((n - 1) * np.sqrt(np.square(t_value))) \/ (np.sqrt(n) * np.sqrt(n - 2 + np.square(t_value)))\n    return col if (g_critical) < g_calculated else 0","ff759f38":"import numpy as np\n\ncols_with_outliers = []\nfor col in df.columns:\n    outlier_col = outlier_cols(df[col])\n    cols_with_outliers.append(outlier_col)\n\nwhile (cols_with_outliers.count(0)):\n    cols_with_outliers.remove(0)\nprint('Columns with outliers are: {}'.format(cols_with_outliers) )","f8ec5fa2":"for col in cols_with_outliers:\n  if col != 'pdays' and col != 'balance':    # pdays and balance have negative values, so scaling will result in NaNs\n    df[col] = (df[col]**(1\/3.7))","16f436f3":"# Columns still possessing outliers\nany_outlier_col = []\nfor col in cols_with_outliers:\n    outlier_col = outlier_cols(df[col])\n    any_outlier_col.append(outlier_col)\n\nwhile (any_outlier_col.count(0)): \n    any_outlier_col.remove(0)\nany_outlier_col","00643e08":"def grubbs_test(x):\n    n = len(x)\n    mean_x = np.mean(x)\n    sd_x = np.std(x)\n    numerator = max(abs(x-mean_x))\n    g_calculated = numerator\/sd_x\n    print(\"Grubbs Calculated Value:\",g_calculated)\n    t_value = stats.t.ppf(1 - 0.05 \/ (2 * n), n - 2)\n    g_critical = ((n - 1) * np.sqrt(np.square(t_value))) \/ (np.sqrt(n) * np.sqrt(n - 2 + np.square(t_value)))\n    print(\"Grubbs Critical Value:\",g_critical)\n    if g_critical > g_calculated:\n        print(\"From grubbs_test we observe that calculated value is lesser than critical value, Accept null hypothesis and conclude that there is no outlier\\n\")\n    else:\n        print(\"From grubbs_test we observe that calculated value is greater than critical value, Reject null hypothesis and conclude that there is an outliers\\n\")","b8429d0c":"for col in any_outlier_col:\n  if col != 'pdays' and col != 'balance':\n    df[col] = (df[col]**(3.7))  ","834e4cd0":"# Plotting histogram\n%matplotlib inline\ndf['balance'].hist(bins = 50, figsize=(10,7), color = '#00A86B') \nplt.show()","605e960a":"# Removing outliers\ncut_off = 11000\nfor i in df['balance']:\n    if i >= cut_off:\n        df['balance'] = df['balance'].replace(i, cut_off)\n\ngrubbs_test(df['balance'])","0a050539":"# Plotting histogram\n%matplotlib inline\ndf['campaign'].hist(bins = 50, figsize=(10,7), color = '#00A86B') \nplt.show()","0040e239":"# Removing outliers\ncut_off = 12\nfor i in df['campaign']:\n    if i >= cut_off:\n        df['campaign'] = df['campaign'].replace(i, cut_off)\n\ngrubbs_test(df['campaign'])","dab7d571":"# Plotting histogram\n%matplotlib inline\ndf['previous'].hist(bins = 50, figsize=(10,7), color = '#00A86B') \nplt.show()","cd0e3eeb":"# Removing Outliers \ncut_off = 8\nfor i in df['previous']:\n    if i >= cut_off:\n        df['previous'] = df['previous'].replace(i, cut_off)\n\ngrubbs_test(df['previous'])","f0dd0d88":"# Plotting histogram\n%matplotlib inline\ndf['pdays'].hist(bins = 50, figsize=(10,7), color = '#00A86B') \nplt.show()","63ef472c":"# Removing Outliers \ncut_off = 500\nfor i in df['pdays']:\n    if i >= cut_off:\n        df['pdays'] = df['pdays'].replace(i, cut_off)\n\ngrubbs_test(df['pdays'])","2f0eccc9":"df.isnull().sum()","977ad3d6":"# fig\nfig= plt.figure(figsize=(12, 12))\n\n# mask\nmask = np.triu(df.corr())\n\n# axes \naxes = fig.add_axes([0, 0, 1, 1])\nsns.heatmap(df.dropna().corr(), annot=True, mask=mask, square=True,fmt='.2g',vmin=-1, vmax=1, center= 0, cmap='viridis',\n            linecolor='white', cbar_kws= {'orientation': 'vertical'}, ax=axes) \n\n# title\naxes.text(-1, -1.5, 'Correlation', color='black', fontsize=24, fontweight='bold')\n\nfig.show()","21e53aea":"corr_matrix = df.corr()\ncorr_matrix['deposit'].sort_values()","c0deb352":"# feature -I\ndf['dur_pdays'] = ((df['duration'] + df['previous']**0.15 - (df['contact']**2)**0.0003 )**2)**0.25\n\n# feature -II\ndf['contact_housing'] = (df['housing'])**(0.2) + (df['campaign'])**(0.35)  + df['contact']**(0.2)","ce94dcc9":"df.head()","b016aae1":"corr_matrix = df.corr()\ncorr_matrix['deposit'].sort_values()","f9825a67":"import sklearn\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 65)\nfor train_index, test_index in split.split(df, df['deposit']):\n    train_set = df.loc[train_index]\n    test_set = df.loc[test_index]\n      \nprint(f\"Rows in train set : {len(train_set)}\\nRows in test set: {len(test_set)}\\n\")","70f5fa0b":"# train_set\ntrain_labels = train_set[\"deposit\"].copy()    # Storing feature in labels variable\ntrain_set = train_set.drop([\"deposit\", 'default', 'job'], axis = 1)       # Dropping 'default' and 'job' column to improve accuracy\n\n# test_set\ntest_labels = test_set[\"deposit\"].copy()     # Storing feature in labels variable\ntest_set = test_set.drop([\"deposit\", 'default', 'job'], axis = 1)   # Dropping 'default' and 'job' column to improve accuracy","123c8150":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.ensemble import ExtraTreesClassifier  \n\n# Declaring and fitting classifier model\netc = ExtraTreesClassifier()\netc.fit(train_set, train_labels)\n    \n# Test set prediction\netc_predict = etc.predict(test_set)\n\n# Evaluating predictions\netc_accuracy = accuracy_score(test_labels, etc_predict)\netc_cm = confusion_matrix(test_labels, etc_predict)\n\n# Printing classification report \nprint('ExtraTreesClassifier Report:\\n')\nprint(classification_report(test_labels, etc_predict))\nprint('Accuracy of ExtraTreesClassifier is: {}'.format(etc_accuracy))","51e3c89f":"from sklearn.ensemble import AdaBoostClassifier\n\n# Declaring and fitting classifier model\nabc = AdaBoostClassifier()\nabc.fit(train_set, train_labels)\n    \n# Test set prediction\nabc_predict = abc.predict(test_set)\n\n# Evaluating predictions\nabc_accuracy = accuracy_score(test_labels, abc_predict)\nabc_cm = confusion_matrix(test_labels, abc_predict)\n\n# Printing classification report \nprint('AdaBoostClassifier Report:\\n')\nprint(classification_report(test_labels, abc_predict))\nprint('Accuracy of AdaBoostClassifier is: {}'.format(abc_accuracy))","2ff50cb7":"from sklearn.ensemble import BaggingClassifier\n\n# Declaring and fitting classifier model\nbc = BaggingClassifier()\nbc.fit(train_set, train_labels)\n    \n# Test set prediction\nbc_predict = bc.predict(test_set)\n\n# Evaluating predictions\nbc_accuracy = accuracy_score(test_labels, bc_predict)\nbc_cm = confusion_matrix(test_labels, bc_predict)\n\n# Printing classification report \nprint('BaggingClassifier Report:\\n')\nprint(classification_report(test_labels, bc_predict))\nprint('Accuracy of BaggingClassifier is: {}'.format(bc_accuracy))","d321f124":"from sklearn.ensemble import GradientBoostingClassifier\n\n# Declaring and fitting classifier model\ngbc = GradientBoostingClassifier()\ngbc.fit(train_set, train_labels)\n    \n# Test set prediction\ngbc_predict = gbc.predict(test_set)\n\n# Evaluating predictions\ngbc_accuracy = accuracy_score(test_labels, gbc_predict)\ngbc_cm = confusion_matrix(test_labels, gbc_predict)\n\n# Printing classification report \nprint('GradientBoostingClassifier Report:\\n')\nprint(classification_report(test_labels, gbc_predict))\nprint('Accuracy of GradientBoostingCLassifier is: {}'.format(gbc_accuracy))","475abca2":"from sklearn.ensemble import RandomForestClassifier\n\n# Declaring and fitting classifier model\nrfc = RandomForestClassifier()\nrfc.fit(train_set, train_labels)\n    \n# Test set prediction\nrfc_predict = rfc.predict(test_set)\n\n# Evaluating predictions\nrfc_accuracy = accuracy_score(test_labels, rfc_predict)\nrfc_cm = confusion_matrix(test_labels, rfc_predict)\n\n# Printing classification report \nprint('RandomForestClassifier Report:\\n')\nprint(classification_report(test_labels, rfc_predict))\nprint('Accuracy of RandomForestClassifier is: {}'.format(rfc_accuracy))","b4b423a5":"from sklearn.tree import DecisionTreeClassifier\n\n# Declaring and fitting classifier model\ndtc = DecisionTreeClassifier()\ndtc.fit(train_set, train_labels)\n    \n# Test set prediction\ndtc_predict = dtc.predict(test_set)\n\n# Evaluating predictions\ndtc_accuracy = accuracy_score(test_labels, dtc_predict)\ndtc_cm = confusion_matrix(test_labels, dtc_predict)\n\n# Printing classification report \nprint('DecisionTreeClassifier Report:\\n')\nprint(classification_report(test_labels, dtc_predict))\nprint('Accuracy of DecisionTreeClassifier is: {}'.format(dtc_accuracy))","e6b20f37":"# !pip install catboost","906ddecb":"from catboost import CatBoostClassifier\n\n# Declaring classifier parameters\n\ncbc_params = {'loss_function':'Logloss', \n          'eval_metric':'AUC', \n          'verbose': 200, \n          'random_seed': 1,\n         }\n\n\n# Declaring classifier model\ncbc = CatBoostClassifier(**cbc_params)\n\n# Fitting classifer to training set\ncbc.fit(train_set, train_labels,               \n          eval_set=(test_set, test_labels), \n          use_best_model=True, \n          plot=True \n);\n\n# Predicting test set\ncbc_predict = cbc.predict(test_set)","738c7383":"# Evaluating predictions\ncbc_accuracy = accuracy_score(test_labels, cbc_predict)\ncbc_cm = confusion_matrix(test_labels, cbc_predict)\n\n# Printing classification report \nprint('CatBoostClassifier Report:\\n')\nprint(classification_report(test_labels, cbc_predict))\nprint('Accuracy of CatBoostClassifier is: {}'.format(cbc_accuracy))","6b5cfd10":"# !pip install xgboost","d99e85a8":"# Declaring classifier parameters\nimport xgboost as xbg\nfrom xgboost import XGBClassifier\n\n# Classifier parameters\nxgb_param = {\n    'eta': 0.3, \n    'max_depth': 3,  \n    'objective': 'multi:softprob',  \n    'num_class': 3} \n\n# Declaring classifier model\nxgb = XGBClassifier(**xgb_param)\n\n# Fitting classifer to training set\nxgb.fit(train_set, train_labels) \n\n# Predicting test set\nxgb_predict = xgb.predict(test_set)","73a311fb":"# Evaluating predictions\nxgb_accuracy = accuracy_score(test_labels, xgb_predict)\nxgb_cm = confusion_matrix(test_labels, xgb_predict)\n\n# Printing classification report \nprint('XGBoostClassifier Report:\\n')\nprint(classification_report(test_labels, xgb_predict))\nprint('Accuracy of XGBClassifier is: {}'.format(xgb_accuracy))","b816ec7e":"# !pip install lightgbm","eebc3759":"# Declaring classifier parameters\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\n\n# Declaring classifier parameters\nlgbm_param = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\",\n        \"num_leaves\" : 40,\n        \"learning_rate\" : 0.004,\n        \"bagging_fraction\" : 0.6,\n        \"feature_fraction\" : 0.6,\n        \"bagging_frequency\" : 6,\n        \"bagging_seed\" : 42,\n        \"verbosity\" : -1,\n        \"seed\": 42\n    }\n         \n# Declaring classifier model\nlgbm = lgb.LGBMClassifier(**lgbm_param)\n\n# Fitting classifer to training set\nlgbm.fit(train_set, train_labels) \n\n# Predicting test set\nlgbm_predict = lgbm.predict(test_set)","d0fe362c":"# Evaluating predictions\nlgbm_accuracy = accuracy_score(test_labels, lgbm_predict)\nlgbm_cm = confusion_matrix(test_labels, lgbm_predict)\n\n# Printing classification report \nprint('LGBMClassifier Report:\\n')\nprint(classification_report(test_labels, lgbm_predict))\nprint('Accuracy of LGBMClassifier is: {}'.format(lgbm_accuracy))","9ffb9694":"model_names = ['ExtraTreesClassifier', 'AdaBoostClassifier', 'BaggingClassifier', 'GradientBoostingClassifier', 'RandomForestClassifier', 'DecisionTreeClassifier', 'CatBoostClassifier', 'XGBClassifier', 'LGBMClassifier']\naccuracies = [etc_accuracy, abc_accuracy, bc_accuracy, gbc_accuracy, rfc_accuracy, dtc_accuracy, cbc_accuracy, xgb_accuracy, lgbm_accuracy]\n\naccuracy_table = pd.DataFrame({'Model':model_names, 'Accuracy':accuracies})\naccuracy_table = accuracy_table.sort_values(by=['Accuracy'], axis=0, ascending = False)\naccuracy_table.reset_index(inplace = True, drop=True)\naccuracy_table.index += 1\naccuracy_table.head(15)","055c2956":"# Defining custom color map\nimport matplotlib.colors\n\nnorm = matplotlib.colors.Normalize(-1,1)\ncolors = [[norm(-1.0), \"#e9fcdc\"], \n          [norm(-0.6), \"#d9f0c9\"], \n          [norm( 0.6), \"#4CBB17\"],\n          [norm( 1.0), \"#0B6623\"]]\n\ncmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n\nfig, ax=plt.subplots()\nx = np.arange(10)\ny = np.linspace(-1,1,10)\nsc = ax.scatter(x,y, c=y, norm=norm, cmap=cmap)\nfig.colorbar(sc, orientation=\"horizontal\")\nplt.show()","eb38aa36":"import pylab\n\nfig = plt.figure(figsize=(14,4.5))\nplt.suptitle(\"Comparison of Top #3 Classifiers\", family='Serif', size=16, ha='center')\n\n# ------------ subplot #1 ----------------\nplt.subplot(131)\nplt.title('CatBoost Classifier', size = 15)\n\n\n# Declaring heatmap labels\ngroup_counts = ['{0:0.0f}'.format(value) for value in cbc_cm.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in cbc_cm.flatten()\/np.sum(cbc_cm)]\n\nlabels = [f\"{v2}\\n{v3}\" for v2, v3 in zip(group_counts, group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\n\n# Plotting heatmap \nsns.heatmap(cbc_cm, annot=labels, annot_kws={\"size\": 15}, fmt = '', cmap=cmap)\n\n# Adding figure labels\nplt.ylabel('Actual Values')\nplt.xlabel('Predicted Values \\n \\n Accuracy: {}'.format(round(cbc_accuracy, 4)))\n\n\n# ------------ subplot #2 ----------------\nplt.subplot(132)\nplt.title('XGBoost Classifier', size = 15)\n\n# Declaring heatmap labels\ngroup_counts = ['{0:0.0f}'.format(value) for value in xgb_cm.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in xgb_cm.flatten()\/np.sum(xgb_cm)]\n\nlabels = [f\"{v2}\\n{v3}\" for v2, v3 in zip(group_counts, group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\n\n# Plotting heatmap \nsns.heatmap(xgb_cm, annot=labels, annot_kws={\"size\": 15}, fmt = '', cmap=cmap)\n\n\n# Adding figure labels\nplt.ylabel('Actual Values')\nplt.xlabel('Predicted Values \\n \\n Accuracy: {}'.format(round(xgb_accuracy, 4)))\n\n\n\n# ------------ subplot #3 ----------------\nplt.subplot(133)\nplt.title('GradientBoosting Classifier', size = 15)\n\n# Declaring heatmap labels\ngroup_counts = ['{0:0.0f}'.format(value) for value in gbc_cm.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in gbc_cm.flatten()\/np.sum(gbc_cm)]\n\nlabels = [f\"{v2}\\n{v3}\" for v2, v3 in zip(group_counts, group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\n\n# Plotting heatmap \nsns.heatmap(gbc_cm, annot=labels, annot_kws={\"size\": 15}, fmt = '', cmap=cmap)\n\n# Adding figure labels\nplt.ylabel('Actual Values')\nplt.xlabel('Predicted Values \\n \\n Accuracy: {}'.format(round(gbc_accuracy, 4)))\n\nfig.tight_layout()\nplt.show()","73949d55":"# Computing feature importance\nfeature_importances = pd.DataFrame(cbc.feature_importances_,\n                                   index = train_set.columns,\n                                   columns=['importance']).sort_values('importance',ascending=False)\n\n# Plotting feature importance\nplt.figure(figsize=(20,8))\nplt.plot(feature_importances)\nplt.scatter(y=feature_importances.importance,x=feature_importances.index)\nplt.title('Importance of Features in Dataframe', fontsize = 16)\nplt.ylabel('Importance', fontsize=14)\nplt.xlabel('Features', fontsize = 14)\nplt.grid()\nplt.show()","3cc8f770":"train_set.columns, train_set.month.value_counts()","f4aa0393":"# Assigning categorical features\ncat_features = [1, 2, 3, 7, 8]  \n\n# Declaring classifer parameters\ncbc_params = {'loss_function':'Logloss',\n          'eval_metric':'AUC',\n          'cat_features': cat_features,\n          'verbose': 200,\n          'random_seed': 1,\n          'iterations': 1000,\n          'max_depth': 7    \n         }\n\n# Declaring model\ncbc_improved = CatBoostClassifier(**cbc_params)\n\n# Fitting model to train set \ncbc_improved.fit(train_set, train_labels,\n          eval_set=(test_set, test_labels),\n          use_best_model=True,\n          plot=True\n         );\n\n# Predicting test set\ncbc_improved_predict = cbc_improved.predict(test_set)","8237040a":"# Evaluating predictions\ncbc_improved_accuracy = accuracy_score(test_labels, cbc_improved_predict)\ncbc_improved_cm = confusion_matrix(test_labels, cbc_improved_predict)\n\n# Printing classification report \nprint('CatBoostClassifier Report:\\n')\nprint(classification_report(test_labels, cbc_improved_predict))\nprint('Accuracy of CatBoostClassifier is: {}'.format(cbc_improved_accuracy))","0a40e28e":"fig = plt.figure(figsize=(14,4.5))\ntitle = plt.suptitle(\"Evaluating Improvement in CatBoost Classifier\", family='Serif', size=16, ha='center')\ntitle.set_position([0.48, 1.05])\n\n# ------------ subplot #1 ----------------\nplt.subplot(121)\nplt.title('Improved CatBoost Classifier', size = 15)\n\n\n# Declaring heatmap labels\ngroup_counts = ['{0:0.0f}'.format(value) for value in cbc_improved_cm.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in cbc_improved_cm.flatten()\/np.sum(cbc_improved_cm)]\n\nlabels = [f\"{v2}\\n{v3}\" for v2, v3 in zip(group_counts, group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\n\n# Plotting heatmap \nsns.heatmap(cbc_improved_cm, annot=labels, annot_kws={\"size\": 15}, fmt = '', cmap=cmap)\n\n# Adding figure labels\nplt.ylabel('Actual Values', size = 12)\nplt.xlabel('Predicted Values \\n \\n Accuracy: {}'.format(round(cbc_improved_accuracy, 4)), size = 12)\n\n\n# ------------ subplot #2 ----------------\nplt.subplot(122)\nplt.title('Previous CatBoost Classifier', size = 15)\n\n# Declaring heatmap labels\ngroup_counts = ['{0:0.0f}'.format(value) for value in cbc_cm.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in cbc_cm.flatten()\/np.sum(cbc_cm)]\n\nlabels = [f\"{v2}\\n{v3}\" for v2, v3 in zip(group_counts, group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\n\n# Plotting heatmap \nsns.heatmap(cbc_cm, annot=labels, annot_kws={\"size\": 15}, fmt = '', cmap=cmap)\n\n# Adding figure labels\nplt.ylabel('Actual Values', size=12)\nplt.xlabel('Predicted Values \\n \\n Accuracy: {}'.format(round(cbc_accuracy, 4)), size = 12)\nplt.show()","cea7ac7b":"## Computing Correlation ","159943aa":"## Catboost Classifier ","6bd5681a":"# 2. Exploratory Data Analysis","68916701":"## ExtraTreesClassifier","6270d142":"* ### 'campaign' Column","86babdd2":"## GradientBoostingClassifier","4cf0e135":"Model accuracy improved to <b>0.8773<\/b> ","01ade425":"## Plotting Correlation Heatmap","d88a64fe":"## Preparing Train-Test Sets and Labels","3cbda497":"## Printing Feature Correlations","4dbda8cc":"# 5. Columns that Still have Outliers","4f1b3bfd":"Added features have good correlation with deposit column.","10795e83":"## Individually Removing Outliers from Columns","2e71d216":"## Evaluating Improvement","51eaebc3":"## Removing Cubical Transform from Columns with Outliers","3d264fec":"# 10. Performance Comparison of All Models","5b838c47":"There is slight improvement in <b>bestTest score<\/b>.  ","0b56997b":"## Balance of Dataset","f5b4fafa":"## Feature Importance for CatBoostClassifier","0d13398a":"# 12. References: \n\n##### <b>I admire the content shared in the following links. These have significantly helped in completing this notebook.<\/b>  \n\n* Data Visualization: https:\/\/medium.com\/analytics-vidhya\/tutorial-exploratory-data-analysis-eda-with-categorical-variables-6a569a3aea55\n\n* Plotly: https:\/\/www.kaggle.com\/subinium\/basic-of-statistical-viz-plotly-seaborn\/notebook\n\n* Outliers: https:\/\/www.kaggle.com\/nareshbhat\/outlier-the-silent-killer\n\n* Outliers: https:\/\/www.kaggle.com\/vikram92\/units-sold-prediction-for-e-commerce\n\n* Notebook Inspiration: https:\/\/www.kaggle.com\/kurazh\/diabetes-prediction-score-0-92-and-eda\n\n* Notebook Inspiration: https:\/\/www.kaggle.com\/emdemor\/brazilian-predictions-of-deaths-and-recoveries\n\n* Notebook Inspiration: https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-heart-attack-90-accuracy-score\n\n* Notebook Inspiration: https:\/\/www.kaggle.com\/mohamedzayton\/drug-classification-rf-nn#Building-Model-1--%3E-RF\n\n* Hyperparameter Optimization: https:\/\/www.kaggle.com\/sh0wmaker\/compare-algorithms-in-drug-classification#Model-Implementation\n\n* Catboost: https:\/\/www.kaggle.com\/mitribunskiy\/tutorial-catboost-overview\n\n* Model Evaluation: https:\/\/www.kaggle.com\/gcmadhan\/bank-campaign-eda-classification-83-accu\n\n* LGBM: https:\/\/towardsdatascience.com\/a-quick-guide-to-lightgbm-library-ef5385db8d10\n\n* Classification Models: https:\/\/www.kaggle.com\/samratp\/lightgbm-xgboost-catboost\n\n* Confusion Matrix: https:\/\/medium.com\/@dtuk81\/confusion-matrix-visualization-fc31e3f30fea\n\n* Confusion Matrix: https:\/\/www.kaggle.com\/agungor2\/various-confusion-matrix-plots\n\n* Subplots: https:\/\/www.kaggle.com\/asimislam\/tutorial-python-subplots\n\n* Accuracy Improvement: https:\/\/www.kaggle.com\/avelinocaio\/top-5-voting-classifier-in-python","624705e8":"* <b>Tertiary<\/b> educated customers prefer to <b>deposit<\/b>. \n* Customers with other education type <b>deposit less<\/b>. ","d44b3656":"* <b>Fewer<\/b> married customers prefer to <b>deposit<\/b>. \n* <b>Larger<\/b> single customers prefer to <b>deposit<\/b>.","0fee9640":"## CatBoost with Categorical Features","a1e531f9":"* There is increase in <b>true positives<\/b>.\n* False positives have reduced by optimizing the parameters of CatBoostClassifier. ","609e2382":"## Verifying no NaN inclusions","4b09f4f0":"## Marital vs Deposit of Customers","74ecd6e1":"## AdaBoostClassifier","82da3e58":"## Grubbs Function for Outlier Detection","88221c69":"<b>month<\/b>, <b>dur_pdays<\/b> and <b>duration<\/b>  are important features","c939fc32":"## XGBoostClassifier\n\nhttps:\/\/towardsdatascience.com\/a-beginners-guide-to-xgboost-87f5d4c30ed7","90cbd87c":"## Splitting Train-Test Set","b1dce013":"## Finding Columns with Outliers","a10ed977":"## Education vs Deposit of Customers","8940650e":"## LGBM Classifier","7eea028c":"## Age vs Deposits of Customers","14ca9984":"# 6. Computing Correlation","f2b7464a":"# 1. Importing Libraries & Analyzing Dataframe","686974b2":"* ### 'pdays' Column","56cae5c5":"## Balance vs Deposits of Customers","6c5f26ec":"* Most of the customers of bank have <b>smaller balance<\/b>. \n* Customers with all range of balance <b>make deposits<\/b>. \n* But, most of the <b>deposits<\/b> are received from customers with balance in range of <b>(0, 1250)<\/b>.","8201f937":"# 9. Classification Using ML Models","93427f03":"## Removing Outliers Using Scaling","0b27b1fb":"* ### 'previous' Column","ec0a15f2":"This is one of the <b>best result<\/b> obtained from different models. This result may be further <b>improved<\/b> by assigning <b>categorical features<\/b> to CatBoostClassifier. ","b79178c2":"* ###  'balance' Column","2bfea23c":"## DecisionTreeClassifier","5d4f4881":"days, duration, previous, campaign, balance seem to consist of outliers","f1db7bed":"## BaggingClassifier","7130b99d":"* Customers from managment, retired, unemployed and student <b>jobtypes<\/b> prefer to <b>deposit<\/b>. \n* Customers in <b>services and blue-collar<\/b> jobs deposit considerably <b>less<\/b>. ","6e83c78c":"## RandomForestClassifier","284b2114":"# 4. Removing Outliers ","60894a48":"# 8. Train-Test Splitting","391199bc":"# 7. Adding Features","d4b374a6":"## Column Distribution","12ea842e":"# 3. Converting Columns to Categorical Numbers","e08b3562":"Dataset is well balanced","81e50c3e":"* Customers of <b>all age<\/b> groups make <b>deposits<\/b>.\n* Most of the customers are in the age group of <b>(25, 55) years<\/b>. A larger part of them are <b>non-depositors<\/b>. \n* Most of the customers <b>above 60<\/b> years make <b>deposits<\/b>. \n* Most of the customers <b>below 25<\/b> years make <b>deposits<\/b>. ","730e1e63":"## Job vs Deposit of Customers","49888527":"## Grubbs Function Function for Validating Outlier Removal","d6d8617b":"## Top #3 Models Evaluation","9851c560":"# 11. Best Model Optimization"}}