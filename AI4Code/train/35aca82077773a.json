{"cell_type":{"29c46afa":"code","d17ad20a":"code","7ac002d7":"code","7bed5e79":"code","fd079c33":"code","119ab120":"code","a826ead7":"code","5da98dc1":"code","e9db5901":"code","1dab299b":"code","215283f3":"markdown","aa1abe0b":"markdown","16fdfd1a":"markdown","eaaee111":"markdown","625914f4":"markdown","50a168d3":"markdown","dc19bee9":"markdown","2e1f3e03":"markdown","015daa1e":"markdown","ad349c3f":"markdown","ec4a0398":"markdown","d3caa7c6":"markdown","bbeba9c9":"markdown"},"source":{"29c46afa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import animation, rc\nfrom IPython.display import display, Math, HTML\nimport seaborn as sns\n\n%matplotlib inline\nfigsize=(10,5)\ndef error(x,y, w0,w1):\n    z = w0+w1*x\n    return np.mean((y-z)**2)\n\ndef plot(x,y,w0=None, w1=None, title=\"\"):\n    plt.figure(figsize=figsize)\n    plt.scatter(x, y,marker='o');\n    plt.xlabel(\"X\");\n    plt.ylabel(\"Y\");\n\n    plt.grid();\n    if w0!=None and w1!=None:\n        z = w1*x+w0\n        plt.plot(x, z,'g')\n        title += f\" MSE:{error(x,y,w0,w1):0.3f}\"\n        plt.vlines(x,y,z,colors='r',linestyles='dotted')\n    plt.title(title)","d17ad20a":"x = np.linspace(1,10,20)\nm = np.random.uniform(0.2,0.25, len(x))\nc = np.random.uniform(-0.3,-0.1, len(x))\ny = m*x + c\n\nplot(x,y)","7ac002d7":"plot(x, y, 0.0,0.1)","7bed5e79":"w0, w1 = np.zeros(2)\nn = len(y)\nalpha = 0.01\ntgt_mse = 0.01\nhist = {\"dJdw0\":[],\"dJdw1\":[],\"w0\":[], \"w1\":[],\"mse\":[]}\nfor i in range(1000):\n    z = w0+w1*x\n    mse = error(x, y, w0, w1)\n    dJdw0 = np.sum((z-y)\/n)\n    dJdw1 = np.sum(((z-y)*x)\/n)\n    w0_ = w0-alpha*dJdw0 \n    w1_ = w1-alpha*dJdw1\n    mse_ = error(x,y, w0_, w1_) \n\n    hist[\"dJdw0\"].append(dJdw0)\n    hist[\"dJdw1\"].append(dJdw1)\n    hist[\"w0\"].append(w0)\n    hist[\"w1\"].append(w1)\n    hist[\"mse\"].append(mse)\n\n    if mse<=tgt_mse:\n        print(f\"Converged after {i+1} steps\")\n        break\n    else:\n        w0,w1=(w0_,w1_)\n        \nplot(x,y, w0, w1)","fd079c33":"\nrc('animation', html='html5')\nc0 = sns.color_palette().as_hex()[0]\nc1 = sns.color_palette().as_hex()[1]\nc2 = sns.color_palette().as_hex()[2]\n\nfig, ax = plt.subplots(figsize=figsize)\nxmin,xmax,ymin,ymax = x.min(),x.max(),y.min(),y.max()\n\ntext = ax.set_title([])\nln, = plt.plot([], [],c=c2)\nlc = plt.vlines([],[],[],colors=c1,linestyles='dotted')\n# initialization function: plot the background of each frame\ndef init():   \n    ax.set_xlim((xmin, xmax))\n    ax.set_ylim((ymin, ymax))\n\n    plt.scatter(x, y,marker='o',c=c0);\n    plt.xlabel(\"X\");\n    plt.ylabel(\"Y\");\n    plt.grid();\n\n    ln.set_data([],[])\n    plt.close()\n    return (ln,)\n\n# animation function. This is called sequentially\ndef animate(i):\n    w0 = hist['w0'][i]\n    w1 = hist['w1'][i]\n    mse = hist['mse'][i]\n    #print(w0,w1)\n    z = w1*x+w0\n    ln.set_data(x,z)\n    lc.set_segments([[[i[0],i[1]],[i[0],i[2]]] for i in zip(x,y,z)])\n    text.set_text(f\"Univariate Linear Regression\\nIteration:{i}, MSE={mse:0.4f}, $\\\\alpha$={alpha:0.3f}\")\n    return (ln,)\n\n\n# call the animator. blit=True means only re-draw the parts that \n# have changed.\nanim = animation.FuncAnimation(fig, animate, \n                               init_func=init,\n                               frames=20, \n                               interval=300, \n                               blit=True, \n                               repeat=True\n                              )\n\n#writer = animation.PillowWriter()  \n#anim.save('lin_reg.gif', writer=writer)\nanim","119ab120":"plt.figure(figsize=figsize)\nplt.plot(hist[\"dJdw0\"][:50], label=r\"$\\partial J\/\\partial w_0$\")\nplt.plot(hist[\"dJdw1\"][:50], label=r\"$\\partial J\/\\partial w_1$\")\nplt.xlabel(\"Iteration\")\nplt.grid();\nplt.legend();","a826ead7":"plt.figure(figsize=figsize)\nplt.plot(hist[\"w0\"][:50], label=r\"$w_0$\")\nplt.plot(hist[\"w1\"][:50], label=r\"$w_1$\")\nplt.xlabel(\"Iteration\")\nplt.grid()\nplt.legend();","5da98dc1":"plt.figure(figsize=figsize)\nplt.plot(hist[\"mse\"][:50], label=r\"$MSE$\");\nplt.xlabel(\"Iteration\")\nplt.grid()\nplt.legend();","e9db5901":"rsq = 1- hist[\"mse\"]\/ np.sum((y-y.mean())**2)\nplt.figure(figsize=figsize)\nplt.plot(rsq[:50], label=r\"$R^2$\");\nplt.xlabel(\"Iteration\")\nplt.grid()\nplt.legend();","1dab299b":"xb = np.hstack((np.ones((x.shape[0],1)), x.reshape(x.shape[0],1)))\nw = np.zeros(1)\nz = np.linalg.inv(np.dot(xb.T, xb))\nw = np.dot(z, np.dot(xb.T,y))\nprint(f\"Analytical solution: w0={w[0]:0.4f}; w1={w[1]:0.4f}\")\nprint(f\"Numerical solution: w0={hist['w0'][-1]:0.4f}; w1={hist['w1'][-1]:0.4f}\")","215283f3":"## Analytical Solution\n\n$\\boxed{w = (x^Tx)^{-1}x^Ty}$\n\nPros:\n\n- Guaranteed to find optimal solution\n\nCons:\n\n- Computationally expensive with large datasets\n- Fails if the matrix is singular (non-invertible)\n","aa1abe0b":"### Model evaluation\n\nMean Squared Error is a quantitative measure of how well the model fits the data. However, MSE is unbounded and is affected by feature scaling. i.e. MSE for the same model will change if data is scaled.\n\n**Coefficient of determination** ($R^2$) is a standardized version for better interpretability. $R^2$ is the fraction of response variance that is captured by the model, and is defined as -\n\n$R^2 = 1 - \\frac{SSE}{SST}$\n\n$SSE = \\displaystyle\\sum_{i=1}^{n} (z_i-y_i)^2$\n\n$SST = \\displaystyle\\sum_{i=1}^{n} (y_i-\\mu_y)^2$\n\n$R^2$ is bounded between 0 and 1 for the training data.\n\nIf $R^2=1$, the model fits the data perfectly.\n\n$R^2$ can be negative when evaluated on previously unseen (test) data.","16fdfd1a":"The linear model can be described by\n\n$z_i = w_0 + w_1x_i$\n\nwhere $w_0$ is the y intercept, and $w_1$ is the slope","eaaee111":"To find the best fitting line, we would have to minimize this error. So the objective or cost function is given by\n\n$J(w_0,w_1) = \\frac{1}{2}MSE$\n\n$\\therefore J(w_0,w_1) = \\frac{1}{2n}\\displaystyle\\sum_{i=1}^{n} (w_0 + w_1x - y_i)^2$\n\nThe $\\frac{1}{2}$ is for mathematical simplicity, so that it cancels out when we take the derivative.","625914f4":"Considering an arbitrary line as shown below, we can compute the error (red lines) between actual data and predicted values.\n\nThe squared error for each data point $i$ is given by \n\nSE = $(z_i-y_i)^2$\n\nThe mean squared error is given by \n\n$MSE = \\frac{\\displaystyle\\sum_{i=1}^{n} (z_i-y_i)^2}{n}$ \n\nwhere $n$ is the number of samples in the dataset.\n\n$MSE = \\frac{1}{n}\\displaystyle\\sum_{i=1}^{n} (w_0 + w_1x - y_i)^2$ \n\n","50a168d3":"Partial derivative of $J$ w.r.t. $w_1$\n\n$\\frac{\\partial J}{\\partial w_1} = \\displaystyle\\sum_{i=1}^{n} \\frac{\\partial J}{\\partial z_i} \\frac{\\partial z_i}{\\partial w_1} $\n\n$\\boxed{\\frac{\\partial J}{\\partial w_1} = \\displaystyle\\sum_{i=1}^{n}\\frac{(z_i-y_i)}{n} x_i }\\tag{4}$\n","dc19bee9":"Partial derivative of $z_1$ w.r.t $w_1$\n\n$\\frac{\\partial z_1}{\\partial w_1} = \\frac{\\partial}{\\partial w_1}(w_1 x_1 + w_0)$\n\n$\\frac{\\partial z_1}{\\partial w_1} = x_1$\n\nIn general\n\n$\\boxed{\\frac{\\partial z_i}{\\partial w_1} = x_i}\\tag{2}$","2e1f3e03":"Steps\n1. Initialize $w_0$ and $w_1$ to random values\n2. Compute z\n3. Compute J\n4. Compute $\\frac{\\partial J}{\\partial w_1}$\n5. Compute $\\frac{\\partial J}{\\partial w_0}$\n6. Update weights $w_0$ and $w_1$\n7. Go back to step 2 and iterate till convergence \n\n","015daa1e":"Compute new weights $w_0$ and $w_1$\n\n$\\boxed{w_{0_{new}} = w_0 - \\alpha \\frac{\\partial J}{\\partial w_0}}\\tag{6}$\n\n$\\boxed{w_{1_{new}} = w_1 - \\alpha \\frac{\\partial J}{\\partial w_1}}\\tag{7}$\n\nwhere $\\alpha$ is the learning rate\n","ad349c3f":"Partial derivative of the cost function $J$ w.r.t. $z_1$ is given by-\n\n$\\frac{\\partial J}{\\partial z_1} = \\frac{1}{2n}[(z_1 - y_1)^2 + (z_2 - y_2)^2 + ...+(z_n - y_n)^2]$\n\nSince everything but the first term is a constant w.r.t. $z_1$ -\n\n$\\frac{\\partial J}{\\partial z_1} = \\frac{2}{2n}(z_1 - y_1)$\n\n$\\frac{\\partial J}{\\partial z_1} = \\frac{(z_1 - y_1)}{n}$\n\nIn general -\n\n$\\boxed{\\frac{\\partial J}{\\partial z_i} = \\frac{(z_i-y_i)}{n}}\\tag{1}$","ec4a0398":"## Univariate Linear Regression\n\nThe goal of univariate linear regression is to model the relationship between a single explanatory variable *x* and continuous valued target (response variable) *y*\n\n\nSay we have a set of data points like this. The goal of univariate linear regression is to find the best fitting line, given these data points.","d3caa7c6":"Partial derivative of $z_1$ w.r.t $w_0$\n\n$\\frac{\\partial z_1}{\\partial w_0} = \\frac{\\partial}{\\partial w_0}(w_1 x_1 + w_0)$\n\nIn general\n\n$\\boxed{\\frac{\\partial z_i}{\\partial w_0} = 1}\\tag{3}$","bbeba9c9":"Partial derivative of $J$ w.r.t $w_0$\n\n$\\frac{\\partial J}{\\partial w_0} = \\displaystyle\\sum_{i=1}^{n}\\frac{\\partial J}{\\partial z_i} \\frac{\\partial z_i}{\\partial w_0} $\n\n$\\boxed{\\frac{\\partial J}{\\partial w_0} = \\displaystyle\\sum_{i=1}^{n}\\frac{(z_i-y_i)}{n}}\\tag{5}$"}}