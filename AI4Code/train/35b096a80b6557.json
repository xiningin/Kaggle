{"cell_type":{"fc0e0ebe":"code","c456b19b":"code","d7c5497b":"code","1dea221b":"code","a84f41cd":"code","ab4ab3af":"code","69e31f64":"code","8e5fffd3":"code","78322115":"code","ec5d0924":"code","eed305bd":"code","d1de2992":"code","686d0827":"code","14aa8e94":"code","8411be6d":"code","952c46df":"code","318185d6":"code","1e63aa02":"code","e1c8b25f":"code","cfae4b1d":"code","60b61663":"code","da8ede45":"code","13b2d3c5":"code","558bf34d":"code","84ea992d":"code","061b9dd5":"code","389ecffc":"code","8c6d9d50":"code","14f025fe":"code","af9fe3bd":"code","5d7f3d40":"code","f22c7066":"code","42ff9e56":"code","e1598899":"code","83c6ae3e":"code","92a67976":"code","6db4ecec":"code","0280a3b6":"code","bb32c65d":"code","e0280b06":"code","488fbb9c":"code","12f004f2":"code","e8d3d011":"code","189ce5a6":"code","73a1c586":"code","e356df9b":"code","7c53cdd9":"code","295bc53e":"code","3d5990ab":"code","bacb0a88":"code","23fbafc2":"code","acbf1a5b":"code","843ca8cb":"code","b58fc4f3":"code","584f7454":"markdown","53f1ff12":"markdown","cd8e1a31":"markdown","b5f30981":"markdown","e1f7da73":"markdown","ec8c3ba2":"markdown","0ed07534":"markdown","d65a9660":"markdown","b36c50e0":"markdown","e7e0cf16":"markdown","029e0860":"markdown","6a9d998e":"markdown","f933a6b9":"markdown","6c82ec00":"markdown","ad98ed38":"markdown","143bd6aa":"markdown","dce70e2d":"markdown","36c37bd1":"markdown","707940e6":"markdown","00f46f80":"markdown","3d1a2243":"markdown","bc337386":"markdown","a75ad3ea":"markdown","b2b5272e":"markdown","8101ea55":"markdown","64811256":"markdown","9f0b4ae5":"markdown","781125c8":"markdown","e5779b67":"markdown","6f072418":"markdown","4bd96738":"markdown","8f47a289":"markdown","814dbaaf":"markdown","bca11ecf":"markdown","4d1ddd01":"markdown","f49f84dd":"markdown"},"source":{"fc0e0ebe":"import numpy as np\nimport pandas as pd\nimport scipy as sp\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom mlxtend.preprocessing import minmax_scaling\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor","c456b19b":"# Load the training data\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", index_col=0)\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col=0)\n\n# Preview the data\ntrain.head()","d7c5497b":"train.describe()","1dea221b":"sns.boxplot(train['target'])","a84f41cd":"cat_features = [feature for feature in train.columns if 'cat' in feature]\ncont_features = [feature for feature in train.columns if 'cont' in feature]","ab4ab3af":"print('Rows and Columns in train dataset:', train.shape)\nprint('Rows and Columns in test dataset:', test.shape)","69e31f64":"print('Missing values in train dataset:', sum(train.isnull().sum()))\nprint('Missing values in test dataset:', sum(test.isnull().sum()))","8e5fffd3":"fig = plt.figure(figsize=(15, 10), facecolor='#f6f5f5')\ngs = fig.add_gridspec(4, 4)\ngs.update(wspace=0.2, hspace=0.05)\n\nbackground_color = \"#f6f5f5\"\n\nrun_no = 0\nfor col in range(0, 4):\n    for row in range(0, 4):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        locals()[\"ax\"+str(run_no)].set_yticklabels([])\n        locals()[\"ax\"+str(run_no)].tick_params(axis='y', which=u'both',length=0)\n        for s in [\"top\",\"right\", 'left']:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nax0.text(-0.3, 5.3, 'Continuous Features Distribution on Train Dataset', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-0.3, 4.7, 'Continuous features have multimodal', fontsize=13, fontweight='light', fontfamily='serif')        \n\nrun_no = 0\nfor col in cont_features:\n    sns.kdeplot(train[col], ax=locals()[\"ax\"+str(run_no)], shade=True, color='#2f5586', edgecolor='black', linewidth=1.5, alpha=0.9, zorder=3)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n    locals()[\"ax\"+str(run_no)].set_ylabel(col, fontsize=10, fontweight='bold').set_rotation(0)\n    locals()[\"ax\"+str(run_no)].yaxis.set_label_coords(1, 0)\n    locals()[\"ax\"+str(run_no)].set_xlim(-0.2, 1.2)\n    locals()[\"ax\"+str(run_no)].set_xlabel('')\n    run_no += 1\n    \nax14.remove()\nax15.remove()","78322115":"train[cont_features].describe() # to view some basic statistical details of the continuous features","ec5d0924":"fig = plt.figure(figsize=(10, 3.5), facecolor='#f6f5f5')\ngs = fig.add_gridspec(1, 1)\ngs.update(wspace=0.2, hspace=0.05)\n\nbackground_color = \"#f6f5f5\"\n\nax0 = fig.add_subplot(gs[0, 0])\nax0.set_facecolor(background_color)\nax0.set_yticklabels([])\nax0.tick_params(axis='y', which=u'both',length=0)\nfor s in [\"top\",\"right\", 'left']:\n    ax0.spines[s].set_visible(False)\n\nax0.text(-0.5, 0.5, 'Target Distribution on Train Dataset', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-0.5, 0.46, 'Target has a bimodal distribution', fontsize=15, fontweight='light', fontfamily='serif')        \n\nsns.kdeplot(train['target'], ax=ax0, shade=True, color='#2f5586', edgecolor='black', linewidth=1.5, alpha=0.9, zorder=3)\nax0.grid(which='major', axis='x', zorder=0, color='gray', linestyle=':', dashes=(1,5))\nax0.set_xlim(-0.5, 10.5)\nax0.set_xlabel('')\nax0.set_ylabel('')\n\nplt.show()","eed305bd":"print('Target')\ntrain['target'].describe()","d1de2992":"fig = plt.figure(figsize=(15, 10), facecolor='#f6f5f5')\ngs = fig.add_gridspec(4, 4)\ngs.update(wspace=0.2, hspace=0.05)\n\nbackground_color = \"#f6f5f5\"\n\nrun_no = 0\nfor col in range(0, 4):\n    for row in range(0, 4):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        locals()[\"ax\"+str(run_no)].set_yticklabels([])\n        locals()[\"ax\"+str(run_no)].tick_params(axis='y', which=u'both',length=0)\n        for s in [\"top\",\"right\", 'left']:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nax0.text(-0.3, 5.3, 'Continuous Features Distribution on Test Dataset', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-0.3, 4.7, 'Continuous features on test dataset resemble train dataset', fontsize=13, fontweight='light', fontfamily='serif')        \n\nrun_no = 0\nfor col in cont_features:\n    sns.kdeplot(test[col], ax=locals()[\"ax\"+str(run_no)], shade=True, color='#2f5586', edgecolor='black', linewidth=1.5, alpha=0.9, zorder=3)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n    locals()[\"ax\"+str(run_no)].set_ylabel(col, fontsize=10, fontweight='bold').set_rotation(0)\n    locals()[\"ax\"+str(run_no)].yaxis.set_label_coords(1, 0)\n    locals()[\"ax\"+str(run_no)].set_xlim(-0.2, 1.2)\n    locals()[\"ax\"+str(run_no)].set_xlabel('')\n    run_no += 1\n    \nax14.remove()\nax15.remove()","686d0827":"test[cont_features].describe()","14aa8e94":"background_color = \"#f6f5f5\"\n\nfig = plt.figure(figsize=(25, 8), facecolor=background_color)\ngs = fig.add_gridspec(2, 5)\ngs.update(wspace=0.2, hspace=0.2)\n\nrun_no = 0\nfor row in range(0, 2):\n    for col in range(0, 5):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\", 'left']:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nax0.text(-0.8, 115, 'Count of categorical features on Train dataset (%)', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-0.8, 107, 'Some features are dominated by one category', fontsize=13, fontweight='light', fontfamily='serif')        \n\nrun_no = 0\nfor col in cat_features:\n    chart_df = pd.DataFrame(train[col].value_counts() \/ len(train) * 100)\n    sns.barplot(x=chart_df.index, y=chart_df[col], ax=locals()[\"ax\"+str(run_no)], color='#2f5586', zorder=3, edgecolor='black', linewidth=1.5)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n    run_no += 1","8411be6d":"background_color = \"#f6f5f5\"\n\nfig = plt.figure(figsize=(25, 8), facecolor=background_color)\ngs = fig.add_gridspec(2, 5)\ngs.update(wspace=0.2, hspace=0.2)\n\nrun_no = 0\nfor row in range(0, 2):\n    for col in range(0, 5):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\", 'left']:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nax0.text(-0.8, 109, 'Count of categorical features on Test dataset (%)', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-0.8, 101, 'Some features are dominated by one category', fontsize=13, fontweight='light', fontfamily='serif')        \n\nrun_no = 0\nfor col in cat_features:\n    chart_df = pd.DataFrame(test[col].value_counts() \/ len(test) * 100)\n    sns.barplot(x=chart_df.index, y=chart_df[col], ax=locals()[\"ax\"+str(run_no)], color='#2f5586', zorder=3, edgecolor='black', linewidth=1.5)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n    run_no += 1","952c46df":"background_color = \"#f6f5f5\"\n\nfig = plt.figure(figsize=(18, 8), facecolor=background_color)\ngs = fig.add_gridspec(1, 2)\nax0 = fig.add_subplot(gs[0, 0])\nax1 = fig.add_subplot(gs[0, 1])\ncolors = [\"#2f5586\", \"#f6f5f5\",\"#2f5586\"]\ncolormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n\nax0.set_facecolor(background_color)\nax0.text(0, -1, 'Features Correlation on Train Dataset', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(0, -0.4, 'Highest correlation in the dataset is 0.6', fontsize=13, fontweight='light', fontfamily='serif')\n\nax1.set_facecolor(background_color)\nax1.text(-0.1, -1, 'Features Correlation on Test Dataset', fontsize=20, fontweight='bold', fontfamily='serif')\nax1.text(-0.1, -0.4, 'Features in test dataset resemble features in train dataset ', \n         fontsize=13, fontweight='light', fontfamily='serif')\n\nsns.heatmap(train[cont_features].corr(), ax=ax0, vmin=-1, vmax=1, annot=True, square=True, \n            cbar_kws={\"orientation\": \"horizontal\"}, cbar=False, cmap=colormap, fmt='.1g')\n\nsns.heatmap(test[cont_features].corr(), ax=ax1, vmin=-1, vmax=1, annot=True, square=True, \n            cbar_kws={\"orientation\": \"horizontal\"}, cbar=False, cmap=colormap, fmt='.1g')\n\nplt.show()","318185d6":"all_data = pd.concat([train, test])\n\nfig, ax = plt.subplots(5, 3, figsize=(14, 24))\nfor i, feature in enumerate(cont_features):\n    plt.subplot(5, 3, i+1)\n    sns.histplot(all_data[feature][::100], \n                 color=\"blue\", \n                 kde=True, \n                 bins=100)\n    plt.xlabel(feature, fontsize=9)\nplt.show()","1e63aa02":"fig, ax = plt.subplots(5, 3, figsize=(24, 30))\nfor i, feature in enumerate(cont_features):\n    plt.subplot(5, 3, i+1)\n    sns.scatterplot(x=feature, \n                    y=\"target\", \n                    data=train[::150], \n                    palette='muted')\n    plt.xlabel(feature, fontsize=9)\nplt.show()","e1c8b25f":"# Separate target from features\ny = train['target']\nfeatures = train.drop(['target'], axis=1)\n\n# List of features for later use\nfeature_list = list(features.columns)\n\n# Preview features\nfeatures.head()","cfae4b1d":"# List of categorical columns\nobject_cols = [col for col in features.columns if 'cat' in col]\n\n# ordinal-encode categorical columns\nX = features.copy()\nX_test = test.copy()\n\n# ordinal encode input variables\nordinal_encoder = OrdinalEncoder()\nX[object_cols] = ordinal_encoder.fit_transform(features[object_cols])\nX_test[object_cols] = ordinal_encoder.transform(test[object_cols])\n\n# Preview the ordinal-encoded features\nX.head()","60b61663":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=42, test_size=0.2)","da8ede45":"# Define the model \nmodel_rf = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=30,\n           max_features='sqrt', max_leaf_nodes=None,\n           min_impurity_decrease=0.0, min_impurity_split=None,\n           min_samples_leaf=2, min_samples_split=5,\n           min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n           oob_score=False, random_state=None, verbose=0, warm_start=False)\n# Train the model\nmodel_rf.fit(X_train, y_train)\npreds_valid = model_rf.predict(X_valid)\nprint(mean_squared_error(y_valid, preds_valid, squared=False))","13b2d3c5":"plt.rcParams[\"axes.labelsize\"] = 12\nrf_prob_train = model_rf.predict(X_train) - y_train\nplt.figure(figsize=(6,6))\nsp.stats.probplot(rf_prob_train, plot=plt, fit=True)\nplt.title('Train Probability Plot for Random Forest', fontsize=10)\nplt.show()\n\nrf_prob_test = model_rf.predict(X_valid) - y_valid\nplt.figure(figsize=(6,6))\nsp.stats.probplot(rf_prob_test, plot=plt, fit=True)\nplt.title('Test Probability Plot for Random Forest', fontsize=10)\nplt.show()","558bf34d":"# Get numerical feature importances\nimportances = list(model_rf.feature_importances_)\n\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n\n# Print out the feature and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]","84ea992d":"# list of x locations for plotting\nx_values = list(range(len(importances)))\n# Make a bar chart\nplt.bar(x_values, importances, orientation = 'vertical', color = 'r', edgecolor = 'k', linewidth = 1.2)\n# Tick labels for x axis\nplt.xticks(x_values, feature_list, rotation='vertical')\n# Axis labels and title\nplt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');","061b9dd5":"# List of features sorted from most to least important\nsorted_importances = [importance[1] for importance in feature_importances]\nsorted_features = [importance[0] for importance in feature_importances]\n# Cumulative importances\ncumulative_importances = np.cumsum(sorted_importances)\n# Make a line graph\nplt.plot(x_values, cumulative_importances, 'g-')\n# Draw line at 95% of importance retained\nplt.hlines(y = 0.95, xmin=0, xmax=len(sorted_importances), color = 'r', linestyles = 'dashed')\n# Format x ticks and labels\nplt.xticks(x_values, sorted_features, rotation = 'vertical')\n# Axis labels and title\nplt.xlabel('Variable'); plt.ylabel('Cumulative Importance'); plt.title('Cumulative Importances');","389ecffc":"# Find number of features for cumulative importance of 95%\n# Add 1 because Python is zero-indexed\nprint('Number of features for 95% importance:', np.where(cumulative_importances > 0.95)[0][0] + 1)","8c6d9d50":"# Extract the names of the most important features\nimportant_feature_names = [feature[0] for feature in feature_importances[0:17]]\n# Create training and testing sets with only the important features\nimportant_train_features = X_train[important_feature_names]\nimportant_test_features = X_valid[important_feature_names]\n# Sanity check on operations\nprint('Important train features shape:', important_train_features.shape)\nprint('Important test features shape:', important_test_features.shape)","14f025fe":"# Train the expanded model on only the important features\nmodel_rf.fit(important_train_features, y_train);\n# Make predictions on test data\nnew_predictions = model_rf.predict(important_test_features)\n# Performance metrics\nerrors = mean_squared_error(y_valid, new_predictions, squared=False)\nprint('RMSE:', errors)\n# Calculate rmse\nrmse = np.mean(100 * (errors \/ y_valid))\n# Calculate and display accuracy\naccuracy = 100 - rmse\nprint('Accuracy:', round(accuracy, 2), '%.')","af9fe3bd":"# Train the model first on the original features\nmodel_rf.fit(X_train, y_train);\n# Make predictions on test data\npredictions = model_rf.predict(X_valid)\n# Performance metrics\nerrors = mean_squared_error(y_valid, predictions, squared=False)\nprint('Metrics for Random Forest Trained on original Data')\nprint('RMSE:', errors)\n# Calculate rmse\nrmse_new = np.mean(100 * (errors \/ y_valid))\n# Calculate and display accuracy\naccuracy = 100 - rmse_new\nprint('Accuracy:', round(accuracy, 2), '%.')","5d7f3d40":"pd.Series(model_rf.feature_importances_, index = X_train.columns).nlargest(10).plot(kind = 'barh',\n                                                                               figsize = (6, 6),\n                                                                              title = 'Feature importance from Random Forest').invert_yaxis();","f22c7066":"xgb_params = {'objective': 'reg:squarederror',\n              'n_estimators': 10000,\n              'learning_rate': 0.036,\n              'subsample': 0.926,\n              'colsample_bytree': 0.118,\n              'grow_policy':'lossguide',\n              'max_depth': 3,\n              'booster': 'gbtree', \n              'reg_lambda': 45.1,\n              'reg_alpha': 34.9,\n              'random_state': 42,\n              'reg_lambda': 0.00087,\n              'reg_alpha': 23.132,\n              'n_jobs': 4}\n\nmodel_XGB = XGBRegressor(**xgb_params)\nmodel_XGB.fit(X_train, y_train) \npredictions_XGB = model_XGB.predict(X_valid)\nprint(mean_squared_error(y_valid, predictions_XGB, squared=False))","42ff9e56":"plt.rcParams[\"axes.labelsize\"] = 12\nxgb_prob_train = model_XGB.predict(X_train) - y_train\nplt.figure(figsize=(6,6))\nsp.stats.probplot(xgb_prob_train, plot=plt, fit=True)\nplt.title('Train Probability Plot for XGBoost', fontsize=10)\nplt.show()\n\nxgb_prob_test = model_XGB.predict(X_valid) - y_valid\nplt.figure(figsize=(6,6))\nsp.stats.probplot(xgb_prob_test, plot=plt, fit=True)\nplt.title('Test Probability Plot for XGBoost', fontsize=10)\nplt.show()","e1598899":"pd.Series(model_XGB.feature_importances_, index = X_train.columns).nlargest(10).plot(kind = 'barh',\n                                                                               figsize = (6, 6),\n                                                                              title = 'Feature importance from XGBoost').invert_yaxis();\n","83c6ae3e":"xgb_1_params = {\n    'n_estimators': 5000,\n    'booster': 'gbtree', \n    'random_state':40,\n    'learning_rate': 0.078,\n    'reg_lambda': 1.75492e-05,\n    'reg_alpha': 14.682, \n    'subsample': 0.803, \n    'colsample_bytree': 0.170, \n    'max_depth': 3\n    }\n\nmodel_XGB_1 = XGBRegressor(**xgb_1_params)\nmodel_XGB_1.fit(X_train, y_train) \npredictions_XGB_1 = model_XGB_1.predict(X_valid)\nprint(mean_squared_error(y_valid, predictions_XGB_1, squared=False))","92a67976":"xgb_2_params = {\n    'objective': 'reg:squarederror',\n    'n_estimators': 5000,\n    'learning_rate': 0.12,\n    'subsample': 0.96,\n    'colsample_bytree': 0.12,\n    'max_depth': 2,\n    'booster': 'gbtree', \n    'reg_lambda': 65.1,\n    'reg_alpha': 15.9,\n    'random_state':40\n}\n\nmodel_XGB_2 = XGBRegressor(**xgb_2_params)\nmodel_XGB_2.fit(X_train, y_train) \npredictions_XGB_2 = model_XGB_2.predict(X_valid)\nprint(mean_squared_error(y_valid, predictions_XGB_2, squared=False))","6db4ecec":"xgb_3_params = {\n    'objective': 'reg:squarederror',\n    \"learning_rate\" : 0.05,\n    'n_estimators': 5000,\n    \"max_depth\":12,\n    \"min_child_weight\" :110,\n    \"gamma\" :0.01,\n    'booster': 'gbtree', \n    \"subsample\" : 0.7,\n    \"colsample_bytree\" : 0.1,\n    \"reg_lambda\" :65,\n    \"reg_alpha\":71,\n    \"max_delta_step\":10}\n\nmodel_XGB_3 = XGBRegressor(**xgb_3_params)\nmodel_XGB_3.fit(X_train, y_train) \npredictions_XGB_3 = model_XGB_3.predict(X_valid)\nprint(mean_squared_error(y_valid, predictions_XGB_3, squared=False))","0280a3b6":"xgb_4_params = {\n    'random_state': 1, \n    'n_jobs': 4,\n    'booster': 'gbtree',\n    'n_estimators': 10000,\n    'learning_rate': 0.0362,\n    'reg_lambda': 0.000874,\n    'reg_alpha': 23.131,\n    'subsample': 0.787,\n    'colsample_bytree': 0.118,\n    'max_depth': 3}\n\nmodel_XGB_4 = XGBRegressor(**xgb_4_params)\nmodel_XGB_4.fit(X_train, y_train) \npredictions_XGB_4 = model_XGB_4.predict(X_valid)\nprint(mean_squared_error(y_valid, predictions_XGB_4, squared=False))","bb32c65d":"xgb_5_params = {'learning_rate': 0.0785, \n          'reg_lambda': 1.754929e-05, \n          'reg_alpha': 14.682, \n          'subsample': 0.803, \n          'colsample_bytree': 0.170, \n          'max_depth': 3,\n          'n_estimators': 5000\n         }\nmodel_XGB_5 = XGBRegressor(**xgb_5_params)\nmodel_XGB_5.fit(X_train, y_train) \npredictions_XGB_5 = model_XGB_5.predict(X_valid)\nprint(mean_squared_error(y_valid, predictions_XGB_5, squared=False))","e0280b06":"lgbm_parameters = {\n    'metric': 'RMSE',\n    'feature_pre_filter': False,\n    'lambda_l1': 0.45,\n    'lambda_l2': 4.8,\n    'learning_rate': 0.005,\n    'num_trees': 80000,\n    'num_leaves': 10, \n    'feature_fraction': 0.4, \n    'bagging_fraction': 1.0, \n    'bagging_freq': 0, \n    'min_child_samples': 100,\n    'num_threads': 4\n}\n\nlgbm_model = LGBMRegressor(**lgbm_parameters)\nlgbm_model.fit(X_train, y_train, eval_set = ((X_valid,y_valid)),verbose = -1, early_stopping_rounds = 1000,categorical_feature=object_cols) \npredictions_LGBM = lgbm_model.predict(X_valid)\nprint(mean_squared_error(y_valid, predictions_LGBM, squared=False))","488fbb9c":"plt.rcParams[\"axes.labelsize\"] = 12\nlgbm_prob_train = lgbm_model.predict(X_train) - y_train\nplt.figure(figsize=(6,6))\nsp.stats.probplot(lgbm_prob_train, plot=plt, fit=True)\nplt.title('Train Probability Plot for LGBM', fontsize=10)\nplt.show()\n\nlgbm_prob_test = lgbm_model.predict(X_valid) - y_valid\nplt.figure(figsize=(6,6))\nsp.stats.probplot(lgbm_prob_test, plot=plt, fit=True)\nplt.title('Test Probability Plot for LGBM', fontsize=10)\nplt.show()","12f004f2":"pd.Series(lgbm_model.feature_importances_, index = X_train.columns).nlargest(10).plot(kind = 'barh',\n                                                                               figsize = (6, 6),\n                                                                              title = 'Feature importance from LGBM').invert_yaxis();\n","e8d3d011":"lgbm_parameters_1 = {\n    'metric': 'RMSE',\n    'feature_pre_filter': False,\n    'reg_alpha': 0.497, \n    'reg_lambda': 0.327, \n    'num_leaves': 50, \n    'learning_rate': 0.032,                      \n    'max_depth': 40,                     \n    'n_estimators': 4060, \n    'min_child_weight': 0.0173,\n    'subsample': 0.949, \n    'colsample_bytree': 0.532, \n    'min_child_samples': 80\n}\n\nlgbm_model_1 = LGBMRegressor(**lgbm_parameters_1)\nlgbm_model_1.fit(X_train, y_train, eval_set = ((X_valid,y_valid)),verbose = -1, early_stopping_rounds = 1000,categorical_feature=object_cols) \npredictions_LGBM_1 = lgbm_model_1.predict(X_valid)\nprint(mean_squared_error(y_valid, predictions_LGBM_1, squared=False))","189ce5a6":"lgbm_parameters_2 = {\n    'metric': 'RMSE',\n    'feature_pre_filter': False,\n    'reg_alpha': 0.499, \n    'reg_lambda': 0.324, \n    'num_leaves': 55, \n    'learning_rate': 0.0329, \n    'max_depth': 32, \n    'n_estimators': 6059, \n    'min_child_weight': 0.01808, \n    'subsample': 0.955, \n    'colsample_bytree': 0.525, \n    'min_child_samples': 77\n}\n\nlgbm_model_2 = LGBMRegressor(**lgbm_parameters_2)\nlgbm_model_2.fit(X_train, y_train, eval_set = ((X_valid,y_valid)),verbose = -1, early_stopping_rounds = 2000,categorical_feature=object_cols) \npredictions_LGBM_2 = lgbm_model_2.predict(X_valid)\nprint(mean_squared_error(y_valid, predictions_LGBM_2, squared=False))","73a1c586":"cat_parameters_1 = {    \n    'iterations':1600,\n    'learning_rate':0.024,\n    'l2_leaf_reg':20,\n    'random_strength':1.5,\n    'grow_policy':'Depthwise',\n    'leaf_estimation_method':'Newton', \n    'bootstrap_type':'Bernoulli',\n    'thread_count':4,\n    'verbose':False,\n    'loss_function':'RMSE',\n    'eval_metric':'RMSE',\n    'od_type':'Iter'\n}\n\ncat_model_1 = CatBoostRegressor(**cat_parameters_1)\ncat_model_1.fit(X_train, y_train, verbose =200) \npredictions_cat_1 = cat_model_1.predict(X_valid)\nprint(mean_squared_error(y_valid, predictions_cat_1, squared=False))","e356df9b":"pd.Series(cat_model_1.feature_importances_, index = X_train.columns).nlargest(10).plot(kind = 'barh',\n                                                                               figsize = (6, 6),\n                                                                              title = 'Feature importance from LGBM').invert_yaxis();\n","7c53cdd9":"a1 = model_rf.feature_importances_\na2 = model_XGB.feature_importances_\na3 = lgbm_model.feature_importances_\na4 = cat_model_1.feature_importances_\n\naxis_x  = X.columns.values\naxis_y1 = minmax_scaling(a1, columns=[0])\naxis_y2 = minmax_scaling(a2, columns=[0])\naxis_y3 = minmax_scaling(a3, columns=[0])\naxis_y4 = minmax_scaling(a4, columns=[0])\n\nplt.style.use('seaborn-whitegrid') \nplt.figure(figsize=(16, 6), facecolor='lightgray')\nplt.title(f'\\nF e a t u r e   I m p o r t a n c e s\\n', fontsize=14)  \n\nplt.scatter(axis_x, axis_y1, s=20, label='Random Forest') \nplt.scatter(axis_x, axis_y2, s=20, label='XGBoost')\nplt.scatter(axis_x, axis_y3, s=20, label='Light GBM') \nplt.scatter(axis_x, axis_y4, s=20, label='CatBoost')\n\nplt.legend(fontsize=12, loc=2)\nplt.show()","295bc53e":"# Use the models to generate predictions\npred_1 = model_rf.predict(X_test)\npred_2 = model_XGB.predict(X_test)\npred_3 = model_XGB_1.predict(X_test)\npred_4 = model_XGB_2.predict(X_test)\npred_5 = model_XGB_3.predict(X_test)\npred_6 = model_XGB_4.predict(X_test)\npred_7 = model_XGB_5.predict(X_test)\npred_8 = lgbm_model.predict(X_test)\npred_9 = lgbm_model_1.predict(X_test)\npred_10= lgbm_model_2.predict(X_test)\npred_11 = cat_model_1.predict(X_test)\n\n# Make sure to check that the weights sum up to ~1 by averaging the weights later \npreds = [pred_1, pred_2, pred_3, pred_4, pred_5, pred_6, pred_7, pred_8, pred_9, pred_10, pred_11]\nweights = [0., 1000., 100., 50., 500., 100., 10., 9., 5., 1., 1.] \n\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\nsample_submission.target = 0.0\n\nfor pred, weight in zip(preds, weights):\n    sample_submission.target += weight * pred \/ sum(weights)\n\nsample_submission.to_csv('submission.csv', index=False)","3d5990ab":"sample_submission.head()","bacb0a88":"from sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression(normalize=True)\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_valid)\nprint(mean_squared_error(y_valid, predictions, squared=False))","23fbafc2":"from sklearn.linear_model import Ridge\n\nmodel = Ridge(alpha=100, solver='cholesky', tol=0.0001, random_state=42)\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_valid)\nprint(mean_squared_error(y_valid, predictions, squared=False))","acbf1a5b":"from sklearn.linear_model import Lasso\n\nmodel = Lasso(alpha=0.1, \n              precompute=True, \n              positive=True, \n              selection='random',\n              random_state=42)\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_valid)\nprint(mean_squared_error(y_valid, predictions, squared=False))","843ca8cb":"from sklearn.linear_model import ElasticNet\n\nmodel = ElasticNet(alpha=0.1, l1_ratio=0.9, selection='random', random_state=42)\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_valid)\nprint(mean_squared_error(y_valid, predictions, squared=False))","b58fc4f3":"from sklearn.linear_model import SGDRegressor\n\nmodel = SGDRegressor(n_iter_no_change=250, penalty=None, eta0=0.0001, max_iter=100000)\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_valid)\nprint(mean_squared_error(y_valid, predictions, squared=False))","584f7454":"## \u2714 5.3: LightGBM\n\nLight GBM is a fast, distributed, high-performance gradient boosting framework based on decision tree algorithm, used for ranking, classification and many other machine learning tasks. Since it is based on decision tree algorithms, it splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms. Also, it is surprisingly very fast, hence the word \u2018Light\u2019.\n\n![LightGBM](https:\/\/miro.medium.com\/max\/1400\/1*mKkwlQF25Rq1ilne5UiEXA.png)\n\n#### Advantages of Light GBM\n- Faster training speed and higher efficiency: Light GBM use histogram based algorithm i.e it buckets continuous feature values into discrete bins which fasten the training procedure.\n- Lower memory usage: Replaces continuous values to discrete bins which result in lower memory usage.\n- Better accuracy than any other boosting algorithm: It produces much more complex trees by following leaf wise split approach rather than a level-wise approach which is the main factor in achieving higher accuracy. However, it can sometimes lead to overfitting which can be avoided by setting the max_depth parameter.\n- Compatibility with Large Datasets: It is capable of performing equally good with large datasets with a significant reduction in training time as compared to XGBOOST.\n- Parallel learning supported.","53f1ff12":"#### Why are we using only one of the models of each regressor to check the most important features for those models?\nThat's because the feature importances depend on the models, not on the parameters. We can check with any variations of the parameters for each of the regressors.\n\n# Step 7: Submit to the competition\n\nWe'll begin by using the trained model to generate predictions, which we'll save to a CSV file.\n\n### But which model to use for prediction?\n\nMaybe we should try to use all of the models we trained! Here is a fun way to put weightage to the predictions of all the models to get the final outcome. \n\n*NOTE:* Remember to put more weightage on the models with lesser RMSE value to get the best result. \n\n### Ensemble\nTry ensembling. Using model blending weights optimisation technique similar to the one used in [this](https:\/\/www.kaggle.com\/gogo827jz\/optimise-blending-weights-with-bonus-0) notebook, we have tried it on our models. Ensembling different models can be necessary to improve scores.","cd8e1a31":"### Observations:\n\n- Train set has 300,000 rows while test set has 200,000 rows.\n- There are 10 categorical features from `cat0` - `cat9` and 14 continuous features from `cont0` - `cont13`.\n- There is no missing values in the train and test dataset but there is no category `G` in `cat6` test dataset.\n- Categorical features ranging from alphabet A - O but it varies from each categorical feature with `cat0`, `cat1`, `cat3`, `cat5` and `cat6` are dominated by one category.\n- Continuous features on train anda test dataset ranging from -0.1 to 1.25 which are a multimodal distribution and they resemble each other.\n- target has a range between 6.8 to 10.5 and has a bimodal distribution.\n\n\nIdeas:\n\nDrop features that are dominated by one category cat0, cat1, cat3, cat5 and cat6 as they don't give variation to the dataset but further analysis still be needed.","b5f30981":"## Thank you so much for reading! Please do upvote if you liked it. :)","e1f7da73":"# Step 1: Import helpful libraries","ec8c3ba2":"We can also prepare the target in the same manner. Ordinal encoding of target variable uses LabelEncoder(). But we do not use it here since it is used to normalize labels, and to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels. Since our target is already numeric, we do not need it here.\n\nNext, we break off a validation set from the training data.","0ed07534":"## \u2714 5.4: CatBoost\nCatBoost is an algorithm for gradient boosting on decision trees. And it is the only boosting algorithm with very less prediction time. Because of its symmetric tree structure. It is comparatively 8x faster than XGBoost while predicting. One main difference between CatBoost and other boosting algorithms is that the CatBoost implements symmetric trees. Though when datasets have many numerical features (like this one), CatBoost takes so much time to train than Light GBM.\n\nBut CatBoost also offers an idiosyncratic way of handling categorical data, requiring a minimum of categorical feature transformation, opposed to the majority of other machine learning algorithms, that cannot handle non-numeric values. From a feature engineering perspective, the transformation from a non-numeric state to numeric values can be a very non-trivial and tedious task, and CatBoost makes this step obsolete. (Though here, we had used feature engineering for conducting training in all the models)\n\n![CatBoost](https:\/\/www.kdnuggets.com\/wp-content\/uploads\/mwiti-catboost-0.png)","d65a9660":"Consequently, we can also make a cumulative importance graph that shows the contribution to the overall importance of each additional variable. The dashed line is drawn at 95% of total importance accounted for.","b36c50e0":"This is just an instance of feature selection with one of our models. But since we do not see any improvement, we move forward without the selected features for our dataset.\n\nLet's check the feature importances for the model. ","e7e0cf16":"Now with the original features. _(We are re-training it back to fit the original features)_","029e0860":"# Step 3: Features and target correlation\n\n### Basic statistics on continuous features\n\n#### Train dataset","6a9d998e":"### Count of categorical features","f933a6b9":"# Step 6: Feature importances\nNow, let's compare all of the models together along with their feature importances in the following graph.","6c82ec00":"As we can see, it gives a better result than XGBoost. LightGBM, apart from being more accurate and time-saving than XGBOOST has been limited in usage due to less documentation available. However, this algorithm has shown far better results and has outperformed existing boosting algorithms.\n\n![Comparison](https:\/\/image.slidesharecdn.com\/xgboostandlightgbm-180201121028\/95\/xgboost-lightgbm-21-638.jpg?cb=1517487076)\n\nFollowing are the probability plot of LGBM and the feature importance for the model.","ad98ed38":"These stats definitely prove that some variables are much more important to our problem than others! Given that there are so many variables with zero importance (or near-zero due to rounding). But it seems like we should be able to get rid of some of them without impacting performance.\n\nThe following graph represents the relative differences in feature importances.","143bd6aa":"#### Learning more about our data","dce70e2d":"But when the Random Forest Regressor is tasked with the problem of predicting for values not previously seen, it will always predict an average of the values seen previously. Meaning, that the Random Forest Regressor is unable to discover trends that would enable it in extrapolating values that fall outside the training set. \n\nThe random forest performs implicit feature selection because it splits nodes on the most important variables, but other machine learning models do not. One approach to improve the models is therefore to use the random forest feature importances to reduce the number of variables in the problem. In our case, we will use the feature importances to decrease the number of features for our random forest model, because, in addition to potentially increasing performance, reducing the number of features will shorten the run time of the model. Let's see if it improves the accuracy of our model.","36c37bd1":"Training with varied parameters of LGBM model.","707940e6":"# Step 4: Prepare the data\n\nThe next code cell separates the target (which we assign to `y`) from the training features (which we assign to `features`).","00f46f80":"#### Number of missing values","3d1a2243":"We see that almost all of our features are important. But we can check that further by creating a new training and testing set retaining only the 18 most important features, and using the model on it.","bc337386":"### Features Correlation\n\nObservations:\n\n- Highest correlation between features is 0.5.\n- Correlation between features on train and test dataset are quite similar.","a75ad3ea":"# Step 2: Load the data\n\nNext, we'll load the training and test data.  \n\nWe set `index_col=0` in the code cell below to use the `id` column to index the DataFrame.  (*If you're not sure how this works, try temporarily removing `index_col=0` and see how it changes the result.*)","b2b5272e":"Now, let's train our dataset in XFBoost with other different parameters.","8101ea55":"# Step 5: Train a model\n\nNow that the data is prepared, the next step is to train a model.  \n\n## \u2714 5.1: Random Forest\n\nFrom the lesson **[Random Forests](https:\/\/www.kaggle.com\/dansbecker\/random-forests)**, we learnt how to fit a random forest model to the data.\n\n#### What is Random Forest, and why are we using it here?\n\nRandom forest is a type of supervised learning algorithm that uses ensemble methods (bagging) to solve both regression and classification problems. The algorithm operates by constructing a multitude of decision trees at training time and outputting the mean\/mode of prediction of the individual trees.\n\n- Each tree is created from a different sample of rows and at each node, a different sample of features is selected for splitting. \n- Each of the trees makes its own individual prediction. \n- These predictions are then averaged to produce a single result. \n\n![Random Forest](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/7\/76\/Random_forest_diagram_complete.png)\n\nThe averaging makes a Random Forest better than a single Decision Tree hence improves its accuracy and reduces overfitting. \n\nA prediction from the Random Forest Regressor is an average of the predictions produced by the trees in the forest. ","64811256":"In the code cell above, we set `squared=False` to get the root mean squared error (RMSE) on the validation data.","9f0b4ae5":"# Step 8: Extras\n\nA few of the linear regression models were further used for experimentation:\n\n- **Linear Regression** : Linear regression is a linear model, e.g. a model that assumes a linear relationship between the input variables (x) and the single output variable (y). More specifically, that y can be calculated from a linear combination of the input variables (x).\n- **Ridge Regression** : Ridge regression is a model tuning method that is used to analyse any data that suffers from multicollinearity. This method performs L2 regularization. \n- **LASSO Regression** : Lasso regression is a regularization technique. It is used over regression methods for a more accurate prediction. This model uses shrinkage. Shrinkage is where data values are shrunk towards a central point as the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters).\n- **Elastic Net** : It is a linear regression model trained with L1 and L2 prior as regularizer. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge.\n- **Stochastic Gradient Descent** : Stochastic gradient descent is an optimization algorithm often used in machine learning applications to find the model parameters that correspond to the best fit between predicted and actual outputs.\n\nBut the RMSE scores from these were much higher compared to the previous models used in this notebook, so it was not used for the final submission. ","781125c8":"Now, we'll need to handle the categorical columns (`cat0`, `cat1`, ... `cat9`).  \n\nFrom the lesson **[Categorical Variables lesson](https:\/\/www.kaggle.com\/alexisbcook\/categorical-variables)**, we'll use ordinal encoding and save our encoded features as new variables `X` and `X_test`.\n\n#### What is Ordinal Encoder and why we choose it?\nIn ordinal encoding, each unique category value is assigned an integer value. The encoding involves mapping each unique label to an integer value.\nFor example, `red` is 1, `green` is 2, and `blue` is 3.\n\nThis is called an ordinal encoding or an integer encoding and is easily reversible. Often, integer values starting at zero are used. The integer values have a natural ordered relationship between each other and machine learning algorithms may be able to understand and harness this relationship.\n\nThere are three common approaches for converting categorical variables to numerical values. They are:\n\n- Ordinal Encoding\n- One-Hot Encoding\n- Dummy Variable Encoding\n\nBut in here, we are using Ordinal because our dataset is naturally in order, alphabetically. Hence, when we encode it, the converted numerics consequently take a natural integral order like in its original form. Take a note of that in the following table. \n\n_Eg: 'A' -> 0.0, 'B' -> 1.0, 'C' -> 2.0,, and so on..._\n\nBut for categorical variables, where no such ordinal relationship exist, it may be advisable to use the other two. Nevertheless, for this dataset, ordinal encoding will suffice. But you can definitely use other encoding approaches as well.","e5779b67":"We can now use this to remove unimportant features. 95% is an arbitrary threshold, but if it leads to noticeably poor performance we can adjust the value. First, we need to find the exact number of features to exceed 95% importance:","6f072418":"Following the same pattern, we try it with on other models.\n\n## \u2714 5.2: XGBoost\n\nFrom the lesson **[XGBoost](https:\/\/www.kaggle.com\/alexisbcook\/xgboost)**, we learnt how to fit a XGBoost model to the data.\n\n#### What is XGBoost?\nXGBoost is termed as Extreme Gradient Boosting Algorithm which is again an ensemble method that works by boosting trees. XGboost makes use of a gradient descent algorithm which is the reason that it is called Gradient Boosting. The whole idea is to correct the previous mistake done by the model, learn from it and its next step improves the performance. The previous results are rectified and performance is enhanced.\n\n![XGBoost Regressor](https:\/\/miro.medium.com\/max\/1400\/1*FLshv-wVDfu-i54OqvZdHg.png)\n\n#### Why is it preferred?\n- Speed and perfoermance: Comparatively faster than other ensemble classifiers\n- Core algorithm is parallelizable: Because the core XGBoost algorithm is parallelizable it can harness the power of multi-core computers. It is also parallelizable onto GPU\u2019s and across networks of computers making it feasible to train on very large datasets as well.\n- Consistently outperforms other algorithm methods : It has shown better performance on a variety of machine learning benchmark datasets.\n- Wide variety of tuning parameters : XGBoost internally has parameters for cross-validation, regularization, user-defined objective functions, missing values, tree parameters, scikit-learn compatible API etc.\n\n","4bd96738":"Comparing the feature importances as before, we see for XGBoost, the outcome is slightly different than Random Forest.","8f47a289":"<p style=\"text-align:center;\"> <span style=\"font-size:32px;\"> <b> Understanding the data and performing regressions <\/b> <\/span> <\/p>\n\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1024\/1*hLWUW_4ZPjKyP01Bnlyv4w.png\" width=\"700px\">\n\n## Acknowledgements:\n\nFor the purpose of training and selecting the model, I took reference from these following notebooks: [\nGetting Started with 30 Days of ML Competition](https:\/\/www.kaggle.com\/alexisbcook\/getting-started-with-30-days-of-ml-competition) and [\n3rd-place solution: Ensembling GBDTs](https:\/\/www.kaggle.com\/kntyshd\/3rd-place-solution-ensembling-gbdts), and is inspired from the different courses offered by Kaggle. And for performing the EDA on the dataset to understand it better, the following notebook helped me a lot: [TPS Feb 2021 EDA](https:\/\/www.kaggle.com\/dwin183287\/tps-feb-2021-eda\/)\n","814dbaaf":"#### Number of rows and columns","bca11ecf":"### Feature Engineering\n\nFeature-engineering using histograms of the cont features show multiple components. For instance, the `cont1` has 7 discrete peaks as shown below. ","4d1ddd01":"#### Test dataset","f49f84dd":"We first test the accuracy with the selected features."}}