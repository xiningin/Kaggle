{"cell_type":{"62bf85d1":"code","927cdde2":"code","891e58fa":"code","c82afd06":"code","4aba1b4f":"code","7cb848eb":"code","24e8f611":"code","898989c8":"code","e2ca3fa4":"code","9ab5cb05":"code","f424dc7d":"code","5e24e83a":"code","a5126bd5":"code","0ce9643f":"markdown","6fa554e4":"markdown","4369db78":"markdown","17c76317":"markdown","563fac78":"markdown","3dc86dff":"markdown","0aa927ca":"markdown","2d6bae61":"markdown","13aa70f7":"markdown","bc9b7193":"markdown"},"source":{"62bf85d1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n%matplotlib inline\nimport pandas\nimport numpy\nimport gensim\nimport nltk\nimport seaborn\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","927cdde2":"alice = open('\/kaggle\/input\/alice-wonderland-dataset\/alice_in_wonderland.txt').read()\nkeywords = pandas.Series(dict(gensim.summarization.mz_entropy.mz_keywords(alice,scores=True)))\nkeywords.nlargest(20).plot.barh()","891e58fa":"keywords2 = pandas.Series(dict(gensim.summarization.mz_entropy.mz_keywords(alice,weighted=False,scores=True)))\nkeywords2.nlargest(20).plot.barh()","c82afd06":"words = list(gensim.summarization.textcleaner.tokenize_by_word(alice))\nn_blocks = len(words)\/\/1024 + 1\n\nfrequencies = pandas.DataFrame(numpy.zeros((n_blocks,keywords.size)),\n                               columns=keywords.index)\nfor (i,word) in enumerate(words):\n    if word in keywords.index:\n        frequencies.loc[i\/\/1024,word]+=1\nfrequencies\/=frequencies.sum(axis=0)\nseaborn.heatmap(frequencies.loc[:,keywords.nlargest(20).index])","4aba1b4f":"seaborn.heatmap(frequencies.loc[:,keywords2.nlargest(20).index])","7cb848eb":"sent_detector = nltk.data.load('tokenizers\/punkt\/english.pickle')\ndef score_sentence(sentence):\n    return sum((keywords2.get(word,0)\n               for word in gensim.summarization.textcleaner.tokenize_by_word(sentence)))\n\nsentences = pandas.Series({sentence:score_sentence(sentence)\n                          for sentence in sent_detector.tokenize(alice)})\nsentences.nlargest(10)","24e8f611":"def weighted_score_sentence(sentence):\n    return sum((keywords.get(word,0)\n               for word in gensim.summarization.textcleaner.tokenize_by_word(sentence)))\n\nweighted_sentences = pandas.Series({sentence:weighted_score_sentence(sentence)\n                                    for sentence in sent_detector.tokenize(alice)})\nweighted_sentences.nlargest(10)","898989c8":"sentence_lengths = pandas.Series({sentence:len(list(gensim.utils.tokenize(sentence)))\n                                 for sentence in sent_detector.tokenize(alice)})\nscore_vs_length = pandas.DataFrame({'length':sentence_lengths,\n                                   'weighted':weighted_sentences,\n                                   'unweighted':sentences})\nscore_vs_length.plot.scatter(x='length',\n                            y='unweighted')","e2ca3fa4":"score_vs_length.plot.scatter(x='length',\n                            y='weighted')","9ab5cb05":"normalized_unweighted = score_vs_length['unweighted']\/score_vs_length['length']\nnormalized_unweighted.nlargest(10)","f424dc7d":"normalized_weighted = score_vs_length['weighted']\/score_vs_length['length']\nnormalized_weighted.nlargest(10)","5e24e83a":"root_normalized_unweighted = score_vs_length['unweighted']\/score_vs_length['length'].apply(numpy.sqrt)\nroot_normalized_unweighted.nlargest(10)","a5126bd5":"root_normalized_weighted = score_vs_length['weighted']\/score_vs_length['length'].apply(numpy.sqrt)\nroot_normalized_weighted.nlargest(10)","0ce9643f":"One possible disadvantage of this is that it will tend to favour longer sentences. Let's see how the sentence scores vary with the length of the sentence.","6fa554e4":"In both cases we can see that the score is approximately linear in the length of the sentence. Therefore we can normalize the scores.","4369db78":"In this Kernel, we are going to use an information theory based keyword extraction algorithm described by Marcello Montemurro and Damian Zanette [Marcello A Montemurro, Damian Zanette, \u201cTowards the quantification of the semantic information encoded in written language\u201d. Advances in Complex Systems, Volume 13, Issue 2 (2010), pp. 135-153, DOI: 10.1142\/S0219525910002530](https:\/\/arxiv.org\/abs\/0907.1558) to find the most significant information in *Alice in Wonderland*. Lewis Carroll, who was a mathematician, would probably approve.\n\nThe algorithm works by dividing a document into chunks, calcualting the entropy of each word's frequency distiribution amongst those chunks, subtracting the entropy that would be expected for a word of the same overall frequency that was randomly distributed across the document, and then scaling by word frequency. The reasoning is that significant words will contribute to the structure of the document, whereas words such as function words that do not convey significant meaning will have a more uniform distribution. Let's see what the 20 most significant words in *Alice in Wonderland* are.","17c76317":"Now for the unweighted keywords.","563fac78":"You may be wondering, what would happen if we used the frequency-weighted entropies for this?","3dc86dff":"We can see that these are mainly the names of characters. Interestingly, however, they don't incluce Alice - since she's present throughout the book, her name is distributed evenly.\n\nWe can also see that a few function words are present in the top 20. This happens because of the frequency weighting - they're very common words, The reason for the frequency weighting is to quantify the total contribution of a word to the entropy of the document. Howver, it is also useful to quantify the significance of an individual instance of a word. For this purpose, the Gensim implementation of the algorithm includes an option to calculate entropy without frequency weighting.","0aa927ca":"Normalizing the scores in this was isn't very useful - it just favours very short sentences with one or two significant words in them. To reduce the effect of sentence length but not remove it completely, let's try normalizing by the square root of the length.","2d6bae61":"Now let's use the unweighted keywords to find significant sentences.","13aa70f7":"This is a bit less heavily weighted towards character names, and we can see traces of *Twinke Twinkle Little Bat* and *The Lobster Quadrille*.\n\nTo get a better idea of how the algorithm is working, let's have a look at how these keywords are distributed across the document: first for the frequency-weighted keywords","bc9b7193":"[Video Commentary](https:\/\/youtu.be\/zC4ZXvAxnHA)"}}