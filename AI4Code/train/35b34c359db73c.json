{"cell_type":{"f44ff126":"code","da3bfddc":"code","2c0ad617":"code","102261fb":"code","17da8849":"code","f662a419":"code","36dbae01":"code","f0fabbbe":"code","65f122b4":"code","b8ce8de3":"code","43758e4b":"code","7626ad6b":"code","71fdb664":"code","553fad98":"code","9855a9d6":"code","0384fa28":"code","61291f25":"code","0cd8cb90":"code","abe37b28":"code","d9ca0e57":"code","464d41b9":"code","f9e15b46":"code","7649de6c":"code","d0551603":"code","ab8b35ea":"code","9074ab00":"code","069c43ec":"code","aa3d5edd":"code","f57205ca":"code","0d320c75":"code","a4f9a611":"code","88cc648c":"code","b0782195":"code","86bfdded":"code","fa682db0":"code","cabd16c6":"code","f9258c6d":"code","1dd1523b":"code","e88fee07":"code","4c089c5e":"code","83b152e3":"code","c8191277":"code","9455e112":"code","8665202b":"code","d4eef925":"code","60866d03":"code","f585af31":"code","52b36f0d":"code","cc2c591e":"code","8faab848":"code","773be6da":"code","c2bf680b":"code","fc38500e":"code","243902fb":"code","af8675df":"code","a5874383":"code","e49dd61b":"code","acdaa97c":"code","5445ff96":"code","f4dc79fd":"code","cf5f39fd":"code","e86c6fd4":"code","51a9f124":"code","9954cfc2":"code","a677696f":"code","8b1c45e3":"code","3201742a":"code","287a927d":"code","0d9834c5":"code","843e78d5":"code","8879347e":"code","e52129ad":"code","f2c75cdc":"code","02f09b76":"code","b2e41001":"code","ae5dcf03":"code","eb523ee3":"code","8c120193":"code","d2e5493d":"code","35b6ea68":"code","4d3bcf79":"code","46143b27":"code","0332a210":"code","56e1b6be":"code","25920527":"code","a31fa7a2":"code","84457b38":"code","59a3de0d":"code","f7adeb17":"code","49ecede8":"code","c7ac7e82":"code","73985757":"code","5336f265":"code","8c9e7759":"code","d856e808":"code","1ad87d14":"code","de8e6fc6":"code","93afb3ba":"code","4560689c":"code","50e5f900":"code","17ba0599":"code","79c47c91":"code","ce42621b":"code","1e82c7c9":"code","ad5c7403":"code","8a8b6e8e":"code","815210a1":"code","afb6d2a8":"code","7fe802b6":"code","f79fed94":"code","ca2a3101":"code","e0c78c13":"code","3a032ccf":"code","b6144d0e":"markdown","5fb5d11c":"markdown","4ac84817":"markdown","8cbab3c8":"markdown","ba9ccfc7":"markdown","bd29aa36":"markdown","57acbd80":"markdown","f023311f":"markdown","0a3b6af5":"markdown","280be5e6":"markdown","1a7ad7d9":"markdown","02ff2967":"markdown","b3980efa":"markdown","c6f3b2ae":"markdown","1715f699":"markdown","8cfa35a1":"markdown","ec01c25a":"markdown","178d9bda":"markdown","4834436f":"markdown","cfec63ed":"markdown","cc755997":"markdown","c720f2a5":"markdown","3118e0e3":"markdown","d280f686":"markdown","a1bca8a9":"markdown","67292be1":"markdown","61f3e4a0":"markdown","3c7a1bb6":"markdown","3e5d83c2":"markdown","08dd3281":"markdown","615f42f8":"markdown","29f372ab":"markdown","183bfd65":"markdown","5dee34e0":"markdown","c02d8636":"markdown","9ef52b9c":"markdown","b27a9780":"markdown","8396cebf":"markdown"},"source":{"f44ff126":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","da3bfddc":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n","2c0ad617":"pip install pycm","102261fb":"from pycm import *","17da8849":"falhas = pd.read_csv('\/kaggle\/input\/faulty-steel-plates\/faults.csv')\nfalhas.head()","f662a419":"falhas.columns","36dbae01":"len(falhas.columns)","f0fabbbe":"X = falhas.iloc[:,0:27]\nY = falhas.iloc[:,27:34]","65f122b4":"X.columns","b8ce8de3":"Y.columns","43758e4b":"from sklearn.preprocessing import MinMaxScaler\n\nsc = MinMaxScaler(feature_range =(0,1))\n\nX_std = pd.DataFrame(sc.fit_transform(X), columns = X.columns)\nprint(X_std.shape)\nX_std.boxplot(figsize = (12.8,8), rot = 90)\nplt.boxplot\nplt.show()","7626ad6b":"X.isna().sum()","71fdb664":"Y.isna().sum()","553fad98":"from sklearn.decomposition import PCA\n# Aplica\u00e7\u00e3o do M\u00e9todo PCA\npca = PCA()\npca.fit(X_std) # ajuste do m\u00e9todo para base padronizada","9855a9d6":"print(pca.explained_variance_ratio_) # % da vari\u00e2ncia explicada por componentes","0384fa28":"PC_values = np.arange(pca.n_components_) + 1\nplt.figure(figsize=(12.8,8))\nplt.plot(PC_values, pca.explained_variance_ratio_, 'ro-', linewidth=1)\nplt.title('Scree Plot - Base original',  fontsize=18)\nplt.xlabel('Componente Principal', fontsize=18)\nplt.ylabel('Vari\u00e2ncia Explicada',  fontsize=18)","61291f25":"PC_values = np.arange(pca.n_components_) + 1\ndf5 = pd.DataFrame({'PCs':PC_values[0:10], 'Variancia_exp':pca.explained_variance_ratio_[0:10]})\ndf5['Variancia_exp_somada'] = df5['Variancia_exp'].cumsum()\nprint(df5)","0cd8cb90":"# 10 vari\u00e1veis j\u00e1 s\u00e3o suficientes para explicar 95,48% da vari\u00e2ncia acumulada da base total.","abe37b28":"X_PCA = PCA(n_components = 10).fit_transform(X_std)","d9ca0e57":"from sklearn.model_selection import train_test_split\n\nxTrain, xTest, yTrain, yTest = train_test_split(X_std, Y, test_size = 0.3, random_state = 0)\nxTrainPCA, xTestPCA, yTrainPCA, yTestPCA = train_test_split(X_PCA, Y, test_size = 0.3, random_state = 0)\n\n#verifica\u00e7\u00e3o\n\nprint('===============================================================================')\nprint('============================Normalizada========================================')\n\nprint(xTrain.shape)\nprint(yTest.shape)\nprint('\\n')\nprint('===============================================================================')\nprint('============================PCA================================================')\n\nprint(xTrainPCA.shape)\nprint(yTestPCA.shape)","464d41b9":"#Bibliotecas de aprendizado de m\u00e1quina\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense","f9e15b46":"model = Sequential()\n\nmodel.add(Dense(256, input_dim = 27, activation='relu')),\nmodel.add(Dense(128, activation='relu')),\nmodel.add(Dense(64, activation='relu')),\nmodel.add(Dense(7, activation='softmax'))\nmodel.summary()","7649de6c":"model.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","d0551603":"history = model.fit(xTrain, yTrain, epochs=100, validation_data=(xTest, yTest), verbose = 0)","ab8b35ea":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Acur\u00e1cia do modelo')\nplt.ylabel('Acur\u00e1cia')\nplt.xlabel('\u00c9poca')\nplt.legend(['treinamento', 'valida\u00e7\u00e3o'], loc='upper left')\nplt.show()","9074ab00":"# Avaliando o desempenho da rede neural MLP\ndesempenho = model.evaluate(xTest , yTest , verbose =0)\nprint(\" Acuracia : %.2f%%\"%(desempenho[1]*100))","069c43ec":"predictions1 = model.predict(xTest)\npredictions1 = pd.DataFrame(predictions1, columns = yTest.columns)","aa3d5edd":"true_class = np.argmax(yTest.to_numpy(), axis =1)\npred_class = np.argmax(predictions1.to_numpy(), axis=1)","f57205ca":"def organiza_CM(true_class, pred_class, yTest):\n  t_clase = list()\n  p_clase = list()\n  \n  for i in range(len(true_class)):\n    for j in range(len(yTest.columns)):\n      if true_class[i] == yTest.columns.get_loc(yTest.columns[j]):\n          t_clase.append(yTest.columns[j])\n      else:\n        pass\n\n  for i in range(len(pred_class)):\n    for j in range(len(yTest.columns)):\n      if pred_class[i] == yTest.columns.get_loc(yTest.columns[j]):\n          p_clase.append(yTest.columns[j])\n      else:\n        pass\n  return t_clase, p_clase","0d320c75":"t_clase, p_clase = organiza_CM(true_class, pred_class, yTest)","a4f9a611":"cm1 = ConfusionMatrix(actual_vector=t_clase, predict_vector=p_clase)\ncm1.plot(cmap=plt.cm.Greens, number_label=True, plot_lib=\"matplotlib\")\nplt.xticks(rotation=90)","88cc648c":"#Acur\u00e1cia por tipo de defeito\ncm1.ACC","b0782195":"model2 = Sequential()\nmodel2.add(Dense(16, input_dim = 10, activation='relu')),\nmodel2.add(Dense(16, activation='relu')),\nmodel2.add(Dense(7, activation='softmax'))\nmodel2.summary()","86bfdded":"model2.compile(optimizer='Adamax',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","fa682db0":"history2 = model2.fit(xTrainPCA, yTrainPCA, epochs=250, validation_data=(xTestPCA, yTestPCA), verbose = 0)","cabd16c6":"plt.plot(history2.history['loss'])\nplt.plot(history2.history['val_loss'])\nplt.title('Loss do modelo')\nplt.ylabel('Loss')\nplt.xlabel('\u00c9poca')\nplt.legend(['treinamento', 'valida\u00e7\u00e3o'], loc='upper left')\nplt.show()","f9258c6d":"plt.plot(history2.history['accuracy'])\nplt.plot(history2.history['val_accuracy'])\nplt.title('Acur\u00e1cia do modelo')\nplt.ylabel('Acur\u00e1cia')\nplt.xlabel('\u00c9poca')\nplt.legend(['treinamento', 'valida\u00e7\u00e3o'], loc='upper left')\nplt.show()","1dd1523b":"# Avaliando o desempenho da rede neural MLP Base de dados PCA\ndesempenho2 = model2.evaluate(xTestPCA , yTestPCA , verbose =0)\nprint(\" Acuracia : %.2f%%\"%(desempenho2[1]*100))","e88fee07":"predictions = model2.predict(xTestPCA)\npredictions = pd.DataFrame(predictions, columns = yTestPCA.columns)","4c089c5e":"true_class = np.argmax(yTestPCA.to_numpy(), axis =1)\npred_class = np.argmax(predictions.to_numpy(), axis=1)","83b152e3":"t_clase_pca, p_clase_pca = organiza_CM(true_class, pred_class, yTestPCA)","c8191277":"cmpca = ConfusionMatrix(actual_vector=t_clase_pca, predict_vector=p_clase_pca)\ncmpca.plot(cmap=plt.cm.Greens, number_label=True, plot_lib=\"matplotlib\")\nplt.xticks(rotation=90)","9455e112":"#Acur\u00e1cia por tipo de defeito para a rede treinada com a base reduzida por PCA\ncmpca.ACC","8665202b":"#Base normal\nfPastry    = Y.Pastry.sum()\/Y.shape[0]\nfZ_Scratch = Y.Z_Scratch.sum()\/Y.shape[0]\nfK_Scatch    = Y.K_Scatch.sum()\/Y.shape[0]\nfStain    = Y.Stains.sum()\/Y.shape[0]\nfDirtiness    = Y.Dirtiness.sum()\/Y.shape[0]\nfBumps = Y.Bumps.sum()\/Y.shape[0]\nfOther_Faults = Y.Other_Faults.sum()\/Y.shape[0]\n\n#PCA\nfPastrypca    = yTestPCA.Pastry.sum()\/yTestPCA.shape[0]\nfZ_Scratchpca = yTestPCA.Z_Scratch.sum()\/yTestPCA.shape[0]\nfK_Scatchpca    = yTestPCA.K_Scatch.sum()\/yTestPCA.shape[0]\nfStainpca    = yTestPCA.Stains.sum()\/yTestPCA.shape[0]\nfDirtinesspca    = yTestPCA.Dirtiness.sum()\/yTestPCA.shape[0]\nfBumpspca = yTestPCA.Bumps.sum()\/yTestPCA.shape[0]\nfOther_Faultspca = yTestPCA.Other_Faults.sum()\/yTestPCA.shape[0]\n","d4eef925":"### BASE NORMAL\n\nprint('Balanceamento por tipo de defeito para a base normal\\n')\nprint('Pastry: %.2f%%'%(100*fPastry))\nprint('Z_Scratch: %.2f%%'%(100*fZ_Scratch))\nprint('K_Scatch: %.2f%%'%(100*fK_Scatch))\nprint('Stain: %.2f%%'%(100*fStain))\nprint('Dirtiness: %.2f%%'%(100*fDirtiness))\nprint('Bumps: %.2f%%'%(100*fBumps))\nprint('Other Faults: %.2f%%'%(100*fOther_Faults))\n\nprint('\\n\\nBalanceamento por tipo de defeito para a base PCA\\n')\nprint('Pastry: %.2f%%'%(100*fPastrypca))\nprint('Z_Scratch: %.2f%%'%(100*fZ_Scratchpca))\nprint('K_Scatch: %.2f%%'%(100*fK_Scatchpca))\nprint('Stain: %.2f%%'%(100*fStainpca))\nprint('Dirtiness: %.2f%%'%(100*fDirtinesspca))\nprint('Bumps: %.2f%%'%(100*fBumpspca))\nprint('Other Faults: %.2f%%'%(100*fOther_Faultspca))","60866d03":"from imblearn.combine import SMOTETomek","f585af31":"df2 = falhas.iloc[:,27:34].mask(falhas.iloc[:,27:34].eq(1), falhas.columns.to_series(), axis=1)\ndf2[df2 == 0] = ''\ndf3 = df2['Pastry'] + df2['Z_Scratch'] + df2['K_Scatch'] + df2['Stains'] + df2['Dirtiness'] + df2['Bumps'] + df2['Other_Faults'] \n\nFault = X_std\nFault['Fault'] = df3\n\nFault","52b36f0d":"X2 = Fault.drop('Fault', axis = 1)\ny2 = Fault['Fault']\n\ny2.head()","cc2c591e":"X_resampled, y_resampled = SMOTETomek().fit_resample(X2, y2)","8faab848":"X_resampled.shape","773be6da":"y_resampled.shape","c2bf680b":"y_resampled = pd.DataFrame(y_resampled)\nX_resampled = pd.DataFrame(X_resampled, columns = X2.columns)","fc38500e":"print('Balanceamento das classes ap\u00f3s imputa\u00e7\u00e3o sint\u00e9tica em %\\n')\nround(100*y_resampled.value_counts() \/ len(y_resampled),2)","243902fb":"#Dummie variables - Voltando cada vari\u00e1vel para a coluna \ny_res2 = pd.get_dummies(y_resampled)","af8675df":"y_res2.columns","a5874383":"#Os nomes aparecem com Fault_, ent\u00e3o vamos acertar para ficar igual","e49dd61b":"y_res2.columns = ['Bumps', 'Dirtiness', 'K_Scatch', 'Other_Faults', 'Pastry', 'Stains', 'Z_Scratch']","acdaa97c":"y_res2[y_res2.columns.to_list()]","5445ff96":"#A ordem est\u00e1 trocada, na hora de montar a matriz de confus\u00e3o a fun\u00e7\u00e3o organiza_CM deve ter a mesma ordem que a original. Vamos acertar.","f4dc79fd":"y_res2 = y_res2[Y.columns.to_list()]","cf5f39fd":"y_res2","e86c6fd4":"#Redividindo a base de dados de treinamento e valida\u00e7\u00e3o para a base expandida de |SMOTE|\nxTrain2, xTest2, yTrain2, yTest2 = train_test_split(X_resampled, y_res2, test_size = 0.3, random_state = 0)","51a9f124":"yTest2.columns","9954cfc2":"model3 = Sequential()\nmodel3.add(Dense(128, input_dim = 27, activation='relu')),\nmodel3.add(Dense(64, activation='relu')),\nmodel3.add(Dense(7, activation='softmax'))\nmodel3.summary()\n\n\nmodel3.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy']) ","a677696f":"history3 = model3.fit(xTrain2, yTrain2, epochs=200, validation_data=(xTest2, yTest2), verbose = 0)","8b1c45e3":"plt.plot(history3.history['loss'])\nplt.plot(history3.history['val_loss'])\nplt.title('Loss do modelo')\nplt.ylabel('Loss')\nplt.xlabel('\u00c9poca')\nplt.legend(['treinamento', 'valida\u00e7\u00e3o'], loc='upper left')\nplt.show()","3201742a":"plt.plot(history3.history['accuracy'])\nplt.plot(history3.history['val_accuracy'])\nplt.title('Acur\u00e1cia do modelo')\nplt.ylabel('Acur\u00e1cia')\nplt.xlabel('\u00c9poca')\nplt.legend(['treinamento', 'valida\u00e7\u00e3o'], loc='upper left')\nplt.show()","287a927d":"desempenho3 = model3.evaluate(xTest2 , yTest2 , verbose =0)\nprint(\" Acuracia : %.2f%%\"%(desempenho3[1]*100))","0d9834c5":"desempenho3 = model3.evaluate(xTest , yTest , verbose =0)\nprint(\" Acuracia : %.2f%%\"%(desempenho3[1]*100))","843e78d5":"predictions3 = model3.predict(xTest)\npredictions3 = pd.DataFrame(predictions3, columns = yTest2.columns)","8879347e":"true_class2 = np.argmax(yTest.to_numpy(), axis =1)\npred_class2 = np.argmax(predictions3.to_numpy(), axis=1)","e52129ad":"t_clase_smote, p_clase_smote = organiza_CM(true_class2, pred_class2, yTest)","f2c75cdc":"cm_smote = ConfusionMatrix(actual_vector=p_clase_smote, predict_vector=t_clase_smote)\ncm_smote.plot(cmap=plt.cm.Greens, number_label=True, plot_lib=\"matplotlib\")\nplt.xticks(rotation=90)","02f09b76":"cm_smote.ACC","b2e41001":"pip install keras-tuner","ae5dcf03":"import math\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import Model\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Dense","eb523ee3":"import keras_tuner as kt\n\ndef build_model(hp):\n  model = Sequential()\n  \n  # Tune das camadas\n  # Escolha da quantidade de neur\u00f4nio entre 0-128\n  hp_units1 = hp.Int('neurons_camada1', min_value=16, max_value=128, step=16)\n  hp_units2 = hp.Int('neurons_camada2', min_value=0, max_value=128, step=16)\n  hp_units3 = hp.Int('neurons_camada3', min_value=0, max_value=128, step=16)\n  \n  ativacao1 =  hp.Choice('ativacao_camada1', values=['relu', 'sigmoid', 'tanh'], default='relu' )\n  ativacao2 =  hp.Choice('ativacao_camada2', values=['relu', 'sigmoid', 'tanh'], default='relu' )\n\n  model.add(Dense(units=hp_units1, activation= 'relu'))\n  model.add(Dense(units=hp_units2, activation= ativacao1))\n  model.add(Dense(units=hp_units3, activation= ativacao2))\n  \n  #Sa\u00edda\n  model.add(Dense(7, activation='softmax'))\n\n  \n  model.compile(\n      optimizer= hp.Choice('otimizador', values=['adam', 'adamax', 'nadam'], default='adam' ),\n      loss= 'categorical_crossentropy',\n      metrics=['accuracy']\n  )\n\n  return model","8c120193":"# HyperBand - keras tuner\ntuner = kt.Hyperband(\n    build_model,\n    objective='val_accuracy',\n    max_epochs=100,\n    directory='keras_tuner_dir',\n    project_name='melhor_model'\n)\n\ntuner.search(xTrain, yTrain, epochs=100, validation_data=(xTest, yTest))","d2e5493d":"#Resumo do melhor modelo encontrado\ntuner.results_summary(1)","35b6ea68":"melhor_modelo = tuner.get_best_models(num_models=2)","4d3bcf79":"history_melhor_modelo = melhor_modelo[0].fit(xTrain, yTrain, epochs=100, validation_data=(xTest, yTest), verbose = 0)","46143b27":"plt.plot(history_melhor_modelo.history['accuracy'])\nplt.plot(history_melhor_modelo.history['val_accuracy'])\nplt.title('Acur\u00e1cia do modelo')\nplt.ylabel('Acur\u00e1cia')\nplt.xlabel('\u00c9poca')\nplt.legend(['treinamento', 'valida\u00e7\u00e3o'], loc='upper left')\nplt.show()","0332a210":"# HyperBand algorithm from keras tuner\ntunerpca = kt.Hyperband(\n    build_model,\n    objective='val_accuracy',\n    max_epochs=100,\n    directory='keras_tuner_dir',\n    project_name='melhor_modelo_PCA'\n)\n\ntunerpca.search(xTrainPCA, yTrainPCA, epochs=100, validation_data=(xTestPCA, yTestPCA))","56e1b6be":"#Resumo do melhor modelo encontrado\ntunerpca.results_summary(1)","25920527":"melhor_modelo_pca = tunerpca.get_best_models(num_models=2)","a31fa7a2":"history_melhor_modelo_pca = melhor_modelo_pca[0].fit(xTrainPCA, yTrainPCA, epochs=100, validation_data=(xTestPCA, yTestPCA), verbose = 0)","84457b38":"plt.plot(history_melhor_modelo_pca.history['accuracy'])\nplt.plot(history_melhor_modelo_pca.history['val_accuracy'])\nplt.title('Acur\u00e1cia do modelo')\nplt.ylabel('Acur\u00e1cia')\nplt.xlabel('\u00c9poca')\nplt.legend(['treinamento', 'valida\u00e7\u00e3o'], loc='upper left')\nplt.show()","59a3de0d":"# HyperBand algorithm from keras tuner\ntunersmote = kt.Hyperband(\n    build_model,\n    objective='val_accuracy',\n    max_epochs=100,\n    directory='keras_tuner_dir',\n    project_name='melhor_modelo_smote'\n)\n\ntunersmote.search(xTrain2, yTrain2, epochs=100, validation_data=(xTest2, yTest2))","f7adeb17":"#Resumo do melhor modelo encontrado\ntunersmote.results_summary(1)","49ecede8":"melhor_modelo_smote = tunersmote.get_best_models(num_models=2)","c7ac7e82":"history_melhor_modelo_smote = melhor_modelo_smote[0].fit(xTrain2, yTrain2, epochs=250, validation_data=(xTest2, yTest2), verbose = 0)","73985757":"plt.plot(history_melhor_modelo_smote.history['accuracy'])\nplt.plot(history_melhor_modelo_smote.history['val_accuracy'])\nplt.title('Acur\u00e1cia do modelo')\nplt.ylabel('Acur\u00e1cia')\nplt.xlabel('\u00c9poca')\nplt.legend(['treinamento', 'valida\u00e7\u00e3o'], loc='upper left')\nplt.show()","5336f265":"tunersmote.search_space_summary(extended=False)","8c9e7759":"desempenho4 = melhor_modelo_smote[0].evaluate(xTest2 , yTest2 , verbose =0)\nprint(\" Acuracia : %.2f%%\"%(desempenho4[1]*100))","d856e808":"desempenho5 = melhor_modelo_smote[0].evaluate(xTest , yTest , verbose =0)\nprint(\" Acuracia : %.2f%%\"%(desempenho5[1]*100))","1ad87d14":"predictions5 = melhor_modelo_smote[0].predict(xTest)\npredictions5 = pd.DataFrame(predictions5, columns = yTest2.columns)","de8e6fc6":"true_class = np.argmax(yTest.to_numpy(), axis =1)\npred_class = np.argmax(predictions5.to_numpy(), axis=1)","93afb3ba":"t_clase_smote, p_clase_smote = organiza_CM(true_class, pred_class, yTest)","4560689c":"cm_smote = ConfusionMatrix(actual_vector=p_clase_smote, predict_vector=t_clase_smote)\ncm_smote.plot(cmap=plt.cm.Greens, number_label=True, plot_lib=\"matplotlib\")\nplt.xticks(rotation=90)","50e5f900":"def cross_val_teste(X, y, modelo):\n  X_std = pd.DataFrame(sc.fit_transform(X), columns = X.columns)\n  Y = y\n  \n  xTrain, xTest, yTrain, yTest = train_test_split(X_std, Y, test_size = 0.3, random_state = 0)\n  \n  modelo[0].fit(xTrain, yTrain, epochs=100, validation_data=(xTest, yTest), verbose = 0)\n  acuracia = modelo[0].evaluate(xTest , yTest , verbose =0)\n\n  return acuracia[1]","17ba0599":"Desempenho = []\n\nfor i in range(0,10):\n  Desempenho.append(cross_val_teste(X, Y, melhor_modelo_smote))","79c47c91":"media_acuracia = np.mean(Desempenho)\ndesvpad_acuracia = np.std(Desempenho)","ce42621b":"print('M\u00e9dia da acur\u00e1cia do melhor modelo SMOTE: %.2f%%'%(100*media_acuracia))\nprint('Desvio padr\u00e3o da acur\u00e1cia do melhor modelo SMOTE: %.2f%%'%(100*desvpad_acuracia))","1e82c7c9":"def cross_val_teste_PCA(X, y, modelo):\n  X_std = pd.DataFrame(sc.fit_transform(X), columns = X.columns)\n  X_PCA = PCA(n_components = 10).fit_transform(X_std)\n  Y = y\n  \n  xTrain, xTest, yTrain, yTest = train_test_split(X_PCA, Y, test_size = 0.3, random_state = 0)\n  \n  modelo[0].fit(xTrain, yTrain, epochs=100, validation_data=(xTest, yTest), verbose = 0)\n  acuracia = modelo[0].evaluate(xTest , yTest , verbose =0)\n\n  return acuracia[1]","ad5c7403":"Desempenhopca = []\n\nfor i in range(0,10):\n  Desempenhopca.append(cross_val_teste_PCA(X, Y, melhor_modelo_pca))","8a8b6e8e":"media_acuracia_pca = np.mean(Desempenhopca)\ndesvpad_acuracia_pca = np.std(Desempenhopca)","815210a1":"print('M\u00e9dia da acur\u00e1cia do melhor modelo PCA: %.2f%%'%(100*media_acuracia_pca))\nprint('desvio padr\u00e3o da acur\u00e1cia do melhor modelo PCA: %.2f%%'%(100*desvpad_acuracia_pca))","afb6d2a8":"from scipy import stats","7fe802b6":"stats.ttest_ind(Desempenho, Desempenhopca)","f79fed94":"#Pelo valor p menor que 0,05, h\u00e1 diferen\u00e7as entre os valores dos modelos. \n# Visualizando via gr\u00e1fico de boxplot","ca2a3101":"desempenhos = [Desempenho, Desempenhopca]\n\nplt.boxplot(desempenhos)\nplt.xticks([1,2], ['SMOTE', 'PCA'])\nplt.show()","e0c78c13":"print('Desempenho via PCA', round(100*cmpca.Overall_ACC,2), '%')","3a032ccf":"print('Desempenho via SMOTE', round(100*cm_smote.Overall_ACC,2), '%')","b6144d0e":"### Acur\u00e1cia na base com SMOTE","5fb5d11c":"### Divis\u00e3o da base para treino e valida\u00e7\u00e3o","4ac84817":"### Treinando o modelo","8cbab3c8":"### Acur\u00e1cia do modelo com PCA","ba9ccfc7":"# Desempenho final - via matriz de confus\u00e3o","bd29aa36":"#**Tratamento da Base de dados**","57acbd80":"### Treinando o modelo","f023311f":"# Verifica\u00e7\u00e3o do desempenho m\u00e9dio","0a3b6af5":"## MLP","280be5e6":"### Acur\u00e1cia por tipo de defeito","1a7ad7d9":"# HIPERPAR\u00c2METROS","02ff2967":"## Avalia\u00e7\u00e3o da base de dados","b3980efa":"Usando o pacote de tunagem Keras-Tuner","c6f3b2ae":"### Matriz de confus\u00e3o","1715f699":"### Acur\u00e1cia do modelo com a base normal","8cfa35a1":"## Divis\u00e3o da base de dados","ec01c25a":"## Leitura","178d9bda":"### Padroniza\u00e7\u00e3o\nMinMaxScaler","4834436f":"### Base reduzida via PCA","cfec63ed":"## Base PCA","cc755997":"Base normalizada","c720f2a5":"### Matriz de confus\u00e3o","3118e0e3":"# MODELO RNA","d280f686":"# PCA","a1bca8a9":"### Como as classes est\u00e3o balanceadas","67292be1":"Dado que os dados n\u00e3o tem balanceamento por tipo de defeito, vamos usar a t\u00e9cnica de imputa\u00e7\u00e3o sint\u00e9tica (SMOTE)","61f3e4a0":"# BALANCEAMENTO DAS CLASSES\n\n","3c7a1bb6":"## Base normal","3e5d83c2":"### Acur\u00e1cia por tipo de defeito","08dd3281":"## Base SMOTE","615f42f8":"### Matriz de confus\u00e3o","29f372ab":"## PCA - Aplica\u00e7\u00e3o da t\u00e9cnica de redu\u00e7\u00e3o de dimens\u00e3o","183bfd65":"### Base normal","5dee34e0":"### Acur\u00e1cia na base de teste normal, sem oversampling ap\u00f3s treinamento com SMOTE.","c02d8636":"## T\u00e9cnica SMOTE - Imputa\u00e7\u00e3o sint\u00e9tica","9ef52b9c":"### Treinando o modelo","b27a9780":"### Matriz de confus\u00e3o","8396cebf":"### Avaliando a diferen\u00e7a de desempenho por teste-t\nEntre o modelo smote e o modelo PCA"}}