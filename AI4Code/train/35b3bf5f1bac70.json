{"cell_type":{"0131f43a":"code","11025522":"code","7b417bfd":"code","06897deb":"code","abb3bf6a":"code","d52e6d23":"code","0e6ccb91":"code","c9f966db":"code","482a04b2":"code","57f5e263":"code","e606173a":"code","03ade78f":"code","256bef91":"code","c2ffce9f":"code","2385516c":"code","0fe1bf58":"markdown","e0ec7c23":"markdown","b7c9fdab":"markdown","ebeda84d":"markdown","d5cc8f63":"markdown","f941e0e4":"markdown","7d754282":"markdown","75c01c0a":"markdown","b61e95bc":"markdown","b4f52727":"markdown"},"source":{"0131f43a":"!pip install apyori ","11025522":"from pandas import Series, DataFrame\nimport pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pylab as plt\nimport seaborn as sns\nfrom apyori import apriori\n\n#Load the prospect dataset\nraw_data = pd.read_csv(\"..\/input\/attrition\/attrition.csv\")\n\nraw_data.head(10)","7b417bfd":"count = raw_data['AGE_GROUP'].value_counts()\nsns.set(style=\"darkgrid\")\nsns.barplot(x=count.index, y=count.values, alpha=0.9)\nplt.title('Frequency Distribution of Age Groups that Churned')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Age Groups', fontsize=12)\nplt.show()\n","06897deb":"count = raw_data['EMP_STATUS'].value_counts()\nsns.set(style=\"darkgrid\")\nsns.barplot(x=count.index, y=count.values, alpha=0.9)\nplt.title('Frequency Distribution of Employment Status that Churned')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Employment Status', fontsize=12)\nplt.show()","abb3bf6a":"count = raw_data['REASON'].value_counts()\nsns.set(style=\"darkgrid\")\nsns.barplot(x=count.index, y=count.values, alpha=0.9)\nplt.title('Frequency Distribution of Reason for Churning')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Reason for churning', fontsize=12)\nplt.show()","d52e6d23":"labels = raw_data['LIFETIME'].astype('category').cat.categories.tolist()\ncounts = raw_data['LIFETIME'].value_counts()\nsizes = [counts[var_cat] for var_cat in labels]\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=False) #autopct is show the % on plot\nax1.axis('equal')\nplt.show()","0e6ccb91":"labels = raw_data['PROBLEMS'].astype('category').cat.categories.tolist()\ncounts = raw_data['PROBLEMS'].value_counts()\nsizes = [counts[var_cat] for var_cat in labels]\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=False) #autopct is show the % on plot\nax1.axis('equal')\nplt.show()","c9f966db":"sns.histplot(binwidth=0.5, x=\"REASON\", hue=\"TYPE\", data=raw_data, stat=\"count\", multiple=\"stack\")","482a04b2":"sns.histplot(binwidth=0.5, x=\"LIFETIME\", hue=\"AGE_GROUP\", data=raw_data, stat=\"count\", multiple=\"stack\")","57f5e263":"basket_str = \"\"\nfor rowNum, row in raw_data.iterrows():\n    \n    #Break lines\n    if (rowNum != 0):\n        basket_str = basket_str + \"\\n\"\n    #Add the rowid as the first column\n    basket_str = basket_str + str(rowNum) \n    #Add columns\n    for colName, col in row.iteritems():\n        basket_str = basket_str + \",\\\"\" + colName + \"=\" + str(col) +\"\\\"\"\n\n#print(basket_str)\nbasket_file=open(\"warranty_basket.csv\",\"w\")\nbasket_file.write(basket_str)\nbasket_file.close()","e606173a":"#read back\nbasket_data=pd.read_csv(\"warranty_basket.csv\",header=None)\nfilt_data = basket_data.drop(basket_data.columns[[0]], axis=1)\nfilt_data","03ade78f":"results= list(apriori(filt_data.values))\nresults[1:10]","256bef91":"rulesList= pd.DataFrame(columns=('LHS', 'RHS', 'COUNT', 'CONFIDENCE','LIFT'))\nrowCount=0\n\n#Convert results into a Data Frame\nfor row in results:\n    for affinity in row[2]:\n        rulesList.loc[rowCount] = [ ', '.join(affinity.items_base) ,\\\n                                    affinity.items_add, \\\n                                    len(affinity.items_base) ,\\\n                                    affinity.confidence,\\\n                                    affinity.lift]\n        rowCount +=1","c2ffce9f":"rulesList.head()","2385516c":"rulesList[(rulesList.COUNT <= 1) & (rulesList.CONFIDENCE > 0.7)].head(10)","0fe1bf58":"We can take a look at the rules by simply doing a head.","e0ec7c23":"We can also filter rules where the count of elements is 1 and the confidence is > 70%","b7c9fdab":"## Build Association Rules\n\nWe now use the apriori algorithm to build association rules. We then extract the results and populate a data frame for future use. The apriori provides the LHS for multiple combinations of the items. We capture the counts along with confidence and lift in this example","ebeda84d":"The CSV contains information about each customer who have left the business. It contains attributes like LIFETIME of the customer, How the customer left, reasons, problems and demographics.\nFor doing association rules mining, the data needs to be in a specific format. Each line should be a transaction with a list of items for that transaction. We will take the CSV file data convert them into values like \"name=value\" to create this specific data structure","d5cc8f63":"## Load the Dataset and Transform\n\nWe first load the data and view it.\n","f941e0e4":"## Exploratory Data Analysis (EDA)","7d754282":"Looking at the rules, we can easily see some patterns. Customers who have left the business between 3 months and 1 year are always in the age group 20-30. Similarly, customers in age group 20-30 always cancelled the service. These are interesting facts that can be analyzed further by the business.","75c01c0a":"In this example, we analyze customer attrition data to discover patterns. These will help us dive deeper into those patterns and do root cause analysis of why they are happening. We will use association rules mining algorithm for this purpose.","b61e95bc":"## Using the Rules","b4f52727":" #  <p style=\"text-align: center;\">Discovering customer attrition patterns<\/p> "}}