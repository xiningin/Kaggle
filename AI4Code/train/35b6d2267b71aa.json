{"cell_type":{"893346ff":"code","a25a87c2":"code","e6cb0811":"code","9cea2298":"code","bb94b7bd":"code","bf0fcd58":"code","11d773b4":"code","edd0387f":"code","5032baef":"code","7fae1bdd":"code","678fc54d":"code","ebc0ebed":"code","635f0128":"code","595688d8":"code","88584491":"code","c2c82e0a":"code","7ca0e814":"code","3428802f":"code","d5aef062":"code","8347a337":"code","a64eacf4":"code","eafad722":"code","9d356938":"code","5a7a96ca":"code","2cad421c":"code","b2e448d9":"code","2072dcbd":"code","47fb0ab2":"code","b6590f07":"code","e982d53e":"code","72278d6d":"code","c73b96e7":"code","ffe748c9":"code","dc862250":"code","32a4650b":"code","40f6114d":"code","b3f969b5":"code","f46e388d":"code","9a44135a":"code","079270d9":"code","c2811a66":"code","42a127f6":"code","f614f85d":"code","fddcadc6":"code","2f183d38":"code","63f749dd":"code","70f979f5":"code","5715f4af":"code","2e217c17":"code","2b2a539f":"code","e72705b1":"code","6dacb5a8":"code","cb889790":"code","44cc0fcb":"code","60c6287f":"code","6cd382c2":"code","bf1ac3d8":"code","5075a89b":"code","1b98f478":"code","7600c662":"code","aee263af":"code","53b6ac75":"code","635e3c22":"code","402f3aea":"code","8a4f6776":"code","d1409381":"code","1ee49a29":"code","b935f6b5":"code","c909b645":"code","a4214310":"code","b1d213e6":"code","4d6a9f55":"code","e53d6fde":"code","fca2a8ab":"code","30f853fc":"code","4decc052":"code","0253871e":"code","bcd51509":"code","f79f1eff":"markdown","96e5ffc3":"markdown","ca657a74":"markdown","fb069e5c":"markdown","b1787e7c":"markdown","0e953c32":"markdown","b5dbb342":"markdown","2583a5bd":"markdown","a13ada68":"markdown","703d485c":"markdown","14045e25":"markdown","16f4314a":"markdown"},"source":{"893346ff":"# !pip install imgaug==0.2.9\n\nfrom imgaug import augmenters as iaa\nimport imgaug\nprint(imgaug.__version__)","a25a87c2":"import numpy as np # linear algebra\nprint(np.__version__)\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nprint(pd.__version__)\nimport random\n\nimport os\nfrom pathlib import Path\n\nimport cv2\nprint(cv2.__version__)\nimport matplotlib.image as mpimg\nfrom matplotlib import pyplot as plt\nimport matplotlib\n(matplotlib.__version__)\n\nimport tensorflow\nprint(tensorflow.__version__)\n\nfrom sklearn.model_selection import StratifiedKFold, KFold\nimport sklearn\nprint(sklearn.__version__)","e6cb0811":"import PIL\nprint(PIL.__version__)","9cea2298":"INPUT_DIR = Path('\/kaggle\/input\/supervisely-filtered-segmentation-person-dataset\/supervisely_person_clean_2667_img')\nIMAGE_DIR = Path(f'{INPUT_DIR}\/images')\nMASK_DIR = Path(f'{INPUT_DIR}\/masks')\n\nINPUT_DIR, IMAGE_DIR, MASK_DIR","bb94b7bd":"image_paths = []\nfor dirname, _, filenames in os.walk(IMAGE_DIR):\n    for filename in filenames:\n        image_paths.append(filename)\n        \nimage_df = pd.DataFrame(image_paths, columns=['path'])\nimage_df = image_df.sort_values(by=['path'], ignore_index=True)\nimage_df.head()","bf0fcd58":"image_df = pd.read_csv('..\/input\/superviselyfilteredsegmentationpersoncsv\/images_with_rle.csv')\nimage_df.head()","11d773b4":"image_df.describe()","edd0387f":"test_img = mpimg.imread(f'{IMAGE_DIR}\/{image_df[\"path\"][1]}')\nplt.figure(figsize=(50, 4))\nplt.imshow(test_img)\nplt.show()","5032baef":"test_mask = mpimg.imread(f'{MASK_DIR}\/{image_df[\"path\"][1]}')\nplt.figure(figsize=(50,4))\nplt.imshow(test_mask)\nplt.show()","7fae1bdd":"def rle_encode(mask):\n    pixels = mask.T.flatten()\n    # We need to allow for cases where there is a '1' at either end of the sequence.\n    # We do this by padding with a zero at each end when needed.\n    use_padding = False\n    if pixels[0] or pixels[-1]:\n        use_padding = True\n        pixel_padded = np.zeros([len(pixels) + 2], dtype=pixels.dtype)\n        pixel_padded[1:-1] = pixels\n        pixels = pixel_padded\n    rle = np.where(pixels[1:] != pixels[:-1])[0] + 2\n    if use_padding:\n        rle = rle - 1\n    rle[1::2] = rle[1::2] - rle[:-1:2]\n    return rle","678fc54d":"def rle_to_string(runs):\n    return ' '.join(str(x) for x in runs)","ebc0ebed":"img_test = cv2.imread(f'{MASK_DIR}\/{image_df[\"path\"][1]}', cv2.IMREAD_GRAYSCALE)\nrle_encode(img_test)","635f0128":"mask_img = cv2.imread(f'{MASK_DIR}\/{image_df[\"path\"][1]}', cv2.IMREAD_GRAYSCALE)\nmask_img.dtype","595688d8":"rle_mask = rle_encode(mask_img)\nrle_mask","88584491":"rle_str = rle_to_string(rle_mask)\nrle_str","c2c82e0a":"def rle_decode(rle_str, mask_shape, mask_dtype):\n    s = rle_str.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    mask = np.zeros(np.prod(mask_shape), dtype=mask_dtype)\n    for lo, hi in zip(starts, ends):\n        mask[lo:hi] = 1\n    return mask.reshape(mask_shape[::-1]).T","7ca0e814":"rle_decoded = rle_decode(rle_str, mask_img.shape, mask_img.dtype)\nplt.figure(figsize=(50,5))\nplt.imshow(rle_decoded)\nplt.show()","3428802f":"# def get_rle(data):\n#     path = data['path']\n#     img = cv2.imread(f'{MASK_DIR}\/{path}', cv2.IMREAD_GRAYSCALE)\n#     return rle_to_string(rle_encode(img))","d5aef062":"# image_df['rle'] = image_df.apply(get_rle, axis=1)\n# image_df.head()","8347a337":"def verify_mask(item):\n    img = cv2.imread(f'{MASK_DIR}\/{item[\"path\"]}', cv2.IMREAD_GRAYSCALE)\n    rle = rle_to_string(rle_encode(img))\n    return rle == item['rle']","a64eacf4":"verify_mask(image_df.loc[1])","eafad722":"# def get_shape(item):\n#     path = item['path']\n#     img = cv2.imread(f'{MASK_DIR}\/{path}', cv2.IMREAD_GRAYSCALE)\n#     return img.shape","9d356938":"# image_df['shape'] = image_df.apply(get_shape, axis=1)\n# image_df[['height', 'width']] = pd.DataFrame(image_df['shape'].tolist(), index=image_df.index)\n# image_df = image_df.drop('shape', axis=1)\n# image_df.head()","5a7a96ca":"# image_df.to_csv('images_with_rle.csv', index=False)","2cad421c":"!pip install \/kaggle\/input\/pycocotools\/pycocotools-2.0-cp37-cp37m-linux_x86_64.whl","b2e448d9":"!git clone https:\/\/github.com\/akTwelve\/Mask_RCNN.git\nos.chdir('Mask_RCNN')\n\n!rm -rf .git\n!rm -rf images assets","2072dcbd":"!wget --quiet https:\/\/github.com\/matterport\/Mask_RCNN\/releases\/download\/v2.0\/mask_rcnn_coco.h5\n!ls -lh mask_rcnn_coco.h5\n\nCOCO_WEIGHTS_PATH = 'mask_rcnn_coco.h5'","47fb0ab2":"import sys\nimport json\nimport time\nfrom PIL import Image, ImageDraw","b6590f07":"from mrcnn.config import Config\nfrom mrcnn import utils\nimport mrcnn.model as modellib\nfrom mrcnn import visualize\nfrom mrcnn.model import log\nfrom mrcnn.visualize import display_images","e982d53e":"MODEL_DIR = os.path.join('..\/Mask_RCNN', 'logs')\nos.mkdir(MODEL_DIR)","72278d6d":"class PersonConfig(Config):\n    \"\"\"Configuration for training on the supervisely dataset.\n    Derives from the base Config class and overrides values specific\n    to the person dataset.\n    \"\"\"\n    # Give the configuration a recognizable name\n    NAME = \"person\"\n\n    # Train on 1 GPU and 8 image per GPU. Batch size is 8 (GPUs * images\/GPU).\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 4\n\n    # Number of classes (including background)\n    NUM_CLASSES = 1 + 1  # background + 1\n\n    # All of our training images are 256x256\n    IMAGE_MIN_DIM = 256\n    IMAGE_MAX_DIM = 256\n\n    # You can experiment with this number to see if it improves training\n    STEPS_PER_EPOCH = 200\n\n    # This is how often validation is run. If you are using too much hard drive space\n    # on saved models (in the MODEL_DIR), try making this value larger.\n    VALIDATION_STEPS = 15\n    \n    BACKBONE = 'resnet50'\n\n    # To be honest, I haven't taken the time to figure out what these do\n#     RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)\n#     TRAIN_ROIS_PER_IMAGE = 32\n#     MAX_GT_INSTANCES = 50\n#     POST_NMS_ROIS_INFERENCE = 500\n#     POST_NMS_ROIS_TRAINING = 1000\n    \n#     DETECTION_MIN_CONFIDENCE = 0.85\n    \n#     LOSS_WEIGHTS = {\n#         \"rpn_class_loss\": 1.,\n#         \"rpn_bbox_loss\": 2.,\n#         \"mrcnn_class_loss\": 2.,\n#         \"mrcnn_bbox_loss\": 2.,\n#         \"mrcnn_mask_loss\": 5.\n#     }\n    \n#     USE_MINI_MASK = False\n    \nconfig = PersonConfig()\nconfig.display()","c73b96e7":"class PersonDataset(utils.Dataset):\n    def __init__(self, df):\n        super().__init__(self)\n        \n        self.IMAGE_SIZE = 256\n        \n        self.add_class(\"types\", 1, \"person\")\n        \n        # Add images\n        for i, row in df.iterrows():\n            self.add_image(source=\"types\",\n                           image_id=row.name, \n                           path=f'{IMAGE_DIR}\/{row.name}', \n                           height=row['height'],\n                           width=row['width'],\n                           annotations=row['rle'])\n            \n    def _resize_image(self, image_path):\n        img = cv2.imread(image_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, (self.IMAGE_SIZE, self.IMAGE_SIZE), interpolation=cv2.INTER_AREA)  \n        return img\n        \n    def load_image(self, image_id):\n        return self._resize_image(self.image_info[image_id]['path'])\n    \n    def image_reference(self, image_id):\n        info = self.image_info[image_id]\n        return info['path']\n    \n    def load_mask(self, image_id):\n        info = self.image_info[image_id]\n        annotation = info['annotations'][0]\n        \n        mask = np.zeros([self.IMAGE_SIZE, self.IMAGE_SIZE, 1], dtype=np.uint8)\n        \n        rle_decoded = rle_decode(annotation, (info['height'], info['width']), np.uint8)\n        sub_mask = cv2.resize(rle_decoded, (self.IMAGE_SIZE, self.IMAGE_SIZE), interpolation=cv2.INTER_NEAREST)\n        mask[:,:, 0] = sub_mask\n        return mask, np.array([1]).astype(np.int32)","ffe748c9":"split_df = image_df.groupby('path')[['rle']].agg(lambda x: list(x))\nsize_df = image_df.groupby('path')[['height', 'width']].mean()\nsplit_df = split_df.join(size_df, on='path')\nsplit_df.head()","dc862250":"dataset = PersonDataset(split_df)\ndataset.prepare()","32a4650b":"dataset.class_names","40f6114d":"for i, info in enumerate(dataset.class_info):\n    print(\"{:3}. {:50}\".format(i, info['name']))","b3f969b5":"# Load random image and mask.\nimage_id = random.choice(dataset.image_ids)\nimage = dataset.load_image(image_id)\nmask, class_ids = dataset.load_mask(image_id)\n# Compute Bounding box\nbbox = utils.extract_bboxes(mask)\n\n# Display image and additional stats\nprint(\"image_id \", image_id, dataset.image_reference(image_id))\nlog(\"image\", image)\nlog(\"mask\", mask)\nlog(\"class_ids\", class_ids)\nlog(\"bbox\", bbox)\n# Display image and instances\nvisualize.display_instances(image, bbox, mask, class_ids, dataset.class_names)","f46e388d":"# Load random image and mask.\nimage_id = np.random.choice(dataset.image_ids, 1)[0]\nimage = dataset.load_image(image_id)\nmask, class_ids = dataset.load_mask(image_id)\noriginal_shape = image.shape\n# Resize\nimage, window, scale, padding, _ = utils.resize_image(\n    image, \n    min_dim=config.IMAGE_MIN_DIM, \n    max_dim=config.IMAGE_MAX_DIM,\n    mode=config.IMAGE_RESIZE_MODE)\nmask = utils.resize_mask(mask, scale, padding)\n# Compute Bounding box\nbbox = utils.extract_bboxes(mask)\n\n# Display image and additional stats\nprint(\"image_id: \", image_id, dataset.image_reference(image_id))\nprint(\"Original shape: \", original_shape)\nlog(\"image\", image)\nlog(\"mask\", mask)\nlog(\"class_ids\", class_ids)\nlog(\"bbox\", bbox)\n# Display image and instances\nvisualize.display_instances(image, bbox, mask, class_ids, dataset.class_names)","9a44135a":"# image_id = np.random.choice(dataset.image_ids, 1)[0]\n# image, image_meta, class_ids, bbox, mask = modellib.load_image_gt(dataset, config, image_id)\n\n# log(\"image\", image)\n# log(\"image_meta\", image_meta)\n# log(\"class_ids\", class_ids)\n# log(\"bbox\", bbox)\n# log(\"mask\", mask)\n\n# display_images([image]+[mask[:,:,i] for i in range(min(mask.shape[-1], 7))])","079270d9":"# visualize.display_instances(image, bbox, mask, class_ids, dataset.class_names)","c2811a66":"for i in range(5):\n    image_id = random.choice(dataset.image_ids)\n\n    image = dataset.load_image(image_id)\n    mask, class_ids = dataset.load_mask(image_id)\n    visualize.display_top_masks(image, mask, class_ids, dataset.class_names, limit=4)","42a127f6":"model = modellib.MaskRCNN(mode='training', config=config, model_dir=MODEL_DIR)","f614f85d":"model.load_weights(COCO_WEIGHTS_PATH,\n                   by_name=True,\n                   exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \"mrcnn_bbox\", \"mrcnn_mask\"])","fddcadc6":"FOLD = 2\nN_FOLDS = 4\n\nkf = KFold(n_splits=N_FOLDS, random_state=16, shuffle=True)\nsplits = kf.split(split_df) # ideally, this should be multilabel stratification\n\ndef get_fold():    \n    for i, (train_index, valid_index) in enumerate(splits):\n        if i == FOLD:\n            return split_df.iloc[train_index], split_df.iloc[valid_index]\n        \ntrain_df, valid_df = get_fold()\n\ntrain_dataset = PersonDataset(train_df)\ntrain_dataset.prepare()\n\nvalid_dataset = PersonDataset(valid_df)\nvalid_dataset.prepare()\n# for train, test in splits:\n#     print('Train: %s, Test: %s' % (split_df.iloc[train], split_df.iloc[test]))\n#     print(train, test)\ntrain_df.shape, valid_df.shape","2f183d38":"train_df.head(10)","63f749dd":"# Ignore all warnings\nimport warnings \nwarnings.filterwarnings(\"ignore\")","70f979f5":"EPOCHS = [10, 15, 20]","5715f4af":"%%time\nmodel.train(train_dataset, valid_dataset, \n            learning_rate=config.LEARNING_RATE*2, \n            epochs=EPOCHS[0], \n            layers='heads')","2e217c17":"history = model.keras_model.history.history","2b2a539f":"# augmentation = iaa.Sequential([\n#     iaa.Fliplr(0.5) # only horizontal flip here\n# ])","e72705b1":"augmentation = iaa.SomeOf((0, 3), [\n    iaa.Fliplr(0.5),\n    iaa.Flipud(0.5),\n    iaa.OneOf([iaa.Affine(rotate=90),\n               iaa.Affine(rotate=180),\n               iaa.Affine(rotate=270)],\n             ),\n    iaa.Affine(scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)}),\n    iaa.Multiply((0.8, 1.5)),\n    iaa.GaussianBlur(sigma=(0.0, 5.0))\n])","6dacb5a8":"%%time\nmodel.train(train_dataset, valid_dataset,\n           learning_rate=config.LEARNING_RATE,\n           epochs=EPOCHS[1],\n           layers='4+',\n           augmentation=augmentation)","cb889790":"new_history = model.keras_model.history.history\nfor k in new_history:\n    history[k] = history[k]+new_history[k]\n# history","44cc0fcb":"%%time\nmodel.train(train_dataset, valid_dataset,\n           learning_rate=config.LEARNING_RATE\/5,\n           epochs=EPOCHS[2],\n           layers='all',\n           augmentation=augmentation)","60c6287f":"new_history = model.keras_model.history.history\nfor k in new_history:\n    history[k] = history[k]+new_history[k]\n# history","6cd382c2":"epochs = range(EPOCHS[-1])\n\nplt.figure(figsize=(18, 6))\nplt.title('History')\n\nplt.subplot(231)\nplt.plot(epochs, history['loss'], label=\"train loss\")\nplt.plot(epochs, history['val_loss'], label=\"valid loss\")\nplt.legend()\nplt.subplot(232)\nplt.plot(epochs, history['mrcnn_class_loss'], label=\"train class loss\")\nplt.plot(epochs, history['val_mrcnn_class_loss'], label=\"valid class loss\")\nplt.legend()\nplt.subplot(233)\nplt.plot(epochs, history['mrcnn_mask_loss'], label=\"train mask loss\")\nplt.plot(epochs, history['val_mrcnn_mask_loss'], label=\"valid mask loss\")\nplt.legend()\nplt.subplot(234)\nplt.plot(epochs, history['mrcnn_bbox_loss'], label=\"train bbox loss\")\nplt.plot(epochs, history['val_mrcnn_bbox_loss'], label=\"valid bbox loss\")\nplt.legend()\nplt.subplot(235)\nplt.plot(epochs, history['rpn_class_loss'], label=\"train rpn_class loss\")\nplt.plot(epochs, history['val_rpn_class_loss'], label=\"valid rpn_class loss\")\nplt.legend()\nplt.subplot(236)\nplt.plot(epochs, history['rpn_bbox_loss'], label=\"train rpn bbox loss\")\nplt.plot(epochs, history['val_rpn_bbox_loss'], label=\"valid rpn bbox loss\")\nplt.legend()\n\nplt.show()","bf1ac3d8":"# import shutil\n# shutil.make_archive('modells', 'zip', '.\/logs')\n# shutil.make_archive('tfjs', 'zip', '.\/tfjs')","5075a89b":"# model.keras_model.summary()","1b98f478":"best_epoch = np.argmin(history['val_loss']) + 1\nprint('Best epoch: ', best_epoch)\nprint('Valid loss: ', history['val_loss'][best_epoch-1])","7600c662":"import glob","aee263af":"# glob_list = glob.glob(f'.\/logs\/person_bg*\/mask_rcnn_person_bg_{best_epoch:04d}.h5')\n# model_path = glob_list[0]\nmodel_path = model.find_last()","53b6ac75":"class InferenceConfig(PersonConfig):\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n    IMAGE_MIN_DIM = 256\n    IMAGE_MAX_DIM = 256\n    DETECTION_MIN_CONFIDENCE = 0.85\n    \ninference_config = InferenceConfig()\n\nmodel = modellib.MaskRCNN(mode='inference',\n                         config=inference_config,\n                         model_dir=MODEL_DIR)\n\nprint('Loading weights from ', model_path)\nmodel.load_weights(model_path, by_name=True)","635e3c22":"IMAGE_SIZE = 256","402f3aea":"def resize_image(image_path):\n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_AREA)  \n    return img","8a4f6776":"for i, row in image_df.sample(10).iterrows():\n    img = cv2.imread(f'{IMAGE_DIR}\/{row[\"path\"]}')\n    img_arr = np.array(img)\n    results = model.detect([img_arr], verbose=1)\n    r = results[0]\n    visualize.display_instances(img, r['rois'], r['masks'], r['class_ids'], \n                                dataset.class_names, r['scores'], figsize=(5,5))","d1409381":"def get_ax(rows=1, cols=1, size=16):\n    \"\"\"Return a Matplotlib Axes array to be used in\n    all visualizations in the notebook. Provide a\n    central point to control graph sizes.\n    \n    Adjust the size attribute to control how big to render images\n    \"\"\"\n    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n    return ax","1ee49a29":"for i in range(5):\n#     img = cv2.imread(f'{IMAGE_DIR}\/{path}')\n    image_id = random.choice(valid_dataset.image_ids)\n    image, image_meta, gt_class_id, gt_bbox, gt_mask = modellib.load_image_gt(valid_dataset, config, image_id)\n    info = valid_dataset.image_info[image_id]\n    \n    print(\"image ID: {}.{} ({}) {}\".format(info[\"source\"], info[\"id\"], image_id, \n                                       dataset.image_reference(image_id)))\n    # Run object detection\n    results = model.detect([image], verbose=1)\n\n    # Display results\n    ax = get_ax(1)\n    r = results[0]\n    visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n                                valid_dataset.class_names, r['scores'], ax=ax,\n                                title=\"Predictions\", figsize=(5,5))\n    log(\"gt_class_id\", gt_class_id)\n    log(\"gt_bbox\", gt_bbox)\n    log(\"gt_mask\", gt_mask)","b935f6b5":"# image_id = random.choice(dataset.image_ids)\nimage_id = 307\nimage, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n    modellib.load_image_gt(dataset, config, image_id)\ninfo = dataset.image_info[image_id]\nprint(\"image ID: {}.{} ({}) {}\".format(info[\"source\"], info[\"id\"], image_id, \n                                       dataset.image_reference(image_id)))\n# Run object detection\nresults = model.detect([image], verbose=1)\n\n# Display results\nax = get_ax(1)\nr = results[0]\nvisualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n                            dataset.class_names, r['scores'], ax=ax, figsize=(5,5),\n                            title=\"Predictions\")\nlog(\"gt_class_id\", gt_class_id)\nlog(\"gt_bbox\", gt_bbox)\nlog(\"gt_mask\", gt_mask)","c909b645":"import skimage.io\n\nimage = skimage.io.imread(f'{IMAGE_DIR}\/{image_df.loc[307][\"path\"]}')\nresults = model.detect([image], verbose=1)\nr = results[0]\nvisualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n                            dataset.class_names, r['scores'])","a4214310":"sys.path.append('samples\/coco\/')\nimport coco","b1d213e6":"class InfConfig(coco.CocoConfig):\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n    \ncocoConf = InfConfig()\ncocoConf.display()","4d6a9f55":"# Create model object in inference mode.\nmodel_coco = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=cocoConf)\n\n# Load weights trained on MS-COCO\nmodel_coco.load_weights(COCO_WEIGHTS_PATH, by_name=True)","e53d6fde":"# COCO Class names\n# Index of the class in the list is its ID. For example, to get ID of\n# the teddy bear class, use: class_names.index('teddy bear')\nclass_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',\n               'bus', 'train', 'truck', 'boat', 'traffic light',\n               'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',\n               'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',\n               'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',\n               'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n               'kite', 'baseball bat', 'baseball glove', 'skateboard',\n               'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n               'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n               'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n               'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',\n               'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',\n               'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',\n               'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',\n               'teddy bear', 'hair drier', 'toothbrush']","fca2a8ab":"import skimage.io\n\nimage = skimage.io.imread(f'{IMAGE_DIR}\/{image_df.loc[307][\"path\"]}')\n# image = skimage.io.imread(f'{IMAGE_DIR}\/ds8_pexels-photo-331713.png')\nresults = model_coco.detect([image], verbose=1)\nr = results[0]\nvisualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n                            class_names, r['scores'])","30f853fc":"TEST_IMAGE_DIR = f'\/kaggle\/input\/segmentation-full-body-mads-dataset\/segmentation_full_body_mads_dataset_1192_img\/images'\nTEST_MASKS_DIR = f'\/kaggle\/input\/segmentation-full-body-mads-dataset\/segmentation_full_body_mads_dataset_1192_img\/masks'","4decc052":"image_paths = []\nfor dirname, _, filenames in os.walk(TEST_IMAGE_DIR):\n    for filename in filenames:\n        image_paths.append(filename)\n        \ntest_df = pd.DataFrame(image_paths, columns=['path'])\ntest_df = test_df.sort_values(by=['path'], ignore_index=True)\ntest_df.head()","0253871e":"for i, row in test_df.sample(15, random_state=42).iterrows():\n    path = row['path']\n    image = skimage.io.imread(f'{TEST_IMAGE_DIR}\/{path}')\n    results = model.detect([image], verbose=1)\n    r = results[0]\n    visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n                                dataset.class_names, r['scores'])","bcd51509":"for i, row in test_df.sample(15, random_state=42).iterrows():\n    path = row['path']\n    image = skimage.io.imread(f'{TEST_IMAGE_DIR}\/{path}')\n    results = model_coco.detect([image], verbose=1)\n    r = results[0]\n    visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n                                class_names, r['scores'])","f79f1eff":"# Training","96e5ffc3":"# Predict","ca657a74":"# Mask-R-CNN","fb069e5c":"## Custom Model","b1787e7c":"## Images","0e953c32":"## Training and Inference","b5dbb342":"# Sample with COCO Dataset","2583a5bd":"## COCO Model","a13ada68":"## Run Length Encoding\n\nFirst show the mask and then encode all masks with run length encoding.","703d485c":"## Run detection","14045e25":"## Configuration\nDefine the configuration for training on the Supervisely dataset. ","16f4314a":"### Define the dataset"}}