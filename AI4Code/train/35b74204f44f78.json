{"cell_type":{"9a50aa86":"code","cdfae810":"code","50d58abf":"code","5c9b06e2":"code","0bf61d77":"code","cccb33b7":"code","cc7222cb":"code","03e5c899":"code","31cc5a0a":"code","3b809cfe":"code","b4256ee3":"code","b43692cc":"code","0228f366":"code","73a67b3c":"code","6c239067":"code","4f7646f3":"code","19ba01ba":"code","02d06b91":"markdown","77343480":"markdown","000ac7ee":"markdown","758aa8b9":"markdown","8cf070c8":"markdown","34af3732":"markdown","a6cc5db6":"markdown","59adbcf5":"markdown","d95df81c":"markdown","796572dd":"markdown","68833d1c":"markdown","aa9fb9b4":"markdown","3ab5220d":"markdown"},"source":{"9a50aa86":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cdfae810":"df = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ndf.head(5)","50d58abf":"data = df.drop(['label'],axis=1)\nlabels = df['label']","5c9b06e2":"labels.unique()","0bf61d77":"data = data.to_numpy()\nlabels = labels.to_numpy()","cccb33b7":"data.shape,labels.shape","cc7222cb":"from sklearn.model_selection import train_test_split\ntrain_data , val_data , train_labels , val_labels = train_test_split(data,labels,test_size=0.05)","03e5c899":"train_data.shape,val_data.shape","31cc5a0a":"train_data = train_data.reshape(39900,28,28,1)\nval_data = val_data.reshape(2100,28,28,1)\ntrain_data = train_data\/255\nval_data = val_data\/255","3b809cfe":"import tensorflow as tf\nfrom tensorflow import keras","b4256ee3":"class myCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self,epoch,logs={}):\n        if(logs.get('val_accuracy')>0.99):\n            print('Reached 99% validation accuracy,stopping training')\n            self.model.stop_training = True\ncallbacks = myCallback()","b43692cc":"model = keras.Sequential([\n    keras.layers.Conv2D(32,(3,3),activation='relu',input_shape=(28,28,1)),\n    keras.layers.MaxPooling2D(2,2),\n    keras.layers.Conv2D(32,(3,3),activation='relu'),\n    keras.layers.MaxPooling2D(2,2),\n    keras.layers.Flatten(),\n    keras.layers.Dense(256,activation='relu'),\n    keras.layers.Dense(10,activation='softmax')\n])\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nmodel.summary()","0228f366":"model.fit(train_data,train_labels,epochs=50,validation_data=(val_data,val_labels),callbacks=[callbacks])","73a67b3c":"test_data = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\ntest_data","6c239067":"test_data = test_data.to_numpy()\ntest_data = test_data.reshape(28000,28,28,1)\ntest_data = test_data \/ 255\npredictions = model.predict(test_data)\npredictions = np.argmax(predictions,axis=1)\npredictions","4f7646f3":"submission = pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')\nsubmission['Label'] = predictions","19ba01ba":"submission.to_csv('mnist_digit_test_predictions.csv',index=False)","02d06b91":"- ## Data Preparation","77343480":"- ## Reading the test dataset","000ac7ee":"- ## Data Preparation for Test set and making predictions on it","758aa8b9":"- ## Callback function for our model","8cf070c8":"- ## Separating the labels from dataset","34af3732":"- ## Creating a Pandas DataFrame with our predictions in it","a6cc5db6":"- ## Exporting our dataframe to CSV file format","59adbcf5":"# `Using Keras Sequential API to create a model that classfies the MNIST Dataset with 99% accuracy on validation set`","d95df81c":"- ## Importing Tensorflow and Keras","796572dd":"- ## Reading the Train data","68833d1c":"# The End\n`If you liked the notebook then don't forget to upvote and suggestions are always welcomed.`\n`Follow me on Linkedin :` __[Atharva_Dumbre](https:\/\/www.linkedin.com\/in\/atharva-dumbre-208b5716b)__","aa9fb9b4":"- ## Training the model on training data","3ab5220d":"- ## Model Creation using Sequential API from Keras"}}