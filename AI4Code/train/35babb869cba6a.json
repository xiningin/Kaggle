{"cell_type":{"80622f30":"code","57c707ad":"code","f7cb9f04":"code","9319e714":"code","f5e563bc":"code","be437c3b":"code","0875c395":"code","f6a6321d":"code","aed85848":"code","2e76ca38":"code","6e925553":"code","8f1987d9":"code","a5d73c0a":"code","2c45e99a":"code","600a1ab6":"code","31775a24":"code","ba0959fa":"code","d5954446":"code","d1c736cb":"code","c9471960":"code","5870e23e":"code","a60b8248":"code","b3c405b0":"code","e099be6e":"code","9819b8d8":"code","393f56c4":"code","0e144dd1":"code","679e61e2":"code","3eb03d58":"code","6a655761":"code","3a742fd4":"code","dad2b781":"code","87f32d92":"code","942c0806":"code","30c01360":"code","bbcfc7df":"code","1f51f7f3":"code","18d4d810":"code","b9da8651":"code","5ff5266f":"code","48db0c7e":"code","3a8307ba":"code","486a28e4":"code","84b775f6":"code","7306d789":"code","0847790f":"code","2bbbe6b1":"code","a41e84d3":"code","a4e52c4c":"code","19c9ddef":"code","d1620f9d":"code","00dfaef7":"code","576cc60b":"code","a751db63":"code","1c0d781c":"code","9a19bd17":"code","8996d3dc":"code","3a014649":"code","7ac9a0e1":"code","4dba08e4":"code","48e5df2c":"code","cc67d9a2":"code","e6db5d34":"code","19b00952":"code","896bea4b":"code","4319cab3":"code","d81f7e0a":"code","069b1a7d":"code","4ee2a68f":"code","7f78eb8b":"code","e5955f09":"code","37354046":"code","6b99eefd":"code","9ac33d6d":"code","e93c9f77":"code","18dd5e5a":"code","e9a286ed":"code","521df4ce":"code","a13e397b":"code","5357e758":"code","3e5edb86":"code","387e541b":"markdown","bb0a7430":"markdown","d999e366":"markdown","4bb50907":"markdown","4783d20a":"markdown","e6306a3f":"markdown","f7fbd0f0":"markdown","4d1f2dc0":"markdown","b563c5da":"markdown","de7010ab":"markdown","4fe546a9":"markdown","198f7b01":"markdown","4151d30a":"markdown","06b142c9":"markdown","ca091da8":"markdown","4374d066":"markdown","2e6076f1":"markdown","be599a52":"markdown","541e5f49":"markdown","d0dbf9d2":"markdown","170cbba6":"markdown","4377406e":"markdown","5ec2fc66":"markdown","a8d2ca6f":"markdown","ebf358bf":"markdown","f4d53646":"markdown","34a5bc7a":"markdown","1d994142":"markdown","6aef4347":"markdown","235e6054":"markdown","e3ceddcd":"markdown","2586549a":"markdown","863a4d6a":"markdown","28cd40a9":"markdown","ce6606ae":"markdown","27d04fe6":"markdown","2bfb1cb9":"markdown","ffc35812":"markdown","ec647d93":"markdown","888aeaf4":"markdown","09d6f083":"markdown","976b9a41":"markdown","bcafc0ec":"markdown","c6400331":"markdown","86c1b53f":"markdown","5f99077a":"markdown","4341dccb":"markdown","9b757c3a":"markdown","ec8760af":"markdown","46eda0e7":"markdown","e001a1bd":"markdown","18987031":"markdown","0061aeb6":"markdown","505d197b":"markdown","e0f6a984":"markdown","e8e5c269":"markdown","179b7890":"markdown","f1b57737":"markdown","18a33cc1":"markdown","4e95eceb":"markdown","e2277ab4":"markdown","f468119a":"markdown","64eecc91":"markdown","d46b1758":"markdown","01964382":"markdown","66172eb0":"markdown","e2cefc5a":"markdown"},"source":{"80622f30":"import warnings\nwarnings.filterwarnings('ignore')\n\n#import sidetable as stb\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nprint('numpy version : ',np.__version__)\nprint('pandas version : ',pd.__version__)\nprint('seaborn version : ',sns.__version__)","57c707ad":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f7cb9f04":"!pip install openpyxl","9319e714":"# load dataset\ndf = pd.read_excel('\/kaggle\/input\/trending-youtube-video-statistics\/youtube_statistics.xlsx')\n\n# show five random data\ndf.sample(5)","f5e563bc":"df.info()","be437c3b":"#Numeric\nnumeric = df.loc[:, (df.dtypes == int) | (df.dtypes == float)].columns.tolist()\nnumeric","0875c395":"#Categorical\ncategorical = df.loc[:, (df.dtypes != int) & (df.dtypes != float)].columns.tolist()\ncategorical","f6a6321d":"df.sample(5)","aed85848":"df[numeric].describe().apply(lambda x: x.apply('{0:.5f}'.format))","2e76ca38":"df[categorical].describe()","6e925553":"df_clean = df.copy()","8f1987d9":"data_missing_value = df_clean.isnull().sum().reset_index()\ndata_missing_value.columns = ['feature','missing_value']\ndata_missing_value['percentage'] = round((data_missing_value['missing_value']\/len(df_clean))*100,2)\ndata_missing_value = data_missing_value.sort_values('percentage', ascending=False).reset_index(drop=True)\ndata_missing_value = data_missing_value[data_missing_value['percentage']>0]\ndata_missing_value","a5d73c0a":"col_missing_value = data_missing_value['feature'].tolist() \ndf_clean[col_missing_value].info()","2c45e99a":"df_clean.dropna(subset=['description'], inplace=True)","600a1ab6":"data_missing_value = df_clean.isnull().sum().reset_index()\ndata_missing_value.columns = ['feature','missing_value']\ndata_missing_value","31775a24":"# Changing trending_date to datetime type: \ndf_clean['trending_date'] = pd.to_datetime(df_clean['trending_date'])","ba0959fa":"# Changing publish_time to datetime type: \ndf_clean['publish_time'] = pd.to_datetime(df_clean['publish_time'])","d5954446":"df_clean[\"comments_disabled\"] = df_clean[\"comments_disabled\"].astype(int)","d1c736cb":"df_clean[\"ratings_disabled\"] = df_clean[\"ratings_disabled\"].astype(int)","c9471960":"df_clean[\"video_error_or_removed\"] = df_clean[\"video_error_or_removed\"].astype(int)","5870e23e":"df_clean.info()","a60b8248":"weekend = ['Saturday', 'Sunday']","b3c405b0":"df_clean['publish_day'] = df_clean['publish_date'].dt.day_name()","e099be6e":"df_clean['publish_daytype'] = df_clean['publish_day'].apply(lambda x: 'Weekend' if x in weekend else 'Weekday')","9819b8d8":"df_clean['publish_trending_interval'] = (df_clean['trending_date'] - df_clean['publish_date']).dt.days","393f56c4":"df_clean.sample(5)","0e144dd1":"df_clean.info()","679e61e2":"df_clean.duplicated().sum()","3eb03d58":"print(str((df_clean.duplicated().sum() \/ len(df_clean)) * 100) + ' %')","6a655761":"#dropping duplicate data\ndf_clean = df_clean.drop_duplicates()","3a742fd4":"print('Total Rows', df_clean.shape[0], ', Total Features', df_clean.shape[1],'\\n')","dad2b781":"df_clean = df_clean.drop(['publish_date','publish_time','description','tags','title','channel_title','trending_date'], axis=1)","87f32d92":"df_clean = df_clean.drop(['publish_day'], axis=1)","942c0806":"df_clean.info()","30c01360":"print('Total Rows', df_clean.shape[0], ', Total Features', df_clean.shape[1],'\\n')","bbcfc7df":"encode_cat = ['publish_daytype']","1f51f7f3":"for cat in encode_cat:\n    onehots = pd.get_dummies(df_clean[cat], prefix=cat)\n    df_clean = df_clean.join(onehots)","18d4d810":"df_clean = df_clean.drop(['publish_daytype'], axis=1)","b9da8651":"df_clean","5ff5266f":"#Numeric\nnumeric = df_clean.loc[:, (df_clean.dtypes == int) | (df_clean.dtypes == float)].columns.tolist()\nnumeric","48db0c7e":"#Categorical\ncategorical = df_clean.loc[:, (df_clean.dtypes != int) & (df_clean.dtypes != float) & (df_clean.columns != 'publish_daytype') & ((df_clean.columns != 'trending_daytype'))].columns.tolist()\ncategorical","3a8307ba":"# boxplot visualization for numerical data\n\nplt.figure(figsize=(15, 25))\nfor i in range(0, len(numeric)):\n    plt.subplot(12, 2, i+1)\n    sns.boxplot(x=df_clean[numeric[i]], color='orange')\n    plt.tight_layout()\n\nplt.show()","486a28e4":"# distribution plot for numerical data\n\nplt.figure(figsize=(10, 30))\nfor i in range(0, len(numeric)):\n    plt.subplot(12, 2,i+1)\n    sns.distplot(df_clean[numeric[i]], color='orange')\n    plt.tight_layout()","84b775f6":"plt.figure(figsize=(10, 10))\nfor i in range(0, len(categorical)):\n    plt.subplot(2, len(categorical), i+1)\n    sns.countplot(df_clean[categorical[i]])\n    plt.tight_layout()","7306d789":"!pip install dython","0847790f":"from dython.nominal import associations\n\nassociations(df_clean, figsize = (15, 15))\nplt.show()","2bbbe6b1":"plt.figure(figsize=(15, 15))\nsns.pairplot(df_clean, diag_kind='kde')\nplt.show()","a41e84d3":"print('Number of rows before filtering outliers:', len(df_clean))\n\nfiltered_entries = np.array([True] * len(df_clean))\nfor col in ['views', 'likes', 'dislikes', 'comment_count', 'No_tags', 'desc_len', 'publish_trending_interval']:\n    Q1 = df_clean[col].quantile(0.25)\n    Q3 = df_clean[col].quantile(0.75)\n    IQR = Q3 - Q1\n    low_limit = Q1 - (1.5 * IQR)\n    high_limit = Q3 + (1.5 * IQR)\n    \n    filtered_entries = ((df_clean[col] >= low_limit) & (df_clean[col] <= high_limit))\n\ndf_clean = df_clean[filtered_entries]\nprint('Number of rows after filtering outliers:', len(df_clean))","a4e52c4c":"# before scaling\ndf_clean.describe().apply(lambda x: x.apply('{0:.5f}'.format))","19c9ddef":"# Normalization\nfrom sklearn.preprocessing import MinMaxScaler\n\ndf_clean['category_id_norm'] = MinMaxScaler().fit_transform(df_clean['category_id'].values.reshape(len(df_clean), 1))\ndf_clean['views_norm'] = MinMaxScaler().fit_transform(df_clean['views'].values.reshape(len(df_clean), 1))\ndf_clean['likes_norm'] = MinMaxScaler().fit_transform(df_clean['likes'].values.reshape(len(df_clean), 1))\ndf_clean['dislikes_norm'] = MinMaxScaler().fit_transform(df_clean['dislikes'].values.reshape(len(df_clean), 1))\ndf_clean['comment_count_norm'] = MinMaxScaler().fit_transform(df_clean['comment_count'].values.reshape(len(df_clean), 1))\ndf_clean['No_tags_norm'] = MinMaxScaler().fit_transform(df_clean['No_tags'].values.reshape(len(df_clean), 1))\ndf_clean['desc_len_norm'] = MinMaxScaler().fit_transform(df_clean['desc_len'].values.reshape(len(df_clean), 1))\ndf_clean['len_title_norm'] = MinMaxScaler().fit_transform(df_clean['len_title'].values.reshape(len(df_clean), 1))\ndf_clean['publish_trending_interval_norm'] = MinMaxScaler().fit_transform(df_clean['publish_trending_interval'].values.reshape(len(df_clean), 1))","d1620f9d":"# after scaling\ndf_clean.describe().apply(lambda x: x.apply('{0:.5f}'.format))","00dfaef7":"from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\ndef evaluation_regression(model, y_pred, x_train, y_train, x_test, y_test):\n    print(\"MAE: %.2f\" % mean_absolute_error(y_test, y_pred))\n    print(\"RMSE: %.2f\" % mean_squared_error(y_test, y_pred, squared=False))\n    print(\"R2 score: %.2f\" % r2_score(y_test, y_pred))\n\ndef show_best_hyperparameter(model, hyperparameters):\n    for key, value in hyperparameters[0].items() :\n        print('Best '+key+':', model.get_params()[key])\n\ndef show_feature_importance(model):\n  feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n  ax = feat_importances.nlargest(25).plot(kind='barh', figsize=(10, 8))\n  ax.invert_yaxis()\n\n  plt.xlabel('score')\n  plt.ylabel('feature')\n  plt.title('feature importance score')","576cc60b":"df_clean.columns","a751db63":"# Split Feature and Label\nX = df_clean[['category_id_norm', 'likes_norm', 'dislikes_norm',\n       'comment_count_norm', 'No_tags_norm', 'desc_len_norm', 'len_title_norm',\n       'publish_trending_interval_norm','publish_daytype_Weekday', 'publish_daytype_Weekend']]\n\ny = df_clean['views_norm'] # target \/ label\n\n#Splitting the data into Train and Test\nfrom sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)","1c0d781c":"# fitting regression model to training set\nfrom sklearn.linear_model import LinearRegression\n\nlr_model = LinearRegression()\nlr_model.fit(X_train, y_train)\n\ny_pred_lr = lr_model.predict(X_test)","9a19bd17":"# evaluation\n\nprint('Coefficients:\\n', lr_model.coef_, end='\\n\\n')\nprint('Intercept:\\n', lr_model.intercept_, end='\\n\\n')\n\nevaluation_regression(lr_model, y_pred_lr, X_train, y_train, X_test, y_test)","8996d3dc":"mae_lr = mean_absolute_error(y_test, y_pred_lr)\nrmse_lr = mean_squared_error(y_test, y_pred_lr)\nr2_lr = r2_score(y_test, y_pred_lr)\ntrain_lr = lr_model.score(X_train, y_train)\ntest_lr = lr_model.score(X_test, y_test)","3a014649":"print('Train Accuracy:', train_lr)\nprint('Test Accuracy:', test_lr)","7ac9a0e1":"from sklearn.linear_model import ElasticNet\n\nen_model = ElasticNet()\nen_model.fit(X_train, y_train)\n\ny_pred_en = en_model.predict(X_test)","4dba08e4":"evaluation_regression(en_model, y_pred_en, X_train, y_train, X_test, y_test)","48e5df2c":"mae_en = mean_absolute_error(y_test, y_pred_en)\nrmse_en = mean_squared_error(y_test, y_pred_en)\nr2_en = r2_score(y_test, y_pred_en)\ntrain_en = en_model.score(X_train, y_train)\ntest_en = en_model.score(X_test, y_test)","cc67d9a2":"print('Train Accuracy:', train_en)\nprint('Test Accuracy:', test_en)","e6db5d34":"from sklearn.tree import DecisionTreeRegressor\n\ndt_model = DecisionTreeRegressor()\ndt_model.fit(X_train, y_train)\ny_pred_dt = dt_model.predict(X_test)","19b00952":"evaluation_regression(dt_model, y_pred_dt, X_train, y_train, X_test, y_test)","896bea4b":"mae_dt = mean_absolute_error(y_test, y_pred_dt)\nrmse_dt = mean_squared_error(y_test, y_pred_dt)\nr2_dt = r2_score(y_test, y_pred_dt)\ntrain_dt = dt_model.score(X_train, y_train)\ntest_dt = dt_model.score(X_test, y_test)","4319cab3":"print('Train Accuracy:', train_dt)\nprint('Test Accuracy:', test_dt)","d81f7e0a":"from sklearn.ensemble import RandomForestRegressor\n\nrf_model = RandomForestRegressor()\nrf_model.fit(X_train, y_train)\ny_pred_rf = rf_model.predict(X_test)","069b1a7d":"evaluation_regression(rf_model, y_pred_rf, X_train, y_train, X_test, y_test)","4ee2a68f":"mae_rf = mean_absolute_error(y_test, y_pred_rf)\nrmse_rf = mean_squared_error(y_test, y_pred_rf)\nr2_rf = r2_score(y_test, y_pred_rf)\ntrain_rf = rf_model.score(X_train, y_train)\ntest_rf = rf_model.score(X_test, y_test)","7f78eb8b":"print('Train Accuracy:', train_rf)\nprint('Test Accuracy:', test_rf)","e5955f09":"from sklearn.svm import SVR\n\nsvr_model = SVR()\nsvr_model.fit(X_train, y_train)\ny_pred_svr = svr_model.predict(X_test)","37354046":"evaluation_regression(svr_model, y_pred_svr, X_train, y_train, X_test, y_test)","6b99eefd":"mae_svr = mean_absolute_error(y_test, y_pred_svr)\nrmse_svr = mean_squared_error(y_test, y_pred_svr)\nr2_svr = r2_score(y_test, y_pred_svr)\ntrain_svr = svr_model.score(X_train, y_train)\ntest_svr = svr_model.score(X_test, y_test)","9ac33d6d":"print('Train Accuracy:', train_svr)\nprint('Test Accuracy:', test_svr)","e93c9f77":"\nevaluation_summary = {\n    'Linear Regression': [mae_lr, rmse_lr, r2_lr],\n    'Elastic Net':[mae_en, rmse_en, r2_en],\n    'Decision Tree':[mae_dt, rmse_dt, r2_dt],\n    'Random Forest':[mae_rf, rmse_rf, r2_rf],\n    'Support Vector Regressor':[mae_svr, rmse_svr, r2_svr]\n}\n\neva_sum = pd.DataFrame(data = evaluation_summary, index = ['MAE', 'RMSE', 'R2 Score'])\neva_sum\n","18dd5e5a":"evaluation_sum_train_test = {\n    \"train\" : [train_lr, train_en, train_dt, train_rf, train_svr],\n    \"test\": [test_lr, test_en, test_dt, test_rf, test_svr]\n}\n\neva_sum_train_test = pd.DataFrame(data = evaluation_sum_train_test, index = ['Linear Regression', 'Elastic Net', 'Decision Tree', 'Random Forest', 'Support Vector Regressor'])\neva_sum_train_test","e9a286ed":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n\nparam_grid = [\n{'n_estimators': [10, 25], 'max_features': [5, 10], \n 'max_depth': [10, 50, None], 'bootstrap': [True, False]}\n]\n\nrf_tuned = RandomForestRegressor()\n\nrf_tuned_model = GridSearchCV(rf_tuned, param_grid, cv=10, scoring='neg_root_mean_squared_error')\nrf_tuned_model.fit(X_train, y_train)","521df4ce":"y_pred_rf_tuned = rf_tuned_model.predict(X_test)","a13e397b":"show_best_hyperparameter(rf_tuned_model.best_estimator_, param_grid)","5357e758":"evaluation_regression(rf_tuned_model, y_pred_rf_tuned, X_train, y_train, X_test, y_test)","3e5edb86":"show_feature_importance(rf_model)","387e541b":"Features that are very influential in predicting the number of views are `likes` and `dislikes`","bb0a7430":"## Setup Train Test Data","d999e366":"# Data Pre-Processing","4bb50907":"Some observations from the statistical summary of the numeric column above:\n\n* A total of 8 numeric columns\n* For numeric columns, the majority appear to have a skewed distribution (Mean <> Median). There may be outliers\n* Seen from the target column `views`, the range of views achieved in this dataset ranges from 4,024 to 125,432,237. With the midpoint at 307.836","4783d20a":"Removed the columns `publish_date`, `publish_time`, `description`, `tags`, `title`, `channel_title` because these features have their own unique values for each existing video","e6306a3f":"# Initialize df_clean","f7fbd0f0":"Missing value is only found in the `description` column with a total of 45 rows of data or about 0.12% of the total data","4d1f2dc0":"From the results of the model selection, we try to do hyperparameter tuning on Random Forest","b563c5da":"Assumptions > 0.5 have a fairly strong correlation:\n- Correlation of `views` with `comment_count` = 0.7\n- Correlation of `views` with `dislikes` = 0.55\n- Correlation of `views` with `likes` = 0.85\n\n- Correlation of `likes` with `comment_count` = 0.79\n- Correlation of `dislikes` with `comment_count` = 0.74","de7010ab":"To find the type of day (Weekday or Weekend), we first get the data on the day the video was uploaded","4fe546a9":"## Statistical summary dengan df.describe()","198f7b01":"## Checking Columns and Missing Values With df.info()","4151d30a":"# Conclusion","06b142c9":"It can be seen from the sample above, between the columns and their values there is nothing strange and in accordance with the data definition","ca091da8":"### Publish Day Type","4374d066":"Change the data type in the `trending_date` and `publish_time` columns from *object* to *datetime*","2e6076f1":"* Based on the experiments that have been carried out, the model chosen to determine Youtube video views is **Tuned Random Forest** with MAE 0.00, RMSE 0.01 and R2 of 0.95 (the best model so far)\n* Features that are very influential in predicting the number of views are `likes` and `dislikes`","be599a52":"## Adjust Data Type","541e5f49":"## Sampling to Understand Data With df.sample()","d0dbf9d2":"## Normalization","170cbba6":"Most videos are published on weekdays","4377406e":"## Univariate Analysis","5ec2fc66":"Change the data type in the bool column (`comments_disabled`, `ratings_disabled`, `video_error_or_removed`) to integer, so that the value becomes 0 or 1","a8d2ca6f":"### Publish Trending Interval (Day)","ebf358bf":"* Skewed positive distribution: `views`, `likes`, `dislikes`, `comment_count`, `No_tags`, `desc_len`, `publish_trending_interval`\n* Skewed negative distribution: `len_title`","f4d53646":"## Random Forest","34a5bc7a":"## Random Forest Tuning ","1d994142":"# Initialized Data","6aef4347":"## Support Vector Regressor","235e6054":"Because only 0.12% of the data is missing in the `description` column, then we drop these rows","e3ceddcd":"From the data type information:\n\n* For column `trending_date` it should be *datetime* because it is date data\n* For column `publish_time` it should be *datetime* because it is time data\n* For bool column will be adjusted to values 0 and 1\n* Data type adjustments will be made in the *Adjust Data Type stage*\n\nThen we can see that:\n\n* The dataframe has a total of 36,791 rows and 18 columns\n* Missing value is visible in the `description` column\n* The regression target seems to be the `views` column with data type int64.\nColumns other than `views` are features\n* From the information above, we can separate the categorical and numeric columns as follows:","2586549a":"Because the column `trending_date` or `publish_date` is time data that will not be used for modeling, so a new column is created to accommodate the time interval difference between these two times. Because the interval distance is what is needed to get the number of views","863a4d6a":"Outliers are in the columns: `views`, `likes`, `dislikes`, `comment_count`, `No_tags`, `desc_len`, `publish_trending_interval`","28cd40a9":"## Linear Regression","ce6606ae":"## Duplicate Values","27d04fe6":"## Feature Importance","2bfb1cb9":"## EDA Conclusion","ffc35812":"Seen a lot of data that is still duplicate as much as 4,228 rows of data or 11% of the total data","ec647d93":"## Multivariate Analysis","888aeaf4":"Because the distribution is not normal, the IQR method is used to overcome outliers","09d6f083":"## Handle Missing Value","976b9a41":"In a pairplot, it can be seen that there is a linear correlation between the `likes` column and the `views` target target","bcafc0ec":"Some of the things we found from this EDA dataset are:\n\n* The data looks valid and there are no defects\n* The majority of numeric columns have a skewed distribution, this must be remembered if we want to do something or use a model that requires the assumption of a normal distribution\n* Outliers are in the columns: `views`, `likes`, `dislikes`, `comment_count`, `No_tags`, `desc_len`, `publish_trending_interval`. We will try to do the outliers cleaning process in the *Pre-Processing Part 2* stage\n* The target column `views` has a strong positive correlation with `comment_count`, `dislikes`, `likes`. Will be used for modeling\n* The `likes` and also `dislikes` columns have a strong positive correlation with `comment_count`. However, because the feature is considered important, it will still be used for modeling","c6400331":"# EDA","86c1b53f":"## Decision Tree","5f99077a":"## Evaluation Summary","4341dccb":"Normalizing so that the scale of each numerical feature has the same scale and is expected to simplify the process of learning the machine learning model data that we created.","9b757c3a":"##  Data Definition","ec8760af":"Feature name  | Description\n-------------------|-------------\ntrending_date|the date when the video was trending\ntitle|video title\nchannel_title|channel name\ncategory_id|video category in encoding label\npublish_time|video publish time\ntags|tags used in video\nviews|number of video views\nlikes|number of likes video\ndislikes|number of video dislikes\ncomment_count|the number of comments on the video\ncomments_disabled|is comments status disabled on videos\nratings_disabled|whether ratings are disabled on videos\nvideo_error_or_removed|is the video error or has it been removed now\ndescription|video description\nNo_tags|number of tags used\ndesc_len|length of the video description\nlen_title|video title length\npublish_date|video publish date","46eda0e7":"Some observations from the statistical summary of the categorical column above:\n\n* A total of 10 categorical columns\n* The majority of columns have a large number of uniques, except for data with a boolean data type, namely `comments_disabled`, `ratings_disabled`, `video_error_or_removed`\n* Seen from the `publish_date` column, the videos in this dataset are videos uploaded from 2017-05-27 to 2018-06-13. Most videos uploaded on 2018-05-17 (219 videos)","e001a1bd":"## Feature Engineering","18987031":"# Import Libraries","0061aeb6":"Added a new column to store the type of day when the video was first published (Weekday or Weekend)","505d197b":"# Load and Describe Data","e0f6a984":"## Outliers","e8e5c269":"After the Random Forest tuning experiment, the results of the R2 Score increased slightly from before","179b7890":"## Redefine Numeric and Categorical","f1b57737":"## Elastic Net","18a33cc1":"## Feature Engineering - Categorical","4e95eceb":"# Modelling","e2277ab4":"## Check Null and Missing Values","f468119a":"Removed the `publish_day` column, because we only need the `publish_daytype` column ","64eecc91":"# Project\n\nPredict views on Youtube videos\n\nDescription:\n\nPredict views on youtube videos by using statistical numbers or other attributes on the video","d46b1758":"Looks like the best modeling is in *Random Forest*:\n* MAE and RMSE values are smaller than others. <br>\n* Then the R2 Score is closer to 1 than the others.\n* For the *Random Forest* model, it is also not underfit or overfit","01964382":"# Data Pre-Processing Part 2","66172eb0":"## Remove Irrelevant Columns","e2cefc5a":"## Load Data"}}