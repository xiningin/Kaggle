{"cell_type":{"6d6ba2ab":"code","419bb00e":"code","b8fda933":"code","cad79b25":"code","a98e3f6e":"code","f6a66675":"code","d2be49f7":"markdown","4363001a":"markdown"},"source":{"6d6ba2ab":"import sklearn.metrics as skmetrics\nimport tensorflow.keras.metrics as tf2metrics\nimport tensorflow.compat.v1 as tf1\nimport numpy as np\nimport pandas as pd","419bb00e":"def sk_auc_pr(y_true, y_prob):\n  precisions, recalls, thresholds = skmetrics.precision_recall_curve(y_true, y_prob)\n  return skmetrics.auc(recalls, precisions)","b8fda933":"def tf1_auc_pr_careful(y_true, y_prob):\n  with tf1.Session() as sess:\n    metric_val, update_op = tf1.metrics.auc(y_true, y_prob, curve=\"PR\",\n                                            summation_method=\"careful_interpolation\")\n    sess.run(tf1.local_variables_initializer())\n    sess.run(update_op)\n    return sess.run(metric_val)\n\ndef tf1_auc_pr_trapezoidal(y_true, y_prob):\n  with tf1.Session() as sess:\n    metric_val, update_op = tf1.metrics.auc(y_true, y_prob, curve=\"PR\")\n    sess.run(tf1.local_variables_initializer())\n    sess.run(update_op)\n    return sess.run(metric_val)","cad79b25":"def tf2_auc_pr(y_true, y_prob):\n  m = tf2metrics.AUC(curve=\"PR\")\n  m.update_state(y_true, y_prob)\n  return m.result().numpy()","a98e3f6e":"y_true = np.array([0, 0, 1, 1])\ny_prob = np.array([0.1, 0.4, 0.35, 0.8])","f6a66675":"df_stats = pd.DataFrame()\n\nfor f in [sk_auc_pr, tf1_auc_pr_careful, tf1_auc_pr_trapezoidal, tf2_auc_pr]:\n  df_stats = df_stats.append({\n      \"Method\": f.__name__,\n      \"AUC PR\": f(y_true, y_prob)\n  }, ignore_index=True)\n\ndf_stats[[\"Method\", \"AUC PR\"]]","d2be49f7":"As we can see, `sk_auc_pr` is basically identical with `tf1_auc_pr_trapezoidal`, while `tf2_auc_pr` is identical with `tf1_auc_pr_careful`.\nThis means that `sk_auc_pr` and `tf1_auc_pr` are all using trapezoidal summation method (by default), while `tf2_auc_pr` are using careful summation method by default.\nThe careful summation method is more recommended.","4363001a":"In this notebook, we will show how to compute AUC-PR (Area under Curve - Precision-Recall) metric in sklearn, tf1 and tf2.\n\nThe goal is both to explain their APIs, as well as comparing their difference when used with different parameters.\n\n![image.png](attachment:image.png)\n\nAbove is an example of PR curve. As you can see, for a multi-class classification task, there are *per-class PR curves*,\nas well as their *micro-average PR Curve*. If you are more interested in the performance for a specific class, you can\nfocus on its per-class metric. Otherwise, you can focus on its micro\/macro average.\n\n**How is per-class PR Curve plotted:** Basically we enumerate a *threshold* $t$ from 0 to 1 (for example, 0, 0.1, ...., 1.0), for each example's prediction score (probability) $y_i \\in y_\\text{pred}$,\nif $y_i > t$, we consider this example as a positive example, otherwise, it is considered as a negative example.\nBy comparing with the ground-truths $y_\\text{true}$, we can compute precision and recall (see https:\/\/en.wikipedia.org\/wiki\/Precision_and_recall) for definitions.\nWe can plot one (precision, recall) pair for each $t$, which eventually becomes PR-curve.\n\nBy computing the areas below the PR-curve, we have AUC (Area Under Curve).\n\nComputing PR curve is affected by many parameters as well. Here we will focus on one parameter:\n\n> `summation_method`: Specifies the Riemann summation method used (https:\/\/en.wikipedia.org\/wiki\/Riemann_sum): 'trapezoidal' [default] that applies the trapezoidal rule; 'careful_interpolation', a variant of it differing only by a more correct interpolation scheme for PR-AUC - interpolating (true\/false) positives but not the ratio that is precision; 'minoring' that applies left summation for increasing intervals and right summation for decreasing intervals; 'majoring' that does the opposite. Note that 'careful_interpolation' is strictly preferred to 'trapezoidal' (to be deprecated soon) as it applies the same method for ROC, and a better one (see Davis & Goadrich 2006 for details) for the PR curve."}}