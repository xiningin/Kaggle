{"cell_type":{"36605125":"code","4bc19851":"code","352862ca":"code","45f80143":"code","fd2f8657":"code","0bcccc11":"code","056e8d6a":"code","51e4e571":"code","47a2f80b":"code","989b79f2":"code","2945c653":"code","82ad862f":"code","027af9e6":"code","02981382":"code","9eb19cba":"code","6d472dee":"code","79c84897":"code","36962999":"code","e9f4388d":"code","6e99fc8e":"code","9e3567ee":"code","2554515d":"code","b7420535":"code","45035fef":"code","0710bd41":"code","70ed9db9":"code","15782a2d":"code","6cdc4aee":"code","b4a05b3f":"code","e5d0b1a1":"code","e2593816":"code","789b51f5":"code","9b1ea1b3":"code","dd5b1cc0":"code","09bef55d":"code","6c20b90f":"code","e19a0436":"code","4de4588c":"code","e44c608f":"code","07c93987":"code","15e90309":"code","caac9c62":"code","98d90bab":"code","d810404c":"code","480bdd5e":"code","0396922a":"code","80845f67":"code","e75e541e":"code","a453720d":"code","3d9cf60a":"code","79457887":"code","b25bfa1d":"code","0e408352":"code","d942271a":"code","de3bed29":"code","db595020":"code","b337fb2f":"code","243a7f1d":"code","cc2dc0a9":"code","56117922":"code","763ff61d":"code","2b1365c5":"markdown","1de143cb":"markdown","5818518d":"markdown","418d4230":"markdown","55ac8899":"markdown","c0db37ac":"markdown","88b61678":"markdown","a087c7cb":"markdown","a94c45db":"markdown","4260c78f":"markdown","a998e39d":"markdown","2f8bd889":"markdown","6b007dc5":"markdown","e13d6aa6":"markdown","b7c81c15":"markdown","6b16a982":"markdown","c824a772":"markdown","78ef6a6b":"markdown","9408b93f":"markdown","bfc56aac":"markdown","4f14a63a":"markdown","82686e52":"markdown","bed815cd":"markdown","600d37a9":"markdown","69df665c":"markdown","0d2e3362":"markdown","0c2dd2f1":"markdown","a947f70a":"markdown","a81355f9":"markdown","d953ce7d":"markdown","206eb6bc":"markdown","8a631763":"markdown","443cb877":"markdown","84fd9eb1":"markdown","5c24b6ec":"markdown","f90358b5":"markdown","54ab421e":"markdown","5ad9b25e":"markdown","a3f0bd5a":"markdown","6d68272a":"markdown","aa975625":"markdown","cae6d4fe":"markdown","934ab88f":"markdown","b2575f22":"markdown","cc6ea4d0":"markdown","f54bf2ae":"markdown","233156b5":"markdown","64e95c24":"markdown","0b9a05a0":"markdown","81bb8d1e":"markdown","5abe58b9":"markdown"},"source":{"36605125":"# Install Comet and import Experiment class\n!pip install comet_ml\nfrom comet_ml import Experiment","4bc19851":"# Start experiment\nexperiment = Experiment(api_key=\"XXXXXXXX\",\n                        project_name=\"XXXXXXXX\",\n                        workspace=\"XXXXXX\")","352862ca":"# Set experiment name, new name for each run\nexperiment.set_name('XXXXXX')","45f80143":"# Data wrangling\nimport numpy as np \nimport pandas as pd \n\n# Visualisation\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom wordcloud import WordCloud\n\n# Text processing\nimport string\nimport re\nimport spacy\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import TweetTokenizer\n\n# Data processing\nfrom sklearn.utils import resample\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn.feature_extraction import text\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import cross_val_predict, GridSearchCV, train_test_split\n\n# Model imports\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier \n\n# Metrics\nfrom sklearn.metrics import accuracy_score, log_loss, precision_score \nfrom sklearn.metrics import recall_score, precision_recall_curve, f1_score, classification_report, confusion_matrix\nfrom collections import defaultdict\n\n\n# Kaggle input\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Silence warnings for clean flow of notebook\nimport warnings\nwarnings.filterwarnings(\"ignore\")","fd2f8657":"df = pd.read_csv('\/kaggle\/input\/climate-change-belief-analysis\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/climate-change-belief-analysis\/test.csv')","0bcccc11":"display(df.head())\ndisplay(test.head())","056e8d6a":"# Detect and remove NaN values as well as duplicate rows\ndf = df.drop_duplicates(subset=['message'])\ndisplay(df.isnull().sum())\ndisplay(test.isnull().sum())","51e4e571":"def remove_blanks(df):\n    \"\"\"\n    Takes in a dataframe, detects empty strings and removes them.\n\n    Parameters:\n    ---------\n    DataFrame \n\n    Returns:\n    ---------\n    DataFrame:Dataframe with no empty strings\n\n    \"\"\"\n    blanks = []\n    for index, tweet in enumerate(df['message']):\n        if type(tweet) == str:\n            if tweet in ['', ' ']:\n                blanks.append(index)\n    print(blanks)\n    return df.drop(blanks)","47a2f80b":"# Remove blanks in train and test datasets\ndf = remove_blanks(df)\ntest = remove_blanks(test)","989b79f2":"def clean_text(text):\n    \"\"\"\n    Takes in text, cleans it by making it lowercase,\n    removes links\/urls, punctuations etc. and returns it a clean text.\n\n    Parameters:\n    ---------\n    text (str):String text\n\n    Returns:\n    ---------\n    str:Clean text\n\n    \"\"\"\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', 'URL', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n\n    return text","2945c653":"# Cleaning text in train and test datasets\ndf['clean_tweet'] = df['message'].apply(lambda x: clean_text(x))\ntest['clean_tweet'] = test['message'].apply(lambda x: clean_text(x))","82ad862f":"stop_words = stopwords.words('english')  # Assign stop_words list\n\n\ndef remove_stopword(text):\n    \"\"\"\n    Takes in text and removes stop words.\n\n    Parameters:\n    ---------\n    text (str):String text\n\n    Returns:\n    ---------\n    str:Text without stop words\n\n    \"\"\"\n    return [word for word in text.split() if word not in stop_words]","027af9e6":"# Removing stop words in train and test datasets\ndf['clean_tweet'] = df['clean_tweet'].apply(lambda x: remove_stopword(x))\ntest['clean_tweet'] = test['clean_tweet'].apply(lambda x: remove_stopword(x))","02981382":"def join_tweet(text):\n    return ' '.join(text)","9eb19cba":"df['clean_tweet'] = df['clean_tweet'].apply(lambda x: join_tweet(x))\ndf.head()","6d472dee":"# Original tweets\nfor index, text in enumerate(df['message'][0:5]):\n    print('Tweet %d:\\n' % (index+1), text)","79c84897":"# Clean tweets\nfor index, text in enumerate(df['clean_tweet'][0:6]):\n    print('Tweet %d:\\n' % (index+1), text)","36962999":"df.head()","e9f4388d":"# Checking number of values under each sentiment\ndf['sentiment'].value_counts()","6e99fc8e":"# Plot sentiment distribution\nfig, ax = plt.subplots(figsize=(10, 8))\ngraph = sns.countplot(x='sentiment', data=df, ax=ax)\nplt.title('Distribution of sentiment group count')","9e3567ee":"# Simple word split to get an idea of the raw tweet length\n# Will add new column for count that can be removed after analysis\ndf['word count'] = df['message'].apply(lambda t: len(t.split()))\ndf.head()","2554515d":"# Get a number summary of the word count variable\ndf.groupby('sentiment')['word count'].describe()","b7420535":"# Plot tweet word count distribution\nfig, ax = plt.subplots(figsize=(10, 5))\n\n# create graphs\nsns.kdeplot(df['word count'][df['sentiment'] == -1], shade=True, label='Anti')\nsns.kdeplot(df['word count'][df['sentiment'] == 0], shade=True,\n            label='Neutral')\nsns.kdeplot(df['word count'][df['sentiment'] == 1], shade=True, label='Pro')\nsns.kdeplot(df['word count'][df['sentiment'] == 2], shade=True, label='News')\n\n# set title and labels\nplt.xlabel('Count of words in tweet')\nplt.ylabel('Density')\nplt.title('Distribution of Tweet Word Count')\nplt.show()","45035fef":"# Create new column to check tweet character length\ndf['count_characters'] = df['message'].apply(lambda c: len(c))\ndf.head()","0710bd41":"# Get a number summary of the word count character variable\ndf.groupby('sentiment')['count_characters'].describe()","70ed9db9":"# Plot tweet character count distribution\nfig, ax = plt.subplots(figsize=(10, 5))\n\n# Create graphs\nsns.kdeplot(df['count_characters'][df['sentiment'] == -1], shade=True,\n            label='Anti')\nsns.kdeplot(df['count_characters'][df['sentiment'] == 0], shade=True,\n            label='Neutral')\nsns.kdeplot(df['count_characters'][df['sentiment'] == 1], shade=True,\n            label='Pro')\nsns.kdeplot(df['count_characters'][df['sentiment'] == 2], shade=True,\n            label='News')\n\n# Set title and label\nplt.xlabel('Total Character Count in Tweet')\nplt.ylabel('Density')\nplt.title('Distribution of Tweet Character Count')\nplt.show()","15782a2d":"# Repeat for punctuation\ndf['punctuation_count'] = df['message'].apply(lambda x: len([i for i in str(x)\n                                              if i in string.punctuation]))\n\n# Get a number summary of the panctuation count variable\ndf.groupby('sentiment')['punctuation_count'].describe()","6cdc4aee":"# Plot tweet punctuation count distribution\nfig, ax = plt.subplots(figsize=(10, 5))\n\n# Create graphs\nsns.kdeplot(df['punctuation_count'][df['sentiment'] == -1], shade=True,\n            label='Anti')\nsns.kdeplot(df['punctuation_count'][df['sentiment'] == 0], shade=True,\n            label='Neutral')\nsns.kdeplot(df['punctuation_count'][df['sentiment'] == 1], shade=True,\n            label='Pro')\nsns.kdeplot(df['punctuation_count'][df['sentiment'] == 2], shade=True,\n            label='News')\n\n# Set title and label\nplt.xlabel('Count of punctuation')\nplt.ylabel('Density')\nplt.title('Distribution of Tweet punctuation Count')\nplt.show()","b4a05b3f":"def hashtags_extract(text):\n    \"\"\"\n    Takes in text, exctracts the hashtags in the text and stores them\n    in a list.\n\n    Parameters:\n    ---------\n    text (str):String text\n\n    Returns:\n    ---------\n    list of str:Hahtags\n\n    \"\"\"\n    text = str(text).lower()\n    hashtags = []\n    for token in text.split():\n        if token.startswith('#'):\n            hashtags.append(token[1:])\n    return hashtags","e5d0b1a1":"# Extract hashtags from train data and group them by sentiment\ndf['hashtags'] = df['message'].apply(lambda x: hashtags_extract(x))\n\ndf_ht = df.groupby('sentiment')['hashtags'].sum()\ndf_ht.head()","e2593816":"# Creating a hashtag list for every sentiment\nnews_ht = df_ht.loc[1, ]\npro_ht = df_ht.loc[2, ]\nneutral_ht = df_ht.loc[0, ]\nanti_ht = df_ht.loc[-1]","789b51f5":"# Plot the top n hashtags for each sentiment\nmy_feqlist = []\nmy_dframe = []\nfor index, ht in enumerate([news_ht, pro_ht, neutral_ht, anti_ht]):\n    my_feqlist.append(nltk.FreqDist(ht))\n    my_dframe.append(pd.DataFrame({'Hashtag': list(my_feqlist[index].keys()),\n                     'Count': list(my_feqlist[index].values())}))\n    # selecting top 10 most frequent hashtags\n    my_dframe[index] = my_dframe[index].nlargest(columns=\"Count\", n=10)\n\nfig, ax = plt.subplots(4, 1, figsize=(15, 30))\nsns.barplot(data=my_dframe[0], x=\"Hashtag\", y=\"Count\", ax=ax[0])\nsns.barplot(data=my_dframe[1], x=\"Hashtag\", y=\"Count\", ax=ax[1])\nsns.barplot(data=my_dframe[2], x=\"Hashtag\", y=\"Count\", ax=ax[2])\nsns.barplot(data=my_dframe[3], x=\"Hashtag\", y=\"Count\", ax=ax[3])\n\nsentiments = ['News', 'Pro', 'Neutral', 'Anti']\nfor index, sent in enumerate(sentiments):\n    ax[index].set(ylabel='Count')\n    ax[index].set(xlabel='Hashtags')\n    ax[index].set(title=f'Hashtags Countplot for {sent} Sentiment')\nplt.show()","9b1ea1b3":"df_1 = df[df['sentiment'] == 1]\ndf_2 = df[df['sentiment'] == 2]\ndf_0 = df[df['sentiment'] == 0]\ndf_minus_1 = df[df['sentiment'] == -1]\n\nAll_messages = \" \".join(sent for sent in df['message'])\nmessages_1 = \" \".join(sent for sent in df_1['message'])\nmessages_2 = \" \".join(sent for sent in df_2['message'])\nmessages_0 = \" \".join(sent for sent in df_0['message'])\nmessages_minus_1 = \" \".join(sent for sent in df_minus_1['message'])","dd5b1cc0":"#fig, ax = plt.subplots(5, 1, figsize  = (35,40))\n# Create and generate a word cloud image:\nwordcloud_all = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(All_messages)\nwordcloud_news = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(messages_1)\nwordcloud_pro = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(messages_2)\nwordcloud_neutral = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(messages_0)\nwordcloud_anti = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(messages_minus_1)\n\nwordcloud_dict = {wordcloud_all:'All Sentiment',\n                  wordcloud_news:'News',\n                  wordcloud_pro:'Pro',\n                  wordcloud_neutral:'Neutral',\n                  wordcloud_anti:'Anti'}\n\n# Display the generated image:\nfor k, v in wordcloud_dict.items():\n    \n    fig, ax = plt.subplots(figsize = (15,7))\n    plt.imshow(k, interpolation='bilinear')\n    plt.title(f'Tweets under {v} Class', fontsize=30)\n    plt.axis('off')\n    \n    plt.show()","09bef55d":"def http_extractor(df):\n\n    df[\"web_pages\"] = df.message.str.findall(r'https?:\/\/\\S+')\n    df[\"web_pages\"] = [''.join(map(str, lists)).lower() for lists in df['web_pages']]\n    pattern_url = r'https?:\/\/\\S+'\n    subs_url = r'url-web'\n    df['post'] = df['web_pages'].replace(to_replace = pattern_url, value = subs_url, regex = True)\n\n    return(df)","6c20b90f":"http_extractor(df).head()","e19a0436":"df[\"postid\"] = ['Yes' if url != '' else 'No' for url in df[\"post\"]]\npost_id = pd.DataFrame(df.groupby('sentiment')['postid'].value_counts())\npost_id","4de4588c":"# Assign feature and response variables\nX = df['clean_tweet']\ny = df['sentiment']","e44c608f":"heights = [len(y[y == label]) for label in [0, 1, 2, -1]]\nbars = pd.DataFrame(zip(heights, [0,1,2,-1]), columns=['heights','labels'])\nbars = bars.sort_values(by='heights',ascending=True)","07c93987":"# Let's pick a class size of roughly half the size of the largest size\nclass_size = 3500\n\nbar_label_df = bars.set_index('labels')\n\nresampled_classes = []\n\nfor label in [0, 1, 2, -1]:\n    # Get number of observations from this class\n    label_size = bar_label_df.loc[label]['heights']\n\n    # If label_size < class size the upsample, else downsample\n    if label_size < class_size:\n        # Upsample\n        label_data = df[['clean_tweet', 'sentiment']][df['sentiment'] == label]\n        label_resampled = resample(label_data,\n                                   # sample with replacement\n                                   # (we need to duplicate observations)\n                                   replace=True,\n                                   # number of desired samples\n                                   n_samples=class_size,\n                                   random_state=27)\n    else:\n        # Downsample\n        label_data = df[['clean_tweet', 'sentiment']][df['sentiment'] == label]\n        label_resampled = resample(label_data,\n                                   # sample without replacement\n                                   # (no need for duplicate observations)\n                                   replace=False,\n                                   # number of desired samples\n                                   n_samples=class_size,\n                                   random_state=27)\n\n    resampled_classes.append(label_resampled)","15e90309":"# Assign feature and response variables from resampled data\nresampled_data = np.concatenate(resampled_classes, axis=0)\n\nX_resampled = resampled_data[:, :-1]\ny_resampled = resampled_data[:, -1]","caac9c62":"# Plot original original data with resampled data\nheights = [len(y_resampled[y_resampled == label]) for label in [0, 1, 2, -1]]\nbars_resampled = pd.DataFrame(zip(heights, [0, 1, 2, -1]),\n                              columns=['heights', 'labels'])\nbars_resampled = bars_resampled.sort_values(by='heights', ascending=True)\n\nfig = go.Figure(data=[\n    go.Bar(name='Original', x=[-1, 0, 2, 1], y=bars['heights']),\n    go.Bar(name='Resampled', x=[-1, 0, 2, 1], y=bars_resampled['heights'])\n])\nfig.update_layout(xaxis_title=\"Sentiment\", yaxis_title=\"Sample size\")\nfig.show()","98d90bab":"df_resampled = pd.DataFrame(X_resampled.reshape(-1,1))\ndf_resampled.columns = ['tweet']\ndf_resampled['sentiment'] = y_resampled\ndf_resampled['sentiment'] = df_resampled['sentiment'].astype('int')","d810404c":"# Splitting data\nX_train, X_test, y_train, y_test = train_test_split(\n                                   df_resampled['tweet'].values,\n                                   df_resampled['sentiment'].values,\n                                   test_size=0.1, random_state=42)","480bdd5e":"# Check shape of the split data\nprint('Training Data Shape:', X_train.shape)\nprint('Testing Data Shape: ', X_test.shape)","0396922a":"# Create a spaCy tokenizer\nspacy.load('en')\nlemmatizer = spacy.lang.en.English()\n\n\ndef tokenize(text):\n    \"\"\"\n    Takes in text, tokenizes words, then lemmatizes tokens.\n\n    Parameters:\n    ---------\n    text (str):String text\n\n    Returns:\n    ---------\n    list of str:Lammitzed tokens\n\n    \"\"\"\n    tokens = lemmatizer(text)\n    return [token.lemma_ for token in tokens]","80845f67":"# Define multiple vectorizers to test which one gives us the best accuracy\nvectorizer_dict = {'CV_1': CountVectorizer(max_df=0.8, min_df=3,\n                                           tokenizer=tokenize,\n                                           stop_words=stop_words),\n                   'CV_2': CountVectorizer(ngram_range=(1, 3), max_df=0.8,\n                                           min_df=3, tokenizer=tokenize,\n                                           stop_words=stop_words),\n                   'CV_3': CountVectorizer(ngram_range=(2, 3), max_df=0.8,\n                                           min_df=3, tokenizer=tokenize,\n                                           stop_words=stop_words),\n                   'TF_1': TfidfVectorizer(max_df=0.8, min_df=3,\n                                           tokenizer=tokenize,\n                                           stop_words=stop_words),\n                   'TF_2': TfidfVectorizer(ngram_range=(1, 3), max_df=0.8,\n                                           min_df=3, tokenizer=tokenize,\n                                           stop_words=stop_words),\n                   'TF_3': TfidfVectorizer(ngram_range=(2, 3), max_df=0.8,\n                                           min_df=3, tokenizer=tokenize,\n                                           stop_words=stop_words)}","e75e541e":"# Define multiple models to test which one gives us the best accuracy\nmodel_dict = {'Logistic Regression': LogisticRegression(max_iter=1000),\n              'Naive Bayes': MultinomialNB(),\n              'LinearSVM': SGDClassifier(random_state=42),\n              'Non-linear SVM': SVC(gamma=\"scale\"),\n              'Neural Network': MLPClassifier(),\n              'Decision Tree': DecisionTreeClassifier(max_depth=6),\n              'XGBoost': XGBClassifier(max_depth=6)\n              }","a453720d":"# Run each classifier for each vectorizer\nclassifier_results_dict = defaultdict(list)\nfor vec_name, vectorizer in vectorizer_dict.items():\n\n    X_train_v = vectorizer.fit_transform(X_train)\n    X_test_v = vectorizer.transform(X_test)\n    print(vec_name)  # keep track of progress\n\n    for mod_name, model in model_dict.items():\n        model.fit(X_train_v, y_train)\n        y_pred_v = model.predict(X_test_v)\n\n        precision_v = round(100*precision_score(y_test, y_pred_v,\n                            average='weighted'), 3)\n        recall_v = round(100*recall_score(y_test, y_pred_v,\n                         average='weighted'), 3)\n        f1_v = round(2*(precision_v*recall_v) \/ (precision_v+recall_v), 3)\n\n        classifier_results_dict['Vectorizer Type'].append(vec_name)\n        classifier_results_dict['Model Name'].append(mod_name)\n        classifier_results_dict[('Precision')].append(precision_v)\n        classifier_results_dict[('Recall')].append(recall_v)\n        classifier_results_dict[('F1-score')].append(f1_v)\n\nclassifier_results_df = pd.DataFrame(classifier_results_dict)","3d9cf60a":"# Checking result\nclassifier_results_df.sort_values(by='F1-score',\n                                  ascending=False).reset_index(drop=True)","79457887":"fig = px.bar(classifier_results_df, x=\"Model Name\", y=\"F1-score\", color='Vectorizer Type',\n             barmode='group', height=400)\nfig.show()","b25bfa1d":"# Best Model\n\n# Best Model: TF_1    Non-linear SVM\nmodel_svc = SVC(gamma=\"scale\")\n\n# Vectorization\nvectorizer = vectorizer_dict['TF_1']\nX_train_TF_1 = vectorizer.fit_transform(X_train)\nX_test_cv = vectorizer.transform(X_test)","0e408352":"#log metrics on Comet,where 'metrics' is a dictionary of metrics\nmetrics = classifier_results_df.to_dict('index')\nexperiment.log_metrics(metrics)","d942271a":"model_svc.fit(X_train_TF_1, y_train)\ny_pred_cv = model_svc.predict(X_test_cv)\n\nprecision_cv = round(100*precision_score(y_test, y_pred_cv,\n                     average='weighted'), 3)\nrecall_cv = round(100*recall_score(y_test, y_pred_cv, average='weighted'), 3)\nf1_cv = round(2*(precision_cv * recall_cv) \/ (precision_cv + recall_cv), 3)","de3bed29":"print(classification_report(y_test, y_pred_cv))","db595020":"# Print metrics scores\nprint('Precision Score:', precision_cv)\nprint('Recall Score:', recall_cv)\nprint('f1_Score:', f1_cv)","b337fb2f":"# Confusion Matrix\nconfusion_matrix(y_test, y_pred_cv)","243a7f1d":"# Enhance confusion matrix using a heatmap\ncm =confusion_matrix(y_test, y_pred_cv)\n\ncategories = ['Anti','Neutral','Pro','News']\nfig, ax = plt.subplots(figsize=(15, 7))\nsns.heatmap(cm, ax = ax, annot=True, fmt='g', xticklabels=categories, yticklabels=categories, cbar=False,\n            cmap='Greens')\nplt.suptitle('Confusion Matrix of Support Vector Classifier used with a TF-IDF Vectorizer')\nplt.xlabel('Predicted labels')\nplt.ylabel('True labels')\n\nplt.show()","cc2dc0a9":"#Creating csv for submission\n\n# Vectorizing and Normalizing the test data\ntest_data = vectorizer.transform(test['clean_tweet'])\ntest_data_norm = preprocessing.normalize(test_data)\n\n# Making a submission dataframe\ndf_submission = pd.DataFrame()\ndf_submission['tweetid'] = test['tweetid']\n\ny_test_data = model_svc.predict(test_data_norm) \ndf_submission['sentiment'] = y_test_data\n\n# Creating a csv file\ndf_submission.to_csv('Submission_file_SVC.csv', index=False)","56117922":"experiment.end()","763ff61d":"experiment.display()","2b1365c5":"The confusion matrix lives up to it's name! This is a matrix that shows the correct predictions vs. the true values that it predicted against. It can be visualised to make it a bit more understandable and appealing.","1de143cb":"Most of the URLs are present in tweets pertaining to pro-man made climate change tweets (class 1) followed closely by News related tweets (class 2). Anti-climate change tweets (class -1) had the least URLs. It can be deducted that the pro climate change tweeters and the News twitters send out tweets that are accompanied by factual proofs to substantiate the contents of the tweets.\n","5818518d":"Comet is a cloud-based platform that allows teams to preserve and compare the different models that have been built during a project. It is an effective tool for version control. We have saved all of our models as Comet experiments.","418d4230":"The top 10 hashtags for each sentiment class will be plotted for better visualisation.","55ac8899":"## 7.2. Length of tweets\n\nThe length of the tweets, based on the number of words, characters and punctuation, can be characteristic of a sentiment class. These will be analysed and visualised.","c0db37ac":"Punctuation and other special characters are removed from the text in order to transform the data into a more useable format.","88b61678":"More than half of the tweets , precisely 50,76%, belong to class 1. This indicates that the majority of tweets collected support the belief that man-made climate change exists. Conversely, 8.58% of the tweets collected are class -1, which represents tweets that do not believe in man-made climate change. Tweets that link to factual news about climate change comprise 24,89% whilst tweets which are neutral (neither supports nor refutes the belief of man-made climate change) make up 15,77% of the dataset. These are represented by the classes 2 and 0 respectively.\n\nThe class imbalance will need to be addressed to avoid the model being biased towards classifying sentiments as the majority class because the model will be well-versed in identifying it.","a087c7cb":"The same analysis for punctuation will be carried out. Some sentiment groups may be likely to use more or less punctuation.","a94c45db":"<a id=\"pp\"><\/a>\n# 8. Feature Engineering\n\nFeature engineering is the process of selecting and transforming variables into an idealformat before using them in predictive model. The features in the data will directly influence the predictive models we use and the results we obtain. In this section we are going to select our feature variable, address the imbalance in the sentiment distribution and transform our features for model training.","4260c78f":"<a id=\"ecomet\"><\/a>\n# 12. End Comet experiment","a998e39d":"<a id=\"eda\"><\/a>\n# 7. Exploratory Data Analysis And Insights","2f8bd889":"<a id=\"me\"><\/a>\n# 10. Model evaluation","6b007dc5":"The easiest way to understand how a confusion matrix works is that it shows the number of times it correctly predicted a class vs the number of times it incorrectly identified a class. The main thing to look for is the diagonal line starting at the top left to bottom right, this shows how many times the model predicted correctly. The higher the values in these areas and lower values in all other areas the better.","e13d6aa6":"A similar pattern as established by the number of words per tweet is displayed by the number of characters per tweet. Classes 1 and 0 have the the first and second maximum number of characters per tweet at at 208 and 166 characters respectively. However, classes 1 and -1 have the highest average number of characters per tweet at ~127 and ~124 characters. A slight difference is that class 2 tweets are on average longer than neutral tweets.","b7c81c15":"## 7.5. URL extraction","6b16a982":"Exploratory Data Analysis (EDA) is a fundamental part of the Machine Learning process. The data is analysed in order to extract information that a model may overlook. In this section, we will summarise the main characteritics of the data and also look into the sentiment classes provided in our training datasets.\n\n","c824a772":"### Count vectorizer\n\nThe Bag-of-Words method describes the occurrence of words within a document. To effectively demonstrate this technique, let us take a corpus (a collection of texts) called C of D documents {d1,d2,....,dD} and N unique tokens have been extracted out of the corpus C. The N tokens (words) will form a list and the size of the bag-of-words matrix (M) will be given by D x N.\n\nEach row in the matrix M contains the frequency of tokens in document D(i).\n\n\n${Example:}$\n\nD1: He is a smart boy. She is also smart.\n\nD2: He likes smart people.\n\n<table>\n  <thead>\n    <tr>\n      <th><\/th>\n      <th>He<\/th>\n      <th>She<\/th>\n      <th>He<\/th>\n      <th>likes<\/th>\n      <th>smart<\/th>\n      <th>boy<\/th>\n      <th>people<\/th>\n    <\/tr>\n  <\/thead>\n  <tbody>\n    <tr>\n      <td>D1<\/td>\n      <td>1<\/td>\n      <td>1<\/td>\n      <td>0<\/td>\n      <td>0<\/td>\n      <td>2<\/td>\n      <td>1<\/td>\n      <td>0<\/td>\n    <\/tr>\n    <tr>\n      <td>D2<\/td>\n      <td>0<\/td>\n      <td>0<\/td>\n      <td>1<\/td>\n      <td>1<\/td>\n      <td>1<\/td>\n      <td>0<\/td>\n      <td>1<\/td>\n    <\/tr>\n  <\/tbody>\n<\/table>","78ef6a6b":"## 7.4. Wordcloud visualisation\n\nA word cloud is an image that visualises words that are associated with a certain topic, or rather in this case a sentiment class(es). The font size of the word indicates the frequency that that particular word occurs at. The bigger the font size, the more frequent the word features.","9408b93f":"A classification report is a representation of the main classification metrics on a per-class\/sentiment basis. Its used to describe the performance of a classifier. This gives a deeper intuition of the classifier behavior over global accuracy.\n\nThe metrics are defined in terms of true and false positives, and true and false negatives. Positive and negative in this case are generic names for the classes of a binary classification problem. A true positive is when the actual class is positive as is the estimated class. A false positive is when the actual class is negative but the estimated class is positive. This is also referred to as a Type 1 error. A true negative is when the actual class is negative as is the estimated class. A false negative is when the actual class is negative but the estimated class is positive. This is also referred to as a Type 2 error. Using this terminology the metrics are defined as follows:\n\n* Precision - This is the ability of a classiifer not to label an instance positive that is actually negative.\n\n\n$$ Precision = \\frac{TP}{TP \\space + FP} = \\frac{TP}{Total \\space Predicted \\space Positive} $$\n\n* Recall - This is the ability of a classifier to find all positive instances. \n\n$$ Recall = \\frac{TP}{TP \\space + FN} = \\frac{TP}{Total \\space Actual \\space Positive}$$\n\n\n* F1 score - The F1 score is a weighted harmonic mean of precision and recall such that the best score is 1.0 and the worst is 0.0. \n\n\n$$F_1 = 2 \\times \\frac {Precision \\space \\times \\space Recall }{Precision \\space + \\space Recall }$$","bfc56aac":"Data pre-processing is the process of identifying errors and modifying or removing parts of the data in order to improve its overall quality. It is important as it enhances the dataset's reliability therefore providing more accurate information,allowing for optimal insights extraction and also receiving improved model results.\n\nUpon basic inspection of the top 5 records of our datasets, the initial data cleaning requirements are as follows:\n\n* Remove duplicates, blank strings & null values\n\n* Remove twitter handles\n\n* Remove punctuation and other redundant special characters\n\n* Filter out stop words","4f14a63a":"<a id=\"load\"><\/a>\n# 4. Load data","82686e52":"<a id=\"dod\"><\/a>\n# 5. Description of the data\n\nTwo datasets were provided, namely train.csv and test.csv. These datasets will be used to train and test the model respectively. They contain tweets, related to climate change, which were collected between April 27, 2015 and February 21, 2018.\n\nThere are three variables defined in the train.csv dataset:\n\n1. Sentiment - Sentiment of tweet\n\n2. Message - Tweet body\n\n3. Tweetid - Twitter unique id\n\n\nThe sentiment variable is divided into four sentiment classes which the tweets are accordingly grouped into based on their content. The classes are stipulated as follows:\n\n![inbox_2205222_8e4d65f2029797e0462b52022451829c_data.png](attachment:inbox_2205222_8e4d65f2029797e0462b52022451829c_data.png)\n\n","bed815cd":"<a id=\"conc\"><\/a>\n# 11. Conclusion\n\nAfter pre-processing, a tweet is compared between the correct predictions versus the true ones. This aids the machine in differentiating between true and predicted sentiment values enabling the machine to seek out key words,characters,phrases that play a role in the sentiment value. Using Natural Language Processes we performed the following to assist our model in determining sentiment values:\n1) Searched for null values and duplicates;\n2) Removed empty strings,special characters and stop words;\n\nOnce this process was complete, the biggest factor hindering optimum results was the class imbalance. Majority of the tweets Pro climate change made up 50.76% of the data and to avoid the model being biased towards classifying sentiments as the majority class, this had to be addressed either through Upsampling or Down-sampling or both. WUpon further analysis, it was picked up that majority of the news sentimen tweets contain hyperlinks and these could be seen as the determining factors of assigning the news sentiment to the relevant tweet. This could not be used as a distinguishng feature as it was not really significant. The classification report shows a representation of the main classification metrics on a per-class\/sentiment basis. Using feature engineering, the data was processed to a point where the best algorithm (non-linear SVC) could predict effectively without bias.\n\nWith the following scores:\n**Precision Score: 83.507\n**Recall Score: 83.5\n**f1_Score: 83.503\n\nThe designed algorithm has a high ability to determine which tweets fall under the relative sentiment and aids to make well informed decisions in determining whether or not a person believes in climate change, based on their novel tweet data.","600d37a9":"The raw data and the preprocessed data can be compared.","69df665c":"## 7.3. Hashtags extraction\n\nHashtags are insightful as they indicate which subtopics were trending at any given time. Finding out which hashtags trended amongst the sentiment classes can indicate which topics were popular within that particular class and may be significant in identifying them.","0d2e3362":"## 6.4. Stop Words\n\nStop words are words commonly used in English such as \"I\", \"is\", \"the\" etc. These do not carry a vast amount of useful information thus they are filtered out. Classification accuracy can also be increased as removing stop words results in fewer and more meaningful tokens. The NLTK library has an English list of stop words which can be downloaded and utilised for this process.","0c2dd2f1":"## 8.4. Vectorization\n\nIn order to analyse preprocessed data, it needs to be converted into features. Depending upon the usage, text features can be constructed using assorted techniques such as Bag-of-Words, Term Frequency\u2013Inverse Document Frequency (TF-IDF) and Word Embedding. For this project, only Bag-of-Words and TF-IDF will be used. These are types of evctorisers. Vectorisers take text and converts it into a matrix of vectors.","a947f70a":"Upon analysis of all the sentiment classes, \"climate change\", \"RT\", \"https\", \"co\" and \"global warming\" are the most popular words\/phrases. Even within the individual sentiment classes, the same five words\/phrases are the most common.","a81355f9":"## 6.5. Full context of tweets before and after data pre-processing","d953ce7d":"<a id=\"introduction\"><\/a>\n# 1. Introduction\n\n## 1.1. Project overview\n\nSentiment analysis is the process of interpreting and classifying the feelings behind a message based on the usage of a series of words through the employment of text analysis techniques. Sentiment anaysis is helpful in determining the general public opinion towards a certain topic. Subsequently, businesses can utilise this information when designing their products and services and also  moulding their marketing strategies. It can form a part of customer feedback. Additionally, these text analysis techniques can be used to extract insights from bodies of text as well.\n\nWith the change in time, consumers have become more conscience about acquiring products\/services from brands that uphold certain values and ideals.They also consider the service provider's stances towards issues such as climate change. In order to appeal to these consumers, organisations should understand their sentiments.They need to understand how their products will be received whilst trying to decrease their environmental impact or carbon footprint. This can be achieved using Machine Learning.\n\nMachine learning is a subset of Artifical Intelligence in which algorithms are applied to teach computers to make decisions, with little to no human intereference. Classification is a branch of Machine Learning where models are trained to pick up patterns in preexisting data then subsequently classify unseen data into appropriate classes. Classifiers will be used to categorise the tweets into four sentiment classes.\n\n\n\n## 1.2. Problem statement\n\nBuild a Machine Learning model that is able to classify whether or not a person believes in climate change, based on their novel tweet data. ","206eb6bc":"Majority of the combinations between the different classifiers and vectorisers performed well classifying above 70% of the dataset correctly. The model that came out on top though is the non-linear SVM with a F1-score of 83.50","8a631763":"<a id=\"cleaning\"><\/a>\n# 6. Data pre-processing","443cb877":"Empty strings are removed because they add no value to the dataset.","84fd9eb1":"## 6.3. Special Characters","5c24b6ec":"## 8.2. Class imbalance\n\nImbalanced data typically refers to a problem with classification problems where the classes are not represented equally. This is evident in the dataset where sentiment class  is twice the size of the second largest sentiment class.\n\nTo rectify this problem,a resampling technique will be employed. The technique entails the following:\n\n1. Add copies of instances from the under-represented class; this is called over-sampling (or more formally sampling with replacement), or\n2. Delete instances from the over-represented class; this is called under-sampling.","f90358b5":"Looking at the number of words per tweet, although classes 0 and 1 have the same maximum number of words per tweet at 31 words, classes -1 and 1 have the highest average number of words per tweet at ~19 words. This suggests that people that sent out tweets which are anti and pro man-made climate change send out tweets with more words. News tweets generally have the least number of words with a maximum of 26 and an average of ~16 words per tweet. They do however also display more of a normal distribution, insinuating that news tweets are more consistent in the number of words. The number of words of tweets which are classified as neutral have the greatest distribution with a standard deviation of ~6 words, they vary from \"few\" to \"many\" words in a tweet.","54ab421e":"## 7.1. Sentiment distribution\n\nThe distribution of the sentiments needs to analysed in order to determine if there is a data imbalance. Datasets are rarely balanced; imbalanced data is a norm in the real world.","5ad9b25e":"## 6.2. Empty Strings","a3f0bd5a":"The tweet sentiments from the provided training dataset show an imbalance in the distribution of classes.  It is clearer to see when visualised.","6d68272a":"## 8.1. Select feature and label variables","aa975625":"# Table of contents\n\n### [1. Introduction](#introduction)\n1.1. Project overview\n\n1.2. Problem statement\n\n\n### [2. Start Comet experiment](#scomet)\n### [3. Package and module imports](#pack)\n### [4. Load data](#load)\n### [5. Description of the data](#dod)\n### [6. Data pre-processing](#cleaning)\n6.1. Null values and duplicates\n\n6.2. Empty strings\n\n6.3. Special characters\n\n6.4. Stop words\n\n6.5. Full context of tweets before and after data pre-processing\n\n### [7. Exploratory Data Analysis](#eda)\n7.1. Sentiment distribution\n\n7.2. Length of tweets\n\n7.3. Hashtags extraction\n\n7.4. Wordcloud visualisation\n\n7.5 URL extraction\n### [8. Feature engineering](#pp)\n\n8.1. Select feature and label variables\n\n8.2. Dealing with class imbalance\n\n8.3. Tokenization and lemmatization\n\n8.4. Vectorization\n\n### [9. Modelling](#mod)\n### [10. Model evaluation](#me)\n### [11. Conclusion](#conc)\n### [12. End Comet experiment](#ecomet)","cae6d4fe":"There are hashtags, namely climate, climate change and trump, which are highly utilised by all four sentiment classes. Cop22 is present in all the sentiment groups, however it is absent within the anti man-made climate change sentiment class. The environment hastag is common in the news and pro man-made climate change classes. The maga hashtag is particularly popular, being the most used one, in the anti man-made climate change class. It is short for \"Make America Great Again\". Besides the maga hastag, there are no other particular hashtags which can be used to identify a specific sentiment class. The feature will not be of major importance when training classifiers.","934ab88f":"## 6.1. Null values and duplicates\n\nMissing values, mainly known as null values, occur due to multiple reasons including errors whilst collecting data. These values are removed because they add no value to the output of the models; all data has to be valid and valued. Duplicates are also removed as they do not provide any new information. The reduced number of values also result in less model run time.","b2575f22":"<a id=\"scomet\"><\/a>\n# 2. Start Comet Experiment","cc6ea4d0":"<a id=\"pack\"><\/a>\n# 3. Package and module imports","f54bf2ae":"### TF-IDF vectorizer\n\nThis is another method which is based on the frequency but it is different to the Bag-Of-Words approach in the sense that it takes into account not just the occurrence of a word in a single document (or tweet), but in the entire corpus.\n\nTF-IDF penalises the common words by assigning them lower weights while giving importance to words which are rare in the entire corpus but appear a significant numbe rof times in the document.\n\nImportant terms related to TF-IDF are:\n\n\n$\\alpha$ = Number of times term t appears in a document\n\n$\\beta$ = Number of terms in the document\n\n$$TF = \\frac{\\alpha}{\\beta}$$\n\n$$IDF = log{(\\frac{N}{n})}$$ \n\nwhere, **N** is the number of documents and **n** is the number of documents a **term t** has appeared in.\n\n$$TF-IDF = TF*IDF$$","233156b5":"After addressing the imbalance in the data, all the sentiment classes feature 3 500 observations. This will give each class equal weighting when the classifiers are trained.","64e95c24":"<a id=\"mod\"><\/a>\n# 9. Modelling\n\nIn this section, machine learning algorithms that will predict the labels from the features presented will be trained. Several models (logistic rgeression, Naive Bayes, linear support vector machine, neural network, decision tree and XGBoost) will be trained in order to test their performance and see which one does the best job classifying our unseen data. Below is a basic breakdown of each model.\n\n1. **Logistic Regression** - Logistic regression is one of the most widely used algorithms for classifying catgorical data. This method predicts the probability of an observation belonging to a class using a logit function. A probability threshold is determined and is used to allocate an observation to a label. 0.5 is typically chosen as the threshold. The logit function is defined as follows:\n\n$$P(X) = \\displaystyle \\frac{e^{\\beta_0 + \\beta_1 X}}{1+e^{\\beta_0 + \\beta_1 X}}$$\n\nwhere $P(X)$ is the probability of X belonging to class 1, and $\\beta_0$ and $\\beta_1$ are the intercept and regression coefficient respectively. $P(X)$ > 0.1 is assigned class 1 and $P(X)$ < 0 is assigned class 0. With multiclasses, a \"One vs Rest\" approach can be used where a logistic regression model is trained for each label or cross-entropy loss.\n\n2. **Naive Bayes** - This method uses the principle of Bayes Theorem (conditional probability) to classify observations into classes. The algorithm assumes that there is independence between variables (X and Y). This is displayed in the equation\n\n$$\nP(X \\cap Y) = P(X)P(Y)\n$$\n\nEven besides this assumption, it works well with multiclass classification. The algorithm calculates the probability of each labels for a given text and then outputs the label with the highest one.\n\n3. **Linear Support Vector Machine** - Support vector machines are based on the idea of hyperplanes seperating classes in a  linear manner. Support vectors are data points found close to the hyperplane and are considered critical points in the dataset as they impact the position of the hyperplane. The further away that points are from hyperplanes, the better.\n\n4. **Neural Network** - A neural network is essentially a network of functions feeding into each other and are designed to recognise patterns.\n\n5. **Decision Tree** - A decision tree algorithms make decisions by selecting an attribute and makes it a decision node. At the decision node, smaller subsets off the data is created by splitting the data according to the outcome of a condition set. This process is repeatedly recursively until the maximum depth of the trees is reached. In classifitcaton, the split points can be made using the Information Gain, Gain Ratio and Gini Index methods.\n\n6. **XGBoost** - XGBoost (eXtreme Gradient Boosting) is a type of ensemble model (a method that trains and predicts many models at once to produce a single superior output). It is based on the concept of boosting weak learners to strong learners.","0b9a05a0":"The amount of characters per tweet will be analysed to pick up an patterns, if present. Some sentiment classes may send out shorter or longer tweets.","81bb8d1e":"## 8.3. Tokenization and lemmatization\n\n\n* Tokenization is the process of splitting a string\/text into a list of tokens. One can think of token as a small part, like a word is a token in a sentence and a sentence is a token in a paragraph.\n\n* Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. It is similar to stemming but it brings context to the words. ","5abe58b9":"The amount of punctuaton displays a number of outliers in each class at 36, 25, 58 and 20 for classes -1, 0, 1 and 2 whilst the averages for each class are ~ 8, 7, 8 and 9. There is a miniscule difference in the means therefore the number of punctuation per tweet can not be as an unique identifier for any of the sentiment classes.\n\nDespite classes -1 and 0 having tweets which have the most characters and words, the differences between these two classes and the other classes, and additonally themselves, are not significant enough to use these two characteristics as features when classifying between the four classes in question. As mentioned above, there are no punctuation patterns that are significant to either class."}}