{"cell_type":{"e0f34449":"code","08401168":"code","ac99dc2c":"code","bdcbfd35":"code","95a0a289":"code","75fd6d8b":"code","8c76ba72":"code","440994e2":"code","a75d549d":"code","2300e654":"code","a9a2fd35":"code","c328fea5":"code","91f69aed":"code","647ad7c2":"code","4a82043a":"code","446a22e0":"code","8c77e195":"code","b1896bf4":"code","22e6e2c1":"code","3c744580":"code","9a3a8df2":"code","6985e766":"code","332ea605":"code","9bb1e5fc":"code","91f5a84f":"code","1030ed42":"code","bff87a79":"code","d68b55df":"code","deddfbb4":"code","065a92b8":"code","c0388d78":"code","6c06c98b":"code","f450e8b5":"code","51d90cb9":"code","f0bac83b":"code","7cdf48af":"code","25259599":"code","2696e30b":"code","a49f845a":"code","cce48f5b":"code","9e7a7439":"code","5343124c":"code","5bbfcf9b":"code","71d63d02":"code","506789b5":"code","caecb53b":"markdown","a67765a8":"markdown","b62e3f5d":"markdown","6de7c6e2":"markdown","57462f33":"markdown","4acacb35":"markdown","8f26ca8e":"markdown","bd544f36":"markdown","f7dbf6c1":"markdown","cd84120a":"markdown","d7a980bc":"markdown","461a0d31":"markdown","2566cccc":"markdown","214d3fa5":"markdown","ef356c4b":"markdown","1da3d0af":"markdown","a16e637e":"markdown","8ae67884":"markdown"},"source":{"e0f34449":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport re\nimport zipfile\n","08401168":"# Will unzip the files so that you can see them..\nwith zipfile.ZipFile(\"\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip\",\"r\") as z:\n    z.extractall(\".\")\n","ac99dc2c":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/working\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","bdcbfd35":"# prepare text samples and their labels\nprint('Loading in comments...')\n\ndata = pd.read_csv(\"\/kaggle\/working\/train.csv\")\nprint(data.head())","95a0a289":"# Feature Imformation \ndata.columns","75fd6d8b":"# Data Dimension \n\ndata.shape ","8c76ba72":"cols_target = ['obscene','insult','toxic','severe_toxic','identity_hate','threat']","440994e2":"# Check Missing Value \n\nprint(data[\"comment_text\"].isna().sum())\n\n# dropna ","a75d549d":"# check missing values in numeric columns\ndata.describe()","2300e654":"unlabelled_in_all = data[(data['toxic']!=1) & (data['severe_toxic']!=1) &\n                             (data['obscene']!=1) & (data['threat']!=1) &\n                             (data['insult']!=1) & (data['identity_hate']!=1)]\nprint('Percentage of unlabelled comments or good comments is ', len(unlabelled_in_all)\/len(data)*100)","a9a2fd35":"labelled_in_all = data[(data['toxic']==1) & (data['severe_toxic']==1) &\n                             (data['obscene']==1) & (data['threat']==1) &\n                             (data['insult']==1) & (data['identity_hate']==1)]\nprint('Percentage of comments which is present in all categories is ', len(labelled_in_all)\/len(data)*100)","c328fea5":"# let's see the total rows in train, test data and the numbers for the various categories\nprint('Total rows in train is {}'.format(len(data)))\nprint(data[cols_target].sum())","91f69aed":"target_data = data[cols_target]\ncolormap = plt.cm.plasma\nplt.figure(figsize=(7,7))\nplt.title('Correlation of features & targets',y=1.05,size=14)\nsns.heatmap(target_data.astype(float).corr(),linewidths=0.1,vmax=1.0,square=True,cmap=colormap,\n           linecolor='white',annot=True)","647ad7c2":"data['block'] =data[cols_target].sum(axis =1)\nprint(data['block'].value_counts())\ndata['block'] = data['block'] > 0 \ndata['block'] = data['block'].astype(int)\nprint(data['block'].value_counts())\n","4a82043a":"# look at the count plot for text length\nsns.set()\nsns.countplot(x=\"block\" , data = data )\nplt.show()","446a22e0":"# Event Rate \n\nprint(\"Percentage Event Rate : \" , round(100*data['block'].sum()\/data.shape[0],2) , \"%\")","8c77e195":"# Let's look at the character length for the rows in the training data and record these\ndata['char_length'] = data['comment_text'].apply(lambda x: len(str(x)))","b1896bf4":"# look at the histogram plot for text length\nsns.set()\ndata['char_length'].hist()\nplt.show()","22e6e2c1":"def clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n    text = re.sub('\\W', ' ', text)\n    text = re.sub('\\s+', ' ', text)\n    text = text.strip(' ')\n    return text","3c744580":"%%time \n# clean the comment_text in train_df [Thanks to Pulkit Jha for the useful pointer.]\ndata['comment_text'] = data['comment_text'].map(lambda com : clean_text(com))","9a3a8df2":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(data['comment_text'], data['block'], test_size=0.2, random_state=42)","6985e766":"print(X_train.shape, X_test.shape)\nprint(y_train.shape, y_test.shape)\n","332ea605":"# import and instantiate TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(max_features = 10000, stop_words='english')\n#vect = TfidfVectorizer(stop_words='english')\nprint(vect)","9bb1e5fc":"%%time \n# learn the vocabulary in the training data, then use it to create a document-term matrix\nX_train_dtm = vect.fit_transform(X_train)\n# examine the document-term matrix created from X_train\nX_train_dtm","91f5a84f":"X_train_dtm.shape","1030ed42":"100*2792162\/ (127656*10000)","bff87a79":"%%time\n# transform the test data using the earlier fitted vocabulary, into a document-term matrix\nX_test_dtm = vect.transform(X_test)\n# examine the document-term matrix from X_test\nX_test_dtm","d68b55df":"# import and instantiate the Logistic Regression model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nlogreg = LogisticRegression(C=1, max_iter = 2000)\n\n\n\n# train the model using X_train_dtm & y_train\nlogreg.fit(X_train_dtm, y_train)\n# compute the training accuracy\ny_pred_train = logreg.predict(X_train_dtm)\nprint('Training accuracy is {}'.format(accuracy_score(y_train, y_pred_train)))\n# compute the predicted probabilities for X_test_dtm\ny_pred_test = logreg.predict(X_test_dtm)\nprint('Test accuracy is {}'.format(accuracy_score(y_test,y_pred_test)))\nprint(confusion_matrix(y_test,y_pred_test))\n","deddfbb4":"#28507 -> comments  are good and predeicted as good \n#2014 -> comments are block and predicted as block\n#164 -> comments are good but predicted as block\n#1230 -> comments are block but predicted as good\n","065a92b8":"(28507 + 2014)\/(28507+2014+164+1230)\n","c0388d78":"import sklearn.metrics as metrics\n# calculate the fpr and tpr for all thresholds of the classification\nprobs = logreg.predict_proba(X_test_dtm)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\n# method I: plt\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n\n","6c06c98b":"from sklearn.metrics import f1_score\n\n\nprint(\"F1 score on Test data : \" ,f1_score(y_test,y_pred_test))\n    \n","f450e8b5":"y_pred_test = logreg.predict_proba(X_test_dtm)[:,1]\n#print(y_pred_test)\ny_pred_test = y_pred_test >= 0.2 # by default it is 0.5\ny_pred_test = y_pred_test.astype(int)\nprint('Test accuracy is {}'.format(accuracy_score(y_test,y_pred_test)))\nprint(confusion_matrix(y_test,y_pred_test))\nprint(\"F1 score on Test data : \" ,f1_score(y_test,y_pred_test))","51d90cb9":"%%time \n\nfrom sklearn.metrics import f1_score\nfrom sklearn.tree import DecisionTreeClassifier \n\ndt_clf = DecisionTreeClassifier()\n# train the model using X_train_dtm & y_train\ndt_clf.fit(X_train_dtm, y_train)\n# compute the training accuracy\ny_pred_train = dt_clf.predict(X_train_dtm)\nprint('Training accuracy is {}'.format(accuracy_score(y_train, y_pred_train)))\n# compute the predicted probabilities for X_test_dtm\ny_pred_test = dt_clf.predict(X_test_dtm)\nprint('Test accuracy is {}'.format(accuracy_score(y_test,y_pred_test)))\nprint(confusion_matrix(y_test,y_pred_test))\nprint(\"F1 score on Test data : \" ,f1_score(y_test,y_pred_test))","f0bac83b":"%%time \nfrom sklearn.metrics import f1_score\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.tree import DecisionTreeClassifier \n\nrf_clf = RandomForestClassifier()\n\n# train the model using X_train_dtm & y_train\nrf_clf.fit(X_train_dtm, y_train)\n# compute the training accuracy\ny_pred_train = rf_clf.predict(X_train_dtm)\nprint('Training accuracy is {}'.format(accuracy_score(y_train, y_pred_train)))\n# compute the predicted probabilities for X_test_dtm\ny_pred_test = rf_clf.predict(X_test_dtm)\nprint('Test accuracy is {}'.format(accuracy_score(y_test,y_pred_test)))\nprint(confusion_matrix(y_test,y_pred_test))\nprint(\"F1 score on Test data : \" ,f1_score(y_test,y_pred_test))","7cdf48af":"# Fine Tuning Random Forest \n\ny_pred_test = rf_clf.predict_proba(X_test_dtm)[:,1]\ny_pred_test = y_pred_test >= 0.05 # by default it is 0.5\ny_pred_test = y_pred_test.astype(int)\nprint('Test accuracy is {}'.format(accuracy_score(y_test,y_pred_test)))\nprint(confusion_matrix(y_test,y_pred_test))\nprint(\"F1 score on Test data : \" ,f1_score(y_test,y_pred_test))","25259599":"%%time\nfrom sklearn.metrics import f1_score\nfrom sklearn.linear_model  import PassiveAggressiveClassifier \n\npa_clf = PassiveAggressiveClassifier()\n\n# train the model using X_train_dtm & y_train\npa_clf.fit(X_train_dtm, y_train)\n# compute the training accuracy\ny_pred_train = pa_clf.predict(X_train_dtm)\nprint('Training accuracy is {}'.format(accuracy_score(y_train, y_pred_train)))\n# compute the predicted probabilities for X_test_dtm\ny_pred_test = pa_clf.predict(X_test_dtm)\nprint('Test accuracy is {}'.format(accuracy_score(y_test,y_pred_test)))\nprint(confusion_matrix(y_test,y_pred_test))\nprint(\"F1 score on Test data : \" ,f1_score(y_test,y_pred_test))","2696e30b":"%%time \nfrom sklearn.metrics import f1_score\nimport xgboost \n\nxgb = xgboost.XGBClassifier()\n# train the model using X_train_dtm & y_train\nxgb.fit(X_train_dtm, y_train)\n# compute the training accuracy\ny_pred_train = xgb.predict(X_train_dtm)\nprint('Training accuracy is {}'.format(accuracy_score(y_train, y_pred_train)))\n# compute the predicted probabilities for X_test_dtm\ny_pred_test = xgb.predict(X_test_dtm)\nprint('Test accuracy is {}'.format(accuracy_score(y_test,y_pred_test)))\nprint(confusion_matrix(y_test,y_pred_test))\nprint(\"F1 score on Test data : \" ,f1_score(y_test,y_pred_test))","a49f845a":"# Fine Tuning XGBOOST\n\ny_pred_test = xgb.predict_proba(X_test_dtm)[:,1]\ny_pred_test = y_pred_test >= 0.06 # by default it is 0.5\ny_pred_test = y_pred_test.astype(int)\nprint('Test accuracy is {}'.format(accuracy_score(y_test,y_pred_test)))\nprint(confusion_matrix(y_test,y_pred_test))\nprint(\"F1 score on Test data : \" ,f1_score(y_test,y_pred_test))","cce48f5b":"import lightgbm \n\nparameters = {\n    'application': 'binary',\n    'objective': 'binary',\n    'metric': 'auc',\n    'is_unbalance': 'true',\n    'boosting': 'gbdt',\n    'num_leaves': 31,\n    'feature_fraction': 0.5,\n    'bagging_fraction': 0.5,\n    'bagging_freq': 20,\n    'learning_rate': 0.05,\n    'verbose': 0\n}\n\ntrain_data = lightgbm.Dataset(X_train_dtm, label=y_train)\ntest_data = lightgbm.Dataset(X_test_dtm, label=y_test)\n\nclf = lightgbm.train(parameters,\n                       train_data,\n                       valid_sets=test_data,\n                       num_boost_round=500,\n                       early_stopping_rounds=10)\n\n\n\n\n","9e7a7439":"# Fine Tuning LIGHT GBM\n\ny_pred_test = clf.predict(X_test_dtm)\ny_pred_test = y_pred_test >= 0.35 # by default it is 0.5\ny_pred_test = y_pred_test.astype(int)\nprint('Test accuracy is {}'.format(accuracy_score(y_test,y_pred_test)))\nprint(confusion_matrix(y_test,y_pred_test))\nprint(\"F1 score on Test data : \" ,f1_score(y_test,y_pred_test))","5343124c":"import eli5\n\neli5.show_weights(logreg,vec = vect, top = 15)  # logistic regression\n# will give you top 15 features or words  which makes a comment toxic ","5bbfcf9b":"eli5.show_weights(xgb,vec = vect,top = 15)  # XGBoost\n# will give you top 15 features or words  which makes a comment toxic ","71d63d02":"X_test.iloc[718]","506789b5":"eli5.show_prediction(logreg, vec = vect, doc =  X_test.iloc[718]) ","caecb53b":"### Importing the required libraries ###","a67765a8":"### Now this kind of problem is ###\n\n1) Multi class problem and not Binary\n\n2) Also all classes are not independent but rather dependent or correlated \n\n3) A comment can belong to multiple classes at the same time for e.g. comment can be toxic and insulting at the same time\n\nLet us simplify the problem by first classifying the comments as \"block\" vs \"allow\" ","b62e3f5d":"## Model Explanation ##","6de7c6e2":"### Passive Aggresive Classifier does not support prediction probability - so can't be fined ###","57462f33":"### Lets us try an Ensemble of Trees ###","4acacb35":"### Let us focus on comments  ###","8f26ca8e":"Next, let's examine the correlations among the target variables.","bd544f36":"### In case of Class Imbalance - we use F1 score as a general measure for the model performance ###\n\nDepending on the Business case - we need to fine tune the model \n\nThere is a Precision vs Recall Trade off \n\nIf you want to capture all toxic tweets  - then some of the good twwets will be misclassified as bad tweets ","f7dbf6c1":"Most of the text length are within 500 characters, with some up to 5,000 characters long.\n\n","cd84120a":"### UNZIP files ###","d7a980bc":"## Lets us build a binary classifier using Logistic Regression ##","461a0d31":"### Advance Models - LightGBM ### ","2566cccc":"## Tweets Explanation ##","214d3fa5":"Indeed, it looks like some of the labels are higher correlated, e.g. insult-obscene has the highest at 0.74, followed by toxic-obscene and toxic-insult.","ef356c4b":"# Welcome to the curse of Accuracy, F1(help) to the rescue #","1da3d0af":"# Let us use a tree base model #","a16e637e":"### Reading the Train File ###","8ae67884":"### Clean the Comments Text ###"}}