{"cell_type":{"608853e9":"code","498ff60b":"code","f1a23c7b":"code","0f60ab2a":"code","6e23c06c":"code","f8349c97":"code","00253502":"code","e4b6d178":"code","c3575e0e":"code","e35ed161":"code","d8e11bea":"code","23d5ecb9":"code","6f956bd1":"code","8123bbaf":"code","6c82cade":"code","f454b9a2":"code","79bbfe26":"code","6f86df3b":"code","9d23c294":"code","905071b9":"code","5e68c5fc":"code","adf1cc75":"code","63d669c0":"code","feeb848b":"code","71f41dd7":"code","409921bf":"code","d9046c68":"code","87d06a85":"code","50ff25a5":"code","dbe19f40":"code","510c7b4e":"code","7822f98c":"markdown","ca0d515a":"markdown","ae026c8c":"markdown","512d1146":"markdown","7deec322":"markdown"},"source":{"608853e9":"import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport numpy as np\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n\nfrom xgboost import XGBRegressor\n\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping","498ff60b":"X = pd.read_parquet(\"..\/input\/kaggle-pog-series-s01e01\/train.parquet\")\nX_test = pd.read_parquet(\"..\/input\/kaggle-pog-series-s01e01\/test.parquet\")\n\nY = X.pop(\"target\")","f1a23c7b":"X","0f60ab2a":"X_test","6e23c06c":"features = [\"categoryId\", \"trending_date\", \"comments_disabled\", \"ratings_disabled\", \"publishedAt\", \"duration_seconds\",  \"has_thumbnail\"]","f8349c97":"X[\"trending_date_year\"] = [i.year for i in X[\"trending_date\"]]\nX[\"trending_date_month\"] = [i.month for i in X[\"trending_date\"]]\nX[\"trending_date_day\"] = [i.day for i in X[\"trending_date\"]]\n\nX[\"publishedAt_year\"] = [i.year for i in X[\"publishedAt\"]]\nX[\"publishedAt_month\"] = [i.month for i in X[\"publishedAt\"]]\nX[\"publishedAt_day\"] = [i.day for i in X[\"publishedAt\"]]\n\nX_test[\"trending_date_year\"] = [i.year for i in X_test[\"trending_date\"]]\nX_test[\"trending_date_month\"] = [i.month for i in X_test[\"trending_date\"]]\nX_test[\"trending_date_day\"] = [i.day for i in X_test[\"trending_date\"]]\n\nX_test[\"publishedAt_year\"] = [i.year for i in X_test[\"publishedAt\"]]\nX_test[\"publishedAt_month\"] = [i.month for i in X_test[\"publishedAt\"]]\nX_test[\"publishedAt_day\"] = [i.day for i in X_test[\"publishedAt\"]]\n\n\ndel(features[1])\ndel(features[-3])\n\nfeatures.append(\"trending_date_year\")\nfeatures.append(\"trending_date_month\")\nfeatures.append(\"trending_date_day\")\n\nfeatures.append(\"publishedAt_year\")\nfeatures.append(\"publishedAt_month\")\nfeatures.append(\"publishedAt_day\")\n\nX = X[features]\nX_test = X_test[features+[\"id\"]]","00253502":"X","e4b6d178":"imputer = SimpleImputer()\n\nl_X = pd.DataFrame(imputer.fit_transform(X))\nl_X_test = pd.DataFrame(imputer.transform(X_test.drop(\"id\", axis=1)))\n\nl_X.columns = X.columns\nl_X_test.columns = X_test.drop(\"id\", axis=1).columns\n\ntest_id = X_test[\"id\"]\n\nX = l_X\nX_test = l_X_test","c3575e0e":"X","e35ed161":"X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, random_state=0)","d8e11bea":"X_train[:10]","23d5ecb9":"def score(X_train=X_train, X_valid=X_valid, Y_train=Y_train, Y_valid=Y_valid, n_estims=10, max_depth=5):\n    model = XGBRegressor(n_estimators=n_estims, max_depth=max_depth)\n\n    model.fit(X_train, Y_train)\n    \n    return mean_squared_error(Y_valid, model.predict(X_valid))","6f956bd1":"# n_estims = [1, 10, 50, 100, 150, 200, 300, 400, 500]\n\n# errors = []\n\n# for i in n_estims:\n#     score_ = score(n_estims=i)\n#     print(f\"Estim: {i}, Score: {score_}\")\n#     errors.append(score_)\n    \n# plt.figure(figsize=(16, 9))\n\n# sns.lineplot(data=errors)","8123bbaf":"# max_depths = [1, 5, 10, 15, 20, 25, 30, 35, 40, 50]\n# errors = []\n\n# for i in max_depths:\n#     score_ = score(n_estims=500, max_depth=i)\n#     print(f\"Estim: {i}, Score: {score_}\")\n#     errors.append(score_)\n    \n# plt.figure(figsize=(16, 9))\n\n# sns.lineplot(data=errors)\n\n# I commented this code because it's takes a long time. So, the best max_depth is 10","6c82cade":"modelXG = XGBRegressor(n_estimators=500, max_depth=10, random_state=0, n_jobs=4, learning_rate=0.01)\n\nmodelXG.fit(X_train, Y_train, early_stopping_rounds=5, eval_set=[(X_valid, Y_valid)], verbose=10)","f454b9a2":"mean_squared_error(Y_valid, modelXG.predict(X_valid))","79bbfe26":"modelXG.fit(X, Y)","6f86df3b":"from tensorflow import keras\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping","9d23c294":"early_stop = EarlyStopping(\n    min_delta=0.001,\n    patience=5,\n    restore_best_weights=True\n)","905071b9":"X[\"trending_date_year\"] = X[\"trending_date_year\"] \/ 2021\nX[\"trending_date_month\"] = X[\"trending_date_month\"] \/ 12\nX[\"trending_date_day\"] = X[\"trending_date_day\"] \/ 31\n\nX[\"publishedAt_year\"] = X[\"publishedAt_year\"] \/ 2021\nX[\"publishedAt_month\"] = X[\"publishedAt_month\"] \/ 12\nX[\"publishedAt_day\"] = X[\"publishedAt_day\"] \/ 31\n\nX[\"categoryId\"] = X[\"categoryId\"] \/ 29","5e68c5fc":"X[\"comments_disabled\"] = X[\"comments_disabled\"].astype(np.int32)\nX[\"ratings_disabled\"] = X[\"ratings_disabled\"].astype(np.int32)","adf1cc75":"model = keras.Sequential([\n    BatchNormalization(input_shape=[len(X.columns)]),\n    Dense(16, activation='relu'),\n    Dense(16, activation='relu'),\n    Dense(1, activation='linear')\n])\n\nmodel.compile(\n    loss='mse',\n    optimizer='adam',\n    metrics=['accuracy']\n)","63d669c0":"X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, random_state=0)","feeb848b":"hist = model.fit(X_train, Y_train, epochs=50, validation_data=(X_valid, Y_valid), callbacks=[early_stop])","71f41dd7":"plt.plot(hist.history[\"loss\"])\nplt.plot(hist.history[\"val_loss\"])","409921bf":"predictions1 = modelXG.predict(X_test)","d9046c68":"X_test[\"trending_date_year\"] = X_test[\"trending_date_year\"] \/ 2021\nX_test[\"trending_date_month\"] = X_test[\"trending_date_month\"] \/ 12\nX_test[\"trending_date_day\"] = X_test[\"trending_date_day\"] \/ 31\n\nX_test[\"publishedAt_year\"] = X_test[\"publishedAt_year\"] \/ 2021\nX_test[\"publishedAt_month\"] = X_test[\"publishedAt_month\"] \/ 12\nX_test[\"publishedAt_day\"] = X_test[\"publishedAt_day\"] \/ 31\n\nX_test[\"categoryId\"] = X_test[\"categoryId\"] \/ 29","87d06a85":"X_test[\"comments_disabled\"] = X_test[\"comments_disabled\"].astype(np.int64)\nX_test[\"ratings_disabled\"] = X_test[\"ratings_disabled\"].astype(np.int64)","50ff25a5":"predictions2 = model.predict(X_test)\n\npreds = predictions1\n\n# for i in range(len(predictions1)):\n#     preds.append(np.array([predictions1[i], predictions2[i]]).mean()[0])\n\ndf = pd.DataFrame({\n    \"id\": test_id,\n    \"target\": np.array(preds).astype(np.float64)\n}).to_csv(\"submission.csv\", index=False)","dbe19f40":"pd.read_csv(\".\/submission.csv\")","510c7b4e":"Y.max()","7822f98c":"# Data preparation","ca0d515a":"# Now creating predictions for test set","ae026c8c":"# Create a model","512d1146":"# Feature engineering","7deec322":"# Creating an ensemble with XGBoost and Deep Learning (Fully-conected network)"}}