{"cell_type":{"537f4980":"code","8dcf1e5b":"code","0d5f739a":"code","1cfef08d":"code","7ebbd608":"code","39bda1cf":"code","34dbc52d":"markdown","5ccc747c":"markdown","20617f75":"markdown","e851a43a":"markdown","4821a635":"markdown","19b47c6b":"markdown","068fdb7b":"markdown"},"source":{"537f4980":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8dcf1e5b":"train_dataset_path = '\/kaggle\/input\/train.csv'\ntest_dataset_path = '\/kaggle\/input\/test.csv'\n\nimport pandas as pd\nfrom xgboost import XGBRegressor\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import mean_absolute_error","0d5f739a":"#Training Data-Set\nX_full = pd.read_csv(train_dataset_path, index_col='Id')\n\n#Tesing Data-Set\nX_test_full = pd.read_csv(test_dataset_path, index_col='Id')\n\n#Training Target \ny_input = X_full.SalePrice\n\n#Training Predicator\nX_input = X_full.drop(columns = 'SalePrice', axis =1)\n\n#Splliting the training data set to TRAINING & VALIDATION data set with 80, 20 ratio.\nX_train_full, X_validation_full, y_train, y_validation = train_test_split(X_input, y_input, train_size = 0.8, random_state =0)\n\n#Categorical Columns:\ncategorical_cols = [col for col in X_train_full.columns\n                    if X_train_full[col].nunique()<10 and X_train_full[col].dtype == 'object']\n#Numerical Columns:\nnumerical_cols = [col for col in X_train_full.columns if X_train_full[col].dtype in ['int64', 'float64']]\n\ntotal_valid_cols = categorical_cols + numerical_cols\n\n#Selecting Only valid columns from Training data set\nX_train = X_train_full[total_valid_cols]\n#Selecting Only valid columns from Validation data set\nX_validation = X_validation_full[total_valid_cols]\n#Selecting Only valid columns from Ting data set\nX_test = X_test_full[total_valid_cols]\nprint(categorical_cols)","1cfef08d":"#Imputing the missing values in Numberical Columns:\n#I've kept strategy as median, however it is subjective can be MEAN or MOST FREQUENT.\nNumerical_transform = SimpleImputer(strategy='median')\n\n#Imputing the missing values & OneHotEncoding in Categorical Columns:\n\n#I've kept strategy as MOST FREQUENT.\n# handle_unknown='ignore' : When an unknown category is encountered during testing, it will just ignore that. \n# If we don't set this with ignore code would result into error in an unknown category case.\nCategorical_transform = Pipeline(steps= [('cat_impute' ,SimpleImputer(strategy='most_frequent')), \n                                         ('cat_onehot', OneHotEncoder(handle_unknown = 'ignore'))])\n\n#ColumnTransformer constructor takes quite a few arguments:\n#The first argument is an array called transformers, which is a list of tuples. The array has the following elements in the same order:\n# - name: a name for the column transformer, which will make setting of parameters and searching of the transformer easy.\n# - transformer: here we\u2019re supposed to provide an estimator.We\u2019re Imputing the values in Numerical Cols and Imputing & encoding in\n#Categorical Columns\n# - column(s): the list of columns which you want to be transformed.\n     \n#Bundle Numerical and Categorical Pre-processing \ncolumn_transform = ColumnTransformer(transformers=[('num', Numerical_transform, numerical_cols),\n                                                   ('cat', Categorical_transform, categorical_cols)])","7ebbd608":"# XGB Model:\nmodel = XGBRegressor(n_estimators=1000,learning_rate = 0.01,random_state=0)\n\n#Bundling preprocessing of columns and MODEL.\nfinal_bundle = Pipeline(steps=[('column_transformation', column_transform), ('model', model)])\nfinal_bundle.fit(X_train, y_train)\ny_hat_validation = final_bundle.predict(X_validation)\n\nprint(\"MAE is\", mean_absolute_error(y_validation,y_hat_validation))","39bda1cf":"preds_test = final_bundle.predict(X_test)\noutput = pd.DataFrame({'ID': X_test.index, 'SalePrice': preds_test}).to_csv('submission_XGB.csv', index=False)","34dbc52d":"# STEP 5: Final Submittion","5ccc747c":"## <div style=\"text-align: center\">LEARN FEATURE ENGINEERING AND FEATURE SELECTION TECHNIQUES with MODEL - XGBOOST <\/div>\n<div style=\"text-align:center\"><img src=\"https:\/\/brainstation-23.com\/wp-content\/uploads\/2018\/12\/ML-real-state.png\"><\/div>","20617f75":"# ** STEP 1. - Libraries and Data Import**","e851a43a":"It is requested for:\n\n* **Beginners** to have a look (it's a straight forward as well as easy to understand attempt).\n* **Experienced** to review and provide insights to improve further.\nThe aim of this notebook is to formalize a workflow for XGBoost.\n\nIf you like it please Up-Vote. Any comments and recommendations would thus be very appreciated.\n","4821a635":"# STEP 2. - Extracting Data","19b47c6b":"# STEP 3. - Transforming Extracted Data","068fdb7b":"# STEP 4: MODEL Creation"}}