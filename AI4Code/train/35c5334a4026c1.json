{"cell_type":{"600d6be8":"code","56936a5f":"code","c8a16664":"code","c295d89c":"code","135134ab":"code","0649ee62":"code","91df608c":"code","437e1083":"code","8e2b7928":"code","bd70a665":"code","cce96e24":"code","51ab5d3b":"code","e80bf5e0":"code","60b6d27d":"code","0a6409a1":"code","cdd28f96":"code","52a140fb":"code","5ed7b830":"code","2dcdbf48":"code","96d22835":"code","0b46a90c":"code","b18b5ad6":"code","09438f91":"code","88447dd7":"code","31b3c15c":"code","5f4c2255":"code","0e4b31da":"code","6df8b66b":"code","d63dabbf":"code","0e2d41cb":"code","56e59564":"code","9cf16418":"code","03bacce7":"code","bac0e19a":"code","f9257795":"code","174704f7":"code","e2866123":"code","cc7fa573":"code","f4d7a619":"code","89cbbcf4":"code","41f187b0":"code","0b3dd383":"code","5ee3f468":"code","5c10b9bc":"code","e0cad9f5":"code","43f88ba1":"code","70f7d97c":"code","34987844":"code","874fddff":"code","810c7fac":"code","a31bd2ac":"code","a0fb9b4a":"code","c8b452c4":"code","bdab4feb":"code","545d8947":"markdown","527637ba":"markdown","6369af30":"markdown"},"source":{"600d6be8":"from IPython.core.display import display, HTML\n\nimport pandas as pd\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nimport os\nimport gc\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.metrics import r2_score\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport numpy.matlib\n\nfrom sklearn.cluster import KMeans\nimport lightgbm as lgb\n\npath_submissions = '\/'\n\ntarget_name = 'target'\nscores_folds = {}","56936a5f":"# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\n# Function to early stop with root mean squared percentage error\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","c8a16664":"kmeans_l_str = \"3 4 3 1 3 0 1 3 5 1 0 4 3 3 3 3 3 1 3 3 6 0 0 3 6 3 0 3 6 3 6 3 3 0 4 6 3 6 3 3 3 0 3 3 0 4 3 3 3 4 0 6 6 6 1 4 1 3 0 3 3 0 3 0 0 6 4 0 6 4 5 2 6 4 4 3 4 0 6 4 4 3 0 0 4 4 6 6 3 4 0 3 3 3 3 6 0 6 6 0 0 3 0 0 3 3 0 0 3 4 3 4\"\nkmeans_l_list = kmeans_l_str.split()\nkmeans_l_arr = np.array(kmeans_l_list).astype(int)","c295d89c":"# data directory\ndata_dir = '..\/input\/optiver-realized-volatility-prediction\/'\ndef lgbm_data():\n    # Function to calculate first WAP\n    def calc_wap1(df):\n        wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n        return wap\n\n    # Function to calculate second WAP\n    def calc_wap2(df):\n        wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n        return wap\n\n    def calc_wap3(df):\n        wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n        return wap\n\n    def calc_wap4(df):\n        wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n        return wap\n\n    # Function to calculate the log of the return\n    # Remember that logb(x \/ y) = logb(x) - logb(y)\n    def log_return(series):\n        return np.log(series).diff()\n\n    # Calculate the realized volatility\n    def realized_volatility(series):\n        return np.sqrt(np.sum(series**2))\n\n    # Function to count unique elements of a series\n    def count_unique(series):\n        return len(np.unique(series))\n\n    # Function to read our base train and test set\n    def read_test():\n        #train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n        test = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n        # Create a key to merge with book and trade data\n        #train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n        test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n        print(f'Our testing set has {test.shape[0]} rows')\n        return test\n\n    # Function to preprocess book data (for each stock id)\n    def book_preprocessor(file_path):\n        df = pd.read_parquet(file_path)\n        # Calculate Wap\n        df['wap1'] = calc_wap1(df)\n        df['wap2'] = calc_wap2(df)\n        df['wap3'] = calc_wap3(df)\n        df['wap4'] = calc_wap4(df)\n        # Calculate log returns\n        df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n        df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n        df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n        df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n        # Calculate wap balance\n        df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n        # Calculate spread\n        df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n        df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) \/ ((df['ask_price2'] + df['bid_price2']) \/ 2)\n        df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n        df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n        df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n        df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n        df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n\n        # Dict for aggregations\n        create_feature_dict = {\n            'wap1': [np.sum, np.std],\n            'wap2': [np.sum, np.std],\n            'wap3': [np.sum, np.std],\n            'wap4': [np.sum, np.std],\n            'log_return1': [realized_volatility],\n            'log_return2': [realized_volatility],\n            'log_return3': [realized_volatility],\n            'log_return4': [realized_volatility],\n            'wap_balance': [np.sum, np.max],\n            'price_spread':[np.sum, np.max],\n            'price_spread2':[np.sum, np.max],\n            'bid_spread':[np.sum, np.max],\n            'ask_spread':[np.sum, np.max],\n            'total_volume':[np.sum, np.max],\n            'volume_imbalance':[np.sum, np.max],\n            \"bid_ask_spread\":[np.sum,  np.max],\n        }\n        create_feature_dict_time = {\n            'log_return1': [realized_volatility],\n            'log_return2': [realized_volatility],\n            'log_return3': [realized_volatility],\n            'log_return4': [realized_volatility],\n        }\n\n        # Function to get group stats for different windows (seconds in bucket)\n        def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n            # Group by the window\n            df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n            # Rename columns joining suffix\n            df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n            # Add a suffix to differentiate windows\n            if add_suffix:\n                df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n            return df_feature\n\n        # Get the stats for different windows\n        df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n        df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n        df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n        df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n        df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n        df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n\n        # Merge all\n        df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n        df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n        df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n        df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n        df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n        # Drop unnecesary time_ids\n        df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n\n\n        # Create row_id so we can merge\n        stock_id = file_path.split('=')[1]\n        df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n        df_feature.drop(['time_id_'], axis = 1, inplace = True)\n        return df_feature\n\n    # Function to preprocess trade data (for each stock id)\n    def trade_preprocessor(file_path):\n        df = pd.read_parquet(file_path)\n        df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n        df['amount']=df['price']*df['size']\n        # Dict for aggregations\n        create_feature_dict = {\n            'log_return':[realized_volatility],\n            'seconds_in_bucket':[count_unique],\n            'size':[np.sum, np.max, np.min],\n            'order_count':[np.sum,np.max],\n            'amount':[np.sum,np.max,np.min],\n        }\n        create_feature_dict_time = {\n            'log_return':[realized_volatility],\n            'seconds_in_bucket':[count_unique],\n            'size':[np.sum],\n            'order_count':[np.sum],\n        }\n        # Function to get group stats for different windows (seconds in bucket)\n        def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n            # Group by the window\n            df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n            # Rename columns joining suffix\n            df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n            # Add a suffix to differentiate windows\n            if add_suffix:\n                df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n            return df_feature\n\n\n        # Get the stats for different windows\n        df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n        df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n        df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n        df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n        df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n        df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n\n        def tendency(price, vol):    \n            df_diff = np.diff(price)\n            val = (df_diff\/price[1:])*100\n            power = np.sum(val*vol[1:])\n            return(power)\n\n        lis = []\n        for n_time_id in df['time_id'].unique():\n            df_id = df[df['time_id'] == n_time_id]        \n            tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n            f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n            f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n            df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n            df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n            # new\n            abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n            energy = np.mean(df_id['price'].values**2)\n            iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n\n            # vol vars\n\n            abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n            energy_v = np.sum(df_id['size'].values**2)\n            iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n\n            lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                       'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n\n        df_lr = pd.DataFrame(lis)\n\n\n        df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n\n        # Merge all\n        df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n        df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n        df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n        df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n        df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n        # Drop unnecesary time_ids\n        df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id','time_id__100'], axis = 1, inplace = True)\n\n\n        df_feature = df_feature.add_prefix('trade_')\n        stock_id = file_path.split('=')[1]\n        df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n        df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n        return df_feature\n\n    # Function to get group stats for the stock_id and time_id\n    def get_time_stock(df):\n        vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n                    'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n                    'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n\n\n        # Group by the stock id\n        df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n        # Rename columns joining suffix\n        df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n        df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n        # Group by the stock id\n        df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n        # Rename columns joining suffix\n        df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n        df_time_id = df_time_id.add_suffix('_' + 'time')\n\n        # Merge with original dataframe\n        df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n        df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n        df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n        return df\n\n    # Funtion to make preprocessing function in parallel (for each stock id)\n    def preprocessor(list_stock_ids, is_train = True):\n\n        # Parrallel for loop\n        def for_joblib(stock_id):\n            # Train\n            if is_train:\n                file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n                file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n            # Test\n            else:\n                file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n                file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n\n            # Preprocess book and trade data and merge them\n            df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n\n            # Return the merge dataframe\n            return df_tmp\n\n        # Use parallel api to call paralle for loop\n        df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n        # Concatenate all the dataframes that return from Parallel\n        df = pd.concat(df, ignore_index = True)\n        return df\n    \n    # Read train and test\n    test = read_test()\n\n    # Get unique stock ids \n    #train_stock_ids = train['stock_id'].unique()\n    # Preprocess them using Parallel and our single stock id functions\n    #train_ = preprocessor(train_stock_ids, is_train = True)\n    #train = train.merge(train_, on = ['row_id'], how = 'left')\n\n    # Get unique stock ids \n    test_stock_ids = test['stock_id'].unique()\n    # Preprocess them using Parallel and our single stock id functions\n    test_ = preprocessor(test_stock_ids, is_train = False)\n    test = test.merge(test_, on = ['row_id'], how = 'left')\n\n    # Get group stats of time_id and stock_id\n    #train = get_time_stock(train)\n    test = get_time_stock(test)\n    \n    # replace by order sum (tau)\n    test['size_tau'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique'] )\n    #train['size_tau_450'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique_450'] )\n    #test['size_tau_450'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_450'] )\n    test['size_tau_400'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_400'] )\n    test['size_tau_300'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_300'] )\n    #train['size_tau_150'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique_150'] )\n    #test['size_tau_150'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_150'] )\n    test['size_tau_200'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_200'] )\n    \n\n    test['size_tau2'] = np.sqrt( 1\/ test['trade_order_count_sum'] )\n    #train['size_tau2_450'] = np.sqrt( 0.25\/ train['trade_order_count_sum'] )\n    #test['size_tau2_450'] = np.sqrt( 0.25\/ test['trade_order_count_sum'] )\n    test['size_tau2_400'] = np.sqrt( 0.33\/ test['trade_order_count_sum'] )\n    test['size_tau2_300'] = np.sqrt( 0.5\/ test['trade_order_count_sum'] )\n    #train['size_tau2_150'] = np.sqrt( 0.75\/ train['trade_order_count_sum'] )\n    #test['size_tau2_150'] = np.sqrt( 0.75\/ test['trade_order_count_sum'] )\n    test['size_tau2_200'] = np.sqrt( 0.66\/ test['trade_order_count_sum'] )\n\n    # delta tau\n    test['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']\n    \n    train = pd.read_csv(\"..\/input\/lgbm-data\/train_lgb_preKmeans.csv\")\n    \n    train = train.drop(\"Unnamed: 0\", axis = 1)\n    \n    # making agg features\n\n    train_p = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n    train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n\n    corr = train_p.corr()\n\n    ids = corr.index\n\n    kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n    print(kmeans.labels_)\n\n    l = []\n    for n in range(7):\n        l.append ( [ (x-1) for x in ( (ids+1)*(kmeans_l_arr == n)) if x > 0] )\n\n\n    mat = []\n    matTest = []\n\n    n = 0\n    for ind in l:\n        print(ind)\n        newDf = train.loc[train['stock_id'].isin(ind) ]\n        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n        newDf.loc[:,'stock_id'] = str(n)+'c1'\n        mat.append ( newDf )\n\n        newDf = test.loc[test['stock_id'].isin(ind) ]    \n        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n        newDf.loc[:,'stock_id'] = str(n)+'c1'\n        matTest.append ( newDf )\n\n        n+=1\n\n    mat1 = pd.concat(mat).reset_index()\n    mat1.drop(columns=['target'],inplace=True)\n\n    mat2 = pd.concat(matTest).reset_index()\n    \n    mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n    mat1 = mat1.pivot(index='time_id', columns='stock_id')\n    mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n    mat1.reset_index(inplace=True)\n\n    mat2 = mat2.pivot(index='time_id', columns='stock_id')\n    mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n    mat2.reset_index(inplace=True)\n    \n    nnn = ['time_id',\n         'log_return1_realized_volatility_0c1',\n         'log_return1_realized_volatility_1c1',     \n         'log_return1_realized_volatility_3c1',\n         'log_return1_realized_volatility_4c1',     \n         'log_return1_realized_volatility_6c1',\n         'total_volume_sum_0c1',\n         'total_volume_sum_1c1', \n         'total_volume_sum_3c1',\n         'total_volume_sum_4c1', \n         'total_volume_sum_6c1',\n         'trade_size_sum_0c1',\n         'trade_size_sum_1c1', \n         'trade_size_sum_3c1',\n         'trade_size_sum_4c1', \n         'trade_size_sum_6c1',\n         'trade_order_count_sum_0c1',\n         'trade_order_count_sum_1c1',\n         'trade_order_count_sum_3c1',\n         'trade_order_count_sum_4c1',\n         'trade_order_count_sum_6c1',      \n         'price_spread_sum_0c1',\n         'price_spread_sum_1c1',\n         'price_spread_sum_3c1',\n         'price_spread_sum_4c1',\n         'price_spread_sum_6c1',   \n         'bid_spread_sum_0c1',\n         'bid_spread_sum_1c1',\n         'bid_spread_sum_3c1',\n         'bid_spread_sum_4c1',\n         'bid_spread_sum_6c1',       \n         'ask_spread_sum_0c1',\n         'ask_spread_sum_1c1',\n         'ask_spread_sum_3c1',\n         'ask_spread_sum_4c1',\n         'ask_spread_sum_6c1',   \n         'volume_imbalance_sum_0c1',\n         'volume_imbalance_sum_1c1',\n         'volume_imbalance_sum_3c1',\n         'volume_imbalance_sum_4c1',\n         'volume_imbalance_sum_6c1',       \n         'bid_ask_spread_sum_0c1',\n         'bid_ask_spread_sum_1c1',\n         'bid_ask_spread_sum_3c1',\n         'bid_ask_spread_sum_4c1',\n         'bid_ask_spread_sum_6c1',\n         'size_tau2_0c1',\n         'size_tau2_1c1',\n         'size_tau2_3c1',\n         'size_tau2_4c1',\n         'size_tau2_6c1'] \n    train = pd.merge(train,mat1[nnn],how='left',on='time_id')\n    test = pd.merge(test,mat2[nnn],how='left',on='time_id')\n    \n    return train, test\ntrain, test = lgbm_data()","135134ab":"train[[\"target\"]]","0649ee62":"lgb_oof_predictions = np.zeros(train.shape[0])\nlgb_pred_predictions = np.zeros(test.shape[0])\nfor fold in range(5):\n    print(f'lgb fold {fold}')\n\n    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]        \n\n    model_lgbm = lgb.Booster(model_file=\"..\/input\/lgbm-baseline-2\/model_\"+str(fold)+\".txt\")\n    \n    lgb_oof_predictions += model_lgbm.predict(train[features]) \/ 5\n\n    lgb_pred_predictions += model_lgbm.predict(test[features]) \/ 5","91df608c":"train[\"target\"].values.reshape(-1)","437e1083":"print(rmspe(train[\"target\"].values.reshape(-1), lgb_oof_predictions))","8e2b7928":"sub_lgb = pd.read_csv(\"..\/input\/optiver-realized-volatility-prediction\/sample_submission.csv\")\nsub_lgb","bd70a665":"sub_lgb[\"target\"] = lgb_pred_predictions","cce96e24":"sub_lgb","51ab5d3b":"sub_lgb.to_csv(\"submission.csv\", index=False)","e80bf5e0":"from IPython.core.display import display, HTML\n\nimport glob\nimport os\nimport gc\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport numpy.matlib\n\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.preprocessing import MinMaxScaler, QuantileTransformer\nfrom sklearn.metrics import r2_score\nfrom sklearn.cluster import KMeans\n\nfrom numpy.random import seed\nseed(42)\n\nimport tensorflow as tf\ntf.random.set_seed(42)\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.backend import sigmoid\nfrom tensorflow.keras.utils import get_custom_objects\nfrom tensorflow.keras.layers import Activation\n\nimport warnings\nwarnings.filterwarnings('ignore')","60b6d27d":"train_p = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\ntrain_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\ncorr = train_p.corr()\nids = corr.index\nkmeans_l_str = \"3 4 3 1 3 0 1 3 5 1 0 4 3 3 3 3 3 1 3 3 6 0 0 3 6 3 0 3 6 3 6 3 3 0 4 6 3 6 3 3 3 0 3 3 0 4 3 3 3 4 0 6 6 6 1 4 1 3 0 3 3 0 3 0 0 6 4 0 6 4 5 2 6 4 4 3 4 0 6 4 4 3 0 0 4 4 6 6 3 4 0 3 3 3 3 6 0 6 6 0 0 3 0 0 3 3 0 0 3 4 3 4\"\nkmeans_l_list = kmeans_l_str.split()\nkmeans_l_arr = np.array(kmeans_l_list).astype(int)","0a6409a1":"corr","cdd28f96":"def ffnn_data():    \n    # data directory\n    data_dir = '..\/input\/optiver-realized-volatility-prediction\/'\n\n    def read_test():\n        # Function to read our base train and test set\n\n        #train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n        test = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n\n        # Create a key to merge with book and trade data\n        #train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n        test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n        print(f'Our testing set has {test.shape[0]} rows')\n\n        return test\n\n    test = read_test()\n\n    def calc_wap1(df):\n        # Function to calculate first WAP\n        wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n        return wap\n\n    def calc_wap2(df):\n        # Function to calculate second WAP\n        wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n        return wap\n\n    def log_return(series):\n        # Function to calculate the log of the return\n        return np.log(series).diff()\n\n    def realized_volatility(series):\n        # Calculate the realized volatility\n        return np.sqrt(np.sum(series**2))\n\n    def count_unique(series):\n        # Function to count unique elements of a series\n        return len(np.unique(series))\n\n    def book_preprocessor(file_path):\n        # Function to preprocess book data (for each stock id)\n\n        df = pd.read_parquet(file_path)\n\n        # Calculate Wap\n        df['wap1'] = calc_wap1(df)\n        df['wap2'] = calc_wap2(df)\n\n        # Calculate log returns\n        df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n        df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n\n        # Calculate wap balance\n        df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n\n        # Calculate spread\n        df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n        df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) \/ ((df['ask_price2'] + df['bid_price2']) \/ 2)\n        df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n        df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n        df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n        df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n        df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n\n        # Dict for aggregations\n        create_feature_dict = {\n            'wap1': [np.sum, np.mean, np.std],\n            'wap2': [np.sum, np.mean, np.std],\n            'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n            'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n            'wap_balance': [np.sum, np.mean, np.std],\n            'price_spread':[np.sum, np.mean, np.std],\n            'price_spread2':[np.sum, np.mean, np.std],\n            'bid_spread':[np.sum, np.mean, np.std],\n            'ask_spread':[np.sum, np.mean, np.std],\n            'total_volume':[np.sum, np.mean, np.std],\n            'volume_imbalance':[np.sum, np.mean, np.std],\n            \"bid_ask_spread\":[np.sum, np.mean, np.std],\n        }\n\n        def get_stats_window(seconds_in_bucket, add_suffix = False):\n            # Function to get group stats for different windows (seconds in bucket)\n\n            # Group by the window\n            df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n\n            # Rename columns joining suffix\n            df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n\n            # Add a suffix to differentiate windows\n            if add_suffix:\n                df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n            return df_feature\n\n        # Get the stats for different windows\n        df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n        df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n        df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n        df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n\n        # Merge all\n        df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n        df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n        df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n\n        # Drop unnecesary time_ids\n        df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200'], axis = 1, inplace = True)\n\n\n        # Create row_id so we can merge\n        stock_id = file_path.split('=')[1]\n        df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n        df_feature.drop(['time_id_'], axis = 1, inplace = True)\n\n        return df_feature\n\n\n    def trade_preprocessor(file_path):\n        # Function to preprocess trade data (for each stock id)\n\n        df = pd.read_parquet(file_path)\n        df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n\n        # Dict for aggregations\n        create_feature_dict = {\n            'log_return':[realized_volatility],\n            'seconds_in_bucket':[count_unique],\n            'size':[np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n            'order_count':[np.mean,np.sum,np.max],\n        }\n\n        def get_stats_window(seconds_in_bucket, add_suffix = False):\n            # Function to get group stats for different windows (seconds in bucket)\n\n            # Group by the window\n            df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n\n            # Rename columns joining suffix\n            df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n\n            # Add a suffix to differentiate windows\n            if add_suffix:\n                df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n            return df_feature\n\n        # Get the stats for different windows\n        df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n        df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n        df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n        df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n\n        def tendency(price, vol):    \n            df_diff = np.diff(price)\n            val = (df_diff\/price[1:])*100\n            power = np.sum(val*vol[1:])\n            return(power)\n\n        lis = []\n        for n_time_id in df['time_id'].unique():\n            df_id = df[df['time_id'] == n_time_id]        \n            tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n            f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n            f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n            df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n            df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n            abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n            energy = np.mean(df_id['price'].values**2)\n            iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n            abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n            energy_v = np.sum(df_id['size'].values**2)\n            iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n\n            lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                       'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n\n        df_lr = pd.DataFrame(lis)\n\n\n        df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n\n        # Merge all\n        df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n        df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n        df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n\n        # Drop unnecesary time_ids\n        df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200','time_id'], axis = 1, inplace = True)\n        df_feature = df_feature.add_prefix('trade_')\n        stock_id = file_path.split('=')[1]\n        df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n        df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n\n        return df_feature\n\n\n    def get_time_stock(df):\n        # Function to get group stats for the stock_id and time_id\n\n        # Get realized volatility columns\n        vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n                    'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n                    'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n\n        # Group by the stock id\n        df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n\n        # Rename columns joining suffix\n        df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n        df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n        # Group by the stock id\n        df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n\n        # Rename columns joining suffix\n        df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n        df_time_id = df_time_id.add_suffix('_' + 'time')\n\n        # Merge with original dataframe\n        df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n        df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n        df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n\n        return df\n\n\n    def preprocessor(list_stock_ids, is_train = True):\n        # Funtion to make preprocessing function in parallel (for each stock id)\n\n        # Parrallel for loop\n        def for_joblib(stock_id):\n            # Train\n            if is_train:\n                file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n                file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n            # Test\n            else:\n                file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n                file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n\n            # Preprocess book and trade data and merge them\n            df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n\n            # Return the merge dataframe\n            return df_tmp\n\n        # Use parallel api to call paralle for loop\n        df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n\n        # Concatenate all the dataframes that return from Parallel\n        df = pd.concat(df, ignore_index = True)\n\n        return df\n\n\n    def rmspe(y_true, y_pred):\n        # Function to calculate the root mean squared percentage error\n        return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\n    def feval_rmspe(y_pred, lgb_train):\n        # Function to early stop with root mean squared percentage error\n        y_true = lgb_train.get_label()\n        return 'RMSPE', rmspe(y_true, y_pred), False\n    \n    # Get unique stock ids \n    #train_stock_ids = train['stock_id'].unique()\n\n    # Preprocess them using Parallel and our single stock id functions\n    #train_ = preprocessor(train_stock_ids, is_train = True)\n    #train = train.merge(train_, on = ['row_id'], how = 'left')\n\n    # Get unique stock ids \n    test_stock_ids = test['stock_id'].unique()\n\n    # Preprocess them using Parallel and our single stock id functions\n    test_ = preprocessor(test_stock_ids, is_train = False)\n    test = test.merge(test_, on = ['row_id'], how = 'left')\n\n    # Get group stats of time_id and stock_id\n    #train = get_time_stock(train)\n    test = get_time_stock(test)\n    \n    # replace by order sum (tau)\n    #train['size_tau'] = np.sqrt(1\/train['trade_seconds_in_bucket_count_unique'])\n    test['size_tau'] = np.sqrt(1\/test['trade_seconds_in_bucket_count_unique'])\n    #train['size_tau_400'] = np.sqrt(1\/train['trade_seconds_in_bucket_count_unique_400'])\n    test['size_tau_400'] = np.sqrt(1\/test['trade_seconds_in_bucket_count_unique_400'])\n    #train['size_tau_300'] = np.sqrt(1\/train['trade_seconds_in_bucket_count_unique_300'])\n    test['size_tau_300'] = np.sqrt(1\/test['trade_seconds_in_bucket_count_unique_300'])\n    #train['size_tau_200'] = np.sqrt(1\/train['trade_seconds_in_bucket_count_unique_200'])\n    test['size_tau_200'] = np.sqrt(1\/test['trade_seconds_in_bucket_count_unique_200'])\n    \n    # tau2 \n    #train['size_tau2'] = np.sqrt(1\/train['trade_order_count_sum'])\n    test['size_tau2'] = np.sqrt(1\/test['trade_order_count_sum'])\n    #train['size_tau2_400'] = np.sqrt(0.25\/train['trade_order_count_sum'])\n    test['size_tau2_400'] = np.sqrt(0.25\/test['trade_order_count_sum'])\n    #train['size_tau2_300'] = np.sqrt(0.5\/train['trade_order_count_sum'])\n    test['size_tau2_300'] = np.sqrt(0.5\/test['trade_order_count_sum'])\n    #train['size_tau2_200'] = np.sqrt(0.75\/train['trade_order_count_sum'])\n    test['size_tau2_200'] = np.sqrt(0.75\/test['trade_order_count_sum'])\n\n    # delta tau\n    #train['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\n    test['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']\n    \n    \n    \n    train = pd.read_csv(\"..\/input\/ffnn-data\/FFNN_trainPrefoldAndqt.csv\")\n    train = train.drop(\"Unnamed: 0\", axis=1)\n    \n    train.replace([np.inf, -np.inf], np.nan,inplace=True)\n    test.replace([np.inf, -np.inf], np.nan,inplace=True)\n    \n\n    # Code to add the just the read data after first execution\n\n    \n    \n    colNames = list(train)\n    colNames.remove('time_id')\n    colNames.remove('target')\n    colNames.remove('row_id')\n    colNames.remove('stock_id')\n    \n    train.replace([np.inf, -np.inf], np.nan,inplace=True)\n    test.replace([np.inf, -np.inf], np.nan,inplace=True)\n    qt_train = []\n\n    for col in colNames:\n        qt = QuantileTransformer(random_state=21,n_quantiles=2000, output_distribution='normal')\n        train[col] = qt.fit_transform(train[[col]])\n        test[col] = qt.transform(test[[col]])    \n        qt_train.append(qt)\n    \n    print(\"FFNN_trainAfterqtPreAGG:\")\n    print(train[\"size_tau2_d\"]) #good\n    \n    preagg_train = train.copy()\n    # Making agg features\n\n    train_p = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n    train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n    corr = train_p.corr()\n    ids = corr.index\n    \n    print(kmeans_l_arr)\n    l = []\n    for n in range(7):\n        l.append ( [ (x-1) for x in ( (ids+1)*(kmeans_l_arr == n)) if x > 0] )\n\n    mat = []\n    matTest = []\n    n = 0\n    for ind in l:\n        print(ind)\n        newDf = train.loc[train['stock_id'].isin(ind) ]\n        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n        newDf.loc[:,'stock_id'] = str(n)+'c1'\n        mat.append ( newDf )\n        newDf = test.loc[test['stock_id'].isin(ind) ]    \n        newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n        newDf.loc[:,'stock_id'] = str(n)+'c1'\n        matTest.append ( newDf )\n        n+=1\n\n    mat1 = pd.concat(mat).reset_index()\n    mat1.drop(columns=['target'],inplace=True)\n    mat2 = pd.concat(matTest).reset_index()\n    \n    matTest = []\n    mat = []\n    kmeans = []\n    \n    mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n    \n    mat1 = mat1.pivot(index='time_id', columns='stock_id')\n    mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n    mat1.reset_index(inplace=True)\n\n    mat2 = mat2.pivot(index='time_id', columns='stock_id')\n    mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n    mat2.reset_index(inplace=True)\n    \n    nnn = ['time_id',\n     'log_return1_realized_volatility_0c1',\n     'log_return1_realized_volatility_1c1',\n     'log_return1_realized_volatility_3c1',\n     'log_return1_realized_volatility_4c1',     \n     'log_return1_realized_volatility_6c1',\n     'total_volume_mean_0c1',\n     'total_volume_mean_1c1', \n     'total_volume_mean_3c1',\n     'total_volume_mean_4c1', \n     'total_volume_mean_6c1',\n     'trade_size_mean_0c1',\n     'trade_size_mean_1c1', \n     'trade_size_mean_3c1',\n     'trade_size_mean_4c1', \n     'trade_size_mean_6c1',\n     'trade_order_count_mean_0c1',\n     'trade_order_count_mean_1c1',\n     'trade_order_count_mean_3c1',\n     'trade_order_count_mean_4c1',\n     'trade_order_count_mean_6c1',      \n     'price_spread_mean_0c1',\n     'price_spread_mean_1c1',\n     'price_spread_mean_3c1',\n     'price_spread_mean_4c1',\n     'price_spread_mean_6c1',   \n     'bid_spread_mean_0c1',\n     'bid_spread_mean_1c1',\n     'bid_spread_mean_3c1',\n     'bid_spread_mean_4c1',\n     'bid_spread_mean_6c1',       \n     'ask_spread_mean_0c1',\n     'ask_spread_mean_1c1',\n     'ask_spread_mean_3c1',\n     'ask_spread_mean_4c1',\n     'ask_spread_mean_6c1',   \n     'volume_imbalance_mean_0c1',\n     'volume_imbalance_mean_1c1',\n     'volume_imbalance_mean_3c1',\n     'volume_imbalance_mean_4c1',\n     'volume_imbalance_mean_6c1',       \n     'bid_ask_spread_mean_0c1',\n     'bid_ask_spread_mean_1c1',\n     'bid_ask_spread_mean_3c1',\n     'bid_ask_spread_mean_4c1',\n     'bid_ask_spread_mean_6c1',\n     'size_tau2_0c1',\n     'size_tau2_1c1',\n     'size_tau2_3c1',\n     'size_tau2_4c1',\n     'size_tau2_6c1'] \n    \n    pre_merge_train = train.copy()\n    \n    train = pd.merge(train,mat1[nnn],how='left',on='time_id')\n    test = pd.merge(test,mat2[nnn],how='left',on='time_id')\n    \n    return train, test, preagg_train, pre_merge_train\ntrain, test, preagg_train, pre_merge_train = ffnn_data()","52a140fb":"colNames = list(train)\ncolNames.remove('time_id')\ncolNames.remove('target')\ncolNames.remove('row_id')\ncolNames.remove('stock_id')","5ed7b830":"# Thanks to https:\/\/bignerdranch.com\/blog\/implementing-swish-activation-function-in-keras\/\ndef swish(x, beta = 1):\n    return (x * sigmoid(beta * x))\n\nget_custom_objects().update({'swish': Activation(swish)})","2dcdbf48":"es = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=20, verbose=0,\n    mode='min',restore_best_weights=True)\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.2, patience=7, verbose=0,\n    mode='min')","96d22835":"def root_mean_squared_per_error(y_true, y_pred):\n         return K.sqrt(K.mean(K.square( (y_true - y_pred)\/ y_true )))","0b46a90c":"path_submissions = '\/'\n\ntarget_name = 'target'\nscores_folds = {}","b18b5ad6":"model_name = 'NN'\npred_name = 'pred_{}'.format(model_name)\n\nn_folds = 5\nkf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2020)\nscores_folds[model_name] = []\ncounter = 1\n\nfeatures_to_consider = list(train)\n\nfeatures_to_consider.remove('time_id')\nfeatures_to_consider.remove('target')\nfeatures_to_consider.remove('row_id')\ntry:\n    features_to_consider.remove('pred_NN')\nexcept:\n    pass\n\ntrain[features_to_consider] = train[features_to_consider].fillna(train[features_to_consider].mean())\ntest[features_to_consider] = test[features_to_consider].fillna(train[features_to_consider].mean())\n\ntrain[pred_name] = 0\ntest['target'] = 0","09438f91":"\ntry:\n    features_to_consider.remove('stock_id')\nexcept:\n    pass","88447dd7":"num_data = train[features_to_consider]\n\nscaler = MinMaxScaler(feature_range=(-1, 1))         \nnum_data = scaler.fit_transform(num_data.values)    \n\ncat_data = train['stock_id']\ntarget =  train[\"target\"]\n\nnum_data_test = test[features_to_consider]\nnum_data_test = scaler.transform(num_data_test.values)\ncat_data_test = test['stock_id']\n\nffnn_oof_predictions =  np.zeros((len(train), 1))\nffnn_test_predictions =  np.zeros((len(test), 1))\n\nfor fold in range(5):\n    print(\"fold:\", fold)\n    model = tf.keras.models.load_model('..\/input\/fork-of-stock-embedding-ffnn-upgrade-3d-b6ad\/saved_model\/model_'+str(fold)+'\/', compile=False)\n    \"\"\"\n    model.compile(\n        keras.optimizers.Adam(learning_rate=0.0060),\n        loss=root_mean_squared_per_error\n    )\n    \"\"\"\n    ffnn_oof_predictions += model.predict([cat_data, num_data])\/5\n    ffnn_test_predictions += model.predict([cat_data_test, num_data_test])\/5\n\n\"\"\"\narray([[0.00309603],\n       [0.00231337],\n       [0.00231337]], dtype=float32)\n\"\"\"\nffnn_test_predictions","31b3c15c":"train","5f4c2255":"del model","0e4b31da":"train[\"target\"].values.reshape(-1), ffnn_oof_predictions.reshape(-1)","6df8b66b":"print(rmspe(train[\"target\"].values.reshape(-1), ffnn_oof_predictions.reshape(-1)))","d63dabbf":"!pip -q install ..\/input\/pytorchtabnet\/pytorch_tabnet-3.1.1-py3-none-any.whl","0e2d41cb":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy.matlib\n\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\n\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom joblib import Parallel, delayed\n\nimport shutil\nimport glob\n\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\nfrom sklearn.metrics import r2_score\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import KFold\n\nfrom pytorch_tabnet.metrics import Metric\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\nimport torch\nfrom torch.optim import Adam, SGD\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n\n\n# setting some globl config\n\nplt.style.use('ggplot')\norange_black = [\n    '#fdc029', '#df861d', '#FF6347', '#aa3d01', '#a30e15', '#800000', '#171820'\n]\nplt.rcParams['figure.figsize'] = (16,9)\nplt.rcParams[\"figure.facecolor\"] = '#FFFACD'\nplt.rcParams[\"axes.facecolor\"] = '#FFFFE0'\nplt.rcParams[\"axes.grid\"] = True\nplt.rcParams[\"grid.color\"] = orange_black[3]\nplt.rcParams[\"grid.alpha\"] = 0.5\nplt.rcParams[\"grid.linestyle\"] = '--'\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","56e59564":"import psutil\npsutil.cpu_count()","9cf16418":"gpu_info = !nvidia-smi\ngpu_info = '\\n'.join(gpu_info)\nprint(gpu_info)","03bacce7":"def tabnet_data():\n    data_dir =  '..\/input\/optiver-realized-volatility-prediction\/'\n    def read_test():\n        # Function to read our base train and test set\n\n        #train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n        test = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n\n        # Create a key to merge with book and trade data\n        #train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n        test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n        #print(f'Our training set has {train.shape[0]} rows')\n        print(f'Our test set has {test.shape[0]} rows')\n        #print(f'Our training set has {train.isna().sum().sum()} missing values')\n        print(f'Our test set has {test.isna().sum().sum()} missing values')\n\n        return test\n\n    test = read_test()\n    \n\n    def calc_wap1(df):\n        # Function to calculate first WAP\n        wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n        return wap\n\n    def calc_wap2(df):\n        # Function to calculate second WAP\n        wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n        return wap\n\n    def log_return(series):\n        # Function to calculate the log of the return\n        return np.log(series).diff()\n\n    def realized_volatility(series):\n        # Calculate the realized volatility\n        return np.sqrt(np.sum(series**2))\n\n    def count_unique(series):\n        # Function to count unique elements of a series\n        return len(np.unique(series))\n\n    def book_preprocessor(file_path):\n        # Function to preprocess book data (for each stock id)\n\n        df = pd.read_parquet(file_path)\n\n        # Calculate Wap\n        df['wap1'] = calc_wap1(df)\n        df['wap2'] = calc_wap2(df)\n\n        # Calculate log returns\n        df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n        df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n\n        # Calculate wap balance\n        df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n\n        # Calculate spread\n        df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n        df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) \/ ((df['ask_price2'] + df['bid_price2']) \/ 2)\n        df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n        df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n        df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n        df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n        df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n\n        # Dict for aggregations\n        create_feature_dict = {\n            'wap1': [np.sum, np.mean, np.std],\n            'wap2': [np.sum, np.mean, np.std],\n            'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n            'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n            'wap_balance': [np.sum, np.mean, np.std],\n            'price_spread':[np.sum, np.mean, np.std],\n            'price_spread2':[np.sum, np.mean, np.std],\n            'bid_spread':[np.sum, np.mean, np.std],\n            'ask_spread':[np.sum, np.mean, np.std],\n            'total_volume':[np.sum, np.mean, np.std],\n            'volume_imbalance':[np.sum, np.mean, np.std],\n            \"bid_ask_spread\":[np.sum, np.mean, np.std],\n        }\n\n        def get_stats_window(seconds_in_bucket, add_suffix = False):\n            # Function to get group stats for different windows (seconds in bucket)\n\n            # Group by the window\n            df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n\n            # Rename columns joining suffix\n            df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n\n            # Add a suffix to differentiate windows\n            if add_suffix:\n                df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n            return df_feature\n\n        # Get the stats for different windows\n        df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n        df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n        df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n        df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n\n        # Merge all\n        df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n        df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n        df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n\n        # Drop unnecesary time_ids\n        df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200'], axis = 1, inplace = True)\n\n\n        # Create row_id so we can merge\n        stock_id = file_path.split('=')[1]\n        df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n        df_feature.drop(['time_id_'], axis = 1, inplace = True)\n\n        return df_feature\n\n\n    def trade_preprocessor(file_path):\n        # Function to preprocess trade data (for each stock id)\n\n        df = pd.read_parquet(file_path)\n        df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n\n        # Dict for aggregations\n        create_feature_dict = {\n            'log_return':[realized_volatility],\n            'seconds_in_bucket':[count_unique],\n            'size':[np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n            'order_count':[np.mean,np.sum,np.max],\n        }\n\n        def get_stats_window(seconds_in_bucket, add_suffix = False):\n            # Function to get group stats for different windows (seconds in bucket)\n\n            # Group by the window\n            df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n\n            # Rename columns joining suffix\n            df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n\n            # Add a suffix to differentiate windows\n            if add_suffix:\n                df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n            return df_feature\n\n        # Get the stats for different windows\n        df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n        df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n        df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n        df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n\n        def tendency(price, vol):    \n            df_diff = np.diff(price)\n            val = (df_diff\/price[1:])*100\n            power = np.sum(val*vol[1:])\n            return(power)\n\n        lis = []\n        for n_time_id in df['time_id'].unique():\n            df_id = df[df['time_id'] == n_time_id]        \n            tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n            f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n            f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n            df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n            df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n            abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n            energy = np.mean(df_id['price'].values**2)\n            iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n            abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n            energy_v = np.sum(df_id['size'].values**2)\n            iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n\n            lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                       'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n\n        df_lr = pd.DataFrame(lis)\n\n\n        df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n\n        # Merge all\n        df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n        df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n        df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n\n        # Drop unnecesary time_ids\n        df_feature.drop(['time_id__400', 'time_id__300', 'time_id__200','time_id'], axis = 1, inplace = True)\n        df_feature = df_feature.add_prefix('trade_')\n        stock_id = file_path.split('=')[1]\n        df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n        df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n\n        def order_sum(df, sec:str):\n            new_col = 'size_tau' + sec\n            bucket_col = 'trade_seconds_in_bucket_count_unique' + sec\n            df[new_col] = np.sqrt(1\/df[bucket_col])\n\n            new_col2 = 'size_tau2' + sec\n            order_col = 'trade_order_count_sum' + sec\n            df[new_col2] = np.sqrt(1\/df[order_col])\n\n            if sec == '400_':\n                df['size_tau2_d'] = df['size_tau2_400'] - df['size_tau2']\n\n\n\n        for sec in ['','_200','_300','_400']:\n            order_sum(df_feature, sec)\n\n        df_feature['size_tau2_d'] = df_feature['size_tau2_400'] - df_feature['size_tau2']\n\n        return df_feature\n\n\n    def get_time_stock(df):\n        # Function to get group stats for the stock_id and time_id\n\n        # Get realized volatility columns\n        vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n                    'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n                    'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n\n        # Group by the stock id\n        df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n\n        # Rename columns joining suffix\n        df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n        df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n        # Group by the stock id\n        df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n\n        # Rename columns joining suffix\n        df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n        df_time_id = df_time_id.add_suffix('_' + 'time')\n\n        # Merge with original dataframe\n        df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n        df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n        df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n\n        return df\n\n    def create_agg_features(train, test):\n\n        # Making agg features\n\n        train_p = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n        train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n        corr = train_p.corr()\n        ids = corr.index\n        kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n        l = []\n        for n in range(7):\n            l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n\n        mat = []\n        matTest = []\n        n = 0\n        for ind in l:\n            newDf = train.loc[train['stock_id'].isin(ind) ]\n            newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n            newDf.loc[:,'stock_id'] = str(n)+'c1'\n            mat.append ( newDf )\n            newDf = test.loc[test['stock_id'].isin(ind) ]    \n            newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n            newDf.loc[:,'stock_id'] = str(n)+'c1'\n            matTest.append ( newDf )\n            n+=1\n\n        mat1 = pd.concat(mat).reset_index()\n        mat1.drop(columns=['target'],inplace=True)\n        mat2 = pd.concat(matTest).reset_index()\n\n        mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n\n        mat1 = mat1.pivot(index='time_id', columns='stock_id')\n        mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n        mat1.reset_index(inplace=True)\n\n        mat2 = mat2.pivot(index='time_id', columns='stock_id')\n        mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n        mat2.reset_index(inplace=True)\n\n        prefix = ['log_return1_realized_volatility', 'total_volume_mean', 'trade_size_mean', 'trade_order_count_mean','price_spread_mean','bid_spread_mean','ask_spread_mean',\n                  'volume_imbalance_mean', 'bid_ask_spread_mean','size_tau2']\n        selected_cols=mat1.filter(regex='|'.join(f'^{x}.(0|1|3|4|6)c1' for x in prefix)).columns.tolist()\n        selected_cols.append('time_id')\n\n        train_m = pd.merge(train,mat1[selected_cols],how='left',on='time_id')\n        test_m = pd.merge(test,mat2[selected_cols],how='left',on='time_id')\n\n        # filling missing values with train means\n\n        features = [col for col in train_m.columns.tolist() if col not in ['time_id','target','row_id']]\n        train_m[features] = train_m[features].fillna(train_m[features].mean())\n        test_m[features] = test_m[features].fillna(train_m[features].mean())\n\n        return train_m, test_m\n\n\n    def preprocessor(list_stock_ids, is_train = True):\n        # Funtion to make preprocessing function in parallel (for each stock id)\n\n        # Parrallel for loop\n        def for_joblib(stock_id):\n            # Train\n            if is_train:\n                file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n                file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n            # Test\n            else:\n                file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n                file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n\n            # Preprocess book and trade data and merge them\n            df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n\n            # Return the merge dataframe\n            return df_tmp\n\n        # Use parallel api to call paralle for loop\n        df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n\n        # Concatenate all the dataframes that return from Parallel\n        df = pd.concat(df, ignore_index = True)\n\n        return df\n    \n    def create_agg_features(train, test):\n\n        # Making agg features\n\n        train_p = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n        train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n        corr = train_p.corr()\n        ids = corr.index\n        kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n        l = []\n        for n in range(7):\n            l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n\n        mat = []\n        matTest = []\n        n = 0\n        for ind in l:\n            newDf = train.loc[train['stock_id'].isin(ind) ]\n            newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n            newDf.loc[:,'stock_id'] = str(n)+'c1'\n            mat.append ( newDf )\n            newDf = test.loc[test['stock_id'].isin(ind) ]    \n            newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n            newDf.loc[:,'stock_id'] = str(n)+'c1'\n            matTest.append ( newDf )\n            n+=1\n\n        mat1 = pd.concat(mat).reset_index()\n        mat1.drop(columns=['target'],inplace=True)\n        mat2 = pd.concat(matTest).reset_index()\n\n        mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n\n        mat1 = mat1.pivot(index='time_id', columns='stock_id')\n        mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n        mat1.reset_index(inplace=True)\n\n        mat2 = mat2.pivot(index='time_id', columns='stock_id')\n        mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n        mat2.reset_index(inplace=True)\n\n        prefix = ['log_return1_realized_volatility', 'total_volume_mean', 'trade_size_mean', 'trade_order_count_mean','price_spread_mean','bid_spread_mean','ask_spread_mean',\n                  'volume_imbalance_mean', 'bid_ask_spread_mean','size_tau2']\n        selected_cols=mat1.filter(regex='|'.join(f'^{x}.(0|1|3|4|6)c1' for x in prefix)).columns.tolist()\n        selected_cols.append('time_id')\n\n        train_m = pd.merge(train,mat1[selected_cols],how='left',on='time_id')\n        test_m = pd.merge(test,mat2[selected_cols],how='left',on='time_id')\n\n        # filling missing values with train means\n\n        features = [col for col in train_m.columns.tolist() if col not in ['time_id','target','row_id']]\n        train_m[features] = train_m[features].fillna(train_m[features].mean())\n        test_m[features] = test_m[features].fillna(train_m[features].mean())\n\n        return train_m, test_m\n    \n    \n\n    \n    # Get unique stock ids \n    #train_stock_ids = train['stock_id'].unique()\n\n    # Preprocess them using Parallel and our single stock id functions\n    #train_ = preprocessor(train_stock_ids, is_train = True)\n    #train = train.merge(train_, on = ['row_id'], how = 'left')\n\n    # Get unique stock ids \n    test_stock_ids = test['stock_id'].unique()\n\n    # Preprocess them using Parallel and our single stock id functions\n    test_ = preprocessor(test_stock_ids, is_train = False)\n    test = test.merge(test_, on = ['row_id'], how = 'left')\n\n    # Get group stats of time_id and stock_id\n    #train = get_time_stock(train)\n    test = get_time_stock(test)\n\n    # Fill inf values\n    #train.replace([np.inf, -np.inf], np.nan,inplace=True)\n    test.replace([np.inf, -np.inf], np.nan,inplace=True)\n    \n    train = pd.read_csv(\"..\/input\/tabnet-data\/train_tabnetPreAGG.csv\")\n    train = train.drop(\"Unnamed: 0\", axis=1)\n    # Aggregating some features\n    train, test = create_agg_features(train,test)\n    \n    return train, test\n\ntrain, test = tabnet_data()","bac0e19a":"train","f9257795":"X = train.drop(['row_id', 'target', 'time_id'], axis = 1)\ny = train['target']\nX_test=test.copy()\nX_test.drop(['time_id','row_id'], axis=1,inplace=True)\n\n","174704f7":"nunique = X.nunique()\ntypes = X.dtypes\n\ncategorical_columns = []\ncategorical_dims =  {}\n\nfor col in X.columns:\n    if  col == 'stock_id':\n        l_enc = LabelEncoder()\n        X[col] = l_enc.fit_transform(X[col].values)\n        X_test[col] = l_enc.transform(X_test[col].values)\n        categorical_columns.append(col)\n        categorical_dims[col] = len(l_enc.classes_)\n    else:\n        scaler = StandardScaler()\n        X[col] = scaler.fit_transform(X[col].values.reshape(-1, 1))\n        X_test[col] = scaler.transform(X_test[col].values.reshape(-1, 1))\n        \n\n\ncat_idxs = [ i for i, f in enumerate(X.columns.tolist()) if f in categorical_columns]\n\ncat_dims = [ categorical_dims[f] for i, f in enumerate(X.columns.tolist()) if f in categorical_columns]","e2866123":"tabnet_params = dict(\n    cat_idxs=cat_idxs,\n    cat_dims=cat_dims,\n    cat_emb_dim=1,\n    n_d = 16,\n    n_a = 16,\n    n_steps = 2,\n    gamma = 2,\n    n_independent = 2,\n    n_shared = 2,\n    lambda_sparse = 0,\n    optimizer_fn = Adam,\n    optimizer_params = dict(lr = (2e-2)),\n    mask_type = \"entmax\",\n    scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False),\n    scheduler_fn = CosineAnnealingWarmRestarts,\n    seed = 42,\n    verbose = 10\n    \n)","cc7fa573":"tabnet_oof_predictions = np.zeros((X.shape[0], 1))\ntabnet_test_predictions = np.zeros((X_test.shape[0], 1))\nfor fold in range(5):\n    print(\"fold:\", fold)\n    clf = TabNetRegressor(**tabnet_params)\n    clf.load_model(\"..\/input\/optiver-volatility-predictions-using-tabnet\/fold\"+str(fold)+\".zip\")\n    tabnet_oof_predictions += clf.predict(X.values) \/ 5\n    tabnet_test_predictions += clf.predict(X_test.values) \/ 5","f4d7a619":"tabnet_test_predictions","89cbbcf4":"oof_ffnn_tabnet_predictions = (ffnn_oof_predictions.reshape(-1)+tabnet_oof_predictions.reshape(-1))\/2 #lgb_oof_predictions.reshape(-1)+","41f187b0":"oof_lgb_ffnn_predictions = (ffnn_oof_predictions.reshape(-1)+lgb_oof_predictions.reshape(-1))\/2","0b3dd383":"oof_total_predictions = tabnet_oof_predictions.reshape(-1)*0.6+ffnn_oof_predictions.reshape(-1)*0.2+lgb_oof_predictions.reshape(-1)*0.2","5ee3f468":"print(\"oof_lgb_ffnn_predictions: \", rmspe(train[\"target\"].values.reshape(-1), oof_lgb_ffnn_predictions.reshape(-1)))","5c10b9bc":"print(\"oof_ffnn_tabnet_predictions: \", rmspe(train[\"target\"].values.reshape(-1), oof_ffnn_tabnet_predictions.reshape(-1)))","e0cad9f5":"print(\"oof_total_predictions: \", rmspe(train[\"target\"].values.reshape(-1), oof_total_predictions.reshape(-1)))","43f88ba1":"sub_csv = pd.read_csv(\"..\/input\/optiver-realized-volatility-prediction\/sample_submission.csv\", index_col = \"row_id\")","70f7d97c":"pred_ffnn_tabnet_predictions = (ffnn_test_predictions.reshape(-1)+tabnet_test_predictions.reshape(-1))\/2 #lgb_pred_predictions+","34987844":"pred_ffnn_lgb_predictions = (ffnn_test_predictions.reshape(-1)+lgb_pred_predictions.reshape(-1))\/2 #tabnet_test_predictions+","874fddff":"pred_total_predictions = tabnet_test_predictions.reshape(-1)*0.6+ffnn_test_predictions.reshape(-1)*0.2+lgb_pred_predictions.reshape(-1)*0.2 #","810c7fac":"lgb_pred_predictions, ffnn_test_predictions, tabnet_test_predictions","a31bd2ac":"pred_ffnn_lgb_predictions","a0fb9b4a":"sub_csv[\"target\"] = pred_total_predictions","c8b452c4":"sub_csv.to_csv(\"submission.csv\")","bdab4feb":"pd.read_csv(\".\/submission.csv\", index_col = \"row_id\")","545d8947":"# FFNN","527637ba":"# tabnet","6369af30":"# lgbm"}}