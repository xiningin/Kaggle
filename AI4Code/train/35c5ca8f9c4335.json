{"cell_type":{"13058f52":"code","19103283":"code","4a22cc50":"code","1bd95f10":"code","a25df492":"code","68e1811f":"code","8a595e2e":"code","9d30e521":"code","d40360fc":"code","6934e43c":"code","a395162f":"code","ee2d92bb":"markdown","e04d7b2f":"markdown","c7dd044b":"markdown","2e0d697f":"markdown","a1f2096c":"markdown","842a2a5e":"markdown","361d7b2e":"markdown","baca0a30":"markdown"},"source":{"13058f52":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport optuna\n\nimport lightgbm as lgbm\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n#plt.style.use('fivethirtyeight')\nimport xgboost as xgb\nimport sklearn\nimport tqdm\nimport random\nimport janestreet\nimport tensorflow as tf\n\nSEED=1111\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n\ntrain = pd.read_csv(\"\/kaggle\/input\/jane-street-market-prediction\/train.csv\")\n\nfrom tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom random import choices","19103283":"train = train[train['weight'] != 0]\n\ntrain.fillna(train.mean(),inplace=True)\n\ntrain['action'] = ((train['resp'].values) > 0).astype(int)\n\n\nfeatures = [c for c in train.columns if \"feature\" in c]\n\nfeatures.remove('feature_0')\n\ntrain['resp'] = (((train['resp'].values)*train['weight']) > 0).astype(int)\ntrain['resp_1'] = (((train['resp_1'].values)*train['weight']) > 0).astype(int)\ntrain['resp_2'] = (((train['resp_2'].values)*train['weight']) > 0).astype(int)\ntrain['resp_3'] = (((train['resp_3'].values)*train['weight']) > 0).astype(int)\ntrain['resp_4'] = (((train['resp_4'].values)*train['weight']) > 0).astype(int)\n\nf_mean = np.mean(train[features[1:]].values,axis=0)\n\nresp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n\na = np.where(train.date==410)[0][0]\n\nX_train=train.loc[:,features].values\n#y_train = (train.loc[:, 'action'])\ny_train = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T\n\nX_test = X_train[a:,:]\nX_train = X_train[:a,:]\n\ny_test = y_train[a:,:]\ny_train = y_train[:a,:]","4a22cc50":"params={\"num_leaves\":450,\n       \"max_bin\":450,    #### 450\n       \"feature_fraction\":0.52,\n       \"bagging_fraction\":0.52,\n       \"objective\":\"binary\",\n       \"learning_rate\":0.05,\n       \"boosting_type\":\"gbdt\",\n       \"metric\":\"auc\"\n       }\nmodels = [] # list of model , we will train \nfor i in range(y_train.shape[1]):\n   \n    d_train = lgbm.Dataset(X_train,label=y_train[:,i])\n    clf = lgbm.train(params,d_train,num_boost_round=1000)\n                     \n    models.append(clf)","1bd95f10":"Preds1=np.mean([model.predict(X_test) for model in models],axis=0)\npredictions1 = np.zeros(len(Preds1))\npredictions1[Preds1>=0.5] = 1\nsum(predictions1==y_test[:,3])\/len(Preds1)","a25df492":"pd.crosstab(y_test[:,3],predictions1 )","68e1811f":"def create_mlp(\n    num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate\n):\n\n    inp = tf.keras.layers.Input(shape=(num_columns,))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n    for i in range(len(hidden_units)):\n        x = tf.keras.layers.Dense(hidden_units[i])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Dropout(dropout_rates[i + 1])(x)\n    \n    x = tf.keras.layers.Dense(num_labels)(x)\n    out = tf.keras.layers.Activation(\"sigmoid\")(x)\n\n    model = tf.keras.models.Model(inputs=inp, outputs=out)\n    model.compile(\n        optimizer=tfa.optimizers.RectifiedAdam(learning_rate=learning_rate),\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing),\n        metrics=tf.keras.metrics.AUC(name=\"AUC\"),\n    )\n\n    return model\n\nepochs = 44   ### 45\nbatch_size = 4096\nhidden_units = [160, 160, 160]\ndropout_rates = [0.2, 0.2, 0.2, 0.2]\nlabel_smoothing = 1e-2\nlearning_rate = 1e-3\n\ntf.keras.backend.clear_session()\ntf.random.set_seed(SEED)\nclf = create_mlp(\n    len(features), 5, hidden_units, dropout_rates, label_smoothing, learning_rate\n    )\n\nclf.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=2)","8a595e2e":"Preds2 =clf.predict(X_test)\nPreds2=np.mean(Preds2,axis=1)\npredictions2 = np.zeros(len(Preds2))\npredictions2[Preds2>=0.5] = 1\n\nsum(predictions2==y_test[:,3])\/len(Preds2)","9d30e521":"pd.crosstab(y_test[:,3],predictions2 )","d40360fc":"predictions3= predictions1\npredictions3[train.weight[a:]>15] = predictions1[train.weight[a:]>15]*predictions2[train.weight[a:]>15]\n\nsum(predictions3==y_test[:,3])\/len(predictions3)","6934e43c":"pd.crosstab(y_test[:,3],predictions3 )","a395162f":"from tqdm import tqdm\nimport janestreet\nthreshold = 0.5\nf= np.median\n\nenv = janestreet.make_env()\nfor (test_df, pred_df) in tqdm(env.iter_test()):\n    if test_df[\"weight\"].item() > 0:\n        x_test = test_df.loc[:, features].values\n        if np.isnan(x_test[:, 1:].sum()):\n            x_test[:, 1:] = np.nan_to_num(x_test[:, 1:]) + np.isnan(x_test[:, 1:]) * f_mean\n            \n        lgbm_prob = f(np.mean([model.predict(x_test) for model in models],axis=0)  )\n        \n\n#        lgbm_pred = np.where(lgbm_prob >= threshold, 1, 0).astype(\"int\")\n        if test_df[\"weight\"].item() > 10:\n            mlp_prob = f(np.mean(clf.predict(x_test),axis=1)  )\n            mlp_pred = np.where(mlp_prob >= threshold, 1, 0).astype(\"int\")\n            lgbm_pred= np.where(lgbm_prob >= threshold, 1, 0).astype(\"int\") \n            pred_df[\"action\"] =  mlp_pred*lgbm_pred\n        else:\n            pred_df[\"action\"] = np.where(lgbm_prob >= threshold, 1, 0).astype(\"int\") \n            \n#        pred = lgbm_prob*0.6+mlp_prob*0.4\n#        pred_df[\"action\"] = np.where(pred >= threshold, 1, 0).astype(\"int\")\n#        pred_df[\"action\"] =  mlp_pred*lgbm_pred\n    else:\n        pred_df[\"action\"] = 0\n    env.predict(pred_df)","ee2d92bb":"**We can see that the ensembe model is an improvement of the previous 2, but the change in the portfolio's value will be more evident (correct prediction of large trades means big improvements in revenue)** ","e04d7b2f":"**This is the Cross Validation Version. The final submission is the same code, without the train-test split so that the final model is trained in all data points**","c7dd044b":"### Import Libraries","2e0d697f":"**The ensemeble prediction occurs for weights above 15, for the rest trades only LightGBM is used for prediction**","a1f2096c":"**This is an MLP model which will be used for ensembling in high risk trades**","842a2a5e":"### Train - Test Split ","361d7b2e":"The first model is a LightGBM model with 450 leaves and 450 maximum bins","baca0a30":"**An important differences compared to other kernels: No days were excluded (e.g. first 85 days) except for the weight=0 days.**\n\n**Secondly, test split starts at the start of a day, not inside a day**"}}