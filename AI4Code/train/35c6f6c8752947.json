{"cell_type":{"0fc81371":"code","72bc6e15":"code","3549131d":"code","6c3d0e9b":"code","04ee397d":"code","396129db":"code","c1cf3d39":"code","406d7c2f":"code","cb2ee4e5":"code","b532e53c":"code","36617de2":"code","3447d452":"code","a0d1a969":"code","77c4ec52":"code","86699be1":"code","cc9791aa":"code","77c6e868":"code","38268fe5":"markdown","64e56a1d":"markdown","fc500f39":"markdown","7afd285b":"markdown","79c4b928":"markdown","1151a969":"markdown","3d3e479d":"markdown","4ede9b62":"markdown","c55a112b":"markdown","d13b9f37":"markdown","51904165":"markdown","d357a418":"markdown","c187fbd5":"markdown","83baf02f":"markdown","09bd6f47":"markdown","fd488c9a":"markdown","14c73ed0":"markdown","56b751fc":"markdown"},"source":{"0fc81371":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Visualization\n!pip install dataprep | grep -v 'already satisfied'\nfrom dataprep.eda import plot, plot_diff, create_report\n\n#Preprocessing and Modelling\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n#Fine-tuning\n!pip install -q -U keras-tuner\nimport keras_tuner as kt\n\n# Warning\nimport warnings\nwarnings.filterwarnings('ignore')","72bc6e15":"train_full = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_full = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n\nprint('Training Set Shape = {}'.format(train_full.shape))\nprint('Training Set Memory Usage = {:.2f}MB'.format(train_full.memory_usage().sum()\/2**20))\n\nprint('Test Set Shape = {}'.format(test_full.shape))\nprint('Test Set Memory Usage = {:.2f}MB'.format(test_full.memory_usage().sum()\/2**20))","3549131d":"plot(train_full)","6c3d0e9b":"create_report(train_full)","04ee397d":"df_train = pd.read_csv('\/kaggle\/input\/disastertweet-prepared2\/train_prepared.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/disastertweet-prepared2\/test_prepared.csv')","396129db":"max_len = 0\n# Find the longest sentence \nfor sentence in pd.concat([df_train.text, df_test.text]):\n    if len(sentence) > max_len: # number of word in a sentence tokenizer is greater max_len\n        max_len = len(sentence)\nmax_len","c1cf3d39":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\ntrain_x = tokenizer.batch_encode_plus(df_train.text.tolist(), max_length=max_len, padding='max_length', return_tensors='tf')\ntest_x = tokenizer.batch_encode_plus(df_test.text.tolist(), max_length=max_len, padding='max_length', return_tensors='tf')\ntrain_y = df_train.target","406d7c2f":"from transformers import TFAutoModel\nmodel = TFAutoModel.from_pretrained('distilbert-base-uncased')","cb2ee4e5":"import gc\ngc.collect()","b532e53c":"train_x_output = model(train_x['input_ids'][:1500])","36617de2":"features = train_x_output.last_hidden_state[:,0,:].numpy()","3447d452":"labels = train_y[:1500]","a0d1a969":"from sklearn.model_selection import train_test_split\ntrain_features, val_features, train_labels, val_labels = train_test_split(features, labels, test_size=0.2, random_state=42)","77c4ec52":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nparameters = {'C': np.linspace(0.0001, 100, 20)}\ngrid_search = GridSearchCV(LogisticRegression(), parameters)\ngrid_search.fit(train_features, train_labels)\n\nprint('best parameters: ', grid_search.best_params_)\nprint('best scrores: ', grid_search.best_score_)","86699be1":"lr_clf = LogisticRegression(C= best_params['C'])\nlr_clf.fit(train_features, train_labels)","cc9791aa":"lr_clf.score(val_features, val_labels)","77c6e868":"def submission_transformer(model, test):\n    \"\"\"For Bert\"\"\"\n    sample_sub = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\n    predictions =  model.predict(test['input_ids'])\n    y_preds = [ np.argmax(x) for x in predictions[0]]\n    sub = pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_preds})\n    sub.to_csv('submission.csv', index=False)","38268fe5":"<a id='3'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">3. Data Pre-processing <\/p>\n\nNow we are going to engineering the data to make it easier for the model to clasiffy.\n\nThis section is very important to reduce the dimensions of the problem.\n\n\n[Content](#0)","64e56a1d":"<a id='1'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">0. What are updated in the last version?<\/p>\n\n## Introduction\nIn [Bert Model](https:\/\/arxiv.org\/abs\/1810.04805) , a new token is added is [CLS] with the purpose of classification. \n\nIn this kernel, I will a lighter Bert Model with uncased version [DistilBert Model](https:\/\/huggingface.co\/transformers\/model_doc\/distilbert.html) to tokenize text data. \n\nAfterward, extract data in CLS feature as input for logistic Regression Method. \n\n[Content](#0)","fc500f39":"<a id='2'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">2. EDA \ud83d\udcca<\/p>\n\n\n[Content](#0)","7afd285b":"<a id=4.1 ><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:left; border-radius: 20px 50px;\">4.1 Preprocessing Data<\/p>\n\n[Content](#0)","79c4b928":"BERT(*Bi-directional Encoder Representations from Transformers*)\n\n    - GLUE Score to 80.5%\n    - MultiNLI accuracy to 86.7%\n    - SQuAD v1.1 question answering Test F1 to 93.3\n    - SQuAD v2.0 Test F1 to 83.1","1151a969":"## Finding the hyperparameter for encoding.","3d3e479d":"<a id=4.2 ><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:left; border-radius: 20px 50px;\">4.2 DistilBERT model and extract [CLS] layer<\/p>\n\n[Content](#0)","4ede9b62":"# Please upvote if you make sense with my kernel.\n# I am so happy to hear your thoughts.","c55a112b":"<a id='1'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">1. Loading Data \ud83d\udc8e<\/p>\n\nJust load the dataset and global variables for colors and so on.\n\n[Content](#0)","d13b9f37":"Range from 120 to 140 characters is the most common in tweet.","51904165":"## The result is not good because the model only train with 1500 records per 6899 in total. \n## And 1500 is less than test size (3088) so submittion does not make sense in this situation.\n## To-Do : \n   ### Solve Memory Problem to enhance model accuary ","d357a418":"<a id=4><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">4. Modelling <\/p>\n[Content](#0)","c187fbd5":"<a id=4.3 ><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:left; border-radius: 20px 50px;\">4.3 Logistic Regression<\/p>\n\n[Content](#0)","83baf02f":"<img src=\"https:\/\/i.ibb.co\/4tzyG1P\/Bert-Classification.png\" alt=\"Bert-Classification\" border=\"0\">","09bd6f47":"Ideally, we need to use all data to train model, but we only use the first 1500 rows because the kernel memory is exhausted (OOW- Out of memory)","fd488c9a":"# Main technics I used in this data\n    * [3.1] Remove 92 duplicated rows\n    * [3.2] Cleaning text\n    * [3.3] Spelling Checker\n    * [3.4] Remove Stemming\n #### Step 3.3 spends a lot time (around 4000s in 4536s in total). \n #### So, I splits Data Preprocessing into [another kernel](http:\/\/https:\/\/www.kaggle.com\/phanttan\/disastertweet-prepareddata). \n #### And the prepared data to save in to [new dataset](http:\/\/https:\/\/www.kaggle.com\/phanttan\/disastertweet-prepared2)\n #### I am so appreciate to you for using\/upvoting it.\n","14c73ed0":"<a id=0><\/a>\n## <p style=\"background-color:lightblue; font-family:newtimeroman; font-size:120%; text-align:left; border-radius: 15px 50px;\">Table of Content<\/p>\n* [0. What are updated in the last version?](#0)\n* [1. Loading Data](#1)\n* [2. EDA ](#2)\n* [3. Data Preprocessing](#3)\n* [4. Modelling](#4)\n    * [4.1 Preprocessing Data](#4.1)\n    * [4.2 DistilBERT model and extract [CLS] layer](#4.2)\n    * [4.3 Logistic Regression](#)\n* [Make a Submission](#10)","56b751fc":"<a id=10 ><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:left; border-radius: 20px 50px;\">Make a Submission<\/p>\n\n[Content](#0)"}}