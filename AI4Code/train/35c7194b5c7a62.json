{"cell_type":{"bdbfeb99":"code","aba9f29a":"code","d068472c":"code","9182c6e2":"code","1aae14d2":"code","6261e59a":"code","36c57362":"code","3aae41d6":"code","2a98b2a9":"code","5738df39":"code","a1a2cf60":"code","c6e012f8":"code","bf1ece37":"code","c71e9bc6":"code","9648cbf7":"code","51da3821":"code","e7eb6456":"code","b828bdb6":"code","324053d3":"code","5dd1a0c2":"code","84365816":"markdown","239963c4":"markdown","3d59ad0e":"markdown","df28c27c":"markdown","98fdc5de":"markdown"},"source":{"bdbfeb99":"!wget \"https:\/\/uploads3.wikiart.org\/images\/pablo-picasso\/self-portrait-1907.jpg!Large.jpg\"\n!wget \"https:\/\/i.pinimg.com\/736x\/dc\/35\/cb\/dc35cb0860162d478b38a2eb159ccc2a.jpg\"\n!ls","aba9f29a":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport numpy as np\nimport time\nfrom PIL import Image\n%matplotlib inline","d068472c":"# A function to load the input images and set its dimensions to 1024 x 768\ndef load_image(image_path):\n    max_dim=512\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_image(img, channels=3)# decodes the image into a tensor\n    img = tf.image.convert_image_dtype(img, tf.float32)\n\n    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n    long_dim = max(shape)\n    scale = max_dim \/ long_dim\n    new_shape = tf.cast(shape * scale, tf.int32)\n\n    img = tf.image.resize(img, new_shape)\n    img = img[tf.newaxis, :]# broadcasting the image array so that it has a batch dimension\n\n    return img","9182c6e2":"def imshow(image, title=None):\n    if(len(image.shape) > 3):# suppose dim is like 1,2,4,2,2,1... it removes the ones so that only 3 values remain W,H,c\n        image=np.squeeze(image, axis=0)\n    plt.imshow(image)\n    if(title):# if there's a title mention it\n        plt.title(title)","1aae14d2":"# Let's see the images\ncontent_img=load_image('dc35cb0860162d478b38a2eb159ccc2a.jpg')\nstyle_img=load_image('self-portrait-1907.jpg!Large.jpg')\n\nplt.figure(figsize=(12,12))\nplt.subplot(1, 2, 1)\nimshow(content_img, 'Target image')\nplt.subplot(1, 2, 2)\nimshow(style_img, 'Style image')","6261e59a":"print(content_img.shape)\nprint(style_img.shape)","36c57362":"# Define the content image representation and load the model\nx=tf.keras.applications.vgg19.preprocess_input(content_img*255)# needs preprocessing for the model to be initialized\nx=tf.image.resize(x, (256,256))# the vgg19 model takes images in 256\nvgg_model=tf.keras.applications.VGG19(include_top=False, weights='imagenet')\nvgg_model.trainable=False\nvgg_model.summary()","3aae41d6":"# Chooose the content and style layers\ncontent_layers=['block4_conv2']\nstyle_layers = ['block1_conv1',\n                'block2_conv1',\n                'block3_conv1', \n                'block4_conv1', \n                'block5_conv1']","2a98b2a9":"# Build the model\ndef my_model(layer_names):\n    # Retrieve the output layers corresponding to the content and style layers\n    vgg_model = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n    vgg_model.trainable = False\n    outputs = [vgg_model.get_layer(name).output for name in layer_names]\n    model=tf.keras.Model([vgg_model.input], outputs)\n    return model","5738df39":"style_extractor = my_model(style_layers)\nstyle_outputs = style_extractor(style_img*255)","a1a2cf60":"# Compute the gram matrix\n# Einsum allows defining Tensors by defining their element-wise computation.\n# This computation is defined by equation, a shorthand form based on Einstein summation.\ndef gram_matrix(input_tensor): # input_tensor is of shape ch, n_H, n_W\n    result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n    input_shape = tf.shape(input_tensor)\n    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32) # Unrolls n_H and n_W\n    return result\/(num_locations)","c6e012f8":"class entire_model(tf.keras.models.Model):\n    def __init__(self, style_layers, content_layers):\n        super(entire_model, self).__init__()\n        self.vgg=my_model(style_layers + content_layers)\n        self.style_layers=style_layers\n        self.content_layers=content_layers\n        self.num_style_layers=len(style_layers)\n        self.vgg.trainable=False\n\n    def call(self, inputs):\n        inputs=inputs*255.0 # Scale back the pixel values\n        preprocessed_input=tf.keras.applications.vgg19.preprocess_input(inputs)\n        outputs=self.vgg(preprocessed_input)# Pass the preprocessed input to my_model\n\n        # Separate the representations of style and content\n        style_outputs, content_outputs=(outputs[:self.num_style_layers], outputs[self.num_style_layers:])\n        # Calculate the gram matrix for each layer in the style output. This will be the final style representation\n        style_outputs=[gram_matrix(layer) for layer in style_outputs]\n\n        # Store the content and style representation in dictionaries in a layer by layer manner\n        content_dict = {content_name:value\n                    for content_name, value\n                    in zip(self.content_layers, content_outputs)}\n\n        style_dict = {style_name:value\n                  for style_name, value\n                  in zip(self.style_layers, style_outputs)}\n\n        return {'content': content_dict, 'style': style_dict}\n        # Returns a dict of dicts with content and style representations, i.e., gram matrix of the style_layers and\n        # the content of the content_layers","bf1ece37":"# Now we extract the style and content features by calling the above class\nextractor=entire_model(style_layers, content_layers)\nstyle_targets = extractor(style_img)['style']\ncontent_targets = extractor(content_img)['content']\n\nresults = extractor(tf.constant(content_img))","c71e9bc6":"style_weight=40\ncontent_weight=10\n\n# Custom weights for different style layers\nstyle_weights = {'block1_conv1': 0.2,\n                 'block2_conv1': 0.19,\n                 'block3_conv1': 0.24,\n                 'block4_conv1': 0.11,\n                 'block5_conv1': 0.26}\n# style_weights = {'block1_conv1': 0.3,\n#                  'block2_conv1': 0.45,\n#                  'block3_conv1': 0.15,\n#                  'block4_conv1': 0.05,\n#                  'block5_conv1': 0.05}","9648cbf7":"def total_cost(outputs):\n    style_outputs=outputs['style']\n    content_outputs=outputs['content']\n    style_loss=tf.add_n([style_weights[name]*tf.reduce_mean((style_outputs[name]-style_targets[name])**2)\n                        for name in style_outputs.keys()])\n    style_loss*=style_weight\/len(style_layers)# Normalize\n\n    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2)\n                             for name in content_outputs.keys()])\n    content_loss*=content_weight\/len(content_layers)\n    loss=style_loss+content_loss\n    return loss","51da3821":"# Define a tf.Variable to contain the image to optimize\ngenerate_image = tf.Variable(content_img)\n# Since this is a float image, define a function to keep the pixel values between 0 and 1\ndef clip_0_1(image):\n    return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)","e7eb6456":"opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)","b828bdb6":"@tf.function()\ndef train_step(image):\n    with tf.GradientTape() as tape:\n        outputs = extractor(image)\n        loss = total_cost(outputs)\n\n    grad = tape.gradient(loss, image)\n    opt.apply_gradients([(grad, image)])\n    image.assign(clip_0_1(image))","324053d3":"num_iterations=1500\nfor i in range(num_iterations):\n    if(i%300==0):\n        plt.figure(figsize=(12,12))\n        plt.subplot(1, 2, 1)\n        imshow(np.squeeze(generate_image.read_value(), 0), f'Step {i}')\n        plt.subplot(1, 2, 2)\n        imshow(style_img, 'Style Image')\n    train_step(generate_image)","5dd1a0c2":"train_step(generate_image)\nplt.imshow(np.squeeze(generate_image.read_value(), 0))\nplt.axis('off')\nfig1 = plt.gcf()\nfig1.savefig('new_image.png', bbox_inches='tight')","84365816":"## imports","239963c4":"## specify urls","3d59ad0e":"In the function below we design a function to load images. We convert the image to the required dimension so that no matter which dimension the photo we upload is in, it will be resized accordingly in order to work.","df28c27c":"# Upvote, Fork and Enjoy :)","98fdc5de":"Hello everyone!\n\nOne way to generate new paintings is to transfer the style of an artist to new existing images of real life. In this notebook, I randomly picked **a cool image and converted it to a painting of Picasso**.\n\n#### This is a work in progress. Please **upvote** to keep this going.\n\nThank you and stay safe!\n\n![](https:\/\/miro.medium.com\/max\/767\/1*B5zSHvNBUP6gaoOtaIy4wg.jpeg)"}}