{"cell_type":{"b54dec96":"code","156eb4f3":"code","fcd6e4a6":"code","f34000a3":"code","bd399874":"code","73f3f8da":"code","ed643913":"code","dd769c7e":"code","da024355":"code","b79537b4":"code","a4870ca7":"code","b7d4f72f":"code","bd7c5d5e":"markdown"},"source":{"b54dec96":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","156eb4f3":"# Importing the libraries\nimport pandas as pd \nimport numpy as np \nimport tensorflow as tf \nimport matplotlib.pyplot as plt \nfrom tensorflow.keras.layers import Input,Conv2D,Dense,Dropout,MaxPooling2D,BatchNormalization,Flatten,GlobalMaxPooling2D\nfrom tensorflow.keras.models import Model,Sequential \nfrom tensorflow.keras.datasets import cifar100","fcd6e4a6":"# Importing the dataset \n(x_train,y_train),(x_test,y_test)=cifar100.load_data()","f34000a3":"# Reading the dataset \nplt.imshow(x_train[23])\nplt.title(y_train.ravel()[23])","bd399874":"# Checking the Shape\nprint('x_train shape: {}'.format(x_train.shape))\nprint('x_test shape: {}'.format(x_test.shape))","73f3f8da":"# Checking the no of classes\nK=len(set(list(y_train.ravel())))\nprint('No of Classes: {}'.format(K))","ed643913":"# Build the model using the functional API\ni = Input(shape=x_train[0].shape)\n\n\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(i)\nx = BatchNormalization()(x)\nx = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D((2,2))(x)\nx = Dropout(0.2)(x)\nx = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D((2,2))(x)\nx = Dropout(0.2)(x)\nx = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\nx = BatchNormalization()(x)\nx = MaxPooling2D((2,2))(x)\nx = Flatten()(x)\nx = Dropout(0.2)(x)\nx = Dense(2048, activation='relu')(x)\nx = Dropout(0.2)(x)\nx = Dense(K, activation='softmax')(x)\n\nmodel = Model(i, x)\nmodel.summary()","dd769c7e":"batch_size=32\nimage_generator=tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0.1,height_shift_range=0.1,horizontal_flip=True,vertical_flip=True)\ndata_generator=image_generator.flow(x_train,y_train.ravel())\nsteps_per_epoch=x_train.shape[0]\/\/32\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nr=model.fit(data_generator,validation_data=(x_test,y_test.ravel()),steps_per_epoch=steps_per_epoch,batch_size=batch_size,epochs=50)","da024355":"plt.plot(r.history['accuracy'],label='accuracy')\nplt.plot(r.history['val_accuracy'],label='val_accuracy')\nplt.legend()\nplt.show()","b79537b4":"plt.plot(r.history['loss'],label='loss')\nplt.plot(r.history['val_loss'],label='val_loss')\nplt.legend()\nplt.show()","a4870ca7":"y_pred=model.predict(x_test)\ny_pred=np.argmax(y_pred,axis=1)\ny_pred","b7d4f72f":"from sklearn.metrics import accuracy_score\nprint(accuracy_score(y_test.ravel(),y_pred))","bd7c5d5e":"* We have 500000 images having 32 X 32 size and 3 Color channels in training set \n* And we have 100000 images having 32 X 32 size and 3 color channels in validation set"}}