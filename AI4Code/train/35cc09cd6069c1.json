{"cell_type":{"18eb68ab":"code","0945a5b0":"code","c5eb4990":"code","7489b296":"code","be3deaef":"code","aef3305f":"code","c91f8585":"code","bee10b0f":"code","1a6067dd":"code","92f5ef0c":"code","b2306b66":"code","d47cf26f":"code","9e5e2e32":"markdown","25f5e138":"markdown","5e415bea":"markdown","9f7ab6df":"markdown","8884c5b2":"markdown","e6fe284b":"markdown","0fe5f612":"markdown","1d826d66":"markdown","0f24ea3c":"markdown","8d90bd64":"markdown","f07ce158":"markdown","20721b00":"markdown","ffab7fc4":"markdown","6db4d1a7":"markdown","eb6996a7":"markdown","e12734cf":"markdown"},"source":{"18eb68ab":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0945a5b0":"from sklearn.datasets import load_iris\nimport numpy as np ","c5eb4990":"iris = load_iris()\nx = iris.data\ny = iris.target","7489b296":"# Normalization\nx = (x - np.min(x))\/(np.max(x) - np.min(x))","be3deaef":"from sklearn.model_selection import train_test_split\nx_train,x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3)","aef3305f":"# %% knn\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3) # With n_neighbors = 3, we show how many times we will divide the data.","c91f8585":"# K Fold CV\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = knn, X = x_train, y = y_train, cv = 10)\n# estimator = knn means use knn algorithm when doing cross validation.\n# We divided our train data with cv = 10 to 10. 9 will train and 1 validation.\n\nprint(\"average accurecy: \", np.mean(accuracies)) \nprint(\"average std: \", np.std(accuracies)) # we look at the data spread to see if it is consistent or not.","bee10b0f":"knn.fit(x_train, y_train)\nprint(\"test accuracy: \",knn.score(x_test, y_test))\n# we test after fit.","1a6067dd":"# Grid Search Cross Validation\n# Thanks to grid search, we train according to each knn value and we do cross validation.\nfrom sklearn.model_selection import GridSearchCV # import GridSearchCV with sklearn.\ngrid = {\"n_neighbors\":np.arange(1,50)} # We want all the neighbors in order from 1 to 50.\nknn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn, grid, cv = 10) # This is the GridSearchCV method.\nknn_cv.fit(x,y)","92f5ef0c":"# Print hyperparameter K value in KNN algorithm\nprint(\"tuned hyperparameter K: \", knn_cv.best_params_) # For the best neighbor value.\nprint(\"best accuracy according to the tuned parameter (best score): \",knn_cv.best_score_) # For the best score","b2306b66":"# Grid Search CV with Logistic Regression example\n\nx = x[:100,:]\ny = y[:100]\nfrom sklearn.linear_model import LogisticRegression\n\ngrid = {\"C\":np.logspace(-3,3,7),\"penalty\":[\"l1\",\"l2\"]}\n# C is referred to as logistic regression regularization. If C is large, it becomes overfit. If C is small, it becomes underfit. that is, we cannot model and learn data.\n# l1 = lasso and l2 = ridge parameters are loss functions. You can search the web for regularization.\n\nlogreg = LogisticRegression(solver=\"liblinear\")\nlogreg_cv = GridSearchCV(logreg, grid, cv = 10)\nlogreg_cv.fit(x_train,y_train)\n\nprint(\"tuned hyperparameter: (best parameters): \",logreg_cv.best_params_)\nprint(\"best accuracy according to the tuned parameter (best score): \",logreg_cv.best_score_)","d47cf26f":"logreg2 = LogisticRegression(C = 1000.0, penalty = \"l2\")\nlogreg2.fit(x_train, y_train)\nprint(\"score: \",logreg2.score(x_test, y_test))","9e5e2e32":"* We need to look at the data after import.","25f5e138":"* We can find  the best parameter with printing.","5e415bea":" # Let's code!\n * Firstly, we need to data. We can use sklearn library that has lots of data.\n * Let's import libraries which we need.\n * We use iris data for this project.","9f7ab6df":"# What is K Fold Cross Validation ?\n* In data mining studies, the data set is divided into training and test sets to test the success of the applied method.\n* The most preferred k value in the literature is 10.\n* We will do K Fold Cross Validation with KNN and Logistic Regression.","8884c5b2":"* Our score is 0.91.\n* We just learned K Fold CV and Grid Search CV.\n* Thank you for read this work page.\n* See you on other projects. Best regards.","e6fe284b":"* We can see easily  C is 1000.0 and penalty is \"l2\". according to the tune hyperparameter  we have the best score that is 0.96.\n* So, when we use these C and penalty values we can see the score.","0fe5f612":"* We can understand easily that test accuracy = 0.93.\n* But, we need to know n_neighbors = 3 is the best value? We need to learn it.So, we use Grid Search Cross Validation.","1d826d66":"* knn = 3 was a good value. We understood this with 0.99 accurecy.\n* We understand that our data is consistent and k is a good value. now we will test with the test values \u200b\u200bwe did not use at the beginning.\n* Now, we will test with x_test and y_test.","0f24ea3c":"# What is Grid Search Cross Validation?\n* Grid search is an approach to parameter tuning that will methodically build and evaluate a model for each combination of algorithm parameters specified in a grid. ","8d90bd64":"* Our data has data, target ,target_names, feature_names, DESCR and filename.\n* We need data and target.\n* Data and target types are numpy.ndarray\n* After that we need to normalize the data.\n","f07ce158":"# What is Normalization?\n* Normalization is a database design technique that reduces data redundancy and eliminates undesirable characteristics like Insertion, Update and Deletion Anomalies.\n* Let's normalize the data.","20721b00":"* We can see best neighbor value is 13\n* Also, we can see the best score is = 0.98 thanks to tuned parameter.","ffab7fc4":"* Now, we can do K Fold after import KNN.","6db4d1a7":"* After the normalization, we have to do model selection which is train_test_split.\n* We have to split data before doing K Fold Cross Validation.","eb6996a7":"# Logistic Regression with Grid Search CV\n* I hope we learn Grid Search CV with KNN. So, we can do with Logistic Regression.\n* We will proceed in the same logic.\n* Let's do it.","e12734cf":"* We splitted our model and we choose test_size 0.3. So, we have %70 train and %30 test.\n* First we will do K fold Cross Validation with KNN model."}}