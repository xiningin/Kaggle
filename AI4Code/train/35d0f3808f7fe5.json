{"cell_type":{"5056a40f":"code","9d529e60":"code","3397e2c3":"code","a71c79f2":"code","66b040ba":"code","e279f69a":"code","87ebed87":"code","d3d47a79":"code","62c6550c":"code","372005f7":"code","171ed441":"markdown","7bf7f94d":"markdown","d3840327":"markdown","a9b4d0ad":"markdown","50b08e9a":"markdown"},"source":{"5056a40f":"!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev","9d529e60":"!pip uninstall -q typing --yes\n!pip install -qU git+https:\/\/github.com\/lezwon\/pytorch-lightning.git@2016_test","3397e2c3":"import os\n\nimport torch\nfrom torch.nn import functional as F\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\nfrom torchvision import transforms\nfrom torch.utils.data import SubsetRandomSampler\nimport pytorch_lightning as pl\nimport numpy as np\nfrom sklearn.model_selection import KFold\nimport torch_xla.core.xla_model as xm\n\n\nimport torch_xla","a71c79f2":"# Set a seed for numpy for a consistent Kfold split\nnp.random.seed(123)","66b040ba":"# Download the dataset in advance\nMNIST(os.getcwd(), train=True, download=True)\nMNIST(os.getcwd(), train=True, download=True)","e279f69a":"class MNISTModel(pl.LightningModule):\n\n    def __init__(self, hparams):\n        super(MNISTModel, self).__init__()\n        self.hparams = hparams\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout2d(0.25)\n        self.dropout2 = nn.Dropout2d(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=self.hparams['lr'])\n\n    def training_step(self, batch, batch_nb):\n        # REQUIRED\n        x, y = batch\n        y_hat = self.forward(x)\n        loss = F.nll_loss(y_hat, y)\n        tensorboard_logs = {'train_loss': loss}\n        return {'loss': loss, 'log': tensorboard_logs, 'progress_bar': {'tpu': torch_xla._XLAC._xla_get_default_device()}}\n\n    def validation_step(self, batch, batch_nb):\n        # OPTIONAL\n        x, y = batch\n        y_hat = self(x)\n        return {'val_loss': F.nll_loss(y_hat, y)}\n\n    def validation_epoch_end(self, outputs):\n        # OPTIONAL\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n        tensorboard_logs = {'val_loss': avg_loss}\n        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n\n    def test_step(self, batch, batch_nb):\n        # OPTIONAL\n        x, y = batch\n        y_hat = self(x)\n        return {'test_loss': F.cross_entropy(y_hat, y)}\n\n    def test_epoch_end(self, outputs):\n        # OPTIONAL\n        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n        logs = {'test_loss': avg_loss}\n        return {'avg_test_loss': avg_loss, 'log': logs, 'progress_bar': logs}\n\n    \n    def prepare_data(self):\n        dataset = MNIST(os.getcwd(), train=True, download=False, transform=transforms.ToTensor())\n        self.mnist_test = MNIST(os.getcwd(), train=False, download=False, transform=transforms.ToTensor())\n        \n        kf = KFold(n_splits=8)\n        splits = list(kf.split(dataset))\n        train_indices, val_indices = splits[self.hparams['fold']]\n        \n        self.mnist_train = torch.utils.data.Subset(dataset, train_indices)\n        self.mnist_val = torch.utils.data.Subset(dataset, val_indices)\n                \n\n    def train_dataloader(self):\n        loader = DataLoader(self.mnist_train, batch_size=64, num_workers=4)\n        return loader\n\n    def val_dataloader(self):\n        loader = DataLoader(self.mnist_val, batch_size=32, num_workers=4)\n        return loader\n\n    def test_dataloader(self):\n        loader = DataLoader(self.mnist_test, batch_size=32, num_workers=4)\n        return loader","87ebed87":"hparams = {'lr': 6.918309709189366e-07, 'fold': 1}","d3d47a79":"# Define a function to initialize and train a model\ndef train(tpu_id):\n    model = MNISTModel(hparams)\n    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n        filepath='checkpoints\/tpu' + str(tpu_id) + '-{epoch}-{val_loss:.2f}',\n        monitor='avg_val_loss',\n        mode='min'\n    )\n    \n    trainer = pl.Trainer(tpu_cores=[tpu_id], precision=16, max_epochs=5, checkpoint_callback=checkpoint_callback)    \n    trainer.use_native_amp = False\n    trainer.fit(model)\n    trainer.test()","62c6550c":"#use joblib to run the train function in parallel on different folds\nimport joblib as jl\nparallel = jl.Parallel(n_jobs=8, backend=\"threading\", batch_size=1)\nparallel(jl.delayed(train)(i+1) for i in range(8))\n","372005f7":"# weights are saved to checkpoints\n!ls -lh checkpoints\/ ","171ed441":"# **Define a Lightning Module**\nDefine a lightning module that takes in fold number in hparams.","7bf7f94d":"## Install PyTorch Lightning","d3840327":"## Train\nUse the `trainer` to train an instance of a model. It takes care of all the TPU setup given a `tpu_id` in `tpu_cores`.","a9b4d0ad":"# Parallel KFold training on TPU using Pytorch Lightning\nThis kernel demonstrates training K instances of a model parallely on each TPU core.","50b08e9a":"### Install XLA\nXLA powers the TPU support for PyTorch"}}