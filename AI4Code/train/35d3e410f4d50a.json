{"cell_type":{"2f18642e":"code","49ec4872":"code","578debeb":"code","81895011":"code","6c2ff2cc":"code","5ade4c9a":"code","fd45bbba":"code","a0974e13":"code","f8021cb8":"code","4e65bfcc":"code","04307469":"code","48387aed":"code","cccba804":"code","c7674ee0":"code","49d0504c":"code","3fb396e1":"code","0ed72f35":"code","6be4691d":"code","c45c3bea":"code","4974fe5b":"code","2b959d6f":"code","73a3d7d3":"code","caf23096":"code","e106a5fd":"code","378a29f9":"code","b12f38d0":"code","961b6dd4":"code","e65939c3":"code","ac144b3e":"code","4081eb56":"code","d1324378":"code","3f757a63":"code","a5a693f6":"code","9ee4da85":"code","390fb658":"code","e5c1a845":"code","7b280a12":"code","ae05d012":"code","9ce34812":"code","5a4e8d93":"code","dcac6897":"code","d48bc737":"code","5006ea04":"code","f6bf9d3d":"code","753bbada":"code","4214a7ad":"code","2775472e":"code","f07d5834":"code","c421d17e":"code","1ff30a79":"code","d4c60742":"code","e1a98304":"code","f74063d4":"code","f95a81ea":"code","7989fd0e":"code","7718a0ab":"code","d8223036":"code","d09bf4e9":"code","0090a5ba":"code","009c341d":"markdown","9f29c5ac":"markdown","4aa026dc":"markdown","16dc5c64":"markdown","839aee74":"markdown","02acca9b":"markdown","97b1885e":"markdown","b96b3192":"markdown","ad20dd36":"markdown","866812e4":"markdown","0e8c4265":"markdown","cb463dd7":"markdown","6c672cbb":"markdown","b9d1815a":"markdown","144a4b7c":"markdown","301c0b1d":"markdown","e7cd432f":"markdown","f4e5b443":"markdown","c7bb151d":"markdown","ef53f82d":"markdown","fcc8a664":"markdown","24f8b242":"markdown","cf1b0c08":"markdown","c58fa483":"markdown","89d61392":"markdown","3d95a5f6":"markdown","93824a52":"markdown","7179278e":"markdown","ab370e26":"markdown","f02aa675":"markdown","a2adc02c":"markdown","9eb9cc9f":"markdown","cf378342":"markdown","d1058f1b":"markdown","977056dd":"markdown","8f228559":"markdown","068a47e1":"markdown","a7e71bd0":"markdown","4d16b951":"markdown","4951531d":"markdown","13094eb2":"markdown","7ebd18a0":"markdown","6220d432":"markdown","e062e5b3":"markdown","d585d175":"markdown","9a8e3781":"markdown","58428f50":"markdown","f890e833":"markdown","2dac5e05":"markdown","ad10016a":"markdown","141c7772":"markdown","bfa2a8c1":"markdown","6f78b20b":"markdown","9e44b863":"markdown","3b1fb078":"markdown","7b0bf91e":"markdown","2474331f":"markdown","a0726ed0":"markdown","21141e04":"markdown","9ee6802a":"markdown","c72379af":"markdown","d5043234":"markdown","f45606ac":"markdown"},"source":{"2f18642e":"import numpy as np\nimport pandas as pd\nimport scipy as sp\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import *\nfrom sklearn.metrics import roc_auc_score\nimport xgboost as xgb\nfrom sklearn.preprocessing import *\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import *\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport mlxtend\nfrom mlxtend.feature_selection import SequentialFeatureSelector as sfs\nfrom sklearn.neural_network import MLPClassifier\n\n%matplotlib inline\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\nSEED = 7\nNFOLDS = 5","49ec4872":"df = pd.read_csv('\/kaggle\/input\/flumadness2020\/flu_train.csv')","578debeb":"df_test = pd.read_csv('\/kaggle\/input\/flumadness2020\/flu_test.csv')","81895011":"test_ID = df_test['ID']","6c2ff2cc":"label_dist = df.flu.value_counts()\/df.shape[0]; label_dist","5ade4c9a":"sns.barplot(y=label_dist.values,x=label_dist.index)\nplt.title(\"Target class distribution\",fontsize=15)\nplt.xlabel(\"Classes\")\nplt.ylabel(\"Distribution in %\")\nplt.show()","fd45bbba":"cat_cols = list(df.select_dtypes(include=['object']).columns)\ncont_cols = list(df.select_dtypes(exclude=['object']).columns)\ncont_cols.remove('flu')","a0974e13":"df.columns","f8021cb8":"df.describe()","4e65bfcc":"f, ax = plt.subplots(figsize= [15,12])\nsns.heatmap(df[cont_cols+['flu']].corr(), annot=True, fmt=\".2f\", ax=ax, \n            cbar_kws={'label': 'Correlation Coefficient'}, cmap='viridis')\nax.set_title(\"Correlation Matrix\", fontsize=15)\nplt.show()","04307469":"bp_cols = [col for col in df.columns if 'BP' in col and 'Ave' not in col]; bp_cols","48387aed":"cor_cols = ['Weight','Height','HHIncome']+bp_cols; cor_cols","cccba804":"missing_perc = df.isna().sum()\/df.shape[0]\nmissing_perc.sort_values(ascending=False)","c7674ee0":"plt.xlabel('Numbers of missing data')\nplt.ylabel('Numbers of columns')\nplt.title('Distribution of missing data')\nprint(\"Total missing values: \", (df.isnull().sum().sum()))\nprint(\"Mean missing values per column : \" , df.isnull().sum().sum()\/len(df.columns))\ndf.isnull().sum().hist(bins=20);","49d0504c":"X = ['Missing Values','Poor','Fair','Good', 'Very Good', 'Excellent']\nY = [df[\"HealthGen\"].isnull().sum(),(df[\"HealthGen\"] == \"Poor\").sum(),(df[\"HealthGen\"] == \"Fair\").sum(),(df[\"HealthGen\"] == \"Good\").sum(),\n    (df[\"HealthGen\"] == \"Vgood\").sum(), (df[\"HealthGen\"] == \"Excellent\").sum()]\nplt.bar(X, Y)\nplt.ylabel('Numbers of people')\nplt.title('Health Ditribution of the People')\nplt.show()","3fb396e1":"# Check the Percentages of people that have the flu and are in poor health:\n\nperc_a = ((df.HealthGen == \"Poor\") & (df.flu == 1)).sum()\/df[\"HealthGen\"].notnull().sum()*100\nperc_b = ((df.HealthGen == \"Fair\") & (df.flu == 1)).sum()\/df[\"HealthGen\"].notnull().sum()*100\nperc_c = ((df.HealthGen == \"Good\") & (df.flu == 1)).sum()\/df[\"HealthGen\"].notnull().sum()*100\nperc_d = ((df.HealthGen == \"Vgood\") & (df.flu == 1)).sum()\/df[\"HealthGen\"].notnull().sum()*100\nperc_e = ((df.HealthGen == \"Excellent\") & (df.flu == 1)).sum()\/df[\"HealthGen\"].notnull().sum()*100\n\n\nX = ['Poor','Fair','Good', 'Very Good', 'Excellent']\nY = [perc_a, perc_b, perc_c, perc_d,perc_e]\nplt.bar(X, Y)\nplt.ylabel('Percentage of people having the flu')\nplt.title('Health Influence of the people on the flu')\nplt.show()","0ed72f35":"pd.read_csv(\"..\/input\/flu-results\/2results_nans_regression.csv\",sep=\";\",index_col=0,keep_default_na=False)","6be4691d":"pd.read_csv(\"..\/input\/flu-results\/3results_nans_mean.csv\",sep=\";\",index_col=0,keep_default_na=False)","c45c3bea":"# continuous variables - impute mean\ndf[cont_cols] = df[cont_cols].fillna(df[cont_cols].mean())\ndf_test[cont_cols] = df_test[cont_cols].fillna(df_test[cont_cols].mean())\n\n# categorical variables - make a separate class\ndf[cat_cols] = df[cat_cols].fillna(\"Missing\")\ndf_test[cat_cols] = df_test[cat_cols].fillna(\"Missing\")","4974fe5b":"df.isna().any().any() or df_test.isna().any().any()","2b959d6f":"for col in cat_cols:\n    print(\"Column {} unique_values {}\".format(col, list(set(df[col].unique()) | set(df_test[col].unique()))))","73a3d7d3":"ordered_dict = {\n    'Education':['Missing','8th Grade','9 - 11th Grade','High School','Some College','College Grad'],\n    'HHIncome':['Missing',' 0-4999',' 5000-9999','10000-14999','15000-19999','20000-24999','25000-34999', \n                '35000-44999', '45000-54999', '55000-64999','65000-74999','75000-99999','more 99999'],\n    'BMICatUnder20yrs':['Missing','UnderWeight','NormWeight','OverWeight','Obese'],\n    'BMI_WHO':['Missing','12.0_18.5','18.5_to_24.9','25.0_to_29.9','30.0_plus'],\n    'HealthGen':['Missing','Poor','Fair','Good','Vgood','Excellent'],\n    'LittleInterest':['Missing','None','Several','Most'],\n    'Depressed':['Missing','None','Several','Most'],\n    'TVHrsDay':['Missing', '0_hrs','0_to_1_hr','1_hr','2_hr','3_hr', '4_hr','More_4_hr'],\n    'CompHrsDay':['Missing', '0_hrs','0_to_1_hr','1_hr','2_hr','3_hr', '4_hr','More_4_hr']\n}","caf23096":"ordered_cols = [key for key in ordered_dict]","e106a5fd":"mapping = [ordered_dict[col] for col in ordered_cols]","378a29f9":"oe = OrdinalEncoder(categories=mapping)\noe.fit(df[ordered_cols])\ndf[ordered_cols] = oe.transform(df[ordered_cols])\ndf_test[ordered_cols] = oe.transform(df_test[ordered_cols])","b12f38d0":"cat_cols = [col for col in cat_cols if col not in ordered_cols]\noe = OrdinalEncoder()\noe.fit(df[cat_cols])\ndf[cat_cols] = oe.transform(df[cat_cols])\ndf_test[cat_cols] = oe.transform(df_test[cat_cols])","961b6dd4":"pd.read_csv(\"..\/input\/flu-results\/8results_label_encoding.csv\",sep=\";\",index_col=0,keep_default_na=False)","e65939c3":"pd.read_csv(\"..\/input\/flu-results\/3results_nans_mean.csv\",sep=\";\",index_col=0,keep_default_na=False)","ac144b3e":"# get columns with missing values in more than 80% examples\ndef get_too_many_missing(data):\n    many_null_cols = [col for col in data.columns if data[col].isnull().sum() \/ data.shape[0] > 0.8]\n    return many_null_cols\n\n# get columns in which a single value is in more than 80% examples\ndef get_too_many_repeated(data):\n    big_top_value_cols = [col for col in data.columns if data[col].value_counts(dropna=False, normalize=True).values[0] > 0.8]\n    return big_top_value_cols\n\nmissing_cols = get_too_many_missing(df)\nrepeated_cols = get_too_many_repeated(df)","4081eb56":"dropped_cols = cor_cols+missing_cols+repeated_cols+['ID']; dropped_cols","d1324378":"pd.read_csv(\"..\/input\/flu-results\/4results_manual_selection.csv\",sep=\";\",index_col=0,keep_default_na=False)","3f757a63":"pd.read_csv(\"..\/input\/flu-results\/5results_all_features.csv\",sep=\";\",index_col=0,keep_default_na=False)","a5a693f6":"pd.read_csv(\"..\/input\/flu-results\/3results_nans_mean.csv\",sep=\";\",index_col=0,keep_default_na=False)","9ee4da85":"X = df[['Race1',\n  'Work',\n  'Diabetes',\n  'HealthGen',\n  'DaysMentHlthBad',\n  'LittleInterest',\n  'Depressed',\n  'SleepTrouble',\n  'PhysActive',\n  'CompHrsDayChild']]\ny = df['flu']\ndf_test = df_test[['Race1',\n  'Work',\n  'Diabetes',\n  'HealthGen',\n  'DaysMentHlthBad',\n  'LittleInterest',\n  'Depressed',\n  'SleepTrouble',\n  'PhysActive',\n  'CompHrsDayChild']]","390fb658":"pd.read_csv(\"..\/input\/flu-results\/xgb_results.csv\",index_col=0)","e5c1a845":"pd.read_csv(\"..\/input\/flu-results\/lgb_results.csv\",index_col=0)","7b280a12":"pd.read_csv(\"..\/input\/flu-results\/mlp_results.csv\",index_col=0)","ae05d012":"pd.read_csv(\"..\/input\/flu-results\/logistic_results.csv\",index_col=0)","9ce34812":"pd.read_csv(\"..\/input\/flu-results\/lgb_results.csv\",index_col=0)","5a4e8d93":"pd.read_csv(\"..\/input\/flu-results\/lgb_results_only_scale_pos_weight.csv\",index_col=0)","dcac6897":"pd.read_csv(\"..\/input\/flu-results\/lgb_results_best.csv\",index_col=0)","d48bc737":"weight_ratio = df.flu.value_counts()[0]\/df.flu.value_counts()[1]","5006ea04":"def fold_metrics(model, x_train, y_train, x_val, y_val, metrics_dict,debug=False): \n    \n    fold_train_overall = model.score(x_train, y_train)\n    fold_train_class_0 = model.score(x_train[y_train==0], y_train[y_train==0])\n    fold_train_class_1 = model.score(x_train[y_train==1], y_train[y_train==1])\n    fold_train_auc = roc_auc_score(y_train,model.predict_proba(x_train)[:,1])\n    y_preds = model.predict(x_train)\n    fold_train_conf = confusion_matrix(y_train,y_preds)\n    fold_train_prec = precision_score(y_train,y_preds)\n    fold_train_recall = recall_score(y_train,y_preds)\n    \n\n    fold_overall = model.score(x_val, y_val)\n    fold_class_0 = model.score(x_val[y_val==0], y_val[y_val==0])\n    fold_class_1 = model.score(x_val[y_val==1], y_val[y_val==1])\n    fold_auc = roc_auc_score(y_val,model.predict_proba(x_val)[:,1])\n    y_preds = model.predict(x_val)\n    fold_conf = confusion_matrix(y_val,y_preds)\n    fold_prec = precision_score(y_val,y_preds)\n    fold_recall = recall_score(y_val,y_preds)\n\n    \n    metrics_dict['train']['overall'].append(fold_train_overall)\n    metrics_dict['train']['class_0'].append(fold_train_class_0)\n    metrics_dict['train']['class_1'].append(fold_train_class_1)\n    metrics_dict['train']['auc'].append(fold_train_auc)\n    metrics_dict['train']['conf'].append(fold_train_conf)\n    metrics_dict['train']['prec'].append(fold_train_prec)\n    metrics_dict['train']['rec'].append(fold_train_recall)\n    \n    \n    metrics_dict['test']['overall'].append(fold_overall)\n    metrics_dict['test']['class_0'].append(fold_class_0)\n    metrics_dict['test']['class_1'].append(fold_class_1)\n    metrics_dict['test']['auc'].append(fold_auc)\n    metrics_dict['test']['conf'].append(fold_conf)\n    metrics_dict['test']['prec'].append(fold_prec)\n    metrics_dict['test']['rec'].append(fold_recall)\n    if debug:\n        print(\"Fold metrics\")\n        print('train\\n',fold_train_conf)\n        print('test\\n',fold_conf)\n        metrics = pd.DataFrame.from_dict({'train':[fold_train_overall,fold_train_class_0, fold_train_class_1,fold_train_auc,fold_train_prec,fold_train_recall],\n                                         'test':[fold_overall,fold_class_0, fold_class_1,fold_auc,fold_prec,fold_recall]})\n        metrics.index = ['overall','class_0','class_1','auroc','precision','recall']\n        display(metrics)","f6bf9d3d":"def mean_fold_metrics(metrics_dict):\n    \n    train_overall = np.mean(metrics_dict['train']['overall'])\n    train_class_0 = np.mean(metrics_dict['train']['class_0'])\n    train_class_1 = np.mean(metrics_dict['train']['class_1'])\n    train_auc = np.mean(metrics_dict['train']['auc'])\n#   element-wise mean\n    train_conf = np.mean(metrics_dict['train']['conf'],axis=0)\n    train_prec = np.mean(metrics_dict['train']['prec'])\n    train_recall = np.mean(metrics_dict['train']['rec'])\n\n    overall = np.mean(metrics_dict['test']['overall'])\n    class_0 = np.mean(metrics_dict['test']['class_0'])\n    class_1 = np.mean(metrics_dict['test']['class_1'])\n    auc = np.mean(metrics_dict['test']['auc'])\n#   element-wise mean\n    conf = np.mean(metrics_dict['test']['conf'],axis=0)\n    prec = np.mean(metrics_dict['test']['prec'])\n    recall = np.mean(metrics_dict['test']['rec'])\n    metrics = pd.DataFrame.from_dict({'train':[train_overall,train_class_0, train_class_1,train_auc,train_prec,train_recall],\n                                     'test':[overall,class_0, class_1,auc,prec,recall]})\n    metrics.index = ['overall','class_0','class_1','auroc','precision','recall']\n    \n    print()\n    print(\"Mean fold metrics\")\n    print('train\\n',train_conf)\n    print('test\\n',conf)\n    display(metrics)\n    return metrics","753bbada":"def train_model(X, X_test,y, folds, params, model_type='lgb',n_jobs=-1, n_estimators=None, plot_feature_importance=True, verbose=500,early_stopping_rounds=None):\n#     add n_estimators and early_stopping as arguments in lgbm\n    print(\"Model type\",model_type)\n    metrics_dict = {'train':{'overall':[],'class_0':[],'class_1':[],'auc':[],'conf':[],'prec':[],'rec':[]},\n                   'test':{'overall':[],'class_0':[],'class_1':[],'auc':[],'conf':[],'prec':[],'rec':[]}}\n    result_dict = {}\n    n_splits = folds.n_splits\n    columns = X.columns\n    # out-of-fold predictions on train data\n    oof = np.zeros((len(X), 1))\n    # averaged predictions on train data(i think this should be \"test\" data)\n    prediction = np.zeros((len(X_test), 1))\n    feature_importance = pd.DataFrame()\n    \n    \n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X,y)):\n        print('Fold nr {}'.format(fold_n))\n        X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n        if model_type=='lgb':\n            model = lgb.LGBMClassifier(**params, importance_type='gain')\n            model.fit(X_train, y_train, \n                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='auc',\n                    verbose=verbose)\n            y_pred_valid = model.predict_proba(X_valid)[:,1]\n    #         we want 0s and 1s for submission\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n        elif model_type=='xgb':\n            model = xgb.XGBClassifier(random_state=SEED,scale_pos_weight=weight_ratio)\n            model.fit(X_train, y_train)\n            y_pred_valid = model.predict_proba(X_valid)[:,1]\n            #         we want 0s and 1s for submission\n\n            y_pred = model.predict(X_test)\n        elif model_type=='logistic':\n            model = LogisticRegression(C=0.000001)\n            model.fit(X_train,y_train)\n            y_pred_valid = model.predict_proba(X_valid)[:,1]\n            y_pred = model.predict(X_test)\n        elif model_type=='mlp':\n            mlp = MLPClassifier()\n            model.fit(X_train,y_train)\n            y_pred_valid = model.predict_proba(X_valid)[:,1]\n            y_pred = model.predict(X_test)\n        else:\n            raise Exception(\"Invalid model type\")\n                    \n        fold_metrics(model,X_train,y_train,X_valid,y_valid,metrics_dict)\n        oof[valid_index] = y_pred_valid.reshape(-1, 1)\n        prediction += y_pred.reshape(-1, 1)\n        if plot_feature_importance:\n            # feature importance\n            fold_importance = pd.DataFrame()\n            fold_importance[\"feature\"] = columns\n            fold_importance[\"importance\"] = model.feature_importances_\n            fold_importance[\"fold\"] = fold_n + 1\n            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n    \n    prediction \/= n_splits\n    \n    result_dict['oof'] = oof\n    \n    result_dict['metrics'] = mean_fold_metrics(metrics_dict)\n    \n    \n    \n    result_dict['prediction'] = prediction.flatten()\n        \n    \n    if plot_feature_importance:\n        feature_importance[\"importance\"] \/= n_splits\n        best_features = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n            by=\"importance\", ascending=False)[:50].reset_index(level=['feature'])\n\n\n        plt.figure(figsize=(16, 12));\n        sns.barplot(x=\"importance\", y=\"feature\", data=best_features);\n        plt.title('{} Features (avg over folds)'.format(model_type.upper()));\n\n        result_dict['feature_importance'] = feature_importance\n        result_dict['top_columns'] = best_features['feature'].unique()\n        \n    return result_dict","4214a7ad":"params = {\n        'max_depth':3,\n        'min_child_weight':1,\n        'scale_pos_weight':weight_ratio,\n        'objective':'binary'\n}","2775472e":"folds = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)","f07d5834":"results = train_model(X,df_test,y,folds,params,'lgb')","c421d17e":"submission = pd.DataFrame(columns=['ID','Prediction'])","1ff30a79":"submission['Prediction'] = results['prediction'].astype(int)","d4c60742":"submission['ID'] = test_ID","e1a98304":"submission[['ID','Prediction']].head()","f74063d4":"submission.Prediction.value_counts()","f95a81ea":"import datetime \n  \nnow = datetime.datetime.now() \nnow = now.strftime(\"%d_%m_%Y_%H_%M\")\nfilename = 'submission{}.csv'.format(now)\nsubmission[['ID','Prediction']].to_csv('..\/working\/{}'.format(filename),index=False)\nprint(filename)","7989fd0e":"pd.read_csv(\"..\/input\/flu-results\/lgb_results.csv\",index_col=0)","7718a0ab":"pd.read_csv(\"..\/input\/flu-results\/lgb_results_only_scale_pos_weight.csv\",index_col=0)","d8223036":"pd.read_csv(\"..\/input\/flu-results\/lgb_results_best.csv\",index_col=0)","d09bf4e9":"pd.read_csv(\"..\/input\/flu-results\/4results_manual_selection.csv\",sep=\";\",index_col=0,keep_default_na=False)","0090a5ba":"pd.read_csv(\"..\/input\/flu-results\/3results_nans_mean.csv\",sep=\";\",index_col=0,keep_default_na=False)","009c341d":"**Manual selection - dropping `dropped_cols`**","9f29c5ac":"**With adjusted scale_pos_weight**","4aa026dc":"This graph shows us the distribution of health of the people. The idea here, is to observe whether the data has a bias.\n* We observe that the health of the people is not evenly distributed. There is a majority of people in good and very good health.\n* There is very few people in poor health.\n* Regarding the distribution of the data, looking at the rate of people having the flu considering their current health could be interesting.\n\n\nNote: About 25% of the data has missing values in this feature.","16dc5c64":"## Feature selection\n\nHere we compared our manual feature selection to stepwise selection. Below we can see all the feauters that we decided to drop based on data exploration. ID is different for every value, therefore it also should be removed. ","839aee74":"**Default model**","02acca9b":"**Manual selection - dropping `dropped_cols`**","97b1885e":"Before encoding categorical variables, we examine unique values in each of them. We can observe that in some features there is a clear ordering of the classes(e.g. \"BMI_WHO\"), while in others the order does not matter (e.g. \"Race1\"). For the former, we should aim to preserve such ordering in the encoding.","b96b3192":"### Unnecessary columns","ad20dd36":"Let us divide the columns in continuous and categorical. This is necessary because different approaches must be used for filling missing data in those two groups. Moreover, categorical variables must be encoded to numerical before we feed them into any model.","866812e4":"Only ~6% of the examples have the flu.","0e8c4265":"## Data standardization","cb463dd7":"At first we used models with default parameters to pick those that are the most promising. ","6c672cbb":"At first, let us see the class distribution of the target variable.","b9d1815a":"# KEN3450, Data Analysis, Spring 2020\n\n### Kaggle competition - #FluMadness2020\n\n**B\u0142a\u017cej Dolicki, i6155906<br>\nPierre Bongrand, i6175237<br>\nTeam on Kaggle: Data Doctors<br>\nFinal score on Kaggle: 0.66148**\n\n\nRemarks:\n* Some parts of this notebook are inspired by: https:\/\/www.kaggle.com\/ysjf13\/cis-fraud-detection-visualize-feature-engineering\n* In tables displaying results, the column \"test\" refers to results from the validation set and the column \"kaggle\" refers to the test set for which labels are not provided\n* This notebook can be run from top to bottom and will output the predictions which brought our best score on the Kaggle leaderboard. ","144a4b7c":"There are multiple methods to achieve that:\n* One-Hot Encoding - for each categorical variable, for each class, a new binary variable is created that indicates if the class is present or not. Depending on the cardinality of categorical variables, this approach vastly increases the total number of features which negatively impacts performance of tree-based models. Reference: https:\/\/towardsdatascience.com\/one-hot-encoding-is-making-your-tree-based-ensembles-worse-heres-why-d64b282b5769 (we know a paper would be better than a blog post, but that's the best we have)\n* Label Encoding - for each categorical variable, we assign an integer to each class. This encoding maintains the initial number of features, however it assigns random order to the classes. To do that, we use OrdinalEncoder() which is included in scikit-learn.","301c0b1d":"**With adjusted scale_post_weight**","e7cd432f":"# Train models","f4e5b443":"Above we can observe the importance of features used in the model.","c7bb151d":"# Submission","ef53f82d":"Adding ordinal encoding for ordinal variables slightly increases the class_1 accuracy, auroc and recall in the validation set and auroc in test set.","fcc8a664":"## Data exploration","24f8b242":"**Best model (more hyperparameters adjusted)**","cf1b0c08":"In general, we can drop columns in which vast majority of columns is missing because they don't provide any information to the model. However, this step requires careful consideration, because in some cases missing values might be meaningful. Also, columns that have too many repeated values are useless.","c58fa483":"From the plot we can see the following correlations:\n* \"Weight\" and \"Height\" are strongly correlated to \"BMI\" (not surprising because BMI is a quotient of weight and squared height)\n* all variables related to blood pressrure are correlated, diastolic variables strongly correlated with each other, same applies to systolic\n* conversely, urine measurements 1 and 2 are barely correlated to each other, although urine flow and volume in a measurement with the same number are indeed correlated\n* no single variable is strongly correlated to the target variable, however we need to bear in mind that this is Pearson correlation which is applicable for two continuous variables while the target variable is binary","89d61392":"### 2. LightGBM is the best model we obtained.","3d95a5f6":"## Filling missing data","93824a52":"Is there still any missing data?","7179278e":"This graph represents the distribution of the missingness in our features.\n\n* We observe that the missingness is not evenly distributed. It is quite the opposite with about 2\/3 of our features to be extreme. Either not having any missigness, or having more than 80% of missing values.\n\n* However, we can see that there is about as much complete features, than features with a lot missing values. ","ab370e26":"**XGBoost - gradient boosting tree-based method**","f02aa675":"We can observe that imputing missing values with mean brings superior auroc, class_1 accuracy, precision and recall on the validation set and superior auroc on the test set from Kaggle.","a2adc02c":"### Results","9eb9cc9f":"### Results - default models","cf378342":"For some algorithms scaling of the variables is crucial, e.g. neural networks. However, we read that it is not that important for tree-based methods since choosing the split does not depend on the scale. The author of xgboost paper confirms this insight: https:\/\/github.com\/dmlc\/xgboost\/issues\/357 We checked it empirically and indeed neither using MinMaxScaler() nor z-score normalization did not bring any improvement in any metric. ","d1058f1b":"The set of selected features should ideally contain variables that are strongly correlated with the target and not correlated with each other.<br> \nThe features that should be dropped based on correlations are:\n* Weight\n* Height\n* All BP features apart from BPSysAve and BPDiaAve (blood pressure might be an important feature, let us keep the averages)\n* HHIncome - HHIncomeMid is the middle value of the ranges in HHIncome, however it has less missing variables","977056dd":"For continuous variables we tried two approaches: replacing missing data with mean and with regression (using https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.IterativeImputer.html).\n<br>**Results for regression:**","8f228559":"Manual selection is slightly better than using all the features, however forward stepwise selection yielded a significant jump from 0.61167 to 0.66133 on the Kaggle test set.","068a47e1":"Secondly, we applied forward and backward stepwise selection using the mlxtend package (http:\/\/rasbt.github.io\/mlxtend\/api_subpackages\/mlxtend.feature_selection\/#sequentialfeatureselector). For both methods, after executing the algorithm, we compared metrics of models for each k to metrics of the best model we had before using stepwise selection. We summed up the differences between every single metric and chose the k with the best performance. As a result, the best k in forward selection was 9 and for backward - 5. The former obtained slightly better results on the test set. ","a7e71bd0":"Firstly, we observe that the percentage of having the flu is really low between 0.25% and 2%, which approximately corresponds to the imbalance of the dataset.\n\nSecondly, when comparing the percentages between people in Poor, Fair and Good physical condition. We observe that having a poor health doesn't impact in such a bad manner the probability of having the flu. \n\nHowever, people in very good and excellent health have way less chances to be found having the flu.","4d16b951":"Below we can see the features found with forward stepwise selection","4951531d":"## Encode categorical variables","13094eb2":"**Default model**","7ebd18a0":"Initially, we focused on xgboost because it had the most promising auroc, however after adjusting a few hyperparameters, in the end LightGBM obtained slightly better results. The most important hyperparamter was `scale_pos_weight` which changes the weight of the postive (minority) class making it more important. In an early stage, after setting scale_pos_weight to sum(negative_examples)\/sum(positive_examples) as adviced by xgboost docs, we jumped from **0.53 to 0.62 AUROC on the Kaggle leaderboard**, even though on validation set auroc droped from 0.74 to 0.70. It is clear that the test set has higher percentage of positive examples than the training set.","6220d432":"**Forward stepwise selection**","e062e5b3":"### Encode remaining categorical variables","d585d175":"### 3. Stepwise feature selection brought much better results than manual feature selection.","9a8e3781":"**LightGBM - another gradient boosting tree-based method**","58428f50":"## Load data","f890e833":"### Results\n**Results for label encoding**","2dac5e05":"**Logistic Regression**","ad10016a":"**Forward stepwise selection**","141c7772":"## Conclusions","bfa2a8c1":"**Results for mean**","6f78b20b":"### Best model","9e44b863":"**All features**","3b1fb078":"## Import necessary libraries","7b0bf91e":"### Encode ordinal variables","2474331f":"For ordinal variables, to encode them in a specific order, we need to add a `mapping` argument to the OridnalEncoder(). We manually assign the order to each of these features based on common sense.","a0726ed0":"**Results for ordinal and label encoding**","21141e04":"### Forward and backward stepwise selection","9ee6802a":"From the column description we know that variables starting with \"BP\" are related to some medical measurements. We can make a safe bet that in variables such as 'BPSysAve','BPSys1','BPDia1' - \"BP\" stands for \"Blood Pressure\", \"Sys\" - \"Systolic\", \"Dia\"-\"Diastolic\" and \"Ave\" - \"Average\". Probably the numbers stand for 3 consecutive mesaurements.","c72379af":"**Neural network - scikit-learn implementation**","d5043234":"### 1. Adjusting `scale_pos_weight` led to the largest improvement (0.09 difference on Kaggle test set).\n(other hyperparameters also helped)","f45606ac":"Let's examine the percentage of missing values per column."}}