{"cell_type":{"fead7051":"code","cd9ca2a5":"code","6657051d":"code","ce4fbb8d":"code","b9b6c8be":"code","56dc145f":"code","761f5b27":"code","95055455":"code","ccff2068":"code","6a8dc9a2":"code","00159a9b":"code","4adae974":"code","ea52b2f5":"code","0ccbe3a0":"code","619fa97c":"code","a89ac786":"code","ab191c8d":"code","ebe0dceb":"code","422bcbb8":"code","05e19317":"code","b26c68b0":"code","f1d83bc9":"code","c3b5f5b9":"code","8fb95916":"code","7f4b3e9a":"code","4b64c199":"code","3197bb9c":"code","4618e268":"code","b59e5335":"code","dba7c4de":"code","eff59ad9":"code","a3aa820c":"markdown","1faa92a2":"markdown"},"source":{"fead7051":"!pip install prophet\n!pip install neuralprophet[live]","cd9ca2a5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport os, glob, math, cv2, gc, logging\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport warnings\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import plot_model\nfrom prophet import Prophet\nfrom neuralprophet import NeuralProphet\n\n\nwarnings.filterwarnings(\"ignore\")\ntf.autograph.set_verbosity(0)\nlogging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\ntf.random.set_seed(42)\nprint(tf.__version__)","6657051d":"train = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/train.csv').set_index(\"row_id\")\ntest = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/test.csv').set_index(\"row_id\")\n\nsample_submission = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/sample_submission.csv\")","ce4fbb8d":"train.head(10)","b9b6c8be":"test.head()","56dc145f":"sample_submission.head()","761f5b27":"print(\"train date range: \" + str(min(train['date'])) + \" to \" +  str(max(train['date'])))\nprint(\"train shape: \" + str(train.shape))\nprint(\"test date range: \" + str(min(test['date'])) + \" to \" +  str(max(test['date'])))\nprint(\"test shape: \" + str(test.shape))\n","95055455":"countries = ['Sweden', 'Finland', 'Norway']\nstores = ['KaggleMart', 'KaggleRama']\nproducts = ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']\n","ccff2068":"train['date'] = pd.to_datetime(train['date'])\ntrain['year'] = train['date'].dt.year\ntrain['month'] = train['date'].dt.month\ntrain['day'] = train['date'].dt.day\ntrain['dayofweek'] = train['date'].dt.dayofweek\n\ntest['date'] = pd.to_datetime(test['date'])\ntest['year'] = test['date'].dt.year\ntest['month'] = test['date'].dt.month\ntest['day'] = test['date'].dt.day\ntest['dayofweek'] = test['date'].dt.dayofweek","6a8dc9a2":"for country in countries:\n    print(f\"\\n--- {country} ---\\n\")\n    fig = plt.figure(figsize=(18, 8), dpi=100)\n    fig.subplots_adjust(hspace=0.25)\n    for i, store in enumerate(stores):\n        for j, product in enumerate(products):\n            ax = fig.add_subplot(2, 3, (i*3+j+1))\n            criteria_string = (train['country']==country)&(train['store']==store)&(train['product']==product)\n            selected_data = train[criteria_string]\n            selected_data.set_index('date').groupby(['year', 'month'])['num_sold'].mean().plot(ax=ax)\n            ax.set_title(f\"{country} - {store} - {product}\")\n            ax.set_xticks(range(0, 48, 12), [f\"Jan {y}\" for y in range(2015, 2019)])\n    plt.show()","00159a9b":"for country in countries:\n    fig = plt.figure(figsize=(20, 10), dpi=100)\n    fig.subplots_adjust(hspace=0.25)\n    for i, store in enumerate(stores):\n        for j, product in enumerate(products):\n            ax = fig.add_subplot(2, 3, (i*3+j+1))\n            criteria_string = (train['country']==country)&(train['store']==store)&(train['product']==product)\n            selected = train[criteria_string]\n            for year in [2015, 2016, 2017, 2018]:\n                selected[selected.year==year].set_index('date').groupby('month')['num_sold'].mean().plot(ax=ax, label=year)\n            ax.set_title(f\"{country} - {store} - {product}\")\n            ax.legend()\n    plt.show()","4adae974":"festivities = pd.read_csv(\"..\/input\/festivities-in-finland-norway-sweden-tsp-0122\/nordic_holidays.csv\",\n                          parse_dates=['date'],\n                          usecols=['date', 'country', 'holiday'])\n\ngdp = pd.read_csv(\"..\/input\/gdp-20152019-finland-norway-and-sweden\/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv\")\ngdp = np.concatenate([gdp[['year', 'GDP_Finland']].values, \n                      gdp[['year', 'GDP_Norway']].values, \n                      gdp[['year', 'GDP_Sweden']].values])\ngdp = pd.DataFrame(gdp, columns=['year', 'gdp'])\ngdp['country'] = ['Finland']*5 + ['Norway']*5 +['Sweden']*5","ea52b2f5":"def smape(y_true, y_pred):\n    denominator = (y_true + tf.abs(y_pred)) \/ 200.0\n    diff = tf.abs(y_true - y_pred) \/ denominator\n    diff = tf.where(denominator==0, 0.0, diff)\n    return tf.reduce_mean(diff)","0ccbe3a0":"scoring_record=pd.DataFrame(columns = ['Country', 'Store', 'Product', 'Train score', 'Val score'])\ntest_np = test.copy()\n\nfor country in countries:\n    for store in stores:\n        for product in products:\n            train_idx = (train['date'] >= '2015-01-01') & (train['date'] < '2018-01-01') &\\\n                        (train['country'] == country) & (train['store'] == store) & (train['product'] == product)\n            train_selected = train.loc[train_idx, ['date', 'num_sold']].reset_index(drop=True)\n            train_selected = train_selected.rename(columns={'date': 'ds', 'num_sold': 'y'})\n            val_idx = (train['date'] >= '2018-01-01') & (train['date'] < '2019-01-01') &\\\n                      (train['country'] == country) & (train['store'] == store) & (train['product'] == product)\n            val = train.loc[val_idx, ['date', 'num_sold']].reset_index(drop=True)\n            val = val.rename(columns={'date': 'ds', 'num_sold': 'y'})\n        \n            model = NeuralProphet(\n                growth='linear',\n                n_changepoints=10,\n                changepoints_range=0.4,\n                trend_reg=1,\n                trend_reg_threshold=False,\n                yearly_seasonality=True,\n                weekly_seasonality=True,\n                daily_seasonality=False,\n                seasonality_mode='additive',\n                seasonality_reg=1,\n                n_forecasts=365,\n                normalize='off'\n            )\n           \n            model.fit(train_selected, freq='D')\n\n            train_predictions = model.predict(train_selected)['yhat1']\n            val_predictions = model.predict(val)['yhat1']\n            \n            train_score = smape(train_selected['y'].values, train_predictions.values)\n            val_score = smape(val['y'].values, val_predictions.values)\n            \n            record_dict = {'Country': country, 'Store': store, 'Product': product, 'Train score': f'{train_score:4f}', 'Val score': f'{val_score:4f}'}\n            scoring_record = scoring_record.append(record_dict, ignore_index=True)\n            print(f'\\nTraining {country} - {store} - {product} - Train SMAPE: {train_score:4f}')\n            print(f'Validation {country} - {store} - {product} - Validation SMAPE: {val_score:4f}\\n')\n\n            test_idx = (test_np['country'] == country) &\\\n                       (test_np['store'] == store) &\\\n                       (test_np['product'] == product)\n            test_selected = test_np.loc[test_idx, ['date']].reset_index(drop=True)\n            test_selected = test_selected.rename(columns={'date': 'ds'})\n            test_selected['y'] = np.nan\n            test_predictions = model.predict(test_selected)['yhat1']\n            test_np.loc[test_idx, 'forecast_neu_prophet'] = test_predictions.values\n","619fa97c":"scoring_record","a89ac786":"scoring_record_holidays=pd.DataFrame(columns = ['Country', 'Store', 'Product', 'Train score', 'Val score'])\ntest_np_holidays = test.copy()\n\nfor country in countries:\n    for store in stores:\n        for product in products:\n            train_idx = (train['date'] >= '2015-01-01') & (train['date'] < '2018-01-01') &\\\n                        (train['country'] == country) & (train['store'] == store) & (train['product'] == product)\n            train_selected = train.loc[train_idx, ['date', 'num_sold']].reset_index(drop=True)\n            train_selected = train_selected.rename(columns={'date': 'ds', 'num_sold': 'y'})\n            val_idx = (train['date'] >= '2018-01-01') & (train['date'] < '2019-01-01') &\\\n                      (train['country'] == country) & (train['store'] == store) & (train['product'] == product)\n            val = train.loc[val_idx, ['date', 'num_sold']].reset_index(drop=True)\n            val = val.rename(columns={'date': 'ds', 'num_sold': 'y'})\n            \n            model = NeuralProphet(\n                growth='linear',\n                n_changepoints=10,\n                changepoints_range=0.4,\n                trend_reg=1,\n                trend_reg_threshold=False,\n                yearly_seasonality=True,\n                weekly_seasonality=True,\n                daily_seasonality=False,\n                seasonality_mode='additive',\n                seasonality_reg=1,\n                n_forecasts=365,\n                normalize='off'\n            )\n            if country == \"Finland\": \n                ct_code = \"FI\"\n            elif country == \"Norway\":\n                ct_code = \"NO\"\n            elif country == \"Sweden\":\n                ct_code = \"SE\"\n            model = model.add_country_holidays(ct_code, mode=\"additive\", lower_window=-1, upper_window=2)\n            model.fit(train_selected, freq='D')\n\n            train_predictions = model.predict(train_selected)['yhat1']\n            val_predictions = model.predict(val)['yhat1']\n            train_score = smape(train_selected['y'].values, train_predictions.values)\n            val_score = smape(val['y'].values, val_predictions.values)\n            \n            record_dict = {'Country': country, 'Store': store, 'Product': product, 'Train score': f'{train_score:4f}', 'Val score': f'{val_score:4f}'}\n            scoring_record_holidays = scoring_record_holidays.append(record_dict, ignore_index=True)\n            \n            print(f'\\nTraining {country} - {store} - {product} - Train SMAPE: {train_score:4f}')\n            print(f'Validation {country} - {store} - {product} - Validation SMAPE: {val_score:4f}\\n')\n\n            test_idx = (test_np_holidays['country'] == country) &\\\n                       (test_np_holidays['store'] == store) &\\\n                       (test_np_holidays['product'] == product)\n            test_selected = test_np_holidays.loc[test_idx, ['date']].reset_index(drop=True)\n            test_selected = test_selected.rename(columns={'date': 'ds'})\n            test_selected['y'] = np.nan\n            test_predictions = model.predict(test_selected)['yhat1']\n            test_np_holidays.loc[test_idx, 'forecast_neu_prophet'] = test_predictions.values","ab191c8d":"test_np_holidays.head()","ebe0dceb":"scoring_record_holidays","422bcbb8":"train[\"month\"] = train[\"month\"].astype('category')\ntrain[\"dayofweek\"] = train[\"dayofweek\"].astype('category')\ntrain[\"day\"] = train[\"day\"].astype('category')\ntrain['year'] = train['date'].dt.year-2014\n\ntest[\"month\"] = test[\"month\"].astype('category')\ntest[\"dayofweek\"] = test[\"dayofweek\"].astype('category')\ntest[\"day\"] = test[\"day\"].astype('category')\ntest['year'] = test['date'].dt.year-2014","05e19317":"train.drop([\"date\"], inplace=True, axis=1)\ntest.drop([\"date\"], inplace=True, axis=1)\n\ntrain = pd.get_dummies(train, columns=[\"country\"], prefix=\"country\")\ntest = pd.get_dummies(test, columns=[\"country\"], prefix=\"country\")\n\ntrain = pd.get_dummies(train, columns=[\"store\"], prefix=\"store\")\ntest = pd.get_dummies(test, columns=[\"store\"], prefix=\"store\")\n\ntrain = pd.get_dummies(train, columns=[\"product\"], prefix=\"product\")\ntest = pd.get_dummies(test, columns=[\"product\"], prefix=\"product\")\n\ntrain = pd.get_dummies(train, columns=[\"month\"], prefix= \"month\")\ntest = pd.get_dummies(test, columns=[\"month\"], prefix=\"month\")\n\ntrain = pd.get_dummies(train, columns=[\"day\"], prefix=\"day\")\ntest = pd.get_dummies(test, columns=[\"day\"], prefix=\"day\")\n\ntrain = pd.get_dummies(train, columns=[\"dayofweek\"], prefix=\"week\")\ntest = pd.get_dummies(test, columns=[\"dayofweek\"], prefix=\"week\")\n\ntrain.head()","b26c68b0":"x_train, x_valid, y_train, y_valid = train_test_split(train[train.columns.tolist()[1:]], train.num_sold, test_size=0.2, random_state=42)\nprint(f\"x_train  shape: {x_train.shape}\")\nprint(f\"y_train  shape: {y_train.shape}\\n\")\nprint(f\"x_valid  shape: {x_valid.shape}\")\nprint(f\"y_valid  shape: {y_valid.shape}\")","f1d83bc9":"train_dataset = tf.data.Dataset.from_tensor_slices((x_train.values.astype(\"float32\"), y_train.values.astype(\"float32\"))).batch(64)\nvalid_dataset = tf.data.Dataset.from_tensor_slices((x_valid.values.astype(\"float32\"), y_valid.values.astype(\"float32\"))).batch(64)","c3b5f5b9":"model1 = tf.keras.models.Sequential([\n    layers.Input(shape=(1,59)),\n    tf.keras.layers.Dense(256, activation=tf.nn.relu6),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(256, activation=tf.nn.relu6),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu6),\n    tf.keras.layers.Dense(1)\n])","8fb95916":"cb_es = tf.keras.callbacks.EarlyStopping(monitor=\"val_smape\", patience=2, mode=\"min\", restore_best_weights=True, verbose=1)\ncb_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_smape\", factor=0.5, patience=2, mode=\"min\", min_lr=0.00001, verbose=1)","7f4b3e9a":"model1.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss=smape, metrics=[smape])\n\nhistory = model1.fit(train_dataset, \n               epochs=100, \n               validation_data=valid_dataset,\n               callbacks=[cb_es, cb_lr],\n               verbose=2)","4b64c199":"model2 = tf.keras.models.Sequential([\n    layers.Input(shape=(1,59)),\n    tf.keras.layers.Dense(256, activation=tf.nn.swish),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(256, activation=tf.nn.swish),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(128, activation=tf.nn.swish),\n    tf.keras.layers.Dense(1)\n])","3197bb9c":"model2.compile(optimizer=tf.keras.optimizers.Adam(0.001), loss=smape, metrics=[smape])\n\nhistory = model2.fit(train_dataset, \n                               epochs=100, \n                               validation_data=valid_dataset,\n                               callbacks=[cb_es, cb_lr],\n                               verbose=2)","4618e268":"preds = model2.predict(test)","b59e5335":"test_np_holidays.reset_index(inplace = True)","dba7c4de":"test_np_holidays.head()","eff59ad9":"#sample_submission[\"num_sold\"] = preds.ravel()\nsample_submission[\"num_sold\"] = test_np_holidays['forecast_neu_prophet']\nsample_submission.to_csv(\"submission.csv\", index=False)\nsample_submission","a3aa820c":"**Neural Prophet Model for forecating**","1faa92a2":"**Neural Prophet Model with Country Holidays**"}}