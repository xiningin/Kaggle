{"cell_type":{"bcbbd95b":"code","ecb5948a":"code","5cece2e8":"code","e7dc2ca7":"code","d30750e4":"code","69f98fe1":"code","803bbab9":"code","a3b4cfbc":"code","3086d5e9":"code","1624921d":"code","ab4d0292":"code","e389ae81":"code","40f74419":"code","c9ad514a":"code","dc83e99e":"code","cd949e95":"code","bb363809":"code","6f99f494":"code","060749d6":"code","034fe888":"code","9dec337f":"code","f3880ebe":"markdown","3d069242":"markdown","be61bece":"markdown","ac481dc0":"markdown","4c65bd4a":"markdown","7541d40b":"markdown","11fa2368":"markdown","beec3835":"markdown","6730b4d4":"markdown","465b0492":"markdown","ddb83f9f":"markdown","1160fd36":"markdown","c6f857e9":"markdown","a7597bc0":"markdown","6458db93":"markdown"},"source":{"bcbbd95b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ecb5948a":"# Install chemparse\n!pip install chemparse","5cece2e8":"# Import modules\nimport chemparse\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nimport xgboost\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score","e7dc2ca7":"df = pd.read_csv(\"\/kaggle\/input\/crystal-system-properties-for-liion-batteries\/lithium-ion batteries.csv\")","d30750e4":"df.head()","69f98fe1":"len(df)","803bbab9":"for col in [\"Formula\", \"Spacegroup\", \"Crystal System\"]:\n    print(f\"Column: {col}\")\n    print(f\"Values: {df[col].unique()}\\n\")\n    print(\"------------------------------------------------------\")","a3b4cfbc":"# Parse the forumlas into a dictionary format\nchem_df = df.Formula.apply(chemparse.parse_formula)\n\n# Convert the dictionary into a dataframe and fill NaN's with zero's\nchem_df = pd.json_normalize(chem_df)\nchem_df = chem_df.fillna(0)\n\n# Join back into the original df\ndf = df.join(chem_df)\n\n# Final result:\ndf.head()","3086d5e9":"# Let's not forget to drop the original formula column\ndf = df.drop(columns=[\"Formula\"])","1624921d":"# Let's look at Spacegroup\nplt.figure(figsize=(12,9))\n\nspaceg_data = df.Spacegroup.value_counts()\n\nspaceg_data.plot(kind=\"bar\")\nplt.show()","ab4d0292":"df.Spacegroup = df.Spacegroup.rank(method=\"dense\").astype(int)","e389ae81":"df[\"Has Bandstructure\"].value_counts().plot(kind=\"bar\")","40f74419":"\ndf[\"Has Bandstructure\"] = df[\"Has Bandstructure\"].map({True:1, False:0})","c9ad514a":"df = df.drop(columns=[\"Materials Id\"])\ndf[\"Crystal System\"] = df[\"Crystal System\"].rank(method=\"dense\").astype(int)","dc83e99e":"correlations = df.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(correlations, annot=True)","cd949e95":"# Let's split up the dataframe into feature data and labels\ny = df[\"Crystal System\"]\nX = df.drop(columns=[\"Crystal System\"])","bb363809":"# Quick look of distribution of Crystal System values\nyvals = y.value_counts() \/ len(y) * 100\nyvals.plot(kind=\"bar\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"% of dataset\")\nplt.show()","6f99f494":"# 80% training data and 20% testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","060749d6":"# Decision tree\nclf = DecisionTreeClassifier()\nclf.fit(X_train, y_train)\nclf.score(X_test, y_test)","034fe888":"# Xgboost classifier\nclf = xgboost.XGBClassifier(verbosity=0)\nclf.fit(X_train, y_train)\nclf.score(X_test, y_test)","9dec337f":"# Let's do 10 splits of the data\n\nkfold = KFold(n_splits = 10)\n\nn = 1\n\nclf_xg = xgboost.XGBClassifier(verbosity=0)\nclf_dt = DecisionTreeClassifier()\n\n# Convert the data to np-arrays to enable indexing in the loop\nX_data = np.array(X)\ny_data = np.array(y)\n\nDT_scores = []\nXG_scores = []\n\nfor train, val in kfold.split(X_data, y_data):\n    \n    # Xgboost\n    clf_xg.fit(X_data[train], y_data[train])\n    acc = clf_xg.score(X_data[val], y_data[val])\n    XG_scores.append(acc)\n    print(f\"XG: Fold {n} ACC: {acc}\")\n\n    # Decision tree\n    clf_dt.fit(X_data[train], y_data[train])\n    acc = clf_dt.score(X_data[val], y_data[val])\n    DT_scores.append(acc)\n    print(f\"Dtree: Fold {n} ACC: {round(acc, 2)}\")\n\n    print(\"---------------------\")\n\n    n += 1\n    \n","f3880ebe":"### Let's also look at a heatmap of all features now that they all are numerical","3d069242":"### Chemical formulas\n#### We are going to use the chemparse-module (https:\/\/pypi.org\/project\/chemparse\/) to parse the forumla column. We basically want each molecule to be split into columns of each atom with values being the amount of atoms in the particular molecule. So for an H2O molecule would the column \"H\" have a value 1 and column O would have a 2. You get it.","be61bece":"We turn the spacegroups into categorical numerical variables","ac481dc0":"### Look at that!\n#### Both models scored 100% on the testing data. Now, the dataset is pretty small, only a bit over 300 observations. Let's do a Kfold of the dataset with Xgboost and see how the results vary across different splits of the data","4c65bd4a":"#### While xgboost performed constantly at 100%, the descision tree classifier har a larger variance in it's results. Varying from 85% to 100% depending on the fold of the data. \n\n|Classifier|Best|Worst|Mean|Std|\n|----------|-----|-----|-------|-----|\n|Decision tree classifier| 100% | 85% |96%|0.05\n|Xgboost | 100% | 100% | 100% | 0 |\n\n\n#### In conclusion, the xgboost classifier outperforms the Decision Tree Classifier with a constant 100% score. Please note, however, that the dataset is very small and overfitting should be completely ruled out. It would be very interesting to test the model on a larger set of unseen data.","7541d40b":"# Classification of crystal system for batteries (100% acc)","11fa2368":"### Importing the data","beec3835":"### Let's look at spacegroup","6730b4d4":"Mapping the values to 1 for true, 0 for false","465b0492":"### Looking at bandstructure","ddb83f9f":"#### We see that the dataset is pretty small, only 339 observations.\n#### We look at the str-type columns and how their unique values look like in order to get an idea of where to use dummies and not.","1160fd36":"### Classification time\n#### Let's split up the dataframe into feature data and labels and look at how the label is distributed","c6f857e9":"### Final processing: get dummies for Crystal System and drop the ID-column","a7597bc0":"#### We can see from this plot that a baseline model that only predict's \"1\" would score a 40% accuracy. In order to get any significant result, we need to score better than this.","6458db93":"### Classifiers\n#### We are going to use Decision Tree Classifier aswell as an Xgboost classifier and compare their results."}}