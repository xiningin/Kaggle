{"cell_type":{"e038fb54":"code","a5c5d31f":"code","635a74fa":"code","769e4380":"code","5dcd5423":"code","49d52947":"code","f3a8a5ac":"code","e7bf3ad4":"code","65082ffa":"code","d65abae6":"code","d8783050":"code","3554b738":"code","94fa9c06":"code","83a6a3a6":"code","0787f107":"code","105891d6":"code","0d32be4f":"code","a82beb30":"code","501c90d6":"code","14bc4c5b":"code","0a1ee55f":"markdown","553ff2d2":"markdown","61098f14":"markdown","aae5ccab":"markdown","95034f30":"markdown","814d5af7":"markdown","ca8c0673":"markdown","55d5bf71":"markdown","a864c7c3":"markdown","8d1472e0":"markdown","fe3d6713":"markdown","ac2a24b9":"markdown","9730350d":"markdown","5464775d":"markdown","29355305":"markdown"},"source":{"e038fb54":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a5c5d31f":"#\/kaggle\/input\/emnist\/emnist-letters-mapping.txt\ntesting_letter = pd.read_csv('\/kaggle\/input\/emnist\/emnist-letters-test.csv')\ntraining_letter = pd.read_csv('\/kaggle\/input\/emnist\/emnist-letters-train.csv')","635a74fa":"print(training_letter.shape)\nprint(testing_letter.shape)","769e4380":"#training_letters\ny1 = np.array(training_letter.iloc[:,0].values)\nx1 = np.array(training_letter.iloc[:,1:].values)\n#testing_labels\ny2 = np.array(testing_letter.iloc[:,0].values)\nx2 = np.array(testing_letter.iloc[:,1:].values)\nprint(y1.shape)\nprint(x1.shape)","5dcd5423":"import matplotlib.pyplot as plt \nfig,axes = plt.subplots(3,5,figsize=(10,8))\nfor i,ax in enumerate(axes.flat):\n    ax.imshow(x1[i].reshape([28,28]))\n","49d52947":"import tensorflow as tf","f3a8a5ac":"# Normalise and reshape data\ntrain_images = x1 \/ 255.0\ntest_images = x2 \/ 255.0\n\ntrain_images_number = train_images.shape[0]\ntrain_images_height = 28\ntrain_images_width = 28\ntrain_images_size = train_images_height*train_images_width\n\ntrain_images = train_images.reshape(train_images_number, train_images_height, train_images_width, 1)\n\ntest_images_number = test_images.shape[0]\ntest_images_height = 28\ntest_images_width = 28\ntest_images_size = test_images_height*test_images_width\n\ntest_images = test_images.reshape(test_images_number, test_images_height, test_images_width, 1)","e7bf3ad4":"# Transform labels\nnumber_of_classes = 37\n\ny1 = tf.keras.utils.to_categorical(y1, number_of_classes)\ny2 = tf.keras.utils.to_categorical(y2, number_of_classes)","65082ffa":"#from tensorflow.keras.preprocessing.image import ImageDataGenerator\n#from tensorflow.keras.models import Model\n#from tensorflow.keras.layers import Dense,Dropout,MaxPooling2D,Conv2D\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau,ModelCheckpoint","d65abae6":"from sklearn.model_selection import train_test_split","d8783050":"train_x,test_x,train_y,test_y = train_test_split(train_images,y1,test_size=0.2,random_state = 42)","3554b738":"model = tf.keras.Sequential([ \n    tf.keras.layers.Conv2D(32,3,input_shape=(28,28,1)),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Flatten(input_shape=(28,28,1)),\n    tf.keras.layers.Dense(512,activation='relu'),\n    tf.keras.layers.Dense(128,activation='relu'),\n    tf.keras.layers.Dense(number_of_classes,activation='softmax')\n])","94fa9c06":"model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])","83a6a3a6":" \nMCP = ModelCheckpoint('Best_points.h5',verbose=1,save_best_only=True,monitor='val_accuracy',mode='max')\nES = EarlyStopping(monitor='val_accuracy',min_delta=0,verbose=0,restore_best_weights = True,patience=3,mode='max')\nRLP = ReduceLROnPlateau(monitor='val_loss',patience=3,factor=0.2,min_lr=0.0001)","0787f107":"history = model.fit(train_x,train_y,epochs=10,validation_data=(test_x,test_y),callbacks=[MCP,ES,RLP])","105891d6":"import seaborn as sns","0d32be4f":"q = len(history.history['accuracy'])\n\nplt.figsize=(10,10)\nsns.lineplot(x = range(1,1+q),y = history.history['accuracy'], label='Accuracy')\nsns.lineplot(x = range(1,1+q),y = history.history['val_accuracy'], label='Val_Accuracy')\nplt.xlabel('epochs')\nplt.ylabel('Accuray')","a82beb30":"train_x2,test_x2,train_y2,test_y2 = train_test_split(train_images,y1,test_size=0.15,random_state = 42)","501c90d6":"history1 = model.fit(train_x2,train_y2,epochs=10,validation_data=(test_x2,test_y2))","14bc4c5b":"q = len(history1.history['accuracy'])\n\nplt.figsize=(10,10)\nsns.lineplot(x = range(1,1+q),y = history1.history['accuracy'], label='Accuracy')\nsns.lineplot(x = range(1,1+q),y = history1.history['val_accuracy'], label='Val_Accuracy')\nplt.xlabel('epochs')\nplt.ylabel('Accuray')","0a1ee55f":"# As the validation score score went down we won't be changing the test_split size. \n# Thus the above model is better than the latter one.","553ff2d2":"## Let's see what type of images we have got. ","61098f14":"## Changing the test_split size to 0.15 to see if validation score increases or not","aae5ccab":"![emnist-byclass.png](attachment:emnist-byclass.png)","95034f30":"### Before feeding in the data to the model we will normalise and reshape the data given to us. This will decrease the complexity of the models and make the model work efficiently as less complex numbers will be there to process.","814d5af7":"# Preprocesssing","ca8c0673":"# <font color ='torcous'>EMNIST Letter Dataset with 96.7% accuracy ","55d5bf71":"## Let's plot Accuracy vs Val_Accuracy to further evaluation..","a864c7c3":"## <font color='#00B2FF'> The EMNIST dataset is a set of handwritten character digits derived from the NIST Special Database 19 and converted to a 28x28 pixel image format and dataset structure that directly matches the MNIST dataset .\n","8d1472e0":"## Adding Callback API's to save best weights and change lr","fe3d6713":"# Lets create a CNN for the classification of these random images.","ac2a24b9":"# <font color='Orange'>Hope everyone who reads this notebook finds it helpful. Do upvote it. Thank You \ud83d\ude4f\ud83d\ude03","9730350d":"# 1. Normalisation and reshaping of data","5464775d":"## Hope everyone who reads this notebook finds it helpful. \n## Do upvote it. Thank You\ud83d\ude4f\ud83d\ude03","29355305":"## Here we are specifically using EMNIST letter dataset. So Let's get started"}}