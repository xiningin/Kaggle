{"cell_type":{"50f1f4b2":"code","2e87509a":"code","64c34825":"code","b70a042a":"code","edcdea9c":"code","22a45cfe":"code","c3bfbb32":"code","ef9637b7":"code","501e16ed":"code","de86f134":"code","fb0037bb":"code","878fcc3a":"code","7a7a89fd":"code","6f64eadb":"code","3afe1dad":"code","47a98f75":"code","fb266d57":"code","52e84f42":"code","73507d45":"code","5b336e98":"code","4f2fc104":"code","bc7897c0":"code","82c39a49":"code","4e5bb9e6":"code","8e35acc2":"code","713bc45d":"code","2482d61f":"markdown","4ea93e0e":"markdown","d60b1e18":"markdown","2593aec0":"markdown","8cbb248b":"markdown","0eb1b589":"markdown","c9a51525":"markdown","b4dd6383":"markdown","e4a072ba":"markdown","77c0766f":"markdown","b9a88f6f":"markdown","f6016c30":"markdown","fba29c10":"markdown","3d3bf234":"markdown","0b6eecc8":"markdown","27ab1672":"markdown","f90e0563":"markdown","43d529d2":"markdown","2b9b80ef":"markdown","f5793f96":"markdown","84426460":"markdown","e3d134d8":"markdown","d18cdb37":"markdown","2ed1dac2":"markdown","e712c981":"markdown"},"source":{"50f1f4b2":"import numpy as np \nimport pandas as pd\nimport category_encoders as ce\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import KNNImputer\nfrom sklearn.metrics import confusion_matrix,precision_recall_curve\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\nimport xgboost as xgb \nfrom xgboost import XGBClassifier\n\npd.set_option(\"display.max_columns\", None)\npd.set_option(\"display.max_rows\", None)","2e87509a":"raw_data = pd.read_csv('..\/input\/credit-card-customers\/BankChurners.csv')\nraw_data.drop(columns=['CLIENTNUM',\n                       'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',\n                       'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'],\n          axis=1,inplace=True)\nraw_data.head()","64c34825":"data = raw_data.copy()\ndata['Attrition_Flag'] = data['Attrition_Flag'].apply(lambda x: 1 if x=='Attrited Customer' else 0)\ndata['Gender'] = data['Gender'].apply(lambda x: 1 if x=='F' else 0)\n\nedu_dict = {'Unknown':np.nan,'Uneducated':0,'High School':1,'College':2,\n             'Graduate':3,'Post-Graduate':4,'Doctorate':5}\ndata['Education_Level'] = data['Education_Level'].apply(lambda x: edu_dict[x])\n\nmarital_onehot = pd.get_dummies(data['Marital_Status'],prefix='Marital_Status')\ndata = pd.concat([data,marital_onehot],axis=1)\ndata.drop(columns=['Marital_Status','Marital_Status_Unknown'],axis=1,inplace=True)\n\nincome_dict = {'Unknown':np.nan,'Less than $40K':0,'$40K - $60K':1,\n                '$60K - $80K':2,'$80K - $120K':3,'$120K +':4}\ndata['Income_Category'] = data['Income_Category'].apply(lambda x: income_dict[x])","b70a042a":"X = data.drop(columns='Attrition_Flag',axis=1)\ny = data['Attrition_Flag']\ntrain_x,test_x,train_y,test_y = train_test_split(X,y,test_size=0.2, shuffle=True, random_state=42)","edcdea9c":"count_enc = ce.CountEncoder(cols='Card_Category')\ncount_encoded = count_enc.fit(train_x['Card_Category'])\ntrain_x = train_x.join(count_encoded.transform(train_x['Card_Category']).add_suffix('_count'))\ntest_x = test_x.join(count_encoded.transform(test_x['Card_Category']).add_suffix('_count'))\ntrain_x.drop(columns='Card_Category',axis=1,inplace=True)\ntest_x.drop(columns='Card_Category',axis=1,inplace=True)","22a45cfe":"Imputer = KNNImputer(missing_values = np.nan, n_neighbors=1, weights='uniform').fit(train_x)\nimputed_train_x = Imputer.transform(train_x)\nimputed_test_x = Imputer.transform(test_x)","c3bfbb32":"np.sum(np.isnan(imputed_train_x)),np.sum(np.isnan(imputed_test_x))","ef9637b7":"imputed_train = np.column_stack((imputed_train_x,train_y))\ndf_imputed_train = pd.DataFrame(imputed_train)\ndf_imputed_train.columns = list(train_x.columns) + ['Attrition_Flag']\n\ncorr = df_imputed_train.corr()\nfig,ax = plt.subplots(figsize=(12,10))\nsns.heatmap(corr,ax=ax,vmin=-1, vmax=1, cmap='coolwarm', annot=False)\nplt.yticks(rotation=0,fontsize=13)\nplt.xticks(rotation=90,fontsize=13)\nax.set_title('Correlation Heatmap',fontsize=15,fontweight='bold')","501e16ed":"columns = np.full(shape=(corr.shape[0],), fill_value=True, dtype=bool)\nfor i in range(corr.shape[0]):\n    for j in range(i+1, corr.shape[0]):\n        if abs(corr.iloc[i,j]) >= 0.9:\n            if columns[j]:\n                columns[j] = False\nsel_columns = df_imputed_train.columns[columns]\ndf_imputed_train = df_imputed_train[sel_columns]\ndf_imputed_train.columns","de86f134":"cleaned_train_x = df_imputed_train.drop(columns='Attrition_Flag',axis=1)\ndf_imputed_test_x = pd.DataFrame(imputed_test_x)\ndf_imputed_test_x.columns = test_x.columns\ncleaned_test_x = df_imputed_test_x.drop(columns='Avg_Open_To_Buy',axis=1)","fb0037bb":"y_cnt_before = train_y.value_counts()\ntrace = go.Pie(labels = y_cnt_before.index, \n               values = y_cnt_before.values,\n               hoverinfo = 'percent+value+label',\n               textinfo = 'percent',\n               textposition = 'inside',\n               textfont = dict(size=14),\n               title = 'Attrition Flag',\n               titlefont = dict(size=15),\n               hole = 0.5,\n               showlegend = True,\n               marker = dict(line=dict(color='black',width=2)))\nfig = go.Figure(data=[trace])\nfig.update_layout(height=500, width=500)\nfig.show()","878fcc3a":"over_sampling = SMOTE(sampling_strategy=0.4, k_neighbors=5, random_state=42)\nunder_sampling = RandomUnderSampler(sampling_strategy=0.5, random_state=42)\npipeline = Pipeline(steps=[('o',over_sampling),('u',under_sampling)])\nou_train_x,ou_train_y = pipeline.fit_resample(cleaned_train_x,train_y)","7a7a89fd":"y_cnt_after = ou_train_y.value_counts()\ntrace = go.Pie(labels = y_cnt_after.index, \n               values = y_cnt_after.values,\n               hoverinfo = 'percent+value+label',\n               textinfo = 'percent',\n               textposition = 'inside',\n               textfont = dict(size=14),\n               title = 'Attrition Flag',\n               titlefont = dict(size=15),\n               hole = 0.5,\n               showlegend = True,\n               marker = dict(line=dict(color='black',width=2)))\nfig = go.Figure(data=[trace])\nfig.update_layout(height=500, width=500)\nfig.show()","6f64eadb":"def rev_recall(pred,dtrain):\n    labels = dtrain.get_label()\n    pred = np.round(1.0 \/ (1.0 + np.exp(-pred)))\n    cm = confusion_matrix(labels, pred)\n    recall =float(cm[1][1]) \/ float(cm[1][0]+cm[1][1])\n    reversed_recall = 1-recall\n    return 'Reversed-Recall',reversed_recall","3afe1dad":"baseline_params = {'n_estimators': 500,\n                   'max_depth': 8,\n                   'learning_rate': 0.3,\n                   'subsample': 0.8,\n                   'colsample_by_tree': 0.6,\n                   'min_child_weight':7,\n                   'verbosity' :1,\n                   'random_state': 42}\n\nbaseline_xgb = xgb.XGBClassifier(**baseline_params)\nbaseline_xgb.fit(ou_train_x,ou_train_y, eval_metric=rev_recall,\n                 eval_set=[(ou_train_x,ou_train_y),(cleaned_test_x,test_y)], \n                 early_stopping_rounds=10,verbose=1)","47a98f75":"pred_y = baseline_xgb.predict(cleaned_test_x)\n\nfig,ax = plt.subplots(figsize=(9,6))\nsns.heatmap(confusion_matrix(test_y,pred_y),ax=ax,\n            annot=True, fmt='d',annot_kws={'size':16},cmap='Blues')\nax.set_xticklabels(['Not Churn','Churn'],fontsize=14)\nax.set_yticklabels(['Not Churn','Churn'],fontsize=14,rotation=0)\nax.set_xlabel('Predict',fontsize=16)\nax.set_ylabel('True',fontsize=16)\nax.set_title('Confusion Matrix on Test Data', fontsize=17, fontweight='bold')","fb266d57":"fig,ax = plt.subplots(figsize=(10,8))\nxgb.plot_importance(baseline_xgb,ax=ax,importance_type='gain',height=0.6, max_num_features=None)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nax.set_title('Feature Importance',fontsize=15,fontweight='bold')","52e84f42":"baseline_params_2 = {'n_estimators': 500,\n                   'max_depth': 8,\n                   'learning_rate': 0.3,\n                   'subsample': 0.8,\n                   'colsample_by_tree': 0.6,\n                   'min_child_weight':7,\n                   'verbosity' :1,\n                   'scale_pos_weight':3,\n                   'random_state': 42}\n\nxgb_2 = xgb.XGBClassifier(**baseline_params_2)\nxgb_2.fit(cleaned_train_x,train_y, eval_metric=rev_recall,\n                 eval_set=[(cleaned_train_x,train_y),(cleaned_test_x,test_y)], \n                 early_stopping_rounds=10,verbose=1)","73507d45":"pred_y_2 = xgb_2.predict(cleaned_test_x)\n\nfig,ax = plt.subplots(figsize=(9,6))\nsns.heatmap(confusion_matrix(test_y,pred_y_2),ax=ax,\n            annot=True, fmt='d',annot_kws={'size':16},cmap='Blues')\nax.set_xticklabels(['Not Churn','Churn'],fontsize=14)\nax.set_yticklabels(['Not Churn','Churn'],fontsize=14,rotation=0)\nax.set_xlabel('Predict',fontsize=16)\nax.set_ylabel('True',fontsize=16)\nax.set_title('Confusion Matrix on Test Data', fontsize=17, fontweight='bold')","5b336e98":"fig,ax = plt.subplots(figsize=(10,8))\nxgb.plot_importance(xgb_2,ax=ax,importance_type='gain',height=0.6, max_num_features=None)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nax.set_title('Feature Importance',fontsize=15,fontweight='bold')","4f2fc104":"baseline_params_3 = {'n_estimators': 500,\n                   'max_depth': 8,\n                   'learning_rate': 0.3,\n                   'subsample': 0.8,\n                   'colsample_by_tree': 0.6,\n                   'min_child_weight':7,\n                   'verbosity' :1,\n                   'random_state': 42}\n\nxgb_3 = xgb.XGBClassifier(**baseline_params_3)\nxgb_3.fit(cleaned_train_x,train_y, eval_metric=rev_recall,\n                 eval_set=[(cleaned_train_x,train_y),(cleaned_test_x,test_y)], \n                 early_stopping_rounds=10,verbose=1)","bc7897c0":"pred_y_3 = xgb_3.predict_proba(cleaned_test_x)\nclass_1_proba = pred_y_3[:,1]\nprecision, recall, thresholds = precision_recall_curve(test_y, class_1_proba)\nf_score = (2*precision*recall)\/(precision+recall)\nidx = np.argmax(f_score)\n\nno_skill = np.sum(test_y) \/ len(test_y)\nfig,ax = plt.subplots(figsize=(8,6))\nax.plot([0,1], [no_skill,no_skill], linestyle='--', label='No Skill')\nax.plot(recall, precision, marker='.', label='XGBoost',alpha=0.7)\nax.scatter(recall[idx], precision[idx], s=100, marker='o', color='black', label='Best')\nax.set_xlabel('Recall')\nax.set_ylabel('Precision')\nax.set_title('Precision Recall Curve')\nax.legend(loc='center left')","82c39a49":"print('best threshold is %f' % (thresholds[idx]))","4e5bb9e6":"best_threshold = thresholds[idx]\nfinal_pred_y_3 = [1 if p>=best_threshold else 0 for p in class_1_proba]\n\nfig,ax = plt.subplots(figsize=(9,6))\nsns.heatmap(confusion_matrix(test_y,final_pred_y_3),ax=ax,\n            annot=True, fmt='d',annot_kws={'size':16},cmap='Blues')\nax.set_xticklabels(['Not Churn','Churn'],fontsize=14)\nax.set_yticklabels(['Not Churn','Churn'],fontsize=14,rotation=0)\nax.set_xlabel('Predict',fontsize=16)\nax.set_ylabel('True',fontsize=16)\nax.set_title('Confusion Matrix on Test Data', fontsize=17, fontweight='bold')","8e35acc2":"all_recalls = [float(confusion_matrix(test_y,y_hat)[1][1]\/np.sum(confusion_matrix(test_y,y_hat),axis=1)[1]) \n               for y_hat in [pred_y,pred_y_2,final_pred_y_3]]\nall_precisions = [float(confusion_matrix(test_y,y_hat)[1][1]\/np.sum(confusion_matrix(test_y,y_hat),axis=0)[1]) \n                  for y_hat in [pred_y,pred_y_2,final_pred_y_3]]\nall_acc = [float((confusion_matrix(test_y,y_hat)[1][1]+confusion_matrix(test_y,y_hat)[0][0])\/len(test_y)) \n           for y_hat in [pred_y,pred_y_2,final_pred_y_3]]\nall_methods = ['SMOTE','Change Class Weight','Move Threshold']","713bc45d":"fig = go.Figure(data=[go.Table(header=dict(values=['Method','Recall', 'Precision','Accuracy'], \n                               fill_color='yellow', line_color='black'),\n                               cells=dict(values=[all_methods,all_recalls,all_precisions,all_acc], \n                               fill_color='lavender', line_color='black'))])\nfig.update_layout(width=800,height=700)\nfig.show()","2482d61f":"As the result shown below, Recall for all three methods are almost the same. Precision and accuracy is the highest when using the method of moving threshold. From my point of view, resampling data and moving threshold are both good choices when dealing with imbalanced data. Drawback of Changing class weight is that precison will be sacrificed. ","4ea93e0e":"Customize evaluation function 'reversed recall'","d60b1e18":"Recall is 92% in baseline model :)","2593aec0":"## Count Encoding ","8cbb248b":"Feature 'Avg_Open_To_Buy' is dropped because highly correlate with the feature 'Credit_Limit'.","0eb1b589":"## Check Correlation","c9a51525":"Imputing is done successfully.","b4dd6383":"## SMOTE + XGBoost","e4a072ba":"Split train and test data in order to use further encoding and imputation methods.","77c0766f":"I think count encoding is suitable for the feature 'Card_Category' because value 'Blue' take the dominant place.","b9a88f6f":"## Impute Missing Values with KNNImputer","f6016c30":"# Conclusion ","fba29c10":"Ordinal features like 'Education_Level' can be encoded with ordinal values to reduce dimension.\nNominal features like 'Marital_Status' is encoded with one-hot encoding.","3d3bf234":"For unknown values in the feature 'Education_Level' and 'Income_Category', I encode them as np.nan for further imputation. For unknown values in the feature 'Marital_Status', I treat it as a separate category.","0b6eecc8":"# Data Preprocessing","27ab1672":"**Precision Recall curve and F-score** are chosen to find the best threshold because they reveal the model performance well when the dataset is imbalanced.","f90e0563":"If you want to see more visualizations, please go to [notebook](https:\/\/www.kaggle.com\/jswxhd\/eda-strategies-for-different-data-attributtes). :)","43d529d2":"Combine oversampling for minor class with undersampling for major class.","2b9b80ef":"1. There is highly positively correlation between 'Avg_Open_To_Buy' and 'Credit_Limit'.\n2. There is highly negatively correlation betweern 'Marital_Status_Single' and 'Marital_Status_Married'.","f5793f96":"## XGBoost + Threshold Moving","84426460":"From my points of view, null values exist in features 'Education_Level' and 'Income_Category' because they were not recorded. Hence, dropping them directly is not a wise choice. Instead we should use imputing method.","e3d134d8":"# Modeling","d18cdb37":"## Ordinal Encoding and one-hot Encoding","2ed1dac2":"## XGBoost + Change Class Weight","e712c981":"Although the recall is the same as I get in SMOTE+XGBoost, the precision goes down a little. It is the side effect of changing class weight when modeling."}}