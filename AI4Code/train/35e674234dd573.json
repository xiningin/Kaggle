{"cell_type":{"a33832da":"code","06f6cebc":"code","69a525d8":"code","31abb435":"code","7289588d":"code","5cc5fcfc":"code","e8c734f3":"code","0aacbe29":"code","bf185d46":"code","01112563":"code","3b62c6a1":"code","96ec2bd8":"code","ba11d010":"code","6496ac4f":"code","5e779d01":"code","ac014748":"code","584addef":"code","bafed5ca":"code","355a233a":"code","26c5cc90":"code","368767ff":"code","10041898":"code","d687aadb":"markdown","2ea22a39":"markdown","1446225b":"markdown","510b2b22":"markdown","d90874fc":"markdown","d7bf7217":"markdown","c184af61":"markdown","a3ce07ec":"markdown","6fa4f3eb":"markdown","5cecb2a6":"markdown","de2f4297":"markdown","3034c009":"markdown"},"source":{"a33832da":"import numpy as np\nimport pandas as pd\ntrain=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","06f6cebc":"for i in range(100):\n    print(train.iloc[i,3])","69a525d8":"for i in range(train.shape[0]):\n    train.iloc[i,3]=' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)\",\" \", train.iloc[i,3]).split())","31abb435":"for i in range(test.shape[0]):\n    test.iloc[i,3]=' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)\",\" \", test.iloc[i,3]).split())","7289588d":"for i in range(100):\n    print(train.iloc[137+i,3])","5cc5fcfc":"!pip3 install tqdm\n!pip3 install transformers","e8c734f3":"import transformers\ntokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')","0aacbe29":"import tqdm\n\ndef create_bert_input_features(tokenizer, docs, max_seq_length):\n    \n    all_ids, all_masks = [], []\n    for doc in tqdm.tqdm(docs, desc=\"Converting docs to features\"):\n        tokens = tokenizer.tokenize(doc)\n        if len(tokens) > max_seq_length-2:\n            tokens = tokens[0 : (max_seq_length-2)]\n        tokens = ['[CLS]'] + tokens + ['[SEP]']\n        ids = tokenizer.convert_tokens_to_ids(tokens)\n        masks = [1] * len(ids)\n        # Zero-pad up to the sequence length.\n        while len(ids) < max_seq_length:\n            ids.append(0)\n            masks.append(0)\n        all_ids.append(ids)\n        all_masks.append(masks)\n    encoded = np.array([all_ids, all_masks])\n    return encoded","bf185d46":"from sklearn.model_selection import train_test_split","01112563":"train_X, val_X, train_Y, val_Y = train_test_split(train['text'], train['target'], test_size=0.15, random_state=42)","3b62c6a1":"train_X, val_X, train_Y, val_Y = train_X.values, val_X.values, train_Y.values, val_Y.values","96ec2bd8":"test_X = test['text'].values","ba11d010":"MAX_SEQ_LENGTH = 500\n\ntrain_features_ids, train_features_masks = create_bert_input_features(tokenizer, train_X, \n                                                                      max_seq_length=MAX_SEQ_LENGTH)\nval_features_ids, val_features_masks = create_bert_input_features(tokenizer, val_X, \n                                                                  max_seq_length=MAX_SEQ_LENGTH)\n#test_features = create_bert_input_features(tokenizer, test_reviews, max_seq_length=MAX_SEQ_LENGTH)\nprint('Train Features:', train_features_ids.shape, train_features_masks.shape)\nprint('Val Features:', val_features_ids.shape, val_features_masks.shape)","6496ac4f":"test_features_ids, test_features_masks = create_bert_input_features(tokenizer, test_X, \n                                                                    max_seq_length=MAX_SEQ_LENGTH)\nprint('Test Features:', test_features_ids.shape, test_features_masks.shape)","5e779d01":"import tensorflow as tf","ac014748":"inp_id = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype='int32', name=\"bert_input_ids\")\ninp_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype='int32', name=\"bert_input_masks\")\ninputs = [inp_id, inp_mask]\n\nhidden_state = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased')(inputs)[0]\npooled_output = hidden_state[:, 0]    \ndense1 = tf.keras.layers.Dense(256, activation='relu')(pooled_output)\ndrop1 = tf.keras.layers.Dropout(0.3)(dense1)\ndense2 = tf.keras.layers.Dense(256, activation='relu')(drop1)\ndrop2 = tf.keras.layers.Dropout(0.3)(dense2)\noutput = tf.keras.layers.Dense(1, activation='sigmoid')(drop2)\n\n\nmodel = tf.keras.Model(inputs=inputs, outputs=output)\nmodel.compile(optimizer=tf.optimizers.Adam(learning_rate=2e-6, \n                                           epsilon=1e-08), \n              loss='binary_crossentropy', metrics=['accuracy',tf.keras.metrics.AUC()])","584addef":"es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n                                      patience=2,\n                                      restore_best_weights=True,\n                                      verbose=1)","bafed5ca":"model.fit([train_features_ids, \n           train_features_masks], train_Y, \n          validation_data=([val_features_ids, \n                            val_features_masks], val_Y),\n          epochs=10, \n          batch_size=20, \n          shuffle=True,\n          callbacks=[es],\n          verbose=1)","355a233a":"predictions = [1 if pr > 0.5 else 0 \n                   for pr in model.predict([val_features_ids, \n                                            val_features_masks], batch_size=200, verbose=0).ravel()]","26c5cc90":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_auc_score\nprint('null accuracy:', max(sum(val_Y)\/val_Y.shape[0],1-sum(val_Y)\/val_Y.shape[0]))\nprint(\"Accuracy: %.2f%%\" % (accuracy_score(val_Y, predictions)*100))\nprint(\"roc auc:\", roc_auc_score(val_Y, predictions))\nprint(classification_report(val_Y, predictions))\npd.DataFrame(confusion_matrix(val_Y, predictions))","368767ff":"test_Y=model.predict([test_features_ids, test_features_masks], batch_size=200, verbose=0)\n\ntest_label=[]\n\nfor i in range(test_Y.shape[0]):\n    if test_Y[i]>=0.5:\n        test_label.append(1)\n    else:\n        test_label.append(0)","10041898":"submission=pd.DataFrame({'id': test['id'], 'target':test_label})\nprint(submission.head(10))\n\nfilename = 'submission_nlp_tweets_bert.csv'\n\nsubmission.to_csv(filename,index=False)","d687aadb":"**read in the data**","2ea22a39":"**fit and tune model**","1446225b":"**18. submit**","510b2b22":"**tweets look much more clean**","d90874fc":"**cross validation**","d7bf7217":"**take a look at the tweets**","c184af61":"**define model**","a3ce07ec":"**1. remove #, @, punctuations and weblinks** <br>\n**copied from https:\/\/stackoverflow.com\/questions\/8376691\/how-to-remove-hashtag-user-link-of-a-tweet-using-regular-expression**","6fa4f3eb":"**seperate train and test**","5cecb2a6":"The author Fanglida Yan has used code from these references in the notebook. <br>\nBERT for classfication: https:\/\/github.com\/dipanjanS\/deep_transfer_learning_nlp_dhs2019\/blob\/master\/notebooks\/6%20-%20Transformers%20-%20DistilBERT.ipynb <br>\nremove @, # and http:\/\/... : https:\/\/stackoverflow.com\/questions\/8376691\/how-to-remove-hashtag-user-link-of-a-tweet-using-regular-expression <br>\n\n0. feature preprocessing <br>\n1. split train and cross validation sets <br>\n2. create features for BERT<br>\n3. build the model in Keras <br>\n4. model tuning and cross validation <br>\n5. make prediction for test set <br>","de2f4297":"**create bert input features**","3034c009":"**predict the test set**"}}