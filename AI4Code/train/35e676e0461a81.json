{"cell_type":{"934df64d":"code","d3f6fdc2":"code","a803ca65":"code","b20a8dc4":"code","f3ee890d":"code","5b099945":"code","aa886d73":"code","b1ba295a":"code","b5912fbd":"code","2b2be4be":"code","042ceea1":"code","16ea78b2":"code","f07b874f":"markdown","af1cf655":"markdown","9df8ae7d":"markdown","f95632c1":"markdown","ddf7cece":"markdown","557f9bea":"markdown","6590652e":"markdown","620f76d3":"markdown","6b0f70f9":"markdown","c510342a":"markdown","105040fa":"markdown","1aa56167":"markdown"},"source":{"934df64d":"import pandas as pd\nimport missingno\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.impute import SimpleImputer\n","d3f6fdc2":"df = pd.read_csv('..\/input\/melbourne-housing-snapshot\/melb_data.csv')\nprint('Shape of dataset: ', df.shape)\ndf.head()","a803ca65":"df.info()","b20a8dc4":"# missing values visualization\nmissingno.matrix(df)","f3ee890d":"# let's look al what percent of the data is missing\nmissing = df.isnull().sum()\nmissing_precent = 100*missing\/len(df)\nmissing_table = pd.concat([missing, missing_precent], axis=1)\nmissing_table.columns = ['missing_value', '% of missing_value']\n\nmissing_table = missing_table.loc[missing_table['missing_value'] != 0].sort_values('missing_value')\nprint('The dataset has total {} columns \\nThere are {} columns that have missing values\\n\\n'.format(df.shape[1], missing_table.shape[0]))\nmissing_table.head()","5b099945":"def data_split(df):\n    y = df.Price\n    X = df.drop(['Price'], axis=1)\n    # for simplicity, I will use only features with numerical values\n    X = X.select_dtypes(exclude=['object'])\n    \n    x_train, x_valid, y_train,y_valid = train_test_split(X, y, \n                                                         test_size=0.3,\n                                                         random_state=42)\n    return x_train, x_valid, y_train, y_valid\n\ndef data_score(xtrain, ytrain,xvalid,yvalid):\n    model = RandomForestRegressor()\n    model.fit(xtrain, ytrain)\n    preds = model.predict(xvalid)\n    return mean_absolute_error(preds, yvalid)","aa886d73":"nul_cols = ['Car','BuildingArea', 'YearBuilt', 'CouncilArea']\n# Droping the rows with null values\ndf_not_null_rows = df.dropna()\n\nreduced_X_train, reduced_X_valid, y_train, y_valid = data_split(df_not_null_rows)\nprint(\"MAE from dropping rows with missing values):\")\nprint(data_score(reduced_X_train, y_train, reduced_X_valid,  y_valid))","b1ba295a":"# Droping the columns with null values\ndf_not_null = df.drop(nul_cols, axis=1)\n\nreduced_X_train, reduced_X_valid, y_train, y_valid = data_split(df_not_null)\nprint(\"MAE from droping columns with missing values):\")\nprint(data_score(reduced_X_train, y_train, reduced_X_valid,  y_valid))","b5912fbd":"imputer = SimpleImputer()\n#using features with only neumeric values for simplicity purpose only\ndf_numeric = df.select_dtypes(exclude='object')\nimputed_df = imputer.fit_transform(df_numeric)\n\n# since imputation converts the data frame into a numpy array, \n# let's convert it inot a dataframe back\nimputed_df = pd.DataFrame(imputed_df)\n\n# since imputation removes column names, let's put them back\nimputed_df.columns = df_numeric.columns\nimputed_df.head()","2b2be4be":"imputed_X_train, imputed_X_valid, y_train, y_valid = data_split(imputed_df)\nprint('MAE from Simple Imputation Approach: ')\nprint(data_score(imputed_X_train,  y_train, imputed_X_valid, y_valid))","042ceea1":"# making a copy to avoid changing the original data during imputatoin\ndf_copy = df.copy()\n\n#Make new columns with the imputation infromation\nfor col in nul_cols:\n    df_copy[col + '_missing'] = df_copy[col].isnull()\n\nprint(df_copy.columns)\ndf_copy.head()","16ea78b2":"#remove object features\ndf_copy = df_copy.select_dtypes(exclude=['object'])\n\nimputer = SimpleImputer()\ndf_copy_imputed = pd.DataFrame(imputer.fit_transform(df_copy))\n\n# puting back the column names after being removed by imputation\ndf_copy_imputed.columns = df_copy.columns\n\nimputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid = data_split(df_copy_imputed)\nprint('MAE from an extension to Imputation: ')\nprint(data_score(imputed_X_train_plus, y_train, imputed_X_valid_plus,  y_valid))","f07b874f":"### Idea 1(a): Droping the Rows with missing values\nOne simple idea is just drop the rows with null values.\n\n![image.png](attachment:84917a92-6c46-4bb9-8f34-5bcec0ca4c95.png)","af1cf655":"As we see there are total four features (Car, BuildingArea, YearBuilt and CouncilArea) with null values. Now let's what percent of data is missing per column.","9df8ae7d":"We see approach two has lower mean absolute error than approach 1 on this dataset.\n\nSince filling missing values with mean, mode or median might not give the exact number the dataset is needed. Since the exact value could be lower or higher than the imputed values and sometimes the missing rows might some unique similarities that can help to get a better ML predictor. So along with filling the missing values by imputation, it is also a good practice to keep tract of the features\/columns where the missing values was. The next idea will describes how to apply this technique.","f95632c1":"This idea might be be not as beneficial if we have a dataset with one or two columns and for those columns most of the values (say more than 75%) are missing. Dropping rows in this case will lead to get rid of the most of the dataset. So what should we do? Well, in that case we can drop that one or two specific columns with missing values.","ddf7cece":"We might want to avoid this approach if most of the columns in the dataset has very few missing values. For example, if more than half of the columns has only 5 or 10 missing values, dropping columns in this case will lead to get rid of the most features of the dataset, which may carry very useful information about the prediction feature. In this situation, we might want to either drop the rows with missing values or we can fill up those missing values with mean or median or mode values of the column. Which leads to the next idea.","557f9bea":"There are many area ideas that we can use to deal with missing values in the dataset. In this article I will discuss four of the simplest ideas. To keep things simple I will avoid additional data cleaning or feature engineering in this article. In order to do that I will work with only numerical features and ignore all object features.\u00a0\n\nNow let's define a helper function to split the dataset and another function to compute the score using the dataset, so we don't need to write it again and again.","6590652e":"### Idea 1(b): Droping the columns with missing value\nAnother simple idea to deal with missing values is just drop the columns with missing values.\n\n![image.png](attachment:adf8ec3b-77ff-499e-b1e0-b933a5289694.png)\n\nHere I will apply the technique to drop the columns with missing values and since the goal here is just to explain how to deal with missing values, I will work with the columns with only numerical values for the sake of simplicity.","620f76d3":"We see the extension of the imputation approach performs slightly worse than the approach with simple imputation on this dataset.\u00a0\n\nOverall we get better performance by removing the rows with null values for this housing dataset and the result might be totally different for another dataset. There is no rule of thumb about which approach will work best on a dataset. The best idea would be to take try to understand the dataset and check which one works best.","6b0f70f9":"Here, I will explain few simple ideas to deal with missing values in tabular dataset. I will use Melbourne Housing Snapshot dataset from Kaggle. Let's take a look at the dataset. As we see YearBuilt, BuildingArea features has some NaN\/missing values.","c510342a":"### Idea2: Imputation\n**SimpleImputer:** Replace the missing values with the mean vlaue along each column. This simple imptation generally performs very well for most of the dataset. Eventhough there are complex techniques to find the imputed values (e.g regression imputation), they might not bring additional benifits once applying to the machine learning models.\n\n![image.png](attachment:59416180-2da5-4166-ba61-7fe3dcc2912e.png)\n\nLike idea-1, I will again use only numeric features. ","105040fa":"### Reference:\n\n[Missing Values Notebook](https:\/\/www.kaggle.com\/alexisbcook\/missing-values\/tutorial)","1aa56167":"### Idea-3: An Extension to Imputation\nNow I will apply imputation technique as before but this time I will keep track of what which values were imputed. Meaning, I will add one additional column for each column  with null values, which have False or True values just to keep track of where the value of original columns was null. \n\n![image.png](attachment:29e88eb7-97be-4d3f-87df-63d2429c5914.png)\n\nThe reason of doing this is, imputed values are not the exact values that the dataset is missing, could be larger or smaller than the actual value. Also there is a change that the rows with missing values are unique in some ways. \n\nUsing this imputation technique sometime helps the ML model to make better predictions."}}