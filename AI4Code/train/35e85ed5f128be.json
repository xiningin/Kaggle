{"cell_type":{"8f7a2585":"code","11e1cee7":"code","8022df32":"code","4b92a369":"code","fca3caaf":"code","d94ff116":"code","32b0e2dd":"code","658d8b25":"code","c890eed9":"code","ee2a6e33":"code","57eaabd2":"code","48ea0fd9":"code","b9f1346e":"code","c364fbd4":"code","9971f83f":"code","e2942144":"code","ce8baf66":"code","7406f1b5":"code","c74fc3be":"code","a02b11c6":"code","80bec184":"code","42ad0f72":"markdown","8a12bec8":"markdown","3aad9320":"markdown","6477c2c4":"markdown","3985c6b6":"markdown","0eeded5c":"markdown","4fb2cdce":"markdown","f9ccbb6a":"markdown","d0d70f19":"markdown","886bd7e8":"markdown","57be713a":"markdown","ff2d094a":"markdown","2dc837cd":"markdown","818345b2":"markdown","51c2694d":"markdown","3695055f":"markdown","ca6fb053":"markdown","7d8341a1":"markdown"},"source":{"8f7a2585":"#Importing the libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.offline as py\nimport plotly.graph_objs as go\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (accuracy_score, log_loss, confusion_matrix)\n#Suppressing warnings\nimport warnings\nwarnings.filterwarnings('ignore')","11e1cee7":"#Importing  the Dataset\nprint('Importing the CSV file.')\ndf = pd.read_csv('\/kaggle\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv')\nprint('File imported successfully!')\n\n#Datatypes in the dataset\nprint('Imported Dataframe Structure : \\n', df.dtypes.value_counts())","8022df32":"df.head(3)","4b92a369":"#Checking the number of 'Yes' and 'No' in 'Attrition'\nax = sns.catplot(x=\"Attrition\", kind=\"count\", palette=\"ch:.25\", data=df);\nax.set(xlabel = 'Attrition', ylabel = 'Number of Employees')\nplt.show()","fca3caaf":"#Identifying columns with missing information\nmissing_col = df.columns[df.isnull().any()].values\nprint('The missing columns in the dataset are: ',missing_col)","d94ff116":"#Extracting the Numeric and Categorical features\ndf_num = pd.DataFrame(data = df.select_dtypes(include = ['int64']))\ndf_cat = pd.DataFrame(data = df.select_dtypes(include = ['object']))\nprint(\"Shape of Numeric: \",df_num.shape)\nprint(\"Shape of Categorical: \",df_cat.shape)","32b0e2dd":"#Dropping 'Attrition' from df_cat before encoding\ndf_cat = df_cat.drop(['Attrition'], axis=1) \n\n#Encoding using Pandas' get_dummies\ndf_cat_encoded = pd.get_dummies(df_cat)\ndf_cat_encoded.head(5)","658d8b25":"#Using StandardScaler to scale the numeric features\nstandard_scaler = StandardScaler()\ndf_num_scaled = standard_scaler.fit_transform(df_num)\ndf_num_scaled = pd.DataFrame(data = df_num_scaled, columns = df_num.columns, index = df_num.index)\nprint(\"Shape of Numeric After Scaling: \",df_num_scaled.shape)\nprint(\"Shape of categorical after Encoding: \",df_cat_encoded.shape)","c890eed9":"#Combining the Categorical and Numeric features\ndf_transformed_final = pd.concat([df_num_scaled,df_cat_encoded], axis = 1)\nprint(\"Shape of final dataframe: \",df_transformed_final.shape)","ee2a6e33":"#Extracting the target variable - 'Attrition'\ntarget = df['Attrition']\n\n#Mapping 'Yes' to 1 and 'No' to 0\nmap = {'Yes':1, 'No':0}\ntarget = target.apply(lambda x: map[x])\n\nprint(\"Shape of target: \",target.shape)\n\n#Copying into commonly used fields for simplicity\nX = df_transformed_final #Features\ny = target #Target","57eaabd2":"#Splitting into Train and Test dataset in 90-10 ratio\nX_train, X_test, y_train, y_test = train_test_split(X,y,train_size = 0.8, random_state = 0, stratify = y)\nprint(\"Shape of X Train: \",X_train.shape)\nprint(\"Shape of X Test: \",X_test.shape)\nprint(\"Shape of y Train: \",y_train.shape)\nprint(\"Shape of y Test: \",y_test.shape)","48ea0fd9":"#Using Gradient Boosting to predict 'Attrition' and create the Trees to identify important features\ngbm = GradientBoostingClassifier(n_estimators = 200, max_features = 0.7, learning_rate = 0.3, max_depth = 5, random_state = 0, verbose = 0)\nprint('Training Gradient Boosting Model')\n\n#Fitting Model\ngbm.fit(X_train, y_train)\nprint('Model Fitting Completed')\n\n#Predicting\nprint('Starting Predictions!')\ny_pred = gbm.predict(X_test)\nprint('Prediction Completed!')","b9f1346e":"print('Accuracy of the model is:  ',accuracy_score(y_test, y_pred))","c364fbd4":"#Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nprint('The confusion Matrix : \\n',cm)","9971f83f":"# Scatter plot \ntrace = go.Scatter(\n    y = gbm.feature_importances_,\n    x = df_transformed_final.columns.values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 10,\n        color = gbm.feature_importances_,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = df_transformed_final.columns.values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Model Feature Importance',\n    hovermode= 'closest',\n     xaxis= dict(\n         ticklen= 5,\n         showgrid=False,\n        zeroline=False,\n        showline=False\n     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        showgrid=False,\n        zeroline=False,\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter')","e2942144":"#Setting Seaborn font-size\nsns.set(font_scale = 1)\n\n#Attrition based on Overtime\nax = sns.catplot(x=\"OverTime\", kind=\"count\",hue=\"Attrition\", palette=\"ch:.25\", data=df);\nax.set(xlabel = 'Overtime', ylabel = 'Number of Employees', title = 'Overtime')\nplt.show()","ce8baf66":"#Stock Option Level\nax = sns.catplot(x=\"StockOptionLevel\", kind=\"count\",hue=\"Attrition\", palette=\"ch:.25\", data=df);\nax.set(xlabel = 'Stock Option Level', ylabel = 'Number of Employees', title = 'Stock Option Level')\nplt.show()","7406f1b5":"#Job Satisfaction\nax = sns.catplot(x=\"JobSatisfaction\", kind=\"count\",hue=\"Attrition\", palette=\"ch:.25\", data=df);\nax.set(xlabel = 'Job Satisfaction', ylabel = 'Number of Employees', title = 'Job Satisfaction')\nplt.show()","c74fc3be":"#JobLevel\nax = sns.catplot(x=\"JobLevel\", kind=\"count\",hue=\"Attrition\", palette=\"ch:.25\", data=df);\nax.set(xlabel = 'Job Level', ylabel = 'Number of Employees', title = 'Job Level')\nplt.show()","a02b11c6":"#EnvironmentSatisfaction\nax = sns.catplot(x=\"EnvironmentSatisfaction\", kind=\"count\",hue=\"Attrition\", palette=\"ch:.25\", data=df);\nax.set(xlabel = 'Environment Satisfaction', ylabel = 'Number of Employees', title = 'Environment Satisfaction')\nplt.show()","80bec184":"#YearsWithCurrManager\nax = sns.catplot(x=\"TotalWorkingYears\", kind=\"count\",hue=\"Attrition\", palette=\"ch:.25\", data=df);\nax.set(xlabel = 'Total Working Years', ylabel = 'Number of Employees', title = 'Total Working Years')\nplt.show()","42ad0f72":"## Step 4 - Visualisation and Identification of Important Features\n\nHere, I have used the 'feature_importances_' array of the Gradient Boosting Model to ascertain the most important features for the prediction of 'Attrition'.\n\nFrom the plot below, we can clearly see that thet following features hold a lot of weightage:\n1. Overtime\n2. StockOptionLevel\n3. JobSatisfaction\n4. JobLevel\n5. EnvironmentSatisfaction\n6. TotalWorkingYears\n\nWe can next plot these individually alongside Attrition to better understand the importance of each.","8a12bec8":"As a next step, the Excel file has been imported using Pandas into the dataframe named 'df'. We can see that there are 26 numeric and 9 categorical fields.","3aad9320":"### 4.1 Overtime\n\nFrom below, we can clearly see that Attrition is a bigger percentage of employees who did Overtime.","6477c2c4":"### 2.1 Encoding Categorical Fields\n\nThe categorical fields have been encoded using the get_dummies() function of Pandas.","3985c6b6":"### 2.2 Scaling Numeric Fields\n\nThe numeric fields have been scaled next for best results. StandardScaler() has been used for the same. Post scaling of the numeric features, they are merged with the categorical ones.","0eeded5c":"### 4.4 Job Level\n\nWe can see that the employees at lower levels are more likely to leave the organisation.","4fb2cdce":"We can clearly see that the 'Attrition' field is highly skewed with just over 200 'Yes' out of a total of 1470 observations. This skewness can result in a prediction which is highly geared towards predicting 'No'.","f9ccbb6a":"### 2.3 Train and Test Split\n\nThe data is next split into training and test dataset using the train_test_split functionality of sklearn.","d0d70f19":"### 4.2 Stock Option Level\n\nFrom below, we can clearly see that Attrition is higher for employees who dont have stock options. Employees with stock options are less likely to leave the organisation.","886bd7e8":"## Step 2 - Feature Engineering\n\nThe numeric and categorical fields need to be treated separately and the target field needs to be separated from the training dataset. The following few steps separate the numeric and categorical fields and drops the target field 'Attrition' from the feature set.","57be713a":"### 4.6 Years With Current Manager\n\nHere we can see that employees with low number of years with current manager are more likely to leave. Attrition is high when the employee has spent not even 1 year with the current manager.","ff2d094a":"Next I check if there are any missing values in the dataframe. As can be seen from the output, there are no missing values in the dataset.","2dc837cd":"### 4.5 Environment Satisfaction\n\nWhile the number of Attrition is similar for all groupings, for Environment Satisfaction of level 1 and 2, Attrition as percentage of employees in those groups is higher than the others. So, employees experiencing lower satisfaction are more likely to leave.","818345b2":"# Attrition Analysis\n\nThe dataset contains a total of 35 fields and 1470 observations. This analysis is based on the 'Attrition' field. This notebook attempts to identify some key features which can help explain 'Attrition' in the organisation. Gradient Boosting has been used to identify the important features to predict 'Attrition'.\n\n### Key Steps\n\nThe following key steps have been followed:\n1. Exploratory Data Analysis\n2. Feature Engineering\n3. Model Fitting\n4. Visualisation and Identification of Important Features\n\n## Step 1 - Exploratory Data Analysis (EDA)\n\nThis includes looking at the distribution of the target variable as well as identifying any missing values among the features.\n\nFirst, all the required libraries are imported.","51c2694d":"We can obtain the confusion matrix to see how the model has performed. From the confusion matrix, we can see that the model has predicted 125 observations correctly and 22 observations incorrectly.","3695055f":"## Step 3 - Model Fitting\n\nHere, I am using the Gradient Boosting model for the decision trees to identify the importance of the features. 'Attrition' is the target variable.","ca6fb053":"### 4.3 Job Satisfaction\n\nWhile the number of Attrition is similar for all groupings, for Job Satisfaction of level 1, Attrition as percentage of employees in that group is higher than the others. So, employees experiencing lower satisfaction are more likely to leave.","7d8341a1":"Using the head(), we can see a small snapshot of the data. The target field 'Attrition' is a categorical field with 'Yes' and 'No'."}}