{"cell_type":{"c383d82e":"code","6f3b7c17":"code","8a351fbb":"code","f7d6f0af":"code","a30630cd":"code","e4a66f8c":"code","f5321a3e":"code","869bad8f":"code","e7ebacb4":"code","02976322":"code","dd3209c6":"code","88fe7d17":"code","88d3cb3f":"code","68c91f1c":"code","524a60c7":"code","23bba284":"code","2b3d7954":"code","7eb16217":"code","f6312b4c":"code","661daac0":"code","109356f6":"code","206afbaf":"code","f89b6cb2":"code","128e13c1":"code","6613c15d":"code","ada4b92f":"code","057dd175":"code","d1525dd6":"code","1ad91b4e":"code","26216662":"code","cb2b7959":"code","f67b5430":"code","7aecf7b2":"code","0b6fefb2":"code","416d6f32":"code","1eb3a9a6":"code","b346ef85":"code","feb58c33":"code","4073bf04":"code","12af8f7b":"code","88e454c5":"code","1a4e1ad6":"code","0e655020":"code","184aa98b":"code","21c7a957":"code","c1ec96bd":"code","26fcca63":"code","3315818a":"code","1efa0d0c":"code","4e41fa4d":"code","98e5b095":"code","62f42528":"code","87b2a779":"code","96f47475":"code","810becdd":"code","833aea23":"code","ee9c37a6":"code","88102b0f":"code","4a1bcce5":"code","ccc00f31":"code","bb7c00a1":"code","703f92a0":"code","b95f27fd":"code","ced2126f":"code","ebf683d7":"code","b8beab33":"markdown","f35a3b64":"markdown","d80f22e3":"markdown","e90d7e0e":"markdown","9a260cf0":"markdown","727d0d53":"markdown","c522a485":"markdown","57d1c3d4":"markdown","cbee0a2f":"markdown","d102cc1a":"markdown","9782f49a":"markdown","5b678b5e":"markdown","fa8150d0":"markdown","17c17a0a":"markdown","72dee81f":"markdown","60e0ace9":"markdown","e8fe1677":"markdown"},"source":{"c383d82e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport itertools # use chain method to efficiently flatten list of lists \nimport matplotlib.pyplot as plt\nimport warnings\n\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6f3b7c17":"# header=None --> there are no headers in data\ncolnames=['user_id','game_title', 'behavior_name','value','temp'] \nusers = pd.read_csv(\"\/kaggle\/input\/steam-video-games\/steam-200k.csv\",header=None,names=colnames,usecols=['user_id','game_title', 'behavior_name','value'])","8a351fbb":"pd.set_option('display.max_rows', 500)\nusers[users.user_id==151603712]","f7d6f0af":"users.isnull().values.any()","a30630cd":"users.info()","e4a66f8c":"users.user_id.nunique()","f5321a3e":"users.game_title.nunique()","869bad8f":"users.drop_duplicates(subset=['user_id','game_title','behavior_name'], keep='first', inplace=True)","e7ebacb4":"users.info()","02976322":"print (\"number of rows we have droped\",200000-199281)\nprint (\"number of games\",users.game_title.nunique(),\"number of users\",users.user_id.nunique())","dd3209c6":"# user statistics:\nuser_games_num = users.game_title.nunique()\nuser_play_hrs = users[users.behavior_name=='play'].value.sum()\nuser_play_avg = users[users.behavior_name=='play'].value.mean()\nuser_play_median = users[users.behavior_name=='play'].value.median()\nuser_purchase_num = users[users.behavior_name=='purchase'].value.sum()\nuser_purchase_avg = user_purchase_num \/ user_games_num\n\nprint('number of games played:',user_games_num)\nprint('number of total hours played: %.2f'%(user_play_hrs))\nprint('number of avg hours played: %.2f'%(user_play_avg))\nprint('number of median hours played: %.2f'%(user_play_median))\nprint('number of purchases: %.0f'%(user_purchase_num))\nprint('number of purchases per game: %.0f'%(user_purchase_avg))","88fe7d17":"# every game involves a purchase event\n\nx = users.groupby(['user_id','behavior_name']).count()\n\n","88d3cb3f":"# aggregate games by total play hrs \/ mean play hrs \/ median play hrs\n\ngame_stats = users[users.behavior_name == 'play'].groupby(['game_title']).agg({'value':[np.sum,np.mean,np.median]})\n\ngame_stats.columns = ['_'.join(col).strip() for col in game_stats.columns.values] # flatten hierarchy in column multi-index","68c91f1c":"# top 15 games in total play hrs\ngame_stats.value_sum.sort_values(ascending=False).head(15)","524a60c7":"# top 15 games in avg play time\ngame_stats.value_mean.sort_values(ascending=False).head(15)","23bba284":"# top 15 games in median play time\ngame_stats.value_median.sort_values(ascending=False).head(15)","2b3d7954":"# consider top 10 games from each statistic\n\na = [game_stats[col].sort_values(ascending=False).head(10).index.tolist() for col in game_stats.columns.values]\n\ntop_tens = list(set(itertools.chain.from_iterable(a))) # join top 10 games by each statistic \n\ntop_tens.sort(key=str.casefold) # sort alphabetically","7eb16217":"total_playtime = game_stats.loc[top_tens,'value_sum']\nmean_playtime = game_stats.loc[top_tens,'value_mean']\nmedian_playtime = game_stats.loc[top_tens,'value_median']","f6312b4c":"# plot top 10 games for each statistic\n\nplt.figure(figsize=(15,15),facecolor=\"w\")\nplt.xticks(rotation='vertical')\nax1 = plt.subplot(3,1,1)\nax2 = plt.subplot(3,1,2,sharex=ax1)\nax3 = plt.subplot(3,1,3,sharex=ax1)\nax1.plot(total_playtime,color='g')\nax2.plot(mean_playtime,color='r')\nax3.plot(median_playtime,color='b')\nax1.set_ylabel('total playtime')\nax2.set_ylabel('avg playtime')\nax3.set_ylabel('median playtime')\n# ax2.set_xlabel(\"x Axis\")\nplt.setp(ax1.get_xticklabels(),visible=False)\nplt.setp(ax2.get_xticklabels(),visible=False)\nplt.setp(ax3.xaxis.get_majorticklabels(), rotation=90 )\n\nplt.tight_layout()\nplt.show()\n\n# NOTE: we have two clear outliers we might want to treat seperately","661daac0":"ax = users[users.behavior_name==\"purchase\"].groupby(['user_id']).count().sort_values(by='value',  axis=0, ascending=False).value.plot.hist(bins=100,figsize=(18,9))\n# plt.xlim(2, 200)\nax.set(yscale ='log')\n# users.groupby(['user_id']).count().sort_values(self, by, axis=0, ascending=True, inpla)","109356f6":"ax=users[users.behavior_name==\"play\"].groupby(['user_id']).sum().sort_values(by='value',  axis=0, ascending=False).value.plot.hist(bins=150,figsize=(18,9))\n# plt.xlim(2, 2000)\nax.set(yscale ='log')","206afbaf":"top_games = users.groupby(['game_title'])['user_id'].count().sort_values(ascending=False)\ntop_games = top_games\/2","f89b6cb2":"# need to devied by 2 since we have 2 rows for each game we should filter only purchase\n\nax = top_games[0:15].plot.bar(figsize=(14,7))\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.001, p.get_height() * 1.02))\n","128e13c1":"users = users[users.game_title.isin(top_games[0:10].index)]","6613c15d":"users.shape","ada4b92f":"users.user_id.nunique()","057dd175":"users.game_title.nunique()","d1525dd6":"from pandas.api.types import is_numeric_dtype\ndef remove_outlier(df):\n    low = .05\n    high = .95\n    quant_df = df.quantile([low, high])\n    for name in list(df.columns):\n        if is_numeric_dtype(df[name]):\n            df = df[ (df[name] < quant_df.loc[high, name])]\n    return df\n\n","1ad91b4e":"#now we need to check the types of the variables in the users\nusers.info()","26216662":"# we need to change the type of the user_id to be string\nusers.user_id=users.user_id.astype('str')","cb2b7959":"#now we need to check the types of the variables in the users\nusers.info()","f67b5430":"users = remove_outlier(users)   ","7aecf7b2":"users.shape","0b6fefb2":"from functools import reduce\ndf = {}\n\n# top 10 games in numbers of users\ngames = top_games[0:10].index\n\nfor i, game in enumerate(games):\n    df_purchase = users[(users.game_title == game) & (users.behavior_name == 'purchase')][['user_id','value']]\n    df_play = users[(users.game_title == game) & (users.behavior_name == 'play')][['user_id','value']]\n    df_game = pd.merge(left = df_purchase, right = df_play, how = 'left', on = ['user_id'], suffixes= ('_purchase','_playtime'))\n    df_game.rename(columns={'value_purchase':'purchase_'+str(i),'value_playtime':'playtime_'+str(i)},inplace=True)\n    df[game] = df_game\n\ndf_final = reduce(lambda left,right: pd.merge(left,right,how='outer',on=['user_id'],suffixes=('',''),sort=False), df.values())\n\ndf_final.fillna(value=0,inplace=True)\n","416d6f32":"df_final.head()","1eb3a9a6":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\n\ndf_final_scale = scaler.fit_transform(df_final)\n\ndf_final_scale = pd.DataFrame(df_final_scale,columns = df_final.columns)\n","b346ef85":"df_final_scale","feb58c33":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn.cluster as cluster\nfrom sklearn.cluster import KMeans\nimport time\nfrom pandas.plotting import scatter_matrix\n%matplotlib inline\nsns.set_context('talk')\nsns.set_color_codes()\nplot_kwds = {'alpha' : 0.25, 's' : 80, 'linewidths':0}","4073bf04":"k = 5\nmodel = KMeans(n_clusters=k, \n               max_iter=10, random_state=1, \n               init='k-means++', n_init=10)\ndf_final_scale['cluster'] = pd.Series(model.fit_predict(df_final_scale))","12af8f7b":"model.labels_","88e454c5":"model.cluster_centers_","1a4e1ad6":"def calc_inertia(k):\n    model = KMeans(n_clusters=k).fit(df_final_scale)\n    return model.inertia_\n\ninertias = [(k, calc_inertia(k)) for k in range(1, 10)]","0e655020":"plt.figure(figsize=(12, 8))\nplt.plot(*zip(*inertias))\nplt.title('Inertia vs. k')\nplt.xlabel('k')\nplt.ylabel('Inertia')\nplt.grid(True)\nplt.text(0.5, 0.120, '*K=5 looks promissing', horizontalalignment='center',verticalalignment='center',color='blue', transform=ax.transAxes)\nplt.text(0.25, 0.4, '*K=2 is too small', horizontalalignment='center',verticalalignment='center',color='red', transform=ax.transAxes)","184aa98b":"# data[['user_id','value_y','cluster']]\n# scatter = scatter_matrix(data[['user_id','value_y','cluster']] ,figsize=(15, 10), s=22 ,alpha=1)\nif 'cluster' in df_final_scale.columns:\n    df_final_scale.drop(['cluster'], axis=1,inplace=True)\nk = 5\nmodel = KMeans(n_clusters=k, \n               max_iter=10, random_state=1, \n               init='k-means++', n_init=10)\ndf_final_scale['cluster'] = pd.Series(model.fit_predict(df_final_scale))\n\n\n","21c7a957":"df_final_scale.cluster.value_counts()","c1ec96bd":"sns.set_context('paper')\nax = df_final_scale.cluster.value_counts().plot.bar(figsize=(14,7))\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.001, p.get_height() * 1.02))","26fcca63":"sns.set_context('paper')\ndata = df_final_scale[['cluster','purchase_0','purchase_1','playtime_0','playtime_1']]\nscatter = scatter_matrix(data ,figsize=(10, 8), s=22 ,alpha=.7)\nsns.set_context('talk')","3315818a":"df_final_scale","1efa0d0c":"for i in df_final_scale.cluster.unique():\n    print(\"this is the statistics for cluster \",i)\n    print(df_final_scale[df_final_scale.cluster==i].describe())","4e41fa4d":"pip install hdbscan","98e5b095":"import hdbscan","62f42528":"if 'cluster' in df_final_scale.columns:\n    df_final_scale.drop(['cluster'], axis=1,inplace=True)\nclusterer = hdbscan.HDBSCAN(min_cluster_size=200)\ncluster_labels = clusterer.fit_predict(df_final_scale)","87b2a779":"cluster_labels ","96f47475":"df_final_scale['cluster'] = cluster_labels","810becdd":"df_final_scale","833aea23":"df_final_scale.cluster.value_counts()","ee9c37a6":"sns.set_context('paper')\nax = df_final_scale.cluster.value_counts().plot.bar(figsize=(14,7))\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.001, p.get_height() * 1.02))","88102b0f":"df_final_scale[df_final_scale.cluster==5]","4a1bcce5":"df_final_scale[df_final_scale.cluster==5].describe()","ccc00f31":"for i in df_final_scale.cluster.unique():\n    print(\"this is the statistics for cluster \",i)\n    print(df_final_scale[df_final_scale.cluster==i].describe())","bb7c00a1":"from scipy.cluster.hierarchy import linkage, dendrogram, fcluster","703f92a0":"if 'cluster' in df_final_scale.columns:\n    df_final_scale.drop(['cluster'], axis=1,inplace=True)\nZ = linkage(df_final_scale, method='ward', metric='euclidean')","b95f27fd":"dn = dendrogram(Z)","ced2126f":"df_final_scale['cluster'] = fcluster(Z, 5, criterion='maxclust')\ndf_final_scale.plot('playtime_0', 'playtime_1', kind='scatter', c=df_final_scale['cluster'], s=100)","ebf683d7":"for i in df_final_scale.cluster.unique():\n    print(\"this is the statistics for cluster \",i)\n    print(df_final_scale[df_final_scale.cluster==i].describe())","b8beab33":"### we see that we have deleted 719 lines no users or games are missing","f35a3b64":"### let's check if there are Null values\n\n","d80f22e3":"## Hdbscan Clustering","e90d7e0e":"## Handling Duplicated values since we asume we should have only two rows max per user per game one for Purchase and one for Play\n### there might be duplicates purchase so in this case we will keep only one and duplicated playtimes and in this case we have decieded also to keep only the first\n> Play is the SUM of all times this user playes this game","9a260cf0":"> ### Building the DF (Users) for clustering we need to have one row per user per game that will include the purchase and the play time in one row","727d0d53":"### using this technique will allow us to cluster only 8810 out of 12393, There are ~4k users we will not be able to cluster we will deal with it later","c522a485":"> ### Filter data for rows with top 10 games only","57d1c3d4":"## First we load the important libraries","cbee0a2f":"## Clustering set up","d102cc1a":"### Lets check how many users we have in the DF","9782f49a":"### Extract top  games in terms of users.","5b678b5e":"## Reading the data naming the columns and dropping the columns without data ","fa8150d0":"### No Nulls","17c17a0a":"### as expected we have 10 games \n\n### Now we need to remove outliers\nI will create a small program that will handle only the numbers and will remove only high values","72dee81f":"### Lets check how many games we have in the DF","60e0ace9":"## we need to scale since the values of the value might be very high","e8fe1677":"## Agglomerative Clustering"}}