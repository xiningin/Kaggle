{"cell_type":{"e0bbd5d3":"code","6cb71818":"code","976a8770":"code","fd3c1e69":"code","42bfdf9c":"code","38fcbe66":"code","00e78c73":"code","503dd3d9":"code","87b9ea66":"code","4c87e797":"code","fd9685f2":"code","fd16329b":"code","beec977b":"code","3cb54157":"code","eab7e5e1":"code","050be36d":"code","18011daf":"code","4184c0f9":"markdown"},"source":{"e0bbd5d3":"import matplotlib.pylab as plt\nimport os\nimport math\nimport random\nimport time\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom transformers import get_cosine_schedule_with_warmup\nfrom transformers import get_linear_schedule_with_warmup\n\nfrom transformers import AdamW\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\nfrom transformers import AutoConfig\nfrom transformers import get_cosine_schedule_with_warmup\n\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nimport gc\ngc.enable()","6cb71818":"submission = pd.read_csv(\"..\/input\/muis-challenge\/submission.csv\")\n\nNUM_FOLDS = 5\nNUM_EPOCHS = 3\nBATCH_SIZE = 8\nMAX_LEN = 128\n\nROBERTA_PATH = \"..\/input\/num-challenge-xmlroberta-fine-tuned\/XLMRobertaModel\"\nTOKENIZER_PATH = \"..\/input\/num-challenge-xmlroberta-fine-tuned\/XLMRobertaModel\"\n\nROBERTA_PATH_Trans = '..\/input\/num-challenge-l-fine-tuned-transformer-mode\/transformer'\nTOKENIZER_PATH_Trans = '..\/input\/num-challenge-l-fine-tuned-transformer-mode\/transformer'\n\nROBERTA_PATH_LARGE = \"..\/input\/num-challenge-xlmroberta-large-fine\/XLMRobertaLargeModel\"\nTOKENIZER_PATH_LARGE = \"..\/input\/num-challenge-xlmroberta-large-fine\/XLMRobertaLargeModel\"","976a8770":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"","fd3c1e69":"def set_random_seed(random_seed):\n    random.seed(random_seed)\n    np.random.seed(random_seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n\n    torch.manual_seed(random_seed)\n    torch.cuda.manual_seed(random_seed)\n    torch.cuda.manual_seed_all(random_seed)\n\n    torch.backends.cudnn.deterministic = True\n","42bfdf9c":"class NumDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length_c,  max_length_w, inference_only=False):\n        super().__init__()\n\n        self.df = df\n        self.inference_only = inference_only\n        self.text = df.clean_text.tolist()\n        self.mask_word = df.mask_word.tolist()\n\n        if not self.inference_only:\n            self.target = torch.tensor(df.synset_id.values, dtype=torch.int32)\n\n        self.encoded = tokenizer.batch_encode_plus(\n            self.text,\n            padding='max_length',\n            max_length=max_length_c,\n            truncation=True,\n            return_attention_mask=True\n        )\n\n        self.encoded_word = tokenizer.batch_encode_plus(\n            self.mask_word,\n            padding='max_length',\n            max_length=max_length_w,\n            truncation=True,\n            return_attention_mask=True\n        )\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        input_ids = torch.tensor(self.encoded['input_ids'][index])\n        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n\n        input_ids_word = torch.tensor(self.encoded_word['input_ids'][index])\n        attention_mask_word = torch.tensor(self.encoded_word['attention_mask'][index])\n\n        if self.inference_only:\n            return (input_ids, attention_mask, input_ids_word, attention_mask_word)\n        else:\n            target = self.target[index]\n            return (input_ids, attention_mask, input_ids_word, attention_mask_word, target)","38fcbe66":"def predict(models0, data_loader0, models1, data_loader1, models2, data_loader2, models3, data_loader3):\n    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n\n    preds = torch.zeros((test.shape[0], 69)).to(DEVICE)\n    for model in models0:\n        model.eval()\n        res = []\n        with torch.no_grad():\n            for batch_num, (input_ids, attention_mask, input_ids_word,\n                            attention_mask_word) in enumerate(tqdm(data_loader0)):\n\n                input_ids = input_ids.to(DEVICE)\n                attention_mask = attention_mask.to(DEVICE)\n                input_ids_word = input_ids_word.to(DEVICE)\n                attention_mask_word = attention_mask_word.to(DEVICE)\n\n                pred = model(input_ids, attention_mask,\n                             input_ids_word,\n                             attention_mask_word\n                             )\n                res.append(pred)\n\n        preds = preds + torch.tensor(0.06) * torch.cat(res, dim=0).softmax(dim=1)\n        \n        \n    del models0, data_loader0\n    gc.collect()\n        \n    for model in models1:\n        model.eval()\n        res = []\n        with torch.no_grad():\n            for batch_num, (input_ids, attention_mask, input_ids_word,\n                            attention_mask_word) in enumerate(tqdm(data_loader1)):\n\n                input_ids = input_ids.to(DEVICE)\n                attention_mask = attention_mask.to(DEVICE)\n                input_ids_word = input_ids_word.to(DEVICE)\n                attention_mask_word = attention_mask_word.to(DEVICE)\n\n                pred = model(input_ids, attention_mask,\n                             input_ids_word,\n                             attention_mask_word\n                             )\n                res.append(pred)\n\n        preds = preds + torch.tensor(0.30) * torch.cat(res, dim=0).softmax(dim=1)\n    \n    del models1, data_loader1, res, pred\n    gc.collect()\n    \n   \n    for model in models2:\n        model.eval()\n        res = []\n        with torch.no_grad():\n            for batch_num, (input_ids, attention_mask, input_ids_word,\n                            attention_mask_word) in enumerate(tqdm(data_loader2)):\n\n                input_ids = input_ids.to(DEVICE)\n                attention_mask = attention_mask.to(DEVICE)\n                input_ids_word = input_ids_word.to(DEVICE)\n                attention_mask_word = attention_mask_word.to(DEVICE)\n\n                pred = model(input_ids, attention_mask,\n                             input_ids_word,\n                             attention_mask_word\n                             )\n                res.append(pred)\n\n        preds = preds + torch.tensor(0.30) * torch.cat(res, dim=0).softmax(dim=1)\n\n    del models2, data_loader2, res, pred\n    gc.collect()\n    \n        \n    for model in models3:\n        model.eval()\n        res = []\n        with torch.no_grad():\n            for batch_num, (input_ids, attention_mask, input_ids_word,\n                            attention_mask_word) in enumerate(tqdm(data_loader3)):\n\n                input_ids = input_ids.to(DEVICE)\n                attention_mask = attention_mask.to(DEVICE)\n                input_ids_word = input_ids_word.to(DEVICE)\n                attention_mask_word = attention_mask_word.to(DEVICE)\n\n                pred = model(input_ids, attention_mask,\n                             input_ids_word,\n                             attention_mask_word\n                             )\n                res.append(pred)\n\n        preds = preds + torch.tensor(0.10) * torch.cat(res, dim=0).softmax(dim=1)\n\n    del models3, data_loader3, res, pred\n    gc.collect()\n   \n    \n    pred_prob, predicted = torch.max(preds.data, 1)\n\n    return predicted.detach().cpu().numpy(), pred_prob.detach().cpu().numpy()","00e78c73":"class NumModel(nn.Module):\n    def __init__(self, path, hidden_size):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(path)\n        config.update({\"output_hidden_states\": True,\n                       \"hidden_dropout_prob\": 0.1,\n                       \"layer_norm_eps\": 1e-7})\n\n        self.roberta = AutoModel.from_pretrained(path, config=config)\n\n        self.attention_c = nn.Sequential(\n            nn.Linear(hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n\n        self.attention_w = nn.Sequential(\n            nn.Linear(hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n\n        self.layer_norm_c = nn.LayerNorm(hidden_size)\n        self.layer_norm_w = nn.LayerNorm(hidden_size)\n\n        self.dropout = nn.Dropout(0.10)\n        self.regressor = nn.Sequential(\n            nn.Linear(hidden_size*2, 69)\n            #nn.LeakyReLU(),\n            #nn.Linear(512, 69)\n        )\n\n        #init_params([self.attention_w, self.attention_c, self.regressor])\n\n    def forward(self, input_ids, attention_mask, input_ids_word, attention_mask_word):\n\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)\n\n        last_layer_hidden_states = self.layer_norm_c(roberta_output.hidden_states[-1])\n\n        weights = self.attention_c(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)\n\n        roberta_output_word = self.roberta(input_ids=input_ids_word,\n                                      attention_mask=attention_mask_word)\n\n        last_layer_hidden_states_word = self.layer_norm_w(roberta_output_word.hidden_states[-1])\n        weights = self.attention_w(last_layer_hidden_states_word)\n        word_vector = torch.sum(weights * last_layer_hidden_states_word, dim=1)\n\n        out = torch.cat([context_vector, word_vector], dim=1)\n        out = self.regressor(self.dropout(out))\n\n        return out","503dd3d9":"class NumModel_v2(nn.Module):\n    def __init__(self, path, hidden_size):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(path)\n        config.update({\"output_hidden_states\": True,\n                       \"hidden_dropout_prob\": 0.1,\n                       \"layer_norm_eps\": 1e-7})\n\n        self.roberta = AutoModel.from_pretrained(path, config=config)\n\n        self.attention_c = nn.Sequential(\n            nn.Linear(hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n\n        self.attention_w = nn.Sequential(\n            nn.Linear(hidden_size, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n\n        self.layer_norm_c = nn.LayerNorm(hidden_size)\n        self.layer_norm_w = nn.LayerNorm(hidden_size)\n\n        #self.dropout = nn.Dropout(0.20)\n        self.regressor = nn.Sequential(\n            nn.Linear(hidden_size*2, 69)\n            #nn.LeakyReLU(),\n            #nn.Linear(512, 69)\n        )\n\n        #init_params([self.attention_w, self.attention_c, self.regressor])\n\n    def forward(self, input_ids, attention_mask, input_ids_word, attention_mask_word):\n\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)\n\n        last_layer_hidden_states = self.layer_norm_c(roberta_output.hidden_states[-1])\n\n        weights = self.attention_c(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)\n\n        roberta_output_word = self.roberta(input_ids=input_ids_word,\n                                      attention_mask=attention_mask_word)\n\n        last_layer_hidden_states_word = self.layer_norm_w(roberta_output_word.hidden_states[-1])\n        weights = self.attention_w(last_layer_hidden_states_word)\n        word_vector = torch.sum(weights * last_layer_hidden_states_word, dim=1)\n\n        out = torch.cat([context_vector, word_vector], dim=1)\n        out = self.regressor(out)\n\n        return out","87b9ea66":"class NumModel_B(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH_B)\n        config.update({\"output_hidden_states\": True,\n                       \"hidden_dropout_prob\": 0.1,\n                       \"layer_norm_eps\": 1e-7})\n\n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH_B, config=config)\n\n        self.attention_c = nn.Sequential(\n            nn.Linear(768, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n\n        self.attention_w = nn.Sequential(\n            nn.Linear(768, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n\n        self.layer_norm_c = nn.LayerNorm(768)\n        self.layer_norm_w = nn.LayerNorm(768)\n\n        self.dropout = nn.Dropout(0.10)\n        self.regressor = nn.Sequential(\n            nn.Linear(768*2, 69)\n            #nn.LeakyReLU(),\n            #nn.Linear(512, 69)\n        )\n\n        #init_params([self.attention_w, self.attention_c, self.regressor])\n\n    def forward(self, input_ids, attention_mask, input_ids_word, attention_mask_word):\n\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)\n\n        last_layer_hidden_states = self.layer_norm_c(roberta_output.hidden_states[-1])\n\n        weights = self.attention_c(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)\n\n        roberta_output_word = self.roberta(input_ids=input_ids_word,\n                                      attention_mask=attention_mask_word)\n\n        last_layer_hidden_states_word = self.layer_norm_w(roberta_output_word.hidden_states[-1])\n        weights = self.attention_w(last_layer_hidden_states_word)\n        word_vector = torch.sum(weights * last_layer_hidden_states_word, dim=1)\n\n        out = torch.cat([context_vector, word_vector], dim=1)\n        out = self.regressor(self.dropout(out))\n\n        return out\n\n","4c87e797":"class NumModel_v3(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH_LARGE)\n        config.update({\"output_hidden_states\": True,\n                       \"hidden_dropout_prob\": 0.1,\n                       \"layer_norm_eps\": 1e-7})\n\n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH_LARGE, config=config)\n\n        self.attention_c = nn.Sequential(\n            nn.Linear(1024, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n\n        self.attention_w = nn.Sequential(\n            nn.Linear(1024, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n\n        self.layer_norm_c = nn.LayerNorm(1024)\n        self.layer_norm_w = nn.LayerNorm(1024)\n\n        self.dropout = nn.Dropout(0.15)\n        self.regressor = nn.Sequential(\n            nn.Linear(1024*2, 69)\n            #nn.LeakyReLU(),\n            #nn.Linear(512, 69)\n        )\n\n        #init_params([self.attention_w, self.attention_c, self.regressor])\n\n    def forward(self, input_ids, attention_mask, input_ids_word, attention_mask_word):\n\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)\n\n        last_layer_hidden_states = self.layer_norm_c(roberta_output.hidden_states[-1])\n\n        weights = self.attention_c(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)\n\n        roberta_output_word = self.roberta(input_ids=input_ids_word,\n                                      attention_mask=attention_mask_word)\n\n        last_layer_hidden_states_word = self.layer_norm_w(roberta_output_word.hidden_states[-1])\n        weights = self.attention_w(last_layer_hidden_states_word)\n        word_vector = torch.sum(weights * last_layer_hidden_states_word, dim=1)\n\n        out = torch.cat([context_vector, word_vector], dim=1)\n        out = self.regressor(self.dropout(out))\n\n        return out","fd9685f2":"class NumModel_B2(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH_B)\n        config.update({\"output_hidden_states\": True,\n                       \"hidden_dropout_prob\": 0.1,\n                       \"layer_norm_eps\": 1e-7})\n\n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH_B, config=config)\n\n        self.attention_c = nn.Sequential(\n            nn.Linear(768, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n\n        self.attention_w = nn.Sequential(\n            nn.Linear(768, 512),\n            nn.Tanh(),\n            nn.Linear(512, 1),\n            nn.Softmax(dim=1)\n        )\n\n        self.layer_norm_c = nn.LayerNorm(768)\n        self.layer_norm_w = nn.LayerNorm(768)\n\n        self.dropout = nn.Dropout(0.10)\n        self.regressor = nn.Sequential(\n            nn.Linear(768*2, 69)\n            #nn.LeakyReLU(),\n            #nn.Linear(512, 69)\n        )\n\n        #init_params([self.attention_w, self.attention_c, self.regressor])\n\n    def forward(self, input_ids, attention_mask, input_ids_word, attention_mask_word):\n\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)\n\n        last_layer_hidden_states = self.layer_norm_c(roberta_output.hidden_states[-1])\n\n        weights = self.attention_c(last_layer_hidden_states)\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)\n\n        roberta_output_word = self.roberta(input_ids=input_ids_word,\n                                      attention_mask=attention_mask_word)\n\n        last_layer_hidden_states_word = self.layer_norm_w(roberta_output_word.hidden_states[-1])\n        weights = self.attention_w(last_layer_hidden_states_word)\n        word_vector = torch.sum(weights * last_layer_hidden_states_word, dim=1)\n\n        out = torch.cat([context_vector, word_vector], dim=1)\n        out = self.regressor(self.dropout(out))\n\n        return out","fd16329b":"models0 = []\nmodels1 = []\nmodels2 = []\nmodels3 = []\n \nfor fold in [0, 1, 2, 3, 4]:\n    print(f\"\\nFold {fold + 1}\/5\")\n    model_path2 = f\"..\/input\/num-challenge-xml-roberta-base-model\/model_{fold+1}.pth\"\n    \n    model = NumModel(TOKENIZER_PATH, 768).to(DEVICE)\n\n    model.load_state_dict(torch.load(model_path2))\n    model.to(DEVICE)\n    model.eval()\n\n    models0.append(model)\n    \n    del model\n    gc.collect()\n    \n\nfor fold in [0]:\n    \n    print(f\"\\nFold {fold + 1}\/5\")\n    model_path1 = \"..\/input\/d\/lhagiimn\/num-challenge-xlm-roberta-base-v2\/model_1.pth\"\n    model = NumModel_v2(TOKENIZER_PATH_LARGE, 1024).to(DEVICE)\n\n    model.load_state_dict(torch.load(model_path1))\n    model.to(DEVICE)\n    model.eval()\n\n    models1.append(model)\n    \n    del model\n    gc.collect()\n\n\n\nfor fold in [0]:\n\n    print(f\"\\nFold {fold + 1}\/5\")\n    model_path0 = \"..\/input\/num-challenge-transformer-model\/model_1.pth\"\n    \n    model = NumModel(TOKENIZER_PATH_Trans, 1280).to(DEVICE)\n\n    model.load_state_dict(torch.load(model_path0))\n    model.to(DEVICE)\n    model.eval()\n\n    models2.append(model)\n    \n    del model\n    gc.collect()\n\nfor fold in [0]:\n\n    print(f\"\\nFold {fold + 1}\/5\")\n    model_path0 = \"..\/input\/d\/lhagiimn\/num-challenge-xlm-roberta-base-v2\/model_2.pth\"\n    \n    model = NumModel_v2(TOKENIZER_PATH, 768).to(DEVICE)\n\n    model.load_state_dict(torch.load(model_path0))\n    model.to(DEVICE)\n    model.eval()\n\n    models3.append(model)\n    \n    del model\n    gc.collect()\n    \n'''\nfor fold in [1, 2, 4]:\n\n    print(f\"\\nFold {fold + 1}\/5\")\n    model_path0 = f\"..\/input\/num-challenge-xml-roberta-base-model\/model_{fold + 1}.pth\"\n    \n    model = NumModel().to(DEVICE)\n\n    model.load_state_dict(torch.load(model_path0))\n    model.to(DEVICE)\n    model.eval()\n\n    models0.append(model)\n    \n    del model\n    gc.collect() \n'''","beec977b":"tokenizer_large = AutoTokenizer.from_pretrained(TOKENIZER_PATH_LARGE)\ntokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\ntokenizer_trans = AutoTokenizer.from_pretrained(TOKENIZER_PATH_Trans)\n\ntest = pd.read_csv('..\/input\/num-challenge-xml-roberta-base-model\/test.csv')\ntest['mask_word'] = test['mask_word'].fillna(\"\u0443\u0442\u0433\u0430\u0433\u04af\u0439 \u0445\u043e\u043e\u0441\u043e\u043d\")\ntest_dataset = NumDataset(test, tokenizer, max_length_c=128,  max_length_w=96, inference_only=True)\ntest_loader0 = DataLoader(test_dataset, batch_size=16, drop_last=False, shuffle=False)\n\ntest = pd.read_csv('..\/input\/d\/lhagiimn\/num-challenge-xlm-roberta-base-v2\/test.csv')\ntest['mask_word'] = test['mask_word'].fillna(\"\u0443\u0442\u0433\u0430\u0433\u04af\u0439 \u0445\u043e\u043e\u0441\u043e\u043d\")\ntest_dataset = NumDataset(test, tokenizer_large, max_length_c=128,  max_length_w=96, inference_only=True)\ntest_loader1 = DataLoader(test_dataset, batch_size=16,drop_last=False, shuffle=False)\n\n\ntest = pd.read_csv('..\/input\/num-challenge-transformer-model\/test.csv')\ntest['mask_word'] = test['mask_word'].fillna(\"\u0443\u0442\u0433\u0430\u0433\u04af\u0439 \u0445\u043e\u043e\u0441\u043e\u043d\")\ntest_dataset = NumDataset(test, tokenizer_trans, max_length_c=128,  max_length_w=96, inference_only=True)\ntest_loader2 = DataLoader(test_dataset, batch_size=16,drop_last=False, shuffle=False)\n\n\ntest = pd.read_csv('..\/input\/d\/lhagiimn\/num-challenge-xlm-roberta-base-v2\/test.csv')\ntest['mask_word'] = test['mask_word'].fillna(\"\u0443\u0442\u0433\u0430\u0433\u04af\u0439 \u0445\u043e\u043e\u0441\u043e\u043d\")\ntest_dataset = NumDataset(test, tokenizer, max_length_c=128,  max_length_w=96, inference_only=True)\ntest_loader3 = DataLoader(test_dataset, batch_size=16,drop_last=False, shuffle=False)\n\n'''\ntest = pd.read_csv('..\/input\/numchallengelocalrobertabase\/test.csv')\ntest['mask_word'] = test['mask_word'].fillna(\"\u0443\u0442\u0433\u0430\u0433\u04af\u0439 \u0445\u043e\u043e\u0441\u043e\u043d\")\ntest_dataset = NumDataset(test, tokenizer_b, max_length_c=128,  max_length_w=96, inference_only=True)\ntest_loader1 = DataLoader(test_dataset, batch_size=16,drop_last=False, shuffle=False)\n'''\n\npred, pred_prob = predict(models0, test_loader0, models1, test_loader1, models2, test_loader2, models3, test_loader3)","3cb54157":"pred","eab7e5e1":"test","050be36d":"submission","18011daf":"test['pred'] = pred+1\ntest['prob'] = pred_prob\/np.max(pred_prob)\n\n\nsubmission = submission.set_index(['text_id'])\ntest = test.set_index(['text_id'])\n\nsubmission['synset_id'] = test.loc[submission.index, 'pred'].values\nsubmission.to_csv('submission.csv')\n\ntest.to_csv('test_for_pseudo.csv')","4184c0f9":"### Finetuned Models\n\n1. **Roberta-base Model:** https:\/\/www.kaggle.com\/lhagiimn\/clrp-pytorch-roberta-pretrain\n2. **XML Roberta-base Model:** https:\/\/www.kaggle.com\/lhagiimn\/num-challenge-xlmroberta-fine-tune \n\n# Trained Models\n\n1. **Model 1:** https:\/\/www.kaggle.com\/lhagiimn\/pytorch-nlp-xlmroberta?scriptVersionId=72909332\n2. **Model 2:** https:\/\/www.kaggle.com\/lhagiimn\/pytorch-nlp-xlmroberta?scriptVersionId=72988512\n3. **Model 3:** https:\/\/www.kaggle.com\/lhagiimn\/num-challenge-roberta-base-model?scriptVersionId=72908195\n\n**Final Model training Notebook:** https:\/\/www.kaggle.com\/lhagiimn\/num-challenge-models-training?scriptVersionId=74036145\n\n# Result\n\n* **V1:** FAILED\n* **V2:** 2 XLM Roberta based Models: CV |0.95516|0.9563|  LB: 0.95759\n* **V3:** 2 XLM Roberta based Models and Roberta-base: CV |0.95516|0.9563|0.9402|  LB: 0.95806 Weights: (1.35, 0.35, 0.30)  #bug\n* **V4:** 2 XLM Roberta based Models and Roberta-base: CV |0.95516|0.9563|0.9402|  LB: 0.95830 Weights: (1.40, 0.20, 0.40)  #bug\n* **V5:** 2 XLM Roberta based Models and Roberta-base: CV |0.95516|0.9563|0.9402|  LB: 0.95795 Weights: (1.34, 0.33, 0.33)  #bug\n* **V6:** XLM Roberta v2: CV |0.9585|  LB: 0.9545  **modification**: input length, max length, optimization step  #find bug\n* **V7:** 3 XLM Roberta based Models and Roberta-base: CV |0.95516|0.9563|0.9585|0.9402|  LB: 0.9574 Weights: (0.25, 0.25, 0.25, 0.25)  #fixed bug\n* **V8:** 3 XLM Roberta based Models and Roberta-base: CV |0.95516|0.9563|0.9585|0.9402|  LB: 0.9579 Weights: (0.50, 0.15, 0.20, 0.15) \n* **V9:** Best LB XLM Roberta Model and Roberta-base Local CV best model: CV |0.95516|0.9456|  LB: 0.95830 Weights: (0.75, 0.25)  \n* **V10:** Best LB XLM Roberta Model and Roberta-base Local CV best model: CV |0.95516|0.9456|  LB: 0.95830 Weights: (0.825, 0.175)  \n* **V11:** Best LB XLM Roberta Model and XLM roberta large model: CV |0.95516|0.9541|  LB: 0.95877 Weights: (0.50, 0.40) \n* **V12:** XLM Roberta large CV best model: CV |0.9725|  LB: 0.95008\n* **V13:** Best LB XLM Roberta Model and XLM roberta large model: CV |0.95516|0.9555|  LB: 0.95818 Weights: (0.50, 0.50) \n* **V14:** Single XLM Roberta Model: CV |0.9675|0.9555|  LB: 0.95489 \n* **V15:** Best XLM Roberta Model and XLM roberta large model: CV |0.9564|0.9572|  LB: 0.9574 Weights: (0.50, 0.50) \n* **V16:** Best XLM Roberta Model and XLM roberta large model: CV |0.9602|0.9648|  LB: 0.9606 Weights: (0.40, 0.60) \n* **V17:** Best XLM Roberta Model and XLM roberta large model: CV |0.9602|0.9648|  LB: 0.95795 Weights: (0.20, 0.80)\n* **V18:** Best XLM Roberta Model and XLM roberta large model and Transformer: CV |0.9602|0.9648|0.9601|  LB:0.96135 Weights: (0.30, 0.40, 0.30) (ver 14 and ver 1)\n* **V19:** Best XLM Roberta Model and XLM roberta large model and Transformer: CV |0.9656|0.9690|0.9601|  LB:0.95959 Weights: (0.40, 0.40, 0.20) (ver 15 and ver 1)\n* **V20:** Best XLM Roberta Model and XLM roberta large model and Transformer: CV |0.9656|0.9690|0.9601|  LB:0.95959 Weights: (0.30, 0.40, 0.30) (ver 15 and ver 1)\n* **V21:** Best XLM Roberta Model and XLM roberta large model and Transformer: CV |0.9656|0.9690|0.9601|  LB:0.95771 Weights: (0.50, 0.50, 0.00) (ver 15 and ver 1)\n* **V22:** Best XLM Roberta Model and XLM roberta large model and Transformer: CV |0.9602|0.9648|0.9601|  LB:0.96171 Weights: (0.35, 0.35, 0.30) (ver 14 and ver 1)\n* **V23:** Best XLM Roberta Model and XLM roberta large model and Transformer: CV |0.9572|0.9625|0.9618|  LB:0.95971 Weights: (0.20, 0.40, 0.40) (ver 20 and ver 5)\n* **V24:** Best XLM Roberta Model and XLM roberta large model and Transformer: CV |0.9602|0.9648|0.9618|  LB:0.95983 Weights: (0.30, 0.35, 0.35) (ver 14 and ver 5)\n* **V25:** Best XLM Roberta Model and XLM roberta large model and Transformer: CV |0.95516|0.9648|0.9618|  LB:0.96135 Weights: (0.40, 0.30, 0.30) (ver 4 (cv), ver 14 and ver 5)\n* **V25:** Best XLM Roberta Model and XLM roberta large model and Transformer: CV |0.95516|0.9602|0.9648|0.9610|  LB:0.9612 Weights: (0.25, 0.15, 0.30, 0.30) (ver 4 (cv), ver 14 and ver 5)\n* **V25:** Best XLM Roberta Model and XLM roberta large model and Transformer: CV |0.95516|0.9602|0.9648|0.9610|  LB:0.9612 Weights: (0.30, 0.10, 0.30, 0.30) (ver 4 (cv), ver 14 and ver 5"}}