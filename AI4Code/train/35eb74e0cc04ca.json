{"cell_type":{"e566df3c":"code","b06e3c51":"code","ee160199":"code","8ac40725":"code","4f3f3a30":"code","1c86d71b":"code","2317eff1":"code","b18c41a5":"code","f8118162":"code","1405dcc9":"code","e95a1267":"code","1da8fbfb":"code","c86e326d":"code","e907830a":"code","326f2120":"code","0e2ceab1":"code","540f40da":"code","58dfb6d9":"code","35acb896":"code","1771f29d":"code","b4cde271":"code","67178aa5":"code","4ad7f89c":"code","45096039":"code","5fd6fbd1":"code","00e57c35":"code","025e2937":"code","1e0b2632":"code","f5e68b11":"code","0bb5750e":"code","b7b2fcda":"code","cc25796f":"code","99a1ca55":"code","c7f96872":"code","a39fd686":"code","6098377f":"code","c6c5660e":"code","aa2c43c3":"code","05fc7e61":"code","5ec8f62f":"code","2faca0fa":"code","4a55a20f":"code","42807d55":"code","17dc4a18":"code","cb502bba":"code","784c8fb3":"code","21520c3f":"code","21c5e7d9":"code","ebd81ccc":"code","540e58b9":"code","371319ac":"code","c13d319e":"code","5b1884b6":"markdown","2a88247e":"markdown","652af7f9":"markdown","0dde52c3":"markdown","a438f5e0":"markdown","6d6e64dd":"markdown","700366f6":"markdown","f463c460":"markdown","4866abef":"markdown","3f68ca6d":"markdown","f30518b9":"markdown","32b0cfcd":"markdown","c69c1514":"markdown","e62995c3":"markdown","4f099fcd":"markdown","87fa746d":"markdown","9bd369cb":"markdown","4e5bbdf6":"markdown","b6dfdeaf":"markdown","2189d1cc":"markdown","68b02c3b":"markdown","129e0ef2":"markdown","fcff2c16":"markdown","52940d81":"markdown","cbd30d5c":"markdown","a86f83ed":"markdown","7aa745d7":"markdown","ff8b3b90":"markdown","901f7ad3":"markdown","1d7971af":"markdown"},"source":{"e566df3c":"#Importing libraries \nimport pandas as pd\nimport numpy as np\nimport pydicom \n\nimport os\nimport random\n\n#Visualisation \nimport matplotlib.pyplot as plt\nimport seaborn as sns","b06e3c51":"import warnings\nwarnings.filterwarnings(\"ignore\");\n%matplotlib inline","ee160199":"INPUT_PATH = '..\/input\/rsna-intracranial-hemorrhage-detection\/rsna-intracranial-hemorrhage-detection\/'","8ac40725":"os.listdir(INPUT_PATH)","4f3f3a30":"sub = pd.read_csv(INPUT_PATH+\"stage_2_sample_submission.csv\")\nsub.head(10)","1c86d71b":"train_df = pd.read_csv(INPUT_PATH+\"stage_2_train.csv\")\n# train_df.head(10)\nlabels =  train_df.Label.values","2317eff1":"train_df = train_df.ID.str.rsplit(\"_\",n=1,expand=True)\ntrain_df.loc[:, \"label\"] = labels\ntrain_df.head()","b18c41a5":"train_df = train_df.rename({0 : \"image\",1 : \"subtype\"}, axis=1)","f8118162":"#find the count of targets under each subtype\nsubtype_count = train_df.groupby(\"subtype\").label.value_counts().unstack()\nsubtype_count","1405dcc9":"#calculating the % target distribution across each subtype\nsubtype_count_per =  subtype_count.loc[:,1]\/train_df.groupby(\"subtype\").size() *100\n\nmulti_target_count = train_df.groupby(\"image\").label.sum()","e95a1267":"#Helper function\ndef random_colors(num_colors : int):\n    colors = []\n    for i in range(num_colors):\n        colors.append('#'+''.join([random.choice('0123456789ABCDEF') for j in range(6)]))\n    return colors ","1da8fbfb":"fig, ax = plt.subplots(3,1, figsize=(30,50))\n\nsns.countplot(train_df.label,ax=ax[0], palette=random_colors(2))\nax[0].set_xlabel(\"Target\", fontsize=40)\nax[0].tick_params(axis='x', labelsize=25 ) \nax[0].tick_params(axis='y', labelsize=25 ) \nax[0].set_title(\"Number of positive and negative targets\",fontsize=40)\n\n\nsns.countplot(x=\"subtype\", hue=\"label\", data=train_df, ax=ax[1], palette=random_colors(6))\nax[1].set_xlabel(\"Number of targets per image\",fontsize=40)\nax[1].set_ylabel(\"Frequency\",fontsize=40)\nax[1].tick_params(axis='x', labelsize=25 ) \nax[1].tick_params(axis='y', labelsize=25 ) \nax[1].set_title(\"Target distrubution\",fontsize=40)\n\nsns.barplot(subtype_count_per.index, subtype_count_per.values, ax=ax[2], palette=random_colors(6))\nplt.xticks(rotation=45)\nax[2].set_ylabel(\"% of positive(1) occurences\",fontsize=40)\nax[2].tick_params(axis='x', labelsize=25 ) \nax[2].tick_params(axis='y', labelsize=25 ) \nax[2].set_title(\"Imbalance in target distrubution\",fontsize=40)","c86e326d":"#Count of images in training dataset\ntrain_df.image.nunique()","e907830a":"train_files = os.listdir(INPUT_PATH+\"stage_2_train\")","326f2120":"#Actual no. of image files \nlen(train_files)","0e2ceab1":"test_files = os.listdir(INPUT_PATH+\"stage_2_test\")\nlen(test_files)","540f40da":"len(train_files)\/len(test_files)","58dfb6d9":"train_files[:5]","35acb896":"fig = plt.figure(figsize=(40,20))\ncolumn= 5; rows = 4\nfor i in range(1, column*rows +1):\n    dcm = pydicom.dcmread(INPUT_PATH+\"stage_2_train\/\"+train_files[i])\n    fig.add_subplot(rows, column, i)\n    plt.imshow(dcm.pixel_array, cmap=plt.cm.bone)\n    fig.add_subplot","1771f29d":"print(dcm)","b4cde271":"image = dcm.pixel_array\nprint(type(image)) #format in which pixel data is stored\nprint(image.dtype) #datatype of the pixel values\nprint(image.shape) #shape of image(wxh)","67178aa5":"plt.imshow(image, cmap=plt.cm.bone)","4ad7f89c":"## A function to correct pixel data and rescale intercercepts ob 12 bit images\ndef dcm_correction(dcm_img):\n        x = dcm_img.pixel_array + 1000\n        px_mode = 4096\n        x[x >= px_mode] = x[x >= px_mode] - px_mode #if there are extra bits in 12-bit grayscale(<=4096)\n        dcm_img.PixelData = x.tobytes()\n        dcm_img.RescaleIntercept = -1000 #setting a common value across all 12-bit US images","45096039":"diff_size = []\nfor i in range(len(INPUT_PATH+\"stage_2_train\/\")):\n    dicom = pydicom.dcmread(INPUT_PATH+\"stage_2_train\/\"+train_files[i])\n    \n    if dicom.BitsStored == 12:\n        diff_size.append(dicom)","5fd6fbd1":"len(diff_size)","00e57c35":"diff_size[1]","025e2937":"diff_size[1].pixel_array","1e0b2632":"dcm_correction(diff_size[1])\ndiff_size[1]","f5e68b11":"diff_rescale = []\nfor i in range(len(INPUT_PATH+\"stage_2_train\/\")):\n    dicom = pydicom.dcmread(INPUT_PATH+\"stage_2_train\/\"+train_files[i])\n    \n    if (int(dicom.RescaleIntercept) != -1024):\n        diff_rescale.append(dicom)","0bb5750e":"diff_rescale[0].pixel_array","b7b2fcda":"dcm_correction(diff_rescale[0])","cc25796f":"diff_rescale[0].pixel_array","99a1ca55":"diff_rescale","c7f96872":"need_correct = []\nfor i in range(len(INPUT_PATH+\"stage_2_train\/\")):\n    dicom = pydicom.dcmread(INPUT_PATH+\"stage_2_train\/\"+train_files[i])\n    \n    if (dicom.BitsStored == 12) and (dicom.PixelRepresentation == 0):\n        need_correct.append(dicom)","a39fd686":"len(need_correct)","6098377f":"#Systemic\/linear windowing\ndef window_image(dcm, window_center, window_width):\n    if (dcm.BitsStored == 12) and (dcm.PixelRepresentation == 0):\n        dcm_correction(dcm)\n\n    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept #reconstructing the image from pixels\n    img_min = window_center - window_width \/\/ 2 #lowest visible value\n    img_max = window_center + window_width \/\/ 2 #highest visible value\n    img = np.clip(img, img_min, img_max)\n\n    return img","c6c5660e":"TRAIN_PATH = INPUT_PATH+\"stage_2_train\/\"\nTEST_PATH = INPUT_PATH+\"stage_2_test\/\"\n\ndef view_images(image, title=''):\n\n    dcm = pydicom.dcmread(os.path.join(TRAIN_PATH,image[3]+'.dcm'))\n\n    fig, (ax1, ax2, ax3, ax4) = plt.subplots(4,1, figsize=(10,24))\n    \n    ax1.set_title(\"Default window\")\n    ax1.imshow(dcm.pixel_array, cmap=plt.cm.bone)\n    \n    ax2.set_title(\"Brain window\")\n    brain_img = window_image(dcm, 40, 80)\n    ax2.imshow(brain_img, cmap=plt.cm.bone)\n    \n    ax3.set_title(\"Subdural window\")\n    subdural_img = window_image(dcm, 80, 200)\n    ax3.imshow(subdural_img, cmap=plt.cm.bone)\n#     ax3.annotate('', xy=(150, 380), xytext=(120, 430),\n#             arrowprops=dict(facecolor='red', shrink=0.05),\n#             )\n#     ax3.annotate('', xy=(220, 430), xytext=(190, 500),\n#             arrowprops=dict(facecolor='red', shrink=0.05),\n#             )\n    \n    ax4.set_title(\"Soft Tissue window\")\n    soft_img = window_image(dcm, 40, 380)\n    ax4.imshow(soft_img, cmap=plt.cm.bone)\n    \n    for ax in fig.axes:\n        ax.axis(\"off\")\n        \n    fig.suptitle(title)\n    plt.show()\n    ","aa2c43c3":"train_df.head()","05fc7e61":"view_images(train_df[(train_df[\"subtype\"] == 'epidural') & (train_df['label'] == 1)][:10].image.values, title='Images wth epidural')","5ec8f62f":"view_images(train_df[(train_df[\"subtype\"] == 'subdural') & (train_df['label'] == 1)][:10].image.values, title='Images wth subdural')","2faca0fa":"view_images(train_df[(train_df[\"subtype\"] == 'subarachnoid') & (train_df['label'] == 1)][:10].image.values, title='Images wth subarachnoid')","4a55a20f":"view_images(train_df[(train_df[\"subtype\"] == 'intraventricular') & (train_df['label'] == 1)][:10].image.values, title='Images wth intraventricular')","42807d55":"view_images(train_df[(train_df[\"subtype\"] == 'intraparenchymal') & (train_df['label'] == 1)][:10].image.values, title='Images wth intraparenchymal')","17dc4a18":"test_case = os.path.join(TRAIN_PATH,'ID_12cadc6af.dcm')\n\ntest_data = pydicom.read_file(test_case)\nplt.imshow(test_data.pixel_array, cmap=plt.cm.bone)","cb502bba":"brain_img = window_image(test_data, 40, 80)\nsubdural_img = window_image(dcm, 80, 200)\nsoft_img = window_image(dcm, 40, 380)","784c8fb3":"brain_img = (brain_img - 0) \/ 80\n# print(brain_img)\nplt.imshow(brain_img, cmap=plt.cm.bone)","21520c3f":"print(subdural_img)\nplt.imshow(subdural_img, cmap=plt.cm.bone)","21c5e7d9":"subdural_img = (subdural_img - (-20))\/200\n# print(subdural_img)\nplt.imshow(subdural_img, cmap=plt.cm.bone)","ebd81ccc":"print(soft_img)\nplt.imshow(soft_img, cmap=plt.cm.bone)","540e58b9":"soft_img = (soft_img - (-150))\/380\n# print(soft_img)\nplt.imshow(soft_img, cmap=plt.cm.bone)","371319ac":"bsb_img = np.array([brain_img, subdural_img, soft_img]).transpose(1, 2, 0)","c13d319e":"plt.imshow(bsb_img, cmap=plt.cm.bone)","5b1884b6":"DICOM images typically contain between 12\u201316 bits\/pixel, which corresponds to approximately 4,096 to 65,536 shades of gray. But most regular computer screens are often limited to 8 bits or 256 shades of gray. \n\nMost images like the one above **display a wide range of tissue densities**(ranging from -1000HU(air) to +1000HU(bone)), but as mentioned above a **computer screen  can only display 256 shades of gray with our eye detecting only about a 6% change** in grayscale ","2a88247e":"# REFERENCES\n[1. For DICOM related info ]( https:\/\/dicomiseasy.blogspot.com\/2012\/08\/chapter-12-pixel-data.html)\n\n[2. To understand CT scans and workflow of radiologists](https:\/\/radiopaedia.org\/articles\/ct-head-an-approach?lang=gb)\n\n[3. Paper referred](https:\/\/arxiv.org\/pdf\/2008.00302.pdf)","652af7f9":"Before concatenating them, we need to make sure that all of theirvalues fall under a same range so we Standardise their values","0dde52c3":"## Look into target distribution","a438f5e0":"**Too much info! what are these different windows?**\n\n[radiopedia.org](https:\/\/radiopaedia.org\/articles\/ct-head-an-approach?lang=gb) shows a typical workflow and well thought process that a radiologist takes when given a task to detect any abnormalties on CT scan of the brain.\n\nFor head CT, bone window and brain window are two important window settings.However, the details of soft tissues such as brain, that shows density lower than that of bones, are lost in the bone window setting. Brain window is the most frequently used setting, and the majority of evaluations of brain abnormality are done using this window setting.\n\nWhat we understand is that, while the brain matter window is able to pick most abnormalities it might cause to miss some diagnosis. So while diagnising something like a hemorrhage we need to look into other windows like the subdural that focus more on the subdural hematoma.\n\n#### **Subdurals could be tricky..** ###\n\nIf you check their definition, they usually are right next to the skull, longish in shape and follows the curvature of the skull. Hence , if you look through a brain window you might miss out on these.. hence it adviced to incoporate a subdural window.\n","6d6e64dd":"So what to do? **Windowing !**","700366f6":"The training dataset is provided as set image`Id` and **multiple labels**, one for each of the subtypes of hemorrhage along with an addition lable for `any`(will be true of any of the subtype labels in true). So this is a **multilable classification task**.\n\nLets split the ID into columns of images and the corresponding diagnosis(subtype)","f463c460":"## **Windowing**\n\nWindowing, also known as grey-level mapping or contrast enhancement is the process in which the CT image greyscale component of an image is manipulated via the CT numbers; doing this will change the appearance of the picture **to highlight particular structures**\n\nHere's where some of the DICOM meta data comes to help -\n\n`Window width` also know as the contrast --> is the measure of the range of CT numbers that an image contains\n\n`Window center` also known as brightness -->  is the midpoint of the range of the CT numbers displayed; window level is decreased the CT image will be brighter and vice versa.\n\n\nThese two could be used to calculate the upper and lower grey levels, to produce different kinds of windows based on the kind of diagnosis.\n","4866abef":"Below are some of the slices from CT scans that are stored as pixel data in DICOM files","3f68ca6d":"So images are of 512x512, we'll downsample them later to deal with the large training set","f30518b9":"**So what happening above ?**\n\nIf the DICOM file is of 12-bit type(41 outliers), then we correct them before generating our windows.\n\nWe then clip the pixel intensities between the lowest and hishest visisble values, to focus only on a narrow region where the abnormality might be present. This means that every pixel value greater than the `img_max` will show up as **white** and belowe `img_min` will show up as **black**\n\nSince each window highlights particular ranges, it makes it easier for a radiologists(the DL system in our case) to see if there are any changes between normal and pathologically altered tissue. So based on the diagnosis, the model would learn to look at only certain windows of tissue desities(features). ","32b0cfcd":"# **Getting into data preprocessing**","c69c1514":"Lets ask questions and try finding answers to it through visualisations :\n1. How many positive and negative targets do we see in the training dataset?\n2. What is the target distribution across each of the labels ?\n3. Is the dataset imbalanced ?","e62995c3":"As we see there are details about the sampling along with the patients details. \nSome of it(like Window center, Window width, Rescale Intercept) can help at better pre-processing of these DICOM files.\n\nLets look the pixel data of one sample and find out the shape of these images","4f099fcd":"So 6.2times more images in train dataset than test dataset","87fa746d":"Look into the meta data that comes with the DICOM File, look for insights that can help us during processing ","9bd369cb":"Next, create the brain, subdural and soft tissue windows","4e5bbdf6":"So we brought all three windows to same scale, now we will combine them into a single 3-channel image","b6dfdeaf":"**Math around it :**\n Eye can detect only 6% change in grayscale, so `100\/6 = 17 shades of gray`\n To display a DICOM(having range of approx, 2000HU) image on computer screen(can only display 256 shades pf gray) = `2000\/256 = 8 --> each shade of gray would have diff of 8HU`\n \n Therefore, each variation would vary by `256\/17*9 = 120HU`\n \n BUT, the difference between normal and pathologically altered tissue is usually a lot less than 120 HU \n ","2189d1cc":"Lets now try to get a picture of how windows would help us","68b02c3b":"So what do we make of it ?\n\n- Less no of positive target values\n- Epidural type has very few positive occurences(<1%)\n- Highly Imbalanced","129e0ef2":"Ok. So no issues in training set.","fcff2c16":"## Lets look into DICOM files\n\n### **What is a DICOM ?**\nDicom is a format that has metadata, as well as Pixeldata attached to it. Below I extract some basic info with an image. You will know about the gender and age of the patient, as well as info how the image is sampled and generated. \n\nSo lets look at some samples from our dataset","52940d81":"Lets take a random sample from the dataset to show the entire preprocessing would be","cbd30d5c":"So Image IDs of form ID_SUBTYPE, which means we would have to make predictions for each subtype under a image ID. \n`any` --> indicates there is at least one subtype present telling us that patient has IH or not\n`Label` --> indicates probability of presence","a86f83ed":"The point of applying windows is **to focus down the 256 shades of grey into a narrow region of HU(Hounsfiled units) that contain the relevnat densities of tissues we are interested** in while diagonising.","7aa745d7":"### Basic Checks before exploring images..","ff8b3b90":"Compare counts of images provided in training dataset with the training files given to check \neverything is fine before proceeeding..","901f7ad3":"### Combining the windows ","1d7971af":"# **Exploratory analysis**\n\nLet us try exploring this dataset, to get some insights and understand our data better"}}