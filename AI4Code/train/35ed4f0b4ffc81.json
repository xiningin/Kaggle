{"cell_type":{"ca763caf":"code","f2ae987b":"code","f7c50744":"code","6e731404":"code","985d4263":"code","e12b8bf9":"code","fc6e0030":"code","2cc681b3":"code","4f346eb6":"code","dadd8e00":"code","2097f388":"code","a5f56da3":"code","48c138d1":"code","9ff2cb1a":"code","bdacb89f":"code","e041172e":"code","dba4ac1f":"code","b742c0a0":"code","20fdd05e":"code","42e09d5a":"code","8ee9d54c":"code","44564a1c":"code","ad07fd80":"code","81d757a3":"code","5b2f6d14":"code","37fd2121":"code","f9f77e5c":"code","dc1411e6":"code","3990f757":"code","0dee46e4":"code","56d85ca6":"markdown","4e01e30d":"markdown","ed3dfa7f":"markdown","a475c952":"markdown","42e5c03f":"markdown","44f0b74b":"markdown","1b51e59f":"markdown","3e35ff06":"markdown","6e9ae4cc":"markdown","12aa57a4":"markdown","2b4d65bb":"markdown","3fd75157":"markdown","c1c46968":"markdown","f252d679":"markdown","089b23df":"markdown","5320de87":"markdown","86014ab1":"markdown","a3558db5":"markdown","922051a0":"markdown","a964d6e9":"markdown","4c724eed":"markdown","a69efe4d":"markdown","fcd617ed":"markdown","3bebb691":"markdown","33e7e93a":"markdown","92545507":"markdown","f45410bf":"markdown","b7701ba3":"markdown","68b04b51":"markdown","ff03da72":"markdown","2aaf4508":"markdown","76a76e83":"markdown","11de56a4":"markdown","7990259a":"markdown"},"source":{"ca763caf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport nltk #Natural Language Toolkit for Processing\nfrom nltk.corpus import stopwords #Get the Stopwords to Remove\n\nimport re #Regular Expressions\nimport html #Messing with HTML content, like &amp;\nimport string #String Processing\n\nimport tensorflow as tf #Import tensorflow in order to use Keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer #Add the keras tokenizer for tweet tokenization\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences #Add padding to help the Keras Sequencing\nimport tensorflow.keras.layers as L #Import the layers as L for quicker typing\nfrom tensorflow.keras.optimizers import Adam #Pull the adam optimizer for usage\n\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy #Loss function being used\nfrom sklearn.model_selection import train_test_split #Train Test Split\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f2ae987b":"twTrain = pd.read_csv(\"..\/input\/covid19-fake-news-dataset-nlp\/Constraint_Train.csv\") #Load the tweet (tw) training set\ntwTrain.head() #Take a peek at the data","f7c50744":"twValid = pd.read_csv(\"..\/input\/covid19-fake-news-dataset-nlp\/Constraint_Val.csv\") #Load the tweet (tw) validation set\ntwValid.head() #Take a peek at the data","6e731404":"twTest = pd.read_csv(\"..\/input\/covid19-fake-news-dataset-nlp\/Constraint_Test.csv\") #Load the tweet (tw) testing set\ntwTest.head() #Take a peek at the data","985d4263":"print(\"Training Set:\\n\", twTrain.isnull().any()) #Check for null values in the training set\nprint(\"Validation Set:\\n\", twValid.isnull().any()) #Check for null values in the validation set\nprint(\"Testing Set:\\n\", twTest.isnull().any()) #Check for null values in the testing set","e12b8bf9":"print(twTrain[\"tweet\"][0]) #Print a simple tweet example\nprint(twTrain[\"tweet\"][300]) #Print a more typical tweet example","fc6e0030":"print(\"Training Labels:\\n\", twTrain[\"label\"].value_counts()) #See the training labels\nprint(\"Validation Labels:\\n\", twValid[\"label\"].value_counts()) #See the validation labels","2cc681b3":"punctuations = string.punctuation #List of punctuations to remove\nprint(punctuations) #See the punctuations the string library has\n\nSTOP = stopwords.words(\"english\") #Get the NLTK stopwords\nprint(STOP) #See what NLTK considers stopwords","4f346eb6":"#CleanTweets: parses the tweets and removes punctuation, stop words, digits, and links.\n#Input: the list of tweets that need parsing\n#Output: the parsed tweets\ndef cleanTweets(tweetParse):\n    for i in range(0,len(tweetParse)):\n        tweet = tweetParse[i] #Putting the tweet into a variable so that it is not calling tweetParse[i] over and over\n        tweet = html.unescape(tweet) #Removes leftover HTML elements, such as &amp;\n        tweet = re.sub(r\"@\\w+\", \" \", tweet) #Completely removes @'s, as other peoples' usernames mean nothing\n        tweet = re.sub(r\"http\\S+\", \" \", tweet) #Removes links, as links provide no data in tweet analysis in themselves\n        \n        tweet = \"\".join([punc for punc in tweet if not punc in punctuations]) #Removes the punctuation defined above\n        tweet = tweet.lower() #Turning the tweets lowercase real quick for later use\n    \n        tweetWord = tweet.split() #Splits the tweet into individual words\n        tweetParse[i] = \"\".join([word + \" \" for word in tweetWord if not word in STOP]) #Checks if the words are stop words\n        \n    return tweetParse #Returns the parsed tweets","dadd8e00":"twTrain[\"cleanTweet\"] = cleanTweets(twTrain[\"tweet\"].copy()) #Clean the training tweets\ntwTrain.head() #Take a look at the dataset","2097f388":"twValid[\"cleanTweet\"] = cleanTweets(twValid[\"tweet\"].copy()) #Clean the validation tweets\ntwValid.head() #Take a peek at the dataset","a5f56da3":"twTest[\"cleanTweet\"] = cleanTweets(twTest[\"tweet\"].copy()) #Clean the testing tweets\ntwTest.head() #Take a peek at the dataset","48c138d1":"print(\"Training: \\n\", twTrain.loc[twTrain[\"cleanTweet\"] == \"\"]) #Check for Training Blank Tweets\nprint(\"Validation: \\n\", twValid.loc[twValid[\"cleanTweet\"] == \"\"]) #Check for Validation Blank Tweets\nprint(\"Testing: \\n\", twTest.loc[twTest[\"cleanTweet\"] == \"\"]) #Check for Testing Blank Tweets","9ff2cb1a":"print(twTrain[\"tweet\"][300]) #Print a more typical tweet example\nprint(twTrain[\"cleanTweet\"][300]) #Print the tweet after processing to show link and stopword removal","bdacb89f":"dummyTrain = pd.get_dummies(twTrain[\"label\"]) #Get the dummies for the training set\nprint(dummyTrain) #Show the dummies","e041172e":"twTrain[\"encodedLabel\"] = dummyTrain[\"real\"] #Get the encoded labels from the \"real\" dummies\ntwTrain.head() #Take a peek at the data","dba4ac1f":"twValid[\"encodedLabel\"] = pd.get_dummies(twValid[\"label\"])[\"real\"] #Get the encoded labels for the validation set\ntwValid.head() #Take a peek at the data","b742c0a0":"trainClean = twTrain[\"cleanTweet\"].copy() #Get the training clean tweets\ntestClean = twTest[\"cleanTweet\"].copy() #Get the testing clean tweets\nvalidClean = twValid[\"cleanTweet\"].copy() #Get the validation clean tweets\n\ntrVaClean = trainClean.append(validClean, ignore_index = True) #Combine the training and validation tweets\nallCleanTweet = trVaClean.append(testClean, ignore_index = True) #Combine all of the tweets into one series\nprint(len(allCleanTweet)) #Print the length to show they are all together","20fdd05e":"token = Tokenizer() #Initialize the tokenizer (set here so all of the datasets are in the same tokenizer)\ntoken.fit_on_texts(allCleanTweet) #Fit the tokenizer to all of the tweets","42e09d5a":"#TokenizeTweet: turn the tweets into tokens for Keras to use\n#Input: a set of tweets\n#Output: a set of padded sequences representing the tweets\ndef tokenizeTweet(tweets):\n    texts = token.texts_to_sequences(tweets) #Convert the tweets into sequences for keras to use\n    texts = pad_sequences(texts, padding='post') #Pad the sequences to make them similar lengths\n    \n    return texts #Return the padded sequences","8ee9d54c":"texts = tokenizeTweet(twTrain[\"cleanTweet\"].copy()) #Collect the tokenized tweet sequences\ntwTrain[\"tweetSequence\"] = list(texts) #Add this data to the dataframe\ntwTrain.head() #Take a peek at the dataset","44564a1c":"textsValid = tokenizeTweet(twValid[\"cleanTweet\"].copy()) #Collect tokenized tweet sequences\ntwValid[\"tweetSequence\"] = list(textsValid) #Add this data to the dataframe\ntwValid.head() #Take a peek at the dataset","ad07fd80":"textsTest = tokenizeTweet(twTest[\"cleanTweet\"].copy()) #Collect tokenized tweet sequences\ntwTest[\"tweetSequence\"] = list(textsTest) #Add this data to the dataframe\ntwTest.head() #Take a peek at the dataset","81d757a3":"size = len(token.word_index) + 1 #Set the number of words for the size\n\ntf.keras.backend.clear_session() #Clear any previous model building\n\nepoch = 3 #Number of runs through the data\nbatchSize = 32 #The number of items in each batch\noutputDimensions = 16 #The size of the output\nunits = 256 #Dimensions of the output space\n\nmodel = tf.keras.Sequential([ #Start the sequential model, doing one layer after another in a sequence\n    L.Embedding(size, outputDimensions, input_length = texts.shape[1]), #Embed the model with the number of words and size\n    L.Bidirectional(L.LSTM(units, return_sequences = True)), #Make it so the model looks both forward and backward at the data\n    L.GlobalMaxPool1D(), #Take the max values over time\n    L.Dropout(0.3), #Make the dropout 0.3, making about a third 0 to prevent overfitting\n    L.Dense(64, activation=\"relu\"), #Create a large dense layer\n    L.Dropout(0.3), #Make the dropout 0.3, making about a third 0 to prevent overfitting\n    L.Dense(3) #Create a small dense layer\n])\n\n\nmodel.compile(loss = SparseCategoricalCrossentropy(from_logits = True), #Compile the model with a SparseCategorical loss function\n              optimizer = 'adam', metrics = ['accuracy'] #Add an adam optimizer and collect the accuracy along the way\n             )\n\nhistory = model.fit(texts, twTrain[\"encodedLabel\"], epochs = epoch, validation_split = 0, batch_size = batchSize) #Fit the model to the data","5b2f6d14":"predict = model.predict_classes(textsValid) #Predict ratings based on the model\nloss, accuracy = model.evaluate(textsValid, twValid[\"encodedLabel\"]) #Get the loss and Accuracy based on the tests\n\n#Print the loss and accuracy\nprint(\"Validation Loss: \", loss)\nprint(\"Validation Accuracy: \", accuracy)","37fd2121":"pd.set_option(\"display.max_colwidth\", 1000) #Show as much of the tweet as possible\n\nvalidLabel = twValid[\"encodedLabel\"].copy() #Get the encoded labels (1 for real, 0 for fake)\nvalidLabel = pd.DataFrame(validLabel) #Convert to a dataframe to hold more data\nvalidLabel[\"predictions\"] = predict #Add the predictions to the dataframe\nvalidLabel[\"tweet\"] = twValid[\"tweet\"].copy() #Add the original tweet for comparison sake\nvalidLabel.head() #Compare","f9f77e5c":"predictTest = model.predict_classes(textsTest) #Predict ratings based on the model","dc1411e6":"tweetTest = twTest[\"tweet\"].copy() #Get the original tweets\ntweetTest = pd.DataFrame(tweetTest) #Put the tweets into a dataframe\ntweetTest[\"prediction\"] = predictTest #Add in the predictions\ntweetTest = tweetTest[[\"prediction\", \"tweet\"]] #Change column order to line up with the validation dataframe's order\ntweetTest.head() #Show the tests","3990f757":"pd.set_option(\"display.max_rows\", 10000) #Show as much as possible\nvalidLabel #Show the validation set","0dee46e4":"tweetTest #Show the test set","56d85ca6":"# Check for Post-Processing Blank Tweets","4e01e30d":"---","ed3dfa7f":"# Validate","a475c952":"# Test Set Predictions","42e5c03f":"# Data Exploration","44f0b74b":"---","1b51e59f":"# Tweets with Predictions: Full Data","3e35ff06":"This code is reworked from my original coronavirus tweet sentiment analysis from earlier in the pandemic (https:\/\/www.kaggle.com\/lunamcbride24\/coronavirus-tweet-processing). I have changed it to use NLTK instead of spacy since those stopwords do not require building a spacy model. I have also used the string library to get punctuation instead of having a bulky hard-coded list and removed the number remover, as I feel that numbers may be a key factor here (especially with the usage of the name Covid-19, since that may have lost the 19 and became just covid, which has a different connotation). These were factors I wanted to change about the original after playing with Keras for TripAdvisor reviews (https:\/\/www.kaggle.com\/lunamcbride24\/hotel-review-keras-classification-project). \n\nThis may be a note to myself, but I did both of those projects half a year ago. This is why you should keep your code well-commented.","6e9ae4cc":"# Covid19 Tweet Truth Analysis","12aa57a4":"---","2b4d65bb":"---","3fd75157":"Coded by Luna McBride","c1c46968":"This is just in case someone is interested to go line by line. Of the ones showing in my dashboard (which is very cropped), the second tweet was flagged as real despite being fake. The wording does seem a bit more reasonable. It probably could have fooled me too.\n\nNote: both this and the test predictions will display their full lists at the bottom of the notebook for ease of access","f252d679":"---","089b23df":"---","5320de87":"---","86014ab1":"Interestingly, the get_dummies function in pandas will create encoded labels, since this is a binary classification problem. The real column created by it would have 1 for real and 0 for not real, which necessarily means fake in this case. That is the same as label encoding in this case.","a3558db5":"That real column shows the encoded values for real vs fake. I will be taking the real column as the encoded values.","922051a0":"# Label Encoding","a964d6e9":"The labels appear to be pretty balanced in number. I will definitely need to get dummies for these to make real and fake into 1 and 0, but the fact that the labels are balanced in number means the model should pick up on these labels without too much difficulty.","4c724eed":"---","a69efe4d":"# Tokenizing and Padding","fcd617ed":"The ones displayed do seem to make sense in context. The \"President Trump Asked What He Would Do If He Were To Catch The Coronavirus https:\/\/t.co\/3MEWhusRZI #donaldtrump #coronavirus\" tweet has less to do with the virus itself or truth claims, which is a bit odd, but the rest make sense. I will have all of the test and validation sets fully shown below for those who want to look deeper. A 91% accuracy on a validation set is very good, so I can reasonably assume that it should be fairly accurate on the test set.","3bebb691":"This dataset contains the training, validation, and test csv's, along with excel documents for the train and test files, a csv with the test file actual values, and ERNIE test results. For this analysis, I will be ignoring the excel files (as they are the same as the csv's) and the ERNIE results. I will be acting as if the test answer file did not exist for the duration of the testing phase as well, thus sticking with a basic approach of train, validate, see what the model decides for the tests.","33e7e93a":"There are no null values in the dataset.","92545507":"It appears there are more dry tweets along with more typical tweets (with hashtags and links). The typical tweet examples exist, so I will have to do more usual tweet cleaning.","f45410bf":"## Test","b7701ba3":"# Tweet Processing","68b04b51":"---","ff03da72":"---","2aaf4508":"There were no blank tweets created in any set. Tweets can become blank if they were just user names and links, so I just needed to make sure.","76a76e83":"## Validation","11de56a4":"# Model Training","7990259a":"# Check for Null Values"}}