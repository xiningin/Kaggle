{"cell_type":{"1889612b":"code","29e10832":"code","fbb7167e":"code","d1d205c1":"code","96228ed1":"code","763eafcc":"code","5d10fbf3":"code","2986dfa5":"code","2046b9b1":"code","1f2f6716":"code","e824b76c":"code","a85aff90":"code","34aef9ae":"code","b5e852cb":"code","4aa20b30":"code","08c24f87":"code","67101015":"code","9c9ae5e0":"code","8c80b05f":"code","a31343d7":"code","0e8acad8":"code","1ba58601":"code","8d758281":"code","93740293":"code","2b2c325b":"code","b4479b0c":"code","03665092":"code","97d7b6bc":"code","3ee81860":"code","a0eb7dca":"code","bf8aca24":"code","0d8b2c21":"code","14fca5c8":"code","021a2e0c":"code","2572cab6":"markdown","17cd075c":"markdown","e61c8e7b":"markdown","6850c163":"markdown","b252409d":"markdown","46b3248d":"markdown","f3eb6cad":"markdown","7b72dcfa":"markdown","874ba0c5":"markdown","09e28d78":"markdown","8f7a49cf":"markdown","54352e38":"markdown","9d255b66":"markdown","c79c5163":"markdown","e66622b7":"markdown","d5e507de":"markdown","d0988a9b":"markdown","3d852448":"markdown","645c3d6c":"markdown","f2b826b4":"markdown","c8c41ece":"markdown","a5c3704f":"markdown","80c624b8":"markdown","ce45675c":"markdown","659439f1":"markdown","a1df8ef5":"markdown","fe9a9891":"markdown","4f81da1f":"markdown","3ef8ed33":"markdown","04bd264f":"markdown","47aa5661":"markdown","b138b3cf":"markdown","dc28b548":"markdown"},"source":{"1889612b":"import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n%matplotlib inline\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom IPython.display import FileLink\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor","29e10832":"df_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_train.drop(['Id'], axis=1, inplace=True)","fbb7167e":"all_features = df_train.columns\nnumerical_features = [x for x in all_features if len(df_train[x].unique()) > 20]        \ncategorical_features = [x for x in all_features if x not in numerical_features]       ","d1d205c1":"def counts_df(dataframe, feature):\n    data = dataframe[feature].value_counts()\n    data = pd.DataFrame({feature:data.index, 'counts':data.values})\n    data.loc[len(data)] = ['NULL', dataframe[feature].isnull().sum()]\n    data['percent'] = data['counts']\/len(dataframe) * 100\n    return data","96228ed1":"# run this cell to view all bar plots.\n'''\nfor feature in categorical_features:\n    data = counts_df(df_train, feature)\n    fig, ax = plt.subplots(figsize=(10,10))\n    sns.barplot(x='counts', y=feature, data=data).set_title(feature)\n    plt.show()\n'''    ","763eafcc":"corr_map = df_train[numerical_features].corr()\nsns.heatmap(corr_map)","5d10fbf3":"df_train['TotalBsmtSF'].corr(df_train['1stFlrSF'])","2986dfa5":"df_train['TotalBsmtSF'].corr(df_train['SalePrice'])","2046b9b1":"df_train['1stFlrSF'].corr(df_train['SalePrice'])","1f2f6716":"df_train['GrLivArea'].corr(df_train['2ndFlrSF'])","e824b76c":"df_train['GrLivArea'].corr(df_train['SalePrice'])","a85aff90":"df_train['2ndFlrSF'].corr(df_train['SalePrice'])","34aef9ae":"df_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","b5e852cb":"drop_ls = ['Id', 'Street', 'Alley', 'Condition2', 'RoofMatl', 'BsmtFinType2', 'Heating', \n           'GarageQual', 'GarageCond', 'PoolQC', 'MiscFeature', 'Condition1', 'Utilities', \n           'Exterior2nd','ExterCond', 'FireplaceQu', 'PoolArea', 'Fence', '3SsnPorch', \n           'LowQualFinSF', 'MiscVal', 'MSZoning', 'LotShape', 'LandContour', '1stFlrSF', \n           '2ndFlrSF', ]","4aa20b30":"all_features = df_train.drop(['SalePrice'], axis=1).columns\n\ncategorical_features = [x for x in all_features if len(df_train[x].unique()) < 30 \n                      and x not in drop_ls]        \nnumerical_features = [x for x in all_features if x not in categorical_features \n                        and x not in drop_ls]  ","08c24f87":"class CustomImputer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        self.fill_values = {\n            'MasVnrType':'BrkFace',\n            'BsmtQual':'None',\n            'BsmtCond':'None',\n            'BsmtExposure':'None',\n            'BsmtFinType1':'None',\n            'Electrical':'SBrkr',\n            'GarageType':'Attchd',\n            'GarageFinish':'Unk',\n            'LotFrontage':X['LotFrontage'].median(),\n            'MasVnrType':0\n        }\n        \n    def transform(self, X, y=None):\n        for feature, fill_value in self.fill_values.items():\n            X[feature].fillna(fill_value, inplace=True)\n        return X\n    \n    def fit_transform(self, X, y=None):\n        self.fit(X, y)\n        return self.transform(X, y)","67101015":"class FeatureEngineering(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        self.drop_ls = ['Id', 'Street', 'Alley', 'Condition2', 'RoofMatl',\n                   'BsmtFinType2', 'Heating', 'GarageQual', 'GarageCond',\n                   'PoolQC', 'MiscFeature', 'Condition1', 'Utilities', \n                   'Exterior2nd','ExterCond', 'FireplaceQu', \n                   'PoolArea', 'Fence', '3SsnPorch', 'LowQualFinSF',\n                   'MiscVal', 'MSZoning', 'LotShape', 'LandContour',\n                   '1stFlrSF', '2ndFlrSF']\n    \n    def transform(self, X, y=None):\n        X['HasLowQualSF'] = X['LowQualFinSF'].astype('int') > 0\n        X['HasMiscFeature'] = X['MiscVal'].astype('int') > 0\n        X['Unfinished'] = X['HouseStyle'].isin(['1.5Unf', '2.5Unf'])\n        X['Residential'] = X['MSZoning'].isin(['RL','RM','RH'])\n        X['RegShape'] = X['LotShape'] == 'Reg'\n        X['IsLevel'] = X['LandContour'] == 'Lvl'\n        X['BsmtFinType1_Unf'] = 1*(X['BsmtFinType1'] == 'Unf')\n        X['HasWoodDeck'] = (X['WoodDeckSF'].astype('int') == 0) * 1\n        X['HasOpenPorch'] = (X['OpenPorchSF'].astype('int') == 0) * 1\n        X['HasEnclosedPorch'] = (X['EnclosedPorch'].astype('int') == 0) * 1\n        X['Has3SsnPorch'] = (X['3SsnPorch'].astype('int') == 0) * 1\n        X['HasScreenPorch'] = (X['ScreenPorch'].astype('int') == 0) * 1\n        X['YearsSinceRemodel'] = X['YrSold'].astype(int) - X['YearRemodAdd'].astype(int)\n        X['Total_Home_Quality'] = X['OverallQual'] + X['OverallCond']\n        X['TotalSF'] = X['TotalBsmtSF'] + X['1stFlrSF'] + X['2ndFlrSF']\n        X['YrBltAndRemod'] = X['YearBuilt'] + X['YearRemodAdd']\n\n        for feature in self.drop_ls:\n            X.drop([feature], axis=1, inplace=True)\n        return X\n    \n    def fit_transform(self, X, y=None):\n        self.fit(X, y)\n        return self.transform(X, y)","9c9ae5e0":"class ReallocateClasses(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        lot_config_keys = ['FR2', 'FR3']\n        neighborhood_keys_one = ['MeadowV', 'IDOTRR', 'BrDale']\n        neighborhood_keys_two = ['BrkSide', 'Edwards', 'OldTown', \n                                 'Sawyer', 'Blueste', 'SWISU', 'NPkVill',\n                                 'NAmes', 'Mitchel']\n        neighborhood_keys_three = ['SawyerW', 'NWAmes', 'Gilbert', 'Blmngtn',\n                                   'CollgCr', 'Crawfor','ClearCr']\n        neighborhood_keys_four = ['Somerst','Veenker', 'Timber']\n        neighborhood_keys_five = ['StoneBr','NridgHt','NoRidge']\n        roof_style_keys_one = ['Gambrel','Gable','Mansard']\n        roof_style_keys_two = ['Flat', 'Hip', 'Shed']\n        exterior_keys_one = ['BrkComm', 'AsphShn', 'CBlock', 'AsbShng']\n        exterior_keys_two = ['MetalSd', 'Wd Sdng', 'WdShing', 'Stucco', \n                             'HdBoard', 'Plywood']\n        exterior_keys_three = ['BrkFace', 'VinylSd', 'CemntBd', 'Stone',\n                               'ImStucc']\n        electrical_keys = ['FuseA', 'FuseF', 'FuseP', 'Mix']\n        functional_keys = ['Maj2', 'Sev', 'Min2', 'Min1', 'Maj1', 'Mod']\n        garage_keys = ['BuiltIn', 'Basement', 'CarPort', '2Types']\n        sale_keys = ['Oth', 'ConLD', 'ConLW',' COD', 'ConLI', 'CWD', 'Con']\n        sale_condition_keys = ['Partial','Abnormal','Family','Alloca','AdjLand']\n        house_style_keys_one = ['1.5Fin', '1.5Unf', '2.5Unf', '2.5Fin', 'SLvl']\n        house_style_keys_two = ['SFoyer']\n        \n        lot_config_map = dict.fromkeys(lot_config_keys, 'other')\n        neighborhood_map = {\n            **dict.fromkeys(neighborhood_keys_one, 1),\n            **dict.fromkeys(neighborhood_keys_two, 2),\n            **dict.fromkeys(neighborhood_keys_three, 3),\n            **dict.fromkeys(neighborhood_keys_four, 4),\n            **dict.fromkeys(neighborhood_keys_five, 5),\n        }\n        roof_style_map = {\n            **dict.fromkeys(roof_style_keys_one, 1),\n            **dict.fromkeys(roof_style_keys_two, 2)\n        }\n        exterior_map = {\n            **dict.fromkeys(exterior_keys_one, 1),\n            **dict.fromkeys(exterior_keys_two, 2),\n            **dict.fromkeys(exterior_keys_three, 3),\n        }\n        electrical_map = dict.fromkeys(electrical_keys, 'other')\n        functional_map = dict.fromkeys(functional_keys, 'damaged')\n        garage_map = dict.fromkeys(garage_keys, 'other')\n        sale_map = dict.fromkeys(sale_keys, 'other')\n        sale_condition_map = dict.fromkeys(sale_condition_keys, 'other')\n        house_style_map = {\n            **dict.fromkeys(house_style_keys_one, '2Story'),\n            **dict.fromkeys(house_style_keys_two, '1Story')\n        }\t\n\n        self.maps = [lot_config_map, neighborhood_map, roof_style_map, exterior_map,\n                    electrical_map, functional_map, garage_map, sale_map, sale_condition_map,\n                    house_style_map]\n\n    def transform(self, X, y=None):\n        for mapping in self.maps:\n            X.replace(mapping, inplace=True)    \n        return X    \n    \n    def fit_transform(self, X, y=None):\n        self.fit(X, y)\n        return self.transform(X, y) ","8c80b05f":"class LogTransform(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        pass\n    \n    def transform(self, X, y=None):\n        X = X.astype('float32')\n        X = np.log(X+1)\n        return X\n    \n    def fit_transform(self, X, y=None):\n        return self.transform(X, y)    ","a31343d7":"preprocessing_pipeline = Pipeline([\n    ('imputer', CustomImputer()),\n    ('add_features', FeatureEngineering())\n])","0e8acad8":"categorical_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n])","1ba58601":"numerical_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('log_transform', LogTransform()),\n    ('scale', StandardScaler())\n])","8d758281":"def correct_types(X):\n    X[categorical_features] = X[categorical_features].astype(str)\n    X[numerical_features] = X[numerical_features].astype(float)\n    return X","93740293":"X_train = df_train.copy()\nX_train = correct_types(X_train.drop(['SalePrice'],axis=1))\npreprocessing_pipeline.fit(X_train)\ncategorical_pipeline.fit(X_train[categorical_features])\nnumerical_pipeline.fit(X_train[numerical_features])","2b2c325b":"def transform_dataset(X):\n    X = preprocessing_pipeline.transform(X)\n    cat_features = categorical_pipeline.transform(X[categorical_features])\n    num_features = numerical_pipeline.transform(X[numerical_features])\n    cat_features = pd.DataFrame(cat_features.todense())\n    num_features = pd.DataFrame(num_features)\n    X = pd.concat([cat_features, num_features], axis=1)\n    X.columns = range(len(X.columns))\n    return X","b4479b0c":"X_train = transform_dataset(X_train)\ny_train = df_train['SalePrice']","03665092":"X_test = df_test.copy()\nX_test = correct_types(X_test)\nX_test = transform_dataset(X_test)","97d7b6bc":"lightgbm = LGBMRegressor(objective='regression', \n                         n_estimators=5000)","3ee81860":"xgboost = XGBRegressor(n_estimators=5000, \n                       objective='reg:linear',\n                       nthread=-1)","a0eb7dca":"gbr = GradientBoostingRegressor(n_estimators=5000,\n                                loss='huber')  ","bf8aca24":"lightgbm.fit(X_train, y_train)\nxgboost.fit(X_train, y_train)\ngbr.fit(X_train, y_train)","0d8b2c21":"lightgbm_preds = lightgbm.predict(X_test)\nxgboost_preds = xgboost.predict(X_test)\ngbr_preds = gbr.predict(X_test)","14fca5c8":"preds = (lightgbm_preds + xgboost_preds + gbr_preds)\/3","021a2e0c":"submission = pd.DataFrame({'Id': df_test.Id, 'SalePrice': preds})\nsubmission.to_csv('submission.csv', index=False)","2572cab6":"## [Basic modelling.](#Basic-modelling)","17cd075c":"## [Basic EDA](#Basic-EDA)","e61c8e7b":"## [Creating pipelines.](#Creating-pipelines)","6850c163":"There appears to be a high correlation between TotalBsmtSF and 1stFlrSF. \n\nAs we want to remove multicollinearity we will drop the feature that has a lower correlation with sale price.","b252409d":"Define a function to correct the datatypes of the features in the datasets.","46b3248d":"Fit all of the transformers in pipelines to the training dataset.","f3eb6cad":"Now we can create the required pipelines. Pipelines allow us to sequentially apply a series of transformations to a dataset.\n\nFirstly, we will create a pipeline to impute part of the missing values and perform feature engineering.","7b72dcfa":"We will now make use of our pipelines in order to preprocess the training and testing datasets.","874ba0c5":"### Analyse the numerical features.","09e28d78":"Load the training and testing datasets.","8f7a49cf":"Transform the training set.","54352e38":"We use this information to determine the set of categorical features and numerical features that will be present in the final dataset.","9d255b66":"Define a function that will recieve a feature as a parameter and return the number of samples associated with each value for that feature.\n\nThis function will be used to generate the data for bar plots.","c79c5163":"We will now make use of this function to analyse each of the categorical features in the dataset.\n\nWe will plot bar plots for each of the features. These bar plots allow us to see which categories contain the majority of the samples as well as which categories contain few samples.\n\nThe information gathered from these bar plots will be used to reassign samples into different categories removing any categories with a negligible number of samples.\n\nWe will perform the reassignment of categories through the use of our pipelines.\n\n**_The strategy of reassignment was mainly to reduce the number of categories where possible. This was done by grouping categories with similar mean sale price values and removing categories with an insignificant number of samples._**","e66622b7":"We will drop 1stFlrSF.\n\nWe do the same for GrLivArea and 2ndFlrSF.","d5e507de":"Separate the categorical and numerical features.","d0988a9b":"We begin by creating a transformer that will impute custom values for each feature.","3d852448":"Lastly, we will create a pipeline that will preprocess the numerical features. This pipeline will impute the missing values, apply a logarithmic transformation and finally remove the mean and scale the features to have unit variance.","645c3d6c":"We will drop 2ndFlrSF.\n\nThere are many other kernels providing in-depth EDA's for this dataset, from these kernels the skew of the distributions can be seen (sns.displot(feature_name) will also allow you to see this).\n\nThe distributions of almost all numerical features are skewed. We will create a custom transformer that will apply a logarithmic transformation to correct the distributions.\n\nAs EDA is not the focus of this kernel we will not analyse the data any further.","f2b826b4":"Load the dataset.","c8c41ece":"Transform the test set.","a5c3704f":"## [Create custom transformers.](#Create-custom-transformers.)","80c624b8":"Create a list of all of the features that will be dropped. These features have been selected as either the feature contains too many missing values or the feature has been used in feature engineering and is no longer necessary.","ce45675c":"## [Using pipelines.](#Using-pipelines)","659439f1":"Define function to transform a given dataset using the created pipelines.","a1df8ef5":"Next, we will create a transformer that will preform category reallocation. This transfomer will allow us to group the samples into the desired categories.","fe9a9891":"Secondly, we create a transformer that will preform feature engineering. This transformer will create new features and drop the unecessary features from the dataset.\n\n[Part of the feature engineering has been taken from this kernel](https:\/\/www.kaggle.com\/lavanyashukla01\/how-i-made-top-0-3-on-a-kaggle-competition)","4f81da1f":"We will begin by analysing the correlation between each of the numerical features.","3ef8ed33":"Finally, we create the transformer that will apply the logarithmic transformations to the dataset.","04bd264f":"### Analyse the categorical features.","47aa5661":"Secondly, we will create a pipeline that will impute the remainder of the missing values for the categorical features as well as encode the categorical features.","b138b3cf":"## [Separate features.](#Separate-features.)","dc28b548":"# Preprocessing pipelines with sklearn.\n***\n#### _Preprocessing pipelines allow a series of transformations to quickly and easily be applied to a dataset._\n**_Table of contents:_**\n>  1. [Basic EDA](#Basic-EDA)\n>  2. [Separate features.](#Separate-features.)\n>  3. [Create custom transformers.](#Create-custom-transformers.)\n>  4. [Creating pipelines.](#Creating-pipelines)\n>  5. [Using pipelines.](#Using-pipelines)\n>  6. [Basic modelling.](#Basic-modelling)\n"}}