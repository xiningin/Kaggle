{"cell_type":{"31731c95":"code","e82f4590":"code","9c27e825":"code","b345700a":"code","70259403":"code","d65e9cfb":"code","c74e8eb5":"code","e45271df":"code","6f9c8028":"code","9eb90d65":"code","65f1434c":"code","e9ab9fd7":"code","fed76e1c":"code","ea1f446a":"code","de1036d4":"code","d21a0377":"code","e9ab2a53":"code","2404bc1e":"code","0a456a90":"code","61d45406":"code","28f55e3f":"code","67992bee":"code","87824671":"code","e792e926":"code","bdcbc8ef":"code","1c70d64d":"code","c2d1dc80":"code","641bbec6":"code","58089430":"code","aaaa564c":"code","eaac78a9":"code","6ba97c41":"code","3d374ad9":"code","b313d4ba":"markdown","fdfa501b":"markdown","59cf20bb":"markdown","19300bff":"markdown","283799e8":"markdown","5bc0b614":"markdown","f9fc074f":"markdown","6dbd38bd":"markdown","c618e8cd":"markdown","6f581dba":"markdown"},"source":{"31731c95":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport datetime as dt\n\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import dendrogram\nfrom scipy.cluster.hierarchy import cut_tree","e82f4590":"retail_df = pd.read_csv(\"..\/input\/learning-data\/OnlineRetail.csv\", sep=\",\", encoding=\"ISO-8859-1\", header=0)","9c27e825":"retail_df.head()","b345700a":"retail_df.info()","70259403":"#missing values\nretail_df.isnull().sum()","d65e9cfb":"#Checking in Percentage\nround(100*(retail_df.isnull().sum())\/len(retail_df),2)","c74e8eb5":"#Drop all the rows having missing values\nretail_df = retail_df.dropna()\nretail_df.info()","e45271df":"# new column: amount \nretail_df['amount'] = retail_df['Quantity']*retail_df['UnitPrice']\nretail_df.head()","6f9c8028":"#monetary\ngrouped_df = retail_df.groupby('CustomerID')['amount'].sum()\ngrouped_df = grouped_df.reset_index()\ngrouped_df.head()","9eb90d65":"#frequency\nfrequency = retail_df.groupby('CustomerID')['InvoiceNo'].count()\nfrequency = frequency.reset_index()\nfrequency.columns = ['CustomerID','frequency']\nfrequency.head()","65f1434c":"#Merging two dataframe\ngrouped_df = pd.merge(grouped_df, frequency, on = 'CustomerID', how = 'inner')\ngrouped_df.head()","e9ab9fd7":"grouped_df.info()","fed76e1c":"#recency\n#Converting Datetime\n\nretail_df['InvoiceDate'] = pd.to_datetime(retail_df['InvoiceDate'], format = '%d-%m-%Y %H:%M')","ea1f446a":"retail_df.info()","de1036d4":"#maxdate\nmax_date = max(retail_df['InvoiceDate'])\nmax_date","d21a0377":"#difference between maxdate and Invoicedate\nretail_df['diff'] = max_date - retail_df['InvoiceDate']\nretail_df.head()\n","e9ab2a53":"#recency\nlast_purchase = retail_df.groupby('CustomerID')['diff'].min()\nlast_purchase = last_purchase.reset_index()\nlast_purchase","2404bc1e":"#merging two data frames\ngrouped_df = pd.merge(grouped_df, last_purchase, on ='CustomerID', how ='inner')\ngrouped_df.columns = ['CustomerID','amount', 'frequency','recency']\ngrouped_df.head()","0a456a90":"# number of days only\ngrouped_df['recency'] = grouped_df['recency'].dt.days\ngrouped_df.head()","61d45406":"# 1. outlier treatment\nplt.boxplot(grouped_df['recency'])","28f55e3f":"# removing (statistical) outliers\nQ1 = grouped_df.amount.quantile(0.05)\nQ3 = grouped_df.amount.quantile(0.95)\nIQR = Q3 - Q1\ngrouped_df = grouped_df[(grouped_df.amount >= Q1 - 1.5*IQR) & (grouped_df.amount <= Q3 + 1.5*IQR)]\n\n# outlier treatment for recency\nQ1 = grouped_df.recency.quantile(0.05)\nQ3 = grouped_df.recency.quantile(0.95)\nIQR = Q3 - Q1\ngrouped_df = grouped_df[(grouped_df.recency >= Q1 - 1.5*IQR) & (grouped_df.recency <= Q3 + 1.5*IQR)]\n\n# outlier treatment for frequency\nQ1 = grouped_df.frequency.quantile(0.05)\nQ3 = grouped_df.frequency.quantile(0.95)\nIQR = Q3 - Q1\ngrouped_df = grouped_df[(grouped_df.frequency >= Q1 - 1.5*IQR) & (grouped_df.frequency <= Q3 + 1.5*IQR)]","67992bee":"# 2. rescaling\nrfm_df = grouped_df[['amount', 'frequency', 'recency']]\n\n# instantiate\nscaler = StandardScaler()\n\n# fit_transform\nrfm_df_scaled = scaler.fit_transform(rfm_df)\nrfm_df_scaled.shape","87824671":"rfm_df_scaled = pd.DataFrame(rfm_df_scaled)\nrfm_df_scaled.columns = ['amount', 'frequency', 'recency']\nrfm_df_scaled.head()","e792e926":"# k-means with some arbitrary k\nkmeans = KMeans(n_clusters=4, max_iter=50)\nkmeans.fit(rfm_df_scaled)","bdcbc8ef":"kmeans.labels_","1c70d64d":"# elbow-curve\/SSD\nssd = []\nrange_n_clusters = [2, 3, 4, 5, 6, 7, 8]\nfor num_clusters in range_n_clusters:\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=50)\n    kmeans.fit(rfm_df_scaled)\n    \n    ssd.append(kmeans.inertia_)\n    \n# plot the SSDs for each n_clusters\n# ssd\nplt.plot(ssd)","c2d1dc80":"# silhouette analysis\nrange_n_clusters = [2, 3, 4, 5, 6, 7, 8]\n\nfor num_clusters in range_n_clusters:\n    \n    # intialise kmeans\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=50)\n    kmeans.fit(rfm_df_scaled)\n    \n    cluster_labels = kmeans.labels_\n    \n    # silhouette score\n    silhouette_avg = silhouette_score(rfm_df_scaled, cluster_labels)\n    print(\"For n_clusters={0}, the silhouette score is {1}\".format(num_clusters, silhouette_avg))","641bbec6":"# final model with k=3\nkmeans = KMeans(n_clusters=3, max_iter=50)\nkmeans.fit(rfm_df_scaled)","58089430":"kmeans.labels_","aaaa564c":"# assign the label\ngrouped_df['cluster_id'] = kmeans.labels_\ngrouped_df.head()","eaac78a9":"# plot\nsns.boxplot(x='cluster_id', y='amount', data=grouped_df)","6ba97c41":"# plot\nsns.boxplot(x='cluster_id', y='frequency', data=grouped_df)","3d374ad9":"# plot\nsns.boxplot(x='cluster_id', y='recency', data=grouped_df)","b313d4ba":"# Step 4: Modelling","fdfa501b":"> Importing Libraries","59cf20bb":"# Silhouette Analysis\n$$\\text{silhouette score}=\\frac{p-q}{max(p,q)}$$\n\n$p$ is the mean distance to the points in the nearest cluster that the data point is not a part of\n\n$q$ is the mean intra-cluster distance to all the points in its own cluster.\n\n* The value of the silhouette score range lies between -1 to 1. \n\n* A score closer to 1 indicates that the data point is very similar to other data points in the cluster, \n\n* A score closer to -1 indicates that the data point is not similar to the data points in its cluster.","19300bff":"**Step 3: Data Preparation**","283799e8":"**Steps**\n1. Importing Dataset\n2. Removing Missing Values\n3. Aggregating RFM Values\n4. Outlier Treatment\n","5bc0b614":"**1.Importing Dataset**","f9fc074f":"**two types of outliers:**\n<br>- statistical\n<br>- domain specific","6dbd38bd":"**Step 2: Clean the Data**","c618e8cd":"**KMeans Clustering**\n<br>I'm going to use Behaviour Segmentation specially RFM model\n<br>**Recenc, Frequency, Monetary**\n","6f581dba":"# Finding the Optimal number of Clusters\n# elbow-curve\/ SSD"}}