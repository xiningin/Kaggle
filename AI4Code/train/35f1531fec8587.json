{"cell_type":{"3a633441":"code","b7d598b7":"code","cfa21f87":"code","87deff81":"code","fa016ee8":"code","acdfe9e5":"code","0f25191b":"code","c255de9d":"code","97b6de6b":"code","fbaae347":"code","36b089fd":"code","704e9d05":"code","6d5e0606":"code","cff034e5":"code","5d251e89":"code","b2221e26":"code","59c675aa":"code","68502631":"code","816c738b":"code","9ffdfe94":"code","b8de60dc":"code","2ab23ecc":"code","74059946":"code","ef26588d":"code","77dc4250":"code","cd63fd1f":"code","fe8e9cb1":"code","ffa96953":"code","337ceffa":"code","576f1801":"code","29ea6c83":"code","7de81aa8":"code","6a4bda23":"code","b83b6d32":"code","0fee744d":"code","b99edc8b":"code","3f14b0f6":"code","4f630e86":"code","4e591778":"code","ee3b951d":"code","fc84cd3c":"code","cab23a87":"code","8a0cea41":"code","f5ad52a0":"code","5cd06eaa":"code","9875459f":"code","c19c7c03":"code","28c838e0":"code","3c3dcfdc":"code","26500540":"code","ec099bd2":"code","ebcca915":"code","35b513b7":"code","ad777b8b":"code","2387f1a7":"code","9084217f":"code","b6731c1a":"code","41dcc167":"code","3568eac0":"code","3e751360":"code","d8fb106a":"code","17daef68":"code","bbdeba89":"code","5b8b75c1":"code","297c120d":"code","9bd1a417":"code","115c2518":"code","4d36f658":"markdown","dc33157c":"markdown","96f69e0f":"markdown","abe1677b":"markdown","5c4da0b0":"markdown","5de749ce":"markdown","394e9330":"markdown","e1b357bc":"markdown","b12c9161":"markdown","39724a22":"markdown","d17e1e29":"markdown","80ff188f":"markdown","1db731ad":"markdown","d50df939":"markdown","a8083c39":"markdown","b517fb73":"markdown","6c7c2811":"markdown","9270c174":"markdown","3dcafa15":"markdown","ce6120c8":"markdown","f1c492a8":"markdown","58e68092":"markdown"},"source":{"3a633441":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nimport re\nfrom wordcloud import WordCloud","b7d598b7":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM, Conv1D, MaxPool1D\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score","cfa21f87":"fake = pd.read_csv(\"\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv\")","87deff81":"fake.head()","fa016ee8":"#Counting by Subjects \nfor key,count in fake.subject.value_counts().iteritems():\n    print(f\"{key}:\\t{count}\")\n    \n#Getting Total Rows\nprint(f\"Total Records:\\t{fake.shape[0]}\")","acdfe9e5":"plt.figure(figsize=(8,5))\nsns.countplot(\"subject\", data=fake)\nplt.show()","0f25191b":"#Word Cloud\ntext = ''\nfor news in fake.text.values:\n    text += f\" {news}\"\nwordcloud = WordCloud(\n    width = 3000,\n    height = 2000,\n    background_color = 'black',\n    stopwords = set(nltk.corpus.stopwords.words(\"english\"))).generate(text)\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()\ndel text","c255de9d":"real = pd.read_csv(\"\/kaggle\/input\/fake-and-real-news-dataset\/True.csv\")\nreal.head()","97b6de6b":"#First Creating list of index that do not have publication part\nunknown_publishers = []\nfor index,row in enumerate(real.text.values):\n    try:\n        record = row.split(\" -\", maxsplit=1)\n        #if no text part is present, following will give error\n        record[1]\n        #if len of piblication part is greater than 260\n        #following will give error, ensuring no text having \"-\" in between is counted\n        assert(len(record[0]) < 260)\n    except:\n        unknown_publishers.append(index)","fbaae347":"#Thus we have list of indices where publisher is not mentioned\n#lets check\nreal.iloc[unknown_publishers].text\n#true, they do not have text like \"WASHINGTON (Reuters)\"","36b089fd":"real.iloc[8970]\n#yep empty\n#will remove this soon","704e9d05":"#Seperating Publication info, from actual text\npublisher = []\ntmp_text = []\nfor index,row in enumerate(real.text.values):\n    if index in unknown_publishers:\n        #Add unknown of publisher not mentioned\n        tmp_text.append(row)\n        \n        publisher.append(\"Unknown\")\n        continue\n    record = row.split(\" -\", maxsplit=1)\n    publisher.append(record[0])\n    tmp_text.append(record[1])","6d5e0606":"#Replace existing text column with new text\n#add seperate column for publication info\nreal[\"publisher\"] = publisher\nreal[\"text\"] = tmp_text\n\ndel publisher, tmp_text, record, unknown_publishers","cff034e5":"real.head()","5d251e89":"#checking for rows with empty text like row:8970\n[index for index,text in enumerate(real.text.values) if str(text).strip() == '']\n#seems only one :)","b2221e26":"#dropping this record\nreal = real.drop(8970, axis=0)","59c675aa":"# checking for the same in fake news\nempty_fake_index = [index for index,text in enumerate(fake.text.values) if str(text).strip() == '']\nprint(f\"No of empty rows: {len(empty_fake_index)}\")\nfake.iloc[empty_fake_index].tail()","68502631":"#Looking at publication Information\n# Checking if Some part of text has been included as publisher info... No such cases it seems :)\n\n# for name,count in real.publisher.value_counts().iteritems():\n#     print(f\"Name: {name}\\nCount: {count}\\n\")","816c738b":"#Getting Total Rows\nprint(f\"Total Records:\\t{real.shape[0]}\")\n\n#Counting by Subjects \nfor key,count in real.subject.value_counts().iteritems():\n  print(f\"{key}:\\t{count}\")","9ffdfe94":"sns.countplot(x=\"subject\", data=real)\nplt.show()","b8de60dc":"#WordCloud For Real News\ntext = ''\nfor news in real.text.values:\n    text += f\" {news}\"\nwordcloud = WordCloud(\n    width = 3000,\n    height = 2000,\n    background_color = 'black',\n    stopwords = set(nltk.corpus.stopwords.words(\"english\"))).generate(str(text))\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()\ndel text","2ab23ecc":"# Adding class Information\nreal[\"class\"] = 1\nfake[\"class\"] = 0","74059946":"#Combining Title and Text\nreal[\"text\"] = real[\"title\"] + \" \" + real[\"text\"]\nfake[\"text\"] = fake[\"title\"] + \" \" + fake[\"text\"]","ef26588d":"# Subject is diffrent for real and fake thus dropping it\n# Aldo dropping Date, title and Publication Info of real\nreal = real.drop([\"subject\", \"date\",\"title\",  \"publisher\"], axis=1)\nfake = fake.drop([\"subject\", \"date\", \"title\"], axis=1)","77dc4250":"#Combining both into new dataframe\ndata = real.append(fake, ignore_index=True)\ndel real, fake","cd63fd1f":"# Download following if not downloaded in local machine\n\n# nltk.download('stopwords')\n# nltk.download('punkt')","fe8e9cb1":"y = data[\"class\"].values\n#Converting X to format acceptable by gensim, removing annd punctuation stopwords in the process\nX = []\nstop_words = set(nltk.corpus.stopwords.words(\"english\"))\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\nfor par in data[\"text\"].values:\n    tmp = []\n    sentences = nltk.sent_tokenize(par)\n    for sent in sentences:\n        sent = sent.lower()\n        tokens = tokenizer.tokenize(sent)\n        filtered_words = [w.strip() for w in tokens if w not in stop_words and len(w) > 1]\n        tmp.extend(filtered_words)\n    X.append(tmp)\n\ndel data","ffa96953":"import gensim","337ceffa":"#Dimension of vectors we are generating\nEMBEDDING_DIM = 100\n\n#Creating Word Vectors by Word2Vec Method (takes time...)\nw2v_model = gensim.models.Word2Vec(sentences=X, size=EMBEDDING_DIM, window=5, min_count=1)","576f1801":"#vocab size\nlen(w2v_model.wv.vocab)\n\n#We have now represented each of 122248 words by a 100dim vector.","29ea6c83":"#see a sample vector for random word, lets say Corona \nw2v_model[\"corona\"]","7de81aa8":"w2v_model.wv.most_similar(\"iran\")","6a4bda23":"w2v_model.wv.most_similar(\"fbi\")","b83b6d32":"w2v_model.wv.most_similar(\"facebook\")","0fee744d":"w2v_model.wv.most_similar(\"computer\")","b99edc8b":"#Feeding US Presidents\nw2v_model.wv.most_similar(positive=[\"trump\",\"obama\", \"clinton\"])\n#First was Bush","3f14b0f6":"# Tokenizing Text -> Repsesenting each word by a number\n# Mapping of orginal word to number is preserved in word_index property of tokenizer\n\n#Tokenized applies basic processing like changing it yo lower case, explicitely setting that as False\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X)\n\nX = tokenizer.texts_to_sequences(X)","4f630e86":"# lets check the first 10 words of first news\n#every word has been represented with a number\nX[0][:10]","4e591778":"#Lets check few word to numerical replesentation\n#Mapping is preserved in dictionary -> word_index property of instance\nword_index = tokenizer.word_index\nfor word, num in word_index.items():\n    print(f\"{word} -> {num}\")\n    if num == 10:\n        break        ","ee3b951d":"# For determining size of input...\n\n# Making histogram for no of words in news shows that most news article are under 700 words.\n# Lets keep each news small and truncate all news to 700 while tokenizing\nplt.hist([len(x) for x in X], bins=500)\nplt.show()\n\n# Its heavily skewed. There are news with 5000 words? Lets truncate these outliers :) ","fc84cd3c":"nos = np.array([len(x) for x in X])\nlen(nos[nos  < 700])\n# Out of 48k news, 44k have less than 700 words","cab23a87":"#Lets keep all news to 700, add padding to news with less than 700 words and truncating long ones\nmaxlen = 700 \n\n#Making all news of size maxlen defined above\nX = pad_sequences(X, maxlen=maxlen)","8a0cea41":"#all news has 700 words (in numerical form now). If they had less words, they have been padded with 0\n# 0 is not associated to any word, as mapping of words started from 1\n# 0 will also be used later, if unknows word is encountered in test set\nlen(X[0])","f5ad52a0":"# Adding 1 because of reserved 0 index\n# Embedding Layer creates one more vector for \"UNKNOWN\" words, or padded words (0s). This Vector is filled with zeros.\n# Thus our vocab size inceeases by 1\nvocab_size = len(tokenizer.word_index) + 1","5cd06eaa":"# Function to create weight matrix from word2vec gensim model\ndef get_weight_matrix(model, vocab):\n    # total vocabulary size plus 0 for unknown words\n    vocab_size = len(vocab) + 1\n    # define weight matrix dimensions with all 0\n    weight_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n    # step vocab, store vectors using the Tokenizer's integer mapping\n    for word, i in vocab.items():\n        weight_matrix[i] = model[word]\n    return weight_matrix","9875459f":"#Getting embedding vectors from word2vec and usings it as weights of non-trainable keras embedding layer\nembedding_vectors = get_weight_matrix(w2v_model, word_index)","c19c7c03":"#Defining Neural Network\nmodel = Sequential()\n#Non-trainable embeddidng layer\nmodel.add(Embedding(vocab_size, output_dim=EMBEDDING_DIM, weights=[embedding_vectors], input_length=maxlen, trainable=False))\n#LSTM \nmodel.add(LSTM(units=128))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n\ndel embedding_vectors","28c838e0":"model.summary()","3c3dcfdc":"#Train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y) ","26500540":"model.fit(X_train, y_train, validation_split=0.3, epochs=6)","ec099bd2":"#Prediction is in probability of news being real, so converting into classes\n# Class 0 (Fake) if predicted prob < 0.5, else class 1 (Real)\ny_pred = (model.predict(X_test) >= 0.5).astype(\"int\")","ebcca915":"accuracy_score(y_test, y_pred)","35b513b7":"print(classification_report(y_test, y_pred))","ad777b8b":"del model","2387f1a7":"#invoke garbage collector to free ram\nimport gc\ngc.collect()","9084217f":"from gensim.models.keyedvectors import KeyedVectors","b6731c1a":"# Takes RAM \nword_vectors = KeyedVectors.load_word2vec_format('..\/input\/googlenewsvectorsnegative300\/GoogleNews-vectors-negative300.bin', binary=True)\nEMBEDDING_DIM=300","41dcc167":"# word_vectors.most_similar('usa')","3568eac0":"# word_vectors.most_similar('fbi')","3e751360":"# word_vectors.most_similar('Republic')","d8fb106a":"embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    try:\n        embedding_vector = word_vectors[word]\n        embedding_matrix[i] = embedding_vector\n    except KeyError:\n        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n\ndel word_vectors ","17daef68":"model = Sequential()\nmodel.add(Embedding(vocab_size, output_dim=EMBEDDING_DIM, weights=[embedding_matrix], input_length=maxlen, trainable=False))\nmodel.add(Conv1D(activation='relu', filters=4, kernel_size=4))\nmodel.add(MaxPool1D())\nmodel.add(LSTM(units=128))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n\ndel embedding_matrix","bbdeba89":"model.summary()","5b8b75c1":"model.fit(X_train, y_train, validation_split=0.3, epochs=12)","297c120d":"y_pred = (model.predict(X_test) > 0.5).astype(\"int\")","9bd1a417":"accuracy_score(y_test, y_pred)","115c2518":"print(classification_report(y_test, y_pred))","4d36f658":"Now, instead of creating word vectors, let us use pre-trained vectors trained on part of **Google News dataset** (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases.  Source: https:\/\/code.google.com\/archive\/p\/word2vec\/\n\n**Please download model file from**: https:\/\/drive.google.com\/file\/d\/0B7XkCwpI5KDYNlNUTTlSS21pQmM\/edit?usp=sharing\n\n\nOr add Dataset from https:\/\/www.kaggle.com\/sandreds\/googlenewsvectorsnegative300\n","dc33157c":"Removing StopWords, Punctuations and single-character words","96f69e0f":"### Exploring Vectors\n\nLets checkout these vectors","abe1677b":"**630 Rows in Fake news with empty text**\n\nAlso noticed fake news have a lot of CPATIAL-CASES. Could preserve Cases of letters, but as we are using Google's pretrained word2vec vectors later on, which haswell-formed lower cases word. We will contert to lower case.\n\nThe text for these rows seems to be present in title itself. Lets merge title and text to solve these cases.","5c4da0b0":"We Create a matrix of mapping between word-index and vectors. We use this as weights in embedding layer\n\nEmbedding layer accepts numecical-token of word and outputs corresponding vercor to inner layer.\n\nIt sends vector of zeros to next layer for unknown words which would be tokenized to 0.\n\n\nInput length of Embedding Layer is the length of each news (700 now due to padding and truncating)","5de749ce":"New column called \"Publisher\" has been added.\n","394e9330":"While looking at texts that do not contain publication info such as which reuter, we noticed one thing.\n\n**Text at index 8970 is empty**","e1b357bc":"**Notice it starts with 1**\n","b12c9161":"### Exploring these trained Vectors","39724a22":"#### Let's create and check our own Word2Vec model with **gensim**","d17e1e29":"**Looking at the similar words, vectors are well formed for these words :)**\n\n\nThese Vectors will be passed to LSTM\/GRU instead of words. 1D-CNN can further be used to extract features from the vectors. \n\n\nKeras has implementation called \"**Embedding Layer**\" which would create word embeddings(vectors). Since we did that with gensim's word2vec, we will load these vectors into embedding layer and make the layer non-trainable.\n\n\n","80ff188f":"### Difference in Text\nReal news seems to have source of publication which is not present in fake news set\n\nLooking at the data:\n- most of text contains reuters information such as \"**WASHINGTON (Reuters)**\".\n- Some text are tweets from Twitter \n- Few text do not contain any publication info","1db731ad":"**Do Upvote if you find this notebook useful.**\n\n**Thanks**","d50df939":"### Vectorization -- Word2Vec\n\nWord2Vec is one of the most popular technique to learn word embeddings using shallow neural network. It was developed by Tomas Mikolov in 2013 at Google.\n\nWord embedding is the most popular representation of document vocabulary. It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc.\n\n[Here](https:\/\/towardsdatascience.com\/introduction-to-word-embedding-and-word2vec-652d0c2060fa) is a nice article about it.\n\n\n","a8083c39":"We cannot pass string words to embedding layer, thus need some way to represent each words by numbers.\n\nTokenizer can represent each word by number","b517fb73":"# Library Imports","6c7c2811":"# Exploring Real news","9270c174":"# Cleaning Data\nRemoving Reuters or Twitter Tweet information from the text \n\n- Text can be splitted only once at \" - \" which is always present after mentioning source of publication, this gives us publication part and text part\n- If we do not get text part, this means publication details was't given for that record\n- The Twitter tweets always have same source, a long text of max 259 characters ","3dcafa15":"### Using Pre-Trained Word2Vec Vectors\n\n**Needs 12GB RAM and 4GB HardDisk Space **","ce6120c8":"# Preprocessing Text","f1c492a8":"# Exploring Fake News","58e68092":"We can pass numerical representation of words into neural network.\n\nWe can use Many-To-One (Sequence-To-Word) Model of RNN, as we have many words in news as input and one output ie Probability of being Real.\n\nFor Many-To-One model, lets use a fixed size input. \n"}}