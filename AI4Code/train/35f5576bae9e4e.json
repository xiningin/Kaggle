{"cell_type":{"ec11440a":"code","7bbeccfb":"code","a53813cd":"code","7a2f6157":"code","6868fff1":"code","ee0eff4e":"code","58271a8f":"code","40c8957e":"code","f3ef50ed":"code","dc8b4c46":"code","81a39b19":"code","3d07d186":"code","26904ef6":"code","50801c4c":"code","5565355a":"code","b7486c6e":"code","418ca1ad":"code","b10cb3e4":"code","45b177ec":"code","c3734bf7":"code","15908371":"code","2f229e77":"code","2bef8403":"code","62cb0a94":"code","5398b712":"code","b88f8747":"code","aa66da4d":"code","5cee06ed":"code","5aaf738c":"code","66de6560":"code","3d08fbf3":"code","ef2c215a":"code","e5c121f8":"code","881ae296":"markdown","cc1bd390":"markdown","f8493a76":"markdown","d3887c4f":"markdown"},"source":{"ec11440a":"import pandas as pd\nimport numpy as np\nimport cv2\nfrom PIL import Image\nimport glob\nimport keras as k\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.optimizers import SGD\nfrom keras.layers import Flatten\nfrom keras.layers import Dense \nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import AveragePooling2D\nfrom keras.optimizers import Adam\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Dropout \nfrom keras.layers import Activation\nimport keras\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.resnet50 import ResNet50\nfrom numpy import loadtxt\nfrom keras.models import Model\nfrom keras.callbacks import ReduceLROnPlateau\nimport tensorflow as tf","7bbeccfb":"train = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")","a53813cd":"print(train.shape)\nprint(test.shape)","7a2f6157":"train.head()","6868fff1":"ytrain = train['label']\nxtrain = train.drop(labels = [\"label\"],axis = 1) \nxtrain.head(5)","ee0eff4e":"xtrain.describe()","58271a8f":"xtrain = xtrain \/ 255.0\ntest = test \/ 255.0\n\nxtrain.head(5)","40c8957e":"xtrain.describe()","f3ef50ed":"xtrain.shape","dc8b4c46":"xtrain = xtrain.values.reshape(-1,28,28,1)                         #Reshaping into size(examples, height, width, channel)\nxtest = test.values.reshape(-1,28,28,1)","81a39b19":"print(28*28)\nxtrain[0]","3d07d186":"ytrain","26904ef6":"ytrain = np.asarray(tf.one_hot(ytrain, 10, axis = -1)) \nytrain[0]","50801c4c":"ytrain[3]","5565355a":"print(xtrain.shape)\nprint(ytrain.shape)\nprint(xtest.shape)","b7486c6e":"x_train, x_val, y_train, y_val = train_test_split(xtrain, ytrain, test_size=0.1, random_state=42)\nprint(x_train.shape)\nprint(x_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)","418ca1ad":"def submission_and_visualization(final_predictions, model_number, model_history):\n    y = final_predictions.copy()\n    y= np.argmax(y, axis = 1)\n    y.reshape(28000,1);\n\n    col1 = np.arange(42000,70000,1)\n    col2 = y.copy()\n\n    final = np.stack((col1, col2), axis = 1)\n    finaldf = pd.DataFrame(data=final)\n    finaldf.rename(columns = {0:'filename', 1:'label'} , inplace = True)\n\n    name = []\n    for i in finaldf['filename']:\n        i = str(i)+\".png\"\n        name.append(i)\n\n    finaldf['filenames'] = name\n    finaldf['filename']=finaldf['filenames']\n    del finaldf['filenames']\n    \n    submission_file = \"Solution\"+str(model_number)+\".csv\"\n    finaldf.to_csv(submission_file, index=False)\n\n    plt.plot(model_history.history['loss'], label='Train loss')\n    plt.plot(model_history.history['val_loss'], label='Validation loss')\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Loss Graph\")\n    plt.legend()\n    plt.show()\n    plt.plot(model_history.history['accuracy'], label = 'Train accuracy')\n    plt.plot(model_history.history['val_accuracy'], label = 'Validation accuracy')\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"Accuracy Graph\")\n    plt.legend()\n    plt.show()\n    \n    return ","b10cb3e4":"model1 = Sequential()\nmodel1.add(Conv2D(16, (3,3), activation = 'relu', input_shape = (28,28,1)))\nmodel1.add(MaxPooling2D((2,2)))\nmodel1.add(Conv2D(32, (3,3), activation = 'relu'))\nmodel1.add(MaxPooling2D((2,2)))\nmodel1.add(Flatten())\nmodel1.add(Dense(100, activation = 'relu'))\nmodel1.add(Dense(64, activation = 'relu'))\nmodel1.add(Dense(10, activation = 'softmax'))\nmodel1.compile(optimizer = Adam(), loss='categorical_crossentropy', metrics=['accuracy'])","45b177ec":"print(model1.summary())","c3734bf7":"# history1 = []                  #Using callback to store history of accuracies and losses through and with which model proggresses. \nhistory1 = model1.fit(x_train, y_train, batch_size=32, epochs = 10, validation_data=(x_val, y_val),  callbacks=[history1]) ","15908371":"import os\nif not os.path.isdir('models'):\n    os.mkdir('models')","2f229e77":"from keras.models import model_from_json","2bef8403":"# serialize model to JSON\nmodel_json = model1.to_json()\nwith open(\"models\/model.json\", \"w\") as json_file:\n    json_file.write(model_json)","62cb0a94":"# serialize weights to HDF5\nmodel1.save_weights(\"models\/model.h5\")\nprint(\"Saved model to disk\")","5398b712":"# os.remove('models\/history1.h5')","b88f8747":"# load json and create model\njson_file = open(\"models\/model.json\", 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n# load weights into new model\nloaded_model.load_weights(\"models\/model.h5\")\nprint(\"Loaded model from disk\")","aa66da4d":"ytest1 = loaded_model.predict(xtest)","5cee06ed":"ytest1","5aaf738c":"xtest.shape","66de6560":"xtest[0:1].shape","3d08fbf3":"ytest2 = loaded_model.predict(xtest[0:1])","ef2c215a":"ytest2","e5c121f8":"np.argmax(ytest2, axis = 1)","881ae296":"### We will use the below defined function after making prediction from the trained model, to create the final submission file and visualising the model performance.","cc1bd390":"# CNN Models :","f8493a76":"# NOTE  \n### 7 models. Starting with a basic model and progressing to more deeper models. \n### Did not consider filtering out the models which failed to give best result as a depiction of the learning process, this being my first CNN project. \n### First 2 models are rather shallow and don't capture the intricacies of the image well Next is a deeper model with BatchNormalisation and Dropout. Using transferlearning, LeNet50 and ResNet50 (with imagenet weights) models are implemented taking on-the-fly augmented data, some training only selected layers at the end of the model. \n### Lastly, a deep CNN model taking in augmented data and working with decaying learning rate which gave the best result.\n### The epochs for the first 6 models are **very low as they were not the best performing models**. Hence their graphs do not really do justice to the actual performance due to less plotting points.\n\n## To skip to the bset performing model jump to **Cell 44**.","d3887c4f":"## Model 1 \n### Basic model without data augmentation, dropout, batch normalisation etc. "}}