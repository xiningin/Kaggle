{"cell_type":{"12a1222e":"code","1ac252a9":"code","2d3d3e59":"code","a039488c":"code","49fee4d1":"code","c8b04a76":"code","45a7b79d":"code","ff5cbd7e":"code","5098b186":"code","15e456be":"code","88d1f5ba":"code","e7982cb4":"code","8e3d9741":"code","dcbefe34":"code","fafeddb9":"code","d7b638c3":"code","51bf7923":"code","7199664d":"code","e5fd8604":"code","1d75923a":"code","9b4d2b14":"code","0fbc8e80":"code","107f6e1f":"code","1472025a":"code","6f733f35":"code","ce83398d":"code","744f3dcd":"code","51094c35":"code","5f036599":"code","fec6c88a":"code","ba5c3111":"code","924e2463":"code","5595a028":"code","2257de74":"code","78eea88a":"code","f6804fc5":"code","df3c25be":"code","17ba6e7b":"code","0b1963f6":"markdown","46ad2413":"markdown","1678ade9":"markdown","8c2c0913":"markdown","ff51858f":"markdown","a3dd4123":"markdown","0d663f79":"markdown","4f2a955e":"markdown","cc297e0e":"markdown","26f16be0":"markdown","e68b6e41":"markdown","5ccf4b68":"markdown","7d2bf845":"markdown","7522ea4d":"markdown","f8715e43":"markdown","c374a66d":"markdown","5651f178":"markdown","83e0629a":"markdown","f85f0d4f":"markdown","790a1932":"markdown","73459867":"markdown","9d9c00e1":"markdown","1fbbe9e4":"markdown","5dfa8763":"markdown","bcdd8823":"markdown","0a9080a4":"markdown","70cd5798":"markdown","084fc7de":"markdown","e70f9cdb":"markdown"},"source":{"12a1222e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import cross_val_score\nfrom numpy import mean\nfrom numpy import std\nimport seaborn as sns\nimport xgboost as xgb\nimport missingno as msno","1ac252a9":"import warnings\nwarnings.filterwarnings(\"ignore\")","2d3d3e59":"!pip install openpyxl","a039488c":"\"\"\"\n2.1 Load data\n\"\"\"\nx_features = pd.read_excel(\"..\/input\/datas-features\/Features data set.xlsx\") # features \ny_sales = pd.read_excel('..\/input\/datas-features\/sales data-set.xlsx') # labels\nz_stores = pd.read_csv(\"..\/input\/datas-features\/stores data-set.csv\") # stores\n","49fee4d1":"\"\"\"\n2.2 Inquering all Data\n\"\"\"\n\nprint(x_features.head())\nprint(y_sales.head())\nprint(z_stores.head())\nprint('-'*10)\nprint('\\n')","c8b04a76":"\"\"\"\n2.3 Groupby working\n\"\"\"\n\n\"\"\"\nThe groupby drop all missing rows\n\"\"\" \n\n# First - Optimize the sales via store in a unique date with all Deparments to be preper\ny_sales['Date'] = pd.to_datetime(y_sales['Date'])\n# optimize the sales via store in a unique date with all Deparments to be preper \ny_sales = y_sales.groupby(['Store', 'Date']).sum()\n# Merging of x_features and z_stores\nx_features = pd.merge(x_features, z_stores)\n\n# Secondly I  transform categorical 'Type' column' feature needed to be number.\ndef Type2num(x):\n     if x == 'A':\n         return 0\n     if x == 'B':\n         return 1\n     if x == 'C':\n         return 2\nx_features['Type'] = x_features['Type'].apply(Type2num)\n\n\n# we have to repeat its again to fit x_features y_sales\nx_features['Date'] = pd.to_datetime(x_features['Date'])\nx_features = x_features.groupby(['Store', 'Date']).sum()\nprint(x_features.dtypes)\nprint(y_sales.dtypes)\nprint(z_stores.dtypes)","45a7b79d":"\"\"\"\n2.3.1 Plotting 'Type' categorical variable above\n\"\"\"\nx_features['Type'].value_counts() #categorical variable\nx_features.Type.value_counts().plot.barh();","ff5cbd7e":"\"\"\"\n2.4 Merging all Data\nAs an inner mean - marge just the common Datas.\n\"\"\"\n\nCombined_table = pd.merge(x_features, y_sales['Weekly_Sales'], how='inner', right_index=True, left_index=True)\nCombined_table.isna().sum()\nCombined_table.info\nCombined_table['Weekly_Sales'].describe()\n# Very well... It seems that your minimum price is larger than zero. Excellent!\n","5098b186":"\"\"\"\n2.4.1 let Calculating profitability per square meter\n\"\"\"\n\n\"\"\"\nRetail chain efficiency is measured as efficiency per square meter of profit. \nLet's see what the efficiency is per square meter? What is the rate of increase \nin efficiency? Next we will examine whether it is possible to predict the rate of \nsales growth based on this figure, and what is the percentage of accuracy of the model?\n\"\"\"\n\nCombined_table['M^2_per_profit'] = Combined_table['Weekly_Sales']\/Combined_table['Size']\nCombind_graf = Combined_table.copy()","15e456be":"\"\"\"\n2.4.2 Inquery by Plotting isnull ?\n\"\"\"\nCombined_table.isnull().sum()\nCombined_table.describe(percentiles=[0.25, 0.5, 0.75, 0.9, 0.95, 0.99])\nmsno.bar(Combined_table)   # checking for null ? by plotting.","88d1f5ba":"\"\"\"\n2.5 inquery by: Skewness @ skew\n\"\"\"\nprint(\"Skewness: %f\" % Combined_table['Weekly_Sales'].skew())\nprint(\"Kurtosis: %f\" % Combined_table['Weekly_Sales'].kurt())\n\"\"\"\n2.5.1) Have appreciable positive skewness.\n2.5.2) Have appreciable positive skewness.\n\"\"\"","e7982cb4":"\"\"\"\n3.1 Histogram plot\n\"\"\"\n# let see how Weekly_Sales corellative to other features ?\n# sns.distplot(Combined_table['Weekly_Sales'])\nsns.displot(data = Combined_table, x = 'Weekly_Sales', kde=True)\n","8e3d9741":"\"\"\"\n3.2 Inquery of outlier by box plot store \/ Weekly_Sales\n\"\"\"\n# first convert the store from index into a column\n# Takea a look over each sotre outlayers via \"Weekly_Sales'.\nCombined_table = Combined_table.reset_index(level=0)\nvar = 'Store'\ndata = pd.concat([Combined_table['Weekly_Sales'], Combined_table[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"Weekly_Sales\", data=data)\nfig.axis(ymin = 180000, ymax = 4000000)\n\n# By inquering with \"Boxplot\" I see that all thefeatures are outlier.","dcbefe34":"\"\"\"\n3.3 scatterplot - Data Visualization\n\"\"\"\n# Let display the relationship between two numerical variables\n# For any combination features. \nsns.set()\ncols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'Weekly_Sales']\nsns_plot = sns.pairplot(Combind_graf[cols].sample(100), height = 2.5) \nplt.show()\n# Here by 'Scatterplot' I could see the relations between all features. The calculating takes a lot of time so it's suggested to take small number sample. Finally, we haven't correlation between the features\n","fafeddb9":"\"\"\"\n3.4 Plotting all features via the label (prediction of sales)\n\"\"\"\n# I try to see if it's any corralation between the features\nCombind_graf[['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'Weekly_Sales']].plot(subplots=True, figsize=(20,15))\nplt.show()\n\n# In other way plotting all features again with 'Combind_graf'  no relations between all of them.\n","d7b638c3":"# In the following Matrix we will see how mach the features are confusing?.\n# As we see not all features are not corelative. \ng = sns.heatmap(Combind_graf[['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'Weekly_Sales']].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")\nplt.show()","51bf7923":"# Since I have two Prameters with correlasition, I drop one of them - 'MarkDown1',\n# Why ?\n# To prevent  Linkage featurs that are corelatived.\ng = sns.heatmap(Combind_graf[['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'Weekly_Sales']].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")\nplt.show()","7199664d":"\"\"\"\nconclusion: Finally The linckage is no so big.\n----------\nAfter checking the accuracy with and without 'MarkDown1' the different\n seem to be small\n\"\"\"\nCombined_table = Combined_table.drop(['MarkDown1'], axis = 1)\n\n","e5fd8604":"# I try to see if it's any corralation between those tow features\nCombind_graf[['Type', 'Size', 'Weekly_Sales']].plot(subplots=True, figsize=(20,15))\nplt.show()","1d75923a":"# I try to see if it's any corralation between 'Type' and 'Weekly_Sales'\nCombind_graf[['Type', 'Weekly_Sales']].plot(subplots=True, figsize=(20,15))\nplt.show()","9b4d2b14":"# I try to see if it's any correlation between 'Size' and 'Weekly_Sales'\nCombind_graf[['Size', 'Weekly_Sales']].plot(subplots=True, figsize=(20,15))\nplt.show()","0fbc8e80":"# I try to see if it's any corralation between 'M^2_per_profit' and 'Weekly_Sales'\nCombind_graf[['M^2_per_profit', 'Weekly_Sales']].plot(subplots=True, figsize=(20,15))\nplt.show()","107f6e1f":"# Till nuw what we saw ? I saw that 'M2^_per_profit' feature is in correlation with \n#  'Type'----> 40% and 'Size'----> 30%. 'M2^_per_profit' feature was create to check profit.\n# Once I did my exploration with 'M2^_per_profit' and it's artificial feature I need drop it.\n# Reducing features revoke dimensional Data Curse problem.\n# This feature 'M2^_per_profit' could be predicable like 'Size' or 'Type' but both of them are better.\nCombined_table = Combined_table.drop(['M^2_per_profit'], axis=1)\n# Let plot it.\n\ng = sns.heatmap(Combind_graf[['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'Weekly_Sales', 'Type', 'Size']].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")\nplt.show()","1472025a":"# Definition of Y (label) and X (inputs)\n# Define label for the new merge table: \"combined_table\"\n# The 'Weekly_Sales' is Indexial so it dosn't take as a label\ny = Combined_table['Weekly_Sales']\n# Define features for the new merge table: \"combined_table\"\nx = Combined_table.drop(['Weekly_Sales'], axis=1)\n\n","6f733f35":"scaler = MinMaxScaler()\nx_scaled = scaler.fit_transform(x)\ny.shape\ny = y.values.reshape(6435, 1)\ny_scaled = scaler.fit_transform(y)\nx.head()\nx.tail()","ce83398d":"# we need to take x_scaled after being notmalized\n# In the first step we will split the data in training and remaining dataset\nx_train, x_valid, y_train, y_valid = train_test_split(x_scaled, y, train_size=0.80, random_state=42)\n\n","744f3dcd":"# Now since we want the valid and test size to be equal (10% each of overall data). \n# we have to define valid_size=0.5 (that is 50% of remaining data)\ntest_size = 0.5\nx_test, x_valid, y_test, y_valid = train_test_split(x_scaled,y, test_size=0.5)\n\n\n# # we need to add an additional dimantion to get numpy arrey to keras\n# x_train = x_train.reshape(x_train.shape[0],1,x_train.shape[1]) \n# x_valid = x_valid.reshape(x_valid.shape[0],1,x_valid.shape[1])\n# x_test = x_test.reshape(x_test.shape[0],1,x_test.shape[1]) \n\nTest_Data = (x_test, y_test)\n\nprint('-'*10)\nprint('\\n')\nprint(x_train.shape), print(y_train.shape)\nprint(x_valid.shape), print(y_valid.shape)\nprint(x_test.shape), print(y_test.shape)\nprint('-'*10)\nprint('\\n')","51094c35":"x_train, y_train = make_regression(n_features = 10, n_informative = 2,\n                                   random_state = 0, shuffle = False, noise=0.1)\n# Define Adaboost model\nmodel1 = AdaBoostRegressor(random_state = 123, n_estimators = 100)\n# Fit regression model\nmodel1.fit(x_train, y_train)\n\nclf1 = AdaBoostRegressor(n_estimators=100)\nscores1 = cross_val_score(clf1, x, y.ravel(), cv=5)\n\nprint('Adaboost score')\nprint(scores1.mean())","5f036599":"# Separate the target variable and rest of the variables using .iloc to subset the data.\nx, y = data.iloc[:,:-1],Combined_table.iloc[:,-1]\n\n# convert the dataset into an optimized data structure\n# called Dmatrix that XGBoost supports and gives it acclaimed performance\n# and efficiency gains. \ndata_dmatrix = xgb.DMatrix(x,label=y)\n","fec6c88a":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, \n                                                    random_state=123)\n\n#  instantiate an XGBoost regressor object by calling the XGBRegressor()\n# class from the XGBoost library with the hyper-parameters\nmodel2 = xgb.XGBRegressor(objective ='reg:linear',\n                colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 100)\n\n\nmodel2.fit(x_train,y_train)\n\n\npreds = model2.predict(x_test)\n\n","ba5c3111":"rmse = np.sqrt(mean_squared_error(y_test, preds))","924e2463":"print(\"RMSE: %f\" % (rmse))","5595a028":"clf2 = xgb.XGBRegressor(n_estimators=100)\nscores2 = cross_val_score(clf2, x, y.ravel(), cv=5)\nprint('XGBboost score')\nprint(scores2.mean())","2257de74":"# Transform the hiper parameters to dictionary.\nparams = {\"objective\":\"reg:linear\",'colsample_bytree': 0.3,\n          'learning_rate': 0.1,\n          'max_depth': 5, 'alpha': 10}\n\n# I use this dictionary with parameters to createe 3 fold\n# cross validation were done by XGBooster's and store in cv+results \n\n","78eea88a":"cv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3,\n                    num_boost_round=50,early_stopping_rounds=10,metrics=\"rmse\", as_pandas=True, seed=123)\n","f6804fc5":"cv_results.head()\n\nprint((cv_results[\"test-rmse-mean\"]).tail(1))","df3c25be":"xg_reg = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)\n\n\n","17ba6e7b":"xgb.plot_tree(xg_reg,num_trees=0)\nplt.rcParams['figure.figsize'] = [500, 100]\nplt.show()","0b1963f6":"# Sub conclusion\nIn Adaboost native way I get    around :53%-55%\nIn XGBoost way I get  99%","46ad2413":"# 6.Modeling\n   ----------","1678ade9":"* # 7. Adaboost as Regression\n  ------------------------","8c2c0913":"\n**5.1** Since I must take some economic vision for my analyzation. I prefer to \ntake only two tables for my inquiry. The two table are: 1) 'features', 2) \n'sales' that correspond to our label. It was done with economic aspect and \nit's the reason why I don't take 'stores'. I combine the 'features' and 'sales' \nand split them to be : features and label\n\"\"\"\n\"\"\"\n5.2 Just to be on the safe side. As we could see, I take some types plots for \nfeatures inquiry. Some plots show no relations and one other show that those \ntwo features are in similarity. If it does maybe I have a linkage that could \ninfluence my accuracy. So, I check the output accuracy with and without this \nfeature - no different. So, I prefer even that to reduce it.\n\"\"\"\n\"\"\"\n5.3 The data are not so big, so maybe the training could be problematic. \nAs a model of economic predicts, I can't do any augmentation with no meaning. \nOther problem corresponds to lost time once the combine tables were done with \nno empty rows and columns. I prefer the bagging to be 3 (cv = 3) - more the \nbagging greatest more time calculating is bigger but maybe more accreditable. \nEach individual tree in the random forest spits out a class prediction and the \nclass with the most votes become our model\u2019s prediction (see figure below).\n","ff51858f":"**5.1 Plotting 'M^2_per_profit' 'Type' and 'Size\" via the label (prediction of main sales features)**","a3dd4123":"# 5. Feature engineering","0d663f79":"XGBoost's hyperparameters\n* learning_rate: step size shrinkage used to prevent overfitting. Range is [0,1]\n* max_depth: determines how deeply each tree is allowed to grow during any boosting round.\n* subsample: percentage of samples used per tree. Low value can lead to underfitting.\n* colsample_bytree: percentage of features used per tree. High value can lead to overfitting.\n* n_estimators: number of trees you want to build.\n* objective: determines the loss function to be used like reg:\n        linear for regression problems, \n        reg:logistic for classification problems with only decision, \n        binary:logistic for classification problems with probability.\n        \n        \nXGBoost also supports regularization parameters to penalize models\n\n\n    gamma: controls whether a given node will split based on the expected \n        reduction in loss after the split. A higher value leads to fewer \n        splits. Supported only for tree-based learners.\n    alpha: L1 regularization on leaf weights. A large value leads to more \n        regularization.\n    lambda: L2 regularization on leaf weights and is smoother than L1 \n        regularization.","4f2a955e":"# 7.4 Visualize Boosting Trees and Feature Importance\n   ------------------------------------------------","cc297e0e":"# 7.1 Adaboost (Adaptive Boosting)\nIs a statistical classification meta-algorithm. Every learning algorithm tends to suit some problem types better \nthan others.\nTogether, synergy and strong learning are created. Adaboost is a serial model - the errors that the first \nstump makes influence how the second stump is made etc .. .   ","26f16be0":"6.3 splitting the test to test @ val","e68b6e41":"# **2. Load and check data**\n","5ccf4b68":"# 4. Feature analysis\n-------------------","7d2bf845":"5.1.3","7522ea4d":"# 7.2 XGBoost is one of the most popular machine learning algorithm\nthese days. Regardless of the type of prediction task at hand; regression\nor classification.\nXGBoost is well known to provide better solutions than other machine learning algorithms\n","f8715e43":"6.2 splitting the Datas to train @ test ","c374a66d":"# sub conclusion\nRefer begining lost (1,060,000$) \nI get a prediction loss 7,992.7$.\nSince we have more fols than I get better loss. As it was XGB 99% score","5651f178":"# **3 Inquery by Plotting** ","83e0629a":"# 1. Introduction\n---------------\n\nThis is my second kernel at Kaggle. I choosed economic data tabular which \nis a good way to introduce feature engineering and ensemble modeling. \nFirstly, I will display some feature analyses than i will focus on the \nfeature engineering. Last part concerns modeling and predicting sales \nmarketing as using an voting procedure. As an economic label problem, \nI prefer to do a regression model Adaboost, XGBboost, and K-fold Cross Validation.\nto be compare.\n\nThis script follows three main parts:\n\n    Feature analysis\n    Feature engineering\n    Modeling (at thr end).","f85f0d4f":"**5.1.1**","790a1932":"**5.1.2**","73459867":"7.0 Types of Boosting Algorithms\n\n    AdaBoost (Adaptive Boosting)\n    Gradient Tree Boosting (Is not here)\n    XGBoost\n    k-fold Cross Validation using XGBoost\n    Visualize Boosting Trees","9d9c00e1":"Other hyper parameters for this subset\n\n* num_boost_round: denotes the number of trees you build \n    (analogous to n_estimators)\n* metrics: tells the evaluation metrics to be watched during CV\n* as_pandas: to return the results in a pandas DataFrame.\n* early_stopping_rounds: finishes training of the model early\n     if the hold-out metric (\"rmse\" in our case) does not \n     improve for a given number of rounds.\n* seed: for reproducibility of results.","1fbbe9e4":"6.1 preprocessing normalization values between 0 and 1\n","5dfa8763":"# 7.3 k-fold Cross Validation using XGBoost\n  ----------------------------------------","bcdd8823":"**5.2 Drop unuse feature**","0a9080a4":"4.2 Plotting Confusional Correlation matrix with no features correlative.\n\"\"\"","70cd5798":"Linear parameter is now deprecated in favor of reg:squarederror. It is an expected good warning In this model.","084fc7de":"\n# **5. Core business analysis and inquery by plotting.**\n\n\n","e70f9cdb":"4.1 Plotting Confusional Correlation matrix between numerical values.\n\"\"\""}}