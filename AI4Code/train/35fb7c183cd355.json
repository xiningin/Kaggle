{"cell_type":{"2f902f42":"code","447dd48d":"code","b54caa2b":"code","b6284be3":"code","0c126526":"code","63fcba45":"code","2ee8aa8f":"code","8f4c72e8":"code","e6056388":"code","9e51b31e":"code","dbbc414f":"code","1635dd3d":"code","c97ca7e2":"code","84e39626":"code","bc76e4ba":"code","8307d3d2":"code","02f88a64":"code","bd4ccf95":"code","7ae52678":"code","9d24eb4e":"code","9550e8be":"code","429eb91d":"code","2006f812":"code","9a7450c9":"code","df3a6061":"code","ee929c54":"code","0cea6a00":"code","22fb7853":"code","44dc05eb":"code","a2cf9cba":"code","26aa8ceb":"markdown","2e70a6b3":"markdown"},"source":{"2f902f42":"!pip install ohmeow-blurr==0.0.22 datasets==1.3.0 fsspec==0.8.5 -qq","447dd48d":"# Change this 2 lines to use to another pretrained model\npretrained_model_name = 'xlm-roberta-large'\nmodel_name = 'xlm-roberta-large'","b54caa2b":"# turn off multithreading to avoid deadlock\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","b6284be3":"from transformers import *\nfrom fastai.text.all import *\n\nfrom blurr.data.all import *\nfrom blurr.modeling.all import *\n\nSEED = 42\nset_seed(SEED, True)","0c126526":"import json\n\nwith open('..\/input\/sclds2021preprocess\/wordlist.json', 'r') as f:\n    wordlist = json.load(f)","63fcba45":"import ast\ndf_converters = {'tokens': ast.literal_eval, 'labels': ast.literal_eval}\n\ntrain_df = pd.read_csv('..\/input\/sclds2021preprocess\/train.csv', converters=df_converters)\nvalid_df = pd.read_csv('..\/input\/sclds2021preprocess\/valid.csv', converters=df_converters)","2ee8aa8f":"len(train_df), len(valid_df)","8f4c72e8":"labels = sorted(list(set([lbls for sublist in train_df.labels.tolist() for lbls in sublist])))\nprint(labels)","e6056388":"task = HF_TASKS_AUTO.TokenClassification\nconfig = AutoConfig.from_pretrained(pretrained_model_name)\nconfig.num_labels = len(labels)\n\nhf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(pretrained_model_name, \n                                                                               task=task, \n                                                                               config=config)\nhf_arch, type(hf_config), type(hf_tokenizer), type(hf_model)","9e51b31e":"before_batch_tfm = HF_TokenClassBeforeBatchTransform(hf_arch, hf_config, hf_tokenizer, hf_model,\n                                                     is_split_into_words=True, \n                                                     tok_kwargs={'return_special_tokens_mask': True})\n\nblocks = (\n    HF_TextBlock(before_batch_tfm=before_batch_tfm, input_return_type=HF_TokenClassInput), \n    HF_TokenCategoryBlock(vocab=labels)\n)\n\ndef get_y(inp): return [(label, len(hf_tokenizer.tokenize(str(entity)))) for entity, label in zip(inp.tokens, inp.labels)]","dbbc414f":"db = DataBlock(\n    blocks=blocks, \n    splitter=RandomSplitter(valid_pct=0.1, seed=SEED),\n    get_x=ColReader('tokens'),\n    get_y=get_y,\n)","1635dd3d":"dls = db.dataloaders(train_df, bs=32)\ndls.show_batch(dataloaders=dls)","c97ca7e2":"@delegates()\nclass TokenCrossEntropyLossFlat(BaseLoss):\n    \"Same as `CrossEntropyLossFlat`, but for mutiple tokens output\"\n    y_int = True\n    @use_kwargs_dict(keep=True, weight=None, ignore_index=-100, reduction='mean')\n    def __init__(self, *args, axis=-1, **kwargs): super().__init__(nn.CrossEntropyLoss, *args, axis=axis, **kwargs)\n    def decodes(self, x):    return L([ i.argmax(dim=self.axis) for i in x ])\n    def activation(self, x): return L([ F.softmax(i, dim=self.axis) for i in x ])","84e39626":"model = HF_BaseModelWrapper(hf_model)\nloss_func = TokenCrossEntropyLossFlat()\nopt_func = partial(Adam)\nlearn_cbs = [HF_BaseModelCallback]\nfit_cbs = [HF_TokenClassMetricsCallback()]\nsplitter = hf_splitter","bc76e4ba":"learn = Learner(dls, model, loss_func=loss_func, opt_func=opt_func, splitter=splitter, cbs=learn_cbs).to_fp16()","8307d3d2":"learn.unfreeze()","02f88a64":"learn.fit_one_cycle(5, 1e-4, moms=(0.8, 0.7, 0.8), cbs=fit_cbs)","bd4ccf95":"learn.recorder.plot_loss()","7ae52678":"print(learn.token_classification_report)","9d24eb4e":"learn.export(f'{model_name}.pkl')","9550e8be":"@patch\ndef blurr_predict(self:Learner, items, rm_type_tfms=None):\n    hf_before_batch_tfm = get_blurr_tfm(self.dls.before_batch)\n    is_split_str = hf_before_batch_tfm.is_split_into_words and isinstance(items[0], str)\n    is_df = isinstance(items, pd.DataFrame)\n    if (not is_df and (is_split_str or not is_listy(items))): items = [items]\n    dl = self.dls.test_dl(items, rm_type_tfms=rm_type_tfms, num_workers=0)\n    with self.no_bar(): probs, _, decoded_preds = self.get_preds(dl=dl, with_input=False, with_decoded=True)\n    trg_tfms = self.dls.tfms[self.dls.n_inp:]\n    outs = []\n    probs, decoded_preds = L(probs), L(decoded_preds)\n    for i in range(len(items)):\n        item_probs = [probs[i]]\n        item_dec_preds = [decoded_preds[i]]\n        item_dec_labels = tuplify([tfm.decode(item_dec_preds[tfm_idx]) for tfm_idx, tfm in enumerate(trg_tfms)])\n        outs.append((item_dec_labels, item_dec_preds, item_probs))\n    return outs","429eb91d":"from string import punctuation\n\ndef reconstruct(num, pred, raw_tokens, raw_address):\n    def complete_word(x):\n        y = x.strip().strip(punctuation)\n        if y != '' and y in wordlist:\n            x = x.replace(y, wordlist[y])\n        return x\n    \n    def normalize_bracket(x):\n        if '(' in x and ')' not in x:\n            x = x + ')'\n        elif ')' in x and '(' not in x:\n            x = '(' + x\n        return x\n    \n    ans = ['\/'] * num\n    for idx in range(num):\n        res = pred[idx]\n        start_poi, end_poi = -1, -1\n        start_str, end_str = -1, -1\n        for i in range(len(res[0])):\n            if 'POI' in res[1][i]:\n                if start_poi == -1: start_poi = i\n                end_poi = i\n            if 'STR' in res[1][i]:\n                if start_str == -1: start_str = i\n                end_str = i\n        \n        if start_poi != -1:\n            txt1 = raw_address[idx]\n            for i in range(start_poi):\n                txt1 = txt1[len(raw_tokens[idx][i]):].strip()\n            for i in range(len(raw_tokens[idx]) - 1, end_poi, -1):\n                txt1 = txt1[:-len(raw_tokens[idx][i])].strip()\n            \n            txt1_check = ''.join(raw_tokens[idx][start_poi:end_poi + 1]).replace(' ', '')\n            assert txt1.replace(' ', '') == txt1_check\n            \n            last = len(txt1)\n            for i in range(end_poi, start_poi - 1, -1):\n                while last > 0 and txt1[last - 1] == ' ':\n                    last -= 1\n                assert last >= len(raw_tokens[idx][i])\n                last -= len(raw_tokens[idx][i])\n                if 'SHORT' in res[1][i]:\n                    txt1 = txt1[:last] + complete_word(raw_tokens[idx][i]) + txt1[last + len(raw_tokens[idx][i]):]\n        else:\n            txt1 = ''\n        \n        if start_str != -1:\n            txt2 = raw_address[idx]\n            for i in range(start_str):\n                txt2 = txt2[len(raw_tokens[idx][i]):].strip()\n            for i in range(len(raw_tokens[idx]) - 1, end_str, -1):\n                txt2 = txt2[:-len(raw_tokens[idx][i])].strip()\n            \n            txt2_check = ''.join(raw_tokens[idx][start_str:end_str + 1]).replace(' ', '')\n            assert txt2.replace(' ', '') == txt2_check\n            \n            last = len(txt2)\n            for i in range(end_str, start_str - 1, -1):\n                while last > 0 and txt2[last - 1] == ' ':\n                    last -= 1\n                assert last >= len(raw_tokens[idx][i])\n                last -= len(raw_tokens[idx][i])\n                if 'SHORT' in res[1][i]:\n                    txt2 = txt2[:last] + complete_word(raw_tokens[idx][i]) + txt2[last + len(raw_tokens[idx][i]):]\n        else:\n            txt2 = ''\n        \n        txt1 = txt1.strip(punctuation)\n        txt2 = txt2.strip(punctuation)\n        txt1 = normalize_bracket(txt1)\n        txt2 = normalize_bracket(txt2)\n        \n        ans[idx] = (txt1 + '\/' + txt2)\n    \n    return ans","2006f812":"def show_diff(df):\n    MAX_ROWS = 50\n    CNT = 0\n    for idx in range(len(df)):\n        if CNT == MAX_ROWS: break\n        row = df.iloc[idx]\n        if row['POI\/street'] != row['pred']:\n            CNT += 1\n            print(idx, row['id'], row['POI\/street'], 'vs', row['pred'])","9a7450c9":"def calc_acc(df):\n    return df.loc[valid_df['pred'] == df['POI\/street'], 'id'].count() \/ len(df)","df3a6061":"raw_tokens = list(valid_df['tokens'])\nraw_address = list(valid_df['raw_address'])","ee929c54":"raw_pred = learn.blurr_predict_tokens(raw_tokens)","0cea6a00":"pred = reconstruct(len(valid_df), raw_pred, raw_tokens, raw_address)","22fb7853":"valid_df['pred'] = pred\nvalid_df.head()","44dc05eb":"# Final evaluation with the same metric used for the competition\ncalc_acc(valid_df)","a2cf9cba":"show_diff(valid_df)","26aa8ceb":"# Training\n- Fine tune a pretrained model from `HuggingFace` using `fastai` and `blurr`\n- Our 1st place solution used **ensembling of many models** by taking the **average of the prediction probabilities for each word** and **the entire dataset was used for training with no validation**\n\n## Steps:\n1. [Preprocessing](https:\/\/www.kaggle.com\/nguyncaoduy\/1-place-scl-ds-2021-voidandtwotsts-preprocess)\n2. [Training](https:\/\/www.kaggle.com\/nguyncaoduy\/1-place-scl-ds-2021-voidandtwotsts-train) - This Notebook\n3. [Ensembling](https:\/\/www.kaggle.com\/nguyncaoduy\/1-place-scl-ds-2021-voidandtwotsts-ensemble)","2e70a6b3":"# Evaluation\n- This is only relevant during model selection and testing\n- For the final training, full dataset is used so the accuracy below doesn't really reflect the power of the model."}}