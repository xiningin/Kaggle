{"cell_type":{"7b2e699a":"code","c05ecbc7":"code","5cee38c5":"code","f19b3930":"code","25a2eec1":"code","01c5b9c8":"code","eda476b1":"code","9d2c2e47":"code","f42cd54f":"code","dd1889b7":"code","7652d473":"code","2099182e":"code","db95d39e":"code","4d15aa3e":"code","e0dc08a1":"code","88231285":"code","c00d10ef":"code","40678904":"code","dbd57cc9":"code","4deab58c":"code","7b49acdc":"code","a01570ec":"code","52f1bc0d":"code","9a47487d":"code","30033efc":"code","7a14030a":"code","4cdeffcc":"code","47db8bd3":"code","97b88299":"code","afcaf3d8":"markdown","ae6a7551":"markdown","a37e448c":"markdown","768519e1":"markdown","f380438d":"markdown","a2d4c191":"markdown","ccb8f63e":"markdown","f5f1dd33":"markdown","aad93b6e":"markdown","44dec271":"markdown","fa9e6c9b":"markdown","9e4ecb59":"markdown","8867f053":"markdown","213873ff":"markdown","094020fa":"markdown","5b976ac4":"markdown","034ef92b":"markdown","8831be81":"markdown","5eaf1a10":"markdown","97ea491e":"markdown","ec9122c1":"markdown","239f498a":"markdown","9ffb6883":"markdown","fa0b3493":"markdown","381b822f":"markdown","001769a3":"markdown","000384ca":"markdown","25413ea5":"markdown"},"source":{"7b2e699a":"!pip install scikit-learn -U","c05ecbc7":"# Intel\u00ae Extension for Scikit-learn installation:\n!pip install scikit-learn-intelex","5cee38c5":"from sklearnex import patch_sklearn\npatch_sklearn()","f19b3930":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn\nimport dateutil.easter as easter\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler","25a2eec1":"# Loading train and test data\ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/train.csv\", parse_dates=['date'])\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/test.csv\", parse_dates=['date'])","01c5b9c8":"train.dtypes","eda476b1":"# figuring out the theoretically possible level combination\ntime_series = ['country', 'store', 'product']\ncombinations = 1\nfor feat in time_series:\n    combinations *= train[feat].nunique()\n    \nprint(f\"There are {combinations} possible combinations\")","9d2c2e47":"time_series = ['country', 'store', 'product']\ncountry_store_product_train = train[time_series].drop_duplicates().sort_values(time_series)\ncountry_store_product_test =test[time_series].drop_duplicates().sort_values(time_series)\n\ncond_1 = len(country_store_product_train) == combinations\nprint(f\"Are all theoretical combinations present in train: {cond_1}\")\ncond_2 = (country_store_product_train == country_store_product_test).all().all()\nprint(f\"Are combinations the same in train and test: {cond_2}\")","f42cd54f":"train_dates = train.date.drop_duplicates().sort_values()\ntest_dates = test.date.drop_duplicates().sort_values()\n\nfig, ax = plt.subplots(1, 1, figsize = (11, 7))\ncmap_cv = plt.cm.coolwarm\n\ncolor_index = np.array([1] * len(train_dates) + [0] * len(test_dates))\n\nax.scatter(range(len(train_dates)), [.5] * len(train_dates),\n           c=color_index[:len(train_dates)], marker='_', lw=15, cmap=cmap_cv,\n           label='train', vmin=-.2, vmax=1.2)\n\nax.scatter(range(len(train_dates), len(train_dates) + len(test_dates)), [.55] * len(test_dates),\n           c=color_index[len(train_dates):], marker='_', lw=15, cmap=cmap_cv,\n           label='test', vmin=-.2, vmax=1.2)\n\ntick_locations = np.cumsum([0, 365, 366, 365, 365, 365])\nfor i in (tick_locations):\n    ax.vlines(i, 0, 2,linestyles='dotted', colors = 'grey')\n    \nax.set_xticks(tick_locations)\nax.set_xticklabels([2015, 2016, 2017, 2018, 2019, 2020], rotation = 0)\nax.set_yticklabels(labels=[])\nplt.ylim([0.45, 0.60])\nax.legend(loc=\"upper left\", title=\"data\")\n\nplt.show()","dd1889b7":"missing_train = pd.date_range(start=train_dates.min(), end=train_dates.max()).difference(train_dates)\nmissing_test = pd.date_range(start=test_dates.min(), end=test_dates.max()).difference(test_dates)\nprint(f\"missing dates in train: {len(missing_train)} and in test: {len(missing_test)}\")","7652d473":"# We create different time granularity\n\ndef process_time(df):\n    df['year'] = df['date'].dt.year\n    df['month'] = df['date'].dt.month\n    df['week'] = df['date'].dt.isocalendar().week\n    df['week'][df['week']>52] = 52\n    df['day'] = df['date'].dt.day\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['quarter'] = df['date'].dt.quarter\n    df['dayofyear'] = df['date'].dt.dayofyear\n    return df\n\ntrain = process_time(train)\ntest = process_time(test)","2099182e":"for product in ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']:\n    print(f\"\\n--- {product} ---\\n\")\n    fig = plt.figure(figsize=(20, 10), dpi=100)\n    fig.subplots_adjust(hspace=0.25)\n    for i, store in enumerate(['KaggleMart', 'KaggleRama']):\n        for j, country in enumerate(['Finland', 'Norway', 'Sweden']):\n            ax = fig.add_subplot(2, 3, (i*3+j+1))\n            selection = (train['country']==country)&(train['store']==store)&(train['product']==product)\n            selected = train[selection]\n            selected.set_index('date').groupby('year')['num_sold'].mean().plot(ax=ax)\n            ax.set_title(f\"{country}:{store}\")\n    plt.show()","db95d39e":"for product in ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']:\n    fig = plt.figure(figsize=(20, 10), dpi=100)\n    fig.subplots_adjust(hspace=0.25)\n    for i, store in enumerate(['KaggleMart', 'KaggleRama']):\n        for j, country in enumerate(['Finland', 'Norway', 'Sweden']):\n            ax = fig.add_subplot(2, 3, (i*3+j+1))\n            selection = (train['country']==country)&(train['store']==store)&(train['product']==product)\n            selected = train[selection]\n            for year in [2015, 2016, 2017, 2018]:\n                selected[selected.year==year].set_index('date').groupby('month')['num_sold'].mean().plot(ax=ax, label=year)\n            ax.set_title(f\"{product} | {country}:{store}\")\n            ax.legend()\n    plt.show()","4d15aa3e":"for product in ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']:\n    print(f\"\\n--- {product} ---\\n\")\n    fig = plt.figure(figsize=(20, 10), dpi=100)\n    fig.subplots_adjust(hspace=0.25)\n    for i, store in enumerate(['KaggleMart', 'KaggleRama']):\n        for j, country in enumerate(['Finland', 'Norway', 'Sweden']):\n            ax = fig.add_subplot(2, 3, (i*3+j+1))\n            selection = (train['country']==country)&(train['store']==store)&(train['product']==product)\n            selected = train[selection]\n            for year in [2015, 2016, 2017, 2018]:\n                selected[selected.year==year].set_index('date').groupby('week')['num_sold'].mean().plot(ax=ax, label=year)\n            ax.set_title(f\"{country}:{store}\")\n            ax.legend()\n    plt.show()","e0dc08a1":"for product in ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']:\n    print(f\"\\n--- {product} ---\\n\")\n    fig = plt.figure(figsize=(20, 10), dpi=100)\n    fig.subplots_adjust(hspace=0.25)\n    for i, store in enumerate(['KaggleMart', 'KaggleRama']):\n        for j, country in enumerate(['Finland', 'Norway', 'Sweden']):\n            ax = fig.add_subplot(2, 3, (i*3+j+1))\n            selection = (train['country']==country)&(train['store']==store)&(train['product']==product)\n            selected = train[selection]\n            for year in [2015, 2016, 2017, 2018]:\n                selected[selected.year==year].set_index('date').groupby('day')['num_sold'].mean().plot(ax=ax, label=year)\n            ax.set_title(f\"{country}:{store}\")\n            ax.legend()\n    plt.show()","88231285":"for product in ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']:\n    print(f\"\\n--- {product} ---\\n\")\n    fig = plt.figure(figsize=(20, 10), dpi=100)\n    fig.subplots_adjust(hspace=0.25)\n    for i, store in enumerate(['KaggleMart', 'KaggleRama']):\n        for j, country in enumerate(['Finland', 'Norway', 'Sweden']):\n            ax = fig.add_subplot(2, 3, (i*3+j+1))\n            selection = (train['country']==country)&(train['store']==store)&(train['product']==product)\n            selected = train[selection]\n            for year in [2015, 2016, 2017, 2018]:\n                selected[selected.year==year].set_index('date').groupby('dayofweek')['num_sold'].sum().plot(ax=ax, label=year)\n            ax.set_title(f\"{country}:{store}\")\n            ax.legend()\n    plt.show()","c00d10ef":"festivities = pd.read_csv(\"..\/input\/festivities-in-finland-norway-sweden-tsp-0122\/nordic_holidays.csv\",\n                          parse_dates=['date'],\n                          usecols=['date', 'country', 'holiday'])","40678904":"gdp = pd.read_csv(\"..\/input\/gdp-20152019-finland-norway-and-sweden\/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv\")\ngdp = np.concatenate([gdp[['year', 'GDP_Finland']].values, \n                      gdp[['year', 'GDP_Norway']].values, \n                      gdp[['year', 'GDP_Sweden']].values])\ngdp = pd.DataFrame(gdp, columns=['year', 'gdp'])\ngdp['country'] = ['Finland']*5 + ['Norway']*5 +['Sweden']*5","dbd57cc9":"def process_target(df):\n    target = pd.DataFrame({'row_id':df['row_id'], 'num_sold':df['num_sold']})\n    target = target.sort_values('row_id').set_index('row_id')\n    return target\n\ndef process_data(df):\n    \"\"\"Return a new dataframe with the engineered features\"\"\"\n\n    new_df = pd.DataFrame({'gdp': np.log(df.merge(gdp, on=['country', 'year'], how='left')['gdp'].values),\n                           'wd4': df.date.dt.weekday == 4, # Friday\n                           'wd56': df.date.dt.weekday >= 5, # Saturday and Sunday\n                          })\n\n    # One-hot encoding (no need to encode the last categories)\n    for country in ['Finland', 'Norway']:\n        new_df[country] = df.country == country\n    new_df['KaggleRama'] = df.store == 'KaggleRama'\n    for product in ['Kaggle Mug', 'Kaggle Hat']:\n        new_df[product] = df['product'] == product\n        \n    # Seasonal variations (Fourier series)\n    # The three products have different seasonal patterns\n    dayofyear = df.date.dt.dayofyear\n    for k in range(1, 3):\n        new_df[f'sin{k}'] = np.sin(dayofyear \/ 365 * 2 * np.pi * k)\n        new_df[f'cos{k}'] = np.cos(dayofyear \/ 365 * 2 * np.pi * k)\n        new_df[f'mug_sin{k}'] = new_df[f'sin{k}'] * new_df['Kaggle Mug']\n        new_df[f'mug_cos{k}'] = new_df[f'cos{k}'] * new_df['Kaggle Mug']\n        new_df[f'hat_sin{k}'] = new_df[f'sin{k}'] * new_df['Kaggle Hat']\n        new_df[f'hat_cos{k}'] = new_df[f'cos{k}'] * new_df['Kaggle Hat']\n\n    # End of year\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d)\n                                      for d in range(24, 32)}),\n                        pd.DataFrame({f\"n-dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d) &\n                                      (df.country == 'Norway')\n                                      for d in range(24, 32)}),\n                        pd.DataFrame({f\"f-jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & \n                                      (df.country == 'Finland')\n                                      for d in range(1, 14)}),\n                        pd.DataFrame({f\"jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) &\n                                      (df.country == 'Norway')\n                                      for d in range(1, 10)}),\n                        pd.DataFrame({f\"s-jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & \n                                      (df.country == 'Sweden')\n                                      for d in range(1, 15)})],\n                       axis=1)\n    \n    # May\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"may{d}\":\n                                      (df.date.dt.month == 5) & (df.date.dt.day == d) \n                                      for d in list(range(1, 10))}),\n                        pd.DataFrame({f\"may{d}\":\n                                      (df.date.dt.month == 5) & (df.date.dt.day == d) & \n                                      (df.country == 'Norway')\n                                      for d in list(range(18, 28))})],\n                       axis=1)\n    \n    # June and July\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"june{d}\":\n                                      (df.date.dt.month == 6) & (df.date.dt.day == d) & \n                                      (df.country == 'Sweden')\n                                      for d in list(range(8, 14))}),\n                       ],\n                       axis=1)\n    \n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n                                         2016: pd.Timestamp(('2016-06-29')),\n                                         2017: pd.Timestamp(('2017-06-28')),\n                                         2018: pd.Timestamp(('2018-06-27')),\n                                         2019: pd.Timestamp(('2019-06-26'))})\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"wed_june{d}\": \n                                      (df.date - wed_june_date == np.timedelta64(d, \"D\")) & \n                                      (df.country != 'Norway')\n                                      for d in list(range(-4, 6))})],\n                       axis=1)\n    \n    # First Sunday of November\n    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n                                         2016: pd.Timestamp(('2016-11-6')),\n                                         2017: pd.Timestamp(('2017-11-5')),\n                                         2018: pd.Timestamp(('2018-11-4')),\n                                         2019: pd.Timestamp(('2019-11-3'))})\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"sun_nov{d}\": \n                                      (df.date - sun_nov_date == np.timedelta64(d, \"D\")) &\n                                      (df.country != 'Norway')\n                                      for d in list(range(0, 9))})],\n                       axis=1)\n    \n    # First half of December (Independence Day of Finland, 6th of December)\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d) &\n                                      (df.country == 'Finland')\n                                      for d in list(range(6, 14))})],\n                       axis=1)\n\n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"easter{d}\": \n                                      (df.date - easter_date == np.timedelta64(d, \"D\"))\n                                      for d in list(range(-2, 11)) + list(range(40, 48)) +\n                                      list(range(50, 59))})],\n                       axis=1)\n    \n    return new_df.astype(np.float32)\n\n\ntrain_test = process_data(train.append(test))\n\nprocessed_train = train_test.iloc[:len(train)].copy()\nprocessed_test = train_test.iloc[len(train):].copy()\n\ntarget = np.ravel(process_target(train))","4deab58c":"def weighting(df, weights):\n    return df.year.replace(weights).values\n    \nweights = weighting(train, {2015:1, 2016:1, 2017:1, 2018:1})","7b49acdc":"processed_train.shape, train.shape, processed_test.shape, test.shape","a01570ec":"def SMAPE(y_true, y_pred):\n    # From https:\/\/www.kaggle.com\/cpmpml\/smape-weirdness\n    denominator = (y_true + np.abs(y_pred)) \/ 200.0\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)\n\ndef SMAPE_exp(y_true, y_pred):\n    y_true = np.exp(y_true)\n    y_pred = np.exp(y_pred)\n    denominator = (y_true + np.abs(y_pred)) \/ 200.0\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)\n\ndef SMAPE_err(y_true, y_pred):\n    # From https:\/\/www.kaggle.com\/cpmpml\/smape-weirdness\n    denominator = (y_true + np.abs(y_pred)) \/ 200.0\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return diff","52f1bc0d":"# due to float calculations the computation is approximated\na = 5\nb = 7\nprint(a * b) # pure multiplicative\nprint(np.exp(np.log(a) + np.log(b))) # multiplicative made additive by log","9a47487d":"from sklearn.linear_model import LinearRegression, HuberRegressor\n\nreg = LinearRegression()\naddictive_model = reg.fit(processed_train, target)\naddictive_fit = addictive_model.predict(processed_train)\nprint(f\"addictive fit: {SMAPE(y_true=target, y_pred=addictive_fit): 0.3f}\")","30033efc":"multiplicative_model = reg.fit(processed_train, np.log(target))\nmultiplicative_fit = np.exp(multiplicative_model.predict(processed_train))\nprint(f\"multiplicative fit: {SMAPE(y_true=target, y_pred=multiplicative_fit):0.3f}\")","7a14030a":"def selective_rounding(preds, lower=0.3, upper=0.7):\n    # selective rounding\n    dec = preds % 1\n    to_round = (dec<=lower)|(dec>=upper)\n    preds[to_round] = np.round(preds[to_round])\n    return preds","4cdeffcc":"reg = LinearRegression()\n\nmodel = reg.fit(processed_train, np.log(target))\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/sample_submission.csv\")\npreds = np.exp(model.predict(processed_test))\n\n# rounding\npreds = selective_rounding(preds, lower=0.3, upper=0.7)\n\nsubmission.num_sold = preds\nsubmission.to_csv(\"submission.csv\", index=False)","47db8bd3":"log_target = np.log(target)\ntrain_set = list(train.index[train.date<'2018-01-01'])\nval_set = list(train.index[train.date>='2018-01-01'])\nmodel = LinearRegression()\n\nmodel.fit(processed_train.iloc[train_set], log_target[train_set])\npreds = np.exp(model.predict(processed_train.iloc[val_set]))\n\n# rounding\npreds = selective_rounding(preds, lower=0.3, upper=0.7)\n\nsmape = SMAPE(y_true=target[val_set], y_pred=preds)\nprint(f\"hold-out smape: {smape}\")","97b88299":"import calendar\n\nall_preds = np.exp(model.predict(processed_train))\n\nyear = 2017\nfor month in range(1, 13):\n    print(f\"\\n*** {calendar.month_name[month]} ***\\n\")\n    for product in ['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker']:\n        print(f\"\\n--- {product} ---\\n\")\n        fig = plt.figure(figsize=(20, 10), dpi=100)\n        fig.subplots_adjust(hspace=0.25)\n        for i, store in enumerate(['KaggleMart', 'KaggleRama']):\n            for j, country in enumerate(['Finland', 'Norway', 'Sweden']):\n                ax = fig.add_subplot(2, 3, (i*3+j+1))\n                selection = train.row_id.isin(train.row_id[train.year==year])\n                selected = train[selection].copy()\n                selected['preds'] = all_preds[selection]\n                selection = (selected['country']==country)&(selected['store']==store)&(selected['product']==product)\n                selection = selection & (selected.month==month)\n                selected = selected[selection].copy()\n                grouped = selected.set_index('date').groupby('day')\n                num_sold_grouped = grouped['num_sold'].mean()\n                preds_grouped = grouped['preds'].mean()\n                num_sold_grouped.plot(ax=ax, label='truth')\n                preds_grouped.plot(ax=ax, label='preds')\n                fests = festivities[(festivities.country==country)&(festivities.date.dt.month==month)&(festivities.date.dt.year==year)]\n                fest_idx = (fests.date.dt.day).values\n                if len(fest_idx) > 0:\n                    plt.plot(fest_idx, num_sold_grouped.values[fest_idx-1], 'o', color='red')\n                ax.set_title(f\"{calendar.month_name[month]}|{country}:{store}\")\n                ax.legend()\n        plt.show()","afcaf3d8":"##### We now process the data and scale it. Since EDA revealed how the different characteristics of the series are mostly main effects (country and store), we focus on finding the way to model the interaction between products and time.","ae6a7551":"##### The middle of the month usually presents less sales. The peak at the end may be influenced by seasonal peaks (end of year).","a37e448c":"#### In this notebook I start first from building a quick EDA revealing patters in the data, then I use EDA insights to build a simple linear model with only essential features (based on a out of time approach).","768519e1":"### If you liked the notebook, consider to upvote, thank you and happy Kaggling!","f380438d":"##### Since we will use Scikit-learn models, let's accelerate them first by using Intel Extension.","a2d4c191":"##### We prepare all the evaluation measures, both at an aggregate level, with exp transformation and at an individual cases level (for error analysis)","ccb8f63e":"##### The first series of panels points out that the country effect is kind of indipendent from store and product. There is an underlying country dynamic that replicates the same no matter the shop or the product sold by it. We also notice that shops differentiate only for the level of sales.","f5f1dd33":"##### Having completed the checks, we process the datetime information, extracting it informative elements at different time granularities:","aad93b6e":"* vers. 20 - introduced selective rounding\n* vers. 22 - adding sin\/cos processing of day of the year at different time lags\n* vers. 25 - updated festivities\n* vers. 27 - added error analysis\n* vers. 28 - revamped regression functional form\n* vers. 30 - adopting AmbrosM feature engineering","44dec271":"##### At a week level we see that differences are due to peaks. Peaks seem different in Spring. Probably is is Easter effect.","fa9e6c9b":"##### We then load the data, it is necessary paying attention to convert the date into datetime","9e4ecb59":"##### For testing purposes we refit the model only on years 2015-2017 using 2018 as an hold-out test.","8867f053":"##### Since EDA demonstrated that there is some kind of yearly change, we overweight more recent observations.","213873ff":"##### And we completed by inspecting at a day of the week level:","094020fa":"##### Having four complete years available allows various types of testing and modelling. In this EDA we will limit to use the last year available (2018) as an hold-out, setting our baseline model to be able to forecast an entire year in the future.","5b976ac4":"##### We now start obeserving recurrences at a monthly level:","034ef92b":"##### As a last check we verify that no date is missing from train and test:","8831be81":"##### We try modelling both an additive and a multiplicative predictor (for multiplicative you just need a log transoformation of the target because exponentiation of the sum of logs of the terms correspond to multiplications of the terms)","5eaf1a10":"##### We now proceed to examine seasonality even more in detail at a week level:","97ea491e":"##### As a second step let's visualize how time is split between train and test.","ec9122c1":"##### As a closing work, we check the fit of the model (residual analysis) in order to figure out if we correctly specified the functional form of the predictor. Holidays are marked in the chart.","239f498a":"##### Our next panels will explore seaasonality based on months:","9ffb6883":"##### Here we notice two important elements: seanality curves are different for each product and they also differ from year to year. Averaging the curves probably is safe bet for the future, as well as considering more relevant the recent years (thus weighting more the year 2018 for instance). For the sticker product, year 2017 seems particularly different from others.","fa0b3493":"##### We are ready to explore the data. In order to highlight the time series characteristics, we create panels of products x countries x shops. We start by aggregating at a year level.","381b822f":"##### Based on the information we got we now proceed to feature engineering and to enrich the data (using festivities and GDP data).","001769a3":"##### Before starting with EDA, it is important to check about the data structure. Apparently we have a combination of time series based on countries, stores and products. Let's first check if all the combinations appear in train and test.","000384ca":"##### It seems that the multiplicative model is better. We now use it to predict on the test set.","25413ea5":"##### Friday and the week-end are the best days, but Sundays are not always at the same level as Saturdays (it depends on the year - why?)."}}