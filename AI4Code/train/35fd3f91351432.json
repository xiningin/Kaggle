{"cell_type":{"ea4f96a6":"code","4d4cdd1c":"code","a2b64163":"code","44a9ea3d":"code","7da50289":"markdown","aee08c92":"markdown","3e89d1ca":"markdown","0d91b447":"markdown","d193322c":"markdown","6b8a44ad":"markdown","a444b5e1":"markdown","69272294":"markdown","9cf321ea":"markdown"},"source":{"ea4f96a6":"from sklearn.datasets import fetch_olivetti_faces\nfaces, labels = fetch_olivetti_faces(return_X_y=True, shuffle=True, random_state=42)","4d4cdd1c":"import numpy as np\nimport matplotlib.pyplot as plt\n\ndef _plot_face(face):\n    if face.shape != (64, 64):\n        face = face.reshape(64, 64)\n    plt.imshow(face, cmap='gray')\n    plt.axis('off')\n\n    \ndef plot_faces(faces, cols=4):\n    faces = np.array(faces)\n    if len(faces.shape) == 1:\n        faces = faces[None, :]\n    m = faces.shape[0]\n    \n    rows = m \/\/ cols\n    if m % cols != 0:\n        rows += 1\n    fig, axes = plt.subplots(rows, cols, figsize=(3*cols, 3*rows))\n    if len(axes.shape) == 1:\n        axes = axes[None, :]\n    \n    for i in range(rows):\n        for j in range(cols):\n            try:\n                plt.sca(axes[i, j])  # set current axes\n                face = faces[i * cols + j]  # get face\n            except IndexError:\n                plt.axis('off')\n                continue\n            _plot_face(face)\n    return fig, axes\n\n# plot all distinct persons\n_, idx = np.unique(labels, return_index=True)\nplot_faces(faces[idx], cols=5);","a2b64163":"from sklearn.decomposition import PCA\n","44a9ea3d":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(faces, labels, test_size=0.2, stratify=labels, random_state=42)","7da50289":"## a) Eigengesichter\n- Plotte die ersten 20 Hauptachsen (Eigenvektoren der Kovarianzmatrix). ","aee08c92":"- Plotte den Anteil der erkl\u00e4rten Varianz in Abh\u00e4ngigkeit der verwendeten Hauptkomponenten. Dazu kannst du das Attribut `explained_variance_ratio_` verwenden.","3e89d1ca":"Finale Version.","0d91b447":"# Dimensionsreduktion und Ensemble-Methoden\n\nIn dieser \u00dcbung werden wir uns der wichtigen Hauptkomponentenanalyse (*Principal Component Analysis* PCA) widmen und verschiedene Ensemble-Methoden verwenden, um ein Modell zur Gesichtserkennung zu entwickeln.\nDazu verwenden wir das *Olivetti-Faces* Datenset, welches aus 400 verschiedenen Bildern von 40 verschiedenen Personen besteht. Jedes Bild hat $64 \\times 64$ Pixel.","d193322c":"## c) Feature Importance\n- Berechne die *Gini Feature Importance*. Du kannst `imshow` f\u00fcr die Visualisierung verwenden.","6b8a44ad":"## b) Inverse Transformation\n\nBerechne eine PCA und plotte die Rekonstruktion von 5 Gesichter basierend auf $5, 10, 20, 50, 100, 200, 300$ und $ 400$ Hauptkomponenten. Dazu kannst du die Methode `inverse_transform` benutzen.","a444b5e1":"## d) Gesichtserkennung\n\nErstelle ein Modell zur Gesichtserkennung. Du kannst dazu Methoden deiner Wahl verwenden. Experimentiere mit verschiedenen Modellen (`SVC`, `RandomForestClassifier`, ...). \n- Erstelle auch einen `VotingClassifier` basierend auf verschiedenen Modellen. \n- Probiere auch eine `PCA` als Preprocessingschritt. Was macht der Parameter `whiten` in der PCA?\n- Wie hoch ist der Accuracy Score auf dem Trainings- und Testset? Kannst du einen Score von 1 auf dem Testset erreichen? Falls nicht, welche Personen werden verwechselt? Plotte die Geichter dieser Personen.\n","69272294":"- Wie gro\u00df sind die zugeh\u00f6rigen Eigenwerte (Varianzen) dieser Eigengesichter?","9cf321ea":"Um ein Bild zu plotten k\u00f6nnen wir `imshow` verwenden. Der folgende Code plottet jeweils das erste Gesicht der 40 verschiedenen Personen."}}