{"cell_type":{"e9f96f1c":"code","33f3ae39":"code","72db6e9a":"code","30af8696":"code","0b46732a":"code","74978858":"code","4c2b6a4e":"code","2f04c18a":"code","3e29d380":"code","57a4e469":"code","29b7230b":"code","387e64df":"code","072d0857":"code","e5f7c2e6":"code","daf58a49":"code","62d399fe":"code","18213edc":"code","3c07336b":"code","95ed959a":"code","6a7cd2f6":"code","57fa17d5":"code","23cb622e":"code","3b52b0ee":"code","3da54d21":"code","958b5617":"code","c340076c":"code","66cdf4e2":"code","dd936a81":"code","5906ca84":"code","f2bff3fe":"code","f964e31e":"code","f3092b38":"code","2d24d7cd":"code","6b6c0d69":"code","03c27c3b":"code","313b085c":"code","7f84d48e":"code","ab05a279":"code","a6271bb3":"code","0c3b5efe":"code","8f7cb404":"code","bf878a22":"code","3bca9723":"markdown","87387842":"markdown","dd29e2b5":"markdown","4c0f34fb":"markdown","ba3b9e27":"markdown","b028fc4d":"markdown","ceff98be":"markdown","495630e4":"markdown","f9f02c04":"markdown","67ce23ca":"markdown","a2e61389":"markdown","7a493972":"markdown","0aa64b9a":"markdown","6c0c4814":"markdown","a0da16da":"markdown","2b69988c":"markdown","2eefc481":"markdown","2703502f":"markdown","99ea18e9":"markdown","6f49719f":"markdown","db20d87d":"markdown","4b345d04":"markdown","36b7bef8":"markdown","9b868993":"markdown","24bbf42c":"markdown","5a2a4b82":"markdown","2fe1096c":"markdown","d9a140c2":"markdown","319088c2":"markdown","a5e23563":"markdown","a72428f9":"markdown","16ced65a":"markdown","57b4fe4a":"markdown","bff35ad8":"markdown","c132a9c6":"markdown","df173e6d":"markdown","f0e39cd9":"markdown","b86351d7":"markdown"},"source":{"e9f96f1c":"__seed = 0\n__n_folds = 5\n__nrows = None\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('ggplot')\n\nfrom tqdm import tqdm_notebook\n\nimport numpy as np\nimport pandas as pd\npd.set_option('max_colwidth', 500)\npd.set_option('max_columns', 500)\npd.set_option('max_rows', 500)\nfrom scipy.stats import chi2_contingency, kruskal, ks_2samp\n\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import StratifiedKFold, cross_validate\n\nfrom string import ascii_lowercase\nimport random\n\n# To avoid target leakage\nfolds1 = StratifiedKFold(n_splits=__n_folds, shuffle=True, random_state=__seed)\nfolds2 = StratifiedKFold(n_splits=__n_folds, shuffle=True, random_state=__seed+2)\nfolds3 = StratifiedKFold(n_splits=__n_folds, shuffle=True, random_state=__seed+4)","33f3ae39":"train = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/train.csv\", index_col = \"id\", nrows = __nrows)\ntest = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/test.csv\", index_col = \"id\", nrows = __nrows)\ntrain.head()","72db6e9a":"def coef_vcramer(contingency_df):\n    '''\n    A partir de la table de contingence de 2 variables, calcule le V de Cramer\n    \n    Param\u00e8tres :\n    ------------\n        - contingency_df : dataframe pandas\n            Table de contingence\n            Peut-\u00eate fabriqu\u00e9 \u00e0 partir d'un pd.crosstab(v1, v2)\n    R\u00e9sultats:\n    ----------\n        - v de Cramer\n    \n    Exemple : (G.Saporta, Probabilit\u00e9s, annalyse des donn\u00e9es et Statistiques, 3\u00e8me \u00e9dition p.150\n    --------------------------------------------------------------------------------------------\n    Tableau de contingence, lieu de vacances des fran\u00e7ais selon leur profession (Saporta p.147) :\n    \n    data={\"Agri\":[41, 47, 13, 59, 17, 26, 4, 9, 19],\n      \"Arti\":[220, 260, 71, 299, 120, 42, 64, 35, 29],\n      \"Cadres\":[685, 775, 450, 1242, 706, 139, 122, 100, 130],\n      \"profinter\":[485, 639, 292, 1250, 398, 189, 273, 68, 193],\n      \"Employ\u00e9s\":[190, 352, 67, 813, 163, 92, 161, 49, 72],\n      \"Ouvriers\":[224, 591, 147, 1204, 181, 227, 306, 74, 114],\n      \"Retrait\u00e9s\":[754, 393, 692, 1158, 223, 25, 195, 47, 115],\n      \"autres\":[31, 34, 2, 225, 42, 33, 5, 6, 14]}\n      \n    a=pd.DataFrame(data, index=[\"Hotel\", \"Loc\", \"rs\", \"rpp\", \"rspa\", \"tente\", \"carav\", \"aj\", \"village\"])\n    print(vcramer(a), \" = 0.12 ? Oui, alors conforme Saporta p.150\")\n    '''\n    chi2 = chi2_contingency(contingency_df)[0]\n    n = contingency_df.sum().sum()\n    r, k = contingency_df.shape\n    return np.sqrt(chi2 \/ (n * min((r-1), (k-1))))","30af8696":"def fit_describe_infos(train, test, __featToExcl = [], target_for_vcramer = None):\n    '''Describe data and difference between train and test datasets.'''\n    \n    stats = []\n    __featToAnalyze = [v for v in list(train.columns) if v not in __featToExcl]\n    \n    for col in tqdm_notebook(__featToAnalyze):\n            \n        dtrain = dict(train[col].value_counts())\n        dtest = dict(test[col].value_counts())\n\n        set_train_not_in_test = set(dtest.keys()) - set(dtrain.keys())\n        set_test_not_in_train = set(dtrain.keys()) - set(dtest.keys())\n        \n        dict_train_not_in_test = {key:value for key, value in dtest.items() if key in set_train_not_in_test}\n        dict_test_not_in_train = {key:value for key, value in dtrain.items() if key in set_test_not_in_train}\n            \n        nb_moda_test, nb_var_test = len(dtest), pd.Series(dtest).sum()\n        nb_moda_abs, nb_var_abs = len(dict_train_not_in_test), pd.Series(dict_train_not_in_test).sum()\n        nb_moda_train, nb_var_train = len(dtrain), pd.Series(dtrain).sum()\n        nb_moda_abs_2, nb_var_abs_2 = len(dict_test_not_in_train), pd.Series(dict_test_not_in_train).sum()\n        \n        if not target_for_vcramer is None:\n            vc = coef_vcramer(pd.crosstab(train[target_for_vcramer], train[col].fillna(-1)))       \n        else:\n            vc = 0\n            \n        stats.append((col, round(vc, 3), train[col].nunique()\n            , str(nb_moda_abs) + '   (' + str(round(100 * nb_moda_abs \/ nb_moda_test, 1))+'%)'\n            , str(nb_moda_abs_2) +'   (' + str(round(100 * nb_moda_abs_2 \/ nb_moda_train, 1))+'%)'\n            , str(train[col].isnull().sum()) +'   (' + str(round(100 * train[col].isnull().sum() \/ train.shape[0], 1))+'%)'\n            , str(test[col].isnull().sum()) +'   (' + str(round(100 * test[col].isnull().sum() \/ test.shape[0], 1))+'%)'\n            , str(round(100 * train[col].value_counts(normalize = True, dropna = False).values[0], 1))\n            , train[col].dtype))\n            \n    df_stats = pd.DataFrame(stats, columns=['Feature', \"Target Cramer's V\"\n        , 'Unique values (train)', \"Unique values in test not in train (and %)\"\n        , \"Unique values in train not in test (and %)\"\n        , 'NaN in train (and %)', 'NaN in test (and %)', '% in the biggest cat. (train)'\n        , 'dtype'])\n    \n    if target_for_vcramer is None:\n        df_stats.drop(\"Target Cramer's V\", axis=1, inplace=True)\n            \n    return df_stats, dict_train_not_in_test, dict_test_not_in_train","0b46732a":"dfi, _, _ = fit_describe_infos(train, test, __featToExcl=['target'], target_for_vcramer='target')\ndfi","74978858":"def color_and_top(nb_mod, feature, typ, top_n=None):\n    \n    if top_n is None:\n        resu = [\"g\", nb_mod]\n    elif nb_mod > 2*top_n:\n        resu = [\"r\", top_n]\n    elif nb_mod > top_n:\n        resu =[\"orange\", top_n]\n    else: \n        resu = [\"g\", nb_mod]\n    \n    title = feature[:20]+\" (\"+typ[:3]+\"-{})\".format(nb_mod)\n    resu.append(title)\n    \n    return resu\n\n\ndef plot_multiple_categorical(df, features, col_target=None, top_n=None\n                              , nb_subplots_per_row = 4, hspace = 1.3, wspace = 0.5\n                              , figheight=15, m_figwidth=4.2, landmark = .01):\n    \n#    sns.set_style('whitegrid')\n    \n    if not (col_target is None):\n        ref = df[col_target].mean() # Reference\n    \n    plt.figure()\n    if len(features) % nb_subplots_per_row >0:\n        nb_rows = int(np.floor(len(features) \/ nb_subplots_per_row)+1)\n    else:\n        nb_rows = int(np.floor(len(features) \/ nb_subplots_per_row))\n    fig, ax = plt.subplots(nb_rows, nb_subplots_per_row, figsize=(figheight, m_figwidth * nb_rows))\n    plt.subplots_adjust(hspace = hspace, wspace = wspace)\n\n    i = 0; n_row=0; n_col=0\n    for feature in features:\n        \n        i += 1\n        plt.subplot(nb_rows, nb_subplots_per_row, i)\n\n        dff = df[[feature, col_target]].copy() # I don't want transform data, only study them\n        \n        # Missing values\n        if dff[feature].dtype.name in [\"float16\", \"float32\", \"float64\"]:\n            dff[feature].fillna(-997, inplace=True)\n            \n        if dff[feature].dtype.name in [\"object\"]:\n            dff[feature].fillna(\"_NaN\", inplace=True)\n            \n        if dff[feature].dtype.name == \"category\" and dff[feature].isnull().sum() > 0:\n            dff[feature] = dff[feature].astype(str).replace('', '_NaN', regex=False).astype(\"category\")\n            \n        # Colors, title\n        bar_colr, top_nf, title = color_and_top(dff[feature].nunique(), feature, str(dff[feature].dtype), top_n)\n        \n        # stats\n        tdf = dff.groupby([feature]).agg({col_target: ['count', 'mean']})\n        tdf = tdf.sort_values((col_target, 'count'), ascending=False).head(top_nf).sort_index()\n        \n        tdf.index = tdf.index.map(str)\n        tdf = tdf.rename(index={'-997.0':'NaN'}) # Missing values\n        if not (top_n is None):\n            tdf.index = tdf.index.map(lambda x: x[:top_n]) # tronque les libell\u00e9s des modalit\u00e9s en abcisse\n        \n        tdf[\"ref\"] = ref\n        tdf[\"ref-\"] = ref-landmark\n        tdf[\"ref+\"] = ref+landmark\n        \n        # First Y axis, on the left\n        plt.bar(tdf.index, tdf[col_target]['count'].values, color=bar_colr) # Count of each category\n        \n        plt.title(title, fontsize=11)\n        plt.xticks(rotation=90)\n        \n        # Second Y axis, on the right\n        xx = plt.xlim()\n        if nb_subplots_per_row == 1:\n            ax2 = fig.add_subplot(nb_rows, nb_subplots_per_row, i, sharex = ax[n_row], frameon = False)\n        else:\n            ax2 = fig.add_subplot(nb_rows, nb_subplots_per_row, i, sharex = ax[n_row, n_col], frameon = False)\n        if not (col_target is None):\n            ax2.plot(tdf[col_target]['mean'].values, marker = 'x', color = 'b', linestyle = \"solid\") # Mean of each Category\n            ax2.plot(tdf[\"ref\"].values, marker = '_', color = 'black', linestyle = \"solid\", linewidth=4.0) # Reference\n            ax2.plot(tdf[\"ref-\"].values, marker = '_', color = 'black', linestyle = \"solid\", linewidth=1.0) # Reference\n            ax2.plot(tdf[\"ref+\"].values, marker = '_', color = 'black', linestyle = \"solid\", linewidth=1.0) # Reference\n        ax2.yaxis.tick_right()\n        ax2.axes.get_xaxis().set_visible(False)\n        plt.xlim(xx)\n\n        n_col += 1\n        if n_col == nb_subplots_per_row:\n            n_col = 0\n            n_row += 1\n            \n    plt.show();","4c2b6a4e":"plot_multiple_categorical(train, [v for v in list(train.columns) if v not in [\"target\"]], \"target\", top_n=17) ","2f04c18a":"def cross_val_and_print(pipe, X=train, y=train[\"target\"], cv=folds1, scoring=\"roc_auc\"\n                        , best_score = 0, comment1=\"\", comment2=\"\"):\n    ''' \n    Cross validate score and print result and print previous result\n    And show the score and the previous best score.\n    '''\n    scores = cross_validate(pipe, X, y, cv = cv, scoring = scoring, return_train_score = True)\n    cv_score = scores[\"test_score\"].mean()\n    \n    if cv == folds1:\n        precision = 1\n    elif cv == folds2: \n        precision = 2\n    else: \n        precision = 3\n    \n    if comment1 == \"\":\n        print(\"CV{} score on valid : {:.7f}  - Previous best valid score : {:.7f} - Train mean score : {:6f}\".\\\n          format(precision, cv_score, best_score, scores[\"train_score\"].mean()))\n    elif comment2 == \"\":\n        print(\"CV{} score on valid for {} : {:.7f}  - Previous best valid score : {:.7f} - Train mean score : {:6f}\".\\\n          format(precision, comment1, cv_score, best_score, scores[\"train_score\"].mean()))\n    else:\n        print(\"CV{} score on valid for {}={} : {:.7f}  - Previous best valid score : {:.7f} - Train mean score : {:6f}\".\\\n          format(precision, comment1, comment2, cv_score, best_score, scores[\"train_score\"].mean()))\n    \n    if cv_score > best_score:\n        best_score = cv_score\n\n    return cv_score, best_score","3e29d380":"# Logistic Regression parameters\nlr_params = {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.123456789, 'max_iter':500}\n\nohe1 = (OneHotEncoder(categories = 'auto', sparse = True, dtype = 'uint8', handle_unknown=\"ignore\"),\n        [f for f in train.columns if not f in [\"target\", \"bin_0\"]])\n    \n# Pipeline ; I use make column transformer, because I will use several encoder\n# and beacause it's a way to drop bin_0\npipe = make_pipeline(make_column_transformer(ohe1), LogisticRegression(**lr_params))","57a4e469":"_, best_score1 = cross_val_and_print(pipe)\n_, best_score2 = cross_val_and_print(pipe, cv=folds2)\n_, best_score3 = cross_val_and_print(pipe, cv=folds3)","29b7230b":"ohe1 = (OneHotEncoder(categories = 'auto', sparse = True, dtype = 'uint8', handle_unknown=\"ignore\"),\n        [f for f in train.columns if not f in [\"target\"]])\npipe = make_pipeline(make_column_transformer(ohe1), LogisticRegression(**lr_params))","387e64df":"_, best_score1 = cross_val_and_print(pipe, best_score=best_score1)\n_, best_score2 = cross_val_and_print(pipe, cv=folds2, best_score=best_score2)","072d0857":"for df in [train, test]:\n    df[\"bin_0_bin_3\"] = df[\"bin_3\"].astype(str) + df[\"bin_0\"].astype(str)","e5f7c2e6":"ohe1 = (OneHotEncoder(categories = 'auto', sparse = True, dtype = 'uint8', handle_unknown=\"ignore\"),\n        [f for f in train.columns if not f in [\"target\", \"bin_0\", \"bin_3\"]])\npipe = make_pipeline(make_column_transformer(ohe1), LogisticRegression(**lr_params))","daf58a49":"_, best_score1 = cross_val_and_print(pipe, best_score=best_score1)\n_, best_score2 = cross_val_and_print(pipe, cv=folds2, best_score=best_score2)\n_, best_score3 = cross_val_and_print(pipe, cv=folds3, best_score=best_score3)","62d399fe":"plot_multiple_categorical(train, [\"ord_0\", \"ord_1\", \"ord_2\", \"ord_3\", \"ord_4\", \"ord_5\"]\n                          , \"target\", nb_subplots_per_row=2) ","18213edc":"def transf_ordinal_features(serie):\n    \n    dtransf = {\"ord_1\":{'Novice':0, 'Contributor':1, 'Expert':2, 'Master':3, 'Grandmaster':4}\n        , \"ord_2\":{'Freezing':0, 'Cold':1, 'Warm':2, 'Hot':3, 'Boiling Hot':4, 'Lava Hot':5}\n        , \"nom_0\":{\"Blue\":1, \"Green\":2, \"Red\":3}\n        , \"nom_1\":{\"Circle\":1, \"Trapezoid\":2, \"Star\":3, \"Polygon\":4, \"Square\":5, \"Triangle\":6}\n        , \"nom_2\":{\"Dog\":1, \"Lion\":2, \"Snake\":3, \"Axolotl\":4, \"Cat\":5, \"Hamster\":6}\n        , \"nom_3\":{\"Finland\":1, \"Russia\":2, \"China\":3, \"Costa Rica\":4, \"Canada\":5, \"India\":6}\n        , \"nom_4\":{\"Bassoon\":1, \"Piano\":2, \"Oboe\":3, \"Theremin\":4}\n        , \"bin_0_bin_3_bis\":{\"T0\":0, \"F1\":2, \"F0\":1}\n              }\n\n    if serie.name == \"ord_0\":\n        new_serie = serie - 1\n    elif serie.name == \"ord_5\":\n        lm = serie.unique()\n        new_serie = serie.map({l:i for i, l in enumerate(list(np.sort(lm)))})\n    elif serie.name in [\"ord_3\", \"ord_4\"]:\n        new_serie = serie.str.lower().map({l:i for i, l in enumerate(list(ascii_lowercase))})\n    else:\n        new_serie = serie.map(dtransf[serie.name])\n        \n    return new_serie","3c07336b":"df = train[[\"target\"]].copy()\nfor f in [\"ord_0\", \"ord_1\", \"ord_2\", \"ord_3\", \"ord_4\", \"ord_5\", \"nom_0\", \"nom_1\", \"nom_2\", \"nom_3\"\n          , \"nom_4\"]:\n    df[f] = transf_ordinal_features(train[f])\n    \nplot_multiple_categorical(df, [\"ord_0\", \"ord_1\", \"ord_2\", \"ord_3\", \"ord_4\", \"ord_5\"\n                              , \"nom_0\", \"nom_1\", \"nom_2\", \"nom_3\", \"nom_4\"]\n                          , \"target\", nb_subplots_per_row=2) \n\ndel df","95ed959a":"class MyFeaturesEngineering(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, list_ordinal_features =[]):\n        \n        self.list_ordinal_features = list_ordinal_features\n        \n\n    def fit(self, x, y=None):\n        \n        return self\n    \n\n    def transform(self, x, y=None):\n        \n        df = x.copy()\n        \n        # Ordinal features\n        for v in self.list_ordinal_features:\n            df[v] = transf_ordinal_features(df[v])\n        \n        return df","6a7cd2f6":"ohe1_feats = [\"bin_0_bin_3\", \"bin_1\", \"bin_2\", \"bin_4\", \"nom_0\", \"nom_1\", \"nom_2\", \"nom_3\", \"nom_4\"\n             , \"nom_5\", \"nom_6\", \"nom_7\", \"nom_8\", \"nom_9\", \"day\", \"month\"]\nordinal_features = [\"ord_0\", \"ord_1\", \"ord_2\", \"ord_3\", \"ord_4\", \"ord_5\"]\n\nfor feat in ordinal_features:\n    ohe1 = (OneHotEncoder(categories = 'auto', sparse = True, dtype = 'uint8', handle_unknown=\"ignore\"),\n        list(set(ohe1_feats+ordinal_features)-{feat}))\n    \n    # To transform ordinal features to integer sorted by there target mean\n    MyFeE = MyFeaturesEngineering(list_ordinal_features = [feat])\n    \n    # Ordinal features : relation with target is linear, we need to normalize before regression\n    StdScalE = (StandardScaler(copy=False), [feat])\n    \n    pipe = make_pipeline(MyFeE, make_column_transformer(ohe1, StdScalE), LogisticRegression(**lr_params))\n    _, _ = cross_val_and_print(pipe, best_score=best_score1, comment1=feat)\n#    _, _ = cross_val_and_print(pipe, cv=folds2, best_score=best_score2, comment1=feat)","57fa17d5":"# How to be sure that ordinal encoding is better for ord_0 ?\n# Let's see on the 2 others combinations of 5 folds.\n\nordinal_features = [\"ord_0\"]\nohe1_feats = [\"bin_0_bin_3\", \"bin_1\", \"bin_2\", \"bin_4\", \"nom_0\", \"nom_1\", \"nom_2\", \"nom_3\", \"nom_4\", \"nom_5\"\n        , \"nom_6\", \"nom_7\", \"nom_8\", \"nom_9\", \"day\", \"month\"] + [\"ord_1\", \"ord_2\", \"ord_3\", \"ord_4\", \"ord_5\"]\n\nohe1 = (OneHotEncoder(categories = 'auto', sparse = True, dtype = 'uint8', handle_unknown=\"ignore\"),\n        ohe1_feats)\n    \n# To transform ordinal features to integer sorted by there target mean\nMyFeE = MyFeaturesEngineering(list_ordinal_features = ordinal_features)\n    \n# Ordinal features : relation with target is linear, we need to normalize before regression\nStdScalE = (StandardScaler(copy=False), ordinal_features)\n    \npipe = make_pipeline(MyFeE, make_column_transformer(ohe1, StdScalE), LogisticRegression(**lr_params))\n_, best_score2 = cross_val_and_print(pipe, cv=folds2, best_score=best_score2)\n_, best_score3 = cross_val_and_print(pipe, cv=folds3, best_score=best_score3)","23cb622e":"ordinal_features = [\"ord_0\", \"ord_1\", \"ord_2\", \"ord_3\", \"ord_4\", \"ord_5\"]\nohe1_feats = [\"bin_0_bin_3\", \"bin_1\", \"bin_2\", \"bin_4\", \"nom_0\", \"nom_1\", \"nom_2\", \"nom_3\", \"nom_4\"\n             , \"nom_5\", \"nom_6\", \"nom_7\", \"nom_8\", \"nom_9\", \"day\", \"month\"]\n\nohe1 = (OneHotEncoder(categories = 'auto', sparse = True, dtype = 'uint8', handle_unknown=\"ignore\"),\n        ohe1_feats)\n    \n# To transform ordinal features to integer sorted by there target mean\nMyFeE = MyFeaturesEngineering(list_ordinal_features = ordinal_features)\n    \n# Ordinal features : relation with target is linear, we need to normalize before regression\nStdScalE = (StandardScaler(copy=False), ordinal_features)\n    \npipe = make_pipeline(MyFeE, make_column_transformer(ohe1, StdScalE), LogisticRegression(**lr_params))","3b52b0ee":"_, best_score1 = cross_val_and_print(pipe, best_score=best_score1)\n_, best_score2 = cross_val_and_print(pipe, cv=folds2, best_score=best_score2)\n_, best_score3 = cross_val_and_print(pipe, cv=folds3, best_score=best_score3)","3da54d21":"plot_multiple_categorical(train, [\"day\", \"bin_0_bin_3\", \"month\"]\n                          , \"target\", nb_subplots_per_row=2) ","958b5617":"for df in [train, test]:\n    df[\"day_bis\"] = df[\"day\"].map({1:3, 2:2, 3:1, 4:0, 5:1, 6:2, 7:3})\n    df[\"bin_0_bin_3_bis\"] = df[\"bin_0_bin_3\"].map({\"F0\":1, \"F1\":2, \"T0\":0, \"T1\":0})\n    \nplot_multiple_categorical(train, [\"day\", \"day_bis\", \"bin_0_bin_3\", \"bin_0_bin_3_bis\"]\n                          , \"target\", nb_subplots_per_row=2) ","c340076c":"ohe1_feats = [\"bin_0_bin_3\", \"bin_1\", \"bin_2\", \"bin_4\", \"nom_0\", \"nom_1\", \"nom_2\", \"nom_3\", \"nom_4\"\n             , \"nom_5\", \"nom_6\", \"nom_7\", \"nom_8\", \"nom_9\", \"day_bis\", \"month\"]\nohe1 = (OneHotEncoder(categories = 'auto', sparse = True, dtype = 'uint8', handle_unknown=\"ignore\"),\n        ohe1_feats)\n    \n# To transform ordinal features to integer sorted by there target mean\nMyFeE = MyFeaturesEngineering(list_ordinal_features = ordinal_features)\n    \n# Ordinal features : relation with target is linear, we need to normalize before regression\nStdScalE = (StandardScaler(copy=False), ordinal_features)\n    \npipe = make_pipeline(MyFeE, make_column_transformer(ohe1, StdScalE), LogisticRegression(**lr_params))\n\n_, best_score1 = cross_val_and_print(pipe, best_score=best_score1)\n_, best_score2 = cross_val_and_print(pipe, cv=folds2, best_score=best_score2)\n_, best_score3 = cross_val_and_print(pipe, cv=folds3, best_score=best_score3)","66cdf4e2":"class MyBinsEncoder(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, nbins=100, nmin=20):\n        \n        self.nbins = nbins\n        self.nmin = nmin\n        \n    def fit(self, x, y=None):\n        \n        temp = pd.concat([x, y], axis=1)\n        \n        # Compute target mean for each value\n        averages = temp.groupby(by=x.name)[y.name].mean()\n        means = dict(zip(averages.index.values, averages.values))\n        \n        # binning in self.nbins bins\n        bins = np.linspace(averages.min(), averages.max(), self.nbins)\n        self.map_ = dict(zip(averages.index.values, np.digitize(averages.values, bins=bins)))\n\n        # But if there are more than self.nmin observations in a original value, keep the original value\n        # instead of bins.\n        count = temp.groupby(by=x.name)[y.name].count()\n        nobs = dict(zip(averages.index.values, count))\n        \n        for key, value in nobs.items():\n            if value > self.nmin:\n                self.map_[key] = key\n        \n        return self\n    \n    def transform(self, x, y=None):\n        \n        temp = x.map(self.map_)\n        # Especially for nom_7, nom_8 and nom_9\n        temp.fillna(random.choice(list(self.map_.values())), inplace=True)\n        temp = temp.astype(str)\n        \n        return temp\n    \n#essai = MyBinsEncoder(nbins=20, nmin=3)\n#essai.fit(train[\"nom_8\"], train[\"target\"])\n#print(essai.map_)\n#av = essai.transform(train[\"nom_8\"])\n\nessai = MyBinsEncoder(nbins=4, nmin=100000)\nessai.fit(train[\"day\"], train[\"target\"])\nprint(essai.map_)\nav = essai.transform(train[\"day\"])\nprint(av.value_counts(dropna=False))\nprint(av.nunique())","dd936a81":"class MyFeaturesEngineering(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, list_ordinal_features =[], feat_to_bins_encode = {}):\n        \n        self.list_ordinal_features = list_ordinal_features\n        \n        self.feat_to_bins_encode = feat_to_bins_encode\n        self.BinsEncoder={}\n        \n\n    def fit(self, x, y=None):\n        \n        # bins encoders\n        for feat, value in self.feat_to_bins_encode.items():\n            self.BinsEncoder[feat] = MyBinsEncoder(nbins=value[0], nmin=value[1])\n            self.BinsEncoder[feat].fit(x[feat], y)\n\n        return self\n    \n\n    def transform(self, x, y=None):\n        \n        df = x.copy()\n        \n        for v in self.feat_to_bins_encode.keys():\n            df[v] = self.BinsEncoder[v].transform(df[v])\n            \n        # Ordinal features\n        for v in self.list_ordinal_features:\n            df[v] = transf_ordinal_features(df[v])\n        \n        return df","5906ca84":"ordinal_features = [\"ord_0\", \"ord_1\", \"ord_2\", \"ord_3\", \"ord_4\", \"ord_5\"]\nohe1_feats = [\"bin_0_bin_3\", \"bin_1\", \"bin_2\", \"bin_4\", \"nom_0\", \"nom_1\", \"nom_2\", \"nom_3\", \"nom_4\"\n             , \"nom_5\", \"nom_6\", \"nom_7\", \"nom_8\", \"nom_9\", \"day_bis\", \"month\"]\n\nfeat = \"nom_5\"\nfor nbins in [[20, 35]]: # combination selected by CV\n    \n    ohe1 = (OneHotEncoder(categories = 'auto', sparse = True, dtype = 'uint8', handle_unknown=\"ignore\")\n            , ohe1_feats)\n\n    MyFeE = MyFeaturesEngineering(list_ordinal_features = ordinal_features\n                                 , feat_to_bins_encode={feat:nbins})\n\n    StdScalE = (StandardScaler(copy=False), ordinal_features)\n    \n    pipe = make_pipeline(MyFeE, make_column_transformer(ohe1, StdScalE), LogisticRegression(**lr_params))\n    _, best_score1 = cross_val_and_print(pipe, best_score=best_score1, comment1=feat, comment2=nbins)","f2bff3fe":"_, best_score2 = cross_val_and_print(pipe, cv=folds2, best_score=best_score2, comment1=feat, comment2=nbins)\n_, best_score3 = cross_val_and_print(pipe, cv=folds3, best_score=best_score3, comment1=feat, comment2=nbins)","f964e31e":"ordinal_features = [\"ord_0\", \"ord_1\", \"ord_2\", \"ord_3\", \"ord_4\", \"ord_5\"]\nohe1_feats = [\"bin_0_bin_3\", \"bin_1\", \"bin_2\", \"bin_4\", \"nom_0\", \"nom_1\", \"nom_2\", \"nom_3\", \"nom_4\"\n             , \"nom_5\", \"nom_6\", \"nom_7\", \"nom_8\", \"nom_9\", \"day_bis\", \"month\"]\n\nfeat = \"nom_6\"\nfor nbins in [[20, 5]]: # combination selected by CV\n    \n    ohe1 = (OneHotEncoder(categories = 'auto', sparse = True, dtype = 'uint8', handle_unknown=\"ignore\")\n            , ohe1_feats)\n\n    MyFeE = MyFeaturesEngineering(list_ordinal_features = ordinal_features\n                                 , feat_to_bins_encode={feat:nbins, \"nom_5\":[20, 35]})\n\n    StdScalE = (StandardScaler(copy=False), ordinal_features)\n    \n    pipe = make_pipeline(MyFeE, make_column_transformer(ohe1, StdScalE), LogisticRegression(**lr_params))\n    _, best_score1 = cross_val_and_print(pipe, best_score=best_score1, comment1=feat, comment2=nbins)","f3092b38":"_, best_score2 = cross_val_and_print(pipe, cv=folds2, best_score=best_score2, comment1=feat, comment2=nbins)\n_, best_score3 = cross_val_and_print(pipe, cv=folds3, best_score=best_score3, comment1=feat, comment2=nbins)","2d24d7cd":"ordinal_features = [\"ord_0\", \"ord_1\", \"ord_2\", \"ord_3\", \"ord_4\", \"ord_5\"]\nohe1_feats = [\"bin_0_bin_3\", \"bin_1\", \"bin_2\", \"bin_4\", \"nom_0\", \"nom_1\", \"nom_2\", \"nom_3\", \"nom_4\"\n             , \"nom_5\", \"nom_6\", \"nom_7\", \"nom_8\", \"nom_9\", \"day_bis\", \"month\"]\n\nfeat = \"nom_8\"\nfor nbins in [[15, 3]]: # choose by CV\n    \n    ohe1 = (OneHotEncoder(categories = 'auto', sparse = True, dtype = 'uint8', handle_unknown=\"ignore\")\n            , ohe1_feats)\n\n    MyFeE = MyFeaturesEngineering(list_ordinal_features = ordinal_features\n                                 , feat_to_bins_encode={feat:nbins, \"nom_5\":[20, 35]})\n\n    StdScalE = (StandardScaler(copy=False), ordinal_features)\n    \n    pipe = make_pipeline(MyFeE, make_column_transformer(ohe1, StdScalE), LogisticRegression(**lr_params))\n    _, best_score1 = cross_val_and_print(pipe, best_score=best_score1, comment1=feat, comment2=nbins)","6b6c0d69":"_, best_score2 = cross_val_and_print(pipe, cv=folds2, best_score=best_score2, comment1=feat, comment2=nbins)\n_, best_score3 = cross_val_and_print(pipe, cv=folds3, best_score=best_score3, comment1=feat, comment2=nbins)","03c27c3b":"ordinal_features = [\"ord_0\", \"ord_1\", \"ord_2\", \"ord_3\", \"ord_4\", \"ord_5\"]\nohe1_feats = [\"bin_0_bin_3\", \"bin_1\", \"bin_2\", \"bin_4\", \"nom_0\", \"nom_1\", \"nom_2\", \"nom_3\", \"nom_4\"\n             , \"nom_5\", \"nom_6\", \"nom_7\", \"nom_8\", \"nom_9\", \"day_bis\", \"month\"]\n\nfor feat in [\"bin_0_bin_3\", \"bin_1\", \"bin_2\", \"bin_4\", \"nom_0\", \"nom_1\", \"nom_2\", \"nom_3\", \"nom_4\"\n             , \"nom_5\", \"nom_6\", \"nom_8\", \"day_bis\", \"month\"]:\n    \n    ohe1 = (OneHotEncoder(categories = 'auto', sparse = True, dtype = 'uint8', handle_unknown=\"ignore\")\n            , list(set(ohe1_feats)-{feat}))\n\n    ohe2 = (OneHotEncoder(categories = 'auto', sparse = True, dtype = 'uint8', drop=\"first\")\n            , [feat])\n\n    MyFeE = MyFeaturesEngineering(list_ordinal_features = ordinal_features\n                                 , feat_to_bins_encode={\"nom_5\":[20, 35], \"nom_8\":[15, 3]})\n\n    StdScalE = (StandardScaler(copy=False), ordinal_features)\n    \n    pipe = make_pipeline(MyFeE, make_column_transformer(ohe1, ohe2, StdScalE)\n                         , LogisticRegression(**lr_params))\n    _, _ = cross_val_and_print(pipe, best_score=best_score1, comment1=feat)","313b085c":"ordinal_features = [\"ord_0\", \"ord_1\", \"ord_2\", \"ord_3\", \"ord_4\", \"ord_5\"]\nohe1_feats = [\"bin_0_bin_3\", \"bin_1\", \"bin_2\", \"bin_4\", \"nom_0\", \"nom_1\", \"nom_2\", \"nom_3\", \"nom_4\"\n             , \"nom_5\", \"nom_6\", \"nom_7\", \"nom_8\", \"nom_9\", \"day_bis\", \"month\"]\n\nfor feat in [\"bin_0_bin_3\", \"bin_1\", \"bin_2\", \"bin_4\", \"nom_0\", \"nom_1\", \"nom_2\", \"nom_3\", \"nom_4\"\n             , \"nom_5\", \"nom_6\", \"nom_8\", \"day_bis\", \"month\"]:\n    \n    ohe1 = (OneHotEncoder(categories = 'auto', sparse = True, dtype = 'uint8', handle_unknown=\"ignore\")\n            , list(set(ohe1_feats)-{feat}))\n\n    lval_to_drop = [train[feat].value_counts().index[0]] # the most frequent\n    ohe2 = (OneHotEncoder(categories = 'auto', sparse = True, dtype = 'uint8', drop=lval_to_drop)\n            , [feat])\n\n    MyFeE = MyFeaturesEngineering(list_ordinal_features = ordinal_features\n                                 , feat_to_bins_encode={\"nom_5\":[20, 35], \"nom_8\":[15, 3]})\n\n    StdScalE = (StandardScaler(copy=False), ordinal_features)\n    \n    pipe = make_pipeline(MyFeE, make_column_transformer(ohe1, ohe2, StdScalE)\n                         , LogisticRegression(**lr_params))\n    _, _ = cross_val_and_print(pipe, best_score=best_score1, comment1=feat)","7f84d48e":"ohe1_feats = [\"bin_0_bin_3\", \"bin_1\", \"nom_5\", \"nom_7\", \"nom_9\", \"month\"]\nohe2_feats = ['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_6', \"day_bis\"]\nohe3_feats = ['bin_2', 'bin_4', 'nom_8']\nordinal_features = [\"ord_0\", \"ord_1\", \"ord_2\", \"ord_3\", \"ord_4\", \"ord_5\"]\n\nMyFeE = MyFeaturesEngineering(list_ordinal_features = ordinal_features\n                                , feat_to_bins_encode={\"nom_5\":[20, 35], \"nom_8\":[15, 3]})\n\nStdScalE = (StandardScaler(copy=False), ordinal_features)\n    \nohe1 = (OneHotEncoder(categories = 'auto', sparse = True, dtype = 'uint8', handle_unknown=\"ignore\")\n        , ohe1_feats)\n\nlval_to_drop = [train[v].value_counts().index[0] for v in ohe2_feats] \nohe2 = (OneHotEncoder(categories = 'auto', sparse = True, dtype = 'uint8', drop=lval_to_drop)\n            , ohe2_feats)\n\nohe3 = (OneHotEncoder(categories = 'auto', sparse = True, dtype = 'uint8', drop='first'), ohe3_feats)\n\n\npipe = make_pipeline(MyFeE, make_column_transformer(ohe1, ohe2, ohe3, StdScalE)\n                        , LogisticRegression(**lr_params))\n_, best_score1 = cross_val_and_print(pipe, best_score=best_score1)\n_, best_score2 = cross_val_and_print(pipe, cv=folds2, best_score=best_score2)\n_, best_score3 = cross_val_and_print(pipe, cv=folds3, best_score=best_score3)","ab05a279":"ordinal_features = [\"ord_0\", \"ord_1\", \"ord_2\", \"ord_3\", \"ord_4\", \"ord_5\"]\nohe1_feats = [\"bin_0_bin_3\", \"bin_1\", \"bin_2\", \"bin_4\", \"nom_0\", \"nom_1\", \"nom_2\", \"nom_3\", \"nom_4\"\n             , \"nom_5\", \"nom_6\", \"nom_7\", \"nom_8\", \"nom_9\", \"day_bis\", \"month\"]\n\nfor feat in ['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_6', \"day_bis\"]:\n    \n    ohe1 = (OneHotEncoder(categories = 'auto', sparse = True, dtype = 'uint8', handle_unknown=\"ignore\")\n            , list(set(ohe1_feats)-{feat}))\n\n    lval_to_drop = [train[feat].value_counts().index[0]] \n    ohe2 = (OneHotEncoder(categories = 'auto', sparse = True, dtype = 'uint8', drop=lval_to_drop)\n            , [feat])\n\n    MyFeE = MyFeaturesEngineering(list_ordinal_features = ordinal_features\n                                 , feat_to_bins_encode={\"nom_5\":[20, 35], \"nom_8\":[15, 3]})\n\n    StdScalE = (StandardScaler(copy=False), ordinal_features)\n    \n    pipe = make_pipeline(MyFeE, make_column_transformer(ohe1, ohe2, StdScalE)\n                         , LogisticRegression(**lr_params))\n    _, _ = cross_val_and_print(pipe, cv=folds2, best_score=best_score2, comment1=feat)    \n#    _, _ = cross_val_and_print(pipe, cv=folds3, best_score=best_score3, comment1=feat)","a6271bb3":"ordinal_features = [\"ord_0\", \"ord_1\", \"ord_2\", \"ord_3\", \"ord_4\", \"ord_5\"]\nohe1_feats = [\"bin_0_bin_3\", \"bin_1\", \"bin_2\", \"bin_4\", \"nom_0\", \"nom_1\", \"nom_2\", \"nom_3\", \"nom_4\"\n             , \"nom_5\", \"nom_6\", \"nom_7\", \"nom_8\", \"nom_9\", \"day_bis\", \"month\"]\n\nfor feat in ['nom_1', 'nom_3', \"day_bis\"]:\n    \n    ohe1 = (OneHotEncoder(categories = 'auto', sparse = True, dtype = 'uint8', handle_unknown=\"ignore\")\n            , list(set(ohe1_feats)-{feat}))\n\n    lval_to_drop = [train[feat].value_counts().index[0]] \n    ohe2 = (OneHotEncoder(categories = 'auto', sparse = True, dtype = 'uint8', drop=lval_to_drop)\n            , [feat])\n\n    MyFeE = MyFeaturesEngineering(list_ordinal_features = ordinal_features\n                                 , feat_to_bins_encode={\"nom_5\":[20, 35], \"nom_8\":[15, 3]})\n\n    StdScalE = (StandardScaler(copy=False), ordinal_features)\n    \n    pipe = make_pipeline(MyFeE, make_column_transformer(ohe1, ohe2, StdScalE)\n                         , LogisticRegression(**lr_params))\n#    _, _ = cross_val_and_print(pipe, cv=folds2, best_score=best_score2, comment1=feat)    \n    _, _ = cross_val_and_print(pipe, cv=folds3, best_score=best_score3, comment1=feat)","0c3b5efe":"ordinal_features = [\"ord_0\", \"ord_1\", \"ord_2\", \"ord_3\", \"ord_4\", \"ord_5\"]\nohe1_feats = [\"bin_0_bin_3\", \"bin_1\", \"bin_2\", \"bin_4\", \"nom_0\", \"nom_1\", \"nom_2\", \"nom_3\", \"nom_4\"\n             , \"nom_5\", \"nom_6\", \"nom_7\", \"nom_8\", \"nom_9\", \"day_bis\", \"month\"]\n\nfor feat in ['bin_2', 'bin_4', 'nom_8']:\n    \n    ohe1 = (OneHotEncoder(categories = 'auto', sparse = True, dtype = 'uint8', handle_unknown=\"ignore\")\n            , list(set(ohe1_feats)-{feat}))\n\n    lval_to_drop = [train[feat].value_counts().index[0]] \n    ohe2 = (OneHotEncoder(categories = 'auto', sparse = True, dtype = 'uint8', drop='first')\n            , [feat])\n\n    MyFeE = MyFeaturesEngineering(list_ordinal_features = ordinal_features\n                                 , feat_to_bins_encode={\"nom_5\":[20, 35], \"nom_8\":[15, 3]})\n\n    StdScalE = (StandardScaler(copy=False), ordinal_features)\n    \n    pipe = make_pipeline(MyFeE, make_column_transformer(ohe1, ohe2, StdScalE)\n                         , LogisticRegression(**lr_params))\n    \n    _, _ = cross_val_and_print(pipe, cv=folds2, best_score=best_score2, comment1=feat)    \n#    _, _ = cross_val_and_print(pipe, cv=folds3, best_score=best_score3, comment1=feat)\n","8f7cb404":"ohe1_feats = [\"bin_0_bin_3\", \"bin_1\", 'bin_2', 'bin_4', 'nom_0', 'nom_2', 'nom_4', \"nom_5\", 'nom_6'\n              , \"nom_7\", \"nom_9\", \"month\"]\nohe2_feats = ['nom_1', 'nom_3', \"day_bis\"]\nohe3_feats = ['nom_8']\nordinal_features = [\"ord_0\", \"ord_1\", \"ord_2\", \"ord_3\", \"ord_4\", \"ord_5\"]\n\nMyFeE = MyFeaturesEngineering(list_ordinal_features = ordinal_features\n                                , feat_to_bins_encode={\"nom_5\":[20, 35], \"nom_8\":[15, 3]})\n\nStdScalE = (StandardScaler(copy=False), ordinal_features)\n    \nohe1 = (OneHotEncoder(categories = 'auto', sparse = True, dtype = 'uint8', handle_unknown=\"ignore\")\n        , ohe1_feats)\n\nlval_to_drop = [train[v].value_counts().index[0] for v in ohe2_feats] \nohe2 = (OneHotEncoder(categories = 'auto', sparse = True, dtype = 'uint8', drop=lval_to_drop)\n            , ohe2_feats)\n\nohe3 = (OneHotEncoder(categories = 'auto', sparse = True, dtype = 'uint8', drop='first'), ohe3_feats)\n\n\npipe = make_pipeline(MyFeE, make_column_transformer(ohe1, ohe2, ohe3, StdScalE)\n                        , LogisticRegression(**lr_params))\n_, best_score1 = cross_val_and_print(pipe, best_score=best_score1)\n_, best_score2 = cross_val_and_print(pipe, cv=folds2, best_score=best_score2)\n_, best_score3 = cross_val_and_print(pipe, cv=folds3, best_score=best_score3)","bf878a22":"lr_params = {'penalty': 'l2', 'solver': 'lbfgs', 'C': 0.123456789, 'max_iter':500}\n\npipe = make_pipeline(MyFeE, make_column_transformer(ohe1, ohe2, ohe3, StdScalE)\n                     , LogisticRegression(**lr_params))\n\n#score, _ = cross_val_and_print(pipe, best_score=best_score1)\n\n# Submission\npred = pipe.fit(train.drop([\"target\"], axis = 1), train[\"target\"]).predict_proba(test)[:, 1]\npd.DataFrame({\"id\": test.index, \"target\": pred}).to_csv(\"submission.csv\", index=False)","3bca9723":"# High carinality features - Binning of values","87387842":"## <a name=\"explo1\"><\/a>First Exploratory","dd29e2b5":"## nom_5","4c0f34fb":"Waouh ! Especially for ord_5 !","ba3b9e27":"On CV 2, the score does not grow ! What a f... ! <br>\n(still two hours before the end of the challenge....)<br>\nLet's see in detail :","b028fc4d":"# Let's submit it","ceff98be":"<a name=\"modl2\"><\/a>\n# Linear relation between ordinal features and target\n## Let's have a special look to ordinal features with all there values","495630e4":"print(chi2, \"1989 conforme Saporta p.150\")\nprint(\"V de Cramer {:.4f}. Est-il \u00e9gale au 0.12 Saporta p.150 ? Oui, alors ok\".format(coef_vcramer(a)))\ndata={\"Agri\":[41, 47, 13, 59, 17, 26, 4, 9, 19],\n      \"Arti\":[220, 260, 71, 299, 120, 42, 64, 35, 29],\n      \"Cadres\":[685, 775, 450, 1242, 706, 139, 122, 100, 130],\n      \"profinter\":[485, 639, 292, 1250, 398, 189, 273, 68, 193],\n      \"Employ\u00e9s\":[190, 352, 67, 813, 163, 92, 161, 49, 72],\n      \"Ouvriers\":[224, 591, 147, 1204, 181, 227, 306, 74, 114],\n      \"Retrait\u00e9s\":[754, 393, 692, 1158, 223, 25, 195, 47, 115],\n      \"autres\":[31, 34, 2, 225, 42, 33, 5, 6, 14]}\n      \na=pd.DataFrame(data, index=[\"Hotel\", \"Loc\", \"rs\", \"rpp\", \"rspa\", \"tente\", \"carav\", \"aj\", \"village\"])\nprint(coef_vcramer(a), \" = 0.12 ? Oui, alors conforme Saporta p.150\")","f9f02c04":"# 2nd place Solution - Categorical Feature Encoding Challenge\n# A simple solution - Alexandre Daubas, Paris","67ce23ca":"<a name=\"modl1\"><\/a>\n# First logistic ridge with some one hot encoders only","a2e61389":"# <a name=\"explo0\"><\/a>Libraries and read data","7a493972":"<a name=\"explo3\"><\/a>\n## Visual Exploratory\nVery important tool to see later the linear relation.","0aa64b9a":"Make clusters with values of day seems usefull.","6c0c4814":"<a name=\"explo2\"><\/a>\n## Cramer'V between features\nThink it is not a optimal way for calculation. But it proves there is not high relation between features.<br>Hence techniques using Trees (Cart, RandomForest, GBM) should by useless for this competition.","a0da16da":"For each feature :<br>\nIn <b>paranthesis<\/b> : type of the feature, number of unique values of the features.<br><br>\n<b>Left axis<\/b> : n obs<br>\n<b>Right axis<\/b> : target mean<br>\n<br>\nIn <b>black<\/b> : target mean on the train dataset.<br>\nIn black : target mean + or - a landmark.<br>\nIn <span style=\"color:blue\">blue<\/span> : target mean for the value.<br>\nIn <span style=\"color:green\">green<\/span> : n obs of all unique values.<br>\nIn <span style=\"color:orange\">orange<\/span> : n obs of only 17 of unique values.<br>\nIn <span style=\"color:red\">red<\/span> : n obs of only few values.<br>\n\nValues are in alphabetic order.<br>","2b69988c":"and nom_0 and nom_2 don't like drop=most frequent","2eefc481":"## Conclusion","2703502f":"print(\"Biggest Cramer'V in train\\n-------------------------\")\nlfeat = [v for v in list(train.columns) if v not in [\"target\"]]\ndone=[]\nfor v1 in lfeat:\n    done.append(v1)\n    for v2 in [v for v in lfeat if v not in done]:\n        c = coef_vcramer(pd.crosstab(train[v1], train[v2]))\n        if c > 0.08:\n            print(\"{}, {}, {:.5f}\".format(v1, v2, c))","99ea18e9":"Let's keep bin_0_bin_3 instead of bin_0 and bin_3.","6f49719f":"# Is bin_0 usefull ?","db20d87d":"\n## <a name=\"explo4\"><\/a>Let's have a look to ordinal features when there are all sorted by target mean","4b345d04":"## Drop the most frequent","36b7bef8":"So in fact nom_0, nom_2, nom_3, nom_4 and nom_6 don't like drop=most frequent","9b868993":"## Read data and Exploratory","24bbf42c":"## nom_8","5a2a4b82":"## Let's try the best combinations","2fe1096c":"## Look for the best one hot encoder for each feature\nRegression love parsimony, let's try to remove one value for each feature : the first one or the most frequent. And find for each feature the best.\n## Drop the first value","d9a140c2":"# Day and bin_0_bin_3","319088c2":"# <a name=\"explo\"><\/a>Before beginning\nAs you will see, all decisions were taken with a rigorous cross validation. That is the first trick.<br>\nI do simple things (I believe), i didn't try cycling Encoder, James-Stein Encoder. I did not try to decode nom_5, nom_6, nom_7, nom_8 and nom_9 : I believe it's impossible without a qubic machine ;-).<br>\nI tried elastic net, but it goes very slowly.<br>\n\nLogistic works well on this competition because features seems to be independant.<br>\nFor each nominal features, I take care to always drop a value. If I didn't, it was because the CV tolds me to do not.\n\nAnd the last trick, is the way to treat ordinal features. Instead of using a one hot encoder or a thermometer encoder, cross validation shows that standard scaler is better. Why ? The relation between each ordinal feature is obviously [linear](#modl3). Modeling the linear relation in regression consists in adjust only one coefficient, instead of p-1 coefficients if you use a one hot encoder. We need to standard scaler to have an ordinal feature between 0 and 1 like other features (regression needs that all features to be on the same scale). With this trick, I jump from 0.80820 to 0.80852 on the public LB.","a5e23563":"### Important trick :\nSo there is a <b>linear relation<\/b> beetween target and each ordinal features.<br>\nhttps:\/\/www.kaggle.com\/c\/cat-in-the-dat\/discussion\/113726#latest-687840<br>\nhttps:\/\/www.kaggle.com\/c\/cat-in-the-dat\/discussion\/117369#latest-680216<br>","a72428f9":"Biggest Cramer'V in train\n-------------------------\nbin_0, nom_8, 0.08647\nbin_0, nom_9, 0.19934\nbin_1, nom_8, 0.08757\nbin_1, nom_9, 0.19784\nbin_2, nom_8, 0.08423\nbin_2, nom_9, 0.20056\nbin_3, nom_8, 0.08536\nbin_3, nom_9, 0.20039\nbin_4, nom_8, 0.08523\nbin_4, nom_9, 0.19910\nnom_0, nom_8, 0.08622\nnom_0, nom_9, 0.19958\nnom_1, nom_8, 0.08693\nnom_1, nom_9, 0.19929\nnom_2, nom_8, 0.08655\nnom_2, nom_9, 0.19898\nnom_3, nom_8, 0.08593\nnom_3, nom_9, 0.19968\nnom_4, nom_8, 0.08588\nnom_4, nom_9, 0.19927\nnom_5, nom_8, 0.08584\nnom_5, nom_9, 0.20026\nnom_6, nom_8, 0.08558\nnom_6, nom_9, 0.19954\nnom_7, nom_8, 0.08598\nnom_7, nom_9, 0.20013\nnom_8, nom_9, 0.19975\nnom_8, ord_0, 0.08579\nnom_8, ord_1, 0.08532\nnom_8, ord_2, 0.08592\nnom_8, ord_3, 0.08576\nnom_8, ord_4, 0.08571\nnom_8, ord_5, 0.08609\nnom_8, day, 0.08537\nnom_8, month, 0.08554\nnom_9, ord_0, 0.20043\nnom_9, ord_1, 0.20002\nnom_9, ord_2, 0.19994\nnom_9, ord_3, 0.19988\nnom_9, ord_4, 0.19973\nnom_9, ord_5, 0.19949\nnom_9, day, 0.20096\nnom_9, month, 0.19957","16ced65a":"<b>bin_0<\/b> seems to be useless, because target means are quite the same for the two values of bin_0. I won't use it at the beginning.<br>\n<b>Ordinal features<\/b> are not sorted.","57b4fe4a":"This was the way to jump to 0.80840 on the Public LB.","bff35ad8":"No, it is not sure, but let's try a combination with bin_3","c132a9c6":"ord_0, ord_3, ord_4 and ord_5 seems to have a linear relation with target.","df173e6d":"and bin_2 and bin_4 don't like drop='first'","f0e39cd9":"## nom_6","b86351d7":"No nan's value.<br>\nbin_0 has 87% of his value in a unique one.<br>\nbin_0, bin_2 and bin_3 have a very low Cramer's V with target.<br>\n\nThere are 87 values of nom_9 in train not in test, and 4 of nom_8.<br>\nTrain and test datasets seem to be similar.\nAnd Bojan Tunguz in https:\/\/www.kaggle.com\/tunguz\/adversicat check it."}}