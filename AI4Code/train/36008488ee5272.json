{"cell_type":{"4039ef3e":"code","348e05b0":"code","6786444d":"code","26b5ddea":"code","e6c05aea":"code","f50ff161":"code","587deb6f":"code","2529da0d":"code","8f30ec55":"code","2676e377":"code","573a9edb":"code","a12f0d43":"code","08293239":"code","08d80ee5":"code","823f5a25":"code","8cb7dc4d":"code","5c4ffcda":"code","629f92c6":"code","341e8dd3":"code","3e4a8c3f":"code","105bf903":"code","13e54921":"code","1768fc06":"code","a275f9ea":"code","c860fb23":"code","54d5e189":"code","6a64c679":"code","0790aed9":"code","b761aa2a":"code","d36589fe":"code","45ffc327":"code","a1120463":"markdown","e3b7a857":"markdown","de69a12d":"markdown","3bb825ae":"markdown","9370b613":"markdown","7584bbc5":"markdown","164cb76a":"markdown","d8e82d06":"markdown","e56ee791":"markdown","41ad11fa":"markdown","80e05d4c":"markdown","bac42d19":"markdown","d8c511b6":"markdown","95def705":"markdown","f0ce991b":"markdown","e2a21bb6":"markdown","19e1e1ab":"markdown","4357fb2d":"markdown","b3986d8c":"markdown","0ded5976":"markdown","91abd8e3":"markdown","257b365e":"markdown","276d234a":"markdown","3cbdf524":"markdown","1e2e6ef4":"markdown","38d39bcb":"markdown","605c9b31":"markdown","21ae0166":"markdown","d62905ff":"markdown","c4e6d52a":"markdown","493e011b":"markdown","81e4743f":"markdown","2d3c50bc":"markdown","1c7d323e":"markdown","991c0363":"markdown","46d4485a":"markdown","3c9c95c2":"markdown","9e0b4d8a":"markdown","1efb8156":"markdown"},"source":{"4039ef3e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n","348e05b0":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom decimal import Decimal\nimport datetime\nimport folium #Longitude and Lattitude mapping.\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder #We don't this in the analysis I am currently showing.\nfrom sklearn.linear_model import LinearRegression #We don't this in the analysis I am currently showing.\nimport seaborn as sns\nfrom itertools import *\nimport os\nimport folium\nfrom folium import plugins\nfrom folium.plugins import MarkerCluster #To be able to cluster our individual data points on folium.\nfrom IPython.display import HTML, display\n\n","6786444d":"calendar = pd.read_csv('..\/input\/seattle\/calendar.csv')\nlisting = pd.read_csv('..\/input\/seattle\/listings.csv')\nreviews = pd.read_csv('..\/input\/seattle\/reviews.csv')","26b5ddea":"reviews.head()","e6c05aea":"calendar.head()","f50ff161":"listing.head()","587deb6f":"calendar = calendar.dropna(axis = 0, subset = ['price'], how = 'any')\nreviews = reviews.dropna(axis = 0, subset = ['comments'], how = 'any')\ncalendar['date'] = pd.to_datetime(calendar['date'])\ncalendar['month'] = calendar.date.dt.month\ncalendar['year'] = calendar.date.dt.year\ncalendar['day'] = calendar.date.dt.day\ncalendar['price'] = pd.to_numeric(calendar['price'].apply(lambda x: str(x).replace('$', '').replace(',', '')), errors='coerce')\n\nlisting['monthly_price'] = pd.to_numeric(listing['monthly_price'].apply(lambda x: str(x).replace('$', '').replace(',', '')), errors='coerce')\nlisting['weekly_price'] = pd.to_numeric(listing['weekly_price'].apply(lambda x: str(x).replace('$', '').replace(',', '')), errors='coerce')\nlisting['price'] = pd.to_numeric(listing['price'].apply(lambda x: str(x).replace('$', '').replace(',', '')), errors='coerce')\nlisting['cleaning_fee'] = pd.to_numeric(listing['cleaning_fee'].apply(lambda x: str(x).replace('$', '').replace(',', '')), errors='coerce')\nlisting['security_deposit'] = pd.to_numeric(listing['security_deposit'].apply(lambda x: str(x).replace('$', '').replace(',', '')), errors='coerce')\nlisting['extra_people'] = pd.to_numeric(listing['extra_people'].apply(lambda x: str(x).replace('$', '').replace(',', '')), errors='coerce')\nlisting = listing.rename(columns = {'id':'listing_id'})","2529da0d":"(listing.isnull().sum()[listing.isnull().sum().nonzero()[0]])\/len(listing) ","8f30ec55":"default_list = listing[['property_type', 'neighbourhood', 'review_scores_value', \n                        'bathrooms', 'bedrooms', 'price', 'longitude', 'latitude']]\n\nnew_list = default_list.dropna(axis = 0, how = 'any')\nnew_list","2676e377":"def Plot(cur, data_list):\n    \"\"\"Description: This function can be used to plot a graph by reading the data_list and grouping by the cursor obejct.\n    \n    Arguments: \n    cur: the cursor object.\n    data_list: list of data.\n    \n    Returns:\n    A graphical reprementation of the cur items in the data_list.\"\"\"\n\n\n    plt.figure(figsize=(20,20))\n    plt.xticks(rotation=90)\n    sns.countplot((data_list)[(cur)],\n                 order = data_list[cur].value_counts().index)\n    plt.show()","573a9edb":"(new_list.property_type.value_counts())\/(new_list.property_type.count())","a12f0d43":"Plot('property_type', new_list)","08293239":"(new_list.neighbourhood.value_counts())\/(new_list.neighbourhood.count())","08d80ee5":"Plot('neighbourhood', new_list)","823f5a25":"new_list_neighbourhood = new_list.groupby('neighbourhood').count()\nnew_list_top_15_neighbourhood = new_list_neighbourhood.nlargest(35,'property_type')\nnew_list_top_15_neighbourhood\n\n","8cb7dc4d":"neighbourhood_list = ['Capitol Hill',\n'Ballard',\n'Belltown',\n'Minor',\n'Queen Anne',\n'Fremont',\n'Wallingford',\n'First Hill',\n'North Beacon Hill',\n'University District',\n'Stevens',\n'Central Business District',\n'Lower Queen Anne',\n'Greenwood',\n'Columbia City',\n'Ravenna',\n'Magnolia',\n'Atlantic',\n'North Admiral',\n'Phinney Ridge',\n'Green Lake',\n'Leschi',\n'Mount Baker',\n'Eastlake',\n'Maple Leaf',\n'Madrona',\n'Pike Place Market',\n'The Junction',\n'Seward Park',\n'Bryant',\n'Genesee',\n'North Delridge',\n'Roosevelt',\n'Crown Hill',\n'Montlake']","5c4ffcda":"#This gives me a list of True\/False statements for each row if the value in the neighbourhood columns is in \n#neighbourhood_list above.\ntrue_false_by_neighbourhood = new_list.neighbourhood.isin(neighbourhood_list) \n\n#I can then put this new list of True\/False statements into our origional new_list. This filters new_list down\n# to 35 categories of the neighbourhood column while still containing 86.72% of the origional data.\nfiltered_neighborhood = new_list[true_false_by_neighbourhood]\nfiltered_neighborhood","629f92c6":"new_list_property_type = new_list.groupby('property_type').count()\nnew_list_top_16_property_type = new_list_property_type.nlargest(16, 'neighbourhood')\nnew_list_top_16_property_type\n\n\n","341e8dd3":"property_type_list = ['House', 'Apartment', 'Townhouse', 'Condominium', 'Loft', 'Bed & Breakfast', 'Cabin']","3e4a8c3f":"#This gives me a list of True\/False statements for each row if the value in the property_type column is in \n#the property_type_list above.\ntrue_false_by_property = filtered_neighborhood.property_type.isin(property_type_list) \n\n#I can then put this new list of True\/False statements into our filtered_neighbourhood list. \n#This filters new_list down seven property types while still containing 98.30% of the data of filtered_neighbourhood.\nfiltered_data = filtered_neighborhood[true_false_by_property]\nfiltered_data","105bf903":"sns.lmplot(data=filtered_data, x='bedrooms', y='price', hue='review_scores_value')","13e54921":"room_premium = (filtered_data.price)\/(filtered_data.bedrooms)\nfiltered_data['Cost Per Bedroom'] = filtered_data['latitude'].add(room_premium)\nfiltered_data","1768fc06":"def Filterlist(cur, filepath):\n    \"\"\"Description: This function can be used to read the file and filter based on the input.\n    \n    Arguments: \n    cur: the cursor object.\n    filepath: data file\n    \n    Returns: \n    The data file that is filtered by the cur object.\"\"\"\n    \n    property_type = [cur]\n    true_false_by_property = filtered_data.property_type.isin(property_type) \n    List = filtered_data[true_false_by_property]\n    return List\n\n","a275f9ea":"House_list = Filterlist('House', filtered_data)\nHouse_list","c860fb23":"def catplot(x_data, y_data, data_list):\n    \"\"\"Description: This function can be used to read the file in the data_list and create a catplot based on the\n    x_data and y_data.\n    \n    Arguments: \n    x_data: Column in data_list that you want to plot on the x axis (Put as string). \n    y_ data: Column in the data_list that you want to plot on the y axis (Put as string).\n    data_list: The datalist that contains the x_data and y_data as columns.\"\"\"\n    sns.catplot(x=x_data, y=y_data, data=data_list, height=8)","54d5e189":"catplot('Cost Per Bedroom', 'neighbourhood', House_list)","6a64c679":"Apartment_list = Filterlist('Apartment', filtered_data)\nApartment_list","0790aed9":"catplot('Cost Per Bedroom', 'neighbourhood', Apartment_list)","b761aa2a":"def folium_plot(x, y, data_list):\n    \"\"\"Find the latitude and longitude columns in data_list and plots them. \n    Then it custers the data into groups to be viewed on different levels.\n    \n    Arguements: \n        latitude: columns containing latitude coordinates and labeled as latitude. Write as string.\n        longitude: columns containing longitude coordinates and labeled as longitude. Write as string.\n        data_list: Data list where columns are held.\n    \n    Returns:\n    Clustered map of all data points.\"\"\"\n    \n    #Creates a map of Seattle.\n    m = folium.Map(location=[47.60, -122.24], zoom_start = 11)\n    m.save('index.html')\n\n\n    #Takes the latitude and longitude coordinates and zips them into a form to be plotted.\n    lat = pd.to_numeric(data_list[x], errors = 'coerce')\n    lon = pd.to_numeric(data_list[y], errors = 'coerce')\n\n    #Zip togethers each list of latitude and longitude coordinates. \n    result = zip(lat,lon)\n    lat_lon = list(result)\n\n\n    mc = MarkerCluster().add_to(m)\n    for i in range(0,len(data_list)):\n        folium.Marker(location=lat_lon[i]).add_to(mc)\n\n    m.save('index.html')\n    display(m)","d36589fe":"folium_plot('latitude', 'longitude', House_list)","45ffc327":"folium_plot('latitude', 'longitude', Apartment_list)","a1120463":"Let's do the same thing for property_type.","e3b7a857":"Immediately I see there are a few property types that hold a majority of the dataset and a number of them that represent a very small percentage of the dataset. The same can be said for neighbourhoods. \n\nnew_list.groupby('neighbourhood').nunique() - This gives me 79 unique values for the neighbourhood column.\n\nTherefore, I want to simplify this list to limit the number of features I have while still maintaining the integrity of the data. ","de69a12d":"It seems that there is much more varability in terms of price for houses depending on the neighbourhood.\n\nQueen Anne, Ballard, Minor, Capitol Hill, and Stevens looked among the pricer neighbourhoods. However, that is debatable since the varability is so high.\n\nFor Apartments there is less varability in the price per room. \n\nQueen Anne, Ballard, Lower Queen Anne, Minor, Capitol Hill, Pike Place Market, Central Business District, Belltown, and Firsthill ranked among the most expensive. ","3bb825ae":"**Business Understanding**","9370b613":"I want to be able to effective measure how expensive each neighbourhood is. I suspect there is a correlation with number of bedrooms listed and price of the property. I will use the below code to plot that. ","7584bbc5":"I now have a list of the top 35 neighbourhoods that represents about 86.72% of the dataset. I am going to put this in a list that I can use to filter our original dataset and remove the neighbourhoods that hold very little data.","164cb76a":"Great! We now have a numeric dataset that we can run some basic analysis on. Let's use seaborn to plot out some basic info regarding this new dataset. I will look at the count of each property type.","d8e82d06":"The chart above indicates that there is a correlation with the number of rooms and price. If I want to view how expensive each neighbourhood I will add another column of price\/number of rooms. This removes the potential for a neighbourhood to be more \"expensive\" just because it has a higher number of rooms available than average.","e56ee791":"**Map of Apartments**","41ad11fa":"I will use our neighbourhood_list we created (above) and see if our new_list dataset has a value within that list. This will filter out 44 (55.70%) of the unique neighbourhood categories while only removing 13.27% of the data.","80e05d4c":"**Results Exaluation**","bac42d19":"Note: When working with kaggle datasets that are already in the system. Double click the csv files to get the file path of kaggle for the import below. In this case the file path is input and seattle (see below).","d8c511b6":"**Data Modeling**","95def705":"Let's start with grouping the new_list dataset by neighbourhood and counting them up. \n\nI will then use the nlargest function to give me the top 35 neighbourhoods.","f0ce991b":"I can see there are a lot of featues that have a high NaN percentage within the listing dataset. However, there are a few\ncolumns that can be used to create a robust analysis. \n\n1. property_type\n2. neighbourhood\n2. review_scores_value\n3. bathrooms\n4. bedrooms\n5. price #You can't find this column in the above list. I had to use listing.columns to find it.\n6. longitude\n7. latitude\n\nI will create a new dataset that includes these features and call it new_list.\n","e2a21bb6":"**Map of Houses**","19e1e1ab":"I am going to use this Filterlist function to filter by 'House' and run some analysis on that section of the data. ","4357fb2d":"We now have a column at the end that gives the price for a single room.","b3986d8c":"I wanted to thank the Kaggle community and my mentors for providing me this opportunity to learn. I just started my Data Science journey a couple of weeks ago and everything I used in this analysis I had to scourge StackOverFlow and YouTube tutorials (while repeatedly banging my head over my keyboard). I would greatly appreciate any suggestions or helpful tips on what to learn next and improve upon. Thank you.\n","0ded5976":"There are a number of NaN values in the calendar and reviews dataset. I will just drop them using the drop method. Usually I would only drop columns that are irrelevant to the analysis and replace NaN values with the mean or mode of the dataset. However, I won't be using the calendar and review datasets much in my analysis but will do a basic cleanup just incase.\n\nThe listing dataset looks very useful so I will take a deeper look into it rather than just dropping all NaN values. \n\n","91abd8e3":"Our list filtered_data took our new_list and removed the property_types and neighbourhoods that contained very little data. As you can see, we still have 2428 data points from our 2463 points in new_list.","257b365e":"I'm now going to create a function that allows me to filter out whichever property_type I want while preserving the original data for further analysis. ","276d234a":"****","3cbdf524":"I used this youtube tutorial to help on this part. [https:\/\/www.youtube.com\/watch?v=2AFGPdNn4FM](http:\/\/)","1e2e6ef4":"Now let's clean up the list by property_type.","38d39bcb":"I cannot work with this data yet because I cannot convert the data objects anywhere with dollar signs and commas associated with pricing. I will use a function to replace all dollar signs and commas with blanks so I can convert it to a numeric dataset. ","605c9b31":"**Introduction**\nI recently visited Seattle with a number of good friends and had nothing but positive comments about the city. I grew up in the San Francisco\/Bay Area all my life and when I stepped foot in Seattle I saw it as a fresh start with exciting new opportunities. In this notebook I will be taking my first step into data science so what better data to explore than with the city that started my curiosity into tech.\n\nI have three questions I want to answer with this dataset. \n1. Where are the most expensive neighbourhoods to rent an AirBnB?\n2. How are property types dispersed throughout Seattle?\n3. Are there areas of potential opportunities depending on neighbourhoods?\n\nI will try to keep this as informative as possible for other beginner data scientists and programers like myself and will include YouTube tutorials I looked at to aid in this project.","21ae0166":"I used this YouTube tutorial, StackOverFlow, and Kaggle for the code below regarding lambda expressions. [https:\/\/www.youtube.com\/watch?v=25ovCm9jKfA&t=111s](http:\/\/) \n\nI am just going through each row in the columns that have a dollar sign and comma and removing them.","d62905ff":"**Data Preparation**","c4e6d52a":"First off let's import the libraries we will need.","493e011b":"****","81e4743f":"I noticed some districts appeared in the Apartment_list that were not amoung the House_list. I am going to plot the points on a map using each AirBnB latitiude and longitude coordinates and compare the two lists.","2d3c50bc":"Now that we have a filtered, clean data list. Let's start plotting our data and see what information we can glean.","1c7d323e":"Comparing the two maps it looks to be that Apartments are clustered around central Seattle (Neighbourhood: Belltown) while Houses are more dispersed up north (Neighbourhoods: Ballard, Fremont, and Madrona). This can explain the variability in terms of price per room since Apartments are clustered in one area and houses are dispersed throughout Seattle. \n\nIn addition, the maps above indicate areas of high AirBnB listings which imply high AirBnB demand. However there are many areas close to high density AirBnB clusters that have little AirBnB's avaliable. My next step would be to create a model to predict prices depending on the features listed in this analysis and compare that to the areas that have few AirBnB rentals but have a high predicted listing price based on their proxity to current listings. We can then run further analysis to see if there are opportunities to promote more AirBnB listings there and updating our models to obtain live data from the AirBnB API's. ","991c0363":"Since about 98.55% of property_types are in the top 7. I will just take the top seven of property types for my list:\n","46d4485a":"A majority of the properties listed are Apartments and Houses which I will focus on later in the analysis.\n\nI will do the same thing for neighbourhood.","3c9c95c2":"Now that all the data is decently cleaned I wanted to take a deeper look at the columns I am dealing with. I will use the code below that goes through each column and gives me the sum of NaN values. I just divide the sum by the length of the dataset to give me a percentage of NaN values.\n\nAnother method is just use the describe function and divide by the length of the listing data (below). You just need to make sure you read it inversely (100% means all values are accounted for and 80% means 20% are NaN.\n\nlisting.describe()\/len(listing)","9e0b4d8a":"I used this YouTube tutorial to learn about folium. I tried basemap and geopandas first but ran into issues with my system. [https:\/\/www.youtube.com\/watch?v=4RnU5qKTfYY&t=796s](http:\/\/)","1efb8156":"**Data Understanding**"}}