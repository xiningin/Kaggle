{"cell_type":{"f06bc242":"code","d73fee59":"code","d98ea925":"code","78d3d7dd":"code","3a7d80e7":"code","82cbdc7e":"code","9fc38729":"code","540a9f45":"code","30aac9b8":"code","114731cb":"code","071995e4":"code","f380c498":"code","fe97b25c":"code","2725886e":"code","3a7925fb":"code","0f9c05fe":"markdown","975bfa13":"markdown","bb675a20":"markdown","a2e62283":"markdown","721c07ca":"markdown","b5b0a2e2":"markdown","646c966b":"markdown","4d5a16ff":"markdown","12a07823":"markdown","5810b065":"markdown","f8b81905":"markdown","f1ca6d12":"markdown","6b30e7c9":"markdown"},"source":{"f06bc242":"import numpy as np\nimport pandas as pd\nimport sklearn\n\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\nimport statistics as stats","d73fee59":"X = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\", index_col='PassengerId')\ny = X.pop('Survived') # target variable\nX = X.drop(['Name','Ticket','Cabin'], axis=1) # drop some columns for simplification\nX['Embarked'].fillna(stats.mode(X['Embarked']), inplace = True) # replace the 2 missing values with the mode\nX","d98ea925":"print(\"Extraction based on data type:\")\nprint(list(X.select_dtypes(exclude=np.number).columns))\n\ncat_cols = ['Pclass','Sex','Embarked']\nprint(\"Columns of Categorical Variables (manual extraction):\")\nprint(cat_cols)","78d3d7dd":"X_cat = X[cat_cols]\nX_cat","3a7d80e7":"print(\"unique labels in respective columns:\")\nfor i in range(0, len(cat_cols)):\n    print(cat_cols[i], \":\", X_cat[cat_cols[i]].unique())","82cbdc7e":"# specify the dict for mapping\n# no need to map 'Pclass'\nsex_map = {\"male\": 0, \"female\": 1}\nembark_map = {\"S\": 1, \"C\": 2, \"Q\": 3}\n\nX_map = X.copy()\n\nX_map['Sex'] = X_cat['Sex'].map(sex_map).astype(int)\nX_map['Embarked'] = X_cat['Embarked'].map(embark_map).astype(int)\n\nX_map","9fc38729":"print(\"unique labels in respective columns:\")\nfor i in range(0, len(cat_cols)):\n    print(cat_cols[i], \":\", X_map[cat_cols[i]].unique())","540a9f45":"from sklearn.preprocessing import OrdinalEncoder\n\nenc = OrdinalEncoder()\n\nX_cat_enc = X.copy()\n\nfor i in range(0, len(cat_cols)):\n    enc.fit(X[cat_cols])\n    X_cat_enc[cat_cols] = enc.transform(X[cat_cols])\n\nX_cat_enc","30aac9b8":"print(\"unique labels in respective columns:\")\nfor i in range(0, len(cat_cols)):\n    print(cat_cols[i], \":\", X_cat_enc[cat_cols[i]].unique())","114731cb":"from sklearn.preprocessing import OneHotEncoder\n\noh_enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\nX_cat_ohenc = X.copy()\n\noh_enc.fit(X[cat_cols])\nX_cat_ohenc = pd.DataFrame(oh_enc.transform(X[cat_cols]))\nX_cat_ohenc.index = X.index\nprint(\"new columnn names of the one-hot encoded variables:\\n\", oh_enc.get_feature_names())\n# DEPRECATED: get_feature_names is deprecated in 1.0 and will be removed in 1.2.\n# Please use get_feature_names_out instead.\n\n# One-hot encoding removed index; put it back\nX_cat_ohenc.columns = oh_enc.get_feature_names()\n\nX_cat_ohenc","071995e4":"# drop the categorical variables from the original dataframe\nX_num = X.drop(cat_cols, axis=1)\n\n# put them back together\nOH_X = pd.concat([X_num, X_cat_ohenc], axis=1)\n\nOH_X","f380c498":"pd.get_dummies(X[cat_cols])","fe97b25c":"X_num = X.drop(cat_cols, axis=1)\nGD_X = pd.concat([X_num, pd.get_dummies(X[cat_cols])], axis=1)\n\nGD_X","2725886e":"from sklearn.datasets import load_iris\nfrom sklearn import preprocessing\n\niris = load_iris() ##code as per the example \ndata = np.array(iris['data'])\ny = np.array(iris['target'])\ntarget_names = iris['target_names']\n\nprint(\"target names:\",target_names)\nprint(\"target variables:\", list(np.unique(y)))","3a7925fb":"le = preprocessing.LabelEncoder()\nle.fit(target_names)\nprint(\"transformed target variables:\", list(np.unique(le.inverse_transform(list(y)))))","0f9c05fe":"The above result shows that maps are not required to be prepared before encoding, and the same encoded result can be obtained by using OrdinalEncoder (sklearn).","975bfa13":"#*Approach 4: get_dummies (pandas)*\n\nConvert categorical variable into dummy\/indicator variables\n\n[https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.get_dummies.html](http:\/\/)\n\nget_dummies() is basically a simple way for one-hot encoding. Similar to sklearn's one-hot encoder, please remember to put the encoded dataframe back together with the numerical variables.","bb675a20":"#**Extract Columns of Categorical Variables**","a2e62283":"#*Approach 1: Mapping (pandas)*\n\n* Map values of Series according to an input mapping or function\n* Used for substituting each value in a Series with another value, that may be derived from a function, a dict or a Series\n\n[https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.Series.map.html](http:\/\/)\n\nLet's look at the unique labels of these variables first","721c07ca":"#*Approach 5: Label Encoder (sklearn)*\n\n* Encode **target labels** with value between 0 and n_classes-1\n* This transformer should be used to **encode target values**, i.e. y, and **not the input X**\n\n[https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html](http:\/\/)","b5b0a2e2":"*Columns generated by the one-hot encoder:*\n\nfrom Pclass: ['x0_1' 'x0_2' 'x0_3']\n\nfrom Sex: ['x1_female' 'x1_male']\n\nfrom Embarked: ['x2_C' 'x2_Q' 'x2_S']","646c966b":"#*Appraoch 2: Ordinal Encoder (sklearn)*\n\n* Encode categorical features as an integer array\n* The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are converted to ordinal integers. This results in a single column of integers (0 to n_categories - 1) per feature\n\n[https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder](http:\/\/)","4d5a16ff":"Some of the categorical varaibles in the dataset are stored as numerical values. Therefore, columns containing categorical variables cannot be extracted simply by extracting columns with \"object\" data type. Plesae see the description below.\n\n**Description of Variables**\n\npclass: A proxy for socio-economic status (SES)\n1st = Upper, 2nd = Middle, 3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\n\nSibling = brother, sister, stepbrother, stepsister\n\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nparch: The dataset defines family relations in this way...\n\nParent = mother, father\n\nChild = daughter, son, stepdaughter, stepson\n\nSome children travelled only with a nanny, therefore parch=0 for them.","12a07823":"#*Approach 3: One-hot Encoder (sklearn)*\n\n* Encode categorical features as a one-hot numeric array\n* The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka \u2018one-of-K\u2019 or \u2018dummy\u2019) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array (depending on the sparse parameter)\n* By default, the encoder derives the categories based on the unique values in each feature. Alternatively, you can also specify the categories manually\n* This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels\n* Note: a one-hot encoding of y labels should use a LabelBinarizer instead\n\n[https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html?highlight=one%20hot%20encoding](http:\/\/)","5810b065":"#**Comparison between different encoding methods**\n\n* map (pandas)\n* OrdinalEncoder (sklearn)\n* One-hot Encoder (sklearn)\n* get_dummies (pandas)\n* LabelEncoder (sklearn)\n\nThe \"titanic disaster\" dataset and iris data set are used for demonstration.","f8b81905":"**Load the titanic dataset**","f1ca6d12":"The target names are ['setosa' 'versicolor' 'virginica'], and the target variables are already stored as integers. Let's use LabelEncoder to inversly transform the [0, 1, 2] into ['setosa' 'versicolor' 'virginica'].","6b30e7c9":"In comparison with the dataframe *OH_X*, get_dummies() does not see 'Pclass' as categorical, therefore no dummy indicators were generated for 'Pclass'. Whether this difference will affect the accuracy requires further investigation. This topic will not be covered in this notebook."}}