{"cell_type":{"9db78fbc":"code","406041be":"code","6ad849ad":"code","4592bedd":"code","bfc89cb7":"code","a0828353":"code","7910d654":"code","4052aedd":"code","b3d49406":"code","8f5d4a1b":"code","24f78c6b":"code","ccdf0d3f":"code","d1832b60":"code","e0311f98":"code","432f69c1":"code","260359bd":"code","4cfd3f69":"code","b5a20370":"code","266ae4f3":"code","cdadaa26":"code","c00e894a":"code","eaa6f3a1":"code","85b7e8a4":"code","0d1f6af4":"code","9a2df66a":"code","0e809463":"code","660ff967":"code","2fa0889f":"code","fa531249":"code","f8192023":"code","92b88b92":"code","87b369b5":"code","9b6419c6":"code","cabac075":"code","01081696":"code","115e97ee":"code","bf03aa49":"code","094302e3":"code","71a87dca":"code","38dbc841":"code","c7bfd401":"code","22c2e6b3":"code","c28c494f":"code","ced6751d":"code","e6d52546":"code","5c0fcdb7":"code","3dbf44cc":"code","d28b2f25":"code","5a4027da":"code","3b56ab01":"code","05face5c":"code","93017354":"code","15352237":"code","8f79bdaa":"code","b75c7f4a":"code","add04306":"code","bc82c001":"code","a4f0b868":"code","419ae76b":"code","bb125e1e":"code","1c0285de":"code","b1bec3ea":"code","3cefdb0c":"code","3814c50e":"code","0d7e9eda":"code","cc9f7422":"code","13dba76f":"code","00edb975":"code","bd540519":"code","8aa86905":"code","6775dc56":"code","4d86b1aa":"code","05bae294":"code","5ceb8e20":"code","6b739d34":"code","8862a0a7":"code","d7a18076":"code","d9d360b0":"code","152e2fa7":"markdown","94531ed3":"markdown","4f010372":"markdown","e759e2a7":"markdown","e482d630":"markdown","a3e0ea1d":"markdown","5a7cd090":"markdown","57a270d8":"markdown","0e5921ff":"markdown","991ad11b":"markdown","67ff0a72":"markdown","0cfdac29":"markdown","ca63bd12":"markdown","efbdc08c":"markdown","73f44505":"markdown","8b0972d6":"markdown","c831f495":"markdown","7ac3d059":"markdown","5bf70f8e":"markdown","117ebd9a":"markdown","faeb0505":"markdown","958c3f91":"markdown","167b4817":"markdown","9a0c4e24":"markdown","9c1ba931":"markdown","e2be2e3a":"markdown","5885dd53":"markdown","5da85fa0":"markdown","6bdb00e4":"markdown","9ec3ce03":"markdown","9530b8da":"markdown","a7b45b9c":"markdown"},"source":{"9db78fbc":"#Library\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport datetime\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nimport plotly.offline as py\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, iplot\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\npy.init_notebook_mode(connected = True)","406041be":"df = pd.read_csv('..\/input\/melbourne-housing-snapshot\/melb_data.csv')\ndf.drop(columns = ['YearBuilt', 'BuildingArea'], inplace = True) #Dropped this column since this data point was poorly sourced\n\n#Categorical Data\ndf['Suburb'] = df['Suburb'].astype('category')\ndf['Postcode'] = df['Postcode'].astype('category')\ndf['Regionname'] = df['Regionname'].astype('category')\ndf['CouncilArea'] = df['CouncilArea'].astype('category')\ndf['SellerG'] = df['SellerG'].astype('category')\ndf['Type'] = df['Type'].astype('category')\ndf['Method'] = df['Method'].astype('category')\n\n\n#Integer Data\ndf['Rooms'] = df['Rooms'].astype('int64')\ndf['Bathroom'] = df['Bathroom'].astype('int64')\ndf['Car'] = df['Car'].fillna(0) \ndf['Car'] = df['Car'].astype('int64')\ndf['Price'] = df['Price'].fillna(0)\ndf['Price'] = df['Price'].astype('float64')\n\n#Date Data\ndf['Date'] = pd.to_datetime(df['Date'])","6ad849ad":"import plotly.express as px\nfig = px.density_mapbox(df, lat='Lattitude', lon='Longtitude', z='Price', radius=10,\n                        center=dict(lat=-37.8, lon=145), zoom=10,\n                        mapbox_style=\"stamen-terrain\", opacity = 0.5, title = 'Melbourne Price Heatmap')\nfig.show()","4592bedd":"df.head()","bfc89cb7":"df.info()","a0828353":"df.groupby(['Date'])['Price'].count().plot(kind = 'bar', figsize = (30,10))","7910d654":"#Setting the index column to be date for future analysis\ndf_date_idx = df.set_index('Date')","4052aedd":"df_date_idx.head()","b3d49406":"dsc = df_date_idx['Price'].describe()\nprint('Basic level stats about the price data\\n', dsc)","8f5d4a1b":"dsc = df_date_idx['Price'].describe()\nmin_dsc = dsc[3]\nmed_dsc = dsc[5]\nmax_dsc = int(med_dsc+(1.5*(dsc[6]-dsc[4])))\nbox_plot = sns.boxplot(y = df_date_idx.Price, data = df_date_idx)\n\nx_tick = box_plot.get_xticks()\nbox_plot.text(x_tick, med_dsc, med_dsc,horizontalalignment='center',size='x-small',color='w',weight='semibold')\n#box_plot.text(x_tick, min_dsc, min_dsc,horizontalalignment='center',size='x-small',color='b',weight='semibold')\n#box_plot.text(x_tick, max_dsc, max_dsc,horizontalalignment='center',size='x-small',color='b',weight='semibold')\n#box_plot.text(x_tick, med_dsc[x_tick], horizontalalignment='center',size='x-small',color='w',weight='semibold')\n\nsns.set(rc={'figure.figsize':(10,10)}, font_scale = 1)","24f78c6b":"df[df['Price'].isin([85000,903000,1.330000e+06])].sort_values(by = ['Price'])","ccdf0d3f":"#Cumelative sum of prices of in Melboune over time with new df.\ndf['Price'].plot(kind = 'line', title = 'House prices in Melbourne', figsize= (30,10))","d1832b60":"#Diff: diff_1 = price_2-price_1\ndf_date_idx['diff'] = df_date_idx['Price'].diff()","e0311f98":"df_date_idx['diff'].iloc[0] = 0","432f69c1":"# Draw Plot\nplot_acf(df_date_idx['diff'].tolist(), lags=10)","260359bd":"#Numerical Dataset\ndf_numerical = pd.concat([df['Price'], df['Distance'], df['Rooms'], df['Bathroom'], df['Car'], df['Landsize']], axis = 1)","4cfd3f69":"sns.heatmap(df_numerical.corr(), annot = True)","b5a20370":"# sns.pairplot(df_numerical)","266ae4f3":"#Removing the outlier from Landsize var, I am going to remove the row exhibiting the Landsize outlier completely\ndf_numerical_2 = df_numerical.drop([df_numerical['Landsize'].argmax()])","cdadaa26":"df_numerical.iloc[df_numerical['Landsize'].argmax(), :]","c00e894a":"sns.boxplot(y = df_numerical_2.Landsize, data = df_numerical_2)","eaa6f3a1":"# Creating df for each real estate agency \ndef df_creator(df):\n    list_real_estate_agencies = list(set(df['SellerG'].to_list()))\n    list_df = []\n    for i in list_real_estate_agencies:\n        list_df.append(df[df['SellerG'] == i])\n        \n    return list_df\n\nlist_seller_df = df_creator(df)","85b7e8a4":"list_seller_df[3]","0d1f6af4":"lnd_size_lst = []\ncols = []\nfor i in list_seller_df:\n    i['Landsize_mean_{d}'.format(d = i['SellerG'].iloc[0])] = i['Landsize'].mean()\n    cols.append('Landsize_mean_{d}'.format(d = i['SellerG'].iloc[0]))\n    lnd_size_lst.append(i['Landsize_mean_{d}'.format(d = i['SellerG'].iloc[0])].iloc[0])\n\nlnd_size_df = pd.DataFrame(lnd_size_lst, columns = ['Mean Landsize'], index = cols)","9a2df66a":"lnd_size_df[0:20].sort_values(by = 'Mean Landsize', ascending = True).plot(kind = 'barh', figsize = (10,30), title = 'Top 20 realestate agencies reporting abnormal mean landsize')","0e809463":"df.iloc[df['Landsize'].argmax(),:]","660ff967":"df.drop(['Landsize'], axis = 1,inplace = True)","2fa0889f":"df_numerical.drop(['Landsize'], axis = 1,inplace = True)","fa531249":"# sns.pairplot(df_numerical)","f8192023":"sns.heatmap(df_numerical.corr(), annot = True)","92b88b92":"ax = (df.groupby(['SellerG'])['Price'].count()\/sum(df.groupby(['SellerG'])['Price'].count())*100).sort_values(ascending = False).head(20).plot(kind = 'bar', figsize = (30,5), title = 'Top 20 realestate agencies contributing to the data set')\nfor p in ax.patches:\n    b = p.get_bbox()\n    val = \"{:.2f}%\".format(b.y1 + b.y0)\n    ax.annotate(val, (p.get_x() * 1.005, p.get_height() * 1.005))","87b369b5":"# Creating df for each real estate agency \ndef df_suburb_creator(df):\n    list_suburb = list(set(df['Suburb'].to_list()))\n    list_df = []\n    for i in list_suburb:\n        list_df.append(df[df['Suburb'] == i])\n        \n    return list_df\n\nlist_suburb_df = df_suburb_creator(df)","9b6419c6":"list_max_price_add = []\nfor i in list_suburb_df:\n    list_max_price_add.append(i.iloc[i['Price'].argmax(),:].to_frame())\n    \ndf_max_price_addr = pd.concat(list_max_price_add, axis = 1)\ndf_max_price_addr = df_max_price_addr.T\ndf_max_price_addr.reset_index(inplace = True)\ndf_max_price_addr.sort_values(by = ['SellerG', 'Suburb'], inplace = True)","cabac075":"fig = px.scatter_mapbox(df_max_price_addr, lat = \"Lattitude\", lon = \"Longtitude\", size = df_max_price_addr.Price.tolist(), color = 'SellerG', opacity = 0.6, color_discrete_sequence=px.colors.qualitative.Dark24)\nfig.update_layout(mapbox_style=\"open-street-map\")\nfig.update_layout(title = 'Highest selling properties in Melbourne by Suburb <br>(Select the drop down list to check each realestate agency and price, to undo go to bottom of list and select NONE)')\n\nupdate_menus = []\nidentity_mtx = np.identity(len(list(set(df_max_price_addr['SellerG'].tolist()))), dtype = 'bool')\nsorted_list = sorted(list(set(df_max_price_addr['SellerG'].tolist())))\nfor idx, x in enumerate(sorted_list):\n    update_menus.append(dict(args = [{'visible':identity_mtx[idx].tolist()}], label = '{i}'.format(i = x), method = 'update'))\n\nupdate_menus.append(dict(args = [{'visible':[True for j in range(0,len(sorted_list))]}], label = 'NONE', method = 'update'))\n    \nfig.update_layout(\n    \n    updatemenus = [\n        dict(\n            buttons = update_menus,\n            direction = \"down\",\n            showactive = True,\n            xanchor = \"right\",\n            pad={\"r\": 10, \"t\": 10},\n            yanchor = \"top\"\n        )\n    ]\n)\nfig.show()","01081696":"ax = df_max_price_addr.set_index('Suburb')['Price'].sort_values( ascending = False)[0:20].plot(kind = 'bar', figsize = (30,10), title = 'Top 20 highest selling properties in Melbourne by Suburb')\nfor p in ax.patches:\n    b = p.get_bbox()\n    val = \"${:.0f}\".format(b.y1 + b.y0)\n    ax.annotate(val, (p.get_x() * 1.005, p.get_height() * 1.005))","115e97ee":"fig = px.scatter_mapbox(df_max_price_addr.set_index('Suburb').sort_values(by = 'Price', ascending = False)[0:20], lat = \"Lattitude\", lon = \"Longtitude\", size = df_max_price_addr.set_index('Suburb')['Price'].sort_values(ascending = False)[0:20].tolist(), color = 'SellerG', opacity = 0.6, color_discrete_sequence=px.colors.qualitative.Dark24)\nfig.update_layout(mapbox_style=\"open-street-map\")\nfig.update_layout(title = 'Top 20 Highest selling properties in Melbourne by realestate agency <br>(Hover to check the realestate agency and price)')\nfig.show()","bf03aa49":"import tensorflow as tf\nimport gc\nfrom tensorflow import feature_column\nfrom tensorflow.keras import layers\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\n\ndel df\ngc.collect()","094302e3":"#Starting from scratch again\ndf = pd.read_csv('..\/input\/melbourne-housing-snapshot\/melb_data.csv')\ndf.info()","71a87dca":"df.groupby(['Suburb']).Price.mean()","38dbc841":"df.drop(['Landsize','Address', 'Postcode', 'Bedroom2', 'CouncilArea', 'Lattitude', 'Longtitude', 'Regionname', 'Propertycount'], axis = 1, inplace = True)","c7bfd401":"df.set_index('Date', inplace = True)","22c2e6b3":"def df_to_dataset(df, shuffle = True, batch_size = 32):\n    df = df.copy()\n    labels = df.pop('Suburb')\n    ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n    if shuffle:\n        ds = ds.shuffle(buffer_size = len(df))\n    ds = ds.batch(batch_size)\n    return ds","c28c494f":"batch_size = 50\nds = df_to_dataset(df, batch_size = batch_size)","ced6751d":"#In a batch of 10 what does the data look like?\nfor feature_batch, label_batch in ds.take(1):\n    print('Every feature in the data set: \\n', list(feature_batch.keys()))\n    for feature in list(feature_batch.keys()):\n        print('A batch of {i}'.format(i = feature),':\\n',feature_batch['{j}'.format(j = feature)])","e6d52546":"class Normalisation():\n    def __init__(self, name, ds):\n        self.name = name\n        self.ds = ds\n    \n    def num_normalization(self):\n        _norm = preprocessing.Normalization()\n        feature_ds = self.ds.map(lambda x, y:x[self.name])\n        _norm.adapt(feature_ds)\n        return _norm\n    \n    def categ_normalization(self, dtype, max_tokens = None):\n        if dtype == 'string':\n            index = preprocessing.StringLookup(max_tokens = max_tokens)\n        else:\n            index = preprocessing.IntegerLookup(max_tokens = max_tokens)\n        \n        feature_ds = self.ds.map(lambda x, y: x[self.name])\n        index.adapt(feature_ds)\n        encoder = preprocessing.CategoryEncoding(max_tokens = index.vocab_size())\n        return lambda feature: encoder(index(feature))","5c0fcdb7":"test_num_layer = Normalisation(name = 'Rooms', ds = ds).num_normalization()\ntest_categ_layer = Normalisation(name = 'Type', ds = ds).categ_normalization(dtype = 'string')","3dbf44cc":"#Numerical data being processed at the layer level\ntest_num_layer(df['Rooms']).shape","d28b2f25":"#One hot encoding for categorical data at layer level (important to note categ inputs need to have an object Dtype in pandas)\ntest_categ_layer(df['Type']).shape","5a4027da":"from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.losses import BinaryCrossentropy, CategoricalCrossentropy\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization\nimport tensorflow as tf\nimport kerastuner as kt\n\nphysical_devices = tf.config.list_physical_devices('GPU')\ntry:\n          tf.config.experimental.set_memory_growth(physical_devices[0], True)\nexcept:\n          # Invalid device or cannot modify virtual devices once initialized.\n    pass","3b56ab01":"import numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\nfrom sklearn.utils.validation import _deprecate_positional_args\n\n# modified code for group gaps; source\n# https:\/\/github.com\/getgaurav2\/scikit-learn\/blob\/d4a3af5cc9da3a76f0266932644b884c99724c57\/sklearn\/model_selection\/_split.py#L2243\nclass PurgedGroupTimeSeriesSplit(_BaseKFold):\n    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n    Allows for a gap in groups to avoid potentially leaking info from\n    train into test if the model has windowed or lag features.\n    Provides train\/test indices to split time series data samples\n    that are observed at fixed time intervals according to a\n    third-party provided group.\n    In each split, test indices must be higher than before, and thus shuffling\n    in cross validator is inappropriate.\n    This cross-validation object is a variation of :class:`KFold`.\n    In the kth split, it returns first k folds as train set and the\n    (k+1)th fold as test set.\n    The same group will not appear in two different folds (the number of\n    distinct groups has to be at least equal to the number of folds).\n    Note that unlike standard cross-validation methods, successive\n    training sets are supersets of those that come before them.\n    Read more in the :ref:`User Guide <cross_validation>`.\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of splits. Must be at least 2.\n    max_train_group_size : int, default=Inf\n        Maximum group size for a single training set.\n    group_gap : int, default=None\n        Gap between train and test\n    max_test_group_size : int, default=Inf\n        We discard this number of groups from the end of each train split\n    \"\"\"\n\n    @_deprecate_positional_args\n    def __init__(self,\n                 n_splits=5,\n                 *,\n                 max_train_group_size=np.inf,\n                 max_test_group_size=np.inf,\n                 group_gap=None,\n                 verbose=False\n                 ):\n        super().__init__(n_splits, shuffle=False, random_state=None)\n        self.max_train_group_size = max_train_group_size\n        self.group_gap = group_gap\n        self.max_test_group_size = max_test_group_size\n        self.verbose = verbose\n\n    def split(self, X, y=None, groups=None):\n        \"\"\"Generate indices to split data into training and test set.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data, where n_samples is the number of samples\n            and n_features is the number of features.\n        y : array-like of shape (n_samples,)\n            Always ignored, exists for compatibility.\n        groups : array-like of shape (n_samples,)\n            Group labels for the samples used while splitting the dataset into\n            train\/test set.\n        Yields\n        ------\n        train : ndarray\n            The training set indices for that split.\n        test : ndarray\n            The testing set indices for that split.\n        \"\"\"\n        if groups is None:\n            raise ValueError(\n                \"The 'groups' parameter should not be None\")\n        X, y, groups = indexable(X, y, groups)\n        n_samples = _num_samples(X)\n        n_splits = self.n_splits\n        group_gap = self.group_gap\n        max_test_group_size = self.max_test_group_size\n        max_train_group_size = self.max_train_group_size\n        n_folds = n_splits + 1\n        group_dict = {}\n        u, ind = np.unique(groups, return_index=True)\n        unique_groups = u[np.argsort(ind)]\n        n_samples = _num_samples(X)\n        n_groups = _num_samples(unique_groups)\n        for idx in np.arange(n_samples):\n            if (groups[idx] in group_dict):\n                group_dict[groups[idx]].append(idx)\n            else:\n                group_dict[groups[idx]] = [idx]\n        if n_folds > n_groups:\n            raise ValueError(\n                (\"Cannot have number of folds={0} greater than\"\n                 \" the number of groups={1}\").format(n_folds,\n                                                     n_groups))\n\n        group_test_size = min(n_groups \/\/ n_folds, max_test_group_size)\n        group_test_starts = range(n_groups - n_splits * group_test_size,\n                                  n_groups, group_test_size)\n        for group_test_start in group_test_starts:\n            train_array = []\n            test_array = []\n\n            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n                train_array_tmp = group_dict[train_group_idx]\n                \n                train_array = np.sort(np.unique(\n                                      np.concatenate((train_array,\n                                                      train_array_tmp)),\n                                      axis=None), axis=None)\n\n            train_end = train_array.size\n \n            for test_group_idx in unique_groups[group_test_start:\n                                                group_test_start +\n                                                group_test_size]:\n                test_array_tmp = group_dict[test_group_idx]\n                test_array = np.sort(np.unique(\n                                              np.concatenate((test_array,\n                                                              test_array_tmp)),\n                                     axis=None), axis=None)\n\n            test_array  = test_array[group_gap:]\n            \n            \n            if self.verbose > 0:\n                    pass\n                    \n            yield [int(i) for i in train_array], [int(i) for i in test_array]\n","05face5c":"class CVTuner(kt.engine.tuner.Tuner):\n    def run_trial(self, trial, X, y, splits, batch_size=32, epochs=1,callbacks=None):\n        val_losses = []\n        for train_indices, test_indices in splits:\n            X_train, X_test = [x[train_indices] for x in X], [x[test_indices] for x in X]\n            y_train, y_test = [a[train_indices] for a in y], [a[test_indices] for a in y]\n            if len(X_train) < 2:\n                X_train = X_train[0]\n                X_test = X_test[0]\n            if len(y_train) < 2:\n                y_train = y_train[0]\n                y_test = y_test[0]\n            \n            model = self.hypermodel.build(trial.hyperparameters)\n            hist = model.fit(X_train,y_train,\n                      validation_data=(X_test,y_test),\n                      epochs=epochs,\n                        batch_size=batch_size,\n                      callbacks=callbacks)\n            \n            val_losses.append([hist.history[k][-1] for k in hist.history])\n        val_losses = np.asarray(val_losses)\n        self.oracle.update_trial(trial.trial_id, {k:np.mean(val_losses[:,i]) for i,k in enumerate(hist.history.keys())})\n        self.save_model(trial.trial_id, model)","93017354":"df.dropna(axis = 1, inplace = True)\ndf.head()","15352237":"df[df['Suburb'] == 'South Melbourne']","8f79bdaa":"def input_tf_creator(df, ds):\n    \"\"\"Input tensor function creator for the house price data set which \n    args: \n    \n    input: df = dataframe which has been reduced\n           ds = tensorflow dataset\n           \n    Returns:\n    \n    output: return input tensor which has been concatenated\n    \n    \"\"\"\n    numerical_features = ['Rooms', 'Bathroom', 'Price']\n    categorical_features = ['Type', 'SellerG']\n    layer_out_num = []\n    layer_out_categ = []\n    \n    #Creating a list of Normalisation data\n    for num_feature in numerical_features:\n        num_obj = Normalisation(name = num_feature, ds = ds).num_normalization()\n        layer_out_num.append(num_obj(df[num_feature]))\n    for categ_features in categorical_features:\n        categ_obj = Normalisation(name = categ_features, ds = ds).categ_normalization(dtype = 'string')\n        layer_out_categ.append(categ_obj(df[categ_features]))\n    \n    #Creating a concatenated input using the Normalisation layers\n    num_conct_tf = tf.concat(values = layer_out_num, axis = -1)\n    categ_conct_tf = tf.concat(values = layer_out_categ, axis = -1)\n    input_tf = tf.concat(values = [num_conct_tf, categ_conct_tf], axis = -1)\n    \n    return input_tf","b75c7f4a":"input_data = input_tf_creator(df, ds)\nprint(input_data.shape)","add04306":"y_input = pd.get_dummies(df.Suburb).values","bc82c001":"y_input.shape","a4f0b868":"def autoencoder(input_data, output_data):\n    i = tf.keras.layers.Input(shape = input_data.shape[-1])\n    x = tf.keras.layers.GaussianNoise(0.3)(i)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    #encoder\n    enc = tf.keras.layers.Dense(139, activation = 'relu', name = 'encoder_layer')(x)\n    \n    #decoder\n    dec = tf.keras.layers.BatchNormalization()(enc)\n    dec = tf.keras.layers.Dropout(0.2)(dec)\n    dec = tf.keras.layers.Dense(input_data.shape[-1], name = 'decoder_layer')(dec)\n    \n    #Output data decoder\n    out = tf.keras.layers.Dense(139, activation = 'relu')(dec)\n    out = tf.keras.layers.BatchNormalization()(out)\n    out = tf.keras.layers.Dense(output_data.shape[-1], activation = 'sigmoid', name = 'Suburb_layer')(out)\n    \n    #model\n    encoder = tf.keras.Model(inputs = i, outputs = enc)\n    decoder = tf.keras.Model(inputs = i, outputs = [dec, out])\n    \n    decoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss = {'decoder_layer':'mse', 'Suburb_layer':'categorical_crossentropy'})\n    decoder.summary()\n    \n    return decoder, encoder","419ae76b":"decoder, encoder = autoencoder(input_data, y_input)","bb125e1e":"TRAINING_ACTION = False\n\nif TRAINING_ACTION:\n    decoder.fit(input_data, (input_data, y_input), epochs = 250, batch_size = 60, validation_split = 0.10, callbacks = [EarlyStopping('val_loss', patience = 10, restore_best_weights = True)])\n    decoder.save_weights('.\/auto_encoder_weights.hdf5')\nelse:\n    decoder.load_weights('..\/input\/params\/auto_encoder_weights (1).hdf5')\n    \ndecoder.trainable = False","1c0285de":"def MLP_model (params, input_data, output_data, encoder):\n    inputs = tf.keras.layers.Input(input_data.shape[-1])\n    x = encoder(inputs)\n    x = tf.keras.layers.Concatenate()([x, inputs])\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(params.Float('init_dropout', 0.0, 0.5))(x)\n    \n    for i in range(params.Int('num_layers', 5, 8)):\n        x = tf.keras.layers.Dense(params.Int('num_layers_{i}', 160, 200))(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dense(params.Int('num_layers_{j}', 80, 100))(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Lambda(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Dropout(params.Float(f'dropout{i}', 0.0, 0.5))(x)\n    x = tf.keras.layers.Dense(output_data.shape[-1], activation = 'sigmoid')(x)\n    \n    model = tf.keras.Model(inputs, outputs = x)\n    model.compile(optimizer = Adam(params.Float('lr', 0.0001, .1, default = 0.01)), loss = CategoricalCrossentropy(label_smoothing = params.Float('label_smoothing', 0.0,1)),  metrics = [tf.keras.metrics.AUC(name = 'auc'), tf.metrics.Precision(name = 'precision')])\n    \n    return model\n    ","b1bec3ea":"model_fn = lambda params: MLP_model(params, input_data, y_input, encoder)\n\ntuner = CVTuner(hypermodel = model_fn, oracle = kt.oracles.BayesianOptimization(objective = kt.Objective('val_auc', direction = 'max'), num_initial_points=4, max_trials = 15))\n\nFOLDS = 5\nSEED = 21\ntf.random.set_seed(SEED)","3cefdb0c":"input_data = input_data.numpy()","3814c50e":"input_data.shape","0d7e9eda":"if TRAINING_ACTION:\n    gkf = PurgedGroupTimeSeriesSplit(n_splits = FOLDS, group_gap=10)\n    splits = list(gkf.split(y_input, groups = df.index.values))\n    tuner.search((input_data, ), (y_input, ), splits = splits, batch_size = 50, epochs = 250, callbacks = [EarlyStopping('val_auc', mode = 'max', patience = 10)])\n    params = tuner.get_best_hyperparameters(1)[0]\n    pd.to_pickle(params, f'.\/best_params_{SEED}.pkl')\n    \n    for fold, (train_idx, test_idx) in enumerate(splits):\n        model = model_fn(params)\n        X_train, X_test = input_data[train_idx], input_data[test_idx]\n        y_train, y_test = y_input[train_idx], y_input[test_idx]\n        model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 250, batch_size = 50, callbacks = [EarlyStopping('val_auc', mode = 'max', patience = 15, restore_best_weights=True)])\n        model.save_weights(f'.\/model_{SEED}_{fold}.hdf5')\n        model.compile(Adam(params.get('lr')\/100), loss = 'categorical_crossentropy')\n        model.fit(X_test, y_test, epochs = 25, batch_size = 50)\n        model.save_weights(f'.\/model_{SEED}_{fold}_finetune.hdf5')\n    tuner.results_summary()","cc9f7422":"models = []\nparams = pd.read_pickle(f'..\/input\/params\/best_params_{SEED}.pkl')\nfor f in range(FOLDS):\n    mdl = model_fn(params)\n    mdl.load_weights(f'..\/input\/params\/model_{SEED}_{f}_finetune.hdf5')\n    models.append(mdl)","13dba76f":"test = pd.read_csv('..\/input\/test-data\/test.csv')\n#test['Bathroom'] = test['Bathroom'].astype('float32')","00edb975":"test.columns","bd540519":"test","8aa86905":"test_input = input_tf_creator(test, ds)","6775dc56":"test_input = test_input.numpy()","4d86b1aa":"pred = [model.predict(test_input) for model in models]","05bae294":"pred_lst = []\nfor i in pred:\n    pred_lst.append(sum(i)\/9)\n\n\nprediction = (pred_lst[0]+pred_lst[1]+pred_lst[2]+pred_lst[3]+pred_lst[4])\/5\n","5ceb8e20":"output_df = pd.DataFrame(list(zip(pd.get_dummies(df.Suburb).columns, prediction.tolist())), columns = ['Suburb', 'Prediction'])","6b739d34":"df_rpt = pd.read_csv('..\/input\/melbourne-housing-snapshot\/melb_data.csv')","8862a0a7":"df_lat_long = []\nsuburb_list = output_df['Suburb'].tolist()\nfor idx, row in df_rpt.iterrows():\n    for idx_2, i in enumerate(suburb_list):\n        if row['Suburb'] == i:\n            df_lat_long.append((i,row['Lattitude'], row['Longtitude']))\n\ndf_lat_long_price = []            \nfor j_idx, out_row in output_df.iterrows():\n    for i in range(0, len(df_lat_long)):\n        if out_row['Suburb'] == df_lat_long[i][0]:\n            df_lat_long_price.append(out_row['Prediction'])\n            ","d7a18076":"df_3 = pd.DataFrame(df_lat_long, columns = ['Suburb', 'Lat', 'Long'])\ndf_3['Prediction'] = df_lat_long_price","d9d360b0":"fig = px.density_mapbox(df_3[df_3['Prediction'] > 0.75], lat='Lat', lon='Long', z='Prediction', radius=10,\n                        center=dict(lat=-37.8, lon=145), zoom=10,\n                        mapbox_style=\"open-street-map\", opacity = 0.5, title = 'Melbourne Suburb Prediction Map')\nfig.show()","152e2fa7":"# Data processing for deep learning","94531ed3":"<a id=\"subsection-three\"><\/a>\n## What % of data is made up of *x* realestate agency? i.e. has the data set been heavily biased to one realestate agency","4f010372":"***Dave comments:***\nThere we go only one property was sold in the dataset at the amazing price of \\\\$85,000 in Footscray which was 1 bedroom, 1 bathroom unit. I googled the property, I have an opinion but I am also aware that given the current housing market situation in Aus, this person made a very good purchase. **Comment below tell me what you guys think? is it worth investing in a property like this? especially in the city!!**","e759e2a7":"***Dave comments:***\nWow! My assumptions that Landsize and Distance from CBD will have an effect on Price were horribly wrong \ud83d\ude02\nMaybe something is wrong with data points in those columns? we will confirm this with the pairplot in a bit. My other assumption (2.) was loosely correct. \n\n***Formal observation:***\nThe correlation matrix is showing positive correlation to Rooms, Bathroom and Price i.e. as price increases, rooms and bathroom also increase. From the previous insight on data distribution it can be assumed that the correlation matrix may not be completely accurate since assumption 1. has been nulled. A nulled assumption 1 implies that land size has no effect on price which seems rather strange. The data distribution and land size variable needs to be explored further.","e482d630":"For now **( \uff9f\u0434\uff9f)\u3064 Bye**","a3e0ea1d":"# Melbourne House Prices EDA Book: A guide on analysing housing data in Melbourne, Victoria.\n![photo-1514395462725-fb4566210144.jpg](attachment:32295691-b5ce-47fb-a4ee-487953224542.jpg)","5a7cd090":"<a id=\"subsection-one\"><\/a>\n## House prices in Melbounre","57a270d8":"Looking good here, the data is in the format I want it to be. FYI, I have replaced N\/A with 0 just to make the analysis easier for to work with.","0e5921ff":"\n### ACF (Auto-Correlation Function) with Seasonality check","991ad11b":"***Dave comments:***\nThis box plot is clear proof that this data point is seriously messed up. We need to fix this.\n\n***Formal observation:***\nThe land size data is very poorly distributed, which makes it hard to incorporate any insightful information. In the next section the objective will be to fix this and observe any impact before and after.","67ff0a72":"***Dave comments:***\nThis is very cool! We can apply some Auto-Correlaiton tools onto this data to see if we can see any recurrent patterns, perhaps there is seasonality in the data! Before we apply an ACF to the data it might be worth applying date-time index to the data so we can now work with a time series data set(This has been done already). To apply ACF effectively we need to remove the trend in the dataset to make the data stationary. By having a stationary data set we can see any seasonality in the data set over time.","0cfdac29":"# Fixing landsize data distribution\nMy plan of action is based on the assumption that the higher the mean landsize for each realestate agency the further away the properties are from the city. This assumption follows very closely with assumption 4 (Certain realestate agencies will be \"expert sellers\" in certain locations within Melbourne), by having a plan of action rooted from a base assumption ensures that the overall analysis can be traced back to the 4 assumptions stated earlier. Furthermore, abnormal mean landsize's compared to the population in data implies that the realestate agency has either reported the number in a different unit or they are servicing properties outside of metropolitan Melbourne, the focus of this notebook will be on properies within the metropolitan Melbourne region.\n\nA correlation matrix\/box plot will be created again so that we can observe the impact before and after the variable has been processed. Before we create a correlation matrix the removal of highly skewing realestate agencies will be done otherwise we simply revert to the previous correlation matrix.","ca63bd12":"***Formal observation:***\nMarshall(4.85%) has been widely popular in selling expensive properties in South-East and Eastern suburbs in Melbourne. Nelson(11.52%) is leading Melbourne CBD, North and North-West Melbourne area. By scale of property prices, there is a larger number of expensive properties sold in the Eastern and South-Eastern suburbs compared to Western and North-Western suburbs, implying that the expected house prices in Eastern and South-Eastern subrubs are greater than Western and North-Western suburbs.","efbdc08c":"## Nomralising numerical and categorical values\nNormalising numerical values with a mean of 0 and std-dev 1 which is the z-nomral distribution. This can be done using tensorflow layer Normalization()","73f44505":"<a id=\"section-three\"><\/a>\n# Elementray Data Analysis\nAs part of the data analysis I will be looking into price distribution and basic stats","8b0972d6":"<a id=\"subsection-two\"><\/a>\n# Correlation of Price to other features\nThere are categorical features and numerical features in the dataset, how do we create a correlation matrix then? lets breakdown the problem like this:\n1. Numerical data correlation matrix\n2. Categorical clustering\n3. Observations\n\n**Some assumptions before progressing:**\n1. Landsize and Price should be highly correlated. I believe that in Melbourne as land size increases the house prices will increase maybe linearly. \n2. Rooms, Bathroom and Carpark will also be highly correlated. Same logic as point 1, the bigger the house the higher the price.\n3. In Australia the CBD(Central Business District) tends to be prime property. The reason for that is every aminety is readily avialable and being close to work is a huge advantage consdering all work offices tend to be within the CBD.\n4. Certain realestate agencies will be \"expert sellers\" in certain locations within Melbourne. The rationale behind this one is simple, a realestate agent might pick an area which makes them the specialist in selling houses in that given area. Working within the scope will yield better results than working all over Melbourne, hence by focusing on a small number of areas a realestate agency should be able to improve their reputation and brand in selling houses.","c831f495":"<a id=\"section-four\"><\/a>\n# Realestate agency behaviorual analytics\n\nFollowing on from the research into fixing the landsize data the next phase of research is into realestate agnecies and their behviour within the Melbourne House Price Dataset. Now this \"behaviour\" I refer to is more about which realestate agency has sold the highest number of properties in the data set and looking into which agency has sold some of the expensive properties in the dataset. The following areas will be discussed in this research:\n\n1. What % of data is made up of *x* realestate agency? i.e. has the data set been heavily biased to one realestate agency\n2. Within Melbourne which agency has been top performing based on suburbs.","7ac3d059":"***Dave comments:***\nOrighty, this pairplot has lot going on so lets take it step-by-step. Firstly, Landsize data has an outlier sticking out. We can get rid of the outlier which will give us some more meaningful insights (will do this in a bit).\n\n***Dave comments:***\nNow lets look at Distance. The Distance variable is exhibiting a sort of positively skewed bell curve characteristics when plotted against Price. It seems that as distance reduces prices increase not strictly linearly, this maybe why the correlation matrix was showing strange values. But from the pairplot we can observe the 4th assumption in full effect which is awesome! There are obvious relationships with distance such as distance increases (moving out of CBD) Rooms, Bathrooms and Car space will increase. I do not want to spend too much time on this at the moment.\n\n***Dave comments:***\nFinally, looking at Rooms, Bathrooms and Car we can observe a loosely positive increase in price as there is a positive 1-unit change in the three variables. Once again this does not indicate that there is a strict linear relationship, we have to always take this kind of analysis with a grain of salt.\n\n***Formal observation:***\nAs mentioned earlier the land size data is not behaving as expected, the cause was mentioned as data distribution. In the pair plot above there is clear evidence that the distribution of land size data is influenced heavily by a few data points, this can be fixed by constraining the land size variable by looking at the box plot. The distance vs price variable has displayed very interesting behavior towards each other. There is a positive skew on the data with the x-axis being the distance variable and y-axis being the price variable. It can be observed that as distance from the CBD(Central Business District) increases the price moves down very quickly. This can be loosely attributed to the behavior of the population wanting to move closer to the city due to better access to amenities, schools, and offices. Rooms, Bathrooms and Car variables are also increasing with distance indicating that as one moves out of the city their chances of getting a property with more rooms, bathrooms and car space increases as price reduces.","5bf70f8e":"# Within Melbourne which agency has been top performing based on suburbs.","117ebd9a":"***Dave comments:***\nFrom the looks of it there are a few outliers after the 75th quartile value of \\\\$1.33 mill. Meidan houe prices are \\\\$903,000 and minium is \\\\$85,000 for a property. \nI will be honest, \\\\$85,000 for a property is very low for Melbourne, let me try to find properties for each of the three catergories.","faeb0505":"***Dave comments:***\nCreated a df for each real estate agency which can allow for compare and contrast.","958c3f91":"***Dave comments:***\nThe top 20 highest selling properties are distributed over 10 realestate agencies. Out of the 20 properties, 8 were sold by Marshall the rest of ther 12 were collectively sold by the other 9 agencies. This confirms that Marshall realestate agency have been able to sell some of the highest priced properties in Melbourne that to with a high frequency! That is a very solid performance from their end.\n\n***Dave comments:***\nA note to anyone reading this from an investment prespective: It is tempting to assume that if you live in Melbourne you want Marshall realtestate agency to sell your property based on this data, however there are also many other factors like agency costs, location of the property etc, so please take this insight with a grain of salt.","167b4817":"## Numerical dataset","9a0c4e24":"***Formal observation:***\nThe graph above shows the top 20 realestate agencies with mean property landsize ranked from highest to lowest. If we do some side research into the top three realestate agencies (Max, Kay, Joe) we should be able to see which area they are servicing and the landsize recorded.","9c1ba931":"<a id=\"section-one\"><\/a>\n# Data Cleaning\n\nI have set the data objects for this dataset before I wanted to do anything with it.","e2be2e3a":"***Dave comments:***\nSo Nelson real estate agency is making up 11.52% of the entire data set after which Jellis(9.69%) and then Hocking Stewart(8.59%). I will be honest I have not heard of Nelson Alexander realestate agency, they seem quite small (I googled them) which makes it very impressive that they were able to make up 11.52% of the data set, consdering there are realesate giants like Ray White and Harcourts in the data as well. Perhaps the data set owner was able to find data more readiy available from Nelson? who knows!","5885dd53":"<a id=\"section-two\"><\/a>\n# Data Distribution\n\nIn this section I am going to explore what is the distribution of the data over the dates collected i.e. uniformly distributed on each day or un-evenly distributed on each day.\nBy visualising the distribution of the data set one can see the quality of the data collected which can help in producing predictive models later on.","5da85fa0":"## Solution for fixing landsize data distribution\nSince, the data recorded for the variable was found to have errors it seems appropriate to **Drop the variable** altogether for the following reasons:\n1. One of the properties with highest landsize in the dataset (389 Gore St, Fitzroy) was found to be incorrect\n2. Confidence in all data values being correct is low due to point 1\n3. Model development will be smoother without the inclusion unreliable data","6bdb00e4":"***Dave comments:***\nWow, this was unexpected, for some reason Kay realestate agency has recorded the landsize as 433014.0 now in Australia properties tend record their units as sqm, so 433014.0 sqm is huge! and that also in Fitzroy which is really close to the city? no way. So I googled the property and I was shocked once again, the landsize recorded here is incorrect, it is actually a tiny 280 sqm property. If you dont believe me here is the link https:\/\/www.realestate.com.au\/sold\/property-house-vic-fitzroy-125881282. The other values were fine (price, rooms, bathrooms, carspace etc)\n\n***Formal observation:***\nBy doing some side research into the 389 Gore St, Fitzroy property sold by Kay realestate shows that the land size recorded in the data set is incorrect compared to the land size published on realestate.com (https:\/\/www.realestate.com.au\/sold\/property-house-vic-fitzroy-125881282). This raise concerns on documentation of other data points including land size value. Going for deep learning prediction the land size variable will be dropped since there is enough compelling evidence to suggest that land size variable itself is unreliable and may contain other incorrect values. The most effective way to do any kind of deep learning or machine learning would be to simply drop the variable all together which will save the model from relying on incorrect data points.\n\n***Dave comments:***\nEffectively what I have said in my formal obvs is that the data point I found to be incorrect has dropped my confidence in the landsize variable altogther, so I am going to drop it, and save myself time and effort in researching into why my NN or ML model is giving me weird predictions. Also realying on the famous heuristic: garbage training data leads to garbage predictions when used on real data.","9ec3ce03":"# Autoencoder (Unsupervised Learning Approach)\n\nThe process of autoencoding achieves data compression where the compression and decompression functions are 1. data specific, 2. lossy and 3. learnded automatically from exammples rather than a human engineer. This kind of architecture is mostly facilitated by Neural Networks. \n\nAn autoencoder is constituted by the 1. encoding function and 2. the decoding function and finally 3. Loss function (tells difference btween decoded against input). The objective is to tune the encoding and decoding funcitons through back stochastic gradient descent (SGD) from the loss function. \n\n**So why autoencoding to identify house prices?**\nAutoencoder are really useful when it comes to denoising information. From my expierence in Jane Street comp it had a strong use case in identifying underlying patterns then reducing the dimensonality of the data(which is very useful in deep learning) which then becames a part of embedded layer for deep learning.","9530b8da":"***Dave comments:***\nThis is proof that the dataset is not uniform with irregular data points recorded for each date. This data set is actually quite poorly distributed, it will be challenging to work with this kind of data for ML. However, lets not loose hope so early on ;)","a7b45b9c":"***Formal observation:***\nWith a lag value of 10-day it can be noted that there is no positive correlaiton in past values and current values. As lag value increases the correlaiton to past is constant and within the rejection range. So far I believe there is no seasonality in the data set i.e. no frequency dependent relationship in the data. The house price data does not show any seasonality.\n\n***Dave comments:***\nMy conculusion so far is based on the information in the graph above, perhaps as we collect more data (over many years) this might change."}}