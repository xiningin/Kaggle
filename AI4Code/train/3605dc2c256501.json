{"cell_type":{"e6c53238":"code","750cb7d9":"code","3c9ec4fe":"code","69cbd8ac":"code","704de974":"code","6e07c3d5":"code","ae581dd7":"code","95e2af2d":"code","e8abb573":"code","d00e30d1":"code","deb407cf":"code","7a1959c3":"code","ffd262e6":"code","04eba44b":"code","db3a0598":"code","d58c0100":"code","4b706ec5":"code","711fd830":"code","bbc7742d":"code","7b9b23a1":"code","9f11a7f6":"code","ab1158a2":"code","ff4ae94c":"code","479e6b8a":"code","f2622da0":"code","c10c9107":"code","88961a8d":"code","f3972aa5":"code","f18ded32":"code","8c6a0249":"code","3b48bf12":"code","4500712a":"code","a5531a80":"code","839a87fb":"code","f95cf47a":"code","6b2a39e4":"code","cc305b7f":"code","05b34a08":"code","63e2429f":"code","9cded18a":"code","5b1635ec":"code","f8ae8d0f":"code","f24843ef":"code","ed941d8a":"code","72df7c2d":"code","219254de":"code","d93ca6df":"code","2b8b67a9":"code","31b44303":"code","79a29c92":"code","99114850":"code","9e85918c":"code","12db5466":"code","0b73f972":"code","48af4097":"code","dc18eb6b":"code","d965b7ca":"code","0943ae89":"code","d5c86eba":"code","d76f13f8":"code","8668cf12":"code","8ac0979c":"code","f43ea3b6":"code","bb71ba42":"code","c0d44e5f":"code","788d44c2":"code","320e53b7":"code","829e01c6":"code","ae9176cc":"code","71017708":"code","4b519c84":"code","aa812afd":"code","0a258d64":"code","54e0b2d2":"code","6dd34d58":"code","a55f95ef":"code","48932a9e":"code","bf994990":"code","67f6c291":"code","0ce14120":"code","dd07feaf":"code","511cb75e":"code","a72804e1":"code","f1f185aa":"code","4bc313af":"code","590d12b0":"code","e66482a9":"code","6a4732ef":"code","5189baa2":"code","baf38c28":"markdown","95512488":"markdown","4b857dce":"markdown","17702c08":"markdown","473cff81":"markdown","20ef2b5b":"markdown","4425dbf3":"markdown","e29aeaea":"markdown","56b4a812":"markdown","71f90e85":"markdown","e0517caa":"markdown","345efd14":"markdown","bc950f6e":"markdown","41ac8814":"markdown","3bebbeb2":"markdown","c921ec02":"markdown","9e8a8225":"markdown","4bf8521f":"markdown","e3b02f1c":"markdown","3e2efa11":"markdown","d28c1724":"markdown","5523a55c":"markdown","ac9167da":"markdown","518fe0b3":"markdown","57e1cbb0":"markdown","69102b18":"markdown","4d42a0f0":"markdown","8e8eb463":"markdown","787b6267":"markdown","e220cc64":"markdown","190f6ebe":"markdown","033b2c3e":"markdown","8dfdc1f7":"markdown","62761c35":"markdown","84083236":"markdown","f2585cb2":"markdown","44871a1f":"markdown","aaa9d59d":"markdown","1dca75fa":"markdown","7550b60e":"markdown","8a86283b":"markdown","e3dcd943":"markdown","6ae4144b":"markdown","bd5fe2bc":"markdown","27621781":"markdown","dff57d18":"markdown","53a27a55":"markdown","18a51851":"markdown","113b4dbc":"markdown","9c512fa4":"markdown","4fa9e6da":"markdown","945548ce":"markdown","d70b7ac2":"markdown","77a4584d":"markdown","6b8ef0d7":"markdown","f0496247":"markdown","b300bdeb":"markdown","93b8fc09":"markdown","1374e765":"markdown","4e14c6f7":"markdown","3a2149da":"markdown","5a52afef":"markdown","75d932af":"markdown","804611e1":"markdown","1c487d85":"markdown"},"source":{"e6c53238":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nimport tensorflow as tf\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nfrom sklearn.metrics import silhouette_score\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.decomposition import PCA\nfrom scipy.cluster import hierarchy\nimport plotly.express as px","750cb7d9":"heart_disease=pd.read_csv('..\/input\/health-care-data-set-on-heart-attack-possibility\/heart.csv')#import data as dataframe\nheart_disease.columns          #chcek columns name","3c9ec4fe":"heart_disease.head()","69cbd8ac":"heart_disease.info()","704de974":"fig, ax=plt.subplots(5,3,figsize=(20,28))\nsns.distplot(heart_disease['age'],bins=10,ax=ax[0,0],axlabel='Age Distribution')\nsns.countplot(x=\"sex\", data=heart_disease,ax=ax[0,1])\nsns.countplot(x=\"cp\", data=heart_disease,ax=ax[0,2])\nsns.distplot(heart_disease['trestbps'],bins=10,ax=ax[1,0],axlabel='resting blood pressure')\nsns.distplot(heart_disease['chol'],bins=10,ax=ax[1,1],axlabel='serum cholestoral in mg\/dl')\nsns.countplot(x=\"fbs\", data=heart_disease,ax=ax[1,2])\nsns.countplot(x=\"restecg\", data=heart_disease,ax=ax[2,0])\nsns.distplot(heart_disease['thalach'],bins=10,ax=ax[2,1],axlabel='maximum heart rate achieved')\nsns.countplot(x=\"exang\", data=heart_disease,ax=ax[2,2])\nsns.distplot(heart_disease['oldpeak'],bins=10,ax=ax[3,0],axlabel='ST depression induced by exercise relative to rest')\nsns.countplot(x='slope',data=heart_disease,ax=ax[3,1])\nsns.countplot(x='ca',data=heart_disease,ax=ax[3,2])\nsns.countplot(x='thal',data=heart_disease,ax=ax[4,0])\nsns.countplot(x='target',data=heart_disease,ax=ax[4,1])\nax[4,1].set_title(' target: 0= less chance of heart attack 1= more chance of heart attack')\nax[4,0].set_title('thal: 0 = normal; 1 = fixed defect; 2 = reversable defect')\nax[3,2].set_title('number of major vessels (0-3) colored by flourosopy')\nax[3,1].set_title('the slope of the peak exercise ST segment')\nax[2,2].set_title('exercise induced angina')\nax[1,2].set_title(\"fasting blood sugar > 120 mg\/dl\")\nax[0,2].set_title(\"chest pain type\")\nax[2,0].set_title('resting electrocardiographic results')","6e07c3d5":"plt.figure(figsize= (10, 6))\ncorrMatrix = heart_disease.corr()\nsns.heatmap(corrMatrix, annot=True)\nplt.show()","ae581dd7":"heart_disease.replace(to_replace= r'^\\s*$', value=np.nan,regex=True, inplace=True ) #replace any unit value that only contains \" \", space\nheart_disease.isnull().any() #check whether each column contains a missing value","95e2af2d":"# cut the Dataframe into two parts, one for features, another for target\nX=heart_disease.drop(['target'],axis=1)\ny=heart_disease['target']","e8abb573":"X[['sex','cp','fbs','restecg','exang','slope','ca','thal']]=(X[['sex','cp','fbs','restecg','exang','slope','ca','thal']].astype(object))\nX.info()","d00e30d1":"#convert all the categorical columns into onehot code, \n#and drop the the first onehot code from each conversion.\nX_encode=pd.get_dummies(X,drop_first=True)\nfrom keras.utils.np_utils import to_categorical \ny_encode=to_categorical(y)","deb407cf":"from keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.models import Sequential\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import scale","7a1959c3":"n_cols=X_encode.shape[1] #find number of node in input layer\nn_cols","ffd262e6":"X_encode_scaled=scale(X_encode)\npd.DataFrame(X_encode_scaled)","04eba44b":"features_train1,features_test1,target_train1,target_test1 = train_test_split(X_encode_scaled,y_encode,test_size=0.2,random_state=42,stratify=y)","db3a0598":"model1=Sequential()\nmodel1.add(Dense(40,activation='relu',input_shape=(n_cols,)))\nmodel1.add(Dropout(0.25))\nmodel1.add(Dense(40,activation='relu'))\nmodel1.add(Dropout(0.25))\nmodel1.add(Dense(2,activation='softmax'))                                             #using softmax as activation function(for classfication)\nmodel1.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['categorical_accuracy']) #using categorical_crossentropy as loss function","d58c0100":"early_stopping_monitor=EarlyStopping(patience=2)        #set a early stop to prevent overfitting\nrecord=model1.fit(features_train1,target_train1,validation_split=0.2,epochs=50,callbacks=[early_stopping_monitor])   #10% of data would be used for validation","4b706ec5":"loss,accuracy=model1.evaluate(features_test1,target_test1)\nprint('loss is ', loss, '\\nDNN accuracy on test data is ' ,accuracy)","711fd830":"# Plot accuracy change vs validation accuracy change based on epochs\nplt.plot(record.epoch, record.history.get('categorical_accuracy'),color='orange')\nplt.plot(record.epoch, record.history.get('val_categorical_accuracy'),color='blue')","bbc7742d":"# Save the trained model\nfrom keras.models import load_model\nmodel1.save('model1.h5')","7b9b23a1":"target_pred_label1=np.argmax(model1.predict(features_test1),axis=1) # use DNN to predict labels on test data\nC_dnn=confusion_matrix(\n    target_test1[:,1],   # array, Gound true (correct) target values\n    target_pred_label1,  # array, Estimated targets as returned by a classifier\n    labels=[0,1],        # array, List of labels to index the matrix.\n    sample_weight=None  # array-like of shape = [n_samples], Optional sample weights\n)\n\ncol_name=pd.MultiIndex.from_product([['Predicted label'], ['low risk','high risk']])\nrow_name=pd.MultiIndex.from_product([['True label'], ['low risk','high risk']])\npd.DataFrame(C_dnn,columns=col_name,index=row_name)","9f11a7f6":"def plot_confusion_matrix(cm, labels_name, title):\n    cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]    # normalization\n    plt.imshow(cm, interpolation='nearest')    \n    plt.title(title)    \n    plt.colorbar()\n    num_local = np.array(range(len(labels_name)))    \n    plt.xticks(num_local, labels_name, rotation=90)    \n    plt.yticks(num_local, labels_name)    \n    plt.ylabel('True label')    \n    plt.xlabel('Predicted label')\n\nplot_confusion_matrix(C_dnn,['low risk','high risk'], \"DNN_pred Confusion Matrix\")\nplt.show()","ab1158a2":"print(classification_report(target_test1[:,1],target_pred_label1))","ff4ae94c":"def Probability1(features,model):\n    prediction_array=model.predict(features)\n    probability_array =prediction_array[:,1]\n    probability_table=pd.DataFrame(probability_array)\n    return probability_table\n# we define a function that calculate the probability of heart disease with gien features.","479e6b8a":"Probability1(features_test1,model1)","f2622da0":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score","c10c9107":"features_train2,features_test2,target_train2,target_test2 = train_test_split(X_encode_scaled,y,test_size=0.2,random_state=42,stratify=y)","88961a8d":"logreg=LogisticRegression(max_iter=3000) # set the max iteration to be 3000 otherwise the process can't be finished\nlogreg.fit(features_train2,target_train2)\ntarget_pred2=logreg.predict(features_test2)","f3972aa5":"from sklearn.metrics import roc_curve\ny_pred_prob = logreg.predict_proba(features_test2)[:,1]\nfpr, tpr, thresholds = roc_curve(target_test2,y_pred_prob)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr,label='Logistic Regression')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Logistic Regression ROC Curve')\nplt.show()","f18ded32":"cv_scores=cross_val_score(logreg,X_encode_scaled,y,cv=5,scoring='roc_auc')\nprint('AUC of logistic model is ',cv_scores.mean())","8c6a0249":"C_logistic=confusion_matrix(\n    target_test2,   # array, Gound true (correct) target values\n    target_pred2,  # array, Estimated targets as returned by a classifier\n    labels=[0,1],        # array, List of labels to index the matrix.\n    sample_weight=None  # array-like of shape = [n_samples], Optional sample weights\n)\npd.DataFrame(C_logistic,columns=col_name,index=row_name)","3b48bf12":"plot_confusion_matrix(C_logistic,['low risk','high risk'], \"Logistic Regression Confusion Matrix\")\nplt.show()","4500712a":"print(classification_report(target_test2,target_pred2))","a5531a80":"def Probability2(features):\n    y_pred_prob = logreg.predict_proba(features)[:,1]\n    return pd.DataFrame(y_pred_prob)","839a87fb":"Probability2(features_test2)","f95cf47a":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV","6b2a39e4":"features_train3,features_test3,target_train3,target_test3 = train_test_split(X_encode_scaled,y,test_size=0.2,random_state=42,stratify=y)","cc305b7f":"param_grid = {'n_neighbors': np.arange(1,50)}\nknn=KNeighborsClassifier()\nknn_cv=GridSearchCV(knn,param_grid,cv=5)\nknn_cv.fit(features_train3,target_train3)\n#find best parameter\nn_neighbor=knn_cv.best_params_\nn_neighbor","05b34a08":"#find score of best parameter\nknn_cv.best_score_\nprint('parameter score (n_neighbors=45)is ', knn_cv.best_score_)","63e2429f":"target_pred3=knn_cv.predict(features_test3)  #calculate the predicted target\nknn_cv.score(features_test3,target_test3)\nprint('KNN accuracy on test data is ',knn_cv.score(features_test3,target_test3) )","9cded18a":"C_KNN=confusion_matrix(\n    target_test3,   # array, Gound true (correct) target values\n    target_pred3,  # array, Estimated targets as returned by a classifier\n    labels=[0,1],        # array, List of labels to index the matrix.\n    sample_weight=None  # array-like of shape = [n_samples], Optional sample weights\n)\npd.DataFrame(C_KNN,columns=col_name,index=row_name)","5b1635ec":"plot_confusion_matrix(C_KNN,['low risk','high risk'], \"KNN Confusion Matrix\")\nplt.show()","f8ae8d0f":"print(classification_report(target_test3,target_pred3))","f24843ef":"features_train4,features_test4,target_train4,target_test4 = train_test_split(X_encode_scaled,y,test_size=0.2,random_state=42,stratify=y)","ed941d8a":"clf = tree.DecisionTreeClassifier()\nclf = clf.fit(features_train4,target_train4)\nimport graphviz \ndot_data = tree.export_graphviz(clf, out_file=None,\n                     filled=True, rounded=True,  \n                     special_characters=True) \ngraph = graphviz.Source(dot_data)  \ngraph ","72df7c2d":"clf_accuracy=clf.score(features_test4,target_test4)\ntarget_pred4=clf.predict(features_test4)\nprint('Decision tree model accuracy on test data is ',clf_accuracy)","219254de":"C_decisiontree=confusion_matrix(\n    target_test4,   # array, Gound true (correct) target values\n    target_pred4,  # array, Estimated targets as returned by a classifier\n    labels=[0,1],        # array, List of labels to index the matrix.\n    sample_weight=None  # array-like of shape = [n_samples], Optional sample weights\n)\npd.DataFrame(C_decisiontree,columns=col_name,index=row_name)","d93ca6df":"plot_confusion_matrix(C_decisiontree,['low risk','high risk'], \"Decision Tree Confusion Matrix\")\nplt.show()","2b8b67a9":"print(classification_report(target_test4,target_pred4))","31b44303":"features_train5,features_test5,target_train5,target_test5 = train_test_split(X_encode_scaled,y,test_size=0.2,random_state=42,stratify=y)","79a29c92":"from sklearn.ensemble import RandomForestClassifier\nparam_grid = {'n_estimators':np.arange(10,101,10)}        # set the range of n_estimators that we want to search for\nrfc = RandomForestClassifier(n_jobs=4)                  # build up a model\nrfc_cv = GridSearchCV(rfc,param_grid,scoring='accuracy', cv=3) \nrfc_cv.fit(features_train5,target_train5) ","99114850":"# fubd the best parameter\nrfc_cv.best_params_","9e85918c":"# find the best score of best parameter\nrfc_cv.best_score_","12db5466":"target_pred5=rfc_cv.predict(features_test5)\n\n# find the accuracy on test data\nrfc_accuracy=rfc_cv.score(features_test5,target_test5)\nprint('Random forest model accuracy on test data is',rfc_accuracy)","0b73f972":"C_rfc=confusion_matrix(\n    target_test5,   # array, Gound true (correct) target values\n    target_pred5,  # array, Estimated targets as returned by a classifier\n    labels=[0,1],        # array, List of labels to index the matrix.\n    sample_weight=None  # array-like of shape = [n_samples], Optional sample weights\n)\npd.DataFrame(C_rfc,columns=col_name,index=row_name)","48af4097":"plot_confusion_matrix(C_rfc,['low risk','high risk'], \"Random Forest Confusion Matrix\")\nplt.show()","dc18eb6b":"print(classification_report(target_test5,target_pred5))","d965b7ca":"features_train6,features_test6,target_train6,target_test6 = train_test_split(X_encode_scaled,y,test_size=0.2,random_state=42,stratify=y)","0943ae89":"param_grid={'C':np.arange(0.1,10,0.1)}\nmodel_svm1=svm.SVC(kernel='linear')\nmodel_svm1_cv=GridSearchCV(model_svm1,param_grid,scoring='accuracy',cv=3)\nmodel_svm1_cv.fit(features_train6,target_train6)","d5c86eba":"print(model_svm1_cv.best_params_) # find best hyperparameter.\nprint(model_svm1_cv.best_score_)  # find the score of the best hyperparameter.","d76f13f8":"target_pred6=model_svm1_cv.predict(features_test6)\n# find the accuracy on test data\nsvm1_accuracy=model_svm1_cv.score(features_test6,target_test6)\nprint('svm with linear kernel model accuracy on test data is',svm1_accuracy)","8668cf12":"C_svm1=confusion_matrix(\n    target_test6,   # array, Gound true (correct) target values\n    target_pred6,  # array, Estimated targets as returned by a classifier\n    labels=[0,1],        # array, List of labels to index the matrix.\n    sample_weight=None  # array-like of shape = [n_samples], Optional sample weights\n)\npd.DataFrame(C_svm1,columns=col_name,index=row_name)","8ac0979c":"plot_confusion_matrix(C_svm1,['low risk','high risk'], \"SVM(kernel=linear) Confusion Matrix\")\nplt.show()","f43ea3b6":"print(classification_report(target_test6,target_pred6))","bb71ba42":"features_train8,features_test8,target_train8,target_test8 = train_test_split(X_encode_scaled,y,test_size=0.2,random_state=42,stratify=y)","c0d44e5f":"param_grid={'C':np.arange(0.1,10,0.1)}\nmodel_svm3=svm.SVC(kernel='rbf',gamma='scale')\nmodel_svm3_cv=GridSearchCV(model_svm3,param_grid,scoring='accuracy',cv=3)\nmodel_svm3_cv.fit(features_train8,target_train8)","788d44c2":"print(model_svm3_cv.best_params_) # find best hyperparameter.\nprint(model_svm3_cv.best_score_)  # find the score of the best hyperparameter.\n","320e53b7":"target_pred8=model_svm3_cv.predict(features_test8)\n# find the accuracy on test data\nsvm3_accuracy=model_svm3_cv.score(features_test8,target_test8)\nprint('svm with rbf kernel model accuracy on test data is',svm3_accuracy)","829e01c6":"C_svm3=confusion_matrix(\n    target_test8,   # array, Gound true (correct) target values\n    target_pred8,  # array, Estimated targets as returned by a classifier\n    labels=[0,1],        # array, List of labels to index the matrix.\n    sample_weight=None  # array-like of shape = [n_samples], Optional sample weights\n)\npd.DataFrame(C_svm3,columns=col_name,index=row_name)","ae9176cc":"plot_confusion_matrix(C_svm3,['low risk','high risk'], \"SVM.SVC(kernel='rbf') Confusion Matrix\")\nplt.show()","71017708":"print(classification_report(target_test8,target_pred8))","4b519c84":"from sklearn.manifold import TSNE","aa812afd":"samples= heart_disease\nsamples[['sex','cp','fbs','restecg','exang','slope','ca','thal']]=(samples[['sex','cp','fbs','restecg','exang','slope','ca','thal']].astype(object))\nsamples.info()","0a258d64":"samples_encode=pd.get_dummies(samples,drop_first=True)\nsamples_encode_scaled=scale(samples_encode)","54e0b2d2":"model_TSNE = TSNE (learning_rate=100)\ntransformed = model_TSNE.fit_transform(samples_encode_scaled)\nxs=transformed[:,0]\nys=transformed[:,1]\nplt.scatter(xs,ys,c=y)","6dd34d58":"# set number of components = 3.\nX_scale=scale(X)\npca = PCA(n_components=3)\nX_PCAtransform=pd.DataFrame(pca.fit_transform(X_scale))\nX_PCAtransform['target'] = y\nfig = px.scatter_3d(\n    X_PCAtransform, \n    x=0, \n    y=1,\n    z=2, \n    color=y, \n    title='3d scatter for PCA',\n      width=700,\n    height=700 \n)\nfig.show()","a55f95ef":"# set number of components = 7.\nX_scale=scale(X)\npca = PCA(n_components=7)\npca.fit(X_scale)\n# plot the eigenvalue of 7 components\nplt.plot(pca.explained_variance_)\nplt.xlabel(r'$j$')\nplt.ylabel(r'$\\lambda_j$');","48932a9e":"# plot scree plot to show the percentage of each eigenvalue which indicates the explained variance.\nper_var=np.round(pca.explained_variance_ratio_*100, decimals=1)\nlabels= ['PC'+str(x) for x in range (1, len(per_var)+1)]\nplt.figure(figsize=(14,4))\nplt.bar(x=range(1,len(per_var)+1),height=per_var, tick_label = labels)\nplt.ylabel('percentage of explained variance')\nplt.xlabel('principal component')\nplt.title('scree plot')","bf994990":"# show the eigenvectors for each component (n_component = 7)\ncolumns_name=heart_disease.columns[:-1]\nindex_name=['pc1','pc2','pc3','pc4','pc5','pc6','pc7']\npd.DataFrame(pca.components_,columns=columns_name,index=index_name)","67f6c291":"# show pca score (n_component=7) for each observations\nscores = pca.transform(X_scale)\npd.DataFrame(scores,columns=index_name)","0ce14120":"# we set hyperparameter n_cluster =2 \nfrom sklearn.cluster import KMeans\nmodel6= KMeans(n_clusters=2)\nmodel6.fit(X_encode_scaled)\nkmeans_labels=model6.predict(X_encode_scaled)","dd07feaf":"grouping1=pd.DataFrame({'kmeans_labels':kmeans_labels,'target':y})\ngrouping1","511cb75e":"ct1=pd.crosstab(grouping1['kmeans_labels'],grouping1['target'])\nct1","a72804e1":"inertia=[]\nfor k in range(1,11):\n    model6=KMeans(n_clusters=k)\n    model6.fit(X_encode_scaled)\n    inertia.append(model6.inertia_)","f1f185aa":"plt.plot(range(1,11),inertia)\nplt.title('Kmeans inertia')\nplt.xlabel('number of clusters')\nplt.ylabel('inertia value')","4bc313af":"sc_scores = []\nclusters = range(2,11)\nfor i in clusters:  \n    model=KMeans(n_clusters=i)\n    model.fit(X_encode_scaled)\n    labels = model.predict(X_encode_scaled).ravel()\n    sc_scores.append(silhouette_score(X_encode_scaled, labels))\nplt.plot(clusters, sc_scores)","590d12b0":"lm1 = linkage(X_encode_scaled, method='ward')\nplt.figure(figsize=(12,4))\ndendrogram(lm1, p=2,truncate_mode='level');","e66482a9":"# cut the tree by 2 clusters\nout=hierarchy.cut_tree(lm1,n_clusters=2).ravel()\ngrouping2=pd.DataFrame({'hierachical_labels':out,'target':y})\ngrouping2","6a4732ef":"ct2=pd.crosstab(grouping2['hierachical_labels'],grouping2['target'])\nct2","5189baa2":"sc_scores = []\nclusters = range(2,11)\nfor i in clusters:  \n    labels = hierarchy.cut_tree(lm1, n_clusters=i).ravel()\n    sc_scores.append(silhouette_score(X_encode_scaled, labels))\nplt.plot(clusters, sc_scores)","baf38c28":"#### Evaluate model","95512488":"#### Model construction","4b857dce":"For `DNN`, we need to convert *categorical feature* columns(cp, fbs, exang,slope and so on) into `one-hot code`. To do that first we need to change those columns type from integer to object. As follows:","17702c08":"#### Model 2: SVM.SVC(kernel='rbf')","473cff81":"### 2.1.2 Logistic Regression\n\nApart from DNN, `logistic regression` model is also suitable for binary classification problem.","20ef2b5b":"confusion matrix and classification report","4425dbf3":"#### Model Construction","e29aeaea":"# DSA5102 Project: Are You Likely to Suffer a Heart Attack?<a id='top'><\/a>\n## Contents <a id='top'><\/a>\n1. <a href=#intro>Introduction<\/a>\n    1. <a href=#back>Background<\/a>\n    1. <a href=#object>Objective<\/a>\n    2. <a href=#data>Data description<\/a>\n1. <a href=#model>Modeling<\/a>\n    1. <a href=#sl>Supervised Learning<\/a>\n    1. <a href=#ul>Unsupervised Learning<\/a>\n1. <a href=#conclusion>Conclusion<\/a>\n1. <a href=#ref>Links<\/a>","56b4a812":"#### DNN Model construction:\n\nsequential model;adam optimizer; loss:categorical crossentropy; metrics: categorical crossentropy","71f90e85":"#### Model Construction","e0517caa":"### 2.1.3 KNN model\n\nWe can also use k nearest neighbors model to classify the data points into two categories.","345efd14":"As the `t-SNE` graph shows,we could find a clear boundary between the two categories(target=0, target=1). From this perspective we can visualize samples in 2D.","bc950f6e":"In this way we can not only find the binary result, but also find the actual predicted probability of heart disease for the given feature.","41ac8814":"From the graph we can see each of the principle component contribute an amount of explained variance, which indicates that original features contribute to the result or target, and it is relatively hard to do feature extraction and compression using `PCA`.","3bebbeb2":"#### Probabilistic Calculation from DNN model:\n\nInstead of just knowing a binary result that whether a patient with given features is likely to have heart disease, we want to know the specific probability of heart disease with gien features.","c921ec02":"At a glance of the table, we know there is not any `null\/NAN` value in the DataFrame, which is good.","9e8a8225":"#### Draw inertia plot\n\nInertia can measure clustering quality, and it is the distance from samples to its cluster. Elbow of graph indicates best number of clusters.","4bf8521f":"### 2.1.4 Decision Tree model\n\nWe can also use `decision tree` as classifier.","e3b02f1c":"#### evaluate model with sihouette score ","3e2efa11":"<a id='back'><\/a>\n### 1.1 Background\nNowadays with the development of remote equipments and  communication technologies, the pace of life has become more compact, which caused people are working longer, and the pressure of workers is also increasing fast.At the same time, in such an environment, people usually do not take good care of their bodies, leading to the onset of various diseases.Heart disease as one of  the most common circulatory system diseases, because of its sudden onset and unpredictability, often can not be prevented to cause a fatal blow to patients. It is a huge threat to health.","d28c1724":"### Data cleaning\nCheck for missing value","5523a55c":"#### Normalize all the data (train and test) ","ac9167da":"### 2.1.6 SVM model\n#### Model 1: SVM.SVC(kernel='linear')","518fe0b3":"#### Evaluate the model","57e1cbb0":"In this way we can not only find the binary result, but also find the actual predicted probability of heart disease for the given feature.","69102b18":"### 2.2.4 Hierarchical Clustering model\nApart from `Kmeans` clustering, we also apply `hierachical clustering` to cluster the dataset without target. We also apply `sihouette score` to evaluate the number of clusters.","4d42a0f0":"We use early stop method to prevent overfitting from training too many times.","8e8eb463":"<a id='model'><\/a>\n# 2. Modeling\n<a href=#top>(back to top)<\/a>","787b6267":"#### Model Construction\nHyperparameter Tuning:","e220cc64":"confusion matrix and classification report","190f6ebe":"### 2.1.1 Deep Neural Network Model","033b2c3e":"Then if there is any unit that only contains \" \" or space value, it will be replaced by a `NAN value`. After the replacement, we check again if there is any `null value`. Fortunately, the data are clean.","8dfdc1f7":"When building `DNN`, we first build a overfited model that can ensure it has enough capacity to pass information. Then we use dropout or regularization methods to reduce overfitting, and finally we do hyperparameter tunning(nodes number). Here is the final stage model that already has gone through this process","62761c35":"From this eigenvalue graph we can see we need to cover 7 principle components in order to represent enough explained variance.","84083236":"## 2.2 Unsupervised Learning <a id='ul'><\/a>\n### 2.2.1 Principle Component Analysis (PCA)\n\nProblem statement:\nClose scrutiny of the dataset,there are 13 features in total, so after using supervised machine learning to build predictive models and measure their performance, next I want to use `principle component analysis` to do feature extraction among the 13 features, and reduce the number of features(dimension reduction), and I want to see which of the features have large influence.","f2585cb2":"`KNN` score is 0.85, which is a quite good number for hyperparameter score","44871a1f":"This is the table of PCA score for each principle component among 303 observations.","aaa9d59d":"### 2.1.5 Random forest model","1dca75fa":"#### Evaluate model\nUsing test dataset to plot ROC curve","7550b60e":"<a id='intro'><\/a>\n# 1. Introduction\n<a href=#top>(back to top)<\/a>","8a86283b":"#### Evaluate the model","e3dcd943":"From above we can conclude that `logistic regression model` and `SVM` with linear kernel perform best among those models.","6ae4144b":"Hyperparameter Tuning:\n\n`n_neighbors` is a hyperparamter which means we should decide its value before we train the model. In order to find the best value of it, I use *hperparamter tunning* to find the optimal n_neighbors between 1 and 49. During the process cross validation will be used to find the best parameter and its score.","bd5fe2bc":"#### Model Construction","27621781":"#### Model Construction\nHyperparameter Tuning:","dff57d18":"#### Crosstable labels vs actual target","53a27a55":"From above we can see the *model accuracy* is 87%, and it also perform well on the confusion matrix. What is more, from the plot we can see there seems to be no overfitting as the difference of two lines are not so big.","18a51851":"#### Model Construction\nHyperparameter Tuning:","113b4dbc":"Confusion Matrix and classification report","9c512fa4":"#### t-SNE for 2-dimensional maps \n\n`t-SNE` is t-distributed stochastic neighbor embeding, which can do dimension reduction in data visualization. And its x-axis is meaningless. ","4fa9e6da":"#### Split data into train set and test set","945548ce":"Confusion Matrix and classification report","d70b7ac2":"#### Evaluate the model","77a4584d":"#### Evaluate the model","6b8ef0d7":"Confusion Matrix and classification report","f0496247":"### 2.2.3 K-means Clustering Model\n\nFor **unsupervised learning**, we can also use `K-means` Clustering model to cluster those given features into certain groups. We also apply `sihouette score` to evaluate the number of cluster.\n#### Model Construction","b300bdeb":"From crosstable we can know `Kmeans` can predict most labels correctly, but the accuracy is not good enough.","93b8fc09":"From above we can see the model performance is 91.5% with *accuracy* 0.87, and it also perform well on the `confusion matrix`. What is more, it also performs well on the ROC curve, which means it has high true positive rate and also has low false positive rate. It indicates that model can perform well on classification and filter.","1374e765":"#### evaluate model with sihouette score ","4e14c6f7":"#### Construct PCA model","3a2149da":"#### Evaluate the model","5a52afef":"<a id='object'><\/a>\n### 1.2 Objective\nTherefore, in this project, my interest is to classify the high-risk and low-risk population by using machine learning (supervised learning and unsupervised learning) through 14 attributes that may be associated with cardiac disease, such as patient's age, gender, type of chest pain, resting blood pressure, fasting blood sugar, etc.In the comparison of each model, we want to select the most accurate and most robust model.","75d932af":"### Data Visualization","804611e1":"#### Probability calculation from logistic regression model","1c487d85":"confusion matrix and classification report"}}