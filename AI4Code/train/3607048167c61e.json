{"cell_type":{"3e7527ec":"code","245fa456":"code","9fb4a309":"code","532f9df7":"code","eb634364":"code","e766a098":"code","f96a3cbe":"code","887e851a":"code","eb663229":"code","dc84daf3":"code","811215cf":"code","3053f041":"code","ef771d76":"code","16b37ed1":"code","07ea0bdb":"code","b2804a1d":"code","9eb693e0":"code","7b94cf08":"code","7937c2a6":"code","c273f987":"code","5c69dc37":"code","3cbf052e":"code","a6c76fb1":"code","5bead0fa":"code","dd84a007":"code","d6d86589":"code","400df052":"code","1f599c1c":"code","29d188a3":"code","77ce7dbd":"code","3b4198fc":"code","c434c94e":"code","aa09b253":"code","a376d76b":"code","59bb21d7":"code","181ad23c":"code","f4a68ada":"code","9cf57570":"code","6355039e":"code","6f7057e0":"code","75f0e87e":"code","2687be03":"code","b0cd243c":"code","65c333df":"code","0c1faec6":"code","95797979":"code","f55fd5da":"code","bee22fdb":"code","2e6b56e4":"code","604133a0":"code","3df3e5fc":"code","73293168":"code","de0bef4f":"code","c69492fd":"code","62f2b416":"code","c29faade":"code","ca5c50e6":"code","3f1e54c0":"code","a2256b6f":"code","f9a73426":"code","b8b2ac2f":"code","e0cd77b7":"code","aad500fd":"code","89b2d769":"code","aa94ddb4":"code","30714cae":"code","5af3be18":"code","3fe2fa7c":"code","111cd533":"code","fc09c8d7":"code","1227f2b5":"code","dd2b58ae":"code","3a45a7c9":"code","5662ec1a":"code","67690623":"code","613ec273":"code","70a54936":"code","b155d8a2":"code","cfdc56be":"code","8136ede4":"code","79383f8b":"code","2daeaf15":"code","6b703779":"code","d76eb4c4":"code","fb5f3dc4":"code","abbef075":"code","abb73337":"code","71ebd0b3":"code","b12edbaa":"code","858938eb":"code","4744c49c":"code","e137e4f5":"code","ce55de53":"code","6d72e6d0":"code","afc71a47":"code","5f80bd34":"code","3eff2a16":"code","0225bb16":"code","284e87e5":"code","59014479":"code","4ce1fb91":"code","2b8abdff":"code","b498a78b":"code","8435de6c":"code","b15d7b6e":"code","d309eada":"code","00aa63a2":"code","7ee0c1c1":"code","a8f72cf4":"code","1510ea12":"code","2744608d":"markdown","6a7000da":"markdown","ae41a5c0":"markdown","8b123bee":"markdown","c3a2971d":"markdown","93d800d8":"markdown","bb5e651f":"markdown","783ce475":"markdown","ba018c69":"markdown","4ac472c3":"markdown","1e597cc4":"markdown","e62068ca":"markdown","0579ebad":"markdown","7062fb0f":"markdown","b48fa46d":"markdown","6fea7362":"markdown","130f2215":"markdown","664e8047":"markdown","ee4b27db":"markdown","568bef67":"markdown","a94431c7":"markdown","e6ac2568":"markdown","1cee7f0c":"markdown","74f3e0e8":"markdown","ab4d9224":"markdown","4725b67e":"markdown","853301eb":"markdown","15ef48eb":"markdown","0d67764b":"markdown","337f3cba":"markdown","7d20c570":"markdown","914aafa5":"markdown","f241d829":"markdown","ad072a67":"markdown","8191f267":"markdown","f06394d6":"markdown","16e3f1ea":"markdown","ba890f9f":"markdown","51c95bd3":"markdown","6b40a0c0":"markdown","3eb9f765":"markdown","1258b088":"markdown","e70938e2":"markdown"},"source":{"3e7527ec":"# %pip install gensim\n# %pip install scikit-plot\n#%pip install PrettyTable","245fa456":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sqlite3 as sql\nimport seaborn as sns\nimport re\nfrom nltk.corpus import stopwords\nimport nltk\nnltk.download('stopwords')\nimport time\n# import umap\n#pip install PrettyTable\n#pip install scikit-plot\n#pip install gensim\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import metrics\nimport scikitplot as skplt\nimport gensim\nfrom datetime import timedelta\nimport os\nfrom scipy import sparse\nfrom prettytable import PrettyTable\nfrom itertools import product\nfrom mpl_toolkits.mplot3d import Axes3D\n\nimport keras\nfrom keras.datasets import imdb\nfrom keras.models import Sequential\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\n# fix random seed for reproducibility\nnp.random.seed(7)\n# import plotly_express as px\n# from plotly.offline import plot","9fb4a309":"# for kaggle comment out below code\nos.chdir(\"..\/input\/amazon-fine-food-reviews\/\")","532f9df7":"#establishing the sql connection\namazon_df_con = sql.connect('database.sqlite')\n\n#reading the data from sql connection\namazon_df = pd.read_sql(\"SELECT * FROM Reviews WHERE Score!= 3\", con=amazon_df_con)\n\nprint(\"size of our dataset is\", amazon_df.shape)\namazon_df.head()","eb634364":"#lets consider every review having score greater than 3 as positive\n# and less than 3 as negative \namazon_df['Score'] = amazon_df.Score.apply(lambda x: 'Positive' if x>3 else 'Negative')\n\n# seeing the first 5 rows of amazon_df dataframe\namazon_df.head()","e766a098":"amazon_df.Score.unique()","f96a3cbe":"print(\"size of our data is\", amazon_df.shape)\nprint(\"\")\namazon_df.info()","887e851a":"# lets sort the data based on the time\namazon_df = amazon_df.sort_values('Time')\n\n#lets drop the duplicate datpoints\namazon_df.duplicated().sum()","eb663229":"print(amazon_df.duplicated(subset=['ProductId', 'Text']).sum())\nprint(amazon_df.duplicated(subset=['ProductId', 'Time', 'Text']).sum())\nprint(amazon_df.duplicated(subset=['UserId', 'Time', 'Text']).sum())","dc84daf3":"#so there are no exact duplicate rows\n#lets remove the datapoints having the same productId and Time and UserId\namazon_df = amazon_df.drop_duplicates(subset=['ProductId','Time','UserId'])\namazon_df = amazon_df.drop_duplicates(subset=['ProductId','Time','Text'])\namazon_df = amazon_df.drop_duplicates(subset=['UserId','Time','Text'])","811215cf":"amazon_df.duplicated(subset=['UserId','Time','Text']).sum()","3053f041":"amazon_df.query('HelpfulnessNumerator>HelpfulnessDenominator')","ef771d76":"# so lets remove the above two observations\nnum_great = amazon_df.query('HelpfulnessNumerator>HelpfulnessDenominator')\namazon_df = amazon_df.drop(num_great.index)","16b37ed1":"amazon_df.query('HelpfulnessNumerator>HelpfulnessDenominator')","07ea0bdb":"print(amazon_df.loc[215861])\namazon_df.loc[215861]['Text']","b2804a1d":"# As we can say from above that 215861 indexed row has invalid text and summary, lets drop that row also\namzon_df = amazon_df.drop(215861)","9eb693e0":"print(\"so we are left with {} observations and {} features\".format(amazon_df.shape[0], amazon_df.shape[1]))","7b94cf08":"for sent in amazon_df['Text'].values[50:52]:\n    if len(re.findall('<.*?>', sent)):\n        print(sent)","7937c2a6":"stopw = set(stopwords.words('english'))\nsnow = nltk.stem.SnowballStemmer('english')\n# lets remove words like not, very from stop words as they are meaninging in the reviews \nreqd_words = set(['only','very',\"doesn't\",'few','not'])\nstopw = stopw - reqd_words","c273f987":"def clean_html(review):\n    '''This function cleans html tags if any\n    , in the review'''\n    \n    cleaner = re.compile('<.*?>')\n    clean_txt  = re.sub(cleaner, ' ', review)\n    return clean_txt\n\ndef cleanpunc(sentence): \n    '''function to clean the word of any punctuation\n    or special characters'''\n    \n    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\|\/]',r' ',cleaned)\n    return  cleaned\nprint(stopw)\nprint('****************************')\nprint(snow.stem('beautiful'))","5c69dc37":"def clean_text(list_of_texts):\n    final_cleaned_reviews = []\n    all_positive_words = []\n    all_negative_words = []\n    start_time = time.time()\n    for i,review in enumerate(list_of_texts.values):\n        review_filtered_words = []\n        html_free = clean_html(review)\n        for h_free_word in html_free.split():\n            for clean_word in cleanpunc(h_free_word).split():\n                if (((clean_word.isalpha()) & (len(clean_word)>2)) & \\\n                    ((clean_word.lower() not in stopw))):\n                    final_word = snow.stem(clean_word.lower())\n                    review_filtered_words.append(final_word)\n                    if amazon_df['Score'].values[i] == 'Positive':\n                        all_positive_words.append(final_word)\n                    else:\n                        all_negative_words.append(final_word)\n        final_str = \" \".join(review_filtered_words)\n        final_cleaned_reviews.append(final_str)\n    end_time = time.time()\n    print('time took is ', (end_time-start_time))\n    return final_cleaned_reviews","3cbf052e":"amazon_df[:3]","a6c76fb1":"amazon_df = amazon_df[:100000]\nfinal_cleaned_reivews = clean_text(amazon_df.Text)","5bead0fa":"%%time\n# adding the cleaned reviews to dataframe\namazon_df['CleanedText']=final_cleaned_reivews","dd84a007":"# now lets clean our Summary column and append it our dataframe\nfinal_cleaned_summary = clean_text(amazon_df.Summary)","d6d86589":"amazon_df['Cleaned_summary'] = final_cleaned_summary\nlen(final_cleaned_summary)","400df052":"amazon_df.head()","1f599c1c":"print(amazon_df.Score.value_counts(normalize=True))\nsns.countplot(amazon_df.Score);","29d188a3":"train_df = amazon_df[:60000]\ncv_df = amazon_df[60000:80000]\ntest_df = amazon_df[80000:100000]","77ce7dbd":"train_df['Score'].value_counts(normalize=True)","3b4198fc":"cv_df['Score'].value_counts(normalize=True)","c434c94e":"test_df['Score'].value_counts(normalize=True)","aa09b253":"train_df.CleanedText.values[:3]","a376d76b":"def word_freq_seq(train_reviews,validation_reviews, test_reviews):\n    count_vect = CountVectorizer()\n    count_vect.fit(train_reviews)\n    count_vect_xtrain = count_vect.transform(train_reviews)\n    word_frequencies = count_vect_xtrain.sum(axis=0)\n    word_count_list = [(word, count) for word, count in zip(count_vect.get_feature_names(), np.array(word_frequencies)[0])]\n    word_freq_df = pd.DataFrame(sorted(word_count_list, key=lambda x: x[1], reverse=True), columns = ['word', 'frequency'])\n    word_freq_df['freq_index'] = np.array(word_freq_df.index)+1\n    print(word_freq_df.head())\n    ax = sns.barplot(data=word_freq_df[:20], y='word', x='frequency')\n    ax.set_title(\"top 20 words\")\n    plt.tight_layout()\n    plt.show()\n    \n    # creating the vocabulary dict which contains the top 5k words and there frequency indexing.\n    train_vocab_dict = {}\n    for row in word_freq_df[:5000].iterrows():\n        train_vocab_dict[row[1]['word']] = [row[1]['frequency'], row[1]['freq_index']]\n    \n    \n    train_reviews_list = []\n    cv_reviews_list = []\n    test_reviews_list = []\n    \n    \n    \n    def gen_seq_from_dict(reviews_list, vocab_index_dict):\n        final_reviews_index_list = []\n        for review in reviews_list:\n            review_list = []\n            for word in review.lower().split():\n                try:\n                    review_list.append(vocab_index_dict[word][1])\n                except:\n                    pass\n            final_reviews_index_list.append(np.array(review_list))\n        return final_reviews_index_list\n    \n        \n    train_encoded_reviews = gen_seq_from_dict(train_reviews, train_vocab_dict)\n    valid_encoded_reviews = gen_seq_from_dict(validation_reviews, train_vocab_dict)\n    test_encoded_reviews = gen_seq_from_dict(test_reviews, train_vocab_dict)\n    \n    return train_encoded_reviews, valid_encoded_reviews, test_encoded_reviews\n    \n    \n    ","59bb21d7":"train_encoded_reviews, valid_encoded_reviews, test_encoded_reviews = word_freq_seq(train_df.CleanedText, cv_df.CleanedText, test_df.CleanedText)","181ad23c":"train_encoded_reviews[:2]","f4a68ada":"valid_encoded_reviews[:2]","9cf57570":"test_encoded_reviews[:2]","6355039e":"len(valid_encoded_reviews)","6f7057e0":"# truncate and\/or pad input sequences\nmax_review_length = 400\nX_train = sequence.pad_sequences(train_encoded_reviews, maxlen=max_review_length)\nX_cv = sequence.pad_sequences(valid_encoded_reviews, maxlen=max_review_length)\nX_test = sequence.pad_sequences(test_encoded_reviews, maxlen=max_review_length)\n\nprint(X_train.shape)\nprint(X_train[1])","75f0e87e":"def text_to_num(series):\n    num_array = []\n    for x in series:\n        if x == 'Positive':\n            num_array.append(1)\n        else:\n            num_array.append(0)\n    return np.array(num_array)","2687be03":"y_train = text_to_num(train_df.Score)\ny_cv = text_to_num(cv_df.Score)\ny_test = text_to_num(test_df.Score)","b0cd243c":"# http:\/\/faroit.com\/keras-docs\/1.2.2\/preprocessing\/text\/\nfrom keras.preprocessing.text import Tokenizer","65c333df":"tok = Tokenizer(num_words=600)\ntok.fit_on_texts(train_df.CleanedText)","0c1faec6":"tok_t2m_train_reviews = tok.texts_to_matrix(train_df.CleanedText, mode='count')","95797979":"tok_t2m_valid_reviews = tok.texts_to_matrix(cv_df.CleanedText, mode='count')","f55fd5da":"tok_t2m_test_reviews = tok.texts_to_matrix(test_df.CleanedText, mode='count')","bee22fdb":"tok_t2m_train_reviews.shape","2e6b56e4":"tok_t2m_train_reviews[100]","604133a0":"tok_seq = Tokenizer(num_words=5000)\ntok_seq.fit_on_texts(train_df.CleanedText)","3df3e5fc":"tok_t2s_train_reviews = tok_seq.texts_to_sequences(train_df.CleanedText)\ntok_t2s_valid_reviews = tok_seq.texts_to_sequences(cv_df.CleanedText)\ntok_t2s_test_reviews = tok_seq.texts_to_sequences(test_df.CleanedText)","73293168":"train_encoded_reviews[0]","de0bef4f":"tok_t2s_train_reviews[0]","c69492fd":"# As keras models same shape vectors to feed into models, lets do sequence padding so all the reviews has same number of features.\nmax_review_length = 400\ntok_t2sp_train_reviews = sequence.pad_sequences(tok_t2s_train_reviews, maxlen=max_review_length)\ntok_t2sp_valid_reviews = sequence.pad_sequences(tok_t2s_valid_reviews, maxlen=max_review_length)\ntok_t2sp_test_reviews = sequence.pad_sequences(tok_t2s_test_reviews, maxlen=max_review_length)\n\nprint(tok_t2sp_train_reviews.shape)\nprint(tok_t2sp_train_reviews[0])","62f2b416":"from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())","c29faade":"# confirm Keras sees the GPU\nfrom keras import backend\nassert len(backend.tensorflow_backend._get_available_gpus()) > 0","ca5c50e6":"# Lets first build one lstm layer model\ndef one_lstm_model(max_review_length):\n    embedding_vecor_length = 32\n    model = Sequential()\n    model.add(Embedding(5000+1, embedding_vecor_length, input_length=max_review_length))\n    model.add(LSTM(100))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    #print(model.summary())\n    return model","3f1e54c0":"from keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization\nfrom keras.layers.convolutional import Conv1D, MaxPooling1D","a2256b6f":"# lets build another model with 2 lstm models and one conv layer\ndef two_lstm_conv_model(max_review_length):\n    embedding_vecor_length = 32\n    model2 = Sequential()\n    model2.add(Embedding(5000+1, embedding_vecor_length, input_length=max_review_length))\n    model2.add(Dropout(0.2))\n    model2.add(Conv1D(32, 3, padding='same', activation='relu'))\n    model2.add(MaxPooling1D())\n    model2.add(LSTM(100, return_sequences=True))\n    model2.add(Dropout(0.2))\n    model2.add(LSTM(100))\n    model2.add(Dense(1, activation='sigmoid'))\n    model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    print(model2.summary())\n    return model2","f9a73426":"X_train.shape, y_train.shape, X_cv.shape, y_cv.shape","b8b2ac2f":"## Function to plot history graphs\n\ndef plot_graphs(history):\n  '''Plots epochs vs train and validation accuracies \n  and also epochs vs train and test losses'''\n  train_acc_list = history.history['accuracy']\n  train_loss_list = history.history['loss']\n  val_acc_list = history.history['val_accuracy']\n  val_loss_list = history.history['val_loss']\n  epochs_list = np.array(history.epoch)+1\n  \n  fig, ax = plt.subplots(1,2, sharex=True)\n  #ax.set_xlabel(\"epochs\")\n  fig.suptitle(\"model graphs\")\n  plt.xlabel('epochs')\n  ax[0].plot(epochs_list, train_acc_list, color='b', label='train_accuracies')\n  ax[0].plot(epochs_list, val_acc_list, color='r', label='validation_accuracies')\n  ax[0].legend()\n  ax[0].grid()\n  ax[1].plot(epochs_list, train_loss_list, color='b', label='train_losses')\n  ax[1].plot(epochs_list, val_loss_list, color='r', label='validation_losses')\n  ax[1].legend()\n  ax[1].grid()\n  #ax[1,0].plot(epochs_list, val_acc_list, color='b', label='validation_accuracies')\n  #ax[1,0].legend()\n  #ax[1,1].plot(epochs_list, val_loss_list, color='r', label='validation_losses')\n  #ax[1,1].legend()\n  for a in ax.flat:\n    a.set(xlabel='epochs', ylabel='values')\n  plt.tight_layout()\n  fig.subplots_adjust(top=0.85) #to prevent overlapping of title\n  plt.show()","e0cd77b7":"one_lstm = one_lstm_model(400)","aad500fd":"early_stopping_monitor = EarlyStopping(patience=3)\nhistory_one_wfi = one_lstm.fit(X_train, y_train,\n                    batch_size=512, epochs=20,\n                    validation_data = (X_cv, y_cv),\n                    callbacks = [early_stopping_monitor])","89b2d769":"history_one_wfi.history['accuracy']","aa94ddb4":"plot_graphs(history_one_wfi)","30714cae":"from prettytable import PrettyTable\ntable = PrettyTable()\ntable.field_names = ['encoding','lstm layers', 'conv layers', 'train accuracy', 'test accuracy']","5af3be18":"# lets see how our model1 performs on test data\nscore1_train = one_lstm.evaluate(X_train, y_train, batch_size=512)\nscore1_test = one_lstm.evaluate(X_test, y_test, batch_size=512)\nprint(score1_train)\nprint(score1_test)","3fe2fa7c":"table.add_row(['custom word frequency indexing',1, 0, 0.957, 0.922])","111cd533":"two_lstm = two_lstm_conv_model(400)","fc09c8d7":"early_stopping_monitor = EarlyStopping(patience=3)\nhistory_two_wfi = two_lstm.fit(X_train, y_train,\n                    batch_size=512, epochs=20,\n                    validation_data = (X_cv, y_cv),\n                    callbacks = [early_stopping_monitor])","1227f2b5":"plot_graphs(history_two_wfi)","dd2b58ae":"score2_train = two_lstm.evaluate(X_train, y_train, batch_size=512)\nscore2_test = two_lstm.evaluate(X_test, y_test, batch_size=512)\nprint(score2_train)\nprint(score2_test)","3a45a7c9":"table.add_row(['custom word frequency indexing',2, 1, 0.9658, 0.9272])","5662ec1a":"one_lstm_w2m = one_lstm_model(600)","67690623":"early_stopping_monitor = EarlyStopping(patience=3)\nhistory_one_t2m = one_lstm_w2m.fit(tok_t2m_train_reviews, y_train,\n                    batch_size=512, epochs=7,\n                    validation_data = (tok_t2m_valid_reviews, y_cv),\n                    callbacks = [early_stopping_monitor])","613ec273":"plot_graphs(history_one_t2m)","70a54936":"score3_train = one_lstm_w2m.evaluate(tok_t2m_train_reviews, y_train, batch_size=512)\nscore3_test = one_lstm_w2m.evaluate(tok_t2m_test_reviews, y_test, batch_size=512)\nprint(score3_train)\nprint(score3_test)","b155d8a2":"table.add_row(['keras text2matrix',1, 0, 0.88, 0.866])","cfdc56be":"two_lstm_w2m = two_lstm_conv_model(600)","8136ede4":"early_stopping_monitor = EarlyStopping(patience=3)\nhistory_two_t2m = two_lstm_w2m.fit(tok_t2m_train_reviews, y_train,\n                    batch_size=512, epochs=5,\n                    validation_data = (tok_t2m_valid_reviews, y_cv),\n                    callbacks = [early_stopping_monitor])","79383f8b":"plot_graphs(history_two_t2m)","2daeaf15":"score4_train = two_lstm_w2m.evaluate(tok_t2m_train_reviews, y_train, batch_size=512)\nscore4_test = two_lstm_w2m.evaluate(tok_t2m_test_reviews, y_test, batch_size=512)\nprint(score4_train)\nprint(score4_test)","6b703779":"table.add_row(['keras text2matrix',2, 1, 0.885, 0.866])","d76eb4c4":"one_lstm_t2sp = one_lstm_model(400)","fb5f3dc4":"early_stopping_monitor = EarlyStopping(patience=4)\nhistory_one_t2sp = one_lstm_t2sp.fit(tok_t2sp_train_reviews, y_train,\n                    batch_size=1024, epochs=20,\n                    validation_data = (tok_t2sp_valid_reviews, y_cv),\n                    callbacks = [early_stopping_monitor])","abbef075":"plot_graphs(history_one_t2sp)","abb73337":"score5_train = one_lstm_t2sp.evaluate(tok_t2sp_train_reviews, y_train, batch_size=1024)\nscore5_test = one_lstm_t2sp.evaluate(tok_t2sp_test_reviews, y_test, batch_size=1024)\nprint(score5_train)\nprint(score5_test)","71ebd0b3":"print(table)","b12edbaa":"table.add_row(['keras text2sequence padded',1, 0, 0.933, 0.904])","858938eb":"two_lstm_t2sp = two_lstm_conv_model(400)","4744c49c":"early_stopping_monitor = EarlyStopping(patience=5)\nhistory_two_t2sp = two_lstm_t2sp.fit(tok_t2sp_train_reviews, y_train,\n                    batch_size=1024, epochs=20,\n                    validation_data = (tok_t2sp_valid_reviews, y_cv),\n                    callbacks = [early_stopping_monitor])","e137e4f5":"plot_graphs(history_two_t2sp)","ce55de53":"score6_train = two_lstm_t2sp.evaluate(tok_t2sp_train_reviews, y_train, batch_size=1024)\nscore6_test = two_lstm_t2sp.evaluate(tok_t2sp_test_reviews, y_test, batch_size=1024)\nprint(score6_train)\nprint(score6_test)","6d72e6d0":"print(table)","afc71a47":"table.add_row(['keras text2sequence padded',2, 1, 0.9756, 0.923])","5f80bd34":"print(table)","3eff2a16":"# keras text to sequence two lstm model confusion matrix\nmodel2_test_predict = two_lstm_t2sp.predict_classes(X_test)\nskplt.metrics.plot_confusion_matrix(y_test, model2_test_predict);","0225bb16":"# custom word frequency two lstm model confusion matrix\nmodel1_test_predict = two_lstm.predict_classes(X_test)\nskplt.metrics.plot_confusion_matrix(y_test, model1_test_predict);","284e87e5":"print(table)","59014479":"from sklearn.utils import class_weight","4ce1fb91":"cls_wt_dict = dict(enumerate(class_weight.compute_class_weight('balanced',np.unique(y_train),y_train)))\ncls_wt_dict","2b8abdff":"pd.Series.value_counts(y_train, normalize=True)","b498a78b":"two_lstm_class_wts = two_lstm_conv_model(400)\nearly_stopping_monitor = EarlyStopping(patience=10)\nhistory_nine_wfi = two_lstm_class_wts.fit(X_train, y_train,\n                    batch_size=1024, epochs=20,\n                    validation_data = (X_cv, y_cv),\n                    class_weight = cls_wt_dict,\n                    callbacks = [early_stopping_monitor])\n","8435de6c":"plot_graphs(history_nine_wfi)","b15d7b6e":"score9_train = two_lstm_class_wts.evaluate(X_train, y_train, batch_size=1024)\nscore9_test = two_lstm_class_wts.evaluate(X_test, y_test, batch_size=1024)\nprint(score9_train)\nprint(score9_test)","d309eada":"table.add_row(['custom word frequency indexing with class wts',2, 1, 0.978, 0.908])","00aa63a2":"skplt.metrics.plot_confusion_matrix(y_test, model1_test_predict);","7ee0c1c1":"# custom word frequency two lstm model confusion matrix\nfinal_model_test_predict_cls_wts = two_lstm_class_wts.predict_classes(X_test)\nskplt.metrics.plot_confusion_matrix(y_test, final_model_test_predict_cls_wts);","a8f72cf4":"skplt.metrics.plot_confusion_matrix(y_test, final_model_test_predict_cls_wts, title='with class weights');\nskplt.metrics.plot_confusion_matrix(y_test, model1_test_predict, title='without class weights');","1510ea12":"print(table)","2744608d":"* Now we have two LSTM models one with single LSTM layer and other with two LSTM layers.\n* Now lets try all the three different encoding vectors on these two models and observe performance difference.","6a7000da":"# Confusion matrix on two best models till now.","ae41a5c0":"## [5.3.1] one lstm model on keras tokenizer word to sequence padded review vectors","8b123bee":"* As we can see using class weights has improved the accuracy of predicting the negative reviews.\n* But using class weights has decreased the overall test accuracy nearly 0.2%, so its a trade off between accurately predicting negative reviews and overall accuracy. \n* Based on the requirement we can select one of the models as best models.","c3a2971d":"## [1.1] Loading the data\n\nThe dataset is available in two forms\n1. .csv file\n2. SQLite Database\n\nIn order to load the data, We have used the sqlite and pandas to read and analyse the data.\n<br> \n\nHere as we only want to get the global sentiment of the recommendations (positive or negative), we will purposefully ignore all Scores equal to 3. If the score is above 3, then the recommendation wil be set to \"positive\". Otherwise, it will be set to \"negative\".","93d800d8":"* Lets see the confusion matrix and see whether our model's performance on predicting negative reviews has increased after using class weights.","bb5e651f":"* AS we can see for the initial 3 epochs our model is learning and both the training and validation has increased, but after the 3rd epoch model started overfitting on the training data and validation accuracy stopped increasing.\n* Now lets try on the second model with contains two lstm layers, dropout and see if they reduce these overfitting.","783ce475":"## [5.3.2] Two lstm model on keras tokenizer word to sequence padded review vectors","ba018c69":"We can see that we have total 525814 data points with 10 features\n\nScore Feature contains two values, either Positive or Negative and it is our target variable","4ac472c3":"> * As we can see our model is not learning from epochs as both trianing and validation accuracy are constant throught out the epochs.\n* This may be because of sparcity of text 2 matrix vectors created.","1e597cc4":"* Lets use the class_weight parameter to balance the class weights and see if we can improve our model further.","e62068ca":"## [4.2] Converting text to vectors using keras tokenizer texts to matrix.","0579ebad":"* On comparing both the confusion matrix, custom word frequency with two lstm model is performing better in predicting negative reviews compared to keras tokenizer text to sequence.\n* As both the models are giving final test accuracy of 0.92, we can consider custom word frequency with two lstm as the best model in the point of prediction of negative reviews.","7062fb0f":"from above we can see that there are no exact duplicate observations. \n<p>\nbut lets see whether there are any observations having same duplicate features like Text, productId, and Time and UserId while having the remaining features different.\n<\/p>\n<p>\nBecause there can't be any review with same Text and UserId with different productId and also no single User can post the same review(Text) at two different times.<\/p>\nSo above subduplicates will also be considered as duplicates and we will delete them for further analysis","b48fa46d":"* As you can see both our custom function word frequency indexing vector and keras tokenizer word 2 sequence vector has almost similar values except some, may be that is because of the difference in filtering or others.","6fea7362":"## [2.1] Data Cleaning: Deduplication\n\nLets see whether any duplicate observations are there in the data and delete them, so that we get unbiased results for the analysis of the data","130f2215":"## [3.1].  Preprocessing Review Text\n\nNow that we have finished deduplication our data requires some preprocessing before we go on further with analysis and making the prediction model.\n\nHence in the Preprocessing phase we do the following in the order below:-\n\n1. Begin by removing the html tags\n2. Remove any punctuations or limited set of special characters like , or . or # etc.\n3. Check if the word is made up of english letters and is not alpha-numeric\n4. Check to see if the length of the word is greater than 2 (as it was researched that there is no adjective in 2-letters)\n5. Convert the word to lowercase\n6. Remove Stopwords\n7. Finally Snowball Stemming the word (it was obsereved to be better than Porter Stemming)","664e8047":"# Objective: \nSentiment analysis on amazon fine food reviews using LSTM's\n\n## data source: \nhttps:\/\/www.kaggle.com\/snap\/amazon-fine-food-reviews\n\n1. Number of reviews: 568,454\n2. Number of users: 256,059\n3. Number of products: 74,258\n4. Timespan: Oct 1999 - Oct 2012\n5. Number of Attributes\/Columns in data: 10 \n6. Attribute Information:\n    1. Id\n    2. ProductId - unique identifier for the product\n    3. UserId - unqiue identifier for the user\n    4. ProfileName\n    5. HelpfulnessNumerator - number of users who found the review helpful\n    6. HelpfulnessDenominator - number of users who indicated whether they found the review helpful or not\n    7. Score - rating between 1 and 5\n    8. Time - timestamp for the review\n    9. Summary - brief summary of the review\n    10. Text - text of the review\nGiven a review, determine whether the review is positive (rating of 4 or 5) or negative (rating of 1 or 2).\n\n<br>\n[Q] How to determine if a review is positive or negative?<br>\n<br> \n[Ans] We could use Score\/Rating. A rating of 4 or 5 can be cosnidered as a positive review. A rating of 1 or 2 can be considered as negative one. A review of rating 3 is considered nuetral and such reviews are ignored from our analysis. This is an approximate and proxy way of determining the polarity (positivity\/negativity) of a review.","ee4b27db":"## [5.2.1] one lstm model on keras tokenizer word to matrix review vectors","568bef67":"#  [2] Exploratory Data Analysis","a94431c7":"## [5.1.2] two lstm model on custom word frequency indexing review vectors","e6ac2568":"## [5.2.2] two lstm model on keras tokenizer word to matrix review vectors","1cee7f0c":"# [6] Conclusions","74f3e0e8":"## [4.3] Converting text data into vectors using keras tokenizer text to sequence.","ab4d9224":"* We should first convert our text reviews in the form of numbers to feed into deep learning model.\n* For converting text into vectors and feed them into our LSTM'S, We will try 3 different approaches.\n1. Word Frequency encoding with sequence padding.\n2. Keras Tokenizer text to matrix converter.\n3. Keras Tokenizer text to seqence converted with sequence padding.","4725b67e":"* Even though the model is learning the train and test accuracies are less compared to custom word frequency indexing.\n* Similar to custom word frequency indexing there is overfitting problem and lets see if our two lstm and one conv layer model can solve these problems.","853301eb":"So lets import the stop words and also write functions for removing and html tags, and punctuations","15ef48eb":"## [4.1] converting text data into vectors using word frequency indexing.","0d67764b":"* As we can see eventhough model training accuracy is increased to 0.96 from 0.95, testing accuracy is increased from 0.922 to 0.927 which is very small improvement.","337f3cba":"From above we can see that there are duplicates with same ProductId, Text, UserId\n<p>\nOut of the obove 3 filters, there are 161681 observations with same UserId, Time, and Text which means that same user has posted same Text at same Time, which is unlikely to happen, so lets delete the above duplicates.\n<\/p>\n","7d20c570":"#  [3] Preprocessing","914aafa5":"# [4] Text Preprocessing","f241d829":"## [5.1.1] one lstm model on custom word frequency indexing review vectors","ad072a67":"* As we can see, with two_lstm_model also its not learning anything.","8191f267":"we can see that there are 84.31 percent positive reviews and 15.68 percent negative reviews, \n<br>\nSo the amazon fine food reviews dataset is higly imbalanced with postive reviews\n<\/br>","f06394d6":"* From above we can see we are getting best metrics from both \"keras text2sequence padded two lstm layers model\" and also \"custom word frequency indexing two lstm model\"\n* Lets see the confusion matrix of both the models and see how these two models are performing on the individual classes.","16e3f1ea":"# [5] Modelling","ba890f9f":"1. Assumed every review with less than 3 rating as negative review and greater than 3 as positive review and stored in a column called Score\n2. Performed Data wrangling by removing subduplicate observations and exploratory data analysis.\n3. Build two models one with single LSTM layer, and other with one conv1d, 2 lstm layers.\n4. Tried both the models on three different encodings, one with custom word frequency indexing, other with keras tokenizer word to matrix, other with keras tokenzier word to sequence with padding.\n6. Out of all the vectorizers, custom word frequency indexing and keras text to sequence using twlo lstm models and one conv layer is giving good test accuracy.\n7. but based on the number of currectly predicted negative reviews as its a minority class, we have decided custom word frequency indexing with two lstm layers and one conv layer is the best model.\n8. Further used the class weight parameter on the best model to see if it improves the accuracy of predicting minority(negative reviews) class.\n9. using class weights has improved the accuracy of predicting the negative reviews.\n10. But using class weights has also decreased the overall test accuracy nearly 0.2%, so its a trade off between accurately predicting negative reviews and overall accuracy.","51c95bd3":"Now lets see whether there are any observations which has helpfulness numerator feature greater than helpfulness denominator, and delete those observations","6b40a0c0":"* For building Word Frequency Indexing we will follow the following steps.\n1. Build a vocabulary using all the text reviews.\n2. Count the number of times each word occurs in the whole corpus.\n3. Assign each word to there counts and sort them in the descending order.\n4. Give the indexing to each word based on the number of counts that partuclar word occured in the corpus.\n5. Loop over each review and encode each word by there frequency index.","3eb9f765":"# [1]. Reading Data","1258b088":"To analyze the reviews and convert to vectors lets perform the following steps\n\n1. iterating through each of the review(# 1st loop)\n2. cleaning any html content of the review \n3. splitting individually all the words of the review into a list \n4. apply cleanpunc function on each word of the list so that we can remove if any punctiations are there\n5. checking whether the cleaned word is there in the stop words\n6. add the word to the new list if the cleaned word is not the stop words, and it is alphabetic and length is greater than 2\n7. Separately store all the positve words and negative words in two separate lists\n8. now make whole new list into a string\n9. the new string is the processed review","e70938e2":"* Two lstm model with keras text to sequence vectors are performing well compared to single lstm model, both train and test accuracies has increased."}}