{"cell_type":{"48300bab":"code","36ebc78c":"code","cd65d556":"code","0b0d6146":"code","61f97568":"code","52e2ce57":"code","1d0cef3a":"code","d895c6b3":"code","1d603df6":"code","6e66a683":"code","c3dbf4dd":"code","6fa28903":"code","2c808854":"code","3a95f14f":"code","b8cec455":"code","7ecc7aaf":"code","1b54f8b1":"code","582f8a73":"markdown","a23ab330":"markdown","bd6b90ce":"markdown"},"source":{"48300bab":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","36ebc78c":"#import libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf","cd65d556":"# plot series\n\ndef plot_series(time, series, format=\"-\", start=0, end=None):\n    plt.plot(time[start:end], series[start:end], format)\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Value\")\n    plt.grid(True)","0b0d6146":"import csv\n\ntime_step = []\nsunspots = []\n\nwith open('\/kaggle\/input\/sunspots\/Sunspots.csv') as csvfile:\n    reader = csv.reader(csvfile, delimiter=',')\n    next(reader)\n    for row in reader:\n        time_step.append(int(row[0]))\n        sunspots.append(float(row[2]))","61f97568":"series = np.array(sunspots)\ntime = np.array(time_step)\n\nplt.figure(figsize=(10,6))\nplot_series(time, series)","52e2ce57":"split_time = 3000\ntime_train = time[:split_time]\nx_train = series[:split_time]\n\ntime_valid = time[split_time:]\nx_valid = series[split_time:]","1d0cef3a":"def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n    series = tf.expand_dims(series, axis=-1)\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size+1, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w : w.batch(window_size+1))\n    ds = ds.shuffle(shuffle_buffer)\n    ds = ds.map(lambda w : (w[:-1], w[-1]))\n    ds = ds.batch(batch_size).prefetch(1)\n    return ds","d895c6b3":"def model_forecast(model, series, window_size):\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w : w.batch(window_size))\n    ds = ds.batch(32).prefetch(1)\n    forecast = model.predict(ds)\n    return forecast","1d603df6":"# Hyperparameters\n\nwindow_size = 60\nbatch_size = 100\nshuffle_buffer_size = 1000","6e66a683":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\n\ntrain_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer=shuffle_buffer_size)\n\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv1D(filters=60,\n                           kernel_size=5,\n                          strides=1,\n                          padding='causal',\n                          activation='relu',\n                          input_shape= [None, 1]),\n    tf.keras.layers.LSTM(60, return_sequences=True),\n    tf.keras.layers.LSTM(60, return_sequences=True),\n    tf.keras.layers.Dense(30, activation='relu'),\n    tf.keras.layers.Dense(10, activation='relu'),\n    tf.keras.layers.Dense(1)\n])\n\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch : 1e-8 * 10 ** (epoch\/20))\n\noptimizer = tf.keras.optimizers.SGD(learning_rate=1e-8, momentum=0.9)\n\nmodel.compile(loss = tf.keras.losses.Huber(),\n             optimizer = optimizer,\n             metrics = ['mae'])\n\nhistory = model.fit(train_set, epochs=100, callbacks=[lr_schedule])","c3dbf4dd":"\nplt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\nplt.axis([1e-4, 1e-3, 40, 90])","6fa28903":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\n\ntrain_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer=shuffle_buffer_size)\n\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv1D(filters=60,\n                           kernel_size=5,\n                          strides=1,\n                          padding='causal',\n                          activation='relu',\n                          input_shape= [None, 1]),\n    tf.keras.layers.LSTM(60, return_sequences=True),\n    tf.keras.layers.LSTM(60, return_sequences=True),\n    #tf.keras.layers.LSTM(60, return_sequences=True),\n    tf.keras.layers.Dense(30, activation='relu'),\n    tf.keras.layers.Dense(10, activation='relu'),\n    tf.keras.layers.Dense(1)\n    #tf.keras.layers.Lambda(lambda x: x * 400)\n])\n\noptimizer = tf.keras.optimizers.SGD(learning_rate=3e-4, momentum=0.9)\n\nmodel.compile(loss = tf.keras.losses.Huber(),\n             optimizer = optimizer,\n             metrics = ['mae'])\n\nhistory = model.fit(train_set, epochs=400)","2c808854":"rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)\nrnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]","3a95f14f":"\nplt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, rnn_forecast)","b8cec455":"tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()","7ecc7aaf":"import matplotlib.image  as mpimg\nimport matplotlib.pyplot as plt\n\n#-----------------------------------------------------------\n# Retrieve a list of list results on training and test data\n# sets for each training epoch\n#-----------------------------------------------------------\nloss=history.history['loss']\n\nepochs=range(len(loss)) # Get number of epochs\n\n\n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.plot(epochs, loss, 'r')\nplt.title('Training loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend([\"Loss\"])\n\nplt.figure()\n\n\n\nzoomed_loss = loss[200:]\nzoomed_epochs = range(200,400)\n\n\n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.plot(zoomed_epochs, zoomed_loss, 'r')\nplt.title('Training loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend([\"Loss\"])\n\nplt.figure()","1b54f8b1":"print(rnn_forecast)","582f8a73":"### Import all libraries","a23ab330":"### Read data","bd6b90ce":"### Model functions and Hyper parameters\n"}}