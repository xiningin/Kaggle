{"cell_type":{"09695669":"code","48168c52":"code","7f455227":"code","266353b2":"code","e45552ae":"code","89132e59":"code","122c30d1":"code","497ecf5e":"code","b6ed4fcd":"code","1bad6d27":"code","500fac7f":"code","16324e9b":"code","e66cd406":"code","440e17e8":"code","18ef8d5e":"code","a6417bb7":"code","d6c13de8":"code","9edd7596":"code","315f60c2":"code","c89d4366":"code","2640678d":"code","03adf6c9":"code","d75601ce":"code","15181197":"code","858eb7cb":"code","dada0c07":"code","f9bcafe9":"code","8ff84236":"code","2ff1692d":"code","4d095d0e":"code","b448e670":"code","3e7faee4":"code","8bccac80":"code","e3820cd1":"code","8a3f4a6c":"code","ed940ca2":"code","9fe8a3c1":"markdown","109a2475":"markdown","4fcfe4ae":"markdown","f33b81ad":"markdown","c4d29f15":"markdown","13ccc4d0":"markdown","c2b9c385":"markdown","767290cd":"markdown","2843a6f7":"markdown","27232df6":"markdown","af17527f":"markdown","f61f1735":"markdown","83008457":"markdown"},"source":{"09695669":"#importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","48168c52":"#importing and defining Data Frame\ndf = pd.read_csv(\"..\/input\/league-of-legends-diamond-ranked-games-10-min\/high_diamond_ranked_10min.csv\")","7f455227":"# cheking info of the df\ndef data_inv(df):\n    print('dataframe: ',df.shape[0])\n    print('dataset variables: ',df.shape[1])\n    print('-'*10)\n    print('dateset columns: \\n')\n    print(df.columns)\n    print('-'*10)\n    print('data-type of each column: \\n')\n    print(df.dtypes)\n    print('-'*10)\n    print('missing rows in each column: \\n')\n    c=df.isnull().sum()\n    print(c[c>0])\ndata_inv(df)","266353b2":"# checking the df:\ndf.head()","e45552ae":"# creating a copy\/checkpoint before deleting unnecessary columns\ndf_1 = df.copy()","89132e59":"#dropping columns\ndf_1 = df_1.drop(['blueGoldDiff', 'blueExperienceDiff','redGoldDiff',\n       'redExperienceDiff','gameId'], axis=1)","122c30d1":"#check number of diferent values\ndf_1.nunique()","497ecf5e":"#the probability of blue team winning is inversely correlated with red team, so the task is to analyse the Blue Team.\ndf_blue = df_1.drop(['redWardsPlaced','redWardsDestroyed',\n       'redFirstBlood', 'redKills', 'redDeaths', 'redAssists',\n       'redEliteMonsters', 'redDragons', 'redHeralds', 'redTowersDestroyed',\n       'redTotalGold', 'redAvgLevel', 'redTotalExperience',\n       'redTotalMinionsKilled', 'redTotalJungleMinionsKilled', 'redCSPerMin', 'redGoldPerMin'], axis=1)","b6ed4fcd":"x = df_blue['blueWins']\ny = df_blue['blueTotalGold']\nplt.bar(x, y)\nplt.xticks(range(0,2))\nplt.show()","1bad6d27":"# Total Gold vs Total minions killed, as we can see more minions killed doesn't equate more gold \nx1 = df_blue['blueTotalMinionsKilled']\ny1 = df_blue['blueTotalGold']\nplt.scatter(x1, y1)\nplt.show()","500fac7f":"# correlation between the variables,\n# To avoid Multicollinearity, the independent variables must not be over 0,7 correlation or the regression output will \n        # be erroneous, for example: Blue Kills is highly correlated with Blue Assists, and one must be omitted from the model\ncorr = df_blue.corr()","16324e9b":"plt.figure(figsize=(16,10))\nsns.heatmap(corr, annot =  True)","e66cd406":"# creating an input table with only the independent variables, ommiting the correlating ones,\n# creating the target variable = Blue Wins\ndf_blue.columns","440e17e8":"unscaled_inputs = df_blue.filter(['blueWardsPlaced', 'blueWardsDestroyed', 'blueFirstBlood',\n       'blueKills', 'blueDeaths','blueEliteMonsters','blueHeralds', 'blueTowersDestroyed','blueAvgLevel','blueTotalMinionsKilled', 'blueTotalJungleMinionsKilled'], axis=1)\ntarget = df_blue.filter(['blueWins'])","18ef8d5e":"# import the libraries needed to create the Custom Scaler\n# note that all of them are a part of the sklearn package\n# moreover, one of them is actually the StandardScaler module, \n# so you can imagine that the Custom Scaler is build on it\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler\n\n# create the Custom Scaler class\n\nclass CustomScaler(BaseEstimator,TransformerMixin): \n    \n    # init or what information we need to declare a CustomScaler object\n    # and what is calculated\/declared as we do\n    \n    def __init__(self,columns,copy=True,with_mean=True,with_std=True):\n        \n        # scaler is nothing but a Standard Scaler object\n        self.scaler = StandardScaler(copy,with_mean,with_std)\n        # with some columns 'twist'\n        self.columns = columns\n        self.mean_ = None\n        self.var_ = None\n        \n    \n    # the fit method, which, again based on StandardScale\n    \n    def fit(self, X, y=None):\n        self.scaler.fit(X[self.columns], y)\n        self.mean_ = np.mean(X[self.columns])\n        self.var_ = np.var(X[self.columns])\n        return self\n    \n    # the transform method which does the actual scaling\n\n    def transform(self, X, y=None, copy=None):\n        \n        # record the initial order of the columns\n        init_col_order = X.columns\n        \n        # scale all features that you chose when creating the instance of the class\n        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]), columns=self.columns)\n        \n        # declare a variable containing all information that was not scaled\n        X_not_scaled = X.loc[:,~X.columns.isin(self.columns)]\n        \n        # return a data frame which contains all scaled features and all 'not scaled' features\n        # use the original order (that you recorded in the beginning)\n        return pd.concat([X_not_scaled, X_scaled], axis=1)[init_col_order]","a6417bb7":"# categorical columns to omit\ncolumns_to_omit = ['blueFirstBlood']","d6c13de8":"# create the columns to scale, based on the columns to omit\n# use list comprehension to iterate over the list\ncolumns_to_scale = [x for x in unscaled_inputs.columns.values if x not in columns_to_omit]","9edd7596":"blue_scaler = CustomScaler(columns_to_scale)","315f60c2":"blue_scaler.fit(unscaled_inputs)","c89d4366":"scaled_inputs = blue_scaler.transform(unscaled_inputs)\nscaled_inputs","2640678d":"from sklearn.model_selection import train_test_split","03adf6c9":"train_test_split(scaled_inputs, target)","d75601ce":"x_train, x_test, y_train, y_test = train_test_split(scaled_inputs, target, train_size=0.8, random_state=20)","15181197":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics","858eb7cb":"reg = LogisticRegression()","dada0c07":"reg.fit(x_train, y_train)","f9bcafe9":"# Regression score\nreg.score(x_train, y_train)","8ff84236":"# The Intercept\nintercept = reg.intercept_\nintercept","2ff1692d":"# Creating a Summary Table to visualize the Variable and respective Coefficients and Odds Ratio\nvariables = unscaled_inputs.columns.values\nvariables","4d095d0e":"summary_table = pd.DataFrame(columns=['Variables'], data = variables)\nsummary_table['Coef'] = np.transpose(reg.coef_)\n# add the intercept at index 0\nsummary_table.index = summary_table.index + 1\nsummary_table.loc[0] = ['Intercept', reg.intercept_[0]]\n# calculate the Odds Ratio and add to the table\nsummary_table['Odds Ratio'] = np.exp(summary_table.Coef)","b448e670":"summary_table.sort_values(by=['Odds Ratio'], ascending=False)","3e7faee4":"import statsmodels.api as sm\nx = sm.add_constant(x_train)\nlogit_model=sm.Logit(y_train,x)\nresult=logit_model.fit()\nprint(result.summary())","8bccac80":"# testing the data is important to evalute the accuracy on a dataset that the model has never seen, to see if it's Overfitting\n  # a test score 10% below the training reveals an overfitting\nreg.score(x_test, y_test)","e3820cd1":"predicted_prob = reg.predict_proba(x_test)\npredicted_prob[:,1]","8a3f4a6c":"df_blue['predicted'] = reg.predict_proba(scaled_inputs)[:,1]","ed940ca2":"df_blue","9fe8a3c1":"## Checking the Data Frame info and first look of the data\/descriptive statistics","109a2475":"## Testing the data","4fcfe4ae":"## Test, Train and Split the data","f33b81ad":"## Importing Libraries & importing the csv data file","c4d29f15":"## Standardizing the data ","13ccc4d0":"## Cleaning the Data Frame","c2b9c385":"1- The Variables \"Blue Wards Placed\", \"Blue Wards Destroyed\"have no statistical significance, so the next step is to remove them from the model\n\n2- The Variable that has the biggest impact in the odds of winning is the number of Kills for the Blue Team, for every Kill the odds increase by 132%.\n    \n3- The Second variable that impacts the outcome is, as expected, the number of Deaths, this time it impacts negatively:for every Death, the odds of winning decreases by 50% for the Blue team\n    \n4- Surprizingly, killing Heralds decreases the odds of winning by about 12%\n\n5- Killing Elite monsters and Total minions killed are the next biggest impact in winning with 30% and 20% increase, respectively","767290cd":"## Plotting some graphs to better visualize the data","2843a6f7":"# Analysing which variables contribute the most for the Blue Team to Win.","27232df6":"Scale just the non categorical variables, in this case 'Blue first Blood' is categorical","af17527f":"# Conclusions","f61f1735":"## Logistic Regression","83008457":"## Calculating P-values"}}