{"cell_type":{"df59545c":"code","d296e1ae":"code","dd252058":"code","db6fdf1d":"code","d93bc7c4":"code","0ea9f627":"code","6432df18":"code","ae23e6e8":"code","d64f4419":"code","f85f9142":"code","f63d52ca":"code","ff77d6c7":"code","5fbd74fa":"code","b0a1ffe1":"code","ad2e33ce":"code","5f1e0bb8":"code","0c61cd42":"code","7901069b":"code","43ae8a92":"code","df0890e4":"code","7cc1067b":"code","dcdeb812":"code","0b685120":"code","38e518c2":"code","0d2dbdcc":"code","a963efc7":"code","de36e89b":"markdown","eb340059":"markdown","8c9b390b":"markdown","49605eea":"markdown","a756e507":"markdown","9880267a":"markdown","6b2484be":"markdown","c2c19312":"markdown","b1075c86":"markdown","8ee20855":"markdown"},"source":{"df59545c":"#%cd ..\n#from google.colab import drive\n#drive.mount('\/content\/gdrive')","d296e1ae":"## Creating SymLink\n#!ln -s \/content\/gdrive\/My\\ Drive\/ \/mydrive\n#!ls \/mydrive","dd252058":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport cv2\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('..\/input\/privatecheckout\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","db6fdf1d":"! conda install -y gdown","d93bc7c4":"!gdown --id 1qTvKYEfpLR-4hcFaeT11aB_bpCX3lQUi --folder ","0ea9f627":"# YOLOv3 configs and weights\n#path=\n!cp .\/models\/yolov3-license-plates.cfg ..\/\n!cp .\/models\/yolov3_character.cfg ..\/\n!cp .\/models\/yolov3_plates_final.weights ..\/\n!cp .\/models\/yolov3_chars_final.weights ..\/\n#CNN model\n!cp .\/models\/cnn_flownormalizing.h5 ..\/","6432df18":"# python imports\nimport cv2 as cv\nimport tensorflow as tf\nimport numpy as np","ae23e6e8":"# Making sure to use GPU with CUDA\n!\/usr\/local\/cuda\/bin\/nvcc --version","d64f4419":"# YOLOv3 model build function from weights and config \ndef get_yolo_net(config, weights):\n    yolo_net = cv.dnn.readNetFromDarknet(config, weights)\n    yolo_net.setPreferableBackend(cv.dnn.DNN_BACKEND_OPENCV)\n    yolo_net.setPreferableTarget(cv.dnn.DNN_TARGET_CPU)\n    return yolo_net","f85f9142":"# YOLOv3 and CNN Parameters\nconfidence_threshold = 0.1  # Confidence threshold\nnms_threshold = 0.6  # Non-maximum suppression threshold\n\nyolo_net_width = 416\nyolo_net_height = 416\n\n# Load all models and configs\n\nplates_yolo_config = \".\/models\/yolov3-license-plates.cfg\"\nplates_yolo_weights = \".\/models\/yolov3_plates_final.weights\"\nplates_classes = ['Plate']\nchars_classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a','b','d', 'h', 'j', 'm', 'p', 'waw', 'ww']\n\nchars_yolo_config = \".\/models\/yolov3_character.cfg\"\nchars_yolo_weights = \".\/models\/yolov3_chars_final.weights\"\n\nplates_yolo_net = get_yolo_net(plates_yolo_config, plates_yolo_weights)\nchars_yolo_net = get_yolo_net(chars_yolo_config, chars_yolo_weights)\n","f63d52ca":"cnn_chars_model = tf.keras.models.load_model('.\/models\/cnn_chars_recognition.h5')","ff77d6c7":"from keras.utils.vis_utils import plot_model\ncnn_chars_model.summary()\nplot_model(cnn_chars_model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","5fbd74fa":"def cnn_char_recognition(image):\n    gray_char = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n    gray_char = cv.resize(gray_char, (75, 100))\n    image = gray_char.reshape((1, 100, 75, 1))\n    image = image \/ 255.0\n    predictions = cnn_chars_model.predict(image)\n    max_confidence_index = np.argmax(predictions)\n    k=[]\n    try:\n        k=chars_classes[max_confidence_index]\n    except:\n        pass\n    return k\n\ndef canny(image, sigma=0.33):\n    lower = int(max(0, (1.0 - sigma) * np.median(image)))\n    upper = int(min(255, (1.0 + sigma) * np.median(image)))\n    edges = cv.Canny(image, lower, upper)\n    return edges","b0a1ffe1":"# CNN model - character recognition predictions from the image\ndef cnn_recognize_plate(frame):\n    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n    thresh_inv = cv.adaptiveThreshold(gray, 255, cv.ADAPTIVE_THRESH_MEAN_C, cv.THRESH_BINARY_INV, 39, 1)\n    edges = canny(thresh_inv)\n\n    contours, _ = cv.findContours(edges.copy(), cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n\n    sorted_ctrs = sorted(contours, key=lambda x: cv.boundingRect(x)[0])\n    area = frame.shape[0] * frame.shape[1]\n\n    chars = []\n    license_plate=[]\n    for i, ctr in enumerate(sorted_ctrs):\n        x, y, w, h = cv.boundingRect(ctr)\n        roi_area = w * h\n        non_max_sup = roi_area \/ area\n\n        if (non_max_sup >= 0.015) and (non_max_sup < 0.09):\n            if (h > 1.2 * w) and (3 * w >= h):\n                char = frame[y:y + h, x:x + w]\n                chars.append(cnn_char_recognition(char))\n                cv.rectangle(frame, (x, y), (x + w, y + h), (90, 0, 255), 2)\n    try:\n        license_plate = \"\".join(chars)\n    except:\n        pass\n    return license_plate\n","ad2e33ce":"def process_license_plate(license_plate):\n    gray = cv.cvtColor(license_plate, cv.COLOR_BGR2GRAY)\n    ret, thresh = cv.threshold(gray, 127, 255, 0)\n\n    contours, _ = cv.findContours(thresh, cv.RETR_LIST, cv.CHAIN_APPROX_SIMPLE)\n    areas = [cv.contourArea(c) for c in contours]\n\n    if len(areas) != 0:\n        max_index = np.argmax(areas)\n        cnt = contours[max_index]\n        x, y, w, h = cv.boundingRect(cnt)\n        cv.rectangle(license_plate, (x, y), (x + w, y + h), (0, 255, 0), 2)\n        processed_license_plate = license_plate[y: y + h, x: x + w]\n    else:\n        processed_license_plate = license_plate\n\n    return processed_license_plate","5f1e0bb8":"def draw_pred(frame, name, conf, left, top, right, bottom, color=(0, 255, 0)):\n    cv.rectangle(frame, (left, top), (right, bottom), color, 3)\n    label = name\n    label_size, base_line = cv.getTextSize(label, cv.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n    top = max(top, label_size[1])\n    cv.rectangle(frame, (left, top - round(1.5 * label_size[1])),\n                 (left + round(1.5 * label_size[0]), top + base_line), (0, 0, 255), cv.FILLED)\n    cv.putText(frame, label, (left, top), cv.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 0), 1)\n","0c61cd42":"# For YOLOv3 model prediction\ndef resize_license_plate(license_plate):\n    scale_percent = 300  # percent of original size\n    width = int(license_plate.shape[1] * scale_percent \/ 100)\n    height = int(license_plate.shape[0] * scale_percent \/ 100)\n    return cv.resize(license_plate, (width, height), interpolation=cv.INTER_AREA)\n","7901069b":"# For YOLOv3 model prediction\ndef predict_boxes(frame, yolo_outputs, is_license_plate=True):\n    classes = []\n    confidences = []\n    boxes = []\n\n    max_confidence = 0.0\n    for output in yolo_outputs:\n        for prediction in output:\n            scores = prediction[5:]\n            class_id = np.argmax(scores)\n            confidence = scores[class_id]\n            max_confidence = max(confidence, max_confidence)\n            if confidence > confidence_threshold:\n                center_x = int(prediction[0] * frame.shape[1])\n                center_y = int(prediction[1] * frame.shape[0])\n\n                width = int(prediction[2] * frame.shape[1])\n                height = int(prediction[3] * frame.shape[0])\n                left = int(center_x - width \/ 2)\n                top = int(center_y - height \/ 2)\n\n                classes.append(class_id)\n                confidences.append(float(confidence))\n                boxes.append([left, top, width, height])\n    indices = cv.dnn.NMSBoxes(boxes, confidences, confidence_threshold, nms_threshold)\n\n    positions = []\n    chars = []\n\n    for index in indices:\n        index = index\n        box = boxes[index]\n        left = box[0]\n        top = box[1]\n        width = box[2]\n        height = box[3]\n        positions.append(left)\n\n        if is_license_plate and max_confidence == confidences[index]:\n            # Draw prediction rectangle for License Plate\n            license_plate = frame[top: top + height, left: left + width]\n            cv.rectangle(frame, (left, top), (left + width, top + height), (0, 255, 0), 3)\n\n            # Process Licence plate to cover to Gray and to enhance contours\n            processed_license_plate = process_license_plate(license_plate)\n\n            draw_pred(frame, plates_classes[0], confidences[index], left, top, left + width, top + height)\n            return \"\", processed_license_plate\n        else:\n            try:\n                char = chars_classes[classes[index]]\n                chars.append(char)\n                draw_pred(frame, char, confidences[index], left, top, left + width, top + height, color=(90, 0, 255))\n            except:\n                pass\n    sorted_chars = [x for _, x in sorted(zip(positions, chars))]\n    return \"\".join(sorted_chars), frame","43ae8a92":"import cv2\ndef get_image_blob(image):\n    return cv2.dnn.blobFromImage(image, 1 \/ 255, (yolo_net_width, yolo_net_height), [0, 0, 0], 1, crop=False)","df0890e4":"from PIL import Image\nimport random\nimport matplotlib.pyplot as plt\n\nrandom_images=[random.randint(450, 653) for x in range(12)]\nplt.figure(figsize=(20, 20), dpi=80)\nfor i in range(len(random_images)):\n    plt.subplot(int(len(random_images)\/\/2),int(len(random_images)\/\/2),i+1)\n    #globals()[f\"my_variable_{i}\"]=Image.open(\"..\/input\/moroccoai-data-challenge-edition-001\/test\/{}.jpg\".format(random_images[i]))\n    plt.imshow(Image.open(\"..\/input\/moroccoai-data-challenge-edition-001\/test\/{}.jpg\".format(random_images[i])))\n    plt.title(\"Data set \"+str(random_images[i])+\".jpg\")\n","7cc1067b":"# Our image initially is in RGB format\n# But now we open it in BGR format as function 'cv2.imread' opens it so\nimage_input = cv2.imread('..\/input\/moroccoai-data-challenge-edition-001\/test\/450.jpg')\n\n# Getting image shape\nimage_input_shape = image_input.shape\n\n# Check point\nprint(image_input_shape)  # tuple of (917, 1222, 3)\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (10.0, 10.0)\nplt.imshow(cv2.cvtColor(image_input, cv2.COLOR_BGR2RGB))\nplt.show()\n\n# The 'cv2.dnn.blobFromImage' function returns 4-dimensional blob\n# from input image after mean subtraction, normalizing, and RB channels swapping\n# Resulted shape has number of images, number of channels, width and height\n# E.G.: blob = cv2.dnn.blobFromImage(image, scalefactor=1.0, size, mean, swapRB=True)\n# Link: https:\/\/www.pyimagesearch.com\/2017\/11\/06\/deep-learning-opencvs-blobfromimage-works\/\nblob = cv2.dnn.blobFromImage(image_input, 1 \/ 255.0, (416, 416), swapRB=True, crop=False)\n\n# Check point\nprint(image_input.shape)  # (917, 1222, 3)\nprint(blob.shape)  # (1, 3, 416, 416)","dcdeb812":"#from google.colab import files\nimport matplotlib.pyplot as plt\nimport cv2 as cv\n\ndef upload():\n  uploaded = files.upload() \n  for name, data in uploaded.items():\n    with open(name, 'wb') as f:\n      f.write(data)\n      print ('saved file', name)\n      return name\n\ndef download(path):\n  files.download(path)\n\ndef imShow(path):\n  %matplotlib inline\n  image = cv.imread(path)\n  height, width = image.shape[:2]\n  resized_image = cv.resize(image,(3*width, 3*height), interpolation = cv.INTER_CUBIC)\n  fig = plt.gcf()\n  fig.set_size_inches(18, 10)\n  plt.axis(\"off\")\n  plt.imshow(cv.cvtColor(resized_image, cv.COLOR_BGR2RGB))\n  plt.show()","0b685120":"import cv2 as cv\ndef predict(input_path, \n            output_car_path, \n            output_license_path_original, \n            output_license_path, \n            video_path=None, \n            is_cnn=False, \n            is_image=True):\n   # This function will take as arguement:\n\n#            input_path --> PATH of Image\n#            output_car_path --> Result\n#            output_license_path_original --> Source \n#            output_license_path --> Result of Recognition (number XXXX-\ufe8f-YY or XXXX-\u0634-YY.. as XXXX-b-YY, XXXX-p-YY ....)\n#            video_path=None, \n#            is_cnn=False, \n#            is_image=True\n\n    \n    stream = cv.VideoCapture(input_path)\n    Frame_PS = 2\n    # Limit number of frames per sec\n    stream.set(cv.CAP_PROP_FPS, Frame_PS)\n\n    if not is_image:\n        vid_writer = cv.VideoWriter(video_path, cv.VideoWriter_fourcc('H','2','6','4'), 30, (\n        round(stream.get(cv.CAP_PROP_FRAME_WIDTH)), round(stream.get(cv.CAP_PROP_FRAME_HEIGHT))))\n\n    # Read Frames from the file if video, else read first frame from image\n    while -1<0:#cv.waitKey(1) < 0:\n        exists, frame = stream.read()\n        if not exists:\n            #cv.waitKey(2000)\n            print(\"End of frames\")\n            vid_writer.release()\n            break\n\n        # DETECT LICENSE PLATE\n        car_image_blob = get_image_blob(frame)\n\n        # Feed the input image to the Yolo Network\n        plates_yolo_net.setInput(car_image_blob)\n\n        # Get All Unconnected Yolo layers\n        plates_yolo_layers = [plates_yolo_net.getLayerNames()[i- 1] for i in plates_yolo_net.getUnconnectedOutLayers()]\n\n        # Forward pass the input to yolov3 net and get outputs\n        plates_output = plates_yolo_net.forward(plates_yolo_layers)\n\n        # Remove the bounding boxes with low confidence and draw box for license plate\n        license_num, processed_license_plate = predict_boxes(frame, plates_output)\n        if is_image:\n            cv.imwrite(output_license_path_original, processed_license_plate.astype(np.uint8))\n\n        if not is_cnn:\n            # IDENTIFY LICENSE PLATE NUMBER USING YOLOV3\n\n            # Resize the license plate so we can feed it to the second trained yolo network\n            resized_license_plate = resize_license_plate(processed_license_plate)\n\n            license_plate_image_blob = get_image_blob(resized_license_plate)\n            # license_plate_image_blob = np.reshape(license_plate_image_blob, (1, 3, yolo_net_width,\n            # yolo_net_height))\n\n            # Feed the input image to the Yolo Network\n            chars_yolo_net.setInput(license_plate_image_blob)\n\n            # Get All Unconnected Yolo layers\n            chars_yolo_layers = [chars_yolo_net.getLayerNames()[i- 1] for i in chars_yolo_net.getUnconnectedOutLayers()]\n\n            # Forward pass the input to yolov3 net and get outputs\n            chars_output = chars_yolo_net.forward(chars_yolo_layers)\n\n            license_number, processed_license_plate = predict_boxes(processed_license_plate, chars_output, is_license_plate=False)\n            print(license_number)\n\n        elif is_cnn:\n            # IDENTIFY LICENSE PLATE NUMBER USING CNN\n            license_number = cnn_recognize_plate(processed_license_plate)\n            print(license_number)\n\n        if is_image:\n            cv.imwrite(output_license_path, processed_license_plate.astype(np.uint8))\n            cv.imwrite(output_car_path, frame.astype(np.uint8))\n            return license_number\n            \n        else:\n            vid_writer.write(frame.astype(np.uint8))\n            print(license_number) # Display at the shell resutl of detection & ","38e518c2":"# CNN prediction \nsaved_image=\"580.jpg\"\nlicense = predict('..\/input\/moroccoai-data-challenge-edition-001\/test\/'+ saved_image, output_car_path = '.\/output_cnn.jpg', output_license_path_original = '.\/license_original_cnn.jpg', output_license_path = '.\/license_cnn.jpg', is_cnn=False, is_image=True)\nimShow('output_cnn.jpg')\nimShow('license_cnn.jpg')","0d2dbdcc":"license = predict('..\/input\/moroccoai-data-challenge-edition-001\/test\/' + saved_image, output_car_path = '..\/output_YOLO.jpg', output_license_path_original = '..\/license_original_YOLO.jpg', output_license_path = '..\/license_YOLO.jpg', is_cnn=False, is_image=True)","a963efc7":"import glob\nfrom os import walk\nimport pandas as pd\nfrom tqdm import trange \n\ntest_path_csv='..\/input\/moroccoai-data-challenge-edition-001\/test.csv'\ntest_path_image='..\/input\/moroccoai-data-challenge-edition-001\/test\/'\ndata=pd.read_csv(test_path_csv)\n#data[\"id_image\"]=test[\"id_image\"]\n#data[\"id_image\"]=0\n#data[\"id_string\"]=0\n#for (dirpath, dirnames, filenames) in walk(test_path):\n#    data[\"img_name\"]=filenames\n\n#for (dirpath, dirnames, filenames) in walk(test_path):\n#    data[\"path\"]=dirpath\n   # break\nfor j in trange(len(data)):\n    data[\"plate_string\"][j]=predict(test_path_image+str(data.image_id[j])+\".jpg\", output_car_path = '.\/output_cnn.jpg', output_license_path_original = '.\/license_original_cnn.jpg', output_license_path = '.\/license_cnn.jpg',video_path=None, is_cnn=False,is_image=True)\n\n#submussion=data#.drop(colomns=['img_name','path'])\n#submussion\n#submussion.to_csv('.\/submission1.csv',index=False)","de36e89b":">**Please Note** that the rest of our notebook is available in [github](https:\/\/github.com\/oublalkhalid\/MoroccoAI-Data-Challenge) (implemented under Colab Pro) here we some difficulties mainly related to the non-compatibility of cuda (dnn) with open-cv we have tried several times to install vesrsion quote in nvidia developper but it does not work! \n","eb340059":"### **You can run this notebook on Colab**","8c9b390b":"### **Copy weights, configs, and models to Colab VM**\n","49605eea":"# \u27bf Full depth Yolo+Flow Normalizing for license plate characters recognition of Moroccan vehicles    ","a756e507":"# **Image Utils**","9880267a":"\n\n# **Functions to predict and drawings on Images**","6b2484be":"# **Predicting Image using Flow Normalizing and YOLOv3**","c2c19312":"\n![https:\/\/raw.githubusercontent.com\/oublalkhalid\/MoroccoAI-Data-Challenge\/main\/images\/workflow.png](https:\/\/raw.githubusercontent.com\/oublalkhalid\/MoroccoAI-Data-Challenge\/main\/images\/workflow.png)\nThis repository guides you through the steps for annotating and training a custom model to detect and blur the license plates on Morrocan licence plates.[Please consult our Pipline and the associated paper in the repository. ](http:\/\/github.com\/oublalkhalid\/MoroccoAI-Data-Challenge)\nWe also mention that more than 3go of data are generated to increase the training data of our Yolo (We also have the weight of yolo tiyni which gives a score of 0.59 on the dataset of this challenge).\n\n> Please follow the associated article for more details - (Please check our paper after submission deadline).\n\n\n### Step 1: Prepare the dataset\n\nWe get from Morroco-IA more than 18900 images of Morrocain licences with the number plate. \n\n1. To strat we create 2 folders 'test' and 'train' and transfer 20% of the images in test and 80% images in train folders respectively.\n\n2. Alternatively, you can use the test and train data used by me for training. Download test.zip and train.zip files from our Github Repository for accessing the annotated images.\n\n3. Annotate the license plates in all the images in the respective test and train folders, using LabelImg, and generate the annotated .xml file for the images within the respective folders. You can get more details on LabelImg from [here\n](https:\/\/github.com\/tzutalin\/labelImg).(Conversion process is mentioned in step 3)\n","b1075c86":"# YOLOv3 & Flow Normalizing prediction","8ee20855":"Our goal is to detect the number lisense  **XXXX-\ufe8f-YY** or **XXXX-\u0634-YY**.. as **XXXX-b-YY**, **XXXX-p-YY** ....; To do this, we will combine all the deep learning framwork that we have made on part 0,1,2 and 3.\n"}}