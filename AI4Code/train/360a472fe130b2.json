{"cell_type":{"cdb93bff":"code","79357390":"code","74f75160":"code","14ae0714":"code","748174a8":"code","cd1019ef":"code","f78ae12b":"code","4558a71e":"markdown","9fa76131":"markdown","b2db199d":"markdown","bd62cde4":"markdown","53f3aa04":"markdown"},"source":{"cdb93bff":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","79357390":"from sklearn import preprocessing\n# Get dataset\ndf = pd.read_csv(\"\/kaggle\/input\/california-housing-train\/california_housing_train.csv\", sep=\",\")\ndf","74f75160":"# Normalize total_bedrooms column\nx_array = np.array(df['total_bedrooms'])\nx_array","14ae0714":"normalized_X = preprocessing.normalize([x_array])\nnormalized_X","748174a8":"# Get column names first\nnames = df.columns\n# Create the Scaler object\nscaler = preprocessing.StandardScaler()\n# Fit your data on the scaler object\nscaled_df = scaler.fit_transform(df)\nscaled_df","cd1019ef":"scaled_df = pd.DataFrame(scaled_df, columns=names)\nscaled_df","f78ae12b":"# Create range for your new columns\nlat_range = zip(range(32, 44), range(33, 45))\nnew_df = pd.DataFrame()\n# Iterate and create new columns, with the 0 and 1 encoding\nfor r in lat_range:\n        new_df[\"latitude_%d_to_%d\" % r] = df[\"latitude\"].apply(\n            lambda l: 1.0 if l >= r[0] and l < r[1] else 0.0)\nnew_df","4558a71e":"***Pros:***\n\n* It is a good technique to use when you do not know the distribution of your data or when you know the distribution is not Gaussian.\n \n* Normalization is useful when your data has varying scales and the algorithm you are using does not make assumptions about the distribution of your data, such as k-nearest neighbors and artificial neural networks.\n\n***Cons:***\n\n* Normalizing the data is sensitive to outliers, so if there are outliers in the data set it is a bad practice. \n\n* Standardization creates a new data not bounded (unlike normalization).","9fa76131":"# Noramalisation","b2db199d":"# Scaling","bd62cde4":"# Normalisation:\n\n   Normalization is a technique often applied as part of data preparation for machine learning. The goal of normalization is to change the values of numeric columns in the dataset to use a common scale, without distorting differences in the ranges of values or losing information.\n   \n![](https:\/\/imgur.com\/FwLGJQw.png)   ","53f3aa04":"# References:\n\n1. https:\/\/medium.com\/@rrfd\/standardize-or-normalize-examples-in-python-e3f174b65dfc\n \n2. https:\/\/towardsdatascience.com\/understand-data-normalization-in-machine-learning-8ff3062101f0\n \n3. https:\/\/www.analyticsvidhya.com\/blog\/2020\/04\/feature-scaling-machine-learning-normalization-standardization\/"}}