{"cell_type":{"8bb54545":"code","8dd34513":"code","025f542d":"code","99433c2d":"code","f9893a39":"code","7ddbe3f6":"code","793b5926":"code","92bceb9e":"code","4678e742":"code","157abe2e":"code","8ad038f9":"code","25e95537":"code","22b5aabb":"code","57a71325":"code","888c08bf":"code","848c0e34":"code","ae234742":"code","d54692d8":"code","6e3b31e3":"code","e715ba11":"code","046ac2cc":"code","9a84b1d5":"code","3e1747b7":"code","9b6feb23":"code","bc69a622":"code","882174fc":"code","d7575bea":"code","dea231df":"code","19a4dbb1":"code","84a16ace":"code","e41811f3":"code","60873ae2":"code","8356dc32":"code","a1e19bef":"code","09e4ce2d":"code","8680e2f1":"code","a2351b68":"code","20164047":"code","13491f45":"code","ccf08fed":"code","94ff8a9d":"code","4f3b3f18":"code","9d6f8a5e":"code","0164c562":"code","c3ae1f46":"code","92dc5946":"code","b6c67481":"code","62f5610d":"code","14cd3188":"markdown","5c2c05d9":"markdown","6c0e6748":"markdown","ecd045d2":"markdown","a7428528":"markdown","4507af20":"markdown","b3c14d66":"markdown","1f843b1f":"markdown","da11ed3a":"markdown","3119dfc7":"markdown","da235ad1":"markdown","91883b15":"markdown","ad393e2f":"markdown","d524a065":"markdown","32651cfa":"markdown","d29421f3":"markdown","4eb0fac5":"markdown","b1f70629":"markdown","972ca6dd":"markdown","f62c060d":"markdown","864991ed":"markdown"},"source":{"8bb54545":"import os\nimport cv2\nimport glob\nimport PIL\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom skimage import data\nfrom skimage.util import montage \nimport skimage.transform as skTrans\nfrom skimage.transform import rotate\nfrom skimage.transform import resize\nfrom PIL import Image, ImageOps  \n\n\n# neural imaging\nimport nilearn as nl\nimport nibabel as nib\nimport nilearn.plotting as nlplt\n!pip install git+https:\/\/github.com\/miykael\/gif_your_nifti # nifti to gif \nimport gif_your_nifti.core as gif2nif\n\n\n# ml libs\nimport keras\nimport keras.backend as K\nfrom keras.callbacks import CSVLogger\nimport tensorflow as tf\nfrom tensorflow.keras.utils import plot_model\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard\nfrom tensorflow.keras.layers.experimental import preprocessing\n\n\n# Make numpy printouts easier to read.\nnp.set_printoptions(precision=3, suppress=True)","8dd34513":"!pip3 install -U segmentation-models\n%env SM_FRAMEWORK=tf.keras\nimport segmentation_models as sm\nimport tensorflow as tf\ntf.keras.backend.set_image_data_format('channels_last')\n\n#Code m\u1edbi\nimport sys\nimport os\nimport glob\nimport random\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nimport cv2\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nfrom sklearn.model_selection import train_test_split\nfrom segmentation_models import Unet, Linknet, PSPNet, FPN\nimport keras\nfrom segmentation_models.utils import set_trainable\nfrom torch.utils.data import Dataset\nfrom keras.models import load_model\nfrom tensorflow.keras import utils as np_utils","025f542d":"# DEFINE seg-areas  \nSEGMENT_CLASSES = {\n    0 : 'Not Tumor',\n    1 : 'Necrotic\/ Core', # or NON-ENHANCING tumor CORE\n    2 : 'Edema',\n    3 : 'Enhancing' # original 4 -> converted into 3 later\n}\n\n# there are 155 slices per volume\n# to start at 5 and use 145 slices means we will skip the first 5 and last 5 \nVOLUME_SLICES = 100\nVOLUME_START_AT = 22 # first slice of volume that we will include","99433c2d":"TRAIN_DATASET_PATH = '..\/input\/brats20-dataset-training-validation\/BraTS2020_TrainingData\/MICCAI_BraTS2020_TrainingData\/'\nVALIDATION_DATASET_PATH =  '..\/input\/brats20-dataset-training-validation\/BraTS2020_ValidationData\/MICCAI_BraTS2020_ValidationData'\n\ntest_image_flair=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_002\/BraTS20_Training_002_flair.nii').get_fdata()\ntest_image_t1=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_002\/BraTS20_Training_002_t1.nii').get_fdata()\ntest_image_t1ce=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_002\/BraTS20_Training_002_t1ce.nii').get_fdata()\ntest_image_t2=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_002\/BraTS20_Training_002_t2.nii').get_fdata()\ntest_mask=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_002\/BraTS20_Training_002_seg.nii').get_fdata()\n\n\nfig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1,5, figsize = (20, 10))\nslice_w = 25\nax1.imshow(test_image_flair[:,:,test_image_flair.shape[0]\/\/2-slice_w], cmap = 'gray')\nax1.set_title('Image TFlair')\nax2.imshow(test_image_t1[:,:,test_image_t1.shape[0]\/\/2-slice_w], cmap = 'gray')\nax2.set_title('Image T1')\nax3.imshow(test_image_t1ce[:,:,test_image_t1ce.shape[0]\/\/2-slice_w], cmap = 'gray')\nax3.set_title('Image T1CE')\nax4.imshow(test_image_t2[:,:,test_image_t2.shape[0]\/\/2-slice_w], cmap = 'gray')\nax4.set_title('Image T2')\nax5.imshow(test_mask[:,:,test_mask.shape[0]\/\/2-slice_w])\nax5.set_title('Mask')\n","f9893a39":"# Skip 50:-50 slices since there is not much to see\nfig, ax1 = plt.subplots(1, 1, figsize = (15,15))\nax1.imshow(rotate(montage(test_image_t1[50:-50,:,:]), 90, resize=True), cmap ='gray')","7ddbe3f6":"# Skip 50:-50 slices since there is not much to see\nfig, ax1 = plt.subplots(1, 1, figsize = (15,15))\nax1.imshow(rotate(montage(test_mask[60:-60,:,:]), 90, resize=True), cmap ='gray')","793b5926":"shutil.copy2(TRAIN_DATASET_PATH + 'BraTS20_Training_001\/BraTS20_Training_001_flair.nii', '.\/test_gif_BraTS20_Training_001_flair.nii')\ngif2nif.write_gif_normal('.\/test_gif_BraTS20_Training_001_flair.nii')","92bceb9e":"niimg = nl.image.load_img(TRAIN_DATASET_PATH + 'BraTS20_Training_013\/BraTS20_Training_013_flair.nii')\nnimask = nl.image.load_img(TRAIN_DATASET_PATH + 'BraTS20_Training_013\/BraTS20_Training_013_seg.nii')\n\nfig, axes = plt.subplots(nrows=4, figsize=(30, 40))\n\n\nnlplt.plot_anat(niimg,\n                title='BraTS20_Training_013_flair.nii plot_anat',\n                axes=axes[0])\n\nnlplt.plot_epi(niimg,\n               title='BraTS20_Training_013_flair.nii plot_epi',\n               axes=axes[1])\n\nnlplt.plot_img(niimg,\n               title='BraTS20_Training_013_flair.nii plot_img',\n               axes=axes[2])\n\nnlplt.plot_roi(nimask, \n               title='BraTS20_Training_013_flair.nii with mask plot_roi',\n               bg_img=niimg, \n               axes=axes[3], cmap='Paired')\n\nplt.show()","4678e742":"# dice loss as defined above for 4 classes\ndef dice_coef(y_true, y_pred, smooth=1.0):\n    class_num = 4\n    for i in range(class_num):\n        y_true_f = K.flatten(y_true[:,:,:,i])\n        y_pred_f = K.flatten(y_pred[:,:,:,i])\n        intersection = K.sum(y_true_f * y_pred_f)\n        loss = ((2. * intersection + smooth) \/ (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))\n   #     K.print_tensor(loss, message='loss value for class {} : '.format(SEGMENT_CLASSES[i]))\n        if i == 0:\n            total_loss = loss\n        else:\n            total_loss = total_loss + loss\n    total_loss = total_loss \/ class_num\n#    K.print_tensor(total_loss, message=' total dice coef: ')\n    return total_loss\n\n\n \n# define per class evaluation of dice coef\n# inspired by https:\/\/github.com\/keras-team\/keras\/issues\/9395\ndef dice_coef_necrotic(y_true, y_pred, epsilon=1e-6):\n    intersection = K.sum(K.abs(y_true[:,:,:,1] * y_pred[:,:,:,1]))\n    return (2. * intersection) \/ (K.sum(K.square(y_true[:,:,:,1])) + K.sum(K.square(y_pred[:,:,:,1])) + epsilon)\n\ndef dice_coef_edema(y_true, y_pred, epsilon=1e-6):\n    intersection = K.sum(K.abs(y_true[:,:,:,2] * y_pred[:,:,:,2]))\n    return (2. * intersection) \/ (K.sum(K.square(y_true[:,:,:,2])) + K.sum(K.square(y_pred[:,:,:,2])) + epsilon)\n\ndef dice_coef_enhancing(y_true, y_pred, epsilon=1e-6):\n    intersection = K.sum(K.abs(y_true[:,:,:,3] * y_pred[:,:,:,3]))\n    return (2. * intersection) \/ (K.sum(K.square(y_true[:,:,:,3])) + K.sum(K.square(y_pred[:,:,:,3])) + epsilon)\n\n\n\n# Computing Precision \ndef precision(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n\n    \n# Computing Sensitivity      \ndef sensitivity(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    return true_positives \/ (possible_positives + K.epsilon())\n\n\n# Computing Specificity\ndef specificity(y_true, y_pred):\n    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n    return true_negatives \/ (possible_negatives + K.epsilon())","157abe2e":"IMG_SIZE=128","8ad038f9":"# source https:\/\/naomi-fridman.medium.com\/multi-class-image-segmentation-a5cc671e647a\n\ndef build_unet(inputs, ker_init, dropout):\n    conv1 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(inputs)\n    conv1 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv1)\n    \n    pool = MaxPooling2D(pool_size=(2, 2))(conv1)\n    conv = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool)\n    conv = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv)\n    \n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv)\n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool1)\n    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv2)\n    \n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool2)\n    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv3)\n    \n    \n    pool4 = MaxPooling2D(pool_size=(2, 2))(conv3)\n    conv5 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool4)\n    conv5 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv5)\n    drop5 = Dropout(dropout)(conv5)\n\n    #T\u1eeb \u0111\u00e2y xu\u1ed1ng l\u00e0 decoder FNC\n    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(drop5))\n    merge7 = concatenate([conv3,up7], axis = 3)\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge7)\n    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv7)\n\n    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(conv7))\n    merge8 = concatenate([conv2,up8], axis = 3)\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge8)\n    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv8)\n\n    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(conv8))\n    merge9 = concatenate([conv,up9], axis = 3)\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge9)\n    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv9)\n    \n    up = Conv2D(32, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(conv9))\n    merge = concatenate([conv1,up], axis = 3)\n    conv = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge)\n    conv = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv)\n    \n    conv10 = Conv2D(4, (1,1), activation = 'softmax')(conv)\n    \n    return Model(inputs = inputs, outputs = conv10)\n\ninput_layer = Input((IMG_SIZE, IMG_SIZE, 2))\n\nmodel_unet_VGG16 = build_unet(input_layer, 'he_normal', 0.2)\nmodel_unet_VGG16.compile(loss=\"categorical_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=0.0001), metrics = ['accuracy',tf.keras.metrics.MeanIoU(num_classes=4), dice_coef, precision, sensitivity, specificity, dice_coef_necrotic, dice_coef_edema ,dice_coef_enhancing] )","25e95537":"model_unet_VGG16.summary()","22b5aabb":"model_unet_resnet.summary()","57a71325":"plot_model(model_unet_VGG16, \n           show_shapes = True,\n           show_dtype=False,\n           show_layer_names = True, \n           rankdir = 'TB', \n           expand_nested = False, \n           dpi = 70)","888c08bf":"# lists of directories with studies\ntrain_and_val_directories = [f.path for f in os.scandir(TRAIN_DATASET_PATH) if f.is_dir()]\n\n# file BraTS20_Training_355 has ill formatted name for for seg.nii file\ntrain_and_val_directories.remove(TRAIN_DATASET_PATH+'BraTS20_Training_355')\n\n\ndef pathListIntoIds(dirList):\n    x = []\n    for i in range(0,len(dirList)):\n        x.append(dirList[i][dirList[i].rfind('\/')+1:])\n    return x\n\ntrain_and_test_ids = pathListIntoIds(train_and_val_directories); \n\n    \ntrain_test_ids, val_ids = train_test_split(train_and_test_ids,test_size=0.2) \ntrain_ids, test_ids = train_test_split(train_test_ids,test_size=0.15) \n\n#L\u1ea5y 20% cho val_ids, train_test_isd = 80%\n#test_ids: 15%, train_isd: 85 (313 t\u1ea5m)\n","848c0e34":"class DataGenerator(keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, list_IDs, dim=(IMG_SIZE,IMG_SIZE), batch_size = 1, n_channels = 2, shuffle=True):\n        'Initialization'\n        self.dim = dim\n        self.batch_size = batch_size\n        self.list_IDs = list_IDs\n        self.n_channels = n_channels\n        self.shuffle = shuffle\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) \/ self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        Batch_ids = [self.list_IDs[k] for k in indexes]\n\n        # Generate data\n        X, y = self.__data_generation(Batch_ids)\n\n        return X, y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, Batch_ids):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, self.n_channels))\n        y = np.zeros((self.batch_size*VOLUME_SLICES, 240, 240))\n        Y = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, 4))\n\n        \n        # Generate data\n        for c, i in enumerate(Batch_ids):\n            case_path = os.path.join(TRAIN_DATASET_PATH, i)\n\n            data_path = os.path.join(case_path, f'{i}_flair.nii');\n            flair = nib.load(data_path).get_fdata()    \n\n            data_path = os.path.join(case_path, f'{i}_t1ce.nii');\n            ce = nib.load(data_path).get_fdata()\n            \n            data_path = os.path.join(case_path, f'{i}_seg.nii');\n            seg = nib.load(data_path).get_fdata()\n  \n\n        \n            for j in range(VOLUME_SLICES):\n                 X[j +VOLUME_SLICES*c,:,:,0] = cv2.resize(flair[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n                 X[j +VOLUME_SLICES*c,:,:,1] = cv2.resize(ce[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n\n                 y[j +VOLUME_SLICES*c] = seg[:,:,j+VOLUME_START_AT];\n                    \n        # Generate masks\n        y[y==4] = 3;\n        mask = tf.one_hot(y, 4);\n        Y = tf.image.resize(mask, (IMG_SIZE, IMG_SIZE));\n        return X\/np.max(X), Y\n        \ntraining_generator = DataGenerator(train_ids)\nvalid_generator = DataGenerator(val_ids)\ntest_generator = DataGenerator(test_ids)","ae234742":"# show number of data for each dir \ndef showDataLayout():\n    plt.bar([\"Train\",\"Valid\",\"Test\"],\n    [len(train_ids), len(val_ids), len(test_ids)], align='center',color=[ 'green','red', 'blue'])\n    plt.legend()\n\n    plt.ylabel('Number of images')\n    plt.title('Data distribution')\n\n    plt.show()\n    \nshowDataLayout()","d54692d8":"#VGG16: 9:32 am ch\u1ea1y Unet v\u1edbi LR: 0.0001, 35EP \ncsv_logger = CSVLogger('training_VGG16_0_doc_0001.log', separator=',', append=False)\n\n\ncallbacks = [\n#     keras.callbacks.EarlyStopping(monitor='loss', min_delta=0,\n#                               patience=2, verbose=1, mode='auto'),\n      keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=2, min_lr=0.000001, verbose=1),\n#  keras.callbacks.ModelCheckpoint(filepath = 'model_.{epoch:02d}-{val_loss:.6f}.m5',\n#                             verbose=1, save_best_only=True, save_weights_only = True)\n        csv_logger\n    ]","6e3b31e3":"#RESNET:  3:02 pm ch\u1ea1y Resnet v\u1edbi LR: 0.0001, 35EP \ncsv_logger = CSVLogger('training_ResNet_0_doc_0001.log', separator=',', append=False)\n\n\ncallbacks = [\n#     keras.callbacks.EarlyStopping(monitor='loss', min_delta=0,\n#                               patience=2, verbose=1, mode='auto'),\n      keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=2, min_lr=0.000001, verbose=1),\n#  keras.callbacks.ModelCheckpoint(filepath = 'model_.{epoch:02d}-{val_loss:.6f}.m5',\n#                             verbose=1, save_best_only=True, save_weights_only = True)\n        csv_logger\n    ]","e715ba11":"#VGG16: 9:32 am ch\u1ea1y Unet v\u1edbi LR: 0.0001, 35EP \nK.clear_session()\n#history =  model_unet_VGG16.fit(training_generator,\n#                    epochs=35,\n#                    steps_per_epoch=len(train_ids),\n#                    callbacks= callbacks,\n#                    validation_data = valid_generator\n#                    )  \n#model_unet_VGG16.save(\"VGG16_backbone_0_doc_0001.h5\")","046ac2cc":"#Resnet: 3:02 pm ch\u1ea1y Resnet v\u1edbi LR: 0.0001, 35EP \nK.clear_session()\n#history =  model_unet_resnet.fit(training_generator,\n#                    epochs=35,\n#                    steps_per_epoch=len(train_ids),\n#                    callbacks= callbacks,\n#                    validation_data = valid_generator\n#                    )  \n#model_unet_resnet.save(\"Resnet_backbone_0_doc_0001.h5\")","9a84b1d5":"plt.plot(history.history['loss'])\nplt.ylabel('loss')\nplt.xlabel('Epoch')","3e1747b7":"plt.plot(history.history['accuracy'])\nplt.ylabel('accuracy')\nplt.xlabel('Epoch')","9b6feb23":"plt.plot(history.history['mean_io_u_1']) #VGG16 th\u00ec k c\u00f3 _1\nplt.ylabel('mean_io_u_1')\nplt.xlabel('Epoch')","bc69a622":"plt.plot(history.history['dice_coef'])\nplt.ylabel('dice_coef')\nplt.xlabel('Epoch')","882174fc":"plt.plot(history.history['precision'])\nplt.ylabel('precision')\nplt.xlabel('Epoch')","d7575bea":"plt.plot(history.history['sensitivity'])\nplt.ylabel('sensitivity')\nplt.xlabel('Epoch')","dea231df":"plt.plot(history.history['specificity'])\nplt.ylabel('specificity')\nplt.xlabel('Epoch')","19a4dbb1":"plt.plot(history.history['dice_coef_necrotic'])\nplt.ylabel('dice_coef_necrotic')\nplt.xlabel('Epoch')","84a16ace":"plt.plot(history.history['dice_coef_edema'])\nplt.ylabel('dice_coef_edema')\nplt.xlabel('Epoch')","e41811f3":"plt.plot(history.history['dice_coef_enhancing'])\nplt.ylabel('dice_coef_enhancing')\nplt.xlabel('Epoch')","60873ae2":"plt.plot(history.history['val_loss'])\nplt.ylabel('val_loss')\nplt.xlabel('Epoch')","8356dc32":"plt.plot(history.history['val_accuracy'])\nplt.ylabel('val_accuracy')\nplt.xlabel('Epoch')","a1e19bef":"plt.plot(history.history['val_mean_io_u_1']) #VGG16 th\u00ec k c\u00f3 _1\nplt.ylabel('val_mean_io_u_1')\nplt.xlabel('Epoch')","09e4ce2d":"plt.plot(history.history['val_dice_coef'])\nplt.ylabel(' val_dice_coef')\nplt.xlabel('Epoch')","8680e2f1":"plt.plot(history.history['val_precision'])\nplt.ylabel(' val_precision')\nplt.xlabel('Epoch')","a2351b68":"plt.plot(history.history['val_sensitivity'])\nplt.ylabel(' val_sensitivity')\nplt.xlabel('Epoch')","20164047":"plt.plot(history.history['val_specificity'])\nplt.ylabel(' val_specificity')\nplt.xlabel('Epoch')","13491f45":"plt.plot(history.history['val_dice_coef_necrotic'])\nplt.ylabel(' val_dice_coef_necrotic')\nplt.xlabel('Epoch')","ccf08fed":"plt.plot(history.history['val_dice_coef_edema'])\nplt.ylabel(' val_dice_coef_edema')\nplt.xlabel('Epoch')","94ff8a9d":"plt.plot(history.history['val_dice_coef_enhancing'])\nplt.ylabel(' val_dice_coef_enhancing')\nplt.xlabel('Epoch')","4f3b3f18":"#Unet: 9:32 am ch\u1ea1y Unet v\u1edbi LR: 0.0001, 35EP \n############ load trained model ################\nmodel = keras.models.load_model('..\/input\/35eplr0-doc-001\/VGG16_backbone_0.001.h5', \n                                   custom_objects={ 'accuracy' : tf.keras.metrics.MeanIoU(num_classes=4),\n                                                   \"dice_coef\": dice_coef,\n                                                   \"precision\": precision,\n                                                   \"sensitivity\":sensitivity,\n                                                   \"specificity\":specificity,\n                                                   \"dice_coef_necrotic\": dice_coef_necrotic,\n                                                   \"dice_coef_edema\": dice_coef_edema,\n                                                   \"dice_coef_enhancing\": dice_coef_enhancing\n                                                  }, compile=False)\n\nhistory_unet_VGG16 = pd.read_csv('..\/input\/train-log\/training_VGG16.log', sep=',', engine='python')\nhist=history_unet_VGG16\n\n############### ########## ####### #######\n\n# hist=history.history\n\nacc=hist['accuracy']\nval_acc=hist['val_accuracy']\n\nepoch=range(len(acc))\n\nloss=hist['loss']\nval_loss=hist['val_loss']\n\ntrain_dice=hist['dice_coef']\nval_dice=hist['val_dice_coef']\n\nf,ax=plt.subplots(1,4,figsize=(16,8))\n\nax[0].plot(epoch,acc,'b',label='Training Accuracy')\nax[0].plot(epoch,val_acc,'r',label='Validation Accuracy')\nax[0].legend()\n\nax[1].plot(epoch,loss,'b',label='Training Loss')\nax[1].plot(epoch,val_loss,'r',label='Validation Loss')\nax[1].legend()\n\nax[2].plot(epoch,train_dice,'b',label='Training dice coef')\nax[2].plot(epoch,val_dice,'r',label='Validation dice coef')\nax[2].legend()\n\nax[3].plot(epoch,hist['mean_io_u'],'b',label='Training mean IOU')\nax[3].plot(epoch,hist['val_mean_io_u'],'r',label='Validation mean IOU')\nax[3].legend()\n\nplt.show()","9d6f8a5e":"#RESNET: 3:02 pm, LR: 0.0001. 35EP\n############ load trained model ################\nmodel = keras.models.load_model('..\/input\/35eplr0-doc-001\/VGG16_backbone_0.001.h5', \n                                   custom_objects={ 'accuracy' : tf.keras.metrics.MeanIoU(num_classes=4),\n                                                   \"dice_coef\": dice_coef,\n                                                   \"precision\": precision,\n                                                   \"sensitivity\":sensitivity,\n                                                   \"specificity\":specificity,\n                                                   \"dice_coef_necrotic\": dice_coef_necrotic,\n                                                   \"dice_coef_edema\": dice_coef_edema,\n                                                   \"dice_coef_enhancing\": dice_coef_enhancing\n                                                  }, compile=False)\n\nhistory_unet_resnet = pd.read_csv('..\/input\/train-log\/training_ResNet.log', sep=',', engine='python')\nhist=history_unet_resnet\n\n############### ########## ####### #######\n\n# hist=history.history\n\nacc=hist['accuracy']\nval_acc=hist['val_accuracy']\n\nepoch=range(len(acc))\n\nloss=hist['loss']\nval_loss=hist['val_loss']\n\ntrain_dice=hist['dice_coef']\nval_dice=hist['val_dice_coef']\n\nf,ax=plt.subplots(1,4,figsize=(16,8))\n\nax[0].plot(epoch,acc,'b',label='Training Accuracy')\nax[0].plot(epoch,val_acc,'r',label='Validation Accuracy')\nax[0].legend()\n\nax[1].plot(epoch,loss,'b',label='Training Loss')\nax[1].plot(epoch,val_loss,'r',label='Validation Loss')\nax[1].legend()\n\nax[2].plot(epoch,train_dice,'b',label='Training dice coef')\nax[2].plot(epoch,val_dice,'r',label='Validation dice coef')\nax[2].legend()\n\nax[3].plot(epoch,hist['mean_io_u_1'],'b',label='Training mean IOU')\nax[3].plot(epoch,hist['val_mean_io_u_1'],'r',label='Validation mean IOU')\nax[3].legend()\n\nplt.show()","0164c562":"# mri type must one of 1) flair 2) t1 3) t1ce 4) t2 ------- or even 5) seg\n# returns volume of specified study at `path`\ndef imageLoader(path):\n    image = nib.load(path).get_fdata()\n    X = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, self.n_channels))\n    for j in range(VOLUME_SLICES):\n        X[j +VOLUME_SLICES*c,:,:,0] = cv2.resize(image[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n        X[j +VOLUME_SLICES*c,:,:,1] = cv2.resize(ce[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n\n        y[j +VOLUME_SLICES*c] = seg[:,:,j+VOLUME_START_AT];\n    return np.array(image)\n\n\n# load nifti file at `path`\n# and load each slice with mask from volume\n# choose the mri type & resize to `IMG_SIZE`\ndef loadDataFromDir(path, list_of_files, mriType, n_images):\n    scans = []\n    masks = []\n    for i in list_of_files[:n_images]:\n        fullPath = glob.glob( i + '\/*'+ mriType +'*')[0]\n        currentScanVolume = imageLoader(fullPath)\n        currentMaskVolume = imageLoader( glob.glob( i + '\/*seg*')[0] ) \n        # for each slice in 3D volume, find also it's mask\n        for j in range(0, currentScanVolume.shape[2]):\n            scan_img = cv2.resize(currentScanVolume[:,:,j], dsize=(IMG_SIZE,IMG_SIZE), interpolation=cv2.INTER_AREA).astype('uint8')\n            mask_img = cv2.resize(currentMaskVolume[:,:,j], dsize=(IMG_SIZE,IMG_SIZE), interpolation=cv2.INTER_AREA).astype('uint8')\n            scans.append(scan_img[..., np.newaxis])\n            masks.append(mask_img[..., np.newaxis])\n    return np.array(scans, dtype='float32'), np.array(masks, dtype='float32')\n        \n#brains_list_test, masks_list_test = loadDataFromDir(VALIDATION_DATASET_PATH, test_directories, \"flair\", 5)\n","c3ae1f46":"def predictByPath(case_path,case):\n    files = next(os.walk(case_path))[2]\n    X = np.empty((VOLUME_SLICES, IMG_SIZE, IMG_SIZE, 2))\n  #  y = np.empty((VOLUME_SLICES, IMG_SIZE, IMG_SIZE))\n    \n    vol_path = os.path.join(case_path, f'BraTS20_Training_{case}_flair.nii');\n    flair=nib.load(vol_path).get_fdata()\n    \n    vol_path = os.path.join(case_path, f'BraTS20_Training_{case}_t1ce.nii');\n    ce=nib.load(vol_path).get_fdata() \n    \n #   vol_path = os.path.join(case_path, f'BraTS20_Training_{case}_seg.nii');\n #   seg=nib.load(vol_path).get_fdata()  \n\n    \n    for j in range(VOLUME_SLICES):\n        X[j,:,:,0] = cv2.resize(flair[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n        X[j,:,:,1] = cv2.resize(ce[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n #       y[j,:,:] = cv2.resize(seg[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n        \n  #  model.evaluate(x=X,y=y[:,:,:,0], callbacks= callbacks)\n    return model.predict(X\/np.max(X), verbose=1)\n\n\ndef showPredictsById(case, start_slice = 60):\n    path = f\"..\/input\/brats20-dataset-training-validation\/BraTS2020_TrainingData\/MICCAI_BraTS2020_TrainingData\/BraTS20_Training_{case}\"\n    gt = nib.load(os.path.join(path, f'BraTS20_Training_{case}_seg.nii')).get_fdata()\n    origImage = nib.load(os.path.join(path, f'BraTS20_Training_{case}_flair.nii')).get_fdata()\n    p = predictByPath(path,case)\n\n    core = p[:,:,:,1]\n    edema= p[:,:,:,2]\n    enhancing = p[:,:,:,3]\n\n    plt.figure(figsize=(18, 50))\n    f, axarr = plt.subplots(1,6, figsize = (18, 50)) \n\n    for i in range(6): # for each image, add brain background\n        axarr[i].imshow(cv2.resize(origImage[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)), cmap=\"gray\", interpolation='none')\n    \n    axarr[0].imshow(cv2.resize(origImage[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)), cmap=\"gray\")\n    axarr[0].title.set_text('Original image Flair')\n    curr_gt=cv2.resize(gt[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE), interpolation = cv2.INTER_NEAREST)\n    axarr[1].imshow(curr_gt, cmap=\"Reds\", interpolation='none', alpha=0.3) # ,alpha=0.3,cmap='Reds'\n    axarr[1].title.set_text('Ground truth')\n    axarr[2].imshow(p[start_slice,:,:,1:4], cmap=\"Reds\", interpolation='none', alpha=0.3)\n    axarr[2].title.set_text('All classes')\n    axarr[3].imshow(edema[start_slice,:,:], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n    axarr[3].title.set_text(f'{SEGMENT_CLASSES[1]} Predicted')\n    axarr[4].imshow(core[start_slice,:,], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n    axarr[4].title.set_text(f'{SEGMENT_CLASSES[2]} Predicted')\n    axarr[5].imshow(enhancing[start_slice,:,], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n    axarr[5].title.set_text(f'{SEGMENT_CLASSES[3]} Predicted')\n    plt.show()\n    \n    \nshowPredictsById(case=test_ids[0][-3:])\nshowPredictsById(case=test_ids[1][-3:])\nshowPredictsById(case=test_ids[2][-3:])\nshowPredictsById(case=test_ids[3][-3:])\nshowPredictsById(case=test_ids[4][-3:])\nshowPredictsById(case=test_ids[5][-3:])\nshowPredictsById(case=test_ids[6][-3:])\n\n\n#mask = np.zeros((10,10))\n#mask[3:-3, 3:-3] = 1 # white square in black background\n#im = mask + np.random.randn(10,10) * 0.01 # random image\n#masked = np.ma.masked_where(mask == 0, mask)\n\n#plt.figure()\n#plt.subplot(1,2,1)\n#plt.imshow(im, 'gray', interpolation='none')\n#plt.subplot(1,2,2)\n#plt.imshow(im, 'gray', interpolation='none')\n#plt.imshow(masked, 'jet', interpolation='none', alpha=0.7)\n#plt.show()","92dc5946":"case = case=test_ids[6][-3:]\npath = f\"..\/input\/brats20-dataset-training-validation\/BraTS2020_TrainingData\/MICCAI_BraTS2020_TrainingData\/BraTS20_Training_{case}\"\ngt = nib.load(os.path.join(path, f'BraTS20_Training_{case}_seg.nii')).get_fdata()\np = predictByPath(path,case)\n\n\ncore = p[:,:,:,1]\nedema= p[:,:,:,2]\nenhancing = p[:,:,:,3]\n\n\ni=40 # slice at\neval_class = 2 #     0 : 'NOT tumor',  1 : 'ENHANCING',    2 : 'CORE',    3 : 'WHOLE'\n\n\n\ngt[gt != eval_class] = 1 # use only one class for per class evaluation \n\nresized_gt = cv2.resize(gt[:,:,i+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE))\n\nplt.figure()\nf, axarr = plt.subplots(1,2) \naxarr[0].imshow(resized_gt, cmap=\"gray\")\naxarr[0].title.set_text('ground truth')\naxarr[1].imshow(p[i,:,:,eval_class], cmap=\"gray\")\naxarr[1].title.set_text(f'predicted class: {SEGMENT_CLASSES[eval_class]}')\nplt.show()","b6c67481":"model_unet_VGG16.compile(loss=\"categorical_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics = ['accuracy',tf.keras.metrics.MeanIoU(num_classes=4), dice_coef, precision, sensitivity, specificity, dice_coef_necrotic, dice_coef_edema, dice_coef_enhancing] )\n# Evaluate the model on the test data using `evaluate`\nprint(\"Evaluate on test data\")\nresults = model_unet_VGG16.evaluate(test_generator, batch_size=100, callbacks= callbacks)\nprint(\"test loss, test acc:\", results)","62f5610d":"#check coi d\u00f9ng c\u00e1i n\u00e0o nha, c\u00e1i nay h\u01a1i r\u1ed1i\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics = ['accuracy',tf.keras.metrics.MeanIoU(num_classes=4), dice_coef, precision, sensitivity, specificity, dice_coef_necrotic, dice_coef_edema, dice_coef_enhancing] )\n# Evaluate the model on the test data using `evaluate`\nprint(\"Evaluate on test data\")\nresults = model.evaluate(test_generator, batch_size=100, callbacks= callbacks)\nprint(\"test loss, test acc:\", results)","14cd3188":"# Loss function\n**Dice coefficient**\n, which is essentially a measure of overlap between two samples. This measure ranges from 0 to 1 where a Dice coefficient of 1 denotes perfect and complete overlap. The Dice coefficient was originally developed for binary data, and can be calculated as:\n\n![dice loss](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/a80a97215e1afc0b222e604af1b2099dc9363d3b)\n\n**As matrices**\n![dice loss](https:\/\/www.jeremyjordan.me\/content\/images\/2018\/05\/intersection-1.png)\n\n[Implementation, (images above) and explanation can be found here](https:\/\/www.jeremyjordan.me\/semantic-segmentation\/)","5c2c05d9":"**Show whole nifti data -> print each slice from 3d data**","6c0e6748":"# Survival prediction","ecd045d2":"**Show segments of tumor using different effects**","a7428528":"![Brats official annotations](https:\/\/www.med.upenn.edu\/cbica\/assets\/user-content\/images\/BraTS\/brats-tumor-subregions.jpg)","4507af20":"**Show segment of tumor for each above slice**","b3c14d66":"# Image data descriptions\n\nAll BraTS multimodal scans are available as  NIfTI files (.nii.gz) -> commonly used medical imaging format to store brain imagin data obtained using MRI and describe different MRI settings \n1. **T1**: T1-weighted, native image, sagittal or axial 2D acquisitions, with 1\u20136 mm slice thickness.\n2. **T1c**: T1-weighted, contrast-enhanced (Gadolinium) image, with 3D acquisition and 1 mm isotropic voxel size for most patients.\n3. **T2**: T2-weighted image, axial 2D acquisition, with 2\u20136 mm slice thickness.\n4. **FLAIR**: T2-weighted FLAIR image, axial, coronal, or sagittal 2D acquisitions, 2\u20136 mm slice thickness.\n\nData were acquired with different clinical protocols and various scanners from multiple (n=19) institutions.\n\nAll the imaging datasets have been segmented manually, by one to four raters, following the same annotation protocol, and their annotations were approved by experienced neuro-radiologists. Annotations comprise the GD-enhancing tumor (ET \u2014 label 4), the peritumoral edema (ED \u2014 label 2), and the necrotic and non-enhancing tumor core (NCR\/NET \u2014 label 1), as described both in the BraTS 2012-2013 TMI paper and in the latest BraTS summarizing paper. The provided data are distributed after their pre-processing, i.e., co-registered to the same anatomical template, interpolated to the same resolution (1 mm^3) and skull-stripped.\n\n","1f843b1f":"Full implementation can be found in my another notebook: https:\/\/www.kaggle.com\/rastislav\/mri-brain-tumor-survival-prediction","da11ed3a":"# Setup env","3119dfc7":"# Prediction examples ","da235ad1":"**Gif representation of slices in 3D volume**\n<img src=\"https:\/\/media1.tenor.com\/images\/15427ffc1399afc3334f12fd27549a95\/tenor.gif?itemid=20554734\">","91883b15":"**Number of data used**\nfor training \/ testing \/ validation","ad393e2f":"**Visualize the training process**","d524a065":"# Load data\nLoading all data into memory is not a good idea since the data are too big to fit in.\nSo we will create dataGenerators - load data on the fly as explained [here](https:\/\/stanford.edu\/~shervine\/blog\/keras-how-to-generate-data-on-the-fly)","32651cfa":"**Override Keras sequence DataGenerator class**","d29421f3":"# Create model || U-Net: Convolutional Networks for Biomedical Image Segmentation\nhe u-net is convolutional network architecture for fast and precise segmentation of images. Up to now it has outperformed the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. It has won the Grand Challenge for Computer-Automated Detection of Caries in Bitewing Radiography at ISBI 2015, and it has won the Cell Tracking Challenge at ISBI 2015 on the two most challenging transmitted light microscopy categories (Phase contrast and DIC microscopy) by a large margin\n[more on](https:\/\/lmb.informatik.uni-freiburg.de\/people\/ronneber\/u-net\/)\n![official definiton](https:\/\/lmb.informatik.uni-freiburg.de\/people\/ronneber\/u-net\/u-net-architecture.png)\n","4eb0fac5":"# Evaluation","b1f70629":"**Add callback for training process**","972ca6dd":"**model architecture** <br>\nIf you are about to use U-NET, I suggest to check out this awesome library that I found later, after manual implementation of U-NET [keras-unet-collection](https:\/\/pypi.org\/project\/keras-unet-collection\/), which also contains implementation of dice loss, tversky loss and many more!","f62c060d":"# Train model\nMy best model was trained with 81% accuracy on mean IOU and 65.5% on Dice loss <br>\nI will load this pretrained model instead of training again","864991ed":"![brats-tumor-subregions.jpg](attachment:18259180-7f11-444c-b084-798efc46533f.jpg)# Problem definiton\n**Segmentation of gliomas in pre-operative MRI scans.**\n\n*Each pixel on image must be labeled:*\n* Pixel is part of a tumor area (1 or 2 or 3) -> can be one of multiple classes \/ sub-regions\n* Anything else -> pixel is not on a tumor region (0)\n\nThe sub-regions of tumor considered for evaluation are: 1) the \"enhancing tumor\" (ET), 2) the \"tumor core\" (TC), and 3) the \"whole tumor\" (WT)\nThe provided segmentation labels have values of 1 for NCR & NET, 2 for ED, 4 for ET, and 0 for everything else.\n\n"}}