{"cell_type":{"6684858d":"code","0f47faa0":"code","ca6c1e6a":"code","58174c9d":"code","8a28d009":"code","9e9ecad7":"code","9e6994c0":"code","4178fe23":"code","890d5fc7":"code","d2157f80":"code","37db7d78":"code","dbd6d22f":"code","f4517116":"code","4aad2462":"code","e4f1b435":"code","52863b85":"code","d71bdddd":"code","5784b123":"code","7b5fbef4":"code","2d2dd9fa":"code","c38d86ed":"code","8716cf2b":"code","02a54228":"code","c3618170":"code","0205ce74":"code","6b53399a":"code","2dfa49fd":"code","cdf46468":"code","9726e5a1":"code","3936b6c7":"code","4d79d255":"code","55d3cff7":"code","e415cc13":"code","cb1f089b":"code","437f0d92":"code","30dc701a":"code","3d1cff77":"code","76ff744f":"code","acbc729c":"code","f4a2dcbc":"code","c7f94bbc":"markdown","113f231f":"markdown","35c32045":"markdown","3897cc3e":"markdown","a1b8639b":"markdown","19e30f61":"markdown","c0dbbe1a":"markdown","ee70180d":"markdown","f12c11ab":"markdown","ebc5e303":"markdown","315f3206":"markdown","156641bd":"markdown","9ff79059":"markdown","82c638d4":"markdown","8ccb0aa6":"markdown","c5c5dfe7":"markdown","bb1647d4":"markdown","31bc0a63":"markdown","e52e5deb":"markdown","d67ec2c2":"markdown","b5ac3309":"markdown","7966cd77":"markdown","db1c1659":"markdown","534320fb":"markdown","2b0a9e05":"markdown","11e02fe8":"markdown","2791982f":"markdown","917aeee5":"markdown","72587906":"markdown","070fcc44":"markdown","631fada3":"markdown","1c6799de":"markdown","8df6de5d":"markdown","f5374b39":"markdown","56b36d21":"markdown","80ed9df5":"markdown","22923c82":"markdown","26a7ae8f":"markdown"},"source":{"6684858d":"# Define the libraries and imports\n# Panda\nimport pandas as pd\n#mat plot\nimport matplotlib.pyplot as plt\n#Sea born\nimport seaborn as sns\n#Num py\nimport numpy as np\n#Sk learn imports\nfrom sklearn import tree,preprocessing\n#ensembles\nfrom sklearn.ensemble import RandomForestClassifier,BaggingClassifier\nimport sklearn.metrics as metrics\n#scores\nfrom sklearn.metrics import confusion_matrix,accuracy_score,roc_curve,roc_auc_score,auc  \n#models\nfrom sklearn.model_selection import StratifiedKFold,train_test_split,cross_val_score,learning_curve,GridSearchCV,validation_curve\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nimport xgboost as xgb\n#export the model\nimport pickle\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')","0f47faa0":"# Load data from the path to the dataSet\ndef load_dataset(dataSet_path):\n    data = pd.read_csv(dataSet_path)\n    return data\n\n#Imputation\ndef impute_data(df):\n    df.dropna(inplace=True)\n\n# Coversion weight to int\ndef weight_to_int(df):\n    df['Weight'] = df['Weight'].str[:-3]\n    df['Weight'] = df['Weight'].apply(lambda x: int(x))\n    return df\n\n# Coversion height to int\ndef height_convert(df_height):\n        try:\n            feet = int(df_height[0])\n            dlm = df_height[-2]\n            if dlm == \"'\":\n                height = round((feet * 12 + int(df_height[-1])) * 2.54, 0)\n            elif dlm != \"'\":\n                height = round((feet * 12 + int(df_height[-2:])) * 2.54, 0)\n        except ValueError:\n            height = 0\n        return height\n\ndef height_to_int(df):\n    df['Height'] = df['Height'].apply(height_convert)\n    \n#One Hot Encoding of a feature\ndef one_hot_encoding(df,column):\n    encoder = preprocessing.LabelEncoder()\n    df[column] = encoder.fit_transform(df[column].values)\n        \n\n#Drop columns that we are not interested in\ndef drop_columns(df):\n    df.drop(df.loc[:, 'Unnamed: 0':'Name' ],axis=1, inplace = True)\n    df.drop(df.loc[:, 'Photo':'Special'],axis=1, inplace = True)\n    df.drop(df.loc[:, 'International Reputation':'Real Face' ],axis=1, inplace = True)\n    df.drop(df.loc[:, 'Jersey Number':'Contract Valid Until' ],axis=1, inplace = True)\n    df.drop(df.loc[:, 'LS':'RB'],axis=1, inplace = True)\n    df.drop(df.loc[:, 'GKDiving':'Release Clause'],axis=1, inplace = True)\n\n#Transform positions to 3 categories 'Striker', 'Midfielder', 'Defender'    \ndef transform_positions(df):\n    for i in ['ST', 'CF', 'LF', 'LS', 'LW', 'RF', 'RS', 'RW']:\n      df.loc[df.Position == i , 'Position'] = 'Striker' \n    \n    for i in ['CAM', 'CDM', 'LCM', 'CM', 'LAM', 'LDM', 'LM', 'RAM', 'RCM', 'RDM', 'RM']:\n      df.loc[df.Position == i , 'Position'] = 'Midfielder' \n    \n    for i in ['CB', 'LB', 'LCB', 'LWB', 'RB', 'RCB', 'RWB','GK']:\n      df.loc[df.Position == i , 'Position'] = 'Defender' ","ca6c1e6a":"# Load dataset\ndf= load_dataset(\"..\/input\/data.csv\")\n# Drop columns that we are not interested in\ndrop_columns(df)\n# Impute the data that is null\nimpute_data(df)\n# transform weight and height to integer values\nweight_to_int(df)\nheight_to_int(df)\n# apply the one hot encoding to the Preferred foot (L,R) => (0,1)\none_hot_encoding(df,'Preferred Foot')\n# transform position to striker, midfielder, defender\ntransform_positions(df)\n# show the 10 first rows\ndf.head(10)","58174c9d":"df.info()","8a28d009":"# Count number of players in each position using countplot\nplt.figure(figsize=(12, 8))\nplt.title(\"Number of Players by position\")\nfig = sns.countplot(x = 'Position', data =df)","9e9ecad7":"# Define categorical skills base on the rating\ndef categorize_skill(df,column):\n    bins = (10,30,50,70,100)\n    group_names = ['Low','Moderate','High','VeryHigh']\n    categories = pd.cut(df[column],bins,labels=group_names)\n    new_column = column+'_cat'\n    df[new_column]=categories\ncategorize_skill(df,\"Finishing\")\ncategorize_skill(df,\"Strength\")\ncategorize_skill(df,\"FKAccuracy\")","9e6994c0":"# Crate Category plot from seaborn on Finishing & ShortPassing By position\nsns.catplot(x=\"Finishing_cat\", y=\"ShortPassing\", hue=\"Position\",\n            markers=[\"^\", \"o\",\"x\"], linestyles=[\"-\", \"--\",\"-\"],\n            kind=\"point\", data=df);","4178fe23":"# Crate Category plot from seaborn on  Strength & Interception By position\nsns.catplot(x=\"Strength_cat\", y=\"Interceptions\", hue=\"Position\",\n            markers=[\"^\", \"o\",\"x\"], linestyles=[\"-\", \"--\",\"-\"],\n            kind=\"point\", data=df);","890d5fc7":"# Crate Category plot from seaborn on FKAccuracy & Penalties By position\nsns.catplot(x=\"FKAccuracy_cat\", y=\"Penalties\", hue=\"Position\",\n            markers=[\"^\", \"o\",\"x\"], linestyles=[\"-\", \"--\",\"-\"],\n            kind=\"point\", data=df);","d2157f80":"# Box plot skills by position\nf, axes = plt.subplots(2, 2, figsize=(15, 15), sharex=False)\nsns.despine(left=True)\nsns.boxplot('Position', 'Jumping', data = df, ax=axes[0, 0])\nsns.boxplot('Position', 'Age', data = df, ax=axes[0, 1])\nsns.boxplot('Position', 'Height', data = df, ax=axes[1, 0])\nsns.boxplot('Position', 'Weight', data = df, ax=axes[1, 1])","37db7d78":"# Bar plot Reaction by Age\nmean_value_per_age = df.groupby('Age')['Reactions'].mean()\np = sns.barplot(x = mean_value_per_age.index, y = mean_value_per_age.values)\np = plt.xticks(rotation=90)","dbd6d22f":"#Scatter plot Finishing by shortPassing classified by position\nax = sns.scatterplot(x=\"ShortPassing\", y=\"Finishing\", hue=\"Position\",data=df)","f4517116":"# Drop some of unuseful coloumns\ndrop_elements = ['Position', 'Finishing_cat', 'Strength_cat', 'FKAccuracy_cat']\ntrain=df.drop(drop_elements, axis = 1)","4aad2462":"# Create the heat map of features correlation\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=False)","e4f1b435":"# Divide the data to train and test\n\n# Drop the elements that has been created for \ndrop_elements = ['Finishing_cat', 'Strength_cat', 'FKAccuracy_cat']\ndf=df.drop(drop_elements, axis = 1)\n\n# Create the unique values for the positions encoded as Defender:0, Midfielder:1, Striker:2\npositions = df[\"Position\"].unique()\nencoder = preprocessing.LabelEncoder()\ndf['Position'] = encoder.fit_transform(df['Position'])\n\n#The Y feature is the position\ny = df[\"Position\"]\n\n#The other features are all but the position\ndf.drop(columns=[\"Position\"],inplace=True)\n\n#Split the data\nX_train_dev, X_test, y_train_dev, y_test = train_test_split(df, y, \n                                                    test_size=0.20, \n                                                    random_state=42 )","52863b85":"# Plot the confusion matrix\ndef plot_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n    \n    Arguments\n    ---------\n    confusion_matrix: numpy.ndarray\n        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n        Similarly constructed ndarrays can also be used.\n    class_names: list\n        An ordered list of class names, in the order they index the given confusion matrix.\n    figsize: tuple\n        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n        the second determining the vertical size. Defaults to (10,7).\n    fontsize: int\n        Font size for axes labels. Defaults to 14.\n        \n    Returns\n    -------\n    matplotlib.figure.Figure\n        The resulting confusion matrix figure\n    \"\"\"\n    df_cm = pd.DataFrame(\n        confusion_matrix, index=class_names, columns=class_names, \n    )\n    fig = plt.figure(figsize=figsize)\n    sns.set(font_scale=1.4)\n    try:\n        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", annot_kws={\"size\": 16})\n    except ValueError:\n        raise ValueError(\"Confusion matrix values must be integers.\")\n    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    return fig","d71bdddd":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Generate a simple plot of the test and training learning curve.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    title : string\n        Title for the chart.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    ylim : tuple, shape (ymin, ymax), optional\n        Defines minimum and maximum yvalues plotted.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n          - None, to use the default 3-fold cross-validation,\n          - integer, to specify the number of folds.\n          - :term:`CV splitter`,\n          - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer\/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : int or None, optional (default=None)\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    train_sizes : array-like, shape (n_ticks,), dtype float or int\n        Relative or absolute numbers of training examples that will be used to\n        generate the learning curve. If the dtype is float, it is regarded as a\n        fraction of the maximum size of the training set (that is determined\n        by the selected validation method), i.e. it has to be within (0, 1].\n        Otherwise it is interpreted as absolute sizes of the training sets.\n        Note that for classification the number of samples usually have to\n        be big enough to contain at least one sample from each class.\n        (default: np.linspace(0.1, 1.0, 5))\n    \"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","5784b123":"def plot_curve(ticks, train_scores, test_scores):\n    train_scores_mean = -1 * np.mean(train_scores, axis=1)\n    train_scores_std = -1 * np.std(train_scores, axis=1)\n    test_scores_mean = -1 * np.mean(test_scores, axis=1)\n    test_scores_std = -1 * np.std(test_scores, axis=1)\n\n    plt.figure()\n    plt.fill_between(ticks, \n                     train_scores_mean - train_scores_std, \n                     train_scores_mean + train_scores_std, alpha=0.1, color=\"b\")\n    plt.fill_between(ticks, \n                     test_scores_mean - test_scores_std, \n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"r\")\n    plt.plot(ticks, train_scores_mean, 'b-', label='Training Error')\n    plt.plot(ticks, test_scores_mean, 'r-', label='Validation Error')\n    plt.legend(fancybox=True, facecolor='w')\n\n    return plt.gca()\n\ndef plot_validation_curve(clf, X, y, param_name, param_range, scoring='accuracy'):\n    plt.xkcd()\n    ax = plot_curve(param_range, *validation_curve(clf, X, y, cv=4, \n                                                   scoring=scoring, \n                                                   param_name=param_name, \n                                                   param_range=param_range, n_jobs=4))\n    ax.set_title('')\n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n    ax.set_xlim(2,12)\n    ax.set_ylim(-0.97, -0.83)\n    ax.set_ylabel('Error')\n    ax.set_xlabel('Model complexity')\n    ax.text(9, -0.94, 'Overfitting', fontsize=14)\n    ax.text(3, -0.94, 'Underfitting', fontsize=14)\n    ax.axvline(7, ls='--')\n    plt.tight_layout()","7b5fbef4":"def train_and_score(clf,X_train,y_train,X_test,y_test):\n    clf = clf.fit(X_train, y_train)\n    preds = clf.predict(X_test)\n    cf = confusion_matrix(y_test,preds)\n\n    print(plot_confusion_matrix(cf, class_names=positions))\n\n    print(\" Accuracy: \",accuracy_score(y_test, preds))\n    print(\" F1 score: \",metrics.f1_score(y_test, preds,average='weighted'))","2d2dd9fa":"LR = LogisticRegressionCV(cv=5,random_state=20, solver='lbfgs',\n                             multi_class='multinomial')\ntrain_and_score(LR,X_train_dev,y_train_dev,X_test,y_test)","c38d86ed":"plot_learning_curve(LR, \"Logistic Regression Curve\", X_train_dev, y_train_dev)","8716cf2b":"#create new a knn model\nknn_model = KNeighborsClassifier()\n#create a dictionary of all values we want to test for n_neighbors\nparam_grid = {'n_neighbors': np.arange(1, 25)}\n#use gridsearch to test all values for n_neighbors\nKNN = GridSearchCV(knn_model, param_grid, cv=5)\n\ntrain_and_score(KNN,X_train_dev,y_train_dev,X_test,y_test)","02a54228":"plot_learning_curve(KNN, \"KNN Regression Curve\", X_train_dev, y_train_dev)","c3618170":"plot_validation_curve(KNeighborsClassifier(), X_train_dev, y_train_dev, param_name='n_neighbors', param_range=range(2,25))","0205ce74":"def min_impurity(X,y):\n    tr_acc = []\n    mln_set = range(75,90)                                 \n\n    for minImp in mln_set:\n        clf = tree.DecisionTreeClassifier(criterion=\"entropy\",min_impurity_decrease=minImp\/100000)\n        scores = cross_val_score(clf, X, y, cv=10)\n        tr_acc.append(scores.mean())\n\n    best_mln = mln_set[np.argmax(tr_acc)]\n    return best_mln\n\nbest_min= min_impurity(X_train_dev,y_train_dev)","6b53399a":"DT = tree.DecisionTreeClassifier(criterion=\"entropy\",min_impurity_decrease=best_min\/100000)\ntrain_and_score(DT,X_train_dev,y_train_dev,X_test,y_test)","2dfa49fd":"plot_learning_curve(DT, \"Decision Tree Learning Curve\", X_train_dev, y_train_dev)","cdf46468":"DTBG = BaggingClassifier(tree.DecisionTreeClassifier(criterion=\"entropy\",min_impurity_decrease=best_min\/100000))#\ntrain_and_score(DTBG,X_train_dev,y_train_dev,X_test,y_test)","9726e5a1":"plot_learning_curve(DTBG, \"Bagging Decision Tree Learning Curve\", X_train_dev, y_train_dev)","3936b6c7":"dtrain = xgb.DMatrix(X_train_dev, label=y_train_dev)\n\ndtest = xgb.DMatrix(X_test,label=y_test)\n\nparam = {\n    'max_depth': 3,  # the maximum depth of each tree\n    'eta': 0.3,  # the training step for each iteration\n    'silent': 1,  # logging mode - quiet\n    'objective': 'multi:softprob',  # error evaluation for multiclass training\n    'num_class': 3}  # the number of classes that exist in this datset\nnum_round = 50  # the number of training iterations\nDTBST = xgb.train(param, dtrain, num_round)\nDTBST.dump_model('dump.raw.txt')\npreds = DTBST.predict(dtest)\nbest_preds = np.asarray([np.argmax(line) for line in preds])\ncf = confusion_matrix(y_test, best_preds)\n\nprint(plot_confusion_matrix(cf, class_names=positions))\nprint(\" Accuracy: \",accuracy_score(y_test, best_preds))\nprint(\" F1 score: \",metrics.f1_score(y_test, best_preds,average='weighted'))","4d79d255":"gridsearch_forest = RandomForestClassifier()\n\nparams = {\n    \"n_estimators\": [1, 10, 100],\n    \"max_depth\": [5,8,15], #2,3,5 85 #5,8,10 88 #5 8 15 89\n    \"min_samples_leaf\" : [1, 2, 4]\n}\n\nRF = GridSearchCV(gridsearch_forest, param_grid=params, cv=5 )\ntrain_and_score(RF,X_train_dev,y_train_dev,X_test,y_test)","55d3cff7":"plot_learning_curve(RF, \"Random Forest Learning Curve\", X_train_dev, y_train_dev)","e415cc13":"SVM = SVC(kernel='linear', C=1)\ntrain_and_score(SVM,X_train_dev,y_train_dev,X_test,y_test)","cb1f089b":"plot_learning_curve(SVM, \"SVM Curve\", X_train_dev, y_train_dev)","437f0d92":"MLP = MLPClassifier(solver='lbfgs', alpha=1e-5,\n                    hidden_layer_sizes=(5, 2), random_state=1)\ntrain_and_score(MLP,X_train_dev,y_train_dev,X_test,y_test)","30dc701a":"MLP = MLPClassifier(solver='lbfgs', alpha=1e-5,\n                    hidden_layer_sizes=(10, 5), random_state=1)\ntrain_and_score(MLP,X_train_dev,y_train_dev,X_test,y_test)","3d1cff77":"MLP = MLPClassifier(solver='lbfgs', alpha=1e-5,\n                    hidden_layer_sizes=(20, 15), random_state=1)\ntrain_and_score(MLP,X_train_dev,y_train_dev,X_test,y_test)","76ff744f":"MLP = MLPClassifier(solver='lbfgs', alpha=1e-5,\n                    hidden_layer_sizes=(50, 20), random_state=1)\ntrain_and_score(MLP,X_train_dev,y_train_dev,X_test,y_test)","acbc729c":"plot_learning_curve(MLP, \"Neural Network Curve\", X_train_dev, y_train_dev)","f4a2dcbc":"plt.style.use('ggplot')\nget_ipython().run_line_magic('matplotlib', 'inline')\nplt.figure()\n\ny1_test=y_test[(y_test ==0) | (y_test ==1)]\nx1_test = X_test[X_test.index.isin(y1_test.index)]\n\n\ny_predict_probabilities = LR.predict_proba(x1_test)[:,1]\nfpr, tpr, _ = roc_curve(y1_test, y_predict_probabilities)\nroc_auc = auc(fpr, tpr)\nplt.plot(fpr, tpr, color='darkorange',\n         lw=2, label='LR (area = %0.3f)' % roc_auc)\n\ny_predict_probabilities = KNN.predict_proba(x1_test)[:,1]\nfpr, tpr, _ = roc_curve(y1_test, y_predict_probabilities)\nroc_auc = auc(fpr, tpr)\nplt.plot(fpr, tpr, color='blue',\n         lw=2, label='KNN (area = %0.3f)' % roc_auc)\n\ny_predict_probabilities = RF.predict_proba(x1_test)[:,1]\nfpr, tpr, _ = roc_curve(y1_test, y_predict_probabilities)\nroc_auc = auc(fpr, tpr)\nplt.plot(fpr, tpr, color='green',\n         lw=2, label='RF (area = %0.3f)' % roc_auc)\n\ny_predict_probabilities = MLP.predict_proba(x1_test)[:,1]\nfpr, tpr, _ = roc_curve(y1_test, y_predict_probabilities)\nroc_auc = auc(fpr, tpr)\nplt.plot(fpr, tpr, color='red',\n         lw=2, label='NN (area = %0.3f)' % roc_auc)\n\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()","c7f94bbc":"**Plot Learning Curve**\n","113f231f":"# 5.Apply different models and algorithms and tune the hyper parameters\n\nIn this section we will train different models, find their score performance, tune the hyper parameters.\n\nFirst, we will define some functions that will help us train the models, evaluate and plot the curves.\n\n**Plot Confusion Matrix**\n\n","35c32045":"# 6. Evaluate the models\n\nAccording to the accuracy we can see that almost all of the models get about the same score. The best model is the Neural Network with 50,20 parameters.\n\nNow let's draw the ROC-AUC curve and plot the different models.","3897cc3e":"Now, let's apply the Decision Tree model with the best minimum impurity.","a1b8639b":"# 2.Preprocess the data\n\n**Load Libraries**\n\nNow, we get to the coding part when we explore that data using python and anaconda libraries. First of all we load the libraires.","19e30f61":"# 5.5. Neural Network\n\nIn this section we will be using the neural network with different parameters of the number of layers and the size. After tuning the parameters we find that the best option is 50-20 NN.","c0dbbe1a":"# 5.5. SVM\n\nIn this section we will be using the neural network with different parameters of the number of layers and the size. After tuning the parameters we find that the best option is 50-20 NN.","ee70180d":"**Features correlation**\n\nLet's correlate our features, but before that let's drop the categorical columns that we have created for plotting.","f12c11ab":"# 4.Divide the data to train and test datasets\n\nOur test size would be 20% from the data, the most recommended into the field. We will have then 80% as training data and 20% for testing.","ebc5e303":"**Reaction by age**","315f3206":"**Interceptions by Strength according to the positions**","156641bd":"**Train the model and score**\n\nThis function is responisble for traing the model and scoring it. We print at the end the Accuracy and F1 Score metrics.","9ff79059":"We notice that most of the players are defenders and midfielders which makes sense, since that in every team we need less strikers.\n\nNow, we want to explore the players skills features to plot them as catagories, but these are numerical. Therefor, we will first transform them.","82c638d4":"Note that all of them are so close.","8ccb0aa6":"# Refrences\n\nThis notebook has been produced using some external kernles and ressources listed below: \n\n[Fifa 19 Kernel][1] \n\n[SK Learn][2] \n\n\n[1]: https:\/\/www.kaggle.com\/brunosette\/classification-of-field-position-decision-tree\/data?fbclid=IwAR1_gS-pS9qXcyqEtRKZvmPXlwjwbK2Fd88NV_9OsayaE8LI44LTpoKJTVQ\n[2]: https:\/\/scikit-learn.org\/stable\/","c5c5dfe7":"**Plot validation curve**","bb1647d4":"# 5.1. Logistic regression\n\nWe apply our first model Logistic regression with a cross validation of 5 folds.","31bc0a63":"Let's make some plots.\n\n**Short Passing by finishing according to the positions**\n","e52e5deb":"We define a set of functions that would help us loading the data and preprocess some of the features that we are interested into.\n\nNote that we are only interested in this project in 3 players positions 'Strikers', 'Midfielders' and 'Defenders'. For more deep project we could have more classes such as specific positions saying 'Left back' or 'center forward'.","d67ec2c2":"**Scatter plot skills and positions**","b5ac3309":"**Interceptions by Strength according to the positions**","7966cd77":"# 8. Feature engineering\n\nAfter comparing the different models we found that the Neural Network is the best model with the highest accuracy. We tried to change our features by deleting correlated features or gathering some features together, but this didn't make the model more accurate.","db1c1659":"We see that some of the features are very correlated, this could be very helpful when we will be doing the features engineering.","534320fb":"# 5.3.3. Random Forest\n\nNow, let's try another ensembling technique the random forest that would search to tune many parameters.","2b0a9e05":"# 5.3. Decision Tree\n\nFirst all, let's define a function that calculate the best minimum impurity a parameter to pass to the decision tree.","11e02fe8":"# 7. AutoML\nWe applied AutoML from SKLearn on the data trying to find the best estimator. We run AutoML on a dedicated server with some capabilities and here we present the code and the output.\n\n![AutoML_Code](https:\/\/i.ibb.co\/PrWWTrz\/AutoML-1.png)\n\nAfter one hour of code running, we got finnaly the results.\n\n![AutoML_Result](https:\/\/i.ibb.co\/vPGXLRx\/AutoML-2.png)\n\nWe see that the best accuracy is : 0.895 which is so close to the Neural Network of 50,20.\n\nFinnaly, we looked for the best estimator.\n\n![AutoML_Result](https:\/\/i.ibb.co\/K6t45hf\/AutoML-3.png)\n\nWe found out that the best estimator accoring to AutoML is the gradient boosting.","2791982f":"# Introduction\n\nThis notebook walks through the whole process of machine learning with its different steps and provide web services and mobile applications to predict the player positions. The main goal is to apply these differenet steps to the [Fifa 19 data set][1].\n\n![Fifa19 DataSet](https:\/\/storage.googleapis.com\/kaggle-datasets-images\/73041\/162580\/a17bc456289e382192bebd8bb595719d\/dataset-cover.jpg?t=2018-11-04-22 9-55)\n\n**Data Set**\n\n[Fifa 19][2] is the most famous and played soccer game around the whole world with over 1.2 billion players. The game contains more than 70000 players in their [databases][3]. \n\nThe data set presented in this challenge contains more than 18000 players with their different features from physical appearence, clubs, wages and their performance. Our goal is to take those feature and predict the player position correctly.\n\n**Steps Applied**\n\n1. Understand the data using Google Facets\n   \n2. Prepocess the data\n\n3. Apply the [EDA][4](Exploratory Data Analysis) - Visualization\n\n4. Divide the data to train and test datasets\n\n5. Apply different models and algorithms and tune the hyper parameters\n\n    5.1. Logistic regression\n    \n    5.2. KNN\n    \n    5.3. Decision Tree\n    \n    5.4. SVM\n    \n    5.5. Neural Network\n\n6. Evaluate the models\n\n7. Apply AutoML on the data\n\n8. Features engineering\n\n9. Create Web service to apply the prediction\n\n[1]: https:\/\/www.kaggle.com\/karangadiya\/fifa19\n[2]: https:\/\/www.ea.com\/games\/fifa\/fifa-19\n[3]: https:\/\/www.fifaindex.com\/\n[4]: https:\/\/en.wikipedia.org\/wiki\/Exploratory_data_analysis","917aeee5":"We want to draw the model complexity curve and see how far the model is learning based on the n_neighbours range.","72587906":"# 1.Understand the data using Google Facets\n\n\nFor this part, to understand a little bit about the data, such as general information and how many data are missing we used [Google Facets][1] a very powerful tool to Understed the data which is critical to build a powerful machine learning system.\n\n\n**Numerical Features**\n\n![Numerical Feature 1](https:\/\/i.ibb.co\/N6DB4Tq\/01-Numerical-Feature.png)\n\n![Numerical Feature 2](https:\/\/i.ibb.co\/fG6wnsZ\/02-Numerical-Feature.png)\n\n![Numerical Feature 3](https:\/\/i.ibb.co\/RzhY43t\/03-Numerical-Feature.png)\n\nWe Observe that there is more than 18.2K players in the dataset. Some of the attributes are missing for some players such as the weak foot and the skills ratings. \n\n**Categorical Features**\n\n![Categorical Feature 1](https:\/\/i.ibb.co\/MMPyzdT\/04-Categoratical-Feature.png)\n\n![Categorical Feature 2](https:\/\/i.ibb.co\/1MK9cxj\/05-Categoratical-Feature.png)\n\nWe Observe that less than 1% of the data doesn't have position and some others features. Position is the Y feature therefore  we could use it as test dataset or deleting it. \n\n**Grouping plotting**\n\nIn This section we tried to plot the data according to different features. The legend represents the actual players positions.\n\n* Finishing per Age per Position\n\n![Finishing per Age per Position](https:\/\/i.ibb.co\/vvZt49L\/06-Analysis.png)\n\nWe Observe that most of the high finishing skills are the strikers, which totally make sense.\n\n* Short Pass per Age per Position\n\n![Short Pass per Age per Position](https:\/\/i.ibb.co\/THc1Yhf\/07-Analysis.png)\n\nIn the same way as the finishing, the high skilled players in the field of passing are the midlifers. \n\n\n\n[1]: https:\/\/pair-code.github.io\/facets\/","070fcc44":"**Physical appearences by position**\n\nLet's plot some physical appearences such as age, height and weight and see how it will affect the players positions.","631fada3":"# 5.3.1. Bagging\n\nNow, let's work with DT but using the bagging model.","1c6799de":"Let's see the features we will be working on.","8df6de5d":"# 9. Web Services\n\nFinnaly, we create a web service that throw a post with the data to predict that could be found on the following link : http:\/\/thefourtwofour.com:1992\/api\/predict\n\nHere an example of a post request and the results :\n![Post Predict](https:\/\/i.ibb.co\/RpkBYQc\/Post.png)","f5374b39":"Now, let's plot all the numerical features for correlation.","56b36d21":"# 5.3.2. Boosting\n\nNow, let's try another ensembling technique the boosting.","80ed9df5":"Let's apply these functions to our model.","22923c82":"# 5.2. K-nearest Neighbours\n\nThen, we apply the KNN model with gread serach that apply to the K (number of neighbors) in a range from 1 to 25 with a cross validation of 5.","26a7ae8f":"# 3.Visualization\n\nIn this section, we will apply the EDA to visualize the data with several plottings.\n\n**Players counts by position**"}}