{"cell_type":{"e0a74524":"code","47e9c190":"code","bc61af1a":"code","b9736ce5":"code","6f3414e9":"code","987d77c4":"code","0386f822":"code","a7d23b17":"code","b7899ece":"code","3863b139":"code","e6e6eace":"code","74280895":"code","e2142f85":"code","b1d66b65":"code","617379df":"code","2337fc46":"code","e736fb8b":"code","50090056":"code","242dfd42":"code","ab219ca9":"code","a8eebe2e":"code","4005e1d0":"code","6634170d":"code","a3c9123d":"code","440a8ae9":"code","b196f353":"code","a0998cda":"code","9dd0a38c":"code","9aac44c3":"code","60230b96":"code","d0b5e790":"code","264df434":"code","b28d07eb":"code","6fea94a5":"code","14ad4cb3":"markdown","315c2c6a":"markdown","016d0402":"markdown","58c2c8c5":"markdown","bef7c2e0":"markdown","b1e1ff81":"markdown","fcda0712":"markdown","3feecf7f":"markdown","e2ead42b":"markdown","68c74106":"markdown","6ec96a44":"markdown","f6a0b80a":"markdown","39c4aa43":"markdown","9071d577":"markdown","a90e40f6":"markdown"},"source":{"e0a74524":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport missingno as msno\nimport os\nimport torch\nimport gc\n","47e9c190":"# determine the supported device\ndef get_device():\n    if torch.cuda.is_available():\n        device = torch.device('cuda:0')\n    else:\n        device = torch.device('cpu') # don't have GPU \n    return device\n\ndevice = get_device()","bc61af1a":"!pip install torchbearer\n!pip install livelossplot","b9736ce5":"import glob\nimport pandas as pd\nfrom typing import List\n\ndef read_csv(path, re, dtype = None):\n    all_files = glob.glob(os.path.join(path, re))     # advisable to use os.path.join as this makes concatenation OS independent\n\n    df = (pd.read_csv(f, dtype=dtype) for f in all_files)\n    df = pd.concat(df, ignore_index=True)\n    return df\n\ndf = read_csv('\/kaggle\/input\/kddbr-2020\/', \"2018*.csv\")\ndf.head()","6f3414e9":"df.info()","987d77c4":"df.shape","0386f822":"input_columns  = df.columns[df.columns.str.contains(\"input\")]\noutput_columns = df.columns[df.columns.str.contains(\"output\")]\n\nprint(input_columns[:10], output_columns[:10])","a7d23b17":"## Extract date information\n\ndef get_date_features(df, column):\n    df['input_dt_sin_quarter']     = np.sin(2*np.pi*df[column].dt.quarter\/4)\n    df['input_dt_sin_day_of_week'] = np.sin(2*np.pi*df[column].dt.dayofweek\/6)\n    df['input_dt_sin_day_of_year'] = np.sin(2*np.pi*df[column].dt.dayofyear\/365)\n    df['input_dt_sin_day']         = np.sin(2*np.pi*df[column].dt.day\/30)\n    df['input_dt_sin_month']       = np.sin(2*np.pi*df[column].dt.month\/12)\n    \n    \ndf['date']  = pd.to_datetime(df['date'])\nget_date_features(df, 'date')\ndf.head()","b7899ece":"input_columns  = df.columns[df.columns.str.contains(\"input\")]\ninput_columns","3863b139":"# Filter onlu nissing values\nnull_columns=df.columns[df.isnull().any()]\nmsno.bar(df.sample(1000)[null_columns])","e6e6eace":"null_columns","74280895":"df = df.fillna(0)","e2142f85":"df[input_columns].shape","b1d66b65":"gc.collect()","617379df":"import torch\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataset import Dataset\n\nclass CustomDataset(Dataset):\n    def __init__(self, data, targets):\n        self.data  = data\n        self.targets = targets\n\n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n            \n        input  = self.data[idx]\n        target = self.targets[idx]\n        \n        return torch.Tensor(input.astype(float)).to(device), torch.Tensor(target.astype(float)).to(device)\n    \ninput, output = CustomDataset(df[input_columns].values, df[output_columns].values).__getitem__(1)\n[input, output]","2337fc46":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EncoderDecoder(nn.Module):\n    def __init__(self, input_size, n_factors, output_size):\n        super(EncoderDecoder,self).__init__()\n        \n        self.encoder = nn.Sequential(\n            nn.Linear(input_size, int(input_size\/2)),\n            nn.ReLU(True),\n            nn.Linear(int(input_size\/2), int(input_size\/4)),\n            nn.ReLU(True),\n            nn.Linear(int(input_size\/4), n_factors),\n            nn.ReLU(True))\n        \n        self.decoder = nn.Sequential(\n            nn.Linear(n_factors, int(output_size\/2)),\n            nn.ReLU(True),\n            nn.Linear(int(output_size\/2), output_size),\n            nn.ReLU(True),            \n            nn.Linear(output_size, output_size))\n        \n        self.dropout = torch.nn.Dropout(0.2)\n        \n    def forward(self,x):\n        x = self.encoder(x)\n        x = self.dropout(x)\n        x = self.decoder(x)\n        return x","e736fb8b":"# Params\ninput_size  = len(input_columns) \noutput_size = len(output_columns)\nn_factors   = 50\n\n\n# Model\nmodel       = EncoderDecoder(input_size, n_factors, output_size).to(device)\nmodel","50090056":"#device      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Train Params\nbatch_size  = 128\nnum_epochs  = 30\n\n\n# Loss and optimizer\n\n# create a nn class (just-for-fun choice :-) \nclass RMSELoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mse = nn.MSELoss()\n        \n    def forward(self,yhat,y):\n        return torch.sqrt(self.mse(yhat,y))\n\ncriterion   = RMSELoss()\noptimizer   = torch.optim.Adam(model.parameters(),weight_decay=1e-5)\n\nmodel","242dfd42":"from sklearn.model_selection import train_test_split\n# Data Loader\nclass NoAutoCollationDataLoader(DataLoader):\n    @property\n    def _auto_collation(self):\n        return False\n\n    @property\n    def _index_sampler(self):\n        return self.batch_sampler\n    \n# Split dataset\n#input_mean[np.isnan(input_mean)] = 0\nX = df[input_columns].values\nY = df[output_columns].fillna(0).values\n\nx_train, x_val, y_train, y_val = train_test_split(X, Y, test_size=0.20, random_state=42)\nlen(x_train), len(x_val)","ab219ca9":"# Pytorch Data Loader\ntrain_loader = NoAutoCollationDataLoader(CustomDataset(x_train, y_train), batch_size=batch_size, shuffle=True)\nval_loader   = NoAutoCollationDataLoader(CustomDataset(x_val, y_val), batch_size=batch_size, shuffle=False)","a8eebe2e":"import torchbearer\nfrom torchbearer import Trial\nfrom torchbearer.callbacks import LiveLossPlot\nfrom torchbearer.callbacks.checkpointers import ModelCheckpoint\nfrom torchbearer.callbacks.early_stopping import EarlyStopping\n\nweigth_path = \"\/kaggle\/working\/weights.pt\"\ncallbacks = [\n                LiveLossPlot(), \n                EarlyStopping(min_delta=1e-3, patience=10, monitor='val_loss', mode='min'),\n                ModelCheckpoint(weigth_path, save_best_only=True, monitor='val_loss', mode='min')\n            ]","4005e1d0":"%matplotlib inline\n\ntrial = Trial(model, optimizer, criterion, metrics=['loss'], callbacks=callbacks).to(device)\ntrial.with_generators(train_generator=train_loader, val_generator=val_loader)\nhist = trial.run(epochs=num_epochs)","6634170d":"!ls \/kaggle\/working\/","a3c9123d":"## Load Model\n\nstate_dict = torch.load(weigth_path, map_location=device)\nmodel.load_state_dict(state_dict[\"model\"])\n#model.eval()","440a8ae9":"# Predict\nval_predictions = model(torch.Tensor(x_val).to(device)).detach().cpu().numpy()","b196f353":"# Load trained model\n\n#val_loader   = NoAutoCollationDataLoader(CustomDataset(x_val, y_val), batch_size=batch_size, shuffle=False)\ntrial           = Trial(model, optimizer, criterion).with_test_generator(val_loader).to(device)\nval_predictions = trial.predict().detach().cpu().numpy()#.reshape(-1)\n\nval_predictions.shape, y_val.shape","a0998cda":"#  Weighted Root Mean Squared Error (WRMSE),\n# 0\t1.00\n# 1\t0.75\n# 2\t0.60\n# 3\t0.50\n# 4\t0.43\n# 5\t0.38\n# 6\t0.33\n\nweigths = [1]*16 + [0.75]*16 + [0.6]*16 + [0.5]*16 + [0.43]*16 + [0.38]*16 + [0.33]*16\n\ndef wrmse(predictions, targets, weigths):\n    #Is it???\n    return np.sqrt((((predictions - targets) ** 2)*weigths).mean())\n\ndef rmse(predictions, targets):\n    return np.sqrt(((predictions - targets) ** 2).mean())\n\nerror_wrmse = wrmse(val_predictions, y_val, weigths)\nerror_rmse  = rmse(val_predictions, y_val)\n\nprint(\"WRMSE:\", error_wrmse)\nprint(\"RMSE:\", error_rmse)","9dd0a38c":"# Clean Memory\ndel x_train\ndel x_val\ndel df\n\ndel train_loader\ndel val_loader\n\ngc.collect()","9aac44c3":"## Load Model\n\nstate_dict = torch.load(weigth_path, map_location=device)\nmodel.load_state_dict(state_dict[\"model\"])\nmodel.eval()","60230b96":"# Load and Prepare data\ndf_pred = read_csv('\/kaggle\/input\/kddbr-2020\/', \"public2019*.csv\").fillna(0)\n\n# transform\ndf_pred['date']  = pd.to_datetime(df_pred['date'])\nget_date_features(df_pred, 'date')\n\ndf_pred.head()","d0b5e790":"# Predict\ninputs = torch.Tensor(df_pred[input_columns].values).to(device)\npred   = model(inputs).cpu().detach().numpy()\npred","264df434":"df_pred_sub = pd.DataFrame(pred)\ndf_pred_sub.columns = output_columns\ndf_pred_sub['id']   = df_pred['id']\ndf_pred_sub.head()","b28d07eb":"###","6fea94a5":"## submission\ndf_sub = []\nfor i, row in df_pred_sub.iterrows():\n    for column, value in zip(output_columns, row.values):\n        id = \"{}_{}\".format(int(row.id), column)\n        df_sub.append([id, value])\n\ndf_sub = pd.DataFrame(df_sub)\ndf_sub.columns = ['id', 'value']\ndf_sub.to_csv('\/kaggle\/working\/submission.csv', index=False)\ndf_sub","14ad4cb3":"# Simple Pytorch Model\n\nThis kernel is a simple basemodel to predict the unavailability of cars in a car rental agency using Pytorch.\nWhen a car is unavailable, its status can be either \u201cin maintenance\u201d or \u201cbeing washed\u201d. The goal is to predict the number of cars entering and leaving each of the two status, for each of the four shifts in a day, for one week ahead.\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F210371%2Fa725239bd0154225ea57089b917ea9ae%2Flocaliza_exemplo.png?generation=1594062202781499&alt=media)\n\n* Prepara Dataset\n* Model\n* Train\n* Prepare Submission","315c2c6a":"### Split Dataset Train\/test","016d0402":"We are using only 2018 data for train, it's because of the limitation of RAM memory here, but it's ok.","58c2c8c5":"We need observe if have null columns:","bef7c2e0":"## Train","b1e1ff81":"Weighted Root Mean Squared Error (WRMSE) - Is it??? ","fcda0712":"## Prepare Submission\n\nWe will use the model in 2019 dataset for submission...","3feecf7f":"### Evaluation","e2ead42b":"We will fill with zero values.","68c74106":"## Model\n\nWe are using an architecture encoder-decoder to predict multiple outputs","6ec96a44":"## Prepara Dataset","f6a0b80a":"### Build Submission","39c4aa43":"### Train \n\nTrain Pytorch Model ","9071d577":"Add timestamp information","a90e40f6":"The problem contains multiples inputs and output...\n\nThe input variables are 557 features that may be correlated to the target variables. They are labeled from 0 up to 556, with the first four being categorical variables, and the rest being numerical. For the numerical variables, all the previous fourteen (14) values are provided (one for each day in two weeks before the tuple date). The input column names follow the syntax below:\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F210371%2Faf05e6e353863f45c6ce70e4618be43f%2Fdatakdd1.png?generation=1594062355235890&alt=media)\n\nThe output variables consist of a set of 16 (sixteen) columns, representing each of the 7 (seven) days ahead of the tuple date, 4 (four) shifts a day and 4 (four) status transitions a day, two for each status (entering or leaving). The column names follow the syntax detailed below:\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F210371%2F9cf037d978a6ff49919c3c1e6cadafbd%2Fdatakdd2.png?generation=1594062372260850&alt=media)"}}