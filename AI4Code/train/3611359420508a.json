{"cell_type":{"b6207824":"code","26414492":"code","1ebf44e5":"code","b3c407df":"code","ae235533":"code","30a25a1f":"code","7faf1a4b":"code","b40b4e49":"code","daa24d90":"code","ecfd5403":"code","8c07121b":"code","6c101705":"code","3a3821ad":"code","b6138b63":"code","ae9696de":"code","7c00e471":"code","8364dc8b":"code","f615555b":"code","1d9ea61b":"code","97dddad5":"code","c88ce50f":"code","4e174af9":"code","790887e2":"code","ad6ba135":"markdown","fedfd973":"markdown","d9437a03":"markdown","f41cb7fc":"markdown"},"source":{"b6207824":"# Importing Various Library \nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n","26414492":"print(os.listdir('..\/input'))","1ebf44e5":"# nRowsRead = 2000 # specify 'None' if want to read whole file\n# clash-of-clans.csv has 50001 rows in reality, but we are only loading\/previewing the first 1000 rows\n#df1 = pd.read_csv('..\/input\/clash-of-clans.csv', delimiter=',', nrows = nRowsRead)\ndf1 = pd.read_csv('..\/input\/clash-of-clans.csv', delimiter=',')\ndf1.dataframeName = 'clash-of-clans.csv'\nnRow, nCol = df1.shape\nprint(f'There are {nRow} rows and {nCol} columns')","b3c407df":"df1.head(5)","ae235533":"df1.info()","30a25a1f":"df1.shape","7faf1a4b":"df1.size","b40b4e49":"len(df1)","daa24d90":"# Use the count method to find the number of non-missing values for each column.\ndf1.count()","ecfd5403":"#The describe method is very powerful and calculates all the descriptive \n# statistics and quartiles in the preceding steps all at once\ndf1.describe()","8c07121b":"#To get a count of the missing values\ndf1.isnull().sum().sum()","6c101705":"df1.info()","3a3821ad":"df1.describe(include=[np.number]).T","b6138b63":"df1.describe(include=[np.object, pd.Categorical]).T","ae9696de":"# Inspect the data types of each column:\ndf1.dtypes","7c00e471":"# Find the memory usage of each column with the memory_usage method\noriginal_mem = df1.memory_usage(deep=True)\noriginal_mem","8364dc8b":"'''There is no need to use 64 bits for the Rating column as it contains only 1 - 5 values. \nLet's convert this column to an 8-bit (1 byte) integer with the astype method:'''\ndf1['Rating'] = df1['Rating'].astype(np.int8)","f615555b":"# Use the dtypes attribute to confirm the data type change:\ndf1.dtypes","1d9ea61b":"# Find the memory usage of each column again and note the large reduction\ndf1.memory_usage(deep=True)","97dddad5":"'''To save even more memory, you will want to consider changing object data types\nto categorical if they have a reasonably low cardinality (number of unique\nvalues). Let's first check the number of unique values for both the object columns:'''\ndf1.select_dtypes(include=['object']).nunique()","c88ce50f":"'''The Date column is a good candidate to convert to Categorical as less than one\npercent of its values are unique:'''\ndf1['Date'] = df1['Date'].astype('category')\ndf1.dtypes","4e174af9":"# Compute the memory usage again:\nnew_mem = df1.memory_usage(deep=True)\nnew_mem","790887e2":"# let's compare the original memory usage with our updated memory\nnew_mem \/ original_mem","ad6ba135":"So we have some Name which are Blank in our data set","fedfd973":"## Introduction\nLearning Python with the given data ....","d9437a03":"Let's take a quick look at what the data looks like:","f41cb7fc":"Distribution graphs (histogram\/bar graph) of sampled columns:"}}