{"cell_type":{"ee444413":"code","28b6c89b":"code","0db7b8cd":"code","29a45bf2":"code","a25350f4":"code","c92cbc97":"code","50090541":"code","d6d4384f":"code","98ab6988":"code","fc1c24dd":"code","98669651":"code","7968754e":"code","c77503aa":"code","c62cd3b3":"code","8e41e212":"code","020d7c3b":"code","c6b494d3":"code","bd6b03a3":"code","e6d95a4b":"code","f6c43283":"code","6e2664ce":"code","461da901":"markdown","a1e44f90":"markdown","0cd7a18c":"markdown","027d2be2":"markdown","9a88e9e3":"markdown","4a63367d":"markdown","d19462b0":"markdown","49a71040":"markdown","f79af09c":"markdown"},"source":{"ee444413":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","28b6c89b":"!pip install git+git:\/\/github.com\/AndLen\/simpletransformers.git --quiet","0db7b8cd":"import os\nimport gc\nfrom simpletransformers.classification import (ClassificationModel, ClassificationArgs)\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport plotly.express as px","29a45bf2":"#this is for silencing some tokenizer warnings\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nRANDOM_STATE = 42","a25350f4":"test = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/test.csv\")\ntraining = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/train.csv\")","c92cbc97":"training_df = training[[\"premise\", \"hypothesis\", \"label\"]]\ntraining_df.columns = [\"text_a\", \"text_b\", \"labels\"]","50090541":"final_test_df = test[[\"premise\", \"hypothesis\"]]\nfinal_test_df.columns = [\"text_a\", \"text_b\"]","d6d4384f":"fig = px.histogram(training, x=\"language\")\nfig.show()","98ab6988":"training_df[\"labels\"].value_counts()","fc1c24dd":"#train_df, test_df = train_test_split(training_df, test_size=0.30, random_state=RANDOM_STATE, stratify=training_df[\"labels\"])\n#eval_df, test_df = train_test_split(test_df, test_size=0.50, random_state=RANDOM_STATE, stratify=test_df[\"labels\"])","98669651":"shuffled_training = training_df.sample(frac=1, random_state=RANDOM_STATE, ignore_index=True)#.reset_index(drop=True)","7968754e":"shuffled_training.shape #train_df.shape ,test_df.shape, eval_df.shape","c77503aa":"gc.collect()\ntorch.cuda.empty_cache()","c62cd3b3":"# Create a ClassificationModel\nmodel_args = ClassificationArgs(num_train_epochs=3, # One big-boy epoch is enough for this chonky boy\n                                overwrite_output_dir=True)\nmodel_args.manual_seed = RANDOM_STATE\nmodel_args.best_model_dir = \"\/kaggle\/working\/best_model\"\nmodel_args.output_dir = \"\/kaggle\/temp\/output\"\n\n#model_args.reprocess_input_data = True\n#model_args.evaluate_during_training = True\n#model_args.evaluate_during_training_verbose = True\nmodel_args.train_batch_size = 24\nmodel_args.eval_batch_size = 24\n\n#model_args.early_stopping_metric = \"mcc\"\n#model_args.early_stopping_metric_minimize = False\n#model_args.save_eval_checkpoints = False\n#model_args.use_early_stopping = True\n#model_args.early_stopping_consider_epochs = True\n#model_args.early_stopping_patience = 3\nmodel_args.learning_rate = 1e-6\nmodel_args.manual_seed = RANDOM_STATE\n\nmodel = ClassificationModel(model_type='xlmroberta', \n                            model_name='joeddav\/xlm-roberta-large-xnli', \n                            args = model_args, \n                            num_labels = 3)","8e41e212":"model.train_model(shuffled_training)\n                  #eval_df=eval_df)","020d7c3b":"result, model_outputs, wrong_predictions = model.eval_model(shuffled_training, \n                                                            acc=sklearn.metrics.accuracy_score)","c6b494d3":"result","bd6b03a3":"final_pred_texts = []\nfor row in final_test_df.itertuples(index = False):\n    final_pred_texts.append(list(row))","e6d95a4b":"predictions, raw_outputs = model.predict(final_pred_texts)","f6c43283":"mypreds = pd.DataFrame(test[\"id\"])\nmypreds[\"prediction\"] = predictions","6e2664ce":"mypreds.to_csv(\"submission.csv\", index=False)","461da901":"## Making things easy with SimpleTransformers","a1e44f90":"## Reading our big-boy data","0cd7a18c":"We split the datasate using stratification when testing","027d2be2":"On the other side, labels are quite balanced","9a88e9e3":"We are renaming columns to Simple Transformers specifications","4a63367d":"## train test eval split, 70:15:15 proportions","d19462b0":"## Predicting on unlabelled test set and saving","49a71040":"Some langauages distributions, quite unbalanced","f79af09c":"1ep = (758, 0.5433722181063844)\n{'mcc': 0.9258332483255106,\n 'acc': 0.9505775577557756,\n 'eval_loss': 0.1682783056610807}\n\n2ep = (1516, 0.3996097231111772)\n{'mcc': 0.9530729053357176,\n 'acc': 0.9687293729372938,\n 'eval_loss': 0.12447924599405329}"}}