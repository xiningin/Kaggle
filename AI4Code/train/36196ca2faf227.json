{"cell_type":{"e045d949":"code","c5b2825b":"code","f5be9948":"code","05b9752b":"code","63bfe455":"code","f5dc9cde":"code","0ab3c05f":"code","2a7d1394":"code","9e6c3098":"code","99a752fb":"code","893ae2b0":"code","bddcb534":"code","140e9149":"code","d39f6ac0":"code","128ff1dd":"code","44573f02":"code","ce6d5d1f":"code","75f0ed2a":"code","b35329a7":"code","8b9dd1ee":"code","d34f0ccb":"code","b7d79b3b":"code","c905ee86":"code","c2071cda":"code","50682c8d":"code","a8632dde":"code","6bdf8138":"code","af4aaf6b":"code","57ecf6af":"code","7f821238":"code","fb7d411c":"code","51dc3ee6":"code","7b6b9cf8":"code","3651fb3e":"code","40d01bae":"code","7c3bddc9":"code","acc964ed":"code","94eea42b":"code","0ea78198":"code","feb4b0c5":"code","61b08416":"code","c5d1507a":"code","3e1feb58":"code","ee689966":"code","eb2e3e07":"code","5aaa56c1":"code","4c704f33":"code","43f4ba4c":"code","cc57db2b":"code","4dc72a57":"code","78e644dd":"code","c86211ae":"code","9e0389d6":"code","1bd59e4c":"code","0fc71a00":"code","fbc89008":"code","72b8bce8":"code","aa216302":"code","649fc91c":"code","41edbe9b":"markdown","f4151dbc":"markdown","199f4d84":"markdown","9440e821":"markdown","fc799872":"markdown","9fc064e0":"markdown","c6ee6017":"markdown","1d93b7e8":"markdown","9e422cec":"markdown","67499faf":"markdown","3a8535d0":"markdown","f1c56669":"markdown","6b7a027e":"markdown","426b8619":"markdown","39060b2c":"markdown","da220408":"markdown","2c0d7876":"markdown","37a085b4":"markdown","247ba6f4":"markdown","984bbd23":"markdown","db745c50":"markdown","2cdc94a1":"markdown","ec4c7d1a":"markdown","d7032b22":"markdown","44879b71":"markdown","9108bc8c":"markdown","cd4223bf":"markdown","854e3f76":"markdown","5d44f708":"markdown","cd2ece8b":"markdown","58c71991":"markdown","1d4f4605":"markdown","55a41429":"markdown","f48240ed":"markdown","e6a1f1a5":"markdown","222af2f9":"markdown","1aeb442c":"markdown","62c8adbc":"markdown","f2104b19":"markdown","bc458c0c":"markdown","98a2fdf7":"markdown","29f5feb4":"markdown","a20ac69a":"markdown","5e48cd92":"markdown"},"source":{"e045d949":"#working with data\nimport pandas as pd\nimport numpy as np\n\n#visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n## Scikit-learn features various classification, regression and clustering algorithms\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn.metrics import average_precision_score, confusion_matrix, accuracy_score, classification_report,f1_score\n\n## Scaling\nfrom sklearn.preprocessing import StandardScaler\n\n## Algo\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')\n","c5b2825b":"#loading Data\nData = pd.read_csv('..\/input\/parkinsondata\/Data-Parkinsons')","f5be9948":"Data.head()","05b9752b":"#fetch all columns\nData.columns","63bfe455":"#checking data Type of each attributes\nData.dtypes","f5dc9cde":"shape_data=Data.shape\nprint('Data set contains \"{x}\" number of rows and \"{y}\" number of columns columns'.format(x=shape_data[0],y=shape_data[1]))","0ab3c05f":"#checking for Null Values\nData.isnull().sum()","2a7d1394":"sns.heatmap(Data.isnull(),yticklabels=False,cbar=False,cmap='viridis')","9e6c3098":"#overview of data\nData.describe().transpose()","99a752fb":"#A Skewness value of 0 in the output denotes a symmetrical distribution\n#A negative Skewness value in the output denotes tail is larger towrds left hand side of data so we can say left skewed\n#A Positive Skewness value in the output denotes tail is larger towrds Right hand side of data so we can say Right skewed\nData.skew()","893ae2b0":"#Univariate analysis of Fundamental frequency\nfig, axes = plt.subplots(1, 3, figsize=(16, 4))\nsns.distplot(Data['MDVP:Fo(Hz)'],bins=30,ax=axes[0])\nsns.distplot(Data['MDVP:Fhi(Hz)'],bins=30,ax=axes[1])\nsns.distplot(Data['MDVP:Flo(Hz)'],bins=30,ax=axes[2])\naxes[0].set_title('Average vocal fundamental frequency')\naxes[1].set_title('Maximum vocal fundamental frequency')\naxes[2].set_title('Minimum vocal fundamental frequency')","bddcb534":"#Univariate analysis of measures of variation in fundamental frequency\nfig, axes = plt.subplots(2, 3, figsize=(16, 8))\nsns.distplot(Data['MDVP:Jitter(%)'],bins=30,ax=axes[0,0],color='green')\nsns.distplot(Data['MDVP:Jitter(Abs)'],bins=30,ax=axes[0,1],color='green')\nsns.distplot(Data['MDVP:RAP'],bins=30,ax=axes[0,2],color='green')\n\nsns.distplot(Data['MDVP:PPQ'],bins=30,ax=axes[1,0],color='green')\nsns.distplot(Data['Jitter:DDP'],bins=30,ax=axes[1,1],color='green')\n#sns.distplot(Data['Shimmer:DDA'],bins=30,ax=axes[1,2])\nfig.tight_layout()","140e9149":"#Univariate analysis of  variation in amplitude\nfig, axes = plt.subplots(2, 3, figsize=(16, 8))\nsns.distplot(Data['MDVP:Shimmer'],bins=30,ax=axes[0,0],color='orange')\nsns.distplot(Data['MDVP:Shimmer(dB)'],bins=30,ax=axes[0,1],color='orange')\nsns.distplot(Data['Shimmer:APQ3'],bins=30,ax=axes[0,2],color='orange')\n\nsns.distplot(Data['Shimmer:APQ5'],bins=30,ax=axes[1,0],color='orange')\nsns.distplot(Data['MDVP:APQ'],bins=30,ax=axes[1,1],color='orange')\nsns.distplot(Data['Shimmer:DDA'],bins=30,ax=axes[1,2],color='orange')\n","d39f6ac0":"#analysis for measures of ratio of noise to tonal components in the voice\nfig, axes = plt.subplots(1, 2, figsize=(16, 4))\nsns.distplot(Data['NHR'],bins=30,ax=axes[0],color='red')\nsns.distplot(Data['HNR'],bins=30,ax=axes[1],color='red')\n","128ff1dd":"#analysis for wo nonlinear dynamical complexity measures\nfig, axes = plt.subplots(1, 2, figsize=(16, 4))\nsns.distplot(Data['RPDE'],bins=30,ax=axes[0],color='purple')\nsns.distplot(Data['D2'],bins=30,ax=axes[1],color='purple')","44573f02":"#Univariate analysis of nonlinear measures of fundamental frequency variation \nfig, axes = plt.subplots(1, 3, figsize=(16, 4))\nsns.distplot(Data['spread1'],bins=30,ax=axes[0])\nsns.distplot(Data['spread2'],bins=30,ax=axes[1])\nsns.distplot(Data['PPE'],bins=30,ax=axes[2])","ce6d5d1f":"#Variation of traget variables\nsns.countplot(x=Data['status'])","75f0ed2a":"corr = Data.corr()\ncorr","b35329a7":"sns.set_context(\"notebook\", font_scale=1.0, rc={\"lines.linewidth\": 3.5})\nplt.figure(figsize=(18,7))\n# create a mask so we only see the correlation values once\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask, 1)] = True\na = sns.heatmap(corr,mask=mask, annot=True, fmt='.2f')\nrotx = a.set_xticklabels(a.get_xticklabels(), rotation=90)\nroty = a.set_yticklabels(a.get_yticklabels(), rotation=30)","8b9dd1ee":"#variation of Spread1 with Target Variable\nsns.kdeplot(Data[Data.status == 0]['spread1'], shade=False,)\nsns.kdeplot(Data[Data.status == 1]['spread1'], shade=True)\nplt.title(\"Variation of Spread1 with Status\")","d34f0ccb":"#variation of HNR with Target Variable, will not consider MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA as they are highly correlatedwith NHR \nsns.boxplot(x='status',y='HNR',data=Data)","b7d79b3b":"#variation of  Maximum, Minimum vocal fundamental frequency\nfig, ax = plt.subplots(1,2,figsize=(14,4))\nsns.kdeplot(Data[Data.status == 0]['MDVP:Flo(Hz)'], shade=False,ax=ax[0])\nsns.kdeplot(Data[Data.status == 1]['MDVP:Flo(Hz)'], shade=True,ax=ax[0])\n\nsns.kdeplot(Data[Data.status == 0]['MDVP:Fhi(Hz)'], shade=False,ax=ax[1])\nsns.kdeplot(Data[Data.status == 1]['MDVP:Fhi(Hz)'], shade=True,color='r',ax=ax[1])","c905ee86":"#variation of MDVP:Jitter(%) with Target Variable, will not consider Jitter(Abs),MDVP:RAP,MDVP:PPQ,NHR as they are highly correlatedwith NHR \nsns.boxplot(x='status',y='MDVP:Jitter(%)',data=Data)","c2071cda":"#variation of Spread1 with Target Variable\nsns.kdeplot(Data[Data.status == 0]['MDVP:Jitter(%)'], shade=False,)\nsns.kdeplot(Data[Data.status == 1]['MDVP:Jitter(%)'], shade=True)\nplt.title(\"Variation of MDVP:Jitter(%) with Status\")","50682c8d":"#Split the data into training and test set in the ratio of 70:30 respectively\nX = Data.drop(['status','name'],axis=1)\ny = Data['status']\n\n# split data into train subset and test subset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100)\n\n# checking the dimensions of the train & test subset\n# to print dimension of train set\nprint(X_train.shape)\n# to print dimension of test set\nprint(X_test.shape)","a8632dde":"#checking the variance \n#high variance means fearure does not affect the target variable\nX_train.var()","6bdf8138":"#dropping correlated values which are have either more then 80% or less then -80%\nX_train.drop(['MDVP:Shimmer','MDVP:Jitter(%)','HNR'],axis=1,inplace=True)\nX_test.drop(['MDVP:Shimmer','MDVP:Jitter(%)','HNR'],axis=1,inplace=True)","af4aaf6b":"#since there is lots of variety in the units of features let's scale it\nscaler=StandardScaler().fit(X_train)\nscaler_x_train=scaler.transform(X_train)\n\nscaler=StandardScaler().fit(X_test)\nscaler_x_test=scaler.transform(X_test)\n","57ecf6af":"# Train and Fit model\nmodel = LogisticRegression(random_state=0)\nmodel.fit(scaler_x_train, y_train)","7f821238":"#predict the Personal Loan Values\ny_Logit_pred = model.predict(scaler_x_test)\ny_Logit_pred","fb7d411c":"# Let's measure the accuracy of this model's prediction\nprint(\"confusion_matrix\")\nprint(confusion_matrix(y_test,y_Logit_pred))","51dc3ee6":"# And some other metrics for Test\ncr=classification_report(y_test, y_Logit_pred, digits=2)\nprint(cr)","7b6b9cf8":"# creating odd list of K for KNN\nmyList = list(range(3,40,2))\n\n# empty list that will hold accuracy scores\nac_scores = []\n\n# perform accuracy metrics for values from 3,5....29\nfor k in myList:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(scaler_x_train, y_train)\n    # predict the response\n    y_pred = knn.predict(scaler_x_test)\n    # evaluate F1 Score\n    scores = f1_score(y_test, y_pred)\n    ac_scores.append(scores)\n\n# changing to misclassification error\nMSE = [1 - x for x in ac_scores]\n\n# determining best k\noptimal_k = myList[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors is %d\" % optimal_k)","3651fb3e":"# instantiate learning model (k = 29)\nknn = KNeighborsClassifier(n_neighbors = 29, weights = 'uniform', metric='euclidean')\n# fitting the model\nknn.fit(scaler_x_train, y_train)\n\n# predict the response\ny_Knn_pred = knn.predict(scaler_x_test)\n\n# Let's measure the accuracy of this model's prediction\nprint(\"confusion_matrix\")\nprint(confusion_matrix(y_test,y_Knn_pred))\n","40d01bae":"# evaluate Model Score\nprint(classification_report(y_test, y_Knn_pred, digits=2))","7c3bddc9":"clf = SVC(gamma=0.05, C=3,random_state=0)\nclf.fit(scaler_x_train , y_train)\n\n# predict the response\nprediction_SVC = clf.predict(scaler_x_test)\n\n# Let's measure the accuracy of this model's prediction\nprint(\"confusion_matrix\")\nprint(confusion_matrix(y_test,prediction_SVC))\n","acc964ed":"# evaluate Model Score\nprint(classification_report(y_test, prediction_SVC, digits=2))","94eea42b":"#Using K fold to check how my algorighm varies throughout my data if we split it in 10 equal bins\nmodels = []\nmodels.append(('Logistic Regression', LogisticRegression()))\nmodels.append(('K-NN', KNeighborsClassifier(n_neighbors = 29, weights = 'uniform', metric='euclidean')))\nmodels.append(('SVM', SVC(gamma=0.05, C=3)))\n\n# evaluate each model\nresults = []\nnames = []\nscoring = 'f1'\nfor name, model in models:\n\tkfold = model_selection.KFold(n_splits=10, random_state=101)\n\tcv_results = model_selection.cross_val_score(model, scaler_x_train, y_train, cv=kfold, scoring=scoring)\n\tresults.append(cv_results)\n\tnames.append(name)\n\tprint(\"Name = %s , Mean F1-Score = %f, SD F1-Score = %f\" % (name, cv_results.mean(), cv_results.std()))","0ea78198":"fig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.plot(results[0],label='Logistic')\nplt.plot(results[1],label='KNN')\nplt.plot(results[2],label='SVC')\nplt.legend()\nplt.show()","feb4b0c5":"#Stacking the idea of stacking is to learn several different weak learners\n# and combine them by training a meta-model to output predictions based on the multiple predictions\n# returned by these weak models. So, we need to define two things in order to build our stacking model:\n# the L learners we want to fit and the meta-model that combines them.\n\n# defining level hetrogenious model\nlevel0 = list()\nlevel0.append(('lr', LogisticRegression()))\nlevel0.append(('knn', KNeighborsClassifier(n_neighbors = 29, weights = 'uniform', metric='euclidean')))\nlevel0.append(('cart', DecisionTreeClassifier()))\nlevel0.append(('svm', SVC(gamma=0.05, C=3)))\nlevel0.append(('bayes', GaussianNB()))\n\n# define meta learner model\nlevel1 = SVC(gamma=0.05, C=3)\n\n# define the stacking ensemble with cross validation of 5\nStack_model = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\n","61b08416":"# predict the response\nStack_model.fit(scaler_x_train, y_train)\nprediction_Stack = Stack_model.predict(scaler_x_test)","c5d1507a":"\n# Let's measure the accuracy of this model's prediction\nprint(\"confusion_matrix\")\nprint(confusion_matrix(y_test,prediction_Stack))\n","3e1feb58":"# evaluate Model Score\nprint(classification_report(y_test, prediction_Stack, digits=2))","ee689966":"#determining false positive rate and True positive rate, threshold\nfpr, tpr, threshold = metrics.roc_curve(y_test, prediction_Stack)\nroc_auc_stack = metrics.auc(fpr, tpr)\n# print AUC\nprint(\"AUC : % 1.4f\" %(roc_auc_stack)) ","eb2e3e07":"#plotting ROC curve\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc_stack)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","5aaa56c1":"#creating model of Random Forest\nRandomForest = RandomForestClassifier(n_estimators = 100,criterion='entropy',max_features=10)\nRandomForest = RandomForest.fit(scaler_x_train, y_train)\n\n# predict the response\nRandomForest_pred = RandomForest.predict(scaler_x_test)\n","4c704f33":"\n# Let's measure the accuracy of this model's prediction\nprint(\"confusion_matrix\")\nprint(confusion_matrix(y_test,RandomForest_pred))\n","43f4ba4c":"# evaluate Model Score\nprint(classification_report(y_test, RandomForest_pred, digits=2))","cc57db2b":"#determining false positive rate and True positive rate, threshold\nfpr, tpr, threshold = metrics.roc_curve(y_test, RandomForest_pred)\nroc_auc_rf = metrics.auc(fpr, tpr)\n# print AUC\nprint(\"AUC : % 1.4f\" %(roc_auc_rf)) ","4dc72a57":"#plotting ROC curve\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc_rf)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","78e644dd":"# Lets check features importance\nfeature_imp = pd.Series(RandomForest.feature_importances_,index=X_train.columns).sort_values(ascending=False)\nfeature_imp","c86211ae":"# Creating a bar plot\nsns.barplot(x=feature_imp, y=feature_imp.index)\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')","9e0389d6":"#create and fit the model\nAdBs = AdaBoostClassifier( n_estimators= 50)\nAdBs  = AdBs.fit(scaler_x_train, y_train)\n\n# predict the response\nAdBs_pred = AdBs.predict(scaler_x_test)\n","1bd59e4c":"# Let's measure the accuracy of this model's prediction\nprint(\"confusion_matrix\")\nprint(confusion_matrix(y_test,AdBs_pred))\n","0fc71a00":"# evaluate Model Score\nprint(classification_report(y_test, AdBs_pred, digits=2))","fbc89008":"#determining false positive rate and True positive rate, threshold\nfpr, tpr, threshold = metrics.roc_curve(y_test, AdBs_pred)\nroc_auc_ada = metrics.auc(fpr, tpr)\n# print AUC\nprint(\"AUC : % 1.4f\" %(roc_auc_ada)) ","72b8bce8":"#plotting ROC curve\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc_ada)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","aa216302":"#Using K fold to check how my algorighm varies throughout my data if we split it in 10 equal bins\nmodels = []\nmodels.append(('Logistic Regression', LogisticRegression()))\nmodels.append(('K-NN', KNeighborsClassifier(n_neighbors = 29, weights = 'uniform', metric='euclidean')))\nmodels.append(('SVM', SVC(gamma=0.05, C=3)))\nmodels.append(('Stacking', StackingClassifier(estimators=level0, final_estimator=level1, cv=5)))\nmodels.append(('Random Forest', RandomForestClassifier(n_estimators = 100,criterion='entropy',max_features=10)))\nmodels.append(('Adaptive Boosting', AdaBoostClassifier( n_estimators= 50)))\n\n# evaluate each model\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n\tkfold = model_selection.KFold(n_splits=10, random_state=101)\n\tcv_results = model_selection.cross_val_score(model, scaler_x_train, y_train, cv=kfold, scoring=scoring)\n\tresults.append(cv_results)\n\tnames.append(name)\n\tprint(\"Name = %s , Mean Accuracy = %f, SD Accuracy = %f\" % (name, cv_results.mean(), cv_results.std()))","649fc91c":"# boxplot algorithm comparison\nfig = plt.figure(figsize=(12,4))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot()\nplt.boxplot(results)\nax.set_xticklabels(names)","41edbe9b":"#### Model Scoring\n<p>1. Accuracy :: 88% <\/p>\n<p>2. Re-call ::  96%<\/p>\n<p>3. Precision :: 90% <\/p>\n<p>4. F1-Score :: 93%<\/p>\n<p> Ratio in target variable is 75% to 25% so we will take f1-score i.e. <b>93%<\/b> as our scoring method <\/p>\n<p> ROC-AUC : <b>77%<\/b> <\/p>","f4151dbc":"#### Conclusion\n<p>Random Forest Algro performs better in terms of overall performace. <\/p>\n<p>Avarage Accuracy over data is 92% with Standard Deviation of 7% and F1-Score of 94% with Standard Deviation of 6%<\/p>","199f4d84":"# Ensemble Techniques Project\n### <u>Data Description and Context:<\/u>\nParkinson\u2019s Disease (PD) is a degenerative neurological disorder marked by decreased dopamine levels in the brain. It manifests itself through a deterioration of movement, including the presence of tremors and stiffness. There is commonly a marked effect on speech, including dysarthria (difficulty articulating sounds), hypophonia (lowered volume), and monotone (reduced pitch range). Additionally, cognitive impairments and changes in mood can occur, and risk of dementia is increased.\nTraditional diagnosis of Parkinson\u2019s Disease involves a clinician taking a neurological history of the patient and observing motor skills in various situations. Since there is no definitive laboratory test to diagnose PD, diagnosis is often difficult, particularly in the early stages when motor effects are not yet severe. Monitoring progression of the disease over time requires repeated clinic visits by the patient. An effective screening process, particularly one that doesn\u2019t require a clinic visit, would be beneficial. Since PD patients exhibit characteristic vocal features, voice recordings are a useful and non-invasive tool for diagnosis. If machine learning algorithms could be applied to a voice recording dataset to accurately diagnosis PD, this would be an effective screening step prior to an appointment with a clinician\n\n### <u>Domain:<\/u>\nMedicine\n\n### <u>Attribute Information:<\/u>\n<p>name - ASCII subject name and recording number<\/p>\n<p>MDVP:Fo(Hz) - Average vocal fundamental frequency<\/p>\n<p>MDVP:Fhi(Hz) - Maximum vocal fundamental frequency<\/p>\n<p>MDVP:Flo(Hz) - Minimum vocal fundamental frequency<\/p>\n<p>MDVP:Jitter(%),MDVP:Jitter(Abs),MDVP:RAP,MDVP:PPQ,Jitter:DDP - Several measures of variation in fundamental frequency<\/p>\n<p>MDVP:Shimmer,MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA - Several measures of variation in amplitude<\/p>\n<p>NHR,HNR - Two measures of ratio of noise to tonal components in the voice<\/p>\n<p>status - Health status of the subject (one) - Parkinson's, (zero) - healthy<\/p>\n<p>RPDE,D2 - Two nonlinear dynamical complexity measures<\/p>\n<p>DFA - Signal fractal scaling exponent<\/p>\n<p>spread1,spread2,PPE - Three nonlinear measures of fundamental frequency variation 9. car name: string (unique for each instance)<\/p>\n\n### <u>Objective:<\/u>\nGoal is to classify the patients into the respective labels using the attributes from their voice recordings","9440e821":"#### Observation\n<p> People who have Parkinson have higher levels of noise to tonal components in the voice.<\/p>","fc799872":"### <font color='green'>Adaptive Boosting<\/font>","9fc064e0":"#### Model Scoring\n<p>1. Accuracy :: 90% <\/p>\n<p>2. Re-call ::  96%<\/p>\n<p>3. Precision :: 92% <\/p>\n<p>4. F1-Score :: 94%<\/p>\n<p> Ratio in target variable is 75% to 25% so we will take f1-score i.e. <b>94%<\/b> as our scoring method","c6ee6017":"#### Observation\n<p> People with Minimum vocal fundamental frequency above 250 give more evidence of not having Parkinson <\/p>\n<p> People with Maximum vocal fundamental frequency from 100-200 give more evidence of having Parkinson <\/p>","1d93b7e8":"### <font color='red'>Task 1: <\/font> Importing data in data frame","9e422cec":"### <font color='red'>Task 4: <\/font> Split the dataset into training and test set in the ratio of 70:30 (Training:Test)","67499faf":"### <font color='red'>Task 3: <\/font> Using univariate & bivariate analysis to check the individual attributes for their basic statistics","3a8535d0":"#### Model Scoring\n<p>1. Accuracy :: 80% <\/p>\n<p>2. Re-call ::  85%<\/p>\n<p>3. Precision :: 89% <\/p>\n<p>4. F1-Score :: 87%<\/p>\n<p> Ratio in target variable is 75% to 25% so we will take f1-score i.e. <b>87%<\/b> as our scoring method","f1c56669":"### <font color='red'>Task 2: <\/font> eye-ball raw data to get a feel of the data","6b7a027e":"#### <font color='green'>Bi-variate analysis<\/font> \n<p> It would take lots of graphs and time to plot each graph with relation to target variable, so let's first find the most co-related columns and then do bi-vaiate analysis of those columns<\/p>","426b8619":"### <font color='red'>Task 8: <\/font> Standard Ensemble model\n### <font color='green'>Random Forest<\/font>","39060b2c":"#### AUC-ROC for Random Forest","da220408":"#### Observation\n<p>NHR is right skewed, most of the values lies aroung 0.00 to 0.04, so all values are very small.<\/p>\n<p>The value HNR seems to be a slight negative skewness<\/p>","2c0d7876":"#### Observations\n<p>The Average vocal fundamental frequency is almost normally distributed with more values ranging 115Hz and 130Hz. <\/p>\n<p>The Maximum vocal fundamental frequency is a positive skewness with more values ranging 100Hz and 250Hz. <\/p>\n<p>The Minimum vocal fundamental frequency is a positive skewness for minimum vocal fundemental frequency with more high values between 75Hz and 125Hhz. <\/p>","37a085b4":"### END","247ba6f4":"### <font color='red'>Task 7: <\/font> Training Meta Classifier","984bbd23":"#### Observation\n<p>MDVP:Jitter(%) have high correlation i.e. above +90% value with Jitter(Abs),MDVP:RAP,MDVP:PPQ,NHR <\/p>\n<p>HNR have High correlation i.e. above -80% with MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA <\/p>\n<p>MDVP:Shimmer has a very correlation with MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA <\/p>\n<p> These Realtion may be coz they are derived from each other <\/p>\n<p>The target variable status has a weak positive corelation with spread1<\/p>","db745c50":"### <font color='red'>Task 5: <\/font> Prepare the data for training","2cdc94a1":"### <font color='red'>Task 6: <\/font> Training with standard classification algorithms","ec4c7d1a":"#### AUC-ROC for AdaBoost","d7032b22":"### <font color='green'>Stacking<\/font>","44879b71":"#### Observation\n<p>From the above graphs, we can observe that all graphs have almost same distribution,and each graph is positively skewed <\/p>","9108bc8c":"#### Observation\n<p>From the above graphs, we can observe that all graphs have almost same distribution,and each graph is positively skewed <\/p>","cd4223bf":"### <font color='green'>C. SVM Algorithm<\/font>","854e3f76":"### <font color='green'>B. KNN<\/font>","5d44f708":"### Data Uderstanding\n <p>1.There is NO MISSING data.<\/p>\n <p>2.The feature 'name' does not add any information, there is no realationship between 'name' and 'status', We can remove this row.<\/p>\n <p>3.All features are numerical.<\/p>\n <p>4.There is lots of variation in units of data, gap between features values is very high, need to scale it.<\/p>\n <p>5.Most of the features are Skewed.<\/p>","cd2ece8b":"#### Observation\n<p>Both RPDE and D2 tends to be normalised graph<\/p>","58c71991":"#### Model Scoring\n<p>1. Accuracy :: 90% <\/p>\n<p>2. Re-call ::  96%<\/p>\n<p>3. Precision :: 92% <\/p>\n<p>4. F1-Score :: 94%<\/p>\n<p> Ratio in target variable is 75% to 25% so we will take f1-score i.e. <b>94%<\/b> as our scoring method <\/p>\n<p> ROC-AUC : <b>81%<\/b> <\/p>","1d4f4605":"#### Model Scoring\n<p>1. Accuracy :: 88% <\/p>\n<p>2. Re-call ::  100%<\/p>\n<p>3. Precision :: 87% <\/p>\n<p>4. F1-Score :: 93%<\/p>\n<p> Ratio in target variable is 75% to 25% so we will take f1-score i.e. <b>93%<\/b> as our scoring method","55a41429":"#### Determining which standard model performed better","f48240ed":"### <font color='red'>Step 1 <\/font> Importing Libraries and Data.","e6a1f1a5":"#### Model Scoring\n<p>1. Accuracy :: 86% <\/p>\n<p>2. Re-call ::  91%<\/p>\n<p>3. Precision :: 91% <\/p>\n<p>4. F1-Score :: 91%<\/p>\n<p> Ratio in target variable is 75% to 25% so we will take f1-score i.e. <b>91%<\/b> as our scoring method <\/p>\n<p> ROC-AUC : <b>79%<\/b> <\/p>","222af2f9":"#### Observation\n<p>More People(75%) have Parkinson then people not having Parkinson as per given dataset<\/p>","1aeb442c":"### <font color='red'>Task 9: <\/font> Compare all the models","62c8adbc":"#### Observation\n<p> if MDVP:jitter(%) value is >0.005 more likely are the chances of having Parkinson. <\/p>","f2104b19":"#### AUC-ROC for stacking","bc458c0c":"### <font color='green'>A. Logistic Regression<\/font>","98a2fdf7":"#### Observation\n<p> People whose spread1 is between -6.5 and -5 have more chances of having Parkinson<\/p>\n<p> People whose spread1 is between -9.5 and -7.5 have more chances of not having Parkinson<\/p>\n","29f5feb4":"### <font color='green'>Conclusion<\/font>\n<p>After compairing Logistic,KNN,SVC algo we can conclude that <b>SVC performed slightly better<\/b><\/p>\n","a20ac69a":"#### <font color='green'>Univariate analysis<\/font> ","5e48cd92":"#### Observation\n<p>All graphs tends to be normalised graph with few outliers<\/p>"}}