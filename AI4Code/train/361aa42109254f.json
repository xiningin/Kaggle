{"cell_type":{"643f3f2b":"code","eef0e8e8":"code","2dabd339":"code","130123b7":"code","25fc0b1f":"code","c50fbf00":"code","603d7a1d":"markdown","581a0dd4":"markdown","5a254374":"markdown","dbfefcd5":"markdown","e291e70e":"markdown"},"source":{"643f3f2b":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import transforms\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets.mnist import MNIST, FashionMNIST\n\nfrom tqdm import tqdm\n\nimport warnings\n\nwarnings.filterwarnings('ignore')","eef0e8e8":"def mnist_dataloader(data_root, batch_size, num_workers=0, pin_memory=False, download=True):\n\n    tf = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize([0.5], [0.5])\n    ])\n\n    train_dataset = MNIST(root=data_root, train=True, transform=tf, download=download)\n\n    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n                                  num_workers=num_workers, pin_memory=pin_memory)\n\n    return train_dataloader","2dabd339":"class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n\n        self.fc = nn.Sequential(\n            nn.Linear(100, 128),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Linear(128, 256),\n            nn.BatchNorm1d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Linear(256, 512),\n            nn.BatchNorm1d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n\n            nn.Linear(512, 1024),\n            nn.BatchNorm1d(1024),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n        self.sam = nn.Sequential(\n            nn.Linear(1024, 1024),\n            nn.Sigmoid()\n        )\n        self.out = nn.Sequential(\n            nn.Linear(1024, 784),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        x = self.fc(x)\n        x_attention = self.sam(x)\n        x = x * x_attention\n        x = self.out(x)\n        x = x.view(x.size(0), 1, 28, 28)\n        return x","130123b7":"class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Linear(28 * 28, 512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(512, 256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(256, 1),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        x = x.view(x.size(0), -1)\n        x = self.model(x)\n\n        return x","25fc0b1f":"batch_size = 64\nnum_epoch = 1\n# num_epoch = 100\nlr = 0.0002","c50fbf00":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprint('Device:{}'.format(device))\n\nGNet = Generator().to(device)\nDNet = Discriminator().to(device)\n\ntrain_dl = mnist_dataloader('.\/dataset\/data\/', batch_size, num_workers=4, pin_memory=True)\n\n\nG_optimizer = optim.Adam(GNet.parameters(), lr=lr, betas=(0.5, 0.999))\nD_optimizer = optim.Adam(DNet.parameters(), lr=lr, betas=(0.5, 0.999))\n\ncriterion = nn.BCELoss()\n\nto_pil = transforms.ToPILImage()\nplt.ion()\nplt.show()\n\nfor epoch in range(num_epoch):\n    running_G_loss = 0.0\n    running_D_loss = 0.0\n    cnt = 0\n\n    pbar = tqdm(enumerate(train_dl))\n    for i, (x, _) in pbar:\n        y_true = torch.ones(x.size(0), 1, requires_grad=False).to(device)\n        y_false = torch.zeros(x.size(0), 1, requires_grad=False).to(device)\n\n        x = x.to(device)\n\n        G_optimizer.zero_grad()\n\n        z = torch.randn(x.size(0), 100).to(device)\n\n        G_out = GNet(z)\n\n        G_loss = criterion(DNet(G_out), y_true)\n\n        G_loss.backward()\n        G_optimizer.step()\n\n        ###################\n\n        D_optimizer.zero_grad()\n\n        D_out = DNet(x)\n        D_loss_real = criterion(D_out, y_true)\n\n        D_out_2 = DNet(G_out.detach())\n        D_loss_fake = criterion(D_out_2, y_false)\n\n        D_loss = (D_loss_real + D_loss_fake) \/ 2\n        D_loss.backward()\n        D_optimizer.step()\n\n        pbar.set_description(\"[Epoch {}\/{}] [Batch {}\/{}] [D loss: {:.5f}] [G loss: {:.5f}]\".format(\n                              epoch, num_epoch, i, len(train_dl), D_loss.item(), G_loss.item()))\n\n        cnt += 1\n        running_D_loss += D_loss.item()\n        running_G_loss += G_loss.item()\n\n    x_rand = torch.randn(2, 100).to(device)\n    G_out = GNet(x_rand)\n    \n    with torch.no_grad():\n        plt.figure()\n        plt.title('epoch:{}'.format(epoch))\n        plt.imshow(to_pil(G_out.to('cpu')[0]), cmap='gray')\n        plt.show()\n\n    epoch_D_loss = running_D_loss \/ cnt\n    epoch_G_loss = running_G_loss \/ cnt\n    print('G_loss:{:0.4f} D_loss:{:0.4f}'.format(epoch_G_loss, epoch_D_loss))\n    if epoch % 10 == 9:\n        dic = {'G': GNet.state_dict(),\n               'D': DNet.state_dict()}\n        torch.save(dic, '.\/model_{}.pt'.format(epoch))","603d7a1d":"## 3.2 Discriminator","581a0dd4":"# 1. Introduction\n\n  Generative Adversarial Networks(GAN),a Unsupervised Learning Algorithm,which was proposed in 2014.You can find the paper at https:\/\/arxiv.org\/abs\/1406.2661.","5a254374":"# 2. DataSet","dbfefcd5":"# 4. Training","e291e70e":"# 3. Model\n## 3.1 Generator"}}