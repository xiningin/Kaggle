{"cell_type":{"e8e070bb":"code","268ee533":"code","0e56a5e5":"code","595a77be":"code","60deddb5":"code","f122f19f":"code","bb6ce959":"code","5c489f96":"code","f2bc581d":"code","e3ffd8df":"code","aaa195f7":"code","91a51a9e":"code","5d579ed1":"code","64938e66":"code","a7333b0f":"code","edd86470":"code","0d257e0a":"code","f227ac18":"code","1119d03e":"code","483932a3":"code","fbf41f48":"code","a0ef49b3":"code","0a1a00b7":"code","f856c20e":"code","08a39eef":"code","ff61fb8d":"code","41e566b1":"code","7b1d689e":"code","89c138e9":"code","f07997e1":"code","4232b719":"code","0e9927a4":"code","0058907a":"code","fa17c6e2":"code","5f9428a5":"code","b19ebc9b":"code","7e4ced4d":"code","83a94b59":"code","c35c6fc5":"code","d8419e65":"code","028eaf6b":"code","a2563af2":"code","a3563fa1":"code","6d090d3f":"code","722fa6d2":"code","0ac1a8ef":"code","dbc06589":"code","5dc873a6":"code","1ec11b97":"code","31cccb60":"code","50541161":"code","3bb0bda9":"code","4902d85a":"code","12713f2c":"code","4fa4d9da":"code","38cd693f":"code","8f0e9d16":"code","e556cb8d":"code","8674750a":"code","257f2330":"code","2db4192b":"code","070f42b0":"code","69b72e43":"code","1fd28dd7":"code","761284b5":"code","48f36377":"code","f180c42a":"code","62a4b640":"code","a7022f30":"code","3ded880c":"code","1a2347f3":"code","98c15528":"code","347323e0":"markdown","42bdb252":"markdown","20f8120c":"markdown","bc868c4c":"markdown","b0661e9c":"markdown","ba893593":"markdown","98bf1161":"markdown","48f31227":"markdown","3e95c61b":"markdown","607f089f":"markdown","6c8502cb":"markdown","3399dc6b":"markdown","22bc2710":"markdown","bdf1a862":"markdown","1d6878a3":"markdown","b125303c":"markdown","6ea17900":"markdown","d7353474":"markdown","2bf7ac62":"markdown","9ed6b1b3":"markdown","525d2fd6":"markdown","9ab90da9":"markdown","e15d0eae":"markdown","b5b7fb36":"markdown","78f9559d":"markdown","171cb1fb":"markdown","f9a7e29c":"markdown","99fa4b6f":"markdown","9bcd2ddb":"markdown","2b3057bf":"markdown","2d7d814c":"markdown","9fb2f209":"markdown","fb93b3e8":"markdown","12ef9f47":"markdown","e7447be2":"markdown","6fbe9f77":"markdown","f9c4a376":"markdown","1c04936b":"markdown","cdf5fb06":"markdown","cc67c506":"markdown","118a7133":"markdown","0e3130c8":"markdown","99423f20":"markdown","f308802b":"markdown","595240d0":"markdown","cdda90fa":"markdown","8f394e11":"markdown","e3a4c6c2":"markdown","45967ed5":"markdown","4fa1a306":"markdown","37b9f08a":"markdown","870ada84":"markdown","9a6886db":"markdown","11fe38ae":"markdown","1b2c8324":"markdown","188925e8":"markdown","cd506621":"markdown","efd40ecf":"markdown","ac9724c5":"markdown","a33f5e00":"markdown","12ab0354":"markdown","5bccfd3c":"markdown","edf592a6":"markdown","769ab3b2":"markdown","ab859f51":"markdown","ea483f9a":"markdown","8865d68d":"markdown","58180c47":"markdown","ebd7bd30":"markdown","46115962":"markdown","ef3aec27":"markdown","3597f179":"markdown"},"source":{"e8e070bb":"import numpy as np \nimport pandas as pd \n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n\n\nimport os\nDATA_DIR='..\/input'\nprint(os.listdir(DATA_DIR))","268ee533":"train_df = pd.read_csv(DATA_DIR+'\/train.csv')\ntest_df = pd.read_csv(DATA_DIR+'\/test.csv')","0e56a5e5":"train_df.head(5)","595a77be":"test_df.head(5)","60deddb5":"train_df.shape","f122f19f":"test_df.shape","bb6ce959":"train_ID = train_df['Id']\ntest_ID = test_df['Id']","5c489f96":"#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain_df.drop(\"Id\", axis = 1, inplace = True)\ntest_df.drop(\"Id\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train_df.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test_df.shape))","f2bc581d":"fig, ax = plt.subplots()\nax.scatter(x = train_df['GrLivArea'], y = train_df['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","e3ffd8df":"#Deleting outliers\ntrain_df = train_df.drop(train_df[(train_df['GrLivArea']>4000) & (train_df['SalePrice']<300000)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(train_df['GrLivArea'], train_df['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","aaa195f7":"sns.distplot(train_df['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train_df['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train_df['SalePrice'], plot=plt)\nplt.show()","91a51a9e":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain_df[\"SalePrice\"] = np.log1p(train_df[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train_df['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train_df['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train_df['SalePrice'], plot=plt)\nplt.show()\n","5d579ed1":"ntrain = train_df.shape[0]\nntest = test_df.shape[0]\ny_train = train_df.SalePrice.values\nall_data = pd.concat((train_df, test_df)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","64938e66":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","a7333b0f":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","edd86470":"#Correlation map to see how features are correlated with SalePrice\ncorrmat = train_df.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","0d257e0a":"all_data['PoolQC'] = all_data['PoolQC'].fillna('None')","f227ac18":"all_data['MiscFeature'] = all_data['MiscFeature'].fillna('None')","1119d03e":"all_data['Alley'] = all_data['Alley'].fillna('None')","483932a3":"all_data['Fence'] = all_data['Fence'].fillna('None')","fbf41f48":"all_data['FireplaceQu'] = all_data['FireplaceQu'].fillna('None')","a0ef49b3":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(\n    lambda x: x.fillna(x.median()))","0a1a00b7":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')","f856c20e":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)","08a39eef":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)","ff61fb8d":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')","41e566b1":"all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)","7b1d689e":"all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])","89c138e9":"all_data = all_data.drop(['Utilities'], axis=1)","f07997e1":"all_data['Functional'] = all_data['Functional'].fillna('Typ')","4232b719":"all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])","0e9927a4":"all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])","0058907a":"all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])","fa17c6e2":"all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])","5f9428a5":"all_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")","b19ebc9b":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","7e4ced4d":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","83a94b59":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))","c35c6fc5":"# Adding total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","d8419e65":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","028eaf6b":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam)","a2563af2":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","a3563fa1":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","6d090d3f":"from sklearn.linear_model import Ridge, RidgeCV, ElasticNetCV, LassoCV, LassoLarsCV\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler","722fa6d2":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","0ac1a8ef":"model_ridge = Ridge()","dbc06589":"alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\ncv_ridge = [rmsle_cv(make_pipeline(RobustScaler(), Ridge(alpha = alpha, random_state=1))).mean() \n            for alpha in alphas]","5dc873a6":"cv_ridge = pd.Series(cv_ridge, index = alphas)\ncv_ridge.plot(title = \"Validation\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmsle\")","1ec11b97":"cv_ridge.min()","31cccb60":"model_ridge = make_pipeline(RobustScaler(), Ridge(alpha = 10, random_state=1))\nridge_res = rmsle_cv(model_ridge)\nprint('Ridge evaluation result : {:<8.3f}'.format(ridge_res.min()))","50541161":"pd_scores = pd.DataFrame(data={'Model':['Ridge'], 'Mean':[ridge_res.mean()], 'Std':[ridge_res.std()],'Min':[ridge_res.min()]})","3bb0bda9":"pd_scores","4902d85a":"model_lassocv = make_pipeline(RobustScaler(), LassoCV(alphas = [1, 0.1, 0.001, 0.0005], random_state=1))\nlassocv_res = rmsle_cv(model_lassocv)\nprint('LassoCV Score : {:<8.4f}, with min value : {:<8.4f} and std : {:<8.4f}'.format(lassocv_res.mean(), lassocv_res.min(),lassocv_res.std()))","12713f2c":"coef = pd.Series(model_lassocv.steps[1][1].fit(train,y_train).coef_, index = train.columns)","4fa4d9da":"print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","38cd693f":"imp_coef = pd.concat([coef.sort_values().head(12),\n                     coef.sort_values().tail(12)])","8f0e9d16":"plt.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Model\")","e556cb8d":"#let's look at the residuals as well:\nplt.rcParams['figure.figsize'] = (6.0, 6.0)\n\npreds = pd.DataFrame({\"preds\":model_lassocv.steps[1][1].predict(train), \"true\":y_train})\npreds[\"residuals\"] = preds[\"true\"] - preds[\"preds\"]\npreds.plot(x = \"preds\", y = \"residuals\",kind = \"scatter\")","8674750a":"lasso_score=[{'Model':'LassoCV', 'Mean':lassocv_res.mean(),'Std':lassocv_res.std(), 'Min':lassocv_res.min()}]\npd_scores = pd_scores.append(lasso_score,ignore_index=True, sort=False)","257f2330":"pd_scores","2db4192b":"model_enet_cv = make_pipeline(RobustScaler(), ElasticNetCV(cv=None, random_state=0))","070f42b0":"enet_score = rmsle_cv(model_enet_cv)\nprint('ElasticNet Score : {:<8.4f}, with min value : {:<8.4f} and std : {:<8.4f}'.format(enet_score.mean(), enet_score.min(),enet_score.std()))","69b72e43":"score = [{'Model':'ElasticNetCV', 'Mean':enet_score.mean(),'Std':enet_score.std(), 'Min':enet_score.min()}]\npd_scores = pd_scores.append(score,ignore_index=True, sort=False)","1fd28dd7":"from sklearn.kernel_ridge import KernelRidge\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nKRR_Score = rmsle_cv(KRR)\nprint('KernelRidge Score : {:<8.4f}, with min value : {:<8.4f} and std : {:<8.4f}'.format(KRR_Score.mean(), KRR_Score.min(),KRR_Score.std()))","761284b5":"score = [{'Model':'KernelRidge', 'Mean':KRR_Score.mean(),'Std':KRR_Score.std(), 'Min':KRR_Score.min()}]\npd_scores = pd_scores.append(score,ignore_index=True, sort=False)","48f36377":"import xgboost as xgb","f180c42a":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nscore1 = rmsle_cv(model_xgb)\nprint('xgboost Score : {:<8.4f}, with min value : {:<8.4f} and std : {:<8.4f}'.format(score1.mean(), score1.min(),score1.std()))","62a4b640":"# dtrain = xgb.DMatrix(train, label = y_train)\n# dtest = xgb.DMatrix(test)\n\n# params = {\"max_depth\":2, \"eta\":0.1}\n# model = xgb.cv(params, dtrain,  num_boost_round=500, early_stopping_rounds=100)\n\n# model.loc[30:,[\"test-rmse-mean\", \"train-rmse-mean\"]].plot()\n\n# model_xgb1 = xgb.XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1) #the params were tuned using xgb.cv\n# score1 = rmsle_cv(model_xgb1)\n# print('xgboost Score : {:<8.4f}, with min value : {:<8.4f} and std : {:<8.4f}'.format(score1.mean(), score1.min(),score1.std()))\n#model_xgb.fit(train, y_train)\n\n# Adding xgboost score to scores board\nscore = [{'Model':'XGBoost', 'Mean':score1.mean(),'Std':score1.std(), 'Min':score1.min()}]\npd_scores = pd_scores.append(score,ignore_index=True, sort=False)","a7022f30":"pd_scores.sort_values(by=['Mean','Std', 'Min'])","3ded880c":"lassocv = model_lassocv.steps[1][1]\nlassocv.fit(train,y_train)\npreds = np.expm1(lassocv.predict(test))\npreds","1a2347f3":"solution = pd.DataFrame({\"id\":test_ID, \"SalePrice\":preds})\nsolution.head(5)","98c15528":"solution.to_csv(\"lasso_sol.csv\", index = False)","347323e0":"- **LassoCV Regression**","42bdb252":"# Modelling","20f8120c":"- **Kernel Ridge Regression**","bc868c4c":"The target variable is right skewed. We need to transform this variable and make it more normally distributed.\nOne way to do it is to apply log(1+x) to all elements of the column using the numpy function log1p.","b0661e9c":"**Define a cross validation strategy**","ba893593":"[Documentation](http:\/\/ww2.amstat.org\/publications\/jse\/v19n3\/Decock\/DataDocumentation.txt)\nfor the Ames Housing Data indicates that there are outliers present in the training data","98bf1161":"Delete the Id column since it's not necessary for the prediction.","48f31227":"let's first concatenate the train and test data in the same dataframe","3e95c61b":"**Label Encoding some categorical variables that may contain information in their ordering set** ","607f089f":"- **FireplaceQu** : data description says NA means \"no fireplace\"","6c8502cb":"LassoCV gave us the best result. Lets fit the model and submit the results.","3399dc6b":"- **Electrical** : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.","22bc2710":"- **xgboost**","bdf1a862":"Add score to scores data frame","1d6878a3":"<p>LassoCV performs better than ridge.<\/p>\n<p>Also LassoCV it does feature selection - setting coefficients of features it deems unimportant to zero. Let's analyze the coefficients:<\/p>","b125303c":"## Base models","6ea17900":"**Skewed features**","d7353474":"- **Exterior1st and Exterior2nd** : Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string","2bf7ac62":"We use the scipy function boxcox1p which computes the Box-Cox transformation of  1+\ud835\udc65 .\n\nNote that setting  \ud835\udf06=0  is equivalent to log1p used above for the target variable.\n\nSee [this page](http:\/\/onlinestatbook.com\/2\/transformations\/box-cox.html) for more details on Box Cox Transformation as well as [the scipy function's page](https:\/\/docs.scipy.org\/doc\/scipy-0.19.0\/reference\/generated\/scipy.special.boxcox1p.html)","9ed6b1b3":"- **Elastic Net Regression**\n<br>\nagain make robust to outliers","525d2fd6":"- **MasVnrArea and MasVnrType** : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type. ","9ab90da9":"- **BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath** : missing values are likely zero for having no basement","e15d0eae":"- **Utilities** : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\"  and 2 NA . Since the house with 'NoSewa' is in the training set, **this feature won't help in predictive modelling**. We can then safely  remove it.","b5b7fb36":"- **Functional** : data description says NA means typical","78f9559d":"- **Alley** : data description says NA means \"no alley access\"","171cb1fb":"- **BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2** : For all these categorical basement-related features, NaN means that there is no  basement.","f9a7e29c":"<p>The main tuning parameter for the Ridge model is alpha - a regularization parameter that measures how flexible our model is. The higher the regularization the less prone our model will be to overfit. However it will also lose flexibility and might not capture all of the signal in the data. <\/p>\n<p>Also lets make our model more robust to outliers. We can do that using RobustScaler function via pipe.<\/p>\n","99fa4b6f":"Get the shape of train_df and test_df before data manipulation","9bcd2ddb":"**Adding one more important feature**","2b3057bf":"- **KitchenQual**: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent)  for the missing value in KitchenQual.","2d7d814c":"<p>The residual plot looks pretty good.<\/p>\n<p>Add LassoCV scores to scores dataframe.<\/p>","9fb2f209":"### Missing Data","fb93b3e8":"- **SaleType** : Fill in again with most frequent which is \"WD\"","12ef9f47":"## Feature engineering","e7447be2":"- **MiscFeature** : data description says NA means \"no misc feature\"","6fbe9f77":"Visualize the results.","f9c4a376":"We impute them by proceeding sequentially through features with missing values","1c04936b":"### Imputing missing values ","cdf5fb06":"**Getting dummy categorical features**","cc67c506":"Since area related features are very important to determine house prices, we add one more feature which is the total area of basement, first and second floor areas of each house","118a7133":"No more missing data.","0e3130c8":"Create a Dataframe mith scores","99423f20":"## Outliers","f308802b":"Getting the new train and test sets. ","595240d0":"## Target Variable","cdda90fa":"**Import libraries**","8f394e11":"**SalePrice** is the variable we need to predict. So let's do some analysis on this variable first.","e3a4c6c2":"Load train and test datasets","45967ed5":"This competition is very important to me as  it helped me to gain alot of knownledge and experience with mutlifeatured regression. I've read a lot of great notebooks but these two has inspired me to write this kernel.\n\n1. [Stacked Regressions to predict House Prices][2] \n2. [Regularized Linear Models][3] by **Alexandru Papiu**  : Great Starter kernel on modelling and Cross-validation\n\nThe overall approach is  hopefully concise and easy to follow.. \n\nIt is pretty much :\n\n- **Imputing missing values**  by proceeding sequentially through the data\n\n- **Transforming** some numerical variables that seem really categorical\n\n- **Label Encoding** some categorical variables that may contain information in their ordering set\n\n-  [**Box Cox Transformation**][4] of skewed features (instead of log-transformation).\n\n- **Getting dummy variables** for categorical features. \n- **Modeling** \n\nThen we choose many base models (mostly sklearn based models + sklearn API of  DMLC's [XGBoost][5], cross-validate them on the data, find the model with the best score and submit the results.\n\n  [1]: https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\n  [2]:https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\n  [3]: https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models\n  [4]: http:\/\/onlinestatbook.com\/2\/transformations\/box-cox.html\n  [5]: https:\/\/github.com\/dmlc\/xgboost\n [6]: https:\/\/github.com\/Microsoft\/LightGBM\n [7]: https:\/\/www.kaggle.com\/humananalog\/xgboost-lasso","4fa1a306":"Save the Id column","37b9f08a":"**Transforming some numerical variables that are really categorical**","870ada84":"The skew seems now corrected and the data appears more normally distributed. ","9a6886db":"- **MSZoning (The general zoning classification)** :  'RL' is by far  the most common value.  So we can fill in missing values with 'RL'","11fe38ae":"We can see at the bottom right two with extremely large GrLivArea that are of a low price. These values are huge oultliers.\nTherefore, we can safely delete them.","1b2c8324":"### More features engeneering","188925e8":"- **LotFrontage** : Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can **fill in missing values by the median LotFrontage of the neighborhood**.","cd506621":"- **PoolQC** : data description says NA means \"No  Pool\". That make sense, given the huge ratio of missing value (+99%) and majority of houses have no Pool at all in general. ","efd40ecf":"**Data Correlation**","ac9724c5":"- **MSSubClass** : NA most likely means No building class. We can replace missing values with None","a33f5e00":"- **Fence** : data description says NA means \"no fence\"","12ab0354":"- **GarageType, GarageFinish, GarageQual and GarageCond** : Fill missing data with None","5bccfd3c":"Note the U-ish shaped curve above. When alpha is too large the regularization is too strong and the model cannot capture all the complexities in the data. If however we let the model be too flexible (alpha small) the model begins to overfit. A value of alpha = 10 is about right based on the plot above.","edf592a6":"Check remaining missing values if any ","769ab3b2":"Lets explore these outliers","ab859f51":"Adding an xgboost model to see if we can improve our score","ea483f9a":"Let' try out the Lasso model. We will do a slightly different approach here and use the built in Lasso CV to figure out the best alpha for us. For some reason the alphas in Lasso CV are really the inverse for the alphas in Ridge.\nLets make it more robust on outliers using the same technique as above","8865d68d":"### Note : \nOutliers removal is note always safe.  We decided to delete these two as they \nare very huge and  really  bad ( extremely large areas for very low  prices). \nThere are probably others outliers in the training data.   \nHowever, removing all them  may affect badly our models if ever there were also\noutliers  in the test data. That's why , instead of removing them all, we will \njust manage to make some of our  models robust on them. You can refer to  the \nmodelling part of this notebook for that. ","58180c47":"<h1>Data Processing<\/h1>","ebd7bd30":"**Box Cox Transformation of (highly) skewed features**","46115962":"Get an overview of the data","ef3aec27":"<p>One thing to note here however is that the features selected are not necessarily the \"correct\" ones - especially since there are a lot of collinear features in this dataset. One idea to try here is run Lasso a few times on boostrapped samples and see how stable the feature selection is.<\/p>\n<p> We can also take a look directly at what the most important coefficients are: <p>","3597f179":"- **GarageYrBlt, GarageArea and GarageCars** : Replacing missing data with 0 (Since No garage = no cars in such garage.)"}}