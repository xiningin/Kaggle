{"cell_type":{"78ed638b":"code","4f203b82":"code","c85d3342":"code","5e3ad105":"code","8c59b65d":"code","0df9b57a":"code","6299bd02":"code","65cb6aab":"code","b1eb1a96":"code","b7dfc471":"code","a184e163":"code","8b29fffa":"code","cb022989":"code","a2049cd0":"code","1a5cc37d":"code","54c6b2fd":"code","45017583":"code","15941503":"code","81503f67":"code","b0d8ce85":"code","e6f4fa7a":"code","28c529f6":"code","022b079d":"code","31ce72fe":"code","ee741a61":"code","4493fce3":"code","cc88538a":"code","56091cd4":"code","5ae1c093":"code","b45d300b":"code","0eb20dea":"code","3448a3e0":"code","eaa7eddd":"code","a12c8874":"code","235ee470":"code","6379bee9":"code","459f525c":"code","dbe405c8":"code","e97b24fd":"code","6bf53c30":"code","65927c2c":"code","ffa28a3a":"code","98d6fbb4":"code","ab98dd6d":"code","b04f4966":"code","6108a777":"code","3017f91d":"code","876d8f2a":"code","533e9494":"code","b44b497b":"code","ea91a40f":"code","13d591f3":"code","83faf586":"code","f45312ca":"code","d6b94fca":"code","a074f2cb":"code","3896d0f4":"code","e86b7c9e":"code","bf02ad8d":"code","9fc04504":"markdown","451ed43a":"markdown","9fb364c5":"markdown","c4d8972d":"markdown","6cdf85ff":"markdown","d49b67e6":"markdown","0403978a":"markdown","95fdc54a":"markdown","62398570":"markdown","e29b7468":"markdown","f107b289":"markdown","8b2b070a":"markdown","3807c5a8":"markdown","d2e2157d":"markdown","3de4e9b2":"markdown","fd340a79":"markdown","da94be15":"markdown"},"source":{"78ed638b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport warnings\nwarnings.simplefilter(action='ignore')","4f203b82":"path = '\/kaggle\/input\/vehicle-dataset-from-cardekho\/Car details v3.csv'\ndf = pd.read_csv(path)","c85d3342":"df.head()","5e3ad105":"df.shape","8c59b65d":"df.describe()","0df9b57a":"df.duplicated().any()","6299bd02":"df = df.drop_duplicates()\ndf.shape","65cb6aab":"df.info()","b1eb1a96":"df.drop(['torque'], axis=1, inplace=True)\ndf.head()","b7dfc471":"#Check missing values\ndf.isnull().any()","a184e163":"#Missing values in Percentage of the Total Sample\ndf.isnull().sum() \/ df.shape[0] * 100","8b29fffa":"#Having more than 6000 samples, only 3% data is missing at max hence dropping such rows\ndf.dropna(axis=0, inplace=True)\ndf.isnull().any()","cb022989":"df.shape","a2049cd0":"#Removing units to handle the column as float columns\n#Meethod-1\ndef remove_unit(df,colum_name) :\n    t = []\n    for i in df[colum_name]:\n        number = str(i).split(' ')[0]\n        t.append(number)\n    return t\n","1a5cc37d":"df['engine'] = remove_unit(df,'engine')\ndf['mileage'] = remove_unit(df,'mileage')\ndf['max_power'] = remove_unit(df,'max_power')\n\ndf['engine'] = pd.to_numeric(df['engine'])\ndf['mileage'] = pd.to_numeric(df['mileage'])\ndf['max_power'] = pd.to_numeric(df['max_power'])\ndf.head()","54c6b2fd":"type(df['engine'][0])","45017583":"#Adding 'age' feature to know how old the car is and dropping 'year' feature as it is useless now\ndf['age'] = 2021 - df['year']\ndf.drop(['year'],axis = 1,inplace = True)","15941503":"print(df['fuel'].unique())\nprint(df['seller_type'].unique())\nprint(df['transmission'].unique())\nprint(df['owner'].unique())","81503f67":"#Ordinal encoding\ndf['owner'] = df['owner'].replace({'First Owner': 1, 'Second Owner': 2, 'Third Owner': 3, 'Fourth & Above Owner': 4, 'Test Drive Car': 5})\ndf.head()","b0d8ce85":"df['seats'].unique()","e6f4fa7a":"# Converting the datatype of 'seats' to string object since it is a categorical data\ndf['seats'] = df['seats'].astype(str)","28c529f6":"fig = make_subplots(rows=3, cols=2,subplot_titles=(\"Selling Price in Rupee\", \"Total KM Driven\", \"Fuel Efficiency in KM per litre\",\n                                                   \"Engine CC\", \"Brake Horse Power(BHP)\", \"Age of Car\",\"Number of Seats\"))\n\nfig.add_trace(go.Histogram(x=df['selling_price'], name=\"Rupee\"), row=1, col=1)\n\nfig.add_trace(go.Histogram(x=df['km_driven'], name=\"KM\"), row=1, col=2)\n\nfig.add_trace(go.Histogram(x=df['mileage'], name=\"KM\/L\"), row=2, col=1)\n\nfig.add_trace(go.Histogram(x=df['engine'], name=\"CC\"), row=2, col=2)\n\nfig.add_trace(go.Histogram(x=df['max_power'], name=\"BHP\"), row=3, col=1)\n\nfig.add_trace(go.Histogram(x=df['age'], name=\"Years\"), row=3, col=2)\n\nfig.update_layout(height=1400, width=800, title_text=\"Distribution of numerical data\")\nfig.show()","022b079d":"fig = make_subplots(rows=3, cols=2,subplot_titles=(\"Selling Price in Rupee\", \"Total KM Driven\", \"Fuel Efficiency in KM per litre\",\n                                                   \"Engine CC\", \"Brake Horse Power(BHP)\", \"Age of Car\",\"Number of Seats\"))\n\nfig.add_trace(go.Box(x=df['selling_price'], name=\"Rupee\"), row=1, col=1)\n\nfig.add_trace(go.Box(x=df['km_driven'], name=\"KM\"), row=1, col=2)\n\nfig.add_trace(go.Box(x=df['mileage'], name=\"KM\/L\"), row=2, col=1)\n\nfig.add_trace(go.Box(x=df['engine'], name=\"CC\"), row=2, col=2)\n\nfig.add_trace(go.Box(x=df['max_power'], name=\"BHP\"), row=3, col=1)\n\nfig.add_trace(go.Box(x=df['age'], name=\"Years\"), row=3, col=2)\n\nfig.update_layout(height=1400, width=800, title_text=\"Distribution of numerical data\")\nfig.show()","31ce72fe":"count_fuel = df['fuel'].value_counts().reset_index()\ncount_fuel = count_fuel.rename(columns = {'index':'fuel','fuel':'count'})\n\ncount_seller = df['seller_type'].value_counts().reset_index()\ncount_seller = count_seller.rename(columns = {'index':'seller_type','seller_type':'count'})\n\ncount_transmission = df['transmission'].value_counts().reset_index()\ncount_transmission = count_transmission.rename(columns = {'index':'transmission','transmission':'count'})\n\ncount_owner = df['owner'].value_counts().reset_index()\ncount_owner = count_owner.rename(columns = {'index':'owner','owner':'count'})\n\ncount_seats = df['seats'].value_counts().reset_index()\ncount_seats = count_seats.rename(columns = {'index':'seats','seats':'count'})","ee741a61":"sns.heatmap(df.corr(), annot=True, cmap=\"RdBu\")\nplt.show()","4493fce3":"sns.pairplot(df)","cc88538a":"# Make a copy of the data for modelling\ndf_model = df.copy()\n\n# Create the 'brand' column by splitting the 'name' column\ndf_model['brand'] = df_model['name'].str.split(' ').str.get(0)\ndf_model.drop(['name'],axis=1,inplace=True)\n\n# Filter the outlier and log-transform the target variable('selling_price')\ndf_model = df_model[df_model['selling_price'] < 2500000]\ndf_model['selling_price'] = np.log(df_model['selling_price'])\n\n# Filter the outlier in 'km_driven' feature\ndf_model = df_model[df_model['km_driven'] < 300000]\n\n# Filter the unwanted rows in 'fuel' feature\ndf_model = df_model[~df_model['fuel'].isin(['CNG','LPG'])]\n\n# Filter the outliers in 'mileage' feature\ndf_model = df_model[(df_model['mileage'] > 5) & (df_model['mileage'] < 35)]\n\n# Filter the outlier in 'max_power' feature and log-transform the data.\ndf_model = df_model[df_model['max_power'] < 300]\ndf_model['max_power'] = np.log(df_model['max_power'])\n\n# Log-transform the 'age' feature data.\ndf_model['age'] = np.log(df_model['age'])\n\ndf_model.head()","56091cd4":"print(df_model['brand'].unique())","5ae1c093":"df_model = pd.get_dummies(data = df_model, drop_first=True)\ndf_model.head()","b45d300b":"df_model.columns","0eb20dea":"X = df_model.drop(['selling_price'],axis=1)\ny = df_model['selling_price']","3448a3e0":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)\nprint(\"x train: \",X_train.shape)\nprint(\"x test: \",X_test.shape)\nprint(\"y train: \",y_train.shape)\nprint(\"y test: \",y_test.shape)","eaa7eddd":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nnum_var = ['km_driven', 'mileage', 'engine', 'max_power', 'age']\nX_train[num_var] = scaler.fit_transform(X_train[num_var])\nX_test[num_var] = scaler.transform(X_test[num_var])","a12c8874":"from sklearn.feature_selection import RFE\nfrom sklearn.ensemble import RandomForestRegressor\nselect = RFE(RandomForestRegressor(n_estimators=100, random_state=42), n_features_to_select=40)\nselect.fit(X_train, y_train)\nX_train_rfe= select.transform(X_train)\nX_test_rfe= select.transform(X_test)","235ee470":"from sklearn.metrics import r2_score\nfrom sklearn.model_selection import cross_val_score\n\nr2_train_scores = []\nr2_test_scores = []\ncv_mean = []\n\ndef car_price_prediction_model(model):\n    model.fit(X_train, y_train)\n    \n    #R2 score of training set\n    y_train_pred = model.predict(X_train)\n    r2_train = r2_score(y_train, y_train_pred)\n    r2_train_scores.append(round(r2_train,2))\n    \n    #R2 score of test set\n    y_test_pred = model.predict(X_test)\n    r2_test = r2_score(y_test, y_test_pred)\n    r2_test_scores.append(round(r2_test,2))\n    \n    # CV score of training set\n    cv_training = cross_val_score(model, X_train, y_train, cv=5)\n    cv_mean_training = cv_training.mean()\n    cv_mean.append(round(cv_mean_training,2))\n    \n    # Printing each score\n    print(\"Training set R2 scores: \",round(r2_train,2))\n    print(\"Test set R2 scores: \",round(r2_test,2))\n    print(\"Training cross validation score: \", cv_training)\n    print(\"Training cross validation mean score: \",round(cv_mean_training,2))\n    \n    fig, ax = plt.subplots(1,2,figsize = (10,4))\n    ax[0].set_title('Residual Plot of Train samples')\n    sns.distplot((y_train-y_train_pred),hist = False,ax = ax[0])\n    ax[0].set_xlabel('y_pred')\n    \n    # Y_test vs Y_train scatter plot\n    ax[1].set_title('y_test vs y_pred_test')\n    ax[1].scatter(x = y_test, y = y_test_pred)\n    ax[1].set_xlabel('y_test')\n    ax[1].set_ylabel('y_pred_test')\n    \n    plt.show()","6379bee9":"from sklearn.metrics import r2_score\nfrom sklearn.model_selection import cross_val_score\n\nr2_train_scores_rfe = []\nr2_test_scores_rfe = []\ncv_mean_rfe = []\n\ndef car_price_prediction_model_rfe(model):\n    model.fit(X_train_rfe, y_train)\n    \n    \n    #R2 score of RFE training set\n    y_train_pred_rfe = model.predict(X_train_rfe)\n    r2_train_rfe = r2_score(y_train, y_train_pred_rfe)\n    r2_train_scores_rfe.append(round(r2_train_rfe,2))\n    \n    #R2 score of RFE test set\n    y_test_pred_rfe = model.predict(X_test_rfe)\n    r2_test_rfe = r2_score(y_test, y_test_pred_rfe)\n    r2_test_scores_rfe.append(round(r2_test_rfe,2))\n\n    # CV score of RFE training set\n    cv_training_rfe = cross_val_score(model, X_train_rfe, y_train, cv=5)\n    cv_mean_training_rfe = cv_training_rfe.mean()\n    cv_mean_rfe.append(round(cv_mean_training_rfe,2))\n    \n    # Printing each score\n    print(\"Training set R2 scores: \",round(r2_train_rfe,2))\n    print(\"Test set R2 scores: \",round(r2_test_rfe,2))\n    print(\"Training cross validation score: \", cv_training_rfe)\n    print(\"Training cross validation mean score: \",round(cv_mean_training_rfe,2))\n    \n    fig, ax = plt.subplots(1,2,figsize = (10,4))\n    ax[0].set_title('Residual Plot of RFE-Train samples')\n    sns.distplot((y_train-y_train_pred_rfe),hist = False,ax = ax[0])\n    ax[0].set_xlabel('residual')\n    \n    # Y_test vs Y_train scatter plot\n    ax[1].set_title('y_test vs y_pred_test_rfe')\n    ax[1].scatter(x = y_test, y = y_test_pred_rfe)\n    ax[1].set_xlabel('y_test')\n    ax[1].set_ylabel('y_pred_test_rfe')\n    \n    plt.show()","459f525c":"from sklearn.linear_model import LinearRegression\nlm = LinearRegression()\ncar_price_prediction_model(lm)","dbe405c8":"car_price_prediction_model_rfe(lm)","e97b24fd":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import RandomizedSearchCV\n\nrg = Ridge()\nalpha = np.logspace(-3,3,num=14)\nrg_rs = RandomizedSearchCV(estimator=rg, param_distributions=dict(alpha=alpha))\ncar_price_prediction_model(rg_rs)","6bf53c30":"car_price_prediction_model_rfe(rg_rs)","65927c2c":"from sklearn.linear_model import Lasso\nfrom sklearn.model_selection import RandomizedSearchCV\n\nls = Lasso()\nalpha = np.logspace(-3,3,num=14)\nls_rs = RandomizedSearchCV(estimator=ls, param_distributions=dict(alpha=alpha))\ncar_price_prediction_model(ls_rs)","ffa28a3a":"car_price_prediction_model_rfe(ls_rs)","98d6fbb4":"from xgboost import XGBRegressor\nxg = XGBRegressor(verbosity= 0)\n\nn_estimators = [100, 500, 900, 1100, 1500]\nmax_depth = [2, 3, 5, 10, 15]\nbooster=['gbtree','gblinear']\nlearning_rate=[0.05,0.1,0.15,0.20]\nmin_child_weight=[1,2,3,4]\nbase_score=[0.25,0.5,0.75,1]\n\nparameter_grid = {\n    'n_estimators': n_estimators,\n    'max_depth':max_depth,\n    'learning_rate':learning_rate,\n    'min_child_weight':min_child_weight,\n    'booster':booster,\n    'base_score':base_score\n    }\n\nxg_rs = RandomizedSearchCV(estimator=xg, param_distributions=parameter_grid)","ab98dd6d":"car_price_prediction_model(xg_rs)","b04f4966":"car_price_prediction_model_rfe(xg_rs)","6108a777":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\n\nrf = RandomForestRegressor()\n\n# Number of trees in Random forest\nn_estimators=list(range(500,1000,100))\n# Maximum number of levels in a tree\nmax_depth=list(range(4,9,4))\n# Minimum number of samples required to split an internal node\nmin_samples_split=list(range(4,9,2))\n# Minimum number of samples required to be at a leaf node.\nmin_samples_leaf=[1,2,5,7]\n# Number of fearures to be considered at each split\nmax_features=['auto','sqrt']\n\n# Hyperparameters dict\nparam_grid = {\"n_estimators\":n_estimators,\n              \"max_depth\":max_depth,\n              \"min_samples_split\":min_samples_split,\n              \"min_samples_leaf\":min_samples_leaf,\n              \"max_features\":max_features}\n\nrf_rs = RandomizedSearchCV(estimator = rf, param_distributions = param_grid, scoring='neg_mean_squared_error', n_iter=10, cv=5, verbose=2, random_state=42, n_jobs=1)","3017f91d":"car_price_prediction_model(rf_rs)","876d8f2a":"car_price_prediction_model_rfe(rf_rs)","533e9494":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\n\ngb = GradientBoostingRegressor()\n\n# Rate at which correcting is being made\nlearning_rate = [0.001, 0.01, 0.1, 0.2]\n# Number of trees in Gradient boosting\nn_estimators=list(range(500,1000,100))\n# Maximum number of levels in a tree\nmax_depth=list(range(4,9,4))\n# Minimum number of samples required to split an internal node\nmin_samples_split=list(range(4,9,2))\n# Minimum number of samples required to be at a leaf node.\nmin_samples_leaf=[1,2,5,7]\n# Number of fearures to be considered at each split\nmax_features=['auto','sqrt']\n\n# Hyperparameters dict\nparam_grid = {\"learning_rate\":learning_rate,\n              \"n_estimators\":n_estimators,\n              \"max_depth\":max_depth,\n              \"min_samples_split\":min_samples_split,\n              \"min_samples_leaf\":min_samples_leaf,\n              \"max_features\":max_features}\n\ngb_rs = RandomizedSearchCV(estimator = gb, param_distributions = param_grid, scoring='neg_mean_squared_error', n_iter=10, cv=5, verbose=2, random_state=42, n_jobs=1)\n#n_jobs = Number of Cores of the laptop used","b44b497b":"car_price_prediction_model(gb_rs)","ea91a40f":"car_price_prediction_model_rfe(gb_rs)","13d591f3":"algo = [\"LinearRegression(OLS)\",\"LinearRegression(Ridge)\",\"LinearRegression(Lasso)\", \"ExtremeGradientBoostingRegressor\",\"RandomForestRegressor\",\"GradientBoostingRegressor\"]\n\nmodel_eval = pd.DataFrame({'Model': algo,'R Squared(Train)': r2_train_scores,'R Squared(Test)': r2_test_scores, 'CV score mean(Train)': cv_mean})\ndisplay(model_eval)","83faf586":"model_eval_RFE = pd.DataFrame({'Model': algo,'R Squared(Train)': r2_train_scores_rfe,'R Squared(Test)': r2_test_scores_rfe,'CV score mean(Train)': cv_mean_rfe})\ndisplay(model_eval_RFE)","f45312ca":"gb_rs.fit(X_train, y_train)","d6b94fca":"predictions = gb_rs.predict(X_test)","a074f2cb":"predictions","3896d0f4":"sns.distplot(y_test-predictions)","e86b7c9e":"plt.scatter(y_test, predictions)","bf02ad8d":"# import pickle\n# #open the file where you want to store the data\n# file = open('gradient_boosting_regressor_model.pkl', 'wb')\n# #dump information to the file\n# pickle.dump(gb_rs, file)","9fc04504":"# **Feature Selection, Feature Engineering and Data Preparation for Modelling**","451ed43a":"### **4. Extreme Gradient Boosting Regressor**","9fb364c5":"**1. Gradient Boosting Regressor is the model I will choose since it has the highest CV score(91%) which mean it generalize better than other models.**","c4d8972d":"**2. Linear model is also a great model choice if we have computational power constraint since the non-linear model are quite computational expensive.**","6cdf85ff":"# **Regression Modelling and Evaluation**","d49b67e6":"### **5. Random Forest Regressor**","0403978a":"# **Model Evaluation**","95fdc54a":"### **2. Linear Regression(Ridge)**","62398570":"### **Removing Units**","e29b7468":"### **6. Gradient Boosting Regressor**","f107b289":"**3. The automatic feature selection(RFE) did not make significant improvement on all of the models. Hence we do not need it unless computational time is of concern.**","8b2b070a":"### **3. Linear Regression(Lasso)**","3807c5a8":"# **Conclusion**","d2e2157d":"## **Univariate Analysis**","3de4e9b2":"### **1. Linear Regression(Ordinary Least Square)**","fd340a79":"# **EDA**","da94be15":"## **Bivariate\/Multivariate Analysis**"}}