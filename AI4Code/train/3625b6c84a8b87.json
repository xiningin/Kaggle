{"cell_type":{"5fcd6597":"code","82d2b9e3":"code","241343d4":"code","bdcf2f6e":"code","c94b5491":"code","0728b1b8":"code","acc7070d":"code","17eccd39":"code","ed1e289a":"code","0498a8dd":"code","7f526ae1":"code","af111e95":"code","bc6103c0":"code","78a5a9fe":"code","ce69c367":"code","9ac738d1":"code","3590f21c":"code","92c61665":"code","954a8cde":"code","779b3472":"code","4cebafd4":"code","cf20dc45":"code","d4fa6e1e":"markdown","6203af10":"markdown","8366b87c":"markdown"},"source":{"5fcd6597":"#import libraries\n\n# General libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport random\n\n\n#feature selection\nfrom sklearn.feature_selection import mutual_info_regression\nfrom scipy import stats\n\n\n# Pre processing libraries\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler \nimport optuna\n\n\n# Training models\nfrom xgboost import XGBRegressor\n\nfrom sklearn.metrics import mean_squared_error\n\n\n# useful functions\n\n# Utility functions from feature scores\ndef make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n","82d2b9e3":"# Load data\ntrain = pd.read_csv(\"..\/input\/now-you-are-playing-with-power\/train.csv\")\ntest = pd.read_csv(\"..\/input\/now-you-are-playing-with-power\/test.csv\", index_col=0)\n\n# check if there is any missing data\n\nprint('------ cols with null data -------')\ncols_with_missing = [col for col in train.columns\n                     if train[col].isnull().any()]\n\nprint(cols_with_missing)\n\n\n# Preview the data\nprint('-----train data -------')\ndisplay(train.shape)\ndisplay(train.head())\n\nprint('-----test data -------')\ndisplay(test.shape)\ndisplay(test.head())\n\ntragetCol = 'result'","241343d4":"train.index = train['id']\ntrain = train.drop(['id'], axis=1) # drope the Id\n\ny = train['output_gen']\n\ntrain = train.drop(['output_gen'], axis=1)\n\ndisplay(train)\ndisplay(y)\ndisplay(test)","bdcf2f6e":"# fill missing data\n\nfrom sklearn.impute import SimpleImputer\n\ncat_train = train['obs_day']\ncat_test = test['obs_day']\n\nnum_train = train.drop(['obs_day'], axis = 1)\nnum_test = test.drop(['obs_day'], axis = 1)\n\n\n# Imputation\nmy_imputer = SimpleImputer()\n\ncombinedData = pd.concat([num_train,num_test])\n\nimputed_fit = my_imputer.fit(combinedData)\n\nimputed_X_train = pd.DataFrame(my_imputer.transform(num_train))\nimputed_X_test = pd.DataFrame(my_imputer.transform(num_test))\n\n# Imputation removed column names; put them back\nimputed_X_train.columns = num_train.columns\nimputed_X_test.columns = num_test.columns\n\nimputed_X_train.index = num_train.index\nimputed_X_test.index = num_test.index\n\ndisplay(imputed_X_train)\ndisplay(imputed_X_test)\n\nds_train = pd.concat([train['obs_day'],imputed_X_train],axis = 1)\nds_test = pd.concat([test['obs_day'],imputed_X_test],axis = 1)\n\ndisplay(ds_train)\ndisplay(ds_test)","c94b5491":"# check cat col missing data\nprint(cat_train.isnull().any())\nprint(cat_test.isnull().any())\nprint(y.isnull().any())\n\nmy_imputer = SimpleImputer(strategy='constant',fill_value = 'sunday')\n\nds_train = pd.DataFrame(my_imputer.fit_transform(ds_train),index = ds_train.index, columns = ds_train.columns)\nds_test = pd.DataFrame(my_imputer.transform(ds_test),index = ds_test.index, columns = ds_test.columns)\n\nprint(ds_train.isnull().any())\nprint(ds_test.isnull().any())","0728b1b8":"print(ds_train.info())","acc7070d":"k = 0\n\nfor cols in ds_train.columns:\n    if k > 0:\n        ds_train[cols] = ds_train[cols].astype(float, errors = 'raise')\n    else:\n        ds_train[cols] = ds_train[cols].astype(str, errors = 'raise')\n    k +=1\n    \nprint(ds_train.info())","17eccd39":"# distingushing cat and numerical culomns\n\nnum_cols = [cname for cname in ds_train.columns if ds_train[cname].dtype in ['int64', 'float64']]\nprint('numericals: ' ,num_cols)\n\nprint('------------------------------------------------')\ncat_cols = [col for col in ds_train.columns if (ds_train[col].dtype in ['str', 'object'])]\n\nprint('categoricals: ',cat_cols)","ed1e289a":"# check unique values in each columns\n\ntrainUni = []\ntestUni = []\n\nfor i in cat_cols:\n    trainUni.append(ds_train[i].unique())\n    testUni.append(ds_test[i].unique())\n    print(ds_train[i].value_counts())\n    print(ds_test[i].value_counts())","0498a8dd":"# transform cat cols to numericals\n\ncatdata = ds_train[cat_cols].copy()\n\ncattest = ds_test[cat_cols].copy()\n\ncatordata = pd.DataFrame()\ntestordata = pd.DataFrame()\n\nall_Cat = pd.concat([catdata,cattest])\n\nfor cols in all_Cat.columns:\n    category_enc = preprocessing.LabelEncoder()\n    category_enc.fit(all_Cat[cols])\n    catordata[cols] = category_enc.transform(catdata[cols])\n    testordata[cols] = category_enc.transform(cattest[cols])\n\ncatordata.index = catdata.index\ntestordata.index = cattest.index","7f526ae1":"# Scaling the numerical columns\n# scale numerical features\n\nnumdata  = ds_train[num_cols].copy()\nnumtest  = ds_test[num_cols].copy()\n\nstsc = StandardScaler()\n\nst_train = pd.DataFrame(stsc.fit_transform(numdata), columns = numdata.columns, index = numdata.index)\n\nst_test = pd.DataFrame(stsc.transform(numtest), columns = numtest.columns,index = numtest.index)\n\n\n\n\nfeatures =  pd.concat([catordata,st_train], axis=1)\n\nfeatures.columns = ds_train.columns\n\ntest_fe =  pd.concat([testordata,st_test], axis=1)\ntest_fe.columns = ds_test.columns\n\nprint(\"scaled and ordinal transformed features-------\")\n\ndisplay(features)\n\nprint(\"scaled and ordinal transformed test data-------\")\ndisplay(test_fe)","af111e95":"# implementing one hot encoder\n\n\nX_model = features.copy()\nX_testing = test_fe.copy()\n\nOneHot_cal= cat_cols\n\n#OneHot_cal.remove('categoryA') # too much categories\n\n#OneHot_cal.remove('categoryC') # too much categories\n\n#OneHot_cal.remove('unit') # probably data-linkage\n\n# one-Hot encoder\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n# Apply one-hot encoder to each column with categorical data\nXoh = pd.DataFrame(OH_encoder.fit_transform(X_model[OneHot_cal]))\nXoh_test = pd.DataFrame(OH_encoder.transform(X_testing[OneHot_cal]))\n\n# One-hot encoding removed index; put it back\nXoh.index = X_model.index\nXoh_test.index = X_testing.index\n\n# reindexing the columns\nhotcols = []\n\nfor cols in OneHot_cal:\n    \n    for cols2 in X_model[cols].unique():\n        \n        hotcols.append(str(cols) + \"_\" + str(cols2))\n\nXoh.columns = hotcols\n\n# For test data we have some more unique values\n\ntesthotcols = []\n\nXoh_test.columns = hotcols  \n\n# Remove categorical columns (will replace with one-hot encoding)\n#X_model = X_model.drop(OneHot_cal, axis=1)\n#X_testing = X_testing.drop(OneHot_cal, axis=1)\n\n# Add one-hot encoded columns to numerical features\n\nX_data = pd.concat([X_model, Xoh], axis=1)\nX_test = pd.concat([X_testing, Xoh_test], axis=1)\n\nprint(\"----One hote encoder columns of train data------\")\nprint(Xoh.columns)\n\nprint(\"----One hote encoder columns of test data-------\")\nprint(Xoh_test.columns)","bc6103c0":"# calculating and charting mutual information scores\n\n#mi_scores = make_mi_scores(X_data, y)\n\n#print('------mutual scores------')\n\n#print(mi_scores.head(10))\n\n#print(mi_scores.describe())","78a5a9fe":"# select features\n#selq = 0.25\n#print('selection threshold:', mi_scores.quantile(q=selq))\n\n#selectCols = [cals for cals in mi_scores.index if mi_scores[cals] > mi_scores.quantile(q=selq) ]\n#print('number of selected cols:', len(selectCols))\n\n#print(selectCols)","ce69c367":"#Defining and presenting all features that we want them to contribute in the model\n\ntrain_data = X_data.copy() #X_data[selectCols]\ntest_data = X_test.copy() #X_test[selectCols]\n\nprint('train shape: ', train_data.shape)\nprint('test shape: ', test_data.shape)","9ac738d1":"# XGBOX model for predicting the target\nX_train, X_valid, y_train, y_valid = train_test_split(train_data, y, test_size=0.25,random_state=0)\n\n# we have to check if spliting the validation data specifically for XGBOX will have any contribution to final overffiting\nX_train2, X_valid2, y_train2, y_valid2 = train_test_split(X_train, y_train, test_size=0.05,random_state=0)\n\n'''\nmodel = XGBRegressor(n_estimators=500, learning_rate=0.05, n_jobs=4)\n\nmodel.fit(X_train2, y_train2,\n          early_stopping_rounds=5,eval_set=[(X_valid2, y_valid2)],verbose=False)\n\npreds_valid = model.predict(X_valid)\n\nprint(mean_squared_error(y_valid, preds_valid, squared=True))\n\n'''","3590f21c":"# parameter optimization\n\n\ndef XGBRobjective(trial):\n \n    param = {\n        'tree_method' : 'gpu_hist',\n        'gpu_id' : 0,\n        'predictor' : 'gpu_predictor',\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02]),\n        'n_estimators': trial.suggest_categorical('n_estimators', [100,500,1000,1500,2000,2500,3000, 3500,4000]),\n        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17,20]),\n        'random_state': trial.suggest_categorical('random_state', [24, 48,2020]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n    }\n    \n    model = XGBRegressor(**param)  \n\n    kf = KFold(n_splits=5,random_state=random.randint(10, 200), shuffle = True)\n\n    scores = []\n\n    for k, (train_rows, valid_rows) in enumerate(kf.split(X_train, y_train)):\n        \n        train_index = X_train.index[train_rows]\n        valid_index = X_train.index[valid_rows]\n\n        Xtrain = X_train.loc[train_index,:]\n        ytrain = y_train.loc[train_index]\n        Xvalid = X_train.loc[valid_index,:]\n        yvalid = y_train.loc[valid_index]\n\n        model.fit(Xtrain, ytrain,\n                  early_stopping_rounds=100,eval_set=[(Xvalid, yvalid)],verbose=False)\n\n        preds_Kflod = model.predict(X_valid)\n        score = mean_squared_error(y_valid, preds_Kflod, squared=True)\n        scores.append(score)\n        print('Fold: %s, Acc: %.5f' % (k+1, score))\n        \n    return np.mean(scores)\n    \n\n\nstudy = optuna.create_study()\nstudy.optimize(XGBRobjective, n_trials=15)\n\nXGBRparms = study.best_params \n\nprint(XGBRparms)\n\n","92c61665":"\n# One of the parameter optimization result, for time that you dont want to run the experiment\n'''\nXGBRparms={'lambda': 0.005034510515662878,\n     'alpha': 0.3435975721343005,\n     'colsample_bytree': 0.9,\n     'subsample': 0.8,\n     'learning_rate': 0.02, \n     'n_estimators': 300,\n     'max_depth': 17,\n     'random_state': 48,\n     'min_child_weight': 1}\n     \n'''","954a8cde":"# k-flod\nparam = {'tree_method' : 'gpu_hist',\n         'gpu_id' : 0,\n         'predictor' : 'gpu_predictor'}\n    \nparam.update(XGBRparms)\n  \n\nsplits = 5\n\nkf = KFold(n_splits=splits,random_state=random.randint(10, 200), shuffle = True)\n\nscores = []\n\nPred_test = pd.DataFrame(columns=['fold_' + str(cols) for cols in range(splits)])\n\nfor k, (train_rows, valid_rows) in enumerate(kf.split(X_train, y_train)):\n        \n    train_index = X_train.index[train_rows]\n    valid_index = X_train.index[valid_rows]\n\n    Xtrain = X_train.loc[train_index,:]\n    ytrain = y_train.loc[train_index]\n    Xvalid = X_train.loc[valid_index,:]\n    yvalid = y_train.loc[valid_index]\n    \n    model = XGBRegressor(**param)\n    model.fit(Xtrain, ytrain,\n              early_stopping_rounds=100,eval_set=[(Xvalid, yvalid)],verbose=False)\n\n    preds_Kflod = model.predict(X_valid)\n    score = mean_squared_error(y_valid, preds_Kflod, squared=True)\n    scores.append(score)\n    print('Fold: %s, Acc: %.5f' % (k+1, score))\n    \n    Pred_test['fold_' + str(k)] = model.predict(test_data)\n    \n    \nPred_test","779b3472":"# Use the model to generate predictions\n\npredictions = Pred_test.mean(axis=1)\n\npredictions","4cebafd4":"output = pd.DataFrame({'id': test.index,\n                       'output_gen': predictions})\n\noutput","cf20dc45":" #Save the predictions to a CSV file\n\n\noutput.to_csv('submission.csv', index=False)\n\nprint('--done--')","d4fa6e1e":"# construct models","6203af10":"# submission","8366b87c":"# Load and review data"}}