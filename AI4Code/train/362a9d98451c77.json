{"cell_type":{"7edfb557":"code","a47e953e":"code","5061aa9b":"code","078d803a":"code","e84e27dd":"code","5572f6ee":"code","682bd796":"code","0e612f7b":"code","4c9ff9e4":"code","96fa8a84":"code","8175b18f":"code","580babbd":"code","8aa4cbbd":"code","e2cdbb0e":"code","2868cce7":"code","c3fa2ae3":"code","2a9dbc90":"code","6b29c5fe":"code","288b0eee":"code","fac9cf28":"code","783bf9dc":"code","0019afc0":"code","9433c353":"code","cbbbb30f":"code","66d26245":"code","b351ccdd":"code","3f271fb3":"code","e219f490":"code","8f225ea1":"code","b2f55e18":"code","78cf0059":"code","eaba833b":"markdown","812970fd":"markdown","b2748218":"markdown","100d4fae":"markdown","1f4547a6":"markdown","89eea83b":"markdown","fcbc9bdc":"markdown","8629bcd9":"markdown","9a60059b":"markdown","f70ae450":"markdown","8471cd8b":"markdown","52f66383":"markdown","42b226f0":"markdown","b623f3ab":"markdown","4975338e":"markdown","d31ac4ad":"markdown","15976c7d":"markdown","45b071c4":"markdown","253a7a87":"markdown"},"source":{"7edfb557":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a47e953e":"!pip install dtreeviz","5061aa9b":"from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn import tree as sklearn_tree\n\nfrom dtreeviz.models.sklearn_decision_trees import ShadowSKDTree\nfrom dtreeviz import trees\n\nimport graphviz\nimport matplotlib.pyplot as plt\n","078d803a":"training = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntraining","e84e27dd":"# for model training consistency\nrandom_state = 1234\n\n# fill missing values\ntraining.fillna({\"Age\":training.Age.mean()}, inplace=True)\n\n# Encode categorical variables\ntraining[\"Sex_label\"] = training.Sex.astype(\"category\").cat.codes\ntraining[\"Cabin_label\"] = training.Cabin.astype(\"category\").cat.codes\ntraining[\"Embarked_label\"] = training.Embarked.astype(\"category\").cat.codes\n","5572f6ee":"training","682bd796":"features = [\"Pclass\", \"Age\"]\ntarget = \"Survived\"\ntree_classifier = DecisionTreeClassifier(max_depth=2, random_state=random_state)\ntree_classifier.fit(training[features], training[target])","0e612f7b":"# the most common visualisation from scikit-learn library for decision trees\nfig = plt.figure(figsize=(10,7))\ndot_data = sklearn_tree.plot_tree(tree_classifier, feature_names=features,\n                                                filled=True, node_ids=True)","4c9ff9e4":"# for dtreeviz, we need to initialize an internal shadow tree, which can be used for all visualizations.\nsk_dtree = ShadowSKDTree(tree_classifier, training[features], training[target], features, target, [\"no\", \"yes\"])\ntrees.dtreeviz(sk_dtree, show_node_labels=False)","96fa8a84":"features = [\"Pclass\", \"Age\", \"Sex_label\", \"Fare\", \"Embarked_label\"]\ntarget = \"Survived\"\ntree_classifier = DecisionTreeClassifier(max_depth=4, random_state=random_state)\ntree_classifier.fit(training[features], training[target])","8175b18f":"fig = plt.figure(figsize=(60,30))\ndot_data = sklearn_tree.plot_tree(tree_classifier, feature_names=features,\n                                                filled=True, node_ids=True)","580babbd":"sk_dtree = ShadowSKDTree(tree_classifier, training[features], training[target], feature_names=features, target_name=target, class_names=[\"no\", \"yes\"])\ntrees.dtreeviz(sk_dtree)","8aa4cbbd":"features = [\"Pclass\", \"Age\", \"Sex_label\", \"Fare\", \"Embarked_label\"]\ntarget = \"Survived\"\ntree_classifier = DecisionTreeClassifier(max_depth=7, random_state=random_state)\ntree_classifier.fit(training[features], training[target])","e2cdbb0e":"fig = plt.figure(figsize=(60,30))\ndot_data = sklearn_tree.plot_tree(tree_classifier, feature_names=features,\n                                                filled=True, node_ids=True)","2868cce7":"sk_dtree = ShadowSKDTree(tree_classifier, training[features], training[target], feature_names=features, target_name=target, class_names=[\"no\", \"yes\"])\ntrees.dtreeviz(sk_dtree)","c3fa2ae3":"# Gini purity for each leaf\ntrees.viz_leaf_criterion(sk_dtree, figsize=(15, 7))","2a9dbc90":"# leaves purities distribution\ntrees.viz_leaf_criterion(sk_dtree, display_type=\"hist\", bins=50, figsize=(15, 7))","6b29c5fe":"# number of samples for each leaf, grouped by target class values\ntrees.ctreeviz_leaf_samples(sk_dtree, figsize=(15, 7))","288b0eee":"# leaves samples distribution\ntrees.viz_leaf_samples(sk_dtree, display_type=\"hist\", bins=50, figsize=(15, 7))","fac9cf28":"# get all samples from a node and make some stats on them\ntrees.describe_node_sample(sk_dtree, node_id=92)","783bf9dc":"trees.describe_node_sample(sk_dtree, node_id=17)","0019afc0":"x = training[features].iloc[3]\nx","9433c353":"# get only the nodes from prediction path\ntrees.dtreeviz(sk_dtree, X=x, show_just_path=True)","cbbbb30f":"# based on predicion path splitting values\nprint(trees.explain_prediction_path(sk_dtree, x, explanation_type=\"plain_english\"))","66d26245":"# features' importance based only on the predition path node\ntrees.explain_prediction_path(sk_dtree, x, explanation_type=\"sklearn_default\")","b351ccdd":"features_reg = [\"Pclass\", \"Survived\", \"Sex_label\", \"Fare\", \"Embarked_label\"]\ntarget_reg = \"Age\"\ntree_regressor = DecisionTreeRegressor(max_depth=3, criterion=\"mae\", random_state=random_state)\ntree_regressor.fit(training[features_reg], training[target_reg])","3f271fb3":"sk_dtree_reg = ShadowSKDTree(tree_regressor, training[features_reg], training[target_reg], feature_names=features_reg, target_name=target_reg)","e219f490":"trees.dtreeviz(sk_dtree_reg)","8f225ea1":"# visualize number of samples from each leaf.\ntrees.viz_leaf_samples(sk_dtree_reg)","b2f55e18":"# visualize the error criteria for each leaf, in our case MAE\ntrees.viz_leaf_criterion(sk_dtree_reg)","78cf0059":"# visualize leaf target distribution with blue circles and the leaf prediction with small vertical black lines. \ntrees.viz_leaf_target(sk_dtree_reg)","eaba833b":"## Leaves purities visualisation","812970fd":"# Decision Tree Regressor\n\nLet's see what types of visualizations we can make for a decision tree regressor.","b2748218":"By interpreting above visualisations, we can see that majority of them have a good purity, but based on very few samples. This is a clear sign of overfiting. <br>\nIn decision trees, the most popular way to reduce overfiting is to reduce the max_depth value and\/or increase the min_samples_split.\n","100d4fae":"Decision tree structure using dtreeviz library![image.png](attachment:image.png)","1f4547a6":"Majority samples from leaf 92 didn't survived(look at ctreeviz_leaf_samples() visualisation). From above stats, we can see that majority are from pclass 3 (lower social economic status), all are men, middle age and paid a low price for the ticket.","89eea83b":"All the samples from leaf 17 survived. They have a high social economic status (mean of pclass is close to 1), all are women, middle age and paid more money on the ticket.\n\nAs we can see, we can get a lot of domain knowledge about the dataset, by interpreting the node samples. To get domain knowledge is one of the most important skill in ML, because it can help to add new features and to better understand the model predictions.","fcbc9bdc":"# Decision Tree Classifier\n\nThe goal of this decision tree model is to classify if a person survived(1)\/didn't survived(0) on Titanic.\n\nLet's start with a small decision tree and see how its structure look.\n","8629bcd9":"As we can see, when the depth of the tree increase, its structure will be harder to interpret, mostly because we have a lot of levels with a lot of nodes. \n\nTree's performance is given by the performance of each individual leaf. So, would be very helpful to understand\/visualise what's happening in the leaves, because they are making the final predictions. <br>\n\nA leaf contains information about the number of samples and its purity. Let's explain what this mean and also to make some visualisations with them.\n\n\n**Purity** <br>\n\nThe goal for splitting a node (classification) is to create another two nodes as pure as possible. Purity measures the distribution of target class values in each node.\n\nTo measure the node purity, the most popular formulas are Gini and Entropy. <br>\nGini can take values between [0, 0.5] and Entropy between [0, 1]\n\nGini (the explanation is the same for Entropy)\n- if we have a node with gini value = 0, then that node contains only values from a single target class (ideal scenario)\n- if the value is 0.5, then that node contains values from all target classes in equal proportions (worst scenario)\n- values between 0 and 0.5 indicates how pure or impure are the node samples.\n","9a60059b":"For the next visualisations, we will compare the default scikit-learn vs dtreeviz tree structure visualisation, using different set of features and different values for max_depth.","f70ae450":"## Leaves samples visualisation","8471cd8b":"We have a lot of leaves with purity close or very close to 0. This is the ideal scenario, but we need to take into account also the number of samples from these leaves. If we have a leaf with purity 0, but only very few samples into it, we cannot be very confident for its predictions, because it's based on few samples. It can be a clear sign of overfitting, the most undesirable situation for each ML engineer :)\n","52f66383":"# New features for dtreeviz\n- add LightGBM library\n- include support for esemble trees\n- better efficiency when dealing with bigger datasets ","42b226f0":"Getting more information about the prediction of a sample, x, we can explain to someone else(non technical person, our clients) why the model made that specific prediction. \n\nBased on our previous example, we can say that the person had a big chance to survive mostly because it was a women, had a good social economic status and paid between paid between [28.86, 149.04]","b623f3ab":"## Get node samples\n\nIt's helpful to get all samples from a node and to investigate the values. In this way, we can descover some patters into the data.\n\nLet's investigate some interesting leaves, like leaf id 92, 17","4975338e":"The goal of both visualisations is to make us understand the tree structure and they are both useful, but I prefer the dtreeviz one because :\n- each node presents information in a visual way vs plain text.\n- from each node, we can easily see the distribution of values from node samples.\n","d31ac4ad":"The goal of this notebook is to help us understand how decision trees are built and to better interpretate their structure.<br>\n\nNowadays, the most used and the most performant types of machine learning algoritms are ensemble of decision trees(RandomForest, XGBoost, LightGBM) for structured data and Deep Learning for unstrutured data.\n\nEven if the decision trees are considerd white box models, my feeling is that we still superficially interpret them. Also, the ML community doesn't have a rich set of good libraries to help us better understand what happend during model training. \n\nIn my opinion, the goal of a ML model isn't only to make good predictions, but also to help us to understand the data! We can achieve this by model interpretation.\n\nIn this notebook, we will use the [dtreeviz](https:\/\/github.com\/parrt\/dtreeviz) library, which is a library for decision tree interpretation. It contains explanatory visualisations for tree structure, leaf nodes information, prediction paths and more. <br>\nRight now, dtreeviz supports scikit-learn, xgboost, pyspark mllib libraries. In this notebook, we will use scikit-learn library for model training.\n\nTo better understand to content of this notebook, I basic knowledge of decision trees is necessary :)\n","15976c7d":"## Prediction path","45b071c4":"For regresssion, the goal of node splitting is to create another two subsets with the lowest target variance possible. [Variance](https:\/\/en.wikipedia.org\/wiki\/Variance) measures how spread are the values out from their average. <br>\n\nFor our above use-case, the splitting process tries to find another two subsets where each subset has low variance for Age values. In other words, people of the same age.\n\nIn above visualization, each splitting node represent a scatter plot between features and target value. The vertical dashed line shows the splitting value and the other two horizontal dashed lines show the mean target values for each subset. Leaf nodes visualize the target values from its samples and the horizontal line represents the target mean value, which is the leaf prediction.","253a7a87":"# Dataset preparation\n\nTitanic competition : https:\/\/www.kaggle.com\/c\/titanic\n\nThe competition's goal is to create a model that predicts which passengers survived on Titanic.\n"}}