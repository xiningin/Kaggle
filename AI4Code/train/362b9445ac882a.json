{"cell_type":{"e2e1236e":"code","b70a1c09":"code","2ad50484":"code","2e1b6954":"code","2aac902c":"code","dc8a3bd2":"code","943d94bf":"code","b46fc441":"markdown","5cba9704":"markdown","86cc7d21":"markdown","a37646c8":"markdown","c37cd8a3":"markdown","d0445d97":"markdown","aa3015af":"markdown"},"source":{"e2e1236e":"import h5py\nimport numpy as np\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nimport torch\nfrom torch import nn\nimport os\nimport PIL.Image as pil_image\n\nimport argparse\nimport copy\n\nimport torch.optim as optim\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.data.dataloader import DataLoader\nfrom tqdm import tqdm\nimport csv\n\n\n\ndef convert_rgb_to_y(img):\n    if type(img) == np.ndarray:\n        return 16. + (64.738 * img[:, :, 0] + 129.057 * img[:, :, 1] + 25.064 * img[:, :, 2]) \/ 256.\n    elif type(img) == torch.Tensor:\n        if len(img.shape) == 4:\n            img = img.squeeze(0)\n        return 16. + (64.738 * img[0, :, :] + 129.057 * img[1, :, :] + 25.064 * img[2, :, :]) \/ 256.\n    else:\n        raise Exception('Unknown Type', type(img))\n\n\ndef convert_rgb_to_ycbcr(img):\n    if type(img) == np.ndarray:\n        y = 16. + (64.738 * img[:, :, 0] + 129.057 * img[:, :, 1] + 25.064 * img[:, :, 2]) \/ 256.\n        cb = 128. + (-37.945 * img[:, :, 0] - 74.494 * img[:, :, 1] + 112.439 * img[:, :, 2]) \/ 256.\n        cr = 128. + (112.439 * img[:, :, 0] - 94.154 * img[:, :, 1] - 18.285 * img[:, :, 2]) \/ 256.\n        return np.array([y, cb, cr]).transpose([1, 2, 0])\n    elif type(img) == torch.Tensor:\n        if len(img.shape) == 4:\n            img = img.squeeze(0)\n        y = 16. + (64.738 * img[0, :, :] + 129.057 * img[1, :, :] + 25.064 * img[2, :, :]) \/ 256.\n        cb = 128. + (-37.945 * img[0, :, :] - 74.494 * img[1, :, :] + 112.439 * img[2, :, :]) \/ 256.\n        cr = 128. + (112.439 * img[0, :, :] - 94.154 * img[1, :, :] - 18.285 * img[2, :, :]) \/ 256.\n        return torch.cat([y, cb, cr], 0).permute(1, 2, 0)\n    else:\n        raise Exception('Unknown Type', type(img))\n\n\ndef convert_ycbcr_to_rgb(img):\n    if type(img) == np.ndarray:\n        r = 298.082 * img[:, :, 0] \/ 256. + 408.583 * img[:, :, 2] \/ 256. - 222.921\n        g = 298.082 * img[:, :, 0] \/ 256. - 100.291 * img[:, :, 1] \/ 256. - 208.120 * img[:, :, 2] \/ 256. + 135.576\n        b = 298.082 * img[:, :, 0] \/ 256. + 516.412 * img[:, :, 1] \/ 256. - 276.836\n        return np.array([r, g, b]).transpose([1, 2, 0])\n    elif type(img) == torch.Tensor:\n        if len(img.shape) == 4:\n            img = img.squeeze(0)\n        r = 298.082 * img[0, :, :] \/ 256. + 408.583 * img[2, :, :] \/ 256. - 222.921\n        g = 298.082 * img[0, :, :] \/ 256. - 100.291 * img[1, :, :] \/ 256. - 208.120 * img[2, :, :] \/ 256. + 135.576\n        b = 298.082 * img[0, :, :] \/ 256. + 516.412 * img[1, :, :] \/ 256. - 276.836\n        return torch.cat([r, g, b], 0).permute(1, 2, 0)\n    else:\n        raise Exception('Unknown Type', type(img))\n\n\ndef calc_psnr(img1, img2):\n    return 10. * torch.log10(1. \/ torch.mean((img1 - img2) ** 2))\n\n\nclass AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n\n\nclass TrainDataset(Dataset):\n    def __init__(self, image_dir, is_train=1, scale=4):\n        super(TrainDataset, self).__init__()\n        lr_dir = os.path.join(image_dir, \"lr\/\")\n        hr_dir = os.path.join(image_dir, \"hr\/\")\n        self.lr_list = sorted([os.path.join(lr_dir, f) for f in os.listdir(lr_dir)])\n        self.hr_list = sorted([os.path.join(hr_dir, f) for f in os.listdir(hr_dir)])\n        cur_len = len(self.hr_list)\n        train_len = round(cur_len*0.9)\n\n        if is_train == 1:\n            self.lr_list = self.lr_list[:train_len]\n            self.hr_list = self.hr_list[:train_len]\n        else:\n            self.lr_list = self.lr_list[train_len:]\n            self.hr_list = self.hr_list[train_len:]\n\n        self.scale = scale\n        self.crop_size = 33\n        self.is_train = is_train\n\n    def __getitem__(self, idx):\n        image = pil_image.open(self.lr_list[idx]).convert('RGB')\n        image = np.array(image).astype(np.float32)\n        ycbcr = convert_rgb_to_ycbcr(image)\n\n        lr = ycbcr[..., 0]\n        lr \/= 255.\n        lr = torch.from_numpy(lr)\n        h, w = lr.size()\n\n        hr = pil_image.open(self.hr_list[idx]).convert('RGB')\n        hr = np.array(hr).astype(np.float32)\n        ycbcr = convert_rgb_to_ycbcr(hr)\n\n        hr = ycbcr[..., 0]\n        hr \/= 255.\n        hr = torch.from_numpy(hr)\n\n        # random crop\n        if self.is_train:\n            rand_h = torch.randint(h - (self.crop_size), [1, 1])\n            rand_w = torch.randint(w - (self.crop_size), [1, 1])\n            lr = lr[rand_h:rand_h + self.crop_size, rand_w:rand_w + self.crop_size]\n            hr = hr[rand_h:rand_h + self.crop_size, rand_w:rand_w + self.crop_size]\n\n        lr = lr.unsqueeze(0)\n        hr = hr.unsqueeze(0)\n         # lr hr pair\n        return lr, hr\n\n    def __len__(self):\n       return len(self.hr_list)\n\n\nclass TestDataset(Dataset):\n    def __init__(self, image_dir, scale=4):\n        super(TestDataset, self).__init__()\n        lr_dir = os.path.join(image_dir, \"lr\/\")\n        self.lr_list = sorted([os.path.join(lr_dir, f) for f in os.listdir(lr_dir)])\n\n        self.scale = scale\n        self.crop_size = 33\n\n    def __getitem__(self, idx):\n        image = pil_image.open(self.lr_list[idx]).convert('RGB')\n        image = np.array(image).astype(np.float32)\n        ycbcr = convert_rgb_to_ycbcr(image)\n\n        lr = ycbcr[..., 0]\n        lr \/= 255.\n        lr = torch.from_numpy(lr)\n\n        lr = lr.unsqueeze(0)\n\n        return lr\n\n    def __len__(self):\n       return len(self.lr_list)","b70a1c09":"class SRCNN(nn.Module):\n    def __init__(self, num_channels=1):\n        super(SRCNN, self).__init__()\n        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=9, padding=9 \/\/ 2)\n        self.conv2 = nn.Conv2d(64, 32, kernel_size=5, padding=5 \/\/ 2)\n        self.conv3 = nn.Conv2d(32, num_channels, kernel_size=5, padding=5 \/\/ 2)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.relu(self.conv2(x))\n        x = self.conv3(x)\n        return x","2ad50484":"\n## fixed options (\uc218\uc815\ud558\uc9c0 \ub9d0 \uac83)\nscale = 4\ntrain_dir = '\/kaggle\/input\/jbnu-bigdathub-super-resolution\/DB\/train\/'\ntest_dir = '\/kaggle\/input\/jbnu-bigdathub-super-resolution\/DB\/test\/'\n\n","2e1b6954":"## Training options\noutputs_dir = '.\/models\/'\nlr = 1e-4\nbatch_size = 16\nnum_workers = 4\nseed = 123\nnum_epochs = 100\n\nuse_pretrain = True\npretrained = '\/kaggle\/input\/jbnu-bigdathub-super-resolution\/pretrained.pth'\n\n## Test options\noutimg_dir = '.\/restored_images\/'\n\nif not os.path.exists(outputs_dir):\n    os.makedirs(outputs_dir)\nif not os.path.exists(outimg_dir):\n    os.makedirs(outimg_dir)\n\n","2aac902c":"cudnn.benchmark = True\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\ntorch.manual_seed(seed)\nmodel = SRCNN().to(device)\n\ncriterion = nn.MSELoss()\noptimizer = optim.Adam([\n    {'params': model.conv1.parameters()},\n    {'params': model.conv2.parameters()},\n    {'params': model.conv3.parameters(), 'lr': lr * 0.1}\n], lr=lr)\n\ntrain_dataset = TrainDataset(train_dir, is_train=1, scale=scale)\ntrain_dataloader = DataLoader(dataset=train_dataset,\n                              batch_size=batch_size,\n                              shuffle=True,\n                              num_workers=num_workers,\n                              pin_memory=True,\n                              drop_last=True)\neval_dataset = TrainDataset(train_dir, is_train=0, scale=scale)\neval_dataloader = DataLoader(dataset=eval_dataset, batch_size=1)\n\n# load pretrained model\nif use_pretrain is True:\n    state_dict = model.state_dict()\n    for n, p in torch.load(pretrained, map_location=lambda storage, loc: storage).items():\n        if n in state_dict.keys():\n            state_dict[n].copy_(p)\n        else:\n            raise KeyError(n)","dc8a3bd2":"best_weights = copy.deepcopy(model.state_dict())\nbest_epoch = 0\nbest_psnr = 0.0\n\nfor epoch in range(num_epochs):\n\n    model.train()\n    epoch_losses = AverageMeter()\n\n    with tqdm(total=(len(train_dataset) - len(train_dataset) % batch_size)) as t:\n        t.set_description('epoch: {}\/{}'.format(epoch, num_epochs - 1))\n\n        for data in train_dataloader:\n            inputs, labels = data\n\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            preds = model(inputs)\n\n            loss = criterion(preds, labels)\n\n            epoch_losses.update(loss.item(), len(inputs))\n\n            # set grad to zero\n            optimizer.zero_grad()\n            # backward\n            loss.backward()\n            # optimizer next step\n            optimizer.step()\n\n            t.set_postfix(loss='{:.6f}'.format(epoch_losses.avg))\n            t.update(len(inputs))\n    torch.save(model.state_dict(), \"{}\/epoch_{}.pth\".format(outputs_dir, epoch))\n\n    model.eval()\n    epoch_psnr = AverageMeter()\n\n    if (epoch + 1) % 5 == 0:\n        for data in eval_dataloader:\n            inputs, labels = data\n\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            with torch.no_grad():\n                preds = model(inputs).clamp(0.0, 1.0)\n\n            epoch_psnr.update(calc_psnr(preds, labels), len(inputs))\n\n        print('eval psnr: {:.2f}'.format(epoch_psnr.avg))\n\n        if epoch_psnr.avg > best_psnr:\n            best_epoch = epoch\n            best_psnr = epoch_psnr.avg\n            best_weights = copy.deepcopy(model.state_dict())\n\nprint('best epoch: {}, psnr: {:.2f}'.format(best_epoch, best_psnr))\ntorch.save(best_weights, os.path.join(outputs_dir, 'best.pth'))\n","943d94bf":"## Test \ucf54\ub4dc\ntest_dataset = TestDataset(test_dir, scale=scale)\ntest_dataloader = DataLoader(dataset=test_dataset, batch_size=1, shuffle=False)\n\nmodel.eval()\n\nimg_num = len(test_dataset.lr_list)\npsnr = torch.zeros(img_num)\n\nf = open('submission.csv', 'w', newline='')\nwr = csv.writer(f)\nwr.writerow(['Id', 'Predicted'])\n\nn = -1\nfor data in test_dataloader:\n    n = n + 1\n    inputs = data\n    inputs = inputs.to(device)\n\n    with torch.no_grad():\n        preds = model(inputs).clamp(0.0, 1.0)\n\n    # crop image into 20 x 20\n    _, _, h, w = preds.size()\n    h2 = int(h \/ 2)\n    w2 = int(w \/ 2)\n    preds_crop = preds[:, :, h2:h2 + 20, w2:w2 + 20] * 255\n\n    preds_crop_array = preds_crop.reshape(20 * 20, 1)\n    preds_crop_array = preds_crop_array.cpu()\n    preds_crop_array = preds_crop_array.numpy()\n\n    idx_list = list(range(n * 400 + 1, (n + 1) * 400 + 1))\n    idx_list = np.array(idx_list)\n    print(\"{}\".format(n))\n    for ii in range(0, 400):\n        wr.writerow([idx_list[ii], preds_crop_array[ii, 0]])\n\n    preds = preds.mul(255.0).cpu().numpy().squeeze(0).squeeze(0)\n\n    image = pil_image.open(test_dataset.lr_list[n]).convert('RGB')\n    image = np.array(image).astype(np.float32)\n    ycbcr = convert_rgb_to_ycbcr(image)\n\n    output = np.array([preds, ycbcr[..., 1], ycbcr[..., 2]]).transpose([1, 2, 0])\n    output = np.clip(convert_ycbcr_to_rgb(output), 0.0, 255.0).astype(np.uint8)\n    output = pil_image.fromarray(output)\n\n    # split\n    _, fname = os.path.split(test_dataset.lr_list[n])\n    filename = \"{}\/{}\".format(outimg_dir, fname)\n    output.save(filename)","b46fc441":"# Test \uc218\ud589\n\n\ud559\uc2b5\ub41c \ubaa8\ub378\ub85c test \uc218\ud589\n\n\uc704\uc758 100 epochs \ud559\uc2b5\ub41c \ubaa8\ub378\uc744 \uc774\uc6a9\n\n\uc131\ub2a5 \ud3c9\uac00\ub97c \uc704\ud574 \uc601\uc0c1\uc758 \uac00\uc6b4\ub370 20x20 \ud06c\uae30\uc758 patch\ub97c \uc774\uc6a9\n\n20x20 \ud06c\uae30\uc758 patch\uc758 YCbCr\uc911 Y \ucc44\ub110\uc5d0 \ub300\ud55c \uac12\uc744 submission.csv\uc5d0 \uc800\uc7a5","5cba9704":"# Training \/ Test options \uc124\uc815\n\n.\/models\/ \uc5d0 \ud559\uc2b5\ub420 \ubaa8\ub378 \ud30c\uc77c \uc800\uc7a5\n\nlearning rate\ub294 1e-4 \uc774\uc6a9\n\nbatch size=16\n\n\ud559\uc2b5\uc740 100 eopchs \uc218\ud589\n\nuse_pretrain = True\uc778 \uacbd\uc6b0 pretrain\ub41c \ubaa8\ub378\uc778 pretrained.pth\ub97c \ubd88\ub7ec\uc640 \ud559\uc2b5 \uc218\ud589\n\n\ubcf5\uc6d0 \uc601\uc0c1\uc758 \uacbd\ub85c\ub294 .\/restored_images\/ \ub85c \uc124\uc815\n\n\uadf8\ub9ac\uace0 \uac01 \ub514\ub809\ud1a0\ub9ac \uc0dd\uc131","86cc7d21":"# Train\uc744 \uc218\ud589\ud558\uae30 \uc704\ud55c \uc804\ucc98\ub9ac \uacfc\uc815\n\ncuda \uc0ac\uc6a9 \uc124\uc815\n\n\uc704\uc5d0\uc11c \uc815\uc758\ud55c SRCNN \ubaa8\ub378 \uc774\uc6a9\n\n\ud559\uc2b5 \uc2dc loss\ub294 MSE (Mean-Squared Error) loss \uc0ac\uc6a9\n\ntrain\/validation dataset\uc5d0 \ub300\ud55c dataloader \uc124\uc815\n\npretrain\ub41c \ubaa8\ub378\uc744 \ubd88\ub7ec\uc634","a37646c8":"# \ud559\uc2b5 \ucf54\ub4dc \uc2dc\uc791\n\n\uac00\ub85c, \uc138\ub85c 4\ubc30 \ud06c\uae30\uc758 \uc601\uc0c1\uc73c\ub85c super-resolution \uc218\ud589\n\ntrain dataset\uacfc test dataset\uc758 \ub514\ub809\ud1a0\ub9ac \uc124\uc815","c37cd8a3":"# Training \ubc0f validation \uc218\ud589\n\n\ub9e4 epoch\uc5d0 \ub300\ud55c \ubaa8\ub378\uc774 .\/models\/epoch_{}.pth\uc5d0 \uc800\uc7a5\n\n5 epochs \ub9c8\ub2e4 validation \uc218\ud589\ud558\uc5ec PSNR\uc774 \uac00\uc7a5 \ub192\uac8c \ub098\uc624\ub294 \ubaa8\ub378\uc744 .\/models\/best.pth\uc5d0 \uc800\uc7a5","d0445d97":"# SRCNN \ubaa8\ub378\n\n3\uac1c\uc758 convolution layer\ub85c \uad6c\uc131\ub418\uc5b4 \uc788\uc74c\n\n\uc774\ub97c \uc218\uc815\ud558\uc5ec \uc131\ub2a5\uc744 \uac1c\uc120\ud560 \uc218 \uc788\uc74c","aa3015af":"# Library \uc124\uc815 \ubc0f \ud544\uc694 \ud568\uc218 \uc815\uc758\n**\uc774\ub294 \uc218\uc815\ud560 \ud544\uc694 \uc5c6\uc74c**"}}