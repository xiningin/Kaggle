{"cell_type":{"c998a74f":"code","effdf98f":"code","f6539d9f":"code","8a37c92e":"code","52eee49c":"code","a45a8271":"code","a974a6bf":"code","3ef6d854":"code","55c67e21":"code","66515374":"code","f979a571":"code","9c8c5309":"code","7dabfa55":"code","5621e355":"code","b0c00ebb":"code","8653698a":"code","29a884dc":"code","f0b4e8a5":"code","ee8d6c4d":"code","4842952d":"code","6f045201":"code","9b447a9e":"code","04c49447":"code","4259cf18":"code","c699d0f0":"code","a323a31a":"markdown","d1fbe877":"markdown","999ca006":"markdown","b2f8b417":"markdown","5d605024":"markdown","21c8e973":"markdown","94725309":"markdown","f0e34dd4":"markdown"},"source":{"c998a74f":"# Some pip install for BERT\n!pip install bert-for-tf2","effdf98f":"# Import all dependencies to be used\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf # machine learning and neural nets\nimport tensorflow_datasets as tfds # for text preprocessing\nimport tensorflow_hub as hub\nimport bert\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer # NLTK class object for determining sentiment of text","f6539d9f":"# Read in the train.csv into a pandas dataframe and get a look at it\ndf = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndf.head()","8a37c92e":"# Check the description and information relevant from the train.csv dataframe\nprint(df.info())","52eee49c":"# Import dependency for string cleaning\nimport re\n\n# Grab a sample string for display purposes\ntext_str = df.loc[984,'text']\n\ndef clean_text(text):\n    '''\n    Function to clean text and remove unnecessary components and ease tokenization.\n    :param: text: the string to be cleaned\n    :output: text: the cleaned string\n    '''\n    text = re.sub(r'https?:\/\/\\S+', '', text) # remove link\n    text = re.sub(r'#\\w+', '', text) # remove hashtags\n    text = re.sub(r'@\\w+', '', text) # remove mentions\n    text = re.sub(r'\\n', ' ', text) # remove linebreaks\n    text = re.sub(r'\\s+', ' ', text) # remove leading and trailing spaces\n    return text\n\n# Show the cleaned version of the sample text\nprint(f'Starting text: {text_str}')\nprint(f'Cleaned text: {clean_text(text_str)}')\n    ","a45a8271":"def bert_encoding(texts, tokenizer, max_len=512):\n    '''\n    Function to encode text into tokens, masks, and segment_ids for BERT embedding layer input.\n    \n    :param: texts - the texts to tokenize\n    :param: tokenizer - the BERT tokenizer that will be used to tokenize the texts\n    :param: max_len - the maximum length of an input sequence (the sequence of tokens to be embedded)\n    \n    :output: all_tokens - the texts turned into tokens and padded for match length, returned as np.array\n    :output: all_masks - masks for each text denoted sequence length and pad length, returned as np.array\n    :output: all_segments - segment_ids for each text, all blank, returned as np.array\n    '''\n    all_tokens = [] # initiated list for tokens\n    all_masks = [] # initiated list for masks\n    all_segments = [] # initiated list for segment_ids\n    \n    # Iterate through all texts\n    for text in texts:\n        \n        # Tokenize text\n        text = tokenizer.tokenize(text)\n        \n        # Make room for the CLS and SEP tokens\n        text = text[:max_len-2]\n        \n        # Create the input sequence beginning with [CLS] and ending with [SEP]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        \n        # Determine how much padding is required (max_length - length of the input sequence)\n        pad_len = max_len - len(input_sequence)\n        \n        # Create token ids, used by BERT\n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        \n        # Pad the tokens by 0's for the pad length determined above\n        tokens += [0] * pad_len\n        \n        # Create the masks for the sequence, with the 1 for each token id and 0 for all padding\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        \n        # All empty segment_ids for the max length\n        segment_ids = [0] * max_len\n        \n        # Append all tokens, masks, and segment_ids to the initialized lists\n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n        \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","a974a6bf":"# Initialize the tokenizer\nBertTokenizer = bert.bert_tokenization.FullTokenizer\n\n# Load the BERT layer\nbert_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1\",\n                            trainable=False)\n\n# Import the vocab files, and the lower case function\nvocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\nto_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n\n# Load the tokenizer with the preloaded vocab file and lower case function\ntokenizer = BertTokenizer(vocabulary_file, to_lower_case)\n\n# Give a sample of how the tokenizer works\ntokenizer.tokenize(\"What cool code, where'd ya learn it all?\")","3ef6d854":"# Clean the text data\ndf['text_clean'] = df['text'].apply(lambda x: clean_text(x))\n\n# Lowercase the text data\ndf['text_lowered'] = df['text_clean'].apply(lambda x: x.lower())\n\n# Set targets\ntargets = df['target']","55c67e21":"# Load dependency for train_test_split function\nfrom sklearn.model_selection import train_test_split\n\n# Prepare the X and Y train and test datasets\nX_train,X_test,Y_train,Y_test = train_test_split(list(df['text_lowered']),targets,test_size=0.2,random_state=28)","66515374":"# Finally, run the X train and test (the cleaned and lowered text data) through the BERT encoding function\n# with the BERT tokenizer we prepped and a max_len parameter chosen by you\ntrain_input = bert_encoding(X_train, tokenizer, max_len=100)\ntest_input = bert_encoding(X_test, tokenizer, max_len=100)","f979a571":"def build_model(bert_layer, max_len=512):\n    '''\n    Function to build a tensorflow machine learning neural network model.\n    \n    :param: bert_layer - the loaded BERT layer from TF hub\n    :param: max_len - the maximum length of an input sequence for encoding purposes, used here to denote the input shape\n    \n    :output: Model - Tensorflow keras model with inputs and outputs designated\n    '''\n    # INPUTS\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name='input_word_ids')\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name='input_mask')\n    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name='segment_ids')\n    \n    # BERT EMBEDDING\n    _, sequence_output = bert_layer([input_word_ids,input_mask,segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    clf_output_ = tf.keras.layers.Reshape((32,24))(clf_output)\n    \n    # HIDDEN\n    hidden = tf.keras.layers.Dense(128,activation='relu')(clf_output)\n    hidden2 = tf.keras.layers.Dense(64,activation='relu')(hidden)\n    drop = tf.keras.layers.Dropout(0.1)(hidden2)\n    \n    # CHANNEL 2 - LSTM\n    lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128,\n                                kernel_regularizer=tf.keras.regularizers.L2(0.001),\n                                                             return_sequences=True))(clf_output_)\n    drop2 = tf.keras.layers.Dropout(0.1)(lstm)\n    cnn = tf.keras.layers.Conv1D(64,5,\n                                kernel_regularizer=tf.keras.regularizers.L2(0.001))(drop2)\n    max_pool = tf.keras.layers.MaxPooling1D(25)(cnn)\n    flat = tf.keras.layers.Flatten(data_format='channels_last')(max_pool)\n    \n    # OUTPUTS\n    concat = tf.keras.layers.Concatenate()([drop,flat])\n    output_target = tf.keras.layers.Dense(1,activation='sigmoid')(concat)\n    \n    return tf.keras.Model(inputs=[input_word_ids,input_mask,segment_ids],outputs=output_target)","9c8c5309":"# Build the model\nmodel = build_model(bert_layer,max_len=100)","7dabfa55":"model.summary()","5621e355":"model.compile(optimizer=tf.keras.optimizers.Adam(lr=2e-6),loss='binary_crossentropy',metrics=['accuracy',tf.keras.metrics.AUC()])","b0c00ebb":"tf.keras.utils.plot_model(model)","8653698a":"train_history = model.fit(train_input,Y_train,epochs=5,batch_size=16,validation_split=0.2)","29a884dc":"predictions = model.predict(test_input)","f0b4e8a5":"from sklearn.metrics import classification_report, confusion_matrix\n\ndef pred_return(x):\n    if x>=0.5:\n        return 1\n    else:\n        return 0\n\nprint(classification_report([pred_return(x) for x in predictions],Y_test))\nprint(confusion_matrix([pred_return(x) for x in predictions],Y_test))","ee8d6c4d":"test_df = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","4842952d":"test_df.head()","6f045201":"test_df['text_clean'] = test_df['text'].apply(lambda x: clean_text(x))\ntest_df['text_lowered'] = test_df['text_clean'].apply(lambda x: x.lower())","9b447a9e":"test_input = bert_encoding(test_df['text_lowered'],tokenizer,max_len=50)","04c49447":"predictions = model.predict(test_input)\ntest_df['target'] = [pred_return(x) for x in predictions]\ntest_df.drop(['keyword','location','text','text_clean','text_lowered'],axis=1,inplace=True)\ntest_df.head()","4259cf18":"assert len(test_df['target']) == 3263","c699d0f0":"test_df.to_csv('submission.csv',index=False,header=True)","a323a31a":"With the text cleaner ready to go, we'll move onto to BERT encoding. \n\nBERT uses three key inputs when embedding tokens, so we'll need to prepare all three at this stage in a function. \n* **tokens** - the tokens from the BERT tokenizer, which must begin with token 101 for CLS and token 102 for SEP\n* **masks** - used to show the BERT embedding what is the regular text and what was padded onto it\n* **segment_ids** - used during embedding, will remain empty at this stage. They need to be passed in during embedding, so we need to include it here.","d1fbe877":"# Text Cleaning and BERT Prep\nSince the release of BERT, machine learning for natural language processing has been significantly altered. Providing the strongest set of multidimensional embeddings for words, BERT is a key piece of any successful machine learning model. We will use BERT below to load the pretrained embeddings for our vocabulary. \n\nWe'll start by cleaning the text and removing hashtags, links, mentions, linebreaks and leading or trailing spaces.","999ca006":"Now that BERT encoding is ready as a function, we can proceed to build the BERT Tokenizer that we'll use for the above function, as well as import the BERT layer from Tensorflow Hub. We also import the vocabulary file to preload the BERT tokenizer, and the lower_case function.","b2f8b417":"BERT's ready to go, so let's get the data caught up now.","5d605024":"# Testing and Refinement","21c8e973":"# Building the Model and Testing\nNow that the data is cleaned, the encoding finished, tokens created, we need to build and run the machine learning model itself. Using tensorflow, we'll create a model that begins with a BERT embedding layer and is followed by LSTM and CNN layers. Here's a breakdown of the model:\n* **Inputs** - there are 3 input layers (input_word_ids,input_mask,segment_ids). These correspond to the outputs of the BERT encoding function, and match the encoded values that the BERT embedding layer requires.\n* **BERT Embedding** - using the BERT layer we imported from TF hub, we insert the three inputs. It provides both a pooled_output and a sequence_output, but we only need the latter. The former is disgarded (denoted by *_*). We also only need the CLF result which is the first sequence output. \n* **Reshaping** - since we're using a LSTM and CNN model, we need to have a 3 dimensional input. The BERT output is only 2 dimensional, so we reshape it. \n* **Long Short Term Memory** - A singular LSTM layer uses 128 units to run the BERT reshaped output. A kernel regularizer limits it to avoid overfitting. It is set to return sequences, so the CNN layer has 3 dimensional input. A dropout layer follows to further reduce overfitting.\n* **Convolutional Neural Network** - Two Convolutional 1 Dimensional layers are sandwiched by Dropout layers. The first CONV1D layer recognizes 128 patterns with a kernel size of 7. The second recognizes 64 patterns with a kernel of 3. Both feature kernel regularizers to reduce overfitting. Max Pooling follows to select the important patterns. \n* **Outputs** - The max pooling output is flattened for input to the final dense hidden layer with sigmoid activation. The final model is returned.","94725309":"# Submission Testing and Saving","f0e34dd4":"# Using BERT, LSTM & CNN to Identify Real Disaster Tweets\nThis notebook will attempt to use Long-Short Term Memory and Convolutional Neural Net layers inside a tensorflow neural network to identify real and fake tweets related to disasters. \n\nWe begin by working on data extraction, cleaning and analysis. Then we build the neural network, before fitting our data inside it for testing and validation. Finally, we run the model to predict the test.csv provided. \n\nLet's begin by importing dependencies and taking a look at the train.csv file."}}