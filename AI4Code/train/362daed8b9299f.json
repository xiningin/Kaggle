{"cell_type":{"f19271ba":"code","6a2c4f1e":"code","77cb8cf6":"code","640cff8e":"code","4122d00c":"code","e5e1f5c8":"code","5bfdf956":"code","1e0f7d0b":"code","642bd36d":"code","05e126f5":"code","f7418460":"code","3f040450":"code","165de77b":"code","9b41f60a":"code","f4c20d40":"code","4cee36ce":"code","b8b9aa98":"code","b9194190":"code","5ea61221":"code","2b51efd1":"code","a0f2cbc8":"code","6f4c2faf":"code","d4d6bcae":"code","b15aea02":"code","75502902":"code","47a0550d":"code","a67e8ad4":"code","b513645b":"code","4254b363":"code","7337bfe8":"code","2ea07441":"code","3c2ed6fd":"code","705e4284":"code","063dfdc8":"code","4b3c48ff":"code","731e7408":"code","16e29a9b":"code","ccca167e":"code","bd0e4560":"code","63970727":"code","a0b5a915":"code","c8ffb42c":"code","b5399657":"code","6de12519":"code","b15705bd":"code","78068d55":"code","9f36b7b3":"code","81b32ca8":"code","0d8cf1dc":"code","3f5da8fa":"code","82d906ed":"code","0910d9cd":"code","21e5b320":"markdown","e9d9259f":"markdown","020edb8b":"markdown","26e3550f":"markdown","31ee313e":"markdown","4b15c943":"markdown","86bd9623":"markdown","db2fc07c":"markdown","55fe78d8":"markdown","a0e47ccd":"markdown","b28ced68":"markdown","1f606b3c":"markdown","dade0c90":"markdown","294e3a6f":"markdown","f1caa426":"markdown","24232e57":"markdown","774c32ba":"markdown","181137d6":"markdown","4b07fbbb":"markdown","73705a95":"markdown","0b9695b8":"markdown"},"source":{"f19271ba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6a2c4f1e":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\nimport nltk\nimport re\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\n#Now keras libraries\nfrom tensorflow.keras.preprocessing.text import one_hot, Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Embedding, Input, LSTM, Conv1D, MaxPool1D, Bidirectional\nfrom tensorflow.keras.models import Model","77cb8cf6":"fake_df = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv')\ntrue_df = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/True.csv')","640cff8e":"fake_df","4122d00c":"true_df","e5e1f5c8":"# Check null values True\ntrue_df.isnull().sum()","5bfdf956":"# Check null values of False\nfake_df.isnull().sum()","1e0f7d0b":"# If we also want to know the memory usage\nfake_df.info()","642bd36d":"true_df.info()","05e126f5":"# Now lets add an target column for indicating real and fake news\ntrue_df['isfake'] = 0\nfake_df['isfake'] = 1","f7418460":"true_df.head()","3f040450":"fake_df.head()","165de77b":"#Now lets concatenate both of them\ndf = pd.concat([true_df, fake_df]).reset_index(drop = True)\ndf","9b41f60a":"# Drop the date column\ndf.drop(columns= ['date'], axis =1, inplace= True)\ndf","f4c20d40":"# Combine \"title\" and \"text\" in single column\ndf['original'] = df['title'] + ' ' + df['text']\ndf.head()","4cee36ce":"df['original'][0]","b8b9aa98":"# Add some additional stopwors in nltk stopwords package\nstop_words = stopwords.words('english')\nstop_words.extend(['from', 'subject', 're', 'edu', 'use'])","b9194190":"stop_words","5ea61221":"# To remove stopwords and words with length less than 3\ndef preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) >3 and token not in stop_words:\n            result.append(token)\n    return result","2b51efd1":"# Now lets apply the defined function of \"original\" text in our datadrame\ndf['clean'] = df['original'].apply(preprocess)","a0f2cbc8":"df","6f4c2faf":"# original news\ndf['original'][0]","d4d6bcae":"# clean news after removing stopwords\nprint(df['clean'][0])","b15aea02":"# Total number of words in dataset\nlist_words = []\nfor i in df.clean:\n    for j in i:\n        list_words.append(j)\nprint ('Total number of words are: {}'.format(len(list_words)))","75502902":"#list_words","47a0550d":"#total unique words\ntotal_unique_words = len(list(set(list_words)))\ntotal_unique_words","a67e8ad4":"# Now lets convert words in \"clean\" column to a string\ndf['clean_joined'] = df['clean'].apply(lambda x: \" \".join(x))\ndf","b513645b":"df['clean_joined'][1]","4254b363":"print (df['clean'][1])","7337bfe8":"print (df['original'][1])","2ea07441":"# First, lets plot the num of samples in subject \nplt.figure(figsize=(8,8))\nsns.countplot(y = 'subject', data = df)","3c2ed6fd":"# fake vs true news\nplt.figure(figsize=(8,8))\nsns.countplot(y = 'isfake', data = df)","705e4284":"# Now lets plot word cloud for Real news. This will show the most common words when the news is Fake.\nplt.figure(figsize=(20,20))\nwc = WordCloud(max_words= 2000, width= 1600, height= 800, stopwords= stop_words).generate(\" \".join(df[df.isfake == 1].clean_joined))\nplt.imshow(wc, interpolation= 'bilinear')","063dfdc8":"# Now lets plot word cloud for Real news. This will show the most common words when the news is Fake.\n# As most new are Political, so words are clearly visible in these plots.\nplt.figure(figsize=(20,20))\nwc = WordCloud(max_words= 2000, width= 1600, height= 800, stopwords= stop_words).generate(\" \".join(df[df.isfake == 1].clean_joined))\nplt.imshow(wc, interpolation= 'bilinear')","4b3c48ff":"# Now maximum length is required to create word embeddings\nmaxlen = -1\nfor news in df.clean_joined:\n    tokens = nltk.word_tokenize(news) #converts text to tokens (words)\n    if (maxlen < len(tokens)):\n        maxlen = len(tokens)\nprint (\"The maximum number of words in any news is =\", maxlen)","731e7408":"#Now lets visualize the distribution of number of words in a text, using a very interactive tool \"Plotly\"\nimport plotly.express as px\nfig = px.histogram(x = [len(nltk.word_tokenize(x)) for x in df.clean_joined], nbins = 100)\nfig.show()","16e29a9b":"# Split data into Train and Test\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(df.clean_joined, df.isfake, test_size = 0.2, random_state = 42)","ccca167e":"from nltk import word_tokenize","bd0e4560":"# Lets create tokenizer to tokenize the words and conert them into a sequence\ntokenizer = Tokenizer(num_words= total_unique_words)\ntokenizer.fit_on_texts(x_train) #It creates vocabulary index (\"word_index\") based on word frequency\ntrain_sequences = tokenizer.texts_to_sequences(x_train) # Replace each word in text with corresponding integer value from \"word_index\"\ntest_sequences = tokenizer.texts_to_sequences(x_test)","63970727":"len(train_sequences)","a0b5a915":"len(test_sequences)","c8ffb42c":"print (\"The encoding for news\\n\", df.clean_joined[0], \"\\n is\\n :\", train_sequences[0])","b5399657":"pad_train = pad_sequences(train_sequences, maxlen = 4405, padding = 'post', truncating= 'post')\npad_test = pad_sequences(test_sequences, maxlen=4405, padding = 'post', truncating= 'post')","6de12519":"# Lets visualize the padding sequence for 1st two samples\nfor i, news in enumerate(pad_train[:2]):\n    print(\"The padded encoding for news\", i+1, \" is : \", news)","b15705bd":"# Lets visualize the padding sequence for 1st two samples\nfor i, news in enumerate(pad_test[:2]):\n    print(\"The padded encoding for news\", i+1, \" is : \", news)","78068d55":"# Now lets build the model\nmodel = Sequential() \n\nmodel.add(Embedding(total_unique_words, output_dim = 128)) #Embedding Layer\n\nmodel.add(Bidirectional(LSTM(128))) #Bi-directional LSTM\n\n#Dense layer\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(Dense(1, activation = 'sigmoid')) # binary classification (0\\1)\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics= ['acc'])\nmodel.summary()","9f36b7b3":"y_train = np.asarray(y_train)","81b32ca8":"# train the model\nmodel.fit(pad_train, y_train, batch_size= 64, validation_split = 0.1, epochs= 2)","0d8cf1dc":"pred = model.predict(pad_test) #prediction","3f5da8fa":"# Lets set the threshold 0.5, i.e if pred >0.5, it implies the news is fake and vice versa.\nprediction = []\nfor i in range (len(pred)):\n    if pred[i].item() > 0.5:\n        prediction.append(1)\n    else:\n        prediction.append(0)","82d906ed":"#accuracy\nfrom sklearn.metrics import accuracy_score\n\naccuracy = accuracy_score(list(y_test), prediction)\n\nprint (\"The model accuracy is :\", accuracy)","0910d9cd":"#confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(list(y_test), prediction)\nplt.figure(figsize=(10,10))\nsns.heatmap(cm, annot = True)","21e5b320":"Lets visualize one sample of each join_cleaned, cleaned and original column","e9d9259f":"## Recurrent Neural Networks (RNN)","020edb8b":"* Feature Engineering","26e3550f":"I am thankful to [Coursera](https:\/\/www.coursera.org\/learn\/nlp-fake-news-detector) and [Ryan Ahmad](https:\/\/www.coursera.org\/instructor\/~48777395) for providing an amazing opportunity for improving NLP skills.","31ee313e":"* The major drawback of ANN is that they don't have time dependency or memory effect.\n* RNN considers the temporal dimension and possess internal memory state.\n* However, RNN suffers from ***Vanishing Gradient*** problem just like ANN. It fails to establish long term dependencies.","4b15c943":"![image.png](attachment:image.png)Difference btw ANN & RNN","86bd9623":"* Data Cleaning","db2fc07c":"## Long Short Term Memory (LSTM)Networks","55fe78d8":"In this project we are going to train a ML model to detect fake news based on Recurrent Neural Network. We will explot \"Natural Language Processing (NLP)\" to convert text into numbers. Theses numbers are then fed to train ML model (LSTM) for predicting the authenticity of news. The tasks performed in this project are:\n* Exploratory data analysis\n* Data cleaning\n* Visualize datasets\n* Prepare the data by performing tokenization and padding\n* Train an LSTM Model\n* Evaluate trained model performance","a0e47ccd":"## Assess Trained Model Performance","b28ced68":"* LSTM works better as compared to RNN because it solves the vanishing gradient problem. All RNNs have feedback loops in the recurrent layer. This lets them maintain information in 'memory' over time. But, it can be difficult to train standard RNNs to solve problems that require learning long-term temporal dependencies. This is because the gradient of the loss function decays exponentially with time (called the vanishing gradient problem). LSTM networks are a type of RNN that uses special units in addition to standard units. LSTM units include a 'memory cell' that can maintain information in memory for long periods of time. A set of gates is used to control when information enters the memory, when it's output, and when it's forgotten. This architecture lets them learn longer-term dependencies. GRUs are similar to LSTMs, but use a simplified structure. They also use a set of gates to control the flow of information, but they don't use separate memory cells, and they use fewer gates. [Source](https:\/\/stats.stackexchange.com\/questions\/222584\/difference-between-feedback-rnn-and-lstm-gru)","1f606b3c":"## **Tokenizer:** It allow us to covert text corpus into a sequence of integers.","dade0c90":"## Acknowledgment","294e3a6f":"## Now we need to apply padding to make sure that all the sequences have same length.","f1caa426":"## Exploratory Data Analysis","24232e57":"Now we will perform **Tokenization** and **Padding**","774c32ba":"## Embedding Layers\n* Embedding layers learn the low dimensional continuous representation of input discrete variables.\n* Embedding layers can learn the ways to represent 10,000 variables with 200 variables (Just like PCA and Auto-encoders)\n* Thus subsequent layer learn more efficiently with less compute resources.","181137d6":"## To understand about \"Keras Tokenizer\" and the following commands in a more ellaborative way, please click [here](https:\/\/stackoverflow.com\/questions\/51956000\/what-does-keras-tokenizer-method-exactly-do)","4b07fbbb":"![image.png](attachment:image.png)","73705a95":"![image.png](attachment:image.png)","0b9695b8":"Now its time to visualize the pre-processed dataset using some advance tools."}}