{"cell_type":{"742563b2":"code","592bda6d":"code","93e9f22f":"code","157fb04b":"code","58dbb9b8":"code","cfe55cc1":"code","c7f946bc":"code","ca033aa2":"code","43570156":"code","8411d8dd":"code","b4c29b6c":"code","16c97a3a":"code","9a186f05":"code","ccfa5b28":"code","a875b856":"code","571abdf6":"code","519a286a":"code","c1d8173a":"code","6dddcd35":"code","a19f1fc3":"code","008f9775":"code","cc84fe3b":"code","7b5559ff":"code","ea5e079a":"code","0b2b672d":"code","b957d9c9":"markdown","050ba1f6":"markdown","89f89ca1":"markdown","907996f6":"markdown","fe9082ec":"markdown","ef90aecf":"markdown","d90ba4e3":"markdown","f65e95a3":"markdown","cd837b62":"markdown","7c090842":"markdown","607bcd8e":"markdown","c7311771":"markdown","602d7093":"markdown","cb8df5ba":"markdown","0f978b1c":"markdown","2113391a":"markdown","9ddee155":"markdown","fdb290a5":"markdown","19a9692c":"markdown","ebe724cb":"markdown","df61106a":"markdown","c3f0518f":"markdown","6415464e":"markdown","ff2b460d":"markdown","8f93da39":"markdown","f45b3d36":"markdown","973a0f99":"markdown","f6d28982":"markdown","7369365b":"markdown","c815252c":"markdown","54a28277":"markdown","60cbc12e":"markdown","0c98e857":"markdown","1e3c7aa3":"markdown","2c4a0885":"markdown","c496485a":"markdown","1160af30":"markdown","4d4a7874":"markdown","da766e29":"markdown","0f0d2915":"markdown","48c32d74":"markdown","0bffad90":"markdown","c0ed36f1":"markdown","04ab0107":"markdown","e2340782":"markdown","5fac96b5":"markdown"},"source":{"742563b2":"! pip install seaborn --upgrade","592bda6d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Plotting libraries\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","93e9f22f":"iris_data = pd.read_csv('\/kaggle\/input\/iris-flower-dataset\/IRIS.csv')","157fb04b":"iris_data","58dbb9b8":"iris_data.info()","cfe55cc1":"iris_data.describe()","c7f946bc":"iris_data.loc[:, 'species'].describe()","ca033aa2":"iris_data.loc[:, 'species'].value_counts().sort_values(ascending=False)","43570156":"iris_data.loc[:, 'species'].value_counts().sort_values(ascending=False) \/ iris_data.loc[:, 'species'].count()","8411d8dd":"sns.countplot(data=iris_data, x='species')","b4c29b6c":"iris_data.loc[:, iris_data.columns[:-1]].mean()","16c97a3a":"iris_data.loc[:, iris_data.columns[:-1]].std()","9a186f05":"f, axs = plt.subplots(2, 2, figsize=(8, 6))\nsns.histplot(data=iris_data, x='petal_length', ax=axs[0,0])\nsns.histplot(data=iris_data, x='petal_width', ax=axs[0,1])\nsns.histplot(data=iris_data, x='sepal_length', ax=axs[1,0])\nsns.histplot(data=iris_data, x='sepal_width', ax=axs[1,1])\nf.tight_layout()","ccfa5b28":"xs0 = np.random.uniform(size=1000)\nxs1 = np.random.normal(size=1000)\nxs2 = np.random.exponential(size=1000)\n\nf, axs = plt.subplots(1, 3, figsize=(10, 3))\nsns.histplot(x=xs0, ax=axs[0])\naxs[0].set_title(\"Uniform\")\nsns.histplot(x=xs1, ax=axs[1])\naxs[1].set_title(\"Normal\")\nsns.histplot(x=xs2, ax=axs[2])\naxs[2].set_title(\"Exponential\")\nf.tight_layout()","a875b856":"gen = np.random.default_rng()\n\nxs0 = gen.exponential(scale=0.2, size=1000)\nxs1 = -gen.exponential(scale=0.1, size=1000)\nxs2 =  np.concatenate((gen.normal(loc=0, scale=1, size=500), np.random.normal(loc=5, scale=1.5, size=500)))\n\nf, axs = plt.subplots(1, 3, figsize=(10, 3))\nsns.histplot(x=xs0, ax=axs[0])\naxs[0].set_title(\"Skew Right\")\nsns.histplot(x=xs1, ax=axs[1])\naxs[1].set_title(\"Skew Left\")\nsns.histplot(x=xs2, ax=axs[2])\naxs[2].set_title(\"Multimodal\")\nf.tight_layout()","571abdf6":"sns.relplot(data=iris_data, x='sepal_width', y='petal_length', hue='species')","519a286a":"g = sns.PairGrid(iris_data, hue='species')\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\ng.add_legend()","c1d8173a":"bball_data = pd.read_csv('\/kaggle\/input\/table37basketballteam\/Table3-7BasketballTeam.csv')","6dddcd35":"sns.catplot(kind='count', data=bball_data, x='Career Stage', height=3)\nsns.catplot(kind='count', data=bball_data, x='Career Stage', col='Shoe Sponsor', height=3)","a19f1fc3":"sns.catplot(kind='count', data=bball_data, x='Position', height=3)\nsns.catplot(kind='count', data=bball_data, x='Position', col='Shoe Sponsor', height=3)","008f9775":"sns.displot(data=bball_data, x='Age', height=3)\nsns.displot(data=bball_data, x='Age', hue='Position', col='Position', height=3)","cc84fe3b":"sns.catplot(kind='box', data=bball_data, x='Position', y='Age')","7b5559ff":"sns.catplot(kind='box', data=bball_data, x='Position', y='Height')","ea5e079a":"f, axs = plt.subplots(1, 3, figsize=(10, 3))\nsns.histplot(data=iris_data, x='sepal_width', ax=axs[0], bins=3)\nsns.histplot(data=iris_data, x='sepal_width', ax=axs[1], bins=10)\nsns.histplot(data=iris_data, x='sepal_width', ax=axs[2], bins=30)\nf.tight_layout()","0b2b672d":"f, axs = plt.subplots(1, 3, figsize=(10, 3))\nsns.histplot(data=iris_data, x='sepal_width', ax=axs[0], binwidth=0.25)\nsns.histplot(data=iris_data, x='sepal_width', ax=axs[1], binwidth=1)\nsns.histplot(data=iris_data, x='sepal_width', ax=axs[2], binwidth=2)\nf.tight_layout()","b957d9c9":"Alternatively, you can also use box plots.","050ba1f6":"For categorical data, you can draw a bar plot, factored across the levels of a feature. \n- If there is a relationship present, then the distribution should differ across the levels of the second variable. \n    - Why? Because if there is no relationship, then the value of the first variable should have no impact on the second","89f89ca1":"### Normalization\n\nThere are two standard methods of normalization: range normalization and the z-transform.\n\nTo conduct range normalization, modify the range of a feature to be within $[low, high]$\n\t\n$$a_i = \\frac{a_i \u2212 \\min\u2061(a)}{\\max\u2061(a) \u2212 \\min\u2061(a)} (high - low ) + low$$\n\nTo apply a z-transform, modify the a feature to be normally distributed with a mean of 0 and standard deviation of 1 (this really only works as expected if the feature is normally distributed to begin with)\n\n$$a_i = \\frac{a_i \u2212 \\bar{a}}{sd(a)}$$\n\nIf the feature is not normally distributed to begin with, the z-transform may distort the data bit; but it might still be useful, you'll need to check.","907996f6":"The covariance and correlation functions can be used to generate a covariance or correlation matrix, showing how every variable is linearly related to every other variable.\n\nCorrelation is not causation.\n- Many times, the relationship between two variables exists because of confounded features that may or may not be directly observed\n- Only careful experimentation can tease out causation\n\t\n\t\nTo measure the similarity between categorical variables, statistical techniques such as Chi-Squared tests and ANOVA can be used. ","fe9082ec":"There are two standard techniques for determining the number of bins when manually creating a histogram:\n\nEqual width binning\n- Splits the values into b bins, each of size range \/ b\n    E.g. [0, 10), [10, 20), \u2026, [90, 100]\n- Good for uniform distributions, but may produce many empty bins for non-uniform distributions\n\nEqual frequency binning\n- Sorts the values from smallest to largest and then puts an equal number of values in each bin\n- The total number of instances in each bin is [count of instances] \/ [number of bins]\n\nThe Seaborn library for python does a pretty good job of calculating the optimal number of bins, but it does have exceptions. See the Seaborn documentation for details.","ef90aecf":"Outliers\n- Outliers can be invalid (actual errors), or valid (correct values, but unusual for some reason)\n- To detect outliers, you can\n    - Examine the minimum and maximum values for sensibility\n    - Look at a box plot to see where the whiskers are","d90ba4e3":"Irregular Cardinality\n- Occurs when there are more or fewer unique values for a categorical variable than expected\n- If all of the values in a feature are the same (Cardinality 1), then that feature should be removed if there are no errors - it will not be useful in the model\n- If the cardinality is close to the number of instances in the dataset, then the feature may be continuous and not categorical (and vice versa)\n- Cardinality values larger than expected may indicate invalid levels that need to be recoded","f65e95a3":"The distribution of a variable is important because it helps us determine the best type of model to use for our solution. \n\nThree of the most common distributions are the **uniform distribution**, **normal distribution**, **exponential distribution**.\n- Uniform distributions occur when each value of a random variable is equally likely\n- Normal distributions occur most often for naturally occuring phenomenon\n- Exponential distributions occur most often when dealing with how long it takes for an event to occur","cd837b62":"Regardless of how missing values are handled, you should try to do it in a way that does not change the underlying distribution of the data.","7c090842":"- Create bar plots of each variable to visualize the distribution of the data","607bcd8e":"## Handling Data Quality Issues","c7311771":"### Binning\n\n- Converts a continuous feature into a categorical feature\n    - Helps some algorithms handle continuous features \"better\"\n    - Helps handle outliers\n    - Discards information in the process\n- If the number of bins is too low, then information is lost with respect to the distribution of the original values\n- If the number of bins is too high, some of those bins may be empty\n- Ideally, you want a number of bins that produces a representation close to the original distribution.","602d7093":"## Advanced Data Exploration","cb8df5ba":"To look for relationships between categorical and continuous variables, you can compare histograms when holding the level of the categorical variable steady.","0f978b1c":"## Data Preparation\n\nOnce the features have been identified, and the quantity assessed and corrected, the final step to conduct any transformations necessary to facilitate learning and model building.","2113391a":"If we consider the career stage of players split up by whether or not they have a shoe sponser, we see that there is no apparent relationship between the two variables.","9ddee155":"Covariance is defined as\n\n$$\\text{cov}(a,b) = \\frac{1}{n - 1} \\sum_i^n \\left( (a_i - \\bar{a})(b_i - \\bar{b}) \\right)$$\n\nwhere $\\bar{a}$ and $\\bar{b}$ are the sample means of the feature vectors $a$ and $b$.\n\nCovariance measures the *linear* relationship between the variables. Values near 0 indicate that there is little to no relationship between the variables. \n\nCovariance maintains the units of each variable, which may not make sense when compared to one another. ","fdb290a5":"## Sampling\n\nOnce the data has been transformed, there is a question of how much of that data to use. When conducting a machine learning experiment, you generally only work with a subset of the data - called a training set. Sampling is the process used to select that subset of data.\n\nIf sampling is not done carefully, the sample will be biased and not accurately represent the population.\n\nTop Sampling\n- Selects a flat % from the top of the dataset\n- Is almost always biased and impacted by the ordering of the data\n\nRandom Sampling\n- Randomly selects a flat % from the dataset\n- Does not preserve relationships in the data\n\nStratified Sampling\n- The dataset is grouped by one or more particular variables, and then s% of each group (called a strata) is selected for the sample. \n- Maintains the relative frequency of each group within the dataset\n\nUnder-Sampling\n- Creates a sample where all groups are equally represented\n- Group the dataset by one or more variables; from each group, randomly sample (without replacement) N instances, where N is the number of instances in the smallest group\n\nOver-Sampling\n- Creates a sample where all groups are equally represented\n- Group the dataset by one or more variables; from each group, randomly sample (with replacement) N instances, where N is the number of instances in the largest group\n\nUnder-sampling and over-sampling can be used to train predictive models that try to ignore sampling bias in the original dataset\n- For example, when data is gathered unequally from different subpopulations","19a9692c":"The `describe` can be used to calculate the summary statistics of a data frame.\n\nThe specific output of the `describe` function varies depending on the nature of the data.","ebe724cb":"## Identifying Data Quality Issues","df61106a":"The `info` method can give us an indication of the datatype for each feature. We are primarily interested in knowing which variables are continuous and which are discrete.","c3f0518f":"For continuous variables:\n- Examine the mean and standard deviation are used to describe the central tendency and variation of the distribution. (We can either refer to the summary statistics, or recalculate them if necessary)","6415464e":"Handling Outliers\n- Clamp the values\n    - $a_i = \\begin{cases} \n            lower & \\text{if $a_i$ < lower} \\\\\n            upper & \\text{if $a_i$ < lower} \\\\\n            a_i & \\text{otherwise}\n        \\end{cases}$\n    - Method 1: Determine upper and lower values is to use the whiskers of a box plot (1.5 * 1st quartile and 3rd quartile)\n    - Method 2: Set the upper and lower values to the mean plus\/minus a multiple of the standard deviation\n    - Always inspect the data to see how much of an impact clamping will have. If too many values will be changed, you may need to do something else. \n- Consider leaving them alone if the values are valid\n- Consider removing the observations if the values are invalid and the feature is important","ff2b460d":"Scatter plots can be used to visualize the relationship between two variables\n- Positive covariance (increase or decrease together)\n- Negative covariance (as one goes up, the other goes down)\n- No apparent relationship","8f93da39":"## Measuring Covariance and Correlation\n\nPlots alone are not sufficient to understanding relationships between variables. Covariance and correlation provide numerical metrics of the strength of these relationships.","f45b3d36":"In addition to the summary statistics, it is also useful to dig a bit deeper into the data.","973a0f99":"A **data quality report** can be used to answer these questions. This report includes,\n- Summary statistics for quantitative: count, mean, median, mode, min, max, standard deviation, percentiles, number of missing values, number of unique values\n- Summary statistics for categorical values: count, count and % missing, how many unique values (cardinality), number of values in each category (along with the %)\n- Basic distribution plots: histograms or bar plots, box plots\n- Basic relationship plots: scatter plot matrix","f6d28982":"In the following example, we look at position segmented by age and notice that while centers have some older players, it is not entirely clear if there is a strong relationship between the variables.","7369365b":"A scatter plot matrix displays scatter plots across all features in one visualization.","c815252c":"However, if we look at position by shoe sponsership, there does appear to be a relationship: guards have more sponsership deals.","54a28277":"For categorical variables:\n- Examine the 1st and 2nd mode and percentage of representation to identify the most common values","60cbc12e":"Stacked bar charts can also be used to compare categorical variables. If there is a relationship, then the proportions of each level should differ by a large margin.","0c98e857":"One of the first things you need to do when conducting an exploratory data analysis is to load the data","1e3c7aa3":"How to handle Missing Values\n- Drop the entire row; this might be acceptable if there is a large amount of data, but can lead to bias\n- Drop the feature if a large number of its values are missing (generally more than 60%)\n- Create a new feature that indicates if the value is present or missing - this can be used to train the model when it should ignore the feature\n- Impute the missing value by replacing it with a mean, median, mode, or other aggregate value\n    - This is normally not the best idea as it can lead to bias in the data\n    - Only consider it if a small number of features are missing (generally less than 30%)\n- Build a predictive [regression] model based on the dataset to try and predict the missing features","2c4a0885":"Distributions can be unimodal or multimodal, and can be skewed right (peak is on the left side) or skewed left (peak is on the right side).\n- When the distribution is multimodal, the mean is a very misleading value; but the presence of a multimodal distribution may indicate multiple clearly distinct \"groups\" within the data","c496485a":"Once the data has been loaded, we can begin asking questions.\n\nFor example,\n- What type of data is stored in each column?\n- What is the distribution of values for each feature?\n- Are there any missing or extreme values?","1160af30":"## Getting to Know Your Data","4d4a7874":"Missing values\n- Why are the values missing? Collection problems? Integration problems? Intentionally missing?\n- If a large portion of the values for a feature are missing (~60% is a good rule of thumb), then it may be best to not use that feature","da766e29":"Once the data has been summarized, it can be analyzed for quality issues.\n- Missing values\n- Irregular counts of unique values (cardinality)\n- Outliers\n\nIf quality issues within the data are not resolved, then the data will not yield a good model.","0f0d2915":"## Summary\n\nThe key outcomes of the data exploration process should include:\n1. Have gotten to know the features, especially their central tendencies, variations, and distributions\n2. Have identified any data quality issues, in particular missing values, irregular cardinality, and outliers\n3. Have corrected any data quality issues related to invalid data\n4. Have recorded any data quality issues due to valid data in a data quality plan, along with potential handling strategies\n5. Be confident enough that good-quality data exists to continue with a project\n\nAny steps taken to transform the data must be recorded so that they can also be applied as new data is made available.","48c32d74":"- Because the mean and standard deviation are fairly useless without context, we want to create histograms and box plots to visualize the distribution of the data.","0bffad90":"Correlation is the normalized covariance, removing units and limiting the range to $[-1, 1]$\n\n$$\\text{corr}(a, b) = \\frac{\\text{cov}(a,b)}{\\bar{\\sigma}_a \\bar{\\sigma}_b}$$\n\nwhere $\\bar{\\sigma}_a$ and  $\\bar{\\sigma}_b$ are the sample standard deviations of the feature vectors $a$ and $b$.","c0ed36f1":"# Exploratory Data Analysis\n\nAn Exploratory Data Analysis (EDA) is one of the first steps in the machine learning process. During an EDA, you examine data to determine its quality and applicability to the problem you are attempting to solve.","04ab0107":"and verify that it has been correctly loaded.","e2340782":"For discrete data, we are given: \n- count of non-empty values; \n- number of unique \"levels\" (also called classes when we talk about the target vector); \n- most common level\n- frequency of the most common level. ","5fac96b5":"For continuous data, we are given: \n- count of non-empty values; \n- sample mean and standard deviation; \n- minimum and maximum values; \n- quantiles."}}