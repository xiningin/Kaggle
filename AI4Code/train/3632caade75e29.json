{"cell_type":{"c437cd14":"code","b763e643":"code","10ca8925":"code","71984fcd":"code","3bafaf9f":"code","237878d7":"code","e5c7771d":"code","9fcc08ca":"code","79c702e2":"code","b4dcb0e1":"code","cd915052":"code","3a62ad85":"code","fdf075ba":"code","1d9bf00e":"code","6b3e798d":"code","0bd81a05":"code","640b6d0b":"code","0cfafc5a":"code","8fdc9a2e":"code","2ecb0bb4":"code","c24c0a1f":"code","a4cfda78":"code","56e47ae1":"code","6ed2867b":"code","0608ea45":"code","90569f2e":"code","f00bcb18":"code","b7adbaab":"code","8ba5fab4":"code","6c23220a":"code","921e6a76":"code","1483ee43":"code","3c5a05bf":"code","9a7d3b85":"code","8f9fc4b5":"code","53185e5d":"code","4113aef5":"code","90ef4746":"code","99e9dad1":"code","cfe681d4":"code","de770509":"code","e2df9430":"code","cfdbb97a":"code","ae3b5da4":"code","df652e57":"code","d9f3e427":"code","b1048c1f":"code","f129530b":"code","940c8c0a":"code","b1d4ffbb":"code","09886dd3":"code","43cc90d8":"code","971be1a2":"code","a08fa937":"code","2be51576":"code","0f450376":"code","a67f7265":"code","4abf57d3":"code","fe94b075":"code","3af94a71":"code","dffbd478":"code","0f6bd058":"code","54186b5b":"code","95d7fb72":"code","39a12c47":"code","f1a07332":"code","6f85d596":"code","b6b92579":"code","f9d04611":"code","f8ce9e03":"code","6a7ed4d9":"code","f9498820":"code","d34dd2e6":"code","fe77e13f":"code","fd39c19e":"code","b5ee7f31":"code","418460ea":"code","95156d77":"code","60f03884":"code","14d04004":"code","64185fa8":"code","ce4d59fa":"code","5a217302":"code","e4a374e3":"code","60f917ad":"code","1d0dd518":"markdown","53c537b9":"markdown","ffb00a7d":"markdown","863469bb":"markdown","5c2503f2":"markdown","50e786e8":"markdown","c8094e14":"markdown","b9b1c5d1":"markdown","9b57a821":"markdown","4da1be07":"markdown","13e7c09e":"markdown","8f53b87c":"markdown","1ddba514":"markdown","371ea5d8":"markdown","00c32af6":"markdown","63d58472":"markdown","172a4ef9":"markdown","9f3a2334":"markdown"},"source":{"c437cd14":"# Data Link-- https:\/\/www.kaggle.com\/arkhoshghalb\/twitter-sentiment-analysis-hatred-speech","b763e643":"#importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nimport re\nimport nltk\nfrom tqdm import tqdm\nimport scipy\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import Counter\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import roc_auc_score\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report","10ca8925":"#loading the data\ndf=pd.read_csv('..\/input\/twitter-sentiment-analysis-hatred-speech\/train.csv')","71984fcd":"print(df.shape)   #check the shape of the data","3bafaf9f":"print(df.head())","237878d7":"print(df.info())","e5c7771d":"df['label'].value_counts()","9fcc08ca":"plt.style.use('ggplot')","79c702e2":"# sn.set(font_scale=1.5)\nsn.set_style('whitegrid');\nax=sn.barplot(x=['Pos','Neg'], y=df.label.value_counts());\nax.set_title('DISTRIBUTION IN DATASET ACCORDING TO THE SENTIMENT',loc='center', pad=20, fontdict={'fontsize': 15,\n        'fontweight': 'bold',\n        'color': 'black',\n        'verticalalignment': 'baseline',\n        });\nax.set(xlabel='Tweets Sentiments', ylabel='No. of Tweets');\nplt.show()\nax.legend()\nax.figure.savefig('Total-positive-negative-counts.png',pad_inches=5)","b4dcb0e1":"y_value_counts=df['label'].value_counts()\nprint(\"Negative tweets  = \",y_value_counts[1], \"with percentage \", (y_value_counts[1]*100)\/(y_value_counts[0]+y_value_counts[1]),'%')\nprint(\"Positive tweets  = \",y_value_counts[0], \"with percentage \", (y_value_counts[0]*100)\/(y_value_counts[0]+y_value_counts[1]),'%')","cd915052":"# sn.set(font_scale=1.5)\nsn.set_style('whitegrid');\nax=sn.barplot(x=['Pos','Neg'], y=df.label.value_counts()*100\/df.label.value_counts().sum(), palette='Greys_d');\nax.set_title('% DISTRIBUTION IN DATASET ACCORDING TO THE SENTIMENT',loc='center', pad=20, fontdict={'fontsize': 15,\n        'fontweight': 'bold',\n        'color': 'black',\n        'verticalalignment': 'baseline',\n        });\nax.set(xlabel='Tweets Sentiments', ylabel='No. of Tweets');\nplt.show()\nax.legend()\nax.figure.savefig('Total-positive-negative-counts.png',pad_inches=5)","3a62ad85":"#lets see the classes through bar graph\ndata=dict(negative=y_value_counts[1],positive=y_value_counts[0])\ncls=data.keys()\nvalue=data.values()\n\nplt.bar(cls,value,color='maroon',width=0.2)","fdf075ba":"df['tweet']=df['tweet'].str.replace(' ','_')\ndf['tweet']=df['tweet'].str.replace('-','_')\ndf['tweet']=df['tweet'].str.lower()","1d9bf00e":"df.tweet[:10]","6b3e798d":"def expand(sent):\n    \"This function will replace english short notations with full form\"\n    \n    sent=re.sub(r\"can't\", \"can not\",sent)\n    sent=re.sub(r\"won't\", \"will not\",sent)\n    \n    sent=re.sub(r\"n\\'t\", \" not\",sent)\n    sent=re.sub(r\"\\'re\", \" are\",sent)\n    sent=re.sub(r\"\\'m\",\" am\",sent)\n    sent=re.sub(r\"\\'s\",\" is\",sent)\n    sent=re.sub(r\"\\'ll\",\" will\",sent)\n    sent=re.sub(r\"\\'ve\",\" have\",sent)\n    sent=re.sub(r\"\\'d\",\" would\",sent)\n    sent=re.sub(r\"\\'t\", \" not\",sent)\n    \n    return sent\n    ","0bd81a05":"# https:\/\/gist.github.com\/sebleier\/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]","640b6d0b":"def preprocess_tweet(text):\n    \"function for preprocess the text data\"\n    \n    preprocessed_tweet=[]\n    \n    for sentence in tqdm(text):\n        sent=expand(sentence)\n        sent=sent.replace(\"\\\\r\",\" \")\n        sent=sent.replace(\"\\\\n\",\" \")\n        sent=sent.replace('\\\\\"',\" \")\n        sent=re.sub(\"[^A-Za-z0-9]+\",\" \",sent)\n        \n        # https:\/\/gist.github.com\/sebleier\/554280\n        sent=\" \".join(i for i in sent.split() if i.lower() not in stopwords)\n        preprocessed_tweet.append(sent.lower().strip())\n        \n    return preprocessed_tweet\n        ","0cfafc5a":"preprocessed_tweets=preprocess_tweet(df['tweet'].values)","8fdc9a2e":"df['tweet']=preprocessed_tweets","2ecb0bb4":"df[\"tweet\"][10]","c24c0a1f":"word_list=[]\nfor i in df[\"tweet\"]:\n    for j in i.split(' '):\n        word_list.append(j)\n    ","a4cfda78":"counter = Counter(word_list)\ntop10 = counter.most_common(11)","56e47ae1":"top10","6ed2867b":"xaxes = [i[1] for i in top10]\nyaxes = [i[0] for i in top10]","0608ea45":"# sn.set(font_scale=1.5)\nsn.set_style('whitegrid');\nsn.set(rc={'figure.figsize':(11.7,8.27)})\nax=sn.barplot(x=xaxes, y=yaxes);\nax.set_title('TOP WORDS IN BUILT WORDLIST',loc='center', pad=20, fontdict={'fontsize': 15,\n        'fontweight': 'bold',\n        'color': 'black',\n        'verticalalignment': 'baseline',\n        });\n# ax.set(xlabel='Tweets Sentiments', ylabel='No. of Tweets');\nplt.show()\nax.legend()\nax.figure.savefig('top10wordscount.png',pad_inches=5)","90569f2e":"df[df['label']==0].tweet","f00bcb18":"positive_word=[]\nnegative_word=[]\nfor i in df[df['label']==0].tweet:\n    for j in i.split(' '):\n        positive_word.append(j)\nfor i in df[df['label']==1].tweet:\n    for j in i.split(' '):\n        negative_word.append(j)","b7adbaab":"positive_counter = Counter(positive_word)\nnegative_counter = Counter(negative_word)","8ba5fab4":"positive_counter.most_common(10),negative_counter.most_common(10)","6c23220a":"negative_counter.get('friday')","921e6a76":"w = ['good','bad','information']\ny1 = []\ny2=[]\nfor i in w:\n    y1.append(positive_counter.get(i))\n    y2.append(negative_counter.get(i))","1483ee43":"y2","3c5a05bf":"\nX_axis = np.arange(len(w))\n  \nplt.bar(X_axis-0.2 , y1, 0.4, label = 'Pos')\nplt.bar(X_axis+0.2 , y2, 0.4, label = 'Neg')\n  \nplt.xticks(X_axis, w)\nplt.xlabel(\"Words\", fontdict={'family': 'serif',\n        'color':  'darkred',\n        'weight': 'normal',\n        'size': 16,\n        })\nplt.ylabel(\"Number of Words\", fontdict={'family': 'serif',\n        'color':  'darkred',\n        'weight': 'normal',\n        'size': 16,\n        })\nplt.title(\"Most Common Words Across Sentiments\", fontdict={'family': 'serif',\n        'color':  'darkblue',\n        'weight': 'bold',\n        'size': 18,\n        })\nplt.legend()\nplt.show()","9a7d3b85":"positive_counter.most_common(10), negative_counter.most_common(10)","8f9fc4b5":"y=df['label']\nx=df.drop(['label'],axis=1)\n\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,stratify=y,random_state=40)\n","53185e5d":"x_test","4113aef5":"vect=TfidfVectorizer(min_df=10)\n\nvect.fit(x_train['tweet'].values)\n\ntrain_tweet=vect.transform(x_train['tweet'].values)\ntest_tweet=vect.transform(x_test['tweet'].values)\n\nprint(train_tweet.shape,y_train.shape)\nprint(test_tweet.shape,y_test.shape)","90ef4746":"#calculating sentiment scores for train data\nx_train_sent=np.ndarray.tolist(x_train[\"tweet\"].values)\n\nsia=SentimentIntensityAnalyzer()\nps=[]\nfor i in range(len(x_train_sent)):\n    ps.append((sia.polarity_scores((x_train_sent[i]))))\n    \nx_train_polarity=np.array(ps)\nx_train_polarity=x_train_polarity.reshape(-1,1)\nx_train_polarity.shape\n","99e9dad1":"#storing only scores of sentiment\nx_t=[]\nfor i in range(len(x_train)):\n    for j in x_train_polarity[0][0]:\n        x_t.append(x_train_polarity[i][0][j])\nx_t=np.array(x_t)\nx_t=x_t.reshape(-1,4)\nx_t.shape","cfe681d4":"#calculating sentiment scores for test data\nx_test_sent=np.ndarray.tolist(x_test[\"tweet\"].values)\n\nsia=SentimentIntensityAnalyzer()\nps=[]\nfor i in range(len(x_test_sent)):\n    ps.append((sia.polarity_scores((x_test_sent[i]))))\n    \nx_test_polarity=np.array(ps)\nx_test_polarity=x_test_polarity.reshape(-1,1)\nx_test_polarity.shape\n","de770509":"#storing only scores of sentiment\nx_tests=[]\nfor i in range(len(x_test)):\n    for j in x_test_polarity[0][0]:\n        x_tests.append(x_test_polarity[i][0][j])\nx_tests=np.array(x_tests)\nx_tests=x_tests.reshape(-1,4)\nx_tests.shape","e2df9430":"from scipy.sparse import hstack","cfdbb97a":"x_tr=hstack((train_tweet,x_t))\nx_te=hstack((test_tweet,x_tests))\n\nprint(x_tr.shape)\nprint(x_te.shape)","ae3b5da4":"print(test_tweet)","df652e57":"wt={0:1,1:5}            #since the data is imbalanced , we assign some more weight to class 1\n\nclf=DecisionTreeClassifier(class_weight=wt)\n\nparameters=dict(max_depth=[1,5,10,50],min_samples_split=[5,10,100,500])\n\nsearch=RandomizedSearchCV(clf,parameters,random_state=10)\nresult=search.fit(x_tr,y_train)\nresult.cv_results_","d9f3e427":"search.best_params_","b1048c1f":"cls = DecisionTreeClassifier(max_depth=50,min_samples_split=5,random_state=10,class_weight=wt)\ncls.fit(x_tr,y_train)","f129530b":"y_pred_train=cls.predict(x_tr)\ny_pred_test=cls.predict(x_te)","940c8c0a":"train_fpr,train_tpr,tr_treshold=roc_curve(y_train,y_pred_train)\ntest_fpr,test_tpr,te_treshold=roc_curve(y_test,y_pred_test)\n\ntrain_auc=auc(train_fpr,train_tpr)\ntest_auc=auc(test_fpr,test_tpr)\n\nplt.plot(train_fpr,train_tpr,label='Train AUC = '+str(train_auc))\nplt.plot(test_fpr,test_tpr,label='Test AUC = '+str(test_auc))\nplt.legend()\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title(\"AUC_Curve\")\nplt.grid()\nplt.show()","b1d4ffbb":"def find_best_threshold(threshold, fpr, tpr):\n    \"\"\"it will give best threshold value that will give the least fpr\"\"\"\n    t = threshold[np.argmax(tpr*(1-fpr))]\n    \n    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n    \n    return t\n\ndef predict_with_best_t(proba, threshold):\n    \"\"\"this will give predictions based on best threshold value\"\"\"\n    predictions = []\n    for i in proba:\n        if i>=threshold:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    return predictions","09886dd3":"#computing confusion matrix for set_1\n\nfrom sklearn.metrics import confusion_matrix\nbest_t = find_best_threshold(tr_treshold, train_fpr, train_tpr)\nprint(\"Train confusion matrix\")\nm_tr=(confusion_matrix(y_train, predict_with_best_t(y_pred_train, best_t)))\nprint(m_tr)\nprint(\"Test confusion matrix\")\nm_te=(confusion_matrix(y_test, predict_with_best_t(y_pred_test, best_t)))\nprint(m_te)","43cc90d8":"print(classification_report(y_test, y_pred_test))","971be1a2":"df3 = pd.DataFrame(classification_report(y_pred_test, \n                                        y_test, digits=2,\n                                        output_dict=True)).T\n\ndf3['support'] = df3.support.apply(int)\n\ndf3.style.background_gradient(cmap='viridis',\n                             subset=pd.IndexSlice['0':'9', :'f1-score'])\n","a08fa937":"dt_df = pd.DataFrame(columns=['tweet', 'sentiment-predicted', 'label'])","2be51576":"def color_negative_red(value):\n    \"\"\"\n    Colors elements in a dateframe\n    green if positive and red if\n    negative. Does not color NaN\n    values.\n    \"\"\"\n\n    if value == 'Pos':\n        color = 'green'\n    else:\n        color='red'\n\n    return 'color: %s' % color","0f450376":"dt_df = pd.DataFrame(columns=['tweet', 'sentiment-predicted', 'label'])\ndt_df['tweet'] = x_test['tweet']\ndt_df['sentiment-predicted'] = y_pred_test\ndt_df['label'] =y_test\ndt_df.replace(to_replace=[0,1],value=['Pos','Neg'], inplace=True)\n(dt_df.sample(10)[['tweet','sentiment-predicted','label']].style\n    .applymap(color_negative_red, subset=['sentiment-predicted','label']))","a67f7265":"vec=CountVectorizer(min_df=10)\nvec.fit(x_train['tweet'].values)\n\nx_tr_count=vec.transform(x_train['tweet'].values)\nx_te_count=vec.transform(x_test['tweet'].values)\nx_tr_count.shape","4abf57d3":"x_tr_data=hstack((x_tr_count,x_t))\nx_te_data=hstack((x_te_count,x_tests))\n\nx_trn=scipy.sparse.csr_matrix(x_tr_count)\nx_tst=scipy.sparse.csr_matrix(x_te_count)","fe94b075":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn.model_selection import KFold","3af94a71":"nb = MultinomialNB()\nparams = {'alpha': [0.1, 0.5, 1, 2]}\ngs = GridSearchCV(nb,param_grid=params, return_train_score=True)\ngs.fit(x_trn,y_train)","dffbd478":"gs.best_estimator_","0f6bd058":"nb = MultinomialNB()\nparams = {'alpha': [0.1, 2]}\ngs = GridSearchCV(nb,param_grid=params, return_train_score=True)\ngs.fit(x_trn,y_train)","54186b5b":"skf = StratifiedKFold(n_splits=8)\nparams = {'alpha': [0.1, 2]}\nnb = MultinomialNB()\ngs = GridSearchCV(nb, cv=skf, param_grid=params, return_train_score=True)\ngs.fit(x_trn,y_train)","95d7fb72":"gs.best_score_","39a12c47":"train_pred=gs.predict(x_trn)\ntest_pred=gs.predict(x_tst)","f1a07332":"train_fpr,train_tpr,tr_treshold=roc_curve(y_train,train_pred)\ntest_fpr,test_tpr,te_treshold=roc_curve(y_test,test_pred)\n\ntrain_auc=auc(train_fpr,train_tpr)\ntest_auc=auc(test_fpr,test_tpr)\n\nplt.plot(train_fpr,train_tpr,label='Train AUC = '+str(train_auc))\nplt.plot(test_fpr,test_tpr,label='Test AUC = '+str(test_auc))\nplt.legend()\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title(\"AUC_Curve\")\nplt.grid()\nplt.show()","6f85d596":"df1 = pd.DataFrame(classification_report(gs.predict(x_tst), \n                                        y_test, digits=2,\n                                        output_dict=True)).T\n\ndf1['support'] = df1.support.apply(int)\n\ndf1.style.background_gradient(cmap='viridis',\n                             subset=pd.IndexSlice['0':'9', :'f1-score'])\n","b6b92579":"mod = MultinomialNB()\nmod.fit(x_trn,y_train)","f9d04611":"train_pred=mod.predict(x_trn)\ntest_pred=mod.predict(x_tst)","f8ce9e03":"train_fpr,train_tpr,tr_treshold=roc_curve(y_train,train_pred)\ntest_fpr,test_tpr,te_treshold=roc_curve(y_test,test_pred)\n\ntrain_auc=auc(train_fpr,train_tpr)\ntest_auc=auc(test_fpr,test_tpr)\n\nplt.plot(train_fpr,train_tpr,label='Train AUC = '+str(train_auc))\nplt.plot(test_fpr,test_tpr,label='Test AUC = '+str(test_auc))\nplt.legend()\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title(\"AUC_Curve\")\nplt.grid()\nplt.show()","6a7ed4d9":"#get the summary of this model\n\nprint(classification_report(test_pred, y_test))","f9498820":"df1 = pd.DataFrame(classification_report(mod.predict(x_tst), \n                                        y_test, digits=2,\n                                        output_dict=True)).T\n\ndf1['support'] = df1.support.apply(int)\n\ndf1.style.background_gradient(cmap='viridis',\n                             subset=pd.IndexSlice['0':'9', :'f1-score'])\n","d34dd2e6":"dt_df = pd.DataFrame(columns=['tweet', 'sentiment-predicted', 'label'])\ndt_df['tweet'] = x_test['tweet']\ndt_df['sentiment-predicted'] = test_pred\ndt_df['label'] =y_test\ndt_df.replace(to_replace=[0,1],value=['Pos','Neg'], inplace=True)\n(dt_df.sample(10)[['tweet','sentiment-predicted','label']].style\n    .applymap(color_negative_red, subset=['sentiment-predicted','label']))","fe77e13f":"from xgboost import XGBClassifier","fd39c19e":"y_train.value_counts()","b5ee7f31":"# xg=XGBClassifier(use_label_encoder=False)\n# param=dict(max_depth=[4,6,8,10],n_estimators=[100,500,1000,1500])\n# search=RandomizedSearchCV(xg,param,random_state=10,)\n# srch=search.fit(x_tr,y_train,)\n# srch.cv_results_","418460ea":"# srch.best_estimator_","95156d77":"xgb = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n              min_child_weight=1, missing=None, monotone_constraints='()',\n              n_estimators=500, n_jobs=8, num_parallel_tree=1, random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None,).fit(x_tr, y_train)\n\nprediction = xgb.predict(x_te) \n\nf1_score(y_test, prediction)","60f03884":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n","14d04004":"kfmodel = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n              min_child_weight=1, missing=None, monotone_constraints='()',\n              n_estimators=500, n_jobs=8, num_parallel_tree=1, random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None,)\nkfold = KFold(n_splits=8, random_state=7, shuffle=True)\nresults = cross_val_score(kfmodel, x_tr, y_train, cv=kfold)\nprint(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\nprint(results)","64185fa8":"train_prediction=xgb.predict(x_tr)","ce4d59fa":"train_fpr,train_tpr,tr_treshold=roc_curve(y_train,train_prediction)\ntest_fpr,test_tpr,te_treshold=roc_curve(y_test,prediction)\n\ntrain_auc=auc(train_fpr,train_tpr)\ntest_auc=auc(test_fpr,test_tpr)\n\nplt.plot(train_fpr,train_tpr,label='Train AUC = '+str(train_auc))\nplt.plot(test_fpr,test_tpr,label='Test AUC = '+str(test_auc))\nplt.legend()\nplt.xlabel('FPR')\nplt.ylabel('TPR')\nplt.title(\"AUC_Curve\")\nplt.grid()\nplt.show()","5a217302":"print(classification_report(y_test, prediction))","e4a374e3":"df2 = pd.DataFrame(classification_report(y_test, prediction, digits=2,\n                                        output_dict=True)).T\n\ndf2['support'] = df2.support.apply(int)\n\ndf2.style.background_gradient(cmap='viridis',\n                             subset=pd.IndexSlice['0':'1', :'f1-score'])\n","60f917ad":"dt_df = pd.DataFrame(columns=['tweet', 'sentiment-predicted', 'label'])\ndt_df['tweet'] = x_test['tweet']\ndt_df['sentiment-predicted'] = prediction\ndt_df['label'] =y_test\ndt_df.replace(to_replace=[0,1],value=['Pos','Neg'], inplace=True)\n(dt_df.sample(10)[['tweet','sentiment-predicted','label']].style\n    .applymap(color_negative_red, subset=['sentiment-predicted','label']))","1d0dd518":"***From the bar graph we can clearly see that there are more not racist tweets than the racist tweets.***","53c537b9":"## XGBOOST","ffb00a7d":"#### TFIDF for text data","863469bb":"##### Now the text data is cleaned","5c2503f2":"***We got auc score= 0.7625***","50e786e8":"Since the data is in text format, we have to preprocess the data and clean the data to vectorize the data.","c8094e14":"## Data Preprocessing","b9b1c5d1":"First we will replace the all blank spaces, - with underscore and convert all the letters to lower case.","9b57a821":"#### Hyperparameter Tuning","4da1be07":"## NAIVE BAYES","13e7c09e":"### CountVectorizer()","8f53b87c":"### DecisionTreeClassifier()","1ddba514":"***Now we are ready with the data.***","371ea5d8":"##### Convert the vectors into scipy.sparse matrix","00c32af6":"## IMPORT LIBRARIES...","63d58472":"We can observe that there are more reviews with 0 label i.e. tweet is not racist\/sexist.<br>\nSo our dataset is imbalanced","172a4ef9":"### Vectorization...","9f3a2334":"### Splitting data into train and test"}}