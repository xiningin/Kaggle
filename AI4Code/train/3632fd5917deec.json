{"cell_type":{"2b1a8011":"code","c08d0689":"markdown"},"source":{"2b1a8011":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndf=pd.read_csv(\"..\/input\/titanic-cleaned-data\/train_clean.csv\")\ndf_test=pd.read_csv(\"..\/input\/titanic-cleaned-data\/test_clean.csv\")\n\ndf.info() # Print a concise summary of a DataFrame.\ndf.head() # Return the first 5 rows.\ndf.columns # Return the column labels of the DataFrame.\ndf.describe() # Generate descriptive statistics.\n\n# Since majority of cabin values are missing -> remove the column\n\n# * PassengerId is unique -> drop column\n# * Name is unique -> drop column\n# * TicketId is unique-> drop column\n# * They do not contribute to the survival probability.\n\ndf.drop([\"Cabin\",\"Name\",\"PassengerId\",\"Ticket\"],axis=1,inplace=True)\ndf_test.drop([\"Cabin\",\"Name\",\"PassengerId\",\"Ticket\"],axis=1,inplace=True)\n\ndf=df[['Age', 'Embarked', 'Fare', 'Parch', 'Pclass', 'Sex', 'SibSp', 'Title', 'Family_Size','Survived']]\ndf_test=df_test[['Age', 'Embarked', 'Fare', 'Parch', 'Pclass', 'Sex', 'SibSp', 'Title', 'Family_Size','Survived']]\n\n# Age and fare are in float -> convert it into integer and then into categories.\n\n# Age is grouped into 7 categories\ndata=[df,df_test]\nfor d in data:\n    d['Age'] = d['Age'].astype(int)\n    d.loc[ d['Age'] <= 10, 'Age'] = 0\n    d.loc[(d['Age'] > 10) & (d['Age'] <= 18), 'Age'] = 1\n    d.loc[(d['Age'] > 18) & (d['Age'] <= 25), 'Age'] = 2\n    d.loc[(d['Age'] > 25) & (d['Age'] <= 30), 'Age'] = 3\n    d.loc[(d['Age'] > 30) & (d['Age'] <= 35), 'Age'] = 4\n    d.loc[(d['Age'] > 35) & (d['Age'] <= 40), 'Age'] = 5\n    d.loc[(d['Age'] > 40) & (d['Age'] <= 65), 'Age'] = 6\n    d.loc[ d['Age'] > 65, 'Age'] = 6\n\n# Fare is grouped into 6 categories\ndata = [df,df_test]\nfor d in data:\n    d.loc[ d['Fare'] <= 8, 'Fare'] = 0\n    d.loc[(d['Fare'] > 8) & (d['Fare'] <= 15), 'Fare'] = 1\n    d.loc[(d['Fare'] > 15) & (d['Fare'] <= 31), 'Fare']   = 2\n    d.loc[(d['Fare'] > 31) & (d['Fare'] <= 99), 'Fare']   = 3\n    d.loc[(d['Fare'] > 99) & (d['Fare'] <= 250), 'Fare']   = 4\n    d.loc[ d['Fare'] > 250, 'Fare'] = 5\n    d['Fare'] = d['Fare'].astype(int)\n\n# Convert Survived from float-> int\ndf.Survived=df.Survived.astype(int)\n\n# Creating test and training samples. Splitting the dataframe into two random samples(80% and 20%) for traing and testing.\n\ntrain, test = train_test_split(df, test_size=0.2)\n\nsurvived_yes=train.loc[train.Survived==1]\nP_yes=len(survived_yes)\/len(train)\nP_yes # Probability of Survival in training data\n\n\nsurvived_no=train.loc[train.Survived==0]\nP_no=len(survived_no)\/len(train)\nP_no # Probability of not Survival in training data\n\n\n# value counts of each category of an attribute.\n\nfor col in train.columns:\n    count=train[col].value_counts() \n    print(count)\n    \natr=list(df.columns.values)\noutput_dataframe= pd.DataFrame(columns = ['Actual', 'Predicted']) \n\nfor i in test.itertuples():\n    test1=list(i)\n    test1.pop(0) # removing Index (unwanted)\n    ans=test1.pop() # removing actual value\n    py=1\n    for i in range(9):\n        val = train[(train[atr[i]] == test1[i]) & (train.Survived == 1)].count().values.item(0)\n        py = py * (val) \/ len(survived_yes)\n        total_yes = py * P_yes\n    pn=1\n    for i in range(9):\n        val = train[(train[atr[i]] == test1[i]) & (train.Survived == 0)].count().values.item(0)\n        pn = pn * (val) \/ len(survived_no)\n        total_no = pn * P_no\n    if total_yes>total_no:\n        list1=[[ans,1]] #Survived\n        output_dataframe=output_dataframe.append(pd.DataFrame(list1,columns=['Actual','Predicted']),ignore_index=True)\n    else:\n        list0=[[ans,0]] #NotSurvived\n        output_dataframe=output_dataframe.append(pd.DataFrame(list0,columns=['Actual','Predicted']),ignore_index=True)\n\n\n# Evaluation metrics\n\nTP=0\nTN=0\nFP=0\nFN=0\nfor index,row in output_dataframe.iterrows():\n    if row['Predicted']== row['Actual'] and row['Predicted']==1:\n        TP += 1\n    elif row['Predicted']== row['Actual'] and row['Predicted']==0:\n        TN +=1\n    elif row['Predicted']==1:\n        FP +=1\n    else: \n        FN +=1\n        \n# Accuracy = [TP + TN] \/ Total Population\naccuracy= (TP+TN)\/len(output_dataframe)\nprint(\"The accuracy for the test set is \",accuracy *100,\"%\")\n\n# Precision = TP \/ [TP + FP]\n# tells us about the success probability of making a correct positive class classification.\nprecision = TP \/ (TP+FP)\nprint(\"The precision for the test set is \",precision *100,\"%\")\n\n# Recall = TP \/ [TP + FN]\n# explains how sensitive the model is towards identifying the positive class.\nrecall =  TP \/ (TP+FN)\nprint(\"The recall for the test set is \",recall *100,\"%\")","c08d0689":"Using na\u00efve Bayes classifier to predict which passengers survived the Titanic shipwreck. Naive Bayes classifier is built from scratch."}}