{"cell_type":{"c6c39def":"code","09b826ea":"code","7c56adc9":"code","6be6e473":"code","62d1f6e6":"code","dc0af702":"code","cfd4a1da":"markdown","8e49700e":"markdown","16b52ac3":"markdown","83918623":"markdown","3a182392":"markdown","ee9a3b0c":"markdown","5616a9ac":"markdown","58285d17":"markdown","51789ab5":"markdown"},"source":{"c6c39def":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected = True)\nimport plotly.graph_objs as go\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n\n# Any results you write to the current directory are saved as output.","09b826ea":"data = pd.DataFrame(dict(x1=[1, 1, 0, 0], x2=[1, 0, 1, 0], result=[0, 1, 1, 0]))\nx_train = np.array(data.loc[:, ['x1', 'x2']]).T\ny_train = np.array(data.loc[:, ['result']]).T\ndata","7c56adc9":"# intialize parameters and layer sizes(Just giving some random numbers, you can change if you wish)\ndef initialize_parameters():\n    parameters = {\"weight1\": np.array([[0.0001, 0.2343], [0.003451, 0.22113]]),\n                  \"bias1\": np.array([[0.13511], [0.0989]]),\n                  \"weight2\": np.array([[0.45641, 0.00099]]),\n                  \"bias2\": np.array([[0.009]]),\n                 }\n    return parameters\n\n# Sigmoid\ndef sigmoid(x):\n    x = 1\/(1+np.exp(-x))\n    return x\n\n# Cost\ndef cost_funtion(out_y, y_train):\n    loss = -y_train*np.log(out_y)-(1-y_train)*np.log(1-out_y)\n    cost = np.sum(loss)\/y_train.shape[1]\n    return cost\n\ndef forward_backward_propagation(parameters, x_train, y_train):\n    #forward\n    h = np.dot(parameters[\"weight1\"],x_train) +parameters[\"bias1\"]\n    out_h = np.tanh(h)\n    y = np.dot(parameters[\"weight2\"],out_h) + parameters[\"bias2\"]\n    out_y = sigmoid(y)\n    \n    #cost\n    cost = cost_funtion(out_y, y_train)\n    \n    #backward\n    sigma_y = out_y-y_train \n    dW2 = np.dot(sigma_y,out_y.T)\/y_train.shape[1]\n    db2 = np.sum(sigma_y,axis =1,keepdims=True)\/y_train.shape[1]\n    \n    sigma_h = np.dot(parameters[\"weight2\"].T,sigma_y)*(1 - np.power(out_h, 2))\n    dW1 = np.dot(sigma_h,x_train.T)\/y_train.shape[1]\n    db1 = np.sum(sigma_h,axis =1,keepdims=True)\/y_train.shape[1]\n    \n    derivatives = {\"dweight1\": dW1,\n             \"dbias1\": db1,\n             \"dweight2\": dW2,\n             \"dbias2\": db2}\n    return derivatives, cost\n\n# update parameters\ndef update_parameters(parameters, x_train, y_train, learning_rate, iteration):\n    cost_list = []\n    cost_index = []\n    for i in range(iteration):\n        derivatives,cost = forward_backward_propagation(parameters, x_train, y_train)\n        parameters = {\"weight1\": parameters[\"weight1\"] - learning_rate*derivatives[\"dweight1\"],\n                      \"bias1\": parameters[\"bias1\"] - learning_rate*derivatives[\"dbias1\"],\n                      \"weight2\": parameters[\"weight2\"] - learning_rate*derivatives[\"dweight2\"],\n                      \"bias2\": parameters[\"bias2\"] - learning_rate*derivatives[\"dbias2\"]}\n        if i % 100 == 0:\n            cost_list.append(cost)\n            cost_index.append(i)    \n    return parameters, cost_index,cost_list\n\n# prediction\ndef predict(parameters, x_train):\n    h = np.dot(parameters[\"weight1\"],x_train) +parameters[\"bias1\"]\n    out_h = np.tanh(h)\n    y = np.dot(parameters[\"weight2\"],out_h) + parameters[\"bias2\"]\n    out_y = sigmoid(y)\n    y_prediction = np.zeros((1,x_train.shape[1]))\n    \n    for i in range(out_y.shape[1]):\n        if out_y[0,i]<= 0.5:\n            y_prediction[0,i] = 0\n        else:\n            y_prediction[0,i] = 1\n    return y_prediction\n\n# 2 - Layer neural network\ndef implement(x_train, y_train, learning_rate, iteration):\n    parameters = initialize_parameters()\n    parameters, cost_index,cost_list = update_parameters(parameters, x_train, y_train, learning_rate, iteration)\n    \n    plt.plot(cost_index,cost_list)\n    plt.title(\"Cost According To The Iterations\")\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()    \n        \n    # predict\n    y_prediction = predict(parameters,x_train)\n    \n    return parameters, y_prediction","6be6e473":"parameters, y_prediction= implement(x_train, y_train, learning_rate=0.3, iteration=1200)\n\nprint(\"Expected Values: \", y_train)\nprint(\"Results : \", y_prediction.astype(int))","62d1f6e6":"def fun(parameters, x_train):\n    h = np.dot(parameters[\"weight1\"],x_train) +parameters[\"bias1\"]\n    out_h = np.tanh(h)\n    y = np.dot(parameters[\"weight2\"],out_h) + parameters[\"bias2\"]\n    out_y = sigmoid(y)\n    return out_y\n\nx1 = x2 = np.arange(0, 1.1, 0.1)\nX1, X2 = np.meshgrid(x1, x2)\nINPUTS = np.array([np.ravel(X1), np.ravel(X2)])\n\nZ_sigmoid = fun(parameters, INPUTS)\nZ_logistic = [1 if Z_sigmoid[0,i]>0.5 else 0 for i in range(Z_sigmoid.size)]\n\nsurface_data = pd.DataFrame(dict(X1=X1.ravel(), X2=X2.ravel(), Z=Z_sigmoid.ravel()))","dc0af702":"trace1 = go.Scatter3d(\n    x=data.x1,\n    y=data.x2,\n    z=data.result,\n    name = \"Values\",\n    mode='markers',  \n)\ntrace2 = go.Mesh3d(\n    x=X1.ravel(), \n    y=X2.ravel(), \n    z=Z_sigmoid.ravel(),\n    name = \"OutputY (Sigmoid Function))\",\n    color='#FFB6C1',\n    opacity=0.9\n)\ntrace3 = go.Mesh3d(\n    x=X1.ravel(), \n    y=X2.ravel(), \n    z=Z_logistic,\n    name = \"Logistic Regression(OutputY)\",\n    color='#008BF8',\n    opacity=0.7\n)\n\nlayout = go.Layout(\n    margin=dict(\n        l=0,\n        r=0,\n        b=0,\n        t=0\n    ), \n    scene=dict(\n        camera=dict( eye=dict(x=1.0700, y=-1.700, z=0.6500) ),\n        xaxis = dict(title='x1'),\n        yaxis = dict(title='x2'),\n        zaxis = dict(title='output'),\n    ),\n)    \nfig = go.Figure(data=[trace1, trace2, trace3], layout=layout)\niplot(fig)","cfd4a1da":"**<h1>Defining Data<\/h1>**<a id=\"2\"><\/a>","8e49700e":"**<h1>Let's Visualize What We've Done<\/h1>**<a id=\"5\"><\/a>","16b52ac3":"**<h1>Deep Learning<\/h1>**<a id=\"3\"><\/a>","83918623":"1. [Tutorial](#1)\n2. [Defining Data](#2)\n3. [Deep Learning](#3)\n4. [Predict Results](#4)\n5. [Visualizing](#5)","3a182392":"**<h1>Tutorial<\/h1>**<a id=\"1\"><\/a>","ee9a3b0c":"**Now you can see just the code but I'll update these two tutorials as soon as possible when I have  time. :\/**","5616a9ac":"![picture](https:\/\/i.ibb.co\/sK97NvL\/DL3.png)","58285d17":"**<h1>Predict Results<\/h1>**<a id=\"4\"><\/a>","51789ab5":"**Before look into this tutorial, I strongly recommend to look at my other kernel which name is: **\n\n[\"Training a perceptron for \"logical OR operation\"](https:\/\/www.kaggle.com\/behic1\/training-a-perceptron-for-logical-or-operation?scriptVersionId=10569058)"}}