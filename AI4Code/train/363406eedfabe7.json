{"cell_type":{"3bc1f506":"code","35f5f5e4":"code","97f71c3b":"code","169d58b0":"code","dc7feb62":"code","9497bf82":"code","8684f33d":"code","fa6b02de":"code","894b0b40":"code","78a108dd":"code","f3e14aec":"code","5344dcbc":"code","41bf9ea6":"code","5faaedc9":"code","d909837f":"code","5a823fd4":"code","b2384dff":"code","36549baa":"code","2168c898":"code","aa347cab":"code","1f4f24bf":"code","050244d9":"code","df9f9b20":"code","6f848287":"code","fb7460c6":"code","c2325265":"code","a69292a5":"code","5e4dc7e3":"code","f034fb48":"code","d82de461":"code","755a7019":"code","2a2646e2":"code","36a5a4af":"code","b04a3b50":"code","f9fe8cf1":"code","d32ce256":"markdown","b0095984":"markdown","e5efa8fb":"markdown","4d599e71":"markdown","3153b183":"markdown","5540f5a6":"markdown","5391f59a":"markdown","7c0e7015":"markdown","6bdad823":"markdown","714a8b9b":"markdown","fea49478":"markdown","fcda338d":"markdown","7facf397":"markdown","b05e6d40":"markdown","71101a10":"markdown","2df8c989":"markdown","09edc297":"markdown","322fed9d":"markdown","44cef89e":"markdown","39f25494":"markdown","837e105a":"markdown","6b9b298d":"markdown","48a89c34":"markdown","621b91d9":"markdown","279d88bd":"markdown","8abe9a6a":"markdown","465577a1":"markdown","d29ff504":"markdown","b9e9fca6":"markdown","6145b72c":"markdown","8f1801ad":"markdown","56681c50":"markdown","463c86f9":"markdown","253654a8":"markdown","66ae1482":"markdown","68c4e2bf":"markdown","819b7bc5":"markdown","0585b78d":"markdown","04bc9a6a":"markdown","3f5becb0":"markdown","1ebb601f":"markdown","788dbc70":"markdown","938c0d35":"markdown","4e81e899":"markdown","9508a598":"markdown","104d90dc":"markdown","0e54492d":"markdown"},"source":{"3bc1f506":"!pip install pynndescent","35f5f5e4":"# importing necessary modules\nimport numpy as np\nimport pandas as pd\n\n# loading clean data (easy to get from original notebook)\ndf = pd.read_csv('..\/input\/clean-2018-kaggle-survey-data\/clean_dataset_v2.csv').drop('Unnamed: 0', axis=1)\n\n# getting non-US residents, males and working in finance\npeople_in_my_context = (df\n                        [(df['Q3-United States of America'] == 0) & # non-US residents\n                         (df['Q1-Male'] == 1) & # males\n                         (df['Q7-Accounting\/Finance'] == 1)]) # working in finance\n\n# getting average likelihood of being in top 20% for this group\nlikelihood_top20 = people_in_my_context.groupby('Q6-Software Engineer')['top20'].mean()\n\n# importing plotly to show in a bar graph\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)\n\n# trace storing data for bar graph\ntrace = go.Bar(x=['Software Engineer', 'NOT Software Engineer'],\n               y=likelihood_top20.values[::-1] * 100,\n               text = ((likelihood_top20 * 100).round(2).astype(str) + '%').values[::-1],\n               textposition='auto',\n               textfont=dict(size=14),\n               width=[0.5,0.5])\n\n# layout of plot\nlayout = go.Layout(title='Likelihood of being on Top 20%: non-US, Male, and working in Finance',\n                   xaxis=dict(tickfont=dict(size=16)), width=800, height=400)\n\n# building figure and plotting\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)","97f71c3b":"# checking effect of US\nUS_effect = df.groupby(['Q3-United States of America','Q6-Software Engineer'])['top20'].mean().reset_index()\n\n# trace storing data for bar graph\ntrace1 = go.Bar(x=['NOT US Resident', 'US Resident'],\n                y=US_effect[US_effect.iloc[:,1] == 0]['top20'] * 100,\n                text = ((US_effect[US_effect.iloc[:,1] == 0]['top20'] * 100).round(2).astype(str) + '%'),\n                textposition='auto',\n                textfont=dict(size=14),\n                name = 'NOT Software Engineer')\n\n# trace storing data for bar graph\ntrace2 = go.Bar(x=['NOT US Resident', 'US Resident'],\n                y=US_effect[US_effect.iloc[:,1] == 1]['top20'] * 100,\n                text = ((US_effect[US_effect.iloc[:,1] == 1]['top20'] * 100).round(2).astype(str) + '%'),\n                textposition='auto',\n                textfont=dict(size=14),\n                name = 'Software Engineer')\n\n# layout of plot\nlayout = go.Layout(title='Likelihood of being on Top 20%: Effect of being in the US',\n                   xaxis=dict(tickfont=dict(size=16)), width=800, height=400,\n                   legend=dict(orientation='h', x=0.25))\n\n# building figure and plotting\nfig = go.Figure(data=[trace1, trace2], layout=layout)\niplot(fig)","169d58b0":"# observing result\npd.set_option('display.max_columns', 200)\ndf.head()","dc7feb62":"# creating new target variables\ndf['top50'] = (df['numerical_compensation'] > df['numerical_compensation'].quantile(0.5)).astype(int)\ndf['percentile'] = np.round(df['numerical_compensation'].rank(pct=True) * 100)\ndf['decile'] = np.round(df['numerical_compensation'].rank(pct=True) * 10) * 10\ndf['quintile'] = np.round(df['numerical_compensation'].rank(pct=True) * 5) * 20","9497bf82":"# list of targets\ntargets = ['top20','top50','percentile','decile', 'quintile', 'numerical_compensation','normalized_numerical_compensation']\n\n# explanatory variables dataframe\nexplanatory_vars_df = df.drop(targets, axis=1)\n\n# target variables dataframe\ntargets_df = df[targets]","8684f33d":"# choice of treatment variable - we do not include it in the model\ntreat_var = 'Q6-Software Engineer'\nW = df[treat_var]\n\n# design matrix, dropping treatment variable\nX = explanatory_vars_df.drop(treat_var,axis=1)\n\n# target variable\ny = targets_df['top20']","fa6b02de":"# importing packages that we'll use\nimport graphviz\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\n\n# let us fit a decision tree\ndt = DecisionTreeClassifier(max_leaf_nodes=3, min_samples_leaf=100)\ndt.fit(X, y)\n\n# let us plot it\ndot_data = tree.export_graphviz(dt, out_file=None, \n                                feature_names=X.columns.str.replace('<','less than'),  \n                                filled=True, rounded=True,  \n                                special_characters=True)  \ngraph = graphviz.Source(dot_data)  \ngraph ","894b0b40":"# creating a df to measure the effects\ndt_effect_df = pd.DataFrame({'cluster': dt.apply(X), 'Q6-Software Engineer': W, 'avg. outcome': y})\n\n# let us check the effects\ndt_effect_df.groupby(['cluster','Q6-Software Engineer']).mean().round(2)","78a108dd":"# libraries\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import KFold, cross_val_score\n\n# CV method\nkf = KFold(n_splits=5, shuffle=True)\n\n# model \net = ExtraTreesClassifier(n_estimators=200, min_samples_leaf=5, bootstrap=True, n_jobs=-1, random_state=42)\n\n# generating validation predictions\nresult = cross_val_score(et, X, y, cv=kf, scoring='roc_auc')\n\n# calculating result\nprint('Cross-validation results (AUC):', result, 'Average AUC:', np.mean(result))","f3e14aec":"# let us train our model with the full data\net.fit(X, y)\n\n# let us check the most important variables\nimportances = pd.DataFrame({'variable': X.columns, 'importance': et.feature_importances_})\nimportances.sort_values('importance', ascending=False, inplace=True)\nimportances.head(10)","5344dcbc":"# and get the leaves that each sample was assigned to\nleaves = et.apply(X)\nleaves","41bf9ea6":"# calculating similarities \nsims_from_first = (leaves[0,:] == leaves[1:,:]).sum(axis=1)\n\n# most similar row w.r.t the first\nmax_sim, which_max = sims_from_first.max(), sims_from_first.argmax()\nprint('The most similar row from the first is:', which_max, ', having co-ocurred', max_sim, 'times with it in the forest')","5faaedc9":"# number of variables that these guys are equal\nn_cols_equal = (X.iloc[0] == X.iloc[6592]).loc[importances['variable']].head(20).sum()\nprint('Rows 0 and 6592 are equal in {} of the 20 most important variables'.format(n_cols_equal))\nX.iloc[[0, 6592]].loc[:,importances['variable'].head(20)]","d909837f":"# let us build a supervised embedding using UMAP\n# 'hamming' metric is equal to the proportion of leaves\n# that two samples have NOT co-ocurred (dissimilarity metric)\nfrom umap import UMAP\nembed = UMAP(metric='hamming', random_state=42).fit_transform(leaves)","5a823fd4":"# opening figure\nimport matplotlib.pyplot as plt\nplt.style.use('bmh')\nfig = plt.figure(figsize=(12,7), dpi=120)\n\n# scatterplot\nplt.scatter(embed[y==0,0], embed[y==0,1], s=2, color='black', alpha=0.8, label='Bottom 80% earners')\nplt.scatter(embed[y==1,0], embed[y==1,1], s=2, color='gold', alpha=0.8, label='Top 20% earners')\n\n# titles\nplt.title('Supervised Embedding built from the leaves of our forest: natural clusters')\nplt.ylabel('$x_1$'); plt.xlabel('$x_0$')\n\n# legend\nplt.legend(markerscale=5)\nplt.show()","b2384dff":"# variable to set the color\nvar = X['Q3-United States of America']\n\n# opening figure\nfig = plt.figure(figsize=(12,7), dpi=120)\n\n# scatterplot\nplt.scatter(embed[var==0,0], embed[var==0,1], s=2, color='red', alpha=0.8, label='NOT US Residents')\nplt.scatter(embed[var==1,0], embed[var==1,1], s=2, color='blue', alpha=0.8, label='US Residents')\n\n# titles\nplt.title('Supervised Embedding: US vs. non-US residents')\nplt.ylabel('$x_1$'); plt.xlabel('$x_0$')\n\n# legend\nplt.legend(markerscale=5)\nplt.show()","36549baa":"# variable to set the color\nvar = X['Q24-10-20 years']\n\n# opening figure\nfig = plt.figure(figsize=(12,7), dpi=120)\n\n# scatterplot\nplt.scatter(embed[var==0,0], embed[var==0,1], s=2, color='red', alpha=0.8, label='NOT 10-20 years of experience')\nplt.scatter(embed[var==1,0], embed[var==1,1], s=2, color='blue', alpha=0.8, label='10-20 years of experience')\n\n# titles\nplt.title('Supervised Embedding: people with 10-20 years of experience')\nplt.ylabel('$x_1$'); plt.xlabel('$x_0$')\n\n# legend\nplt.legend(markerscale=5)\nplt.show()","2168c898":"# variable to set the color\nvar = X['Q11-Build prototypes to explore applying machine learning to new areas']\n\n# opening figure\nfig = plt.figure(figsize=(12,7), dpi=120)\n\n# scatterplot\nplt.scatter(embed[var==0,0], embed[var==0,1], s=2, color='red', alpha=0.8, label='NOT build ML prototypes')\nplt.scatter(embed[var==1,0], embed[var==1,1], s=2, color='blue', alpha=0.8, label='build ML prototypes')\n\n# titles\nplt.title('Supervised Embedding: people that build ML prototypes')\nplt.ylabel('$x_1$'); plt.xlabel('$x_0$')\n\n# legend\nplt.legend(markerscale=5)\nplt.show()","aa347cab":"# opening figure\nfig = plt.figure(figsize=(12,7), dpi=120)\n\n# scatterplot\nvar = W\n#plt.scatter(embed[var==0,0], embed[var==0,1], s=10, color='red', alpha=0.8, label='NOT Software Engineers')\n#plt.scatter(embed[var==1,0], embed[var==1,1], s=10, color='blue', alpha=0.8, label='US Software Engineers')\nplt.scatter(embed[np.logical_and(y==0, W==0),0], embed[np.logical_and(y==0, W==0),1], s=45, color='red', alpha=0.5, label='Bottom 80% earners | NOT Software Engineers', marker='o')\nplt.scatter(embed[np.logical_and(y==1, W==0),0], embed[np.logical_and(y==1, W==0),1], s=45, color='red', alpha=0.5, label='Top 20% earners | NOT Software Engineers', marker='x')\nplt.scatter(embed[np.logical_and(y==0, W==1),0], embed[np.logical_and(y==0, W==1),1], s=75, color='blue', alpha=0.8, label='Bottom 80% earners | Software Engineers', marker='o')\nplt.scatter(embed[np.logical_and(y==1, W==1),0], embed[np.logical_and(y==1, W==1),1], s=75, color='blue', alpha=0.8, label='Top 20% earners | Software Engineers', marker='x')\n\n# titles\nplt.title('Checking distribution of Software Engineers and top earners for one cluster')\nplt.ylabel('$x_1$'); plt.xlabel('$x_0$')\n#plt.ylim(-5.6,-1.00); plt.xlim(-7.5,-3.4)\nplt.ylim(-0.45,1.0); plt.xlim(-6.7,-5.99)\n\n# legend\nplt.legend(markerscale=1.5)","1f4f24bf":"# so we can be agle to draw rectangles\nimport matplotlib.patches as patches\n\n# opening figure\nfig = plt.figure(figsize=(12,7), dpi=120)\n\n# scatterplot\nvar = W\n#plt.scatter(embed[var==0,0], embed[var==0,1], s=10, color='red', alpha=0.8, label='NOT Software Engineers')\n#plt.scatter(embed[var==1,0], embed[var==1,1], s=10, color='blue', alpha=0.8, label='US Software Engineers')\nplt.scatter(embed[np.logical_and(y==0, W==0),0], embed[np.logical_and(y==0, W==0),1], s=45, color='red', alpha=0.5, label='Bottom 80% earners | NOT Software Engineers', marker='o')\nplt.scatter(embed[np.logical_and(y==1, W==0),0], embed[np.logical_and(y==1, W==0),1], s=45, color='red', alpha=0.5, label='Top 20% earners | NOT Software Engineers', marker='x')\nplt.scatter(embed[np.logical_and(y==0, W==1),0], embed[np.logical_and(y==0, W==1),1], s=75, color='blue', alpha=0.8, label='Bottom 80% earners | Software Engineers', marker='o')\nplt.scatter(embed[np.logical_and(y==1, W==1),0], embed[np.logical_and(y==1, W==1),1], s=75, color='blue', alpha=0.8, label='Top 20% earners | Software Engineers', marker='x')\n\n# rectangle showing neighborhood\nrect = patches.Rectangle((-6.6,0.2),0.2,0.4,linewidth=3,linestyle='dashed',edgecolor='black',facecolor='none')\nax = plt.gca(); ax.add_patch(rect)\n\n\n# computing effects\nembed_df = pd.DataFrame({'x0':embed[:,0], 'x1':embed[:,1], 'W':W, 'y':y})\nembed_df['x0'] = embed_df['x0'].astype(float) \nembed_df['x1'] = embed_df['x1'].astype(float) \nlocal_effect = embed_df.query('0.2 < x1 < 0.6').query('-6.6 < x0 < -6.4').groupby('W')['y'].mean()\n\n# arrow\nplt.annotate('', xy=(-6.4, 0.4), xytext=(-6.25, 0.3),\n            arrowprops=dict(facecolor='black', shrink=0.05))\n\n# rectangle\nrect = patches.Rectangle((-6.24,0.13),0.11,0.32,linewidth=1,edgecolor='black',facecolor='white', zorder=100)\nax = plt.gca(); ax.add_patch(rect)\n\n\n# red ratio\nplt.scatter([-6.08 - 0.13], [-0.3 + 0.63 + 0.08], c=['red'], marker='x', s=60, alpha=0.7, zorder=101)\nplt.annotate('+', xy=(-6.085 - 0.13, -0.39 + 0.63  + 0.08), zorder=101)\nplt.plot([-6.1- 0.13, -6.06- 0.13], [-0.34 + 0.63 + 0.08,-0.34 + 0.63 + 0.08], 'k', zorder=101)\nplt.scatter([-6.1 - 0.13], [-0.38 + 0.63 + 0.08], c=['red'], marker='x', s=60, alpha=0.7, zorder=101)\nplt.scatter([-6.06 - 0.13], [-0.38 + 0.63 + 0.08], c=['red'], marker='o', s=60, alpha=0.7, zorder=101)\nplt.annotate('= {0:.0f}%'.format(local_effect[0]*100), xy=(-6.05- 0.13, -0.34 + 0.63+ 0.08), zorder=101)\n\n# blue ratio\nplt.scatter([-6.08 - 0.13], [-0.46 + 0.63 + 0.08], c=['blue'], marker='x', s=60, alpha=0.7, zorder=101)\nplt.annotate('+', xy=(-6.085 - 0.13, -0.55 + 0.63 + 0.08), zorder=101)\nplt.plot([-6.1 - 0.13, -6.06 - 0.13], [-0.50 + 0.63 + 0.08,-0.50 + 0.63 + 0.08], 'k', zorder=101)\nplt.scatter([-6.1 - 0.13], [-0.54 + 0.63 + 0.08], c=['blue'], marker='x', s=60, alpha=0.7, zorder=101)\nplt.scatter([-6.06 - 0.13], [-0.54 + 0.63 + 0.08], c=['blue'], marker='o', s=60, alpha=0.7, zorder=101)\nplt.annotate('= {0:.0f}%'.format(local_effect[1]*100), xy=(-6.05 - 0.13, -0.50 + 0.63+ 0.08), zorder=101)\n\n# titles\nplt.title('Looking at a local neighborhood')\nplt.ylabel('$x_1$'); plt.xlabel('$x_0$')\n#plt.ylim(-5.6,-1.00); plt.xlim(-7.5,-3.4)\nplt.ylim(-0.45,1.0); plt.xlim(-6.7,-5.99)\n\n# legend\nplt.legend(markerscale=1.5)","050244d9":"# importing NNDescent\nfrom pynndescent import NNDescent\n\n# let us use neighborhoods to estimate treatment effects\nindex = NNDescent(leaves, metric='hamming')\n\n# querying 100 nearest neighbors\nnearest_neighs = index.query(leaves, k=201)","df9f9b20":"# creating a df with treatment assignments and outcomes\ny_df = pd.DataFrame({'neighbor': range(X.shape[0]), 'y':y, 'W':W})\n\n# creating df with nearest neighbors\nnearest_neighs_df = pd.DataFrame(nearest_neighs[0]).drop(0, axis=1)\n\n# creating df with nearest neighbor weights\nnearest_neighs_w_df = pd.DataFrame(1 - nearest_neighs[1]).drop(0, axis=1)\n\n# processing the neighbors df\nnearest_neighs_df = (nearest_neighs_df\n                     .reset_index()\n                     .melt(id_vars='index')\n                     .rename(columns={'index':'reference','value':'neighbor'})\n                     .reset_index(drop=True))\n\n# processing the neighbor weights df\nnearest_neighs_w_df = (nearest_neighs_w_df\n                       .reset_index()\n                       .melt(id_vars='index')\n                       .rename(columns={'index':'reference','value':'weight'})\n                       .reset_index(drop=True))\n\n# joining the datasets and adding weighted y variable\nnearest_neighs_df = (nearest_neighs_df\n                     .merge(nearest_neighs_w_df)\n                     .drop('variable', axis=1)\n                     .merge(y_df, on='neighbor', how='left')\n                     .assign(y_weighted = lambda x: x.y*(x.weight))\n                     .sort_values('reference'))\n\n# let us check the neighbors dataframe\nnearest_neighs_df.head(10)","6f848287":"# processing to get the effects\ntreat_effect_df = nearest_neighs_df.assign(count=1).groupby(['reference','W']).sum()\ntreat_effect_df['y_weighted'] = treat_effect_df['y_weighted']\/treat_effect_df['weight']\ntreat_effect_df['y'] = treat_effect_df['y']\/treat_effect_df['count']\ntreat_effect_df = treat_effect_df.pivot_table(values=['y', 'y_weighted','weight','count'], columns='W', index='reference')\n\n# calculating treatment effects\ntreat_effect_df.loc[:,'effect'] = treat_effect_df['y'][1] - treat_effect_df['y'][0]\ntreat_effect_df.loc[:,'effect_weighted'] = treat_effect_df['y_weighted'][1] - treat_effect_df['y_weighted'][0]\n\n# not computing effect for clusters with few examples\nmin_sample_effect = 10\ntreat_effect_df.loc[(treat_effect_df['count'][0] < min_sample_effect) | (treat_effect_df['count'][1] < min_sample_effect), 'effect_weighted'] = np.nan\ntreat_effect_df.loc[(treat_effect_df['count'][0] < min_sample_effect) | (treat_effect_df['count'][1] < min_sample_effect), 'effect'] = np.nan\n\n# observing the result\ntreat_effect_df.head(10)","fb7460c6":"# opening figure\nfig = plt.figure(figsize=(12,7), dpi=120)\n\n# scatterplot\nplt.scatter(embed[:,0], embed[:,1], s=2, \n            c=treat_effect_df['effect_weighted'], \n            alpha=0.8, vmin=-0.08, vmax=0.08, cmap='viridis')\n\n# titles\nplt.title('Effect of being a Software Engineer: Top 20% earner probability lift')\nplt.ylabel('$x_1$'); plt.xlabel('$x_0$')\n\n# legend\nplt.colorbar()\nplt.show()","c2325265":"# let us fit a decision tree\nfrom sklearn.tree import DecisionTreeRegressor\ndt = DecisionTreeRegressor(max_leaf_nodes=5, min_samples_leaf=100)\ndt.fit(X.iloc[treat_effect_df['effect_weighted'].dropna().index], treat_effect_df['effect_weighted'].dropna())","a69292a5":"# let us plot a decision tree\nimport graphviz \ndot_data = tree.export_graphviz(dt, out_file=None, \n                                feature_names=X.columns.str.replace('<','less than'),  \n                                filled=True, rounded=True,  \n                                special_characters=True)  \ngraph = graphviz.Source(dot_data)  \ngraph ","5e4dc7e3":"# class for computing effects decision tree\nclass TreatmentEffectEstimate:\n    \n    # initializing\n    def __init__(self, explanatory_vars, target_vars, model=ExtraTreesClassifier(n_estimators=200, min_samples_leaf=5, bootstrap=True, n_jobs=-1)):\n        \n        # storing variables\n        self.explanatory_vars = explanatory_vars\n        self.target_vars = target_vars\n        self.model = model\n                \n    # running CV for model parameters\n    def compute_cv_results(self, metric='accuracy'):\n        \n        # CV method\n        kf = KFold(n_splits=5, shuffle=True)\n\n        # generating validation predictions\n        result = cross_val_score(self.model, self.X, self.y, cv=kf, scoring=metric)\n\n        # calculating result\n        return result, np.mean(result)\n    \n    # generating manifold with UMAP\n    def compute_manifold(self):\n        \n        # let us check the embedding with response and treatments\n        self.embed = UMAP(metric='hamming').fit_transform(self.leaves)\n        \n        # returning \n        return self.embed\n    \n    # running model and neighbors\n    def compute_effects(self, treat_var, target_var, n_neighbors=201, dt_max_leaves=5, dt_min_samples_leaf=100):\n        \n        # separating explanatory vars, treatment var and target var\n        self.X = self.explanatory_vars.drop(treat_var, axis=1)\n        self.W = self.explanatory_vars[treat_var].copy()\n        self.y = self.target_vars[target_var].copy()\n        \n        # creating a df with treatment assignments and outcomes\n        self.y_df = pd.DataFrame({'neighbor': range(self.X.shape[0]), 'y':self.y, 'W':self.W})\n        \n        # let us train our model with the full data\n        self.model.fit(self.X, self.y)\n\n        # and get the leaves that each sample was assigned to\n        self.leaves = self.model.apply(self.X)\n        \n        # let us use neighborhoods to estimate treatment effects in the neighborhood\n        self.index = NNDescent(self.leaves, metric='hamming')\n        \n        # querying 100 nearest neighbors\n        self.nearest_neighs = self.index.query(self.leaves, k=n_neighbors)\n        \n        # creating df with nearest neighbors\n        self.nearest_neighs_df = pd.DataFrame(self.nearest_neighs[0]).drop(0, axis=1)\n\n        # creating df with nearest neighbor weights\n        self.nearest_neighs_w_df = pd.DataFrame(1 - self.nearest_neighs[1]).drop(0, axis=1)\n\n        # processing the neighbors df\n        self.nearest_neighs_df = (self.nearest_neighs_df\n                                  .reset_index()\n                                  .melt(id_vars='index')\n                                  .rename(columns={'index':'reference','value':'neighbor'})\n                                  .reset_index(drop=True))\n\n        # processing the neighbor weights df\n        self.nearest_neighs_w_df = (self.nearest_neighs_w_df\n                                    .reset_index()\n                                    .melt(id_vars='index')\n                                    .rename(columns={'index':'reference','value':'weight'})\n                                    .reset_index(drop=True))\n\n        # joining the datasets and adding weighted y variable\n        self.nearest_neighs_df = (self.nearest_neighs_df\n                                  .merge(self.nearest_neighs_w_df)\n                                  .drop('variable', axis=1)\n                                  .merge(self.y_df, on='neighbor', how='left')\n                                  .assign(y_weighted = lambda x: x.y*(x.weight))\n                                  .sort_values('reference'))\n        \n        # processing to get the effects\n        self.treat_effect_df = self.nearest_neighs_df.assign(count=1).groupby(['reference','W']).sum()\n        self.treat_effect_df['y_weighted'] = self.treat_effect_df['y_weighted']\/self.treat_effect_df['weight']\n        self.treat_effect_df['y'] = self.treat_effect_df['y']\/self.treat_effect_df['count']\n        self.treat_effect_df = self.treat_effect_df.pivot_table(values=['y', 'y_weighted','weight','count'], columns='W', index='reference')\n\n        # calculating treatment effects\n        self.treat_effect_df.loc[:,'effect'] = self.treat_effect_df['y'][1] - self.treat_effect_df['y'][0]\n        self.treat_effect_df.loc[:,'effect_weighted'] = self.treat_effect_df['y_weighted'][1] - self.treat_effect_df['y_weighted'][0]\n\n        # not computing effect for clusters with few examples\n        self.min_sample_effect = 10\n        self.treat_effect_df.loc[(self.treat_effect_df['count'][0] < self.min_sample_effect) | (self.treat_effect_df['count'][1] < self.min_sample_effect), 'effect_weighted'] = np.nan\n        self.treat_effect_df.loc[(self.treat_effect_df['count'][0] < self.min_sample_effect) | (self.treat_effect_df['count'][1] < self.min_sample_effect), 'effect'] = np.nan\n        \n        # let us fit a decision tree\n        dt = DecisionTreeRegressor(max_leaf_nodes=dt_max_leaves, min_samples_leaf=dt_min_samples_leaf, random_state=42)\n        dt.fit(self.X.iloc[self.treat_effect_df['effect_weighted'].dropna().index], self.treat_effect_df['effect_weighted'].dropna())\n        \n        # let us plot a decision tree\n        dot_data = tree.export_graphviz(dt, out_file=None, \n                                        feature_names=self.X.columns.str.replace('<','less than'),  \n                                        filled=True, rounded=True,  \n                                        special_characters=True)  \n        graph = graphviz.Source(dot_data)  \n        return graph ","f034fb48":"# instance of our causal inference model\ntee = TreatmentEffectEstimate(explanatory_vars_df, targets_df)","d82de461":"# treatment variable\ntreat_var = 'Q6-Software Engineer'\ntarget_var = 'quintile'\n\n# calculating\ntee.compute_effects(treat_var, target_var)","755a7019":"# treatment variable\ntreat_var = 'Q6-Data Scientist'\ntarget_var = 'quintile'\n\n# calculating\ntee.compute_effects(treat_var, target_var)","2a2646e2":"# treatment variable\ntreat_var = 'Q1-Female'\ntarget_var = 'quintile'\n\n# calculating\ntee.compute_effects(treat_var, target_var)","36a5a4af":"# treatment variable\ntreat_var = 'Q16-R'\ntarget_var = 'quintile'\n\n# calculating\ntee.compute_effects(treat_var, target_var)","b04a3b50":"# treatment variable\ntreat_var = 'Q16-Python'\ntarget_var = 'quintile'\n\n# calculating\ntee.compute_effects(treat_var, target_var)","f9fe8cf1":"# treatment variable\ntreat_var = 'Q31-Genetic Data'\ntarget_var = 'quintile'\n\n# calculating\ntee.compute_effects(treat_var, target_var)","d32ce256":"## Improving our clustering: Forests of Extremely Randomized Trees\n\nTo avoid known caveats of decision trees, let us use forests and explore how can we perform clustering with them. First, let us validate and fit an Extremely Randomized Trees model to our data (I'll soon get to why we use ERT instead of a regular RF). As before, we **do not include** our treatment variable (Software Engineer) as an explanatory variable in the model.","b0095984":"The number surprised me further. For people in the same context as myself, Software Engineers **are 2 times less likely to be on the Top 20%** than people with other titles! How could the model assign a positive weight for the Software Engineer title, then? It can be because Software Engineers earn more **in the US**, for instance:","e5efa8fb":"The result is quite interesting:\n\n1. The average effect of being female (root of the tree) is a drop of 2 percentiles in your salary rank, but\n2. If you\u00b4re in the age range of 22-24, the effect of being female (gender gap) is really small\n3. As you remove groups of younger pepole (22-24, 18-21 and 25-29), the gap increases, to a drop of almost 4 percentiles\n4. Having used cloud providers before also makes a difference, even showing that being a woman could make you earn a little bit more if you have used cloud providers. Could this be a proxy for other variable?\n\nThis suggests that the gap is larger for **older** and **experienced** women, which fall into the leftmost cluster in the tree. I found a [BBC article](https:\/\/www.bbc.com\/news\/business-42723960) that cites a paper with similar conclusions. The reason why there is a gender gap in the first place is a very hard question, and one I do not dare to answer here. There is [another video from MinutePhysics](https:\/\/www.youtube.com\/watch?v=E_ME4P9fQbo) that sheds a bit of light in this question and should help those who want to reflect on the issue.","4d599e71":"There's a lot going on here, so let us take our time to analyze the plot. The first thing to remember is that we used our leaf co-occurrence similarities to build it: `UMAP` tries to organize similar respondents close to each other and dissimilar ones far from each other. Using `hamming` distance is the same as our leaf co-occurrence similarity. Therefore, we're actually seeing the natural clusters in our data in a two dimensional approximation! As we did a supervised embedding, using similarities from a forest model trained to predict Top 20% earners, most of these guys are neatly grouped in tight regions.\n\nRemember that we chose ERT over RF in the beginning? It's because ERT, in my experience, builds \"better\" visualizations, because the similarities I've been getting from it are smoother. I wrote a [blog post](https:\/\/gdmarmerola.github.io\/forest-embeddings\/) about it, with some experiments! Feel free to take a look!\n\nIn this map, if we take any local neighboorhood, we will find what we've been looking for: ***comparable*** individuals! To check that this is true, let us start by coloring the map by US vs. non-US residents:","3153b183":"We'll use these variables further in the analysis. For now, let us talk about Simpson's Paradox, to illustrate why being aware of causality can help us a lot in making hypotheses of what impacts earnings.","5540f5a6":"## Effect of using Genetic Data\n\nThis one puzzled me as it was [the most valuable type of data you could use](https:\/\/www.kaggle.com\/andresionek\/what-makes-a-kaggler-valuable) in the original kernel. ","5391f59a":"And there we have it: we found comparable individuals using forests, calculated treatment effects using neighborhoods, and built a summary of the effects using a decision tree!\n\nThe tree tells us that:\n\n1. If you're not in the US, the effect of being a Software Engineer on your salary is close to zero\n2. If you're in the US, the effect is larger when you have 10-20 years of experience or are 25-29 years old, and build ML prototypes as part of your job description.\n\nAnd that's it! Does that make sense to you? If you want, you can fork the notebook and change the `max_leaf_nodes` parameter to build more clusters. \n\n**I hope you liked the tutorial!!!!** Any feedback is deeply appreciated!\n\nIf you want to continue exploring, please check the effect of other interesting variables below, or fork the notebook and do your own experiments! If you want to read more about causal inference, I recommend reading about [Generalzed Random Forests](https:\/\/github.com\/grf-labs\/grf) and the work of [Susan Athey](https:\/\/www.youtube.com\/watch?v=yKs6msnw9m8), as it was my inspiration for the simplified methodology I present here.","7c0e7015":"and then I create two new dataframes: `explanatory_vars_df` for my explanatory variables, and `targets_df` for my target variables.","6bdad823":"Then we do some `pandas` magic to prepare neighborhood data in a wat that is easy to compute treatment effects:","714a8b9b":"## Survey Data\n\nI'll use the excellent clean survey data used to fit the Top 20% earner model in the original Kernel. We have 185 dummy variables, representing answers to the survey questions. We also have the original target variable `top20`, a flag which is true if the respondent is among the Top 20% earners. To have more flexibility, I forked the Kernel and added `numerical_compensation` and `normalized_numerical_compensation`, the first just plain numerical compensation from the survey, and the second normalized by cost of living.","fea49478":"The most important variable is being or not in the US, as expected, but there's a lot of other variables that show up. Cool, but how do we find comparable individuals using this model? As with a single decision tree, we use its leaf assignments:","fcda338d":"As we can see, being a Software Engineer seems to **reduce** your chances of being a Top 20% earner outside the US, while it **increases** your chances in the US. Also, just being in the US seems to **greatly improve** your chances of being a top 20% earner and reduces your chances of being a Software Engineer (16% to 12%, respectively). In the optimization process, the model may have tried to weight all the features and got confused by the underlying **confounder** (being or not in the US) and thus assigned a positive weight for the Software Engineer title as a general trend, even when this is not the case for many people. \n\nTo get a better result, we can improve this model in two ways: (1) make it more expressive, such that being a Software Engineer can have different impacts for different people (as opposed to being a single weight for everyone) and (2) make it causal, such that we accurately control for **confounders**, variables that affect both the outcome and the variable we want to measure. In this Notebook, I'll use the excellent clean data brought by the first analysis and try to implement both improvements to build an expressive causal inference model just by combining off-the-shelf ML models and algorithms. \n\nSo... Let us begin!","7facf397":"# A Causal Look at What Makes a Kaggler Valuable\n\nEach year, the Kaggle community answers the \"State of Data Science\" survey, which asks data scientists worldwide which tools they use most, which industry they're in, what education they've been through, among other things. Fueled by the rich data, many people posted their analyses on the web, from looking at [where people learn data science](https:\/\/www.kaggle.com\/sudalairajkumar\/where-do-people-learn-ml-ds) and [which MOOCs they do](https:\/\/www.kaggle.com\/ogakulov\/the-mooc-wars-kaggle-s-perspective) to [comparing the Kaggle and StackOverflow communities](https:\/\/www.kaggle.com\/carlossouza\/comparing-kaggle-and-stackoverflow-communities). \n\nOne of the analyses, in particular, caught my eye: [\"What Makes a Kaggler Valuable?\"](https:\/\/www.kaggle.com\/andresionek\/what-makes-a-kaggler-valuable\/notebook). By using survey data, the author builds a model to predict whether a survey respondent's salary will be at the Top 20% of Kagglers and proceeds to make hypotheses about what you should do if you want to improve your salary, based on model coefficients. One of these hypotheses, for instance, is that if you want to earn more you should strive to be a Software Engineer, as its associated coefficient is larger than the coefficients of any other job description:\n\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/1*axtNbF4Y96vgymVXSvs-mg.png\" alt=\"drawing\" style=\"width:700px;\"\/>\n\nI was suprised by this, as it did not match with my personal experience (I'm a brazilian data scientist working in finance). I was curious to see if this was true for people in the same context as myself. So, I loaded the same data used to train the model and checked how much Software Engineers were earning in comparison to people with other titles, for my particular subgroup (non-US, Male and working in finance): ","b05e6d40":"In this dataframe, we can explicitly compare outcomes for treated and untreated samples. We do not calculate effects for neighborhoods where there's not a relevant number of treated or untreated samples (at least 10 samples, for instance), hence the NaN's in the effect columns.\n\nNow, we can return to our embedding and check where being a Software Engineer will give you a lift in compensation:","71101a10":"In general, we should do a hyperparameter search and choose the best model, but I find that ERT is hard to overfit, and this set of hyperparameters works well in most cases (in my personal experience). Nevertheless, an AUC close to 90% indicates that this model was successful in finding which variables predict the outcome, which in turn should lead to good clusters. Let us continue by fitting it to the full dataset, and checking the most important variables:","2df8c989":"## Finding comparable individuals: Decision Trees\n\nWe want to measure the effect of being a Software Engineer on compensation. So the first thing we should do is actually find individuals who are comparable on every other feature that also impacts compensation. One of the first models that comes to mind is a decision tree. We fit the tree on all the explanatory variables, but leaving our treatment variable (being or not a Software Engineer) **out**. We use the letter `W` to represent our treatment variable, `X` for our explanatory variables and `y` for the outcome.","09edc297":"**Again, I'd really like to thank you for reading. If you have suggestions and want to discuss the method, feel free to contact me. **\n\n**Hope you liked the reading! See you soon!**","322fed9d":"They concentrated in the two islands in the upper left corner of the map. If you backtrack to the last map, you'll notice that one of these islands contains US Residents and the other contains non-US Residents. Finally, what about people that build ML prototypes? ","44cef89e":"US residents are completely separated from non-US residents. No local (and thus comparable) neighborhood has any mix with respect to this variable. Let us check how people with 10-20 years of experience are distributed, the second most important variable:","39f25494":"In this local neighborhood (where samples are comparable), the rate of top earner Software Engineers is 92% compared to 74% for non-Software Engineers, a lift of 18 percentage points. This is strong evidence that if someone in this neighborhood changed jobs to Software Engineer, we would see an improvement on his\/her salary.","837e105a":"Cool. Our similarities seem to make sense. Now, to get a global view, we can do the coolest thing: reducing the dimensionlity of the embedding to 2 dimensions, putting all the similar individuals close together and dissimilar far apart on a plane. We do this with the help of a dimensionality reduction algorithm, such as `UMAP`:","6b9b298d":"Main observations:\n\n1. The same working student lift that appeared previously appears here\n2. If you have less than a year of experience, the lift of being a Data Scientist is not relevant\n3. Being a Data Scientist helps you the most if you have not used cloud providers (+3 percentiles).\n\nI found the cloud provider conclusion counterintuitive, but maybe companies are now leaving the cloud for Data Engineers and Data Scientists are only expected to perform analysis. All other effects are not very large to begin with.","48a89c34":"Let us check one local neighborhood now:","621b91d9":"## Effect of being proficient in R\n\nLet us now move to programming skills, starting with R.","279d88bd":"To make things more interesting, I use the raw compensation variables to create even more targets:\n\n1. `top50`: whether or not the salary is above the median\n2. `percentile`, `decile` and `quintile`: in which percentile, decile and quintile the salary is in (all values are in percentiles)","8abe9a6a":"For cluster 1 (non-US), being a software engineer decreases your chance of being in the top 20% by 2 percentage points. On the other hand, we have a lift of 16 p.p for cluster 3 (US, not build prototypes) and 23 p.p. for cluster 4, (US, builds prototypes). So, if you're in the US and your job description includes building ML prototypes, you should make even more money by being a Software Engineer!  \n\nThat conclusion makes a lot of sense, which gives us confidence that we are on the right path. However, decision trees have a lot of weaknesses, such as being greedy (and thus sub-optimal) and overfit when they grow too much. Again, to our luck, there's a simple way to solve this: forests!","465577a1":"The tree split the data among three terminal nodes (leaves): (1) non-US residents, with the worst salaries, (3) US residents that build prototypes with ML, with best salaries and (4) US residents that do not build prototypes with ML, with average salaries. These are our best bet at comparable groups with respect to all variables (apart from our treatment variable) that predict compensation. \n\nLet us now add our treatment variable and check how being a Software Engineer impacts compensation in each of these groups:","d29ff504":"Given this intuition, we can devise an algorithm to systematically calculate treatment effects:\n\n1. Using our new similarity metric, we search, for each sample, its 200 nearest neighbors\n2. For each of these clusters, compute the average outcome for treated and not treated individuals\n3. Calculate lift as the difference between the average outcome for treated individuals and the average outcome for not treated individuals\n\n\nWe use `NNDescent` to get nearest neighbors:","b9e9fca6":"In the plot, blue points are Software Engineers while red points are not. Circles represent Bottom 80% earners and crosses represent Top 20% earners. There are regions where we get a good mix of blue and red points. As we **did not include** Software Engineer as an explanatory variable in the model, it could not cluster Software Engineers together to make predictions. Thus,  we traded predictive power for dispersion, such that we can  maximize the probability that for every untreated point (NOT Software Engineer) there is a handful of treated counterparts (Software Engineer), making it easy to compute treatment effects (lift given by changing Software Engineer variable while holding all else constant). \n\nNote that leaving the treatment variable out does not guarantee it is evenly distributed on the map: it is not uncommon for treated samples to concentrate somewhere due to correlations with other variables, which makes impossible to calculate treatment effects. That's why drawing conclusions from observational studies is hard: treatment assignments may be highly correlated with the explanatory variables and outcome.","6145b72c":"## Effect of being a Software Engineer\n\nLet us calculate this one again, using the `quintile` target.","8f1801ad":"Here, we have in the `reference` variable all the samples in our dataset, and in the `neighbor` variable the 200 nearest neighbors for the corresponding `reference`. `weight` is the similarity from `neighbor` to `reference`. `W` is our treatment variable (if the `neighbor` is a Software Engineer or not) and `y` and `y_weighted` our outcomes, the first just pure outcomes and the second weighted by similarity.\n\nMore pandas magic leads us to the explicit treatment effect dataframe:","56681c50":"## Neighborhoods and Treatment Effects\n\nLet us try to compute treatment effects now. To get a feel for how we can do it, let us take a look at one of our clusters, and how software engineers are distributed in it:","463c86f9":"The data in the `leaves` variable may seem confusing at first, but it really encodes all the relevant structure in the data. In this case, each row is a survey respondent and each column is a tree in the forest. The values are the leaf assignments: in which leaf of each tree each individual ended up into. The same logic applies here: when two individuals end up in the same leaf in a tree, they are similar. But how can we aggregate all leaf nodes and build a single similarity measure for the whole forest?","253654a8":"## Effect of being a Data Scientist\n\nLet us now check the effect of being a Data Scientist!","66ae1482":"## Effect of being proficient in Python\n\nLet us take a look at Python now.","68c4e2bf":"Some new conclusions appear:\n\n1. In average, using Genetic data will set you back 1 percentile in salary\n2. If you use AWS, using Genetic data should decrease your salary even more, setting you back 2 percentiles\n3. If you don't use AWS, but are based on the US the lift is also bad, at -3 percentiles\n4. However, if you're not in the US things get better, specially if you're in Academics\/Education (+3 percentiles)\n\nIt does make sense to me. Genetic Data is very specific, and it may be more valued in academia or very specific industries. I can't comment on the US vs. non-US trend, as I really don't know what could explain the difference in lift.   ","819b7bc5":"Some cool observations:\n\n1. Proficiency in R has an opposite effect compared to being a Software Engineer: it does not give you a lift if you're in the US\n2. I found very interesting that people in Academics\/Education would enjoy a bigger lift by knowing R (2 percentiles). It coincides a lot with my (potentially biased) personal experience in academia, where R's bleeding edge packages tackling very recent academic problems were more valued than neat, but more conservative Python modules.","0585b78d":"Very interesting observations as well:\n\n1. If you don't have much experience (less than 1 year), being proficient in Python should decrease your earnings. Does that mean you shouldn't learn it? Not at all! We're looking at the short term, immediate earnings. So, actually, be cool: if the rewards do not come right away, do not panic and keep learning and getting more experience!\n2. As you get experience, knowing Python will improve your salary the most if you use AWS (or other cloud providers, if AWS is a proxy for that). Makes sense, as the language is production-friendly and has lots of support in the cloud.\n3. Finally, seems like if you're not in the US you'll also make the most of using Python. This is rather unexpected... What do you think?","04bc9a6a":"## Effect of gender\n\nLet us check the effect of gender.","3f5becb0":"Cool! So let's analyze this result: in the clusters on the left-hand side (mostly US-based or experienced professionals), we have a lift of a least 8 p.p for most of the respondents, while in the clusters on the right-hand side we have a lot of places that have zero effect and a few places where the effect is negative. Some points do not appear as there was not enough treated and untreated samples to calculate effects. There is also some noise due to the fact that for some clusters there is not much treated individuals (you can change the minimum number of treated or not treated individuals to consider an effect valid using the `min_sample_effect` parameter in the previous cells). As cool as this final map is, it would be nice to have a interpretable summary of what are the attributes that discriminate the effect of being a Software Engineer.\n\nAgain, Decision Trees come to the rescue!","1ebb601f":"Known and new patterns appear here:\n\n1. If you're in the US, you can expect to go up 5 percentiles in salary by being a Software Engineer, but\n2. If you analyze and understand data to influence product or business decisions, the lift is smaller (+3 percentiles)\n3. If you don't, the lift is larger (+7 percentiles)\n4. If you're not in the US, you should expect to go down 0.5 percentile by being a Software Engineer, and even less (-3 percentiles) if you analyze and understand data to influence product or business decisions\n5. If you're a Student and a Software Engineer, you should expect to be 10 percentiles above your counterparts (obvious, as a working student really should earn a lot more)\n\nGiven my limited experience, I think that this makes sense: the US is home to many companies that value and hire a lot of Software Engineers, and to analyze and understand data to influence product or business decisions seems like a job that a data analyst or scientist could help more. What do you think?","788dbc70":"## Looking at other treatment variables\n\nFor convenience, I built a class to easily compute all the things that we saw in the tutorial. I apply it to some interesting variables below, using the `quintile` target, which reveals more nuance about changes in salary.","938c0d35":"How do they compare on the most relevant variables?","4e81e899":"## Simpson's Paradox\n\nSimpson's Paradox is a statistical paradox where it's possible to draw two opposite conclusions, depending on how we divide or aggregate our data. I really recommend [this video from MinutePhysics](https:\/\/www.youtube.com\/watch?v=ebEkn-BiW5k) which does a great job of explaining it. I'll use the same example they did. \n\nSuppose you want to measure the effect of money on happiness for people and cats, and you draw the following plots to analyze it: \n\n<img src=\"https:\/\/github.com\/gdmarmerola\/random-stuff\/blob\/master\/cats-people-separate.jpg?raw=true\" alt=\"drawing\" style=\"width:700px;\"\/>\n\nBy the two plots, it is clear that more money makes both people and cats sadder. But suppose that cats are richer and happier than people in the first place, and you had aggregated the two populations for the analysis:\n\n<img src=\"https:\/\/github.com\/gdmarmerola\/random-stuff\/blob\/master\/cats-people-together.jpg?raw=true\" alt=\"drawing\" style=\"width:700px;\"\/>\n\nHere, it would **incorrectly appear that money makes you happier** as the **general trend** between the populations points this way. Most models are not guaranteed to perform these controls and could show the general trend when we look at the effect of an individual variable. We could try to regularize the model to mitigate the effect of the internal correlation between the variables, but this could lead overestimating the effect of money (the model could drop the \"cats vs. people\" attribute) or underestimating it (drop the \"money\" attribute). Which variable would be dropped could depend on an arbitrary choice, such as using L2 regularization instead of L1. \n\nThe catch here is obvious but actually quite hard to control in practice: we should only draw conclusions from ***comparable*** individuals on **all other attributes apart from the variable we want to analyze**. The cat vs. human comparison in the example makes it easy for us to understand this, but when we look at real, messy, and high-dimensional data (just like the Kaggle survey) it is not clear for us, humans, which individuals are comparable. But wait: we actually have a good ***model*** to determine which individuals are comparable! So that's how our causal inference journey from ML begins!   ","9508a598":"## Clustering based on effects: Decision Trees\n\nThe last step in our method helps us interpret what attributes discriminate treatment effects. To do that, we use a Decision Tree Regressor, fitting our explanatory variables to **treatment effects**. This way, it'll build interpretable clusters for us.","104d90dc":"## Supervised Embeddings: Similarity by leaf co-occurrence \n\nThe data stored in the `leaves` object actually describes a supervised [high-dimensional sparse embedding of our data](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomTreesEmbedding.html). One way to compute similarities in this embedding that I find very cool is to count at how many trees individuals were assigned to the same leaf. To illustrate this, let us count how many times the leaf indices from first row from `leaves` are equal to all the others:","0e54492d":"These guys are actually scattered across the map and clusters, but are tightly packed in local neighborhoods, maintaining our argument that we can find comparable individuals.\n\nCool, huh? We're now actually very close to our ultimate goal of building an actual causal inference model. Keep going, we're getting there!"}}