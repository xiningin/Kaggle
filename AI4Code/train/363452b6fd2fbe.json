{"cell_type":{"93eb4116":"code","a56056db":"code","7e5d04f8":"code","a7c7a065":"code","9283abc3":"code","55b4f807":"code","b748432e":"code","4f8ea461":"code","2d5f1bdf":"code","f6f5d7f6":"code","cf9e2a4a":"code","21e50caf":"code","17b10969":"code","666fc0c9":"code","98bcf8f6":"code","887b3e05":"code","e9cb123e":"code","1a72c738":"code","5844d2eb":"code","73718bc6":"code","54eabb81":"code","2786c090":"code","0594b8b2":"code","ea6fc611":"code","c595103a":"code","f01c9d5f":"code","7a24e5a1":"code","f3de952e":"code","9763da8e":"code","46573b03":"code","84cb2b8f":"code","343451a8":"code","032dd38d":"code","68d435d5":"markdown","15923dfc":"markdown","d4503efd":"markdown","be37e0d9":"markdown","fe66f2a0":"markdown","8cdedb57":"markdown","37e2f756":"markdown","dc7fd516":"markdown","685f9ff7":"markdown","eca50b63":"markdown","8e430fc3":"markdown","5e2e9762":"markdown","2f1228cf":"markdown","b34686d7":"markdown","436ab32d":"markdown","c6ba1d9f":"markdown","91d4d487":"markdown","0b69dd2f":"markdown","543ad2f7":"markdown","290dbc5f":"markdown","090058be":"markdown","36f86e69":"markdown","241fcb54":"markdown","b7731c03":"markdown","cf67ec0c":"markdown","70346feb":"markdown","dcac9076":"markdown","cf27c35c":"markdown","943c0d23":"markdown","218fce96":"markdown","8c729d70":"markdown","8b5db1ce":"markdown","f5c56db3":"markdown","2819f2eb":"markdown","752eef15":"markdown","024a30e9":"markdown","3a2182da":"markdown","a03624fb":"markdown"},"source":{"93eb4116":"# Download YOLOv5\n!git clone https:\/\/github.com\/ultralytics\/yolov5  # clone repo\n%cd yolov5\n\n# Install dependencies\n%pip install -qr requirements.txt  \n\n# change directory\n%cd ..\/\nimport torch\nprint(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")","a56056db":"# Install W&B \n!pip install -q --upgrade wandb\n\n# Login \nimport wandb\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient() \n\npersonal_key_for_api = user_secrets.get_secret(\"kk\")\n\n! wandb login $personal_key_for_api","7e5d04f8":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport gc\nimport cv2\nfrom IPython.core.display import Video, display\nfrom tqdm import tqdm\nimport shutil\nfrom shutil import copyfile\nimport matplotlib.pyplot as plt\nimport subprocess\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n\nfrom IPython.core.magic import register_line_cell_magic\n\nfrom os import listdir\nfrom os.path import isfile, join\nfrom glob import glob\nimport yaml\n\n@register_line_cell_magic\ndef writetemplate(line, cell):\n    with open(line, 'w') as f:\n        f.write(cell.format(**globals()))","a7c7a065":"Selected_fold=1\nBATCH_SIZE = 16\nEPOCHS = 35\nIMG_SIZE=736","9283abc3":"# --- Read data ---\nTRAIN_PATH = '\/kaggle\/input\/nfl-health-and-safety-helmet-assignment\/images\/'\n# Read in the data CSV files\ntrain_df = pd.read_csv(\"\/kaggle\/input\/nfl-health-and-safety-helmet-assignment\/image_labels.csv\")\n#train_df=train_df.head(n=5000)\nprint('Number of ground truth bounding boxes: ', len(train_df))\n# Number of unique labels\n\ncategory_name_to_id  = {label: i for i, label in enumerate(train_df.label.unique())}\nprint('Classes_id: ', category_name_to_id )\n\nClasses=['Helmet','Helmet-Blurred','Helmet-Difficult','Helmet-Sideline','Helmet-Partial']\nprint('Classes: ', Classes )","55b4f807":"train_meta=train_df.drop_duplicates(subset=['image'])\ntrain_meta.reset_index(inplace = True)\ntrain_meta[\"split\"]=\"train\"\ntrain_meta.head()","b748432e":"from sklearn.model_selection import KFold,StratifiedKFold\nsfolder = StratifiedKFold(n_splits=5,random_state=42,shuffle=True)\nX = train_meta[['image']]\ny = train_meta[['label']]\n\nfold_no = 1\nfor train, valid in sfolder.split(X,y):\n    if fold_no==Selected_fold:\n        train_meta.loc[valid, \"split\"] = \"valid\"\n    fold_no += 1\n     \ntrain_meta.head()","4f8ea461":"print(\"Dataset Size:\",len(train_meta),\n      \"Training Images:\", len(train_meta[train_meta.split=='train']),\n      \"Validation Images:\",len(train_meta[train_meta.split=='valid']))","2d5f1bdf":"os.makedirs('NFL\/images\/train', exist_ok=True)\nos.makedirs('NFL\/images\/valid', exist_ok=True)\nos.makedirs('NFL\/labels\/train', exist_ok=True)\nos.makedirs('NFL\/labels\/valid', exist_ok=True)","f6f5d7f6":"for i in tqdm(range(len(train_meta))):\n    row = train_meta.loc[i]\n    if row.split == 'train':\n        copyfile(f'{TRAIN_PATH}{row.image}', f'NFL\/images\/train\/{row.image}')\n    else:\n        copyfile(f'{TRAIN_PATH}{row.image}', f'NFL\/images\/valid\/{row.image}') ","cf9e2a4a":"with open('\/kaggle\/working\/NFL\/train.txt', 'w') as f:\n    for path in glob('\/kaggle\/working\/NFL\/images\/train\/*'):\n        f.write(path+'\\n')\n            \nwith open('\/kaggle\/working\/NFL\/val.txt', 'w') as f:\n    for path in glob('\/kaggle\/working\/NFL\/images\/val\/*'):\n        f.write(path+'\\n')\n\ndata = dict(\n    train = '\/kaggle\/working\/NFL\/images\/train',\n    val = '\/kaggle\/working\/NFL\/images\/valid',\n    \n    nc    = 5, # number of classes\n    names = Classes # classes\n    )\n\nwith open('\/kaggle\/working\/yolov5\/data\/data.yaml', 'w') as outfile:\n    yaml.dump(data, outfile, default_flow_style=False)\n\n%cat \/kaggle\/working\/yolov5\/data\/data.yaml","21e50caf":"!ls '\/kaggle\/working\/yolov5\/data'","17b10969":"# Get the raw bounding box by parsing the row value of the label column.\ndef get_bbox(row):\n    bboxes = []\n    bbox = []\n    \n    b1=float(row.left) \n    b2=float(row.width)  \n    b3=float(row.top)\n    b4=float(row.height) \n    \n    x_min=b1  \n    y_min=b3   \n    x_max=b1+b2    \n    y_max=b3+b4 \n    \n    bbox.append(float(x_min))\n    bbox.append(float(y_min))\n    bbox.append(float(x_max))\n    bbox.append(float(y_max))\n    bboxes.append(bbox)\n    \n    \n    return bboxes\n\n\n# Scale the bounding boxes according to the size of the resized image. \ndef scale_bbox(row, bboxes,w,h):\n    # Get scaling factor\n    scale_x = IMG_SIZE\/w\n    scale_y = IMG_SIZE\/h\n    \n    scaled_bboxes = []\n    for bbox in bboxes:\n        x = float(bbox[0]*scale_x)\n        y = float(bbox[1]*scale_y)\n        x1 = float(bbox[2]*(scale_x))\n        y1= float(bbox[3]*scale_y)\n\n        scaled_bboxes.append([x, y, x1, y1]) # xmin, ymin, xmax, ymax\n        \n    return scaled_bboxes\n\n# Convert the bounding boxes in YOLO format.\ndef get_yolo_format_bbox(img_w, img_h, bboxes):\n    yolo_boxes = []\n    for bbox in bboxes:\n        w = bbox[2] - bbox[0] # xmax - xmin\n        h = bbox[3] - bbox[1] # ymax - ymin\n        xc = bbox[0] + float(w\/2) # xmin + width\/2\n        yc = bbox[1] + float(h\/2) # ymin + height\/2\n        \n        yolo_boxes.append([xc\/img_w, yc\/img_h, w\/img_w, h\/img_h]) # x_center y_center width height\n    \n    return yolo_boxes","666fc0c9":"for i in tqdm(range(len(train_meta))):\n    row = train_meta.loc[i]\n    # Get image\n    image_name = row.image\n    \n    img = cv2.imread(TRAIN_PATH+image_name)\n    \n    height, width, _ = img.shape \n\n    image_n = image_name[:-4]\n    # Get split\n    split = row.split\n    # Get  label id\n    #label = category_name_to_id [row.label]\n    if row.split=='train':\n        file_name = f'\/kaggle\/working\/NFL\/labels\/train\/{image_n}.txt'\n    else:\n        file_name = f'\/kaggle\/working\/NFL\/labels\/valid\/{image_n}.txt'\n\n    with open(file_name, 'w') as f:\n        for index2, row1 in train_df.query(\"image == @image_name\").iterrows():\n\n            label = category_name_to_id[row1.label]\n            bboxes = get_bbox(row1)\n            # Scale bounding boxes\n            scale_bboxes = scale_bbox(row1, bboxes,width, height)\n            # Format for YOLOv5\n            yolo_bboxes = get_yolo_format_bbox(IMG_SIZE, IMG_SIZE, scale_bboxes)\n            for bbox in yolo_bboxes:\n                bbox = [label]+bbox\n                bbox = [str(i) for i in bbox]\n                bbox = ' '.join(bbox)\n                f.write(bbox)\n                f.write('\\n')","98bcf8f6":"import os\nlist = os.listdir(\"\/kaggle\/working\/NFL\/labels\/train\/\") # dir is your directory path\nnumber_files = len(list)\nprint(number_files)\n\n#%cat \/kaggle\/working\/NFL\/labels\/train\/57503_003731_Sideline_frame428.txt","887b3e05":"%cd yolov5\/","e9cb123e":"#best_weights = '\/kaggle\/input\/nfl-weights\/yolov5\/kaggle-NFL\/exp\/weights\/best.pt' --weights {best_weights} \\\n!python train.py --img {IMG_SIZE} \\\n                 --batch {BATCH_SIZE} \\\n                 --epochs {EPOCHS} \\\n                 --data data.yaml \\\n                 --weights yolov5s.pt \\\n                 --project kaggle-NFL \\\n                 --cache","1a72c738":"%cd \"..\/\"\npath = \"NFL\"\nshutil.rmtree(path)","5844d2eb":"plt.figure(figsize = (20,10))\nplt.axis('off')\nplt.imshow(plt.imread('\/kaggle\/working\/yolov5\/kaggle-NFL\/exp\/labels.jpg'));\n","73718bc6":"import matplotlib.pyplot as plt\nplt.figure(figsize = (15, 15))\nplt.axis('off')\nplt.imshow(plt.imread('\/kaggle\/working\/yolov5\/kaggle-NFL\/exp\/train_batch0.jpg'))\n","54eabb81":"#!ls \/kaggle\/working\/yolov5\/kaggle-NFL\/exp\nplt.figure(figsize=(10,10))\nplt.axis('off')\nplt.imshow(plt.imread('\/kaggle\/working\/yolov5\/kaggle-NFL\/exp\/P_curve.png'));","2786c090":"plt.figure(figsize=(10,10))\nplt.axis('off')\nplt.imshow(plt.imread('\/kaggle\/working\/yolov5\/kaggle-NFL\/exp\/PR_curve.png'));","0594b8b2":"plt.figure(figsize=(10,10))\nplt.axis('off')\nplt.imshow(plt.imread('\/kaggle\/working\/yolov5\/kaggle-NFL\/exp\/F1_curve.png'));","ea6fc611":"plt.figure(figsize=(10,10))\nplt.axis('off')\nplt.imshow(plt.imread('\/kaggle\/working\/yolov5\/kaggle-NFL\/exp\/R_curve.png'));","c595103a":"ig, ax = plt.subplots(3, 2, figsize = (2*5,3*5), constrained_layout = True)\nfor row in range(3):\n    ax[row][0].imshow(plt.imread(f'\/kaggle\/working\/yolov5\/kaggle-NFL\/exp\/val_batch{row}_labels.jpg'))\n    ax[row][0].set_xticks([])\n    ax[row][0].set_yticks([])\n    ax[row][0].set_title(f'\/kaggle\/working\/yolov5\/kaggle-NFL\/exp\/val_batch{row}_labels.jpg', fontsize = 12)\n    \n    ax[row][1].imshow(plt.imread(f'\/kaggle\/working\/yolov5\/kaggle-NFL\/exp\/val_batch{row}_pred.jpg'))\n    ax[row][1].set_xticks([])\n    ax[row][1].set_yticks([])\n    ax[row][1].set_title(f'\/kaggle\/working\/yolov5\/runs\/kaggle-NFL\/val_batch{row}_pred.jpg', fontsize = 12)","f01c9d5f":"plt.figure(figsize=(10,10))\nplt.axis('off')\nplt.imshow(plt.imread('\/kaggle\/working\/yolov5\/kaggle-NFL\/exp\/results.png'));","7a24e5a1":"plt.figure(figsize=(10,10))\nplt.axis('off')\nplt.imshow(plt.imread('\/kaggle\/working\/yolov5\/kaggle-NFL\/exp\/confusion_matrix.png'));","f3de952e":"ex_video = '\/kaggle\/input\/nfl-health-and-safety-helmet-assignment\/test\/58102_002798_Endzone.mp4'\n\nfrac = 0.6\ndisplay(Video(ex_video, embed=True, height=int(720*frac), width=int(1280*frac)))","9763da8e":"img_ext = 'png'\nimage_name = '58102_002798_Endzone'\nframe_dir = '\/kaggle\/worling\/NFL\/mp4_img\/'\n\nos.makedirs(frame_dir, exist_ok=True)\ncmd = 'ffmpeg -i \\\"{}\\\" -qscale:v 2 \\\"{}\/{}_%d.{}\\\"'.format(ex_video, frame_dir, image_name, img_ext)\nprint(cmd)\nsubprocess.call(cmd, shell=True)","46573b03":"%cd yolov5\/","84cb2b8f":"best_w = '\/kaggle\/working\/yolov5\/kaggle-NFL\/exp\/weights\/best.pt' #New\n#best_w='\/kaggle\/input\/nfl-weights\/yolov5\/kaggle-NFL\/exp\/weights\/best.pt' # last\nIMG_S=720\n\nproject_name = '58102_002798_Endzone'\n\n!python detect.py --weights {best_w} \\\n                  --source {frame_dir} \\\n                  --img {IMG_S} \\\n                  --save-txt \\\n                  --save-conf \\\n                  --project {project_name}","343451a8":"video_name = '58102_002798_Endzone_fps50.mp4'\n\ntmp_video_path = os.path.join('\/kaggle\/working\/', f'tmp_{video_name}')\nvideo_path = os.path.join('\/kaggle\/working\/', video_name)\n\nframe_rate = 60\n\nimages = [img for img in os.listdir(f'{project_name}\/exp')]\nimages.remove('labels')\nimages.sort(key = lambda x: int(x.split('_')[-1][:-4]))\n\nframe = cv2.imread(os.path.join('58102_002798_Endzone\/exp', images[0]))\nheight, width, layers = frame.shape\n\nvideo = cv2.VideoWriter(tmp_video_path, cv2.VideoWriter_fourcc(*'MP4V'),\n                        frame_rate, (width,height))\n\nfor f in images:\n    img = cv2.imread(os.path.join('58102_002798_Endzone\/exp', f))\n    video.write(img)\n\nvideo.release()\n\n# Not all browsers support the codec, we will re-load the file at tmp_video_path\n# and convert to a codec that is more broadly readable using ffmpeg\n\nif os.path.exists(video_path):\n    os.remove(video_path)\n    \nsubprocess.run([\"ffmpeg\", \"-i\", tmp_video_path, \"-crf\", \"18\", \"-preset\", \"veryfast\",\n                \"-vcodec\",\"libx264\", video_path,])\n\nos.remove(tmp_video_path)","032dd38d":"frac = 0.60\ndisplay(Video(video_path, embed=True, height=int(720*frac), width=int(1280*frac)))","68d435d5":"# inference","15923dfc":"### Display video","d4503efd":"### GT Vs Pred","be37e0d9":"![download.jpg](attachment:07de9c65-7c16-40e7-a821-d5354296394c.jpg)","fe66f2a0":"### Batch Image","8cdedb57":"### (Loss, Map) Vs Epoch\n","37e2f756":"## PR Curve","dc7fd516":"# \ud83c\udf58 Hyperparameters","685f9ff7":"# Select a Model\nSelect a pretrained model to start training from. \n* Here we select YOLOv5s, the smallest and fastest model available.\n* I will try YOLO5m","eca50b63":"# Detecting","8e430fc3":"### Class Distribution","5e2e9762":"## F1 Curve","2f1228cf":"### Hi kagglers, This is `Training` notebook using `YOLOv5`.\n\n\n### Other notebooks in the competition\n- [NFL Extra Images Detectron2 [Training]](https:\/\/www.kaggle.com\/ammarnassanalhajali\/training-nfl-extra-images-detectron2)\n- [NFL Big Data Bowl 2021:Animating Players](https:\/\/www.kaggle.com\/ammarnassanalhajali\/nfl-big-data-bowl-2021-animating-players)\n\n\n### Please if this kernel is useful, <font color='red'>please upvote !!<\/font>","b34686d7":"## \ud83c\udf6e Create Labels for YOLOv5\n\nTo label your images,a `.txt` file with the same name of the image,will be created (if no objects in image, no *.txt file is required)\nThe *.txt file specifications are:\n\n* One row per object\n* Each row is class x_center y_center width height format.\n* Box coordinates must be in normalized xywh format (from 0 - 1). If your boxes are in pixels, divide x_center and width by image width, and y_center and height by image height.\n* Class numbers are zero-indexed (start from 0).\n\n> \ud83d\udccd Note: We don't have to remove the images without bounding boxes from the training or validation sets. ","436ab32d":"# \ud83c\udf6e Loading Data","c6ba1d9f":"All training results are saved to runs\/train\/ with incrementing run directories, i.e. runs\/train\/exp2, runs\/train\/exp3 etc. ","91d4d487":"# \ud83c\udf5c Create `Data.YAML` file\n\nThe `data.yaml`, is the dataset configuration file that defines:\n\n1. the dataset root directory and relative paths to train\/val\/test image directories (or paths to *.txt files with image paths).\n1. the number of classes.\n1. a list of class names.\n\n> \ud83d\udccd Note: The `data.yaml` is created in the `yolov5\/data` directory as required. ","0b69dd2f":"# \ud83d\udcda YOLOv5\nYOLO, \"You Only Look Once\", has a long and succesful history with real time object detection.","543ad2f7":"### Please if this kernel is useful, <font color='red'>please upvote !!<\/font>","290dbc5f":"# \ud83d\ude85 Train with W&B","090058be":"# \ud83d\udd28 Weights & Biases\n* Weights & Biases is a set of tools that tracks machine learning experiments, visualizes metrics, and shares results.\n* Weights & Biases is directly integrated into YOLOv5, providing experiment metric tracking, model and dataset versioning, rich model prediction visualization, and more.","36f86e69":"## R Curve","241fcb54":"The label file corresponding to the above image contains 2 persons (class 0) and a tie (class 27):","b7731c03":"## P Curve","cf67ec0c":"### Confusion Matrix","70346feb":"# \u2600\ufe0f Importing Libraries","dcac9076":"![model_comparison.png](attachment:6f64ed0a-fd0e-43de-9d26-77412d6e87cc.png)","cf27c35c":"# \ud83d\ude80 NFL training and inference YOLOv5","943c0d23":"# References","218fce96":"![91506361-c7965000-e886-11ea-8291-c72b98c25eec.jpg](attachment:812ff98c-03ef-48f5-b171-0c8b3b0fab54.jpg)","8c729d70":"# \ud83c\udf5a Splitting Dataset","8b5db1ce":"![10.png](attachment:caf5c201-af01-4c90-b306-3e6e43787992.png)","f5c56db3":"1. https:\/\/github.com\/ultralytics\/yolov5\/wiki\/Train-Custom-Data\n1. https:\/\/www.kaggle.com\/eneszvo\/yolov5-helmet-detection-train-and-inference\n1. https:\/\/ultralytics.com\/yolov5\n1. https:\/\/docs.wandb.ai\/\n1. https:\/\/www.kaggle.com\/ayuraj\/train-covid-19-detection-using-yolov5\n1. https:\/\/www.kaggle.com\/awsaf49\/vinbigdata-cxr-ad-yolov5-14-class-train","2819f2eb":"# Removing Files","752eef15":"### Create frames from video","024a30e9":"## \ud83c\udf5a Organize Directories\n\nI organized train and val images and labels according to the example below.\n\n```\n\/Kaggle\/working\n    \/NFL\n         \/images\n             \/train\/img0.jpg\n             \/val\n         \/labels\n             \/train\/img0.txt\n             \/val\n    \/yolov5\n```","3a2182da":"#  \u2b07\ufe0f Download YOLOv5\nClone this repo and install requirements.txt dependencies, including Python>=3.8 and PyTorch>=1.7.","a03624fb":"### make video from frames"}}