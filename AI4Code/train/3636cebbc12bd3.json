{"cell_type":{"01270840":"code","cae136c8":"code","3b430b5e":"code","34b7b914":"code","a64077d2":"code","dec9abb7":"code","58e4ae6b":"code","d7be8706":"code","62e90cee":"code","0b89231f":"code","3508a824":"code","6213431a":"code","63425657":"code","db8d8761":"code","74ae3453":"code","2c7b6599":"code","f02adf8d":"code","9814f05d":"markdown","358c0b92":"markdown","3e9d45db":"markdown","61aff9ae":"markdown"},"source":{"01270840":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau","cae136c8":"# Read the .tsv file into a pandas DataFrame\npath = \"..\/input\/restaurant-reviews\/Restaurant_Reviews.tsv\"\ndf = pd.read_csv(path, sep='\\t')","3b430b5e":"# Show the first 5 elements from the DataFrame\ndf.head()","34b7b914":"# Get insights about the dataset\nprint(df.info())\nprint(\"\\n--------------------------------\\n\")\nprint(df.describe())","a64077d2":"# Check how many examples of each class\ndf.Liked.value_counts()","dec9abb7":"# Split the sentences and the associated labels into two DataFrames\ndf_sentence = df[\"Review\"]\ndf_label = df[\"Liked\"]","58e4ae6b":"# Split the sentences and labels into training and validation sets\ntrain_sentences, val_sentences, train_labels, val_labels = train_test_split(df_sentence.to_numpy(),\n                                                                            df_label.to_numpy(),\n                                                                            test_size=0.1, \n                                                                            random_state=42)\n\nlen(train_sentences), len(val_sentences), len(train_labels), len(val_labels)","d7be8706":"# Place the USE into an encoder_layer for later use in model\n# For faster training but less accuarcy you can use: \n# https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4\nuse_url = \"https:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/5\"\nsentence_encoder_layer = hub.KerasLayer(use_url,\n                                        # shape of inputs coming to our model \n                                        input_shape=[],\n                                        # data type of inputs coming to the USE layer\n                                        dtype=tf.string,\n                                        # keep the pretrained weights (this is a feature \n                                        # extractor without fine-tuning)\n                                        trainable=False, \n                                        name=\"USE\")","62e90cee":"# Build model\nmodel = tf.keras.Sequential([\n  sentence_encoder_layer,\n  tf.keras.layers.Dense(64, activation=\"relu\"),\n  tf.keras.layers.Dense(1, activation=\"sigmoid\")\n], name=\"model_USE_large_v5\")\n\n# Compile model\nmodel.compile(loss=\"binary_crossentropy\",\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"accuracy\"])","0b89231f":"# Check the models architecture\nmodel.summary()","3508a824":"# Setup callback functions to avoid overfitting the model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\nmodel_checkpoint = ModelCheckpoint(filepath=\".\/model_checkpoint\/checkpoint.ckpt\",\n                                   # set to False to save the entire model\n                                   save_weights_only=True,\n                                   # set to False to save every model every epoch\n                                   save_best_only=True,\n                                   save_freq=\"epoch\",\n                                   monitor=\"val_loss\",\n                                   verbose=1)\n\nearly_stopping = EarlyStopping(patience=5, \n                               monitor=\"val_loss\",\n                               verbose=1)\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', \n                              factor=0.2, \n                              patience=2, \n                              min_lr=0.0005,\n                              verbose=1)\n\ncallbacks=[model_checkpoint, \n           early_stopping, \n           reduce_lr]","6213431a":"# Train the model\nEPOCHS = 50\n\nhistory_model = model.fit(train_sentences,\n                          train_labels,\n                          epochs=EPOCHS,\n                          validation_data=(val_sentences, val_labels),\n                          callbacks=callbacks)","63425657":"# Reload the best model which was saved by the ModelCheckpoint callback \n# function and evaluate the model with the validation sentences\/labels\nmodel.load_weights('.\/model_checkpoint\/checkpoint.ckpt')\neval_model = model.evaluate(val_sentences, val_labels)","db8d8761":"# Check the loss and accuracy plots\ndef plot_loss_curves(history):\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    accuracy = history.history['accuracy']\n    val_accuracy = history.history['val_accuracy']\n\n    epochs = range(len(history.history['loss']))\n\n    # Plot loss\n    plt.plot(epochs, loss, label='training_loss')\n    plt.plot(epochs, val_loss, label='val_loss')\n    plt.title('Loss')\n    plt.xlabel('Epochs')\n    plt.legend()\n\n    # Plot accuracy\n    plt.figure()\n    plt.plot(epochs, accuracy, label='training_accuracy')\n    plt.plot(epochs, val_accuracy, label='val_accuracy')\n    plt.title('Accuracy')\n    plt.xlabel('Epochs')\n    plt.legend();\n    \nplot_loss_curves(history_model)","74ae3453":"# Make predictions with the validation sentences\nmodel_pred_probs = model.predict(val_sentences)\nmodel_pred_probs[:10]","2c7b6599":"# Convert prediction probabilities into labels\nmodel_preds = tf.squeeze(tf.round(model_pred_probs))\nmodel_preds[:10]","f02adf8d":"# Calculate the metrics Accuracy, Precision, Recall and F1-Score\ndef calculate_results(y_true, y_pred):\n    # Calculate model accuracy\n    model_accuracy = accuracy_score(y_true, y_pred) * 100\n    # Calculate model precision, recall and f1 score using \"weighted\" average\n    model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, \n                                                                                 y_pred, \n                                                                                 average=\"weighted\")\n    model_results = {\"accuracy\": round(model_accuracy, 4),\n                  \"precision\": round(model_precision, 4),\n                  \"recall\": round(model_recall, 4),\n                  \"f1\": model_f1}\n    return model_results\n\ncalculate_results(val_labels,\n                  model_preds)","9814f05d":"## NOTE FOR THE NEXT STEP!\n\nIf you want to use the Universal-Sentence-Encoder (USE) from TensorFlow HUB, you have to ***enable the the internet connection*** in this kaggle notebook, else you get the following error:\n\n> URLError: <urlopen error [Errno -3] Temporary failure in name resolution>\n\n* To enable the internet on kaggle you have to verify your phone number in the settings section.\n\n* You can also see the following kaggle post: https:\/\/www.kaggle.com\/product-feedback\/63544","358c0b92":"This approach uses **TensorFlow Hub's Universal-Sentence-Encoder large v.5 (USE)**.\n\nThis notebook is a ***step by step guide*** which can be used very easily by yourself.\n\n'The Universal Sentence Encoder encodes text into high dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks.'\n\nYou can find more informations on the tfhub page:\n\nhttps:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/5","3e9d45db":"# TensorFlow Hub's Universal-Sentence-Encoder - a Step by Step guide for binary classification","61aff9ae":"## ***If this notebook has helped you, I would appreciate it if you would give me an upvote.***\nThank you very much."}}