{"cell_type":{"52ebd012":"code","9cf9142d":"code","5ad07e64":"code","27e7b6fa":"code","a549eae5":"code","229317e7":"code","3b1df6d5":"code","4f9c549a":"code","f841659b":"code","4a9db1a9":"code","b1972957":"code","a1e4fa18":"code","be7c1f42":"code","8a9bb0ce":"code","a87d141f":"code","b6212bcc":"code","27e648ef":"code","b77f9e09":"markdown","5cc0662e":"markdown","33dca1ff":"markdown","ab971c47":"markdown","09da372b":"markdown","dc201380":"markdown","1a081b9a":"markdown","c1d45297":"markdown","e6ab75a9":"markdown","07814608":"markdown","098c0d97":"markdown","b9a7a236":"markdown","c3b7e0cc":"markdown"},"source":{"52ebd012":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pickle # I need this to pickle python objects and save them on disks, because I need them on another project\n\n","9cf9142d":"dataset = pd.read_csv('..\/input\/fake-news-detection\/data.csv')","5ad07e64":"print(\"Total instances : \", len(dataset))\ndataset.head()","27e7b6fa":"print(\"Total NaNs:\")\ndataset.isna().sum()","a549eae5":"dataset=  dataset.drop(['URLs'], axis=1)\ndataset = dataset.dropna()","229317e7":"dataset.head()","3b1df6d5":"X = dataset.iloc[:,:-1].values\ny = dataset.iloc[:,-1].values","4f9c549a":"from nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport re\nps = PorterStemmer()\nfor i in range(len(dataset)):\n    X[i][0] = ' '.join([ps.stem(word) for word in re.sub('[^a-zA-Z]', ' ', X[i][0]).lower().split() if not word in stopwords.words('english')])\n    X[i][1] = ' '.join([ps.stem(word) for word in re.sub('[^a-zA-Z]', ' ', X[i][1]).lower().split() if not word in stopwords.words('english')])","f841659b":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features=5000)\nmat_body = cv.fit_transform(X[:,1]).todense()\npickle.dump(cv, open(r\"cv_body.pkl\", \"wb\"))","4a9db1a9":"cv_head = CountVectorizer(max_features=5000)\nmat_head = cv_head.fit_transform(X[:,0]).todense()\npickle.dump(cv_head, open(r\"cv_head.pkl\", \"wb\"))","b1972957":"print(\"Body matrix :\", mat_body.shape, \"Heading matrix :\", mat_head.shape)","a1e4fa18":"\nX_mat = np.hstack(( mat_head, mat_body))","be7c1f42":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_mat,y, test_size=0.2, random_state=0)","8a9bb0ce":"from sklearn.tree import DecisionTreeClassifier\nclassifier_dtr = DecisionTreeClassifier(criterion='entropy')\nclassifier_dtr.fit(X_train, y_train)\ny_pred_dtr = classifier_dtr.predict(X_test)","a87d141f":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred_dtr)","b6212bcc":"print(cm)","27e648ef":"from sklearn.externals import joblib\njoblib.dump(classifier_dtr, \"classifier_dtr_fakenews_nourl.pkl\")","b77f9e09":"Looks pretty good. It classified 29 instances incorrectly out of 798 instances. Thats 96.36 % accuracy!","5cc0662e":"Splitting the Dataset into training and testing sets","33dca1ff":"Let's check out the confusion matrix to see how well our model performed.","ab971c47":"Now lets initialize CountVectorizer with max_features=5000, so that we only focus on 5000 most frequent terms.\nAfter that we fit the CountVectorizer objects to Heading and Body columns of X to get the parse matrices","09da372b":"This is the most CPU intensive step. Here we preprocess the Heading and Body columns to get the Bag of Words model. It removes the punctuations, converts all the characters to lowercase and stem them using PorterStemmer","dc201380":"Now lets create our DecisionTreeClassifier and fit it into the training set","1a081b9a":"I am a noob and this is my first attempt at fake news classification. Suggestions are welcome. Thanks for watching my notebook. If you like it please upvote.","c1d45297":"**Fake News Classification Using DecisionTreeClassifier**","e6ab75a9":"Lets check out the shapes of our matrices :","07814608":"Lets seperate our features and targets","098c0d97":"Perfect!\nStacking the body and heading matrices together to get our feature matrix","b9a7a236":"*In this notebook I train a decision tree classifier to classify fake and genuine news. I would be using the Fake News detection dataset available on kaggle to train the classifier.*","c3b7e0cc":"The dataset contains 4009 total instances out of which, 21 instances do not have the 'Body' element(NaN). So we drop all the 21 of them so that our dataset is free of NaNs. We are also dropping the 'URLs' column because we want to fit our classifier only on the heading and Body columns."}}