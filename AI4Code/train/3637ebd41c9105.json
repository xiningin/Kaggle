{"cell_type":{"93376d12":"code","98e5aa5f":"code","f92bc021":"code","9bb4fec7":"code","43bcf6c9":"code","c8f4ec9c":"code","0dfddac5":"code","73b5cc59":"code","ede33af2":"code","a4b30f0f":"code","4ae3d55d":"code","b365da01":"code","9df14834":"code","201e597b":"code","ad00b8de":"code","db9c173d":"code","097e509e":"code","c710e81b":"code","1d76d947":"code","fd0b1e3e":"code","b3a2ec99":"code","4c4c7877":"code","93a4b247":"code","2fa3b5ab":"code","fd657c97":"code","61b456aa":"code","a6d179de":"code","8bf63b78":"code","94192d6a":"code","5d5d70c5":"code","ed122c0f":"code","bf88cd6a":"code","627582d6":"markdown","6004d2ca":"markdown","2dcb93d8":"markdown","fb00531d":"markdown","16e0d448":"markdown","cd697116":"markdown","17bda402":"markdown","56b4f4f4":"markdown","10f44c04":"markdown","d04e1574":"markdown","858bf2bb":"markdown","69b3a9f8":"markdown","65c4f537":"markdown","a913ed7a":"markdown","70233c75":"markdown","541cb689":"markdown","15ce8abd":"markdown","b4c14553":"markdown","b0783da8":"markdown","51347bdd":"markdown","ee11856d":"markdown","0edc0ee2":"markdown","ca9479b4":"markdown","c5341313":"markdown","4b1801e0":"markdown","6168c25a":"markdown"},"source":{"93376d12":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","98e5aa5f":"data = pd.read_csv('..\/input\/regression\/Ames_Housing_Sales.csv', sep=',')","f92bc021":"data.shape","9bb4fec7":"data.dtypes.value_counts()","43bcf6c9":"categorical=data.dtypes[data.dtypes == np.object]\ncategorical.index","c8f4ec9c":"n=0\nfor x in categorical.index:\n    print (x, len(data[x].unique()))\n    n=n+len(data[x].unique())\nn","0dfddac5":"categorical_var = data.dtypes[data.dtypes == np.object]  \ncategorical_var = categorical_var.index.tolist()  # list of categorical fields","73b5cc59":"categorical_var","ede33af2":"data=pd.get_dummies(data, columns=categorical_var)","a4b30f0f":"data.shape","4ae3d55d":"data.dtypes.value_counts()","b365da01":"from sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(data, test_size=0.3, random_state=42)","9df14834":"# Create a list of float colums to check for skewing\nmask = data.dtypes == np.float\nfloat_cols = data.columns[mask]\nfloat_cols","201e597b":"len(float_cols)","ad00b8de":"train[float_cols].skew().sort_values(ascending=False)","db9c173d":"skew_limit = 0.75\nskew_vals = train[float_cols].skew()\n\nskew_cols = (skew_vals\n             .sort_values(ascending=False)\n             .to_frame()\n             .rename(columns={0:'Skew'})\n             .query('abs(Skew) > {0}'.format(skew_limit)))\n\nskew_cols","097e509e":"for col in skew_cols.index.tolist():\n    if col == \"SalePrice\":\n        continue\n    train[col] = np.log1p(train[col])\n    test[col]  = np.log1p(test[col])  ","c710e81b":"feature_cols = [x for x in train.columns if x != 'SalePrice']\nX_train = train[feature_cols]\ny_train = train['SalePrice']\n\nX_test = test[feature_cols]\ny_test = test['SalePrice']","1d76d947":"from sklearn.metrics import mean_squared_error\n\ndef rmse(y_true,y_predicted):\n    return np.sqrt(mean_squared_error(y_true,y_predicted))","fd0b1e3e":"from sklearn.linear_model import LinearRegression\nlinearr=LinearRegression().fit(X_train,y_train)\nLinearRegression_rmse=rmse(y_test,linearr.predict(X_test))\nprint(LinearRegression_rmse)","b3a2ec99":"f = plt.figure(figsize=(6,6))\nax = plt.axes()\n\nax.plot(y_test, linearr.predict(X_test), marker='o', ls='', ms=3.0)\n\nlim = (0, y_test.max())\n\nax.set(xlabel='Actual Price', \n       ylabel='Predicted Price', \n       xlim=lim,\n       ylim=lim,\n       title='Linear Regression Results');","4c4c7877":"# Mute the sklearn warning about regularization\nimport warnings\nwarnings.filterwarnings('ignore', module='sklearn')","93a4b247":"from sklearn.linear_model import RidgeCV\n\nalphas = [0.005, 0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 80]\n\nRidgeCV = RidgeCV(alphas=alphas, cv=4).fit(X_train,y_train)\nRidgeCV_rmse = rmse(y_test,RidgeCV.predict(X_test))","2fa3b5ab":"print(RidgeCV.alpha_,RidgeCV_rmse)","fd657c97":"from sklearn.linear_model import LassoCV\n\nalphas2 = np.array([1e-5, 5e-5, 0.0001, 0.0005])\n\nlassoCV = LassoCV(alphas=alphas2, max_iter=5e4, cv=3).fit(X_train, y_train)\n\nlassoCV_rmse = rmse(y_test, lassoCV.predict(X_test))","61b456aa":"print(lassoCV.alpha_, lassoCV_rmse)  # Lasso is slower","a6d179de":"len(lassoCV.coef_) # Remember that number of coefficients is the same as number of features","8bf63b78":"len(lassoCV.coef_.nonzero()[0])  ","94192d6a":"from sklearn.linear_model import ElasticNetCV\n\nl1_ratios = np.linspace(0.1, 0.9, 9)\n\nelasticNetCV = ElasticNetCV(alphas=alphas2, \n                            l1_ratio=l1_ratios,\n                            max_iter=1e4).fit(X_train, y_train)\nelasticNetCV_rmse = rmse(y_test, elasticNetCV.predict(X_test))","5d5d70c5":"print(elasticNetCV.alpha_, elasticNetCV.l1_ratio_, elasticNetCV_rmse)","ed122c0f":"new_df = [['Linear', LinearRegression_rmse], ['Ridge',RidgeCV_rmse], ['Lasso', lassoCV_rmse], ['ElasticNet',elasticNetCV_rmse]]\ntable_rmse=pd.DataFrame(new_df,columns=['Model','RMSE']).set_index('Model')\ntable_rmse","bf88cd6a":"lim = (0, y_test.max())\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\nfig.suptitle('Regression Results:')\nax1.plot(y_test, RidgeCV.predict(X_test), marker='o', ls='', ms=3.0, color='green')\nax1.set(xlabel='Actual Price', ylabel='Predicted Price', xlim=lim, ylim=lim, title='RidgeCV Results')\nax2.plot(y_test, lassoCV.predict(X_test), marker='o', ls='', ms=3.0, color='red')\nax2.set(xlabel='Actual Price', ylabel='Predicted Price', xlim=lim, ylim=lim, title='LassoCV Results')\nax3.plot(y_test, elasticNetCV.predict(X_test), marker='o', ls='', ms=3.0, color='blue')\nax3.set(xlabel='Actual Price', ylabel='Predicted Price', xlim=lim, ylim=lim, title='ElasticNetCV Results')\n","627582d6":"L1 regularization \"Lasso\" will selectively shrink coefficients, some of them until zero, thus performing feature elimination, following we can see how many of our coefficients are not zero after lasso:","6004d2ca":"There are a number of columns which correspond to skewed features, as we know a log transformation can be applied to them. Take into account that our label \"SalePrice\" is in this group too, we will leave it in such group only to see its corresponding skew, but then we will omit it from the transformation. ","2dcb93d8":"### Let's read our csv file Ames_Housing_Sales: ","fb00531d":"# Comparing the RMSE calculation for every model in a table:","16e0d448":"### Build the same linear regression but now with L1 regularization \"Lasso\" and Cross Validation:","cd697116":"Let's create a list of categorial data and one-hot encode them. Pandas one-hot encoder (get_dummies) works well with data that is defined as a categorical.","17bda402":"Let's take into account this rmse as we will build models with regularization and best hyperparameters to reduce error metrics.","56b4f4f4":"Define X_train, X_test, y_train and y_test:","10f44c04":"# Building our regression models: \n### We will start with a basic linear regression model and compute its root-mean-squared error.","d04e1574":"As we can see above, the number of features has increased considerably because of the encoding.","858bf2bb":"### Now let's fit a linear regression with L2 regularization \"Ridge\" and Cross Validation to find the best alpha value:\n","69b3a9f8":"Let's find out how many unique values are in each object type column:","65c4f537":"Encoding using get_dummies function:","a913ed7a":"Let's print the skew of each feature in the training dataset:","70233c75":"# Plotting actual vs predicted house prices for the three models:","541cb689":"Above we see 258 corresponds to the sum of all unique values for each column, so after one-hot encoding them we should get 258 features.","15ce8abd":"We see above that our model has 294 coefficients, where 22 of them are zero or we could say the features were eliminated.","b4c14553":"### Hi, welcome to my project! Today we will build regression models, linear, ridge, lasso and elastic net, compute their error metrics for our case of study and choose the best one.  ","b0783da8":"# Encoding of categorical variables:","51347bdd":"In order to obtain error metric for this model we will declare a function \"rmse\" which takes the actual and predicted values and returns the root-mean-squared error. Use sklearn's mean_squared_error.","ee11856d":"**From the table above we can conclude that our Ridge regression is the best model for our case of study, due to the fact that has the lowest RMSE.**","0edc0ee2":"### Now try Elastic Net, with the same alphas as in Lasso, and l1_ratios between 0.1 and 0.9:","ca9479b4":"We confirm that 258 features were obtained by the encoding of categorical variables as we said before.\nNext, split the data in train and test data sets:","c5341313":"#### Apply the log transformation to the features with skew greater than 0.75 excluding the label:","4b1801e0":"We can determine how many of these coefficients remain non-zero.","6168c25a":"Plotting the predicted vs actual sale price based on the model."}}