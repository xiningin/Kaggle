{"cell_type":{"0a8d530a":"code","2bfabdbd":"code","ed0b650b":"code","069a13a7":"code","aeafb8b2":"code","d040bb02":"code","3fa3cec4":"code","d778a4fd":"code","ab9318fa":"code","0d5b71a5":"code","1188334f":"code","0e806dfc":"code","489f1bc9":"code","9431fc7a":"code","0497db61":"code","59975bf3":"code","f7b8831e":"code","1bdefb58":"code","1b10f5d5":"code","2971be4f":"code","8d3b235d":"code","dd82c0a1":"code","fab95196":"code","1a656746":"code","efd9e3e0":"code","de5853b4":"code","21debd2e":"code","3e4e551d":"code","ce8aec71":"code","b18e8964":"code","c73b3d0e":"code","cea5945c":"markdown","54c0273e":"markdown","3d7c4e1e":"markdown","44aeabcb":"markdown","588fc288":"markdown","8808243a":"markdown","89254c89":"markdown","c764ef21":"markdown","465e1d06":"markdown","41cd77f1":"markdown"},"source":{"0a8d530a":"#importing libraries\nimport pandas as pd \nimport numpy as np\nimport keras\nimport tensorflow\nimport seaborn as sns\nimport matplotlib.pyplot as plt","2bfabdbd":"#loading our dataset\ndf=pd.read_csv(\"..\/input\/creditcard.csv\")\ndf.tail()","ed0b650b":"#Heatmap to see the correlations between the variables\nplt.figure(figsize=(15,10))\nsns.heatmap(df.corr())\nprint(\"there is no correlation between the variables\")","069a13a7":"from sklearn.preprocessing import StandardScaler\n\n#We are going to standarize the column Amount due the range of values it has.\ndf['normAmount']= StandardScaler().fit_transform(df['Amount'].values.reshape(-1,1))\n","aeafb8b2":"df= df.drop(['Amount'],axis=1)\ndf= df.drop(['Time'],axis=1)","d040bb02":"#Splitting the dataset into X and Y\nX= df.iloc[:,df.columns != 'Class']\nY= df.iloc[:,df.columns == 'Class']","3fa3cec4":"#Splitting the dataset into the train set and the test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.3,random_state=2019)","d778a4fd":"X_train=np.array(X_train)\nX_test=np.array(X_test)\ny_train=np.array(y_train)\ny_test =np.array(y_test)","ab9318fa":"X_train.shape","0d5b71a5":"#importing the libraries of the DNN\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout","1188334f":"#Defining the neural network\nmodel = Sequential([\n    Dense(units=16,input_dim=29,activation='relu'),\n    Dense(units=24,activation='relu'),\n    Dropout(0.5),\n    Dense(units=20,activation='relu'),\n    Dense(units=24,activation='relu'),\n    Dense(units=1,activation='sigmoid'),\n])","0e806dfc":"model.summary()","489f1bc9":"model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\nmodel.fit(X_train,y_train,batch_size=15,epochs=5)","9431fc7a":"score= model.evaluate(X_test,y_test)\nprint(score)","0497db61":"y_pred= model.predict(X_test)\ny_test=pd.DataFrame(y_test)","59975bf3":"#Defining the confusion matrix\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","f7b8831e":"#Confusion matrix of our Test set\nc_mat=confusion_matrix(y_test,y_pred.round())\nplot_confusion_matrix(c_mat,classes=[0,1])","1bdefb58":"#Confusion matrix of the dataset\ny_pred=model.predict(X)\ny_expected=pd.DataFrame(Y)\ncnf_matrix=confusion_matrix(y_expected,y_pred.round())\nplot_confusion_matrix(cnf_matrix,classes=[0,1])\nplt.show()","1b10f5d5":"#creating an index for the fraudulent and normal transactions\nfraud_index=np.array(df[df.Class==1].index)\nnormal_index=df[df.Class == 0].index\ncount_fraud=len(fraud_index)\n","2971be4f":"rand_normal_index = np.random.choice(normal_index,count_fraud,replace=False)\nrand_normal_index = np.array(rand_normal_index)\n","8d3b235d":"undersample_index=np.concatenate([fraud_index,rand_normal_index])\nprint(len(undersample_index))","dd82c0a1":"#undersampling the dataset\nunder_sample_data = df.iloc[undersample_index,:]\nX_undersample = under_sample_data.iloc[:,under_sample_data.columns !='Class']\nY_undersample = under_sample_data.iloc[:,under_sample_data.columns =='Class']\n","fab95196":"#Splitting the dataset\nX_train, X_test, y_train, y_test = train_test_split(X_undersample,Y_undersample,test_size=0.3,random_state=2019)\nX_train=np.array(X_train)\nX_test=np.array(X_test)\ny_train=np.array(y_train)\ny_test =np.array(y_test)","1a656746":"#training the model\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\nmodel.fit(X_train,y_train,batch_size=15,epochs=5)","efd9e3e0":"#confusion matrix of the test (for the undersampling)\ny_pred =model.predict(X_test)\ny_expected=pd.DataFrame(y_test)\ncnf_matrix=confusion_matrix(y_expected,y_pred.round())\nplot_confusion_matrix(cnf_matrix,classes=[0,1])\nplt.show()","de5853b4":"#confusion matrix for the dataset (for the undersampling)\ny_pred=model.predict(X)\ny_expected=pd.DataFrame(Y)\ncnf_matrix=confusion_matrix(y_expected,y_pred.round())\nplot_confusion_matrix(cnf_matrix,classes=[0,1])\nplt.show()","21debd2e":"#creating the oversample\nfrom imblearn.over_sampling import SMOTE\nX_resample, y_resample = SMOTE().fit_sample(X,Y.values.ravel())\nX_resample = pd.DataFrame(X_resample)\ny_resample = pd.DataFrame(y_resample)","3e4e551d":"#Splitting the dataset\nX_train, X_test, y_train, y_test = train_test_split(X_resample,y_resample,test_size=0.3,random_state=1492)\nX_train=np.array(X_train)\nX_test=np.array(X_test)\ny_train=np.array(y_train)\ny_test =np.array(y_test)","ce8aec71":"#training the model\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\nmodel.fit(X_train,y_train,batch_size=15,epochs=5)","b18e8964":"#confusion matrix of the test (oversampling)\ny_pred =model.predict(X_test)\ny_expected=pd.DataFrame(y_test)\ncnf_matrix=confusion_matrix(y_expected,y_pred.round())\nplot_confusion_matrix(cnf_matrix,classes=[0,1])\nplt.show()","c73b3d0e":"#confusion matrix of the dataset (oversampling)\ny_pred =model.predict(X)\ny_expected=pd.DataFrame(Y)\ncnf_matrix=confusion_matrix(y_expected,y_pred.round())\nplot_confusion_matrix(cnf_matrix,classes=[0,1])\nplt.show()","cea5945c":"## Preprocessing the data <a class=\"anchor\" id=\"preprocessing\"><\/a>","54c0273e":"# Training the DNN <a class='anchor' id='training' ><\/a>","3d7c4e1e":"# Conclusions <a class='anchor' id='conclusions' ><\/a>","44aeabcb":"# Credit card fraud detection \nIn this kernel I'm trying to build an algorythm that detect if a transaction made with a credit card is fraudulent or not. For this purpose, I'm using a dataset of about 300.000 transactions with credit card. The data has been anonymized to protect the privacy of the customers.\n\n# Table of Contents:\n* [1-Exploratory Analysis](#exploratory)\n* [2- Preprocessing the data](#preprocessing)\n* [3- Deep Neural Network](#DNN)\n* [4- Training the DNN](#training)\n* [5- Evaluation of the DNN](#evaluation)\n* [6- Undersampling the dataset](#undersampling)\n* [7- SMOTE](#smote)\n* [8- Conclusions](#conclusions)\n\n\n","588fc288":"# Undersampling the dataset <a class='anchor' id='undersampling' ><\/a>","8808243a":"# SMOTE <a class='anchor' id='smote'><\/a>","89254c89":"# Evaluation of the DNN <a class='anchor' id=evaluation><\/a>","c764ef21":"### I have created a Deep Neural Network (DNN) that is able to detect the 99% of the fraudulent transactions. However, it detects some of the normal transactions as fraudulents. Nonetheless, the amount of work of the fraud detection department has been significantly reduced. ","465e1d06":"# Deep Neural Network(DNN)<a class='anchor' id='DNN'><\/a>","41cd77f1":"## Exploratory Analysis <a class=\"anchor\" id=\"exploratory\"><\/a>"}}