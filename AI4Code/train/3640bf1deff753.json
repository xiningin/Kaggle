{"cell_type":{"5279ad94":"code","b27a1f40":"code","ff5987ec":"code","17d9e8d8":"code","55eea0a7":"code","9bbcaaf3":"code","6e40b230":"code","4f30b5a1":"code","4ecaff88":"code","6a5f9b41":"code","3ad61e98":"code","0fffb568":"code","e7598914":"code","f434d217":"code","15a4fbc5":"code","3c07d65b":"code","1e7ba53e":"code","4995ab54":"code","d8160eec":"code","041f538a":"code","9a4a0e33":"code","cabff9e7":"code","fe561eb4":"code","22d80428":"code","bbb97973":"code","cda064ca":"code","9a5c3f2e":"code","640e1ffa":"code","c6df7099":"code","53a80538":"code","3d4b4296":"code","7542531f":"code","3c3f5dd7":"code","bf02ee1b":"markdown"},"source":{"5279ad94":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport string\nimport re\nfrom tqdm import tqdm\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","b27a1f40":"stop = set(stopwords.words('english'))\nprint(stopwords.words('english')[:20])","ff5987ec":"#loading the data\ntrain_set = pd.read_csv(\"..\/input\/disaster-tweet\/train.csv\")\ntest_set = pd.read_csv(\"..\/input\/disaster-tweet\/test.csv\")\n\ntrain_set.shape , test_set.shape","17d9e8d8":"#checking the data\ntrain_set.head()","55eea0a7":"train_set.tail()","9bbcaaf3":"train_set.text[0]","6e40b230":"train_set['target'].unique()","4f30b5a1":"#class distribution\n# 0 (for Non Disaster) is more than 1(for disaster) tweets\nclass_dist = train_set.target.value_counts()\nsns.barplot(class_dist.index , class_dist)","4ecaff88":"#checking null values\nnull_vals = train_set.isnull().sum()\nsns.barplot(null_vals.index , null_vals)","6a5f9b41":"#cleaning the dataset\ndef remove_spec(text):\n    text = re.sub('<.*?>+' , '' , text)\n    text = text.lower()\n    return text\n\n#removing punctuations\ndef remove_punctuation(text):\n    table = str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\n#def remove urls\ndef remove_urls(text):\n    text = re.sub(r\"https?:\\\/\\\/t.co\\\/[A-Za-z0-9]+\" , '' , text)\n    return text\n\n#remove emojis\ndef remove_emoji(text):\n    emojis = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                        \"]+\" , flags=re.UNICODE)\n    text = re.sub(emojis , '' , text)\n    return text","3ad61e98":"train_set[\"cleaned_text\"] = train_set['text'].apply(lambda x : remove_punctuation(x))\ntrain_set[\"cleaned_text\"] = train_set[\"cleaned_text\"].apply(lambda x: remove_urls(x))\ntrain_set[\"cleaned_text\"] = train_set[\"cleaned_text\"].apply(lambda x: remove_emoji(x))\ntrain_set[\"cleaned_text\"] = train_set[\"cleaned_text\"].apply(lambda x: remove_spec(x))","0fffb568":"train_set[\"cleaned_text\"].head()","e7598914":"train_set[\"cleaned_text\"][7610]","f434d217":"#creting words corpus\ndef create_corpus(dataset):\n    corpus = []\n    for review in tqdm(dataset[\"cleaned_text\"]):\n        words = [word.lower() for word in word_tokenize(review) if (word.isalpha() == 1) & (word not in stop)]\n        corpus.append(words)\n        \n    return corpus\n\ncorpus = create_corpus(train_set)","15a4fbc5":"corpus","3c07d65b":"#embedding dictionary\n\nembedding_dict = {}\n\nwith open(\"..\/input\/glove6b100dtxt\/glove.6B.100d.txt\" , encoding='utf8') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:] , 'float32')\n        embedding_dict[word] = vectors\n        \nf.close()","1e7ba53e":"#sentence tokenizaion\nmax_len = 20\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(corpus)\n\nsequences = tokenizer.texts_to_sequences(corpus)\ncorpus_pad = pad_sequences(sequences , maxlen= max_len , padding='post' , truncating='post')","4995ab54":"#unique word present\nword_index = tokenizer.word_index\nprint(f\"The Number of unique words = {len(word_index)}\")","d8160eec":"#creating embedding matrix using embedding_dict\n\nnum_words = len(word_index) + 1\nembedding_matrix = np.zeros((num_words , 100))\n\nfor word , i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n        \n    emb_vect = embedding_dict.get(word)\n    if emb_vect is not None:\n        embedding_matrix[i] = emb_vect  ","041f538a":"#ceating model\nmodel = keras.models.Sequential([\n    keras.layers.Embedding(num_words , 100 , embeddings_initializer = keras.initializers.Constant(embedding_matrix) ,\n                          input_length = max_len , trainable = False),\n    keras.layers.SpatialDropout1D(0.4),\n    keras.layers.LSTM(64 , dropout = 0.2 , recurrent_dropout = 0.2),\n    keras.layers.Dense(1 , activation = 'sigmoid')\n])\n\nmodel.compile(loss = 'binary_crossentropy' , optimizer = keras.optimizers.Adam(learning_rate=1e-4) , metrics = ['accuracy'])","9a4a0e33":"model.summary()","cabff9e7":"Xtrain , Xtest , ytrain , ytest = train_test_split(corpus_pad , train_set['target'].values , test_size = 0.2 , random_state = 42)\nXtrain.shape , ytrain.shape ","fe561eb4":"Xtrain , Xvalid = Xtrain[:4500 , :] , Xtrain[4500: , :]\nytrain , yvalid = ytrain[:4500] , ytrain[4500:]\nXtrain.shape , Xvalid.shape","22d80428":"history = model.fit(Xtrain , ytrain , batch_size=32 , epochs=50 , validation_data = (Xvalid , yvalid) , verbose = 2)","bbb97973":"#Accuracy vs epoch\nplt.title('Accuracy')\nplt.plot(history.history['accuracy'] , label = 'train')\nplt.plot(history.history['val_accuracy'] , label = 'test')\nplt.legend()\nplt.show()","cda064ca":"#Loss vs Epoch\nepoch_count = range(1 , len(history.history['loss'])+1)\nplt.plot(epoch_count , history.history['loss'] , 'r--')\nplt.plot(epoch_count , history.history['val_loss'] , 'b-')\n\nplt.legend(['Training Loss' , 'Validation Loss'])\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.show()","9a5c3f2e":"#prediction\n#clean test data\ntest_set['cleaned_text'] = test_set[\"text\"].apply(lambda x : remove_punctuation(x))\ntest_set['cleaned_text'] = test_set[\"cleaned_text\"].apply(lambda x : remove_emoji(x))\ntest_set[\"cleaned_text\"] = test_set[\"cleaned_text\"].apply(lambda x : remove_urls(x))\ntest_set['cleaned_text'] = test_set[\"cleaned_text\"].apply(lambda x : remove_spec(x))","640e1ffa":"#creating test corpus\ntest_corpus = create_corpus(test_set)","c6df7099":"#Encoding test text to sequences\ntest_sequences  = tokenizer.texts_to_sequences(test_corpus)\ntest_corpus_pad = pad_sequences(test_sequences , maxlen= max_len , padding = 'post' , truncating = \"pre\")","53a80538":"prediction = model.predict(test_corpus_pad)\nprint(prediction)\nprint(\"-\"*100)\nprint(prediction.shape)","3d4b4296":"prediction = np.round(prediction).astype(int).reshape(3263)\nprediction[20:50]","7542531f":"#creating submission file\nsubmission = pd.DataFrame({'id':test_set['id'] , 'target':prediction})\nsubmission.to_csv(\"submission.csv\" , index = False)","3c3f5dd7":"submission.head(20)","bf02ee1b":"what in this Notebook?\n-loading the data\n-then split and clean it from punctuations,special characters, and emojis\n-build and train a LSTM model \n-then predict the test set which is disaster tweets or not"}}