{"cell_type":{"354dd606":"code","3afe3921":"code","253a7a7b":"code","1cdf97b1":"code","ce9aa50a":"code","193691ef":"code","88dcdd82":"code","3aa16207":"code","9876c0c3":"code","1044e662":"code","7620df3a":"code","5908769b":"code","3ee93305":"code","3223c15c":"code","5076594d":"code","44bc0c78":"code","907488c3":"code","f77c9091":"code","6dfe3591":"code","c4b0f2c7":"code","f7898b8a":"code","d8a9a5d6":"code","687df196":"code","6ba68c83":"code","67573ef5":"code","33c7ecbd":"code","b9f6bf9f":"code","f1321e7c":"code","c4890f21":"code","117a7522":"code","effc2e11":"code","bddb4342":"code","0b6623d5":"code","74360bfc":"code","0b4d8058":"code","b49ec84c":"code","7cbb9af7":"code","83dee296":"code","cdaf8431":"code","cff55501":"code","21bf6359":"code","4d8c60fd":"markdown","a2eb6947":"markdown","0e0ea8cf":"markdown","9ae16488":"markdown","de13ff7f":"markdown","c7cd1ae3":"markdown","844cff03":"markdown","a3f63d1e":"markdown","2a696c6c":"markdown","b69b463d":"markdown","1db93472":"markdown","eb8fa00f":"markdown","ce102181":"markdown","19f56643":"markdown","b2ceeb20":"markdown","4f3341a2":"markdown","0186a451":"markdown","58cf6cd6":"markdown","fd2b5b9c":"markdown","8b10e083":"markdown"},"source":{"354dd606":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3afe3921":"cardf = pd.read_csv('..\/input\/cardata\/car data.csv')","253a7a7b":"%config Completer.use_jedi = False","1cdf97b1":"cardf.head()","ce9aa50a":"cardf.shape","193691ef":"cols = cardf.columns\nprint(cols)","88dcdd82":"for i in cols:\n    print(cardf[i].unique())","3aa16207":"cardf.isnull().sum()","9876c0c3":"new_cardf = cardf[['Year','Selling_Price', 'Present_Price', 'Kms_Driven',\n       'Fuel_Type', 'Seller_Type', 'Transmission', 'Owner']]","1044e662":"new_cardf.head()","7620df3a":"from datetime import date\ntoday = date.today()\nnew_cardf['Current_Year'] = today.year","5908769b":"new_cardf.head(2)","3ee93305":"new_cardf['Cars_age'] = new_cardf['Current_Year'] - new_cardf['Year']","3223c15c":"new_cardf.head(2)","5076594d":"new_cardf.drop(['Current_Year','Year'], axis=1 , inplace=True)","44bc0c78":"new_cardf.head(2)","907488c3":"new_cardf = pd.get_dummies(new_cardf, drop_first =True)","f77c9091":"new_cardf.head()","6dfe3591":"    new_cardf.corr()","c4b0f2c7":"import seaborn as sns\nimport matplotlib.pyplot as plt","f7898b8a":"sns.pairplot(new_cardf)","d8a9a5d6":"corrit = new_cardf.corr()\ntops_features = corrit.index\nplt.figure(figsize=(30,30))\nfig = sns.heatmap(new_cardf[tops_features].corr() , annot = True, cmap='RdYlGn')\n","687df196":"X = new_cardf.iloc[:,1:]\ny = new_cardf.iloc[:,0]","6ba68c83":"print(\" X \" , X.head())\nprint()\nprint(\" y \", y.head())","67573ef5":"from sklearn.ensemble import ExtraTreesRegressor\nmodel = ExtraTreesRegressor()\nmodel.fit(X,y)","33c7ecbd":"print(model.feature_importances_)","b9f6bf9f":"top_imp = pd.Series(model.feature_importances_, index= X.columns)\ntop_imp.nlargest(5).plot(kind= 'barh')\nplt.show()","f1321e7c":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size= 0.2, random_state = 42)","c4890f21":"X_train.shape\n","117a7522":"from sklearn.ensemble import RandomForestRegressor\n","effc2e11":"n_estimators = [int(x) for x in np.linspace(start= 100, stop = 1300, num=12)]\nmax_features = ['auto', 'sqrt']\n\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\nmin_samples_split = [3, 5, 10, 15, 100]\nmin_samples_leaf = [1, 2, 5, 10]","bddb4342":"random_est = {'n_estimators':n_estimators ,\n              'max_features':max_features,\n              'max_depth':max_depth,\n              'min_samples_split':min_samples_split,\n              'min_samples_leaf':min_samples_leaf\n             }","0b6623d5":"from sklearn.model_selection import RandomizedSearchCV","74360bfc":"rf = RandomForestRegressor()","0b4d8058":"rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_est,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)","b49ec84c":"rf_random.fit(X_train , y_train)","7cbb9af7":"rf_random.best_params_","83dee296":"pred = rf_random.predict(X_test)\nprint(pred)","cdaf8431":"sns.distplot(y_test-pred)","cff55501":"plt.scatter(y_test,pred)\n","21bf6359":"from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(y_test, pred))\nprint('MSE:', metrics.mean_squared_error(y_test, pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, pred)))","4d8c60fd":"now lets train the data set , first lets split it in train and test data","a2eb6947":"**> *now to create a feautre of how old the car is***","0e0ea8cf":"> **now as we have calcualted the age of the car , we can now drop year and current year columns from our dataset**","9ae16488":"Important Features","de13ff7f":"NOW WE WILL CHECK FOR CORRELATIONS BETWEEN THE COLUMNS OF OUR DATASET (-1 to 1) more correlated as goes near to 1","c7cd1ae3":"CHECKING ALL UNIQUE VALUES","844cff03":"WE ARE PRINTING JUST THE FIRST 10 ROWS","a3f63d1e":"now lets visualize the above correlations","2a696c6c":"> **this will give best parameters**","b69b463d":"checking missing values in our dataset","1db93472":"> **we use drop first to del 1st column to protect it from dummy variable tap(google about it)**","eb8fa00f":"As we can see that there are no null values in our dataset so it is good for us ","ce102181":"now lets plot the top most important features","19f56643":"Now we will use random forest regressor ","b2ceeb20":"WE ARE GOING TO LOAD THE DATASET","4f3341a2":"we will use the random grid to search for best hyperparameters","0186a451":"set hyper parameters ","58cf6cd6":"NOW WE WILL CREATE ANOTHER COLUMN WITH THE  CURRENT YEAR  THAT WILL HELP US IN FINDING OUT HOW OLD THE CAR IS","fd2b5b9c":"AS WE DONT NEED THE CAR NAME BECAUSE THERE CAN BE MULTIPLE AND WE DONT HAVE SUCH USAGE OF IT RIGHT NOW","8b10e083":"> **NOW ITS TIME TO CONVERT THE CATEGORICAL VALUES INTO OTHER VALUES FOR EX FUEL TYPE TO AN INTEGER VALUE , BY USING GET DUMMIES INSIDE PANDAS IN ONE HOT ENCODING**"}}