{"cell_type":{"291ccaae":"code","ebb3ee74":"code","292bf70c":"code","579cf46f":"code","cce1daf9":"code","b84108f9":"code","36492ecb":"code","5b69ea6a":"code","7ec23636":"code","69f1cdd3":"code","0740fb46":"code","6d03eca6":"code","ca71f931":"code","7f6d0fe5":"code","1136ec6c":"code","f007342a":"code","06fc8e5f":"code","631269aa":"code","7df62c72":"code","53f75aa1":"code","8a24cb20":"code","51aa4aeb":"code","38cd0c19":"code","fd360d30":"code","c0ca52ab":"code","62949b9a":"code","f38d7c63":"code","d1cbf58b":"code","f8fed820":"code","ed610f1c":"code","6ac16228":"code","d32f7787":"code","77a09bb8":"code","c5d9b044":"code","8639e220":"code","0604a17b":"code","a2db5305":"code","282b7779":"code","03334294":"code","4f356fcc":"code","b9e76976":"code","33fa89cf":"code","609e9a3e":"code","31d835c4":"code","0ab759de":"code","20ccf36e":"code","eb6b46c6":"code","8c6ead4d":"code","0f0360d6":"code","c7c04fe9":"code","6407f970":"code","e8bc2519":"code","7c099ae6":"code","94b8bdef":"code","8e67f39c":"code","e9e16b1e":"code","7d2ab1e0":"code","90dced95":"code","1654bd69":"code","310de761":"code","baf0035d":"code","1ea80053":"code","7b23876d":"code","394a98bf":"code","8310388d":"code","2ce4b0ce":"code","40b4559e":"code","9cc38439":"code","6897d95e":"code","579b0c36":"code","bfa0d156":"code","a946d7d7":"code","f1b7393c":"code","809ae03c":"code","869933f9":"code","5fbb0ac8":"code","e3debf93":"code","8969b3a1":"code","7ecb21ae":"code","7cdb6fbf":"code","565a0f3f":"code","6df29709":"code","5fe09ca4":"code","9a12d902":"code","4a51369f":"code","8555f413":"code","d5e7d4c9":"code","4c1f6088":"code","b90cac5f":"code","6a3df4c6":"code","c4735e40":"code","99473423":"code","ff201baf":"code","8a396a75":"code","5747ef9e":"code","809bd989":"code","685cf3e2":"code","db91ec5e":"code","299c356a":"code","65054273":"code","ca91c8a7":"code","643a48c2":"code","805040c9":"code","159d5a32":"code","33b5f73c":"code","451725b6":"code","d52add96":"code","04dd6fe8":"code","1e02aa62":"code","ffc60869":"code","c860a2eb":"code","ea8fe878":"code","2af4365d":"code","681eb8dc":"code","eaa91c1a":"code","bb604b75":"code","40d21499":"code","20a9f507":"code","d12b5c93":"code","1c490298":"code","293c0cdd":"code","f53ad671":"code","48c93ef9":"code","c507aba1":"code","fb70e44b":"code","9e24ee55":"code","dff272cb":"code","cd4a92de":"code","32fc20c8":"code","fad38d8f":"code","2531fa02":"code","0d2bac82":"code","90653615":"code","65ef0a09":"code","12e23ff4":"code","43679f08":"code","53c222df":"code","8c7cf266":"code","96697de7":"code","da4cdd1a":"code","4fa11717":"code","5249b3bb":"code","e8b48ba3":"code","e4158409":"code","5264a983":"code","e26d9404":"code","3c8c6834":"code","c56c328f":"code","1b08ddc4":"code","e1e8d14e":"code","ba3f9603":"code","10ea75f1":"code","3a8d7f12":"code","207b255f":"code","a4e6d389":"code","e1e7ecb4":"code","661851cf":"code","6f6e6eb3":"code","75d57209":"code","729b3d09":"code","924c50b6":"code","4e124b88":"code","0ba371fb":"code","ddf2cf2d":"code","a769bd41":"code","58e138ae":"code","92bb11e8":"code","5b0c909c":"code","08b674ee":"code","7ecb7c8f":"code","4899b450":"code","6b7e2112":"code","5fb2eda4":"code","43c1edb1":"code","3756bf19":"code","68deca84":"code","3512cf56":"code","3bfc6f11":"code","31a4bc39":"code","17aad14d":"code","766988ca":"code","72402650":"code","10c9b4b7":"code","e26cf2f9":"code","bed7f105":"code","5bcd79d6":"code","f9506876":"code","161ed5af":"code","1285541a":"code","95720527":"code","d42f976b":"code","f8b86e92":"code","92087f4b":"code","6e0572dc":"code","5e3d7a7b":"code","85f97ff4":"code","529419b3":"code","00a8d5cd":"code","ac969448":"code","246893fd":"markdown","c820814a":"markdown","91df8230":"markdown","f30c18c2":"markdown","731abf7c":"markdown","3f6f5b8c":"markdown","0bc6451f":"markdown","931bf053":"markdown","6ce1fa1a":"markdown","d93741f8":"markdown","4062284d":"markdown","b76a73c5":"markdown","93c36f67":"markdown","cbef840a":"markdown","bf542ed2":"markdown","439a6594":"markdown","7f6fb575":"markdown","60be620a":"markdown","5b9a7396":"markdown","15154140":"markdown","0172c059":"markdown","ca43d061":"markdown","3ff03b90":"markdown","5aba187e":"markdown","154358d5":"markdown","4abaa454":"markdown","0e7b1db6":"markdown","9f5689cc":"markdown","976a37f0":"markdown","1582ba5b":"markdown","21483071":"markdown","b4116a63":"markdown","9675b25e":"markdown","be1206ed":"markdown","488fcc5f":"markdown","2c8b813b":"markdown","9cf250bf":"markdown","047eedfd":"markdown","e7d05481":"markdown","7bedc157":"markdown","bb6862a3":"markdown","60acbec8":"markdown","946dbd2d":"markdown","b876e45d":"markdown","3079b0a0":"markdown","e4e30f20":"markdown","514fea1b":"markdown","27334ad0":"markdown","570c4c1c":"markdown","4b1993e4":"markdown","ea101049":"markdown","54b6fa93":"markdown","966a3779":"markdown","cb46b78e":"markdown","09bae487":"markdown","23644de9":"markdown","44aca379":"markdown","607fea88":"markdown","b1d4412b":"markdown","8ebccb8b":"markdown","7281e921":"markdown","2d5d600b":"markdown","497e3eed":"markdown","453363e4":"markdown","5a99d35e":"markdown","f789ad9d":"markdown","be78cb98":"markdown","d0a2c02f":"markdown","369eff93":"markdown","a60467a8":"markdown","40cba358":"markdown","4b9e7834":"markdown","0b442110":"markdown","8408c0f9":"markdown","8f349aea":"markdown","f2655b50":"markdown","85e64c1a":"markdown","4fefdc4d":"markdown","daf5f5ad":"markdown","2be716d1":"markdown","8e0894e0":"markdown","ba8cb3df":"markdown","41b9699d":"markdown","d084de1d":"markdown","a151a259":"markdown","68d649d6":"markdown","c2b6a160":"markdown","91931c7d":"markdown","bee64778":"markdown","82aa2bf9":"markdown","833a26bf":"markdown","0353cd04":"markdown","279c189f":"markdown","d44c31b6":"markdown","857d7cdb":"markdown","9a70c6f6":"markdown","4fa68e48":"markdown","e1b5c44e":"markdown","e03f98c2":"markdown","015cfcb1":"markdown","26b6ada1":"markdown","2eeddb5e":"markdown","075881b9":"markdown","f66e2421":"markdown","46b9be46":"markdown","94ab6fb3":"markdown","33854393":"markdown"},"source":{"291ccaae":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom pandas import plotting\nfrom matplotlib.ticker import ScalarFormatter\nfrom matplotlib import gridspec\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nimport math\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.impute import KNNImputer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import TruncatedSVD\n\nimport xgboost as xgb\nfrom xgboost import plot_importance\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import tree as Arbol\nfrom sklearn.tree import export_graphviz\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\n\n\n\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.cluster import KMeans\n\n\n\n#Librerias a instalar\nimport sys\n#!{sys.executable} -m pip install pandas-profiling \nimport pandas_profiling as pdp\nimport gc\ngc.enable()","ebb3ee74":"dataset = pd.read_csv(\"..\/input\/properati-dataset\/DS_Proyecto_01_Datos_Properati.csv\")","292bf70c":"dataset.columns","579cf46f":"dataset.shape","cce1daf9":"dataset.head()","b84108f9":"dataset.tail()","36492ecb":"dataset.describe()","5b69ea6a":"dataset.isnull().sum()","7ec23636":"dataset[\"property_type\"].unique()","69f1cdd3":"dataset[\"property_type\"].value_counts()","0740fb46":"plt.figure(figsize=(10,5))\nplt.yscale('log')\nplt.gca().yaxis.set_major_formatter(ScalarFormatter())\nax = sns.countplot(data = dataset, x = \"property_type\")\nax.set_xticklabels(ax.get_xticklabels(),rotation=40,ha=\"right\")\n\n\nplt.show()","6d03eca6":"fig= plt.subplots(figsize=(20,18),constrained_layout=True)\ngrid = gridspec.GridSpec(2, 1, height_ratios=[1, 3])\n\nax1=plt.subplot(grid[0])\nsns.countplot(data=dataset,y=\"l2\",order=dataset[\"l2\"].value_counts().index,ax=ax1,color=\"g\")\n\nax1.set_yticklabels(ax1.get_yticklabels(),fontsize=\"medium\")\nax1.set_title(\"Distribucion segun el G.B.A.\", fontsize= 'large')\n\nax2=plt.subplot(grid[1])\nsns.countplot(data=dataset,x=\"l3\",order=dataset[\"l3\"].value_counts().index,ax=ax2,color=\"b\")\n\n\nax2.set_title(\"Distribucion segun barrios\", fontsize= 'large')\nax2.set_xticklabels(ax2.get_xticklabels(),rotation=90,ha=\"right\")\nplt.yticks(fontsize= 11)\nax1.grid()\nax2.grid()\nplt.show()","ca71f931":"dataset = dataset[((dataset['property_type'] == \"Departamento\") |(dataset['property_type'] == \"Casa\") | (dataset['property_type'] == \"PH\"))  & (dataset['l2'] == \"Capital Federal\")] \ndataset.shape","7f6d0fe5":"reporte = pdp.ProfileReport(dataset, title=\"Pandas Profiling Reporte\",minimal=True)\nreporte","1136ec6c":"dataset.shape","f007342a":"sns.scatterplot(data=dataset, x='surface_total', y='surface_covered')\nplt.grid()\nplt.show()","06fc8e5f":"dataset.drop(dataset.loc[dataset['surface_covered'] > dataset['surface_total']].index,inplace=True ,axis=0)","631269aa":"sns.scatterplot(data=dataset, x='surface_total', y='surface_covered')\nplt.grid()\nplt.show()","7df62c72":"dataset.shape","53f75aa1":"for i in range(dataset.shape[1]):\n    print(i,len(pd.unique(dataset.iloc[:,i])))","8a24cb20":"dataset = dataset.drop([\"l1\",\"l2\",\"currency\",\"operation_type\"],axis=1)","51aa4aeb":"dataset.columns","38cd0c19":"for i in range(dataset.shape[1]):\n    num=len(pd.unique(dataset.iloc[:,i]))\n    porcentaje=float(num)\/dataset.shape[0]*100\n    print(\"%d, %d, %.1f%%\"%(i,num,porcentaje))","fd360d30":"duplicado = dataset.duplicated()\nprint(duplicado.any())\nprint(dataset[duplicado])","c0ca52ab":"print(dataset.shape)\ndataset.drop_duplicates(inplace=True)\nprint(dataset.shape)","62949b9a":"dataset = dataset.drop([\"start_date\",\"end_date\",\"title\",\"description\"],axis=1)","f38d7c63":"fig =plt.subplots(figsize=(8,8))\nsns.heatmap(dataset.corr(), annot=True)","d1cbf58b":"dataset1 = dataset.copy()","f8fed820":"dataset1.isnull().sum()","ed610f1c":"X= dataset1.drop([\"price\",\"created_on\",\"lat\",\"lon\",\"property_type\",\"l3\"],axis=1)\ny=dataset1[\"price\"]\nX.head()","6ac16228":"X.isnull().sum()","d32f7787":"tree = DecisionTreeRegressor()\nresultado = []\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\ntipos = ['mean', 'median', 'most_frequent', 'constant']\nfor t in tipos:\n    imputer=SimpleImputer(strategy=t)\n    imputer.fit(X)\n    X_trans= imputer.transform(X)\n    tree = DecisionTreeRegressor(max_depth=10,random_state=42)\n    tree.fit(X_trans,y)\n    y_pred = tree.predict(X_trans)\n    r2 = metrics.r2_score(y, y_pred)\n    RMSE=np.sqrt(mean_squared_error(y,y_pred))\n    resultado.append(r2)\n    print(\"La estrategia utilizada--->\",t)\n    print(\"RMSE segun el tipo de estrategia:\",RMSE)\n    print(\"----------------------------------------\")\n","77a09bb8":"print('Missing: %d' % sum(np.isnan(X_trans).flatten()))","c5d9b044":"vecinos = [6]\nfor v in vecinos:\n    KNN_imputer=KNNImputer(n_neighbors=v)\n    KNN_imputer.fit(X)\n    X_knn= KNN_imputer.transform(X)\n    tree = DecisionTreeRegressor(max_depth=10,random_state=42)\n    tree.fit(X_knn,y)\n    y_pred = tree.predict(X_knn)\n    r2 = metrics.r2_score(y, y_pred)\n    RMSE=np.sqrt(mean_squared_error(y,y_pred))\n    resultado.append(r2)\n    print(\"La cantidad de vecinos utilizados--->\",v)\n    print(\"RMSE segun el tipo de estrategia:\",RMSE)\n    print(\"----------------------------------------\")","8639e220":"print('Missing: %d' % sum(np.isnan(X_knn).flatten()))","0604a17b":"tipos2 = ['ascending', 'descending', 'roman', 'arabic', 'random']\nfor t in tipos2:\n    iter_imputer=IterativeImputer(imputation_order=t)\n    iter_imputer.fit(X)\n    X_iter= iter_imputer.transform(X)\n    tree = DecisionTreeRegressor(max_depth=10,random_state=42)\n    tree.fit(X_iter,y)\n    y_pred = tree.predict(X_iter)\n    r2 = metrics.r2_score(y, y_pred)\n    RMSE=np.sqrt(mean_squared_error(y,y_pred))\n    resultado.append(r2)\n    print(\"El metodo utilizado es--->\",t)\n    print(\"RMSE segun el tipo de estrategia:\",RMSE)\n    print(\"----------------------------------------\")","a2db5305":"print('Missing: %d' % sum(np.isnan(X_iter).flatten()))","282b7779":"dataset.isnull().sum()","03334294":"KNN_imputer=KNNImputer(n_neighbors=6)\nKNN_imputer=KNN_imputer.fit(dataset.iloc[:,6:9])\ndataset.iloc[:,6:9]=KNN_imputer.transform(dataset.iloc[:,6:9])","4f356fcc":"KNN_imputer=KNNImputer(n_neighbors=6)\nKNN_imputer=KNN_imputer.fit(dataset.iloc[:,1:3])\ndataset.iloc[:,1:3]=KNN_imputer.transform(dataset.iloc[:,1:3])","b9e76976":"dataset.isnull().sum()","33fa89cf":"fig =plt.subplots(figsize=(8,8))\nsns.heatmap(dataset.corr(), annot=True)","609e9a3e":"dataset.shape","31d835c4":"for x in [\"Casa\",\"PH\", \"Departamento\"]:\n    Q1 = dataset[dataset[\"property_type\"]==x][\"price\"].quantile(0.25)\n    Q3 = dataset[dataset[\"property_type\"]==x][\"price\"].quantile(0.75)\n    IQR = Q3 - Q1\n    lim_min = dataset[dataset[\"property_type\"]==x][\"price\"].quantile(0.01)\n    lim_max = Q3 + (IQR*1.5)\n    print(x)\n    print(\"el precio maximo es {}, el precio minimo es {} y el IQR {}\" .format(lim_max,lim_min,IQR))\n    print(\"-------------------------------------------------------------------\")\n    ","0ab759de":"dptos=dataset[dataset[\"property_type\"]==\"Departamento\"]\nphs=dataset[dataset[\"property_type\"]==\"PH\"]\ncasas=dataset[dataset[\"property_type\"]==\"Casa\"]","20ccf36e":"dptos= dptos[(dptos.price <= 392500.0) & (dptos.price >=65000.0) ]\nphs=phs[(phs.price <= 428500.0) & (phs.price >=75000.0) ]\ncasas=casas[(casas.price <= 706000.0) & (casas.price >=110000.0) ]","eb6b46c6":"figure,(ax1,ax2,ax3) = plt.subplots(3,constrained_layout=True,figsize=(10,10))\nax1.set_title(\"Relacion tipo de propiedad y precio(En miles de Dolares)\")\nax2.set_title(\"Relacion tipo de propiedad y precio(En miles de Dolares)\")\nax3.set_title(\"Relacion tipo de propiedad y precio(En miles Dolares)\")\nsns.boxplot(data=dptos,x=\"price\",y=\"property_type\",ax=ax1)\nsns.boxplot(data=phs,x=\"price\",y=\"property_type\",ax=ax2)\nsns.boxplot(data=casas,x=\"price\",y=\"property_type\",ax=ax3)\nax1.grid()\nax2.grid()\nax3.grid()\nplt.show()","8c6ead4d":"dataset[\"rooms\"].value_counts().sort_index()","0f0360d6":"rooms_min_list=[]\nrooms_max_list=[]\n\nfor x in [\"Casa\",\"PH\", \"Departamento\"]:\n    Q1 = dataset[dataset[\"property_type\"]==x][\"rooms\"].quantile(0.25)\n    Q3 = dataset[dataset[\"property_type\"]==x][\"rooms\"].quantile(0.75)\n    IQR = Q3 - Q1\n    lim_min = dataset[dataset[\"property_type\"]==x][\"rooms\"].quantile(0.01)\n    lim_max = Q3 + (IQR*1.5)\n    \n    rooms_min_list.append(lim_min)\n    rooms_max_list.append(lim_max)\n    \n    print(x)\n    print(\"el N maximo de habitaciones es {}, el N minimo de habitaciones es {} y el IQR {}\" .format(lim_max,lim_min,IQR))\n    print(\"-------------------------------------------------------------------\")\n#dataset[dataset[\"property_type\"]==x][\"rooms\"].quantile(0.01)  ","c7c04fe9":"for i in rooms_max_list:\n    floor_max=math.floor(i)\n    print(floor_max)","6407f970":"dptos= dptos[(dptos.rooms <= 4.0) & (dptos.rooms >=1.0) ]\nphs=phs[(phs.rooms <= 5.0) & (phs.rooms >=2.0) ]\ncasas=casas[(casas.rooms <= 6.0) & (casas.rooms >=2.0) ]","e8bc2519":"figure,(ax1,ax2,ax3) = plt.subplots(3,constrained_layout=True,figsize=(10,10))\nax1.set_title(\"Relacion tipo de propiedad y cantidad de habitaciones\")\nax2.set_title(\"Relacion tipo de propiedad y cantidad de habitaciones\")\nax3.set_title(\"Relacion tipo de propiedad y cantidad de habitaciones\")\nsns.boxplot(data=dptos,x=\"rooms\",y=\"property_type\",ax=ax1)\nsns.boxplot(data=phs,x=\"rooms\",y=\"property_type\",ax=ax2)\nsns.boxplot(data=casas,x=\"rooms\",y=\"property_type\",ax=ax3)\nax1.grid()\nax2.grid()\nax3.grid()\nplt.show()","7c099ae6":"dataset[\"bathrooms\"] = dataset[\"bathrooms\"].astype(\"int32\")\ndataset[\"bathrooms\"].value_counts().sort_index()","94b8bdef":"bath_min_list=[]\nbath_max_list=[]\n\nfor x in [\"Casa\",\"PH\", \"Departamento\"]:\n    Q1 = dataset[dataset[\"property_type\"]==x][\"bathrooms\"].quantile(0.25)\n    Q3 = dataset[dataset[\"property_type\"]==x][\"bathrooms\"].quantile(0.75)\n    IQR = Q3 - Q1\n    lim_min = dataset[dataset[\"property_type\"]==x][\"bathrooms\"].quantile(0.01)\n    lim_max = Q3 + (IQR*1.5)\n    \n    bath_min_list.append(lim_min)\n    bath_max_list.append(lim_max)\n    \n    print(x)\n    print(\"el N maximo de ba\u00f1os es {}, el N minimo de ba\u00f1os es {} y el IQR {}\" .format(lim_max,lim_min,IQR))\n    print(\"-------------------------------------------------------------------\")\n#dataset[dataset[\"property_type\"]==x][\"rooms\"].quantile(0.01)  ","8e67f39c":"for i in bath_max_list:\n    b_floor_max=math.floor(i)\n    print(b_floor_max)","e9e16b1e":"dptos= dptos[(dptos.bathrooms <= 3.0) & (dptos.bathrooms >=1.0) ]\nphs=phs[(phs.bathrooms <= 3.0) & (phs.bathrooms >=1.0) ]\ncasas=casas[(casas.bathrooms <= 4.0) & (casas.bathrooms >=1.0) ]","7d2ab1e0":"figure,(ax1,ax2,ax3) = plt.subplots(3,constrained_layout=True,figsize=(10,10))\nax1.set_title(\"Relacion tipo de propiedad y cantidad de ba\u00f1os\")\nax2.set_title(\"Relacion tipo de propiedad y cantidad de ba\u00f1os\")\nax3.set_title(\"Relacion tipo de propiedad y cantidad de ba\u00f1os\")\nsns.boxplot(data=dptos,x=\"bathrooms\",y=\"property_type\",ax=ax1)\nsns.boxplot(data=phs,x=\"bathrooms\",y=\"property_type\",ax=ax2)\nsns.boxplot(data=casas,x=\"bathrooms\",y=\"property_type\",ax=ax3)\nax1.grid()\nax2.grid()\nax3.grid()\nplt.show()","90dced95":"dataset[\"bedrooms\"].value_counts().sort_index()","1654bd69":"bedr_min_list=[]\nbedr_max_list=[]\n\nfor x in [\"Casa\",\"PH\", \"Departamento\"]:\n    Q1 = dataset[dataset[\"property_type\"]==x][\"bedrooms\"].quantile(0.25)\n    Q3 = dataset[dataset[\"property_type\"]==x][\"bedrooms\"].quantile(0.75)\n    IQR = Q3 - Q1\n    lim_min = dataset[dataset[\"property_type\"]==x][\"bedrooms\"].quantile(0.01)\n    lim_max = Q3 + (IQR*1.5)\n    \n    bedr_min_list.append(lim_min)\n    bedr_max_list.append(lim_max)\n    \n    print(x)\n    print(\"el N maximo de dormitorios es {}, el N minimo de dormitorios es {} y el IQR {}\" .format(lim_max,lim_min,IQR))\n    print(\"-------------------------------------------------------------------\")\n#dataset[dataset[\"property_type\"]==x][\"rooms\"].quantile(0.01)  ","310de761":"for i in bedr_max_list:\n    br_floor_max=math.floor(i)\n    print(br_floor_max)","baf0035d":"dptos= dptos[(dptos.bedrooms <= 3.0) & (dptos.bedrooms >=0.0) ]\nphs=phs[(phs.bedrooms <= 4.0) & (phs.bedrooms >=1.0) ]\ncasas=casas[(casas.bedrooms <= 5.0) & (casas.bedrooms >=1.0) ]","1ea80053":"figure,(ax1,ax2,ax3) = plt.subplots(3,constrained_layout=True,figsize=(10,10))\nax1.set_title(\"Relacion tipo de propiedad y cantidad de dormitorios\")\nax2.set_title(\"Relacion tipo de propiedad y cantidad de dormitorios\")\nax3.set_title(\"Relacion tipo de propiedad y cantidad de dormitorios\")\nsns.boxplot(data=dptos,x=\"bedrooms\",y=\"property_type\",ax=ax1)\nsns.boxplot(data=phs,x=\"bedrooms\",y=\"property_type\",ax=ax2)\nsns.boxplot(data=casas,x=\"bedrooms\",y=\"property_type\",ax=ax3)\nax1.grid()\nax2.grid()\nax3.grid()\nplt.show()","7b23876d":"surfC_min_list=[]\nsurfC_max_list=[]\n\nfor x in [\"Casa\",\"PH\", \"Departamento\"]:\n    Q1 = dataset[dataset[\"property_type\"]==x][\"surface_covered\"].quantile(0.25)\n    Q3 = dataset[dataset[\"property_type\"]==x][\"surface_covered\"].quantile(0.75)\n    IQR = Q3 - Q1\n    lim_min = dataset[dataset[\"property_type\"]==x][\"surface_covered\"].quantile(0.01)\n    lim_max = Q3 + (IQR*1.5)\n    \n    surfC_min_list.append(lim_min)\n    surfC_max_list.append(lim_max)\n    \n    print(x)\n    print(\"el maximo de surface covered es {}, el minimo de surface covered es {} y el IQR {}\" .format(lim_max,lim_min,IQR))\n    print(\"-------------------------------------------------------------------\")\n#dataset[dataset[\"property_type\"]==x][\"rooms\"].quantile(0.01)  ","394a98bf":"dptos= dptos[(dptos.surface_covered <= 164.86) & (dptos.surface_covered >=25.0) ]\nphs=phs[(phs.surface_covered <= 214.5) & (phs.surface_covered >=31.0) ]\ncasas=casas[(casas.surface_covered <= 405.0) & (casas.surface_covered >=47.33) ]","8310388d":"figure,(ax1,ax2,ax3) = plt.subplots(3,constrained_layout=True,figsize=(10,10))\nax1.set_title(\"Relacion tipo de propiedad y superficie cubierta\")\nax2.set_title(\"Relacion tipo de propiedad y superficie cubierta\")\nax3.set_title(\"Relacion tipo de propiedad y superficie cubierta\")\nsns.boxplot(data=dptos,x=\"surface_covered\",y=\"property_type\",ax=ax1)\nsns.boxplot(data=phs,x=\"surface_covered\",y=\"property_type\",ax=ax2)\nsns.boxplot(data=casas,x=\"surface_covered\",y=\"property_type\",ax=ax3)\nax1.grid()\nax2.grid()\nax3.grid()\nplt.show()","2ce4b0ce":"surfT_min_list=[]\nsurfT_max_list=[]\n\nfor x in [\"Casa\",\"PH\", \"Departamento\"]:\n    Q1 = dataset[dataset[\"property_type\"]==x][\"surface_total\"].quantile(0.25)\n    Q3 = dataset[dataset[\"property_type\"]==x][\"surface_total\"].quantile(0.75)\n    IQR = Q3 - Q1\n    lim_min = dataset[dataset[\"property_type\"]==x][\"surface_total\"].quantile(0.01)\n    lim_max = Q3 + (IQR*1.5)\n    \n    surfT_min_list.append(lim_min)\n    surfT_max_list.append(lim_max)\n    \n    print(x)\n    print(\"el maximo de surface total es {}, el minimo de surface total es {} y el IQR {}\" .format(lim_max,lim_min,IQR))\n    print(\"-------------------------------------------------------------------\")\n#dataset[dataset[\"property_type\"]==x][\"rooms\"].quantile(0.01)  ","40b4559e":"dptos= dptos[(dptos.surface_total <= 182.91) & (dptos.surface_total >=26.0) ]\nphs=phs[(phs.surface_total <= 275.0) & (phs.surface_total >=35.0) ]\ncasas=casas[(casas.surface_total <= 583.12) & (casas.surface_total >=56.81) ]","9cc38439":"figure,(ax1,ax2,ax3) = plt.subplots(3,constrained_layout=True,figsize=(10,10))\nax1.set_title(\"Relacion tipo de propiedad y superficie total\")\nax2.set_title(\"Relacion tipo de propiedad y superficie total\")\nax3.set_title(\"Relacion tipo de propiedad y superficie total\")\nsns.boxplot(data=dptos,x=\"surface_total\",y=\"property_type\",ax=ax1)\nsns.boxplot(data=phs,x=\"surface_total\",y=\"property_type\",ax=ax2)\nsns.boxplot(data=casas,x=\"surface_total\",y=\"property_type\",ax=ax3)\nax1.grid()\nax2.grid()\nax3.grid()\nplt.show()","6897d95e":"conc_1=[dptos,phs,casas]\ndataset= pd.concat(conc_1)","579b0c36":"dataset.head(5)","bfa0d156":"dataset.shape","a946d7d7":"fig =plt.subplots(figsize=(8,8))\nsns.heatmap(dataset.corr(), annot=True)","f1b7393c":"dataset.head()","809ae03c":"dataset[\"fechas\"] = pd.to_datetime(dataset.created_on, format=\"%Y-%m-%d\")\ndataset.drop(\"created_on\",axis=1,inplace=True)\ndataset.head()","869933f9":"dataset['a\u00f1o'] = pd.DatetimeIndex(dataset['fechas']).year\ndataset.drop(\"fechas\",axis=1,inplace=True)\ndataset.head()","5fbb0ac8":"Labelencoder= LabelEncoder()\nLabelencoder.fit(dataset[\"a\u00f1o\"])\ndataset[\"a\u00f1os\"] = Labelencoder.transform(dataset[\"a\u00f1o\"])\ndataset.drop(\"a\u00f1o\",axis=1,inplace=True)\ndataset.head(10)","e3debf93":"Labelencoder= LabelEncoder()\nLabelencoder.fit(dataset[\"property_type\"])\ndataset[\"Tipo_propiedad\"] = Labelencoder.transform(dataset[\"property_type\"])\ndataset.drop(\"property_type\",axis=1,inplace=True)","8969b3a1":"list(Labelencoder.classes_)","7ecb21ae":"Labelencoder.transform([\"Casa\", \"Departamento\", \"PH\"])","7cdb6fbf":"dataset.head()","565a0f3f":"dataset_cluster= dataset.copy()","6df29709":"dataset = pd.get_dummies(data=dataset,prefix=\"barrio\",columns=[\"l3\"],drop_first=True)\ndataset.head()","5fe09ca4":"dataset[\"Tipo_propiedad\"].head","9a12d902":"dataset.shape","4a51369f":"dataset.columns","8555f413":"fig =plt.subplots(figsize=(8,8))\ndataset_prob=dataset[[\"lat\",\"lon\",\"rooms\",\"bedrooms\",\"bathrooms\",\"surface_total\",\"surface_covered\",\"price\",\"a\u00f1os\",\"Tipo_propiedad\"]]\ndataset_prob.head()\nsns.heatmap(dataset_prob.corr(), annot=True)","d5e7d4c9":"reporte = pdp.ProfileReport(dataset, title=\"Pandas Profiling Reporte\",minimal=True)\nreporte","4c1f6088":"dataset1 = dataset.copy()\nX = dataset1.drop(\"price\",axis=1)\ny = dataset1[\"price\"]","b90cac5f":"from sklearn import preprocessing","6a3df4c6":"robust_sc = preprocessing.RobustScaler()\nstandard_sc = preprocessing.StandardScaler() \nminmax_sc = preprocessing.MinMaxScaler() ","c4735e40":"for x in [robust_sc,standard_sc,minmax_sc]:\n    scaler = x.fit(X)\n    X_new = x.transform(X)\n    tree = DecisionTreeRegressor(max_depth=10,random_state=42)\n    tree.fit(X_new,y)\n    y_pred = tree.predict(X_new)\n    r2 = metrics.r2_score(y, y_pred)\n    RMSE=np.sqrt(mean_squared_error(y,y_pred))\n    resultado.append(r2)\n    print(\"El escalado Utilizado--->\",x)\n    print(\"RMSE segun el tipo de estrategia:\",RMSE)\n    print(\"----------------------------------------\")","99473423":"X = dataset.drop(\"price\",axis=1).values\ny = dataset[\"price\"].values","ff201baf":"X_train,X_test,y_train,y_test= train_test_split(X,y,test_size=0.3,random_state=42)","8a396a75":"X_train.shape,X_test.shape","5747ef9e":"for x in [robust_sc,standard_sc,minmax_sc]:\n    scaler = x.fit(X_train)\n    X_train_new = x.transform(X_train)\n    X_test_new = x.transform(X_test)\n    \n    \n    tree = DecisionTreeRegressor(max_depth=10,random_state=42)\n    tree.fit(X_train_new,y_train)\n    \n    y_pred = tree.predict(X_test_new)\n    r2 = metrics.r2_score(y_test, y_pred)\n    RMSE=np.sqrt(mean_squared_error(y_test,y_pred))\n    resultado.append(r2)\n    print(\"El escalado utilizado--->\",x)\n    print(\"R2 segun el tipo de estrategia:\",r2)\n    print(\"RMSE segun el tipo de estrategia:\",RMSE)\n    print(\"----------------------------------------\")\n","809bd989":"minmax_sc = preprocessing.MinMaxScaler() \n\nX_train=minmax_sc.fit_transform(X_train)\nX_test=minmax_sc.transform(X_test)","685cf3e2":"tree = DecisionTreeRegressor(max_depth=10,random_state=42)\ntree.fit(X_train,y_train)\n    \ny_pred = tree.predict(X_test)\nr2 = metrics.r2_score(y_test, y_pred)\nRMSE=np.sqrt(mean_squared_error(y_test,y_pred))\n\n\nprint(\"R2 segun el tipo de estrategia:\",r2)\nprint(\"RMSE segun el tipo de estrategia:\",RMSE)\n\n","db91ec5e":"X_full_test =X_test.copy()\nX_full_train = X_train.copy()","299c356a":"pca_p=PCA\n\n\"\"\"for i in range(2,65):\n    pca_p=PCA(n_components=i,random_state=42)\n    X_train_new=pca_p.fit_transform(X_train)\n    X_test_new=pca_p.transform(X_test)\n    \n    tree = DecisionTreeRegressor(max_depth=10,random_state=42)\n    tree.fit(X_train_new,y_train)\n    \n    y_pred = tree.predict(X_test_new)\n    r2 = metrics.r2_score(y_test, y_pred)\n    RMSE=np.sqrt(mean_squared_error(y_test,y_pred))\n    print(\"----------------------------------------\")\n    print(\"modelo:\",i)\n    print(\"R2 segun el tipo de estrategia:\",r2)\n    print(\"RMSE segun el tipo de estrategia:\",RMSE)\n    print(\"----------------------------------------\")\"\"\"\n    ","65054273":"svd=TruncatedSVD\n\n\"\"\"for i in range(2,65):\n    svd=TruncatedSVD(n_components=i,random_state=42)\n    X_train_new=svd.fit_transform(X_train)\n    X_test_new=svd.transform(X_test)\n    \n    tree = DecisionTreeRegressor(max_depth=10,random_state=42)\n    tree.fit(X_train_new,y_train)\n    \n    y_pred = tree.predict(X_test_new)\n    r2 = metrics.r2_score(y_test, y_pred)\n    RMSE=np.sqrt(mean_squared_error(y_test,y_pred))\n    print(\"----------------------------------------\")\n    print(\"modelo:\",i)\n    print(\"R2 segun el tipo de estrategia:\",r2)\n    print(\"RMSE segun el tipo de estrategia:\",RMSE)\n    print(\"----------------------------------------\")\"\"\"","ca91c8a7":"pca_p=PCA(n_components=45,random_state=42)\nX_train=pca_p.fit_transform(X_train)\nX_test=pca_p.transform(X_test)\n\n\ntree = DecisionTreeRegressor(max_depth=10,random_state=42)\ntree.fit(X_train,y_train)\n    \ny_pred = tree.predict(X_test)\nr2 = metrics.r2_score(y_test, y_pred)\nRMSE=np.sqrt(mean_squared_error(y_test,y_pred))\nprint(\"----------------------------------------\")\nprint(\"R2 segun el tipo de estrategia:\",r2)\nprint(\"RMSE segun el tipo de estrategia:\",RMSE)\nprint(\"----------------------------------------\")","643a48c2":"X_train.shape,X_test.shape","805040c9":"pca = PCA(45).fit(X_test)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.title(\"Plot CEV para PCA\")\nplt.xlabel('Numero de componentes')\nplt.ylabel('cumulative explained variance');","159d5a32":"svd = TruncatedSVD(43).fit(X_test)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.title(\"Plot CEV para SVD\")\nplt.xlabel('Numero de componentes')\nplt.ylabel('cumulative explained variance');","33b5f73c":"MAX_DEPTH_range = range(1, 21)\nscores = []\narbol_r2_train_pred=[]\narbol_r2_test_pred=[]\n\nfor k in MAX_DEPTH_range:\n    new_arbol_regressor = DecisionTreeRegressor(max_depth=k,random_state=42)\n    new_arbol_regressor.fit(X_train, y_train)\n    scores.append(new_arbol_regressor.score(X_test, y_test))\n    \n    y_train_pred = new_arbol_regressor.predict(X_train)\n    arbol_r2_train = metrics.r2_score(y_train, y_train_pred)\n    arbol_r2_train_pred.append(arbol_r2_train)\n    \n    y_test_pred = new_arbol_regressor.predict(X_test)\n    arbol_r2_test = metrics.r2_score(y_test, y_test_pred)\n    arbol_r2_test_pred.append(arbol_r2_test)\n    \n\nfig = plt.figure(figsize=(15,4))\n\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\n\nax1.scatter(MAX_DEPTH_range,scores,marker=\"o\")\nax2.scatter(MAX_DEPTH_range,arbol_r2_train_pred,marker=\"o\",label=\"train\")\nax2.scatter(MAX_DEPTH_range,arbol_r2_test_pred,marker=\"o\",label=\"test\")\n\nax1.set_xticks(MAX_DEPTH_range)\nax2.set_xticks(MAX_DEPTH_range)\n\nax2.legend()\n\n\nax1.set_title('exactitud')\nax1.set_xlabel('max depth')\nax1.set_ylabel('exactitud')\n\nax2.set_title('r2')\nax2.set_xlabel('max depth')\nax2.set_ylabel('r2')\n\nplt.show()","451725b6":"arbol_proyecto1 = DecisionTreeRegressor(max_depth= 19,random_state=42)\narbol_proyecto1.fit(X_train,y_train)","d52add96":"arbol_proyecto2 = DecisionTreeRegressor(max_depth= 10,random_state=42)\narbol_proyecto2.fit(X_train,y_train)","04dd6fe8":"models = [\"Arbol Proyecto 1\",\"Arbol Proyecto 2\"]\n\nfor i, model in enumerate([arbol_proyecto1, arbol_proyecto2]):\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n   \n\n    print(f'Modelo: {models[i]}')\n    \n    r2_train= metrics.r2_score(y_train, y_train_pred)\n    r2_test= metrics.r2_score(y_test, y_test_pred)\n    \n    adj_r2_train = 1 - ((1 - r2_train) * (len(y_train) - 1)) \/ (len(y_train) - X_train.shape[1] - 1)\n    adj_r2_test = 1 - ((1 - r2_test) * (len(y_test) - 1)) \/ (len(y_test) - X_test.shape[1] - 1)\n\n    RMSE_train=np.sqrt(mean_squared_error(y_train,y_train_pred))\n    RMSE_test=np.sqrt(mean_squared_error(y_test,y_test_pred))\n\n    print(\"R2 en Train:\", r2_train)\n    print(\"R2 ajustado en train:\", adj_r2_train)\n    print(\"--------------------------\")\n    print(\"R2 en test:\", r2_test)\n    print(\"R2 ajustado en Test:\", adj_r2_test)\n    print(\"--------------------------\")\n    print(\"RMSE en train:\",RMSE_train)\n    print(\"RMSE en test:\",RMSE_test)\n\n    plt.figure(figsize = (14,4))\n   \n    ax = plt.subplot(1,2,2)\n    ax.scatter(y_test,y_test_pred, s =2)\n    \n    lims = [\n    np.min([ax.get_xlim(), ax.get_ylim()]),  # min of both axes\n    np.max([ax.get_xlim(), ax.get_ylim()]),  # max of both axes]\n    ]\n    \n    ax.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n    plt.xlabel('y (test)')\n    plt.ylabel('y_pred (test)')\n    \n    plt.tight_layout()\n    plt.show()","1e02aa62":"y_train.min,y_train.max","ffc60869":"y_test.min,y_test.max","c860a2eb":"forest_def = RandomForestRegressor(random_state=42)\n\nforest_def.fit(X_train,y_train)","ea8fe878":"y_train_pred_RFD = forest_def.predict(X_train)\ny_test_pred_RFD = forest_def.predict(X_test)\n\nrmse_train_RFD = np.sqrt(mean_squared_error(y_train, y_train_pred_RFD))\nrmse_test_RFD = np.sqrt(mean_squared_error(y_test, y_test_pred_RFD))\n\nprint(\"RMSE en Train es:\",rmse_train_RFD)\nprint(\"RMSE en Test es:\",rmse_test_RFD)","2af4365d":"forest = RandomForestRegressor(random_state=42)\nparams_forest = {\n    \"n_estimators\":[500],\n    \"criterion\":[\"mse\"],\n    \"max_depth\":[None],\n    \"max_features\":[\"sqrt\"],\n    \"bootstrap\":[False]\n}","681eb8dc":"model_RF = GridSearchCV(forest,param_grid=params_forest, cv=2,n_jobs=-1)\nmodel_RF.fit(X_train,y_train)","eaa91c1a":"print(\"Mejores parametros: \"+str(model_RF.best_params_))\nprint(\"Mejor Score: \"+str(model_RF.best_score_)+'\\n')\n\nscores = pd.DataFrame(model_RF.cv_results_)\nscores.sort_values(by=\"rank_test_score\")","bb604b75":"forest_gv=model_RF.predict(X_test)","40d21499":"Rf_Result=model_RF.score(X_test,y_test)","20a9f507":"y_train_pred_RF = model_RF.predict(X_train)\ny_test_pred_RF = model_RF.predict(X_test)\n\nrmse_train_RF = np.sqrt(mean_squared_error(y_train, y_train_pred_RF))\nrmse_test_RF = np.sqrt(mean_squared_error(y_test, y_test_pred_RF))\n\nprint(\"RMSE en Train es:\",rmse_train_RF)\nprint(\"RMSE en Test es:\",rmse_test_RF)","d12b5c93":"\"\"\"gbm = xgb.XGBRegressor(verbosity=1)\nparams_xgb = {\n        \"n_estimators\":[1000,1500],\n        \"learning_rate\":[0.1],\n        'max_depth': [500,1000],\n        'gpu_id': [0],\n        \"predictor\":[\"gpu_predictor\"],\n        'tree_method': ['gpu_hist'],\n        \"updater\":[\"grow_gpu_hist\"],\n        \"sampling_method\":[\"gradient_based\"],\n        \"updater\":[\"grow_gpu_hist\"]\n}\"\"\"","1c490298":"#model_xgb = GridSearchCV(gbm,param_grid=params_xgb, cv=2,n_jobs=-1)\n#model_xgb.fit(X_train,y_train)","293c0cdd":"#print(\"Mejores parametros: \"+str(model_xgb.best_params_))\n#print(\"Mejor Score: \"+str(model_xgb.best_score_)+'\\n')\n","f53ad671":"#scores = pd.DataFrame(model_xgb.cv_results_)\n#scores.sort_values(by=\"rank_test_score\")","48c93ef9":"#xgb_gv=model_xgb.predict(X_test)","c507aba1":"#xgb_Result=model_xgb.score(X_test,y_test)","fb70e44b":"#y_train_pred_xgb = model_xgb.predict(X_train)\n#y_test_pred_xgb = model_xgb.predict(X_test)\n\n#rmse_train_xgb = np.sqrt(mean_squared_error(y_train, y_train_pred_xgb))\n#rmse_test_xgb = np.sqrt(mean_squared_error(y_test, y_test_pred_xgb))\n\n#print(\"RMSE en Train es:\",rmse_train_xgb)\n#print(\"RMSE en Test es:\",rmse_test_xgb)","9e24ee55":"XGBfin=xgb.XGBRegressor(booster= 'gbtree', deterministic_histogram= True, gpu_id= 0,\n                        learning_rate= 0.1, max_depth= 500, n_estimators= 1000,\n                         predictor= 'gpu_predictor',\n                        sampling_method= 'gradient_based',\n                        tree_method= 'gpu_hist', updater= 'grow_gpu_hist',reg_lambda=1.2)","dff272cb":"XGBfin.fit(X_train,y_train)","cd4a92de":"y_train_pred_xgb = XGBfin.predict(X_train)\ny_test_pred_xgb = XGBfin.predict(X_test)\n\nrmse_train_xgb = np.sqrt(mean_squared_error(y_train, y_train_pred_xgb))\nrmse_test_xgb = np.sqrt(mean_squared_error(y_test, y_test_pred_xgb))\n\nprint(\"RMSE en Train es:\",rmse_train_xgb)\nprint(\"RMSE en Test es:\",rmse_test_xgb)","32fc20c8":"bagging = BaggingRegressor(random_state=42)\nparams_bagging={\n    \"n_estimators\":[10],\n    \"bootstrap\":[True],\n    \"warm_start\":[True],   \n}","fad38d8f":"model_bagging = GridSearchCV(bagging,param_grid=params_bagging, cv=2,n_jobs=-1)\nmodel_bagging.fit(X_train,y_train)","2531fa02":"print(\"Mejores parametros: \"+str(model_bagging.best_params_))\nprint(\"Mejor Score: \"+str(model_bagging.best_score_)+'\\n')\n\nscores = pd.DataFrame(model_bagging.cv_results_)\nscores.sort_values(by=\"rank_test_score\")","0d2bac82":"bagging_gv=model_bagging.predict(X_test)","90653615":"bagging_result=model_bagging.score(X_test,y_test)","65ef0a09":"y_train_pred_bagging = model_bagging.predict(X_train)\ny_test_pred_bagging = model_bagging.predict(X_test)\n\nrmse_train_bagging = np.sqrt(mean_squared_error(y_train, y_train_pred_bagging))\nrmse_test_bagging = np.sqrt(mean_squared_error(y_test, y_test_pred_bagging))\n\nprint(\"RMSE en Train es:\",rmse_train_bagging)\nprint(\"RMSE en Test es:\",rmse_test_bagging)","12e23ff4":"ada_boost=AdaBoostRegressor(random_state=42)\nparams_adaboost={\n    \"n_estimators\":[50],\n    \"learning_rate\":[0.1],\n    \"loss\":[\"linear\"]\n    \n}","43679f08":"model_ada = GridSearchCV(ada_boost,param_grid=params_adaboost, cv=2,n_jobs=-1)\nmodel_ada.fit(X_train,y_train)","53c222df":"print(\"Mejores parametros: \"+str(model_ada.best_params_))\nprint(\"Mejor Score: \"+str(model_ada.best_score_)+'\\n')\n\nscores = pd.DataFrame(model_ada.cv_results_)\nscores.sort_values(by=\"rank_test_score\")","8c7cf266":"ada_gv=model_ada.predict(X_test)","96697de7":"ada_result=model_ada.score(X_test,y_test)","da4cdd1a":"y_train_pred_ada = model_ada.predict(X_train)\ny_test_pred_ada = model_ada.predict(X_test)\n\nrmse_train_ada = np.sqrt(mean_squared_error(y_train, y_train_pred_ada))\nrmse_test_ada = np.sqrt(mean_squared_error(y_test, y_test_pred_ada))\n\nprint(\"RMSE en Train es:\",rmse_train_ada)\nprint(\"RMSE en Test es:\",rmse_test_ada)","4fa11717":"estimadores = [\n    (\"kNN\",KNeighborsRegressor()),\n    (\"DTR\",DecisionTreeRegressor()),\n    (\"LR\",LinearRegression())\n] ","5249b3bb":"stacking =StackingRegressor(estimators=estimadores)","e8b48ba3":"stacking.fit(X_train,y_train)","e4158409":"stack_gv=stacking.predict(X_test)","5264a983":"stack_result=stacking.score(X_test,y_test)\nstack_result","e26d9404":"y_train_pred_stack = stacking.predict(X_train)\ny_test_pred_stack = stacking.predict(X_test)\n\nrmse_train_stack = np.sqrt(mean_squared_error(y_train, y_train_pred_stack))\nrmse_test_stack = np.sqrt(mean_squared_error(y_test, y_test_pred_stack))\n\nprint(\"RMSE en Train es:\",rmse_train_stack)\nprint(\"RMSE en Test es:\",rmse_test_stack)","3c8c6834":"\"\"\"randomforestfin = RandomForestRegressor(random_state=42,bootstrap=False, criterion= \"mse\", max_depth= None, max_features= \"sqrt\", n_estimators= 500)\n\nXGBfin=xgb.XGBRegressor(booster= 'gbtree', deterministic_histogram= True, gpu_id= 0,\n                        learning_rate= 0.1, max_depth= 500, n_estimators= 1000,\n                         predictor= 'gpu_predictor',\n                        sampling_method= 'gradient_based',\n                        tree_method= 'gpu_hist', updater= 'grow_gpu_hist',reg_lambda=1.2)\n                        \nbaggingfin=BaggingRegressor(random_state=42,bootstrap= True, n_estimators= 10, warm_start= True)\n\nadafin=AdaBoostRegressor(random_state=42,learning_rate= 0.1, loss= 'linear', n_estimators= 50)\n\nstacking\n\"\"\"\n","c56c328f":"dataset_cluster.head()","1b08ddc4":"data_clus=dataset_cluster.copy()\ndata_clus.head()","e1e8d14e":"lat_lon = data_clus[[\"lat\",\"lon\"]]\nlat_lon.head()","ba3f9603":"K_clusters = range(1,10)\nkmeans = [KMeans(n_clusters=i) for i in K_clusters]\ny_lat = data_clus[['lat']]\nX_lon = data_clus[['lon']]\nscore = [kmeans[i].fit(y_lat).score(y_lat) for i in range(len(kmeans))]\n# Visualize\nplt.plot(K_clusters, score)\nplt.xlabel('Number of Clusters')\nplt.ylabel('Score')\nplt.title('Elbow Curve')\nplt.show()","10ea75f1":"kmeans = KMeans(n_clusters = 10, init ='k-means++')\nkmeans.fit(lat_lon[lat_lon.columns[0:2]]) # Compute k-means clustering.\nlat_lon['cluster_label'] = kmeans.fit_predict(lat_lon[lat_lon.columns[0:2]])\ncenters = kmeans.cluster_centers_ # Coordinates of cluster centers.\nlabels = kmeans.predict(lat_lon[lat_lon.columns[0:2]]) # Labels of each point\nlat_lon.head(10)","3a8d7f12":"lat_lon.plot.scatter(x = 'lat', y = 'lon', c=labels, s=50, cmap='viridis')\nplt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)","207b255f":"kmeans = KMeans(n_clusters = 10, init ='k-means++')\nkmeans.fit(data_clus[data_clus.columns[0:2]]) # Compute k-means clustering.\ndata_clus['clust_LatLon'] = kmeans.fit_predict(data_clus[data_clus.columns[0:2]])\ncenters = kmeans.cluster_centers_ # Coordinates of cluster centers.\nlabels = kmeans.predict(data_clus[data_clus.columns[0:2]]) # Labels of each point\ndata_clus.head(10)","a4e6d389":"Labelencoder= LabelEncoder()\nLabelencoder.fit(data_clus[\"l3\"])\ndata_clus[\"barrios\"] = Labelencoder.transform(data_clus[\"l3\"])\ndata_clus.drop(\"l3\",axis=1,inplace=True)","e1e7ecb4":"data_clus.head()","661851cf":"prop_Ty = data_clus[[\"rooms\",\"bedrooms\",\"bathrooms\",\"surface_total\",\"surface_covered\",\"Tipo_propiedad\",\"barrios\",\"a\u00f1os\"]]\nprop_Ty.head()","6f6e6eb3":"K_clusters = range(1,10)\nkmeans = [KMeans(n_clusters=i) for i in K_clusters]\ny_lat = data_clus[['rooms']]\nX_lon = data_clus[['surface_total']]\nscore = [kmeans[i].fit(y_lat).score(y_lat) for i in range(len(kmeans))]\n# Visualize\nplt.plot(K_clusters, score)\nplt.xlabel('Number of Clusters')\nplt.ylabel('Score')\nplt.title('Elbow Curve')\nplt.show()","75d57209":"kmeans = KMeans(n_clusters = 10, init ='k-means++')\nkmeans.fit(prop_Ty[prop_Ty.columns[0:8]]) # Compute k-means clustering.\ndata_clus['clust_prop_Ty'] = kmeans.fit_predict(prop_Ty[prop_Ty.columns[0:8]])\ncenters = kmeans.cluster_centers_ # Coordinates of cluster centers.\nlabels = kmeans.predict(prop_Ty[prop_Ty.columns[0:8]]) # Labels of each point\ndata_clus.head(10)","729b3d09":"fig =plt.subplots(figsize=(8,8))\nsns.heatmap(data_clus.corr(), annot=True)","924c50b6":"data_clus.drop([\"lat\",\"lon\"],axis=1,inplace=True)","4e124b88":"fig =plt.subplots(figsize=(8,8))\nsns.heatmap(data_clus.corr(), annot=True)","0ba371fb":"X_opc=data_clus.drop(\"price\",axis=1)\ny_opc=data_clus[\"price\"]","ddf2cf2d":"for x in [robust_sc,standard_sc,minmax_sc]:\n    scaler = x.fit(X_opc)\n    X_new = x.transform(X_opc)\n    tree = DecisionTreeRegressor(max_depth=10,random_state=42)\n    tree.fit(X_new,y_opc)\n    y_pred = tree.predict(X_new)\n    r2 = metrics.r2_score(y_opc, y_pred)\n    RMSE=np.sqrt(mean_squared_error(y_opc,y_pred))\n    resultado.append(r2)\n    print(\"El escalado Utilizado--->\",x)\n    print(\"RMSE segun el tipo de estrategia:\",RMSE)\n    print(\"----------------------------------------\")","a769bd41":"X_tr_opc,X_te_opc,y_tr_opc,y_te_opc=train_test_split(X_opc,y_opc,test_size=0.3,random_state=42)","58e138ae":"for x in [robust_sc,standard_sc,minmax_sc]:\n    scaler = x.fit(X_tr_opc)\n    X_train_new = x.transform(X_tr_opc)\n    X_test_new = x.transform(X_te_opc)\n    \n    \n    tree = DecisionTreeRegressor(max_depth=10,random_state=42)\n    tree.fit(X_train_new,y_tr_opc)\n    \n    y_pred = tree.predict(X_test_new)\n    r2 = metrics.r2_score(y_te_opc, y_pred)\n    RMSE=np.sqrt(mean_squared_error(y_te_opc,y_pred))\n    resultado.append(r2)\n    print(\"El escalado utilizado--->\",x)\n    print(\"R2 segun el tipo de estrategia:\",r2)\n    print(\"RMSE segun el tipo de estrategia:\",RMSE)\n    print(\"----------------------------------------\")\n","92bb11e8":"stand_Scal = preprocessing.StandardScaler() \n\nX_tr_opc=stand_Scal.fit_transform(X_tr_opc)\nX_te_opc=stand_Scal.transform(X_te_opc)\n","5b0c909c":"pca_p=PCA\n\nfor i in range(2,11):\n    pca_p=PCA(n_components=i,random_state=42)\n    X_tr_new=pca_p.fit_transform(X_tr_opc)\n    X_te_new=pca_p.transform(X_te_opc)\n    \n    tree = DecisionTreeRegressor(max_depth=10,random_state=42)\n    tree.fit(X_tr_new,y_tr_opc)\n    \n    y_pred = tree.predict(X_te_new)\n    r2 = metrics.r2_score(y_te_opc, y_pred)\n    RMSE=np.sqrt(mean_squared_error(y_te_opc,y_pred))\n    print(\"----------------------------------------\")\n    print(\"modelo:\",i)\n    print(\"R2 segun el tipo de estrategia:\",r2)\n    print(\"RMSE segun el tipo de estrategia:\",RMSE)\n    print(\"----------------------------------------\")\n    ","08b674ee":"pca = PCA(6).fit(X_te_opc)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.title(\"Plot CEV para PCA\")\nplt.xlabel('Numero de componentes')\nplt.ylabel('cumulative explained variance');","7ecb7c8f":"svd=TruncatedSVD\n\nfor i in range(2,10):\n    svd=TruncatedSVD(n_components=i,random_state=42)\n    X_tr_new=svd.fit_transform(X_tr_opc)\n    X_te_new=svd.transform(X_te_opc)\n    \n    tree = DecisionTreeRegressor(max_depth=10,random_state=42)\n    tree.fit(X_tr_new,y_tr_opc)\n    \n    y_pred = tree.predict(X_te_new)\n    r2 = metrics.r2_score(y_te_opc, y_pred)\n    RMSE=np.sqrt(mean_squared_error(y_te_opc,y_pred))\n    print(\"----------------------------------------\")\n    print(\"modelo:\",i)\n    print(\"R2 segun el tipo de estrategia:\",r2)\n    print(\"RMSE segun el tipo de estrategia:\",RMSE)\n    print(\"----------------------------------------\")\n    ","4899b450":"svd = TruncatedSVD(7).fit(X_te_opc)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.title(\"Plot CEV para SVD\")\nplt.xlabel('Numero de componentes')\nplt.ylabel('cumulative explained variance');","6b7e2112":"pca_p=PCA(n_components=6,random_state=42)\nX_tr_opc=pca_p.fit_transform(X_tr_opc)\nX_te_opc=pca_p.transform(X_te_opc)\n\n\ntree = DecisionTreeRegressor(max_depth=10,random_state=42)\ntree.fit(X_tr_opc,y_tr_opc)\n    \ny_pred = tree.predict(X_te_opc)\nr2 = metrics.r2_score(y_te_opc, y_pred)\nRMSE=np.sqrt(mean_squared_error(y_te_opc,y_pred))\nprint(\"----------------------------------------\")\nprint(\"R2 segun el tipo de estrategia:\",r2)\nprint(\"RMSE segun el tipo de estrategia:\",RMSE)\nprint(\"----------------------------------------\")","5fb2eda4":"randomforestfinal = RandomForestRegressor(random_state=42,bootstrap=False, criterion= \"mse\", max_depth= None, max_features= \"sqrt\", n_estimators= 1000)\nrandomforestfinal.fit(X_tr_opc,y_train)","43c1edb1":"y_train_pred_RF = randomforestfinal.predict(X_tr_opc)\ny_test_pred_RF = randomforestfinal.predict(X_te_opc)\n\nrmse_train_RF = np.sqrt(mean_squared_error(y_tr_opc, y_train_pred_RF))\nrmse_test_RF = np.sqrt(mean_squared_error(y_te_opc, y_test_pred_RF))\n\nprint(\"RMSE en Train es:\",rmse_train_RF)\nprint(\"RMSE en Test es:\",rmse_test_RF)","3756bf19":"XGBfin=xgb.XGBRegressor(booster= 'gbtree', deterministic_histogram= True, gpu_id= 0,\n                        learning_rate= 0.1, max_depth= 500, n_estimators= 1000,\n                         predictor= 'gpu_predictor',\n                        sampling_method= 'gradient_based',\n                        tree_method= 'gpu_hist', updater= 'grow_gpu_hist',reg_lambda=1.2)","68deca84":"XGBfin.fit(X_tr_opc,y_train)","3512cf56":"y_train_pred_xgb = XGBfin.predict(X_tr_opc)\ny_test_pred_xgb = XGBfin.predict(X_te_opc)\n\nrmse_train_xgb = np.sqrt(mean_squared_error(y_tr_opc, y_train_pred_xgb))\nrmse_test_xgb = np.sqrt(mean_squared_error(y_te_opc, y_test_pred_xgb))\n\nprint(\"RMSE en Train es:\",rmse_train_xgb)\nprint(\"RMSE en Test es:\",rmse_test_xgb)","3bfc6f11":"X_opc2 =data_clus[[\"clust_LatLon\",\"clust_prop_Ty\"]]\ny_opc2=data_clus[\"price\"]","31a4bc39":"X_tr_2,X_te_2,y_tr_2,y_te_2=train_test_split(X_opc2,y_opc2,test_size=0.3,random_state=42)","17aad14d":"stand_Scal = preprocessing.StandardScaler() \n\nX_tr_2=stand_Scal.fit_transform(X_tr_2)\nX_te_2=stand_Scal.transform(X_te_2)\n","766988ca":"randomforestfinal = RandomForestRegressor(random_state=42,bootstrap=False, criterion= \"mse\", max_depth= None, max_features= \"sqrt\", n_estimators= 1000)\nrandomforestfinal.fit(X_tr_2,y_train)","72402650":"y_train_pred_RF = randomforestfinal.predict(X_tr_2)\ny_test_pred_RF = randomforestfinal.predict(X_te_2)\n\nrmse_train_RF = np.sqrt(mean_squared_error(y_tr_2, y_train_pred_RF))\nrmse_test_RF = np.sqrt(mean_squared_error(y_te_2, y_test_pred_RF))\n\nprint(\"RMSE en Train es:\",rmse_train_RF)\nprint(\"RMSE en Test es:\",rmse_test_RF)","10c9b4b7":"XGBfin=xgb.XGBRegressor(booster= 'gbtree', deterministic_histogram= True, gpu_id= 0,\n                        learning_rate= 0.1, max_depth= 500, n_estimators= 1000,\n                         predictor= 'gpu_predictor',\n                        sampling_method= 'gradient_based',\n                        tree_method= 'gpu_hist', updater= 'grow_gpu_hist',reg_lambda=1.2)","e26cf2f9":"XGBfin.fit(X_tr_2,y_train)","bed7f105":"y_train_pred_xgb = XGBfin.predict(X_tr_2)\ny_test_pred_xgb = XGBfin.predict(X_te_2)\n\nrmse_train_xgb = np.sqrt(mean_squared_error(y_tr_2, y_train_pred_xgb))\nrmse_test_xgb = np.sqrt(mean_squared_error(y_te_2, y_test_pred_xgb))\n\nprint(\"RMSE en Train es:\",rmse_train_xgb)\nprint(\"RMSE en Test es:\",rmse_test_xgb)","5bcd79d6":"minimo = dataset[\"price\"].min()\nmaximo = dataset[\"price\"].max()\nprint(\"El precio minimo es:\",minimo)\nprint(\"El precio maximo es:\",maximo)\nval = maximo-minimo\nprint(\"La diferencia es:\",val)","f9506876":"X_full_test.shape,X_full_train.shape","161ed5af":"randomforestfinal = RandomForestRegressor(random_state=42,bootstrap=False, criterion= \"mse\", max_depth= None, max_features= \"sqrt\", n_estimators= 1000)\nrandomforestfinal.fit(X_full_train,y_train)","1285541a":"y_train_pred_RF = randomforestfinal.predict(X_full_train)\ny_test_pred_RF = randomforestfinal.predict(X_full_test)\n\nrmse_train_RF = np.sqrt(mean_squared_error(y_train, y_train_pred_RF))\nrmse_test_RF = np.sqrt(mean_squared_error(y_test, y_test_pred_RF))\n\nprint(\"RMSE en Train es:\",rmse_train_RF)\nprint(\"RMSE en Test es:\",rmse_test_RF)","95720527":"len(randomforestfinal.estimators_)","d42f976b":"randomforestplot = RandomForestRegressor(random_state=42,bootstrap=False, criterion= \"mse\", max_depth= 3, max_features= \"sqrt\", n_estimators= 1000)\nrandomforestplot.fit(X_full_train,y_train)","f8b86e92":"arbol_5 = randomforestplot.estimators_[5]\n\n\nwith plt.style.context('grayscale'):\n    fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=800)\n    Arbol.plot_tree(arbol_5,\n                   feature_names = dataset.drop(\"price\",axis=1).columns, \n                   filled = True);\n    fig.savefig('rf_individualtree.png')","92087f4b":"arbol_700 = randomforestplot.estimators_[700]\n\n\nwith plt.style.context('grayscale'):\n    fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=800)\n    Arbol.plot_tree(arbol_700,\n                   feature_names = dataset.drop(\"price\",axis=1).columns, \n                   filled = True);\n    fig.savefig('rf_individualtree.png')","6e0572dc":"arbol_458 = randomforestplot.estimators_[458]\n\n\nwith plt.style.context('grayscale'):\n    fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=800)\n    Arbol.plot_tree(arbol_458,\n                   feature_names = dataset.drop(\"price\",axis=1).columns, \n                   filled = True);\n    fig.savefig('rf_individualtree.png')","5e3d7a7b":"importancia = randomforestfinal.feature_importances_\n\nfor i,v in enumerate(importancia):\n    \n    plt.bar([x for x in range(len(importancia))], importancia)\nplt.title(\"importancia de Features\")\nplt.ylabel(\"values\")\nplt.xlabel(\"Features\")\nplt.show()","85f97ff4":"XGBfin=xgb.XGBRegressor(booster= 'gbtree', deterministic_histogram= True, gpu_id= 0,\n                        learning_rate= 0.1, max_depth= 500, n_estimators= 1000,\n                         predictor= 'gpu_predictor',\n                        sampling_method= 'gradient_based',\n                        tree_method= 'gpu_hist', updater= 'grow_gpu_hist',reg_lambda=1.2)","529419b3":"XGBfin.fit(X_full_train,y_train)","00a8d5cd":"y_train_pred_xgb = XGBfin.predict(X_full_train)\ny_test_pred_xgb = XGBfin.predict(X_full_test)\n\nrmse_train_xgb = np.sqrt(mean_squared_error(y_train, y_train_pred_xgb))\nrmse_test_xgb = np.sqrt(mean_squared_error(y_test, y_test_pred_xgb))\n\nprint(\"RMSE en Train es:\",rmse_train_xgb)\nprint(\"RMSE en Test es:\",rmse_test_xgb)\n","ac969448":"plot_importance(XGBfinal,max_num_features=10)\nplt.show()","246893fd":"### Por cuestiones de practicidad solo se evaluara RandomForest y XGB","c820814a":"## Punto especifico de Parte A: Eliminacion y Deteccion de Outliers","91df8230":"#### Rooms","f30c18c2":"Debido a que todas las pruebas se realizaron utilizando PCA y clustering, se dificulta responder las preguntas de la parte C. Por ello ahora se probara un modelo con todas las variables sin la utilizacion de componentes principales.","731abf7c":"## <font color=greenyellow>Desafio Opcional","3f6f5b8c":"#### Surface_Covered","0bc6451f":"#### <font color=greenyellow>StackingRegressor","931bf053":"### Sugerencias para desarrollar el proyecto:\n\nEste proyecto no cuenta con m\u00ednimos entregables indicados en la consigna, pero ten en cuenta lo siguiente:\n\nen la Parte A debes implementar al menos tres de las transformaciones de datos propuestas.\n\nen la Parte B, al menos un modelo debe ser optimizado por Grid Search o Random Search; el otro puede ser optimizado por b\u00fasqueda manual (es decir, puedes dejar los mejores par\u00e1metros que encontraste probando ).\n\nen la Parte C, debes responder al menos una pregunta. Obviamente, \u00a1cuanto m\u00e1s hagas, m\u00e1s aprender\u00e1s y mejor ser\u00e1 tu proyecto!\n","6ce1fa1a":"#### <font color=greenyellow>A continuacion se presenta un analisis comparativo:\n\nAcademicamente entenderemos a train como \"parcial\" y a test como \"diferencia\", con esta idea en mente vemos que en Random Forest al tener un train de 5546 y un test de 27151, nos da un diferencial de \"21.605\" (27151-5546), como dijimos en lineas anteriores estamos ante alta varianza o un caso de sobreajuste.\n\nAl momento de ver este analisis en XGBoost, vemos un diferencial de 13.784 (28047-14263), puede parecer un modelo mas balanceado, pero en realidad es un modelo que sufre de alto sesgo y alta varianza.\n\nEstos valores deben ser analizados en contexto, quizas un RMSE en train de 5546 sea el numero optimo o quizas sea un valor muy alto para determinar precios, esta pregunta solo puede ser respondida teniendo un real conocimiento en la materia que nos ata\u00f1e. Dado el caso de que este fuese un valor optimo nos encontramos ante un \"sesgo inevitable\", un sesgo que por mas optimizado que este el modelo, sera imposible de reducir. Pero que nos servira de base para poder identificar al \"sesgo evitable\" al calcular la diferencia entre el error de train y test.\n\nLa situacion es diferente al trabajar la varianza, aqui no estamos ante una \"varianza inevitable\", siempre sera posible reducirla totalmente, pero a costas de afectar el sesgo.\n\nteniendo estos conceptos en mente se presenta la situacion de cual es el error optimo o numero magico para el modelo. Si esta fuese una funcion que las personas ejecutaran con gran calidad, el error humano seria un error optimo. El caso del proyecto no presenta una situacion donde los humanos deberian ser tomados como base de error optimo, ante esta circunstancia solo podemos reducir el error teniendo en cuenta las circunstancias mencionadas o tener conocimientos especificos que nos permitan concluir en cual es el error optimo.\n","d93741f8":"<font color=greenyellow>Modelo de Arbol de decision con Optimizacion:\n\n    *R2 en Train: 0.952\n    *R2 ajustado en train: 0.952\n\n    *R2 en test: 0.766\n    *R2 ajustado en Test: 0.766\n\n    *RMSE en train: 64721.069\n    *RMSE en test: 146092.758","4062284d":"### Codo en 3","b76a73c5":"Segunda pregunta:\u00bfC\u00f3mo es la distribuci\u00f3n de errores (regresi\u00f3n) o qu\u00e9 clases se confunden entre s\u00ed (clasificaci\u00f3n)? \u00bfD\u00f3nde falla? \u00bfA qu\u00e9 se debe?","93c36f67":"#### Surface_Total","cbef840a":"## <font color=greenyellow>Parte B en el desafio Opcional","bf542ed2":"##### Tip: ajuste del surface total y covered","439a6594":"### Aplicacion del KNNImputer en dataset","7f6fb575":"### Plot CEV para SVD","60be620a":"### <font color=greenyellow>Random Forest (GANADOR!!)","5b9a7396":"##  <font color=greenyellow>Consigna\nEn este proyecto profundizar\u00e1s lo desarrollado en el proyecto 01 (\u201cPrimer modelo de Machine Learning\u201d). El objetivo es aplicar las t\u00e9cnicas incorporadas (Transformaci\u00f3n de Datos, Optimizaci\u00f3n de Hiperpar\u00e1metros, Modelos Avanzados, etc.) para generar un modelo que tenga un mejor desempe\u00f1o que el modelo generado en el proyecto anterior. Luego, interpreta ese modelo para responder la siguiente pregunta: \u00bfqu\u00e9 podemos aprender de nuestro problema estudiando el modelo que generamos?\nEl trabajo se organiza en tres partes:\n\n\n <font color=greenyellow>PARTE A - Transformaci\u00f3n de Datos:\nElige cu\u00e1les de las siguientes tareas son apropiadas para su dataset. Implementa las transformaciones que elegiste. Es importante que justifiques por qu\u00e9 las haces:\n1.\tDetecci\u00f3n y eliminaci\u00f3n de Outliers\n2.\tEncoding\n3.\tImputaci\u00f3n de valores faltantes\n4.\tEscalado de datos\n5.\tGeneraci\u00f3n de nuevas variables predictoras\/reducci\u00f3n de dimensionalidad (SVD\/PCA).\nVuelve a entrenar el modelo implementado en la Entrega 01 - en particular, el \u00e1rbol de decisi\u00f3n - con este nuevo dataset transformado . Eval\u00faa su desempe\u00f1o a partir del dataset obtenido luego de transformar los datos. \u00bfHay una mejora en su desempe\u00f1o? Compara con el desempe\u00f1o obtenido en el proyecto 01. Sea cual sea la respuesta, intenta explicar a qu\u00e9 se debe.\n\n\n <font color=greenyellow>PARTE B - Modelos Avanzados:\n\n6.\tElige dos de los modelos avanzados vistos Compara con el desempe\u00f1o obtenido en el proyecto 01 (en el caso de regresi\u00f3n, considera una regresi\u00f3n lineal con atributos polin\u00f3micos y regularizaci\u00f3n). Entr\u00e9nalos y eval\u00faalos con sus argumentos por defecto. No te olvides de hacer un train\/test split y usar Validaci\u00f3n Cruzada.\n7.\tOptimiza sus hiperpar\u00e1metros mediante Validaci\u00f3n Cruzada y Grid Search o Random Search.\n8.\tCompara el desempe\u00f1o de los nuevos modelos entre s\u00ed y con el modelo de la Parte A. \u00bfCu\u00e1l elegir\u00edas? Justifica.\n\n\n <font color=greenyellow>PARTE C - Interpretaci\u00f3n de modelos:\nDe acuerdo a lo que el modelo permite, responde algunas o todas las siguientes preguntas:\n\n9.\t\u00bfQu\u00e9 variables fueron relevantes para el modelo para hacer una predicci\u00f3n? \u00bfCu\u00e1les no? Si usaste una regresi\u00f3n lineal con regularizaci\u00f3n, presta atenci\u00f3n a los par\u00e1metros (pendientes) obtenidas. Si usaste un modelo de ensamble en \u00e1rboles, adem\u00e1s de ver la importancia de cada atributo, tambi\u00e9n elige algunos \u00e1rboles al azar y observa qu\u00e9 atributos considera importantes. \u00bfEn qu\u00e9 se diferencian esos \u00e1rboles? \u00bfPor qu\u00e9? Finalmente, responde, \u00bfcoincide con lo que esperabas a partir de tu experiencia con este dataset?\n10.\t\u00bfC\u00f3mo es la distribuci\u00f3n de errores (regresi\u00f3n) o qu\u00e9 clases se confunden entre s\u00ed (clasificaci\u00f3n)? \u00bfD\u00f3nde falla? \u00bfA qu\u00e9 se debe?\n\n","15154140":"#### Los metodos ascending y roman logran un empate en cuanto al error, pero no logran superar KNNimputer con 6 vecinos. A pesar de esto vale la pena destacar que fue el metodo mas rapido.","0172c059":"## Identificacion de columnas con un solo valor","ca43d061":"### Alcanzado el checkpoint","3ff03b90":"### Features mas importantes:\n\n    +F0:lat\n\n    +F1:Lon\n\n    +F5:Surface_total\n\n    +F6:surface_covered\n\n    +F2:Rooms","5aba187e":"#### Bedrooms","154358d5":"#### Precios","4abaa454":"notas: La transformacion de datos trabajada en este dataset, busco lograr la mayor simpliicidad y entendimiento, aun asi se decidio aplicar todos los puntos indicados en la parte A, no solo porque otorgaban un mejor rendimiento, sino que tambien era una forma de probar mis habilidades para el analisis de datos.\n\nTodas el tratamiento respeto los datos y cuido de que no se vuelvan muy artificiales. Es importante notar que los Monoambientes son una categoria de propiedad que en este dataset se mezcla con los departamentos.\n\nDurante este proyecto me tome el atrevimiento de incluir ciertos tratamientos de datos que no fueron requeridos pero que considere que serian de utilidad, el resultado asi lo demuestra.","0e7b1db6":"### Parametros Finales ","9f5689cc":"### Precios","976a37f0":"##### <font color=greenyellow>Analisis de errores:\n\n<font color=greenyellow>Random Forest nos pone ante estos resultados:\n\n    RMSE en Train es: 5546.829574670481\n    RMSE en Test es: 27151.648215062414\n\nUn analisis sobre estos valores, y sobretodo en train, nos permite determinar que estamos ante un caso de sesgo bajo(esto sera desarrollado en futuras lineas). Pero el punto realmente importante aparece al momento de ver el RMSE de test, aqui se hace presente que Random Forest sufre de varianza. Una solucion simple seria agregar mas datos para reducir la varianza, esto a costa de aumentar el sesgo del modelo (caso de compensacion entre sesgo y varianza).\n\nLas posibles opciones para reducir la varianza:\n\n    Agregar datos al momento de entrenar\n    utilizar regularizacion\n    Disminuir la complejidad del modelo\n    creacion de variables sinteticas\n    \n    \n<font color=greenyellow>XGBoost plantea estos resultados:\n\n    RMSE en Train es: 14263.824628566206\n    RMSE en Test es: 28047.999659796813\n    \nA simple vista los valores de train y test son mas \"cercanos\" o su diferencia es menor, aquello nos indica que es un modelo mas balanceado, a pesar de sufrir un sesgo mayor.\n\nLas soluciones para reducir la varianza:\n\n    Agregar datos al momento de entrenar\n    utilizar regularizacion\n    Disminuir la complejidad del modelo\n    creacion de variables sinteticas\n    \nLas soluciones para reducir el sesgo:\n\n    Aumentar el tama\u00f1o del modelo\n    Creacion de variables sinteticas\n    Reducir o eliminar la regularizacion","1582ba5b":"###  A continuacion se creara una copia de X_train y X_test antes de ser sometido a PCA, para poder ser utilizado en la parte C de este proyecto","21483071":"## <font color=greenyellow>Clustering segun tipo de propiedad","b4116a63":"### Teoria 1 Imputacion Estadistica","9675b25e":"### Aplicacion de MinMaxScaler","be1206ed":"##  <font color=greenyellow>Entregable\nUn Notebook de Jupyter con la resoluci\u00f3n de la consigna. El Notebook debe poder ejecutarse sin errores.\nReferencias\nAp\u00f3yate en las bit\u00e1coras, los notebooks trabajados y las presentaciones vistas en clase para resolver tu proyecto. Tambi\u00e9n ser\u00e1 de mucha utilidad la documentaci\u00f3n de las librer\u00edas de Python. No dudes en consultar comunidades online como Stack Overflow y, por supuesto, buscar en la web ( googlear).\n","488fcc5f":"#### XGBoost","2c8b813b":"### Debido a que los arboles no tienen una profundiad limitada, el plot se vuelve de poco valor, ya que no se puede leer la informacion contenido, por ello se reducira Max_Depth a los fines didacticos","9cf250bf":"## <font color=greenyellow>Benchmark: ->RMSE en test: 42970.524<-","047eedfd":"## Punto especifico de Parte A: Encoding","e7d05481":"#### <font color=greenyellow>XGBoost","7bedc157":"## Punto especifico de Parte A: Imputacion de Valores Faltantes","bb6862a3":"## <font color=greenyellow>Escalado de datos con StandardScaler","60acbec8":"## <font color=greenyellow>Checklist de Evaluacion","946dbd2d":"## <font color=greenyellow>PCA","b876e45d":"## <font color=greenyellow>PARTE B - Modelos Avanzados\nEn la optimizaci\u00f3n de hiperpar\u00e1metros, debes justificar los par\u00e1metros que elegiste para optimizar y el rango de cada uno.\n","3079b0a0":"# Mejores Resultados","e4e30f20":"## <font color=greenyellow>Aplicacion de Standard Scaler","514fea1b":"## Resultados obtenidos:\n\n## <font color=greenyellow>Random Forest-    RMSE en Test es: 29441.251                                    <font color=greenyellow>(Ganador con PCA)\n","27334ad0":"#### <font color=greenyellow>Random Forest con Gridsearch","570c4c1c":"## Reentreno del mejor modelo utilizado en el Proyecto 1","4b1993e4":"Random Forest:\n\nPara el Random Forest las variables importantes dueron:\n\n    Surface_Total\n    Surface_Covered\n    Lat\n    Bathrooms\n    Rooms\n    \nLa aparicion de latitud como una variable importante es algo que no esperaba, ya que no tuvo ningun tratamiento como separacion de latitudes en bins segun barrios o algun tipo de analisis en general. En cuanto a las otras variables, si eran esperables, las superficies demuestran ser una gran diferencia.\n\nEn cuanto a los arboles analizados (5, 700 y 458), nos muestran que las variables mencionadas lideran los arboles, quizas sorprenda algo la mencion de los barrios en los nodos, pero dado la cantidad de arboles es normal su aparicion.","ea101049":"#### <font color=greenyellow>BaggingRegressor","54b6fa93":"## Parte B","966a3779":"#### Nota importante: los gridsearch fueron reducidos dejando los valores que mejores resultados lograron, esto fue realizado para no producir demoras en la ejecucion completa del proyecto.","cb46b78e":"## PARTE A:","09bae487":"## Eliminacion de Filas Duplicadas","23644de9":"### <font color=greenyellow>XGBoost","44aca379":"## ---------------------------","607fea88":"### Teoria 3 Imputacion Iterativa","b1d4412b":"Primera pregunta:\n\u00bfQu\u00e9 variables fueron relevantes para el modelo para hacer una predicci\u00f3n? Si usaste un modelo de ensamble en \u00e1rboles, adem\u00e1s de ver la importancia de cada atributo, tambi\u00e9n elige algunos \u00e1rboles al azar y observa qu\u00e9 atributos considera importantes. \u00bfEn qu\u00e9 se diferencian esos \u00e1rboles? \u00bfPor qu\u00e9? Finalmente, responde, \u00bfcoincide con lo que esperabas a partir de tu experiencia con este dataset?","8ebccb8b":"#### El uso de constant es el que mejores resultados otorga, aunque no es una diferencia tan elevada con respecto a los otros tipos de estrategia.","7281e921":"## Metodo IQR","2d5d600b":"## Casa = 0, Dpto = 1 y PH = 2","497e3eed":"# Resultado: \n\n\n## RMSE en Train es: 13919.758111427444\n## RMSE en Test es: 34288.677026731246","453363e4":"## Punto Opcional de Parte A: PCA o STD","5a99d35e":"### <font color=greenyellow>XGBoost","f789ad9d":"### Escalado sin tratar Y","be78cb98":"#### La columna 14 o Property Type tiene tan poco porcentaje debido a que solo estamos trabajando con 3 tipos de propiedades","d0a2c02f":"##### Vemos que los valores otorgados no son enteros, por ende usaremos el floor, para reducirlos al entero mas cercano. Se utilizo el floor, el numero 5 fue alcanzado.","369eff93":"Una de las soluciones posibles es la incorporacion de variables. En este punto no me refiero a variables polinomicas sino a variables como:\n\n    Tiene pileta o no\n    Tiene patio o no\n    Tiene terraza o no\n    Tiene seguridad privada o no\n    Esta en un barrio privado o no\n    Esta cerca de escuelas, hospitales\n    Esta cerca de centro de ocio\n    \nQuizas todos estos elementos esten en las variables que no se pueden analizar por no saber NLP al momento de desarrollar este proyecto.","a60467a8":"### <font color=greenyellow>A continuacion solo se trabajara con las 2 variables creadas con clustering y se veran los resultados logrados","40cba358":"#### Nota: El knn con mejor rendimiento se obtiene con 6 vecinos, debido a la demora de ejecutar estas lineas, solo se dejara el numero 6.","4b9e7834":"### Teoria 2 Imputacion con KNN","0b442110":"## Para el desafio opcional","8408c0f9":"#### La cantidad de vecinos en 6 otorga los mejores resultados y logra superar al Constant del SimpleImputer.","8f349aea":"### Parametros optimizados del proyecto 2","f2655b50":"## Identificacion de columnas con pocos valores","85e64c1a":"## PARTE C:","4fefdc4d":"#### Nota: Aunque el modelo presenta peores resultados en Train, vemos que Test obtuvo mejoras, logrando un gran avance con respecto al modelo optimizado del proyecto 1 al aplicar en el nuevo dataset del proyecto 2. Esto nos deja en condiciones de establecer estos resultados como el nuevo Benchmark del Proyecto 2.","daf5f5ad":"#### Consideramos como duplicados a las filas que tienen los mismos valores para \"cada columna\" en el mismo orden, la eliminacion de estos duplicados mejora  mejora el rendimiento de los modelos predictivos.","2be716d1":"### Plot CEV para PCA","8e0894e0":"#### Breve analisis de los datos presentes (es el mismo analisis visto en el proyecto 1).","ba8cb3df":"## Identificacion de Filas con datos duplicados","41b9699d":"#### <font color=greenyellow>Random Forest default","d084de1d":"#### <font color=greenyellow>AdaBoostRegressor","a151a259":"### A continuacion se introduce una forma de visualizar los datos mas interactiva","68d649d6":"Nota: No se puede dudar de las mejoras obtenidas, el cambio del RMSE es impresionante, al iniciar este proyecto no espere ese nivel de mejora, por lo que realmente me soprendio. El unico detalle es el overfit residual que tenemos, que varia segun el modelo utilizado, pero aun asi existe, esto significa que los datos aun pueden mejorarse y que el modelo esta memorizando demasiado en Train.","c2b6a160":"##  <font color=greenyellow>Resumen del proyecto\nAplica transformaci\u00f3n de datos y entrena Modelos Avanzados para desarrollar con mayor profundidad tu modelo de Machine Learning. \u00bfQu\u00e9 puedes aprender del problema que est\u00e1s abordando mediante el estudio de tu propio modelo?\nLa realizaci\u00f3n y entrega del Proyecto es individual.\n","91931c7d":"#### Random Forest","bee64778":"## <font color=greenyellow>Aplicacion del PCA con 6","82aa2bf9":"#### Las columnas: L1, L2, Currency y Operation Type son columnas de valores unicos, estas pueden generar errores en los modelos de prediccion ya que son pedictores de Cero varianza. por lo siguiente seran eliminadas.","833a26bf":"## <font color=greenyellow>PARTE A - Transformaci\u00f3n de Datos\n\nDebes justificar por qu\u00e9 cre\u00e9s que las transformaciones elegidas aplican en este dataset. Ten en cuenta que, en el manejo de valores at\u00edpicos o en la imputaci\u00f3n de valores faltantes, los valores obtenidos deben tener sentido. Por ejemplo, valores mayores que cero para superficies, n\u00famero de ba\u00f1os, etc.\n\nDebes re entrenar un modelo del Proyecto 01 y comparar su desempe\u00f1o con el modelo obtenido en el Proyecto 01. Una aclaraci\u00f3n: con reentrenar nos referimos a usar el mismo proceso de entrenamiento junto con sus hiperpar\u00e1metros. Pero puede ocurrir - y, de hecho, se espera - que el dataset contenga m\u00e1s atributos que los utilizados en el Proyecto 01.\n","0353cd04":"### <font color=greenyellow>Random Forest:\n\n####    RMSE en Train es: 5546.772210955095\n####    RMSE en Test es: 29441.25111212885\n\n### <font color=greenyellow>Random Forest opcional con clustering:\n\n####    RMSE en Train es: 13918.198400267815\n####    RMSE en Test es: 40039.89829540988\n\n### <font color=greenyellow>Random Forest solo con las 2 variables creadas mediante clustering:\n\n####    RRMSE en Train es: 51146.57137703558\n####    RMSE en Test es: 51501.3647248728","279c189f":"### Aplicacion del PCA con 45","d44c31b6":"### <font color=greenyellow>Random Forest (GANADOR!!)","857d7cdb":"### Modelos a utilizar:\n\n    +Random Forest\n    +XGBoost\n    +BaggingReggresor\n    +AdaboostRegressor\n    +Stacking\n    ","9a70c6f6":"#  <font color=greenyellow>Proyecto Numero 2","4fa68e48":"<font color=greenyellow>Modelo de Arbol de decision (proyecto 1) con Optimizacion aplicado con datos del proyecto 1:\n\n    *R2 en Train: 0.952\n    *R2 ajustado en train: 0.952\n\n    *R2 en test: 0.766\n    *R2 ajustado en Test: 0.766\n\n    *RMSE en train: 64721.069\n    *RMSE en test: 146092.758\n    \n<font color=greenyellow>Modelo de Arbol de decision (proyecto 1) con Optimizacion aplicado con datos del Proyecto 2:\n\n    *R2 en Train: 0.971\n    *R2 ajustado en train: 0.971\n\n    *R2 en test: 0.669\n    *R2 ajustado en Test: 0.668\n\n    *RMSE en train: 13684.963916520812\n    *RMSE en test: 39679.42253912968\n    \n<font color=greenyellow>Modelo de Arbol de decision (proyecto 2) con Optimizacion aplicado con datos del Proyecto 2:\n    \n    *R2 en Train: 0.790\n    *R2 ajustado en train: 0.789\n\n    *R2 en test: 0.700\n    *R2 ajustado en Test: 0.699\n\n    *RMSE en train: 36870.24537871894\n    *RMSE en test: 42970.52433399813\n    ","e1b5c44e":"##  <font color=greenyellow>DESAF\u00cdO OPCIONAL\nAplica una t\u00e9cnica de Clustering sobre el dataset. Puedes combinar con t\u00e9cnicas de reducci\u00f3n de dimensionalidad para facilitar la visualizaci\u00f3n. \u00bfQu\u00e9 clusters encuentras? \u00bfA qu\u00e9 pueden corresponder? Te dejamos preguntas que pueden servir como disparadoras: \u00bfqu\u00e9 barrios se parecen m\u00e1s entre s\u00ed?\u00bfqu\u00e9 tipos de propiedades se parecen m\u00e1s entre s\u00ed?\n","e03f98c2":"dataset[\"rooms\"].value_counts().sort_index()#### Bathrooms","015cfcb1":"## <font color=greenyellow>Punto mejorado despues de la primera correccion:","26b6ada1":"## Punto especifico de Parte A: Escalado de datos","2eeddb5e":"##### Vemos que los valores otorgados no son enteros, por ende usaremos el floor, para reducirlos al entero mas cercano. Se utilizo el floor porque el numero 4 sigue siendo un valor representativo, aunque poco.","075881b9":"##### Vemos que los valores otorgados no son enteros, por ende usaremos el ceil, para elevarlos al entero mas cercano. Se utilizo el Ceil porque el numero 6 sigue siendo un valor representativo","f66e2421":"### Parametros optimizados del proyecto 1","46b9be46":"### <font color=greenyellow>Kmeans Aplicado en Lat y Lon","94ab6fb3":"## <font color=greenyellow>PARTE C - Interpretaci\u00f3n de modelos\n\nDebes estudiar qu\u00e9 variables utiliza el modelo para predecir y responder la pregunta: \u00bfcoincide con lo que esperabas a partir de tu experiencia con este dataset?\n\nEs muy importante que analices los errores del modelo. \u00bfD\u00f3nde es mayor el error? \u00bfd\u00f3nde acierta?\n\nDebes ser cr\u00edtico\/a con la metodolog\u00eda utilizada. \u00bfQu\u00e9 mejorar\u00edas? Ten en cuenta siempre terminar con una discusi\u00f3n sobre lo realizado y conclusiones obtenidas.\n","33854393":"XGBoost:\n\nPara el XGB las variables importantes fueron:\n    \n    Lat\n    Lon\n    Surface_Total\n    Surface_covered\n    Rooms\n    \nAqui no solo sorprende la aparicion de latitud,sino que tambien tenemos la longitud como una variable importante, en cuanto a las demas, eran esperadas. Pero el punto clave de este analisis no es que lat y lon esten en el top 5 de variables, el punto clave es que sean las variables mas importante para predecir."}}