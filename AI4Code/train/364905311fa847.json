{"cell_type":{"f612155b":"code","1cd46e26":"code","109fec18":"code","6e9dae8b":"code","d0494a16":"code","9bbfb9c4":"code","d677ab00":"code","4bfa80c2":"code","8f613653":"code","2f42d124":"code","8bdc3e7c":"code","46daa7c0":"code","31d7026e":"code","a1e94041":"code","5797a405":"code","928abce0":"code","96c04d96":"code","d39f2164":"code","7bae3698":"code","5a2ccc97":"code","f25ce30f":"code","e2850fca":"code","bb59559a":"code","9fd74b71":"code","3b73f8c2":"code","560a128e":"code","d1b49b3b":"code","7d1bafa8":"code","df073750":"code","f6a808aa":"code","0c7cc523":"code","17809bd4":"code","56d1608b":"code","fd167926":"code","88e18236":"code","9eaec4dc":"code","5241b1da":"code","802c36d6":"code","f8b0920c":"code","e44da8fb":"code","99964dd9":"code","d0ef7d68":"code","af1e91a8":"code","8524ff9a":"code","49cb3881":"code","f38e173b":"code","ef471ddb":"code","fa2c68e0":"code","b239918e":"code","c6226d50":"markdown","b37e0e72":"markdown","f5c16696":"markdown","1886fa84":"markdown","8f218475":"markdown","cbd739f1":"markdown","392f44b2":"markdown","373776de":"markdown","b56bea16":"markdown","63bd4bfc":"markdown","3b501b08":"markdown","038083e6":"markdown","7e25fed0":"markdown","67f827a6":"markdown","97e28d0b":"markdown","4200a1b0":"markdown","895a84ab":"markdown","e2e2e5fc":"markdown","b2bd6bb4":"markdown","9b8510cc":"markdown","885e82b6":"markdown","eb664a4b":"markdown","e7bae32d":"markdown","8ea987e5":"markdown","1f97b8aa":"markdown","176c5d4a":"markdown","a9e49387":"markdown","896d848d":"markdown","1c261ccd":"markdown","0893fe22":"markdown","e9c6c809":"markdown","3c376233":"markdown","d711a829":"markdown","a9d04b70":"markdown","709b67ad":"markdown","0a5ef5a4":"markdown","40646253":"markdown","21eaf418":"markdown","d1441640":"markdown","68e0667e":"markdown","bcd6e07c":"markdown","d47ec8ab":"markdown","43f56f3c":"markdown","234e060b":"markdown","551d91af":"markdown","6013cbeb":"markdown","cf7199aa":"markdown","52dec530":"markdown","3bf39752":"markdown","5f936f84":"markdown","ffa60bae":"markdown","3aba163e":"markdown","0bcf0593":"markdown","cef10e95":"markdown","e5bd5f38":"markdown","f2d09294":"markdown","cde9626e":"markdown","49b0f245":"markdown","81ee8cc8":"markdown","b5f8fbef":"markdown","075f82b4":"markdown","c6468223":"markdown"},"source":{"f612155b":"#Check installed version of packages\n%reload_ext watermark\n%watermark -v -p numpy,pandas,torch,transformers","1cd46e26":"#Import required libraries \nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import rc\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom textwrap import wrap\nfrom collections import defaultdict\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_recall_curve,auc\n\n\nimport transformers\nfrom transformers import BertModel, BertTokenizerFast\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n\nimport torch\nfrom torch import nn,optim\nfrom torch.utils.data import Dataset,DataLoader\nimport torch.nn as nn  \nimport torch.nn.functional as F  \ndevice=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu');\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\nsns.set(style='whitegrid',palette='muted',font_scale=1.2)\ncolor_palette=['#01BEFE','#FFDD00','#FF7D00','#FF006D','#ADFF02','#8F00FF']\nsns.set_palette(sns.color_palette(color_palette))\n\nrcParams['figure.figsize']= 12,6\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nseed=42\nnp.random.seed(seed)\ntorch.manual_seed(seed)","109fec18":"!nvidia-smi","6e9dae8b":"df=pd.read_csv('..\/input\/reviews-for-top-30-apps-in-india-in-play-store\/App_reviews.csv')\ndf.head()","d0494a16":"df.shape","9bbfb9c4":"print(f'There are {df.shape[0]} reviews in the dataset')","d677ab00":"df.info()","4bfa80c2":"sns.countplot(df.score)\nplt.xlabel('Review score')","8f613653":"def to_sentiment(score):\n  score=int(score)\n  if score <=4:\n    return 0\n  else :\n    return 1\n\ndf['sentiment']=df.score.apply(to_sentiment)\ndf.head()","2f42d124":"ax=sns.countplot(df.sentiment)\nplt.xlabel('Review sentiment')\nclass_names=['Negative','Positive']\nax.set_xticklabels(class_names)\nplt.show()","8bdc3e7c":"Pre_trained_model='bert-base-uncased'","46daa7c0":"tokenizer=BertTokenizerFast.from_pretrained(Pre_trained_model);","31d7026e":"sample_text=\"The animal didn't cross the street because it was too tired\"\n\n#Convert text to tokens & token_ids\ntokens=tokenizer.tokenize(sample_text)\ntoken_ids=tokenizer.convert_tokens_to_ids(tokens)\n\nprint(f'Sentence : {sample_text}')\nprint(f'Tokens :{tokens}')\nprint(f'Token IDs : {token_ids}')","a1e94041":"encoding=tokenizer.encode_plus(\n    sample_text,\n    max_length=32,\n    add_special_tokens=True,   # 'Add [SEP] & [CLS]'\n    pad_to_max_length=True,\n    truncation=True,\n    return_attention_mask=True,  # Reurns array of 0's & 1's to distinguish padded tokens from real tokens.\n    return_token_type_ids=False,\n    return_tensors='pt'         # Returns pytorch tensors\n)\n\nencoding.keys()","5797a405":"# Check input_ids\nprint('Maximum length of input_ids for each sentence : {}'.format(len(encoding['input_ids'][0])))\nencoding['input_ids'][0]","928abce0":"\n#check attention mask\nprint(f\"Maximum length of attention mask for each sentence : {len(encoding['attention_mask'][0])}\")\nencoding['attention_mask'][0]","96c04d96":"special_tokens=tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])\nspecial_tokens","d39f2164":"token_lens=[]\nfor content in df.content:\n  tokens_content=tokenizer.encode(content,max_length=150,truncation=True)\n  token_lens.append(len(tokens_content))","7bae3698":"#Plot the tokens\nsns.distplot(token_lens)\nplt.xlim([0,150])\nplt.xlabel('Token count')","5a2ccc97":"Max_length=100","f25ce30f":"class reviews_India_Dataset(Dataset):\n\n  def __init__(self,reviews,targets,tokenizer,max_length):\n    self.reviews=reviews\n    self.targets=targets\n    self.tokenizer=tokenizer\n    self.max_length=max_length\n\n  def __len__(self):\n    return len(self.reviews)\n\n  def __getitem__(self,item):\n    review = str(self.reviews[item])\n    targets = self.targets[item]\n\n    encoding = self.tokenizer.encode_plus(\n        review,\n        max_length=Max_length,\n        add_special_tokens=True,\n        pad_to_max_length=True,\n        truncation=True,\n        return_attention_mask=True,\n        return_token_type_ids=False,\n        return_tensors='pt'\n       )\n    return {\n        'review_text':review,\n        'input_ids':encoding['input_ids'].flatten(),\n        'attention_mask':encoding['attention_mask'].flatten(),\n        'targets' : torch.tensor(targets,dtype=torch.long)\n    }","e2850fca":"df_train,df_test=train_test_split(df, test_size=0.2, random_state=42)\ndf_valid,df_test = train_test_split(df_test,test_size=0.5,random_state=42)\n\nprint('Print the shape of datasets...')\nprint(f'Training dataset : {df_train.shape}')\nprint(f'Testing dataset : {df_test.shape}')\nprint(f'Validation dataset : {df_valid.shape}')","bb59559a":"batch_size=32\ndef data_loader(df, tokenizer, max_length, batch):\n  ds=reviews_India_Dataset(\n      reviews=df.content.to_numpy(),\n      targets=df.sentiment.to_numpy(),\n      tokenizer=tokenizer,\n      max_length=Max_length\n  )\n\n  return DataLoader(\n      ds,\n      batch_size=batch_size,\n      num_workers=4\n  )\n\n# Load datasets\ntrain_DataLoader=data_loader(df_train,tokenizer,Max_length,batch_size)\ntest_DataLoader=data_loader(df_test,tokenizer,Max_length,batch_size)\nvalid_DataLoader=data_loader(df_valid,tokenizer,Max_length,batch_size)","9fd74b71":"data=next(iter(train_DataLoader))\ndata.keys()","3b73f8c2":"print('Shape of the data keys...')\nprint(f\"Input_ids : {data['input_ids'].shape}\")\nprint(f\"Attention_mask : {data['attention_mask'].shape}\")\nprint(f\"targets : {data['targets'].shape}\")","560a128e":"bert_model = BertModel.from_pretrained(Pre_trained_model)","d1b49b3b":"class SentimentClassifier(nn.Module):\n  def __init__(self, n_classes):\n    super(SentimentClassifier, self).__init__()\n    self.bert = BertModel.from_pretrained(Pre_trained_model)\n    self.drop = nn.Dropout(p=0.5)\n    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n  def forward(self, input_ids, attention_mask):\n    _, pooled_output = self.bert(\n      input_ids=input_ids,\n      attention_mask=attention_mask\n    )\n    output = self.drop(pooled_output)\n    return self.out(output)","7d1bafa8":"model = SentimentClassifier(len(class_names))         #Create an instance \/ object\nmodel = model.to(device)                              # Move instance to GPU           ","df073750":"input_ids = data['input_ids'].to(device)\nattention_mask = data['attention_mask'].to(device)\nprint(input_ids.shape)      # batch size x seq length\nprint(attention_mask.shape) # batch size x seq length","f6a808aa":"F.softmax(model(input_ids,attention_mask), dim=1)","0c7cc523":"epochs=5\noptimizer=AdamW(model.parameters(),lr=2e-5,correct_bias=False)\ntotal_steps=len(train_DataLoader)*epochs\n\nscheduler=get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,\n    num_training_steps=total_steps\n)\n\nloss_fn=nn.CrossEntropyLoss().to(device)","17809bd4":"def train(\n  model,\n  data_loader,\n  loss_fn,\n  optimizer,\n  device,\n  scheduler,\n  n_observations\n):\n  model = model.train()\n  losses = []\n  correct_predictions = 0\n  for d in data_loader:\n    input_ids = d[\"input_ids\"].to(device)\n    attention_mask = d[\"attention_mask\"].to(device)\n    targets = d[\"targets\"].to(device)\n    #Feed data to BERT model\n    outputs = model(\n      input_ids=input_ids,\n      attention_mask=attention_mask\n    )\n    _, preds = torch.max(outputs, dim=1)\n    loss = loss_fn(outputs, targets)\n    correct_predictions += torch.sum(preds == targets)\n    losses.append(loss.item())\n    loss.backward()\n    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)     # Clip gradients to avoid exploding gradient problem\n    optimizer.step()\n    scheduler.step()\n    optimizer.zero_grad()\n  return correct_predictions.double() \/ n_observations, np.mean(losses)","56d1608b":"def eval_model(model, data_loader,device,loss_fn, n_observations):\n  model = model.eval()\n  losses = []\n  correct_predictions = 0\n  with torch.no_grad():\n    for d in data_loader:\n      input_ids = d[\"input_ids\"].to(device)\n      attention_mask = d[\"attention_mask\"].to(device)\n      targets = d[\"targets\"].to(device)\n      # Feed data to BERT model\n      outputs = model(\n        input_ids=input_ids,\n        attention_mask=attention_mask\n      )\n      _, preds = torch.max(outputs, dim=1)\n      loss = loss_fn(outputs, targets)\n      correct_predictions += torch.sum(preds == targets)\n      losses.append(loss.item())\n  return correct_predictions.double() \/ n_observations, np.mean(losses)","fd167926":"%%time\nhistory = defaultdict(list)\nbest_accuracy = 0\nfor epoch in range(epochs):\n  print(f'Epoch {epoch + 1}\/{epochs}')\n  print('-' * 10)\n  train_acc, train_loss = train(\n    model,\n    train_DataLoader,\n    loss_fn,\n    optimizer,\n    device,\n    scheduler,\n    len(df_train)\n  )\n  print(f'Train loss {train_loss} accuracy {train_acc}')\n  val_acc, val_loss = eval_model(\n    model,\n    valid_DataLoader,\n    device,\n    loss_fn,\n    len(df_valid)\n  )\n  print(f'Validation  loss {val_loss} accuracy {val_acc}')\n  print()\n  history['train_acc'].append(train_acc)\n  history['train_loss'].append(train_loss)\n  history['val_acc'].append(val_acc)\n  history['val_loss'].append(val_loss)\n  if val_acc > best_accuracy:\n    torch.save(model.state_dict(), 'best_model_state.bin')\n    best_accuracy = val_acc","88e18236":"plt.plot(history['train_acc'], label='train accuracy')\nplt.plot(history['val_acc'], label='validation accuracy')\nplt.title('Training history')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.ylim([0, 1]);","9eaec4dc":"test_acc, _ = eval_model(\n  model,\n  test_DataLoader,\n  device,\n  loss_fn,\n  len(df_test)\n)","5241b1da":"test_acc.item()","802c36d6":"def get_predictions(model, data_loader):\n  model = model.eval()\n  review_texts = []\n  predictions = []\n  prediction_probs = []\n  real_values = []\n\n  with torch.no_grad():\n    for d in data_loader:\n      texts = d[\"review_text\"]\n      input_ids = d[\"input_ids\"].to(device)\n      attention_mask = d[\"attention_mask\"].to(device)\n      targets = d[\"targets\"].to(device)\n      outputs = model(\n        input_ids=input_ids,\n        attention_mask=attention_mask\n      )\n      _, preds = torch.max(outputs, dim=1)\n      review_texts.extend(texts)\n      predictions.extend(preds)\n      prediction_probs.extend(outputs)\n      real_values.extend(targets)\n\n  predictions = torch.stack(predictions).cpu()\n  prediction_probs = torch.stack(prediction_probs).cpu()\n  real_values = torch.stack(real_values).cpu()\n  return review_texts, predictions, prediction_probs, real_values","f8b0920c":"y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n  model,\n  test_DataLoader\n)","e44da8fb":"class_report=classification_report(y_test, y_pred, target_names=class_names)\nprint(class_report)","99964dd9":"def show_confusion_matrix(confusion_matrix):\n  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(),rotation=0, ha='right')\n  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(),rotation=30, ha='right')\n  plt.ylabel('True sentiment')\n  plt.xlabel('Predicted sentiment');\ncm = confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\nshow_confusion_matrix(df_cm)","d0ef7d68":"# calculate model precision-recall curve\nprecision, recall, _ = precision_recall_curve(y_test, y_pred)\n# plot the model precision-recall curve\nplt.plot(recall, precision, marker='.', label='BERT')\n# axis labels\nplt.xlabel('Recall')\nplt.ylabel('Precision')\n# show the legend\nplt.legend()\n# show the plot\nplt.show()","af1e91a8":"#Calculate AUC_score for PR curve\nauc_score = auc(recall, precision)\nprint('PR AUC_score: %.3f' % auc_score)","8524ff9a":"review_text=y_review_texts[1]\ntrue_sentiment=y_test[1]\npred_df=pd.DataFrame({\n    'class_names':class_names,\n    'values':y_pred_probs[1]\n})\n\nprint('\\n'.join(wrap(review_text)))\nprint()\nprint(f'True Sentiment : {class_names[true_sentiment]}')","49cb3881":"sns.barplot(x='values',y='class_names',data=pred_df,orient='h')\nplt.xlabel('Probability')\nplt.ylabel('Sentiment')\nplt.xlim([0,1]);","f38e173b":"review_text='Fake Fake Fake Fake Fake, Please be aware of them, they are very dangerous people please be aware of them. \\\nThey ask your pan and aadhara number for interview, the ppl who calls are unprofessional they talk rubbish and try to trap you in pit'","ef471ddb":"encoded_review=tokenizer.encode_plus(\n    review_text,\n    max_length=Max_length,\n    add_special_tokens=True,\n    return_token_type_ids=False,\n    pad_to_max_length=True,\n    truncation=True,\n    return_attention_mask=True,\n    return_tensors='pt'\n)","fa2c68e0":"input_ids=encoded_review['input_ids'].to(device)\nattention_mask=encoded_review['attention_mask'].to(device)\n\noutput=model(input_ids,attention_mask)\n_,pred=torch.max(output,dim=1)\n\nprint(f'Review_text : {review_text}')\nprint(f'Sentiment: {class_names[pred]}')","b239918e":"path=\".\/Sentiment_Analysis_Bert.bin\"\ntorch.save(model.state_dict(),path)","c6226d50":"# Let's train our sentiment classifier","b37e0e72":"That is model performing well on test data, we are getting 88.11 % accuracy which is 0.611% more than validation accuracy. It seems model is generalizing well.","f5c16696":"### Plot PR (Precesion-Recall) curve","1886fa84":"Let's see how good our model on unseen data.","8f218475":"## Save our model\n","cbd739f1":"Apply softmax function to outputs to get the predicted probabilities from our trained model.","392f44b2":"### Let's check missing values if there are any.","373776de":"BERT do not process the raw text & it needs to be converted to numbers by tokenization.","b56bea16":"We have highly class imbalanced data. Most of the reviews have score 5 in the dataset.","63bd4bfc":"#### Let's have a look at example batch from **training DataLoader**","3b501b08":"### Let\u2019s create an instance and move it to the GPU","038083e6":"## Predicting on Raw text","7e25fed0":"### Let's convert our reviews into Positive & Negative sentiment.","67f827a6":"### Now load the scraped dataset","97e28d0b":"This confirms our model having difficulty in classifying negative reviews. This is because the classes were imbalanced i.e., the number of positive classes more than negative. This imbalanced data made model biased towards positive class, so it is not classifying negative reviews correctly. This can be avoided with balanced data & also classes can be balanced using sampling methods such as SMOTE.","4200a1b0":"Encoding contains input_ids & attention masks of same length 32 for each sentence.","895a84ab":"### Do we have class imbalanced data?","e2e2e5fc":"### **Choosing Sequence length for our dataset**","b2bd6bb4":"## Let's build a Sentiment Classifier","9b8510cc":"Using above helper functions, write our training loop & store training history:","885e82b6":"Now write a helper function to train our model:","eb664a4b":"### **Let's split the data**","e7bae32d":"### **Import the required libraries**","8ea987e5":"Let's look at training vs validation accuracy:","1f97b8aa":"Still classes are not balanced. Will proceed anyway..","176c5d4a":"#### Check GPU info on Google colab","a9e49387":"Now move the  batch training data to the GPU:","896d848d":"### **Now add Special tokens**","1c261ccd":"Let's load a pre-trained [BertTokenizerFast]('https:\/\/huggingface.co\/transformers\/model_doc\/bert.html#transformers.BertTokenizerFast'):","0893fe22":"Let's us use our model to predict sentiment on raw text:","e9c6c809":"We got 87.5 % validation accuracy for 5 epochs, not bad.The number of epochs \ncan be increased to get higher validation accuracy.\n","3c376233":"It appears that the length of most tokens lies between 3 & 30. On safer side, we are taking maximum sequence length of 100.","d711a829":"Kaggle is using Tesla P100 gpu.","a9d04b70":"Now create a classifier that uses BERT model","709b67ad":"Let's have a look at the classification report:","0a5ef5a4":"We are getting Area under the curve (AUC) for precision-recall curve is 0.925, this is good enough for model to generalize well on unseen imbalanced dataset.","40646253":"#### Let's tokenize the text data","21eaf418":"Now will plot confusion matrix:","d1441640":"Similarly, write a helper function to evaluate our model:","68e0667e":"#### Let's load the basic BERT model","bcd6e07c":"#### Now will create a helper function for **Data loader**","d47ec8ab":"As per original paper on BERT, we will use [AdamW]('https:\/\/huggingface.co\/transformers\/main_classes\/optimizer_schedules.html#adamw') optimizer for correcting weight decay. We will also use, linear scheduler with no warm up steps:","43f56f3c":"Bert expects  special tokens for each sentence,               \n[SEP] - marker for ending of the sentence        \n[CLS] - marker for start of each sentence for sentence classification            \n[PAD] - marker for padding sentence to a specific length                    \nand everything else encodes as             \n[UNK] - marker for all tokens expect tokens in trained data.","234e060b":"Pytorch dataset is of map-style i.e., it implements __getitem__  &  __len__  protocols, & represents a map from indices\/keys to datasamples.","551d91af":"Now we look at the confidence of each sentiemt of our model:","6013cbeb":"## References:\n\n* [Sentiment analysis with BERT](https:\/\/www.curiousily.com\/posts\/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python\/#data-preprocessing)\n* [ROC & PR curves for imbalanced datasets](https:\/\/machinelearningmastery.com\/roc-curves-and-precision-recall-curves-for-imbalanced-classification\/)\n* [Evaluation Metrics, ROC-Curves and imbalanced datasets](http:\/\/www.davidsbatista.net\/blog\/2018\/08\/19\/NLP_Metrics\/#:~:text=The%20curve%20is%20a%20plot,false%20positives%20and%20false%20negatives.)","cf7199aa":"Now use the tokenizer to encode the raw text:","52dec530":"# Model Evaluation","3bf39752":"PR curve is an intuitive measure when evaluating binary classifiers on imbalanced datasets. Since the classes are imbalanced in our data, we are using PR curve as a metric for model evaluation.","5f936f84":"Now let's have a look at an example from our test data:","ffa60bae":"Wow, our model is generalizing well. This model can be fine tuned further to improve its performance.","3aba163e":"Now let's create a **Pytorch dataset**","0bcf0593":"#### In this notebook, Sentiment analysis will be performed using BERT with Huggingface Pytorch library. To build a sentiment classifier, we scraped reviews for Top 30 Android apps in India from Google Play store using Google play scraper.\n\n### [BERT]('https:\/\/huggingface.co\/transformers\/model_doc\/bert.html') \nIt is a Bidirectional Encoding Representations from Transformers. It\u2019s a bidirectional transformer pre-trained using a combination of masked language modeling objective and next sentence prediction on a large corpus comprising the Toronto Book Corpus and Wikipedia. It is simple & extremly powerful & open sourced by the team at [HuggingFace]('https:\/\/huggingface.co\/').","cef10e95":"The **pooled_output** is a summary of the content, according to BERT & it is obtained by applying **BertPooler** on **last_hidden_state.**      \nWe are using a **dropout layer** for some regularization and a **fully-connected layer** for our output.","e5bd5f38":"Will use a sample text to understand tokenization process:","f2d09294":"Oops, the attributes reviewCreatedVersion, replyContent, & repliedAt contains missing values.","cde9626e":"Now we will write helper function to get the predictions from our model:","49b0f245":"#### Let's store the token length of each review","81ee8cc8":"We are getting f1-score=0.88, not bad. This model can be fine tuned further to get high f1-scores.","b5f8fbef":"## **Data Processing**","075f82b4":"Let's get predictions from our model:","c6468223":"#### Let's convert token_ids to tokens to look at special tokens"}}