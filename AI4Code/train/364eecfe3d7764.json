{"cell_type":{"356331b9":"code","d28c6341":"code","c86b305e":"code","66fdc7c5":"code","101c1f22":"code","a7f4e9c5":"code","c152493f":"code","5c4d4dde":"code","e818e0de":"code","35aaf9c6":"code","37ce1908":"code","72f3d7c5":"code","7c975a5b":"code","acc8eec2":"code","28b09f9b":"code","76678ad2":"code","f318c691":"code","5796c911":"code","2a894637":"markdown","89c77742":"markdown","6ab130eb":"markdown","a167c1e9":"markdown","a4b42da3":"markdown","a3ce26c0":"markdown","495d9c19":"markdown","9295f244":"markdown","3e8433f8":"markdown"},"source":{"356331b9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom IPython.core.display import display,Image\nfrom string import Template\nimport pandas as pd\nimport numpy as np\nimport IPython.display\nimport warnings ","d28c6341":"display(Image(filename='\/kaggle\/input\/cnn-images\/cnn\/conv.png'))","c86b305e":"display(Image(filename='\/kaggle\/input\/cnn-images\/cnn\/conv2.png'))","66fdc7c5":"class Conv:\n    \n    def __init__(self, num_filters):\n        self.num_filters = num_filters\n        \n        #why divide by 9...Xavier initialization\n        self.filters = np.random.randn(num_filters, 3, 3)\/9\n    \n    def iterate_regions(self, image):\n        #generates all possible 3*3 image regions using valid padding\n        \n        h,w = image.shape\n        \n        for i in range(h-2):\n            for j in range(w-2):\n                im_region = image[i:(i+3), j:(j+3)]\n                yield im_region, i, j\n                \n    def forward(self, input):\n        self.last_input = input\n        \n        h,w = input.shape\n        \n        output = np.zeros((h-2, w-2, self.num_filters))\n        \n        for im_regions, i, j in self.iterate_regions(input):\n            output[i, j] = np.sum(im_regions * self.filters, axis=(1,2))\n        return output\n    \n    def backprop(self, d_l_d_out, learn_rate):\n        '''\n        Performs a backward pass of the conv layer.\n        - d_L_d_out is the loss gradient for this layer's outputs.\n        - learn_rate is a float.\n        '''\n        d_l_d_filters = np.zeros(self.filters.shape)\n\n        for im_region, i, j in self.iterate_regions(self.last_input):\n            for f in range(self.num_filters):\n                d_l_d_filters[f] += d_l_d_out[i,j,f] * im_region\n\n        #update filters\n        self.filters -= learn_rate * d_l_d_filters\n\n        return None","101c1f22":"display(Image(filename='\/kaggle\/input\/cnn-images\/cnn\/maxpool.png'))","a7f4e9c5":"display(Image(filename='\/kaggle\/input\/cnn-images\/cnn\/maxpool2.png'))","c152493f":"class MaxPool:\n    def iterate_regions(self, image):\n        h, w, _ = image.shape\n        \n        new_h = h \/\/ 2\n        new_w = w \/\/ 2\n        \n        for i in range(new_h):\n            for j in range(new_w):\n                im_region = image[(i*2):(i*2+2), (j*2):(j*2+2)]\n                yield im_region, i, j\n                \n    def forward(self, input):\n        \n        self.last_input = input\n        \n        h, w, num_filters = input.shape\n        output = np.zeros((h\/\/2, w\/\/2, num_filters))\n        \n        for im_region, i, j in self.iterate_regions(input):\n            output[i,j] = np.amax(im_region,axis=(0,1))\n            \n        return output\n    \n    def backprop(self, d_l_d_out):\n        '''\n        Performs a backward pass of the maxpool layer.\n        Returns the loss gradient for this layer's inputs.\n        - d_L_d_out is the loss gradient for this layer's outputs.\n        '''\n        d_l_d_input = np.zeros(self.last_input.shape)\n\n        for im_region, i, j in self.iterate_regions(self.last_input):\n            h, w, f = im_region.shape\n            amax = np.amax(im_region, axis=(0,1))\n\n            for i2 in range(h):\n                for j2 in range(w):\n                    for f2 in range(f):\n                        #if the pixel was the max value, copy the gradient to it\n                        if(im_region[i2,j2,f2] == amax[f2]):\n                            d_l_d_input[i*2+i2, j*2+j2 ,f2] = d_l_d_out[i, j, f2]\n                            break;\n        return d_l_d_input","5c4d4dde":"display(Image(filename='\/kaggle\/input\/cnn-images\/cnn\/softmax.png'))","e818e0de":"display(Image(filename='\/kaggle\/input\/cnn-images\/cnn\/sogtmax2.png'))","35aaf9c6":"class Softmax:\n    def __init__(self, input_len, nodes):\n        # We divide by input_len to reduce the variance of our initial values\n        self.weights = np.random.randn(input_len, nodes)\/input_len\n        self.biases = np.zeros(nodes)\n    \n    def forward(self, input):\n        \n        self.last_input_shape = input.shape\n        \n        input = input.flatten()\n        self.last_input = input\n        \n        input_len, nodes = self.weights.shape\n        \n        totals = np.dot(input, self.weights) + self.biases\n        self.last_totals = totals\n        \n        exp = np.exp(totals)\n        return(exp\/np.sum(exp, axis=0)) \n    \n    def backprop(self, d_l_d_out, learn_rate):\n        \"\"\"  \n        Performs a backward pass of the softmax layer.\n        Returns the loss gradient for this layers inputs.\n        - d_L_d_out is the loss gradient for this layers outputs.\n        \"\"\"\n        \n        #We know only 1 element of d_l_d_out will be nonzero\n        for i, gradient in enumerate(d_l_d_out):\n            if(gradient == 0):\n                continue\n            \n            #e^totals\n            t_exp = np.exp(self.last_totals)\n            \n            #Sum of all e^totals\n            S = np.sum(t_exp)\n            \n            #gradients of out[i] against totals\n            d_out_d_t = -t_exp[i] * t_exp\/ (S**2)\n            d_out_d_t[i] = t_exp[i] * (S-t_exp[i]) \/(S**2)\n            \n            # Gradients of totals against weights\/biases\/input\n            d_t_d_w = self.last_input\n            d_t_d_b = 1\n            d_t_d_inputs = self.weights\n            \n            #Gradients of loss against totals\n            d_l_d_t = gradient * d_out_d_t\n            \n            #Gradients of loss against weights\/biases\/input\n            d_l_d_w = d_t_d_w[np.newaxis].T @ d_l_d_t[np.newaxis]\n            d_l_d_b = d_l_d_t * d_t_d_b  \n            d_l_d_inputs = d_t_d_inputs @ d_l_d_t\n            \n            #update weights\/biases\n            self.weights -= learn_rate * d_l_d_w\n            self.biases -= learn_rate * d_l_d_b\n            return d_l_d_inputs.reshape(self.last_input_shape)","37ce1908":"import mnist\n\ntrain_images = mnist.train_images()[:1000]\ntrain_labels = mnist.train_labels()[:1000]\ntest_images = mnist.test_images()[:1000]\ntest_labels = mnist.test_labels()[:1000]","72f3d7c5":"display(Image(filename='\/kaggle\/input\/cnn-images\/cnn\/train.png'))","7c975a5b":"conv = Conv(8)\npool = MaxPool()\nsoftmax = Softmax(13 * 13 * 8, 10)\n\ndef forward(image, label):\n    # We transform the image from [0, 255] to [-0.5, 0.5] to make it easier\n    # to work with. This is standard practice.\n    \n    out = conv.forward((image\/255) - 0.5)\n    out = pool.forward(out)\n    out = softmax.forward(out)\n    \n    #calculate cross-entropy loss and accuracy\n    loss = -np.log(out[label])\n    acc = 1 if(np.argmax(out) == label) else 0\n    \n    return out, loss, acc\n\n\ndef train(im, label, lr=0.005):\n    #forward\n    out,loss,acc = forward(im, label)\n    \n    #calculate initial gradient\n    gradient = np.zeros(10)\n    gradient[label] = -1\/out[label]\n    \n    \n    #Backprop\n    gradient = softmax.backprop(gradient, lr)\n    gradient = pool.backprop(gradient)\n    gradient = conv.backprop(gradient, lr)\n    \n    return loss, acc\n    \n    \nprint('MNIST CNN initialized')\n\nfor epoch in range(3):\n    print('----EPOCH %d ---'%(epoch+1))\n    \n    #shuffle the training data\n    permutation = np.random.permutation(len(train_images))\n    train_images = train_images[permutation]\n    train_labels = train_labels[permutation]\n\n\n    loss = 0\n    num_correct = 0\n\n    for i, (im, label) in enumerate(zip(train_images, train_labels)):\n\n        #print stats every 100 steps\n        if(i>0 and i %100 == 99):\n            print('[Step %d] Past 100 steps: Average Loss %.3f | Accuracy: %d%%' %(i + 1, loss \/ 100, num_correct))\n\n            loss = 0\n            num_correct = 0\n        l, acc = train(im, label)\n        loss += l\n        num_correct += acc","acc8eec2":"display(Image(filename='\/kaggle\/input\/backprop-cnn\/backprop_CNN\/1.png'))","28b09f9b":"display(Image(filename='\/kaggle\/input\/backprop-cnn\/backprop_CNN\/2.png'))","76678ad2":"display(Image(filename='\/kaggle\/input\/backprop-cnn\/backprop_CNN\/3.png'))","f318c691":"display(Image(filename='\/kaggle\/input\/backprop-cnn\/backprop_CNN\/4.png'))","5796c911":"display(Image(filename='\/kaggle\/input\/backprop-cnn\/backprop_CNN\/5.png'))","2a894637":"# Visualization of Neural Network using tensorspace.js\n\n\n\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1000\/1*_iuD-XPoKrBKG2TyftR8zA.gif\" width=\"1024\" height=\"1024\">","89c77742":"# Convolution Layer","6ab130eb":"# Convolutional Neural Network\n\n**A Convolutional Neural Network (ConvNet\/CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects\/objects in the image and be able to differentiate one from the other. The pre-processing required in a ConvNet is much lower as compared to other classification algorithms. While in primitive methods filters are hand-engineered, with enough training, ConvNets have the ability to learn these filters\/characteristics.**","a167c1e9":"# Softmax Layer","a4b42da3":"# Backpropagation indepth","a3ce26c0":"# MaxPooling Layer","495d9c19":"# Training","9295f244":"![image.png](attachment:image.png)","3e8433f8":"*A Max Pooling layer can\u2019t be trained because it doesn\u2019t actually have any weights, but we still need to implement a backprop() method for it to calculate gradients. We\u2019ll start by adding forward phase caching again. All we need to cache this time is the input:*\n\n*During the forward pass, the Max Pooling layer takes an input volume and halves its width and height dimensions by picking the max values over 2x2 blocks. The backward pass does the opposite: we\u2019ll double the width and height of the loss gradient by assigning each gradient value to where the original max value was in its corresponding 2x2 block.*"}}