{"cell_type":{"ee298c40":"code","21cf0a7a":"code","35a16200":"code","c4cef760":"code","dbc93550":"code","733ff85d":"code","939e2729":"code","fd1d10cd":"code","45dcc605":"code","a3a1d4f9":"code","88525ad0":"code","979fc6cf":"code","96ddcfa0":"code","b0c98c2a":"code","7b9b8b97":"code","c577248c":"code","82f796bf":"code","5129aa8d":"code","e8c41990":"code","30c5be5a":"code","9ac91845":"code","31c1325c":"code","8f3607c1":"code","2b3afbd9":"code","046d57f4":"code","87e068b9":"code","c22ee9cb":"code","7dacbbb8":"code","d34b0fd8":"code","8676ce7b":"code","08955e7c":"code","e678f49d":"code","96cc5e90":"code","ebbeb040":"code","6136ad09":"code","1dc6755e":"code","3eb2c540":"code","e7271cfb":"code","a90e6be1":"code","6f2bbf2a":"code","99c21090":"code","bfe34183":"code","8975649c":"code","407224b3":"code","68b00906":"code","ee25e652":"code","759eac57":"code","dd629e54":"code","343a408f":"code","8a21257c":"code","80e2bcc1":"code","3bb584d3":"code","e1b8b973":"code","e7a013b3":"code","6c262631":"code","8fb3301e":"code","71e22c33":"code","42ce6ef9":"code","268c40e8":"code","723c1dd9":"code","daf583f4":"code","ebc262e7":"code","e6d04c06":"code","7c66be26":"code","f953affe":"code","4e9f4c86":"code","58315c03":"code","f35cbd96":"code","d525b2ab":"code","5fdaced0":"code","9b17c3df":"code","46f4cf0e":"code","1c02270b":"markdown","3aed8198":"markdown","e35b00b6":"markdown","35c14c19":"markdown","9cb81d53":"markdown","bbfa5a97":"markdown","465f4a0f":"markdown","15eae754":"markdown","c0cb4afd":"markdown","7f25a935":"markdown","ed219b08":"markdown","46e4520f":"markdown","693dd972":"markdown","5c9340aa":"markdown","75913966":"markdown","c14100f1":"markdown","04ebc717":"markdown","6258483c":"markdown","39d3f431":"markdown","7af39683":"markdown","a68f0307":"markdown","07f956c9":"markdown","534d00ee":"markdown","5831663e":"markdown","3b2d6037":"markdown","01b2544d":"markdown","86d8ddbc":"markdown","b0e8f233":"markdown","c5596578":"markdown","f5583f03":"markdown","d264c54f":"markdown","bd4f90b2":"markdown","3bbe1dfa":"markdown","7603db45":"markdown","176bf120":"markdown","edc1f05d":"markdown","196c0449":"markdown","7a3ced23":"markdown","06523993":"markdown","e834d9d8":"markdown","9ce50162":"markdown","c83f18ea":"markdown","ed6ccccf":"markdown","8d942ab0":"markdown","dab53d62":"markdown","c4cccb8e":"markdown","1e741838":"markdown","d77af968":"markdown","e5955194":"markdown","b52a09b7":"markdown","0f7cd5bd":"markdown","7c1ddd5e":"markdown","094f4153":"markdown","ec24290f":"markdown","5f44b98d":"markdown","1b42fca9":"markdown","5fde348a":"markdown","50b1103e":"markdown","de87728b":"markdown","080c5ecc":"markdown","3a7406bc":"markdown","ea05c3f6":"markdown","10b064bf":"markdown","1d14867f":"markdown","0c01b246":"markdown","8d72436d":"markdown","e23c6cfd":"markdown","01d72c54":"markdown","b0c68094":"markdown","3227bdbc":"markdown","09a6cc0b":"markdown","b3bea5c2":"markdown","df8b22db":"markdown","dee1bff7":"markdown","95e46f48":"markdown","3c9decbf":"markdown","8d90345b":"markdown","63b37a0b":"markdown","e9d5ed6c":"markdown","4b5f1a43":"markdown","1ec468fd":"markdown","c70c362c":"markdown","12614d21":"markdown","b7fedcd6":"markdown","6da355f6":"markdown","09c799ed":"markdown"},"source":{"ee298c40":"import pandas as pd \nimport seaborn as sns\nimport re\nimport gc\nimport os\nimport numpy as np\nimport operator\nfrom wordcloud import WordCloud, STOPWORDS\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\npd_ctx = pd.option_context('display.max_colwidth', 100)\n\nimport nltk\nfrom nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n\nfrom gensim.models import KeyedVectors\n\nimport tensorflow as tf\n\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics as metrics\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D,GRU\nfrom tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, SpatialDropout1D, GlobalMaxPooling1D, Concatenate\nfrom tensorflow.keras.models import Model,load_model\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","21cf0a7a":"df_train = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/test.csv')\nquora_data = df_train['question_text'].append(df_test['question_text'])","35a16200":"print(\"\\033[1mTrain set info\\033[0m\")\nprint(df_train.info())","c4cef760":"# Ki\u1ec3m tra c\u00e1c tr\u01b0\u1eddng d\u1eef li\u1ec7u c\u1ee7a c\u00e2u h\u1ecfi \u0111\u01b0\u1ee3c \u0111\u00e1nh sincere\nprint(\"\\033[1mSincere Questions: \\033[0m\")\ndisplay(df_train[df_train['target']==0].head())\n# Ki\u1ec3m tra c\u00e1c tr\u01b0\u1eddng d\u1eef li\u1ec7u c\u1ee7a c\u00e2u h\u1ecfi \u0111\u01b0\u1ee3c \u0111\u00e1nh sincere\nprint(\"\\033[1mInsincere Questions: \\033[0m\")\ndisplay(df_train[df_train['target']==1].head())","dbc93550":"df_train.target.value_counts()","733ff85d":"pos_len = len(df_train[df_train['target'] == 1])\nneg_len = len(df_train[df_train['target'] == 0])\ntotal = len(df_train)\nprint(\"\\033[1mTotal = \\033[0m\", total)\nprint(\"\\033[1mSincere questions:\\033[0m {neg} ({percent: .2f}% )\".format(neg = neg_len, percent = neg_len \/ total * 100))\nprint(\"\\033[1mInsincere questions:\\033[0m {pos} ({percent: .2f}% )\".format(pos = pos_len, percent = pos_len \/ total * 100))\nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nax.bar(['sincere', 'insincere'], df_train.target.value_counts())\nplt.show()","939e2729":"df_test.info()","fd1d10cd":"# Shuffle t\u1eadp train \u0111\u1ec3 ki\u1ec3m tra nh\u1eefng gi\u00e1 tr\u1ecb ng\u1eabu nhi\u00ean\ntrain = df_train.sample(frac=1).reset_index(drop=True)\ndisplay(train.sample(n=10, random_state=344))","45dcc605":"### do vi\u1ec7c lemming words y\u00eau c\u1ea7u s\u1eed d\u1ee5ng th\u01b0 vi\u1ec7n wordnet \u0111\u1ec3 lookup c\u00e1c t\u1eeb c\u00f3 trong \u0111\u00f3\n# nltk.download('wordnet')\n# nltk.download('punkt')","a3a1d4f9":"def clean_tag(x):\n  if '[math]' in x:\n    x = re.sub('\\[math\\].*?math\\]', 'MATH EQUATION', x) #replacing with [MATH EQUATION]\n    \n  if 'http' in x or 'www' in x:\n    x = re.sub('(?:(?:https?|ftp):\\\/\\\/)?[\\w\/\\-?=%.]+\\.[\\w\/\\-?=%.]+', 'URL', x) #replacing with [url]\n  return x","88525ad0":"puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', \n        '\u2022', '~', '@', '\u00a3', '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`', '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a', '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', \n        '\u2588', '\u2026', '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u25ba', '\u2212', '\u00a2', '\u00ac', '\u2591', '\u00a1', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', \n        '\u2014', '\u2039', '\u2500', '\u2592', '\uff1a', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00b8', '\u22c5', '\u2018', '\u221e', \n        '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u2264', '\u2021', '\u221a', '\u25c4', '\u2501', \n        '\u21d2', '\u25b6', '\u2265', '\u255d', '\u2661', '\u25ca', '\u3002', '\u2708', '\u2261', '\u263a', '\u2714', '\u21b5', '\u2248', '\u2713', '\u2663', '\u260e', '\u2103', '\u25e6', '\u2514', '\u201f', '\uff5e', '\uff01', '\u25cb', \n        '\u25c6', '\u2116', '\u2660', '\u258c', '\u273f', '\u25b8', '\u2044', '\u25a1', '\u2756', '\u2726', '\uff0e', '\u00f7', '\uff5c', '\u2503', '\uff0f', '\uffe5', '\u2560', '\u21a9', '\u272d', '\u2590', '\u263c', '\u263b', '\u2510', \n        '\u251c', '\u00ab', '\u223c', '\u250c', '\u2109', '\u262e', '\u0e3f', '\u2266', '\u266c', '\u2727', '\u232a', '\uff0d', '\u2302', '\u2716', '\uff65', '\u25d5', '\u203b', '\u2016', '\u25c0', '\u2030', '\\x97', '\u21ba', \n        '\u2206', '\u2518', '\u252c', '\u256c', '\u060c', '\u2318', '\u2282', '\uff1e', '\u2329', '\u2399', '\uff1f', '\u2620', '\u21d0', '\u25ab', '\u2217', '\u2208', '\u2260', '\u2640', '\u2654', '\u02da', '\u2117', '\u2517', '\uff0a', \n        '\u253c', '\u2740', '\uff06', '\u2229', '\u2642', '\u203f', '\u2211', '\u2023', '\u279c', '\u251b', '\u21d3', '\u262f', '\u2296', '\u2600', '\u2533', '\uff1b', '\u2207', '\u21d1', '\u2730', '\u25c7', '\u266f', '\u261e', '\u00b4', \n        '\u2194', '\u250f', '\uff61', '\u25d8', '\u2202', '\u270c', '\u266d', '\u2523', '\u2534', '\u2513', '\u2728', '\\xa0', '\u02dc', '\u2765', '\u252b', '\u2120', '\u2712', '\uff3b', '\u222b', '\\x93', '\u2267', '\uff3d', \n        '\\x94', '\u2200', '\u265b', '\\x96', '\u2228', '\u25ce', '\u21bb', '\u21e9', '\uff1c', '\u226b', '\u2729', '\u272a', '\u2655', '\u061f', '\u20a4', '\u261b', '\u256e', '\u240a', '\uff0b', '\u2508', '\uff05', \n        '\u254b', '\u25bd', '\u21e8', '\u253b', '\u2297', '\uffe1', '\u0964', '\u2582', '\u272f', '\u2587', '\uff3f', '\u27a4', '\u271e', '\uff1d', '\u25b7', '\u25b3', '\u25d9', '\u2585', '\u271d', '\u2227', '\u2409', '\u262d', \n        '\u250a', '\u256f', '\u263e', '\u2794', '\u2234', '\\x92', '\u2583', '\u21b3', '\uff3e', '\u05f3', '\u27a2', '\u256d', '\u27a1', '\uff20', '\u2299', '\u2622', '\u02dd', '\u220f', '\u201e', '\u2225', '\u275d', '\u2610', \n        '\u2586', '\u2571', '\u22d9', '\u0e4f', '\u2601', '\u21d4', '\u2594', '\\x91', '\u279a', '\u25e1', '\u2570', '\\x85', '\u2662', '\u02d9', '\u06de', '\u2718', '\u272e', '\u2611', '\u22c6', '\u24d8', '\u2752', \n        '\u2623', '\u2709', '\u230a', '\u27a0', '\u2223', '\u2751', '\u25e2', '\u24d2', '\\x80', '\u3012', '\u2215', '\u25ae', '\u29bf', '\u272b', '\u271a', '\u22ef', '\u2669', '\u2602', '\u275e', '\u2017', '\u0702', '\u261c', \n        '\u203e', '\u271c', '\u2572', '\u2218', '\u27e9', '\uff3c', '\u27e8', '\u0387', '\u2717', '\u265a', '\u2205', '\u24d4', '\u25e3', '\u0361', '\u201b', '\u2766', '\u25e0', '\u2704', '\u2744', '\u2203', '\u2423', '\u226a', '\uff62', \n        '\u2245', '\u25ef', '\u263d', '\u220e', '\uff63', '\u2767', '\u0305', '\u24d0', '\u2198', '\u2693', '\u25a3', '\u02d8', '\u222a', '\u21e2', '\u270d', '\u22a5', '\uff03', '\u23af', '\u21a0', '\u06e9', '\u2630', '\u25e5', \n        '\u2286', '\u273d', '\u26a1', '\u21aa', '\u2741', '\u2639', '\u25fc', '\u2603', '\u25e4', '\u274f', '\u24e2', '\u22b1', '\u279d', '\u0323', '\u2721', '\u2220', '\uff40', '\u25b4', '\u2524', '\u221d', '\u264f', '\u24d0', \n        '\u270e', '\u037e', '\u2424', '\uff07', '\u2763', '\u2702', '\u2724', '\u24de', '\u262a', '\u2734', '\u2312', '\u02db', '\u2652', '\uff04', '\u2736', '\u25bb', '\u24d4', '\u25cc', '\u25c8', '\u275a', '\u2742', '\uffe6', \n        '\u25c9', '\u255c', '\u0303', '\u2731', '\u2556', '\u2749', '\u24e1', '\u2197', '\u24e3', '\u267b', '\u27bd', '\u05c0', '\u2732', '\u272c', '\u2609', '\u2589', '\u2252', '\u2625', '\u2310', '\u2668', '\u2715', '\u24dd', \n        '\u22b0', '\u2758', '\uff02', '\u21e7', '\u0335', '\u27aa', '\u2581', '\u258f', '\u2283', '\u24db', '\u201a', '\u2670', '\u0301', '\u270f', '\u23d1', '\u0336', '\u24e2', '\u2a7e', '\uffe0', '\u274d', '\u2243', '\u22f0', '\u264b', \n        '\uff64', '\u0302', '\u274b', '\u2733', '\u24e4', '\u2564', '\u2595', '\u2323', '\u2738', '\u212e', '\u207a', '\u25a8', '\u2568', '\u24e5', '\u2648', '\u2743', '\u261d', '\u273b', '\u2287', '\u227b', '\u2658', '\u265e', \n        '\u25c2', '\u271f', '\u2320', '\u2720', '\u261a', '\u2725', '\u274a', '\u24d2', '\u2308', '\u2745', '\u24e1', '\u2667', '\u24de', '\u25ad', '\u2771', '\u24e3', '\u221f', '\u2615', '\u267a', '\u2235', '\u235d', '\u24d1', \n        '\u2735', '\u2723', '\u066d', '\u2646', '\u24d8', '\u2236', '\u269c', '\u25de', '\u0bcd', '\u2739', '\u27a5', '\u2195', '\u0333', '\u2237', '\u270b', '\u27a7', '\u220b', '\u033f', '\u0367', '\u2505', '\u2964', '\u2b06', '\u22f1', \n        '\u2604', '\u2196', '\u22ee', '\u06d4', '\u264c', '\u24db', '\u2555', '\u2653', '\u276f', '\u264d', '\u258b', '\u273a', '\u2b50', '\u273e', '\u264a', '\u27a3', '\u25bf', '\u24d1', '\u2649', '\u23e0', '\u25fe', '\u25b9', \n        '\u2a7d', '\u21a6', '\u2565', '\u2375', '\u230b', '\u0589', '\u27a8', '\u222e', '\u21e5', '\u24d7', '\u24d3', '\u207b', '\u239d', '\u2325', '\u2309', '\u25d4', '\u25d1', '\u273c', '\u264e', '\u2650', '\u256a', '\u229a', \n        '\u2612', '\u21e4', '\u24dc', '\u23a0', '\u25d0', '\u26a0', '\u255e', '\u25d7', '\u2395', '\u24e8', '\u261f', '\u24df', '\u265f', '\u2748', '\u21ac', '\u24d3', '\u25fb', '\u266e', '\u2759', '\u2664', '\u2209', '\u061b', \n        '\u2042', '\u24dd', '\u05be', '\u2651', '\u256b', '\u2553', '\u2573', '\u2b05', '\u2614', '\u2638', '\u2504', '\u2567', '\u05c3', '\u23a2', '\u2746', '\u22c4', '\u26ab', '\u030f', '\u260f', '\u279e', '\u0342', '\u2419', \n        '\u24e4', '\u25df', '\u030a', '\u2690', '\u2719', '\u2199', '\u033e', '\u2118', '\u2737', '\u237a', '\u274c', '\u22a2', '\u25b5', '\u2705', '\u24d6', '\u2628', '\u25b0', '\u2561', '\u24dc', '\u2624', '\u223d', '\u2558', \n        '\u02f9', '\u21a8', '\u2659', '\u2b07', '\u2671', '\u2321', '\u2800', '\u255b', '\u2755', '\u2509', '\u24df', '\u0300', '\u2656', '\u24da', '\u2506', '\u239c', '\u25dc', '\u26be', '\u2934', '\u2707', '\u255f', '\u239b', \n        '\u2629', '\u27b2', '\u279f', '\u24e5', '\u24d7', '\u23dd', '\u25c3', '\u2562', '\u21af', '\u2706', '\u02c3', '\u2374', '\u2747', '\u26bd', '\u2552', '\u0338', '\u265c', '\u2613', '\u27b3', '\u21c4', '\u262c', '\u2691', \n        '\u2710', '\u2303', '\u25c5', '\u25a2', '\u2750', '\u220a', '\u2608', '\u0965', '\u23ae', '\u25a9', '\u0bc1', '\u22b9', '\u2035', '\u2414', '\u260a', '\u27b8', '\u030c', '\u263f', '\u21c9', '\u22b3', '\u2559', '\u24e6', \n        '\u21e3', '\uff5b', '\u0304', '\u219d', '\u239f', '\u258d', '\u2757', '\u05f4', '\u0384', '\u259e', '\u25c1', '\u26c4', '\u21dd', '\u23aa', '\u2641', '\u21e0', '\u2607', '\u270a', '\u0bbf', '\uff5d', '\u2b55', '\u2798', \n        '\u2040', '\u2619', '\u275b', '\u2753', '\u27f2', '\u21c0', '\u2272', '\u24d5', '\u23a5', '\\u06dd', '\u0364', '\u208b', '\u0331', '\u030e', '\u265d', '\u2273', '\u2599', '\u27ad', '\u0700', '\u24d6', '\u21db', '\u258a', \n        '\u21d7', '\u0337', '\u21f1', '\u2105', '\u24e7', '\u269b', '\u0310', '\u0315', '\u21cc', '\u2400', '\u224c', '\u24e6', '\u22a4', '\u0313', '\u2626', '\u24d5', '\u259c', '\u2799', '\u24e8', '\u2328', '\u25ee', '\u2637', \n        '\u25cd', '\u24da', '\u2254', '\u23e9', '\u2373', '\u211e', '\u250b', '\u02fb', '\u259a', '\u227a', '\u0652', '\u259f', '\u27bb', '\u032a', '\u23ea', '\u0309', '\u239e', '\u2507', '\u235f', '\u21ea', '\u258e', '\u21e6', '\u241d', \n        '\u2937', '\u2256', '\u27f6', '\u2657', '\u0334', '\u2644', '\u0368', '\u0308', '\u275c', '\u0321', '\u259b', '\u2701', '\u27a9', '\u0bbe', '\u02c2', '\u21a5', '\u23ce', '\u23b7', '\u0332', '\u2796', '\u21b2', '\u2a75', '\u0317', '\u2762', \n        '\u224e', '\u2694', '\u21c7', '\u0311', '\u22bf', '\u0316', '\u260d', '\u27b9', '\u294a', '\u2041', '\u2722']\n\ndef clean_punct(x):\n  x = str(x)\n  for punct in puncts:\n    if punct in x:\n      x = x.replace(punct, ' ')\n    return x","979fc6cf":"mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'bitcoin', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization', \n                'electroneum':'bitcoin','nanodegree':'degree','hotstar':'star','dream11':'dream','ftre':'fire','tensorflow':'framework','unocoin':'bitcoin',\n                'lnmiit':'limit','unacademy':'academy','altcoin':'bitcoin','altcoins':'bitcoin','litecoin':'bitcoin','coinbase':'bitcoin','cryptocurency':'cryptocurrency',\n                'simpliv':'simple','quoras':'quora','schizoids':'psychopath','remainers':'remainder','twinflame':'soulmate','quorans':'quora','brexit':'demonetized',\n                'iiest':'institute','dceu':'comics','pessat':'exam','uceed':'college','bhakts':'devotee','boruto':'anime',\n                'cryptocoin':'bitcoin','blockchains':'blockchain','fiancee':'fiance','redmi':'smartphone','oneplus':'smartphone','qoura':'quora','deepmind':'framework','ryzen':'cpu','whattsapp':'whatsapp',\n                'undertale':'adventure','zenfone':'smartphone','cryptocurencies':'cryptocurrencies','koinex':'bitcoin','zebpay':'bitcoin','binance':'bitcoin','whtsapp':'whatsapp',\n                'reactjs':'framework','bittrex':'bitcoin','bitconnect':'bitcoin','bitfinex':'bitcoin','yourquote':'your quote','whyis':'why is','jiophone':'smartphone',\n                'dogecoin':'bitcoin','onecoin':'bitcoin','poloniex':'bitcoin','7700k':'cpu','angular2':'framework','segwit2x':'bitcoin','hashflare':'bitcoin','940mx':'gpu',\n                'openai':'framework','hashflare':'bitcoin','1050ti':'gpu','nearbuy':'near buy','freebitco':'bitcoin','antminer':'bitcoin','filecoin':'bitcoin','whatapp':'whatsapp',\n                'empowr':'empower','1080ti':'gpu','crytocurrency':'cryptocurrency','8700k':'cpu','whatsaap':'whatsapp','g4560':'cpu','payymoney':'pay money',\n                'fuckboys':'fuck boys','intenship':'internship','zcash':'bitcoin','demonatisation':'demonetization','narcicist':'narcissist','mastuburation':'masturbation',\n                'trignometric':'trigonometric','cryptocurreny':'cryptocurrency','howdid':'how did','crytocurrencies':'cryptocurrencies','phycopath':'psychopath',\n                'bytecoin':'bitcoin','possesiveness':'possessiveness','scollege':'college','humanties':'humanities','altacoin':'bitcoin','demonitised':'demonetized',\n                'bras\u00edlia':'brazilia','accolite':'accolyte','econimics':'economics','varrier':'warrier','quroa':'quora','statergy':'strategy','langague':'language',\n                'splatoon':'game','7600k':'cpu','gate2018':'gate 2018','in2018':'in 2018','narcassist':'narcissist','jiocoin':'bitcoin','hnlu':'hulu','7300hq':'cpu',\n                'weatern':'western','interledger':'blockchain','deplation':'deflation', 'cryptocurrencies':'cryptocurrency', 'bitcoin':'blockchain cryptocurrency',}\n\ndef correct_mispell(x):\n  words = x.split()\n  for i in range(0, len(words)):\n    if mispell_dict.get(words[i]) is not None:\n      words[i] = mispell_dict.get(words[i])\n    elif mispell_dict.get(words[i].lower()) is not None:\n      words[i] = mispell_dict.get(words[i].lower())\n        \n  words = \" \".join(words)\n  return words","96ddcfa0":"def remove_stopwords(x):\n  x = [word for word in x.split() if word not in STOPWORDS]\n  x = ' '.join(x)\n  return x","b0c98c2a":"contraction_mapping = {\n \"I'm\": 'I am',\n \"I'm'a\": 'I am about to',\n \"I'm'o\": 'I am going to',\n \"I've\": 'I have',\n \"I'll\": 'I will',\n \"I'll've\": 'I will have',\n \"I'd\": 'I would',\n \"I'd've\": 'I would have',\n \"i'm\": 'i am',\n \"i'm'a\": 'i am about to',\n \"i'm'o\": 'i am going to',\n \"i've\": 'i have',\n \"i'll\": 'i will',\n \"i'll've\": 'i will have',\n \"i'd\": 'i would',\n \"i'd've\": 'i would have',\n 'Whatcha': 'What are you',\n 'whatcha': 'what are you',\n \"amn't\": 'am not',\n \"ain't\": 'are not',\n \"aren't\": 'are not',\n \"'cause\": 'because',\n \"can't\": 'can not',\n \"can't've\": 'can not have',\n \"could've\": 'could have',\n \"couldn't\": 'could not',\n \"couldn't've\": 'could not have',\n \"daren't\": 'dare not',\n \"daresn't\": 'dare not',\n \"dasn't\": 'dare not',\n \"didn't\": 'did not',\n 'didn\u2019t': 'did not',\n \"don't\": 'do not',\n 'don\u2019t': 'do not',\n \"doesn't\": 'does not',\n \"e'er\": 'ever',\n \"everyone's\": 'everyone is',\n 'finna': 'fixing to',\n 'gimme': 'give me',\n \"gon't\": 'go not',\n 'gonna': 'going to',\n 'gotta': 'got to',\n \"hadn't\": 'had not',\n \"hadn't've\": 'had not have',\n \"hasn't\": 'has not',\n \"haven't\": 'have not',\n \"he've\": 'he have',\n \"he's\": 'he is',\n \"he'll\": 'he will',\n \"he'll've\": 'he will have',\n \"he'd\": 'he would',\n \"he'd've\": 'he would have',\n \"here's\": 'here is',\n \"how're\": 'how are',\n \"how'd\": 'how did',\n \"how'd'y\": 'how do you',\n \"how's\": 'how is',\n \"how'll\": 'how will',\n \"isn't\": 'is not',\n \"it's\": 'it is',\n \"'tis\": 'it is',\n \"'twas\": 'it was',\n \"it'll\": 'it will',\n \"it'll've\": 'it will have',\n \"it'd\": 'it would',\n \"it'd've\": 'it would have',\n 'kinda': 'kind of',\n \"let's\": 'let us',\n 'luv': 'love',\n \"ma'am\": 'madam',\n \"may've\": 'may have',\n \"mayn't\": 'may not',\n \"might've\": 'might have',\n \"mightn't\": 'might not',\n \"mightn't've\": 'might not have',\n \"must've\": 'must have',\n \"mustn't\": 'must not',\n \"mustn't've\": 'must not have',\n \"needn't\": 'need not',\n \"needn't've\": 'need not have',\n \"ne'er\": 'never',\n \"o'\": 'of',\n \"o'clock\": 'of the clock',\n \"ol'\": 'old',\n \"oughtn't\": 'ought not',\n \"oughtn't've\": 'ought not have',\n \"o'er\": 'over',\n \"shan't\": 'shall not',\n \"sha'n't\": 'shall not',\n \"shalln't\": 'shall not',\n \"shan't've\": 'shall not have',\n \"she's\": 'she is',\n \"she'll\": 'she will',\n \"she'd\": 'she would',\n \"she'd've\": 'she would have',\n \"should've\": 'should have',\n \"shouldn't\": 'should not',\n \"shouldn't've\": 'should not have',\n \"so've\": 'so have',\n \"so's\": 'so is',\n \"somebody's\": 'somebody is',\n \"someone's\": 'someone is',\n \"something's\": 'something is',\n 'sux': 'sucks',\n \"that're\": 'that are',\n \"that's\": 'that is',\n \"that'll\": 'that will',\n \"that'd\": 'that would',\n \"that'd've\": 'that would have',\n 'em': 'them',\n \"there're\": 'there are',\n \"there's\": 'there is',\n \"there'll\": 'there will',\n \"there'd\": 'there would',\n \"there'd've\": 'there would have',\n \"these're\": 'these are',\n \"they're\": 'they are',\n \"they've\": 'they have',\n \"they'll\": 'they will',\n \"they'll've\": 'they will have',\n \"they'd\": 'they would',\n \"they'd've\": 'they would have',\n \"this's\": 'this is',\n \"those're\": 'those are',\n \"to've\": 'to have',\n 'wanna': 'want to',\n \"wasn't\": 'was not',\n \"we're\": 'we are',\n \"we've\": 'we have',\n \"we'll\": 'we will',\n \"we'll've\": 'we will have',\n \"we'd\": 'we would',\n \"we'd've\": 'we would have',\n \"weren't\": 'were not',\n \"what're\": 'what are',\n \"what'd\": 'what did',\n \"what've\": 'what have',\n \"what's\": 'what is',\n \"what'll\": 'what will',\n \"what'll've\": 'what will have',\n \"when've\": 'when have',\n \"when's\": 'when is',\n \"where're\": 'where are',\n \"where'd\": 'where did',\n \"where've\": 'where have',\n \"where's\": 'where is',\n \"which's\": 'which is',\n \"who're\": 'who are',\n \"who've\": 'who have',\n \"who's\": 'who is',\n \"who'll\": 'who will',\n \"who'll've\": 'who will have',\n \"who'd\": 'who would',\n \"who'd've\": 'who would have',\n \"why're\": 'why are',\n \"why'd\": 'why did',\n \"why've\": 'why have',\n \"why's\": 'why is',\n \"will've\": 'will have',\n \"won't\": 'will not',\n \"won't've\": 'will not have',\n \"would've\": 'would have',\n \"wouldn't\": 'would not',\n \"wouldn't've\": 'would not have',\n \"y'all\": 'you all',\n \"y'all're\": 'you all are',\n \"y'all've\": 'you all have',\n \"y'all'd\": 'you all would',\n \"y'all'd've\": 'you all would have',\n \"you're\": 'you are',\n \"you've\": 'you have',\n \"you'll've\": 'you shall have',\n \"you'll\": 'you will',\n \"you'd\": 'you would',\n \"you'd've\": 'you would have',\n 'jan.': 'january',\n 'feb.': 'february',\n 'mar.': 'march',\n 'apr.': 'april',\n 'jun.': 'june',\n 'jul.': 'july',\n 'aug.': 'august',\n 'sep.': 'september',\n 'oct.': 'october',\n 'nov.': 'november',\n 'dec.': 'december',\n 'I\u2019m': 'I am',\n 'I\u2019m\u2019a': 'I am about to',\n 'I\u2019m\u2019o': 'I am going to',\n 'I\u2019ve': 'I have',\n 'I\u2019ll': 'I will',\n 'I\u2019ll\u2019ve': 'I will have',\n 'I\u2019d': 'I would',\n 'I\u2019d\u2019ve': 'I would have',\n 'i\u2019m': 'i am',\n 'i\u2019m\u2019a': 'i am about to',\n 'i\u2019m\u2019o': 'i am going to',\n 'i\u2019ve': 'i have',\n 'i\u2019ll': 'i will',\n 'i\u2019ll\u2019ve': 'i will have',\n 'i\u2019d': 'i would',\n 'i\u2019d\u2019ve': 'i would have',\n 'amn\u2019t': 'am not',\n 'ain\u2019t': 'are not',\n 'aren\u2019t': 'are not',\n '\u2019cause': 'because',\n 'can\u2019t': 'can not',\n 'can\u2019t\u2019ve': 'can not have',\n 'could\u2019ve': 'could have',\n 'couldn\u2019t': 'could not',\n 'couldn\u2019t\u2019ve': 'could not have',\n 'daren\u2019t': 'dare not',\n 'daresn\u2019t': 'dare not',\n 'dasn\u2019t': 'dare not',\n 'doesn\u2019t': 'does not',\n 'e\u2019er': 'ever',\n 'everyone\u2019s': 'everyone is',\n 'gon\u2019t': 'go not',\n 'hadn\u2019t': 'had not',\n 'hadn\u2019t\u2019ve': 'had not have',\n 'hasn\u2019t': 'has not',\n 'haven\u2019t': 'have not',\n 'he\u2019ve': 'he have',\n 'he\u2019s': 'he is',\n 'he\u2019ll': 'he will',\n 'he\u2019ll\u2019ve': 'he will have',\n 'he\u2019d': 'he would',\n 'he\u2019d\u2019ve': 'he would have',\n 'here\u2019s': 'here is',\n 'how\u2019re': 'how are',\n 'how\u2019d': 'how did',\n 'how\u2019d\u2019y': 'how do you',\n 'how\u2019s': 'how is',\n 'how\u2019ll': 'how will',\n 'isn\u2019t': 'is not',\n 'it\u2019s': 'it is',\n '\u2019tis': 'it is',\n '\u2019twas': 'it was',\n 'it\u2019ll': 'it will',\n 'it\u2019ll\u2019ve': 'it will have',\n 'it\u2019d': 'it would',\n 'it\u2019d\u2019ve': 'it would have',\n 'let\u2019s': 'let us',\n 'ma\u2019am': 'madam',\n 'may\u2019ve': 'may have',\n 'mayn\u2019t': 'may not',\n 'might\u2019ve': 'might have',\n 'mightn\u2019t': 'might not',\n 'mightn\u2019t\u2019ve': 'might not have',\n 'must\u2019ve': 'must have',\n 'mustn\u2019t': 'must not',\n 'mustn\u2019t\u2019ve': 'must not have',\n 'needn\u2019t': 'need not',\n 'needn\u2019t\u2019ve': 'need not have',\n 'ne\u2019er': 'never',\n 'o\u2019': 'of',\n 'o\u2019clock': 'of the clock',\n 'ol\u2019': 'old',\n 'oughtn\u2019t': 'ought not',\n 'oughtn\u2019t\u2019ve': 'ought not have',\n 'o\u2019er': 'over',\n 'shan\u2019t': 'shall not',\n 'sha\u2019n\u2019t': 'shall not',\n 'shalln\u2019t': 'shall not',\n 'shan\u2019t\u2019ve': 'shall not have',\n 'she\u2019s': 'she is',\n 'she\u2019ll': 'she will',\n 'she\u2019d': 'she would',\n 'she\u2019d\u2019ve': 'she would have',\n 'should\u2019ve': 'should have',\n 'shouldn\u2019t': 'should not',\n 'shouldn\u2019t\u2019ve': 'should not have',\n 'so\u2019ve': 'so have',\n 'so\u2019s': 'so is',\n 'somebody\u2019s': 'somebody is',\n 'someone\u2019s': 'someone is',\n 'something\u2019s': 'something is',\n 'that\u2019re': 'that are',\n 'that\u2019s': 'that is',\n 'that\u2019ll': 'that will',\n 'that\u2019d': 'that would',\n 'that\u2019d\u2019ve': 'that would have',\n 'there\u2019re': 'there are',\n 'there\u2019s': 'there is',\n 'there\u2019ll': 'there will',\n 'there\u2019d': 'there would',\n 'there\u2019d\u2019ve': 'there would have',\n 'these\u2019re': 'these are',\n 'they\u2019re': 'they are',\n 'they\u2019ve': 'they have',\n 'they\u2019ll': 'they will',\n 'they\u2019ll\u2019ve': 'they will have',\n 'they\u2019d': 'they would',\n 'they\u2019d\u2019ve': 'they would have',\n 'this\u2019s': 'this is',\n 'those\u2019re': 'those are',\n 'to\u2019ve': 'to have',\n 'wasn\u2019t': 'was not',\n 'we\u2019re': 'we are',\n 'we\u2019ve': 'we have',\n 'we\u2019ll': 'we will',\n 'we\u2019ll\u2019ve': 'we will have',\n 'we\u2019d': 'we would',\n 'we\u2019d\u2019ve': 'we would have',\n 'weren\u2019t': 'were not',\n 'what\u2019re': 'what are',\n 'what\u2019d': 'what did',\n 'what\u2019ve': 'what have',\n 'what\u2019s': 'what is',\n 'what\u2019ll': 'what will',\n 'what\u2019ll\u2019ve': 'what will have',\n 'when\u2019ve': 'when have',\n 'when\u2019s': 'when is',\n 'where\u2019re': 'where are',\n 'where\u2019d': 'where did',\n 'where\u2019ve': 'where have',\n 'where\u2019s': 'where is',\n 'which\u2019s': 'which is',\n 'who\u2019re': 'who are',\n 'who\u2019ve': 'who have',\n 'who\u2019s': 'who is',\n 'who\u2019ll': 'who will',\n 'who\u2019ll\u2019ve': 'who will have',\n 'who\u2019d': 'who would',\n 'who\u2019d\u2019ve': 'who would have',\n 'why\u2019re': 'why are',\n 'why\u2019d': 'why did',\n 'why\u2019ve': 'why have',\n 'why\u2019s': 'why is',\n 'will\u2019ve': 'will have',\n 'won\u2019t': 'will not',\n 'won\u2019t\u2019ve': 'will not have',\n 'would\u2019ve': 'would have',\n 'wouldn\u2019t': 'would not',\n 'wouldn\u2019t\u2019ve': 'would not have',\n 'y\u2019all': 'you all',\n 'y\u2019all\u2019re': 'you all are',\n 'y\u2019all\u2019ve': 'you all have',\n 'y\u2019all\u2019d': 'you all would',\n 'y\u2019all\u2019d\u2019ve': 'you all would have',\n 'you\u2019re': 'you are',\n 'you\u2019ve': 'you have',\n 'you\u2019ll\u2019ve': 'you shall have',\n 'you\u2019ll': 'you will',\n 'you\u2019d': 'you would',\n 'you\u2019d\u2019ve': 'you would have'\n}\n\ndef clean_contractions(text):\n#     text = text.lower()\n    specials = [\"\u2019\", \"\u2018\", \"\u00b4\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    \n    text = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in text.split(\" \")])\n    return text","7b9b8b97":"lemmatizer = WordNetLemmatizer()\ndef lemma_text(x):\n    x = x.split()\n    x = [lemmatizer.lemmatize(word) for word in x]\n    x = ' '.join(x)\n    return x","c577248c":"def data_cleaning(x):\n  x = clean_tag(x)\n  x = clean_punct(x)\n  x = correct_mispell(x)\n  x = remove_stopwords(x)\n  x = clean_contractions(x)\n  x = lemma_text(x)\n  return x","82f796bf":"def Preprocess(doc):\n    corpus=[]\n    for text in tqdm(doc):\n        text=clean_contractions(text)\n        text=correct_mispell(text)\n        text=re.sub(r'[^a-z0-9A-Z]',\" \",text)\n        text=re.sub(r'[0-9]{1}',\"#\",text)\n        text=re.sub(r'[0-9]{2}','##',text)\n        text=re.sub(r'[0-9]{3}','###',text)\n        text=re.sub(r'[0-9]{4}','####',text)\n        text=re.sub(r'[0-9]{5,}','#####',text)\n        corpus.append(text)\n    return corpus","5129aa8d":"tqdm.pandas(desc=\"progress-bar\")\ndf_test['question_text_cleaned'] = df_test['question_text'].progress_map(lambda x: data_cleaning(x))\ndf_train['question_text_cleaned'] = df_train['question_text'].progress_map(lambda x: data_cleaning(x))","e8c41990":"df_train.head(5)","30c5be5a":"%%time\n### unzipping all the pretrained embeddings\n!unzip ..\/input\/quora-insincere-questions-classification\/embeddings.zip","9ac91845":"!du -h .\/","31c1325c":"%%time\n\n### Loading t\u1eadp Google News Pretrained Embeddings v\u00e0o b\u1ed9 nh\u1edb\nfile_name=\".\/GoogleNews-vectors-negative300\/GoogleNews-vectors-negative300.bin\"\nmodel_embed=KeyedVectors.load_word2vec_format(file_name,binary=True)\n# model_embed = load_embed1('.\/glove.840B.300d\/glove.840B.300d.txt')\n\n### X\u00e2y d\u1ef1ng t\u1eadp t\u1eeb v\u1ef1ng d\u1ef1a tr\u00ean d\u1eef li\u1ec7u c\u1ee7a t\u1eadp Google News\ndef vocab_build(corpus):\n    vocab={}\n    for text in tqdm(corpus):\n        for word in text.split():\n            try:\n                vocab[word]+=1\n            except KeyError:\n                vocab[word]=1\n    return vocab\n\n\n### Ki\u1ec3m tra t\u1eadp Vocabulary xem t\u1eadp vocab \u0111\u00f3 bao ph\u1ee7 bao nhi\u00eau ph\u1ea7n tr\u0103m t\u1eadp d\u1eef li\u1ec7u c\u1ee7a m\u00ecnh\ndef check_voc(vocab,model):\n    embed_words=[]\n    out_vocab={}\n    total_words=0\n    total_text=0\n    for i in tqdm(vocab):\n        try:\n            vec=model[i]\n            embed_words.append(vec)\n            total_words+=vocab[i]\n        except KeyError:\n            out_vocab[i]=vocab[i]\n            total_text+=vocab[i]\n    print(\"The {:.2f}% of vocabularies have Covered of corpus\".format(100*len(embed_words)\/len(vocab)))\n    print(\"The {:.2f}% of total text had coverded \".format((100*total_words\/(total_words+total_text))))\n    return out_vocab","8f3607c1":"### x\u00e2y t\u1eadp vocab v\u00e0 ki\u1ec3m tra \u0111\u1ed9 ph\u1ee7 c\u1ee7a t\u1eadp vocab v\u1edbi d\u1eef li\u1ec7u \u0111\u00e3 \u0111\u01b0\u1ee3c x\u1eed l\u00fd\ntotal_text=pd.concat([df_train.question_text_cleaned,df_test.question_text_cleaned])\nvocabulary=vocab_build(total_text)\noov=check_voc(vocabulary,model_embed) #oov: out of vocab","2b3afbd9":"df_test['question_text_cleaned_2'] = Preprocess(df_test['question_text'])\ndf_train['question_text_cleaned_2'] = Preprocess(df_train['question_text'])\ntotal_text_2=pd.concat([df_train.question_text_cleaned_2,df_test.question_text_cleaned_2])\nvocabulary2=vocab_build(total_text_2)\noov2=check_voc(vocabulary2, model_embed)","046d57f4":"sort_oov=dict(sorted(oov2.items(), key=operator.itemgetter(1),reverse=True))\ndict(list(sort_oov.items())[:50])","87e068b9":"del oov, oov2,sort_oov,total_text,total_text_2\ngc.collect()","c22ee9cb":"def get_word_index(vocab):\n    word_index=dict((w,i+1) for i,w in enumerate(vocab.keys()))\n    return word_index\ndef fit_one_hot(word_index,corpus):\n    sent=[]\n    for text in tqdm(corpus):\n        li=[]\n        for word in text.split():\n            try:\n                li.append(word_index[word])\n            except KeyError:\n                li.append(0)\n        sent.append(li)\n    return sent","7dacbbb8":"train,val=train_test_split(df_train,test_size=0.2,stratify=df_train.target,random_state=123)\nvocab_size=len(vocabulary2)+1\nmax_len=40\n\nword_index=get_word_index(vocabulary2)\n### Chu\u1ea9n b\u1ecb d\u1eef li\u1ec7u \u0111\u00e3 \u0111\u01b0\u1ee3c x\u1eed l\u00fd\ntrain_text=train['question_text_cleaned_2']\nval_text=val['question_text_cleaned_2']\ntest_text=df_test['question_text_cleaned_2']\n\n### m\u00e3 h\u00f3a c\u00e2u trong t\u1eadp train sang d\u1ea1ng onehot cho d\u1ec5 x\u1eed l\u00fd\nencodes=fit_one_hot(word_index,train_text)\ntrain_padded=pad_sequences(encodes,maxlen=max_len,padding=\"post\")\n\n### m\u00e3 h\u00f3a c\u00e2u trong t\u1eadp validation sang d\u1ea1ng onehot cho d\u1ec5 x\u1eed l\u00fd\nencodes_=fit_one_hot(word_index,val_text)\nval_padded=pad_sequences(encodes_,maxlen=max_len,padding=\"post\")\n\n### m\u00e3 h\u00f3a c\u00e2u trong t\u1eadp test sang d\u1ea1ng onehot cho d\u1ec5 x\u1eed l\u00fd\nencodes__=fit_one_hot(word_index,test_text)\ntest_padded=pad_sequences(encodes__,maxlen=max_len,padding=\"post\")","d34b0fd8":"count=0\nembedding_mat=np.zeros((vocab_size,300))\nfor word,i in tqdm(word_index.items()):\n    try:\n        vec=model_embed[word]\n        embedding_mat[i]=vec\n    except KeyError:\n        count+=1\n        continue\n\nprint(\"Number of Out of Vocabulary\",count)","8676ce7b":"def get_model_origin(matrix):\n    inp = Input(shape=(max_len,))\n    x = Embedding(vocab_size,300,weights=[matrix],input_length=max_len,trainable=False)(inp)\n    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n    x = Conv1D(64,3,activation=\"relu\")(x)\n    x = GlobalMaxPool1D()(x)\n    x = Dense(128, activation=\"relu\")(x)\n    x = Dropout(0.2)(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    return model","08955e7c":"opt=Adam(learning_rate=0.001)\nBATCH_SIZE = 1024\nbin_loss=tf.keras.losses.BinaryCrossentropy(from_logits=False, label_smoothing=0, name='binary_crossentropy')\n\n### X\u00e1c \u0111\u1ecbnh \u0111i\u1ec3m callback \u0111\u1ec3 gi\u1ea3m learning rate, v\u00e0 restore l\u1ea1i tr\u1ecdng s\u1ed1 t\u1ed1t nh\u1ea5t k\u1ec1 tr\u01b0\u1edbc \nearly_stopping=tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",patience=3,mode=\"min\",restore_best_weights=True)\n\n### Gi\u1ea3m learning rate khi model kh\u00f4ng \u0111\u01b0\u1ee3c c\u1ea3i thi\u00ean (c\u00e0ng h\u1ecdc c\u00e0ng ngu)\nreduce_lr=tf.keras.callbacks.ReduceLROnPlateau( monitor=\"val_loss\", factor=0.2, patience=2, verbose=1, mode=\"auto\")\n\nmy_callbacks=[early_stopping,reduce_lr]","e678f49d":"strategy = None\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n    strategy = tf.distribute.TPUStrategy(tpu)\n    print('Use TPU')\nexcept ValueError:\n    if len(tf.config.list_physical_devices('GPU')) > 0:\n        strategy = tf.distribute.MirroredStrategy()\n        print('Use GPU')\n    else:\n        strategy = tf.distribute.get_strategy()\n        print('Use CPU')","96cc5e90":"with strategy.scope():\n    google_model = get_model_origin(embedding_mat)\n    google_model.compile(loss=bin_loss, optimizer=opt, metrics=['accuracy'])\nhistory=google_model.fit(train_padded, train.target, batch_size=BATCH_SIZE, epochs=30, validation_data=(val_padded, val.target),callbacks=my_callbacks)","ebbeb040":"# summarize history for loss\nplt.figure(figsize=(15, 5)).add_subplot(1, 2, 1)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n# plt.show()\n# summarize history for accuracy\nplt.figure(figsize=(15, 5)).add_subplot(1, 2, 2)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","6136ad09":"google_y_pre=google_model.predict(val_padded, verbose=1)\nbest_score = 0\nbest_thresh = 0\nfor thresh in np.arange(0.1,0.5,0.01):\n    if(best_score < metrics.f1_score(val.target,(google_y_pre>thresh).astype(int))):\n        best_score = metrics.f1_score(val.target,(google_y_pre>thresh))\n        best_thresh = round(thresh, 2)\n    print(\"threshold {0:2.2f} f1 score:{1:2.3f}\".format(thresh,metrics.f1_score(val.target,(google_y_pre>thresh).astype(int))))\nprint(\"\\033[1mBest result {0:2.3f} in thresh_hold {1:2.2f}\\033[0m\".format(best_score, best_thresh))","1dc6755e":"del model_embed, history, best_score, best_thresh, google_model, google_y_pre\ngc.collect()","3eb2c540":"def get_model(matrix):\n    inp = Input(shape=(max_len,))\n    x = Embedding(vocab_size, 300, weights=[matrix], trainable=False)(inp)\n    x = SpatialDropout1D(0.3)(x)\n    x1 = Bidirectional(LSTM(256, return_sequences=True))(x)\n    x2 = Bidirectional(GRU(128, return_sequences=True))(x1)\n    max_pool1 = GlobalMaxPooling1D()(x1)\n    max_pool2 = GlobalMaxPooling1D()(x2)\n    conc = Concatenate()([max_pool1, max_pool2])\n    predictions = Dense(1, activation='sigmoid')(conc)\n    model = Model(inputs=inp, outputs=predictions)\n    adam = optimizers.Adam(lr=0.001)\n    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n    return model","e7271cfb":"strategy = None\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n    strategy = tf.distribute.TPUStrategy(tpu)\n    print('Use TPU')\nexcept ValueError:\n    if len(tf.config.list_physical_devices('GPU')) > 0:\n        strategy = tf.distribute.MirroredStrategy()\n        print('Use GPU')\n    else:\n        strategy = tf.distribute.get_strategy()\n        print('Use CPU')","a90e6be1":"with strategy.scope():\n    google_model = get_model(embedding_mat)\n    google_model.compile(loss=bin_loss, optimizer=Adam(lr=0.001), metrics=['accuracy'])\nhistory=google_model.fit(train_padded, train.target, batch_size=BATCH_SIZE, epochs=30, validation_data=(val_padded, val.target),callbacks=my_callbacks)","6f2bbf2a":"# summarize history for loss\nplt.figure(figsize=(15, 5)).add_subplot(1, 2, 1)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n# plt.show()\n# summarize history for accuracy\nplt.figure(figsize=(15, 5)).add_subplot(1, 2, 2)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","99c21090":"google_y_pre=google_model.predict(val_padded, verbose=1)\nbest_score = 0\nbest_thresh = 0\nfor thresh in np.arange(0.1,0.5,0.01):\n    if(best_score < metrics.f1_score(val.target,(google_y_pre>thresh).astype(int))):\n        best_score = metrics.f1_score(val.target,(google_y_pre>thresh))\n        best_thresh = round(thresh, 2)\n    print(\"threshold {0:2.2f} f1 score:{1:2.3f}\".format(thresh,metrics.f1_score(val.target,(google_y_pre>thresh).astype(int))))\nprint(\"\\033[1mBest result {0:2.3f} in thresh_hold {1:2.2f}\\033[0m\".format(best_score, best_thresh))","bfe34183":"threshold=best_thresh\ngoogle_y_test_pre=google_model.predict(test_padded, batch_size=BATCH_SIZE, verbose=1)\ngoogle_y_test_pre=(google_y_test_pre>thresh).astype(int)","8975649c":"del embedding_mat, history, best_score, best_thresh\ngc.collect()","407224b3":"def load_embed(file):\n    def get_coefs(word,*arr):\n        return word, np.asarray(arr, dtype='float32')\n\n    if file == '..\/input\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec':\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n    else:\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n\n    return embeddings_index","68b00906":"!tree -h .\/","ee25e652":"glove_path = '.\/glove.840B.300d\/glove.840B.300d.txt'\nparagram_path = '.\/paragram_300_sl999\/paragram_300_sl999.txt'\nwiki_path = '.\/wiki-news-300d-1M\/wiki-news-300d-1M.vec'","759eac57":"%%time\nglove_embed = load_embed(glove_path)\nprint(\"\\033[1mGlove Coverage: \\033[0m]\")\noov_glove = check_voc(vocabulary2, glove_embed)","dd629e54":"strategy = None\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n    strategy = tf.distribute.TPUStrategy(tpu)\n    print('Use TPU')\nexcept ValueError:\n    if len(tf.config.list_physical_devices('GPU')) > 0:\n        strategy = tf.distribute.MirroredStrategy()\n        print('Use GPU')\n    else:\n        strategy = tf.distribute.get_strategy()\n        print('Use CPU')","343a408f":"count=0\nglove_embedding_mat=np.zeros((vocab_size,300))\nfor word,i in tqdm(word_index.items()):\n    try:\n        vec=glove_embed[word]\n        glove_embedding_mat[i]=vec\n    except KeyError:\n        count+=1\n        continue\n\nprint(\"Number of Out of Vocabulary\",count)","8a21257c":"with strategy.scope():\n    glove_model = get_model(glove_embedding_mat)\n    glove_model.compile(loss=bin_loss, optimizer=Adam(lr=0.001), metrics=['accuracy'])\nglove_history=glove_model.fit(train_padded, train.target, batch_size=BATCH_SIZE, epochs=30, validation_data=(val_padded, val.target),callbacks=my_callbacks)","80e2bcc1":"# summarize history for loss\nplt.figure(figsize=(15, 5)).add_subplot(1, 2, 1)\nplt.plot(glove_history.history['loss'])\nplt.plot(glove_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n# plt.show()\n# summarize history for accuracy\nplt.figure(figsize=(15, 5)).add_subplot(1, 2, 2)\nplt.plot(glove_history.history['accuracy'])\nplt.plot(glove_history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","3bb584d3":"glove_y_pre=glove_model.predict(val_padded, verbose=1)\nbest_score = 0\nbest_thresh = 0\nfor thresh in np.arange(0.1,0.5,0.01):\n    if(best_score < metrics.f1_score(val.target,(glove_y_pre>thresh).astype(int))):\n        best_score = metrics.f1_score(val.target,(glove_y_pre>thresh))\n        best_thresh = round(thresh, 2)\n    print(\"threshold {0:2.2f} f1 score:{1:2.3f}\".format(thresh,metrics.f1_score(val.target,(glove_y_pre>thresh).astype(int))))\nprint(\"\\033[1mBest result {0:2.3f} in thresh_hold {1:2.2f}\\033[0m\".format(best_score, best_thresh))","e1b8b973":"threshold=best_thresh\nglove_y_test_pre=glove_model.predict(test_padded, batch_size=BATCH_SIZE, verbose=1)\nglove_y_test_pre=(glove_y_test_pre>thresh).astype(int)","e7a013b3":"del glove_embed, glove_embedding_mat, glove_history, best_score, best_thresh\ngc.collect()","6c262631":"strategy = None\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n    strategy = tf.distribute.TPUStrategy(tpu)\n    print('Use TPU')\nexcept ValueError:\n    if len(tf.config.list_physical_devices('GPU')) > 0:\n        strategy = tf.distribute.MirroredStrategy()\n        print('Use GPU')\n    else:\n        strategy = tf.distribute.get_strategy()\n        print('Use CPU')","8fb3301e":"%%time\nparagram_embed = load_embed(paragram_path)\nprint(\"\\033[1mParagram Coverage: \\033[0m]\")\noov_paragram = check_voc(vocabulary2, paragram_embed)","71e22c33":"count=0\npara_embedding_mat=np.zeros((vocab_size,300))\nfor word,i in tqdm(word_index.items()):\n    try:\n        vec=paragram_embed[word.lower()]\n        para_embedding_mat[i]=vec\n    except KeyError:\n        count+=1\n        continue\n\nprint(\"Number of Out of Vocabulary\",count)","42ce6ef9":"with strategy.scope():\n    para_model = get_model(para_embedding_mat)\n    para_model.compile(loss=bin_loss, optimizer=Adam(lr=0.001), metrics=['accuracy'])\npara_history=para_model.fit(train_padded, train.target, batch_size=BATCH_SIZE, epochs=30, validation_data=(val_padded, val.target),callbacks=my_callbacks)","268c40e8":"# summarize history for loss\nplt.figure(figsize=(15, 5)).add_subplot(1, 2, 1)\nplt.plot(para_history.history['loss'])\nplt.plot(para_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n# plt.show()\n# summarize history for accuracy\nplt.figure(figsize=(15, 5)).add_subplot(1, 2, 2)\nplt.plot(para_history.history['accuracy'])\nplt.plot(para_history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","723c1dd9":"para_y_pre=para_model.predict(val_padded, verbose=1)\nbest_score = 0\nbest_thresh = 0\nfor thresh in np.arange(0.1,0.5,0.01):\n    if(best_score < metrics.f1_score(val.target,(para_y_pre>thresh).astype(int))):\n        best_score = metrics.f1_score(val.target,(para_y_pre>thresh))\n        best_thresh = round(thresh, 2)\n    print(\"threshold {0:2.2f} f1 score:{1:2.3f}\".format(thresh,metrics.f1_score(val.target,(para_y_pre>thresh).astype(int))))\nprint(\"\\033[1mBest result {0:2.3f} in thresh_hold {1:2.2f}\\033[0m\".format(best_score, best_thresh))","daf583f4":"threshold=best_thresh\npara_y_test_pre=para_model.predict(test_padded, batch_size=BATCH_SIZE, verbose=1)\npara_y_test_pre=(para_y_test_pre>thresh).astype(int)","ebc262e7":"del paragram_embed, para_embedding_mat, para_history, best_score, best_thresh\ngc.collect()","e6d04c06":"strategy = None\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n    strategy = tf.distribute.TPUStrategy(tpu)\n    print('Use TPU')\nexcept ValueError:\n    if len(tf.config.list_physical_devices('GPU')) > 0:\n        strategy = tf.distribute.MirroredStrategy()\n        print('Use GPU')\n    else:\n        strategy = tf.distribute.get_strategy()\n        print('Use CPU')","7c66be26":"%%time\nwiki_embed = load_embed(wiki_path)\nprint(\"\\033[1mWiki Coverage: \\033[0m]\")\noov_wiki = check_voc(vocabulary2, wiki_embed)","f953affe":"count=0\nwiki_embedding_mat=np.zeros((vocab_size,300))\nfor word,i in tqdm(word_index.items()):\n    try:\n        vec=wiki_embed[word]\n        wiki_embedding_mat[i]=vec\n    except KeyError:\n        count+=1\n        continue\n\nprint(\"Number of Out of Vocabulary\",count)","4e9f4c86":"with strategy.scope():\n    wiki_model = get_model(wiki_embedding_mat)\n    wiki_model.compile(loss=bin_loss, optimizer=Adam(lr=0.001), metrics=['accuracy'])\nwiki_history=wiki_model.fit(train_padded, train.target, batch_size=BATCH_SIZE, epochs=30, validation_data=(val_padded, val.target),callbacks=my_callbacks)","58315c03":"# summarize history for loss\nplt.figure(figsize=(15, 5)).add_subplot(1, 2, 1)\nplt.plot(wiki_history.history['loss'])\nplt.plot(wiki_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\n# plt.show()\n# summarize history for accuracy\nplt.figure(figsize=(15, 5)).add_subplot(1, 2, 2)\nplt.plot(wiki_history.history['accuracy'])\nplt.plot(wiki_history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","f35cbd96":"wiki_y_pre=wiki_model.predict(val_padded, verbose=1)\nbest_score = 0\nbest_thresh = 0\nfor thresh in np.arange(0.1,0.5,0.01):\n    if(best_score < metrics.f1_score(val.target,(wiki_y_pre>thresh).astype(int))):\n        best_score = metrics.f1_score(val.target,(wiki_y_pre>thresh))\n        best_thresh = round(thresh, 2)\n    print(\"threshold {0:2.2f} f1 score:{1:2.3f}\".format(thresh,metrics.f1_score(val.target,(wiki_y_pre>thresh).astype(int))))\nprint(\"\\033[1mBest result {0:2.3f} in thresh_hold {1:2.2f}\\033[0m\".format(best_score, best_thresh))","d525b2ab":"threshold=best_thresh\nwiki_y_test_pre=wiki_model.predict(test_padded, batch_size=BATCH_SIZE, verbose=1)\nwiki_y_test_pre=(wiki_y_test_pre>thresh).astype(int)","5fdaced0":"del wiki_embed, wiki_embedding_mat, wiki_history, best_score, best_thresh\ngc.collect()","9b17c3df":"y_pre=0.25*(google_y_pre + glove_y_pre + para_y_pre + wiki_y_pre)\n# y_pre=0.20*google_y_pre + 0.35*glove_y_pre + 0.15*para_y_pre + 0.30*wiki_y_pre\nbest_score = 0\nbest_thresh = 0\nfor thresh in np.arange(0.1,0.5,0.01):\n    if(best_score < metrics.f1_score(val.target,(y_pre>thresh).astype(int))):\n        best_score = metrics.f1_score(val.target,(y_pre>thresh))\n        best_thresh = round(thresh, 2)\n    print(\"threshold {0:2.2f} f1 score:{1:2.3f}\".format(thresh,metrics.f1_score(val.target,(y_pre>thresh).astype(int))))\nprint(\"\\033[1mBest result {0:2.3f} in thresh_hold {1:2.2f}\\033[0m\".format(best_score, best_thresh))","46f4cf0e":"# y_test_pre = 0.25 * (google_y_test_pre + glove_y_test_pre + para_y_test_pre + wiki_y_test_pre)\ny_test_pre = 0.2*google_y_test_pre + 0.3*glove_y_test_pre + 0.2*para_y_test_pre + 0.3*wiki_y_test_pre\ny_test_pre=(y_test_pre>thresh).astype(int)\n### T\u1ea1o File submission\nsubmit=pd.DataFrame()\nsubmit[\"qid\"]=df_test.qid\nsubmit[\"prediction\"]=y_test_pre\nsubmit.to_csv(\"submission.csv\",index=False)\nprint(\"ok\")","1c02270b":"C\u00f3 th\u1ec3 th\u1ea5y \u0111\u01b0\u1ee3c r\u1eb1ng s\u1ed1 t\u1eeb ```out_of_vocab``` gi\u1ea3m \u0111i kh\u00e1 nhi\u1ec1u, \u1edf m\u1ee9c 70336 t\u1eeb, v\u00e0 qua \u0111\u00f3 c\u00f3 th\u1ec3 k\u1ef3 v\u1ecdng m\u00f4 h\u00ecnh \u0111em l\u1ea1i \u0111\u1ed9 ch\u00ednh x\u00e1c cao h\u01a1n.<br>\nV\u00e0 em v\u1eabn ch\u01b0a c\u00f3 c\u00e1ch \u0111\u1ec3 x\u1eed l\u00fd nh\u1eefng t\u1eeb ```out_of_vocab``` nh\u01b0 n\u00e0y \ud83e\udd7a\ud83e\udd7a\ud83e\udd7a","3aed8198":"S\u1ed1 l\u01b0\u1ee3ng t\u1eeb c\u1ee7a anh **Wiki** c\u0169ng \u1edf m\u1ee9c kh\u00e1 cao khi g\u1ea7n ti\u1ec7m c\u1eadn v\u1edbi anh **Google News**, v\u1eady c\u00f3 th\u1ec3 l\u00e0 ma tr\u1eadn embeddings s\u1ebd c\u00f3 nhi\u1ec1u t\u1eeb kh\u00f4ng t\u00ecm \u0111\u01b0\u1ee3c v\u00e0 hi\u1ec7u n\u0103ng model c\u0169ng s\u1ebd kh\u00f4ng t\u1ed1t b\u1eb1ng anh **Glove**","e35b00b6":"### **b. Kh\u1ea3o s\u00e1t d\u1eef li\u1ec7u trong t\u1eadp **test**** ","35c14c19":"So v\u1edbi 2 file embeddings \u1edf tr\u00ean l\u00e0 **Google News** v\u00e0 **Glove** th\u00ec s\u1ed1 l\u01b0\u1ee3ng t\u1eeb n\u1eb1m ngo\u00e0i t\u1eadp vocab ```out_of_vocab``` t\u0103ng l\u00ean kh\u00e1 nhi\u1ec1u khi \u0111\u1ea1t con s\u1ed1 157867 t\u1eeb n\u1ebfu kh\u00f4ng chu\u1ea9n ho\u00e1 h\u1ebft sang d\u1ea1ng lowercase<br>\nB\u00ean c\u1ea1nh \u0111\u00f3 th\u00ec **Paragram** cho \u0111\u1ed9 ph\u1ee7 t\u1ec7 nh\u1ea5t v\u1edbi **81.11%**. n\u1ebfu kh\u00f4ng chu\u1ea9n ho\u00e1 sang lowercase<br>\nC\u00f3 th\u1ec3 k\u1ef3 v\u1ecdng v\u1edbi ma tr\u1eadn embeddings sinh ra t\u1eeb **Paragram** th\u00ec m\u00f4 h\u00ecnh s\u1ebd \u0111\u1ea1t hi\u1ec7u qu\u1ea3 t\u1ec7 nh\u1ea5t.<br>\nTuy nhi\u00ean khi chu\u1ea9n ho\u00e1 sang lowercase v\u00e0 lookup th\u00ec \u0111\u1ed9 ph\u1ee7 c\u1ee7a **Paragram** t\u0103ng l\u00ean r\u00f5 r\u1ec7t v\u00e0 c\u00f3 s\u1ed1 t\u1eeb ```out_of_vocab``` gi\u1ea3m m\u1ea1nh xu\u1ed1ng c\u00f2n 61842 t\u1eeb, do v\u1eady ma tr\u1eadn embeddings s\u1ebd gi\u00fap model \u0111\u1ea1t hi\u1ec7u qu\u1ea3 cao h\u01a1n","9cb81d53":"\u1ede trong b\u1ed9 d\u1eef li\u1ec7u c\u00f3 3 tr\u01b0\u1eddng, v\u00e0 kh\u1ea3 n\u0103ng d\u1eef li\u1ec7u train n\u1eb1m \u1edf tr\u01b0\u1eddng **```question_text```** <br>\nV\u1eady ta s\u1ebd th\u1eed kh\u1ea3o s\u00e1t d\u1eef li\u1ec7u trong tr\u01b0\u1eddng **```question_text```**","bbfa5a97":"#### **Reference**: https:\/\/www.kaggle.com\/canming\/ensemble-mean-iii-64-36\n**C\u00e1ch l\u00e0m:**<br>\nS\u1eed d\u1ee5ng th\u01b0 vi\u1ec7n x\u1eed l\u00fd ng\u00f4n ng\u1eef t\u1ef1 nhi\u00ean nltk trong vi\u1ec7c preprocess data<br>\n\u1ede \u0111\u00e2y y\u00eau c\u1ea7n ph\u1ea3i s\u1eed d\u1ee5ng wordnet, m\u1ed9t th\u01b0 vi\u1ec7n t\u1eeb \u0111\u1ed3ng ngh\u0129a, tr\u00e1i ngh\u0129a, nltk punkt \u0111\u01b0\u1ee3c y\u00eau c\u1ea7u \u0111\u1ec3 tokenize words<br>\n- B\u01b0\u1edbc 1: ```clean_tag()```: lo\u1ea1i b\u1ecf \u0111i c\u00e1c bi\u1ec3u th\u1ee9c to\u00e1n h\u1ecdc, c\u00e1c \u0111\u1ecba ch\u1ec9 li\u00ean k\u1ebft\n- B\u01b0\u1edbc 2: ```clean_puncts()```: lo\u1ea1i b\u1ecf \u0111i c\u00e1c k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t c\u00f3 trong c\u00e2u\n- B\u01b0\u1edbc 3: ```correct_misspell()```: trong b\u1ed9 d\u1eef li\u1ec7u c\u1ee7a **Quora** do \u0111\u00e2y l\u00e0 c\u00e1c c\u00e2u h\u1ecfi c\u1ee7a ng\u01b0\u1eddi d\u00f9ng n\u00ean kh\u00f4ng tr\u00e1nh kh\u1ecfi vi\u1ec7c g\u00f5 sai, hay l\u00e0 c\u00f3 nh\u1eefng t\u1eeb kh\u00f4ng chu\u1ea9n v\u00ed d\u1ee5 l\u00e0 l\u1eabn l\u1ed9n gi\u1eefa ti\u1ebfng Anh-Anh v\u1edbi ti\u1ebfng Anh-M\u1ef9 cho n\u00ean c\u1ea7n ph\u1ea3i fix nh\u1eefng t\u1eeb n\u00e0y v\u00e0 \u0111\u01b0a n\u00f3 v\u1ec1 d\u1ea1ng chu\u1ea9n\n- B\u01b0\u1edbc 4: ```remove_stopwords()```: lo\u1ea1i b\u1ecf \u0111i c\u00e1c stopwords c\u00f3 trong c\u00e2u\n- B\u01b0\u1edbc 5: ```clean_contractions()```: chuy\u1ec3n nh\u1eefng t\u1eeb vi\u1ebft t\u1eaft v\u1ec1 d\u1ea1ng \u0111\u1ea7y \u0111\u1ee7 v\u1ed1n c\u00f3\n- B\u01b0\u1edbc 6: ```lemming_words()```: lemming c\u00e1c t\u1eeb v\u1ec1 d\u1ea1ng nguy\u00ean b\u1ea3n c\u1ee7a n\u00f3\n\n\n**V\u1ea5n \u0111\u1ec1:**<br>\nTuy nhi\u00ean \u0111\u1eddi kh\u00f4ng nh\u01b0 l\u00e0 m\u01a1, cu\u1ed9c thi n\u00e0y c\u1ee7a **Quora** kh\u00f4ng cho ph\u00e9p s\u1eed d\u1ee5ng Internet n\u00ean vi\u1ec7c lemming word b\u1eb1ng c\u00e1ch lookup t\u1eeb \u0111i\u1ec3n n\u00e0y s\u1ebd kh\u00f4ng ho\u1ea1t \u0111\u1ed9ng<br>\n**Gi\u1ea3i ph\u00e1p:**<br>\nC\u1ea7n t\u00ecm ra m\u1ed9t ph\u01b0\u01a1ng ph\u00e1p lemming words m\u1edbi","465f4a0f":"C\u00f3 th\u1ec3 th\u1ea5y vi\u1ec7c lo\u1ea1i b\u1ecf nhi\u1ec5u theo h\u00e0m ti\u1ec1n x\u1eed l\u00fd ```data_clean()``` \u0111\u00e3 ho\u1ea1t \u0111\u1ed9ng t\u1ed1t, chu\u1ea9n ho\u00e1 c\u00e1c c\u00e2u m\u1ed9t c\u00e1ch ch\u00ednh x\u00e1c<br>\nTuy nhi\u00ean l\u1ea1i g\u00e2y m\u1ed9t tr\u1edf ng\u1ea1i l\u00e0 n\u00f3 l\u00e0m m\u1ea5t \u0111i kh\u00e1 nhi\u1ec1u t\u1eeb, c\u00f3 th\u1ec3 n\u00f3 kh\u00f4ng qu\u00e1 quan tr\u1ecdng nh\u01b0ng \u00edt nhi\u1ec1u c\u0169ng mang m\u1ed9t ph\u1ea7n \u00fd ngh\u0129a<br>\nTa s\u1ebd so s\u00e1nh hi\u1ec7u n\u0103ng gi\u1eefa 2 h\u00e0m ti\u1ec1n x\u1eed l\u00fd \u1edf b\u01b0\u1edbc sau, khi ta build t\u1eadp ```vocabulary``` v\u00e0 ki\u1ec3m tra \u0111\u1ed9 ph\u1ee7 \u0111\u1ed1i v\u1edbi t\u1eadp d\u1eef li\u1ec7u \u0111\u01b0\u1ee3c x\u1eed l\u00fd","15eae754":"\u1ede trong h\u00e0m n\u00e0y em s\u1eed d\u1ee5ng Optimizer l\u00e0 ```Adam```. V\u1ec1 c\u01a1 b\u1ea3n ```Adam``` l\u00e0 s\u1ef1 k\u1ebft h\u1ee3p c\u1ee7a ```Momentum``` v\u00e0 ```RMSprop```. Em s\u1ebd kh\u00f4ng \u0111i s\u00e2u v\u00e0o 2 optimizer ```Momentum``` v\u00e0 ```RMSprop```.<br>\nC\u00f3 th\u1ec3 hi\u1ec3u ```Momentum``` l\u00e0 m\u1ed9t qu\u1ea3 c\u1ea7u lao xu\u1ed1ng d\u1ed1c theo tr\u1ecdng l\u1ef1c, c\u00f2n ```Adam``` nh\u01b0 m\u1ed9t qu\u1ea3 c\u1ea7u s\u1eaft r\u1ea5t n\u1eb7ng v\u00e0 c\u00f3 ma s\u00e1t.<br>\nGi\u1ed1ng v\u1edbi ```Adadelta``` v\u00e0 ```RMSprop```, n\u00f3 duy tr\u00ec trung b\u00ecnh b\u00ecnh ph\u01b0\u01a1ng \u0111\u1ed9 d\u1ed1c (slope) qu\u00e1 kh\u1ee9 vt v\u00e0 c\u0169ng \u0111\u1ed3ng th\u1eddi duy tr\u00ec trung b\u00ecnh \u0111\u1ed9 d\u1ed1c qu\u00e1 kh\u1ee9 mt, gi\u1ed1ng ```Momentum```.\n\n<p align=\"center\">\n  <img width=\"300\" src=\"https:\/\/images.viblo.asia\/1848e210-757a-4fd3-a662-2834b9c68f45.png\">\n<\/p>\n\nV\u1edbi c\u00f4ng th\u1ee9c update:\n$$g_n \\leftarrow \\nabla f(\\theta_{n-1})$$\n$$m_n \\leftarrow (\\frac{\\beta_1}{1-\\beta_1^{n}}).m_{n-1} + (\\frac{1 - \\beta_1}{1-\\beta_1^{n}}).g_n$$\n$$v_n \\leftarrow (\\frac{\\beta_2}{1-\\beta_2^{n}}).v_{n-1} + (\\frac{1 - \\beta_2}{1-\\beta_2^{n}}).g_n \\odot g_n$$\n$$\\theta_n \\leftarrow \\theta_{n-1} - \\frac{a.m_n}{\\sqrt{v_n} + \\epsilon}$$\n\nHi\u1ec3u m\u1ed9t c\u00e1ch ng\u1eafn g\u1ecdn th\u00ec ```Adam``` l\u00e0 optimizer t\u1ed1t nh\u1ea5t hi\u1ec7n nay\n\n\u0110\u00e1nh gi\u00e1 \u0111\u1ed9 m\u1ea5t m\u00e1t th\u00f4ng tin d\u1ef1a tr\u00ean metric ```binary_cross_entropy```\n\nTrong qu\u00e1 tr\u00ecnh model h\u1ecdc, n\u1ebfu ta fix c\u1ee9ng ```epoch``` th\u00ec nhi\u1ec1u kh\u1ea3 n\u0103ng s\u1ebd g\u1eb7p t\u00ecnh tr\u1ea1ng khi model \u0111\u1ea1t m\u1ed9t ng\u01b0\u1ee1ng \u0111\u1ee7 t\u1ed1t s\u1ebd b\u1eaft \u0111\u1ea7u qu\u00e1 tr\u00ecnh h\u1ecdc sai (t\u1ee9c c\u00e0ng h\u1ecdc th\u00ec \u0111\u1ed9 hi\u1ec7u qu\u1ea3 c\u00e0ng gi\u1ea3m). N\u00ean \u1edf \u0111\u00e2y em s\u1ebd t\u1ea1o m\u1ed9t h\u00e0m callbacks \u0111\u1ec3 ph\u1ee5c h\u1ed3i l\u1ea1i tr\u1ecdng s\u1ed1 t\u1ed1t nh\u1ea5t k\u1ec1 tr\u01b0\u1edbc m\u00e0 model h\u1ecdc \u0111\u01b0\u1ee3c \n\nV\u00e0 em s\u1ebd gi\u1ea3m learning rate m\u1ed7i l\u1ea7n model \u0111i qua m\u1ed9t ```epoch``` \u0111\u1ec3 t\u0103ng \u0111\u1ed9 hi\u1ec7u qu\u1ea3 c\u1ee7a m\u00f4 h\u00ecnh, v\u1edbi m\u1ed7i l\u1ea7n gi\u1ea3m 90%<br>\nT\u1ee9c l\u00e0: $$LR = LR*factor$$","c0cb4afd":"Ch\u1ea1y chay th\u00ec m\u00f4 h\u00ecnh n\u00e0y t\u1ed1n kh\u00e1 nhi\u1ec1u th\u1eddi gian n\u00ean em \u0111\u00e3 s\u1eed d\u1ee5ng TPU cho n\u00f3 nhanh h\u01a1n t\u00ed \ud83e\udd26\u200d\u2640\ufe0f","7f25a935":"Chu\u1ea9n b\u1ecb model \u0111\u1ec3 train, \u1edf \u0111\u00e2y em s\u1eed d\u1ee5ng Keras m\u1ed9t ph\u1ea7n v\u00ec n\u00f3 c\u00f3 th\u1ec3 s\u1eed d\u1ee5ng v\u1edbi GPU v\u00e0 TPU<br>\nB\u00ean c\u1ea1nh \u0111\u00f3 Keras h\u1ed7 tr\u1ee3 build model LSTM:\n\n- LSTM l\u00e0 m\u1ed9t m\u1ea1ng c\u1ea3i ti\u1ebfn c\u1ee7a RNN nh\u1eb1m gi\u1ea3i quy\u1ebft c\u00e1c v\u1ea5n \u0111\u1ec1 nh\u1edb c\u00e1c b\u01b0\u1edbc d\u00e0i c\u1ee7a RNN (M\u1ea1ng RNN ch\u1ee9a c\u00e1c v\u00f2ng l\u1eb7p b\u00ean trong cho ph\u00e9p th\u00f4ng tin c\u00f3 th\u1ec3 l\u01b0u l\u1ea1i \u0111\u01b0\u1ee3c nh\u1eb1m gi\u1ea3i quy\u1ebft v\u1ea5n \u0111\u1ec1 nh\u1edb th\u00f4ng tin c\u1ee7a m\u1ea1ng n\u01a1 ron truy\u1ec3n th\u1ed1ng)\n- C\u00f3 th\u1ec3 coi LSTM l\u00e0 m\u1ed9t d\u1ea1ng \u0111\u1eb7c bi\u1ebft c\u1ee7a m\u1ea1ng n\u01a1 ron h\u1ed3i quy, n\u00f3 c\u00f3 kh\u1ea3 n\u0103ng h\u1ecdc \u0111\u01b0\u1ee3c c\u00e1c ph\u1ee5 thu\u1ed9c xa \n- LSTM \u0111\u01b0\u1ee3c thi\u1ebft k\u1ebf \u0111\u1ec3 tr\u00e1nh \u0111\u01b0\u1ee3c v\u1ea5n \u0111\u1ec1 ph\u1ee5 thu\u1ed9c xa (long-term dependency). Vi\u1ec7c nh\u1edb th\u00f4ng tin trong su\u1ed1t th\u1eddi gian d\u00e0i l\u00e0 \u0111\u1eb7c t\u00ednh m\u1eb7c \u0111\u1ecbnh c\u1ee7a ch\u00fang, ch\u1ee9 ta kh\u00f4ng c\u1ea7n ph\u1ea3i hu\u1ea5n luy\u1ec7n n\u00f3 \u0111\u1ec3 c\u00f3 th\u1ec3 nh\u1edb \u0111\u01b0\u1ee3c. T\u1ee9c l\u00e0 ngay n\u1ed9i t\u1ea1i c\u1ee7a n\u00f3 \u0111\u00e3 c\u00f3 th\u1ec3 ghi nh\u1edb \u0111\u01b0\u1ee3c m\u00e0 kh\u00f4ng c\u1ea7n b\u1ea5t k\u00ec can thi\u1ec7p n\u00e0o.\n\n**M\u00f4 h\u00ecnh RNN**\n![rnn](https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/LSTM3-SimpleRNN.png)\n\n> M\u1ecdi m\u1ea1ng h\u1ed3i quy \u0111\u1ec1u c\u00f3 d\u1ea1ng l\u00e0 m\u1ed9t chu\u1ed7i c\u00e1c m\u00f4-\u0111un l\u1eb7p \u0111i l\u1eb7p l\u1ea1i c\u1ee7a m\u1ea1ng n\u01a1-ron. V\u1edbi m\u1ea1ng RNN chu\u1ea9n, c\u00e1c m\u00f4-dun n\u00e0y c\u00f3 c\u1ea5u tr\u00fac r\u1ea5t \u0111\u01a1n gi\u1ea3n, th\u01b0\u1eddng l\u00e0 m\u1ed9t t\u1ea7ng tanh\n\n**M\u00f4 h\u00ecnh LSTM**\n![lstm](https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/LSTM3-chain.png)\n\n> LSTM c\u0169ng c\u00f3 ki\u1ebfn tr\u00fac d\u1ea1ng chu\u1ed7i nh\u01b0 v\u1eady, nh\u01b0ng c\u00e1c m\u00f4-\u0111un trong n\u00f3 c\u00f3 c\u1ea5u tr\u00fac kh\u00e1c v\u1edbi m\u1ea1ng RNN chu\u1ea9n. Thay v\u00ec ch\u1ec9 c\u00f3 m\u1ed9t t\u1ea7ng m\u1ea1ng n\u01a1-ron, ch\u00fang c\u00f3 t\u1edbi 4 t\u1ea7ng t\u01b0\u01a1ng t\u00e1c v\u1edbi nhau m\u1ed9t c\u00e1ch r\u1ea5t \u0111\u1eb7c bi\u1ec7t\n\nS\u1eed d\u1ee5ng embedding layer, m\u1ee5c \u0111\u00edch l\u00e0 \u0111\u1ec3 embedding sang m\u1ed9t kh\u00f4ng gian m\u1edbi c\u00f3 chi\u1ec1u nh\u1ecf h\u01a1n, gi\u1ea3m chi\u1ec1u d\u1eef li\u1ec7u<br>\nBidirectional(LSTM) \u0111\u1ec3 x\u00e2y model LSTM<br>\nLSTM c\u0169ng l\u00e0 m\u1ea1ng CNN n\u00ean c\u1ea7n qua 2 l\u1edbp l\u00e0 Convo1D v\u00e0 Pool1D (convolution v\u00e0 pooling)","ed219b08":"# **5. Hu\u1ea5n luy\u1ec7n v\u00e0 D\u1ef1 \u0111o\u00e1n**","46e4520f":"In ra c\u00e1c t\u1eeb v\u00e0 s\u1ed1 l\u1ea7n xu\u1ea5t hi\u1ec7n trong Vocabulary<br>\nV\u00e0 do \u0111\u00f3 nh\u1eefng c\u00e2u m\u00e0 v\u1edbi nhi\u1ec1u l\u1ea7n xu\u1ea5t hi\u1ec7n c\u1ee7a c\u00e1c t\u1eeb n\u00e0y c\u00f3 kh\u1ea3 n\u0103ng cao l\u00e0 c\u00e1c c\u00e2u kh\u00f4ng \u0111\u1ea1t ti\u00eau chu\u1ea9n","693dd972":"\u1ede \u0111\u00e2y em s\u1ebd t\u1ea1o ra m\u1ed9t h\u00e0m ti\u1ec1n x\u1eed l\u00fd kh\u00e1c v\u1edbi m\u1ee5c \u0111\u00edch gi\u1eef l\u1ea1i nhi\u1ec1u t\u1eeb nh\u1ea5t c\u00f3 th\u1ec3, nh\u1eb1m \u0111\u1ea1t \u0111\u01b0\u1ee3c \u0111\u1ed9 ph\u1ee7 cao v\u1edbi file pretrained<br>\n\u1ede trong h\u00e0m n\u00e0y, c\u00e1c c\u00f4ng vi\u1ec7c bao g\u1ed3m:\n- ```clean_contractions()```: chuy\u1ec3n nh\u1eefng t\u1eeb vi\u1ebft t\u1eaft v\u1ec1 d\u1ea1ng \u0111\u1ea7y \u0111\u1ee7 v\u1ed1n c\u00f3\n- lo\u1ea1i b\u1ecf \u0111i c\u00e1c s\u1ed1 trong c\u00e2u\n\n\u1ede trong h\u00e0m n\u00e0y em tr\u00e1nh vi\u1ec7c lo\u1ea1i b\u1ecf \u0111i c\u00e1c t\u1eeb nh\u01b0 \u1edf h\u00e0m tr\u00ean (```remove_stopwords()```, ```clean_tag()```, ```clean_puncts()```, ```correct_mispell()```)<br>\nV\u00e0 em s\u1ebd kh\u00f4ng convert words sang d\u1ea1ng ```lowercase```","5c9340aa":"So v\u1edbi 2 anh **Glove** th\u00ec \u0111\u1ed9 ph\u1ee7 c\u1ee7a anh **Wiki** th\u1ea5p h\u01a1n m\u1ed9t ch\u00fat, tuy nhi\u00ean qua v\u00ed d\u1ee5 c\u1ee7a anh **Paragram** th\u00ec kh\u00f4ng n\u00f3i tr\u01b0\u1edbc \u0111\u01b0\u1ee3c \u0111i\u1ec1u g\u00ec","75913966":"Th\u1ea5y \u0111\u01b0\u1ee3c m\u1ed9t \u0111i\u1ec1u r\u00f5 r\u00e0ng l\u00e0 ```loss``` c\u1ee7a t\u1eadp validate \u0111\u00e3 gi\u1ea3m \u0111i v\u00e0 c\u00f3 h\u00ecnh d\u1ea1ng \u1ed5n \u0111\u1ecbnh h\u01a1n so v\u1edbi model c\u0169 khi m\u00e0 v\u1ec1 nh\u1eefng ```epoch``` cu\u1ed1i th\u00ec loss kh\u00f4ng t\u0103ng nhi\u1ec1u nh\u01b0 model c\u0169.<br>\n```accuracy``` c\u0169ng \u0111\u01b0\u1ee3c c\u1ea3i thi\u1ec7n h\u01a1n ph\u1ea7n n\u00e0o v\u00e0 c\u0169ng \u1ed5n \u0111\u1ecbnh h\u01a1n m\u1ed9t ch\u00fat, \u1edf m\u1ee9c cao","c14100f1":"Tuy \u0111\u1ed9 ph\u1ee7 kh\u00f4ng \u0111\u1ea1t \u0111\u01b0\u1ee3c nh\u01b0 anh **Paragram** v\u00e0 s\u1ed1 l\u01b0\u1ee3ng t\u1eeb ```out_of_vocab``` x\u1ea5p x\u1ec9 anh **Google News** nh\u01b0ng anh **Wiki** n\u00e0y cho hi\u1ec7u n\u0103ng r\u1ea5t \u0111\u00e1ng n\u1ec3 khi m\u00e0 ma tr\u1eadn embeddings c\u1ee7a anh n\u00e0y gi\u00fap model \u0111\u1ea1t ```f1_score``` x\u1ea5p x\u1ec9 anh **Glove** tuy r\u1eb1ng **Glove** v\u1eabn c\u00f3 ph\u1ea7n nh\u1ec9nh h\u01a1n","04ebc717":"# **4. Chu\u1ea9n b\u1ecb Model**","6258483c":"### **a. Kh\u1ea3o s\u00e1t d\u1eef li\u1ec7u trong t\u1eadp **train****","39d3f431":"# **3. Build t\u1eadp vocab**","7af39683":"Trong h\u00e0m **```clean_puncts()```** n\u00e0y th\u00ec em s\u1ebd lo\u1ea1i b\u1ecf \u0111i c\u00e1c k\u00ed t\u1ef1 \u0111\u1eb7c bi\u1ec7t v\u00e0 c\u00e1c emoji c\u1ea3m x\u00fac<br>\nB\u1edfi v\u00ec vi\u1ec7c lo\u1ea1i b\u1ecf emoji s\u1ebd kh\u00f4ng l\u00e0m \u1ea3nh h\u01b0\u1edfng nhi\u1ec1u t\u1edbi \u00fd ngh\u0129a c\u1ee7a c\u00e2u, t\u01b0\u01a1ng t\u1ef1 nh\u01b0 c\u00e1c d\u1ea5u c\u00e2u v\u00e0 k\u00ed t\u1ef1 \u0111\u1eb7c bi\u1ec7t<br>\nC\u0169ng nh\u01b0 \u00fd \u1edf tr\u00ean, c\u00f3 qu\u00e1 nhi\u1ec1u emoji v\u00e0 k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t l\u00e0m nhi\u1ec5u b\u1ed9 d\u1eef li\u1ec7u kh\u00e1 nhi\u1ec1u","a68f0307":"### **b. Paragram**","07f956c9":"Merge c\u00e1c h\u00e0m ti\u1ec1n x\u1eed l\u00fd v\u00e0o m\u1ed9t h\u00e0m duy nh\u1ea5t \u0111\u1ec3 thao t\u00e1c","534d00ee":"V\u1edbi h\u00e0m preprocess th\u1ee9 nh\u1ea5t l\u00e0 ```data_clean()``` th\u00ec th\u1ea7y c\u00f3 th\u1ec3 th\u1ea5y k\u1ebft qu\u1ea3 \u0111\u1ed9 bao ph\u1ee7 \u0111\u1ea1t **81.04%**, h\u01a1i th\u1ea5p m\u1ed9t ch\u00fat d\u00f9 d\u1eef li\u1ec7u \u0111\u00e3 \u0111\u01b0\u1ee3c x\u1eed l\u00fd t\u1eeb tr\u01b0\u1edbc<br>\nNh\u01b0ng t\u1ec7 h\u1ea1i l\u00e0 t\u1eadp vocab ch\u1ec9 bao ph\u1ee7 **25.40%** corpus<br>\n**Nh\u1eadn x\u00e9t:**<br>\nC\u00f3 th\u1ec3 l\u00e0 do vi\u1ec7c \u0111\u1ee5ng ch\u1ea1m t\u1edbi nhi\u1ec1u t\u1eeb trong c\u00e2u khi lo\u1ea1i b\u1ecf n\u00f3, v\u00e0 s\u1eeda l\u1ed7i sai ch\u00ednh t\u1ea3. \u0110i\u1ec1u n\u00e0y \u0111\u00e3 khi\u1ebfn gi\u1ea3m \u0111i \u0111\u00e1ng k\u1ec3 s\u1ed1 l\u01b0\u1ee3ng c\u00e1c t\u1eeb \u0111\u01b0\u1ee3c t\u00ecm th\u1ea5y trong file pretrained\\\n**Gi\u1ea3i ph\u00e1p:**<br>\nS\u1eed d\u1ee5ng m\u1ed9t h\u00e0m Preprocess m\u1edbi m\u00e0 \u00edt \u0111\u1ee5ng ch\u1ea1m t\u1edbi c\u00e1c t\u1eeb, ch\u1ec9 x\u1eed l\u00fd m\u1ed9t c\u00e1ch \u0111\u01a1n gi\u1ea3n c\u00e1c c\u00e2u","5831663e":"File **Google News** c\u00f3 k\u00edch th\u01b0\u1edbc kh\u00e1 nh\u1ecf: ```3.4G``` nh\u01b0ng n\u00f3 l\u1ea1i \u1edf d\u1ea1ng binary c\u1ee7a word embeddings n\u00ean \u0111\u00e3 c\u00f3 th\u01b0 vi\u1ec7n KeyedVector h\u1ed7 tr\u1ee3 v\u00e0 vi\u1ec7c load kh\u00e1 nhanh n\u00ean em s\u1ebd s\u1eed d\u1ee5ng th\u1eb1ng **Google News** n\u00e0y l\u00e0m tham chi\u1ebfu cho c\u00e1c embeddings kh\u00e1c trong t\u01b0\u01a1ng lai<br>\n\u1ede d\u01b0\u1edbi em s\u1ebd \u0111\u1ecbnh ngh\u0129a c\u00e1c h\u00e0m build ```vocab``` v\u00e0 check \u0111\u1ed9 ph\u1ee7<br>\nDo embeddings l\u00e0 file c\u00f3 word \u0111\u00ednh k\u00e8m vector m\u00f4 t\u1ea3 cho n\u00f3 n\u00ean em s\u1ebd lookup c\u00e1c t\u1eeb c\u00f3 trong c\u00e2u v\u1edbi file embeddings v\u00e0 t\u00ecm vector c\u1ee7a ch\u00fang","3b2d6037":"T\u1eeb word_index \u00e1nh x\u1ea1 sang t\u1eadp train v\u00e0 encode sang m\u00e3 h\u00f3a onehot\\\nCh\u1eafc ch\u1eafn kh\u00f4ng th\u1ec3 tr\u00e1nh kh\u1ecfi vi\u1ec7c c\u00e1c chu\u1ed7i c\u00f3 \u0111\u1ed9 d\u00e0i kh\u00f4ng b\u1eb1ng nhau n\u00ean vi\u1ec7c c\u1ea7n ph\u1ea3i l\u00e0m l\u00e0 padding t\u1ea5t c\u1ea3 c\u00e1c chu\u1ed7i\\\nC\u1eaft \u0111i c\u00e1c chu\u1ed7i c\u00f3 \u0111\u1ed9 d\u00e0i l\u1edbn h\u01a1n 40 v\u00e0 b\u00f9 s\u1ed1 0 v\u00e0 nh\u1eefng chu\u1ed7i c\u00f3 \u0111\u1ed9 d\u00e0i nh\u1ecf h\u01a1n 40","01b2544d":"Do c\u00f3 th\u1eb1ng ```wiki``` l\u00e0 ngo\u1ea1i \u0111\u1ea1o v\u1edbi \u0111u\u00f4i l\u00e0 ```.vec``` n\u00ean c\u1ea7n \u0111\u1ecbnh ngh\u0129a ri\u00eang cho n\u00f3 1 ```statement```","86d8ddbc":"Xo\u00e1 c\u00e1c bi\u1ebfn kh\u00f4ng d\u00f9ng t\u1edbi","b0e8f233":"\u1ede ```epoch``` cu\u1ed1i c\u00f9ng th\u00ec \u0111\u00fang nh\u01b0 d\u1ef1 \u0111o\u00e1n khi m\u00e0 ```loss``` c\u1ee7a m\u00f4 h\u00ecnh \u0111i k\u00e8m v\u1edbi ma tr\u1eadn embeddings c\u1ee7a **Paragram** \u0111\u1ea1t \u0111\u01b0\u1ee3c ```loss``` \u1edf m\u1ee9c th\u1ea5p nh\u1ea5t l\u00e0 ```0.5x``` (x h\u1ecdc ti\u1ec3u h\u1ecdc) v\u00e0 \u0111i k\u00e8m v\u1edbi \u0111\u00f3 l\u00e0 ```accuracy``` cao \u0111\u1ea1t m\u1ee9c ```0.9791```.<br>\nTuy nhi\u00ean \u0111i\u1ec1u quan tr\u1ecdng h\u01a1n \u0111\u00f3 ch\u00ednh l\u00e0 \u0111\u00e1nh gi\u00e1 hi\u1ec7u n\u0103ng c\u1ee7a m\u00f4 h\u00ecnh th\u00f4ng qua ```f1_score```","c5596578":"**```clean_contractions()```**: Map nh\u1eefng t\u1eeb vi\u1ebft t\u1eaft sang d\u1ea1ng ho\u00e0n ch\u1ec9nh, tr\u00e1nh s\u1ef1 hi\u1ec3u l\u1ea7m v\u00e0 kh\u00f4ng l\u00e0m m\u1ea5t \u0111i \u00fd ngh\u0129a c\u1ee7a c\u00e2u h\u1ecfi<br>\nM\u1ee5c \u0111\u00edch l\u00e0 \u0111\u1ec3 \u0111\u1ea1t \u0111\u01b0\u1ee3c \u0111\u1ed9 ph\u1ee7 t\u1ed1t nh\u1ea5t c\u1ee7a vocab \u0111\u01b0\u1ee3c build t\u1eeb file embeddings","f5583f03":"X\u00e2y d\u1ef1ng t\u1eadp vocab v\u00e0 ki\u1ec3m tra \u0111\u1ed9 bao ph\u1ee7 c\u1ee7a t\u1eadp vocab v\u1edbi t\u1eadp vocab c\u1ee7a pretrained model t\u1eeb Google News<br>\nTrong block code n\u00e0y em s\u1ebd x\u00e2y t\u1eadp vocab d\u1ef1a tr\u00ean b\u1ed9 d\u1eef li\u1ec7u \u0111\u01b0\u1ee3c x\u1eed l\u00fd v\u1edbi h\u00e0m preprocess th\u1ee9 nh\u1ea5t l\u00e0 ```data_clean()```","d264c54f":"### **b. Th\u1eed nghi\u1ec7m model 2**","bd4f90b2":"C\u00f3 th\u1ec3 th\u1ea5y r\u1eb1ng \u0111\u1ed9 ph\u1ee7 c\u1ee7a anh **Paragram** c\u00f2n t\u1ed1t h\u01a1n anh **Glove** khi \u0111\u1ea1t t\u1edbi **81.11%** v\u00e0 \u0111\u1ed9 ph\u1ee7 v\u1edbi ```corpus``` \u0111\u1ea1t t\u1edbi **38.97%**.<br> K\u00ec v\u1ecdng l\u00e0 s\u1ebd t\u1ea1o ra ma tr\u1eadn embeddings gi\u00fap cho model c\u00f3 hi\u1ec7u n\u0103ng t\u1ec7 nh\u1ea5t","3bbe1dfa":"Nh\u01b0 th\u1ea7y c\u00f3 th\u1ec3 th\u1ea5y th\u00ec k\u1ebft qu\u1ea3 t\u1ed1t nh\u1ea5t loanh quanh \u1edf ng\u01b0\u1ee1ng t\u1ed1i \u0111a **```0.67x```**<br> (x>3)\nTa s\u1ebd s\u1eed d\u1ee5ng ng\u01b0\u1ee1ng trung b\u00ecnh l\u00e0 **```0.675```** n\u00e0y \u0111\u1ec3 tham chi\u1ebfu v\u1edbi c\u00e1c ma tr\u1eadn embeddings \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng v\u1edbi 3 anh c\u00f2n l\u1ea1i.","7603db45":"R\u00f5 r\u00e0ng khi s\u1eed d\u1ee5ng v\u1edbi file embeddings **Glove** th\u00ec k\u1ebft qu\u1ea3 \u0111\u00e3 c\u00f3 s\u1ef1 c\u1ea3i thi\u1ec7n v\u1edbi ```f1_score``` loanh quanh **```0.69```** t\u1ee9c **69%**<br>\nTuy nhi\u00ean s\u1ef1 c\u1ea3i thi\u1ec7n l\u00e0 kh\u00f4ng nhi\u1ec1u <br>\n\u0110i\u1ec1u n\u00e0y c\u00f3 th\u1ec3 k\u1ebft lu\u1eadn r\u1eb1ng s\u1ed1 t\u1eeb ```out_of_vocab``` v\u00e0 \u0111\u1ed9 ph\u1ee7 c\u1ee7a vocab th\u1ef1c s\u1ef1 \u1ea3nh h\u01b0\u1edfng t\u1edbi hi\u1ec7u n\u0103ng c\u1ee7a model.<br>","176bf120":"Model \u0111\u01b0\u1ee3c thi\u1ebft k\u1ebf v\u1edbi c\u00f9ng ```optimizer``` l\u00e0 ```Adam``` v\u1edbi ```learning_rate=0.001``` v\u00e0 v\u1edbi metric ```accuracy``` v\u00e0 h\u00e0m ```loss``` \u0111\u01b0\u1ee3c \u0111\u00e1nh gi\u00e1 d\u1ef1a tr\u00ean ```binary_crossentropy```","edc1f05d":"Nh\u01b0 th\u1ea7y c\u00f3 th\u1ec3 th\u1ea5y hi\u1ec7u qu\u1ea3 \u0111\u01b0\u1ee3c c\u1ea3i thi\u1ec7n m\u1ed9t c\u00e1ch r\u00f5 r\u1ec7t:\n- \u0110\u1ed9 ph\u1ee7 c\u1ee7a vocab v\u1edbi corpus t\u0103ng g\u1ea5p **2.5 l\u1ea7n** l\u00ean **61.37%**\n- \u0110\u1ed9 ph\u1ee7 t\u0103ng l\u00ean **90.81%**, m\u1ed9t con s\u1ed1 kh\u00e1 \u1ea5n t\u01b0\u1ee3ng\n\nN\u1ebfu nh\u01b0 chuy\u1ec3n c\u00e1c t\u1eeb v\u1ec1 d\u1ea1ng ```lowercase``` th\u00ec 2 con s\u1ed1 t\u01b0\u01a1ng \u0111\u01b0\u01a1ng l\u00e0 **38.85%** v\u1edbi corpus v\u00e0 **89.47%** v\u1edbi vocab.<br>\nNg\u01b0\u1eddi Anh \u0111\u1eb7t c\u00e1i t\u00f4i c\u1ee7a h\u1ecd r\u1ea5t cao, n\u00ean trong t\u1ea5t c\u1ea3 c\u00e1c c\u00e2u v\u0103n c\u1ee7a h\u1ecd th\u00ec t\u1eeb 'I' (ch\u1ec9 b\u1ea3n th\u00e2n ng\u01b0\u1eddi n\u00f3i) lu\u00f4n \u0111\u01b0\u1ee3c \u0111\u1eb7t \u1edf tr\u1ea1ng th\u00e1i in hoa ```uppercase```n\u00ean n\u1ebfu ch\u00fang ta chu\u1ea9n ho\u00e1 c\u00e1c t\u1eeb v\u1ec1 ```lowercase``` th\u00ec s\u1ebd kh\u00f4ng lookup \u0111\u01b0\u1ee3c t\u1eeb \u0111\u00f3 trong file pretrained embeddings v\u00e0 s\u1ebd g\u00e2y ra hi\u1ec7n t\u01b0\u1ee3ng thi\u1ebfu s\u00f3t trong ma tr\u1eadn embeddings m\u00e0 ch\u00fang ta chu\u1ea9n b\u1ecb x\u00e2y d\u1ef1ng","196c0449":"### **c. Wiki**","7a3ced23":"\u1ede \u0111\u00e2y em s\u1ebd c\u1ed1 \u0111\u1ecbnh s\u1eed d\u1ee5ng th\u1eb1ng **Google News** l\u00e0m ti\u00eau chu\u1ea9n<br>\nKi\u1ec3m tra word embeddings \u0111\u00e3 gi\u1ea3i n\u00e9n v\u1edbi **Google News** \u0111\u00ednh k\u00e8m trong file embeddings.zip<br>\n\u0110\u1ecbnh ngh\u0129a c\u00e1c h\u00e0m \u0111\u1ec3 build vocab t\u1eeb file **Google News** v\u00e0 h\u00e0m ki\u1ec3m tra \u0111\u1ed9 bao ph\u1ee7 c\u1ee7a vocab \u0111\u1ed1i v\u1edbi t\u1eadp d\u1eef li\u1ec7u \u0111\u00e3 \u0111\u01b0\u1ee3c preprocess<br>\n&nbsp;<br>\nTrong b\u00e0i n\u00e0y, em s\u1ebd gi\u1ea3i n\u00e9n t\u1ec7p embedding \u0111\u1ec3 s\u1eed d\u1ee5ng t\u1eadp c\u00e1c vector v\u1edbi c\u00e1c t\u1eeb cho s\u1eb5n<br>\nC\u00e1c t\u1eeb trong file embeddings \u0111\u1ec1u \u0111\u00ednh k\u00e8m m\u1ed9t vector, v\u00e0 em s\u1ebd x\u00e2y ma tr\u1eadn embeddings d\u1ef1a v\u00e0o file embeddings \u0111\u00f3","06523993":"In ra f1 score, ```f1_score``` t\u1ed1t nh\u1ea5t \u1edf \u0111\u00e2y l\u00e0 ```0.673``` t\u1ee9c ```67.3%``` \u1ee9ng v\u1edbi h\u00e0m Preprocess th\u1ee9 2 t\u1ee9c l\u00e0 kh\u00f4ng \u0111\u1ee5ng ch\u1ea1m nhi\u1ec1u \u0111\u1ebfn c\u00e2u h\u1ecfi<br>\nT\u01b0\u01a1ng \u1ee9ng l\u00e0 f1_score = ```0.633``` t\u1ee9c ```63.3%``` \u0111\u1ed1i v\u1edbi h\u00e0m Preprocess \u0111\u1ea7u ti\u00ean khi lo\u1ea1i b\u1ecf ```stop_words```, ```lemma_words```, ...\nC\u00f3 th\u1ec3 nh\u1eadn x\u00e9t r\u1eb1ng:\n- V\u1edbi c\u00e1ch ti\u1ebfp c\u1eadn d\u1ef1a tr\u00ean pretrained th\u00ec vi\u1ec7c ti\u1ec1n x\u1eed l\u00fd d\u1eef li\u1ec7u qu\u00e1 nhi\u1ec1u th\u1ef1c s\u1ef1 c\u00f3 \u1ea3nh h\u01b0\u1edfng l\u1edbn t\u1edbi hi\u1ec7u n\u0103ng c\u1ee7a model khi m\u00e0 n\u00f3 l\u00e0m m\u1ea5t \u0111i ph\u1ea7n nhi\u1ec1u \u00fd ngh\u0129a c\u00e1c c\u00e1c t\u1eeb c\u00f3 trong c\u00e2u h\u1ecfi\n- C\u00f3 th\u1ec3 do c\u00e1ch c\u00e0i \u0111\u1eb7t kh\u00e1c nhau nh\u01b0ng v\u1ec1 c\u01a1 b\u1ea3n v\u1edbi c\u00e1ch ti\u1ebfp c\u1eadn d\u1ef1a v\u00e0o pretrained th\u00ec ta kh\u00f4ng n\u00ean x\u1eed l\u00fd nhi\u1ec5u c\u1ee7a d\u1eef li\u1ec7u nhi\u1ec1u, c\u00f3 ch\u0103ng l\u00e0 lo\u1ea1i b\u1ecf \u0111i c\u00e1c digits, c\u00e1c c\u00f4ng th\u1ee9c to\u00e1n v\u00e0 c\u00e1c \u0111\u01b0\u1eddng d\u1eabn li\u00ean k\u1ebft ","e834d9d8":"X\u00e2y d\u1ef1ng ma tr\u1eadn embeddings d\u1ef1a tr\u00ean t\u1eadp t\u1eeb v\u1ef1ng c\u00f3 tr\u01b0\u1edbc \u1edf trong file embeddings<br>\n\u1ede \u0111\u00e2y m\u1ed7i h\u00e0ng s\u1ebd c\u00f3 vector embeddings cho m\u1ed7i t\u1eeb duy nh\u1ea5t","9ce50162":"# **M\u00f4 t\u1ea3 b\u00e0i to\u00e1n**\n\n**Quora Insincere Question Classification** l\u00e0 m\u1ed9t b\u00e0i to\u00e1n c\u1ee7a **Quora** \u0111\u1eb7t ra, s\u1eed d\u1ee5ng s\u1ef1 tr\u1ee3 gi\u00fap t\u1eeb c\u1ed9ng \u0111\u1ed3ng, gi\u00fap h\u1ecd ph\u00e2n lo\u1ea1i nh\u1eefng c\u00e2u h\u1ecfi kh\u00f4ng ch\u00e2n th\u00e0nh. Nhi\u1ec7m v\u1ee5 c\u1ee7a b\u00e0i to\u00e1n l\u00e0 s\u1eed d\u1ee5ng t\u1eadp d\u1eef li\u1ec7u m\u00e0 **Quora** cung c\u1ea5p \u0111\u1ec3 ph\u00e2n lo\u1ea1i ra nh\u1eefng c\u00e2u h\u1ecfi mang h\u00e0m \u00fd kh\u00f4ng ch\u00e2n th\u00e0nh, mang n\u1ed9i dung x\u1ea5u \u0111\u1ed9c, g\u00e2y hi\u1ec3u l\u1ea7m.\n\n- **\u0110\u1ea7u v\u00e0o**: C\u00e2u h\u1ecfi d\u01b0\u1edbi d\u1ea1ng v\u0103n b\u1ea3n (plain text)\n- **\u0110\u1ea7u ra**: Yes\/No (insincere (1) or not (0))","c83f18ea":"# **H\u01b0\u1edbng Ti\u1ebfp C\u1eadn**\n- Ti\u1ec1n x\u1eed l\u00fd b\u1ed9 d\u1eef li\u1ec7u\n    - B\u1ecf \u0111i c\u00e1c bi\u1ec3u th\u1ee9c to\u00e1n h\u1ecdc v\u00e0 c\u00e1c \u0111\u01b0\u1eddng d\u1eabn li\u00ean k\u1ebft\n    - B\u1ecf \u0111i c\u00e1c k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t, c\u00e1c ch\u1eef s\u1ed1\n    - S\u1eeda nh\u1eefng t\u1eeb sai ch\u00ednh t\u1ea3 v\u00e0 extend c\u00e1c t\u1eeb vi\u1ebft t\u1eaft\n    - B\u1ecf \u0111i stop_words trong c\u00e2u\n    - Lemming c\u00e1c t\u1eeb c\u00f3 trong c\u00e2u (c\u00f3 th\u1ec3 hi\u1ec3u l\u00e0 chuy\u1ec3n c\u00e1c t\u1eeb v\u1ec1 d\u1ea1ng nguy\u00ean th\u1ec3)\n    \n> **M\u1ee5c \u0111\u00edch**: Lo\u1ea1i b\u1ecf \u0111i c\u00e1c t\u1eeb g\u00e2y nhi\u1ec5u cho b\u1ed9 d\u1eef li\u1ec7u v\u00e0 m\u1edf r\u1ed9ng \u0111\u1ed9 bao ph\u1ee7 c\u1ee7a t\u1eadp Vocab v\u1edbi b\u1ed9 d\u1eef li\u1ec7u\n- X\u00e2y d\u1ef1ng t\u1eadp vocab: \n    - S\u1eed d\u1ee5ng c\u00e1c file trong embeddings.zip m\u00e0 quora cung c\u1ea5p \u0111\u1ec3 x\u00e2y d\u1ef1ng t\u1eadp vocab \n    - C\u00e1c file embeddings c\u00f3 d\u1ea1ng text, m\u1ed7i d\u00f2ng ch\u1ee9a 1 word v\u00e0 \u0111i k\u00e8m \u0111\u00f3 l\u00e0 vector c\u1ee7a n\u00f3 \u0111\u00e3 \u0111\u01b0\u1ee3c pretrained\n    - X\u00e2y t\u1eadp vocab t\u1eeb c\u00e1c word c\u00f3 trong t\u1eeb \u0111i\u1ec3n v\u00e0 vector \u0111i k\u00e8m v\u1edbi ch\u00fang\n    - Ki\u1ec3m tra \u0111\u1ed9 ph\u1ee7 c\u1ee7a vocab x\u00e2y d\u1ef1ng d\u1ef1a tr\u00ean t\u1eadp embeddings v\u1edbi d\u1eef li\u1ec7u \u0111\u01b0\u1ee3c cho\n- X\u00e2y d\u1ef1ng ma tr\u1eadn embeddings:\n    - S\u1eed d\u1ee5ng file pretrained \u0111\u1ec3 t\u1ea1o ra ma tr\u1eadn embeddings d\u1ef1a v\u00e0o c\u00e1c t\u1eeb c\u00f3 trong t\u1eadp vocab\n    - L\u1ea5y c\u00e1c t\u1eeb \u0111\u1ec3 lookup vector descriptor c\u1ee7a c\u00e1c t\u1eeb \u0111\u00f3 v\u00e0 x\u00e2y ma tr\u1eadn\n- T\u1ea1o model v\u00e0 hu\u1ea5n luy\u1ec7n d\u1ef1 \u0111o\u00e1n\n- K\u1ebft h\u1ee3p k\u1ebft qu\u1ea3 c\u1ee7a nhi\u1ec1u model v\u1edbi c\u00e1c ma tr\u1eadn embeddings \u0111\u00e3 x\u00e2y d\u1ef1ng v\u00e0 cho ra k\u1ebft qu\u1ea3 t\u1ed1t nh\u1ea5t","ed6ccccf":"\u1ede trong b\u1ed9 d\u1eef li\u1ec7u test th\u00ec ch\u1ec9 c\u00f3 2 tr\u01b0\u1eddng d\u1eef li\u1ec7u l\u00e0 **```qid```** \u0111\u1ea1i di\u1ec7n cho id c\u1ee7a c\u00e2u h\u1ecfi<br>\nV\u00e0 **```question_text```** \u0111\u1ea1i di\u1ec7n cho d\u1eef li\u1ec7u test<br>\nC\u0169ng may m\u1eafn khi m\u00e0 \u1edf trong t\u1eadp **df_test** l\u1ea1i kh\u00f4ng c\u00f3 d\u1eef li\u1ec7u k\u00ec l\u1ea1 n\u00e0o c\u1ea3 (```null```, ```none```, ```missing```)<br>\nV\u00e0 c\u00e1i ta c\u1ea7n quan t\u00e2m \u1edf \u0111\u00e2y ch\u1ec9 l\u00e0 tr\u01b0\u1eddng d\u1eef li\u1ec7u **```question_text```** \u0111\u1ec3 \u0111\u01b0a v\u00e0o model","8d942ab0":"Trong block code n\u00e0y, em s\u1ebd s\u1eed d\u1ee5ng h\u00e0m Preprocess th\u1ee9 2 (h\u00e0m process \u0111\u01a1n gi\u1ea3n h\u01a1n)","dab53d62":"G\u00f3i vi\u1ec7c t\u1ea1o model v\u00e0o m\u1ed9t h\u00e0m n\u00e9m v\u00e0o **```stategy.scope()```** \u0111\u1ec3 enable kh\u1ea3 n\u0103ng ch\u1ea1y v\u1edbi TPU, t\u0103ng t\u1ed1c \u0111\u1ed9 train","c4cccb8e":"C\u00f3 th\u1ec3 th\u1ea5y \u0111\u01b0\u1ee3c r\u1eb1ng m\u00f4 h\u00ecnh ho\u1ea1t \u0111\u1ed9ng kh\u00e1 hi\u1ec7u qu\u1ea3 v\u1edbi t\u1eadp **```train```** khi m\u00e0 ```loss``` gi\u1ea3m \u0111\u1ec1u v\u00e0 ```accuracy``` t\u0103ng \u0111\u1ec1u qua c\u00e1c epoch<br>\nTuy nhi\u00ean th\u00ec v\u1edbi t\u1eadp **```test```** l\u1ea1i ho\u1ea1t \u0111\u1ed9ng kh\u00f4ng \u0111\u01b0\u1ee3c hi\u1ec7u qu\u1ea3 khi ```loss``` v\u00e0 ```accuracy``` c\u00f3 h\u00ecnh d\u1ea1ng tr\u1ed3i s\u1ee5t nh\u00ecn nh\u01b0 s\u00f3ng bi\u1ec3n. V\u00e0 t\u1ec7 h\u01a1n l\u00e0 v\u1edbi t\u1eadp **```test```** th\u00ec ```loss``` v\u1ec1 c\u00e1c epoch cu\u1ed1i l\u1ea1i t\u0103ng kh\u00e1 cao.<br>\nV\u1eady th\u00ec c\u00f3 th\u1ec3 k\u1ebft lu\u1eadn l\u00e0 do **model** \u0111\u00e3 x\u00e2y d\u1ef1ng **ho\u1ea1t \u0111\u1ed9ng ch\u01b0a hi\u1ec7u qu\u1ea3**, c\u1ea7n ph\u1ea3i xem x\u00e9t l\u1ea1i<br>","1e741838":"# **2. Ti\u1ec1n x\u1eed l\u00fd d\u1eef li\u1ec7u**","d77af968":"Compile model v\u1edbi 30 epoch cho m\u00e1u, batch_size \u0111\u1ec3 \u1edf m\u1ee9c cao \u0111\u1ec3 t\u1eadn d\u1ee5ng s\u1ee9c m\u1ea1nh t\u00ednh to\u00e1n c\u1ee7a TPU v\u00e0 s\u1eed d\u1ee5ng v\u1edbi t\u1eadp validation \u0111\u1ec3 validate d\u1eef li\u1ec7u, truy\u1ec1n v\u00e0o tham s\u1ed1 callback \u0111\u1ec3 m\u00f4 h\u00ecnh c\u00e0ng h\u1ecdc c\u00e0ng kh\u00f4n ch\u1ee9 kh\u00f4ng \u0111\u01b0\u1ee3c ngu \u0111i<br>\nM\u1ed9t ph\u1ea7n do em c\u00f3 c\u1eadu b\u1ea1n c\u0169ng train v\u1edbi b\u1ecdn Keras nh\u01b0ng khi f1 score \u0111\u1ea1t \u0111\u01b0\u1ee3c t\u1ed1t r\u1ed3i th\u00ec n\u00f3 b\u1eaft \u0111\u1ea7u tri\u1ec7u ch\u1ee9ng ngu \u0111i khi m\u00e0 loss t\u1eb1ng (overfitting)","e5955194":"C\u0169ng nh\u01b0 3 anh tr\u00ean l\u00e0 **Glove**, **Paragram** v\u00e0 **Google News** th\u00ec anh **Wiki** n\u00e0y c\u0169ng c\u00f3 ```loss``` c\u1ee7a t\u1eadp test t\u0103ng d\u1ea7n v\u1ec1 epoch cu\u1ed1i v\u00e0 ```accuracy``` kh\u00f4ng \u0111\u01b0\u1ee3c c\u1ea3i thi\u1ec7n nhi\u1ec1u.<br>\nV\u1eady n\u00ean ta c\u1ea7n ph\u1ea3i config l\u1ea1i model, \u0111\u1ec3 \u0111\u1ea1t \u0111\u01b0\u1ee3c hi\u1ec7u qu\u1ea3 cao h\u01a1n","b52a09b7":"# **6. So s\u00e1nh, m\u1edf r\u1ed9ng**\nDo s\u1eed d\u1ee5ng 1 pretrained l\u00e0 **Google News** n\u00ean k\u1ebft qu\u1ea3 thu \u0111\u01b0\u1ee3c \u1edf m\u1ee9c l\u00e0 **```0.68x``` (x<=3)** f1_score<br>\n&nbsp;<br>\nB\u1edfi v\u00ec 3 th\u1eb1ng c\u00f2n l\u1ea1i (kh\u00f4ng ph\u1ea3i **Google News**) \u0111\u1ec1u kh\u00f4ng \u1edf d\u1ea1ng binary v\u00e0 kh\u00f4ng ph\u1ea3i \u1edf d\u1ea1ng chu\u1ea9n word_vector n\u00ean kh\u00f4ng \u0111\u1ecdc \u0111\u01b0\u1ee3c b\u1eb1ng KeyedVectors, em s\u1ebd \u0111\u1ecbnh ngh\u0129a m\u1ed9t h\u00e0m \u0111\u1ec3 \u0111\u1ecdc c\u00e1c file embeddings c\u00f2n l\u1ea1i <br>\nTuy nhi\u00ean do t\u1ef1 \u0111\u1ecbnh ngh\u0129a n\u00ean th\u1eddi gian load d\u1eef li\u1ec7u v\u00e0o bi\u1ebfn kh\u00e1 l\u00e0 l\u00e2u","0f7cd5bd":"Load c\u00e1c file embeddings v\u00e0o bi\u1ebfn<br>\nTuy nhi\u00ean do load th\u1ee7 c\u00f4ng kh\u00f4ng qua th\u01b0 vi\u1ec7n n\u00ean vi\u1ec7c load kh\u00e1 l\u00e0 ch\u1eadm, m\u1ea5t kho\u1ea3ng 9' \u0111\u1ec3 load c\u1ea3 3 anh<br>\nB\u1ecdn ```KeyedVector``` c\u00f3 h\u1ed7 tr\u1ee3 load ```glove``` nh\u01b0ng ph\u1ea3i covert qua binary n\u00ean v\u1ec1 t\u1ed1c \u0111\u1ed9 th\u00ec c\u0169ng kh\u00f4ng h\u01a1n anh th\u1ee7 c\u00f4ng l\u00e0 m\u1ea5y","7c1ddd5e":"Ki\u1ec3m tra l\u1ea1i xem c\u00e1c h\u00e0m c\u00f3 ho\u1ea1t \u0111\u1ed9ng \u0111\u00fang nh\u01b0 k\u1ef3 v\u1ecdng kh\u00f4ng","094f4153":"C\u00f3 th\u1ec3 th\u1ea5y trong tr\u01b0\u1eddng target c\u00f3 2 gi\u00e1 tr\u1ecb l\u00e0 ```0``` v\u00e0 ```1```<br>\nD\u1ef1 \u0111o\u00e1n th\u00ec nh\u1eefng c\u00e2u h\u1ecfi \u0111\u00e1nh d\u1ea5u ```0``` l\u00e0 nh\u1eefng c\u00e2u h\u1ecfi sincere, t\u01b0\u01a1ng t\u1ef1 ```1``` l\u00e0 insincere<br>\nV\u1eady c\u00f3 th\u1ec3 xem nh\u01b0 \u0111\u00e2y l\u00e0 b\u00e0i to\u00e1n ph\u00e2n l\u1edbp nh\u1ecb ph\u00e2n<br>\nNh\u01b0ng v\u1eabn c\u1ea7n ph\u1ea3i c\u00f3 \u0111\u00e1nh gi\u00e1 s\u00e2u h\u01a1n v\u1ec1 b\u1ed9 d\u1eef li\u1ec7u n\u00e0y, nh\u00ecn nhanh qua c\u00f3 th\u1ec3 th\u1ea5y b\u1ed9 d\u1eef li\u1ec7u train \u0111\u01b0\u1ee3c cho kh\u00e1 l\u00e0 m\u1ea5t c\u00e2n b\u1eb1ng","ec24290f":"Oke v\u1eady th\u00ec c\u00f3 th\u1ec3 th\u1ea5y \u0111\u01b0\u1ee3c r\u1eb1ng hi\u1ec7u n\u0103ng c\u1ee7a model v\u1edbi c\u00e1c ma tr\u1eadn embeddings \u0111\u1ebfn t\u1eeb v\u1ecb tr\u00ed c\u1ee7a 4 anh l\u00e0 **Google News**, **Glove**, **Paragram** v\u00e0 **Wiki** kh\u00e1 t\u01b0\u01a1ng \u0111\u1ed3ng v\u1edbi nhau, duy ch\u1ec9 c\u00f3 anh **Google News** l\u00e0 th\u1ec3 hi\u1ec7n k\u00e9m h\u01a1n 3 anh c\u00f2n l\u1ea1i, c\u1ea3 v\u1ec1 \u0111\u1ed9 ph\u1ee7, c\u1ea3 v\u1ec1 ```f1_score``` v\u00e0 nh\u1eefng y\u1ebfu t\u1ed1 kh\u00e1c.<br>\nTuy nhi\u00ean ch\u00fang ta l\u1ea1i c\u00f3 th\u1ec3 th\u1ea5y \u0111\u01b0\u1ee3c r\u1eb1ng, v\u1edbi c\u1ea3 4 model n\u00e0y t\u1ea1i sao l\u1ea1i kh\u00f4ng k\u1ebfp h\u1ee3p ch\u00fang v\u00e0o v\u1edbi nhau \u0111\u1ec3 t\u1ea1o ra m\u1ed9t ```predict``` hi\u1ec7u qu\u1ea3, th\u00ec d\u01b0\u1edbi \u0111\u00e2y ch\u00ednh l\u00e0 s\u1ef1 k\u1ebft h\u1ee3p c\u1ee7a c\u1ea3 4 models<br>\nThu\u1eadt ng\u1eef chuy\u00ean ng\u00e0nh g\u1ecdi l\u00e0 **Stack Models Prediction**<br>\nV\u1edbi **Stack Models Prediction** th\u00ec ch\u00fang ta c\u00f3 nhi\u1ec1u chi\u1ebfn l\u01b0\u1ee3c, nh\u01b0ng \u1edf \u0111\u00e2y em s\u1ebd nh\u00e2n m\u1ed7i th\u1eb1ng ```predict``` v\u1edbi c\u00f9ng m\u1ed9t h\u1ec7 s\u1ed1.<br>\nDo c\u00f3 4 th\u1eb1ng n\u00ean h\u1ec7 s\u1ed1 cho m\u1ed7i th\u1eb1ng l\u00e0 ```0.25``` v\u00e0 ta \u0111\u01b0\u1ee3c m\u1ed9t ```predict``` m\u1edbi.<br>\nV\u00e0 vi\u1ec7c gh\u00e9p c\u00e1c **predicts** l\u1ea1i ho\u1ea1t \u0111\u1ed9ng kh\u00e1 hi\u1ec7u qu\u1ea3 khi ```f1_score``` c\u1ee7a m\u00f4 h\u00ecnh \u0111\u1ea1t t\u1edbi **70%** t\u1ee9c ```0.70```","5f44b98d":"L\u1ea5y ra index c\u1ee7a t\u1eeb trong t\u1eadp t\u1eeb \u0111i\u1ec3n ph\u1ee5c v\u1ee5 cho m\u00e3 h\u00f3a onehot<br>\n\u1ede d\u01b0\u1edbi \u0111\u00e2y em s\u1ebd \u0111\u1ecbnh ngh\u0129a 2 h\u00e0m l\u00e0:\n- ```get_word_index()```: l\u1ea5y ra index c\u1ee7a words trong t\u1eadp t\u1eeb v\u1ef1ng\n- ```fit_one_hot()```: m\u00e3 ho\u00e1 word_index sang d\u1ea1ng onehot d\u1ef1a v\u00e0o corpus","1b42fca9":"L\u1ea7n \u0111\u1ea7u ch\u1ea1y kh\u00f4ng \u0111\u1ecbnh ngh\u0129a l\u1ea1i ```strategy.scope()``` th\u00ec em g\u1eb7p m\u1ed9t hi\u1ec7n t\u01b0\u1ee3ng l\u1ea1 \u0111\u00f3 l\u00e0 v\u1edbi vi\u1ec7c build model m\u1edbi v\u00e0 ch\u1ea1y trong ```strategy.scope()``` th\u00ec model c\u00f3 v\u1ebb b\u1ecb sai kh\u00e1 nhi\u1ec1u khi ```loss``` l\u00ean t\u1edbi m\u1ee9c ```0.22``` v\u1edbi ```compile``` l\u1ea7n 1 v\u00e0 ```loss``` \u1edf m\u1ee9c ```0.27``` v\u1edbi l\u1ea7n ```compile``` th\u1ee9 2<br>\nDo v\u1eady em \u0111o\u00e1n kh\u1ea3 n\u0103ng l\u00e0 do th\u1eb1ng ```strategy.scope()``` gi\u1eef l\u1ea1i m\u1ed9t s\u1ed1 gi\u00e1 tr\u1ecb g\u00ec \u0111\u00f3 n\u00ean em \u0111\u00e3 \u0111\u1ecbnh ngh\u0129a l\u1ea1i th\u1eb1ng ```strategy.scope()```","5fde348a":"C\u00f3 th\u1ec3 th\u1ea5y r\u1eb1ng t\u1eadp train c\u00f3 3 tr\u01b0\u1eddng d\u1eef li\u1ec7u\n- **```qid```**: \u0111\u1ecbnh danh id cho c\u00e2u h\u1ecfi\n- **```question_text```**: n\u1ed9i dung c\u00e2u h\u1ecfi\n- **```target```**: ph\u00e2n l\u1edbp c\u00e2u h\u1ecfi\n    - 0: c\u00e2u h\u1ecfi mang t\u00ednh ch\u1ea5t ch\u00e2n th\u00e0nh (sincere)\n    - 1: c\u00e2u h\u1ecfi kh\u00f4ng mang t\u00ednh ch\u1ea5t ch\u00e2n th\u00e0nh (insincere)\n    \nKh\u00e1 may m\u1eafn khi t\u1eadp d\u1eef li\u1ec7u **Quora** cung c\u1ea5p kh\u00f4ng c\u00f3 b\u1ea5t k\u00ec tr\u01b0\u1eddng d\u1eef li\u1ec7u n\u00e0o c\u00f3 gi\u00e1 tr\u1ecb b\u1ea5t th\u01b0\u1eddng (```null```, ```none```, ```missing```)\n    \nT\u1eadp d\u1eef li\u1ec7u train bao g\u1ed3m **1306122** d\u00f2ng d\u1eef li\u1ec7u, g\u1ea7n **1.31 tri\u1ec7u d\u00f2ng**, kh\u00e1 l\u1edbn. C\u1ea7n s\u1eed d\u1ee5ng m\u00f4 h\u00ecnh c\u00f3 hi\u1ec7u qu\u1ea3 t\u00ednh to\u00e1n nhanh, n\u00ean trong b\u00e0i n\u00e0y em s\u1ebd kh\u00f4ng s\u1eed d\u1ee5ng m\u00f4 h\u00ecnh v\u1edbi thu\u1eadt to\u00e1n ensemble","50b1103e":"X\u00e1c \u0111\u1ecbnh c\u00e1c t\u1eadp ```train```, ```test``` v\u00e0 kh\u1edfi t\u1ea1o m\u1ed9t t\u1eadp ch\u1ee9a d\u1eef li\u1ec7u c\u1ee7a c\u1ea3 2 \u0111\u1ec3 v\u1ec1 sau check \u0111\u1ed9 bao ph\u1ee7 c\u1ee7a vocab v\u1edbi t\u1eadp d\u1eef li\u1ec7u \u0111\u00e3 sinh","de87728b":"Nh\u01b0 c\u00f3 th\u1ec3 th\u1ea5y \u0111\u1ed1i v\u1edbi t\u1eadp pretrained **Google News** d\u00f9 \u0111\u1ed9 ph\u1ee7 kh\u00e1 cao nh\u01b0ng s\u1ed1 l\u01b0\u1ee3ng t\u1eeb n\u1eb1m ngo\u00e0i t\u1eadp embeddings kh\u00e1 l\u00e0 l\u1edbn khi l\u00ean t\u1edbi 99931 t\u1eeb<br>\nV\u00e0 c\u1ea7n ph\u1ea3i c\u00f3 s\u1ef1 c\u1ea3i thi\u1ec7n v\u1ec1 s\u1ed1 l\u01b0\u1ee3ng t\u1eeb ```out_of_vocab``` n\u1ebfu kh\u00f4ng th\u00ec hi\u1ec7u qu\u1ea3 c\u1ee7a model s\u1ebd k\u00e9m.","080c5ecc":"L\u1ea5y ra ```thresh_hold``` cho ```result``` t\u1ed1t nh\u1ea5t v\u00e0 s\u1eed d\u1ee5ng n\u00f3 \u0111\u1ec3 submit l\u00ean Kaggle","3a7406bc":"V\u1edbi b\u00e0i to\u00e1n li\u00ean quan \u0111\u1ebfn ng\u00f4n ng\u1eef th\u00ec hi\u1ec7u qu\u1ea3 c\u1ee7a m\u00f4 h\u00ecnh ph\u1ee5 thu\u1ed9c l\u1edbn v\u00e0o vi\u1ec7c ti\u1ec1n x\u1eed l\u00fd d\u1eef li\u1ec7u, t\u1ee9c lo\u1ea1i b\u1ecf \u0111i nhi\u1ec5u \u1edf trong b\u1ed9 d\u1eef li\u1ec7u.<br>\nV\u00e0 ph\u01b0\u01a1ng ph\u00e1p ph\u1ed5 bi\u1ebfn \u0111\u1ed1i v\u1edbi c\u00e1c b\u00e0i to\u00e1n li\u00ean quan \u0111\u1ebfn ng\u00f4n ng\u1eef \u0111\u01b0\u1ee3c tr\u00ecnh b\u00e0y \u1edf d\u01b0\u1edbi nh\u01b0 sau","ea05c3f6":"Qua bi\u1ec3u \u0111\u1ed3 c\u00f3 th\u1ec3 d\u1ec5 d\u00e0ng nh\u1eadn th\u1ea5y t\u1eadp d\u1eef li\u1ec7u train b\u1ecb m\u1ea5t c\u00e2n b\u1eb1ng nhi\u1ec1u:\n- L\u1edbp c\u00e2u h\u1ecfi sincere c\u00f3 **1225312** (chi\u1ebfm **93.81%**) s\u1ed1 l\u01b0\u1ee3ng d\u1eef li\u1ec7u\n- L\u1edbp c\u00e2u h\u1ecfi insincere c\u00f3 **80810** (chi\u1ebfm **6.19%**) s\u1ed1 l\u01b0\u1ee3ng d\u1eef li\u1ec7u\n\nC\u00f3 th\u1ec3 th\u1ea5y t\u1eadp d\u1eef li\u1ec7u b\u1ecb l\u1ec7ch nhi\u1ec1u (l\u00ean t\u1edbi **15 l\u1ea7n**) cho n\u00ean c\u1ea7n ph\u1ea3i s\u1eed d\u1ee5ng metrics f1_score \u0111\u1ec3 \u0111\u00e1nh gi\u00e1 \u0111\u1ed9 hi\u1ec7u qu\u1ea3 c\u1ee7a m\u00f4 h\u00ecnh h\u01a1n l\u00e0 so **accuracy**<br>\nDo **f1_score** quan t\u00e2m \u0111\u1ebfn ph\u00e2n b\u1ed1 c\u1ee7a d\u1eef li\u1ec7u n\u00ean trong b\u00e0i n\u00e0y n\u00f3 s\u1ebd mang nhi\u1ec1u \u00fd ngh\u0129a h\u01a1n l\u00e0 **accuracy**.<br>\nV\u00e0 trong cu\u1ed9c thi **Kaggle** c\u0169ng b\u1ea3o l\u00e0 s\u1eed d\u1ee5ng **f1_score** thay v\u00ec **accuracy**\n\nF1 score l\u00e0 s\u1ef1 **harmonic mean** c\u1ee7a **Precision** v\u00e0 **Recall**. F1 Score nh\u1eadn gi\u00e1 tr\u1ecb trong kho\u1ea3ng ```0``` - ```1```<br>\n$$Precision = \\frac{TP}{TP+FP}$$\n$$Recall = \\frac{TP}{TP+FN}$$\n$$F1 = \\frac{2}{Recall^{-1} + Precision^{-1}}$$\n\n**Nh\u1eadn x\u00e9t:**<br>\nC\u00f3 th\u1ec3 coi \u0111\u00e2y l\u00e0 b\u00e0i to\u00e1n ph\u00e2n lo\u1ea1i nh\u1ecb ph\u00e2n. Th\u01b0\u1eddng th\u00ec ta s\u1ebd ngh\u0129 \u0111\u1ebfn c\u00e1c m\u00f4 h\u00ecnh tuy\u1ebfn t\u00ednh nh\u01b0 **Logistic Regression** hay **SVM** \u0111\u1ec3 ph\u00e2n lo\u1ea1i. Nh\u01b0ng \u0111\u00e2y l\u00e0 b\u00e0i to\u00e1n thi\u00ean v\u1ec1 ti\u1ec1n x\u1eed l\u00fd do \u0111\u1ea7u v\u00e0o l\u00e0 chu\u1ed7i c\u00e1c text, n\u00ean quan tr\u1ecdng l\u00e0 vi\u1ec7c m\u00ecnh x\u1eed l\u00fd chu\u1ed7i nh\u01b0 th\u1ebf n\u00e0o. Trong b\u00e0i n\u00e0y em s\u1ebd s\u1eed d\u1ee5ng th\u1eb1ng **Keras** v\u1edbi **LSTM Model** v\u00e0 **Embedings** c\u00e1c chu\u1ed7i text \u0111\u1ec3 gi\u1ea3m chi\u1ec1u d\u1eef li\u1ec7u","10b064bf":"### **a. \u0110\u1ecbnh ngh\u0129a c\u00e1c h\u00e0m x\u1eed l\u00fd**","1d14867f":"C\u00f3 th\u1ec3 th\u1ea5y r\u1eb1ng model m\u1edbi ho\u1ea1t \u0111\u1ed9ng hi\u1ec7u qu\u1ea3 h\u01a1n h\u1eb3n model c\u0169 khi m\u00e0 n\u00f3 \u0111\u00e3 \u0111\u1ea9y \u0111\u01b0\u1ee3c ```f1_score``` l\u00ean th\u00eam 0.01 t\u1ee9c 1% t\u1eeb m\u1ee9c ```0.67``` l\u00e0m c\u01a1 s\u1edf l\u00ean m\u1ee9c ```0.68x``` (x b\u00e9 t\u00ed teo).<br>\nTuy nhi\u00ean \u0111\u00e2y \u0111\u01b0\u1ee3c coi l\u00e0 m\u1ee9c c\u1ea3i thi\u1ec7n kh\u00e1 t\u1ed1t so v\u1edbi model c\u0169, ```loss``` c\u1ee7a t\u1eadp validate c\u0169ng nh\u1ecf h\u01a1n","0c01b246":"Khi s\u1eed d\u1ee5ng m\u00f4 h\u00ecnh v\u1edbi ma tr\u1eadn embeddings sinh ra t\u1eeb **Glove** th\u00ec n\u00f3 c\u0169ng cho ra m\u1ed9t k\u1ebft qu\u1ea3 kh\u00e1 gi\u1ed1ng v\u1edbi **Google News** khi m\u00e0 **```train```** th\u00ec m\u00f4 h\u00ecnh ho\u1ea1t \u0111\u1ed9ng kh\u00e1 hi\u1ec7u qu\u1ea3 <br>\nTuy nhi\u00ean c\u0169ng nh\u01b0 **Google News** th\u00ec \u0111\u1ed1i v\u1edbi t\u1eadp test, c\u00e0ng v\u1ec1 epoch cu\u1ed1i th\u00ec ```loss``` c\u00e0ng t\u0103ng cao, tuy nhi\u00ean c\u00f3 s\u1ef1 c\u1ea3i thi\u1ec7n h\u01a1n **Google News** khi m\u00e0 ```accuracy``` c\u00f3 h\u00ecnh d\u1ea1ng \u1ed5n \u0111\u1ecbnh h\u01a1n m\u1ed9t ch\u00fat.","8d72436d":"# **1. Kh\u1ea3o s\u00e1t d\u1eef li\u1ec7u**","e23c6cfd":"M\u1ed7i l\u1ea7n shuffle l\u1ea1i cho ra m\u1ed9t k\u1ebft qu\u1ea3 kh\u00e1c, nh\u01b0ng nh\u00ecn chung qua v\u00e0i l\u1ea7n th\u1ef1c nghi\u1ec7m quan s\u00e1t, em nh\u1eadn th\u1ea5y trong t\u1eadp d\u1eef li\u1ec7u \u1edf tr\u00ean:\n- C\u00f3 nhi\u1ec1u c\u00e2u m\u00e0 c\u00e1c t\u1eeb trong \u0111\u00f3 vi\u1ebft sai ch\u00ednh t\u1ea3, l\u1eabn l\u1ed9n gi\u1eefa ti\u1ebfng Anh-Anh v\u00e0 ti\u1ebfng Anh-M\u1ef9\n- Nhi\u1ec1u c\u00e2u s\u1eed d\u1ee5ng c\u00e1c t\u1eeb vi\u1ebft t\u1eaft: He's m\u00e0 \u0111\u00e1ng nh\u1ebd n\u00ean l\u00e0 He is.\n- C\u00f3 m\u1ed9t s\u1ed1 t\u1eeb vi\u1ebft t\u1eaft cho t\u00ean c\u00e1c t\u1ed5 ch\u1ee9c, hay c\u00e1c chu\u1ea9n m\u00e3 ho\u00e1 ki\u1ec3u RSA, DES\/3DES, ...\n- Nhi\u1ec1u c\u00e2u s\u1eed d\u1ee5ng c\u00e1c k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t cho n\u00ean s\u1ebd g\u00e2y \u1ea3nh h\u01b0\u1edfng l\u1edbn t\u1edbi m\u00f4 h\u00ecnh d\u1ef1 \u0111o\u00e1n\n\nV\u1ec1 m\u1eb7t n\u1ed9i dung h\u00e0m \u00fd th\u00ec c\u00f3 th\u1ec3 kh\u00f4ng \u1ea3nh h\u01b0\u1edfng nh\u01b0ng s\u1ebd \u1ea3nh h\u01b0\u1edfng khi x\u00e2p t\u1eadp t\u1eeb \u0111i\u1ec3n, c\u00f9ng m\u1ed9t \u00fd ngh\u0129a nh\u01b0ng c\u00e1c t\u1eeb l\u1ea1i \u0111\u01b0\u1ee3c t\u00ednh nhi\u1ec1u l\u1ea7n gi\u00e1 tr\u1ecb\n\n> **Th\u1ea5y r\u1eb1ng, b\u1ed9 d\u1eef li\u1ec7u \u0111\u00e3 cho c\u00f3 kh\u00e1 nhi\u1ec1u nhi\u1ec5u => c\u1ea7n lo\u1ea1i b\u1ecf nhi\u1ec1u nhi\u1ec1u nh\u1ea5t c\u00f3 th\u1ec3**\n\n**C\u00e1ch x\u1eed l\u00fd**\n- Lo\u1ea1i b\u1ecf stopwords c\u00f3 trong c\u00e2u\n- Stem c\u00e1c t\u1eeb c\u00f3 trong c\u00e2u (driver, driving, driven, drove -> drive)\n- Chu\u1ea9n ho\u00e1 c\u00e1c t\u1eeb v\u1ec1 d\u1ea1ng lowercase\n- Lo\u1ea1i b\u1ecf c\u00e1c k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t\n- Lo\u1ea1i b\u1ecf c\u00e1c \u0111\u01b0\u1eddng d\u1eabn li\u00ean k\u1ebft, lo\u1ea1i b\u1ecf c\u00e1c c\u00f4ng th\u1ee9c to\u00e1n h\u1ecdc, ...","01d72c54":"C\u0169ng nh\u01b0 2 anh \u1edf tr\u00ean th\u00ec m\u00f4 h\u00ecnh v\u1edbi ma tr\u1eadn embeddings c\u1ee7a **Paragram** l\u1ea1i cho ```loss``` c\u1ee7a t\u1eadp **```test```** t\u0103ng d\u1ea7n v\u1ec1 nh\u1eefng epoch cu\u1ed1i.<br>\nTuy nhi\u00ean \u0111\u00e2y v\u1eabn l\u00e0 k\u1ebft qu\u1ea3 ch\u1ea5p nh\u1eadn \u0111\u01b0\u1ee3c do s\u1ef1 ch\u00eanh l\u1ec7ch kh\u00f4ng qu\u00e1 nhi\u1ec1u","b0c68094":"Qua nhi\u1ec1u l\u1ea7n th\u1eed nghi\u1ec7m c\u00f3 th\u1ec3 th\u1ea5y r\u1eb1ng anh **Paragram** cho hi\u1ec7u n\u0103ng kh\u00f4ng nh\u01b0 k\u00ec v\u1ecdng khi anh \u1ea5y \u0111\u1ea1t \u0111\u1ed9 ph\u1ee7 t\u1ed1t nh\u1ea5t, oov th\u1ea5p nh\u1ea5t **(khi chu\u1ea9n ho\u00e1 text v\u1ec1 ```lowercase```)**, tuy nhi\u00ean model khi train v\u1edbi ma tr\u1eadn embeddings c\u1ee7a anh **Paragram** cho hi\u1ec7u n\u0103ng v\u1eabn k\u00e9m h\u01a1n anh **Glove**","3227bdbc":"### **b. Ti\u1ec1n x\u1eed l\u00fd c\u00e1c t\u1eadp d\u1eef li\u1ec7u**\nC\u00f4ng \u0111o\u1ea1n ti\u1ec1n x\u1eed l\u00fd kh\u00e1 nhanh khi t\u1ed1n trung b\u00ecnh l\u00e0 30 gi\u00e2y","09a6cc0b":"**```lemma_text()```**: \u0111\u1ec3 lo\u1ea1i b\u1ecf nh\u1eefng t\u1eeb c\u00f3 c\u00e1c digit cu\u1ed1i ki\u1ec3u s\/es\/ed m\u00e0 kh\u00f4ng l\u00e0m m\u1ea5t \u0111i nhi\u1ec1u \u00fd ngh\u0129a c\u1ee7a c\u00e2u (eg: words -> word)\nVi\u1ec7c s\u1eed d\u1ee5ng lemming thay v\u00ec stemming s\u1ebd ph\u1ea7n n\u00e0o c\u1ea3i thi\u1ec7n \u0111\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a m\u00f4 h\u00ecnh do:\n- **Stemming** c\u1eaft \u0111i ph\u1ea7n k\u00ed t\u1ef1 \u0111\u1eb7c bi\u1ec7t cu\u1ed1i t\u1eeb m\u1ed9t c\u00e1ch m\u00e1y m\u00f3c\n- **Lemming** th\u00ec l\u1ea1i th\u00f4ng qua t\u1eeb \u0111\u00f3, look up trong b\u1ea3ng t\u1eeb v\u1ef1ng \u0111\u01b0\u1ee3c thi\u1ebft k\u1ebf tr\u01b0\u1edbc v\u00e0 thay th\u1ebf v\u00e0o ch\u1ed7 \u0111\u00f3\nDo v\u1eady th\u00ec tuy lemming y\u00eau c\u1ea7u th\u1eddi gian x\u1eed l\u00fd l\u00e2u h\u01a1n, nh\u01b0ng \u0111em l\u1ea1i gi\u00e1 tr\u1ecb cao h\u01a1n v\u1ec1 \u0111\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a m\u00f4 h\u00ecnh","b3bea5c2":"\u1ede trong h\u00e0m **```clean_tag()```** em s\u1ebd lo\u1ea1i b\u1ecf b\u1eb1ng c\u00e1c bi\u1ec3u th\u1ee9c to\u00e1n h\u1ecdc, v\u00e0 n\u1ebfu c\u00f3 li\u00ean k\u1ebft trong c\u00e2u th\u00ec c\u0169ng s\u1ebd lo\u1ea1i b\u1ecf \u0111i v\u00e0 thay th\u1ebf b\u1eb1ng URL<br>\nDo c\u00e1c bi\u1ec3u th\u1ee9c to\u00e1n h\u1ecdc v\u00e0 \u0111\u01b0\u1eddng d\u1eabn li\u00ean k\u1ebft kh\u00f4ng c\u00f3 nhi\u1ec1u gi\u00e1 tr\u1ecb v\u1ec1 m\u1eb7t \u00fd ngh\u0129a trong ph\u00e2n lo\u1ea1i c\u00e2u h\u1ecdc sincere hay kh\u00f4ng sincere m\u00e0 l\u1ea1i l\u00e0m nhi\u1ec5u b\u1ed9 d\u1eef li\u1ec7u kh\u00e1 nhi\u1ec1u","df8b22db":"Theo d\u1ef1 \u0111o\u00e1n th\u00ec tr\u01b0\u1eddng **```target```** s\u1ebd l\u00e0 \u0111\u00e1nh d\u1ea5u c\u00e1c c\u00e2u h\u1ecfi sincere v\u00e0 insincere n\u00ean c\u1ea7n xem th\u1eed l\u00e0 trong tr\u01b0\u1eddng **```target```** \u0111\u00f3 c\u00f3 nh\u1eefng gi\u00e1 tr\u1ecb g\u00ec","dee1bff7":"### **a. Th\u1eed nghi\u1ec7m model 1**","95e46f48":"### **c. Nh\u1eadn x\u00e9t**\nNh\u01b0ng c\u00f3 th\u1ec3 th\u1ea5y m\u1ed9t \u0111i\u1ec1u l\u00e0 b\u1ed9 d\u1eef li\u1ec7u **train** v\u00e0 **test** d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o \u0111\u1ec1u \u1edf d\u1ea1ng plain text v\u00e0 m\u00f4 h\u00ecnh c\u1ee7a ch\u00fang ta s\u1ebd kh\u00f4ng th\u1ec3 hi\u1ec3u n\u1ed5i<br>\nV\u1eady h\u01b0\u1edbng gi\u1ea3i quy\u1ebft \u1edf \u0111\u00e2y em ngh\u0129 l\u00e0 l\u00e0 s\u1eed d\u1ee5ng m\u00e3 ho\u00e1 **one_hot** \u0111\u01b0a d\u1eef li\u1ec7u v\u1ec1 d\u1ea1ng binary \u0111\u1ec3 cho m\u00f4 h\u00ecnh c\u00f3 th\u1ec3 hi\u1ec3u \u0111\u01b0\u1ee3c","3c9decbf":"#### **Reference:** https:\/\/www.kaggle.com\/theoviel\/improve-your-score-with-some-text-preprocessing","8d90345b":"# **B\u00e1o c\u00e1o b\u00e0i t\u1eadp l\u1edbn m\u00f4n H\u1ecdc m\u00e1y**\n**Sinh vi\u00ean:** Phan H\u1ea3i Anh<br>\n**MSSV:** 19021213<br>","63b37a0b":"**```remove_stopwords()```**: t\u1ee9c l\u00e0 nh\u1eefng t\u1eeb, ch\u1eef d\u1ea1ng *'do'*, *'does'*, *'did'*, *'should'*, ...<br>\nTuy nhi\u00ean qua l\u1ea7n ch\u1ea1y m\u00e0 kh\u00f4ng lo\u1ea1i b\u1ecf stopwords th\u00ec k\u1ebft qu\u1ea3 thu nh\u1eadn l\u1ea1i kh\u1ea3 thi h\u01a1n v\u1edbi f1_score \u0111\u1ea1t trung b\u00ecnh ```0.64```<br>\nV\u1edbi l\u1ea7n ch\u1ea1y m\u00e0 lo\u1ea1i b\u1ecf stopwords th\u00ec k\u1ebft qu\u1ea3 thu nh\u1eadn l\u1ea1i \u0111\u01b0\u1ee3c v\u1edbi f1_score loanh quanh ```0.63x``` (x h\u1ecdc ti\u1ec3u h\u1ecdc) v\u00e0 cao nh\u1ea5t l\u00e0 ```0.633``` qua nhi\u1ec1u l\u1ea7n th\u1eed<br>\n- Qua \u0111\u00f3 c\u00f3 th\u1ec3 \u0111\u01b0a ra k\u1ebft lu\u1eadn r\u1eb1ng vi\u1ec7c lo\u1ea1i b\u1ecf stopwords trong m\u00f4 h\u00ecnh em x\u00e2y d\u1ef1ng \u1edf b\u00e0i n\u00e0y \u0111\u00e3 l\u00e0m m\u1ea5t \u0111i m\u1ed9t ph\u1ea7n \u00fd ngh\u0129a c\u1ee7a c\u00e2u v\u00e0 qua \u0111\u00f3 gi\u1ea3m \u0111\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a m\u00f4 h\u00ecnh\n- C\u00f3 v\u1ebb nh\u01b0 vi\u1ec7c lo\u1ea1i b\u1ecf stop_words \u0111\u00e3 gi\u00e1n ti\u1ebfp lo\u1ea1i b\u1ecf \u0111i nh\u1eefng t\u1eeb c\u00f3 trong vocab \u0111\u01b0\u1ee3c build t\u1eeb GoogleNews v\u00e0 l\u00e0m gi\u1ea3m \u0111\u1ed9 ph\u1ee7 c\u1ee7a vocab \u0111\u1ed1i v\u1edbi d\u1eef li\u1ec7u \u0111\u01b0\u1ee3c x\u1eed l\u00fd","e9d5ed6c":"Khi truy\u1ec1n optimizer b\u1eb1ng bi\u1ebfn ```opt``` c\u0169 th\u00ec \u0111\u00e3 x\u1ea3y ra l\u1ed7i v\u00e0 b\u1eaft bu\u1ed9c ph\u1ea3i t\u1ef1 pass l\u1ea1i metrics m\u1edbi v\u00e0o trong ```model.compile()``` do \u0111\u00f3 em \u0111\u00e3 truy\u1ec1n v\u00e0o optimizer l\u00e0 ```Adam``` v\u1edbi ```learning_rate``` b\u1eb1ng ```0.001``` gi\u1ed1ng \u1edf tr\u00ean \u0111\u1ec3 \u0111\u1ea3m b\u1ea3o s\u1ef1 c\u00f4ng b\u1eb1ng cho th\u1eed nghi\u1ec7m","4b5f1a43":"N\u1ebfu kh\u00f4ng kh\u1edfi t\u1ea1o l\u1ea1i ```strategy.scope()``` th\u00ec em b\u1ecb g\u1eb7p l\u1ed7i l\u00e0 ```loss``` c\u1ee7a model m\u1edbi s\u1ebd cao h\u01a1n m\u1ee9c b\u00ecnh th\u01b0\u1eddng d\u1eabn \u0111\u1ebfn vi\u1ec7c ```model``` ph\u1ea3i ch\u1ea1y qua nhi\u1ec1u epoch h\u01a1n v\u00e0 t\u1ed1n th\u1eddi gian \u0111\u1ec3 train h\u01a1n<br>\nN\u00ean em ph\u1ea3i th\u1ef1c hi\u1ec7n vi\u1ec7c init l\u1ea1i ```strategy.scope()``` m\u1ed7i l\u1ea7n t\u1ea1o v\u00e0 compile m\u1ed9t model m\u1edbi.","1ec468fd":"Sau khi build model ta \u0111\u01b0\u1ee3c:<br>\n&nbsp;<br>\n![](https:\/\/scontent-hkt1-1.xx.fbcdn.net\/v\/t1.15752-9\/188040073_326765032379284_902345591056315145_n.png?_nc_cat=101&ccb=1-3&_nc_sid=ae9488&_nc_ohc=4BVyEHYLeQ4AX9of6VB&_nc_ht=scontent-hkt1-1.xx&oh=dc02410144f78632c7701dcc691dac97&oe=60DD8312)","c70c362c":"C\u00f3 th\u1ec3 d\u1ec5 d\u00e0ng nh\u1eadn th\u1ea5y anh **Google News** cho \u0111\u1ed9 ph\u1ee7 kh\u00f4ng t\u1ed1t b\u1eb1ng anh **Glove** khi anh **Glove** \u0111\u1ea1t \u0111\u1ed9 ph\u1ee7 tr\u00ean **99.39%** v\u00e0 **72.81%** v\u1edbi corpus qua \u0111\u00f3 c\u00f3 th\u1ec3 \u0111o\u00e1n \u0111\u01b0\u1ee3c r\u1eb1ng ma tr\u1eadn embeddings c\u1ee7a anh **Glove** s\u1ebd gi\u00fap cho m\u00f4 h\u00ecnh \u0111\u1ea1t \u0111\u01b0\u1ee3c hi\u1ec7u su\u1ea5t cao h\u01a1n","12614d21":"Model c\u0169 kh\u00e1 ch\u1eadt v\u1eadt \u0111\u1ec3 \u0111\u1ea1t m\u1ee9c maximum l\u00e0 ```0.675``` th\u00ec model m\u1edbi d\u1ec5 d\u00e0ng \u0111\u1ea1t \u0111\u01b0\u1ee3c m\u1ee9c tr\u00ean ```0.68``` c\u1ee7a ```f1_score``` v\u00e0 x\u1ea5p x\u1ec9 \u1edf ng\u01b0\u1ee1ng ```0.69```<br>\nV\u1eady c\u00f3 th\u1ec3 y\u00ean t\u00e2m s\u1eed d\u1ee5ng model m\u1edbi b\u1edfi v\u00ec hi\u1ec7u su\u1ea5t v\u00e0 \u0111\u1ed9 \u1ed5n \u0111\u1ecbnh c\u1ee7a n\u00f3","b7fedcd6":"### **a. Glove**","6da355f6":"S\u1eeda nh\u1eefng t\u1eeb d\u1ec5 g\u00e2y hi\u1ec3u l\u1ea7m **```correct_misspell()```**, \u1edf trong m\u1ea3ng \u0111\u1ecbnh ngh\u0129a s\u1eb5n th\u00ec em s\u1ebd convert ti\u1ebfng Anh-Anh sang ti\u1ebfng Anh-M\u1ef9, b\u00ean c\u1ea1nh \u0111\u00f3 s\u1eeda sai cho nh\u1eefng t\u1eeb d\u1ec5 vi\u1ebft sai<br>\nB\u1edfi v\u00ec Quora fetch c\u00e1c c\u00e2u h\u1ecfi c\u1ee7a ng\u01b0\u1eddi d\u00f9ng n\u00ean vi\u1ec7c sai ch\u00ednh t\u1ea3 l\u00e0 kh\u00f4ng th\u1ec3 tr\u00e1nh kh\u1ecfi<br>\nM\u1ee5c \u0111\u00edch l\u00e0 \u0111\u1ec3 convert sang c\u00e0ng nhi\u1ec1u t\u1eeb c\u00e0ng t\u1ed1t, qua \u0111\u00f3 \u0111\u1ea1t \u0111\u01b0\u1ee3c \u0111\u1ed9 ph\u1ee7 cao v\u1edbi vocab \u0111\u01b0\u1ee3c build t\u1eeb t\u1ec7p GoogleNews \u0111\u00ednh k\u00e8m trong file ```embeddings.zip```","09c799ed":"B\u00e2y gi\u1edd em s\u1ebd th\u1eed nghi\u1ec7m m\u1ed9t model m\u1edbi, d\u1ef1a tr\u00ean ngu\u1ed3n tham kh\u1ea3o t\u1eeb t\u00e1c gi\u1ea3 **[taziz437](https:\/\/www.kaggle.com\/taziz437)** v\u1edbi b\u00e0i d\u1ef1 thi [TariqAziz](https:\/\/www.kaggle.com\/taziz437\/tariqaziz) <br>\nV\u1edbi **model** c\u0169 t\u1ee9c ```get_model_origin``` th\u00ec em \u0111\u00e3 build model v\u1edbi m\u00f4 h\u00ecnh **LSTM** 128 chi\u1ec1u<br>\nC\u00f2n v\u1edbi t\u00e1c gi\u1ea3 **[taziz437](https:\/\/www.kaggle.com\/taziz437)** anh \u1ea5y \u0111\u00e3 k\u1ebft h\u1ee3p m\u00f4 h\u00ecnh **LSTM** 256 chi\u1ec1u v\u00e0 **GRU** 128 chi\u1ec1u \u0111\u1ec3 t\u1ea1o model v\u1edbi 2 l\u1edbp **Pooling1D**<br>\nTheo c\u1ea3m quan \u0111\u00e1nh gi\u00e1 th\u00ec c\u00f3 v\u1ebb **model** n\u00e0y s\u1ebd cho m\u1ed9t hi\u1ec7u n\u0103ng t\u1ed1t h\u01a1n v\u00e0 \u1ed5n \u0111\u1ecbnh h\u01a1n so v\u1edbi **model** m\u00e0 em \u0111\u00e3 t\u1ef1 t\u1ea1o \u1edf tr\u00ean.<br>\nD\u00f4ng d\u00e0i th\u00ec c\u0169ng \u0111\u1ebfn th\u1ebf, em s\u1ebd th\u1eed nghi\u1ec7m model m\u1edbi \u1edf ngay b\u00ean d\u01b0\u1edbi."}}