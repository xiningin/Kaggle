{"cell_type":{"362d43be":"code","56e8f21b":"code","ec74ead2":"code","5cc3da86":"code","6ea24312":"code","ce9dd64d":"code","811ec963":"code","67609e75":"code","92bf1ddd":"code","2b12d3aa":"code","3c20dd6b":"code","a5d56cad":"code","d7254dbb":"code","74bb4739":"code","de027eec":"code","6c2ca8b0":"code","f28a4cf8":"code","0e82190d":"code","7db6bbd7":"code","80ec0b22":"code","1b3ff39f":"code","9a0c4003":"code","96791dbd":"code","767ebb93":"code","1f18155e":"code","c46fec46":"code","acab6727":"code","3dcde27e":"code","ddf0ec57":"code","7564ecb2":"code","662d9076":"code","268ce8f7":"code","577b9a1a":"code","39fe6e9d":"code","6b18d6ba":"code","2e0ff55b":"code","a7e3d9c2":"code","b832e59f":"code","89298f00":"code","7a11eff1":"code","d447f832":"code","81b5278a":"code","f61c69b3":"code","cde02931":"code","8cfe8934":"code","098bf69b":"code","a8bac926":"markdown","7b0bc718":"markdown","97857cf5":"markdown","9a3700dd":"markdown","3a5465bc":"markdown","d283116e":"markdown","9bff7929":"markdown","cbc8e43c":"markdown","44844aa1":"markdown","550103a5":"markdown","6f2253e9":"markdown","0704edd5":"markdown","4777c8d8":"markdown","64c0567f":"markdown","dd69569f":"markdown","0e17ae6f":"markdown","ef9a9c02":"markdown","c031e096":"markdown","77d66937":"markdown","35a851c8":"markdown","0bc8961d":"markdown","32e9f42f":"markdown","f87f16e5":"markdown","955b22f6":"markdown","e3ec26ee":"markdown","c84e13c1":"markdown","70204ac5":"markdown","d710bf76":"markdown","7a0cc128":"markdown","234df675":"markdown","78b64ebe":"markdown","ff6214e5":"markdown","6644b571":"markdown","38431f1a":"markdown","fcf25a82":"markdown","a8add0cb":"markdown"},"source":{"362d43be":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\n\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","56e8f21b":"train_df = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/test.csv\")\nsub_df = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/sample_submission.csv\")","ec74ead2":"train_df.shape","5cc3da86":"train_df.head()","6ea24312":"train_df.columns","ce9dd64d":"train_df.dtypes","811ec963":"train_df.head()","67609e75":"train_df.isna().sum().sum()","92bf1ddd":"sns.countplot(x=\"target\", data=train_df)","2b12d3aa":"for col in ['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4']:\n    print(col)\n    print(train_df[col].value_counts())","3c20dd6b":"for col in ['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4']:\n    \n    f, ax = plt.subplots(1, 1, figsize=(10,5))\n    \n    sns.set(style=\"white\", context=\"talk\")\n    \n    sns.countplot(x=col, data=train_df, ax=ax, palette=\"Set1\")\n    f.tight_layout()","a5d56cad":"for col in ['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4']:\n    \n    f, ax = plt.subplots(1, 1, figsize=(20,8))\n    \n    sns.set(style=\"white\", context=\"talk\")\n    \n    sns.countplot(x=col, hue=\"target\", data=train_df, ax=ax, palette=\"Set2\")\n    f.tight_layout()","d7254dbb":"for col in ['nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6','nom_7', 'nom_8']:\n    print(col)\n    print(train_df[col].value_counts())","74bb4739":"for col in ['nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8']:\n    \n    f, ax = plt.subplots(1, 1, figsize=(20,8))\n    \n    \n    sns.countplot(x=col, data=train_df, ax=ax, palette=\"Set1\")\n    f.tight_layout()","de027eec":"for col in ['nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8']:\n    \n    f, ax = plt.subplots(1, 1, figsize=(20,8))\n    \n    \n    sns.countplot(x=col, hue=\"target\", data=train_df, ax=ax, palette=\"Set2\")\n    f.tight_layout()","6c2ca8b0":"for col in [\"ord_0\", \"ord_1\", \"ord_2\", \"ord_3\", \"ord_4\", \"ord_5\"]:\n    print(col)\n    print(train_df[col].value_counts())","f28a4cf8":"for col in [\"ord_0\", \"ord_1\", \"ord_2\", \"ord_3\", \"ord_4\", \"ord_5\"]:\n    \n    f, ax = plt.subplots(1, 1, figsize=(20,8))\n    \n    sns.countplot(x=col, data=train_df, ax=ax, palette=\"Set1\")\n    f.tight_layout()\n    \nimport gc\ngc.collect();","0e82190d":"for col in [\"ord_0\", \"ord_1\", \"ord_2\", \"ord_3\", \"ord_4\", \"ord_5\"]:\n    \n    f, ax = plt.subplots(1, 1, figsize=(20,8))\n    \n    sns.countplot(x=col, hue=\"target\", data=train_df, ax=ax, palette=\"Set2\")\n    f.tight_layout()\n    \nimport gc\ngc.collect();","7db6bbd7":"train_df.loc[train_df['bin_3'] == 'T', 'bin_3'] = 1\ntrain_df.loc[train_df['bin_3'] == 'F', 'bin_3'] = 0\n\ntest_df.loc[test_df['bin_3'] == 'T', 'bin_3'] = 1\ntest_df.loc[test_df['bin_3'] == 'F', 'bin_3'] = 0\n\ntrain_df.loc[train_df['bin_4'] == 'Y', 'bin_4'] = 1\ntrain_df.loc[train_df['bin_4'] == 'N', 'bin_4'] = 0\n\ntest_df.loc[test_df['bin_4'] == 'Y', 'bin_4'] = 1\ntest_df.loc[test_df['bin_4'] == 'N', 'bin_4'] = 0","80ec0b22":"from sklearn.preprocessing import LabelEncoder\n\n\n\ncolumns = ['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']\n\nfor col in columns:\n    \n    le = LabelEncoder()\n    le.fit(train_df[col].to_list()+test_df[col].to_list())\n    train_df[col] = le.transform(train_df[col])\n    test_df[col] = le.transform(test_df[col])\n    \nimport gc\ngc.collect();","1b3ff39f":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nimport gc\n\nfor col in [\"ord_0\", \"ord_1\", \"ord_2\", \"ord_3\", \"ord_4\", \"ord_5\"]:\n    \n    print(col, \"one-hot-encoding\")\n    le = LabelEncoder()\n    le.fit(train_df[col].to_list()+test_df[col].to_list())\n    train_df[col] = le.transform(train_df[col])\n    test_df[col] = le.transform(test_df[col])\n\n    \n    ohe = OneHotEncoder()\n    ohe.fit(np.array(train_df[col].values.tolist() + test_df[col].values.tolist()).reshape(-1, 1))\n    train_col = ohe.transform(train_df[col].values.reshape(-1,1)).toarray().astype(int)\n    test_col = ohe.transform(test_df[col].values.reshape(-1,1)).toarray().astype(int)\n\n    dfOneHot = pd.DataFrame(train_col, columns = [col+\".\"+str(int(i)) for i in range(train_col.shape[1])])\n    train_df = pd.concat([train_df, dfOneHot], axis=1)\n\n    dfOneHot = pd.DataFrame(test_col, columns = [col+\".\"+str(int(i)) for i in range(test_col.shape[1])])\n    test_df = pd.concat([test_df, dfOneHot], axis=1)\n\n    train_df.drop(col, axis=1, inplace=True);\n    test_df.drop(col, axis=1, inplace=True);\n    \n    del train_col; del test_col; del dfOneHot;\n    gc.collect();","9a0c4003":"train_df.columns","96791dbd":"train_df.head()","767ebb93":"train_df.describe()","1f18155e":"train_columns = train_df.columns.to_list()","c46fec46":"for elem in [\"id\",\"target\"]:\n    train_columns.remove(elem)","acab6727":"y = train_df['target']\nX = train_df.drop(['target'], axis=1)\nX = X[train_columns]\n\nclf_stats_df = pd.DataFrame(columns=[\"clf_name\", \"F1-score\", \"auc-score\"])","3dcde27e":"from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, RepeatedStratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nimport time\nimport seaborn as sns\nimport scikitplot as skplt\n\n# create a 80\/20 split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=42, test_size=0.2, stratify = y)\n\nimport xgboost as xgb\n\nstart_time = time.time()\n\npredictions_probas_list = np.zeros([len(yvalid), 2])\npredictions_test_xgb = np.zeros(len(test_df))\nnum_of_folds = 2\nnum_fold = 0\n    #feature_importance_df = pd.DataFrame()\n\nfolds = StratifiedKFold(n_splits=num_of_folds, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Stratified Fold:\", num_fold)\n    num_fold = num_fold + 1\n    print()\n\n    clf_stra_xgb = xgb.XGBClassifier(n_estimators = 4000,\n                                     objective= 'binary:logistic',\n                                     nthread=-1,\n                                     seed=42)\n\n    clf_stra_xgb.fit(xtrain_stra, ytrain_stra, eval_set=[(xtrain_stra, ytrain_stra), (xvalid_stra, yvalid_stra)], \n                early_stopping_rounds=100, eval_metric='auc', verbose=250)\n\n    #fold_importance_df = pd.DataFrame()\n    #fold_importance_df[\"feature\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].index\n    #fold_importance_df[\"fscore\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].values\n    #fold_importance_df[\"fold\"] = n_fold + 1\n    #feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    predictions = clf_stra_xgb.predict(xvalid)\n    predictions_probas = clf_stra_xgb.predict_proba(xvalid)\n    predictions_probas_list += predictions_probas\/num_of_folds\n\n    predictions_test_xgb += clf_stra_xgb.predict_proba(test_df[xtrain.columns])[:,1]\/num_of_folds\n\n\npredictions = np.argmax(predictions_probas_list, axis=1)\n\nprint()\nprint(classification_report(yvalid, predictions))\n\nprint()\nprint(\"CV f1_score\", f1_score(yvalid, predictions, average = \"macro\"))\n\nprint()\nprint(\"CV roc_auc_score\", roc_auc_score(yvalid, predictions_probas_list[:,1], average = \"macro\"))\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_confusion_matrix(yvalid, predictions, normalize=True)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_roc(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_ks_statistic(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_precision_recall(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_cumulative_gain(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_lift_curve(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(12, 38)})\nxgb.plot_importance(clf_stra_xgb, title='Feature importance', xlabel='F score', ylabel='Features')\n\nclf_stats_df = clf_stats_df.append({\"clf_name\": \"clf_stra_xgb\",\n                     \"F1-score\":f1_score(yvalid, predictions, average = \"macro\"),\n                     \"auc-score\": roc_auc_score(yvalid, predictions_probas_list[:,1], average = \"macro\")}, ignore_index=True)\n\nprint()\nprint(\"elapsed time in seconds: \", time.time() - start_time)\nprint()\nimport gc\ngc.collect();","ddf0ec57":"from sklearn.feature_selection import SelectFromModel\n\nselection = SelectFromModel(clf_stra_xgb, threshold=0.001, prefit=True)\nselect_X = selection.transform(X)\n\n# https:\/\/stackoverflow.com\/questions\/41088576\/is-there-away-to-output-selected-columns-names-from-selectfrommodel-method\nfeature_idx = selection.get_support()\nfeature_name = X.columns[feature_idx]\nselect_X = pd.DataFrame(select_X, columns = feature_name)","7564ecb2":"from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, RepeatedStratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nimport time\nimport seaborn as sns\nimport scikitplot as skplt\n\n# create a 80\/20 split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(select_X, y, random_state=42, test_size=0.2, stratify = y)\n\nimport xgboost as xgb\n\nstart_time = time.time()\n\npredictions_probas_list = np.zeros([len(yvalid), 2])\npredictions_test_fs_xgb = np.zeros(len(test_df))\nnum_of_folds = 2\nnum_fold = 0\n    #feature_importance_df = pd.DataFrame()\n\nfolds = StratifiedKFold(n_splits=num_of_folds, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Stratified Fold:\", num_fold)\n    num_fold = num_fold + 1\n    print()\n\n    clf_stra_fs_xgb = xgb.XGBClassifier(n_estimators = 4000,\n                                     objective= 'binary:logistic',\n                                     nthread=-1,\n                                     seed=42)\n\n    clf_stra_fs_xgb.fit(xtrain_stra, ytrain_stra, eval_set=[(xtrain_stra, ytrain_stra), (xvalid_stra, yvalid_stra)], \n                early_stopping_rounds=100, eval_metric='auc', verbose=250)\n\n    #fold_importance_df = pd.DataFrame()\n    #fold_importance_df[\"feature\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].index\n    #fold_importance_df[\"fscore\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].values\n    #fold_importance_df[\"fold\"] = n_fold + 1\n    #feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    predictions = clf_stra_fs_xgb.predict(xvalid)\n    predictions_probas = clf_stra_fs_xgb.predict_proba(xvalid)\n    predictions_probas_list += predictions_probas\/num_of_folds\n\n    predictions_test_fs_xgb += clf_stra_fs_xgb.predict_proba(test_df[xtrain.columns])[:,1]\/num_of_folds\n\n\npredictions = np.argmax(predictions_probas, axis=1)\n\nprint()\nprint(classification_report(yvalid, predictions))\n\nprint()\nprint(\"CV f1_score\", f1_score(yvalid, predictions, average = \"macro\"))\n\nprint()\nprint(\"CV roc_auc_score\", roc_auc_score(yvalid, predictions_probas_list[:,1], average = \"macro\"))\n\nprint()\nprint(\"elapsed time in seconds: \", time.time() - start_time)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_confusion_matrix(yvalid, predictions, normalize=True)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_roc(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_ks_statistic(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_precision_recall(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_cumulative_gain(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_lift_curve(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(12, 38)})\nxgb.plot_importance(clf_stra_fs_xgb, title='Feature importance', xlabel='F score', ylabel='Features')\n\nclf_stats_df = clf_stats_df.append({\"clf_name\": \"clf_stra_fs_xgb\",\n                     \"F1-score\":f1_score(yvalid, predictions, average = \"macro\"),\n                     \"auc-score\": roc_auc_score(yvalid, predictions_probas_list[:,1], average = \"macro\")}, ignore_index=True)\n\nprint()\nimport gc\ngc.collect();","662d9076":"from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nfrom sklearn.metrics import roc_auc_score\n\nxtrain, xvalid, ytrain, yvalid = train_test_split(select_X, y, stratify = y, random_state=42, test_size=0.2)\n\n# https:\/\/github.com\/hyperopt\/hyperopt\/wiki\/FMin\n\ndef objective(space):\n\n    clf = xgb.XGBClassifier(n_estimators = 3000,\n                            max_depth = space['max_depth'],\n                            min_child_weight = space['min_child_weight'],\n                            subsample = space['subsample'],\n                            gamma=space['gamma'],\n                            colsample_bytree=space['colsample_bytree'],\n                            #reg_alpha = space['reg_alpha'],\n                            #reg_lambda = space['reg_lambda'],\n                            scale_pos_weight = space[\"scale_pos_weight\"],\n                            objective= 'binary:logistic',\n                            nthread=-1,\n                            seed=42)\n\n    eval_set  = [( xtrain, ytrain), ( xvalid, yvalid)]\n\n    clf.fit(xtrain, ytrain,\n            eval_set=eval_set, eval_metric=\"auc\",\n            early_stopping_rounds=100, verbose=250)\n\n    pred = clf.predict_proba(xvalid)[:,1]\n    auc = roc_auc_score(yvalid, pred)\n    print(\"SCORE:\", auc)\n\n    return{'loss':1-auc, 'status': STATUS_OK }\n\n# https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html#parameters-for-tree-booster\nspace ={\n        'eta': hp.uniform('eta', 0.025, 0.5),\n        'max_depth': hp.choice('max_depth', range(3, 12)),\n        'min_child_weight': hp.quniform ('min_child_weight', 1, 10, 1),\n        'subsample': hp.uniform ('subsample', 0.5, 1),\n        'gamma': hp.uniform('gamma', 0.1, 1),\n        'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n        #'reg_alpha': hp.uniform ('reg_alpha', 0, 1),\n        #'reg_lambda': hp.uniform ('reg_lambda', 0, 10),\n        'scale_pos_weight': hp.uniform(\"scale_pos_weight\", 1, np.round(train_df.target.value_counts()[0]\/train_df.target.value_counts()[1],5))\n    }\n\n\ntrials = Trials()\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=2,\n            trials=trials,\n            verbose = 0)\n\nprint(best)","268ce8f7":"import gc\ngc.collect();","577b9a1a":"best","39fe6e9d":"from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, RepeatedStratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nimport time\nimport seaborn as sns\nimport scikitplot as skplt\n\n# create a 80\/20 split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(select_X, y, random_state=42, test_size=0.2, stratify = y)\n\nimport xgboost as xgb\n\nstart_time = time.time()\n\npredictions_probas_list = np.zeros([len(yvalid), 2])\npredictions_test_fs_tuned_xgb = np.zeros(len(test_df))\nnum_of_folds = 2\nnum_fold = 0\n    #feature_importance_df = pd.DataFrame()\n\nfolds = StratifiedKFold(n_splits=num_of_folds, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Stratified Fold:\", num_fold)\n    num_fold = num_fold + 1\n    print()\n\n    clf_stra_fs_tuned_xgb = xgb.XGBClassifier(n_estimators = 4000,\n                                           objective= 'binary:logistic',\n                                           nthread=-1,\n                                           eta = best['eta'],\n                                           max_depth=best['max_depth'],\n                                           min_child_weight=best['min_child_weight'],\n                                           subsample=best['subsample'],\n                                           gamma=best['gamma'],\n                                           colsample_bytree=best['colsample_bytree'],\n                                           #reg_alpha = best['reg_alpha'],\n                                           #reg_lambda = best['reg_lambda'],\n                                           scale_pos_weight = best['scale_pos_weight'],\n                                           seed=42)\n    \n\n    clf_stra_fs_tuned_xgb.fit(xtrain_stra, ytrain_stra, eval_set=[(xtrain_stra, ytrain_stra), (xvalid_stra, yvalid_stra)], \n                early_stopping_rounds=100, eval_metric='auc', verbose=250)\n\n    #fold_importance_df = pd.DataFrame()\n    #fold_importance_df[\"feature\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].index\n    #fold_importance_df[\"fscore\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].values\n    #fold_importance_df[\"fold\"] = n_fold + 1\n    #feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    predictions = clf_stra_fs_tuned_xgb.predict(xvalid)\n    predictions_probas = clf_stra_fs_tuned_xgb.predict_proba(xvalid)\n    predictions_probas_list += predictions_probas\/num_of_folds\n\n    predictions_test_fs_tuned_xgb += clf_stra_fs_tuned_xgb.predict_proba(test_df[xtrain.columns])[:,1]\/num_of_folds\n\n\npredictions = np.argmax(predictions_probas_list, axis=1)\n\nprint()\nprint(classification_report(yvalid, predictions))\n\nprint()\nprint(\"CV f1_score\", f1_score(yvalid, predictions, average = \"macro\"))\n\nprint()\nprint(\"CV roc_auc_score\", roc_auc_score(yvalid, predictions_probas_list[:,1], average = \"macro\"))\n\nprint()\nprint(\"elapsed time in seconds: \", time.time() - start_time)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_confusion_matrix(yvalid, predictions, normalize=True)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_roc(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_ks_statistic(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_precision_recall(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_cumulative_gain(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_lift_curve(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(12, 38)})\nxgb.plot_importance(clf_stra_fs_tuned_xgb, title='Feature importance', xlabel='F score', ylabel='Features')\n\nclf_stats_df = clf_stats_df.append({\"clf_name\": \"clf_stra_fs_tuned_xgb\",\n                     \"F1-score\":f1_score(yvalid, predictions, average = \"macro\"),\n                     \"auc-score\": roc_auc_score(yvalid, predictions_probas_list[:,1], average = \"macro\")}, ignore_index=True)\n\nprint()\nimport gc\ngc.collect();","6b18d6ba":"from imblearn.over_sampling import SMOTE\n\nX_resampled, y_resampled = SMOTE(random_state=42).fit_resample(select_X, y)\nX_resampled = pd.DataFrame(X_resampled, columns= select_X.columns)\ny_resampled = pd.Series(y_resampled)\n\nimport gc\ngc.collect();","2e0ff55b":"from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, RepeatedStratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nimport time\nimport seaborn as sns\nimport scikitplot as skplt\n\n# create a 80\/20 split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(X_resampled, y_resampled, random_state=42, test_size=0.2, stratify=y_resampled)\n\nimport xgboost as xgb\n\nstart_time = time.time()\n\npredictions_probas_list = np.zeros([len(yvalid), 2])\npredictions_test_fs_smote_xgb = np.zeros(len(test_df))\nnum_of_folds = 2\nnum_fold = 0\n    #feature_importance_df = pd.DataFrame()\n\nfolds = StratifiedKFold(n_splits=num_of_folds, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Stratified Fold:\", num_fold)\n    num_fold = num_fold + 1\n    print()\n\n    clf_stra_fs_smote_xgb = xgb.XGBClassifier(n_estimators = 4000,\n                                     objective= 'binary:logistic',\n                                     nthread=-1,\n                                     seed=42)\n\n    clf_stra_fs_smote_xgb.fit(xtrain_stra, ytrain_stra, eval_set=[(xtrain_stra, ytrain_stra), (xvalid_stra, yvalid_stra)], \n                early_stopping_rounds=100, eval_metric='auc', verbose=250)\n\n    #fold_importance_df = pd.DataFrame()\n    #fold_importance_df[\"feature\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].index\n    #fold_importance_df[\"fscore\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].values\n    #fold_importance_df[\"fold\"] = n_fold + 1\n    #feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    predictions = clf_stra_fs_smote_xgb.predict(xvalid)\n    predictions_probas = clf_stra_fs_smote_xgb.predict_proba(xvalid)\n    predictions_probas_list += predictions_probas\/num_of_folds\n\n    predictions_test_fs_smote_xgb += clf_stra_fs_smote_xgb.predict_proba(test_df[xtrain.columns])[:,1]\/num_of_folds\n\n\npredictions = np.argmax(predictions_probas, axis=1)\n\nprint()\nprint(classification_report(yvalid, predictions))\n\nprint()\nprint(\"CV f1_score\", f1_score(yvalid, predictions, average = \"macro\"))\n\nprint()\nprint(\"CV roc_auc_score\", roc_auc_score(yvalid, predictions_probas_list[:,1], average = \"macro\"))\n\nprint()\nprint(\"elapsed time in seconds: \", time.time() - start_time)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_confusion_matrix(yvalid, predictions, normalize=True)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_roc(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_ks_statistic(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_precision_recall(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_cumulative_gain(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_lift_curve(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(12, 38)})\nxgb.plot_importance(clf_stra_fs_smote_xgb, title='Feature importance', xlabel='F score', ylabel='Features')\n\nclf_stats_df = clf_stats_df.append({\"clf_name\": \"clf_stra_fs_smote_xgb\",\n                     \"F1-score\":f1_score(yvalid, predictions, average = \"macro\"),\n                     \"auc-score\": roc_auc_score(yvalid, predictions_probas_list[:,1], average = \"macro\")}, ignore_index=True)\n\nprint()\nimport gc\ngc.collect();","a7e3d9c2":"from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n\nxtrain, xvalid, ytrain, yvalid = train_test_split(X_resampled, y_resampled, random_state=42, test_size=0.2, stratify=y_resampled)\n\n# https:\/\/github.com\/hyperopt\/hyperopt\/wiki\/FMin\n\ndef objective(space):\n\n    clf = xgb.XGBClassifier(n_estimators = 3000,\n                            max_depth = int(space['max_depth']),\n                            min_child_weight = space['min_child_weight'],\n                            subsample = space['subsample'],\n                            gamma=space['gamma'],\n                            colsample_bytree=space['colsample_bytree'],\n                            #reg_alpha = space['reg_alpha'],\n                            #reg_lambda = space['reg_lambda'],\n                            objective= 'binary:logistic',\n                            nthread=-1,\n                            seed=42)\n\n    eval_set  = [( xtrain, ytrain), ( xvalid, yvalid)]\n\n    clf.fit(xtrain, ytrain,\n            eval_set=eval_set, eval_metric=\"auc\",\n            early_stopping_rounds=100, verbose=250)\n\n    pred = clf.predict_proba(xvalid)[:,1]\n    auc = roc_auc_score(yvalid, pred)\n    print(\"SCORE:\", auc)\n\n    return{'loss':1-auc, 'status': STATUS_OK }\n\n# https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html#parameters-for-tree-booster\nspace ={\n        'eta': hp.uniform('eta', 0.025, 0.5),\n        'max_depth': hp.choice('max_depth', range(3, 12)),\n        'min_child_weight': hp.quniform ('min_child_weight', 1, 10, 1),\n        'subsample': hp.uniform ('subsample', 0.5, 1),\n        'gamma': hp.uniform('gamma', 0.1, 1),\n        'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n        #'reg_alpha': hp.uniform ('reg_alpha', 0, 1),\n        #'reg_lambda': hp.uniform ('reg_lambda', 0, 10)\n    }\n\n\ntrials = Trials()\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=2,\n            trials=trials,\n            verbose = 0)\n\nprint(best)\n\nimport gc\ngc.collect();","b832e59f":"best","89298f00":"from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, RepeatedStratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nimport time\nimport seaborn as sns\nimport scikitplot as skplt\n\n# create a 80\/20 split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(X_resampled, y_resampled, random_state=42, test_size=0.2, stratify=y_resampled)\n\nimport xgboost as xgb\n\nstart_time = time.time()\n\npredictions_probas_list = np.zeros([len(yvalid), 2])\npredictions_test_fs_smote_tuned_xgb = np.zeros(len(test_df))\nnum_of_folds = 2\nnum_fold = 0\n    #feature_importance_df = pd.DataFrame()\n\nfolds = StratifiedKFold(n_splits=num_of_folds, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Stratified Fold:\", num_fold)\n    num_fold = num_fold + 1\n    print()\n\n    clf_stra_fs_smote_tuned_xgb = xgb.XGBClassifier(n_estimators = 4000,\n                                           objective= 'binary:logistic',\n                                           nthread=-1,\n                                           eta = best['eta'],\n                                           max_depth=int(best['max_depth']),\n                                           min_child_weight=best['min_child_weight'],\n                                           subsample=best['subsample'],\n                                           gamma=best['gamma'],\n                                           colsample_bytree=best['colsample_bytree'],\n                                           #reg_alpha = best['reg_alpha'],\n                                           #reg_lambda = best['reg_lambda'],\n                                           seed=42)\n    \n\n    clf_stra_fs_smote_tuned_xgb.fit(xtrain_stra, ytrain_stra, eval_set=[(xtrain_stra, ytrain_stra), (xvalid_stra, yvalid_stra)], \n                early_stopping_rounds=100, eval_metric='auc', verbose=250)\n\n    #fold_importance_df = pd.DataFrame()\n    #fold_importance_df[\"feature\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].index\n    #fold_importance_df[\"fscore\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].values\n    #fold_importance_df[\"fold\"] = n_fold + 1\n    #feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    predictions = clf_stra_fs_smote_tuned_xgb.predict(xvalid)\n    predictions_probas = clf_stra_fs_smote_tuned_xgb.predict_proba(xvalid)\n    predictions_probas_list += predictions_probas\/num_of_folds\n\n    predictions_test_fs_smote_tuned_xgb += clf_stra_fs_smote_tuned_xgb.predict_proba(test_df[xtrain.columns])[:,1]\/num_of_folds\n\n\npredictions = np.argmax(predictions_probas, axis=1)\n\nprint()\nprint(classification_report(yvalid, predictions))\n\nprint()\nprint(\"CV f1_score\", f1_score(yvalid, predictions, average = \"macro\"))\n\nprint()\nprint(\"CV roc_auc_score\", roc_auc_score(yvalid, predictions_probas_list[:,1], average = \"macro\"))\n\nprint()\nprint(\"elapsed time in seconds: \", time.time() - start_time)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_confusion_matrix(yvalid, predictions, normalize=True)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_roc(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_ks_statistic(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_precision_recall(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_cumulative_gain(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_lift_curve(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(12, 38)})\nxgb.plot_importance(clf_stra_fs_smote_tuned_xgb, title='Feature importance', xlabel='F score', ylabel='Features')\n\nclf_stats_df = clf_stats_df.append({\"clf_name\": \"clf_stra_fs_smote_tuned_xgb\",\n                     \"F1-score\":f1_score(yvalid, predictions, average = \"macro\"),\n                     \"auc-score\": roc_auc_score(yvalid, predictions_probas_list[:,1], average = \"macro\")}, ignore_index=True)\n\nprint()\nimport gc\ngc.collect();","7a11eff1":"del X_resampled;\ndel y_resampled;","d447f832":"from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, RepeatedStratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nimport time\nimport seaborn as sns\nimport scikitplot as skplt\n\n# create a 80\/20 split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=42, test_size=0.2, stratify = y)\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nstart_time = time.time()\n\npredictions_probas_list = np.zeros([len(yvalid), 2])\npredictions_test_random_forest = np.zeros(len(test_df))\nnum_of_folds = 2\nnum_fold = 0\n    #feature_importance_df = pd.DataFrame()\n\nfolds = StratifiedKFold(n_splits=num_of_folds, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Stratified Fold:\", num_fold)\n    num_fold = num_fold + 1\n    print()\n\n    clf_stra_random_forest = RandomForestClassifier(n_estimators = 150,\n                                     n_jobs=-1,\n                                     random_state=42)\n\n    clf_stra_random_forest.fit(xtrain_stra, ytrain_stra)\n\n    #fold_importance_df = pd.DataFrame()\n    #fold_importance_df[\"feature\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].index\n    #fold_importance_df[\"fscore\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].values\n    #fold_importance_df[\"fold\"] = n_fold + 1\n    #feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    predictions = clf_stra_random_forest.predict(xvalid)\n    predictions_probas = clf_stra_random_forest.predict_proba(xvalid)\n    predictions_probas_list += predictions_probas\/num_of_folds\n\n    predictions_test_random_forest += clf_stra_random_forest.predict_proba(test_df[xtrain.columns])[:,1]\/num_of_folds\n\n\npredictions = np.argmax(predictions_probas, axis=1)\n\nprint()\nprint(classification_report(yvalid, predictions))\n\nprint()\nprint(\"CV f1_score\", f1_score(yvalid, predictions, average = \"macro\"))\n\nprint()\nprint(\"CV roc_auc_score\", roc_auc_score(yvalid, predictions_probas_list[:,1], average = \"macro\"))\n\nprint()\nprint(\"elapsed time in seconds: \", time.time() - start_time)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_confusion_matrix(yvalid, predictions, normalize=True)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_roc(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_ks_statistic(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_precision_recall(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_cumulative_gain(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_lift_curve(yvalid, predictions_probas)\n\n# sns.set(rc={'figure.figsize':(12, 38)})\n# xgb.plot_importance(clf_stra_xgb, title='Feature importance', xlabel='F score', ylabel='Features')\n\nclf_stats_df = clf_stats_df.append({\"clf_name\": \"clf_stra_random_forest\",\n                     \"F1-score\":f1_score(yvalid, predictions, average = \"macro\"),\n                     \"auc-score\": roc_auc_score(yvalid, predictions_probas_list[:,1], average = \"macro\")}, ignore_index=True)\n\nprint()\nimport gc\ngc.collect();","81b5278a":"from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n\nxtrain, xvalid, ytrain, yvalid = train_test_split(X, y, stratify = y, random_state=42, test_size=0.2)\n\n# https:\/\/github.com\/hyperopt\/hyperopt\/wiki\/FMin\n\ndef objective(space):\n\n    clf = RandomForestClassifier(n_estimators = space['n_estimators'],\n                                 criterion = space['criterion'],\n                                 max_features = space['max_features'],\n                                 max_depth = space['max_depth'],\n                                 n_jobs=-1,\n                                 random_state=42)\n\n\n    clf.fit(xtrain, ytrain)\n\n    pred = clf.predict_proba(xvalid)[:,1]\n    auc = roc_auc_score(yvalid, pred)\n    print(\"SCORE:\", auc)\n\n    return{'loss':1-auc, 'status': STATUS_OK }\n\n# https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html#parameters-for-tree-booster\nspace = {\n    'max_depth': hp.choice('max_depth', range(1,20)),\n    'max_features': hp.choice('max_features', range(1,150)),\n    'n_estimators': hp.choice('n_estimators', range(100,500)),\n    'criterion': hp.choice('criterion', [\"gini\", \"entropy\"])\n}\n\n\ntrials = Trials()\nbest = fmin(fn=objective,\n            space=space,\n            algo=tpe.suggest,\n            max_evals=2,\n            trials=trials,\n            verbose = 0)\n\nprint(best)","f61c69b3":"best","cde02931":"from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, RepeatedStratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nimport time\nimport seaborn as sns\nimport scikitplot as skplt\n\n# create a 80\/20 split of the data \nxtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=42, test_size=0.2, stratify = y)\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nstart_time = time.time()\n\npredictions_probas_list = np.zeros([len(yvalid), 2])\npredictions_test_tuned_random_forest = np.zeros(len(test_df))\nnum_of_folds = 2\nnum_fold = 0\n    #feature_importance_df = pd.DataFrame()\n\nfolds = StratifiedKFold(n_splits=num_of_folds, shuffle=False, random_state = 42)\n\nfor train_index, valid_index in folds.split(xtrain, ytrain):\n    xtrain_stra, xvalid_stra = xtrain.iloc[train_index,:], xtrain.iloc[valid_index,:]\n    ytrain_stra, yvalid_stra = ytrain.iloc[train_index], ytrain.iloc[valid_index]\n\n    print()\n    print(\"Stratified Fold:\", num_fold)\n    num_fold = num_fold + 1\n    print()\n\n    clf_stra_random_forest_tuned = RandomForestClassifier(n_estimators = best[\"n_estimators\"],\n                                                          max_features = best[\"max_features\"],\n                                                          max_depth = best[\"max_depth\"],\n                                                          criterion = [\"gini\", \"entropy\"][best[\"criterion\"]],\n                                                          n_jobs=-1,\n                                                          random_state=42)\n\n    clf_stra_random_forest_tuned.fit(xtrain_stra, ytrain_stra)\n\n    #fold_importance_df = pd.DataFrame()\n    #fold_importance_df[\"feature\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].index\n    #fold_importance_df[\"fscore\"] = pd.DataFrame.from_dict(data=clf_stra_xgb.get_fscore(), orient=\"index\", columns=[\"FScore\"])[\"FScore\"].values\n    #fold_importance_df[\"fold\"] = n_fold + 1\n    #feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    predictions = clf_stra_random_forest_tuned.predict(xvalid)\n    predictions_probas = clf_stra_random_forest_tuned.predict_proba(xvalid)\n    predictions_probas_list += predictions_probas\/num_of_folds\n\n    predictions_test_tuned_random_forest += clf_stra_random_forest_tuned.predict_proba(test_df[xtrain.columns])[:,1]\/num_of_folds\n\n\npredictions = np.argmax(predictions_probas, axis=1)\n\nprint()\nprint(classification_report(yvalid, predictions))\n\nprint()\nprint(\"CV f1_score\", f1_score(yvalid, predictions, average = \"macro\"))\n\nprint()\nprint(\"CV roc_auc_score\", roc_auc_score(yvalid, predictions_probas_list[:,1], average = \"macro\"))\n\nprint()\nprint(\"elapsed time in seconds: \", time.time() - start_time)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_confusion_matrix(yvalid, predictions, normalize=True)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_roc(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_ks_statistic(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_precision_recall(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_cumulative_gain(yvalid, predictions_probas)\n\nsns.set(rc={'figure.figsize':(8,8)})\nskplt.metrics.plot_lift_curve(yvalid, predictions_probas)\n\n# sns.set(rc={'figure.figsize':(12, 38)})\n# xgb.plot_importance(clf_stra_xgb, title='Feature importance', xlabel='F score', ylabel='Features')\n\nclf_stats_df = clf_stats_df.append({\"clf_name\": \"clf_stra_tuned_random_forest\",\n                     \"F1-score\":f1_score(yvalid, predictions, average = \"macro\"),\n                     \"auc-score\": roc_auc_score(yvalid, predictions_probas_list[:,1], average = \"macro\")}, ignore_index=True)\n\nprint()\nimport gc\ngc.collect();","8cfe8934":"clf_stats_df","098bf69b":"sub_df['target'] = predictions_test_xgb\nsub_df.to_csv('clf_xgboost.csv', index=False)\n\nsub_df['target'] = predictions_test_fs_xgb\nsub_df.to_csv('clf_xgboost_fs.csv', index=False)\n\nsub_df['target'] = predictions_test_fs_tuned_xgb\nsub_df.to_csv('clf_xgboost_fs_tuned.csv', index=False)\n\nsub_df['target'] = predictions_test_fs_smote_xgb\nsub_df.to_csv('clf_xgboost_fs_smote.csv', index=False)\n\nsub_df['target'] = predictions_test_fs_smote_tuned_xgb\nsub_df.to_csv('clf_xgboost_fs_smote_tuned.csv', index=False)\n\nsub_df['target'] = predictions_test_random_forest\nsub_df.to_csv('clf_random_forest.csv', index=False)\n\nsub_df['target'] = predictions_test_tuned_random_forest\nsub_df.to_csv('clf_random_forest_tuned.csv', index=False)","a8bac926":"# Random Forest training with default parameters","7b0bc718":"# XGBoost Training with feature selection and with SMOTE and after Tuning","97857cf5":"In this competition, you will be predicting the probability [0, 1] of a binary target column.\n\nThe data contains binary features (bin_*), nominal features (nom_*), ordinal features (ord_*) as well as (potentially cyclical) day (of the week) and month features. The string ordinal features ord_{3-5} are lexically ordered according to string.ascii_letters.","9a3700dd":"# The target feature","3a5465bc":"# # Bivariate - Exploring the 'bin' features in respect of the target variable","d283116e":"# XGBoost Training after Feature Selection with default parameters","9bff7929":"![](https:\/\/i5.walmartimages.com\/asr\/9167cad6-5213-4052-b09a-e81d9fdc313b_1.50035d03658475373548d614f6825c4a.jpeg?odnHeight=450&odnWidth=450&odnBg=FFFFFF)\n[image-source](https:\/\/i5.walmartimages.com\/asr\/9167cad6-5213-4052-b09a-e81d9fdc313b_1.50035d03658475373548d614f6825c4a.jpeg?odnHeight=450&odnWidth=450&odnBg=FFFFFF)","cbc8e43c":"# Check for NA missing values","44844aa1":"# Univariate - Exploring the nominal features","550103a5":"# Inspecting all classifiers and their performance","6f2253e9":"![](https:\/\/st3.depositphotos.com\/1051435\/12643\/i\/950\/depositphotos_126432526-stock-photo-a-group-of-cats-sitting.jpg)\n[image-source](https:\/\/st3.depositphotos.com\/1051435\/12643\/i\/950\/depositphotos_126432526-stock-photo-a-group-of-cats-sitting.jpg)","0704edd5":"# HyperOpt Tuning for XGBoost after Feature Selection","4777c8d8":"# XGBoost Training after feature Selection and Tuning","64c0567f":"# Machine Learning Training","dd69569f":"# Dealing the class Imbalance with SMOTE\n- Dealing class imbalance with SMOTE and training xgboost with default parameters","0e17ae6f":"# Feature Engineering for Nominal Features","ef9a9c02":"# XGBoost with default parameters","c031e096":"# Univariate - Exploring the bin features","77d66937":"# HyperOpt Tuning for SMOTE XGBoost","35a851c8":"# Loading main libraries","0bc8961d":"# Preparing for submission","32e9f42f":"# Bivariate - Exploring the Ordinal Features in respect of the target","f87f16e5":"# Class Imbalance with SMOTE and feature Selection and XGBoost training with default parameters","955b22f6":"# CATegorical Feature Encoding Challenge EDA + ML","e3ec26ee":"No missing values which is great!","c84e13c1":"# Feature Engineering for Binary features","70204ac5":"# Random Forest after Tuning","d710bf76":"# Univariate - Exploring the Ordinal Features","7a0cc128":"# Bivariate - Exploring the Nominal Features in respect of target","234df675":"# EDA\nExploratory Data Analysis","78b64ebe":"# Feature Engineering for Ordinal Features\nI will use the one hot encoding method","ff6214e5":"# XGBoost Feature Selection","6644b571":"# Random Forest Tuning","38431f1a":"> Thank you very much for checking out my kernel! Please upvote if you found it helpful or leave a comment if you have any suggestions for improvements.","fcf25a82":"![](https:\/\/www.zirous.com\/wp-content\/uploads\/2019\/01\/Word-Cloud-02.png)\n[image-source](https:\/\/www.zirous.com\/wp-content\/uploads\/2019\/01\/Word-Cloud-02.png)","a8add0cb":"We can see that high cardinality nominal deatures cannot be depicted with seaborn due to the fact that there are many categories."}}