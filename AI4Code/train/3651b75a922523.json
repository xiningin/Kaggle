{"cell_type":{"4cd58ca6":"code","0355ec27":"code","bae39385":"code","a7566257":"code","0fdb2d85":"code","80bc8f0e":"code","2a3c8249":"code","fbb32950":"code","cebdb303":"code","57c5d8ed":"code","264f687a":"code","823fe0fc":"code","8e22219c":"code","f38ac000":"code","2dc24b61":"code","1720e4ce":"code","3dc375af":"code","f17c2e96":"code","dfe1de16":"code","2cd5c264":"code","c74114c7":"code","512ed165":"code","40583656":"markdown","d6ff2173":"markdown","1f85bd1a":"markdown","e974e002":"markdown","b0230b45":"markdown","a8ea4e98":"markdown","0c209be3":"markdown","31bdc33c":"markdown","4581e044":"markdown"},"source":{"4cd58ca6":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.preprocessing import OneHotEncoder","0355ec27":"data = pd.read_csv('\/kaggle\/input\/infopulsehackathon\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/infopulsehackathon\/test.csv')","bae39385":"ohe = OneHotEncoder(sparse=False)\nohe.fit(data.select_dtypes(object))\ndata = pd.concat((data,pd.DataFrame(ohe.transform(data.select_dtypes(object)))),axis=1)\ntest = pd.concat((test,pd.DataFrame(ohe.transform(test.select_dtypes(object)))),axis=1)\n\ndata.drop(data.select_dtypes(object).columns,axis=1,inplace=True)\ntest.drop(test.select_dtypes(object).columns,axis=1,inplace=True)","a7566257":"target = data['Energy_consumption']\ndata = data.drop(['Energy_consumption'],axis=1)","0fdb2d85":"cols = data.columns","80bc8f0e":"from sklearn.impute import SimpleImputer\nimp = SimpleImputer(missing_values=-2,strategy=\"median\")\ndata_imp = imp.fit_transform(data)\ntest_imp = imp.fit_transform(test)","2a3c8249":"def trip_featurize(df,feat1,feat2, feat3):\n    df[f'{feat1}-{feat2}-{feat3}'] = df[feat1] - df[feat2] - df[feat3]\n    df[f'{feat1}-{feat2}+{feat3}'] = df[feat1] - df[feat2] + df[feat3]\n    df[f'{feat1}-{feat2}*{feat3}'] = df[feat1] - df[feat2] * df[feat3]\n    df[f'{feat1}-{feat2}\/{feat3}'] = df[feat1] - df[feat2] \/ df[feat3]\n    \n    df[f'{feat1}+{feat2}-{feat3}'] = df[feat1] + df[feat2] - df[feat3]\n    df[f'{feat1}+{feat2}+{feat3}'] = df[feat1] + df[feat2] + df[feat3]\n    df[f'{feat1}+{feat2}*{feat3}'] = df[feat1] + df[feat2] * df[feat3]\n    df[f'{feat1}+{feat2}\/{feat3}'] = df[feat1] + df[feat2] \/ df[feat3]\n    \n    df[f'{feat1}*{feat2}-{feat3}'] = df[feat1] * df[feat2] - df[feat3]\n    df[f'{feat1}*{feat2}+{feat3}'] = df[feat1] * df[feat2] + df[feat3]\n    df[f'{feat1}*{feat2}*{feat3}'] = df[feat1] * df[feat2] * df[feat3]\n    df[f'{feat1}*{feat2}\/{feat3}'] = df[feat1] * df[feat2] \/ df[feat3]\n    \n    df[f'{feat1}\/{feat2}-{feat3}'] = df[feat1] \/ df[feat2] - df[feat3]\n    df[f'{feat1}\/{feat2}+{feat3}'] = df[feat1] \/ df[feat2] + df[feat3]\n    df[f'{feat1}\/{feat2}*{feat3}'] = df[feat1] \/ df[feat2] * df[feat3]\n    df[f'{feat1}\/{feat2}\/{feat3}'] = df[feat1] \/ df[feat2] \/ df[feat3]\ndef double_featurize(df,feat1,feat2):\n    df[f'{feat1}-{feat2}'] = df[feat1] - df[feat2]\n    df[f'{feat1}+{feat2}'] = df[feat1] + df[feat2]\n    df[f'{feat1}*{feat2}'] = df[feat1] * df[feat2]\n    df[f'{feat1}\/{feat2}'] = df[feat1] \/ df[feat2]","fbb32950":"double_featurize(data,'feature_5','feature_122')\ndouble_featurize(test,'feature_5','feature_122')\n\ndouble_featurize(data,'feature_122','feature_33')\ndouble_featurize(test,'feature_122','feature_33')\n\ndouble_featurize(data,'feature_129','feature_5')\ndouble_featurize(test,'feature_129','feature_5')\n\ndouble_featurize(data,'feature_122','feature_248')\ndouble_featurize(test,'feature_122','feature_248')\n\ntrip_featurize(data,'feature_5','feature_122', 'feature_33')\ntrip_featurize(test,'feature_5','feature_122', 'feature_33')\n\ntrip_featurize(data,'feature_122','feature_264', 'feature_33')\ntrip_featurize(test,'feature_122','feature_264', 'feature_33')\n\ntrip_featurize(data,'feature_122','feature_248', 'feature_5')\ntrip_featurize(test,'feature_122','feature_248', 'feature_5')\n\ntrip_featurize(data,'feature_229','feature_250', 'feature_5')\ntrip_featurize(test,'feature_229','feature_250', 'feature_5')","cebdb303":"data_val = data.values\ntest_val = test.values","57c5d8ed":"counts = []\nfor column in cols:\n    counts.append(data[column].value_counts())","264f687a":"bins = np.zeros(data.shape)\nfor i in range(data.shape[0]):\n    for j in range(len(cols)):\n        bins[i][j] = counts[j][data_val[i][j]]","823fe0fc":"test_bins = np.zeros(test.shape)\nfor i in range(test.shape[0]):\n    for j in range(len(cols)):\n        try:\n            test_bins[i][j] = counts[j][test_val[i][j]]\n        except:\n            test_bins[i][j] = 1","8e22219c":"data_val = np.concatenate((data_imp,bins),axis=1)\ntest = np.concatenate((test_imp,test_bins),axis=1)","f38ac000":"from sklearn.model_selection import KFold\nimport lightgbm as lgb","2dc24b61":"folds = KFold(shuffle=True,random_state=40,n_splits=5)","1720e4ce":"params = {\n 'min_data_in_leaf': 47,\n 'num_leaves': 61,\n 'lr': 0.06172399472541107,\n 'min_child_weight': 0.0070492703809497,\n 'colsample_bytree': 0.06169,\n 'bagging_fraction': 0.3809,\n 'min_child_samples': 35,\n 'subsample': 0.6077180918189186,\n 'max_depth': 4,\n 'objective': 'regression',\n 'seed': 1337,\n 'feature_fraction_seed': 1337,\n 'bagging_seed': 1337,\n 'drop_seed': 1337,\n 'data_random_seed': 1337,\n 'boosting_type': 'gbdt',\n 'verbose': 1,\n 'boost_from_average': True,\n 'metric': 'mse',\n 'cat_l2': 24.38,\n 'cat_smooth': 18.49,\n 'feature_fraction': 0.1045,\n 'lambda_l1': 2.306,\n 'lambda_l2':18.02,\n 'max_cat_threshold':50,\n 'min_gain_to_split':  0.2585,\n 'min_sum_hessian_in_leaf':0.001923,         \n}","3dc375af":"target = target.values","f17c2e96":"from sklearn.metrics import mean_squared_error","dfe1de16":"pred = np.zeros((1,test.shape[0]))\nscores = 0\nfeatures = range(data_val.shape[1])\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(data_val)):\n    x_trn, y_trn = data_val[trn_idx], target[trn_idx]\n    x_val, y_val = data_val[val_idx], target[val_idx]\n    \n    train = lgb.Dataset(x_trn,label=y_trn)\n    valid = lgb.Dataset(x_val,label=y_val)\n    \n    model = lgb.train(params,train,num_boost_round=10000,early_stopping_rounds=100,valid_sets=(train,valid),verbose_eval=False)\n    \n    valid_prediction = model.predict(x_val,num_iteration=model.best_iteration)\n    test_prediction = model.predict(test,num_iteration=model.best_iteration)\n    \n    score = mean_squared_error(y_val, valid_prediction)\n    scores += score\n    pred += test_prediction\n    \n    print('Validation fold {} : '.format(fold),score)\n    \nprint(\"SCORE\",scores\/5)","2cd5c264":"pred \/= 5","c74114c7":"sub = pd.read_csv('\/kaggle\/input\/infopulsehackathon\/sample_submission.csv')","512ed165":"sub['Energy_consumption'] = pred[0]\nsub.to_csv('submission.csv',index=False)","40583656":"Impute -2 values with median, as the column they are in are mostly categorical and imputing with mean would not suffice.","d6ff2173":"### Submission","1f85bd1a":"These parameters are a result of Bayesian Optimization.","e974e002":"### Data preprocessing","b0230b45":"Some of the things tried which failed:\n- PCA transformation as features\n- Overfit a DNN on a projection onto 10 PCA components\n- Remove outliers with z-score\n- Isolation Forest to create outlier score as a feature\n- Remove columns which comprise mostly of one value\n- Remove 'Id', which is almost the most important feature, lol","a8ea4e98":"# Energy consumption\n\nCongrats to everyone who participated. This is my way of takling this competition. It was quite easy to reach the 613K mark on LB, as pure parameter optimization with no feature engineering, except OHE, was enough to get there.\nTL;DR \n- Impute nans(-2)\n- Create feature interactions to boost expected gain with xgbfir\n- Add features which indicate the frequency of each value of each column\n- Bayesian optimization\n- validate on kfold","0c209be3":"And in conclusion: you shouldn't have trusted your validation on this competition :)\nAlthough I do think, that the way the data was split into private and public leaderboards is somewhat wrong. If you remove only 400 rows with extreme values through z-score, your local validation will go up. Sure thing, because it is way easier for the model and you are plainly overfitting. But the results from this removal on LB were way too big. Just because of those outliers you'd gain around 200-300K MSE on LB.\nAnd a constant theme on this competition was public LB score being much better, than local validation score. You can see above that some folds score have a drastic MSE, when others have a very low score. I'd like to hear more from the organizers on how exactly they split their data, that public and private LBs score much better, than local validation. Usually it's the other way around.","31bdc33c":"Use xgbfir to get the features which interract the most to get most Expected gain. Just have to cycle through all the possible operators","4581e044":"### Model and cross-validation"}}