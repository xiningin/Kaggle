{"cell_type":{"11184e18":"code","48334471":"code","4f8873da":"code","fb2c6bf6":"code","a518dced":"code","6e6a2d26":"code","bc97a315":"code","d13c1f18":"code","4df168e8":"code","2bce2538":"code","8489749c":"code","6988989f":"code","ff968cbf":"code","747c8bf1":"code","e4a4e92f":"code","b3df744e":"code","4efc0e02":"code","0dd565d4":"code","cae625d4":"code","ccd372e8":"code","be8524de":"code","9c52328c":"code","5e98f95b":"code","c98104a4":"code","bb97dffc":"code","7d134f45":"code","93eb1c29":"code","98e68407":"code","cd705c1e":"code","e10843e7":"code","506266a0":"code","4202a54a":"code","7c9b1505":"code","4c411726":"code","70cdd034":"code","546e3cec":"markdown","d133973d":"markdown","4505c0a4":"markdown","b0796317":"markdown","4a04b1dc":"markdown","0673183c":"markdown","db3e00f4":"markdown","ad836f56":"markdown","d9c15675":"markdown","0a1b3f1b":"markdown","a737409a":"markdown","b7608570":"markdown","862721d1":"markdown","485ac7c2":"markdown","85141256":"markdown","f1bd2256":"markdown","08d3188a":"markdown","43545b46":"markdown","afe618ef":"markdown","c6b23758":"markdown","ce7134b2":"markdown","f8d36cbb":"markdown","43088aca":"markdown","abd7b556":"markdown","f28c65bc":"markdown","5c4552fe":"markdown"},"source":{"11184e18":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.cluster import KMeans","48334471":"seller = pd.read_csv('\/kaggle\/input\/facebook-live-sellers-in-thailand-uci-ml-repo\/Live.csv')","4f8873da":"# first five row\nseller.head()","fb2c6bf6":"# size of datset\nseller.shape","a518dced":"# statistical summary of numerical variables\nseller.describe()","6e6a2d26":"# summary about dataset\nseller.info()","bc97a315":"# check for missing values\nseller.isna().sum() ","d13c1f18":"seller = seller.dropna(axis=1)","4df168e8":"# we have drop 4 columns\nseller.isna().sum() ","2bce2538":"# check the unique values\n\nprint(seller['status_id'].unique())","8489749c":"# check the number of unique values\n\nprint(seller['status_id'].nunique())","6988989f":"# check the unique values\n\nseller['status_published'].unique()","ff968cbf":"# check the number of unique values\n\nprint(seller['status_published'].nunique())","747c8bf1":"# check the unique values\n\nseller['status_type'].unique()","e4a4e92f":"# check the number of unique values\nseller['status_type'].nunique","b3df744e":"seller.drop(['status_id', 'status_published'], axis=1, inplace=True)","4efc0e02":"# check the summary\nseller.info()","0dd565d4":"sns.pairplot(seller)","cae625d4":"X = seller\n\ny = seller['status_type']","ccd372e8":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\nX['status_type'] = le.fit_transform(X['status_type'])\n\ny = le.transform(y)","be8524de":"# check x\nX.head()","9c52328c":"from sklearn.preprocessing import MinMaxScaler\n\nmin_scal = MinMaxScaler()\n\nX = min_scal.fit_transform(X)","5e98f95b":"X = pd.DataFrame(X, columns=[seller.columns])","c98104a4":"X.head()","bb97dffc":"k_means = KMeans(n_clusters=2, random_state=42) \n\nk_means.fit(X)","7d134f45":"# model parameter study\nk_means.cluster_centers_","93eb1c29":"# calculate model inertia\n\nk_means.inertia_","98e68407":"from sklearn.metrics import silhouette_score\n\n# Silhouette analysis\nrange_n_clusters = [2, 3, 4, 5, 6]\n\nfor num_clusters in range_n_clusters:\n    \n    # intialise kmeans\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=50)\n    kmeans.fit(X)\n    \n    cluster_labels = kmeans.labels_\n    \n    # silhouette score\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    print(\"For n_clusters={0}, the silhouette score is {1}\".format(num_clusters, silhouette_avg))","cd705c1e":"labels = k_means.labels_\n\n# check how many of the samples were correctly labeled\ncorrect_labels = sum(y == labels)\n\nprint(\"Result: {} out of {} samples were correctly labeled.\".format(correct_labels, y.size))","e10843e7":"print('Accuracy score: {0:0.2f}'. format(correct_labels\/float(y.size)))","506266a0":"cs = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 7)\n    \n    kmeans.fit(X)\n    \n    cs.append(kmeans.inertia_)\n\n# plot the \nplt.plot(range(1, 11), cs)\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\nplt.show()","4202a54a":"# K-Means model with 3 clusters\n\nk_means3 = KMeans(n_clusters=3,max_iter = 400, n_init = 10, random_state=7)\n\nk_means3.fit(X)\n\n# check how many of the samples were correctly labeled\nlabels = k_means3.labels_\n\ncorrect_labels3 = sum(y == labels)\nprint(\"Result: %d out of %d samples were correctly labeled.\" % (correct_labels3, y.size))\nprint('Accuracy score: {0:0.2f}'. format(correct_labels3\/float(y.size)))","7c9b1505":"# Inertia\nprint(\"Inertia: \",k_means3.inertia_)\n\n# silhouette score\nsilhouette_avg3 = silhouette_score(X, (k_means3.labels_))\nprint(\"The silhouette score is \", silhouette_avg3)","4c411726":"# K-Means model with 4 clusters\n\nk_means4 = KMeans(n_clusters=4, max_iter = 400, n_init = 10, random_state=7)\n\nk_means4.fit(X)\n\n# check how many of the samples were correctly labeled\nlabels = k_means4.labels_\n\ncorrect_labels4 = sum(y == labels)\nprint(\"Result: %d out of %d samples were correctly labeled.\" % (correct_labels4, y.size))\nprint('Accuracy score: {0:0.2f}'. format(correct_labels4\/float(y.size)))","70cdd034":"# Inertia\nprint(\"Inertia: \",k_means4.inertia_)\n\n# silhouette score\nsilhouette_avg4 = silhouette_score(X, (k_means4.labels_))\nprint(\"The silhouette score is \", silhouette_avg4)","546e3cec":"# Declare feature vector and target variable","d133973d":"### Drop missing columns","4505c0a4":"- By the above plot, we can see that there is a kink at k=2.\n\n- Hence k=2 can be considered a good number of the cluster to cluster this data.\n\n- But, we have seen that I have achieved a weak classification accuracy of 1% with k=2.\n\n- we will try with k=4,5","b0796317":"We can see that, there are 3 categorical columns and remaining 9 numerical columns.","4a04b1dc":"**We have achieved a relatively high accuracy of 62% with k=4.**","0673183c":"### 1. Inertia\n* Inertia is not a normalized metric.\n* The lower values of inertia are better and zero is optimal.\n* But in very high-dimensional spaces, euclidean distances tend to become inflated (this is an instance of curse of dimensionality). \n* Running a dimensionality reduction algorithm such as PCA prior to k-means clustering can alleviate this problem and speed up the computations.","db3e00f4":"# Feature Scaling","ad836f56":"# Load dataset","d9c15675":"# Import libraries","0a1b3f1b":"* The KMeans algorithm clusters data by trying to separate samples in n groups of equal variances, minimizing a criterion known as inertia, or within-cluster sum-of-squares Inertia, or the within-cluster sum of squares criterion, can be recognized as a measure of how internally coherent clusters are.\n* The k-means algorithm divides a set of N samples X into K disjoint clusters C, each described by the mean j of the samples in the cluster. The means are commonly called the cluster centroids.\n* The K-means algorithm aims to choose centroids that minimize the inertia, or within-cluster sum of squared criterion.","a737409a":"### 2.Silhouette Analysis\nThe silhouette is a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually.\n![1.PNG](attachment:1.PNG)\n \n* p  is the mean distance to the points in the nearest cluster that the data point is not a part of\n\n* q  is the mean intra-cluster distance to all the points in its own cluster.\n\n* The value of the silhouette score range lies between -1 to 1.\n\n    * A score closer to 1 indicates that the data point is very similar to other data points in the cluster,\n\n    * A score closer to -1 indicates that the data point is not similar to the data points in its cluster.","b7608570":"We can see that there are 6997 unique labels in the status_id variable. The total number of instances in the dataset is 7050. So, it is approximately a unique identifier for each of the instances. Thus this is not a variable that we can use. Hence, I will drop it.","862721d1":"# What is K-Means Clustering?\n\nK-Means clustering is the most popular unsupervised machine learning algorithm. This is an algorithm that tries to minimize the distance of the points in a cluster with their centroid\n\n## Steps in K-Means algorithm:\n1. Choose the number of clusters K.\n2. Select at random K points, the centroids(not necessarily from your dataset).\n3. Assign each data point to the closest centroid \u2192 that forms K clusters.\n4. Compute the mean of data points within a cluster and place the new centroid of each cluster.\n5. Reassign each data point to the new closest centroid. If any reassignment\u00a0. took place, go to step 4, otherwise, the model is ready.\n\n## How to choose the value of K?\nThere are different techniques available to find the optimal value of K. The most common technique is the **elbow method.**\nThe elbow method is used to determine the optimal number of clusters in K-means clustering. The elbow method plots the value of the Within cluster sum of square of the data points(WCSS) produced by different values of K. The below diagram shows how the elbow method works:-\n\n![995b8b58-06f1-4884-a2a1-f3648428e947.png](attachment:995b8b58-06f1-4884-a2a1-f3648428e947.png)\n\nWe can see that if K increases, average distortion will decrease. We should choose that k where the distortion decline drastically. From the above diagram the best value of k will be 3.","485ac7c2":"### Observation\n* The lesser the model inertia, the better the model fit.\n* We can see that the model has very high inertia. So, this is not a good model fit to the data.","85141256":"# Encoding ","f1bd2256":"#  K-Means model with different clusters","08d3188a":"# Use elbow method to find optimal number of clusters","43545b46":"We have achieved a weak classification accuracy of 1% by our unsupervised model.","afe618ef":"# Exploratory data analysis","c6b23758":"We can see that there are 4 categories of labels in the status_type variable.\n\n","ce7134b2":"### Explore categorical variables","f8d36cbb":"Again, we can see that there are 6913 unique labels in the status_published variable. The total number of instances in the dataset is 7050. So, it is also a approximately a unique identifier for each of the instances. Thus this is not a variable that we can use. Hence, I will drop it also.","43088aca":"### Drop `status_id` and `status_published` variable from the dataset","abd7b556":"# Evaluation\n\nThere is no perfect way to evaluate clustering if you don't have the labels, however since this is just an exercise, we do have the labels, so we take advantage of this to evaluate our clusters, keep in mind, you usually won't have this luxury in the real world.\n\nAnd we can check model performances with interia and silhouette","f28c65bc":"# K-Means model with two clusters","5c4552fe":"### 3. Lables"}}