{"cell_type":{"6a97c47e":"code","77631e7a":"code","29a3aeb6":"code","41572463":"code","3c516c1d":"code","857aa7ba":"code","f8ad17a2":"code","c0fbfab0":"code","45710f8a":"code","8c122743":"code","45c6bfa3":"code","476734c0":"code","2ee61ffb":"code","40e9afe0":"code","4f4dc60f":"code","793aea3d":"code","67d92e16":"code","1e5e8db4":"code","799928f1":"code","2e73db3b":"code","697bfc52":"code","f5a70e79":"code","0a4a5080":"code","9a1afdf0":"code","27daef7e":"code","ee8b5927":"code","ca45ad19":"code","c8867aec":"code","24bda8ef":"code","073ce27f":"code","6fff1188":"code","0d698de6":"code","2901cf58":"code","cddd977a":"code","c0c0953b":"code","63c46a54":"code","73559603":"code","2b50d2af":"code","f4020eb3":"code","f83a5b70":"code","a067a515":"markdown","da2765e4":"markdown","95adb7c3":"markdown","a702a346":"markdown","28cd16c0":"markdown","4869b7c8":"markdown","25550c9e":"markdown","744b8c88":"markdown","b7ec18ef":"markdown","07c0c09f":"markdown","2018167a":"markdown","275d906d":"markdown","910f93be":"markdown","c05b6459":"markdown","620b10f2":"markdown","07b9926c":"markdown","2a4aa7bf":"markdown","8dd653b6":"markdown","7678afe2":"markdown","a78fd413":"markdown","59910b7a":"markdown","6b0c586c":"markdown","0d0ef54b":"markdown","6d69d253":"markdown","2f3dd32b":"markdown","c558102e":"markdown","fae029fd":"markdown","2ae29072":"markdown","66a9239e":"markdown","8441919c":"markdown","23ce0dc4":"markdown","ea766b56":"markdown","2eebc1c2":"markdown"},"source":{"6a97c47e":"#Imports\nimport sys, warnings\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom imblearn.over_sampling import SMOTE\n\n\nfrom sklearn import decomposition, tree, preprocessing\nfrom sklearn.preprocessing import scale, RobustScaler\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV,RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report,accuracy_score,recall_score,precision_score,f1_score,confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectKBest, f_classif\nimport xgboost as xgb\nfrom xgboost import XGBClassifier","77631e7a":"#Loading the data\ndf_unclean = pd.read_csv(\"\/kaggle\/input\/company-bankruptcy-prediction\/data.csv\")","29a3aeb6":"df_unclean.head()","41572463":"# get null values\nnull_values = df_unclean.isnull().sum().sum()\nprint('Null:', null_values)\n# get nan values\nnan_values = np.isnan(df_unclean.values).sum()\nprint('Nan:', nan_values)","3c516c1d":"df_unclean.info()","857aa7ba":"df_unclean.describe()","f8ad17a2":"Y = df_unclean['Bankrupt?']\nX = df_unclean.drop('Bankrupt?', axis=1)\n\n#X_cv , Y_cv will be used for model and parameter selection with cross validation\n#X_cv_unclean,X_test,Y_cv_unclean,Y_test  = train_test_split(X_scaled, Y, test_size=0.1, stratify = Y, random_state = 42)\nX_train_unscaled,X_test_unscaled,Y_train,Y_test  = train_test_split(X, Y, test_size=0.1, stratify = Y, random_state = 42)","c0fbfab0":"from scipy.stats import skew","45710f8a":"#drop the features with zero standard deviation (only one value accross all features)\nX_train_unscaled.drop(X_train_unscaled.std()[X_train_unscaled.std() == 0].index.values, axis=1)\nX_test_unscaled.drop(X_train_unscaled.std()[X_train_unscaled.std() == 0].index.values, axis=1)\n                   \n#finding the skewed features to log transform them \nskewness = X_train_unscaled.apply(lambda x : skew(x))\nskewness = skewness[abs(skewness)>0.5]\nskewed_features = skewness.index\n\nprint(\"Number of Skewed Features: \\n\",len(skewed_features))\n\n#Log transform the skewed features\nX_train_unscaled[skewed_features] = np.log1p(X_train_unscaled[skewed_features])\nX_test_unscaled[skewed_features] = np.log1p(X_test_unscaled[skewed_features])\n\n#standardizing the data \nscaler = preprocessing.StandardScaler()\nscaler.fit(X_train_unscaled.loc[:, X_train_unscaled.columns])\n\nX_train = X_train_unscaled.copy()\nX_test = X_test_unscaled.copy()\n\nX_train.loc[:, X_train_unscaled.columns] = scaler.transform(X_train_unscaled.loc[:, X_train_unscaled.columns])\nX_test.loc[:, X_test_unscaled.columns] = scaler.transform(X_test_unscaled.loc[:, X_test_unscaled.columns])\n\n#new datafram for display purposes\ndf_train = pd.concat([X_train,Y_train],axis=1)","8c122743":"df_train.hist(figsize = (40,30),bins = 50)\nplt.show()","45c6bfa3":"corr_mat = df_train.corr()\n#lower trainglular part of the correlation matrix\nlt_mat = corr_mat.where(np.tril(np.ones(corr_mat.shape)).astype(np.bool))\n\nfig, ax = plt.subplots(figsize=(30,30))   \n\ncmap = sns.diverging_palette(220, 20, as_cmap=True)\nsns.heatmap(lt_mat, linewidths=0.5,cmap=cmap, ax=ax)\nplt.show()","476734c0":"corr_cols = corr_mat.abs().nlargest(21,'Bankrupt?')['Bankrupt?'].index\nprint(\"Features that are the most correlated (positively or negatively) to the label:\\n\",corr_cols)","2ee61ffb":"cm = df_train[corr_cols].corr()\n#lower trainglular part of the correlation matrix\nlt_cm = cm.where(np.tril(np.ones(cm.shape)).astype(np.bool))\n\nsns.set(font_scale=1.25)\nfig, ax = plt.subplots(figsize=(14,14))   \ncmap = sns.diverging_palette(220, 20, as_cmap=True)\n\nheatmap = sns.heatmap(lt_cm, cbar=True, annot=True, cmap=cmap, fmt='.2f',annot_kws={'size': 10})","40e9afe0":"#For display purposes, we get rid of the columns that showed the greatest similarity below.\nnew_cols = ['Bankrupt?', ' Net Income to Total Assets',\n       ' ROA(A) before interest and % after tax',\n       ' Net worth\/Assets', \n       ' Persistent EPS in the Last Four Seasons',\n       ' Current Liability to Assets',\n       ' Working Capital to Total Assets',\n       ' Current Liability to Current Assets', ' Net Value Per Share (A)',\n       \" Net Income to Stockholder's Equity\",\n       ' Liability to Equity', ' Equity to Long-term Liability']\nsns.set()\nsns.pairplot(df_train[new_cols],height=4)\nplt.show();","4f4dc60f":"df_train = df_train[df_train[' Equity to Long-term Liability'] < 10]\ndf_train = df_train[df_train[' Equity to Long-term Liability'] > -1]\ndf_train = df_train[df_train[' Net worth\/Assets'] > -15]\ndf_train = df_train[df_train[' Working Capital to Total Assets'] > -7.5]\ndf_train = df_train[df_train[\" Net Income to Stockholder's Equity\"] > -10]\ndf_train = df_train[df_train[\" Current Liability to Assets\"] < 6]\ndf_train = df_train[df_train[' Liability to Equity'] < 20]\ndf_train = df_train[df_train[' Liability to Equity'] > -5]\ndf_train = df_train[df_train[' Working Capital to Total Assets'] > -5]\n\nprint(\"Remaining dataset size: \",df_train.shape[0])\n","793aea3d":"#adds 5 new features based on the pair plots\ndef add_poly_features(old_df):\n\n    \n    new_df = old_df.copy()\n    \n    new_df[' Working Capital to Total Assets-s2'] = ( old_df[' Working Capital to Total Assets'] - old_df[' Working Capital to Total Assets'].min() ) ** 2\n    new_df[' Working Capital to Total Assets-s2'] = (new_df[' Working Capital to Total Assets-s2']-new_df[' Working Capital to Total Assets-s2'].mean() ) \/new_df[' Working Capital to Total Assets-s2'].std()\n    \n    new_df[' Liability to Equity-s2'] = np.exp(-( old_df[' Liability to Equity'] - old_df[' Liability to Equity'].min() ) )\n    new_df[' Liability to Equity-s2'] = (new_df[' Liability to Equity-s2']-new_df[' Liability to Equity-s2'].mean())\/new_df[' Liability to Equity-s2'].std()\n    \n    new_df[' Net Value Per Share (A)-s2'] = (old_df[' Net Value Per Share (A)']- old_df[' Net Value Per Share (A)'].min() ) ** 2\n    new_df[' Net Value Per Share (A)-s2'] = (new_df[' Net Value Per Share (A)-s2']-new_df[' Net Value Per Share (A)-s2'].mean())\/new_df[' Net Value Per Share (A)-s2'].std()\n    \n    new_df[' Net Income to Total Assets-s2'] = (old_df[' Net Income to Total Assets']- old_df[' Net Income to Total Assets'].min() ) ** 2\n    new_df[' Net Income to Total Assets-s2'] = (new_df[' Net Income to Total Assets-s2']-new_df[' Net Income to Total Assets-s2'].mean())\/new_df[' Net Income to Total Assets-s2'].std()\n    \n    new_df[' Persistent EPS in the Last Four Seasons-s2'] = (old_df[' Persistent EPS in the Last Four Seasons'] - old_df[' Persistent EPS in the Last Four Seasons'].min() ) ** 2\n    new_df[' Persistent EPS in the Last Four Seasons-s2'] = (new_df[' Persistent EPS in the Last Four Seasons-s2']-new_df[' Persistent EPS in the Last Four Seasons-s2'].mean())\/new_df[' Persistent EPS in the Last Four Seasons-s2'].std()\n    \n    return new_df\n    ","67d92e16":"#adding the new features\ndf_train = add_poly_features(df_train)\n\nX_test = add_poly_features(X_test)\n\nY_modified = df_train['Bankrupt?']\nX_modified = df_train.drop('Bankrupt?', axis=1)","1e5e8db4":"pca = decomposition.PCA(n_components=30)\npca.fit(X_modified)\nX_PCA = pca.transform(X_modified)","799928f1":"fig = plt.figure(1, figsize=(10, 10))\nax = Axes3D(fig, rect=[0, 0, 1, 1], elev=10, azim=70, autoscale_on=True)\n\nax.scatter(X_PCA[Y_modified == 1, 0], X_PCA[Y_modified == 1, 1], X_PCA[Y_modified == 1, 2],alpha= 0.5)\nax.scatter(X_PCA[Y_modified == 0, 0], X_PCA[Y_modified == 0, 1], X_PCA[Y_modified == 0, 2],alpha= 0.5)\n\nfor name, label in [('Bankrupt', 1), ('Not Bankrupt', 0)]:\n    ax.text3D(X_PCA[Y_modified == label, 0].mean()*2,\n              X_PCA[Y_modified == label, 1].mean()*2,\n              X_PCA[Y_modified == label, 2].mean()*2,\n               name,\n              horizontalalignment='center',\n              bbox=dict(alpha=1, edgecolor='w', facecolor='w'))\n\nax.set_xlim(-10,10)\nax.set_ylim(-10,10)\nax.set_zlim(-10,10)\n\nplt.show()","2e73db3b":"explained_variance = pca.explained_variance_ratio_\nprint('Explained variation per principal component: {}'.format(explained_variance))\n    \n\nexplained_variance = np.insert(explained_variance, 0, 0)\ncumulative_variance = np.cumsum(np.round(explained_variance, decimals=4)) ","697bfc52":"pc_df = pd.DataFrame(['PC1', 'PC2', 'PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10'], columns=['PC'])\nexplained_variance_df = pd.DataFrame(explained_variance, columns=['Explained Variance'])\ncumulative_variance_df = pd.DataFrame(cumulative_variance, columns=['Cumulative Variance'])\n\n\ndf_explained_variance = pd.concat([pc_df, explained_variance_df[1:11], cumulative_variance_df[1:11]], axis=1)\ndf_explained_variance","f5a70e79":"plt.plot(cumulative_variance)\nplt.title(\"Cumulative Variance\")\nplt.show()","0a4a5080":"Y_cv = df_train['Bankrupt?']\nX_cv = df_train.drop('Bankrupt?', axis=1)\n","9a1afdf0":"#obtaining the 30 best original features by ANOVA F measure for comparison\nbest_k = SelectKBest(score_func=f_classif , k=30)\nbest_k.fit(X_cv, Y_cv)\nX_k = best_k.transform(X_cv)\n\nmask = best_k.get_support()\nnew_features = X_cv.columns[mask]\nprint(\"10 best original features:\\n\",new_features[:10])","27daef7e":"#creating plots\nfig, ((ax1, ax2, ax5, ax6), (ax3, ax4, ax7, ax8)) = plt.subplots(2, 4,figsize=(25, 15))\n\n# ------------------------ Principal Components -------------------------------------\nax1.scatter(X_PCA[Y_cv == 0, 0],X_PCA[Y_cv == 0, 1], color='r', label=\"Not Bankrupt\",edgecolor='k')\nax1.scatter(X_PCA[Y_cv == 1,0],X_PCA[Y_cv == 1, 1], color='b', label=\"Bankrupt\",edgecolor='k')\nax1.set_title(\"1st and 2nd Principal Components\")\nax1.legend()\n\nax2.scatter(X_PCA[Y_cv == 0, 0],X_PCA[Y_cv == 0, 2], color='r', label=\"Not Bankrupt\",edgecolor='k')\nax2.scatter(X_PCA[Y_cv == 1,0],X_PCA[Y_cv == 1, 2], color='b', label=\"Bankrupt\",edgecolor='k')\nax2.set_title(\"1st and 3rd Principal Components\")\nax2.legend()\n\nax3.scatter(X_PCA[Y_cv == 0, 1],X_PCA[Y_cv == 0, 2], color='r', label=\"Not Bankrupt\",edgecolor='k')\nax3.scatter(X_PCA[Y_cv == 1,1],X_PCA[Y_cv == 1, 2], color='b', label=\"Bankrupt\",edgecolor='k')\nax3.set_title(\"2nd and 3rd Principal Components\")\nax3.legend()\n\nax4.scatter(X_PCA[Y_cv == 0, 1],X_PCA[Y_cv == 0, 3], color='r', label=\"Not Bankrupt\",edgecolor='k')\nax4.scatter(X_PCA[Y_cv == 1,1],X_PCA[Y_cv == 1, 3], color='b', label=\"Bankrupt\",edgecolor='k')\nax4.set_title(\"1st and 4th Principal Components\")\nax4.legend()\n\n# ------------------------ Original Features -------------------------------------\nax5.scatter(X_k[Y_cv == 0, 0],X_k[Y_cv == 0, 3], color='r', label=\"Not Bankrupt\",edgecolor='k')\nax5.scatter(X_k[Y_cv == 1,0],X_k[Y_cv == 1, 3], color='b', label=\"Bankrupt\",edgecolor='k')\nax5.set_title(new_features[0]+\" vs \"+new_features[3])\nax5.legend()\n\nax6.scatter(X_k[Y_cv == 0, 0],X_k[Y_cv == 0, 6], color='r', label=\"Not Bankrupt\",edgecolor='k')\nax6.scatter(X_k[Y_cv == 1,0],X_k[Y_cv == 1, 6], color='b', label=\"Bankrupt\",edgecolor='k')\nax6.set_title(new_features[0]+\" vs \"+new_features[6])\nax6.legend()\n\nax7.scatter(X_k[Y_cv == 0, 3],X_k[Y_cv == 0, 6], color='r', label=\"Not Bankrupt\",edgecolor='k')\nax7.scatter(X_k[Y_cv == 1,3],X_k[Y_cv == 1, 6], color='b', label=\"Bankrupt\",edgecolor='k')\nax7.set_title(new_features[3]+\" vs \"+new_features[6])\nax7.legend()\n\nax8.scatter(X_k[Y_cv == 0, 0],X_k[Y_cv == 0, 7], color='r', label=\"Not Bankrupt\",edgecolor='k')\nax8.scatter(X_k[Y_cv == 1,0],X_k[Y_cv == 1, 7], color='b', label=\"Bankrupt\",edgecolor='k')\nax8.set_title(new_features[0]+\" vs \"+new_features[7])\nax8.legend()","ee8b5927":"#Forming the training set\n#getting the best 10 principal components\nPC_df = pd.DataFrame(data = X_PCA[:,:10]\n             , columns = ['PC1', 'PC2', 'PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10'])\n#getting the best 30 features\nbestk_df = pd.DataFrame(data = X_k, columns = new_features )\n\nX_cv = pd.concat( [bestk_df, PC_df] , axis=1)\n\n#Forming the test set\n\n#getting the best 10 principal components\nX_PCA_test = pca.transform(X_test)                   \nPC_df_test = pd.DataFrame(data = X_PCA_test[:,:10]\n             , columns = ['PC1', 'PC2', 'PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10']) \n\n#getting the best 30 features\nX_k_test = best_k.transform(X_test)\nbestk_test_df = pd.DataFrame(data = X_k_test, columns = new_features )\n          \nX_test = pd.concat( [bestk_test_df, PC_df_test] , axis=1)\n","ca45ad19":"print(\"Bankrupt?\")\nprint(\"No   : \" , df_train['Bankrupt?'].value_counts()[0] , \" -> \", round( 100*df_train['Bankrupt?'].value_counts()[0]\/len(df_train) ),\"%\" )\nprint(\"Yes  : \" , df_train['Bankrupt?'].value_counts()[1] , \"  -> \", round( 100*df_train['Bankrupt?'].value_counts()[1]\/len(df_train) ),\"%\" )","c8867aec":"smt = SMOTE(sampling_strategy=0.3)\nX_smt, y_smt = smt.fit_resample(X_cv, Y_cv)","24bda8ef":"sss = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)","073ce27f":"with warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\", category=UserWarning) #ignore constant feature\n    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\n    Log_param_grid = {\"C\":np.logspace(-8,1,10), \"penalty\":[\"none\",\"l1\",\"l2\"]}\n            \n    #logistic regression - Original Features\n    log_model = LogisticRegression(class_weight ='balanced',solver ='saga', max_iter=5000, random_state = 42)  \n    search = GridSearchCV(log_model, Log_param_grid, scoring ='f1',\n                                  cv=sss, n_jobs=-1 )\n    results = search.fit(X_cv, Y_cv)\n\n    print(\"Cross Validation Resuts for Logistic Regression without Resampled Data\")\n    print(\"Best F1 Score: \",results.best_score_)\n    print(\"Parameters: \", results.best_params_)\n","6fff1188":"with warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\", category=UserWarning) #ignore constant feature\n    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\n\n    best_logit = LogisticRegression( C = 0.01, penalty='l2', class_weight ='balanced', solver ='saga', max_iter = 5000, random_state = 42 )\n    best_logit.fit(X_cv, Y_cv)\n\n    # Testing \n    y_pred = best_logit.predict(X_test)\n\n    # Model Evaluation metrics \n\n    print('Accuracy Score : ' + str(accuracy_score(Y_test,y_pred)))\n    print('Precision Score : ' + str(precision_score(Y_test,y_pred)))\n    print('Recall Score : ' + str(recall_score(Y_test,y_pred)))\n    print('F1 Score : ' + str(f1_score(Y_test,y_pred)))","0d698de6":"print(\"With resampled training data:\")\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\", category=UserWarning) #ignore constant feature\n    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\n    Log_param_grid = {\"C\":np.logspace(-8,1,10), \"penalty\":[\"none\",\"l1\",\"l2\"]}\n            \n    #logistic regression\n    log_model = LogisticRegression(class_weight ='balanced',solver ='saga', max_iter=500,  random_state = 42)  \n    search = GridSearchCV(log_model, Log_param_grid, scoring ='f1',cv=sss, n_jobs=-1)\n          \n    results = search.fit(X_smt, y_smt)\n      \n    print(\"Cross Validation Resuts for Logistic Regression with Resampled Data\")\n    print(\"Best F1 Score: \",results.best_score_)\n    print(\"Parameters: \", results.best_params_)","2901cf58":"with warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\", category=UserWarning) #ignore constant feature\n    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n    \n    best_logit = LogisticRegression( C = 1, penalty='l1', class_weight ='balanced', solver ='saga', max_iter = 500,  random_state = 42 )\n    best_logit.fit(X_smt, y_smt)\n\n    y_pred = best_logit.predict(X_test)\n\n    # Model Evaluation metrics \n\n    print('Accuracy Score : ' + str(accuracy_score(Y_test,y_pred)))\n    print('Precision Score : ' + str(precision_score(Y_test,y_pred)))\n    print('Recall Score : ' + str(recall_score(Y_test,y_pred)))\n    print('F1 Score : ' + str(f1_score(Y_test,y_pred)))","cddd977a":"with warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\", category=UserWarning) #ignore constant feature\n    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\n    knn_grid = { 'n_neighbors': [3, 5, 7, 9, 11],'metric':['euclidean','manhattan']  }\n\n    knn_model = KNeighborsClassifier(algorithm='brute')\n    search = GridSearchCV(knn_model, knn_grid, scoring='f1', cv=sss, n_jobs=-1)\n    results = search.fit(X_cv, Y_cv)\n\n    print(\"Cross Validation Resuts for KNN with Resampled Data\")\n    print(\"Best F1 Score: \",results.best_score_)\n    print(\"Parameters: \", results.best_params_)","c0c0953b":"with warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\", category=UserWarning) #ignore constant feature\n    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\n    knn_model = KNeighborsClassifier(algorithm='brute', n_neighbors=3, metric='manhattan' )\n    knn_model.fit(X_cv, Y_cv)\n\n    #testing\n    y_pred = knn_model.predict(X_test)\n\n    # Model Evaluation metrics \n\n    print('Accuracy Score : ' + str(accuracy_score(Y_test,y_pred)))\n    print('Precision Score : ' + str(precision_score(Y_test,y_pred)))\n    print('Recall Score : ' + str(recall_score(Y_test,y_pred)))\n    print('F1 Score : ' + str(f1_score(Y_test,y_pred)))","63c46a54":"with warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\", category=UserWarning) #ignore constant feature\n    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\n    tree_param = {'criterion':['gini','entropy'],'max_depth':[5,7,10,12,15,20,30,40,50]}\n\n    search = GridSearchCV(RandomForestClassifier( random_state = 42),tree_param , scoring ='f1',cv=sss, n_jobs=-1 )\n    results = search.fit(X_smt, y_smt)\n\n    print(\"Cross Validation Resuts for Random Forest with Resampled Data\")\n    print(\"Best F1 Score: \",results.best_score_)\n    print(\"Parameters: \", results.best_params_)","73559603":"with warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\", category=UserWarning) #ignore constant feature\n    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n    \n    forest = RandomForestClassifier(criterion=\"entropy\", max_depth= 20, random_state = 42 )\n    forest.fit(X_smt, y_smt)\n\n    # Testing \n    y_pred =  forest.predict(X_test)\n\n    # Model Evaluation metrics \n    print('Accuracy Score : ' + str(accuracy_score(Y_test,y_pred)))\n    print('Precision Score : ' + str(precision_score(Y_test,y_pred)))\n    print('Recall Score : ' + str(recall_score(Y_test,y_pred)))\n    print('F1 Score : ' + str(f1_score(Y_test,y_pred)))\n\n    print(classification_report(Y_test,y_pred))","2b50d2af":"dtrain_origin = xgb.DMatrix(X_cv, label=Y_cv)\ndtrain_smt = xgb.DMatrix(X_smt, label=y_smt)\n    \ndtest = xgb.DMatrix(X_test, label=Y_test)","f4020eb3":"# TO DO : Implement parameter tuning my random search\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\", category=UserWarning) #ignore constant feature\n    warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\n    params = {\"objective\":'reg:logistic','learning_rate': 0.05,\n                'max_depth': 10, 'booster': 'gbtree' }\n    \n    #For the non-resmapled taining data\n    cv_origin_results = xgb.cv(dtrain=dtrain_origin, params=params, nfold=10,\n                    num_boost_round=60,early_stopping_rounds=10,metrics=\"auc\", as_pandas=True, seed=42)\n    \n    # Print cv_results\n    mean_AUC = cv_origin_results['test-auc-mean'].max()\n    boost_rounds = cv_origin_results['test-auc-mean'].idxmax()\n    print(\"\\t For the non-resmapled data: AUC {} for {} rounds\".format(mean_AUC, boost_rounds))\n    \n    #For the training data with resampling\n    cv_smt_results = xgb.cv(dtrain=dtrain_smt, params=params, nfold=10,\n                    num_boost_round=60,early_stopping_rounds=10,metrics=\"auc\", as_pandas=True, seed=42)\n    # Print cv_results\n    mean_AUC = cv_smt_results['test-auc-mean'].max()\n    boost_rounds = cv_smt_results['test-auc-mean'].idxmax()\n    print(\"\\t For the resmapled data: AUC {} for {} rounds\".format(mean_AUC, boost_rounds))\n    ","f83a5b70":"# TO DO ","a067a515":"## Comparison of Models","da2765e4":"### TO DO: Complete XGBoost","95adb7c3":"#### Train-Test Split","a702a346":"Now we obtain the test performance (test set was not resampled) with the best parameter configuration:","28cd16c0":"A visual seperation between the two classes is present for all of the feature pairs above, however, whether the models can succesfully learn decision boundaries that correspond to this seperation is uncertain.\n\nIn preivous versions of this notebook, it was observed that using only PCA components did not outperform the original features for most models and when it did, the difference in performance was negligable. Therefore, we will instead add the best 10 principal components to the best 30 original + engineered features to obtain our final training dataset.","4869b7c8":"It appears that none of the existing features is very highly correlated with the label but high colinearity between the features is present to the point where some features behave identically. This is not suprising for many of the financial indicators, such as the pair of \n\n**Debt ratio %**  and  **Net Worth\/Assets**                   \nsince:                                               \n    ***Net Worth\/Assets = (Assets \u2013 Liabilities)\/Assets***                                       \n    and                                  \n    ***Debt ratio = Liablities \/ Assets***                      \nso we should have:                             \n    **Net Worth\/Assets = 1 - Debt ratio**\n    \nThese types of relationships are present for most of the indicators, with variations such as the difference between \"current liabilities\" versus \"total liablities\" leading to differences between the information provided by the indicators, which we observe with the \"Current Liabilites to Assets\" feature. This feature has a correlation coefficient of 0.84 with \"Debt Ratio %\" \n\nA similar case can be made for \"Borrowing Dependency\" and \"Liability to Equity\" which have a correlation coefficient of 0.96.\n\nThe high correlation coefficients here do not necessarily mean that the features are redundant as the differences between them may provide valuable insights about bankruptcy likelihood. \n\nOn the other hand, the correlation coefficinet of 1 between \"Net Value Per Share(A)\", \"Net Value Per Share(B)\" and \"Net Value Per Share(C)\" indicates an obvious redundance.\n","25550c9e":"\n\n---\n\n\n## **Ideas**\n\n\n*   Try feature selection methods that account for redundancy (ex: MRMR)\n*   Do research on the financial indicators and try to engineer financially logical features from the existing set.\n","744b8c88":"  Both accuracy and F1 score appear to be similar to the non-resampled version. \n  \n## K Nearest Neighbours Classifier","b7ec18ef":"The first three and the next three features are groups of almost identical, highly correlated features. Therefore, we will display the 1st, 4th and 7th features instead of the first three.","07c0c09f":"### XGBoost Classifier\n\n--- incomplete ---- ","2018167a":"## General structure of the dataset\n\n### Displaying the first few rows","275d906d":"Some of the columns are likely to have high colinearity and be redundant, which we will try to analyze later. Now we observe the relationships between the features.\n\nThis section was inspired by: https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python","910f93be":"\n#### With Dataset Resampling\nWe will evaluate whether resampling the data to increase the representation of the positive labelled class improves the testing F1 score.","c05b6459":"### Extracing new featurs with PCA\n\nPrincipal components will also be used as an alternative set of features.","620b10f2":"Now we obtain the test performance with the best parameter configuration:","07b9926c":"Now, looking at the pair plots, we will add new features to the dataset.\n\nInspired by the polynomial feature engineering in: https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset","2a4aa7bf":"## Machine Learning Models\n\nThe following methods will be evaluated using,\n\nFirst, simple models to observe the difficulty of the task:\n- Logistic Regression Classifier\n- K Nearest Neighbours\n\nAs will be seen, performance on these algorithms will not be satisfactory, so they will be followed by more complex models to maximize performance:\n- Random Forest\n- XGBoost\n\n### Logistic Regression\n\nTo search for the best parameter set, F1 score will be used as the evaluation metric. If accuracy is used, simply labelling all data points as not Bankrupt is enough to obtain 96-97% accuracy due to the high imbalance between the classes. F1 score better displays performance, given that the positive class is only 3% of the data.\n\n#### Without Dataset Resampling","8dd653b6":"Looking at the pair plots above, we will determine outlier thresholds for certain features and drop outliers that do not obey the general trend.","7678afe2":"The first three principal components demonstrate a visible seperation between the two classes.","a78fd413":"It appears that performance (F1 Score) on the cross validation set does not translate well to the test set. Also, F1 score is lower than it was for all variations of logistic regression.\n\n### Random Forest Classifier","59910b7a":"We see that approximately 30 principal components are needed to reach a cumulative explained variance of 80%. \n\nNow we will compare the spread of the first 4 principal components to the spread of the 4 original highest ranked features according to ANOVA F measure.","6b0c586c":"No missing values are present in the dataset.\n\n### Displaying the feature names and types","0d0ef54b":"#### Balancing the training set\n\nA version of the training data resampled to increase the ratio of \"Bankrupt? = 1\" labelled data points will be evaluated along with the non-resampled training set. A combination of over and under-sampling ( SMOTE + Tomek ) will be used.","6d69d253":"One thing to point out is that there are groups of features that appear highly correlated with each other as well as the label. When we use a feature selection method we will get a number of redundant features in the process as well.\nBefore applying feature selection methods based on the statistical proeperties of the individual  features, it may help to drop features that are duplicates or inverted versions of each other in terms of the information they provide. ","2f3dd32b":"### Log Transform and Standardization","c558102e":"Testing the best performer:","fae029fd":"## Procedure Followed\n\nAs we will see in the following analysis, there will be a number of decisions to be made on how to approach our data:\n\n**1 - Feautures comprising our data are on different scales, should we transform the data to account for it?** \n  - For features with skewed distributions, we will first apply log transform.\n  - We will standardize the dataset.\n  - Looking at the plots of the most important feature pairs, we will elminate data points that do not obey the main trend observed\n    \n**2- Our data is drastically unbalanced with the positive label representing only 3% of data points. Then, should we include steps to balance our training dataset? If so, which methods should we use?** \n\n  - We will carry out the entire process first without and later with artificially balaced data. Then, we will compare the results.\n    \n**3- Our data is very high dimensional. Should we choose subsets of the available features?**\n\n  - We will use ANOVA F measure to assign scores to each feature and select k features with the highest scores. We will train our models with a range of different k values and keep the best performing feature set.\n  - We will also use PCA to extract features and repeat the process with k principle components, varying k.","2ae29072":"# Company Bankruptcy Prediction \n\nAuthor: Mert Ertugrul\n\nStatus: In-progress\n\nThis notebook was prepared to work on the task \"Company Bankruptcy Prediction\" found at the following link: https:\/\/www.kaggle.com\/fedesoriano\/company-bankruptcy-prediction\n\nDataset description provided: \"The data were collected from the Taiwan Economic Journal for the years 1999 to 2009. Company bankruptcy was defined based on the business regulations of the Taiwan Stock Exchange.\"\n\n-Dataset includes 95 features, almost all are numerical. High dimensinality will be an important factor in model choice, feature selection and extraction.","66a9239e":"It is apparent that our features come in groups of different distributions, some resembling exponential while some more similar to normal or skewed normal distributions. We also know that most features are numerical and are on different scales. Log transformation could not eliminate the skewness of most distributions.\n","8441919c":"\n### Displaying the distributions of the features","23ce0dc4":"### Looking for missing values","ea766b56":"Now we inspect the correlation matrix of the features to observe the relationships between them.","2eebc1c2":"We see that all features other than  \"Net Income Flag\" and \"Liability-Assets Flag\" are numerical. "}}