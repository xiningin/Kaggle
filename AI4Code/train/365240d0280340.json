{"cell_type":{"fd79523c":"code","1e622081":"code","e42baad6":"code","03940694":"code","83c78160":"code","e18df757":"code","a986a2a6":"code","78abc17d":"code","d0268a40":"code","716e238a":"code","1c31a71e":"code","8bbdab16":"code","b66a6277":"code","8a3b38d7":"code","d9efd661":"code","fb84ecc8":"code","257f0786":"code","f45dfcd3":"code","2bdb2596":"code","255882db":"code","127fee01":"code","3265fd74":"code","49751f12":"code","a9118592":"code","21c22ce2":"code","9bd1973b":"code","8b3048d6":"code","6fc4abd5":"code","5d0f2a0a":"code","1241e260":"code","477d2d9d":"code","e3b265fc":"code","f5fff8fa":"code","a6ccb365":"code","505dd9fb":"code","4b2a39df":"code","f2fbfbd6":"code","246c8eb6":"code","1e556285":"code","ebd612ff":"code","af8ed933":"code","67de9860":"code","2ed32dd2":"code","b0625978":"code","26bbfb0f":"code","ef35ac88":"code","fcfa2c77":"code","e0907a5c":"code","89c89238":"code","c50876ee":"code","e332ca67":"code","6bd70f25":"code","ba342a63":"code","75f0bb46":"code","77247ced":"code","2f498137":"code","3afd61d7":"code","e44c75b6":"code","ce0f29d0":"code","146f0d7a":"code","8f018337":"code","26e721a3":"code","f1c89185":"code","0a66b29a":"code","0b9e999f":"code","ee837fdc":"code","e7533bf5":"code","9b096d47":"code","84b337f8":"code","b3fb3c0a":"code","91c8e3e8":"code","a7cbdc36":"code","f36b1dc3":"code","af8a5a77":"code","7d1a7a77":"code","cb91db73":"code","91dbcfb9":"code","a6c3d954":"code","a0412f4a":"code","a2c6bdf1":"code","ccb57fd0":"code","bbd156c2":"code","0ec6eda6":"code","770a8bbc":"code","f9f24ffd":"code","32252844":"code","8381cd87":"code","da56a6dd":"code","e4117d55":"code","52726db2":"code","b2fdb571":"code","eb7b64c6":"code","339f3c17":"code","dc728e53":"code","69b7197b":"code","95cc6532":"code","d49f2ac5":"code","a433c7cd":"code","b0d461cb":"code","b6eef676":"code","a2b70560":"code","a00fbbb1":"code","94af4145":"code","3a196653":"markdown","3fc3e26a":"markdown","1fb9aed9":"markdown","d50d855c":"markdown","baa5ceb2":"markdown","14c5958e":"markdown","f3c9b1c3":"markdown","dd16db55":"markdown","e2727793":"markdown","960a0b91":"markdown","c7f01e15":"markdown","4a2719dc":"markdown","8b449137":"markdown","6050800e":"markdown","dddaad73":"markdown","0700c5f5":"markdown","822e37d9":"markdown","72cedc0e":"markdown"},"source":{"fd79523c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1e622081":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom itertools import product\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport re\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import plot_importance","e42baad6":"train = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\ntest = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/test.csv\")\nitem_category = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")\nitem = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/items.csv\")\nshop = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/shops.csv\")","03940694":"'''dataset = train.pivot_table(index = ['shop_id','item_id'],values = ['item_cnt_day'],columns = ['date_block_num'],fill_value = 0, aggfunc='sum')\ndataset.reset_index(inplace = True)\ndataset.head()'''","83c78160":"'''test.head()'''","e18df757":"'''dataset = pd.merge(test,dataset,on = ['item_id','shop_id'],how = 'left')\ndataset.fillna(0,inplace = True)\ndataset.head()'''","a986a2a6":"'''dataset.drop(['shop_id','item_id','ID'],inplace = True, axis = 1)\ndataset.head()'''","78abc17d":"'''dataset.shape'''","d0268a40":"'''# X we will keep all columns execpt the last one \nX_train = np.expand_dims(dataset.values[:,:-1],axis = 2)\n# the last column is our label\ny_train = dataset.values[:,-1:]\n\n# for test we keep all the columns execpt the first one\nX_test = np.expand_dims(dataset.values[:,1:],axis = 2)\n\n# lets have a look on the shape \nprint(X_train.shape,y_train.shape,X_test.shape)'''","716e238a":"'''from keras.models import Sequential\nfrom keras.layers import LSTM,Dense,Dropout'''","1c31a71e":"'''baseline_model = Sequential()\nbaseline_model.add(LSTM(units = 64,input_shape = (33,1)))\nbaseline_model.add(Dense(1))\n\nbaseline_model.compile(loss = 'mse',optimizer = 'adam', metrics = ['mean_squared_error'])\nbaseline_model.summary()'''","8bbdab16":"'''baseline_model.fit(X_train,y_train,batch_size = 4096,epochs = 10)'''","b66a6277":"'''# creating submission file \nsubmission_pfs = baseline_model.predict(X_test)\n# creating dataframe with required columns \nsubmission = pd.DataFrame({'ID':test['ID'],'item_cnt_month':submission_pfs.ravel()})\n# creating csv file from dataframe\nsubmission.to_csv('sub_pfs.csv',index = False)'''","8a3b38d7":"'''submission.head()'''","d9efd661":"def basic_EDA(df):\n    print(\"-------Sample data---------\")\n    print(df.head())\n    print(\"-------Description---------\")\n    print(df.describe())\n    print(\"-------Information---------\")\n    print(df.info())\n    print(\"-------Columns---------\")\n    print(df.columns)\n    print(\"-------Data Types---------\")\n    print(df.dtypes)\n    print(\"-------NULL and NA Values---------\")\n    print(df.isnull().sum())\n    print(df.isna().sum())\n    print(\"-------Shape---------\")\n    print(df.shape)","fb84ecc8":"basic_EDA(train)","257f0786":"train['date'] = pd.to_datetime(train['date'],format = '%d.%m.%Y')","f45dfcd3":"train.head()","2bdb2596":"train.dtypes","255882db":"boxplot = train.boxplot(column=['item_cnt_day'])","127fee01":"boxplot = train.boxplot(column=['item_price'])","3265fd74":"train[train.item_price > 100000]","49751f12":"train[train.item_price < 0]","a9118592":"train[train.item_cnt_day >= 1000]","21c22ce2":"train[train.item_cnt_day < 0]","9bd1973b":"#Let's check if the neg item_cnt_day are return?\n'''neg = train[train.item_cnt_day < 0]\nneg.head()\n\npos = train[train.item_cnt_day > 0]\npos.head()'''\n#A function to check if the -ve item_cnt_day are return or not, but this function takes along time to run\n#need to optimise it in some way so that it can be run in a reasonable amount of time\n#Untill that time we will remove the -ve and continue as such.\n'''return_counter = 0\nfor ind in neg.index:\n    for i in range(ind):\n        if ((neg['shop_id'][ind] == pos['shop_id'][i]) and \n        (neg['item_id'][ind] == pos['item_id'][i]) and \n        (neg['item_cnt_day'][ind] + pos['item_cnt_day'][i] == 0)):\n            return_counter = return_counter + 1\n        \n            \nprint(return_counter)'''","8b3048d6":"train = train[(train.item_price < 300000 ) & (train.item_price > 0) & (train.item_cnt_day < 1000) & (train.item_cnt_day > 0)]\ntrain.reset_index(inplace = True)\ntrain.head()","6fc4abd5":"basic_EDA(shop)","5d0f2a0a":"shop[\"city\"] = shop.shop_name.str.split(\" \").map( lambda x: x[0] )\nshop[\"type\"] = shop.shop_name.str.split(\" \").map( lambda x: x[1] )","1241e260":"shop.head()","477d2d9d":"shop['city'].unique()","e3b265fc":"shop['type'].unique()","f5fff8fa":"shop[\"shop_type\"] = LabelEncoder().fit_transform( shop.type )\nshop[\"shop_city\"] = LabelEncoder().fit_transform( shop.city )\nshop.head()","a6ccb365":"shop = shop[['shop_id', 'shop_city', 'shop_type']]\nshop.head()","505dd9fb":"basic_EDA(item_category)","4b2a39df":"print(item_category['item_category_name'].str.split('-').map(lambda x: x[0]))\n#print(item_category['item_category_name'].str.split('-').map(lambda x: x[1]))","f2fbfbd6":"item_category","246c8eb6":"print(item_category['item_category_name'].str.split('-').map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip()))","1e556285":"item_category['item_type'] = item_category['item_category_name'].str.split('-').map(lambda x: x[0])\nitem_category['item_name'] = item_category['item_category_name'].str.split('-').map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\nitem_category.head()","ebd612ff":"item_category['item_type'].unique()","af8ed933":"item_category['item_name'].unique()","67de9860":"item_category['item_type_code'] = LabelEncoder().fit_transform(item_category.item_type)\nitem_category['item_name_code'] = LabelEncoder().fit_transform(item_category.item_name)\nitem_category.head()","2ed32dd2":"item_category = item_category[['item_category_id', 'item_type_code', 'item_name_code']]\nitem_category.head()","b0625978":"basic_EDA(item)","26bbfb0f":"list(item.item_name.unique())","ef35ac88":"item.shape","fcfa2c77":"def clean_text(item_name):\n    item_name = item_name.lower()\n    item_name = re.sub(r'[^\\w\\s]', '', item_name)\n    item_name = re.sub(r'\\d+', '', item_name)\n    item_name = re.sub(' +', ' ', item_name)\n    item_name = item_name.strip()\n    return item_name","e0907a5c":"item['clean_item_name'] = item['item_name'].apply(clean_text)\nitem.head()","89c89238":"item['sub_name1'] = item['clean_item_name'].str.split(' ').map(lambda x: x[0])\nitem['sub_name2'] = item['clean_item_name'].str.split(' ').map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\n'''\nitem['sub_name3'] = item['clean_item_name'].str.split(' ').map(lambda x: x[2].strip() if len(x) > 2 else x[1].strip() if len(x) > 1 else x[0].strip())\nitem['sub_name4'] = item['clean_item_name'].str.split(' ').map(lambda x: x[3].strip() if len(x) > 3 else x[2].strip() if len(x) > 2 else x[1].strip() if len(x) > 1 else x[0].strip())\nitem['sub_name5'] = item['clean_item_name'].str.split(' ').map(lambda x: x[4].strip() if len(x) > 4 else x[3].strip() if len(x) > 3 else x[2].strip() if len(x) > 2 else x[1].strip() if len(x) > 1 else x[0].strip())\n'''\nitem.head()","c50876ee":"item.sub_name1.unique().shape, item.sub_name2.unique().shape","e332ca67":"item.item_id.unique().shape, item.item_category_id.unique().shape","6bd70f25":"item['sub_name_1'] = LabelEncoder().fit_transform(item.sub_name1)\nitem['sub_name_2'] = LabelEncoder().fit_transform(item.sub_name2)\n'''item['sub_name_3'] = LabelEncoder().fit_transform(item.sub_name3)\nitem['sub_name_4'] = LabelEncoder().fit_transform(item.sub_name4)\nitem['sub_name_5'] = LabelEncoder().fit_transform(item.sub_name5)'''\nitem.head()","ba342a63":"item = item[['item_id', 'item_category_id', 'sub_name_1', 'sub_name_2']]\nitem.head()","75f0bb46":"basic_EDA(test)","77247ced":"len(set(test.item_id) - set(test.item_id).intersection(set(train.item_id)))","2f498137":"len(set(test.shop_id) - set(test.shop_id).intersection(set(train.shop_id)))","3afd61d7":"matrix = []\ncols = ['date_block_num','shop_id','item_id']\nfor i in range(34):\n    sales = train[train.date_block_num==i]\n    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))\n    \nmatrix = pd.DataFrame(np.vstack(matrix), columns=cols)\nmatrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix.sort_values(cols,inplace=True)","e44c75b6":"matrix.head()","ce0f29d0":"matrix.dtypes, matrix.shape","146f0d7a":"train['revenue'] = train['item_price'] *  train['item_cnt_day']","8f018337":"train.head()","26e721a3":"cols = ['date_block_num','shop_id','item_id']\ngroup = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)","f1c89185":"group.head()","0a66b29a":"group.dtypes, group.shape","0b9e999f":"boxplot = group.boxplot(column = ['item_cnt_month'])","ee837fdc":"matrix = pd.merge(matrix, group, on=cols, how='left')\nmatrix['item_cnt_month'] = (matrix['item_cnt_month']\n                            .astype(np.float32)\n                            .fillna(0))","e7533bf5":"matrix.head(10)","9b096d47":"matrix.dtypes, matrix.shape","84b337f8":"test['date_block_num'] = 34\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\ntest['shop_id'] = test['shop_id'].astype(np.int8)\ntest['item_id'] = test['item_id'].astype(np.int16)\ntest.head()","b3fb3c0a":"test.dtypes, test.shape","91c8e3e8":"matrix = pd.concat([matrix, test], ignore_index = True, keys = cols, sort = False)\nmatrix.fillna(0, inplace = True)\nmatrix.head()","a7cbdc36":"matrix = pd.merge(matrix, shop, on = 'shop_id', how = 'left')\nmatrix = pd.merge(matrix, item, on = 'item_id', how = 'left')\nmatrix = pd.merge(matrix, item_category, on = 'item_category_id', how = 'left')\nmatrix.head()","f36b1dc3":"matrix.dtypes","af8a5a77":"matrix.drop(['ID'], axis=1, inplace=True)\nmatrix.head()","7d1a7a77":"matrix['shop_city'] = matrix['shop_city'].astype(np.int8)\nmatrix['shop_type'] = matrix['shop_type'].astype(np.int8)\nmatrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\nmatrix['sub_name_1'] = matrix['sub_name_1'].astype(np.int16)\nmatrix['sub_name_2'] = matrix['sub_name_2'].astype(np.int16)\n'''matrix['sub_name_3'] = matrix['sub_name_3'].astype(np.int16)\nmatrix['sub_name_4'] = matrix['sub_name_4'].astype(np.int16)\nmatrix['sub_name_5'] = matrix['sub_name_5'].astype(np.int16)'''\nmatrix['item_type_code'] = matrix['item_type_code'].astype(np.int8)\nmatrix['item_name_code'] = matrix['item_name_code'].astype(np.int8)\nmatrix.dtypes","cb91db73":"matrix.head()","91dbcfb9":"matrix['month'] = matrix['date_block_num'] % 12\nmatrix.head()","a6c3d954":"days = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\nmatrix['days'] = matrix['month'].map(days).astype(np.int8)\nmatrix.head()","a0412f4a":"matrix.dtypes","a2c6bdf1":"#item_cnt_month of previous month, previous 2, 3, 6, and 12 month back\ndef lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted[col+'_lag_'+str(i)] = shifted[col+'_lag_'+str(i)].astype(np.float16)\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'item_cnt_month')\nmatrix.head()","ccb57fd0":"#Average and lag by date_block_num\ngroup = matrix.groupby(['date_block_num']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num'], how='left')\nmatrix['date_avg_item_cnt'] = matrix['date_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_avg_item_cnt')\nmatrix.drop(['date_avg_item_cnt'], axis=1, inplace=True)\nmatrix.head(10)","bbd156c2":"'''#Average and Lag by month\ngroup = matrix.groupby(['month']).agg({'item_cnt_month':['mean']})\ngroup.columns = ['month_avg_item_cnt']\ngroup.reset_index(inplace = True)\n\nmatrix = pd.merge(matrix, group, on=['month'], how = 'left')\nmatrix['month_avg_item_cnt'] = matrix['month_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'month_avg_item_cnt')\nmatrix.head(10)'''","0ec6eda6":"#Average and Lag by data_block_num and shop_id\ngroup = matrix.groupby(['date_block_num','shop_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_shop_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id'], how='left')\nmatrix['date_shop_avg_item_cnt'] = matrix['date_shop_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_avg_item_cnt')\nmatrix.head(10)","770a8bbc":"'''#Average and Lag by month and shop_id\ngroup = matrix.groupby(['month','shop_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'month_shop_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['month', 'shop_id'], how='left')\nmatrix['month_shop_avg_item_cnt'] = matrix['month_shop_avg_item_cnt'].astype(np.float16)\n#matrix = lag_feature(matrix, [1], 'month_shop_avg_item_cnt')\nmatrix.head(10)'''","f9f24ffd":"#Average and Lag by data_block_num and item_id\ngroup = matrix.groupby(['date_block_num','item_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'item_id'], how='left')\nmatrix['date_item_avg_item_cnt'] = matrix['date_item_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_item_avg_item_cnt')\nmatrix.head(10)","32252844":"'''#Average and Lag by month and item_id\ngroup = matrix.groupby(['month','item_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'month_item_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['month', 'item_id'], how='left')\nmatrix['month_item_avg_item_cnt'] = matrix['month_item_avg_item_cnt'].astype(np.float16)\n#matrix = lag_feature(matrix, [1], 'month_item_avg_item_cnt')\nmatrix.head(10)'''","8381cd87":"'''#Average and Lag by month and shop_city\ngroup = matrix.groupby(['month','shop_city']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'month_shop_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['month', 'shop_city'], how='left')\nmatrix['month_shop_city_avg_item_cnt'] = matrix['month_shop_city_avg_item_cnt'].astype(np.float16)\n#matrix = lag_feature(matrix, [1], 'month_shop_city_avg_item_cnt')\nmatrix.head(10)'''","da56a6dd":"'''#Average and Lag by month and shop_type\ngroup = matrix.groupby(['month','shop_type']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'month_shop_type_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['month', 'shop_type'], how='left')\nmatrix['month_shop_type_avg_item_cnt'] = matrix['month_shop_type_avg_item_cnt'].astype(np.float16)\n#matrix = lag_feature(matrix, [1], 'month_shop_type_avg_item_cnt')\nmatrix.head(10)'''","e4117d55":"'''#Average and Lag by date_block_num and shop_city\ngroup = matrix.groupby(['date_block_num','shop_city']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_shop_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_city'], how='left')\nmatrix['date_shop_city_avg_item_cnt'] = matrix['date_shop_city_avg_item_cnt'].astype(np.float16)\n#matrix = lag_feature(matrix, [1], 'date_shop_city_avg_item_cnt')\nmatrix.head(10)'''","52726db2":"'''#Average and Lag by date_block_num and shop_type\ngroup = matrix.groupby(['date_block_num','shop_type']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_shop_type_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_city'], how='left')\nmatrix['date_shop_type_avg_item_cnt'] = matrix['date_shop_type_avg_item_cnt'].astype(np.float16)\n#matrix = lag_feature(matrix, [1], 'date_shop_type_avg_item_cnt')\nmatrix.head(10)'''","b2fdb571":"'''#Average and Lag by month and item_category_id\ngroup = matrix.groupby(['month','item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'month_cat_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['month', 'item_category_id'], how='left')\nmatrix['month_cat_avg_item_cnt'] = matrix['month_cat_avg_item_cnt'].astype(np.float16)\n#matrix = lag_feature(matrix, [1], 'month_cat_avg_item_cnt')\nmatrix.head(10)'''","eb7b64c6":"'''#Average and Lag by month and item_type_code\ngroup = matrix.groupby(['month','item_type_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'month_item_type_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['month', 'item_type_code'], how='left')\nmatrix['month_item_type_avg_item_cnt'] = matrix['month_item_type_avg_item_cnt'].astype(np.float16)\n#matrix = lag_feature(matrix, [1], 'month_item_type_avg_item_cnt')\nmatrix.head(10)'''","339f3c17":"'''#Average and Lag by month and item_name_code\ngroup = matrix.groupby(['month','item_name_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'month_item_name_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['month', 'item_name_code'], how='left')\nmatrix['month_item_name_avg_item_cnt'] = matrix['month_item_name_avg_item_cnt'].astype(np.float16)\n#matrix = lag_feature(matrix, [1], 'month_item_name_avg_item_cnt')\nmatrix.head(10)'''","dc728e53":"'''#Average and Lag by data_block_num and item_category_id\ngroup = matrix.groupby(['date_block_num','item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_cat_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'item_category_id'], how='left')\nmatrix['date_cat_avg_item_cnt'] = matrix['date_cat_avg_item_cnt'].astype(np.float16)\n#matrix = lag_feature(matrix, [1], 'date_cat_avg_item_cnt')\nmatrix.head(10)'''","69b7197b":"'''#Average and Lag by data_block_num and item_type_code\ngroup = matrix.groupby(['date_block_num','item_type_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_type_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'item_type_code'], how='left')\nmatrix['date_item_type_avg_item_cnt'] = matrix['date_item_type_avg_item_cnt'].astype(np.float16)\n#matrix = lag_feature(matrix, [1], 'date_item_type_avg_item_cnt')\nmatrix.head(10)'''","95cc6532":"'''#Average and Lag by month and item_name_code\ngroup = matrix.groupby(['date_block_num','item_name_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_name_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'item_name_code'], how='left')\nmatrix['date_item_name_avg_item_cnt'] = matrix['date_item_name_avg_item_cnt'].astype(np.float16)\n#matrix = lag_feature(matrix, [1], 'date_item_name_avg_item_cnt')\nmatrix.head(10)'''","d49f2ac5":"matrix.columns","a433c7cd":"matrix.info()","b0d461cb":"X_train = matrix[matrix.date_block_num <= 33].drop(['item_cnt_month'], axis=1)\nY_train = matrix[matrix.date_block_num <= 33]['item_cnt_month']\nX_valid = matrix[matrix.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = matrix[matrix.date_block_num == 33]['item_cnt_month']\nX_test = matrix[matrix.date_block_num == 34].drop(['item_cnt_month'], axis=1)","b6eef676":"X_train.shape, X_test.shape","a2b70560":"model = XGBRegressor(\n    max_depth=8,\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,    \n    seed=42)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds = 10)","a00fbbb1":"Y_test = model.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('submission.csv', index=False)","94af4145":"def plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\nplot_features(model, (10,14))","3a196653":"# A Baseline Model with LSTM","3fc3e26a":"# Import libraries and Data","1fb9aed9":"### Score of the Baseline Model 1.02207","d50d855c":"**Translation:** array (['PC', 'Accessories', 'Tickets (Digit)', 'Delivery of goods',\n       'Game Consoles', 'Games', 'Android Games', 'MAC Games',\n       'PC Games', 'Payment Cards (Cinema, Music, Games)', 'Payment Cards',\n       'Cinema', 'Books', 'Music', 'Gifts', 'Programs',\n       'Service', 'Service', 'Net media (spire)',\n       'Blank media (piece)', 'Batteries'], dtype = object)","baa5ceb2":"**So there are no new shop_id in test but there are 363 new item_id in test that are not in train.**","14c5958e":"## Item","f3c9b1c3":"***Translation***\n1. PC - Headsets \/ Headphones\n1. Accessories - PS2 1\n1. Accessories - PS3 2\n1. Accessories - PS4 3\n1. Accessories - PSP <br\/>\n\n**It looks like the 'item_category_name' is 'Type of item - Item name'**","dd16db55":"## Test","e2727793":"Transation using Google Translation\n\n!\u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56 \u0444\u0440\u0430\u043d\t->\t! Yakutsk Ordzhonikidze, 56 francs (Yakutsk is a city)\n\n\u0410\u0434\u044b\u0433\u0435\u044f \u0422\u0426 \"\u041c\u0435\u0433\u0430\"\t->\tAdygea shopping center \"Mega\" (Adygea is a city)\n\n\u0412\u043e\u043b\u0436\u0441\u043a\u0438\u0439 \u0422\u0426 \"\u0412\u043e\u043b\u0433\u0430 \u041c\u043e\u043b\u043b\"\t->\tVolzhsky shopping center \"Volga Mall\" (Volzhsky is a city)\n\n**Thus there is a pattern of \"city name\" + \"Type of shop\"** <br\/>\n**We can perform feature extraction from this**","960a0b91":"**So there are some which does not have the '-'**\n\n**So let's Google translate them and see what they are**\n1. Service\n1. Blank media (spire)\n1. Blank media (piece)\n1. Batteries\n\n**So they have the item type and not the specific item**","c7f01e15":"## Shop","4a2719dc":"**Translation:** array (['Headsets \/ Headphones', 'PS2', 'PS3', 'PS4', 'PSP', 'PSVita',\n       'XBOX 360', 'XBOX ONE', 'Tickets (Digit)', 'Delivery of goods',\n       'Other', 'Accessories for games', 'Number', 'Additional editions',\n       'Collector's Editions', 'Standard Editions',\n       'Payment cards (Cinema, Music, Games)', 'Live!', 'Live! (Numeral)',\n       'PSN', 'Windows (Digital)', 'Blu', 'DVD', 'Collectible',\n       'Artbooks, encyclopedias', 'Audiobooks', 'Audiobooks (Digital)',\n       'Audiobooks 1C', 'Business literature', 'Comics, manga',\n       'Computer literature', 'Methodical materials 1C', 'Postcards',\n       'Educational literature', 'Guides', 'Fiction', 'Local CD',\n       'Brand CD', 'MP3', 'Vinyl', 'Music Video',\n       'Gift Editions', 'Attributes', 'Gadgets, Robots, Sports',\n       'Stuffed Toys', 'Board Games',\n       'Board games (compact)', 'Postcards, stickers', 'Development',\n       'Certificates, services', 'Souvenirs', 'Souvenirs (hinged)',\n       'Bags, Albums, Mouse pads', 'Figures', '1C: Enterprise 8',\n       'MAC (Digit)', 'Home & Office', 'Home & Office (Digit)',\n       'Educational', 'Educational (Digit)', 'Service', 'Tickets',\n       'Blank media (spire)', 'Blank media (piece)',\n       'Batteries'], dtype = object)","8b449137":"# Model Making","6050800e":" '! IN THE POWER OF NAVIGATION (PLAST.) D ',<br \/> \n '! ABBYY FineReader 12 Professional Edition Full [PC, Digital]',<br \/> \n '*** IN THE RAYS OF GLORY (UNV) D',<br \/> \n '*** BLUE WAVE (Univ) D',<br \/> \n '*** BOX (GLASS) D',<br \/> \n '*** NEW AMERICAN GRAFFITI (UNI) D',<br \/> \n........................................<br\/>\n '\/\/ MONGOL S. Bodrov (Region)',<br \/> \n '\/\/ NOT LEAVING A TRACE (Region)',<br \/> \n '\/ BOMB FOR THE BRIDE \/ 2DVD \/ D',<br \/> \n '\/ GOLDEN COLLECTION m \/ f-72',<br \/> \n '\/ ONCE IN CHINA-2',<br \/> \n '\/THE LAST CHANCE',<br \/> \n '\/ THE CURSE OF EL CHARRO',<br \/> \n '\/ NORTH AND SOUTH \/CH.2\/',<br \/> \n '\/ DEADLINE LAYOUT',<br \/> \n '\/YOU ARE DEAD',<br \/> \n '\/ MULTIPLIER SADNESS vol. 2 (ser. 3-4)',<br \/> \n '007 Legends [PS3 Russian Version]',<br \/> \n '007 Legends [PC, Jewel, Russian version]',<br \/> \n '10 YEARS LATER (BD) ',<br \/> \n '10 YEARS LATER (region) ',<br \/> \n 'THE 10 MOST POPULAR MOVIES OF THE XX CENTURY 10DVD (rem)',<br \/> \n '10 MOST POPULAR DOMESTIC COMEDIES OF THE XX CENTURY (BD) ',<br \/> \n '100 Best classical melodies (mp3-CD) (Digipack)',<br \/> \n '100 Best relax melodies (mp3-CD) (CD-Digipack)',<br \/> \n '100 Best romantic melodies (mp3-CD) (Digipack)',<br \/> \n ... <br\/>\n '1C: Audiobooks. Vagin I.O. Human Resource Management [PC, Digital version] ', <br \/> \n '1C: Audiobooks. Vagin I.O., Glushchay A. Teach yourself to laugh [PC, Digital version] ',<br \/> \n '1C: Audiobooks. Vagin I.O. Joy Management [PC, Digital] ',<br \/> \n '1C: Audiobooks. Vasil Bykov. Go and not come back [PC, Digital] ',<br \/> \n '1C: Audiobooks. Great businessmen or how money was made in the twentieth century (Digipack) ',<br \/> \n '1C: Audiobooks. Verne J. 20,000 Leagues Under the Sea ',<br \/> \n '1C: Audiobooks. Verne J. Children of Captain Grant ',<br \/> \n '1C: Audiobooks. Fun lessons for the little ones. ABC and Account (Jewel) ',<br \/> \n ....<br\/>\n '1C: Collection of toys \"Behind Enemy Lines\" (Steam version) [PC, Digital version]',<br\/>\n '1C: Collection of toys \"Behind Enemy Lines: Saboteurs 2\" [PC, Digital version]',<br\/>\n '1C: Collection of toys \"Behind Enemy Lines: Saboteurs\" [PC, Digital version]',<br\/>\n '1C: Collection of toys \"Vivisector. The beast inside\" [PC, Digital version]',<br\/>\n '1C: Collection of toys \"Luntik. Preparing for school\" [PC, Digital version]',<br\/>\n '1C: Collection of toys \"Blood Magic: Shadow Time\" [PC, Digital]',<br\/>\n '1C: Collection of toys \"Madagascar\"',<br\/>\n '1C: Collection of toys \"Well, wait! Issue 3. Song for a hare\" [PC, Digital version]',<br\/>\n '1C: Rex and the Wizards Toy Collection [PC, Digital]',<br\/>\n '1C: Rex and the Time Machine Toy Collection [PC, Digital]',,<br \/>\n <br\/>\n**SO NO consistency in the text data like that of item_category, there are small consistency, maybe we can use those with the help of NLP**","dddaad73":"# Data Exploration and feature extraction","0700c5f5":"## Item Category","822e37d9":"## Train","72cedc0e":"### Could use differernt feature generation like based on months of the year and add a categorical feature based on month, and mean based on month and other stuff"}}