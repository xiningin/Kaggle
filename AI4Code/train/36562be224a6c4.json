{"cell_type":{"bd7ee8f2":"code","37ed9e25":"code","b0441f2c":"code","8d6507d9":"code","b7e4c853":"code","aeb15178":"code","3f9b5684":"code","d6932f39":"code","16e202d0":"code","fe1a31bd":"code","20fb26c2":"code","ffb11120":"code","25500764":"code","9a657f18":"code","ee7b2886":"code","fa7ebd74":"code","134852df":"code","1cbecce3":"code","ecdf1374":"code","a4e112a8":"code","63edcc39":"code","8b3f3a0c":"code","dcb03e8f":"code","1ec9e44c":"markdown","b3919f08":"markdown","2103f5e9":"markdown","3753ee92":"markdown","ccd845f2":"markdown","003daf89":"markdown","60ef83dd":"markdown","efe220ed":"markdown","5a76c833":"markdown","2b70422f":"markdown","601da764":"markdown","29a286d6":"markdown","c7c70589":"markdown","fe2d1f52":"markdown","f142761b":"markdown","6f34f72e":"markdown"},"source":{"bd7ee8f2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\n\nfrom sklearn import preprocessing\n\nfrom yellowbrick.classifier import ConfusionMatrix\nfrom yellowbrick.classifier import ClassificationReport\nfrom yellowbrick.classifier import ROCAUC\nplt.style.use('ggplot')\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","37ed9e25":"from subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","b0441f2c":"dataset = pd.read_csv('..\/input\/titanic\/train.csv')\ndataset.head()","8d6507d9":"dataset.describe(include = \"all\")","b7e4c853":"dataset.shape","aeb15178":"dataset.isnull().sum(axis=0)","3f9b5684":"sns.countplot(dataset['Embarked'])","d6932f39":"# File missing values in embarked with S which is the most frequent item.\ndataset = dataset.fillna({\"Embarked\": \"S\"})","16e202d0":"## One hot encoding is used since no ordering is available for Sex (male, female) feature.\ndataset = pd.get_dummies(dataset, columns=['Sex'])\ndataset.head()","fe1a31bd":"## One hot encoding is used since no ordering is available for Sex (male, female) feature.\ndataset = pd.get_dummies(dataset, columns=['Embarked'])\ndataset.head()","20fb26c2":"feat_names = ['Pclass', 'Sex_male', 'Sex_female', 'Embarked_C', 'Embarked_Q', 'Embarked_S', 'Parch', 'SibSp', 'Fare']\ntarg_names = ['Dead (0)', 'Survived (1)'] # 0 - Dead, 1 - Survived\n\ntrain_class = dataset[['Survived']]\ntrain_feature = dataset[feat_names]\ntrain_feature.head()","ffb11120":"clf = DecisionTreeClassifier(random_state=0)\nscoring = {'acc': 'accuracy',\n           'prec_macro': 'precision_macro',\n           'rec_macro': 'recall_macro',\n           'f1_macro': 'f1_macro'}\nscores = cross_validate(clf, train_feature, train_class, cv=10, scoring=scoring)\n# print(scores.keys())\n\nprint ('Accuracy score : %.3f' % scores['test_acc'].mean())\nprint ('Precisoin score : %.3f' % scores['test_prec_macro'].mean())\nprint ('Recall score : %.3f' % scores['test_rec_macro'].mean())\nprint ('F1 score : %.3f' % scores['test_f1_macro'].mean())","25500764":"para_grid = {\n    'min_samples_split' : range(10,500,20),\n    'max_depth': range(1,20,2),\n    'criterion': (\"gini\", \"entropy\")\n}\n\nclf_tree = DecisionTreeClassifier()\nclf_cv = GridSearchCV(clf_tree,\n                   para_grid,\n                   scoring='accuracy',\n                   cv=5,\n                   n_jobs=-1)\nclf_cv.fit(train_feature,train_class)\n\nbest_parameters = clf_cv.best_params_\nprint(best_parameters)","9a657f18":"clf = clf_cv.best_estimator_\nscoring = {'acc': 'accuracy',\n           'prec_macro': 'precision_macro',\n           'rec_macro': 'recall_macro',\n           'f1_macro': 'f1_macro'}\nscores = cross_validate(clf, train_feature, train_class, cv=10, scoring=scoring)\n#print(scores.keys())\n\nprint ('Accuracy score : %.3f' % scores['test_acc'].mean())\nprint ('Precisoin score : %.3f' % scores['test_prec_macro'].mean())\nprint ('Recall score : %.3f' % scores['test_rec_macro'].mean())\nprint ('F1 score score : %.3f' % scores['test_f1_macro'].mean())","ee7b2886":"# Create a holdout sample for further testing\n# train_class, train_feature\nX_train, X_test, y_train, y_test = train_test_split(train_feature, train_class, test_size=0.33)\nprint (str(X_train.shape) +\",\"+ str(y_train.shape))\nprint (str(X_test.shape) +\",\"+ str(y_test.shape))","fa7ebd74":"clf2 = clf_cv.best_estimator_\nclf2.fit(X_train,y_train)\npredictions = clf2.predict(X_test)\nprint(metrics.classification_report(y_test,predictions, target_names=targ_names, digits=3))","134852df":"fig, ax = plt.subplots(figsize=(7,3))\nvisualizer = ClassificationReport(clf2, classes=targ_names, support=True, cmap='RdPu')\nvisualizer.score(X_test, y_test)\nfor label in visualizer.ax.texts:\n    label.set_size(14)\ng = visualizer.poof()","1cbecce3":"fig, ax = plt.subplots(figsize=(3,3))\ncm = ConfusionMatrix(clf2, classes=[0, 1], cmap='RdPu')\ncm.score(X_test, y_test)\nfor label in cm.ax.texts:\n    label.set_size(14)\ncm.poof()","ecdf1374":"modelviz = clf_cv.best_estimator_\nvisualizer = ROCAUC(modelviz, classes=[\"Dead\", \"Survived\"])\n\nvisualizer.fit(X_train, y_train)        # Fit the training data to the visualizer\nvisualizer.score(X_test, y_test)        # Evaluate the model on the test data\nvisualizer.show()                       # Finalize and render the figure","a4e112a8":"import graphviz\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\n\ndata = export_graphviz(clf,out_file=None,feature_names=feat_names,class_names=targ_names,   \n                         filled=True, rounded=True,  \n                         special_characters=True)\ngraph = graphviz.Source(data)\ngraph","63edcc39":"importances = clf.feature_importances_\nindices = np.argsort(importances)\n\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [feat_names[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","8b3f3a0c":"# Loading test dataset\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\n# Fit the model\nclf.fit(train_feature, train_class)\n\n# Replace missing Fare values with mean\nmeanFare = dataset['Fare'].mean()\ntest = test.fillna({\"Fare\": meanFare})\n# Categorical -> One hot encoding\ntest = pd.get_dummies(test, columns=['Sex'])\ntest = pd.get_dummies(test, columns=['Embarked'])\n\n#set ids as PassengerId and predict survival\nids = test['PassengerId']\ntest_feature = test[feat_names]\npredictions = clf.predict(test_feature)\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.head()","dcb03e8f":"output.to_csv('submission.csv', index=False)","1ec9e44c":"**Parameter tuning - gridSearchCV**","b3919f08":"**Confusion Matrix**","2103f5e9":"**ROC Curve**","3753ee92":"As you can see from the root of the decision tree Sex_female feaure gives the most information context to differentiate survived    \nand dead classes. This is clearly seen in the feature importance value as well. Sex_male and encoded Embarked feature adds very    \nsmall value for the prediction.","ccd845f2":"**Missing value identification\/ handling**","003daf89":"**Model evaluation with tuned parameters using cross validation**","60ef83dd":"**Classification report analysis**","efe220ed":"**Applying model - with default values**","5a76c833":"**Loading data file**","2b70422f":"**Draw the decision tree using graphviz**\n\nNote : Run this on yourown to see the graph since this cannot be published.","601da764":"**Creating submission file**","29a286d6":"**Understanding Feature Importance**","c7c70589":"**Workspace preparation**","fe2d1f52":"<img src=\"https:\/\/uci-seed-dataset.s3.ap-south-1.amazonaws.com\/AccuracyOnly.jpg\" width=\"400\">   \n\n\n<h1 style=\"color:red;font-size:40px;\">Hello kagglers,<\/h1>Accuracy is not the only thing you measure in a model. It typically depends on the problem. Most common example would be class skewness problem where one class appears in the dataset rarely in an application like Anomaly Detection. This simple kernel give you more insights into other evaluation methods along with few key concepts you need.\n\n<\/br><\/br>\n1. Missing value identification and handling.    \n1. Handling categorical variables (One-hot encoding).    \n1. Hyper Parameter tuning - GridSearchCV    \n1. Model evaluation (Accuracy, Precision, Recall, F1Score)   \n1. Classification report and confusion matrix analysis. \n1. ROC Curve for the trained clasifier\n1. Decision tree visualization using graphviz.    \n1. Understanding feature importance.","f142761b":"**Searching for folders inside input folder**","6f34f72e":"**Handling categorical variables**"}}