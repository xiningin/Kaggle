{"cell_type":{"ab77b6e9":"code","52109a50":"code","3fb22ad4":"code","eec6655f":"code","76aba7be":"code","5882813b":"code","72e6215a":"code","4078d079":"code","d72ea2fa":"markdown","1ac4bf04":"markdown","2965bba9":"markdown"},"source":{"ab77b6e9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport collections","52109a50":"images_path='..\/input\/shopee-product-matching\/train_images'\ntrain_df=pd.read_csv('..\/input\/shopee-product-matching\/train.csv')\ntest_df=pd.read_csv('..\/input\/shopee-product-matching\/test.csv')\nsample_sub=pd.read_csv('..\/input\/shopee-product-matching\/sample_submission.csv')\n\nprint('Shape of Train: ',train_df.shape)\ntrain_df.head()","3fb22ad4":"print('Number of Groups in our dataset: ',train_df['label_group'].nunique())","eec6655f":"#Lets take 2 examples\ndef plot_img(ids_,lbl_id):\n    images=[]\n    for id_ in ids_:\n        img=cv2.imread(os.path.join(images_path,id_))\n        img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n        images.append(img)\n    \n    \n    f, axes_ = plt.subplots(1,len(images))\n    plt.title(str(lbl_id))\n    for ix,img in enumerate(images):\n        axes_[ix].imshow(img)    ","76aba7be":"#Let's take 2 label ids\nids_0=train_df['image'][train_df['label_group']==3648931069]\nids_1=train_df['image'][train_df['label_group']==4093212188]\nids_2=train_df['image'][train_df['label_group']==2395904891]\n\n#Plot images belonging to these 2 groups\nplot_img(ids_0,3648931069)\nplot_img(ids_1,4093212188)\nplot_img(ids_2,2395904891)","5882813b":"#Now let's take a look at unique labels distribution\nlabels_uq=train_df['label_group'].unique()\nsns.displot(labels_uq,kde=True)","72e6215a":"#Easiest way of implement f1 score is through sklearn library\n#Also, let's take a look at difference between accuracy score and f1 score\nfrom sklearn.metrics import f1_score,accuracy_score\ntrue_y=[1,1,0,0,0,0,0,0,0,0,0,0]\npred_y=[0,0,0,0,0,0,0,0,0,0,0,0]\n\nprint('f1 Score: {:.3f}'.format(f1_score(true_y,pred_y,)))\n\ntrue_y=[1,1,0,0,0,0,0,0,0,0,0,0]\npred_y=[0,0,0,0,0,0,0,0,0,0,0,0]\n\nprint('Accuracy Score: {:.3f}'.format(accuracy_score(true_y,pred_y,)))","4078d079":"'''As we see, f1 score is only concerned about positive labels. So,if we look at our f1 score above we \ncan say that our model is performing poorly. \nBut if we look at accuracy score of 0.83, we may think that our model is performing good in predicting labels\neven though it is not able to predict positive class.\nThis is why we must not rely on accuracy only and evaluate our model using different methods.'''","d72ea2fa":"**Images Plotting**","1ac4bf04":"**F1 Score**\n* This metric is measure of precision and recall of the data.\n* precision-> Out of all predicted positive labels how many are actually positive.\n* recall-> Also called Senstivity and it means out of all positive labels how many were predicted positive.\n![](https:\/\/i.imgur.com\/qFmteYs.png)\n","2965bba9":"* In this dataset, we are given images,descriptions and their respective groups.\n* Images that have same group will have same type of product in them."}}