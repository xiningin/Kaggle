{"cell_type":{"73c08077":"code","088f6342":"code","bfa3c3fb":"code","f7de5576":"code","41ba2fc2":"code","c4add028":"code","cf961c48":"code","cc44a089":"code","9eb6c89d":"code","2ae33628":"code","0f91a84f":"code","c2359e10":"code","b4027871":"code","a5188b48":"code","a036e53d":"code","b6275c1a":"code","de156a7a":"code","88ee78d8":"code","ad88737f":"code","45f9f929":"code","80b06d73":"code","15b5ffc5":"code","d8e46fd3":"code","f95140d2":"code","84171e47":"code","0df9ace0":"code","617d7f48":"code","5e425d72":"code","8a1aa56d":"code","7b186b29":"code","72a4ab62":"code","d2cf61c2":"code","f19a1103":"code","404ba63e":"code","42c8d0c9":"code","81cfbf46":"code","1c8b7904":"code","cb7349d7":"code","0eb8c9a2":"code","7d3220ba":"code","4ebf6146":"code","f6a029ee":"code","da7c13d0":"code","09b349f5":"code","15510768":"code","9ab07da4":"code","b297dedb":"code","3614659d":"code","1ffceac4":"markdown","9b5e9785":"markdown"},"source":{"73c08077":"!pip install scikit_optimize","088f6342":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import KNNImputer\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import roc_auc_score, roc_curve, log_loss, classification_report, confusion_matrix\nfrom lightgbm import LGBMClassifier\nfrom imblearn.over_sampling import SMOTE","bfa3c3fb":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.impute import KNNImputer\nfrom sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, confusion_matrix, log_loss, plot_roc_curve, auc, precision_recall_curve\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import StratifiedKFold \nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n%matplotlib inline\nfrom xgboost import XGBClassifier\nfrom functools import partial\nfrom skopt import gp_minimize\nfrom skopt import space\nfrom skopt.plots import plot_convergence\n\nsns.set_style('whitegrid')","f7de5576":"data_train = pd.read_csv('\/content\/drive\/MyDrive\/train_s3TEQDk.csv')\ndata_test = pd.read_csv('\/content\/drive\/MyDrive\/test_mSzZ8RL.csv')","41ba2fc2":"data_train.head()","c4add028":"data_test.head()","cf961c48":"data_train.info()","cc44a089":"data_test.info()","9eb6c89d":"# I created a test_df from train set for validation.\n# I dropped the \"target\" values from test_X and I saved true target values as \"results\" dataframe.\n\ntest_df = data_train.iloc[:10000, :]\nresults = test_df[[\"Is_Lead\",\"ID\"]]\n\n\ntest_X = test_df.drop (\"Is_Lead\", axis=1)\ntest_X","2ae33628":"# I merged test_X, train and test datasets for feature transformation.\ndf = pd.concat ([test_X, data_train.iloc[10000:, :]], axis=0).reset_index (drop=True)\ndf.info()","0f91a84f":"# I noticed that there is an imbalanced dataset problem.\ndf[\"Is_Lead\"].value_counts() \/ len(df)","c2359e10":"def grab_col_names(dataframe, cat_th=10, car_th=20):\n    # Grabs the columns which is categorical, numerical, categorical but cardinal and numerical but categorical.\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique () < cat_th and\n                   dataframe[col].dtypes != \"O\" or (\"id\" in col)]\n\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique () > car_th and\n                   dataframe[col].dtypes == \"O\"]\n\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print (f\"Observations: {dataframe.shape[0]}\")\n    print (f\"Variables: {dataframe.shape[1]}\")\n    print (f'cat_cols: {len (cat_cols)}')\n    print (f'num_cols: {len (num_cols)}')\n    print (f'cat_but_car: {len (cat_but_car)}')\n    print (f'num_but_cat: {len (num_but_cat)}')\n\n    return cat_cols, cat_but_car, num_cols, num_but_cat","b4027871":"categorical_cols, categorical_but_cardinal, numeric_cols, numeric_but_categorical = grab_col_names(df)","a5188b48":"print(\"Categorical columns : {}\".format(categorical_cols))\nprint(\"=\"*80)\nprint(\"Cat_But_Car columns : {}\".format(categorical_but_cardinal))\nprint(\"=\"*80)\nprint(\"Numeric columns : {}\".format(numeric_cols))\nprint(\"=\"*80)\nprint(\"Num_But_Cat columns : {}\".format(numeric_but_categorical))","a036e53d":"def cat_summary(dataframe, col_name, plot=False):\n    # Shows summary of categorical columns.\n    df = pd.DataFrame ({col_name: dataframe[col_name].value_counts (),\n                          \"Ratio\": 100 * dataframe[col_name].value_counts () \/ len (dataframe)})\n    print(df)\n\n    if plot:\n        plt.figure(figsize=(7,7))\n        plt.pie (df[\"Ratio\"], labels=df.index, \n                labeldistance=1.15, wedgeprops = { 'linewidth' : 1, 'edgecolor' : 'white' }, autopct = \"%1.1f%%\",\n                pctdistance=0.85, textprops={'fontsize': 10})\n     \n        #draw circle\n        centre_circle = plt.Circle((0,0),0.70,fc='white')\n        fig = plt.gcf()\n        fig.gca().add_artist(centre_circle)\n        plt.show();\n        print(\"=\"*50)","b6275c1a":"# Number of unique class of each feature.\ndf.nunique()","de156a7a":"graph_cols = [col for col in categorical_cols if df[col].nunique() < 40]\nfor col in graph_cols:\n    cat_summary (df, col, plot=True)","88ee78d8":"def target_summary_with_cat(dataframe, target, categorical_col, plot=False):\n    # Shows some of descriptive statistical metrics of target according to each categorical class.\n    df = pd.DataFrame({\"TARGET_MEAN\": dataframe.groupby(categorical_col)[target].mean(),\n                        \"COUNT\": dataframe.groupby(categorical_col)[target].count()})\n    print(df)\n    if plot==True:\n        sns.barplot(x=df.index, y=df[\"TARGET_MEAN\"])\n        plt.xticks(rotation=45)\n        plt.xlabel(df.index.name.upper())\n        plt.show();\n        print(\"=\"*50)","ad88737f":"cats = [col for col in df.columns if (col in categorical_cols + categorical_but_cardinal) & (col not in [\"ID\",\"Is_Lead\"]) & (df[col].nunique() < 20)]\nfor col in cats:\n    target_summary_with_cat(df, \"Is_Lead\", col, plot=True)","45f9f929":"def target_summary_with_num(dataframe, target, numerical_col):\n    # Shows average of target according to numerical columns.\n    df = dataframe.groupby (target).agg ({numerical_col: \"mean\"}).astype('int64')\n    print(df)\n    print(\"=\"*50)","80b06d73":"for col in numeric_cols:\n    target_summary_with_num(df, \"Is_Lead\", col)","15b5ffc5":"def data_prep(dataframe):\n    # Labeling \"Gender\" feature.\n    dataframe.loc[dataframe[\"Gender\"] == \"Male\", \"NEW_GENDER\"] = 0\n    dataframe.loc[dataframe[\"Gender\"] == \"Female\", \"NEW_GENDER\"] = 1\n\n    # Labeling \"Occupation\" feature\n    dataframe.loc[dataframe[\"Occupation\"] == \"Entrepreneur\", \"OCCUPATION\"] = 0\n    dataframe.loc[dataframe[\"Occupation\"] == \"Other\", \"OCCUPATION\"] = 1\n    dataframe.loc[dataframe[\"Occupation\"] == \"Salaried\", \"OCCUPATION\"] = 2\n    dataframe.loc[dataframe[\"Occupation\"] == \"Self_Employed\", \"OCCUPATION\"] = 3\n\n    # Labeling \"Channel_Code\" feature\n    dataframe.loc[dataframe[\"Channel_Code\"] == \"X1\", \"NEW_CHANNEL_CODE\"] = 0\n    dataframe.loc[dataframe[\"Channel_Code\"] == \"X2\", \"NEW_CHANNEL_CODE\"] = 1\n    dataframe.loc[dataframe[\"Channel_Code\"] == \"X3\", \"NEW_CHANNEL_CODE\"] = 2\n    dataframe.loc[dataframe[\"Channel_Code\"] == \"X4\", \"NEW_CHANNEL_CODE\"] = 3\n\n    # Labeling \"Credit_Product\" feature.\n    dataframe.loc[dataframe[\"Credit_Product\"] == \"No\", \"CREDIT_PROD\"] = 0\n    dataframe.loc[dataframe[\"Credit_Product\"] == \"Yes\", \"CREDIT_PROD\"] = 1\n\n    # Labeling \"Is_Active\" feature.\n    dataframe.loc[dataframe[\"Is_Active\"] == \"No\", \"ACTIVE\"] = 0\n    dataframe.loc[dataframe[\"Is_Active\"] == \"Yes\", \"ACTIVE\"] = 1\n\n    new_df = dataframe.copy()\n\n    del_cols = [\"Gender\", \"Occupation\", \"Credit_Product\", \"Is_Active\", \"Channel_Code\"]\n\n    new_df.drop (del_cols, axis=1, inplace=True)\n\n    return new_df\n","d8e46fd3":"df_2 = data_prep(df)\ndf_2.head()","f95140d2":"multiclass_cat_cols = [col for col in df_2.columns if (df_2[col].nunique () > 2) & (df_2[col].dtype == \"O\") & (col != \"ID\")]\nmulticlass_cat_cols","84171e47":"# Using Label Encoding for all multiclass categorical columns before filling the missing values.\ndf_2[multiclass_cat_cols] = df_2[multiclass_cat_cols].apply (lambda series: pd.Series (\n    LabelEncoder ().fit_transform (series[series.notnull ()]),\n    index=series[series.notnull ()].index))","0df9ace0":"df_2.head()","617d7f48":"# Group Age and Vintage column to convert to categorical feature\ndf_2['age'] = df_2['Age'].apply(lambda x: 10*np.floor(x\/10))\ndf_2['vintage'] = df_2['Vintage'].apply(lambda x: 10*np.floor(x\/10))\ndf_2.drop(['Age','Vintage'],axis=1,inplace=True)\ndf_2.head()","5e425d72":"import seaborn as sns\n#get correlations of each features in dataset\ncorrmat = df_2.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(df_2[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","8a1aa56d":"df_2.isnull().sum()","7b186b29":"df_2['CREDIT_PROD']=np.where(df_2['CREDIT_PROD'].isnull(),1,0)\nfrequent=df_2['CREDIT_PROD'].mode()[0]\ndf_2['CREDIT_PROD'].fillna(frequent,inplace=True)","72a4ab62":"df_2.isnull().sum()","d2cf61c2":"# Dividing dataset into three parts as test_data, subb_data and train_data.\ntest_data = df_2.loc[df_2[\"ID\"].isin (results[\"ID\"])].reset_index(drop=True)\nsubb_data = df_2.loc[df_2[\"ID\"].isin (data_test[\"ID\"])].reset_index(drop=True)\ntrain_data = df_2[df_2.notnull ().all (axis=1)].reset_index(drop=True)","f19a1103":"# Using train_data for setting up lgbm model.\nX = train_data.drop ([\"ID\", \"Is_Lead\"], axis=1)\ny = train_data['Is_Lead']","404ba63e":"# Imbalanced dataset.\ny.value_counts () \/ len(y)","42c8d0c9":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2 ,random_state = 42)\n\nclf = XGBClassifier()\n\nclf.fit(X_train, y_train)\n\ny_train_pred = clf.predict_proba(X_train)\ny_train_pred_pos = y_train_pred[:,1]\n\ny_val_pred = clf.predict_proba(X_val)\ny_val_pred_pos = y_val_pred[:,1]\n\nauc_train = roc_auc_score(y_train, y_train_pred_pos)\nauc_test = roc_auc_score(y_val, y_val_pred_pos)\n\nprint(f\"Train AUC Score {auc_train}\")\nprint(f\"Test AUC Score {auc_test}\")\n\nfpr, tpr, _ = roc_curve(y_val, y_val_pred_pos)","81cfbf46":"from sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\ndef cross_val(X, y, model, params, folds=9):\n\n    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=21)\n    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n        print(f\"Fold: {fold}\")\n        x_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n        x_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n\n        alg = model(**params)\n        alg.fit(x_train, y_train,\n                eval_set=[(x_test, y_test)],\n                early_stopping_rounds=100,\n                verbose=400)\n\n        pred = alg.predict_proba(x_test)[:, 1]\n        roc_score = roc_auc_score(y_test, pred)\n        print(f\"roc_auc_score: {roc_score}\")\n        print(\"-\"*50)\n    \n    return alg","1c8b7904":"lgb_params= {'learning_rate': 0.045, \n             'n_estimators': 20000, \n             'max_bin': 94,\n             'num_leaves': 10, \n             'max_depth': 27, \n             'reg_alpha': 8.457, \n             'reg_lambda': 6.853, \n             'subsample': 0.749}","cb7349d7":"from lightgbm import LGBMClassifier\nlgb_model = cross_val(X, y, LGBMClassifier, lgb_params)","0eb8c9a2":"xgb_params= {'n_estimators': 20000, \n             'max_depth': 6, \n             'learning_rate': 0.0201, \n             'reg_lambda': 29.326, \n             'subsample': 0.818, \n             'colsample_bytree': 0.235, \n             'colsample_bynode': 0.820, \n             'colsample_bylevel': 0.453}","7d3220ba":"from xgboost import XGBClassifier\nxgb_model = cross_val(X, y, XGBClassifier, xgb_params)","4ebf6146":"cat_params= {'n_estimators': 20000, \n                  'depth': 4, \n                  'learning_rate': 0.023, \n                  'colsample_bylevel': 0.655, \n                  'bagging_temperature': 0.921, \n                  'l2_leaf_reg': 10.133}","f6a029ee":"!pip install catboost","da7c13d0":"from catboost import CatBoostClassifier\ncat_model = cross_val(X, y, CatBoostClassifier, cat_params)","09b349f5":"from sklearn.tree import DecisionTreeClassifier\n\nDT = DecisionTreeClassifier()\n\nDT.fit(X_train,y_train)\n\nDT.score(X_train,y_train)","15510768":"from sklearn.ensemble import RandomForestClassifier\n\nRF = RandomForestClassifier()\n\nRF.fit(X_train,y_train)\n\nRF.score(X_train,y_train)","9ab07da4":"DT.score(X_val,y_val)","b297dedb":"df = {'ID':test_1['ID'],'Is_Lead':values}","3614659d":"from google.colab import files\n\ndf.to_csv('submit.csv',index=0)\nfiles.download('submit.csv')","1ffceac4":"###Approaching Categorical Features\nCategorical variables\/features are any feature type can be classified into two major types:\n\n**Nominal**\n\n**Ordinal**\n\nNominal variables are variables that have two or more categories which do not have any kind of order associated with them. For example, if gender is classified into two groups, i.e. male and female, it can be considered as a nominal variable.\n\n\nOrdinal variables on the other hand, have \u201clevels\u201d or categories with a particular order associated with them. For example, an ordinal categorical variable can be a feature with three different levels: low, medium and high.  Order is important.\n\n\n**List of ordinal variables in this data**\n* Age\n* Occupation\n* Vintage","9b5e9785":"Features\n* ID: Unique ID for customer\n* Gender: Gender of Customer\n* Age: Age of Customer\n*Region Code: Region Code based on location\n*Occupation: Work nature\n*Channel Code: Code of Channel for customer\n*Vintage: Period of Vintage in years\n*Credit_Product: Has Credit Product or not\n*Avg_Account_Balance: Average Amount in Account\n*Is_Active: Active customer or not\n*Is_Lead: (Target) Probability of Customer showing interest (class 1)\n"}}