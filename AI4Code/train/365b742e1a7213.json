{"cell_type":{"9626b66c":"code","097e8708":"code","98347e49":"code","6491a181":"code","05fd007d":"code","d82f2b97":"code","20d58663":"code","d564e03a":"code","3b90df1c":"code","8c12819a":"code","02022ffa":"code","4820905d":"code","54af0f8b":"code","361584fb":"markdown","36271648":"markdown","1378ab1c":"markdown","b3c0c92a":"markdown","dc1e3509":"markdown","26b950b0":"markdown","8f35e316":"markdown","404217bc":"markdown","8dd8231c":"markdown","a866b4a5":"markdown","bbebf373":"markdown","d7129ba4":"markdown","8eb6be66":"markdown","f623f2d9":"markdown","be568b7a":"markdown","1fba0a52":"markdown","1825950f":"markdown","f851a8d9":"markdown","fb75bc9a":"markdown","40ecfb98":"markdown"},"source":{"9626b66c":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\nlabels = train.columns.drop(['id', 'target'])\ntarget = train['target']\n\nprint('Train rows:', train.shape[0])\nprint('Test rows:', test.shape[0])\nprint('Data columns:', test.columns.drop(['id']).shape[0])","097e8708":"import matplotlib.pyplot as plt\n\nd_names = ('train.csv', 'test.csv')\ny_pos = range(len(d_names))\n \nplt.bar(\n    y_pos, \n    (train.shape[0], test.shape[0]), \n    align='center', \n    alpha=0.8\n)\nplt.xticks(y_pos, d_names)\nplt.ylabel('Number of rows') \nplt.title('\ud83d\ude31 Wow!')\nplt.show()","98347e49":"from sklearn.linear_model import LogisticRegression\n\nm = LogisticRegression(\n    penalty='l1',\n    C=0.1\n)\nm.fit(train[labels], target)\nm.predict_proba(test[labels])[:,1]","6491a181":"from xgboost import XGBClassifier\n\nm = XGBClassifier(\n    max_depth=2,\n    gamma=2,\n    eta=0.8,\n    reg_alpha=0.5,\n    reg_lambda=0.5\n)\nm.fit(train[labels], target)\nm.predict_proba(test[labels])[:,1]","05fd007d":"from sklearn.ensemble import IsolationForest\n\nisf = IsolationForest(n_jobs=-1, random_state=1)\nisf.fit(train[labels], train['target'])\n\nprint(isf.score_samples(train[labels]))","d82f2b97":"isf.predict(train[labels])","20d58663":"from sklearn.ensemble import ExtraTreesClassifier\n\nTOP_FEATURES = 15\n\nforest = ExtraTreesClassifier(n_estimators=250, max_depth=5, random_state=1)\nforest.fit(train[labels], train['target'])\n\nimportances = forest.feature_importances_\nstd = np.std(\n    [tree.feature_importances_ for tree in forest.estimators_],\n    axis=0\n)\nindices = np.argsort(importances)[::-1]\nindices = indices[:TOP_FEATURES]\n\nprint('Top features:')\nfor f in range(TOP_FEATURES):\n    print('%d. feature %d (%f)' % (f + 1, indices[f], importances[indices[f]]))","d564e03a":"plt.figure()\nplt.title('Top feature importances')\nplt.bar(\n    range(TOP_FEATURES), \n    importances[indices],\n    yerr=std[indices], \n)\nplt.xticks(range(TOP_FEATURES), indices)\nplt.show()","3b90df1c":"from sklearn.feature_selection import RFE\n\nrfe = RFE(XGBClassifier(n_jobs=-1, random_state=1))\n\nrfe.fit(train[labels], train['target'])\n\nprint('Selected features:')\nprint(labels[rfe.support_].tolist())","8c12819a":"train['target'].value_counts().plot(kind='bar', title='Count (target)');","02022ffa":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(ratio='minority', n_jobs=-1)\nX_sm, y_sm = smote.fit_resample(train[labels], train['target'])\n\ndf = pd.DataFrame(X_sm, columns=labels)\ndf['target'] = y_sm\n\ndf['target'].value_counts().plot(kind='bar', title='Count (target)');","4820905d":"models = [\n    LogisticRegression(),\n    XGBClassifier(max_depth=2)\n]\n\npreds = pd.DataFrame()\nfor i, m in enumerate(models):\n    m.fit(train[labels], target),\n    preds[i] = m.predict_proba(test[labels])[:,1]\n\nweights = [1, 0.3]\npreds['weighted_pred'] = (preds * weights).sum(axis=1) \/ sum(weights)\npreds.head()","54af0f8b":"from mlxtend.classifier import StackingClassifier\n\nm = StackingClassifier(\n    classifiers=[\n        LogisticRegression(),\n        XGBClassifier(max_depth=2)\n    ],\n    use_probas=True,\n    meta_classifier=LogisticRegression()\n)\n\nm.fit(train[labels], target),\npreds['stack_pred'] = m.predict_proba(test[labels])[:,1]\npreds.head()","361584fb":"![](https:\/\/raw.githubusercontent.com\/rafjaa\/curso-mineracao-de-dados-aplicada\/master\/img\/kernel_overfitting\/creativity.jpg)\n\n# Dealing with very small datasets\n\nIn this kernel we will see some techniques to handle very small datasets, where the main challenge is to avoid overfitting.\n\n1. <a href=\"#t1\">Why small datasets lead to overfitting?<\/a>\n2. <a href=\"#t2\">Use simple models<\/a>\n3. <a href=\"#t3\">Beware the outliers<\/a>\n4. <a href=\"#t4\">Select the features<\/a>\n5. <a href=\"#t5\">Balance the dataset with synthetic samples (SMOTE)<\/a>\n6. <a href=\"#t6\">Combine models for the final submission<\/a>\n7. <a href=\"#t7\">References<\/a>","36271648":"<h1 id=\"t4\">4. Select the features<\/h1>\n\nWhile removing outliers consists of deleting rows from the dataset, feature selection consists of deleting columns that do not contribute to the prediction. There is a wide variety of methods, such as analysis of its correlation with the target, importance analysis and recursive elimination.\n\nLet's see an example of how to identify the most relevant features for classification using a tree model:","1378ab1c":"<h1 id=\"t7\">References<\/h1>\n\n**Overfitting:**\n* IRIC's Bioinformatics Platform. Overfitting and Regularization. https:\/\/bioinfo.iric.ca\/overfitting-and-regularization\/\n* PATNI, Shubham. Generalisation, Training-Validation & Test data. Machine Learning- Part 6. https:\/\/medium.com\/@shubhapatnim86\/generalisation-training-validation-test-data-machine-learning-part-6-1de9dbb7d3d5\n* Rants on Machine Learning. What to do with \u201csmall\u201d data? https:\/\/medium.com\/rants-on-machine-learning\/what-to-do-with-small-data-d253254d1a89\n* Towards Data Science. Breaking the curse of small datasets in Machine Learning: Part 1. https:\/\/towardsdatascience.com\/breaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d\n\n**Models:**\n* scikit-learn. LogisticRegression. https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html\n* Towards Data Science. Logistic Regression\u200a\u2014\u200aDetailed Overview. https:\/\/towardsdatascience.com\/logistic-regression-detailed-overview-46c4da4303bc\n* XGBoost. XGBoost Parameters. https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\n\n**Outlier detection:**\n* Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \u201cIsolation forest.\u201d Data Mining, 2008. ICDM\u201808. Eighth IEEE International Conference on.\n* Machine Learning Valencia. Anomaly Detection. Valencian Summer School 2015. https:\/\/www.slideshare.net\/mlvlc\/l14-anomaly-detection\n* scikit-learn. Novelty and Outlier Detection. https:\/\/scikit-learn.org\/stable\/modules\/outlier_detection.html\n* VADALI, SaiGayatri.Day 7: Data cleaning\u200a\u2014\u200aAll you need to know about it. https:\/\/becominghuman.ai\/day-7-data-cleaning-all-that-you-need-to-know-about-it-23b05738abe7\n\n**Feature selection:**\n* scikit-learn. Feature selection. https:\/\/scikit-learn.org\/stable\/modules\/feature_selection.html\n\n**SMOTE**\n* ALENCAR, Rafael. Resampling strategies for imbalanced datasets. https:\/\/www.kaggle.com\/rafjaa\/resampling-strategies-for-imbalanced-datasets\n* Chawla, Nitesh V., et al. \"SMOTE: synthetic minority over-sampling technique.\" Journal of artificial intelligence research 16 (2002)\n* imbalanced-learn. https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/generated\/imblearn.over_sampling.SMOTE.html\n\n**Stacking**\n* mlxtend. StackingClassifier. http:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/StackingClassifier\/\n* Tang, J., S. Alelyani, and H. Liu. \"Data Classification: Algorithms and Applications.\" Data Mining and Knowledge Discovery Series, CRC Press (2015): pp. 498-500.\n* Wolpert, David H. \"Stacked generalization.\" Neural networks 5.2 (1992): 241-259.","b3c0c92a":"Let's load the data from the **Don't Overfit! II** competition:","dc1e3509":"<h1 id=\"t3\">3. Beware the outliers<\/h1>\n\nOutliers are extreme values that fall a long way outside of the other observations. In a small dataset, the impact of an outlier can be much greater, since it will have a heavy weight for the model:\n\n![](https:\/\/raw.githubusercontent.com\/rafjaa\/curso-mineracao-de-dados-aplicada\/master\/img\/kernel_overfitting\/outlier.png)\n<center><strong>Source:<\/strong> <a href=\"https:\/\/becominghuman.ai\/day-7-data-cleaning-all-that-you-need-to-know-about-it-23b05738abe7\">https:\/\/becominghuman.ai\/day-7-data-cleaning-all-that-you-need-to-know-about-it-23b05738abe7<\/a><\/center>\n\n<br>\n\nThe scikit-learn library has several implementations of outliers detection techniques:\n\n![](https:\/\/raw.githubusercontent.com\/rafjaa\/curso-mineracao-de-dados-aplicada\/master\/img\/kernel_overfitting\/outl_detection.png)\n<center><strong>Source:<\/strong> <a href=\"https:\/\/scikit-learn.org\/stable\/auto_examples\/plot_anomaly_comparison.html\">https:\/\/scikit-learn.org\/stable\/auto_examples\/plot_anomaly_comparison.html<\/a><\/center>\n\n<br>\n\nLet's do an experiment with the <code>IsolationForest<\/code> technique, which uses random forests for efficient detection of outliers in high-dimensional datasets. From scikit-learn documentation:\n\n> The IsolationForest \u2018isolates\u2019 observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.\n> \n> Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node.\n> \n> This path length, averaged over a forest of such random trees, is a measure of normality and our decision function.\n\n![](https:\/\/raw.githubusercontent.com\/rafjaa\/curso-mineracao-de-dados-aplicada\/master\/img\/kernel_overfitting\/isolation_forest.jpg)\n<center><strong>Source:<\/strong> <a href=\"https:\/\/www.slideshare.net\/mlvlc\/l14-anomaly-detection\">https:\/\/www.slideshare.net\/mlvlc\/l14-anomaly-detection<\/a><\/center>\n\n<br>\n\nLet's score each sample of our dataset using a isolation forest (the lower, the more abnormal):","26b950b0":"<h1 id=\"t6\">6. Combine models for the final submission<\/h1>\n\nCombine the prediction of several models or the same model with different values of hyperparameters reduces variance and enhances generalization.\n\n![](https:\/\/raw.githubusercontent.com\/rafjaa\/curso-mineracao-de-dados-aplicada\/master\/img\/kernel_overfitting\/combine.jpg)\n<center><strong>Source:<\/strong> <a href=\"https:\/\/medium.com\/rants-on-machine-learning\/what-to-do-with-small-data-d253254d1a89\">https:\/\/medium.com\/rants-on-machine-learning\/what-to-do-with-small-data-d253254d1a89<\/a><\/center>\n\n<br>\n\nOften, combining weak models that are poorly correlated with each other can lead to superior results than a strong individual model. There are several ways to do this. The simplest is to perform a weighted average of the various predictions:","8f35e316":"**Caution: for a very small dataset, very robust local validation is required to determine whether or not a feature contributes to the final solution. **","404217bc":"<h1 id=\"t2\">2. Use simple models<\/h1>\n\nAs we saw, few samples allow several models to \"explain\" the data. By thinking graphically, complex models can make crazy curves that will almost perfectly explain the training data, but possibly will perform poorly over the test data.\n\n![](https:\/\/raw.githubusercontent.com\/rafjaa\/curso-mineracao-de-dados-aplicada\/master\/img\/kernel_overfitting\/overfitting_curve.png)\n<center><strong>Source:<\/strong> <a href=\"https:\/\/bioinfo.iric.ca\/overfitting-and-regularization\/\">https:\/\/bioinfo.iric.ca\/overfitting-and-regularization\/<\/a><\/center>\n\n<br>\n\nAvoid complex models with many parameters, thus limiting their generalization and possibility of overfitting. Regularization techniques like L1 and L2 also help make the models more conservative. For tree-based models, reducing their maximum depth also limits the model's ability to see patterns and non-existent relationships.\n\nA good model to get started is Logistic Regression, a linear model used when the dependent variable (target) is categorical (classification tasks).\n\n![](https:\/\/raw.githubusercontent.com\/rafjaa\/curso-mineracao-de-dados-aplicada\/master\/img\/kernel_overfitting\/logisticregression.png)\n<center><strong>Source:<\/strong> <a href=\"https:\/\/towardsdatascience.com\/logistic-regression-detailed-overview-46c4da4303bc\">https:\/\/towardsdatascience.com\/logistic-regression-detailed-overview-46c4da4303bc<\/a><\/center>\n\n<br>\n\nIn this model, we can control the regularization by means of the <code>penalty<\/code> and <code>C<\/code> parameters (inverse of regularization strength - smaller values specify stronger regularization) to deal with the overfitting:","8dd8231c":"**Beware, with few samples, it becomes a challenge to adjust the algorithms to correctly identify the outliers.**","a866b4a5":"We can plot a chart of the importances:","bbebf373":"![](https:\/\/raw.githubusercontent.com\/rafjaa\/curso-mineracao-de-dados-aplicada\/master\/img\/kernel_overfitting\/mem2.jpg)","d7129ba4":"<h1 id=\"t5\">5. Balance the dataset with synthetic samples (SMOTE)<\/h1>\n\n<br>\n\nLet's look at the distribution of target values:","8eb6be66":"A visual inspection of the number of rows:","f623f2d9":"For tree-based models like XGBoost, we can control the overfitting by tuning a series of parameters:\n\n- Restricting the maximum depth of trees via <code>max_depth<\/code> (low values)\n- Making the model more conservative via <code>gamma<\/code> and <code>eta<\/code> (high values)\n- L1 and L2 regularization via <code>reg_alpha<\/code> and <code>reg_lambda<\/code> (high values)","be568b7a":"In addition to being extremely small, our training dataset has the unbalanced <code>target<\/code> binary variable, which can undermine some models' predictability. We will perform an oversampling, which consists of creating new samples to increase the <code>0<\/code> minority class. For this we will use the SMOTE technique.\n\nSMOTE (Synthetic Minority Oversampling TEchnique) consists of synthesizing elements for the minority class, based on those that already exist. It works randomly picingk a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.\n\n![](https:\/\/raw.githubusercontent.com\/rafjaa\/machine_learning_fecib\/master\/src\/static\/img\/smote.png)\n<center><strong>Source:<\/strong> <a href=\"https:\/\/www.kaggle.com\/rafjaa\/resampling-strategies-for-imbalanced-datasets\">https:\/\/www.kaggle.com\/rafjaa\/resampling-strategies-for-imbalanced-datasets<\/a><\/center>\n\n<br>\n\nWe'll use the SMOTE implementation from the library <code>imbalanced-learn<\/code>, with the parameter <code>ratio='minority'<\/code> to resample the minority class:","1fba0a52":"Another more sophisticated way of combining predictions is the use of a meta-classifier, which receives as input the prediction of other classifiers, and performs the final predict. From the <code>mlxtend<\/code> library documentation:\n\n> Stacking is an ensemble learning technique to combine multiple classification models via a meta-classifier. The individual classification models are trained based on the complete training set; then, the meta-classifier is fitted based on the outputs -- meta-features -- of the individual classification models in the ensemble. The meta-classifier can either be trained on the predicted class labels or probabilities from the ensemble.\n\n![](https:\/\/raw.githubusercontent.com\/rafjaa\/curso-mineracao-de-dados-aplicada\/master\/img\/kernel_overfitting\/stack.png)\n<center><strong>Source:<\/strong> <a href=\"http:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/StackingClassifier\/\">http:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/StackingClassifier\/<\/a><\/center>\n\n<br>\n\nAs an example, we will use the <code>StackingClassifier<\/code> of the <code>mlxtend<\/code> library:","1825950f":"**Note that because it is a nonlinear model with several parameters, it tends to be more prone to overfitting than a simple linear model such as logistic regression.**","f851a8d9":"<h1 id=\"t1\">1. Why small datasets lead to overfitting?<\/h1>\n\nThe goal of a machine learning model is to **generalize** patterns in training data so that you can correctly predict new data that has never been presented to the model. Overfitting occurs when a model adjusts excessively to the training data, seeing patterns that do not exist, and consequently performing poorly in predicting new data:\n\n![](https:\/\/raw.githubusercontent.com\/rafjaa\/curso-mineracao-de-dados-aplicada\/master\/img\/kernel_overfitting\/under_over.png)\n<center><strong>Source:<\/strong> <a href=\"https:\/\/medium.com\/@shubhapatnim86\/generalisation-training-validation-test-data-machine-learning-part-6-1de9dbb7d3d5\">https:\/\/medium.com\/@shubhapatnim86\/generalisation-training-validation-test-data-machine-learning-part-6-1de9dbb7d3d5<\/a><\/center>\n\n<br>\n\nThe fewer samples for training, the more models can fit our data. In an extreme example (a), for just one training point, any model will be able to \"explain\" it, however simple or complex the model may be. As we get to have more samples (b, c), fewer models are able to explain them:\n\n![](https:\/\/raw.githubusercontent.com\/rafjaa\/curso-mineracao-de-dados-aplicada\/master\/img\/kernel_overfitting\/few_samples.png)\n<center><strong>Source:<\/strong> <a href=\"https:\/\/towardsdatascience.com\/breaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d\">https:\/\/towardsdatascience.com\/breaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d<\/a><\/center>\n\n<br>\n\nThat way, for a dataset with only 250 samples, we need to be very careful not to be fooled by overfitting. In this kernel we will see some tips that can help.\n\n![](https:\/\/raw.githubusercontent.com\/rafjaa\/curso-mineracao-de-dados-aplicada\/master\/img\/kernel_overfitting\/meme1.jpeg)","fb75bc9a":"In the next example, we will try to identify the most important features by successively training a model and recursively eliminating those that do not contribute to a good final solution (according to the selected model).\n\nWe will use the recursive feature elimination <code>RFE<\/code> from the scikit-learn library and the <code>XGBClassifier<\/code> model:","40ecfb98":"Now, let's predict the outliers (1 for inliers, -1 for outliers):"}}