{"cell_type":{"cb7d7206":"code","ef01f551":"code","44c806bc":"code","57957546":"code","adb0f2ad":"code","a9505e40":"code","60bb2016":"code","bfc13c2f":"code","d3b64512":"code","aa67374e":"code","4c66a607":"code","d4b5733f":"code","14e5298c":"code","3a948f8c":"code","52a62f08":"code","ec3c88e3":"code","bdaab6df":"code","1eae70f5":"code","97869be5":"code","5a5a0aef":"code","9b7e15dc":"code","9e936cdf":"code","f02c2631":"code","ffe759e7":"code","54030f9d":"code","b7e545d5":"code","beb3c400":"code","326c98eb":"code","3fce197d":"code","a511d7c1":"code","46e443cd":"markdown","50b8e248":"markdown","b3ebc33c":"markdown","aaae5a75":"markdown","5cc340a0":"markdown","3e5c07aa":"markdown","887120e1":"markdown","e2451be3":"markdown","da7d253a":"markdown","ec4a2c5c":"markdown","f0569ffd":"markdown","c337910b":"markdown","593e0070":"markdown","478e69bf":"markdown","095811c0":"markdown","ddb45d9d":"markdown","31544248":"markdown","4d0b6e50":"markdown","bdc4faf5":"markdown","3c986f58":"markdown","49bb8fa4":"markdown","6b4ef940":"markdown","c0ada871":"markdown","80e0c763":"markdown"},"source":{"cb7d7206":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.multiclass import OneVsOneClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import VotingClassifier\n\nimport seaborn as sn\n\nhbeat_signals = pd.read_csv(\"..\/input\/mitbih-arrhythmia-database-de-chazal-class-labels\/DS1_signals.csv\", header=None)\nhbeat_labels = pd.read_csv(\"..\/input\/mitbih-arrhythmia-database-de-chazal-class-labels\/\/DS1_labels.csv\", header=None)\n\nprint(\"+\"*50)\nprint(\"Signals Info:\")\nprint(\"+\"*50)\nprint(hbeat_signals.info())\nprint(\"+\"*50)\nprint(\"Labels Info:\")\nprint(\"+\"*50)\nprint(hbeat_labels.info())\nprint(\"+\"*50)","ef01f551":"hbeat_signals.head()","44c806bc":"# Collect data of different hheartbeats in different lists\n#class 0\ncl_0_idx = hbeat_labels[hbeat_labels[0] == 0].index.values\ncl_N = hbeat_signals.iloc[cl_0_idx]\n#class 1\ncl_1_idx = hbeat_labels[hbeat_labels[0] == 1].index.values\ncl_S = hbeat_signals.iloc[cl_1_idx]\n#class 2\ncl_2_idx = hbeat_labels[hbeat_labels[0] == 2].index.values\ncl_V = hbeat_signals.iloc[cl_2_idx]\n#class 3\ncl_3_idx = hbeat_labels[hbeat_labels[0] == 3].index.values\ncl_F = hbeat_signals.iloc[cl_3_idx]\n\n# make plots for the different hbeat classes\nplt.subplot(221)\nfor n in range(3):\n    cl_N.iloc[n].plot(title='Class N (0)', figsize=(10,8))\nplt.subplot(222)\nfor n in range(3):\n    cl_S.iloc[n].plot(title='Class S (1)')\nplt.subplot(223)\nfor n in range(3):\n    cl_V.iloc[n].plot(title='Class V (2)')\nplt.subplot(224)\nfor n in range(3):\n    cl_F.iloc[n].plot(title='Class F (3)')\n","57957546":"#check if missing data\nprint(\"Column\\tNr of NaN's\")\nprint('+'*50)\nfor col in hbeat_signals.columns:\n    if hbeat_signals[col].isnull().sum() > 0:\n        print(col, hbeat_signals[col].isnull().sum()) \n","adb0f2ad":"joined_data = hbeat_signals.join(hbeat_labels, rsuffix=\"_signals\", lsuffix=\"_labels\")\n\n#rename columns\njoined_data.columns = [i for i in range(180)]+['class']","a9505e40":"#get correlaction matrix\ncorr_matrix = joined_data.corr()","60bb2016":"print('+'*50)\nprint('Top 10 high positively correlated features')\nprint('+'*50)\nprint(corr_matrix['class'].sort_values(ascending=False).head(10))\nprint('+'*50)\nprint('Top 10 high negatively correlated features')\nprint('+'*50)\nprint(corr_matrix['class'].sort_values().head(10))","bfc13c2f":"%matplotlib inline\n\nfrom pandas.plotting import scatter_matrix\n\n#Take features with the larges correlations\nfeatures = [79,80,78,77]\nscatter_matrix(joined_data[features], figsize=(20,15), c =joined_data['class'], alpha=0.5);","d3b64512":"print('-'*20)\nprint('Class\\t %')\nprint('-'*20)\nprint(joined_data['class'].value_counts()\/len(joined_data))\njoined_data.hist('class');\nprint('-'*20)","aa67374e":"print(\"class\\t%\")\njoined_data['class'].value_counts()\/len(joined_data)","4c66a607":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2,random_state=42)\n\nfor train_index, test_index in split.split(joined_data, joined_data['class']):\n    strat_train_set = joined_data.loc[train_index]\n    strat_test_set = joined_data.loc[test_index]    ","d4b5733f":"print(\"class\\t%\")\nstrat_train_set['class'].value_counts()\/len(strat_train_set)","14e5298c":"def compare_conf_matrix_scores(models, X, y):\n    \"\"\"\n    This function compares predictive scores and confusion matrices fro different ML algorithms\n    \"\"\"\n    \n    for i, model in enumerate(models):\n\n        # perform Kfold cross-validation returning prediction scores of each test fold.\n        labels_train_pred = cross_val_predict(model, X, y, cv=5)\n        print('+'*50)\n        print('Model {} Confusion matrix'.format(i+1))\n        print('+'*50)\n        print(confusion_matrix(y, labels_train_pred))\n        print('+'*50)\n\n        prec_score = precision_score(y, labels_train_pred, average='macro')\n        rec_score = recall_score(y, labels_train_pred, average='macro')\n        f1_sc = f1_score(y, labels_train_pred, average='macro')\n        print('Precision score: {}\\nRecall Score: {}\\nf1 score: {}'.format(prec_score,rec_score, f1_sc))\n    print('+'*50)\n    \n#produce labels and features sets for the training stage\nstrat_features_train = strat_train_set.drop('class', 1)\nstrat_labels_train = strat_train_set['class']","3a948f8c":"#initiate ML the classifiers\n\n# one versus one clasifier\nova_clf = OneVsOneClassifier(SGDClassifier(random_state=42, n_jobs=-1))\n\n#random forest\nforest_clf = RandomForestClassifier(random_state=42, n_jobs=-1)\n\n#Support vector machines\nsvm_clf = LinearSVC(random_state=42)\nsvc = SVC(decision_function_shape='ovo', random_state=42, max_iter=1000)\n\nwarnings.filterwarnings('ignore')\n\ncompare_conf_matrix_scores([ova_clf, forest_clf, svm_clf, svc], strat_features_train, strat_labels_train)","52a62f08":"#initialize standardscaler instance\nscaler = StandardScaler()\n\n#standarized data, i.e,  substract mean and devides by variance\nstd_features = scaler.fit_transform(strat_features_train)","ec3c88e3":"# make plots for the different hbeat classes (standarized)\nfig = plt.figure(figsize=(10,8))\nplt.subplot(221)\nplt.plot(figsize=(10,8))\nx = np.linspace(0,179,180)\nfor n in range(3):\n    plt.title('Class N (0)')\n    plt.plot(x, std_features[cl_0_idx[n]])\nplt.subplot(222)\nfor n in range(3):\n    plt.title('Class S (1)')\n    plt.plot(x, std_features[cl_1_idx[n]])\nplt.subplot(223)\nfor n in range(3):\n    plt.title('Class V (2)')\n    plt.plot(x, std_features[cl_2_idx[n]])\nplt.subplot(224)\nfor n in range(3):\n    plt.title('Class F (3)')\n    plt.plot(x, std_features[cl_3_idx[n]])","bdaab6df":"fig= plt.figure(figsize=(25,15))\nfor n in range(9):\n    plt.subplot(3,3,n+1)\n    scatter = plt.scatter(std_features[:,(n+1)*10],std_features[:,-1*(n+1)*5], alpha=0.5, c=strat_labels_train)\n    plt.xlabel('Feat. {}'.format((n+1)*10))\n    plt.ylabel('Feat. {}'.format(180-1*(n+1)*5))\nplt.rc('font', size=20)\nplt.rc('legend', fontsize=20)\n#plt.rc('axes', labelsize=40)\n#plt.legend(*scatter.legend_elements(), loc=\"best\", title=\"Classes\");","1eae70f5":"warnings.filterwarnings('ignore')\n\ncompare_conf_matrix_scores([ova_clf, forest_clf, svm_clf, svc], std_features, strat_labels_train)","97869be5":"# K nearest neighbors\nknn_clf = KNeighborsClassifier(n_jobs=-1)\n\n#Gaussian Naive Bayes\ngnb_clf = GaussianNB()\n\n#Stochastic gradient classifier\nsgd_clf = SGDClassifier(n_jobs=-1,random_state=42)\n\ncompare_conf_matrix_scores([knn_clf, gnb_clf, sgd_clf], std_features, strat_labels_train)","5a5a0aef":"#parameter grid\nforest_param_grid = {'n_estimators': [50,100,200,300], 'max_depth':[2,4,8]}\nknn_param_grid = {'n_neighbors':[2,4,8,10], 'weights':['uniform', 'distance']}\n\nwarnings.filterwarnings('ignore')\n\n#initialize classifiers\nforest = RandomForestClassifier(random_state=42, n_jobs=-1)\nknn = KNeighborsClassifier(n_jobs=-1)\n\n#initialize grid search\nforest_grid_search = GridSearchCV(forest, forest_param_grid, cv=5, scoring=\"f1_macro\")\nknn_grid_search = GridSearchCV(knn, knn_param_grid, cv=5, scoring=\"f1_macro\")\n\n#fit classifiers using gridsearch\nforest_grid_search.fit(std_features, strat_labels_train)\nknn_grid_search.fit(std_features, strat_labels_train)\n\nprint(\"+\"*50)\nprint(\"Model\\t\\tBest params\\t\\tBest score\")\nprint(\"-\"*50)\nprint(\"Random Forest\\t\\t\", forest_grid_search.best_params_, forest_grid_search.best_score_)\nprint(\"-\"*50)\nprint(\"KNN\\t\\t\", knn_grid_search.best_params_, knn_grid_search.best_score_)\nprint(\"+\"*50)","9b7e15dc":"forest_param_grid = {'n_estimators': [158,160,162], 'max_depth':[83,80,87]}\nforest_grid_search = GridSearchCV(forest, forest_param_grid, cv=5, scoring=\"f1_macro\")\nforest_grid_search.fit(std_features, strat_labels_train)\n\nprint(\"+\"*50)\nprint('Model\\t\\tBest params\\t\\tBest score')\nprint(\"-\"*50)\nprint(\"Random Forest\\t\\t\", forest_grid_search.best_params_, forest_grid_search.best_score_)\nprint(\"+\"*50)","9e936cdf":"#parameter grid\nsvc_param_grid = {'C':[10], 'gamma':[0.1,1,10]}\n\nwarnings.filterwarnings('ignore')\n\n#initialize classifiers\nsvc = SVC(kernel='rbf',decision_function_shape='ovo',random_state=42, max_iter = 500)\n\n#initialize grid search\nsvc_grid_search = GridSearchCV(svc, svc_param_grid, cv=3, scoring=\"f1_macro\")\n\n#fit classifiers using gridsearch\nsvc_grid_search.fit(std_features, strat_labels_train)\n\nprint(\"+\"*50)\nprint('Model\\t\\tBest params\\t\\tBest score')\nprint(\"-\"*50)\nprint(\"SVC\\t\\t\", svc_grid_search.best_params_, svc_grid_search.best_score_)\nprint(\"+\"*50)\n","f02c2631":"best_forest = forest_grid_search.best_estimator_\nbest_knn = knn_grid_search.best_estimator_\nbest_svc = svc_grid_search.best_estimator_\n\ncompare_conf_matrix_scores([best_forest, best_knn, best_svc], std_features, strat_labels_train)","ffe759e7":"#init scaler\nscaler = StandardScaler()\n\n#fit scaler to train data\nscaler.fit(strat_features_train)\n\n#produce labels and features sets for the test stage\nstrat_features_test = strat_test_set.drop('class', 1)\nstrat_labels_test = strat_test_set['class']\n\n#transform the test data\nstd_features_test = scaler.transform(strat_features_test)\n\n#predict values for the test data\nforest_pred = best_forest.predict(std_features_test)\nknn_pred = best_knn.predict(std_features_test)\nsvc_pred = best_svc.predict(std_features_test)\n\n#determine f1 score\nforest_f1 = f1_score(strat_labels_test, forest_pred, average='macro')\nknn_f1 = f1_score(strat_labels_test, knn_pred, average='macro')\nsvc_f1 = f1_score(strat_labels_test, svc_pred, average='macro')\n\n#determine confusion matrix\nprint('+'*50)\nprint('Random Forest Confusion matrix (f1 score: {})'.format(forest_f1))\nprint('+'*50)\nprint(confusion_matrix(strat_labels_test, forest_pred))\nprint('+'*50)\nprint('KNN Confusion matrix (f1 score: {})'.format(knn_f1))\nprint('+'*50)\nprint(confusion_matrix(strat_labels_test, knn_pred))\nprint('+'*50)\nprint('SVC Confusion matrix (f1 score: {})'.format(svc_f1))\nprint('+'*50)\nprint(confusion_matrix(strat_labels_test, svc_pred))\n","54030f9d":"#initialize ensemble\nensemble=VotingClassifier(estimators=[('Random Forest', best_forest), ('KNN', best_knn), ('SVC', best_svc)], voting='hard')\n\n#fit ensemble\nensemble.fit(std_features,strat_labels_train)\n\ncompare_conf_matrix_scores([ensemble], std_features, strat_labels_train)","b7e545d5":"#predict values for the test data\nensemble_pred = ensemble.predict(std_features_test)\n\n#determine f1 score\nensemble_f1 = f1_score(strat_labels_test, ensemble_pred, average='macro')\n\n#determine confusion matrix\nprint('+'*50)\nprint('Ensemble Confusion matrix (f1 score: {})'.format(ensemble_f1))\nprint('+'*50)\nprint(confusion_matrix(strat_labels_test, ensemble_pred))\nprint('+'*50)","beb3c400":"import numpy as np\nimport matplotlib.pyplot as plt\n\nh = 0.155 # step size in the mesh\n\n# we create an instance Classifier and fit the data. We are picking features 70 and 145.\nX, y = std_features[:,[70,145]], strat_labels_train\n\n#initialize classifiers\nclf1 = RandomForestClassifier(max_depth=83,n_estimators=158, n_jobs=-1,random_state=42)\nclf2 = KNeighborsClassifier(n_jobs=-1, n_neighbors=4,weights='distance')\nclf3 = SVC(C=10,decision_function_shape='ovo', gamma=0.1, kernel='rbf', max_iter=500, random_state=42)\nclf4 = ensemble=VotingClassifier(estimators=[('Random Forest', clf1), ('KNN', clf2), ('SVC', clf3)], voting='hard')\n#fit classifiers\nclf1.fit(X,y)\nclf2.fit(X,y)\nclf3.fit(X,y)\nclf4.fit(X,y)\n\n# Plot the decision boundary. For that, we will assign a color to each point in the mesh [x_min, x_max]x[y_min, y_max].\nx_min, x_max = X[:, 0].min(), X[:, 0].max()\ny_min, y_max = X[:, 1].min(), X[:, 1].max()\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n#fig titles\ntt =['Random Forest (depth=83)', 'KNN (k=4)','Kernel (RBF) SVM', 'Hard Voting']\n\nfig= plt.figure(figsize=(20,15))\n\nfor idx, clf in enumerate([clf1, clf2, clf3, clf4]):\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    #plot decision boundary\n    plt.subplot(2,2,idx+1)\n    plt.pcolormesh(xx, yy, Z, alpha=0.2)\n\n    # Plot training points\n    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.5)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.xlabel('Feat. 70')\n    plt.ylabel('Feat. 145')\n    plt.title(tt[idx])\n#    plt.legend(*scatter.legend_elements(), loc=\"best\", title=\"Classes\");    ","326c98eb":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.optimizers import SGD\n\n#define parameters\nbatch_size = len(std_features)\/\/300\n\n#build model\nmodel = keras.Sequential([\n    keras.layers.Dense(200, activation='relu', input_shape=(180,)),\n    keras.layers.Dense(100, activation='relu'),\n    keras.layers.Dense(4, activation='softmax')\n])\n\n#transform the test data\nstrat_features_test = strat_test_set.drop('class', 1)\nstrat_labels_test = strat_test_set['class']\n\n#standardize \nstd_features_test = scaler.transform(strat_features_test)\n\n#change labels to categorical, requaried to use 'categorical_crossentropy'\ncategorical_labels_train = to_categorical(strat_labels_train, num_classes=None)\ncategorical_labels_test = to_categorical(strat_labels_test, num_classes=None)\n\n#stochastic gradient descent optimizer\nsgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n\n#compile model\nmodel.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])\n\n#fit model and get scores\nmodel.fit(std_features, categorical_labels_train,epochs=20,batch_size=batch_size)\n\nscore = model.evaluate(std_features_test, categorical_labels_test, batch_size=batch_size)","3fce197d":"# check metrics on the test data\ny_pred1 = model.predict(std_features_test)\ny_pred = np.argmax(y_pred1, axis=1)\n\nf1_sc = f1_score(strat_labels_test, y_pred , average=\"macro\")\nconf_mat = confusion_matrix(strat_labels_test, y_pred)\n\n# Print f1, precision, and recall scores\nprint('+'*50)\nprint('Neural Network Confusion matrix (f1 score: {})'.format(f1_sc))\nprint('+'*50)\nprint(conf_mat)\nprint('+'*50)","a511d7c1":"#best knn model n_neighbors = 4, weights='distance', n_jobs=-1\n#best random forest max_depth=83, n_estimators=158, n_jobs=-1\nbest_knn, best_forest, best_svc","46e443cd":"Let's have a look at how the data looks like for the different types of heartbeats.","50b8e248":"# Heartbeat classification from ECG morphology using Machine learning.\n\n## Motivation\n\nAcording to [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Heart_arrhythmia) \nArrhythmia affects millions of people in the world. In Europe and North America, as of 2014, atrial fibrillation affects about 2% to 3% of the population. Atrial fibrillation and atrial flutter resulted in 112,000 deaths in 2013, up from 29,000 in 1990. Sudden cardiac death is the cause of about half of deaths due to cardiovascular disease and about 15% of all deaths globally. About 80% of sudden cardiac death is the result of ventricular arrhythmias. Arrhythmias may occur at any age but are more common among older people. Arrhythmias are coused by problems with the electrical conduction system of the heart. A number of tests can help with diagnosis including an electrocardiogram (ECG) and Holter monitor. Regarding ECG, the diagnosis is based on the carefully analysis that a specialized doctor perform on the shape and structure of the independent heartbeats. This process is tedious and requires time. \n![ecg](.\/pics\/ecg.png)\n\nIn this work, we aim to classify the heart beats extracted from an ECG using machine learning, based only on the lineshape (morphology) of the individual heartbeats. The goal would be to develop a method that automatically detects anomallies and help for the prompt diagnosis of arrythmia.\n\n## Data\n\nThe original data comes from the [MIT-BIH Arrythmia database](https:\/\/physionet.org\/content\/mitdb\/1.0.0\/). Some details of the dataset are briefly summarized below:\n\n+ 48.5 hour excerpts of two-channel ambulatory ECG recordings\n+ 48 subjects studied by the BIH Arrhythmia Laboratory between 1975 and 1979.\n+ 23 recordings randomly selected from a set of 4000 24-hour ambulatory ECG recordings collected from a mixed at Boston's Beth Israel Hospital.\n+ 25 recordings were selected from the same set to include less common but clinically significant arrhythmias.\n+ Two or more cardiologists independently annotated each record (approximately 110,000 annotations in all).\n\nAlthough the one that its currently being used here is taken from [kaggle](https:\/\/www.kaggle.com\/alexandrefarb\/mitbih-arrhythmia-database-de-chazal-class-labels). In this dataset the single heartbeats from the ECG were extracted using the [Pam-Tompkins algorithm](https:\/\/en.wikipedia.org\/wiki\/Pan-Tompkins_algorithm). Each row of the dataset represents a QRS complex as the one schematically shown below:\n<img src=\".\/pics\/qrs.png\" width=\"300\">\nThese QRS are taken from the MLII lead from the ECG. As observed in the firts figure above, there is also the V1 lead, which is not used in this work.\nFor further details on how the data was generated, the interested can read the original paper by [Chazal et al.](https:\/\/www.ncbi.nlm.nih.gov\/pubmed\/15248536)\n\nThe different arrythmia classes are:\n\n0. Normal\n1. Supraventricular ectopic beat\n2. Ventricular ectopic beat\n3. Fusion Beat\n\nThis means that we are facing a multi-class classification problem with four classes. \n\n## Strategy.\n\nTo achieve our goal we will go the following way:\n\n1. Data standardisation\n2. Selection of three promising ML algorithms.\n3. Fine tunning of the best models\n4. Model comparison\n5. Build  a Neural Network\n6. Compare \n\nRather important, in order to evaluate the performance of our models is to choose the appropiate metrics. In this case we will be checking the confusion matrix and the f1 score with macro averaging.\n\n\n## Data Loading and first insights.\n","b3ebc33c":"Further fine tunning of the Random forest model.","aaae5a75":"Although not perfect, the models do a pretty decent job to classify the different classes of heartbeats.\n\n## Ensemble model\n\nOne last thing we can try is to merge these three models using assemble in order to maximize the classification power. This we do by using a Voting classifier and set the voting Hyperparameter to 'hard'. This basically means that, for a given input, each model will give classification, i.e., 0, 1, 2 or 3 and the classification with the higher votes wins.","5cc340a0":"After some fine tuning of the random forest we have increase its efficiency from 0.82 to 0.85. Also the KNN model improved from 0.89 to 0.9, whereas the SVM model improved from 0.78 to 0.91. Let us now check the confusion matrix from these improved models.","3e5c07aa":"So the standard scaler improves the ovo_clf and the Linear-SVM. The SVC model also has an important improvemnet. In the case of the random forest it does not change much, however,  random forest is still the best model so far. Lets try another couple of models, for instance:\n\n1. k-Nearest Neighbors (KNN)\n2. Gaussian Naive Bayes (GNB)\n3. Stochastic Gradient Descent (SGD)","887120e1":"Apparently, as observed from these fplots, there are some sort of clustering for the 0 and 2 classes. also class 4 seem to show a particular pattern. This is good, since it means that in principle a good ML algorithm might learn something from these features.\n\nLet us now check again our firts four models with the standardized data and see if they improve.","e2451be3":"It seem that our KNN model performs reasonably good with a f1 score of 0.89, which is similar to the one we got with the randome forest classifier (0.83). Actually, the KNN model does a much better job in classifying instances of class 1 and 3. Also the SVM model is a promissing one with a  0.78 f1 score. Therrfore we will choose **Random Forest**, **KNN** and **SVM** as the models to use to do our classification task.\n\nLet us now try to fine tune these three models and see if we can improve their performances.\n\n## Model fine tunning via GridSearch cross validation.\n\nSince SVM model takes much more time to optimze via grid searchcv, we first start with the random forest and KNN models.","da7d253a":"We will firts start with the following models:\n\n1. One vs. One classifier (OVO)\n2. Random Forest\n3. linear Support Vector Machine \n4. Support Vector Machine (SVM)\n\nNote that we are at the moment not doing any standardization. We will do that after this step, just to compare if there is an improvement by standardizing the data.","ec4a2c5c":"Nice, we see that the amount of data with classes 0 to 3 in the train set maps to those from the original data.\n\nWe are ready to pick some ML models to start training with our data.\nWe will use a brute force approach in the sense that we will try several models at once. For each model, we will do a 5-fold cross validation and depending on its metrics we will choose the best among them. To do that we will write a simple function that takes a list of models, and perfom the cross validation for each and prints its metrics, i.e., confusion matrix, precission, recall and f1 score.","f0569ffd":"Let's plot some of these features to see how they correlate.","c337910b":"The confusion matrix clearly tell us that these model are not classifying the data correctly. For instance, for model 1, we can see that class 3 has not been correctly classified at all, whereas class 1 is poorly classified (25 from 181 were correctly classified). The precision and recall scores clearly tell us that our model just classifies correctly about 40% of the instances for classes other than N (0). \n\nThe most promissing model is model 2, i.e., Random Forest. It has the best scores and its confusion matrix eshibits reasonable predictions.\n\nLet us now try what happends when we standardize the features and try again with these models.","593e0070":"Before fitting models, lets have a look how does the data look like after standardization. The following figure shows the same plots as the one we did at the begining, but using the data after using StandardScaler.","478e69bf":"## Conclusions and outlook\n\nWe have perform a multi-class classification task on a dataset consisting of ECG heartbeats, we have mapped several classifier models and selected the three among seven which showed better classification performance as using n-fold cross validation. These three models were further improved by optimizing some of their hyperparameters via GridSearch cross validation. From these procedure we have found that the SVM model was the one that performed the best with a f1 score of 0.92. \n\nIn order to see if we can do better, we have merged these three models into one ensemble model using hard voting classifier. However, this did not outperform the SVM model. Finally, we built and trained an ANN which showed a very nice performance, however, still below that of the SVM model.\n\nAlthough, the work here done looks rather promising we have to consider that the dataset is highly unbalanced. There is not enough data for classes 1 to 3. It would be good to have more representation of these classes in the dataset, since this will increase the classification performance of the models. One way to solve this problem would be to try some data augmentation procedure. For instance, by determining the principal components of these classes and with these randomly generate data that can be used. Also, the use of the V1 data from the ECG in order to increase the amount of features would help, together with introduction of other important features like area under the QRS curve, peak maxima and minim, inflection points, etc.\n","095811c0":"After standardization it makes also sense to have a look at some of the features to see if we can observe some clustering, patterns or characteristics of the data dependending on the classes.","ddb45d9d":"Let's now check if the train data fulfills the stratified conditions after the split was done.","31544248":"Nice, now we can try these models with our test data. but first we have to standardize the test data using the same scaler we used for the train data.","4d0b6e50":"This means that there are no missing values to fill. We can now proceed to check if there are some correlations on the data.","bdc4faf5":"The correlation among the selected features is strongly linear. \n\nWe now check if the data is balanced, this means, if there is a balenced amount of data for the diferent classes we want to study.","3c986f58":"The above plot shows that the dataset is quite unbalanced. There is very few data for classes 1, 2 and 3, whereas class 1 is about 90% of the total dataset. \n\nNow since the data is rather clean and without missing values we can start preparing our data to do some machine learning.\n\n# Machine learning \n\nWe now first produce some test and train data from our dataset. Be carefull that there is not many data for instances with classes 1 to 3. This means that we have to split the data not randomly, but trying to keep data with these clases in the train and test data. Hence, we will perform a stratified data split.\n\nFor instance, lets check, once more, which percentage of data of each class we have in our data set.","49bb8fa4":"The ensemble classifier performs better that KNN and random forest, although SVM still does a better job by its own. Lets check it with test data.","6b4ef940":"## Neural network\n\nTo finish, we will now train an Artifitial Neural Network (ANN) using TensorFlow and Keras. We will then build a ANN with an input layer, two hidden layers, one with 200 and the other one with 100 neurones. And an output layer with four neurons, one for each class. For the Hidden layers we will use a ReLu activation funtion, whereas for the output layer we will use a softmax function. ","c0ada871":"This esemble model in particular seems not to perform better than the SVM model for instance. Although they are really close.\n\nJust to have a closer look on the differences of these four models, let us make plots on how these models draw their decision boundaries. ","80e0c763":"Lets fine tune the SVM model."}}