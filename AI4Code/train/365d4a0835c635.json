{"cell_type":{"3c6454b4":"code","c4e5ad09":"code","f269baaa":"code","1a773843":"code","89507910":"code","7d30f159":"code","794638cd":"code","e41e298c":"code","dc94a1a3":"code","483e2c9a":"markdown","4e39ba8c":"markdown","ba236ecc":"markdown","864c9f97":"markdown","a7a8ede3":"markdown","2cd69d88":"markdown","4fb221d6":"markdown","83844623":"markdown","958c03bc":"markdown"},"source":{"3c6454b4":"import numpy as np\nimport pandas as pd\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Flatten, Dense, Embedding\n\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import CategoricalCrossentropy\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c4e5ad09":"data = pd.read_csv('\/kaggle\/input\/urdu-news-headlines\/UrduNewsHeadlines.csv')\ndata = data['\u0642\u0648\u0645\u06cc\\t \u0648\u0632\u06cc\u0631\u0627\u0639\u0638\u0645 \u0639\u0645\u0631\u0627\u0646 \u062e\u0627\u0646 \u0646\u06d2 \u06a9\u0644 \u0627\u06cc\u0633\u06cc \u062c\u06af\u06c1 \u062c\u0627\u0646\u06d2 \u06a9\u0627 \u0641\u06cc\u0635\u0644\u06c1 \u06a9\u0631 \u0644\u06cc\u0627 \u06a9\u06c1 \u0627\u067e\u0648\u0632\u06cc\u0634\u0646 \u0631\u06c1\u0646\u0645\u0627 \u0628\u06be\u06cc \u062f\u0646\u06af  ']\ndata = [x.split('\\t')[1].strip() for x in data]\ndata[:10]","f269baaa":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(data)\nvocab_size = len(tokenizer.word_index) + 1\nvocab_size","1a773843":"train_data = tokenizer.texts_to_sequences(data)\ntraining_examples = []\nlabels = []\n\nfor e in train_data:\n    for i in range(1, len(e)):\n        training_examples.append(e[:i])\n        labels.append(e[i])\n        \nmax_len = max([len(x) for x in training_examples])\nX = pad_sequences(training_examples, maxlen=max_len)\nX","89507910":"Y = to_categorical(labels, num_classes=vocab_size)\nY","7d30f159":"model = Sequential([\n    Embedding(vocab_size, 300),\n    Bidirectional(LSTM(20)),\n    Flatten(),\n    Dense(vocab_size, activation='softmax')\n])\n\noptim = Adam(learning_rate=0.01)\nloss = CategoricalCrossentropy()\nmodel.compile(optimizer=optim, loss=loss, metrics=['accuracy'])","794638cd":"BS = 32\nEPOCHS = 20\n\nhistory = model.fit(X, Y, batch_size=BS, epochs=EPOCHS)","e41e298c":"model.summary()","dc94a1a3":"seed_text = \"\u0648\u0632\u06cc\u0631\u0627\u0639\u0638\u0645 \u0639\u0645\u0631\u0627\u0646 \u062e\u0627\u0646 \u0646\u06d2\"\nnext_words = 30\n  \nfor _ in range(next_words):\n    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n    token_list = pad_sequences([token_list], maxlen=max_len-1, padding='pre')\n    predicted = model.predict_classes(token_list, verbose=0)\n    output_word = \"\"\n    for word, index in tokenizer.word_index.items():\n        if index == predicted:\n            output_word = word\n            break\n    seed_text += \" \" + output_word\nprint(seed_text)","483e2c9a":"## Dataset preparation for language modeling\nHere is a tricky thing you need to understand. we need to prepare dataset for a language model. The task of a language model is to predict what's going to be the next word.Here I am going to loop through each sentence and will prepare training examples by taking previous words as input and next coming word as output (to be predicted.)","4e39ba8c":"## One-hot encoding of output labels\nThe main problem is to predict the word that is going to be the next word. so it is a multiclass classification problem where number of classes would be equal to the number of words in vocabulary. So to solve it, we need to place softmax activation funcction at the output layer of our neural network. for that i am doing one hot encoding of our outputs in next cell","ba236ecc":"## Model definition\nIn the cell below, I am using Bidirectional LSTM to train our language generator model.","864c9f97":"In this notebook, I am going to use Urdu Headlines dataset provided by **Adnan Zaidi**. Thanks to him for this effort. I will be using LSTM in keras with tensorflow backend to train this Language model and will use it as text generator for Urdu News. You must have heard that deep learning models are always hungry for data. so this model will not perform that much well as we have near 2300 news headlines but performance will increase if we will have more data in future and we will improve our model.","a7a8ede3":"Text generator is an Application of language model. Language model can be simply defined as the model that will calculate the probability of sentence to be the next sentance. If i talk on samller level, Language model is model that will calculate probability of any sentence to be the next sentence and then we can calculate the probability of whole sentence using chian rulw of probability.","2cd69d88":"## Reading data\nCSV file in this dataset contains only headlines of news. I am going to read file and extract all news headlines to a list to preprocess the text so we can use it for training.","4fb221d6":"# Urdu Text Generator using News Dataset\nKeeping in mind that urdu language is one of the language that requires a lot of reserch and work remaining in NLP area if we will compare it with English and some other popular languages. I find this interesting and full of opportunities for young researchers like to work on these gaps. ","83844623":"## Tokenization\nNow we have data that contains around 2300 news headlines. It time to prepare our dataset. I am using keras built in tokeniser to tokenize sentences.","958c03bc":"## Text generation\nNow, we have trained an LSTM model to which if we give some words as input, it will output next word. To generate text what i will do is to input it some starting words and it will predict something. then i will concatenate the predicted word to input and give it to model again and so on."}}