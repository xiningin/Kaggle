{"cell_type":{"6d9253a5":"code","c97dfd12":"code","571c1800":"code","c1321802":"code","f543136b":"code","447d66bf":"code","7c404139":"code","2d6c1b3b":"code","64641bfd":"code","03bd30dc":"code","b739a02b":"code","0c2a8e3f":"code","1cc2aea0":"code","d44a6818":"code","eb3a02ca":"code","7f29e9f8":"code","4efd2791":"code","2750e550":"code","22741dd0":"markdown","f41ca2fe":"markdown","fa680c95":"markdown","47181765":"markdown","8062b438":"markdown","537b02a1":"markdown","cc3ded2f":"markdown","6b7cb3e1":"markdown","267c7948":"markdown","d579c95b":"markdown","8a796637":"markdown","1315ff73":"markdown","2566b557":"markdown","71253ba1":"markdown"},"source":{"6d9253a5":"import numpy as np \nimport pandas as pd \nimport json\nimport os\nfrom pandas import json_normalize #package for flattening json in pandas df\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\npd.set_option('display.max_colwidth', 1)\npd.options.display.max_rows = 999","c97dfd12":"# Load the review data\nwith open(\"..\/input\/yelp-dataset\/yelp_academic_dataset_review.json\") as reviews:\n    \n    words = [\"poison\",\"diar\",\"sick\",\"puk\",\"salmonella\",\"chunder\",\"spew\",\"vom\",\"throw up\",\"threw up\",\"nause\"]\n    count = 0 \n    \n    highRiskReviews = pd.DataFrame(columns=['business_id', 'date', 'text'])\n    normalReviews = pd.DataFrame(columns=['business_id', 'date', 'text'])\n    \n    # Iterate through the reviews and save high risk reviews to highRiskReviews\n    for index,review in enumerate(reviews):\n        reviewData = json.loads(review)\n        \n        # if text contains one of the key words, and is 1 star or less then append to the data frame\n        if any(word in reviewData['text'] for word in words) and reviewData['stars'] < 2:\n            highRiskReviews = highRiskReviews.append({'business_id': reviewData['business_id'], 'date': reviewData['date'], 'text': reviewData['text']}, ignore_index=True)\n        \n        # because of the size of the data set, pick approx one in every 200, without key words as normal reviews\n        if np.random.randint(1,180) == 1 and not any(word in reviewData['text'] for word in words):\n            normalReviews = normalReviews.append({'business_id': reviewData['business_id'], 'date': reviewData['date'], 'text': reviewData['text']}, ignore_index=True)\n            count += 1\n        \n    print(highRiskReviews.info())\n    print(normalReviews.info())","571c1800":"highRiskReviews.sample(n=5)","c1321802":"normalReviews.sample(n=5)","f543136b":"def formatDataFromID(business_ids, highRisk = 0):\n    \n    X = pd.DataFrame()\n\n    with open(\"..\/input\/yelp-dataset\/yelp_academic_dataset_business.json\") as businesses:\n        \n        for business in businesses:\n\n            businessData = json.loads(business)\n\n            if businessData['business_id'] in business_ids:\n                if businessData['categories']:\n                    if 'Food' in businessData['categories'] or 'Restaurants' in businessData['categories']:\n\n                        # Drop columns that are unlikely to be useful features (think about dropping review count and number of stars as this will leak data)\n                        columnsToDrop = [\"hours\", \"name\", \"address\", \"city\", \"state\", \"postal_code\", \"latitude\", \"longitude\", \"is_open\", \"BusinessParking\", \"GoodForMeal\"]\n                        [businessData.pop(column, None) for column in columnsToDrop]\n\n                        # Add in category data, represented as a string list\n                        categoryColumns = businessData[\"categories\"].split(\",\")\n                        for category in categoryColumns:\n                            businessData['category_' + category.strip()] = 1\n\n                        # Add in attribute data, represented as a string dict\n                        if businessData[\"attributes\"]:\n                            for key,value in businessData[\"attributes\"].items():\n                                if key == \"Ambience\":\n                                    if businessData[\"attributes\"][\"Ambience\"] != \"None\":\n                                        ambienceType = businessData[\"attributes\"].get(key)\n                                        for key,value in eval(ambienceType).items():\n                                            if value == 0:\n                                                businessData['ambience_' + key] = 0\n                                            else:\n                                                businessData['ambience_' + key] = 1\n\n                        # Assign the target variable (whether the restaurant has a high risk of food poisoning)\n                        businessData['highRisk'] = highRisk\n\n                        X = X.append(json_normalize(businessData, sep='_'), ignore_index=True)\n        return X","447d66bf":"highRiskBusinesses = highRiskReviews['business_id'].tolist()\nnormalBusinesses = normalReviews['business_id'].tolist()\n\nX_highRisk = formatDataFromID(highRiskBusinesses, 1)\nX_normal = formatDataFromID(normalBusinesses)","7c404139":"print(\"There are {} high risk businesses with an average rating of {:.1f} stars.\".format(highRiskReviews['business_id'].nunique(), X_highRisk['stars'].mean()))\nprint(\"There are {} normal businesses with an average rating of {:.1f} stars.\".format(normalReviews['business_id'].nunique(), X_normal['stars'].mean()))","2d6c1b3b":"# Merge the high and normal data together\nX = pd.concat([X_highRisk, X_normal], ignore_index=True)\n\n# Ambience and categories columns have already been parsed, so we can drop them\nX_clean = X.drop(['attributes_Ambience', 'categories', 'category_Restaurants', 'business_id','attributes_BusinessParking','attributes_BikeParking', 'attributes_Music'], axis=1)\n\n# Drop columns with more than 90% NaN values, these are likely to be spelling mistakes or obscure features\ncols_with_missing = [col for col in X.columns if X[col].isnull().sum() > X.shape[0]*0.9]\nX_clean = X_clean.drop(cols_with_missing, axis=1)\n\n# Replace NaNs with 0's as they are almost all categorical data\nX_clean = X_clean.fillna(0)\n\n# Find object columns\nobjects = (X_clean.dtypes == 'object')\nobject_cols = list(objects[objects].index)\n\n# Drop high cardinality objects\nlow_cardinality_cols = [col for col in object_cols if X_clean[col].nunique() < 10]\n\n# Columns to be dropped from the dataset\nhigh_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n\n# Change the column data to be strings, this doesn't matter as they will be OH encoded\nfor col in object_cols:\n    X_clean[col] = X_clean[col].astype(str)\n\nX_clean = X_clean.drop(high_cardinality_cols, axis=1)\n\n# OH encode the data\nOH_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\nOH_cols = pd.DataFrame(OH_encoder.fit_transform(X_clean[low_cardinality_cols]))\n\n# Put the index back in\nOH_cols.index = X_clean.index\n\n# Drop the columns that have been OH encoded\nnum_X = X_clean.drop(low_cardinality_cols, axis=1)\nOH_X = pd.concat([num_X, OH_cols], axis=1)","64641bfd":"OH_X.info()","03bd30dc":"y = OH_X['highRisk']\nOH_X= OH_X.drop('highRisk', axis=1)\n\nX_train, X_valid, y_train, y_valid = train_test_split(OH_X, y, train_size=0.85, test_size=0.15, random_state=1)","b739a02b":"# Define the model\nmodel = XGBRegressor(n_estimators=500)\n\n# early stopping rounds will stop when the prediction stops improving. This is compares values by evaluating X_valid, y_valid\nmodel.fit(X_train, y_train,\n        early_stopping_rounds=5,\n        eval_set=[(X_valid, y_valid)],\n        verbose=False)\n\npredictions = model.predict(X_valid)\nprint(\"Mean absolute error: {:.2f}\".format(mean_absolute_error(predictions, y_valid)))","0c2a8e3f":"# Find the restaurants with the highest and lowest predicted risk of food poisoning\nhighestRiskRestaurant = predictions.max()\nlowestRiskRestaurant = predictions.min()\n\nprint(\"The highest risk restaurant has a food poisoning prediction of {:.0f}%.\".format(highestRiskRestaurant*100))\nprint(\"The lowest risk restaurant has a food poisoning prediction of {:.0f}%.\".format(lowestRiskRestaurant*100))\n\n# Locate these in predictions array\nhighRiskIndexInPredictionsArray = np.where(predictions == highestRiskRestaurant)[0]\nlowRiskIndexInPredictionsArray = np.where(predictions == lowestRiskRestaurant)[0]\n\n# Find the row index in the validation data, to find the restaurant_id\nhighRiskRow = X_valid.iloc[highRiskIndexInPredictionsArray]\nlowRiskRow = X_valid.iloc[lowRiskIndexInPredictionsArray]","1cc2aea0":"highRiskReviewBusinessID = X.iloc[highRiskRow.index]['business_id']\nlowRiskReviewBusinessID = X.iloc[lowRiskRow.index]['business_id']","d44a6818":"with open(\"..\/input\/yelp-dataset\/yelp_academic_dataset_review.json\") as reviews:\n    \n    highestRiskReviews = pd.DataFrame(columns=['text'])\n    lowestRiskReviews = pd.DataFrame(columns=['text'])\n    \n    # Iterate through the reviews and save high risk reviews to highRiskReviews\n    for review in reviews:\n        reviewData = json.loads(review)\n        if reviewData['business_id'] == highRiskReviewBusinessID.values[0]:\n            highestRiskReviews = highestRiskReviews.append({'text': reviewData['text']}, ignore_index=True)\n        \n        if reviewData['business_id'] == lowRiskReviewBusinessID.values[0]:\n            lowestRiskReviews = lowestRiskReviews.append({'text': reviewData['text']}, ignore_index=True)","eb3a02ca":"X.loc[X['business_id'] == lowRiskReviewBusinessID.values[0]]","7f29e9f8":"lowestRiskReviews.head()","4efd2791":"X.loc[X['business_id'] == highRiskReviewBusinessID.values[0]]","2750e550":"highestRiskReviews.head()","22741dd0":"# Classifying businesses that may have given customers food poisoning\nFirst up, I'm looking for restaurants where customers have complained about having food poisoning symptoms in their Yelp reviews. There's 1.6m reviews in the Yelp data set, so to avoid overloading RAM this is done line by line. To classify a business as having a high risk of food poisoning, I will iterate through every review and search for the following strings:\n\n> \"poison\", \"diar\", \"sick\", \"puk\", \"salmonella\", \"chunder\", \"spew\", \"vom\", \"throw up\", \"threw up\", \"nause\"","f41ca2fe":"The mean absolute error is pretty high, but we're comparing the prediction values against 1s and 0s. I'm not sure the MAE is the best method for determining the error, or does this mean that this method is only 5% better than a guess? Let's see what the reviews for the highest and lowest predicted risk are like.","fa680c95":"Here we do some additional pre-processsing, cleaning and encoding of the data.","47181765":"And a sample of the normal reviews are shown below:","8062b438":"# Pre-processing data\nNow we have two data sets, one for high risk, and one for normal, we can use the business ID to find the business information. The function below matches up the business ID with the relevant business within the Yelp businesses data set. This contains all of the information about the business - the type of business, cost, number of stars, number of reviews etc.\n\nThis function is also checking that the business serves food, parses some of the nested data, and then uses json_normalize to flatten it into a data frame. If the data is from the businesses with a high risk of food poisoning the *highRisk* flag is stored as the target variable.","537b02a1":"To find the reviews for the highest and lowest risk we need to find the reviews that match the highest and lowest risk business IDs.","cc3ded2f":"By looking at the data, it's clear that the high risk reviews are a good indicator for food poisoning. There are some reviews that don't relate to food consumption, which makes sense as Yelp covers many types of businesses. This will be corrected for later. \n\nThere's approximately a 50:50 split of high risk, and normal review data.\n\nA sample of the high risk reviews are shown below:","6b7cb3e1":"It's clear that there is a strong correllation between restaurant rating, and risk of food poisoning. This model could be run on live review data sets to notify authorities when restaurants start to flag food poisoning. Alternatively authorities could skim off the top 10% restaurants most likely to have caused food poisoning and send their inspectors to pay them a visit.\n\nThis is my first notebook, I learnt a lot of Python and spent most of the time on this preparing the data. I'd appreciate your thoughts and advice on how I can make it better.","267c7948":"It's likely that the strongest predictor of risk of food poisoning will be having a low number of stars (which is the average of the customer reviews).","d579c95b":"# Predicting the risk of food poisoning based on restaurant attributes\nThis notebook uses Yelp restaurant reviews to classify businesses that have an elevated risk of giving customers food poisoning. Business attributes such as ratings, ambience and type of restaurant are used to create a model to predict the risk of a restaurant giving customers food poisoning.  This is my first notebook and I would really appreciate your comments, advice and ideas for improvements.\n\nThis tool could be used to help governments save money and improve public health by efficiently prioritising food hygeine inspections. The data is already available through online customer reviews. Here are some stats on the economic impact of poor food hygeine:\n\n* An estimated 600 million \u2013 almost 1 in 10 people in the world \u2013 fall ill after eating contaminated food and 420 000 die every year, resulting in the loss of 33 million healthy life years (DALYs).\n* US$110 billion is lost each year in productivity and medical expenses resulting from unsafe food in low- and middle-income countries.\n* Unsafe food containing harmful bacteria, viruses, parasites or chemical substances, causes more than 200 diseases \u2013 ranging from diarrhoea to cancers.\n\n**This notebook is split into 5 sections:**\n1. Classifying businesses that may have given customers food poisoning\n2. Pre-processing business data\n3. Fitting and predicting\n4. Assessing the lowest risk restaurant, based on the prediction made using validation data\n5. Assessing the highest risk restaurant, based on the prediction made using validation data\n","8a796637":"# The lowest risk restaurant, based on the prediction from restaurant attributes:","1315ff73":"After pre-processing there are 31k rows of restaurant data in the data frame, classified with a food poisoning risk of either high or normal. Nice!","2566b557":"# The highest risk restaurant, based on the prediction from restaurant attributes:","71253ba1":"# Fitting and predicting\nUsing XGB regressor to fit the training data and to make predictions on the risk of food poisoning using the validation data."}}