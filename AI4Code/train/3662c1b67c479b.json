{"cell_type":{"22d9a682":"code","347e7f32":"code","c193f7ba":"code","d6cc23eb":"code","1142475e":"code","3946c414":"code","93117e65":"code","11bd3d08":"code","2a0434df":"code","4a6ecb0a":"code","29c61c9e":"code","ce8251de":"code","c9bc6663":"code","a934df32":"code","3378e486":"code","801ddb93":"code","6c00dbd5":"code","a1a597ed":"code","e0c8e90d":"code","a8225ea5":"code","88610bc6":"code","8980db5d":"code","44ab1333":"code","a3c15796":"markdown","7e282637":"markdown","f9d2a658":"markdown","7bcbc542":"markdown","2c804c4b":"markdown","f0a6fcaa":"markdown","7c648562":"markdown"},"source":{"22d9a682":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","347e7f32":"# Importing libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt","c193f7ba":"original_df = pd.read_csv(r'\/kaggle\/input\/titanic\/train.csv')\n\n#print(original_df.info())\n\n#print(original_df.describe())","d6cc23eb":"#print(original_df.head())","1142475e":"#original_df.nunique()","3946c414":"# Correcting\n# As seen it looks like we have a few outliers in \"Fare\", let\u00b4s have a lot at it\n\n#sns.histplot(x=\"Fare\",data=original_df,bins=30)\n#plt.show()\n#sns.boxplot(y=\"Fare\",x=\"Pclass\",data=original_df)\n#plt.show()\n\n# One fare was USD 512 in 1912 (equivalent to USD 14,743 in 2021).\n# The median fare value for the first class was USD 60 (USD 1,680 in 2021) - 8.5 times smaller than 512\n# However, after some outside research we found that the most expensive Ticket was even expensiver than out USD 512 outlier. So we will not change a thing in this parameter.","93117e65":"# Completing\n# Variables with missing values: Age, Cabin and Embarked\n\n# First let\u00b4s have a look at the Age variable\n#sns.boxplot(y=\"Age\",data=original_df, x=\"Pclass\")\n#plt.show()\n#sns.boxplot(y=\"Age\",data=original_df, x=\"Sex\")\n#plt.show()\n#sns.boxplot(y=\"Age\",data=original_df, x=\"Parch\")\n#plt.show()\n#sns.boxplot(y=\"Age\",data=original_df, x=\"SibSp\")\n#plt.show()\n#sns.scatterplot(x=\"Age\",data=original_df, y=\"Fare\")\n#plt.show()\n#sns.catplot(y=\"Age\",data=original_df, x=\"SibSp\", hue='Pclass',kind='box')\n#plt.show()\n\n# Looks like the median Age per Pclass group might be a good fit for inputing the missing values. We could also combine this metric with groups per SibSp.","11bd3d08":"# Now let\u00b4s have a look at the Cabin variable\nprint(\"We have {} unique Cabin values from {} non-missing values\".format(original_df.Cabin.nunique(),original_df.Cabin.value_counts().sum()))\n\n\n# From the Cabin variable, we can extract the deck information (first letter of cabin)\n# However, most of the Cabin information are from passengers on the first class\n# That being said, we could delete this variable for our model.\n\n# For the missing values in the Embarked variable, as there just a few, we could simply input the most commun value","2a0434df":"# Creating (and removing)\n\n# In this section we will create variables based on others variables.\n# Looking at the data we came with a few ideas: \"Name title\", \"Family Size\", \"Is Alone\", \"Fare\/Age ratio\", \"Age Categories\"","4a6ecb0a":"# Fucntion to clean, create, convert and complete the dataset\n\ndef dataset_adjusted(df):\n    #COMPLETING\n    # Inputing missing values\n    df[\"Age\"] = df['Age'].fillna(df.groupby(\"Pclass\")['Age'].transform('median'))\n    df['Embarked']= df['Embarked'].fillna(df['Embarked'].value_counts().index[0])\n    \n    # CREATING\n    # Name title\n    df['Name_Title'] = df['Name'].str.split(\", \").str[1].str.split(\".\").str[0]\n    # There are lots of titles with just a few observations. We will clean this up\n    title_names = (df['Name_Title'].value_counts() < 10) \n    df['Name_Title'] = df['Name_Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n    \n    # Family Size\n    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n    \n    # Is alone?\n    df['IsAlone'] = 0\n    df.loc[df['FamilySize']==1,'IsAlone']=1\n    \n    # Fare\/Age ratio\n    #df['Fare_Age'] = df['Fare']\/df['Age']\n    \n    # Age Categories\n    # Infant < 2 | Child < 8 | Teenage < 17 | Young Adult < 30 | Adult < 60 | Senior > 60\n    def age_category(row):\n        if row<=2:\n            return \"Infant\"\n        elif row<=8:\n            return \"Child\"\n        elif row<=17:\n            return \"Teenage\"\n        elif row<=30:\n            return \"Young_Adult\"\n        elif row<=60:\n            return \"Adult\"\n        return \"Senior\"\n    #df['Age_Category'] = df[\"Age\"].apply(age_category)\n    \n    # CONVERTING\n    categorical_columns = ['Sex','Embarked','Name_Title']#,'Age_Category']\n    df = pd.concat([df,pd.get_dummies(df[categorical_columns],drop_first=True)],axis=1).drop(columns=categorical_columns)\n    \n    # CLEANING\n    df = df.drop(columns=['PassengerId','Name','Ticket','Cabin','SibSp','Parch'])\n    \n    \n    return df\n\nmodified_df = dataset_adjusted(original_df)\nmodified_df","29c61c9e":"# Scatter matrix\n\n#pd.plotting.scatter_matrix(modified_df,figsize=(20,20))\n","ce8251de":"# Correlation heatmap\n\nsns.heatmap(modified_df.corr())","c9bc6663":"#sns.catplot(y=\"Fare_Age\",x=\"Survived\",col='Pclass',data=modified_df,kind='violin')","a934df32":"#sns.catplot(x=\"IsAlone\",hue=\"Survived\",col='Pclass',data=modified_df,kind='count')","3378e486":"# Scaling our data\nfrom sklearn.preprocessing import StandardScaler\n\nx = modified_df.drop(columns=['Survived'])\ny = modified_df.Survived\n\nscaler = StandardScaler()\nscaler.fit(x)\nx = pd.DataFrame(scaler.transform(x),columns=x.columns)","801ddb93":"# Training Model for Predicitons\n\nfrom sklearn.svm import SVC\n\n#model_1 = SVC(C=100, gamma=0.01)\n#model_1.fit(x,y)\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel_2 = RandomForestClassifier(bootstrap=True,max_depth=100, max_features=3, min_samples_leaf=3, min_samples_split=10)\nmodel_2.fit(x,y)\n\n#from sklearn.ensemble import GradientBoostingClassifier\n#model_3 = GradientBoostingClassifier(n_estimators=1000, learning_rate=0.01)\n#model_3.fit(x,y)\n\nfrom sklearn.model_selection import cross_val_score\ncross_val_score(model_2,x,y,cv=5,scoring=\"accuracy\")","6c00dbd5":"# get importance\nimportance = model_2.feature_importances_\n# summarize feature importance\nfor i,v in enumerate(importance):\n\tprint('Feature: {}, Score: {}'.format(x.columns[i],v))\n# plot feature importance\nplt.bar([x for x in range(len(importance))], importance)\nplt.show()","a1a597ed":"test_df = pd.read_csv(r'..\/input\/titanic\/test.csv')","e0c8e90d":"test_df.Fare.fillna(test_df.Fare.median(),inplace=True)\nmodified_test = dataset_adjusted(test_df)\nmodified_test.info()","a8225ea5":"modified_test = pd.DataFrame(scaler.transform(modified_test),columns=modified_test.columns)","88610bc6":"predictions = model_2.predict(modified_test)","8980db5d":"submission = pd.concat([pd.DataFrame(test_df.PassengerId),pd.DataFrame(predictions,columns=['Survived'])],axis=1).set_index('PassengerId')","44ab1333":"submission.to_csv('submission.csv')","a3c15796":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nnames = [\n    \"Nearest Neighbors\",\n    \"Linear SVM\",\n    \"RBF SVM\",\n    \"Gaussian Process\",\n    \"Decision Tree\",\n    \"Random Forest\",\n    \"Neural Net\",\n    \"AdaBoost\",\n    \"Naive Bayes\",\n    \"QDA\",\n]\n\nclassifiers = [\n    KNeighborsClassifier(),\n    SVC(),\n    GaussianProcessClassifier(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    MLPClassifier(),\n    AdaBoostClassifier(),\n    GaussianNB(),\n    QuadraticDiscriminantAnalysis(),\n]\n\nfrom sklearn.model_selection import cross_val_score\n\nvalores = []\nfor modelo in classifiers:\n    valores.append(cross_val_score(modelo,x,y,cv=5,scoring=\"accuracy\"))","7e282637":"# **Choosing a model**\n\nWe will use different models and cross validdation to choose the best model for our dataset","f9d2a658":"# Tuning Hyperparameters\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\n#param_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001]}\nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [100, 200, 300, 1000]\n}\ngrid = GridSearchCV(SVC(),param_grid,verbose=2,cv=3,scoring=\"accuracy\")\ngrid.fit(x,y)\n\nprint(grid.best_estimator_)","7bcbc542":"# **1. First glance at the data**\n\nIn this step we will look at how the data is structured, if there are missing values and a few statistics from the variables","2c804c4b":"# **2. Correcting, Completing, Creating and Converting**\n\n* Correcting: Fix aberrant values. Use with caution\n* Completing: Input missing values\n* Creating: Feature Enginnering\n* Converting: Categorical vvariables to numeric","f0a6fcaa":"# **Exploring our Data**\n\n* Before building our model, let\u00b4s have another lot at our dataset after the chenges we've made","7c648562":"Done! Here are some thoughts about what we have:\n* We have 11 feature variables - 1 target (Survived)\n\n* There are a few missing values in the variables \"Embarked\" and \"Age\"\n* There are lots of missing values on \"Cabin\" - I'm afraid this variable will be no use, but we let\u00b4s dive into that later\n\n* \"PassengerID\", \"Name\", \"Ticket\" have lots of unique values, this will not help us at all.\n* We could get the title from the \"Name\" variable, the others variables will have no use.\n\n* We have some outliers on the \"Fare\" variable, we need to look into it deeper."}}