{"cell_type":{"f48c4c8c":"code","b72f559b":"code","8adbef9f":"code","a662a619":"code","2556f4c4":"code","6517a5d0":"code","333823e0":"code","d945ece9":"code","82ca0433":"code","8c924afb":"code","64f0ad25":"code","da6c3e8d":"code","9acd88af":"code","e1fbde5f":"code","927229ad":"code","f6799d19":"code","6644ed4f":"code","57da71d0":"code","d0fd2d3f":"code","9370aa71":"code","aa9e07b3":"code","49774396":"code","ea779985":"code","e31fa1bc":"code","477171d6":"code","12c8f80a":"code","2c17dbea":"code","d8b70323":"code","7711e79e":"code","4e7ad04a":"code","8d61b6f6":"code","e77c0afb":"code","96f3448e":"code","514c5844":"code","aa1bb879":"code","3788cc0b":"code","3dfd6d78":"code","a8dd52c0":"code","45441d57":"code","f6cde79e":"code","ffdb439a":"code","b18da7d8":"code","cf7e6bfb":"code","fee562ca":"code","11d46c17":"markdown","cef4e556":"markdown","5a051b4c":"markdown","20f8f553":"markdown","ce1a4079":"markdown","518384b1":"markdown","d60e9f6d":"markdown","7befdca9":"markdown","9f50b94d":"markdown","76f8b952":"markdown","20aba49a":"markdown","d0721aa1":"markdown","d34d9f26":"markdown","dd83fe2f":"markdown","ae72e94f":"markdown","5ae0a85b":"markdown","e264cc77":"markdown","87d914b8":"markdown","1d790d2a":"markdown","a1a1ed27":"markdown","00b61e02":"markdown","46effb01":"markdown","49162300":"markdown","d0f7c9b9":"markdown","d1bd8fef":"markdown","aaeb23c7":"markdown","9abc5de0":"markdown","05d04785":"markdown","4848e85a":"markdown","a10f0c25":"markdown","4a8b5894":"markdown","8be5789f":"markdown","dea4f980":"markdown","3b2257ad":"markdown","b7a9e8b4":"markdown","d11ea28b":"markdown","4f08fe05":"markdown","44308c87":"markdown","39b38fca":"markdown","dc5f6aa5":"markdown","ff2ca1bb":"markdown","7dd9ee67":"markdown","5a64fd67":"markdown","f56a1470":"markdown","5a98bdfe":"markdown","6127a201":"markdown","fc63d7ae":"markdown"},"source":{"f48c4c8c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics as metrics\nfrom sklearn.preprocessing import PolynomialFeatures ","b72f559b":"df = pd.read_csv('https:\/\/raw.githubusercontent.com\/satishgunjal\/datasets\/master\/Fish.csv')\nprint('Dimension of dataset= ', df.shape)\ndf.head(5) # head(n) returns first n records only. Can also use sample(n) for random n records.","8adbef9f":"df1 = df.rename(columns={'Length1':'VerticalLen','Length2':'DiagonalLen','Length3':'CrossLen'})\ndf1.sample(5) # Display random 5 records","a662a619":"df1.info()","2556f4c4":"df1.corr()","6517a5d0":"plt.rcParams[\"figure.figsize\"] = (10,6) # Custom figure size in inches\nsns.heatmap(df1.corr(), annot =True)\nplt.title('Correlation Matrix')","333823e0":"df2 = df1.drop(['VerticalLen', 'DiagonalLen', 'CrossLen'], axis =1) # Can also use axis = 'columns'\nprint('New dimension of dataset is= ', df2.shape)\ndf2.head(3)","d945ece9":"sns.pairplot(df2, kind = 'scatter', hue = 'Species')","82ca0433":"def outlier_detection(dataframe):\n  \"\"\"\n  Find the outlier in given dataset. To get the index fo the outlier data, please input single column dataframe\n\n  Input Parameters\n  ----------------\n  dataframe : single column dataframe\n  \n  Output Parameters\n  -----------------\n  outlier : Index of outlier training examples.\n  \"\"\"\n  Q1 = dataframe.quantile(0.25)\n  Q3 = dataframe.quantile(0.75)\n  IQR = Q3 - Q1\n  upper_end = Q3 + 1.5 * IQR\n  lower_end = Q1 - 1.5 * IQR \n  outlier = dataframe[(dataframe > upper_end) | (dataframe < lower_end)]\n  return outlier","8c924afb":"sns.boxplot(data= df2['Weight'] )\nplt.title('Outlier Detection Based on Weight')","64f0ad25":"# 'Species' column contains categorical values, so using list slicing to iterate over all the columns except first one\nfor column in df2.columns[1:]: \n    print('\\nOutliers in column \"%s\" ' % column)\n    outlier = outlier_detection(df2[column])\n    print(outlier)","da6c3e8d":"#Lets create temp dataframe without 'Weight' feature for plotting the boxplot\ndf_temp = df2.drop(['Weight'], axis = 'columns')","9acd88af":"sns.boxplot(data= df_temp[df_temp.Species == 'Perch'] )\nplt.title('Outlier Detection For Pearch Species')","e1fbde5f":"df_Perch = df2[df2.Species == 'Perch']\nfor column in df_Perch.columns[1:]: \n    print('\\nOutliers in column \"%s\" ' % column)\n    outlier = outlier_detection(df_Perch[column])\n    print(outlier)","927229ad":"sns.boxplot(data= df_temp[df_temp.Species == 'Bream'] )\nplt.title('Outlier Detection For Bream Species')","f6799d19":"df_Bream = df2[df2.Species == 'Bream']\nfor column in df_Bream.columns[1:]: \n    print('\\nOutliers in column \"%s\" ' % column)\n    outlier = outlier_detection(df_Bream[column])\n    print(outlier)","6644ed4f":"sns.boxplot(data= df_temp[df_temp.Species == 'Roach'] )\nplt.title('Outlier Detection For Roach Species')","57da71d0":"df_Roach = df2[df2.Species == 'Roach']\nfor column in df_Roach.columns[1:]: \n    print('\\nOutliers in column \"%s\" ' % column)\n    outlier = outlier_detection(df_Roach[column])\n    print(outlier)","d0fd2d3f":"sns.boxplot(data= df_temp[df_temp.Species == 'Pike'] )\nplt.title('Outlier Detection For Pike Species')","9370aa71":"df_Pike = df2[df2.Species == 'Pike']\nfor column in df_Pike.columns[1:]: \n    print('\\nOutliers in column \"%s\" ' % column)\n    outlier = outlier_detection(df_Pike[column])\n    print(outlier)","aa9e07b3":"sns.boxplot(data= df_temp[df_temp.Species == 'Smelt'] )\nplt.title('Outlier Detection For Smelt Species')","49774396":"df_Smelt = df2[df2.Species == 'Smelt']\nfor column in df_Smelt.columns[1:]: \n    print('\\nOutliers in column \"%s\" ' % column)\n    outlier = outlier_detection(df_Smelt[column])\n    print(outlier)","ea779985":"sns.boxplot(data= df_temp[df_temp.Species == 'Parkki'] )\nplt.title('Outlier Detection For Parkki Species')","e31fa1bc":"df_Parkki = df2[df2.Species == 'Parkki']\nfor column in df_Parkki.columns[1:]: \n    print('\\nOutliers in column \"%s\" ' % column)\n    outlier = outlier_detection(df_Parkki[column])\n    print(outlier)","477171d6":"sns.boxplot(data= df_temp[df_temp.Species == 'Whitefish'] )\nplt.title('Outlier Detection For Whitefish Species')","12c8f80a":"df_Whitefish = df2[df2.Species == 'Whitefish']\nfor column in df_Whitefish.columns[1:]: \n    print('\\nOutliers in column \"%s\" ' % column)\n    outlier = outlier_detection(df_Whitefish[column])\n    print(outlier)","2c17dbea":"df3 = df2.drop([35,54,157,158])\ndf3.shape","d8b70323":"df3.isna().sum()","7711e79e":"df3[df3.Weight <= 0]","4e7ad04a":"df4 = df3.drop([40])\ndf4.shape","8d61b6f6":"dummies_species = pd.get_dummies(df4.Species) # store the dummy variables in 'dummies_species' dataframe\ndummies_species.head(3) # To do get individual dummy value","e77c0afb":"df5 = pd.concat([df4, dummies_species],axis = 'columns')\ndf5.head(3)","96f3448e":"df6 = df5.drop(['Species','Whitefish'], axis = 'columns')\ndf6.head(3)","514c5844":"X = df6[['Height', 'Width', 'Bream', 'Parkki' ,'Perch', 'Pike', 'Roach', 'Smelt']] # Or can use df6.iloc[:,[1,2,3,4,5,6,7,8]]\ny = df6[['Weight']]","aa1bb879":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2)\nprint('X_train dimension= ', X_train.shape)\nprint('X_test dimension= ', X_test.shape)\nprint('y_train dimension= ', y_train.shape)\nprint('y_train dimension= ', y_test.shape)","3788cc0b":"def polynomial_plot(feature, label):\n  # Create 1D array. We can use 'squeeze' function to reduce the 2D array to 1D array\n  x_coordinates = feature\n  y_coordinates = np.squeeze(label)\n\n # Contruct first degree polynomial function\n  linear_func = np.poly1d(np.polyfit(x_coordinates, y_coordinates, 1))\n  # Contruct second degree polynomial function\n  quadratic_func = np.poly1d(np.polyfit(x_coordinates, y_coordinates, 2))\n \n  # Generate evenly spaced values\n  values = np.linspace(x_coordinates.min(), x_coordinates.max(), len(x_coordinates))\n\n  plt.scatter(x_coordinates,y_coordinates, color='blue')  \n  plt.plot(values, linear_func(values), color='cyan', linestyle='dashed', label='Linear Function')\n  plt.plot(values, quadratic_func(values), color='red', label='Quadratic Function')\n  plt.xlabel('%s From Test Data'%(feature.name))\n  plt.ylabel('Weight')\n  plt.rcParams[\"figure.figsize\"] = (10,6) # Custom figure size in inches\n  plt.legend()\n  plt.title(\"Linear Vs Quadratic Function For Feature %s\" % (feature.name))\n  plt.show()  ","3dfd6d78":"polynomial_plot(X_train.Width, y_train)","a8dd52c0":"polynomial_plot(X_train.Height, y_train)","45441d57":"poly = PolynomialFeatures(degree = 2) \nX_poly = poly.fit_transform(X_train) \npoly.fit(X_poly, y_train) ","f6cde79e":"lm = linear_model.LinearRegression() \nlm.fit(X_poly, y_train) ","ffdb439a":"predictions = lm.predict(poly.fit_transform(X_test))\nprint('r2_score= ', metrics.r2_score(y_test, predictions))","b18da7d8":"predictedWeight = pd.DataFrame(predictions, columns=['Predicted Weight']) # Create new dataframe of column'Predicted Weight'\nactualWeight = pd.DataFrame(y_test)\nactualWeight = actualWeight.reset_index(drop=True) # Drop the index so that we can concat it, to create new dataframe\ndf_actual_vs_predicted = pd.concat([actualWeight,predictedWeight],axis =1)\ndf_actual_vs_predicted.T","cf7e6bfb":"plt.scatter(y_test, predictions)\nplt.xlabel('Weight From Test Data')\nplt.ylabel('Weight Predicted By Model')\nplt.rcParams[\"figure.figsize\"] = (10,6) # Custom figure size in inches\nplt.title(\"Weight From test Data Vs Weight Predicted By Model\")","fee562ca":"sns.distplot((y_test-predictions))\nplt.rcParams[\"figure.figsize\"] = (10,6) # Custom figure size in inches\nplt.title(\"Histogram of Residuals\")","11d46c17":"# Step 6: Evaluating the Model\n\nPlot a histogram of the residuals.","cef4e556":"## Understanding Training Results\n* If training is successful then we get the result like above. Where all the default values used by LinearRgression() model are displayed. If required we can also pass these values in fit method. We are not going to change any of these values for now.","5a051b4c":"# Step 5: Build Machine Learning Model","20f8f553":"### Outlier detection for Roach species","ce1a4079":" \n# Step 3: Understand The Data\n* There are total 159 rows(training samples) and 7 columns in dataset. \n* Each column details are as below \n \n| Column Name | Details\n| ------------|--------------\n| Species     | Species name of fish \n| Weight      | Weight of fish in gram     \n| Length1     | Vertical length in CM\n| Length2     | Diagonal length in CM\n| Length3     | Cross length in CM\n| Height      | Height in CM\n| Width       | Diagonal width in CM   \n \n* Features\/input values\/independent variables are 'Species', 'Length1','Length2', 'Length3', 'Height' and 'Width'\n* Target\/output value\/dependent variable is 'Weight'\n* So, we have to estimate the weight of the fish based on its measurement values.\n \nLet's change the name of columns lenght1,length2 and length3  as per the content of it.","518384b1":"From above first and second degree polynomial plots, it's clear that second degree polynomial feature will provide better fit.\n \nNote: If you are interested you can make few changes in function 'polynomial_plot()' and plot higher order polynomial for testing. Just be careful of 'Overfitting' the training data.","d60e9f6d":"There are no outliers for 'Pearch' species","7befdca9":"Lets plot first and second degree polynomical for feature 'Width'.","9f50b94d":"# Steps 2: Load The Data","76f8b952":"As you can see only row 54 and 35 are common across majority features, so we can safely remove them","20aba49a":"# Step 1: Import The Required Files\n* numpy : Numpy is the core library for scientific computing in Python. It is used for working with arrays and matrices.\n* pandas: Used for data manipulation and analysis\n* matplotlib : It\u2019s plotting library, and we are going to use it for data visualization\n* seaborn : It is also data visualization library, based on matplotlib\n* linear_model: Sklearn linear regression model\n* train_test_split : helper function from Sklearn library for splitting the dataset\n* sklearn.metrics : Library encapsulate functions to measure the model performance\n* PolynomialFeatures : This used to generate polynomial features. In this study we are going to generate second degree features.","d0721aa1":"## Polynomial Regression Using Sklearn Library\n* We are going to use second degree polynomial feature, which will give us quadratic equation to fit the data.\n* I have also tried this problem without using polynomial features. [Multiple Linear Regression Fish Weight Prediction](https:\/\/www.kaggle.com\/satishgunjal\/multiple-linear-regression-fish-weight-prediction)\n\n","d34d9f26":"No outlier for Whitefish species","dd83fe2f":"## Predicting The Test Data\n* Check below table for weight from test data and predicted weight by our model\n* We will also plot the scatter plot of weight from test data vs predicted weight","ae72e94f":"## One Hot Encoding\n* Since Species is categorical value, we are going to use One Hot Encoding to convert it into numerical format.\n* For more details about it please refer [One Hot Encoding](https:\/\/satishgunjal.com\/one_hot_encoding\/)","5ae0a85b":"# Conclusion\n* As you can see from above results our model score is over 90%, which is very good.\n* And there are no negative weight values\n* So our approach of finding the outlier based on species and using second degree polynomial features worked!","e264cc77":"## Reading Correlation Matrix \n* Correlation coefficient range from -1 to +1\n* Sign(+\/-) indicate the direction and amount indicate the strength of correlation\n* +1.00 means perfect positive relationship\n* 0.00 means no relationship\n* -1.00 means perfect negative relationship\n* The correlation between 'VerticalLen', 'DiagonalLen' and 'Crosslen' is almost 1. This may cause 'Multicolinearity'.\n \nLet's drop the 'VerticalLen', 'DiagonalLen' and 'Crosslen' column.","87d914b8":"Now lets create new dataframe using dummies_species dataframe","1d790d2a":"![fish_poly_header.png](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/fish_poly_header.png)\n\n\nIn this study I am using Python 3 environment to create a machine learning model to predict the weight of the fish based on the body measurement data of seven types of fish species. You can download the dataset from Kaggle. [Fish market](https:\/\/www.kaggle.com\/aungpyaeap\/fish-market)\n \nI am going to use Polynomial feature with Linear model from sklearn library. Since there are multiple features its **Polynomial Multiple Linear Regression** problem.\n \nI have documented the code and tried to explain every important concept or library I have used during this study. I hope it will be helpful.","a1a1ed27":"## Visualization Using Polynomial Function\n* Since we have multiple species, the relation between features(Height, Width) and Weight is not linear\n* Line won't be good fit to the given data and may result in under fitting.\n* Lets visualize the data using multiple degrees of polynomial\n* We are going to use below numpy functions to plot different degrees of polynomial functions\n* **numpy.polyfit**: Used to fit a polynomial function of given degrees. It returns the coefficients (which minimizes the squared error) of the polynomial equation\n  - This function takes three arguments to fit a polynomial function to given data. X values, Y values and degrees (1,2,3...n)\n  - Polynomial equation of degree 1 is linear equation: **y = mx + b**\n  - Polynomial equation of degree 2 is quadratic equation: **y = ax^2 + bx + c**\n  - So polyfit function will return the coefficients of the polynomial equation. For linear equation coeff will be [m,b] and for quadratic equation coeff will be [a,b,c]\n* **numpy.poly1d**: Is used to define the polynomial function using the coefficients returned by 'numpy.polyfit'\n  - It takes polynomial coefficients as argument and construct a polynomial.\n  - For e.g. if there are three coeff then it will construct quadratic equation\n* **numpy.linspace**: Takes three arguments (start, stop and num) and generates evenly spaced values(same as 'num') within 'start' and 'stop' range\n* Finaly to draw a polynomial function we will plot 'values' generated 'linspace' function on X axis and on Y axis 'poly_func' output for every 'values'","00b61e02":"## Outlier Detection and Removal\n \n* Outlier is an extremely high or extremely low value in our data\n* We use below formula to identify the outlier\n  ```\n    ( Greater than Q3 + 1.5 * IQR ) OR ( Lower than Q1 -1.5 * IQR )\n \n    where,\n    Q1  = First quartile\n    Q3  = Third quartile\n    IQR = Interquartile range (Q3 - Q1)\n  ```\n \n* We will use box plot for outlier visualization. \n* Vertical line on the left side of box plot represent the 'min' value of dataset and vertical line on right side of box plot represent the 'max' value of dataset. Any value which is outside this range is outlier and represented by '*'\n \nLets write function for outlier detection","46effb01":"### Outlier detection for Whitefish species","49162300":"## Create Feature Matrix X and Label Vector y\nFor more details about creating training and test sets please refer [Train Test Split](https:\/\/satishgunjal.com\/train_test_split\/)","d0f7c9b9":"## Ordinary Least Squares Algorithm\n\n* Lets the train the model using Ordinary Least Squares Algorithm\n* This is one of the most basic linear regression algorithm.\n* Mathematical formula used by ordinary least square algorithm is as below,\n\n   ![ordinary_least_squares_formlua.png](https:\/\/github.com\/satishgunjal\/images\/blob\/master\/ordinary_least_squares_formlua_1.png?raw=true)\n* The objective of Ordinary Least Square Algorithm is to minimize the residual sum of squares. Here the term residual means 'deviation of predicted value(Xw) from actual value(y)'\n* Note that, problem with ordinary least square model is size of coefficients increase exponentially with increase in model complexity","d1bd8fef":"As you can see only rows 157 and 158 are common across multiple features, so we can safely remove them","aaeb23c7":"* For features Weight, VerticalLen and DiagonalLen index number 142, 143 and 144 are the outliers.\n* For feature CrossLen index 144 is outlier\n* For features Height and Width there are no outliers\n* Important thing to note is, we haven't consider the individual fish species count and physical properties while finding the outliers. For e.g. Pike are bigger and Smelt are smaller fish species. We have 56 training examples for 'Perch' and only 6 training examples for 'Whitefish' fish species.\n* Though we have small dataset, species wise outlier detection will help to make our model more accurate.\n \nSo lets find the outlier for each species.","9abc5de0":"Lets find the outliers for every feature column","05d04785":"### Outlier detection for Perch species","4848e85a":"No outliers for Parkki species","a10f0c25":"### Outlier detection for Smelt species","4a8b5894":"### Outlier detection for Pike species","8be5789f":"### Dropping the outlier rows\n* If we don't consider species specific measurement then outlier are 142, 143 and 144\n* And as per species specific measurement outliers are 35, 54, 157 and 158\n* Lets drop the species specific outliers only","dea4f980":"Let's print the detailed information about our dataset","3b2257ad":"Since we have dummy variables we can drop the 'Species' column and to avoide the 'Dummy Variable Trap' problem we will also drop 'Whitefish' column","b7a9e8b4":"## Check for null values","d11ea28b":"## Visulization Using Pairplot","4f08fe05":"No outlier for Pike species","44308c87":"### Outlier detection for Parkki species","39b38fca":"Lets plot first and second degree polynomical for feature 'Height'.","dc5f6aa5":"## Correlation Check\n* Correlation helps us investigate and establish relationships between variables\n* Note that high amount of correlation between independent variables suggest that linear regression estimation will be unreliable\n","ff2ca1bb":"## Model Score\nR-squared is statistical measure of how close the data to the fitted regression line. It is also known as coefficient of determination but since we have multiple features here we can also call it 'coefficient of multiple determination'","7dd9ee67":"From the above pair plot, we can see that there seems to be some correlations between  Height, Width and the Weight. Note that since we have multiple species the correlation between Height and Width of all species is not exactly linear with Weight.\n\nNow, since we have the final dataset ready lets analyze and remove the outliers if any","5a64fd67":"There are no outliers for 'Perch' fish species","f56a1470":"### Outlier detection based on Weight","5a98bdfe":"## Using Domain Knowledge For Data Cleaning\nNow lets use some common sense and find and remove the training data where weight of fish is 0 or negative","6127a201":"# Step 4: Data Analysis Cleaning and Visualization","fc63d7ae":"### Outlier detection for Bream species"}}