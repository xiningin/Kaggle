{"cell_type":{"7cdeb3aa":"code","761bec4c":"code","23722742":"code","50cc41b6":"code","82df8d8c":"code","169c794e":"code","a6f77f67":"code","d46c71f2":"code","0587c809":"code","079ebe7c":"code","ed99adb8":"code","26fc8a4a":"code","98d8cef3":"code","16f2a877":"code","d8a62985":"code","05702d29":"code","cdb7ad3c":"code","7d2bd0ed":"code","f260c65e":"code","8c97c8ba":"code","6b6a2a25":"code","8b8d417f":"code","9de48c3e":"code","1c2d86f9":"code","28bbdbba":"code","08c8dae4":"code","0b5a4505":"code","a354e557":"code","2ad1fbe4":"code","fe2476f0":"code","abb82c86":"code","7349ddf6":"code","ff49b21e":"code","4301c985":"code","93ec24ee":"code","47a01418":"markdown","92711bc8":"markdown","969c3641":"markdown","ad81ea1f":"markdown","9689ad33":"markdown","2e5a9af6":"markdown","7c1d9ac0":"markdown","c7808b8a":"markdown","2d4c38be":"markdown","25b782d9":"markdown","03162bc4":"markdown","3a3a7571":"markdown"},"source":{"7cdeb3aa":"!pip install image-classifiers==1.0.0b1\n!pip install -U efficientnet","761bec4c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport cv2\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow as tf\n\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\nfrom keras.utils.np_utils import to_categorical\nfrom keras.layers import Dropout, Flatten\nfrom tensorflow.keras.layers import Convolution2D,Activation,GlobalAveragePooling2D,MaxPooling2D,Flatten,Dense,Dropout,Input,Reshape,Lambda\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nimport cv2\nfrom sklearn.model_selection import train_test_split\nimport pickle\nimport os\nimport pandas as pd\nimport random\nimport keras\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping,ReduceLROnPlateau\nfrom keras.models import Model\nfrom classification_models.keras import Classifiers\nimport efficientnet.keras as efn \nimport math\nfrom keras.callbacks import TensorBoard\nfrom efficientnet.tfkeras import EfficientNetB4\nimport tensorflow.keras.layers as L\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('fivethirtyeight')\nplt.rcParams['figure.figsize'] = [16, 8]\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nprint('Using Tensorflow version:', tf.__version__)","23722742":"try:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","50cc41b6":"# For tf.dataset\nAUTO = tf.data.experimental.AUTOTUNE\n\n# Data access\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()\n\n# Configuration\nEPOCHS = 10\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync","82df8d8c":"train_df = pd.read_csv('..\/input\/shopee-product-detection-student\/train.csv')\ntest_df = pd.read_csv('..\/input\/shopee-product-detection-student\/test.csv')\n\n\ntrain_df.shape, test_df.shape","169c794e":"train_df.head()\n","a6f77f67":"def show_train_img(category):\n    \n    fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(24, 10))\n    \n    train_path = '..\/input\/shopee-product-detection-student\/train\/train\/train\/'\n    ten_random_samples = pd.Series(os.listdir(os.path.join(train_path, category))).sample(10).values\n    \n    for idx, image in enumerate(ten_random_samples):\n        final_path = os.path.join(train_path, category, image)\n        img = cv2.imread(final_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        axes.ravel()[idx].imshow(img)\n        axes.ravel()[idx].axis('off')\n    plt.tight_layout()","d46c71f2":"show_train_img('01')","0587c809":"def show_test_img():\n    \n    fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(24, 10))\n    \n    test_path = '..\/input\/shopee-product-detection-student\/test\/test\/test'\n    ten_random_samples = pd.Series(os.listdir(test_path)).sample(10).values\n    \n    for idx, image in enumerate(ten_random_samples):\n        final_path = os.path.join(test_path, image)\n        img = cv2.imread(final_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        axes.ravel()[idx].imshow(img)\n        axes.ravel()[idx].axis('off')\n    plt.tight_layout()","079ebe7c":"show_test_img()","ed99adb8":"# pick random samples\n\ndataset_path = {}\n\ncategories = np.sort(train_df['category'].unique())\n\nfor cat in categories:\n    try:\n        dataset_path[cat] = train_df[train_df['category'] == cat]['filename'].sample(2400)\n    except:\n        dataset_path[cat] = train_df[train_df['category'] == cat]['filename'].sample(frac=1.)","26fc8a4a":"category_list = ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09',\n                 '10', '11', '12', '13', '14', '15', '16', '17', '18', '19',\n                 '20', '21', '22', '23', '24', '25', '26', '27', '28', '29',\n                 '30', '31', '32', '33', '34', '35', '36', '37', '38', '39',\n                 '40', '41']","98d8cef3":"train_paths = []\n\nfor idx, key in enumerate(dataset_path.keys()):\n    if key == idx:\n        for path in dataset_path[idx]:\n            train_paths.append(os.path.join(GCS_DS_PATH, 'train', 'train', 'train', category_list[idx], path))","16f2a877":"labels = []\n\nfor label in dataset_path.keys():\n    labels.extend([label] * len(dataset_path[label]))","d8a62985":"from tensorflow.keras.utils import to_categorical\n\n# convert to numpy array\ntrain_paths = np.array(train_paths)\n\n# convert to one-hot-encoding-labels\ntrain_labels = to_categorical(labels)","05702d29":"from sklearn.model_selection import train_test_split\n\ntrain_paths, valid_paths, train_labels, valid_labels = train_test_split(train_paths, \n                                                                        train_labels, \n                                                                        stratify=train_labels,\n                                                                        test_size=0.2, \n                                                                        random_state=2020)\n\ntrain_paths.shape, valid_paths.shape, train_labels.shape, valid_labels.shape","cdb7ad3c":"test_paths = []\n\nfor path in test_df['filename']:\n    test_paths.append(os.path.join(GCS_DS_PATH,  'test', 'test', 'test', path))    \ntest_paths = np.array(test_paths)","7d2bd0ed":"def decode_image(filename, label=None, image_size=(512, 512)):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0\n    image = tf.image.resize(image, image_size)\n    \n    if label is None:\n        return image\n    else:\n        return image, label","f260c65e":"def data_augment(image, label=None):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_brightness(image, max_delta=0.5)\n    image = tf.image.random_contrast(image, lower=0.7, upper=1.3)\n#     image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n#     image = tf.image.random_hue(image, max_delta=0.2)\n    \n    if label is None:\n        return image\n    else:\n        return image, label","8c97c8ba":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_paths, train_labels))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .map(data_augment, num_parallel_calls=AUTO)\n    .cache()\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((valid_paths, valid_labels))\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(test_paths)\n    .map(decode_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n)","6b6a2a25":"import keras.backend as K\n\ndef categorical_focal_loss_with_label_smoothing(gamma=2.0, alpha=0.25,ls=0.1, classes=42.0):\n    \"\"\"\n    Implementation of Focal Loss from the paper in multiclass classification\n    Formula:\n        loss = -alpha*((1-p)^gamma)*log(p)\n        y_ls = (1 - \u03b1) * y_hot + \u03b1 \/ classes\n    Parameters:\n        alpha -- the same as wighting factor in balanced cross entropy\n        gamma -- focusing parameter for modulating factor (1-p)\n        ls    -- label smoothing parameter(alpha)\n        classes     -- No. of classes\n    Default value:\n        gamma -- 2.0 as mentioned in the paper\n        alpha -- 0.25 as mentioned in the paper\n        ls    -- 0.1\n        classes     -- 4\n    \"\"\"\n    def focal_loss(y_true, y_pred):\n        # Define epsilon so that the backpropagation will not result in NaN\n        # for 0 divisor case\n        epsilon = K.epsilon()\n        # Add the epsilon to prediction value\n        #y_pred = y_pred + epsilon\n        #label smoothing\n        y_pred_ls = (1 - ls) * y_pred + ls \/ classes\n        # Clip the prediction value\n        y_pred_ls = K.clip(y_pred_ls, epsilon, 1.0-epsilon)\n        # Calculate cross entropy\n        cross_entropy = -y_true*K.log(y_pred_ls)\n        # Calculate weight that consists of  modulating factor and weighting factor\n        weight = alpha * y_true * K.pow((1-y_pred_ls), gamma)\n        # Calculate focal loss\n        loss = weight * cross_entropy\n        # Sum the losses in mini_batch\n        loss = K.sum(loss, axis=1)\n        return loss\n    \n    return focal_loss","8b8d417f":"import tensorflow as tf\n\ndef outer_product(x):\n    #Einstein Notation  [batch,1,1,depth] x [batch,1,1,depth] -> [batch,depth,depth]\n    phi_I = tf.einsum('ijkm,ijkn->imn',x[0],x[1])\n    \n    # Reshape from [batch_size,depth,depth] to [batch_size, depth*depth]\n    phi_I = tf.reshape(phi_I,[-1,x[0].shape[3]*x[1].shape[3]])\n    \n    # Divide by feature map size [sizexsize]\n    size1 = int(x[1].shape[1])\n    size2 = int(x[1].shape[2])\n    phi_I = tf.divide(phi_I, size1*size2)\n    \n    # Take signed square root of phi_I\n    y_ssqrt = tf.multiply(tf.sign(phi_I),tf.sqrt(tf.abs(phi_I)+1e-12))\n    \n    # Apply l2 normalization\n    z_l2 = tf.nn.l2_normalize(y_ssqrt, axis=1)\n    return z_l2","9de48c3e":"from keras import backend as K\n\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","1c2d86f9":"EPOCHS = 40\nLR_START = 0.0001\nLR_MAX = 0.00005 * strategy.num_replicas_in_sync\nLR_MIN = 0.0001\nLR_RAMPUP_EPOCHS = 10\nLR_SUSTAIN_EPOCHS = 4\nLR_EXP_DECAY = .8\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) \/ LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n\nlr = tf.keras.callbacks.LearningRateScheduler(lrfn)\n\ny = [lrfn(x) for x in range(EPOCHS)]\nplt.plot(y)","28bbdbba":"%%time\n\n# with strategy.scope(): # Call TPU\n#     input_tensor = Input(shape=(512,512,3))\n#     model1 = efn.EfficientNetB7(weights='imagenet', include_top=False, input_tensor=input_tensor,input_shape=(512, 512, 3))\n#     model2 = efn.EfficientNetB7(weights='noisy-student', include_top=False, input_tensor=input_tensor,input_shape=(512, 512, 3))\n#     for i, layer in enumerate(model1.layers):\n#         layer._name = 'model1_' + layer.name\n#         layer.trainable = False #Freeze all layers\n#     last_layer1 = model1.get_layer('model1_top_conv')\n#     last_output1 = last_layer1.output\n#     for i, layer in enumerate(model2.layers):\n#         layer._name = 'model2_' + layer.name\n#         layer.trainable = False #Freeze all layers\n#     last_layer2 = model2.get_layer('model2_top_conv')\n#     last_output2 = last_layer2.output\n    \n    \n#     model1_ = Model(inputs=model1.input, outputs=last_output1)\n#     model2_ = Model(inputs=model2.input, outputs=last_output2)\n#     model1_.compile(Adam(lr=0.0003, decay=1e-3),loss=categorical_focal_loss_with_label_smoothing(gamma=2.0, alpha=0.75, ls=0.125, classes=42.0))\n#     model2_.compile(Adam(lr=0.0003, decay=1e-3),loss=categorical_focal_loss_with_label_smoothing(gamma=2.0, alpha=0.75, ls=0.125, classes=42.0))\n#     d1=model1_.output\n#     d2=model2_.output\n\n#     bilinear = Lambda(outer_product, name='outer_product1')([d1,d2])\n    \n#     predictions=Dense(42, activation='softmax', name='predictions')(bilinear)\n#     model = Model(inputs=model1.input, outputs=predictions)\n#     #opt = Adam(lr=0.0003, decay=1e-3)\n#     model.summary()\n#     model.compile(optimizer=\"adam\", loss=categorical_focal_loss_with_label_smoothing(gamma=2.0, alpha=0.75, ls=0.125, classes=42.0),metrics=[f1,'categorical_accuracy'])\n    \nwith strategy.scope():\n    model = tf.keras.Sequential([efn.EfficientNetB7(input_shape=(512, 512, 3),\n                                                    weights='imagenet',\n                                                    include_top=False),\n                                 L.GlobalAveragePooling2D(),\n                                 L.Dense(42,activation='softmax')])\n    \n    \n        \n    model.compile(optimizer='adam',\n                  loss = 'categorical_crossentropy',\n                  metrics=['categorical_accuracy'])\n    model.summary()","08c8dae4":"train_steps = train_labels.shape[0] \/\/ BATCH_SIZE  # 86428 \/ 128 = 675\nval_steps = valid_labels.shape[0] \/\/ BATCH_SIZE  # 86428 \/ 128 = 675\n\nmc = tf.keras.callbacks.ModelCheckpoint('weights.h5', monitor='loss', save_best_only=True, save_weights_only=True)\n\nhistory = model.fit(\n    train_dataset, \n    steps_per_epoch=train_steps,\n    validation_data=valid_dataset,\n    validation_steps=val_steps,\n    callbacks=[lr, mc],\n    epochs=EPOCHS,\n)","0b5a4505":"for i,layer in enumerate(model.layers):\n  print(\"{}: {}\".format(i,layer))","a354e557":"for layer in model.layers:\n    layer.trainable = True\n    \nhistory = model.fit(\n    train_dataset, \n    steps_per_epoch=train_steps,\n    validation_data=valid_dataset,\n    validation_steps=val_steps,\n    epochs=20,\n)","2ad1fbe4":"model.save_weights(\"model.h5\")","fe2476f0":"def display_training_curves(training, validation, yaxis):\n    if yaxis == \"loss\":\n        ylabel = \"Loss\"\n        title = \"Loss vs. Epochs\"\n    else:\n        ylabel = \"Accuracy\"\n        title = \"Accuracy vs. Epochs\"\n        \n    fig = go.Figure()\n        \n    fig.add_trace(\n        go.Scatter(x=np.arange(1, EPOCHS+1), mode='lines+markers', y=training, marker=dict(color=\"dodgerblue\"),\n               name=\"Train\"))\n    \n    fig.add_trace(\n        go.Scatter(x=np.arange(1, EPOCHS+1), mode='lines+markers', y=validation, marker=dict(color=\"darkorange\"),\n               name=\"Val\"))\n    \n    fig.update_layout(title_text=title, yaxis_title=ylabel, xaxis_title=\"Epochs\", template=\"plotly_white\")\n    fig.show()","abb82c86":"display_training_curves(\n    history.history['categorical_accuracy'], \n    history.history['val_categorical_accuracy'], \n    'accuracy')","7349ddf6":"# Get training and test loss histories\ntraining_loss = history.history['loss']\ntest_loss = history.history['val_loss']\n\n# Create count of the number of epochs\nepoch_count = range(1, len(training_loss) + 1)\n\n# Visualize loss history\nplt.plot(epoch_count, training_loss, 'r--')\nplt.plot(epoch_count, test_loss, 'b-')\nplt.legend(['Training Loss', 'Test Loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show()","ff49b21e":"test_dataset_tta = (\n        tf.data.Dataset\n        .from_tensor_slices(test_paths)\n        .map(decode_image, num_parallel_calls=AUTO)\n        .cache()\n        .map(data_augment, num_parallel_calls=AUTO)\n        .batch(BATCH_SIZE)\n)\n\ntta_times = 5\nprobabilities = []\n\nfor i in range(tta_times+1):\n    print('TTA Number: ', i, '\\n')\n    probabilities.append(model.predict(test_dataset_tta, verbose=1))\n    \ntta_pred = np.mean(probabilities, axis=0)","4301c985":"# change with prediction\ntest_df['category'] = tta_pred.argmax(axis=1)\n\n# then add zero-padding\ntest_df['category'] = test_df['category'].apply(lambda x: str(x).zfill(2))","93ec24ee":"test_df.to_csv('sub_with_tta.csv', index=False)","47a01418":"# SHOPEE CODE LEAGUE 2020\n## Product Detection Challenge","92711bc8":"Scatter plots","969c3641":"### **Stage 2**\n\n**Unfreeze all layers and re-train**\n\nWe can skip this stage if we don't have enough time to train","ad81ea1f":"# Efficient-Net\nIn May 2019, Google published both a very exciting paper and source code for a newly designed CNN called EfficientNet, that set new records for both accuracy and computational efficiency. Here\u2019s the results of EfficientNet, scaled to different block layers (B1, B2, etc) vs. most other popular CNN\u2019s.\n\n![](https:\/\/miro.medium.com\/max\/985\/1*nQ5HYZ1xiIGn092Y5H5SIQ.jpeg)\n\nAs the image shows, EfficientNet tops the current state of the art both in accuracy and in computational efficiency. How did they do this?\n\n## Model scaling\nThey learned that CNN\u2019s must be scaled up in depth, width, and input image resolution together to improve the performance of the model. The scaling method is named compound scaling and suggests that instead of scaling only one model attribute out of depth, width, and resolution; strategically scaling all three of them together delivers better results.\n\nThere is a synergy in scaling depth, width and image-resolution together, and after an extensive grid search derived the theoretically optimal formula of \u201ccompound scaling\u201d using the following co-efficients:\n\nDepth = 1.20\nWidth = 1.10\nResolution = 1.15\nDepth simply means how deep the networks is which is equivalent to the number of layers in it. Width simply means how wide the network is. One measure of width, for example, is the number of channels in a Conv layer whereas Resolution is simply the image resolution that is being passed to a CNN.\n\nIn other words, to scale up the CNN, the depth of layers should increase 20%, the width 10% and the image resolution 15% to keep things as efficient as possible while expanding the implementation and improving the CNN accuracy. This compound scaling formula is used to scale up the EfficientNet from B0-B7\n\n**Swish Activation**\n![](https:\/\/miro.medium.com\/max\/1400\/0*EhAHcCmGOzQUgQ0k)\n\nReLu works pretty well but it got a problem, it nullifies negative values and thus derivatives are zero for all negative values. There are many known alternatives to tackle this problem like leaky ReLu, Elu, Selu etc., but none of them has proven consistent.\n\nGoogle Brain team suggested a newer activation that tends to work better for deeper networks than ReLU which is a Swish activation. They proved that if we replace Swish with ReLu on InceptionResNetV2, we can achieve 0.6% more accuracy on ImageNet dataset.\n\n> Swish(x) = x * sigmoid(x)\n\nThere are other things like MBConv Block etc. If you want to know more details, you can read the articles in reference below","9689ad33":"* Kaggle kernel has a lot of limitation: cannot use ModelCheckpoint, the amount of data that can be trained & training time is limited etc\n* Anyway, this is only baseline for anyone who wants to train a model uses TPU at Kaggle kernel\n* To get a great score, we must use stacking-ensemble method (only for competition, not for real-life)\n![](https:\/\/scontent.fhan5-1.fna.fbcdn.net\/v\/t1.0-9\/104756059_581915206028557_4469050410893255072_n.jpg?_nc_cat=109&_nc_sid=730e14&_nc_ohc=78c28WfAhYwAX92hd4F&_nc_ht=scontent.fhan5-1.fna&oh=c6fa725e8bac7ec144a03bdea75a3248&oe=5F23AC2F)\n\n![](https:\/\/miro.medium.com\/max\/820\/1*Y-629BmgDNFpLumnklJyaA.png)","2e5a9af6":"**Test TPU**","7c1d9ac0":"**Visualize result**","c7808b8a":"**BiLinear Layer (outer_product())**","2d4c38be":"**Focal Loss + Label Smoothing**","25b782d9":"### Make a submission with TTA\n![](https:\/\/preview.ibb.co\/kH61v0\/pipeline.png)\n* TTA is simply to apply different transformations to test image like: rotations, flipping and translations.\n* Then feed these different transformed images to the trained model and average the results to get more confident answer","03162bc4":"**F1 Score**","3a3a7571":"**Plot loss**"}}