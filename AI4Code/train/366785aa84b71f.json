{"cell_type":{"c324a61d":"code","0531bcfa":"code","2cdfeef6":"code","c9560e43":"code","88fec99f":"code","e0de820d":"code","63440a4d":"code","bc0853e3":"code","a2cfa10d":"code","dff40d4a":"code","fb19c64a":"code","4d64cfbf":"code","a9004426":"code","f4e28c7d":"code","7906660d":"code","dd7aa6d1":"code","65a2248b":"code","265e54e1":"code","390d65ba":"code","42997d04":"code","a2e5dfb8":"code","c8a1309f":"code","050a493e":"code","5b5a49f5":"code","1a50ab2e":"code","7b51f099":"code","d818ca33":"code","b1fe7486":"code","67603b73":"code","38bcf73b":"code","27a9beeb":"code","43b1db02":"code","b9424825":"code","04e54827":"code","26e047b4":"code","8533752f":"code","ec2b186c":"code","c84aad9a":"code","bcedb21a":"code","f9e62d0c":"code","b0cc648b":"code","1609f723":"code","95cbf2c7":"code","9d41cac8":"code","a84d0bc2":"code","c9a0e2ee":"code","c6d5c09c":"code","1e0d9a4d":"code","a74aeaea":"code","e0731767":"code","d43aa972":"code","3859c053":"code","a87cfd5f":"code","0a5afb9c":"code","8621f8fb":"code","3306f5d2":"code","27b4d8a7":"code","fee8bba8":"markdown","17cbeb8f":"markdown","95809fce":"markdown","17689ccd":"markdown","86369653":"markdown","c82adba2":"markdown","22cb0b69":"markdown","8a6c9a6c":"markdown","4b32a652":"markdown"},"source":{"c324a61d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0531bcfa":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","2cdfeef6":"submission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndescription = pd.read_table(\"..\/input\/house-prices-advanced-regression-techniques\/data_description.txt\", delim_whitespace=True, error_bad_lines=False)","c9560e43":"train.columns","88fec99f":"train['SalePrice'].describe()","e0de820d":"sns.distplot(train['SalePrice']);","63440a4d":"corrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","bc0853e3":"k = 10 \ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.75)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","a2cfa10d":"train_numeric = train.select_dtypes(include='number')\ntrain_numeric","dff40d4a":"sns.distplot(train_numeric['SalePrice'], bins=50, kde=True, rug=True)","fb19c64a":"train_non_numeric = train.select_dtypes(exclude='number')\n\n\nplt.figure(figsize=(25,7))\nsns.countplot(x=\"SaleCondition\", data=train_non_numeric)","4d64cfbf":"train_non_numeric = train.select_dtypes(exclude='number')\n\n\nplt.figure(figsize=(25,7))\nsns.countplot(x=\"HouseStyle\", data=train_non_numeric)","a9004426":"train_non_numeric = train.select_dtypes(exclude='number')\n\n\nplt.figure(figsize=(25,7))\nsns.countplot(x=\"Functional\", data=train_non_numeric)","f4e28c7d":"train_non_numeric = train.select_dtypes(exclude='number')\n\n\nplt.figure(figsize=(25,7))\nsns.countplot(x=\"LandContour\", data=train_non_numeric)","7906660d":"import math\ndef plot_multiple_countplots(df, cols):\n    num_plots = len(cols)\n    num_cols = math.ceil(np.sqrt(num_plots))\n    num_rows = math.ceil(num_plots\/num_cols)\n        \n    fig, axs = plt.subplots(num_rows, num_cols)\n    \n    for ind, col in enumerate(cols):\n        i = math.floor(ind\/num_cols)\n        j = ind - i*num_cols\n        \n        if num_rows == 1:\n            if num_cols == 1:\n                sns.countplot(x=df[col], ax=axs)\n            else:\n                sns.countplot(x=df[col], ax=axs[j])\n        else:\n            sns.countplot(x=df[col], ax=axs[i, j])\n            \n            \nplot_multiple_countplots(train_non_numeric, ['LandContour', 'HouseStyle', 'Functional', 'SaleCondition'])","dd7aa6d1":"sns.relplot(x='YearBuilt', y='SalePrice', data=train, aspect=2.0)","65a2248b":"train['SaleCondition'].value_counts()","265e54e1":"train['HouseStyle'].value_counts()","390d65ba":"train['RoofStyle'].value_counts()","42997d04":"train['Functional'].value_counts()","a2e5dfb8":"train['SaleType'].value_counts()","c8a1309f":"train_roof_year = train.groupby(['RoofStyle', 'SalePrice'])['YearBuilt'].count().reset_index()\ntrain_roof_year_pivot = train_roof_year.pivot(index='RoofStyle', columns='SalePrice', values='YearBuilt').fillna(0)\nsns.heatmap(train_roof_year_pivot, annot=True, fmt='.0f', cmap=\"YlGnBu\")","050a493e":"cols = ['YearBuilt', 'RoofStyle', 'HouseStyle', 'SaleType', 'Functional', 'SaleCondition', 'SalePrice']\ntrain_test = train[cols]\ntrain_test.head()","5b5a49f5":"numeric_columns = set(train_test.select_dtypes(include=['number']).columns)\nnon_numeric_columns = set(train_test.columns) - numeric_columns\nprint(numeric_columns)\nprint(non_numeric_columns)","1a50ab2e":"for c in non_numeric_columns:\n    cnt = train_test[c].value_counts()\n    small_cnts = list(cnt[cnt < 5].index)\n    \n    s_replace = {}\n    for sm in small_cnts:\n        s_replace[sm] = 'other'\n    \n    train_test[c] = train_test[c].replace(s_replace)\n    train_test[c] = train_test[c].fillna('other')","7b51f099":"from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# we are going to look at feature importances so we like putting random features to act as a benchmark.\ntrain_test['rand0'] = np.random.rand(train_test.shape[0])\ntrain_test['rand1'] = np.random.rand(train_test.shape[0])\ntrain_test['rand2'] = np.random.rand(train_test.shape[0])\n\n# testing for relationships.\n# for numeric targets.\nreg = GradientBoostingRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, loss='ls', random_state=1)\n# for categorical targets.\nclf = GradientBoostingClassifier(n_estimators=100, max_depth=5, learning_rate=0.1, loss='deviance', random_state=1)\n\ntrain_test['YearBuilt'] = train_test['YearBuilt'].fillna(0) # only YearBuilt should have missing values.\n        \n# try to predict one feature using the rest of others to test collinearity, so it's easier to interpret the results\nfor c in cols:\n    # c is the thing to predict.\n    \n    if c not in ['rand0', 'rand1', 'rand2']: \n\n        X = train_test.drop([c], axis=1) # drop the thing to predict.\n        X = pd.get_dummies(X)\n        y = train_test[c]\n\n        print(c)\n\n        if c in non_numeric_columns:\n            scoring = 'accuracy'\n            model = clf\n            scores = cross_val_score(clf, X, y, cv=5, scoring=scoring)\n            print(scoring + \": %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n        elif c in numeric_columns:\n            scoring = 'neg_root_mean_squared_error'\n            model = reg\n            scores = cross_val_score(reg, X, y, cv=5, scoring=scoring)\n            print(scoring.replace('neg_', '') + \": %0.2f (+\/- %0.2f)\" % (-scores.mean(), scores.std() * 2))\n        else:\n            print('what is this?')\n\n        model.fit(X, y)\n        train_importances = pd.DataFrame(data={'feature_name': X.columns, 'importance': model.feature_importances_}).sort_values(by='importance', ascending=False)\n        top5_features = train_importances.iloc[:5]\n        print('top 5 features:')\n        print(top5_features)\n\n        print()","d818ca33":"# SaleType, SaleCondition\n\nsns.relplot(x='SaleCondition', y='SaleType', size='SalePrice', sizes=(10, 1000), data=train, aspect=3.0)","b1fe7486":"missing = train.isnull().sum()\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nmissing.plot.bar()","67603b73":"train.shape","38bcf73b":"test.shape","27a9beeb":"pd.set_option(\"display.max_columns\", 81)","43b1db02":"train.isna().sum().sum()","b9424825":"dataframe_train = train.drop('Id',1)\nY_train = dataframe_train['SalePrice']\ndf_features_train = dataframe_train.drop('SalePrice',1)\n\nprint(Y_train.shape, df_features_train.shape)","04e54827":"df_features_test = test.drop('Id',1)\n\nprint(df_features_test.shape)","26e047b4":"na_total = df_features_train.isnull().sum().sort_values(ascending=False) ## FInding total null values\ndf_features_train.fillna(0,inplace=True)","8533752f":"na_total = df_features_test.isnull().sum().sort_values(ascending=False) ## FInding total null values\ndf_features_test.fillna(0,inplace=True)","ec2b186c":"numeric_cols = [x for x in df_features_train.columns if ('Area' in x) | ('SF' in x)]+['LotFrontage','MiscVal','EnclosedPorch','3SsnPorch','ScreenPorch','OverallQual','OverallCond','YearBuilt']","c84aad9a":"categorical_cols = [x for x in df_features_train.columns if x not in numeric_cols]","bcedb21a":"numeric_cols_test = [x for x in df_features_test.columns if ('Area' in x) | ('SF' in x)]+['LotFrontage','MiscVal','EnclosedPorch','3SsnPorch','ScreenPorch','OverallQual','OverallCond','YearBuilt']","f9e62d0c":"categorical_cols_test = [x for x in df_features_test.columns if x not in numeric_cols_test]","b0cc648b":"train_num = df_features_train[numeric_cols]\ntrain_cat = df_features_train[categorical_cols]","1609f723":"test_num = df_features_test[numeric_cols_test]\ntest_cat = df_features_test[categorical_cols_test]","95cbf2c7":"Y_train = np.log(Y_train)\nY_train.hist()","9d41cac8":"train = pd.concat([train_cat,train_num],axis=1)\ntrain.shape","a84d0bc2":"test = pd.concat([test_cat,test_num],axis=1)\ntest.shape","c9a0e2ee":"train_objs_num = len(train)\nprint(train_objs_num)","c6d5c09c":"dataset = pd.concat(objs=[train, test], axis=0)\ndataset = pd.get_dummies(dataset)\n\ntrain = dataset[:train_objs_num]\ntest = dataset[train_objs_num:]\nprint(train.shape,test.shape)","1e0d9a4d":"from sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.impute import SimpleImputer","a74aeaea":"model = linear_model.LinearRegression()\nmodel.fit(train,Y_train)","e0731767":"prediction =  model.predict(test)\nfinal_prediction = np.exp(prediction)","d43aa972":"print(final_prediction)","3859c053":"main_file_path = '..\/input\/house-prices-advanced-regression-techniques\/test.csv'\ndataframe = pd.read_csv(main_file_path)\ndataframe.head()\ndataframe.info()","a87cfd5f":"dataframe['Id']","0a5afb9c":"Submission = pd.DataFrame()\nSubmission['Id'] = dataframe.Id\nSubmission.info()","8621f8fb":"Submission['SalePrice'] = final_prediction","3306f5d2":"print(Submission.head())","27b4d8a7":"Submission.to_csv('submission.csv', index=False)","fee8bba8":"# Data exploration","17cbeb8f":"# EDA?!","95809fce":"I guess, everything was in exploration part, so let's move on.","17689ccd":"![](https:\/\/miro.medium.com\/max\/2000\/0*piTmM4QW13q8CiOL.jpg)","86369653":"\ud83d\ude44\ud83d\ude2d","c82adba2":"Next, we loop through each variable and fit a model to predict it using the other variables. We use a simple model of Gradient Boosting Model (GBM) and K-fold validation.\nDepending on whether the target variable is **numerical or categorical**, we apply different models and scores.\n*When the target is numerical, we use the Gradient Boosting Regressor model and RMSE.* \n*When the target is categorical, we use the Gradient Boosting Classifier model and Accuracy.*\nFor each target, we print out the K-fold validation score and the most important 5 predictors.\nWe also **add three features rand0, rand1, rand2** composed of random numbers. They **serve as anchors when comparing the relationship between variables**. If one predictor is less important or similar compared to these random variables, then it is not an important predictor of the target variable.\n","22cb0b69":"We can see that there is a strong relationship between SaleType and SaleCondition.\nLet\u2019s use a scatter plot to visualize them: the x-axis as SaleCondition and the y-axis as SaleType, while the size of the dots represents the SalePrice.","8a6c9a6c":"> Separate into train and test splits. Arshat Spabekova said in 21st century.\n","4b32a652":"We have 1 numerical and 6 categorical variables. \n\nLet's use K-fold cross-validation to test for relationships among variables. \nFirst, transform the categorical variables. Since will be using 5-fold cross-validation, we need to make sure there are at least 5 observations for each category level."}}