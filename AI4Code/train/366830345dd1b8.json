{"cell_type":{"112f7f66":"code","117c3767":"code","d68d5088":"code","5c9a2ba1":"code","09783836":"code","261559f8":"code","100b8fd5":"code","f16a72c3":"code","6e5814c3":"code","f11c96f6":"code","3dfa9b01":"code","551815b0":"code","4f209c97":"code","7434f4e5":"code","e459e023":"code","6531c4a6":"code","47a4057d":"code","262b6c36":"code","e555e1eb":"code","ed6b2908":"code","e347f39d":"code","03bd0099":"code","96b8bb85":"code","7701c8cd":"code","409395d8":"code","95552714":"code","2c77f96a":"code","65508106":"code","a8587927":"code","92c6d702":"code","baa181ce":"code","af678a37":"code","5b0c42a0":"code","1a542d5c":"code","0b01b1f7":"code","232ed664":"code","c2b1148e":"markdown","b9865de4":"markdown","7615d1dd":"markdown","35dc1ce0":"markdown","cc733eef":"markdown","de05ca4e":"markdown","d092e50b":"markdown","ca69a1f5":"markdown","1fcccb28":"markdown","7fac10e3":"markdown","d51063f2":"markdown","82d58f06":"markdown"},"source":{"112f7f66":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# NLTK modules\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\n\nimport re\n\nfrom gensim.models import Word2Vec # Word2Vec module\nfrom gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_punctuation, remove_stopwords, strip_numeric, stem_text\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\nfrom imblearn.over_sampling import RandomOverSampler\n\n# Tensorflow libraries\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing import text, sequence\n# from tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import regularizers\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","117c3767":"# Training data\ntrain_df = pd.read_csv('\/kaggle\/input\/janatahack-independence-day-2020-ml-hackathon\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/janatahack-independence-day-2020-ml-hackathon\/test.csv')\n\nsubmission_df = pd.read_csv('\/kaggle\/input\/janatahack-independence-day-2020-ml-hackathon\/sample_submission_UVKGLZE.csv')","d68d5088":"print(train_df.isnull().sum())\nprint(train_df.columns)","5c9a2ba1":"# Converting binary column to category\ntarget_cols = ['Computer Science', 'Physics', 'Mathematics','Statistics', 'Quantitative Biology', 'Quantitative Finance']\n","09783836":"y_data = train_df[target_cols]","261559f8":"# Plot category data\nplt.figure(figsize=(10,6))\ny_data.sum(axis=0).plot.bar()\nplt.show()\n","100b8fd5":"# Stemmer object\nporter = PorterStemmer()\nwnl = WordNetLemmatizer()\n\nclass DataPreprocess:\n    \n    def __init__(self):\n        self.filters = [strip_tags,\n                       strip_numeric,\n                       strip_punctuation,\n                       lambda x: x.lower(),\n                       lambda x: re.sub(r'\\s+\\w{1}\\s+', '', x),\n                       remove_stopwords]\n    def __call__(self, doc):\n        clean_words = self.__apply_filter(doc)\n        return clean_words\n    \n    def __apply_filter(self, doc):\n        try:\n            cleanse_words = set(preprocess_string(doc, self.filters))\n#             filtered_words = set(wnl.lemmatize(w) if w.endswith('e') or w.endswith('y') else porter.stem(w) for w in cleanse_words)\n            return ' '.join(cleanse_words)\n        except TypeError as te:\n            raise(TypeError(\"Not a valid data {}\".format(te)))","f16a72c3":"train_df['train_or_test'] = 0\ntest_df['train_or_test'] = 1\n\nfeature_col = ['ID', 'TITLE', 'ABSTRACT', 'train_or_test']","6e5814c3":"# Concat train and test data\ncombined_set = pd.concat([train_df[feature_col], test_df[feature_col]])","f11c96f6":"combined_set","3dfa9b01":"# Combine the Title and Abstract data\ncombined_set['TEXT'] = combined_set['TITLE'] + combined_set['ABSTRACT']\n\n# articles['Processed'] = articles['TEXT'].apply(DataPreprocess())\n# Drop unwanted columns\ncombined_set = combined_set.drop(['TITLE', 'ABSTRACT'], axis=1)\n","551815b0":"# Invoke data preprocess operation on the text data\ncombined_set['Processed'] = combined_set['TEXT'].apply(DataPreprocess())","4f209c97":"combined_set.columns","7434f4e5":"train_set = combined_set.loc[combined_set['train_or_test'] == 0]\ntest_set = combined_set.loc[combined_set['train_or_test'] == 1]\n# Drop key reference column\ntrain_set = train_set.drop('train_or_test', axis=1)\ntest_set = test_set.drop('train_or_test', axis=1)","e459e023":"train_set[0:2].values","6531c4a6":"train_data = train_set['Processed']\ntest_data = test_set['Processed']\n\ny = y_data.values\n\nX_train, X_valid, y_train, y_valid = train_test_split(train_data, y, test_size=0.3, random_state=42)\n","47a4057d":"def label_encoding(y_train):\n    \"\"\"\n        Encode the given list of class labels\n        :y_train_enc: returns list of encoded classes\n        :labels: actual class labels\n    \"\"\"\n    lbl_enc = LabelEncoder()\n    \n    y_train_enc = lbl_enc.fit_transform(y_train)\n    labels = lbl_enc.classes_\n    \n    return y_train_enc, labels\n\n\ndef word_embedding(train, test, max_features, max_len=200):\n    try:\n        # Keras Tokenizer class object\n        tokenizer = text.Tokenizer(num_words=max_features)\n        tokenizer.fit_on_texts(train)\n        \n        train_data = tokenizer.texts_to_sequences(train)\n        test_data = tokenizer.texts_to_sequences(test)\n        \n        # Get the max_len\n        vocab_size = len(tokenizer.word_index) + 1\n        \n        # Padd the sequence based on the max-length\n        x_train = sequence.pad_sequences(train_data, maxlen=max_len, padding='post')\n        x_test = sequence.pad_sequences(test_data, maxlen=max_len, padding='post')\n        # Return train, test and vocab size\n        return tokenizer, x_train, x_test, vocab_size\n    except ValueError as ve:\n        raise(ValueError(\"Error in word embedding {}\".format(ve)))","262b6c36":"max_features = 6000\nmax_len = 200\n\ntokenizer, x_pad_train, x_pad_valid, vocab_size = word_embedding(X_train, X_valid, max_features)","e555e1eb":"x_pad_train.shape\nprint(\"Vocab size: {}\".format(vocab_size))","ed6b2908":"# def build_rnn(vocab_size,output_dim, max_len):\n#     # Building RNN model\n#     model = Sequential([\n#         keras.layers.Embedding(vocab_size,200,\n#                               input_length=max_len),\n#         keras.layers.Bidirectional(keras.layers.LSTM(128,return_sequences=True)),\n#         keras.layers.GlobalMaxPool1D(), # Remove flatten layer\n#         keras.layers.Dense(128, activation='relu'),\n#         keras.layers.Dropout(0.4),\n#         keras.layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(0.02)),\n#         keras.layers.Dropout(0.3),\n#         keras.layers.Dense(32, activation='relu', kernel_regularizer=keras.regularizers.l2(0.02)),\n#         keras.layers.Dense(output_dim, activation='sigmoid')\n#     ])\n\n#     return model\n\ndef build_rnn(vocab_size,output_dim, max_len):\n    # Building RNN model\n    model = Sequential([\n        keras.layers.Embedding(vocab_size,200,\n                              input_length=max_len),\n        keras.layers.BatchNormalization(),\n        keras.layers.Bidirectional(keras.layers.LSTM(256,return_sequences=True)),\n        keras.layers.GlobalMaxPool1D(), # Remove flatten layer\n        keras.layers.Dense(256, activation='relu'),\n        keras.layers.Dropout(0.5),\n        keras.layers.Dense(128, activation='relu'),\n        keras.layers.Dropout(0.5),\n        keras.layers.Dense(64, activation='relu'),\n        keras.layers.Dropout(0.5),\n        keras.layers.Dense(32, activation='relu'),\n        keras.layers.Dense(output_dim, activation='sigmoid')\n    ])\n\n    return model","e347f39d":"rnn_model = build_rnn(vocab_size, 6, max_len)\n\n# Summary of the model\nrnn_model.summary()","03bd0099":"# Compile the model\nrnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])","96b8bb85":"history = rnn_model.fit(x_pad_train, \n                        y_train,\n                        batch_size=256,\n                       epochs=7,\n                       verbose=1,\n                       validation_split=0.2)","7701c8cd":"score = rnn_model.evaluate(x_pad_valid, y_valid, verbose=1)\n\nprint(\"Loss:%.3f Accuracy: %.3f\" % (score[0], score[1]))","409395d8":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\n\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train','test'], loc='upper left')\nplt.show()\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\n\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train','test'], loc='upper left')\nplt.show()","95552714":"len(test_data)","2c77f96a":"# tokenizer = text.Tokenizer(num_words=5000)\ntokenizer.fit_on_sequences(test_data)\n\nX_test = tokenizer.texts_to_sequences(test_data)\nx_pad_test = sequence.pad_sequences(X_test, maxlen=max_len, padding='post')","65508106":"x_pad_test","a8587927":"y_preds = rnn_model.predict(x_pad_test)","92c6d702":"for arr in y_preds:\n    for i in range(len(arr)):\n        if arr[i]>0.5:\n            arr[i] = 1\n        else:\n            arr[i] = 0","baa181ce":"y_preds = y_preds.astype('int32')","af678a37":"y_preds","5b0c42a0":"pred_df = pd.DataFrame(y_preds, columns=target_cols)","1a542d5c":"submission_df[target_cols] = pred_df[target_cols]","0b01b1f7":"submission_df","232ed664":"submission_df.to_csv(\"rnn_model_04.csv\", index=False)","c2b1148e":"## Data Preparation","b9865de4":"The data distribution is not balanced for all the classes. There are some imbalance sample data in the given training set.","7615d1dd":"## <span style=\"color:blue\">Explore Data<\/span>\n","35dc1ce0":"## Prepare Test Data","cc733eef":"# AnalyticVidhya - NLP Topic Modeling\n\n## Problem statement\n\nResearchers have access to large online archives of scientific articles. As a consequence, finding relevant articles has become more difficult. Tagging or topic modelling provides a way to give token of identification to research articles which facilitates recommendation and search process.\n\nGiven the abstract and title for a set of research articles, predict the topics for each article included in the test set. \n\nNote that a research article can possibly have more than 1 topic. The research article abstracts and titles are sourced from the following 6 topics: \n\n1. Computer Science\n\n2. Physics\n\n3. Mathematics\n\n4. Statistics\n\n5. Quantitative Biology\n\n6. Quantitative Finance\n\n## Dataset\n\nThe dataset consists of three files `train.csv`, `test.csv` and `sample_submission.csv`.\n\n|Fields| Description|\n|-------|-----------|\n|ID |Unique ID for each article|\n|TITLE|Title of the research article|\n|ABSTRACT|Abstract of the research article|\n|Computer Science|Whether article belongs to topic computer science (1\/0)|\n|Physics\t|Whether article belongs to topic physics (1\/0)|\n|Mathematics\t|Whether article belongs to topic Mathematics (1\/0)|\n|Statistics\t|Whether article belongs to topic Statistics (1\/0)|\n|Quantitative Biology\t|Whether article belongs to topic Quantitative Biology (1\/0)|\n|Quantitative Finance|Whether article belongs to topic Quantitative Finance (1\/0)|\n\n## Approach\n\nIn this notebook, there are two approaches followed,\n1. Preprocess the text data and convert them to pad sequence\n2. Construct a **Recurrent Neural Network(RNN)** to train the dataset and predict the test data\n","de05ca4e":"## <span style=\"color:blue\">Import Libraries<\/span>","d092e50b":"**Pre-process the text data** \n\nWe have combined the train and test dataset before applying the pre-processing steps. It will make us to execute the preprocessing pipeline only once for the entire dataset, otherwise we will have to run it separately for test dataset as well. ","ca69a1f5":"## <span style=\"color:blue\">Word Embedding<\/span>","1fcccb28":"Checking missing values and data columns.","7fac10e3":"## Split data","d51063f2":"## Combine Train and Test Data","82d58f06":"## <span style=\"color:blue\">Loading Dataset<\/span>"}}