{"cell_type":{"7265e6db":"code","b104e861":"code","e777d3ee":"code","756ba3df":"code","99e507d5":"code","f0d636ea":"code","d5a51a64":"code","98945df2":"code","b9a1303a":"code","11b9dbb6":"code","ddf23367":"code","e3917f1f":"code","eede7ddb":"code","773d75e1":"code","fc47bf8c":"code","a80f9d96":"code","c9b4c592":"code","91db126d":"code","61908376":"code","c2f3ec6d":"code","10d50105":"code","64d42f61":"code","ac1be004":"code","f910438d":"code","5b4c616d":"code","ab61eac8":"code","fc3b05a3":"markdown","2d5f4c1c":"markdown","db66983b":"markdown","2e176139":"markdown","6163bb67":"markdown","3354d8c9":"markdown","3661092d":"markdown","d5b995e3":"markdown","36f940b5":"markdown","135fa519":"markdown","207aef65":"markdown","841c2db3":"markdown","4eeb79f7":"markdown","47b13b63":"markdown","12f53e8a":"markdown","214a0713":"markdown","97edbf85":"markdown","e25a62d8":"markdown"},"source":{"7265e6db":"import os\n\nimport numpy as np\nimport pandas as pd\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tqdm.notebook import tqdm\nimport plotly.express as px","b104e861":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","e777d3ee":"model_name = 'jplu\/tf-xlm-roberta-large'\nn_epochs = 10\nmax_len = 80\n\n# Our batch size will depend on number of replicas\nbatch_size = 16 * strategy.num_replicas_in_sync","756ba3df":"train = pd.read_csv('\/kaggle\/input\/contradictory-my-dear-watson\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/contradictory-my-dear-watson\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/contradictory-my-dear-watson\/sample_submission.csv')","99e507d5":"twitter = pd.read_csv('\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv',\n                      encoding = 'latin1',\n                      names=['polarity','id','date','flag','user','text'])","f0d636ea":"print(\"twitter data shape: \", len(twitter), len(twitter.columns))","d5a51a64":"twitter.head()","98945df2":"twitter_premise, twitter_hypothesis = train_test_split(twitter['text'], test_size=0.5, random_state=2020)","b9a1303a":"print(len(twitter_premise))\nprint(len(twitter_hypothesis))","11b9dbb6":"twitter_df = pd.DataFrame()\ntwitter_df['premise'] = twitter_premise.values\ntwitter_df['hypothesis'] = twitter_hypothesis.values\ntwitter_df.head()","ddf23367":"# First load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)","e3917f1f":"%%time\n\n# Convert the text so that we can feed it to `batch_encode_plus`\ntrain_text = train[['premise', 'hypothesis']].values.tolist()\ntest_text = test[['premise', 'hypothesis']].values.tolist()\ntwitter_text = twitter_df[['premise', 'hypothesis']].values.tolist()\n\n# Now, we use the tokenizer we loaded to encode the text\ntrain_encoded = tokenizer.batch_encode_plus(\n    train_text,\n    pad_to_max_length=True,\n    max_length=max_len\n)\n\ntest_encoded = tokenizer.batch_encode_plus(\n    test_text,\n    pad_to_max_length=True,\n    max_length=max_len\n)\n\ntwitter_encoded = tokenizer.batch_encode_plus(\n    twitter_text,\n    pad_to_max_length=True,\n    max_length=max_len\n)","eede7ddb":"### train\nx_train, x_valid, y_train, y_valid = train_test_split(\n    train_encoded['input_ids'], train.label.values, \n    test_size=0.2, random_state=2020\n)\n\n### test\nx_test = test_encoded['input_ids']\n\n### twitter\nx_twitter_train1, x_twitter_train2 = train_test_split(\n    twitter_encoded['input_ids'], \n    test_size=0.5, random_state=2020\n)","773d75e1":"%%time\n\nauto = tf.data.experimental.AUTOTUNE\n\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(batch_size)\n    .prefetch(auto)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_valid, y_valid))\n    .batch(batch_size)\n    .cache()\n    .prefetch(auto)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(x_test)\n    .batch(batch_size)\n)\n\n### for creating pseudo labels ###\ntrain_twitter_dataset1 = (\n    tf.data.Dataset\n    .from_tensor_slices(x_twitter_train1)\n    .batch(batch_size)\n)\n\n### for creating pseudo labels ###\ntrain_twitter_dataset2 = (\n    tf.data.Dataset\n    .from_tensor_slices(x_twitter_train2)\n    .batch(batch_size)\n)","fc47bf8c":"with strategy.scope():\n    # First load the transformer layer\n    transformer_encoder = TFAutoModel.from_pretrained(model_name)\n\n    # This will be the input tokens \n    input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n\n    # Now, we encode the text using the transformers we just loaded\n    last_hidden_states = transformer_encoder(input_ids)[0]\n\n    # Only extract the token used for classification, which is <s>\n    cls_token = last_hidden_states[:, 0, :]\n\n    # Finally, pass it through a 3-way softmax, since there's 3 possible laels\n    out = Dense(3, activation='softmax')(cls_token)\n\n    # It's time to build and compile the model\n    model = Model(inputs=input_ids, outputs=out)\n    model.compile(\n        Adam(lr=1e-5), \n        loss='sparse_categorical_crossentropy', \n        metrics=['accuracy']\n    )\n\nmodel.summary()","a80f9d96":"n_steps = len(x_train) \/\/ batch_size\n\ntrain_history1 = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=n_epochs\n)","c9b4c592":"test_preds1 = model.predict(test_dataset, verbose=1)","91db126d":"twitter_preds = model.predict(train_twitter_dataset1, verbose=1)\ntwitter_pseudo_labels_train1 = twitter_preds.argmax(axis=1)\ntwitter_preds = model.predict(train_twitter_dataset2, verbose=1)\ntwitter_pseudo_labels_train2 = twitter_preds.argmax(axis=1)","61908376":"twitter_dataset_ssl1 = (\n    tf.data.Dataset\n    .from_tensor_slices((x_twitter_train1, twitter_pseudo_labels_train1))\n    .repeat()\n    .shuffle(2048)\n    .batch(batch_size)\n    .prefetch(auto)\n)\n\ntwitter_dataset_ssl2 = (\n    tf.data.Dataset\n    .from_tensor_slices((x_twitter_train2, twitter_pseudo_labels_train2))\n    .repeat()\n    .shuffle(2048)\n    .batch(batch_size)\n    .prefetch(auto)\n)","c2f3ec6d":"n_steps = len(x_twitter_train1) \/\/ batch_size\nn_steps_val = len(x_valid) \/\/ batch_size\n\nmodel.fit(\n    twitter_dataset_ssl1,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    validation_steps=n_steps_val,\n    epochs=2\n)\n\ntest_preds2 = model.predict(test_dataset, verbose=1)","10d50105":"n_steps = len(x_twitter_train2) \/\/ batch_size\nn_steps_val = len(x_valid) \/\/ batch_size\n\nmodel.fit(\n    twitter_dataset_ssl2,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    validation_steps=n_steps_val,\n    epochs=2\n)\n\ntest_preds3 = model.predict(test_dataset, verbose=1)","64d42f61":"test_preds = (0.92)*test_preds1 + (0.05)*test_preds2 + (0.03)*test_preds3\ntest_preds = test_preds.argmax(axis=1)\nsubmission['prediction'] = test_preds","ac1be004":"submission.to_csv('submission.csv', index=False)\nsubmission.head()","f910438d":"hist = train_history1.history","5b4c616d":"px.line(\n    hist, x=range(1, len(hist['loss'])+1), y=['accuracy', 'val_accuracy'], \n    title='Model Accuracy', labels={'x': 'Epoch', 'value': 'Accuracy'}\n)","ab61eac8":"px.line(\n    hist, x=range(1, len(hist['loss'])+1), y=['loss', 'val_loss'], \n    title='Model Loss', labels={'x': 'Epoch', 'value': 'Loss'}\n)","fc3b05a3":"### New Comment! \n\nNow we need to create new tf.datasets with our newly created pseudo labels.  We'll use this to do 1 more round of modeling i.e. 1 round of self-training to update our model based on our pseudo labels.  ","2d5f4c1c":"## Setting up the TPUs\n\n### original commentz:\n\nThis line is necessary in order to initialize the TPUs. \n\nHere, \"replicas\" simply means number of \"cores\". In the case of GPUs or CPUs, the number of replicas will be 1. [Read this](https:\/\/cloud.google.com\/tpu\/docs\/tpus#replicas) for more information.","db66983b":"# Imports","2e176139":"## Load datasets\n\n### original comment:\n\nJust regular CSV files. Nothing scary here!","6163bb67":"### New Comment!\n\nSo we run model.fit twice, each time making prediction on the test set.  Why?  This will give us more control over model\\prediction blending on the end and will help us from overfitting.","3354d8c9":"## Convert to tf.data.Dataset\n\n### original comment:\n\n`tf.data.Dataset` is one of many different ways to define the input to our models. Here, it is a good choice since it is easily compatible with TPUs. Read more about it [in this article](https:\/\/towardsdatascience.com\/how-to-use-dataset-in-tensorflow-c758ef9e4428).","3661092d":"### New Comment! \n\nLoading our twitter data.","d5b995e3":"## Define variables\n\n### original comment:\n\nMake sure to keep those variables in mind as you navigate this notebook! They are all placed below so you can easily change and rerun this notebook.\n\nDon't worry about the model right now. We will come back to it later.","36f940b5":"### New Comment!\n\nWe're going to blend in preds from SSL (semi-supervised learning), so we need to save the predictions from our 1st run.  This will make more sense as we go.","135fa519":"### New Comment!\n\nSo now we take our model trained on the competition data only, and predict on our twitter to get pseudo lables for 1 round of self training.  The hope is that our unlabeled data can help identify our boundaries more accurately.  In a sense, we're kind of acting like an EM algorithm except we're not running EM very long at all.  Our original model takes guesses on where these points should land, and then we update our model but using those predictions as pseudo labels.","207aef65":"## Encode Training data\n\n### original comment:\n\nNow, we need to encode the training and test data into `tokens`, which are numerical representation of our words. To learn more, [read this](https:\/\/huggingface.co\/transformers\/main_classes\/tokenizer.html).","841c2db3":"### original comment:\n\nTrain and validation split happens here.\n\n### New Comment!\n\nWe split our twitter data into 2 training sets.  Why?  Because this will give us more control over our model\\prediction blending in the end.","4eeb79f7":"### New Comment!\n\nHere we predict on test set and submit.  We give the 1st model the most weight.  The predictions from our self-training are given less weight.  Why?  Honestly, because these weights work.  One intuition is that the self-training clearly has some new information to add, but it's not doing all the heavy lifting so giving too much weight to the self-training rounds would cause overfitting \\ model distortion.","47b13b63":"## Visualize Training History\n\n### original comment\n\nWith Plotly Express, this can be done in one function call:","12f53e8a":"### New Comment! \n\nHere we split the data in half at random, and we pair the data together.  So 1 half will represent our 'premise' and the other half with be or 'hypothesis'.  Since we're doing this at random, I image that most of these pairs will have nothing to do with one another, but in some cases they might result in a nice 'premise' and 'hypothesis' pair.  A good error analysis would bee to look at the pairs that get a high score.","214a0713":"----------------------------------------------------------------------------------------------------------\n\n## XLM RoBERTa Large + Semi-Supervised Learning\n\n**Description:** First I forked the notebook below since the tensorflow wiring and some parameter tuning was already accomplished (no point in recreating the wheel).  From this base code I wanted to add external data for 1 round of self-training.  In my prior work, excessive self-training has not shown promise.  <=3 rounds of self-training in my prior work has proven useful.  For deep learning, 1 round of self-training has proven useful when blending in the predictions vs. just taking the final predictions after all training is completed.  In this version of self-training, I'm including more than just the most confident examples.  There are other ideas that could have been tried here as well, but I will leave those to the reader.\n\n**Forked:** https:\/\/www.kaggle.com\/yeayates21\/fork-contra-watson-concise-keras-xlm-r-on-tpu\n\n**Additions:** \n - Added twitter data and preformed 1 round of self-training\n\n**Acknowledgments:** \n - [xhlulu](https:\/\/www.kaggle.com\/xhlulu)\n\n**Learning Resources:**\n - See self-training sections:  \n   - http:\/\/pages.cs.wisc.edu\/~jerryzhu\/pub\/sslicml07.pdf\n   - http:\/\/www.cs.cmu.edu\/~10701\/slides\/17_SSL.pdf\n \n ----------------------------------------------------------------------------------------------------------","97edbf85":"### New Comment!\n\nWe don't add training plots for the self training because we're not running for too many rounds and we're just blending in the predictions.","e25a62d8":"## Train the model\n\n### original comment:\n\nIt's time to teach our lovely XLM-Roberta how to infer natural language. Notice here we are using `strategy.scope()`. We need to load `transformer_encoder` inside this scope in order to tell Tensorflow that we want our model on the TPUs. Otherwise, it will try to load it in your CPU machine!\n\nXLM-Roberta is one of the best models out there for multilingual classification tasks. Essentially, it is a model that was trained on inherently multilingual text, and used methods that helped it become larger, train longer and on more data! Highly recommend you to read [this blog post by the authors](https:\/\/ai.facebook.com\/blog\/-xlm-r-state-of-the-art-cross-lingual-understanding-through-self-supervision\/), as well as the [Huggingface docs](https:\/\/huggingface.co\/transformers\/model_doc\/xlmroberta.html) on the subject.\n\n### New Comment!\n\nA great article on the TensorFlow logic:  http:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/\n\n![](http:\/\/jalammar.github.io\/images\/distilBERT\/bert-output-tensor-selection.png)"}}