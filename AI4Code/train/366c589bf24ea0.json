{"cell_type":{"de3d5f13":"code","e873d7ec":"code","9252827a":"markdown"},"source":{"de3d5f13":"import numpy as np\nimport matplotlib.pyplot as plt\n\nclass LogisticRegression:\n    def __init__(self):\n        self._theta = None\n        self._J_history = None\n\n    def fit(self, X, y, alpha=0.01, iter_num=1200, epsion=1e-4):\n        X = np.c_[np.ones(X.shape[0]), X]\n        y = np.c_[y]\n        theta = np.zeros((X.shape[1],1))\n        iter_n = 0\n        m = X.shape[0]\n\n        def sigmoid(z):\n            return 1.0 \/ (1 + np.exp(-z))\n\n        def costFun(X, theta, y):\n            m = X.shape[0]\n            return (1 \/ m) * np.dot(y.T, np.log(X.dot(theta)) + np.dot((1 - y).T, np.log(X.dot(theta))))\n\n        while iter_n < iter_num:\n            hx = sigmoid(X.dot(theta)).reshape(-1,1)\n            last_theta = theta\n            theta -= (alpha \/ m) * X.T.dot(hx - y)\n            iter_n += 1\n        self._theta = theta\n    def score(self,X,y):\n        X = np.c_[np.ones(X.shape[0]), X]\n        y = np.c_[y]\n        m = X.shape[0]\n        mse = (1\/m)*np.sum(np.square(self.sigmoid(X.dot(self._theta))- y))\n        var = (1\/m)*np.sum(np.square(y - y.mean()))\n        return 1 - mse\/var\n    def sigmoid(self,z):\n        return 1.0 \/ (1 + np.exp(-z))\n    def predict_proba(self,X):\n        X = np.c_[np.ones(X.shape[0]), X]\n        res = []\n        for i in self.sigmoid(X.dot(self._theta))>0.5:\n            if i[0]:\n                res.append(1)\n            else:\n                res.append(0)\n        return np.array(res)\n\n    def showLogis(self,X, y):\n        X = np.c_[np.ones(X.shape[0]), X]\n        y = np.c_[y]\n        m, f = X.shape\n        plt.figure('Line diagram')\n        plt.scatter(X[y[:,0]==1,1],X[y[:,0]==1,2],c='b',label='good')\n        plt.scatter(X[y[:,0]==0,1],X[y[:,0]==0,2],c='r',label='bad')\n        plt.legend(loc='best')\n        # \u753b\u5206\u754c\u7ebf\n        min_x = min(X[:, 1])  # The minimum of x1\n        max_x = max(X[:, 2])  # The maximum value of x1\n        y_min_x = (-self._theta[0] - self._theta[1] * min_x) \/ self._theta[2]\n        #The minimum value of x1 corresponds to the value of x2\n        y_max_x = (-self._theta[0] - self._theta[1] * max_x) \/ self._theta[2]\n        # The maximum value of x1 is x2\n        plt.plot([min_x, max_x], [y_min_x, y_max_x], '-g')  # Painting line\n        plt.show()\n\n    def testAccuracy(self,X, y):\n        X = np.c_[np.ones(X.shape[0]), X]\n        y = np.c_[y]\n        m = X.shape[0]\n        count = 0\n\n        for i in range(m):\n            h = self.sigmoid(np.dot(X[i, :], self._theta))  # Calculate the predicted value\n            if bool(np.where(h >= 0.5, 1, 0)) == bool(y[i]):\n                count += 1\n        return count \/ m\n    def accuracy(self,X, y):\n        m = X.shape[0]  # \u6837\u672c\u4e2a\u6570\n        count = 0\n        y_pred = self.predict_proba(X)\n        return np.sum(y == y_pred)\/len(y)","e873d7ec":"import numpy as np\nfrom sklearn.datasets import make_moons\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib as mpl\n\n\nmpl.rcParams['axes.unicode_minus'] = False\nh = .02\nx, y = make_moons(250,noise=0.25)\nfrom matplotlib.colors import ListedColormap\n# Drawing board\ncm_bright = ListedColormap(['#FF0000', '#0000FF'])\n# standardized\nx= StandardScaler().fit_transform(x)\n# Cut into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\n# Find the maximum and minimum values for each axis\nx_min, x_max = x[:, 0].min() - .5, x[:, 0].max() + .5\ny_min, y_max = x[:, 1].min() - .5, x[:, 1].max() + .5\n# Grid point\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n# Into the model\nclf = LogisticRegression()\n# Training data\nclf.fit(X_train, y_train)\n# To calculate R2\nscore = clf.score(X_test, y_test)\nZ = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\ncm = plt.cm.RdBu\nplt.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n\n# Plot sample points\ncm_bright = ListedColormap(['#FF0000', '#0000FF'])\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,\n               edgecolors='k',label=\"The training set\")\nplt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.5,\n               edgecolors='k',label=\"The test set\")\nplt.title(\"Logistic regression classification\")\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.xticks()\nplt.yticks()\nplt.legend()\n# put R2\nplt.text(xx.max() - .3, yy.min() + .3, ('%  s%.2f' % (\"%\",score*100)),size=15, horizontalalignment='right')\nplt.show()\n# Display accuracy\nprint(clf.accuracy(x,y))","9252827a":"Here is the test program"}}