{"cell_type":{"8057175a":"code","01f816fd":"code","01cb35a0":"code","c762856e":"code","81f56652":"code","1c17871b":"code","66c6ee4e":"code","5a7b9803":"code","f656eff9":"code","14c6de90":"code","7be0b73b":"code","10dc4003":"code","28dd6da9":"code","6b323818":"code","c468a289":"code","1a41892a":"code","f3ebfc9c":"code","b1224a6e":"code","ea8cdb60":"code","ecbf193a":"code","3fa2be54":"code","c6f2b06b":"code","21a88c79":"code","a6507107":"code","894d22b1":"code","7106d25d":"code","b827964e":"code","6d695eca":"code","7d494f2a":"code","93dc3813":"code","adf44b2a":"code","f5a11cbd":"code","969808b9":"code","6dcc4fa6":"code","ed908ae8":"code","e7ca3926":"code","00ce339b":"code","2ced16a1":"code","3f4e7602":"code","913fffd4":"code","920bda4d":"code","a727fa83":"code","3550c9e0":"code","59afea80":"code","4f876e7d":"code","d950a68b":"code","a67bd723":"code","89fe8609":"code","25f80ffe":"code","9b792fd7":"code","2e626679":"code","7dcca73c":"code","fd93bb97":"code","3573afba":"code","9a39bb84":"code","5e0ef83d":"code","39fe3f96":"code","4565355f":"code","6eb0ab18":"code","1ac54535":"code","943bef8e":"code","a0cf39d9":"code","4f911195":"code","1cea9920":"code","a6b2408f":"code","7d6d3fd7":"code","1e1673d6":"code","92380103":"code","237a3fa6":"code","64cd3da3":"code","d80d6be6":"code","3dae637c":"markdown","f89bd8c9":"markdown","db0cb3df":"markdown","c41e260b":"markdown","f853583f":"markdown","fae7ff41":"markdown","6f06a39e":"markdown","9fe6066f":"markdown","e80d6f23":"markdown","2a9c83f6":"markdown","9d6a2cfd":"markdown","fdd3216c":"markdown","ab63726a":"markdown","dfe4b4eb":"markdown"},"source":{"8057175a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","01f816fd":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import SimpleImputer, IterativeImputer, KNNImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler, RobustScaler, StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier, XGBRFClassifier","01cb35a0":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntrain.head()","c762856e":"test.head()","81f56652":"train.info()","1c17871b":"train.drop(\"PassengerId\", axis = 1, inplace = True)","66c6ee4e":"train.head()","5a7b9803":"train.dtypes","f656eff9":"# Check missing values\ntrain.isna().sum()","14c6de90":"# Check some statistics about the numerical data\ntrain.describe().T","7be0b73b":"# Check relation between Age and Survived\nplt.figure(figsize = (10, 10))\npd.crosstab(train.Age, train.Survived).plot(kind = \"hist\", color = [\"salmon\", \"lightblue\"])\n\nplt.title(\"Age vs. Survived\");","10dc4003":"plt.figure(figsize = (10, 10))\npd.crosstab(train.Pclass, train.Survived).plot(kind = \"bar\", color = [\"salmon\", \"lightblue\"])\nplt.title(\"Pclass vs. Survived\");","28dd6da9":"# Check the relationship between Sex and Survived\nplt.figure(figsize = (10 ,10))\npd.crosstab(train.Sex, train.Survived).plot(kind = \"bar\", color = [\"salmon\", \"lightblue\"]);","6b323818":"# Let's visualize correlation matrix\nfig, ax = plt.subplots(figsize = (10, 10))\ncorr_matrix = train.corr()\nax = sns.heatmap(data = corr_matrix,\n                 annot = True,\n                 square = True,\n                 linewidths = 0.5,\n                 cmap = \"YlGnBu\")\nax.set_title(\"Correlation Matrix\");","c468a289":"# Check out correlation between features and target\nplt.figure(figsize = (10, 10))\ntrain.drop(\"Survived\", axis = 1).corrwith(train.Survived).plot(kind = \"bar\",\n                                                               grid = True)\nplt.title(\"Correlation between features and target\");","1a41892a":"train.drop(\"SibSp\", axis = 1, inplace = True)\ntrain.head()","f3ebfc9c":"sns.catplot(x = \"Sex\", y = \"Age\", hue = \"Survived\", data = train);","b1224a6e":"sns.catplot(x = \"Embarked\", y = \"Pclass\", hue = \"Survived\", data = train);","ea8cdb60":"# Let's extract some clues from \"Name\"\n# Split Name into first_name and last_name\n\ntrain[\"first_name\"] = train.Name.apply(lambda x: x.split(\",\")[1])\ntrain[\"last_name\"] = train.Name.apply(lambda x: x.split(\",\")[0])\ntrain.drop(\"Name\", axis = 1, inplace = True)","ecbf193a":"train.head()","3fa2be54":"for label, content in train.items():\n    if pd.isnull(content).sum():\n        train[label + \"_was_missing\"] = pd.isnull(content)","c6f2b06b":"train.head()","21a88c79":"train.dtypes","a6507107":"train.head()","894d22b1":"# Group the data as categorical or numerical\ncategorical_features = [\"Sex\", \"Ticket\", \"Cabin\", \"Embarked\", \"first_name\", \"last_name\", \"Age_was_missing\", \"Cabin_was_missing\", \"Embarked_was_missing\"]\nnumerical_features = [\"Pclass\", \"Age\", \"Parch\", \"Fare\"]","7106d25d":"categorical_transformer = Pipeline(steps = [\n                          (\"imputer\", SimpleImputer(strategy = \"constant\", fill_value = \"missing\")),\n                          (\"onehot\", OneHotEncoder(handle_unknown = \"ignore\"))\n])","b827964e":"numerical_transformer = Pipeline(steps = [\n                        (\"imputer\", SimpleImputer(strategy = \"median\"))\n])","6d695eca":"preprocessor = ColumnTransformer(transformers = [\n               (\"cat\", categorical_transformer, categorical_features),\n               (\"num\", numerical_transformer, numerical_features)\n])","7d494f2a":"base_pipe = Pipeline(steps = [\n            (\"preprocessor\", preprocessor),\n            (\"scaler\", StandardScaler(with_mean = False)),\n            (\"classifier\", RandomForestClassifier())\n])","93dc3813":"# Split data into X & y\nX = train.drop(\"Survived\", axis = 1)\ny = train.Survived","adf44b2a":"# Split X & y into train & test\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size = 0.2,\n                                                    random_state = 7)","f5a11cbd":"# Fit the pipe to the data\nbase_pipe.fit(X_train, y_train)","969808b9":"# Score the pipe\nbase_pipe.score(X_test, y_test)","6dcc4fa6":"# First thing to do: Choosing best estimator for our problem\nmodels = {\n    \"RandomForestClassifier\" : RandomForestClassifier(),\n    \"DecisionTreeClassifier\" : DecisionTreeClassifier(),\n    \"KNeighborsClassifier\" : KNeighborsClassifier(),\n    \"LogisticRegression\" : LogisticRegression(),\n    \"SVM\" : SVC(),\n    \"XGBClassifier\" : XGBClassifier(),\n    \"XGBRFClassifier\" : XGBRFClassifier()}","ed908ae8":"def fit_and_score(models, X_train, X_test, y_train, y_test):\n    \"\"\"\n    Fits and scores the given estimators\\n\n    Returns the scores dictionary.\n    \"\"\"\n    scores = {}\n    for name, model in models.items():\n        pipe = Pipeline(steps = [\n               (\"preprocessor\", preprocessor),\n               (\"scaler\", StandardScaler(with_mean = False)),\n               (\"classifier\", model)\n        ])\n        \n        pipe.fit(X_train, y_train)\n        scores[name] = pipe.score(X_test, y_test)\n    return scores","e7ca3926":"scores = fit_and_score(models = models,\n                       X_train = X_train,\n                       X_test = X_test,\n                       y_train = y_train,\n                       y_test = y_test)\nscores","00ce339b":"#imputers_to_test = [SimpleImputer(), IterativeImputer(), KNNImputer(n_neighbors = 3)]\nscalers_to_test = [StandardScaler(with_mean = False), MinMaxScaler(), RobustScaler()]","2ced16a1":"params = {\"scaler\" : scalers_to_test,\n     \"classifier__n_estimators\" : [100],\n     \"classifier__max_depth\" : [2, 5]}","3f4e7602":"search = GridSearchCV(estimator = base_pipe,\n                      param_grid = params,\n                      cv = 3)","913fffd4":"search.fit(X_train, y_train)","920bda4d":"search.best_params_","a727fa83":"search.score(X_test, y_test)","3550c9e0":"# Try other imputers\nnumerical_transformer = Pipeline(steps = [\n                        (\"imputer\", IterativeImputer())\n])","59afea80":"preprocessor = ColumnTransformer(transformers = [\n               (\"cat\", categorical_transformer, categorical_features),\n               (\"num\", numerical_transformer, numerical_features)\n])","4f876e7d":"preprocessor","d950a68b":"pipeline = Pipeline(steps = [\n           (\"preprocessor\", preprocessor),\n           (\"scaler\", StandardScaler(with_mean = False)),\n           (\"classifier\", RandomForestClassifier())\n])","a67bd723":"pipeline.fit(X_train, y_train)","89fe8609":"pipeline.score(X_test, y_test)","25f80ffe":"scores2 = fit_and_score(models = models,\n                        X_train = X_train,\n                        X_test = X_test,\n                        y_train = y_train,\n                        y_test = y_test)\nscores2","9b792fd7":"pipeline = Pipeline(steps = [\n           (\"preprocessor\", preprocessor),\n           (\"scaler\", StandardScaler(with_mean = False)),\n           (\"classifier\", XGBClassifier())\n])","2e626679":"params2 = {\n    \"classifier__max_depth\" : np.arange(3, 10, 2),\n    \"classifier__min_child_weight\" : [1, 2, 3],\n    \"classifier__eta\" : [.3]\n}","7dcca73c":"search2 = GridSearchCV(estimator = pipeline,\n                       param_grid = params2,\n                       cv = 3)","fd93bb97":"search2.fit(X_train, y_train)","3573afba":"search2.best_estimator_","9a39bb84":"search2.score(X_test, y_test)","5e0ef83d":"test.head()","39fe3f96":"# Check missing values\ntest.isna().sum()","4565355f":"test.drop(\"PassengerId\", axis = 1, inplace = True)","6eb0ab18":"test.drop(\"SibSp\", axis = 1, inplace = True)","1ac54535":"test[\"first_name\"] = test.Name.apply(lambda x: x.split(\",\")[1])\ntest[\"last_name\"] = test.Name.apply(lambda x: x.split(\",\")[0])\ntest.drop(\"Name\", axis = 1, inplace = True)","943bef8e":"for label, content in test.items():\n    if pd.isnull(content).sum():\n        test[label + \"_was_missing\"] = pd.isnull(content)","a0cf39d9":"test.head()","4f911195":"len(test)","1cea9920":"test.drop(\"Fare_was_missing\", axis = 1, inplace = True)","a6b2408f":"test[\"Embarked_was_missing\"] = train.Embarked_was_missing[:len(test)]","7d6d3fd7":"test.head()","1e1673d6":"y_preds = search.predict(test)\ny_preds","92380103":"# Check submission\nsubmission = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\nsubmission.head()","237a3fa6":"submission[\"Survived\"] = y_preds","64cd3da3":"submission.head()","d80d6be6":"submission.to_csv(\"my_sub.csv\", index = False)","3dae637c":"**Let's continue with RandomForestClassifier!**","f89bd8c9":"**We have to apply same operations which we applied to train dataset**","db0cb3df":"### Create Baseline Model with Pipeline","c41e260b":"### Make Predictions on Test Data","f853583f":"**According to correlation figure, we can drop the SibSp column, because this column has no effect on the target column, so it is useless.**","fae7ff41":"**Let's do some visualizations!**","6f06a39e":"We can drop the PassengerId column comfortly, because it is not useful for us.","9fe6066f":"**Let's visualize the categorical data**","e80d6f23":"### Import Necessary Tools","2a9c83f6":"**Let's continue with XGBClassifier!**","9d6a2cfd":"### Hyperparameter Tuning","fdd3216c":"### Get Data Ready","ab63726a":"**Add binary columns which tells us if the data was missing or not**\n\n**Why add a binary column indicating whether the data was missing or not?**\n\n**A value may be missing for a reason. In other words, absence of evidence may be evidence of absence.**","dfe4b4eb":"### Exploratory Data Analysis (EDA)"}}