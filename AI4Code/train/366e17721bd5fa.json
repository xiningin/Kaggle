{"cell_type":{"342338f9":"code","a206af5c":"code","7a5ffcf4":"code","02125326":"code","83fa2bd0":"code","351bfba9":"code","cc7f24f6":"code","ee46f43a":"code","e500c797":"code","649e5cbc":"code","127d17e3":"code","0fe3c9bd":"code","28e6715e":"code","9c82fd25":"code","a28f9ef6":"code","abb642f1":"code","5ec29cd8":"code","c60dae8a":"code","0de0b422":"code","c104c108":"code","ca329958":"code","351ecae1":"code","5bf74da5":"code","8d756bc1":"code","b27b9eb7":"code","5dd5ac37":"code","a59e0107":"code","1710310a":"code","24640d3b":"code","f9a39f46":"code","9afd745f":"code","d55f9f8f":"code","c9d8abee":"markdown","1bdbd81f":"markdown","3c6ef976":"markdown","e0ecff1d":"markdown","c1674809":"markdown","cff183a3":"markdown","f41f5cc5":"markdown","e9c6bac0":"markdown","34a75f15":"markdown","5f4fdf28":"markdown","9e4a65d3":"markdown","5f95a551":"markdown"},"source":{"342338f9":"!pip install opencv-contrib-python","a206af5c":"!git clone https:\/\/github.com\/AliaksandrSiarohin\/first-order-model","7a5ffcf4":"import cv2\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport warnings\nimport urllib.request\nwarnings.filterwarnings(\"ignore\")\nfrom IPython.display import HTML","02125326":"%matplotlib inline","83fa2bd0":"'''\n    params:\n        img_path : path or url to image\n        size : size of image to resize, default: None (do not resize)\n        scale : scale image between 0 and 1 by dividing with 255.0, default: True\n    return:\n        image as numpy array\n'''\ndef imread(img_path, size=None, scale=True):\n    if img_path.startswith(\"http:\/\/\") or img_path.startswith(\"https:\/\/\") or img_path.startswith(\"www.\"):\n        resp = urllib.request.urlopen(img_path)\n        img = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n        img = cv2.imdecode(img, cv2.IMREAD_COLOR)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        if size is not None:\n            img = cv2.resize(img, size)\n        if scale:\n            img = np.array(img\/255.0)\n        return img\n    img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, size)\n    if scale:\n        img = np.array(img\/255.0)\n    return img","351bfba9":"'''\n    params:\n        video_path : path or url to video\n        size : size of frame to resize, default: None (do not resize)\n        scale : scale image between 0 and 1 by dividing with 255.0, default: True\n    return:\n        list of video frames as numpy array\n'''\ndef vidread(video_path, size=None, scale=True):\n    vc = cv2.VideoCapture(video_path)\n    vid = []\n    while vc.isOpened():\n        ret, img = vc.read()\n        if not ret:\n            break\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        if size is not None:\n            img = cv2.resize(img, size)\n        if scale:\n            img = img\/255.0\n        vid.append(img)\n    vc.release()\n    return vid","cc7f24f6":"'''\n    params:\n        save_path : path to save video\n        frames : list of video frames as numpy array\n        fps : framerate of video to save, default: 20\n        size : size of video frames to save, default: (256,256)\n    return:\n        None\n'''\ndef vidsave(save_path, frames, fps=20, size=(256,256)):\n    #revert scaling factor of image and convert to uint8\n    frames = np.array(frames)*255.0\n    frames = frames.astype(np.uint8)\n    writer = cv2.VideoWriter(save_path, \n                     cv2.VideoWriter_fourcc(*'MJPG'),\n                     fps, size)\n    for frame in frames:\n        writer.write(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n    writer.release()","ee46f43a":"'''\n    params:\n        source : source image\n        driving : frames of driving video\n        generated : frames of generated video\n    return:\n        animation to display in html\n'''\ndef display(source, driving, generated=None):\n    fig = plt.figure(figsize=(8 + 4 * (generated is not None), 6))\n\n    ims = []\n    for i in range(len(driving)):\n        cols = [source]\n        cols.append(driving[i])\n        if generated is not None:\n            cols.append(generated[i])\n        im = plt.imshow(np.concatenate(cols, axis=1), animated=True)\n        plt.axis('off')\n        ims.append([im])\n\n    ani = animation.ArtistAnimation(fig, ims, interval=50, repeat_delay=1000)\n    plt.close()\n    return ani","e500c797":"'''\n    params:\n        images : list of images to display\n    return:\n        None\n'''\ndef display_image_grid(images):\n    plt.title(\"Plot Images\")\n    plt.axis('off')\n    plt.imshow(np.concatenate(images, axis=1))","649e5cbc":"'''\n    changing current working path to first-order-model repo.\n'''\nprev_path = os.getcwd()\nos.chdir(\"first-order-model\")","127d17e3":"config_path = \"config\/vox-256.yaml\"\ncheckpoint_path = \"..\/..\/input\/first-order-motion\/vox-cpk.pth.tar\"","0fe3c9bd":"from demo import load_checkpoints, make_animation\n\ngenerator, kp_detector = load_checkpoints(\n    config_path=config_path, \n    checkpoint_path=checkpoint_path\n)","28e6715e":"source_image_path = \"..\/..\/input\/first-order-motion\/got-03.png\"\ndriving_video_path = \"..\/..\/input\/first-order-motion\/10.mp4\"","9c82fd25":"source_image = imread(source_image_path, size=(256, 256))\ndriving_video = vidread(driving_video_path, size=(256,256))","a28f9ef6":"HTML(display(source_image, driving_video).to_html5_video())","abb642f1":"# with relative = True (source image is animated relative to transforms of itself)\npredictions = make_animation(source_image, driving_video, generator, kp_detector, relative=True)\nHTML(display(source_image, driving_video, predictions).to_html5_video())","5ec29cd8":"save_path = \"..\/got03.avi\"\nvidsave(save_path, predictions, fps=20, size=(256,256))","c60dae8a":"# with relative = False (source image is animated with respect to transform of driving video)\npredictions = make_animation(source_image, driving_video, generator, kp_detector, relative=False,\n                            adapt_movement_scale=False)\nHTML(display(source_image, driving_video, predictions).to_html5_video())","0de0b422":"predictions = make_animation(source_image, driving_video, generator, kp_detector, relative=False, \n                             adapt_movement_scale=True)\nHTML(display(source_image, driving_video, predictions).to_html5_video())","c104c108":"'''\n    params:\n        image : image to crop faces from\n        haarcascade_path : path to haarcascde xml file\n        size : size of cropped face images\n        margin_around : margin around detect box around face.\n            this is used to include some parts around faces like hair, neck soulders etc. value is tuned based on output needed.\n    return:\n        list of cropped images of faces\n'''\ndef crop_faces(image, haarcascade_path, size=(256,256), margin_around=50):\n    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    detector = cv2.CascadeClassifier(haarcascade_path)\n    rects = detector.detectMultiScale(gray, scaleFactor=1.05, minNeighbors=10, minSize=(30,30), flags=cv2.CASCADE_SCALE_IMAGE)\n    crop_images = []\n    for x, y, w, h in rects:\n        a = 0 if y - margin_around < 0 else y - margin_around\n        b = 0 if x - margin_around < 0 else x - margin_around\n        img = image[a:y + h + margin_around, b:x + w + margin_around]\n        img = cv2.resize(img, size)\n        crop_images.append(img)\n    return crop_images","ca329958":"!wget https:\/\/raw.githubusercontent.com\/opencv\/opencv\/master\/data\/haarcascades\/haarcascade_frontalface_default.xml","351ecae1":"source_image_url = \"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcTTOfrmyuVZ8YH0Z0jlTEKHeRhMQHwEu70XeXGSq0nPdTigAvJ6OF8F3O-deRz4P1VjEOE&usqp=CAU\"\ndriving_video_path = \"..\/..\/input\/first-order-motion\/00.mp4\"","5bf74da5":"source_image = imread(source_image_url, size=None, scale=False)\ndriving_video = vidread(driving_video_path, size=(256,256))","8d756bc1":"display_image_grid([source_image])","b27b9eb7":"haarcascade_path = \"haarcascade_frontalface_default.xml\"\nmargin_around = 20","5dd5ac37":"crop_images = crop_faces(source_image, haarcascade_path=haarcascade_path, size=(256,256), margin_around=margin_around)\ncrop_images = np.array(crop_images)\/255.0","a59e0107":"print(\"Number of Faces in image: \", len(crop_images))\ndisplay_image_grid(crop_images)","1710310a":"face_index = 0","24640d3b":"HTML(display(crop_images[face_index], driving_video).to_html5_video())","f9a39f46":"predictions = make_animation(crop_images[face_index], driving_video, generator, kp_detector, relative=True)\nHTML(display(crop_images[face_index], driving_video, predictions).to_html5_video())","9afd745f":"save_path = \"..\/messi.avi\"\nvidsave(save_path, predictions, fps=20, size=(256,256))","d55f9f8f":"os.chdir(prev_path)\n!rm -rf first-order-model","c9d8abee":"# Installing Dependencies\n\n- OpenCV : for reading images, video and face detection using cascade classifier to crop faces from images\n- First Motion Model: cloning first order motion from github\n- numpy, matplotlib : for array manipulation and plotting","1bdbd81f":"# Source image from path\n- source image is loaded from path in system\n- using a driving video we animate source image","3c6ef976":"# Source Image from URL\n- loading image from a internet using its url.","e0ecff1d":"# Helper Functions\n- imread : read and load image from path or url in numpy array.\n- vidread : read and load video from path or url as list of frames in numpy array.\n- vidsave : saving video file\n- display : display video and images as html\n- display_image_grid : display images as grid","c1674809":"Get all cropped face images from source image to transfer motion from driving video.","cff183a3":"# First Order Motion Model for Image Animation\nby Aliaksandr Siarohin, St\u00e9phane Lathuili\u00e8re, Sergey Tulyakov, Elisa Ricci and Nicu Sebe\n###### NeurIPS 2019\n\nImage animation consists of generating a video sequence so that an object in a source image is animated according to the motion of a driving video This Framework addresses this problem without using any annotation or prior information about the specific object to animate.Once trained on a set of videos depicting objects of the same category (e.g. faces, human bodies), this can be applied to any object of this class.\n\nThe model consist of two parts:\n- Motion estimation model : predict dense motion fields\n- Generation model : use source image and results from motion estimation model to generate frame.\n\n![first order motion model architecture](https:\/\/aliaksandrsiarohin.github.io\/first-order-model-website\/pipeline.png)\n\n- [First order motion page](https:\/\/aliaksandrsiarohin.github.io\/first-order-model-website\/)\n- [Project Github](https:\/\/github.com\/AliaksandrSiarohin\/first-order-model)\n- [Paper Link](http:\/\/papers.nips.cc\/paper\/8935-first-order-motion-model-for-image-animation)\n\nI will be explaining more detailed information in later versions.","f41f5cc5":"creating and saving animated video based on motion from driving video","e9c6bac0":"Select a cropped image from all cropped faces we get using its index","34a75f15":"downloading face haarcascade xml file from opencv github","5f4fdf28":"# Face detection to crop faces from image\n- It uses faces haarcascde to detect faces which is fast and give decent results.\n- We detect all faces and return list of crops of these faces.","9e4a65d3":"here we generate animated video of source image using motion of driving image","5f95a551":"# Loading pretrained model\n- model is created using config file and model checkpoints are loaded.\n- We have two models:\n    1. keypoint detector\n    2. generator model"}}