{"cell_type":{"cef4cb74":"code","83e048b8":"code","019bdd1c":"code","d976c6bd":"code","23ca332a":"code","a2037ecd":"code","75070bc5":"code","52eb63a7":"code","dd961538":"code","4191af8a":"code","2a6cdfca":"code","94d77f77":"code","91b18974":"code","cf5b1ee2":"code","d51fa5c6":"code","f032a0fb":"code","1d13be16":"code","9084e838":"code","ee1ec99b":"code","0e4e3573":"code","f4e13588":"code","cd329da1":"code","d1c2edbc":"code","036c6f60":"code","416aebe3":"code","cf40a687":"code","2aec07b5":"code","f3e91c12":"code","a6eb4b85":"markdown","9bfb7c5a":"markdown","e7a9d226":"markdown","c3bfc9f0":"markdown","a0f1a563":"markdown","af9a5dc8":"markdown","14cdf431":"markdown","c9cdb842":"markdown","4bfd38b6":"markdown","05f9b899":"markdown","09c2ca06":"markdown","df3794bb":"markdown","e0e4b7ca":"markdown"},"source":{"cef4cb74":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","83e048b8":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport PIL\nimport tensorflow as tf\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import (\n    BatchNormalization, Conv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense)\nfrom tensorflow.keras import backend as K","019bdd1c":"%%time\n!cp -r ..\/input\/capital-alphabets-28x28\/datasetSmall ..\/working","d976c6bd":"# changing the folder names of lower case dataset\nfor i in ['train','test','validation']:\n    path = f'.\/datasetSmall\/{i}'\n    files = os.listdir(path)\n    for file in files:\n        os.rename(os.path.join(path, file), os.path.join(path, f'Z{file}'))","23ca332a":"%%time\n!rsync -a ..\/input\/capital-alphabets-28x28\/dataset\/train\/ .\/datasetSmall\/train\/","a2037ecd":"%%time\n!rsync -a ..\/input\/capital-alphabets-28x28\/dataset\/test\/ .\/datasetSmall\/test\/","75070bc5":"%%time\n!rsync -a ..\/input\/capital-alphabets-28x28\/dataset\/validation\/ .\/datasetSmall\/validation\/","52eb63a7":"# Lets check the folder name\n!ls .\/datasetSmall\/test","dd961538":"# pixels range from 0 to 255 so dividing by 255 so value lie between 0 and 1\ntrain_datagen = ImageDataGenerator(rescale=1.\/255)\ntrain = train_datagen.flow_from_directory(\n    directory = \".\/datasetSmall\/train\",\n    target_size=(300, 300))","4191af8a":"val = train_datagen.flow_from_directory(\n    directory = \".\/datasetSmall\/validation\",\n    target_size=(300, 300))","2a6cdfca":"test = train_datagen.flow_from_directory(\n    directory = \".\/datasetSmall\/test\",\n    target_size=(300, 300))","94d77f77":"model = tf.keras.models.Sequential([\n    #input shape is the desired size of the image 300x300 with 3 bytes color\n    # This is the first convolution\n    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n    tf.keras.layers.MaxPooling2D(2, 2),\n    # The second convolution\n    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    # The third convolution\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    # The fourth convolution\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    # The fifth convolution\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    # Flatten the results to feed into a DNN\n    tf.keras.layers.Flatten(),\n    # 512 neuron hidden layer\n    tf.keras.layers.Dense(512, activation='relu'),\n    # 52 output neuron.\n    tf.keras.layers.Dense(52, activation='softmax')\n])","91b18974":"model.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","cf5b1ee2":"model.summary()","d51fa5c6":"from tqdm import tqdm\n# extracting values from val test generator\nval.reset()\nX_val,y_val = next(val)\nfor i in tqdm(range(int(len(val)\/32)-1)): #1st batch is already fetched before the for loop.\n    img, label = next(val)\n    X_val = np.append(X_val, img, axis=0 )\n    y_val = np.append(y_val, label, axis=0)\nprint(X_val.shape, y_val.shape)","f032a0fb":"checkpointer = EarlyStopping(monitor = 'val_accuracy', verbose = 1, restore_best_weights=True,mode=\"max\",patience = 15)\n# checkpointer to monitor accuracy and preventing overfit","1d13be16":"%%time\nhistory = model.fit(\n            train ,\n            steps_per_epoch =len(train)\/\/32, #batch size is 32\n            epochs=200,\n            verbose=1,\n            validation_data=(X_val,y_val),\n            callbacks = [checkpointer])","9084e838":"training_loss = history.history['loss']\nval_loss = history.history['val_loss']\nplt.rcParams['figure.figsize'] = [10, 5]\nplt.style.use(['default'])\n# Create count of the number of epochs\nepoch_count = range(1, len(training_loss) + 1)\n\n# Visualize loss history\nplt.plot(epoch_count, training_loss, 'r--')\nplt.plot(epoch_count, val_loss, 'b-')\nplt.legend(['Training Loss', 'Val Loss'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.show()","ee1ec99b":"training_accuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\n\n# Create count of the number of epochs\nepoch_count = range(1, len(training_accuracy) + 1)\n\n# Visualize loss history\n\nplt.plot(epoch_count, training_accuracy, 'r--')\nplt.plot(epoch_count, val_accuracy, 'b-')\nplt.legend(['Training Accuracy', 'Val Accuracy'])\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim(top = 1)\nplt.show()","0e4e3573":"print(f\"Final Train accuracy = {model.evaluate(train ,batch_size=32,steps=len(test)\/\/32)[1]*100}%\")\nprint(f\"Validation accuracy = {model.evaluate(val ,batch_size=32,steps=len(test)\/\/32)[1]*100}%\")","f4e13588":"# saving the model\nmodel.save(\"alpharecognition.h5\")","cd329da1":"x_test,y_test = next(test)\npredict = model.predict(x_test)","d1c2edbc":"print(f\"Test accuracy = {model.evaluate(test ,batch_size=32,steps=len(test)\/\/32)[1]*100}%\")","036c6f60":"figure = plt.figure(figsize=(20, 8))\nfor i, index in enumerate(np.random.choice(x_test.shape[0], size=24, replace=False)):\n    ax = figure.add_subplot(4, 6, i + 1, xticks=[], yticks=[])\n    ax.imshow(np.squeeze(x_test[index]))\n    predict_index = np.argmax(predict[index])\n    true_index = np.argmax(y_test[index])\n    ax.set_title(\"{} ({})\".format(chr(predict_index+65 if predict_index<26 else predict_index+71), \n                                  chr(true_index+65 if true_index<26 else true_index+71)),\n                                  color=(\"green\" if predict_index == true_index else \"red\"))","416aebe3":"for i in tqdm(range(int(len(test)\/32)-1)): #1st batch is already fetched before the for loop in above cells.\n    img, label = next(test)\n    x_test = np.append(x_test, img, axis=0 )\n    y_test = np.append(y_test, label, axis=0)\nprint(x_test.shape, y_test.shape)","cf40a687":"from sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import confusion_matrix\nprediction = model.predict(x_test)\ncm = confusion_matrix(y_test.argmax(axis=1), prediction.argmax(axis=1))\nlabels = [chr(i+65) if i<26 else chr(i+71) for i in range(52)]","2aec07b5":"plt.rcParams['figure.figsize'] = [80, 80]\nfont = {'family' : 'normal',\n        'weight' : 'bold',\n        'size'   : 26}\n\nplt.rc('font', **font)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\ndisp.plot(cmap=plt.cm.Blues)\nplt.show()","f3e91c12":"%%time\n!rm -r .\/datasetSmall","a6eb4b85":"## Model Creation","9bfb7c5a":"## Data Preparation","e7a9d226":"## Model's Performance","c3bfc9f0":"We need to first make all uppercase and lowercase train test files in same directory so we and change the folder name as they would have same folder name, a == A. So i am changing the lowercase folder name to Z_ meaning Za , Zb ... so they will come last alphabetically. This will help when we get our prediction index. if index will be in between 0-25 then we habe upper case else lower.","a0f1a563":"> rsync command merges folders with same name. So train folder will have A,B,C....Z,Za,Zb,Zc ... and so on","af9a5dc8":"## Importing Libraries","14cdf431":"> ###  If you find this notebook helpful, do consider to upvote.","c9cdb842":"## Plotting  Confusion Matrix","4bfd38b6":"> We see that model is doing great and getting wrong in poor or same(upper and lower case) images","05f9b899":"## Test Accuracy","09c2ca06":"> The model could perform better if we provide clean and more data. Also some same looking letter is also hitting the accuracy like letter i,s,z,x,y,w,v,u...etc","df3794bb":"## Processing Datasets","e0e4b7ca":"## Test Predictions Visualization"}}