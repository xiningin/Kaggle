{"cell_type":{"c841513b":"code","8b14361b":"code","df4b4264":"code","df71cd0a":"code","f75ce380":"code","7ba99502":"code","c8d0dd96":"code","a91c316f":"code","33d5b98e":"code","1e206b55":"code","b65c8f6f":"code","805a3558":"code","0a8495fd":"code","ad363515":"code","f832f9a5":"code","077e5d8e":"code","a33647ed":"code","fb1d8c62":"code","16acb01b":"code","82fdac53":"code","35355090":"code","2ad877d0":"code","faa55d9b":"code","79bf1b9e":"code","af9c4b73":"code","eeafe0a5":"code","e6f482fd":"code","20f71050":"code","38b0b50f":"code","b7e846f0":"code","90ddd7ca":"code","afce51e8":"code","368cb3a9":"code","e3b9e05d":"code","5078bd20":"code","e97a3665":"code","8116cce9":"code","9e55d6b9":"code","24e56757":"code","925309c6":"code","b25bd580":"code","a91b7571":"code","9dcbe61b":"code","4db4da83":"code","0d3fb9b2":"code","da7cf926":"code","9ca11453":"code","0f584d0e":"code","5f19408b":"code","fd91fdeb":"code","e88d1eee":"code","2e5b415d":"code","3311a3a8":"code","bbe9d88f":"code","10d3fcb2":"code","d375ba21":"code","89ce688b":"code","5541c801":"code","ae0b24ff":"code","80e24d67":"code","ab08f6d4":"code","a136fb3c":"code","9dc7e46e":"code","d3a02c0d":"code","7dae3d44":"code","61941c20":"code","8a9c9f82":"code","29e9c335":"code","5266c688":"code","4818acab":"code","82e7cdec":"code","7f12bb79":"code","edcd3033":"code","942aa461":"code","0c650edd":"code","d45f91b3":"code","a90474de":"code","af218e3c":"code","23b546f4":"markdown","bf63419b":"markdown","52bd9c29":"markdown","6347e3be":"markdown","8ae86495":"markdown","ece715c9":"markdown","7c132ac2":"markdown","a18e8d99":"markdown","5f216357":"markdown","a221c48d":"markdown","7dd260c8":"markdown","e1e3be1b":"markdown","d9d481a8":"markdown","52872ac1":"markdown","fbbb2f85":"markdown","4cc81422":"markdown","b145f167":"markdown","97469961":"markdown","67483d9e":"markdown","84bf43c2":"markdown","6e83dce1":"markdown","9a0bea0b":"markdown","7f42b2b9":"markdown","2d35139b":"markdown","3d32d003":"markdown","c17324bc":"markdown","7d74593a":"markdown","ff62b1f9":"markdown","7d13a619":"markdown","240b760d":"markdown","b8065471":"markdown","67e75c61":"markdown","089d5a98":"markdown","3a14d0a8":"markdown","4d0df320":"markdown","514f49c6":"markdown","3f1203d7":"markdown","2c7c325a":"markdown","59d86480":"markdown","f6c5f9ec":"markdown","7704323f":"markdown","e29a3944":"markdown","d2b59aa0":"markdown","c122fe9e":"markdown","327700ac":"markdown","4bd0b8d9":"markdown","24de8075":"markdown","cd847d4b":"markdown","bcfd6904":"markdown","389a82b7":"markdown","6d0668fe":"markdown","83bd9340":"markdown"},"source":{"c841513b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8b14361b":"#import Library\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n%matplotlib inline","df4b4264":"data = pd.read_csv(\"..\/input\/default-of-credit-card-clients-dataset\/UCI_Credit_Card.csv\")","df71cd0a":"data.head()","f75ce380":"#Adjusting rows and columns\ndata.set_index('ID',inplace=True)\ndata.rename(columns = {\"SEX\":\"GENDER\",\"MARRIAGE\":'MARITAL_STATUS', 'PAY_0' : 'PAY_1', \"LIMIT_BAL\" : \"CREDIT_LIMIT\", \"default.payment.next.month\" : \"DEFAULT_STATUS\"}, inplace = True)","7ba99502":"print(\"For Data Description and Explaination of Variables, please refer to `APPENDIX`\")\nprint()\ndata.info()","c8d0dd96":"# creat a copy, in case we need variables in the format of number for analysis\ndata_original=data.copy()","a91c316f":"data.describe()","33d5b98e":"data.GENDER.unique()","1e206b55":"data[\"GENDER\"]=data[\"GENDER\"].apply(lambda x: \"Male\" if x==1 else \"Female\")","b65c8f6f":"data.EDUCATION.unique()","805a3558":"data[\"EDUCATION\"] = data[\"EDUCATION\"].apply(lambda x: \"Graduate School\" if x==1 else (\"Univeristy\" if x==2 else (\"High School\" if x==3 else \"Others\")))","0a8495fd":"data.MARITAL_STATUS.unique()","ad363515":"data.PAY_1.unique()","f832f9a5":"for i in data.iloc[:,5:11]:\n    data[i] = data[i].apply(lambda x: \"No Delay\" if x<0 else \"Delay\")","077e5d8e":"#Generating Frequency Distribution histograms of Numerical Variables\n\nhist_num = data.iloc[:,:-1].hist(figsize=(20,15))\nplt.suptitle('Frequency Distribution of Numerical Varibales', x=0.5, y=1.05, ha='center', fontsize='xx-large')\nplt.tight_layout()","a33647ed":"# Log the Credit Limit in new column\ndata[\"Log_limit\"] = np.log(data.CREDIT_LIMIT)\n\n# Log the Payment Amounts in new columns\nindex=1\nfor i in data.iloc[:,-8:-2]:\n    log_pay_amt=\"Log_pay_amt\"+str(index)\n    data[log_pay_amt] = np.log(data[i]+1)\n    index=index+1\n    \ndata_log_transformed = data_original.copy()\ndata_log_transformed[\"CREDIT_LIMIT\"] = data[\"Log_limit\"]\n\n# New data_log_transformed dataframe \ndata_log_transformed = data_original.copy()\ndata_log_transformed[\"CREDIT_LIMIT\"] = data[\"Log_limit\"]\n# Log the Payment Amounts in new columns\nindex=1\nfor i in data_log_transformed.iloc[:,-7:-1]:\n    log_pay_amt=\"Log_pay_amt\"+str(index)\n    data_log_transformed[log_pay_amt] = np.log(data_log_transformed[i]+1)\n    index=index+1\n    \n# Log the Bill Amounts in new columns\nindex=1\nfor i in data_log_transformed[['BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6']]:\n    log_pay_amt=\"Log_bill_amt\"+str(index)\n    data_log_transformed[log_pay_amt] = data_log_transformed[i].apply( lambda x: np.log1p(x) if (x>0) else 0 )\n    index += 1\n    \n    \ndata_log_transformed = data_log_transformed.drop(\n    columns=['BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6',\n             'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3','PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6',\n            'CREDIT_LIMIT'])\n\n#Check the result of logged distribution of Credit Limit and Payment Amount\nhist_num = data.iloc[:,-7:].hist(figsize=(20,15))\nplt.suptitle(\"Frequency Distribution of Log (Credit Limit) and Log(Payment Amount + 1) variables\", x=0.5, y=1.05, ha='center', fontsize='xx-large')\nplt.tight_layout()","fb1d8c62":"fig, axes = plt.subplots(figsize=(15,15),nrows=3, ncols=3)\nplt.suptitle('Frequency Distribution of Categorical Varibales', x=0.5, y=1.05, ha='center', fontsize='x-large')\ndata['GENDER'].value_counts().plot.bar(ax=axes[0,0],title=\"Gender\")\ndata['EDUCATION'].value_counts().plot.bar(ax=axes[0,1],title=\"Education\")\ndata['MARITAL_STATUS'].value_counts().plot.bar(ax=axes[0,2],title=\"Marital_Status\")\ni_row=1\ni_col=0\ncount=1\nfor i in data.iloc[:,5:11]:\n    data[i].value_counts().plot.bar(ax=axes[i_row,i_col],title=\"Pay_Status_\"+str(count))\n    count=count+1\n    i_col=i_col+1\n    if count>=4:\n        i_row=2\n    if count==4:\n        i_col=0\n# set title and axis labels\nplt.tight_layout()\nplt.show()\n","16acb01b":"plt.suptitle('Frequency Distribution of Target Value', x=0.5, y=1.05, ha='center', fontsize='x-large')\ndata['DEFAULT_STATUS'].value_counts().plot.bar(title=\"Defaulted or not\")","82fdac53":"ave_defalt_rate = round(np.mean(data[\"DEFAULT_STATUS\"]),3)*100\nprint(\"The average default rate of the dataset is \" + str(ave_defalt_rate) +\"%\")","35355090":"f, ax = plt.subplots(figsize = (20, 20))   \n# this is to set fig size\n\ncorrelational_matrix = data_original.corr()\n#calculate correlation matrix and assign it\n\nmask = np.triu(np.ones_like(correlational_matrix, dtype = np.bool))\n\n\ncmap = sns.diverging_palette(500, 10, as_cmap = True) \n# this is just to set color range\n\n# Range of correlational coefficients: -1 through 1\n\nsns.heatmap(correlational_matrix,        \n            mask = mask,\n            cmap = cmap,                 #set color range\n            vmax = 1,                    #affects the color range\n            center = 0,                  #affects the color range\n            square = True,               \n            annot= True,                 #add annotation\n            fmt=\".1f\",                   #set decimal place\n            linewidths = 1,              #set line width\n            cbar_kws = {\"shrink\": 0.5})  #shrink color bar by 0.5 times","2ad877d0":"data_log_transformed.head()","faa55d9b":"# Visualization of Credit Limite and Default Count using histogram\nfig, ax = plt.subplots(figsize = (10, 6))\nplt.hist(data.Log_limit, bins = 40, alpha = 1, color = \"yellow\", label=\"Total\")\nplt.hist(data.query('DEFAULT_STATUS == 0').Log_limit, bins = 40, alpha = 1, color = \"orange\", label = \"Not Default\")\nplt.hist(data.query('DEFAULT_STATUS == 1').Log_limit, bins = 40, alpha = 1, color = \"red\", label = \"Default\")\nplt.xlabel(\"Log of Credit Limit\")\nplt.ylabel(\"Count\")\nplt.title(\"Log value of Credit Limit VS Number of Defalt\")\nplt.grid()\nplt.legend()\nplt.show()","79bf1b9e":"subset = data[['PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', \n               'PAY_5', 'PAY_6', 'DEFAULT_STATUS']]\n\nf, axes = plt.subplots(2, 3, figsize=(15, 13), facecolor='white')\nf.suptitle('FREQUENCY OF DEFAULT (BY HISTORY OF DEFAULT)')\n\nax1 = sns.countplot(x=\"PAY_1\", hue=\"DEFAULT_STATUS\", data=subset, palette=\"Greens\", ax=axes[0,0])\nax2 = sns.countplot(x=\"PAY_2\", hue=\"DEFAULT_STATUS\", data=subset, palette=\"Blues\", ax=axes[0,1])\nax3 = sns.countplot(x=\"PAY_3\", hue=\"DEFAULT_STATUS\", data=subset, palette=\"Greens\", ax=axes[0,2])\nax4 = sns.countplot(x=\"PAY_4\", hue=\"DEFAULT_STATUS\", data=subset, palette=\"Blues\", ax=axes[1,0])\nax5 = sns.countplot(x=\"PAY_5\", hue=\"DEFAULT_STATUS\", data=subset, palette=\"Greens\", ax=axes[1,1])\nax6 = sns.countplot(x=\"PAY_6\", hue=\"DEFAULT_STATUS\", data=subset, palette=\"Blues\", ax=axes[1,2]);","af9c4b73":"print(\"Histogram by Age\")\n\nplt.figure(figsize = (8 , 6))\nsns.distplot(data.query('DEFAULT_STATUS == 1').AGE, bins = 20, color=\"green\")\nmean_age = data.AGE.mean()\nplt.axvline(mean_age,0,1, color = \"blue\")","eeafe0a5":"#define a function to categorize age group\n\ndef get_group (age):\n        if age < 40:\n            return \"Young\"\n        elif age >= 60:\n            return \"Old\"\n        else:\n            return \"Middle\"","e6f482fd":"# apply to \"AGE\" and create a new column\ndata[\"Age_group\"] = data[\"AGE\"].apply(get_group)","20f71050":"print(\"Age Group VS Default Rate\")\n\nplt.figure(figsize = (5 , 5))\nsns.barplot(x = 'Age_group', y = \"DEFAULT_STATUS\", data = data)\nplt.show()","38b0b50f":"# Define a function to plot barplot between categorical variable and default status\ndef plot_cat(categorical_variable):\n    sns.barplot(x = categorical_variable, y=\"DEFAULT_STATUS\", data=data)\n    plt.figure(figsize=(10,6))\n    plt.show()","b7e846f0":"print(\"Default Rate VS Gender\")\nplot_cat(\"GENDER\")","90ddd7ca":"print(\"Default Rate VS Education Background\")\nplot_cat(\"EDUCATION\")","afce51e8":"print(\"Default Rate VS Marital Status\")\nplot_cat(\"MARITAL_STATUS\")","368cb3a9":"print(\"Default Rate VS History of Delaying Payment in previous month (SEP)\")\nplot_cat(\"PAY_1\")","e3b9e05d":"print(\"Grouped age and history payment status vs Default Rate\")\n\nsns.barplot(x = 'Age_group', y = \"DEFAULT_STATUS\", data = data, hue = \"PAY_1\")\nplt.show()","5078bd20":"print(\"Grouped age and education background vs Default Rate\")\n\nsns.barplot(x = 'Age_group', y = \"DEFAULT_STATUS\", data = data, hue = \"EDUCATION\")\nplt.show()","e97a3665":"print(\"Ploting log of PAY_AMT Histogram\")\n\nplt.figure(figsize = (8 , 6))\nsns.distplot(data.query('DEFAULT_STATUS == 1').Log_pay_amt1, bins = 20, color=\"green\")\nmean_amt1 = data.Log_pay_amt1.mean()\nplt.axvline(mean_amt1,0,1, color = \"blue\")","8116cce9":"# Define our function \ndef log_amt(x):\n    if x<2.5:\n        return \"low\"\n    elif x>=2.5 and x<9:\n        return \"medium\"\n    else:\n        return \"high\"","9e55d6b9":"# Apply get_amt function to SEP(PAY_AMT1) and create new column\ndata[\"log_amt1_group\"]=data[\"Log_pay_amt1\"].apply(lambda x:log_amt(x))","24e56757":"print(\"Default Status VS Payment Amount in Sep by Group\")\nplot_cat(\"log_amt1_group\")","925309c6":"# Apply get_amt function to AUG(PAY_AMT2) and create new column\ndata[\"log_amt2_group\"]=data[\"Log_pay_amt2\"].apply(lambda x:log_amt(x))","b25bd580":"print(\"Default Status VS Payment Amount in Aug by Group\")\nplot_cat(\"log_amt2_group\")","a91b7571":"# Apply the same formula to other months\ndata[\"log_amt3_group\"]=data[\"Log_pay_amt3\"].apply(lambda x:log_amt(x))\ndata[\"log_amt4_group\"]=data[\"Log_pay_amt4\"].apply(lambda x:log_amt(x))\ndata[\"log_amt5_group\"]=data[\"Log_pay_amt5\"].apply(lambda x:log_amt(x))\ndata[\"log_amt6_group\"]=data[\"Log_pay_amt6\"].apply(lambda x:log_amt(x))","9dcbe61b":"f, axes = plt.subplots(2, 2, figsize=(15, 13), facecolor='white')\n\nax1 = sns.barplot(x = \"log_amt3_group\", y=\"DEFAULT_STATUS\", data=data, ax=axes[0,0])\nax2 = sns.barplot(x = \"log_amt4_group\", y=\"DEFAULT_STATUS\", data=data, ax=axes[0,1])\nax3 = sns.barplot(x = \"log_amt5_group\", y=\"DEFAULT_STATUS\", data=data, ax=axes[1,0])\nax4 = sns.barplot(x = \"log_amt6_group\", y=\"DEFAULT_STATUS\", data=data, ax=axes[1,1])\n","4db4da83":"import sklearn\nfrom sklearn.model_selection import train_test_split","0d3fb9b2":"import statsmodels.api as sm","da7cf926":"data_original.info()","9ca11453":"y = data_original['DEFAULT_STATUS']\nX = data_original.drop(columns=['DEFAULT_STATUS'])","0f584d0e":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","5f19408b":"log_reg = sm.Logit(y_train, X_train).fit() ","fd91fdeb":"print(log_reg.summary()) ","e88d1eee":"yhat = log_reg.predict(X_test) \nprediction = list(map(round, yhat)) \n\nfrom sklearn.metrics import (confusion_matrix,  \n                           accuracy_score) \n  \n# confusion matrix \ncm = confusion_matrix(y_test, prediction)  \nprint (\"Confusion Matrix : \\n\", cm)  \n  \n# accuracy score of the model \nprint('Test accuracy = ', accuracy_score(y_test, prediction))\n","2e5b415d":"X_train=X_train[[\"CREDIT_LIMIT\", \"GENDER\", \"EDUCATION\",\"MARITAL_STATUS\",\"PAY_1\", \"PAY_2\", \"PAY_3\", \"BILL_AMT1\", \"PAY_AMT1\", \"PAY_AMT2\"]]\nX_test=X_test[[\"CREDIT_LIMIT\", \"GENDER\", \"EDUCATION\",\"MARITAL_STATUS\",\"PAY_1\", \"PAY_2\", \"PAY_3\", \"BILL_AMT1\", \"PAY_AMT1\", \"PAY_AMT2\"]]","3311a3a8":"log_reg1 = sm.Logit(y_train, X_train).fit() ","bbe9d88f":"print(log_reg1.summary()) ","10d3fcb2":"yhat = log_reg1.predict(X_test) \nprediction = list(map(round, yhat)) \n\nfrom sklearn.metrics import (confusion_matrix,  \n                           accuracy_score) \n  \n# confusion matrix \ncm = confusion_matrix(y_test, prediction)  \nprint (\"Confusion Matrix : \\n\", cm)  \n  \n# accuracy score of the model \nprint('Test accuracy = ', accuracy_score(y_test, prediction))\n","d375ba21":"data.info()","89ce688b":"data_d=data.drop(columns=['CREDIT_LIMIT','AGE','PAY_AMT1','PAY_AMT1','PAY_AMT2','PAY_AMT3',\n                          'PAY_AMT4','PAY_AMT5','PAY_AMT6','Log_pay_amt1','Log_pay_amt1','Log_pay_amt2',\n                          'Log_pay_amt3','Log_pay_amt4','Log_pay_amt5','Log_pay_amt6'])","5541c801":"data_d=pd.get_dummies(data_d)\ny = data_d['DEFAULT_STATUS']\nX = data_d.drop(columns=['DEFAULT_STATUS'])","ae0b24ff":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","80e24d67":"log_reg2 = sm.Logit(y_train, X_train).fit() ","ab08f6d4":"print(log_reg2.summary()) ","a136fb3c":"yhat = log_reg2.predict(X_test) \nprediction = list(map(round, yhat)) \n\nfrom sklearn.metrics import (confusion_matrix,  \n                           accuracy_score) \n  \n# confusion matrix \ncm = confusion_matrix(y_test, prediction)  \nprint (\"Confusion Matrix : \\n\", cm)  \n  \n# accuracy score of the model \nprint('Test accuracy = ', accuracy_score(y_test, prediction))","9dc7e46e":"X_train = X_train['Log_limit']\nX_test = X_test['Log_limit']","d3a02c0d":"log_reg3 = sm.Logit(y_train, X_train).fit()\nprint(log_reg3.summary())","7dae3d44":"yhat = log_reg3.predict(X_test) \nprediction = list(map(round, yhat)) \n\nfrom sklearn.metrics import (confusion_matrix,  \n                           accuracy_score) \n  \n# confusion matrix \ncm = confusion_matrix(y_test, prediction)  \nprint (\"Confusion Matrix : \\n\", cm)  \n  \n# accuracy score of the model \nprint('Test accuracy = ', accuracy_score(y_test, prediction))","61941c20":"from sklearn.utils import resample","8a9c9f82":"data.columns","29e9c335":"data_majority = data[data.DEFAULT_STATUS==0]\ndata_minority = data[data.DEFAULT_STATUS==1]\n\nprint(data_majority.DEFAULT_STATUS.count())\nprint(\"-----------\")\nprint(data_minority.DEFAULT_STATUS.count())\nprint(\"-----------\")\nprint(data.DEFAULT_STATUS.value_counts())","5266c688":"# Upsample minority class\ndata_minority_upsampled = resample(data_minority, \n                                 replace=True,     # sample with replacement\n                                 n_samples=23364,    # to match majority class\n                                 random_state=777) # reproducible results\n\n# Combine majority class with upsampled minority class\ndata_upsampled = pd.concat([data_majority, data_minority_upsampled])\n# Display new class counts\ndata_upsampled.DEFAULT_STATUS.value_counts()","4818acab":"# Downsample majority class\ndata_majority_downsampled = resample(data_majority, \n                                 replace=False,    # sample without replacement\n                                 n_samples=6636,     # to match minority class\n                                 random_state=777) # reproducible results\n\n# Combine minority class with downsampled majority class\ndata_downsampled = pd.concat([data_majority_downsampled, data_minority])\n# Display new class counts\ndata_downsampled.DEFAULT_STATUS.value_counts()","82e7cdec":"## remember to pip install imbalanced-learn\n\nfrom imblearn.over_sampling import SMOTE","7f12bb79":"y = data_log_transformed['DEFAULT_STATUS']\nX = data_log_transformed.drop(columns=['DEFAULT_STATUS'])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","edcd3033":"smote = SMOTE(random_state=123)\nX_SMOTE, y_SMOTE = smote.fit_sample(X_train, y_train)\nprint(len(y_SMOTE))\nprint(y_SMOTE.sum())","942aa461":"y_SMOTE.value_counts()","0c650edd":"log_reg_smote = sm.Logit(y_SMOTE, X_SMOTE).fit()\nprint(log_reg_smote.summary())","d45f91b3":"#-------------- \n# logistic regression \n#--------------\nyhat = log_reg_smote.predict(X_test) \nprediction = list(map(round, yhat)) \n\nfrom sklearn.metrics import (confusion_matrix,  \n                           accuracy_score) \n  \n# confusion matrix \ncm = confusion_matrix(y_test, prediction)  \nprint (\"Confusion Matrix : \\n\", cm)  \n  \n# accuracy score of the model \nprint('Test accuracy = ', accuracy_score(y_test, prediction))","a90474de":"#-------------- \n# kernel SVM \n#--------------\nfrom sklearn.svm import SVC\nclassifier1 = SVC(kernel=\"rbf\")\nclassifier1.fit( X_SMOTE, y_SMOTE )\ny_pred = classifier1.predict( X_test )\n\ncm = confusion_matrix( y_test, y_pred )\nprint(\"Accuracy on Test Set for kernel-SVM = %.2f\" % ((cm[0,0] + cm[1,1] )\/len(X_test)))\n# confusion matrix \nprint (\"Confusion Matrix : \\n\", cm) ","af218e3c":"exit()","23b546f4":"**c. Marital Status**","bf63419b":"### Training logistic regression model with chosen independent variables","52bd9c29":"> `Observation:` \n>\n> Logged PAY_AMT graph is more curved than the original graph\n>\n> Other than \"zero payments\", the remaining data tends to be normally distributed","6347e3be":">`Interpretation`\n>\n>- `Low` Payment Amount in the past tends to have higher` Default Rate in Target Month\n>- `High` Payment Amount in the past tends to have lower Default Rate in Target Month\n>- This pattern is consistant throught out from April to Sep Data ","8ae86495":"# 5. Conclusion","ece715c9":"# 4. Modelling","7c132ac2":">`Interpretation:`\n>   \n>- Higher age group has higher default rate\n\n> Default Rate Ranking by `Age Group`:\n> \n> `Old Age` > `Middle Age` > `Young Age`","a18e8d99":"Since our dependent variable \"DEFAULT_STATUS\" is categorical, we can split the distributions by \"default\/not-default\" to look at the characteristics of the default\/not-default groups in relation to our independent variables. We want to see how differentiated our target variable is against each individual independent variable  ","5f216357":">let's combine 0,4,5 and 6 to `\"others\"` catergory and we can got the following categories:\n>\n>1. Graduate School\n>2. University\n>3. High School\n>4. Others\n","a221c48d":"**b. Education**\n","7dd260c8":"#### Let's categorize the age into following age groups\n* age < 40 `young`\n* age >=40 and age < 60 `middle`\n* greater than 60 `old`","e1e3be1b":"#### Correlation Matrix","d9d481a8":">`Interpretation:`\n   > \n>- Male has higher default rate than Female","52872ac1":">`Interpretation`\n>\n>- `Low` Payment Amount in `Sep` has `higher` Default Rate in Target Month\n>- `High` Payment Amount in `Sep` has `lower` Default Rate in Target Month\n","fbbb2f85":"#### Data Description\n\n\n\n| CODE |  REPLACED NAME | DESCRIPTION | UNIT |\n|---|---|---|---|\n|X1|CREDIT_LIMIT|Amount of the given credit| NT dollar|\n|X2|GENDER|Gender|1 = male; 2 = female|\n|X3|EDUCATION|Education level|1 = graduate school; 2 = university; 3 = high school; 4 = others|\n|X4|MARITAL_STATUS|Marital status|1 = married; 2 = single; 3 = others|\n|X5|AGE|Age |year|\n|X6|PAY_1|Repayment status - Sep|-1 = pay duly; 1 = payment delay for 1 month; 2 = payment delay for 2 months|\n|X7|PAY_2|Repayment status - Aug|-1 = pay duly; 1 = payment delay for 1 month; 2 = payment delay for 2 months|\n|X8|PAY_3|Repayment status - Jul|-1 = pay duly; 1 = payment delay for 1 month; 2 = payment delay for 2 months|\n|X9|PAY_4|Repayment status - Jun|-1 = pay duly; 1 = payment delay for 1 month; 2 = payment delay for 2 months|\n|X10|PAY_5|Repayment status - May|-1 = pay duly; 1 = payment delay for 1 month; 2 = payment delay for 2 months|\n|X11|PAY_6|Repayment status - Apr|-1 = pay duly; 1 = payment delay for 1 month; 2 = payment delay for 2 months|\n|X12|BILL_AMT1|Amount of bill statement - Sep| NT dollar|\n|X13|BILL_AMT2|Amount of bill statement - Aug| NT dollar|\n|X14|BILL_AMT3|Amount of bill statement - Jul| NT dollar|\n|X15|BILL_AMT4|Amount of bill statement - Jun| NT dollar|\n|X16|BILL_AMT5|Amount of bill statement - May| NT dollar|\n|X17|BILL_AMT6|Amount of bill statement - Apr| NT dollar|\n|X18|PAY_AMT1|Amount paid in Sep| NT dollar|\n|X19|PAY_AMT2|Amount paid in Aug| NT dollar|\n|X20|PAY_AMT3|Amount paid in Jul| NT dollar|\n|X21|PAY_AMT4|Amount paid in Jun| NT dollar|\n|X22|PAY_AMT5|Amount paid in May| NT dollar|\n|X23|PAY_AMT6|Amount paid in Apr| NT dollar|\n|Y|DEFAULT_STATUS|Default payment in Oct|default = 1, not default = 0|\n","4cc81422":"# 3. EDA","b145f167":"### Using just log CREDIT_LIMIT as independent variable\n","97469961":">`Interpretation:`\n >   \n>- Customer with delay history in Sep (`PAY_1`) has higher chance of default in Oct (`Target Value`)","67483d9e":"> **a. Gender**","84bf43c2":">`Interpretation`\n>\n>- the default rate of customer having `High School` Education background is very close across all age agroup\n>- The default rate of customer having `University` and `Graduate School` Education background is high for `Old` Age >Group, and reletively low for `Young` and `Middle` age group","6e83dce1":"# 7. Appendix","9a0bea0b":">let's combine 0 and 3 to `\"others\"` catergory and we can get the following categories\n>\n>1. Married\n>2. Single\n>3. Others","7f42b2b9":"#### Generate Frequency Distribution Histogram of Categorical Variables","2d35139b":">`Interpretation`\n>\n>- The peak on the left represent customer with zero payment in Sep (`PAY_AMT1`)\n>- Other than zero-payment customers, the of the logged payment amount data are close to normal distribution.","3d32d003":">According to appendix, we can got the following categories:\n>\n>1. Male\n>2. Female","c17324bc":"#### Let's try using log to make them more close to bell the curve by apply log function","7d74593a":">`Interpretation`\n>\n>Cards with lower Logged `Credit Limit` (below 11) tend to have higher defaults.","ff62b1f9":">`Interpretation`\n>- There is a positive correlation between `Default Status` and `PAY_1 to PAY_6`. However, the correlation is still >considered low, because it is only between 0.2 and 0.3\n>\n>- There is a negative correlation between `Default Status` and `Credit Limit`, However, the -0.2 correlation is considered as low.\n>\n>- There is a strong relationship for variables within the same category, such as \"Bill_AMT\", \"Payment History\"","7d13a619":">`Interpretation:`\n>   \n>- Majority of the credit card holders are between 20 to 30 years old","240b760d":"#### Gerenal Description of the Data Set\n\n>- The data set has 30,000 samples\n>- Average Credit Limit is 167,484\n>- Majority of the samples are female, average age is 35 years old\n>- The average defaul rate is 22.1%\n\nWe noticed that some categorical variables do not match data description, for example:\n>- `Education` has maximum value of 6 which is not in the data description \n>- `Matrital Status` has minimum value of 0 which is not in the data description \n\nSo let's take a look at the unique values of categorical variables","b8065471":"### Sample of data using over-sampling, under-sampling and SMOTE algorithm","67e75c61":"#### History of Payment Amount VS Default Status\nUse Function to categorize the log of payment amount in Sep (`PAY_1`) into the following groups:\n* log of previous_payment_amount in September(Log_pay_amt1) < 2.5 `low`\n* Log_pay_amt1 >=2.5 and Pay_amt1 < 9 `medium`\n* Log_pay_amt1 >= 9 `high`","089d5a98":">`Interpretation`\n>\n>Customer default in Oct(Target Value) has more history of delaying payment in the past 6 months.","3a14d0a8":">`Interpretation`\n>- Some variables have strongly right-skewed distribution(eg: Credit Limit, Pay_amt)","4d0df320":"# 1. Problem Statment\n\n**I. Background & Problem Statement**\n\nCredit card is one of the major consumer lending product in the market. When a credit card is defaulted, bank can use the opportunity to sell loan, If bank recognize that they are not able sell it, they will write if off and result in significant finance losses to the bank. Predicting accurately which customers are most likely to default represents significant business opportunity for all banks, at the same time it helps banks set up their proactive default prevention guideline to improve their bottom line.\n\nThis kernal focuses on Taiwan borrowers who constitute a portion of the Taiwan loan market. Our goal is to build a suitable model to predict whether a customer will default on credit card payment(Y) based on variables such as personal information and historical payment status (X).\n\n**II. Data Description**\nOur dataset consists of 30000 borrowers, who held at least one loan that entered repayment between April to September 2005. The borrower\u2019s personal information is provided in the form of 5 categorial attributes and 12 billing and payment history. \n\nClick [here](https:\/\/archive.ics.uci.edu\/ml\/datasets\/default+of+credit+card+clients) for the source of the data from `UCI machine learning repository ` [1]\n\n**III. Design & Findings**\nWe started with some data cleaning and wrangling to better visualize our data and prepare it for model training. Through frequency plots and histograms, we observed that many of our price data are heavily right skewed and therefore applied a log function to balance out the distribution. We further observed that the dataset also has more females than males. The age is also mostly younger than compared to the elderly. This can provide some explanation as to why the credit limit is right skewed, since the younger generation tends to have lower credits limits. Across the past 6 months, there are also more delayed payments as compared to no delay.\n\nWe then carried our exploratory data analysis to identify features that were highly correlated with our target class, and therefore able to select the useful features. From the heat map, it seems that pay status 1-6 can be can potentially be useful predictors of default status. Borrowers that defaulted in October, tends to also delay in the past 6 months. From data analysis, elderly defaulted more than the young. Male defaulted more than the females. Higher education defaulted lesser than the lower education borrowers. First month\u2019s delay often leads to delay in the subsequent months. Features that we found to have high predictive value for Default are the PAY_X (ie the repayment status in previous months), the LIMIT_BAL & the PAY_AMTX (amount paid in previous months).\n\nWe also observed that our dataset was highly imbalanced; only ~22% of the target variable belonged to the \u201cdefault\u201d class. We therefore explored several methods such as upsampling of the minority class, downsampling of the majority class and synthetic construction of data by the SMOTE algorithm. \n\nWe then proceeded to build a classification model. We chose to try both logistic regression and SVM which are known for their effectiveness in classification problems. The logistic regression and SVM achieved accuracy of 73% and 75% on the test set respectively. More time and further extensive model experiments are required to fine tune the models to gain higher accuracy.\nFor the purpose of this project, we will start with data cleaning and wragling, follow by exploritary data analysis to understand the potential relationship between variables. In the end, we will perform logistic regression and kernal SVM [2] to explore the non-linearity of our model for the prediction of our target variable","514f49c6":">Let's combine the unique values to following categories:\n>\n>1. No delay\n>2. Delay\n","3f1203d7":"# 2. DATA WRANGLING","2c7c325a":">`Interpretation:`\n    >\n>- Customer with higher education background have lower default rate\n\n> Default Rate Ranking by `Education` Background:\n> \n> `High School` > `University` > `Graduate School` > `Others`","59d86480":">`Interpretation`\n>\n> We can see here that there is an imbalance of y target class. There is a lower proportion of defaults, ~20%, that makes the base naive model predicting all to be non-default right 78% of the time. Hence, we need to explore some data sampling methods so as to create a more robust model which is reliable with unseen data","f6c5f9ec":"We started with some inital data cleaning and wrangling to better visualize our data, and prepare it for model training\n\nGaining insights from our initial data visualization we found that many of our price data are heavily right skewed and therefore applied a log function to balance out the distribution. From our exploratory data analysis, we were able to identify features that were highly correlated with our target class, and therefore able to select the useful features\n\nA second issue was our highly imbalance target class which had only ~22% of the total counts. We therefore explored severa methods such as upsampling of the lesser class, downsampling of the higher class and also synthetic construction of data by the SMOTE algorithm\n\nFeatures that we found to have high predictive value for Default are the PAY_X (ie the repayment status in previous months), the LIMIT_BAL & the PAY_AMTX (amount paid in previous months).\n\nWe chose to try both logistic regression and linear kernal SVM which are known for their effective in classification problems, and explored several feature selection process and compared the results. We concluded that more time and further extensive model experiments are required to fine tune the models to gain higher accuracy","7704323f":"#### Generate Frequency Distribution Histogram of Target Value","e29a3944":"**d. Historical payment status**","d2b59aa0":">`Interpretation`\n>\n>- `Low` Payment Amount in `Aug` has `higher` Default Rate in Target Month\n>- `High` Payment Amount in `Aug` has `lower` Default Rate in Target Month\n>- This is consistant with our previous observation","c122fe9e":"## Multivariate Analysis","327700ac":"# 6. References","4bd0b8d9":"### Implementing dummy variables for modeling with categorical variables","24de8075":"### Pros and Cons of Up\/down Sampling \n\nUpsampling of minority class (DEFAULT_STATUS=1) has the risk of overfitting the model since it increases the counts\/occurance of minority class. Thus it is expected to perform better than downsampling[3] \n\nDownsampling can discard potentially useful information and the sample can be biased, but it helps improving the run time\n\nA third method is to create a syntetic sample by using the [SMOTE](https:\/\/machinelearningmastery.com\/smote-oversampling-for-imbalanced-classification\/) (Synthetic Minority Oversampling TEchnique)[4] algorithm, which is an oversampling method which creates syntetic samples from the minority class instead of creating copies. It selects 2 or more similar instances and perturb them one at a time by random amount. \n\nSpecifically, a random example from the minority class is first chosen. Then k of the nearest neighbors for that example are found (typically k=5). A randomly selected neighbor is chosen and a synthetic example is created at a randomly selected point between the two examples in feature space.","cd847d4b":">`Interpretation:`\n>    \n>- Customer with `Single` marital status has lower default rate\n>- Customer with `Other` marital status has higher default rate\n\n> Default Rate Ranking by `Marital Status`:\n> \n> `Others` > `Marries` > `Single`","bcfd6904":"## Univariate Analysis","389a82b7":"[1] Yeh, I. C., & Lien, C. H. (2009). The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. Expert Systems with Applications, 36(2), 2473-2480.\n\n[2] Zoltan, C. (2018, November 13) SVM and Kernel SVM https:\/\/towardsdatascience.com\/svm-and-kernel-svm-fed02bef1200\n\n[3] Weiss, Gary M., McCarthy K., and Bibi Zabar (2007). \"Cost-sensitive learning vs. sampling: Which is best for handling unbalanced classes with unequal error costs?.\" DMIN 7 : 35-41.\n\n[4] Brownlee, J. (2020, January 17). SMOTE for Imbalanced Classification with Python https:\/\/machinelearningmastery.com\/smote-oversampling-for-imbalanced-classification\/","6d0668fe":"### Training logistic regression model with all independent variables","83bd9340":">`Interpretation`\n>\n>Customer with history of payment delay in Sep(`PAY_1`) has higher default rate in Oct(`Target Value`)\n>and this trend pattern is consistant in all age groups"}}