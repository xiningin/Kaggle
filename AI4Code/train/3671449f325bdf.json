{"cell_type":{"f13da8b0":"code","180ac171":"code","0286d751":"code","50c89707":"code","d1e7abeb":"code","08ddbcb9":"code","7f024648":"code","94382b92":"code","72881d84":"code","f7254b14":"code","74ca1dea":"code","05607142":"code","bde07d12":"code","e4596118":"code","95444e80":"code","2559c522":"code","24e084b4":"code","36d301c8":"code","b04c7949":"code","d037a270":"code","ecba5814":"code","4581ee6c":"code","50d9cc46":"code","f25fa3fa":"code","2b096467":"markdown","e9800731":"markdown","afd7e615":"markdown","8c6cfbf8":"markdown","38088fde":"markdown","3b164401":"markdown","da57caf1":"markdown","63e74c98":"markdown","7bfee8aa":"markdown","e74485c6":"markdown","27947d10":"markdown","c0a5c21e":"markdown","db3e7ffd":"markdown","735a7d61":"markdown"},"source":{"f13da8b0":"#This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# importing pckgs\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nimport re\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n","180ac171":"# loading tran & test data\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\") #training set\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\") #test set\n\n# Store our passenger ID for easy access\nPassengerId = test['PassengerId']","0286d751":"# Exploring train set\ntrain.info()\ntrain.describe(include='all')","50c89707":"# Exploring test set\ntest.info()\ntest.describe(include='all')","d1e7abeb":"train.isna().sum()","08ddbcb9":"# Taken from user Pegasus - https:\/\/www.kaggle.com\/ritesh1993\/titanic-basic\n# Gives the length of the name\ntrain['Name_length'] = train['Name'].apply(len)\ntest['Name_length'] = test['Name'].apply(len)\n# Feature that tells whether a passenger had a cabin on the Titanic\ntrain['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\n#train.head(5)\n#test.head(5)\n\nfull_data = [train, test] # combine dataset (for cleaning\/feature engineering on both datasets)","7f024648":"# Feature engineering steps taken from Sina - https:\/\/www.kaggle.com\/sinakhorami\/titanic-best-working-classifier\n# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1","94382b92":"# Create new feature IsAlone from FamilySize\nfor dataset in full_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n    ","72881d84":"# Remove all NULLS in the Embarked column\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n    ","f7254b14":"# Remove all NULLS in the Fare column and create a new feature CategoricalFare\nfor dataset in full_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)","74ca1dea":"# Create a New feature CategoricalAge\nfor dataset in full_data:\n    age_avg = dataset['Age'].mean()\n    age_std = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)\ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)","05607142":"# Define function to extract titles from passenger names\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"","bde07d12":"# Create a new feature Title, containing the titles of passenger names\nfor dataset in full_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)","e4596118":"# Group all non-common titles into one single grouping \"Rare\"\nfor dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n","95444e80":"# data cleaning\nfor dataset in full_data:\n    # Mapping Sex\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n    \n    # Mapping titles\n    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \n    # Mapping Embarked\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    \n    # Mapping Fare\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \n    # Mapping Age\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4 ;","2559c522":"# Feature selection\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp'] # elements to drop\n\ntrain = train.drop(drop_elements, axis = 1)\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\ntest  = test.drop(drop_elements, axis = 1)\n\n# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\ny = train['Survived'].ravel()\ntrain = train.drop(['Survived'], axis=1)\nX = train.values # Creates an array of the train data\ntest = test.values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3,random_state=15)\n\n\n","24e084b4":"from sklearn.metrics import accuracy_score\ny_train.shape, X_test.shape, X_train.shape, y_test.shape","36d301c8":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrandom_forest = RandomForestClassifier(n_estimators=200)\nrandom_forest.fit(X_train,y_train)\nrf_predictions = random_forest.predict(test)\n\n# Evaluate acc\ny_pred = random_forest.predict(X_test)\nacc_rf = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_rf)","b04c7949":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train,y_train)\nlg_pred = model.predict(test)\n\n# Evaluate acc\ny_pred = model.predict(X_test)\nacc_lr = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_lr)","d037a270":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train,y_train)\n\ndt_pred = decision_tree.predict(test)\n\n# Evaluate acc\ny_pred = decision_tree.predict(X_test)\nacc_dt = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_dt)","ecba5814":"# import xgboost as xgb\n\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nclf = XGBClassifier()\n\nxgbc_model = XGBClassifier(silent=False, \n                      scale_pos_weight=1,\n                      learning_rate=0.01,  \n                      colsample_bytree = 0.4,\n                      subsample = 0.8,\n                      objective='binary:logistic', \n                      n_estimators=1000, \n                      reg_alpha = 0.3,\n                      max_depth=4, \n                      gamma=10)\nxgbc_model.fit(X_train,y_train,verbose=True)\n\nxgbc_pred = xgbc_model.predict(test)\n\n# Evaluate acc\ny_pred = xgbc_model.predict(X_test)\nacc_xgbc = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_xgbc)","4581ee6c":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\ngaussian_pred = gaussian.predict(test)\n\n# Evaluate acc\ny_pred = gaussian.predict(X_test)\nacc_gaussian = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_gaussian)","50d9cc46":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\ngbk.fit(X_train, y_train)\ngbk_pred = gbk.predict(test)\n\n# Evaluate acc\ny_pred = gbk.predict(X_test)\nacc_gbk = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_gbk)","f25fa3fa":"# Generate Submission File\ndef submit(x,y):\n    submission = pd.DataFrame({ 'PassengerId': PassengerId,\n                            'Survived': x })\n    \n    submission.to_csv(f\"Submission_{y}.csv\", index=False)\n\n    \nsubmit(rf_predictions, 'randomforest') # submit randomforest\nsubmit(lg_pred, 'logReg') # submit logistic regression\nsubmit(dt_pred, 'decisionTree') # submit decision tree\nsubmit(xgbc_pred, 'xgbc') # submit xgbc\nsubmit(gaussian_pred, 'gaussianNB') # submit gaussian naive bayes\nsubmit(gbk_pred, 'gradientBoost') # submit gradient boost classifier","2b096467":"## Gradient Boosting Classifer","e9800731":"# Loading dataset\nThis is my first kernel. \n\n**Please feel free to comment your inputs\/suggestions if you have any. I strive to learn and your feedback is much appreciated :) **","afd7e615":"# Further improvement\n\n* I will update this to include ensembles. Maybe stacking with weights. \n* Try other models \n* Try SMOTE?\n* Finetune feature engineering & cleaning section through visualization techniques\n\nSuggestions is much appreciated. ","8c6cfbf8":"## RandomForestClassifier","38088fde":"# Feature engineering & Data Cleaning\n\nThis section was taken from:\n* Sina - https:\/\/www.kaggle.com\/sinakhorami\/titanic-best-working-classifier\n> (Please look at their notebooks to get great feature engineering\/data cleaning insights)\n\nIn this section we will try to extract new information from the given dataset.","3b164401":"# (Quick) Data exploration","da57caf1":"# Modeling\n* RandomForestClassifier\n* LogisticRegression\n* DecisionTree\n* XGBClassifier\n* GaussianNB\n* GradientBoostingClassifier","63e74c98":"## Logistic Regression","7bfee8aa":"## XGBClassifier","e74485c6":"## Decision Tree","27947d10":"# Submitting\n\nOur predictions are now finally ready to be submitted\n","c0a5c21e":"Score: \n* RandomForest       - 0.76076\n* LogisticRegression - 0.78648 (Best score)\n* DecisionTree       - 0.72727\n* XGBClassifier      - 0.77990\n* GaussianNB.        - 0.71291","db3e7ffd":"## Gaussian Naive Bayes","735a7d61":"NOTE: Age and Cabin seems too have a lot of missing values"}}