{"cell_type":{"badca541":"code","1c7dd86f":"code","878848ea":"code","c0d0257e":"code","be91b8d6":"code","e67c8ff4":"code","ffef771f":"code","fc5e869c":"code","f82931d4":"code","1492ae91":"code","33c8bd26":"code","1bf0eaa5":"code","ce2db75a":"code","e7900d4a":"markdown","bc6b927e":"markdown","5fbfc55c":"markdown","4784b35f":"markdown","a9695b51":"markdown","72ec2b03":"markdown","d8921431":"markdown","ad1db263":"markdown","b9706bd5":"markdown","1287e069":"markdown","8b5a8ab6":"markdown","2d9f893a":"markdown"},"source":{"badca541":"import pandas  as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\n\n# read in the files that I am going to use:\ngender   = pd.read_csv('..\/input\/titanic\/gender_submission.csv')  \nperfect  = pd.read_csv('..\/input\/submission-solution\/submission_solution.csv')\nallZeros = pd.read_csv('..\/input\/titanic-all-zeros-csv-file\/all_0s.csv')\n","1c7dd86f":"accuracy_score( perfect['Survived'] , gender['Survived'] )","878848ea":"titanic_cm = confusion_matrix( perfect['Survived'] , gender['Survived'] )\nprint(titanic_cm)","c0d0257e":"# convert the ndarray to a pandas dataframe\ncm_df = pd.DataFrame(titanic_cm)\n# set the size of the figure\nplt.figure(figsize = (5,5))\nsn.heatmap(cm_df, \n           annot=True, annot_kws={\"size\": 25},\n           fmt=\"d\",         # decimals format\n           xticklabels=False, \n           yticklabels=False,\n           cmap=\"viridis\", \n           cbar=False)\nplt.show()","be91b8d6":"from statsmodels.graphics.mosaicplot import mosaic\ndata = {('False', 'Negative'): cm_df.iloc[1,0],  ('True', 'Negative'): cm_df.iloc[0,0], ('True', 'Positive'): cm_df.iloc[1,1], ('False', 'Positive'): cm_df.iloc[0,1]}\nmosaic(data, gap = 0.02)\nplt.show();","e67c8ff4":"cm_df_2 = cm_df.copy()\n# swap elements\ncm_df_2.iloc[0,1] = cm_df.iloc[1,1]\ncm_df_2.iloc[1,1] = cm_df.iloc[0,1]\n# add names\ncm_df_2.columns =[\"Negative\", \"Positive\"]\ncm_df_2.index   =[\"True\",     \"False\"]\n# plot\ncm_df_2.plot.bar(stacked=True, color=[\"olive\", \"mediumslateblue\"], figsize=(5,5));","ffef771f":"tn, fp, fn, tp = confusion_matrix(perfect['Survived'] , gender['Survived']).ravel()\nprint(\"Number of true negatives  (tn) = \",tn)\nprint(\"Number of true positives  (tp) = \",tp)\nprint(\"Number of false negatives (fn) = \",fn)\nprint(\"Number of false positives (fp) = \",fp)\nprint(\"Precision                                          = tp \/ (tp + fp) =\", tp \/ (tp + fp))\nprint(\"Recall or 'sensitivity' (aka. true positive rate)  = tp \/ (tp + fn) =\", tp \/ (tp + fn))\nprint(\"Specificity             (aka. true negative rate)  = tn \/ (tn + fp) =\", tn \/ (tn + fp))\nprint(\"Fall out                (aka. false positive rate) = fp \/ (fp + tn) =\", fp \/ (fp + tn))\nprint(\"Miss rate               (aka. false negative rate) = fn \/ (fn + tp) =\", fn \/ (fn + tp))","fc5e869c":"from sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve( perfect['Survived'] , gender['Survived'] )\n\nfrom sklearn.metrics import roc_auc_score\nroc_auc = roc_auc_score( perfect['Survived'] , gender['Survived'] )\nprint(\"AUC = \", roc_auc)\n\nplt.figure(figsize = (8,5))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = %0.5f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic curve')\nplt.legend(loc=\"lower right\")\nplt.show()","f82931d4":"f1_score( perfect['Survived'] , gender['Survived'] )","1492ae91":"\naccuracy_score( perfect['Survived'] , allZeros['Survived'] )\n","33c8bd26":"titanic_cm = confusion_matrix( perfect['Survived'] , allZeros['Survived'] )\nprint(titanic_cm)","1bf0eaa5":"# convert the ndarray to a pandas dataframe\ncm_df = pd.DataFrame(titanic_cm)\n# set the size of the figure\nplt.figure(figsize = (5,5))\nsn.heatmap(cm_df, \n           annot=True, annot_kws={\"size\": 25},\n           fmt=\"d\",         # decimals format\n           xticklabels=False, \n           yticklabels=False,\n           cmap=\"viridis\", \n           cbar=False)\nplt.show()","ce2db75a":"tn, fp, fn, tp = confusion_matrix( perfect['Survived'] , allZeros['Survived']).ravel()\nprint(\"Number of true negatives  (tn) = \",tn)\nprint(\"Number of true positives  (tp) = \",tp)\nprint(\"Number of false negatives (fn) = \",fn)\nprint(\"Number of false positives (fp) = \",fp)\nprint(\"Precision                                          = tp \/ (tp + fp) =\", tp \/ (tp + fp))\nprint(\"Recall or 'sensitivity' (aka. true positive rate)  = tp \/ (tp + fn) =\", tp \/ (tp + fn))\nprint(\"Specificity             (aka. true negative rate)  = tn \/ (tn + fp) =\", tn \/ (tn + fp))\nprint(\"Fall out                (aka. false positive rate) = fp \/ (fp + tn) =\", fp \/ (fp + tn))\nprint(\"Miss rate               (aka. false negative rate) = fn \/ (fn + tp) =\", fn \/ (fn + tp))","e7900d4a":"when we are choosing the classifier that best models for our problem, it is usual to choose the one that gives the highest $F_1$ score.\n\n## No survivors\n\nAs an exercise we shall repeat the above procedure, this time for the file containing all zeros, indicating no survivors:","bc6b927e":"We can calculate even more things with these values:","5fbfc55c":"This value (here with more decimal places) is what we see on the leaderboard.\nNow we shall calculate the confusion matrix (Note the order, which is important: the ground truth file, followed by our estimate file)","4784b35f":"# Titanic: In all the confusion...\n\nIn this notebook we shall take a look at an aspect of classification known as the **confusion matrix**, here applied to the [Titanic data](https:\/\/www.kaggle.com\/c\/titanic). We shall then take a look at the ROC curve, as well as the $F_1$ score.\n\nA confusion matrix is a tool designed to help us understand a little better how well our classifier is performing. An *accuracy score*, like that returned by kaggle for our submission file, lets us know a number indicating what ratio of predictions were correct (0 is not one classification was correct, and 1 is perfect!). The confusion matrix does the same thing, but goes into a little more detail; this time it provides us with four values:\n* The number of times our classifier produced **true negatives** (TN)\n* The number of times our classifier produced **true positives** (TP)\n* The number of times our classifier produced **false positives** (FP), a type I error\n* The number of times our classifier produced **false negatives** (FN), a type II error\n\nwhich scikit-learn returns in the following format, hence the name matrix (note that there is no standard convention for arrangement of this matrix):\n\n![image.png](attachment:image.png)\n\n\nThe *accuracy* is given by $\\frac{(TN + TP)}{(TN + TP + FP +FN)}$, in other words, the true values divided by all the values. And finally, another measure one may come across is the **$F_1$ score**, which is given by:\n\n$$ F_1 = 2\\frac{precision . recall}{precision + recall}$$\n\n\nwhere the *precision* is given by $\\frac{TP}{TP + FP}$, and *recall* by $\\frac{TP}{TP + FN}$.\n\nThese Wikipedia pages have excellent descriptions of the meaning of these terms: \n* [Confusion matrix](https:\/\/en.wikipedia.org\/wiki\/Confusion_matrix)\n* [False positives and false negatives](https:\/\/en.wikipedia.org\/wiki\/False_positives_and_false_negatives)\n* [Type I and type II errors](https:\/\/en.wikipedia.org\/wiki\/Type_I_and_type_II_errors)\n* [Receiver operating characteristic](https:\/\/en.wikipedia.org\/wiki\/Receiver_operating_characteristic)\n* [F1 score](https:\/\/en.wikipedia.org\/wiki\/F1_score)\n\n\n\nI am now going to produce two confusion matrices for two different Titanic submission files. Rather than run some code here to create a `submission.csv` I am going to make use of a file that we all have access to, and also a file we can easily create:\n1. `gender_submission.csv`   this is the file that comes free with the competition\n2. `all_0s.csv`              is a submission file in which all the passengers have, sadly, been assigned a 0 (i.e. no survivors)\n\nWe shall compare these files containing our results to what is known as the *ground truth*, contained in `submission_solution.csv`. This is the infamous file that contains the answers, and will score a 1.00000 on the LB.\n\nWe are going to be using the following routines from the [scikit-learn library](https:\/\/scikit-learn.org\/stable\/):\n\n* [sklearn.metrics.accuracy_score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.accuracy_score.html): this routine is analogous to the one used by kaggle to calculate your score on the leaderboard when you submit your file.\n* [sklearn.metrics.confusion_matrix](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.confusion_matrix.html): this routine will calculate our confusion matrix.\n* [sklearn.metrics.f1_score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.f1_score.html) which returns the $F_1$ score.\n* [sklearn.metrics.roc_curve](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_curve.html) which computes the Receiver Operating Characteristic (ROC) curve.\n* [sklearn.metrics.roc_auc_score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_auc_score.html) which computes the Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n\nFirst we shall import the libraries that we will need, and then we shall read in the three csv files as pandas dataframes:","a9695b51":"Finally we shall calculate the $F_1$ score","72ec2b03":"This time we can see that now we have no false positives (whereas before we had 46) so we have correctly identified all of the 260 people who died (true negatives). But this is no great surprise given that we 'killed' everyone in the file! And all the rest fall into the quadrant of false negatives. We can see that the $TP + FP = 0$, so the *precision* is undefined (python returns a ***nan***), in this situation the $F_1$ would return a $0$.\n\n### Related reading\n* [False positives, false negatives and the discrimination threshold](https:\/\/www.kaggle.com\/carlmcbrideellis\/discrimination-threshold-false-positive-negative)","d8921431":"or make a [*mosaic plot*](https:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.graphics.mosaicplot.mosaic.html)  with proportional areas (note the different arrangement)","ad1db263":"In the Titanic competition the submission file contains 418 values that we have to classify. From our above score we can see that `0.7655502392344498 x 418 = 320` In other words, the `gender_submission.csv` file correctly identifies 320 of the values. What the confusion matrix tells us now, in more detail, is that we got 214 of the people who did not survive right, and 106 of the people who did survive right. But also that we killed 52 of the passengers in our model, and brought 46 of them back to life!\n## ROC curve\nIn the above calculations we have also obtained the *true positive rate* (TPR, also known as the *recall*) and the *false positive rate* (FPR, or the *fall out*). If we plot the true positive rate with respect to the false positive rate we obtain what is known as the [receiver operating characteristic curve](https:\/\/en.wikipedia.org\/wiki\/Receiver_operating_characteristic) (or **ROC curve**). It is often deemed interesting to also calculate the *area under the curve* (**AUC**). An AUC score of 1 means a perfect classification, whereas a score of 0.5 means that your classifier is as about as useful as randomly flipping a coin. In this example, with only one point on the ROC curve, it is easy to calculate the AUC score using the [trapezoidal rule](https:\/\/en.wikipedia.org\/wiki\/Trapezoidal_rule):\n\n$$AUC = FPR\\frac{TPR}{2} + (1-FPR)\\frac{TPR +1}{2}$$","b9706bd5":"or perhaps more intuitively, as a [bar plot]( https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.plot.bar.html)","1287e069":"(It is important to note that the convention used by *scikit-learn* is the count of true negatives is $C_{0,0}$, false negatives is $C_{1,0}$, true positives is $C_{1,1}$ and false positives is $C_{0,1}$).\n\nWe can make this look nicer, with some colours, using a [seaborn.heatmap](https:\/\/seaborn.pydata.org\/generated\/seaborn.heatmap.html):","8b5a8ab6":"This is our accuracy score. Now for the confusion matrix for the case of 'all zeros':","2d9f893a":"## gender_submission.csv\n\nFirst let us calculate the accuracy score for the `gender_submission.csv` file:"}}