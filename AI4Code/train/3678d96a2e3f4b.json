{"cell_type":{"61938df0":"code","995d903b":"code","43cc68b1":"code","4f582708":"code","562090ba":"code","5077667d":"code","e534726d":"code","f9707992":"code","541336e4":"markdown","42a07510":"markdown","a0bf60f3":"markdown"},"source":{"61938df0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","995d903b":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport datetime as dt\nfrom datetime import timedelta\nfrom sklearn.preprocessing import PolynomialFeatures \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom statsmodels.tsa.api import Holt,SimpleExpSmoothing,ExponentialSmoothing\nfrom sklearn.metrics import mean_squared_error,r2_score\nimport statsmodels.api as sm\nfrom fbprophet import Prophet\ncovid=pd.read_csv(\"..\/input\/novel-corona-virus-2019-dataset\/covid_19_data.csv\")\n#covid.head()","43cc68b1":"covid_india=covid[covid['Country\/Region']==\"India\"]\ncovid_usa=covid[covid['Country\/Region']==\"US\"]\n\n#Converting the date into Datetime format\ncovid_india[\"ObservationDate\"]=pd.to_datetime(covid_india[\"ObservationDate\"])\ncovid_usa[\"ObservationDate\"]=pd.to_datetime(covid_usa[\"ObservationDate\"])\n#Taking the data starting 1st March 2020 as that is most relevant for India\nstart_date = pd.to_datetime(\"1-Mar-2020\")\ncovid_india_MarchOnwards = covid_india[covid_india[\"ObservationDate\"] > start_date]\ncovid_usa_MarchOnwards = covid_usa[covid_usa[\"ObservationDate\"] > start_date]\n\n#print(covid_india_MarchOnwards)\ncovid_india_MarchOnwards.head()\ncovid_usa_MarchOnwards.head()\n#Grouping the data based on the Date \naggByDate_India=covid_india_MarchOnwards.groupby([\"ObservationDate\"]).agg({\"Confirmed\":'sum',\"Recovered\":'sum',\"Deaths\":'sum'})\naggByDate_USA=covid_usa_MarchOnwards.groupby([\"ObservationDate\"]).agg({\"Confirmed\":'sum',\"Recovered\":'sum',\"Deaths\":'sum'})\nprint(\"Number of Confirmed Cases India\",aggByDate_India[\"Confirmed\"].iloc[-1])\nprint(\"Number of Recovered Cases\",aggByDate_India[\"Recovered\"].iloc[-1])\nprint(\"Number of Death Cases\",aggByDate_India[\"Deaths\"].iloc[-1])\nprint(\"Number of Active Cases\",aggByDate_India[\"Confirmed\"].iloc[-1]-aggByDate_India[\"Recovered\"].iloc[-1]-aggByDate_India[\"Deaths\"].iloc[-1])\nprint(\"Number of Closed Cases\",aggByDate_India[\"Recovered\"].iloc[-1]+aggByDate_India[\"Deaths\"].iloc[-1])\nprint(\"Approximate Number of Confirmed Cases per day\",round(aggByDate_India[\"Confirmed\"].iloc[-1]\/aggByDate_India.shape[0]))\nprint(\"Approximate Number of Recovered Cases per day\",round(aggByDate_India[\"Recovered\"].iloc[-1]\/aggByDate_India.shape[0]))\nprint(\"Approximate Number of Death Cases per day\",round(aggByDate_India[\"Deaths\"].iloc[-1]\/aggByDate_India.shape[0]))\nprint(\"Number of New Cofirmed Cases in last 24 hours are\",aggByDate_India[\"Confirmed\"].iloc[-1]-aggByDate_India[\"Confirmed\"].iloc[-2])\nprint(\"Number of New Recoverd Cases in last 24 hours are\",aggByDate_India[\"Recovered\"].iloc[-1]-aggByDate_India[\"Recovered\"].iloc[-2])\nprint(\"Number of New Death Cases in last 24 hours are\",aggByDate_India[\"Deaths\"].iloc[-1]-aggByDate_India[\"Deaths\"].iloc[-2])\n","4f582708":"plt.figure(figsize=(10,5))\n#plt.plot(aggByDate_India[\"Confirmed\"],label=\"Confirmed Cases India\")\n#plt.plot(aggByDate_India[\"Recovered\"],label=\"Recovered Cases India\")\nplt.plot(aggByDate_India[\"Deaths\"],label=\"Death Cases India\")\n#plt.plot(aggByDate_USA[\"Confirmed\"],label=\"Confirmed Cases US\")\n#plt.plot(aggByDate_USA[\"Recovered\"],label=\"Recovered Cases US\")\nplt.plot(aggByDate_USA[\"Deaths\"],label=\"Death Cases US\")\n\nplt.xticks(rotation=90)\nplt.ylabel(\"Number of Cases: Log Scale\")\nplt.xlabel(\"Date\")\nplt.yscale(\"log\")\nplt.title(\"COVID Fatalities Trends in India and USA\")\nplt.legend()","562090ba":"# Let's explore CORD-19 data. To begin with let's find all those antivirals that have been mentioned in \n# context of India along with reference count\n# To start with, let us focus only on PDFs where text has been successfully extracted\n# Per https:\/\/arxiv.org\/pdf\/2004.10706.pdf, Each PDF parse has an associated SHA \n# while each XML parse is named using its associated PMC ID.\ncord19mdata=pd.read_csv(\"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\")\n#Since we are starting with PDFs only - which has about 80% of articles, remove entries that dont have SHA\ncord19mdata=cord19mdata[cord19mdata[\"sha\"]==cord19mdata[\"sha\"]] #filters out entries having sha=NaN\n#Next find those PDFs where text extraction was successful\n#cord19mdata=cord19mdata[cord19mdata[\"has_pdf_parse\"]] #filters out entries for which pdf parse is not available\nshaTojsonPath = {}\n#since sha entry is a pointer to json file, lets have a method that can give full path to json file\n\nallFileNamestoPath ={}\nfor dirname, _, files in os.walk('\/kaggle\/input\/CORD-19-research-challenge'):\n    for filename in files:\n        allFileNamestoPath[filename] = os.path.join(dirname,filename)\n\n#def shatoJSONFilePath(shaid): #returns path of .json file\n    #for dirname, _, files in os.walk('\/kaggle\/input\/CORD-19-research-challenge'):\n        #if shaid+'.json' in files:\n            #return os.path.join(dirname,shaid+\".json\")\ndef shatoJSONFilePath(shaid):\n    return allFileNamestoPath.get(shaid+\".json\")\n\nfor shaid in cord19mdata[\"sha\"]:\n    shaTojsonPath[shaid] = shatoJSONFilePath(shaid)\n#Since we would need json file for most of our analysis, let's add json file path to metadata itself\n#cord19mdata[\"jsonPath\"]=cord19mdata.apply(lambda x: shatoJSONFilePath(x[\"sha\"]),axis=1) \n#remove rows where we could not find jsonPath\n#cord19mdata=cord19mdata[cord19mdata[\"jsonPath\"]==cord19mdata[\"jsonPath\"]]\n#cord19mdata.shape","5077667d":"import json\nimport re\n#Now open each json file as per jsonPath (technically, that means do json.load on file at jsonPath)\n#There might be multiple body_text nodes in json file, aggregate all body_text commentary as the text in PDF \n#(technically, start with empty '' pdfText and keep doing pdfText = pdfText + body_text_entry)\n#Create a mapping of shaId with fullBodyText (technically Python dictionary shaToPDFText)\n\nshaToPDFText = {}\n#for shaid,jsonPath in zip(cord19mdata[\"sha\"],cord19mdata[\"jsonPath\"]):\nfor shaid in cord19mdata[\"sha\"]:\n    jsonPath = shaTojsonPath.get(shaid)\n    pdfText = ''\n    if (jsonPath is not None):\n        with open(jsonPath, 'r') as jsonfile:\n            jsonfileObj = json.load(jsonfile)\n            for body_text_entry in jsonfileObj[\"body_text\"]:\n                pdfText = pdfText + (re.sub('[^a-zA-Z0-9]', ' ', body_text_entry[\"text\"].lower()))\n    #print(pdfText)\n    shaToPDFText[shaid] = pdfText\n","e534726d":"#Now let's find out what drugs are being talked about\n#We want to match all stems and affixes from https:\/\/en.wikipedia.org\/wiki\/Drug_nomenclature\n\ndrugStemsAffixes = ['vir ', 'cillin ', ' cef', 'mab ', 'ximab ', 'zumab ', 'ciclib ', 'lisib ', 'tinib ', 'vastatin ', 'prazole ', 'lukast ', 'grel','axine ','olol ','oxetine ','sartan ','pril ','oxacin ','uine ','barb','xaban ','afil ', 'prost','quine ','parib ', 'tide ', 'vec ', 'imsu ','-caine ','sone ']\ndrugsInPDFs = []\ncytoDrugsInPDFs = []\nMIN_DRUG_LEN = 6\ndef singleStranded(pdfText):\n    return ((pdfText.rfind('single-stranded')>0) or (pdfText.rfind('single stranded')>0))\ndef positiveSense(pdfText):\n    return ((pdfText.rfind('positive-sense')>0) or (pdfText.rfind('positive sense')>0))\ndef singleStandedPositiveSense(pdfText):\n    return (singleStranded(pdfText) and positiveSense(pdfText))\ndef sarsmerscoronacovid(pdfText):\n    #return ((pdfText.rfind('sars')>0) or (pdfText.rfind('mers')>0) or (pdfText.rfind('corona')>0) or (pdfText.rfind('covid')>0))\n    #may not be worth filtering for this criteria\n    return True\ndef sarsmerscoronasingleStandedPositiveSense(pdfText):\n    return (singleStranded(pdfText) and positiveSense(pdfText) and sarsmerscoronacovid(pdfText))\ndef rnaNucleoCapsid(txt):\n    return ((txt.rfind('rna ')>0) and (txt.rfind('nucleoprotein')>0) and (txt.rfind('capsid')>0))\ndef matrixProtein(pdfText):\n    return ((pdfText.rfind('matrix protein')>0) or (pdfText.rfind('matrix-protein')>0))\ndef spikeglycoProtein(pdfText):\n    return ((pdfText.rfind('spike protein')>0) or (pdfText.rfind('glycoprotein')>0))\ndef humanActivator(pdfText):\n    return ((pdfText.rfind('tmprss2')>0) or (pdfText.rfind('chromosome 21')>0) or (pdfText.rfind('21q22.3')>0))\ndef sarsSinglePositivestrandRNAnucleocapsidMatrixSpike(txt):\n    return(sarsmerscoronasingleStandedPositiveSense(txt) and rnaNucleoCapsid(txt) and matrixProtein(txt) and spikeglycoProtein(txt))\ndef isDrugName(txt):\n    toremovewords = [\"vaccine\",\"urine\", \"quarantine\", \"april\",\"bovine\",\"intestine\", \"examine\", \"decline\", \"determine\",\"routine\",\"canine\", \"hyaline\", \"polypeptide\", \"nucleotide\", \"epinephrine\", \"cytokine\", \"chemokine\",\"line\",\"swine\",\"baseline\",\"medicine\",\"equine\",\"serine\",\"tyrosine\",\"nine\",\"murine\",\"porcine\",\"alanine\",\"peptide\",\"saline\",\"define\",\"feline\",\"online\",\"prost\",\"creatinine\",\"guideline\",\"cysteine\",\"norepinephrine\",\"glutamine\", \"paracrine\",\"adenosine\",\"ovine\",\"alkaline\",\"telemedicine\",\"endocrine\",\"glycine\",\"uterine\",\"autocrine\",\"pipeline\",\"combine\",\"machine\",\"caprine\",\"medline\",\"midline\",\"ukraine\",\"caffeine\",\"dourine\", \"fine\",\"lysine\",\"arginine\",\"histamine\"]\n    #some of these words were when I was searching with drug suffix ine instead of quine\n    if (txt in toremovewords):\n        return False\n    return True\ndef antibody47D11(pdfText):\n    return (pdfText.rfind('antibody 47d11')>0 )\n\n\nfor stemAfix in drugStemsAffixes:\n    maxPDFsForTesting = 150000\n    pdfsProcessed = 0\n    for shaid in shaToPDFText:\n        txt = shaToPDFText[shaid]\n        pdfsProcessed = pdfsProcessed + 1\n        #print(pdfText)\n        if (pdfsProcessed < maxPDFsForTesting):\n            iterator=re.finditer(stemAfix,txt)\n            for m in iterator:\n                drugFound = shaToPDFText[shaid][shaToPDFText[shaid].rfind(' ',0, m.end()-2):m.end()]\n                if ((len(drugFound) >= MIN_DRUG_LEN) and isDrugName(drugFound.rstrip().lstrip())):\n                    if ( (antibody47D11(txt)) or ( sarsSinglePositivestrandRNAnucleocapsidMatrixSpike (txt) and humanActivator(txt) ) ):\n                        drugsInPDFs.append(drugFound)\n                if ((len(drugFound) >= MIN_DRUG_LEN) and isDrugName(drugFound.rstrip().lstrip())):\n                    if ( (txt.rfind('cytokine storm')>0) or (txt.rfind('cytokine-storm')>0) ):\n                        cytoDrugsInPDFs.append(drugFound)\n                \ndrugs_set = list(set(drugsInPDFs))\ncyto_drugs_set = list(set(cytoDrugsInPDFs))\n\ncount=[]\nfor d in drugs_set:\n    count.append(-drugsInPDFs.count(d))\ndrugs_set=list(np.array(drugs_set)[np.array(count).argsort()]) \nprint(len(drugs_set))\n\ncount=[]\nfor d in cyto_drugs_set:\n    count.append(-cytoDrugsInPDFs.count(d))\ncyto_drugs_set=list(np.array(cyto_drugs_set)[np.array(count).argsort()]) \nprint(len(cyto_drugs_set))\n\n\n\n\n","f9707992":"import plotly.express as px\n#Now lets count what drug has been mentioned the most\ndrugsDF = pd.DataFrame(drugs_set,columns=[\"Drug\"])\ncytodrugsDF = pd.DataFrame(cyto_drugs_set,columns=[\"Drug\"])\n\ndef count1(drug,druglist):\n    return druglist.count(drug)\ndrugsDF['CountInText'] = drugsDF.apply(lambda x: count1(x[\"Drug\"],drugsInPDFs),axis=1) \ncytodrugsDF['CountInText'] = cytodrugsDF.apply(lambda x: count1(x[\"Drug\"],cytoDrugsInPDFs),axis=1) \n\n#lets plot 10 most mentioned drugs\n#MAXPLOT=10 \n#plt.figure(figsize=(20,5))\n#plt.bar(drugsDF[\"Drug\"][(-drugsDF[\"CountInText\"].to_numpy()).argsort()[:MAXPLOT]], drugsDF[\"CountInText\"][(-drugsDF[\"CountInText\"].to_numpy()).argsort()[:MAXPLOT]])\n#plt.xticks(rotation=90,fontsize=12)\n#plt.yticks(fontsize=12)\n#plt.ylabel(\"Counts\",fontsize=15)\n#plt.title(\"Drug mentions\")\n#plt.show()\ndrugsDF[\"all\"] = \"all\" #single root hack for plotly treemap\ncytodrugsDF[\"cytoStorm\"] = \"cytoStorm\" #single root hack for plotly treemap\n\ndrugsplot = px.treemap(drugsDF,path=['all', 'Drug'], values='CountInText')\ndrugsplot.show()\n\ncytodrugsplot = px.treemap(cytodrugsDF,path=['cytoStorm', 'Drug'], values='CountInText')\ncytodrugsplot.show()","541336e4":"As we begin to explore COVID related data, it would be good to start with the current situation, say in India and US. Especially given the population size etc. for India, number of deaths is probably (obviously unfortunately) a useful parameter to see trends on a log scale on. The relevant period starts only in March, so let's plot 1st March onwards. ","42a07510":"**I have been hopeful that one of the existing drugs that has been tested for safety will be helpful in controlling the COVID situation even if it is not a perfect match. This notebook attempts to find such candidate drugs by reading all the scientific articles available at COVID research database (about 50,000 PDFs, many XMLSs too), looking for drug names as mentioned in the drug nomenclature at https:\/\/en.wikipedia.org\/wiki\/Drug_nomenclature. It then tries to narrow down the applicable drugs by looking for structural matches with COVID19 as per genomics available at https:\/\/www.sciencedirect.com\/science\/article\/pii\/S1684118220300827. Additionally, it looks for applicability to relevant virus families. Finally it also tries to search for drugs that have applicability in terms of human activator gene that COVID uses to attach itself with the person it is trying to infect - TMPRSS2. This work tries to use some human genetics information about the activator from https:\/\/www.sciencedirect.com\/topics\/biochemistry-genetics-and-molecular-biology\/tmprss2.**\nAlso, as per https:\/\/www.ibtimes.sg\/scientists-find-human-antibody-47d11-that-neutralizes-covid-19-virus-infecting-living-cells-44388 antibody 47D11 is super useful, so we will search for ALL drugs that might have that. Further, as per recent research (refer https:\/\/www.webmd.com\/lung\/news\/20200417\/cytokine-storms-may-be-fueling-some-covid-deaths), Cytokine storms are particularly dangerous in this context and it may be worth looking at ALL medicines that help with that.\nLets read all scientific articles (to begin with PDFs only). As per information at https:\/\/arxiv.org\/pdf\/2004.10706.pdf, the research data identifies such papers through associated SHA while each XML parse is named using it's associated PMC ID. We will start with PDFs for now since that seems to cover 80% of the usual 80-20. Although PDF Json will be identified by SHA info in the metadata, the actual file (SHAID + json suffix) can be in any of the sub-directories, so the first step is to find the actual path to the PDF Json. Once found, create a python dictionary (say, shaToPDFText) that maps each sha entry to text that accumulates all \"body_text\" entries from the json file.****","a0bf60f3":"Once we have collected all the PDF text, try and find mention of the drugs by leveraging stems and affixes mentioned for Drug nomenclature at https:\/\/en.wikipedia.org\/wiki\/Drug_nomenclature. To narrow down the drugs to what might be relevant to COVID situation, check if the accumulated PDF text mentions SARS\/MERS\/Corona. Also, as per https:\/\/www.sciencedirect.com\/science\/article\/pii\/S1684118220300827, COVID-19 is containing single-stranded (positive-sense) RNA associated with a nucleoprotein within a capsid comprised of matrix protein. So we will try and narrow drugs for that. Further, Similar to SARS-CoV, SARS-CoV-2 (COVID-19) uses a protease called TMPRSS2 to complete this process. In order to attach virus receptor (spike protein) to its cellular ligand (ACE2), activation by TMPRSS2 as a protease is needed. TMPRSS2 is present on human chromosome 21 (chromosome 21q22.3 to be precise), so that might be useful information too - please see https:\/\/www.sciencedirect.com\/topics\/biochemistry-genetics-and-molecular-biology\/tmprss2.\nAlso, as per latest research (refer https:\/\/www.webmd.com\/lung\/news\/20200417\/cytokine-storms-may-be-fueling-some-covid-deaths), Cytokine storms are particularly dangerous in this context and it may be worth looking at ALL medicines that help with that. "}}