{"cell_type":{"67c70a7d":"code","afbb1d31":"code","862e5966":"code","45a6bb4f":"code","0557a451":"code","7200f6c3":"code","51a02290":"code","b865d947":"code","344f4cdd":"code","626164a5":"code","515d15df":"code","3c156efe":"code","0809eb24":"code","b6aff472":"code","f4cfc435":"code","08af8b26":"code","f2ae8a1e":"code","ae750f3e":"code","442fb464":"code","ec585fae":"code","a23f0163":"code","4665e28f":"code","d9df4c97":"code","a185496b":"code","9ff25fad":"code","0258b66c":"code","4deac4c6":"code","fffa0976":"code","7e73b027":"code","933fad6b":"code","b4367cc4":"code","ffc396ec":"code","e9c3bc0c":"code","bbde1d7f":"code","cc7b19e1":"code","45eb1878":"code","f2d3c21f":"code","6f5edd68":"code","7b24b9aa":"code","fcd5b009":"code","cae26b35":"code","5bdf3571":"code","ee406bc1":"code","39d9cbbe":"code","a7a3711e":"code","56abe545":"code","ddf2e63f":"code","27d55d78":"code","5a6a8050":"code","144546fa":"code","d731881d":"code","dfd510c3":"code","809a75a8":"code","07c56d57":"code","d6fd436b":"code","a631f522":"code","c1b8847b":"code","d492238a":"code","aa052920":"code","9b8ce7b5":"code","48f8fefa":"code","8aac4a5e":"code","922c5b1d":"code","43341a96":"code","e2ecc51b":"code","51f0f233":"code","54c1d775":"code","b6443f9a":"code","eb8d67f1":"code","2917c3a3":"code","86b8bf6e":"code","c2159248":"code","1ac43582":"code","eda7d243":"code","e4283d93":"code","5ffca203":"code","b2473e9a":"code","114410d8":"code","a03004a6":"code","b9f01e70":"markdown","0b2677b9":"markdown","6d02d746":"markdown","700b0b22":"markdown","f46f79de":"markdown","8b9fdbfa":"markdown","0e013db6":"markdown","474cd3e0":"markdown","520fdddb":"markdown","afff20f0":"markdown","fc35a0bd":"markdown","4c7ebf1e":"markdown","394ec14a":"markdown","5987a542":"markdown","fcc1aeb0":"markdown","6e07ac48":"markdown","388a2779":"markdown","972e3d11":"markdown","43a8fc49":"markdown","7ad7eda4":"markdown","50876eff":"markdown","2f52b90f":"markdown","fe11d5ae":"markdown","af8e68da":"markdown","aa1123a8":"markdown","4d224b79":"markdown"},"source":{"67c70a7d":"# Data Wrangling \nimport numpy as np\nimport pandas as pd \n\n# Data Visualisation \n%matplotlib inline\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n# Machine Learning \nfrom sklearn.linear_model import LogisticRegression, SGDClassifier, Perceptron \nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, mean_squared_error","afbb1d31":"train_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')\ncombine = [train_data, test_data]","862e5966":"train_data.head()","45a6bb4f":"test_data.head()","0557a451":"train_data.describe()","7200f6c3":"train_data.describe(percentiles = [.08, .07, .06])","51a02290":"plt.figure(figsize = (15, 6))\nsns.heatmap(train_data.corr(), annot = True)","b865d947":"train_data.isnull().sum()","344f4cdd":"test_data.isnull().sum()","626164a5":"for dataset in combine: \n    dataset['age'] = dataset['age_in_days']\/\/365\n    dataset.drop(['age_in_days'], axis = 1, inplace = True)\ntrain_data.head()","515d15df":"train_data[['sourcing_channel', 'target']].groupby('sourcing_channel', as_index = False).mean()","3c156efe":"train_data['IncomeBands'] = pd.cut(train_data['Income'], 5)\ntrain_data[['IncomeBands', 'target']].groupby('IncomeBands', as_index = False).count()","0809eb24":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\nscaler = MinMaxScaler()\nscaler = scaler.fit(train_data[['Income']])\nx_scaled = scaler.transform(train_data[['Income']])\nx_scaled","b6aff472":"# print(scaler.mean_)\nprint(scaler.scale_)","f4cfc435":"train_data['scaled_income'] = x_scaled\ntrain_data.head()","08af8b26":"train_data['IncomeBands'] = pd.cut(train_data['scaled_income'], 5)\ntrain_data[['IncomeBands', 'target']].groupby('IncomeBands', as_index = False).count()","f2ae8a1e":"print(train_data['Income'].mean())\nprint(train_data['Income'].median())","ae750f3e":"plt.hist(train_data['Income'])\nplt.show()","442fb464":"upper_bound = 0.95\nlower_bound = 0.1\nres = train_data['Income'].quantile([lower_bound, upper_bound])\nprint(res)","ec585fae":"true_index = (train_data['Income'] < res.loc[upper_bound])\ntrue_index","a23f0163":"false_index = ~true_index","4665e28f":"no_outlier_data = train_data[true_index].copy()\nno_outlier_data.head()","d9df4c97":"no_outlier_data['IncomeBands'] = pd.cut(no_outlier_data['Income'], 5)\nno_outlier_data[['IncomeBands', 'target']].groupby('IncomeBands', as_index = False).count()","a185496b":"combine = [train_data, test_data]\nfor dataset in combine: \n    dataset.loc[ dataset['Income'] <= 23603.99, 'Income'] = 0\n    dataset.loc[(dataset['Income'] > 23603.99) & (dataset['Income'] <= 109232.0), 'Income'] = 1\n    dataset.loc[(dataset['Income'] > 109232.0) & (dataset['Income'] <= 194434.0), 'Income'] = 2\n    dataset.loc[(dataset['Income'] > 194434.0) & (dataset['Income'] <= 279636.0), 'Income'] = 3\n    dataset.loc[(dataset['Income'] > 279636.0) & (dataset['Income'] <= 364838.0), 'Income'] = 4\n    dataset.loc[(dataset['Income'] > 364838.0) & (dataset['Income'] <= 450040.0), 'Income'] = 5\n    dataset.loc[ dataset['Income'] > 450040.0, 'Income'] = 6\n    \ntrain_data.head()","9ff25fad":"train_data.loc[false_index, 'Income'] = 5\ntrain_data.head()","0258b66c":"train_data.drop(['IncomeBands', 'scaled_income'], axis = 1, inplace = True)\ntrain_data.head()","4deac4c6":"train_data['AgeBands'] = pd.cut(train_data['age'], 5)\ntrain_data[['AgeBands', 'target']].groupby('AgeBands', as_index = False).count()","fffa0976":"for dataset in combine:    \n    dataset.loc[ dataset['age'] <= 37.4, 'age'] = 0\n    dataset.loc[(dataset['age'] > 37.4) & (dataset['age'] <= 53.8), 'age'] = 1\n    dataset.loc[(dataset['age'] > 53.8) & (dataset['age'] <= 70.2), 'age'] = 2\n    dataset.loc[(dataset['age'] > 70.2) & (dataset['age'] <= 86.6), 'age'] = 3\n    dataset.loc[ dataset['age'] > 86.6, 'age'] = 4\ntrain_data.drop('AgeBands', axis = 1, inplace = True)\ncombine = [train_data, test_data]\ntrain_data.head()","7e73b027":"train_data[['age', 'application_underwriting_score']].groupby('age').mean()","933fad6b":"train_data['PremBand'] = pd.cut(train_data['no_of_premiums_paid'], 5)\ntrain_data[['PremBand', 'application_underwriting_score']].groupby('PremBand').count()","b4367cc4":"print(train_data['application_underwriting_score'].mean())\nprint(train_data['application_underwriting_score'].std())","ffc396ec":"print(train_data[train_data['sourcing_channel'] == 'A']['application_underwriting_score'].std())\ntrain_data[['sourcing_channel', 'target']].groupby('sourcing_channel', as_index = False).mean()","e9c3bc0c":"# print(train_data[train_data['sourcing_channel'] == 'C']['application_underwriting_score'].std())\ntrain_data[['sourcing_channel', 'application_underwriting_score']].groupby('sourcing_channel', as_index = False).mean()","bbde1d7f":"train_data[['residence_area_type', 'application_underwriting_score']].groupby('residence_area_type', as_index = False).mean()","cc7b19e1":"train_data.dtypes","45eb1878":"combine = [train_data, test_data]\nfor dataset in combine: \n    mask1 = dataset['application_underwriting_score'].isnull()\n    for source in ['A', 'B', 'C', 'D', 'E']:\n        mask2 = (dataset['sourcing_channel'] == source)\n        source_mean = dataset[dataset['sourcing_channel'] == source]['application_underwriting_score'].mean()\n        dataset.loc[mask1 & mask2, 'application_underwriting_score'] = source_mean\ntrain_data.head()","f2d3c21f":"dataset['application_underwriting_score'].isnull()","6f5edd68":"test_data[test_data['Count_3-6_months_late'].isnull()]","7b24b9aa":"sns.countplot(x = 'Count_3-6_months_late', data = train_data, hue = 'target')","fcd5b009":"sns.countplot(x = 'Count_6-12_months_late', data = train_data, hue = 'target')","cae26b35":"combine = [train_data, test_data]\nfor dataset in combine: \n    dataset['late_premium'] = 0.0\ntrain_data.head()","5bdf3571":"combine = [train_data, test_data]\nfor dataset in combine:\n        dataset.loc[(dataset['Count_3-6_months_late'].isnull()),  'late_premium'] = np.NaN\n        dataset.loc[(dataset['Count_3-6_months_late'].notnull()), 'late_premium'] = dataset['Count_3-6_months_late'] + dataset['Count_6-12_months_late'] + dataset['Count_more_than_12_months_late']\ntrain_data.head() ","ee406bc1":"train_data['target'].corr(train_data['late_premium'])","39d9cbbe":"plt.figure(figsize = (15, 6))\nsns.heatmap(test_data.corr(), annot = True)","a7a3711e":"sns.regplot(x = 'perc_premium_paid_by_cash_credit', y = 'late_premium', data = train_data)","56abe545":"sns.countplot(x = 'late_premium', data = train_data, hue = 'target')","ddf2e63f":"train_data[['late_premium', 'target']].groupby('late_premium').mean()","27d55d78":"# for dataset in [train_data]:\ntrain_data.loc[(train_data['target'] == 0) & (train_data['late_premium'].isnull()),'late_premium'] = 7\ntrain_data.loc[(train_data['target'] == 1) & (train_data['late_premium'].isnull()),'late_premium'] = 2\ntrain_data.head()","5a6a8050":"print(train_data.isnull().sum())\nprint('\\n')\nprint(test_data.isnull().sum())","144546fa":"guess_prem = np.zeros(5)\nfor dataset in [test_data]:\n    for i in range(1, 6):\n        guess_df = dataset[(dataset['Income'] == i)]['late_premium'].dropna()\n\n        # age_mean = guess_df.mean()\n        # age_std = guess_df.std()\n        # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n        premium_guess = guess_df.median()\n        guess_prem[i - 1] = int(premium_guess) \n\n    for j in range(1, 6):\n        dataset.loc[(dataset.late_premium.isnull()) & (dataset.Income == j), 'late_premium'] = guess_prem[j - 1] + 1\n\n    dataset['late_premium'] = dataset['late_premium'].astype(int)\n\ntest_data.head(10)","d731881d":"train_data.drop(['Count_3-6_months_late', 'Count_6-12_months_late', 'Count_more_than_12_months_late'], axis = 1, inplace = True)\ntest_data.drop(['Count_3-6_months_late', 'Count_6-12_months_late', 'Count_more_than_12_months_late'], axis = 1, inplace = True)","dfd510c3":"# Converting Area Type and sourcing channel to Ordinal Variables\ncombine = [train_data, test_data]\nfor dataset in combine: \n    dataset['residence_area_type'] = dataset['residence_area_type'].map( {'Urban' : 1, 'Rural' : 0} )\n    dataset['sourcing_channel'] = dataset['sourcing_channel'].map( {'A' : 0, 'B' : 1, 'C' : 2, 'D' : 3, 'E' : 4} )\ntrain_data.head()\n","809a75a8":"train_data['application_underwriting_score'] = train_data['application_underwriting_score']\/100\ntrain_data.head()","07c56d57":"upper_bound = 0.95\nres = train_data['no_of_premiums_paid'].quantile([upper_bound])\nprint(res)\n","d6fd436b":"true_index = train_data['no_of_premiums_paid'] < res.loc[upper_bound]\nfalse_index = ~true_index\ntrue_index","a631f522":"train_data['PremBand'] = pd.cut(train_data[true_index]['no_of_premiums_paid'], 4)\ntrain_data[['PremBand', 'application_underwriting_score']].groupby('PremBand').count()","c1b8847b":"# combine = [train_data, test_data]\n# for dataset in combine: \n#     dataset.loc[ dataset['no_of_premiums_paid'] <= 6.25, 'no_of_premiums_paid'] = 0\n#     dataset.loc[(dataset['no_of_premiums_paid'] > 6.25) & (dataset['no_of_premiums_paid'] <= 10.5), 'no_of_premiums_paid'] = 1\n#     dataset.loc[(dataset['no_of_premiums_paid'] > 10.50) & (dataset['no_of_premiums_paid'] <= 14.75), 'no_of_premiums_paid'] = 2\n#     dataset.loc[(dataset['no_of_premiums_paid'] > 14.75) & (dataset['no_of_premiums_paid'] <= 19.0), 'no_of_premiums_paid'] = 3\n#     dataset.loc[ dataset['no_of_premiums_paid'] > 19.0, 'no_of_premiums_paid'] = 4\n    \n# train_data.drop('PremBand', axis = 1, inplace = True)\n# train_data.head()","d492238a":"upper_bound = 0.90\nres = train_data['premium'].quantile([upper_bound])\nprint(res)\ntrue_index = train_data['premium'] < res.loc[upper_bound]\nfalse_index = ~true_index\ntrue_index","aa052920":"train_data['PremBand'] = pd.cut(train_data[true_index]['premium'], 4)\ntrain_data[['PremBand', 'target']].groupby('PremBand').count()","9b8ce7b5":"test_data.head()","48f8fefa":"combine = [train_data]\nfor dataset in combine: \n    dataset.loc[ dataset['premium'] <= 5925.0, 'premium'] = 0\n    dataset.loc[(dataset['premium'] > 5925.00) & (dataset['premium'] <= 10650.0), 'premium'] = 1\n    dataset.loc[(dataset['premium'] > 10650.0) & (dataset['premium'] <= 15375.0), 'premium'] = 2\n    dataset.loc[(dataset['premium'] > 15375.0) & (dataset['premium'] <= 201200.0), 'premium'] = 3\n    dataset.loc[ dataset['premium'] > 201200.0, 'premium'] = 4\ntrain_data.drop('PremBand', axis = 1, inplace = True)\ntrain_data.head()\ncombine = [train_data, test_data]","8aac4a5e":"train_data['PremBand'] = pd.cut(train_data['perc_premium_paid_by_cash_credit'], 4)\ntrain_data[['PremBand', 'target']].groupby('PremBand').mean()","922c5b1d":"combine = [train_data, test_data]\nfor dataset in combine: \n    dataset.loc[ dataset['perc_premium_paid_by_cash_credit'] <= 0.25, 'perc_premium_paid_by_cash_credit'] = 0\n    dataset.loc[(dataset['perc_premium_paid_by_cash_credit'] > 0.25) & (dataset['perc_premium_paid_by_cash_credit'] <= 0.5), 'perc_premium_paid_by_cash_credit'] = 1\n    dataset.loc[(dataset['perc_premium_paid_by_cash_credit'] > 0.5) & (dataset['perc_premium_paid_by_cash_credit'] <= 0.75), 'perc_premium_paid_by_cash_credit'] = 2\n    dataset.loc[ dataset['perc_premium_paid_by_cash_credit'] > 0.75, 'perc_premium_paid_by_cash_credit'] = 3\ntrain_data.drop('PremBand', axis = 1, inplace = True)\ntrain_data.head()","43341a96":"test_data.head()","e2ecc51b":"train_data[['perc_premium_paid_by_cash_credit', 'late_premium']] = train_data[['perc_premium_paid_by_cash_credit', 'late_premium']].astype(int)\ntest_data[['perc_premium_paid_by_cash_credit']] = test_data[['perc_premium_paid_by_cash_credit']].astype(int)\ntest_data.head()","51f0f233":"X_train = train_data.drop(['id', 'target', 'premium', 'perc_premium_paid_by_cash_credit'], axis = 1).copy()\ny_train = train_data['target']\nX_test = test_data.drop(['id', 'perc_premium_paid_by_cash_credit'], axis = 1).copy()\nprint(X_train.shape, y_train.shape, X_test.shape)","54c1d775":"X_train.head()","b6443f9a":"X_test.head()","eb8d67f1":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, y_train) * 100, 2)\nacc_log","2917c3a3":"coeff_data = pd.DataFrame(train_data.columns.delete(0))\ncoeff_data.columns = ['Feature']\ncoeff_data['Correlation'] = pd.Series(logreg.coef_[0])\ncoeff_data.sort_values(by = 'Correlation', ascending = False)","86b8bf6e":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\ny_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, y_train) * 100, 2)\nacc_gaussian","c2159248":"knn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, y_train) * 100, 2)\nacc_knn","1ac43582":"# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, y_train)\ny_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, y_train) * 100, 2)\nacc_perceptron","eda7d243":"# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train, y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, y_train) * 100, 2)\nacc_sgd","e4283d93":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier(max_depth = 7)\ndecision_tree.fit(X_train, y_train)\ny_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, y_train) * 100, 2)\nacc_decision_tree","5ffca203":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators = 10)\nrandom_forest.fit(X_train, y_train)\ny_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, y_train)\nacc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\nacc_random_forest","b2473e9a":"pred_values = pd.DataFrame({\n    'Model': ['KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', \n              'Decision Tree'],\n    'Score': [acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_decision_tree]})\npred_values.sort_values(by='Score', ascending=False)","114410d8":"submission = pd.DataFrame({\n        \"id\": test_data[\"id\"],\n        \"target\": y_pred\n    })\nsubmission.to_csv('submission.csv', index=False)","a03004a6":"submission.describe()","b9f01e70":"## Conversion to numerical data","0b2677b9":"Let's make the data splits","6d02d746":"## Building our models","700b0b22":"# Predicting whether a person will default on their premium\n\nImporting necessary libraries","f46f79de":"### We also need to convert the premium column","8b9fdbfa":"**Perceptron Algorithm**","0e013db6":" Add  a new variable 'late premium' for late premiums","474cd3e0":"If it's more than 7 then the loan is never sanctioned. So, let's set those values first. ","520fdddb":"### Replacing the late_premium value in the test data ","afff20f0":"### Let's try and deal with outlier values","fc35a0bd":"We might need to make income groups to understand the relations better ","4c7ebf1e":"### Logistic Regression ","394ec14a":"Read data into dataframes ","5987a542":"**Let's also make groups for Age**","fcc1aeb0":"## EDA ","6e07ac48":"### Let's also work on no of premiums paid","388a2779":"We can set the values of underwriting score on the basis of the sourcing channel","972e3d11":"So, we can collect all the values in this range and let go of the other ones. ","43a8fc49":"Let's make groups for the new income range","7ad7eda4":"k - Nearest Neighbours","50876eff":"Further conversions","2f52b90f":"Let's standardize our data by using a standard scaler","fe11d5ae":"Finally convert percentage premium paid","af8e68da":"**Inference**\n\n* 93% of the people have paid their premiums. \n* The age of people is very varied between 21 and 103","aa1123a8":"### Let's try and fill the missing values\n\n#### Application Under writing score","4d224b79":"## Data Wrangling "}}