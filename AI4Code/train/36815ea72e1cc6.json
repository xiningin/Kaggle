{"cell_type":{"2c7bc13d":"code","65073d51":"code","409c3158":"code","62f5e894":"code","d3174989":"code","e8207704":"code","b649654c":"code","962e5664":"code","0a1dfed6":"code","348fed8c":"code","fd97b710":"code","d6de7d12":"code","821b9762":"code","c1fc00e2":"code","0b809e69":"code","160da545":"code","d713063d":"code","7c81baea":"code","b73ce34e":"code","aee55ca5":"code","5dd0e0b3":"code","e4327a1c":"code","45028cd4":"code","cf76e0bb":"code","25aed33d":"code","c6ca944b":"code","1bba8078":"code","f7e4249e":"code","82178b9c":"code","753d5f28":"code","eb787b67":"code","4e2d0f41":"code","4f7a8499":"code","4f69f070":"code","e8999adb":"code","31f66de1":"code","327b8f05":"code","201fab42":"code","945df76e":"code","53cb4252":"markdown","35c5bb69":"markdown","8a732c92":"markdown","0ebc759c":"markdown","d3be860a":"markdown","eb7edb75":"markdown","3d49dca3":"markdown","7367c517":"markdown","6691670a":"markdown","b7e987db":"markdown","28ebe805":"markdown","8abd4448":"markdown","cf1818d2":"markdown","e295c7e3":"markdown","406b8228":"markdown","38758eeb":"markdown","6a24b9c5":"markdown","db21e264":"markdown","86270837":"markdown","4bec2c47":"markdown","713a704b":"markdown","68face35":"markdown"},"source":{"2c7bc13d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport warnings\nwarnings.filterwarnings('ignore')\n\nsns.set_style(\"whitegrid\")\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","65073d51":"train = pd.read_csv( f\"\/kaggle\/input\/pep-december-test1\/train.csv\")\ntest = pd.read_csv( f\"\/kaggle\/input\/pep-december-test1\/test.csv\")","409c3158":"print( \"No of instances in train dataset - \", train.shape[0], \"& features : \",train.shape[1] )\nprint( \"No of instances in test dataset - \", test.shape[0], \"& features : \",test.shape[1] )","62f5e894":"#let's preview the dataset\ntrain.sample(7)","d3174989":"train.describe()","e8207704":"train.info()","b649654c":"# Let's see null values in the dataset\ntrain.isnull().sum() \/ train.shape[0]","962e5664":"for col in [ 'gill-attachment', \"ring-type\"]:\n    val = round( ( train[col].isnull().sum() ) * 100 \/ train.shape[0], 2 )\n    print( f\"{val}% missing values present in {col} feature\")\n","0a1dfed6":"# let'see any duplicated instance present in the dataset\ntrain.duplicated().sum()","348fed8c":"sns.countplot( train['season'] )","fd97b710":"cat_col = train.select_dtypes('object').columns\ncat_col","d6de7d12":"def plot_count(x,fig):\n    plt.subplot(10,2,fig)\n    plt.title(x+' Histogram')\n    sns.countplot(train[x],palette=(\"magma\"))\n    \n    plt.subplot(10,2,(fig+1))\n    plt.title(x+' vs season')\n    sns.countplot(x=train[x], hue = train.season , palette=(\"coolwarm\"))\n    \nplt.figure(figsize=(15,20))\n\nplot_count('edible-poisonous', 1)\nplot_count('cap-shape', 3)\nplot_count('cap-color', 5)\nplot_count('does-bruise-or-bleed', 7)\n\nplt.tight_layout()","821b9762":"def plot_count(x,fig):\n    plt.subplot(10,2,fig)\n    plt.title(x+' Histogram')\n    sns.countplot(train[x],palette=(\"magma\"))\n    \n    plt.subplot(10,2,(fig+1))\n    plt.title(x+' vs season')\n    sns.countplot(x=train[x], hue = train.season , palette=(\"coolwarm\"))\n    \nplt.figure(figsize=(15,20))\n\nplot_count('gill-attachment', 1)\nplot_count('gill-color', 3)\nplot_count('stem-color', 5)\nplot_count('has-ring', 7)\n\nplt.tight_layout()","c1fc00e2":"def plot_count(x,fig):\n    plt.subplot(10,2,fig)\n    plt.title(x+' Histogram')\n    sns.countplot(train[x],palette=(\"magma\"))\n    \n    plt.subplot(10,2,(fig+1))\n    plt.title(x+' vs season')\n    sns.countplot(x=train[x], hue = train.season , palette=(\"coolwarm\"))\n    \nplt.figure(figsize=(15,20))\n\nplot_count('ring-type', 1)\nplot_count('habitat', 3)\n\nplt.tight_layout()","0b809e69":"#let's see numerical columns\nnum_col = train.select_dtypes( exclude = 'object').columns\nnum_col","160da545":"for i in [ 'cap-diameter' , 'stem-height' , 'stem-width']:\n    plt.figure( figsize = ( 20, 8 ) )\n    plt.subplot( 2 , 2 , 1 )\n    sns.distplot( train[i]  )\n    plt.subplot( 2, 2, 2 )\n    sns.boxplot( train[i] )\n    plt.show()\n    \nplt.tight_layout()","d713063d":"plt.figure( figsize = (15,8))\nsns.scatterplot( x = 'cap-diameter', y = 'stem-height' , data = train , hue = 'season', style=\"season\", palette=\"deep\")","7c81baea":"plt.figure( figsize = (15,8))\nsns.scatterplot( x = 'cap-diameter', y = 'stem-width' , data = train , hue = 'season', style=\"season\", palette=\"deep\")","b73ce34e":"#let's impute missing value with mode of the feature\ndf= train.copy()\nfrom sklearn.impute import SimpleImputer\nimp = SimpleImputer(strategy=\"most_frequent\")\ntrain[['gill-attachment', \"ring-type\"]] = imp.fit_transform( train[['gill-attachment', \"ring-type\"]] )","aee55ca5":"train.isna().sum()","5dd0e0b3":"df = train.copy()\ndef dummies( data  ):\n    cols = data.select_dtypes( 'object').columns\n    temp = pd.get_dummies( data[cols] , drop_first = True )\n    data = pd.concat( [data,temp] , axis = 1)\n    data.drop(cols , axis = 1 , inplace = True )\n    return data\ntrain = dummies( train  )","e4327a1c":"train.head()","45028cd4":"#let's remove 'id' feature from the dataset \ntrain.drop( 'id', axis = 1 , inplace = True )","cf76e0bb":"from sklearn.preprocessing import StandardScaler\nnum_col = [ \"cap-diameter\" , \"stem-height\" , \"stem-width\"]\nscaler = StandardScaler()\ntrain[ num_col ] = scaler.fit_transform(train[ num_col ]  )\ntrain.describe()","25aed33d":"from sklearn.model_selection import train_test_split , GridSearchCV, RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression ,  SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.multiclass import OneVsOneClassifier , OneVsRestClassifier\nfrom sklearn.metrics import accuracy_score\n\nfrom tqdm import tqdm","c6ca944b":"#here we need to do stratifed samplied to get proportion of classes in both training and testing dataset\nX =  train.drop(['season'], axis=1)\ny =  train['season']\n# y = y.astype('object')\nxtrain, xtest , ytrain , ytest = train_test_split(X, y, test_size = 0.2 , stratify = y , random_state=42 )","1bba8078":"log = LogisticRegression(solver='liblinear')\nsgd = SGDClassifier()\nforest = RandomForestClassifier()\nsvc = SVC()","f7e4249e":"# ovo = OneVsOneClassifier( log  , n_jobs= -1)\n# ovo.fit(xtrain, ytrain)\n\n# # predict on training data\n# train_pred = ovo.predict(xtrain)\n# train_acc = accuracy_score( ytrain, train_pred)\n# train_acc","82178b9c":"def oneVsOne( models ):\n    df1 = pd.DataFrame( columns = ['Model', \"Training score\", \"Testing score\"])\n    fitted_models = []\n    for i in tqdm( models ):\n        model = i\n        ovo = OneVsOneClassifier( model , n_jobs= -1)\n        ovo.fit(xtrain, ytrain)\n        \n        fitted_models.append( ovo )\n\n        #predict on training data\n        train_pred = ovo.predict(xtrain)\n        train_acc = accuracy_score( ytrain, train_pred )\n\n        #predict on testing data\n        test_pred = ovo.predict(xtest)\n        test_acc = accuracy_score( ytest, test_pred )\n        \n        var_name = type(model).__name__\n        row = { \"Model\":  var_name , \"Training score\": train_acc , \"Testing score\": test_acc }\n        df1 = df1.append( row,ignore_index=True )\n    return df1, fitted_models\n    \nmodels = [ log, sgd,  forest, svc ]\nprint( \"ONE Vs ONE\")\nscore, fitted_model = oneVsOne( models )\nscore","753d5f28":"def oneVsrest( models ):\n    df1 = pd.DataFrame( columns = ['Model', \"Training score\", \"Testing score\"])\n    fitted_models = []\n    for i in tqdm( models ):\n        model = i\n        ovr = OneVsRestClassifier( model , n_jobs= -1)\n        ovr.fit(xtrain, ytrain)\n        \n        #add the fitted model to the list\n        fitted_models.append( ovr )\n\n        #predict on training data\n        train_pred = ovr.predict(xtrain)\n        train_acc = accuracy_score( ytrain, train_pred )\n\n        #predict on testing data\n        test_pred = ovr.predict(xtest)\n        test_acc = accuracy_score( ytest, test_pred )\n        \n        var_name = type(model).__name__\n        row = { \"Model\":  var_name , \"Training score\": train_acc , \"Testing score\": test_acc }\n        df1 = df1.append( row,ignore_index=True )\n    return df1, fitted_models\n    \nmodels = [ log, sgd,  forest, svc ]\nprint( \"ONE Vs REST\")\nscore1, fm1 = oneVsrest( models )\nscore1","eb787b67":"# # declare parameters for hyperparameter tuning\n# params = [ { \n#     #'C':[1, 10, 100, 1000], 'kernel':['linear']},\n#     {'C':[1, 10, 100, 1000], 'kernel':['rbf'], 'gamma':[0.1, 0.2,  0.4, 0.6,  0.8, 0.9]},\n# #    {'C':[1, 10, 100, 1000], 'kernel':['poly'], 'degree': [2,3,4] ,'gamma':[0.01,0.02,0.03,0.04,0.05]\n# } \n#         ]\n\n# grid_search = GridSearchCV(estimator = svc,  \n#                            param_grid = params,\n#                            scoring = 'accuracy',\n#                            cv = 4,\n#                            verbose=0)\n\n# grid_search.fit(xtrain, ytrain)\n\n# # best score achieved during the GridSearchCV\n# print('GridSearch CV best score : {:.4f}'.format(grid_search.best_score_))\n\n# # print parameters that give the best results\n# print('best paramter :','', (grid_search.best_params_))\n\n# # print estimator that was chosen by the GridSearch\n# print('Best Estimator :', (grid_search.best_estimator_))","4e2d0f41":"test = pd.read_csv( f\"\/kaggle\/input\/pep-december-test1\/test.csv\")\ntest.info()","4f7a8499":"test.describe()","4f69f070":"test.isnull().sum()","e8999adb":"df_test = test.copy()\n#impute null values in cat feature\ncat_col = ['gill-attachment', \"ring-type\"]\ntest[cat_col] = imp.fit_transform( test[ cat_col ] )\n#one hot encoding\ntest = dummies( test  )\ntest.drop( 'id', axis = 1 , inplace = True )\nnum_col = [ \"cap-diameter\" , \"stem-height\" , \"stem-width\"]\n#standardization\nscaler = StandardScaler()\ntest[ num_col ] = scaler.fit_transform( test[ num_col ]  )","31f66de1":"df_test.shape , test.shape\n","327b8f05":"final_model = fitted_model[ models.index(svc)]\ny_pred  = final_model.predict( test )","201fab42":"output = pd.DataFrame( { \"id\" : df_test['id'] , \"season\": y_pred })\noutput.head()","945df76e":"# Let's create the csv file for submitting the data\noutput.to_csv( \"season_submission.csv\", index = False )\nprint( \"success!!!!\")","53cb4252":"### Testing Dataset","35c5bb69":"### 3. Model Training","8a732c92":"### Submission","0ebc759c":"- In ring-type feature, none has more number of records. It may be insignificant feature\n- In habitat feature , most number of records in woods, grasses and leaves.","d3be860a":"- if cap-diameter greater than 10 and stem-width greater than 50 , higher the chance of season will be 1 and 3.","eb7edb75":"#### 2.1  Imputer Missing values","3d49dca3":"- Poisonous has more records than edible. Season1 and season 3 has higher number of edible and poisonous records than others.\n\n- Convex shape has more number of records. Convex and bell has more number of recors in Season 1 and season 3\n\n- Brown and white cap-color has higher number of records. It's mostly grown in Season 1 and season 3\n\n- Does-bruise-or-bleed feature - true has grown in season1 and season 3. Season 2 and season 4 has fewer records with this feature","7367c517":"#### 2.3 Feature Scaling","6691670a":"from looking at the distribution of season, there are some imbalances in the traget variable.","b7e987db":"There are missing values in gill-attachment and ring -type feature. Let's see missing value percentage of those features","28ebe805":"Now that we have applied one hot encoding and standardization on testing data. Let's the shape of original test data and transformed test data","8abd4448":"Lets see the features of test data","cf1818d2":"### Model Evaluation\n\n- RandomForest training accuracy is good. But, testing score is low . it is overfitting.\n- In both OneVsOne and OneVsRest classfier, svm model works well. Let's perform hyperparameter tuning on this model.\n","e295c7e3":"Let's apply all the imputing, scaling techniques in testing dataset like we have done in training dataset.","406b8228":"- Stem-width , stem-height and capdiameter are normally distributed. There are some outlier present in the data.","38758eeb":"- Stem-color and gill-color feature has more number of records in brown, white , yellow and grren color. Other color has low no. of records. We can ignore or bin with the top color\n- In Gill-attachment featur,e decurrent has very low records in season 2","6a24b9c5":"Now that,we have removed missing values in our dataset. Let's do one-hot encoding on categorical variables","db21e264":"- cap-diameter greater than 40, there will be a higher chance that the season fall under either 2 or 3\n- stem-height greater than 25 , there will be a higher chance that the season fall under either 1 or 3","86270837":"### 2. Data cleaning","4bec2c47":"now, we can see that there are 10 categorical variables and 5 continuous variable in the dataset. Our target variable is season.","713a704b":"### EDA","68face35":"#### 2.2 Categorical Encoding\n"}}