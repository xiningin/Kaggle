{"cell_type":{"1acda1d7":"code","27007b93":"code","d4557c9f":"code","00b01974":"code","47dfa8f2":"code","cf194aee":"code","c9f7c93e":"code","f1341723":"code","b49e0735":"code","a85c3005":"code","af923f30":"code","6b6753a2":"code","c0a5a924":"code","fdfa20d7":"code","fc849b31":"code","c422681b":"code","3d9df9f9":"code","2120def0":"code","d2b9d6d3":"code","bb919062":"markdown","e88cb46d":"markdown","388f7fa2":"markdown","67b783c7":"markdown","10aab5bd":"markdown","96ee80d9":"markdown","e618926a":"markdown","473b8fd7":"markdown"},"source":{"1acda1d7":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.stats import rankdata\n\ndef ridge_cv (vec, X, y, X_test, folds, stratified ):\n    kf = StratifiedKFold(n_splits=FOLDS,shuffle=True,random_state=123)\n    val_scores = []\n    rmse_scores = []\n    X_less_toxics = []\n    X_more_toxics = []\n\n    preds = []\n    for fold, (train_index,val_index) in enumerate(kf.split(X,stratified)):\n        X_train, y_train = X[train_index], y[train_index]\n        X_val, y_val = X[val_index], y[val_index]\n        model = Ridge()\n        model.fit(X_train, y_train)\n\n        rmse_score = mean_squared_error ( model.predict (X_val), y_val, squared = False) \n        rmse_scores.append (rmse_score)\n\n        X_less_toxic = vec.transform(df_val['less_toxic'])\n        X_more_toxic = vec.transform(df_val['more_toxic'])\n\n        p1 = model.predict(X_less_toxic)\n        p2 = model.predict(X_more_toxic)\n\n        X_less_toxics.append ( p1 )\n        X_more_toxics.append ( p2 )\n\n        # Validation Accuracy\n        val_acc = (p1< p2).mean()\n        val_scores.append(val_acc)\n\n        pred = model.predict (X_test)\n        preds.append (pred)\n\n        print(f\"FOLD:{fold}, rmse_fold:{rmse_score:.5f}, val_acc:{val_acc:.5f}\")\n\n    mean_val_acc = np.mean (val_scores)\n    mean_rmse_score = np.mean (rmse_scores)\n\n    p1 = np.mean ( np.vstack(X_less_toxics), axis=0 )\n    p2 = np.mean ( np.vstack(X_more_toxics), axis=0 )\n\n    val_acc = (p1< p2).mean()\n\n    print(f\"OOF: val_acc:{val_acc:.5f}, mean val_acc:{mean_val_acc:.5f}, mean rmse_score:{mean_rmse_score:.5f}\")\n    \n    preds = np.mean ( np.vstack(preds), axis=0 )\n    \n    return p1, p2, preds\n","27007b93":"df_val = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\ndf_test = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")","d4557c9f":"features = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n\njc_train_df = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\")\nprint(f\"Train: {jc_train_df.shape[0]}\")\njc_test_df = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv\")\ntemp_df = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/test_labels.csv\")\njc_test_df = jc_test_df.merge ( temp_df, on =\"id\")\n\n#drop test data not used for scoring\njc_test_df = jc_test_df.query (\"toxic != -1\")\nprint(f\"Test: {jc_test_df.shape[0]}\")\n\njc_df = jc_train_df.append ( jc_test_df ) \nprint(f\"Train+Test:{jc_df.shape[0]}\")\n\njc_df.head()","00b01974":"print(f'duplicated by text:{jc_df.duplicated(\"comment_text\").sum()}')\n\njc_df[\"toxic_subtype_sum\"]=jc_df[features].sum(axis=1)\njc_df[\"toxic_behaviour\"]=jc_df[\"toxic_subtype_sum\"].map(lambda x: x > 0)\n\ntot_toxic_behaviour = jc_df[\"toxic_behaviour\"].sum()\nprint(f'comments with toxic behaviour:{tot_toxic_behaviour}')\n","47dfa8f2":"f,ax=plt.subplots(1,2,figsize=(10,4))\njc_df['toxic_behaviour'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_xlabel('% comments with toxic behaviours')\nax[0].set_ylabel('')\n\nsns.countplot(x='toxic_subtype_sum',data=jc_df.query(\"toxic_subtype_sum > 0\"),ax=ax[1])\n\nplt.suptitle('Toxic Comment Classification Challenge')\nplt.show()\n\ndf = jc_df.query(\"toxic_subtype_sum > 0\").groupby(features).agg({\"id\":\"count\"}).reset_index().sort_values(by=\"id\", ascending=False).head(10)\ndf = df.rename (columns={\"id\":\"count\"})\ndf[\"perc\"] = df[\"count\"]\/tot_toxic_behaviour\ndf.head(10)","cf194aee":"df = jc_df.query (\"toxic == 1 or severe_toxic == 1\").groupby([\"toxic\",\"severe_toxic\"]).agg({\"id\":\"count\"}).reset_index()\ndf = df.rename (columns={\"id\":\"count\"})\ndf[\"perc\"] = df[\"count\"]\/tot_toxic_behaviour\ndf","c9f7c93e":"df = jc_df.query (\"toxic == 0 and toxic_subtype_sum > 0\" ).groupby(features).agg({\"id\":\"count\"}).reset_index()\ndf = df.rename (columns={\"id\":\"count\"})\ndf[\"perc\"] = df[\"count\"]\/tot_toxic_behaviour\ndf.sort_values(by=\"count\", ascending=False)","f1341723":"toxic = 1.0\nsevere_toxic = 2.0\nobscene = 1.0\nthreat = 1.0\ninsult = 1.0\nidentity_hate = 2.0\n\ndef create_train (df):\n    df['y'] = df[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]].max(axis=1)\n    df['y'] = df[\"y\"]+df['severe_toxic']*severe_toxic\n    df['y'] = df[\"y\"]+df['obscene']*obscene\n    df['y'] = df[\"y\"]+df['threat']*threat\n    df['y'] = df[\"y\"]+df['insult']*insult\n    df['y'] = df[\"y\"]+df['identity_hate']*identity_hate\n    \n    \n    \n    df = df[['comment_text', 'y', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].rename(columns={'comment_text': 'text'})\n\n    #undersample non toxic comments  on Toxic Comment Classification Challenge\n\n    min_len = (df['y'] >= 1).sum()\n    df_y0_undersample = df[df['y'] == 0].sample(n=int(min_len*1.5),random_state=201)\n    df = pd.concat([df[df['y'] >= 1], df_y0_undersample])\n                                                \n    return df\n \njc_train_df = create_train (jc_train_df)\njc_test_df = create_train (jc_test_df)\n\n\n                           \njc_df = jc_train_df.append(jc_test_df)                           \n\nsns.countplot(x='y',data=jc_df)\n\nplt.title('Target distribution for train')\nplt.show()\n\n","b49e0735":"FOLDS = 5\n\nvec = TfidfVectorizer(analyzer='char_wb', max_df=0.5, min_df=3, ngram_range=(4, 6) )\nX = vec.fit_transform(jc_df['text'])\ny = jc_df[\"y\"].values\nX_test = vec.transform(df_test['text'])\n\nstratified = np.around ( y )\njc_p1, jc_p2, jc_preds =  ridge_cv (vec, X, y, X_test, FOLDS, stratified )","a85c3005":"features = [\"toxicity\",\"severe_toxicity\",\"obscene\",\"insult\",\"identity_attack\", \"sexual_explicit\"]\ncols = ['id', 'comment_text', 'toxicity', 'severe_toxicity', 'obscene', 'threat','insult', 'identity_attack', 'sexual_explicit', 'toxicity_annotator_count']\n\n\njuc_df = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/all_data.csv\")\nprint(f\"jigsaw-toxic-comment-classification-challenge shape:{juc_df.shape[0]}\")\nprint(f'duplicated by id:{juc_df.duplicated(\"id\").sum()}, duplicated by text:{juc_df.duplicated(\"comment_text\").sum()}')\n\njuc_df[[\"id\", \"comment_text\"] + features + ['toxicity_annotator_count']].head()","af923f30":"plt.hist (juc_df[\"toxicity_annotator_count\"].clip(0,100), bins=100)\nplt.title (\"Annotator count distribution\")\nplt.show()","6b6753a2":"juc_df = juc_df.query (\"toxicity_annotator_count > 5\")\nprint(f\"juc_df:{juc_df.shape}\")\n\njuc_df['y'] = juc_df[[ 'severe_toxicity', 'obscene', 'sexual_explicit','identity_attack', 'insult', 'threat']].sum(axis=1)\n\njuc_df['y'] = juc_df.apply(lambda row: row[\"toxicity\"] if row[\"toxicity\"] <= 0.5 else row[\"y\"] , axis=1)\njuc_df = juc_df[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\nmin_len = (juc_df['y'] > 0.5).sum()\ndf_y0_undersample = juc_df[juc_df['y'] <= 0.5].sample(n=int(min_len*1.5),random_state=201)\njuc_df = pd.concat([juc_df[juc_df['y'] > 0.5], df_y0_undersample])\n\nplt.hist (juc_df[\"y\"], bins=100)\nplt.title('Target distribution for train')\nplt.show()","c0a5a924":"FOLDS = 5\nvec = TfidfVectorizer(analyzer='char_wb', max_df=0.5, min_df=3, ngram_range=(4, 6) )\nX = vec.fit_transform(juc_df['text'])\ny = juc_df[\"y\"].values\nX_test = vec.transform(df_test['text'])\n\nstratified = (np.around ( y, decimals = 1  )*10).astype(int)\njuc_p1, juc_p2, juc_preds =  ridge_cv (vec, X, y, X_test, FOLDS, stratified )","fdfa20d7":"rud_df = pd.read_csv(\"..\/input\/ruddit-jigsaw-dataset\/Dataset\/ruddit_with_text.csv\")\nrud_df.head()","fc849b31":"plt.hist(rud_df['offensiveness_score'], bins=50)\nplt.title(\"Offensiveness distribution\")\nplt.show()","c422681b":"print(f\"rud_df:{rud_df.shape}\")\nrud_df['y'] = rud_df['offensiveness_score'].map(lambda x: 0.0 if x <=0 else x)\nrud_df = rud_df[['txt', 'y']].rename(columns={'txt': 'text'})\nmin_len = (rud_df['y'] < 0.5).sum()\n\nplt.hist (rud_df[\"y\"], bins=100)\nplt.title('Target distribution for train')\nplt.show()","3d9df9f9":"FOLDS = 5\nvec = TfidfVectorizer(analyzer='char_wb', max_df=0.5, min_df=3, ngram_range=(4, 6) )\nX = vec.fit_transform(rud_df['text'])\ny = rud_df[\"y\"].values\nX_test = vec.transform(df_test['text'])\n\nstratified = (np.around ( y, decimals = 1  )*10).astype(int)\nrud_p1, rud_p2, rud_preds =  ridge_cv (vec, X, y, X_test, FOLDS, stratified )","2120def0":"jc_max = max(jc_p1.max() , jc_p2.max())\njuc_max = max(juc_p1.max() , juc_p2.max())\nrud_max = max(rud_p1.max() , rud_p2.max())\n\n\np1 = jc_p1\/jc_max + juc_p1\/juc_max + rud_p1\/rud_max\np2 = jc_p2\/jc_max + juc_p2\/juc_max + rud_p2\/rud_max\n\nval_acc = (p1< p2).mean()\nprint(f\"Ensemble: val_acc:{val_acc:.5f}\")","d2b9d6d3":"score = jc_preds\/jc_max + juc_preds\/juc_max + rud_preds\/rud_max  \n## to enforce unique values on score\ndf_test['score'] = rankdata(score, method='ordinal')\n\ndf_test[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)\n\ndf_test.head()","bb919062":"## Unintended Bias in Toxicity Classification\n\n* [Jigsaw Unintended Bias in Toxicity Classification](https:\/\/www.kaggle.com\/c\/jigsaw-unintended-bias-in-toxicity-classification\/data)\n* [TensorFlow Datasets link](https:\/\/www.tensorflow.org\/datasets\/catalog\/civil_comments)\n* [Kaggle Dataset](https:\/\/www.kaggle.com\/julian3833\/jigsaw-unintended-bias-in-toxicity-classification) thanks to @julian3833 !\n\n\nThe comments in this dataset come from an archive of the Civil Comments platform, a commenting plugin for independent news sites. These public comments were created from 2015 - 2017 and appeared on approximately 50 English-language news sites across the world. When Civil Comments shut down in 2017, they chose to make the public comments available in a lasting open archive to enable future research.\n\nJigsaw sponsored this effort and extended annotation of this data by human raters for various toxic conversational attributes.\n\nThe original data, published on figshare, includes the public comment text, some associated metadata such as article IDs, timestamps and commenter-generated \"civility\" labels, but does not include user ids. Jigsaw extended this dataset by adding additional labels for toxicity, identity mentions, as well as covert offensiveness. \n\nThis data set was  released for the Jigsaw Unintended Bias in Toxicity Classification Kaggle challenge. \n\n\nEach comment in Train has a toxicity label (target),  this attribute (and all others) are fractional values which represent the fraction of human raters who believed the attribute applied to the given comment. \n\nThe data also has several additional toxicity subtype attributes: severe_toxicity, obscene, threat, insult, identity_attack, sexual_explicit.\n\n*Note that - unlike other Jigsaw dataset - there is not a threat subtype and there is a sexual_explicit subtype*   \n\nAdditionally, a subset of comments have been labelled with a variety of identity attributes, representing the identities that are mentioned in the comment (male, female, transgender, ..., black, white, asian, ...)\n\n*Note that the data contains different comments that can have the exact same text. Different comments that have the same text may have been labeled with different targets or subgroups.*\n\n\n### Labelling Schema\n\nTo obtain the toxicity labels, each comment was shown to up to 10 annotators. Annotators were asked to: \"Rate the toxicity of this comment\"\n\n* Very Toxic (a very hateful, aggressive, or disrespectful comment that is very likely to make you leave a discussion or give up on sharing your perspective)\n* Toxic (a rude, disrespectful, or unreasonable comment that is somewhat likely to make you leave a discussion or give up on sharing your perspective)\n* Hard to Say\n* Not Toxic\n\nThese ratings were then aggregated with the target value representing the fraction of annotations that annotations fell within the former two categories.\n\nTo collect the identity labels, annotators were asked to indicate all identities that were mentioned in the comment. An example question that was asked as part of this annotation effort was: \"What genders are mentioned in the comment?\"\n\n* Male\n* Female\n* Transgender\n* Other gender\n* No gender mentioned\n\nAgain, these were aggregated into fractional values representing the fraction of raters who said the identity was mentioned in the comment.\n\n*Note: Some comments were seen by many more than 10 annotators (up to thousands), due to sampling and strategies used to enforce rater accuracy.*\n\n**TIP**: After the end of the competition test_labels were released so you can append test data to train to have more data","e88cb46d":"## Ruddit: Norms of Offensiveness for English Reddit Comments\n\n* [Ruddit: Norms of Offensiveness for English Reddit Comments](https:\/\/github.com\/hadarishav\/Ruddit)\n* [Kaggle Dataset](https:\/\/www.kaggle.com\/rajkumarl\/ruddit-jigsaw-dataset) thanks to @rajkumarl !\n* [Paper](https:\/\/aclanthology.org\/2021.acl-long.210)\n\n\nRuddit is a dataset of English language Reddit comments that has fine-grained, real-valued scores between -1 (maximally supportive) and 1 (maximally offensive).\n\n\nOffensiveness is evaluated via annotation with Best\u2013Worst Scaling:\n\"We followed the procedure described in Kiritchenko and Mohammad (2016) to obtain BWS annotations. Annotators were presented with 4 comments (4-tuple) at a time and asked to select the comment that is most offensive (least supportive) and the comment that is least offensive (most supportive). We randomly generated 2N distinct 4-tuples (where N is the number of comments in the dataset), such that each comment was seen in eight different 4-tuples and no two 4-tuples had more than 2 items in common. Kiritchenko and Mohammad (2016) show that in a word-level sentiment task, using just three annotations per 4-tuple produces highly reliable results. However, since we work with long comments and a relatively more difficult task, we got each tuple annotated by 6 annotators. Since each comment is seen in 8 different 4-tuples, we obtain 8 X 6 = 48 udgements per comment.\"\n\n\n\n\n\n","388f7fa2":"## Toxic Comment Classification Challenge\n* [Jigsaw Toxic Comment Classification Challenge competition](https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge\/data)\n* [TensorFlow Datasets link](https:\/\/www.tensorflow.org\/datasets\/catalog\/wikipedia_toxicity_subtypes)\n* [Kaggle Dataset](https:\/\/www.kaggle.com\/julian3833\/jigsaw-toxic-comment-classification-challenge) thanks to @julian3833 !\n\nThis dataset was released for the Jigsaw Toxic Comment Classification Challenge competition on Kaggle\n\nThe comments in this dataset come from an archive of Wikipedia talk page comments. \nThese have been annotated by Jigsaw human raters for a variety of toxicity subtypes: toxic, severe_toxic, obscene, threat, insult, identity_hate\nThe toxicity and toxicity subtype labels are binary values (0 or 1) indicating whether the majority of annotators assigned that attribute to the comment text.\n\n\n**TIP**: After the end of the competition test_labels were released so you can append test data to train to have more data","67b783c7":"# Submission","10aab5bd":"An importat attribute of this dataset is 'toxicity_annotator_count': it counts the raters of the comment.\n\n**TIP** yuo can use it to remove comments with few raters","96ee80d9":"## Ensemble","e618926a":"Dataset consist of 223_549 (Train: 159_571, Test: 63_978) not duplicated comments.\n\n10.1% (22_468) comments are labelled with toxic behaviors\n\n32.8% of comments with toxic behaviors are labelled as (\"toxic\"), while 25.5% as (\"toxic\",\"obscene\",\"insult\") and 11.6% as (\"toxic\",\"obscene\") \n\n8.7% of \"toxic\" comments are also labelled as \"severe_toxic\" and \"severe_toxic\" are always \"toxic\" comments  ","473b8fd7":"# How far can we push linear models in LB ? :)\n\ncheck out these forerunners too\n\n* https:\/\/www.kaggle.com\/julian3833\/jigsaw-incredibly-simple-naive-bayes-0-768 \n* https:\/\/www.kaggle.com\/steubk\/jrsotc-ridgeregression\n* https:\/\/www.kaggle.com\/samarthagarwal23\/the-benchmark-0-81-tfidf-ridge\n\nIn this version I exploit test labels - realesed after Jigsaw competitions end - to augment trainset"}}