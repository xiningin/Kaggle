{"cell_type":{"010174c0":"code","4cda3746":"code","e8c53be7":"code","7a1189e4":"code","beebf675":"code","7a7386ad":"code","374a2506":"code","c77f538b":"code","0571fad7":"code","5c175e6c":"code","35aa43b5":"code","7f9fb29d":"code","c6be48e8":"code","9ca14a9a":"code","db92547e":"code","788913f1":"code","89054106":"code","3762d4e5":"code","a2ddee8c":"code","cc9712a1":"code","4f9e57a5":"code","cbc1184c":"code","b24536d6":"code","7e02ca08":"code","d3b18826":"code","4f6e6ea0":"code","bc2a224b":"code","8800f207":"code","16d63072":"code","f1b15e35":"code","acbe7ea6":"code","1ee1c970":"code","4b8b9914":"code","8219dbf3":"code","e90a016f":"code","b484def6":"code","f21a9210":"code","b0d4d8d6":"code","9cedaa88":"code","d184e7d6":"code","8223c70c":"code","fc76cdd5":"code","ef56a258":"code","61ed0a0a":"code","96cb3989":"code","4b1d1903":"code","cce4586e":"code","eed72e54":"code","0d72c4e7":"code","0f562551":"code","e9a33bc3":"code","299a6e9c":"code","a3dd0496":"code","aacc06a7":"code","c941e5c4":"code","50bb08f5":"code","f9ff9f4e":"code","fb66b1ca":"code","d49ad759":"code","6fdd2659":"code","dd6f4704":"code","7ce15ebc":"code","c88f4f63":"code","2458954f":"code","48f33222":"code","6502ab4e":"code","7ed3af45":"code","60372b5c":"code","1b93237e":"code","1e181275":"code","035ad741":"code","ae84771a":"code","e0234146":"code","2778de43":"code","3633b640":"code","bce6acba":"code","8abc84f4":"code","a1e8c6cf":"code","4e33790c":"code","cac2f487":"code","e574ad1b":"code","b78f9c57":"code","d8e48e5d":"code","665d118c":"code","4b34c6a6":"code","7d1ee14d":"code","ee694c8e":"code","139b04e4":"code","3edde57e":"code","2871de57":"code","b1c8a058":"code","5283b981":"code","f3028a01":"code","a7de6f02":"code","d3d8d2f3":"code","dc318c31":"code","abd3a6de":"code","66162f81":"code","1f263907":"code","8a87c14a":"code","17aec250":"code","687bb20d":"code","aa796e99":"code","46f961cb":"code","191f9648":"code","d99b0274":"code","c9f8290f":"code","f39324ac":"code","3593d3f9":"code","572e122c":"code","f47fe697":"code","f348391d":"code","4a5b8af4":"code","74ae10c0":"code","b25474fd":"code","aab58e6a":"code","0a837fe4":"code","cc047f9f":"code","c017674a":"code","ec2bbf78":"code","be7302c6":"code","fe97842c":"code","ed710399":"code","63ae13fa":"code","a9ad6c98":"code","d6ae9d69":"code","95394ea5":"code","40de17ea":"code","1da0983b":"code","38ce97ef":"code","7e6093e0":"code","acde6c28":"code","4dc0fc76":"markdown","7f69aa2d":"markdown","eaf24fa1":"markdown","ac30afec":"markdown","90bfc138":"markdown","5fd21e93":"markdown","25cd9a03":"markdown","642bf7bc":"markdown","cfe73b6f":"markdown","29900a45":"markdown","188ceb14":"markdown","9afc0f0d":"markdown","1a9880f2":"markdown","250365cb":"markdown","d9ee574b":"markdown","5da1afe2":"markdown","42934f7a":"markdown","fcaf4d0c":"markdown","e021db57":"markdown","680639e6":"markdown","066e90be":"markdown","076de979":"markdown","dd686719":"markdown","b529a11a":"markdown","53520a08":"markdown","827d1b1a":"markdown","8adec9ef":"markdown","fc72921a":"markdown","d9da9888":"markdown","a03fc226":"markdown","a183931a":"markdown","7cc4541f":"markdown","4230bf20":"markdown","e3ff3139":"markdown","0f318be0":"markdown","3d45f8d1":"markdown","7fcce976":"markdown","dddabb0f":"markdown","a466c6cd":"markdown","cca9a358":"markdown","473ce607":"markdown","d4e48e44":"markdown"},"source":{"010174c0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sklearn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport os\nprint(os.listdir(\"..\/input\"))\n# Any results you write to the current directory are saved as output.","4cda3746":"# Load data train dan test ke dalam pandas dataframe\ntrain = pd.read_csv(\"..\/input\/train.csv\", nrows = 1000000) # kita hanya gunakan 1 juta baris data\ntest = pd.read_csv(\"..\/input\/test.csv\")","e8c53be7":"# menghasilkan jumlah baris dan jumlah kolom (bentuk data) pada data train dengan fungsi .shape\ntrain.shape","7a1189e4":"# menghasilkan jumlah baris dan jumlah kolom (bentuk data) pada data test dengan fungsi .shape\ntest.shape","beebf675":"# menampilkan 10 data teratas\ntrain.head(10)","7a7386ad":"# fungsi describe() untuk mengetahui statistika data untuk data numeric seperti count, mean, standard deviation, maximum, mininum, dan quartile.\ntrain.describe()","374a2506":"#cek nilai yang hilang \/ missing values di dalam data train\ntrain.isnull().sum().sort_values(ascending=False)","c77f538b":"#cek nilai yang hilang \/ missing values di dalam data test\ntest.isnull().sum().sort_values(ascending=False)","0571fad7":"#drop\/hapus data missing values\ntrain = train.drop(train[train.isnull().any(1)].index, axis = 0)","5c175e6c":"train.shape","35aa43b5":"# periksa kolom target yaitu kolom fare_amount\ntrain['fare_amount'].describe()","7f9fb29d":"# Seleksi nilai negatif tsb, dan menghasilkan 38 kolom fare_amount memiliki nilai negatif \nfrom collections import Counter\nCounter(train['fare_amount']<0)","c6be48e8":"# Hapus nilai negatif kemudian cek dimensi data dengan fungsi .shape\ntrain = train.drop(train[train['fare_amount']<0].index, axis=0)\ntrain.shape","9ca14a9a":"# dan terlihat pada output tidak ada lagi nilai negatif pada kolom fare_amount\ntrain['fare_amount'].describe()","db92547e":"# terlihat pada output jumlah tarif tertiggi adalah $500\ntrain['fare_amount'].sort_values(ascending=False)","788913f1":"train['passenger_count'].describe()","89054106":"# dari output diatas nilai max bernilai 208 passengers. Dengan asumsi bahwa bus adalah 'taksi' di NYC, kita tidak berpikir bus dapat membawa 208 penumpang! Mari kita lihat sebaran bidang ini\n# ini PASTI outlier. Mari kita drop\/hapus\ntrain[train['passenger_count']>6]","3762d4e5":"# # hapus data\/outliers tsb \ntrain = train.drop(train[train['passenger_count']==208].index, axis = 0)","a2ddee8c":"# jauh lebih rapi sekarang! Jumlah penumpang maksimal adalah 6.\ntrain['passenger_count'].describe()","cc9712a1":"# mari kita eksplore kolom pickup latitude dan longitudes\ntrain['pickup_latitude'].describe()","4f9e57a5":"train[train['pickup_latitude']<-90]","cbc1184c":"train[train['pickup_latitude']>90]","b24536d6":"# lalu kita hapus data outliers tsb\ntrain = train.drop(((train[train['pickup_latitude']<-90])|(train[train['pickup_latitude']>90])).index, axis=0)","7e02ca08":"#12 baris terhapus\ntrain.shape","d3b18826":"# lakukan operasi yang sama untuk kolom pickup longitude\ntrain['pickup_longitude'].describe()","4f6e6ea0":"# cek data yang bernilai lebih dari -180\ntrain[train['pickup_longitude']<-180]","bc2a224b":"# cek data yang bernilai lebih dari 180\ntrain[train['pickup_longitude']>180]","8800f207":"# hapus data\/outliers tsb \ntrain = train.drop(((train[train['pickup_longitude']<-180])|(train[train['pickup_longitude']>180])).index, axis=0)","16d63072":"#11 kolom terhapus\ntrain.shape","f1b15e35":"#lakukan operasi yang sama untuk kolom dropoff latitude and longitude\n# cek data yang bernilai lebih dari -90\ntrain[train['dropoff_latitude']<-90]","acbe7ea6":"# cek data yang bernilai lebih dari 90\ntrain[train['dropoff_latitude']>90]","1ee1c970":"# hapus data\/outliers tsb \ntrain = train.drop(((train[train['dropoff_latitude']<-90])|(train[train['dropoff_latitude']>90])).index, axis=0)","4b8b9914":"#8 kolom terhapus\ntrain.shape","8219dbf3":"train.dtypes","e90a016f":"train['key'] = pd.to_datetime(train['key'])\ntrain['pickup_datetime']  = pd.to_datetime(train['pickup_datetime'])","b484def6":"# ubah\/konversi tipe data tsb pada data test juga\ntest['key'] = pd.to_datetime(test['key'])\ntest['pickup_datetime']  = pd.to_datetime(test['pickup_datetime'])","f21a9210":"#cek tipe data tsb setelah di konversi\ntrain.dtypes","b0d4d8d6":"test.dtypes","9cedaa88":"#cek data train setelah di cleansing\ntrain.head()","d184e7d6":"#cek data test setelah di cleansing\ntest.head()","8223c70c":"def haversine_distance(lat1, long1, lat2, long2):\n    data = [train, test]\n    for i in data:\n        R = 6371  #radius of earth in kilometers\n        #R = 3959 #radius of earth in miles\n        phi1 = np.radians(i[lat1])\n        phi2 = np.radians(i[lat2])\n    \n        delta_phi = np.radians(i[lat2]-i[lat1])\n        delta_lambda = np.radians(i[long2]-i[long1])\n    \n        #a = sin\u00b2((\u03c6B - \u03c6A)\/2) + cos \u03c6A . cos \u03c6B . sin\u00b2((\u03bbB - \u03bbA)\/2)\n        a = np.sin(delta_phi \/ 2.0) ** 2 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda \/ 2.0) ** 2\n    \n        #c = 2 * atan2( \u221aa, \u221a(1\u2212a) )\n        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n    \n        #d = R*c\n        d = (R * c) #in kilometers\n        i['H_Distance'] = d\n    return d","fc76cdd5":"haversine_distance('pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude')","ef56a258":"train['H_Distance'].head(10)","61ed0a0a":"test['H_Distance'].head(10)","96cb3989":"train.head(10)","4b1d1903":"test.head(10)","cce4586e":"data = [train,test]\nfor i in data:\n    i['Year'] = i['pickup_datetime'].dt.year\n    i['Month'] = i['pickup_datetime'].dt.month\n    i['Date'] = i['pickup_datetime'].dt.day\n    i['Day of Week'] = i['pickup_datetime'].dt.dayofweek\n    i['Hour'] = i['pickup_datetime'].dt.hour","eed72e54":"train.head()","0d72c4e7":"test.head()","0f562551":"plt.figure(figsize=(15,7))\nplt.hist(train['passenger_count'], bins=15)\nplt.xlabel('No. of Passengers')\nplt.ylabel('Frequency')","e9a33bc3":"plt.figure(figsize=(15,7))\nplt.scatter(x=train['passenger_count'], y=train['fare_amount'], s=1.5)\nplt.xlabel('No. of Passengers')\nplt.ylabel('Fare')","299a6e9c":"plt.figure(figsize=(15,7))\nplt.scatter(x=train['Date'], y=train['fare_amount'], s=1.5)\nplt.xlabel('Date')\nplt.ylabel('Fare')","a3dd0496":"plt.figure(figsize=(15,7))\nplt.hist(train['Hour'], bins=100)\nplt.xlabel('Hour')\nplt.ylabel('Frequency')","aacc06a7":"plt.figure(figsize=(15,7))\nplt.scatter(x=train['Hour'], y=train['fare_amount'], s=1.5)\nplt.xlabel('Hour')\nplt.ylabel('Fare')","c941e5c4":"plt.figure(figsize=(15,7))\nplt.hist(train['Day of Week'], bins=100)\nplt.xlabel('Day of Week')\nplt.ylabel('Frequency')","50bb08f5":"plt.figure(figsize=(15,7))\nplt.scatter(x=train['Day of Week'], y=train['fare_amount'], s=1.5)\nplt.xlabel('Day of Week')\nplt.ylabel('Fare')","f9ff9f4e":"train.sort_values(['H_Distance','fare_amount'], ascending=False)","fb66b1ca":"len(train)","d49ad759":"bins_0 = train.loc[(train['H_Distance'] == 0), ['H_Distance']]\nbins_1 = train.loc[(train['H_Distance'] > 0) & (train['H_Distance'] <= 10),['H_Distance']]\nbins_2 = train.loc[(train['H_Distance'] > 10) & (train['H_Distance'] <= 50),['H_Distance']]\nbins_3 = train.loc[(train['H_Distance'] > 50) & (train['H_Distance'] <= 100),['H_Distance']]\nbins_4 = train.loc[(train['H_Distance'] > 100) & (train['H_Distance'] <= 200),['H_Distance']]\nbins_5 = train.loc[(train['H_Distance'] > 200) & (train['H_Distance'] <= 300),['H_Distance']]\nbins_6 = train.loc[(train['H_Distance'] > 300),['H_Distance']]\nbins_0['bins'] = '0'\nbins_1['bins'] = '0-10'\nbins_2['bins'] = '11-50'\nbins_3['bins'] = '51-100'\nbins_4['bins'] = '100-200'\nbins_5['bins'] = '201-300'\nbins_6['bins'] = '>300'\ndist_bins =pd.concat([bins_0,bins_1,bins_2,bins_3,bins_4,bins_5,bins_6])\n#len(dist_bins)\ndist_bins.columns","6fdd2659":"plt.figure(figsize=(15,7))\nplt.hist(dist_bins['bins'], bins=75)\nplt.xlabel('Bins')\nplt.ylabel('Frequency')","dd6f4704":"Counter(dist_bins['bins'])","7ce15ebc":"#pickup latitude and longitude = 0\ntrain.loc[((train['pickup_latitude']==0) & (train['pickup_longitude']==0))&((train['dropoff_latitude']!=0) & (train['dropoff_longitude']!=0)) & (train['fare_amount']==0)]","c88f4f63":"train = train.drop(train.loc[((train['pickup_latitude']==0) & (train['pickup_longitude']==0))&((train['dropoff_latitude']!=0) & (train['dropoff_longitude']!=0)) & (train['fare_amount']==0)].index, axis=0)","2458954f":"#1 row dropped\ntrain.shape","48f33222":"#Check in test data\ntest.loc[((test['pickup_latitude']==0) & (test['pickup_longitude']==0))&((test['dropoff_latitude']!=0) & (test['dropoff_longitude']!=0))]\n#No records! PHEW!","6502ab4e":"#dropoff latitude and longitude = 0\ntrain.loc[((train['pickup_latitude']!=0) & (train['pickup_longitude']!=0))&((train['dropoff_latitude']==0) & (train['dropoff_longitude']==0)) & (train['fare_amount']==0)]","7ed3af45":"train = train.drop(train.loc[((train['pickup_latitude']!=0) & (train['pickup_longitude']!=0))&((train['dropoff_latitude']==0) & (train['dropoff_longitude']==0)) & (train['fare_amount']==0)].index, axis=0)","60372b5c":"#3 rows dropped\ntrain.shape","1b93237e":"#Checking test data\n#Again no records! AWESOME!\ntest.loc[((test['pickup_latitude']!=0) & (test['pickup_longitude']!=0))&((test['dropoff_latitude']==0) & (test['dropoff_longitude']==0))]","1e181275":"high_distance = train.loc[(train['H_Distance']>200)&(train['fare_amount']!=0)]","035ad741":"high_distance","ae84771a":"high_distance.shape","e0234146":"high_distance['H_Distance'] = high_distance.apply(\n    lambda row: (row['fare_amount'] - 2.50)\/1.56,\n    axis=1\n)","2778de43":"#The distance values have been replaced by the newly calculated ones according to the fare\nhigh_distance","3633b640":"#sync the train data with the newly computed distance values from high_distance dataframe\ntrain.update(high_distance)","bce6acba":"train.shape","8abc84f4":"train[train['H_Distance']==0]","a1e8c6cf":"train[(train['H_Distance']==0)&(train['fare_amount']==0)]","4e33790c":"train = train.drop(train[(train['H_Distance']==0)&(train['fare_amount']==0)].index, axis = 0)","cac2f487":"#4 rows dropped\ntrain[(train['H_Distance']==0)].shape","e574ad1b":"#Between 6AM and 8PM on Mon-Fri\nrush_hour = train.loc[(((train['Hour']>=6)&(train['Hour']<=20)) & ((train['Day of Week']>=1) & (train['Day of Week']<=5)) & (train['H_Distance']==0) & (train['fare_amount'] < 2.5))]\nrush_hour","b78f9c57":"train=train.drop(rush_hour.index, axis=0)","d8e48e5d":"train.shape","665d118c":"#Between 8PM and 6AM on Mon-Fri\nnon_rush_hour = train.loc[(((train['Hour']<6)|(train['Hour']>20)) & ((train['Day of Week']>=1)&(train['Day of Week']<=5)) & (train['H_Distance']==0) & (train['fare_amount'] < 3.0))]\n#print(Counter(non_work_hours['Hour']))\n#print(Counter(non_work_hours['Day of Week']))\nnon_rush_hour\n#keep these. Since the fare_amount is not <2.5 (which is the base fare), these values seem legit to me.","4b34c6a6":"#Saturday and Sunday all hours\nweekends = train.loc[((train['Day of Week']==0) | (train['Day of Week']==6)) & (train['H_Distance']==0) & (train['fare_amount'] < 3.0)]\nweekends\n#Counter(weekends['Day of Week'])\n#keep these too. Since the fare_amount is not <2.5, these values seem legit to me.","7d1ee14d":"train.loc[(train['H_Distance']!=0) & (train['fare_amount']==0)]","ee694c8e":"scenario_3 = train.loc[(train['H_Distance']!=0) & (train['fare_amount']==0)]","139b04e4":"len(scenario_3)","3edde57e":"#We do not have any distance values that are outliers.\nscenario_3.sort_values('H_Distance', ascending=False)","2871de57":"scenario_3['fare_amount'] = scenario_3.apply(\n    lambda row: ((row['H_Distance'] * 1.56) + 2.50), axis=1\n)","b1c8a058":"scenario_3['fare_amount']","5283b981":"train.update(scenario_3)","f3028a01":"train.shape","a7de6f02":"train.loc[(train['H_Distance']==0) & (train['fare_amount']!=0)]","d3d8d2f3":"scenario_4 = train.loc[(train['H_Distance']==0) & (train['fare_amount']!=0)]","dc318c31":"len(scenario_4)","abd3a6de":"#Using our prior knowledge about the base price during weekdays and weekends for the cabs.\n#I do not want to impute these 1502 values as they are legible ones.\nscenario_4.loc[(scenario_4['fare_amount']<=3.0)&(scenario_4['H_Distance']==0)]","66162f81":"scenario_4.loc[(scenario_4['fare_amount']>3.0)&(scenario_4['H_Distance']==0)]","1f263907":"scenario_4_sub = scenario_4.loc[(scenario_4['fare_amount']>3.0)&(scenario_4['H_Distance']==0)]","8a87c14a":"len(scenario_4_sub)","17aec250":"scenario_4_sub['H_Distance'] = scenario_4_sub.apply(\nlambda row: ((row['fare_amount']-2.50)\/1.56), axis=1\n)","687bb20d":"train.update(scenario_4_sub)","aa796e99":"train.shape","46f961cb":"train.columns","191f9648":"test.columns","d99b0274":"#not including the pickup_datetime columns as datetime columns cannot be directly used while modelling. Features need to extracted from the \n#timestamp fields which will later be used as features for modelling.\ntrain = train.drop(['key','pickup_datetime'], axis = 1)\ntest = test.drop(['key','pickup_datetime'], axis = 1)","c9f8290f":"train.columns","f39324ac":"test.columns","3593d3f9":"x_train = train.iloc[:,train.columns!='fare_amount']\ny_train = train['fare_amount'].values\nx_test = test","572e122c":"x_train.shape","f47fe697":"x_train.columns","f348391d":"y_train.shape","4a5b8af4":"x_test.shape","74ae10c0":"x_test.columns","b25474fd":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor()\nrf.fit(x_train, y_train)\nrf_predict = rf.predict(x_test)\n#print(rf_predict)","aab58e6a":"submission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['fare_amount'] = rf_predict\nsubmission.to_csv('submission_1.csv', index=False)\nsubmission.head(20)","0a837fe4":"import lightgbm as lgbm","cc047f9f":"params = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'nthread': -1,\n        'verbose': 0,\n        'num_leaves': 31,\n        'learning_rate': 0.05,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'subsample_freq': 1,\n        'colsample_bytree': 0.6,\n        'reg_aplha': 1,\n        'reg_lambda': 0.001,\n        'metric': 'rmse',\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 10,\n        'scale_pos_weight':1     \n    }","c017674a":"pred_test_y = np.zeros(x_test.shape[0])\npred_test_y.shape","ec2bbf78":"train_set = lgbm.Dataset(x_train, y_train, silent=True)\ntrain_set","be7302c6":"model = lgbm.train(params, train_set = train_set, num_boost_round=300)","fe97842c":"print(model)","ed710399":"pred_test_y = model.predict(x_test, num_iteration = model.best_iteration)","63ae13fa":"print(pred_test_y)","a9ad6c98":"submission['fare_amount'] = pred_test_y\nsubmission.to_csv('submission_LGB.csv', index=False)\nsubmission.head(20)","d6ae9d69":"import xgboost as xgb ","95394ea5":"dtrain = xgb.DMatrix(x_train, label=y_train)\ndtest = xgb.DMatrix(x_test)","40de17ea":"dtrain","1da0983b":"#set parameters for xgboost\nparams = {'max_depth':7,\n          'eta':1,\n          'silent':1,\n          'objective':'reg:linear',\n          'eval_metric':'rmse',\n          'learning_rate':0.05\n         }\nnum_rounds = 50","38ce97ef":"xb = xgb.train(params, dtrain, num_rounds)","7e6093e0":"y_pred_xgb = xb.predict(dtest)\nprint(y_pred_xgb)","acde6c28":"submission['fare_amount'] = y_pred_xgb\nsubmission.to_csv('submission_XGB.csv', index=False)\nsubmission.head(20)","4dc0fc76":"The fares throught the month mostly seem uniform, with the maximum fare received on the 12th","7f69aa2d":"**Data sudah selesai di cleansing, dan selanjutnya siap untuk di masukkan kedalam model machine learning :)**","eaf24fa1":"**2. Does the date and time of pickup affect the fare?**","ac30afec":"Now, for **EDA**. The following are my considerations - \n1. Does the number of passengers affect the fare? \n2. Does the date and time of pickup affect the fare?\n3. Does the day of the week affect the fare?\n4. Does the distance travelled affect the fare?\n\nFirst, let's split the datetime field 'pickup_datetime' to the following - \n* year\n* month\n* date\n* hour\n* day of week\n\nUsing these we shall calculate the day of the week and come to our conclusions about how pickup_location affects the fare.\nAlso, create a new field 'distance' to fetch the distance between the pickup and the drop.","90bfc138":"**1. Does the number of passengers affect the fare? **","5fd21e93":"From the above 2 graphs we can see that single passengers are the most frequent travellers, and the highest fare also seems to come from cabs which carry just 1 passenger.","25cd9a03":"There are values which are greater than 100 kms! In NYC I am not sure why people would take cabs to travel more than a 100 kms. Since the number of bins for 100-200 kms is quite high, I will keep these. These outliers could be because of typos or missing values in the latitude or longitude. Remove fields of the following - \n1.  Pickup latitude and pickup longitude are 0 but dropoff latitude and longitude are not 0, but the fare is 0\n2. vice versa of point 1.\n3. Pickup latitude and pickup longitude are 0 but dropoff latitude and longitude are not 0, but the fare is NOT 0. Here I will have to impute the distance values in both the train and test data.","642bf7bc":"The fares, however, seem to be high betweeb 5AM and 10AM, and 2PM to 4PM. Maybe people who live far away prefer to leave earlier to avoid rush hour traffic?  ","cfe73b6f":"We can calulate the distance in a sphere when latitudes and longitudes are given by [Haversine formula](https:\/\/en.wikipedia.org\/wiki\/Haversine_formula)\n\n**haversine(\u03b8) = sin\u00b2(\u03b8\/2)**\n\nEventually, the formual boils down to the following where \u03c6 is latitude, \u03bb is longitude, R is earth\u2019s radius (mean radius = 6,371km) to include latitude and longitude coordinates (A and B in this case).\n\n**a = sin\u00b2((\u03c6B - \u03c6A)\/2) + cos \u03c6A . cos \u03c6B . sin\u00b2((\u03bbB - \u03bbA)\/2)**\n\n**c = 2 * atan2( \u221aa, \u221a(1\u2212a) )**\n\n**d = R \u22c5 c**\n\n**d = Haversine distance**\n\n*Refer [this](https:\/\/community.esri.com\/groups\/coordinate-reference-systems\/blog\/2017\/10\/05\/haversine-formula) page for more info and examples on Haversine formula*","29900a45":"**Selanjutnya periksa kolom dropoff_latitude dan dropoff_longitudes**","188ceb14":"Dari hasil diatas data test ternyata tidak memiliki missing values","9afc0f0d":"1938 rows! As you can see from the DF above, the abnormally high distances are due to either the pickup or dropoff co-ordinates being incorrect or 0. However, since all these values have fares, I do not wish to drop them as they contain crucial data. Instead, I will replace the initial distance values with distance values calculated using the fare using the following formula \n\n> *distance = (fare_amount - 2.5)\/1.56*","1a9880f2":"**BOOSTING USING LGBM**\n\nThis is my first attempt at using a boosting algorithm such as LGBM. Let's see if LGBM really lives up to its hype of improving scores. My intital score with just the RF was 3.39 and placed me in the top 20%.","250365cb":"**PART 1 --> DATA CLEANSING & EXPLORATORY DATA ANALYSIS (EDA)**\n\nKegiatan yang akan kita lakukan:\n* Melihat bentuk data (shape) dari data train dan test set\n* Cek data NaN, bila ada maka hapus\/drop data NaN tsb\n* Cek outliers, bila ada maka hapus\/drop outliers tsb\n* Konversi jenis kolom yang relevan \/ Type conversion of relevant fields","d9ee574b":"Missing values adalah nilai yang tidak terdefinisi di dataset. Bentuknya beragam, bisa berupa blank cell, ataupun simbol-simbol tertentu seperti NaN (Not a Number), NA (Not Available), ?, -, dan sebagainya. Missing values dapat menjadi masalah dalam analisis data serta tentunya dapat mempengaruhi hasil modelling machine learning. **Dari hasil diatas data train mengandung 10 data missing values pada kolom\/field dropoff_latitude dan dropoff_longitude.**","5da1afe2":"These 27159 rows need to be imputed using the following formula - \n> *distance = (fare_amount - 2.5)\/1.56*","42934f7a":"Check the H_Distance fields which are greater than 200 kms cause there is no way that people would travel more than 200 kms at the most in NYC in a CAB!","fcaf4d0c":"**Selanjutnya periksa kolom passenger_count**","e021db57":"**SCENARIO 2**\n\nFare is not 0 and is less than the base amount, but Distance is 0.\n\nDelete these rows as the minimum is $2.50, and these fares are incorrect values.","680639e6":"**SCENARIO 1**\n\nFare and Distance are both 0. According to the table above, we shall delete them as they do not provide us any info with regards to the data.","066e90be":"Interesting! The time of day definitely plays an important role. The frequency of cab rides seem to be the lowest at 5AM and the highest at 7PM.","076de979":"Now that we have calculated the distance, we shall create columns for the following - \n* year\n* month\n* date\n* hour\n* day of week","dd686719":"* Garis lintang berkisar dari -90 hingga 90.\n* Garis bujur berkisar dari -180 hingga 180.\n\nUraian di atas dengan jelas menunjukkan beberapa outlier. Mari kita saring mereka","b529a11a":"Nah, day of the week doesn't seem to have that much of an influence on the number of cab rides","53520a08":"key and pickup_datetime tampaknya menjadi kolom datetime yang dalam format objek. Mari kita ubah menjadi datetime:","827d1b1a":"**BOOSTING USING XGBM**","8adec9ef":"**SCENARIO 4**\n\nFare is  not 0, but Distance is 0. These values need to be imputed.","fc72921a":"**Selanjutnya periksa kolom pickup_latitude dan pickup_longitudes**","d9da9888":"Diatas dapat terlihat hasil dimensi\/shape data setelah drop\/hapus missing values","a03fc226":"**3. Does the day of the week affect the fare?**","a183931a":"We can see a few rows with distance =0. This could be due to 2 reasons \n1. The cab waited the whole time and the passenger eventually cancelled. *That's why the pickup and drop co-ordinates are the same and maybe, the passenger was charged for the waiting time.*\n2. The pickup and drop co-ordinates were not entered. In other words, these are **missing values**!\n\n28667 rows are too many rows to be deleted. We need to impute these missing values. I have a plan. I intend to impute the missing distance values with the fare and average price per kilometer of NYC cabs.\n\nA quick Google search gave me the following prices  - \n\n* $$2.5 base-price  +  $1.56\/km --> 6AM to 8PM Mon-Fri\n\n* $$3.0 base-price  +  $1.56\/km --> 8PM to 6AM Mon-Fri and Sat&Sun\n\nHowever, before we proceed with the above steps, lets check for the following scenarios to impute the missing fare amount and the H_Distance in train data.\n\n![image.png](attachment:image.png)\n","7cc4541f":"**SCENARIO 3**\n\nFare is 0, but Distance is not 0. These values need to be imputed.\n\nI can calculate the fare as I have the distance. I shall use the following formula\n> *fare = 2.5 + 1.56(H_Distance)*","4230bf20":"The highest fares seem to be on a Sunday and Monday, and the lowest on Wednesday and Friday. Maybe people travel far distances on Sunday and Monday (visiting family and returning back home), and hence, the high fares. And guess people just want to stay at home on a Friday after a hectic week at work, or grab a drink from close by. Hmmm..","e3ff3139":"**Periksa tipe data setiap kolom data train**","0f318be0":"**Periksa dan Cleansing setiap kolom pada data train:**\n1. fare_amount\n2. passanger_count\n3. pickup_longitude\n4. pickup_latitude\n5. dropoff_longitude \n6. dropoff_latitude","3d45f8d1":"**4. Does the distance affect the fare?**\n\nThis is a no-brainer. I am confident that the distance would affect the fare a great deal. But I will visualise it.\n\nFirstly, let's check the frequency of the distances that we calculated using Haversine formula. I will do so by creating bins (0-10 kms, 10-20 kms, and so on, and check for any outliers)","7fcce976":"Now we shall check for rows where the distance values are 0","dddabb0f":"From scenario 2, I can understand that the distance is 0, but the fare is all the minimum fare of $2.5. This could be because the passenger booked the cab but ended up cancelling to pay the base fare (not sure how this works in NYC, but I'm assuming that's how it is)","a466c6cd":"There are 4 rows. There 4 rows do not help us in anyway as we do not know either the distance or the fare to impute the missing values. So we shall drop them ","cca9a358":"And that's a wrap! With the Random Forest code I, got a score of 3.39, which was in the top 20%, with LGBM I got a score of 3.37 (which wasn't a great improvement from my initial RF model but helped me jump a couple of places up the leaderboard), and with XGBoost I got a 3.61, which is the worst of all my submissions. Maybe parameter tuning would help further. :) ","473ce607":"**PART 2 --> MODELLING AND PREDICTION**\n\nFINALLY! Data cleansing is done! Now to split the x and y variables and proceed to modelling. I shall use the random forest method for prediction","d4e48e44":"data target adalah fitur dari dataset yang ingin Anda pahami lebih dalam.\n**Dari output diatas**, kolom target Fare amount\/jumlah tarif memiliki nilai negatif, yang tidak masuk akal. Dan kita Hapus kolom ini."}}