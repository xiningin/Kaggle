{"cell_type":{"cd432ca6":"code","29990a16":"code","1cf0a7da":"code","89ec2687":"code","001ed7c6":"code","fb34f3f9":"code","c6e81e32":"code","b75c3848":"code","990a728f":"code","cf4d6c3e":"code","33eb796f":"code","fecba758":"code","ccc47d54":"code","e9625f1d":"markdown","3eba60a4":"markdown","54e5e98e":"markdown","aa37b606":"markdown","f137fd3d":"markdown","598c269a":"markdown","a3ad27a7":"markdown","48eb3493":"markdown","d64b2759":"markdown","654e33ea":"markdown","3231a991":"markdown","e0556f67":"markdown","3cd4218e":"markdown","c4d4a76f":"markdown","1558f674":"markdown","8a82f2a0":"markdown","39a69be0":"markdown","23942e73":"markdown","ec1cf936":"markdown","e1b8abcf":"markdown","a0cc6a80":"markdown","446e1fb9":"markdown","ca02664f":"markdown","1b0aa08b":"markdown"},"source":{"cd432ca6":"import numpy as np","29990a16":"def sigmoid(x):\n    return (1 \/ (1 + np.exp(-x)))","1cf0a7da":"def setParameters(X, Y, hidden_size):\n    np.random.seed(3)\n    input_size = X.shape[0] # number of neurons in input layer\n    output_size = Y.shape[0] # number of neurons in output layer.\n    W1 = np.random.randn(hidden_size, input_size)*np.sqrt(1\/input_size)\n    b1 = np.zeros((hidden_size, 1))\n    W2 = np.random.randn(output_size, hidden_size)*np.sqrt(1\/hidden_size)\n    b2 = np.zeros((output_size, 1))\n    return {'W1': W1, 'W2': W2, 'b1': b1, 'b2': b2}","89ec2687":"# Python implementation\n# np.random.randn(output_size, hidden_size)*np.sqrt(1\/hidden_size)","001ed7c6":"# np.random.randn(output_size, hidden_size)*0.01","fb34f3f9":"def forwardPropagation(X, params):\n    Z1 = np.dot(params['W1'], X)+params['b1']\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(params['W2'], A1)+params['b2']\n    y = sigmoid(Z2)  \n    return y, {'Z1': Z1, 'Z2': Z2, 'A1': A1, 'y': y}","c6e81e32":"def cost(predict, actual):\n    m = actual.shape[1]\n    cost__ = -np.sum(np.multiply(np.log(predict), actual) + np.multiply((1 - actual), np.log(1 - predict)))\/m\n    return np.squeeze(cost__)","b75c3848":"def backPropagation(X, Y, params, cache):\n    m = X.shape[1]\n    dy = cache['y'] - Y\n    dW2 = (1 \/ m) * np.dot(dy, np.transpose(cache['A1']))\n    db2 = (1 \/ m) * np.sum(dy, axis=1, keepdims=True)\n    dZ1 = np.dot(np.transpose(params['W2']), dy) * (1-np.power(cache['A1'], 2))\n    dW1 = (1 \/ m) * np.dot(dZ1, np.transpose(X))\n    db1 = (1 \/ m) * np.sum(dZ1, axis=1, keepdims=True)\n    return {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}","990a728f":"def updateParameters(gradients, params, learning_rate = 1.2):\n    W1 = params['W1'] - learning_rate * gradients['dW1']\n    b1 = params['b1'] - learning_rate * gradients['db1']\n    W2 = params['W2'] - learning_rate * gradients['dW2']\n    b2 = params['b2'] - learning_rate * gradients['db2']\n    return {'W1': W1, 'W2': W2, 'b1': b1, 'b2': b2}","cf4d6c3e":"def fit(X, Y, learning_rate, hidden_size, number_of_iterations = 5000):\n    params = setParameters(X, Y, hidden_size)\n    cost_ = []\n    for j in range(number_of_iterations):\n        y, cache = forwardPropagation(X, params)\n        costit = cost(y, Y)\n        gradients = backPropagation(X, Y, params, cache)\n        params = updateParameters(gradients, params, learning_rate)\n        cost_.append(costit)\n    return params, cost_","33eb796f":"import sklearn.datasets\nX, Y = sklearn.datasets.make_moons(n_samples=500, noise=.2)\nX, Y = X.T, Y.reshape(1, Y.shape[0])","fecba758":"params, cost_ = fit(X, Y, 0.3, 5, 5000)","ccc47d54":"import matplotlib.pyplot as plt\nplt.plot(cost_)","e9625f1d":"Hello people! I want to show you how to build two layer neural network from scratch using python!","3eba60a4":"I set the learning rate to 0.3, the number of neurons in the hidden layer to 5 and the number of iterations to 5000.\nFeel free to try with different values.\nLet\u2019s draw a graph showing how the cost function changed with every episode:","54e5e98e":"# All About Loops\nWe need to run many interations to find the parameters that return the minimum cost. Let\u2019s loops it!\n","aa37b606":"# Cost function\nWe just looked at forward propagation and obtained a prediction (y). We calculate it using a cost function. The below graph explains:","f137fd3d":"You don\u2019t have to use Xavier initialization, you can also use this:\n","598c269a":"# Activation Function\nIn the hidden layer, we will use the tanh activation function and in the output layer, I will use the sigmoid function. It is easy to find information on both the sigmoid function and the tanh function graph. I don\u2019t want to bore you with explanations, so I will just implement it.","a3ad27a7":"# Forward Propagation\n![](https:\/\/i.ibb.co\/fSDbWFR\/1-m-O1-O1o-Usu9-Fk-YK-81-DPf-TQ.png)","48eb3493":"We update our parameters and find the best parameter that gives us the minimum possible cost. I\u2019m not going to delve into derivatives, but note that on the graph above, if you are on the right sight of the parabola, the derivative (slope) will be positive, so the parameter will decrease and move left approaching the parameter that returns the minimum cost. On the left side, the slope will be negative, so the parameter increases towards the value we want. Let\u2019s look at the cost function we will use:","d64b2759":"# Updating Parameters\nNow that we have our derivatives, we can use the equation below:\n![](https:\/\/i.ibb.co\/XymnY9w\/1-k-AU9-URu-Nx-T9v-A4-CGPw-ZYUw.png)\nIn that equation, alpha (\u03b1) is the learning rate hyperparameter. We need to set it to some value before the learning begins. The term to the right of the learning rate is the derivative. We know alpha and derivatives, let\u2019s update our parameters.","654e33ea":"X input, Y actual output.","3231a991":"What are the params and cache in def backPropagation(X, Y, params, cache)? When we use forward propagation, we store values to use during backpropagation. Params are parameters (weight and biases).","e0556f67":"![](https:\/\/i.ibb.co\/pRXSfLx\/1-T7-U1-JGo-Tlw3oc-Tb-Y-1-TTeg.png)","3cd4218e":"Python code for cost function:\n","c4d4a76f":"Hidden_size means the number of neurons in the hidden layer. It looks like a hyperparameter. Because you set it before learning begins! What return params, cost_ tells us. params are the best parameters we found and cost_ is just cost we estimated in every episode.","1558f674":"I\u2019d recommend never setting weights to zero or a big number when initializing parameters.","8a82f2a0":"Why we are storing {\u2018Z1\u2019: Z1, \u2018Z2\u2019: Z2, \u2018A1\u2019: A1, \u2018y\u2019: y}? Because we will use them when back-propagating.","39a69be0":"# Importing Libraries\nThe only library we need is NumPy.","23942e73":"# Setting Parameters\nWhat are parameters and hyperparameters? Parameters are weights and biases. Hyperparameters effect parameters and are before the learning begins. Setting hyperparameters perfectly correctly at first is not a piece of cake, you\u2019ll need to tinker and tweak your values. The learning rate, number of iterations, and regularization rate, among others, can all be considered as hyperparameters.\nWondering how to set the matrices sizes? The answer just below!\n![Setting parameters](https:\/\/i.ibb.co\/6tLhtbM\/1-3-MP-7v-Gy-Nt7np-C-69z-JI9g.png)\nWhat does all that mean? For example:\n* (layer 0 so L = 0) number of neurons in input layers = 3\n* (layer 1 so L = 1) number of neurons in hidden layers = 5\n* (layer 2 so L = 2) number of neurons in output layers = 1\n![Example](https:\/\/i.ibb.co\/W2t1zWS\/1-LSMk-P5-YLdb8q-Uv9-QP18-Ky-Q.png)\nI hope this all makes sense! Let\u2019s set the parameters:","ec1cf936":"![](https:\/\/i.ibb.co\/fvMVmwv\/1-a-Kn-E6-Em-YIn-Ho-SQe-Yxbk86g.png)\n![](https:\/\/i.ibb.co\/XymnY9w\/1-k-AU9-URu-Nx-T9v-A4-CGPw-ZYUw.png)","e1b8abcf":"It seems we got great result! Thank you for reading.And please upvote this if you like. <br>\nAlso you can read my article on medium: https:\/\/medium.com\/better-programming\/how-to-build-2-layer-neural-network-from-scratch-in-python-4dd44a13ebba","a0cc6a80":"# Let\u2019s Try Our Code!\nUse [sklearn](https:\/\/scikit-learn.org\/stable\/?source=post_page---------------------------) to create a dataset.","446e1fb9":"We define W1, b1, W2, and b2. It doesn\u2019t hurt if you set your biases to zero at first. However, be very careful when initializing weights. Never set the weights to zero at first. Why exactly? Well, if you do, then in Z = Wx + b, Z will always be zero. If you are building a multi-layer neural network, neurons in every layer will behave like there is one neuron. So how do we initialize weights at first? I use Xavier initialization.\n![](https:\/\/i.ibb.co\/H7XfnmV\/1-x-Y0t-FMVjwdo-JQs8x9by-J1-Q.png)","ca02664f":"The diagram above should give you a good idea of what forward propagation is. The implementation in Python is:","1b0aa08b":"# Backpropagation\nWe\u2019ve found the cost, now let\u2019s go back and find the derivative of our weights and biases. "}}