{"cell_type":{"3c67c511":"code","ae17be10":"code","912024ca":"code","e578bc15":"code","1a8802e1":"code","690c6559":"code","bd7394b5":"code","118852a7":"code","1107650e":"code","5984d2d2":"code","af9d3706":"code","09ad3b85":"code","49672bd5":"code","99263e12":"code","00d6acff":"code","57c2b048":"code","27588a9a":"code","4492a66b":"code","11f1bf4f":"code","69c540be":"code","cd6a9758":"code","d768c26f":"code","3639dd14":"code","d75bb2ff":"code","9d6862b1":"code","dad8ea25":"code","ee336434":"code","6b7c5af6":"code","2eb8c140":"code","e71051ee":"code","3789ad5c":"code","f1bb2126":"code","de7fbc70":"code","ef131933":"code","2bf3aee1":"code","84aea912":"code","9ad3e03b":"code","ca09246c":"code","1004b90f":"code","2be203c8":"code","4682c57e":"code","98c5b624":"code","549543c3":"code","bc458da7":"code","99abe75e":"code","8863110a":"code","4e37024d":"code","7090cf45":"code","f3a7c6b5":"code","a89bef27":"code","c9dacd4e":"code","2562ce62":"code","a1b111af":"code","ec87a1b9":"code","4f200f73":"code","7806526c":"code","1e661721":"code","f9ea8e90":"code","4f7b2861":"code","7efad626":"code","2f0d76f5":"code","e27fd304":"code","d750b0c8":"code","c6f97c0f":"code","28d4b3e1":"code","0e9afe5a":"code","c6a7f2fb":"code","643e7cd3":"code","b76573e8":"markdown","eb2329e9":"markdown","b6f775f2":"markdown","714af11a":"markdown","6b2e454f":"markdown","5685b3ce":"markdown","b5c0f221":"markdown","e99e391c":"markdown","5ef7dd44":"markdown","692c98d4":"markdown","3e365bb6":"markdown","ffababeb":"markdown","6a495aa8":"markdown","4a82f40e":"markdown","da634f21":"markdown","284e79c2":"markdown","249114cd":"markdown","ad08e86a":"markdown","3728b4d6":"markdown","198e695e":"markdown","dbe7950e":"markdown","a92a945d":"markdown","ef36cebe":"markdown","99bb7ca4":"markdown","e3fe2bfc":"markdown","888c019b":"markdown"},"source":{"3c67c511":"#Load the librarys\nimport pandas as pd #To work with dataset\nimport numpy as np #Math library\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns #Graph library that use matplot in background\nimport matplotlib.pyplot as plt #to plot some parameters in seaborn\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import PowerTransformer, StandardScaler,Normalizer,RobustScaler,MaxAbsScaler,MinMaxScaler,QuantileTransformer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.neighbors import KNeighborsClassifier\n# Import StandardScaler from scikit-learn\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.impute import KNNImputer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline, Pipeline\n\nfrom sklearn.manifold import TSNE\n# Import train_test_split()\n# Metrics\nfrom sklearn.metrics import roc_auc_score, average_precision_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import roc_curve\nfrom datetime import datetime, date\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.linear_model import LinearRegression, RidgeCV\n\nimport lightgbm as lgbm\nfrom catboost import CatBoostRegressor\nimport tensorflow as tf \nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import LearningRateScheduler\n#import smogn\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\n# For training random forest model\nimport lightgbm as lgb\nfrom scipy import sparse\nfrom sklearn.neighbors import KNeighborsRegressor \nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans \n# Model selection\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression,f_classif\nfrom sklearn.feature_selection import mutual_info_regression\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\n\nfrom itertools import combinations\n#import smong \n\nimport category_encoders as ce\nimport warnings\nimport optuna \nwarnings.filterwarnings('ignore')","ae17be10":"!pip install klib \nimport klib ","912024ca":"%%time\ntrain = pd.read_csv('..\/input\/ventilator-pressure-prediction\/train.csv')\npressure_values = np.sort( train.pressure.unique() )\ntest = pd.read_csv('..\/input\/ventilator-pressure-prediction\/test.csv')\nsubmission = pd.read_csv('..\/input\/ventilator-pressure-prediction\/sample_submission.csv')","e578bc15":"train.head()","1a8802e1":"train.columns","690c6559":"train.info()","bd7394b5":"train.isnull().sum().values","118852a7":"test.isnull().sum().values","1107650e":"train[train.isnull().sum(axis=1) >=1].shape","5984d2d2":"# summarize the number of rows with missing values for each column\nfor i in range(train.shape[1]):\n    # count number of rows with missing values\n    n_miss = train.iloc[:,i].isnull().sum()\n    perc = n_miss \/ train.shape[0] * 100\n    print('> %d, Missing: %d (%.1f%%)' % (i, n_miss, perc))","af9d3706":"klib.missingval_plot(train)","09ad3b85":"train.duplicated(subset='id', keep='first').sum()","49672bd5":"len(train)-len(train.drop_duplicates())","99263e12":"train.describe().T","00d6acff":"plot = klib.corr_plot(train, annot=False, figsize=(12,10))","57c2b048":"plot.figure.savefig('figure1.pdf')","27588a9a":"klib.corr_plot(train, split='pos') # displaying only positive correlations, other settings include threshold, cmap...\nklib.corr_plot(train, split='neg') # displaying only negative correlations","4492a66b":"# Comparing the datasets length\nfig, ax = plt.subplots(figsize=(5, 5))\npie = ax.pie([len(train), len(test)],\n             labels=[\"Train dataset\", \"Test dataset\"],\n             colors=[\"salmon\", \"teal\"],\n             textprops={\"fontsize\": 15},\n             autopct='%1.1f%%')\nax.axis(\"equal\")\nax.set_title(\"Dataset length comparison\", fontsize=18)\nfig.set_facecolor('white')\nplt.show","11f1bf4f":"klib.cat_plot(train[['R', 'C', 'u_out']], figsize=(50,15))","69c540be":"klib.dist_plot(train[['R','C','u_in','u_out','pressure']])","cd6a9758":"train[train.select_dtypes(['float64']).columns] = train[train.select_dtypes(['float64']).columns].apply(pd.to_numeric)\ntrain[train.select_dtypes(['object','int64']).columns] = train.select_dtypes(['object','int64']).apply(lambda x: x.astype('category'))\ntest[test.select_dtypes(['float64']).columns] = test[test.select_dtypes(['float64']).columns].apply(pd.to_numeric)\ntest[test.select_dtypes(['object','int64']).columns] = test.select_dtypes(['object','int64']).apply(lambda x: x.astype('category'))","d768c26f":"cat_columns = train.drop(['id','pressure','breath_id'], axis=1).select_dtypes(exclude=['float64']).columns\nnum_columns = train.drop(['id','pressure','breath_id'], axis=1).select_dtypes(include=['int64','float64','category']).columns","3639dd14":"num_columns","d75bb2ff":"# Numerical features distribution \ni = 1\nplt.figure()\nfig, ax = plt.subplots(3, 2,figsize=(20, 24))\nfor feature in num_columns:\n    plt.subplot(3, 2,i)\n    sns.histplot(train[feature],color=\"red\", kde=True,bins=100, label='train')\n    sns.histplot(test[feature],color=\"olive\", kde=True,bins=100, label='test')\n    plt.xlabel(feature, fontsize=9); plt.legend()\n    i += 1\nplt.show()","9d6862b1":"train.corr()['pressure'][:-1].plot.barh(figsize=(8,6),alpha=.6,color='darkblue')\nplt.xlim(-.075,.075);\nplt.xticks([-0.065, -0.05 , -0.025,  0.   ,  0.025,  0.05 ,  0.065],\n           [str(100*i)+'%' for i in [-0.065, -0.05 , -0.025,  0.   ,  0.025,  0.05 ,  0.065]],fontsize=12)\nplt.title('Correlation between target and numerical variables',fontsize=14);","dad8ea25":"train.corr().style.background_gradient(cmap='viridis')","ee336434":"v0 = sns.color_palette(palette='viridis').as_hex()[0]\nfig = plt.figure(figsize=(18,6))\nsns.boxplot(data=train[num_columns], color=v0,saturation=.5);\nplt.xticks(fontsize= 14)\nplt.title('Box plot of train numerical columns', fontsize=16);","6b7c5af6":"fig = plt.figure(figsize=(18,6))\nsns.boxplot(data=test[num_columns], color=v0,saturation=.5);\nplt.xticks(fontsize= 14)\nplt.title('Box plot of test numerical columns', fontsize=16);","2eb8c140":"fig = plt.figure(figsize=(10,5))\nsns.barplot(y=train.drop(['breath_id'],axis=1)[cat_columns].nunique().values, x=train[cat_columns].nunique().index, color='blue', alpha=.5)\nplt.xticks(rotation=0)\nplt.title('Number of categorical unique values',fontsize=16);","e71051ee":"labels = train['u_out'].astype('category').cat.categories.tolist()\ncounts = train['u_out'].value_counts()\nsizes = [counts[var_cat] for var_cat in labels]\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot\nax1.axis('equal')\nplt.show()","3789ad5c":"train['R'].astype('category').cat.categories.tolist()","f1bb2126":"labels = train['R'].astype('category').cat.categories.tolist()\ncounts = train['R'].value_counts()\nsizes = [counts[var_cat] for var_cat in range(3)]\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot\nax1.axis('equal')\nplt.show()","de7fbc70":"labels = train['C'].astype('category').cat.categories.tolist()\ncounts = train['C'].value_counts()\nsizes = [counts[var_cat] for var_cat in range(3)]\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot\nax1.axis('equal')\nplt.show()","ef131933":"plt.plot(range(80),train['pressure'].iloc[0:80])\nplt.plot(range(80),train['pressure'].iloc[80:160])\nplt.plot(range(80),train['pressure'].iloc[160:240])\nplt.plot(range(80),train['pressure'].iloc[240:320])\nplt.plot(range(80),train['pressure'].iloc[320:400])\nplt.plot(range(80),train['pressure'].iloc[400:480])","2bf3aee1":"cat_columns\n","84aea912":"fig = plt.figure(figsize=(30,10))\ngrid =  gridspec.GridSpec(1,3,figure=fig,hspace=.4,wspace=.4)\nn =1\nfor i in range(1):\n    for j in range(len(cat_columns)):\n        ax = fig.add_subplot(grid[i, j])\n        sns.violinplot(data =  train, y = 'pressure' , x =cat_columns[j] ,ax=ax, alpha =.7, fill=True,palette='viridis')\n        ax.set_title(cat_columns[j],fontsize=14)\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n        n += 1\nfig.suptitle('Violin plot of target with categorical features', fontsize=16,y=.93);","9ad3e03b":"fig = plt.figure(figsize=(30,10))\ngrid =  gridspec.GridSpec(1,3,figure=fig,hspace=.4,wspace=.4)\nn =1\nfor i in range(1):\n    for j in range(len(cat_columns)):\n        ax = fig.add_subplot(grid[i, j])\n        sns.kdeplot(data =  train,hue  = cat_columns[j] , x ='pressure' ,ax=ax, alpha =.7, fill=True,palette='viridis')\n        ax.set_title(cat_columns[j],fontsize=14)\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n        n += 1\nfig.suptitle('kdeplot plot of target with categorical features', fontsize=16,y=.93);","ca09246c":"# Categorical features distribution \ni = 1\nplt.figure()\nfig, ax = plt.subplots(2, 2,figsize=(10,10))\nfor feature in cat_columns:\n    plt.subplot(2, 2,i)\n    sns.histplot(train[feature],color=\"blue\", label='train')\n    sns.histplot(test[feature],color=\"olive\", label='test')\n    plt.xlabel(feature, fontsize=9); plt.legend()\n    i += 1\nplt.show()","1004b90f":"train.pressure.nunique()","2be203c8":"train['pressure'].describe()","4682c57e":"train['pressure'].describe().iloc[1:].plot.barh(color=v0,alpha=.5,figsize=(12,5))\nplt.title('Target data statistics',fontsize=16)\nplt.yticks(fontsize=14)\nplt.xticks(np.arange(0,10.8,.5));","98c5b624":"train.pressure.value_counts()[train.pressure.value_counts()<1000].plot.kde()","549543c3":"# Categorical features distribution \nplt.figure()\nsns.countplot(train['pressure'], label='pressure')\nplt.xlabel(feature, fontsize=9); plt.legend()\nplt.xticks(rotation=45)\nplt.show()","bc458da7":"import plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom scipy import stats\nf, axes = plt.subplots(nrows=3, ncols=1, figsize=(12, 12))\nf.suptitle('pressure', fontsize=16)\ng = sns.kdeplot(train['pressure'], shade=True, label=\"%.2f\"%(train['pressure'].skew()), ax=axes[0])\ng = g.legend(loc=\"best\")\nstats.probplot(train['pressure'], plot=axes[1])\nsns.boxplot(x='pressure', data=train, orient='h', ax=axes[2]);\nplt.tight_layout()\nplt.show()","99abe75e":"y=train['pressure']\nplt.figure(figsize=(12,6))\nsns.boxplot(x=y, width=.4);\nplt.axvline(np.percentile(y,.1), label='.1%', c='blue', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(y,2), label='2%', c='gold', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(y,10), label='10%', c='red', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(y,90), label='90%', c='red', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(y,99), label='99%', c='blue', linestyle=':', linewidth=3)\nplt.axvline(np.percentile(y,98), label='98%', c='gold', linestyle=':', linewidth=3)\nplt.legend()\nplt.title('Box plot of target data', fontsize=16)\nplt.xticks(np.arange(0,10.8,.5));","8863110a":"bins = [np.percentile(y,0),np.percentile(y,10),  np.percentile(y,90), np.percentile(y,100)]\n# Bin labels\nlabels1 = [ 'Low', 'Medium', 'High']\ntrainessai=train.copy()\n# Bin the continuous variable ConvertedSalary using these boundaries\ntrainessai['target_binned'] = pd.cut(trainessai['pressure'], \n                                bins=bins,labels=labels1 )","4e37024d":"labels = trainessai['target_binned'].astype('category').cat.categories.tolist()\ncounts = trainessai['target_binned'].value_counts()\nsizes = [counts[var_cat] for var_cat in labels]\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True) #autopct is show the % on plot\nax1.axis('equal')\nplt.show()","7090cf45":"del train \ndel test ","f3a7c6b5":"train = pd.read_csv('..\/input\/ventilator-pressure-prediction\/train.csv')","a89bef27":"%%time \nm = TSNE()\ndf_numeric =train.drop(['id','breath_id','pressure'], axis=1).iloc[0:8000]._get_numeric_data()\ndf_numeric=df_numeric.dropna()\nX_train =RobustScaler().fit_transform(df_numeric)\ndel df_numeric \n# Fit and transform the t-SNE model on the numeric dataset\ntsne_features = m.fit_transform(X_train)\nprint(tsne_features.shape)","c9dacd4e":"trainessai=trainessai.iloc[0:8000]\ntrainessai['x']=tsne_features[:, 0]\ntrainessai['y']=tsne_features[:, 1]\n# Color the points according to Army Component\nsns.scatterplot(x='x', y='y', hue='pressure', data=trainessai)\n# Show the plot\nplt.show()","2562ce62":"trainessai=trainessai.iloc[0:8000]\ntrainessai['x']=tsne_features[:, 0]\ntrainessai['y']=tsne_features[:, 1]\n# Color the points according to Army Component\nsns.scatterplot(x='x', y='y', hue='target_binned', data=trainessai)\n# Show the plot\nplt.show()","a1b111af":"# Import MiniBatchKmeans \nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.preprocessing import  RobustScaler \nfrom yellowbrick.cluster import KElbowVisualizer","ec87a1b9":"kmeans1 = MiniBatchKMeans(n_clusters=3 ,random_state=42)\nkmean_label1= kmeans1.fit_predict(X_train)\nprint(kmean_label1)","4f200f73":"train.shape","7806526c":"train1=train.drop(['id','breath_id'], axis=1).iloc[0:8000]._get_numeric_data()\n\ntrain1['cluster'] = kmean_label1\ntrain1['cluster'] = train1['cluster'].astype('object')\n","1e661721":"train1.columns","f9ea8e90":"fig = plt.figure(figsize=(18,26))#,constrained_layout=True)\ngrid =  gridspec.GridSpec(7, 2, figure= fig, hspace= .2, wspace= .05)\nn =0\n\nfor i in range(7):\n    for j in range(2):\n        ax = fig.add_subplot(grid[i, j])\n        sns.scatterplot(data=train1, y='pressure', x=train1.columns[j], hue= 'cluster', ax=ax, palette='viridis', alpha=.6 )\n        ax.set_title(train1.columns[j],fontsize=16)\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n        ax.legend(loc='lower left',ncol=20)\n        n += 1\n        \nfig.suptitle('Scatter plot of Target, Numerical and Cluster features', fontsize=20,y=.90)\nfig.text(0.11,0.5, \"Target\", ha=\"center\", va=\"center\", rotation=90, fontsize=18);","4f7b2861":"#Getting the Centroids\ncentroids = kmeans1.cluster_centers_ \n#Getting unique labels\nu_labels = np.unique(kmean_label1)\n \n#plotting the results:\n \nfor i in u_labels:\n    plt.scatter(train1.iloc[kmean_label1 == i ,2] , train1.iloc[kmean_label1 == i , 3] , label = i)\n\nplt.scatter(centroids[:,1] , centroids[:,2] , s = 80, color = 'k')\nplt.legend()\nplt.show()","7efad626":"import seaborn as sns \nred = sns.light_palette(\"red\", as_cmap=True)\ncross_tab=pd.crosstab(train1['cluster'], train1['pressure'], margins = True)\nH=cross_tab\/cross_tab.loc[\"All\"] # Divide by column totals\nH.style.background_gradient(cmap=red)","2f0d76f5":"cross_tab","e27fd304":"fig = plt.figure(figsize=(18,26))#,constrained_layout=True)\ngrid =  gridspec.GridSpec(3, 2, figure= fig, hspace= .2, wspace= .05)\nn =1\nfor i in range(3):\n    for j in range(2):\n        ax = fig.add_subplot(grid[i, j])\n        sns.scatterplot(data=train1, y=train1.columns[j+1], x=train1.columns[j], hue= 'cluster', ax=ax, palette='viridis', alpha=.6 )\n        ax.set_title(train1.columns[j],fontsize=16)\n        ax.set_xlabel('')\n        ax.set_ylabel('')\n        ax.legend(loc='lower left',ncol=20)\n        n += 1\n        \nfig.suptitle('Scatter plot of Target, Numerical and Cluster features', fontsize=20,y=.90)\nfig.text(0.11,0.5, \"Pressure\", ha=\"center\", va=\"center\", rotation=90, fontsize=18);","d750b0c8":"\n\n# Create a series out of the Country column\ncluster = train1.cluster\n\n# Get the counts of each category\ncluster_counts = cluster.value_counts()\n\n# Print the count values for each category\nprint(cluster_counts)\n\n","c6f97c0f":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"darkgrid\")\nsns.barplot(cluster_counts.index,cluster_counts.values, alpha=0.9)\nplt.title('Frequency Distribution of cluster')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('cluster', fontsize=12)\nplt.show()","28d4b3e1":"# select non-numeric columns\ncat_columns = train.drop(['id','pressure','breath_id'], axis=1).select_dtypes(exclude=['int64','float64']).columns","0e9afe5a":"# select the float columns\nnum_columns = train.drop(['id','pressure','breath_id'], axis=1).select_dtypes(include=['int64','float64']).columns","c6a7f2fb":"all_columns = (num_columns)\nprint(cat_columns)\nprint(num_columns)\nprint(all_columns)","643e7cd3":"if set(all_columns) == set(train.drop(['id','pressure','breath_id'], axis=1).columns):\n    print('Ok')\nelse:\n    # Let's see the difference \n    print('dans all_columns mais pas dans train  :', set(all_columns) - set(train.drop(['id','target'], axis=1).columns))\n    print('dans X.columns   mais pas dans all_columns :', set(train.drop(['id','target'], axis=1).columns) - set(all_columns))","b76573e8":"## Categorical features distribution","eb2329e9":"###  exploring target data main statistics","b6f775f2":"## Convert Dtypes ","714af11a":"### Correlation ","6b2e454f":"### Box plot of numerical columns","5685b3ce":"## EDA \n\n### Explore the data\n\n    Null Data\n    Categorical data\n    Itrain.isnull().sum().valuess there Text data\n    wich columns will we use\n    IS there outliers that can destory our algo\n    IS there diffrent range of data\n    Curse of dimm...\n    \n\n####  Null Data \n**How sparse is my data?**\nMost data sets contain missing values, often represented as NaN (Not a Number). If you are working with Pandas you can easily check how many missing values exist in each column.","b5c0f221":"# Kmeans : \n","e99e391c":"### Test data ","5ef7dd44":"\n## Number of categorical unique values","692c98d4":"## t-SNE visualization of high-dimensional data\n\nt-SNE intuition t-SNE is super powerful, but do you know exactly when to use it? When you want to visually explore the patterns in a high dimensional dataset. \n","3e365bb6":"## KDE plot of target with features","ffababeb":"\n## Step 2: Load the data\n\nNext, we'll load the training and test data.\n\nWe set index_col=0 in the code cell below to use the id column to index the DataFrame. (If you're not sure how this works, try temporarily removing index_col=0 and see how it changes the result.)\n","6a495aa8":"### Num Features ","4a82f40e":"## Bin Target :","da634f21":"**Histograms : numerical data seems to be similar to train numerical data.**\n### Zooming on the correlation between numerical variables and target.","284e79c2":"### Visual Exploratory ","249114cd":"### Num\/Cat Features ","ad08e86a":"### Numerical features distribution\n#### Histograms of numerical features","3728b4d6":"Numerical Data seems to be with few outliers appearing in the box plot Also test numerical data seems to looks like the train ones.\n","198e695e":"<a id=\"top\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Data Modeling<\/center><\/h3>\n\nModeling is the part of the Cross-Industry Standard Process for Data Mining (CRISP-DM) process model that i like best. Our data is already in good shape, and now we can search for useful patterns in our data.\n\n<a id=\"top\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Data Evaluation  <\/center><\/h3>\n\n\n\n\n**MAE**\n\nRegression predictive modeling is the task of approximating a mapping function (f) from input variables (X) to a continuous output variable (y).\n\nRegression is different from classification, which involves predicting a category or class label.\n\nEvaluating Regression Models\n\nA common question by beginners to regression predictive modeling projects is:\n\n    How do I calculate accuracy for my regression model?\n\nAccuracy (e.g. classification accuracy) is a measure for classification, not regression.\n\nWe cannot calculate accuracy for a regression model.\n\nThe skill or performance of a regression model must be reported as an error in those predictions.\n\nThis makes sense if you think about it. If you are predicting a numeric value like a height or a dollar amount, you don\u2019t want to know if the model predicted the value exactly (this might be intractably difficult in practice); instead, we want to know how close the predictions were to the expected values.\n\nError addresses exactly this and summarizes on average how close predictions were to their expected values.\n\nThere are three error metrics that are commonly used for evaluating and reporting the performance of a regression model; they are:\n\n    Mean Squared Error (MSE).\n    Root Mean Squared Error (RMSE).\n    Mean Absolute Error (MAE)\n\n**Mean Absolute Error**, or MAE, is a popular metric because, like RMSE, the units of the error score match the units of the target value that is being predicted.\n\nUnlike the RMSE, the changes in MAE are linear and therefore intuitive.\n\nThat is, MSE and RMSE punish larger errors more than smaller errors, inflating or magnifying the mean error score. This is due to the square of the error value. The MAE does not give more or less weight to different types of errors and instead the scores increase linearly with increases in error.\n\nAs its name suggests, the MAE score is calculated as the average of the absolute error values. Absolute or abs() is a mathematical function that simply makes a number positive. Therefore, the difference between an expected and predicted value may be positive or negative and is forced to be positive when calculating the MAE.\n\nThe MAE can be calculated as follows:\n\n    MAE = 1 \/ N * sum for i to N abs(y_i \u2013 yhat_i)\n\nWhere y_i is the i\u2019th expected value in the dataset, yhat_i is the i\u2019th predicted value and abs() is the absolute function.\n\nwe have done all EDA needed to chose the best preprocessing steps and begin modeling .\nWork is in progress .. \n\n**Upvote if you find it useful .**","dbe7950e":"## Target \n### Pressure graph","a92a945d":"## Box plot of target data with percentile of .1% and 99.9%","ef36cebe":"## check that we have all column","99bb7ca4":"It's clear tat there isn't any clear relation between numerical variables and target.\n\nNow Exploring correlation between all numerical variables. First we get a correlation grid of all numercial variables and target\n","e3fe2bfc":"<a id=\"top\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>Data Preparation<\/center><\/h3>\n\n## Data preprocessing\n\nData preprocessing comes after you've cleaned up your data and after you've done some exploratory analysis to understand your dataset. Once you understand your dataset, you'll probably have some idea about how you want to model your data. Machine learning models in Python require numerical input, so if your dataset has categorical variables, you'll need to transform them. Think of data preprocessing as a prerequisite for modeling:\n\nOutlier Handling\n\nScaling\n\nFeature Engineering\n\nFeature Selection \n\n\n","888c019b":"\n<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\"><center>CRISP-DM Methodology<\/center><\/h3>\n\n* [Buissness Understanding](#1)\n* [Data Understanding](#2)\n* [Data Preparation](#3)\n* [Data Modeling](#4)   \n* [Data Evaluation](#5)\n\nIn this section we overview our selected method for engineering our solution. CRISP-DM stands for Cross-Industry Standard Process for Data Mining. It is an open standard guide that describes common approaches that are used by data mining experts. CRISP-DM includes descriptions of the typical phases of a project, including tasks details and provides an overview of the data mining lifecycle. The lifecycle model consists of six phases with arrows indicating the most important and frequent dependencies between phases. The sequence of the phases is not strict. In fact, most projects move back and forth between phases as necessary. It starts with business understanding, and then moves to data understanding, data preparation, modelling, evaluation, and deployment. The CRISP-DM model is flexible and can be customized easily.\n## Buissness Understanding\n\n    Tasks:\n\n    1.Determine business objectives\n\n    2.Assess situation\n\n    3.Determine data mining goals\n\n    4.Produce project plan\n\n## Data Understanding\n     Tasks:\n\n    1.Collect data\n\n    2.Describe data\n\n    3.Explore data    \n\n## Data Preparation\n    Tasks\n    1.Data selection\n\n    2.Data preprocessing\n\n    3.Feature engineering\n\n    4.Dimensionality reduction\n\n            Steps:\n\n            Data cleaning\n\n            Data integration\n\n            Data sampling\n\n            Data dimensionality reduction\n\n            Data formatting\n\n            Data transformation\n\n            Scaling\n\n            Aggregation\n\n            Decomposition\n\n## Data Modeling :\n\nModeling is the part of the Cross-Industry Standard Process for Data Mining (CRISP-DM) process model that i like best. Our data is already in good shape, and now we can search for useful patterns in our data.\n\n    Tasks\n    1. Select modeling technique Select technique\n\n    2. Generate test design\n\n    3. Build model\n\n    4. Assess model\n\n## Data Evaluation :\n    Tasks\n\n    1.Evaluate Result\n\n    2.Review Process\n\n    3.Determine next steps\n\n<a id=\"top\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n<center>Buissness Understanding<\/center><\/h3>\n\n    \nWhat do doctors do when a patient has trouble breathing? They use a ventilator to pump oxygen into a sedated patient's lungs via a tube in the windpipe. But mechanical ventilation is a clinician-intensive procedure, a limitation that was prominently on display during the early days of the COVID-19 pandemic. At the same time, developing new methods for controlling mechanical ventilators is prohibitively expensive, even before reaching clinical trials. High-quality simulators could reduce this barrier.\n\nCurrent simulators are trained as an ensemble, where each model simulates a single lung setting. However, lungs and their attributes form a continuous space, so a parametric approach must be explored that would consider the differences in patient lungs.\n\nPartnering with Princeton University, the team at Google Brain aims to grow the community around machine learning for mechanical ventilation control. They believe that neural networks and deep learning can better generalize across lungs with varying characteristics than the current industry standard of PID controllers.\n\nIn this competition, you\u2019ll simulate a ventilator connected to a sedated patient's lung. The best submissions will take lung attributes compliance and resistance into account.\n\nIf successful, you'll help overcome the cost barrier of developing new methods for controlling mechanical ventilators. This will pave the way for algorithms that adapt to patients and reduce the burden on clinicians during these novel times and beyond. As a result, ventilator treatments may become more widely available to help patients breathe.\n\n**Eval Metric**: The competition will be scored as the mean absolute error between the predicted and actual pressures during the inspiratory phase of each breath. The expiratory phase is not scored. The score is given by:\n\n|X\u2212Y|\n\nwhere X is the vector of predicted pressure and Y is the vector of actual pressures across all breaths in the test set.\n    \n  **Reminder**\n\n* id - globally-unique time step identifier across an entire file\n\n* breath_id - globally-unique time step for breaths\n\n* R - lung attribute indicating how restricted the airway is (in cmH2O\/L\/S). Physically, this is the change in pressure per change in flow (air volume per time). Intuitively, one can imagine blowing up a balloon through a straw. We can change R by changing the diameter of the straw, with higher R being harder to blow.\n\n* C - lung attribute indicating how compliant the lung is (in mL\/cmH2O). Physically, this is the change in volume per change in pressure. Intuitively, one can imagine the same balloon example. We can change C by changing the thickness of the balloon\u2019s latex, with higher C having thinner latex and easier to blow.\n\n* time_step - the actual time stamp.\n\n* u_in - the control input for the inspiratory solenoid valve. Ranges from 0 to 100.\n\n* u_out - the control input for the exploratory solenoid valve. Either 0 or 1.\n\n* pressure - the airway pressure measured in the respiratory circuit, measured in cmH2O.\n    \n<a id=\"top\"><\/a>\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#1777C4; border:0' role=\"tab\" aria-controls=\"home\">\n<center>Data Understanding<\/center><\/h3>\n\n    \n## Step 1: Import helpful libraries"}}