{"cell_type":{"9559666e":"code","899be447":"code","e4be20de":"code","10c7483f":"code","4acbc187":"code","13c06b0f":"code","be28a95a":"code","cf86786e":"code","c1eceeb9":"code","3cfa02e8":"code","aeb52679":"code","19ab1f72":"code","0c8a0ca4":"code","ec9d9869":"code","c93c6981":"code","bb5892ab":"code","5df9f4de":"code","dd8960dc":"code","16fcf35a":"code","cb73495f":"code","5c61bc23":"code","99dfd25f":"code","7d94938c":"code","55520377":"code","4157b211":"code","98008e4f":"code","ae92e979":"code","c6891597":"code","f3a87b38":"code","c0a132e5":"code","e7a9f29a":"code","a77e0374":"code","93092f46":"code","2675720a":"code","a57e6d42":"code","4dacd54b":"code","1a9aa199":"code","08704a1c":"code","2edbb6f7":"code","b65516d9":"code","2f43cf84":"code","8197de67":"code","1f169730":"code","ceacdebf":"code","44183513":"code","62dc1861":"code","9c238d8a":"code","0effbafe":"code","5d721040":"code","b9c0fdae":"code","ff460a81":"code","e65d4054":"code","81bd1b7b":"code","e6f85cc4":"code","777008df":"code","b3632497":"code","5f4016f4":"code","05eb3008":"code","c0864956":"code","f2db67c2":"code","8123767e":"code","ca168c14":"code","392efb61":"code","db948b32":"code","2d9fa086":"markdown","5605ffaf":"markdown","b42904f1":"markdown","defd6969":"markdown","50f72490":"markdown","3aef3e0b":"markdown","252952b9":"markdown","b9efbf45":"markdown","fc72b9ec":"markdown","fd2dc2e5":"markdown","2fcfae7d":"markdown","c55bb4e6":"markdown","6cf345ea":"markdown","f93c440c":"markdown","db7dff3f":"markdown","0089607f":"markdown","9ef46d98":"markdown","deb16cd2":"markdown","95555793":"markdown","175ac4d1":"markdown","d3f765f0":"markdown","207f0144":"markdown","db2c26ea":"markdown","a87b75df":"markdown","5d401f47":"markdown","e0b7f36c":"markdown","7d76bd21":"markdown","2e127ed2":"markdown","bd899da8":"markdown","13c56d89":"markdown","a6eaf3aa":"markdown"},"source":{"9559666e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","899be447":"#system specific\nimport os\nimport sys\n#string operations\nimport string\nimport re\n#assert the python version to be used\nassert sys.version_info >= (3,5)\n#data load and data operations\nimport numpy as np\nimport pandas as pd\n#visualization and exploration\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n#consistent plots\nfrom pylab import rcParams\nrcParams['figure.figsize']= 12,5\nrcParams['axes.labelsize']=12\nrcParams['xtick.labelsize']=12\nrcParams['ytick.labelsize']=12\n#pandas auto profiler\nimport pandas_profiling as pp\n#view all the data columns\npd.options.display.max_columns = None\n#handle unwanted warnings\nimport warnings\nwarnings.filterwarnings(action='ignore',category=DeprecationWarning)\nwarnings.filterwarnings(action='ignore',category=FutureWarning)","e4be20de":"#load the data into pandas dataframe\ntrain_orig = pd.read_csv('\/kaggle\/input\/customer-bank\/train_s3TEQDk.csv',delimiter=',',engine='python')\ntest_orig =  pd.read_csv('\/kaggle\/input\/customer-bank\/test_mSzZ8RL.csv',delimiter=',',engine='python')\nsubmission_orig = pd.read_csv('\/kaggle\/input\/customer-bank\/sample_submission_eyYijxG.csv',delimiter=',',engine='python')","10c7483f":"#create a copy of the original dataset\ntrain = train_orig.copy()\ntest = test_orig.copy()","4acbc187":"#view the top few rows\ntrain.head(10)","13c06b0f":"#check the last few rows\ntrain.tail()","be28a95a":"#check the info \ntrain.info()","cf86786e":"#check basic stats of the numerical features\ntrain.describe().transpose()","c1eceeb9":"#generate profile report for the train data\npp.ProfileReport(train)","3cfa02e8":"#check for duplicates in the ID column\ntrain['ID'] = train['ID'].str.lower()\ntrain['ID'].duplicated().any()","aeb52679":"#drop the ID column from the analysis \ntrain.drop('ID',axis=1,inplace=True)\n#drop the column from the test dataset \ntest.drop('ID',axis=1,inplace=True)","19ab1f72":"#create pair plot\nsns.pairplot(data=train,hue='Is_Lead',diag_kind='kde')\nplt.show()","0c8a0ca4":"#check the balance of the data wrt the target feature\nsns.countplot('Is_Lead',data=train)\nplt.title('Count of Credit Card Lead')\nplt.show()","ec9d9869":"#check the exact number of target labels per class\ntrain['Is_Lead'].value_counts().sort_values(ascending=False)","c93c6981":"#create separate list of numerical and categorical features\ncat_features = train.select_dtypes(include='object').columns.to_list()\nnum_features = train.select_dtypes(exclude='object').columns.to_list()","bb5892ab":"#check the unique value counts per categorical feature\nfor col in cat_features:\n    print('\\n')\n    print(train[col].value_counts().sort_values(ascending=False))","5df9f4de":"#countplot of all the categorical features \nfig = plt.figure(figsize=(20, 35))\nfor index, col in enumerate(cat_features):\n    plt.subplot(4, 2, index+1)\n    sns.countplot(data=train, x=col,hue='Is_Lead') \n    plt.title('Countplot with '+col)","dd8960dc":"#check the numerical features\nnum_features","16fcf35a":"sns.distplot(train['Avg_Account_Balance'],bins=70)\nplt.title('Plot of Average Account Balance (Histogram)')\nplt.grid()\nplt.show()","cb73495f":"#test for normality -- > clearly the data is not normal\nfrom scipy.stats import shapiro\nstats,p = shapiro(train['Avg_Account_Balance'])\nprint('p-value for the Shapiro wilk test %.3f'% (p))\nif p>0.05:\n    \n    print('Data is probably Gaussian')\nelse:\n    print('Data is probably not Gaussian')","5c61bc23":"#boxplot of the avg account balance vs the region code\nplt.figure(figsize=(21,9))\nsns.boxplot(x= 'Region_Code',y='Avg_Account_Balance',data=train)\nplt.title(f'Average Account Balance versus Region Code')\nplt.ylabel('Average Account Balance')\nplt.yscale('log')\nplt.show()","99dfd25f":"#boxplot of the avg account balance vs the region code\nsns.boxplot(x= 'Is_Lead',y='Avg_Account_Balance',data=train)\nplt.title(f'Average Account Balance versus Is_Lead')\nplt.ylabel('Average Account Balance')\nplt.yscale('log')\nplt.show()","7d94938c":"#check the correlation\ntrain[['Age','Vintage','Avg_Account_Balance']].corr()","55520377":"#visualise the correlation \nsns.heatmap(train[['Age','Vintage','Avg_Account_Balance']].corr(),square=True,cmap='summer')\nplt.title('Correlation plot of the numerical features')\nplt.show()","4157b211":"#create a cross table of region code and credit card lead\nregion_code_lead = train.groupby(['Region_Code','Is_Lead']).size().unstack().fillna(0)\nregion_code_lead","98008e4f":"#chi-square test\nfrom scipy.stats import chi2_contingency\n\ndef chisq_of_df_cols(df, c1, c2):\n    '''\n    The function performs the chi-square test between two categorical features\n    Arguments: dataframe df, columns c1 and c2\n    Returns: Chi-square test parameters critical stat, p-value,degrees of freedom and expected\n    '''\n    groupsizes = df.groupby([c1, c2]).size()\n    ctsum = groupsizes.unstack(c1)\n    return(chi2_contingency(ctsum.fillna(0)))\n\n#perform chi square test on the categorical features\nstat, p, dof, expected = chisq_of_df_cols(train, 'Region_Code', 'Is_Lead')\n\n#determine whether dependent or independent based on the p-value\nprint('stat=%.3f, p=%.3f' % (stat, p))\nif p > 0.05:\n    print('Probably independent')\nelse:\n    print('Probably dependent')","ae92e979":"#chi square test between occupation and is lead\nstat, p, dof, expected = chisq_of_df_cols(train, 'Credit_Product', 'Is_Lead')\n\n#determine whether dependent or independent based on the p-value\nprint('stat=%.3f, p=%.5f' % (stat, p))\nif p > 0.05:\n    print('Probably independent')\nelse:\n    print('Probably dependent')\n","c6891597":"#check the data\ntrain.head()","f3a87b38":"#define a seed to be used\nseed = 51\ntest_ratio = 0.2","c0a132e5":"#split into the train and validation test\nfrom sklearn.model_selection import train_test_split\n#split based on stratified on the Is_Lead to have similar representation in train and test data\ntrain_set,valid_set = train_test_split(train,test_size=test_ratio,random_state=seed,stratify=train['Is_Lead'])\n#print the shape of the split data\nprint(train_set.shape,valid_set.shape)","e7a9f29a":"#check the proportion split in the train set \ntrain_set['Is_Lead'].value_counts() \/ len(train_set)","a77e0374":"#check the proportion split in the valid set \nvalid_set['Is_Lead'].value_counts() \/ len(valid_set)","93092f46":"train_set.head(3)","2675720a":"#cut the Age feature into categorical bins \ntrain_set['Age_Category'] = pd.cut(train_set['Age'],bins=[20,40,60,80,90])\nvalid_set['Age_Category'] = pd.cut(valid_set['Age'],bins=[20,40,60,80,90])\ntest['Age_Category'] = pd.cut(test['Age'],bins=[20,40,60,80,90])","a57e6d42":"train_set.head(2)","4dacd54b":"#drop the redundant Age feature\ntrain_set.drop('Age',axis=1,inplace=True)\nvalid_set.drop('Age',axis=1,inplace=True)\ntest.drop('Age',axis=1,inplace=True)","1a9aa199":"train_set.info()","08704a1c":"#split the shuffled data into train and target features\nX_train = train_set.drop('Is_Lead',axis=1)\ny_train = train_set['Is_Lead']\n\nX_valid = valid_set.drop('Is_Lead',axis=1)\ny_valid = valid_set['Is_Lead']","2edbb6f7":"#remove the credit product from the cat features to prevent encoding null values \ncat_features = X_train.select_dtypes(include=['object','category']).columns.to_list()\n#cat_features.remove('Credit_Product')","b65516d9":"cat_features","2f43cf84":"#label encode the categorical features\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\n#encode all the cat features\nwarnings.filterwarnings(action='ignore',message='')\nfor col in cat_features:\n    X_train[col] = encoder.fit_transform(X_train[col])\n    X_valid[col] = encoder.transform(X_valid[col])\n    test[col] = encoder.transform(test[col])\nX_train.head()","8197de67":"test.head(2)","1f169730":"test['Gender'].unique()","ceacdebf":"from sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import StandardScaler\npt = PowerTransformer()\nscaler = StandardScaler()\n\n#apply the transformation\nX_train = pt.fit_transform(X_train)\nX_valid = pt.transform(X_valid)\ntest = pt.transform(test)\n#apply the scaling\nX_train = scaler.fit_transform(X_train)\nX_valid = scaler.transform(X_valid)\ntest = scaler.transform(test)","44183513":"#check shape of y_train\ny_train.shape","62dc1861":"#reshape the target array \ny_train = y_train.values\ny_valid = y_valid.values\ny_train = y_train.reshape(y_train.shape[0],1)\ny_valid = y_valid.reshape(y_valid.shape[0],1)\nprint(y_train.shape,y_valid.shape)","9c238d8a":"from sklearn.decomposition import PCA\nnum_components = [2,3,4,5,6]\n#check the total explained variance ratio using different number of principal components \nfor comp in num_components:\n    pca = PCA(n_components=comp)\n    X_train_pca = pca.fit_transform(X_train)\n    print('Total Explained Variance Ratio using {} components = {}'.format(comp,np.sum(pca.explained_variance_ratio_)))","0effbafe":"#apply the pca transformation\npca = PCA(n_components=6)\nX_train = pca.fit_transform(X_train)\nX_valid = pca.transform(X_valid)\ntest = pca.transform(test)","5d721040":"#evaluation metrics\nfrom sklearn.metrics import roc_auc_score","b9c0fdae":"#concatenate to recreate the entire training data for final training and prediction \nX =  np.vstack((X_train,X_valid))\ny =  np.vstack((y_train,y_valid))","ff460a81":"X.shape,y.shape","e65d4054":"#import the required tensorflow keras libraries\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\n#keras wrapper for scikit learn to perform cross validation or grid search\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier","81bd1b7b":"#define the model --> required for the Keras Classifier class \ndef create_model(optimizer='adam',init='glorot_uniform',dropout=0.0):\n    model = Sequential()\n    #add the layers\n    model.add(Dense(units=500,input_dim=X_train.shape[1],activation='relu',kernel_initializer=init))\n    model.add(Dense(units=300,activation='relu',kernel_initializer=init))\n    model.add(Dropout(dropout))\n    model.add(Dense(units=100,activation='relu',kernel_initializer=init))\n    model.add(Dropout(dropout))\n    model.add(Dense(units=50,activation='relu',kernel_initializer=init))\n    model.add(Dropout(dropout))\n    model.add(Dense(units=1,activation='sigmoid',kernel_initializer=init))\n    #compile the model\n    model.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n    #return the model\n    return model","e6f85cc4":"#define the early stop criteria\nearly_stop = EarlyStopping(monitor='val_loss',patience=10,restore_best_weights=True)\n#create the model\nmodel = KerasClassifier(build_fn=create_model,epochs=100,batch_size=16,verbose=0)","777008df":"model = create_model(optimizer='adam',dropout=0.25)\nhistory = model.fit(X_train,y_train,epochs=100,callbacks=[early_stop],validation_data=(X_valid,y_valid),\n          verbose=2)","b3632497":"valid_predictions = model.predict(X_valid)","5f4016f4":"model_score = model.evaluate(X_valid,y_valid,verbose=1)\nprint('%s: %.2f%% ' %(model.metrics_names[1],model_score[1]*100 ))","05eb3008":"#roc auc score on the validation set \nprint(roc_auc_score(y_valid,valid_predictions))","c0864956":"model.save('credit_lead_train_split.h5')","f2db67c2":"#train the final model on the entire dataset\nhistory = model.fit(X,y,epochs=50,verbose=2,validation_split=0.1,callbacks=[early_stop])","8123767e":"#predictions on the test dataset\ntest_predictions = model.predict(test)","ca168c14":"#create the submission file\ndf_test = pd.DataFrame(test_orig['ID'],columns=['ID'])\ndf_test['Is_Lead'] = test_predictions\ndf_test.head()\ndf_test.to_csv('credit_lead.csv',index=False)","392efb61":"#roc auc score on the validation set \nprint(roc_auc_score(y_valid,valid_predictions))","db948b32":"#save the final model trained on the entire training dataset\nmodel.save('credit_lead_train_full.h5')","2d9fa086":"_There is strong variation of the average account balance based on the region code_","5605ffaf":"#### _Label encode the categorical features_","b42904f1":"_Important to note that the missing values in Credit_Product got encoded to a different value. Instead of imputing with the most frequent value using simple imputer would not have been a reasonable choice unless it is advocated by the bank_\n\n_Another standby option is to use the <b> iterative imputer<\/b> which would first require the column to be changed to integer type and then iterative apply regression technique to impute the missing values._","defd6969":"# _Import Libraries and Load the Data_","50f72490":"_Following observations can be derived from the countplot_\n- _The proportion of leads for customers with a credit product is higher compared to those who do not have a credit product_\n- _The proportion of leads for active customers is higher compared to the non active customers_\n- _Channel code X3 and X2 have higher positive leads compared to the other channels_ \n- _The gender of the customer does not really matter in deciding who has a better lead_ \n- _Some of the region codes contribute to the maximum positive credit card leads_","3aef3e0b":"# _Credit Card Lead Prediction_\n***\nHappy Customer Bank is a mid-sized private bank that deals in all kinds of banking products, like Savings accounts, Current accounts, investment products, credit products, among other offerings.\n\nThe bank also cross-sells products to its existing customers and to do so they use different kinds of communication like tele-calling, e-mails, recommendations on net banking, mobile banking, etc. \n\nIn this case, the Happy Customer Bank wants to cross sell its credit cards to its existing customers. The bank has identified a set of customers that are eligible for taking these credit cards.\n\nNow, the bank is looking for your help in identifying customers that could show higher intent towards a recommended credit card, given:\n- Customer details (gender, age, region etc.)\n- Details of his\/her relationship with the bank (Channel_Code,Vintage, 'Avg_Asset_Value etc.)\n\n<b> Data Dictionary <\/b>\n\n- ID - Unique Identifier for a row\n\n- Gender - Gender of the Customer\n\n- Age - Age of the Customer (in Years)\n\n- Region_Code - Code of the Region for the customers\n\n- Occupation - Occupation Type for the customer\n\n- Channel_Code - Acquisition Channel Code for the Customer  (Encoded)\n\n- Vintage - Vintage for the Customer (In Months)\n\n- Credit_Product - If the Customer has any active credit product (Home loan,Personal loan, Credit Card etc.)\n\n- Avg_Account_Balance - Average Account Balance for the Customer in last 12 Months\n\n- Is_Active - If the Customer is Active in last 3 Months\n\n- Is_Lead(Target) - If the Customer is interested for the Credit Card\n\n0 : Customer is not interested\n\n1 : Customer is interested\n\n<b> Evaluation Metric: roc_auc <\/b>","252952b9":"_The kde plots on the diagonal are throwing some interesting insight on the data._\n- _The Age feature clearly has clusters of more rightly bins. One age group is younger between 20 to 40 years of age while the other is 40 to 60 years of Age. There are also customer 60 and 80 years of age but their numbers are not as high compared to the younger age groups.Secondly, those in the 40 to 60 age range have leads whereas the younger ones are not.Perhaps, this could be associated with the Vintage which would be more visible for the higher age group_\n\n- <b> _Vintage_ <\/b> _is a colloquial term used to describe mortgage-backed securities (MBS) that have been \"seasoned.\" That is, they've been issued long enough, and enough on-time payments have been made, that the risk of default is lower. Vintage is the age of an item as it relates to the year it was created._ _Lower vintages has more 1 credit card lead while the higher values have not. Could it be that the one who have higher vintage would be least interested to go for a credit card as they might have funds already after a number of on time payments have been made_\n\n- _The average account balance for both the leads 0 and 1 appears to be similar and currently appears not to be one of the main deciding feature to classify the credit card lead_\n\n- <font color = 'blue'> <b> _The kde plot on the diagonal for the Age and Vintage clearly indicates there are separate clusters in the data and separate models based on these clusters can also be a viable option for actual production_<\/b> <\/font>\n\n","b9efbf45":"_Rank-1 array can lead to serious bugs in modeling and production. Reshape it to proper 2D array with the second dimension as 1_","fc72b9ec":"#### _Chi-Square Test_","fd2dc2e5":"<b> _There are no duplicate customer ID's in the dataset_ <\/b>","2fcfae7d":"<b> _Is Lead is also dependent on the occupation of the customer. While this was apparent from the earlier countplots, the statistical test also proves it to be significant_ <\/b>","c55bb4e6":"<b> _The ID variable is not to be used for prediction. Hence it can be dropped. Secondly, the profile report and the info clearly reflects that there are no duplicate customer records in the dataset. However, just to be sure, lets convert the ID column to lower case and check if there are still duplicates_ <\/b>","6cf345ea":"<b> \nOne of the key challenge is to deal with the missing values in the credit_product feature.The missing value is roughly 11% of the entire train dataset size.\n\nOptions that can be explored\n- _drop all the rows with the missing values_\n- _impute with the mode or most frequent values. this won't be a wise choice considering that the feature is boolean and can have a strong impact on the model performance. Also, the choice of imputation has to tally well with the stakeholders going to make use of this model. Hence without consulting with the business, this could be a dangerous choice_\n- _Impute using KNNImpute method, would be safer than other options but again will not be fool proof_\n- _Predict the class of the credit product using all of the other labels except the Is_Lead as we would also need to do so for the test dataset_\n- _Lastly, have a separate label for the missing values. This can be handled during the label encoding_\n\n\n\n<\/b>","f93c440c":"_The average account balance as expected is right skewed_","db7dff3f":"## _Deep Learning Model_","0089607f":"<b> _Customer credit card lead and region code are dependent according to the chi-square test_ <\/b>","9ef46d98":"### _Generate Pandas Profile Report_","deb16cd2":"## _Data Preparation_","95555793":"_The split is the train and valid set is similar with respect to the target class_","175ac4d1":"_The median account balance of the customer is higher for the customer who are positive leads for a credit card_","d3f765f0":"#### _Test whether the customer lead and the region code related_","207f0144":"- _There are quite a lot of customers who are not active in the last 3 months compared to those who are active_\n- _There are much more customers without any credit product compared to the ones that have it_\n- _Self employed customers followed by salaried form the majority_\n","db2c26ea":"## _Load the data_","a87b75df":"<b>_Credit product_<\/b> _feature has a lot of missing values. As per the meta data, this feature reflects any active credit product like home loan, personal loan or credit card that the customer already has. The feature is boolean and it is difficult to make a choice for these customers where the data is missing._","5d401f47":"### _Dimensionality Reduction_\n_So far all of the features are included and no feature engineering steps are performed specifically. While dimensionality reduction using PCA would help to remove the redundant feature, the performance of the model cannot be guranteed to be improved._","e0b7f36c":"## _Exploratory Data Analysis_","7d76bd21":"### _Power Transformation & Scaling_\n","2e127ed2":"_Except for the Credit_Product feature, all other features and target variables do not have null values. For a faster exploration of all the features, pandas profile report would be helpful._","bd899da8":"_The number of leads is roughly 31% of the customer without credit card lead. The data is imbalanced as expected. The split of the data can be based on stratification methodology to have similar representation in the train and validation split_","13c56d89":"### _Split the dataset_","a6eaf3aa":"_Age and Vintage have high correlation while it cannot be said for the other features_"}}