{"cell_type":{"c9d6242d":"code","b9b79691":"code","5d67b313":"code","c59f87c0":"code","a3c5bfc4":"code","1c5be937":"code","3a966bfd":"code","251fabf0":"code","18f226e3":"code","f6ea328e":"code","17eefd75":"code","b9f4d060":"code","6d86b595":"code","69c97524":"code","4606c8fe":"code","5588fffa":"code","8f27f55e":"code","3f82fd22":"code","320ff0ef":"code","bf739882":"markdown","c389dd16":"markdown","c2bd4a76":"markdown","4ac317a3":"markdown","3cfa5cae":"markdown","60a18519":"markdown","822612d1":"markdown","0cb42346":"markdown","c0e975f8":"markdown","e3cae369":"markdown","77070857":"markdown","ef6a819d":"markdown"},"source":{"c9d6242d":"#Loading Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import RobustScaler\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,classification_report,confusion_matrix,roc_curve,auc\n\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.over_sampling import SMOTE\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","b9b79691":"# loading data\ndf = pd.read_csv('..\/input\/creditcard.csv')\n","5d67b313":"## Analyse the Data\n#  Dispcriptive statistics\ndf.shape       # shape - gives the total number of rows and columns\n                      # it has 284807 rows and 31 columns","c59f87c0":"df.head()            # head () function - gives the top 5 rows\n                     # it has 'Time' 'Amount' 'Class' and 28Variables(for security reasons actuall names are hidden and represented as V1,V2..etc.,)\n                     # from the Table data identify 'Features(input)' and 'Labels(output)'\n                     # As per the Data we Decide 'Class' is Our Label\/output\n                         # Class = 0 --No Fraud\n                         # Class = 1 -- Fraud\n                     # remaining all Columns we are taking as our 'Features(inputs)'\n                     # check for CATOGORICAL Values , if there are any Catogorical Values convert it into \"numerical format\"\n                     # as ML understands only Numerical format data","a3c5bfc4":"# checking datatypes   \ndf.info()           # all the features are of float datatype except the 'Class' which is of int type","1c5be937":"# check for missing values in each feature and label\ndf.isnull().sum()       # missing values represented by 1 or more than 1\n                           # no missing values represented by  0\n                           # here there are no missing values","3a966bfd":"# statistical summary of the data \n# mean,standard deviation,count etc.,\ndf.describe()","251fabf0":"# Check the Class distribution of 'Output Class'\n# to identify whether our data is 'Balanced' or 'Imbalanced'\n\nprint(df['Class'].value_counts() )      # 0 - NonFraud Class\n                                        # 1 - Fraud Class\n\n# to get in percentage use 'normalize = True'\nprint('\\nNoFrauds = 0 | Frauds = 1\\n')\nprint(df['Class'].value_counts(normalize = True)*100)","18f226e3":"# visualizing throug bar graph\ndf['Class'].value_counts().plot(kind = 'bar', title = 'Class Distribution\\nNoFrauds = 0 | Frauds = 1'); # semicolon(;) to avoid '<matplotlib.axes._subplots.AxesSubplot at 0xe81c4b518>' in output","f6ea328e":"# No Missing Data, No Duplicates\n# No Feature Selection as the feature names are hidden for security reasons\n# \n# As the Data is PCA transformed we assume that the variables v1 - v28 are scaled \n# we scale leftout 'Time' and 'Amount' features\n\n\n#visualizing through density plots using seaborn\nimport seaborn as sns\nfig, (ax1, ax2,ax3) = plt.subplots(ncols=3, figsize=(20, 5))\n\nax1.set_title(' Variable V1-V28\\nAssuming as Scaled')  # plotting only few variables\nsns.kdeplot(df['V1'], ax=ax1)                          # kde - kernel density estimate\nsns.kdeplot(df['V2'], ax=ax1)\nsns.kdeplot(df['V3'], ax=ax1)\nsns.kdeplot(df['V25'], ax=ax1)\nsns.kdeplot(df['V28'], ax=ax1)\n\nax2.set_title('Time Before Scaling')\nsns.kdeplot(df['Time'], ax=ax2)\n\nax3.set_title('Amount Before Scaling')            \nsns.kdeplot(df['Amount'], ax=ax3)\n\nplt.show()","17eefd75":"#Scaling data using RobustScaler\nfrom sklearn.preprocessing import StandardScaler,RobustScaler\nrb = RobustScaler()\ndf['Time'] = rb.fit_transform(df['Time'].values.reshape(-1,1))\ndf['Amount'] = rb.fit_transform(df['Amount'].values.reshape(-1,1))\ndf.head()","b9f4d060":"# lets Analyse why the accuracy is misleading(high) \n\nx = df.drop('Class',axis = 1)\ny = df['Class']\n\n#train and test split\nxTrain,xTest,yTrain,yTest = train_test_split(x,y,test_size = 0.3,random_state = 42)\n\n# spot check algorithms\nclassifiers = {\"Logistic Regression\":LogisticRegression(),\n               \"DecisionTree\":DecisionTreeClassifier(),\n               \"LDA\":LinearDiscriminantAnalysis()}        \n# as the dataset is too big computation time will be high\n# bcoz of which iam using only 3 classifiers\n\nfor name,clf in classifiers.items():\n    accuracy      = cross_val_score(clf,xTrain,yTrain,scoring='accuracy',cv = 5)\n    accuracyTest  = cross_val_score(clf,xTest,yTest,scoring='accuracy',cv = 5)\n    \n    precision     = cross_val_score(clf,xTrain,yTrain,scoring='precision',cv = 5)\n    precisionTest = cross_val_score(clf,xTest,yTest,scoring='precision',cv = 5)\n    \n    recall        = cross_val_score(clf,xTrain,yTrain,scoring='recall',cv= 5)\n    recallTest    = cross_val_score(clf,xTest,yTest,scoring='recall',cv = 5)\n    \n    print(name,'---','Train-Accuracy :%0.2f%%'%(accuracy.mean()*100),\n                     'Train-Precision: %0.2f%%'%(precision.mean()*100),\n                     'Train-Recall   : %0.2f%%'%(recall.mean()*100))\n    \n    print(name,'---','Test-Accuracy :%0.2f%%'%(accuracyTest.mean()*100),\n                     'Test-Precision: %0.2f%%'%(precisionTest.mean()*100),\n                     'Test-Recall   : %0.2f%%'%(recallTest.mean()*100),'\\n')\n","6d86b595":"# 1. split the 'Original Train data ' into train & test\n# 2. Oversample or UnderSample the splitted train data\n# 3. fit the model with Oversample or Undersampled train data\n# 4. perform 'prediction' on Oversample or Undersampled train data\n# 5. Finally perform 'prediction' on Original TEST Data\n\n#step 1\nxTrain_rus,xTest_rus,yTrain_rus,yTest_rus = train_test_split(xTrain,yTrain,test_size = 0.2,random_state = 42)\n\n#step 2\nrus = RandomUnderSampler()\nx_rus,y_rus = rus.fit_sample(xTrain_rus,yTrain_rus)\n\n#converting it to DataFrame to Visualize in pandas\ndf_x_rus = pd.DataFrame(x_rus)\ndf_x_rus['target'] = y_rus\nprint(df_x_rus['target'].value_counts())\nprint(df_x_rus['target'].value_counts().plot(kind = 'bar',title = 'RandomUnderSampling\\nFrauds = 1 | NoFrauds = 0'))\n\n","69c97524":"#step 3\nlr = LogisticRegression()\nlr.fit(x_rus,y_rus)\n\n#step 4\nyPred_rus = lr.predict(xTest_rus)\n\nrus_accuracy = accuracy_score(yTest_rus,yPred_rus)\nrus_classReport = classification_report(yTest_rus,yPred_rus)\n#print('\\nTrain-Accuracy %0.2f%%'%(rus_accuracy*100),\n#      '\\nTrain-ClassificationReport:\\n',rus_classReport,'\\n')\n\n#step 5\nyPred_actual = lr.predict(xTest)\ntest_accuracy = accuracy_score(yTest,yPred_actual)\ntest_classReport = classification_report(yTest,yPred_actual)\nprint('\\nTest-Accuracy %0.2f%%'%(test_accuracy*100),\n      '\\n\\nTest-ClassificationReport:\\n',test_classReport)\n","4606c8fe":"#step 1\nxTrain_ros,xTest_ros,yTrain_ros,yTest_ros = train_test_split(xTrain,yTrain,test_size=0.2,random_state=42)\n\n#step 2\nros = RandomOverSampler()\nx_ros,y_ros = ros.fit_sample(xTrain_ros,yTrain_ros)\n\n#Converting it to dataframe to visualize in pandas\ndf_x_ros = pd.DataFrame(x_ros)\ndf_x_ros['target'] = y_ros\nprint(df_x_ros['target'].value_counts())\nprint(df_x_ros['target'].value_counts().plot(kind = 'bar',title = 'RandomOverSampling\\nFrauds = 0 | NoFrauds = 1'))\n","5588fffa":"#step 3\nlr = LogisticRegression()\nlr.fit(x_ros,y_ros)\n\n#step 4\nyPred_ros = lr.predict(xTest_ros)\n\nros_accuracy = accuracy_score(yTest_ros,yPred_ros)\nros_classReport = classification_report(yTest_ros,yPred_ros)\nprint('\\nTrain-Accuracy %0.2f%%'%(rus_accuracy*100),\n      '\\nTrain-ClassificationReport:\\n',rus_classReport,'\\n')\n\n#step 5\nyPred_actual = lr.predict(xTest)\ntest_accuracy = accuracy_score(yTest,yPred_actual)\ntest_classReport = classification_report(yTest,yPred_actual)\nprint('\\nTest-Accuracy %0.2f%%'%(test_accuracy*100),\n      '\\n\\nTest-ClassificationReport:\\n',test_classReport)","8f27f55e":"#step 1\nxTrain_smote,xTest_smote,yTrain_smote,yTest_smote = train_test_split(xTrain,yTrain,test_size = 0.2,random_state = 42 )\n\n#step2\nsmote = SMOTE()\nx_smote,y_smote = smote.fit_sample(xTrain_smote,yTrain_smote)\n#Converting it to dataframe to visualize in pandas\ndf_x_smote = pd.DataFrame(x_smote)\ndf_x_smote['target'] = y_smote\nprint(df_x_smote['target'].value_counts())\nprint(df_x_smote['target'].value_counts().plot(kind = 'bar',title = 'SMOTE\\nFrauds = 0 | NoFrauds = 1'))\n\n","3f82fd22":"rfc = RandomForestClassifier(random_state = 42)\nrfc.fit(x_smote,y_smote)\nypred_smote = rfc.predict(xTest_smote)\n\nrfc_prediction=rfc.predict(xTest)\nprint('RFC-Accuracy',accuracy_score(yTest,rfc_prediction),'\\n')\nprint('Confusion_Matrix:\\n',confusion_matrix(yTest,rfc_prediction),'\\n')\nprint('Classification Report',classification_report(yTest,rfc_prediction))","320ff0ef":"#auc score\nrfc_fpr,rfc_tpr,_ = roc_curve(yTest,rfc_prediction)\nrfc_auc = auc(rfc_fpr,rfc_tpr)\nprint('RandomForestClassifier-auc : %0.2f%%'%(rfc_auc * 100))\n\n#roc curve\nplt.figure()\nplt.plot(rfc_fpr,rfc_tpr,label ='RFC(auc = %0.2f%%)'%(rfc_auc *100))\nplt.plot([0,1],[0,1],'k--')\nplt.legend()\nplt.title('Smote with RandomForestClassifier\\nROC Curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","bf739882":"##### Conclusion : \nwith almost all classifiers the  accuracy is around 99.9%\nbut there is a change in precision and recall score\n\nso the Accuracy Metric when working with imbalance datasets are misleading (very high)\n","c389dd16":"#### 1.Loading Libraries and Data","c2bd4a76":"From Above analysis we found our dataset is 'Imbalanced'\nNon Frauds = 99.82%\nFrauds = 0.17%\n\nMost of the Transactions are 'Non Fraud' Transactions.If the event to be predicted is from the Minority Class ( here Minority Class is detecting Fraud Cases ) and the event rate is less than 5% it is usually referred as 'rare event'\n\nso this dataset needs to be balanced\n\nIf we apply ML Algorithms on this Dataset before Balancing it,ML Algorithms probably 'overfit' it assumes all the Transactions as 'Non Frauds'\n\n Over Fitting  : Good performance on TRAIN DATA , Bad Performance on TEST DATA\/UNSEEN DATA\n Under Fitting : Bad Performance on Both TRAIN & TEST DATA","4ac317a3":"## Credit Card Fraud Detection\nThis is an Imbalance Dataset.Resampling is a common practice to address the imbalanced dataset issue. Although\nthere are many techniques within resampling, here i\u2019ll be using the three most popular techniques.\n#### 1.Random Under Sampling\nRandomly eleminates Majority class instances\/records until it balance with the Minority class \nDisadvantage: As it eleminates randomly there is a possibility of elemination of USEFUL DATA thus making the algorithm predicts inaccurately\n#### 2.RandomOverSampling\nRandomly replicates the Minority Class to increase its frequency (in numbers to match Majority class) in this technique there is 'no loss of information' but possibility of 'overfitting' the model since it is replicating the data\n#### 3.Synthetic Minority Oversampling TEchnique ( SMOTE)\nIncrease minority class by introducing synthetic examples  through  connecting all k (default = 5) minority class nearest neighbors using feature space similarity (Euclidean distance).\n\n### Outline:\n1. Loading Libraries and data\n2. Summarizing and Visualizing Data\n3. Preparing data :     \ni) Data Cleaning : By removing duplicates, marking missing values and even imputing missing values  \nii)   Feature Selection : redundant features may be removed and new features developed.\niii)  Data Transform : attributes are scaled or redistributed in order to best expose the structure of the problem later to learning algorithms\n4. Why Accuracy is not a Good Performance Metric when dealing with Imbalance Dataset\n5. Right Way of Resampling\n6. Resampling imbalance dataset through Random Under Sampler\n7. Random Over Sampling  \n8. SMOTE sampling\n9. Smote with Random Forest Classifier  \n10.Smote - Confusion_Matrix , ROC_AUC curve , classification_report\n\n#### References:\n1.Mastering Machine Learning - Manohar Swaminathan  \n2.Machine Learning Mastery -jason brownlee   \n3.Right way of sampling - nick becker blog\n        \n        \n        ","3cfa5cae":"#### 2. summarizing and visualizing data","60a18519":"#### 7. Random Over Sampling","822612d1":"#### SMOTE ","0cb42346":"####  4. Why Accuracy Metric is Misleading When Dealing with Imbalanced Datasets ?","c0e975f8":"#### 3.preparing Data","e3cae369":"#### Data Visualization","77070857":"#### 5. Right way of Resampling\n 1. split the 'Original Train data ' into train & test\n 2. oveSample or underSample the splitted train data\n 3. fit the model with upsample or downsampled train data\n 4. perform 'prediction' on upsample or downsampled train data\n 5. Finally perform 'prediction' on 'Original Test Data'","ef6a819d":"#### 6. Random Under Sampling"}}