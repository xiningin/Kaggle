{"cell_type":{"f6147206":"code","b692b597":"code","c7f746a4":"code","27041a2e":"code","9c6d43b6":"code","81a8a3d1":"code","5d6ef498":"code","1d2aae30":"code","4ff18a57":"code","35d0d31b":"code","d8993a3b":"code","e8c9f4be":"code","16e2fc43":"code","c3a52082":"code","89c1c492":"markdown","4e004086":"markdown","ed760247":"markdown","ccf1435f":"markdown","26dfe5cf":"markdown","d6d2a758":"markdown","f5d5a01d":"markdown"},"source":{"f6147206":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport math\n\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom sklearn.metrics import log_loss\n\nimport gc\nimport warnings\nwarnings.simplefilter('ignore')","b692b597":"trainF = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntest  = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\ntrainTs = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrainTn = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\nsub = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","c7f746a4":"display(trainF.describe())\ndisplay(trainTs.describe())","27041a2e":"display(trainF.head())\ndisplay(trainTs.sample(6))\ndisplay(test.head())\nsub.head()","9c6d43b6":"print('total missing values in dataset = ', trainF.isna().sum().sum())\n#categorical features\ncat_feat = trainF.columns[trainF.dtypes == 'object'].tolist()\nprint(cat_feat)\ntrain = trainF.merge(trainTs, on= 'sig_id')\ntrain.shape, trainTs.shape","81a8a3d1":"target_cols = [col for col in trainTs.columns if col != 'sig_id']\nc_feats = ['cp_type', 'cp_time', 'cp_dose']\nfor feat in c_feats:\n    col = target_cols + [feat]\n    c_sumTs = train[col].groupby([feat]).sum().sum(1)\n    sns.countplot(c_sumTs) ;\n    sns.barplot(c_sumTs.index, c_sumTs.values) ;\n    plt.show()","5d6ef498":"train[col+['cp_type']].groupby('cp_type').sum().sum(1)","1d2aae30":"def cat2num(df):\n    df.loc[:, 'cp_time'] = df['cp_time'].map({24: 0, 48: 1, 72: 2})\n    df.loc[:, 'cp_type'] = df['cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_dose'] = df['cp_dose'].map({'D1':0, 'D2':1})\n    return df\ntrain = cat2num(train)\ntest = cat2num(test)\n\nprint('Number of different labels:', len(target_cols))\nnum_feat = [x for x in train.columns if x not in trainTs]","4ff18a57":"df = pd.concat([train[num_feat], test[num_feat]], axis= 0)\n\nfeatures_g = list(train.columns[4:776])\nfeatures_c = list(train.columns[776:876])\ngc_fe = ['g_sum', 'g_mean', 'g_std', 'g_kurt', 'g_skew', 'c_sum', 'c_mean', 'c_std', \n         'c_kurt', 'c_skew', 'gc_sum', 'gc_mean', 'gc_std', 'gc_kurt', 'gc_skew']\n\ndf['g_sum'] = df[features_g].sum(axis = 1)\ndf['g_mean'] = df[features_g].mean(axis = 1)\ndf['g_std'] = df[features_g].std(axis = 1)\ndf['g_kurt'] = df[features_g].kurtosis(axis = 1)\ndf['g_skew'] = df[features_g].skew(axis = 1)\ndf['c_sum'] = df[features_c].sum(axis = 1)\ndf['c_mean'] = df[features_c].mean(axis = 1)\ndf['c_std'] = df[features_c].std(axis = 1)\ndf['c_kurt'] = df[features_c].kurtosis(axis = 1)\ndf['c_skew'] = df[features_c].skew(axis = 1)\ndf['gc_sum'] = df[features_g + features_c].sum(axis = 1)\ndf['gc_mean'] = df[features_g + features_c].mean(axis = 1)\ndf['gc_std'] = df[features_g + features_c].std(axis = 1)\ndf['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\ndf['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n\ntrain[gc_fe] = df[gc_fe].iloc[:train.shape[0],:]\ntest[gc_fe] = df[gc_fe].iloc[train.shape[0]:, :]\nnum_feat = num_feat + gc_fe","35d0d31b":"from sklearn.multioutput import MultiOutputClassifier\nfrom xgboost import XGBClassifier","d8993a3b":"params = {'colsample_bytree': 0.6522,\n          'gamma': 3.6975,\n          'learning_rate': 0.0503,\n          'max_delta_step': 2.0706,\n          'max_depth': 10,\n          'min_child_weight': 31.5800,\n          'n_estimators': 166,\n          'subsample': 0.8639,\n          'verbosity':0\n         }\n\nclf = MultiOutputClassifier(XGBClassifier(**params, tree_method='gpu_hist'))\n\nNFOLDS = 5\nX = train[num_feat].values \ny = train[target_cols].values\nX_test = test[num_feat].values","e8c9f4be":"oof_preds = np.zeros(y.shape)\ntest_preds = np.zeros((test.shape[0], y.shape[1]))\noof_losses = []\nkf = KFold(n_splits=NFOLDS)\nfor fn, (trn_idx, val_idx) in enumerate(kf.split(X, y)):\n    print('Starting fold: ', fn)\n    X_train, X_val = X[trn_idx], X[val_idx]\n    y_train, y_val = y[trn_idx], y[val_idx]\n    \n    # drop where cp_type==ctl_vehicle (baseline)\n    ctl_mask = X_train[:,0]== 1 #'ctl_vehicle'\n    X_train = X_train[~ctl_mask,:]\n    y_train = y_train[~ctl_mask,:]\n    \n    clf.fit(X_train, y_train)\n    val_preds = clf.predict_proba(X_val) # list of preds per class\n    val_preds = np.array(val_preds)[:,:,1].T # take the positive class\n    oof_preds[val_idx] = val_preds\n    \n    loss = log_loss(np.ravel(y_val), np.ravel(val_preds))\n    print(f'fold {fn} loss {loss}')\n    oof_losses.append(loss)\n    preds = clf.predict_proba(X_test)\n    preds = np.array(preds)[:,:,1].T # take the positive class\n    test_preds += preds \/ NFOLDS\n    \nprint('Mean OOF loss across folds', np.mean(oof_losses))\nprint('STD OOF loss across folds', np.std(oof_losses))","16e2fc43":"control_mask = test['cp_type'] == 1\nsub.iloc[:, 1:] = test_preds\nsub.iloc[control_mask, 1:] = 0\nsub.to_csv('submission.csv', index = False)","c3a52082":"sub.head()","89c1c492":"## About this Competition\n\nscientists seek to identify a protein target associated with a disease and develop a molecule that can modulate that protein target. As a shorthand to describe the biological activity of a given molecule, scientists assign a label referred to as mechanism-of-action or MoA for short.\n\nHence, our task is to use the training dataset to develop an algorithm that automatically labels each case in the test set as one or more MoA classes. Note that since drugs can have multiple MoA annotations, the task is formally a multi-label classification problem.\n\nBased on the MoA annotations, the accuracy of solutions will be evaluated on the average value of the logarithmic loss function applied to each drug-MoA annotation pair.","4e004086":"***train_features.csv*** \/ ***test_features.csv*** -Features for the training set. \n<br>Features g- signify gene expression data, and \nc- signify cell viability data. \ncp_type indicates samples treated with a compound (cp_vehicle) or with a control perturbation (ctrl_vehicle); control perturbations have no MoAs; \ncp_time and cp_dose indicate treatment duration (24, 48, 72 hours) and dose (high or low).\n<br>***train_targets_scored.csv*** - The binary MoA targets that are scored.\n<br>***sample_submission.csv*** - A submission file in the correct format","ed760247":"## Feature engineering","ccf1435f":"## Analysing cp- features","26dfe5cf":"I would be grateful for any correction, suggestion or discussion ):","d6d2a758":"## References\n*  https:\/\/www.kaggle.com\/fchmiel\/xgboost-baseline-multilabel-classification\n*  https:\/\/www.kaggle.com\/kushal1506\/moa-pytorch-feature-engineering-0-01846\n","f5d5a01d":"## XGBClassifier"}}