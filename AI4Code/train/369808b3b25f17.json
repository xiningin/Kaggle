{"cell_type":{"d5a9e33d":"code","6cb8c330":"code","eac0d924":"code","9d9d65e2":"code","2070cfdc":"code","94fc133f":"code","6b768a00":"code","b4633428":"code","646d55da":"code","dc77af6b":"code","f222f14c":"code","e776562d":"code","7940a494":"code","66b3174d":"code","552985ef":"code","5e87968b":"code","e8e2faaa":"code","3116b8a8":"code","636f3510":"code","fa479d3e":"code","a013fef0":"code","95fbc451":"code","93fa0b0e":"code","1ea41ddc":"code","60518b49":"code","e5fe0940":"code","ae035296":"code","2fa538c4":"code","6a222df2":"code","03f62623":"code","61243e77":"code","12b1511c":"code","ee79a620":"code","22a6bbd3":"code","065f75b8":"code","5b0060b4":"code","f9cd7889":"code","7d5be909":"code","d1fc4b05":"code","9250d922":"code","e369030d":"code","dbc12514":"code","eeef937f":"markdown","c37ca250":"markdown","328aadd4":"markdown","0bef3176":"markdown","6feb6bad":"markdown","b9fbd09d":"markdown","e64bd8f6":"markdown","69101a72":"markdown","2d672338":"markdown","c5a129b1":"markdown","bd7f2a1a":"markdown","3ed356fe":"markdown","c3c66ea5":"markdown","72396640":"markdown","b1d61479":"markdown","e6be46d7":"markdown","058a16cc":"markdown","bc9b0a1a":"markdown","6b276be7":"markdown","f30056fc":"markdown","f298e3f6":"markdown","977b9505":"markdown","774b8f9d":"markdown","565fccbe":"markdown","7e2078de":"markdown","ea47bd43":"markdown"},"source":{"d5a9e33d":"import numpy as np # linear algebra\nimport matplotlib.pyplot as plt # plotting\nimport seaborn as sns # data visualization\nfrom collections import Counter\nimport os\nimport csv\nfrom gensim.models import Word2Vec\nfrom sklearn.manifold import TSNE","6cb8c330":"import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\ndf = pd.read_csv('\/kaggle\/input\/wordgame\/wordgame_20180814.csv') \npal = sns.color_palette(\"Paired\", 10) # color palette\ndf.sample(5, random_state=24)","eac0d924":"len(df)","9d9d65e2":"sources_names = [\"AS\", \"BC\", \"CC\",\"ECF\",\"GOG\",\"LEF\",\"SAS\",\"TF\",\"U2\",\"WP\"] \n#plt.figure(figsize=(16, 4))\n\nsns.set(rc={'figure.figsize':(16,4)})\nax = sns.barplot(df.source.value_counts().sort_index(), sources_names, orient=\"h\", palette=pal)\nax.set_title(\"Sources\")\nax.set_xlabel('Word association count')\nplt.show()","2070cfdc":"len(df.author.unique())","94fc133f":"plt.figure(figsize=(16, 4))\nmedian_wa = df.author.value_counts().median()\nmean_wa = df.author.value_counts().mean()\nplt.axvline(median_wa, color='k', linestyle='dashed', linewidth=1)\nplt.text(median_wa+30, 1, \"median: \"+str(int(median_wa)))\nplt.axvline(mean_wa, color='k', linestyle='dashed', linewidth=1)\nplt.text(mean_wa+30, 3, \"mean: \"+str(int(mean_wa)))\ng= plt.hist(df.author.value_counts(), bins=50, color=\"orange\")\nax = plt.gca()\n#ax.set_xscale('log')\nax.set_yscale('log')\nplt.title('Distribution of number of word associations per user')\nplt.xlabel('Word assocation count per user')\nplt.ylabel('Frequency (logaritmic)')\nplt.gca()\nplt.xlim([1, 12000])\nplt.show()","6b768a00":"def preprocess(x):\n    # replace\n    x = x.replace('\\n',\" \") # (wrongplanet)\n    x = x.replace(\"`\", \"'\")\n    x = x.replace(\"$$\", \"$\") #allows $-signs but not multiple\n\n    #remove \n    for symbol in [\"_\",\"~\",\"^\",\"xd\"]:\n        x = x.replace(symbol, \"\")\n        \n    if(x.find(\"said:\") > 1):  #classic comics: removes posts containing quotes\n        return ''\n    \n    # remove everything following ...\n    for symbol in [\"quote:\",\"sent from my\",\"edited by\",\"posted via\",\n                   \"\/\",\"(\",\",\",\"*\",\"\\u00a0\",\"--\",\"*\",'\"',\".\",\"!\",\"?\",\"=\",\"[\",\"{\",\":\",\";\",\">\",\"<\"]:\n        x = x.split(symbol)[0]\n\n    #remove leading characters\n    for symbol in [\"+\",\"-\",\"&\",\"'\",\" \"]:\n        x = x.lstrip(symbol)\n\n    #remove trailing characters\n    for symbol in [\"-m\", \"'\", \" \", '\\u00a0']:\n        x = x.rstrip(symbol)\n    return x\n\n# convert word columns to string\ndf['word1'] = df['word1'].astype(str) \ndf['word2'] = df['word2'].astype(str)\n\n# delete all words containing non-ascii characters\ndf['word1'] = df['word1'].apply(lambda x: bytes(x, 'utf-8').decode('ascii','ignore'))\ndf['word2'] = df['word2'].apply(lambda x: bytes(x, 'utf-8').decode('ascii','ignore'))\n\n#convert all to lowercase\ndf['word1'] = df['word1'].apply(lambda x: x.lower())\ndf['word2'] = df['word2'].apply(lambda x: x.lower())\n\n# clean data\ndf['word1'] = df['word1'].apply(lambda x: preprocess(x))\ndf['word2'] = df['word2'].apply(lambda x: preprocess(x))\n\n# replace empty word with np.NaN\ndf = df.replace('',np.NaN)\n\n# drop all pairs containing NaN values\ndf = df.dropna(axis=0, how='any').reset_index(drop=True)\n\n# cut off all data longer than 25 characters, since words are rarely longer \ndf['word1'] = df['word1'].apply(lambda x: x[:25] if len(x)>25 else x)\ndf['word2'] = df['word2'].apply(lambda x: x[:25] if len(x)>25 else x)\n\n# remove pairs with identical words\ndf = df[df.word1 != df.word2]","b4633428":"from wordcloud import WordCloud\n\nlong_string = ','.join(list(df['word2'].values))\n\nwordcloud = WordCloud(background_color=\"honeydew\", width=1000, height=300, max_words=5000, random_state=24)\nwordcloud.generate(long_string)\nwordcloud.to_image()","646d55da":"## save cleaned\/preprocessed data\ndf.to_csv(\"\/kaggle\/working\/wordgame_clean.csv\", sep=',', index=False)","dc77af6b":"len(df.drop_duplicates(['word2']))","f222f14c":"# compute term frequency (TF) feature\nnum_terms = len(df)\ndf['tf'] = df.groupby(['word2'])['word2'].transform('count')\ndf['rtf'] = df.tf.apply(lambda x: x*100\/num_terms) # relative term frequency (percentage!)\nfrequent_words = df.drop_duplicates(subset=['word2']).sort_values(by=['tf'], ascending=False).head(20)\nfrequent_words = frequent_words[::-1] # reverse \n\nfig, ax = plt.subplots(figsize=(16, 4))\nax.clear()\nax.barh(frequent_words.word2, frequent_words.rtf, color=\"orange\", alpha=0.4)\nfor i in range(0,len(frequent_words)):\n    ax.text(frequent_words.rtf.iloc[i], i, frequent_words.tf.iloc[i], ha='right', va='center')\n    ax.text(0.001, i, frequent_words.word2.iloc[i], ha='left', va='center') \nax.set_yticks([])\nplt.title(\"Most frequent words\")\nplt.xlabel('Relative term frequency')\nplt.ylabel('')\ntestplt = plt.show()","e776562d":"counts = np.array(df.word1.value_counts())\ntokens = np.array(df.word1.value_counts().index)\nranks = np.arange(1, len(counts)+1)\nindices = np.argsort(-counts)\nfrequencies = counts[indices]\n\nplt.figure(figsize=(10, 6))\n#plt.plot([1, 10e2], [10e2, 1], color='grey', linestyle='--', linewidth=1) # adds a diagonal line to the plot\nplt.loglog(ranks, frequencies, marker=\".\", alpha=0.4, c=\"orange\")\nfor n in list(np.logspace(-0.5, np.log10(len(counts)), 25).astype(int))[:-1]:\n    dummy = plt.text(ranks[n], frequencies[n], \" \" + tokens[indices[n]], verticalalignment=\"bottom\", \n                     horizontalalignment=\"left\")\nplt.title(\"Zipfs plot\")\nplt.xlabel(\"Frequency rank\")\nplt.ylabel(\"Absolute frequency\")\nplt.xlim(1, 10e5)\nplt.ylim(1, 10e2)\nplt.grid(True)\nplt.show()","7940a494":"df['pair'] = df.apply(lambda x: str(x.word1) + \":\" + str(x.word2), axis=1)\ndf['pf'] = df.groupby(['pair'])['pair'].transform('count')\ndf['rpf'] = df.pf.apply(lambda x: x*100\/num_terms) # ","66b3174d":"frequent_pairs = df.drop_duplicates(subset=['pair']).sort_values(by=['pf'], ascending=False).head(20)\nfrequent_pairs = frequent_pairs[::-1]\n\nfig, ax = plt.subplots(figsize=(16, 4))\nax.clear()\nax.barh(frequent_pairs.word2, frequent_pairs.rpf, color=\"orange\", alpha=0.4)\nfor i in range(0,len(frequent_pairs)):\n    ax.text(frequent_pairs.rpf.iloc[i], i, frequent_pairs.pf.iloc[i], ha='right', va='center')\n    ax.text(0, i, frequent_pairs.pair.iloc[i], ha='left', va='center') \nax.set_yticks([])\nplt.title(\"Most frequent word associations\")\nplt.xlabel('Relative term frequency')\nplt.ylabel('')\nplt.show()","552985ef":"df['len1'] = df['word1'].apply(lambda x:len(x))\ndf['len2'] = df['word2'].apply(lambda x:len(x))\ndf['ldiff'] = abs(df['len1'] - df['len2']) # length difference between word1 and word2 \n\nplt.figure(figsize=(16, 4))\nf = plt.hist(df.len2, bins=np.arange(0,25), color='orange') \nplt.axvline(df.len2.mean(), color='k', linestyle='dashed', linewidth=1)\nplt.text(df.len2.mean()+0.1, 1000, str(round(df.len2.mean(),2)))\nplt.title('Word length distribution')\nplt.xlabel('Word length')\nplt.ylabel('Frequency')\naxes = plt.gca()\naxes.set_xlim([0,27])\nplt.show()","5e87968b":"# find common prefix\ndf['prefix'] = df.apply(lambda r: os.path.commonprefix([r.word1, r.word2]), axis=1)\ndf['pl']= (df['prefix'].apply(lambda x: len(x))) # compute common prefix length\n\n# find common suffix\ndf['suffix'] = df.apply(lambda r: os.path.commonprefix([r.word1[::-1], r.word2[::-1]]), axis=1)\ndf['suffix'] = df['suffix'].apply(lambda x:x[::-1]) #un-reverse suffix\ndf['sl']= (df['suffix'].apply(lambda x: len(x))) # compute common affix length","e8e2faaa":"frequent_prefixes = pd.DataFrame(df.prefix.value_counts()[1::].head(20))\nfrequent_prefixes['pc'] = frequent_prefixes['prefix']\nfrequent_prefixes['prefix'] = frequent_prefixes.index\nfrequent_prefixes = frequent_prefixes.iloc[::-1] # reverse dataframe\n\nfig, ax = plt.subplots(figsize=(16, 4))\nax.clear()\nax.barh(frequent_prefixes.prefix, frequent_prefixes.pc, color=\"orange\", alpha=0.4)\nfor i in range(0, len(frequent_prefixes)):\n    ax.text(frequent_prefixes.pc.iloc[i]+10, i, frequent_prefixes.prefix.iloc[i]+\"-\", ha='left', va='center')\nax.set_yticks([])\nplt.title(\"Most frequent prefixes\")\nplt.xlabel('Prefix frequency')\nplt.ylabel('')\nplt.show()","3116b8a8":"frequent_suffixes = pd.DataFrame(df.suffix.value_counts()[1::].head(20))\nfrequent_suffixes['sc'] = frequent_suffixes['suffix']\nfrequent_suffixes['suffix'] = frequent_suffixes.index\nfrequent_suffixes = frequent_suffixes.iloc[::-1] # reverse dataframe\n\nfig, ax = plt.subplots(figsize=(16, 4))\nax.clear()\nax.barh(frequent_suffixes.suffix, frequent_suffixes.sc, color=\"orange\", alpha=0.4)\nfor i in range(0, len(frequent_suffixes)):\n    ax.text(frequent_suffixes.sc.iloc[i]+10, i,  \"-\"+frequent_suffixes.suffix.iloc[i], ha='left', va='center')\nax.set_yticks([])\nplt.title(\"Most frequent suffixes\")\nplt.xlabel('Suffix frequency')\nplt.ylabel('')\nplt.show()","636f3510":"import Levenshtein\ndf['edit'] = df.apply(lambda r:Levenshtein.distance(r.word1, r.word2), axis=1)\n# rough normalization of edit distances\ndf['norm_edit'] = df.apply(lambda r:r.edit\/max(r.len1,r.len2), axis=1) ","fa479d3e":"# prepare df for annotation\neditdf = df.drop_duplicates(['pair'])\n\nfig, ax = plt.subplots()\nfig.set_figwidth(16)\nfig.set_figheight(5)\n\n# scatter all points (including duplicate pairs)\nax.scatter(df.pl, df.edit, alpha=0.1, s=50, c=\"orange\")\n\n# annotate unique points only\nfor i in range(0, len(editdf), 3009):\n    if editdf.pl.iloc[i] > 0 and editdf.edit.iloc[i] < 6:\n        ax.annotate(editdf.pair.iloc[i], (editdf.pl.iloc[i], editdf.edit.iloc[i]))\n        \nplt.title(\"Prefix lengths and edit distances\")\nplt.xlabel(\"Common prefix length\")\nplt.ylabel(\"Levenstein distance\")\nplt.show()","a013fef0":"fig, ax = plt.subplots()\nfig.set_figwidth(16)\nfig.set_figheight(5)\nax.scatter(df.sl, df.edit, alpha=0.1, s=50, c=\"orange\")\n\nfor i in range(0, len(editdf),3003):\n    if editdf.sl.iloc[i] > 1 and editdf.edit.iloc[i] < 10:\n        ax.annotate(editdf.pair.iloc[i], (editdf.sl.iloc[i], editdf.edit.iloc[i]))\nplt.title(\"Suffix lengths and edit distances\")\nplt.xlabel(\"Common suffix length\")\nplt.ylabel(\"Edit distance\")\n#ax.gca()\nplt.xlim([-1,25])\nplt.show()","95fbc451":"# create the documents for model training\ncopydf = df.copy() # copy the data\ncopydf['word2'] = copydf.word2.apply(lambda x: [x]) \nclouddf = copydf.groupby(['word1']).agg({'word2':'sum'}).reset_index() # group by cue\n# add cue word also to word list, as many times as it has associations\nclouddf['document'] = clouddf.apply(lambda r: [r.word1]*len(r.word2) + r['word2'], axis=1)\n\n# create W2V model\nw2v_model = Word2Vec(clouddf.document, size=300, min_count=1, sg=1, workers=10)\n\n# create word corpus\ncorpus = list((dict(Counter(df.word1.tolist()).most_common()).keys()))[::-1]\n\n# add word vector to 4000 most frequent words (t-sne can't handle the full dataset)\ncorpusdf = pd.DataFrame(corpus[-4000::], columns=['word'])\ncorpusdf['wordvector'] = corpusdf.word.apply(lambda x: w2v_model.wv[x])\n\n# Compute reduced word-vectors using t-sne\ntsnemodel = TSNE(random_state=42).fit_transform(corpusdf['wordvector'].tolist()) \ncorpusdf['x'] = tsnemodel[:,0]\ncorpusdf['y'] = tsnemodel[:,1]","93fa0b0e":"# plot \ncorpusdf = corpusdf[::-1] # reverse\nfig, ax = plt.subplots()\nfig.set_figwidth(16)\nfig.set_figheight(16)\nsc = ax.scatter(corpusdf.y, corpusdf.x, marker='o', c=corpusdf.index, \n                cmap=plt.get_cmap('plasma'), s=180, alpha=0.1)\nfor j in range(0,4000,19): #len(corpusdf), 20):\n    ax.annotate(corpusdf.word.iloc[j], (corpusdf.y.iloc[j], corpusdf.x.iloc[j]), fontsize='9')\nplt.title(\"Reduced word vectors (W2V + t-SNE)\")\nplt.show()","1ea41ddc":"df['sim'] = df.apply(lambda r:w2v_model.wv.similarity(r.word1, r.word2), axis=1)","60518b49":"plt.figure(figsize=(16, 3))\nfor i in [6,8,9]:\n    s = df[df['sourceID']==i]\n    plt.axvline(s.sim.mean(), color=pal[i], linestyle='dashed', linewidth=1)\n    sns.kdeplot(s.sim, label=s.iloc[0]['source'], alpha=0.5, color=pal[i])  \nplt.title('Word similarity distribution')\nplt.xlabel(\"Word2Vec similarity\")\nplt.ylabel('Density')\nplt.legend(loc='upper right')\nplt.show()","e5fe0940":"from gensim.models.keyedvectors import KeyedVectors\n# this might take a while, and requires a computer with a decent amount of memory (16+ GB)\nw2v_news_model = KeyedVectors.load_word2vec_format('\/kaggle\/input\/googlenewsvectorsnegative300\/GoogleNews-vectors-negative300.bin', binary=True)","ae035296":"# replace spaces by underscores (word2vec convention)\ndf['word1w2v'] = df.word1.apply(lambda x:(x.replace(\" \", \"_\")))\ndf['word2w2v'] = df.word2.apply(lambda x:(x.replace(\" \", \"_\"))) \n# check if model contains words\ndf['inw2v1'] = df['word1w2v'].apply(lambda x:(x in w2v_news_model.vocab))\ndf['inw2v2'] = df['word2w2v'].apply(lambda x:(x in w2v_news_model.vocab)) \nprint(\"Percentage found: \" + str(df.inw2v1.sum()*100\/len(df)))\n\n# apply title capitilization to words not found in the model\ndf.loc[~df.inw2v1, 'word1w2v'] = df.loc[~df.inw2v1].word1w2v.apply(lambda x:x.title())\ndf.loc[~df.inw2v2, 'word2w2v'] = df.loc[~df.inw2v2].word2.apply(lambda x:x.title())\n# check if model contains the words (again)\ndf['inw2v1'] = df['word1w2v'].apply(lambda x:(x in w2v_news_model.vocab))\ndf['inw2v2'] = df['word2w2v'].apply(lambda x:(x in w2v_news_model.vocab))\nprint(\"Percentage found: \" + str(df.inw2v1.sum()*100\/len(df)))\n\n# uppercase words not found in the model\ndf.loc[~df.inw2v1, 'word1w2v'] = df.loc[~df.inw2v1].word1w2v.apply(lambda x:x.upper())\ndf.loc[~df.inw2v2, 'word2w2v'] = df.loc[~df.inw2v2].word2w2v.apply(lambda x:x.upper())\n# check if model contains the words (again)\ndf['inw2v1'] = df['word1w2v'].apply(lambda x:(x in w2v_news_model.vocab))\ndf['inw2v2'] = df['word2w2v'].apply(lambda x:(x in w2v_news_model.vocab))\nprint(\"Percentage found: \" + str(df.inw2v1.sum()*100\/len(df)))\n\n# check if both words in words association are found\ndf['inw2v'] = df.apply(lambda r:(r.inw2v1 & r.inw2v2), axis=1)\ndf = df.drop('inw2v1', 1) # remove\ndf = df.drop('inw2v2', 1) # remove","2fa538c4":"# compute similarity\ndf['news_sim'] = np.nan # similarity for unknown word pairs\ndf.loc[df.inw2v, 'news_sim'] = df.loc[df.inw2v].apply(lambda r:w2v_news_model.similarity(r.word1w2v, r.word2w2v), axis=1)\n# show pairs with highest similarity\ndf.drop_duplicates(subset=['pair']).sort_values(by=['news_sim'], ascending=False).head(5).pair.tolist()","6a222df2":"# remove data without similarity scores for plotting\nw2vdf = df[df.inw2v==True] \n\nplt.figure(figsize=(16, 3))\nfor i in [6,8,9]: #[3,7]: #range(0,10): #[6,8,9]:\n    s = w2vdf[w2vdf['sourceID']==i]\n    plt.axvline(s.news_sim.mean(), color=pal[i], linestyle='dashed', linewidth=1)\n    sns.kdeplot(s.news_sim, label=s.iloc[0]['source'], alpha=0.5, color=pal[i])  \nplt.title('Word similarity distribution (Google news vectors)')\nplt.xlabel(\"Word2Vec similarity\")\nplt.ylabel('Density')\nplt.legend(loc='upper right')\nplt.show()","03f62623":"from sklearn.manifold import TSNE\n# create word corpus\ncorpus = list((dict(Counter(w2vdf.word1w2v.tolist()).most_common()).keys()))[::-1]\n\n# add word vector to 4000 most frequent words (t-sne can't handle the full dataset)\ncorpusdf = pd.DataFrame(corpus[-4000::], columns=['word'])\ncorpusdf['news_wordvector'] = corpusdf.word.apply(lambda x: w2v_news_model[x])\n\n# Compute reduced word-vectors using t-sne\ntsnemodel = TSNE(random_state=42).fit_transform(corpusdf['news_wordvector'].tolist()) \ncorpusdf['x'] = tsnemodel[:,0]\ncorpusdf['y'] = tsnemodel[:,1]\n\ncorpusdf = corpusdf[::-1] # reverse\nfig, ax = plt.subplots()\nfig.set_figwidth(16)\nfig.set_figheight(16)\nsc = ax.scatter(corpusdf.y, corpusdf.x, marker='o', c=corpusdf.index, \n                cmap=plt.get_cmap('plasma'), s=180, alpha=0.1)\nfor j in range(0,4000,19): #len(corpusdf), 20):\n    ax.annotate(corpusdf.word.iloc[j], (corpusdf.y.iloc[j], corpusdf.x.iloc[j]), fontsize='9')\nplt.title(\"Reduced word vectors (W2V News vectors + t-SNE)\")\nplt.show()","61243e77":"from itertools import chain\nfrom nltk.corpus import wordnet\n\ndef isSynonym(r):\n    synonyms = wordnet.synsets(r.word1)\n    lemmas = set(chain.from_iterable([w.lemma_names() for w in synonyms]))\n    return (r.word2 in lemmas)\n\ndef isAntonym(r):\n    h = []\n    for syn in wordnet.synsets(r.word1):\n        for l in syn.lemmas():\n            if l.antonyms():\n                h.append(l.antonyms()[0].name().split('.')[0]) \n    return (r.word2 in h)\n\ndef isHypernym(r):\n    h = []\n    for syn in wordnet.synsets(r.word1):\n        if syn.hypernyms():\n            h.append(syn.hypernyms()[0].name().split('.')[0])\n    return (r.word2 in h)\n\ndef isHyponym(r):\n    h = []\n    for syn in wordnet.synsets(r.word1):\n        if syn.hyponyms():\n            h.append(syn.hyponyms()[0].name().split('.')[0]) \n    return (r.word2 in h)","12b1511c":"# identify nouns \nnouns = {x.name().split('.', 1)[0] for x in wordnet.all_synsets('n')} \ndf['noun1'] = df.word1.apply(lambda x:(x in nouns)) \ndf['noun2'] = df.word2.apply(lambda x:(x in nouns)) \n\n# identify synonyms\ndf['synonym'] = df.apply(isSynonym, axis=1)\n# identify antonyms\ndf['antonym'] = df.apply(isAntonym, axis=1)\n# identify hypernyms\ndf['hypernym'] = df.apply(isHypernym, axis=1)\n# identify hyponyms\ndf['hyponym'] = df.apply(isHyponym, axis=1)","ee79a620":"dfsem = df[df.synonym | df.antonym | df.hypernym | df.hyponym]\nlen(dfsem)*100\/len(df)","22a6bbd3":"# prepare for plotting\nsemdf = dfsem[~dfsem.news_sim.isnull()].drop_duplicates(['pair'])\n\n# some pairs multiple relationships! e.g. wound:hurt (last in this list defines its color)... \ncolumns = [semdf.synonym, semdf.antonym, semdf.hypernym, semdf.hyponym]\ncolors = [\"orange\", \"purple\", \"limegreen\", \"grey\"]\n\nsemdf['color'] = \"white\" # add column for colors\nfor i in range(0,4):\n    semdf.loc[columns[i], 'color'] = colors[i]\n    \n#plot \nfig, ax = plt.subplots()\nfig.set_figwidth(16)\nfig.set_figheight(5)\n\n# hacking some labels into the plot\nfor i in range(0,4):\n    ax.scatter([], [], alpha=0.2, s=50, c=colors[i], label=columns[i].name)\n    \n# plot all points\nax.scatter(semdf.news_sim, semdf.pf, alpha=0.2, s=50, c=semdf.color, label=\"\")\n\n# draw mean line\nfor i in range(0,4):\n    ax.axvline(semdf[columns[i]].news_sim.mean(), color=colors[i], linestyle='dashed', linewidth=1)\n\n# annotate some points\nfor i in range(0, len(semdf), 5):\n    if semdf.pf.iloc[i] > 17:\n        ax.annotate(semdf.pair.iloc[i], (semdf.news_sim.iloc[i], semdf.pf.iloc[i]))\n        \nplt.title(\"Word associations with semantic relationships\")\nplt.xlabel(\"Similarity (news vectors)\")\nplt.ylabel(\"Pair frequency\")\nplt.legend(loc='upper right')\nplt.gca()\nplt.ylim(0,70)\nplt.show()","065f75b8":"# save semantic features\ndf.to_csv(\"\/kaggle\/working\/wordgame_semantics.csv\", sep=',', index=False)","5b0060b4":"import csv\n# read AoA data\nreader = csv.reader(open('\/kaggle\/input\/wordgame\/AoA_ratings_Kuperman.csv', 'r'))\n# create AoA dictionary\naoa_dict = {k:v for k,v in reader if v!='NA'}\n# add AoA feature to data\ndf['AoA'] = df['word2'].apply(lambda x:float(aoa_dict[x]) if x in aoa_dict else np.nan).astype('float') \n\nprint(\"Percentage of words with known Age-of-acquisition: \" + str(round(len(df[~df['AoA'].isnull()])\/len(df)*100,1)))","f9cd7889":"# prepare a df for plotting\naoadf = df[~df.AoA.isnull()].drop_duplicates(['word2'])\n\n#plot \nfig, ax = plt.subplots()\nfig.set_figwidth(16)\nfig.set_figheight(5)\nax.scatter(aoadf.AoA, aoadf.tf, alpha=0.3, s=50, c=\"orange\")\nfor i in range(2, len(aoadf), 140):\n    if aoadf.tf.iloc[i] > 50 or aoadf.AoA.iloc[i] > 16:\n        ax.annotate(aoadf.word2.iloc[i], (aoadf.AoA.iloc[i], aoadf.tf.iloc[i]))\nplt.title(\"Age of acquisition vs. word frequency\")\nplt.xlabel(\"Age-of-acquisition\")\nplt.ylabel(\"Word frequency\")\nplt.gca()\nplt.ylim([0,700])\nplt.show()","7d5be909":"aoadf = df[~df.AoA.isnull()]\n\nplt.figure(figsize=(16, 5))\nfor i in [2,8,9]: #range(0,10,1):\n    s = aoadf[aoadf['sourceID']==i]\n    plt.axvline(s.AoA.mean(), color=pal[i], linestyle='dashed', linewidth=1)\n    sns.kdeplot(s['AoA'], label=s.iloc[0]['source'], alpha=0.5, color=pal[i])  \nplt.title('Histogram of Age-of-acquisition scores')\nplt.xlabel(\"Age-of-acquisition\")\nplt.ylabel('Density')\nplt.legend(loc='upper right')\nplt.show()","d1fc4b05":"# read concreteness data\nreader = csv.reader(open('\/kaggle\/input\/wordgame\/Concreteness_english.csv', 'r'))\n# create concreteness dictionary\nconc_dict = {k:v for k,v in reader if v!='NA'}\n# add concreteness feature to data\ndf['CR'] = df['word2'].apply(lambda x:conc_dict.get(x)).astype('float')\n# percentage of words with known concreteness rating\nlen(df[~df.CR.isnull()])\/len(df)*100","9250d922":"# prepare data for plotting\ncrdf = df[~df.CR.isnull()].drop_duplicates(['word2'])\n\n# remove rows with nan aoa's\nfig, ax = plt.subplots()\nfig.set_figwidth(16)\nfig.set_figheight(5)\nax.scatter(crdf.CR, crdf.tf, alpha=0.3, s=50, c=\"orange\")\nfor i in range(0, len(crdf), 5):\n    if crdf.tf.iloc[i] > 200 and crdf.CR.iloc[i] < 4.9:\n        ax.annotate(crdf.word2.iloc[i], (crdf.CR.iloc[i], crdf.tf.iloc[i]))\nplt.title(\"Concreteness rating vs. word frequency\")\nplt.xlabel(\"Concreteness rating\")\n#plt.xticks([1.0,5.0])\nplt.ylabel(\"Word frequency\")\nplt.gca()\nplt.xlim([1,5.01])\nplt.ylim([0,700])\nplt.show()","e369030d":"crdf = df[~df.CR.isnull()]\n\nplt.figure(figsize=(16, 5))\nfor i in [2,5,9]: #range(0,10,1):\n    s = crdf[crdf['sourceID']==i]\n    plt.axvline(s.CR.mean(), color=pal[i], linestyle='dashed', linewidth=1)\n    sns.kdeplot(s['CR'], alpha=0.5, label=s.iloc[0]['source'], color=pal[i])  \nplt.title('Histogram of Concreteness Ratings')\nplt.xlabel('Concreteness Rating')\nplt.ylabel('Density')\nplt.legend(loc='upper right')\nplt.show()","dbc12514":"# save psycholinguistic features\ndf.to_csv(\"\/kaggle\/working\/wordgame_psycho.csv\", sep=',', index=False)","eeef937f":"### Syntactic Features\n#### Word length","c37ca250":"This concludes the syntactic feature extraction. In the next section we move on to the semantic features such as word similarity. :)","328aadd4":"### Word association frequency","0bef3176":"## Methodology","6feb6bad":"### Data cleaning","b9fbd09d":"## Introduction\nIn this notebook we analyse word associations from online word association games. The word association game is a game frequently played on internet forums. In this game, each player posts a word that was his (first) association for the word in the previous post (e.g. user1 writes 'green', user2 posts 'grass', user3 posts 'tree' and so on). \n\nWe analyse these word associations to obtain new insights into structure of the mental lexicon and simply because it is fun to analyse this type of data. ;)","e64bd8f6":"##### Zipfs law\nAccording to Zipfs law, the frequency of any word (in a large sample of words) is inversely\nproportional to its rank in the frequency. ","69101a72":"## Dataset & Exploratory Data Analysis\nWe collected this data in 2017 using ScraPy (web scraping) from multiple sources. All data was shuffled and anonymized. \n\nThe dataset contains the following five columns:\n- _author_ : an numeric identifier for each user\n- _word1_ : the cue\n- _word2_ : the association\n- _source_ : abbreviation of the source name \n- _sourceID_ : an numeric identifier for the source","2d672338":"#### Age-of-Acquisition\nAge of Acquisition is a psycholinguistic variable referring to the age at which a word is typically learned. Source: http:\/\/crr.ugent.be\/archives\/806. ","c5a129b1":"Number of unique words:","bd7f2a1a":"## Feature Extraction","3ed356fe":"Percentage of pairs with semantic relationship(s): ","c3c66ea5":"### Psycholinguistic features","72396640":"## Semantic Features\nHere we extract features related to word meaning. We start with word embedding models.","b1d61479":"### Word Embedding Model (Word2Vec)","e6be46d7":"### WordNet\nWordNet is a lexical database for the English language. Here we use it to identify word pairs as synonyms, antonyms, hypernyms and hyponyms. We also identify nouns.","058a16cc":"Distribution of word association count per user:","bc9b0a1a":"#### Concreteness rating\nAnother phycholinguistic variable is the concreteness rating. (source: http:\/\/crr.ugent.be\/archives\/1330). Words like 'fire' have a high concreteness rating, whereas words like 'fail' and 'generic' have low concreteness ratings. ","6b276be7":"Number of unique users:","f30056fc":"### Common affixes\nWord association can be either semantical (e.g. 'blue' -> 'sky') or syntactic (e.g. *'bowels' -> 'vowels'*). Here we find the common prefixes and suffixes (e.g. in the previous example the suffix is *'owels'*) and the length of these common affixes.","f298e3f6":"The number of word associations in this dataset:","977b9505":"The word associations are scraped from the following sources:\n- AC : AspiesCentral (https:\/\/www.autismforums.com\/forums\/)\n- BC : BleepingComputer (https:\/\/www.bleepingcomputer.com\/forums\/)\n- CC : ClassicComics (https:\/\/classiccomics.org\/)\n- ECF : E-CigaretteForum (https:\/\/www.e-cigarette-forum.com\/)\n- GOG : Online game store (https:\/\/www.gog.com\/forum)\n- LEF : Learn English Forum (https:\/\/learnenglish.vanillacommunity.com) \n- SAS : Social Anxiety Support (https:\/\/www.socialanxietysupport.com\/forum\/)\n- TF : The Fishy (https:\/\/forum.thefishy.co.uk\/) \n- U2 : @U2 Forum (https:\/\/forum.atu2.com\/)\n- WP : Wrong Planet (https:\/\/wrongplanet.net\/forums\/)\n\nMost data is (American) English and includes post from people from many different countries. LEF is a forum for people who want to learn English and therefore word associations from this source could contain more mistakes. Data from TF contains British English, since this is a forum for British football supporters. AC and WP are internet forums for people with autism, which might also effect the type of word associations. \n\nNumber of word association from each source:","774b8f9d":"### Levenstein Distance\n\nLevenshtein distance (also called Edit Distance) is a measure of the similarity between two strings. It is minimum the number of deletions, insertions, or substitutions required to transform the first string into the second.","565fccbe":"### Word frequency","7e2078de":"### Pre-trained Word Embedding model\nNext make a more semantically meaningful word embedding, that is not influenced by the word frequencies in the dataset, by using a pre-trained model that is trained on different data. We use this pre-trained Google News corpus word vector model: https:\/\/drive.google.com\/file\/d\/0B7XkCwpI5KDYNlNUTTlSS21pQmM\/edit.","ea47bd43":"# Word Association Analysis\n\n    A. Louwe \n    https:\/\/www.github.com\/louweal"}}