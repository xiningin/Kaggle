{"cell_type":{"ec3239e8":"code","0e1ea6ae":"code","218f0588":"code","b9c7dfda":"code","36781602":"code","8151c8e1":"code","e8595d70":"code","746f61b3":"code","6d79c276":"code","a3002c0a":"code","cf6ba91c":"code","a92abb5a":"code","a48f2ffa":"code","48b6ac4f":"code","25157b1d":"code","9bb93afc":"code","7575862f":"code","e0fe769a":"code","66696020":"code","b1072a05":"code","a11ee7d0":"code","f2e26607":"code","656a5613":"code","6b9069de":"code","88688a09":"code","23642756":"code","4d9f28ea":"markdown","977ce427":"markdown","1f7a4954":"markdown","a85a1a50":"markdown","7cdc4eaa":"markdown","2010847f":"markdown","8c7179d0":"markdown","dfb19510":"markdown","2654cc05":"markdown"},"source":{"ec3239e8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0e1ea6ae":"df=pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')","218f0588":"df","b9c7dfda":"df.isnull().sum() # So there is no need for missing value imputation","36781602":"df.describe()","8151c8e1":"import seaborn as sns\nimport matplotlib.pyplot as plt\n","e8595d70":"fig = plt.figure(figsize=(8,20))\n\nfor index,i in enumerate(df.columns[:-1]):\n    ax = fig.add_subplot(len(df.columns),1,index+1)\n    ax.hist(df[i].loc[df['Outcome']==0],color='Green',bins=50)\n    ax.hist(df[i].loc[df['Outcome']==1],color='Red',bins=50)\n    ax.legend(['Non-Diabetic','Diabetic'])\n    \n    ax.set_title('Distribution of '+ i)\n    \n\nplt.tight_layout()\nplt.show()","746f61b3":"sorted(df['Glucose'].unique())[1]","6d79c276":"# Removing and replacing the lower outliers with values lesser than the percentile described\ndef outlier_correction(df,feature,percentile):\n    df.loc[((df[feature]==0)|(df[feature]<df[feature].quantile(percentile)))\n                              ,feature]=df[feature].quantile(percentile)\n\n    ","a3002c0a":"# another alternative to remove the lowest(In our case zero) and replace them with the next smallest value\ndef outlier_minima(df,feature):\n    df.loc[df[feature]==df[feature].min(),feature]=sorted(df[feature].unique())[1]\n    #return df","cf6ba91c":"outlier_minima(df,'Glucose')\noutlier_minima(df,'BloodPressure')\noutlier_minima(df,'SkinThickness')\noutlier_minima(df,'BMI')","a92abb5a":"df.describe()","a48f2ffa":"# Distribution after outlier correction\nfig = plt.figure(figsize=(8,20))\n\nfor index,i in enumerate(df.columns[:-1]):\n    ax = fig.add_subplot(len(df.columns),1,index+1)\n    ax.hist(df[i].loc[df['Outcome']==0],color='Green',bins=50)\n    ax.hist(df[i].loc[df['Outcome']==1],color='Red',bins=50)\n    ax.legend(['Non-Diabetic','Diabetic'])\n    \n    ax.set_title('Distribution of '+ i)\n    \n\nplt.tight_layout()\nplt.show()","48b6ac4f":"sns.pairplot(df,hue='Outcome')","25157b1d":"#Converting to arrays\nX=df.iloc[:,:-1]\ny=df.iloc[:,-1]","9bb93afc":"np.unique(y,return_counts=True) # Imbalanced Data, Better to use stratification","7575862f":"\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn import metrics\nfrom sklearn.utils import shuffle\nimport lightgbm\nfrom lightgbm import LGBMClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#X=df\n#shuffle data\nX, y = shuffle(X, y, random_state=20)\n#\n#model\n#clf= XGBClassifier()\nclf=RandomForestClassifier(class_weight='balanced')\n#clf=lightgbm.LGBMClassifier(is_unbalance=True)\n#clf = KNeighborsClassifier(n_neighbors=10, weights = 'distance')\n#\n#leave-one-out\ny_pred = np.array([])\ny_true = np.array([])\nloo = LeaveOneOut()\nfor train_index, test_index in loo.split(X):\n    X_train = X.iloc[train_index,:]\n    X_test = X.iloc[test_index,:]\n    y_train = y[train_index]\n    y_test = y[test_index]\n    #\n    #normalize data\n    X_train_mean = np.mean(X_train, axis=0)\n    X_train_std = np.std(X_train, axis=0)\n    X_train_norm = (X_train - X_train_mean)\/X_train_std\n    X_test_norm = (X_test - X_train_mean)\/X_train_std\n    #\n    #train\n    clf.fit(X_train_norm, y_train)\n    #\n    #test\n    y_pred = np.append(y_pred,clf.predict(X_test_norm))\n    y_true = np.append(y_true,y_test)\n#\n#metrics\nprint(\"Labels: Non-Diabetic, Diabetic\")\nprint(\"Confusion matrix\")\nprint(metrics.confusion_matrix(y_true, y_pred))\nprint(\"Precision\")\nprint(metrics.precision_score(y_true, y_pred, average=None))\nprint(\"Recall\")\nprint(metrics.recall_score(y_true, y_pred, average=None))\nprint(\"F1 score\")\nprint(metrics.f1_score(y_true, y_pred, average=None))\nprint(metrics.f1_score(y_true, y_pred, average='weighted'))\nprint(metrics.accuracy_score(y_true,y_pred))","e0fe769a":"from sklearn.model_selection import cross_val_score\nclf = RandomForestClassifier(max_depth=8,class_weight={0:0.28,1:0.72})\nscores = cross_val_score(clf, X, y, cv=5)\n\nf1=cross_val_score(clf, X, y, cv=5,scoring='f1_weighted')\nscores","66696020":"f1","b1072a05":"scores.mean()","a11ee7d0":"f1.mean()","f2e26607":"from scipy import stats\nfrom scipy.stats import randint\nfrom sklearn.model_selection import RandomizedSearchCV\nimport sklearn\n#from sklearn import cross_validation\n\n#shuffle data\nX, y = shuffle(X, y, random_state=20)\n\n\nclf1=RandomForestClassifier(class_weight='balanced')\nparam_dist = {'n_estimators': [100,150,200,300],\n              'min_samples_split':[1,2,4],\n              'max_depth': [3, 5, 6, 8, 9],\n              'min_samples_leaf':[1,2,3]\n             }\n              \n              \n             \nscorer = sklearn.metrics.make_scorer(sklearn.metrics.f1_score, average = 'weighted')\n#numFolds = 10\n#kfold_5 = cross_validation.KFold(n = len(X), shuffle = True, n_folds = numFolds)\n\nclfcv = RandomizedSearchCV(clf1, \n                         param_distributions = param_dist,\n                         cv = 5,  \n                         n_iter = 20, \n                         scoring = scorer, \n                         error_score = 0, \n                         verbose = 3, \n                         n_jobs = -1,random_state=20)","656a5613":"clfcv.fit(X,y)","6b9069de":"clfcv.best_score_","88688a09":"clfcv.best_params_","23642756":"clfcv.best_estimator_","4d9f28ea":"The f1 score obtained after Random search cross validation for tuning the model","977ce427":"Although not great results could be achieved by giving higher weights to the diabetic styate one could reduce false negatives and there is a decent accuracy and F1 score","1f7a4954":"After Random search Cv we get an F1 score of 77.18,\nThus although the predictions will not be up to the mark, upon tuning the model using Randomsied search cross validation, an F1 score of 77.18 has been obtained, Also since it is an imbalanced data set, it is better to rely on the F1 score as we had already seen in the Leave one out cross validation step that there are a lot of False Negatives","a85a1a50":"### Machine learning phase","7cdc4eaa":"### Trying Leave one out with several algorithms, Post normalization","2010847f":"From this we can understand that the zero points in BMI, Skin thickness, Blood pressure and Glucose are clearly outliers, However insulin could still be zero as the subject might not have been administered Insulin, thus the above mentioned outliers need to be treated","8c7179d0":"Not a linear distribution clearly, so it would be better to try ensemble techniques","dfb19510":"|Algorithm | Accuracy |  F1 Score |\n|----------|----------|-----------|\n| KNN-10   |  0.58    |   0.54    |\n| LGB      |  0.53    |   0.529   |\n| RF       |  0.63    |   0.56    |","2654cc05":"### Random forest most effective, 5 fold cross validation, No normalisation"}}