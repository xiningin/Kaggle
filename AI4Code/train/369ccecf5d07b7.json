{"cell_type":{"8a163774":"code","14d48078":"code","f98643fe":"code","59a5467e":"code","70401309":"code","54253477":"code","aefbf439":"code","038e2ce7":"code","e2c4c38e":"code","47017f2c":"code","70ddfbba":"code","c82110ea":"code","be676d10":"code","80cde284":"code","09037ccc":"code","0d535a7e":"code","a5752f66":"code","dc91c748":"code","05ec31ef":"code","d45c9e6a":"code","2d03c180":"code","1eed90d2":"code","aa9a3ab3":"code","2df4fa1f":"code","b6129434":"code","c8ca30a6":"code","5ed81bbc":"code","31c55208":"code","81fc4dea":"code","4a44e942":"code","0e9cfe34":"code","2bc2efa4":"code","1deec5a5":"code","efb5e274":"code","277577dd":"code","d1459e13":"code","c91b767e":"code","99f56c67":"code","80f652cc":"code","ae106a50":"code","f6f1d6a7":"code","c197dc64":"code","91446a3e":"code","390b2603":"code","c758e804":"code","26ec7895":"code","a8f60600":"code","4f425f53":"code","29526504":"code","2a07a6ed":"code","73a07053":"code","9e877587":"code","fbd913d1":"code","73778563":"code","acf06479":"code","de65c038":"code","7d2640d6":"code","b07fa8e2":"code","1f79cd6a":"code","f966163b":"code","be1777a3":"code","fba1beca":"code","9900e3e3":"code","d17bb05d":"code","8a8bbae8":"code","640132dd":"code","00866f42":"code","4c8d8cb6":"code","b8d31e83":"code","0a8b030f":"code","e20b5f4d":"code","dd5be031":"code","9f135f12":"code","4587fdd6":"code","e9915c0e":"code","9095a733":"code","eba2eb9f":"code","a6bef1fd":"code","38c66cc8":"code","b735b4dc":"code","7d7fa685":"code","2b1b80d9":"code","a3c9de4d":"code","d335c4c5":"code","a869c39b":"code","00bcec7d":"code","72e09b89":"code","c8956f79":"code","42d6767e":"markdown","bb8c76f9":"markdown","ab2b6fa9":"markdown","8dadd84c":"markdown","f7579eb7":"markdown","2e2e1a32":"markdown","0472fb74":"markdown","c12ce6bc":"markdown","c78f65dc":"markdown","db32d634":"markdown","6897f4a5":"markdown","3c62106e":"markdown","9f643c0f":"markdown","9e460bab":"markdown","8b59072a":"markdown","3a523dc9":"markdown","b6614e03":"markdown","a77003d4":"markdown","67cbbd91":"markdown","91332335":"markdown","796cb27b":"markdown","d31b81b3":"markdown","a7cad2f2":"markdown","177527da":"markdown","59261fe8":"markdown","1e235174":"markdown","8e962272":"markdown","abe90b32":"markdown","56245be2":"markdown","431a71c6":"markdown","101b7aed":"markdown","7baa33d0":"markdown","9ca24253":"markdown","90ce2283":"markdown","d5bb3ac4":"markdown"},"source":{"8a163774":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","14d48078":"!pip install pandas-profiling\n!pip install missingno\n!pip install catboost\n!pip install mlens\n\nimport numpy as np\nimport pandas as pd\nimport pandas_profiling\nimport missingno as msno\n\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score,roc_curve\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n%matplotlib inline\n\n# Close warnings\nimport warnings\nwarnings.filterwarnings('ignore')","f98643fe":"df=pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndf.head(5)","59a5467e":"df.tail(5)","70401309":"df.info()","54253477":"df.profile_report()","aefbf439":"# Unique values for the variables \n\nfor i, col in enumerate(df):\n    unique=df.iloc[:,i].unique()\n    unique.sort()\n    display(col, unique)","038e2ce7":"# Error correction regarding 0 values those do not make sense\n\nprint('0 values in below features are replaced as nan:')\nfor i, col in enumerate(df):\n    if i> 0 and i<6:\n        df[col].replace(to_replace=0, value=np.nan, inplace=True)\n        display(col)","e2c4c38e":"# Ref <https:\/\/www.kaggle.com\/kingychiu\/home-credit-eda-distributions-and-outliers>\n\ntotal_nans = df.isna().sum()\nnan_precents = (df.isna().sum()\/df.isna().count()*100)\nfeature_overview_df  = pd.concat([total_nans, nan_precents], axis=1, keys=['NaN Count', 'NaN Pencent'])\nfeature_overview_df['Type'] = [df[c].dtype for c in feature_overview_df.index]\npd.set_option('display.max_rows', None)\ndisplay(feature_overview_df)\npd.set_option('display.max_rows', 20)","47017f2c":"# get the number of missing data points per column\nmissing_values_count = df.isnull().sum()\n\n# how many total missing values do we have?\ntotal_cells = np.product(df.shape)\ntotal_missing = missing_values_count.sum()\n\n# percent of data that is missing\n(total_missing\/total_cells) * 100","70ddfbba":"fig, ax = plt.subplots(4, 2, figsize=(16,16))\n\nsns.distplot(df.Pregnancies, bins = 20, ax=ax[0,0])\nsns.distplot(df.Glucose, bins = 20, ax=ax[0,1])\nsns.distplot(df.BloodPressure, bins = 20, ax=ax[1,0])\nsns.distplot(df.SkinThickness, bins = 20, ax=ax[1,1])\nsns.distplot(df.Insulin, bins = 20, ax=ax[2,0])\nsns.distplot(df.BMI, bins = 20, ax=ax[2,1])\nsns.distplot(df.DiabetesPedigreeFunction, bins = 20, ax=ax[3,0])\nsns.distplot(df.Age, bins = 20, ax=ax[3,1])","c82110ea":"msno.matrix(df)","be676d10":"msno.bar(df)\nplt.show()","80cde284":"# Nullity correlation\n\nmsno.heatmap(df)","09037ccc":"msno.dendrogram(df)","0d535a7e":"# Making a copy of df with null values\n\ndf_with_null = df","a5752f66":"# Imputation\n\nmy_imputer = SimpleImputer()\ndf = pd.DataFrame(my_imputer.fit_transform(df))","dc91c748":"df.columns = df_with_null.columns","05ec31ef":"df.isnull().sum()","d45c9e6a":"for x_index, col_1 in enumerate(df):\n    for y_index, col_2 in enumerate(df):\n        if (x_index != y_index &  y_index != (x_index-1)):\n            \n            fig = plt.gcf()\n            fig.set_size_inches(9, 6)\n            sns.scatterplot(x=df.iloc[:, x_index], y=df.iloc[:, y_index], hue=df.iloc[:,-1], data=df);\n            plt.show()","2d03c180":"df.describe([0.01,0.1,0.25,0.5,0.75,0.99]).T","1eed90d2":"df.corr()","aa9a3ab3":"fig, ax = plt.subplots(figsize=(12,8)) \nsns.heatmap(df.iloc[:,0:len(df)].corr(), annot = True, fmt = \".2f\", linewidths=0.5, ax=ax) \nplt.show()","2df4fa1f":"# Features and Target Variable\n\nX = df.drop(['Outcome'], axis=1)\ny = df[\"Outcome\"]","b6129434":"# Applying Standard Scaling to get optimized results\n\nscale = StandardScaler()\nX = scale.fit_transform(X)","c8ca30a6":"# Split the data into training\/testing sets\n\nX_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, shuffle=y, stratify=y, random_state = 42)","5ed81bbc":"# Set cross validation \n\nsss = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)","31c55208":"# Data with Null Values\n\nX_with_null = df_with_null.drop(['Outcome'], axis=1)\ny_with_null = df_with_null[\"Outcome\"]\n\nscale = StandardScaler()\nX_with_null = scale.fit_transform(X_with_null)\n\nX_train_with_null, X_test_with_null, y_train_with_null, y_test_with_null= train_test_split(\n    X_with_null, y_with_null, test_size=0.2, shuffle=y, stratify=y, random_state = 42)\n","81fc4dea":"# Create the model\nlog_reg = LogisticRegression(random_state=42)\n\n# Fit the model\nlog_reg.fit(X_train, y_train)\n\n# Predict the model\ny_pred = log_reg.predict(X_test)\n\n# Accuracy Score\naccuracy_score(y_test, y_pred)","4a44e942":"logit_roc_auc = roc_auc_score(y, log_reg.predict(X))\n\nfpr, tpr, thresholds = roc_curve(y, log_reg.predict_proba(X)[:,1])\n\nplt.figure()\nplt.plot(fpr, tpr, label='AUC (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Ratio')\nplt.ylabel('True Positive Ratio')\nplt.title('ROC')\nplt.show()","0e9cfe34":"#  Model tuning\n\nlog_reg_params = {\"C\":np.logspace(-1, 1, 10),\n                  \"penalty\": [\"l1\",\"l2\"], \n                  \"solver\":['lbfgs', 'liblinear', 'sag', 'saga'], \n                  \"max_iter\":[1000]}\nlog_reg = LogisticRegression(random_state=42)\nlog_reg_cv_model = GridSearchCV(log_reg, log_reg_params, cv=sss)\nlog_reg_cv_model.fit(X_train, y_train)\nprint(\"Best score:\" + str(log_reg_cv_model.best_score_))\nprint(\"Best parameters: \" + str(log_reg_cv_model.best_params_))","2bc2efa4":"# Create the model\nnb = GaussianNB()\n\n# Fit the model\nnb_model = nb.fit(X_train, y_train)\n\n# Predict the model\ny_pred = nb_model.predict(X_test)\n\n# Accuracy Score\naccuracy_score(y_test, y_pred)","1deec5a5":"#  Model tuning\n\nnb_params = {'var_smoothing': np.logspace(0,-9, num=100)}\nnb = GaussianNB()\nnb_cv_model = GridSearchCV(nb, nb_params, cv=sss)\nnb_cv_model.fit(X_train, y_train)\nprint(\"Best score:\" + str(nb_cv_model.best_score_))\nprint(\"Best parameters: \" + str(nb_cv_model.best_params_))","efb5e274":"# Create the model\nknn = KNeighborsClassifier()\n\n# Fit the model\nknn_model = knn.fit(X_train, y_train)\n\n# Predict the model\ny_pred = knn_model.predict(X_test)\n\n# Accuracy Score\naccuracy_score(y_test, y_pred)","277577dd":"#  Model tuning\n\nknn_params = {\"n_neighbors\": np.arange(1,50)}\nknn = KNeighborsClassifier()\nknn_cv_model = GridSearchCV(knn, knn_params, cv=sss)\nknn_cv_model.fit(X_train, y_train)\nprint(\"Best score:\" + str(knn_cv_model.best_score_))\nprint(\"Best parameters: \" + str(knn_cv_model.best_params_))","d1459e13":"kernel = \"linear\"\nsvm_model = SVC(kernel = kernel,random_state=42).fit(X_train, y_train)\nsvm_model\ny_pred = svm_model.predict(X_test)\nprint(i, \"accuracy score:\", accuracy_score(y_test, y_pred))\nsvc_params = {\"C\": [0.00001, 0.0001, 0.001, 0.1, 1, 5, 10 ,50 ,100, 500,1000],\n             \"gamma\": [0.0001, 0.001, 0.1, 1, 5, 10 ,50 ,100]}\nsvc = SVC(kernel = kernel)\nsvc_linear_cv_model = GridSearchCV(svc,svc_params, \n                            cv = sss, \n                            n_jobs = -1, \n                            verbose = 2 )\nsvc_linear_cv_model.fit(X_train, y_train)\nprint(\"Best parameters: \" + str(svc_linear_cv_model.best_params_))","c91b767e":"kernel = \"rbf\"\nsvm_model = SVC(kernel = kernel,random_state=42).fit(X_train, y_train)\nsvm_model\ny_pred = svm_model.predict(X_test)\nprint(i, \"accuracy score:\", accuracy_score(y_test, y_pred))\nsvc_params = {\"C\": [0.00001, 0.0001, 0.001, 0.1, 1, 5, 10 ,50 ,100, 500,1000],\n             \"gamma\": [0.0001, 0.001, 0.1, 1, 5, 10 ,50 ,100]}\nsvc = SVC(kernel = kernel)\nsvc_rbf_cv_model = GridSearchCV(svc,svc_params, \n                            cv = sss, \n                            n_jobs = -1, \n                            verbose = 2 )\nsvc_rbf_cv_model.fit(X_train, y_train)\nprint(\"Best parameters: \" + str(svc_rbf_cv_model.best_params_))","99f56c67":"# Create the model\ncart = DecisionTreeClassifier(random_state=42)\n\n# Fit the model\ncart_model = cart.fit(X_train, y_train)\n\n# Predict the model\ny_pred = cart_model.predict(X_test)\n\n# Accuracy Score\naccuracy_score(y_test, y_pred)","80f652cc":"# Model Tuning\n\ncart_grid = {\"max_depth\": range(1,10),\n            \"min_samples_split\" : list(range(2,50)) }\ncart = DecisionTreeClassifier(random_state=42)\ncart_cv = GridSearchCV(cart, cart_grid, cv = sss, n_jobs = -1, verbose = 2)\ncart_cv_model = cart_cv.fit(X_train, y_train)\nprint(\"Best Parameters: \" + str(cart_cv_model.best_params_))","ae106a50":"# Create the model\nbagging = BaggingClassifier(n_estimators = 500, max_samples = 0.5, max_features = 0.5, random_state=42) \n\n# Fit the model\nbagging_model = bagging.fit(X_train, y_train)\n\n# Predict the model\ny_pred = bagging_model.predict(X_test)\n\n# Accuracy Score\naccuracy_score(y_test, y_pred)","f6f1d6a7":"# Model Tuning\n\nbagging_params = {\n 'n_estimators': [10, 100, 500, 1000],\n 'max_samples' : [0.05, 0.1, 0.2, 0.5]\n}\nbagging = BaggingClassifier(random_state=42) \nbagging_cv_model = GridSearchCV(bagging, bagging_params, \n                             cv = sss, \n                             n_jobs = -1, \n                             )\nbagging_cv_model.fit(X_train, y_train)\nprint(\"Best parameters: \" + str(bagging_cv_model.best_params_))","c197dc64":"# Create the model\nrf = RandomForestClassifier(random_state=42)\n\n# Fit the model\nrf_model = rf.fit(X_train, y_train)\n\n# Predict the model\ny_pred = rf_model.predict(X_test)\n\n# Accuracy Score\naccuracy_score(y_test, y_pred)","91446a3e":"# Model Tuning\n\nrf_params = {\"max_depth\": [2,5,8,10],\n            \"max_features\": [2,3,5,7],\n            \"n_estimators\": [10,100,200,500,1000],\n            \"min_samples_split\": [2,5,10]}\nrf_model = RandomForestClassifier(random_state=42)\nrf_cv_model = GridSearchCV(rf_model, \n                           rf_params, \n                           cv = sss, \n                           n_jobs = -1, \n                           verbose = 2) \nrf_cv_model.fit(X_train, y_train)\nprint(\"Best Parameters: \" + str(rf_cv_model.best_params_))","390b2603":"# Create the model\nex_tree = ExtraTreesClassifier(n_estimators=300, random_state=42) \n\n# Fit the model\nex_tree_model = ex_tree.fit(X_train, y_train)\n\n# Predict the model\ny_pred = ex_tree_model.predict(X_test)\n\n# Accuracy Score\naccuracy_score(y_test, y_pred)","c758e804":"# Model Tuning\n\nex_tree_params = {\n 'n_estimators': [50, 100, 200, 300],\n 'min_samples_leaf' : [5, 20, 50],\n 'min_samples_split' : [5, 15, 30]}\nex_tree = ExtraTreesClassifier(random_state=42) \nex_tree_cv_model = GridSearchCV(ex_tree, ex_tree_params, \n                             cv = sss, \n                             n_jobs = -1, \n                             verbose = 2)\nex_tree_cv_model.fit(X_train, y_train)\nprint(\"Best parameters: \" + str(ex_tree_cv_model.best_params_))","26ec7895":"# Create the model\ngbc = GradientBoostingClassifier(random_state=42)\n\n# Fit the model\ngbc_model = gbc.fit(X_train, y_train)\n\n# Predict the model\ny_pred = gbc_model.predict(X_test)\n\n# Accuracy Score\naccuracy_score(y_test, y_pred)","a8f60600":"# Model Tuning\n\ngbc_params = {\n        \"learning_rate\" : [0.001, 0.01, 0.1, 0.05],\n        \"n_estimators\": [100,500,1000],\n        \"max_depth\": [3,5,10],\n        \"min_samples_split\": [2,5,10]}\ngbc = GradientBoostingClassifier(random_state=42)\ngbc_cv_model = GridSearchCV(gbc, gbc_params, cv = sss, n_jobs = -1, verbose = 2)\ngbc_cv_model.fit(X_train, y_train)\nprint(\"Best parameters: \" + str(gbc_cv_model.best_params_))","4f425f53":"# Create the model\nxgb = XGBClassifier(random_state=42)\n\n# Fit the model\nxgb_model = xgb.fit(X_train, y_train)\n\n# Predict the model\ny_pred = xgb_model.predict(X_test)\n\n# Accuracy Score\naccuracy_score(y_test, y_pred)","29526504":"# Model Tuning\n\nxgb_params = {\n        'n_estimators': [100, 500, 1000, 2000],\n        'subsample': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5, 6],\n        'learning_rate': [0.1, 0.01, 0.02, 0.05]}\nxgb = XGBClassifier(random_state=42)\nxgb_cv_model = GridSearchCV(xgb, xgb_params, cv = sss, n_jobs = -1, verbose = 2)\nxgb_cv_model.fit(X_train, y_train)\nprint(\"Best parameters: \" + str(xgb_cv_model.best_params_))","2a07a6ed":"# Create the model\nxgb = XGBClassifier(random_state=42)\n\n# Fit the model\nxgb_model_with_null = xgb.fit(X_train_with_null, y_train_with_null)\n\n# Predict the model\ny_pred_with_null = xgb_model_with_null.predict(X_test_with_null)\n\n# Accuracy Score\naccuracy_score(y_test_with_null, y_pred_with_null)","73a07053":"# Model Tuning\n\nxgb_params = {\n        'n_estimators': [100, 500, 1000, 2000],\n        'subsample': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5, 6],\n        'learning_rate': [0.1, 0.01, 0.02, 0.05]}\nxgb = XGBClassifier(random_state=42)\nxgb_cv_model_with_null = GridSearchCV(xgb, xgb_params, cv = sss, n_jobs = -1, verbose = 2)\nxgb_cv_model_with_null.fit(X_train_with_null, y_train_with_null)\nprint(\"Best parameters: \" + str(xgb_cv_model_with_null.best_params_))","9e877587":"# Create the model\nlgbm = LGBMClassifier(random_state=42)\n\n# Fit the model\nlgbm_model = lgbm.fit(X_train, y_train)\n\n# Predict the model\ny_pred = lgbm_model.predict(X_test)\n\n# Accuracy Score\naccuracy_score(y_test, y_pred)","fbd913d1":"# Model Tuning\n\nlgbm_params = {\n        'n_estimators': [100, 500, 1000, 2000],\n        'subsample': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5, 6],\n        'learning_rate': [0.1, 0.01, 0.02, 0.05],\n        \"min_child_samples\": [5, 10, 20]}\nlgbm = LGBMClassifier(random_state=42)\nlgbm_cv_model = GridSearchCV(lgbm, lgbm_params, \n                             cv = sss, \n                             n_jobs = -1, \n                             verbose = 2)\nlgbm_cv_model.fit(X_train, y_train)\nprint(\"Best parameters: \" + str(lgbm_cv_model.best_params_))","73778563":"# Create the model\nlgbm = LGBMClassifier(random_state=42)\n\n# Fit the model\nlgbm_model = lgbm.fit(X_train_with_null, y_train_with_null)\n\n# Predict the model\ny_pred_with_null = lgbm_model.predict(X_test_with_null)\n\n# Accuracy Score\naccuracy_score(y_test_with_null, y_pred_with_null)","acf06479":"# Model Tuning\n\nlgbm_params = {\n        'n_estimators': [100, 500, 1000, 2000],\n        'subsample': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5, 6],\n        'learning_rate': [0.1, 0.01, 0.02, 0.05],\n        \"min_child_samples\": [5, 10, 20]}\nlgbm = LGBMClassifier(random_state=42)\nlgbm_cv_model_with_null = GridSearchCV(lgbm, lgbm_params, \n                             cv = sss, \n                             n_jobs = -1, \n                             verbose = 2)\nlgbm_cv_model_with_null.fit(X_train_with_null, y_train_with_null)\nprint(\"Best parameters: \" + str(lgbm_cv_model_with_null.best_params_))","de65c038":"# Create the model\ncatboost = CatBoostClassifier(random_state=42,verbose = False)\n\n# Fit the model\ncatboost_model = catboost.fit(X_train, y_train)\n\n# Predict the model\ny_pred = catboost_model.predict(X_test)\n\n# Accuracy Score\naccuracy_score(y_test, y_pred)","7d2640d6":"# Model Tuning\n\ncatboost_params = {\n        'depth':[2, 3, 4],\n        'loss_function': ['Logloss', 'CrossEntropy'],\n        'l2_leaf_reg':np.arange(2,31)}\ncatboost = CatBoostClassifier(random_state=42,verbose = False)\ncatboost_cv_model = GridSearchCV(catboost, catboost_params, \n                             cv = sss, \n                             n_jobs = -1, \n                             verbose = 2)\ncatboost_cv_model.fit(X_train, y_train)\nprint(\"Best parameters: \" + str(catboost_cv_model.best_params_))","b07fa8e2":"# Create the model\ncatboost = CatBoostClassifier(random_state=42,verbose = False)\n\n# Fit the model\ncatboost_model = catboost.fit(X_train_with_null, y_train_with_null)\n\n# Predict the model\ny_pred = catboost_model.predict(X_test_with_null)\n\n# Accuracy Score\naccuracy_score(y_test_with_null, y_pred_with_null)","1f79cd6a":"# Model Tuning\n\ncatboost_params = {\n        'depth':[2, 3, 4],\n        'loss_function': ['Logloss', 'CrossEntropy'],\n        'l2_leaf_reg':np.arange(2,31)}\ncatboost = CatBoostClassifier(random_state=42,verbose = False)\ncatboost_cv_model_with_null = GridSearchCV(catboost, catboost_params, \n                             cv = sss, \n                             n_jobs = -1, \n                             verbose = 2)\ncatboost_cv_model_with_null.fit(X_train_with_null, y_train_with_null)\nprint(\"Best parameters: \" + str(catboost_cv_model_with_null.best_params_))","f966163b":"# Create the model\nada=AdaBoostClassifier(n_estimators=50, random_state=42) \n\n# Fit the model\nada_model = ada.fit(X_train, y_train)\n\n# Predict the model\ny_pred = ada_model.predict(X_test)\n\n# Accuracy Score\naccuracy_score(y_test, y_pred)\n","be1777a3":"# Model Tuning\n\nada_params = {\n 'n_estimators': [50, 100, 200, 300],\n 'learning_rate' : [0.01,0.05,0.1,0.3,1],\n }\nada=AdaBoostClassifier(random_state=42)\nada_cv_model = GridSearchCV(ada, ada_params, \n                             cv = sss,\n                           n_jobs = -1)\nada_cv_model.fit(X_train, y_train)\nprint(\"Best parameters: \" + str(ada_cv_model.best_params_))","fba1beca":"# Create the model\nmlpc = MLPClassifier(random_state=42)\n\n# Fit the model\nmlpc_model = mlpc.fit(X_train, y_train)\n\n# Predict the model\ny_pred = mlpc_model.predict(X_test)\n\n# Accuracy Score\naccuracy_score(y_test, y_pred)","9900e3e3":"# Model Tuning\n\nmlpc_params = {\n        \"alpha\": [0.1, 0.01, 0.02, 0.005, 0.0001,0.00001],\n        \"hidden_layer_sizes\": [(10,10,10),\n                               (100,100,100),\n                               (100,100),\n                               (3,5), \n                               (5, 3)],             \n        \"solver\" : [\"lbfgs\",\"adam\",\"sgd\"],\"max_iter\":[1000]}\nmlpc = MLPClassifier(random_state=42)\nmlpc_cv_model = GridSearchCV(mlpc, mlpc_params, \n                             cv = sss, \n                             n_jobs = -1, \n                             verbose = 2)\nmlpc_cv_model.fit(X_train, y_train)\nprint(\"Best parameters: \" + str(mlpc_cv_model.best_params_))","d17bb05d":"# Tuned Logistic Regression Model\n\nparam = log_reg_cv_model.best_params_\nlog_reg = LogisticRegression(**param, random_state=42)\nlog_reg_tuned = log_reg.fit(X_train, y_train)\ny_pred = log_reg_tuned.predict(X_test)\nlog_reg_final = accuracy_score(y_test, y_pred)\nlog_reg_final\n","8a8bbae8":"# Tuned Gaussian Naive Bayes Model\n\nparam = nb_cv_model.best_params_\nnb = GaussianNB(**param)\nnb_tuned = nb.fit(X_train, y_train)\ny_pred = nb_tuned.predict(X_test)\nnb_final = accuracy_score(y_test, y_pred)\nnb_final","640132dd":"# Tuned KNN Model\n\nparam = knn_cv_model.best_params_\nknn = KNeighborsClassifier(**param)\nknn_tuned = knn.fit(X_train, y_train)\ny_pred = knn_tuned.predict(X_test)\nknn_final = accuracy_score(y_test, y_pred)\nknn_final","00866f42":"# Tuned SVC Linear Model\n\nparam = svc_linear_cv_model.best_params_\nsvc_linear = SVC(**param, kernel = 'linear',random_state=42)\nsvc_tuned = svc_linear.fit(X_train, y_train)\ny_pred = svc_tuned.predict(X_test)\nsvc_linear_final = accuracy_score(y_test, y_pred)\nsvc_linear_final","4c8d8cb6":"# Tuned SVC RBF Model\n\nparam = svc_rbf_cv_model.best_params_\nsvc_rbf = SVC(**param, kernel = 'rbf', random_state=42)\nsvc_tuned = svc_rbf.fit(X_train, y_train)\ny_pred = svc_tuned.predict(X_test)\nsvc_rbf_final = accuracy_score(y_test, y_pred)\nsvc_rbf_final","b8d31e83":"# Tuned CART Model\n\nparam = cart_cv_model.best_params_\ncart = DecisionTreeClassifier(**param, random_state=42)\ncart_tuned = cart.fit(X_train, y_train)\ny_pred = cart_tuned.predict(X_test)\ncart_final = accuracy_score(y_test, y_pred)\ncart_final","0a8b030f":"# Tuned Bagging Classifier Model\n\nparam = bagging_cv_model.best_params_\nbagging = BaggingClassifier(**param, random_state=42) \nbagging_tuned = bagging.fit(X_train, y_train)\ny_pred = bagging_tuned.predict(X_test)\nbagging_final = accuracy_score(y_test, y_pred)\nbagging_final","e20b5f4d":"# Tuned Random Forest Model \n\nparam = rf_cv_model.best_params_\nrf_tuned = RandomForestClassifier(**param, random_state=42)\nrf_tuned.fit(X_train, y_train)\ny_pred = rf_tuned.predict(X_test)\nrf_final = accuracy_score(y_test, y_pred)\nrf_final","dd5be031":"# Tuned Extra Trees Classifier Model\n\nparam = ex_tree_cv_model.best_params_\nex_tree = ExtraTreesClassifier(**param, random_state=42) \nex_tree_tuned = ex_tree.fit(X_train, y_train)\ny_pred = ex_tree_tuned.predict(X_test)\nex_tree_final = accuracy_score(y_test, y_pred)\nex_tree_final","9f135f12":"# Tuned GBC Model\n\nparam = gbc_cv_model.best_params_\ngbc = GradientBoostingClassifier(**param, random_state=42)\ngbc_tuned =  gbc.fit(X_train,y_train)\ny_pred = gbc_tuned.predict(X_test)\ngbc_final = accuracy_score(y_test, y_pred)\ngbc_final","4587fdd6":"# Tuned XGB Model\n\nparam = xgb_cv_model.best_params_\nxgb = XGBClassifier(**param, random_state=42)\nxgb_tuned =  xgb.fit(X_train,y_train)\ny_pred = xgb_tuned.predict(X_test)\nxgb_final = accuracy_score(y_test, y_pred)\nxgb_final","e9915c0e":"# Tuned XGB - Handling Missing Values Internally\n\nparam = xgb_cv_model_with_null.best_params_\nxgb_with_null = XGBClassifier(**param, random_state=42)\nxgb_tuned =  xgb_with_null.fit(X_train_with_null,y_train_with_null)\ny_pred_with_null = xgb_tuned.predict(X_test_with_null)\nxgb_final_with_null = accuracy_score(y_test_with_null, y_pred_with_null)\nxgb_final_with_null","9095a733":"# Tuned LGBM Model\n\nparam = lgbm_cv_model.best_params_\nlgbm = LGBMClassifier(**param, random_state=42)\nlgbm_tuned = lgbm.fit(X_train, y_train)\ny_pred = lgbm_tuned.predict(X_test)\nlgbm_final = accuracy_score(y_test, y_pred)\nlgbm_final","eba2eb9f":"# Tuned LGBM Model - Handling Missing Values Internally\n\nparam = lgbm_cv_model_with_null.best_params_\nlgbm_with_null = LGBMClassifier(**param, random_state=42)\nlgbm_tuned = lgbm_with_null.fit(X_train_with_null, y_train_with_null)\ny_pred_with_null = lgbm_tuned.predict(X_test_with_null)\nlgbm_final_with_null = accuracy_score(y_test_with_null, y_pred_with_null)\nlgbm_final_with_null","a6bef1fd":"# Tuned CatBoost Model\n\nparam = catboost_cv_model.best_params_\ncatboost = CatBoostClassifier(**param, random_state=42)\ncatboost_tuned = catboost.fit(X_train, y_train)\ny_pred = catboost_tuned.predict(X_test)\ncatboost_final = accuracy_score(y_test, y_pred)\ncatboost_final","38c66cc8":"# Tuned CatBoost Model- Handling Missing Values Internally\n\nparam = catboost_cv_model_with_null.best_params_\ncatboost_with_null = CatBoostClassifier(**param, random_state=42)\ncatboost_tuned = catboost_with_null.fit(X_train_with_null, y_train_with_null)\ny_pred = catboost_tuned.predict(X_test_with_null)\ncatboost_final_with_null = accuracy_score(y_test_with_null, y_pred_with_null)\ncatboost_final_with_null","b735b4dc":"# Tuned AdaBoost Model\n\nparam = ada_cv_model.best_params_\nada = AdaBoostClassifier(**param,random_state=42)\nada_tuned = ada.fit(X_train, y_train)\ny_pred = ada_tuned.predict(X_test)\nada_final = accuracy_score(y_test, y_pred)\nada_final","7d7fa685":"# Tuned MLPC Model\n\nparam = mlpc_cv_model.best_params_\nmlpc = MLPClassifier(**param, random_state=42)\nmlpc_tuned = mlpc.fit(X_train, y_train)\ny_pred = mlpc_tuned.predict(X_test)\nmlpc_final = accuracy_score(y_test, y_pred)\nmlpc_final","2b1b80d9":"accuracy_scores = {\n'log_reg_final': log_reg_final,\n'nb_final': nb_final,\n'knn_final': knn_final,\n'svc_linear_final': svc_linear_final,\n'svc_rbf_final': svc_rbf_final,\n'cart_final': cart_final,\n'bagging': bagging_final,\n'ex_tree_final': ex_tree_final,    \n'rf_final': rf_final,\n'gbc_final': gbc_final,\n'xgb_final': xgb_final,\n'xgb_final_with_null': xgb_final_with_null,\n'lgbm_final': lgbm_final,\n'lgbm_final_with_null': lgbm_final_with_null,\n'catboost_final': catboost_final,\n'catboost_final_with_null': catboost_final_with_null,\n'ada_final': ada_final,\n'mlpc_final': mlpc_final  \n}\n\naccuracy_scores = pd.Series(accuracy_scores).to_frame('Accuracy_Score')\naccuracy_scores = accuracy_scores.sort_values(by='Accuracy_Score', ascending=False)\naccuracy_scores['rank'] = (accuracy_scores.reset_index().index +1)\naccuracy_scores","a3c9de4d":"# Ensembing first 5 models","d335c4c5":"model_1 = [('Adaboost', ada),\n         ('LGBM', lgbm),\n         ('XGB', xgb),\n         ('KNN', knn),  \n         ('Random Forest', rf),\n         ('GBC', gbc),\n          ]","a869c39b":"voting_reg = VotingClassifier(model_1, voting='soft')\nvoting_reg.fit(X_train, y_train)\ny_pred = voting_reg.predict(X_test)\nprint(f\"Voting Classifier's accuracy: {accuracy_score(y_pred, y_test):.4f}\")","00bcec7d":"# Ensembing with null models","72e09b89":"model_2 = [('XGB', xgb_with_null),\n         ('LGBM', lgbm_with_null),\n          ('catboost', catboost_with_null)\n         ]","c8956f79":"voting_reg = VotingClassifier(model_2, voting='soft')\nvoting_reg.fit(X_train_with_null, y_train_with_null)\ny_pred_with_null = voting_reg.predict(X_test_with_null)\nprint(f\"Voting Classifier's accuracy: {accuracy_score(y_pred_with_null, y_test_with_null):.4f}\")","42d6767e":"### 4.3) Correlations   <a id=\"43\"><\/a>  [^](#ToC)<br>","bb8c76f9":"### 5.2.3) KNN  <a id=\"523\"><\/a>  [^](#ToC)<br>","ab2b6fa9":"### 5.2.13) LGBM - Handling Missing Values Internally <a id=\"5213\"><\/a>  [^](#ToC)<br>","8dadd84c":"## 4.1) Visual Data Exploration   <a id=\"41\"><\/a>  [^](#ToC)<br>","f7579eb7":"### 5.2.16) Multi-Layer Perceptron Classifier <a id=\"5216[](http:\/\/)\"><\/a>  [^](#ToC)<br>","2e2e1a32":"### 5.2.4) SVC  <a id=\"524\"><\/a>  [^](#ToC)<br>","0472fb74":"### 4.2) Descriptives   <a id=\"42\"><\/a>  [^](#ToC)<br>","c12ce6bc":"# 2) Retrieving Data <a id=\"2\"><\/a>  [^](#ToC)<br>","c78f65dc":"### 5.2.6) Bagging Classifier <a id=\"526[](http:\/\/)[](http:\/\/)\"><\/a>  [^](#ToC)<br>","db32d634":"### 5.2.1) Logistic Regression Model  <a id=\"521\"><\/a>  [^](#ToC)<br>","6897f4a5":"# 4) Data Exploration <a id=\"4\"><\/a>                  [^](#ToC)<br>","3c62106e":"### 5.2.15) CatBoost - Handling Missing Values Internally <a id=\"5215\"><\/a>  [^](#ToC)<br>","9f643c0f":"### 5.2.14) CatBoost <a id=\"5214\"><\/a>  [^](#ToC)<br>","9e460bab":"# 3) Data Preperation  <a id=\"3\"><\/a>  [^](#ToC)<br>","8b59072a":"### 5.2.11) XGBoost - Handling Missing Values Internally <a id=\"5211\"><\/a>  [^](#ToC)<br>","3a523dc9":"### 5.2.15) AdaBoost Classifier <a id=\"5215[](http:\/\/)\"><\/a>  [^](#ToC)<br>","b6614e03":"### 5.2.5) CART  <a id=\"525\"><\/a>  [^](#ToC)<br>","a77003d4":"# 6) Model Comparison <a id=\"6\"><\/a>                  [^](#ToC)<br>","67cbbd91":"## 5.2) Model Execution and Tuning    <a id=\"52\"><\/a>  [^](#ToC)<br><br>","91332335":"### 5.2.12) LGBM <a id=\"5212\"><\/a>  [^](#ToC)<br>","796cb27b":"# 1) Import Libraries <a id=\"1\"><\/a>                  [^](#ToC)<br>","d31b81b3":"## 5.1) Data Preparation   <a id=\"5[](http:\/\/)1\"><\/a>  [^](#ToC)<br>","a7cad2f2":"## 6.3) Ensembling Best Models <a id=\"63\"><\/a>  [^](#ToC)<br>","177527da":"### 5.2.9) Gradient Boosting Classifier <a id=\"529\"><\/a>  [^](#ToC)<br>","59261fe8":"# 5) Data Modeling <a id=\"5\"><\/a>                  [^](#ToC)<br>","1e235174":"# Building a Machine Learning Classifier Model for Pima Indians Diabetes","8e962272":"## 6.2) Accuracy Scores <a id=\"62\"><\/a>  [^](#ToC)<br>","abe90b32":"## Table of Contents <a id=\"ToC\"><\/a>\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n\n1. [Import Libraries](#1)<br> \n    \n2. [Retrieving Data](#2) <br>\n\n3. [Data Preperation](#3) <br>\n 3.1. [Physically impossible values](#31) <br>\n 3.2. [Missing values](#32) <br>\n\n4. [Data Exploration ](#4) <br>\n 4.1. [Visual Data Exploration ](#41) <br>\n 4.2. [Descriptives](#42) <br>\n 4.3. [Correlations](#43) <br>\n\n5. [Data Modeling ](#5) <br>\n 5.1. [Data Preperation](#51) <br>\n 5.2. [Model Execution and Tuning](#52) <br>\n    5.2.1. [Logistic Regression Model](#521) <br>\n    5.2.2. [Gaussian Naive Bayes](#522) <br>\n    5.2.3. [KNN](#523) <br>\n    5.2.4. [SVC](#524) <br>\n    5.2.5. [CART](#525) <br>\n    5.2.6. [Bagging Classifier Model](#526) <br> \n    5.2.7. [Ransom Forest](#527) <br>\n    5.2.8. [Extra Trees Classifier](#528) <br>\n    5.2.9. [Gradient Boosting Classifier](#529) <br>\n    5.2.10. [XGBoost](#5210) <br>\n    5.2.11. [XGBoost - Handling Missing Values Internally](#5211) <br>\n    5.2.12. [LGBM](#5212) <br>\n    5.2.13. [LGBM - Handling Missing Values Internally](#5213) <br>\n    5.2.14. [CatBoost](#5214) <br>\n    5.2.15. [CatBoost - Handling Missing Values Internally](#5215) <br>\n    5.2.16. [AdaBoost Model](#5216) <br> \n    5.2.17. [Multi-Layer Perceptron Classifier](#5217) <br>    \n6. [Model Comparison ](#6) <br>\n6.1. [Tuned Models ](#61) <br>\n6.2. [Accuracy Scores ](#62) <br>\n6.3. [Ensembling Best Models ](#63) <br>","56245be2":"### 5.2.8) Extra Trees Classifier <a id=\"528[](http:\/\/)\"><\/a>  [^](#ToC)<br>","431a71c6":"### 3.1) Physically impossible values   <a id=\"31\"><\/a>  [^](#ToC)<br>","101b7aed":"## 6.1) Tuned Models <a id=\"61\"><\/a>  [^](#ToC)<br>","7baa33d0":"### 5.2.7) Random Forest  <a id=\"527\"><\/a>  [^](#ToC)<br>","9ca24253":"### 5.2.2) Gaussian Naive Bayes  <a id=\"522\"><\/a>  [^](#ToC)<br>","90ce2283":"### 3.2) Missing values   <a id=\"32\"><\/a>  [^](#ToC)<br>","d5bb3ac4":"### 5.2.10) XGBoost  <a id=\"5210\"><\/a>  [^](#ToC)<br>"}}