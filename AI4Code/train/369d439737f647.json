{"cell_type":{"7bf47939":"code","2f6ada99":"code","0886fcac":"code","78bd8cc2":"code","8965ad1f":"code","c0b010be":"code","366fec73":"code","0d532569":"code","51c8b13c":"code","f230d73b":"code","087209ba":"code","462b7830":"code","088d5f2e":"code","5955b953":"code","f2eb2e43":"code","be8c0da3":"code","c78d34eb":"code","4d6a245d":"code","009bd2ca":"code","da1a80e2":"code","6c511cce":"code","43706172":"code","74f7f567":"code","fc8412a6":"code","d50db8d9":"code","073cf990":"code","f2bbc3e4":"code","eafb79c0":"code","43556059":"code","9143f18e":"code","35a7723d":"code","5b707537":"code","a30016a1":"code","7db9c18e":"code","0806a3f8":"code","152b090d":"code","73dd2bb2":"code","c3e456b5":"code","8ddc37c9":"code","e061878c":"code","aba351e3":"markdown","d04eea23":"markdown","89f906dd":"markdown","c05c1fe3":"markdown","1f8139d0":"markdown","21194215":"markdown","e234b255":"markdown","44368e8f":"markdown","eab63ba9":"markdown","476d3c52":"markdown","77706137":"markdown","14c6da70":"markdown","d7c45314":"markdown","4bad705e":"markdown","10dbe402":"markdown","fa65e633":"markdown","173ba19c":"markdown","9fe79e32":"markdown","404f8cf7":"markdown","7d43c922":"markdown","f2cf0b0b":"markdown","b93da30d":"markdown","b3f8b5b0":"markdown"},"source":{"7bf47939":"import pandas as pd\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\nfrom sklearn import tree\nfrom sklearn.ensemble import BaggingClassifier\nimport numpy as np\nfrom sklearn import metrics\nnp.random.seed(122)\n","2f6ada99":"stroke = pd.read_csv(\"..\/input\/stroke\/udar.csv\", sep=\";\")\nstroke.rename(columns={'wynik':'result','wiek': 'age', 'stan_kliniczny': 'clinical condition', 'cukier': 'sugar'}, inplace=True)\nstroke['result'] = [1 if i=='zgon' else 0 for i in stroke['result']]\nstroke.head(6)\n\n","0886fcac":"print(stroke.describe())","78bd8cc2":"corr=stroke.corr()\ncorr.style.background_gradient(cmap='coolwarm')","8965ad1f":"stroke['result'].sum()\n","c0b010be":"len(stroke)-stroke['result'].sum()\n","366fec73":"def draw_histograms(df, variables, n_rows, n_cols):\n    fig=plt.figure()\n    for i, var_name in enumerate(variables):\n        ax=fig.add_subplot(n_rows, n_cols,i+1)\n        df[var_name].hist(bins=10, ax=ax)\n        ax.set_title(var_name)\n    fig.tight_layout()\n    plt.show()\ndraw_histograms(stroke, stroke.columns, 3, 3)","0d532569":"for column_name in ['age', 'cholesterol', 'CPK', 'clinical condition', 'sod', 'sugar'] :\n    plt.figure()\n    stroke[column_name].plot(kind='box')\n    plt.show()","51c8b13c":"def remove_outliers(df, column_name):\n    df[column_name]=df[column_name][df[column_name]<np.quantile(df[column_name],0.99)]\n    df.dropna(inplace=True)\n    df[column_name]=df[column_name][df[column_name]>np.quantile(df[column_name],0.01)]\n    df.dropna(inplace=True)\n    return df","f230d73b":"for name in ['age', 'cholesterol', 'CPK', 'clinical condition', 'sod', 'sugar']:\n    remove_outliers(stroke,name)","087209ba":"print(stroke)","462b7830":"train, test= train_test_split(stroke, test_size=0.3, random_state=42)\n\nfeature_cols = train.columns[1:7]\nX_tr = train[feature_cols]\ny_tr = train['result']\nscaler = MinMaxScaler()\nX_tr = scaler.fit_transform(train)\n\nX_test = test[feature_cols]\ny_test = test['result']\nscaler = MinMaxScaler()\nX_test = scaler.fit_transform(test)\n\nX_tr=pd.DataFrame(X_tr)\nX_tr=X_tr.iloc[:,1:7]\ny_tr=pd.DataFrame(y_tr)\n\nX_test=pd.DataFrame(X_test)\nX_test=X_test.iloc[:,1:7]\n\n","088d5f2e":"clf = DecisionTreeClassifier().fit(X_tr, y_tr)\nX_tr.columns=stroke.columns[1:7]\n\n","5955b953":"fig = plt.figure(figsize=(20,20))\n_ = tree.plot_tree(clf, feature_names=X_tr.columns,class_names=(\"stable\",\"died\"),\n                   filled=True)","f2eb2e43":"clf2 = DecisionTreeClassifier(max_leaf_nodes=6).fit(X_tr, y_tr)\nX_tr.columns=stroke.columns[1:7]","be8c0da3":"fig = plt.figure(figsize=(15,20))\n_ = tree.plot_tree(clf2, feature_names=X_tr.columns,class_names=(\"stable\",\"died\"),\n                   filled=True)","c78d34eb":"y_pred=clf.predict(X_test)\ny_test=np.array(y_test)\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\nprint(cnf_matrix)\n","4d6a245d":"acc=(cnf_matrix[0,0]+cnf_matrix[1,1])\/cnf_matrix.sum()\nacc","009bd2ca":"def bagging(tree_num, X,y,sample_prop, y_test):\n    bag = BaggingClassifier(DecisionTreeClassifier(), n_estimators=tree_num, max_samples=sample_prop,\n                        random_state=33).fit(X,y.values.ravel())\n    y_pred=bag.predict(X_test)\n    y_test=np.array(y_test)\n    cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n    acc=(cnf_matrix[0,0]+cnf_matrix[1,1])\/cnf_matrix.sum()\n    return acc","da1a80e2":"df=pd.DataFrame()\nfor i in range(1,101):\n    df=df.append({'Accuracy':bagging(i,X_tr,y_tr,0.7,y_test),'Number of trees': i}, ignore_index=True)\nprint(df[:20])    ","6c511cce":"plt.figure()\nplt.plot(df['Number of trees'],df['Accuracy'])","43706172":"df[df['Accuracy']==max(df['Accuracy'])]","74f7f567":"df2=pd.DataFrame()\nfor i in np.arange(0.5,0.9,0.01):\n    df2=df2.append({'Accuracy':bagging(20,X_tr,y_tr,i,y_test),'Sample proportion': i}, ignore_index=True)\nprint(df2[:20]) ","fc8412a6":"df2[df2['Accuracy']==max(df2['Accuracy'])]","d50db8d9":"plt.figure()\nplt.plot(df2['Sample proportion'],df2['Accuracy'])","073cf990":"bagging(25,X_tr,y_tr,0.87,y_test)","f2bbc3e4":"from sklearn.naive_bayes import GaussianNB","eafb79c0":"gnb = GaussianNB()\ny_pred = gnb.fit(X_tr, y_tr.values.ravel()).predict(X_test)","43556059":"cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\nacc=(cnf_matrix[0,0]+cnf_matrix[1,1])\/cnf_matrix.sum()\nacc","9143f18e":"cnf_matrix","35a7723d":"from sklearn.datasets import make_blobs\nX, y = make_blobs(100, 2, centers=2, random_state=2, cluster_std=2)\nplt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu');\n","5b707537":"model = GaussianNB()\nmodel.fit(X, y);\nrng = np.random.RandomState(0)\nXnew = [-6, -14] + [14, 18] * rng.rand(2000, 2)\nynew = model.predict(Xnew)","a30016a1":"plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')\nlim = plt.axis()\nplt.scatter(Xnew[:, 0], Xnew[:, 1], c=ynew, s=20, cmap='RdBu', alpha=0.1)\nplt.axis(lim);","7db9c18e":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X_tr, y_tr.values.ravel())\ny_pred=logreg.predict(X_test)","0806a3f8":"logreg.coef_","152b090d":"logreg.intercept_","73dd2bb2":"coefs=np.append(logreg.intercept_,logreg.coef_)","c3e456b5":"data = {'Variable': ['Intercept','Age', 'Cholesterol', 'CPK', 'Clinical condition', 'Sod', 'Sugar'], 'Coefficient': coefs }    \ndf_coefs = pd.DataFrame(data)  \nprint(df_coefs)  ","8ddc37c9":"cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\nacc=(cnf_matrix[0,0]+cnf_matrix[1,1])\/cnf_matrix.sum()\nacc","e061878c":"cnf_matrix","aba351e3":"The most influential on the result is sugar and its high level determines a high risk of death. And just like in the case of decision trees, age is another differentiating variable. Along with its increase, the risk of death after a heart attack decreases statistically.","d04eea23":"## Classification methods  <a name=\"clmethods\"><\/a>\nThe project will use many methods to classify a given object into a given class. The problem will be binary, but most of the algorithms used can be used to classify into more than two numbers of classes. The project will present the mathematical basis of each method. The data set will be divided into training and testing in a ratio of 70\/30. After building the model on the training set, it will be tested on the test set.\nWill be used:\n\n\u2022 Decision trees\n\n\u2022 Bagging - extension of decision trees\n\n\u2022 Naive Bayes classifier\n\n\u2022 Logistic and probit regression\n\nThe Confusion Matrix will be used to assess the quality of the model.","89f906dd":"### Correlations between variables <a name=\"corr\"><\/a>","c05c1fe3":"#### Checking the relationship between accuracy and the number of trees in bagging.","1f8139d0":"## Data description <a name=\"datadesc\"><\/a>\n\nIn my report, I will use data on the condition of 590 stroke patients. Their condition was divided into \"deceased\" and \"stable\". The explanatory\/exogenous variables in this set are:\n\n\u2022 Age\n\n\u2022 Cholesterol level\n\n\u2022 CPK\n\n\u2022 Clinical condition\n\n\u2022 Sod\n\n\u2022\tSugar level\n\n### Basic data descriptive statistics\n","21194215":"## Conclusions <a name=\"conc\"><\/a>\n* The best match was obtained by the Bagging method, it was 91.1%.\n\n* The data set used turned out to be very consistent, even the worst result obtained by logistic regression is quite high.\n\n* On the other hand, the naive Bayes classifier did not too badly, despite its \"naivety\". It is based on the assumption that the variables are independent but in fact they are correlated with each other.\n\nDepending on the specificity of the data, other classification models can also be selected. \nFor example:\n\n\u2022 The k-nearest neighbors method\n\n\u2022 Weighted method of k-nearest neighbors\n\n\u2022 Probit Model\n\n\u2022 LDA and QDA\n\n It is also possible to build many models with different parameters (e.g. in the case of k-nearest neighbors models, enter the parameter of the number of neighbors) and then compare the results according to selected criteria.","e234b255":"339 patients in the post-stroke base were classified as stable and 221 died.","44368e8f":"### Naive Bayes <a name=\"nb\"><\/a>","eab63ba9":"The best proportion for this data set and exactly 20 trees turned out to be 87%, then the accuracy is 91.1%.","476d3c52":"The highest correlation is between the sugar level and the result (1-died, 0-stable). Next comes the negative relationship between age and death. It may seem absurd at first, but if someone has a heart attack at a young age, it may mean that he \"worked hard\" for the heart attack and his lifestyle was so unhealthy that he had less chance of survival than older patients in better condition.\nIt will probably be visible later in the analysis, e.g. in decision trees and logistic regression.","77706137":"## Classification problems <a name=\"Classificationproblems\"><\/a>\nClassification is the assignment of objects to the appropriate class on the basis of its characteristics. A given object can be assigned to only one class.\nExamples of classification problems include:\n\n\u2022 Forecasting whether the borrower will pay off the loan based on his credit history, earnings, age, place of residence and marital status\n\n\u2022 The answer to the question whether a person is at risk of a heart attack based on age, lifestyle and medical data\n\n\u2022 Predicting whether a given customer is willing to resign from the company's services (and attempting to prevent it in advance) based on their behavior and personal data\n\n### Why not linear regression?\nIn the case when the explained variable is binary, there is no point in fitting it to the data of the linear model. In this case, it is reasonable to use logit, probit or other classification methods.\n![image.png](attachment:image.png)\nsource: https:\/\/lh4.googleusercontent.com\/G_bEqIti6lEIUxtTnZjKm5v1fSFoozkoZ2PR_LzLV-kV6rmzT0q0VtEoGsgkboXHvDh2ArqZsvH6Y5fhMRithSf6JtWehNKDuKGnj_1KM_n-30CW_wOoGpvjYag3diNDE5qt7P8","14c6da70":"# Table of contents\n1. [Classification problems](#Classificationproblems)\n2. [Data Description](#datadesc)\n3. [Visualizations](#visual)\n    1. [Correlations between variables](#corr)\n    2. [Outliers](#out)\n4. [Classification methods](#clmethods)\n    1. [Decision tree](#tree)\n    2. [Bagging](#bag)\n    3. [Naive Bayes](#nb)\n    4. [Logistic Regression](#logreg)\n5. [Conclusions](#conc)","d7c45314":"### Bagging <a name=\"bag\"><\/a>","4bad705e":"In logistic Regression, the accuracy on the test set was 83.33%.","10dbe402":"It is very easy to trace such a decision tree. Blue means assignment to the death class. The more intense it is, the more likely the person will die after the stroke. Orange color means assignment to the steable state class.","fa65e633":"# Constructing prediction models for the risk of death after a stroke\n## Assessment of the quality and correctness of selected classification models\n","173ba19c":"The best accuracy was achieved for 7, 21 and 25 trees and was approximately 90%.\n#### The relationship between accuracy and the proportion of data from the test set used to build trees.","9fe79e32":"## Decision tree <a name=\"tree\"><\/a>\n","404f8cf7":"The accuracy score equals 83.33%.","7d43c922":"### Logistic Regression <a name=\"logreg\"><\/a>","f2cf0b0b":"In the naive Bayes algorithm, the accuracy on the test set was 85.1%.","b93da30d":"### Outliers <a name=\"out\"><\/a>\nThere are outliers in the data. They need to be dealt with somehow, as they can greatly distort subsequent results. Extreme observations occur in almost all variables except for CPK (creatine kinase level).\nFrom each column containing outliers, 1% of the largest and smallest observations was removed.","b3f8b5b0":"## Visualizations <a name=\"visual\"><\/a>"}}