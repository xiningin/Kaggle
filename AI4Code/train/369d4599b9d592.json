{"cell_type":{"2c1dfd47":"code","bc61a144":"code","99cec54f":"code","633b49b4":"code","082b28d8":"code","c423acf4":"code","cc9727c8":"code","d41a3c82":"code","dd6a77e1":"code","04a274a1":"code","d513d927":"code","81de3894":"code","30b57c42":"code","e29c6732":"code","af14a88d":"code","08ee88db":"code","4b06b302":"code","6f857476":"code","db30b67b":"code","2a9438b3":"code","c68bb013":"code","74bb8ff0":"code","6103cfcd":"code","286ede15":"code","4da33a15":"code","74d610a5":"code","a8ad2dff":"code","6382e2de":"code","96a48c34":"code","878917fa":"code","7811a66a":"code","92d4e3da":"markdown","9b1daf35":"markdown","bf167427":"markdown","1a6ba55f":"markdown","317e5237":"markdown","d4e82a0e":"markdown","54640a79":"markdown","1f321af4":"markdown","39ce214e":"markdown","09ace383":"markdown","86c491f8":"markdown","9c0e3292":"markdown","001553dc":"markdown","51f5e54f":"markdown","ecd51db3":"markdown","46cd8075":"markdown","2cfda945":"markdown","a73c8435":"markdown","b7e8beb8":"markdown"},"source":{"2c1dfd47":"import pandas as pd\npd.set_option('max_colwidth', 400)\nimport numpy as np\nimport re\nfrom unidecode import unidecode\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, log_loss,confusion_matrix\n\nimport spacy\nimport nltk\nfrom nltk import FreqDist\nfrom gensim.models.phrases import Phrases, Phraser\nfrom gensim.models import Word2Vec\n\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\n#plt.style.use('seaborn')\nimport seaborn as sns\n%matplotlib inline\n\nimport  warnings\nwarnings.filterwarnings(\"ignore\")","bc61a144":"# =============================================================================\n# Importa\u00e7\u00e3o de Todos os Sub-datasets\n# =============================================================================\nfiles = {'customers'    : '\/kaggle\/input\/brazilian-ecommerce\/olist_customers_dataset.csv',\n         'geolocation'  : '\/kaggle\/input\/brazilian-ecommerce\/olist_geolocation_dataset.csv',\n         'items'        : '\/kaggle\/input\/brazilian-ecommerce\/olist_order_items_dataset.csv',\n         'payment'      : '\/kaggle\/input\/brazilian-ecommerce\/olist_order_payments_dataset.csv',\n         'orders'       : '\/kaggle\/input\/brazilian-ecommerce\/olist_orders_dataset.csv',\n         'products'     : '\/kaggle\/input\/brazilian-ecommerce\/olist_products_dataset.csv',\n         'sellers'      : '\/kaggle\/input\/brazilian-ecommerce\/olist_sellers_dataset.csv',\n         'review'       : '\/kaggle\/input\/brazilian-ecommerce\/olist_order_reviews_dataset.csv',\n         }\n\ndfs = {}\nfor key, value in files.items():\n    dfs[key] = pd.read_csv(value)\n    \nfor k, v in dfs.items():\n    print(f'{k}: {v.shape}')","99cec54f":"# =============================================================================\n# Sele\u00e7\u00e3o do Dataset relevante\n# =============================================================================\ndf = dfs['customers'].merge(dfs['orders'], how='left', on='customer_id')\ndf = df.merge(dfs['review'], how='left', on='order_id')\n\nnp.random.seed(100)\ndf.sample(1).T","633b49b4":"# =============================================================================\n# Comprimento das senten\u00e7as\n# =============================================================================\ndf['review_comment_message'].str.len().describe()","082b28d8":"print('\\nFormato do dataset antes da remo\u00e7\u00e3o de duplicados e nulos:', df.shape)\n\n# =============================================================================\n# Quantidade de textos vazios\n# =============================================================================\nprint('Avalia\u00e7\u00f5es nulas: ', df['review_comment_message'].isnull().sum())\n\n# =============================================================================\n# Quantidade de registros duplicados\n# =============================================================================\nprint('Registros duplicados: ', df['review_comment_message'].duplicated(keep=False).sum())\n\n# =============================================================================\n# Adequa\u00e7\u00f5es finais\n# =============================================================================\ndf = df[~df['review_comment_message'].isna()].reset_index(drop=True)\ndf = df[df['review_comment_message'].str.contains(\"\\w\")]\ndf = df[df['review_comment_message'].str.len() > 5]\ndf = df.drop_duplicates('review_comment_message').reset_index(drop=True)\n\nprint('\\nFormato do dataset ap\u00f3s remo\u00e7\u00e3o de duplicados e nulos:', df.shape)\nprint('Avalia\u00e7\u00f5es nulas: ', df['review_comment_message'].isnull().sum())\nprint('Registros duplicados: ', df['review_comment_message'].duplicated(keep=False).sum())","c423acf4":"print(df['review_comment_message'].str.len().describe())\nprint(\"\\nSenten\u00e7as de 6 caracteres\")\nprint(df['review_comment_message'][df['review_comment_message'].str.len() == 6].head())","cc9727c8":"rating_counts = df['review_score'].value_counts().reset_index().sort_values('index').iloc[:,1].tolist()\nrating_p = round(df['review_score'].value_counts(normalize=True).reset_index().sort_values('index').iloc[:,1] * 100, 1).apply(lambda x: '{} %'.format(x)).tolist()\n\nfig, ax = plt.subplots(figsize=(12,7))\nax = sns.countplot(x=df['review_score'])\nplt.xticks(fontsize=12)\nplt.title('Distrui\u00e7\u00e3o das Classes de Avalia\u00e7\u00e3o',fontsize=20)\nax.set_ylabel('')\nax.set_xlabel('Nota da Avalia\u00e7\u00e3o', size=15)\nfor i, v in enumerate(rating_counts):\n    ax.text(i-0.2, v+400, rating_p[i], size=20)","d41a3c82":"# =============================================================================\n# Balanceamento\n# =============================================================================\npos = df[['review_comment_message', 'review_score']][df['review_score'] == 5]\nneg = df[['review_comment_message', 'review_score']][df['review_score'] <= 2]\n\ndf_ml = pd.concat([pos, neg])\nmaping = {5: 'Positivo'\n         ,2: 'Negativo'\n         ,1: 'Negativo'}\ndf_ml = df_ml.replace(maping)\n\nnp.random.seed(seed=100)\npos = np.random.choice(df_ml['review_comment_message'][df_ml['review_score'] == 'Positivo'], 10000, replace=False)\nneg = np.random.choice(df_ml['review_comment_message'][df_ml['review_score'] == 'Negativo'], 10000, replace=False)\nselect = np.concatenate([pos, neg])\n\ndf_ml = df_ml[df_ml['review_comment_message'].isin(select)].reset_index(drop=True)\ndf_ml = df_ml.sample(frac=1).reset_index(drop=True)\n\nprint(df_ml['review_score'].value_counts(normalize=True))\nprint(\"\\n\", df_ml.shape)","dd6a77e1":"# =============================================================================\n# Instala\u00e7\u00e3o do pacote em Portugu\u00eas do Spacy\n# =============================================================================\n!python -m spacy download pt","04a274a1":"# =============================================================================\n# Classe para Preprocessamento de Texto\n# =============================================================================\nnlp = spacy.load('pt', disable=['parser', 'ner'])\nstemmer = nltk.stem.RSLPStemmer()\n\nclass DataPrep:\n            \n    def __init__(self):\n        print('DataPrep ready.')\n        \n    def remove_stopwords(self, texto):\n        \"\"\" Fun\u00e7\u00e3o para remover stopwords e outras palavras predefinidas\"\"\"\n        stop_words = [word for word in nlp.Defaults.stop_words]\n                \n        #remover = ['lojas', 'americanas', 'americana', 'blackfriday', 'black', 'friday']\n        \n        #stop_words.extend(remover)\n        \n        \n        \n        texto_limpo = \" \".join([i for i in texto if i not in set(stop_words)])\n        return texto_limpo\n    \n    def clean_text(self, texto):\n        \"\"\" Fun\u00e7\u00e3o para aplicar a remo\u00e7\u00e3o de stopwords, caracteres n\u00e3o alfab\u00e9ticos e outras palavras curtas\"\"\"\n        df_corpus = []\n        for i in range(len(texto)):\n            df_c = re.sub('[^A-Za-z\u00e1\u00e0\u00e2\u00e3\u00e9\u00e8\u00ea\u00ed\u00ef\u00f3\u00f4\u00f5\u00f6\u00fa\u00e7\u00f1\u00c1\u00c0\u00c2\u00c3\u00c9\u00c8\u00cd\u00cf\u00d3\u00d4\u00d5\u00d6\u00da\u00c7\u00d1]', ' ', texto[i]).lower().split()\n            df_corpus.append(df_c)\n        df_corpus= pd.Series(df_corpus).apply(lambda x: ' '.join([w for w in x if len(w)>2]))\n        corpus = [self.remove_stopwords(r.split()) for r in df_corpus]\n        return corpus\n\n    def lemmatization(self, texto):\n        \"\"\" Fun\u00e7\u00e3o para extrair o lema das palavras\"\"\"\n        global nlp        \n        output = []\n        for sent in texto:\n            doc = nlp(\" \".join(sent)) \n            output.append([token.lemma_ for token in doc])\n        return output\n\n    def lemmatize(self, texto):\n        \"\"\" Fun\u00e7\u00e3o para aplicar a limpeza do texto e a lemmatiza\u00e7\u00e3o\"\"\"\n        token = self.lemmatization(pd.Series(self.clean_text(texto)).apply(lambda x: x.split()))\n        token_lemma = []\n        for i in range(len(token)):\n            token_lemma.append(' '.join(token[i]))\n        return token_lemma\n    \n    def list_freq(self, texto, terms=30):\n        \"\"\" Fun\u00e7\u00e3o para listar palavras mais frequentes\"\"\"\n        all_words = ' '.join([text for text in texto])\n        all_words = all_words.split()\n        fdist = FreqDist(all_words)\n        words_df = pd.DataFrame({'word':list(fdist.keys()), 'count':list(fdist.values())})\n        d = words_df.nlargest(columns=\"count\", n=terms) \n        return d\n        print(d[:terms])\n           \n    def steming(self, texto):\n        \"\"\" Fun\u00e7\u00e3o extrair a raiz (stem) das palavras\"\"\"\n        global stemmer\n        output_1 = []\n        for sent in texto:\n            doc = \" \".join(sent)\n            output_2 = []\n            for w in doc.split():\n                f = stemmer.stem(w)  \n                output_2.append(f)\n            output_1.append(output_2)    \n        return output_1\n        \n    def stemize(self, texto):\n        \"\"\" Fun\u00e7\u00e3o para aplicar o steming\"\"\"\n        token = self.steming(pd.Series(self.clean_text(texto)).apply(lambda x: x.split()))\n        token_lemma = []\n        for i in range(len(token)):\n            token_lemma.append(' '.join(token[i]))\n        return token_lemma \n    \n    def rm_accents(self, texto) -> list:\n        '''A fun\u00e7\u00e3o ir\u00e1 remover acentos e cedilha'''\n        fixed = list()\n        for linha in texto:\n            unidecoded_text = unidecode(linha)\n            fixed.append(unidecoded_text)\n        return fixed","d513d927":"%%time\n# =============================================================================\n# Pr\u00e9-processamento do corpus\n# =============================================================================\ndp = DataPrep()\n\ndf_ml['review_comment_message'] = dp.rm_accents(df_ml['review_comment_message'])\ncorpus = dp.lemmatize(df_ml['review_comment_message'])","81de3894":"# =============================================================================\n# Bag of Words\n# =============================================================================\nfrom PIL import Image\nfrom wordcloud import WordCloud\n\nword = ' '.join(corpus)\n\nwc = WordCloud(\n    background_color='black',\n    max_words=2000\n)\nwc.generate(word)\n\nfig = plt.subplots(figsize=(8,8))\nplt.imshow(wc, interpolation='bilinear')\nplt.axis('off')\nplt.show()","30b57c42":"%%time\n# =============================================================================\n# Vetoriza\u00e7\u00e3o TF-IDF\n# =============================================================================\nvectorizer = TfidfVectorizer(min_df=2, max_df=0.75, analyzer='word',\n                             strip_accents='unicode', use_idf=True,\n                             ngram_range=(1,2), max_features=10000)\n\nX = vectorizer.fit_transform(corpus).toarray()\ny = df_ml.loc[:,\"review_score\"].values\n\nprint('X shape: ', X.shape, '\\ny shape: ', y.shape)","e29c6732":"feature_names = vectorizer.get_feature_names()\nfeature_names[:20]","af14a88d":"%%time\n# =============================================================================\n# Splitting the dataset into the Training set and Test set\n# =============================================================================\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# =============================================================================\n# Fitting Naive Bayes to the Training set\n# =============================================================================\nclassifier_nb = MultinomialNB()\nclassifier_nb.fit(X_train, y_train)\n# =============================================================================\n# Predicting the Test set results\n# =============================================================================\ny_pred = classifier_nb.predict(X_test)\ny_pred_prob = classifier_nb.predict_proba(X_test)\n# =============================================================================\n# Model Evaluation\n# =============================================================================\nprint('Acur\u00e1cia de: ', \"{0:.1f}\".format(accuracy_score(y_test, y_pred)*100),'%')\nprint('Log Loss de: ', round(log_loss(y_test, y_pred_prob, eps=1e-15),4))","08ee88db":"# =============================================================================\n# Matriz de Confus\u00e3o\n# =============================================================================\ncm = confusion_matrix(y_test, y_pred, labels=['Positivo', 'Negativo'])\n'''\ndefault:\n[[TN, FP,\n  FN, TP]]\n  \nlabels = [1,0]:\n[[TP, FP,\n  FN, TN]]\n'''\n\nfig, ax = plt.subplots(figsize=(7,6))\nsns.heatmap(cm, annot=True, ax=ax, fmt='.0f'); #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels')\nax.set_ylabel('Real labels')\nax.set_title('Confusion Matrix')\nax.xaxis.set_ticklabels(['Positivo', 'Negativo'])\nax.yaxis.set_ticklabels(['Positivo', 'Negativo']);\nplt.show()\n\nsens = cm[0,0] \/ (cm[0,0] + cm[1,0])\nesp = cm[1,1] \/ (cm[0,1] + cm[1,1])\n\nprint('Sensibilidade: ', round(sens,2))\nprint('Especificidade: ', round(esp,2))","4b06b302":"import random\nrandom.seed(5)\n# =============================================================================\n# Teste de entrada no modelo\n# =============================================================================\ntestes = list()\nfor i in range(5):\n    testes.append(random.choice(df_ml['review_comment_message']))\n\n# =============================================================================\n# Prepara\u00e7\u00e3o dos dados\n# =============================================================================\ndp = DataPrep()\ntestes = dp.rm_accents(testes)\ncorpus2 = dp.lemmatize(testes)\n\n# =============================================================================\n# Resultados\n# =============================================================================    \nprint(\"\\nClassifica\u00e7\u00e3o Predita:\")\ntestes_transform = vectorizer.transform(corpus2)\nfor i in range(len(testes)):\n    print(\"{} {:-<16} {}\".format([i+1], classifier_nb.predict(testes_transform)[i], testes[i]))\n\nprint(\"\\nProbabilidaes:\")\ntestes_transform = vectorizer.transform(corpus2)\nfor i in range(len(testes)):\n    print(\"{} {:-<16} {}\".format([i+1], str([round(x,2) for x in classifier_nb.predict_proba(testes_transform)[i].tolist()]), testes[i]))","6f857476":"# =============================================================================\n# Fun\u00e7\u00f5es para Modelagem de T\u00f3picos\n# =============================================================================\n\n# Coletar t\u00f3picos e seus pesos\ndef get_topics_terms_weights(weights, feature_names):\n    feature_names = np.array(feature_names)\n    sorted_indices = np.array([list(row[::-1]) for row in np.argsort(np.abs(weights))])\n    sorted_weights = np.array([list(wt[index]) for wt, index in zip(weights, sorted_indices)])\n    sorted_terms = np.array([list(feature_names[row]) for row in sorted_indices])\n    \n    topics = [np.vstack((terms.T, term_weights.T)).T for terms, term_weights in zip(sorted_terms, sorted_weights)]\n    \n    return topics\n\n\n# Imprimir os componentes de cada t\u00f3pico\ndef print_topics_udf(topics, total_topics=1,\n                     weight_threshold=0.0001,\n                     display_weights=False,\n                     num_terms=None):\n    \n    for index in range(total_topics):\n        topic = topics[index]\n        topic = [(term, float(wt))\n                 for term, wt in topic]\n        #print(topic)\n        topic = [(word, round(wt,2)) \n                 for word, wt in topic \n                 if abs(wt) >= weight_threshold]\n                     \n        if display_weights:\n            print('Topic #'+str(index)+' with weights')\n            print(topic[:num_terms]) if num_terms else topic\n        else:\n            print('Topic #'+str(index+1)+' without weights')\n            tw = [term for term, wt in topic]\n            print(tw[:num_terms]) if num_terms else tw\n\ndef get_topics_udf(topics, total_topics=1,\n                     weight_threshold=0.0001,\n                     num_terms=None):\n    \n    topic_terms = []\n    \n    for index in range(total_topics):\n        topic = topics[index]\n        topic = [(term, float(wt))\n                 for term, wt in topic]\n        #print(topic)\n        topic = [(word, round(wt,2)) \n                 for word, wt in topic \n                 if abs(wt) >= weight_threshold]\n        \n        topic_terms.append(topic[:num_terms] if num_terms else topic)\n\n    return topic_terms","db30b67b":"%%time\n# =============================================================================\n# Non-Negative Matrix Factorization (NMF)\n# =============================================================================\n'''\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.NMF.html\nsklearn.decomposition.NMF(n_components=None, init=None, solver='cd', beta_loss='frobenius', \n                          tol=0.0001, max_iter=200, random_state=None, alpha=0.0, l1_ratio=0.0, \n                          verbose=0, shuffle=False)[source]\n'''\nfrom sklearn.decomposition import NMF\nnum_topics = 7\n\nnmf = NMF(n_components=num_topics, random_state=42, alpha=0.1, l1_ratio=0.5, init='nndsvd')\nnmf.fit(X)\n\nnmf_weights = nmf.components_\nnmf_feature_names = vectorizer.get_feature_names()\n\nprint(nmf)","2a9438b3":"# =============================================================================\n# T\u00f3picos e pesos de suas principais palavras \n# =============================================================================\ntopics = get_topics_terms_weights(nmf_weights, nmf_feature_names)\nprint_topics_udf(topics, total_topics=num_topics, num_terms=5, display_weights=True)","c68bb013":"# =============================================================================\n# Transforma\u00e7\u00e3o e inser\u00e7\u00e3o dos t\u00f3picos no Dataset\n# =============================================================================\ntopic_values = nmf.transform(X)\ndf_ml['topic'] = topic_values.argmax(axis=1)\n\nlabels = {0:'Entrega'\n         ,1:'Recebimento do Produto'\n         ,2:'Entrega'\n         ,3:'Qualidade do Produto'\n         ,4:'Recomenda\u00e7\u00e3o da Loja\/Vendedor' \n         ,5:'Entrega' \n         ,6:'Entrega'\n}\n\ndf_ml = df_ml.replace(labels)\nnp.random.seed(seed=2)\ndf_ml.sample(15)","74bb8ff0":"# =============================================================================\n# Distribui\u00e7\u00e3o Geral\n# =============================================================================\nprint(\"Distribui\u00e7\u00e3o geral do corpus:\\n\", df_ml['topic'].value_counts(normalize=True))\n\n# =============================================================================\n# Distribui\u00e7\u00e3o Avalia\u00e7\u00f5es Negativas\n# =============================================================================\nprint(\"\\nDistribui\u00e7\u00e3o das avalia\u00e7\u00f5es Negativas:\\n\", df_ml['topic'][df_ml['review_score'] == 'Negativo'].value_counts(normalize=True))\n\n# =============================================================================\n# Distribui\u00e7\u00e3o Avalia\u00e7\u00f5es Positivas\n# =============================================================================\nprint(\"\\nDistribui\u00e7\u00e3o das avalia\u00e7\u00f5es Positivas:\\n\", df_ml['topic'][df_ml['review_score'] == 'Positivo'].value_counts(normalize=True))","6103cfcd":"%%time\nsent = [line.split() for line in corpus]\nphrases = Phrases(sent, min_count=1, threshold=2, progress_per=1000) \nbigram = Phraser(phrases)\nsentences = bigram[sent]","286ede15":"# =============================================================================\n# Construindo o Modelo\n# =============================================================================\nw2v_model = Word2Vec(min_count=20\n                    ,window=3\n                    ,sg=0\n                    ,size=100\n                    ,sample=6e-5\n                    ,alpha=0.03\n                    ,min_alpha=0.0007\n                    ,negative=20\n                    ,workers=7\n                    ,seed=42)","4da33a15":"%%time\nw2v_model.build_vocab(sentences, progress_per=100)","74d610a5":"%%time\nw2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)","a8ad2dff":"w2v_model.init_sims(replace=True)\nprint(\"Model has %d terms\" % len(w2v_model.wv.vocab))","6382e2de":"# =============================================================================\n# Termos mais similares\n# =============================================================================\nw2v_model.wv.most_similar(positive=[\"recomendar\"])","96a48c34":"# =============================================================================\n# Similaridade entre dois termos\n# =============================================================================\nw2v_model.wv.similarity(\"comprar\", 'atrasar')","878917fa":"from sklearn.manifold import TSNE\ndef display_closestwords_tsnescatterplot(model, word, n):\n    \n    arr = np.empty((0,100), dtype='f')\n    word_labels = [word]\n\n    # get close words\n    close_words = model.wv.similar_by_word(word, topn=n)\n    \n    # add the vector for each of the closest words to the array\n    arr = np.append(arr, np.array([model[word]]), axis=0)\n    for wrd_score in close_words:\n        wrd_vector = model[wrd_score[0]]\n        word_labels.append(wrd_score[0])\n        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n        \n    # find tsne coords for 2 dimensions\n    tsne = TSNE(n_components=2, random_state=0)\n    np.set_printoptions(suppress=True)\n    Y = tsne.fit_transform(arr)\n    \n    fig = plt.figure(figsize=(15,15))\n    x_coords = Y[:, 0]\n    y_coords = Y[:, 1]\n    \n    # display scatter plot\n    plt.scatter(x_coords, y_coords)\n    \n    k=1\n    for label, x, y in zip(word_labels, x_coords, y_coords):\n        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n    plt.xlim(x_coords.min()-k, x_coords.max()+k)\n    plt.ylim(y_coords.min()-k, y_coords.max()+k)\n    plt.show()","7811a66a":"display_closestwords_tsnescatterplot(w2v_model, 'entregar', 100)","92d4e3da":"* Leandro Alencar \u2013 1931133007\n* Maycon Alves \u2013 1931133015\n* Nilson Michiles - 1931133032","9b1daf35":"<img style=\"float: left;\" src=\"http:\/\/sindser.org.br\/s\/wp-content\/uploads\/2013\/09\/iesb1.jpg\"  width=\"400\" height=\"400\">\n\n## Instituto de Educa\u00e7\u00e3o Superior de Bras\u00edlia\n## P\u00f3s Gradua\u00e7\u00e3o em Ci\u00eancia de Dados\n## T\u00f3picos Avan\u00e7ados em ML e DL","bf167427":"# 4. An\u00e1lise de Sentimentos <a name=\"4\"><\/a> \n## 4.1. Corpus\nUm corpus pode ser conceituado como um conjunto de textos escritos e registros orais em uma determinada l\u00edngua e que serve como base de an\u00e1lise. Para fins de compara\u00e7\u00e3o, em ci\u00eancia da computa\u00e7\u00e3o, um corpus \u00e9 semelhante a um dataset, entretanto, corpus \u00e9 mais comentado em Processamento de Linguagem Natural (PLN) por ser mais utilizado na \u00e1rea de lingu\u00edstica, e um conjunto de corpus \u00e9 chamado de corpora. Ter um corpus \u00e9 a primeira etapa para iniciar um processo de an\u00e1lise de PLN.\n\nPara organizarmos um corpus, iremos realizar o pr\u00e9-processamento dos textos com a classe e suas fun\u00e7\u00f5es abaixo, que ir\u00e3o remover caract\u00e9res n\u00e3o alfab\u00e9ticos e acentua\u00e7\u00f5es, transformar todas as letras em min\u00fasculas e \"lemmatizar\" as palavras.\n\nLemmatization \u00e9 um termo em ingl\u00eas que n\u00e3o possui tradu\u00e7\u00e3o, mas, em lingu\u00edstica, diz respeito ao processo de agrupar as formas flexionadas de uma palavra para que possam ser analisadas como um \u00fanico item, identificado pelo \"lema\" (ou raiz) da palavra. Na lingu\u00edstica computacional, \u00e9 o processo algor\u00edtmico de determinar o lema de uma palavra com base no significado pretendido. Depende da identifica\u00e7\u00e3o correta da parte pretendida da fala e do significado de uma palavra em uma frase, bem como do contexto maior em torno dessa frase, como frases vizinhas ou mesmo um documento inteiro. Existe outra t\u00e9cnica mais simples que tamb\u00e9m busca o mesmo prop\u00f3sito, o \"stemming\", mas n\u00e3o ser\u00e1 utilizada.","1a6ba55f":"# 7. Conclus\u00e3o <a name=\"7\"><\/a>","317e5237":"### 4.2.1. N-Gram\nO par\u00e2metro \"ngram_range\" diz respeito a qual o comprimento pretendido para os n-gramas, que ser\u00e3o nossas colunas. Um n-grama corresponde a um token, ou unidade do termo dentro de uma senten\u00e7a, e pode ser de tamanho 1, unigrama, de tamanho 2, bigrama, de tamanho 3, trigrama e de tamanho 4 em diante s\u00e3o chamados de n-grama. O \"ngram_range\" de 1 a 2 significa que teremos unigramas e bigramas compondo as colunas na nossa matriz de Bag of Words. Por exemplo, abaixos h\u00e1 uma sele\u00e7\u00e3o dos 20 primeiros tokens, incluindo unigramas e bigramas:","d4e82a0e":"----\n## Bibliotecas\n- Pandas \u2013 Manipula\u00e7\u00e3o e an\u00e1lise de dados \n- Numpy \u2013  \u00c1lgebra linear\n- re \u2013 Express\u00f5es regulares\n- Scikit-learn - Aprendizado de m\u00e1quina\n- Matplotlib \u2013 Visualiza\u00e7\u00e3o gr\u00e1fica\n- Seaborn - Visualiza\u00e7\u00e3o gr\u00e1fica    \n\n\n> Processamento de Linguagem Natural:\n- Spacy\n- NLTK\n- Gensim\n\n----","54640a79":"## 3.2. Balanceamento de Classes\nO balanceamento de classes observa a harmonia da distribui\u00e7\u00e3o das observa\u00e7\u00f5es sobre as quais iremos realizar classifica\u00e7\u00f5es preditivas e o desbalanceamento pode acarretar problemas de enviesamento do modelo. Para evitar este problema, o balanceamento das classes ser\u00e1 realizado com a diminui\u00e7\u00e3o do corpus para igualar a quantidade de classes, onde ser\u00e3o selecionadas as avalia\u00e7\u00f5es de pontua\u00e7\u00e3o 2 e 1 como negativas, enquanto as de pontua\u00e7\u00e3o 5 ser\u00e3o definidas como positivas. De cada conjunto ser\u00e3o selecionadas 20.000 amostras, resultando em um corpus igualmente distribu\u00eddo.","1f321af4":"# 4.3. Multinomial Naive Bayes\n\nConforme a pr\u00f3pria defini\u00e7\u00e3o no algor\u00edtimo pelo Scikit-Learn, os m\u00e9todos ing\u00eanuos (\"naive\") de Bayes s\u00e3o um conjunto de algoritmos de aprendizado supervisionado baseados na aplica\u00e7\u00e3o do teorema de Bayes com a suposi\u00e7\u00e3o \"ing\u00eanua\" de independ\u00eancia condicional entre cada par de recursos, dado o valor da vari\u00e1vel de classe:\n\n$$P(xi | y)$$\n\nO classificador multinomial Naive Bayes \u00e9 adequado para classifica\u00e7\u00e3o com recursos discretos (por exemplo, contagem de palavras para classifica\u00e7\u00e3o de texto). A distribui\u00e7\u00e3o multinomial normalmente requer contagens de recursos inteiros, no entanto, na pr\u00e1tica, contagens fracion\u00e1rias como tf-idf tamb\u00e9m podem funcionar.","39ce214e":"# 3. An\u00e1lises Explorat\u00f3rias <a name=\"3\"><\/a>\nEm primeiro lugar faremos an\u00e1lises descritiva para compreender o comprimento das senten\u00e7as que ir\u00e3o compor o nosso corpus, bem como a ocorr\u00eancias de avalia\u00e7\u00f5es nulas ou em branco e a distribui\u00e7\u00e3o das classes referentes \u00e0s notas das avalia\u00e7\u00f5es. \u00c9 necess\u00e1rio corrigir quaisquer distor\u00e7\u00f5es com o intuito de preservar a assertividade dos resultados.","09ace383":"Processamento de Linguagem Natural \u00e9 um ramo da Intelig\u00eancia Artificial o qual consiste em utilizar t\u00e9cnicas que possibilitem a intera\u00e7\u00e3o entre computadores e seres humanos usando a linguagem natural, com o objetivo final de ler, decifrar, entender e entender as linguagens humanas de uma maneira valiosa. Foi poss\u00edvel perceber grande capacidade de aplica\u00e7\u00f5es interessantes como Word Embeddings, Modelagem de T\u00f3picos e An\u00e1lise de Sentimento.\n\nA An\u00e1lise de Sentimento apresentou resultados muito satisfat\u00f3rios. Foi poss\u00edvel treinar o modelo de classifica\u00e7\u00e3o em um rico conjundo de dados da L\u00edngua Portuguesa e, em conjunto, realizar Modelagem de T\u00f3picos para descobrir que \"Entrega\" \u00e9 um motivo de \"amor e \u00f3dio\" os consumidores: apresentou a maior concentra\u00e7\u00e3o de avalia\u00e7\u00f5es tanto positivas (entregues antes do prazo), quanto negativas (prazo n\u00e3o cumprido ou at\u00e9 mesmo entrega n\u00e3o realizada).\n\n\u00c9 interessante e motivador perceber que transformar texto em vetores num\u00e9ricos para que o computador possa interpret\u00e1-los, apesar de parecer um tema complexo, para alguns casos \u00e9 uma atividade capaz de ser realizada com relativa facilidade, sem a necessidade de aplica\u00e7\u00f5es com Redes Neurais mais complexas.","86c491f8":"# 6. Word2Vec <a name=\"6\"><\/a>\n### Word Embeddings\nO Word2vec \u00e9 um grupo de modelos relacionados que s\u00e3o usados para produzir incorpora\u00e7\u00f5es de palavras (*word embeddings*). Esses modelos s\u00e3o redes neurais rasas de duas camadas, treinadas para reconstruir contextos lingu\u00edsticos de palavras. O Word2Vec usa como entrada um grande corpus de texto e produz um espa\u00e7o vetorial, tipicamente com v\u00e1rias centenas de dimens\u00f5es, com cada palavra exclusiva no corpus sendo atribu\u00edda a um vetor correspondente no espa\u00e7o. Os vetores de palavras s\u00e3o posicionados no espa\u00e7o vetorial, de modo que as palavras que compartilham contextos comuns no corpus sejam localizadas pr\u00f3ximas umas das outras no espa\u00e7o.\n\nIremos aplicar este modelo no corpus para encontrar similaridades entre os termos mais frequentes.","9c0e3292":"# 4.2. Bag of Words\nO modelo de classifica\u00e7\u00e3o utiliza o m\u00e9todo de Bag of Words (Saco de Palavras), que consiste em criar uma matriz em que as colunas correspondem a cada palavra encontrada no corpus, ap\u00f3s o pr\u00e9-processamento, e as linhas representam os documentos (nesse caso, cada uma das avalia\u00e7\u00f5es). Dessa maneiras, caso a palavra conste no documento, ser\u00e1 atribu\u00eddo valor 1 para aquela coluna, caso contr\u00e1rio o valor ser\u00e1 0. \n\nNo entanto, n\u00e3o utilizaremos uma contagem simples como exemplificado anteriormente (1 ou 0), iremos aplicar a estat\u00edstica do TF-IDF Vectorizer que utiliza a frequ\u00eancia do termo dentro do texto e entre todos os textos do corpus para atribuir um peso estat\u00edstico capaz de refletir a import\u00e2ncia daquela palavra.","001553dc":"# 5. Modelagem de T\u00f3picos <a name=\"5\"><\/a>\n### Non-Negative Matrix Factorization\nA fatora\u00e7\u00e3o de matriz n\u00e3o negativa (NMF ou NNMF), \u00e9 um grupo de algoritmos em an\u00e1lise multivariada e \u00e1lgebra linear em que uma matriz V \u00e9 fatorada (geralmente) em duas matrizes W e H, com a propriedade de que todas as tr\u00eas matrizes n\u00e3o t\u00eam elementos negativos. Essa n\u00e3o-negatividade torna as matrizes resultantes mais f\u00e1ceis de inspecionar. Al\u00e9m disso, em aplica\u00e7\u00f5es como processamento de espectrogramas de \u00e1udio ou atividade muscular, a n\u00e3o-negatividade \u00e9 inerente aos dados considerados. Como o problema n\u00e3o \u00e9 exatamente solucion\u00e1vel em geral, geralmente \u00e9 aproximado numericamente. A NMF encontra aplica\u00e7\u00f5es em campos como astronomia, vis\u00e3o computacional, agrupamento de documentos, quimiometria, processamento de sinais de \u00e1udio, sistemas de recomenda\u00e7\u00e3o, e bioinform\u00e1tica.\n\nPara esta etapa, primeiramente foi definido o n\u00famero de 3 t\u00f3picos para serem selecionados, por\u00e9m acarretou em muitas ocorr\u00eancias de classifica\u00e7\u00f5es err\u00f4neas. A estrat\u00e9gia adotada, ent\u00e3o, foi de aumentar o n\u00famero de t\u00f3picos do NMF a fim de segregar melhor as diferen\u00e7as, juntando, manualmente, t\u00f3picos semelhantes e terminando esta etapa de classifica\u00e7\u00e3o com um total de 4 categorias. Essa estrat\u00e9gia mostrou-se bastante eficiente e reduziu muito o n\u00famero de classifica\u00e7\u00f5es incoerentes.","51f5e54f":"# 4.4. Resultados do Modelo\n\nNosso modelo de classifica\u00e7\u00e3o multinomial Naive Bayes foi treinado sobre uma matriz de 20.000 linhas (senten\u00e7as) e 10.000 colunas (termos n-grama), com sele\u00e7\u00e3o de 20% dos dados para teste e demonstrou um desempenho extremamente satisfat\u00f3rio, com acur\u00e1cia igual a 92,2%, sensibilidade de 95% e especificidade de 90%.\n\nDurante o desenvolvimento do modelo, percebemos que a utiliza\u00e7\u00e3o de bigramas aumentou a acur\u00e1cia em 1% em rela\u00e7\u00e3o a utilizar somente unigramas, bem como diminuiu a perda logar\u00edtmica (\"log loss\"), que demonstra a \"certeza\" do modelo ao definir uma classifica\u00e7\u00e3o entre positiva e negativa.\n\nEstat\u00edstica    | Valor\n:------------  | -------------\nAcur\u00e1cia       | 92,2%\nSensibilidade  | 95%\nEspecificidade | 90%","ecd51db3":"----\n# An\u00e1lise de Sentimento Olist Dataset\n### Table of contents\n* [1. Introdu\u00e7\u00e3o](#1)\n* [2. Dados](#2)\n* [3. An\u00e1lises Explorat\u00f3rias](#3)\n* [4. An\u00e1lise de Sentimentos](#4)\n* [5. Modelagem de T\u00f3picos](#5)\n* [6. Word Embeddings - Word2Vec](#6)\n* [7. Conclus\u00e3o](#7)\n----","46cd8075":"## 3.1. Corpus selecionado\nFoi poss\u00edvel identificar ocorr\u00eancias de textos de avalia\u00e7\u00f5es com comprimentos de apenas 1 caractere, avalia\u00e7\u00f5es nulas e registros duplicados.\nAp\u00f3s remorvemos todos os registros duplicados e nulos, e estabelecermos o comprimento m\u00ednimo de 6 caracteres, terminamos com um conjunto de dados de 36.698 observa\u00e7\u00f5es.","2cfda945":"# 2. Dados <a name=\"2\"><\/a>\nA base de dados est\u00e1 dispon\u00edvel no Kaggle, em https:\/\/www.kaggle.com\/olistbr\/brazilian-ecommerce#olist_products_dataset.csv, e cont\u00e9m mais de 100 mil regitros de pedidos realizados na plataforma da Olist. S\u00e3o 8 conjuntos de dados divididos com informa\u00e7\u00f5es sobre:\n\n* Consumidores\n* Vendedores\n* Produtos\n* Pedidos\n* Artigos dos pedidos\n* Avalia\u00e7\u00e3o de pedidos\n* Pagamentos\n* Geolocaliza\u00e7\u00e3o\n\nCada conjunto possui informa\u00e7\u00f5es exclusivas sobre o assunto em quest\u00e3o, podendo ser cruzados entre si para se obter informa\u00e7\u00f5es sobre a caracter\u00edstica escolhida. No caso deste trabalho, realizaremos o cruzamento entre **Consumidores**, **Pedidos**, e **Avalia\u00e7\u00f5es** e aplicaremos t\u00e9cnicas de Processamento de Linguagem Natural, como An\u00e1lise de Sentimentos, Modelagem de T\u00f3picos e Word Embeddings com o **Word2Vec**.","a73c8435":"# 1. Introdu\u00e7\u00e3o <a name=\"1\"><\/a>\n\nO Olist \u00e9 uma loja de departamentos dos marketplaces e um grande descomplicador para qualquer lojista que deseja vender nos principais e-commerces do Brasil ou busca alternativas para aumentar as vendas. \u00c9 poss\u00edvel anunciar produtos nos sites das grandes redes varejistas, com suporte em gest\u00e3o, log\u00edstica e SAC. A proposta \u00e9 simples: o vendedor cadastra seu produto de forma r\u00e1pida na plataforma do Olist e logo ele \u00e9 anunciado em redes como Mercado Livre, Americanas.com, Submarino, Casas Bahia entre outros players do mercado brasileiro.\n\nNo Brasil, milhares de compras s\u00e3o efetuadas pela Internet todos os anos, trata-se de um mercado em constante ritmo de crescimento, de modo que seria extremamente relevante identificar comportamentos inerentes a este contexto, como, por exemplo, quais assuntos s\u00e3o mais frequentes nas avalia\u00e7\u00f5es de compra e qual o n\u00edvel de satisfa\u00e7\u00e3o dos consumidores em rela\u00e7\u00e3o a esses assuntos.\n\nPara buscar essas respostas, iremos aplicar t\u00e9nicas de Processamento de Linguagem Natural, como An\u00e1lise de Sentimentos e Modelagem de T\u00f3picos.\n\n\n\n\n","b7e8beb8":"### Visualiza\u00e7\u00e3o Gr\u00e1fica\nPara visualizar melhor graficamente, iremos utilizar o m\u00f3dulo TSNE, do Scikit-Learn, para reduzir os vetores de palavras em apenas 2 dimens\u00f5es e iremos desenhar apenas os 100 termos mais frequentes, por limita\u00e7\u00f5es visuais."}}