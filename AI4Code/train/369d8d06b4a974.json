{"cell_type":{"0d89dcb0":"code","31c99382":"code","7035cab5":"code","6168bbe0":"code","18f9822d":"code","89c9e110":"code","c6941ba8":"code","c158dd40":"code","842ec25b":"code","7d991102":"code","51da7763":"code","61adf88c":"code","1254c604":"code","d9e0d67a":"code","77ac3aec":"code","6bb39324":"code","790f67d6":"code","dd0824da":"code","816b43aa":"code","6034eb13":"markdown","eaa35eff":"markdown","2ea358aa":"markdown","1026c24a":"markdown","009d2113":"markdown","6a37e80d":"markdown","918395a0":"markdown"},"source":{"0d89dcb0":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","31c99382":"def initialize(dim):\n    w = np.zeros((dim,1))\n    assert(w.shape == (dim, 1))\n    return w","7035cab5":"def calc_yhat(w,X):\n    y_hat = np.dot(w.T,X)\n    return y_hat","6168bbe0":"def calc_cost(w,y_hat,Y):\n    m = Y.shape[1]\n    cost = 1\/m * np.sum(np.power(y_hat-Y,2))\n    return cost","18f9822d":"def calculate_gradient(w,X,Y): \n    m = X.shape[1]\n    y_hat = calc_yhat(w,X)\n    grad = 2 \/ m * np.dot(X,(y_hat-Y).T)\n    return grad","89c9e110":"def gradient_descent(X,Y,learning_rate,epochs,details):\n    costs = []\n    w = initialize(X.shape[0])\n    for i in range(epochs+1):\n        if i % details == 0:\n            y_hat = calc_yhat(w,X)\n            cost = calc_cost(w,y_hat,Y)\n            costs.append(cost)\n            print(\"Cost after {} iterations is {}\".format(i,cost))\n        \n        grad = calculate_gradient(w,X,Y)\n        w = w - learning_rate * grad\n        \n    return w,costs","c6941ba8":"def run_model(X,Y,learning_rate,epochs, details):\n    '''Input Shape:\n        X : (1,M)\n        Y : (1,M)\n        details : epoch interval after which cost is printed''' \n    ones = np.ones((1,X.shape[1]))                    \n    X = np.append(ones,X,axis = 0)                 # To match with both b0 and b1 in w     (Y = b0 + b1X)      \n\n    w,costs = gradient_descent(X,Y,learning_rate,epochs,details)\n    d = {\n        'Intercept' : w[0,0],\n        'Slope'     : w[1,0],\n        'Costs'     : costs\n        }\n    return d                                       # Return a dictionary of intercept,slope and Cost minimization details","c158dd40":"X = np.random.rand(1,2000)\nY = 5*X + np.random.randn(1,2000)*0.1\nplt.scatter(X,Y)","842ec25b":"result = run_model(X,Y,0.01,1000,100)","7d991102":"result['Slope']","51da7763":"result['Intercept']","61adf88c":"result['Costs']","1254c604":"plt.plot(result['Costs'])","d9e0d67a":"df = pd.read_csv('..\/input\/..\/input\/salary-data-prediction\/Salary_Data.csv')\ndf.head(10)","77ac3aec":"X = df[['YearsExperience']].values\nY = df[['Salary']].values\nplt.scatter(X,Y)","6bb39324":"from sklearn.preprocessing import MinMaxScaler\nmmc = MinMaxScaler()\nX[:] = mmc.fit_transform(X[:])\nY[:] = mmc.fit_transform(Y[:])","790f67d6":"result = run_model(X,Y,0.01,20,2)","dd0824da":"plt.plot(result['Costs'])","816b43aa":"for key,value in result.items():\n    print(key, value)","6034eb13":"## Training on a dataset","eaa35eff":"### 1. Initializing the parameters","2ea358aa":"### Training the model on simple ndarrays","1026c24a":"### 3.Optimization using gradient descent","009d2113":"# Linear Regression From Scratch\n### Steps:\n 1. Initialize the parametes\n 2. Repeat until convergence or given number of iterations <br>\n     i. Calculate Predicted values for dependent variable as per current parameters (y_hat)<br>\n     ii. Calculate current loss (Cost function)<br>\n     iii. Calculate gradients (Derivative of cost function wrt X)<br>\n     iv. Update Parameters (Gradient Descent)<br>","6a37e80d":"### 2. Calculating Y_hat and cost function","918395a0":"### 4. Combining all functions to create a model"}}