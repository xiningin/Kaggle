{"cell_type":{"d2d4f100":"code","1d708f7f":"code","b9fc1e58":"code","716cf3c4":"code","068d7abb":"code","612a8dc0":"code","480334c2":"code","36a1c8f0":"code","914edc37":"code","e5b720c1":"code","47f1d1f5":"code","c827606e":"code","1c5f9825":"code","3c5721b9":"code","cc23efee":"code","902060b3":"code","570b573f":"code","d45bec0f":"code","c56caf57":"code","506ec772":"code","10e114f6":"code","ea68f8ce":"code","96013e25":"markdown","2c7c7c7b":"markdown","5652b5ad":"markdown","e1897815":"markdown","8daf97c1":"markdown","178f0602":"markdown","fd3df535":"markdown","b1f6f890":"markdown","a8424377":"markdown","1cc809ea":"markdown","8ff821a5":"markdown","2f896f5d":"markdown","3ff75f2a":"markdown","4741aa6d":"markdown","35622c2c":"markdown","3b1ffe54":"markdown","99eec8a4":"markdown","942474b3":"markdown","b581652a":"markdown","728b832a":"markdown","af83b5e1":"markdown","9571c4b6":"markdown"},"source":{"d2d4f100":"import torch\nimport torch.nn as nn\nfrom torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\nimport torch.nn.functional as F\nimport torchvision\nfrom albumentations import Compose,Resize,OneOf,RandomBrightness,RandomContrast,Normalize,HorizontalFlip,Blur,ElasticTransform,GridDistortion,OpticalDistortion,GaussNoise \nfrom albumentations.pytorch import ToTensor\nimport pandas as pd\nimport numpy as np\nimport nibabel as nib\nfrom PIL import Image\nimport time\nimport matplotlib.pyplot as plt\nimport cv2\nimport copy\nfrom tqdm import tqdm_notebook as tqdm","1d708f7f":"seed = 271\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","b9fc1e58":"class Covid19_CT_Dataset(torch.utils.data.Dataset):\n    def __init__(self, cts_path, masks_path, transform=None, transform2=None):\n        self.transforms = transform\n        self.transforms2 = transform2\n        self.cts_path = cts_path\n        self.masks_path = masks_path\n        self.len = np.array(nib.load(self.cts_path).get_fdata()).shape[-1]\n        \n    def __getitem__(self,index):\n        ct = nib.load(self.cts_path)\n        ct = np.rot90(np.array(ct.get_fdata()))\n        image = ct[:,:,index]\n        image = Image.fromarray(image)\n        image = np.array(image.convert('RGB'))\n        \n        ct_mask = nib.load(self.masks_path)\n        ct_mask = np.rot90(np.array(ct_mask.get_fdata()))\n        mask = ct_mask[:,:,index]\n        \n        labels = np.unique(mask).astype(\"uint8\")\n        labels = labels[1:]\n        target_mask = np.zeros((mask.shape[0], mask.shape[1], 3))\n        for label in labels:\n            target_mask[:,:, label-1 : label] = np.expand_dims(mask, -1)==label\n        \n        if self.transforms is not None:\n            augument = self.transforms(image=image,mask=target_mask)\n            image = augument['image']\n            target_mask = augument['mask']\n            \n        if self.transforms2 is not None:\n            image = self.transforms2(image=image)['image']\n        \n        target_mask = ToTensor()(image=target_mask)['image']\n        \n        image = ToTensor()(image=image)['image']\n        \n        return image, target_mask\n    \n    def __len__(self):\n        return self.len","716cf3c4":"imgsize = 256\ntransforms = {\n    'both': Compose([\n                    Resize(imgsize,imgsize),\n                    HorizontalFlip(p=0.5), \n                    OneOf([ElasticTransform(alpha=120, sigma=120*0.05, alpha_affine=120*0.03), GridDistortion(), OpticalDistortion(distort_limit=2, shift_limit=0.5)], p=0.3),\n                    ]),\n    \n    'image': Compose([\n                    OneOf([RandomBrightness(limit=0.1, p=0.4), RandomContrast(limit=0.1, p=0.4)]),\n                    GaussNoise(),\n                    Blur(p=0.1, blur_limit = 3),\n                    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n                    ]),\n        \n\n}","068d7abb":"csv_file = r'..\/input\/covid19-ct-scans\/metadata.csv'\ndf = pd.read_csv(csv_file)\ncts_path = df['ct_scan'].tolist()\nmasks_path = df['lung_and_infection_mask'].tolist()\ndataset = Covid19_CT_Dataset(cts_path[0],masks_path[0],transform=transforms['both'], transform2=None)","612a8dc0":"img_id = 100\nplt.figure(figsize=(8,8))\nplt.imshow(dataset[img_id][0].permute(1,2,0).numpy(), cmap='bone')\nplt.imshow(dataset[img_id][1].permute(1,2,0).numpy(), alpha=0.5, cmap='Reds')","480334c2":"class DiceBCELoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceBCELoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        BCE = nn.BCELoss()(inputs, targets)\n        num = targets.size(0)     \n        inputs = inputs.reshape(num, -1)\n        targets = targets.reshape(num, -1)\n        \n        intersection = (inputs * targets).sum(1) \n        dice = (2.*intersection + smooth)\/(inputs.sum(1) + targets.sum(1) + smooth) \n        dice_loss = 1 - dice.sum()\/num\n\n        loss_final = 3*BCE + dice_loss\n        return loss_final\n    \n    \n\nclass IoU(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(IoU, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n              \n        \n        intersection = (inputs * targets).sum()\n        total = (inputs + targets).sum()\n        union = total - intersection \n        \n        IoU = (intersection + smooth)\/(union + smooth)\n                \n        return IoU * 100\n\n    \n    \nclass DiceScore(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceScore, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n              \n        \n        intersection = (inputs * targets).sum()                            \n        dice_score = (2.*intersection + smooth)\/(inputs.sum() + targets.sum() + smooth) \n        return dice_score\n    \n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2):\n        super(FocalLoss, self).__init__()\n        self.alpha = 1\n        self.gamma = 2\n        \n    def forward(self, inputs, targets):\n        num = targets.size(0)\n        inputs = inputs.reshape(num, -1)\n        targets = targets.reshape(num, -1)\n        BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n        batch_loss = self.alpha * (1 - inputs) ** self.gamma * BCE_loss\n        loss = batch_loss.mean()\n        \n        return loss","36a1c8f0":"def compute_iou(model, loader, threshold=0.3):\n    model.eval()\n    valloss = 0\n    \n    with torch.no_grad():\n        for data, target in loader:\n            \n            data = data.to(device)\n            target = target.to(device)\n            \n            outputs = model(data)\n\n            out_cut = np.copy(outputs.data.cpu().numpy())\n            out_cut[np.nonzero(out_cut < threshold)] = 0.0\n            out_cut[np.nonzero(out_cut >= threshold)] = 1.0\n\n            picloss = DiceScore()(out_cut, target.data.cpu().numpy())\n            valloss += picloss\n\n    return valloss \/len(loader)","914edc37":"class ConvBlock(nn.Module):\n    def __init__(self,in_channels,out_channels,kernel_size=(3,3),padding=1):\n        super(ConvBlock,self).__init__()\n        self.conv = nn.Conv2d(in_channels,out_channels,kernel_size,padding=padding)\n        self.batchnorm = nn.BatchNorm2d(out_channels,eps=1e-4)\n        self.relu = nn.ReLU(inplace=True)\n        \n    def forward(self,x):\n        x = self.conv(x)\n        x = self.batchnorm(x)\n        x = self.relu(x)\n        return x\n        \n        \nclass StackEncoder(nn.Module):\n    def __init__(self,channel1,channel2,kernel_size=(3,3),padding=1):\n        super(StackEncoder,self).__init__()\n        self.maxpool = nn.MaxPool2d(kernel_size=2,stride=2)\n        self.block = nn.Sequential(\n            ConvBlock(channel1,channel2,kernel_size,padding),\n            ConvBlock(channel2,channel2,kernel_size,padding),     \n        )\n        \n    def forward(self,x):\n        copy_out = self.block(x)\n        poolout = self.maxpool(copy_out)\n        return copy_out,poolout\n     \n        \nclass StackDecoder(nn.Module):\n    def __init__(self,copy_channel,channel1,channel2,kernel_size=(3,3),padding=1):\n        super(StackDecoder,self).__init__()\n        self.unConv = nn.ConvTranspose2d(channel1,channel1,kernel_size=(2,2),stride=2)\n        self.block = nn.Sequential(\n            ConvBlock(channel1+copy_channel,channel2,kernel_size,padding),\n            ConvBlock(channel2,channel2,kernel_size,padding),\n            ConvBlock(channel2,channel2,kernel_size,padding),\n        )\n        \n    def forward(self,x,down_copy):\n            _, channels, height, width = down_copy.size()  \n            x = self.unConv(x)\n            x = torch.cat([x, down_copy], 1)\n            x = self.block(x)\n            return x\n        \n        \nclass Unet(nn.Module):\n    def __init__(self):\n        super(Unet,self).__init__()\n        \n        self.down1 = StackEncoder(3,32,kernel_size=(3,3))             \n        self.down2 = StackEncoder(32,64,kernel_size=(3,3))            \n        self.down3 = StackEncoder(64,128,kernel_size=(3,3))           \n        self.down4 = StackEncoder(128,256,kernel_size=(3,3))          \n        \n        self.center = ConvBlock(256,256,kernel_size=(3,3),padding=1)  \n        \n        self.up4 = StackDecoder(256,256,128,kernel_size=(3,3))        \n        self.up3 = StackDecoder(128,128,64,kernel_size=(3,3))         \n        self.up2 = StackDecoder(64,64,32,kernel_size=(3,3))           \n        self.up1 = StackDecoder(32,32,16,kernel_size=(3,3))           \n        self.conv = Conv2d(16,3,kernel_size=(1,1),bias=True)\n        \n    def forward(self,x):\n        copy1,out = self.down1(x)  \n        copy2,out = self.down2(out)  \n        copy3,out = self.down3(out)\n        copy4,out = self.down4(out)\n        \n        out = self.center(out)\n        \n        up4 = self.up4(out,copy4)\n        up3 = self.up3(up4,copy3)\n        up2 = self.up2(up3,copy2)\n        up1 = self.up1(up2,copy1)\n        \n        out = self.conv(up1)\n        out = nn.Sigmoid()(out)\n\n\n        return out","e5b720c1":"class ConvBlock(nn.Module):\n    def __init__(self,in_channels,out_channels,kernel_size=(3,3),padding=1):\n        super(ConvBlock,self).__init__()\n        self.conv = nn.Conv2d(in_channels,out_channels,kernel_size,padding=padding,bias=False)\n        self.batchnorm = nn.BatchNorm2d(out_channels,eps=1e-5)\n        self.relu = nn.ReLU(inplace=True)\n        \n    def forward(self,x):\n        x = self.conv(x)\n        x = self.batchnorm(x)\n        x = self.relu(x)\n        return x\n\n    \n    \nclass StackDecoder(nn.Module):\n    def __init__(self,channel1,channel2,kernel_size=(3,3),padding=1):\n        super(StackDecoder,self).__init__()\n        self.unConv = nn.ConvTranspose2d(channel1,channel1,kernel_size=(2,2),stride=2)\n        self.block = nn.Sequential(\n            ConvBlock(channel1,channel1\/\/2,kernel_size,padding),\n            ConvBlock(channel1\/\/2,channel2,kernel_size,padding),\n        )\n        \n    def forward(self,x):\n        x = self.unConv(x)\n        x = self.block(x)\n        return x\n\n    \n    \nclass ResNextUnet(nn.Module):\n    def __init__(self):\n        super(ResNextUnet,self).__init__()\n        self.base_model = torchvision.models.resnext50_32x4d(pretrained=True)\n        self.resblock = list(self.base_model.children())\n        self.down0 = nn.Sequential(*self.resblock[:3])\n        self.down1 = nn.Sequential(*self.resblock[4])\n        self.down2 = nn.Sequential(*self.resblock[5])\n        self.down3 = nn.Sequential(*self.resblock[6])\n        self.down4 = nn.Sequential(*self.resblock[7])\n        self.center = nn.Sequential(\n            nn.Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False),\n            nn.BatchNorm2d(1024, eps=1e-05),\n            nn.Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False),\n            nn.BatchNorm2d(1024, eps=1e-05),\n            nn.Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False),\n            nn.BatchNorm2d(2048, eps=1e-05),\n            nn.ReLU(inplace=True)\n        )\n        self.up3 = StackDecoder(2048, 1024)\n        self.up2 = StackDecoder(1024, 512)\n        self.up1 = StackDecoder(512, 256)\n        self.last_block = StackDecoder(256, 64, 3, 1)\n        self.classifier = nn.Conv2d(64, 2, 3, padding=1)\n                       \n        \n    def forward(self,x):\n        d0 = self.down0(x)           #128     \n        d1 = self.down1(d0)          #128\n        d2 = self.down2(d1)          #64\n        d3 = self.down3(d2)          #32\n        d4 = self.down4(d3)          #16\n        \n        u4 = self.center(d4) + d4    #16\n        u3 = self.up3(u4) + d3       #32\n        u2 = self.up2(u3) + d2       #64\n        u1 = self.up1(u2) + d1       #128\n        \n        out = self.last_block(u1)    #256\n        out = self.classifier(out)\n        out = nn.Sigmoid()(out)\n        \n        return out","47f1d1f5":"def train(model, criterion, optimizer, lr_scheduler, num_epochs=20):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_loss = 100\n    print_freq = int(len(dataloader['train'])\/20)\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n        \n        for phase in ['train','val']:\n            if phase == 'train':\n                model.train() \n            else:\n                model.eval() \n\n            running_loss = 0.0\n            size = 0\n            pb = tqdm(dataloader[phase], total=len(dataloader[phase]))\n            for i, (images, masks) in enumerate(pb):\n                images = images.to(device)\n                masks = masks.to(device)\n\n                optimizer.zero_grad()       \n\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(images)\n                    loss = criterion[0](outputs, masks) + 4*criterion[1](outputs, masks)\n                    loss = loss \/ accumulation_steps\n                    if phase == 'train':\n                        loss.backward()\n                        if (i + 1 ) % accumulation_steps == 0:\n                            optimizer.step()\n                            lr_scheduler.step()\n\n                running_loss += loss.item() * images.size(0)\n                size += images.size(0)\n                pb.set_postfix(loss=running_loss\/size)\n \n            epoch_loss = running_loss \/ dataset_size[phase]\n            print()\n            print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n            \n            if phase == 'train':\n                train_loss.append(epoch_loss)\n                \n            elif phase == 'val':\n                iou = compute_iou(model, dataloader[phase], threshold=0.3)\n                print(\"DICE score : \", iou)\n                val_loss.append(epoch_loss)\n                \n                \n            if phase == 'val' and epoch_loss <= best_loss:\n                print('New Optimal Found!')\n                best_loss = epoch_loss\n                torch.save(model.state_dict(), saved_path)\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed \/\/ 60, time_elapsed % 60))\n    print()\n    \n    model.load_state_dict(best_model_wts)\n    return model","c827606e":"csv_file = r'..\/input\/covid19-ct-scans\/metadata.csv'\ndf = pd.read_csv(csv_file)\ncts_path = df['ct_scan'].tolist()\nmasks_path = df['lung_and_infection_mask'].tolist()\ndataset_list = []\nfor i in range(20):\n    dataset_list.append(Covid19_CT_Dataset(cts_path[i],masks_path[i],transform=transforms['both'], transform2=transforms['image']))\n      \ndataset_train = torch.utils.data.ConcatDataset(dataset_list[:16])\ndataset_val = torch.utils.data.ConcatDataset(dataset_list[16:])\n\n\ntrain_loader = torch.utils.data.DataLoader(dataset_train, batch_size=8, shuffle=True, num_workers=2)\nval_loader = torch.utils.data.DataLoader(dataset_val, batch_size=8, shuffle=False, num_workers=2)\ndataset_size = {'train':len(dataset_train), 'val':len(dataset_val)}\ndataloader = {'train':train_loader, 'val':val_loader}","1c5f9825":"train_loss = []\nval_loss = []\naccumulation_steps = 32\/\/8  #8 is batch_size\nmodel = Unet()\nmodel.to(device)\ncriterion = [DiceBCELoss(), FocalLoss()]             \noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\nlr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n\nmodel = train(model, criterion, optimizer, lr_scheduler, num_epochs=70)","3c5721b9":"#model_path = r'..\/input\/covid19-unet-weightpytorch\/covid19_Unet_seg.pth'\n#model = Unet()\n#model.load_state_dict(torch.load(model_path, map_location=device))\n#model.to(device)\n#model.eval()","cc23efee":"from IPython.display import Image","902060b3":"Image(r'..\/input\/covid-19-display-image\/covid19 display image\/Figure_1.png')","570b573f":"Image(r'..\/input\/covid-19-display-image\/covid19 display image\/ct.PNG')","d45bec0f":"Image(r'..\/input\/covid-19-display-image\/covid19 display image\/infection_and_lung.PNG')","c56caf57":"Image(r'..\/input\/covid-19-display-image\/covid19 display image\/infection.PNG')","506ec772":"import vtk\nfrom vtkmodules.util.vtkImageImportFromArray import *\nimport SimpleITK as sitk","10e114f6":"class KeyPressInteractorStyle(vtk.vtkInteractorStyleTrackballCamera):\n     \n    def __init__(self,parent=None):\n        self.parent = vtk.vtkRenderWindowInteractor()\n        if(parent is not None):\n            self.parent = parent\n \n        self.AddObserver(\"KeyPressEvent\",self.keyPress)\n \n    def keyPress(self,obj,event):\n        key = self.parent.GetKeySym()\n        if key == 'Up':\n            \n            gradtfun.AddPoint(-100, 1.0)\n            gradtfun.AddPoint(10, 1.0)\n            gradtfun.AddPoint(20, 1.0)\n            \n            volumeProperty.SetGradientOpacity(gradtfun)\n            renWin.Render()\n        if key == 'Down':\n            \n            tfun.AddPoint(1129, 0)\n            tfun.AddPoint(1300.0, 0.1)\n            tfun.AddPoint(1600.0, 0.2)\n            tfun.AddPoint(2000.0, 0.1)\n            tfun.AddPoint(2200.0, 0.1)\n            tfun.AddPoint(2500.0, 0.1)\n            tfun.AddPoint(2800.0, 0.1)\n            tfun.AddPoint(3000.0, 0.1)\n            renWin.Render()\n    \n\ndef StartInteraction():\n    renWin.SetDesiredUpdateRate(10)\n\ndef EndInteraction():\n    renWin.SetDesiredUpdateRate(0.001)\n\ndef ClipVolumeRender(obj):\n    obj.GetPlanes(planes)\n    volumeMapper.SetClippingPlanes(planes)","ea68f8ce":"ds = sitk.ReadImage(path)\ndata = sitk.GetArrayFromImage(ds)  \n\nspacing = ds.GetSpacing()            \n\nsrange = [np.min(data),np.max(data)]\nimg_arr = vtkImageImportFromArray()      \nimg_arr.SetArray(data)                   \nimg_arr.SetDataSpacing(spacing)           \norigin = (0,0,0)\nimg_arr.SetDataOrigin(origin)             \nimg_arr.Update()   \n\nren = vtk.vtkRenderer()\nrenWin= vtk.vtkRenderWindow()\nrenWin.AddRenderer(ren)   \nrenWin.AddRenderer(ren)\niren = vtk.vtkRenderWindowInteractor()\niren.SetRenderWindow(renWin) \niren.SetInteractorStyle(KeyPressInteractorStyle(parent = iren))\nmin = srange[0]\nmax = srange[1]\ndiff = max - min             \ninter = 4200 \/ diff\nshift = -min\n\nshifter = vtk.vtkImageShiftScale() \nshifter.SetShift(shift)\nshifter.SetScale(inter)\nshifter.SetOutputScalarTypeToUnsignedShort()\nshifter.SetInputData(img_arr.GetOutput())\nshifter.ReleaseDataFlagOff()\nshifter.Update()\n\ntfun = vtk.vtkPiecewiseFunction()  \ntfun.AddPoint(1129, 0)\ntfun.AddPoint(1300.0, 0.1)\ntfun.AddPoint(1600.0, 0.12)\ntfun.AddPoint(2000.0, 0.13)\ntfun.AddPoint(2200.0, 0.14)\ntfun.AddPoint(2500.0, 0.16)\ntfun.AddPoint(2800.0, 0.17)\ntfun.AddPoint(3000.0, 0.18)\n\ngradtfun = vtk.vtkPiecewiseFunction()  \ngradtfun.AddPoint(-1000, 9)\ngradtfun.AddPoint(0.5, 9.9)\ngradtfun.AddPoint(1, 10)\n\nctfun = vtk.vtkColorTransferFunction()  \nctfun.AddRGBPoint(0.0, 0.5, 0.0, 0.0)\nctfun.AddRGBPoint(600.0, 1.0, 0.5, 0.5)\nctfun.AddRGBPoint(1280.0, 0.9, 0.2, 0.3)\nctfun.AddRGBPoint(1960.0, 0.81, 0.27, 0.1)\nctfun.AddRGBPoint(2200.0, 0.9, 0.2, 0.3)\nctfun.AddRGBPoint(2500.0, 1, 0.5, 0.5)\nctfun.AddRGBPoint(3024.0, 0.5, 0.5, 0.5)\n\nvolumeMapper = vtk.vtkGPUVolumeRayCastMapper()\nvolumeMapper.SetInputData(shifter.GetOutput())   \nvolumeProperty = vtk.vtkVolumeProperty()         \nvolumeProperty.SetColor(ctfun)  \nvolumeProperty.SetScalarOpacity(tfun)\nvolumeProperty.SetInterpolationTypeToLinear()    \nvolumeProperty.ShadeOn()            \n\nnewvol = vtk.vtkVolume()                      \nnewvol.SetMapper(volumeMapper)\nnewvol.SetProperty(volumeProperty)\n\noutline = vtk.vtkOutlineFilter()\noutline.SetInputConnection(shifter.GetOutputPort())\n\noutlineMapper = vtk.vtkPolyDataMapper()\noutlineMapper.SetInputConnection(outline.GetOutputPort())\n\noutlineActor = vtk.vtkActor()\noutlineActor.SetMapper(outlineMapper)\n\nren.AddActor(outlineActor)\nren.AddVolume(newvol)\nren.SetBackground(0, 0, 0)\nrenWin.SetSize(600, 600)\n\nplanes = vtk.vtkPlanes()\n\nboxWidget = vtk.vtkBoxWidget()\nboxWidget.SetInteractor(iren)\nboxWidget.SetPlaceFactor(1.0)\nboxWidget.PlaceWidget(0,0,0,0,0,0)\nboxWidget.InsideOutOn()\nboxWidget.AddObserver(\"StartInteractionEvent\", StartInteraction)\nboxWidget.AddObserver(\"InteractionEvent\",  ClipVolumeRender)\nboxWidget.AddObserver(\"EndInteractionEvent\",  EndInteraction)\n\noutlineProperty = boxWidget.GetOutlineProperty()\noutlineProperty.SetRepresentationToWireframe()\noutlineProperty.SetAmbient(1.0)\noutlineProperty.SetAmbientColor(1, 1, 1)\noutlineProperty.SetLineWidth(9)\n\nselectedOutlineProperty = boxWidget.GetSelectedOutlineProperty()\nselectedOutlineProperty.SetRepresentationToWireframe()\nselectedOutlineProperty.SetAmbient(1.0)\nselectedOutlineProperty.SetAmbientColor(1, 0, 0)\nselectedOutlineProperty.SetLineWidth(3)\n\nren.ResetCamera()\niren.Initialize()\nrenWin.Render()\niren.Start()","96013e25":"By the way, if you find this notebook helpful, please **Upvote**(Thank you!)","2c7c7c7b":"# **Covid19 segmentation and 3D reconstruction**","5652b5ad":"# 3D Reconstruction","e1897815":"You can also modify and improve it","8daf97c1":"You could download parameters(Unet, trained less than 40 epochs,you could see it as a baseline) from this site : https:\/\/www.kaggle.com\/qiyuange\/covid19-unet-weightpytorch","178f0602":"Read More ->\nU-Net: Convolutional Networks for Biomedical Image Segmentation \nhttps:\/\/arxiv.org\/abs\/1505.04597","fd3df535":"# View Result","b1f6f890":"Lung and infection area","a8424377":"# data augumentation","1cc809ea":"Infection area","8ff821a5":"CT-scan","2f896f5d":"In this part, I used code from this blog(it's my first time use vtk, thanks), which is : https:\/\/blog.csdn.net\/weixin_42724859\/article\/details\/103615980?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522160134702319724848316641%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=160134702319724848316641&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v3~pc_rank_v2-3-103615980.first_rank_ecpm_v3_pc_rank_v2&utm_term=vtk+%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA+python&spm=1018.2118.3001.4187 ","3ff75f2a":"NOTE : If you want to try codes below yourself ,please download the notebook to your computer. \nI can't run it successfully on kaggle","4741aa6d":"# DiceLoss, BCELoss, FocalLoss, IOU","35622c2c":"content\n--------------------\n\nmodel:\n* Unet\n* Unet with ResNext50 architecture\n\ntrick:\n* BCELOSS, DiceLoss, FocalLoss\n* albumentations augumentation\n* gradient accumulation\n\nvisualize:\n* 3d reconstruction","3b1ffe54":"# Dataset","99eec8a4":"# visualize data","942474b3":"# 1.Unet","b581652a":"![infection_and_lung.PNG](attachment:infection_and_lung.PNG)","728b832a":"# Train function","af83b5e1":"The green stands for left lung, red stands for right lung and blue stands for infection area","9571c4b6":"# 2.Unet with resnext50 architecture"}}