{"cell_type":{"c7af0df8":"code","4b7d8f04":"code","3801bf67":"code","92647127":"code","97737ad7":"code","ed61e301":"code","2c592b7c":"code","8ccc2cf1":"code","f607a7d8":"code","653a8e9f":"code","65cc34eb":"code","9e68278f":"code","6ff0ba3d":"code","8e80d5a6":"code","38f7e2d2":"markdown","96ca713e":"markdown","24f6a6df":"markdown","c5267833":"markdown","3c2091ae":"markdown","6aed5719":"markdown","adb35fa0":"markdown","a525a91a":"markdown","5b000149":"markdown","8eb92ec1":"markdown","527ed6c3":"markdown","f616dd40":"markdown","10086ff3":"markdown","72620c4a":"markdown","b821c6c3":"markdown","d5e368bc":"markdown","d3a0ab82":"markdown","7a1d2f8c":"markdown","c92b63fb":"markdown","3a772da2":"markdown","8aac8419":"markdown","99d33f4d":"markdown","512ab7cd":"markdown","3df3151b":"markdown","5826ebca":"markdown"},"source":{"c7af0df8":"from transformers import BertConfig, TFBertModel\nfrom IPython.display import clear_output\n\n# load a configuration of BERT\nconfig = BertConfig()\nprint(config)","4b7d8f04":"# Model is randomly initialized \nmodel = TFBertModel(config)","3801bf67":"checkpoint = \"bert-base-cased\"\nmodel = TFBertModel.from_pretrained(checkpoint); clear_output()","92647127":"from transformers import BertTokenizer\n\ntokenizer = BertTokenizer.from_pretrained(checkpoint); clear_output()","97737ad7":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint); clear_output()\n\n# We can now use the tokenizer\ntokenizer(\"Using a Transformer [MASK] is simple\")","ed61e301":"sequence = \"Using a Transformer [MASK] is simple\"\ntokens = tokenizer.tokenize(sequence)\n\nprint(tokens)","2c592b7c":"ids = tokenizer.convert_tokens_to_ids(tokens)\n\nprint(ids)","8ccc2cf1":"tokenizer.decode(ids)","f607a7d8":"sequence = \"Using a Transformer [MASK] is simple\"\n\nmodel_inputs = tokenizer(sequence)\nprint(\"Output of tokenizer object =\",tokenizer.decode(model_inputs[\"input_ids\"]))\n\ntokens = tokenizer.tokenize(sequence)\nids = tokenizer.convert_tokens_to_ids(tokens)\nprint(\"Output of decoding the input tokens =\",tokenizer.decode(ids))","653a8e9f":"from transformers import AutoTokenizer\n\ncheckpoint = \"bert-base-cased\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint); clear_output()\n\n''' Correct sentences\n- Python is a popular programming language often used to write scripts for operating systems.\n- Use this process to download and compile the source code from the developer.\n- As of January 2020, Python 2 will be in EOL (End Of Life) status and receive no further official support.\n'''\n\nraw_inputs = [\"Python is a popular [MASK] language often used to write scripts for operating systems.\", \n              \"Use this process to [MASK] and compile the source code from the developer.\",\n              \"As of January 2020, Python 2 will be in EOL (End Of Life) status and receive no further [MASK] support.\"]\ninputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"tf\")","65cc34eb":"# See the result of decoded tokens\ntokenizer.batch_decode(inputs['input_ids'], attention_mask = inputs.attention_mask)","9e68278f":"from transformers import TFAutoModelForMaskedLM\n\n# Use Masked Language Model for this problem\nmodel = TFAutoModelForMaskedLM.from_pretrained(checkpoint); clear_output()\noutputs = model(inputs['input_ids'], \n                attention_mask=inputs['attention_mask'])","6ff0ba3d":"import tensorflow as tf\n\n# Postprocess to get the result\nprobabilities = tf.math.softmax(outputs.logits, axis=-1)\npred = tf.math.argmax(probabilities, axis=-1).numpy()\n\n# Since Padding tokens for this model is 0, \n# we can easily mask them out by multiplying with attention mask\ntokenizer.batch_decode(pred * inputs.attention_mask.numpy(), \n                       skip_special_tokens = True)","8e80d5a6":"list(zip(tokenizer.all_special_tokens, tokenizer.all_special_ids))","38f7e2d2":"## Encoding\nis about **tokenization**, followed by the **conversion to input IDs**.\n\n---\n\nOnce we've done the Tokenization process, the next step is to convert each token to *input ID*. \n- ***Why?*** because we won't gonna use these tokens as an direct input to the model. Instead, we will use `input_ids` as an input. Then, the model's embedding layer will use these `input_ids` to map each token to a single tensor (embedding vector) to represent each token. \n- ***What is the number in those tensors?*** they are learnable parameter that are randomly initialized and will be adjusted along the course of training.\n\n![](https:\/\/sv1.picz.in.th\/images\/2021\/07\/03\/suIq1k.png)","96ca713e":"## And more!\n---\nUnsurprisingly, there are many more techniques out there. To name a few:\n\n- Byte-level BPE, as used in GPT-2\n- WordPiece, as used in BERT\n- SentencePiece or Unigram, as used in several multilingual models","24f6a6df":"Note that the decode method not only converts the indices back to tokens, but also groups together the tokens that were part of the same words to produce a readable sentence. ","c5267833":"These ids can then be used as **inputs to a model** as seen earlier in this chapter.","3c2091ae":"However, if you know the type of model you want to use, you can use the class that defines its architecture directly. <br>\n-> Let\u2019s take a look at how this works with a BERT model. The first thing we\u2019ll need to do to initialize a BERT model is load a configuration object:","6aed5719":"### 2. Convert each token to input ID\n---\nConvert those tokens into numbers, so embedding layer can map each token to a tensor (*embedding vector*). The conversion to input IDs is handled by the `convert_tokens_to_ids` tokenizer method:","adb35fa0":"Without BertConfig, we instead loaded a pretrained model via the bert-base-cased identifier. This is a model checkpoint that was trained by the authors of BERT themselves; \nyou can find more details about it in its model card. \nThe identifier used to load the model can be the identifier of any model on the \ud83e\udd17Model Hub, as long as it is compatible with the BERT architecture (because we use `TFBertModel` class).\n\n<div class=\"alert alert-info\">\n<h3>Knowledge is transferred<\/h3>\n     This model is now initialized with all the weights of the checkpoint. It can be used directly for inference on the tasks it was trained on, and it can also be fine-tuned on a new task. By training with pretrained weights rather than from scratch, we can quickly achieve good results.\n<\/div>\n\nActually, as seen previous section, we could replace `TFBertModel` with the equivalent `TFAutoModel` class.","a525a91a":"## 1. Word-based\n---\nIt\u2019s generally very easy to set up and use with only a few rules, and it often yields decent results. For example, in the image below, the goal is to **split the raw text into words** and find a numerical representation for each word:\n\n![](https:\/\/huggingface.co\/course\/static\/chapter2\/word_based_tokenization.png)\n\nWe can end up with some pretty large \u201cvocabularies\u201d, where a vocabulary is defined by the total number of independent tokens that we have in our corpus. Each word gets assigned an ID (starting from 0 and going up to the size of the vocabulary). \n$$\\text{The model uses these IDs to identify each word.}$$\n\nFinally, we need a custom token to represent words that are not in our vocabulary (aka. the \u201cunknown\u201d token). It\u2019s generally a bad sign if you see that the tokenizer is producing a lot of these tokens, as it wasn\u2019t able to retrieve a sensible representation of a word and you\u2019re losing information along the way. \n\nOne way to reduce the amount of unknown tokens is to go one level deeper, using a *character-based tokenizer*.","5b000149":"Loading tokenizers is as simple as it is with models. Actually, it\u2019s based on the same two methods: `from_pretrained` This will load the tokenizer algorithm used by the pre-trained model (a bit like the architecture of the model) as well as its vocabulary (a bit like the weights of the model).\n\nLoading the BERT tokenizer trained with the same checkpoint as BERT is done the same way as loading the model, except we use the `BertTokenizer` class:","8eb92ec1":"## 3.Subword tokenization\n---\nSubword tokenization *algorithms* rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into subwords.\n\nFor instance, \u201cannoyingly\u201d might be considered a rare word and could be decomposed into \u201cannoying\u201d and \u201cly\u201d. Both are likely to appear more frequently so we stop decomposing them (while at the same time the meaning of \u201cannoyingly\u201d is kept by the composite meaning of \u201cannoying\u201d and \u201cly\u201d)\n\nHere is an example showing how a subword tokenization algorithm would tokenize the sequence \u201cLet\u2019s do tokenization!\u201c:\n\n![](https:\/\/huggingface.co\/course\/static\/chapter2\/bpe_subword.png)\n\nThis approach is especially useful in agglutinative languages such as Turkish, where you can form (almost) arbitrarily long complex words by stringing together subwords.","527ed6c3":"### 1. Tokenization \n---\nThe first step is to split the text into tokens. There are multiple rules that can govern that process, which is why we need to instantiate the tokenizer using the name of the model, to make sure we use the same rules that were used when the model was pretrained.\n\nThe tokenization process is done by the `tokenize` method of the tokenizer: ","f616dd40":"# Models\nfull original tutorial at https:\/\/huggingface.co\/course\/chapter2\/\n\n---\nIn this section we\u2019ll take a closer look at creating and using a model. `TFAutoModel` class gives a handy way to instantiate any model from a checkpoint. The `TFAutoModel` and all of its relatives are actually simple *wrappers* over models available in the library.","10086ff3":"If you got wierd result, that might because you forgot to filter out special tokens before decoding. We can see all the special tokens and their ids by:","72620c4a":"Similar to `TFAutoModel`, the `AutoTokenizer` class will grab the proper tokenizer class in the library based on the checkpoint name, and can be used directly with any checkpoint:","b821c6c3":"The tokenizer added the special word (for DistilBERT, [CLS] and [SEP]). This is because the model was pretrained with those, so to get the same results for inference we need to add them as well. \n- **Note** that some models don\u2019t add special words, or add different ones. In any case, the `tokenizer` knows which ones are expected and will deal with this for you.","d5e368bc":"For full list of Tokenizer properties, check out this [link](https:\/\/huggingface.co\/transformers\/internal\/tokenization_utils.html#transformers.tokenization_utils_base.SpecialTokensMixin.additional_special_tokens)","d3a0ab82":"# Tokenizer\n---\nNow that we have created the model, let\u2019s try using it to make some predictions. Transformer models can only process numbers \u2014 numbers that the ***tokenizer*** generates. Tokenizers can take care of casting the inputs to the appropriate framework\u2019s tensors.\n\nTo convert the raw text to numbers, there are a lot of ways to go about this. The goal is to find the **most meaningful representation** \u2014 that is, the one that makes the most sense to the model \u2014 and, if possible, the **smallest** representation.\n\nLet\u2019s take a look at some examples of tokenization algorithms,","7a1d2f8c":"Creating a model from the default configuration initializes it with **random values**: if we want to use this model, we have to train it from scratch. We could train the model from scratch on the task at hand, but this would <u>require a long time and a lot of data<\/u>\n\n$$\\text{To avoid unnecessary and duplicated effort, it\u2019s imperative to be able to share and reuse models that have already been trained.}$$\n\nLoading a Transformer model that is already trained is simple \u2014 we can do this using the `from_pretrained` method:","c92b63fb":"## 2. Character-based\n---\nCharacter-based tokenizers split the text into characters, rather than words. This has two primary benefits:\n- The vocabulary is much smaller.\n- There are much fewer Unknown tokens.\n\n![](https:\/\/huggingface.co\/course\/static\/chapter2\/character_based_tokenization.png)\n\nWith these concerns, this approach isn\u2019t perfect either: \n- We\u2019ll end up with a very large amount of tokens to be processed by our model.\n- Since the representation is now based on characters rather than words, one could argue that, intuitively, it\u2019s less meaningful: each character doesn\u2019t mean a lot on its own.\n    - However, this again differs according to the language; in Chinese, for example, each character carries more information than a character in a Latin language.","3a772da2":"## Decoding\nDecoding is going the other way around: from vocabulary indices, we want to get a string. This can be done with the `decode` method as follows:","8aac8419":"<div class=\"alert alert-warning\">\n    <h3>Before tokenization, do preprocessing<\/h3>\n    If we want to completely cover a language with a word-based tokenizer, we\u2019ll need to have an identifier for each word in the language, which will generate a huge amount of tokens. For example, there are over 500,000 words in the English language, so to build a map from each word to an input ID we\u2019d need to keep track of that many IDs. Furthermore, words like \u201cdog\u201d are represented differently from words like \u201cdogs\u201d, and the model will initially have no way of knowing that \u201cdog\u201d and \u201cdogs\u201d are similar: it will identify the two words as unrelated. <br><br>\n<\/div>","99d33f4d":"The output from Tokenizer consists of `input_ids`, `token_type_ids`, and `attention_mask`. We will go into detail of each in the following section.","512ab7cd":"This tokenizer is a subword tokenizer: it splits the words until it obtains tokens that can be represented by its vocabulary.","3df3151b":"# Full example\n---\nNote that we used some methods that perform parts of the tokenization pipeline *separately* to show the intermediate results of those steps, ***but in practice***, you should call the tokenizer directly on your inputs :","5826ebca":"## Special tokens\nIf we take a look at the input IDs returned by the tokenizer, we will see they are a tiny bit different from what we had earlier:"}}