{"cell_type":{"fa4a8147":"code","a2dd972e":"code","147402b4":"code","8b9ad33f":"code","0e532341":"code","5fe45845":"code","9cc34ab4":"code","3373f7ad":"code","41330300":"code","266d083b":"code","0126509f":"code","0c4ed625":"code","bc914075":"code","5ddcc1b9":"code","4d51b613":"code","f04e9a62":"code","9a52b69c":"code","ad4ed3da":"code","2b30b557":"code","f8e4e047":"code","3b469da5":"code","a58bb56e":"code","3cdda11b":"code","34926c7b":"code","c4497d78":"code","0f286a17":"code","22b90498":"code","24413bad":"markdown","c884595d":"markdown","c60b0843":"markdown","789631eb":"markdown","79c85a44":"markdown","bfec498c":"markdown","0665b2fc":"markdown","3cc78e65":"markdown","cd25a8d0":"markdown","da45b6a4":"markdown","71e3b2a2":"markdown","e2703247":"markdown"},"source":{"fa4a8147":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random\nimport math\nimport gc\nfrom sklearn.preprocessing import RobustScaler,StandardScaler,MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import metrics\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom scipy import stats\nfrom plotly.subplots import make_subplots\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","a2dd972e":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/test.csv')\n\ncols = [e for e in test.columns if e not in ('Id')]\ncontinous_features = cols[:10]\ncategorical_features = cols[10:]","147402b4":"train.head()","8b9ad33f":"test.head()","0e532341":"train.info()","5fe45845":"test.info()","9cc34ab4":"#Check if there'is null values\ntrain.isnull().sum()","3373f7ad":"#Check if there'is null values\ntest.isnull().sum()","41330300":"train[continous_features].describe()","266d083b":"test[continous_features].describe()","0126509f":"# plot continous features \ni = 1\nplt.figure()\nfig, ax = plt.subplots(2, 5,figsize=(20, 12))\nfor feature in continous_features:\n    plt.subplot(2, 5,i)\n    sns.histplot(train[feature],color=\"blue\", kde=True,bins=100, label='train_'+feature)\n    sns.histplot(test[feature],color=\"olive\", kde=True,bins=100, label='test_'+feature)\n    plt.xlabel(feature, fontsize=9); plt.legend()\n    i += 1\nplt.show() ","0c4ed625":"sns.catplot(x=\"Cover_Type\", kind=\"count\", palette=\"ch:.25\", data=train)","bc914075":"train.Cover_Type.value_counts()","5ddcc1b9":"corr = train[continous_features+['Cover_Type']].corr()\ncorr.style.background_gradient(cmap='coolwarm').set_precision(3)","4d51b613":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_memory = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_memory = df.memory_usage().sum() \/ 1024**2\n    if verbose: \n        print(f\"Memory usage of dataframe after reduction {end_memory} MB\")\n        print(f\"Reduced by {100 * (start_memory - end_memory) \/ start_memory} % \")\n    return df","f04e9a62":"train[cols] = reduce_mem_usage(train[cols])","9a52b69c":"test[cols] = reduce_mem_usage(test[cols])","ad4ed3da":"# delete the sample with target 5\ntrain.drop(train[train['Cover_Type']==5].index,inplace=True)","2b30b557":"# generate new features \ncols = [e for e in test.columns if e not in ('Id')]\n\ntrain['binned_elevation'] = [math.floor(v\/50.0) for v in train['Elevation']]\ntest['binned_elevation'] = [math.floor(v\/50.0) for v in test['Elevation']]\n\ntrain['Horizontal_Distance_To_Roadways_Log'] = [np.log(v+300) for v in train['Horizontal_Distance_To_Roadways']]\ntest['Horizontal_Distance_To_Roadways_Log'] = [np.log(v+300) for v in test['Horizontal_Distance_To_Roadways']]\n\ntrain['Soil_Type12_32'] = train['Soil_Type32'] + train['Soil_Type12']\ntest['Soil_Type12_32'] = test['Soil_Type32'] + test['Soil_Type12']\ntrain['Soil_Type23_22_32_33'] = train['Soil_Type23'] + train['Soil_Type22'] + train['Soil_Type32'] + train['Soil_Type33']\ntest['Soil_Type23_22_32_33'] = test['Soil_Type23'] + test['Soil_Type22'] + test['Soil_Type32'] + test['Soil_Type33']\n\ncols = [e for e in test.columns if e not in ('Id')]","f8e4e047":"scaler = StandardScaler()\ntrain[cols] = scaler.fit_transform(train[cols])\ntest[cols] = scaler.transform(test[cols])","3b469da5":"# I optained these parameters using OPTUNA\n# check this kernel to learn more about OPTUNA : https:\/\/www.kaggle.com\/hamzaghanmi\/lgbm-hyperparameter-tuning-using-optuna\nparams = {'objective': 'multiclass',  'random_state': 48,'n_estimators': 20000,\n            'n_jobs': -1,'reg_alpha': 0.9481920810028138, 'reg_lambda': 8.15049828410672, 'colsample_bytree': 0.5, 'subsample': 0.8,\n          'learning_rate': 0.2, 'max_depth': 100, 'num_leaves': 26, 'min_child_samples': 88, 'cat_smooth': 78}","a58bb56e":"preds = [] \nkf = StratifiedKFold(n_splits=10,random_state=48,shuffle=True)\nacc=[]  # list contains accuracy for each fold\nn=0\nfor trn_idx, test_idx in kf.split(train[cols],train['Cover_Type']):\n    X_tr,X_val = train[cols].iloc[trn_idx],train[cols].iloc[test_idx]\n    y_tr,y_val = train['Cover_Type'].iloc[trn_idx],train['Cover_Type'].iloc[test_idx]\n    \n    model = LGBMClassifier(**params)\n    model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=100,verbose=False)\n    \n    preds.append(model.predict(test[cols]))\n    acc.append(accuracy_score(y_val, model.predict(X_val)))\n\n    print(f\"fold: {n+1} , accuracy: {round(acc[n]*100,3)}\")  \n    n+=1  \n    \n    del X_tr,X_val,y_tr,y_val\n    gc.collect()     ","3cdda11b":"print(f\"the mean Accuracy is : {round(np.mean(acc)*100,3)} \")","34926c7b":"# most 30 important features for lgb model\nfrom optuna.integration import lightgbm as lgb\nlgb.plot_importance(model, max_num_features=30, figsize=(10,10))\nplt.show() ","c4497d78":"preds","0f286a17":"sub = pd.read_csv('\/kaggle\/input\/tabular-playground-series-dec-2021\/sample_submission.csv')\nprediction = stats.mode(preds)[0][0]\nsub['Cover_Type'] = prediction\nsub.to_csv('submission.csv', index=False)","22b90498":"sub","24413bad":"# Exploratory Data Analysis","c884595d":"# Let's build a lightgbm model","c60b0843":"* We reduced the train dataset from 1.7 GB to 240 MB \n* We reduced the test dataset from 419 MB to 60 MB","789631eb":"* the data is unbalanced \ud83d\ude15\n* we have only one sample with target 5 !!! \ud83d\ude30 ","79c85a44":"# I hope that you find this kernel usefull\ud83c\udfc4","bfec498c":"## Feature Engineering","0665b2fc":"## Target distibution","3cc78e65":"## let's reduce the memory usage","cd25a8d0":"## Features correlation(I will use only Continous Features)","da45b6a4":"* No Null values \ud83d\ude00","71e3b2a2":"## Let's Make a Submission","e2703247":"* Now we have 10 arrays and each array was calculated for the i-th fold (we used 10 folds).<br>\n* So I'm going to use the <b>mode<\/b> in order to generate the final prediction.<br>\n<img src=\"https:\/\/k8schoollessons.com\/wp-content\/uploads\/2019\/06\/Median-Mode-Mean-and-Range-1.jpg\" width=\"450px\"\/>"}}