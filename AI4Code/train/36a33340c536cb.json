{"cell_type":{"22ac85f5":"code","13f4add6":"code","963e50ad":"code","86c8051e":"code","dd00d3dd":"code","34371528":"code","242447d4":"code","a1e9a1e0":"code","42d51218":"code","03251a99":"code","1c9751b4":"code","471dfbce":"code","cb3743c3":"code","26e44bc5":"markdown","ea512972":"markdown","08cf1f68":"markdown","4aa90373":"markdown","a1b53394":"markdown","766eb940":"markdown","d3054059":"markdown"},"source":{"22ac85f5":"import numpy as np\nimport pandas as pd \nimport os\nimport seaborn as sns; sns.set(rc={'figure.figsize':(16,9)})\nimport matplotlib.pyplot as plt\n\n\ngeo = pd.read_csv(\"..\/input\/olist_geolocation_dataset.csv\", dtype={'geolocation_zip_code_prefix': str})\n\n\n# tratamento do campo CEP, para posteriores consultas\ngeo['geolocation_zip_code_prefix_1_digits'] = geo['geolocation_zip_code_prefix'].str[0:1]\ngeo['geolocation_zip_code_prefix_2_digits'] = geo['geolocation_zip_code_prefix'].str[0:2]\ngeo['geolocation_zip_code_prefix_3_digits'] = geo['geolocation_zip_code_prefix'].str[0:3]\ngeo['geolocation_zip_code_prefix_4_digits'] = geo['geolocation_zip_code_prefix'].str[0:4]\ngeo['geolocation_zip_code_prefix_5_digits'] = geo['geolocation_zip_code_prefix'].str[0:5]\n\n\n# exclus\u00e3o de ordens realizadas fora do territ\u00f3rio brasileiro\ngeo = geo[geo.geolocation_lat <= 5.27438888]\ngeo = geo[geo.geolocation_lng >= -73.98283055]\ngeo = geo[geo.geolocation_lat >= -33.75116944]\ngeo = geo[geo.geolocation_lng <=  -34.79314722]\n\n# convers\u00e3o de coordenadas para Mercator\nfrom datashader.utils import lnglat_to_meters as webm\nx, y = webm(geo.geolocation_lng, geo.geolocation_lat)\ngeo['x'] = pd.Series(x)\ngeo['y'] = pd.Series(y)","13f4add6":"geo.head(10)","963e50ad":"# corre\u00e7\u00e3o dos CEP's para plotagem \ngeo['geolocation_zip_code_prefix'] = geo['geolocation_zip_code_prefix'].astype(int)\ngeo['geolocation_zip_code_prefix_1_digits'] = geo['geolocation_zip_code_prefix_1_digits'].astype(int)\ngeo['geolocation_zip_code_prefix_2_digits'] = geo['geolocation_zip_code_prefix_2_digits'].astype(int)\ngeo['geolocation_zip_code_prefix_3_digits'] = geo['geolocation_zip_code_prefix_3_digits'].astype(int)\ngeo['geolocation_zip_code_prefix_4_digits'] = geo['geolocation_zip_code_prefix_4_digits'].astype(int)\ngeo['geolocation_zip_code_prefix_5_digits'] = geo['geolocation_zip_code_prefix_5_digits'].astype(int)\n\nbrazil = geo\nagg_name = 'geolocation_zip_code_prefix'\n#brazil[agg_name].describe().to_frame()","86c8051e":"# plot wtih holoviews + datashader - bokeh with map background\nimport holoviews as hv\nimport geoviews as gv\nimport datashader as ds\nfrom colorcet import fire, rainbow, bgy, bjy, bkr, kb, kr\nfrom datashader.colors import colormap_select, Greys9\nfrom holoviews.streams import RangeXY\nfrom holoviews.operation.datashader import datashade, dynspread, rasterize\nfrom bokeh.io import push_notebook, show, output_notebook\noutput_notebook()\nhv.extension('bokeh')\n\n%opts Overlay [width=800 height=600 toolbar='above' xaxis=None yaxis=None]\n%opts QuadMesh [tools=['hover'] colorbar=True] (alpha=0 hover_alpha=0.2)\n\nT = 0.05\nPX = 1\n\ndef plot_map(data, label, agg_data, agg_name, cmap):\n    url=\"http:\/\/server.arcgisonline.com\/ArcGIS\/rest\/services\/Canvas\/World_Dark_Gray_Base\/MapServer\/tile\/{Z}\/{Y}\/{X}.png\"\n    geomap = gv.WMTS(url)\n    points = hv.Points(gv.Dataset(data, kdims=['x', 'y'], vdims=[agg_name]))\n    agg = datashade(points, element_type=gv.Image, aggregator=agg_data, cmap=cmap)\n    zip_codes = dynspread(agg, threshold=T, max_px=PX)\n    hover = hv.util.Dynamic(rasterize(points, aggregator=agg_data, width=50, height=25, streams=[RangeXY]), operation=hv.QuadMesh)\n    hover = hover.options(cmap=cmap)\n    img = geomap * zip_codes * hover\n    img = img.relabel(label)\n    return img\n\n# plot wtih datashader - image with black background\nimport datashader as ds\nfrom datashader import transfer_functions as tf\nfrom functools import partial\nfrom datashader.utils import export_image\nfrom IPython.core.display import HTML, display\nfrom colorcet import fire, rainbow, bgy, bjy, bkr, kb, kr\n\nbackground = \"black\"\ncm = partial(colormap_select, reverse=(background!=\"black\"))\nexport = partial(export_image, background = background, export_path=\"export\")\ndisplay(HTML(\"<style>.container { width:100% !important; }<\/style>\"))\nW = 700 \n\ndef create_map(data, cmap, data_agg, export_name='img'):\n    pad = (data.x.max() - data.x.min())\/50\n    x_range, y_range = ((data.x.min() - pad, data.x.max() + pad), \n                             (data.y.min() - pad, data.y.max() + pad))\n\n    ratio = (y_range[1] - y_range[0]) \/ (x_range[1] - x_range[0])\n\n    plot_width  = int(W)\n    plot_height = int(plot_width * ratio)\n    if ratio > 1.5:\n        plot_height = 550\n        plot_width = int(plot_height \/ ratio)\n        \n    cvs = ds.Canvas(plot_width=plot_width, plot_height=plot_height, x_range=x_range, y_range=y_range)\n\n    agg = cvs.points(data, 'x', 'y', data_agg)\n    img = tf.shade(agg, cmap=cmap, how='eq_hist')\n    return export(img, export_name)\n\ndef filter_data(level, name):\n    df = geo[geo[level] == name]\n    #remove outliers\n    df = df[(df.x <= df.x.quantile(0.999)) & (df.x >= df.x.quantile(0.001))]\n    df = df[(df.y <= df.y.quantile(0.999)) & (df.y >= df.y.quantile(0.001))]\n    return df","dd00d3dd":"americana = geo[geo['geolocation_city'] == 'americana']\nagg_name = 'geolocation_zip_code_prefix'\n\n\nplot_map(americana, 'CEPs que realizaram compras em Americana', ds.min(agg_name), agg_name, cmap=rainbow)\n","34371528":"orders_df = pd.read_csv('..\/input\/olist_orders_dataset.csv')\norder_items = pd.read_csv('..\/input\/olist_order_items_dataset.csv')\norder_reviews = pd.read_csv('..\/input\/olist_order_reviews_dataset.csv')\ncustomer = pd.read_csv('..\/input\/olist_customers_dataset.csv', dtype={'customer_zip_code_prefix': str})\n\n# getting the first 3 digits of customer zipcode\ncustomer['customer_zip_code_prefix_3_digits'] = customer['customer_zip_code_prefix'].str[0:3]\ncustomer['customer_zip_code_prefix_3_digits'] = customer['customer_zip_code_prefix_3_digits'].astype(int)\n\nbrazil_geo = geo.set_index('geolocation_zip_code_prefix_3_digits').copy()\n\norders = orders_df.merge(order_items, on='order_id')\norders = orders.merge(customer, on='customer_id')\norders = orders.merge(order_reviews, on='order_id')\n\n","242447d4":"gp = orders.groupby('customer_zip_code_prefix_3_digits')['price'].sum().to_frame()\nsaopaulo = filter_data('geolocation_state', 'SP').set_index('geolocation_zip_code_prefix_3_digits')\nrevenue = saopaulo.join(gp)\nagg_name = 'revenue'\nrevenue[agg_name] = revenue.price \/ 1000","a1e9a1e0":"plot_map(revenue, 'Receita dos pedidos (R$ 1000,00)', ds.mean(agg_name), agg_name, cmap=fire)","42d51218":"create_map(revenue, fire, ds.mean(agg_name), 'revenue_brazil')","03251a99":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n%matplotlib inline \nimport seaborn as sns\nimport datetime\nimport scipy.stats as stats","1c9751b4":"customers = pd.read_csv(\"..\/input\/olist_customers_dataset.csv\")\ngeoloc = pd.read_csv(\"..\/input\/olist_geolocation_dataset.csv\")\nitems = pd.read_csv(\"..\/input\/olist_order_items_dataset.csv\")\npayments = pd.read_csv(\"..\/input\/olist_order_payments_dataset.csv\")\nreviews = pd.read_csv(\"..\/input\/olist_order_reviews_dataset.csv\")\norders = pd.read_csv(\"..\/input\/olist_orders_dataset.csv\")","471dfbce":"cities = customers[\"customer_city\"].nunique()\nc1 = customers.groupby('customer_city')['customer_id'].nunique().sort_values(ascending=False)\nprint(\"Temos \",cities,\" cidades no dataset. As 5 cidades com mais ordens s\u00e3o:\")\nc2 = c1.head(5)\nprint(c2)\nprint(\"\\nAs 5 maiores cidades com ordens representam\", round(c2.sum()\/customers.shape[0]*100,1),\"% de todas as ordens\")\nplt.figure(figsize=(16,8))\nc2.plot(kind=\"bar\",rot=0)","cb3743c3":"fig, ax = plt.subplots(figsize=(9, 8), subplot_kw=dict(aspect=\"equal\"))\nexplode = (0.1, 0, 0, 0)\ncolors = ['#f45a5a', '#449dfc', '#93f96d', '#f9c86d']\nlegend = [\"Cart\u00e3o de Cr\u00e9dito\", \"Boleto\", \"Voucher\", \"D\u00e9bito\"]\n\np = payments[\"payment_type\"][payments[\"payment_type\"] != \"not_defined\"].value_counts()\np.plot(kind=\"pie\", legend=False, labels=None, startangle=0, explode=explode, autopct='%1.0f%%', pctdistance=0.6, shadow=True, textprops={'weight':'bold', 'fontsize':16}, \n       colors=colors, ax=ax)\nax.legend(legend, loc='best', shadow=True, prop={'weight':'bold', 'size':12}, bbox_to_anchor=(0.6, 0, 0.5,1))\nplt.title(\"Forma de Pagamento\", fontweight='bold', size=16)\nplt.ylabel(\"\")","26e44bc5":"As 5 cidades que mais compraram produtos:","ea512972":"**Projeto Integrador 3\nUnivesp \nEngenharia de Computa\u00e7\u00e3o\nAmericana SP**\n\n\n\u201cGEST\u00c3O DA CADEIA DE SUPRIMENTOS NO CONTEXTO DA GLOBALIZA\u00c7\u00c3O 4.0: SISTEMAS E ESTRAT\u00c9GIAS PARA NOVAS OPORTUNIDADES DE MERCADO\u201d\n\nINSER\u00c7\u00c3O DE PEQUENOS EMPRES\u00c1RIOS NO CONTEXTO DE BIG DATA\n\nEste Projeto tem como objetivo demonstrar, atrav\u00e9s de uma ferramenta de an\u00e1lise de dados, como seria poss\u00edvel, para pequenos empres\u00e1rios, alinhar-se \u00e0s novas tecnologias, considerando importante estar atento \u00e0s desvantagens competitivas frente aos grandes varejistas.\n\nPara tanto, ser\u00e1 realizado um estudo suscinto atrav\u00e9s da an\u00e1lise de um dataset disponibilizado pela empresa Olist, a qual intermedia vendas online.\n\nNeste dataset, temos um compilado de 100 mil ordens realizadas entre os anos de 2016 e 2018. Pressup\u00f5e-se que, para os pequenos comerciantes, n\u00e3o haveria acesso a tantas informa\u00e7\u00f5es sobre como os consumidores realizam suas compras online. \n\nEntretanto, com uma simples an\u00e1lise deste dataset, percebe-se que muitas informa\u00e7\u00f5es importantes est\u00e3o dispon\u00edveis. Ainda, s\u00e3o diversos os datasets dispon\u00edveis para consulta, de maneira que a obten\u00e7\u00e3o de informa\u00e7\u00f5es n\u00e3o se mostra empecilho para o correto alinhamento com as novas abordagens de an\u00e1lise e prospec\u00e7\u00e3o de dados.\n\nTal estudo tem como base Notebooks disponibilizados por @kaggle.com\/andresionek, @kaggle.com\/anshumoudgil, @kaggle.com\/paulinan, entre outros, os quais agradecemos imensamente pelas an\u00e1lises p\u00fablicas disponibilizadas.","08cf1f68":"Realizamos o tratamento do dados dos CEP's, e centralizamos nossos dados inicialmente na vari\u00e1vel brazil","4aa90373":"Fun\u00e7\u00f5es auxiliares para plotar mapas","a1b53394":"Especificamente para a cidade de Americana, verificamos que as compras realizadas est\u00e3o distribu\u00eddas por toda a cidade.","766eb940":"Pode-se tamb\u00e9m analisar de forma espec\u00edfica as compras realizadas:","d3054059":"Realizamos incialmente uma consulta aos 10 primeiros registros, exemplificando uma consulta simples"}}