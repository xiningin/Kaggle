{"cell_type":{"14ba8050":"code","ce065fb7":"code","f589602c":"code","aad5d285":"code","8c53bee5":"code","4897cfd6":"code","c52600e4":"code","5ac55403":"code","6586127f":"code","13c1a364":"code","9eaee010":"code","2f031387":"markdown","e12dabaa":"markdown","93ba20af":"markdown","134c135f":"markdown","e8618859":"markdown","9390040d":"markdown"},"source":{"14ba8050":"import numpy as np\nimport pandas as pd\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf","ce065fb7":"data = pd.read_csv('..\/input\/sentiment-analysis-for-financial-news\/all-data.csv', names=['Label', 'Text'], encoding='latin-1')","f589602c":"data","aad5d285":"data.info()","8c53bee5":"def get_sequences(texts):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(texts)\n    \n    sequences = tokenizer.texts_to_sequences(texts)\n    print(\"Vocab length:\", len(tokenizer.word_index) + 1)\n    \n    max_seq_length = np.max(list(map(lambda x: len(x), sequences)))\n    print(\"Maximum sequence length:\", max_seq_length)\n    \n    sequences = pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n    \n    return sequences","4897cfd6":"def preprocess_inputs(df):\n    df = df.copy()\n    \n    sequences = get_sequences(df['Text'])\n    \n    label_mapping = {\n        'negative': 0,\n        'neutral': 1,\n        'positive': 2\n    }\n    \n    y = df['Label'].replace(label_mapping)\n    \n    train_sequences, test_sequences, y_train, y_test = train_test_split(sequences, y, train_size=0.7, shuffle=True, random_state=1)\n    \n    return train_sequences, test_sequences, y_train, y_test","c52600e4":"train_sequences, test_sequences, y_train, y_test = preprocess_inputs(data)","5ac55403":"train_sequences","6586127f":"y_train","13c1a364":"inputs = tf.keras.Input(shape=(train_sequences.shape[1],))\nx = tf.keras.layers.Embedding(\n    input_dim=10123,\n    output_dim=128,\n    input_length=train_sequences.shape[1]\n)(inputs)\nx = tf.keras.layers.GRU(256, return_sequences=True, activation='tanh')(x)\nx = tf.keras.layers.Flatten()(x)\noutputs = tf.keras.layers.Dense(3, activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory = model.fit(\n    train_sequences,\n    y_train,\n    validation_split=0.2,\n    batch_size=32,\n    epochs=100,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=3,\n            restore_best_weights=True\n        )\n    ]\n)","9eaee010":"results = model.evaluate(test_sequences, y_test, verbose=0)\n\nprint(\"    Test Loss: {:.5f}\".format(results[0]))\nprint(\"Test Accuracy: {:.2f}%\".format(results[1] * 100))","2f031387":"# Task for Today  \n\n***\n\n## Financial News Sentiment Prediction  \n\nGiven *financial news headlines*, let's try to predict the **sentiment** of a given headline.\n\nWe will use a TensorFlow RNN to make our predictions.","e12dabaa":"# Getting Started","93ba20af":"# Training","134c135f":"# Data Every Day  \n\nThis notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  \n\n***\n\nCheck it out!  \nhttps:\/\/youtu.be\/JrtXX4cHgBI","e8618859":"# Results","9390040d":"# Preprocessing"}}