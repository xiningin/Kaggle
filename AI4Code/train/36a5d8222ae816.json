{"cell_type":{"b7380a66":"code","460312fe":"code","3e37d31d":"code","9dd84e48":"code","666268b4":"code","9fb572bb":"code","5f3f0c7f":"code","c5e70343":"code","3fff151b":"code","6c77b7a6":"code","cdb5e584":"code","e9a2ba38":"code","8d5f6ca5":"code","1f131138":"code","1356af43":"code","de8ef9de":"code","2022730b":"code","8dd3f4e0":"code","c6509fb8":"code","baa95970":"code","69ae2242":"code","0f104799":"code","6edf2320":"code","05d4c133":"code","0a32cc0b":"code","7d134052":"code","ffbc5390":"code","abe10dfd":"code","db63931a":"code","55ca0f2f":"code","6a2f0b69":"code","a00ee2db":"code","56e300d0":"code","717afbee":"code","5550b71f":"code","15bc4297":"markdown","60a0995f":"markdown","c0237edc":"markdown","21487e25":"markdown","edef6671":"markdown","4ecc581d":"markdown","c1213ae3":"markdown","8baf8c74":"markdown","ad957a20":"markdown","eb2f3ebf":"markdown","04ebbf6c":"markdown","b98b503f":"markdown","323d2607":"markdown","0d3e4db3":"markdown","fe40289b":"markdown"},"source":{"b7380a66":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt","460312fe":"## Read the CSV File Using Pandas read_csv function\ndf = pd.read_csv('..\/input\/oasis_longitudinal.csv')\n\n# print the concise summery of the dataset\ndf.info()\ndf.head()","3e37d31d":"#print concise summery of the dataset\ndf.describe()","9dd84e48":"#since the dataset contain null values also \n#count total rows in each column which contain null values\ndf.isna().sum()","666268b4":"#'duplicated()' function in pandas return the duplicate row as True and othter as False\n#for counting the duplicate elements we sum all the rows\nsum(df.duplicated())","9fb572bb":"#fill null value with their column mean and median\ndf[\"SES\"].fillna(df[\"SES\"].median(), inplace=True)\ndf[\"MMSE\"].fillna(df[\"MMSE\"].mean(), inplace=True)","5f3f0c7f":"#from sklearn.preprocessing import StandardScaler\n#s_sc = StandardScaler()\n#col_to_scale = ['MR Delay']\n#df[col_to_scale] = s_sc.fit_transform(df[col_to_scale])\n#df.head(5)\n\n#This is Scaler but in this project we don`t need it.","c5e70343":"#see how many people have Alzheimer\n#same person visits two or more time so only take the single visit data\nsns.set_style(\"whitegrid\")\nex_df = df.loc[df['Visit'] == 1]\nsns.countplot(x='Group', data=ex_df)","3fff151b":"#We have three groups so convert Converted Group Into Demented\n\nex_df['Group'] = ex_df['Group'].replace(['Converted'], ['Demented'])\ndf['Group'] = df['Group'].replace(['Converted'], ['Demented'])\nsns.countplot(x='Group', data=ex_df)","6c77b7a6":"# bar drawing function\ndef bar_chart(feature):\n    Demented = ex_df[ex_df['Group']=='Demented'][feature].value_counts()\n    Nondemented = ex_df[ex_df['Group']=='Nondemented'][feature].value_counts()\n    df_bar = pd.DataFrame([Demented,Nondemented])\n    df_bar.index = ['Demented','Nondemented']\n    df_bar.plot(kind='bar',stacked=True, figsize=(8,5))\n    print(df_bar)\n                \n                \n# Gender  and  Group ( Female=0, Male=1)\nbar_chart('M\/F')\nplt.xlabel('Group',fontsize=13)\nplt.xticks(rotation=0,fontsize=12)\nplt.ylabel('Number of patients',fontsize=13)\nplt.legend()\nplt.title('Gender and Demented rate',fontsize=14)","cdb5e584":"plt.figure(figsize=(10,5))\nsns.violinplot(x='M\/F', y='CDR', data=df)\nplt.title('Violin plots of CDR by Gender',fontsize=14)\nplt.xlabel('Gender',fontsize=13)\nplt.ylabel('CDR',fontsize=13)\nplt.show()","e9a2ba38":"plt.figure(figsize=(10,5))\nsns.violinplot(x='CDR', y='Age', data=df)\nplt.title('Violin plot of Age by CDR',fontsize=14)\nplt.xlabel('CDR',fontsize=13)\nplt.ylabel('Age',fontsize=13)\nplt.show()","8d5f6ca5":"#find the outliers in each of the column\ndef outliers_iqr(ys):\n    quartile_1, quartile_3 = np.percentile(ys, [25, 75])\n    iqr = quartile_3 - quartile_1\n    lower_bound = quartile_1 - (iqr * 1.5)\n    upper_bound = quartile_3 + (iqr * 1.5)\n    return np.where((ys > upper_bound) | (ys < lower_bound))\n\nlist_atributes = ['MR Delay','EDUC', \"SES\", \"MMSE\", 'eTIV', \"nWBV\", \"ASF\"]\nprint(\"Outliers: \\n\")\nfor item in list_atributes:\n    print(item,': ',outliers_iqr(df[item]))","1f131138":"from pylab import rcParams\nrcParams['figure.figsize'] = 8,5\ncols = ['Age','MR Delay', 'EDUC', 'SES', 'MMSE', 'CDR','eTIV','nWBV','ASF']\nx=df.fillna('')\nsns_plot = sns.pairplot(x[cols])","1356af43":"#boxplots which shows the IQR(Interquartile Range )\nfig, axes = plt.subplots(2,3,figsize = (16,6))\nfig.suptitle(\"Box Plot\",fontsize=14)\nsns.set_style(\"whitegrid\")\nsns.boxplot(data=df['SES'], orient=\"v\",width=0.4, palette=\"colorblind\",ax = axes[0][0]);\nsns.boxplot(data=df['EDUC'], orient=\"v\",width=0.4, palette=\"colorblind\",ax = axes[0][1]);\nsns.boxplot(data=df['MMSE'], orient=\"v\",width=0.4, palette=\"colorblind\",ax = axes[0][2]);\nsns.boxplot(data=df['CDR'], orient=\"v\",width=0.4, palette=\"colorblind\",ax = axes[1][0]);\nsns.boxplot(data=df['eTIV'], orient=\"v\",width=0.4, palette=\"colorblind\",ax = axes[1][1]);\nsns.boxplot(data=df['ASF'], orient=\"v\",width=0.4, palette=\"colorblind\",ax = axes[1][2]);\n#xlabel(\"Time\");","de8ef9de":"#convet the charecter data into numeric\ngroup_map = {\"Demented\": 1, \"Nondemented\": 0}\n\ndf['Group'] = df['Group'].map(group_map)\ndf['M\/F'] = df['M\/F'].replace(['F','M'], [0,1])","2022730b":"def plot_correlation_map( df ):\n    corr = df.corr()\n    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n    cmap = sns.diverging_palette( 240 , 10 , as_cmap = True )\n    _ = sns.heatmap(corr,cmap = cmap,square=True, cbar_kws={ 'shrink' : .9 }, ax=ax, annot = True, annot_kws = { 'fontsize' : 12 })","8dd3f4e0":"plot_correlation_map(df)","c6509fb8":"# Encode columns into numeric\nfrom sklearn.preprocessing import LabelEncoder\nfor column in df.columns:\n    le = LabelEncoder()\n    df[column] = le.fit_transform(df[column])","baa95970":"from sklearn.model_selection import train_test_split\n\nfeature_col_names = [\"M\/F\", \"Age\", \"EDUC\", \"SES\", \"MMSE\", \"eTIV\", \"nWBV\", \"ASF\"]\npredicted_class_names = ['Group']\n\nX = df[feature_col_names].values\ny = df[predicted_class_names].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)","69ae2242":"from sklearn import metrics\ndef plot_confusion_metrix(y_test,model_test):\n    cm = metrics.confusion_matrix(y_test, model_test)\n    plt.figure(1)\n    plt.clf()\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n    classNames = ['Nondemented','Demented']\n    plt.title('Confusion Matrix')\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    tick_marks = np.arange(len(classNames))\n    plt.xticks(tick_marks, classNames)\n    plt.yticks(tick_marks, classNames)\n    s = [['TN','FP'], ['FN', 'TP']]\n    for i in range(2):\n        for j in range(2):\n            plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n    plt.show()","0f104799":"from sklearn.metrics import roc_curve, auc\ndef report_performance(model):\n\n    model_test = model.predict(X_test)\n\n    print(\"\\n\\nConfusion Matrix:\")\n    print(\"{0}\".format(metrics.confusion_matrix(y_test, model_test)))\n    print(\"\\n\\nClassification Report: \")\n    print(metrics.classification_report(y_test, model_test))\n    #cm = metrics.confusion_matrix(y_test, model_test)\n    plot_confusion_metrix(y_test, model_test)\n\ntotal_fpr = {}\ntotal_tpr = {}\ndef roc_curves(model):\n    predictions_test = model.predict(X_test)\n    fpr, tpr, thresholds = roc_curve(predictions_test,y_test)\n    roc_auc = auc(fpr, tpr)\n    total_fpr[str((str(model).split('(')[0]))] = fpr\n    total_tpr[str((str(model).split('(')[0]))] = tpr\n    plt.figure()\n    plt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic')\n    plt.legend(loc=\"lower right\")\n    plt.show()","6edf2320":"total_accuracy = {}\ndef accuracy(model):\n    pred = model.predict(X_test)\n    accu = metrics.accuracy_score(y_test,pred)\n    print(\"\\nAcuuracy Of the Model: \",accu,\"\\n\\n\")\n    total_accuracy[str((str(model).split('(')[0]))] = accu","05d4c133":"from sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.tree import ExtraTreeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.naive_bayes import GaussianNB","0a32cc0b":"rfc=RandomForestClassifier(criterion='gini',max_depth=8,max_features='auto',n_estimators=200)\n\nparam_grid = { \n    'n_estimators': [200],\n    'max_features': ['auto'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini']\n}\n\n#CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5,scoring = 'roc_auc')\nrfc.fit(X_train, y_train.ravel())\n#print(\"Best parameters set found on development set:\")\n#print(rfc.best_params_)\nreport_performance(rfc) \nroc_curves(rfc)\naccuracy(rfc)\n\nfeat_importances = pd.Series(rfc.feature_importances_, index=feature_col_names)\nfeat_importances.nlargest(8).plot(kind='barh')\nplt.title(\"Feature Importance:\")\nplt.show()","7d134052":"svm = SVC(kernel=\"linear\", C=0.1,random_state=0)\nsvm.fit(X_train, y_train.ravel())\nreport_performance(svm) \nroc_curves(svm)\naccuracy(svm)","ffbc5390":"clf_dtc = DecisionTreeClassifier(criterion='entropy',max_depth=5,random_state=0)\nclf_dtc.fit(X_train, y_train.ravel())\nreport_performance(clf_dtc) \nroc_curves(clf_dtc)\naccuracy(clf_dtc)\n#importances = clf.feature_importances_\n\n\nfeat_importances = pd.Series(clf_dtc.feature_importances_, index=feature_col_names)\nfeat_importances.nlargest(8).plot(kind='barh')\nplt.title(\"Feature Importance:\")\nplt.show()","abe10dfd":"params = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [1,2,3,4,5]\n        }\n\nclf_xgb = XGBClassifier(random_state=0)\nclf_xgb.fit(X_train, y_train.ravel())\nreport_performance(clf_xgb) \nroc_curves(clf_xgb)\naccuracy(clf_xgb)","db63931a":"from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection","55ca0f2f":"vote_est = [('etc',ensemble.ExtraTreesClassifier()),\n            ('gb',GradientBoostingClassifier()),\n            ('abc',AdaBoostClassifier()),\n    ('rfc', ensemble.RandomForestClassifier(criterion='gini', max_depth=8, max_features='auto', n_estimators=200)),\n    #('svc', svm.SVC(probability=True)),\n    #('xgb', XGBClassifier()),\n            ('lbgm',LGBMClassifier())\n           ]\n\n\nvote_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\nvote_hard_cv = model_selection.cross_validate(vote_hard, X_train, y_train.ravel())\nvote_hard.fit(X_train, y_train.ravel())\nreport_performance(vote_hard) \nroc_curves(vote_hard)\naccuracy(vote_hard)\n#pred = vote_hard.predict(X_test)\n#accu = metrics.accuracy_score(y_test,pred)\n#print(\"\\nAcuuracy Of the Model: \",accu,\"\\n\\n\")\n\nvote_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\nvote_soft_cv = model_selection.cross_validate(vote_soft, X_train, y_train.ravel())\nvote_soft.fit(X_train, y_train.ravel())\nreport_performance(vote_soft) \nroc_curves(vote_soft)\naccuracy(vote_soft)\n#pred = vote_soft.predict(X_test)\n#accu = metrics.accuracy_score(y_test,pred)\n#print(\"\\nAcuuracy Of the Model: \",accu,\"\\n\\n\")","6a2f0b69":"clfs =[ExtraTreesClassifier(),GradientBoostingClassifier(),AdaBoostClassifier()]","a00ee2db":"for model in clfs:\n    print(str(model).split('(')[0],\": \")\n    model.fit(X_train,y_train.ravel())\n    X = pd.DataFrame(X_train)\n    report_performance(model)\n    roc_curves(model)\n    accuracy(model)","56e300d0":"data = total_accuracy.values()\nlabels = total_accuracy.keys()","717afbee":"plt.plot([i for i, e in enumerate(data)], data, 'ro'); plt.xticks([i for i, e in enumerate(labels)], [l[0:16] for l in labels])\nplt.title(\"Model Vs Accuracy\",fontsize = 14)\nplt.xlabel('Model',fontsize = 13)\nplt.xticks(rotation = 50)\nplt.ylabel('Accuracy',fontsize = 13)","5550b71f":"for i in total_fpr.keys():\n    plt.plot(total_fpr[i],total_tpr[i],lw=1, label=i)\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.legend()","15bc4297":"#### Mini\u2013Mental State Examination (MMSE)\n> The Mini\u2013Mental State Examination (MMSE) or Folstein test is a 30-point questionnaire that is used extensively in clinical and research settings to measure cognitive impairment. It is commonly used in medicine and allied health to screen for dementia. It is also used to estimate the severity and progression of cognitive impairment and to follow the course of cognitive changes in an individual over time; thus making it an effective way to document an individual's response to treatment. The MMSE's purpose has been not, on its own, to provide a diagnosis for any particular nosological entity.\n> - **Cognitive Impairment:** Cognitive impairment is when a person has trouble remembering, learning new things, concentrating, or making decisions that affect their everyday life. Cognitive impairment ranges from mild to severe.\n\n> Any score greater than or equal to 24 points (out of 30) indicates a normal cognition. Below this, scores can indicate severe (\u22649 points), moderate (10\u201318 points) or mild (19\u201323 points) cognitive impairment. The raw score may also need to be corrected for educational attainment and age. That is, a maximal score of 30 points can never rule out dementia.\n\n\n|   **Method**  |**Score**|  **Interpretation**                   |                   \n|---------------|---------|--------------------------------------|\n| Single Cutoff |   <24   |        Abnormal                      |\n|     Range     |   <21   |  Increased Odds of Dementia          |\n|               |   <25   |  Decreased Odds of Dementia          |\n|   Education   |    21   |  Abnormal for 8th Grade Education    |\n|               |   <23   |  Abnormal for High School Education  |\n|               |   <24   |  Abnormal for College Education      |\n|   Severity    |  24-30  |  No Cognitive Impairment             |\n|               |  18-23  |  Mild Cognitive Impairment           |\n|               |   0-17  |  Severe Cognitive Impairment         |","60a0995f":"### XGBOOST","c0237edc":"####  Estimated Total Intracranial Volume(eTIV)\n> Total intracranial volume (TIV\/ICV) is an important covariate for volumetric analyses of the brain and brain regions, especially in the study of neurodegenerative diseases, where it can provide a proxy of maximum pre-morbid brain volume.<br><br>\n> 1. > Unlike brain atrophy in the patients with AD, TIV did not vary over time. Mean TIV did not differ significantly between any of the subject groups. There was no association between TIV and age or age at symptom onset. The only significant predictor of TIV was sex. Men showed an approximately \u223c12% larger eTIV than women. \n\n> 2. > We measured TIV with a semiautomated segmentation technique on T1- and T2-weighted MR images in 55 controls, 10 AD patients, and two persons at risk of familial AD. Whole-brain volumes also were measured and normalized with TIVs.\n\n> 3. > The TIV normalization of cross-sectional brain volumes significantly reduced interindividual variation; the coefficient of variation (CV) was reduced from 10.0% to 6.0% in controls (P <.001). The TIVs measured on T1-weighted images had low variability (CV, 0.16%) and did not differ significantly from those measured on T2-weighted images (P =.16). The TIV normalization of serial brain-volume measurements reduced interimage differences caused by voxel-scaling variations (CV reduced from 1.3% to 0.5%, P =.002) in 10 controls and five AD patients.","21487e25":"### Alzheimer\n  \n\n- Alzheimer's is a type of dementia that causes problems with memory, thinking and behavior. Symptoms usually develop slowly and get worse over time, becoming severe enough to interfere with daily tasks. <br><br>\n\n- Alzheimer's is not a normal part of aging. The greatest known risk factor is increasing age, and the majority of people with Alzheimer's are 65 and older. But Alzheimer's is not just a disease of old age. Approximately 200,000 Americans under the age of 65 have younger-onset Alzheimer\u2019s disease (also known as early-onset Alzheimer\u2019s). <br><br>\n\n-  Alzheimer's is the sixth leading cause of death in the United States. Those with Alzheimer's live an average of eight years after their symptoms become noticeable to others, but survival can range from four to 20 years, depending on age and other health conditions. <br><br>\n\n- Alzheimer's has no current cure, but treatments for symptoms are available and research continues. Although current Alzheimer's treatments cannot stop Alzheimer's from progressing, they can temporarily slow the worsening of dementia symptoms and improve quality of life for those with Alzheimer's and their caregivers. Today, there is a worldwide effort under way to find better ways to treat the disease, delay its onset, and prevent it from developing.","edef6671":"# Alzheimer\u2019s Analysis\n\n## Introduction\n### About Dementia\nDementia is not a specific disease. It\u2019s an overall term that describes a group of symptoms associated with a decline in memory or other thinking skills severe enough to reduce a person\u2019s ability to perform everyday activities. Alzheimer\u2019s disease accounts for 60 to 80 percent of cases. Vascular dementia, which occurs after a stroke, is the second most common dementia type. But there are many other conditions that can cause symptoms of dementia, including some that are reversible, such as thyroid problems and vitamin deficiencies.\n\nDementia is a general term for loss of memory and other mental abilities severe enough to interfere with daily life. It is caused by physical changes in the brain. Alzheimer's is the most common type of dementia, but there are many kinds.","4ecc581d":"#### Clinical Dementia Rating (CDR)\n> The CDR is a 5-point scale used to characterize six domains of cognitive and functional performance applicable to Alzheimer disease and related dementias: Memory, Orientation, Judgment & Problem Solving, Community Affairs, Home & Hobbies, and Personal Care. The necessary information to make each rating is obtained through a semi-structured interview of the patient and a reliable informant or collateral source (e.g., family member).<br><br>\n> The CDR table provides descriptive anchors that guide the clinician in making appropriate ratings based on interview data and clinical judgment. In addition to ratings for each domain, an overall CDR score may be calculated through the use of an algorithm. This score is useful for characterizing and tracking a patient's level of impairment\/dementia:\n\n| Score  | Description               |\n|--------|---------------------------|\n|    0   |        Normal             |\n|    0.5 | Very Mild Dementia        | \n|    1   | Mild Dementia             |\n|    2   | Moderate Dementia         |\n|    3   | Severe Dementia           |[](http:\/\/)","c1213ae3":"### SVM","8baf8c74":"## Other Classifiers","ad957a20":"#### Atlas Scaling Factor   (ASF)\nA unified approach for morphometric and functional data analysis in young, old, and demented adults using automated atlas-based head size normalization: reliability and validation against manual measurement of total intracranial volume.\n\nBasically, total intracranial volume is found to correlate with the determinant of the transform matrix used to align an image with an atlas. The work demonstrates that a one-parameter scaling factor provides a reasonable TIV estimation","eb2f3ebf":"## ROC","04ebbf6c":"### RandomForestClassifier ","b98b503f":"### Understanding the data\n> **Summary**: This set consists of a longitudinal collection of 150 subjects aged 60 to 96. Each subject was scanned on two or more visits, separated by at least one year for a total of 373 imaging sessions. For each subject, 3 or 4 individual T1-weighted MRI scans obtained in single scan sessions are included. The subjects are all right-handed and include both men and women. 72 of the subjects were characterized as nondemented throughout the study. 64 of the included subjects were characterized as demented at the time of their initial visits and remained so for subsequent scans, including 51 individuals with mild to moderate Alzheimer\u2019s disease. Another 14 subjects were characterized as nondemented at the time of their initial visit and were subsequently characterized as demented at a later visit.\n\n**Dataset Description** \n- We will be using the longitudinal MRI data.\n- The dataset consists of a longitudinal MRI data of 150 subjects aged 60 to 96.\n- Each subject was scanned at least once.\n- Everyone is right-handed.\n- 72 of the subjects were grouped as 'Nondemented' throughout the study.\n- 64 of the subjects were grouped as 'Demented' at the time of their initial visits and remained so throughout the study.\n- 14 subjects were grouped as 'Nondemented' at the time of their initial visit and were subsequently characterized as 'Demented' at a later visit. These fall under the 'Converted' category.\n\n\n| COL  | Description                         |\n|------|-------------------------------------|\n| EDUC | Years of Education                  |\n| SES  | Socioeconomic Status                |\n| MMSE | Mini Mental State Examination       |\n| CDR  | Clinical Dementia Rating            |\n| eTIV | Estimated Total Intracranial Volume |\n| nWBV | Normalize Whole Brain Volume        |\n| ASF  | Atlas Scaling Factor                |","323d2607":"### DecisionTreeClassifier","0d3e4db3":"### Voting Classifier","fe40289b":"## Model Comparison"}}