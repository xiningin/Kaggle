{"cell_type":{"323b28c5":"code","3efc9bc7":"code","42329d44":"code","0d4df6ff":"code","1fd752a9":"code","e1590963":"code","a091fa60":"code","6889138e":"code","bbe160e9":"code","465cfd5f":"code","52aec154":"code","0a2f37bb":"code","825b6c48":"code","9c737f5a":"code","ec6426d0":"code","5afe9ca9":"code","bf3c951b":"code","98420e86":"code","d5b6be3b":"code","4c9c83a2":"code","9f81ee46":"code","1a66a4f5":"code","d2290745":"code","cce3274e":"code","79fbf6f1":"code","82b5b07d":"code","bef41b25":"code","dd261b6b":"code","cfa10e38":"code","48927255":"code","2a6766b7":"code","bd4719df":"code","f427cb43":"markdown","3d0fede6":"markdown","665bada7":"markdown","dc849596":"markdown"},"source":{"323b28c5":"!conda install -y pytorch torchvision -c pytorch\n!pip install -U sentence-transformers","3efc9bc7":"# DataFrame\nimport pandas as pd\n\n# Matplot\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Scikit-learn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.manifold import TSNE\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM, Layer, Input,Reshape\nfrom keras import utils\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\n# nltk\nimport nltk\nfrom nltk.corpus import stopwords\nfrom  nltk.stem import SnowballStemmer\n\n# Word2vec\nimport gensim\n\n# Utility\nimport re\nimport numpy as np\nimport seaborn as sns\nimport os\nfrom collections import Counter\nimport logging\nimport time\nimport pickle\nimport itertools\n\n# Set log\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)","42329d44":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0d4df6ff":"filename = '..\/input\/yelp-undersampled\/yelp_reviews_undersampled.csv'\n\ndf_review_data = pd.concat(pd.read_csv(filename, chunksize=10**6), ignore_index=True)","1fd752a9":"df_review_data.shape","e1590963":"# Label Encoding\n\nfrom sklearn import preprocessing\n# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder()\n# Encode labels in column 'labels'\ndf_review_data['labels']= label_encoder.fit_transform(df_review_data['labels'])\ndf_review_data","a091fa60":"df_review_data.labels.value_counts()","6889138e":"import matplotlib.pyplot as plt \n\n# Plotting the sentiment distribution\nplt.figure()\npd.value_counts(df_review_data['labels']).plot.bar(title=\"Sentiment distribution in df\")\nplt.xlabel(\"Sentiment\")\nplt.ylabel(\"No. of rows in df\")\nplt.show()","bbe160e9":"def get_top_data(df, top_n = 5000):\n    df_positive = df[df['labels'] == 2].head(top_n)\n    df_negative = df[df['labels'] == 1].head(top_n)\n    df_neutral = df[df['labels'] == 0].head(top_n)\n    df_small = pd.concat([df_positive, df_negative, df_neutral])\n    return df_small\n\n# Function call to get the top 10000 from each sentiment\ndf_review_data_undersampled = get_top_data(df_review_data, top_n=100000)\n\n# After selecting top few samples of each sentiment\nprint(\"After segregating and taking equal number of rows for each sentiment:\")\nprint(df_review_data_undersampled['labels'].value_counts())","465cfd5f":"df = df_review_data_undersampled.copy()\ndf[\"text\"].fillna(\"\", inplace = True)","52aec154":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()#min_df=2,max_df=0.5, ngram_range=(1,3))\nvectors = vectorizer.fit_transform(df.text)\nfeature_names = vectorizer.get_feature_names()\nvectors.shape","0a2f37bb":"from sklearn.feature_selection import VarianceThreshold\nconstant_filter = VarianceThreshold(threshold = 0.0002)\nconstant_filter.fit(vectors)","825b6c48":"feature_list = []\nfor index in constant_filter.get_support(indices=True):\n    feature_list.append(feature_names[index])\nprint('Number of selected features: ' ,len(list(feature_list)),'\\n')\nprint('List of selected features: \\n' ,list(feature_list))","9c737f5a":"x_train_filter = constant_filter.transform(vectors)\nx_train_filter = pd.DataFrame(x_train_filter.todense(), columns=feature_list)\nprint('TRAIN SIZE (filtered):', x_train_filter.shape)","ec6426d0":"def get_correlation(data, threshold):\n    corr_col = set()\n    cormat = data.corr()\n    for i in range(len(cormat.columns)):\n        for j in range(i):\n            if abs(cormat.iloc[i,j]) > threshold:\n                colname = cormat.columns[i]\n                corr_col.add(colname)\n    return corr_col\n\ncorr_features = get_correlation(x_train_filter, 0.75)","5afe9ca9":"print(\"Removing Correlated Features\")\nprint(\"============================\")\nprint(\"Old size:\", len(feature_list))\nfor feature in corr_features:\n    feature_list.remove(feature)\nprint(\"New size:\", len(feature_list)) ","bf3c951b":"def filter_sentence(x):\n    filtered = []\n    for sentence in list(x):\n        new_sentence = ''\n        for word in sentence.split():\n            if word in feature_list:\n                new_sentence += word + ' '\n        filtered.append(new_sentence)\n        \n    return filtered","98420e86":"x_data_filtered = filter_sentence(df.text)","d5b6be3b":"from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('distilbert-base-nli-mean-tokens')","4c9c83a2":"sentence_embeddings = model.encode(x_data_filtered)\nsentence_embeddings.shape","9f81ee46":"# x_train, x_test, y_train, y_test = train_test_split(sentence_embeddings, df['labels'].values, test_size=0.1, random_state=42, stratify=df['labels'].values)\nx_train, x_test, y_train, y_test = train_test_split(df.text, df['labels'].values, test_size=0.1, random_state=42, stratify=df['labels'].values)","1a66a4f5":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(x_train)\n\nvocab_size = len(tokenizer.word_index) + 1\nprint(\"Total words\", vocab_size)\nmaxlen = 300\nprint(maxlen)\n\nx_train = pad_sequences(tokenizer.texts_to_sequences(x_train), maxlen=maxlen)\nx_test = pad_sequences(tokenizer.texts_to_sequences(x_test), maxlen=maxlen)","d2290745":"embedding_layer = Embedding(sentence_embeddings.shape[0], sentence_embeddings.shape[1], weights=[sentence_embeddings], input_length=300, trainable=False)","cce3274e":"def build_cnn_lstm(embedding_layer):\n    from keras.layers import Conv1D, MaxPooling1D\n    from tensorflow.keras.regularizers import l2\n    # Initialize a sequebtial model\n    model = Sequential()\n#     model.add(Input(768,))\n#     # Add embedding layer\n    model.add(embedding_layer)\n#     model.add(Reshape((768,1)))\n    model.add(Conv1D(filters=128, kernel_size=2, padding='valid', activation='relu'))\n    model.add(MaxPooling1D(pool_size=2))\n    model.add(LSTM(250, kernel_regularizer=l2(0.01)))\n    model.add(Dropout(0.5))\n    model.add(Dense(3, activation='softmax'))\n    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","79fbf6f1":"model = build_cnn_lstm(embedding_layer)\nmodel.summary()","82b5b07d":"# from sklearn.model_selection import train_test_split\n# x_data = sentence_embeddings\n# y_data = df['labels'].values\n# x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.1, random_state=42, stratify=y_data)\n# x_train.shape","bef41b25":"from keras.callbacks import ModelCheckpoint\ncheckpoint_filepath = 'weights.best.hdf5'\nhistory = model.fit(x_train, y_train,\n                    validation_split=0.1,\n                    epochs=36,\n                    batch_size=512,\n                    callbacks=[ModelCheckpoint(filepath=checkpoint_filepath,\n                                              save_weights_only=True,\n                                              monitor='val_accuracy',\n                                              mode='max',\n                                              save_best_only=True)],\n                    verbose=1)","dd261b6b":"# model.load_weights(\"..\/input\/yelp-reviews-feature-selection-lstmcnn\/weights.best.hdf5\")","cfa10e38":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\npred = model.predict(x_test)\npred = np.argmax(pred, axis=1)\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test,pred))\nprint(\"Score: \",round(accuracy_score(y_test,pred)*100,2))\nprint(\"Classification Report:\")\nprint(classification_report(y_test,pred))","48927255":"import numpy as np\nimport seaborn as sns\ncr = classification_report(y_test,pred)\ncr = cr.split()\ncr = cr[4:19]\nprecision = []\nrecall = []\nf1 = []\nsupport = []\ncount = 0\n\nfor text in cr:\n    if count == 1:\n        precision.append(float(text))\n    elif count == 2:\n        recall.append(float(text))\n    elif count == 3:\n        f1.append(float(text))\n    elif count == 4:\n        support.append(int(text))\n    count += 1\n    if count == 5:\n        count = 0\n        \ncm = pd.DataFrame(zip(precision, recall, f1), columns = ['precision', 'recall', 'f1'], index = ['positive','neutral','negative'])\nsns.heatmap(cm, annot = True,vmin=0, vmax=1, cmap=sns.color_palette(\"YlOrRd\"), linewidths=0.01,)","2a6766b7":"from sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, pred, pos_label=2)\nfrom sklearn.metrics import auc\nauc = auc(fpr, tpr)\nplt.figure(1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr, label='Keras (area = {:.3f})'.format(auc))\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc='best')\nplt.show()","bd4719df":"# df.to_csv('export.csv', index=False, header=True)\n# from IPython.display import FileLink\n# FileLink(r'export.csv')","f427cb43":"maxlen = 300","3d0fede6":"Feature selection","665bada7":"Trying out different models","dc849596":"Export Results"}}