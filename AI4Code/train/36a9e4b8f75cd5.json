{"cell_type":{"cc64a368":"code","c6f795bb":"code","25249bdf":"code","491f3aff":"code","0888e234":"code","b1519ae9":"code","9fcd98c7":"code","229f1afc":"code","37b5a966":"code","32711620":"code","0992f7c2":"code","14e4f96d":"code","3b03a2d1":"code","737b5ccf":"markdown","3b63d40a":"markdown","01807e01":"markdown","4a948ca0":"markdown"},"source":{"cc64a368":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import metrics \n\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport itertools\nimport time\nimport random\n\n%matplotlib inline\nsns.set_style(\"whitegrid\")\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nnotebookstart = time.time()","c6f795bb":"train_sales = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\nsubmission_file = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv')\ncalendar = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')\n\nprint(\"Train Sales Shape: {} Rows, {} Columns\".format(*train_sales.shape))\ndisplay(train_sales.head())\nprint(\"Submission File Shape: {} Rows, {} Columns\".format(*submission_file.shape))\ndisplay(submission_file.head(2))\nprint(\"Calendar Shape: {} Rows, {} Columns\".format(*calendar.shape))\ncalendar['date'] = pd.to_datetime(calendar['date'])\ndisplay(calendar.head(5))","25249bdf":"days = range(1, 1913 + 1)\ntime_series_columns = [f'd_{i}' for i in days]\ntime_series_data = train_sales[time_series_columns]","491f3aff":"calendar_merge_cols = ['d','date', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI']\nplot_pd = pd.merge(calendar.loc[:,calendar_merge_cols], train_sales.iloc[:,6:].T, left_on='d', right_index = True, how = 'left')\\\n    .set_index('date')","0888e234":"plot_pd.head()","b1519ae9":"f, ax = plt.subplots(5,1, figsize = [15,10])\n# Demand Sum\ni = 0\ntime_series_data.sum(axis = 0).plot(ax=ax[i], c = 'b')\nax[i].set_title(\"Product Sum Over Time\")\nax[i].set_xlabel(\"Date\")\nax[i].set_ylabel(\"Product Quantity Demanded\")\n\n# Demand Mean\ni += 1\ntime_series_data.mean(axis = 0).plot(ax=ax[i], c = 'k')\nax[i].set_title(\"Product Mean Over Time\")\nax[i].set_xlabel(\"Date\")\nax[i].set_ylabel(\"Product Quantity Demanded\")\n\n# Demand Standard Deviation\ni += 1\ntime_series_data.std(axis = 0).plot(ax=ax[i], c = 'r')\nax[i].set_title(\"Product Stdev Over Time\")\nax[i].set_xlabel(\"Date\")\nax[i].set_ylabel(\"Product Quantity Demanded\")\n\n# Demand Median\ni += 1\ntime_series_data.median(axis = 0).plot(ax=ax[i], c = 'b')\nax[i].set_title(\"Product Median Over Time\")\nax[i].set_xlabel(\"Date\")\nax[i].set_ylabel(\"Q Demanded\")\n\n# Zero to None-Zero Proportion\ni += 1\npd.Series(np.count_nonzero(time_series_data.values, axis = 0) \/ time_series_data.shape[0]).plot(ax=ax[i], c = 'm')\nax[i].set_title(\"Non-Zero Proportion\")\nax[i].set_xlabel(\"Date\")\nax[i].set_ylabel(\"Percent Non-Zero\")\n\nplt.tight_layout(pad=1)\nplt.show()","9fcd98c7":"def flat_function(train, prediction_size, func, kwargs):\n    train = func(train, **kwargs)\n    train = np.tile(train.transpose(), (prediction_size, 1)).transpose()\n    return train\n\ndef step_function(train, step_count, prediction_size, func, kwargs):\n    for step_i in range(0, step_count):\n        step_pred = func(train[:,-prediction_size:], **kwargs)\n        train = np.concatenate((train, step_pred.reshape(-1,1)), axis = 1)\n    return train \n\n# Custom Heuristics","229f1afc":"####################################################################################################\nsimple_models = [\n    # Method, Rolling, Func, Kwargs\n    ('Flat Mean', False, np.mean, dict(axis = 1)),\n    ('Rolling Mean', True, np.mean, dict(axis = 1)),\n    ('Flat Median', False, np.median, dict(axis = 1)),\n    ('Rolling Median', True, np.median, dict(axis = 1)),\n]\ntest_windows = [\n    7,\n    14,\n    28,\n    56\n]\n\nsub_size = 28*2\npred_step_size = 1\nts_metrics = ['rmse','mse','mae']\nplot_metric = 'rmse'\nfull_eval = {}\nfull_oof = {}\n\ncolumns = 1\nrows = math.ceil(len(simple_models)\/columns)\nn_plots = rows*columns\nf,ax = plt.subplots(rows, columns, figsize = [16,rows*4])\n\nfor method_i, (method, rolling, func, kwargs) in enumerate(simple_models):\n    palette = itertools.cycle(sns.color_palette(\"Dark2\", 15))\n    ax = plt.subplot(rows, columns, method_i+1)\n    \n    for plot_i, prediction_size in enumerate(test_windows):\n        time_splits = time_series_data.shape[1] \/\/ prediction_size\n        rows_to_consider = time_splits * prediction_size\n        train_subset = time_series_data.iloc[:, -rows_to_consider:].values\n        time_split_results_list = []\n        train_oof = np.zeros(train_subset.shape)\n        \n        # Time-Split Backtesting..\n        for i in range(0, time_splits - 1):\n            train = train_subset[:,(i)*prediction_size:(i+1)*prediction_size]\n            validation = train_subset[:,(i+1)*prediction_size:(i+2)*prediction_size]\n\n            # Step predicitons or flat values\n            if not rolling:\n                train = flat_function(train=train, prediction_size=prediction_size,\n                                      func=func, kwargs=kwargs)\n            elif rolling:\n                train = step_function(train=train, step_count=prediction_size,\n                            prediction_size=prediction_size, func=func, kwargs=kwargs)\n            else:\n                raise(\"rolling Variables Not Correctly Defined\")\n                \n            rmse = metrics.mean_squared_error(validation, train[:,-prediction_size:],\n                                              squared = False)\n            mse = metrics.mean_squared_error(validation, train[:,-prediction_size:],\n                                             squared = True)\n            mae = metrics.mean_absolute_error(validation, train[:,-prediction_size:])\n            time_split_results_list.append([i, [mse, rmse, mae]])\n            train_oof[:,(i+1)*prediction_size:(i+2)*prediction_size] = train[:,-prediction_size:]\n\n        if not rolling:\n            tmp_matrix = flat_function(\n                train=time_series_data.iloc[:, -prediction_size:].values,\n                prediction_size=sub_size, func=func, kwargs=kwargs)\n        elif rolling:\n            tmp_matrix = step_function(\n                train=time_series_data.iloc[:, -prediction_size:].values,\n                step_count=sub_size,\n                prediction_size=prediction_size, func=func, kwargs=kwargs)\n            tmp_matrix = tmp_matrix[:,-sub_size:]\n        \n        forecast = pd.DataFrame(np.concatenate((tmp_matrix[:,:28], tmp_matrix[:,28:])),\n                                columns = [f'F{i}' for i in range(1, 28 + 1)])\n\n        validation_ids = train_sales['id'].values\n        evaluation_ids = [i.replace('validation', 'evaluation') for i in validation_ids]\n        ids = np.concatenate([validation_ids, evaluation_ids])\n\n        predictions = pd.DataFrame(ids, columns=['id'])\n        \n        predictions = pd.concat([predictions, forecast], axis=1)\n        predictions.to_csv(f\"{method}-{prediction_size}_sub.csv\", index=False)\n\n        # Evaluation\n        time_split_results = pd.DataFrame(\n            time_split_results_list, columns = ['time_slice', 'metrics'])\n        time_split_results[ts_metrics] = pd.DataFrame(\n            time_split_results['metrics'].values.tolist(),\n            index=time_split_results.index)\n        time_split_results.drop([\"metrics\"],axis =1, inplace=True)\n\n        # Overall Eval\n        overall_eval = time_split_results[ts_metrics].mean(axis = 0).to_dict()\n        for k, v in overall_eval.items():\n            overall_eval[k] = round(v, 2)\n            \n        full_oof[f\"{method}-{str(prediction_size)}\"] = np.concatenate([train_oof, tmp_matrix], axis = 1)\n        full_eval[f\"{method}-{str(prediction_size)}\"] = overall_eval\n\n        # Plot\n        line_info = (f\"{method}-{prediction_size} \"\n                     f\"{overall_eval}\")\n        ax.plot(np.linspace(0, rows_to_consider,\n                    num=int(rows_to_consider \/ prediction_size))[1:],\n                time_split_results[plot_metric].values,\n                label=line_info, alpha = .8)\n        \n    ax.set_title(f\"{method}\")\n    ax.set_xlabel(\"Time\")\n    ax.set_ylabel(f\"{plot_metric} Error\")\n    ax.legend(fontsize='medium', loc='center left', bbox_to_anchor=(1, 0.5))\n\nplt.tight_layout(pad=1)\nplt.show()","37b5a966":"for k in full_oof.keys():\n    print(k, full_oof[k].shape)","32711620":"time_series_data.shape","0992f7c2":"# Plot Parameters\nsample_n = 20\nrows = sample_n\ncolumns = 1\nquantile = .99\nground_truth_rolling_average = 2\nplot_method_subset = \"Rolling Mean\"\nplot_last_n_points = 1800\n\n\n# Plot Loop\nf,ax = plt.subplots(columns, rows, figsize = [40,5*sample_n])\nsamples = [random.randint(0, time_series_data.shape[0]) for _ in range(sample_n)]\nfor plot_i, index in enumerate(samples):\n    ax = plt.subplot(rows, columns, plot_i+1)\n    \n    # Plot Predictions\n    for method in [x for x in full_oof.keys() if plot_method_subset in x]:\n        ax.plot(full_oof[method][index,-plot_last_n_points:], label = f\"{method}\", alpha = .6)\n    \n    # Plot Ground Truth\n    ground_truth = pd.Series(time_series_data.values[index,-(plot_last_n_points-sub_size):])\\\n        .rolling(window = ground_truth_rolling_average).mean()\n    ax.plot(ground_truth,\n            alpha = .3, label=\"Ground Truth\", c='k')\n    ax.axvline(x = plot_last_n_points-sub_size, linewidth=5, color='r')\n    \n    # Annotage Plot\n    ax.set_title(f\"{method} - Row:{index}\")\n    ax.set_ylim(0, np.quantile(ground_truth.dropna(), quantile))\n    ax.set_ylabel(\"Product Demand\")\n    ax.set_xlabel(\"Time\")\n    ax.legend(fontsize='medium', loc='center left', bbox_to_anchor=(1, 0.5))\n    \nplt.tight_layout(pad=1)\nplt.show()","14e4f96d":"results = pd.DataFrame(full_eval).T\nresults.sort_values(by=\"rmse\", ascending=True)","3b03a2d1":"print(\"Notebook Runtime: %0.2f Minutes\"%((time.time() - notebookstart)\/60))","737b5ccf":"# KISS - Simple Heuristics Only \n\n_Keep is Simple, Stupid - by Nick Brooks, March 2020_\n\n\n**KISS Series:** <br>\n[Predicting Future Demand Competition Notebook](https:\/\/www.kaggle.com\/nicapotato\/cutting-edge-kiss-method)","3b63d40a":"**Observations:** <br>\n- Increasing product sold over time\n- Standard Deviation has a bump in the middle\n- Median Product Demand is Zero\n- Increasing Non-Zero Proportion suggests that new products are introduced over time.\n\n***\n\n#### Simple Heuristics ONLY","01807e01":"#### Quick Exploration","4a948ca0":"#### Load"}}