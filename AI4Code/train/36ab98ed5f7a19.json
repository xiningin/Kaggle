{"cell_type":{"8305ff4a":"code","be87af80":"code","375b1e3d":"code","4dbc3f81":"code","b35b3f55":"code","1ec9ac51":"code","4b59d107":"code","94b4733a":"code","35a9e0b8":"code","9dff0dd5":"code","719f96f6":"code","e0792229":"code","352e7e29":"code","e251be47":"code","1ead6d0c":"code","214e98e8":"code","5e79a93d":"code","5999ba7f":"code","6a57180c":"code","83b0d8cc":"code","109e7267":"code","6095bca4":"code","202e4278":"markdown","3ea707f2":"markdown","14be460e":"markdown","3034b008":"markdown","9afe571e":"markdown","7a21b738":"markdown","da855e49":"markdown","93b7c1f4":"markdown","ad05a29e":"markdown","69ff5563":"markdown","4496dec3":"markdown","44e068bb":"markdown","135c6ef4":"markdown","741d2385":"markdown","5e30f062":"markdown","0f4915cd":"markdown","234fc5b3":"markdown","f3b20b67":"markdown","36ed0fc9":"markdown","5246c1f7":"markdown","e21fbd1b":"markdown","ed9152b8":"markdown","9211b0b0":"markdown","abeb2ba3":"markdown","c16a2654":"markdown","efb7068a":"markdown","e9fab3af":"markdown","df8b8fb7":"markdown","404a02a0":"markdown"},"source":{"8305ff4a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","be87af80":"# importing dataset\nimport pandas as pd\ndataset = pd.read_csv(\"..\/input\/detection-of-parkinson-disease\/parkinsons.csv\")","375b1e3d":"# Displaying the head and tail of the dataset\n\ndataset.head()","4dbc3f81":"dataset.tail()","b35b3f55":"# Displaying the shape and datatype for each attribute\nprint(dataset.shape)\ndataset.dtypes","1ec9ac51":"# Dispalying the descriptive statistics describe each attribute\n\ndataset.describe()","4b59d107":"# Heatmap visulisation for each attribute coefficient correlation.\nimport seaborn as sb\ncorr_map=dataset.corr()\nsb.heatmap(corr_map,square=True)","94b4733a":"# Now visualise the heat map with correlation coefficient values for pair of attributes.\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# K value means how many features required to see in heat map\nk=10\n\n# finding the columns which related to output attribute and we are arranging from top coefficient correlation value to downwards.\ncols=corr_map.nlargest(k,'status')['status'].index\n\n# correlation coefficient values\ncoff_values=np.corrcoef(dataset[cols].values.T)\nsb.set(font_scale=1.25)\nsb.heatmap(coff_values,cbar=True,annot=True,square=True,fmt='.2f',\n           annot_kws={'size': 10},yticklabels=cols.values,xticklabels=cols.values)\nplt.show()","35a9e0b8":"# correlation coefficient values in each attributes.\ncorrelation_values=dataset.corr()['status']\ncorrelation_values.abs().sort_values(ascending=False)","9dff0dd5":"# Checking null values\ndataset.info()","719f96f6":"# Checking null value sum\ndataset.isna().sum()","e0792229":"# split the dataset into input and output attribute.\n\ny=dataset['status']\ncols=['MDVP:RAP','Jitter:DDP','DFA','NHR','MDVP:Fhi(Hz)','name','status']\nx=dataset.drop(cols,axis=1)","352e7e29":"# Splitting the dataset into trianing and test set\n\ntrain_size=0.80\ntest_size=0.20\nseed=5\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,train_size=train_size,test_size=test_size,random_state=seed)","e251be47":"# Spotcheck and compare algorithms with out applying feature scale.......\n\nn_neighbors=5\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n# keeping all models in one list\nmodels=[]\nmodels.append(('LogisticRegression',LogisticRegression()))\nmodels.append(('knn',KNeighborsClassifier(n_neighbors=n_neighbors)))\nmodels.append(('SVC',SVC()))\nmodels.append((\"decision_tree\",DecisionTreeClassifier()))\nmodels.append(('Naive Bayes',GaussianNB()))\n\n# Evaluating Each model\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nnames=[]\npredictions=[]\nerror='accuracy'\nfor name,model in models:\n    fold=KFold(n_splits=10,random_state=0)\n    result=cross_val_score(model,x_train,y_train,cv=fold,scoring=error)\n    predictions.append(result)\n    names.append(name)\n    msg=\"%s : %f (%f)\"%(name,result.mean(),result.std())\n    print(msg)\n    \n\n# Visualizing the Model accuracy\nfig=plt.figure()\nfig.suptitle(\"Comparing Algorithms\")\nplt.boxplot(predictions)\nplt.show()","1ead6d0c":"# Spot Checking and Comparing Algorithms With StandardScaler Scaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn. preprocessing import StandardScaler\npipelines=[]\npipelines.append(('scaled Logisitic Regression',Pipeline([('scaler',StandardScaler()),('LogisticRegression',LogisticRegression())])))\npipelines.append(('scaled KNN',Pipeline([('scaler',StandardScaler()),('KNN',KNeighborsClassifier(n_neighbors=n_neighbors))])))\npipelines.append(('scaled SVC',Pipeline([('scaler',StandardScaler()),('SVC',SVC())])))\npipelines.append(('scaled DecisionTree',Pipeline([('scaler',StandardScaler()),('decision',DecisionTreeClassifier())])))\npipelines.append(('scaled naive bayes',Pipeline([('scaler',StandardScaler()),('scaled Naive Bayes',GaussianNB())])))\n\n# Evaluating Each model\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nnames=[]\npredictions=[]\nfor name,model in models:\n    fold=KFold(n_splits=10,random_state=0)\n    result=cross_val_score(model,x_train,y_train,cv=fold,scoring=error)\n    predictions.append(result)\n    names.append(name)\n    msg=\"%s : %f (%f)\"%(name,result.mean(),result.std())\n    print(msg)\n    \n\n# Visualizing the Model accuracy\nfig=plt.figure()\nfig.suptitle(\"Comparing Algorithms\")\nplt.boxplot(predictions)\nplt.show()","214e98e8":"# Decision Tree Tunning Algorithms\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nscaler=StandardScaler().fit(x_train)\nrescaledx=scaler.transform(x_train)\nparam_grid=dict()\nmodel=DecisionTreeClassifier()\nfold=KFold(n_splits=10,random_state=5)\ngrid=GridSearchCV(estimator=model,param_grid=param_grid,scoring=error,cv=fold)\ngrid_result=grid.fit(rescaledx,y_train)\n\nprint(\"Best: %f using %s \"%(grid_result.best_score_,grid_result.best_params_))","5e79a93d":"# Logistic Regression Tuning Algorithm\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nscaler=StandardScaler().fit(x_train)\nrescaledx=scaler.transform(x_train)\nc=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\nparam_grid=dict(C=c)\nmodel=LogisticRegression()\nfold=KFold(n_splits=10,random_state=5)\ngrid=GridSearchCV(estimator=model,param_grid=param_grid,scoring=error,cv=fold)\ngrid_result=grid.fit(rescaledx,y_train)\n\nprint(\"Best: %f using %s \"%(grid_result.best_score_,grid_result.best_params_))","5999ba7f":"# Ensemble and Boosting algorithm to improve performance\n\n#Ensemble\n# Boosting methods\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n# Bagging methods\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nensembles=[]\nensembles.append(('scaledAB',Pipeline([('scale',StandardScaler()),('AB',AdaBoostClassifier())])))\nensembles.append(('scaledGBC',Pipeline([('scale',StandardScaler()),('GBc',GradientBoostingClassifier())])))\nensembles.append(('scaledRFC',Pipeline([('scale',StandardScaler()),('rf',RandomForestClassifier(n_estimators=10))])))\nensembles.append(('scaledETC',Pipeline([('scale',StandardScaler()),('ETC',ExtraTreesClassifier(n_estimators=10))])))\n\n# Evaluate each Ensemble Techinique\nresults=[]\nnames=[]\nfor name,model in ensembles:\n    fold=KFold(n_splits=10,random_state=5)\n    result=cross_val_score(model,x_train,y_train,cv=fold,scoring=error)\n    results.append(result)\n    names.append(name)\n    msg=\"%s : %f (%f)\"%(name,result.mean(),result.std())\n    print(msg)\n    \n# Visualizing the compared Ensemble Algorithms\nfig=plt.figure()\nfig.suptitle('Ensemble Compared Algorithms')\nplt.boxplot(results)\nplt.show()","6a57180c":"# GradientBoosting ClassifierTuning\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nscaler=StandardScaler().fit(x_train)\nrescaledx=scaler.transform(x_train)\nn_estimators=[10,20,30,40,50,100,150,200,250,300]\nlearning_rate=[0.001,0.01,0.1,0.3,0.5,0.7,1.0]\nparam_grid=dict(n_estimators=n_estimators,learning_rate=learning_rate)\nmodel=GradientBoostingClassifier()\nfold=KFold(n_splits=10,random_state=5)\ngrid=GridSearchCV(estimator=model,param_grid=param_grid,scoring=error,cv=fold)\ngrid_result=grid.fit(rescaledx,y_train)\n\nprint(\"Best: %f using %s \"%(grid_result.best_score_,grid_result.best_params_))","83b0d8cc":"# Extra Trees Classifier Classifier Tuning\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\nscaler=StandardScaler().fit(x_train)\nrescaledx=scaler.transform(x_train)\nn_estimators=[10,20,30,40,50,100,150,200]\nparam_grid=dict(n_estimators=n_estimators)\nmodel=ExtraTreesClassifier()\nfold=KFold(n_splits=10,random_state=5)\ngrid=GridSearchCV(estimator=model,param_grid=param_grid,scoring=error,cv=fold)\ngrid_result=grid.fit(rescaledx,y_train)\n\nprint(\"Best: %f using %s \"%(grid_result.best_score_,grid_result.best_params_))","109e7267":"# Finalize Model\n# we finalized the Extra Trees Classification Algoriothm and evaluate the model for Detection parkinsons disease\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nscaler=StandardScaler().fit(x_train)\nscaler_x=scaler.transform(x_train)\nmodel=ExtraTreesClassifier(n_estimators=30)\nmodel.fit(scaler_x,y_train)\n\n#Transform the validation test set data\nscaledx_test=scaler.transform(x_test)\ny_pred=model.predict(scaledx_test)\ny_predtrain=model.predict(scaler_x)","6095bca4":"accuracy_mean=accuracy_score(y_train,y_predtrain)\naccuracy_matric=confusion_matrix(y_train,y_predtrain)\nprint(\"train set\",accuracy_mean)\nprint(\"train set matrix\",accuracy_matric)\n\naccuracy_mean=accuracy_score(y_test,y_pred)\naccuracy_matric=confusion_matrix(y_test,y_pred)\nprint(\"test set\",accuracy_mean)\nprint(\"test set matrix\",accuracy_matric)","202e4278":"# 2(b). Classification Models With Feature Scale.","3ea707f2":"# 2(c). Regularisation Tuning For Top 2 Classification Algorithms.","14be460e":"# 1(b). Describing Descriptive Statistics","3034b008":"# 1(f). Handle Outliers\n\nWe didn't find any outliers in our dataset so we can safely go ahead.","9afe571e":"Above is the correlation values in descending order, we have correaltion values in each attribute so we are going to drop from MDVP:RAP column to MDVP:Fhi(Hz) because it have less correlation with other columns.\n\nIf we decrease the column count then accuracy will increase gradually because we are not keeping the irrelevant features.","7a21b738":"# 2(g). Fit and Predict The Best Algorithm.","da855e49":"# 2(d). Ensemble and Boosting Classification Algorithms With Feature Scale.","93b7c1f4":"# 2(h). Accuracy Of An Algorithm.","ad05a29e":"# 2(e). Regularisation Tuning For Top 2 Ensemble and Boosting Classification Algorithms.","69ff5563":"After applying tuning to top 2 ensemble algorithms we got accuracy like\n\n1. Gradient Boosting Classification Algorithm  0.916667 using {'learning_rate': 0.1, 'n_estimators': 100} \n2. Extra Trees Classification Algoriothm 0.917083 using {'n_estimators': 30} ","4496dec3":"Now we are going to tuning to top 2 ensemble algorithms.\n\n1. Gradient Boosting Classification Algorithm\n2. Extra Trees Classification Algoriothm","44e068bb":"# 2(f). Compare All 4 Tunned Algorithms And Selecting The Best Algorithm\n\ncomparing all 4 algorithms top 2 algorithm and top 2 ensemble algorithms.\n\n1. Decision Tree Classification Algorithm Best: 0.852500 using {} \n2. Logistic Regression Classification Algorithm Best: 0.853333 using {'C': 0.1} \n3. Gradient Boosting Classification Algorithm  0.916667 using {'learning_rate': 0.1, 'n_estimators': 100} \n4. Extra Trees Classification Algoriothm 0.917083 using {'n_estimators': 30}","135c6ef4":"# 1(g). Feature Split\n\nSplitting the dataset into input and output attributes.....\n\nwe are going to drop irrelavant column values from our dataset so that we can get better accuracy...","741d2385":"We got accuracy for ensemble algorithms likewise...\n\n1. Ada Boost Classification Algorithm : 0.854167 (0.073409)\n2. Gradient Boosting Classification Algorithm : 0.916667 (0.029226)\n3. Random Forest Classification Algorithm : 0.884583 (0.084411)\n4. Extra Trees Classification Algoriothm : 0.897917 (0.080434)","5e30f062":"Prediction we got without applying feature scaling\n\n1. Logistic Regression Classification Algorithm : 0.859583 (0.114429)\n2. K-Nearest Neighbors classification Algorithm : 0.834167 (0.118714)\n3. Support Vector Machine classification Algorithm : 0.821667 (0.117951)\n4. Decision Tree Classification Algorithm : 0.840000 (0.106771)\n5. Naive bayes Classification Algorithm : 0.735833 (0.071715)","0f4915cd":"# 1(c). Visualising Descriptive Statistics\n\nHistogram plot visualisation for each attribute will be so diffcult because we have high dimensional column 23.\n\nSo Better, we can use heat map to find the correlations coefficient values. we will remove the less correlation coefficient columns. We can remove the irrelavant features it will minimize the accuracy of an algorithm.\n\nIt will be better if we take relavent features columns then we can achive to get good accuracy..","234fc5b3":"After Applying Tuning to top 2 algorithms.\n\n1. Decision Tree Classification Algorithm Best: 0.852500 using {} \n2. Logistic Regression Classification Algorithm Best: 0.853333 using {'C': 0.1} ","f3b20b67":"# 1(e). Label Encoder\/One Hot Encoder\n\nEncoding the Categorical values into numerical values is not required in this dataset. Because all values we have floating type only. we have name column as a categorical values but we are not going to use that column in model prediction.\n\nSo no need to apply label encoding...","36ed0fc9":"Well as u saw above heatmap plot it looks like we did. We got coerrelation coefficient values for each pair of values.\n\nBut we just visualized top 10 coefficient values.\n\nNow we need to print all the coefficient values in each attribute,later we can decide which attribute have relavant and irrelavant features.","5246c1f7":"# 1(d). Checking Null or Empty Values (Data Cleaning)","e21fbd1b":"# 1(h). Resample Evaluate performance model\n\nSplitting the dataset into training and test set...","ed9152b8":"# 1. Feature Enineering\/Data Pre Processing\n\n\n# 1(a). Import Dataset","9211b0b0":"Extra Trees Classification Algoriothm 0.917083 using {'n_estimators': 30} giving the best accuracy performance so we are going to use this ensemble algorithm to fit and predict our dataset","abeb2ba3":"we dont have any null values so now we can safely go ahead...","c16a2654":"# Table Of Content:\n\n# **1. Feature Enineering\/Data Pre Processing**\n\n* 1(a). Import Dataset\n* 1(b). Describing Descriptive Statistics\n* 1(c). Visualising Descriptive Statistics\n* 1(d). Checking Null or Empty Values (Data Cleaning)\n* 1(e). Label Encoder\/One Hot Encoder\n* 1(f). Handle Outliers\n* 1(g). Feature Split\n* 1(h). Resample Evaluate performance model\n# **2. Modeling**\n\n* 2(a). Classification Models Without Feature Scale.\n* 2(b). Classification Models With Feature Scale.\n* 2(c). Regularisation Tuning For Top 2 Classification Algorithms.\n* 2(d). Ensemble and Boosting Classification Algorithms With Feature Scale.\n* 2(e). Regularisation Tuning For Top 2 Ensemble and Boosting Classification Algorithms.\n* 2(f). Compare All 4 Tunned Algorithms And Selecting The Best Algorithm\n* 2(g). Fit and Predict The Best Algorithm.\n* 2(h). Accuracy Of An Algorithm.","efb7068a":"We got training accuracy of 100% and the test set accuracy 92.3% pretty good right....\n\nour model not underfit or overfit it fitted perfectly..\n\nif any suggesion please let me know.","e9fab3af":"# 2. Modeling\n\n# 2(a). Classification Models Without Feature Scale.","df8b8fb7":"Prediction we got without applying feature scaling\n\n1. Logistic Regression Classification Algorithm : 0.859583 (0.114429)\n2. K-Nearest Neighbors classification Algorithm : 0.834167 (0.118714)\n3. Support Vector Machine classification Algorithm : 0.821667 (0.117951)\n4. Decision Tree Classification Algorithm : 0.840000 (0.106771)\n5. Naive bayes Classification Algorithm : 0.735833 (0.071715)\n\n\nPrediction we got with applying feature scaling\n\n1. Logistic Regression Classification Algorithm : 0.859583 (0.114429)\n2. K-Nearest Neighbors classification Algorithm : 0.834167 (0.118714)\n3. Support Vector Machine classification Algorithm : 0.821667 (0.117951)\n4. Decision Tree Classification Algorithm : 0.865833 (0.076508)\n5. Naive bayes Classification Algorithm : 0.735833 (0.071715)","404a02a0":"As per above accuracy we are going to pickup top 2 best performance algorithms.\n\n1. Decision Tree Classification Algorithm\n2. Logistic Regression Classification Algorithm"}}