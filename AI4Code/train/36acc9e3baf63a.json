{"cell_type":{"fb24fdbc":"code","ef601b06":"code","3642c9cf":"code","3906ca89":"code","499ee8d2":"code","f5a3dcae":"code","bece8ae5":"code","9189c130":"code","9043f867":"code","e273b800":"code","e9972fc0":"code","6c18afa9":"code","887b25ce":"code","32841930":"code","2aa62d99":"code","f353b83f":"code","75869032":"code","efb4f467":"code","6c7364a4":"code","74e84220":"code","4fb3a03c":"code","8f1d90a3":"code","a5f943ce":"code","766ece27":"code","da29f7eb":"code","f2d87d81":"code","0823661b":"code","7acf7e8c":"code","1c14cf15":"markdown","2a0619aa":"markdown","ffe756ea":"markdown","2274e6dd":"markdown","a0dacd80":"markdown","4b28c9b2":"markdown","974ebe36":"markdown","a5d99618":"markdown","07000a0d":"markdown","dfab1608":"markdown","4fa29550":"markdown","46ec4967":"markdown","423e250f":"markdown","105af113":"markdown","b5124585":"markdown","90a9a202":"markdown","d3bbfab9":"markdown","67c05897":"markdown","6a87f152":"markdown","2db4da79":"markdown","9d864a42":"markdown","721b7a9f":"markdown","c0dd3d10":"markdown","583f6b56":"markdown","d66330b5":"markdown","4753db7a":"markdown","b72dfe48":"markdown","b7173a02":"markdown","d61c2017":"markdown","e2635c24":"markdown","13bb472a":"markdown","a89d3001":"markdown","fcf3a643":"markdown","2934e305":"markdown","f86c25f5":"markdown","9481f064":"markdown","7e8bfab0":"markdown","543bd1bd":"markdown","c22039a5":"markdown","0d3e82fd":"markdown","f22b024b":"markdown","24c47c0a":"markdown","316e416a":"markdown","9e9ba2fa":"markdown","06125a32":"markdown","918afcf9":"markdown","365509db":"markdown","42e167b7":"markdown","e434f4af":"markdown","0d3148c4":"markdown","9ddb16ec":"markdown","c224fded":"markdown","464ed20b":"markdown","ec6f0230":"markdown","e13cdab4":"markdown","a83fc537":"markdown","74d32ee1":"markdown","60288ed7":"markdown","cafae1ce":"markdown"},"source":{"fb24fdbc":"from IPython.display import YouTubeVideo\n\nYouTubeVideo('iDulhoQ2pro', width=480)","ef601b06":"from IPython.display import YouTubeVideo\n\nYouTubeVideo('-9evrZnBorM', width=480)","3642c9cf":"from IPython.display import YouTubeVideo\n\nYouTubeVideo('-MCYbmU9kfg', width=480)","3906ca89":"import pandas as pd\n\n\ntrain_df = pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/train.csv\")\ntrain_df.sample(5)","499ee8d2":"train_df.loc[3, [\"text\", \"selected_text\", \"sentiment\"]]","f5a3dcae":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    print(a)\n    print(b)\n    print(c)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\ns_1 = \"This is a sentence.\"\ns_2 = \"This is another sentence.\"\n\nprint(jaccard(s_1,s_2))\n\n\n\ns_1 = \"sentence. this is\"\ns_2 = \"is this sentence.\"\n\nprint(jaccard(s_1,s_2))\n\n\ns_1 = \"This is a Sentence.\"\ns_2 = \"THIS IS a sentenCE.\"\n\nprint(jaccard(s_1,s_2))\n\n","bece8ae5":"#\u00a0We need to install pytorch lightning. Notice that we can use pip directly here since this is \n#\u00a0the training notebook. When working with the inference, you need to load it from a dataset though.\n!pip install pytorch_lightning","9189c130":"# Pytorch Lightning, Pytorch, transformers, and tokenizers imports\nfrom pytorch_lightning.core.lightning import LightningModule\nfrom pytorch_lightning import Trainer, seed_everything\n\nfrom torch import nn\nimport torch\nfrom torch.optim import AdamW\nfrom torch.utils.data import Dataset\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.utils.data.dataloader import DataLoader\n\n\nfrom transformers import RobertaModel, RobertaConfig\nfrom tokenizers import ByteLevelBPETokenizer\n\nimport numpy as np\n\n\n# Some constants\nLEARNING_RATE = 0.2 * 3e-5\nMAX_LEN = 192\nTRAIN_BATCH_SIZE = 16\nVALID_BATCH_SIZE = 8\nEPOCHS = 3\nROBERTA_PATH = \"..\/input\/roberta-base\/\"\n\n#\u00a0Create the RoBERTa tokenizer. Notice that we are using the one from \n#\u00a0tokenizers and passing a local file. These would be much easier to do if there was \n#\u00a0a from_pretrained method and if the transformers RoBERTa tokenizer could return \nroberta_tokenizer = ByteLevelBPETokenizer(\n    vocab_file=f\"{ROBERTA_PATH}\/vocab.json\", \n    merges_file=f\"{ROBERTA_PATH}\/merges.txt\", \n    lowercase=True, # TODO: Does keeping uppercase help?\n    add_prefix_space=True # Otherwise, will have an extra space at the start of a sequence.\n)\n\n#\u00a0Create the RoBERTa config\nroberta_config = RobertaConfig.from_pretrained(\"roberta-base\", output_hidden_states=True)","9043f867":"#\u00a0Let's check the config\nroberta_config","e273b800":"class LightningTwitterModel(LightningModule):\n    \n    def __init__(self):\n        super().__init__()\n        self.config = roberta_config\n        self.roberta = RobertaModel.from_pretrained(\"roberta-base\", config=self.config)\n        # You can also try other rates or even different regularization layers\n        self.dropout = nn.Dropout(0.1)\n        #\u00a0Start and end positions\n        self.num_targets = 2\n        # 2 here is for the use of two hidden layers\n        self.classifier = nn.Linear(self.config.hidden_size * 2, self.num_targets)\n        torch.nn.init.normal_(self.classifier.weight, std=0.02)\n        \n    \n    def forward(self, ids, attention_mask):\n        #\u00a0We get the hidden layers (12 here) as the third output\n        _, _, hidden_layers = self.roberta(\n            ids,\n            attention_mask=attention_mask\n        )\n        # Other things that you can explore: use more that just the two last hidden layers.\n        x = torch.cat((hidden_layers[-1], hidden_layers[-2]), dim=-1)\n        x = self.dropout(x)\n        logits = self.classifier(x)\n\n        #\u00a0Some processing to get two outputs (start and end) in the correct format\n        start_logits, end_logits = logits.split(1, dim=-1)\n\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        return start_logits, end_logits\n    ","e9972fc0":"model = LightningTwitterModel()","6c18afa9":"#\u00a0We pass pytorch tensors to check that the model works. Notice the use of unsqueeze to \n#\u00a0get the batch dimension correct.\noutput = model(torch.tensor([0, 1313, 2, 2, 42, 16, 41, 6344, 3545, 328, 2], dtype=torch.long).unsqueeze(0), \n      torch.tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=torch.long).unsqueeze(0), \n)\n\nstart_pos_logit = output[0]\nend_pos_logit = output[1]\nprint(start_pos_logit)\nprint(end_pos_logit)","887b25ce":"#\u00a0Here is the computation for the sentiment ids\nfor sentiment in [\"positive\", \"negative\", \"neutral\"]:\n    print(roberta_tokenizer.encode(sentiment).ids[0])","32841930":"#\u00a0Here it is in action \nfrom transformers import RobertaTokenizerFast\n\n\nanother_roberta_tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\nanother_roberta_tokenizer.encode_plus(\"This is sentence one\", \"This is sentence two\")","2aa62d99":"def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n    \n    # We add a space in front for the Roberata tokenizer. Same thing fot the selected text. \n    #\u00a0As turns out, doing this processing step could be improved. Check the many top solutions \n    #\u00a0for better approaches.\n    tweet = \" \" + \" \".join(str(tweet).split())\n    selected_text = \" \" + \" \".join(str(selected_text).split())\n\n    len_st = len(selected_text) - 1\n    idx0 = None\n    idx1 = None\n\n    #\u00a0Find the start and end \n    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n        if \" \" + tweet[ind: ind+len_st] == selected_text:\n            idx0 = ind\n            idx1 = ind + len_st - 1\n            break\n\n    char_targets = [0] * len(tweet)\n    if idx0 != None and idx1 != None:\n        for ct in range(idx0, idx1 + 1):\n            char_targets[ct] = 1\n\n    #\u00a0Tokenize the tweet text and get ids and offsets\n    # One detail here: we need to use the tokenizer from the tokenizers\n    # library since the one from transformers doesn't provide offsets\n    #\u00a0(or maybe I am wrong, please correct me in the comments if that is the case).\n    tokenzed_tweet = tokenizer.encode(tweet)\n    input_ids_orig = tokenzed_tweet.ids\n    tweet_offsets = tokenzed_tweet.offsets\n    \n    \n    target_idx = []\n    for j, (offset1, offset2) in enumerate(tweet_offsets):\n        if sum(char_targets[offset1: offset2]) > 0:\n            target_idx.append(j)\n    \n    targets_start = target_idx[0]\n    targets_end = target_idx[-1]\n\n    sentiment_id = {\n        'positive': 1313,\n        'negative': 2430,\n        'neutral': 7974\n    }\n    #\u00a00 seems to be the starting token. 2 is the ending one. Between the two, we add the sentiment.\n    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n    # The mask is as long as the input_ids\n    mask = [1] * (len(input_ids))\n    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n    #\u00a0We add 4 since there are always 4 tokens preceeding the target.\n    targets_start += 4\n    targets_end += 4\n\n    \n    #\u00a0How much to pad the text to have the same sequence lengths. \n    padding_length = max_len - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([1] * padding_length)\n        mask = mask + ([0] * padding_length)\n        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n    \n    return {\n        #\u00a0Tokenized input\n        'ids': input_ids,\n        'attention_mask': mask,\n        'targets_start': targets_start,\n        'targets_end': targets_end,\n        'offsets': tweet_offsets, \n        #\u00a0Original input\n        'tweet': tweet,\n        'selected_text': selected_text,\n        'sentiment': sentiment\n    }","f353b83f":"max_len = 14\nprocessed_sample = process_data(\"This is a very AWESOME tweet!\", \"very AWESOME\", \"positive\", roberta_tokenizer, max_len)\ntoken_ids = processed_sample[\"ids\"]\nattention_mask = processed_sample[\"attention_mask\"]\n\n\nassert len(token_ids) == max_len\nassert len(attention_mask) == max_len\n\nprint(token_ids)\nprint(attention_mask)\nprint(processed_sample[\"targets_start\"])\nprint(processed_sample[\"targets_end\"])\nprint(processed_sample[\"offsets\"])","75869032":"max_len = 20\nprocessed_sample = process_data(\"This is an AWESOME tweet!!!!\", \"AWESOME\", \"negative\", roberta_tokenizer, 20)\ntoken_ids = processed_sample[\"ids\"]\n\nassert len(token_ids) == max_len\nprint(token_ids)\nprint(processed_sample[\"offsets\"])","efb4f467":"class TweetDataset(Dataset):\n    def __init__(self, df, max_len=96):\n        self.df = df\n        self.max_len = max_len\n        self.labeled = 'selected_text' in df\n        self.tokenizer = roberta_tokenizer\n\n    def __getitem__(self, index):\n        data = {}\n        row = self.df.iloc[index]\n        \n        ids, masks, tweet, offsets = self.get_input_data(row)\n        data['ids'] = ids\n        data['masks'] = masks\n        data['tweet'] = tweet\n        data['offsets'] = offsets\n        \n        if self.labeled:\n            start_idx, end_idx = self.get_target_idx(row, tweet, offsets)\n            data['start_idx'] = start_idx\n            data['end_idx'] = end_idx\n        \n        return data\n\n    def __len__(self):\n        return len(self.df)\n    \n    def get_input_data(self, row):\n        tweet = \" \" + \" \".join(row.text.lower().split())\n        encoding = self.tokenizer.encode(tweet)\n        sentiment_id = self.tokenizer.encode(row.sentiment).ids\n        ids = [0] + sentiment_id + [2, 2] + encoding.ids + [2]\n        offsets = [(0, 0)] * 4 + encoding.offsets + [(0, 0)]\n                \n        pad_len = self.max_len - len(ids)\n        if pad_len > 0:\n            ids += [1] * pad_len\n            offsets += [(0, 0)] * pad_len\n        \n        ids = torch.tensor(ids)\n        masks = torch.where(ids != 1, torch.tensor(1), torch.tensor(0))\n        offsets = torch.tensor(offsets)\n        \n        return ids, masks, tweet, offsets\n        \n    def get_target_idx(self, row, tweet, offsets):\n        selected_text = \" \" +  \" \".join(row.selected_text.lower().split())\n\n        len_st = len(selected_text) - 1\n        idx0 = None\n        idx1 = None\n\n        for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n            if \" \" + tweet[ind: ind+len_st] == selected_text:\n                idx0 = ind\n                idx1 = ind + len_st - 1\n                break\n\n        char_targets = [0] * len(tweet)\n        if idx0 != None and idx1 != None:\n            for ct in range(idx0, idx1 + 1):\n                char_targets[ct] = 1\n\n        target_idx = []\n        for j, (offset1, offset2) in enumerate(offsets):\n            if sum(char_targets[offset1: offset2]) > 0:\n                target_idx.append(j)\n\n        start_idx = target_idx[0]\n        end_idx = target_idx[-1]\n        \n        return start_idx, end_idx","6c7364a4":"def get_train_val_loaders(df, train_idx, val_idx, batch_size=8):\n    train_df = df.iloc[train_idx]\n    val_df = df.iloc[val_idx]\n\n    train_loader = DataLoader(\n        TweetDataset(train_df), \n        batch_size=batch_size, \n        shuffle=True, \n        num_workers=2,\n        drop_last=True)\n\n    val_loader = DataLoader(\n        TweetDataset(val_df), \n        batch_size=batch_size, \n        shuffle=False, \n        num_workers=2)\n\n    return train_loader, val_loader","74e84220":"#\u00a0Here, I am using a very basic 20\/80% split schema. Better schema should be used of course\n#\u00a0for example stratified kfold over the sentiment column + averaging over many seeds for stability.\ntrain_idx_end = int(len(train_df) * 0.8)\ntrain_idx = train_df.index[:train_idx_end]\nval_idx = train_df.index[train_idx_end:]\n\ntrain_loader, val_loader = get_train_val_loaders(train_df, train_idx, val_idx)","4fb3a03c":"len(train_loader)","8f1d90a3":"len(val_loader)","a5f943ce":"def train_dataloader(self):\n    return train_loader\n\ndef val_dataloader(self):\n    return val_loader\n\n\n\nLightningTwitterModel.train_dataloader = train_dataloader\nLightningTwitterModel.val_dataloader = val_dataloader\n","766ece27":"#\u00a0The loss function \ndef compute_loss(start_logits, end_logits, start_positions, end_positions):\n    # Sum of BCE for start and end logits. \n    loss_fct = nn.CrossEntropyLoss()\n    start_loss = loss_fct(start_logits, start_positions)\n    end_loss = loss_fct(end_logits, end_positions)\n    total_loss = (start_loss + end_loss)\n    return total_loss","da29f7eb":"#\u00a0The evaluation function + some utiliy functions\ndef get_selected_text(text, start_idx, end_idx, offsets):\n    selected_text = \"\"\n    for ix in range(start_idx, end_idx + 1):\n        selected_text += text[offsets[ix][0]: offsets[ix][1]]\n        if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\n            selected_text += \" \"\n    return selected_text\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\n\n# This function works with batches of inputs\ndef compute_jaccard_score(text, start_idx, end_idx, start_logits, end_logits, offsets):\n\n    #\u00a0Some processing so that the output of the model can be used here\n    start_idx = start_idx.cpu().detach().numpy()\n    end_idx = end_idx.cpu().detach().numpy()\n    start_logits = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n    end_logits = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n\n\n    \n    # TODO: Make this vectorized\n    # Since we have a batch of each input, we need to iterate over these\n    # We will take the average of the scores across batches\n    score = 0\n    for i in range(len(text)):\n        start_pred = np.argmax(start_logits[i])\n        end_pred = np.argmax(end_logits[i])\n        if start_pred > end_pred:\n            pred = text[i]\n        else:\n            pred = get_selected_text(text[i], start_pred, end_pred, offsets[i])\n\n        true = get_selected_text(text[i], start_idx[i], end_idx[i], offsets[i])\n    \n        score += jaccard(true, pred)\n    \n    return score \/ len(text)\n","f2d87d81":"def training_step(self, batch, batch_nb):\n    input_ids = batch['ids']\n    attention_masks = batch['masks']\n    tweet = batch['tweet']\n    offsets = batch['offsets']\n    start_positions = batch['start_idx']\n    end_positions = batch['end_idx']\n\n    \n    start_logits, end_logits = self(input_ids, attention_masks)\n\n\n    loss = compute_loss(start_logits, end_logits, start_positions, end_positions)\n\n    logs = {'train_loss': loss}\n    return {'loss': loss, 'log': logs}\n\n\ndef validation_step(self, batch, batch_nb):\n    input_ids = batch['ids']\n    attention_masks = batch['masks']\n    tweet = batch['tweet']\n    offsets = batch['offsets']\n    start_positions = batch['start_idx']\n    end_positions = batch['end_idx']  \n\n    start_logits, end_logits = self(input_ids, attention_masks)\n\n\n    loss = compute_loss(start_logits, end_logits, start_positions, end_positions)\n\n    score = compute_jaccard_score(tweet, start_positions, end_positions, start_logits, end_logits, offsets)\n    return {'val_loss': loss, 'val_score': score}\n\ndef validation_epoch_end(self, outputs):\n\n    return {'val_loss': torch.mean(torch.tensor([output['val_loss'] for output in outputs])).detach(), \n            'val_score': torch.mean(torch.tensor([output['val_score'] for output in outputs])).detach()}\n\n\ndef configure_optimizers(self):\n    param_optimizer = list(self.named_parameters())\n    no_decay = [\n        \"bias\",\n        \"LayerNorm.bias\",\n        \"LayerNorm.weight\"\n    ]\n    optimizer_parameters = [\n        {\n            'params': [\n                p for n, p in param_optimizer if not any(\n                    nd in n for nd in no_decay\n                )\n            ],\n            'weight_decay': 0.001\n        },\n        {\n            'params': [\n                p for n, p in param_optimizer if any(\n                    nd in n for nd in no_decay\n                )\n            ],\n            'weight_decay': 0.0\n        },\n    ]\n    return AdamW(\n        optimizer_parameters,\n        lr=LEARNING_RATE\n    )\n\n\nLightningTwitterModel.training_step = training_step\nLightningTwitterModel.validation_step = validation_step\nLightningTwitterModel.validation_epoch_end = validation_epoch_end\nLightningTwitterModel.configure_optimizers = configure_optimizers","0823661b":"seed_everything(42)\nmodel = LightningTwitterModel()\n#\u00a0Remove the fast_dev_run when running \"for real\":) \ntrainer = Trainer(gpus=1, max_epochs=3, deterministic=True, fast_dev_run=True)\n# Notice that instead of defining the training and validation dataloaders in the model class, you can pass\n# them to the fit method.\ntrainer.fit(model)","7acf7e8c":"%reload_ext tensorboard\n%tensorboard --logdir lightning_logs\/","1c14cf15":"Few more things to notice: \n    \n    \n- As you can see, the second token id is the sentiment\n- Token ids are padded with 1s whereas mask ids are padded with 0s\n- The offsets are the start and end of each word. They are paddeds with 0s\n- The padded token ids and mask ids have the length of `max_len`\n\nLet's try a different example by varying few things","2a0619aa":"Before we start, you can skip this part entierly if you are in a hurry. \n\n\nFew weeks ago, I organized a twitter poll (see what I did there, twitterception) to check which library I should use: \n\n![pytorch_lightnin.png](attachment:pytorch_lightnin.png)\n\n\n\nThe winner was [**PytorchLightning**](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning) so I decided to train the model using it. If you are more familiar with Pytorch, here is a [video](https:\/\/www.youtube.com\/watch?v=QHww1JH7IDU) explaining how to move from one to the other. \n\n\nAlright! In PytorchLightning, there are two main concepts: \n\n\n1. LightningModule\n2. Trainer\n\n\nFirst thing, we define a simple model that inherits from the `LightningModule` class:  \n\n\n``` python \n\nimport torch\nfrom torch.nn import functional as F\nfrom torch import nn\nfrom pytorch_lightning.core.lightning import LightningModule\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\nimport os\nfrom torchvision import datasets, transforms\nfrom torchvision.datasets import MNIST\nfrom torch.optim import Adam\nfrom pytorch_lightning import Trainer, seed_everything\n\n\n\n\nclass LitMNIST(LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.layer_1 = torch.nn.Linear(28 * 28, 128)\n        self.layer_2 = torch.nn.Linear(128, 256)\n        self.layer_3 = torch.nn.Linear(256, 10)\n\n    def forward(self, x):\n        batch_size, _, _, _ = x.size()\n        x = x.view(batch_size, -1)\n        x = self.layer_1(x)\n        x = torch.relu(x)\n        x = self.layer_2(x)\n        x = torch.relu(x)\n        x = self.layer_3(x)\n        x = torch.log_softmax(x, dim=1)\n        return x\n\n    def prepare_data(self):\n        # download only\n        MNIST(os.getcwd(), train=True, download=True)\n\n    def train_dataloader(self):\n        transform=transforms.Compose([transforms.ToTensor(),\n                                      transforms.Normalize((0.1307,), (0.3081,))])\n        mnist_train = MNIST(os.getcwd(), train=True, download=False, transform=transform)\n        return DataLoader(mnist_train, batch_size=64)\n\n\n    def configure_optimizers(self):\n        return Adam(self.parameters(), lr=1e-3)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n\n        # add logging\n        logs = {'loss': loss}\n        return {'loss': loss, 'log': logs}\n\n```\n\n\nThis model can be run similarly to any Pytorch model: \n\n\n``` python \nnet = LitMNIST()\nx = torch.Tensor(1, 1, 28, 28)\nout = net(x)\nprint(out)\n#\u00a0tensor([[-2.2469, -2.2845, -2.3024, -2.3162, -2.2317, -2.3438, -2.2973, -2.3393,\n#\u00a0         -2.3107, -2.3608]], grad_fn=<LogSoftmaxBackward>)\n\n\n```\n\nThe different thing is that in addition to the model per say (`forward` and `__init__` methods mainly), we have here \nthe **data processing**, **optimizer**, and the **training loop** within the LitMNIST class. These steps contain less boilerplate as well.\n\nNext concept, is the `[Trainer](https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/trainer.html)`: \n\n\n``` python \nmodel = LitMNIST()\ntrainer = Trainer(gpus=1, max_epochs=5, deterministic=True)\ntrainer.fit(model)\n```\n\nAs you can see, here we have specified that the trainer should run using one GPU. We will see later that\nit is as easy (modulo some small changes) to use TPUs and that's what we will do. Before moving to the next section, notice that there are other alternatives to PytorchLightning (if you don't like the syntax for example): Pytorch of course and many more. Among these, you can use [fastai v2](https:\/\/github.com\/fastai\/fastai2) library. Maybe I will explore this in a later notebook. ;)","ffe756ea":"Few remarks about this metric: \n    \n* the computation happens at the word level: a word is extracted for each new space. \n* the order of words doesn't matter (second example)\n* upper and lower case words (even when mixed cases) are similar (third example)\n* the evaluation metric is the mean over all the entries\n* the provided implementation isn't robust when both strings are empty","2274e6dd":"For those familiar with NLP and \"modern\" techniques using transformers, you can skip this section entirely. \nFor all the others, time to learn something new! \n\n\n","a0dacd80":"As stated in the evaluation page:\n    \n> The metric in this competition is the word-level Jaccard score \n\nHere is a link to the Wikipedia [page](https:\/\/en.wikipedia.org\/wiki\/Jaccard_index). \n\nKaggle has also provided a Python implementation: \n\n``` python \ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\n```\n\n\nAs far as I know, this metric is also popular in computer vision object detection tasks. Let's have a look at one simple example: ","4b28c9b2":"<a id=\"train\"><\/a>\n# How to train?","974ebe36":"Alright, time to add the different [`DataLoader`](https:\/\/pytorch.org\/docs\/stable\/data.html#torch.utils.data.DataLoader)s. <sup>[5](#dataloader)<\/sup>.\n\nFor that, we will need:\n\n- A Dataset: this is the way Pytorch understands the data. There are a lot of details [here](https:\/\/pytorch.org\/docs\/stable\/data.html) (about dataset styles and so), so  check them if you are curious.\n- A Sampler: this is the way Pytorch samples from the provided. \n\n\nOnce we have these, we can make a DataLoader: one for training, and one for validation. If you want have more folds, you will have a pair of (train, val) DataLoaders for each fold. For the sake of simplicity, we will only make a train\/validation split here. \n\n\n\n\n<a name=\"dataloader\">5<\/a>: This usual business for Pytorch, nothing specific to PytorchLightning.","a5d99618":"Now that the model is defined, one trick is to check that it works by passing one input sample. This serves as a sanity check: have you correctly named the layers that you call in the `forwad` method, is your input what you think it is and so on. ","07000a0d":"In what follows, I introduce the `TweetDataset`. This variation comes from this [notebook](https:\/\/www.kaggle.com\/shoheiazuma\/tweet-sentiment-roberta-pytorch), so thanks [sazuma](https:\/\/www.kaggle.com\/shoheiazuma) for this work as well. For the sake of readability and modularity, the `process_data` function has been \nsplit into two methods: \n\n- `get_input_data`: given a row from a DataFrame, returns the following values: ids, masks, tweet, offsets\n- `get_target_idx`: returns the start and end indices given a row, a tweet text, and offsets\n\nThese two methods are then called to make the data dict in the `__getitem__` method. That's it.","dfab1608":"Alright, enough with the theory, time to apply what we have learned so far to this competition's dataset. ","4fa29550":"# Table of content","46ec4967":"That's it for now. Congratulations for getting to the end!\n\nThis has been a very long notebook and I hope you got something out of it. To be honest, the primary intended reader is myself and I have learned a ton from writing it over the past few weeks. As some parting (cheap) wisdom: the best way to learn something is to teach yourself how to do it.\n\nFor further exploration, check the following resources: \n\n* Another BERT variation, this time from Google: [Albert](https:\/\/arxiv.org\/pdf\/1909.11942.pdf)\n* Nothing to do with NLP, but this was handy: how to create a [table of content](https:\/\/www.kaggle.com\/dcstang\/create-table-of-contents-in-a-notebook#Making-a-Table-of-Contents) in Kaggle\n* Good RoBERTa explanation + TF implementation [details](https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/discussion\/143281)\n* NLP's Imagenet moment blog [post](https:\/\/thegradient.pub\/nlp-imagenet\/)\n* Encoder-Decoder in transformers [explained](https:\/\/medium.com\/huggingface\/encoder-decoders-in-transformers-a-hybrid-pre-trained-architecture-for-seq2seq-af4d7bf14bb8)\n* Another great RoBERTa notebook, this time using Tensorflow by [Chris Deotte](https:\/\/www.kaggle.com\/cdeotte): https:\/\/www.kaggle.com\/cdeotte\/tensorflow-roberta-0-705. The tokenization part is well explained there with a nice graph. Same thing for a model representation. \n* For those that want to get better using fastai, here is a nice [notebook](https:\/\/www.kaggle.com\/maroberti\/fastai-with-transformers-bert-roberta)\n* Another fastai + transformers medium blog [post](https:\/\/towardsdatascience.com\/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2)\n* Same as above but this time as a [notebook](https:\/\/www.kaggle.com\/maroberti\/fastai-with-transformers-bert-roberta)\n* A blog post comparing different [tokenizers](https:\/\/towardsdatascience.com\/comparing-transformer-tokenizers-686307856955)\n* The videos in this notebook are from https:\/\/twitter.com\/ykilcher so thanks to him as well!\n* Good blog post on why Pytorch Lightning might be [worth it](https:\/\/towardsdatascience.com\/supercharge-your-ai-research-with-pytorch-lightning-337948a99eec)\n\nSee you next time. ","423e250f":"<a id=\"detour\"><\/a>\n##\u00a0A small detour: [PytorchLightning](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning)\n","105af113":"From this config, we can deduce few things: \n    \n    \n- `bos_token_id` is 0\n- `eos_token_id` is 2\n- `vocab_size` is 50265, i.e. how many different token ids.\n- `hidden_size` is 768","b5124585":"<a id=\"going-beyond\"><\/a>\n# Going beyond","90a9a202":"Alright, we are almost there. Time to train the model that we have defined above. Few things to add: \n    \n- a loss function: we will go with binary cross entropy for each indix (start and end) and sum these. \n- an evaluation function: this is useful to see how our model performs on the validation dataset. We will be using \nthe competition's metric: Jaccard.\n- an optimizer: this defines the gradient descent algorithm. Will be using AdamW. \n\n\nAnother thing to keep in mind is seeding your model before running it. For that, Pytorch Lightning provide a `seed_everything` function (should be familiar to those used to running Kaggle kernels ;)). To learn more about this, check the Pytorch official [documentation](https:\/\/pytorch.org\/docs\/stable\/notes\/randomness.html).","d3bbfab9":"Alright, enough with transformers. Time to get to BERT!","67c05897":"<a id=\"roberta\"><\/a>\n# What is RoBERTa anyway?","6a87f152":"Another handy thing to know, is how does the RoBERTa tokenizer handle concatenated sequences? \nTo answer this, we can use the `encode_plus` function. \nCredit for this tip goes to Chris Deotte: \n    \n![2_2_roberta_trick.png](attachment:2_2_roberta_trick.png)","2db4da79":"Hello all! \n\nI am new<sup>[1](#new)<\/sup> to both [NLP](https:\/\/en.wikipedia.org\/wiki\/Natural_language_processing) tasks (in its transformers variation at least) and [TPU](https:\/\/fr.wikipedia.org\/wiki\/Tensor_Processing_Unit) training, so this is the best way for me to learn both in the same time (galaxy-brain smartness). \n\nMore specifically, this notebook will focus on understanding and applying the [RoBERTa](https:\/\/huggingface.co\/transformers\/model_doc\/roberta.html) model to the current challenge.\n\nTo start, I will quickly describe and explain how the model works. Then I will build and train a model using the competition's dataset.\n\nBefore starting, notice that some parts of the applied section here are inspired from this great [notebook](https:\/\/www.kaggle.com\/abhishek\/roberta-on-steroids-pytorch-tpu-training) by [Abhishek Thakur](https:\/\/www.kaggle.com\/abhishek), so give it an upvote at least if you have enjoyed his work. Also, a lot of my understanding comes from reading a lot of discussions shared by [Chris Deotte](https:\/\/www.kaggle.com\/cdeotte), so thanks again!\n\n\nAs a teaser, here is a timeline<sup>[2](#transformer-logo)<\/sup> of what we will explore in the next sections: \n\n\n![timeline_nlp.png](attachment:timeline_nlp.png)\n\nFinally<sup>[3](#ad)<\/sup>, check this long discussion if you want to learn more about [TPUs](https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/discussion\/146266). \n\nLet's go!\n\n\n\n<a name=\"new\">1<\/a>: Well I used to be new but now, thanks to writing this notebook and reading hundreds of disucssions, blog posts, and notebooks, I am a little bit wiser. The road ahead is still long but I have made some progress. :)\n\n<a name=\"transformer-logo\">2<\/a>: The transformer-architecture-as-a-logo is blurry, might need to do it again later.\n\n\n<a name=\"ad\">3<\/a>: A minor self-promotion here. The discussion got some upvotes so it must be interesting. ;)\n\n","9d864a42":"What type of fine-tuning should we do here? As stated in the previous section, the first thing to do\nis understand the task and look for something similar in previously fine-tuned tasks: in research papers, previous\nKaggle competitions, i.e. don't be a hero and invent your own (or do it if you have enough experience). \n\nBased on other notebooks, it seems that `selected_text` can be treated as an answer and the `text` as a question. \nSo we can get inspiration from a Q&A task. \n","721b7a9f":"We also get integration with `tensoarboard` but it doesn't seem to work [here](https:\/\/www.kaggle.com\/aagundez\/using-tensorboard-in-kaggle-kernels). Any tips are welcome in the comments!","c0dd3d10":"And as before, we will add new methods to our `LightningTwitterModel` class: \n    \n- `configure_optimizers`: this is how the optimizer should be set up. Nothing fancy here, just the regular stuff.\n- `training_step`: given a batch of data, how to get the targets and then compute the loss\n- `validation_step`: given a batch of data, how to get the targets and then compute the loss and score\n- `validation_end`: how to aggregate the different validation scores at the end of one epoch?","583f6b56":"In the example above, notice that the \"negativity\" (probably a sentiment of anger or disgust) can be understood from the expression \"leave me alone\". ","d66330b5":"What about training on TPUs? \n\nWell, you just need to change the trainer: \n    \n`tpu_trainer = Trainer(num_tpu_cores=8, progress_bar_refresh_rate=5, max_epochs=3)`\n\nYou also need to install the `pytorch-xla` library: this is how Pytorch can communicate with TPUs.\n\n```\u00a0bash\n!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev\n!export XLA_USE_BF16=1\n```\n\nFinally, to make it run here on the Kaggle notebook, you will need to change the Accelerator to **TPU v3-8** (on the right panel, in the Settings) and re-run the cells. If you need more details, let me know in the comments!","4753db7a":"Alright, now that we have a model, we need to feed it some data. For that, we will need to: \n    \n    \n- Preprocess the data\n- Tokenize the dataset\n\nOnce these steps done, we will be able to add the `train_dataloader`, `val_dataloader`, and `test_dataloader` methods. \n","b72dfe48":"This will be a (very) long notebook so feel free to skip to any part you like using this table of content: \n\n- [Introduction](#intro)\n- [Before you start: NLP with Transformers](#before-you-start)\n * [Transformers 101](#transformers-101)\n * [BERT and the NLP revolution](#bert)\n- [What is RoBERTa anyway?](#roberta)\n- [How does it work?](#how)\n- [How to train?](#train)\n * [A small detour: Pytorch Lightning](#detour)\n * [Tokens](#tokens)\n- [Application: Tweet Sentiment Extraction](#application)\n * [Understanding the task](#understanding)\n * [Evaluation metric](#loss)\n * [Data processing](#processing)\n * [The model](#model)\n * [Training](#training)\n- [Going beyond](#going-beyond)","b7173a02":"Final step: training. That's the best part in Pytorch Lightning: \n    \n- you create an instance of your `model`\n- you setup a `Trainer` instance with the type of the accelerator\n- you `.fit` your `model`\n\nThat's it, really. Since this is a demo notebook, I will also pass the `fast_dev_run=True` option \nin the Trainer to only run for one step and get the results faster. ","d61c2017":"Before we start, we can do some preliminary work: \n    \n    \n``` python \nsentiment_id = {\n    'positive': 1313,\n    'negative': 2430,\n    'neutral': 7974\n}\n\n```\n\nYou may ask, where does these values come from? And why do we need them in the first place?\n\n\n1. You get these ids by running the RoBERTa tokenizer on the three types of sentiments: \n\n\n``` python \n\nfor sentiment in [\"negative\", \"positive\", \"neutral\"]:\n    print(roberta_tokenizer.encode(sentiment).id)\n\n```\n\n2. So instead of re-running the same computation over and over again for each row, we store these into a dict (a cheap cache). \nWe need these values to correctly tokenize the `text` + `extracted_text`. More about this soon.","e2635c24":"<a id=\"training\"><\/a>\n## Training ","13bb472a":"Now, we need to add two new methods to our `LightningTwitterModel` class: \n    \n- train_dataloader: this should return the training dataloader\n- val_dataloader: same thing but for the validation","a89d3001":"<a id=\"intro\"><\/a>\n# Introduction ","fcf3a643":"![roberta_meets_tpus.jpg](attachment:roberta_meets_tpus.jpg)","2934e305":"As stated before, we will be using PytorchLighning alongside the [transformers](https:\/\/huggingface.co\/transformers\/) (awesome) library from [huggingface](https:\/\/huggingface.co\/). \n\nHere is what we need: \n\n\n- A pretrained RoBERTa base\n- Adding a \"head\" (i.e. some new layers) for the task\n\n\n\nLet's go!","f86c25f5":"From the FAIR blog post again: \n    \n    \n> RoBERTa builds on BERT\u2019s language masking strategy, wherein the system learns to predict intentionally hidden sections of text within otherwise unannotated language examples. RoBERTa, which was implemented in PyTorch, modifies key hyperparameters in BERT, including removing BERT\u2019s next-sentence pretraining objective, and training with much larger mini-batches and learning rates. This allows RoBERTa to improve on the masked language modeling objective compared with BERT and leads to better downstream task performance. We also explore training RoBERTa on an order of magnitude more data than BERT, for a longer amount of time. We used existing unannotated NLP data sets as well as CC-News, a novel set drawn from public news articles.\n\n> After implementing these design changes, our model delivered state-of-the-art performance on the MNLI, QNLI, RTE, STS-B, and RACE tasks and a sizable performance improvement on the GLUE benchmark. With a score of 88.5, RoBERTa reached the top position on the GLUE leaderboard, matching the performance of the previous leader, XLNet-Large. These results highlight the importance of previously unexplored design choices in BERT training and help disentangle the relative contributions of data size, training time, and pretraining objectives.\n\n\nHere are the main changes made to the BERT model to make it better:\n    \n    \n>Our modifications are simple, they include: (1)training the model longer, with bigger batches,over more data; (2) removing the next sentence prediction objective; (3) training on longer sequences; and (4) dynamically changing the masking pattern applied to the training data\n\n\nAs mentioned in the transformers [documentation](https:\/\/huggingface.co\/transformers\/model_doc\/roberta.html), here are some of the tips summarized:\n    \n* This implementation is the same as BertModel with a tiny embeddings tweak as well as a setup for Roberta pretrained models.\n\n* RoBERTa has the same architecture as BERT, but uses a byte-level BPE as a tokenizer (same as GPT-2) and uses a different pre-training scheme.\n\n* RoBERTa doesn\u2019t have token_type_ids, you don\u2019t need to indicate which token belongs to which segment. Just separate your segments with the separation token tokenizer.sep_token (or `<\/s>`)\n\n\nSo, the **TL;DR** of RoBERTa vs BERT are the following: \n\n\n1. Use of different tokenizers: byte-level BPE vs WordPiece \n2. Different pre-training procedures: no NSP for RoBERTa\n3. Longer training with larger batches on more data\n4. Dynamically changing the masking strategy\n\nThere are many more details that we won't explore here. For more details, check the original [paper](https:\/\/arxiv.org\/pdf\/1907.11692.pdf). Also, if you like videos, check this out (a visual analysis of the paper on the next cell): ","9481f064":"<a id=\"transformers-101\"><\/a>\n\n## Transformers 101","7e8bfab0":"![roberta_paper_header.png](attachment:roberta_paper_header.png)","543bd1bd":"<a id=\"before-you-start\"><\/a>\n# Before you start: NLP with Transformers","c22039a5":"<a id=\"loss\"><\/a>\n## Evaluation metric","0d3e82fd":"<a id=\"processing\"><\/a>\n##\u00a0Data processing","f22b024b":"<a id=\"understanding\"><\/a>\n## Understanding the task","24c47c0a":"Time is short, so let's train a model!","316e416a":"Let's unpack this whole `process_data` function: \n\n\n* Start by finding the positions of start and end words in from the `selected_text` within the `text`\n* Tokenizing the tweet `text` and extracting ids and offsets\n* Adding 0, sentiment_token_id, 2, 2 to the token_ids and four (0,0) at the start of the offsets and one (0, 0) at the end.\n* Adding 4 to the start and end target tokens. This is done since there are always 4 tokens before: 0, sentiment_token_id, 2, 2.\n* Finally, we pad the tokenized input, the mask, and the offsets to match the `max_len`: each sequence has its special padding token.\n\n\n\nNext, we will run it on different inputs and check the outputs.","9e9ba2fa":"From the [FAIR](https:\/\/ai.facebook.com\/blog\/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems\/) team's blog post:\n\n> A robustly optimized method for pretraining natural language processing (NLP) systems that improves on Bidirectional Encoder Representations from Transformers, or BERT, the self-supervised method released by Google in 2018. BERT is a revolutionary technique that achieved state-of-the-art results on a range of NLP tasks while relying on unannotated text drawn from the web, as opposed to a language corpus that\u2019s been labeled specifically for a given task. The technique has since become popular both as an NLP research baseline and as a final task architecture. BERT also highlights the collaborative nature of AI research \u2014 thanks to Google\u2019s open release, we were able to conduct a replication study of BERT, revealing opportunities to improve its performance. Our optimized method, RoBERTa, produces state-of-the-art results on the widely used NLP benchmark, General Language Understanding Evaluation (GLUE).","06125a32":"Once the transformer model came out, a lot of innovation happened. And among the most important (or at least a precursor for other things) for the NLP world was [BERT](https:\/\/en.wikipedia.org\/wiki\/BERT_(language_model)). \n\n\n<img src=\"https:\/\/vignette.wikia.nocookie.net\/muppet\/images\/e\/e1\/Bert_smile.png\/revision\/latest\/scale-to-width-down\/280?cb=20110630173259\" width=200>\n\n\nWell, not the [Sesame Street](https:\/\/en.wikipedia.org\/wiki\/Sesame_Street) [character](https:\/\/muppet.fandom.com\/wiki\/Bert) of course. :) \n\n\nWhat is **BERT** then? \n\n\nIt is short for **Bidirectional Encoder Representations from Transformers**. This is a pre-training self-supervised language model \nbased on the transformers building block. It has been developed by Google around 2018 (last quarter of 2018). \n\nAs the name suggests, since this is a language model (i.e. nothing to predict, only learning a language representation), only the encoder part from the transformer's architecture is necessary.\n\nOne of the main innovations compared to previous models that also use the transformer as a building block ([ELMo](https:\/\/allennlp.org\/elmo) and [GPT-1](https:\/\/openai.com\/blog\/language-unsupervised\/) for example) is the **bidirectionality** of the connections (check graph below): \n\n\n<img src=\"https:\/\/miro.medium.com\/max\/1234\/1*KbAUVetHPMreJdcbicmJrw.png\" alt=\"bert_vs_gpt_vs_elmo\" width=720>\n\n\nTo be more precise, BERT could be described as being non-directional since each input sequence is fed at once. But that wouldn't\nmake a good Sesame Street name, would it? ;)\n\n\nNow you might ask how is the model trained if there isn't any privileged direction (thus can't predict next-word for example)? There are two things to know: \n\n\n- **MLM** (Masked Language Model): the first idea is to train a language model by masking some of the tokens and asking the model to predict these masked tokens (only the masked ones). For this task, the model uses a mask embedding (check the next graph)\n- **NSP** (Next Sentence Prediction): the second idea is to concatenate two sentences in two different ways, either using two consecutive sentences (in most cases there is a casual connection between the two) or one sentence followed by a randomly selected one. Then letting the model learn how to distinguish between the two scenarios. For this, the model uses a sequence embedding (check the next graph).\n\nThese two powerful ideas are combined when training a BERT model. That's not the whole story of course and many more \ndetails are missing. To wrap up, here is how the embedding layers look like: \n\n![bert_embeddings.png](attachment:bert_embeddings.png)\n\n\nand some tips from the transformers' [documentation](https:\/\/huggingface.co\/transformers\/model_doc\/bert.html): \n\n\n\n- BERT is a model with absolute position embeddings so it\u2019s usually advised to pad the inputs on the right rather than the left.\n\n- BERT was trained with a masked language modeling (MLM) objective. It is therefore efficient at predicting masked tokens and at NLU in general, but is not optimal for text generation. Models trained with a causal language modeling (CLM) objective are better in that regard.\n\n- Alongside MLM, BERT was trained using a next sentence prediction (NSP) objective using the [CLS] token as a sequence approximate. The user may use this token (the first token in a sequence built with special tokens) to get a sequence prediction rather than a token prediction. However, averaging over the sequence may yield better results than using the [CLS] token.\n\n\n\n\n\nThat's it for BERT. The ideas aren't necessarily very complex to understand in isolation but understanding everything and making the different connections might take some time. So don't feel bad if you don't understand everything at first. Read more later and revist the concepts you have learned. \n\nYou can find the paper [here](https:\/\/arxiv.org\/pdf\/1810.04805.pdf). I also recommend checking the transformers documentation [page](https:\/\/huggingface.co\/transformers\/model_doc\/bert.html) and this medium blog [post](https:\/\/towardsdatascience.com\/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270). Finally, if you prefer watching videos, check this one (next cell): \n","918afcf9":"<a id=\"model\"><\/a>\n##\u00a0The model","365509db":"And with the `Dataset` defined, we can get the train and validation DataLoaders. I will use 20\/80% split for the sake of \nsimplicity (but you should experiment with better options). ","42e167b7":"First thing to do is to understand what we need to predict and the data at hand. \nHere we are given some tweets (this is from a popular dataset), associated sentiments (positive, negative, or neutral), and \nthe **text that helps extract the sentiment**. \n\nFor example: \n\n```\n\ntext = Sooo SAD I will miss you here in San Diego!!!\n\nsentiment = negative \n\nselected_text = Sooo SAD\n\n\n```\n\nThe task is thus to extract a sub-part of a given sentence that captures the sentiment.","e434f4af":"<a id=\"application\"><\/a>\n# Application: Tweet Sentiment Extraction","0d3148c4":"We are almost there. Before we start training, in the context of this notebook, training here means using RoBERTa as a backbone and then adding some\nadditional layers to make the generic backbone work well for the task at hand. \n\nFor that, we need the following things: \n     \n- The model: RoBERTa backbone + other layers.\n- How to get the [tokens](https:\/\/en.wikipedia.org\/wiki\/Lexical_analysis#Tokenization): transforming textual information into numbers.\n- Training: loading data, setting the model, backpropagation, optimizer, and so on. The usual stuff really.\n","9ddb16ec":"<a id=\"tokens\"><\/a>\n##\u00a0Tokens","c224fded":"<a id=\"how\"><\/a>\n# How does it work?","464ed20b":"Now that the model is defined, we can move to the extraction of tokens. \n\nBut first, what are tokens? From [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Lexical_analysis#Tokenization): \n\n\n> Tokenization is the process of demarcating and possibly classifying sections of a string of input characters.\n\nTokens are not necessarily words and aren't always obtained by splitting on space but are rather entities that make parsing the text possible. For more details about tokens, check this [link](https:\/\/nlp.stanford.edu\/IR-book\/html\/htmledition\/tokenization-1.html). \n\n\nNow, how to get these? Well, as many things in the ML world, there is a library for that: introducing [tokenizers](https:\/\/github.com\/huggingface\/tokenizers). \n\nYou can also access tokenizer classes directly from the transformers library. Here are some details from the [documentation](https:\/\/huggingface.co\/transformers\/main_classes\/tokenizer.html): \n\n\n> A tokenizer is in charge of preparing the inputs for a model. The library comprise tokenizers for all the models. Most of the tokenizers are available in two flavors: a full python implementation and a \u201cFast\u201d implementation based on the Rust library tokenizers. The \u201cFast\u201d implementations allows (1) a significant speed-up in particular when doing batched tokenization and (2) additional methods to map between the original string (character and words) and the token space (e.g. getting the index of the token comprising a given character or the span of characters corresponding to a given token). Currently no \u201cFast\u201d implementation is available for the SentencePiece-based tokenizers (for T5, ALBERT, CamemBERT, XLMRoBERTa and XLNet models).\n\n\nAs you have seen above, RoBERTa uses a different tokenizer than the one used by BERT: byte-level BPE vs WordPiece. Here are the main differences between the two: \n\n* RoBERTa's default tokenizer works at the byte-level vs word pieces for BERT.\n* RoBERTa's tokenizer keeps all combined tokens (up to the vocabulary max size) whereas BERT's only keeps those that increase\n\nFor more details, check this StackOverflow [thread](https:\/\/stackoverflow.com\/questions\/55382596\/how-is-wordpiece-tokenization-helpful-to-effectively-deal-with-rare-words-proble) and this medium blog [post](https:\/\/towardsdatascience.com\/comparing-transformer-tokenizers-686307856955) with comparaison of different tokenizers. Finally, if you want to learn more about BPE implementation, check this [blog](https:\/\/towardsdatascience.com\/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10). \n\n\n\nNext, to get the tokenizer for RoBERTa, it is as simple as the following using the `tokenizers` library: \n\n\n``` python \n\nfrom tokenizers import ByteLevelBPETokenizer\n\n\ntokenizer = ByteLevelBPETokenizer(vocab_file=\"path\/to\/vocab.json\", \n                                  merges_file=\"path\/to\/merges.txt\", \n                                  add_prefix_space=True)\n\ntokenizer.encode(\"A sentence to encode\")\n\n```\n\n\nor directly using the `transformers` library\n\n``` python \n\nfrom transformers import RobertaTokenizerFast\n\ntokenizer = RobertaTokenizerFast(vocab_file=\"path\/to\/vocab.json\", \n                                 merges_file=\"path\/to\/merges.txt\", \n                                 add_prefix_space=True)\n\ntokenizer.encode(\"A sentence to encode\")\n\n```\n\nFew more things to know about the RoBERTa's tokenizer:\n\n\n- The first 4 token ids are the following (special tokens): `{\"<s>\": 0, \"<pad>\": 1, \"<\/s>\": 2, \"<unk>\": 3}`\n- RoBERTa's default vocabulary size is **50262** (compare this to BERT's uncased 30522)\n- You can use the classmethod `from_pretrained` to get the default vocab and merges files from the internet (well this\nis a generic fact for all transformers's tokenizers).\n","ec6f0230":"So as you can see, the first token id is 0, the end of the first sentence is 2, the start of the seconde sentence is 2,\nand the last token is 2. Alright, time to write the processing function<sup>[4](#processing)<\/sup>.\n\n<a name=\"processing\">4<\/a>: Most of the processing function is from [Abhishek Thakur](https:\/\/www.kaggle.com\/abhishek) great kernel so again, thanks to him!","e13cdab4":"Sweet! Time to train!","a83fc537":"This is only the training part. For inference, you can check this [notebook](https:\/\/www.kaggle.com\/yassinealouini\/model-inference-only). You only need to save the model as a dataset and load it. \nOne more thing to notice is that the inference part uses GPU and not TPU since it wasn't allowed. Some people have reported some discrepancy between training on TPU vs training on GPU then predicting on GPU. If you know why, please share your insights in the comments below.","74d32ee1":"Alright, enough with BERT. Let's move to RoBERTa. Could it be BERT's mom (as suggested by [Maximilien Roberti](https:\/\/www.kaggle.com\/maroberti))?\n\n\n![bert_mom.webp](attachment:bert_mom.webp)\n\nAlright, joke aside, RoBERTa is an NLP model that introduces slight variations on the BERT model (more about these below) and achieves better results. \n\nIt has been proposed by researchers (list in the screenshot below) from [FAIR](https:\/\/ai.facebook.com\/) around 2019. Since then, the model has been used in many settings and in particular at Kaggle to win competitions: [here](https:\/\/www.kaggle.com\/c\/google-quest-challenge\/discussion\/129978) and [here](https:\/\/www.kaggle.com\/c\/tensorflow2-question-answering\/discussion\/127339). ","60288ed7":"<a id=\"bert\"><\/a>\n## BERT and the NLP revolution","cafae1ce":"I will try to be precise and concise. As far as I know, NLP applications started gaining momentum once the [transformer](https:\/\/en.wikipedia.or\/wiki\/Transformer_(machine_learning_model)) paper was released (around end of 2017). \n\n\nIn its essence, a **transformer** model is a seq2seq self-supervised task that only uses self-attention. Voil\u00e0, easy. ;)\n\nLet's unpack this for a second: \n\n\n- [seq2seq](https:\/\/en.wikipedia.org\/wiki\/Seq2seq): this means that the model uses more than one sample to produce more than one output\n- [self-supervised](https:\/\/project.inria.fr\/paiss\/files\/2018\/07\/zisserman-self-supervised.pdf): this is in contrast to supervised learning, i.e. no labels are needed, only training data. \n- [self-attention](https:\/\/jalammar.github.io\/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention\/): a technique used in seq2seq models to better incorporate the context by leveraging many of the hidden states (and not the last one).  \n\nIn more details, it is a stack of encoders and decoders where inputs flow through encoders, outputs flow through decoders and intermediate representations are shared between the decoder and encoder (check the graph below):\n\n<img src=\"http:\/\/jalammar.github.io\/images\/t\/transformer_resideual_layer_norm_3.png\" width=480>\n\nSince the inputs and outputs are textual, there is a need for an embedding layer: a layer that takes as input words and returns number mappings (i.e. one word \ngets mapped to a unique number). \n\n\n\nHere is an implementation of an encoder\/decoder architecture (from the [annotated](http:\/\/nlp.seas.harvard.edu\/2018\/04\/03\/attention.html) paper):\n\n``` python \nclass EncoderDecoder(nn.Module):\n    \"\"\"\n    A standard Encoder-Decoder architecture. Base for this and many \n    other models.\n    \"\"\"\n    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n        super(EncoderDecoder, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_embed = src_embed\n        self.tgt_embed = tgt_embed\n        self.generator = generator\n        \n    def forward(self, src, tgt, src_mask, tgt_mask):\n        \"Take in and process masked src and target sequences.\"\n        return self.decode(self.encode(src, src_mask), src_mask,\n                            tgt, tgt_mask)\n    \n    def encode(self, src, src_mask):\n        return self.encoder(self.src_embed(src), src_mask)\n    \n    def decode(self, memory, src_mask, tgt, tgt_mask):\n        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n```\n\nEach encoder and decoder blocks are made of two essential elements: a self-attention block and a feed-forward one (check the graph below). The decoder block has one additional self-attention block for the signal it receives from the encoder. There are many other details I am omitting, so check the resources for more details. Here is how the diagram looks like: \n\n<img src=\"https:\/\/miro.medium.com\/max\/1400\/1*BHzGVskWGS_3jEcYYi6miQ.png\" width=480>\n\n\nOne final detail, notice that the original transformer paper had **6** of these stacked. Many variations on the original transformer will make slight changes to one single block and also on how many blocks are stacked and how they are connected (and other details). I will stop here before loosing too much audience. \n\n\nIf you haven't checked the famous [illustrated transformer](http:\/\/jalammar.github.io\/illustrated-transformer\/) blog post, go read it now. From there, you will come across two other great resources: the original [paper](https:\/\/arxiv.org\/pdf\/1706.03762.pdf) and an [annotated](http:\/\/nlp.seas.harvard.edu\/2018\/04\/03\/attention.html) version (with code). You can also add this [medium](https:\/\/medium.com\/inside-machine-learning\/what-is-a-transformer-d07dd1fbec04) post to your transformers reading list. Finally, if you prefer videos, check this one (next cell): "}}