{"cell_type":{"23292ce0":"code","9f9ec382":"code","0d13ec91":"code","451b3ec0":"code","8568558d":"code","ebd6024a":"code","2e583356":"code","d3bd20b3":"code","4058875d":"code","75ba1ea8":"code","05138a14":"code","99abb16b":"code","a9bdf724":"code","57328d7d":"code","09f81157":"code","6fcd1842":"code","0febccb3":"code","e4c8113f":"code","21444029":"code","53217a21":"code","7233fbc8":"code","224031aa":"markdown","2ff40385":"markdown","d5ae85f1":"markdown","a7a332ee":"markdown","816cc821":"markdown","35918221":"markdown","a8b76cda":"markdown","2047d253":"markdown","175b8ac7":"markdown"},"source":{"23292ce0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom string import punctuation\n\n# Import NLTK packages - natural language processing\nimport nltk\nfrom nltk.tokenize import word_tokenize\n#from nltk.stem import WordNetLemmatizer\n#lemmatizer = WordNetLemmatizer()\nfrom nltk.stem import PorterStemmer\nstemmer = PorterStemmer()\n\n# Import scikit learn - machine learning\nfrom sklearn.preprocessing import MinMaxScaler\nscale = MinMaxScaler()\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9f9ec382":"training = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n\ntraining['train_test'] = 1\ntest['train_test'] = 0\ntest['target'] = np.NaN\nall_data = pd.concat([training, test], sort=True)","0d13ec91":"all_data.head()","451b3ec0":"all_data.info()\n# There is some missing info in the keyword column and a lot missing in the location column ","8568558d":"# First, we want to check if the target class is balanced or unbalanced in the training data\n# i.e. if there are far more disaster tweets that non-disaster tweets\nsns.set_palette(\"pastel\")\nax = sns.barplot(training['target'].value_counts().index, training['target'].value_counts()\/len(training))\nax.set_title(\"Disaster vs non-disaster tweets\")\nax.set_ylabel(\"Percentage\")\nax.set_xlabel(\"Target\");\n\n#Target class is fairly balanced","ebd6024a":"# Investigate the 'keyword' column\nprint('There are {} unique values in the keyword column'.format(len(all_data['keyword'].unique())))\nprint('The first 10 keywords are '+str(all_data['keyword'].unique().tolist()[:10]))\nprint(\"\\n\")\n\n# Investigate the 'location' column\nprint('There are {} unique values in the location column'.format(len(all_data['location'].unique())))\nprint('The first 10 locations are '+str(all_data['location'].unique().tolist()[:10]))\n\n# The keyword column seems more useful than the location column. There are way more unique values in the location column and the values are not entered in a consistent manner","2e583356":"# Extract meta features from the 'text' column\n\nall_data['word_count'] = all_data['text'].apply(lambda x:len(str(x).split()))\nall_data['unique_word_count'] = all_data['text'].apply(lambda x:len(set(str(x).split())))\nall_data['mean_word_length'] = all_data['text'].apply(lambda x: np.mean([len(word) for word in str(x).split()]))\nall_data['character_count'] = all_data['text'].apply(lambda x:len(str(x)))\nall_data['digit_count'] = all_data['text'].apply(lambda x: np.sum([len(char) for char in x if char.isdigit() == True]))\nall_data['non_word_char_count'] = all_data['text'].apply(lambda x: len(re.findall('r\\W',x)))\nall_data['number_of_URLs'] = all_data['text'].apply(lambda x:len(re.findall(r'http\\S+|www\\.\\S+',x)))\nall_data['number_of_hashtags'] = all_data['text'].apply(lambda x: len(re.findall('#',x)))\nall_data['number_of_mentions'] = all_data['text'].apply(lambda x: len(re.findall('@',x)))","d3bd20b3":"# Plot distributions of some of these meta features to make differences between both classes apparent\nlist_of_meta_features_1 = ['word_count','unique_word_count','mean_word_length','character_count','digit_count']\ndisaster_tweets = all_data['target'] == 1\nnot_disaster_tweets = all_data['target'] == 0\n\nfig,axes = plt.subplots(nrows=len(list_of_meta_features_1),figsize=(10,20))\nfor i, feature in enumerate(list_of_meta_features_1):\n    sns.distplot(all_data.loc[not_disaster_tweets][feature], label = 'Not Disaster Tweet',color = 'green',ax = axes[i]).set_title('Distribution for '+feature)\n    sns.distplot(all_data.loc[disaster_tweets][feature], label = 'Disaster Tweet',color = 'red',ax = axes[i])\n    axes[i].legend()\nplt.tight_layout()","4058875d":"# Define dictionary of contractions\ncontractions = {\n  \"ain't\": \"am not\",\n  \"aren't\": \"are not\",\n  \"can't\": \"cannot\",\n  \"can't've\": \"cannot have\",\n  \"'cause\": \"because\",\n  \"could've\": \"could have\",\n  \"couldn't\": \"could not\",\n  \"couldn't've\": \"could not have\",\n  \"didn't\": \"did not\",\n  \"doesn't\": \"does not\",\n  \"don't\": \"do not\",\n  \"hadn't\": \"had not\",\n  \"hadn't've\": \"had not have\",\n  \"hasn't\": \"has not\",\n  \"haven't\": \"have not\",\n  \"he'd\": \"he would\",\n  \"he'd've\": \"he would have\",\n  \"he'll\": \"he will\",\n  \"he'll've\": \"he will have\",\n  \"he's\": \"he is\",\n  \"how'd\": \"how did\",\n  \"how'd'y\": \"how do you\",\n  \"how'll\": \"how will\",\n  \"how's\": \"how is\",\n  \"I'd\": \"I would\",\n  \"I'd've\": \"I would have\",\n  \"I'll\": \"I will\",\n  \"I'll've\": \"I will have\",\n  \"I'm\": \"I am\",\n  \"I've\": \"I have\",\n  \"isn't\": \"is not\",\n  \"it'd\": \"it had\",\n  \"it'd've\": \"it would have\",\n  \"it'll\": \"it will\",\n  \"it'll've\": \"it will have\",\n  \"it's\": \"it is\",\n  \"let's\": \"let us\",\n  \"ma'am\": \"madam\",\n  \"mayn't\": \"may not\",\n  \"might've\": \"might have\",\n  \"mightn't\": \"might not\",\n  \"mightn't've\": \"might not have\",\n  \"must've\": \"must have\",\n  \"mustn't\": \"must not\",\n  \"mustn't've\": \"must not have\",\n  \"needn't\": \"need not\",\n  \"needn't've\": \"need not have\",\n  \"o'clock\": \"of the clock\",\n  \"oughtn't\": \"ought not\",\n  \"oughtn't've\": \"ought not have\",\n  \"shan't\": \"shall not\",\n  \"sha'n't\": \"shall not\",\n  \"shan't've\": \"shall not have\",\n  \"she'd\": \"she would\",\n  \"she'd've\": \"she would have\",\n  \"she'll\": \"she will\",\n  \"she'll've\": \"she will have\",\n  \"she's\": \"she is\",\n  \"should've\": \"should have\",\n  \"shouldn't\": \"should not\",\n  \"shouldn't've\": \"should not have\",\n  \"so've\": \"so have\",\n  \"so's\": \"so is\",\n  \"that'd\": \"that would\",\n  \"that'd've\": \"that would have\",\n  \"that's\": \"that is\",\n  \"there'd\": \"there had\",\n  \"there'd've\": \"there would have\",\n  \"there's\": \"there is\",\n  \"they'd\": \"they would\",\n  \"they'd've\": \"they would have\",\n  \"they'll\": \"they will\",\n  \"they'll've\": \"they will have\",\n  \"they're\": \"they are\",\n  \"they've\": \"they have\",\n  \"to've\": \"to have\",\n  \"wasn't\": \"was not\",\n  \"we'd\": \"we had\",\n  \"we'd've\": \"we would have\",\n  \"we'll\": \"we will\",\n  \"we'll've\": \"we will have\",\n  \"we're\": \"we are\",\n  \"we've\": \"we have\",\n  \"weren't\": \"were not\",\n  \"what'll\": \"what will\",\n  \"what'll've\": \"what will have\",\n  \"what're\": \"what are\",\n  \"what's\": \"what is\",\n  \"what've\": \"what have\",\n  \"when's\": \"when is\",\n  \"when've\": \"when have\",\n  \"where'd\": \"where did\",\n  \"where's\": \"where is\",\n  \"where've\": \"where have\",\n  \"who'll\": \"who will\",\n  \"who'll've\": \"who will have\",\n  \"who's\": \"who is\",\n  \"who've\": \"who have\",\n  \"why's\": \"why is\",\n  \"why've\": \"why have\",\n  \"will've\": \"will have\",\n  \"won't\": \"will not\",\n  \"won't've\": \"will not have\",\n  \"would've\": \"would have\",\n  \"wouldn't\": \"would not\",\n  \"wouldn't've\": \"would not have\",\n  \"y'all\": \"you all\",\n  \"y'alls\": \"you alls\",\n  \"y'all'd\": \"you all would\",\n  \"y'all'd've\": \"you all would have\",\n  \"y'all're\": \"you all are\",\n  \"y'all've\": \"you all have\",\n  \"you'd\": \"you had\",\n  \"you'd've\": \"you would have\",\n  \"you'll\": \"you you will\",\n  \"you'll've\": \"you you will have\",\n  \"you're\": \"you are\",\n  \"you've\": \"you have\"\n}\n\n","75ba1ea8":"# Write function to pre-process text\ndef pre_processing(text):\n    \n    # Convert to lowercase\n    text = text.lower()\n    \n    # Replace contractions\n    for word in text.split():\n        if word in contractions:\n            text = text.replace(word,contractions[word.lower()])\n            \n    # Use regex to remove URLs\n    text = re.sub(r'(https|http)?:\\\/\\\/(\\w|\\.|\\\/|\\?|\\=|\\&|\\%)*\\b', '', text, flags=re.MULTILINE)\n    \n    # Use regex to remove special characters\n    text = re.sub(\"[^a-zA-Z]\",\" \",text)\n    \n    tokens = word_tokenize(text)\n    \n    # Use stemmer to obtain root words\n    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n    single_string = ' '.join(stemmed_tokens)\n    return single_string\n\nall_data['text_clean'] = all_data['text'].apply(lambda x:pre_processing(x))","05138a14":"all_data_scaled = all_data.copy()\nall_data_scaled[['word_count','unique_word_count','mean_word_length','character_count','digit_count','non_word_char_count','number_of_hashtags','number_of_URLs','number_of_mentions']] = scale.fit_transform(all_data_scaled[['word_count','unique_word_count','mean_word_length','character_count','digit_count','non_word_char_count','number_of_hashtags','number_of_URLs','number_of_mentions']])","99abb16b":"all_data_scaled.keyword = all_data_scaled.keyword.fillna('no_keyword')\nall_data_dummies = pd.get_dummies(all_data_scaled,columns=['keyword'])","a9bdf724":"training_additional_features = all_data_dummies[all_data_dummies['train_test']==1].drop(['id', 'location', 'text', 'text_clean','target','train_test'],axis=1)\ntest_additional_features = all_data_dummies[all_data_dummies['train_test']==0].drop(['id', 'location', 'text', 'text_clean','target','train_test'],axis=1)\n\nX_train = all_data_dummies[all_data_dummies['train_test']==1].text_clean\ny_train = all_data_dummies[all_data_dummies['train_test']==1].target\nX_test = all_data_dummies[all_data_dummies['train_test']==0].text_clean","57328d7d":"vect = TfidfVectorizer().fit(X_train)\nX_train_vectorized = vect.transform(X_train)\nX_test_vectorized = vect.transform(X_test)","09f81157":"# Function to add additional features to sparse matrix\n\ndef add_feature(X, feature_to_add):\n    from scipy.sparse import csr_matrix, hstack   \n    return hstack([X, csr_matrix(feature_to_add).T], 'csr')\n\nfor feature in training_additional_features.columns:\n    X_train_vectorized = add_feature(X_train_vectorized, training_additional_features[feature])\n    X_test_vectorized = add_feature(X_test_vectorized, test_additional_features[feature])","6fcd1842":"# Convert sparse matrix to array\n\nX_train_array = X_train_vectorized.toarray()\nX_test_array = X_test_vectorized.toarray()","0febccb3":"# Using Multinomial Naive Bayes classifier model\n\nnb = MultinomialNB()\nnb_cv = cross_val_score(nb, X_train_array,y_train, cv=5)\nprint(nb_cv)\nprint(nb_cv.mean())","e4c8113f":"# Using logistic regression\nlr = LogisticRegression(max_iter = 2000, solver='liblinear')\nlr_cv = cross_val_score(lr,X_train_array,y_train,cv=5)\nprint(lr_cv)\nprint(lr_cv.mean())","21444029":"rf = RandomForestClassifier(random_state = 1)\nrf_cv = cross_val_score(rf,X_train_array,y_train,cv=5)\nprint(rf_cv)\nprint(rf_cv.mean())","53217a21":"# Use voting classifier to aggregate effects of different classifiers\nvoting_clf = VotingClassifier(estimators = [('lr',lr),('nb',nb),('rf',rf)], voting = 'hard')\ncv = cross_val_score(voting_clf,X_train_array,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","7233fbc8":"voting_clf.fit(X_train_array,y_train)\npredictions = voting_clf.predict(X_test_array).astype(int)\nsubmission = pd.DataFrame({'id':test.id,'target':predictions})\nsubmission.to_csv('submission_v7.csv', index =False)","224031aa":"# 6) Convert the values in the keyword column into dummy variables","2ff40385":"# 4) Text pre-processing","d5ae85f1":"# 7) Run different classification algorithms on the vectorized training set and evaluate performance using 5-fold cross validation","a7a332ee":"# 1) Import important libraries and packages","816cc821":"# 5) Scale the additional features between 0 and 1 using MinMax Scaler","35918221":"# Kaggle Project: Real or Not? Natural Language Processing with Disaster Tweets\n\nDisaster relief organisations and news agencies programmatically monitor Twitter to receive real-time information about emergencies and disasters. This notebook seeks to build a machine learning model to predict whether a given tweet is about a disaster.\n\n# Acknowledgements\nhttps:\/\/www.youtube.com\/watch?v=I3FBJdiExcg - This project was modelled after Ken Jee's Beginner Walk-Through for the Titanic Kaggle Competition\n\nhttps:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert\n\nhttps:\/\/www.kaggle.com\/saga21\/disaster-tweets-comp-introduction-to-nlp\n\nhttps:\/\/www.kaggle.com\/yurimuniz\/classifying-tweets-step-by-step","a8b76cda":"# 3) Exploratory data analysis of entire dataset","2047d253":"# 8) Use voting classifier to aggregate the effects of different classifiers","175b8ac7":"# 2) Load datasets"}}