{"cell_type":{"0f96da95":"code","57117aad":"code","eab40b5f":"code","f2793d36":"code","0a73943f":"code","9163e5fb":"code","52656233":"code","365f4c60":"code","1272433b":"markdown","e8769395":"markdown","f339ad08":"markdown","2444430a":"markdown","28250869":"markdown","7867ec60":"markdown","ee3658a2":"markdown","95bd084d":"markdown"},"source":{"0f96da95":"## importing packages\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom bokeh.layouts import row, column\nfrom bokeh.models import ColumnDataSource, CustomJS, Label, Range1d, Slider, Span\nfrom bokeh.plotting import figure, output_notebook, show\n\noutput_notebook()\n\n## reading data\ndf_train = pd.read_csv(\"\/kaggle\/input\/osic-pulmonary-fibrosis-progression\/train.csv\")\n","57117aad":"sns.distplot(df_train.FVC, color = \"brown\");","eab40b5f":"## evaluation metric function\ndef laplace_log_likelihood(actual_fvc, predicted_fvc, confidence, return_values = False):\n    \"\"\"\n    Calculates the modified Laplace Log Likelihood score for this competition.\n    \"\"\"\n    sd_clipped = np.maximum(confidence, 70)\n    delta = np.minimum(np.abs(actual_fvc - predicted_fvc), 1000)\n    metric = - np.sqrt(2) * delta \/ sd_clipped - np.log(np.sqrt(2) * sd_clipped)\n\n    if return_values:\n        return metric\n    else:\n        return np.mean(metric)\n\n\n## default benchmark\nlaplace_log_likelihood(df_train.FVC, np.mean(df_train.FVC), np.std(df_train.FVC))\n","f2793d36":"def plot_metric_constants(actual_fvc, constant_fvc, constant_confidence):\n    \"\"\"\n    Generatings a bokeh plot for a constant value of predicted FVC and confidence.\n    \"\"\"\n    lll = laplace_log_likelihood(actual_fvc, constant_fvc, constant_confidence)\n\n    df = pd.DataFrame({\n        \"actual_FVC\": actual_fvc,\n        \"predicted_FVC\": constant_fvc,\n        \"confidence\": constant_confidence,\n        \"metric\": laplace_log_likelihood(actual_fvc, constant_fvc, constant_confidence, return_values = True)\n    }).sort_values(\"actual_FVC\")\n    \n    source = ColumnDataSource(df)\n    \n    tooltips = [\n        (\"Actual FVC\", \"@actual_FVC{0}\"),\n        (\"Predicted FVC\", \"@predicted_FVC{0}\"),\n        (\"Confidence\", \"@confidence{0}\"),\n        (\"Metric\", \"@metric{0.000}\")\n    ]\n    \n    v = figure(\n        plot_width = 345,\n        plot_height = 345,\n        y_range = Range1d(-4, -25),\n        tooltips = tooltips,\n        title = f\"Metric values for FVC = {constant_fvc}, Confidence = {constant_confidence}\"\n    )\n\n    v.circle(\"actual_FVC\", \"metric\", source = source, size = 3, color = \"deepskyblue\", alpha = 0.8)\n    \n    mean = Span(\n        location = lll,\n        dimension = \"width\",\n        line_color = \"red\",\n        line_dash = \"dashed\",\n        line_width = 1.5\n    )\n\n    v.add_layout(mean)\n    \n    score = Label(\n        x = 3500,\n        y = lll + 1.25,\n        text = f\"Laplace Log Likelihood = {round(lll, 3)}\",\n        text_font_size = \"7pt\"\n    )\n\n    v.add_layout(score)\n    \n    v.xaxis.axis_label = \"Actual FVC\"\n    v.yaxis.axis_label = \"Metric Value\"\n\n    return v\n\n\nv1 = plot_metric_constants(df_train.FVC, 2690, 100)\nv2 = plot_metric_constants(df_train.FVC, 2000, 100)\nv3 = plot_metric_constants(df_train.FVC, 3000, 100)\nv4 = plot_metric_constants(df_train.FVC, 4000, 100)\n\nv5 = plot_metric_constants(df_train.FVC, 2000, 833)\nv6 = plot_metric_constants(df_train.FVC, 2690, 833)\nv7 = plot_metric_constants(df_train.FVC, 3000, 833)\nv8 = plot_metric_constants(df_train.FVC, 4000, 833)\n\nv9 = plot_metric_constants(df_train.FVC, 2690, 70)\nv10 = plot_metric_constants(df_train.FVC, 2690, 100)\nv11 = plot_metric_constants(df_train.FVC, 2690, 833)\nv12 = plot_metric_constants(df_train.FVC, 2690, 1000)\n\nshow(column(row(v1, v2), row(v3, v4), row(v5, v6), row(v7, v8), row(v9, v10), row(v11, v12)))\n","0a73943f":"source = ColumnDataSource(data = dict(\n    x = range(70, 2001),\n    y = [- np.sqrt(2) * 500 \/ x - np.log(np.sqrt(2) * x) for x in range(70, 2001)]\n))\n\ntooltips = [\n    (\"Confidence\", \"@x\"),\n    (\"Metric\", \"@y\")\n]\n\nv1 = figure(\n    plot_width = 300,\n    plot_height = 300,\n    tooltips = tooltips,\n    title = \"Metric values across confidence\"\n)\n\nv1.line(\"x\", \"y\", source = source, width = 4, color = \"coral\", alpha = 0.8)\n\nv1.y_range.flipped = True\n\nv1.xaxis.axis_label = \"Confidence (Uncertainty)\"\nv1.yaxis.axis_label = \"Metric Value\"\n\nv2 = figure(\n    plot_width = 300,\n    plot_height = 300,\n    tooltips = tooltips,\n    y_range = Range1d(-4, -25),\n    title = \"Metric values across confidence\"\n)\n\nv2.line(\"x\", \"y\", source = source, width = 4, color = \"coral\", alpha = 0.8)\n\nv2.xaxis.axis_label = \"Confidence (Uncertainty)\"\nv2.yaxis.axis_label = \"Metric Value\"\n\n\nslider_actual_FVC = Slider(start = 827, end = 6399, value = 2500, step = 1, title = \"Actual FVC\")\nslider_predicted_FVC = Slider(start = 827, end = 6399, value = 3000, step = 1, title = \"Predicted FVC\")\n\ncallback = CustomJS(args = dict(\n    source = source,\n    actual_FVC = slider_actual_FVC,\n    predicted_FVC = slider_predicted_FVC\n), code = \"\"\"\n    var data = source.data\n    var actual_FVC = actual_FVC.value\n    var predicted_FVC = predicted_FVC.value\n\n    var x = data['x']\n    var y = data['y']\n    var delta = Math.min(Math.abs(actual_FVC - predicted_FVC), 1000)\n\n    for (var i = 0; i < x.length; i++) {\n        y[i] = - Math.sqrt(2) * delta \/ x[i] - Math.log(Math.sqrt(2) * x[i])\n    }\n\n    source.change.emit();\n\"\"\")\n\nslider_actual_FVC.js_on_change(\"value\", callback)\nslider_predicted_FVC.js_on_change(\"value\", callback)\n\nshow(column(slider_actual_FVC, slider_predicted_FVC, row(v1, v2)))\n","9163e5fb":"df_preds = pd.read_csv(\"\/kaggle\/input\/osic-lgb-baseline-predictions\/train.csv\")\n\ndef plot_fvc_metric_model(actual_fvc, predicted_fvc, predicted_confidence, constant = False):\n    \"\"\"\n    Generatings a bokeh plot for the metric values across actual values of FVC.\n    \"\"\"\n    lll = laplace_log_likelihood(actual_fvc, predicted_fvc, predicted_confidence)\n    metric_values = laplace_log_likelihood(actual_fvc, predicted_fvc, predicted_confidence, return_values = True)\n\n    df = pd.DataFrame({\n        \"actual_FVC\": actual_fvc,\n        \"predicted_FVC\": predicted_fvc,\n        \"confidence\": predicted_confidence,\n        \"metric\": metric_values\n    }).sort_values(\"actual_FVC\")\n    \n    source = ColumnDataSource(df)\n    \n    tooltips = [\n        (\"Actual FVC\", \"@actual_FVC{0}\"),\n        (\"Predicted FVC\", \"@predicted_FVC{0}\"),\n        (\"Confidence\", \"@confidence{0}\"),\n        (\"Metric\", \"@metric{0.000}\")\n    ]\n    \n    if type(predicted_confidence) != int:\n        title = f\"Metric values over FVC\"\n        offset = 0.35\n    else:\n        title = f\"Metric values over FVC (Confidence = {predicted_confidence})\"\n        offset = 0.11\n    \n    v = figure(\n        plot_width = 600,\n        plot_height = 600,\n        y_range = Range1d(max(metric_values) + 0.5, min(metric_values) - 0.5),\n        tooltips = tooltips,\n        title = title\n    )\n\n    v.circle(\"actual_FVC\", \"metric\", source = source, size = 3, color = \"mediumseagreen\", alpha = 0.8)\n    \n    mean = Span(\n        location = lll,\n        dimension = \"width\",\n        line_color = \"red\",\n        line_dash = \"dashed\",\n        line_width = 1.5\n    )\n\n    v.add_layout(mean)\n    \n    score = Label(\n        x = 3750,\n        y = lll + offset,\n        text = f\"Laplace Log Likelihood = {round(lll, 3)}\",\n        text_font_size = \"12pt\"\n    )\n\n    v.add_layout(score)\n    \n    v.xaxis.axis_label = \"Actual FVC\"\n    v.yaxis.axis_label = \"Metric Value\"\n\n    return v\n\n\ndef plot_confidence_metric_model(actual_fvc, predicted_fvc, predicted_confidence):\n    \"\"\"\n    Generatings a bokeh plot for the metric values across predicted values of confidence.\n    \"\"\"\n    lll = laplace_log_likelihood(actual_fvc, predicted_fvc, predicted_confidence)\n    metric_values = laplace_log_likelihood(actual_fvc, predicted_fvc, predicted_confidence, return_values = True)\n\n    df = pd.DataFrame({\n        \"actual_FVC\": actual_fvc,\n        \"predicted_FVC\": predicted_fvc,\n        \"confidence\": predicted_confidence,\n        \"metric\": metric_values\n    }).sort_values(\"confidence\")\n    \n    source = ColumnDataSource(df)\n    \n    tooltips = [\n        (\"Actual FVC\", \"@actual_FVC{0}\"),\n        (\"Predicted FVC\", \"@predicted_FVC{0}\"),\n        (\"Confidence\", \"@confidence{0}\"),\n        (\"Metric\", \"@metric{0.000}\")\n    ]\n    \n    v = figure(\n        plot_width = 600,\n        plot_height = 600,\n        y_range = Range1d(max(metric_values) + 0.5, min(metric_values) - 0.5),\n        tooltips = tooltips,\n        title = \"Metric values over Confidence\"\n    )\n\n    v.circle(\"confidence\", \"metric\", source = source, size = 3, color = \"gold\", alpha = 0.8)\n    \n    mean = Span(\n        location = lll,\n        dimension = \"width\",\n        line_color = \"red\",\n        line_dash = \"dashed\",\n        line_width = 1.5\n    )\n\n    v.add_layout(mean)\n    \n    score = Label(\n        x = 600,\n        y = lll + 0.35,\n        text = f\"Laplace Log Likelihood = {round(lll, 3)}\",\n        text_font_size = \"12pt\"\n    )\n\n    v.add_layout(score)\n    \n    v.xaxis.axis_label = \"Predicted Confidence\"\n    v.yaxis.axis_label = \"Metric Value\"\n\n    return v\n\n\ndef plot_pred_metric_model(actual_fvc, predicted_fvc, predicted_confidence):\n    \"\"\"\n    Generatings a bokeh plot for the metric values across predicted values of FVC.\n    \"\"\"\n    lll = laplace_log_likelihood(actual_fvc, predicted_fvc, predicted_confidence)\n    metric_values = laplace_log_likelihood(actual_fvc, predicted_fvc, predicted_confidence, return_values = True)\n\n    df = pd.DataFrame({\n        \"actual_FVC\": actual_fvc,\n        \"predicted_FVC\": predicted_fvc,\n        \"confidence\": predicted_confidence,\n        \"metric\": metric_values\n    }).sort_values(\"predicted_FVC\")\n    \n    source = ColumnDataSource(df)\n    \n    tooltips = [\n        (\"Actual FVC\", \"@actual_FVC{0}\"),\n        (\"Predicted FVC\", \"@predicted_FVC{0}\"),\n        (\"Confidence\", \"@confidence{0}\"),\n        (\"Metric\", \"@metric{0.000}\")\n    ]\n    \n    v = figure(\n        plot_width = 600,\n        plot_height = 600,\n        y_range = Range1d(max(metric_values) + 0.5, min(metric_values) - 0.5),\n        tooltips = tooltips,\n        title = \"Metric values over predicted FVC\"\n    )\n\n    v.circle(\"predicted_FVC\", \"metric\", source = source, size = 3, color = \"turquoise\", alpha = 0.8)\n    \n    mean = Span(\n        location = lll,\n        dimension = \"width\",\n        line_color = \"red\",\n        line_dash = \"dashed\",\n        line_width = 1.5\n    )\n\n    v.add_layout(mean)\n    \n    score = Label(\n        x = 3500,\n        y = lll + 0.35,\n        text = f\"Laplace Log Likelihood = {round(lll, 3)}\",\n        text_font_size = \"12pt\"\n    )\n\n    v.add_layout(score)\n    \n    v.xaxis.axis_label = \"Predicted FVC\"\n    v.yaxis.axis_label = \"Metric Value\"\n\n    return v\n\n\nsns.pairplot(df_preds[[\"FVC\", \"FVC_pred\", \"Confidence\"]]);\n","52656233":"v1 = plot_fvc_metric_model(df_preds.FVC, df_preds.FVC_pred, df_preds.Confidence)\nv2 = plot_fvc_metric_model(df_preds.FVC, df_preds.FVC_pred, 833)\n\nshow(column(v1, v2))\n","365f4c60":"v1 = plot_confidence_metric_model(df_preds.FVC, df_preds.FVC_pred, df_preds.Confidence)\nv2 = plot_pred_metric_model(df_preds.FVC, df_preds.FVC_pred, df_preds.Confidence)\n\nshow(column(v1, v2))\n","1272433b":"## FVC Distribution\nLet's look at the distribution of the target variable **FVC**.\n","e8769395":"## Interactive Simulation\nYou can play around with combinations of FVC values to see how the metric value varies over confidence.\n\n* Any difference between the FVC values greater than 1000 will yield the same metric values since the delta is capped at 1000. \n* The metric value is only dependent on the difference of FVC values and not on the absolute values.\n","f339ad08":"While ML models optimize on the point predictions of a target variable (like FVC), it will be interesting to see how the confidence predictions can be optimized. I'm sure through the course of the competition we will see some interesting approaches for the same.\n\n## ML Prediction\nLet's look at the out-of-fold predictions of some of the ML models. The example used below are the predictions from this kernel: https:\/\/www.kaggle.com\/yasufuminakama\/osic-lgb-baseline\n","2444430a":"## Laplace Log Likelihood\n![](https:\/\/i.imgur.com\/tEIZvli.png)\n*Image Credits: https:\/\/en.wikipedia.org\/wiki\/Laplace_distribution*\n\nThe evaluation metric of this competition is a modified version of Laplace Log Likelihood. Read more about it on the [Evaluation Page](https:\/\/www.kaggle.com\/c\/osic-pulmonary-fibrosis-progression\/overview\/evaluation).\n\n![LLL.png](attachment:LLL.png)\n\nThis notebook explores the functioning of this metric with some examples and visualizations.\n","28250869":"You can clearly see that if you predict a value with high error and low standard deviation it can lead to large penalties. On the flip side if you predict values with high standard deviation the range of penalties is higher. So that's what you need to balance while optimizing the predictions!\n","7867ec60":"**-8.023** is the default score to beat while cross-validating models on train data. Any model scoring worse than this is not useful. You can get this default score to beat for each (fold of) validation data as well.\n\n## Constant Prediction\nLet's try out a few combinations of predicting a constant value for FVC as well as confidence to compare how the errors vary with the actual FVC values.\n","ee3658a2":"FVC ranges from **827 to 6399** and seems to have a (visually) normal distribution with **mean 2690** and **standard deviation 833**.\n\n## Evaluation Metric\nNote that not only do we need to predict the FVC but also a confidence value of the prediction. The term *confidence* is a bit confusing (as lower value means more confident) and it might be better to just consider it as the *standard deviation* (or the *uncertainty*).   \n\nLet's define a function and then use it with some examples and understand the metric.\n","95bd084d":"Optimizing the confidence predictions through a model or optimizer scores **-6.851** like what was done in this model \/ submission which is better than if just the standard deviation was used to predict confidence which scores **-7.373**. There are some interesting approaching shared in public baseline notebooks of how confidence predictions can be optimized.\n"}}