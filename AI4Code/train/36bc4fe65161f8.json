{"cell_type":{"fc1d2de7":"code","5a0f6c49":"code","0ec178dc":"code","1af5531f":"code","732f2cec":"code","195ee494":"code","3b07c8e8":"code","4660c13d":"code","12402f58":"code","898b0935":"code","d73acd9e":"code","d220fc37":"code","81b844e7":"code","52dc4e86":"code","4f3946ea":"code","b852678e":"code","92a843e9":"code","662cebb5":"code","de3ca0cc":"code","2ce7516f":"code","4e3ae998":"code","63a5953f":"code","0e7269f0":"code","3207eeb4":"code","5d79cc9b":"code","596c200d":"code","68fd7104":"code","777d1114":"code","788e034c":"code","0da57f47":"code","414b9e90":"code","04870df5":"code","91e11c8f":"code","13299610":"code","e6c8b4ef":"code","c56c85a7":"code","61b75856":"code","a9694445":"code","a6c1abb3":"code","23d7e0ec":"markdown","ae6164ce":"markdown","969e66d7":"markdown","5ef2ed58":"markdown","e84d6619":"markdown","eac750f3":"markdown","6c81e96c":"markdown","b0604cb7":"markdown","13cb340f":"markdown","07d25c9c":"markdown","9da42f71":"markdown","8a877d2a":"markdown","fef259be":"markdown","273b5ca5":"markdown"},"source":{"fc1d2de7":"import sys\nimport random\nimport numpy as np\nimport os\n\nfrom tqdm import tqdm_notebook as tqdm\nimport torch\nfrom pathlib import Path\nfrom matplotlib import pyplot as plt\nimport pandas as pd\n\nfrom fastai.vision.data import ImageDataBunch, DatasetType, ImageList, ResizeMethod\nfrom fastai.vision.transform import get_transforms\nfrom fastai.vision.learner import Learner\nfrom fastai.metrics import accuracy\nfrom fastai.train import ClassificationInterpretation\nfrom PIL import Image\n\nsys.path.append('..\/input\/efficientnet-pytorch\/efficientnet-pytorch\/EfficientNet-PyTorch-master')\n\nimport efficientnet_pytorch","5a0f6c49":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","0ec178dc":"CLASSES = [\n    'Beijing', 'Brisbane', 'Geneva', 'HongKong', 'Luanda',\n    'Melbourne', 'Seoul', 'Singapore', 'Sydney', 'Zurich']\n\nSEED = 420\nBS = 64\n\naugs = get_transforms(\n    do_flip=True,\n    max_rotate=10.,\n    max_zoom=1.1,\n    max_lighting=0.2,\n    max_warp=0.2,\n    p_affine=0.75)","1af5531f":"seed_everything(SEED)","732f2cec":"data = ImageDataBunch.from_folder(\n    '..\/',\n    train='..\/input\/synthetic-image-classification\/synimg\/train', valid_pct=0.2,\n    ds_tfms=augs, seed=SEED,\n    classes=CLASSES, size=64, test=None, resize_method=ResizeMethod.PAD, padding_mode='zeros')","195ee494":"assert len(data.classes) == len(CLASSES)","3b07c8e8":"test_df = pd.read_csv('..\/input\/synthetic-image-classification\/synimg\/synimg\/test\/data_nostyle.csv')[['filepath']]","4660c13d":"data.add_test(ImageList.from_df(test_df, '..\/input\/synthetic-image-classification\/synimg\/')) ","12402f58":"data.show_batch()","898b0935":"data.show_batch(ds_type=DatasetType.Valid)","d73acd9e":"data.show_batch(ds_type=DatasetType.Test)","d220fc37":"all_files = []\nfor p in Path('..\/input\/synthetic-image-classification\/synimg\/synimg\/train').iterdir():\n    if not p.is_dir():\n        continue\n\n    for f in p.iterdir():\n        all_files.append(f)","81b844e7":"assert len(all_files) == len(pd.read_csv('..\/input\/synthetic-image-classification\/synimg\/synimg\/train\/data.csv'))","52dc4e86":"sizes = [Image.open(f).size for f in tqdm(all_files)]","4f3946ea":"heights = [s[0] for s in sizes]\nwidths = [s[1] for s in sizes]","b852678e":"plt.hist(heights)\nplt.title('Height distribution')\nplt.show()","92a843e9":"plt.hist(widths)\nplt.title('Width distribution')\nplt.show()","662cebb5":"pd.read_csv('..\/input\/synthetic-image-classification\/synimg\/synimg\/train\/data.csv').style_name.value_counts().plot.bar()","de3ca0cc":"model = efficientnet_pytorch.EfficientNet.from_name(\n    'efficientnet-b0', override_params={'num_classes': len(CLASSES)})","2ce7516f":"learn = Learner(data, model, metrics=[accuracy])","4e3ae998":"learn.lr_find()","63a5953f":"learn.recorder.plot()","0e7269f0":"# For testing\n# learn.fit_one_cycle(1, 2e-02)","3207eeb4":"learn.fit_one_cycle(20, 2e-02)","5d79cc9b":"learn.recorder.plot_losses()","596c200d":"learn.validate()","68fd7104":"learn.fit_one_cycle(4, 2e-3)","777d1114":"learn.validate()","788e034c":"learn.recorder.plot_losses()","0da57f47":"interpretation = ClassificationInterpretation.from_learner(learn)","414b9e90":"interpretation.plot_top_losses(12, figsize=(15, 12))","04870df5":"interpretation.plot_confusion_matrix(figsize=(8, 8), dpi=60)","91e11c8f":"test_preds, _ = learn.TTA(ds_type=DatasetType.Test)","13299610":"pred_names = [data.classes[i] for i in test_preds.argmax(1)]","e6c8b4ef":"output_df = pd.read_csv('..\/input\/synthetic-image-classification\/sample_submission.csv')","c56c85a7":"output_df['style_name'] = pred_names","61b75856":"output_df.head()","a9694445":"output_df.to_csv('submission.csv', index=False)","a6c1abb3":"pd.read_csv('submission.csv')","23d7e0ec":"## Create dataset","ae6164ce":"### Top losses","969e66d7":"#### Test set","5ef2ed58":"# SHOKUNIN 2019: Synthetic Image Classification\n\nThis kernel solves the TW Synthetic Image Classification problem using [EfficientNet-B0](https:\/\/ai.googleblog.com\/2019\/05\/efficientnet-improving-accuracy-and.html) trained from scratch using [1cycle policy](https:\/\/arxiv.org\/pdf\/1803.09820.pdf) training and the Fast.ai library on Pytorch.\n\nCurrent gets > 0.94 on the LB with an underfitted model.","e84d6619":"## EDA","eac750f3":"## Interpret results","6c81e96c":"### Confusion matrix","b0604cb7":"#### Train\/validation set","13cb340f":"## Create submission","07d25c9c":"## Model training","9da42f71":"## Tunable params","8a877d2a":"### Class balance","fef259be":"### Image examples ","273b5ca5":"### Size distribution"}}