{"cell_type":{"f0f9ac00":"code","54f22086":"code","a75528ce":"code","9c8704c8":"code","ac551b05":"code","bff32888":"code","139e080d":"code","14e3f4fc":"code","1f5220f4":"code","51ba028b":"code","7f898e8d":"code","13d6f28d":"code","4de99115":"code","6d33ce44":"code","1b94361f":"code","f2a20b39":"code","566d9b1c":"code","f6e489a8":"code","1567a9fb":"code","d6b9bce8":"code","0c7a5309":"code","6667cc0b":"code","85458fbd":"markdown"},"source":{"f0f9ac00":"!pip install transformers datasets torch numpy IPython tqdm matplotlib\nfrom transformers import BertTokenizer, BertForMaskedLM\nfrom transformers import BertConfig, DistilBertConfig, DistilBertModel, BertModel, DistilBertTokenizer, BertForSequenceClassification, DistilBertForSequenceClassification\nfrom transformers import AdamW\nfrom transformers import TrainingArguments, Trainer\nimport numpy as np\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn import DataParallel\nimport torch\nimport datasets\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nfrom tqdm.notebook import tqdm","54f22086":"num_hidden_layers = 6\nlearning_rate = 1e-5\nbatch_size = 32\ntask = \"cola\"\nmetric_name = \"pearson\" if task == \"stsb\" else \"matthews_correlation\" if task == \"cola\" else \"accuracy\"\nmodel_checkpoint = \"robert-base\"\n\ntokens_num_generator = 15 if task in [\"cola\", \"sst2\"] else 25\ntokens_num_samples = 100 \n\ncuda_available = torch.cuda.is_available()\ndevice = \"cuda\" if cuda_available else \"cpu\"\n\ntask_to_keys = {\n    \"cola\": (\"sentence\", None),\n    \"mnli\": (\"premise\", \"hypothesis\"),\n    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n    \"mrpc\": (\"sentence1\", \"sentence2\"),\n    \"qnli\": (\"question\", \"sentence\"),\n    \"qqp\": (\"question1\", \"question2\"),\n    \"rte\": (\"sentence1\", \"sentence2\"),\n    \"sst2\": (\"sentence\", None),\n    \"stsb\": (\"sentence1\", \"sentence2\"),\n    \"wnli\": (\"sentence1\", \"sentence2\"),\n}\n\nsentence1_key, sentence2_key = task_to_keys[task]\ndataset = datasets.load_dataset(\"glue\", task, )\nmetric = datasets.load_metric(\"glue\", task)","a75528ce":"### METRIC UTILS ###\n\ndef compute_metrics_with_hidden_state(eval_pred):\n    predictions, labels = eval_pred\n    predictions = predictions[0]\n    if task != \"stsb\":\n        predictions = np.argmax(predictions, axis=1)\n    else:\n        predictions = predictions[:, -1]\n\n    return metric.compute(predictions=predictions, references=labels)\n\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = predictions\n    if task != \"stsb\":\n        predictions = np.argmax(predictions, axis=1)\n    else:\n        predictions = predictions[:, -1]\n\n    return metric.compute(predictions=predictions, references=labels)","9c8704c8":"device = torch.device(\"cuda\" if cuda_available else \"cpu\")","ac551b05":"### PREPARE MODELS AND TOKENIZER ###\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nteacher = BertForSequenceClassification.from_pretrained('bert-base-uncased', output_hidden_states=True).to(device)\ngenerator = BertForMaskedLM.from_pretrained('bert-base-uncased', output_hidden_states=True).to(device)\nstudent = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', output_hidden_states=True).to(device)","bff32888":"### DEFINE FUNCTION FOR DECODING DATASET FOR DISTILLATION\n\ndef preprocess_function(examples):\n    if sentence2_key is None:\n        return tokenizer(examples[sentence1_key], truncation=True, padding='max_length', max_length=tokens_num_samples)\n    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True, padding='max_length', max_length=tokens_num_samples)\n    \nencoded_dataset = dataset.map(preprocess_function, batched=True)","139e080d":"### DEFINE GENERATOR CLASS ###\n\nclass PDistil(torch.nn.Module):\n    def __init__(self, threshold=0.5, mask_token=tokenizer.vocab[\"[MASK]\"], vocab_size=tokenizer.vocab_size):\n        super(PDistil, self).__init__()\n        self.masker = generator\n        self.threshold = threshold\n        self.mask_token = mask_token\n        self.vocab_size = vocab_size\n    \n\n    def forward(self, input_ids, attention_mask=None, langs=None, \n                  token_type_ids=None, position_ids=None, lengths=None, \n                  cache=None, head_mask=None, inputs_embeds=None, labels=None, \n                  output_attentions=None, output_hidden_states=None, return_dict=None):\n    \n        x = input_ids\n        mask_idx = x == self.mask_token\n        generator_logit = self.masker(x, attention_mask=attention_mask, \n                                      token_type_ids=token_type_ids).logits\n        gumbel = torch.nn.functional.gumbel_softmax(generator_logit,tau=0.1, hard=True)\n        new_x = torch.matmul(gumbel, torch.arange(self.vocab_size).to(device).reshape(-1,1).float()).long().squeeze(-1)\n        new_x[mask_idx == 0] = x[mask_idx == 0]\n        return new_x\n    \ngen_model = PDistil()\ngen_model = gen_model.to(device)","14e3f4fc":"a = torch.tensor([0,1]).to(device)\nb = torch.tensor([0,4,5]).to(a.device)","1f5220f4":"### FINE-TUNE TEACHER MODEL ###\n\nteacher_trainer_args = TrainingArguments(\n    \"test-glue\",\n    past_index=-1,\n    evaluation_strategy=\"epoch\",\n    learning_rate=learning_rate,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model=metric_name,\n\n)\n\nteacher_trainer = Trainer(\n    model=teacher,\n    args=teacher_trainer_args,\n    train_dataset=encoded_dataset[\"train\"],\n    eval_dataset=encoded_dataset[\"validation\"],\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics_with_hidden_state,\n)\nteacher_trainer.train()","51ba028b":"### DEFINE CUSTOM TORCH.DATA.UTILS.DATASET ###\n\nclass PKD_set(Dataset):\n    def __init__(self, masked_set, original_set=None):\n        self.masked_tokens = torch.tensor(masked_set[\"input_ids\"])\n        self.attention_mask = torch.tensor(masked_set[\"attention_mask\"])\n        self.tokens = None\n        if original_set is not None:\n            self.tokens = torch.tensor(original_set[\"input_ids\"])\n        self.labels = torch.tensor(masked_set[\"label\"])\n        self.token_type = torch.tensor(masked_set[\"token_type_ids\"])\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        if self.tokens is not None:\n            return (self.masked_tokens[idx], \n                self.tokens[idx],\n                self.labels[idx],\n                self.attention_mask[idx],\n                self.token_type[idx])\n        return (self.masked_tokens[idx], \n                self.labels[idx],\n                self.attention_mask[idx],\n                self.token_type[idx])","7f898e8d":"### CUSTOM LOSSES ###\n\ndef loss_pt(student_hidden_state, teacher_hidden_state):\n    teacher_hidden_state = F.normalize(teacher_hidden_state, p=2, dim=2)\n    student_hidden_state = F.normalize(student_hidden_state, p=2, dim=2)\n    return F.mse_loss(teacher_hidden_state.float(), student_hidden_state.float()).half()\n\ndef loss_kd(logits, teacher_logits):\n    T = 10\n    alpha = 0.7\n    return nn.KLDivLoss()(F.log_softmax(logits \/ T, dim=1),\n                            F.softmax(teacher_logits \/ T, dim=1)) * T * T\n\ndef PKD_loss(adv_batch_logits, orig_batch_logits, labels, hidden_states):\n    teacher_adv_logits, student_adv_logits = adv_batch_logits\n    teacher_orig_logits, student_orig_logits = orig_batch_logits\n\n    loss_adv_kd = loss_kd(student_adv_logits, teacher_adv_logits) \n    loss_orig_kd = loss_kd(student_orig_logits, teacher_orig_logits) \n    loss_ce = F.cross_entropy(student_orig_logits, labels)\n\n    loss_pt_ = loss_pt(torch.stack(hidden_states[0][:-1]).transpose(0, 1),\n                torch.stack(hidden_states[1][-7:-1]).transpose(0, 1))\n\n    return ((1\/3 * loss_adv_kd + 1\/3 * loss_orig_kd + 1\/3*loss_ce + beta *loss_pt_), \\\n          loss_adv_kd.detach().cpu(), loss_orig_kd.detach().cpu(), loss_ce.detach().cpu(), loss_pt_.detach().cpu())","13d6f28d":"### DEFINE TRAIN PROCESS FOR GENERATOR AND UTILS FOLLOWING HUGGING FACE API ###\n\ntrainer_args = TrainingArguments(\n    \"test-glue\",\n    past_index=-1,\n    evaluation_strategy=\"epoch\",\n    learning_rate=learning_rate,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=1,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    #metric_for_best_model=metric_name,\n)\n\n\ndef gen_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n\n        teacher_outputs = teacher(outputs)\n        teacher_logits = teacher_outputs.logits\n        student_outputs = student(outputs)\n        student_logits = student_outputs.logits\n\n        loss = -loss_kd(student_logits, teacher_logits)\n        return (loss, outputs) if return_outputs else loss","4de99115":"### PARAMETERS FOR MAIN LOOP ###\n\nlearning_rate_student = 1e-5\nopt = AdamW(student.parameters(), lr = learning_rate_student, weight_decay = 0.01)\nnum_epochs_student = 1\nbeta = 1\n\nopt_gen = AdamW(gen_model.parameters(), lr = learning_rate, weight_decay = 0.01)\nloss_orig_kd, loss_adv_kd, loss_ce, metric_, loss_pt_, loss_joint = [], [], [], [], [], []\nval_loss_orig_kd, val_loss_adv_kd, val_loss_ce, val_metric_, val_loss_pt_, val_loss_joint = [], [], [], [], [], []\nloss_gen = []","6d33ce44":"### DEFINE FUNCTION FOR GENERATOR TRAINING\n\ndef generator_train_step():\n### DEFINE FUNCTION FOR DECODING DATASET FOR GENERATOR AND DECODE DATASET WITH GIVEN PROBABILTY FOR MASKS ###\n\n    def masked_preprocess_function(examples):\n        masked_input = list(map(masking, examples[sentence1_key]))\n        if sentence2_key is None:\n            return tokenizer(masked_input, truncation=True, padding='max_length', max_length=tokens_num_generator)\n        masked_input_1 = list(map(masking, examples[sentence2_key]))\n        return tokenizer(masked_input, masked_input_1, truncation=True, padding='max_length', max_length=tokens_num_generator)\n\n    masked_encoded_dataset = dataset.map(masked_preprocess_function, batched=True) \n\n    train_set  = PKD_set(masked_encoded_dataset[\"train\"])\n    valid_set  = PKD_set(masked_encoded_dataset[\"validation\"])\n    test_set  = PKD_set(masked_encoded_dataset[\"test\"])\n\n    train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=cuda_available)\n    valid_dataloader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, pin_memory=cuda_available)\n    test_dataloader = DataLoader(test_set, batch_size=batch_size, shuffle=True, pin_memory=cuda_available)\n    \n    for step, batch in tqdm(enumerate(train_dataloader)):\n        masked_tokens = batch[0].to(device)\n        labels = batch[1].to(device)\n        attention_mask = batch[2].to(device)\n        token_type = batch[3].to(device)\n        opt_gen.zero_grad()\n        \n        output = gen_model(masked_tokens, attention_mask=attention_mask, token_type_ids=token_type)\n        \n        teacher_outputs = teacher(output, attention_mask=attention_mask, token_type_ids=token_type)\n        teacher_logits = teacher_outputs.logits\n        student_outputs = student(output,  attention_mask=attention_mask)\n        student_logits = student_outputs.logits\n\n        loss = -loss_kd(student_logits, teacher_logits)\n        loss.backward()\n        opt_gen.step()\n        loss_gen.append(loss.detach().cpu().numpy())\n        \n    clear_output()  \n    fig, ax = plt.subplots(1,1,figsize=(8,5))\n    ax.plot(loss_gen)\n    ax.set_title(\"Generator loss\")\n    plt.show()","1b94361f":"### DEFINE FUNCTION  FOR STUDENT TRAINING\n\ndef student_train_step():\n    ### DEFINE FUNCTION FOR DECODING MASKING DATASET FOR DISTILLATION ###\n\n    def masked_sample_preprocess_function(examples):\n        masked_input = list(map(masking, examples[sentence1_key]))\n        if sentence2_key is None:\n            return tokenizer(masked_input, truncation=True, padding='max_length', max_length=tokens_num_samples)\n        masked_input_1 = list(map(masking, examples[sentence2_key]))\n        return tokenizer(masked_input, masked_input_1, truncation=True, padding='max_length', max_length=tokens_num_samples)\n\n    masked_encoded_dataset_samples = dataset.map(masked_sample_preprocess_function, batched=True) \n\n    ### GET DATASETS AND DATALOADERS FOR TRAIN, VALID AND TEST STAGES ###\n\n    train_set  = PKD_set(masked_encoded_dataset_samples[\"train\"], encoded_dataset[\"train\"])\n    valid_set  = PKD_set(masked_encoded_dataset_samples[\"validation\"], encoded_dataset[\"validation\"])\n    test_set  = PKD_set(masked_encoded_dataset_samples[\"test\"], encoded_dataset[\"test\"])\n\n    train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=cuda_available)\n    valid_dataloader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, pin_memory=cuda_available)\n    test_dataloader = DataLoader(test_set, batch_size=batch_size, shuffle=True, pin_memory=cuda_available)\n    \n    for step, batch in tqdm(enumerate(train_dataloader)):\n        masked_tokens = batch[0].to(device)\n        orig_tokens = batch[1].to(device)\n        labels = batch[2].to(device)\n        attention_mask = batch[3].to(device)\n        token_type = batch[4].to(device)\n\n# GENERATE ADVERSARIAL TOKENS\n        adv_tokens = gen_model(masked_tokens, attention_mask=attention_mask, token_type_ids=token_type)\n\n# GET ADVERSARIAL LOGITS FROM TEACHER AND STUDENT\n        student_adv_output = student(adv_tokens, attention_mask=attention_mask)\n        student_adv_logits = student_adv_output.logits\n\n        teacher_adv_output = teacher(adv_tokens, attention_mask=attention_mask, token_type_ids=token_type)\n        teacher_adv_logits = teacher_adv_output.logits\n\n# GET ORIGINAL LOGITS FROM TEACHER AND STUDENT\n        student_orig_output = student(orig_tokens,  attention_mask=attention_mask)\n        student_orig_logits = student_orig_output.logits\n\n        teacher_orig_output = teacher(orig_tokens, attention_mask=attention_mask, token_type_ids=token_type )\n        teacher_orig_logits = teacher_orig_output.logits\n\n#GET HIDDEN STATES FROM TEACHER AND STUDENT\n        student_hidden_states = student_orig_output.hidden_states\n        teacher_hidden_states = teacher_orig_output.hidden_states\n\n        loss = PKD_loss( \n          (student_adv_logits, teacher_adv_logits),\n          (student_orig_logits, teacher_orig_logits),\n          labels, \n          (student_hidden_states, teacher_hidden_states)\n        )\n\n#OPTIMIZING STEP\n        opt.zero_grad()\n        loss[0].backward()\n        opt.step()\n\n#SAVE STATISTICS\n        loss_joint.append(loss[0].detach().cpu().numpy())\n        loss_adv_kd.append(loss[1])\n        loss_orig_kd.append(loss[2])\n        loss_ce.append(loss[3])\n        loss_pt_.append(loss[4])\n        metric_.append(compute_metrics((student_orig_logits.detach().cpu().numpy(), labels.detach().cpu().numpy()))[metric_name])\n      \n        if step % 100 == 0:\n            clear_output()\n            fig, ax = plt.subplots(3, 2, figsize=(15,10))\n            ax[0,0].plot(loss_joint)\n            ax[0,0].set_title(\"Joint loss\")\n            ax[0,1].plot(loss_adv_kd)\n            ax[0,1].set_title(\"Adverserial KD loss\")\n            ax[1,0].plot(loss_orig_kd)\n            ax[1,0].set_title(\"Original KD loss\")\n            ax[1,1].plot(loss_ce)            \n            ax[1,1].set_title(\"Cross entropy loss\")\n            ax[2,0].plot(loss_pt_)\n            ax[2,0].set_title(\"Patient loss\")\n            ax[2,1].plot(metric_)\n            ax[2,1].set_title(metric_name)\n            plt.show()\n    with torch.no_grad():\n        for step, batch in tqdm(enumerate(valid_dataloader)):\n            masked_tokens = batch[0].to(device)\n            orig_tokens = batch[1].to(device)\n            labels = batch[2].to(device)\n            attention_mask = batch[3].to(device)\n            token_type = batch[4]\n\n    # GENERATE ADVERSARIAL TOKENS\n            adv_tokens = gen_model(masked_tokens)\n\n    # GET ADVERSARIAL LOGITS FROM TEACHER AND STUDENT\n            student_adv_output = student(adv_tokens)\n            student_adv_logits = student_adv_output.logits\n\n            teacher_adv_output = teacher(adv_tokens)\n            teacher_adv_logits = teacher_adv_output.logits\n\n    # GET ORIGINAL LOGITS FROM TEACHER AND STUDENT\n            student_orig_output = student(orig_tokens)\n            student_orig_logits = student_orig_output.logits\n\n            teacher_orig_output = teacher(orig_tokens)\n            teacher_orig_logits = teacher_orig_output.logits\n\n    #GET HIDDEN STATES FROM TEACHER AND STUDENT\n            student_hidden_states = student_orig_output.hidden_states\n            teacher_hidden_states = teacher_orig_output.hidden_states\n\n            loss = PKD_loss( \n              (student_adv_logits, teacher_adv_logits),\n              (student_orig_logits, teacher_orig_logits),\n              labels, \n              (student_hidden_states, teacher_hidden_states)\n            )\n    #SAVE STATISTICS\n            fig1, ax1 = plt.subplots(3, 2, figsize=(10,10))\n            val_loss_joint.append(loss[0].detach().cpu().numpy())\n            val_loss_adv_kd.append(loss[1])\n            val_loss_orig_kd.append(loss[2])\n            val_loss_ce.append(loss[3])\n            val_loss_pt_.append(loss[4])\n            val_metric_.append(compute_metrics((student_orig_logits.detach().cpu().numpy(), labels.detach().cpu().numpy()))[metric_name])\n        ax1[0,0].plot(val_loss_joint)\n        ax1[0,0].set_title(\"Validation Joint loss\")\n        ax1[0,1].plot(val_loss_adv_kd)\n        ax1[0,1].set_title(\"Validation Original KD loss\")\n        ax1[1,0].plot(val_loss_orig_kd)\n        ax1[1,0].set_title(\"Validation Adverserial KD loss\")\n        ax1[1,1].plot(val_loss_ce)            \n        ax1[1,1].set_title(\"Validation Cross entropy loss\")\n        ax1[2,0].plot(val_loss_pt_)\n        ax1[2,0].set_title(\"Validation Patient loss\")\n        ax1[2,1].plot(val_metric_)\n        ax1[2,1].set_title(metric_name)\n        plt.show()","f2a20b39":"### MAIN LOOP ###\n\nfor i in range(num_epochs_student):\n\n    ### MASKING FUNCTION ###\n\n    PROBA = 0.3\n\n    def masking(s, p=PROBA):\n        s = np.array(s.split())\n        s[np.random.uniform(size=len(s)) < p] = \"[MASK]\"\n        return ' '.join(s)\n\n    generator_train_step()\n    student_train_step()","566d9b1c":"student.eval()","f6e489a8":"student_eval_args = TrainingArguments(\n    \"test-glue\",\n    past_index=-1,\n    per_device_eval_batch_size=batch_size,\n    metric_for_best_model=metric_name,\n\n)\nstudent_tester = Trainer(\n                    model=student,\n                    args=student_eval_args,\n                    eval_dataset = encoded_dataset[\"validation\"],\n                    tokenizer=tokenizer,\n                    compute_metrics=compute_metrics_with_hidden_state,\n                )\n\nstudent_tester.evaluate()","1567a9fb":"test_set  = PKD_set(encoded_dataset[\"test\"])\ntest_dataloader = DataLoader(test_set, batch_size=batch_size, shuffle=True)\nloss__, metric__ = [], []\nwith torch.no_grad():\n    for step, batch in tqdm(enumerate(test_dataloader)):\n        tokens = batch[0].to(device)\n        labels = batch[1].to(device)\n        attention_mask = batch[2].to(device)\n        token_type = batch[3].to(device)\n        output = student(tokens, attention_mask=attention_mask)\n        loss = F.cross_entropy(output.logits, labels)\n        loss__.append(loss.detach().cpu().numpy())\n        metric__.append(compute_metrics((student_orig_logits.detach().cpu().numpy(), labels.detach().cpu().numpy()))[metric_name])\nprint(loss__.mean())\nprint(metric__.mean())","d6b9bce8":"test_set.masked_tokens","0c7a5309":"import os\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = '1'","6667cc0b":"device = \"cpu\"\nstudent = student.to(device)","85458fbd":"# Task: 'mrpc' \nproba = 0.3\nbatch_size = 16\nlr = 1e-5\ntest_accuracy = 0.8260869565217391,\ntest_f1 = 0.87468671679198,\n"}}