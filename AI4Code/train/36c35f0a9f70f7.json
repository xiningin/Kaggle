{"cell_type":{"539c8cc9":"code","7981ebf0":"code","b8cb3f80":"code","827cb53c":"code","41dd0e4b":"code","62236fda":"code","3514eab5":"code","2e465265":"code","45753987":"code","e6bb1bc5":"code","6d5f1d94":"code","73af393b":"code","eb1f121d":"code","ed6df1ea":"code","b6d37932":"code","83009f54":"code","1000604a":"code","c95827c7":"code","d93a85d8":"code","5c8e49f4":"code","d985574a":"code","26170747":"code","08ffa454":"code","744b224e":"code","ca33f294":"code","ee5410d3":"code","686b6980":"code","ec631cd5":"code","a3c1a530":"code","38b2778a":"code","42f2f1f7":"code","f3300866":"code","428714ba":"code","ffc940a2":"code","9390444c":"code","d51002e3":"code","d8498eff":"code","96862207":"code","2bfd3dfb":"code","0566c65c":"code","ff284468":"code","b9f1ef2f":"code","e46419be":"code","6c4ffec6":"code","3e3bb474":"code","b76af21c":"code","8fb1bbb8":"code","16db27e9":"code","cf9ab0c9":"code","a0797509":"code","a42dd0a6":"code","cbf3f3cc":"code","c1b85dd3":"code","b5cdf643":"code","a11a2bbc":"code","fed1993f":"code","2e603372":"code","03b0c61e":"code","6f0e6def":"code","d0ed1a0f":"code","729978cd":"code","716f3169":"markdown","27d5c71d":"markdown","e65ee3c7":"markdown","a8cc6fe6":"markdown","8e0b67ed":"markdown","eb8b8e4f":"markdown","bf126d06":"markdown","a777a63c":"markdown","a0e31ffd":"markdown"},"source":{"539c8cc9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7981ebf0":"#importing the libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix \n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report,accuracy_score","b8cb3f80":"#import Dataset\ndataset = pd.read_csv('..\/input\/ckdisease\/kidney_disease.csv')","827cb53c":"dataset.head()","41dd0e4b":"dataset.shape","62236fda":"dataset.dtypes","3514eab5":"dataset[['htn','dm','cad','pe','ane']]=dataset[['htn','dm','cad','pe','ane']].replace(to_replace={'yes':1,'no':0})\ndataset[['rbc','pc']] = dataset[['rbc','pc']].replace(to_replace={'abnormal':1,'normal':0})\ndataset[['pcc','ba']] = dataset[['pcc','ba']].replace(to_replace={'present':1,'notpresent':0})\ndataset[['appet']] = dataset[['appet']].replace(to_replace={'good':1,'poor':0,'no':np.nan})\ndataset['classification']=dataset['classification'].replace(to_replace={'ckd':1.0,'ckd\\t':1.0,'notckd':0.0,'no':0.0})\ndataset.rename(columns={'classification':'class'},inplace=True)","2e465265":"# Further cleaning\ndataset['pe'] = dataset['pe'].replace(to_replace='good',value=0) # Not having pedal edema is good\ndataset['appet'] = dataset['appet'].replace(to_replace='no',value=0)\ndataset['cad'] = dataset['cad'].replace(to_replace='\\tno',value=0)\ndataset['dm'] = dataset['dm'].replace(to_replace={'\\tno':0,'\\tyes':1,' yes':1, '':np.nan})\ndataset.drop('id',axis=1,inplace=True)","45753987":"dataset.head()","e6bb1bc5":"# '?' character remove process in the dataset\nfor i in ['rc','wc','pcv']:\n    dataset[i] = dataset[i].str.extract('(\\d+)').astype(float)","6d5f1d94":"# Filling missing numeric data in the dataset with mean\nfor i in ['age','bp','sg','al','su','bgr','bu','sc','sod','pot','hemo','rc','wc','pcv']:\n    dataset[i].fillna(dataset[i].mean(),inplace=True)","73af393b":"dataset.isnull().sum()","eb1f121d":"dataset = dataset.dropna(axis=1) ","ed6df1ea":"dataset.shape","b6d37932":"dataset.isnull().sum()","83009f54":"dataset.head()","1000604a":"#Data preprocessing\nX = dataset.iloc[:,:-1].values\ny = dataset.iloc[:,-1].values","c95827c7":"# Feature Scaling\nsc = StandardScaler()\nX = sc.fit_transform(X)","d93a85d8":"#Splitting the dataset in to training and testing set\nX_train , X_test , y_train , y_test   = train_test_split(X,y,test_size = 0.2 , random_state=123)  ","5c8e49f4":"# Training the Logistic Regression model on the Training set\nlg = LogisticRegression(random_state = 0)\nlg.fit(X_train, y_train)","d985574a":"#predictin the test result\ny_pred_lg = lg.predict(X_test) ","26170747":"#calculate accuracy\nscore_lg = accuracy_score(y_pred_lg,y_test)\nscore_lg","08ffa454":"print(\"train score - \" + str(lg.score(X_train, y_train)))\nprint(\"test score - \" + str(lg.score(X_test, y_test)))","744b224e":"#Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm_lg = confusion_matrix(y_test,y_pred_lg)\nsns.set(font_scale=1.4) # for label size\nsns.heatmap(cm_lg, annot=True, annot_kws={\"size\": 16}) # font size\n\nplt.show()","ca33f294":"print(classification_report(y_test, y_pred_lg))","ee5410d3":"#fitting Decision Tree to the training set \ndtc = DecisionTreeClassifier(criterion='entropy',random_state=0)\ndtc.fit(X_train,y_train)","686b6980":"#predictin the test result\ny_pred_dtc = dtc.predict(X_test) ","ec631cd5":"#calculate accuracy\nscore_dtc = accuracy_score(y_pred_dtc,y_test)\nscore_dtc","a3c1a530":"print(\"train score - \" + str(dtc.score(X_train, y_train)))\nprint(\"test score - \" + str(dtc.score(X_test, y_test)))","38b2778a":"#Making the Confusion Matrix\ncm_dtc = confusion_matrix(y_test,y_pred_dtc)\n\nsns.set(font_scale=1.4) # for label size\nsns.heatmap(cm_dtc, annot=True, annot_kws={\"size\": 16}) # font size\n\nplt.show()","42f2f1f7":"print(classification_report(y_test, y_pred_dtc))","f3300866":"#fitting KNN to the training set\nknn= KNeighborsClassifier(n_neighbors=5 , metric='minkowski',p=2  )\nknn.fit(X_train,y_train)","428714ba":"#predictin the test result\ny_pred_knn = knn.predict(X_test) ","ffc940a2":"#calculate accuracy\nscore_dtc = accuracy_score(y_pred_knn,y_test)\nscore_dtc","9390444c":"print(\"train score - \" + str(knn.score(X_train, y_train)))\nprint(\"test score - \" + str(knn.score(X_test, y_test)))","d51002e3":"#Making the Confusion Matrix\ncm_knn = confusion_matrix(y_test,y_pred_knn)\n\nsns.set(font_scale=1.4) # for label size\nsns.heatmap(cm_knn, annot=True, annot_kws={\"size\": 16}) # font size\n\nplt.show()","d8498eff":"print(classification_report(y_test, y_pred_knn))","96862207":"#fitting SVM to the training set\nsvm = SVC(kernel='linear', random_state=0)\nsvm.fit(X_train,y_train)","2bfd3dfb":"#predictin the test result\ny_pred_svm = svm.predict(X_test) ","0566c65c":"score_svm = accuracy_score(y_pred_svm,y_test)\nscore_svm","ff284468":"print(\"train score - \" + str(svm.score(X_train, y_train)))\nprint(\"test score - \" + str(svm.score(X_test, y_test)))","b9f1ef2f":"#Making the Confusion Matrix\ncm_svm = confusion_matrix(y_test,y_pred_svm)\n\nsns.set(font_scale=1.4) # for label size\nsns.heatmap(cm_svm, annot=True, annot_kws={\"size\": 16}) # font size\n\nplt.show()","e46419be":"print(classification_report(y_test, y_pred_svm))","6c4ffec6":"#fitting kernal SVM to the training set\nksvm = SVC(kernel='rbf', random_state=0 )\nksvm.fit(X_train,y_train)","3e3bb474":"#predictin the test result\ny_pred_ksvm = ksvm.predict(X_test) ","b76af21c":"#calculate accuracy\nscore_ksvm = accuracy_score(y_pred_ksvm,y_test)\nscore_ksvm","8fb1bbb8":"print(\"train score - \" + str(ksvm.score(X_train, y_train)))\nprint(\"test score - \" + str(ksvm.score(X_test, y_test)))","16db27e9":"#Making the Confusion Matrix\ncm_ksvm = confusion_matrix(y_test,y_pred_ksvm)\nsns.set(font_scale=1.4) # for label size\nsns.heatmap(cm_ksvm, annot=True, annot_kws={\"size\": 16}) # font size\n\nplt.show()","cf9ab0c9":"print(classification_report(y_test, y_pred_ksvm))","a0797509":"#fitting Random Forest classification to the training set \nrfc = RandomForestClassifier(n_estimators=10 , criterion='entropy',random_state=0)\nrfc.fit(X_train,y_train)","a42dd0a6":"#predictin the test result\ny_pred_rfc = rfc.predict(X_test) ","cbf3f3cc":"#calculate accuracy\nscore_rfc = accuracy_score(y_pred_rfc,y_test)\nscore_rfc","c1b85dd3":"print(\"train score - \" + str(rfc.score(X_train, y_train)))\nprint(\"test score - \" + str(rfc.score(X_test, y_test)))","b5cdf643":"#Making the Confusion Matrix\ncm = confusion_matrix(y_test,y_pred_rfc)\n\nsns.set(font_scale=1.4) # for label size\nsns.heatmap(cm, annot=True, annot_kws={\"size\": 16}) # font size\n\nplt.show()","a11a2bbc":"print(classification_report(y_test, y_pred_rfc))","fed1993f":"#fitting kernal Navie bayes to the training set \nknb = GaussianNB()\nknb.fit(X_train,y_train)","2e603372":"#predictin the test result\ny_pred_knb = knb.predict(X_test) ","03b0c61e":"#calculate accuracy\nscore_knb = accuracy_score(y_pred_knb,y_test)\nscore_knb","6f0e6def":"print(\"train score - \" + str(knb.score(X_train, y_train)))\nprint(\"test score - \" + str(knb.score(X_test, y_test)))","d0ed1a0f":"#Making the Confusion Matrix\ncm_knb = confusion_matrix(y_test,y_pred_knb)\n\nsns.set(font_scale=1.4) # for label size\nsns.heatmap(cm_knb, annot=True, annot_kws={\"size\": 16}) # font size\n\nplt.show()","729978cd":"print(classification_report(y_test, y_pred_knb))","716f3169":"## Load Modules and helper functions","27d5c71d":"## Decision Tree Classifier","e65ee3c7":"## Support Vector Machine","a8cc6fe6":"## Logistic Regression\n","8e0b67ed":"### Cleaning and preprocessing of data for training","eb8b8e4f":"## Kernal Navie Bayes","bf126d06":"## kernal SVM","a777a63c":"## K Nearest Neighbors Classifier","a0e31ffd":"## Random Forest classification"}}