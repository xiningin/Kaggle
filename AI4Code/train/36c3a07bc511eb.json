{"cell_type":{"b8ff1891":"code","f0f0e30f":"code","c3c0d8ea":"code","1a0a7381":"code","81497020":"code","02be09c0":"code","a56b1b95":"code","19e646d5":"code","fce2b942":"code","6bd054de":"code","eea35e5a":"code","65707957":"code","07053179":"code","5741482b":"code","fff28ee8":"code","e671a718":"code","9fb045fa":"code","834b7814":"code","d16262d9":"code","b140faea":"code","6a2773d8":"code","116382c1":"code","0d889d35":"code","b2d979db":"code","e10ad63a":"code","95546e2d":"code","e5861191":"code","afbbaaa8":"code","4a4954d6":"code","67ac840a":"code","d06561a2":"code","953072a7":"code","c624b50f":"code","61eb8b54":"code","88135d83":"code","55ad3561":"code","550f245b":"code","0c5d4db5":"code","12900d63":"code","0ff765de":"markdown","45425148":"markdown","522aa4b6":"markdown","0930bb39":"markdown","6e8a99aa":"markdown","b90868a0":"markdown","0bc40d41":"markdown","cb6a35e0":"markdown"},"source":{"b8ff1891":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy as sp\nfrom sklearn.cluster import KMeans\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f0f0e30f":"# Assign sets important according to segmentations preparation. \n# Consuquently segmentation sets has to be controlled by experts\n\n\ndf = pd.read_csv(\"\/kaggle\/input\/violent-crime-rates-by-us-state\/US_violent_crime.csv\").copy()\ndf.head()","c3c0d8ea":"# firstly lets do it index 'Unnamed: 0' value in observation units\n\ndf.index = df.iloc[:,0]\ndf.index","1a0a7381":"df.head()","81497020":"# stil during 'Unnamed: 0' let's get out of the list\n\ndf = df.iloc[:,1:5]\ndf.head()","02be09c0":"df.index.name = \"Index\"\ndf.head()","a56b1b95":"# Let's see if I have any missing observations\n\ndf.isnull().sum()","19e646d5":"# here all of them have to be numerical values. Here we have made it this too\n\ndf.info()","fce2b942":"df.describe().T","6bd054de":"# here it is appropriate to visualize the data to try to understand it.\n# For example, 3 Assault (Assault) histogram looks like 3 peaks or accumulation.\n\ndf.hist(figsize = (10,10));","eea35e5a":"# The number of part sand (n_cluster) we need to determine per work can be the same as the number of variables we need to concentrate on.\n\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters = 4)\nkmeans","65707957":"# lets look model properties\n\n#?kmeans","07053179":"k_fit = kmeans.fit(df)","5741482b":"# Let's create the chunks of the model to be fit\n\nk_fit.n_clusters","fff28ee8":"# create the centers of these sets\n\nk_fit.cluster_centers_","e671a718":"# If I want to visualize, now let's reduce the set numbers to 2\nkmeans = KMeans(n_clusters = 2)\nk_fit = kmeans.fit(df)              ","9fb045fa":"sets = k_fit.labels_","834b7814":"# Let's visualize the data we reduced to 2 sets.\n\nplt.scatter(df.iloc[:,0], df.iloc[:,1], c = sets, s = 50, cmap = \"viridis\")\n\ncenters = k_fit.cluster_centers_                                 # We want to create 2 centers and show them on the visual.\n\nplt.scatter(centers[:,0], centers[:,1], c = \"black\", s = 200, alpha = 0.5);","d16262d9":"# Let us import 3D visualization. Otherwise it is necessary to download\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\n# Let's create our sets again, this time it will be 3 dimensional variable\n\nkmeans = KMeans(n_clusters = 3)\nk_fit = kmeans.fit(df)\nsets = k_fit.labels_\ncenters = kmeans.cluster_centers_","b140faea":"plt.rcParams['figure.figsize'] = (16, 9)\nfig = plt.figure()\nax = Axes3D(fig)\nax.scatter(df.iloc[:, 0], df.iloc[:, 1], df.iloc[:, 2]);","6a2773d8":"# Let's look at the visualization of these sets and centers on the plot.\n\nfig = plt.figure()\nax = Axes3D(fig)\nax.scatter(df.iloc[:, 0], df.iloc[:, 1], df.iloc[:, 2], c=sets)\nax.scatter(centers[:, 0], centers[:, 1], centers[:, 2], \n           marker='*', \n           c='#050505', \n           s=1000);","116382c1":"# To provide cluster numbers and information about which states (observations) these numbers belong to\n# If we want, we can take the model with 2 variables or 3 variables above, let's take the 3 ones\n\nkmeans = KMeans(n_clusters = 3)\nk_fit = kmeans.fit(df)\nsets = k_fit.labels_","0d889d35":"# to see which set and index you have for the top 10 states\n\npd.DataFrame({\"Provinces\" : df.index, \"Sets\": sets})[0:15]","b2d979db":"# to look at the set number that each belongs to\n\ndf[\"set_no\"] = sets\n\ndf.head()","e10ad63a":"# eger kume no 0 dan basliyorsa biz 1 den baslamasini istiyorsak soyle yapabiliriz\n\ndf[\"set_no\"] = df[\"set_no\"] + 1\n\ndf.head()","95546e2d":"# It is called one from 2 to 50. The number of sets should decrease, because we should approach zero, because we reduce the sands.\n# When you have 10 thousand customers, you are not interested in 100 people. It is necessary to put the customers with high degrees or features into a segment(sets).\n#!pip install yellowbrick\nfrom yellowbrick.cluster import KElbowVisualizer\nkmeans = KMeans()\nvisualizer = KElbowVisualizer(kmeans, k=(2,50))\nvisualizer.fit(df) \nvisualizer.poof()  \n\n# We understand the presentation from gorsel each point segment (set), ie, the set of elements with similar properties in it\n# For example, when the customer enters our site, a presentation can be made about what the monthly income it brings to us.","e5861191":"# Let's take our model above again\n# To provide cluster numbers and information about which states (observations) these numbers belong to\n# If we want we can take the model with 2 variables or 3 variables above, let's take the 4 normal ones\n\nkmeans = KMeans(n_clusters = 4)\nk_fit = kmeans.fit(df)\nsets = k_fit.labels_","afbbaaa8":"# to see which set and index you have for the top 10 states\n\npd.DataFrame({\"Provinces\" : df.index, \"Sets\": sets})[0:10]","4a4954d6":"df = pd.read_csv(\"\/kaggle\/input\/violent-crime-rates-by-us-state\/US_violent_crime.csv\").copy()\ndf.index = df.iloc[:,0]\ndf = df.iloc[:,1:5]\n#del df.index.name\ndf.index.name = \"Index\"\ndf.head()","67ac840a":"from scipy.cluster.hierarchy import linkage\n\nhc_complete = linkage(df, \"complete\")\nhc_average = linkage(df, \"average\")\nhc_single = linkage(df, \"single\")","d06561a2":"# We can watch its features and see what it does\n\ndir(hc_complete)","953072a7":"# We need to create Dendogram\n\nfrom scipy.cluster.hierarchy import dendrogram\n\nplt.figure(figsize=(15, 10))\nplt.title('Hierarchical Clustering - Dendogram')\nplt.xlabel('Indexs')\nplt.ylabel('Distance')\ndendrogram(\n    hc_complete,\n    leaf_font_size=10\n);","c624b50f":"# another form of representation and the number of elements below it\n\nfrom scipy.cluster.hierarchy import dendrogram\n\nplt.figure(figsize=(15, 10))\nplt.title('Hierarchical Clustering - Dendogram')\nplt.xlabel('Indexs')\nplt.ylabel('Distance')\ndendrogram(\n    hc_complete,\n    truncate_mode = \"lastp\",\n    p = 4,\n    show_contracted = True\n);","61eb8b54":"from scipy.cluster.hierarchy import dendrogram\n\nplt.figure(figsize=(15, 10))\nplt.title('Hierarchical Clustering - Dendogram')\nplt.xlabel('Indexs')\nplt.ylabel('Distance')\nden = dendrogram(\n    hc_complete,\n    leaf_font_size=10\n);","88135d83":"#?den","55ad3561":"#?dendrogram","550f245b":"# When we look at the dendogram, it will be logical to divide it into 4 large clusters. Then we say n_cluster = 4.\n\nfrom sklearn.cluster import AgglomerativeClustering\ncluster = AgglomerativeClustering(n_clusters = 4, \n                                  affinity = \"euclidean\", \n                                  linkage = \"ward\")\n\ncluster.fit_predict(df)","0c5d4db5":"# if we want to see which state is in which bank\n\npd.DataFrame({\"Provinces\" : df.index, \"Sets\": cluster.fit_predict(df)})[0:10]","12900d63":"df[\"set_no\"] = cluster.fit_predict(df)\ndf.head()","0ff765de":"# Hierarchical Clustering","45425148":"## Determination of Optimum Set Number","522aa4b6":"## Sets and observation units","0930bb39":"# K-MEANS Model and Visualizations","6e8a99aa":"# Exploratory Data Analysis","b90868a0":"# Conclusion\n\n\n\n    1. If this tutorial is not enough you can check also https:\/\/www.picostat.com\/dataset\/r-dataset-package-datasets-usarrests\n\n    2. I used here K_Means and Hierarchical Clustering Model but you can try also PCA Model. Results of both models:\n\n    3. After this tutorial, my aim is to prepare 'kernel' which is connected to Deep Learning 'not clear' data set.\n   \n If you have any suggestions, please could you write for me? I wil be happy for comment and critics!\n \n \n Thank you for your suggestion and votes ;)\n\n","0bc40d41":"## Optimum Set Number","cb6a35e0":"## Visualizations"}}