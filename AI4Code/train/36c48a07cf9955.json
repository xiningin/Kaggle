{"cell_type":{"c79122a3":"code","e91f8853":"code","580f47a0":"code","47bec4e7":"code","56046f6d":"code","7737aedc":"code","a433eed6":"code","7c6923b2":"code","e7ed8a81":"code","4276621c":"markdown","190bdd0d":"markdown","be33efd1":"markdown","f0706379":"markdown","c327df1a":"markdown","77c77a3e":"markdown"},"source":{"c79122a3":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e91f8853":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nfrom sklearn.utils import shuffle","580f47a0":"raw = pd.DataFrame(pd.read_csv(\n    \"\/kaggle\/input\/can-humans-really-be-random\/survey.csv\"\n).drop(\n    \"Timestamp\", axis=1\n).values.reshape(-1, 1), columns=[\"0\"])\n\nfor i in range(1, 21):\n    raw.insert(i, column=str(i), value=raw.iloc(axis=1)[i-1])\nfor column in raw:\n    raw[column] = raw[column].shift(list(raw.columns).index(column))\n\nraw = raw.dropna()\nraw = raw.reset_index().drop(\"index\", axis=1).astype(np.int)\n\nscaler = MinMaxScaler()\nraw = pd.DataFrame(scaler.fit_transform(raw.values.reshape(-1, 1)).reshape(raw.values.shape)).drop(20, axis=1)\nraw","47bec4e7":"X = np.array(raw.iloc(axis=1)[0:19]).reshape(-1, 19)\ny = np.array(raw.iloc(axis=1)[19]).reshape(-1, 1)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y)","56046f6d":"import matplotlib.pyplot as plt\nfrom matplotlib.style import use\n\nuse(\"dark_background\")","7737aedc":"fig, ax = plt.subplots(5, 4)\nfig.set_size_inches((24, 18))\nfig.subplots_adjust(hspace=0.6, wspace=0.6)\nfig.patch.set_facecolor('#333')\n\n\nplots = 0\ncols = [\"aquamarine\", \"azure\", \"gold\", \"coral\", \"wheat\", \"purple\"] * 4\n\n\nbins = 10\nfor i in range(5):\n    for j in range(4):\n        ax[i, j].hist(raw[raw.columns[plots]], bins=bins, color=str(\"xkcd:\" + cols[plots]))\n        \n        ax[i, j].title.set_text(f\"Question: {raw.columns[plots]}\")\n        ax[i, j].grid()\n        ax[i, j].set_axisbelow(False)\n        ax[i, j].patch.set_facecolor('#333')\n\n        plots += 1\n\nplt.show()","a433eed6":"import os\n\nos.environ['TF_CPP_MIN_LOG_LEVEL']  =  '3'\nimport tensorflow as tf\nos.environ['TF_CPP_MIN_LOG_LEVEL']  =  '0'\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.utils import shuffle\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense, Dropout, LSTM, Activation\nfrom tensorflow.keras.metrics import *\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam","7c6923b2":"model = Sequential()\nmodel.add(Dense(16, activation=\"elu\", input_shape=(19,)))\nmodel.add(Dense(32, activation=\"softplus\"))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1))\nmodel.add(Activation(\"relu\"))\n\nmodel.compile(\n    loss=\"mean_squared_error\",\n    optimizer=\"sgd\",\n    metrics=[\"accuracy\"]\n)\n\nhistory = model.fit(\n    X_train, y_train,\n    epochs=100,\n    verbose=0,\n    validation_split=0.1\n).history\n\nfig, ax = plt.subplots(1, 2)\nfig.set_size_inches((24, 8))\n\nfig.subplots_adjust(hspace=0.5, wspace=0.5)\nfig.patch.set_facecolor('#333')\n\nax[0].plot(history[\"loss\"], label = \"loss\", color='blue')\nax[0].plot(history[\"val_loss\"], label = \"val_loss\", color='aquamarine')\nax[0].patch.set_facecolor('#333')\nax[0].legend()\n\nax[1].plot(history[\"accuracy\"], label = \"accuracy\", color=\"gold\")\nax[1].plot(history[\"val_accuracy\"], label = \"val_accuracy\", color=\"khaki\")\nax[1].patch.set_facecolor('#333')\nax[1].legend()\n\nplt.show()\n\npreds = model.predict(X_valid)\nprint(f\"MAE: {mean_absolute_error(preds, y_valid)}\")","e7ed8a81":"model = Sequential()\nmodel.add(Dense(16, activation=\"relu\", input_shape=(19,)))\nmodel.add(Dense(32, activation=\"softplus\"))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10))\nmodel.add(Activation(\"softmax\"))\n\nmodel.compile(\n    loss=\"sparse_categorical_crossentropy\",\n    optimizer=\"sgd\",\n    metrics=[\"accuracy\"]\n)\n\nhistory = model.fit(\n    X_train, y_train,\n    epochs=100,\n    verbose=0,\n    validation_split=0.1\n).history\n\nfig, ax = plt.subplots(1, 2)\nfig.set_size_inches((24, 8))\n\nfig.subplots_adjust(hspace=0.5, wspace=0.5)\nfig.patch.set_facecolor('#333')\n\nax[0].plot(history[\"loss\"], label = \"loss\", color='blue')\nax[0].plot(history[\"val_loss\"], label = \"val_loss\", color='aquamarine')\nax[0].patch.set_facecolor('#333')\nax[1].plot(history[\"accuracy\"], label = \"accuracy\", color=\"gold\")\nax[1].plot(history[\"val_accuracy\"], label = \"val_accuracy\", color=\"khaki\")\nax[1].patch.set_facecolor('#333')\n\nplt.legend()\nplt.show()\n\npreds = model.predict(X_valid)\npreds = np.array([np.argmax(i) for i in preds])\n\nprint(f\"MAE: {mean_absolute_error(preds, y_valid)}\")","4276621c":"## Dependencies","190bdd0d":"# Modelling","be33efd1":"# Question Answer Distribution\nBelow is the visualization of the question answers.","f0706379":"## DNN Regression","c327df1a":"# What does this mean?\nWell. With a method, a DNN regressor, the loss is quite low. Could this mean that it shows that humans are not really capable of being random? You choose for yourself...","77c77a3e":"## DNN Classification"}}