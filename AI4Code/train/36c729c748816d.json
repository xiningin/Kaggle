{"cell_type":{"a6d190ca":"code","8b1b7038":"code","fca81982":"code","962db335":"code","bb825cc1":"code","1382a578":"code","87ff65d2":"code","07cec559":"code","73bf5f78":"code","9178c7f9":"code","ec6e4fcd":"code","88190799":"code","98661463":"code","ff5dd72a":"code","eb6b4aea":"code","a7fd4112":"code","241223f9":"code","62538e60":"code","df5a1086":"code","c2e0a9dd":"code","0584878c":"code","bb1b8743":"code","431565c8":"code","f5d242ef":"code","a4c529e7":"code","726e16cb":"code","363ef10b":"code","f5fc0bbd":"code","328a692f":"code","39aa05ba":"code","0132a107":"code","d4c785ce":"code","64962319":"code","68f5f4aa":"code","9eb2fa13":"code","5edbb3f6":"code","395a2a5b":"code","517d3b5b":"code","9d355363":"code","e07c061d":"code","a19b05e2":"code","4abbe0a7":"code","c1986447":"code","03be30fc":"code","60af5ece":"code","862c30f3":"code","679abde9":"markdown","9664b247":"markdown","9232883e":"markdown","48c338d5":"markdown","b51658ec":"markdown","341e070c":"markdown","8b6e008d":"markdown","f7b24c72":"markdown","a773c0ae":"markdown","03fe1d19":"markdown","47d7710c":"markdown","bddd5108":"markdown","f98635ac":"markdown","fadaec84":"markdown","34d7eaa0":"markdown"},"source":{"a6d190ca":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8b1b7038":"import gc","fca81982":"calendar = pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv\")\nsell_prices = pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv\")\nsales_train = pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv\")\nsample_sub = pd.read_csv(\"\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv\")\n","962db335":"sell_prices.isnull().sum()","bb825cc1":"def memory_reduction(dataset):\n    column_types = dataset.dtypes\n    temp = None\n    for x in range(len(column_types)):\n        column_types[x] = str(column_types[x])\n    for x in range(len(column_types)):\n        temp = dataset.columns[x]\n        if dataset.columns[x] == \"date\":\n            dataset[temp] = dataset[temp].astype(\"datetime64\")\n        if column_types[x] == \"int64\" and dataset.columns[x] != \"date\":\n            dataset[temp] = dataset[temp].astype(\"int16\")\n        if column_types[x] == \"object\" and dataset.columns[x] != \"date\":\n            dataset[temp] = dataset[temp].astype(\"category\")\n        if column_types[x] == \"float64\" and dataset.columns[x] != \"date\":\n            dataset[temp] = dataset[temp].astype(\"float16\")\n    return dataset","1382a578":"calendar = memory_reduction(calendar)\nsell_prices = memory_reduction(sell_prices)\nsales_train = memory_reduction(sales_train)\nsample_sub = memory_reduction(sample_sub)","87ff65d2":"#we arent removing data for the moment like we did previously \n# Next we need to transform our dates to usable or model efficient formats\n# For that we will be creating a day column and week num columns and reducing there memory\ncalendar[\"day\"] = pd.DatetimeIndex(calendar[\"date\"]).day\ncalendar[\"day\"] = calendar[\"day\"].astype(\"int8\")\ncalendar[\"week_num\"] = (calendar[\"day\"] - 1) \/\/ 7 + 1\ncalendar[\"week_num\"] = calendar[\"week_num\"].astype(\"int8\")","07cec559":"# Now lets see it \ncalendar.tail()","73bf5f78":"#we have to add a category named missing in order to omit the NaN values as we have described object data type to category data type\ncalendar[\"event_name_1\"] = calendar[\"event_name_1\"].cat.add_categories('missing')","9178c7f9":"from sklearn.preprocessing import OrdinalEncoder\n# we are droping date\n# we are stripping d_ values to int\n# changing Nan values to mising in events \n# and also integer encoding the values of catagorical variable\ndef prep_calendar(df):\n    df = df.drop([\"date\", \"weekday\"], axis=1)\n    df = df.assign(d = df.d.str[2:].astype(int))\n    df[\"event_type_1\"] = df[\"event_type_1\"].cat.add_categories('missing')\n    df[\"event_name_2\"] = df[\"event_name_2\"].cat.add_categories('missing')\n    df[\"event_type_2\"] = df[\"event_type_2\"].cat.add_categories('missing')\n    \n    df = df.fillna(\"missing\")\n    cols = list(set(df.columns) - {\"wm_yr_wk\", \"d\"})\n    df[cols] = OrdinalEncoder(dtype=\"int\").fit_transform(df[cols])\n    df = memory_reduction(df)\n    return df\n\ncalendar = prep_calendar(calendar)","ec6e4fcd":"calendar.head()","88190799":"sales_train.head()","98661463":"sales_train.shape","ff5dd72a":"#this is basically changing sales data from wide to long format \n\n\ndef reshape_sales(df, drop_d = None):\n    if drop_d is not None:\n        df = df.drop([\"d_\" + str(i + 1) for i in range(drop_d)], axis=1)\n    df = df.assign(id=df.id.str.replace(\"_validation\", \"\"))\n    df = df.reindex(columns=df.columns.tolist() + [\"d_\" + str(1913 + i + 1) for i in range(2 * 28)])\n    df = df.melt(id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n                 var_name='d', value_name='demand')\n    df = df.assign(d=df.d.str[2:].astype(\"int16\"))\n    return df\n\nsales_train = reshape_sales(sales_train, 488)","eb6b4aea":"sales_train.head()","a7fd4112":"from sklearn.preprocessing import OrdinalEncoder","241223f9":"#now we are calculating rolling mean and standard deviation\ndef prep_sales(df):\n    #this is shifting the data by 28\n    df['lag_t28'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28))\n    #rolling mean window 7\n    df['rolling_mean_t7'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).mean())\n    #rolling mean window 15\n    df['rolling_mean_t15'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(15).mean())\n    #rolling mean window 30\n    df['rolling_mean_t30'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).mean())\n    #rolling std window 7\n    df['rolling_std_t7'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).std())\n    #rolling mean window 30\n    df['rolling_std_t30'] = df.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).std())\n\n    # Remove rows with NAs except for submission rows. rolling_mean_t180 was selected as it produces most missings\n    df = df[(df.d >= 1914) | (pd.notna(df.rolling_mean_t30))]\n    df = memory_reduction(df)\n\n    return df\n\nsales_train = prep_sales(sales_train)","62538e60":"sales_train.head()","df5a1086":"sell_prices.dtypes","c2e0a9dd":"#we have only calculated rolling mean, std , cummulative relation btween them \n\ndef prep_selling_prices(df):\n    \n    gr = df.groupby([\"store_id\", \"item_id\"])[\"sell_price\"]\n    df[\"sell_price_rel_diff\"] = gr.pct_change()\n    df[\"sell_price_rel_diff\"] = df[\"sell_price_rel_diff\"].astype(\"float16\")\n    df[\"sell_price_rel_diff\"] = df[\"sell_price_rel_diff\"].fillna(0) \n    df[\"sell_price_roll_sd7\"] = gr.transform(lambda x: x.rolling(7).std())\n    df[\"sell_price_roll_sd7\"] = df[\"sell_price_roll_sd7\"].astype(\"float16\")\n    df[\"sell_price_roll_sd7\"] = df[\"sell_price_roll_sd7\"].fillna(0)\n    df[\"sell_price_cumrel\"] = (gr.shift(0) - gr.cummin()) \/ (1 + gr.cummax() - gr.cummin())\n    df[\"sell_price_cumrel\"] = df[\"sell_price_cumrel\"].astype(\"float16\")\n    df[\"sell_price_cumrel\"] = df[\"sell_price_cumrel\"].fillna(0)\n    df = memory_reduction(df)\n    return df\n\nsell_prices = prep_selling_prices(sell_prices)","0584878c":"sell_prices.dtypes","bb1b8743":"sell_prices.head()","431565c8":"#merging with calendar\nsales_train = sales_train.merge(calendar, how=\"left\", on=\"d\")\ngc.collect()\nsales_train.head()","f5d242ef":"sales_train = sales_train.merge(sell_prices, how=\"left\", on=[\"wm_yr_wk\", \"store_id\", \"item_id\"])\nsales_train.drop([\"wm_yr_wk\"], axis=1, inplace=True)\ngc.collect()\nsales_train.head()","a4c529e7":"del sell_prices\ngc.collect()\nsales_train.info()","726e16cb":"gc.collect()","363ef10b":"sales_train.tail()","f5fc0bbd":"\"\"\"\ncat_id_cols = [\"item_id\", \"dept_id\", \"store_id\", \"cat_id\", \"state_id\"]\ncat_cols = cat_id_cols + [\"wday\", \"month\", \"year\", \"event_name_1\", \n                          \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n# if you want to check the progress of it use \n# from tqdm import tqdm\n# for i, v in tqdm(enumerate(cat_id_cols)):\n# In loop to minimize memory use\nfor i, v in enumerate(cat_id_cols):\n    sales_train[v] = OrdinalEncoder(dtype=\"int\").fit_transform(sales_train[[v]])\n\nsales_train = memory_reduction(sales_train)\ngc.collect()\nsales_train.head()\n\"\"\"","328a692f":"\"\"\"\nnum_cols = [\"sell_price\", \"sell_price_rel_diff\", \"sell_price_roll_sd7\", \"sell_price_cumrel\",\n            \"lag_t28\", \"rolling_mean_t7\", \"rolling_mean_t15\", \"rolling_mean_t30\", \n            \"rolling_std_t7\", \"rolling_std_t30\"]\nbool_cols = [\"snap_CA\", \"snap_TX\", \"snap_WI\"]\ndense_cols = num_cols + bool_cols\n\n# Need to do column by column due to memory constraints\nfor i, v in enumerate(num_cols):\n    sales_train[v] = sales_train[v].fillna(sales_train[v].median())\n    \nsales_train.head()\n\"\"\"","39aa05ba":"\"\"\"\ntest = sales_train[sales_train.d >= 1914]\ntest[\"id\"] = test[\"id\"].astype(\"str\")\ntest = test.assign(id=test.id + \"_\" + np.where(test.d <= 1941, \"validation\", \"evaluation\"),\n                   F=\"F\" + (test.d - 1913 - 28 * (test.d > 1941)).astype(\"str\"))\ntest.head()\ngc.collect()\n\"\"\"","0132a107":"\"\"\"\n# Input dict for training with a dense array and separate inputs for each embedding input\ndef make_X(df):\n    X = {\"dense1\": df[dense_cols].to_numpy()}\n    for i, v in enumerate(cat_cols):\n        X[v] = df[[v]].to_numpy()\n    return X\n\n# Submission data\nX_test = make_X(test)\n\n# One month of validation data\nflag = (sales_train.d < 1914) & (sales_train.d >= 1914 - 28)\nvalid = (make_X(sales_train[flag]),\n         sales_train[\"demand\"][flag])\n\n# Rest is used for training\nflag = sales_train.d < 1914 - 28\nX_train = make_X(sales_train[flag])\ny_train = sales_train[\"demand\"][flag]\n                             \ndel sales_train, flag\ngc.collect()\n\"\"\"","d4c785ce":"\"\"\"\ngc.collect()\n\"\"\"","64962319":"\"\"\"\ny_train.head()\n\"\"\"","68f5f4aa":"#lets first redue the data to d 1913 for train and rest for the purpose of test \n\ntrain = sales_train[sales_train['d'] <= 1913]\ntest = sales_train[sales_train['d'] >= 1913]\ntest = sales_train[sales_train['d'] >= 1941]\ndel sales_train\ngc.collect()","9eb2fa13":"train.info()","5edbb3f6":"test.info()","395a2a5b":"gc.collect()","517d3b5b":"#new memory reduction\ndef memory_reduction(dataset):\n    column_types = dataset.dtypes\n    temp = None\n    for x in range(len(column_types)):\n        column_types[x] = str(column_types[x])\n    for x in range(len(column_types)):\n        temp = dataset.columns[x]\n        if dataset.columns[x] == \"date\":\n            dataset[temp] = dataset[temp].astype(\"datetime64\")\n        if column_types[x] == \"int16\" and dataset.columns[x] != \"date\":\n            dataset[temp] = dataset[temp].astype(\"int8\")\n        if column_types[x] == \"object\" and dataset.columns[x] != \"date\":\n            dataset[temp] = dataset[temp].astype(\"category\")\n        if column_types[x] == \"float16\" and dataset.columns[x] != \"date\":\n            dataset[temp] = dataset[temp].astype(\"float16\")\n    return dataset","9d355363":"train = memory_reduction(train)\ntrain.info()","e07c061d":"# saving ids for future \ntrain_ids = train['id']\ntest_ids = test['id']","a19b05e2":"#droping id for training purpose\ntrain = train.drop('id', axis = 1)\ntest = test.drop('id', axis = 1)","4abbe0a7":"gc.collect()","c1986447":"columns = ['item_id' ,'dept_id', 'cat_id', 'store_id', 'state_id']\node = OrdinalEncoder()\ntrain = ode.fit_transform(train)\ngc.collect()","03be30fc":"from sklearn.linear_model import LinearRegression","60af5ece":"lreg = LinearRegression()","862c30f3":"lreg.fit(X_train, y_train)","679abde9":"Now we are creating embeddings for catagorcal variables rather than using dummies **please change it if you want too **","9664b247":"lets start with CALENDAR","9232883e":"### Categories to countinous columns","48c338d5":"# After this is Data Transformation for NN purpose if you need Other wise skip to Heading ML Model Trainng","b51658ec":"## First a regression model to check that every thing works fine and a model can be created\n","341e070c":"# ML MODEL BUILDING","8b6e008d":"Frist priority is to reduce the memory of the data sets ","f7b24c72":"COMBINING DATA","a773c0ae":"Now we will be configuring SALES data ","03fe1d19":"### Creating X and y columns","47d7710c":"NOW lets configure our sellin price ","bddd5108":"# DATA WRANGLING","f98635ac":"## DATA SPLIT AND PREP FOR ML MODEL","fadaec84":"importing libraries","34d7eaa0":"for that we are using ordinal encoder from sklearn package"}}