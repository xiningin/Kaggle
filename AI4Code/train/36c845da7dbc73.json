{"cell_type":{"f7e150f0":"code","1b51856c":"code","f83b1e36":"code","4ef8e3a7":"code","298e623a":"code","070b807e":"code","b0905dcd":"code","b0ac1045":"code","639b8994":"code","5698150f":"code","27446688":"code","13e7cca1":"code","3975a031":"code","dc90fe1a":"code","becb2b5d":"code","02f51aff":"code","9208a01a":"code","7d5c58e0":"code","c9c22f92":"code","1388e8f1":"code","b2874c18":"code","2c0f79a3":"code","9b62ad9f":"code","220a0481":"code","7ae62e03":"code","555d75ef":"code","a63a29fa":"code","4d1a75eb":"code","ca22635e":"code","f35c07a7":"code","1d34b1ed":"code","9b618bb0":"code","e2fff678":"code","78be1e85":"code","7fd025a3":"code","28cf7a35":"code","8e85a237":"code","511e1d9f":"code","c5dae688":"code","612cb301":"code","4d4e20b5":"code","4bbc3c63":"code","04af4bfc":"code","311789b2":"code","5c45938f":"code","5b63b8d7":"code","bf79f4cb":"code","2af48972":"code","2256a66c":"code","5474e8b6":"code","693ab8e8":"code","56ef103d":"markdown","4a998ee8":"markdown","18855293":"markdown","d29701ad":"markdown","51996353":"markdown","02cfdaf6":"markdown","bc7586f4":"markdown","2c15e565":"markdown","d62a812f":"markdown","d7732024":"markdown","025344e6":"markdown","8e556a1c":"markdown","1a738f6c":"markdown","a70b1a8b":"markdown","fb96e347":"markdown","e6afb0ef":"markdown","1b06c6bb":"markdown","d058d40f":"markdown","e229bfd6":"markdown","b27aa09e":"markdown","cbdec7b8":"markdown","ec3c9c2e":"markdown","7cd5baa8":"markdown","bad806ca":"markdown","1f442bc8":"markdown","e24fe575":"markdown","783ec9e4":"markdown","5b56ee11":"markdown","ec426ebf":"markdown","a9accf6b":"markdown","1481429a":"markdown","2c89aebd":"markdown"},"source":{"f7e150f0":"import numpy as np \nfrom numpy import hstack\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\n\nfrom sklearn.model_selection import train_test_split,RandomizedSearchCV\nfrom sklearn.metrics import classification_report, accuracy_score\n\nfrom sklearn.impute import KNNImputer\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,VotingClassifier,AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nplt.style.use('fivethirtyeight')\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","1b51856c":"train_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntrain_data.head(20)","f83b1e36":"train_data.describe()","4ef8e3a7":"train_data.info()","298e623a":"features = []\nfor i in train_data.columns:\n    if i !='Survived':\n        features.append(i)\n\nfeatures","070b807e":"train_data.isnull().sum()","b0905dcd":"for i in features:\n    print(\"\\n\\n\"+i)\n    print (train_data[[i, \"Survived\"]].groupby([i], as_index=False).mean())","b0ac1045":"train_data.drop(['PassengerId','Ticket'],axis=1,inplace=True)","639b8994":"\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\n\ntrain_data['Title'] = train_data['Name'].apply(get_title)\ntrain_data.drop('Name',axis=1,inplace=True)\ntrain_data['Title'].unique()","5698150f":"train_data.groupby(['Title']).size()","27446688":"train_data['Title'] = train_data['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Others')\n\ntrain_data['Title'] = train_data['Title'].replace('Mlle', 'Miss')\ntrain_data['Title'] = train_data['Title'].replace('Ms', 'Miss')\ntrain_data['Title'] = train_data['Title'].replace('Mme', 'Mrs')\n","13e7cca1":"train_data[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","3975a031":"def cabin_group(cabin):\n    if type(cabin) == str:\n        letter =  cabin[0]\n    else:\n        letter = -1\n    \n    if type(letter) == str and ord(letter) <= 70:\n        return letter\n    elif letter == -1:\n        return '-1'\n    else:\n        return 'OtherCabin'\n    \n    \ntrain_data['Cabin_Group'] = train_data['Cabin'].apply(cabin_group)\ntrain_data.drop('Cabin',axis=1,inplace=True)\ntrain_data.groupby(['Cabin_Group']).size()","dc90fe1a":"train_data[['Cabin_Group', 'Survived']].groupby(['Cabin_Group'], as_index=False).mean()","becb2b5d":"bucket = pd.qcut(train_data['Fare'], 5)\nbucket.unique()","02f51aff":"def FareGroup(fare):\n    if float(fare):\n        if fare > 0.0 and fare <= 7.854:\n            return 1\n        elif fare > 7.854 and fare <= 10.5:\n            return 2\n        elif fare > 10.5 and fare <= 21.679:\n            return 3\n        elif fare > 21.679 and fare <= 39.688:\n            return 4\n        elif fare > 39.688 and fare <= 512.329:\n            return 5\n        else:\n            return -1\n    else:\n        return -1\n    \n    \ntrain_data['FareRange'] = train_data['Fare'].apply(FareGroup)\ntrain_data.drop('Fare',axis=1,inplace=True)\nprint(train_data.groupby(['FareRange']).size())","9208a01a":"print (train_data[['FareRange', 'Survived']].groupby(['FareRange'], as_index=False).mean())","7d5c58e0":"bucket = pd.qcut(train_data['Age'], 5).unique()\nbucket","c9c22f92":"def AgeGroup(age):\n    if float(age):\n        if age > 19.0 and age <= 25.0:\n            return 19\n        elif age > 25.0 and age <= 31.8:\n            return 25\n        elif age > 31.8 and age <= 41.0:\n            return 31\n        elif age > 41.0 and age <= 80.0:\n            return 41\n        elif age < 19.0:\n            return 0\n        else:\n            return -1\n    else:\n        return -1\n    \n    \ntrain_data['AgeGroup'] = train_data['Age'].apply(AgeGroup)\ntrain_data.drop('Age',axis=1,inplace=True)\nprint (train_data[['AgeGroup', 'Survived']].groupby(['AgeGroup'], as_index=False).mean())","1388e8f1":"features = []\nfor i in train_data.columns:\n    if i !='Survived':\n        features.append(i)\n\nfeatures","b2874c18":"train_data.dropna(subset=['Embarked'],inplace=True)\ntrain_data.groupby(['Embarked']).size()","2c0f79a3":"train_data['AgeGroup'] = train_data['AgeGroup'].apply(lambda x: np.nan if x == -1 else x)\ntrain_data['FareRange'] = train_data['FareRange'].apply(lambda x: np.nan if x == -1 else x)\n\n\nimputer = KNNImputer(n_neighbors= 5)\nAgeGroupImputed = imputer.fit_transform([train_data['AgeGroup']])\nFareRangeImputed = imputer.fit_transform([train_data['FareRange']])\nFareRangeImputed = pd.Series(FareRangeImputed[0])\nAgeGroupImputed = pd.Series(AgeGroupImputed[0])\ntrain_data['FareRange'] = FareRangeImputed\ntrain_data['AgeGroup'] = AgeGroupImputed\n\ntrain_data.head(20)","9b62ad9f":"sex = pd.get_dummies(train_data['Sex'],drop_first=True)\nembark = pd.get_dummies(train_data['Embarked'],drop_first=True)\ntitle = pd.get_dummies(train_data['Title'],drop_first=True)\nfare = pd.get_dummies(train_data['FareRange'],drop_first=True)\ncabin = pd.get_dummies(train_data['Cabin_Group'],drop_first=True)\nage = pd.get_dummies(train_data['AgeGroup'],drop_first=True)\n\ntrain_data.drop(['AgeGroup','Sex',\"Embarked\",'Title','Cabin_Group','FareRange'],axis=1,inplace=True)\n\ntrain_data = pd.concat([train_data,sex,embark,title,cabin,fare,age],axis=1)\ntrain_data.head(20)","220a0481":"X_train, X_test, y_train, y_test = train_test_split(train_data.drop('Survived',axis=1), \n                                                    train_data['Survived'], test_size=0.30, \n                                                    random_state=101)","7ae62e03":"RandomForestModel = RandomForestClassifier(random_state=3)\nRandomForestModel.fit(X_train,y_train)\naccuracy = accuracy_score(y_test,RandomForestModel.predict(X_test))\nclassificationRep=classification_report(y_test,RandomForestModel.predict(X_test))\nprint(\"Classification Report: \\n\"+classificationRep)\nprint(\"Accuracy: \"+str(accuracy))","555d75ef":"LGBMModel = LGBMClassifier()\nLGBMModel.fit(X_train,y_train)\naccuracy = accuracy_score(y_test,LGBMModel.predict(X_test))\nclassificationRep=classification_report(y_test,LGBMModel.predict(X_test))\nprint(\"Classification Report: \\n\"+classificationRep)\nprint(\"Accuracy: \"+str(accuracy))","a63a29fa":"XGBModel = XGBClassifier()\nXGBModel.fit(X_train,y_train)\naccuracy = accuracy_score(y_test,XGBModel.predict(X_test))\nclassificationRep=classification_report(y_test,XGBModel.predict(X_test))\nprint(\"Classification Report: \\n\"+classificationRep)\nprint(\"Accuracy: \"+str(accuracy))","4d1a75eb":"AdaBoostModel = AdaBoostClassifier()\nAdaBoostModel.fit(X_train,y_train)\naccuracy = accuracy_score(y_test,AdaBoostModel.predict(X_test))\nclassificationRep=classification_report(y_test,AdaBoostModel.predict(X_test))\nprint(\"Classification Report: \\n\"+classificationRep)\nprint(\"Accuracy: \"+str(accuracy))","ca22635e":"\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\ncriterion = ['gini', 'entropy']\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","f35c07a7":"RandomForestModel2 = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = RandomForestModel2, param_distributions = random_grid, n_iter = 50, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, y_train)\nparams = rf_random.best_params_\nparams","1d34b1ed":"RandomForestModel2 = RandomForestClassifier(n_estimators=params['n_estimators'],min_samples_split=params['min_samples_split'],min_samples_leaf = params['min_samples_leaf'],max_features = params['max_features'],max_depth = params['max_depth'],bootstrap=params['bootstrap'])\nRandomForestModel2.fit(X_train,y_train)\naccuracy = accuracy_score(y_test,RandomForestModel2.predict(X_test))\nclassificationRep=classification_report(y_test,RandomForestModel2.predict(X_test))\nprint(\"Classification Report: \\n\"+classificationRep)\nprint(\"Accuracy: \"+str(accuracy))","9b618bb0":"random_grid_lgbm = { \n                   'num_leaves':[int(x) for x in np.linspace(start = 1, stop = 200, num = 10)],\n                   'max_depth':[int(x) for x in np.linspace(start = 10, stop = 100, num = 10)], \n                   'learning_rate':[0.1,0.001], \n                   'n_estimators':[int(x) for x in np.linspace(start = 1, stop = 100, num = 10)]}\n","e2fff678":"LGBMModel2 = LGBMClassifier()\nrf_random = RandomizedSearchCV(estimator = LGBMModel2, param_distributions = random_grid_lgbm, n_iter = 50, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n\nrf_random.fit(X_train, y_train)\nrandom_grid_lgbm = rf_random.best_params_\nrandom_grid_lgbm","78be1e85":"LGBMModel2 = LGBMClassifier(n_estimators=random_grid_lgbm['n_estimators'],num_leaves=random_grid_lgbm['num_leaves'],max_depth = random_grid_lgbm['max_depth'],learning_rate=random_grid_lgbm['learning_rate'])\nLGBMModel2.fit(X_train,y_train)\naccuracy = accuracy_score(y_test,LGBMModel2.predict(X_test))\nclassificationRep=classification_report(y_test,LGBMModel2.predict(X_test))\nprint(\"Classification Report: \\n\"+classificationRep)\nprint(\"Accuracy: \"+str(accuracy))","7fd025a3":"xgparam = {'eta': [0.01,0.1,0.2],\n'min_child_weight': [int(x) for x in np.linspace(start = 1, stop = 10, num = 10)],\n'max_depth': [int(x) for x in np.linspace(start = 10, stop = 100, num = 10)],\n'gamma':np.linspace(start = 0.0, stop = 0.2, num = 20)}\n\nXGBModel2 = XGBClassifier()\nrf_random = RandomizedSearchCV(estimator = XGBModel2, param_distributions = xgparam, n_iter = 50, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n\nrf_random.fit(X_train, y_train)\nxgparam = rf_random.best_params_\nxgparam","28cf7a35":"XGBModel2 = XGBClassifier(eta=xgparam['eta'],min_child_weight=xgparam['min_child_weight'],max_depth = xgparam['max_depth'],gamma=xgparam['gamma'])\nXGBModel2.fit(X_train,y_train)\naccuracy = accuracy_score(y_test,XGBModel2.predict(X_test))\nclassificationRep=classification_report(y_test,XGBModel2.predict(X_test))\nprint(\"Classification Report: \\n\"+classificationRep)\nprint(\"Accuracy: \"+str(accuracy))","8e85a237":"adaparam = {\n    'n_estimators' : [int(x) for x in np.linspace(start = 10, stop = 100, num = 20)],\n    'learning_rate' : np.linspace(start = 0.01, stop = 1, num = 20)\n}\n\nAdaBoostModel2 = AdaBoostClassifier()\nrf_random = RandomizedSearchCV(estimator = AdaBoostModel2, param_distributions = adaparam, n_iter = 50, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n\nrf_random.fit(X_train, y_train)\nadaparam = rf_random.best_params_\nadaparam","511e1d9f":"AdaBoostModel2 = AdaBoostClassifier(n_estimators=adaparam['n_estimators'],learning_rate=adaparam['learning_rate'])\nAdaBoostModel2.fit(X_train,y_train)\naccuracy = accuracy_score(y_test,AdaBoostModel2.predict(X_test))\nclassificationRep=classification_report(y_test,AdaBoostModel2.predict(X_test))\nprint(\"Classification Report: \\n\"+classificationRep)\nprint(\"Accuracy: \"+str(accuracy))","c5dae688":"\nvotingModel = VotingClassifier(estimators=[('RF', RandomForestModel2), ('LGBM', LGBMModel2),\n('XGB', XGBModel2),('ADA',AdaBoostModel2)], voting='hard')\n\nvotingModel = votingModel.fit(X_train, y_train)\n\naccuracy = accuracy_score(y_test,votingModel.predict(X_test))\nclassificationRep=classification_report(y_test,votingModel.predict(X_test))\nprint(\"Classification Report: \\n\"+classificationRep)\nprint(\"Accuracy: \"+str(accuracy))","612cb301":"#blender\nBlend_X = list()\ny1 = RandomForestModel2.predict(X_test)\ny1 = y1.reshape(len(y1), 1)\nBlend_X.append(y1)\n\ny2 = LGBMModel2.predict(X_test)\ny2 = y2.reshape(len(y2), 1)\nBlend_X.append(y2)\n\ny3 = XGBModel2.predict(X_test)\ny3 = y3.reshape(len(y3), 1)\nBlend_X.append(y3)\n\ny4 = AdaBoostModel2.predict(X_test)\ny4 = y4.reshape(len(y4), 1)\nBlend_X.append(y4)\n\nBlend_X = hstack(Blend_X)\nfrom sklearn.linear_model import LogisticRegression\nBlenderModel = LogisticRegression()\nBlenderModel.fit(Blend_X, y_test)\n\naccuracy = accuracy_score(y_test,BlenderModel.predict(Blend_X))\nclassificationRep=classification_report(y_test,BlenderModel.predict(Blend_X))\nprint(\"Classification Report: \\n\"+classificationRep)\nprint(\"Accuracy: \"+str(accuracy))","4d4e20b5":"test_data.describe()","4bbc3c63":"test_data.info()","04af4bfc":"test_data_original = test_data.copy()","311789b2":"features = []\nfor i in test_data.columns:\n    if i !='Survived':\n        features.append(i)\n\nfeatures","5c45938f":"test_data.isnull().sum()","5b63b8d7":"test_data.drop(['PassengerId','Ticket'],axis=1,inplace=True)\n\ntest_data['Title'] = test_data['Name'].apply(get_title)\ntest_data.drop('Name',axis=1,inplace=True)\ntest_data['Title'] = test_data['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Others')\n\ntest_data['Title'] = test_data['Title'].replace('Mlle', 'Miss')\ntest_data['Title'] = test_data['Title'].replace('Ms', 'Miss')\ntest_data['Title'] = test_data['Title'].replace('Mme', 'Mrs')\n\n\ntest_data.groupby(['Title']).size()","bf79f4cb":"test_data['Cabin_Group'] = test_data['Cabin'].apply(cabin_group)\nprint(test_data.groupby(['Cabin_Group']).size())\ntest_data.drop('Cabin',axis=1,inplace=True)\n\ntest_data['FareRange'] = test_data['Fare'].apply(FareGroup)\ntest_data.drop('Fare',axis=1,inplace=True)\nprint(test_data.groupby(['FareRange']).size())\n\ntest_data['AgeGroup'] = test_data['Age'].apply(AgeGroup)\nprint(test_data.groupby(['AgeGroup']).size())\ntest_data.drop('Age',axis=1,inplace=True)","2af48972":"features = []\nfor i in test_data.columns:\n    features.append(i)\n\nfeatures\n\ntest_data.dropna(subset=['Embarked'],inplace=True)\ntest_data.groupby(['Embarked']).size()\n\ntest_data['AgeGroup'] = test_data['AgeGroup'].apply(lambda x: np.nan if x == -1 else x)\ntest_data['FareRange'] = test_data['FareRange'].apply(lambda x: np.nan if x == -1 else x)\n\n\nimputer = KNNImputer(n_neighbors= 5)\nAgeGroupImputed = imputer.fit_transform([test_data['AgeGroup']])\nFareRangeImputed = imputer.fit_transform([test_data['FareRange']])\nFareRangeImputed = pd.Series(FareRangeImputed[0])\nAgeGroupImputed = pd.Series(AgeGroupImputed[0])\ntest_data['FareRange'] = FareRangeImputed\ntest_data['AgeGroup'] = AgeGroupImputed\n\ntest_data.head(20)\n\nsex = pd.get_dummies(test_data['Sex'],drop_first=True)\nembark = pd.get_dummies(test_data['Embarked'],drop_first=True)\ntitle = pd.get_dummies(test_data['Title'],drop_first=True)\nfare = pd.get_dummies(test_data['FareRange'],drop_first=True)\ncabin = pd.get_dummies(test_data['Cabin_Group'],drop_first=True)\nage = pd.get_dummies(test_data['AgeGroup'],drop_first=True)\n\ntest_data.drop(['AgeGroup','Sex',\"Embarked\",'Title','Cabin_Group','FareRange'],axis=1,inplace=True)\n\ntest_data = pd.concat([test_data,sex,embark,title,cabin,fare,age],axis=1)\ntest_data.head(5)\n\n\n","2256a66c":"#blender\nBlend_Y = list()\ny1 = RandomForestModel2.predict(test_data)\ny1 = y1.reshape(len(y1), 1)\nBlend_Y.append(y1)\n\ny2 = LGBMModel2.predict(test_data)\ny2 = y2.reshape(len(y2), 1)\nBlend_Y.append(y2)\n\ny3 = XGBModel2.predict(test_data)\ny3 = y3.reshape(len(y3), 1)\nBlend_Y.append(y3)\n\ny4 = AdaBoostModel2.predict(test_data)\ny4 = y4.reshape(len(y4), 1)\nBlend_Y.append(y4)\n\n\nBlend_Y = hstack(Blend_Y)\n\ny_pred = BlenderModel.predict(Blend_Y)","5474e8b6":"passengerId = test_data_original.iloc[:,0]\npassengerId","693ab8e8":"submission = pd.DataFrame({'PassengerId':passengerId,'Survived':y_pred})\nsubmission.to_csv('submission.csv',index=False)","56ef103d":"**FARE**\n\nI'm dividing the fare to 5 buckets or ranges, which I'd be using later for my model.","4a998ee8":"**Fine Tuning AdaBoost**","18855293":"# Blending Ensemble Method","d29701ad":"I'll replace the new columns with the one hot encoded values and append them to my dataframe.","51996353":"**Fine tuning Random Forest**","02cfdaf6":"**Age**\n\nCreating a similar range for age as well, like Fare and assigning -1 to Null values to be handled later.","bc7586f4":"**Age and Fare**\n\nHere, I'd be using KNN to predict te missing values (which I've classified as -1 in previous function). For this task, I'd be using KNNImputer.","2c15e565":"# Stacking (Voting) ensemble","d62a812f":"We can replace some of the titles (such as Ms -> Miss) and the group the ones which appear lesser compared to others in a new group. We'll then check the mean number of records of each title's relation to the output.","d7732024":"> **Getting the information of the number of records per columns with null\/NaN value**","025344e6":"**Predict**\n\nUsing the model with best  accuracy for prediction","8e556a1c":"> **Get information about the data like count, mean, standard deviation etc.**","1a738f6c":"# Model Building","a70b1a8b":"Let's check the number of records for each of these titles.","fb96e347":"> **Checking the impact of each feature on target variable**","e6afb0ef":"# WORKING WITH TEST DATA","1b06c6bb":"Checking the relation between this new feature with target.","d058d40f":"In my [previous notebook](https:\/\/www.kaggle.com\/arunima24\/exploratory-data-analysis-on-titanic-dataset), I've done an EDA and Feature Engineering on the titanic dataset to understand the data pattern and relationship. Here, I would be continuing on the same pattern to use the different ensemble techniques to predict the accuracy.","e229bfd6":"I'll update my features array with these new features and dropping the old ones.","b27aa09e":"**Fine tuning XGBoost**","cbdec7b8":"# Reading Dataset","ec3c9c2e":"**CABIN**\n\nNext, I'm starting with the Cabin feature. For this, I'll assign -1 to NaN\/null vales (and deal with them later) and group the others using the first letter of the values and create a new column 'Cabin_Group'.","7cd5baa8":"# FEATURE ENGINEERING\nWe see from the above table that some columns have a large amount of similar data or contain NaN values which would be difficult to be used in a model if no changes are made to them. For example, \n\n**PassengerID**: The ID has no impact on survival, so we might ignore the column altogether.\n\n**Cabin**: The Cabin can be grouped according to the classes, and we can try to derive a pattern from it.\n\n**Fare**: There are 200+ different fares that the passengers have paid. Instead of analysing them individually for a prediction, we can use them as a range.\n\n**Ticket**: The Ticket has no impact on survival, so we might ignore the column altogether.\n\n**Age**: Again, like Fare, we need to treat them as age group instead of considering individual ages \n\n**Name**: The names are not important, but the salutation is. We can use that in addition to the Sex and Age feature for prediction.","bad806ca":"**Fine tuning LGBM Classifier**","1f442bc8":"> **Creating an array using the columns which would be used for the various graphs for EDA**","e24fe575":"If you like it, please upvote. Also let me know in the comments, if my strategy is incorrect somewhere or you have a better solution for any part or any suggestions on how I should improve the accuracy of my models.","783ec9e4":"# Importing Libraries","5b56ee11":"# Fine Tuning\n\nOnce I've tested with the common ensemble techniques, I'd now fine tune the hyperparamters and test what impact the fine tuning will have on the final prediction.","ec426ebf":"# Splitting data to train and test dataset","a9accf6b":"# Handling Null Values\n\nWe have null values in Age, Cabin and Embarked features. I'll try different methods o handling null values for these features.","1481429a":"**Embarked**\n\nSince we have only 2 null records for Embarked feature, I'm going to save some time and just drop them.","2c89aebd":"**NAME**\n\nLet's look at Name feature. My idea is to get the salutation from each name and store it as a new column 'Title' in the dataframe."}}