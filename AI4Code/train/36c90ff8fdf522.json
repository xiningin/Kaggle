{"cell_type":{"134df0ff":"code","7a33d27b":"code","dcbf5ca9":"code","c47057b2":"code","b44ec15f":"code","8e297a03":"code","6e2f193b":"code","6bfa4dfa":"code","0de0f3b5":"code","588da997":"code","a5b40f24":"code","504486ba":"code","f4b92fff":"code","4f69c55f":"code","3322563a":"code","65d90e5d":"code","9fc530aa":"code","39653c9c":"code","b1124e40":"code","5db99279":"code","a92c09fc":"code","9e68c059":"code","d73d24a7":"code","188fb824":"code","c0ba7237":"code","017fe032":"code","9fd8792e":"code","6ee0e6bb":"code","256e02d0":"code","56a274b2":"code","d25607ea":"code","0ae6737e":"code","581fc4aa":"code","b7f23eeb":"code","8adad60b":"code","c828ebbd":"code","d6d10b27":"code","b7ba7d8a":"code","1e837094":"code","de127c0e":"code","a88af236":"code","e823beb7":"code","b294fe1e":"code","ae4de4a2":"code","a9114d6e":"code","4600ea24":"code","0c022639":"code","2966a1d1":"code","4b3f2c96":"code","3fb34362":"code","46c80a48":"code","f1b4baa2":"code","f7f9e90c":"code","8e6078e6":"code","7e085473":"code","59db42f1":"code","308719ab":"code","167ceec1":"code","161d36dd":"code","26b226bd":"code","58dae7aa":"code","4db66583":"code","22ac7a5b":"code","97e30bd0":"code","4349943c":"code","d2cf85ba":"code","39f1ee20":"code","ed9aafc5":"code","7171a35d":"code","68553d41":"code","7adf6311":"code","cc2a593d":"code","ae661ce5":"code","679136f1":"code","df5b65dc":"code","cd6815c9":"code","cafe3ea9":"code","44981e73":"code","debda328":"code","9941056f":"code","2079af11":"code","bd44623d":"code","93fd739d":"code","6b70e697":"code","2b9f37df":"code","743b365a":"code","2e6d7eb4":"code","fa19d23f":"code","fcaae643":"code","126a2b10":"markdown","0541ea16":"markdown","361790f8":"markdown","9c867c45":"markdown","06978184":"markdown","25301f23":"markdown","cc405317":"markdown","c0c6811f":"markdown","b0b5cb1f":"markdown","78a91d25":"markdown","fa04528f":"markdown","95c0ef00":"markdown","07e10823":"markdown","b0eaacf7":"markdown","99e4da9d":"markdown","43643cdc":"markdown","5e102129":"markdown","fa325b67":"markdown","d98129b1":"markdown","e7355952":"markdown","f702767b":"markdown","e8451f58":"markdown","a3ddb80b":"markdown","1dffd84e":"markdown","5d77c081":"markdown","b1a06011":"markdown","a0e47748":"markdown","55d5f3f2":"markdown","3179a074":"markdown","a72bea0d":"markdown","da43fe3e":"markdown","1f2ebf07":"markdown"},"source":{"134df0ff":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7a33d27b":"import numpy as np \nimport pandas as pd \nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom colorama import Fore\n\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nimport math\n\nimport warnings  \nwarnings.filterwarnings('ignore')\n\nnp.random.seed(7)","dcbf5ca9":"df = pd.read_csv('..\/input\/aceawaterprediction\/Aquifer_Petrignano.csv')\ndf.head()","c47057b2":"df.shape","b44ec15f":"df.isnull().sum()","8e297a03":"df = df[df.Rainfall_Bastia_Umbra.notna()]\ndf.head()","6e2f193b":"df = df.reset_index(drop=True)\ndf = df.drop(['Depth_to_Groundwater_P24', 'Temperature_Petrignano'], axis=1)","6bfa4dfa":"df.head()","0de0f3b5":"# Simplifying the column names\ndf.columns = ['date', 'rainfall', 'depth_to_groundwater', 'temperature', 'drainage_volume', 'river_hydrometry']\n# Separating the target column\ntargets = ['depth_to_groundwater'] \nfeatures = [feature for feature in df.columns if feature not in targets]\ndf.head()","588da997":"print(np.min(df['date']))\nprint(np.max(df['date']))","a5b40f24":"from datetime import datetime, date\n\ndf['date'] = pd.to_datetime(df['date'], format='%d\/%m\/%Y')\ndf.head().style.set_properties(subset=['date'], **{'background-color': 'lightblue'})","504486ba":"f, ax = plt.subplots(nrows = 5, ncols = 1, figsize=(15, 25))\nfor i, column in enumerate(df.drop('date', axis=1).columns):\n    sns.lineplot(x=df['date'], y=df[column].fillna(method='ffill'), ax=ax[i], color='royalblue')\n    ax[i].set_title(f'Feature: {column}', fontsize=15)\n    ax[i].set_ylabel(column, fontsize=12)\n    ax[i].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])","f4b92fff":"# Chronological order\ndf = df.sort_values(by='date')","4f69c55f":"df['difference'] = df['date'] - df['date'].shift(1)\ndf[['date', 'difference']].head()","3322563a":"# We note that the values are equal - equidistant time stamps\ndf['difference'].sum(), df['difference'].count()","65d90e5d":"df.drop('difference', axis=1)\ndf.isnull().sum()","9fc530aa":"df[df['drainage_volume']==0]","39653c9c":"df[df['drainage_volume']==0]","b1124e40":"np.max(df['drainage_volume'])","5db99279":"np.inf","a92c09fc":"copied = df['river_hydrometry'].copy()\ndf['river_hydrometry'] = df['river_hydrometry'].replace(0, np.nan)\n\ncopied2 = df['drainage_volume'].copy()\ndf['drainage_volume'] = df['drainage_volume'].replace(0, np.nan)","9e68c059":"f, ax = plt.subplots(nrows=2, ncols=1, figsize=(15, 15))\n\n# We are showcasing the presence of zero-values in both the features\n\nsns.lineplot(x=df['date'], y=copied, ax=ax[0], color='darkorange', label='original')\nsns.lineplot(x=df['date'], y=df['river_hydrometry'].fillna(np.inf), ax=ax[0], color='dodgerblue', label='modified')\nax[0].set_title('Feature: Hydrometry', fontsize=14)\nax[0].set_ylabel(ylabel='Hydrometry', fontsize=14)\nax[0].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\n\n\nsns.lineplot(x=df['date'], y=copied2, ax=ax[1], color='darkorange', label='original')\nsns.lineplot(x=df['date'], y=df['drainage_volume'].fillna(np.inf), ax=ax[1], color='dodgerblue', label='modified')\nax[0].set_title('Feature: Drainage', fontsize=14)\nax[0].set_ylabel(ylabel='Drainage', fontsize=14)\nax[0].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])","d73d24a7":"df.T.loc[:,2300:2340].isna()","188fb824":"f, ax = plt.subplots(nrows=1, ncols=1, figsize=(16, 5))\n\nsns.heatmap(df.T.isna(), cmap='Blues')\nax.set_title('Missing Values', fontsize=16)\n\nfor tick in ax.yaxis.get_major_ticks():\n    tick.label.set_fontsize(15)\n    \nplt.show()","c0ba7237":"f, ax = plt.subplots(nrows=4, ncols=1, figsize = (15, 12))\n\nsns.lineplot(x=df['date'], y=df['drainage_volume'].fillna(0), ax=ax[0], color='darkorange')\nsns.lineplot(x=df['date'], y=df['drainage_volume'].fillna(np.inf), ax=ax[0], color='royalblue')\nax[0].set_title('Fill NULL with 0')\nax[0].set_ylabel(ylabel='Drainage Volume')\n\nmean = df['drainage_volume'].mean()\nsns.lineplot(x=df['date'], y=df['drainage_volume'].fillna(mean), ax=ax[1], color='darkorange')\nsns.lineplot(x=df['date'], y=df['drainage_volume'].fillna(np.inf), ax=ax[1], color='royalblue')\nax[0].set_title('Fill NULL with Mean')\nax[0].set_ylabel(ylabel='Drainage Volume')\n\nsns.lineplot(x=df['date'], y=df['drainage_volume'].ffill(), ax=ax[2], color='darkorange')\nsns.lineplot(x=df['date'], y=df['drainage_volume'].fillna(np.inf), ax=ax[2], color='royalblue')\nax[0].set_title('Fill NULL with Ffill')\nax[0].set_ylabel(ylabel='Drainage Volume')\n\nsns.lineplot(x=df['date'], y=df['drainage_volume'].interpolate(), ax=ax[3], color='darkorange')\nsns.lineplot(x=df['date'], y=df['drainage_volume'].fillna(np.inf), ax=ax[3], color='royalblue')\nax[0].set_title('Fill NULL with Interpolate')\nax[0].set_ylabel(ylabel='Drainage Volume')\n\n\nfor i in range(4):\n    ax[i].set_xlim([date(2019, 5, 1), date(2019, 10, 1)])","017fe032":"df['drainage_volume'] = df['drainage_volume'].interpolate()\ndf['river_hydrometry'] = df['river_hydrometry'].interpolate()\ndf['depth_to_groundwater'] = df['depth_to_groundwater'].interpolate()","9fd8792e":"fig, ax = plt.subplots(ncols=2, nrows=3, sharex=True, figsize=(16,12))\nsns.lineplot(df['date'], df['drainage_volume'], color='royalblue', ax=ax[0, 0])\nax[0, 0].set_title('Daily Drainage volume')\n\nresampling = df[['date', 'drainage_volume']].resample('7D', on='date').sum().reset_index(drop=False)\nsns.lineplot(resampling['date'], resampling['drainage_volume'], color='royalblue', ax=ax[1, 0])\nax[1, 0].set_title('Weekly Drainage volume')\n\nresampling = df[['date', 'drainage_volume']].resample('M', on='date').sum().reset_index(drop=False)\nsns.lineplot(resampling['date'], resampling['drainage_volume'], color='royalblue', ax=ax[2, 0])\nax[2, 0].set_title('Monthly Drainage volume')\n\nfor i in range(3):\n    ax[i, 0].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\n\nsns.lineplot(df['date'], df['temperature'], color='royalblue', ax=ax[0, 1])\nax[0, 1].set_title('Daily temperature')\n\nresampling = df[['date', 'temperature']].resample('7D', on='date').sum().reset_index(drop=False)\nsns.lineplot(resampling['date'], resampling['temperature'], color='royalblue', ax=ax[1, 1])\nax[1, 1].set_title('Weekly temperature')\n\nresampling = df[['date', 'temperature']].resample('M', on='date').sum().reset_index(drop=False)\nsns.lineplot(resampling['date'], resampling['temperature'], color='royalblue', ax=ax[2, 1])\nax[2, 1].set_title('Monthly temperature')\\\n\nfor i in range(3):\n    ax[i, 1].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\nplt.show()","6ee0e6bb":"# We can see that weekly graphs are quite smoothened out, we can make use of them\n\ndownsample = df[['date','depth_to_groundwater', 'temperature','drainage_volume', 'river_hydrometry','rainfall']].resample('7D', on='date').mean().reset_index(drop=False)\ndf = downsample.copy()","256e02d0":"window_size = 52 # Our data is in weekly granularity and 52 weeks - 1 year\nf, ax = plt.subplots(nrows=2, ncols = 1, figsize=(15, 12))\n\n# The first year values will be NULL as we require 52 previous observations to calculate\nsns.lineplot(x=df['date'], y=df['drainage_volume'], ax=ax[0], color='royalblue')\nsns.lineplot(x=df['date'], y=df['drainage_volume'].rolling(window_size).mean(), ax=ax[0], color='black')\nsns.lineplot(x=df['date'], y=df['drainage_volume'].rolling(window_size).std(), ax=ax[0], color='orange')\nax[0].set_title('Ground water - Non stationary \\nNon constant mean and non constant variance')\nax[0].set_ylabel('Drainage Volume')\nax[0].set_xlim()\n\nsns.lineplot(x=df['date'], y=df['temperature'], ax=ax[1], color='royalblue')\nsns.lineplot(x=df['date'], y=df['temperature'].rolling(window_size).mean(), ax=ax[1], color='black')\nsns.lineplot(x=df['date'], y=df['temperature'].rolling(window_size).std(), ax=ax[1], color='orange')\nax[1].set_title('Temperature - Non stationary \\nVariance is time-dependent (seasonality)')\nax[1].set_ylabel('Temperature')\nax[1].set_xlim()","56a274b2":"from statsmodels.tsa.stattools import adfuller\n\nres = adfuller(df['depth_to_groundwater'].values)\nres","d25607ea":"df.head()","0ae6737e":"f, ax = plt.subplots(nrows=3, ncols=2, figsize=(15, 9))\n\ndef adfullers(series, title, ax):\n    res = adfuller(series)\n    significance = 0.05\n    adf_res = res[0]\n    p = res[1]\n    crit_1 = res[4]['1%']\n    crit_5 = res[4]['5%']\n    crit_10 = res[4]['10%']\n    \n    # This will let us know at what significance level is our data stationary\n    if (p<significance) & (adf_res<crit_1):\n        linecolor='green'\n    elif (p<significance) & (adf_res<crit_5):\n        linecolor='orange'\n    elif (p<significance) & (adf_res<crit_10):\n        linecolor='red'\n    else:\n        linecolor='purple'\n    sns.lineplot(x=df['date'], y=series, ax=ax, color=linecolor)\n    ax.set_title(f'ADF statistic {adf_res}, p-value: {p:0.3f}\\n Critical value 1% {crit_1:0.3f} Critical value 5% {crit_5:0.3f} Critical value 10% {crit_10:0.3f}')\n    ax.set_ylabel(title)\n    \nadfullers(df['rainfall'].values, 'Rainfall', ax[0, 0])\nadfullers(df['temperature'].values, 'Temperature', ax[1, 0])\nadfullers(df['river_hydrometry'].values, 'River_Hydrometry', ax[0, 1])\nadfullers(df['drainage_volume'].values, 'Drainage_Volume', ax[1, 1])\nadfullers(df['depth_to_groundwater'].values, 'Depth_to_Groundwater', ax[2, 0])\n\nf.delaxes(ax[2, 1])\nplt.tight_layout()\nplt.show()","581fc4aa":"# Log transforms\ndf['depth_to_groundwater_log'] = np.log(abs(df['depth_to_groundwater']))\nf, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 6))\nadfullers(df['depth_to_groundwater_log'], 'Transformed Depth to groundwater', ax[0])\nsns.distplot(df['depth_to_groundwater_log'], ax=ax[1])","b7f23eeb":"# First order diferencing\ndiff = np.diff(df['depth_to_groundwater'])\n# As first value is NULL, we have to add 0 to make it equal length\ndf['depth_to_groundwater_diff_1'] = np.append([0], diff)\ndf['depth_to_groundwater_diff_1']","8adad60b":"f, ax=plt.subplots(nrows=1, ncols=1, figsize=(15, 6))\nadfullers(df['depth_to_groundwater_diff_1'], 'Difference\\n Depth to groundwater', ax)","c828ebbd":"df['year'] = pd.DatetimeIndex(df['date']).year\ndf['month'] = pd.DatetimeIndex(df['date']).month\ndf['day'] = pd.DatetimeIndex(df['date']).day\ndf['day_of_year'] = pd.DatetimeIndex(df['date']).dayofyear\ndf['week_of_year'] = pd.DatetimeIndex(df['date']).weekofyear\ndf['quarter'] = pd.DatetimeIndex(df['date']).quarter\ndf['season'] = df['month'] % 12 \/\/ 3 + 1\ndf[['date', 'year', 'month', 'day', 'day_of_year', 'week_of_year', 'quarter', 'season']].head()","d6d10b27":"f, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 3))\n\nsns.lineplot(x=df['date'], y=df['month'], color='royalblue')\nax.set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\nplt.show()","b7ba7d8a":"# Convert them into cyclic values\ndf['month_sin'] = np.sin(2*np.pi*df['month']\/12)\ndf['month_cos'] = np.cos(2*np.pi*df['month']\/12)\n\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(6,6))\nsns.scatterplot(x=df.month_sin, y=df.month_cos, color='royalblue')\nplt.show()","1e837094":"from statsmodels.tsa.seasonal import seasonal_decompose\n\ncore_columns =  [\n    'rainfall', 'temperature', 'drainage_volume', \n    'river_hydrometry', 'depth_to_groundwater'\n]\n\nfor column in core_columns:\n    decomp = seasonal_decompose(df[column], period=52, model='additive', extrapolate_trend='freq')\n    df[f\"{column}_trend\"] = decomp.trend\n    df[f\"{column}_seasonal\"] = decomp.seasonal","de127c0e":"fig, ax = plt.subplots(ncols=2, nrows=4, sharex=True, figsize=(16, 8))\n\nfor i, column in enumerate(['temperature', 'depth_to_groundwater']):\n    res = seasonal_decompose(df[column], freq=52, model='additive', extrapolate_trend='freq')\n    ax[0,i].set_title('Decomposition of {}'.format(column), fontsize=16)\n    res.observed.plot(ax=ax[0,i], legend=False, color='dodgerblue')\n    ax[0,i].set_ylabel('Observed', fontsize=14)\n    res.trend.plot(ax=ax[1,i], legend=False, color='dodgerblue')\n    ax[1,i].set_ylabel('Trend', fontsize=14)\n    res.seasonal.plot(ax=ax[2,i], legend=False, color='dodgerblue')\n    ax[2,i].set_ylabel('Seasonal', fontsize=14)\n    res.resid.plot(ax=ax[3,i], legend=False, color='dodgerblue')\n    ax[3,i].set_ylabel('Residual', fontsize=14)\n\nplt.show()","a88af236":"weeks_in_month = 4\n\nfor column in core_columns:\n    df[f'{column}_seasonal_shift_b_2m'] = df[f'{column}_seasonal'].shift(-2 * weeks_in_month)\n    df[f'{column}_seasonal_shift_b_1m'] = df[f'{column}_seasonal'].shift(-1 * weeks_in_month)\n    df[f'{column}_seasonal_shift_1m'] = df[f'{column}_seasonal'].shift(1 * weeks_in_month)\n    df[f'{column}_seasonal_shift_2m'] = df[f'{column}_seasonal'].shift(2 * weeks_in_month)\n    df[f'{column}_seasonal_shift_3m'] = df[f'{column}_seasonal'].shift(3 * weeks_in_month)","e823beb7":"f, ax = plt.subplots(nrows=5, ncols=1, figsize=(15, 12))\nf.suptitle('Seasonal Components of Features', fontsize=16)\n\nfor i, column in enumerate(core_columns):\n    sns.lineplot(x=df['date'], y=df[column + '_seasonal'], ax=ax[i], color='royalblue', label='P25')\n    ax[i].set_ylabel(ylabel=column, fontsize=14)\n    ax[i].set_xlim([date(2017, 9, 30), date(2020, 6, 30)])\n    \nplt.tight_layout()\nplt.show()","b294fe1e":"f, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))\n\ncorrmat = df[core_columns].corr()\n\nsns.heatmap(corrmat, annot=True, vmin=-1, vmax=1, cmap='coolwarm_r', ax=ax[0])\nax[0].set_title('Correlation Matrix of Core features', fontsize=16)\n\nshifted_cols = [\n    'depth_to_groundwater_seasonal',\n    'temperature_seasonal_shift_b_2m',\n    'drainage_volume_seasonal_shift_2m',\n    'river_hydrometry_seasonal_shift_3m'\n]\n\ncorrmat = df[shifted_cols].corr()\nsns.heatmap(corrmat, annot=True, vmin=-1, vmax=1, cmap='coolwarm_r', ax=ax[1])\nax[1].set_title('Correlation Matrix of shifted features', fontsize=16)\nplt.tight_layout()\nplt.show()","ae4de4a2":"from pandas.plotting import autocorrelation_plot\n\nautocorrelation_plot(df['depth_to_groundwater_diff_1'])\nplt.show()","a9114d6e":"from statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\nf, ax = plt.subplots(nrows=2, ncols=1, figsize=(16, 8))\n\nplot_acf(df['depth_to_groundwater_diff_1'], lags=100, ax=ax[0])\nplot_pacf(df['depth_to_groundwater_diff_1'], lags=100, ax=ax[1])\n\nplt.show()","4600ea24":"from sklearn.model_selection import TimeSeriesSplit\nX=df['date']\ny=df['depth_to_groundwater']\nfolds = TimeSeriesSplit(n_splits=3)","0c022639":"f, ax = plt.subplots(nrows=3, ncols=2, figsize=(16, 9))\nfor i, (train_index, valid_index) in enumerate(folds.split(X)):\n    X_train, X_valid = X[train_index], X[valid_index]\n    y_train, y_valid = y[train_index], y[valid_index]\n    \n    sns.lineplot(X_train, y_train, ax=ax[i, 0], color='royalblue', label='train')\n    sns.lineplot(x=X_train[(len(X_train) - len(X_valid)):len(X_train)], y=y_train[(len(X_train)-len(X_valid)):len(X_train)], ax=ax[i,1], color='royalblue', label='train')\n    for j in range(2):\n        sns.lineplot(x=X_valid, y=y_valid, ax=ax[i, j], color='darkorange', label='validation')\n    ax[i, 0].set_title(f\"Rolling Window with Adjusting Training Size (Split {i+1})\", fontsize=16)\n    ax[i, 1].set_title(f\"Rolling Window with Constant Training Size (Split {i+1})\", fontsize=16)\n    \nfor i in range(3):\n    ax[i, 0].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\n    ax[i, 1].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\n    \nplt.tight_layout()\nplt.show()","2966a1d1":"training_size = int(0.85*len(df))\ntest_size = len(df)-training_size\n\nsingle_var = df[['date', 'depth_to_groundwater']].copy()\nsingle_var.columns=['ds','y']\n\ntraining_set = single_var.iloc[:training_size, :]\n# For univariate, we have to rename column\ncleaned = training_set.copy()\ncleaned.rename(columns={'time_series':'ds','variable':'y'}, inplace=True)\nx_train, y_train = pd.DataFrame(single_var.iloc[:training_size, 0]), pd.DataFrame(single_var.iloc[:training_size, 1])\nx_valid, y_valid = pd.DataFrame(single_var.iloc[training_size:, 0]), pd.DataFrame(single_var.iloc[training_size:, 1])","4b3f2c96":"from sklearn.metrics import mean_absolute_error, mean_squared_error\nimport math\n\nfrom fbprophet import Prophet\n\nmodel = Prophet()\nmodel.fit(cleaned)","3fb34362":"y_pred = model.predict(x_valid)\nscore_mae = mean_absolute_error(y_valid, y_pred.tail(test_size)['yhat'])\nscore_rmse = math.sqrt(mean_squared_error(y_valid, y_pred.tail(test_size)['yhat']))","46c80a48":"Fore","f1b4baa2":"print(Fore.GREEN + 'RMSE: {}'.format(score_rmse))","f7f9e90c":"f, ax = plt.subplots(1, figsize=(15, 10))\n\nmodel.plot(y_pred, ax=ax)\nsns.lineplot(x=x_valid['ds'], y=y_valid['y'], ax=ax, color='orange', label='Observed')\nax.set_title(f'Prediction \\n MAE: {score_mae:.2f}, RMSE: {score_rmse:.2f}', fontsize=14)\nax.set_xlabel('Date')\nax.set_ylabel('Depth to Groundwater')\nplt.show()","8e6078e6":"from statsmodels.tsa.arima_model import ARIMA\n\nmodel = ARIMA(y_train, order=(1,1,1))\nmodel_fit = model.fit()\ny_pred, se, conf = model_fit.forecast(90)\n\nscore_mae = mean_absolute_error(y_valid, y_pred)\nscore_rmse = math.sqrt(mean_squared_error(y_valid, y_pred))\n\nprint(Fore.GREEN + 'RMSE: {}'.format(score_rmse))","7e085473":"len(y_train)","59db42f1":"len(y_valid)","308719ab":"# The model will start predicting from 510+ onwards\n# We will plot the validation data (which succeeds the current data) to check\nf, ax = plt.subplots(1, figsize=(15, 10))\n\nmodel_fit.plot_predict(1, 600, ax=ax)\nsns.lineplot(x=x_valid.index, y=y_valid['y'], ax=ax, color='black', label='Ground truth')\nax.set_title(f'Prediction \\n MAE: {score_mae:.2f}, RMSE: {score_rmse:.2f}', fontsize=14)\nax.set_xlabel('Date')\nax.set_ylabel('Depth to Groundwater')\nplt.show()","167ceec1":"# The model will start predicting from 510+ onwards\n# We will plot the validation data (which succeeds the current data) to check\nf, ax = plt.subplots(1, figsize=(15, 10))\nsns.lineplot(x=x_valid.index, y=y_pred, ax=ax, color='red', label='predicted')\nsns.lineplot(x=x_valid.index, y=y_valid['y'], ax=ax, color='black', label='Ground truth')\n\nax.set_xlabel('Date')\nax.set_ylabel('Depth to Groundwater')","161d36dd":"!pip install pmdarima","26b226bd":"from statsmodels.tsa.arima_model import ARIMA\nimport pmdarima as pm\n\nmodel = pm.auto_arima(y_train, start_p=1, start_q=1, test='adf', max_p=3, max_q=3, m=1, d=None, seasonal=False, start_P=0, D=0, trace=True, error_action='ignore', suppress_warnings=True, stepwise=True)\nprint(model.summary())","58dae7aa":"model.plot_diagnostics(figsize=(16,8))\nplt.show()","4db66583":"from sklearn.preprocessing import MinMaxScaler\n\ndf_rnn = single_var['y']\ndf_rnn","22ac7a5b":"df_rnn.shape","97e30bd0":"# We use filter here as we require a 2D array for the scaler \ntesting = single_var.filter('y')\ntesting.shape","4349943c":"labels = testing.values\nscaler = MinMaxScaler(feature_range=(-1, 0))\nlabels_scaled = scaler.fit_transform(labels)\n\nlabels_scaled[:20]","d2cf85ba":"labels_scaled.shape","39f1ee20":"training_size","ed9aafc5":"rolling_window = 52\ntrain, test = labels_scaled[:training_size-rolling_window, :], labels_scaled[training_size-rolling_window:, :]","7171a35d":"train.shape","68553d41":"test.shape","7adf6311":"def create_dataset(dataset, look_back=1):\n    X, Y = [], []\n    # start from 52\n    # Take 0:51, append it to X\n    # Take the y-value at 52, append it to y\n    for i in range(look_back, len(dataset)):\n        a = dataset[i-look_back:i, 0]\n        X.append(a)\n        Y.append(dataset[i, 0])\n    return np.array(X), np.array(Y)\n\nx_train, y_train = create_dataset(train, rolling_window)\nx_test, y_test = create_dataset(test, rolling_window)","cc2a593d":"x_train.shape","ae661ce5":"x_train[0]","679136f1":"y_train.shape","df5b65dc":"# Reshape the data as the input expects - [samples, time steps\/sequence, features]\n# The data will be passed in as a sequence\n# If we are stacking LSTMs, we need to return sequences\nx_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\nx_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n\nprint(len(x_train), len(x_test))","cd6815c9":"x_train[3]","cafe3ea9":"x_train.shape","44981e73":"y_train.shape","debda328":"from keras.models import Sequential\nfrom keras.layers import Dense, LSTM \n\nmodel = Sequential()\nmodel.add(LSTM(128, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))\nmodel.add(LSTM(64, return_sequences=False))\nmodel.add(Dense(24))\nmodel.add(Dense(1))\n\nmodel.compile(optimizer='adam', loss='mse')","9941056f":"model.fit(x_train, y_train, batch_size=1, epochs=5, validation_data=(x_test, y_test))\nmodel.summary()","2079af11":"x_train","bd44623d":"# Lets predict with the model\n# We are passing in sequences and getting an output for each\ntrain_predict = model.predict(x_train)\ntest_predict = model.predict(x_test)","93fd739d":"train_predict.shape","6b70e697":"# Re-scale the predictions back\ntrain_predict = scaler.inverse_transform(train_predict)\ny_train = scaler.inverse_transform([y_train])\n\ntest_predict = scaler.inverse_transform(test_predict)\ny_test = scaler.inverse_transform([y_test])\n\n# RSME and MAE error\nscore_rmse = np.sqrt(mean_squared_error(y_test[0], test_predict[:,0]))\nscore_mae = mean_absolute_error(y_test[0], test_predict[:,0])\nprint(Fore.GREEN + 'RMSE: {}'.format(score_rmse))","2b9f37df":"x_train_ticks = single_var.head(training_size)['ds']\ny_train = single_var.head(training_size)['y']\nx_test_ticks = single_var.tail(test_size)['ds']\n\n# Plot the forecast\nf, ax = plt.subplots(1)\nf.set_figheight(6)\nf.set_figwidth(15)\n\nsns.lineplot(x=x_train_ticks, y=y_train, ax=ax, label='Train Set') #navajowhite\nsns.lineplot(x=x_test_ticks, y=test_predict[:,0], ax=ax, color='green', label='Prediction') #navajowhite\nsns.lineplot(x=x_test_ticks, y=y_test[0], ax=ax, color='orange', label='Ground truth') #navajowhite\n\nax.set_title(f'Prediction \\n MAE: {score_mae:.2f}, RMSE: {score_rmse:.2f}', fontsize=14)\nax.set_xlabel(xlabel='Date', fontsize=14)\nax.set_ylabel(ylabel='Depth to Groundwater', fontsize=14)\n\nplt.show()","743b365a":"feature_columns = [\n    'rainfall',\n    'temperature',\n    'drainage_volume',\n    'river_hydrometry',\n]\ntarget_column = ['depth_to_groundwater']\n\ntrain_size = int(0.85 * len(df))\n\nmultivariate_df = df[['date'] + target_column + feature_columns].copy()\nmultivariate_df.columns = ['ds', 'y'] + feature_columns\n\ntrain = multivariate_df.iloc[:train_size, :]\n# Split the feature columns and label column\nx_train, y_train = pd.DataFrame(multivariate_df.iloc[:train_size, [0,2,3,4,5]]), pd.DataFrame(multivariate_df.iloc[:train_size, 1])\nx_valid, y_valid = pd.DataFrame(multivariate_df.iloc[train_size:, [0,2,3,4,5]]), pd.DataFrame(multivariate_df.iloc[train_size:, 1])\n\ntrain.head()","2e6d7eb4":"x_train.head()","fa19d23f":"from fbprophet import Prophet\n\n\n# Train the model\nmodel = Prophet()\nmodel.add_regressor('rainfall')\nmodel.add_regressor('temperature')\nmodel.add_regressor('drainage_volume')\nmodel.add_regressor('river_hydrometry')\n\nmodel.fit(train)\ny_pred = model.predict(x_valid)\n\n# Calcuate metrics\nscore_mae = mean_absolute_error(y_valid, y_pred['yhat'])\nscore_rmse = math.sqrt(mean_squared_error(y_valid, y_pred['yhat']))\n\nprint(Fore.GREEN + 'RMSE: {}'.format(score_rmse))","fcaae643":"# Plot the forecast\nf, ax = plt.subplots(1)\nf.set_figheight(6)\nf.set_figwidth(15)\n\nmodel.plot(y_pred, ax=ax)\nsns.lineplot(x=x_valid['ds'], y=y_valid['y'], ax=ax, color='orange', label='Ground truth') #navajowhite\n\nax.set_title(f'Prediction \\n MAE: {score_mae:.2f}, RMSE: {score_rmse:.2f}', fontsize=14)\nax.set_xlabel(xlabel='Date', fontsize=14)\nax.set_ylabel(ylabel='Depth to Groundwater', fontsize=14)\n\nplt.show()","126a2b10":"If the data is not stationary but we want to use a model such as ARIMA (that requires this characteristic), the data has to be transformed. The two most common methods to transform series into stationarity ones are:\n\n* Transformation: e.g. log or square root to stabilize non-constant variance\n* Differencing: subtracts the current value from the previous","0541ea16":"Differencing can be done in different orders:\n* First order differencing: linear trends with $z_i = y_i - y_{i-1}$\n* Second-order differencing: quadratic trends with $z_i = (y_i - y_{i-1}) - (y_{i-1} - y_{i-2})$\n* and so on...","361790f8":"The insights from the given plots\n\n* Standarized residual - The residual error fluctutates around a mean of zero and has a uniform variance between (-4, 4)\n\n* Histogram plus estimated density - The plot suggests normal distribution with mean zero\n\n* Normal Q-Q - The blue dots are over the red line for the most part, suggests low skewing\n\n* Correlogram - The ACF plot shows the residual errors are not autocorrelated","9c867c45":"<h2>RNNs and LSTMs<\/h2>\n\nWe will create a multi-layered LSTM model to forecast our given model. We will go through the following steps:\n\n* Creation of dataset\n* Feature normalization\n* Splitting data\n* Reshaping and cleaning\n* Model creation and training\n* Predicting","06978184":"Our time series can be of two forms - Univariate or multivariate\n* Univariate - single time-dependent variable\n* Multivariate - multiple time-dependent variables\n\nWe will see how to do cross-validation using time-series data","25301f23":"<h1>Introduction<\/h1>\n\nThis notebook will dive deep into timeseries analysis, the pre-processing and cleaning methodologies required. We will also look at statistical models for forecasting future values.","cc405317":"Our sequences in the train set are in the format:\n\n1st row - [0, 1, 2, 3, 4....]\n2nd row - [1, 2, 3, 4, 5....]\n3rd row - [2, 3, 4, 5, 6....]\n\nSo each output corresponding to each row, will form a complete forecast, taking `rollback` number of values behind it, to make a prediction.","c0c6811f":"<h2>Stationarity<\/h2>\n\nSome time-series models, such as such as ARIMA, assume that the underlying data is stationary. Stationarity describes that the time-series has\n\n* constant mean and mean is not time-dependent\n* constant variance and variance is not time-dependent\n* constant covariance and covariance is not time-dependent\n\nThe check for stationarity can be done in different ways\n\n* Visually - Plot time series and check for trends or seasonality\n* Basic statistics - Split the time series and compute mean\/variance of each partition\n* Statistical test - Augmented Dickey fuller test","b0b5cb1f":"<h2>ARIMA model<\/h2>\nThe auto-regressive integrated moving average (ARIMA) describes the autocorrelations in the data. **the model assumes the data to be stationary**.\n\n<h3>Steps to analyze ARIMA<\/h3>\n\n* Check stationarity - If time series has trend\/seasonality component, must be made stationary before using ARIMA.\n\n* Difference - If time series is non-stationary, needs to be stationarized through differencing. Take first difference and check stationarity, if not, do it with different forms of differencing. \n\n* Filter out validation sample - Validate how accurate a model is. We use the split of training and validation sets.\n\n* Select AR and MA terms - Use ACF and PACF to decide whether to have AR terms, MA terms or both.\n\n* Build the model - Build model and set number of periods to forecast to N\n\n* Validate model - Compare predicted values to the actuals in the validation sample.","78a91d25":"<h2>Cyclic features<\/h2>\n\nOur new time columns are cyclic in nature. The months will cycle between 1 and 12 for every year. When the difference between months increment by 1 during a year, between two years,the `month` column will jump from 12 (December) to 1 (January), this is a (-11) difference which confuses some models.","fa04528f":"<h2>Changing the granularity of data<\/h2>\n\nWe can perform resampling for additional information on the given data\n\n* Upsampling - Frequency of samples is increased (Days to hours)\n* Downsampling - Frequency of samples is decreased (Days to weeks)\n\nWe will perform downsampling with the .resample() function","95c0ef00":"We see that interpolation is the best option for our dataset","07e10823":"<h2>Lag<\/h2>\n\nCalculating each variable with `shift()` to compare the correlation with other variables","b0eaacf7":"<h2>Feature creation<\/h2>","99e4da9d":"<h2>Data visualization<\/h2>\n\nWe have the following features\n\n* Rainfall - Quantity of rain falling (mm)\n* Temperature - Temperature in celsius\n* Volume - Indicates water volume taken from drinking water treatment plant (cubic m)\n* Hydrometry - Indictaes groundwater level (m)\n\nTarget label\n\n* Depth to groundwater - Groundwater level (m from ground floor)","43643cdc":"<h2>Unit root test<\/h2>\n\nIt is a characteristic that makes it non-stationary and the ADF test belongs to this category of tests. A unit root is said to exist in a time series of alpha=1 in below equation:\n\n\n$Y_t = \t\\alpha Y_{t-1} + \t\\beta X_{e} + \\epsilon $\n\nwhere Yt is the vlaue of the time series at time 't' and Xe is an exogenous variable. The presence of unit root implies that the time series is non-stationary.\n\n<h3>Augmented Dickey-fuller<\/h3>\n\nIt is a statistical test called a unit-root test. They are a cause for nonstationary. \n* H0 - Null hypothesis - Time series has unit root (not stationary)\n* H1 - Alternate hypothesis - Time series has no unit root (time series is stationary)\n\nIf the null hypothesis can't be rejected, we can conclude that it is stationary. For the purpose of hypothesis testing, we can work with either p-values or critical values.\n\n* p-value > significance level (default: 0.05): Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\n* p-value <= significance level (default: 0.05): Reject the null hypothesis (H0), the data does not have a unit root and is stationary.\n\nIf we want to work with critical values, where the null hypothesis can be rejected if the test statistic is less than the critical value:\n\n* ADF statistic > critical value: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\n* ADF statistic < critical value: Reject the null hypothesis (H0), the data does not have a unit root and is stationary.","5e102129":"<h2>Autocorrelation analysis<\/h2>\n\nAfter a time series has been stationarized by differencing, the next step in fitting an ARIMA model is to determine whether AR or MA terms are needed to correct any autocorrelation that remains in the differenced series. By looking at the autocorrelation function (ACF) and partial autocorrelation (PACF) plots of the differenced series, you can tentatively identify the numbers of AR and\/or MA terms that are needed.\n\n* **Autocorrelation Function (ACF):** P = Periods to lag for eg: (if P= 3 then we will use the three previous periods of our time series in the autoregressive portion of the calculation) P helps adjust the line that is being fitted to forecast the series. P corresponds with MA parameter\n* **Partial Autocorrelation Function (PACF):** D = In an ARIMA model we transform a time series into stationary one(series without trend or seasonality) using differencing. D refers to the number of differencing transformations required by the time series to get stationary. D corresponds with AR parameter.\nAutocorrelation plots help in detecting seasonality.","fa325b67":"<h2>Multivariate Prophet<\/h2>","d98129b1":"<h2>Models for univariate time series<\/h2>\n\nWe will go for univariate time series analysis first. Only one variable is varying over time. Example - a temperature sensor. Every second, we only have a single-dimension value i.e. the temperature.","e7355952":"Through this we can observe some trends:\n\n* depth_to_groundwater: reaches its maximum around May\/June and its minimum around November\n* temperature: reaches its maxmium around August and its minimum around January\n* drainage_volume: reaches its minimum around July.\n* river_hydrometry: reaches its maximum around February\/March and its minimum around September","f702767b":"**NOTE**\n\nHere we are not predicting the future. We are just predicting the labels using the data we have. For predicting future values, we will make one prediction, append it to the data, pop out the first value, and make another prediction.\n\nHere we assume that we have a sequence at each step (which is present in our x_valid) and instead of using the predictions made at each step, we're just using the pattern we know upto that point to make the next value prediction.\n\nThis wouldn't be possible if we didn't have the dataset (as we wouldn't know what value to append, without predicting it) but here we know","e8451f58":"<h2>Prophet<\/h2>\n\nOur first model, for modeling the statistical problem is Prophet. It is an open-source library developed by Facebook, for univariate forecasting. It implements an additive time series forecasting model, and the implementation supports trends, seasonality and holidays. ","a3ddb80b":"<h2>EDA<\/h2>","1dffd84e":"<h2>Modeling for time series<\/h2>","5d77c081":"<h2>Auto-ARIMA<\/h2>","b1a06011":"We use the TimeSeriesSplit method provided by Sklearn for the data. This cross-validation object is a variation of KFold. In the kth split, it returns first k folds as train set and the (k+1)th fold as test set.\n\nNote that unlike standard cross-validation methods, successive training sets are supersets of those that come before them.\n\nTraining set has size: `i*n_samples\/\/(n_splits+1) + n_samples % (n_splits+1)` in the `i`th split. In each split, test indices must be higher than before, and thus shuffling in cross validator is inappropriate. The training size will keep increasing.\n\nBelow we can see two cases:\n\n1. We use the normal KFold method for time series data. The data keeps on growing and the successive data (relative to the training data) is used as a testing set\n\n2. We keep the training data size constant, and convert the test set of the previous batch, into the training set of the new batch, while simultaneously selecting the successive data points (relative to the new training data) as our testing data.","a0e47748":"<h2>Time series decomposition<\/h2>\n\nIt involves splitting up a series into level, trend, seasonality and noise. The components are elaborated as follows\n\n* Level - Average value in the series\n* Trend - Increasing or decreasing value in the series\n* Seasonality - Repeating short-term cycle in series\n* Noise - Random variation in the series\n\nIt allows us to thinki about a time series, and understand problems during time series analysis and forecasting. All the series have a level and noise. Trend and seasonality is optional. We can think of the components as additive or multiplicative\n\n* **Additive**: $y(t) = Level + Trend + Seasonality + Noise$\n* **Multiplicative**: $y(t) = Level * Trend * Seasonality * Noise$\n","55d5f3f2":"We also note that there are some zero values that seem to be nukll for `drainage_volume` and `river_hydrometry`. We will be replace them with NAN values and fill them afterwards. We will plot graphs to see where we encounter the missing values","3179a074":"**So what does the above code do?**\n\nWhat we have done is created a dataset with dimensions - (Total days-Rollback) * Rollback. What this data represents is as follows:\n\nTake the first record - It denotes the first 52 values in the time series (0:51) while the y-value denotes the 52nd value. \n\nThe second record denotes the 1-52 values in the time series,  while the y-value denotes the 53rd value.","a72bea0d":"### We have multiple ways to handle missing values\n\n* Fill NaN with outlier or zero - Filling missing values with outliers such as 0 or infinity. This is a very naive approach. We can use values like -999 or something instead.\n\n* Fill NaN with mean values - This is also not sufficient, a naive apporach\n\n* Fill NaN with last value - A cascading fill operation, this could work better\n\n* Fill NaN value with linearly interpolated values - Make use of neighbouring values to fill up the current cell.","da43fe3e":"We will fill up the NULL values with np.inf, this will give us a pseudo blank space in the graph. We will fill up the blank space with the actual value of the plot given.","1f2ebf07":"<h2>Pre-processing<\/h2>\n\nWe have to check two major things\n\n* Chronological order of dates - The dates should be in chronological order. This can be achieved by sorting the dates\n\n* Equidistant intervals - The difference between adjacent dates should be uniform and constant. We can decide on a constant time interval and resample data."}}