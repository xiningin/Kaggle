{"cell_type":{"a101197b":"code","2190c47b":"code","53ae32ee":"code","53b6f2ae":"code","cc0cb97b":"code","3e16b7cd":"code","14fed4c8":"code","de5487da":"code","df3fcb7c":"code","677d2ed7":"code","a78f1c12":"code","278715d8":"code","f8c460de":"code","f2aa22c7":"code","246060fc":"code","b7e5ddde":"code","6d121438":"code","f52940d9":"code","7f80bc22":"code","44e103cd":"code","2001139f":"code","b52b2148":"code","8adac3fe":"code","b623eb51":"code","ee2150a1":"code","add046fa":"code","98c1bcad":"code","4845cf0d":"code","77f20eeb":"code","9e1cce86":"code","46f6bb1c":"code","8abb23bc":"code","6373ecd2":"code","763ccdb2":"code","0b181bea":"code","e48885fe":"code","6db79b04":"code","ca9766dd":"code","ed05171e":"code","83584c54":"code","1c68d033":"code","a4170345":"code","29564751":"code","b7bf4f06":"code","ee02c1df":"code","b1c57456":"code","c845ab61":"code","09fe149b":"code","17d28286":"code","60120d35":"code","b6612123":"code","5a19a163":"code","2a97a506":"code","f58936cc":"code","6c8204f0":"code","4ced7424":"code","0507fb17":"code","86f152ff":"code","f29f48d6":"code","9506467f":"code","92e1d63c":"code","1b3d99e8":"code","46a42133":"code","4ced96bf":"code","5ded7f9f":"code","d055ff87":"code","b7a4e421":"code","59f46d12":"code","f0bd8452":"code","ad9ef7d4":"code","19bfc49c":"code","c41b24ad":"code","910493ff":"code","a0fe1d16":"code","d107e116":"code","30fc82e9":"code","037e9e76":"code","8216a971":"code","a1411652":"code","c29f37f0":"code","0d643127":"code","e852df28":"code","8a943143":"code","b599bda5":"code","a0a3ff51":"code","d0a40eec":"code","b7af2bb9":"code","1ed3b384":"code","1e5c6c22":"code","99f9ad93":"code","4b021510":"code","16d5b0f8":"code","278ad195":"code","e7336cac":"code","08f51f38":"code","08af722a":"code","30f0b311":"code","63435493":"code","ea653e95":"code","37947ce0":"markdown","87af4b09":"markdown","d1fecedc":"markdown","e6c6fd89":"markdown","ab616af8":"markdown","5a7b484c":"markdown","e8e24812":"markdown","12c18c83":"markdown","d4013601":"markdown","18fda9a2":"markdown","e0c5404e":"markdown","133e39ed":"markdown","6a3e4ff0":"markdown","c5495ba3":"markdown","da876673":"markdown","e5eff86b":"markdown","ef62a057":"markdown","d46cec00":"markdown","17e56a40":"markdown","4b03bac9":"markdown","8c2cdbc3":"markdown","4ae21622":"markdown","6633b8d7":"markdown","de6f0804":"markdown","1a21ff9f":"markdown","bb17bbf7":"markdown","b4f4651d":"markdown","4d1f98f4":"markdown","7db0cecc":"markdown","0f0e9e11":"markdown","8be7f83b":"markdown","19ca10a5":"markdown","3a54faa9":"markdown","a167e89b":"markdown","7f36fe27":"markdown","caf50dbc":"markdown","22615bc0":"markdown","1af528e7":"markdown","ca9d5727":"markdown","0f156ed9":"markdown","032ca7c8":"markdown","89442e7b":"markdown","2e320849":"markdown","185d2762":"markdown","60c6be04":"markdown","9ab70a51":"markdown","18cbb337":"markdown","a1473dff":"markdown","ce539b3d":"markdown","c1f07f53":"markdown","bb1c90e5":"markdown","b2358684":"markdown","213283a4":"markdown","fe9612d9":"markdown","4685a88d":"markdown","cada1063":"markdown","43cb955c":"markdown","a5e32bd3":"markdown","103c9a09":"markdown","87612d87":"markdown","a19cfa7c":"markdown","7b398a50":"markdown","6c486b9d":"markdown","7e5cec5d":"markdown","a1023764":"markdown","2bc49cf3":"markdown","0ab0439c":"markdown","4bbb3357":"markdown","1b887e14":"markdown","a683b550":"markdown","1f198ea4":"markdown","537d321b":"markdown","29bbb724":"markdown","4d96f0c9":"markdown","1eadbf88":"markdown","50fa1fc3":"markdown","a383cd34":"markdown","5f297265":"markdown","2eb2176c":"markdown","e65cdeda":"markdown","811171d1":"markdown","2402a6a5":"markdown","f796cdc1":"markdown","016abebb":"markdown","a680626b":"markdown","26d43a7c":"markdown","ccc503f3":"markdown","dd533ead":"markdown","028871c6":"markdown","aaa3af53":"markdown","91ed632c":"markdown","7f677864":"markdown","39491720":"markdown","710001ca":"markdown","bad70a8b":"markdown","e166018c":"markdown"},"source":{"a101197b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nimport matplotlib.pyplot as plot\nimport seaborn as sns\n%matplotlib inline\nsns.set(style=\"ticks\")","2190c47b":"#let us start by importing the relevant libraries\n\n%matplotlib inline\nimport warnings\nimport seaborn as sns\nwarnings.filterwarnings('ignore')\n#import the necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score,confusion_matrix, classification_report,roc_auc_score\nfrom scipy.stats import zscore\nfrom sklearn.model_selection import train_test_split","53ae32ee":"vehdf= pd.read_csv('\/kaggle\/input\/unsupervised-dataset\/vehicle.csv')\nvehdf.head()","53b6f2ae":"vehdf.shape\n# 846 rows, 19 columns","cc0cb97b":"# Check data type and other imp information of each column\nvehdf.info()","3e16b7cd":"vehdf.describe().T","14fed4c8":"# Explore distribution of vehicle in each class\nvehdf['class'].value_counts()","de5487da":"vehdf['class']=vehdf['class'].map({'car':1,'bus':2,'van':3, 1:1, 2:2, 3:3 }) \n#To make sure values dont change even after running twice or more\nvehdf['class'].value_counts()","df3fcb7c":"# Check data type and other imp information of each column\nvehdf.info()","677d2ed7":"vehdf.isnull().sum()","a78f1c12":"from sklearn.impute import SimpleImputer\n\nnewdf = vehdf.copy()\n\nimputer = SimpleImputer(missing_values=np.nan, strategy='median', verbose=1)\n#fill missing values with median column values\ntransformed_values = imputer.fit_transform(newdf)\ncolumn = vehdf.columns\nnewdf = pd.DataFrame(transformed_values, columns = column )\nnewdf.describe().T","278715d8":"print(\"Original null value count:\", vehdf.isnull().sum())\nprint(\"\\n\\nCount after we impiuted the NaN value: \", newdf.isnull().sum())","f8c460de":"newdf.describe().T","f2aa22c7":"plt.rcParams.update({'font.size': 32})\nplt.style.use('seaborn-whitegrid')\n\nnewdf.hist(bins=20, figsize=(60,40), color='lightblue', edgecolor = 'red',layout=(7,3))\nplt.show()\nplt.rcParams.update({'font.size': 10})","246060fc":"#Let us use seaborn distplot to analyze the distribution of our columns and see the skewness in attributes\nf, ax = plt.subplots(1, 6, figsize=(30,5))\n\nvis1 = sns.distplot(newdf[\"scaled_variance.1\"],bins=10, ax= ax[0])\nvis2 = sns.distplot(newdf[\"scaled_variance\"],bins=10, ax=ax[1])\nvis3 = sns.distplot(newdf[\"skewness_about.1\"],bins=10, ax= ax[2])\nvis4 = sns.distplot(newdf[\"skewness_about\"],bins=10, ax=ax[3])\nvis6 = sns.distplot(newdf[\"scatter_ratio\"],bins=10, ax=ax[5])\n\nf.savefig('subplot.png')","b7e5ddde":"skewValue = newdf.skew()\nprint(\"skewValue of dataframe attributes:\\n\", skewValue)","6d121438":"#Summary View of all attribute , Then we will look into all the boxplot individually to trace out outliers\n\nax = sns.boxplot(data=newdf, orient=\"h\")\n","f52940d9":"from scipy.stats import iqr\n\nQ1 = newdf.quantile(0.25)\nQ3 = newdf.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","7f80bc22":"def outlierCount(aSeries):\n    \n    q1 = aSeries.quantile(0.25)\n    q3 = aSeries.quantile(0.75)\n   \n    iqr = q3-q1 #Interquartile range\n    fence_low  = q1-1.5*iqr\n    fence_high = q3+1.5*iqr\n    outliers_low = aSeries[(aSeries < fence_low)]\n    outliers_high= aSeries[(aSeries > fence_high)]\n    return outliers_low.count()>0 or outliers_high.count()>0","44e103cd":"outlier=[]\nfor col in newdf.columns:\n    if(outlierCount(newdf[col])):\n        outlier.append(col)\n        \n#######\nprint(\"Outlier columns are : \\n\",outlier)","2001139f":"plt.figure(figsize= (20,15))\n\nplt.subplot(3,3,1)\nsns.boxplot(x= newdf['radius_ratio'], color='red')\n\nplt.subplot(3,3,2)\nsns.boxplot(x= newdf['pr.axis_aspect_ratio'], color='orange')\n\nplt.subplot(3,3,3)\nsns.boxplot(x= newdf['max.length_aspect_ratio'], color='green')\n\nplt.show()\n\n","b52b2148":"plt.figure(figsize= (20,15))\n\nplt.subplot(3,3,1)\nsns.boxplot(x= newdf.scaled_variance, color='brown')\n\nplt.subplot(3,3,2)\nsns.boxplot(x= newdf['scaled_variance.1'], color='yellow')\n\nplt.subplot(3,3,3)\nsns.boxplot(x= newdf['scaled_radius_of_gyration.1'], color='lightblue')\n\nplt.show()","8adac3fe":"plt.figure(figsize= (20,15))\n\nplt.subplot(3,3,1)\nsns.boxplot(x= newdf.skewness_about, color='purple')\n\nplt.subplot(3,3,2)\nsns.boxplot(x= newdf['skewness_about.1'], color='grey')\n\nplt.show()","b623eb51":"#Function to display outliers of the attributes\n\ndef handleOutlier(aSeries):\n    \n    q1 = aSeries.quantile(0.25)\n    q3 = aSeries.quantile(0.75)\n   \n    iqr = q3-q1 #Interquartile range\n    fence_low  = q1-1.5*iqr\n    fence_high = q3+1.5*iqr\n    outliers_low = aSeries[(aSeries < fence_low)]\n    outliers_high= aSeries[(aSeries > fence_high)]\n    \n    if( outliers_low.count()>0):\n        print (\"25th Quantile value: \", q1)\n        print('Outlier low Count =', outliers_low.count())\n        print('List of Low outliers:')\n        print(outliers_low.sort_values(ascending = True))\n    else:\n        print(\"Zero low outliers \\n\")\n    \n    if(outliers_high.count()>0):\n        print (\"75th Quantile value: \", q3)\n        print('Outlier High Count = ', outliers_high.count())\n        print('List of High outliers:')\n        print(outliers_high.sort_values(ascending = False))\n    else:\n        print(\"Zero high outliers\")   ","ee2150a1":"# There are certain outliers on the right side( high ouliers).Lets analyse them and make decision on their treatment\nhandleOutlier(newdf['radius_ratio'])","add046fa":"# Lets observe full rows for these outliers\nnewdf.loc[[388,135,37]]","98c1bcad":"# All these are for class van. Lets observe maximum radius_ratio for class van\nnewdf[newdf['class']==3]['radius_ratio'].sort_values( ascending=False).head(8)","4845cf0d":"# values of radius ratio for outlier are far away  from the max value 250. Lets replace these values with 250\nnewdf.loc[[388,135,37],'radius_ratio']=250.0","77f20eeb":"#Double check the values if replaced correctly\nnewdf.loc[[388,135,37]]","9e1cce86":" handleOutlier(newdf['pr.axis_aspect_ratio'])","46f6bb1c":"# Lets observe full rows for these outliers\nnewdf.loc[[4,100,37,135,291,388,523,706]]","8abb23bc":"# Lets Check for Bus first\nnewdf[newdf['class']==2]['pr.axis_aspect_ratio'].sort_values( ascending=False).head(8)","6373ecd2":"newdf.drop([4,100], inplace=True)","763ccdb2":"# Lets Check for van now first\nnewdf[newdf['class']==3]['pr.axis_aspect_ratio'].sort_values( ascending=False).head(20)","0b181bea":"newdf.drop([37,135,291,388,523,706], inplace=True)","e48885fe":" handleOutlier(newdf['max.length_aspect_ratio'])","6db79b04":"newdf.loc[[655]]","ca9766dd":"newdf[newdf['class']==3]['max.length_aspect_ratio'].sort_values( ascending=True).head(5)","ed05171e":"# Lets observe full rows for these outliers\nnewdf.loc[[391,815,127,544]]","83584c54":"# Lets Check for van(3) now first\nnewdf[newdf['class']==3]['max.length_aspect_ratio'].sort_values( ascending=False).head(20)","1c68d033":"newdf.drop(391, inplace=True)","a4170345":"# Lets Check for bus(2) now\nnewdf[newdf['class']==2]['max.length_aspect_ratio'].sort_values( ascending=False).head(20)","29564751":"newdf.drop([127,815,544], inplace=True)","b7bf4f06":"handleOutlier(newdf['scaled_variance.1'])","ee02c1df":"# Lets observe full row for this outliers\nnewdf.loc[[835]]","b1c57456":"# The outlier belongs to class bus(2). Lets observe max values as it is high outlier\nnewdf[newdf['class']==2]['scaled_variance.1'].sort_values( ascending=False).head(8)","c845ab61":" handleOutlier(newdf['scaled_radius_of_gyration.1'])","09fe149b":"# Lets observe full row for this outliers\nnewdf.loc[[655,230,498,381,79,47]]","17d28286":"# The outliers belong to class bus. Lets observe max values as it is high outlier\nnewdf[newdf['class']==2]['scaled_radius_of_gyration.1'].sort_values( ascending=False).head(20)","60120d35":"# The outlier belongs to class van. Lets observe max values as it is high outlier\nnewdf[newdf['class']==3]['scaled_radius_of_gyration.1'].sort_values( ascending=False).head(20)","b6612123":" handleOutlier(newdf['skewness_about.1'])","5a19a163":"#Lets observe the full row of the outlier\nnewdf.loc[[132]]\n# Outlier belongs to class 1 that is car","2a97a506":"##Lets observe max values for car class\nnewdf[newdf['class']==1]['skewness_about.1'].sort_values( ascending=False).head(20)","f58936cc":" handleOutlier(newdf['skewness_about'])","6c8204f0":"#Lets observe the full row of the outlier\nnewdf.loc[[761,623,516,123,797,505,400,346,113,796,190,44]]\n# Outlier belongs to class 1 that is car","4ced7424":"##Lets observe max values for car class\nnewdf[newdf['class']==1]['skewness_about'].sort_values( ascending=False).head(20)","0507fb17":"plt.figure(figsize= (20,15))\nplt.subplot(8,8,1)\nsns.boxplot(x= newdf['pr.axis_aspect_ratio'], color='orange')\n\nplt.subplot(8,8,2)\nsns.boxplot(x= newdf.skewness_about, color='purple')\n\nplt.subplot(8,8,3)\nsns.boxplot(x= newdf.scaled_variance, color='brown')\nplt.subplot(8,8,4)\nsns.boxplot(x= newdf['radius_ratio'], color='red')\n\nplt.subplot(8,8,5)\nsns.boxplot(x= newdf['scaled_radius_of_gyration.1'], color='lightblue')\n\nplt.subplot(8,8,6)\nsns.boxplot(x= newdf['scaled_variance.1'], color='yellow')\n\nplt.subplot(8,8,7)\nsns.boxplot(x= newdf['max.length_aspect_ratio'], color='lightblue')\n\nplt.subplot(8,8,8)\nsns.boxplot(x= newdf['skewness_about.1'], color='pink')\n\nplt.show()\n","86f152ff":"def correlation_heatmap(dataframe,l,w):\n    #correlations = dataframe.corr()\n    correlation = dataframe.corr()\n    plt.figure(figsize=(l,w))\n    sns.heatmap(correlation, vmax=1, square=True,annot=True,cmap='viridis')\n    plt.title('Correlation between different fearures')\n    plt.show();\n    \n# Let's Drop Class column and see the correlation Matrix & Pairplot Before using this dataframe for PCA as PCA should only be perfromed on independent attribute\ncleandf= newdf.drop('class', axis=1)\n#print(\"After Dropping: \", cleandf)\ncorrelation_heatmap(cleandf, 30,15)","f29f48d6":"sns.pairplot(cleandf, diag_kind=\"kde\")","9506467f":"#display how many are car,bus,van. \nnewdf['class'].value_counts()\n\nsplitscaledf = newdf.copy()\nsns.countplot(newdf['class'])\nplt.show()","92e1d63c":"#now separate the dataframe into dependent and independent variables\nX = newdf.iloc[:,0:18].values\ny = newdf.iloc[:,18].values","1b3d99e8":"from sklearn.preprocessing import StandardScaler\n#We transform (centralize) the entire X (independent variable data) to normalize it using standardscalar through transformation. \n#We will create the PCA dimensions on this distribution. \nsc = StandardScaler()\nX_std =  sc.fit_transform(X)          \n","46a42133":"cov_matrix = np.cov(X_std.T)\nprint(\"cov_matrix shape:\",cov_matrix.shape)\nprint(\"Covariance_matrix\",cov_matrix)","4ced96bf":"eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\nprint('Eigen Vectors \\n%s', eigenvectors)\nprint('\\n Eigen Values \\n%s', eigenvalues)","5ded7f9f":"\n# Make a set of (eigenvalue, eigenvector) pairs:\n\neig_pairs = [(eigenvalues[index], eigenvectors[:,index]) for index in range(len(eigenvalues))]\n\n# Sort the (eigenvalue, eigenvector) pairs from highest to lowest with respect to eigenvalue\neig_pairs.sort()\n\neig_pairs.reverse()\nprint(eig_pairs)\n\n# Extract the descending ordered eigenvalues and eigenvectors\neigvalues_sorted = [eig_pairs[index][0] for index in range(len(eigenvalues))]\neigvectors_sorted = [eig_pairs[index][1] for index in range(len(eigenvalues))]\n\n# Let's confirm our sorting worked, print out eigenvalues\nprint('Eigenvalues in descending order: \\n%s' %eigvalues_sorted)","d055ff87":"tot = sum(eigenvalues)\nvar_explained = [(i \/ tot) for i in sorted(eigenvalues, reverse=True)]  # an array of variance explained by each \n# eigen vector... there will be 18 entries as there are 18 eigen vectors)\ncum_var_exp = np.cumsum(var_explained)  # an array of cumulative variance. There will be 18 entries with 18 th entry \n# cumulative reaching almost 100%","b7a4e421":"\nplt.bar(range(1,19), var_explained, alpha=0.5, align='center', label='individual explained variance')\nplt.step(range(1,19),cum_var_exp, where= 'mid', label='cumulative explained variance')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal components')\nplt.legend(loc = 'best')\nplt.show()","59f46d12":"# P_reduce represents reduced mathematical space....\n\nP_reduce = np.array(eigvectors_sorted[0:8])   # Reducing from 18 to 7 dimension space\n\nX_std_8D = np.dot(X_std,P_reduce.T)   # projecting original data into principal component dimensions\n\nreduced_pca = pd.DataFrame(X_std_8D)  # converting array to dataframe for pairplot\n\nreduced_pca.head()","f0bd8452":"sns.pairplot(reduced_pca, diag_kind='kde') ","ad9ef7d4":"#now split the data into 70:30 ratio\n\n#orginal Data\nOrig_X_train,Orig_X_test,Orig_y_train,Orig_y_test = train_test_split(X_std,y,test_size=0.30,random_state=1)\n\n#PCA Data\npca_X_train,pca_X_test,pca_y_train,pca_y_test = train_test_split(reduced_pca,y,test_size=0.30,random_state=1)\n","19bfc49c":"svc = SVC() #instantiate the object","c41b24ad":"#fit the model on orighinal raw data\nsvc.fit(Orig_X_train,Orig_y_train)","910493ff":"#predict the y value\nOrig_y_predict = svc.predict(Orig_X_test)","a0fe1d16":"#now fit the model on pca data with new dimension\nsvc1 = SVC() #instantiate the object\nsvc1.fit(pca_X_train,pca_y_train)\n\n#predict the y value\npca_y_predict = svc1.predict(pca_X_test)","d107e116":"#display accuracy score of both models\n\nprint(\"Model Score On Original Data \",svc.score(Orig_X_test, Orig_y_test))\nprint(\"Model Score On Reduced PCA Dimension \",svc1.score(pca_X_test, pca_y_test))\n\nprint(\"Before PCA On Original 18 Dimension\",accuracy_score(Orig_y_test,Orig_y_predict))\nprint(\"After PCA(On 8 dimension)\",accuracy_score(pca_y_test,pca_y_predict))","30fc82e9":"# Calculate Confusion Matrix & PLot To Visualize it\n\ndef draw_confmatrix(y_test, yhat, str1, str2, str3, datatype ):\n    cm = confusion_matrix( y_test, yhat, [1,2,3] )\n    print(\"Confusion Matrix For :\",datatype, \"\\n\",cm )\n    sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels = [str1, str2,str3] , yticklabels = [str1, str2,str3] )\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()\n    \n\ndraw_confmatrix(Orig_y_test, Orig_y_predict, \"Car \", \"Bus\", \"Van \",\"Original Data Set\" )\n\ndraw_confmatrix(pca_y_test, pca_y_predict,\"Car \", \"Bus\", \"Van \", \"For Reduced Dimensions Using PCA \")\n\n#Classification Report Of Model built on Raw Data\nprint(\"Classification Report For Raw Data:\", \"\\n\", classification_report(Orig_y_test,Orig_y_predict))\n\n#Classification Report Of Model built on Principal Components:\n\nprint(\"Classification Report For PCA:\",\"\\n\", classification_report(pca_y_test,pca_y_predict))","037e9e76":"splitscaledf.head()","8216a971":"splitscale_X = splitscaledf.iloc[:,0:18].values\nsplitscale_y = splitscaledf.iloc[:,18].values","a1411652":"#splitting the data in train and test sets into 70:30 Ratio\n\nSplitScale_X_train, SplitScale_X_test, SplitScale_y_train, SplitScale_y_test = train_test_split(splitscale_X,splitscale_y, test_size = 0.3, random_state = 10)","c29f37f0":"ssx_train_sd = StandardScaler().fit_transform(SplitScale_X_train)\nssx_test_sd = StandardScaler().fit_transform(SplitScale_X_test)\n\nprint(len(ssx_train_sd))\nprint(len(ssx_test_sd))\n","0d643127":"# generating the covariance matrix and the eigen values for the PCA analysis\ncov_matrix_1 = np.cov(ssx_train_sd.T) # the relevanat covariance matrix\nprint('Covariance Matrix \\n%s', (cov_matrix_1))\n\n#generating the eigen values and the eigen vectors\ne_vals, e_vecs = np.linalg.eig(cov_matrix_1)\nprint('Eigenvectors \\n%s' %(e_vecs))\nprint('\\nEigenvalues \\n%s' %e_vals)","e852df28":"# Step 3 (continued): Sort eigenvalues in descending order\n\n# Make a set of (eigenvalue, eigenvector) pairs\neig_pairs = [(e_vals[index], e_vecs[:,index]) for index in range(len(e_vals))]\n\n# Sort the (eigenvalue, eigenvector) pairs from highest to lowest with respect to eigenvalue\neig_pairs.sort()\n\neig_pairs.reverse()\nprint(eig_pairs)\n\n# Extract the descending ordered eigenvalues and eigenvectors\neigvalues_sorted = [eig_pairs[index][0] for index in range(len(e_vals))]\neigvectors_sorted = [eig_pairs[index][1] for index in range(len(e_vals))]\n\n# Let's confirm our sorting worked, print out eigenvalues\nprint('Eigenvalues in descending order: \\n%s' %eigvalues_sorted)","8a943143":"tot = sum(e_vals)\nvar_explained = [(i \/ tot) for i in sorted(e_vals, reverse=True)]  \n# an array of variance explained by each eigen vector... there will be 18 entries as there are 18 eigen vectors)\ncum_var_exp = np.cumsum(var_explained) \n# an array of cumulative variance. There will be 18 entries with 18 th entry cumulative reaching almost 100%","b599bda5":"plt.bar(range(1,19), var_explained, alpha=0.5, align='center', label='individual explained variance')\nplt.step(range(1,19),cum_var_exp, where= 'mid', label='cumulative explained variance')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal components')\nplt.legend(loc = 'best')\nplt.show()","a0a3ff51":"# P_reduce represents reduced mathematical space....\n\nP_reduce_1 = np.array(eigvectors_sorted[0:8])   # Reducing from 18 to 8 dimension space\n\nX_train_std_pca = np.dot(ssx_train_sd,P_reduce_1.T)   # projecting original data into principal component dimensions\n\nX_test_std_pca = np.dot(ssx_test_sd,P_reduce_1.T) \n\nProjected_df_train = pd.DataFrame(X_train_std_pca)\nProjected_df_test = pd.DataFrame(X_test_std_pca)","d0a40eec":"sns.pairplot(Projected_df_train, diag_kind='kde')","b7af2bb9":"sns.pairplot(Projected_df_test, diag_kind='kde')","1ed3b384":"ssx_train_sd.shape, P_reduce_1.T.shape, X_train_std_pca.shape, X_test_std_pca.shape","1e5c6c22":"clf1 = SVC()\nclf1.fit(ssx_train_sd, SplitScale_y_train)\nprint ('Before PCA score', clf1.score(ssx_test_sd, SplitScale_y_test))\n\nclf2 = SVC()\nclf2.fit(X_train_std_pca, SplitScale_y_train)\nprint ('After PCA score', clf2.score(X_test_std_pca, SplitScale_y_test))\n\n#predict the y value\npca_yhat_predict= clf2.predict(X_test_std_pca)\n\n#orginal data yhat value\norig_yhat_predict = clf1.predict(ssx_test_sd)\n\nprint(\"Before PCA On Original 18 Dimension\",accuracy_score(SplitScale_y_test,orig_yhat_predict))\nprint(\"After PCA(On 8 dimension)\",accuracy_score(SplitScale_y_test,pca_yhat_predict))","99f9ad93":"\ndraw_confmatrix(SplitScale_y_test, orig_yhat_predict,\"Car \", \"Bus\", \"Van \", \"Original Data Set\" )\n\ndraw_confmatrix(SplitScale_y_test, pca_yhat_predict,\"Car \", \"Bus\",\"Van \",  \"For Reduced Dimensions Using PCA \")\n\n#Classification Report Of Model built on Raw Data\nprint(\"Classification Report For Raw Data:\", \"\\n\", classification_report(SplitScale_y_test,orig_yhat_predict))\n\n#Classification Report Of Model built on Principal Components:\n\nprint(\"Classification Report For PCA:\",\"\\n\", classification_report(SplitScale_y_test,pca_yhat_predict))","4b021510":"import itertools\n\ndef classifiers_hypertune(name,rf,param_grid,x_train_scaled,y_train,x_test_scaled,y_test,CV):\n    CV_rf = GridSearchCV(estimator=rf, param_grid=param_grid, cv=CV, verbose= 1, n_jobs =-1 )\n    CV_rf.fit(x_train_scaled, y_train)\n    \n    y_pred_train = CV_rf.predict(x_train_scaled)\n    y_pred_test = CV_rf.predict(x_test_scaled)\n    \n    print('Best Score: ', CV_rf.best_score_)\n    print('Best Params: ', CV_rf.best_params_)\n    \n    \n    \n    #Classification Report\n    print(name+\" Classification Report: \\n\")\n    print(classification_report(y_test, y_pred_test))\n    \n   \n    #Confusion Matrix for test data\n    draw_confmatrix(y_test, y_pred_test,\"Car\", \"Bus\", \"Van\", \"Original Data Set\" )\n    print(\"SVM Accuracy Score:\",round(accuracy_score(y_test, y_pred_test),2)*100)\n    ","16d5b0f8":"\n#Training on SVM Classifier\nfrom sklearn.model_selection import GridSearchCV\nsvmc = SVC()\n\n#Let's See What all parameters one can tweak \nprint(\"SVM Parameters:\", svmc.get_params())\n\n# Create the parameter grid based on the results of random search \nparam_grid = [\n  {'C': [0.01, 0.05, 0.5, 1], 'kernel': ['linear']},\n  {'C': [0.01, 0.05, 0.5, 1],  'kernel': ['rbf']},\n ]\n\nparam_grid_1 = [\n  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n ]","278ad195":"classifiers_hypertune(\"Support Vector Classifier\", svmc, param_grid,X_train_std_pca, SplitScale_y_train, X_test_std_pca, SplitScale_y_test,10)","e7336cac":"classifiers_hypertune(\"Support Vector Classifier\", svmc, param_grid,ssx_train_sd, SplitScale_y_train, ssx_test_sd, SplitScale_y_test,10)","08f51f38":"classifiers_hypertune(\"Support Vector Classifier_iterarion2\", svmc, param_grid_1,X_train_std_pca, SplitScale_y_train, X_test_std_pca, SplitScale_y_test,10)","08af722a":"classifiers_hypertune(\"Support Vector Classifier\", svmc, param_grid_1,ssx_train_sd, SplitScale_y_train, ssx_test_sd, SplitScale_y_test,10)","30f0b311":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\n\nmodel.fit(ssx_train_sd, SplitScale_y_train)\nprint ('Before PCA score', model.score(ssx_test_sd, SplitScale_y_test))\n\nmodel.fit(X_train_std_pca, SplitScale_y_train)\nprint ('After PCA score', model.score(X_test_std_pca, SplitScale_y_test))\n\n","63435493":"from sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB()\n\nnb.fit(ssx_train_sd, SplitScale_y_train)\nprint ('Before PCA score', nb.score(ssx_test_sd, SplitScale_y_test))\n\nnb.fit(X_train_std_pca, SplitScale_y_train)\nprint ('After PCA score', nb.score(X_test_std_pca, SplitScale_y_test))\n","ea653e95":"from sklearn.tree import DecisionTreeClassifier\ndt_model = DecisionTreeClassifier(criterion = 'entropy' )\n\ndt_model.fit(ssx_train_sd, SplitScale_y_train)\nprint ('Before PCA score', dt_model.score(ssx_test_sd, SplitScale_y_test))\n\ndt_model.fit(X_train_std_pca, SplitScale_y_train)\nprint ('After PCA score', dt_model.score(X_test_std_pca, SplitScale_y_test))\n","37947ce0":"- lets observe the class Van(3) values of high outliers","87af4b09":"- All fields are numeric except class- no need to convert other data types<br>\n- There are missing values in many columns like circularity, distance circularity, radius ratio .. etc","d1fecedc":"### Confusion Matrix: \n","e6c6fd89":"- Now lets check high outliers ","ab616af8":"##### Pairplot Analysis : On Test PCA Data Set","5a7b484c":"### Univariate Analysis Using Boxplot:\n\nIn descriptive statistics, a box plot is a method for graphically depicting groups of numerical data through their quartiles. Box plots may also have lines extending vertically from the boxes (whiskers) indicating variability outside the upper and lower quartiles, hence the terms box-and-whisker plot and box-and-whisker diagram. Outliers may be plotted as individual points.\n","e8e24812":"#### Quick Eyeballing To See if there is any missing values","12c18c83":"##### For skewness_about.1","d4013601":"#### Quick Insights: \nWe can see that :<br>\n> - class, hollow_ratio,max.length_rectangularity, , max.length_aspect_ratio, compactness has no missing values rest all features are having some kind of missing values <br>\n> - All attributes are of numerical type\n#### Finding The Missing Value: \n    Let's find the count of each attribite & treat the missing values ","18fda9a2":"#### Understanding the relationship between all independent attribute:\n\nWe will be using data correlation: \n\nData Correlation: Is a way to understand the relationship between multiple variables and attributes in your dataset. Using Correlation, you can get some insights such as:\n\n- One or multiple attributes depend on another attribute or a cause for another attribute.\n- One or multiple attributes are associated with other attributes.\n\nSpearman and Pearson are two statistical methods to calculate the strength of correlation between two variables or attributes. Pearson Correlation Coefficient can be used with continuous variables that have a linear relationship.\n\n#### Pearson Correlation Coefficient: \n\nWe will use Pearson Correlation Coefficient to see what all attributes are linearly related and also visualize the same in the seaborns scatter plot. \n","e0c5404e":"##### For pr.axis_aspect_ratio","133e39ed":"#### Calculating Eigen Vectors & Eigen Values: Using numpy linear algebra function","6a3e4ff0":"#### Calculate Confusion Matrix & Plot To Visualize it","c5495ba3":"##### Checking Outliers Using IQR: Upper whisker\n\nThe interquartile range (IQR), also called the midspread or middle 50%, or technically H-spread, is a measure of statistical dispersion, being equal to the difference between 75th and 25th percentiles, or between upper and lower quartiles, IQR = Q3 \u2212 Q1.\n  ","da876673":"#### Let's Apply Grid Search & Cross-Validation:To Tune Our Model and Validate The Model's Accuracy Score","e5eff86b":"##### Obseravtion: \n\nIf you carefully observe above, our orginal dataframe vehdf and new dataframe newdf , we will find that , After we imputed the dataframe series , using simpleimputer, we can see that the missing NaN values from our orginal vehdf dataframe columns are treated and replaced using median strategy. ","ef62a057":"> As we now have the IQR scores, it\u2019s time to get hold on outliers. The below code will give an output with some true and false values. The data point where we have False that means these values are valid whereas True indicates presence of an outlier.","d46cec00":"#### Load the Dataset and find the datatypes","17e56a40":"#### Fitting SVC ON PCA Data: ","4b03bac9":"- Index 4 and 100 belongs to class Bus(2) while others belong to class van(3). \n- Lets observe max values of this column for both bus and van","8c2cdbc3":"### Extra inferences\n\n#### Let's Look How Some Other Classifier Models Perform On Original Data & PCA treated data sets\n\n#### Logistic Regression","4ae21622":"### Approach 2: ","6633b8d7":"#### Quick Insights:\nAs observed in our correlation heatmap our pairplot seems to validate the same. Scaled Variance & Scaled Variance.1 seems to be have very strong positive correlation with value of 0.98. skewness_about_2 and hollow_ratio also seems to have strong positive correation with coeff: 0.89\n\nscatter_ratio and elongatedness seems to be have very strong negative correlation. elongatedness and pr.axis_rectangularity seems to have strong negative correlation.","de6f0804":"#### Let us check The Pairplot Of Reduced Dimension After PCA: ","1a21ff9f":"#### Iteration 2 : On original Data Set","bb17bbf7":"#### Dimensionality Reduction\n\nNow 8 dimensions seems very reasonable. With 8 variables we can explain over 95% of the variation in the original data!","b4f4651d":"#### Let's Plot The Box Plot Once Again To See if outliers are removed.","4d1f98f4":"##### For skewness_about","7db0cecc":"##### Quick Observation : \n   - Most of the data attributes seems to be normally distributed \n   - scaled valriance 1 and skewness about 1 and 2, scatter_ratio, seems to be right skewed .\n   - pr.axis_rectangularity seems to be having outliers as there are some gaps found in the bar plot. ","0f0e9e11":"- lets first check low outliers ","8be7f83b":"\n#### On Orginal Data Set: \n\n   - When we apploed GridSearchCV on Our model which is using orginal data set, we saw\n      - Best Score:  0.9725557461406518\n      - Best Params:  {'C': 1, 'kernel': 'rbf'}\n      - Accuracy Score : We saw an improvement in accuracy score to  95 % which is better than what we observed in approach 2.\n","19ca10a5":"> There are values in continuity like 4,3,2 hence 2 does not look very low. We will leave this outlier as is.","3a54faa9":"    - So we can see that how Principal component analysis can help us cherry pick only the relevant features by analysing the relationship between independent attributes to see which one will be more helpful in building our model without introducing any impurity in it. \n    \n    - We can increase the models performance both in terms of processing speed and the ease of implementing it . \n    \n    - Also we learned that one should always make sure data is normalized\/scaled before perfroming PCA, else the result will not be as per expectations\n    - Also it is recommended though it's not a rule , to split your data set into test-train before you apply data normalization\/scaling. This will help our model to perfrom more realistically in production environment. \n    - If one scale the model before splitting it into train-test , there is a high chance of data leakage . By data leakage i mean that our model will be alaready influenced by test data , as it has tasted a bit of test data even before it will fit and tested on it. \n    \n    - Also once one have tried every bit of techniques of removing outliers, treating the missing value, normalizing  the data whenever required, applied the diffrent models to come up with better model based on metrics which suits more the problem statement. It is advisable to fine tune it using hyperparameter tuning techniques which tunes the model performances and also employs Cross-fold validation internally to make sure our model is ready to face production environment. \n    \n- It would be good if we also measure how other classifier model like logistic regression, decisiontree , emsemble model like randomforrestclassifier will perfrom both on original data & PCA data. It would be interesting to see the confusion matrix and various others metrics like recall score, precision score, f1-score etc. \n\n- But Since our objective was to see how PCA data set impacts the model perfromance specially on training data set , we have here achived our objective and saw that PCA is great tool to make our model perfrom better . ","a167e89b":"#### Performing EDA: \n\n  - Finding Any Missing Value\n  - Finding Outliers\n  - Understanding attributes using descriptive statistics\n  - visualizing attribute distribution using univariate analysis\n  - Finding attribute correlation and analysing which attribute is more important","7f36fe27":"#### Let's further tweak the parameters to see if we can improve our model accuracy :","caf50dbc":"### Project on vehicle data set","22615bc0":"#### For Original Data Set: ","1af528e7":"#### Decisiontree Classifier: ","ca9d5727":"- Cars are almost double in number as compared to bus and van. van is least in number<br>\n- Let us change the data type of class to numerical","0f156ed9":"#### It is clealry visible from the pairplot above that:\nAfter dimensionality reduction using PCA our attributes have become independent with no correlation among themselves. As most of them have cloud of data points with no linear kind of relationship.","032ca7c8":"#### Let's Perfrom The PCA and See How Our Model Perform:\n\n##### When we split our data set into test train and then apply scaling. \n   \nIn previous  process of PCA and model performance comparison, we scaled our data before splitting them into train and test, which may have lead to some kind of data leakages. Let's see how our PCA process and model accuracy behaves when we scale our data after splitting the into train-test set ","89442e7b":"#### Quick Insights: \n\n    - We can see that after we performed PCA our new dataframe with reduced dimesnions has no to zero linear relationship among themseleves, which is the main objective of using PCA tool. Almost all attribures have cloud of data in the mathematical space with no clear positive or negative correlation.\n    ","2e320849":"#### Fit SVC Model ON Train-test Data: \n\nLet's build two Support Vector Classifier Model one with 18 original independent variables and the second one with only the 7 new reduced variables constructed using PCA.","185d2762":"#### Separate The Data Into Independent & Dependent attribute","60c6be04":"- There are values in continuity like 982,987,998 hence 1018 does not look very high. We will leave this outlier as is.","9ab70a51":"> For bus we can see values around 75 and max value 76. It is better to drop this row as the values 103 is significantly higher","18cbb337":"#### Pairplot Analysis : On Training PCA Data Set","a1473dff":"#### Descriptive statistical summary \n\ndescribe() Function gives the mean, std and IQR values. It excludes character column and calculate summary statistics only for numeric columns.","ce539b3d":"> Outliers Removed","c1f07f53":"#### Hypertuning SVM using hyper Parameters: \n\n##### Iteration 1: \n\n#### In Case Of PCA: \n","bb1c90e5":"- Again for bus max length aspect ratio is 8 and Junp from 8 to 19\/22 is too high. Lets drop this outlier from train set","b2358684":"*In above columns outliers are present*<br> <br>\n**Lets handle the outliers one by one**","213283a4":"#### Scaling The Independent Data Set: ","fe9612d9":"- From 72 to 97 it is big jump in value and then other outlier values are even higher upto 138.<br>\n- It is better to drop these rows","4685a88d":"#### Naive Bayes","cada1063":"#### Sort eigenvalues in descending order","43cb955c":"#### Quick Insights : From Correlation Hetamap: \n\n##### Strong\/fare Correlation: \n\n          - Scaled Variance & Scaled Variance.1 seems to be strongly correlated with value of 0.98\n          - skewness_about_2 and hollow_ratio seems to be strongly correlated, corr coeff: 0.89\n          - ditance_circularity and radius_ratio seems to have high positive correlation with corr coeff: 0.81\n          - compactness & circularity , radius_ratio & pr.axis_aspect_ratio also seems very averagely correlated with coeff: 0.67.\n          - scaled _variance and scaled_radius_of_gyration, circularity & distance_circularity also seems to be highly correlated with corr coeff: 0.79\n          - pr.axis_recatngularity and max.length_recatngularity also seems to be strongly correlated with coeff: 0.81 \n          - scatter_ratio and elongatedness seems to be have strong negative correlation val : 0.97\n          - elongatedness and pr.axis_rectangularity seems to have strong negative correlation, val:  0.95\n       \n          \n   \n##### Little To No Correlation:   \n          -max_length_aspect_ratio & radius_ratio have average correlation with coeff: 0.5\n          - pr.axis_aspect_ratio & max_length_aspect_ratio seems to have very little correlation\n          - scaled_radius_gyration & scaled_radisu_gyration.1 seems to be very little correlated\n          - scaled_radius_gyration.1 & skewness_about seems to be very little correlated\n          - skewness_about & skewness_about.1 not be correlated\n          - skewness_about.1 and skewness_about.2 are not correlated.\n        \nlet's visualize the same with pairplot , to see how it looks visually . \n\n        \n#### Pairplot Analysis:           ","a5e32bd3":"##### For scaled_variance.1","103c9a09":"### Approach 1: ","87612d87":"#### Quick Observation: \n    - From above we plot we can clealry observer that 8 dimension() are able to explain 95 %variance of data. \n    - so we will use first 8 principal components and calulate the reduced dimensions.","a19cfa7c":"##### Quick Insights On descriptive stats: \n   -  Compactness has mean and median values almost similar , it signifies that it is normally distribited and has no skewness\/outlier \n   - circularity : it also seems to be normally distribted as mean and median has similar values\n   - scatter_ratio, Scaled variance 1 & 2 feature seems to be having some kind of skewness and outlier ","7b398a50":"#### Understanding each attributes : \n\nUnivariate \n\n  - Quick descriptive statistics to make some meaningful sense of data \n  - Plotting univariate distribution\n  - Finding outliers & skewness in data series.  \n  - Treating outliers\n   ","6c486b9d":"##### Closing Comments: ","7e5cec5d":"#### Treating the missing values","a1023764":"- Value is well in range of max value of skewness_about for cars. we will not delete or replace it","2bc49cf3":"##### For max.length_aspect_ratio","0ab0439c":"#### Calculating covariance matrix:\n\nCovariance matrix should be 18*18 matrix","4bbb3357":"#### Quick Insights On Confusion Matrix: \n         \n     - In our approach 1: we saw that our model actual instances of car : 135, bus : 57 and van : 59   \n     - But in our second approach where we split the data set and then scaled , we saw the actual categorization instances as car : 128, bus : 66, van: 57 \n     \n#### Confusion Matrix For : \n\n#### Original Data Set:\n \n[[121   4   3]\n [  1  64   1]\n [  4   0  53]]<br>\n#### Reduced Dimensions Using PCA <br>\n [[118   5   5]\n [  2  63   1]\n [  8   0  49]]\n\n","1b887e14":"> row with index 391 is for van(3) and others are for bus(2). lets observe max values as ouliers are hgh in nature","a683b550":"- Value is well in range of max value of skewness_about.1 for cars. we will not delete or replace it","1f198ea4":"#### Let's Split Our Data Into Test & Train Data Set","537d321b":"#### Visualizing The plot : Principal Componenet Vs Explained Variance Ratio","29bbb724":"#### Fitting Model and measuring score simply on Original Data : ","4d96f0c9":"- Outlier is double the max value which is 12. better drop this row","1eadbf88":"### Dimensionality Reduction:  ","50fa1fc3":"> Values ouliers for vans are almost in range of max. We will neither delete them nor replace them-leave as is","a383cd34":"#### Univariate Analysis & its distributions","5f297265":"#### Plotting The Explained Variance and Princiapl Components:  ","2eb2176c":" \nWe found from our pairplot analysis that, Scaled Variance & Scaled Variance.1 and elongatedness and pr.axis_rectangularity to be strongly correlated , so they need to dropped of treated carefully before we go for model building. \n\n#### Choosing the right attributes which can be the right choice for model building\n\nsince our objective is to recognize whether an object is a van or bus or car based on some input features. so our main assumption is, there is little or no multicollinearity between the features. if our dataset has perfectly positive or negative attributes as can be obseverd for our correlation analysis, there is a high chance that the performance of the model will be impacted by a problem called \u2014 \u201cMulticollinearity\u201d. Multicollinearity happens when one predictor variable in a multiple regression model can be linearly predicted from the others with a high degree of accuracy. This can lead to skewed or misleading results.\n\nif two features is highly correlated then there is no point using both features.in that case, we can drop one feature. SNS heatmap gives us the correlation matrix where we can see which features are highly correlated. \n\nFrom above correlation matrix we can see that there are many features which are highly correlated. if we carefully analyse, we will find that many  features are there which having more than 0.9 correlation. so we can decide to get rid of those columns whose correlation is +-0.9 or above.There are 8 such columns:\n\n- max.length_rectangularity\n- scaled_radius_of_gyration\n- skewness_about.2\n- scatter_ratio\n- elongatedness\n- pr.axis_rectangularity\n- scaled_variance\n- scaled_variance.1\n\n#### Outcome: \nAlso we observed that more than 50 % of our attributes ar highly correlated , so  what we can we do best to deal with this kind problem of Multicollinearity\n\n##### Well, There are multiple ways to deal with this problem. The easiest way is to delete or eliminate one of the perfectly correlated features.  We can pick one of the tiwo highly correalated variables and drop another one. like in our case Scaled Variance & Scaled Variance.1 are having strong positive correlation , so we can pick one and drop one as they will only make our dimension redundant.\n\n##### Similarly between elongatedness and pr.axis_rectangularity we can pick one as they have very strong negative correlation. This approach can be used to select the feature we want to carry forward for model analysis. But there is another better approach called PCA\n\n##### Another method, is to use a dimension reduction algorithm such as Principle Component Analysis (PCA). We will go for PCA and analyse the same going forward: \n","e65cdeda":"##### Quick Observation: \n   - On training data set we saw that our support vector classifier without performing PCA has an accuracy score of 96 % \n   - But when we applied the SVC model on PCA componenets(reduced dimensions) our model scored 95 %. \n   - Considering that original dataframe had 18 dimensions and After PCA dimension reduced to 8, our model has fared well in terms of accuracy score. ","811171d1":"- lets observe the class Bus(2) values of high outliers","2402a6a5":"- Values ouliers for Bus are almost in range of max. We will neither delete them nor replace them-leave as is","f796cdc1":"### It Seems that Support Vectore Classifier is a better model to classifiy the given silhoutte info as van, bus, car","016abebb":"#### Quick Insight: \n\n#### Hyper tuning of Model on PCA Data Set: \n\n   - We generally tune some of the important hyperparameters of the model which are not the model parameter. like here we played with C value : penalty and the type of kernel : rbf\/liner  \n   \n   - GridSearchCV get's the best paramter from the array of parameters and find the best model and score for us. \n   - Here on perfroming gridsearch hyper tuning of SVM model we got the best parameter to  be \n    : Best Score:  0.9691252144082333\n    : Best Params:  {'C': 1, 'kernel': 'rbf'}\n    : Accruacy Score : 92 % which seems to be similar to than what we measured earlier in our appraoch 2\n    \n    - We can further play with hyper parameters and see if it helps to score our model better. ","a680626b":"#### Let's train the model with both original data and pca data with new dimension\n\n#### Fitting SVC model On Original Data","26d43a7c":"##### For radius_ratio","ccc503f3":"##### For scaled_radius_of_gyration.1","dd533ead":"#### Sort eigenvalues in descending order","028871c6":"#### Let's Apply StandardScaler to normalize our data set","aaa3af53":"\n### Principal Component Analysis(PCA):\n\n     - Basically PCA is a dimension reduction methodology which aims to reduce a large set of (often correlated) variables into a smaller set of (uncorrelated) variables, called principal components, which holds sufficient information without loosing the the relevant info much.\n     \n     - Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components.\n     \n### We will perform PCA in following steps:\n   - Split our data into train and test data set\n   - normalize the training set using standard scalar\n   - Calculate the covariance matrix.\n   - Calculate the eigenvectors and their eigenvalues.\n   - Sort the eigenvectors according to their eigenvalues in descending order.\n   - Choose the first K eigenvectors (where k is the dimension we'd like to end up with).\n   - Build new dataset with reduced dimensionality.\n   ","91ed632c":"##### Quick Comment: \n\nWe can see that all out boxplot for all the attributes which had outlier have been treated, removed and some are ignored. ","7f677864":"#### Calculating covariance matrix:\n\nCovariance matrix should be 18*18 matrix\n","39491720":"#### Observation on boxplots: \n   - pr.axis_aspect_ratio, skewness_about, max_length_aspect_ratio, skewness_about_1, scaled_radius_of_gyration.1, scaled_variance.1, radius_ratio, skewness_about, scaled_variance.1 are some of the attributes with outliers. which is visible with all dotted points   ","710001ca":"#### 5 Point Summary","bad70a8b":"#### Quick Comments: \n\n#### Confusion Metric Analysis ON Original  Data: \n\nConfusion Matrix For : \n Original Data Set \n [[132   1   2]\n [  1  56   0]\n [  3   3  53]]\n     \n       - Our model on original data set has correctly classified 132 car out of 135 actuals cars and has errored in one case where it has wrongly predicted car to be a bus and two cases where it has wrongly predicted car to be a van.  \n       - IN case of 57 actual buses our svm model has correcly classified 56 cars and it has errored in only one case where it has wrongly predicted bus to be a car\n       - In case of 59 instances of actual vans , our model has correctly classified 53 vans , It has faltered in classifying wrongly 3 vans to be a car and 3 vans to be a bus. \n       \n       \n#### Confusion Metric Analysis ON Reduced Dimesnion After PCA : \n\nFor Reduced Dimensions Using PCA:\n[[132   1   2]\n [  2  55   0]\n [  2   4  53]]\n    \n    - Out of 135 actual instances of cars our model has correctly predicted 132 cars and errored in 1 instance where it wrongly classified car to be a bus and in 2 instances where it wrongly classified car to be a van. \n    - Out of 57 actuals buses , our model has correclty classified 55 of them to be a  bus and faltered in 2 cases where it wrongly classified 2 buses to be a car .\n    \n    - Out of 59 actual vans , our model has correclty classified 53 of them to be a van. It has faltered in 2 cases where it wrongly classified 2 vans to be a car and 4 vans to be a bus.\n","e166018c":"- We can see a slight improvement in best model which was picked by our gridsearchcv method: 98.28 % and also we saw a slight increase in model accuracy score : 96 % "}}