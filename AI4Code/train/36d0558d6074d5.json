{"cell_type":{"12186c22":"code","aa00d792":"code","3d6c459e":"code","86f1eecb":"code","8201f6c3":"code","f5d035c7":"code","78fe3e3a":"code","1a4d5586":"code","eedbeb70":"code","7f786d94":"code","d53fcef8":"code","7c797d28":"code","e64991d1":"code","b485e1f9":"code","60f81639":"code","94c36c57":"code","99994029":"code","43140969":"code","d0c4f657":"code","4b65b35c":"code","02ee2723":"code","178d6020":"code","f3039319":"code","be552d36":"code","196f3b78":"code","7dbf8ed2":"code","ab477152":"code","064db890":"code","007b3613":"code","70b8428d":"code","39fbb02e":"code","daf9b8c9":"code","e93d0ce5":"code","188653a4":"code","aefe99bb":"code","e3de6bec":"code","2e5d6eeb":"code","59fdf378":"code","4cc9a98f":"code","c7d410db":"code","873f1fb0":"code","a8ec5d56":"code","a9972b67":"code","f761d13a":"code","1b39c0aa":"code","0a4a78d5":"code","2ac2e258":"code","d8338705":"code","19f3e437":"code","a1e8a9df":"code","f0fdfeeb":"code","74e86933":"code","7b73855d":"code","c2f8727a":"markdown","efce38da":"markdown","e22859bb":"markdown","6f5ab4fa":"markdown","14653e62":"markdown","ff03592b":"markdown","6ac8a6ff":"markdown","1407db19":"markdown","4d81a9c4":"markdown","f85cfaf1":"markdown","31c8cd5c":"markdown","0126bc85":"markdown","7143a611":"markdown","d6be29ac":"markdown","2159e7e3":"markdown","a5107c5e":"markdown","aa216f7b":"markdown","dec2c31c":"markdown","760b47b7":"markdown","518d3a7d":"markdown","5fe9d31c":"markdown","2d1daa48":"markdown","fc829c37":"markdown","01731ffe":"markdown","fd462454":"markdown","67688385":"markdown"},"source":{"12186c22":"# Import libraries: visualizaiton, text analysis and classifiers\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport cufflinks as cf\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\ncf.go_offline()\nimport plotly.graph_objs as go\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.pipeline import Pipeline\n\nimport nltk\nimport string\nimport nltk.corpus\nimport nltk.stem.snowball\nfrom nltk.corpus import wordnet\n# Get default English stopwords and extend with punctuation\nstopwords = nltk.corpus.stopwords.words('english')\nstopwords.extend(string.punctuation)\nstopwords.append('')\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom wordcloud import WordCloud, ImageColorGenerator\n# You first have to install the worldcloud libraries (like all other ones)\n# I used: conda install -c https:\/\/conda.anaconda.org\/conda-forge wordcloud in the command prompt","aa00d792":"# Load happy dataset\nhappy = pd.read_csv('..\/input\/happydb\/cleaned_hm.csv')\nhappy.head(2)","3d6c459e":"# Load demographic dataset\ndemographic = pd.read_csv('..\/input\/demographic-cleaned\/demographic_cleaned.csv')\n\n# RUN THIS FUNCTION ONLY ONCE AND SAVE demographic_cleaned.csv IN YOUR ACTIVE DIRECTORY\n# clean_string function to remove text string but keep number string, float and integer\n# I ran the clean_string function below only once and saved the cleaned dataframe as demographic_cleaned.csv\n# For all subsequent analysis, I then import demographic_cleaned.csv directly in order to avoid cleaning data each time.\n\n#def clean_string(df, column):\n#    for i in range(0,len(df)):\n#        if type(df[column][i])==str:\n#            df[column][i] = df[column][i].split('.')[0]\n#            if df[column][i].isdigit()==False:\n#                df[column][i]=50\n#    return df\n#demographic = clean_string(demographic, 'age')\n\n#demographic.head(2)\n#demographic.to_csv(\"demographic_cleaned.csv\", index=False)","86f1eecb":"happy.info()","8201f6c3":"demographic.info()","f5d035c7":"#Dataset reduction during development\/testing stages\n#happy = happy[1:50000] #Smaller dataset to test code before final run with full dataset\n#demographic = demographic[1:5000] # Could also use .sample(frac=0.10)","78fe3e3a":"#Create additional column with length of cleaned_hm\nhappy['length'] = happy['cleaned_hm'].apply(lambda x: len(x.split()))\n#happy[happy['reflection_period']== '24h'].head(2)\nhappy.head(2)","1a4d5586":"happy = pd.merge(happy, demographic, on='wid', validate = 'm:1')\n#pd.pivot_table(happy, values='hmid',index='reflection_period', aggfunc='count')\nhappy.head(3)","eedbeb70":"# reflection_period: replace 3m by 1, 24h by 0\nhappy.loc[happy['reflection_period']=='24h', 'reflection_period']=0\nhappy.loc[happy['reflection_period']=='3m', 'reflection_period']=1\nhappy.rename(columns={'3m': '3m or 24h'}, inplace=True)","7f786d94":"# Switching relection_priod column to the end in order to keep all features to analyse together\ncols = happy.columns.tolist()\ncols=['hmid',\n 'wid',\n 'cleaned_hm',\n 'num_sentence',\n 'predicted_category',\n 'length',\n 'age',\n 'country',\n 'gender',\n 'marital',\n 'parenthood',\n 'reflection_period']\nhappy = happy[cols]\nhappy.head(2)","d53fcef8":"pd.pivot_table(happy, values='hmid',index='marital', aggfunc='count')","7c797d28":"# Replace marital with numerical values: married=1 or not-married=0\nhappy.loc[happy['marital']=='single', 'marital']=0\nhappy.loc[happy['marital']=='married', 'marital']=1\nhappy.loc[happy['marital']=='separated', 'marital']=0\nhappy.loc[happy['marital']=='divorced', 'marital']=0\nhappy.loc[happy['marital']=='widowed', 'marital']=0\n#happy[(happy['marital']!=1) & (happy['marital']!=0)]","e64991d1":"happy['marital'].dropna(inplace=True) # drops rows with missing marital values","b485e1f9":"pd.pivot_table(happy, values='hmid',index='marital', aggfunc='count')","60f81639":"pd.pivot_table(happy, values='hmid',index='gender', aggfunc='count')","94c36c57":"# Replace gender with numerical values\nhappy.loc[happy['gender']=='m', 'gender']=0\nhappy.loc[happy['gender']=='f', 'gender']=1\nhappy.loc[happy['gender']=='o', 'gender']=0","99994029":"pd.pivot_table(happy, values='hmid',index='predicted_category', aggfunc='count')","43140969":"# Replace predicted_category by numerical values aligned to 360 Living categories\nhappy.loc[happy['predicted_category']=='achievement', 'predicted_category']=3\nhappy.loc[happy['predicted_category']=='affection', 'predicted_category']=6\nhappy.loc[happy['predicted_category']=='bonding', 'predicted_category']=5\nhappy.loc[happy['predicted_category']=='enjoy_the_moment', 'predicted_category']=2\nhappy.loc[happy['predicted_category']=='exercise', 'predicted_category']=1\nhappy.loc[happy['predicted_category']=='leisure', 'predicted_category']=7\nhappy.loc[happy['predicted_category']=='nature', 'predicted_category']=4\nhappy.head(2)","d0c4f657":"pd.pivot_table(happy, values='hmid',index='parenthood', aggfunc='count')","4b65b35c":"# Replace parenthood\nhappy.loc[happy['parenthood']=='n', 'parenthood']=0\nhappy.loc[happy['parenthood']=='y', 'parenthood']=1","02ee2723":"pd.pivot_table(happy, values='hmid',index='country', dropna=True,aggfunc='count').sort_values('hmid', ascending=False).head(8)","178d6020":"# Replace country by numerical values only for top5 countries, all others are 5\nhappy.loc[happy['country']=='USA', 'country']=1\nhappy.loc[happy['country']=='IND', 'country']=0\nhappy.loc[happy['country']=='VEN', 'country']=0\nhappy.loc[happy['country']=='CAN', 'country']=0\nhappy.loc[happy['country']=='GBR', 'country']=0\npd.pivot_table(happy, values='hmid',index='country', dropna=True,aggfunc='count').sort_values('hmid', ascending=False).head(8)","f3039319":"def convert_string(x):\n    if isinstance(x, str) == True:\n        return 0\n    else: return x\nhappy['country']=happy['country'].apply(lambda x: convert_string(x))\n#happy['country']=int(happy['country'])\n#pd.pivot_table(happy, values='hmid',index='country', dropna=True,aggfunc='count').sort_values('country', ascending=True)\npd.pivot_table(happy, values='hmid',index='country', dropna=True,aggfunc='count').sort_values('hmid', ascending=False).head(8)","be552d36":"happy=happy[happy['age']<150] # Drop rows with age exceeding 150 years\n# Create plotly interactive chart df[column][i].isdigit()==False\nhappy['age'].iplot(kind='hist', bins=50, filename = 'happyDB_Age')","196f3b78":"happy.loc[happy['age']<25,  'age']=0\nhappy.loc[happy['age']>=25, 'age']=1\nhappy.head(1)","7dbf8ed2":"#happy_numerical = happy.drop({'hmid', 'wid', 'country', 'age', 'cleaned_hm', 'num_sentence'}, axis=1)\nfig, (axis1,axis2, axis3) = plt.subplots(1,3,figsize=(16,3))\nsns.distplot(happy['predicted_category'], bins=20, ax=axis1)\nsns.distplot(happy['length'], bins=20, ax=axis2)\nsns.distplot(happy['num_sentence'], bins=20, ax=axis3)","ab477152":"g = sns.FacetGrid(data=happy, col='marital')\ng.map(plt.hist,'predicted_category', bins=50)","064db890":"sns.boxplot(x='predicted_category', y='length', data=happy, palette = 'viridis')","007b3613":"happy2 = happy.drop({'hmid', 'wid', 'num_sentence'}, axis=1)\nsns.heatmap(happy2.groupby('predicted_category').mean().corr(), cmap='coolwarm')","70b8428d":"# Extract ith column of happy dataframe only for values equal to 0 or 1\ni=11 # Selecting feature in this column (counting from 0)\nhappy_class = happy[(happy.ix[:,i]==0) | (happy.ix[:,i]==1)]\nX=happy_class['cleaned_hm']\ny=happy_class.ix[:,i]\ny = np.asarray(y,dtype=np.float64) # Convert y to float64 format","39fbb02e":"CV = CountVectorizer()","daf9b8c9":"X = CV.fit_transform(X) # Create vectors with frequency info for each rating text description","e93d0ce5":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)","188653a4":"nb = MultinomialNB()","aefe99bb":"nb.fit(X_train, y_train)","e3de6bec":"predictions = nb.predict(X_test)\nacc_nb = nb.score(X_test, y_test) # Return the mean accuracy\nprint('Classification Report \\n',classification_report(y_test, predictions))\nprint('\\n Confusion Matrix')\ncm = pd.DataFrame(confusion_matrix(y_test, predictions), ['Actual: 0', 'Actual: 1'], ['Predicted: 0', 'Predicted: 1'])\nprint(cm)","2e5d6eeb":"acc_nb","59fdf378":"pipeline = Pipeline([\n    ('bow', CountVectorizer()),\n\t('tfidf', TfidfTransformer()),\n\t('Classifier', MultinomialNB())\n\t])","4cc9a98f":"# Extract ith column of happy dataframe only for values equal to 0 or 1\ni=7 # Selecting feature in this column (counting from 0)\nhappy_class = happy[(happy.ix[:,i]==0) | (happy.ix[:,i]==1)]\nX=happy_class['cleaned_hm']\ny=happy_class.ix[:,i]\ny = np.asarray(y,dtype=np.float64) # Convert y to float64 format\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)","c7d410db":"pipeline.fit(X_train, y_train)","873f1fb0":"predictions = pipeline.predict(X_test)\nprint('Classification Report \\n',classification_report(y_test, predictions))\nprint('\\n Confusion Matrix')\ncm = pd.DataFrame(confusion_matrix(y_test, predictions), ['Actual: 0', 'Actual: 1'], ['Predicted: 0', 'Predicted: 1'])\nprint(cm)","a8ec5d56":"# Apply MultinomialNB classifier to other features, save scores for display\n# Same logic as above but for each feature\naccuracies_data=np.arange(5.0) # Create dataframe accuracies_data to store all scores\n\nfor i in range(7,12):\n    # 1 Extract column i of happy dataframe only for values equal to 0 or 1, then split test from train data\n    happy_class = happy[(happy.ix[:,i]==0) | (happy.ix[:,i]==1)]\n    X = happy_class['cleaned_hm'] # Text column only\n    y = happy_class.ix[:,i]\n    y = np.asarray(y, dtype=np.float64)\n    # Create vectors with frequency info for each text description\n    CV = CountVectorizer()\n    X  = CV.fit_transform(X) # creates vectors with frequency info for each rating text description\n    \n    # 2 Train Test Split, create instance and train classifier\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n    nb = MultinomialNB()\n    nb.fit(X_train, y_train)\n\n    # 3 Apply predictor for test data\n    predictions = nb.predict(X_test)\n    accuracies_data[i-7] = nb.score(X_test, y_test)\naccuracies_data","a9972b67":"# Display results\naccuracies_data = accuracies_data.round(4) # Round all values to 4 digits\nobjects = ('country (USA or not)',\n           'gender (female or male\/other)',\n           'marital (married or not)',\n           'parenthood (yes or no)',\n           'Reflexion (3 months or 24 hours)')\ny_pos = np.arange(len(objects))   \nplt.figure(figsize=(12,3))\nplt.barh(y_pos, accuracies_data, left = 0, align='center', alpha=0.5,\n         color=['green', 'grey', 'purple', 'orange', 'red'], tick_label= accuracies_data)\nplt.yticks(y_pos, objects, rotation='horizontal')\nplt.xticks(np.arange(0.5, 1, step=0.05))\n#plt.axes([0.2,0.1,0.9,0.9])\nplt.xlabel('Accuracy (f1-score)')\nfor i, v in enumerate(accuracies_data):\n    plt.text(1, i, str(v), color='black', bbox=dict(facecolor='white', alpha=0.5))\nplt.title('Classifier Outcome')\nplt.show()","f761d13a":"happy.head(2)","1b39c0aa":"happy = pd.read_csv('..\/input\/happydb\/cleaned_hm.csv')\nhappy=happy.head(5000) # Development\/Testing: 5000\ndemographic = pd.read_csv('..\/input\/demographic-cleaned\/demographic_cleaned.csv')\n#demographic = demographic[1:50000] #.sample(frac=0.1\n# List of common words to remove, many thanks Chen-Chen for the initial inputs\nnolist = ['happy', 'day', 'got', 'went', 'today', 'made', 'one', 'two', 'time', 'last', 'first', 'going',\n'getting', 'took', 'found', 'lot', 'really', 'saw', 'see', 'month', 'week', 'day', 'yesterday',\n'year', 'ago', 'now', 'still', 'since', 'something', 'great', 'good', 'long', 'thing', 'toi', 'without',\n'yesteri', '2s', 'toand', 'ing', 'got', 'came', 'could', 'happiness', 'new', 'able', 'finally', 'like',\n'old', 'years', 'many', '2', 'get', 'taj', 'nice', 'top', 'back']\n\nhappy = pd.merge(happy, demographic, on='wid')\nhappy.drop('age', axis=1, inplace=True)\n\n#happy=happy[happy['age']<150] # Drop rows with age exceeding 150 years\n#happy.loc[happy['age']<25,  'age'] = 0\n#happy.loc[happy['age']>=25, 'age'] = 1","0a4a78d5":"def text_process(mess):\n    \"\"\"\n    Takes in a string of text, then performs the following:\n    1. Remove all punctuation\n    2. Remove all stopwords\n    3. Returns a list of the cleaned text\n    \"\"\"\n    # Check characters to see if they are in punctuation\n    nopunc = [char for char in mess if char not in string.punctuation]\n    # Join the characters again to form the string.\n    nopunc = ''.join(nopunc)   \n    # Now just remove any stopwords\n    return [word for word in nopunc.split()\n    if ((word.lower() not in stopwords) & (word.lower() not in nolist))]\n# Apply to entire happy dataset, column cleaned_hm\nhappy['cleaned_hm'] = happy['cleaned_hm'].apply(text_process) #Sample of 10,000 rows\nhappy.head(1)","2ac2e258":"# Convert list into string by joining words with a space\nfor i in range(0, len(happy)):\n    happy['cleaned_hm'][i] = ' '.join(happy['cleaned_hm'][i])","d8338705":"text = ' '.join(happy['cleaned_hm'].tolist())\nwordcloud = WordCloud(background_color=\"white\", height=2200, width=4000).generate(text)\nplt.figure( figsize=(18,7) )\nplt.imshow(wordcloud.recolor(colormap=plt.get_cmap('Set2')), interpolation='bilinear')\nplt.axis(\"off\")","19f3e437":"k = 8\n# Extract unique values of predicted_category and add to a list in descending orders (by count of responses)\npivot_category = pd.pivot_table(happy, values='hmid',index=happy.ix[:,k], aggfunc='count')\npivot_category.sort_values('hmid', axis=0, ascending=False, inplace=True, kind='quicksort', na_position='last')\nCategory_labels =pivot_category.T.columns.tolist()\nif len(Category_labels)>10:\n    Category_labels = Category_labels[0:9]\nCategory_labels","a1e8a9df":"# Select text from this sub-category j (from above list) and convert into a string with spaces\nj=1\ntext = ' '.join(happy[happy.ix[:,k]==Category_labels[j]]['cleaned_hm'].tolist())","f0fdfeeb":"# Find frequency of each word in this text\n# break the string into list of words, reset variables str2 and str3\nstr = text.split()         \nstr2 = []\nstr3 = pd.DataFrame([['A','B']], [Category_labels[j]], ['Word','Frequency'])\n \n# Check for duplicate by looping until string values is not present in str\nfor word in str:\n    if (word not in str2) & ((word+'s') not in str2):\n        str2.append(word) # insert value in str2\n\n# Create dataframe to store and report results\nstr3.ix[0][0]=str2[0]\nstr3.ix[0][1]=str.count(str2[0]) \/ len(text)\nfor i in range(1, len(str2)):\n    # count the frequency of each word(present # in str2 and add with word to DataFrame\n    freq = str.count(str2[i]) \/ len(text)\n    str4 = pd.DataFrame([[str2[i],freq]], [Category_labels[j]], ['Word','Frequency'])\n    str3 = str3.append(str4)  \n\nstr3.sort_values(by = 'Frequency', ascending=False, inplace=True)\nstr4 = str3.head(10)","74e86933":"objects = str4['Word']\nx_pos = np.arange(len(objects))   \nplt.figure(figsize=(15,3))\ncolors = tuple(np.where(str4['Frequency']>(0.66*str4['Frequency'].max()), 'g', 'orange'))\nplt.bar(x_pos, str4['Frequency'], align='center', alpha=0.5,\n         color=colors)\nplt.ylim(0, str4['Frequency'].max())\nplt.xticks(x_pos, objects, rotation=90)\nplt.xlabel('Most frequent words')\nplt.ylabel('Frequency in this sub-category')\n#for i, v in enumerate(str4['Frequency']):\n#plt.text(1, i, str(v), color='black', bbox=dict(facecolor='white', alpha=0.5))\nplt.title('Keywords in sub-category: {}'.format(Category_labels[j]))\nplt.show()","7b73855d":"for k in range(8, 12): # Feature k\n    \n    # Extract unique values of predicted_category and add to a list in descending orders (by count of responses)\n    pivot_category = pd.pivot_table(happy, values='hmid',index=happy.ix[:,k], aggfunc='count')\n    pivot_category.sort_values('hmid', axis=0, ascending=False, inplace=True, kind='quicksort', na_position='last')\n    Category_labels =pivot_category.T.columns.tolist()\n    add_title = ''\n    if len(Category_labels)>10:\n        Category_labels = Category_labels[0:8]\n        add_title = '(Top 8)'\n\n    j = 0 # Labels j for selected features\n    str5={0:[]}\n    fig = plt.figure(figsize=(18, 8))\n    plt.title('Feature: '+happy.columns[k])\n    fig.subplots_adjust(hspace=0.4, wspace=0.3)\n\n    #fig, axis = plt.subplots(1,len(Category_labels),figsize=(18,3))\n    for j in range(0, len(Category_labels)):\n        l=j+1\n        text = ' '.join(happy[happy.ix[:,k]==Category_labels[j]]['cleaned_hm'].tolist())\n        str = text.split()         \n        str2 = []\n        str3 = pd.DataFrame([['A','B']], [Category_labels[j]], ['Word','Frequency'])\n    \n    # Check for duplicate by looping until string values is not present in str\n        for word in str:\n            if (word not in str2) & ((word+'s') not in str2):\n                str2.append(word) # insert value in str2\n\n    # Create dataframe to store and report results\n        str3.ix[0][0]=str2[0]\n        str3.ix[0][1]=str.count(str2[0]) \/ len(text)\n        for i in range(1, len(str2)):\n            # count the frequency of each word(present # in str2 and add with word to DataFrame\n            freq = str.count(str2[i]) \/ len(text) \n            str4 = pd.DataFrame([[str2[i],freq]], [Category_labels[j]], ['Word','Frequency'])\n            str3 = str3.append(str4)  \n\n        str3.sort_values(by = 'Frequency', ascending=False, inplace=True)\n        str4 = str3.head(10)\n        str5.update({j: str4}) #add result to the list str 5 to create graphs in next section \n    \n        # Graphing\n        plt.subplot(2, 4, l)\n        objects = str5[j]['Word']\n        x_pos = np.arange(len(objects))   \n        colors = tuple(np.where(str5[j]['Frequency']>(0.66*str5[j]['Frequency'].max()), 'g', 'orange'))\n        sns.barplot(x=x_pos, y=str5[j]['Frequency'], palette=colors, alpha=0.6, data=happy);\n        plt.text(0, (str5[j]['Frequency'].max()*1.05), 'Label: ' + Category_labels[j]\n        +'\\nFeature: '+happy.columns[k]+' '+add_title)\n        plt.ylim((str5[j]['Frequency'].max()*0.1), (str5[j]['Frequency'].max()*1.2))\n        plt.xticks(x_pos, objects, rotation=90)\n\nplt.show()","c2f8727a":"## 1. Exploratory Data Analysis (EDA) and feature engineering\n### 1.1 Dataset size and description","efce38da":"## 1.4 Converting categorical features into numerical values\n### reflection_period","e22859bb":"# HappyDB: applying data science to analyze 100,000 happy moments\n\n## Introduction\n\n### Dataset\nHappyDB is a dataset of more than 100,000 happy moments crowd-sourced via Amazon\u2019s Mechanical Turk.\n\nEach worker was given the following task:\n\n1. What made you happy today?\n2. Reflect on the past 24 hours, and recall three actual events that happened to you that made you happy.\n3. Write down your happy moment in a complete sentence. (Write three such moments.)\n\nThe second dataset contains demographic information of the respondents: age, gender, marriage status, etc.\n\n### Objectives\nThe goal of this notebook is to analyze these happy moments and advance the understanding of the causes of happiness by applying data analysis and machine-learning techniques.\n\n\n### Table of content\nThere are three main sections:\n* Sections 1 and 2 conduct some Exploratory Data Analysis (EDA), feature engineering and descriptive analysis;\n* Sections 3 to 5 apply a MultinomialNB Classifier model to predict gender, marriage status, parenthood, age and country based on the happy moment text content, and using Term Frequency, Inverse Document Frequency (TFIDF);\n* Section 6 extracts meaningful keywords by type of responses and respondents' characteristics using Python's standard Text Processing techniques.\n\n### In summary\n- Prediction model:\n    - The happy moments text content allows to **predict the country** (USA or not) and **age** (younger than 25 or not) with **prediction accuracies greater than 0.80**.\n    - The other features can be predicted with a **precision between 0.60 and 0.70** (not that bad, but not as good).\n- The last sections **summarizes what makes up happy moments**:\n    - By category\n    - By country\n    - By gender\n    - By marital status\n    - By parenthood\n\nThis notebook expands the initial analysis from Chen Chen in his very good kernel from Chen Chen titled HappyDB analysis.\n### Check other Kaggle notebooks from [Yvon Dalat](https:\/\/www.kaggle.com\/ydalat):\n* [Titanic, a step-by-step intro to Machine Learning](https:\/\/www.kaggle.com\/ydalat\/titanic-a-step-by-step-intro-to-machine-learning): **a practice run ar EDA and ML-classification**\n* [HappyDB, a step-by-step application of Natural Language Processing](https:\/\/www.kaggle.com\/ydalat\/happydb-what-100-000-happy-moments-are-telling-us): **find out what 100,000 happy moments are telling us**\n* [Work-Life Balance survey, an Exploratory Data Analysis of lifestyle best practices](https:\/\/www.kaggle.com\/ydalat\/work-life-balance-best-practices-eda): **key insights into the factors affecting our work-life balance**\n*  [Work-Life Balance survey, a Machine-Learning analysis of best practices to rebalance our lives](https:\/\/www.kaggle.com\/ydalat\/work-life-balance-predictors-and-clustering): **discover the strongest predictors of work-life balance**\n\n**Interested in more facts and data to balance your life, check https:\/\/amzn.to\/2MFO6Iy\n![360%20Living%20guide.jpg](attachment:360%20Living%20guide.jpg)**\n\n\n## Import libraries","6f5ab4fa":"## 4.1 Preparing data","14653e62":"# 6 Text Processing and Analysis\n## 6.1 Pre-Processing","ff03592b":"# 2. Descriptive Analysis\n## 2.1 Distribution\n### age distribution","6ac8a6ff":"### marital status","1407db19":"### predicted_category","4d81a9c4":"### gender","f85cfaf1":"**Observation**:\n- Most responses are in categories 3 \"Achievement\" and 6 \"Affection\"\n- All messages, except for some outliers, are less than 100 words, and less than 5 sentences\n\n## 2.2 Bi-variate analysis","31c8cd5c":"## 1.2 Text length (count of words)","0126bc85":"#### Observations: using Term Frequency, Inverse Document Frequency (TFIDF) does not improve the quality of the prediction for this dataset.\n\n# 5 Evaluating prediction accuracies for multiple features","7143a611":"# 3. MultinomialNB Classifier for selected feature\n## 3.1 Preparing data","d6be29ac":"## Load datasets","2159e7e3":"### parenthood and country","a5107c5e":"## 6.2 Word Cloud","aa216f7b":"# 4 Prediction using Term Frequency, Inverse Document Frequency (TFIDF), then applying the same MultinomialNB Classifier\n- TF-IDF stands for term frequency-inverse document frequency.\n- The tf-idf weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus.\n- The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus.\n- Variations of the tf-idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query.","dec2c31c":"## 4.3 Predictions","760b47b7":"## 5.1 Predictions for country,\tgender,\tmarital, parenthood, 3m or 24h\n- 6th column   age:        0 younger than 25, 1 older than 25\n- 7th column   country:    1 USA, O outside of USA (India, Venezuela, Canada, GBR)\n- 8th column   gender:     1 female, 0 male excluding others\n- 9th column   marital:    1 married, 0 single or divorced or separated or widowed\n- 10th column  parenthood: 1 yes, 0 no\n- 11th column  3m or 24h:  1 3 months, 0 24 hours","518d3a7d":"**Observation**: age feature is close to a normal Gaussian distrubution, slightly skewed to the right.\n\n### Distribution of predicted_category, length and num_sentence","5fe9d31c":"## 6.3 Key words analysis for selected label of predicted_category feature\nThis section extracts the 10 most frequently used keywords (excl. punctuations and stop words) for a selected label  (number j) of the predicted_category.","2d1daa48":"## 1.3 Merging datasets","fc829c37":"## 6.3 Key words analysis for all labels of multiple features\nThe goal in this section is to extract the most frequent, meaningful keywords:\n- For selected features: predicted_category, country, gender, marital,parenthood\n- And within each feature, for each possible value or label. I limit countries to the top8 respondents, as the list is too long. ","01731ffe":"## 3.2 Train Test Split and training model","fd462454":"## 3.3 Predictions and model evalaution","67688385":"## 4.2 Train Test Split and training model"}}