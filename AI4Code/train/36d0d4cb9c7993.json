{"cell_type":{"c9f802c6":"code","0004fe47":"code","460e157d":"code","f15ceea8":"code","da381e5b":"code","54fbe214":"code","a32058af":"code","56ceca83":"code","982b9aa6":"code","e2dfa7e5":"code","e7b91c49":"code","ef632574":"markdown","c9b44bec":"markdown","ff36ad21":"markdown","8cc03e51":"markdown","7b68b61d":"markdown","7dad0596":"markdown"},"source":{"c9f802c6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0004fe47":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs","460e157d":"X,Y=make_blobs(n_samples=500,centers=2,n_features=2,random_state=3)","f15ceea8":"print(X.shape,Y.shape)","da381e5b":"plt.style.use(\"seaborn\")\nplt.scatter(X[:,0],X[:,1],c=Y,cmap=plt.cm.flag)\nplt.xlim(-8,4)\nplt.ylim(-4,8)\nplt.show()","54fbe214":"def sigmoid(z):\n    return (1.0)\/(1.0+(np.exp(-z)))\n\ndef predict(X,w):\n    z=np.dot(X,w)\n    predictions=sigmoid(z)\n    return predictions\n\ndef loss(X,Y,w):\n    #also called binary cross entropy\n    Y_=predict(X,w)\n    cost=np.mean((-Y*np.log(Y_)-(1-Y)*(np.log(1-Y_))))\n    return cost\n\ndef update(X,Y,w,lr):\n    #for one epoch\n    Y_=predict(X,w)\n    dw=np.dot(X.T,Y_-Y)\n    m=X.shape[0]\n    w=w-lr*dw\/float(m)\n    return w\n\ndef train(X,Y,lr=0.5,max_epochs=100):\n    #modifing the input to handle the bias term\n    ones=np.ones((X.shape[0],1))\n    X=np.hstack((ones,X))\n    \n    # initiating with some random weights w\n    \n    w=np.zeros(X.shape[1])# n+1 entries\n    \n    #iterate overall epochs and make updates\n    for epoch in range(max_epochs):\n        w = update(X,Y,w,lr)\n        \n        if epoch % 10 == 0:\n            l=loss(X,Y,w)\n            \n            print(\"epoch %d loss %.4f\"%(epoch,l))\n        \n    return w","a32058af":"w = train(X,Y,lr=0.9,max_epochs=3500)\nprint(w)","56ceca83":"def getPrediction(X_test,w,labels=True):\n    if X_test.shape[1] != w.shape[0]:\n        ones=np.ones((X_test.shape[0],1))\n        X_test=np.hstack((ones,X_test))\n        \n    probs = predict(X_test,w)\n    \n    if not labels:\n        return probs\n    \n    else:\n        labels=np.zeros(probs.shape)\n        labels[probs >=0.5] = 1\n        \n    return labels\n","982b9aa6":"x1=np.linspace(-8,2,10)\n#generate equally distant 10 points ranging from -8 to 2\nx2 = -(w[0]+w[1]*x1)\/w[2]","e2dfa7e5":"plt.scatter(X[:,0],X[:,1],c=Y,cmap=plt.cm.flag)\nplt.xlim(-8,4)\nplt.ylim(-4,8)\nplt.plot(x1,x2,c=\"blue\")\nplt.show()","e7b91c49":"Y_=getPrediction(X,w,labels=True)\ntraining_acc=np.sum(Y_==Y)\/Y.shape[0]\n\nprint(training_acc)","ef632574":"## Formation of Model and helper functions","c9b44bec":"### ploting the line in the scatter plot","ff36ad21":"## Finding accuracy","8cc03e51":"## Generating Data","7b68b61d":"Both single layer perceptron and logistic regression are used to separate linearly separable Datasets","7dad0596":"## visualization of decision surface"}}