{"cell_type":{"b17ef934":"code","3ffa380e":"code","06626298":"code","126db8c3":"code","6461a7a7":"code","52de0213":"code","12663436":"code","e365e820":"code","e561826e":"code","26b5228a":"code","b0f303c3":"code","0f6dcdf5":"code","450966e5":"code","5898ec92":"code","9da9afe5":"code","71666e59":"markdown","55909766":"markdown","7b30f930":"markdown","36dd0947":"markdown","858119e6":"markdown","b8d51ba4":"markdown","38ab5bb2":"markdown","c34fd511":"markdown","35a27f91":"markdown","43b468fe":"markdown","03f7090e":"markdown","6c2a30ac":"markdown","0790c668":"markdown","0718bc74":"markdown","0b37ff4a":"markdown"},"source":{"b17ef934":"# The input files are Sheet_1 & Sheet_2.\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","3ffa380e":"# Sheet_1 is the responses to the therapy bot\n\ndf = pd.read_csv('..\/input\/Sheet_1.csv', encoding='latin-1')\n\n# Print a good looking dataframe\n\ndf = df.drop([\"Unnamed: 3\", \"Unnamed: 4\", \"Unnamed: 5\", \"Unnamed: 6\",\"Unnamed: 7\"], axis = 1)\ndf = df.rename(columns={\"v1\":\"class\", \"v2\":\"Responses\"})\n\ndf.head()","06626298":"df[\"class\"].value_counts()","126db8c3":"# Sheet_2 is the resumes.\n\ndf2 = pd.read_csv('..\/input\/Sheet_2.csv', encoding='latin-1')\n\ndf2 = df2.rename(columns={\"v1\":\"class\", \"v2\":\"Resumes\"})\n\ndf2.head()","6461a7a7":"df2[\"class\"].value_counts()","52de0213":"\n\n## Bag of response words\n\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n\ndef wordcloud(dataframe):\n    stopwords = set(STOPWORDS)\n    wordcloud = WordCloud(background_color=\"white\",stopwords=stopwords).generate(\" \".join([i for i in dataframe.str.upper()]))\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.title(\"bag_composition\")\n\nwordcloud(df['response_text'])\n\n# Looks positive overall, 'friend' is largest, because the bot said: 'Describe a time when you\n# have acted as a resource for someone else.'","12663436":"# Bag of resume words\n\nwordcloud(df2['resume_text'])  \n\n# Looks reasonable, 'data' and 'research' are largest","e365e820":"import operator\nfrom pprint import pprint\n\ndef word_freq_bag(dataframe):\n\n    counts = dict()\n    bag = []\n    counter = 0\n    for line in dataframe:\n        # print(line)\n        words = line.split()\n        # print(words)\n        for each in words:\n            bag.append(each)\n    # print(bag)\n    for word in bag:\n        # print(word)\n        # print(counts.get(word,0))\n        counts[bag[counter]] = counts.get(word,0)+1        \n        #print(counts)\n        counter += 1\n        \n        # if counter == 50: # if you uncomment the print statements, use break (limited memory)\n         #   break \n\n    key = None\n    value = None\n    keyvalue = dict() # would like to sort by count, dicts are orderless\n    for key in counts:\n        value = counts[key]\n        keyvalue[key] = value\n        # rank in descending order of frequency, get a list of tuples sorted by the second\n        # element in each tuple\n        valueSort1 = sorted(keyvalue.items(), key=operator.itemgetter(1), reverse = True)\n        valueSort0 = sorted(keyvalue.items(), key=operator.itemgetter(0), reverse = False)\n    # print(counter) # total number of non-unique words\n    # print(len(valueSort1)) # every key has a value\n    # print(len(valueSort0)) # so these return the same number\n    return(valueSort1, valueSort0, counts) \n\n    \nresponse_word_freq_bag = word_freq_bag(df['response_text'])\n# take a look at the words picked out in ascending order of 1st letter \nprint(response_word_freq_bag[1])","e561826e":"# Print the word frequencies in the bag in descending order. \nprint(response_word_freq_bag[0]) # you can confirm with a find (& replace) tool. E.g. R IDE","26b5228a":"# Compute & view for the resumes, if you have a few minutes\n# resume_word_freq_bag = word_freq_bag(df2['resume_text']) # Compute the word frequences  \n# print(resume_word_freq_bag[0]) # See the most common words (this is commented out, because\n# there are 93177 non-unique words (and 1\/5th of that number, ~17k, unique words)\n\n# Here is the output down to occurences of 50, if you don't want to uncomment it and run it - \n# [('and', 4629), ('to', 2522), ('of', 2430), ('-', 2021), \n#  ('the', 1706), ('\\x8a\\x97\u00a2', 1685), ('in', 1683), ('for', 1381), \n#  ('a', 735), ('with', 709), ('on', 637), ('VT', 448), ('as', 354), \n#  ('University', 353), ('data', 346), ('*', 308), ('Research', 275), \n#  ('by', 238), ('I', 235), ('research', 221), ('at', 214), \n#  ('from', 212), ('&', 192), ('including', 185), ('May', 181), \n#  ('analysis', 178), ('using', 173), ('all', 172), ('work', 171), \n#  ('Vermont', 170), ('June', 166), ('an', 165), ('development', 163), \n#  ('2013', 162), ('August', 161), ('new', 151), ('Assistant', 150), \n#  ('Science', 149), ('me', 147), ('Burlington', 143), ('Developed', 143), \n#  ('2014', 142), ('that', 142), ('EXPERIENCE', 132), ('management', 131), \n#  ('College', 128), ('New', 127), ('laboratory', 127), ('software', 126), \n#  ('Email', 125), ('Indeed:', 125), ('WORK', 125), ('Present', 125), \n#  ('EDUCATION', 123), ('2011', 123), ('Environmental', 120), ('was', 119), \n#  ('Engineering', 118), ('o', 114), ('quality', 114), ('2012', 112), \n#  ('support', 112), ('2009', 112), ('January', 111), ('project', 111), \n#  ('students', 111), ('Laboratory', 108), ('team', 107), ('2007', 107), \n#  ('or', 105), ('Scientist', 105), ('2010', 103), ('design', 99), \n#  ('equipment', 99), ('technical', 99), ('September', 99), ('2015', 97), \n#  ('Department', 96), ('2008', 96), ('NY', 95), ('testing', 94), \n#  ('process', 93), ('2005', 93), ('December', 89), ('State', 89), \n#  ('years)', 89), ('other', 88), ('Data', 88), ('Center', 88), \n#  ('The', 87), ('2006', 86), ('experience', 85), ('my', 83), ('is', 81), \n#  ('system', 81), ('water', 81), ('use', 80), ('Biology', 80), ('lab', 79), \n#  ('skills', 78), ('2016', 78), ('MA', 78), ('Medical', 77), ('projects', 76), \n#  ('systems', 76), ('2004', 76), ('March', 75), ('US', 75), \n#  ('ADDITIONAL', 74), ('field', 74), ('business', 74), ('INFORMATION', 73), \n#  ('Development', 73), ('2001', 73), ('reports', 72), ('through', 72), \n#  ('production', 72), ('test', 72), ('product', 71), ('School', 70), \n#  ('April', 70), ('various', 69), ('training', 69), ('materials', 68), \n#  ('samples', 68), ('environmental', 68), ('Engineer', 67), ('SKILLS', 67), \n#  ('October', 66), ('NH', 66), ('Responsible', 65), ('time', 65), \n#  ('July', 64), ('Health', 63), ('into', 62), ('Manager', 62), ('2003', 62), \n#  ('\/', 62), ('Assisted', 61), ('Microsoft', 61), ('Performed', 61), \n#  ('customer', 60), ('any', 60), ('years', 59), ('Senior', 59), ('National', 59), \n#  ('2000', 58), ('based', 58), ('well', 57), ('Provided', 57), ('IBM', 57), \n#  ('working', 57), ('Analysis', 57), ('technology', 57), ('techniques', 57), \n#  ('included', 57), ('February', 56), ('were', 56), ('their', 56), ('Project', 56),\n#  ('control', 56), ('this', 56), ('study', 56), ('cell', 56), ('information', 55), \n#  ('Inc', 55), ('multiple', 55), ('applications', 55), ('program', 55),\n#  ('Design', 55), ('company', 54), ('Technician', 53), ('Management', 53),\n#  ('products', 53), ('develop', 53), ('1999', 53), ('during', 52), \n#  ('scientific', 52), ('Managed', 51), ('be', 51), ('Worked', 51), \n#  ('Chemistry', 51), ('staff', 51), ('Responsibilities', 51), \n#  ('professional', 50), ('manufacturing', 50), ('such', 50),","b0f303c3":"# This is a little bit of an unconventional training set. The bag of words model does not\n# pick out features of individual training examples that caused them to be identified as a\n# particular class. Instead, it has an aggregate collection of words from all training examples\n# identified as a particular class. It classifieds new examples based on this aggregate.\n\ntrain_df = df.sample(frac = 0.5, axis=0)\ntest_df = df.sample(frac = 0.5, axis=0)\n\ntrain_df2 = df2.sample(frac = 0.5, axis=0)\ntest_df2 = df2.sample(frac = 0.5, axis=0)\n\ntrain_df_flagged = train_df.loc[train_df['class'] == 'flagged']\ntrain_df_flagged_bag = word_freq_bag(train_df_flagged['response_text'])\nprint(train_df_flagged_bag[0]) # for example, here you can see the flagged responses' words\ntrain_df_not_flagged = train_df.loc[train_df['class'] == 'not_flagged']\ntrain_df_not_flagged_bag = word_freq_bag(train_df_not_flagged['response_text'])\ntrain_df2_flagged = train_df2.loc[train_df2['class'] == 'flagged']\ntrain_df2_flagged_bag = word_freq_bag(train_df2_flagged['resume_text'])\ntrain_df2_not_flagged = train_df2.loc[train_df2['class'] == 'not_flagged']\ntrain_df_not_flagged.tail() # it's a random sample, so the entries are not ordered, but\n# you can see that you are indexing what you want - here's a subset of not_flagged responses\ntrain_df2_not_flagged_bag = word_freq_bag(train_df2_not_flagged['resume_text'])\n\n# You can see printed out first is the number of non-unique words & the number of unique words\n# The number of unique words is printed twice, this was to check that each list was the same\n# length returned from the word_freq_bag function.\n# All good.","0f6dcdf5":"# You can use the print statements in this code below here to get a clearer picture.\n\n# to convert a string to a dataframe with pandas, in order to reuse word_freq_bag from above\nimport io\nimport sys\n\n# mydict1={'response:2,'response2:2,'response6':2,'response8':1}\n# mydict2={'response':1,'response2':5,'response8':7}\n# mykey=[sum(value * mydict1[key] for key,value in mydict2.items())]\n# print(mykey)\n\ndef score(a, b):\n    mykey=[sum(value * a.get(key,0) for key,value in b.items())]\n    # print(b.items())\n    # print(a)\n    # print(b)\n    # global score_counter\n    # score_counter += 1\n    # if score_counter == 10: # to limit memory so you can run without needing to restart\n    #     sys.exit()\n    return(mykey)\n\ndef classifier_accuracy(text_type, text_id, test_dataframe, train_dataframe_flagged,\n                        train_dataframe_flagged_bag, \n                        train_dataframe_not_flagged_bag, length):\n    counter = 0\n    false_positive = 0\n    false_negative = 0\n    true_positive = 0\n    true_negative = 0\n    random_guess = 1-len(train_dataframe_flagged)\/length\n    test_dataframe_flagged = test_dataframe.loc[test_dataframe['class'] == 'flagged']\n    positive_text = len(test_dataframe_flagged)\n    print(positive_text)\n    test_dataframe_not_flagged = test_dataframe.loc[test_dataframe['class'] == 'not_flagged']\n    negative_text = len(test_dataframe_not_flagged)\n    print(negative_text)\n    \n    # print(train_dataframe_flagged_bag[2])\n    for i in test_dataframe[text_id]:\n        split_char = i.split('_') # get id from resume_id\n        integer = int(float(split_char[1])) -1 # acess by index, one less that resume_id\n        text_class = test_dataframe['class'][integer]\n        # print(text_class)\n        # print(text_class)\n        text = test_dataframe[text_type][integer]\n        # convert string to dataframe, make more than wide enough for the text column\n        df3 = pd.read_fwf(io.StringIO(text), header=None, widths=[1000000], names=[text_type])\n        # df3 # if you want to see the dataframe for the response\/resume\n        text_bag = word_freq_bag(df3[text_type]) \n        # print(text_bag[2])\n        # print(\"ok, here goes:\")\n        \n        \n        score_flagged = score(text_bag[2], train_dataframe_flagged_bag[2]) # 2 is counts dict\n        #print(score_flagged)\n        score_not_flagged = score(text_bag[2], train_dataframe_not_flagged_bag[2])\n        #print(score_not_flagged)\n        if score_flagged > score_not_flagged and text_class == 'flagged':\n            counter += 1\n            true_positive += 1\n        elif score_flagged <= score_not_flagged and text_class == 'not_flagged':\n            counter += 1\n            true_negative += 1\n        elif score_flagged > score_not_flagged and text_class == 'not_flagged':\n            counter += 0\n            false_positive += 1\n        elif score_flagged <= score_not_flagged and text_class == 'flagged':\n            counter += 0\n            false_negative += 1\n    return('number of test examples: ' + str(length), 'correct identifications: ' + str(counter), 'pick_randomly_performace: ' + str(random_guess), 'model_performance: ' + str(counter\/length), 'false negatives: ' + str((positive_text-false_negative)\/positive_text), 'false positives: ' + str((positive_text-true_positive)\/positive_text), 'false positive count: ' + str(false_positive) , 'false negative count: ' + str(false_negative))\n\nscore_counter = 0\nprint(classifier_accuracy('response_text', 'response_id', test_df, train_df_flagged, train_df_flagged_bag, train_df_not_flagged_bag, 40))\nprint(classifier_accuracy('resume_text', 'resume_id', test_df2, train_df2_flagged, train_df2_flagged_bag, train_df2_not_flagged_bag, 62))\n    ","450966e5":"# converting string of words in a response, for example, into a data frame (Appendix work)\n\n# text = test_df['response_text'][0]\n# df3 = pd.read_fwf(io.StringIO(text), header=None, widths=[1000000], names=['response_text'])\n# df3\n# text_bag = word_freq_bag(df3['response_text'])\n# text_bag\n\n# This is the summing in the bag\n\nmydict1={'response':2,'response2':2,'response6':2,'response8':1}\nmydict2={'respose':1,'respone':5,'response8':7}\nmykey=[sum(value * mydict1.get(key,0) for key,value in mydict2.items())]\nprint(mykey)","5898ec92":"# To see the effect of CountVectorizer, you can note the number of words \n# now in the pool of words versus seperating words by hand:\nfrom sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer()\ntrain_df = df.sample(frac = 1, axis=0)  # set this to 0.5, you'll have a \n                                        # test set of 40 responses\ntrain_df_mat = vect.fit_transform(train_df['response_text'])\nprint(train_df_mat.shape) \n# to split into a train and test set, make the sample frac = 0.5, and you will get\n# ~ 250-500 words in a sum total of half of the responses\n# list of words represented in this train set\nprint(vect.get_feature_names())\n# confirm number of words is 660 (and note the omission of the 832 - 660 = 172 words that were \n# caught with CountVectorizer. For example, below \"themselves\" & \"themselves,\" are rolled\n# together into one \"themselves\".) \n# This is a benefit of sklearn.\nprint(len(vect.get_feature_names()))","9da9afe5":"# repeat for resume set\n# Again, CountVectorizer, significantly decreases the number of words (down from ~17k to ~11k).\nvect2 = CountVectorizer()\ntrain_df2 = df2.sample(frac = 1, axis=0) # if the frac is changed to 0.5, \n                                         # get 62 resumes (0.5(floor(125))) \u2714\ufe0f\ufe0f\n                                         # ~ 6-9k words in a sum total of half \n                                         # of the resumes, depending on the sample  \ndf2_mat = vect2.fit_transform(train_df2['resume_text'])\nprint(df2_mat.shape)  \n\n# With the full pool of resumes, here we see CountVectorizer finds 11466 unique words.","71666e59":"# 4) Create the bags of words. \n##There will be 4 bags - one of flagged responses, one of not_flagged responses, one of flagged resumes, and one of not_flagged resumes.","55909766":"Then, You're going to multiply together the counts of each response\/resume of words that exist both in a new response\/resume and also in its parent bag, and you are going to sum the products from these multiplications. Whichever bag produces a higher sum, gets the response\/resume. For the model to be successful, it must classify more than 50% of the responses\/resumes\ncorrectly.","7b30f930":" 1. Add Stop Words. The most common word is \"to\" in the responses\n    pool, you can put more weight on other words by removing words like\n    \"to\" before you put the words in the bag.\n\n 1. Normalize. Although, because these are resumes, this may not make\n        much of a difference, because most will be 1 page and some 2 pages,\n        with a few, generally CVs, being longer.\n\n    \n\n 1. Use CountVectorizer to narrow down the pool of words words,\n    remove duplicates, etc.\n\n\n###You can take a look at 3, \"Use CountVectorizer...\" below.","36dd0947":"There is a benefit to using CountVectorizer from sklearn - you saw that the words in the bag are more accurately represented as they would be in natural language becuase duplicates were combined. \nThe total number of words decreased. \n\nThis is only one idea. Your idea(s) may be better.\n\n###I hope this is useful. \n###Thank you. SP","858119e6":" \n\n        trial   | response accuracy |   BL  | false (+) | false (-) | resume accuracy  |   BL   | false (+) | false (-) \n        ------- | ----------------- | ----- | --------- | --------- | ---------------- | ------ | --------- | ---------\n        1       |        0.58       |  0.63 |   0.00    |   1.00    |         0.76     |  0.73  |    1.00   |    0.00  \n        2       |        0.75       |  0.68 |   0.22    |   0.78    |         0.74     |  0.73  |    1.00   |    0.00\n        3       |        0.55       |  0.65 |   0.00    |   1.00    |         0.77     |  0.73  |    1.00   |    0.00\n        4       |        0.70       |  0.65 |   0.00    |   1.00    |         0.73     |  0.68  |    1.00   |    0.00\n        5       |        0.78       |  0.70 |   0.89    |   0.11    |         0.71     |  0.69  |    1.00   |    0.00\n        6       |        0.48       |  0.60 |   0.00    |   1.00    |         0.77     |  0.71  |    1.00   |    0.00\n        7       |        0.70       |  0.75 |   1.00    |   0.00    |         0.71     |  0.74  |    1.00   |    0.00\n        8       |        0.70       |  0.70 |   0.80    |   0.20    |         0.69     |  0.76  |    1.00   |    0.00\n        9       |        0.78       |  0.73 |   0.82    |   0.18    |         0.74     |  0.71  |    1.00   |    0.00  \n        10      |        0.68       |  0.68 |   0.80    |   0.20    |         0.77     |  0.76  |    1.00   |    0.00\n        ------- | ----------------- | ----- | --------- | --------- | ---------------- | ------ | --------- | ---------\n        average          0.67         0.68    0.45        0.55            0.74             0.73        1.00     0.00","b8d51ba4":"The classifier's accuracy on the responses is printed first and its accuracy on the resumes is printed second. It is performing very ***just** OK*.  An accuracy below 100% is technically not acceptable for the responses, but an accuracy as low as 60% may be acceptable for the resumes for large companies that solicit a very high number of resumes, as they can afford to miss some candidates, and getting nearly 2\/3 of the good candidates is pretty good. \n\nYou can improve it now. What are your ideas? If you want you can consider some of the extension ideas.","38ab5bb2":"Enjoy!","c34fd511":"To move forward with the bag of words model, you need to sample 0.50 of the responses\/resumes and then split each of those samples into two bags, one that is full of responses\/resumes that were flagged and one that is full of those that were not flagged.","35a27f91":"# 2) Generate wordclouds to confirm our data is useful.","43b468fe":"See previous input cell for false positive calculation and false negative calculation.\n\nIf you run 10 trials, your results now look something like this:","03f7090e":"> This section of the notebook demonstrates the bag of words model\n> applied to chat responses sent to a bot and to resumes sent to a\n> jobsite.\n\n**Part I: Bag of Words**\n\n------------------------------------------------------------------------","6c2a30ac":"# 1) Read in our data.","0790c668":"**Part II: Deep natural language processing**\n\n------------------------------------------------------------------------","0718bc74":"#3) Separate the words.","0b37ff4a":"#Extension Ideas"}}