{"cell_type":{"0a1a80e5":"code","187a2505":"code","3e554cfb":"code","e6986379":"code","d1537db3":"code","0929ca70":"code","b4434ffa":"code","2088c93e":"code","0b125edc":"code","61acff59":"code","a335f8b3":"code","061d1428":"code","06443091":"code","4c22aa8b":"code","d8b53b65":"code","baaad5b5":"code","d76cb153":"code","0576e399":"code","29ca8dc3":"code","11a382bf":"code","6c89b385":"code","574e89a3":"code","7fe554b0":"code","0fc77cce":"code","7e184dd8":"markdown","934f85f2":"markdown","a387cb56":"markdown","06613e2a":"markdown","1a6bc639":"markdown","8dbcda56":"markdown","7bab9924":"markdown","52d477cf":"markdown","1bc55914":"markdown","a0fd223d":"markdown","92b15707":"markdown","f048d913":"markdown","411d4374":"markdown","29598dc0":"markdown","fbfed7c0":"markdown","e7e13e90":"markdown","7294f44e":"markdown","ce6469bc":"markdown","95b2a52a":"markdown","44821557":"markdown","fb52c8a9":"markdown","1ae701d2":"markdown","351f649a":"markdown","da6e1052":"markdown","a521b756":"markdown","fec46dbb":"markdown","50e95b58":"markdown","bc2df233":"markdown","283585d8":"markdown","8946383f":"markdown"},"source":{"0a1a80e5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sn","187a2505":"data = pd.read_csv('..\/input\/Wine.csv')","3e554cfb":"data.head(3)","e6986379":"data.columns = ['class','alcohol','malicAcid','ash','ashalcalinity','magnesium','totalPhenols','flavanoids','nonFlavanoidPhenols','proanthocyanins','colorIntensity','hue','od280_od315','proline']","d1537db3":"data.head(3)","0929ca70":"print('There are %d missing values in total.' % data.isna().sum().sum())","b4434ffa":"sn.countplot(data['class'], palette='Blues_d');","2088c93e":"corr = data.corr()\nfig, ax = plt.subplots(figsize=(10,10))\nsn.heatmap(corr,ax=ax, cmap=sn.diverging_palette(20, 220, n=200), square=True, annot=True, cbar_kws={'shrink': .8})\nax.set_xticklabels(data.columns, rotation=45, horizontalalignment='right');","0b125edc":"X = data.drop(['class'], axis=1)\nY = data['class']","61acff59":"from sklearn.model_selection import train_test_split","a335f8b3":"random_state = 2\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=random_state, shuffle=True)","061d1428":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import StratifiedKFold","06443091":"estimator = LogisticRegression(solver='liblinear', multi_class='auto')\nselector = RFECV(estimator, step=1, cv = StratifiedKFold(10));\nselector.fit(X, Y);","4c22aa8b":"plt.figure()\nplt.xlabel('Number of Features')\nplt.ylabel('Cross Validation Score')\ngrid_scores = plt.plot(range(1, len(selector.grid_scores_) + 1), selector.grid_scores_, zorder = 3);\nbest_number = plt.scatter(selector.n_features_, np.max(selector.grid_scores_), color='red', zorder = 5);\nplt.legend([best_number],['Optimal Number of Features'], loc='lower right');","d8b53b65":"from sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier","baaad5b5":"classifiers = []\nclassifiers.append(('Logistic Regression', LogisticRegression(solver='liblinear', multi_class='auto')))\nclassifiers.append(('Support Vector Classifier', SVC(kernel='linear')))\nclassifiers.append(('GaussianNB', GaussianNB()))\nclassifiers.append(('K-Nearest Neighbors',KNeighborsClassifier(n_neighbors=3)))\nclassifiers.append(('Decision Tree', DecisionTreeClassifier()))\nclassifiers.append(('Multi-Layer Perceptron', MLPClassifier(hidden_layer_sizes=(15),solver='sgd',learning_rate_init=0.01,max_iter=500)))\nclassifiers.append(('eXtreme Gradient Boosting', XGBClassifier()))","d76cb153":"from sklearn.model_selection import StratifiedKFold, cross_val_score","0576e399":"kfold = StratifiedKFold(n_splits=10, random_state=random_state)\ncv_results = []\nfor name, classifier in classifiers:\n    result = cross_val_score(classifier, X, Y, cv=kfold);\n    cv_results.append((name, result));","29ca8dc3":"results = pd.DataFrame(cv_results, columns=['classifier','cvscore'])\nresults['cvscore'] = [np.mean(i) for i in results['cvscore']]","11a382bf":"sn.set_style('whitegrid')\nax = sn.barplot(x='cvscore',y='classifier', data=results.sort_values('cvscore'), palette='Blues_d')\nax.set(xlabel='Cross Validation Score', ylabel='');","6c89b385":"print('The best performing model is: %s\\nWith Cross-Validation Score of: %.2f' % (results.iloc[results['cvscore'].idxmax()][0], results.iloc[results['cvscore'].idxmax()][1]))","574e89a3":"estimator = GaussianNB()\nestimator.fit(X_train, Y_train)\nY_predict = estimator.predict(X_test)","7fe554b0":"from sklearn.metrics import accuracy_score\nprint('Prediction accuracy is: %.2f' % (100*accuracy_score(Y_predict, Y_test)))","0fc77cce":"X_test[Y_predict != Y_test]","7e184dd8":"We will build a list of tuples containing the name of the classifier and the classifier itself:","934f85f2":"First we check whether the data contains any missing values","a387cb56":"Then we will use `train_test_split()` from `sklearn.model_selection` to split it further into training and testing subsets.","06613e2a":"# Models ranking","1a6bc639":"We need to import the mentioned classifiers:","8dbcda56":"We can see that columns are given arbitrary numbers. The real columns names are provided in the dataset page, we will assign the columns names to the real ones:","7bab9924":"## 3.2. Variables correlation","52d477cf":"Let's check for the variables that our model predicted wrong","1bc55914":"# Conclusion","a0fd223d":"It is always a good idea to try many classifiers and compair their results, and pick the one with best accuracy. Different algorithms may perform differently on different datasets.\nWe will try the following models:\n* Logistic Regression\n* Support Vector Classifier\n* Naive Bayes\n* K-Nearest Neighbours\n* Decision Trees\n* Multi-Layer Perceptron\n* XGBoost Classifier","92b15707":"The dataset was clean and didn't require any real preprocessing and missing values handling.\nAlso the variables were really predictive for the target variable, many models scored very high (+90%) and the best model scored %96.30","f048d913":"First, we need to seperate the variables and the target from the original dataset as follows:","411d4374":"# 4. Split train\/test data","29598dc0":"Recrusive Feature Elimination didn't eliminate any feature, so apparently all features contribute to the clasification.\nWe will keep all of them.","fbfed7c0":"# Loading libraries and dataset","e7e13e90":"The data is clean and has no missing values, no further processing is needed","7294f44e":"First of all, we start by loading the dataset using `pd.read_csv()` function","ce6469bc":"We use the training split we created earlier in order to train our model, then we will use it to predict the class of testing samples:","95b2a52a":"# Building the models","44821557":"## 3.1. Count of different wine classes","fb52c8a9":"Some variables may not be predictive for the target wine class, we will use feature elimination to try to eliminate them in order to improve the data quality we will feed into the model","1ae701d2":"# 5. Feature Elimination","351f649a":"The `train_test_split()` function takes the following arguments:\n* `X`: the variables, the whole dataset, without the target variable (wine class)\n* `Y`: the target variable, which is the wine class\n* `test_size`: represents the proportion of the original data to be used as testing set (here I chose 30%)\n* `shuffle`: since the original dataset is grouped by the wine class, it is preferable to rearrange everythign randomly, so we set `shuffle` to `True`","da6e1052":"The variable the least correlated with the target variable (class) is **ash**, we can drop it but we will leave this for feature elimination.","a521b756":"We notice that **GuassianNB** scored the highest, so this the model that we will pick","fec46dbb":"# 3. Data Analysis","50e95b58":"# 2. Missing values","bc2df233":"# Wine Classification\nIn this kernel we will use the data from [Wine Varaieties Dataset](https:\/\/www.kaggle.com\/brynja\/wineuci) to perform a simple classification to predict the wine class.","283585d8":"To evaluate the performance of our models, we will ues the `cross_val_score()` function","8946383f":"To evaluate the accuracy of our predictions, we will use `accuracy_score()`:"}}