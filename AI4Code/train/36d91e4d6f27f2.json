{"cell_type":{"cad43f26":"code","cb3eaf3f":"code","0ddb7f48":"code","5fad2ed0":"code","0dde9779":"code","eea15cc3":"code","ea3bd90e":"code","60e65c65":"code","82825d4c":"code","6d38a953":"code","10f43336":"code","93a7de6f":"code","6363e85a":"code","a3d4fcbf":"code","a010f4d2":"code","82e0cac7":"code","ca16403c":"code","d4411475":"code","a6cd8128":"code","62b7292a":"code","9bb974a2":"code","167fe3e8":"code","10d73001":"markdown","62cb3858":"markdown"},"source":{"cad43f26":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom math import radians, sin, cos, asin, sqrt\nfrom sklearn import preprocessing, model_selection, metrics\nfrom sklearn.model_selection import train_test_split\nfrom pandas import to_datetime\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.neural_network import MLPRegressor\nimport xgboost as xgb\nfrom xgboost import XGBRegressor, XGBClassifier\nfrom sklearn import neighbors\nfrom sklearn.svm import SVR\nimport time\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_absolute_error\n\n\ndef haversine_distance(lat1, lon1, lat2, lon2):\n    \"\"\"\n    Compute distance between two pairs of (lat, lng)\n    \"\"\"\n    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n    a = sin(dlat \/ 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon \/ 2) ** 2\n    return 2 * 6371 * asin(sqrt(a))\n","cb3eaf3f":"# Read tables\n\ncustomers = pd.read_csv('..\/input\/brazilian-ecommerce\/olist_customers_dataset.csv')\ngeolocation =  pd.read_csv('..\/input\/brazilian-ecommerce\/olist_geolocation_dataset.csv')\norder_items =  pd.read_csv('..\/input\/brazilian-ecommerce\/olist_order_items_dataset.csv')\norders =  pd.read_csv('..\/input\/brazilian-ecommerce\/olist_orders_dataset.csv')\nproducts = pd.read_csv('..\/input\/brazilian-ecommerce\/olist_products_dataset.csv')\nsellers = pd.read_csv('..\/input\/brazilian-ecommerce\/olist_sellers_dataset.csv')\n","0ddb7f48":"np.deg2rad(geolocation.geolocation_lat)","5fad2ed0":"orders.head()\n","0dde9779":"orders.groupby('order_status').count()","eea15cc3":"sellers","ea3bd90e":"order_items = pd.merge(order_items, sellers[['seller_id', 'seller_zip_code_prefix']], left_on='seller_id', right_on='seller_id').drop(['order_item_id','shipping_limit_date'], axis=1)\norder_items","60e65c65":"customers","82825d4c":"orders","6d38a953":"merge_df = pd.merge(order_items, orders, left_on='order_id', right_on='order_id', how='left')\nmerge_df = pd.merge(merge_df, customers[['customer_id', 'customer_zip_code_prefix']], how='left',left_on='customer_id',right_on='customer_id')\ngeo = geolocation.groupby('geolocation_zip_code_prefix').mean().reset_index()\n","10f43336":"merge_df = pd.merge(merge_df, geo[['geolocation_zip_code_prefix', 'geolocation_lat', 'geolocation_lng']], how='left', \n                    left_on='seller_zip_code_prefix', right_on='geolocation_zip_code_prefix').rename(columns={'geolocation_lat': 'seller_lat', \n                                                                                                              'geolocation_lng': 'seller_lon'})\nmerge_df = pd.merge(merge_df, geo[['geolocation_zip_code_prefix', 'geolocation_lat', 'geolocation_lng']], how='inner', \n                    left_on='customer_zip_code_prefix', right_on='geolocation_zip_code_prefix').rename(columns={'geolocation_lat': 'customer_lat', \n                                                                                                              'geolocation_lng': 'customer_lon'})","93a7de6f":"merge_df = pd.merge(merge_df, products[['product_id','product_category_name','product_photos_qty','product_weight_g','product_length_cm','product_height_cm','product_width_cm']], \n                   left_on='product_id', right_on='product_id', how='left')\n","6363e85a":"merge_df['product_volume_cm3']=merge_df.product_length_cm * merge_df.product_height_cm * merge_df.product_width_cm\nmerge_df.order_delivered_customer_date = pd.to_datetime(merge_df.order_delivered_customer_date)\nmerge_df.order_delivered_carrier_date = pd.to_datetime(merge_df.order_delivered_carrier_date)\nmerge_df.order_purchase_timestamp = pd.to_datetime(merge_df.order_purchase_timestamp)\nmerge_df.order_estimated_delivery_date = pd.to_datetime(merge_df.order_estimated_delivery_date)\nmerge_df['purchase_month']=merge_df.order_purchase_timestamp.dt.month\nmerge_df['purchase_day_of_week']=merge_df.order_purchase_timestamp.dt.day_of_week\nmerge_df['actual_delivery_time']=(merge_df.order_delivered_customer_date-merge_df.order_purchase_timestamp).dt.days\nmerge_df['carrier_delivery_time']=(merge_df.order_delivered_carrier_date-merge_df.order_purchase_timestamp).dt.days\nmerge_df['estimated_delivery_time']=(merge_df.order_estimated_delivery_date-merge_df.order_purchase_timestamp).dt.days\n\nmerge_df['distance'] = merge_df.apply(\n    lambda row: haversine_distance(\n        row['seller_lat'],\n        row['seller_lon'],\n        row['customer_lat'],\n        row['customer_lon'],\n    ),\n    axis=1,\n)","a3d4fcbf":"merge_df=merge_df.drop(['order_status','product_length_cm','product_height_cm', 'order_delivered_carrier_date', \n                        'product_width_cm', 'order_id', 'product_id', 'order_purchase_timestamp', 'order_delivered_customer_date', \n                        'product_category_name', 'seller_id', 'customer_zip_code_prefix', 'seller_zip_code_prefix', 'customer_id', 'order_estimated_delivery_date', \n                        'geolocation_zip_code_prefix_x', 'geolocation_zip_code_prefix_y', 'order_approved_at', 'product_photos_qty', 'seller_lat', \n                        'seller_lon', 'customer_lat', 'customer_lon'], axis=1, errors='ignore')\nmerge_df = merge_df.dropna()\nmerge_df","a010f4d2":"# Generate a mask for the upper triangle (taken from seaborn example gallery)\nmask = np.zeros_like(merge_df[merge_df.columns[::-1]].corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nplt.figure(figsize=(10, 6))\nsns.heatmap(merge_df[merge_df.columns[::-1]].corr(), cmap=sns.diverging_palette(20, 220, n=200), annot=True, mask=mask, center = 0, )\nplt.title(\"Heatmap of all the Features\", fontsize = 30)\n#plt.yticks(rotation=0)","82e0cac7":"target = 'estimated_delivery_time'\nfeatures = ['freight_value', \n            'product_volume_cm3', \n            'product_weight_g', \n            #'purchase_day_of_week', \n            #'price',\n            #'estimated_delivery_time',\n            'carrier_delivery_time',\n            'distance'\n           ]\n\n#num_pipeline = Pipeline([\n#    ('imputer', SimpleImputer(strategy = 'median')),\n#    ('std_scaler', StandardScaler())\n#])\n\nX = merge_df[features]\ny = merge_df[target]\n\nX_data = StandardScaler().fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X_data, y, test_size=0.2, random_state = 14)\n\n\nmodels = [\n           ['Lasso: ', Lasso()],\n           ['Ridge: ', Ridge()],\n           ['KNeighborsRegressor: ',  neighbors.KNeighborsRegressor()],\n           ['RandomForest ',RandomForestRegressor()],\n           ['ExtraTreeRegressor :',ExtraTreesRegressor()],\n           ['GradientBoostingRegressor: ', GradientBoostingRegressor()] ,\n           ['XGBoost: ', XGBRegressor()] ,\n           #['MLPRegressor: ', MLPRegressor(  activation='relu', solver='adam',learning_rate='adaptive',max_iter=10000,learning_rate_init=0.01,alpha=0.01)]\n         ]\n\nmodel_data = []\nfor name,curr_model in models :\n    curr_model_data = {}\n    curr_model.random_state = 100\n    curr_model_data[\"Name\"] = name\n    start = time.time()\n    curr_model.fit(X_train,y_train)\n    end = time.time()\n    curr_model_data[\"Train_Time\"] = end - start\n    curr_model_data[\"Train_R2_Score\"] = metrics.r2_score(y_train,curr_model.predict(X_train))\n    curr_model_data[\"Test_R2_Score\"] = metrics.r2_score(y_test,curr_model.predict(X_test))\n    curr_model_data[\"Test_RMSE\"] = sqrt(mean_squared_error(y_test,curr_model.predict(X_test)))\/100\n    model_data.append(curr_model_data)\n","ca16403c":"model_scores = pd.DataFrame(model_data)\nmodel_scores","d4411475":"model_scores.plot(x=\"Name\", y=['Train_R2_Score', 'Test_R2_Score', 'Test_RMSE'], kind=\"bar\" , title = 'R2 Score Results' , figsize= (10,8)) ;","a6cd8128":"regressor = RandomForestRegressor(n_estimators=200, random_state=0)\nregressor.fit(X_train, y_train)\ny_pred = regressor.predict(X_test)\n","62b7292a":"print(\"Random Forest Regressor's Mean Absolute Error: \" + str(mean_absolute_error(y_pred, y_test)))\ny.describe()","9bb974a2":"from sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import learning_curve\n\ncv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\n\nestimator = RandomForestRegressor()\ntrain_sizes, train_scores, test_scores, fit_times, _ = learning_curve(estimator, X_train,pd.Series.ravel(y_train), cv=cv, n_jobs=4, \n                                                                      train_sizes=np.linspace(.1, 1.0, 5), return_times=True)","167fe3e8":"_, axes = plt.subplots(1, 3, figsize=(20, 5))\n\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\nfit_times_mean = np.mean(fit_times, axis=1)\nfit_times_std = np.std(fit_times, axis=1)\n\n# Plot learning curve\naxes[0].grid()\naxes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\naxes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1,\n                     color=\"g\")\naxes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\naxes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\naxes[0].legend(loc=\"best\")\n\n# Plot n_samples vs fit_times\naxes[1].grid()\naxes[1].plot(train_sizes, fit_times_mean, 'o-')\naxes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n                     fit_times_mean + fit_times_std, alpha=0.1)\naxes[1].set_xlabel(\"Training examples\")\naxes[1].set_ylabel(\"fit_times\")\naxes[1].set_title(\"Scalability of the model\")\n\n# Plot fit_time vs score\naxes[2].grid()\naxes[2].plot(fit_times_mean, test_scores_mean, 'o-')\naxes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1)\naxes[2].set_xlabel(\"fit_times\")\naxes[2].set_ylabel(\"Score\")\naxes[2].set_title(\"Performance of the model\")\n","10d73001":"# Data Analysis","62cb3858":"Prediction accuracy is 51% with a mean square error of 4 days. The model accuracy is at an acceptable level. There are several point to take note in order to improve the accuracy of the model\n\n1. There are outliers in the data that can be removed (max delivery time is 125 days while the mean is roughly 3)\n2. Distance is not a good representation of how hard it may take for an object to go from A to B. Transport infrastructure, availabilty of flights, metro, trains and developments in sellers' location can influence the time taken to deliver. We do not have these information\n3. The records suggest that Olist's delivery system between 2017 and 2018 was inefficient and thus making any hopes to properly predicting the delivery time tedious\n5. Lastly, the fact that our model's MAE was shorter than 5 days means the regressors are within two-thirds of our target data's standard deviation, which in several situations is an acceptable level of accuracy. The extreme randomness of the data's distribution makes it difficult to reach very high levels of accuracy"}}