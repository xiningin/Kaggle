{"cell_type":{"d4281aad":"code","0c45f6a1":"code","0c48cda3":"code","b3b96f5e":"code","a9dae07e":"code","df4c963c":"code","69212418":"code","513a3f1f":"code","1202b46a":"code","570f4033":"code","1e8e3112":"code","f2e2104e":"code","4b399a46":"code","44332757":"code","5161a679":"code","fe904ae8":"code","72308049":"code","d39ee9c7":"code","3b30e921":"code","a733091e":"code","4c513010":"code","8186edc5":"code","1f09ab6d":"code","a36fc06a":"code","3381da85":"code","36647d5f":"code","cbb8582a":"code","081063a5":"code","b4498ce9":"code","f65fc141":"code","4da6a9ab":"code","192e666e":"code","db2a4630":"code","4fa4c6ae":"code","c0455450":"code","d3881c9c":"code","adde9067":"markdown","aed1bef3":"markdown","ebd3ea84":"markdown","4b058eb6":"markdown","b268d132":"markdown","81e15b10":"markdown","a7719c87":"markdown","f3667cce":"markdown","36c1b983":"markdown","4aef7acd":"markdown","76829fa8":"markdown","f36dee74":"markdown","dc48a71c":"markdown","a2ba2e2f":"markdown","09f5ae9a":"markdown","5ae63167":"markdown","f3eeffab":"markdown","df79b488":"markdown","571852a4":"markdown","543d851d":"markdown","2ca10b89":"markdown","622f9875":"markdown","ae9718fc":"markdown","1df46ad5":"markdown","3c9bee33":"markdown","ec43e814":"markdown","31bb729b":"markdown","edc23291":"markdown","f025a0e8":"markdown","a98666f0":"markdown","f0cf657b":"markdown","69e0084a":"markdown"},"source":{"d4281aad":"import pickle\n\n# To store\\load the data\nimport pandas as pd\n\n# To do linear algebra\nimport numpy as np\n\n# To create plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# To compute similarities between vectors\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# data load progress bars\nfrom tqdm import tqdm\n\nfrom collections import deque\n\n# To create deep learning models\nimport tensorflow as tf\nimport keras\nfrom keras.layers import Input, Embedding, Reshape, Dot, Concatenate, Dense, Dropout\nfrom keras.models import Model\n\n# To stack sparse matrices\nfrom scipy.sparse import vstack\nimport gc","0c45f6a1":"# filter out unncessary warnings\nimport warnings\nwarnings.filterwarnings('ignore')","0c48cda3":"# remove unnecessary TF logs\nimport logging\ntf.get_logger().setLevel(logging.ERROR)","b3b96f5e":"# check keras and TF version used\nprint('TF Version:', tf.__version__)\nprint('Keras Version:', keras.__version__)\n# TF Version: 1.15.0\n# Keras Version: 2.2.5","a9dae07e":"# Load data for all movies\nmovie_titles = pd.read_csv('\/kaggle\/input\/netflix-prize-data\/movie_titles.csv',\n                           encoding = 'ISO-8859-1', \n                           # we need header=None because the first row of the csv file is not header but actual data \n                           header = None, \n                           names = ['Id', 'Year', 'Name']).set_index('Id')\n\nprint('Shape Movie-Titles:\\t{}'.format(movie_titles.shape))\nmovie_titles.head(5)","df4c963c":"# Load single data-file\ndf_raw_1 = pd.read_csv('\/kaggle\/input\/netflix-prize-data\/combined_data_1.txt', \n                     header=None, \n                     names=['User', 'Rating', 'Date'], \n                     usecols=[0, 1, 2])\n\ndf_raw_2 = pd.read_csv('\/kaggle\/input\/netflix-prize-data\/combined_data_2.txt', \n                     header=None, \n                     names=['User', 'Rating', 'Date'], \n                     usecols=[0, 1, 2])\n\n# df_raw_3 = pd.read_csv('\/kaggle\/input\/netflix-prize-data\/combined_data_3.txt', \n#                      header=None, \n#                      names=['User', 'Rating', 'Date'], \n#                      usecols=[0, 1, 2])\n\n# df_raw_4 = pd.read_csv('\/kaggle\/input\/netflix-prize-data\/combined_data_4.txt', \n#                      header=None, \n#                      names=['User', 'Rating', 'Date'], \n#                      usecols=[0, 1, 2])\n\n# df_raw = pd.concat([df_raw_1, df_raw_2, df_raw_3, df_raw_4]).reset_index().drop(['index'], axis=1)\ndf_raw = pd.concat([df_raw_1, df_raw_2]).reset_index(drop=True)\n\n# del df_raw_1, df_raw_2, df_raw_3, df_raw_4\ndel df_raw_1, df_raw_2\n_=gc.collect()","69212418":"# Find rows in the format of \"1: NaN NaN\" that label the start of a new movie\ntmp_movies = df_raw[df_raw['Rating'].isna()]['User'].reset_index() # make the original index a new column\nmovie_indices = [[index, int(movie[:-1])] for index, movie in tmp_movies.values]\n\n# Shift the movie_indices by one to get start and end row indices of all movies\nshifted_movie_indices = deque(movie_indices)\nshifted_movie_indices.rotate(-1)\n\n# Gather all dataframes\nuser_data = []\n\n# Iterate over all movies\nfor [df_id_1, movie_id], [df_id_2, next_movie_id] in zip(movie_indices, shifted_movie_indices):\n    \n    # Check if it is the last movie in the file\n    if df_id_1<df_id_2:\n        tmp_df = df_raw.loc[df_id_1+1:df_id_2-1].copy()\n    else:\n        tmp_df = df_raw.loc[df_id_1+1:].copy()\n        \n    # Create movie_id column\n    tmp_df['Movie'] = movie_id\n    \n    # Append dataframe to list\n    user_data.append(tmp_df)\n\n# Combine all dataframes\ndf = pd.concat(user_data)\nprint('Shape User-Ratings:\\t{}'.format(df.shape))\ndisplay(df.sample(10))\n\ndel user_data, df_raw, tmp_movies, tmp_df, shifted_movie_indices, movie_indices, df_id_1, movie_id, df_id_2, next_movie_id","513a3f1f":"fig, ax = plt.subplots(1, 1, figsize=(14, 6))\n\ndata = movie_titles['Year'].value_counts().sort_index()\nx = data.index.map(int)\ny = data.values\n\nsns.barplot(x, y)\nxmin, xmax = plt.xlim()\nxtick_labels = [x[0]] + list(x[10:-10:10]) + [x[-1]]\nplt.xticks(ticks=np.linspace(xmin, xmax, 10), labels=xtick_labels);\n\ndel fig, ax, data, x, y, xmin, xmax, xtick_labels","1202b46a":"fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n\ndata = df['Rating'].value_counts().sort_index()\nx = data.index.map(int)\ny = data.values\n\nsns.barplot(x, y)\nxmin, xmax = plt.xlim()\nplt.xticks(ticks=[0,1,2,3,4,5]);\n\ndel fig, ax, data, x, y, xmin, xmax","570f4033":"fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n\ndata = df.groupby('Movie')['Rating'].count()\nsns.distplot(data[data  < 10000], kde=False, ax=ax[0], bins=[i for i in range(0,10000,200)]);\nsns.distplot(data[data  > 10000], kde=False, ax=ax[1], bins=[i for i in range(10000,150000,5000)]);\n\ndel fig, ax, data","1e8e3112":"fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n\ndata = df.groupby('User')['Rating'].count()\nsns.distplot(data[data  < 200], kde=False, ax=ax[0]);\nsns.distplot(data[data  > 200], kde=False, ax=ax[1], bins=[i for i in range(100,1000,20)]);\n\ndel fig, ax, data","f2e2104e":"# Filter sparse movies\nmin_movie_ratings = 1000\nfilter_movies = (df['Movie'].value_counts()>min_movie_ratings)\nfilter_movies = filter_movies[filter_movies].index.tolist()\n\n# Filter sparse users\nmin_user_ratings = 200\nfilter_users = (df['User'].value_counts()>min_user_ratings)\nfilter_users = filter_users[filter_users].index.tolist()\n\n# Actual filtering\ndf_filtered = df[(df['Movie'].isin(filter_movies)) & (df['User'].isin(filter_users))]\n\nprint('Shape User-Ratings unfiltered:\\t{}'.format(df.shape))\nprint('Shape User-Ratings filtered:\\t{}'.format(df_filtered.shape))\n\ndel df, filter_movies, filter_users, min_movie_ratings, min_user_ratings","4b399a46":"df_filtered = df_filtered.drop('Date', axis=1).sample(frac=1, random_state=7).reset_index(drop=True)\n\n# splits\nsplit1 = df_filtered.shape[0]-200000\nsplit2 = df_filtered.shape[0]-100000\n\n# Split train, val & test sets\ndf_train = df_filtered[:split1]\ndf_val = df_filtered[split1:split2]\ndf_test = df_filtered[split2:]\n\n# don't delete df_filtered yet","44332757":"df_train.shape, df_val.shape, df_test.shape","5161a679":"df_filtered['User'].unique().shape[0], df_train['User'].unique().shape[0]","fe904ae8":"df_filtered['Movie'].unique().shape[0], df_train['Movie'].unique().shape[0]","72308049":"del df_filtered\n_=gc.collect()","d39ee9c7":"# Number of minimum votes to be considered\nm = 1000\n\n# Mean rating for all movies\nC = df_train['Rating'].mean()\n\n# Mean rating for all movies separately\nR = df_train.groupby(['Movie']).mean()['Rating'].values\n\n# Rating freqency for all movies separately\nv = df_train.groupby(['Movie']).count()['Rating'].values","3b30e921":"# Weighted formula to compute the weighted rating\nweighted_score = v\/(v+m)*R+m\/(v+m)*C\ndel m, C, R, v\n_=gc.collect()","a733091e":"# convert weighted_score into a dataframe\nindex=np.sort(df_train['Movie'].unique())\nweighted_mean = pd.DataFrame(weighted_score, index = index, columns = ['weighted score'])\nweighted_mean.index.name = 'Id'\ndisplay(weighted_mean.head(5))\n\ndel index, weighted_score","4c513010":"# Combine the aggregated dataframes (wighted_mean & movie_titles)\ncombined_df = weighted_mean.join(movie_titles)\nprint(weighted_mean.shape)\nprint(movie_titles.shape)\nprint(combined_df.shape)\ndisplay(combined_df.head(5))\n\ndel weighted_mean","8186edc5":"# Join labels and predictions based on mean movie rating\npredictions_df = df_test.set_index('Movie').join(combined_df)\npredictions_df.head(5)","1f09ab6d":"# Compute RMSE and MAE\ny_true = predictions_df['Rating']\ny_pred = predictions_df['weighted score']\n\nrmse = np.sqrt(mean_squared_error(y_true=y_true, y_pred=y_pred))\nmae = mean_absolute_error(y_true=y_true, y_pred=y_pred)\nprint(\"The RMSE Value for the Mean Rating Recommender:\", rmse)\nprint(\"The MAE Value for the Mean Rating Recommender:\", mae)\n\ndel predictions_df, y_true, y_pred, rmse, mae","a36fc06a":"from collections import OrderedDict\ntemp=combined_df.reset_index()[['Id','weighted score']].sort_values('weighted score', ascending=False)\n\n# {movie_id: weighted_score, ....}\nrank_dic=OrderedDict()\nfor i in range(temp.shape[0]):\n    rank_dic[temp.iloc[i,0]]=temp.iloc[i,1]\n\ndel temp","3381da85":"def average_precision_at_k(rel: [int], pred: [int], k: int) -> float:\n    # this function works only for a single user\n    # rel is an list of movie id's for all relevant movies in the test set \n    # pred is the prediction of the model excluding those ratings already in the training set\n    # pred is a list of movie id's whose scores are ranked from high to low\n    # len(pred) should be large enough for k\n    # k is the cutoff\n    temp=0\n    true_positive=0\n    for i in range(min(k,len(pred))):\n        if pred[i] in rel:\n            true_positive+=1\n            temp+=true_positive\/(i+1)\n    return temp\/len(rel)","36647d5f":"test_user_ids={}\nfor ele in list(df_test['User'].unique()):\n    test_user_ids[ele]=True","cbb8582a":"# {user_id: [movie_id_1, movie_id_2, ...], ...}\ntrain_movie_ids={}\n# {user_id: average_rating, ...}\ntrain_movie_average_ratings={}\nct=0\n\nfor i in range(df_train.shape[0]):\n    if i % (df_train.shape[0]\/\/100)==0: \n        print(str(ct)+' percent of job done')\n        ct+=1\n        \n    user = df_train.iloc[i, 0]\n    if user in test_user_ids:\n        try:\n            train_movie_ids[user].append(df_train.iloc[i,2])\n            train_movie_average_ratings[user]+=df_train.iloc[i,1]\n        except:\n            train_movie_ids[user]=[df_train.iloc[i,2]]\n            train_movie_average_ratings[user]=df_train.iloc[i,1]\n\ndel user","081063a5":"for key in test_user_ids:\n    train_movie_average_ratings[key]\/=len(train_movie_ids[key])","b4498ce9":"# {user_id: [movie_id_1, movie_id_2, ...], ...}\ntest_movie_ids={}\n\nfor i in range(df_test.shape[0]):\n    user=df_test.iloc[i,0]\n    if df_test.iloc[i,1]>train_movie_average_ratings[user]:\n        try:\n            test_movie_ids[user].append(df_test.iloc[i,2])\n        except:\n            test_movie_ids[user]=[df_test.iloc[i,2]]","f65fc141":"# result stores average precision at k for users in test set\nresult=[]\nk=100\n\nfor user in test_movie_ids:\n    \n    rel=test_movie_ids[user]\n    copy_dic=rank_dic.copy()\n    for ele in train_movie_ids[user]:\n        copy_dic.pop(ele)\n    pred=list(copy_dic.keys())\n    result.append(average_precision_at_k(rel, pred, k))","4da6a9ab":"print('there are {} users in the test set'.format(len(test_user_ids)))\nprint('there are {} users whose ratings in the test set are all negative'.format(len(test_user_ids)-len(test_movie_ids)))\nprint('mean average precision is {}'.format(sum(result)\/len(result)))","192e666e":"df_train.to_pickle('df_train.pkl')\ndf_val.to_pickle('df_val.pkl')\ndf_test.to_pickle('df_test.pkl')\ncombined_df.to_pickle('combined_df.pkl')","db2a4630":"test_user_ids_list=list(test_user_ids.keys())\n\nwith open('test_user_ids_list.txt', 'wb') as fp:\n    pickle.dump(test_user_ids_list, fp)","4fa4c6ae":"test_movie_ids_list=[]\nfor key in test_movie_ids:\n    test_movie_ids_list.append([key, test_movie_ids[key]])\n    \nwith open('test_movie_ids_list.txt', 'wb') as fp:\n    pickle.dump(test_movie_ids_list, fp)","c0455450":"train_movie_ids_list=[]\nfor key in train_movie_ids:\n    train_movie_ids_list.append([key, train_movie_ids[key]])\n    \nwith open('train_movie_ids_list.txt', 'wb') as fp:\n    pickle.dump(train_movie_ids_list, fp)","d3881c9c":"train_movie_average_ratings_list=[]\nfor key in train_movie_average_ratings:\n    train_movie_average_ratings_list.append([key, train_movie_average_ratings[key]])\n    \nwith open('train_movie_average_ratings_list.txt', 'wb') as fp:\n    pickle.dump(train_movie_average_ratings_list, fp)","adde9067":"###  get the desired format","aed1bef3":"Weighted Rating Formula\n\nweighted rating (\ud835\udc4a\ud835\udc45)=(\ud835\udc63\/(\ud835\udc63+\ud835\udc5a))\ud835\udc45+(\ud835\udc5a\/(\ud835\udc63+\ud835\udc5a))\ud835\udc36\n\nwhere:\n\n\ud835\udc45 = average for the movie (mean) = (Rating)\n\n\ud835\udc63 = number of votes for the movie = (votes)\n\n\ud835\udc5a = minimum votes required\n\n\ud835\udc36 = the mean vote across the whole report","ebd3ea84":"**list of unique users in the test**","4b058eb6":"## Filter out rarely rated movies and users who don't give enough ratings","b268d132":"**test_movie_ids is a dicitionary that stores positively rated movies in the test sets for each user in the test sets**","81e15b10":"**looks like exponential decay**","a7719c87":"## Import dependencies","f3667cce":"### movie release year","36c1b983":"**define the function that calculates map@k**","4aef7acd":"**create dictionaries that store movie id's and the average ratings of users in the train set**","76829fa8":"# This project is based on the a mini project from Springboard ML Career Track bootcamp. Some part of the code is directly taken from there. In Part 1 of the project we build a very basic movie recommender using a global method, which means the predictions of the movie ratings will be exactly the same for all users.","f36dee74":"## split train, cross validation and test sets","dc48a71c":"**divide the sum of ratings by the number of movies to get average rating for each user**","a2ba2e2f":"## EDA","09f5ae9a":"## global recommender system using weighted rating","5ae63167":"### distribution of ratings","f3eeffab":"###  at this stage, non-deleted variables are: \n### df_train, df_val, df_test, combined_df\n### test_user_ids, test_movie_ids, train_movie_ids, train_movie_average_ratings","df79b488":"### evaluation RMSE and MAE","571852a4":"### load user-movie ratings","543d851d":"### check if all movies are included in the training set","2ca10b89":"**delete df_filtered now to save memory**","622f9875":"## Load datasets","ae9718fc":"### check if all users are included in the training set","1df46ad5":"**loop through all users in test set and apply the function we defined**","3c9bee33":"**also looks like exponential decay**","ec43e814":"### distribution of number of ratings movies receive","31bb729b":"## distribution of ratings users give","edc23291":"**first define an ordered dict that stores movie id and weighted average movie score from high to low**","f025a0e8":"### load movie titles","a98666f0":"### evaluation mean average precision at cutoff k","f0cf657b":"### at this stage, non-deleted variables are: df_train, df_val, df_test, combined_df","69e0084a":"**it seems that users tend to give good ratings**"}}