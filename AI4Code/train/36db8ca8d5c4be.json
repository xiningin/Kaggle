{"cell_type":{"4a49f197":"code","38094e09":"code","472b4b7f":"code","cffdb4c2":"code","00b29a50":"code","b83cf8c4":"code","fd0d1565":"code","e933eb0a":"code","735399a2":"code","6c7c7a47":"code","14082d40":"code","336d2b2e":"code","7b6cf436":"code","cea2cb33":"code","d1235c86":"code","26a6379a":"code","741a3d41":"code","41ad9386":"code","175e8640":"code","cf70a3d5":"code","464d3b59":"code","000f7475":"code","cf83e725":"code","d1cad911":"markdown","40ea48f2":"markdown","be96d745":"markdown","138e0021":"markdown","4bb538bf":"markdown","5d9b3d5f":"markdown","3de40ccb":"markdown","d9148ac1":"markdown","99d3e657":"markdown"},"source":{"4a49f197":"import os\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras.applications import InceptionV3\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Input, Flatten, Dense, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.metrics import Recall, Precision\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom sklearn.metrics import confusion_matrix, classification_report","38094e09":"path = \"..\/input\/covid19-radiography-database\/COVID-19 Radiography Database\"\nlabels = [\"COVID-19\", \"NORMAL\", \"Viral Pneumonia\"]","472b4b7f":"# Randomly view 5 images in each category\n\nfig, axs = plt.subplots(len(labels), 5, figsize = (15, 15))\n\nclass_len = {}\nfor i, c in enumerate(labels):\n    class_path = os.path.join(path, c)\n    all_images = os.listdir(class_path)\n    sample_images = random.sample(all_images, 5)\n    class_len[c] = len(all_images)\n    \n    for j, image in enumerate(sample_images):\n        img_path = os.path.join(class_path, image)\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        axs[i, j].imshow(img)\n        axs[i, j].set(xlabel = c, xticks = [], yticks = [])\n\nfig.tight_layout()","cffdb4c2":"# Make a pie-chart to visualize the percentage contribution of each category.\nfig, ax = plt.subplots()\nax.pie(\n    class_len.values(),\n    labels = class_len.keys(),\n    autopct = \"%1.1f%%\"\n)\nfig.show()\n# The dataset is imbalance so we will have to take care of that later.","00b29a50":"# We do not have separate folders for training and validation. \n# We need to read training and validation images from the same folder such that:\n# 1. There is no data leak i.e. Training images should not appear as validation images.                 \n# 2. We must be able to apply augmentation to training images but not validation images.  \n# We shall adopt the following strategy:\n# 1. Use the same validation_split in ImageDataGenerator for training and validation.\n# 2. Use the same seed when using flow_from_directory for training and validation. \n# To veify the correctness of this approach, you can print filenames from each generator and check for overlap.\n\n# Another problem is that along with the 3 image data folders, there are files we are not making use of.\n# To be sure that images are read from the correct folders, we can specify the directory and the labels.\n\n# Note that we use simple augmentation to avoid producing unsuitable images.\n\ndatagen_train = ImageDataGenerator(\n    rescale = 1.\/255, \n    validation_split = 0.2,\n    rotation_range = 5,\n    width_shift_range = 0.05,\n    height_shift_range = 0.05,\n    zoom_range = 0.01\n)\n\ndatagen_val = ImageDataGenerator(\n    rescale = 1.\/255, \n    validation_split = 0.2 \n)    \n\ntrain_generator = datagen_train.flow_from_directory(\n    directory = path,\n    classes = labels,\n    seed = 42,\n    batch_size = 32, \n    shuffle = True,\n    subset = 'training'\n)\n\nval_generator = datagen_val.flow_from_directory(\n    directory = path,\n    classes = labels,\n    seed = 42,\n    batch_size = 32, \n    shuffle = True,\n    subset = 'validation'\n)","b83cf8c4":"# To veify the correctness of this approach (empty set is expected)\nset(val_generator.filenames).intersection(set(train_generator.filenames))","fd0d1565":"# Check out labeling\nval_generator.class_indices","e933eb0a":"basemodel = InceptionV3(\n    include_top = False, \n    weights = 'imagenet', \n    input_tensor = Input((256, 256, 3)),\n)","735399a2":"basemodel.trainable = True","6c7c7a47":"basemodel.summary()","14082d40":"# Add classification head to the model\nheadmodel = basemodel.output\nheadmodel = GlobalAveragePooling2D()(headmodel)\nheadmodel = Flatten()(headmodel) \nheadmodel = Dense(256, activation = \"relu\")(headmodel)\nheadmodel = Dropout(0.3)(headmodel)\nheadmodel = Dense(128, activation = \"relu\")(headmodel)\nheadmodel = Dropout(0.3)(headmodel)\nheadmodel = Dense(3, activation = \"softmax\")(headmodel) # 3 classes\n\nmodel = Model(inputs = basemodel.input, outputs = headmodel)","336d2b2e":"model.summary()","7b6cf436":"# Compile the model\n\n# Given that COVID-19 spreads very quickly, it is important that we identify as many cases as possible.\n# We do not care a lot about Flase Positives (precision), because it may be okay to declare normal people as being COVID-19 positive.\n# However, we really really care about False Negatives (recall), because it is NOT okay to declare COVID-19 positive people as being normal!\n\nMyList = [\"accuracy\"]\nMyList += [Recall(class_id = i) for i in range(len(labels))] \nMyList += [Precision(class_id = i) for i in range(len(labels))]\n\nmodel.compile(\n    loss = \"categorical_crossentropy\",\n    optimizer = \"adam\",\n    metrics = MyList\n)\n","cea2cb33":"# Using early stopping to exit training if validation loss is not decreasing even after certain epochs (patience)\nearlystopping = EarlyStopping(\n    monitor = \"recall\",\n    patience = 20\n)\n\n# save the best model with lower loss\ncheckpointer = ModelCheckpoint(\n    filepath = \"weights.hdf5\", \n    save_best_only = True\n)","d1235c86":"# Previously we found that there was class imbalance. \n# We shall use class weights to tackle this before moving to training.\n\ntotal_wt = sum(class_len.values())\n\nweights = {\n    0: 0.5 * (1 - class_len[labels[0]]\/total_wt),\n    1: 0.5 * (1 - class_len[labels[1]]\/total_wt),\n    2: 0.5 * (1 - class_len[labels[2]]\/total_wt)\n}\nweights","26a6379a":"# Finally, fit the neural network model to the data.\n\nhistory = model.fit_generator(\n    train_generator,\n    class_weight = weights,\n    validation_data = val_generator,\n    steps_per_epoch = 32,\n    epochs = 100, \n    callbacks = [earlystopping, checkpointer]\n)","741a3d41":"# Plotting training and validation loss per epoch\n\ntrain_loss = history.history[\"loss\"]\nvalid_loss = history.history[\"val_loss\"]\n\nepochs = range(len(train_loss)) \n\nplt.plot(epochs, train_loss)\nplt.plot(epochs, valid_loss)\nplt.legend([\"Training Loss\", \"Validation Loss\"])\nplt.title(\"Training and Validation Loss\")","41ad9386":"# Plotting training and validation accuracy per epoch\n\ntrain_acc = history.history[\"accuracy\"]\nvalid_acc = history.history[\"val_accuracy\"]\n\nepochs = range(len(train_acc)) \n\nplt.plot(epochs, train_acc)\nplt.plot(epochs, valid_acc)\nplt.legend([\"Training Accuracy\", \"Validation Accuracy\"])\nplt.title(\"Training and Validation Accuracy\")","175e8640":"# Plotting training and validation recall per epoch\n\nfig, axs = plt.subplots(1, 3, figsize = (15, 5))\n\ntrain_rec_0 = history.history[\"recall\"]\nvalid_rec_0 = history.history[\"val_recall\"]\ntrain_rec_1 = history.history[\"recall_1\"]\nvalid_rec_1 = history.history[\"val_recall_1\"]\ntrain_rec_2 = history.history[\"recall_2\"]\nvalid_rec_2 = history.history[\"val_recall_2\"]\n\nepochs = range(len(train_rec_0)) \n\naxs[0].plot(epochs, train_rec_0)\naxs[0].plot(epochs, valid_rec_0)\naxs[0].legend([\"Training Recall (Class 0)\", \"Validation Recall (Class 0)\"])\naxs[0].set_title(\"Training and Validation Recall for class 0\")\n\naxs[1].plot(epochs, train_rec_1)\naxs[1].plot(epochs, valid_rec_1)\naxs[1].legend([\"Training Recall (Class 1)\", \"Validation Recall (Class 1)\"])\naxs[1].set_title(\"Training and Validation Recall for class 1\")\n\naxs[2].plot(epochs, train_rec_2)\naxs[2].plot(epochs, valid_rec_2)\naxs[2].legend([\"Training Recall (Class 2)\", \"Validation Recall (Class 2)\"])\naxs[2].set_title(\"Training and Validation Recall for class 2\")\n\nfig.tight_layout()","cf70a3d5":"# Plotting training and validation precision per epoch\n\nfig, axs = plt.subplots(1, 3, figsize = (15, 5))\n\ntrain_pre_0 = history.history[\"precision\"]\nvalid_pre_0 = history.history[\"val_precision\"]\ntrain_pre_1 = history.history[\"precision_1\"]\nvalid_pre_1 = history.history[\"val_precision_1\"]\ntrain_pre_2 = history.history[\"precision_2\"]\nvalid_pre_2 = history.history[\"val_precision_2\"]\n\nepochs = range(len(train_pre_0)) \n\naxs[0].plot(epochs, train_pre_0)\naxs[0].plot(epochs, valid_pre_0)\naxs[0].legend([\"Training Precision (Class 0)\", \"Validation Precision (Class 0)\"])\naxs[0].set_title(\"Training and Validation Precision for class 0\")\n\naxs[1].plot(epochs, train_pre_1)\naxs[1].plot(epochs, valid_pre_1)\naxs[1].legend([\"Training Precision (Class 1)\", \"Validation Precision (Class 1)\"])\naxs[1].set_title(\"Training and Validation Precision for class 1\")\n\naxs[2].plot(epochs, train_pre_2)\naxs[2].plot(epochs, valid_pre_2)\naxs[2].legend([\"Training Precision (Class 2)\", \"Validation Precision (Class 2)\"])\naxs[2].set_title(\"Training and Validation Precision for class 2\")\n\nfig.tight_layout()","464d3b59":"# Confusion Matrix \n\n# Since we do not have a lot of data, we did not split into training-validation-testing.\n# Instead we split into training-validation.\n# Strictly speaking, we should verify performance against new images from testing dataset.\n# However, we shall use images in validation dataset for testing. \n\n# There is one problem. Previously, we set shuffle = True in our generator.\n# This makes it difficult to obtain predictions and their corresponding ground truth labels.\n# Thus, we shall call the generator again, but this time set shuffle = False.\n\nval_generator = datagen_val.flow_from_directory(\n    directory = path,\n    classes = labels,\n    seed = 42,\n    batch_size = 32, \n    shuffle = False,\n    subset = 'validation'\n)\n\n# Obtain predictions\npred = model.predict_generator(val_generator) # Gives class probabilities\npred = np.round(pred) # Gives one-hot encoded classes\npred = np.argmax(pred, axis = 1) # Gives class labels\n\n# Obtain actual labels\nactual = val_generator.classes\n    \n# Now plot matrix\ncm = confusion_matrix(actual, pred, labels = [0, 1, 2])\nsns.heatmap(\n    cm, \n    cmap=\"Blues\",\n    annot = True, \n    fmt = \"d\"\n)\nplt.show()","000f7475":"# Classification Report\nprint(classification_report(actual, pred))","cf83e725":"# These results are not too great but not too bad either. \n# In this notebook, we saw how we can identify COVID-19 from Chest X-Rays.\n# Cheers. Happy Learning!","d1cad911":"We shall start with InceptionV3 architecture trained on imagenet and fine tune.","40ea48f2":"# Model training","be96d745":"# Model evaluation","138e0021":"# Detecting COVID-19 with Chest X-Rays","4bb538bf":"The data set used may be found at\nhttps:\/\/www.kaggle.com\/tawsifurrahman\/covid19-radiography-database","5d9b3d5f":"# Creating image generators","3de40ccb":"# Importing Libraries","d9148ac1":"# Visualizing Classes","99d3e657":"# Model building"}}