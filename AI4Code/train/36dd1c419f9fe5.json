{"cell_type":{"2a1beff8":"code","22c2447b":"code","97510a19":"code","5a3aa590":"code","d161eb94":"code","b33471bf":"code","1c7eae11":"code","fc6cd129":"code","44b515c1":"code","9c0077a9":"code","75a1461b":"code","cc1b0d20":"code","fbeb7d4e":"code","16dcd95e":"code","290a43ed":"code","4af43e56":"code","402de60f":"code","889d1bda":"code","b7567633":"code","83a39d98":"code","edbf898b":"code","89340631":"code","4cb0ecad":"code","57ea7519":"code","968f08dc":"code","173505d9":"code","7b637bcb":"code","7b3288ca":"code","1da264c1":"code","8b22883e":"code","ef260031":"code","aea564b6":"code","f49d15d7":"code","19074f71":"code","ca504413":"code","60ba49a4":"code","235a76ec":"code","afd8864f":"code","f52ba024":"code","76b951e0":"code","7245977b":"code","77ee46ac":"code","fd7d9087":"code","aab55edd":"code","8b892b37":"code","bb515658":"code","24fb4c0c":"code","27eecb13":"code","7ab3d9a5":"code","b1f50ae9":"code","903b2540":"markdown","82cb7886":"markdown","13dbe0f3":"markdown","61557d7a":"markdown","55876ec4":"markdown","b02f8a4f":"markdown","a66dcd0c":"markdown","c0d286a4":"markdown","1de94bc5":"markdown","355e154c":"markdown","5e589ed7":"markdown","7ab02946":"markdown","04de7605":"markdown","d11da5d9":"markdown","947a07a3":"markdown","918ecba1":"markdown","f0448164":"markdown","e5d03b25":"markdown","0bb028a2":"markdown","94d2eeca":"markdown","87852fc9":"markdown","a5fb5872":"markdown","476e4120":"markdown","4ad7aa84":"markdown","707ae20e":"markdown","9eda1421":"markdown","c5c61f37":"markdown","98c2d33a":"markdown"},"source":{"2a1beff8":"%reset -f\n\n# Import modules\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport sys\nfrom utils import *\n\nmpl.rcParams.update({'font.size': 13})\nmpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=wong_colors.values())\n\nprint('Python version', sys.version.split()[0])","22c2447b":"# Import training and test sets\nfolder = '\/kaggle\/input\/titanic\/'\n\ntrain = pd.read_csv(folder + 'train.csv')\ntest = pd.read_csv(folder + 'test.csv')\n\nprint('Training set shape:', train.shape)\nprint('Test set shape:', test.shape)\n\n# Convert columns names to lowercase\ntrain.columns = train.columns.str.lower()\ntest.columns = test.columns.str.lower()","97510a19":"train.head()","5a3aa590":"pd.concat([train, test]).describe()","d161eb94":"df_info(train)","b33471bf":"df_info(test)","1c7eae11":"df = train.sort_values('ticket', ignore_index=True)\ndf.loc[df['embarked'].isnull()]","fc6cd129":"(df['ticket'] == '113572').sum()","44b515c1":"df.loc[40:80, 'embarked'].value_counts()","9c0077a9":"train.loc[train['embarked'].isnull(), 'embarked'] = 'S'","75a1461b":"test.loc[test['fare'].isnull(), 'fare'] = test['fare'].mean()","cc1b0d20":"def plot_discrete_feature(train, feature):\n\n    survived = train.groupby(feature)['survived'].mean() * 100\n    x = range(len(survived))\n\n    _, axs = plt.subplots(1, 2, figsize=(14,5))\n\n    plt.sca(axs[0])\n    plt.bar(x, train.groupby(feature)['survived'].size())\n    plt.ylabel('N. of passengers')\n\n    plt.sca(axs[1])\n    plt.bar(x, survived)\n    plt.bar(x, 100 - survived, bottom=survived)\n    plt.ylabel('Survival percentage')\n    plt.legend(['Survived', 'Not survived'], loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\n\n    \n    for ax in axs:\n        set_spines_vis(ax=ax)      \n        ax.set_xticks(x)\n        ax.set_xticklabels(survived.index)\n        ax.set_xlabel(feature)","fbeb7d4e":"def plot_continuous_feature(train, feature, binwidth):\n\n    plt.figure(figsize=(10,5))\n    sns.histplot(train, x=feature, hue='survived', binwidth=binwidth, legend=False)\n    plt.legend(['Survived', 'Not survived'], frameon=False)\n    set_spines_vis()","16dcd95e":"plot_discrete_feature(train, 'pclass')","290a43ed":"plot_continuous_feature(train, 'fare', 5)","4af43e56":"plot_discrete_feature(train, 'sex')","402de60f":"plot_continuous_feature(train, 'age', 1)","889d1bda":"plot_discrete_feature(train, 'sibsp')","b7567633":"plot_discrete_feature(train, 'parch')","83a39d98":"plot_discrete_feature(train, 'embarked')","edbf898b":"def get_dummies(train, test, columns):\n\n    # Concatenate training and test sets\n    df = pd.concat([train[columns], test[columns]])\n\n    # Convert categorical variables into dummy variables\n    df = pd.get_dummies(df)\n\n    X_train = df.iloc[:train.shape[0]]\n    X_test = df.iloc[train.shape[0]:]\n\n    return X_train, X_test","89340631":"train['pclass'] = train['pclass'].astype('category')\ntest['pclass'] = test['pclass'].astype('category')\n\nfeatures = ['pclass', 'sex', 'sibsp', 'parch', 'embarked']\nX_train, X_test = get_dummies(train, test, features)\n\ny_train = train['survived']","4cb0ecad":"lr = LogisticRegression(max_iter=1000)\n\nsklearn_fit_eval(lr, {'X':X_train, 'y':y_train}, cv=5);","57ea7519":"def save_submission(y, passenger, filename):\n    submission = pd.DataFrame({'PassengerId':passenger,'Survived':y})\n    submission.to_csv(filename, index=False)\n\nsave_submission(lr.predict(X_test), test['passengerid'], 'submission_1.csv')","968f08dc":"def group_age(age):\n\n    age = age.fillna(-0.5)\n\n    bins = [-1, 0, 5, 12, 18, 35, 60, 150]\n    labels = ['Missing', 'Infant', 'Child', 'Teenager', 'Young adult', 'Adult', 'Senior']\n\n    return pd.cut(age, bins, labels=labels)","173505d9":"train['age_group'] = group_age(train['age'])\ntest['age_group'] = group_age(test['age'])\n\ntrain.groupby('age_group')['survived'].mean().plot.bar(rot=45, ylabel='Proportion of survivors')\nset_spines_vis()","7b637bcb":"train['name'].sample(10)","7b3288ca":"titles = train['name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\ntitles.unique()","1da264c1":"titles_map = {\n    'Mr' :         'Mr',\n    'Mme':         'Mrs',\n    'Ms':          'Mrs',\n    'Mrs' :        'Mrs',\n    'Master' :     'Master',\n    'Mlle':        'Miss',\n    'Miss' :       'Miss',\n    'Capt':        'Officer',\n    'Col':         'Officer',\n    'Major':       'Officer',\n    'Dr':          'Officer',\n    'Rev':         'Officer',\n    'Jonkheer':    'Royalty',\n    'Don':         'Royalty',\n    'Sir' :        'Royalty',\n    'Countess':    'Royalty',\n    'Dona':        'Royalty',\n    'Lady' :       'Royalty'\n}\n\ndef extract_title(names):\n    '''Extracts the title from the passenger names.'''\n\n    return names.str.extract(' ([A-Za-z]+)\\.', expand=False).map(titles_map)","8b22883e":"train['title'] = extract_title(train['name'])\ntest['title'] = extract_title(test['name'])\n\ntrain.groupby('title')['survived'].mean().plot.bar(rot=45, ylabel='Proportion of survivors')\nset_spines_vis()","ef260031":"features = ['pclass', 'sex', 'sibsp', 'parch', 'embarked', 'age_group', 'title']\nX_train, X_test = get_dummies(train, test, features)\n\nsklearn_fit_eval(lr, {'X':X_train, 'y':y_train}, cv=5);","aea564b6":"save_submission(lr.predict(X_test), test['passengerid'], 'submission_2.csv')","f49d15d7":"models = {\n    'Logistic regression':LogisticRegression(max_iter=1000),\n    'Decision tree':DecisionTreeClassifier(random_state=0),\n    'Random forest':RandomForestClassifier(random_state=0),\n    'Neural network':MLPClassifier(max_iter=1000, random_state=0)\n}\n\nfit_params = {'X':X_train, 'y':y_train}\n\nfor k, v in models.items():\n    \n    print('{:>19}; '.format(k), end='')\n    sklearn_fit_eval(v, fit_params, cv=5);","19074f71":"create_params = {'max_iter': 1000, 'C': np.logspace(-1, 1, 5)}\nfit_eval_params = {'fit_params': fit_params, 'cv': 5, 'verbose': False}\n\nplot_models_metrics(LogisticRegression, create_params, sklearn_fit_eval, fit_eval_params, axes_params={'xscale': 'log'})","ca504413":"create_params['C'] = 10**0.5\nmodels['Logistic regression'] = LogisticRegression(**create_params)","60ba49a4":"create_params = {'random_state': 0, 'max_depth': np.arange(2, 7)}\n\nplot_models_metrics(DecisionTreeClassifier, create_params, sklearn_fit_eval, fit_eval_params)","235a76ec":"create_params['max_depth'] = 3\nmodels['Decision tree'] = DecisionTreeClassifier(**create_params)","afd8864f":"create_params = {'random_state': 0, 'max_depth': np.arange(2, 10)}\n\nplot_models_metrics(RandomForestClassifier, create_params, sklearn_fit_eval, fit_eval_params)","f52ba024":"create_params['max_depth'] = 5\nmodels['Random forest'] = RandomForestClassifier(**create_params)","76b951e0":"# Tune the initial learning rate\nvals = list(np.logspace(-4, -2, 4))\ncreate_params = {'max_iter':1000, 'random_state':0, 'learning_rate_init':vals}\n\nsklearn_plot_losses(MLPClassifier, create_params, fit_params, ['{:.5f}'.format(val) for val in vals])","7245977b":"create_params['learning_rate_init'] = 0.002\n\n# Tune the batch size\ncreate_params['batch_size'] = [128, 256, 512]\n\nsklearn_plot_losses(MLPClassifier, create_params, fit_params)","77ee46ac":"create_params['batch_size'] = 256","fd7d9087":"create_params['hidden_layer_sizes'] = list(range(2, 5))\nfit_eval_params = {'fit_params': fit_params, 'cv': 5, 'verbose': False}\n\nplot_models_metrics(MLPClassifier, create_params, sklearn_fit_eval, fit_eval_params)","aab55edd":"create_params['hidden_layer_sizes'] = 3\ncreate_params['alpha'] = list(np.logspace(-3, 0, 4))\n\nplot_models_metrics(MLPClassifier, create_params, sklearn_fit_eval, fit_eval_params, axes_params={'xscale': 'log'})","8b892b37":"create_params['alpha'] = 0.1\nmodels['Neural network'] = MLPClassifier(**create_params)","bb515658":"%%time\n\ngrid_params = {\n    'hidden_layer_sizes': range(2, 5),\n    'alpha': np.logspace(-3, 0, 4)\n}\n\ngrid = GridSearchCV(MLPClassifier(learning_rate_init=0.002, batch_size=256, max_iter=1000, random_state=0), grid_params, cv=5)\ngrid.fit(**fit_params)\n\nprint('Best accuracy: {:.2f}%'.format(grid.best_score_*100))\nprint('Best parameters: {}'.format(grid.best_params_))","24fb4c0c":"for k, v in models.items():\n    \n    print('{:>19}; '.format(k), end='')\n    sklearn_fit_eval(v, fit_params, cv=5);","27eecb13":"save_submission(models['Random forest'].predict(X_test), test['passengerid'], 'submission_3.csv')","7ab3d9a5":"prob = np.zeros((X_test.shape[0],2))\n\nfor model in models.values():\n    \n    prob += model.predict_proba(X_test)\n\ny_test = prob.argmax(axis=1)","b1f50ae9":"save_submission(y_test, test['passengerid'], 'submission_4.csv')","903b2540":"The title of each name might be useful information. We will create a new column with this.","82cb7886":"# Titanic: EDA, model comparison and tuning\n\n# Table of contents\n\n* [Introduction](#Introduction)\n* [Import data](#Import-data)\n* [Exploratory data analysis](#Exploratory-data-analysis)\n* [Feature engineering](#Feature-engineering)\n* [Model selection and tuning](#Model-selection-and-tuning)\n* [Conclusions and further developments](#Conclusions-and-further-developments)\n* [Acknowledgements](#Acknowledgements)\n\n# Introduction\n\nThe objective of the Kaggle competition [Titanic: Machine Learning from Disaster](https:\/\/www.kaggle.com\/c\/titanic) is to develop machine learning models to predict which passengers survived the shipwreck. This type of machine learning is called [classification](https:\/\/en.wikipedia.org\/wiki\/Statistical_classification). More specifically, since there are only two different states we are classifying (survived or not survived), it is called [binary classification](https:\/\/en.wikipedia.org\/wiki\/Binary_classification).\n\nThis notebook focus on exploratory data analysis, feature engineering, and model selection and hyperparameter tuning. If you have further ideas to improve this notebook or anything you don\u2019t understand, please leave a comment.\n\n# Import data","13dbe0f3":"The random forest had the highest validation accuracy (82.83%). Now we will predict which passengers of the test set survived.","61557d7a":"Most of them embarked at Southampton, and so we will use that value.","55876ec4":"The validation accuracy increased from 78.90% to 82.38%. Now we will predict which passengers of the test set survived.","b02f8a4f":"Another way to engineer features is by extracting data from text columns. Let's look at a random sample of the column `name`.","a66dcd0c":"We will start by using the Scikit-learn [LogisticRegression](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html) class to predict if the passengers survived or not. We can\u2019t use the test set to evaluate the accuracy of our model because it does not have the column `survived`. We would need to submit to Kaggle every time we wanted to assess the accuracy of the predictions. We could also fit and predict using the entire training dataset, but this would likely overestimate the accuracy since our model would perform worse on unseen data. Instead, we can use cross-validation techniques to train and test our model on different splits of our data and then average the accuracy scores. We will use the [k-fold cross-validation method](https:\/\/en.wikipedia.org\/wiki\/Cross-validation_(statistics)#k-fold_cross-validation), in which the original sample is randomly partitioned into *k* equal sized subsamples. Of the *k* subsamples, a single subsample is retained as the validation data, and the remaining *k* \u2212 1 subsamples are used as training data. The cross-validation process is then repeated *k* times, with each of the *k* subsamples used exactly once as the validation data. The *k* results can then be averaged to produce a single estimation. We will use the utility function **sklearn_fit_eval** with a 5-fold cross validation.","c0d286a4":"Regarding the decision tree and random forest classifiers, we decided to tune the tree's maximum depth.","1de94bc5":"The test set predictions were submitted to Kaggle, and an accuracy of 78.23% was obtained. Although the validation accuracy increased, the test accuracy decreased. The validation and test sets might have different distributions.\n\nWe can try to improve our models by using ensemble techniques. These techniques involve making a prediction that is the average of those obtained with different models. In classification, a hard voting ensemble involves summing the votes for all class labels and predicting the class with the most votes. A soft voting ensemble involves summing the predicted probabilities for class labels and predicting the class label with the largest sum probability. We will use soft voting.","355e154c":"We can observe that:\n\n* Passengers with upper-class tickets had better chances of survival than the rest;\n* Females survived in much higher proportion than males;\n* People younger than five had more chances of surviving, while the rest did not;\n* Passengers with no family members or with a lot of members have fewer chances of surviving;\n* Passengers with lower fares had worse chances of survival;\n* The port where the passenger embarked seems to influence the chances of survival.\n\nThe fare seems to capture the same information as the ticket class, and hence it will not be used. The columns `pclass`, `sex` and `embarked` are categorical features. Although the column `pclass` is numeric, there is not a numeric relationship between the different values. For instance, class 3 is not the triple of class 1. Most machine learning algorithms cannot understand text labels, and so we have to convert our values into numbers. The pandas function [get_dummies](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.get_dummies.html) turns each categorical feature into a series of zeros and ones.","5e589ed7":"The columns `age` and `cabin` have too many missing values and will not be used. Now let's plot our data.","7ab02946":"Now let's tune the hyperparameters of the neural network. We will start with the learning rate and minibatches' size. The utility function **sklearn_plot_losses** is used to compare the loss curves.","04de7605":"This method was much more computationally expensive and led to the same values. Nevertheless, it is always a good method to use when we have the computational power and want to get the most optimised results. It is good practice to use the utility functions to perform a coarser search and the class GridSearchCV to refine the results. We will now fit and evaluate the models with the optimised hyperparameters.","d11da5d9":"The test set predictions were submitted to Kaggle, and an accuracy of 78.71% was obtained.\n\n# Conclusions and further developments\n\nThis notebook uses machine learning models to predict which passengers survived the Titanic shipwreck. It explores how feature engineering, model selection, and hyperparameter optimization influences the accuracy of our model. The best model has a 78.95% test accuracy. To further improve our models, we can:\n\n* Read more about the titanic and this Kaggle competition to get ideas for new features.\n* Use different models such as support vector machines or gradient boosted trees.\n\nAlso, there are several other excellent notebooks related to this competition. Thanks for reading my notebook and any comments and suggestions are very welcome.\n\n# Acknowledgements\n\nThese excellent references inspired some of the code used in this notebook:\n* [Titanic Data Science Solutions](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions) by Manav Sehgal\n* [Titanic - Advanced Feature Engineering Tutorial](https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial) by Gunes Evitan\n* [How am I doing with my score?](https:\/\/www.kaggle.com\/pliptor\/how-am-i-doing-with-my-score\/report) by Oscar Takeshita\n* [Predicting the Survival of Titanic Passengers](https:\/\/towardsdatascience.com\/predicting-the-survival-of-titanic-passengers-30870ccc7e8) by Niklas Donges","947a07a3":"We have too many unique values, which might lead to overfitting. We will group these titles into the categories `Mr`, `Mrs`, `Master`, `Miss`, `Officer` and `Royalty`.","918ecba1":"The logistic regression model has the highest validation accuracy. Now let\u2019s optimise the hyperparameters of each model. The utility function **plot_models_metrics** is used to compare the training and validation accuracies and training times. Although we tunned several hyperparameters for each model, only those that led to a considerable improvement are shown. We will start with the hyperparameter that controls the regularisation of the logistic regression model.","f0448164":"The columns of this DataFrame are:\n\n* `passengerid` - Unique identifier of each passenger (used for submissions).\n* `survived` - Whether the passenger survived or not (0=No, 1=Yes).\n* `pclass` - The class of the ticket the passenger purchased (1=1st, 2=2nd, 3=3rd).\n* `name` - Name of the passenger.\n* `sex` - The passenger's sex.\n* `age` - The passenger's age in years.\n* `sibsp` - The number of siblings or spouses the passenger had aboard the Titanic.\n* `parch` - The number of parents or children the passenger had aboard the Titanic.\n* `ticket` - The passenger's ticket number.\n* `fare` - The fare the passenger paid.\n* `cabin` - The passenger's cabin number.\n* `embarked` - The port where the passenger embarked (C=Cherbourg, Q=Queenstown, S=Southampton).\n\nBoth sets have 11 features. The extra column of the training set is the target variable `survived`.\n\n# Exploratory data analysis\n\nNow let\u2019s look at the columns\u2019 data types and the number of missing values.","e5d03b25":"Regarding the missing fare value, we will assign the mean value.","0bb028a2":"We can also use the class [GridSearchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html) of the Scikit-learn library to tune the hyperparameters. Instead of varying each hyperparameter each time, this class tries all combinations of values.","94d2eeca":"No other passenger shares that ticket. Now let\u2019s see where other passengers with similar tickets embarked:","87852fc9":"The validation accuracy (78.90%) is lower than the training accuracy (79.24%), which might mean the model is overfitting. We will deal with the overfitting in section [Model selection and hyperparameter tuning](#Model-selection-and-hyperparameter-tuning). Now let\u2019s calculate the predictions for the test set.","a5fb5872":"The model converges faster when using a learning rate of 0.01, but the loss curve is not smooth. A value of 0.00215 will be used instead. A similar approach is used for the minibatches' size. Now let\u2019s tune the number of units of our hidden layer and regularization used.","476e4120":"They both have the same ticket. Let\u2019s see how many passengers have that ticket.","4ad7aa84":"The test set predictions were submitted to Kaggle, and an accuracy of 78.95% was obtained.\n\n# Model selection and tuning\n\nWe can also try different machine learning models to improve performance. Each algorithm has different strengths and weaknesses, and so we need to select the algorithm that works best with our specific data. We will use the Scikit-learn library to compare the performance of the following algorithms:\n\n* [LogisticRegression](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html) - Logistic regression classifier.\n* [DecisionTreeClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html) - Decision tree classifier.\n* [RandomForestClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html) - Random forest classifier.\n* [MLPClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neural_network.MLPClassifier.html) - Neural network classifier.","707ae20e":"The test set predictions were submitted to Kaggle, and an accuracy of 76.79% was obtained.\n\n# Feature engineering\n\nFeature engineering is the process of transforming raw data into features that better represent the underlying problem and enable more accurate predictive models. One common way to engineer a feature is using a technique called binning. Binning is when you take a continuous feature, like the age, and separate it into several ranges (or 'bins'), turning it into a categorical variable. This can be useful when there are patterns in the data that are nonlinear and you're using a linear model like logistic regression. Let\u2019s group the `age` column into the following age groups:\n\nAge group | Age interval\n--- | ---\nInfant | 0-5\nChild  | 5-12\nTeenager | 12-18\nAdult | 18-60\nSenior | 60+","9eda1421":"We can see that:\n\n* Only 38.4% of the passengers survived;\n* The age ranges from 0.17 to 80;\n* Some columns have missing values (missing values of column `survived` are from the test set).","c5c61f37":"Let's calculate our model's accuracy with the column `age` grouped in bins and the additional feature `title`.","98c2d33a":"The columns' data types seem to be correct, but we need to convert some features so that machine learning algorithms can process them. The following columns have missing values:\n\n* `age` and `cabin` of both datasets\n* `embarked` of the training set\n* `fare` of the test set\n\nThe passengers that do not have the port where they embarked are:"}}