{"cell_type":{"0d71bc14":"code","49bb7bb7":"markdown","d6573db1":"markdown","dff2dc51":"markdown","83405a3f":"markdown","512ed688":"markdown","6bed4764":"markdown","70d4b42b":"markdown","dc49caac":"markdown","37c4c7ca":"markdown","3b323730":"markdown","353e7d34":"markdown","e0da4865":"markdown","f6235958":"markdown","7fecd9e5":"markdown","9426ddff":"markdown","fea4454e":"markdown","638468ee":"markdown","4bc26c82":"markdown","507081bd":"markdown","acdfb140":"markdown","e5008f9c":"markdown","dd78fc8c":"markdown","e21cda01":"markdown","1b124444":"markdown","ecde9851":"markdown","11a123fb":"markdown","e7717f97":"markdown","d0a2dbb7":"markdown","26a3d894":"markdown","9d185453":"markdown","de1f1d76":"markdown","018d1d58":"markdown","b0711754":"markdown","b37b670f":"markdown","e3d7e805":"markdown","eeedeba3":"markdown","62b374bf":"markdown","42ac8b09":"markdown","0ad8e4c4":"markdown","23c46da4":"markdown","dd488378":"markdown","9d4fa715":"markdown","7fc9fe64":"markdown","bee2a464":"markdown","d4870e6c":"markdown","c24f5ad1":"markdown","f5e18920":"markdown","6978468f":"markdown","c4a6109e":"markdown"},"source":{"0d71bc14":"import streamlit as st\nimport numpy as np\nimport pandas as pd\nimport re\nimport sys \nimport os\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nimport textract\nimport spacy\nfrom fuzzywuzzy import fuzz\n\nimport cv2\nfrom PIL import Image \nimport pytesseract \nfrom pdf2image import convert_from_path \n\nfrom spacy.tokenizer import Tokenizer\nfrom spacy.lang.en import English","49bb7bb7":"# Sample Execution","d6573db1":"# Introduction","dff2dc51":"For this task, we will be using pdf2image and poppler. pdf2image is a popular python library to convert pdfs to images. Poppler is an open source set of libraries and command line tools, very useful for dealing with PDF files. We can take help of poppler to extract a single page from the pdf, convert that one page into an image, use pytessesract to extract text from this image and finally delete the image.","83405a3f":"# Problem Statement","512ed688":"**NOTE** : You will have to download poppler from the link and provide the path to the bin folder on your local device.","6bed4764":"**Before starting with the code, do check what the following lists are used for.**","70d4b42b":"![Capture8.PNG](attachment:Capture8.PNG)","dc49caac":"![image.png](attachment:image.png)","37c4c7ca":"![Capture3.PNG](attachment:Capture3.PNG)\n","3b323730":"# Finding The Context","353e7d34":"**Myself <a href='https:\/\/www.linkedin.com\/in\/aryan-parab-0b44991b2\/'>Aryan Parab<\/a> and my team member<a href='https:\/\/www.linkedin.com\/in\/aden-gomes-2b2a091b1\/'> Aden Gomes<\/a> created many such notebooks as part of the course work under \"Master in Data Science Programme\" at Suven , under mentor-ship of <a href='https:\/\/www.linkedin.com\/in\/rocky-jagtiani-3b390649\/'>Rocky Jagtiani<\/a> .**","e0da4865":"![Capture5.PNG](attachment:Capture5.PNG)","f6235958":"![Capture7.PNG](attachment:Capture7.PNG)","7fecd9e5":"# The Entire Code","9426ddff":"#Make the necessary imports\nimport streamlit as st\nimport numpy as np\nimport pandas as pd\nimport re\nimport sys \nimport os\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nimport textract\nimport spacy\nfrom fuzzywuzzy import fuzz\n\nimport cv2\nfrom PIL import Image \nimport pytesseract \nfrom pdf2image import convert_from_path \n\nfrom spacy.tokenizer import Tokenizer\nfrom spacy.lang.en import English\n\nstop_words = set()\nstop_words.add(\".\")\nstop_words.add(\",\")\nstop_words.add(\"!\")\nstop_words.add(\"(\")\nstop_words.add(\")\")\n\npdf = []                #to store the names of the documents in the user entered path\ndocument_text = []      #to store all the text(data) from the documents\n\ndocument_text_split=[]  #it is a list of lists. Lets take an example to explain. document_text \n                        #is a list and each element of document_text\n                        #contains the text data from a document. This text data is only one string\n                        #and not seperated into\n                        #pages. To do so, we will be splitting this string data by \"\\n\".\n                        #Thus document_text_split element 0 is a list which\n                        #contains the text lines splitted by \"\\n\"(or new line in the document)\n\nresult=[]               #to store the output for matching sentences and context\nimage_names = []        #to store the names of images which will get generated\n\n#pytesseract is used to extract data from images (documents in format \".png\"\/\".jpg\/'.jpeg\")\n#to use pytessesract on windows, it is necessary to provide the path for tesseract.exe\npytesseract.pytesseract.tesseract_cmd = 'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe'\ndef fileget(text1):\n    counter=-1              #counting the documents\n    path=text1               #path entered by the user to the directory\n\n    for f in os.listdir(path):\n        if f.endswith('.txt') or f.endswith('.pdf') or f.endswith('.xlsx') or f.endswith('.docx') or f.endswith('.pptx') or f.endswith('.png') or f.endswith('.jpg') :\n            text='' \n            pdf.append(f)       #selecting pdfs, ppts, excel files, text files,images and storing them\n            path1 = path+f      #path1 is the complete path of the curren document\n            counter=counter+1   #counting the number on documents\n            #if our document is an image stored in \".png\"\/\".jpg\/'.jpeg\" format, we will directly use\n            #pytesseract to extract text from it\n\n            if f.endswith('.png') or f.endswith('.jpg'):    \n                img=cv2.imread(path1)\n                text=pytesseract.image_to_string(img)\n\n            #else we will use textract to extract text from others(including scanned pdfs)\n            else:\n                text = textract.process(path1)\n                text = b''.join([text])             #to get the text in a proper format\n                inter = text.decode(\"utf-8\")       \n\n            #You might be familiar that one cannot select text if a document is scanned. For example I used\n            #CamScanner to scan my Aadhar Card. We cannot directly extract text from such type of pdfs. \n            #If we use textract to do so, it will give the output in the form of \"\\x0c\\x0c\\x0c\\x0c\\x0c\".\n\n            #How to get the text from such documents?\n            #We first use poppler to get individual pages of a scanned document and save each page as an image\n            #Then we use pytesseract to extract the text from those images\n            #After doing so, we delete the images formed.\n\n            #Thus to achieve it, we do the following\n            #First check whether it is a scanned pdf\n            if \"\\x0c\\x0c\\x0c\\x0c\\x0c\" in inter:\n                pages=convert_from_path(path1,poppler_path=r'C:\\\\Users\\\\Admin\\\\OneDrive\\\\Desktop\\\\poppler-0.68.0\\\\bin')\n                #poppler is used to get each page of that pdf and store it in pages\n                image_counter=1\n                text = \" \"           #to store the text of current document\n\n                #The following steps involve creating images of each page. The names\n                #of these images will be stored in this list and we will use it while\n                #deleting the images that were created \n\n                for page in pages:\n\n                    filename= \"page_\"+str(image_counter)+\"_\"+str(counter)+\".jpg\"    #creating a unique image file for the cureent page\n                    page.save(path+filename,'JPEG')             #storing image of one page at the path\n                    image_counter=image_counter+1\n                    filename=path+filename\n\n                    text = text + str(((pytesseract.image_to_string(Image.open(filename))))) #extracting text from images using pytesseract\n                    image_names.append(filename)                #storing filenames to delete later\n\n                \n            inter=text \n            document_text.append(inter) \n\n     #images of each page from the pdf will be created. Here we will\n     #delete those images\n    for doc in image_names:     \n        os.remove(doc)\n              \n     \n    #now we will split the text in each document by paragraphs. Paragraphs usually start on \"\\n\" or newline\n    for i in document_text:\n        document_text_split.append(i.split('\\n'))\n    \n    #return document_text_split\n\n\ndef findcontext(context_to_find, match=80):\n     \n      count=-1     \n      lenfeature = len(context_to_find.split(\" \"))  \n\n      for one_document in document_text_split:\n        count=count+1       #keeping track of documents\n        \n        for line in one_document:  #taking each sentence from the document\n            #preparing sentence to compare\n            word_tokens = nltk.word_tokenize(line)\n            filtered_word_tokens = [w for w in word_tokens if not w in stop_words]\n            for i in range (len(word_tokens)-lenfeature+1):\n                wordtocompare = \"\"\n                j=0\n                for j in range(i, i+lenfeature):\n                    wordtocompare = wordtocompare+\" \"+word_tokens[j].lower()\n                wordtocompare.strip()\n                \n                if wordtocompare==\"\": continue\n                else:\n                    #using fuzzywuzzy package to compare the sentence and word\n                    if fuzz.ratio(wordtocompare,context_to_find.lower())>= match: \n\n                        result.append([line,context_to_find,pdf[count]])   #storing the sentence, context_to_find,\n                                                                           #and the pdf name the sentence belongs to\n\n\n\n    st.title(\"Document Context Extractor\")\n    text = st.text_input(\"Path\")                         #getting directory path from the user\n    if st.button(\"Get file\"):\n      document_text_split = fileget(text)  \n      st.markdown(\"The PDFs in the folder chosen are:\")\n      for i in pdf:\n      st.markdown(i)\n    context_to_find=st.text_input(\"Enter context to find: \")              #get the context_to_find to search \n    result = findfeatures1(context_to_find,document_text_split)                                                 \n    for i in result:\n        st.markdown(i[2] + \" :\\n\" + i[0] + \"\\n\")                     #printing the output","fea4454e":"![Capture6.PNG](attachment:Capture6.PNG)","638468ee":"Now we will get two outputs one of all the documents in the folder and the next is the output of the text in the document folder similar to the context we added.","4bc26c82":"![Capture12.PNG](attachment:Capture12.PNG)","507081bd":"Finally , we need to call all the above mentioned functions as well.","acdfb140":"Then, we loop through all the documents present. If the document is\u00a0.pdf,\u00a0.ppt,\u00a0.xlsx\u00a0, we use textract to extract the text. Textract is a very convenient library which can remove all text from any document in a formatted manner. \n\n**For more information check out the link**\n>https:\/\/textract.readthedocs.io\/en\/stable\/","e5008f9c":"Now, for extracting text data from images(.png\u00a0,\u00a0.jpeg), we will be taking the help pytesseract. A very useful library, a single statement of pytessesract in our code can extract all the text from our image files. Of course one can add attributes and other parameters for their desired results.\n\n**For more information on pytesseract, check out**\nhttps:\/\/nanonets.com\/blog\/ocr-with-tesseract\/\n","dd78fc8c":"The text data extracted is stored in the form of a list. Each element of that list represents one document and the respective document name is stored in another list (pdf). Thus at the time of output\u00a0,we can easily show to which pdf the matching phrase belongs to. Also, to extract only the relevant phrases, each element of the documents list is split by '\\n' or new line. New line in a text document usually means starting of a new paragraph. Here, we use the\u00a0.split('\\n') function to achieve my goal as the text is in str format. Thus, we get a list of lists, the outer elements are the entire document while each element is the text data of that document split up by '\\n'.","e21cda01":"# Conclusion","1b124444":"![image.png](attachment:image.png)![](http:\/\/)","ecde9851":"1. Accept the path where all the files from which text data is to be extracted as an input.\n2. Using the python libraries, extract text data from excel, word, text, ppt, pdf, image files.\n3. Accept a context to find in these documents (eg. Global Warming).\n4. Display the result which contains the text extracted and the name of the respective document from which it was extracted. \n","11a123fb":"Thus, we get the following output.\nWe have successfully extracted the context required from the documents in the path provided. The user only had to provide two inputs to get the desired result instead of opening each document and searching manually for the same.\nThe output is not completely proper due to pharses that give high match with the context. I will be trying to return page number for the context along with the extracted text and document name.","e7717f97":"One solution is as follows. After using textract on such scanned pdfs, we get the output in a certain format ('\\x0c\\x0c\\\u2026.'). By this, we can identify whether the document is a scanned pdf or not. After verifying  this, the next step will be converting the pages to images and using pytesseract to extract the text from those images. Simple right! Now how to get select a single page from a pdf and convert it\u00a0?","d0a2dbb7":"![Capture2.PNG](attachment:Capture2.PNG)","26a3d894":"Even after this, there is still one type of file from which data has not been extracted properly yet. Scanned pdfs are pdfs from which one cannot directly select the text. Example of a scanned pdf is when we use CamScanner to scan our Passport or some legal document or just a text book. We are familiar that we cannot directly select the text present in this pdf. So what to do?","9d185453":"Many a times we face a problem wherein we want to find some specific data from multiple files. It is quite labourous to open up each file and find the required information. For example in your college folder, you have PPTs and PDFs related to all your subjects (eg. Maths, Chemistry, History, Computers etc.) in an unorganized manner and you want to find something realted to, let's say, the greenhouse effect. It would be tiresome to open up each pdf or ppt and scroll over to get the information regarding the same. So to minimize this tedious task, ypu can refer to the following code.","de1f1d76":"First we get the path from the user. Here, we will be extracting text from files with extension\u00a0.txt,\u00a0.pdf,\u00a0.xlsx,\u00a0.ppt,\u00a0.png,\u00a0.jpg,\u00a0.jpeg. One requirement is that all the files from which the data is to be extracted must be in the same directory (folder). The user has to input the path of this folder.","018d1d58":"![Capture16.PNG](attachment:Capture16.PNG)","b0711754":"![Capture1.PNG](attachment:Capture1.PNG)","b37b670f":"1. All the files from which text is to be extracted must be present in the same folder\/path.\n2. The following code will work on your local pc and not on kaggle as it requires a few necessary installations(pytesseract and poppler).","e3d7e805":"![image.png](attachment:image.png)","eeedeba3":"![Capture11.PNG](attachment:Capture11.PNG)","62b374bf":"**NOTE** : to use pytesseract, you will have to download pytesseract from the link provided and then specify the path as follows.**","42ac8b09":"**For more information and installing pdf2image and poppler\u00a0, refer**\nhttps:\/\/pypi.org\/project\/pdf2image\/","0ad8e4c4":"First, we provide a path as an input. The code will now extract all the text data from the documents. This will take sometime further we provide a context to find from all the files selected.","23c46da4":"# Getting The Files","dd488378":"**The following kernal contains the entire code for you to directly pickup.**","9d4fa715":"# Important\n","7fc9fe64":"![Capture15.PNG](attachment:Capture15.PNG)","bee2a464":"![Capture4.PNG](attachment:Capture4.PNG)","d4870e6c":"Well, all the text extracted from either images, documents or scanned pdf needs to be collected at a particular place. Thus, the text from each document is appended in document_text.","c24f5ad1":"# Necessary Imports","f5e18920":"We iterate through each document and for each document, we iterate through its lines. Then we compare the line and context using fuzzywuzzy. If fuzzratio is greater than 80, we store the line, context and pdf name(current one)in the final list. After completion\u00a0, we print out this list.","6978468f":"Now that we have extracted all the text data required, lets move to the next step.\nWe will ask the user user to enter a word or phrase as an input. Then, we will use fuzzywuzzy package to compare the match between the user entered context and a line from the text.","c4a6109e":"**Now to delete the files.**"}}