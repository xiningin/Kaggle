{"cell_type":{"3e5d7aea":"code","1d9eb570":"code","2c797399":"code","cbd475b8":"code","e6ca40b1":"code","63f032f5":"code","a8a69d54":"code","fe987066":"code","6acbf1fa":"code","e92282ca":"code","a474a97f":"code","2d3667f2":"code","9de2f930":"code","e0059d1c":"code","e3827b08":"code","e58b65ba":"code","10d9ed44":"code","35246246":"code","96d092bb":"code","1ecbd809":"code","a65d4ea9":"code","5709cbb1":"code","b06dfc10":"code","93bf611c":"code","2d2187df":"code","48d0bacd":"code","108c14f0":"markdown","b13370b0":"markdown","8324c699":"markdown"},"source":{"3e5d7aea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1d9eb570":"# Import The Libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping","2c797399":"sns.set(style='white', context='notebook', palette='deep')","cbd475b8":"# Import The Dataset\n\ntrain = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\nsubmissoion = pd.read_csv('\/kaggle\/input\/digit-recognizer\/sample_submission.csv')","e6ca40b1":"train","63f032f5":"test","a8a69d54":"# Split The Train_Set\n\ny_train = train['label']\nX_train = train.drop('label', axis = 1)\ng = sns.countplot(y_train.value_counts())","fe987066":"del train","6acbf1fa":"# Check For Null Values\n\nX_train.isnull().any().describe()","e92282ca":"test.isnull().any().describe()","a474a97f":"\nX_train = X_train \/ 255.0\ntest = test \/ 255.0","2d3667f2":"# Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1)\n\nX_train = X_train.values.reshape(-1, 28, 28, 1)\ntest = test.values.reshape(-1, 28, 28, 1)","9de2f930":"# Label Encoding\n# # Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\n\ny_train = to_categorical(y_train, num_classes = 10)","e0059d1c":"# Split The Data\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size= 0.1, random_state = 2)","e3827b08":"g = plt.imshow(X_train[0][:, :, 0])","e58b65ba":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\nfrom keras.optimizers import RMSprop\n\nmodel = Sequential()\nmodel.add(Conv2D(32, (5,5), activation = 'relu', padding = 'Same', input_shape = (28, 28, 1)))\nmodel.add(Conv2D(32, (5,5), activation = 'relu', padding = 'Same', input_shape = (28, 28, 1)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size = (2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, (5,5), activation = 'relu', padding = 'Same'))\nmodel.add(Conv2D(64, (5,5), activation = 'relu', padding = 'Same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size = (2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = 'softmax'))","10d9ed44":"model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])","35246246":"model.summary()","96d092bb":"#Early Stop :- To prevent over fitting we will stop the learning after 10 epochs and val_loss value not decreased\n\nearlystop = EarlyStopping(patience=10)\n\n#Learning Rate Reduction :-  We will reduce the learning rate when the accuracy not increase for 2 steps\n\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=2, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)\ncallbacks = [earlystop, learning_rate_reduction]","1ecbd809":"batch_size = 86\nepochs = 2","a65d4ea9":"# Data augmentation\n\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\ndatagen.fit(X_train)","5709cbb1":"# With epochs = 10 i get 99% accuracy, here i will do just 2 epoch\nhistory = model.fit_generator(datagen.flow(X_train, y_train, batch_size = batch_size),\n                            epochs = epochs, \n                            validation_data = (X_test, y_test),\n                            verbose = 2,\n                            steps_per_epoch=X_train.shape[0] \/\/ batch_size,\n                            callbacks = callbacks\n)","b06dfc10":"# Save The Model \nmodel_yaml = model.to_yaml()\nwith open(\"model.yaml\", \"w\") as yaml_file:\n    yaml_file.write(model_yaml)\n# serialize weights to HDF5\nmodel.save_weights(\"model.h5\")\nprint(\"Saved model to disk\")","93bf611c":"# Predict\nresult = model.predict(test)","2d2187df":"# select the indix with the maximum probability\nresult = np.argmax(result, axis = 1)\n\nresult = pd.Series(result, name=\"Label\")","48d0bacd":"submission = pd.concat([pd.Series(range(1,28001), name = \"ImageId\"), result],axis = 1)\nsubmission.to_csv(\"cnn_mnist_datagen.csv\", index=False)","108c14f0":"## Callbacks ","b13370b0":"## Normalization\n\nWe perform a grayscale normalization to reduce the effect of illumination's differences.\nMoreover the CNN converg faster on [0..1] data than on [0..255].","8324c699":"## Build CNN  "}}