{"cell_type":{"1d814409":"code","4623af92":"code","49051b64":"code","eacc1893":"code","e541805d":"code","d1e822f3":"code","35fb7f9b":"code","c0cb1219":"code","ef887dd5":"code","22321d3d":"code","3e7e4148":"code","f078c9ed":"code","b8d6a163":"code","080debfe":"code","913e015a":"markdown","894e3a6a":"markdown","a58e8e98":"markdown","65a2c994":"markdown","bf77bd13":"markdown","80b749db":"markdown","6fe17a55":"markdown","09859f3d":"markdown"},"source":{"1d814409":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4623af92":"import random\nimport numpy as np\nimport pandas as pd","49051b64":"train_df = pd.read_csv(\"\/kaggle\/input\/killer-shrimp-invasion\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/killer-shrimp-invasion\/test.csv\")\n\ncolumns = list(train_df.columns)\nfeatures = columns[1:-1]\ncategoricals = ['Substrate']\ntarget = columns[-1]\n\nprint(train_df.Presence.value_counts())","eacc1893":"# filling none with medians and modes (when categoricals)\n\nnumerical_features = [f for f in features if f not in categoricals]\ntrain_df[numerical_features] = train_df[numerical_features].fillna(train_df[numerical_features].median())\ntest_df[numerical_features] = test_df[numerical_features].fillna(test_df[numerical_features].median())\n\ntrain_df[categoricals] = train_df[categoricals].fillna(train_df[categoricals].mode().iloc[0])\ntest_df[categoricals] = test_df[categoricals].fillna(test_df[categoricals].mode().iloc[0])","e541805d":"# data standartization\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ndf = pd.concat([train_df[numerical_features], test_df[numerical_features]], ignore_index=True)\nscaler.fit(df[numerical_features])\nx_test = test_df.copy()[features]\nx_test[numerical_features] = scaler.transform(x_test[numerical_features])","d1e822f3":"from catboost import CatBoostClassifier\nfrom imblearn.over_sampling import RandomOverSampler # this will duplicate the minor class rows necessary times to get balanced\nfrom imblearn.under_sampling import RandomUnderSampler # this will sample from major class rows the number of minor class rows\nfrom sklearn.metrics import accuracy_score, roc_auc_score, log_loss, mean_squared_error, mean_absolute_error, f1_score","35fb7f9b":"rus = RandomUnderSampler() \n_, _ = rus.fit_resample(train_df[features], train_df.Presence)\ntrain_idx = rus.sample_indices_\n\n\nx_train = train_df.loc[train_idx, features]\ny_train = train_df.loc[train_idx, target]\nx_train[numerical_features] = scaler.transform(x_train[numerical_features])\n\nmodel = CatBoostClassifier(iterations=50, verbose=False)\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_train)\nscore = roc_auc_score(y_train, y_pred)\nprint('score: {}'.format(score))","c0cb1219":"import shap\nshap.initjs()\n\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(x_train)","ef887dd5":"s_i = 56\nshap.force_plot(explainer.expected_value, shap_values[s_i,:], x_train.iloc[s_i,:])","22321d3d":"s_i = 1\nshap.force_plot(explainer.expected_value, shap_values[s_i,:], x_train.iloc[s_i,:])","3e7e4148":"shap.dependence_plot(\"Temperature_today\", shap_values, x_train)","f078c9ed":"shap.summary_plot(shap_values, x_train)","b8d6a163":"shap.summary_plot(shap_values, x_train, plot_type=\"bar\")","080debfe":"y_test = model.predict(x_test)\n\nresult = pd.read_csv(\"\/kaggle\/input\/killer-shrimp-invasion\/temperature_submission.csv\")\nresult.Presence = y_test\nresult.to_csv(\"submission.csv\", index=False)","913e015a":"Here are two examples that shows features statistical-effects that influence on deceision:","894e3a6a":"We can suppose from the picture above that: \n*     1) Higher values of substrate and exposure are for negative answer - no killers.\n*     2) Higher values of depth and temperature in common  for positive answer.","a58e8e98":"To figure out how features influence on the tree deceision we can use Shaplet effects that can be easily computed and visualized by cute and awesome lib \"shap\" - it can work with catboost, xgboost and lightgbm out of the box.","65a2c994":"# Model interpretation","bf77bd13":"https:\/\/github.com\/slundberg\/shap","80b749db":"Here we can view Shapley effect of Temperature for each sample. Third dimension is shown by color.","6fe17a55":"So, classes in train data are unbalanced.\n\nBy the way, I tried to submit only zeros - public board responced with 50%. \n\nMay be test is really balanced, or only public test.","09859f3d":"Here we can see how features effect is distributed for each sample with its value:"}}