{"cell_type":{"477c4fa0":"code","6514d5f3":"code","45ea97bd":"code","94646b0d":"code","ff75da0a":"code","37177979":"code","00df599f":"code","a25b0ba0":"code","c95c52dc":"code","d35b0bf0":"code","bff6da70":"code","e0bc0328":"code","9903400d":"code","945d4468":"code","6b5806a0":"code","6a151c53":"code","ad08e90c":"markdown","f2d38a66":"markdown","b43c4912":"markdown","338984f3":"markdown","30eacdb7":"markdown","f30d9710":"markdown","4979d82e":"markdown","7b3dc3a1":"markdown","80976909":"markdown"},"source":{"477c4fa0":"# Imprting the needed utilities for NN\nimport tensorflow as tf\nfrom tensorflow.keras import Model, Input, models\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\nfrom tensorflow.keras.layers import BatchNormalization, MaxPooling2D, ReLU, Dropout, Flatten, Dense, InputLayer, Concatenate, Add, SeparableConv2D, Layer\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import CategoricalCrossentropy, BinaryCrossentropy\nfrom tensorflow.keras.regularizers import l1, l2\nfrom tensorflow.keras.initializers import TruncatedNormal, he_uniform, he_normal\n\n# Data Preprocessing utilities\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import KFold\n\n# Importing the plotting utilities\nimport pylab as plot\nfrom matplotlib import pyplot as plt\n\n# Setting the parameters for plotting\nparams = {'legend.fontsize': 20,\n          'legend.handlelength': 5,\n          'xtick.labelsize' : 30,\n          'ytick.labelsize' : 30,\n         'axes.titlesize' : 50}\n\nplot.rcParams.update(params)\n\n# Base Libraries\nimport numpy as np\nimport pandas as pd ","6514d5f3":"# Importing train and test data\ntrain_features, test_features = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv'), pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\n# Sample submission file\nsubmission = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\n\n# Target data\ntrain_targets_nonscored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_nonscored.csv')\ntrain_targets = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\n\n# Getting the indices with MoA reactions\n# indices = train_features[train_features['cp_type'] == 'trt_cp'].index.to_list()","45ea97bd":"# Dropping the control rows since they have no MoA\n# train_targets = train_targets.iloc[indices, :].reset_index(drop=True)\n# train_targets_nonscored = train_targets_nonscored.iloc[indices, :].reset_index(drop=True)\n# train_features = train_features.iloc[indices, :].reset_index(drop=True)\n\n# Encoding categorical variables\ncp_dose = {'D1': 1, 'D2': -1} # Dictionary\ncp_ctrl = {'trt_cp': 1, 'ctl_vehicle': -1}\n\n# Mapping dictionaries to columns\ntrain_features['cp_dose'], test_features['cp_dose'] = train_features['cp_dose'].map(cp_dose), test_features['cp_dose'].map(cp_dose)\ntrain_features['cp_type'], test_features['cp_type'] = train_features['cp_type'].map(cp_ctrl), test_features['cp_type'].map(cp_ctrl)\n\n# Deleting unecessary columns of data\ndel train_features['sig_id']\n# del train_features['cp_type']\ndel test_features['sig_id']\n# del test_features['cp_type']","94646b0d":"# Min_max scaler\ntrain_len = train_features.shape[0]\ncomb = pd.concat([train_features, test_features])\n# Breakdown of the data\ngenes_comb = comb.iloc[:,2:774]\ncells_comb = comb.iloc[:,774:]\n# PCA Versions\npca_genes = PCA(n_components=20).fit_transform(genes_comb)\npca_cells = PCA(n_components=10).fit_transform(cells_comb)\npca_all = PCA(n_components=100).fit_transform(comb)\n\ncomb_scaled = MinMaxScaler().fit_transform(comb)\n\nscaled_train_features = comb_scaled[0:train_len]\nscaled_test_features = comb_scaled[train_len:]","ff75da0a":"# Converting the data set to a float64 dtype\n# Training\nX = train_features.astype('float64').copy()\nXs = pd.DataFrame(scaled_train_features.astype('float64')).copy()\n\npca_all_train = pca_all[:train_len]\npca_cells_train = pca_cells[:train_len]\npca_genes_train = pca_genes[:train_len]\n\n\n# Testing\nX_test = test_features.astype('float64').copy()\nXs_test = pd.DataFrame(scaled_test_features.astype('float64')).copy()\n\npca_all_test = pca_all[train_len:]\npca_cells_test = pca_cells[train_len:]\npca_genes_test = pca_genes[train_len:]\n\n\n# Targets\ny = train_targets.iloc[:,1:].astype('float64').copy()\ny_non = train_targets_nonscored.iloc[:,1:].astype('float64').copy()  # The non_scored drugs\n\n# Concatenating the non-scored data to the scored one\ny_all = pd.concat([train_targets, y_non], axis=1).iloc[:,1:].astype('float64').copy()","37177979":"# Training \nCells = X.iloc[:,773:].copy()\nCells_scaled = Xs.iloc[:,773:].copy() \nGenes = X.iloc[:,3:773].copy()\nGenes_scaled = Xs.iloc[:,3:773].copy()\n# Cells_image = Cells.values.reshape(Cells.shape[0], 10, 10, 1)\n# Cells_image_scaled = Cells_scaled.values.reshape(Cells.shape[0], 10, 10, 1)\n\n# Testing\nCells_test = X_test.iloc[:,773: ].copy()\nCells_test_scaled = Xs_test.iloc[:,773: ].copy()\nGenes_test = X_test.iloc[:,3:773].copy()\nGenes_test_scaled = Xs_test.iloc[:,3:773].copy()\n# Cells_image_test = Cells_test.values.reshape(Cells_test.shape[0], 10, 10, 1)\n# Cells_image_test_scaled = Cells_test_scaled.values.reshape(Cells_test.shape[0], 10, 10, 1)\n\n# Input pipelines\nall_input = Input(shape=(X.shape[1]), name='All')\ncells_input = Input(shape=(Cells.shape[1]), name='Cells')\ngenes_input = Input(shape=(Genes.shape[1]), name='Genes')\n# cell_image_input = Input(shape=(10,10,1), name='Cells_image')\npca_cell_input = Input(shape=pca_cells.shape[1], name='PCA_cells')\npca_gene_input = Input(shape=pca_genes.shape[1], name='PCA_genes')\npca_all_input = Input(shape=pca_all.shape[1], name='PCA_all')","00df599f":"def residual_module(x, add, std=1, Unit=32, seed=11):\n    \"\"\" A residual connection module \"\"\"\n    out = Dense(Unit, activation='relu', kernel_initializer=TruncatedNormal(0, std, seed))(x)\n    out = BatchNormalization()(out)\n    out = Dense(y_all.shape[1], activation='sigmoid', kernel_initializer=TruncatedNormal(0, std, seed))(out)\n    \n    return Add()([out, add])\n\n\ndef linear_NN(x, std, seed=1, unit=32):\n    \"\"\" Symmetric Linear NNs ran on cells, genes, all_data \"\"\"\n    c = Dense(unit, activation='selu', kernel_initializer=TruncatedNormal(0, std, seed))(x)\n    c = Dense(unit * 2, kernel_initializer=TruncatedNormal(0, std, seed))(c)\n    c = BatchNormalization()(c)\n    c = Dense(unit * 4, kernel_initializer=TruncatedNormal(0, std, seed))(c)\n    c = BatchNormalization()(c)\n    c = Dense(unit * 8, kernel_initializer=TruncatedNormal(0, std, seed))(c)\n    \n    return c","a25b0ba0":"all_output = linear_NN(all_input, std=1e-1, seed=3, unit=64)\np_all = BatchNormalization()(all_output)\np_all = Dense(y_all.shape[1], activation='sigmoid', kernel_initializer=TruncatedNormal(0, 3, 1))(p_all)\n\npca_cell_output = linear_NN(pca_cell_input, std=1e-1, seed=4, unit=64)\np_pca_cell = BatchNormalization()(pca_cell_output)\np_pca_cell = Dense(y_all.shape[1], activation='sigmoid', kernel_initializer=TruncatedNormal(0, 5, 123))(p_pca_cell)\n\npca_gene_output = linear_NN(pca_gene_input, std=1e-1, seed=5, unit=64)\np_pca_gene = BatchNormalization()(pca_gene_output)\np_pca_gene = Dense(y_all.shape[1], activation='sigmoid', kernel_initializer=TruncatedNormal(0, 4, 123))(p_all)\n\npca_all_output = linear_NN(pca_all_input, std=1e-1, seed=6, unit=64)\np_pca_all = BatchNormalization()(pca_all_output)\np_pca_all = Dense(y_all.shape[1], activation='sigmoid', kernel_initializer=TruncatedNormal(0, 3, 123))(p_pca_all)\n\n\ncomb = Concatenate()([all_output, pca_cell_output, pca_gene_output, pca_all_output])\ncomb = Dense(512, kernel_initializer=TruncatedNormal(0, 5, 13))(comb)\ncomb = BatchNormalization()(comb)\n\nout = residual_module(comb, p_all, std=5e-2, Unit=2048, seed=5)\n\nout = residual_module(out, p_pca_cell, std=5e-2, Unit=512, seed=7)\n    \nout = residual_module(out, p_pca_gene, std=5e-2, Unit=256, seed=9)\n\nout = residual_module(out, p_pca_all, std=5e-2, Unit=128, seed=11)\n\nout = Dense(y_all.shape[1], activation='sigmoid', kernel_initializer=TruncatedNormal(0, 2, 11))(out)\n\nout = Add()([out, p_all, p_pca_all, p_pca_gene, p_pca_cell])\nout = BatchNormalization()(out)\n\nout = Dense(y_all.shape[1], activation='sigmoid', kernel_initializer=TruncatedNormal(0, 2.5, 11))(out)\n\nmodel = Model(inputs=[all_input, pca_cell_input, pca_gene_input, pca_all_input], outputs=[out])","c95c52dc":"model.count_params() \/ 1e6","d35b0bf0":"plot_model(model, show_shapes=1, show_layer_names=0)","bff6da70":"def scheduler(epoch, lr): return max(1e-25, lr * 0.96 ** (epoch \/\/ 50))\n\nBATCH_SIZE = 128\nN_FOLDS = 5\nkf = KFold(n_splits=N_FOLDS, random_state=1, shuffle=True)\nhistory, index = {}, 0\nprediction = np.zeros((test_features.shape[0], y.shape[1]))\n\nfor train_indices, val_indices in kf.split(X, y):\n    \n    print(f'{index + 1}th fold, Validation Indices: ', val_indices[:5])\n    # Gene, y, and Cell data divided into Train and Validation splits\n    X_train, X_val = Xs.loc[train_indices], Xs.loc[val_indices]\n#     train_cells, val_cells = Cells_scaled.iloc[train_indices], Cells_scaled.iloc[val_indices]\n#     train_genes, val_genes = Genes_scaled.iloc[train_indices], Genes_scaled.iloc[val_indices]\n    y_train, y_val = y_all.iloc[train_indices], y_all.iloc[val_indices]\n    \n    pca_gene_train, pca_gene_val = pca_genes_train[train_indices], pca_genes_train[val_indices]\n    pca_cell_train, pca_cell_val = pca_cells_train[train_indices], pca_cells_train[val_indices]\n    pca_All_train, pca_all_val = pca_all_train[train_indices], pca_all_train[val_indices]\n    \n    # Instantiating the model\n    model = Model(\n        inputs=[all_input, pca_cell_input, pca_gene_input, pca_all_input], \n        outputs=[out])\n    model.compile(optimizer=Adam(0.004), loss=BinaryCrossentropy())\n    \n    # Fitting\n    history[index] = model.fit(\n        x=[X_train, pca_cell_train, pca_gene_train, pca_All_train], \n        y=y_train, epochs=600, batch_size=BATCH_SIZE, verbose=0, \n        validation_data=([X_val, pca_cell_val, pca_gene_val, pca_all_val], y_val),\n        callbacks=[ \n            ReduceLROnPlateau(factor=0.95, patience=20, verbose=True, monitor='loss', min_lr=1e-45, min_delta=1e-4),\n            EarlyStopping(monitor='loss', patience=200, restore_best_weights=True, min_delta=1e-3, verbose=True)\n        ]\n    )\n    \n    model_prediction = model.predict(\n        [Xs_test, pca_cells_test, pca_genes_test, pca_all_test], \n        batch_size=BATCH_SIZE, verbose=False)[:,:y.shape[1]]\n    \n    prediction += model_prediction \/ N_FOLDS\n    \n    index += 1\n    print('#----------------#----------------#----------------#----------------#')","e0bc0328":"num_cols = 3\nfig, axes = plt.subplots(len(history), num_cols, figsize=(40,60))\nfig.legend([\"blue\", \"orange\"], prop={\"size\":10})\n\nfor i in range(len(history)):\n    d = pd.DataFrame(history[i].history)\n    d['Epoch'] = range(0,d.shape[0])\n\n#     d.iloc[:,:].plot(x=\"Epoch\", y=[\"loss\",\"val_loss\"], ax=axes[i][0])\n    for j in range(num_cols):\n        d.iloc[d.shape[0]\/\/num_cols*j:d.shape[0]\/\/num_cols*(j+1),:].plot(\n            x=\"Epoch\", y=[\"loss\",\"val_loss\"], ax=axes[i][j], title=f'{i+1}th fold')","9903400d":"# prediction = model.predict([Cells_test_scaled, Genes_test_scaled, Xs_test, pca_cells_test, pca_genes_test, pca_all_test])[:,:y.shape[1]]\n# prediction[prediction > 0.5] = 1 \n# prediction[prediction < 1e-1] = 0\nsubmission.iloc[:, 1:] = prediction\nsubmission.to_csv('\/kaggle\/working\/submission.csv', index=False)","945d4468":"pd.read_csv('\/kaggle\/working\/submission.csv').describe().T['max'].values","6b5806a0":"pd.read_csv('\/kaggle\/working\/submission.csv').describe()","6a151c53":"# model.save('.\/NN01')","ad08e90c":"# Plotting and Visualization\n- Plotting the loss and val_loss in each fold\n- Trying to make assumptions about the state of the model and improve it\n- Difference between val_loss and loss, and the oscilations within the graph are things to look for","f2d38a66":"# Saving Model\n- To put into the <a href='https:\/\/www.kaggle.com\/damoonshahhosseini\/nnmoa'>dataset<\/a> and aggregate their results.\n- The naming convention used should follow the one discussed above","b43c4912":"## Input Pipelines\nDifferent Input structure will be fed to the Models:\n- all_data -> All the columns in a row\n- Cells(_test) -> Only the features with the name cell in them\n- Genes(-test) -> Only the features with the name gene in them \n- Cell_image(_test) -> cells data converted to 10x10 image so it can be treated as an image","338984f3":"# Importing and preprocessing\n- Excluding the roww with cp_type equal to ctrl_vehicle since control pertubations have no MoAs\n- Normalizing the cp_time column\n- Mapping cp_dose values to 1 and -1\n- Setting the type of all data to float64 in order to get them ready for Neural Network layers.\n- Run min_max scaler on the combination of X_train and X_test then split","30eacdb7":"## Prediction\n- Replacing the sample submission with my submission\n- Writting the submission into the directory","f30d9710":"# Summary\n- `cp_dose` is the only used categorical variable.\n- <i> <b>Control<\/b> patients have been not been used <\/i> since they don't have MoA.\n- Numerours Neural Networks have been used and aggregated to make predictions\n- Neural Network with different entries will run in parallel\n- Some of the structures used are Linear, Residual Connections, CNN, inception-style feature extraction and mix of all.\n- Also the high performing Neural Netowrks have been saved in <a href=\"https:\/\/www.kaggle.com\/damoonshahhosseini\/nnmoa\">NN-MoA<\/a> dataset for futher improvement","4979d82e":"# Models\n### Models will be named based on their input and structure.\n- all_data: have an AD in their name\n- cells: C, Genes: G, Cells_image: CI\n- L: Linear, M: mix, I: inception, R: residual\n- Pattern: [input_type]_[NN_type]_[ext]\n\n### `Notes`:\n1. The weights that are used in sigmoid layers should be relatively low (>=1e-3) for higher accuracy and better convergence.\n2. The goal is write to minimalist Neural Networks (less than 1e6 params) to focus on particular entries and techniques, then aggregate all of them in one network.\n3. Multi-input NN (uni-output):\n    - Make a prediction in sub-NNs then aggregate the results to get a final result\n    - Use inter-connected NNs\n    - Use structures to find patterns within the data.","7b3dc3a1":"Checking mean, std, and quantiles of each drug: \n- Max should be in order of e-1.\n- Min should be close to zero.\n- Std should be relatively small (<1e-2)","80976909":"# Scaling data\n- Using MinMaxScaler to scale the data between 0 to 1 relative to their columns\n- This will increase the performace of the Neural Network for convergence"}}