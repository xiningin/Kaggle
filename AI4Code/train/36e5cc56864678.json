{"cell_type":{"8142c4ab":"code","f9f178f6":"code","6f37317a":"code","e242535d":"code","58debd97":"code","be623d72":"code","c258c55f":"code","52c80d2b":"code","77358c76":"code","49e5b5eb":"code","23baed0f":"code","cf412a6f":"code","3882dfd6":"code","4c93e5c1":"code","2c5f705e":"code","98862fa5":"code","8c487086":"markdown","a42f6713":"markdown","d381f48e":"markdown","7782c3c7":"markdown","8715c390":"markdown","b0c14306":"markdown","d2dba7e3":"markdown","eae2ef80":"markdown","891c8d5b":"markdown","e0a7ad7b":"markdown","557017e1":"markdown"},"source":{"8142c4ab":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","f9f178f6":"heart_df = pd.read_csv(\"..\/input\/heart.csv\")\nheart_df.info()\nheart_df.head()\nheart_df.describe()","6f37317a":"fig, ax = plt.subplots(1,2,figsize=(20,10))\nsns.boxplot(x='sex', y='age', hue='target', data=heart_df, orient='v', ax=ax[0])\nsns.boxplot(x='target', y='thalach', data=heart_df, ax=ax[1])\n\n","e242535d":"sns.pairplot(heart_df,hue='target')","58debd97":"sns.distplot(heart_df['target'], kde=False)","be623d72":"plt.figure(figsize=(60,40))\nsns.heatmap(heart_df.corr(),annot=True)\n","c258c55f":"fig, ax = plt.subplots(len(heart_df.drop('target',axis=1).columns),figsize=(30,100))\nfor i,column in enumerate(heart_df.drop('target',axis=1).columns):\n    sns.countplot(x=column,data=heart_df,hue='target',ax=ax[i])\n    ax[i].set_title(column, size=18)","52c80d2b":"sns.heatmap(heart_df.isnull(), yticklabels=False, cmap=\"viridis\")","77358c76":"sns.countplot(x='target',data=heart_df)","49e5b5eb":"X = heart_df.drop('target',axis=1)\ny = heart_df['target']\n","23baed0f":"from sklearn.linear_model import LogisticRegression\n\ndef perform_logistic_regression(X_train, X_test, C_param,  y_train):\n    lr = LogisticRegression(C=C_param)\n    lr.fit(X_train, y_train)\n    return lr.predict(X_test)\n\ndef Normalize_dataframe(raw_df):\n    from sklearn.preprocessing import StandardScaler\n    return pd.DataFrame(StandardScaler().fit_transform(raw_df))","cf412a6f":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25, random_state=133)\nnorm_X_train, norm_X_test,y_train, y_test = train_test_split(Normalize_dataframe(X),y,test_size=0.25,random_state=133)\n\ny_predicted = perform_logistic_regression(X_train, X_test, 0.5, y_train)\ny_predicted_normalized = perform_logistic_regression(norm_X_train, norm_X_test, 0.5, y_train)\n","3882dfd6":"from sklearn.metrics import classification_report, confusion_matrix\nprint('Confusion matrix for unscaled X \\n')\nprint(confusion_matrix(y_predicted,y_test))\nprint('\\n')\n\nprint('Confusion matrix for scaled X \\n')\nprint(confusion_matrix(y_predicted_normalized,y_test))\nprint('\\n')\n","4c93e5c1":"list_of_C = [0.01, 0.1, 1, 10, 100]\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score\nfive_fold = KFold(n_splits=5)\ni = 0\ny_pred_c = []\naccuracies_with_C = []\nfor C in list_of_C:\n    accuracy_score_list = []\n    for fold_number,indices in enumerate(five_fold.split(norm_X_train, y_train)):\n        y_pred = (perform_logistic_regression(Normalize_dataframe(X).iloc[indices[0],:], Normalize_dataframe(X).iloc[indices[1],:], C, y[indices[0]]))\n        accuracy_sc = accuracy_score(y_pred,y[indices[1]])\n        accuracy_score_list.append(accuracy_sc)\n        print('Iteration: ', fold_number, ' Accuracy Score: ', accuracy_sc)\n    accuracies_with_C.append(np.mean(accuracy_score_list))\npd.DataFrame(accuracies_with_C, index=list_of_C, columns=['Accuracy'])\n\n    \n    ","2c5f705e":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\nmodels = {  'Logistic Regression': LogisticRegression(C=0.01),\n            'RandomForestClassifier':RandomForestClassifier(n_estimators=500), \n            'DecisionTreeClassifier': DecisionTreeClassifier(), \n            'SVM Classifier': SVC(), \n            'KNN Classifier': KNeighborsClassifier()}","98862fa5":"tenFold = KFold(n_splits=10)\n\ndef runModels(X,y):\n    model_accuracies = {}\n    for model in models:\n        accuracy_score_list = []\n        for fold_number,indices in enumerate(tenFold.split(norm_X_train, y_train)):\n            models[model].fit(X=X.iloc[indices[0],:],y=y[indices[0]])\n            y_pred = models[model].predict(X.iloc[indices[1],:])\n            accuracy_sc = accuracy_score(y_pred,y[indices[1]])\n            accuracy_score_list.append(accuracy_sc)\n        model_accuracies[model] = np.mean(accuracy_score_list)\n    return model_accuracies\nnormalized_accuracies = pd.DataFrame.from_dict(runModels(Normalize_dataframe(X),y), orient='index', columns=['Scaled inputs'])\nregular_accuracies = pd.DataFrame.from_dict(runModels(X,y), orient='index', columns=['Unscaled Inputs'])\npd.concat([normalized_accuracies,regular_accuracies],axis=1)","8c487086":"Next I want to tune the cost regularization parameter C in logistic regression. I will attempt to use K-Fold cross validation to find the best C value.","a42f6713":"I'm starting off by importing the main libraries for this data analysis.","d381f48e":"Let's start off by reading in the csv and taking a look at the features and their types.","7782c3c7":"Here it looks like it's pretty well balanced. \nNext, we can look at a correlation plot to see if there is any anamolous correlation between input variables. This could also be seen from the pairplot from earlier. ","8715c390":"Now I am breaking the data up into the predictors and the predicted. Notice that the predictor variable values are very different in terms of dimensions. If we use any models that rely on distance or L2 norm calculations to determine costs, we would need to standardize features. Standardization means that the dimensions of all the variables are similar. There are many ways to do this. ","b0c14306":"There doesn't appear to be any pairs of highly correlated variables so we can proceed further.","d2dba7e3":"In the way of exploratory data analysis, let us start off by lookng at some box plots of some variables. I have set a hue by the target column so we can find any trends from the box plots for patients who have heart disease and those who don't.","eae2ef80":"Another one of my favorite plots to look at is the pairplot. This is a pairwise scatterplot of every combination of input variables. We can also hue this by the target to see how separable the classes are.","891c8d5b":"In order to be confident in my assertion, I will run 10-fold cross-validation on each model. ","e0a7ad7b":"Let's compare this with some other popular classification models. I want to see how scaling the variables affects classification performance for each of these methods. So I will run each model on both the scaled and unscaled version of the training set and see how their performances change. ","557017e1":"Next we need to see if the data is balanced. The dataset is said to be balanced if both classes of the predicted variable have more or less equal representation. We can do this is many ways - the simplest is a histogram of the two classes"}}