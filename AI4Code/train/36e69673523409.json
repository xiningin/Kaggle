{"cell_type":{"21524627":"code","8a43fd0b":"code","3441e351":"code","bc3e4712":"code","44b7384d":"code","9f36495a":"code","75848496":"code","3d1823aa":"code","63b1ada3":"code","0caf061e":"code","d2f31f69":"code","1baebd3a":"code","d8099eb9":"code","7a433439":"code","e113651b":"code","a5359155":"code","d7ea8bd1":"code","7e97d059":"code","3f3ad26c":"code","a2ac08a7":"code","592562fb":"code","e1c4de50":"code","8c29e0da":"code","7de4c698":"code","4296a5df":"code","f96ad1ec":"code","2cbd5110":"code","bf070619":"code","3b79286e":"code","cf4ce488":"code","50acfe5b":"code","100175ac":"code","abef9377":"code","32414ab9":"code","403568ea":"code","4df356d7":"code","528865dc":"code","459268b0":"code","3ff0f501":"code","d6f3e0bd":"code","80bb7e36":"code","b1604da3":"code","c088109e":"code","3ac917d9":"code","ad58fdc7":"code","75483814":"markdown","28bce358":"markdown","1e17d714":"markdown","148d1aa2":"markdown","f4a7f756":"markdown","6abd54e0":"markdown","f700c5aa":"markdown","99b0344e":"markdown","d1334a5d":"markdown","bfbf147b":"markdown","719ac473":"markdown","ca584497":"markdown","8178d0d9":"markdown"},"source":{"21524627":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","8a43fd0b":"import warnings\nwarnings.filterwarnings('ignore')\n# read file\nvoice=pd.read_csv('..\/input\/voice.csv')\nvoice.head()","3441e351":"voice.info()","bc3e4712":"voice.describe()","44b7384d":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nvoice[\"label\"] = le.fit_transform(voice[\"label\"])\nle.classes_","9f36495a":"voice[:]=preprocessing.MinMaxScaler().fit_transform(voice)\nvoice.head()","75848496":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.subplots(4,5,figsize=(15,15))\nfor i in range(1,21):\n    plt.subplot(4,5,i)\n    plt.title(voice.columns[i-1])\n    sns.kdeplot(voice.loc[voice['label'] == 0, voice.columns[i-1]], color= 'green', label='F')\n    sns.kdeplot(voice.loc[voice['label'] == 1, voice.columns[i-1]], color= 'blue', label='M')","3d1823aa":"from sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn import neighbors\nfrom sklearn import naive_bayes\nfrom sklearn import tree\nfrom sklearn import ensemble\nfrom sklearn import svm\nfrom sklearn import neural_network\nimport xgboost","63b1ada3":"# Split the data\ntrain, test = train_test_split(voice, test_size=0.3)","0caf061e":"train.head()","d2f31f69":"x_train = train.iloc[:, :-1]\ny_train = train[\"label\"]\nx_test = test.iloc[:, :-1]\ny_test = test[\"label\"]","1baebd3a":"x_train3 = train[[\"meanfun\",\"IQR\",\"Q25\"]]\ny_train3 = train[\"label\"]\nx_test3 = test[[\"meanfun\",\"IQR\",\"Q25\"]]\ny_test3 = test[\"label\"]","d8099eb9":"def classify(model,x_train,y_train,x_test,y_test):\n    from sklearn.metrics import classification_report\n    target_names = ['female', 'male']\n    model.fit(x_train,y_train)\n    y_pred=model.predict(x_test)\n    print(classification_report(y_test, y_pred, target_names=target_names, digits=4))","7a433439":"def knn_error(k,x_train,y_train,x_test,y_test):\n    error_rate = []\n    K=range(1,k)\n    for i in K:\n        knn = neighbors.KNeighborsClassifier(n_neighbors = i)\n        knn.fit(x_train, y_train)\n        y_pred = knn.predict(x_test)\n        error_rate.append(np.mean(y_pred != y_test))\n    kloc = error_rate.index(min(error_rate))\n    print(\"Lowest error is %s occurs at k=%s.\" % (error_rate[kloc], K[kloc]))\n\n    plt.plot(K, error_rate, color='blue', linestyle='dashed', marker='o',\n             markerfacecolor='red', markersize=10)\n    plt.title('Error Rate vs. K Value')\n    plt.xlabel('K')\n    plt.ylabel('Error Rate')\n    plt.show()\n    return K[kloc]","e113651b":"k=knn_error(21,x_train,y_train,x_test,y_test)","a5359155":"model = neighbors.KNeighborsClassifier(n_neighbors = k)\nclassify(model,x_train,y_train,x_test,y_test)","d7ea8bd1":"k=knn_error(21,x_train3,y_train3,x_test3,y_test3)","7e97d059":"model = neighbors.KNeighborsClassifier(n_neighbors = k)\nclassify(model,x_train,y_train,x_test,y_test)","3f3ad26c":"model=naive_bayes.GaussianNB()\nclassify(model,x_train,y_train,x_test,y_test)","a2ac08a7":"model=naive_bayes.GaussianNB()\nclassify(model,x_train3,y_train3,x_test3,y_test3)","592562fb":"#Find the best parameter to prune the tree\ndef dt_error(n,x_train,y_train,x_test,y_test):\n    nodes = range(2, n)\n    error_rate = []\n    for k in nodes:\n        model = tree.DecisionTreeClassifier(max_leaf_nodes=k)\n        model.fit(x_train, y_train)\n        y_pred = model.predict(x_test)\n        error_rate.append(np.mean(y_pred != y_test))\n    kloc = error_rate.index(min(error_rate))\n    print(\"Lowest error is %s occurs at n=%s.\" % (error_rate[kloc], nodes[kloc]))\n    plt.plot(nodes, error_rate, color='blue', linestyle='dashed', marker='o',\n             markerfacecolor='red', markersize=10)\n    plt.xlabel('Tree Size')\n    plt.ylabel('Cross-Validated MSE')\n    plt.show()\n    return nodes[kloc]","e1c4de50":"n=dt_error(10,x_train,y_train,x_test,y_test)","8c29e0da":"#prune tree\npruned_tree = tree.DecisionTreeClassifier(criterion = 'gini', max_leaf_nodes = n)\nclassify(pruned_tree,x_train,y_train,x_test,y_test)","7de4c698":"n=dt_error(15,x_train3,y_train3,x_test3,y_test3)","4296a5df":"#prune tree\npruned_tree = tree.DecisionTreeClassifier(criterion = 'gini', max_leaf_nodes = n)\nclassify(pruned_tree,x_train3,y_train3,x_test3,y_test3)","f96ad1ec":"def rf_error(n,x_train,y_train,x_test,y_test):\n    error_rate = []\n    e=range(1,n,20)\n    for i in e:\n        model = ensemble.RandomForestClassifier(n_estimators = i)\n        model.fit(x_train, y_train)\n        y_pred = model.predict(x_test)\n        error_rate.append(np.mean(y_pred != y_test))\n    nloc = error_rate.index(min(error_rate))\n    print(\"Lowest error is %s occurs at n=%s.\" % (error_rate[nloc], e[nloc]))\n\n    plt.plot(e, error_rate, color='blue', linestyle='dashed', marker='o',\n             markerfacecolor='red', markersize=10)\n    plt.title('Error Rate vs. n Value')\n    plt.xlabel('n')\n    plt.ylabel('Error Rate')\n    plt.show()\n    return e[nloc]","2cbd5110":"e=rf_error(100,x_train,y_train,x_test,y_test)","bf070619":"model=ensemble.RandomForestClassifier(n_estimators = e)\nclassify(model,x_train,y_train,x_test,y_test)","3b79286e":"e=rf_error(100,x_train3,y_train3,x_test3,y_test3)","cf4ce488":"model=ensemble.RandomForestClassifier(n_estimators = e)\nclassify(model,x_train3,y_train3,x_test3,y_test3)","50acfe5b":"model = xgboost.XGBClassifier()\nclassify(model,x_train,y_train,x_test,y_test)","100175ac":"model = xgboost.XGBClassifier()\nclassify(model,x_train3,y_train3,x_test3,y_test3)","abef9377":"def svm_kernel(x_train,y_train,x_test,y_test):\n    rate=[]\n    kernel=['rbf','poly','linear']\n    for i in kernel:\n        model=svm.SVC(kernel=i).fit(x_train,y_train)\n        y_pred=model.predict(x_train)\n        print(i, ' in-sample accuracy in SVM: ', accuracy_score(y_train,y_pred))\n        y_pred=model.predict(x_test)\n        print(i, ' out-of-sample accuracy in SVM: ', accuracy_score(y_test,y_pred))\n        rate.append(accuracy_score(y_test,y_pred))\n    nloc = rate.index(max(rate))\n    print(\"Highest accuracy is %s occurs at %s kernel.\" % (rate[nloc], kernel[nloc]))\n    return kernel[nloc]","32414ab9":"def svm_error(k,C,x_train,y_train,x_test,y_test):\n    error_rate = []\n    C=range(1,C)\n    for i in C:\n        model=svm.SVC(kernel=k,C=i).fit(x_train,y_train)\n        model.fit(x_train, y_train)\n        y_pred = model.predict(x_test)\n        error_rate.append(np.mean(y_pred != y_test))\n    cloc = error_rate.index(min(error_rate))\n    print(\"Lowest error is %s occurs at C=%s.\" % (error_rate[cloc], C[cloc]))\n\n    plt.plot(C, error_rate, color='blue', linestyle='dashed', marker='o',\n             markerfacecolor='red', markersize=10)\n    plt.title('Error Rate vs. C Value')\n    plt.xlabel('C')\n    plt.ylabel('Error Rate')\n    plt.show()\n    return C[cloc]","403568ea":"k=svm_kernel(x_train,y_train,x_test,y_test)","4df356d7":"c=svm_error(k,10,x_train,y_train,x_test,y_test)","528865dc":"model=svm.SVC(kernel=k,C=c)\nclassify(model,x_train,y_train,x_test,y_test)","459268b0":"k=svm_kernel(x_train3,y_train3,x_test3,y_test3)","3ff0f501":"c=svm_error(k,10,x_train3,y_train3,x_test3,y_test3)","d6f3e0bd":"model=svm.SVC(kernel=k,C=c)\nclassify(model,x_train3,y_train3,x_test3,y_test3)","80bb7e36":"def nn_error(n,x_train,y_train,x_test,y_test):\n    error_rate = []\n    hidden_layer=range(1,n)\n    for i in hidden_layer:\n        model = neural_network.MLPClassifier(solver='adam', alpha=1e-5,\n                                       hidden_layer_sizes=i,\n                                       activation='logistic',random_state=17,\n                                       max_iter=2000)\n        model.fit(x_train, y_train)\n        y_pred = model.predict(x_test)\n        error_rate.append(np.mean(y_pred != y_test))\n    kloc = error_rate.index(min(error_rate))\n    print(\"Lowest error is %s occurs at C=%s.\" % (error_rate[kloc], hidden_layer[kloc]))\n\n    plt.plot(hidden_layer, error_rate, color='blue', linestyle='dashed', marker='o',\n             markerfacecolor='red', markersize=10)\n    plt.title('Error Rate vs. Hidden Layer Size')\n    plt.xlabel('Size')\n    plt.ylabel('Error Rate')\n    plt.show()\n    return hidden_layer[kloc]","b1604da3":"h=nn_error(20,x_train,y_train,x_test,y_test)","c088109e":"model = neural_network.MLPClassifier(solver='adam', alpha=1e-5,\n                                       hidden_layer_sizes=h,\n                                       activation='logistic',random_state=17,\n                                       max_iter=2000)\nclassify(model,x_train,y_train,x_test,y_test)","3ac917d9":"h=nn_error(20,x_train3,y_train3,x_test3,y_test3)","ad58fdc7":"model = neural_network.MLPClassifier(solver='adam', alpha=1e-5,\n                                       hidden_layer_sizes=h,\n                                       activation='logistic',random_state=17,\n                                       max_iter=2000)\nclassify(model,x_train3,y_train3,x_test3,y_test3)","75483814":"K-Nearest Neighbors  \nNaive Bayes  \nDecision Tree  \nRandom Forest  \nXgBoost  \nSupport Vector Machine  \nNeural Network","28bce358":"## Random Forest\nUsing ensemble.RandomForestClassifier() to build the model.","1e17d714":"## Naive Bayes\nUsing naive_bayes.GaussianNB() to build the model.","148d1aa2":"## K-Nearest Neighbors\nUsing neighbors.KNeighborsClassifier() to build the model.","f4a7f756":"Using K-Nearest Neighbors, Naive Bayes, Decision Tree, Random Forest, XgBoost, Support Vector Machine, Neural Network to build models","6abd54e0":"## XgBoost\nUsing xgboost.XGBClassifier() to build the model.","f700c5aa":"## Decision Tree\nUsing tree.DecisionTreeClassifier() to build the model.","99b0344e":"We can see that the highest accurracy is 98.74% which is made by XgBoost. XgBoost is a powerful algorithm, and very popular in Data Science competition. Next time I will try to oppotimize the parameters of XgBoost.","d1334a5d":"## Support Vector Machine\nUsing svm.SVC() to build the model.","bfbf147b":"## Neural Network\nUsing neural_network.MLPClassifier to build the model.","719ac473":"Preprocessing: label encoder and normalization","ca584497":"At first glance, most significant features are Q25, IQR and meanfun. We will build models by using the 20 features and the 3 distinct features.","8178d0d9":"Visualization"}}