{"cell_type":{"c8d0cf7d":"code","554736f6":"code","8ea92c78":"code","15aaf573":"code","368a1298":"code","f775f4f1":"code","a375f204":"code","b1ff9882":"code","61d2bab7":"code","96e15db4":"code","43d86343":"code","04bff33a":"code","04660831":"code","9385ed16":"code","98bf51c7":"code","829f860a":"code","ac3b41b3":"code","07533fb8":"code","380abc9b":"code","27fd3e24":"code","68143660":"code","0d2f80b3":"code","0db063ba":"code","9adb6691":"code","4ba9ef64":"code","862dbdc0":"code","ce7be9e8":"code","3e658b50":"code","240246c4":"code","2b957a80":"code","dc264626":"code","ae5e2bce":"code","e08f5a54":"code","161214fd":"markdown","678d1813":"markdown","33877db9":"markdown","4c7a98ed":"markdown","52f087bd":"markdown","c86e0896":"markdown","9f704d3e":"markdown","75e4e013":"markdown","98498a9c":"markdown","3a63ead6":"markdown","07e3fc6d":"markdown","cd47a2f0":"markdown","ba0a1af0":"markdown","1fcd8e63":"markdown","07f6842b":"markdown","7910585b":"markdown","5fd9b942":"markdown","cd409129":"markdown","8e31c56c":"markdown","fb71f2a4":"markdown","a96af89e":"markdown","a09aa6f7":"markdown","c66504a9":"markdown","e4f246f9":"markdown","1fb20802":"markdown","274a0f43":"markdown"},"source":{"c8d0cf7d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import svm\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\n\n\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","554736f6":"train = pd.read_csv(\"..\/input\/homework-2-jacob-betts\/train.csv\")\ntest = pd.read_csv(\"..\/input\/homework-2-jacob-betts\/eval.csv\")\nsample_submission = pd.read_csv(\"..\/input\/homework-2-jacob-betts\/sample_submission.csv\")\nprint(\"Loaded Successfully\")","8ea92c78":"train.head()","15aaf573":"test.head()","368a1298":"sample_submission.head()","f775f4f1":"train.describe()","a375f204":"train.info()","b1ff9882":"test.info()","61d2bab7":"train.isnull().sum()","96e15db4":"test.isnull().sum()","43d86343":"train.hist(bins = 35, figsize=(20,15))","04bff33a":"train[\"esrb_rating\"].value_counts()","04660831":"rating_map = {'E' : 0,\n           'ET' : 1,\n           'T' : 2,\n           'M' : 3}\n\ntrain['esrb'] = train['esrb_rating'].map(rating_map)\n#test['esrb'] = test['esrb_rating'].map(rating_map)\ntrain[\"esrb\"].value_counts()","9385ed16":"corr_matrix = train.corr()\n\ncorr_matrix[\"esrb\"].sort_values(ascending=False)","98bf51c7":"from pandas.plotting import scatter_matrix\n\nattributes = [\"esrb\", \"strong_janguage\", \"blood_and_gore\", \"blood\", \"strong_sexual_content\", \"sexual_themes\",]\n\nscatter_matrix(train[attributes], figsize=(10,10))","829f860a":"plt.figure(figsize=(30,30))\n\ncor = train.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)","ac3b41b3":"train[\"violence\"].value_counts()","07533fb8":"train.drop([\"title\"], \n            axis=1, inplace=True)\n\ntest.drop([], \n            axis=1, inplace=True)\n\ntrain.info()","380abc9b":"label = LabelEncoder()\nX = train.drop([\"esrb_rating\", \"esrb\"], axis=1)\ny = train[\"esrb_rating\"]\ny = label.fit_transform(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, shuffle=True)\n\n","27fd3e24":"lgr = LogisticRegression()\nlgr_param = [\n    {\n        'C': [1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 20, 500, 600, 400, 300],\n        'solver' : ['liblinear'],\n        'penalty' : ['l1'],\n        'fit_intercept' : [False],\n        'class_weight' : [None]\n        \n    }\n]\n\ngrid_lgr = GridSearchCV(estimator=lgr, param_grid=lgr_param)\ngrid_lgr.fit(X_train, y_train)\nlgr_predict = grid_lgr.predict(X_test)\nprint(grid_lgr.best_score_)\nprint(grid_lgr.best_params_)\nprint(lgr_predict)","68143660":"lgr.get_params().keys()","0d2f80b3":"svm = svm.SVC()\nsvm_param = [\n    {\n        'C':[.1, 1, 10,20, 30, 40, 50, 100, 1000, 1200, 1300],\n        'gamma':[1, .1, .01, .001],\n        'kernel' :['rbf']\n    }\n]\n\ngrid_svm = GridSearchCV(estimator=svm, param_grid=svm_param)\ngrid_svm.fit(X_train, y_train)\nsvm_predict = grid_svm.predict(X_test)\nprint(grid_svm.best_score_)\nprint(grid_svm.best_params_)","0db063ba":"dt = DecisionTreeClassifier()\ndt_param = [\n    {\n        'criterion': ['gini'], \n        'max_depth': [2, 3, 4, 20, 24, 23, 25, 26, 27, 40, None],\n        'max_features' : [3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 30, None]\n    }\n]\n\ngrid_dt = GridSearchCV(estimator=dt, param_grid=dt_param)\ngrid_dt.fit(X_train, y_train)\ndt_predict = grid_dt.predict(X_test)\nprint(grid_dt.best_score_)\nprint(grid_dt.best_params_)","9adb6691":"dt.get_params().keys()","4ba9ef64":"rfc = RandomForestClassifier()\nrfc_param = [\n    {\n        \"n_estimators\":[500], \n        \"max_features\": [3, \"auto\"], \n        \"max_depth\" : [None],\n        'min_samples_leaf' : [2,],\n        'min_samples_split' : [6,]\n    }\n]\ngrid_rfc = GridSearchCV(rfc, rfc_param)\ngrid_rfc.fit(X_train, y_train)\nrfc_predict = grid_rfc.predict(X_test)\nprint(grid_rfc.best_score_)\nprint(grid_rfc.best_params_)","862dbdc0":"kn = KNeighborsClassifier()\nkn_param = [\n    {\n        'n_neighbors': [1, 2, 3, 4, 5, 7, 9, 10, 12, 15, 18, 20, 25, 30, 40, 50, 60, 70, 80, 100, 150, 200], \n        'weights': ['uniform', 'distance'],\n        'metric' : ['euclidean', 'manhattan']\n    }\n]\n\ngrid_kn = GridSearchCV(estimator=kn, param_grid=kn_param, cv=5)\ngrid_kn.fit(X_train, y_train)\nkn_predict = grid_kn.predict(X_test)\nprint(grid_kn.best_score_)\nprint(grid_kn.best_params_)\n","ce7be9e8":"kn.get_params().keys()","3e658b50":"# import xgboost as xgb\n\n\nxgb_param = [\n    {\n    'n_estimators' : [200],\n    'gamma':[1, .1, 0, .01, .001],\n    'max_depth' : [2, 3, 5, 7, 20, 22]\n}\n]\n\nxgb = xgb.XGBClassifier(objective='multi:softprob', eval_metric='mlogloss', use_label_encoder=False)\ngrid_xgb = GridSearchCV(estimator=xgb, param_grid=xgb_param, cv=5)\ngrid_xgb.fit(X_train, y_train)\nxgb_predict = grid_xgb.predict(X_test)\nprint(grid_xgb.best_score_)\nprint(grid_xgb.best_params_)\n","240246c4":"lgrR = []\nsvmR = []\ndtR = []\nrfcR = []\nknR = []\nxgbR = []\n\nfor i in range(20):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, shuffle=True)\n    \n    #lgr\n    grid_lgr.fit(X_train, y_train)\n    lgr_predict2 = grid_lgr.predict(X_test)\n    lgrR.append(accuracy_score(y_test, lgr_predict2))\n    \n    #svm\n    grid_svm.fit(X_train, y_train)\n    svm_predict2 = grid_svm.predict(X_test)\n    svmR.append(accuracy_score(y_test, svm_predict2))\n    \n    #dt\n    grid_dt.fit(X_train, y_train)\n    dt_predict2 = grid_dt.predict(X_test)\n    dtR.append(accuracy_score(y_test, dt_predict2))\n    \n    #rfc\n    grid_rfc.fit(X_train, y_train)\n    rfc_predict2 = grid_rfc.predict(X_test)\n    rfcR.append(accuracy_score(y_test, rfc_predict2))\n    \n    #kn\n    grid_kn.fit(X_train, y_train)\n    kn_predict2 = grid_kn.predict(X_test)\n    knR.append(accuracy_score(y_test, kn_predict2))\n    \n    #xgb\n    grid_xgb.fit(X_train, y_train)\n    xgb_predict2 = grid_xgb.predict(X_test)\n    xgbR.append(accuracy_score(y_test, xgb_predict2))\n    \n","2b957a80":"print(lgrR)\nprint(svmR)\nprint(dtR)\nprint(rfcR)\nprint(knR)\nprint(xgbR)","dc264626":"modelsR = pd.DataFrame({\"LGR\":lgrR, \"SVM\":svmR, \"DT\":dtR, \"RFC\":rfcR, \"KN\":knR, \"XGB\":xgbR})\nmodelsR.describe()","ae5e2bce":"lgr_submit = GridSearchCV(lgr, lgr_param)\nlgr_submit.fit(X_train, y_train)\nlgr_predict2 = lgr_submit.predict(test)\noutput = pd.DataFrame({\"id\" : test[\"id\"], \"esrb_rating\" : lgr_predict2})\n\nrating_map2 = {0:'E',\n            1:'ET',\n            2:'M',\n            3:'T'}\n\noutput['esrb_rating'] = output['esrb_rating'].map(rating_map2)\noutput.to_csv('submission.csv', index = False)\nprint(\"It worked! Good luck on your submission.\")\noutput.head()","e08f5a54":"print(output.to_markdown())","161214fd":"# Decision Tree","678d1813":"# Checking for Outliers","33877db9":"It seems as though there may be a correlation between the \"blood\", the \"blood and gore\", and the strong_language feature, which is mispelled as \"strong_janguage\" in our dataset for some reason, and an increased ESRB rating. Nothing far above .5 is seen however. ","4c7a98ed":"# K-Nearest Neighbors","52f087bd":"# Choosing Model","c86e0896":"This will create multiple instances of our models, and record the accuracy for each run. We'll then put all of these scores into a dataset so we can easily compare each models performance stats. ","9f704d3e":"# Data Analysis part 2","75e4e013":"# Data Analysis","98498a9c":"# Support Vector Machine","3a63ead6":"Neither our training or test sets contain any null values, which will save us some hassle. ","07e3fc6d":"# Checking for Empty\/Null Values","cd47a2f0":"Dropping the title feature, since it is irrelevant and not present in the test set. ","ba0a1af0":"Looking at the table, we can see that LGR has the best chance of giving us a higher score. It has a lower variance between its lowest and highest recorded values, and a higher max and min score than the other options. ","1fcd8e63":"# Random Forest Search","07f6842b":"# Generating Submission","7910585b":"# Checking for Correlation","5fd9b942":"Here we convert our numerical predictions back into categorical data before saving our prediction dataset as our submission file. ","cd409129":"After initially examining the data, it would appear that we have a dataset of with 1421 entries, and 35 features for our training set, and a dataset with 474 entries and 33 features for our test set. Our test set does not have \"titles\" or \"esrb_ratings\". Looking at the features for both datasets, we can see that many of them are very similar. We may be able to combine these similar features. For instance, we could combine all of the different categories of violence, into the violence feature. This feature may indeed already be the combination of the other violence features. Further examination is needed.\n\nWe can also see that most of our data is numerical, while the rating we are trying to predict is categorical. We will have to take care of this later.","8e31c56c":"# Boost","fb71f2a4":"This is where we take care of that categorical feature\/target vector.","a96af89e":"# Train Test Split","a09aa6f7":"Our initial theory that the violence feature may be an aggregate or total count of all violence themed entries is incorrect. There are only 88 positive entries in the violence trait.","c66504a9":"# Logistic Regression","e4f246f9":"# Building our Models","1fb20802":"# Importing the Data","274a0f43":"There don't appear to be any outliers. All values range from 0 to 1 except for the id, and the rating. "}}