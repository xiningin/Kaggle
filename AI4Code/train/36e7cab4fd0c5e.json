{"cell_type":{"a787b11a":"code","1afcdf0c":"code","8a710a43":"code","dea5fe4d":"code","45b69406":"code","472d4ab6":"code","5bf30cec":"code","e9951cf8":"code","7719ad32":"code","bb98b439":"code","371055ee":"code","8c2794c1":"code","2813feb0":"code","75a30a49":"code","ea415e23":"code","ce6afcc2":"markdown","653d88f9":"markdown","0b98beab":"markdown","5b86ff6d":"markdown","0e46de69":"markdown","b73e7d0f":"markdown"},"source":{"a787b11a":"%%capture\n\nimport torch\n#import librosa\nimport torchaudio\nimport torchaudio.functional as F\nimport torchaudio.transforms as T\n\n\nimport os, glob\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","1afcdf0c":"class CONFIG(object):\n    \"\"\"docstring for CONFIG\"\"\"\n    def __init__(self):\n        self.base= '..\/input\/audio-cats-and-dogs\/cats_dogs\/train\/'\n        self.cata= ['dog', 'cat']\n    \nconfig= CONFIG()","8a710a43":"#Taking First 5 samples\n# Path for Audio files\ndog_paths= [config.base+'dog\/'+path for path in os.listdir(config.base+'dog')[:5]]\ncat_paths= [config.base+'cat\/'+path for path in os.listdir(config.base+'cat')[:5]]","dea5fe4d":"#@title Prepare data and utility functions. {display-mode: \"form\"}\n#@markdown\n#@markdown You do not need to look into this cell.\n#@markdown Just execute once and you are good to go.\n#@markdown\n#@markdown In this tutorial, we will use a speech data from [VOiCES dataset](https:\/\/iqtlabs.github.io\/voices\/), which is licensed under Creative Commos BY 4.0.\n\n#-------------------------------------------------------------------------------\n# Preparation of data and helper functions.\n#-------------------------------------------------------------------------------\nimport io\nimport os\nimport math\nimport tarfile\nimport multiprocessing\n\nimport scipy\nimport librosa\nimport boto3\nfrom botocore import UNSIGNED\nfrom botocore.config import Config\nimport requests\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom IPython.display import Audio, display\n\n[width, height] = matplotlib.rcParams['figure.figsize']\nif width < 10:\n  matplotlib.rcParams['figure.figsize'] = [width * 2.5, height]\n\n_SAMPLE_DIR = \"_sample_data\"\nSAMPLE_WAV_URL = \"https:\/\/pytorch-tutorial-assets.s3.amazonaws.com\/steam-train-whistle-daniel_simon.wav\"\nSAMPLE_WAV_PATH = os.path.join(_SAMPLE_DIR, \"steam.wav\")\n\nSAMPLE_WAV_SPEECH_URL = \"https:\/\/pytorch-tutorial-assets.s3.amazonaws.com\/VOiCES_devkit\/source-16k\/train\/sp0307\/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\nSAMPLE_WAV_SPEECH_PATH = os.path.join(_SAMPLE_DIR, \"speech.wav\")\n\nSAMPLE_RIR_URL = \"https:\/\/pytorch-tutorial-assets.s3.amazonaws.com\/VOiCES_devkit\/distant-16k\/room-response\/rm1\/impulse\/Lab41-SRI-VOiCES-rm1-impulse-mc01-stu-clo.wav\"\nSAMPLE_RIR_PATH = os.path.join(_SAMPLE_DIR, \"rir.wav\")\n\nSAMPLE_NOISE_URL = \"https:\/\/pytorch-tutorial-assets.s3.amazonaws.com\/VOiCES_devkit\/distant-16k\/distractors\/rm1\/babb\/Lab41-SRI-VOiCES-rm1-babb-mc01-stu-clo.wav\"\nSAMPLE_NOISE_PATH = os.path.join(_SAMPLE_DIR, \"bg.wav\")\n\nSAMPLE_MP3_URL = \"https:\/\/pytorch-tutorial-assets.s3.amazonaws.com\/steam-train-whistle-daniel_simon.mp3\"\nSAMPLE_MP3_PATH = os.path.join(_SAMPLE_DIR, \"steam.mp3\")\n\nSAMPLE_GSM_URL = \"https:\/\/pytorch-tutorial-assets.s3.amazonaws.com\/steam-train-whistle-daniel_simon.gsm\"\nSAMPLE_GSM_PATH = os.path.join(_SAMPLE_DIR, \"steam.gsm\")\n\nSAMPLE_TAR_URL = \"https:\/\/pytorch-tutorial-assets.s3.amazonaws.com\/VOiCES_devkit.tar.gz\"\nSAMPLE_TAR_PATH = os.path.join(_SAMPLE_DIR, \"sample.tar.gz\")\nSAMPLE_TAR_ITEM = \"VOiCES_devkit\/source-16k\/train\/sp0307\/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n\nS3_BUCKET = \"pytorch-tutorial-assets\"\nS3_KEY = \"VOiCES_devkit\/source-16k\/train\/sp0307\/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n\nYESNO_DATASET_PATH = os.path.join(_SAMPLE_DIR, \"yes_no\")\nos.makedirs(YESNO_DATASET_PATH, exist_ok=True)\nos.makedirs(_SAMPLE_DIR, exist_ok=True)\n\ndef _fetch_data():\n  uri = [\n    (SAMPLE_WAV_URL, SAMPLE_WAV_PATH),\n    (SAMPLE_WAV_SPEECH_URL, SAMPLE_WAV_SPEECH_PATH),\n    (SAMPLE_RIR_URL, SAMPLE_RIR_PATH),\n    (SAMPLE_NOISE_URL, SAMPLE_NOISE_PATH),\n    (SAMPLE_MP3_URL, SAMPLE_MP3_PATH),\n    (SAMPLE_GSM_URL, SAMPLE_GSM_PATH),\n    (SAMPLE_TAR_URL, SAMPLE_TAR_PATH),\n  ]\n  for url, path in uri:\n    with open(path, 'wb') as file_:\n      file_.write(requests.get(url).content)\n\n_fetch_data()\n\ndef _download_yesno():\n  if os.path.exists(os.path.join(YESNO_DATASET_PATH, \"waves_yesno.tar.gz\")):\n    return\n  torchaudio.datasets.YESNO(root=YESNO_DATASET_PATH, download=True)\n\nYESNO_DOWNLOAD_PROCESS = multiprocessing.Process(target=_download_yesno)\nYESNO_DOWNLOAD_PROCESS.start()\n\ndef _get_sample(path, resample=None):\n  effects = [\n    [\"remix\", \"1\"]\n  ]\n  if resample:\n    effects.append([\"rate\", f'{resample}'])\n  return torchaudio.sox_effects.apply_effects_file(path, effects=effects)\n\ndef get_speech_sample(*, resample=None):\n  return _get_sample(SAMPLE_WAV_SPEECH_PATH, resample=resample)\n\ndef get_sample(*, resample=None):\n  return _get_sample(SAMPLE_WAV_PATH, resample=resample)\n\ndef get_rir_sample(*, resample=None, processed=False):\n  rir_raw, sample_rate = _get_sample(SAMPLE_RIR_PATH, resample=resample)\n  if not processed:\n    return rir_raw, sample_rate\n  rir = rir_raw[:, int(sample_rate*1.01):int(sample_rate*1.3)]\n  rir = rir \/ torch.norm(rir, p=2)\n  rir = torch.flip(rir, [1])\n  return rir, sample_rate\n\ndef get_noise_sample(*, resample=None):\n  return _get_sample(SAMPLE_NOISE_PATH, resample=resample)\n\ndef print_metadata(metadata, src=None):\n  if src:\n    print(\"-\" * 10)\n    print(\"Source:\", src)\n    print(\"-\" * 10)\n  print(\" - sample_rate:\", metadata.sample_rate)\n  print(\" - num_channels:\", metadata.num_channels)\n  print(\" - num_frames:\", metadata.num_frames)\n  print(\" - bits_per_sample:\", metadata.bits_per_sample)\n  print(\" - encoding:\", metadata.encoding)\n  print()\n\ndef print_stats(waveform, sample_rate=None, src=None):\n  if src:\n    print(\"-\" * 10)\n    print(\"Source:\", src)\n    print(\"-\" * 10)\n  if sample_rate:\n    print(\"Sample Rate:\", sample_rate)\n  print(\"Shape:\", tuple(waveform.shape))\n  print(\"Dtype:\", waveform.dtype)\n  print(f\" - Max:     {waveform.max().item():6.3f}\")\n  print(f\" - Min:     {waveform.min().item():6.3f}\")\n  print(f\" - Mean:    {waveform.mean().item():6.3f}\")\n  print(f\" - Std Dev: {waveform.std().item():6.3f}\")\n  print()\n  print(waveform)\n  print()\n\ndef plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n  waveform = waveform.numpy()\n\n  num_channels, num_frames = waveform.shape\n  time_axis = torch.arange(0, num_frames) \/ sample_rate\n\n  figure, axes = plt.subplots(num_channels, 1)\n  if num_channels == 1:\n    axes = [axes]\n  for c in range(num_channels):\n    axes[c].plot(time_axis, waveform[c], linewidth=1)\n    axes[c].grid(True)\n    if num_channels > 1:\n      axes[c].set_ylabel(f'Channel {c+1}')\n    if xlim:\n      axes[c].set_xlim(xlim)\n    if ylim:\n      axes[c].set_ylim(ylim)\n  figure.suptitle(title)\n  plt.show(block=False)\n\ndef plot_specgram(waveform, sample_rate, title=\"Spectrogram\", xlim=None):\n  waveform = waveform.numpy()\n\n  num_channels, num_frames = waveform.shape\n  time_axis = torch.arange(0, num_frames) \/ sample_rate\n\n  figure, axes = plt.subplots(num_channels, 1)\n  if num_channels == 1:\n    axes = [axes]\n  for c in range(num_channels):\n    axes[c].specgram(waveform[c], Fs=sample_rate)\n    if num_channels > 1:\n      axes[c].set_ylabel(f'Channel {c+1}')\n    if xlim:\n      axes[c].set_xlim(xlim)\n  figure.suptitle(title)\n  plt.show(block=False)\n\ndef play_audio(waveform, sample_rate):\n  waveform = waveform.numpy()\n\n  num_channels, num_frames = waveform.shape\n  if num_channels == 1:\n    display(Audio(waveform[0], rate=sample_rate))\n  elif num_channels == 2:\n    display(Audio((waveform[0], waveform[1]), rate=sample_rate))\n  else:\n    raise ValueError(\"Waveform with more than 2 channels are not supported.\")\n\ndef inspect_file(path):\n  print(\"-\" * 10)\n  print(\"Source:\", path)\n  print(\"-\" * 10)\n  print(f\" - File size: {os.path.getsize(path)} bytes\")\n  print_metadata(torchaudio.info(path))\n\ndef plot_spectrogram(spec, title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n  fig, axs = plt.subplots(1, 1)\n  axs.set_title(title or 'Spectrogram (db)')\n  axs.set_ylabel(ylabel)\n  axs.set_xlabel('frame')\n  im = axs.imshow(librosa.power_to_db(spec), origin='lower', aspect=aspect)\n  if xmax:\n    axs.set_xlim((0, xmax))\n  fig.colorbar(im, ax=axs)\n  plt.show(block=False)\n\ndef plot_mel_fbank(fbank, title=None):\n  fig, axs = plt.subplots(1, 1)\n  axs.set_title(title or 'Filter bank')\n  axs.imshow(fbank, aspect='auto')\n  axs.set_ylabel('frequency bin')\n  axs.set_xlabel('mel bin')\n  plt.show(block=False)\n\ndef get_spectrogram(\n    n_fft = 400,\n    win_len = None,\n    hop_len = None,\n    power = 2.0,\n):\n  waveform, _ = get_speech_sample()\n  spectrogram = T.Spectrogram(\n      n_fft=n_fft,\n      win_length=win_len,\n      hop_length=hop_len,\n      center=True,\n      pad_mode=\"reflect\",\n      power=power,\n  )\n  return spectrogram(waveform)\n\ndef plot_pitch(waveform, sample_rate, pitch):\n  figure, axis = plt.subplots(1, 1)\n  axis.set_title(\"Pitch Feature\")\n  axis.grid(True)\n\n  end_time = waveform.shape[1] \/ sample_rate\n  time_axis = torch.linspace(0, end_time,  waveform.shape[1])\n  axis.plot(time_axis, waveform[0], linewidth=1, color='gray', alpha=0.3)\n\n  axis2 = axis.twinx()\n  time_axis = torch.linspace(0, end_time, pitch.shape[1])\n  ln2 = axis2.plot(\n      time_axis, pitch[0], linewidth=2, label='Pitch', color='green')\n\n  axis2.legend(loc=0)\n  plt.show(block=False)\n\ndef plot_kaldi_pitch(waveform, sample_rate, pitch, nfcc):\n  figure, axis = plt.subplots(1, 1)\n  axis.set_title(\"Kaldi Pitch Feature\")\n  axis.grid(True)\n\n  end_time = waveform.shape[1] \/ sample_rate\n  time_axis = torch.linspace(0, end_time,  waveform.shape[1])\n  axis.plot(time_axis, waveform[0], linewidth=1, color='gray', alpha=0.3)\n\n  time_axis = torch.linspace(0, end_time, pitch.shape[1])\n  ln1 = axis.plot(time_axis, pitch[0], linewidth=2, label='Pitch', color='green')\n  axis.set_ylim((-1.3, 1.3))\n\n  axis2 = axis.twinx()\n  time_axis = torch.linspace(0, end_time, nfcc.shape[1])\n  ln2 = axis2.plot(\n      time_axis, nfcc[0], linewidth=2, label='NFCC', color='blue', linestyle='--')\n\n  lns = ln1 + ln2\n  labels = [l.get_label() for l in lns]\n  axis.legend(lns, labels, loc=0)\n  plt.show(block=False)","45b69406":"# Loading audio data into Tensor\n# To load audio data, you can use torchaudio.load.\n\n# This function accepts path-like object and file-like object.\n\n# The returned value is a tuple of waveform (Tensor) and sample rate (int).\n\n# By default, the resulting tensor object has dtype=torch.float32 and its value range is normalized within [-1.0, 1.0].\n\nwaveform, sample_rate = torchaudio.load(dog_paths[0])\nprint_stats(waveform, sample_rate=sample_rate)\nplot_waveform(waveform, sample_rate)\nplot_specgram(waveform, sample_rate)\nplay_audio(waveform, sample_rate)","472d4ab6":"waveform, sample_rate = torchaudio.load(cat_paths[0])\nprint_stats(waveform, sample_rate=sample_rate)\nplot_waveform(waveform, sample_rate)\nplot_specgram(waveform, sample_rate)\nplay_audio(waveform, sample_rate)","5bf30cec":"waveform, sample_rate = waveform, sample_rate = torchaudio.load(cat_paths[0])\nprint_stats(waveform, sample_rate=sample_rate)\n\n# Save without any encoding option.\n# The function will pick up the encoding which\n# the provided data fit\npath = \"save_example_default.wav\"\ntorchaudio.save(path, waveform, sample_rate)","e9951cf8":"waveform, sample_rate = waveform, sample_rate = torchaudio.load(cat_paths[0])\nformats = [\n  \"mp3\",\n  \"flac\",\n  \"vorbis\",\n  \"sph\",\n  \"amb\",\n  \"gsm\",\n]\n\nfor f in formats:\n    print(f)\n    path = f\"save_example.{f}\"\n    torchaudio.save(path, waveform, sample_rate)\n  ","7719ad32":"# Load the data\nwaveform1, sample_rate1 = torchaudio.load(cat_paths[0])\n\n# Define effects\neffects = [\n  [\"lowpass\", \"-1\", \"300\"], # apply single-pole lowpass filter\n  [\"speed\", \"0.8\"],  # reduce the speed\n                     # This only changes sample rate, so it is necessary to\n                     # add `rate` effect with original sample rate after this.\n  [\"rate\", f\"{sample_rate1}\"],\n  [\"reverb\", \"-w\"],  # Reverbration gives some dramatic feeling\n]\n\n# Apply effects\nwaveform2, sample_rate2 = torchaudio.sox_effects.apply_effects_tensor(\n    waveform1, sample_rate1, effects)\n\nplot_waveform(waveform1, sample_rate1, title=\"Original\", xlim=(-.1, 3.2))\nplot_waveform(waveform2, sample_rate2, title=\"Effects Applied\", xlim=(-.1, 3.2))\nprint_stats(waveform1, sample_rate=sample_rate1, src=\"Original\")\nprint_stats(waveform2, sample_rate=sample_rate2, src=\"Effects Applied\")","bb98b439":"plot_specgram(waveform1, sample_rate1, title=\"Original\", xlim=(0, 3.04))\nplay_audio(waveform1, sample_rate1)\nplot_specgram(waveform2, sample_rate2, title=\"Effects Applied\", xlim=(0, 3.04))\nplay_audio(waveform2, sample_rate2)","371055ee":"sample_rate = 8000\n\nrir_raw, _ = get_rir_sample(resample=sample_rate)\n\nplot_waveform(rir_raw, sample_rate, title=\"Room Impulse Response (raw)\", ylim=None)\nplot_specgram(rir_raw, sample_rate, title=\"Room Impulse Response (raw)\")\nplay_audio(rir_raw, sample_rate)","8c2794c1":"rir = rir_raw[:, int(sample_rate*1.01):int(sample_rate*1.3)]\nrir = rir \/ torch.norm(rir, p=2)\nrir = torch.flip(rir, [1])\n\nprint_stats(rir)\nplot_waveform(rir, sample_rate, title=\"Room Impulse Response\", ylim=None)","2813feb0":"speech, _ = get_speech_sample(resample=sample_rate)\n\nspeech_ = torch.nn.functional.pad(speech, (rir.shape[1]-1, 0))\naugmented = torch.nn.functional.conv1d(speech_[None, ...], rir[None, ...])[0]\n\nplot_waveform(speech, sample_rate, title=\"Original\", ylim=None)\nplot_waveform(augmented, sample_rate, title=\"RIR Applied\", ylim=None)\n\nplot_specgram(speech, sample_rate, title=\"Original\")\nplay_audio(speech, sample_rate)\n\nplot_specgram(augmented, sample_rate, title=\"RIR Applied\")\nplay_audio(augmented, sample_rate)","75a30a49":"sample_rate = 8000\nspeech, _ = get_speech_sample(resample=sample_rate)\nnoise, _ = get_noise_sample(resample=sample_rate)\nnoise = noise[:, :speech.shape[1]]\n\nplot_waveform(noise, sample_rate, title=\"Background noise\")\nplot_specgram(noise, sample_rate, title=\"Background noise\")\nplay_audio(noise, sample_rate)\n\nspeech_power = speech.norm(p=2)\nnoise_power = noise.norm(p=2)\n\nfor snr_db in [20, 10, 3]:\n  snr = math.exp(snr_db \/ 10)\n  scale = snr * noise_power \/ speech_power\n  noisy_speech = (scale * speech + noise) \/ 2\n\n  plot_waveform(noisy_speech, sample_rate, title=f\"SNR: {snr_db} [dB]\")\n  plot_specgram(noisy_speech, sample_rate, title=f\"SNR: {snr_db} [dB]\")\n  play_audio(noisy_speech, sample_rate)","ea415e23":"waveform, sample_rate = torchaudio.load(cat_paths[0])\n\nn_fft = 1024\nwin_length = None\nhop_length = 512\n\n# define transformation\nspectrogram = T.Spectrogram(\n    n_fft=n_fft,\n    win_length=win_length,\n    hop_length=hop_length,\n    power=2.0,\n)\n# Perform transformation\nspec = spectrogram(waveform)\n\nprint_stats(spec)\nplot_spectrogram(spec[0], title='torchaudio')","ce6afcc2":"# Saving audio to file","653d88f9":"# DATA AUGMENTATION","0b98beab":"# Loading audio data into Tensor","5b86ff6d":"# Adding background noise","0e46de69":"### Preparing data and utility functions (skip this section)","b73e7d0f":"# Spectrogram"}}