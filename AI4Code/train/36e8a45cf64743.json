{"cell_type":{"68ea9337":"code","a201aa1e":"code","38a8f8d5":"code","e79e9340":"code","d4983dd0":"code","d708a438":"code","a1caab71":"code","2373d517":"code","71f78f82":"code","25345a81":"code","cb3ff655":"code","98a23bb0":"code","a14d3331":"code","c800f4d4":"code","414dc40d":"code","58312d5b":"code","45bb76ed":"code","61718a7f":"code","c8bbb265":"code","224f09c5":"code","01c5bd76":"code","7b24f879":"code","24f9925e":"code","2e51286e":"code","5dca15a8":"code","a00f81df":"code","b0e41ebe":"code","ce0ded03":"code","401d7e67":"code","42d63a92":"code","b15c1514":"code","b44604f8":"code","c50300fd":"markdown","21dc171d":"markdown","67cf45ca":"markdown","536397b9":"markdown","9fcc81d8":"markdown","51e548b7":"markdown","fe32054f":"markdown","b16e0396":"markdown","1f2c49b7":"markdown","5ecbf889":"markdown","1899e68f":"markdown","ca12f01d":"markdown","8b2c042b":"markdown","7cdd5cb8":"markdown","4423b796":"markdown","5e14d7b9":"markdown","a63f0f83":"markdown","f3be1826":"markdown","d1f5dfe1":"markdown","b034e51e":"markdown","3a2c9219":"markdown","54f2acdd":"markdown","f6e62a09":"markdown","dd7dcb61":"markdown","c9a31da0":"markdown","f0259dd6":"markdown","b2c897ca":"markdown","680e519d":"markdown","d4b8fd5f":"markdown","198aa410":"markdown","ee578301":"markdown","a0182fa4":"markdown","311a3cd2":"markdown","232506d5":"markdown","c7c90c1c":"markdown","111eea07":"markdown"},"source":{"68ea9337":"# will need this later for dynamic time warping\n!pip install dtw-python","a201aa1e":"import numpy as np\nimport pandas as pd \nfrom random import sample\nimport random\nimport itertools\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# dynamic time warping\nfrom dtw import *","38a8f8d5":"# load data\nsales = pd.read_csv(\"..\/input\/m5-forecasting-uncertainty\/sales_train_validation.csv\")","e79e9340":"sales.loc[sales.item_id == 'HOBBIES_1_001']\\\n    .sort_values(\"id\")\\\n    .head()","d4983dd0":"# make time series columns integers for more meaningful plotting (can order numbers)\n_cols = list(sales.columns)\nsales.columns = pd.Index(_cols[:6] + [int(c.replace(\"d_\",\"\")) for c in _cols[6:]])\ndel _cols","d708a438":"def series_from_id(_id:str) -> pd.DataFrame:\n    \"\"\"\n    Get a daily time series for a single id\n    \"\"\"\n    return sales.loc[sales.id == _id]\\\n    .iloc[:,6:]\\\n    .T\n\n# Create a global lookup table for fast plotting by department\ndaily_sales_dept_lookup = sales[[\"dept_id\"] + list(sales.columns[6:])]\\\n    .melt(id_vars = \"dept_id\")\\\n    .groupby(\"dept_id variable\".split())\\\n    .agg({\"value\":\"sum\"})\n\ndef series_from_dept(dept:str) -> pd.DataFrame:\n    return daily_sales_dept_lookup.loc[dept]\n\n# create a global lookup table for fast plotting by item\ndaily_sales_item_lookup = sales[[\"item_id\"] + list(sales.columns[6:])]\\\n    .melt(id_vars = \"item_id\")\\\n    .groupby(\"item_id variable\".split())\\\n    .agg({\"value\":\"sum\"})\n\ndef series_from_item(item:str) -> pd.DataFrame:\n    return daily_sales_item_lookup.loc[item]\n\n\"\"\"\nTime series for particular items are quite noisy on a daily level. \nProvide the ability to bin sales (for examply - to a weekly bin) for more stable plots\n\"\"\"\ndef series_from_id_binned(_id:str, bin_every:int = 7) -> pd.DataFrame:\n    \"\"\"\n    Get the sales for an id, grouped by a fixed interval (default 7 - weekly)\n    \"\"\"\n    t = series_from_id(_id).reset_index()\n    t[\"index\"] = t.index.map(lambda x: x - (x % bin_every))\n    t.columns = pd.Index([\"day\", \"sales\"])\n    return t.groupby(\"day\")\\\n        .agg({\"sales\":\"sum\"})\n\ndef series_from_dept_binned(dept:str, bin_every:int = 7) -> pd.DataFrame:\n    \"\"\"\n    Get the sales for a department, grouped by a fixed interval (default 7 - weekly)\n    \"\"\"\n    t = series_from_dept(dept).reset_index()\n    t[\"variable\"] = t.index.map(lambda x: x - (x % bin_every))\n    return t.groupby(\"variable\")\\\n        .agg({\"value\":\"sum\"})\n\ndef series_from_item_binned(item:str, bin_every:int = 7) -> pd.DataFrame:\n    \"\"\"\n    Get the sales for an item (across stores), grouped by a fixed interval (default 7 - weekly)\n    \"\"\"\n    t = series_from_item(item).reset_index()\n    t[\"variable\"] = t.index.map(lambda x: x - (x % bin_every))\n    return t.groupby(\"variable\")\\\n        .agg({\"value\":\"sum\"})","a1caab71":"fig, axes = plt.subplots(nrows = 5, figsize = (12,20))\n_ids = sales[\"id\"].sample(n = 5, random_state = 1)\nfor i in range(len(_ids)):\n    series_from_id(_ids.iloc[i]).plot(ax = axes[i])\ndel _ids","2373d517":"# bin the items by week and plot again\nfig, axes = plt.subplots(nrows = 5, figsize = (12,20))\n_ids = sales[\"id\"].sample(n = 5, random_state = 1)\nfor i in range(len(_ids)):\n    series_from_id_binned(_ids.iloc[i], bin_every = 7).plot(ax = axes[i])\n","71f78f82":"fig, axes = plt.subplots(nrows = 5, figsize = (12,20))\nrandom.seed(2)\n_ids = sample(list(sales[\"item_id\"].unique()), 5)\nfor i in range(len(_ids)):\n    series_from_item_binned(_ids[i], bin_every = 7).plot(ax = axes[i])\n    axes[i].set_title(\"Item: %s\" % _ids[i])","25345a81":"fig, axes = plt.subplots(nrows = 5, figsize = (12,20))\nrandom.seed(3)\n_ids = sample(list(sales[\"dept_id\"].unique()), 5)\nfor i in range(len(_ids)):\n    series_from_dept_binned(_ids[i], bin_every = 7).plot(ax = axes[i])\n    axes[i].set_title(\"Department: %s\" % _ids[i])","cb3ff655":"# plotting 10 series, for demonstration\ndaily_sales_item_lookup.pivot_table(index = \"variable\", columns = \"item_id\", values = \"value\")\\\n    .iloc[:,:5]\\\n    .plot(figsize = (12,6))","98a23bb0":"# Create a lookup table for scaled series\ndaily_sales_item_lookup_scaled = daily_sales_item_lookup\\\n    .pivot_table(index = \"variable\", columns = \"item_id\", values = \"value\").copy()\ndaily_sales_item_lookup_scaled = daily_sales_item_lookup_scaled.div(daily_sales_item_lookup_scaled.mean(axis = 0), axis = 1)\n# bin by week\ndaily_sales_item_lookup_scaled_weekly = daily_sales_item_lookup_scaled.copy().reset_index()\ndaily_sales_item_lookup_scaled_weekly[\"variable\"] = daily_sales_item_lookup_scaled_weekly.variable.map(lambda x: x - (x%7))\ndaily_sales_item_lookup_scaled_weekly = daily_sales_item_lookup_scaled_weekly.groupby(\"variable\").mean()","a14d3331":"# plot those same series, but this time normalized by the series' means. \nrandom.seed(1)\ndaily_sales_item_lookup_scaled_weekly.iloc[:,random.sample(range(1000),10)]\\\n    .plot(figsize = (12,6))","c800f4d4":"from sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster.hierarchy import fcluster, ward, dendrogram","414dc40d":"# heirarchical clustering of scales weekly item sales. \nclf = AgglomerativeClustering(n_clusters=None, distance_threshold = 0).fit(daily_sales_item_lookup_scaled_weekly.T.values)","58312d5b":"# given a linkage model, plog dendogram, with the colors indicated by the a cutoff point at which we define clusters\ndef plot_dendrogram(model, **kwargs):\n    # Create linkage matrix and then plot the dendrogram\n\n    # create the counts of samples under each node\n    counts = np.zeros(model.children_.shape[0])\n    n_samples = len(model.labels_)\n    for i, merge in enumerate(model.children_):\n        current_count = 0\n        for child_idx in merge:\n            if child_idx < n_samples:\n                current_count += 1  # leaf node\n            else:\n                current_count += counts[child_idx - n_samples]\n        counts[i] = current_count\n\n    linkage_matrix = np.column_stack([model.children_, model.distances_,\n                                      counts]).astype(float)\n\n    # Plot the corresponding dendrogram\n    dendrogram(linkage_matrix, **kwargs)\n    return linkage_matrix\n\nplt.figure(figsize = (14,6))\nplt.title('Hierarchical Clustering Dendrogram')\n# plot the top three levels of the dendrogram\nZ = plot_dendrogram(clf, p=5, color_threshold = 110)\nplt.show()","45bb76ed":"# extract clusters from dendogram\nclusters = fcluster(Z, 100, criterion='distance')\n# create a lookup table for series in a given cluster\ndaily_sales_item_lookup_scaled_clustered = daily_sales_item_lookup_scaled_weekly.T.reset_index()\ndaily_sales_item_lookup_scaled_clustered[\"cluster\"] = clusters\ndaily_sales_item_lookup_scaled_clustered = daily_sales_item_lookup_scaled_clustered.set_index(\"cluster item_id\".split())\\\n    .sort_index()\n","61718a7f":"# cluster 1\nrandom.seed(1)\ndaily_sales_item_lookup_scaled_clustered.loc[1]\\\n    .T\\\n    .iloc[:, random.sample(range(daily_sales_item_lookup_scaled_clustered.loc[1].shape[0]), 10)]\\\n    .plot(figsize = (12,6))","c8bbb265":"# series 2\nrandom.seed(1)\ndaily_sales_item_lookup_scaled_clustered.loc[2]\\\n    .T\\\n    .iloc[:, random.sample(range(daily_sales_item_lookup_scaled_clustered.loc[2].shape[0]), 10)]\\\n    .plot(figsize = (12,6))","224f09c5":"# cluster 3\nrandom.seed(1)\ndaily_sales_item_lookup_scaled_clustered.loc[3]\\\n    .T\\\n    .iloc[:, random.sample(range(daily_sales_item_lookup_scaled_clustered.loc[3].shape[0]), 10)]\\\n    .plot(figsize = (12,6))","01c5bd76":"# cluster 7\nrandom.seed(1)\ndaily_sales_item_lookup_scaled_clustered.loc[7]\\\n    .T\\\n    .iloc[:, random.sample(range(daily_sales_item_lookup_scaled_clustered.loc[7].shape[0]), 10)]\\\n    .plot(figsize = (12,6))","7b24f879":"# show two series that look similar but are misaligned, for demonstration purposes\nfig, [ax1,ax2] = plt.subplots(nrows = 2, figsize = (12,6))\ndaily_sales_item_lookup_scaled_weekly[\"HOBBIES_1_062\"].plot(ax = ax1, color = \"C0\")\ndaily_sales_item_lookup_scaled_weekly[\"HOUSEHOLD_2_040\"].plot(ax = ax2, color = \"C1\")\nax1.set_title(\"HOBBIES_1_062\", fontsize= 14)\nax2.set_title(\"HOUSEHOLD_2_040\", fontsize= 14)\nax1.set_xlabel(\"\")\nax2.set_xlabel(\"Days since start\")","24f9925e":"## Align and plot with the Rabiner-Juang type VI-c unsmoothed recursion\ndtw(\n     daily_sales_item_lookup_scaled_weekly[\"HOUSEHOLD_2_040\"],\\\n    daily_sales_item_lookup_scaled_weekly[\"HOBBIES_1_062\"],\\\n    keep_internals=True, \n    step_pattern=rabinerJuangStepPattern(3, \"c\"))\\\n    .plot(type=\"twoway\",offset=10)","2e51286e":"def get_dtw_diff_matrix(cols:list):\n    \"\"\"\n    From a list of series, compute a distance matrix by computing the \n    DTW distance of all pairwise combinations of series.\n    \"\"\"\n    diff_matrix = {}\n    cross = itertools.product(cols, cols)\n    for (col1, col2) in cross:\n        series1 = daily_sales_item_lookup_scaled_weekly[col1]\n        series2 = daily_sales_item_lookup_scaled_weekly[col2]\n        diff = dtw(\n            series1, \n            series2,\n            keep_internals=True, \n            step_pattern=rabinerJuangStepPattern(2, \"c\")\n            )\\\n            .normalizedDistance\n        diff_matrix[(col1, col2)] = [diff]\n    return diff_matrix","5dca15a8":"# sample 50 series, and compute the DTW distance matrix\nrandom.seed(1)\nsample_cols = random.sample(list(daily_sales_item_lookup_scaled_weekly.columns), 50)\ndtw_diff_dict = get_dtw_diff_matrix(sample_cols)\n# make into a df\ndtw_diff_df = pd.DataFrame(dtw_diff_dict).T.reset_index()\\\n    .rename(columns = {\"level_0\":\"item1\", \"level_1\":\"item2\", 0:\"diff\"})\\\n    .pivot_table(index = \"item1\", columns = \"item2\", values = \"diff\")","a00f81df":"# plot a similarity matrix, with a dendogram imposed\nimport seaborn as sns\nsns.clustermap(1-dtw_diff_df)","b0e41ebe":"# ward clustering from difference matrix, where distance is Dynamic time warping distance instead of Euclidean\nt = ward(dtw_diff_df)\n# extract clusters\ndtw_clusters = pd.DataFrame({\"cluster\":fcluster(t, 1.15)}, index = dtw_diff_df.index)","ce0ded03":"dtw_clusters.cluster.value_counts().sort_index().plot.barh()\nplt.title(\"Frequency of DTW clusters\", fontsize = 14)","401d7e67":"# cluster 1\ndaily_sales_item_lookup_scaled_weekly.T.merge(\n    dtw_clusters.loc[dtw_clusters.cluster == 1], \n    left_index = True,\n    right_index = True\n)\\\n    .T\\\n    .plot(figsize = (12,4))","42d63a92":"def plot_dtw(series1:str, series2:str) -> None:\n    dtw(daily_sales_item_lookup_scaled_weekly[series1],\\\n            daily_sales_item_lookup_scaled_weekly[series2],\\\n        keep_internals=True, \n        step_pattern=rabinerJuangStepPattern(2, \"c\"))\\\n        .plot(type=\"twoway\",offset=5)\n\nplot_dtw(\"FOODS_1_119\", \"HOUSEHOLD_2_423\")\nplot_dtw(\"FOODS_2_043\", \"HOUSEHOLD_2_423\")\nplot_dtw(\"HOBBIES_1_300\", \"HOUSEHOLD_2_423\")\n","b15c1514":"# cluster 5\ndaily_sales_item_lookup_scaled_weekly.T.merge(\n    dtw_clusters.loc[dtw_clusters.cluster == 5], \n    left_index = True,\n    right_index = True\n)\\\n    .T\\\n    .plot(figsize = (12,4))","b44604f8":"# see which items are in cluster 5\nplot_dtw(\"FOODS_3_247\", \"FOODS_3_284\")\nplot_dtw(\"FOODS_3_247\", \"HOBBIES_1_122\")\nplot_dtw(\"FOODS_3_247\", \"HOUSEHOLD_1_164\")\nplot_dtw(\"FOODS_3_247\", \"HOUSEHOLD_1_429\")\nplot_dtw(\"FOODS_3_247\", \"HOUSEHOLD_2_318\")","c50300fd":"#### look at a single item's rows to get a feel for the data structure\n","21dc171d":"The chart above plots the two series, and uses dotted lines to indicate which points in time are compared betwen the two series. \n\nYou can see that the by looking forward\/backwards in time, the flat parts of the series are compared, and the sections of the series that have positive sales are compared. This way, the two series aren't found to be dissimilar just because they start at a different time point. ","67cf45ca":"#### Sales data only\n\nFor this notebook, I'll only use `sales_train_validation.csv` dataset, which includes daily sales at an item, store, department and category level. ","536397b9":"## Time Series Clustering For Forecasting Preparation\n\nThe M5 dataset is a set of time series of daily sales by item Categories, Departments, Stores, and Items. These levels of granularity have a heirarchical structure, in that: \n- A `Category` has multiple `Departments`\n- A `Department` spans across multiple `Stores`\n- A `Department` contains multiple `Items`\n- A combination of `Item` and `Store` is a the most granular level of series (SKU). \n\nUnlike more vanilla time series forecasting tasks - where one uses a single series to predict the future of the series, this structure allows for transfer learning between series. \n\nFor example: It may be that a group of items have similar sales patterns, due to seasonality in demand or supply, etc. Thus, it would make sense for a model to learn learn shared patterns to provide more robust forecasts. \n\nOn the other hand, demand for groups of items might differ substancially due to external forces. Training a model to try and find similarities between these groups might only introduce noise - as the demand\/supply for these items are structurally different.\n\n### Clustering series\n\nThe purpose of this notebook is to demonstrate how we might group items that have similar underlying structure before diving into modeling using several clustering approaches. This way, we could pre-process series within groups in a similar way, and train a model for each cluster which specializes in learning this underlying structure. \n\nIn this notebook I'll use a naive heirarchical method to cluster the series, point out the shortcoming of this approach, and speak about a method to approach these shortcomings called Dynamic Time Warping.","9fcc81d8":"#### Plotting 5 unique items (`id`'s)\n","51e548b7":"---\n\n### 5 Example `items`","fe32054f":"---\n\n### Clustering on an item level\n\nFor the purposes of this notebook, I want to pick a granularity to cluster series on that will be simultaneously useful, but not subject to too much noise. \n\nClustering entire departments together doesn't seeem sufficiently useful, as there are only 7 departments - and could probably be forecasted individually. \n\nClustering at the lowest level of granularity (`item_id`) would likely be very volitile, and sort of defeats the purpose of clustering items. I want to understand what _items are similar_ - both in terms of demand and how Walmart changes. Though it could be that demand and store strategies change between stores, clustering items directly (independent of store) serves to achieve this purpose most directly. \n\nI'm also going to group by week, to remove weekly seasonality (for simplicity). This implicitly assumes that all items have similar weekly seasonality. ","b16e0396":"---\n\n## Heirarchical clustering\n\nAs a quick and dirty first approach I'll cluster the series using a simple [Heirarchical Clustering](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering) method. To do so, we treat each time series as a vector of the same length, and cluster them as we would any other set of vectors. \n\nIn this way, we're grouping together series that behave similar at the same points in time. This method does not recognize _shifts_ in otherwise similar series, as I'll discuss later. ","1f2c49b7":"#### Plot samples of series from clusters\n\nTo get a feel for what this clustering method has actually produced, we can plot a sample of series from each cluster, and try and see what is in common between them. ","5ecbf889":"These series are similar in that they all start with few sales, and then experience a period of increasing sales. Seeing the DTW alignment: ","1899e68f":"If you were to describe why these to series are alike you might say: _\"They both start with zero sales, and then steady sales start.\"_\n\nHeirarchical clustering is blind to these types of similarities, however, as it measures the similarity between vectors element-wise - for example using Euclidean distance. Thus, even two identical series might not be recognized as similar if they're shifted from one another. \n\nTo address this, I use a method for comparing misaligned time series called [Dynamic Time Warping](https:\/\/en.wikipedia.org\/wiki\/Dynamic_time_warping). This dynamic programming algorithm allows for elements in one series to be compared to elements in another series that are either _forward or backwards in time_. This allows for two series to be recognized as similar in shape, even if they're not aligned. \n\nIn this section, I demonstrate how one might use this method or a similar method to better group time series which share the same overall shape. ","ca12f01d":"---\n\n### Clustering using DTW\n\nNow I'll use the same hierarchical clustering method, but instead of using Euclidean distance to compare series, I'll use normalized DTW distance.\n\nThe DTW algorithm time complexity is quadratic in the size of each vector. If there are $m$ items in the dataset, each of them with $n$ elements, then the time to compute a full distance matrix between the series in this dataset is $O(m^2n^2)$, which would take a long time for a dataset of this size. To speed up this demonstration, I'll only use a sample of the items in this dataset.\n","8b2c042b":"---\n\n## Dynamic Time Warping\n\nYou might notice that according to the clustering method above, two series that might look \"similar\" to the human eye might not be in the same clusters **if they're not aligned**. For example, consider the following two series:","7cdd5cb8":"---\n\n## Conclusion and Next steps\n\nIn this notebook, I showed two methods one could use to cluster time series: heirarchical clustering using Euclidean distance and clustering using DTW distance. \n\nAs a follow up, I'd like to demonstrate how one could use these methods to improve modeling performance. For example: instead of training a model on the entire dataset, it may make sense to train one model per cluster, as the underlying structure of the series' are similar within a cluster. Or, one could pre-process (scaling, clipping) the series within a cluster in a similar way. ","4423b796":"We can repeat this for another cluster: ","5e14d7b9":"#### Notes\n\n- We see some clear differences in the series shapes - some have traffic all throughout while others don't start until later on\n- Plus the scale is different. The first series gets 1-2 sales a day, while the second gets 2-6 (after a period of relative quiet)\n\n#### Action Items\n\n- To avoid dealing with very noisy series, I'll bin sales by week, which will remove weekly seasonality and lead the number of sales per time unit to be less volitile. ","a63f0f83":"Similar to cluster 1 - the series in cluster 7 are similar in that they all have a period of few sales at the begining before sales pick up. Unlike cluster 1 - where sales pick up around 300 days since the begining of the time frame, the series in cluster 7 pick up sales around 1300-1500 days into the time period. \n\nThey might all be pre-processed in the same way before training, by cutting of the first 1300 datapoints, as there is no signal to be learned before sales pick up 1300 days into the time period. ","f3be1826":"The series in cluster two generally show a period of decreasing trend, and then they stabalize around 700-1000 days into the time frame. \n\n","d1f5dfe1":"#### Functions for isolating particular...\n\n- item_id's (combination of item and store)\n- items (aggregated over stores)\n- departments (aggregated over items and stores)","b034e51e":"### Load packages and data","3a2c9219":"#### Rescaling series\n\nSome items are sold at a way higher rate than others. When we cluster, we don't want the result of our clustering to split popular vs unpopular items. Rather, we're interested in distinguishing the shape between items.\n\nThus, I'll rescale each item according to it's global mean.","54f2acdd":"### Dynamic Time Warping distance: example\n\nTo get an intuition for how dynamic time warping (DTW) works, it's best to see an example. Using the two series from above: ","f6e62a09":"## Notes\n\n- It's clear that there are some interventions that Walmart makes - as sales may dip all of a sudden quite dramatically. For example, it looks like after months of steady sales, sales dip for the first item plotted above - maybe because they're taken off the shelves at one or more store. \n- Similarly, the last chart shows that an item may be introduced and then removed from the shelves on a periodic basis. \n- Predicting how volitile these series are will rely on identifying which items are likely to have these types of interventions - as a sudden decrease in sales may lead to high inter-quartile ranges for a period's sales. ","dd7dcb61":"We can see that what the series in this cluster have in common is that they have intermittent of no sales, followed by periods of stable sales. \n\nShowing the series pair-wise makes this clearer:","c9a31da0":"(I'll skip the breakdown of clusters 4-6 and jump straight to cluster 7, as this gets quite repetitive. ","f0259dd6":"##### Showing cluster 1","b2c897ca":"- Each item in multiple rows: once for each of the stores it's sold in\n- All items are part of the same category (e.g, `HOBBIES`), and department (`HOBBIES_1`)\n- The actual time series are stored in wide format - the series are stored in columes ranging from `1` to `1913`, indicating the days since the start of the time period in question","680e519d":"#### Notes\n\n- For the first and second series, **sales only start at a certain point**, and then they are quite strong. This is probably because these items were introduced by Walmart sometime during this time period. \n- This is important, as if we are to train a model over the entire time period, then we will dilute any real signal with a period of zero sales. Instead, we need a way to clip the begining of the series that has no sales, and only start training after the point that we have data. ","d4b8fd5f":"You can see that the periods of no sales are aligned betwen the two series, and the periods with stable sales are aligned. According to to DTW distance function, the series above are shown to be similar, though if we compared them element-wise we would not be able to detect this. ","198aa410":"The series in cluster 1 seem to be similar in that they start with very few sales up until day 300-500, and then have relatively steady sales past that point. \n\nThe series in this cluster might be pre-processed in a similar way, by cutting off the first 300 days of data, for example. ","ee578301":"#### Extract clusters\n\nUsing this DTW distance matrix, we can perform heirarchical clustering and extract the clusters formed using this new distance function. ","a0182fa4":"Cluster 3... well this cluster doesn't tell a very clear story. In the dendogram above, you'll see that the this cluster is the largest and forms the widest terminal grouping, meaning that the series in this cluster are note found to be very well grouped together. In this way, cluster 3 is a sort of \"other\" grouping, which would likely be the least helpful as a pre-processing step for modeling. ","311a3cd2":"Now that we're scale invariant, we can focus on shape. ","232506d5":"### Notes\n\n- At the Department level, this starts to look like a time series that which are more straight forward to work with. We see clear trend, seasonal (monthly, as I'm binning by weeks as it is) and periodic components. \n- Getting quantiles for these series could be done though the confidence intervals of a traditional parametric time series model (S-ARIMA), or a quantile regression. ","c7c90c1c":"---\n\n### 5 example departments\n\nFinally, plotting 5 randomly selected departments","111eea07":"---\n\n### Get a feel for some time series' shapes\n\nBefore we jump into pre-processing, it's useful to look at a sample of series to see if anything jumps out at us that will motivate how to move forward. "}}