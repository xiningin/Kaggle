{"cell_type":{"0b64a40e":"code","75e0e0c0":"code","92d2ad24":"code","d0751cab":"code","7d422b83":"code","f1859bb6":"code","04bdd56f":"code","9aa1ccb3":"code","667fddea":"code","61aaef66":"code","5db7b4a5":"code","b3cf4389":"code","9a80041a":"code","a5adfd25":"code","a2970b3a":"code","924a10e6":"code","5d1d1f2a":"code","a6d0916f":"code","a5604d18":"code","c4e36d61":"code","c9140058":"code","2552516a":"code","16a309c9":"code","7f609ac0":"markdown","7b15ca9b":"markdown","c7d7e179":"markdown","be9256bd":"markdown","0ee03a13":"markdown","6417d6d3":"markdown","98f4de52":"markdown","3a447bb0":"markdown","a0992de4":"markdown","d8f383c5":"markdown","500df8dd":"markdown","f933d976":"markdown","c53b8866":"markdown","09fad9e2":"markdown","ff737264":"markdown","0b68686a":"markdown","3dbb0ff8":"markdown","61962630":"markdown","34a59c97":"markdown","5b5adf6a":"markdown","ea5aee25":"markdown","f2bb4cde":"markdown","67414282":"markdown","c3bf5271":"markdown","0b03d32b":"markdown","f2f382a4":"markdown","fa7b23b3":"markdown","39e659a7":"markdown","b0a38c2c":"markdown","b4c5813c":"markdown","f0bc71f7":"markdown","a8430b3e":"markdown","d7ae8565":"markdown","1da33d78":"markdown","efab24a8":"markdown","6e82aca7":"markdown","b4e55d5b":"markdown","02de0b93":"markdown","6ee9c676":"markdown","b5471ba4":"markdown","2d4d7538":"markdown"},"source":{"0b64a40e":"import soundfile as sf\nimport matplotlib.pyplot as plt\nfrom IPython.display import Audio\n%matplotlib inline","75e0e0c0":"SAMPLE_ID = \"1e05620be\"\nSAMPLE_FILE = f\"..\/input\/rfcx-species-audio-detection\/train\/{SAMPLE_ID}.flac\"","92d2ad24":"audio_signal, sampling_rate = sf.read(SAMPLE_FILE)","d0751cab":"audio_signal.shape","7d422b83":"sampling_rate","f1859bb6":"fig, ax = plt.subplots(figsize=(12, 8))\n\n\nax.set_title(\"Audio signal for {}\")\nax.plot(audio_signal)\n","04bdd56f":"Audio(SAMPLE_FILE)","9aa1ccb3":"fig, ax = plt.subplots(figsize=(20, 10))\n\n\nax.set_title(f\"Audio signal for {SAMPLE_ID}\")\nax.plot(audio_signal[:2000])","667fddea":"import librosa\nfrom librosa.display import specshow\nimport numpy as np\n\n\ntransformed_audio_signal = librosa.stft(audio_signal)","61aaef66":"transformed_audio_signal[0, 0]","5db7b4a5":"transformed_audio_signal.shape","b3cf4389":"fig, ax = plt.subplots(figsize=(15, 3))\n\n\ntransformed_amplitude = librosa.amplitude_to_db(np.abs(transformed_audio_signal) ** 2, \n                                                ref=np.max)\n\nim = specshow(transformed_amplitude, y_axis='log', x_axis='time', ax=ax)\n\nfig.colorbar(im, format='%+2.0f dB')","9a80041a":"fig, ax = plt.subplots(figsize=(15, 3))\n\n\ntransformed_angle = librosa.amplitude_to_db(np.angle(transformed_audio_signal), \n                                                ref=np.max)\n\nim = specshow(transformed_angle, y_axis='log', x_axis='time', ax=ax)\n\nfig.colorbar(im, format='%+2.0f dB')","a5adfd25":"fig, ax = plt.subplots(figsize=(12, 8))\n\n# How many FFT components are kept?\nN_FFT = 2048\n\nmel_scale_signal = librosa.filters.mel(sampling_rate, N_FFT) \n\nim = specshow(mel_scale_signal, x_axis='linear', ax=ax)\n\nax.set_ylabel('Mel filter')\n\nax.set_title('Mel filter bank')\n\nfig.colorbar(im)\n\nfig.tight_layout()\n\n","a2970b3a":"\nfig, ax = plt.subplots(figsize=(15, 3))\n\nS = librosa.feature.melspectrogram(y=audio_signal, sr=sampling_rate)\nS_dB = librosa.power_to_db(S, ref=np.max)\n\n\nimg = librosa.display.specshow(S_dB, x_axis='time', y_axis='mel', sr=sampling_rate,\n                               fmax=16000, ax=ax)\n\nfig.colorbar(img, ax=ax, format='%+2.0f dB')\n\nax.set(title='Mel-frequency spectrogram')","924a10e6":"S = librosa.feature.melspectrogram(y=audio_signal, sr=sampling_rate, n_mels=128, fmax=16000)\n\nmfccs = librosa.feature.mfcc(S=librosa.power_to_db(S))","5d1d1f2a":"fig, ax = plt.subplots(figsize=(15, 5))\n\n\nim = specshow(mfccs, x_axis='time', y_axis='mel', ax=ax, sr=48000,\n                             fmax=24000, fmin=40)\n\n\nfig.colorbar(im, format='%+2.0f dB')\n\n\nax.set_title(f'MFCC for {SAMPLE_ID}')\n","a6d0916f":"def compute_and_plot_mfccs(sample_id=\"009b760e6\"):\n    sample_file = f\"..\/input\/rfcx-species-audio-detection\/train\/{sample_id}.flac\"\n    audio_signal, sampling_rate = sf.read(sample_file)\n    S = librosa.feature.melspectrogram(y=audio_signal, sr=sampling_rate, n_mels=128,\n                                       fmax=16000) # 16k is better?\n\n    mfccs = librosa.feature.mfcc(S=librosa.power_to_db(S))\n    fig, ax = plt.subplots(figsize=(15, 5))\n\n\n    mfccs = librosa.feature.mfcc(S=librosa.power_to_db(S))\n\n    im = specshow(mfccs, x_axis='time', y_axis='mel', ax=ax, sr=48000,\n                                 fmax=24000, fmin=40)\n\n\n    fig.colorbar(im, format='%+2.0f dB')\n\n\n    ax.set_title(f'MFCC for {sample_id}')","a5604d18":"compute_and_plot_mfccs()","c4e36d61":"compute_and_plot_mfccs(\"015aa6c7c\")","c9140058":"compute_and_plot_mfccs(\"013716dbf\")","2552516a":"# As a bonus, here are more power spectrograms\n\n\n\ndef compute_and_plot_power_spectrogram(sample_id=\"009b760e6\"):\n    sample_file = f\"..\/input\/rfcx-species-audio-detection\/train\/{sample_id}.flac\"\n    audio_signal, sampling_rate = sf.read(sample_file)\n    transformed_audio_signal = librosa.stft(audio_signal)\n\n    fig, ax = plt.subplots(figsize=(15, 3))\n\n\n    transformed_amplitude = librosa.amplitude_to_db(np.abs(transformed_audio_signal) ** 2, \n                                                    ref=np.max)\n\n    im = specshow(transformed_amplitude, y_axis='log', x_axis='time', ax=ax)\n\n    fig.colorbar(im, format='%+2.0f dB')","16a309c9":"compute_and_plot_power_spectrogram()","7f609ac0":"In any signal that is \"wave\"-like, the [Fourier transform](https:\/\/en.wikipedia.org\/wiki\/Fourier_transform) is the king of transformations. \n\nWhy so? The answer lays in the nature of the signal that is a superposition of many \"fundamentals\", i.e. each \"fundmental\" having a frequency and an amplitude thus we can map the (time, amplitude) space into (frequencey, amplitude) space.\n\n\nSo naturally, we will start with this and see different variations.\n\n\nTo be more precise, we will use the short-time Fourier transform which has two main differences: \n\n1. Instead of working with an idalized signal, we work with the sampled one (this we will use discrete Fourier transform)\n\n2. Instead of working with the whole time duration at once, we will use overlapping windows (thus the short-time).\n\n\nLet's apply this transformation using the librosa.stft function.","7b15ca9b":"To start, let's have look at one of the audio files. For that, we will use\nthe `soundfile` library (that comes with `librosa`) to open a `.flac` sample.","c7d7e179":"# Mel Spectrogram","be9256bd":"\n    \n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/5\/5d\/Animal_hearing_frequency_range.svg\">","0ee03a13":"Here are some sound ranges for animals in comparaison: ","6417d6d3":"So overall, a **mono** audio signal (as any wave function) is one dimensional variation of \namplitude over time. \n\nOne final thing is sampling depth, i.e. how many different sounds can be recorded.\nUsually it is either 16 or 32 bits. In what follows, we won't care much about this \nsince the audio signal is represented as a float.\n\nNext, let's see how we can extract features from this audio signal.","98f4de52":"We have already explored the first step.\n\nAs for step 2, we have seen the mel scale but not yet how to map the power spectrum \nusing the triangular overlapping windows. Let's do this.","3a447bb0":"The sampling rate is **48kz.** That means that for each **second**, **48000** samples are captured by the audio recorder (a microphone let's say).\n\nFrom there, we can deduce the sample length in seconds (i.e. how long is the audio file).\n\nIt has **288000** samples and **48000** samples per second as we have mentionnde thus the total duration is:\n\n**2880000 \/ 48000 = 60s**\n\nWe will check this later when playing the audio file. \n\nLet's display the audio signal now.","a0992de4":"To recap: **MFCCs** are obtained by transforming a spectrum using **DCT** with the **Mel scale**. \n\nFor the curious, the term **cepstrum** in MFCC comes from spectrum with the first 4 letters reversed (ceps <-> spec). This is to show that cepstrum are spectrograms of spectrograms (applying an inverse transform).\n\nThat's it for today's notebook. I hope you have enjoyed this short introduction to the Mel-frequency cepstrum (and other related concepts).","d8f383c5":"Let's check another MFCCs for another sample. For that, we need to \nassemble previous elements in a function.","500df8dd":"The transformed signal is a complex number. It has the following shape: \n    \n(1025, 5626)","f933d976":"# Mel Scale","c53b8866":"## Log scale","09fad9e2":"I am new to **audio processing** and have read a lot of things about different Mel transformations and how they are useful as features.\n\nSo, as usual, I thought that the best way for me to gain deeper knowledge is to \nshare a notebook detailing how the transformation works and why it is useful.\n\nLet's go!","ff737264":"You can hear some soothing brid songs on a rainforest. Also, the audio signal is indeed **60s** long. All is good.","0b68686a":"## Triangular Overlapping Windows","3dbb0ff8":"Let's start with the definition. Will it is one of the many possible definitions, DCT I: \n\n\n<img src=\"https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/be8dacb1e78120e504f6fa9d98757c5fc1cd8f89\">","61962630":"As its name indicates, it is a particualr [windows function](https:\/\/en.wikipedia.org\/wiki\/Window_function) that has a [triangular shape](https:\/\/en.wikipedia.org\/wiki\/Window_function#Triangular_window), nothing more to that.\n\nNow the interesting part is to divide the frequency space not uniformely as we would do in a spectrogram but using the Mel scale. To achieve this, \nwe take the Mel spectre that is a tringuaglar overlopping window family and take the dot product with the FFT of the audio signal.","34a59c97":"# Fourier (and Short-time Fourier) Transform","5b5adf6a":"<img width=400 src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/5\/51\/Fourier_unit_pulse.svg\/1280px-Fourier_unit_pulse.svg.png\">\n\n<center> Example of Fourier transforms from Wikipedia <\/center>","ea5aee25":"# MFCC","f2bb4cde":"For the curious: the `.flac` is a loseless audio format (in contrast with MP3 format which is compressed).\n\n","67414282":"# Prelude: what is an audio signal?","c3bf5271":"Nothing fancy, a varying value roughly between 0.5 and -0.5. You can also listen to the audio clip by pressing the play button below.\nI am using the Audio widget to play the file.","0b03d32b":"# Introduction","f2f382a4":"# Resources to go beyond\n\n\n- A good videdo intro to audio processing for classification: https:\/\/www.youtube.com\/watch?v=Z7YM-HAz-IY\n- A good introduction to many audio processing terms: https:\/\/www.vocitec.com\/docs-tools\/blog\/sampling-rates-sample-depths-and-bit-rates-basic-audio-concepts\n- STFT wiki page: https:\/\/en.wikipedia.org\/wiki\/Short-time_Fourier_transform\n- STFT librosa: http:\/\/man.hubwiz.com\/docset\/LibROSA.docset\/Contents\/Resources\/Documents\/generated\/librosa.core.stft.html\n- Nyquist frequency: https:\/\/en.wikipedia.org\/wiki\/Nyquist_frequency \n- Cepstrum: https:\/\/en.wikipedia.org\/wiki\/Cepstrum\n- Mel Scale: https:\/\/en.wikipedia.org\/wiki\/Mel_scale\n- A two videos quick introduction to audio processing for classification: https:\/\/www.youtube.com\/watch?v=Z7YM-HAz-IY and https:\/\/www.youtube.com\/watch?v=-GddLd2_0ok (lots of similiar topics)\n- MFCC: https:\/\/stackoverflow.com\/questions\/1638126\/how-to-make-mfcc-algorithm\n- Hearing range: https:\/\/en.wikipedia.org\/wiki\/Hearing_range\n- DCT: https:\/\/en.wikipedia.org\/wiki\/Discrete_cosine_transform\n- FLAC format: https:\/\/en.wikipedia.org\/wiki\/FLAC\n- Another good MFCC notebook: https:\/\/www.kaggle.com\/seriousran\/mfcc-feature-extraction-for-sound-classification\/data\n- Theory heavy wiki page about spectral density:https:\/\/en.wikipedia.org\/wiki\/Spectral_density#Explanation","fa7b23b3":"Let's have a look at the transformed audio signal.","39e659a7":"Now that we have all the preliminary ingredients laid out, let's see how the algorithm works.\n\nHere are the main steps (from the Wiki page): \n    \n    \n1.     Take the Fourier transform of (a windowed excerpt of) a signal.\n2.     Map the powers of the spectrum obtained above onto the mel scale, using **triangular overlapping windows**.\n3.     Take the logs of the powers at each of the mel frequencies.\n4.     Take the discrete cosine transform of the list of mel log powers, as if it were a signal.\n5.     The MFCCs are the amplitudes of the resulting spectrum.\n    \n    \nLet's explore each step.","b0a38c2c":"# The Algorithm","b4c5813c":"Let's zoom in a bit on the signal to see if there are any patterns? We will take a look\nat the **2000** first samples.","f0bc71f7":"We can see that both MFCCs are quite distinct so this will be useful to train a classification model.","a8430b3e":"Alright, we have seen that **power spectogram** (via the stft) gives a nice representation that can be useful to extract features.\n\nCan we do better?\n\nThe answer is yes. For that we will start with the Mel scale and it will become \nclear why it is useful once I explain what it is.","d7ae8565":"Let's plot the spectre (amplitude and angle)","1da33d78":"![Screenshot_2021-01-30%20Rainforest%20Connection%20Species%20Audio%20Detection%20Kaggle.png](attachment:Screenshot_2021-01-30%20Rainforest%20Connection%20Species%20Audio%20Detection%20Kaggle.png)","efab24a8":"This is the final step.\n\nAnother transform you might ask? How is it defined? Why is it useful?","6e82aca7":"Next, we will compute and plot the **MFCCs**, the big boys of this notebook.","b4e55d5b":"## Discrete Cosine Transform","02de0b93":"Let's describe what we are seeing: \n\n- In the first graph: this is the spectrogram\n- In the second spectrogram: this is something similar to the spectrogram but this time with the \nphase of the STFT and not its amplitude.","6ee9c676":"As its name indicates, we transform the sampled original signal using modulated cosines.\nIt is closely related to DFT but with only real values and only using cosines.\n\n\nAlright, you might ask now: okay, for what?\n\n\nOne possible explanation is that DCT is useful to \"de-correlate\" the signal, i.e. parts of a signal that are very close or similar will be \nmapped to more distinct parts. This isn't very precise but this is the best I have found. :p\n\nThis final step is achieved using  [scipy.fftpack.dct](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.fftpack.dct.html) function and using DCT II by default.","b5471ba4":"This is the easiest step, we take the **log** (or more preciesly the **[decibel](https:\/\/en.wikipedia.org\/wiki\/Decibel)**) of each power component.\n\nThis is achieved in librosa using [power_to_db](https:\/\/librosa.org\/doc\/main\/generated\/librosa.power_to_db.html?highlight=power_to_db) function for example.","2d4d7538":"Once we have introdced the Mel scale, we can now explore some transformations\nbased on it. \n\nLet's start with the Mel spectrogram."}}