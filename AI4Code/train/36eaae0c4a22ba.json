{"cell_type":{"d4b63bf8":"code","686164bb":"code","85ac9feb":"code","ee77db6b":"code","106bc9a3":"code","21c27504":"code","7956596a":"code","f10919ad":"code","22746bd7":"code","17428683":"code","8d945045":"code","23ee5612":"code","7ebaf48e":"code","3cf1e89d":"code","db6764b1":"code","03e1cb1a":"code","70688812":"code","ee488e72":"code","5bce2b07":"code","38c34678":"code","d5cf43b7":"code","74d169fd":"code","48dd39c9":"code","b61fca40":"code","b117c6dd":"code","a5e1df24":"code","9c4a27f8":"code","58ed7d50":"code","fec46f34":"code","f229669e":"code","690af359":"code","1d316505":"code","4b671575":"code","59bb8bfb":"code","6cd11d32":"code","6e8dd2e9":"code","a1ae53f2":"code","0b84905b":"code","8f8015e1":"code","70caa8e9":"code","97f675b3":"code","68bff40d":"code","78d930d6":"code","130f40f0":"code","ca6a8987":"code","54a8c16c":"code","88551e34":"code","26f59e8e":"code","26888c6c":"code","f0835fc9":"code","48d90be2":"code","1a6e66d0":"code","f51a12e4":"code","bd36adad":"code","76f6c4bd":"code","adcc4052":"code","c2c12239":"code","fa097124":"code","32bfdbee":"code","39e3c79a":"code","97fa3e88":"code","7161a2f0":"code","f25b62bc":"code","dd9eb6b9":"code","84b4aaa4":"code","ffd7ba73":"code","076b3877":"markdown","0b41096c":"markdown","3cc28531":"markdown","e624b4f8":"markdown"},"source":{"d4b63bf8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\n\n# to show whole column and rows \npd.set_option('display.max_columns',5400)\npd.set_option('display.max_rows',5400)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n        \n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","686164bb":"#Reading datas\ntrain = pd.read_csv('..\/input\/covid19-global-forecasting-week-5\/train.csv')\ntest = pd.read_csv('..\/input\/covid19-global-forecasting-week-5\/test.csv')\nsubmission = pd.read_csv('..\/input\/covid19-global-forecasting-week-5\/submission.csv')","85ac9feb":"train","ee77db6b":"test","106bc9a3":"submission","21c27504":"print ('train dataset shape : ', train.shape,'\\n', 'test dataset shape : ',test.shape)","7956596a":"#General information about train data set\n\ntrain.info()","f10919ad":"#General information about train data set\n\ntest.info()","22746bd7":"# Checking null values \n\ntrain.isnull().sum()","17428683":"# Checking % null values \n\nround(100*(train.isnull().sum() )\/ train.shape[0],3)","8d945045":"round(100*(test.isnull().sum() )\/ train.shape[0],3)","23ee5612":"train.columns","7ebaf48e":"#checking values where country is not null\n\ntrain.loc[~train['County'].isnull()]","3cf1e89d":"#checking values where country is not null\n\ntrain.loc[train['County'].isnull()]","db6764b1":"# Here we are considering only country wise. Same can be performed for county as well as province_state.\n# So dropping 'County', 'Province_State'\n\ntrain = train.drop(['County', 'Province_State'],axis = 1)\ntest  = test.drop(['County', 'Province_State'],axis = 1)\ntrain","03e1cb1a":"# converting date column from object type to date time \n\ntrain['Date'] = pd.to_datetime(train['Date'])\ntest['Date'] = pd.to_datetime(test['Date'])\ntrain.info()","70688812":"# Creating separate df for confirmed cases & Fatalities\n\nby_tv = train.groupby('Target')\nconfirmed_df = by_tv.get_group('ConfirmedCases')\nconfirmed_df","ee488e72":"fatality_df = by_tv.get_group('Fatalities')\nfatality_df","5bce2b07":"# Plotting mean confirmed cases country wise \n\nplt.figure(figsize=(30,100))\nax0=sns.barplot(x = 'TargetValue',y= 'Country_Region', data = confirmed_df,estimator = np.mean, ci =None)\n\nfor p in ax0.patches:\n  val = p.get_width() # height of each bar\n  x = p.get_x() + p.get_width() + 10.0 #x-cordinate of the text\n  y = p.get_y() + p.get_height()\/2 # y-coordinate of the text\n  ax0.annotate(round(val,2),(x,y)) # attaching bar height to each bar of the barplot\n\nplt.show()\n\n","38c34678":"# Plotting mean fatalities country wise\n\nplt.figure(figsize=(30,100))\n\na = sns.barplot(x = 'TargetValue', y = 'Country_Region', estimator = np.mean, data = fatality_df,ci =None)\n\nfor p in a.patches:\n  val = p.get_width()\n  x = p.get_x() + p.get_width() + 10\n  y = p.get_y() + p.get_height()\/2\n  a.annotate(round(val,2),(x,y))\n\nplt.show()","d5cf43b7":"#country vs targetValue\n\nfig = px.pie(train, values='TargetValue', names='Country_Region')\n\nfig.show()\n","74d169fd":"# ploting confirmed cases country wise with time \n\ncountries =set( confirmed_df['Country_Region'])\n\nlen(countries)","48dd39c9":"#Plotting confirmed cases with date for all countries\n\ncountry_group =train.groupby('Target')\ndf = country_group.get_group('ConfirmedCases')\n\n# df= country_group.get_group('US') # as we want to see the trend for US\ndf = pd.DataFrame(df.groupby(['Country_Region','Date'])['TargetValue'].sum()) # as multiple values present for single date\ndf.reset_index(inplace = True)\ndf = df.loc[df['Date'] >= '2020-02-25'] # As cases ae reported in high number after 25th Feb,2020\n\nfig = px.line(df, x='Date', y ='TargetValue', color = 'Country_Region', title = 'Confirmed cases of all countries with date')\nfig.show()\n\n\n","b61fca40":"#Plotting fatality cases with date for all ocuntries\n\ncountry_group =train.groupby('Target')\ndf = country_group.get_group('Fatalities')\n\n# df= country_group.get_group('US') # as we want to see the trend for US\ndf = pd.DataFrame(df.groupby(['Country_Region','Date'])['TargetValue'].sum()) # as multiple values present for single date\ndf.reset_index(inplace = True)\ndf = df.loc[df['Date'] >= '2020-02-25'] # As cases ae reported in high number after 25th Feb,2020\n\nfig = px.line(df, x='Date', y ='TargetValue', color = 'Country_Region', title = 'Fatalities cases of all countries with date')\nfig.show()\n\n\n","b117c6dd":"#Plotting confirmed cases with date for India\n\n\ncountry_group =confirmed_df.groupby('Country_Region')\n\ndf= country_group.get_group('India')\ndf = pd.DataFrame(df.groupby(['Date'])['TargetValue'].sum())\ndf.reset_index(inplace = True)\ndf = df.loc[df['Date'] >= '2020-02-25']\n\nfig = px.line(df, x='Date', y ='TargetValue',title ='confirmed cases with date for India')\n\nfig.show()\n","a5e1df24":"\n#Plotting fatalities cases with date for India\n\ncountry_group = fatality_df.groupby('Country_Region')\n\ndf= country_group.get_group('India')\ndf = pd.DataFrame(df.groupby(['Date'])['TargetValue'].sum())\ndf.reset_index(inplace = True)\ndf = df.loc[df['Date'] >= '2020-02-25']\n\nfig = px.line(df, x='Date', y ='TargetValue', title ='fatalities cases with date for India')\n\n\nfig.show()\n\n\n","9c4a27f8":"#Plotting confirmed cases with date for 'United Arab Emirates'\n\ncountry_group =confirmed_df.groupby('Country_Region')\n\ndf= country_group.get_group('United Arab Emirates') # as we want to see the trend for US\ndf = pd.DataFrame(df.groupby(['Date'])['TargetValue'].sum()) # as multiple values present for single date\ndf.reset_index(inplace = True)\ndf = df.loc[df['Date'] >= '2020-02-25'] # As cases ae reported in high number after 25th Feb,2020\n\nfig = px.line(df, x='Date', y ='TargetValue',title =\"confirmed cases with date for 'United Arab Emirates'\")\nfig.show()\n\n\n","58ed7d50":"#Plotting fatalities with date for 'United Arab Emirates'\n\ncountry_group =fatality_df.groupby('Country_Region')\n\ndf= country_group.get_group('United Arab Emirates') # as we want to see the trend for US\ndf = pd.DataFrame(df.groupby(['Date'])['TargetValue'].sum()) # as multiple values present for single date\ndf.reset_index(inplace = True)\ndf = df.loc[df['Date'] >= '2020-02-25'] # As cases ae reported in high number after 25th Feb,2020\n\nfig = px.line(df, x='Date', y ='TargetValue',title =\"fatalities with date for 'United Arab Emirates'\")\nfig.show()","fec46f34":"\n#Creating Features from date columns\n\ndef date_feature(df):\n  df['day'] = df['Date'].dt.day\n  df['month'] = df['Date'].dt.month\n#   df['dayofweek'] = df['Date'].dt.dayofweek  \n#   df['weekofyear'] = df['Date'].dt.weekofyear #these are not selected as they dont give good result -reults were checked\n#   df['quarter'] = df['Date'].dt.quarter\n\n  return df\n  \n","f229669e":"train = date_feature(train)\ntest = date_feature(test)\ntrain","690af359":"# dropping date column\n\ntrain.drop(['Date'],axis =1, inplace =True)\ntest.drop(['Date'],axis =1, inplace =True)","1d316505":"train.columns","4b671575":"# Rearranging columns of train\n\ntrain = train [['Id', 'Country_Region', 'Population','day', 'month','Weight','Target', 'TargetValue']]\n# Rearranging columns of test\n\ntest = test [['ForecastId','Country_Region', 'Population','day', 'month','Weight','Target']]\n\ntrain","59bb8bfb":"country_train = set(train['Country_Region']) #unique countries in train dataset\ncountry_test = set(test['Country_Region']) #unique countries in test dataset\n\ncountry_list = [i for i in country_train if i in country_test]\n\nprint('no. of unique countries in train dataset = ', len(country_train),'\\n','no. of unique countries in train dataset = ',len(country_test))\nprint('no. of unique countries after varification =', len(country_list))","6cd11d32":"target_train = set(train['Target'])\ntarget_test = set(test['Target'])\n\ntarget_list = [i for i in target_train if i in target_test]\n\nprint('no. of unique Target values in train dataset = ', len(target_train),'\\n','no. of unique Target values in train dataset = ',len(target_test))\nprint('no. of unique Target values after varification =', len(target_list))","6e8dd2e9":"# encoding target values \n\ncombine = [train,test]\nfor dataset in combine:\n    dataset['Target'] = dataset['Target'].map({'ConfirmedCases':0,'Fatalities':1}).astype(int)\ntrain","a1ae53f2":"#Encoding Country names\n\ncombine = [train,test]\ncountry = train['Country_Region'].unique()\nnum = [item for item in range(1,len(country)+1)]\ncountry_num = dict(zip(country,num))\nfor dataset in combine:\n    dataset['Country_Region'] = dataset['Country_Region'].map(country_num).astype(int)\n\ntrain","0b84905b":"#Removing id from train dataset\nid_train = train.pop('Id')\ntrain","8f8015e1":"# for test dataset\n\nid_test = test.pop('ForecastId')\ntest","70caa8e9":"# Spliting into X and y \n\ny = train.pop('TargetValue')\nX = train\nX","97f675b3":"# Spliting into train and test \n\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test, y_train,y_test = train_test_split(X,y, test_size = 0.10,random_state =7)\nX_train","68bff40d":"X_test","78d930d6":"print('X_train shape : ',X_train.shape, '\\n','X_test shape : ',X_test.shape)","130f40f0":"print('y_train shape : ',y_train.shape, '\\n','y_test shape : ',y_test.shape)","ca6a8987":"col = X_train.columns","54a8c16c":"# # Standardising for faster convergence\n\n# from sklearn.preprocessing import StandardScaler\n\n# scaler = StandardScaler()\n\n# X_train[col] = scaler.fit_transform(X_train[col])\n\n\n","88551e34":"# X_train","26f59e8e":"# X_test[col] = scaler.transform(X_test[col])\n# X_test","26888c6c":"# # Scaling test data set\n\n# test[col] = scaler.transform(test[col])\n# test","f0835fc9":"# Searching for best parameters by Gridsearch\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\n\n\n# Hyperparameter tuning for random forest\n\n# param_rf = {\n#     'max_depth': [8,10],\n#     'min_samples_leaf': range(50, 450, 50),\n#     'min_samples_split':range(50, 300, 50),\n#     'n_estimators': [100,150,200],\n    \n# }\n\n# rf = RandomForestRegressor(n_jobs=-1, max_features='auto',random_state=105)\n\n# folds= KFold(n_splits = 3, shuffle = True, random_state = 90)\n\n# grid_rf = GridSearchCV(estimator = rf, param_grid = param_rf, \n#                           cv = folds, n_jobs = -1,verbose = 1,scoring = 'r2')\n\n\n# # Fitting\n# grid_rf.fit(X_train, y_train)","48d90be2":"# #best params\n# grid_rf.best_params_","1a6e66d0":"# Random forest\n\nrf = RandomForestRegressor(n_jobs = -1,random_state=7)\n\nrf.fit(X_train,y_train)","f51a12e4":"#Predicting\n\ny_train_pred = rf.predict(X_test)\npd.DataFrame({'y_train_test':y_test, 'y_train_pred': y_train_pred})","bd36adad":"# importing metrics\n\nfrom sklearn.metrics import r2_score\n\nr2_score(y_test,y_train_pred)\n","76f6c4bd":"#Predicting on test data for submission\n\ntest_pred = rf.predict(test)\ntest_pred","adcc4052":"# #Hyperparameters tuning\n\n# hyper={'learning_rate': [0.1,0.2,0.3,0.5],\n#           'n_estimators':[100,500,1000,1500,2000],\n#           'subsample':[0.3,0.50,.75],\n#           'reg_alpha':[0,1]\n#           }\n# xgb=XGBRegressor(max_depth=20,tree_method='gpu_hist', gpu_id=0)\n\n# folds= KFold(n_splits=10,shuffle=True,random_state=100)\n\n# xcv=GridSearchCV(estimator=xgb,\n#                     param_grid=hyper,\n#                     cv=folds,\n#                     verbose=1,\n#                     n_jobs=-1,\n#                     return_train_score=True,\n#                     scoring='neg_mean_absolute_error'\n#                     )\n# xcv.fit(X_train,y_train)","c2c12239":"# Using XGboost\n\n# trying with xgboost\n\nxgb=XGBRegressor(max_depth=20,random_state=7,learning_rate=0.3,n_estimators=2000,n_jobs = -1,tree_method='gpu_hist', gpu_id=0)\n\n\nxgb.fit(X_train,y_train)\n","fa097124":"y_pred_xgb = xgb.predict(X_test)\n\npd.DataFrame({'y_train_test':y_test, 'y_train_pred': y_pred_xgb})","32bfdbee":"#Predicting on test data for submission\n\ntest_predx = xgb.predict(test)\ntest_predx","39e3c79a":"#Creatin submission file xgb\n\nsub = pd.DataFrame({'Id': id_test , 'TargetValue': test_predx})\nsub","97fa3e88":"# #Creatin submission file rf\n\n# sub = pd.DataFrame({'Id': id_test , 'TargetValue': test_pred})\n# sub","7161a2f0":"m=sub.groupby(['Id'])['TargetValue'].quantile(q=0.05).reset_index()\nn=sub.groupby(['Id'])['TargetValue'].quantile(q=0.5).reset_index()\nq=sub.groupby(['Id'])['TargetValue'].quantile(q=0.95).reset_index()","f25b62bc":"m.columns = ['Id' , 'q0.05']\nn.columns = ['Id' , 'q0.5']\nq.columns = ['Id' , 'q0.95']","dd9eb6b9":"m = pd.concat([m,n['q0.5'] , q['q0.95']],1)\nm","84b4aaa4":"id_list = []\nvariable_list = []\nvalue_list = []\nfor index, row in m.iterrows():\n  id_list.append(row['Id'])\n  variable_list.append('q0.05')\n  value_list.append(row['q0.05'])\n\n  id_list.append(row['Id'])\n  variable_list.append('q0.5')\n  value_list.append(row['q0.5'])\n\n  id_list.append(row['Id'])\n  variable_list.append('q0.95')\n  value_list.append(row['q0.95'])\n\nsub = pd.DataFrame({'Id':id_list, 'variable': variable_list, 'value':value_list})\nsub","ffd7ba73":"sub = sub.astype({'Id':int})\nsub['variable']=sub['variable'].str.replace(\"q\",\"\", regex=False)\nsub['ForecastId_Quantile']=sub['Id'].astype(str)+'_'+sub['variable']\nsub['TargetValue']=sub['value']\nsub=sub[['ForecastId_Quantile','TargetValue']]\nsub.reset_index(drop=True,inplace=True)\nsub.to_csv(\"submission.csv\",index=False)\nsub","076b3877":"This is a Regression Problem. \nReasons:\n1) We have more than one features (such as population, weight, date) as independent variable.Time series problem has only one independent variable ('Date' or 'Time') \n2) Also here data points are independent of each other","0b41096c":"## **Model Building**","3cc28531":"**Feature Enginnering :**","e624b4f8":"**Exploratory Data Analysis : **"}}