{"cell_type":{"9465b50b":"code","70083345":"code","90ccb80b":"code","89355093":"code","429c5fef":"code","2c88a08d":"code","ce11c373":"code","939cc8d9":"code","dff0bb13":"code","b95b7f36":"code","589356eb":"code","5aec7ec1":"code","25103362":"code","f01779be":"code","6f32621e":"markdown","9a5fc703":"markdown","57a57cb8":"markdown","566bbd4a":"markdown","ffdde24c":"markdown","f2dc3004":"markdown","b96e67b2":"markdown","9e96e80e":"markdown","403d9835":"markdown","d8516145":"markdown","74ef9e6e":"markdown","99467f55":"markdown"},"source":{"9465b50b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedShuffleSplit\nimport xgboost as xgb\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","70083345":"#Reading Data\ntrain_data = pd.read_csv('..\/input\/widsdatathon2021\/TrainingWiDS2021.csv')\n\n\ntest_data = pd.read_csv('..\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv')","90ccb80b":"#Creating flags\ntrain_data['istrain'] = 1\n\ntest_data['istrain'] = 0","89355093":"#Combining Data\ncombined_data = pd.concat([train_data, test_data], axis = 0)","429c5fef":"combined_data['istrain'].value_counts()","2c88a08d":"df_numeric = combined_data.select_dtypes(exclude=['object'])\n\ndf_obj = combined_data.select_dtypes(include=['object']).copy()\n    \nfor c in df_obj:\n    df_obj[c] = pd.factorize(df_obj[c])[0]\n    \ncombined_data = pd.concat([df_numeric, df_obj], axis=1)\n\ny = combined_data['istrain']\n\n#dropping identifiers and target. Not dropping icu_id\n\ncombined_data.drop(['readmission_status','istrain','Unnamed: 0','encounter_id','diabetes_mellitus','hospital_id'], axis = 1, inplace = True)","ce11c373":"train_data.shape","939cc8d9":"test_data.shape","dff0bb13":"combined_data.shape","b95b7f36":"#I'll use an XGB classifier here with default parameters, you may use anything\n\nskf = StratifiedShuffleSplit(n_splits = 5, random_state = 1234,test_size =0.05)\nxgb_params = {'objective': 'binary:logistic','use_label_encoder':False,'n_estimators':100}   \nclf = xgb.XGBClassifier(**xgb_params, seed = 10)    ","589356eb":"scores = []\n\navg_loss = []\n\nfor train_index, test_index in skf.split(combined_data, y):\n       \n        x0, x1 = combined_data.iloc[train_index], combined_data.iloc[test_index]\n        \n        y0, y1 = y.iloc[train_index], y.iloc[test_index]        \n        \n        print(x0.shape)\n        \n        clf.fit(x0, y0, eval_set=[(x1, y1)],\n               eval_metric=['logloss','auc'], verbose=True,early_stopping_rounds=50)\n                \n        prval = clf.predict_proba(x1)[:,1]\n        \n        roc = roc_auc_score(y1,prval)\n\n        scores.append(roc)\n        \n        avg_loss.append(clf.best_score)\n\n        print ('XGB Val OOF AUC=',roc)\n    \nprint(\"Log Loss Stats {0:.8f},{1:.8f}\".format(np.array(avg_loss).mean(), np.array(avg_loss).std()))","5aec7ec1":"print('%.8f (%.8f)' % (np.array(scores).mean(), np.array(scores).std()))","25103362":"# print(clf.feature_importances_)","f01779be":"from xgboost import plot_importance\nfrom matplotlib import pyplot\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 20,10\nplot_importance(clf,max_num_features=20) #top 20 features only\npyplot.show()\n\n#As expected, icu_id is the top feature \n","6f32621e":"<h1> Model training <\/h1>","9a5fc703":"<h1> References <\/h1>\n\n1. https:\/\/www.kaggle.com\/gcspkmdr\/tabular-playground-mar-adversarial-validation\n\n2. https:\/\/www.kaggle.com\/c\/widsdatathon2021\/discussion\/222991\n\n3. https:\/\/machinelearningmastery.com\/feature-importance-and-feature-selection-with-xgboost-in-python\/\n","57a57cb8":"Hello.\n\nThis competition has been an amazing experience for me. I wish I could have invested as much time as I wanted to.\nI have published a few kernels and discussions, hope you found those useful.\n\nSharing an amazing technique I use often in business scenario, to validate if the train and test sets are similar or not.\n\nSecond place solution winners have takled about using AV to drop 15k training samples ( Link in Reference)\n\n** Please upvote this Kernel if you find it useful **","566bbd4a":"AUC score is close to 1 here, implying the train and test datasets are distinguishable.\nThis means normal validation techniques might not work well here.\n\nIt can also indicate the case of a target leakage for train dataset.I have intentionally left icu_id in the features :). Removing it results in and AUC of .7849,which is still quite high.\n\nIt can be used to identify anomalies and refine trianing set if needed.\n\nIntuitively, test data for this use case will only be 100% complete after 24 hours have passed. \nIn Healthcare,typically with time diagnostic techniques get more and more advanced,so if there is a temporal relationship between train and test, Adversarial validation is a good use case to identify those.","ffdde24c":"<h1 Moment of truth :AUC <\/h1","f2dc3004":"<h1> Adversarial Validation <\/h1>\nDrawing a Parallel from Adversarial networks, We make parts of dataset compete against each other to analyse for differences and similarities.\nSimply put, these are the steps for Adversarial Validation:\n\n\n1. Combine the training and testing dataset.Create a flag that indicates the source dataset.\n2. Train a base classifier on the created flag\n3. Analyze AUC, accuracy etc to see how similar training and testing datasets are. \n\n**Generally, The closer AUC is to .5, the better it is.**\n\n<h1> Implications <\/h1>","b96e67b2":"Looking at what features are helping the model distinguish the best,so we may drop it.","9e96e80e":"combined_data.head()","403d9835":"Adversarial Validation can help us analyze whether a data science problem can be formulated or not. It also checks for data stability.\nIn business context, AV is a great analysis to apply at the EDA phase. Any situation where the characteristics of Train and Test data itself are very different, ML might not be the best choice to solve a problem.","d8516145":"<h1> Combining Data <\/h1>","74ef9e6e":"<h1> Feature importance <\/h1>","99467f55":"<h1> Creating Flags <\/h1>"}}