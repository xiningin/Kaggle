{"cell_type":{"fbacdf4d":"code","bc229363":"code","8647faad":"code","8d39d5c7":"code","48d19d45":"code","eb424868":"code","49036b8d":"code","b6c7b7a2":"code","d704be19":"code","b616d291":"code","9353f917":"code","f34c3f0a":"code","96f83bb0":"code","1697b557":"code","25002eba":"code","4cd231b1":"code","fdd96234":"code","0e26503d":"code","57ec185b":"code","bd38ad52":"code","2a43d995":"markdown"},"source":{"fbacdf4d":"import numpy as np\nimport pandas as pd\nimport sklearn\nimport scipy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import linregress\nfrom sklearn.metrics import classification_report,accuracy_score, confusion_matrix\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 14, 8\nRANDOM_SEED = 42\nLABELS = [\"Normal\", \"Fraud\"]","bc229363":"data = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndata.head()","8647faad":"data.info()","8d39d5c7":"data.isnull().values.any()","48d19d45":"fraud = data[data['Class']==1]\n\nnormal = data[data['Class']==0]","eb424868":"print(fraud.shape,normal.shape)","49036b8d":"fraud.Amount.describe()","b6c7b7a2":"normal.Amount.describe()","d704be19":"##b.Plot histograms for the frequency\/number of fraudulent and non-fraudulent transactions against Amount\nf, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nf.suptitle('Amount per transaction by class')\nbins = 50\nax1.hist(fraud.Amount, bins = bins, color='red')\nax1.set_title('Fraud')\nax2.hist(normal.Amount, bins = bins, color='green')\nax2.set_title('Normal')\nplt.xlabel('Amount ($)')\nplt.ylabel('Number of Transactions')\nplt.xlim((0, 20000))\nplt.yscale('log')\nplt.show();","b616d291":"##c.Draw boxplots showing summary statistics for the Amount column\nplt.figure()\nplt.yscale('log')\nsns.set_context({\"figure.figsize\": (14, 8)})\ng = sns.boxplot(data = data, x = 'Class', y = 'Amount')\nplt.title(\"Distribution of Transaction Amount\", fontsize=15)\nplt.xlabel('Class', fontsize=20); plt.xticks(fontsize=15)\nplt.ylabel('Amount', fontsize=20); plt.yticks(fontsize=15)","9353f917":"data1= data.sample(frac = 0.1,random_state=1)\n\ndata1.shape","f34c3f0a":"data.shape","96f83bb0":"Fraud = data1[data1['Class']==1]\n\nValid = data1[data1['Class']==0]\n\noutlier_fraction = len(Fraud)\/float(len(Valid))","1697b557":"print(outlier_fraction)\n\nprint(\"Fraud Cases : {}\".format(len(Fraud)))\n\nprint(\"Valid Cases : {}\".format(len(Valid)))","25002eba":"##d.Generate a correlation matrix illustrating using a heatmap the relationship between the different variables\n## Correlation\nimport seaborn as sns\n#get correlations of each features in dataset\ncorrmat = data1.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(data[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","4cd231b1":"##e.Generate a scatterplot for Amount and V2 showing a line of best fit using the equation of a straight line is y = mx + c, where m is the slope of the line and c is the y intercept\nf, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nf.suptitle('Amount by class vs Version 2')\n\nax1.scatter(fraud.V2, fraud.Amount, color='red')\nax1.set_title('Fraud')\nstats = linregress(fraud.V2, fraud.Amount)\nm = stats.slope\nb = stats.intercept\nax1.plot(fraud.V2, m * fraud.V2 + b, linewidth=3)\n\nax2.scatter(normal.V2, normal.Amount,  color='green')\nax2.set_title('Normal')\nstats = linregress(normal.V2, normal.Amount)\nm = stats.slope\nb = stats.intercept\nax2.plot(normal.V2, m * normal.V2 + b, linewidth=3)\n\nplt.xlabel('V2')\nplt.ylabel('Amount')\n\nplt.show()\n","fdd96234":"#Create independent and Dependent Features\ncolumns = data1.columns.tolist()\n# Filter the columns to remove data we do not want \ncolumns = [c for c in columns if c not in [\"Class\"]]\n# Store the variable we are predicting \ntarget = \"Class\"\n# Define a random state \nstate = np.random.RandomState(42)\nX = data1[columns]\nY = data1[target]\nX_outliers = state.uniform(low=0, high=1, size=(X.shape[0], X.shape[1]))\n# Print the shapes of X & Y\nprint(X.shape)\nprint(Y.shape)","0e26503d":"##f.Build an outlier detection model for your data using the Isolation Forest and the Local Outlier Factor\nclassifiers = {\n    \"Isolation Forest\":IsolationForest(n_estimators=100, max_samples=len(X), \n                                       contamination=outlier_fraction,random_state=state, verbose=0),\n    \"Local Outlier Factor\":LocalOutlierFactor(n_neighbors=20, algorithm='auto', \n                                              leaf_size=30, metric='minkowski',\n                                              p=2, metric_params=None, contamination=outlier_fraction)\n}","57ec185b":"type(classifiers)","bd38ad52":"##g.Analyze the models using Errors, Confusion Matrix, Accuracy Score and Classification Report to identify the strengths and weaknesses of the models\nn_outliers = len(Fraud)\nfor i, (clf_name,clf) in enumerate(classifiers.items()):\n    #Fit the data and tag outliers\n    if clf_name == \"Local Outlier Factor\":\n        y_pred = clf.fit_predict(X)\n        scores_prediction = clf.negative_outlier_factor_\n    else:    \n        clf.fit(X)\n        scores_prediction = clf.decision_function(X)\n        y_pred = clf.predict(X)\n    #Reshape the prediction values to 0 for Valid transactions , 1 for Fraud transactions\n    y_pred[y_pred == 1] = 0\n    y_pred[y_pred == -1] = 1\n    n_errors = (y_pred != Y).sum()\n    # Run Classification Metrics\n    print(\"{}: {}\".format(clf_name,n_errors))\n    print(\"Accuracy Score :\")\n    print(accuracy_score(Y,y_pred))\n    print(\"Confusion Matrix :\")\n    print(confusion_matrix(Y,y_pred))\n    print(\"Classification Report :\")\n    print(classification_report(Y,y_pred))","2a43d995":"G. Analysis of the models using:\n    i. Errors\n    ii. Confusion Matrix\n    iii. Accuracy Score\n    iv. Classification Report \nto identify the strengths and weaknesses of the models\n\n# Observations\n\n1. Errors\nIsolation forest detected 75 errors which is lower than Local Outlier Factor(LOF) which detected 97 errors.\n2. Confusion Matrix\nIsolation Forest detected 28394 true negatives, 38 false positives, 37 false negatives and 12 true positives\nLOF detected 28383 true negatives, 49 false positives, 48 false negatives and 1 true positive\n3. Accuracy Score\nIsolation Forest has a 99.74% more accurate than LOF of 99.65%\n4. Classification report\nWhen comparing error precision & recall forthe 2 models , the Isolation Forest detected 24% ofthe fraudulent cases versus LOF detected 2% of the fraudulent cases\n\nH. Discuss as a conclusion the best model and how to use it in the future in identifying fraudulent credit card transactions.\n\nIsolation forest is a the best model between the two to used in fraud detection activity since its accuracy score is very high and it was able to detect most of the fraudulent cases than LOF.\n\nIsolation forest is an unsupervised algorithm that can be attached to transaction systems to detect fraudulent activity in very large datasets since it detects anomalies through distance and density measures from what is considered normal.The logic argument goes: isolating anomaly observations is easier because only a few conditions are needed to separate those cases from the normal observations. This tool helps to improve detection rates and react faster to new fraud attacks. Its benefit is that it can exploit subsampling of data to achieve a low linear time-complexity and a small memory-requirement, and to deal with the effects of swamping (when normal instances are too close to anomalies) and masking (when the number of anomalies is high) effectively."}}