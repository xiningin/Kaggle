{"cell_type":{"3a03bb71":"code","bc2ead38":"code","519eec0b":"code","e0fbcca4":"code","86a006f4":"code","6eaa1ffb":"code","fe43a4a8":"code","e19f7526":"code","0aecb8e3":"code","c91814ad":"code","a19b0fd1":"code","379ba679":"code","f1e19e4e":"code","6710de5e":"code","5ccacb40":"code","021f114f":"code","162d5d5c":"code","f626898e":"code","ff37bb8e":"code","3d72cfa3":"code","60ae63bc":"code","f43cef4c":"code","b161d33b":"code","a4d8da94":"code","8011d98f":"code","547e459a":"code","261dcdfd":"code","b64e99c5":"code","f176dc74":"code","354880ca":"code","a8d1b623":"code","e3c5252a":"code","bf1c1725":"code","4b5fb831":"code","38fbfb9a":"code","dc1a910e":"code","470deb1f":"code","3af62c7e":"markdown","9eae4dac":"markdown","58210e26":"markdown","bba244e2":"markdown","214c5799":"markdown","84f198bb":"markdown","3cc1d59d":"markdown","223f5c1f":"markdown","b82bb30c":"markdown","0a6af402":"markdown","ef6756f6":"markdown","b0435193":"markdown","8f846445":"markdown","92f2b57f":"markdown"},"source":{"3a03bb71":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nimport math","bc2ead38":"shakespeare_url = 'https:\/\/raw.githubusercontent.com\/karpathy\/char-rnn\/master\/data\/tinyshakespeare\/input.txt'\nfilepath = keras.utils.get_file('shakespeare.txt', shakespeare_url)\nwith open(filepath) as f:\n    shakespeare_text = f.read()","519eec0b":"print(shakespeare_text[:148])","e0fbcca4":"''.join(sorted(set(shakespeare_text.lower())))","86a006f4":"tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\ntokenizer.fit_on_texts(shakespeare_text)","6eaa1ffb":"tokenizer.texts_to_sequences(['First'])","fe43a4a8":"tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])","e19f7526":"max_id = len(tokenizer.word_index) #number of distinct characters\ndataset_size = tokenizer.document_count #total number of characters","0aecb8e3":"[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\ntrain_size = dataset_size * 90 \/\/ 100\ndataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])","c91814ad":"n_steps = 100\nwindow_length = n_steps + 1 #target = input shifted 1 character ahead\ndataset = dataset.window(window_length, shift=1, drop_remainder=True)","a19b0fd1":"dataset = dataset.flat_map(lambda window: window.batch(window_length))","379ba679":"np.random.seed(42)\ntf.random.set_seed(42)","f1e19e4e":"batch_size = 32\ndataset = dataset.shuffle(10000).batch(batch_size)\ndataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))","6710de5e":"dataset = dataset.map(\n    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))","5ccacb40":"dataset = dataset.prefetch(1)","021f114f":"for X_batch, Y_batch in dataset.take(1):\n    print(X_batch.shape, Y_batch.shape)","162d5d5c":"model = keras.models.Sequential([\n    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id], dropout=0.2), \n    keras.layers.GRU(128, return_sequences=True, dropout=0.2), \n    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation='softmax'))])","f626898e":"model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\nhistory = model.fit(dataset, epochs=10)","ff37bb8e":"def preprocess(texts):\n    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n    return tf.one_hot(X, max_id)","3d72cfa3":"X_new = preprocess(['How are yo'])\nY_pred = np.argmax(model(X_new), axis=-1)\ntokenizer.sequences_to_texts(Y_pred + 1)[0][-1] #1st sentence, last character","60ae63bc":"def next_char(text, temperature):\n    X_new = preprocess([text])\n    y_proba = model(X_new)[0, -1:, :]\n    rescaled_logits = tf.math.log(y_proba) \/ temperature\n    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n    return tokenizer.sequences_to_texts(char_id.numpy())[0]","f43cef4c":"tf.random.set_seed(42)\n\nnext_char('How are yo', temperature=1)","b161d33b":"def complete_text(text, n_chars=50, temperature=1):\n    for _ in range(n_chars):\n        text += next_char(text, temperature)\n    return text","a4d8da94":"tf.random.set_seed(42)\nprint(complete_text('t', temperature=0.2))","8011d98f":"print(complete_text('t', temperature=1))","547e459a":"print(complete_text('t', temperature=2))","261dcdfd":"tf.random.set_seed(42)","b64e99c5":"dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\ndataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\ndataset = dataset.flat_map(lambda window: window.batch(window_length))\ndataset = dataset.batch(1)\ndataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\ndataset = dataset.map(\n    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\ndataset = dataset.prefetch(1)","f176dc74":"batch_size = 32\nencoded_parts = np.array_split(encoded[:train_size], batch_size)\ndatasets = []\nfor encoded_part in encoded_parts:\n    dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n    dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n    datasets.append(dataset)\ndataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\ndataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\ndataset = dataset.map(\n    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\ndataset = dataset.prefetch(1)","354880ca":"model = keras.models.Sequential([\n    keras.layers.GRU(128, return_sequences=True, stateful=True, \n                     dropout=0.2, batch_input_shape=[batch_size, None, max_id]), \n    keras.layers.GRU(128, return_sequences=True, stateful=True, dropout=0.2), \n    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation='softmax'))])","a8d1b623":"class ResetStatesCallback(keras.callbacks.Callback):\n    def on_epoch_begin(self, epoch, logs):\n        self.model.reset_states()","e3c5252a":"model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\nhistory = model.fit(dataset, epochs=50, callbacks=[ResetStatesCallback()])","bf1c1725":"stateless_model = keras.models.Sequential([\n    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id]), \n    keras.layers.GRU(128, return_sequences=True), \n    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation='softmax'))])","4b5fb831":"stateless_model.build(tf.TensorShape([None, None, max_id]))","38fbfb9a":"stateless_model.set_weights(model.get_weights())\nmodel = stateless_model","dc1a910e":"tf.random.set_seed(42)\n\nprint(complete_text('t'))","470deb1f":"print(complete_text('t', temperature=0.2))","3af62c7e":"To use the model with different batch_sizes, we need to create a stateless copy. We can get rid of dropout since it is only used during training:","9eae4dac":"## Creating and Training the Model","58210e26":"To set the weights, we must first build the model:","bba244e2":"The stateful RNN needs to know the batch size since it will preserve a state for each input:","214c5799":"The following function gives more diversity while generate the predicted text (temperature close to 0 favor high probability characters, high temperatire gives all characteran equal probability:","84f198bb":"## Stateful RNN","3cc1d59d":"## Loading the Data and Preparing the Dataset","223f5c1f":"We will chop the sequential dataset into multiple windows:","b82bb30c":"We will convert the input's character ID to one hot vectors:","0a6af402":"We will compile and fit the model for more epochs because each epoch is much shorter than earlier and there is only one instance per batch:","ef6756f6":"At the end of the epoch, we will restart the states by using a callback:","b0435193":"## Using the Model to Generate Text","8f846445":"We will define a preprocess function for the inputs:\n","92f2b57f":"**Reference: Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow by Aurelien Geron**"}}