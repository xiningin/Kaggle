{"cell_type":{"68ea8695":"code","31c7c8e9":"code","e41b51db":"code","2912bf7e":"code","3968fe9c":"code","db8e9a66":"code","3993c381":"code","5c8685a0":"code","bb8b19ef":"code","1d8494c8":"code","c146e412":"code","7cacb015":"code","c316a347":"code","97b502e5":"code","cb5c43ae":"code","265e16fd":"code","4707f5f0":"code","9201aa62":"code","ef3b882b":"code","ae8c2fd6":"code","5b6aba25":"code","19afdc03":"code","f49fd0dc":"code","267c5c84":"code","55be06c1":"code","6dd17696":"code","1d942e51":"code","87cfc366":"code","0548475e":"code","3beb27e8":"code","56539e81":"code","ba57ab91":"markdown","e280d83a":"markdown","b6e5bd12":"markdown","47455924":"markdown","758fbcab":"markdown","c13c3706":"markdown","7f37b314":"markdown","ccb735e2":"markdown","5be8e1c3":"markdown","5cc979e5":"markdown","8b97d0ff":"markdown","5b68cd9f":"markdown","f8f570cb":"markdown","4a4f6abe":"markdown","d35fc0a1":"markdown","5136d160":"markdown","41010f9c":"markdown","6ecc096d":"markdown","d9840dd7":"markdown","afdaf024":"markdown","bcbeaced":"markdown","d2ff6299":"markdown","44ca26c8":"markdown"},"source":{"68ea8695":"import numpy as np\r\nimport pandas as pd\r\nimport matplotlib.pyplot as plt\r\nimport seaborn as sns\r\nfrom scipy import stats\r\nfrom scipy.stats import norm\r\n\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.linear_model import LinearRegression\r\nfrom sklearn.metrics import r2_score\r\n\r\nimport os\r\nimport warnings\r\nwarnings.filterwarnings('ignore')\r\n%matplotlib inline","31c7c8e9":"cwd = os.getcwd()\r\ndf_train = pd.read_csv(cwd + \\\\\\train.csv')\r\nprint(df_train.head())","e41b51db":"df_train.info()\r\ndf_train.shape","2912bf7e":"print(\"Skewness: %f\" % df_train['SalePrice'].skew())\r\nprint(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())\r\nsns.distplot(df_train['SalePrice'], fit = norm)\r\n(mu, sigma) = norm.fit(df_train['SalePrice'])\r\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)])\r\nplt.ylabel('Frequency')\r\nplt.title('SalePrice distribution')","3968fe9c":"df_train_1 = df_train.copy()\r\ndf_train_1['SalePrice'] = np.log(df_train['SalePrice'])\r\n\r\nprint(\"Skewness: %f\" % df_train_1['SalePrice'].skew())\r\nprint(\"Kurtosis: %f\" % df_train_1['SalePrice'].kurt())\r\nsns.distplot(df_train_1['SalePrice'], fit = norm)\r\n(mu, sigma) = norm.fit(df_train['SalePrice'])\r\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)])\r\nplt.ylabel('Frequency')\r\nplt.title('SalePrice distribution')","db8e9a66":"corr_matrix = df_train_1.corr()\r\nf, ax = plt.subplots(figsize=(12, 9))\r\nsns.heatmap(corr_matrix, vmax=0.8, cmap=\"RdYlGn\")","3993c381":"cols = corr_matrix.nlargest(10, 'SalePrice')['SalePrice'].index #Sort and select top 10 highly correlated variables\r\ncm = np.corrcoef(df_train[cols].values.T) #Correlation Matrix of the 10 variables\r\nhm = sns.heatmap(cm, vmax= 1, annot= True, fmt='0.2f', square=True, yticklabels=cols, xticklabels=cols, cmap=\"RdYlGn\") ","5c8685a0":"df_train_1.columns","bb8b19ef":"df_train_2 = df_train_1[['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', '1stFlrSF', 'YearBuilt', 'FullBath', 'SalePrice']]","1d8494c8":"sns.pairplot(df_train_2)","c146e412":"total = df_train_2.isnull().sum()\r\npercent = (total\/1459)*100\r\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total No. of Missings', 'Percentage']).sort_values(by=['Percentage'], ascending=False)\r\nprint(missing_data)","7cacb015":"print(\"Skewness: %f\" % df_train_2['GrLivArea'].skew())\r\nprint(\"Kurtosis: %f\" % df_train_2['GrLivArea'].kurt())\r\ndf_train_2.plot.scatter('GrLivArea', 'SalePrice')\r\nf, ax = plt.subplots()\r\nsns.distplot(df_train_2['GrLivArea'], fit=norm)","c316a347":"# Removing the 2 outliers\r\ndf_train_3 = df_train_2.drop(df_train_2['GrLivArea'].sort_values(ascending=False)[:2].index, axis=0)\r\n\r\n# Taking log transform\r\ndf_train_3['GrLivArea'] = np.log(df_train_3['GrLivArea'])\r\n\r\n# Plotting the graphs\r\nprint(\"Skewness: %f\" % df_train_3['GrLivArea'].skew())\r\nprint(\"Kurtosis: %f\" % df_train_3['GrLivArea'].kurt())\r\ndf_train_3.plot.scatter('GrLivArea', 'SalePrice')\r\nf, ax = plt.subplots()\r\nsns.distplot(df_train_3['GrLivArea'], fit=norm)","97b502e5":"print(\"Skewness: %f\" % df_train_3['GarageCars'].skew())\r\nprint(\"Kurtosis: %f\" % df_train_3['GarageCars'].kurt())\r\ndf_train_3.plot.scatter('GarageCars', 'SalePrice')\r\nf, ax = plt.subplots()\r\nsns.boxplot(x='GarageCars', y='SalePrice', data=df_train_3)\r\n\r\ndf_train_3['GarageCars'].value_counts()","cb5c43ae":"print(\"Skewness: %f\" % df_train_3['TotalBsmtSF'].skew())\r\nprint(\"Kurtosis: %f\" % df_train_3['TotalBsmtSF'].kurt())\r\ndf_train_3.plot.scatter('TotalBsmtSF', 'SalePrice')\r\nf, ax = plt.subplots()\r\nsns.distplot(df_train_3['TotalBsmtSF'], fit=norm)","265e16fd":"df_train_4 = df_train_3.copy()\r\ndf_train_4.loc[df_train_4['TotalBsmtSF']>0, 'TotalBsmtSF'] = np.log(df_train_3.loc[df_train_3['TotalBsmtSF']>0, 'TotalBsmtSF'])\r\n\r\nprint(\"Skewness: %f\" % df_train_4.loc[df_train_4['TotalBsmtSF']>0, 'TotalBsmtSF'].skew())\r\nprint(\"Kurtosis: %f\" % df_train_4.loc[df_train_4['TotalBsmtSF']>0, 'TotalBsmtSF'].kurt())\r\ndf_train_4[df_train_4['TotalBsmtSF']>0].plot.scatter('TotalBsmtSF', 'SalePrice')\r\nf, ax = plt.subplots()\r\nsns.distplot(df_train_4[df_train_4['TotalBsmtSF']>0]['TotalBsmtSF'], fit=norm)\r\n","4707f5f0":"print(\"Skewness: %f\" % df_train_4['1stFlrSF'].skew())\r\nprint(\"Kurtosis: %f\" % df_train_4['1stFlrSF'].kurt())\r\ndf_train_4.plot.scatter('1stFlrSF', 'SalePrice')\r\nf, ax = plt.subplots()\r\nsns.distplot(df_train_4['1stFlrSF'], fit=norm)","9201aa62":"df_train_4['1stFlrSF'] = np.log(df_train_4['1stFlrSF'])\r\n\r\nprint(\"Skewness: %f\" % df_train_4['1stFlrSF'].skew())\r\nprint(\"Kurtosis: %f\" % df_train_4['1stFlrSF'].kurt())\r\ndf_train_4.plot.scatter('1stFlrSF', 'SalePrice')\r\nf, ax = plt.subplots()\r\nsns.distplot(df_train_4['1stFlrSF'], fit=norm)","ef3b882b":"print(\"Skewness: %f\" % df_train_4['YearBuilt'].skew())\r\nprint(\"Kurtosis: %f\" % df_train_4['YearBuilt'].kurt())\r\ndf_train_4.plot.scatter('YearBuilt', 'SalePrice')\r\nf, ax = plt.subplots(figsize=(17, 7))\r\nsns.boxplot(x='YearBuilt', y='SalePrice', data=df_train_4)","ae8c2fd6":"print(\"Skewness: %f\" % df_train_4['FullBath'].skew())\r\nprint(\"Kurtosis: %f\" % df_train_4['FullBath'].kurt())\r\ndf_train_4.plot.scatter('FullBath', 'SalePrice')\r\nf, ax = plt.subplots()\r\nsns.boxplot(x='FullBath', y='SalePrice', data=df_train_4)","5b6aba25":"SalePrice = stats.probplot(df_train_4['SalePrice'], plot=plt)\r\nf, ax = plt.subplots()\r\nTotalBsmtSF = stats.probplot(df_train_4[df_train_4['TotalBsmtSF']>0]['TotalBsmtSF'], plot=plt)\r\nf, ax = plt.subplots()\r\nOverallQual = stats.probplot(df_train_4['OverallQual'], plot=plt)\r\nf, ax = plt.subplots()\r\nGrLivArea = stats.probplot(df_train_4['GrLivArea'], plot=plt)\r\nf, ax = plt.subplots()\r\nGarageCars = stats.probplot(df_train_4['GarageCars'], plot=plt)\r\nf, ax = plt.subplots()\r\nFstFlrSF = stats.probplot(df_train_4['1stFlrSF'], plot=plt)\r\nf, ax = plt.subplots()\r\nYearBuilt = stats.probplot(df_train_4['YearBuilt'], plot=plt)\r\nf, ax = plt.subplots()\r\nFullBath = stats.probplot(df_train_4['FullBath'], plot=plt)\r\n","19afdc03":"train_set, val_set = train_test_split(df_train_4, test_size=0.2, random_state=10)","f49fd0dc":"train_set_features = train_set[['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', '1stFlrSF', 'YearBuilt', 'FullBath']]\r\ntrain_set_labels = train_set[['SalePrice']]","267c5c84":"lr = LinearRegression()\r\nlr.fit(train_set_features, train_set_labels)","55be06c1":"print(\"Predictions:\\t\", lr.predict(train_set_features.iloc[:5].values))\r\nprint(\"Labels:\\t\\t\", train_set_labels.iloc[:5].values)","6dd17696":"val_set_features = val_set[['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', '1stFlrSF', 'YearBuilt', 'FullBath']]\r\nval_set_labels = pd.DataFrame(val_set[['SalePrice']].values, columns=['SalePrice'])\r\n#print(val_set_labels)\r\npredictions = lr.predict(val_set_features)\r\n#print(predictions)\r\n#print(val_set_labels.values)\r\n\r\ndf_predictions = pd.DataFrame(predictions, columns = ['Predictions'])\r\nval_test_comparison = pd.concat([df_predictions, val_set_labels], axis=1)\r\n\r\nlin_mse = mean_squared_error(val_test_comparison['SalePrice'], val_test_comparison['Predictions'])\r\nlin_rmse = np.sqrt(lin_mse)\r\nprint(\"Coefficients: \\t\", lr.coef_)\r\nprint(\"RMSE\\t\", lin_rmse)\r\n\r\nr2 = r2_score(val_test_comparison['SalePrice'], val_test_comparison['Predictions'])\r\nprint(\"R Squared: \", r2)\r\n\r\n# Calculating Adjusted R Squared:\r\n\r\nr2_adj = 1 - (1-r2)*(len(predictions)-1)\/(len(predictions)-val_set_features.shape[1]-1)\r\nprint(\"Adjusted R Squared: \", r2_adj)","1d942e51":"df_test = pd.read_csv(cwd + \"\\\\test.csv\")\r\n\r\ndf_test_1 = df_test[['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', '1stFlrSF', 'YearBuilt', 'FullBath']]\r\n\r\nNulls = df_test_1.isnull().sum()\r\nprint(Nulls)\r\n","87cfc366":"from sklearn.impute import SimpleImputer as Imputer\r\n\r\nimputer = Imputer(strategy=\"median\") # Creating an Imputer instance of the class SimpleImputer\r\nimputer.fit(df_test_1) # Call the fit method of Imputer class; This calculates the median of all the attributes\r\nX = imputer.transform(df_test_1) # All the missing values in the train set are replaced by their corresponding medians and returns a plain numpy array\r\n\r\ndf_test_2 = pd.DataFrame(X, columns=df_test_1.columns)\r\nprint(df_test_2.isnull().sum().max())","0548475e":"df_test_3 = df_test_2.copy()\r\ndf_test_3['GrLivArea'] = np.log(df_test_2['GrLivArea'])\r\ndf_test_3.loc[df_test_3['TotalBsmtSF']>0, 'TotalBsmtSF'] = np.log(df_test_2.loc[df_test_2['TotalBsmtSF']>0, 'TotalBsmtSF'])\r\n#df_test_2['TotalBsmtSF'] = np.log(df_test_2['TotalBsmtSF'])\r\ndf_test_3['1stFlrSF'] = np.log(df_test_2['1stFlrSF'])","3beb27e8":"Predictions_ = lr.predict(df_test_3)\r\npredictions_df = pd.DataFrame(Predictions_, columns=['SalePrice'])\r\nsubmission = pd.concat([df_test['Id'], predictions_df], axis=1)\r\nsubmission","56539e81":"#submission.to_excel(cwd + \"\\Submission.csv\", index=False)\r\nsubmission.to_csv('Submission.csv', encoding='utf-8', index=False)","ba57ab91":"The sales price distribution is positively skewed (as evident from the graph with mean shifted towards the left). In order to eliminate the positive skewness, the most common technique is to transform the variable using log.","e280d83a":"# 5. YearBuilt","b6e5bd12":"We find that there a few '0's on the TotalBsmtSF axis fromt he scatter plot and thus taking a log transformation is not possible for the zeroes. Hence, we keep the zeroes as it is and take log only for the non-zero values.","47455924":"## Importing and understanding the dataset","758fbcab":"## Model Building","c13c3706":"# Probability Plots","7f37b314":"Splitting the features and labels:","ccb735e2":"The dataset has 80 features (including the target variable: SalePrice) and 1460 observations. 79 of those are variables that describe the various aspects of purchasing a house in Ames, Lows, US and the last variable is the target variable i.e., the SalePrice. \r\n\r\nFirst, let's try understanding the target variable, the SalePrice.","5be8e1c3":"# 6. FullBath                    ","5cc979e5":"Let's try checking the actual and predicted median house values","8b97d0ff":"We can notice that the two points towards the right extreme seem to be outliers. Let's remove those and take log transformation to reduce the positive skewness.","5b68cd9f":"Making all the NAs as Nulls","f8f570cb":"Let's condition each attribute before training the Model by handling the outliers and missing data:\r\n\r\n## Handling Missing Values","4a4f6abe":"# 4. 1stFlrSF","d35fc0a1":"From the heat map it is clear that variables like OverallQual, YearBuilt, YearRemodAdd, TotalBsmtSF, 1stFlrSF, GrLivArea, FullBath, TotalRmsAbvDrd, GarageYrBlt, Fireplaces, GarageCars and GarageArea have very high correlation with SalePrice. \r\n\r\nLet's see the Pearson's correlation coefficient of top 10 highly correlated variables w.r.t SalePrice","5136d160":"Now that we have a clear understanding of all the attributes, let's now split the dataset into training and validation sets. For this lets use the train_test_split class","41010f9c":"## Time to run the Model on the Test Dataset provided:","6ecc096d":"The predictions look awesome!!! They are very similar. Now lets run this model on the validation set.","d9840dd7":"# 2. GarageCars","afdaf024":"Luckily, we donot have any missing data in the attributes. Now let's concentrate on handling outliers.\r\n\r\n## Handling Outliers\r\n# 1. GrLivArea","bcbeaced":"We can notice here that some variables like (GarageCars, GarageArea) and (YearBuilt, YearRemodAdd) are very inter-related and hence any one of those can be ignored as one is very well represented by the other. \r\n\r\nHence considering only the following attributes: OverallQual, GrLivArea, GarageCars, TotalBSMTSF, 1stFlrSF, YearBuilt and FullBath","d2ff6299":"The GarageCars attribute shows a promising trend with SalePrice. '4' Garagecars is a bit out of trend probably because of lack of observations (only 5 observations) available.\r\n\r\n# 3. TotalBsmtSF","44ca26c8":"The SalePrice is now close to normal and the skewness has drastically reduced to 0.12.\r\n\r\nLet's now focus on other variables. It is not necessary to use all the 79 variables to train the model. We shall use only the attributes that have high correlation with SalePrice"}}