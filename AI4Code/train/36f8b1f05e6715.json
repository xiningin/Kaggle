{"cell_type":{"0518809d":"code","ae5da5ba":"code","3306483e":"code","7fa9e763":"code","1dcae814":"code","3a877a93":"code","518151bd":"code","a9d94b8b":"code","03de40b9":"code","07017edb":"code","275f4772":"code","bbf78a9b":"code","6c4d206b":"code","697427b3":"code","fc38c6e6":"code","469d80d2":"code","d102d574":"code","d8c715f7":"code","95b54ebb":"code","559b7bca":"code","d398f3c2":"code","cfda1f0f":"code","10b36a29":"code","74316077":"code","99133769":"code","651376b0":"code","2f6ea1db":"code","0750b3ce":"markdown","8ec38e2b":"markdown","14edf0c3":"markdown","9c1d3819":"markdown","6b33334e":"markdown","8a5be13c":"markdown","7bbf8ebc":"markdown"},"source":{"0518809d":"%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport gc, itertools, time, math\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport category_encoders as ce\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, log_loss, roc_auc_score\nfrom sklearn.preprocessing import OneHotEncoder, KBinsDiscretizer, LabelEncoder, RobustScaler\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import plot_metric\nfrom sklearn.model_selection import train_test_split","ae5da5ba":"# \u8fde\u7eed\u5b57\u6bb5\u548c\u5206\u7c7b\u578b\u5b57\u6bb5\ncontinue_var = ['I' + str(i) for i in range(1, 14)]\ncat_features = ['C' + str(i) for i in range(1,27)]","3306483e":"# \u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u6d4b\u8bd5\u96c6\u6837\u672c\u91cf\u662f\u8bad\u7ec3\u96c6\u76846\u500d\u591a\ncol_names_train = ['Label'] + continue_var + cat_features\ncol_names_test = col_names_train[1:]\n\nreader = pd.read_csv('\/kaggle\/input\/criteo-dataset\/dac\/train.txt', sep='\\t', \n                     names=col_names_train, chunksize=100000, iterator=True)\n\ntrain = pd.DataFrame()\nstart = time.time()  \nfor i, chunk in enumerate(reader): \n    if train.shape[0] > 1000000:\n        break\n    train = pd.concat([train, chunk.sample(frac=.05, replace=False, random_state=911)], axis=0)  \n    # print(train.shape[0])\n    if i % 20 == 0:\n        print('Processing Chunk No. {}'.format(i)) \nprint('Reading data costs %.2f seconds'%(time.time() - start))\n\n#train = pd.read_csv('\/kaggle\/input\/criteo-dataset\/dac\/train.txt', \n#                       sep='\\t', names=col_names_train,\n#                       chunksize=100000) # ten chunks: first 1,000,000\n\ntest = pd.read_csv('\/kaggle\/input\/criteo-dataset\/dac\/test.txt',sep='\\t', names=col_names_test)\n#train = train.get_chunk(1000000)\nprint('train has {} rows and {} columns'.format(train.shape[0], train.shape[1]))\nprint('test has {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n#train = train.convert_dtypes()\n#test = test.convert_dtypes()","7fa9e763":"# plt.rcParams['figure.facecolor'] = 'white'\n#train['C1'].value_counts().head(50).plot.bar(color='blue',alpha=0.7)\n#test['C1'].value_counts().head(50).plot.bar(color='red',alpha=0.7)","1dcae814":"train.info()","3a877a93":"ls = list(train.columns)\nls.remove('Label')\n\nunbalance = []\nshort = []\nprint('\u8f93\u51fa\u539f\u59cb\u6570\u636e\u679a\u4e3e\u503c\u5bf9\u7167\u8868')\nprint('-'*50)\nfor i in ls:\n    train_len = len(train[i].astype(str).value_counts())\n    test_len = len(test[i].astype(str).value_counts())\n    if (test_len > train_len) and test_len > 100 and train[i].dtype == 'object':\n        unbalance.append(i)\n    elif (test_len > train_len) and test_len <= 100 and train[i].dtype == 'object':\n        short.append(i)\n    else:\n        pass\n    print(i, ' | train: {} | test\uff1a{}'.format(train_len, test_len))","518151bd":"# unbalance\nprint('\u5408\u5e76\u957f\u5c3e\u679a\u4e3e\u503c\uff0c\u4f7f\u5206\u7c7b\u7279\u5f81\u5728\u8bad\u7ec3\u96c6\u4e0e\u6d4b\u8bd5\u96c6\u5206\u5e03\u4e00\u81f4')\nprint('-'*50)\nDISTRI = 0.7 # \u6570\u5b57\u8d8a\u5927\uff0c\u5219\u679a\u4e3e\u503c\u5206\u5f97\u8d8a\u591a\u8d8a\u8be6\u7ec6\nstart = time.time()\nfor col in short:\n    print('processing: {}'.format(col))\n    d1 = train[col].astype(str).value_counts()  # \u8bad\u7ec3\u96c6\u9891\u6570\u8868\n    envalue = d1[: int(len(d1) * DISTRI)].index   # \u53d6\u6309\u7167\u9891\u6570\u6392\u5e8f\u540e\u524dn%\u9879\u76ee\u7684\u679a\u4e3e\u503c\n    train[col] = np.where(train[col].isin(envalue), train[col], 'longtail')\n    test[col] = np.where(test[col].isin(envalue) , test[col], 'longtail')\nprint(\"the program costs {:.2f} seconds\".format(time.time() - start))\n\nDISTRI = 0.5 # \u6570\u5b57\u8d8a\u5927\uff0c\u5219\u679a\u4e3e\u503c\u5206\u5f97\u8d8a\u591a\u8d8a\u8be6\u7ec6\nstart = time.time()\nfor col in unbalance:\n    print('processing: {}'.format(col))\n    d1 = train[col].astype(str).value_counts()  # \u8bad\u7ec3\u96c6\u9891\u6570\u8868\n    envalue = d1[: int(len(d1) * DISTRI)].index   # \u53d6\u6309\u7167\u9891\u6570\u6392\u5e8f\u540e\u524dn%\u9879\u76ee\u7684\u679a\u4e3e\u503c\n    train[col] = np.where((train[col].isin(envalue)) | (train[col].isna()) , train[col], 'longtail')\n    test[col] = np.where((test[col].isin(envalue)) | (test[col].isna()), test[col], 'longtail')\nprint(\"the program costs {:.2f} seconds\".format(time.time() - start))\nprint('\\n')\nprint('\u8f93\u51fa\u7ecf\u8fc7\u5904\u7406\u540e\u7684\u679a\u4e3e\u503c\u5bf9\u7167\u8868')\nprint('-'*50)\nfor i in ls:\n    train_len = len(train[i].astype(str).value_counts())\n    test_len = len(test[i].astype(str).value_counts())\n    print(i, ' | train: {} | test\uff1a{}'.format(train_len, test_len))","a9d94b8b":"train.info()","03de40b9":"# \u8fde\u7eed\u578b\u5b57\u6bb5\u7528\u5206\u7ec4\u5747\u503c\u586b\u5145\nfill_mean = lambda x: x.fillna(x.mean())\nstart = time.time()\nfor col in continue_var:\n    print('filling NA value of {} ...'.format(i))\n    train[col] = train[col].groupby(train['C7']).apply(fill_mean)\n    test[col] = test[col].groupby(test['C7']).apply(fill_mean)\n    train[col] = train[col].fillna(test[col].mean())\n    test[col] = test[col].fillna(test[col].mean())\n    train[col] = train[col].astype('float64')\n    test[col] = test[col].astype('float64')\nprint(\"filling NA costs {:.2f} seconds\".format(time.time() - start))\n\ntrain = train.fillna('unknown')\ntest = test.fillna('unknown')","07017edb":"def robust_transfer(df):\n    '''transfer multiple columns using robustscaler\n       \u5bf9\u8fde\u7eed\u578b\u5b57\u6bb5\u4f7f\u7528robustscaler\n    '''\n    start = time.time()\n    Scaler = RobustScaler().fit(df) \n    df1 = Scaler.transform(df)\n    print(\"encoding is over! the program costs {:.2f} seconds\".format(time.time() - start))\n    return df1\n\n#train0 = robust_transfer(train[features])\n#test0 = robust_transfer(test[features])\n\ndef cut_bins(df, con_cols, qnumbers):\n    '''cut columns into bins\n       \u5bf9\u8fde\u7eed\u578b\u5b57\u6bb5\u8fdb\u884c\u5206\u6876\n    '''\n    start = time.time()\n    for col in con_cols:\n        df[col] = pd.qcut(df[col], qnumbers, duplicates='drop')\n        print('processing: {}'.format(col))\n    print(\"cutting is over! the program costs {:.2f} seconds\".format(time.time() - start))\n    return df\n\n#train = cut_bins(train, continue_var, 6)\n#test = cut_bins(test, continue_var, 6)\n#train[continue_var] = robust_transfer(train[continue_var])\n#test[continue_var] = robust_transfer(test[continue_var])","275f4772":"## def recode_variable(train, test, col, n):\n##     '''recode longtail categorical columns\n##     '''\n##     d1 = train[col].astype(str).value_counts() # \u8bad\u7ec3\u96c6\u9891\u6570\u8868\n##     envalue = d1[: int(len(d1) * n)].index     # \u53d6\u6309\u7167\u9891\u6570\u6392\u5e8f\u540e\u524dn%\u9879\u76ee\u7684\u679a\u4e3e\u503c\n##     train[col] = np.where(train[col].isin(envalue), train[col], 'longtail')\n##     test[col] = np.where(test[col].isin(envalue), test[col], 'longtail')\n##     return train, test\n##     \n## train, test = recode_variable(train, test, 'C19', 0.15)","bbf78a9b":"\n\nTRAIN_LEN = len(train)\n# dt_all = pd.concat([train, test])\n# del train, test\n# gc.collect()\n# handle missing values\n# col_names = dt_all.columns\n# na_dict = dict.fromkeys(col_names[1:14], -10) \n# na_dict.update(dict.fromkeys(col_names[14:], 'NA'))\n# print(na_dict)\n# dt_all = dt_all.fillna(na_dict)\n\ndef transfrom_target_encoder(dt_all, cat_features):\n    start = time.time()\n    for col in cat_features:    \n        dt_all[col] = ce.TargetEncoder().fit_transform(dt_all[col], dt_all['Label'])\n    print(\"encoding is over! the program costs {:.2f} seconds\".format(time.time() - start))\n    return dt_all\n\n\ndef transfrom_target_encoder0(train, test, cat_features):\n    '''fit on train data, transfrom test data\n    '''\n    start = time.time()  \n    ce_target_encoder = ce.TargetEncoder(cols = cat_features).fit(train, train['Label'])\n    train = ce_target_encoder.transform(train)\n    test['Label'] = np.nan\n    test = ce_target_encoder.transform(test)\n    print(\"encoding is over! the program costs {:.2f} seconds\".format(time.time() - start))\n    return train, test\n\ndef transfrom_count_encoder(dt_all, cat_features):\n    start = time.time()\n    for col in cat_features:    \n        dt_all[col] = ce.CountEncoder().fit_transform(dt_all[col])\n    print(\"encoding is over! the program costs {:.2f} seconds\".format(time.time() - start))\n    return dt_all\n\ndef transfrom_label_encoder(dt_all, cat_features):\n    '''do label encoding\n    '''\n    start = time.time()\n    for col in cat_features:\n        dt_all[col] = LabelEncoder().fit_transform(dt_all[col]).astype(str)\n        print('processing: {}'.format(col))\n    print(\"encoding is over! the program costs {:.2f} seconds\".format(time.time() - start))\n    return dt_all\n\ndef cross_features(df, features, outputFeaturelist = False):\n    '''generate synthetic features\n    '''\n    start = time.time()\n    for col in features:\n        for col2 in features:\n            feature_name = col + 'X' + col2\n            features.append(feature_name)\n            df[feature_name] = df[col] * df[col2]\n        print('processing: {}'.format(col))\n    print(\"encoding is over! the program costs {:.2f} seconds\".format(time.time() - start))\n    if outputFeaturelist:\n        return df, features\n    else:\n        return df\n\n#train, continue_var = cross_features(train, continue_var, outputFeaturelist = True)\n#test = cross_features(test)\n#continue_var\n# train, test = transfrom_target_encoder0(train, test)\n# dt_all = transfrom_label_encoder(dt_all)\n#train = transfrom_label_encoder(train, continue_var)\n#test = transfrom_label_encoder(test, continue_var)","6c4d206b":"test.info()","697427b3":"#x_train = dt_all[:TRAIN_LEN]\n#test = dt_all[TRAIN_LEN:]\n# test = test.drop(['C9','I12','I10','C20'], axis=1)\n#cat = ['C'+str(i) for i in range(1,27)]\n# cat.remove(['C9','C20'])\n#x_train = x_train[continue_var]\n#test = test[continue_var]\n'''\nlabel_encoder = LabelEncoder()\nfor col in cat_features:\n    x_train[col]= label_encoder.fit_transform(x_train[col]) \n    test[col]= label_encoder.fit_transform(test[col]) \n'''","fc38c6e6":"train['I13_I6'] = train['I13'] * train['I6']\ntest['I13_I6'] = test['I13'] * test['I6']","469d80d2":"y_train = train[['Label']]\nx_train = train.drop(['Label'], axis=1)\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.20, stratify=y_train, random_state=256)\n#cat_features = [i for i in range(0, 35)]","d102d574":"x_train","d8c715f7":"#test.iloc[:,11:35]","95b54ebb":"def plot_catboost(y_label):\n    '''plot catboost learning curve\n    '''\n    learn_error = pd.read_csv('.\/catboost_info\/learn_error.tsv', sep='\\t')\n    test_error = pd.read_csv('.\/catboost_info\/test_error.tsv', sep='\\t')\n    metric = pd.concat([learn_error, test_error.iloc[:,1]], axis=1)\n    metric.columns = ['iterations','train','test']\n    plt.rcParams['figure.facecolor'] = 'white'\n    metric.plot(x='iterations',y=['train','test'])\n    plt.ylabel(y_label)\n    plt.grid()\n    plt.show()\n    \ndef plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n    '''This function prints and plots the confusion matrix.\n       Normalization can be applied by setting `normalize=True`.\n    '''\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n    plt.rcParams['figure.facecolor'] = 'white'\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","559b7bca":"model = CatBoostClassifier(\n    iterations=100,\n    learning_rate=0.4,\n    task_type='GPU',\n    loss_function='Logloss',\n    depth=8,\n)\n\nfit_model = model.fit(\n    x_train, y_train,\n    eval_set=(x_val, y_val),\n    cat_features = cat_features,\n    verbose=10\n)\n\nfeature_im = fit_model.feature_importances_\n\n# plot_catboost('logloss')\n\ndef feature_selection(features, feature_importance, DISTRI=0.7, selection=False, plot=True):\n    '''do feature selection\n    '''\n    feimp = pd.DataFrame({'feature': features,\n                          'importance': feature_importance}).sort_values(by=['importance'], \n                                                           ascending=False)\n    if selection:\n        feimp = feimp.iloc[: int(feimp.shape[0] * DISTRI),]\n    else:\n        pass\n    \n    if plot:\n        plt.figure(figsize=(7, 10))\n        sns.barplot(data = feimp, x='importance', y='feature')\n        plt.title('Catboost Feature Importances')\n    else:\n        pass\n    return feimp\n    \nfeimp = feature_selection(x_train.columns, feature_im, 0.7)\nfeimp","d398f3c2":"feimp = feature_selection(x_train.columns, feature_im, 0.7, selection=True, plot=False)\nfeimp","cfda1f0f":"x_train = x_train[feimp['feature']]\nx_val = x_val[feimp['feature']]\ntest = test[feimp['feature']]","10b36a29":"cat_features0 = [i for i in feimp['feature'] if 'C' in i]\n# cat_features0","74316077":"model = CatBoostClassifier(\n    iterations=100,\n    learning_rate=0.4,\n    task_type='GPU',\n    loss_function='Logloss',\n     #gpu_ram_part=0.9,\n     #boosting_type='Plain',\n     #max_ctr_complexity=2,\n     depth=8,\n     #gpu_cat_features_storage='CpuPinnedMemory'\n)\n\nfit_model = model.fit(\n    x_train, y_train,\n    eval_set=(x_val, y_val),\n    cat_features=cat_features0,\n    verbose=10\n)\n\nfeature_im = fit_model.feature_importances_\n\nplot_catboost('logloss')\n\n\ny_test = model.predict(test, \n                       prediction_type='Probability',\n                       ntree_end=model.get_best_iteration(), \n                       thread_count=-1,\n                       verbose=None)\n\ny_val_pred = model.predict(x_val, \n                       prediction_type='Probability',\n                       ntree_start=0,\n                       ntree_end=model.get_best_iteration(), \n                       thread_count=-1,\n                       verbose=None)\n\ny_test_pred = y_test[:,1]\ny_val_pred = y_val_pred[:,1]\ny_val_class = np.where(y_val_pred > 0.5,1,0)\nprint('Out of folds logloss is {:.4f}'.format(log_loss(y_val, y_val_pred)))\n\nsubmission = pd.read_csv('..\/input\/criteo-display-ad-challenge\/random_submission.zip', compression='zip')\nsubmission = pd.DataFrame({'Id': submission['Id'], 'Predicted': y_test_pred })\nsubmission.to_csv('submission.csv',index = False)\n\nsubmission = pd.DataFrame({'Id': submission['Id'], 'Predicted': y_test_pred * 0.98})\nsubmission.to_csv('submission2.csv',index = False)\n\nsubmission = pd.DataFrame({'Id': submission['Id'], 'Predicted': y_test_pred * 1.02})\nsubmission.to_csv('submission3.csv',index = False)\nsubmission","99133769":"feimp = pd.DataFrame({'feature': x_train.columns,\n              'importance': feature_im }).sort_values(by=['importance'], \n                                                           ascending=False)\nplt.figure(figsize=(7, 10))\nsns.barplot(data = feimp, x='importance', y='feature')\nplt.title('Catboost Feature Importances')","651376b0":"class_names = ['0','1']\nplot_confusion_matrix(confusion_matrix(y_val, y_val_class),\n                      classes=class_names, \n                      normalize=True, \n                      title='Confusion Matrix')","2f6ea1db":"'''\nimport xgboost as xgb\nmodel = XGBClassifier(\n    objective='binary:logistic',\n    tree_method = 'gpu_hist',\n    n_jobs=-1,\n    n_estimators=1000,\n    max_depth=16,\n    colsample_bytree=0.8, \n    subsample=0.8, \n    learning_rate=0.2,\n    min_child_weight=6# \u53f6\u5b50\u4e0a\u7684\u6700\u5c0f\u6837\u672c\u6570\n)\n\n\nmodel.fit(\n    x_train, \n    y_train, \n    eval_metric='logloss', \n    eval_set=[(x_train, y_train), (x_val, y_val)], \n    verbose=True, \n    early_stopping_rounds = 10\n)\n\nplt.rcParams['figure.facecolor'] = 'white'\nevals_result = model.evals_result()\nax = plot_metric(evals_result, metric = 'logloss')\nplt.title('Xgboost Learning Curve')\nplt.show()\n\n\nfig,ax = plt.subplots(figsize=(7,10))\nxgb.plot_importance(model,\n                ax=ax,\n                height=0.5).set(xlabel='feature importance',\n                                         title='',\n                                         ylabel='feature')\n\ny_test_pred = model.predict_proba(test)[:,1]\ny_val_pred = model.predict_proba(x_val)[:,1]\ny_val_class = np.where(y_val_pred > 0.5,1,0)\nprint('Out of folds logloss is {:.4f}'.format(log_loss(y_val, y_val_pred)))\n\nsubmission = pd.read_csv('..\/input\/criteo-display-ad-challenge\/random_submission.zip', compression='zip')\nsubmission = pd.DataFrame({'Id': submission['Id'], 'Predicted': y_test_pred})\nsubmission.to_csv('submission_xgb.csv',index = False)\nsubmission\n\nclass_names = ['0','1']\nplot_confusion_matrix(confusion_matrix(y_val, y_val_class),\n                      classes=class_names, \n                      normalize=True, \n                      title='Normalized Confusion Matrix: Xgboost')\n'''","0750b3ce":"## Xgboost","8ec38e2b":"https:\/\/contrib.scikit-learn.org\/category_encoders\/targetencoder.html","14edf0c3":"\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u5b58\u5728\u5206\u5e03\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002\u5f88\u591aid\u7c7b\u7c7b\u522b\u5b57\u6bb5\u7684\u679a\u4e3e\u503c\u5728\u6d4b\u8bd5\u96c6\u4e2d\u51fa\u73b0\uff0c\u5374\u672a\u5728\u8bad\u7ec3\u96c6\u4e2d\u51fa\u73b0\u3002\u8fd9\u4e5f\u5bfc\u81f4\u4e86\u63d0\u4ea4\u540e\u7684\u5206\u6570\u548clocal cv\u5206\u6570\u6781\u4e3a\u4e0d\u4e00\u81f4\u3002","9c1d3819":"## Automatic Feature Selection Using Catboost Feature Importance\n\n\n\u9996\u5148\u5bf9\u8bad\u7ec3\u96c6\u5efa\u7acb\u4e00\u4e2aCatboost\u7b97\u6cd5\uff0c\u8f93\u51fa\u6bcf\u4e2a\u7279\u5f81\u7684\u91cd\u8981\u6027\uff0c\u518d\u6839\u636e\u91cd\u8981\u6027\u6392\u5e8f\uff0c\u4e22\u5f03\u6700\u4e0d\u91cd\u8981\u7684\u540en%\u7684\u7279\u5f81","6b33334e":"## Catboost Fitting for Prediction","8a5be13c":"https:\/\/www.kaggle.com\/heatherqiu\/ctr-features-gbdt-lr\n\nhttps:\/\/www.kaggle.com\/akishen74\/ctr-practice\/","7bbf8ebc":"\u8fd9\u91cc\u6ce8\u610f\u5c3d\u91cf\u5747\u5300\u5730\u4ece\u8bad\u7ec3\u96c6\u4e2d\u8bfb\u53d6\u6570\u636e\uff0c\u4ec5\u4ec5\u91c7\u6837\u6570\u636e\u96c6\u4e2d\u7684\u524dn\u884c\u662f\u4e0d\u591f\u7684\u3002\u8fd9\u6837\u5904\u7406\u4e4b\u540e\u6210\u7ee9\u63d0\u5347\u4e860.005\u5de6\u53f3"}}