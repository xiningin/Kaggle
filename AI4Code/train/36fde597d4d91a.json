{"cell_type":{"b588b9de":"code","d91c278d":"code","636d3741":"code","69c5e454":"code","cd33a356":"code","2ebe9790":"code","96979256":"code","66a129da":"code","57f9a362":"code","3be3a772":"code","b8b43f8f":"code","b96fad32":"code","6e14e43d":"code","bc157ea6":"markdown"},"source":{"b588b9de":"!pip install lofo-importance","d91c278d":"import cupy\nimport cudf\nimport cuml\nfrom tqdm import tqdm\nimport os, sys\nimport torch\n\nPATH = \"..\/input\/ubiquant-market-prediction\"\n\n\ndf = cudf.read_csv(f\"{PATH}\/train.csv\", nrows=200000)\nprint(df.shape)\ndf.head()","636d3741":"df[\"time_id\"].max()","69c5e454":"WINDOW = 5\nSTART = 60\nN_SPLITS = 5\n\ncv = []\n\nfor i in range(N_SPLITS):\n    train_ind = cupy.where(df[\"time_id\"].values <= START + i*WINDOW)[0]\n    val_ind = cupy.where((df[\"time_id\"].values > START + i*WINDOW) & (df[\"time_id\"].values <= START + (i+1)*WINDOW))[0]\n    cv.append((cupy.asnumpy(train_ind), cupy.asnumpy(val_ind)))\n    print(len(train_ind), len(val_ind))","cd33a356":"features = [col for col in df.columns if col not in {\"row_id\", \"target\"}]\nlen(features)","2ebe9790":"from lofo import Dataset, LOFOImportance, plot_importance\nfrom sklearn.model_selection import GroupKFold\n\nds = Dataset(df.to_pandas(), target=\"target\", features=features,\n    feature_groups=None,\n    auto_group_threshold=0.6\n)","96979256":"import xgboost as xgb\n\n\nparam = {'objective': 'reg:squarederror',\n         'learning_rate': 0.1,\n         'max_depth': 5,\n         \"min_child_weight\": 200,\n         \"tree_method\": 'gpu_hist', \"gpu_id\": 0,\n         'disable_default_eval_metric': 1,\n         \"n_estimators\": 300\n    }\n\nmodel = xgb.XGBRegressor(**param)","66a129da":"lofo_imp = LOFOImportance(ds, cv=cv, scoring=\"neg_mean_squared_error\", model=model)\n\nimportance_df = lofo_imp.get_importance()\nimportance_df","57f9a362":"importance_df[\"feature_full_name\"] = importance_df[\"feature\"].values\nimportance_df[\"feature\"] = importance_df[\"feature_full_name\"].apply(lambda x: x[:100])","3be3a772":"plot_importance(importance_df, figsize=(16, 12))","b8b43f8f":"plot_importance(importance_df.head(16), figsize=(16, 12))","b96fad32":"plot_importance(importance_df.tail(16), figsize=(16, 12))","6e14e43d":"importance_df.to_csv(\"feature_importance.csv\", index=False)","bc157ea6":"# Ubiquant Feature Importance with LOFO\n\n![](https:\/\/raw.githubusercontent.com\/aerdem4\/lofo-importance\/master\/docs\/lofo_logo.png)\n\n**LOFO** (Leave One Feature Out) Importance calculates the importances of a set of features based on **a metric of choice**, for **a model of choice**, by **iteratively removing each feature from the set**, and **evaluating the performance** of the model, with **a validation scheme of choice**, based on the chosen metric.\n\nLOFO first evaluates the performance of the model with all the input features included, then iteratively removes one feature at a time, retrains the model, and evaluates its performance on a validation set. The mean and standard deviation (across the folds) of the importance of each feature is then reported.\n\nWhile other feature importance methods usually calculate how much a feature is used by the model, LOFO estimates how much a feature can make a difference by itself given that we have the other features. Here are some advantages of LOFO:\n* It generalises well to unseen test sets since it uses a validation scheme.\n* It is model agnostic.\n* It gives negative importance to features that hurt performance upon inclusion.\n* It can group the features. Especially useful for high dimensional features like TFIDF or OHE features. It is also good practice to group very correlated features to avoid misleading results.\n* It can automatically group highly correlated features to avoid underestimating their importance.\n\nhttps:\/\/github.com\/aerdem4\/lofo-importance"}}