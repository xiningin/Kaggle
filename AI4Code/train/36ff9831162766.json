{"cell_type":{"9f2b92ac":"code","b2ba0882":"code","e98cfbf6":"code","0d0ee296":"code","eae2c8eb":"code","6fe80314":"code","42b30d84":"code","f084542b":"markdown","6e04ce4d":"markdown","494c30f8":"markdown","6b33d376":"markdown","78121253":"markdown","189f24be":"markdown"},"source":{"9f2b92ac":"# You will have to restart the kernel after running this\n!pip install bertopic -qq","b2ba0882":"%%capture\n\nimport pandas as pd\nfrom bertopic import BERTopic\nfrom sentence_transformers import SentenceTransformer\n\ndf = pd.read_csv(\"..\/input\/chaii-qa-simple-google-translate\/train_with_google_translations.csv\")\n\nsentence_model = SentenceTransformer(\"all-mpnet-base-v2\")\ntopic_model = BERTopic(embedding_model=sentence_model)\n\n\ntopics, probs = topic_model.fit_transform(df[\"context_gtrans\"].dropna().tolist())","e98cfbf6":"topic_model.get_topic_info()","0d0ee296":"# This gets the words most associated with the topic\nfor topic_num in set(topics):\n    print(\"\\n\\n\", f\"Topic: {topic_num}\", [x[0] for x in topic_model.get_topic(topic_num)])","eae2c8eb":"from plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\n\nfig = topic_model.visualize_topics(height=1000, width=1000)\n\n\niplot(fig)","6fe80314":"topic_model.visualize_hierarchy(height=1000, width=1000)","42b30d84":"topic_model.visualize_heatmap(height=1000, width=1000)","f084542b":"### Here we can see that most of the topics are about India, math, science, and history","6e04ce4d":"### I think these topics are pretty coherent\n\nHere are some that are easy to pick out\n\nTopic 1: India  \nTopic 4: Thinkers and Inventors \nTopic 5: Agriculture  \nTopic 6: Europe  \nTopic 7: Rivers and Bodies of Water    \nTopic 9: East Asia  \nTopic 10: Acting and Movies  \nTopic 13: Solar System  \nTopic 15: World War II  \nTopic 17: Disease  \nTopic 18: Chemistry  \nTopic 20: Biology  \nTopic 26: Exotic Mammals  \nTopic 27: Nuclear Chemistry\/Physics  \nTopic 30: Language   \nTopic -1: Stopwords  \n\nNote: Because this is a stochastic process, if you run the model again it will not generate the exact same results.","494c30f8":"### This won't help your score at all, but if you are curious to see what topics are in the training contexts, here you go. \n\n<p >\n<img src=\"https:\/\/maartengr.github.io\/BERTopic\/logo.png\" alt=\"drawing\" width=\"200\" style=\"float: left; margin: 20px;\"\/>\n <\/p>\n \nSince I wouldn't understand the topics unless they are in English, I'm working with a translated set. Thank you to @jacob34 (Clear 'n Simple) for translating everything to English in this notebook https:\/\/www.kaggle.com\/jacob34\/chaii-qa-simple-google-translate\n    \nI'll be using BERTopic, which can do a pretty good job of finding topics. If you don't know about this package, it is actually quite impressive. See the website here for more details: https:\/\/maartengr.github.io\/BERTopic\/\n\nI turned the GPU on because the sentence transformer will go much faster.\n\n","6b33d376":"### Topics in 2 dimensions\nHover over the bubbles to see the topic name, or use the slider at the bottom to highlight a topic.","78121253":"### Topics through hierarchical clustering\nAnother way of visualizing how the topics are associated","189f24be":"### Hover over a square to see the similarities between the topics"}}