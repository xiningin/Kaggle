{"cell_type":{"c8eb252d":"code","ae06daec":"code","adca427b":"code","8df7587e":"code","7090dbaa":"markdown","55501486":"markdown","5ec5b57c":"markdown","73ca5754":"markdown","67f4d109":"markdown","bbd68cc8":"markdown"},"source":{"c8eb252d":"!pip install kaleido","ae06daec":"import re\nimport json\nimport glob\nfrom dataclasses import dataclass\nimport multiprocessing\nimport numpy as np\nimport pandas as pd\nfrom scipy.interpolate import interp1d\nimport scipy.signal as signal\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nINPUT_PATH = '..\/input\/indoor-location-navigation'\n\n@dataclass\nclass ReadData:\n    startTime   : int\n    endTime     : int\n    acce        : np.ndarray\n    acce_uncali : np.ndarray\n    gyro        : np.ndarray\n    gyro_uncali : np.ndarray\n    magn        : np.ndarray\n    magn_uncali : np.ndarray\n    ahrs        : np.ndarray\n    wifi        : np.ndarray\n    ibeacon     : np.ndarray\n    waypoint    : np.ndarray\n\nre_LINE_HEADER = re.compile('\\\\d{13}\\tTYPE_')\ndef to_logical_lines(line):\n    mutch_list = list(re.finditer(re_LINE_HEADER, line))\n    assert(len(mutch_list) > 0)\n    index_list = [m.start() for m in mutch_list] + [len(line)]\n    logical_lines = (line[index_list[i]:index_list[i+1]] for i in range(len(index_list)-1))\n    for logical_line in logical_lines:\n        yield logical_line\n    return\n\ndef read_data_file(data_filename, is_test=False):\n    acce = list()\n    acce_uncali = list()\n    gyro = list()\n    gyro_uncali = list()\n    magn = list()\n    magn_uncali = list()\n    ahrs = list()\n    wifi = list()\n    ibeacon = list()\n    waypoint = list()\n    startTime = None\n    endTime   = None\n\n    with open(data_filename, 'r', encoding='utf-8') as file:\n        lines = file.readlines()\n\n    for line in lines:\n        line = line.strip()\n        if line.startswith('#\\tstartTime'):\n            startTime = int(re.split('[:\\t]', line)[2])\n            continue\n        if line.startswith('#\\tendTime'):\n            endTime = int(re.split('[:\\t]', line)[2])\n            continue\n        if (not line) or line.startswith('#'):\n            continue\n        for line_data in to_logical_lines(line):\n            line_data = line_data.split('\\t')\n\n            if line_data[1] == 'TYPE_ACCELEROMETER':\n                acce.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n                continue\n\n            if line_data[1] == 'TYPE_ACCELEROMETER_UNCALIBRATED':\n                if len(line_data) > 5:\n                    acce_uncali.append([int(line_data[0]),\n                                        float(line_data[2]), # x\n                                        float(line_data[3]), # y\n                                        float(line_data[4]), # z\n                                        float(line_data[5]), # x\n                                        float(line_data[6]), # y\n                                        float(line_data[7]), # z\n                                        int(  line_data[8]), # accuracy\n                                        ])\n                else:\n                    acce_uncali.append([int(line_data[0]),\n                                        float(line_data[2]), # x\n                                        float(line_data[3]), # y\n                                        float(line_data[4]), # z\n                                        ])\n                continue\n\n            if line_data[1] == 'TYPE_GYROSCOPE':\n                gyro.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n                continue\n\n            if line_data[1] == 'TYPE_GYROSCOPE_UNCALIBRATED':\n                if len(line_data) > 5:\n                    gyro_uncali.append([int(line_data[0]),\n                                        float(line_data[2]), # x\n                                        float(line_data[3]), # y\n                                        float(line_data[4]), # z\n                                        float(line_data[5]), # x\n                                        float(line_data[6]), # y\n                                        float(line_data[7]), # z\n                                        int(  line_data[8]), # accuracy\n                                        ])\n                else:\n                    gyro_uncali.append([int(line_data[0]),\n                                        float(line_data[2]), # x\n                                        float(line_data[3]), # y\n                                        float(line_data[4]), # z\n                                        ])\n                continue\n\n            if line_data[1] == 'TYPE_MAGNETIC_FIELD':\n                magn.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n                continue\n\n            if line_data[1] == 'TYPE_MAGNETIC_FIELD_UNCALIBRATED':\n                if len(line_data) > 5:\n                    magn_uncali.append([int(line_data[0]),\n                                        float(line_data[2]), # x\n                                        float(line_data[3]), # y\n                                        float(line_data[4]), # z\n                                        float(line_data[5]), # x\n                                        float(line_data[6]), # y\n                                        float(line_data[7]), # z\n                                        int(  line_data[8]), # accuracy\n                                        ])\n                else:\n                    magn_uncali.append([int(line_data[0]),\n                                        float(line_data[2]), # x\n                                        float(line_data[3]), # y\n                                        float(line_data[4]), # z\n                                        ])\n                continue\n\n            if line_data[1] == 'TYPE_ROTATION_VECTOR':\n                ahrs.append([int(line_data[0]), float(line_data[2]), float(line_data[3]), float(line_data[4])])\n                continue\n\n            if line_data[1] == 'TYPE_WIFI':\n                sys_ts = line_data[0]\n                ssid = line_data[2]\n                bssid = line_data[3]\n                rssi = line_data[4]\n                lastseen_ts = line_data[6]\n                wifi_data = [sys_ts, ssid, bssid, rssi, lastseen_ts]\n                wifi.append(wifi_data)\n                continue\n\n            if line_data[1] == 'TYPE_BEACON':\n                ts = line_data[0]\n                uuid = line_data[2]\n                major = line_data[3]\n                minor = line_data[4]\n                rssi = line_data[6]\n                if is_test:\n                    real_ts = line_data[9]\n                    ibeacon_data = [ts, '_'.join([uuid, major, minor]), rssi, real_ts]\n                else:\n                    ibeacon_data = [ts, '_'.join([uuid, major, minor]), rssi]\n                ibeacon.append(ibeacon_data)\n                continue\n\n            if line_data[1] == 'TYPE_WAYPOINT':\n                waypoint.append([int(line_data[0]), float(line_data[2]), float(line_data[3])])\n                continue\n\n    acce = np.array(acce)\n    acce_uncali = np.array(acce_uncali)\n    gyro = np.array(gyro)\n    gyro_uncali = np.array(gyro_uncali)\n    magn = np.array(magn)\n    magn_uncali = np.array(magn_uncali)\n    ahrs = np.array(ahrs)\n    wifi = np.array(wifi)\n    ibeacon = np.array(ibeacon)\n    waypoint = np.array(waypoint)\n\n    return ReadData(startTime, endTime, acce, acce_uncali, gyro, gyro_uncali, magn, magn_uncali, ahrs, wifi, ibeacon, waypoint)\n\ndef split_ts_seq(ts_seq, sep_ts):\n    \"\"\"\n\n    :param ts_seq:\n    :param sep_ts:\n    :return:\n    \"\"\"\n    tss = ts_seq[:, 0].astype(float)\n    unique_sep_ts = np.unique(sep_ts)\n    ts_seqs = []\n    start_index = 0\n    for i in range(0, unique_sep_ts.shape[0]):\n        end_index = np.searchsorted(tss, unique_sep_ts[i], side='right')\n        if start_index == end_index:\n            continue\n        ts_seqs.append(ts_seq[start_index:end_index, :].copy())\n        start_index = end_index\n\n    # tail data\n    if start_index < ts_seq.shape[0]:\n        ts_seqs.append(ts_seq[start_index:, :].copy())\n\n    return ts_seqs\n\n\ndef correct_trajectory(original_xys, end_xy):\n    \"\"\"\n\n    :param original_xys: numpy ndarray, shape(N, 2)\n    :param end_xy: numpy ndarray, shape(1, 2)\n    :return:\n    \"\"\"\n    corrected_xys = np.zeros((0, 2))\n\n    A = original_xys[0, :]\n    B = end_xy\n    Bp = original_xys[-1, :]\n\n    angle_BAX = np.arctan2(B[1] - A[1], B[0] - A[0])\n    angle_BpAX = np.arctan2(Bp[1] - A[1], Bp[0] - A[0])\n    angle_BpAB = angle_BpAX - angle_BAX\n    AB = np.sqrt(np.sum((B - A) ** 2))\n    ABp = np.sqrt(np.sum((Bp - A) ** 2))\n\n    corrected_xys = np.append(corrected_xys, [A], 0)\n    for i in np.arange(1, np.size(original_xys, 0)):\n        angle_CpAX = np.arctan2(original_xys[i, 1] - A[1], original_xys[i, 0] - A[0])\n\n        angle_CAX = angle_CpAX - angle_BpAB\n\n        ACp = np.sqrt(np.sum((original_xys[i, :] - A) ** 2))\n\n        AC = ACp * AB \/ ABp\n\n        delta_C = np.array([AC * np.cos(angle_CAX), AC * np.sin(angle_CAX)])\n\n        C = delta_C + A\n\n        corrected_xys = np.append(corrected_xys, [C], 0)\n\n    return corrected_xys\n\n\ndef correct_positions(rel_positions, reference_positions):\n    \"\"\"\n\n    :param rel_positions:\n    :param reference_positions:\n    :return:\n    \"\"\"\n    rel_positions_list = split_ts_seq(rel_positions, reference_positions[:, 0])\n    if len(rel_positions_list) != reference_positions.shape[0] - 1:\n        # print(f'Rel positions list size: {len(rel_positions_list)}, ref positions size: {reference_positions.shape[0]}')\n        del rel_positions_list[-1]\n    assert len(rel_positions_list) == reference_positions.shape[0] - 1\n\n    corrected_positions = np.zeros((0, 3))\n    for i, rel_ps in enumerate(rel_positions_list):\n        start_position = reference_positions[i]\n        end_position = reference_positions[i + 1]\n        abs_ps = np.zeros(rel_ps.shape)\n        abs_ps[:, 0] = rel_ps[:, 0]\n        # abs_ps[:, 1:3] = rel_ps[:, 1:3] + start_position[1:3]\n        abs_ps[0, 1:3] = rel_ps[0, 1:3] + start_position[1:3]\n        for j in range(1, rel_ps.shape[0]):\n            abs_ps[j, 1:3] = abs_ps[j-1, 1:3] + rel_ps[j, 1:3]\n        abs_ps = np.insert(abs_ps, 0, start_position, axis=0)\n        corrected_xys = correct_trajectory(abs_ps[:, 1:3], end_position[1:3])\n        corrected_ps = np.column_stack((abs_ps[:, 0], corrected_xys))\n        if i == 0:\n            corrected_positions = np.append(corrected_positions, corrected_ps, axis=0)\n        else:\n            corrected_positions = np.append(corrected_positions, corrected_ps[1:], axis=0)\n\n    corrected_positions = np.array(corrected_positions)\n\n    return corrected_positions\n\n\ndef init_parameters_filter(sample_freq, warmup_data, cut_off_freq=2):\n    order = 4\n    filter_b, filter_a = signal.butter(order, cut_off_freq \/ (sample_freq \/ 2), 'low', False)\n    zf = signal.lfilter_zi(filter_b, filter_a)\n    _, zf = signal.lfilter(filter_b, filter_a, warmup_data, zi=zf)\n    _, filter_zf = signal.lfilter(filter_b, filter_a, warmup_data, zi=zf)\n\n    return filter_b, filter_a, filter_zf\n\n\ndef get_rotation_matrix_from_vector(rotation_vector):\n    q1 = rotation_vector[0]\n    q2 = rotation_vector[1]\n    q3 = rotation_vector[2]\n\n    if rotation_vector.size >= 4:\n        q0 = rotation_vector[3]\n    else:\n        q0 = 1 - q1*q1 - q2*q2 - q3*q3\n        if q0 > 0:\n            q0 = np.sqrt(q0)\n        else:\n            q0 = 0\n\n    sq_q1 = 2 * q1 * q1\n    sq_q2 = 2 * q2 * q2\n    sq_q3 = 2 * q3 * q3\n    q1_q2 = 2 * q1 * q2\n    q3_q0 = 2 * q3 * q0\n    q1_q3 = 2 * q1 * q3\n    q2_q0 = 2 * q2 * q0\n    q2_q3 = 2 * q2 * q3\n    q1_q0 = 2 * q1 * q0\n\n    R = np.zeros((9,))\n    if R.size == 9:\n        R[0] = 1 - sq_q2 - sq_q3\n        R[1] = q1_q2 - q3_q0\n        R[2] = q1_q3 + q2_q0\n\n        R[3] = q1_q2 + q3_q0\n        R[4] = 1 - sq_q1 - sq_q3\n        R[5] = q2_q3 - q1_q0\n\n        R[6] = q1_q3 - q2_q0\n        R[7] = q2_q3 + q1_q0\n        R[8] = 1 - sq_q1 - sq_q2\n\n        R = np.reshape(R, (3, 3))\n    elif R.size == 16:\n        R[0] = 1 - sq_q2 - sq_q3\n        R[1] = q1_q2 - q3_q0\n        R[2] = q1_q3 + q2_q0\n        R[3] = 0.0\n\n        R[4] = q1_q2 + q3_q0\n        R[5] = 1 - sq_q1 - sq_q3\n        R[6] = q2_q3 - q1_q0\n        R[7] = 0.0\n\n        R[8] = q1_q3 - q2_q0\n        R[9] = q2_q3 + q1_q0\n        R[10] = 1 - sq_q1 - sq_q2\n        R[11] = 0.0\n\n        R[12] = R[13] = R[14] = 0.0\n        R[15] = 1.0\n\n        R = np.reshape(R, (4, 4))\n\n    return R\n\n\ndef get_orientation(R):\n    flat_R = R.flatten()\n    values = np.zeros((3,))\n    if np.size(flat_R) == 9:\n        values[0] = np.arctan2(flat_R[1], flat_R[4])\n        values[1] = np.arcsin(-flat_R[7])\n        values[2] = np.arctan2(-flat_R[6], flat_R[8])\n    else:\n        values[0] = np.arctan2(flat_R[1], flat_R[5])\n        values[1] = np.arcsin(-flat_R[9])\n        values[2] = np.arctan2(-flat_R[8], flat_R[10])\n\n    return values\n\n\ndef compute_steps(acce_datas):\n    step_timestamps = np.array([])\n    step_indexs = np.array([], dtype=int)\n    step_acce_max_mins = np.zeros((0, 4))\n    sample_freq = 50\n    window_size = 22\n    low_acce_mag = 0.6\n    step_criterion = 1\n    interval_threshold = 250\n\n    acce_max = np.zeros((2,))\n    acce_min = np.zeros((2,))\n    acce_binarys = np.zeros((window_size,), dtype=int)\n    acce_mag_pre = 0\n    state_flag = 0\n\n    warmup_data = np.ones((window_size,)) * 9.81\n    filter_b, filter_a, filter_zf = init_parameters_filter(sample_freq, warmup_data)\n    acce_mag_window = np.zeros((window_size, 1))\n\n    # detect steps according to acceleration magnitudes\n    for i in np.arange(0, np.size(acce_datas, 0)):\n        acce_data = acce_datas[i, :]\n        acce_mag = np.sqrt(np.sum(acce_data[1:] ** 2))\n\n        acce_mag_filt, filter_zf = signal.lfilter(filter_b, filter_a, [acce_mag], zi=filter_zf)\n        acce_mag_filt = acce_mag_filt[0]\n\n        acce_mag_window = np.append(acce_mag_window, [acce_mag_filt])\n        acce_mag_window = np.delete(acce_mag_window, 0)\n        mean_gravity = np.mean(acce_mag_window)\n        acce_std = np.std(acce_mag_window)\n        mag_threshold = np.max([low_acce_mag, 0.4 * acce_std])\n\n        # detect valid peak or valley of acceleration magnitudes\n        acce_mag_filt_detrend = acce_mag_filt - mean_gravity\n        if acce_mag_filt_detrend > np.max([acce_mag_pre, mag_threshold]):\n            # peak\n            acce_binarys = np.append(acce_binarys, [1])\n            acce_binarys = np.delete(acce_binarys, 0)\n        elif acce_mag_filt_detrend < np.min([acce_mag_pre, -mag_threshold]):\n            # valley\n            acce_binarys = np.append(acce_binarys, [-1])\n            acce_binarys = np.delete(acce_binarys, 0)\n        else:\n            # between peak and valley\n            acce_binarys = np.append(acce_binarys, [0])\n            acce_binarys = np.delete(acce_binarys, 0)\n\n        if (acce_binarys[-1] == 0) and (acce_binarys[-2] == 1):\n            if state_flag == 0:\n                acce_max[:] = acce_data[0], acce_mag_filt\n                state_flag = 1\n            elif (state_flag == 1) and ((acce_data[0] - acce_max[0]) <= interval_threshold) and (\n                    acce_mag_filt > acce_max[1]):\n                acce_max[:] = acce_data[0], acce_mag_filt\n            elif (state_flag == 2) and ((acce_data[0] - acce_max[0]) > interval_threshold):\n                acce_max[:] = acce_data[0], acce_mag_filt\n                state_flag = 1\n\n        # choose reasonable step criterion and check if there is a valid step\n        # save step acceleration data: step_acce_max_mins = [timestamp, max, min, variance]\n        step_flag = False\n        if step_criterion == 2:\n            if (acce_binarys[-1] == -1) and ((acce_binarys[-2] == 1) or (acce_binarys[-2] == 0)):\n                step_flag = True\n        elif step_criterion == 3:\n            if (acce_binarys[-1] == -1) and (acce_binarys[-2] == 0) and (np.sum(acce_binarys[:-2]) > 1):\n                step_flag = True\n        else:\n            if (acce_binarys[-1] == 0) and acce_binarys[-2] == -1:\n                if (state_flag == 1) and ((acce_data[0] - acce_min[0]) > interval_threshold):\n                    acce_min[:] = acce_data[0], acce_mag_filt\n                    state_flag = 2\n                    step_flag = True\n                elif (state_flag == 2) and ((acce_data[0] - acce_min[0]) <= interval_threshold) and (\n                        acce_mag_filt < acce_min[1]):\n                    acce_min[:] = acce_data[0], acce_mag_filt\n        if step_flag:\n            step_timestamps = np.append(step_timestamps, acce_data[0])\n            step_indexs = np.append(step_indexs, [i])\n            step_acce_max_mins = np.append(step_acce_max_mins,\n                                           [[acce_data[0], acce_max[1], acce_min[1], acce_std ** 2]], axis=0)\n        acce_mag_pre = acce_mag_filt_detrend\n\n    return step_timestamps, step_indexs, step_acce_max_mins\n\n\ndef compute_stride_length(step_acce_max_mins):\n    K = 0.4\n    K_max = 0.8\n    K_min = 0.4\n    para_a0 = 0.21468084\n    para_a1 = 0.09154517\n    para_a2 = 0.02301998\n\n    stride_lengths = np.zeros((step_acce_max_mins.shape[0], 2))\n    k_real = np.zeros((step_acce_max_mins.shape[0], 2))\n    step_timeperiod = np.zeros((step_acce_max_mins.shape[0] - 1, ))\n    stride_lengths[:, 0] = step_acce_max_mins[:, 0]\n    window_size = 2\n    step_timeperiod_temp = np.zeros((0, ))\n\n    # calculate every step period - step_timeperiod unit: second\n    for i in range(0, step_timeperiod.shape[0]):\n        step_timeperiod_data = (step_acce_max_mins[i + 1, 0] - step_acce_max_mins[i, 0]) \/ 1000\n        step_timeperiod_temp = np.append(step_timeperiod_temp, [step_timeperiod_data])\n        if step_timeperiod_temp.shape[0] > window_size:\n            step_timeperiod_temp = np.delete(step_timeperiod_temp, [0])\n        step_timeperiod[i] = np.sum(step_timeperiod_temp) \/ step_timeperiod_temp.shape[0]\n\n    # calculate parameters by step period and acceleration magnitude variance\n    k_real[:, 0] = step_acce_max_mins[:, 0]\n    k_real[0, 1] = K\n    for i in range(0, step_timeperiod.shape[0]):\n        k_real[i + 1, 1] = np.max([(para_a0 + para_a1 \/ step_timeperiod[i] + para_a2 * step_acce_max_mins[i, 3]), K_min])\n        k_real[i + 1, 1] = np.min([k_real[i + 1, 1], K_max]) * (K \/ K_min)\n\n    # calculate every stride length by parameters and max and min data of acceleration magnitude\n    stride_lengths[:, 1] = np.max([(step_acce_max_mins[:, 1] - step_acce_max_mins[:, 2]),\n                                   np.ones((step_acce_max_mins.shape[0], ))], axis=0)**(1 \/ 4) * k_real[:, 1]\n\n    return stride_lengths\n\n\ndef compute_headings(ahrs_datas):\n    headings = np.zeros((np.size(ahrs_datas, 0), 2))\n    for i in np.arange(0, np.size(ahrs_datas, 0)):\n        ahrs_data = ahrs_datas[i, :]\n        rot_mat = get_rotation_matrix_from_vector(ahrs_data[1:])\n        azimuth, pitch, roll = get_orientation(rot_mat)\n        around_z = (-azimuth) % (2 * np.pi)\n        headings[i, :] = ahrs_data[0], around_z\n    return headings\n\n\ndef compute_step_heading(step_timestamps, headings):\n    step_headings = np.zeros((len(step_timestamps), 2))\n    step_timestamps_index = 0\n    for i in range(0, len(headings)):\n        if step_timestamps_index < len(step_timestamps):\n            if headings[i, 0] == step_timestamps[step_timestamps_index]:\n                step_headings[step_timestamps_index, :] = headings[i, :]\n                step_timestamps_index += 1\n        else:\n            break\n    assert step_timestamps_index == len(step_timestamps)\n\n    return step_headings\n\n\ndef compute_rel_positions(stride_lengths, step_headings):\n    rel_positions = np.zeros((stride_lengths.shape[0], 3))\n    for i in range(0, stride_lengths.shape[0]):\n        rel_positions[i, 0] = stride_lengths[i, 0]\n        rel_positions[i, 1] = -stride_lengths[i, 1] * np.sin(step_headings[i, 1])\n        rel_positions[i, 2] = stride_lengths[i, 1] * np.cos(step_headings[i, 1])\n\n    return rel_positions\n\ndef compute_step_prediction(example):\n    T_ref   = example.waypoint[:, 0]\n    xy_true = example.waypoint[:, 1:]\n    delta_xy_true = np.diff(xy_true, axis=0)\n        \n    acce_datas = example.acce\n    ahrs_datas = example.ahrs\n    step_timestamps, step_indexs, step_acce_max_mins = compute_steps(acce_datas)\n    headings = compute_headings(ahrs_datas)\n    stride_lengths = compute_stride_length(step_acce_max_mins)\n    step_headings  = compute_step_heading(step_timestamps, headings)\n    rel_positions  = compute_rel_positions(stride_lengths, step_headings)\n    if T_ref[-1] > rel_positions[-1, 0]:\n        rel_positions = [np.array([[0, 0, 0]]), rel_positions, np.array([[T_ref[-1], 0, 0]])]\n    else:\n        rel_positions = [np.array([[0, 0, 0]]), rel_positions]\n    rel_positions = np.concatenate(rel_positions)\n    T_rel = rel_positions[:, 0]\n    delta_xy_pred = np.diff(interp1d(T_rel, np.cumsum(rel_positions[:, 1:], axis=0), axis=0)(T_ref), axis=0)\n    \n    return delta_xy_true, delta_xy_pred\n\ndef compute_step_prediction_error(filename):\n    example = read_data_file(filename)\n    delta_xy_true, delta_xy_pred = compute_step_prediction(example)\n    delta_xy_err = delta_xy_pred - delta_xy_true\n    err_distance = np.sqrt(np.sum(delta_xy_err**2, axis=1))\n    return err_distance\n\ndef visualize_trajectory(waypoint,\n                         trajectries,\n                         floor_plan_filename,\n                         width_meter,\n                         height_meter,\n                         radius=6.0):\n    fig = go.Figure()\n\n    fig.add_trace(\n        go.Scattergl(\n            x=waypoint[:, 0],\n            y=waypoint[:, 1],\n            mode='markers',\n            marker=dict(size=6, color='blue', symbol='circle'),\n            name='true waypoint',\n        ))\n\n    for i in range(waypoint.shape[0]):\n        x0 = waypoint[i, 0] - radius\n        y0 = waypoint[i, 1] - radius\n        x1 = waypoint[i, 0] + radius\n        y1 = waypoint[i, 1] + radius\n        fig.add_shape(\n            type=\"circle\",\n            xref=\"x\", yref=\"y\",\n            x0=x0, y0=y0, x1=x1, y1=y1,\n            line_color=\"LightSeaGreen\",\n        )\n\n    for index, trajectory in enumerate(trajectries):\n        fig.add_trace(\n            go.Scattergl(\n                x=trajectory[:, 0],\n                y=trajectory[:, 1],\n                mode='lines + markers',\n                line=dict(shape='linear', color='green', width=1, dash='solid'),\n                name=f'trajectory_{index}',\n            ))\n\n    # add floor plan\n    floor_plan = Image.open(floor_plan_filename)\n    fig.update_layout(images=[\n        go.layout.Image(\n            source=floor_plan,\n            xref=\"x\",\n            yref=\"y\",\n            x=0,\n            y=height_meter,\n            sizex=width_meter,\n            sizey=height_meter,\n            sizing=\"contain\",\n            opacity=1,\n            layer=\"below\",\n        )\n    ])\n\n    # configure\n    fig.update_xaxes(autorange=False, range=[0, width_meter])\n    fig.update_yaxes(autorange=False, range=[0, height_meter], scaleanchor=\"x\", scaleratio=1)\n    fig.update_layout(\n        autosize=True,\n        width=900,\n        height=50 + 900 * height_meter \/ width_meter,\n        template=\"plotly_white\",\n        margin=dict(\n            l=0,\n            r=0,\n            b=0,\n            t=0,\n            pad=0\n        ),\n    )\n    return fig","adca427b":"sub = pd.read_csv(f'{INPUT_PATH}\/sample_submission.csv')\ntmp = sub['site_path_timestamp'].apply(lambda x: pd.Series(x.split('_')))\nsites = np.unique(tmp[0])\nprocesses = multiprocessing.cpu_count()\nERR = []\nfor site in sites:\n    filelist = glob.glob(f'{INPUT_PATH}\/train\/{site}\/*\/*.txt')\n    with multiprocessing.Pool(processes=processes) as pool:\n        err = pool.imap_unordered(compute_step_prediction_error, filelist)\n        # err = tqdm(err)\n        err = list(err)\n    ERR.extend(err)\nERR = np.concatenate(ERR, axis=0)\nERR_mean = np.mean(ERR)\nplt.figure(figsize=(6, 4))\nplt.hist(ERR, bins=100)\nplt.grid(True)\nplt.xlim([0, 30])\nplt.ylim([0, 6000])\nplt.plot([ERR_mean, ERR_mean], [0, 6000], color='red')\nplt.xlabel('prediction error [m]')\nplt.show()","8df7587e":"site  = \"5cd56865eb294480de7167b6\"\nfloor = \"F2\"\npath  = \"5cfdd9006fa436000a02de9e\"\n\nfloor_plan_filename = f'{INPUT_PATH}\/metadata\/{site}\/{floor}\/floor_image.png'\nfloor_json_filename = f'{INPUT_PATH}\/metadata\/{site}\/{floor}\/floor_info.json'\npath_filename       = f'{INPUT_PATH}\/train\/{site}\/{floor}\/{path}.txt'\n\nwith open(floor_json_filename) as f:\n    json_data = json.load(f)\nwidth_meter  = json_data['map_info']['width']\nheight_meter = json_data['map_info']['height']\n\nexample = read_data_file(path_filename)\nwaypoint = example.waypoint[:, 1:]\n_, delta_xy_pred = compute_step_prediction(example)\n\ntrajectries_1 = []\nfor i in range(delta_xy_pred.shape[0]):\n    trajectry = waypoint[i, :] + np.concatenate([np.zeros((1, 2)), delta_xy_pred[i:i+1, :]])\n    trajectries_1.append(trajectry)\nfig_1 = visualize_trajectory(\n    waypoint,\n    trajectries_1,\n    floor_plan_filename,\n    width_meter,\n    height_meter,\n)\nfig_1.write_image('trajectries_1.png')\n\ntrajectries_2 = [waypoint[0, :] + np.cumsum(np.concatenate([np.zeros((1, 2)), delta_xy_pred]), axis=0)]\nfig_2 = visualize_trajectory(\n    waypoint,\n    trajectries_2,\n    floor_plan_filename,\n    width_meter,\n    height_meter,\n)\nfig_2.write_image('trajectries_2.png')\n\nfig = plt.figure(figsize=(16.0, 8.0))\nax1 = fig.add_subplot(1, 2, 1)\nax1.imshow(np.array(Image.open('trajectries_1.png')))\nax1.tick_params(labelbottom=False,\n                labelleft=False,\n                labelright=False,\n                labeltop=False)\nax1.spines['right'].set_visible(False)\nax1.spines['left'].set_visible(False)\nax1.spines['top'].set_visible(False)\nax1.spines['bottom'].set_visible(False)\nax1.tick_params('x', length=0, which='major')\nax1.tick_params('y', length=0, which='major')\n\nax2 = fig.add_subplot(1, 2, 2)\nax2.imshow(np.array(Image.open('trajectries_2.png')))\nax2.tick_params(labelbottom=False,\n                labelleft=False,\n                labelright=False,\n                labeltop=False)\nax2.spines['right'].set_visible(False)\nax2.spines['left'].set_visible(False)\nax2.spines['top'].set_visible(False)\nax2.spines['bottom'].set_visible(False)\nax2.tick_params('x', length=0, which='major')\nax2.tick_params('y', length=0, which='major')\n\nfig.tight_layout()\nplt.show()","7090dbaa":"## Introduction\n\nThis notebook provides background ideas of [the cost minimization notebook](https:\/\/www.kaggle.com\/saitodevel01\/indoor-post-processing-by-cost-minimization).\nHonestly, I was a little wondering about the reaction of the notebook.\nThis method is equivalent to the Kalman smoother under a probabilistic model, neither magic nor trick.\nBut formulation as an optimization problem is more flexible, it is easy to combine other heuristics such that\n[\"Snap to Grid\"](https:\/\/www.kaggle.com\/robikscube\/indoor-navigation-snap-to-grid-post-processing).\nAnd, understanding the probabilistic model behind the cost function allows you to improve post-processing method.","55501486":"## Problem Formulation as Linear Dynamical System\n\nLinear Dynamical System (LDS) or Linear Gaussian State Space Model (LGSSM) is defined by following equation and graphical model.\n$$\n\\begin{array}{ll}\nz_n = A z_{n-1} + w_n & w_n \\sim \\mathcal{N}(w_n \\mid 0, \\Gamma) \\\\\nx_n = C z_n     + v_n & v_n \\sim \\mathcal{N}(v_n \\mid 0, \\Sigma)\n\\end{array}\n$$\nwhere, $z$ is the latent variable and $x$ is the observed variable.\n$A, C, \\Gamma, \\Sigma$ are parameters of LDS.\n\n![graphical_model.png](attachment:12eebeca-d63c-4b8c-96ad-63f9f9296fa7.png)\n\nThe initial distribution of the latent variable is also given as a gaussian distribution.\n$$\np(z_1) = \\mathcal{N}(z_1 \\mid \\mu_0, P_0)\n$$\n$P_0^{-1} = 0$ means that there is no prior information.\n\nThere is another variant of LDS with observable external input $u$.\n$$\n\\begin{array}{ll}\nz_n = A z_{n-1} + B u_n + w_n & w_n \\sim \\mathcal{N}(w_n \\mid 0, \\Gamma) \\\\\nx_n = C z_n     + D u_n + v_n & v_n \\sim \\mathcal{N}(v_n \\mid 0, \\Sigma)\n\\end{array}\n$$\n\nFor this competition, transition process between waypoints can be modeled as the following LDS,\n$$\n\\begin{array}{ll}\nz_n = z_{n-1} + \\Delta z_n + w_n & w_n \\sim \\mathcal{N}(w_n \\mid 0, \\Gamma) \\\\\nx_n = z_n     + v_n              & v_n \\sim \\mathcal{N}(v_n \\mid 0, \\Sigma)\n\\end{array}\n$$\nTherefore, true waypoint position (the latent variable of LDS) can be estimated by inference of LDS called the Kalman filter and smoother.\n\n| symbol | description | |\n|---|---|:---:|\n| $z_n$        | true waypoint position | (unknown) | \n| $\\Delta z_n$ | relative position prediction of sensor model between two waypoints | (known) |\n| $w_n$        | prediction error of sensor model | (unknown) |\n| $x_n$        | prediction of wifi model | (known) |\n| $v_n$        | prediction error of wifi model | (unknown) |","5ec5b57c":"Caption: Visualizations of predictions based on $\\Delta \\hat{X}$ defined in [my notebook](https:\/\/www.kaggle.com\/saitodevel01\/indoor-post-processing-by-cost-minimization).\n(Left) Single step prediction. (Right) Multiple step prediction.\nThe radius of circles centered on waypoints is 6m.","73ca5754":"## Parameters of LDS\n\n$\\Gamma$ and $\\Sigma$ are the parameters of the above LDS model.\nThe definition of these parameters is the variance of the prediction error.\n$$\n\\Gamma = \\mathbb{E}[w_n w_n^T] \\\\\n\\Sigma = \\mathbb{E}[v_n v_n^T]\n$$\nThese parameters determine which of two models, wifi and sensor, to prioritize.\n\nAnd treating these parameters as constant values doesn't lead to the best results\nbecause the variance of wifi models is greatly depending on paths and timestamps.\nI think proper error estimation of machine learning models is another key technique to capture this competition.","67f4d109":"## Inference of LDS\n\nThe recurrence formula of the Kalman filter and smoother is given by following equations.\n(See [the Bishop's PRML book](https:\/\/www.microsoft.com\/en-us\/research\/people\/cmbishop\/prml-book\/) for derivation).\n\n#### Kalman filter\n$$\np(z_n \\mid x_{1:n}) = \\mathcal{N}(z_n \\mid \\mu_n, V_n) \\\\\np(z_{n+1} \\mid x_{1:n}) = \\mathcal{N}(z_{n+1} \\mid A \\mu_n, P_n)\n$$\n\n$$\nK_n   = P_{n-1} C^T (C P_{n-1} C^T + \\Sigma)^{-1} \\\\\n\\mu_n = A \\mu_{n-1} + K_n (x_n - C A \\mu_{n-1}) \\quad (n \\neq 1) \\\\\nV_n   = (I - K_n C) P_{n-1} \\\\\nP_n   = A V_n A^T + \\Gamma  \\\\\n\\mu_1 = \\mu_0 + K_1 (x_1 - C \\mu_0)\n$$\n\n#### Kalman smoother (Rauch-Tung-Striebel smoother)\n$$\np(z_n \\mid x_{1:N}) = \\mathcal{N}(z_n \\mid \\hat{\\mu}_n, \\hat{V}_n) \n$$\n\n$$\nJ_n = V_n A^T (P_n)^{-1} \\\\\n\\hat{\\mu}_n = \\mu_n + J_n (\\hat{\\mu}_{n+1} - A \\mu_n) \\quad (n \\neq N) \\\\\n\\hat{V}_n   = V_n + J_n (\\hat{V}_{n+1} - P_n) J_n^T   \\quad (n \\neq N) \\\\\n\\hat{\\mu}_N = \\mu_N \\\\\n\\hat{V}_N   = V_N\n$$\n\nThe distribution of the latent variable obtained by the Kalman smoother is the posterior distribution with all observation (including future information).\n\nAnd, the joint distribution of the latent variables can also be expressed by Bayes' theorem as follows,\n$$\np(z_1, \\cdots, z_N \\mid x_1, \\cdots, x_N) \\propto p(z_1, \\cdots, z_N, x_1, \\cdots, x_N) \\\\\n= p(z_1) p(x_1 \\mid z_1) \\prod_{n=2}^{N} p(z_n \\mid z_{n-1}) p(x_n \\mid z_n)\n$$\n\n$$\n\\ln p(z_1, \\cdots, z_N \\mid x_1, \\cdots, x_N)\n= - \\frac{1}{2} \\left\\{\n(z_1 - \\mu_0)^T P_0^{-1} (z_1 - \\mu_0) + (x_1 - C z_1)^T \\Sigma^{-1} (x_1 - C z_1) + \n\\sum_{n=2}^{N} \\left[ (z_n - A z_{n-1})^T \\Gamma^{-1} (z_n - A z_{n-1}) + (x_n - C z_n)^T \\Sigma^{-1} (x_n - C z_n) \\right]\n\\right\\} + \\text{const.}\n$$\n\nSince the joint distribution of the latent variables is a multivariate gaussian distribution,\nthe logarithm of the probability density function is quadratic with respect to the latent variables.\nTherefore, the logarithm of the probability density function can be written as\n$$\n\\ln p(z_1, \\cdots, z_N \\mid x_1, \\cdots, x_N) = - \\frac{1}{2} (Z - Z^{*})^T Q (Z - Z^{*}) + \\text{const.}\n$$\n\nHere, $Z^{*}$ is the posterior mean of the joint distribution, and $Z^{*}$ is obtained by minimizing the following quadratic function.\n$$\nL(z_1, \\cdots, z_N) = (z_1 - \\mu_0)^T P_0^{-1} (z_1 - \\mu_0) + (x_1 - C z_1)^T \\Sigma^{-1} (x_1 - C z_1) + \n\\sum_{n=2}^{N} \\left[ (z_n - A z_{n-1})^T \\Gamma^{-1} (z_n - A z_{n-1}) + (x_n - C z_n)^T \\Sigma^{-1} (x_n - C z_n) \\right]\n$$\n\nFor this competition, by assuming $P_0^{-1} = 0$, the logarithm of the joint distribution is expressed as follows,\n$$\n\\ln p(z_1, \\cdots, z_N \\mid x_1, \\cdots, x_N, \\Delta z_2, \\cdots, \\Delta z_N) = - \\dfrac{1}{2} \\left\\{\n(x_1 - z_1)^T \\Sigma^{-1} (x_1 - z_1) + \\sum_{n=2}^{N} \\left[ (z_n - (z_{n-1} + \\Delta z_n))^T \\Gamma^{-1} (z_n - (z_{n-1} + \\Delta z_n)) + (x_n - z_n)^T \\Sigma^{-1} (x_n - z_n) \\right]\n\\right\\} + \\text{const.}\n$$\n\nThe advantage of using Kalman filter and smoother is that the computational complexity of inference is reduced from cubic to linear with respect to length of the sequence ($N$).\nHowever, by using a sparse matrix solver, it is possible to obtain linear order complexity even with the direct method.\n[The cost minimization notebook](https:\/\/www.kaggle.com\/saitodevel01\/indoor-post-processing-by-cost-minimization) solves the posterior mean directly.","bbd68cc8":"## Prediction Accuracy: Wi-Fi vs Sensors\n\nIn the early stage of the competition, the accuracy of the wifi model of the public notebooks was about 8m.\nAnd, in the late stage of the competition, [this discussion](https:\/\/www.kaggle.com\/c\/indoor-location-navigation\/discussion\/232995)\nsuggests that the accuracy of top participants' wifi models (without post-processing) is about 5 to 6m.\nOn the other hand, predicting distances between waypoints using sensor data is much more accurate than pure wifi model.\nThis figure shows a histogram of the prediction error of $\\Delta \\hat{X}$\ndefined in [my notebook](https:\/\/www.kaggle.com\/saitodevel01\/indoor-post-processing-by-cost-minimization).\nThe average error is about 2.7m, and you can get much better results using machine learning.\nThis suggests that, if the position of the previous waypoint is known,\npredicting the position of the next waypoint from sensor data is more accurate than predicting from wifi model.\nA problem is, in test-case, any waypoint is not given, so you must predict at least initial position by other method anyway.\nAnd, using only sensor data will result in significantly worse results because the prediction error is accumulated.\n"}}