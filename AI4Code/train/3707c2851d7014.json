{"cell_type":{"68af57ba":"code","1b63bcc9":"code","ba762ad6":"code","7b135c40":"code","3d0b9fe7":"code","844bf295":"code","45844640":"code","85953313":"code","528f4e1a":"code","c1e1b685":"code","6b573031":"code","67e637c4":"code","bf850f91":"code","ba3d9d0a":"code","fab004f3":"code","9d283ff1":"code","6ffa8cd6":"code","fc1fd8ed":"code","4c603b7f":"code","e877696b":"code","f40cd26e":"code","57cfd344":"code","599d0660":"code","5a89408b":"code","d9e90e7c":"code","e42e9a13":"code","3a58dc90":"code","8496b185":"code","55103ac0":"code","ef45615a":"code","cebc6d85":"code","210b75e8":"code","2547c65c":"code","1a5e758d":"code","156bf583":"code","923eddc6":"code","6af90927":"code","e1978ace":"code","d481c2e7":"code","f83ba9a8":"code","849f855b":"code","6486fc31":"code","fade8a4b":"code","798a54f6":"code","285f4770":"code","653a8d9e":"code","b4db9db8":"code","9a240aab":"code","99c0a808":"code","fa894bb3":"code","c7c81de3":"code","0399fe8b":"code","f97555fd":"code","cb3fa321":"code","d90fecc2":"code","75be7a9a":"markdown","3f4b7e5a":"markdown","2e2c5cfa":"markdown","30cfaab8":"markdown","5ee341da":"markdown","d5c24d1c":"markdown","08ec82e8":"markdown","485e80fa":"markdown","e0766b34":"markdown","5fc734cc":"markdown","120930ab":"markdown","e11d31d2":"markdown","f00e2fb8":"markdown","011fb5b7":"markdown","d54cff5f":"markdown","28137fd5":"markdown"},"source":{"68af57ba":"# imports\nimport numpy as np\nimport pandas as pd\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","1b63bcc9":"PATH = '..\/input\/'\nos.listdir(PATH)","ba762ad6":"train = pd.read_csv('..\/input\/train.tsv',sep = '\\t')\ntest = pd.read_csv('..\/input\/test.tsv',sep = '\\t')\nsub = pd.read_csv('..\/input\/sampleSubmission.csv' , sep = ',')","7b135c40":"train.head()","3d0b9fe7":"test.head()","844bf295":"class_count = train['Sentiment'].value_counts()\nclass_count","45844640":"x = np.array(class_count.index)\ny = np.array(class_count.values)\nplt.figure(figsize=(8,5))\nsns.barplot(x,y)\nplt.xlabel('Sentiment ')\nplt.ylabel('Number of reviews ')\n","85953313":"print('Number of sentences in training set:',len(train['SentenceId'].unique()))\nprint('Number of sentences in test set:',len(test['SentenceId'].unique()))\nprint('Average words per sentence in train:',train.groupby('SentenceId')['Phrase'].count().mean())\nprint('Average words per sentence in test:',test.groupby('SentenceId')['Phrase'].count().mean())","528f4e1a":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nstopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='black',\n        stopwords=stopwords,\n        max_words=200,\n        max_font_size=40, \n        scale=3,\n        random_state=1 # chosen at random by flipping a coin; it was heads\n).generate(str(data))\n\n    fig = plt.figure(1, figsize=(15, 15))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","c1e1b685":"show_wordcloud(train['Phrase'],'Most Common Words from the whole corpus')","6b573031":"\nshow_wordcloud(train[train['Sentiment'] == 0]['Phrase'],'Negative Reviews')","67e637c4":"\nshow_wordcloud(train[train['Sentiment'] == 1]['Phrase'],'Somewhat Negative Reviews')\n","bf850f91":"show_wordcloud(train[train['Sentiment'] == 2]['Phrase'],'Neutral Reviews')","ba3d9d0a":"\nshow_wordcloud(train[train['Sentiment'] == 3]['Phrase'],'Somewhat Positive Reviews')","fab004f3":"\nshow_wordcloud(train[train['Sentiment'] == 4]['Phrase'],'Postive Reviews')","9d283ff1":"from nltk.tokenize import TweetTokenizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntokenizer = TweetTokenizer()","6ffa8cd6":"vectorizer = TfidfVectorizer(ngram_range=(1, 3), tokenizer=tokenizer.tokenize)\nfull_text = list(train['Phrase'].values) + list(test['Phrase'].values)\nvectorizer.fit(full_text)\ntrain_vectorized = vectorizer.transform(train['Phrase'])\ntest_vectorized = vectorizer.transform(test['Phrase'])","fc1fd8ed":"y = train['Sentiment']","4c603b7f":"from sklearn.model_selection import train_test_split\nx_train , x_val, y_train , y_val = train_test_split(train_vectorized,y,test_size = 0.2)","e877696b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.multiclass import OneVsRestClassifier","f40cd26e":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score","57cfd344":"lr = LogisticRegression()\novr = OneVsRestClassifier(lr)\novr.fit(x_train,y_train)\nprint(classification_report( ovr.predict(x_val) , y_val))\nprint(accuracy_score( ovr.predict(x_val) , y_val ))","599d0660":"svm = LinearSVC()\nsvm.fit(x_train,y_train)\nprint(classification_report( svm.predict(x_val) , y_val))\nprint(accuracy_score( svm.predict(x_val) , y_val ))","5a89408b":"estimators = [ ('svm',svm) , ('ovr' , ovr) ]\nclf = VotingClassifier(estimators , voting='hard')\nclf.fit(x_train,y_train)\nprint(classification_report( clf.predict(x_val) , y_val))\nprint(accuracy_score( clf.predict(x_val) , y_val ))","d9e90e7c":"from keras.utils import to_categorical\ntarget=train.Sentiment.values\ny=to_categorical(target)\ny","e42e9a13":"max_features = 13000\nmax_words = 50\nbatch_size = 128\nepochs = 3\nnum_classes=5\n","3a58dc90":"from sklearn.model_selection import train_test_split\nX_train , X_val , Y_train , Y_val = train_test_split(train['Phrase'],y,test_size = 0.20)","8496b185":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense,GRU,LSTM,Embedding\nfrom keras.optimizers import Adam\nfrom keras.layers import SpatialDropout1D,Dropout,Bidirectional,Conv1D,GlobalMaxPooling1D,MaxPooling1D,Flatten\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping","55103ac0":"tokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(X_train))\nX_train = tokenizer.texts_to_sequences(X_train)\nX_val = tokenizer.texts_to_sequences(X_val)\n","ef45615a":"X_test = tokenizer.texts_to_sequences(test['Phrase'])\nX_test =pad_sequences(X_test, maxlen=max_words)","cebc6d85":"len(X_test)\n","210b75e8":"X_train =pad_sequences(X_train, maxlen=max_words)\nX_val = pad_sequences(X_val, maxlen=max_words)\nX_test =pad_sequences(X_test, maxlen=max_words)","2547c65c":"model_GRU=Sequential()\nmodel_GRU.add(Embedding(max_features,100,mask_zero=True))\nmodel_GRU.add(GRU(64,dropout=0.4,return_sequences=True))\nmodel_GRU.add(GRU(32,dropout=0.5,return_sequences=False))\nmodel_GRU.add(Dense(num_classes,activation='softmax'))\nmodel_GRU.compile(loss='categorical_crossentropy',optimizer=Adam(lr = 0.001),metrics=['accuracy'])\nmodel_GRU.summary()","1a5e758d":"%%time\nhistory1=model_GRU.fit(X_train, Y_train, validation_data=(X_val, Y_val),epochs=epochs, batch_size=batch_size, verbose=1)","156bf583":"y_pred1=model_GRU.predict_classes(X_test, verbose=1)\nsub.Sentiment=y_pred1\nsub.to_csv('sub1_GRU.csv',index=False)\nsub.head()","923eddc6":"model2_GRU=Sequential()\nmodel2_GRU.add(Embedding(max_features,100,mask_zero=True))\nmodel2_GRU.add(GRU(64,dropout=0.4,return_sequences=True))\nmodel2_GRU.add(GRU(32,dropout=0.5,return_sequences=False))\nmodel2_GRU.add(Dense(num_classes,activation='sigmoid'))\nmodel2_GRU.compile(loss='binary_crossentropy',optimizer=Adam(lr = 0.001),metrics=['accuracy'])\nmodel2_GRU.summary()","6af90927":"%%time\nhistory2=model2_GRU.fit(X_train, Y_train, validation_data=(X_val, Y_val),epochs=epochs, batch_size=batch_size, verbose=1)","e1978ace":"y_pred2=model2_GRU.predict_classes(X_test, verbose=1)\nsub.Sentiment=y_pred2\nsub.to_csv('sub2_GRU.csv',index=False)\nsub.head()","d481c2e7":"model3_LSTM=Sequential()\nmodel3_LSTM.add(Embedding(max_features,100,mask_zero=True))\nmodel3_LSTM.add(LSTM(64,dropout=0.4,return_sequences=True))\nmodel3_LSTM.add(LSTM(32,dropout=0.5,return_sequences=False))\nmodel3_LSTM.add(Dense(num_classes,activation='sigmoid'))\nmodel3_LSTM.compile(loss='binary_crossentropy',optimizer=Adam(lr = 0.001),metrics=['accuracy'])\nmodel3_LSTM.summary()","f83ba9a8":"%%time\nhistory3=model3_LSTM.fit(X_train, Y_train, validation_data=(X_val, Y_val),epochs=epochs, batch_size=batch_size, verbose=1)","849f855b":"y_pred3=model3_LSTM.predict_classes(X_test, verbose=1)\nsub.Sentiment=y_pred3\nsub.to_csv('sub3_LSTM.csv',index=False)\nsub.head()","6486fc31":"model4_BGRU = Sequential()\nmodel4_BGRU.add(Embedding(max_features, 100, input_length=max_words))\nmodel4_BGRU.add(SpatialDropout1D(0.25))\nmodel4_BGRU.add(Bidirectional(GRU(64,dropout=0.4,return_sequences = True)))\nmodel4_BGRU.add(Bidirectional(GRU(32,dropout=0.5,return_sequences = False)))\nmodel4_BGRU.add(Dense(5, activation='sigmoid'))\nmodel4_BGRU.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel4_BGRU.summary()","fade8a4b":"%%time\nhistory4=model4_BGRU.fit(X_train, Y_train, validation_data=(X_val, Y_val),epochs=epochs, batch_size=batch_size, verbose=1)","798a54f6":"y_pred4=model4_BGRU.predict_classes(X_test, verbose=1)\nsub.Sentiment=y_pred4\nsub.to_csv('sub4_BGRU.csv',index=False)\nsub.head()","285f4770":"model5_CNN= Sequential()\nmodel5_CNN.add(Embedding(max_features,100,input_length=max_words))\nmodel5_CNN.add(Dropout(0.2))\nmodel5_CNN.add(Conv1D(64,kernel_size=3,padding='same',activation='relu',strides=1))\nmodel5_CNN.add(GlobalMaxPooling1D())\nmodel5_CNN.add(Dense(128,activation='relu'))\nmodel5_CNN.add(Dropout(0.2))\nmodel5_CNN.add(Dense(num_classes,activation='sigmoid'))\nmodel5_CNN.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel5_CNN.summary()","653a8d9e":"%%time\nearly_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n\nhistory5=model5_CNN.fit(X_train, Y_train, validation_data=(X_val, Y_val),epochs=3, batch_size=batch_size, verbose=1,callbacks = [early_stop])","b4db9db8":"y_pred5=model5_CNN.predict_classes(X_test, verbose=1)\nsub.Sentiment=y_pred5\nsub.to_csv('sub5_CNN.csv',index=False)\nsub.head()","9a240aab":"model6_CnnGRU= Sequential()\nmodel6_CnnGRU.add(Embedding(max_features,100,input_length=max_words))\nmodel6_CnnGRU.add(Conv1D(64,kernel_size=3,padding='same',activation='relu'))\nmodel6_CnnGRU.add(MaxPooling1D(pool_size=2))\nmodel6_CnnGRU.add(Dropout(0.25))\nmodel6_CnnGRU.add(GRU(128,return_sequences=True))\nmodel6_CnnGRU.add(Dropout(0.3))\nmodel6_CnnGRU.add(Flatten())\nmodel6_CnnGRU.add(Dense(128,activation='relu'))\nmodel6_CnnGRU.add(Dropout(0.5))\nmodel6_CnnGRU.add(Dense(5,activation='sigmoid'))\nmodel6_CnnGRU.compile(loss='binary_crossentropy',optimizer=Adam(lr=0.001),metrics=['accuracy'])\nmodel6_CnnGRU.summary()","99c0a808":"%%time\nhistory6=model6_CnnGRU.fit(X_train, Y_train, validation_data=(X_val, Y_val),epochs=3, batch_size=batch_size, verbose=1,callbacks=[early_stop])","fa894bb3":"y_pred6=model6_CnnGRU.predict_classes(X_test, verbose=1)\nsub.Sentiment=y_pred6\nsub.to_csv('sub6_CnnGRU.csv',index=False)\nsub.head()","c7c81de3":"model7_GruCNN = Sequential()\nmodel7_GruCNN.add(Embedding(max_features,100,input_length=max_words))\nmodel7_GruCNN.add(Dropout(0.2))\nmodel7_GruCNN.add(Bidirectional(GRU(units=128 , return_sequences=True)))\nmodel7_GruCNN.add(Conv1D(32 , kernel_size=3 , padding='same' , activation='relu'))\nmodel7_GruCNN.add(GlobalMaxPooling1D())\nmodel7_GruCNN.add(Dense(units = 64 , activation='relu'))\nmodel7_GruCNN.add(Dropout(0.5))\nmodel7_GruCNN.add(Dense(units=5,activation='sigmoid'))\nmodel7_GruCNN.compile(loss='binary_crossentropy' , optimizer = 'adam' , metrics=['accuracy'])\nmodel7_GruCNN.summary()","0399fe8b":"%%time\nhistory7 = model7_GruCNN.fit(X_train, Y_train, validation_data=(X_val, Y_val),epochs=4, batch_size=batch_size, verbose=1,callbacks=[early_stop])","f97555fd":"y_pred7=model7_GruCNN.predict_classes(X_test, verbose=1)\nsub.Sentiment=y_pred7\nsub.to_csv('sub7_GruCNN.csv',index=False)\nsub.head()","cb3fa321":"sub_all=pd.DataFrame({'model1':y_pred1,'model2':y_pred2,'model3':y_pred3,'model4':y_pred4,'model5':y_pred5,'model6':y_pred6,'model7':y_pred7})\npred_mode=sub_all.agg('mode',axis=1)[0].values\nsub_all.head()","d90fecc2":"pred_mode=[int(i) for i in pred_mode]\nsub.Sentiment=pred_mode\nsub.to_csv('ensemble_mode.csv',index=False)\nsub.head()","75be7a9a":"##  Contents\n* [Introduction](#introduction)\n* [EDA](#eda)\n* [Different Machine Learning Models ](#ml)\n  *  [  N-Grams Method ](#N-Grams)\n  *  [GRU model ](#gru)\n  *  [ LSTM model](#lstm)\n  *  [Bidirectional-GRU model](#bgru)\n  *  [ CNN model ](#cnn)\n  *  [ CNN-GRU ](#cgru)\n  *  [GRU-CNN ](#gruc)\n* [Final Ensemble](#en) \n    ","3f4b7e5a":"<a id='bgru'><\/a>\n## <center>Bidirectional-GRU<\/center>","2e2c5cfa":"<a id='introduction'><\/a>\n## <center> Introduction <\/center>\nThe Rotten Tomatoes movie review dataset is a corpus of movie reviews used for sentiment analysis, originally collected by Pang and Lee [1]. In their work on sentiment treebanks, Socher et al. [2] used Amazon's Mechanical Turk to create fine-grained labels for all parsed phrases in the corpus. This competition presents a chance to benchmark your sentiment-analysis ideas on the Rotten Tomatoes dataset. You are asked to label phrases on a scale of five values: negative, somewhat negative, neutral, somewhat positive, positive. Obstacles like sentence negation, sarcasm, terseness, language ambiguity, and many others make this task very challenging.","30cfaab8":"<a id='gru'><\/a>\n## <center>GRU<\/center>","5ee341da":"### The sentiment labels are:\n\n0 - negative\n1 - somewhat negative\n2 - neutral\n3 - somewhat positive\n4 - positive","d5c24d1c":"<a id='lstm'><\/a>\n## <center>LSTM<\/center>","08ec82e8":"<a id='gruc'><\/a>\n## <center>GRU-CNN<\/center>","485e80fa":"<a id='ml'><\/a>\n## <center>  Different Machine Learning Models <\/center>\n","e0766b34":"### Training Logistic Reagression model and an SVM.","5fc734cc":"<a id='cnn'><\/a>\n## <center>CNN<\/center>","120930ab":"<a id='eda'><\/a>\n## <center>EDA<\/center>\nBasic exploration of data to check labels , the number of phrases for each label and average phrase length in the each sentiment.","e11d31d2":"<a id='N-Grams'><\/a>\n##  <center>1.N-Grams<\/center>","f00e2fb8":"<a id='en'><\/a>\n## <center>Ensembling all the predictions<\/center>","011fb5b7":"# <center> Movie Review Sentiment Analysis <\/center>\n## <center> Classify the sentiment of sentences from the Rotten Tomatoes dataset <\/center>","d54cff5f":"### Using Word Clouds to see the higher fequency words from each sentiment","28137fd5":"<a id='cgru'><\/a>\n## <center>CNN-GRU<\/center>"}}