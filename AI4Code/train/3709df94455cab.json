{"cell_type":{"ba3ead8b":"code","585d55c8":"code","867093ef":"code","7620fbb7":"code","1f5ce28b":"code","053020e1":"code","bdbc1fdc":"code","02066607":"code","bb30234b":"code","9b79d135":"code","28e38940":"code","56e767e8":"code","a74f90a4":"markdown","92559fb8":"markdown","d7541e77":"markdown","24ec41c9":"markdown","bc3cbf3e":"markdown","7b7da658":"markdown","d0a761f7":"markdown"},"source":{"ba3ead8b":"import gym\nimport numpy as np","585d55c8":"env = gym.make(\"FrozenLake-v1\")","867093ef":"env.action_space.n","7620fbb7":"env.observation_space","1f5ce28b":"alpha = 0.4","053020e1":"gamma = 0.999","bdbc1fdc":"q_table = dict([(x, [1, 1, 1, 1]) for x in range(16)])","02066607":"q_table","bb30234b":"def choose_action(observ):\n    return np.argmax(q_table[observ])","9b79d135":"for i in range(10000):\n\n    observ = env.reset()\n    \n    action = choose_action(observ)\n\n    prev_observ = None\n    prev_action = None\n\n    t = 0\n    \n    for t in range(2500):\n         \n        env.render()\n\n        observ, reward, done, info = env.step(action)\n\n        action = choose_action(observ)\n\n        if not prev_observ is None:\n\n            q_old = q_table[prev_observ][prev_action]\n            q_new = q_old\n\n            if done:\n                q_new += alpha * (reward - q_old)\n                \n            else:\n                q_new +=  alpha * (reward + gamma * q_table[observ][action] - q_old)\n\n            new_table = q_table[prev_observ]\n            new_table[prev_action] = q_new\n\n            q_table[prev_observ] = new_table\n\n        prev_observ = observ\n        prev_action = action\n \n        print(\"Episode {} finished after {} timesteps with r={}.\".format(i, t, reward))\n        break","28e38940":"q_table","56e767e8":"#End","a74f90a4":"## Define choose action method\n* Pick the action with the highest q value","92559fb8":"## Import Packages\n* gym - collection of environments for reinforcement learning algorithms.\n* numpy - package for scientific computing with Python","d7541e77":"Purpose:\nDemonstrate application of Q-learning using SARSA algorithms in Reinforcement Learning through the OpenAI Gym environment FrozenLake\n\nGoal: Use Q-Learning SARSA to retrieve a freesbee without falling into a hole on a frozen lake\n\n\nMethod: \nState Action Reward State Action (SARSA) uses optimal policy that allows us to maximize cumulative rewards over the lifetime of the agent","24ec41c9":"## Train the agent\n* Train the agent for 10000 episodes.\n* Invoke the reset method to start an episode.\n* Choose an arbitarary initial action.\n* Initialize previous observation and action\n* Iterate over 2500 timsteps. For each timestep: \n    * Render the environment for visulaization.\n    * Execute the chosen action and store the return values.\n    * Choose the next action based on observations returned by the previous action.\n    * If this is not the first action in the episode,\n        * get the q value for the previous action and previous observation.\n        * if s is not terminal,compute  Q(St, At)\u2190 Q(St, At) + \u03b1[Rt+1 + \u03b3Q(St+1, At+1) \u2212 Q(St, At)]. \n        * If the episode is over, diregard the discount factor when computing Q value.\n        * update the q table\n    * Update the previous observation and previous action parameters\n    * If the episode has terminated, print out the info. \n* Note that the executed action is not always the chosen action. Some episodes terminate in goal and some in a hole.         \n        ","bc3cbf3e":"## Spaces\n* Every environment comes with an action_space and an observation_space. \n* These attributes are of type Space, and they describe the format of valid actions and observations.","7b7da658":"## Define parameters\n* Define the learning rate (alpha) and the discount factor (gamma)\n* Create a Q-Table and intiialize it with 1 for each state-action pair","d0a761f7":"## Define the environment\nThe agent needs to navigate across a frozen lake with the snow melting in a few parts, to retreive a frisbee.\nThe surface is described using a grid like the following:\n* SFFF       (S: starting point, safe)\n* FHFH       (F: frozen surface, safe)\n* FFFH       (H: hole, fall to your doom)\n* HFFG       (G: goal, where the frisbee is located)\n\nThe episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise."}}