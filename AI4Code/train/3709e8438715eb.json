{"cell_type":{"f610f5d6":"code","5771ee07":"code","1c7bb3e0":"code","0b002949":"code","bfb1f8c5":"code","0b70a808":"code","b0e77da2":"code","0e0b4cca":"code","51e1f595":"code","f38ce088":"code","6e09eae8":"code","719e09b3":"markdown"},"source":{"f610f5d6":"import pandas as pd\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score","5771ee07":"data = pd.read_csv(\"..\/input\/company-bankruptcy-prediction\/data.csv\")","1c7bb3e0":"from sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE\n\ny = data[\"Bankrupt?\"]\nX = data.drop(columns=[\"Bankrupt?\"])\n\nover_sample=SMOTE()\nX_ros, y_ros=over_sample.fit_resample(X,y)\n\nX_train, X_test, y_train, y_test = train_test_split(X_ros, y_ros, test_size=0.3)\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","0b002949":"from sklearn.neighbors import KNeighborsClassifier\n\ndef KNC(X_train, X_test, y_train, y_test, n_neighbors=2):\n    KNC = KNeighborsClassifier(n_neighbors=n_neighbors)\n    KNC.fit(X_train, y_train)\n    \n    y_prediction = KNC.predict(X_test)\n    \n    confusionMatrix = confusion_matrix(y_test, y_prediction)\n    classificationReport = classification_report(y_test, y_prediction)\n    \n    fpr, tpr, _ = roc_curve(y_test, y_prediction)\n    precision, recall, _ = precision_recall_curve(y_test, y_prediction)\n\n    fig = ff.create_annotated_heatmap(z=confusion_matrix(y_test, y_prediction), x=[\"Predicted 0\", \"Predicted 1\"], y=[\"True 0\", \"True 1\"], colorscale=\"ice\")\n    fig.show()\n    \n    fig = px.line(x=fpr, y=tpr, title=\"ROC Curve\", labels=dict(x=\"False Positive Rate\", y=\"True Positive Rate\"))\n    fig.show()\n    \n    fig = px.line(x=precision, y=recall, title=\"Precision Recall Curve\", labels=dict(x=\"Precision\", y=\"Recall\")) \n    fig.show()\n    \n    accuracyScore = accuracy_score(y_test, y_prediction)\n    rocaucScore = roc_auc_score(y_test, y_prediction)\n    f1Score = f1_score(y_test, y_prediction, average=None)\n    aucScore = auc(fpr, tpr)\n    precisionScore = precision_score(y_test, y_prediction, average=None)\n    recallScore = recall_score(y_test, y_prediction, average=None)\n\n\n    \n    fig = go.Figure(data=[go.Table(header=dict(values=['Type', 'Scores']),\n                 cells=dict(values=[[\"Accuracy\", \"ROC AUC\", \"AUC\", \"F1\", \"Precision\", \"Recall\"], [accuracyScore, rocaucScore, aucScore, f1Score, precisionScore, recallScore]]))\n                         ])\n    fig.show()\n    \n    print(classificationReport)\n    \n    \n    return y_prediction","bfb1f8c5":"y_prediction = KNC(X_train, X_test, y_train, y_test)","0b70a808":"from sklearn.svm import SVC\n\n\ndef SVClassifier(X_train, X_test, y_train, y_test):\n    svc = SVC()\n    svc.fit(X_train, y_train)\n    \n    y_prediction = svc.predict(X_test)\n    confusionMatrix = confusion_matrix(y_test, y_prediction)\n    classificationReport = classification_report(y_test, y_prediction)\n    \n    fpr, tpr, _ = roc_curve(y_test, y_prediction)\n    precision, recall, _ = precision_recall_curve(y_test, y_prediction)\n\n    fig = ff.create_annotated_heatmap(z=confusion_matrix(y_test, y_prediction), x=[\"Predicted 0\", \"Predicted 1\"], y=[\"True 0\", \"True 1\"], colorscale=\"ice\")\n    fig.show()\n    \n    fig = px.line(x=fpr, y=tpr, title=\"ROC Curve\", labels=dict(x=\"False Positive Rate\", y=\"True Positive Rate\"))\n    fig.show()\n    \n    fig = px.line(x=precision, y=recall, title=\"Precision Recall Curve\", labels=dict(x=\"Precision\", y=\"Recall\")) \n    fig.show()\n    \n    accuracyScore = accuracy_score(y_test, y_prediction)\n    rocaucScore = roc_auc_score(y_test, y_prediction)\n    f1Score = f1_score(y_test, y_prediction, average=None)\n    aucScore = auc(fpr, tpr)\n    precisionScore = precision_score(y_test, y_prediction, average=None)\n    recallScore = recall_score(y_test, y_prediction, average=None)\n\n\n    \n    fig = go.Figure(data=[go.Table(header=dict(values=['Type', 'Scores']),\n                 cells=dict(values=[[\"Accuracy\", \"ROC AUC\", \"AUC\", \"F1\", \"Precision\", \"Recall\"], [accuracyScore, rocaucScore, aucScore, f1Score, precisionScore, recallScore]]))\n                         ])\n    fig.show()\n    \n    print(classificationReport)\n    \n    \n    return y_prediction","b0e77da2":"y_prediction = SVClassifier(X_train, X_test, y_train, y_test)","0e0b4cca":"from sklearn.linear_model import LogisticRegression\n\ndef Logistic(X_train, X_test, y_train, y_test):\n    LR = LogisticRegression()\n    LR.fit(X_train, y_train)\n    \n    y_prediction = LR.predict(X_test)\n    confusionMatrix = confusion_matrix(y_test, y_prediction)\n    classificationReport = classification_report(y_test, y_prediction)\n    \n    fpr, tpr, _ = roc_curve(y_test, y_prediction)\n    precision, recall, _ = precision_recall_curve(y_test, y_prediction)\n\n    fig = ff.create_annotated_heatmap(z=confusion_matrix(y_test, y_prediction), x=[\"Predicted 0\", \"Predicted 1\"], y=[\"True 0\", \"True 1\"], colorscale=\"ice\")\n    fig.show()\n    \n    fig = px.line(x=fpr, y=tpr, title=\"ROC Curve\", labels=dict(x=\"False Positive Rate\", y=\"True Positive Rate\"))\n    fig.show()\n    \n    fig = px.line(x=precision, y=recall, title=\"Precision Recall Curve\", labels=dict(x=\"Precision\", y=\"Recall\")) \n    fig.show()\n    \n    accuracyScore = accuracy_score(y_test, y_prediction)\n    rocaucScore = roc_auc_score(y_test, y_prediction)\n    f1Score = f1_score(y_test, y_prediction, average=None)\n    aucScore = auc(fpr, tpr)\n    precisionScore = precision_score(y_test, y_prediction, average=None)\n    recallScore = recall_score(y_test, y_prediction, average=None)\n\n\n    \n    fig = go.Figure(data=[go.Table(header=dict(values=['Type', 'Scores']),\n                 cells=dict(values=[[\"Accuracy\", \"ROC AUC\", \"AUC\", \"F1\", \"Precision\", \"Recall\"], [accuracyScore, rocaucScore, aucScore, f1Score, precisionScore, recallScore]]))\n                         ])\n    fig.show()\n    \n    print(classificationReport)\n    \n    \n    return y_prediction","51e1f595":"y_prediction = Logistic(X_train, X_test, y_train, y_test)","f38ce088":"from sklearn.ensemble import RandomForestClassifier\n\n\ndef RandomForest(X_train, X_test, y_train, y_test):\n    RFC = RandomForestClassifier()\n    RFC.fit(X_train, y_train)\n    \n    y_prediction = RFC.predict(X_test)\n    confusionMatrix = confusion_matrix(y_test, y_prediction)\n    classificationReport = classification_report(y_test, y_prediction)\n    \n    fpr, tpr, _ = roc_curve(y_test, y_prediction)\n    precision, recall, _ = precision_recall_curve(y_test, y_prediction)\n\n    fig = ff.create_annotated_heatmap(z=confusion_matrix(y_test, y_prediction), x=[\"Predicted 0\", \"Predicted 1\"], y=[\"True 0\", \"True 1\"], colorscale=\"ice\")\n    fig.show()\n    \n    fig = px.line(x=fpr, y=tpr, title=\"ROC Curve\", labels=dict(x=\"False Positive Rate\", y=\"True Positive Rate\"))\n    fig.show()\n    \n    fig = px.line(x=precision, y=recall, title=\"Precision Recall Curve\", labels=dict(x=\"Precision\", y=\"Recall\")) \n    fig.show()\n    \n    accuracyScore = accuracy_score(y_test, y_prediction)\n    rocaucScore = roc_auc_score(y_test, y_prediction)\n    f1Score = f1_score(y_test, y_prediction, average=None)\n    aucScore = auc(fpr, tpr)\n    precisionScore = precision_score(y_test, y_prediction, average=None)\n    recallScore = recall_score(y_test, y_prediction, average=None)\n\n\n    \n    fig = go.Figure(data=[go.Table(header=dict(values=['Type', 'Scores']),\n                 cells=dict(values=[[\"Accuracy\", \"ROC AUC\", \"AUC\", \"F1\", \"Precision\", \"Recall\"], [accuracyScore, rocaucScore, aucScore, f1Score, precisionScore, recallScore]]))\n                         ])\n    fig.show()\n    \n    print(classificationReport)\n    \n    \n    \n    return y_prediction","6e09eae8":"%%time\ny_prediction = RandomForest(X_train, X_test, y_train, y_test)","719e09b3":"# Oversampling Method\n[Oversampling](https:\/\/imbalanced-learn.org\/stable\/over_sampling.html)"}}