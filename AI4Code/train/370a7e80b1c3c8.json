{"cell_type":{"5988c906":"code","537379aa":"code","070c55f0":"code","0e6d7e8b":"code","9cbf5f7c":"code","cf0c5b1d":"code","13cc3d16":"code","25f0aad6":"code","ca9008dc":"code","aa3f3aab":"code","99084369":"code","360b75b7":"code","19e506d4":"code","b52e7d6f":"code","70e28948":"code","ddc52556":"code","ccfe9f4d":"code","e8ccec0d":"markdown","7321d920":"markdown","1ad82a3e":"markdown","6d41b7b7":"markdown","3f0e8aca":"markdown","bec1a17f":"markdown","4ff5a6aa":"markdown","d996b886":"markdown","339edf74":"markdown","4e888aec":"markdown"},"source":{"5988c906":"# Importing some libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification","537379aa":"# Let's make the example dataset\nX,y = make_classification(n_samples = 1000, n_classes = 2, weights = [1,1], random_state = 1)","070c55f0":"# Let's check shape of the dataset created\nprint(X.shape)    # ----> Independent variables\nprint(y.shape)    # ----> Dependent variable\/Labels","0e6d7e8b":"# Let's split the data into train and test sets\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(X, y, test_size = 0.3, random_state = 56)","9cbf5f7c":"from sklearn.metrics import roc_curve, roc_auc_score","cf0c5b1d":"from sklearn.ensemble import RandomForestClassifier\nrf_model = RandomForestClassifier()\nrf_model.fit(X_train, y_train)\ny_train_pred = rf_model.predict_proba(X_train)\ny_test_pred = rf_model.predict_proba(X_test)\nprint(f\"Random Forest train roc_auc : {roc_auc_score(y_train, y_train_pred[:,1])}\")\nprint(f\"Random Forest test roc_auc : {roc_auc_score(y_test, y_test_pred[:,1])}\")","13cc3d16":"y_train_pred","25f0aad6":"from sklearn.linear_model import LogisticRegression\nlog_classifier = LogisticRegression()\nlog_classifier.fit(X_train, y_train)\ny_train_pred = log_classifier.predict_proba(X_train)\ny_test_pred = log_classifier.predict_proba(X_test)\nprint(f\"Logistic Regression train roc_auc : {roc_auc_score(y_train, y_train_pred[:,1])}\")\nprint(f\"Logistic Regression test roc_auc : {roc_auc_score(y_test, y_test_pred[:,1])}\")","ca9008dc":"from sklearn.ensemble import AdaBoostClassifier\nadb_classifier = AdaBoostClassifier()\nadb_classifier.fit(X_train, y_train)\ny_train_pred = adb_classifier.predict_proba(X_train)\ny_test_pred = adb_classifier.predict_proba(X_test)\nprint(f\"ADA Boost Classifier train roc_auc : {roc_auc_score(y_train, y_train_pred[:,1])}\")\nprint(f\"ADA Boost Classifier test roc_auc : {roc_auc_score(y_test, y_test_pred[:,1])}\")","aa3f3aab":"from sklearn.neighbors import KNeighborsClassifier\nknn_classifier = KNeighborsClassifier()\nknn_classifier.fit(X_train, y_train)\ny_train_pred = knn_classifier.predict_proba(X_train)\ny_test_pred = knn_classifier.predict_proba(X_test)\nprint(f\"KNN Classifier train roc_auc : {roc_auc_score(y_train, y_train_pred[:,1])}\")\nprint(f\"KNN Classifier test roc_auc : {roc_auc_score(y_test, y_test_pred[:,1])}\")","99084369":"# Taking the mean of the predictions made by all the models\npred = []\nfor model in [rf_model,log_classifier,adb_classifier,knn_classifier]:\n    pred.append(pd.Series(model.predict_proba(X_test)[:,1]))\nfinal_prediction = pd.concat(pred,axis = 1).mean(axis = 1)\nprint(f\"Ensemble test roc-auc: {roc_auc_score(y_test,final_prediction)}\")","360b75b7":"# Calculating the roc curve values\nfpr,tpr,thresholds = roc_curve(y_test, final_prediction)","19e506d4":"thresholds","b52e7d6f":"# Checking the accuracy of the model with respect to each of the threshold values\n# Here, we are assigning the values in the final_prediction to 1 if > threshold otherwise 0\n\nfrom sklearn.metrics import accuracy_score\naccuracy = []\nfor thres in thresholds:\n    y_pred = np.where(final_prediction > thres,1,0)\n    accuracy.append(accuracy_score(y_test, y_pred, normalize = True))\n    \naccuracy = pd.concat([pd.Series(thresholds), pd.Series(fpr), pd.Series(tpr), pd.Series(accuracy)],\n                        axis = 1)\naccuracy.columns = ['Thresholds', 'FPR', 'TPR', 'Accuracy']\naccuracy.sort_values(by ='Accuracy', ascending = False, inplace = True)\naccuracy.reset_index(drop = True,inplace = True)","70e28948":"accuracy","ddc52556":"# Plot showing roc curve\ndef plot_roc_curve(fpr, tpr):\n    plt.plot(fpr, tpr, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()","ccfe9f4d":"plot_roc_curve(fpr,tpr)","e8ccec0d":"#### Thanks and If you liked the notebook, then give an upvote","7321d920":"### KNN Classifier","1ad82a3e":"So, if you don't know about \".predict_prob\". Don't worry I'll explain to you.\n#### Basically, .predict_prob gives you the probablities of the predicted output in the form of probablities your model assigned to both class 0 and 1. \n#### As you can see above the first column is showing the probablities of label 1 and second column showing the probablities of lable 0.","6d41b7b7":"### Now I'll be showing how to select the best threshold value for maximum accuracy","3f0e8aca":"### Logistic Regression","bec1a17f":"###### Using make_classification I'm going to create an example dataset","4ff5a6aa":"##### From the above dataframe we can select the best threshold value for our binary classification according to the problem we are trying to solve based on TPR, FPR and Accuracy. ","d996b886":"### ADA Boost Classifier","339edf74":"### Hey fellas, hope you all are good. \n#### Today I am going to show you how to select the best threshold for the binary classification problems. I learned this concept and thought to share this with all of you.\n#### So, stay with me and keep looking\ud83d\ude09","4e888aec":"### Random Forest"}}