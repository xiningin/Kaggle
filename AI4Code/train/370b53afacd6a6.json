{"cell_type":{"f98b20bb":"code","d9250a3c":"code","86ad60e2":"code","1fb135dc":"code","0001e870":"code","b54b502a":"code","270eae15":"code","0b3c888a":"code","93c92ed5":"code","00dcf046":"code","2cebbeb6":"code","bbe0c29c":"code","21371f86":"code","d799f9e0":"code","0acb2475":"code","aeb3c09b":"code","d6df5e54":"code","b5807e39":"code","d3019984":"code","f2641899":"code","bb5ff791":"code","84f0aaa4":"code","52010ac7":"code","032588fa":"code","b417c9f6":"code","cfbfccf4":"code","dfb4a3cd":"code","788bcd03":"code","0008ce3c":"code","e453e1b2":"code","8c1ba71c":"code","aa096e3f":"code","b6355621":"code","737107c3":"code","0a5645ff":"code","882673cf":"code","8675ca5d":"code","a1966e02":"code","6bc66964":"code","13aee3e8":"code","048c66de":"code","a93cacbd":"code","33b98ee7":"code","edf16aad":"code","d8454c1b":"code","c1525a78":"code","26059998":"code","638f9423":"code","daed85ed":"code","a359ab90":"code","9fbf0334":"code","50893ba3":"code","8a69dd0a":"code","c286f27a":"code","e1fa739d":"code","224529c5":"code","d4ce0671":"code","2e233657":"code","75ef5938":"code","d254ee58":"code","431de8ec":"code","71c6f897":"code","2abeff9e":"code","e6a91c04":"code","5df47ba5":"code","ee3013fa":"code","70caac19":"code","721bbf36":"code","77175453":"code","fc8dc1b4":"code","b6d3d39b":"code","36d6b865":"code","b6c06cbd":"code","9322822d":"code","864f01fc":"code","e6154201":"code","0ebfe8b7":"code","010776ea":"code","80cb5292":"code","6f2e3781":"code","824addd0":"code","be0f3f43":"code","9e8d2e34":"code","4014290f":"code","8d8e591c":"code","4564d19c":"code","87719bdb":"markdown","7fd0196e":"markdown","50fefd85":"markdown","eefcf890":"markdown","2569db0d":"markdown","5fb0a413":"markdown","58b51b7a":"markdown","f5b40a02":"markdown","ff19d7b7":"markdown","0169b8b8":"markdown","d059a62f":"markdown","faed52ec":"markdown","414743a7":"markdown","018d3a22":"markdown","014e0d28":"markdown","d3e3c414":"markdown","ee4d35ab":"markdown","4ea9c7d4":"markdown","93411eb1":"markdown","beeea057":"markdown","6bb9cc29":"markdown","213c2f40":"markdown","8ebc63ee":"markdown","5bd40953":"markdown","f04dea87":"markdown","8ac2ec83":"markdown","76d14858":"markdown","24a7a90f":"markdown","9db1a63f":"markdown","09c1dd55":"markdown","a03a89c3":"markdown","1a0c65be":"markdown","e4b56be3":"markdown","26eb6e31":"markdown","1fdd219c":"markdown"},"source":{"f98b20bb":"import numpy as np\nimport pandas as pd\nimport os\nimport gc\nimport warnings\nwarnings.filterwarnings(\"ignore\") # ignore python warnings of deprecation\n\ndf = pd.read_csv('..\/input\/loan.csv', na_values=['#NAME?']) # '#NAME?' in the datafile will be converted to NaN","d9250a3c":"df.shape","86ad60e2":"df.head(5)","1fb135dc":"df.info()","0001e870":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport plotly\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\n\n%matplotlib inline","b54b502a":"plt.subplots(figsize=(20,10))\nplt.title(\"The distribution of loan amounts by status\").set_size(40)\nsns.boxplot(x=\"loan_amnt\", y=\"loan_status\", data=df)","270eae15":"df['int_round'] = df['int_rate'].round(0).astype(int)\n\nplt.figure(figsize = (10,8))\n\n#Exploring the Int_rate\nplt.subplot(211)\ng = sns.distplot(np.log(df[\"int_rate\"]))\ng.set_xlabel(\"\", fontsize=12)\ng.set_ylabel(\"Distribuition\", fontsize=12)\ng.set_title(\"Int Rate Log distribuition\", fontsize=20)\n\nplt.subplot(212)\ng1 = sns.countplot(x=\"int_round\",data=df, \n                   palette=\"Set2\")\ng1.set_xlabel(\"Int Rate\", fontsize=12)\ng1.set_ylabel(\"Count\", fontsize=12)\ng1.set_title(\"Int Rate Normal Distribuition\", fontsize=20)\n\nplt.subplots_adjust(wspace = 0.2, hspace = 0.6,top = 0.9)\n\nplt.show()","0b3c888a":"print(df.loan_status.value_counts())\n\nplt.figure(figsize = (12,14))\n\nplt.subplot(311)\ng = sns.countplot(x=\"loan_status\", data=df)\ng.set_xticklabels(g.get_xticklabels(),rotation=45)\ng.set_xlabel(\"\", fontsize=12)\ng.set_ylabel(\"Count\", fontsize=15)\ng.set_title(\"Loan Status Count\", fontsize=20)\n\nplt.subplot(312)\ng1 = sns.boxplot(x=\"loan_status\", y=\"total_acc\", data=df)\ng1.set_xticklabels(g1.get_xticklabels(),rotation=45)\ng1.set_xlabel(\"\", fontsize=12)\ng1.set_ylabel(\"Total Acc\", fontsize=15)\ng1.set_title(\"Duration Count\", fontsize=20)\n\nplt.subplot(313)\ng2 = sns.violinplot(x=\"loan_status\", y=\"loan_amnt\", data=df)\ng2.set_xticklabels(g2.get_xticklabels(),rotation=45)\ng2.set_xlabel(\"Duration Distribuition\", fontsize=15)\ng2.set_ylabel(\"Count\", fontsize=15)\ng2.set_title(\"Loan Amount\", fontsize=20)\n\nplt.subplots_adjust(wspace = 0.2, hspace = 0.7,top = 0.9)\n\nplt.show()","93c92ed5":"df['issue_month'], df['issue_year'] = df['issue_d'].str.split('-', 1).str\nplt.figure(figsize=(12,8))\nsns.barplot('issue_year', 'loan_amnt', data=df, palette='tab10')\nplt.title('Issuance of Loans', fontsize=16)\nplt.xlabel('Year', fontsize=14)\nplt.ylabel('Average loan amount issued', fontsize=14)","00dcf046":"f, ((ax1, ax2)) = plt.subplots(1, 2)\ncmap = plt.cm.coolwarm\n\nby_credit_score = df.groupby(['issue_year', 'grade']).loan_amnt.mean()\nby_credit_score.unstack().plot(legend=False, ax=ax1, figsize=(14, 4), colormap=cmap)\nax1.set_title('Loans issued by Credit Score', fontsize=14)\n    \n    \nby_inc = df.groupby(['issue_year', 'grade']).int_rate.mean()\nby_inc.unstack().plot(ax=ax2, figsize=(14, 4), colormap=cmap)\nax2.set_title('Interest Rates by Credit Score', fontsize=14)\n\nax2.legend(bbox_to_anchor=(-1.0, -0.3, 1.7, 0.1), loc=5, prop={'size':12},\n           ncol=7, mode=\"expand\", borderaxespad=0.)","2cebbeb6":"# drop the columns we created for data visualiztion","bbe0c29c":"df.drop(['int_round','issue_year','issue_month'], axis=1, inplace=True)","21371f86":"df_onehot = df.copy()\ndf_onehot = pd.get_dummies(df_onehot, columns=['loan_status',\"term\",\"grade\",'home_ownership'], prefix = ['loan_status',\"term\",\"grade\",'home_ownership'])\n\nprint(df_onehot.head())","d799f9e0":"df_correlations = df_onehot.corr()\n\ntrace = go.Heatmap(z=df_correlations.values,\n                   x=df_correlations.columns,\n                   y=df_correlations.columns,\n                  colorscale=[[0.0, 'rgb(165,0,38)'], \n                              [0.1111111111111111, 'rgb(215,48,39)'], \n                              [0.2222222222222222, 'rgb(244,109,67)'], \n                              [0.3333333333333333, 'rgb(253,174,97)'], \n                              [0.4444444444444444, 'rgb(254,224,144)'], \n                              [0.5555555555555556, 'rgb(224,243,248)'], \n                              [0.6666666666666666, 'rgb(171,217,233)'], \n                              [0.7777777777777778, 'rgb(116,173,209)'], \n                              [0.8888888888888888, 'rgb(69,117,180)'], \n                              [1.0, 'rgb(49,54,149)']],\n            colorbar = dict(\n            title = 'Level of Correlation',\n            titleside = 'top',\n            tickmode = 'array',\n            tickvals = [-0.52,0.2,0.95],\n            ticktext = ['Negative Correlation','Low Correlation','Positive Correlation'],\n            ticks = 'outside'\n        )\n                  )\n\n\nlayout = {\"title\": \"Correlation Heatmap\"}\ndata=[trace]\n\nfig = dict(data=data, layout=layout)\niplot(fig, filename='labelled-heatmap')","0acb2475":"df_onehot.corr()[\"loan_status_Current\"].sort_values(ascending=False).head(15)","aeb3c09b":"df_onehot.corr()[\"loan_status_Charged Off\"].sort_values(ascending=False).head(15) ","d6df5e54":"df_onehot.corr()[\"loan_status_Fully Paid\"].sort_values(ascending=False).head(15)","b5807e39":"df_onehot.corr()[\"loan_status_Late (31-120 days)\"].sort_values(ascending=False).head(15)","d3019984":"%reset -sf","f2641899":"import numpy as np\nimport pandas as pd\nimport os\nimport gc\ngc.collect()","bb5ff791":"#del df, df_onehot\ngc.collect()\ndf = pd.read_csv('..\/input\/loan.csv', na_values=['#NAME?'], low_memory=False) # '#NAME?' in the datafile will be converted to NaN","84f0aaa4":"df[\"loan_status\"].value_counts()","52010ac7":"# get the number of missing data points per column\nmissing_values_count = df.isnull().sum()\nprint(missing_values_count)","032588fa":"def null_values(df):\n        mis_val = df.isnull().sum()\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        print (\"Dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns with missing values.\")\n        return mis_val_table_ren_columns","b417c9f6":"# Missing values statistics\nmiss_values = null_values(df)\nmiss_values.head(50)","cfbfccf4":"# Drop irrelevant columns\ndf.drop(['id', 'member_id', 'emp_title', 'url', 'desc', 'zip_code', 'title'], axis=1, inplace=True)","dfb4a3cd":"df.drop(['dti_joint','annual_inc_joint','verification_status_joint','il_util','mths_since_rcnt_il','total_cu_tl',\n         'inq_fi','all_util','max_bal_bc','open_rv_24m','open_rv_12m','total_bal_il','open_il_24m','total_bal_il',\n         'open_il_24m','open_il_6m','open_acc_6m','open_il_12m','inq_last_12m','mths_since_last_record','mths_since_last_major_derog'],axis=1, inplace=True)","788bcd03":"miss_values = null_values(df)\nmiss_values.head(20)","0008ce3c":"df['mths_since_last_delinq'] = df['mths_since_last_delinq'].fillna(df['mths_since_last_delinq'].median())","e453e1b2":"\n# Determining the loans that are bad from loan_status column\n\n#bad_loan = [\"Charged Off\", \"Default\", \"Does not meet the credit policy. Status:Charged Off\", \"In Grace Period\", \n#            \"Late (16-30 days)\", \"Late (31-120 days)\"]\n\n\n","8c1ba71c":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\n\nlabel_encoder = LabelEncoder()\ninteger_encoded = label_encoder.fit_transform(df[\"loan_status\"])\ndf['loan_status']= integer_encoded","aa096e3f":"unique, counts = np.unique(integer_encoded, return_counts=True)\nprint('Unique values:\\n', np.asarray((unique, counts)).T)","b6355621":"a=label_encoder.inverse_transform(unique)\na","737107c3":"# maybe move it before define y\ndf['issue_d']= pd.to_datetime(df['issue_d']).apply(lambda x: int(x.strftime('%Y')))\ndf['last_pymnt_d']= pd.to_datetime(df['last_pymnt_d'].fillna('2016-01-01')).apply(lambda x: int(x.strftime('%m')))\ndf['last_credit_pull_d']= pd.to_datetime(df['last_credit_pull_d'].fillna(\"2016-01-01\")).apply(lambda x: int(x.strftime('%m')))\ndf['earliest_cr_line']= pd.to_datetime(df['earliest_cr_line'].fillna('2007-08-01')).apply(lambda x: int(x.strftime('%m')))\ndf['next_pymnt_d'] = pd.to_datetime(df['next_pymnt_d'].fillna(value = '2016-02-01')).apply(lambda x:int(x.strftime(\"%Y\")))","0a5645ff":"from sklearn import preprocessing","882673cf":"count = 0\n\nfor col in df:\n    if df[col].dtype == 'object':\n        if len(list(df[col].unique())) <= 2:     \n            le = preprocessing.LabelEncoder()\n            df[col] = le.fit_transform(df[col])\n            count += 1\n            print (col)\n            \nprint('%d columns were label encoded.' % count)","8675ca5d":"df = pd.get_dummies(df)\nprint(df.shape)","a1966e02":"unique, counts = np.unique(df['loan_status'], return_counts=True)\nprint('Unique values:\\n', np.asarray((unique, counts)).T) #check unique values","6bc66964":"#df.dropna(inplace=True)","13aee3e8":"miss_values = null_values(df[(df[\"loan_status\"]==3) | (df[\"loan_status\"]==4)]) # loan_status 3,4 depend on these 3 columns, we need to keep them for tree boosting algorithms\nmiss_values.head(20)\n#tot_coll_amt,tot_cur_bal, total_rev_hi_lim","048c66de":"no_three_column=df.drop(['tot_coll_amt','tot_cur_bal', 'total_rev_hi_lim'],1)\nmissing=null_values(no_three_column)\nmissing.head(20)","a93cacbd":"no_three_column.dropna(inplace=True)","33b98ee7":"no_three_column.shape","edf16aad":"#df[(df[\"loan_status\"]==3) | (df[\"loan_status\"]==4)].dropna(inplace=True)","d8454c1b":"df2=df.loc[(df[\"loan_status\"]!=3) | (df[\"loan_status\"]!=4)].dropna()","c1525a78":"df=pd.concat([df2,df[(df[\"loan_status\"]==3) | (df[\"loan_status\"]==4)]],0)","26059998":"null_values(df) # these missing values are important for decision trees since they are only related to loan_status=3,4","638f9423":"df.shape","daed85ed":"no_three_column.head(10)","a359ab90":"X = no_three_column.drop('loan_status',1)\ny = no_three_column['loan_status']","9fbf0334":"#build test and training sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)","50893ba3":"#y_train.shape\nunique, counts = np.unique(y_test, return_counts=True)\nprint('Unique values:\\n', np.asarray((unique, counts)).T)","8a69dd0a":"from sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n# log_reg_sm = LogisticRegression()\nlog_reg.fit(X_train, y_train)","c286f27a":"from sklearn.metrics import accuracy_score\n\nnormal_ypred = log_reg.predict(X_test)\nprint(accuracy_score(y_test, normal_ypred))","e1fa739d":"#from sklearn.preprocessing import StandardScaler\n# normalize dataset\n#norm_X = StandardScaler() # initiate scaler\n#X_train_norm = norm_X.fit_transform(X_train) # get normalization parameters based on train dataset \n#X_test_norm = norm_X.transform(X_test) # apply normalization to test data based on train dataset parameters","224529c5":"X = df.drop('loan_status',1)\ny = df['loan_status']","d4ce0671":"#build test and training sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)","2e233657":"import gc # clean unused variables from RAM\ngc.collect()","75ef5938":"#y_train.shape\nunique, counts = np.unique(y_test, return_counts=True)\nprint('Unique values:\\n', np.asarray((unique, counts)).T)","d254ee58":"import lightgbm as lgb\n\nparam = {'boosting_type': 'gbdt','num_leaves':35,'nthread': 4, 'num_trees':100, 'objective':'multiclass', 'metric' : 'softmax',\n        'num_class': 10}\n","431de8ec":"train_data = lgb.Dataset(X_train, y_train, silent=False) \ntest_data = lgb.Dataset(X_test, y_test, silent=False) \nmodel = lgb.train(param, train_set = train_data, num_boost_round=20, verbose_eval=4)","71c6f897":"preds = model.predict(X_test, num_iteration = model.best_iteration)","2abeff9e":"from sklearn.metrics import accuracy_score\n#predicted=np.where(preds > 0.5, 1, 0)\n#preds[preds > 0.5] = 1\n#preds[preds <= 0.5] = 0\n#\n(preds)\npredictions = []\n\nfor x in preds:\n    predictions.append(np.argmax(x))","e6a91c04":"acc_lgbm = accuracy_score(y_test,predictions)\nprint('Overall accuracy of Light GBM model:', acc_lgbm)","5df47ba5":"np.unique(predictions)","ee3013fa":"lgb.plot_importance(model, max_num_features=21, importance_type='split')\n#tot_coll_amt,tot_cur_bal, total_rev_hi_lim","70caac19":"import seaborn as sns\nfrom sklearn.metrics import confusion_matrix,accuracy_score, roc_curve, auc\nsns.set_style(\"whitegrid\")\nimport matplotlib.pyplot as plt","721bbf36":"def conf_m(cm):\n  labels = ['Charged Off', 'Current', 'Default',\n         'not meet policy. Status:Charged Off',\n         'not meet policy. Status:Fully Paid', 'Fully Paid',\n         'In Grace Period', 'Issued', 'Late (16-30 days)',\n         'Late (31-120 days)']\n  plt.figure(figsize=(8,6))\n  sns.heatmap(cm, xticklabels = labels, yticklabels = labels, annot = True, fmt='d', cmap=\"Blues\", vmin = 0.2);\n  plt.title('Confusion Matrix')\n  plt.ylabel('True Class')\n  plt.xlabel('Predicted Class')\n  return plt.show()","77175453":"#Print Confusion Matrix\nplt.figure()\ncm = confusion_matrix(y_test, predictions)\nconf_m(cm)","fc8dc1b4":"import imblearn\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.under_sampling import OneSidedSelection\ndel X, y, X_train, X_test, y_train, y_test\ngc.collect()","b6d3d39b":"X = no_three_column.drop('loan_status',1)\ny = no_three_column['loan_status']","36d6b865":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)","b6c06cbd":"oss = OneSidedSelection()\nX_resampled, y_resampled = oss.fit_sample(X, y)","9322822d":"np.unique(y_resampled)","864f01fc":"X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=14)","e6154201":"train_data = lgb.Dataset(X_train, y_train, silent=False) \ntest_data = lgb.Dataset(X_test, y_test, silent=False) \nparam = {'boosting_type': 'gbdt','num_leaves':45,'nthread': 4, 'objective':'multiclass', 'metric' : 'softmax',\n        'num_class': 10}\nmodel = lgb.train(param, train_set = train_data, verbose_eval=5, num_boost_round=40)","0ebfe8b7":"preds = model.predict(X_test, num_iteration = model.best_iteration)","010776ea":"from sklearn.metrics import accuracy_score\n\n(preds)\npredictions = []\n\nfor x in preds:\n    predictions.append(np.argmax(x))","80cb5292":"acc_lgbm = accuracy_score(y_test,predictions)\nprint('Overall accuracy of Light GBM model:', acc_lgbm)","6f2e3781":"from sklearn.metrics import precision_recall_fscore_support as score\nprecision, recall, fscore, support = score(y_test, predictions)\n\nprint('precision: {}'.format(precision))\nprint('recall: {}'.format(recall))\nprint('fscore: {}'.format(fscore))\nprint('support: {}'.format(support))","824addd0":"plt.figure()\ncm = confusion_matrix(y_test, predictions)\nconf_m(cm)","be0f3f43":"# this is required for XG boost because column names containing those [] < characters can not be trained\n#import re\n#regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n#X_train.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_train.columns.values]\n#X_test.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_test.columns.values]\n","9e8d2e34":"from xgboost import XGBClassifier\n\n\nmodel = XGBClassifier(objective='multi:softmax', nthread=-1 )\neval_set = [(X_test, y_test)]\nmodel.fit(X_train, y_train, early_stopping_rounds=1, eval_metric=\"merror\", eval_set=eval_set, verbose=True )\n# make predictions for test data\ny_pred = model.predict(X_test)\npredictions = [round(value) for value in y_pred]\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","4014290f":"from sklearn.metrics import precision_recall_fscore_support as score\nprecision, recall, fscore, support = score(y_test, predictions)\n\nprint('precision: {}'.format(precision))\nprint('recall: {}'.format(recall))\nprint('fscore: {}'.format(fscore))\nprint('support: {}'.format(support))","8d8e591c":"#Print Confusion Matrix\nplt.figure()\ncm = confusion_matrix(y_test, predictions)\nlabels = ['Charged Off', 'Current', 'Default',\n       'Does not meet the credit policy. Status:Charged Off',\n       'Does not meet the credit policy. Status:Fully Paid', 'Fully Paid',\n       'In Grace Period', 'Issued', 'Late (16-30 days)',\n       'Late (31-120 days)']\nplt.figure(figsize=(8,6))\nsns.heatmap(cm, xticklabels = labels, yticklabels = labels, annot = True, fmt='d', cmap=\"Blues\", vmin = 0.2);\nplt.title('Confusion Matrix')\nplt.ylabel('True Class')\nplt.xlabel('Predicted Class')\nplt.show()","4564d19c":"from xgboost import plot_importance\nfrom matplotlib import pyplot\n\nplot_importance(model)\npyplot.show()","87719bdb":"## visualize how many loans were issued by credit score","7fd0196e":"### First, we convert some critical category value, such as loan_status, term, grade, home_ownership into new columns and assign a 1 or 0 (True\/False) value to the column. This has the benefit of not weighting a value improperly.","50fefd85":"## define x and y","eefcf890":"### Check which top 15 coloumns have positive correlation with the four most frequent loan status - loan_status_Current, loan_status_Charged Off, loan_status_Fully Paid and loan_status_Late (31-120 days) in the data respectively.","2569db0d":"# Another interesting value to a Loan is the interest rate. Let's look this attribute","5fb0a413":"## checking information of data, from below information, we can see that there are some missing data in few columns compared to total number of id","58b51b7a":"## lets fill up the most missing values column with its median","f5b40a02":"# Feature engineering","ff19d7b7":"# lightGBM model","0169b8b8":"## get dummies for all columns","d059a62f":"# import sklearn to encode the variables","faed52ec":"## for our target value, check the count of different credit statuses","414743a7":"## drop the NaN values, make sure there is no na in training and test dataset","018d3a22":"## use sklearn to split the dataset into training and test","014e0d28":"## check again for missing values, fill up empty cells","d3e3c414":"#  Data Visualization Before Data Preprocessing","ee4d35ab":"#  issuance of loans by years","4ea9c7d4":"# fill up NaN values","93411eb1":"### view the accuracy, result is good (95.5% )\n","beeea057":"# LightGBM Balanced data","6bb9cc29":"#  the distribuition of the LOAN AMOUNT","213c2f40":"## use head() to check what the data looks like now, as we expected","8ebc63ee":"## define a function for calculating the number of missing values and percentage compared to whole data set","5bd40953":"## XG Boost","f04dea87":"## next, drop the columns with too many missing values, set the cut point around 75%","8ac2ec83":"## first, drop the columns that might not be correlated with the target value","76d14858":"## let's see the shape of data. We have 887379 rows with 74 columns","24a7a90f":"## take a brief look at first 5 rows of data with all columns","9db1a63f":"## Importing packages, using pandas to read csv file","09c1dd55":"## fit the training dataset into logistic regression model","a03a89c3":"# Loan Status Distribuition","1a0c65be":"1. ### now we use df_correlations dataframe to analyze data correlation","e4b56be3":"## run the function to list the missing values with percentages","26eb6e31":"# Let us find out which columns have positive correlation with each loan status","1fdd219c":"## let us check the data for missing values"}}