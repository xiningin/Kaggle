{"cell_type":{"de0ebfa2":"code","57c772c0":"code","596315e6":"code","aa013e09":"code","6c3a1c17":"code","81166180":"code","d836c34f":"code","3bc60740":"code","5d12a8c2":"code","5e081b74":"code","2c70466e":"code","02f5a5d3":"code","e463a603":"code","d0c8e8ef":"code","acc98ee9":"code","552cf1a2":"code","a28cd46a":"code","56cf3f85":"code","ec74b5e3":"code","c8c05d42":"code","125781ba":"code","dc9be1d0":"code","7fffe329":"code","6d37714b":"code","dffc96ac":"code","8bdb6c89":"code","4c921aa2":"code","27d9acd8":"code","16ba931e":"code","17c14dc1":"code","bef05712":"code","7394b858":"code","63f085e5":"code","78d58c86":"code","eca8e8ba":"markdown","29f93c50":"markdown","5357f462":"markdown","18d995a5":"markdown","dc687854":"markdown","afa3aa56":"markdown","3c8b3686":"markdown","c23c2866":"markdown","193ba166":"markdown","c030edb1":"markdown","144ba99e":"markdown","e7d9c62c":"markdown","3377aa00":"markdown","1f5b16fa":"markdown","dd48d834":"markdown","eda10107":"markdown","9c63f0db":"markdown","a93a6e08":"markdown","e898ce53":"markdown","098bc42f":"markdown","50962005":"markdown","77ebc339":"markdown","ff0dbb87":"markdown","61c05520":"markdown","0e995250":"markdown","bcb11d41":"markdown"},"source":{"de0ebfa2":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndata = pd.read_csv('..\/input\/weatherAUS.csv')","57c772c0":"data.shape","596315e6":"data.columns","aa013e09":"data.info()","6c3a1c17":"data.head()","81166180":"data_null_percent = pd.Series(index=data.columns)\n\nfor column_name in data:\n    data_null_percent[column_name] = data[column_name].count()\/data.shape[0]\n    \ndata_null_percent_sorted = data_null_percent.sort_values()","d836c34f":"data_null_percent_sorted.plot.barh()","3bc60740":"data = data.drop(columns=['Cloud9am','Cloud3pm', 'Evaporation', 'Sunshine','RISK_MM'])","5d12a8c2":"data = data.dropna()\ndata.isnull().any()","5e081b74":"data.shape","2c70466e":"data.head()","02f5a5d3":"from sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(data, test_size=0.2)","e463a603":"print(\"train: \" + str(train.shape) + \", test: \" + str(test.shape))","d0c8e8ef":"train[\"RainToday\"] = train[\"RainToday\"].map({\"No\":0, \"Yes\":1})\ntrain[\"RainTomorrow\"] = train[\"RainTomorrow\"].map({\"No\":0, \"Yes\":1})\n\ntest[\"RainToday\"] = test[\"RainToday\"].map({\"No\":0, \"Yes\":1})\ntest[\"RainTomorrow\"] = test[\"RainTomorrow\"].map({\"No\":0, \"Yes\":1})","acc98ee9":"def category_impact_plot(variable, subplot_position):\n    plt.subplot(subplot_position)\n    pd.pivot_table(train, index=variable, values='RainTomorrow').plot.bar(figsize=(25,5), ax=plt.gca()) \n   \nplt.figure(1)\ncategory_impact_plot(\"WindGustDir\", 131)\ncategory_impact_plot(\"WindDir9am\", 132)\ncategory_impact_plot(\"WindDir3pm\", 133)\n","552cf1a2":"categorical_variables = [\"WindGustDir\", \"WindDir9am\", \"WindDir3pm\"]\n\ntrain = pd.get_dummies(train, columns=categorical_variables)\ntest = pd.get_dummies(test, columns=categorical_variables)","a28cd46a":"train.head()","56cf3f85":"location_pivot = train.pivot_table(index=\"Location\", values=\"RainTomorrow\")\nlocation_pivot_sorted = location_pivot.sort_values(by=[\"RainTomorrow\"])\n\nlocation_pivot_sorted.plot.barh(figsize=(10,12))\nplt.ylabel('')","ec74b5e3":"train = pd.get_dummies(train, columns=[\"Location\"])\ntest = pd.get_dummies(test, columns=[\"Location\"])","c8c05d42":"train[\"Month\"] = pd.to_datetime(train[\"Date\"]).dt.month\ntest[\"Month\"] = pd.to_datetime(test[\"Date\"]).dt.month","125781ba":"date_pivot = train.pivot_table(index=\"Month\", values=\"RainTomorrow\")#.sort_index(ascending=False)\n\ndate_pivot.plot.barh()\nplt.ylabel('')","dc9be1d0":"train = pd.get_dummies(train, columns=[\"Month\"])\ntest = pd.get_dummies(test, columns=[\"Month\"])","7fffe329":"# the preprocessing.minmax_scale() function allows us to quickly and easily rescale our data\nfrom sklearn.preprocessing import minmax_scale\n\n# Added 2 backets to make it a dataframe. Otherwise you will get a type error stating cannot iterate over 0-d array.\ndef apply_minmax_scale(dataset, features):\n    for feature in features:\n        dataset[feature] = minmax_scale(dataset[[feature]])\n        \nnumerical_features = [\"MinTemp\",\"MaxTemp\", \"Rainfall\", \"WindGustSpeed\", \"WindSpeed9am\",\n                     \"WindSpeed3pm\", \"Humidity9am\", \"Humidity3pm\", \"Pressure9am\", \n                     \"Pressure3pm\", \"Temp9am\", \"Temp3pm\"]\n\napply_minmax_scale(train, numerical_features)\napply_minmax_scale(test, numerical_features)\n\ntrain[numerical_features].head()","6d37714b":"rainTomorrow_yes = train[train[\"RainTomorrow\"] == 1]\nrainTomorrow_no = train[train[\"RainTomorrow\"] == 0]","dffc96ac":"def variable_impact_plot(variable, subplot_position):\n    plt.subplot(subplot_position)\n    rainTomorrow_yes[variable].plot.hist(figsize=(25,10), alpha=0.5, color=\"blue\", bins=50, ax=plt.gca())\n    rainTomorrow_no[variable].plot.hist(figsize=(25,10), alpha=0.5, color=\"yellow\", bins=50, ax=plt.gca())\n    plt.ylabel('')\n    plt.xticks([], [])\n    plt.yticks([], [])\n    plt.title(variable)\n\nplt.figure(1)\nvariable_impact_plot(\"MinTemp\", 341)\nvariable_impact_plot(\"MaxTemp\", 342)\nvariable_impact_plot(\"Rainfall\", 343)\nvariable_impact_plot(\"WindGustSpeed\", 344)\nvariable_impact_plot(\"WindSpeed9am\", 345)\nvariable_impact_plot(\"WindSpeed3pm\", 346)\nvariable_impact_plot(\"Humidity9am\", 347)\nvariable_impact_plot(\"Humidity3pm\", 348)\nplt.figure(2)\nvariable_impact_plot(\"Pressure9am\", 341)\nvariable_impact_plot(\"Pressure3pm\", 342)\nvariable_impact_plot(\"Temp9am\", 343)\nvariable_impact_plot(\"Temp3pm\", 344)","8bdb6c89":"# columns we will be using all the way down\ncolumns = list(train.columns[1:])\ncolumns.remove(\"RainTomorrow\")","4c921aa2":"import seaborn as sns\n\n# custom function to set the style for heatmap\ndef plot_correlation_heatmap(df):\n    corr = df.corr()\n    sns.set(style=\"white\")\n    mask = np.zeros_like(corr, dtype=np.bool)\n    mask[np.triu_indices_from(mask)] = True\n\n    f, ax = plt.subplots(figsize=(30, 25))\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n    plt.show()\n\nplot_correlation_heatmap(train[columns])","27d9acd8":"# Applying Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlogisticRegression = LogisticRegression()\nlogisticRegression.fit(train[columns], train[\"RainTomorrow\"])\ncoefficients = logisticRegression.coef_\nprint(coefficients)","16ba931e":"feature_importance = pd.Series(coefficients[0], index=columns)\nprint(feature_importance)","17c14dc1":"# Plotting as a horizontal Bar chart\nfeature_importance.plot.barh(figsize=(10,25))\nplt.show()","bef05712":"ordered_feature_importance = feature_importance.abs().sort_values()\nordered_feature_importance.plot.barh(figsize=(10,25))\nplt.show()","7394b858":"predictors = [\"Pressure3pm\", \"WindGustSpeed\", \"Pressure9am\", \"Humidity3pm\"]\n\nlr = LogisticRegression()\nlr.fit(train[predictors], train[\"RainTomorrow\"])\npredictions = lr.predict(test[predictors])\nprint(predictions)","63f085e5":"# Calculating the accuracy using the k-fold cross validation method with k=10\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(lr, train[predictors], train[\"RainTomorrow\"], cv=10)\nprint(scores)","78d58c86":"# Taking the mean of all the scores\naccuracy = scores.mean()\nprint(accuracy)","eca8e8ba":"# Does Location affect the formation of rain?","29f93c50":"The dataset has below columns :\n*  **DateThe** \u2014 date of observation\n*  **Location** \u2014 The common name of the location of the weather station\n*  **MinTemp** \u2014 The minimum temperature in degrees celsius\n*  **MaxTemp** \u2014 The maximum temperature in degrees celsius\n*  **Rainfall** \u2014 The amount of rainfall recorded for the day in mm\n*  **Evaporation** \u2014 The so-called Class A pan evaporation (mm) in the 24 hours to 9am\n*  **Sunshine** \u2014 The number of hours of bright sunshine in the day.\n*  **WindGustDir** \u2014 The direction of the strongest wind gust in the 24 hours to midnight\n*  **WindGustSpeed** \u2014 The speed (km\/h) of the strongest wind gust in the 24 hours to midnight\n*  **WindDir9am** \u2014 Direction of the wind at 9am\n*  **WindDir3pm** \u2014 Direction of the wind at 3pm\n*  **WindSpeed9am** \u2014 Wind speed (km\/hr) averaged over 10 minutes prior to 9am\n*  **WindSpeed3pm** \u2014 Wind speed (km\/hr) averaged over 10 minutes prior to 3pm\n*  **Humidity9am** \u2014 Humidity (percent) at 9am\n*  **Humidity3pm** \u2014 Humidity (percent) at 3pm\n*  **Pressure9am** \u2014 Atmospheric pressure (hpa) reduced to mean sea level at 9am\n*  **Pressure3pm** \u2014 Atmospheric pressure (hpa) reduced to mean sea level at 3pm\n*  **Cloud9am** \u2014 Fraction of sky obscured by cloud at 9am. This is measured in \"oktas\", which are a unit of eigths. It records how many eigths of the sky are obscured by cloud. A 0 measure indicates completely clear *  sky whilst an 8 indicates that it is completely overcast.\n*  **Cloud3pm** \u2014 Fraction of sky obscured by cloud (in \"oktas\": eighths) at 3pm. See Cload9am for a description of the values\n*  **Temp9am** \u2014 Temperature (degrees C) at 9am\n*  **Temp3pm** \u2014 Temperature (degrees C) at 3pm\n*  **RainToday** \u2014 Boolean: 1 if precipitation (mm) in the 24 hours to 9am exceeds 1mm, otherwise 0\n*  **RISK_MM** \u2014 The amount of rain. A kind of measure of the \"risk\".\n*  **RainTomorrow** \u2014 The target variable. Did it rain tomorrow?","5357f462":"Yes, **Location** obviously affect the formation of tomorrow's rain! So, we're going to use this variable, and in order to use this categorical variable we have to create dummies.","18d995a5":"# Importing libraries and dataset","dc687854":"There's a certain tendency, season 6-8 is a rainy season.","afa3aa56":"The type of machine learning we will be doing is called **classification**, because when we make predictions we are classifying each day as rainy or not. More specifically, we are performing **binary classification**, which means that there are only two different states we are classifying.","3c8b3686":"Visualization of how categorical variables impact on forming tomorrow's rain","c23c2866":"We can see that there is correlation about 30-50% between some variables. That's not enough to remove one of them and rely on the other.\n\nApart from that, we should remove one of each of our dummy variables to reduce the collinearity in each. We'll remove:\n* WindGustDir_E\n* WindDir9am_E\n* WindDir3pm_E","193ba166":"Create dummy variables for **WindGustDir, WindDir9am, WindDir3pm**","c030edb1":"# Feature selection\nIn order to select the best-performing features, we need a way to measure which of our features are relevant to our outcome - in this case, the impact on forming tomorrow's rain. One effective way is by training a logistic regression model using all of our features, and then looking at the coefficients of each feature.\n\nThe scikit-learn LogisticRegression class has an attribute in which coefficients are stored after the model is fit, LogisticRegression.coef_. We first need to train our model, after which we can access this attribute.","144ba99e":"# Does Date affect the formation of rain?","e7d9c62c":"**Cloud9pm, Cloud3pm, Evaporation, and Sunshine** must be droped since significant amount of records in these columns is missed. Also we should exclude **RISK_MM** because it can leak the answers to the model and reduce its predictability.","3377aa00":"# Exploring the data","1f5b16fa":"# Visualization of how numerical variables impact on forming tomorrow's rain","dd48d834":"# Null values\nLet's get rid of columns with significant amount of null values. And in the rest columns we will drop rows with null values. ","eda10107":"Let's drop rows with null values in them.","9c63f0db":"We'll train a model with the top 4 scores.","a93a6e08":"# Split into train and test\nWe must be aware of one important thing: any change we make to the train data, we also need to make to the test data, otherwise we will be unable to use our model. ","e898ce53":"# Rescaling\nLooking at our numeric columns, we can see a big difference between the range of each.  In order to make sure these values are equally weighted within our model, we'll need to rescale the data.\n\nRescaling simply stretches or shrinks the data as needed to be on the same scale, in our case between 0 and 1.","098bc42f":"The plot we generated shows a range of both positive and negative values. Whether the value is positive or negative isn't as important in this case, relative to the magnitude of the value. If you think about it, this makes sense. A feature that indicates strongly whether a it's not going to rain tomorrow is just as useful as a feature that indicates strongly that a it's going to rain tomorrow, given they are mutually exclusive outcomes.\n\nTo make things easier to interpret, we'll alter the plot to show all positive values, and have sorted the bars in order of size:","50962005":"# Deal with categorical variables\nTo apply such algorithms as Logistic Regression we need to convert the non-numeric data into numeric data. Categorical variables with only 2 possible values can be converted into variables with 0s and 1s as values. For categorical variables with 3 and more possible value we will create dummy variables.","77ebc339":"We are intrested in variables with plots where blue and yellow areas have different shapes. Such variables have impact(positive or negative) on forming tomorrow's rain. The most obvious one is **Humidity3pm**! The rest is not that clear, we will use another feature selection method.","ff0dbb87":"The coef() method returns a NumPy array of coefficients, in the same order as the features that were used to fit the model. To make these easier to interpret, we can convert the coefficients to a pandas series, adding the column names as the index:","61c05520":"The content is daily weather observations from numerous Australian weather stations.\n\nThe target RainTomorrow means: Did it rain the next day? Yes or No.","0e995250":"# Collinearity\nWe now have 73 possible feature columns we can use to train our model. One thing to be aware of as you start to add more features is a concept called collinearity. Collinearity occurs where more than one feature contains data that are similar.\n\nThe effect of collinearity is that your model will overfit - you may get great results on your test data set, but then the model performs worse on unseen data (like the test set).\n\n A common way to spot collinearity is to plot correlations between each pair of variables in a heatmap.","bcb11d41":"Convert values in columns \"RainToday\" and \"RainTomorrow\" from **\"No\" and \"Yes\"** to **0 and 1**."}}