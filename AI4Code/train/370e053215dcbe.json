{"cell_type":{"b8ff1b5e":"code","7f74328a":"code","5bb41f61":"code","e5171ca5":"code","824b35a2":"code","a990f0dd":"code","6d158009":"code","aa3825bc":"code","801cba91":"code","3cd1a48d":"code","814de0d8":"code","93136b44":"code","09841778":"code","c8ccf3c7":"code","0ee9a253":"code","0d134083":"code","9ab75657":"code","31aa6b46":"code","b1a13f85":"code","8abb20cb":"code","b7c008c2":"code","f36f46a4":"code","3fb2e87e":"code","7e9f452c":"code","e9b72c87":"code","365c41b7":"code","38d5320a":"code","957fd49d":"markdown","8d5fb04e":"markdown","f62032cd":"markdown","ce985e9b":"markdown","3494349b":"markdown","20207792":"markdown","bacdc697":"markdown","edb8bcee":"markdown","0272deb7":"markdown","76a9fa01":"markdown","96d3b2a7":"markdown","f202871c":"markdown","5d910127":"markdown","f8784216":"markdown","c6f09de2":"markdown","bd17cdf3":"markdown","eb0efd3d":"markdown","0ed9ba70":"markdown","eef95def":"markdown","60b7ffbb":"markdown","a1bce46f":"markdown","a7e48c76":"markdown","0934a4cd":"markdown","28d934b1":"markdown","8a1cfa0c":"markdown","60f56c16":"markdown","6840e59b":"markdown","c9d07713":"markdown","09758aeb":"markdown","2c94dbbc":"markdown","295ac200":"markdown","bc483a7b":"markdown","2fea0323":"markdown","24e9386e":"markdown","5e0e1d48":"markdown"},"source":{"b8ff1b5e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random as r\nfrom scipy import stats","7f74328a":"titanic = pd.read_csv(\"..\/input\/titanic\/train.csv\")","5bb41f61":"titanic.head()","e5171ca5":"titanic.info()","824b35a2":"plt.hist(titanic[\"Age\"])\nplt.text(0.7, 0.9, f\"Mean: {round(np.mean(titanic.Age), 3)}\", transform = plt.gca().transAxes)\nplt.title(\"Distribution of Age on the Titanic\")\nplt.show()","a990f0dd":"def central_limit_theorem(titanic, column, N, n): #N is number of samples, n is size of each sample\n    \n    titanic = titanic[np.isfinite(titanic[\"Age\"])] #Removing missing data, as we have 714 known ages\n    \n    sample_means = []\n    for i in range(N):\n        sample_means.append(np.mean(r.sample(list(titanic[column]), n))) #For N times, Getting a list of sample means of size n \n    return sample_means\n\ndef clt_graph(titanic, column, N, n):\n    sample_means = central_limit_theorem(titanic, column, N, n)\n    true_mean_diff = round(np.mean(titanic.Age) - np.mean(sample_means), 2)\n    plt.hist(sample_means)\n    plt.text(0.7, 0.9, f\"Mean: {round(np.mean(sample_means), 2)}\", transform = plt.gca().transAxes)\n    plt.text(0.7, 0.8, f\"Difference: {true_mean_diff}\", transform = plt.gca().transAxes)\n    plt.title(f\"Distribution of Sample Age Means (n = {n}; N = {N})\")\n    plt.show()","6d158009":"[clt_graph(titanic, \"Age\", 10000, i) for i in [2, 30, 60, 100]]","aa3825bc":"[clt_graph(titanic, \"Age\", i, 30) for i in [10, 50, 100, 1000]]","801cba91":"#The sample_mean2 paramter allows us to enter another sample mean if we want the probability of a range of two values\n#The greater_than parameter allows us to either get the probability to the left or right of our z-score\n\ndef z_probability_sample_means(titanic, column, n, sample_mean, sample_mean2 = 0, greater_than = False):\n    \n    sample_mean_list = central_limit_theorem(titanic, column, 10000, n) #getting sample means of age, following a normal distribution by CLT.\n    \n    pop_mean = np.mean(sample_mean_list) #Again, this approximates the true population mean if N and n are large enough\n    \n    standard_error = np.std(titanic[column]) \/ np.sqrt(n) #Getting the standard error\n    \n    z_score = (sample_mean - pop_mean) \/ standard_error #Calculating the z-score (std's from the mean)\n    \n    if sample_mean2 != 0: #These statements will activate if we want a probability within a range of two sample means\n        z_score2 = (sample_mean2 - pop_mean) \/ standard_error\n        return f\"Interpretation: The probability that we acquire a sample mean between {round(min(sample_mean, sample_mean2), 2)} and {round(max(sample_mean, sample_mean2), 2)} is {round((stats.norm.cdf(max(z_score, z_score2)) - stats.norm.cdf(min(z_score, z_score2))) * 100, 2)}%\"\n    \n    if greater_than == False:\n        return f\"Interpretation: The probability that we acquire a sample mean less than {sample_mean} is {round(stats.norm.cdf(z_score) * 100, 2)}%\" #Finally, we get the corresponding value from the z-score table\n    else:\n        return f\"Interpretation: The probability that we acquire a sample mean greater than {sample_mean} is {round((1 - stats.norm.cdf(z_score)) * 100, 2)}%\" #Since the z_score area gives us the area to the left, we must subract this area from 1 if we want a probability of a sample mean being greater than some value.","3cd1a48d":"z_probability_sample_means(titanic, \"Age\", sample_mean = 25, n = 50, greater_than = False)","814de0d8":"z_probability_sample_means(titanic, \"Age\", sample_mean = 30, n = 50, greater_than = True)","93136b44":"z_probability_sample_means(titanic, \"Age\", sample_mean = 28, sample_mean2 = 30, n = 50)","09841778":"def z_confidence_interval_sample_mean(titanic, column, n, conf):\n    \n    titanic = titanic[np.isfinite(titanic[\"Age\"])] #Remove missing values\n    \n    sample_mean = np.mean(r.sample(list(titanic[column]), n)) #getting a single sample mean\n    \n    standard_error = np.std(titanic[column]) \/ np.sqrt(n) #same formula we used previously for SE\n    \n    z_star = stats.norm.ppf(((1 - conf) \/ 2) + conf) #For any confidence level, the probability we want from our z table is halfway between that value and 1 because we just take into account both tails of the distribution\n    \n    CI =  sample_mean - (z_star * standard_error), sample_mean + (z_star * standard_error)\n    \n    return f\"Interpretation: With a sample mean of {round(sample_mean, 2)}, we can be {round(conf * 100, 2)}% confident that the true population mean lies between {round(CI[0], 2)} and {round(CI[1], 2)}.\"\n    ","c8ccf3c7":"z_confidence_interval_sample_mean(titanic, \"Age\", n = 50, conf = 0.95)","0ee9a253":"z_confidence_interval_sample_mean(titanic, \"Age\", n = 50, conf = 0.50)","0d134083":"z_confidence_interval_sample_mean(titanic, \"Fare\", n = 50, conf = 0.80)","9ab75657":"def t_confidence_interval_sample_mean(titanic, column, n, conf):\n        \n    titanic = titanic[np.isfinite(titanic[\"Age\"])] #Remove missing values\n    \n    sample = r.sample(list(titanic[column]), n)\n    sample_mean = np.mean(sample) #Getting sample mean and standard deviation from our sample of n <= 30\n    sample_std = np.std(sample)\n    \n    standard_error = sample_std \/ np.sqrt(n) #same formula we used previously for SE\n    \n    t_star = stats.t.ppf(((1 - conf) \/ 2) + conf, n - 1) #Same process as Z table, but df = n - 1 and a t table.\n    \n    CI =  sample_mean - (t_star * standard_error), sample_mean + (t_star * standard_error)\n    \n    return f\"Interpretation: With a sample mean of {round(sample_mean, 2)}, we can be {round(conf * 100, 2)}% confident that the true population mean lies between {round(CI[0], 2)} and {round(CI[1], 2)}.\"","31aa6b46":"t_confidence_interval_sample_mean(titanic, \"Age\", n = 10, conf = 0.95)","b1a13f85":"t_confidence_interval_sample_mean(titanic, \"Age\", n = 25, conf = 0.95)","8abb20cb":"t_confidence_interval_sample_mean(titanic, \"Fare\", n = 25, conf = 0.80)","b7c008c2":"grouped_survival = titanic.groupby(\"Survived\").size()\nplt.bar(grouped_survival.index.values, grouped_survival.values, tick_label = [\"Died\", \"Survived\"])\nplt.title(\"Population Distribution of Titanic Survival\")\nplt.show()","f36f46a4":"def z_confidence_interval_sample_prop(titanic, column, n, conf, success_value):\n    \n    sample = r.sample(list(titanic[column]), n) #Getting our sample of size n\n    \n    success_counter = 0\n    for j in sample: #This section gets a list of proportions occurances for each discrete value, in this case 0 and 1\n        if j == success_value:\n            success_counter += 1\n    p = success_counter \/ len(sample) #We get our sample proportion\n    \n    if n * p * (1 - p) < 10:\n        return \"Normality not achieved.  Get a bigger sample size.\"\n    \n    standard_error = np.sqrt(p * (1 - p) \/ n)\n    \n    z_star = stats.norm.ppf(((1 - conf) \/ 2) + conf)\n    \n    CI =  p - (z_star * standard_error), p + (z_star * standard_error)\n    \n    return f\"Interpretation: With a sample proportion of {round(p, 3)}, we can be {round(conf * 100, 3)}% confident that the true population proportion lies between {round(CI[0], 3)} and {round(CI[1], 3)}.\"","3fb2e87e":"z_confidence_interval_sample_prop(titanic, \"Survived\", 20, 0.95, 1)","7e9f452c":"z_confidence_interval_sample_prop(titanic, \"Survived\", 50, 0.95, 1)","e9b72c87":"z_confidence_interval_sample_prop(titanic, \"Survived\", 50, 0.50, 1)","365c41b7":"z_confidence_interval_sample_prop(titanic, \"Pclass\", 50, 0.50, 1)","38d5320a":"z_confidence_interval_sample_prop(titanic, \"Pclass\", 75, 0.95, 1)","957fd49d":"As we increase n (the size of each sample), the distribution of sample means normalizes more and more, and the spread (standard deviation) of the distribution decreases. Since this is not so easy to visualize here, I also included the difference between the true population mean and the mean of sample means on each plot.  We can see that the difference generally becomes smaller as n increases, showing the mean of sample means approaching the true population mean.","8d5fb04e":"**Definition**: The Z-Score tells us how many standard deviations above or below the mean of a normal distribution an individual data point is.  The basic formula is **Z = (x - mu) \/ sigma**, where x is some data point, mu is the population mean, and sigma is the population standard deviation.  By looking up the corresponding probability in the z table, we can see what proportion of the population is below the data point.  In this case with the Titanic dataset, sample means will be our data points, and we can test how certain ones deviate from the mean of sample means (or population mean).","f62032cd":"We might require a sample size greater than 50 (depends on the behavior of the sample when I run the kernel on Kaggle).  This is because there are more discrete values than within the survival variable, and as the number of discrete values goes up, we need bigger sample sizes to achieve normality as p has a greater chance of being smaller!","ce985e9b":"**Definition**: The Central Limit Theorem (CLT) states that the distribution of sample means approximates a normal distribution (also known as a \u201cbell curve\u201d), as the sample size becomes larger, assuming that all samples are identical in size, and regardless of the population distribution shape.  Typically, the CLT works best when we acquire sample sizes of at least 30.\n\n**Why It's Useful**: Most often, when working with data, distributions aren't perfectly normal.  The CLT allows us to normalize data of any distribution so we can more easily perform confidence intervals and hypothesis testing of sample means, which will be covered in the next section.","3494349b":"First, we will graph the population distribution of survivors for reference.","20207792":"Now that we have a normal distribution of sample means of ages derived with the CLT, we can use it to find the probability that an individual sample mean will fall in a range of values.  It is important to know the z-score formula for specifically dealing with sample means:","bacdc697":"## Central Limit Theorem","edb8bcee":"First, we will load a distribution of the data:","0272deb7":"## The Data","76a9fa01":"**Why It's Useful**: This is useful for the same reasons as our previous tests, except now we are able to collect samples of titanic passengers involving discrete variables instead of continuous variables.","96d3b2a7":"**Definition**: In the previous sections, we have worked with distributions involving continuous variables, as we worked with Titanic passenger ages.  Now, we will work with discrete variables, in this case the boolean survival variable, as we solve confidence intervals for sample proportions.  Now, we are working with a binomial distribution, meaning we have a \"success\" value and every other value is considered a \"failure\".  Now, in order to achieve normality and perform this test, we must satisfy the equation **n * p * (1-p) > 10**, similarly to how we had to satisfy the central limit theorem with sample means.  Then, we will use our z-score table just like before and our confidence interval formula is **Confidence interval bounds = p +- (z star * standard error)**, where **standard error = sqrt(p * (1 - p) \/ n)** and p is the proportion of successes in our sample of size n.","f202871c":"Notice that as our confidence interval becomes bigger, the interval of possible population means becomes bigger.  Of course, we know the population mean of ages on the Titanic, which is about 29.7, but we are pretending we only have access of a sample of the data and the standard deviation of the population.\n\nNow, let's perform a confidence interval on a different continuous variable, the fare to get on the Titanic.","5d910127":"## Hypothesis Testing and Errors (Coming Soon!)","f8784216":"We know the mean age (population mean, or mu) on the Titanic was about 29.7.  We will calculuate the probability that we acquire values less\/greater than or within a range of sample means.","c6f09de2":"When N is small, the distribution os sample means is not at all normal, but as it gets larger, the distribution approaches normality.  This illustrates the **law of large numbers**,  meaning the distribution approaches its true shape as N becomes larger.  This applies to distributions of any shape.","bd17cdf3":"Clearly, this is not a normal distribution, as the titanic contained far fewer 10-20 year olds than 20-30 year olds. For the Central Limit Theorem, however, the population distribution is irrelevant as our distribution of sample means will normalize regardless.  The population mean is 29.699.","eb0efd3d":"The same concepts as before exist in relation to higher confidence levels resulting in larger intervals. The formulas are just slightly different.  We can also see that 20 is not a big enough sample size to normalize our distribution, but 50 should be!\n\nNow, let's run this test on a different discrete variable, the passenger class, and define a success as 1st class.","0ed9ba70":"We observe similar patterns as we did with our z probabilities, but notice that our interval becomes smaller as we increase n and therefore increase degrees of freedom.  This is because when our sample size gets bigger, we approach normality and are able to make a better guess of our population mean.  Keep in mind that our interval also becomes larger when we increase the confidence level.  \n\nNow, let's test this on the Fare variable again.","eef95def":"Now, let's see what happens for different amounts of samples (N):","60b7ffbb":"**Z = (sample mean - population mean) \/ (standard error)**","a1bce46f":"**Why It's Useful**: In the case of the titanic dataset, let's say someone dove into the ocean and was somehow able to acquire a random sample of ages of passengers before having to swim back to shore.  They might want to test the accuracy of their sample by comparing its mean to the true population mean, and figuring out the probability that a value below or above their sample mean could have occurred.  This way, they could see if there's a high or low probability of bias in the way they sampled (we will also cover hypothesis testing later on).","a7e48c76":"On Kaggle, it is easy to find EDA, visualizations, and machine learning algorithms, though it is harder to come across notebooks illustrating how we can use probability and mathematical statistics with our data.  This is because we are given so much information through our datasets, that we can easily model and story-tell, which is Kaggle's main purpose.  In real life on the job, however, data analysts, data scientists, and statisticians must have a firm grasp on statistics and probability because many problems involve more unknowns, and sometimes probability, hypothesis testing, etc may be necessary to derive insights about the population.\n\nIn this notebook, I will provide a tutorial on representing various statistical concepts in Python.  I will use the well-known Titanic dataset in order to show how we can apply statistical methods to real-life data.  This notebook will continue being updated throughout the next few months, and will follow the cirriculum of both my Regression and Mathematical Statistics classes.  Leave a like and\/or comment if you enjoy or have any suggestions! \n\nNote: This notebook assumes have an understanding of basic statistics (knowing some formulas and definitions of mean, variance, normality, etc).  The main purpose is brushing up on these concepts and showing how we can implement statistics into our datasets in Python beyond simple EDA and visualizations.","0934a4cd":"# A Guide to Statistics and Probability in Python","28d934b1":"## Z-Score Probabilities With Sample Means","8a1cfa0c":"In the case of the Titanic data, we only have 891 out of over 2200 total passengers on board, though this is easily a big enough sample to well represent the population.  **Therefore, we will treat the 891 passengers in our dataset as the population, but we will often assume we don't have all of this information throughout the tutorial, as these statistical concepts are most useful when we only acquire small samples.  The point is that we can use our knowledge of the actual population statistics to prove the validity of our work.  In practice, these concepts will be most useful when working with a dataset that only represents a sample.**\n\nNote that there are some missing values in the age column, so we will drop those and have 714 values when working with age data.","60f56c16":"## T Confidence Intervals With Sample Means","6840e59b":"First, we will load our libraries and the data.","c9d07713":"**Z = (sample mean - population mean) \/ (population standard deviation \/ sqrt(sample size))**","09758aeb":"**Definition**: Confidence intervals of sample means can give us information on the population mean of data when we acquire a single sample of data and know the population standard deviation, but not the corresponding mean.  We take our single sample mean and use it to get a range of values with the following formula: **Confidence interval bounds = sample mean +- (z star * standard error)**.  We already know the formula for standard error from before, and we get z star by locating the corresponding z-value (using the z-score table) of probability halfway between our desired confidence level and 100% to account for both tails of the normal distribution.  For example, if we want a 95% confidence interval, which is standard, we locate the z_value corresponding to 0.975, which is 1.96.  With the two bounds we get from the formula, we can be 95% (or any desired confidence level) confident that the true population mean lies between the two values.\n\n**Why It's Useful**: It is rare that we know the population standard deviation without knowing the population mean, but when this does happen, it's useful to estimate the interval containing the population mean with some degree of certaintly.  Typically, this degree is 95%.  Perhaps we acquire a mean of a sample of titanic passenger ages, and want to know the certainty that the true mean falls in some interval so we can manage the risk of spreading false information.  ","2c94dbbc":"We will define our success in the binomial distribution as surviving, so the population proportion value is **0.384**.  Of course, in practice when performing these tests, we do not have access to this value as we are attempting to estimate it, but we will use it as reference to get an idea of the accuracy of our intervals.","295ac200":"## Z Confidence Intervals With Sample Means","bc483a7b":"## Z Confidence Intervals With Sample Proportions","2fea0323":"**Definition**: This will be very similar to our previous section where we used z scores to compute confidence intervals.  However, now let's say we acquire a sample size of under 30, so the central limit theorem no longer holds, and we have no access to the population standard deviation.  We will require a value for degrees of freedom, which is the number of values that we allow to vary in our dataset to account for the lack of normality, always **df = sample size - 1**.  Now, our formula will be **Confidence interval bounds = sample mean +- (t star * standard error)**, where our standard error is now **sample standard deviation \/ sqrt(sample size)** and our t star value is calculated with the t-score table, selecting a a probability value and number of degrees of freedom.","24e9386e":"**Why It's Useful**: This is useful for the same reasons as our Z confidence intervals, but now let's say we are only able to acquire sample sizes of less than 30 of passenger ages on the titanic.  ","5e0e1d48":"The overall denominator of this formula represents the standard deviation of the sample (standard error), meaning the formula can also be written as:"}}