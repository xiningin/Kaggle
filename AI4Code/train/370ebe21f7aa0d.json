{"cell_type":{"3c246962":"code","e1ececc9":"code","b03c57fd":"code","45c25959":"code","cb66b48a":"markdown"},"source":{"3c246962":"import functools\nfrom IPython.core.display import display, HTML\nfrom nltk import PorterStemmer\nimport torch\nfrom transformers import *\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n","e1ececc9":"import numpy as np\nimport pandas as pd\n\n# keep only docsuments with covid -cov-2 and cov2\ndef search_focus(df):\n    dfa = df[df['abstract'].str.contains('covid')]\n    dfb = df[df['abstract'].str.contains('-cov-2')]\n    dfc = df[df['abstract'].str.contains('cov2')]\n    dfd = df[df['abstract'].str.contains('ncov')]\n    frames=[dfa,dfb,dfc,dfd]\n    df = pd.concat(frames)\n    df=df.drop_duplicates(subset='title', keep=\"first\")\n    return df\n\n# load the meta data from the CSV file using 3 columns (abstract, title, authors),\ndf=pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv', usecols=['title','journal','abstract','authors','doi','publish_time','sha','full_text_file'])\nprint (df.shape)\n#drop duplicates\n#df=df.drop_duplicates()\n#drop NANs \ndf=df.fillna('no data provided')\ndf = df.drop_duplicates(subset='title', keep=\"first\")\n# convert abstracts to lowercase\ndf[\"abstract\"] = df[\"abstract\"].str.lower()+df[\"title\"].str.lower()\ndf = df[df['publish_time'].str.contains('2020')]\n#show 5 lines of the new dataframe\ndf=search_focus(df)\nprint (df.shape)\ndf.head()","b03c57fd":"import os\nimport json\nfrom pprint import pprint\nfrom copy import deepcopy\nimport math\nfrom IPython.core.display import display, HTML\n\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    \n    for section, text in texts:\n        texts_di[section] += text\n\n    body = \"\"\n\n    for section, text in texts_di.items():\n        #print (section)\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    #print('_______')\n    return body\n\ndf['body']=''\ndf['method_results']=''\nfor index, row in df.iterrows():\n    if os.path.exists('\/kaggle\/input\/CORD-19-research-challenge\/'+row['full_text_file']+'\/'+row['full_text_file']+'\/pdf_json\/'+row['sha']+'.json'):\n        with open('\/kaggle\/input\/CORD-19-research-challenge\/'+row['full_text_file']+'\/'+row['full_text_file']+'\/pdf_json\/'+row['sha']+'.json') as json_file:\n            data = json.load(json_file)\n            body=format_body(data['body_text'])\n            #print (body)\n            body=body.replace(\"\\n\", \"\")\n            body=body.replace(\",\", \"\")\n            body=body.lower()\n            df.loc[index, 'body'] = body\n            if body!='' and 'method' in body and 'results' in body:\n                temp=body.split('method')[1]\n                method=temp.split('results')[0]\n                df.loc[index, 'method_results'] = method\n\ndf=df[df['body'].str.contains('severe')]\ndf=df[df['body'].str.contains('risk')]\nprint (df.shape)\ndf.head()","45c25959":"#tell the system how many sentences are needed\nmax_sentences=5\n\n# function to stem keywords into a common base word\ndef stem_words(words):\n    stemmer = PorterStemmer()\n    singles=[]\n    for w in words:\n        singles.append(stemmer.stem(w))\n    return singles\n\n# BERT pretrained question answering module\ndef answer_question(question,text,tokenizer,model):\n    input_text = \"[CLS] \" + question + \" [SEP] \" + text + \" [SEP]\"\n    input_ids = tokenizer.encode(input_text)\n    token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]\n    start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))\n    all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n    #print(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\n    answer=(' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\n    # show qeustion and text\n    #tokenizer.decode(input_ids)\n    return answer\n\n\n# list of lists for topic words realting to tasks\ndisplay(HTML('<h1>COVID-19 Risk Factors<\/h1>'))\n#display(HTML('<h3>Table of Contents (ctrl f to search for the hash tagged words below to find that data table)<\/h3>'))\n#tasks = [['comorbidities','comorbid'],['risk factor','risk factors'],['cancer patient', 'cancer patients'],['hypertension','hyperten'],['heart', 'disease'],['chronic', 'bronchitis'],['cerebral', 'infarction'],['diabetes', 'diabete'],['copd','copd'],[\"blood type\",\"type\"],['smoking','smok'],['basic','reproductive','number'],[\"incubation\", \"period\", \"days\"]]\n#tasks = ['hypertension','diabetes','heart disease','male','copd','smoking','age','stroke','cerbrovascular','cancer','respiratory','kidney disease','drinking','tuberculosis','bmi']\ntasks = ['age']\nz=0\nfor terms in tasks:\n    stra=' '\n    stra=' '.join(terms)\n    k=str(z)\n    z=z+1\n# loop through the list of lists\nz=0\nfor search_words in tasks:\n    df_table = pd.DataFrame(columns = [\"pub_date\",\"title\",\"link\",\"journal\",\"severe\",\"sever sig.\",\"severe age adj.\",\"Severe OR Calculated or Extracted\",\"fatality\",\"fatality sig.\",\"fatality age adj.\",\"Fatality OR Calculated or Extracted\",\"design\",\"sample\",\"study pop.\",\"risk factor\"])\n    str1=''\n    # a make a string of the search words to print readable search\n    str1=search_words\n    df1=df[df['body'].str.contains(search_words)]\n    \n    display(HTML('<h3>Task Topic: '+str1+'<\/h3>'))\n    #display(HTML('# '+str1+' <a><\/a>'))\n    z=z+1\n    # record how many sentences have been saved for display\n    # loop through the result of the dataframe search\n    for index, row in df1.iterrows():\n        odds=''\n        pub_sentence=''\n        sentences_used=0\n        #break apart the absracrt to sentence level\n        sentences = row['body'].split('. ')\n        #loop through the sentences of the abstract\n        for sentence in sentences:\n            # missing lets the system know if all the words are in the sentence\n            missing=0\n            #loop through the words of sentence\n            for word in search_words:\n                #if keyword missing change missing variable\n                if word not in sentence:\n                    missing=missing+1\n            if tasks[0][0] in sentence and 'or=' in sentence or tasks[0][0] in sentence and 'hr=' in sentence or tasks[0][0] in sentence and 'rr=' in sentence or tasks[0][0] in sentence and 'aor=' in sentence or tasks[0][0] in sentence and 'ahr=' in sentence:\n                odds=odds+'...'+sentence\n            # after all sentences processed show the sentences not missing keywords limit to max_sentences\n            if missing < len(search_words)-1 and sentences_used < max_sentences and len(sentence)<1000 and sentence!='':\n                sentence=sentence.capitalize()\n                if sentence[len(sentence)-1]!='.':\n                    sentence=sentence+'.'\n                pub_sentence=pub_sentence+'<br><br>'+sentence\n        if pub_sentence!='':\n            sentence=pub_sentence\n            sentences_used=sentences_used+1\n            if row['method_results']!='':\n                text=row['method_results'][ 0 : 1000 ]\n            else:\n                text=row['abstract'][ 0 : 1000 ]\n            \n            question='how many patients or cases were in the study, review or analysis?'\n            sample=answer_question(question,text,tokenizer,model)\n            sample=sample.replace(\"#\", \"\")\n            sample=sample.replace(\" , \", \",\")\n            if sample=='19' or sample=='' or '[SEP]'in sample:\n                sample='unk'\n            if len(sample)>50:\n                sample='unk'\n            sample=sample.replace(\" \", \"\")\n            \n            question='what type or kind of review or study was conducted?'\n            design=answer_question(question,text,tokenizer,model)\n            design=design.replace(\" ##\", \"\")\n            if '[SEP]'in design or '[CLS]' in design or len(design)>75:\n                design='unk'\n            \n            question='what is the name of the hospital or country?'\n            study_pop=answer_question(question,text,tokenizer,model)\n            study_pop=study_pop.replace(\" ##\", \"\")\n            if '[SEP]'in study_pop:\n                study_pop='unk'\n                \n            #question='How many severe outcomes?'\n            #severe=answer_question(question,text,tokenizer,model)\n            #severe=severe.replace(\" ##\", \"\")\n            #severe=severe.replace(\" . \", \".\")\n            #severe=severe.replace(\" \u00b7 \", \".\")\n            #if '[SEP]'in severe:\n                #severe='unk'\n            \n            #question='How many fatalities of deaths?'\n            #fatality=answer_question(question,text,tokenizer,model)\n            #fatality=fatality.replace(\" ##\", \"\")\n            #fatality=fatality.replace(\" . \", \".\")\n            #fatality=fatality.replace(\" \u00b7 \", \".\")\n            #if '[SEP]'in fatality:\n                #fatality='unk'\n            \n            \n            \n            #question='what was the or= with '+tasks[0][0]+'?'\n            #calc=answer_question(question,text,tokenizer,model)\n            \n            authors=row[\"authors\"].split(\" \")\n            link=row['doi']\n            title=row[\"title\"]\n            linka='https:\/\/doi.org\/'+link\n            linkb=title\n            journal=row['journal']\n            if journal=='no data provided':\n                journal=row['full_text_file']\n            sentence='<p align=\"left\">'+sentence+'<\/p>'\n            final_link='<p align=\"left\"><a href=\"{}\">{}<\/a><\/p>'.format(linka,linkb)\n            #to_append = [row['publish_time'],title,linka,row['journal'],severe,'ss','saa','sOR','fatality','fs','faa','fOR',design,sample,study_pop]\n            to_append = [row['publish_time'],title,linka,journal,odds,'-','-','-','-','-','-','-',design,sample,study_pop,search_words]\n            df_length = len(df_table)\n            df_table.loc[df_length] = to_append\n    filename=str1+'.csv'\n    df_table=df_table.sort_values('pub_date', ascending=False)\n    df_table.to_csv(filename,index = False)\n        #display(HTML('<b>'+sentence+'<\/b> - <i>'+title+'<\/i>, '+'<a href=\"https:\/\/doi.org\/'+link+'\" target=blank>'+authors[0]+' et al.<\/a>'))\n    df_table=HTML(df_table.to_html(escape=False,index=False))\n    display(df_table)\nprint (\"done\")","cb66b48a":"# Risk Factor BERT QA fills table for curation\n\nHaving some success filling columns using a simple pandas based search and relevance scoring at sentence level to locate relevant articles using only abstracts and using BERT question answering to answer questions about some of the column values.\n\nStill a work in progress, but wanted to share in case someone finds this useful."}}