{"cell_type":{"d1bc5979":"code","851756dc":"code","3d15eee6":"code","e8d429f5":"code","346b4d25":"code","b34db2d1":"code","729b1fbe":"code","75282069":"code","ea732d60":"code","163ab46f":"code","35ff05b8":"code","e4f7c5bf":"code","c976380b":"code","d2a8984a":"code","2e1456f4":"code","8dc6de60":"code","8dbfb8f1":"code","7d9ec3f9":"code","eb3736b8":"code","709bd60e":"code","52f5d0e4":"code","5c715be3":"code","940b8e50":"code","111bd481":"code","601fb646":"code","550ef77e":"code","53c61369":"code","46c6ff95":"code","a02eca88":"code","afa14040":"code","92cb6c75":"code","118e0b81":"code","3a59af7d":"code","281784c9":"code","08d6653c":"code","8a9fa17a":"code","0bbb6479":"code","6a19a750":"code","a921ddaf":"markdown","ab81b300":"markdown","68e940a3":"markdown","418f3f9e":"markdown","ea4a8b1e":"markdown","0c510755":"markdown","a49c3e2d":"markdown","e4c5748b":"markdown","74ebbab6":"markdown"},"source":{"d1bc5979":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as  plt\nimport sklearn\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","851756dc":"data= pd.read_csv(\"..\/input\/german-credit-data-with-risk\/german_credit_data.csv\")\ndata\n","3d15eee6":"data= data.drop(['Unnamed: 0'], axis=1)\ndata","e8d429f5":"from sklearn.preprocessing import LabelBinarizer\nlb= LabelBinarizer()\ndata[\"Risk\"]= lb.fit_transform(data[\"Risk\"])","346b4d25":"sns.countplot('Risk', data=data)\nplt.title('Risk Distribution', fontsize=14)\nplt.show()","b34db2d1":"ax = sns.scatterplot(x=\"Duration\", y=\"Age\", hue=\"Risk\", data=data)","729b1fbe":"ax = sns.scatterplot(x=\"Age\", y=\"Duration\", hue=\"Risk\", data=data)","75282069":"ax = sns.scatterplot(x=\"Credit amount\", y=\"Age\", hue=\"Risk\", data=data)","ea732d60":"from scipy.stats import norm\n\nf, (ax1,ax2) =plt.subplots(1,2, figsize=(20, 6))\n\ncredit_amount_dist = data['Credit amount'].loc[data['Risk'] == 1].values\nsns.distplot(credit_amount_dist,ax=ax1, fit=norm, color='#FB8861')\nax1.set_title('Credit amount distribution for good transactions', fontsize=14)\n\ncredit_amount_dist = data['Credit amount'].loc[data['Risk'] == 0].values\nsns.distplot(credit_amount_dist,ax=ax2, fit=norm, color='#56F9BB')\nax2.set_title('Credit amount distribution for bad transactions', fontsize=14)\n","163ab46f":"from sklearn.preprocessing import StandardScaler\nSC= StandardScaler()\ncredit=data['Credit amount'].values\ncredit= credit.reshape(-1,1)\ndata[\"Credit amount\"]= SC.fit_transform(credit)","35ff05b8":"Saving_accounts= data[\"Saving accounts\"]\nSaving_accounts.isnull().values.sum()\n\n","e4f7c5bf":"Checking_accounts= data[\"Checking account\"]\nChecking_accounts.isnull().values.sum()\n\n","c976380b":"data[\"Saving accounts\"].fillna('NoSavingAcc', inplace= True)\ndata[\"Checking account\"].fillna('NoCheckAcc', inplace= True)","d2a8984a":"interval = (0, 12, 24, 36, 48, 60, 72, 84)\ncats = ['year1', 'year2', 'year3', 'year4', 'year5', 'year6', 'year7']\ndata[\"Duration\"] = pd.cut(data.Duration, interval, labels=cats)","2e1456f4":"interval = (18, 25, 35, 60, 120)\n\ncats = ['Student', 'Youth', 'Adult', 'Senior']\ndata[\"Age\"] = pd.cut(data.Age, interval, labels=cats)","8dc6de60":"data","8dbfb8f1":"data = data.merge(pd.get_dummies(data.Purpose, drop_first=True, prefix='Purpose'), left_index=True, right_index=True)\ndata = data.merge(pd.get_dummies(data.Sex, drop_first=True, prefix='Sex'), left_index=True, right_index=True)\ndata = data.merge(pd.get_dummies(data[\"Saving accounts\"], drop_first=True, prefix='Savings'), left_index=True, right_index=True)\ndata = data.merge(pd.get_dummies(data[\"Checking account\"], drop_first=True, prefix='Check'), left_index=True, right_index=True)\ndata = data.merge(pd.get_dummies(data.Housing, drop_first=True, prefix='Housing'), left_index=True, right_index=True)\ndata = data.merge(pd.get_dummies(data.Job, drop_first=True, prefix='Job'), left_index=True, right_index=True)\ndata = data.merge(pd.get_dummies(data.Duration, drop_first=True, prefix='Duration'), left_index=True, right_index=True)\ndata = data.merge(pd.get_dummies(data.Age, drop_first=True, prefix='Age'), left_index=True, right_index=True)","7d9ec3f9":"del data[\"Checking account\"]\ndel data[\"Saving accounts\"]\ndel data[\"Job\"]\ndel data[\"Duration\"]\ndel data[\"Sex\"]\ndel data[\"Purpose\"]\ndel data[\"Housing\"]\ndel data[\"Age\"]","eb3736b8":"X= data.drop('Risk', axis= 1)\ny=data[\"Risk\"]","709bd60e":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.25, random_state= 0)\n","52f5d0e4":"X_train","5c715be3":"from sklearn.decomposition import TruncatedSVD\nsvd = TruncatedSVD(n_components=17, n_iter=7, random_state= 0)\nX_train_svd= svd.fit_transform(X_train)","940b8e50":"explained_variance=svd.explained_variance_ratio_\nexplained_variance","111bd481":"with plt.style.context('dark_background'):\n    plt.figure(figsize=(10, 10))\n    plt.bar(range(17), explained_variance, alpha=0.5, align='center',\n            label='individual explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.legend(loc='best')\n    plt.tight_layout()","601fb646":"X_train_svd= pd.DataFrame(X_train_svd)\nX_train_svd","550ef77e":"X_test_svd= svd.transform(X_test)\n","53c61369":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report #To evaluate our model\nfrom sklearn.model_selection import GridSearchCV\n# Algorithmns models to be compared\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier","46c6ff95":"classifier = LogisticRegression()\nparameters = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\ngrid_search = GridSearchCV(estimator= classifier,param_grid= parameters, cv=5,  n_jobs= -1)","a02eca88":"grid_search.fit(X_train, y_train)\ny_pred = grid_search.predict(X_test)\n","afa14040":"cm= confusion_matrix(y_test, y_pred)\nlabels = ['Bad', 'Good']\nprint(classification_report(y_test, y_pred, target_names=labels))","92cb6c75":"parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\nsvc = SVC()\ngrid_search = GridSearchCV(estimator= svc, param_grid= parameters, cv=5, n_jobs= -1)","118e0b81":"grid_search.fit(X_train, y_train)\ny_pred = grid_search.predict(X_test)","3a59af7d":"cm= confusion_matrix(y_test, y_pred)\nlabels = ['Bad', 'Good']\nprint(classification_report(y_test, y_pred, target_names=labels))","281784c9":"parameters = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n 'criterion' :['gini', 'entropy']\n}\nclassifier= RandomForestClassifier()\ngrid_search= GridSearchCV(estimator=classifier, param_grid=parameters, cv= 5, n_jobs= -1)","08d6653c":"grid_search.fit(X_train, y_train)\ny_pred = grid_search.predict(X_test)","8a9fa17a":"cm= confusion_matrix(y_test, y_pred)\nlabels = ['Bad', 'Good']\nprint(classification_report(y_test, y_pred, target_names=labels))","0bbb6479":"classifier= XGBClassifier()\nclassifier.fit(X_train, y_train)\ny_pred= classifier.predict(X_test)","6a19a750":"cm= confusion_matrix(y_test, y_pred)\nlabels = ['Bad', 'Good']\nprint(classification_report(y_test, y_pred, target_names=labels))","a921ddaf":"Upon analysis, we notice that there are only 2 columns which have missing values, Savings account and Checking account.  It is likely that there were no mistakes in updating the datasets, but the users didnt actually have a savings or a checking account. We'll impute the 'NaN' by 'NoSavingAcc'\/ 'NoChecAcc and only then. we will LabelEncode\/ OneHotEncode the data. ","ab81b300":"The distribution is uneven in the sense that there are nearly 700 good transactions and 300 bad transactions\nThis is a very likely scenario because from a dataset of large no. of transactions, there are obviously more no of genuine transactions.\nBasically, our dataset is imbalanced as a primitive model which preicts 1 always will also obtain an accuracy of 70% .\n2 things can be done: \n1. Resample (Over-sample\/ Under-sample) the dataset and obtain an even distribution of the 2 classes.\n2. Use the precision\/Recall scores to evaluate the model.\nWe'll first make visualizations on our dataset, and then use these techniques.","68e940a3":"We'll convert age column into a categorical column by creating intervals . This will help us know the customer base in a broader sense. We'll create an interval for Students, Youth, Adults and Senior Citizens.","418f3f9e":"An inference that can be made regarding the Credit amount is that people with lower credit amount have a risk possibiity of 1, i.e. those transactions are likely to be genuine.","ea4a8b1e":"We'll first convert the target variable (**Risk**) to numeric form so that we can make some visualizations. We'll be using **LabelBinarize**r function present in the sklearn library for that purpose,","0c510755":"I have imported an updated version of the dataset with an additional column for risk profile- Good\/Bad to make predictions with the dataset whether the transaction is likely to be a fraud or genuine.","a49c3e2d":"Similiar observation can be made regarding Age columns. They are organized into groups for various age segments. \nNext what we will be doing is creating categorical columns for both duration and Ages and plot a histogram to see the frequency distribution plot.","e4c5748b":"From the scatterplot we can see a lot of straight lines. Duration is a continous variable from 0-70 . The lines show that it is practically possible to convert them into categories of Time duration groups. ","74ebbab6":"Now, we'll encode the categorial data into 0s and 1s creating seperate columns, and we will aslso take care of the dummy variable trap using drop_first feature. Our data is now encoded and we'll drop the original features"}}