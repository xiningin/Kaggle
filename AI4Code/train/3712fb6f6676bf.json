{"cell_type":{"4c3dfd9e":"code","d4cb4fb4":"code","3e39628c":"code","63ca9dce":"code","f69abc12":"code","49185ce5":"code","f08c2ecb":"code","a6d6564b":"code","1ddcce5c":"code","095f27f4":"code","f9a73459":"code","4b44a81c":"code","03c507d3":"code","437deaca":"code","05fa7a4d":"code","de103e8f":"code","b603ddda":"code","0b24f1b6":"code","63df17c7":"code","d0264d08":"code","12683a3e":"code","0a02a919":"code","6e6a50e2":"code","06d6b2d8":"code","f471d34b":"code","86f1dee1":"code","0d94fb4d":"code","b8caebd1":"code","62fe59f0":"code","23fd45f6":"code","ce1df13e":"code","0be8d9c5":"code","cdae7597":"code","26d8dfc0":"code","cea7911c":"code","fdcf06ba":"markdown","2a869e34":"markdown","1ed340e2":"markdown","77892b65":"markdown"},"source":{"4c3dfd9e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d4cb4fb4":"# Importando as bibliotecas\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.metrics import accuracy_score","3e39628c":"# Carregando o dataframe\ndf = pd.read_csv('\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv')\n\ndf.shape","63ca9dce":"# Verificando as colunas\ndf.info()","f69abc12":"# Olhando os dados\ndf.head().T","49185ce5":"# Removendo as colunas desnecess\u00e1rias\ndf.drop(['id'], axis=1, inplace=True)\ndf.drop(['Unnamed: 32'], axis=1, inplace=True)\n\ndf.info()","f08c2ecb":"# Verificando a vari\u00e1vel target\ndf['diagnosis'].value_counts()","a6d6564b":"# Separando o dataframe\ntrain, test = train_test_split(df, random_state=42)\n\ntrain.shape, test.shape","1ddcce5c":"# Obtendo as colunas para treinamento\nfeatures = [c for c in df.columns if c not in ['diagnosis']]\n\nfeatures","095f27f4":"# Vamos executar o RandomForest padr\u00e3o\nrf_padrao = RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=42, oob_score=True)\n\nrf_padrao.fit(train[features], train['diagnosis'])\n\naccuracy_score(test['diagnosis'], rf_padrao.predict(test[features]))","f9a73459":"# Vamos usar o GridSearchCV para encontrar os melhores par\u00e2metros de execu\u00e7\u00e3o do nosso modelo de RF\n\n# Instanciando o modelo\nrf = RandomForestClassifier(random_state=42, n_jobs=-1, oob_score=True)\n\n# Definindo um dicion\u00e1rio com os par\u00e2metros a serem testados\nrf_param = {\n    'n_estimators': [100, 150, 200, 250],\n    'max_depth': [2, 5, 10],\n    'min_impurity_decrease': [0.01, 0.02, 0.0005]\n}\n\n# Instanciando o GridSearch\nrf_grid = GridSearchCV(rf, rf_param, cv=5, scoring='accuracy')\n\n# Treinando o modelo com GridSearch\nrf_grid.fit(train[features], train['diagnosis'])","4b44a81c":"# Vamos verificar os resultados\npd.DataFrame(rf_grid.cv_results_)","03c507d3":"# Vamos reduzir as colunas\npd.DataFrame(rf_grid.cv_results_)[['params','rank_test_score','mean_test_score']]","437deaca":"# Vamos reduzir as colunas e ordenar pelo rank\npd.DataFrame(rf_grid.cv_results_)[['params','rank_test_score','mean_test_score']].sort_values(by=['rank_test_score'])","05fa7a4d":"# Imprime os par\u00e2metros que produziram o \".best_score_\".\nrf_grid.best_params_","de103e8f":"# Imprimindo o best_score\nrf_grid.best_score_","b603ddda":"# Vamos 'carregar' o melhor estimator no modelo para us\u00e1-lo depois\nrf_top = rf_grid.best_estimator_\n\n# Treinar o modelo\nrf_top.fit(train[features], train['diagnosis'])\n\n# E testar\naccuracy_score(test['diagnosis'], rf_top.predict(test[features]))","0b24f1b6":"# Vamos executar o GBM padr\u00e3o\ngbm_padrao = GradientBoostingClassifier(n_estimators=200, random_state=42)\n\ngbm_padrao.fit(train[features], train['diagnosis'])\n\naccuracy_score(test['diagnosis'], gbm_padrao.predict(test[features]))","63df17c7":"# Vamos usar o GridSearchCV para encontrar os melhores par\u00e2metros de execu\u00e7\u00e3o do nosso modelo de GBM\n\n# Instanciando o modelo\ngbm = GradientBoostingClassifier(random_state=42)\n\n# Definindo uma dicion\u00e1rio com os par\u00e2metros\ngbm_param = {\n    'n_estimators': [100, 150, 200, 250],\n    'learning_rate': [0.2, 0.05, 0.0001],\n    'min_impurity_decrease': [0.01, 0.02, 0.0005]\n}\n\n# Instanciando o GridSearch\ngbm_grid = GridSearchCV(gbm, gbm_param, cv=5, scoring='accuracy')\n\n# Treinando o modelo com GridSearch\ngbm_grid.fit(train[features], train['diagnosis'])","d0264d08":"# Vamos reduzir as colunas e ordenar pelo rank\npd.DataFrame(gbm_grid.cv_results_)[['params','rank_test_score','mean_test_score']].sort_values(by=['rank_test_score'])","12683a3e":"# Imprime os par\u00e2metros que produziram o \".best_score_\".\ngbm_grid.best_params_","0a02a919":"# Imprimindo o best_score\ngbm_grid.best_score_","6e6a50e2":"# Vamos 'carregar' o melhor estimator no modelo para us\u00e1-lo - GBM\ngbm_top = gbm_grid.best_estimator_\n\n# Treinar o modelo\ngbm_top.fit(train[features], train['diagnosis'])\n\n# E testar\naccuracy_score(test['diagnosis'], gbm_top.predict(test[features]))","06d6b2d8":"# Vamos executar o AdaBoost padr\u00e3o\nada_padrao = AdaBoostClassifier(random_state=42)\n\nada_padrao.fit(train[features], train['diagnosis'])\n\naccuracy_score(test['diagnosis'], ada_padrao.predict(test[features]))","f471d34b":"# Vamos usar o GridSearchCV para encontrar os melhores par\u00e2metros de execu\u00e7\u00e3o do nosso modelo de AdaBoost\n\n# Instanciando o modelo\nada = AdaBoostClassifier(random_state=42)\n\n# Definindo uma dicion\u00e1rio com os par\u00e2metros\nada_param = {\n    'n_estimators': [100, 150, 200, 250],\n    'learning_rate': [0.2, 0.05, 0.0001]\n}\n\n# Instanciando o GridSearch\nada_grid = GridSearchCV(ada, ada_param, cv=5, scoring='accuracy')\n\n# Treinando o modelo com GridSearch\nada_grid.fit(train[features], train['diagnosis'])","86f1dee1":"# Vamos reduzir as colunas e ordenar pelo rank\npd.DataFrame(ada_grid.cv_results_)[['params','rank_test_score','mean_test_score']].sort_values(by=['rank_test_score'])","0d94fb4d":"# Imprime os par\u00e2metros que produziram o \".best_score_\".\nada_grid.best_params_","b8caebd1":"# Imprimindo o best_score\nada_grid.best_score_","62fe59f0":"# Vamos 'carregar' o melhor estimator no modelo para us\u00e1-lo - AdaBoost\nada_top = ada_grid.best_estimator_\n\n# Treinar o modelo\nada_top.fit(train[features], train['diagnosis'])\n\n# E testar\naccuracy_score(test['diagnosis'], ada_top.predict(test[features]))","23fd45f6":"# Vamos executar o melhor estimator com base nos par\u00e2metros campe\u00f5es\nada_top = AdaBoostClassifier(random_state = 42, learning_rate = 0.2, n_estimators = 200)\n\n# Treinar o modelo\nada_top.fit(train[features], train['diagnosis'])\n\n# E testar\naccuracy_score(test['diagnosis'], ada_top.predict(test[features]))","ce1df13e":"# Importando o VotingClassifier\nfrom sklearn.ensemble import VotingClassifier","0be8d9c5":"# Executando o VotingClassifier Hard com a vers\u00e3o padr\u00e3o dos modelos\nvoting_padrao_hard = VotingClassifier(\n    estimators=[('rfp', rf_padrao), ('gbmp', gbm_padrao), ('adap', ada_padrao)],\n    voting='hard', n_jobs=-1)\n\n# Treinando os modelos\nvoting_padrao_hard.fit(train[features], train['diagnosis'])\n\n# Prevendo os valores de teste\naccuracy_score(test['diagnosis'], voting_padrao_hard.predict(test[features]))","cdae7597":"# Executando o VotingClassifier Soft com a vers\u00e3o padr\u00e3o dos modelos\nvoting_padrao_soft = VotingClassifier(\n    estimators=[('rfp', rf_padrao), ('gbmp', gbm_padrao), ('adap', ada_padrao)],\n    voting='soft', n_jobs=-1, weights=[3, 2, 1])\n\n# Treinando os modelos\nvoting_padrao_soft.fit(train[features], train['diagnosis'])\n\n# Prevendo os valores de teste\naccuracy_score(test['diagnosis'], voting_padrao_soft.predict(test[features]))","26d8dfc0":"# Executando o VotingClassifier Hard com a vers\u00e3o top dos modelos\nvoting_top_hard = VotingClassifier(\n    estimators=[('rft', rf_top), ('gbmt', gbm_top), ('adat', ada_top)],\n    voting='hard', n_jobs=-1)\n\n# Treinando os modelos\nvoting_top_hard.fit(train[features], train['diagnosis'])\n\n# Prevendo os valores de teste\naccuracy_score(test['diagnosis'], voting_top_hard.predict(test[features]))","cea7911c":"# Executando o VotingClassifier Soft com a vers\u00e3o top dos modelos\nvoting_top_soft = VotingClassifier(\n    estimators=[('rft', rf_top), ('gbmt', gbm_top), ('adat', ada_top)],\n    voting='soft', n_jobs=-1, weights=[1, 2, 3])\n\n# Treinando os modelos\nvoting_top_soft.fit(train[features], train['diagnosis'])\n\n# Prevendo os valores de teste\naccuracy_score(test['diagnosis'], voting_top_soft.predict(test[features]))","fdcf06ba":"## AdaBoost","2a869e34":"## GBM","1ed340e2":"## Ensemble de Modelos","77892b65":"# IESB - Miner II - Aula 10 - GridSearchCV"}}