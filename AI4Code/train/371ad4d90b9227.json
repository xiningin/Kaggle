{"cell_type":{"a97d8f3c":"code","0e9d57f1":"code","f07371b1":"code","c31169ca":"code","b22f0aad":"code","e207db4e":"code","c6102aa5":"code","d7aa8280":"code","1dcc3f26":"code","f545a5e4":"code","c18ec189":"code","77da7031":"code","f86b2bd8":"code","dc2d03bd":"code","422ab05a":"code","5a3346ad":"code","8abac2ad":"code","0c301a38":"markdown","bf3af557":"markdown","e63fbc09":"markdown","cd411066":"markdown","f2ff7628":"markdown","a4461727":"markdown","1b86fa23":"markdown","7a23b5a2":"markdown","d7d2bc24":"markdown","4a4987e6":"markdown","10d2fefa":"markdown","4c01736f":"markdown","86828f99":"markdown"},"source":{"a97d8f3c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom datetime import datetime, timedelta #It's for time series\n\n","0e9d57f1":"\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","f07371b1":"from sklearn.preprocessing import StandardScaler #standardize everything to the same scale","c31169ca":"df=pd.read_csv('..\/input\/classifieddata\/Classified Data', index_col=0) #drop unnamed index column","b22f0aad":"df.head()","e207db4e":"#create an instance (scaler) \nscaler=StandardScaler()\nscaler.fit(df.drop('TARGET CLASS', axis=1)) #fit to data (feature columns) but not target class","c6102aa5":"\nscaled_features=scaler.transform(df.drop('TARGET CLASS', axis=1))\n","d7aa8280":"\nscaled_features","1dcc3f26":"\ndf_feat=pd.DataFrame(scaled_features, columns=df.columns[:-1])","f545a5e4":"df_feat.head()","c18ec189":"from sklearn.model_selection import train_test_split\n\nX =df_feat\ny =df['TARGET CLASS']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)","77da7031":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train, y_train)\npredictions = knn.predict(X_test)\n\npredictions\n","f86b2bd8":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import mean_squared_error, r2_score, accuracy_score,recall_score,precision_score\n","dc2d03bd":"print(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test, predictions))\nmat = confusion_matrix(y_test, predictions)\n\nsns.heatmap(mat.T, square = True, annot=True, fmt='d', cbar=False,cmap='Blues')\nplt.xlabel('True label')\nplt.ylabel('predicted label')\nplt.show()","422ab05a":"error_rate= []\n    \nfor i in range(1,40):\n    \n    knn=KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i=knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test)) #add column to list with append\n                                #average of where predictions were not equal (!=) to the actual test values.\n        #Take the (np.mean) of that and then append that error rates to this list.","5a3346ad":"plt.figure(figsize=(10,6))\nplt.title('Error Rate vs K value')\n\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', \n         marker='o', markerfacecolor='red', markersize=10,)\n\nplt.xlabel('K')\nplt.ylabel('Error Rate')\nplt.grid()\n\nplt.show()","8abac2ad":"knn=KNeighborsClassifier(n_neighbors=17)\nknn.fit(X_train,y_train)\npredictions=knn.predict(X_test)\n\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test, predictions))","0c301a38":"TARGET CLASS is our target variable.\nWe want to know if people fall inside the target calss (1) or do not (0)","bf3af557":"# K Nearest Neighbours (KNN)\n## step-by-step instructional","e63fbc09":"This specific dataset is a good example of how a company may classify their data for use in a technical interview.  Column representation is unknown and they will want the interview to analyze an anonymized data source. \nBut we will have to use these features to predict a TARGET CLASS.\n\nWe need to use these features that we don't know it represents in order to predict a target class. A 1 or 0 because the K and classifier predicts the class of a given test observation by identifying the observations that are nearest to it.\n\nThe scale of the variable actually matters a lot and any variables that are on a large scale will have a much larger effect on the distance between observations and because of this when you're using KNN as a classifier what you are going to want to do is try to standardize everything to the same scale.","cd411066":"We start with a higher error rate with a lower K value.  As we get a higher K value we go up and down and lower our error rate.  We start to get to our minimum around 34 or 35 but it bounces up and down so our minimum should be around 17.  It is currently bumpy because of our dataset has an error rate that is already low at 0.5.","f2ff7628":"Predictions of what TARGET CLASS people belong to based off of anonymized features","a4461727":"We were able to squeeze a little more accuracy, precison and recall out of our model,  up to 95% using this elbow method\n\n### So what did we do?\n1. Standardize data to same scale\n2. Put scaled features into dataframe\n3. Performed train test split on scaled features\n4. Stared with low K value\n5. Checked out confusion matrix and classification method\n6. Did elbow method to determine a lower K value\n\n \n\n","1b86fa23":"An array of values.  Scaled version of original df values","7a23b5a2":"K Nearest Neighbours is a classification algorithm that operates on\na very simple principle.  Its purpose is to use a database in which the data points are separated into several classes to predict the classification of a new sample point. It can be used to solve both classification and regression problems.\n\nTraining Algorithm:\n1. Store all the Data\nPrediction Algorithm:\n1. Calculate the distance from x to all points in your data\n2. Sort the points in your data by increasing distance from x\n3. Predict the majority label of the \u201ck\u201d closest points\n\n![image.png](attachment:image.png)\n\n[](http:\/\/)\n","d7d2bc24":"* Use scaler to make transformation. In this case variable name scaled_features\n* Transform method performs a standardization by centering and scaling. Do this after you fit model\n* Transform your data such that its distribution will have a mean value 0 and standard deviation of 1","4a4987e6":"* The model doesn't look terrible with K=1\n* Let's see if we can choose an even better K value\n* Let's use the elbow method to choose the correct K value\n* Will make a list and iterate many models using many different K values and plot out their error rate and see which one has the lowest error rate.\n\n* We will check every possible K value from 1 to 40\n* Then create a model at that specific k value and fit that model to my training set.\n\n\n\n","10d2fefa":"\nChoosing a K will affect what class a new point is assigned to:\nLow K value can pick up more noise than higher K value which will pick up more bias and smooth out model\n\n![image.png](attachment:image.png)\n\nFew parameters\n* K - points that you will look at near your test point\n* Distance Metric (The distance between your new test point and the old training points)\n","4c01736f":"Now lets make df with new scaled features without TARGET CLASS.\nGrab columns but the last one (TARGET CLASS)","86828f99":"Now our data is ready to be put ionto a machine learning algorithim, KNN, which depends on the distance between each feature.\n\nWe are trying to come up with a model to predict whether someone will be inside that TARGET CLASS or not."}}