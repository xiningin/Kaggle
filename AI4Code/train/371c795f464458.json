{"cell_type":{"9b906a4a":"code","53196d53":"code","4adedb4d":"code","2b82042f":"code","4e34950e":"code","97326a7e":"code","9c1ad10c":"code","37b9c260":"code","87a3376d":"code","7c1de2f1":"code","69f18743":"code","95a903de":"code","4e101e4a":"code","6cc195d0":"code","61f92102":"code","77464978":"code","9f05b614":"markdown","90e3fb70":"markdown","36b2d7bb":"markdown","07c87a9d":"markdown","a8896640":"markdown","2391fc2d":"markdown","add14e3e":"markdown","5388ea1f":"markdown","e5e57c6c":"markdown","687e3f9f":"markdown","9b4a36a6":"markdown","0d316a40":"markdown"},"source":{"9b906a4a":"import pandas as pd\nimport numpy as np\nimport datetime\nimport random\nimport glob\nimport cv2\nimport os\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import BatchNormalization,Activation,Dropout,Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.layers import Flatten, Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback, ReduceLROnPlateau\nimport warnings \nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\n# \u4e71\u6570\u30b7\u30fc\u30c9\u56fa\u5b9a\nseed_everything(2020)","53196d53":"inputPath = '..\/input\/5th-datarobot-ai-academy-deep-learning\/images\/train_images\/'\n# \u753b\u50cf\u8aad\u307f\u8fbc\u307f\nimage = cv2.imread(inputPath+'1_bathroom.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\ndisplay(image.shape)\ndisplay(image[0][0])\n# \u753b\u50cf\u3092\u8868\u793a\nplt.figure(figsize=(8,4))\nplt.imshow(image)","4adedb4d":"# \u753b\u50cf\u306e\u30b5\u30a4\u30ba\u5909\u66f4\nimage = cv2.resize(image,(256,256))\ndisplay(image.shape)\ndisplay(image[0][0])\n# \u753b\u50cf\u3092\u8868\u793a\nplt.figure(figsize=(8,4))\nplt.imshow(image)","2b82042f":"train = pd.read_csv('..\/input\/5th-datarobot-ai-academy-deep-learning\/train.csv')\ndisplay(train.shape)\ndisplay(train.head())","4e34950e":"def load_images(df,inputPath,size,roomType):\n    images = []\n    for i in df['id']:\n        basePath = os.path.sep.join([inputPath, \"{}_{}*\".format(i,roomType)])\n        housePaths = sorted(list(glob.glob(basePath)))\n        for housePath in housePaths:\n            image = cv2.imread(housePath)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            image = cv2.resize(image, (size, size))\n        images.append(image)\n    return np.array(images) \/ 255.0\n\n# load train images\ninputPath = '..\/input\/5th-datarobot-ai-academy-deep-learning\/images\/train_images\/'\nsize = 64\nroomType = 'frontal'\ntrain_images = load_images(train,inputPath,size,roomType)\ndisplay(train_images.shape)\ndisplay(train_images[0][0][0])","97326a7e":"train_x, valid_x, train_images_x, valid_images_x = train_test_split(train, train_images, test_size=0.2)\ntrain_y = train_x['price'].values.reshape(-1,1)\nvalid_y = valid_x['price'].values.reshape(-1,1)\ndisplay(train_images_x.shape)\ndisplay(valid_images_x.shape)\ndisplay(train_y.shape)\ndisplay(valid_y.shape)","9c1ad10c":"def create_cnn(inputShape):\n    model = Sequential()\n    \"\"\"\n    \u6f14\u7fd2:kernel_size\u3092\u5909\u66f4\u3057\u3066\u307f\u3066\u304f\u3060\u3055\u3044\n    \"\"\"    \n    model.add(Conv2D(filters=32, kernel_size=(5, 5), strides=(1, 1), padding='same',\n                     activation='relu', kernel_initializer='he_normal', input_shape=inputShape))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.1))\n\n    model.add(Conv2D(filters=64, kernel_size=(5, 5), strides=(1, 1), padding='same', \n                     activation='relu', kernel_initializer='he_normal'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.1))\n    \"\"\"\n    \u6f14\u7fd2:\u3082\u3046\u4e00\u5c64Conv2D->MaxPooling2D->BatchNormalization->Dropout\u3092\u8ffd\u52a0\u3057\u3066\u304f\u3060\u3055\u3044\n    \"\"\"    \n    \n    model.add(Flatten())\n    \n    model.add(Dense(units=256, activation='relu',kernel_initializer='he_normal'))  \n    model.add(Dense(units=32, activation='relu',kernel_initializer='he_normal'))    \n    model.add(Dense(units=1, activation='linear'))\n    \n    model.compile(loss='mape', optimizer='adam', metrics=['mape']) \n    return model","37b9c260":"# callback parameter\nfilepath = \"cnn_best_model.hdf5\" \nes = EarlyStopping(patience=5, mode='min', verbose=1) \ncheckpoint = ModelCheckpoint(monitor='val_loss', filepath=filepath, save_best_only=True, mode='auto') \nreduce_lr_loss = ReduceLROnPlateau(monitor='val_loss',  patience=2, verbose=1,  mode='min')\n\n# \u8a13\u7df4\u5b9f\u884c\ninputShape = (size, size, 3)\nmodel = create_cnn(inputShape)\nhistory = model.fit(train_images_x, train_y, validation_data=(valid_images_x, valid_y),epochs=30, batch_size=32,\n    callbacks=[es, checkpoint, reduce_lr_loss])\n","87a3376d":"def mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100\n\n# load best model weights\nmodel.load_weights(filepath)\n\n# \u8a55\u4fa1\nvalid_pred = model.predict(valid_images_x, batch_size=32).reshape((-1,1))\nmape_score = mean_absolute_percentage_error(valid_y, valid_pred)\nprint (mape_score)","7c1de2f1":"model.summary()","69f18743":"plot_model(model, to_file='cnn.png')","95a903de":"loss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(len(loss))\nplt.plot(epochs, loss, 'bo' ,label = 'training loss')\nplt.plot(epochs, val_loss, 'b' , label= 'validation loss')\nplt.title('Training and Validation loss')\nplt.legend()\nplt.show()","4e101e4a":"from tensorflow.keras.applications import VGG16\n\ndef vgg16_feature_extraction(inputShape):\n    backbone = VGG16(weights='imagenet',\n                    include_top=False,\n                    input_shape=inputShape)\n    \n    model = Sequential(layers=backbone.layers)     \n    model.add(Flatten())  \n    model.add(Dense(units=256, activation='relu',kernel_initializer='he_normal'))    \n    model.add(Dense(units=32, activation='relu',kernel_initializer='he_normal'))    \n    model.add(Dense(units=1, activation='linear'))\n    model.trainable = False    \n    model.compile(loss='mape', optimizer='adam', metrics=['mape']) \n    model.summary()\n    return model\n\n# \u8a13\u7df4\u5b9f\u884c\ninputShape = (size, size, 3)\nmodel = vgg16_feature_extraction(inputShape)\nhistory = model.fit(train_images_x, train_y, validation_data=(valid_images_x, valid_y),epochs=30, batch_size=32,\n    callbacks=[es, checkpoint, reduce_lr_loss])","6cc195d0":"def vgg16_finetuning(inputShape):\n    backbone = VGG16(weights='imagenet',\n                    include_top=False,\n                    input_shape=inputShape)\n    \"\"\"\n    \u6f14\u7fd2:Convolution Layer\u306e\u91cd\u307f\u3092\u5168\u90e8\u8a13\u7df4\u3057\u3066\u307f\u3066\u304f\u3060\u3055\u3044\uff01\n    \"\"\"    \n    \n    for layer in backbone.layers[:15]:\n        layer.trainable = False\n    for layer in backbone.layers:\n        print(\"{}: {}\".format(layer, layer.trainable))\n        \n    model = Sequential(layers=backbone.layers)     \n    model.add(GlobalAveragePooling2D())\n    model.add(Dense(units=256, activation='relu',kernel_initializer='he_normal'))  \n    model.add(Dense(units=32, activation='relu',kernel_initializer='he_normal'))    \n    model.add(Dense(units=1, activation='linear'))\n    \n    model.compile(loss='mape', optimizer='adam', metrics=['mape']) \n    model.summary()\n    return model\n\n# callback parameter\nfilepath = \"transfer_learning_best_model.hdf5\" \nes = EarlyStopping(patience=5, mode='min', verbose=1) \ncheckpoint = ModelCheckpoint(monitor='val_loss', filepath=filepath, save_best_only=True, mode='auto') \nreduce_lr_loss = ReduceLROnPlateau(monitor='val_loss',  patience=2, verbose=1,  mode='min')\n\n# \u8a13\u7df4\u5b9f\u884c\ninputShape = (size, size, 3)\nmodel = vgg16_finetuning(inputShape)\nhistory = model.fit(train_images_x, train_y, validation_data=(valid_images_x, valid_y),epochs=30, batch_size=32,\n    callbacks=[es, checkpoint, reduce_lr_loss])\n\n# load best model weights\nmodel.load_weights(filepath)\n\n# \u8a55\u4fa1\nvalid_pred = model.predict(valid_images_x, batch_size=32).reshape((-1,1))\nmape_score = mean_absolute_percentage_error(valid_y, valid_pred)\nprint (mape_score)","61f92102":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ndatagen = ImageDataGenerator(horizontal_flip=True,\n                             vertical_flip=True,\n                             )\n\ninputPath = '..\/input\/5th-datarobot-ai-academy-deep-learning\/images\/train_images\/'\nimage = cv2.imread(inputPath+'1_bathroom.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nimage_data = image \/ 255.0\nimage_data = image_data.reshape(1,image_data.shape[0],image_data.shape[1],image_data.shape[2])\n\nprint (image_data.shape)\nfor batch in datagen.flow(image_data,batch_size=1):\n    plt.imshow(image_data[0])\n    plt.show() \n    plt.imshow(batch[0])\n    plt.show()       \n    break","77464978":"\"\"\"\n\u6f14\u7fd2:ImageDataGenerator\u4e2d\u306b\u65b0\u305f\u306a\u4e09\u3064\u3092\u8ffd\u52a0\u3057\u3066\u307f\u3066\u304f\u3060\u3055\u3044\uff01\n                             rotation_range=90,\n                             width_shift_range=0.2,\n                             height_shift_range=0.2,\n\"\"\"    \n\n# callback parameter\nfilepath = \"data_aug_best_model.hdf5\" \nes = EarlyStopping(patience=5, mode='min', verbose=1) \ncheckpoint = ModelCheckpoint(monitor='val_loss', filepath=filepath, save_best_only=True, mode='auto') \nreduce_lr_loss = ReduceLROnPlateau(monitor='val_loss',  patience=2, verbose=1,  mode='min')\n\n# \u8a13\u7df4\u5b9f\u884c\ndatagen = ImageDataGenerator(horizontal_flip=True,\n                             vertical_flip=True,\n                             )\ninputShape = (size, size, 3)\nbatch_size = 32\nmodel = vgg16_finetuning(inputShape)\ndatagen.fit(train_images_x,augment=True)\ntrain_datagen = datagen.flow(train_images_x, train_y, batch_size=batch_size, shuffle=True)\nhistory = model.fit(train_datagen, validation_data=(valid_images_x, valid_y),\n    steps_per_epoch=len(train_images_x) \/ batch_size, epochs=30,                \n    callbacks=[es, checkpoint, reduce_lr_loss])\n\n\n\n# load best model weights\nmodel.load_weights(filepath)\n\n# \u8a55\u4fa1\nvalid_pred = model.predict(valid_images_x, batch_size=32).reshape((-1,1))\nmape_score = mean_absolute_percentage_error(valid_y, valid_pred)\nprint (mape_score)","9f05b614":"# \u8ee2\u79fb\u5b66\u7fd2","90e3fb70":"# \u6570\u5024\u30c7\u30fc\u30bf\u3092\u8aad\u307f\u8fbc\u307f","36b2d7bb":"# \u30e2\u30c7\u30eb\u8a55\u4fa1","07c87a9d":"# \u753b\u50cf\u51e6\u7406","a8896640":"# \u8a13\u7df4\u3001\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u4f5c\u6210","2391fc2d":"# \u30c7\u30fc\u30bf\u6c34\u5897\u3084\u3057","add14e3e":"# \u8a13\u7df4\u5c65\u6b74\u53ef\u8996\u5316","5388ea1f":"# \u30e2\u30c7\u30eb\u53ef\u8996\u5316","e5e57c6c":"# CNN\u30e2\u30c7\u30eb\u3092\u5b9a\u7fa9\u3059\u308b","687e3f9f":"# \u30e2\u30c7\u30eb\u8a13\u7df4","9b4a36a6":"# \u753b\u50cf\u3092\u8aad\u307f\u8fbc\u307f","0d316a40":"# \u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u30a4\u30f3\u30dd\u30fc\u30c8"}}