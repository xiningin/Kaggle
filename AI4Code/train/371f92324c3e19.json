{"cell_type":{"4ec30045":"code","65f981fc":"code","84d02ad3":"code","3bb9be47":"code","6701b9ea":"code","fadc9a93":"code","26f9fe9d":"code","6f939d45":"code","46a6f47e":"code","6f4cea6f":"code","fc8590b2":"code","08b3d2d1":"code","d0648cb8":"code","931e65a5":"code","88aa52da":"code","60fbd246":"code","faac8e37":"code","bc3dc54e":"code","9ee739e0":"code","5cef9a74":"code","2fa4095c":"code","e2848060":"code","ec9486c6":"code","e5f15f9d":"code","18a6b295":"code","e0c9ff7e":"code","5efd0fb7":"code","6f2972f8":"code","465142c4":"code","7f936a10":"code","27a96eda":"code","441f2d9c":"code","8c41977e":"code","36422a3e":"code","d9532f8d":"code","1ae5dd20":"code","2bacf739":"markdown","a1f32c82":"markdown","d3d62e41":"markdown","0a9bfcf4":"markdown","55e2147c":"markdown","57b0f86a":"markdown","bfe18764":"markdown","b2a31b9f":"markdown","7aacb69a":"markdown","2a6d36c6":"markdown","8e3fe7ea":"markdown","1724c8f1":"markdown","16558818":"markdown","a1df694b":"markdown","467bdf54":"markdown","2def71fb":"markdown","bbc0999d":"markdown","757545e9":"markdown","e97bc42a":"markdown","a183e707":"markdown","9d3b217f":"markdown","23438a2d":"markdown","afa06692":"markdown","0f9f13cc":"markdown","c0016cce":"markdown","683735f7":"markdown"},"source":{"4ec30045":"import os\nimport nltk\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nfrom datasets import Dataset\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import TrainingArguments, Trainer","65f981fc":"# Constants\ntrain_csv = \"..\/input\/feedback-prize-2021\/train.csv\"\nsubmission_csv = \"..\/input\/feedback-prize-2021\/sample_submission.csv\"\ntrain_text_path = \"..\/input\/feedback-prize-2021\/train\"\ntest_text_path = \"..\/input\/feedback-prize-2021\/test\"\n\n# Load DF\ndf = pd.read_csv(train_csv, dtype={'discourse_id': int, 'discourse_start': int, 'discourse_end': int})\ndf.head()","84d02ad3":"# No nulls\ndf.isnull().sum()","3bb9be47":"text_id = df['id'][0]","6701b9ea":"def get_text(file_id):\n    a_file = f\"{train_text_path}\/{file_id}.txt\"\n    with open(a_file, \"r\") as fp:\n        txt = fp.read()\n    return txt\n\ntxt = get_text(text_id)\nprint(txt)","fadc9a93":"df_example = df[df['id'] == text_id]\ndf_example","26f9fe9d":"# Creadits for this part of visualisation _> https:\/\/www.kaggle.com\/thedrcat\nimport spacy\nfrom spacy import displacy\nfrom pylab import cm, matplotlib\n\ncolors = {\n            'Lead': '#8000ff',\n            'Position': '#2b7ff6',\n            'Evidence': '#2adddd',\n            'Claim': '#80ffb4',\n            'Concluding Statement': 'd4dd80',\n            'Counterclaim': '#ff8042',\n            'Rebuttal': '#ff0000'\n         }\n\ndef visualize(example, df):\n    ents = []\n    for i, row in df[df['id'] == example].iterrows():\n        ents.append({\n                        'start': int(row['discourse_start']), \n                         'end': int(row['discourse_end']), \n                         'label': row['discourse_type']\n                    })\n        \n    with open(f'{train_text_path}\/{example}.txt', 'r') as file: data = file.read()\n    doc2 = {\n                \"text\": data,\n                \"ents\": ents,\n                \"title\": example\n            }\n\n    options = {\"ents\": df.discourse_type.unique().tolist(), \"colors\": colors}\n    displacy.render(doc2, style=\"ent\", options=options, manual=True, jupyter=True)","6f939d45":"examples = df['id'].sample(n=3, random_state=42).values.tolist()\n\nfor ex in examples:\n    visualize(ex,df)\n    print('\\n')","46a6f47e":"import plotly.express as px\nvalues = df['discourse_type'].value_counts().to_dict()\nfig =px.bar(x=list(values.keys()),y = list(values.values()) )\nfig.update_xaxes(title=\"Classes\")\nfig.update_yaxes(title = \"Number of Rows\")\nfig.update_layout(showlegend = True,\n    title = {\n        'text': 'Discourse Type Distribution ',\n        'y':0.95,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\nfig.show()","6f4cea6f":"import plotly.express as px\nimport numpy as np\nvalues = df['discourse_type_num'].value_counts().to_dict()\nfig =px.bar(x=list(values.keys()),y = list(values.values()) ,color = np.unique(list(values.keys())))\nfig.update_xaxes(title=\"Classes\")\nfig.update_yaxes(title = \"Number of Rows\")\nfig.update_layout(showlegend = True,\n    title = {\n        'text': 'Discourse Type Num Distribution ',\n        'y':0.95,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\nfig.show()","fc8590b2":"id_to_class = dict(enumerate(df['discourse_type'].unique().tolist() + ['No Class']))\nclass_to_id = {v: k for k, v in id_to_class.items()}\nprint(id_to_class)\nclass_to_id","08b3d2d1":"text_ids = df['id'].unique().tolist()\ntext_id = text_ids[5]\ntext = get_text(text_id)\nprint(text)","d0648cb8":"# Extract element boundaries and classes  with to_records\ndf_text = df[df['id'] == text_id]\nelements = df_text[['discourse_start', 'discourse_end', 'discourse_type']].to_records(index=False).tolist()\nelements","931e65a5":"# Fill \"No class\" chunks: beginning and end\ninitial_idx = 0\nfinal_idx = len(text)\n# Add element at the beginning if it doesn't in index 0\nnew_elements = []\nif elements[0][0] != initial_idx:\n    starting_element = (0, elements[0][0]-1, 'No Class')\n    new_elements.append(starting_element)\n# Add element at the end if it doesn't in index \"-1\"\nif elements[-1][1] != final_idx:\n    closing_element = (elements[-1][1]+1, final_idx, 'No Class')\n    new_elements.append(closing_element)\n    \nelements += new_elements\nelements = sorted(elements, key=lambda x: x[0])\n# See first element (new)\nelements","88aa52da":"# Add \"No class\" elements inbetween separated elements \nnew_elements = []\nfor i in range(1, len(elements)-1):\n    if elements[i][0] != elements[i-1][1] + 1 and elements[i][0] != elements[i-1][1]:\n        new_element = (elements[i-1][1] + 1, elements[i][0]-1, 'No Class')\n        new_elements.append(new_element)\n\nelements += new_elements\nelements = sorted(elements, key=lambda x: x[0])\nelements","60fbd246":"# Finall \"fill_gaps\" functions, wrapping up the above cells\ndef fill_gaps(elements, text):\n    \"\"\"Add \"No Class\" elements to a list of elements (see get_elements) \"\"\"\n    initial_idx = 0\n    final_idx = len(text)\n\n    # Add element at the beginning if it doesn't in index 0\n    new_elements = []\n    if elements[0][0] != initial_idx:\n        starting_element = (0, elements[0][0]-1, 'No Class')\n        new_elements.append(starting_element)\n\n\n    # Add element at the end if it doesn't in index \"-1\"\n    if elements[-1][1] != final_idx:\n        closing_element = (elements[-1][1]+1, final_idx, 'No Class')\n        new_elements.append(closing_element)\n\n    elements += new_elements\n    elements = sorted(elements, key=lambda x: x[0])\n\n    # Add \"No class\" elements inbetween separated elements \n    new_elements = []\n    for i in range(1, len(elements)-1):\n        if elements[i][0] != elements[i-1][1] + 1 and elements[i][0] != elements[i-1][1]:\n            new_element = (elements[i-1][1] + 1, elements[i][0]-1, 'No Class')\n            new_elements.append(new_element)\n\n    elements += new_elements\n    elements = sorted(elements, key=lambda x: x[0])\n    return elements\n\n\ndef get_elements(df, text_id, do_fill_gaps=True, text=None):\n    \"\"\"Get a list of (start, end, class) elements for a given text_id\"\"\"\n    text = get_text(text_id) if text is None else text\n    df_text = df[df['id'] == text_id]\n    elements = df_text[['discourse_start', 'discourse_end', 'discourse_type']].to_records(index=False).tolist()\n    if do_fill_gaps:\n        elements = fill_gaps(elements, text)\n    return elements","faac8e37":"def get_x_samples(df, text_id, do_fill_gaps=True):\n    \"\"\"Create a dataframe of the sentences of the text_id, with columns text, label \"\"\"\n    text = get_text(text_id)\n    elements = get_elements(df, text_id, do_fill_gaps, text)\n    sentences = []\n    for start, end, class_ in elements:\n        elem_sentences = nltk.sent_tokenize(text[start:end])\n        sentences += [(sentence, class_) for sentence in elem_sentences]\n    df = pd.DataFrame(sentences, columns=['text', 'label'])\n    df['label'] = df['label'].map(class_to_id)\n    return df\n\nget_x_samples(df, text_ids[1])","bc3dc54e":"# # This takes a while. I created a dataset with the output here: https:\/\/www.kaggle.com\/julian3833\/feedback-df-sentences\n# x = []\n# for text_id in tqdm(text_ids):\n#    x.append(get_x_samples(df, text_id))\n\n# df_sentences = pd.concat(x)","9ee739e0":"df_sentences = pd.read_csv(\"..\/input\/feedback-df-sentences\/df_sentences.csv\")\ndf_sentences = df_sentences[df_sentences.text.str.split().str.len() >= 3]\ndf_sentences.head()","5cef9a74":"df_sentences.to_csv(\"df_sentences.csv\", index=False)","2fa4095c":"len(df_sentences)","e2848060":"MODEL_CHK = \"..\/input\/huggingface-bert\/bert-base-cased\"\nNUM_LABELS = 8\nNUM_EPOCHS = 2","ec9486c6":"ds_train = Dataset.from_pandas(df_sentences.iloc[:340000])\nds_val = Dataset.from_pandas(df_sentences.iloc[340000:])","e5f15f9d":"transformers.logging.set_verbosity_warning() # Silence some annoying logging of HF\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_CHK)\ndef preprocess_function(examples):    \n    return tokenizer(examples[\"text\"], truncation=True, max_length=256)\n\n# Tokenizer dataset\nds_train_tokenized = ds_train.map(preprocess_function, batched=True)\nds_val_tokenized = ds_val.map(preprocess_function, batched=True)","18a6b295":"model = AutoModelForSequenceClassification.from_pretrained(MODEL_CHK, num_labels=NUM_LABELS)","e0c9ff7e":"os.environ['TOKENIZERS_PARALLELISM'] = 'false'\nos.environ['WANDB_DISABLED'] = 'true'\ntraining_args = TrainingArguments(\n    output_dir='feeeback-classifier',\n    learning_rate=2e-5,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    num_train_epochs=NUM_EPOCHS,\n    weight_decay=0.01,\n    report_to=\"none\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=ds_train_tokenized,\n    eval_dataset=ds_val_tokenized,\n    tokenizer=tokenizer,\n    #data_collator=data_collator,\n)","5efd0fb7":"trainer.train()","6f2972f8":"trainer.save_model(\"feedback-bert-trained\")","465142c4":"TEST_PATH='..\/input\/feedback-prize-2021\/test'\ndef get_test_text(a_id):\n    a_file = f\"{TEST_PATH}\/{a_id}.txt\"\n    with open(a_file, \"r\") as fp:\n        txt = fp.read()\n    return txt\n\ndef create_df_test():\n    test_ids = [f[:-4] for f in os.listdir(TEST_PATH)]\n    test_data = []\n    for test_id in test_ids:\n        text = get_test_text(test_id)\n        sentences = nltk.sent_tokenize(text)\n        id_sentences = []\n        idx = 0 \n        for sentence in sentences:\n            id_sentence = []\n            words = sentence.split()\n            # I created this heuristic for mapping words in senteces to \"word indexes\"\n            # This is not definitive and might have strong drawbacks and problems\n            for w in words:\n                id_sentence.append(idx)\n                idx+=1\n            id_sentences.append(id_sentence)\n        test_data += list(zip([test_id] * len(sentences), sentences, id_sentences))\n    df_test = pd.DataFrame(test_data, columns=['id', 'text', 'ids'])\n    return df_test","7f936a10":"df_test = create_df_test()\ndf_test.head()","27a96eda":"ds_test = Dataset.from_pandas(df_test)\nds_test_tokenized = ds_test.map(preprocess_function, batched=True)","441f2d9c":"# Get the predictions!!\ntest_predictions = trainer.predict(ds_test_tokenized)","8c41977e":"# Turn logits into classes\ndf_test['predictions'] = test_predictions.predictions.argmax(axis=1)\n\n# Turn class ids into class labels\ndf_test['class'] = df_test['predictions'].map(id_to_class)\ndf_test.head()","36422a3e":"# Turn the word ids into this weird predictionstring required\ndf_test['predictionstring'] = df_test['ids'].apply(lambda x: ' '.join([str(i) for i in x]))\ndf_test.head()","d9532f8d":"# Drop \"No class\" sentences\ndf_test = df_test[df_test['class'] != 'No Class']\ndf_test.head()","1ae5dd20":"# And submit!! \ud83e\udd1e\ud83e\udd1e \ndf_test[['id', 'class', 'predictionstring']].to_csv(\"submission.csv\", index=False)","2bacf739":"# Create a sentence classification datasety\n\nThe steps for solving the problem statement are \n\n1. Split the texts into sentences (x)\n2. Assign each sentence a class (y).\n3. Train a normal sequence classifier on those sentences\n\nThere are 7 classes and the labeled sections (sometimes) exceed sentences. We will preprocess them to have only sentences. That way, we avoid the problem of detecting when a element starts and when it ends for now.\n","a1f32c82":"## Insights\n\n* For some examples, the entire text is densely split into spans of different categories. In some other examples, the annotators omit some words and the splits look very subjective. It's an indicator that annotations may be noisy.\n* Order seems to be important: start with the lead, mix claims and evidence, finish with concluding statement. We may need to incorporate this into our models.\n* There may be 2 spans of the same class next to each other - it will be important to separate them!","d3d62e41":"## Let's see the first example in some more detail","0a9bfcf4":"## Check For Null Values \nwe can see that there are no null values in the dataset\n","55e2147c":"# Some Visual Things ","57b0f86a":"# DATA DISTRIBUTIONs\n## Discourse Type Distribution**","bfe18764":"## Prepare test dataset","b2a31b9f":"## discourse type num   distribution","7aacb69a":"# Sentence Classifier with HuggingFace \ud83e\udd17","2a6d36c6":"## Tokenize","8e3fe7ea":"## From the [Data tab](https:\/\/www.kaggle.com\/c\/feedback-prize-2021\/data):\n\n> *  id -                 ID code for essay response\n> *  discourse_id -       ID code for discourse element\n> *  discourse_start -    character position where discourse element begins in the essay response\n> *  discourse_end -      character position where discourse element ends in the essay response\n> *  discourse_text -     text of discourse element\n> *  discourse_type -     classification of discourse element\n> *  discourse_type_num - enumerated class label of discourse element\n> *  predictionstring -   the word indices of the training sample, as required for predictions\n","1724c8f1":"# Load Model ","16558818":"# A Little Look Into The Data\nIn the dataset folder we have `train.csv` file which contains labels for text files in train folder. Let's take a  look at the `train.csv`","a1df694b":"## Please, _DO_ upvote if you find it useful or interesting!! ","467bdf54":"# Submit\n\nWe will apply a process similar to the one we applied to the original train data, splitting each text into its sentences.\n\nSee the [Evaluation tab](https:\/\/www.kaggle.com\/c\/feedback-prize-2021\/overview\/evaluation) for details about the `predictionstring` column","2def71fb":"# Imports","bbc0999d":"\n\n## Dataset functions: `fill_gaps()`, `get_elements()`, and `get_x_samples()`\n\nHere we write the functions `fill_gaps` which will to just that. I leave the code I use for developing and below there is the condensed function.","757545e9":"## Predict","e97bc42a":"## Encode classes as ints\nSome sections don't belong to any class. We will label them as `No Class` so we can discard those sections and avoid false positives.","a183e707":"# \ud83d\udcd6 Feedback Prize - BERT \ud83e\udd17 Sentence Classifier\n\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/31779\/logos\/header.png)\n## Please, _DO_ upvote if you find it useful or interesting!! \n","9d3b217f":"### Prepare trainer","23438a2d":"# Modeling!!!\n\nWe will use a `BERT` and the `Trainer` API from Hugging Face for this notebook. \n\nWe are using a dataset to avoid using internet (a restriction of the competition for submission notebooks)\n\nReferences:\n* https:\/\/huggingface.co\/docs\/transformers\/training\n* https:\/\/huggingface.co\/docs\/transformers\/custom_datasets","afa06692":"## HuggingFace Dataset","0f9f13cc":"For now, we are submitting one row per sentence and not \"elements\". \n\nHow to convert sentences into \"elements\" (blocks of setences) is not clear since there are times when various sentences with the same class are flagged in independent \"elements\".","c0016cce":"## Train","683735f7":"## Build the full dataframe for sentence classification"}}