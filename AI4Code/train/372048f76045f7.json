{"cell_type":{"f1b122e2":"code","53053248":"code","12d47734":"code","48a90062":"code","c0b71cba":"markdown","dc75268c":"markdown","805c73eb":"markdown","61495d14":"markdown","98fda0c4":"markdown"},"source":{"f1b122e2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport spacy # required library to create word vectors and import the LinearSVC\nfrom sklearn.model_selection import train_test_split # required to split data into training and validation data\nfrom sklearn.svm import SVC # model to be used --> separates vectors into regions for classification\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","53053248":"# Download the spacy large model to be able to create the vectors\nnlp = spacy.load('en_core_web_lg')\n\n# Read the data\ntrain_data = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_data = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\n\n# Using \"disable_pipes\" to speed up the vector transformation\nwith nlp.disable_pipes():\n    train_vecs = np.array([nlp(text).vector for text in train_data.text]) # doc vectors for training set\n    test_vecs = np.array([nlp(text).vector for text in test_data.text]) # doc vectors for testing set","12d47734":"X_train, X_valid, y_train, y_valid = train_test_split(train_vecs, train_data.target, test_size=0.2, random_state=1)\n\n# creating function to create separate models\ndef my_model():\n    # using RBF kernel to separate the region\n    model = SVC(random_state=1, kernel='rbf', max_iter=20000)\n    return model\n\nmodel_val = my_model()\nmodel_val.fit(X_train, y_train)\nprint(f'Accuracy: {model_val.score(X_valid, y_valid)}')","48a90062":"# model definition and training\nmodel = my_model()\nmodel.fit(train_vecs, train_data.target)\n\n# create predictions\npredictions = model.predict(test_vecs)\n\n# organize into the way needed for the competition\nfinal_preds = pd.concat([test_data.id, pd.DataFrame(predictions, columns=['target'])], axis=1)\n\nfinal_preds.to_csv('submission.csv', index=False)\n\nprint(\"Your submission was successfully saved!\")","c0b71cba":"## Using the LinearSVC Model\nAs explained on https:\/\/scikit-learn.org\/stable\/modules\/svm.html#svm:\n> Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n\nThe SVC model classifies in the way shown in the diagrams, dividing the space into multiple regions (image from the same website as above):\n\n![LinearSVC](https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_iris_svc_0011.png)\n\nAfter training with different types of models, I found that for this competition, the SVC model with the RBF kernel works best, so we will use it to predict the sentiments behind tweets.","dc75268c":"This seems to be getting an accuracy of about 82.3%.","805c73eb":"# Word Vectors and SVC Model on NLP with Disaster Tweets Competition\nIn this notebook, I will create an SVC model and train it on the word vectors created from the dataset in the NLP with Disaster Tweets competition.\n\nFirst, here are the basic imports.","61495d14":"## Creating Word Vectors\nWord vectors (or word embeddings) are effectively ways of encoding the definition of a word in a vector, or a sequence of numbers.\n\nThis allows us to train a model on the sequence of numbers rather than the word itself.\n\nHere is a more formal definition (and an example) from the Kaggle \"Natural Language Processing\" course:\n> Word embeddings (also called word vectors) represent each word numerically in such a way that the vector corresponds to how that word is used or what it means. Vector encodings are learned by considering the context in which the words appear. Words that appear in similar contexts will have similar vectors. For example, vectors for \"leopard\", \"lion\", and \"tiger\" will be close together, while they'll be far away from \"planet\" and \"castle\".\n\nIn this case, we will be loading in the average of the word vectors of each tweet; this usually works really well.","98fda0c4":"## Final Training and Submission\nNow, we will do the final training with all of the training data, and we will use the model to predict the outcomes for the testing set."}}