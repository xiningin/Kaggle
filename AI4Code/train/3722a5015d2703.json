{"cell_type":{"39e83a5e":"code","359a8dd4":"code","df0d45a5":"code","f28fb109":"code","ee403344":"code","e455f5b8":"code","be87af76":"code","0c017ab1":"code","564243a9":"code","4b579eea":"code","c6a93d3d":"code","59095fc4":"code","9253c821":"code","8f82e889":"code","72bcd8c8":"code","d1a39adb":"code","0d63e311":"code","c4a0e915":"code","0c35ebd2":"code","ec136f2b":"code","ff52e9dc":"code","4a3ca8e9":"code","4b6d2aae":"code","7572fa95":"code","db640c82":"code","03e54603":"code","72adac49":"code","cceb42a7":"code","e286d744":"code","6712ebf0":"code","e2af89b9":"code","d8c8925a":"code","dff3f278":"code","9f89ee36":"code","30c5c9b5":"code","d4fb26cc":"code","6211a653":"code","325b29a4":"code","f48f68c1":"code","0c615d49":"code","fc137ed6":"code","a8af08ac":"code","50c6b748":"code","69325230":"markdown","9d5543c5":"markdown","d037775b":"markdown","783ce632":"markdown","ac8ccad4":"markdown","ace1fa74":"markdown","1cd1a67e":"markdown","fd71862b":"markdown","ec953efd":"markdown","27231266":"markdown","e74608de":"markdown","7b427fc0":"markdown","50fe12cc":"markdown","e9daa6e7":"markdown","f6a727c4":"markdown","c37a26ed":"markdown","5e366ef4":"markdown","668894dd":"markdown","b8c38a4d":"markdown","25725d78":"markdown","33cc2854":"markdown","f4666222":"markdown","b38160c6":"markdown","06d7f5b7":"markdown","bb691f19":"markdown","bac28c2b":"markdown","44d5c624":"markdown","1936c2ea":"markdown","ee43d792":"markdown"},"source":{"39e83a5e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","359a8dd4":"toxic = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv')\ntoxic.head()","df0d45a5":"intoxication = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv')\nintoxication.head()","f28fb109":"toxicity = pd.read_csv('..\/input\/jigsaw-toxic-severity-rating\/sample_submission.csv')\ntoxicity.head()","ee403344":"import re\nimport string\n\nimport nltk\nfrom nltk.probability import FreqDist\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nfrom nltk import pos_tag\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\nfrom wordcloud import WordCloud\nfrom tqdm.auto import tqdm\nimport matplotlib.style as style\nstyle.use('fivethirtyeight')\nfrom sklearn.metrics import plot_roc_curve\nfrom numpy import interp\nfrom itertools import cycle","e455f5b8":"intoxication.isnull().sum()","be87af76":"#3rd row, 3rd column \n\nintoxication.iloc[2,2]","0c017ab1":"intoxication['less_toxic'].value_counts()","564243a9":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\n# post with the most comments\n\ntoxicity[toxicity['score'] == toxicity['score'].max()]['comment_id'].iloc[0]","4b579eea":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\ndef remove_line_breaks(text):\n    text = text.replace('\\r', ' ').replace('\\n', ' ')\n    return text\n\n#remove punctuation\ndef remove_punctuation(text):\n    re_replacements = re.compile(\"__[A-Z]+__\")  # such as __NAME__, __LINK__\n    re_punctuation = re.compile(\"[%s]\" % re.escape(string.punctuation))\n    '''Escape all the characters in pattern except ASCII letters and numbers'''\n    tokens = word_tokenize(text)\n    tokens_zero_punctuation = []\n    for token in tokens:\n        if not re_replacements.match(token):\n            token = re_punctuation.sub(\" \", token)\n        tokens_zero_punctuation.append(token)\n    return ' '.join(tokens_zero_punctuation)\n\ndef remove_special_characters(text):\n    text = re.sub('[^a-zA-z0-9\\s]', '', text)\n    return text\n\ndef lowercase(text):\n    text_low = [token.lower() for token in word_tokenize(text)]\n    return ' '.join(text_low)\n\ndef remove_stopwords(text):\n    stop = set(stopwords.words('english'))\n    word_tokens = nltk.word_tokenize(text)\n    text = \" \".join([word for word in word_tokens if word not in stop])\n    return text\n\n#remobe one character words\ndef remove_one_character_words(text):\n    '''Remove words from dataset that contain only 1 character'''\n    text_high_use = [token for token in word_tokenize(text) if len(token)>1]      \n    return ' '.join(text_high_use)   \n    \n#%%\n# Stemming with 'Snowball stemmer\" package\ndef stem(text):\n    stemmer = nltk.stem.snowball.SnowballStemmer('english')\n    text_stemmed = [stemmer.stem(token) for token in word_tokenize(text)]        \n    return ' '.join(text_stemmed)\n\ndef lemma(text):\n    wordnet_lemmatizer = WordNetLemmatizer()\n    word_tokens = nltk.word_tokenize(text)\n    text_lemma = \" \".join([wordnet_lemmatizer.lemmatize(word) for word in word_tokens])       \n    return ' '.join(text_lemma)\n\n\n#break sentences to individual word list\ndef sentence_word(text):\n    word_tokens = nltk.word_tokenize(text)\n    return word_tokens\n#break paragraphs to sentence token \ndef paragraph_sentence(text):\n    sent_token = nltk.sent_tokenize(text)\n    return sent_token    \n\n\ndef tokenize(text):\n    \"\"\"Return a list of words in a text.\"\"\"\n    return re.findall(r'\\w+', text)\n\ndef remove_numbers(text):\n    no_nums = re.sub(r'\\d+', '', text)\n    return ''.join(no_nums)\n\n\n\ndef clean_text(text):\n    _steps = [\n    remove_line_breaks,\n    remove_one_character_words,\n    remove_special_characters,\n    lowercase,\n    remove_punctuation,\n    remove_stopwords,\n    stem,\n    remove_numbers\n]\n    for step in _steps:\n        text=step(text)\n    return text   \n#%%","c6a93d3d":"#https:\/\/stackoverflow.com\/questions\/55557004\/getting-attributeerror-float-object-has-no-attribute-replace-error-while\n#To avoid with tqdm AttributeError: 'float' object has no attribute\n\nintoxication[\"more_toxic\"] = intoxication[\"more_toxic\"].astype(str)\nintoxication[\"more_toxic\"] = [x.replace(':',' ') for x in intoxication[\"more_toxic\"]]","59095fc4":"#intoxication['more_toxic'] = pd.Series([clean_text(i) for i in tqdm(intoxication['more_toxic'])])","9253c821":"words = intoxication[\"more_toxic\"].values","8f82e889":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nls = []\n\nfor i in words:\n    ls.append(str(i))","72bcd8c8":"ls[:5]","d1a39adb":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\n# The wordcloud \nplt.figure(figsize=(16,13))\nwc = WordCloud(background_color=\"black\", max_words=1000, max_font_size= 200,  width=1600, height=800)\nwc.generate(\" \".join(ls))\nplt.title(\"More Toxic Topics\", fontsize=20)\nplt.imshow(wc.recolor( colormap= 'viridis' , random_state=17), alpha=0.98, interpolation=\"bilinear\", )\nplt.axis('off')","0d63e311":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nmost_toxic = toxicity.sort_values('score', ascending =False)[['comment_id', 'score']].head(12)\n\nmost_toxic['score1'] = most_toxic['score']\/1000","c4a0e915":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nplt.figure(figsize = (8,8))\n\nsns.barplot(data = most_toxic, y = 'comment_id', x = 'score1', color = 'c')\nplt.xticks(fontsize=27, rotation=0)\nplt.yticks(fontsize=31, rotation=0)\nplt.xlabel('Toxicity', fontsize = 21)\nplt.ylabel('')\nplt.title('Most toxic posts', fontsize = 30);","0c35ebd2":"import gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nimport numpy as np\nnp.random.seed(2018)\nimport nltk","ec136f2b":"stemmer = SnowballStemmer('english')","ff52e9dc":"nltk.download('wordnet')","4a3ca8e9":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\ndef lemmatize_stemming(text):\n    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n\ndef preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n            result.append(lemmatize_stemming(token))\n    return result","4b6d2aae":"intoxication['more_toxic'].iloc[0]","7572fa95":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\ndoc_sample = intoxication['more_toxic'].iloc[1]\nprint('original document: ')\n\nwords = []\n\nfor word in doc_sample.split(' '):\n    words.append(word)\n    \n    \nprint(words)\nprint('\\n\\n tokenized and lemmatized document: ')\nprint(preprocess(doc_sample))","db640c82":"intoxication['more_toxic'] = intoxication['more_toxic'].astype(str)","03e54603":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nwords = []\n\nfor i in intoxication['more_toxic']:\n        words.append(i.split(' '))","72adac49":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\ndictionary = gensim.corpora.Dictionary(words)\n\ncount = 0\nfor k, v in dictionary.iteritems():\n    print(k, v)\n    count += 1\n    if count > 10:\n        break","cceb42a7":"# Filter out tokens in the dictionary by their frequency.\n\ndictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)","e286d744":"bow_corpus = [dictionary.doc2bow(doc) for doc in words]\nbow_corpus[4310]","6712ebf0":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nbow_doc_4310 = bow_corpus[4310]\n\nfor i in range(len(bow_doc_4310)):\n    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n                                               dictionary[bow_doc_4310[i][0]], \nbow_doc_4310[i][1]))","e2af89b9":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nfrom gensim import corpora, models\n\ntfidf = models.TfidfModel(bow_corpus)\ncorpus_tfidf = tfidf[bow_corpus]\n\nfrom pprint import pprint\n\nfor doc in corpus_tfidf:\n    pprint(doc)\n    break","d8c8925a":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nlda_model = gensim.models.LdaMulticore(bow_corpus,\n                                       num_topics=10,\n                                       id2word=dictionary,\n                                       passes=2,\n                                       workers=2)","dff3f278":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nfor idx, topic in lda_model.print_topics(-1):\n    print('Topic: {} \\nWords: {}'.format(idx, topic))","9f89ee36":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nlda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf,\n                                             num_topics=10,\n                                             id2word=dictionary,\n                                             passes=2,\n                                             workers=4)\n\nfor idx, topic in lda_model_tfidf.print_topics(-1):\n    print('Topic: {} Word: {}'.format(idx, topic))","30c5c9b5":"#4th row, 2nd column \n\nintoxication.iloc[4,1]","d4fb26cc":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nunseen_document = 'why dont you be a model for a cheesy blue vagina syndrome. A lot are experiencing this vaginal disease'\nbow_vector = dictionary.doc2bow(preprocess(unseen_document))\n\nfor index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))","6211a653":"from sklearn.model_selection import train_test_split\nfrom sklearn import model_selection, naive_bayes, svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer","325b29a4":"processed_text = intoxication['more_toxic']","f48f68c1":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\nvectorizer = TfidfVectorizer()\ntfidf = vectorizer.fit_transform(processed_text)\nprint(tfidf.shape)\nprint('\\n')\n#print(vectorizer.get_feature_names())","0c615d49":"y = intoxication['more_toxic']","fc137ed6":"X_train_tf, X_test_tf, y_train_tf, y_test_tf = train_test_split(tfidf, y, test_size=0.2, random_state=42)","a8af08ac":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\n# fit the training dataset on the NB classifier\nNaive = naive_bayes.MultinomialNB()\nNaive.fit(X_train_tf,y_train_tf)\n# predict the labels on validation dataset\npredictions_NB_tf = Naive.predict(X_test_tf)\n# Use accuracy_score function to get the accuracy\nprint(\"Naive Bayes Accuracy -> \",accuracy_score(predictions_NB_tf, y_test_tf)*100)\nprint(classification_report(predictions_NB_tf,y_test_tf))","50c6b748":"#Code by Leon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction\n\n#Save for the next since in subreddit.nsfw  ALL is False\n\n#logmodel = LogisticRegression()\n#logmodel.fit(X_train_tf, y_train_tf)\n\n#predictions_LR_tf = logmodel.predict(X_test_tf)\n\n#print(\"LR Accuracy -> \",accuracy_score(predictions_LR_tf, y_test_tf)*100)\n#print(classification_report(predictions_LR_tf,y_test_tf))","69325230":"#Create Corpus -> term document frequency\n\ndoc2bow() simply counts the number of occurrences of each distinct word, converts the word to its integer word ID and returns the result as a sparse vector.","9d5543c5":" #Nowadays, everything it's toxic: men, women, codes, opinions, comments, Notebooks.\n \n #\"If that's not good enough for you hey so long sucker, see ya, bon voyage, arrivederci, later loser, goodbye, good riddance, peace out, let the doorknob hit ya where the good Lord split ya, don't come back around here no more, hasta la vista, kick rocks, and get the hell out.\"\n \n![](https:\/\/i.pinimg.com\/originals\/1d\/e6\/a9\/1de6a924c6ec13e1ac7abbce794243cb.jpg) pinterest.com\n \nBest toxic funny quote. Source: My wife and Kids\n \nhttps:\/\/www.imdb.com\/title\/tt0655255\/characters\/nm0001834\n \nIn fact, I'm not most of my time positive. Besides, I've my Blacklist. Which isn't very positive.   ","d037775b":"<center style=\"font-family:verdana;\"><h1 style=\"font-size:200%; padding: 10px; background: \"#navyblue\";\"><b style=\"color:#DC143C;\">Toxic Positivity<\/b><\/h1><\/center>\n\n\" Toxic positivity involves dismissing negative emotions and responding to distress with false reassurances rather than empathy. It comes from feeling uncomfortable with negative emotions. It is often well-intentioned but can cause alienation and a feeling of disconnection.\"\n\nExamples of Toxic Positivity: Outstanding, stunning, astonishing, excellent, great and many other words that I constantly apply while I'm commenting.","783ce632":"#TF\/IDF","ac8ccad4":"#Natural Language Processing\n\nTopic Modelling","ace1fa74":"![](https:\/\/www.boredpanda.com\/blog\/wp-content\/uploads\/2021\/09\/Ominous-Positivity-Memes-2021-Fall-Winter-614b169b33f54__700.jpg)boredpanda.com","1cd1a67e":"#<font color=\"#EC7063\">Classifying Toxicity<\/font>\n\nClassifying Toxicity in Online Comment forums: End-to-End Project, By Luke Newman\n\n\"Comments were labeled with a probability of making another person disengage from a conversation. Toxic was defined as any probability greater than 0.5.\"\n\n\"In that process, we'll removing unwanted characters, tokenize, removing stopwords and contractions, speech tagging, lemmatize. Then the modeling process:\"\n\n\"Embedding (Countvectorizer\/TF-IDF\/Word2Vec;; Dimensionality reduction (LSA);; Classification (LR\/NN).\"\n\n\" Long Short-Term Memory (LSTM).That RNN allows us to capture spacial patterns of word usage, which are extremely relevant in how humans communicate.\"\n\n\"How can this model be used to increase civility in our online conversations? It depends on the platform.\" In Social media, real time toxicity trackers can be placed above comments as people type.\"\n\n\" It would be useful to check if users who have a history of being toxic change their behavior\"\n\nhttps:\/\/towardsdatascience.com\/classifying-toxicity-in-online-comment-forums-end-to-end-project-57720af39d0b","fd71862b":"#<font color=\"#EC7063\">Ethical considerations for detoxification.<\/font>\n\n\n#<font color=\"#EC7063\">Reducing Toxicity<\/font>\n\nReducing Toxicity in Language Models by Lilian Weng\n\n\"Training data annotations for toxicity detection on the high level can be collected by:\n\n\"Expert coding: An expert has enough knowledge or training to complete the annotation tasks with good quality, such as a researcher who studies prejudice, a student with moderate level of training, or a NLP practitioner. It is more expensive but produces high-quality data.\"\n\n\"Crowdsourcing: Crowdsourcing platform pairs a large number of non-expert annotators with tasks. It is easier to scale up but demands more attention on quality control.\"\n\n\"Professional moderators: Professional moderators are experienced, well-trained on the tasks, but their goals are likely to optimize for the output specific to the platform.\"\n\n\"Synthetic data: Training dataset can also be manually created by relevant content creators to cover a broad range of toxic content types.\"\n\n\"Good practices to improve the data quality:\"\n\n\"Test data: A small set of annotations collected from a few experts can be used as test questions (Zampieri et al. 2019) to filter out human annotators on the crowdsourcing platform who cannot achieve a certain threshold.\"\n\n\"Clear guidelines: Detailed instructions are useful to guide annotators to produce aligned and consistent labels. Without any guideline, annotators are encouraged to apply their personal perceptions, which could be problematic because (1) subjective interpretation of toxic content varies across individuals greatly and (2) it is tricky to mark certain types of noise like sarcasm and irony without any guideline.\"\n\n\"Majority vote: It is very common that we need labels from multiple annotators per sample and take the majority vote.\"\n\n\"Understanding annotators\u2019 identities: Demographic background has a big impact on the annotator\u2019s understanding of the task. We should aim to recruit diverse and qualified annotators.\"\n\nhttps:\/\/lilianweng.github.io\/lil-log\/2021\/03\/21\/reducing-toxicity-in-language-models.html","ec953efd":"#Show the output of the model","27231266":"#That is the more toxic? I think I could make it toxier, toxiest.","e74608de":"#Screw them all. I'm out of here. IDcab and their new found attack dog jossi have won. Adios.\n\nThat sentence is written in the output above. Precisely, above the 1st number one.","7b427fc0":"#It seems that the validation data is more interesting:  What!!!!!!!!?!?!!?!?!!?!?!?!?!!!!!!!!!!!!!!!!!.","50fe12cc":"If you feel that you're being attacked. Probably, you've already been attacked.\nBesides, ignoring things, words, people and work can be impregnated by toxicity as I wrote at the title. Much more than any offensive language. Or maybe, you're in the Blacklist\ud83d\ude04.","e9daa6e7":"#<font color=\"#EC7063\">Datasets<\/font>\n\n\"Hate Speech and Offensive Language Dataset (2017): contains about 25k tweets, each labelled manually as one of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech. \"\n\n\"Jigsaw Toxic Comments Classification Dataset (2018): contains about 160k examples extracted from Wikipedia discussion pages, each annotated for 7 classes: toxic, severe toxic, obscene, threat, insult, identity hate and non-toxic. The labelling process involved 5000 crowdsourced annotators. \"\n\n\"Jigsaw Unintended Bias in Toxicity Classification Dataset (2019): contains about 2 Millions comments from the Civil Comments platform, which shut down in 2017. This data is annotated for toxicity, toxicity sub-types, and mentions of identities, which enables evaluation of unintended bias with respect to identity mentions. \"\n\n\"OLID (Offensive Language Identification Dataset; 2019): contains 14,100 English tweets, annotated according to the three-level taxonomy as described here. \"\n\n\"SOLID (Semi-Supervised Offensive Language Identification Dataset; 2020): contains 9+ Millions tweets annotated following OLID\u2019s three level taxonomy. \"\n\n\"RealToxicityPrompts dataset (2020): contains 100k sentence snippets from the web with Perspective API toxicity scores for studying the risk of neural toxic degeneration in language models.\"\n\nhttps:\/\/lilianweng.github.io\/lil-log\/2021\/03\/21\/reducing-toxicity-in-language-models.html","f6a727c4":"#Acknowledgement\n\nLeon Wolber https:\/\/www.kaggle.com\/leonwolber\/reddit-nlp-topic-modeling-prediction","c37a26ed":"#<font color=\"#EC7063\">Some References for those that intend to go further<\/font>\n\n\nGehman, S., Gururangan, S., Sap, M., Choi, Y., & Smith, N. A. (2020). RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models. Findings of the Association for Computational Linguistics: EMNLP 2020, 3356\u20133369.\n\nPavlopoulos, J., Sorensen, J., Dixon, L., Thain, N., & Androutsopoulos, I. (2020). Toxicity Detection: Does Context Really Matter? ArXiv:2006.00998 [Cs].\n\nVaidya, A., Mai, F., & Ning, Y. (2020). Empirical Analysis of Multi-Task Learning for Reducing Model Bias in Toxic Comment Detection. ArXiv:1909.09758 [Cs].\n\nZhou, X., Sap, M., Swayamdipta, S., Smith, N. A., & Choi, Y. (2021). Challenges in Automated Debiasing for Toxic Language Detection. ArXiv:2102.00086 [Cs].\n\nSheth, A., Shalin, V. L., & Kursuncu, U. (2021). Defining and Detecting Toxicity on Social Media: Context and Knowledge are Key. ArXiv:2104.10788 [Cs].\n\nXu, A., Pathak, E., Wallace, E., Gururangan, S., Sap, M., & Klein, D. (2021). Detoxifying Language Models Risks Marginalizing Minority Voices. ArXiv:2104.06390 [Cs].\n\nSchick, T., Udupa, S., & Sch\u00fctze, H. (2021). Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP. ArXiv:2103.00453 [Cs].","5e366ef4":"#\"See ya sucker. Don't come back here no more, and get the Hell out!\n\n#Stick your vote there!\" Lot's of funny toxicity!","668894dd":"#Create TF\/IDF again","b8c38a4d":"#<font color=\"#EC7063\">BERT, LSTM, FastText, PMI (n-gram similarity)<\/font>\n\n\n\"SOLID (Semi-Supervised Offensive Language Identification Dataset; Rosenthal et al. 2020) contains 9+ M tweets annotated with the same taxonomy system as for OLID. . SOLID is constructed by:\"\n\n\"First, train a diverse set of supervised models on the labeled dataset OLID. The paper experimented with PMI (n-gram-based similarity), FastText (shallow neural model similar to BoW model), LSTM and BERT.\"\n\n\"For each sample in the unannotated dataset, each model predicts a confidence score for the target class. The scores are aggregated by taking avg() or min(). Samples with high confidence are added into the dataset.\"\n\n\"BERT model performance does not improve when the supervised dataset is large enough for a simple task, but can benefit from a big semi-supervised dataset if the original supervised dataset is too small for the task.\"\n\nhttps:\/\/lilianweng.github.io\/lil-log\/2021\/03\/21\/reducing-toxicity-in-language-models.html","25725d78":"#<font color=\"#EC7063\">ML, AI, NLP to support healthy conversations.<\/font>\n\nNowadays, everything is labeled as toxic: Toxic Positivity, relationship, masculinity, friendship, toxic workplace and so on.\n\n\"Toxicity isn\u2019t just an issue of a healthy discourse. It poses a threat to press, platforms, and society. Machine Learning, AI and NLP can help to improve conversations\"\n\n\"Jigsaw\u2019s API is helping platforms , to support healthy conversations.\"\n\nhttps:\/\/jigsaw.google.com\/the-current\/toxicity\/","33cc2854":"#Your notebook tried to allocate more memory than is available. It has restarted.\n\n#That's the most toxic in Kaggle!","f4666222":"#Cleaning functions","b38160c6":"#Reading the output above is funny. But I've more important things to do. Hence I collapsed it.","06d7f5b7":"#Create the dictionary\n\nEvery unique word in more_toxic","bb691f19":"#A lot of fucking again, bitch.","bac28c2b":"#<font color=\"#EC7063\">Toxicity in AI Text Generation<\/font>\n\nToxicity in AI Text Generation, By Julia Nikulski\n\n\"We should be aware of the potential harm caused by applying language models in user-facing projects. Why language models tend to create such hateful language and how this issue can be addressed.\"\n\n\"Various research papers have investigated toxicity and social bias inherent in autoregressive LMs \u2014 such as GPT-2 [1] \u2014 and bidirectional encoder models \u2014 such as BERT [2]. \"\n\n\"Unfortunately, language that is non-toxic in a specific context can be \u201creshuffled\u201d by language models.\"\n\n\"Methods as data-based or decoding-based strategies can help toxicity generation be reduced.\"\n\nhttps:\/\/towardsdatascience.com\/toxicity-in-ai-text-generation-9e9d9646e68f","44d5c624":"#A cheesy blue vagina syndrome. Very likely, it's a man's quote. He had already tasted that vajayjay.\n\n#Now,he is despising it. Son of a Bitch!","1936c2ea":"#\"Assholes\") appears only 1 time. After I mention that twice.","ee43d792":"#Fucker; Shit; Suck; Pennis; Cunt; Damn; Die. How old are them? South Park is heavier than this."}}