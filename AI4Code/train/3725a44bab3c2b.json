{"cell_type":{"db614982":"code","8509a58f":"code","8ee0669d":"code","c0da3bfb":"code","fef8ab64":"code","0e74d5a2":"code","499b6d85":"code","455e7a6f":"code","fed21ed8":"code","069987c2":"code","ce04fe1c":"code","90dc07b1":"code","2f695080":"code","bdab4c51":"code","d2ccea38":"code","304c7a53":"code","2ef40881":"code","19adac88":"code","ca8fb0fb":"code","a9d3043e":"code","ee97d64d":"code","a09ded08":"code","8815b3f1":"code","b5a321b8":"code","dbb28af7":"code","d1393a09":"code","d3a198c8":"code","ac106f00":"code","2dfea03d":"code","29ab1db8":"code","fb5ceb03":"code","269498b8":"code","a3295d5c":"code","2ec35494":"code","b3bf1ccd":"code","86b7b61b":"markdown","99129d25":"markdown","cc389f6f":"markdown","a0c0d419":"markdown","ffa87d5b":"markdown","78caceb8":"markdown","a9c41b30":"markdown","6d5fa321":"markdown","f72148ec":"markdown"},"source":{"db614982":"from IPython.display import Image\nImage(url= \"https:\/\/img4.cityrealty.com\/neo\/i\/p\/mig\/airbnb_guide.jpg\")","8509a58f":"import numpy as np                 # Linear Algebra\nimport pandas as pd                # Data Processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib                  # 2D Plotting Library\nimport matplotlib.pyplot as plt\nimport seaborn as sns              # Python Data Visualization Library based on matplotlib\nimport geopandas as gpd            # Python Geospatial Data Library\nplt.style.use('fivethirtyeight')\n%matplotlib inline\n\nimport plotly as plotly                # Interactive Graphing Library for Python\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot, plot\ninit_notebook_mode(connected=True)\n\nimport folium\nimport folium.plugins\n# featuretools for automated feature engineering\nimport featuretools as ft\n\n# ignore warnings from pandas\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport wordcloud\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nimport sklearn\nfrom sklearn.linear_model import LinearRegression,RidgeCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import r2_score,mean_squared_error,make_scorer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score, mean_absolute_error\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression,LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder,StandardScaler,MinMaxScaler\nfrom xgboost import XGBRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import StratifiedShuffleSplit, KFold, GridSearchCV, RandomizedSearchCV,train_test_split,cross_val_score\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.tree import ExtraTreeRegressor,DecisionTreeRegressor\nfrom sklearn.neural_network import MLPRegressor\n","8ee0669d":"data=pd.read_csv('..\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv')","c0da3bfb":"data.head()","fef8ab64":"data.describe()","0e74d5a2":"data.info()","499b6d85":"data.isna().sum()","455e7a6f":"plt.figure(figsize=(12,4))\nsns.heatmap(data.isnull(),cbar=False,cmap='viridis',yticklabels=False)\nplt.title('Missing value in the dataset');","fed21ed8":"df_airbnb = data.fillna({'reviews_per_month':0})","069987c2":"df_airbnb = df_airbnb.dropna()\ndata_viz = data.dropna()","ce04fe1c":"col = \"room_type\"\ngrouped = data_viz[col].value_counts().reset_index()\ngrouped = grouped.rename(columns = {col : \"count\", \"index\" : col})\n\n## plot\ntrace = go.Pie(labels=grouped[col], values=grouped['count'], pull=[0.05, 0], marker=dict(colors=[\"#6ad49b\", \"#a678de\"]))\nlayout = go.Layout(title=\"\", height=400, legend=dict(x=0.1, y=1.1))\nfig = go.Figure(data = [trace], layout = layout)\niplot(fig)","90dc07b1":"col = \"neighbourhood_group\"\ngrouped = data_viz[col].value_counts().reset_index()\ngrouped = grouped.rename(columns = {col : \"count\", \"index\" : col})\n\n## plot\ntrace = go.Pie(labels=grouped[col], values=grouped['count'], pull=[0.05, 0], marker=dict(colors=[\"#6ad49b\", \"#a678de\"]))\nlayout = go.Layout(title=\"\", height=400, legend=dict(x=0.1, y=1.1))\nfig = go.Figure(data = [trace], layout = layout)\niplot(fig)","2f695080":"plt.figure(figsize=(10,6))\nsns.set_style(\"darkgrid\")\nplt.title('Distribution of Airbnb price')\nsns.kdeplot(data=data_viz['price'], shade=True).set(xlim=(0))","bdab4c51":"ent_home = data_viz[data_viz.room_type == 'Entire home\/apt']\nproom = data_viz[data_viz.room_type == 'Private room']\nsh_room = data_viz[data_viz.room_type == 'Shared room']\n\nplt.figure(figsize=(10,6))\nsns.set_style(\"darkgrid\")\nsns.kdeplot(data=ent_home['price'],label='Entire home\/apt', shade=True)\nsns.kdeplot(data=proom['price'],label='Private room', shade=True)\nsns.kdeplot(data=sh_room['price'],label='Shared room', shade=True)","d2ccea38":"sns.lmplot(x='price',y='number_of_reviews',data=data_viz,aspect=2,height=6)\nplt.xlabel('Price')\nplt.ylabel('No of Reviews')\nplt.title('Price vs Reviews');","304c7a53":"plt.figure(figsize=(14,6))\nsns.boxplot(x='neighbourhood_group', y='availability_365',data=data_viz,palette='rainbow')\nplt.title('Box plot of neighbourhood_group vs Availability');","2ef40881":"name =  data_viz['name']\nname  = name.dropna()","19adac88":"from wordcloud import WordCloud, ImageColorGenerator\ntext = \" \".join(str(each) for each in name)\nwordcloud = WordCloud(max_words=200,colormap='Set3', background_color=\"white\").generate(text)\nplt.figure(figsize=(10,6))\nplt.figure(figsize=(15,10))\nplt.imshow(wordcloud, interpolation='Bilinear')\nplt.axis(\"off\")\nplt.figure(1,figsize=(12, 12))\nplt.show()","ca8fb0fb":"es = ft.EntitySet(id = 'airbnb')","a9d3043e":"df_airbnb","ee97d64d":"# Create an entity from the client dataframe\n# This dataframe already has an index and a time index\nes = es.entity_from_dataframe(entity_id = 'airbnb', dataframe = df_airbnb, \n                              index = 'id', time_index = 'last_review')","a09ded08":"es","8815b3f1":"# Perform deep feature synthesis without specifying primitives\nfeatures, feature_names = ft.dfs(entityset=es, target_entity='airbnb', \n                                 max_depth = 2)","b5a321b8":"print(feature_names)\nfeatures.head()","dbb28af7":"f, ax = plt.subplots(figsize=(10, 8))\ncorr = features.corr()\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(240,10,as_cmap=True),\n            square=True, ax=ax)","d1393a09":"features['price']=features['price'].replace(0,features['price'].mean())\nfeatures.drop(['name','host_name','host_id','latitude','longitude'], axis=1, inplace=True)","d3a198c8":"# Dummy variable\ncategorical_columns = ['neighbourhood_group','neighbourhood', 'room_type']\ndf_encode = pd.get_dummies(data = features, prefix = 'OHE', prefix_sep='_',\n               columns = categorical_columns,\n               drop_first =True,\n              dtype='int8')\nprint(features.columns)","ac106f00":"# Lets verify the dummay variable process\nprint('Columns in original data frame:\\n',features.columns.values)\nprint('\\nNumber of rows and columns in the dataset:',features.shape)\nprint('\\nColumns in data frame after encoding dummy variable:\\n',df_encode.columns.values)\nprint('\\nNumber of rows and columns in the dataset:',df_encode.shape)","2dfea03d":"from scipy.stats import boxcox\ny_bc,lam, ci= boxcox(df_encode['price'],alpha=0.05)\nci,lam","29ab1db8":"x = df_encode.drop(['price'], axis = 1)\ny = df_encode.price\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.33,random_state=353)","fb5ceb03":"clfs = []\nseed = 3\n\nclfs.append((\"LinearRegression\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"LogReg\", LinearRegression())])))\n\nclfs.append((\"XGB\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"XGB\", XGBRegressor())]))) \nclfs.append((\"KNN\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"KNN\", KNeighborsRegressor())]))) \n\nclfs.append((\"DTR\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"DecisionTrees\", DecisionTreeRegressor())]))) \n\nclfs.append((\"RFRegressor\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"RandomForest\", RandomForestRegressor())]))) \n\nclfs.append((\"GBRegressor\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"GradientBoosting\", GradientBoostingRegressor(max_features=15, \n                                                                       n_estimators=600))]))) \n\nclfs.append((\"MLP\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"MLP Regressor\", MLPRegressor())])))\n\n\nclfs.append((\"EXT Regressor\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"ExtraTrees\", ExtraTreeRegressor())])))\nclfs.append((\"SV Regressor\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"ExtraTrees\", SVR())])))\n\nscoring = 'r2'\nn_folds = 10\nmsgs = []\nresults, names  = [], [] \n\nfor name, model  in clfs:\n    kfold = KFold(n_splits=n_folds, random_state=seed)\n    cv_results = cross_val_score(model, x_train, y_train, \n                                 cv=kfold, scoring=scoring, n_jobs=-1)    \n    names.append(name)\n    results.append(cv_results)    \n    msg = \"%s: %f (+\/- %f)\" % (name, cv_results.mean(),  \n                               cv_results.std())\n    msgs.append(msg)\n    print(msg)","269498b8":"lr = LinearRegression().fit(x_train,y_train)\n\ny_train_pred = lr.predict(x_train)\ny_test_pred = lr.predict(x_test)\n\nprint(lr.score(x_test,y_test))","a3295d5c":"forest = RandomForestRegressor(n_estimators = 100,\n                              criterion = 'mse',\n                              random_state = 1,\n                              n_jobs = -1)\nforest.fit(x_train,y_train)\nforest_train_pred = forest.predict(x_train)\nforest_test_pred = forest.predict(x_test)\n\nprint('MSE train data: %.3f, MSE test data: %.3f' % (\nmean_squared_error(y_train,forest_train_pred),\nmean_squared_error(y_test,forest_test_pred)))\nprint('R2 train data: %.3f, R2 test data: %.3f' % (\nr2_score(y_train,forest_train_pred),\nr2_score(y_test,forest_test_pred)))","2ec35494":"plt.figure(figsize=(10,6))\n\nplt.scatter(forest_train_pred,forest_train_pred - y_train,\n          c = 'black', marker = 'o', s = 35, alpha = 0.5,\n          label = 'Train data')\nplt.scatter(forest_test_pred,forest_test_pred - y_test,\n          c = 'c', marker = 'o', s = 35, alpha = 0.7,\n          label = 'Test data')\nplt.xlabel('Predicted values')\nplt.ylabel('Tailings')\nplt.legend(loc = 'upper left')\nplt.hlines(y = 0, xmin = 0, xmax = 60000, lw = 2, color = 'red')\nplt.show()\n","b3bf1ccd":"## Applying L2 Regularization\nscorer = make_scorer(mean_squared_error, greater_is_better = False)\n\ndef rmse_cv_train(model):\n    rmse= np.sqrt(-cross_val_score(model, x_train, y_train, scoring = scorer, cv = 10))\n    return(rmse)\n\ndef rmse_cv_test(model):\n    rmse= np.sqrt(-cross_val_score(model, x_test, y_test, scoring = scorer, cv = 10))\n    return(rmse)\nridge = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60])\nridge.fit(x_train, y_train)\nalpha = ridge.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Try again for more precision with alphas centered around \" + str(alpha))\nridge = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, \n                          alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,\n                          alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4], \n                cv = 10)\nridge.fit(x_train, y_train)\nalpha = ridge.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Ridge RMSE on Training set :\", rmse_cv_train(ridge).mean())\nprint(\"Ridge RMSE on Test set :\", rmse_cv_test(ridge).mean())\ny_train_rdg = ridge.predict(x_train)\ny_test_rdg = ridge.predict(x_test)\n\n# Plot residuals\nplt.scatter(y_train_rdg, y_train_rdg - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_rdg, y_test_rdg - y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with Ridge regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train_rdg, y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_rdg, y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with Ridge regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()","86b7b61b":">The general problem of feature engineering is taking disparate data, often distributed across multiple tables, and combining it into a single table that can be used for training a machine learning model. Featuretools has the ability to do this for us, creating many new candidate features with minimal effort. These features are combined into a single table that can then be passed on to our model.\n\n>Now that we know what we are trying to avoid (tedious manual feature engineering), let's figure out how to automate this process. Featuretools operates on an idea known as Deep Feature Synthesis. You can read the original paper here, and although it's quite readable, it's not necessary to understand the details to do automated feature engineering. The concept of Deep Feature Synthesis is to use basic building blocks known as feature primitives (like the transformations and aggregations done above) that can be stacked on top of each other to form new features. The depth of a \"deep feature\" is equal to the number of stacked primitives.\n\n> Featuretools builds on simple ideas to create a powerful method, and we will build up our understanding in much the same way.\n\n###### The first part of Featuretools to understand is an entity. This is simply a table, or in pandas, a DataFrame. We corral multiple entities into a single object called an EntitySet. This is just a large data structure composed of many individual entities and the relationships between them.","99129d25":"### Predictive Modelling","cc389f6f":"### Experimentation with Automated Feature Enginnering\n#### [Feature Tools](https:\/\/www.featuretools.com\/)","a0c0d419":"### Entities\nAn entity is simply a table, which is represented in Pandas as a dataframe. Each entity must have a uniquely identifying column, known as an index.\n\nWhen we create an entity in featuretools, we have to identify which column of the dataframe is the index. If the data does not have a unique index we can tell featuretools to make an index for the entity by passing in make_index = True and specifying a name for the index. If the data also has a uniquely identifying time index, we can pass that in as the time_index parameter.\n\nFeaturetools will automatically infer the variable types (numeric, categorical, datetime) of the columns in our data, but we can also pass in specific datatypes to override this behavior. As an example, even though the repaid column in the loans dataframe is represented as an integer, we can tell featuretools that this is a categorical feature since it can only take on two discrete values. This is done using an integer with the variables as keys and the feature types as values.\n\nIn the code below we create the entities and add them to the EntitySet. The syntax is relatively straightforward.","ffa87d5b":"##### Automated Deep Feature Synthesis\n> In addition to manually specifying aggregation and transformation feature primitives, we can let featuretools automatically generate many new features. We do this by making the same ft.dfs function call, but without passing in any primitives. We just set the max_depth parameter and featuretools will automatically try many all combinations of feature primitives to the ordered depth.\n\nWhen running on large datasets, this process can take quite a while, but for our example data, it will be relatively quick. For this call, we only need to specify the entityset, the target_entity, and the max_depth.","78caceb8":"## Exploratory Data Analysis\n#### Data Visualizations and More..............","a9c41b30":"# New York City Airbnb Open Data\n## Dataset Description\n>Since 2008, guests and hosts have used Airbnb to expand on traveling possibilities and present more unique, personalized way of experiencing the world. This dataset describes the listing activity and metrics in NYC, NY for 2019","6d5fa321":"## If you find this Kernal Awesome & Useful too Please Hit the Upvote!!!\n##### Cheers!!!!","f72148ec":"<b> Deep feature synthesis has created 5 new features out of the existing data! While we could have created all of these manually, I am glad to not have to write all that code by hand. <\/b>"}}