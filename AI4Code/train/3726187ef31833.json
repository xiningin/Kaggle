{"cell_type":{"b9bfb0cc":"code","116f1021":"code","d4b931b7":"code","bced33b6":"code","490db78b":"code","0d9bf0eb":"code","ec84a1f9":"code","3adecf9a":"code","6a517f3e":"code","a58c0e5e":"code","0a314529":"code","0938d910":"code","4c8c4a7c":"code","8a4fad22":"code","344fc9f1":"code","30e2603f":"code","a843b820":"code","80a94138":"code","ed1ca0cb":"code","be147c52":"code","2131c2d4":"code","0c24cdff":"code","dfef9e77":"code","1deff2cf":"code","e70afd9e":"code","632778ac":"code","5ab2a622":"code","8f46389d":"code","e6d2b16b":"code","67f07704":"code","8dc12fbf":"code","ad5e65b4":"code","28e7f793":"code","d954d093":"code","85bd4f65":"code","9df00f3e":"code","2471676a":"code","ff5666b5":"code","d778dd39":"code","8af39845":"code","d09c9175":"code","7c1ac1d9":"code","ef9eef0f":"code","96f4e09f":"code","aca5175f":"code","ceee25bb":"code","b901a25f":"code","72e92192":"code","52e61f46":"code","a4e1fa02":"code","1c9b8c2b":"code","ff1863cf":"code","ab852f26":"code","c4cf0ee3":"markdown","3ee87765":"markdown","9a70be44":"markdown","5155e6f9":"markdown","300c9678":"markdown","3d995eab":"markdown","41a57c75":"markdown","56cb8ddb":"markdown","4aaca4ad":"markdown","dd9e35be":"markdown","be889a58":"markdown","a4fa713d":"markdown","4a57cd00":"markdown","17f81b6e":"markdown","3ea091f3":"markdown","111c65ad":"markdown","4d71f2d2":"markdown","8e37ad44":"markdown","81164cc8":"markdown","3b2b175b":"markdown","9734b6bd":"markdown","fba034fb":"markdown","3e14d6f1":"markdown","34d38392":"markdown","3787a1f1":"markdown","cef1a659":"markdown","dbbb0f45":"markdown","a1fc860a":"markdown","5555813f":"markdown","3645314a":"markdown","1f6f5dcd":"markdown","892c58f9":"markdown","61565a90":"markdown","8a694ba8":"markdown","e40a564b":"markdown","2d515ef9":"markdown","fc33de74":"markdown","5f224705":"markdown","6f9e3fcf":"markdown","26b87391":"markdown","96822fc9":"markdown","6a680eaa":"markdown","0f3722eb":"markdown","57c2e35b":"markdown","66daf149":"markdown","26916d8c":"markdown","1a81d049":"markdown","54f802ad":"markdown","92241850":"markdown","2f902373":"markdown","ae7faeea":"markdown","3ed639df":"markdown","708dd9e5":"markdown","4638ee7b":"markdown","84a472b5":"markdown","c182ceab":"markdown","e1ecb33a":"markdown","a836d15a":"markdown","2bee494b":"markdown","8cfb7f0c":"markdown","44936005":"markdown","7eabf038":"markdown","1fbf321a":"markdown","eb198257":"markdown","ecdec605":"markdown","29d78401":"markdown","e9b8dfa7":"markdown","57f83426":"markdown","64bc51bc":"markdown","41c890ad":"markdown"},"source":{"b9bfb0cc":"#import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","116f1021":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')","d4b931b7":"train_df.columns.values","bced33b6":"#preview the data\ntrain_df.head(10)","490db78b":"train_df.describe()","0d9bf0eb":"train_df.describe(include=['O'])","ec84a1f9":"train_df.isnull().sum().sort_values(ascending=False)","3adecf9a":"train_df[['Pclass','Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","6a517f3e":"train_df[['Sex','Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","a58c0e5e":"train_df[['Parch','Survived']].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","0a314529":"train_df[['SibSp','Survived']].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","0938d910":"g = sns.FacetGrid(train_df, col='Sex', row='Survived', margin_titles=True)\ng.map(plt.hist, 'Age', bins=20)","4c8c4a7c":"grid = sns.FacetGrid(train_df, col = 'Survived', row = 'Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","8a4fad22":"grid = sns.FacetGrid(train_df, row='Embarked', size=2.2, aspect = 1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()","344fc9f1":"grid = sns.FacetGrid(train_df, row='Embarked', col='Survived', size = 2.2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\ngrid.add_legend()","30e2603f":"train_df = train_df.drop(['Cabin','Ticket'], axis = 1)\ntest_df = test_df.drop(['Cabin', 'Ticket'], axis=1)","a843b820":"combine = [train_df, test_df]\n\nfor data in combine:\n    data['Title'] = data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    \npd.crosstab(train_df['Title'], [train_df['Sex'], train_df['Survived']])","80a94138":"grid = sns.countplot(x='Title', data=train_df)\ngrid = plt.setp(grid.get_xticklabels(), rotation=45)","ed1ca0cb":"for data in combine:\n    data['Title'] = data['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    data['Title'] = data['Title'].replace('Ms', 'Miss')\n    data['Title'] = data['Title'].replace('Mme', 'Mrs')\n    data['Title'] = data['Title'].replace('Mlle', 'Miss')\n    \ntrain_df[['Title','Survived']].groupby(['Title'], as_index=False).mean()","be147c52":"grid = sns.countplot(x='Title', data=train_df)\ngrid = plt.setp(grid.get_xticklabels(), rotation=45)","2131c2d4":"title_mapping = {'Mr':1, 'Miss':2, 'Mrs':3, 'Master':4, 'Rare':5}\nfor data in combine:\n    data['Title'] = data['Title'].map(title_mapping)\n    data['Title'] = data['Title'].fillna(0)\n    \ntrain_df.head()","0c24cdff":"train_df = train_df.drop(['Name', 'PassengerId'], axis = 1)\ntest_df = test_df.drop(['Name'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.shape, test_df.shape","dfef9e77":"for data in combine:\n    data['Sex'] = data['Sex'].map({'female':1, 'male':0}).astype(int)\n    \ntrain_df.head()","1deff2cf":"for data in combine:\n    mean = train_df['Age'].mean()\n    std = test_df['Age'].std()\n    null_count = data['Age'].isnull().sum()\n    rand_age = np.random.randint(mean - std, mean + std, size = null_count)\n    \n    age_slice = data[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    data[\"Age\"] = age_slice\n    data[\"Age\"] = train_df[\"Age\"].astype(int)\n    \ntrain_df[\"Age\"].isnull().sum()","e70afd9e":"train_df.head()","632778ac":"train_df['AgeBand']= pd.cut(train_df['Age'],5)\ntrain_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","5ab2a622":"for data in combine:\n    data.loc[data['Age'] <=16, 'Age'] = 0\n    data.loc[(data['Age'] >16 ) & (data['Age'] <= 32), 'Age']=1\n    data.loc[(data['Age'] > 32) & (data['Age'] <= 48), 'Age'] = 2\n    data.loc[(data['Age'] > 48) & (data['Age'] <= 64), 'Age'] = 3\n    data.loc[ data['Age'] > 64, 'Age']\ntrain_df.head()","8f46389d":"train_df = train_df.drop(['AgeBand'], axis=1)\ncombine = [train_df, test_df]","e6d2b16b":"for data in combine:\n    data['FamilySize'] = data['SibSp'] + data['Parch'] + 1\n    \ntrain_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","67f07704":"for data in combine:\n    data['IsAlone']=0\n    data.loc[data['FamilySize'] == 1, 'IsAlone']=1\n    \ntrain_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","8dc12fbf":"train_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ntest_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ncombine = [train_df, test_df]\n\ntrain_df.head()","ad5e65b4":"for data in combine:\n    data['AgeClass'] = data['Age']* data['Pclass']","28e7f793":"freq = train_df.Embarked.dropna().mode()[0]\nfreq","d954d093":"for data in combine:\n    data['Embarked'] = data['Embarked'].fillna(freq)\n    \ntrain_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","85bd4f65":"#convert categorical feature to numeric\nfor data in combine:\n    data['Embarked'] = data['Embarked'].map({'S':0,'C':1,'Q':2}).astype(int)\n    \ntrain_df.head()","9df00f3e":"train_df[\"Fare\"].isnull().sum(), test_df[\"Fare\"].isnull().sum()","2471676a":"test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)","ff5666b5":"train_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","d778dd39":"for data in combine:\n    data.loc[ data['Fare'] <= 7.91, 'Fare'] = 0\n    data.loc[(data['Fare'] > 7.91) & (data['Fare'] <= 14.454), 'Fare'] = 1\n    data.loc[(data['Fare'] > 14.454) & (data['Fare'] <= 31), 'Fare']   = 2\n    data.loc[ data['Fare'] > 31, 'Fare'] = 3\n    data['Fare'] = data['Fare'].astype(int)\n\ntrain_df = train_df.drop(['FareBand'], axis=1)\ncombine = [train_df, test_df]\n    \ntrain_df.head()","8af39845":"test_df.head()","d09c9175":"X_train = train_df.drop('Survived', axis=1)\nY_train = train_df['Survived']\nX_test = test_df.drop('PassengerId', axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","7c1ac1d9":"# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","ef9eef0f":"sgd = SGDClassifier(max_iter=5, tol=None)\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\n\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nprint(acc_sgd, '%')","96f4e09f":"logreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\n\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nprint(acc_log, '%')","aca5175f":"pcpt = Perceptron(max_iter=5)\npcpt.fit(X_train, Y_train)\nY_pred = pcpt.predict(X_test)\n\nacc_pcpt = round(pcpt.score(X_train, Y_train)*100, 2)\nprint(acc_pcpt, '%')","ceee25bb":"rf = RandomForestClassifier(n_estimators=100)\nrf.fit(X_train, Y_train)\nY_pred = rf.predict(X_test)\n\nacc_rf = round(rf.score(X_train, Y_train) * 100, 2)\nprint(acc_rf, '%')","b901a25f":"dt = DecisionTreeClassifier()\ndt.fit(X_train, Y_train)\nY_pred = dt.predict(X_test)\n\nacc_dt = round(dt.score(X_train, Y_train)*100,2)\nprint(acc_dt, '%')","72e92192":"svc = LinearSVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\n\nacc_svc = round(svc.score(X_train, Y_train)*100, 2)\nprint(acc_svc, '%')","52e61f46":"knn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\n\nacc_knn = round(knn.score(X_train, Y_train)*100, 2)\nprint(acc_knn, '%')","a4e1fa02":"gnb = GaussianNB()\ngnb.fit(X_train, Y_train)\nY_pred = gnb.predict(X_test)\n\nacc_gnb = round(gnb.score(X_train, Y_train)*100, 2)\nprint(acc_gnb, '%')","1c9b8c2b":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_rf, acc_gnb, acc_pcpt, \n              acc_sgd, acc_svc, acc_dt]})\nmodels.sort_values(by='Score', ascending=False)","ff1863cf":"# Random Forest\nrf = RandomForestClassifier(n_estimators=100)\nrf.fit(X_train, Y_train)\nY_pred = rf.predict(X_test)\n\nacc_rf = round(rf.score(X_train, Y_train) * 100, 2)\nprint(acc_rf, '%')","ab852f26":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\n\nsubmission.to_csv('submission.csv', index=False)","c4cf0ee3":"As we can observe - \n* **Ticket** - is mix of numeric and alphanumeric data types.\n* **Cabin** - is alphanumeric, also has some missing data\n* **Age** - has some missing data","3ee87765":"* The Cabin feature has 687 missing values, we might need to drop this feature since 77% of it is missing.\n* The Age feature has 177 (19%) missing values\n* The Embarked has 2 missing values, which can be filled easily.","9a70be44":"### 2. Converting a categorical feature (Sex) to numerical value.","5155e6f9":"### 5. Decision Tree","300c9678":"### 4. Create new feature - FamilySize based on Parch and SibSp to get total count of family members on board","3d995eab":"#### 1. Age and Sex","41a57c75":"**New features need to be created from existing features** - \n* Extract titles from passenger names\n* Convert categorical feature like sex to numerical values\n* Age bands - to turn the continous numerical feature into an ordinal categorical feature\n* Feature called Family based on Parch and SibSp to get total count of family members on board\n* Age times class\n* Convert categorical Embarked feature to numeric feature.\n* Fare range feature ","56cb8ddb":"Our submission to the competition site Kaggle results in scoring 0.77033 ranked 6040.","4aaca4ad":"Let us replace Age with ordinals based on these bands.","dd9e35be":"**Distribution of categorical features**","be889a58":"#### 3. Sex, Embarked and Pclass","a4fa713d":"**Observations:-**\n* Female passengers had much better survival rate than males, except in Embarked = C, where males had higher survival rate. This could be correlation between Pclass and Embarked and in turn Pclass and Survived.\n* Males had better survival rate in Pclass=3 when compared with Pclass= 2 for C and Q ports.\n* Ports of embarkation have varying survival rates for Pclass=3 and among male passengers. Correlating\n\n**Decisions:-**\n* Complete and add Embarked feature to model training.","4a57cd00":"### 3. Perceptron","17f81b6e":"Now we need to tackle the issue with the age features missing values. We will create an array that contains random numbers, which are computed based on the mean age value in regards to the standard deviation and is_null.","3ea091f3":"**Distribution of numerical feature values**","111c65ad":"### Submission","4d71f2d2":"### 3. To turn the continous numerical feature (Age) into an ordinal categorical feature.","8e37ad44":"Certain titles mostly survived (Mme, Ms, Lady, Sir) or did not (Rev, Don, Jonkheer).\n\nWe can replace many titles with a more common name or classify them as Rare.","81164cc8":"## Competition Description\n\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.","3b2b175b":"### 6. Linear SVC","9734b6bd":"### 4. Random Forest","fba034fb":"Let us create Age bands and determine correlations with Survived.","3e14d6f1":"We can observe here that -\n* **Names** are unique across the dataset\n* **Sex** has two values (male, female), with 65% male (577\/891)\n* **Cabin** values have several duplicates. Several passengers shared same cabin.\n* **Emabarked** takes three possible values. **S** port used by most passengers (72%)\n* **Ticket** feature have 22% duplicates (unique = 681)","34d38392":"#### 4. Embarked, Sex and Fare","3787a1f1":"**Embarked features has 2 missing values**, we need to fill these with most common occurance.","cef1a659":"## Pivoting data","dbbb0f45":"*Convert categorical titles to ordinal*","a1fc860a":"**Observations:-**\n* Higher fare paying passengers had better survival.\n* Port of embarkation correlates with survival rates.\n\n**Decisions:-**\n* Consider Fare feature.","5555813f":"Now we are ready to trian our model and predict the required solution. Our problem is a classification and regression problem. We need to identify relation between output **(Survived or not)** with other variables or features **(like Age, sex, Pclass)**","3645314a":"We observe that **female** had very high survival rate at **74%**.","1f6f5dcd":"### Which is the best model ?","892c58f9":"### 7. Fare range feature","61565a90":"### 7. KNN","8a694ba8":"**We can now safely drop the Name feature from training and testing datasets.**\n**We also do not need the PassengerId feature in the training dataset**","e40a564b":"### 6. Convert categorical Embarked feature to numeric feature.","2d515ef9":"The **Parch and SibSp** features have zero correlation for certain values. So it may be best to derive a feature or a set of features from these individual features.","fc33de74":"To confirm some of our observations and assumptions, we can quickly analyze our feature correlations by pivoting features against each other.\n\nIt makes sense doing so only for features which are categorical (Sex), ordinal (Pclass) or discrete (SibSp, Parch) type.","5f224705":"### 2. Logistic Regression","6f9e3fcf":"Create another feature IsAlone, where if FamilySize = 1 then IsAlone = 1 else 0","26b87391":"Let us create Fare bands and determine correlations with Survived.","96822fc9":"### 8. Gaussian Naive Bayes","6a680eaa":"The most frequent occuring value is **S**","0f3722eb":"### 5. Age times Class","57c2e35b":"Now we can drop Parch, SibSp and FamilySize feature in favour of IsAlone.","66daf149":"**Which features contain blank, null or empty values?**","26916d8c":"**Categorical features** - (Classify the samples into sets of similar samples) Survived, Sex, Embarked, Pclass\n\n**Numerical features** - (Change from sample to sample) Age, Fare, SibSp, Parch\n\n**Target feature** is Survived","1a81d049":"**Observations**-\n* Female passengers had much better survival rate than male passengers.\n* Infants (Age <=4 ) had high survival rate.\n* Older passenger (Age = 80) survived.\n* Large number of passengers between 15-25 years old did not survived.\n* Most passengers were between 15-35 age range.","54f802ad":"#### References\n* __[A journey through Titanic](https:\/\/www.kaggle.com\/omarelgabry\/a-journey-through-titanic)__\n* __[Titanic Data science solutions](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions)__\n* __[End to End Project with Python](https:\/\/www.kaggle.com\/niklasdonges\/end-to-end-project-with-python)__","92241850":"We observe significant correlation among **Pclass=1** and Survived. The upper class passengers were more likely to have survived.","2f902373":"#### 2. Age and Pclass","ae7faeea":"**Features needed to be dropped from the dataset** -\n* PassengerId - as it does not contribute to survival\n* Name - doest not contribute to survivale\n* Cabin - contains 77% missing data (drop from both train and test dataset)\n* Ticket - doest not contribute to survival and also contains 22% duplicates\n\nFirst we will drop the Cabin and Ticket features.","3ed639df":"*Plot Title and Sex*","708dd9e5":"**Which features are available in the dataset?**","4638ee7b":"Both **Decision Tree and Random Forest** score the same, we choose to use Random Forest, but first let us check, how random forest performs, when we use cross validation.","84a472b5":"## Visualize data","c182ceab":"## Load Dataset","e1ecb33a":"Converting Sex feature to a new feature where female = 1 and male = 0","a836d15a":"We can see here that -\n* Around **38%** samples survived representative of the actual survial rate at **32%**\n* The **Age** feature ranges from 0.42 to 80\n* The **Pclass** feature denotes the class of ticket which are categorized as 1,2,3\n* **Fares** varied significantly with few passengers (<1%) paying as high as $512.","2bee494b":"We can now remove the AgeBand feature.","8cfb7f0c":"## Feature Engineering","44936005":"### 1. Stochastic gradient descent (SGD) learning","7eabf038":"## Building Machine Learning Models","1fbf321a":"Understanding correlations between numerical features and our goal Survived.","eb198257":"**Observations :-**\n* Pclass = 3 had most passengers, however most did not survive.\n* Most passengers in Pclass = 1 survived.\n* Infant passengers in Pclass = 2 and 3 mostly survived.\n* Pclass varies in terms of Age distribution of passengers.\n\n**Decisions :-**\n* Consider Pclass for model training.","ecdec605":"Convert the Fare feature to ordinal values based on the FareBand.","29d78401":"Our test dataset contains one missing value for Fare feature, so we need to replace it with most frequently occuring value.","e9b8dfa7":"### 1. Extract Title feature using regular expressions.","57f83426":"## Analyse Data","64bc51bc":"**Import libraries**","41c890ad":"**Decisions**\n* Consider Age and Sex in our model training\n* Remove null values from Age features\n* Band age groups"}}