{"cell_type":{"579159d1":"code","1f36e240":"code","82383476":"code","8d15facc":"code","41eb9cf1":"code","d4355d4f":"code","9c1195e8":"markdown","cbd7297c":"markdown","6f129402":"markdown"},"source":{"579159d1":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing import sequence\nimport numpy as np","1f36e240":"sentences = [\n    'I love my cat',\n    'You love my dog',\n    'i dont like coffee',\n    'my mother is very beautiful',\n    'i hate it when people touch my shoulder',\n    'i adore my brother and sister',\n    'I love my dog',\n    'i dont like milk cream',\n    'folwers are very nice',\n    'i was hurt when i did not qualify for the exams',\n    'i dont like exams',\n    'i adore small puppies',\n    'i dont like mosquitoes',\n    'i love small talks',\n    'i get offended cery easily',\n    'i like readiing books',\n    'i get angry when someone disrespects me',\n    'i appreciate your tough muscles',\n    'i hate that you love her',\n    'you cant see me because i am bad',\n    'i have a crush on you'\n    \n]\n\n\noutput = [1,1,0,1,0,1,1,0,1,0,0,1,0,1,0,1,0,1,0,0,1]\noutput = np.array(output).reshape(-1,1)\ntokenizer = Tokenizer(num_words =100,oov_token='<OOV>')\ntokenizer.fit_on_texts(sentences)\nword_index = tokenizer.word_index\nsequences = tokenizer.texts_to_sequences(sentences)\npadded = pad_sequences(sequences,15)\nprint(output.shape,padded.shape)\nprint(output[:4])\nprint(padded[:4])","82383476":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(100,5, input_length = 15),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(9)),\n    tf.keras.layers.Dense(8, activation = 'relu'),\n    tf.keras.layers.Dense(1, activation = 'sigmoid')\n])\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","8d15facc":"model.fit(padded, output,epochs=55, batch_size=8, verbose=2)\n# Final evaluation of the model\n#scores = model.evaluate(X_test, y_test, verbose=0)\n#print(\"Accuracy: %.2f%%\" % (scores[1]*100))","41eb9cf1":"sentences1 = [\n    'fast and furious is a very good movies',\n    'I love my cat movie',\n    'i dont like bhoot',\n    'i appreciate your hard work',\n    'you dont like me'\n]\noutput1 = [1,1,0,1,0]\noutput1 = np.array(output).reshape(-1,1)\nsequences1 = tokenizer.texts_to_sequences(sentences1)\npadded1 = pad_sequences(sequences1,15)\nmodel.predict(padded1)\nmodel.evaluate(padded1,output1)","d4355d4f":"sentences1 = [\n    'fast and furious is a very good movies',\n    'I love my cat movie',\n    'i dont like bhoot',\n    'i appreciate your hard work',\n    'you dont like me',\n    'i hate that you love me',\n    'you see me because i am good '\n]\nsequences1 = tokenizer.texts_to_sequences(sentences1)\npadded1 = pad_sequences(sequences1,15)\nmodel.predict(padded1)\n","9c1195e8":"Let's now create a test toy dataset of our own and perform preprocessing on it. In order to feed our training data in our Neural Network we need to convert our data to word vectors of finite length so that our network can learn from the data from here onwards.\nIn order to create the word vectors from sentences we need to tokenize the words so that we can treat each word as a seperate entity.\nAlso, for giving the input of finite length we need to use a vector of fixed size, but we also know that each sentence is not going to be of same size that's why we need to pad the sequence to make it of a finite length.","cbd7297c":"The network that we are going to use is a Sequential model, with Bidirectional LSTM layer as one of the layer. The first layer is going to be the word embedding layer where words are stored in a multidimensional space in such a way that the words which are related appear close to each other after training.\nThe output is going to be binary that's we are going to use binary crossentropy loss, with adam optimizer.","6f129402":"**** It is to be told in advance that this program is not meant to be a working model, it is just an attempt to explain how can we use the similar process without the involvment of much complications to achieve the desired results****\nIn order to perform text processing we need various tools to create word embeddings, for that we are going to use Tensorflow and Keras.\nAlso we will be using keras preprocessing tools for operations like padding and sequence generation as well."}}