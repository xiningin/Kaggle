{"cell_type":{"fa929fd9":"code","fbcd3b93":"code","f2f59176":"code","6a0ea33e":"code","10d822ad":"code","cf1857f1":"code","47a49580":"code","a3476b84":"code","b17fb6f8":"code","25b16332":"code","e7b92555":"code","3f528489":"code","29785216":"code","71d33cdc":"code","f7512e48":"code","b42dad8f":"code","1bf38f75":"code","cc8a0519":"code","4f646320":"code","85d94e4e":"code","97bd6179":"markdown","6e8c77bc":"markdown","abb1435b":"markdown","f31d66a9":"markdown","cfb83b02":"markdown","06fa80e2":"markdown","d1d8e6fe":"markdown","954241ff":"markdown","683fd78e":"markdown","7c1e9344":"markdown","01bbd4a2":"markdown","4d0220ef":"markdown","abb9057d":"markdown","a2eddccc":"markdown","7188a90a":"markdown","2dcd12d5":"markdown","2be9120c":"markdown","2151423f":"markdown","addeb5b5":"markdown","c097126f":"markdown","72fea013":"markdown"},"source":{"fa929fd9":"import os\nimport itertools\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import log_loss, confusion_matrix\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n\n# ignore deprecation warnings in sklearn\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Default configurations\n\nplt.style.use('seaborn-pastel')\npd.set_option('display.max_columns', 500)\ncolPlots = ['skyblue','lightgreen']\n\nprint('Libraries imported and default configuration set!')","fbcd3b93":"def plotBar(df, fig, col, fontS = 9):\n\n    \"\"\" Function that draw a tuned bar plot \n    \n    Arguments\n    ---------\n    df:    Pandas DataFrame\n    fig:   Figure to tune\n    col:   Column of the dataframe to plot\n    fontS: Font size of the lables that will be printed above the bars\n    \n    \"\"\"\n    \n    nGroups = tmpDF.shape[0]\n    eArr = np.array(tmpDF['e'])\n    pArr = np.array(tmpDF['p'])\n    index = np.arange(nGroups)\n    bar_width = 0.425\n    opacity = 0.65\n    \n    plt.tick_params(top='off', bottom='on', left='off', right='off', labelleft='off', labelbottom='on')\n\n    plt.gca().spines['top'].set_visible(False)\n    plt.gca().spines['left'].set_visible(False)\n    plt.gca().spines['right'].set_visible(False)\n\n    bar1 = plt.bar(index, eArr, bar_width, alpha=opacity, color = colPlots[0], label='e')\n\n    for bar in bar1:\n        height = bar.get_height()\n        if height > 0 :\n            plt.gca().text(bar.get_x() + bar.get_width()\/2, (bar.get_height()+42), str(int(height)),\n                           ha='center', color='black', fontsize=fontS)\n\n    bar2 = plt.bar(index + bar_width, pArr, bar_width, alpha = opacity,color = colPlots[1], label = 'p')\n\n    for bar in bar2:\n        height = bar.get_height()\n        if height > 0 :\n            plt.gca().text(bar.get_x() + bar.get_width()\/2, (bar.get_height()+40), str(int(height)),\n                           ha='center', color='black', fontsize=fontS)\n    b,t = plt.ylim()\n    plt.ylim(top=(t*1.10))\n\n    plt.title('Class count per ' + col, fontstretch = 'semi-condensed', fontsize = 16)\n    plt.xticks(index + bar_width\/2, list(tmpDF.index))\n    plt.tight_layout()\n   \n    return fig\n","f2f59176":"def plot_confusion_matrix(cm, target_names, title='Confusion matrix', cmap=None, figSize = [8,6], x_rot = 45, normalize=True):\n    \n    \"\"\" given a sklearn confusion matrix (cm), make a nice plot\n\n    Arguments\n    ---------\n    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n    target_names: given classification classes such as [0, 1, 2], the class names, for example: ['high', 'medium', 'low']\n    title:        the text to display at the top of the matrix\n    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n    figSize:     Size of the confusion matrix plot (*)\n    x_rot:        Degree of rotation of the X-axis labels (*)\n    normalize:    If False, plot the raw numbers, If True, plot the proportions\n    \n    \n    Credit --> https:\/\/www.kaggle.com\/grfiv4\/plot-a-confusion-matrix\n    (*)    --> Changes I've included in the function.\n    \"\"\"\n    \n    accuracy = np.trace(cm) \/ float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n    \n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    plt.figure(figsize=(figSize))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=x_rot)\n        plt.yticks(tick_marks, target_names)\n\n    thresh = cm.max() \/ 1.5 if normalize else cm.max() \/ 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\",\n                     fontweight = 'demibold')\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\",\n                     fontweight = 'demibold')\n\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()","6a0ea33e":"# Search files in the folder\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","10d822ad":"# Import the dataset into a pandas dataframe\n\nfullDF = pd.read_csv('\/kaggle\/input\/mushroom-classification\/mushrooms.csv')\nprint('Data imported!')","cf1857f1":"# Let's see the head of the dataset\nfullDF.head()","47a49580":"# Check shape and statistics of the dataset\nprint(fullDF.shape)\nfullDF.describe()","a3476b84":"# Check if there're NaN values\n\nfullDF.info()","b17fb6f8":"# Let's see the different unique values per column\n\nfor col in fullDF.columns:\n    print(col, '(',len(fullDF[col].unique()),' values ) -->', fullDF[col].unique())\n    if col == 'class':\n        intTotal = 1\n    elif len(fullDF[col].unique()) > 1:\n        intTotal = intTotal*(len(fullDF[col].unique())-1)   ","25b16332":"fullDF.groupby('class').count().iloc[:,0].plot(kind='pie', startangle = 90, textprops={'size': 12},\n                                               autopct='%1.1f%%', figsize=(7,7), colors = colPlots,\n                                               explode = (0, 0.025), labels = ['',''], wedgeprops={'alpha' : 0.65})\nplt.axis('off')\nplt.legend(['e','p'],loc=1, frameon=False, fontsize = 12)\nplt.title('Target distribution', fontsize = 15)\nplt.show()","e7b92555":"# Create bar plots\n\ni = 1\nfig1 = plt.figure(figsize=(20, 12))\n\nfor col in fullDF.columns:\n    \n    if len(fullDF[col].unique()) <= 3 and col != 'class' and col != 'veil-type' :\n        tmpDF = fullDF.groupby([col, 'class']).count()['veil-type'].reset_index()\n        tmpDF = tmpDF.pivot(index=col, columns='class', values='veil-type').fillna(0)\n        plotBar(tmpDF,fig1.add_subplot(3, 2, i), col, 15)\n        if i == 1 or i == 3 :\n            plt.legend(frameon=False, bbox_to_anchor=(-0.1,1), loc=\"upper left\", fontsize = 14)\n        elif i == 2 or i == 4 : \n            plt.legend(frameon=False, bbox_to_anchor=(1.04,1), loc=\"upper left\", fontsize = 14)\n        i += 1\n            \ni=1\nfig2 = plt.figure(figsize=(20,68))\n\nfor col in fullDF.columns:\n    \n    if len(fullDF[col].unique()) > 3 :\n        \n        tmpDF = fullDF.groupby([col, 'class']).count()['veil-type'].reset_index()\n        tmpDF = tmpDF.pivot(index=col, columns='class', values='veil-type').fillna(0)\n        plotBar(tmpDF,fig2.add_subplot(17, 1, i), col, 15)\n        plt.legend(frameon=False, bbox_to_anchor=(-0.1,1), loc=\"upper left\", fontsize = 14)\n        i = i + 1\n\nprint('Plot process completed.')","3f528489":"# Drop the column veil-type and encode the target value into a binary variable (0\/1)\n\nfullDF['class'] = fullDF['class'].map({'e' : 0, 'p' : 1})\nfullDF.drop('veil-type', axis = 1, inplace=True)\nfullDF.info()","29785216":"# First, we create the new features with pandas dummies\n# Then, split into train and test the dataset.\n# Finally we check both the train and test datasets.\n\nfullDF = pd.get_dummies(data=fullDF, drop_first=True)\ncolFeat = list(fullDF.select_dtypes(include=['int64', 'uint8']).columns)\ntestDF = fullDF[colFeat]\ntestDF.info()","71d33cdc":"train, test = train_test_split(testDF, test_size=0.25, random_state=16, stratify=testDF['class'])\ntrain.head()","f7512e48":"test.head()","b42dad8f":"# Sanity check\n\nprint('Full dataset shape              -->', fullDF.shape)\nprint('Train dataset shape             -->', train.shape)\nprint('Test dataset shape              -->', test.shape)\nprint('Sum of train and test datasets  --> (',(train.shape[0] + test.shape[0]), ')')","1bf38f75":"# Hyperparameters lists\n\ncriterion_list = ['gini', 'entropy']\nmax_feature_list = ['auto', 'log2', None]\nmax_depth_list = np.arange(3,56)\n\n# Create the parameter grid\n\nparam_grid = {'max_depth' : max_depth_list, \n              'max_features' : max_feature_list, \n              'criterion' : criterion_list} \n\n# Create a random search object\n\nrandomRF_class = RandomizedSearchCV(estimator = RandomForestClassifier(n_estimators=100, \n                                                                       random_state = 16),\n                                    param_distributions = param_grid, n_iter = 100,\n                                    scoring='accuracy', cv = 4, return_train_score= True)\n\n# Fit to the training data\n\nrandomRF_class.fit(train.drop('class', axis= 1), train['class'])\n\n# Get top-10 results and print them\n\nclf_DF = pd.DataFrame(randomRF_class.cv_results_)\nclf_DF = clf_DF.sort_values(by=['rank_test_score']).head(10)\n\nprint('Top 10 results (Random Forest - All features)\\n')\nfor index, row in clf_DF.iterrows():\n        print('Accuracy score (test): {:.8f} - Parameters: criterion    --> {}'.format(row['mean_test_score'], \n                                                                                       row['param_criterion'])) \n        print('                                                max_features --> {}'.format(row['param_max_features']))\n        print('                                                max_depth    --> {}'.format(row['param_max_depth']))","cc8a0519":"# Create the classifier\n\nclassRF = RandomForestClassifier(n_estimators=100, random_state = 16, \n                                 max_features = randomRF_class.best_params_['max_features'], \n                                 max_depth = randomRF_class.best_params_['max_depth'], \n                                 criterion = randomRF_class.best_params_['criterion'])\n\n# Fit and predict\n\nclassRF.fit(train.drop('class', axis= 1), train['class'])\nclassRF_Pred = classRF.predict(test.drop('class', axis= 1))\nclassRF_PP = classRF.predict_proba(test.drop('class', axis= 1))[:,1]\n\n# Create and plot the confusion matrix\n\nconfMatrix = confusion_matrix(classRF_Pred, test['class'])\nplot_confusion_matrix(confMatrix, ['e','p'], cmap='GnBu', figSize = [6,4], x_rot = 0)\n\n# Compute and print the scores and metrics\n\nprint('Accuracy score  --> {:.8f}'.format(accuracy_score(test['class'], classRF_Pred)))\nprint('Precision score --> {:.8f}'.format(precision_score(test['class'], classRF_Pred)))\nprint('Recall score    --> {:.8f}'.format(recall_score(test['class'], classRF_Pred)))\nprint('F1 score        --> {:.8f}'.format(f1_score(test['class'], classRF_Pred)))\nprint('Log Loss error  --> {:.8f}'.format(log_loss(test['class'], classRF_PP)))","4f646320":"# Hyperparameters lists\n\nkernel_list = ['linear', 'rbf']\nC_list = [0.001, 0.01, 0.1, 1, 5, 10, 20, 50, 100]\ngamma_list = [0.0001, 0.001, 0.01, 0.1, 1, 5, 10]\n\n# Create the parameter grid\n\nparam_grid = {'kernel' : kernel_list, 'C' : C_list, 'gamma' : gamma_list} \n\n# Create a random search object\n\nrandomSVM_class = RandomizedSearchCV(estimator = SVC(random_state = 16),\n                                     param_distributions = param_grid, n_iter = 25,\n                                     scoring='accuracy', cv = 4, \n                                     return_train_score= True)\n\n# Fit to the training data\n\nrandomSVM_class.fit(train.drop('class', axis= 1), train['class'])\n\n# Get top-10 results and print them\n\nclf_DF = pd.DataFrame(randomSVM_class.cv_results_)\nclf_DF = clf_DF.sort_values(by=['rank_test_score']).head(10)\n\nprint('Top 10 results (Random Forest - All features)\\n')\nfor index, row in clf_DF.iterrows():\n        print('Accuracy score (test): {:.8f} - Parameters: C        --> {}'.format(row['mean_test_score'], \n                                                                                  row['param_C'])) \n        print('                                                gamma    --> {}'.format(row['param_gamma']))\n        print('                                                kernel   --> {}'.format(row['param_kernel']))\n","85d94e4e":"# Create the classifier\n\nclassSVC = SVC(kernel = randomSVM_class.best_params_['kernel'], \n               C = randomSVM_class.best_params_['C'], \n               gamma = randomSVM_class.best_params_['gamma'], \n               random_state = 16, \n               probability = True) # to be able to create the predict_proba\n\n# Fit and predict\n\nclassSVC.fit(train.drop('class', axis= 1), train['class'])\nclassSVC_Pred = classSVC.predict(test.drop('class', axis= 1))\nclassSVC_PP = classSVC.predict_proba(test.drop('class', axis= 1))[:,1]\n\n# Create and plot the confusion matrix\n\nconfMatrix = confusion_matrix(classSVC_Pred, test['class'])\nplot_confusion_matrix(confMatrix, ['e','p'], cmap='GnBu', figSize = [6,4], x_rot = 0)\n\n# Compute and print the scores and metrics\n\nprint('Accuracy score  --> {:.8f}'.format(accuracy_score(test['class'], classSVC_Pred)))\nprint('Precision score --> {:.8f}'.format(precision_score(test['class'], classSVC_Pred)))\nprint('Recall score    --> {:.8f}'.format(recall_score(test['class'], classSVC_Pred)))\nprint('F1 score        --> {:.8f}'.format(f1_score(test['class'], classSVC_Pred)))\nprint('Log Loss error  --> {:.8f}'.format(log_loss(test['class'], classSVC_PP)))","97bd6179":"First, we'll develop a ***Random Forest Classifier***, and then a ***Kernalized SVM*** model.\n\nWe'll use a *RandomizedSearchCV* to evaluate the model with several combination of parameters and, depending on the results, we'll decide if we go further tunning the model, or just keep the best finding. Let's begin.","6e8c77bc":"As we see, the target have a balanced distribution. Now let's plot the features distribution per class (target)","abb1435b":"### 2) Import data","f31d66a9":"### 7) Conclusion","cfb83b02":"It looks like we don't need to search any further, we have our *perfect classifier*! Now, let's create and fit the classifier with the parameters we got from the *RandomizerSearchCV*, and do some extra checks, scores and compute the log loss metric, to measure our classifier with unseen data.","06fa80e2":"We can draw some conclusions about the features:\n\n   * There're non-informative features like *veil-color*, *gill-attachment* and *ring-number* as the distribution between **e** and **p** is around 50% in the most representative value  \n   * There're high-informative features like *odor*, *spore-print-color* and *gill-color* as the distribution of some of their values are above 75-80% for one of the target values.\n   \nNow let's do some *Feature Enginering* to encode the categorical features to be able to run a model on it.","d1d8e6fe":"First conclusions:\n* All the features are categorical.\n* No NaN values.\n* One feature (*veil-type*) with only one category, we'll drop it, but before we'll use it.\n\nFor this classification problem, we'll develop a Random Forest and Kernalized SVM classifier models. Now, let's do a deep analysis on the data with some plots.","954241ff":"Being a full categorical dataset, with the correct approach on the encoding task, we see that we can create a model that achieve a perfect score on the classification task. Now we can rely our trust that we will not die if we classify a mushroom with these 2 models :)","683fd78e":"### 1) Importing libraries, default configurations and function definition","7c1e9344":"### 4) Exploratory data analysis & visualization","01bbd4a2":"To begin, let's see the first rows of the dataset, it shape, its statistics, detailed information of the features and finally, check if there're NaN values.","4d0220ef":"### 5) Feature engineering","abb9057d":"I can confirm that we have the *perfect classifier*. For sanity check, I'll still do the ***Kernalized SVM classifier*** following the same procedure, just to see if we can create another model that perfectly classify the test data.","a2eddccc":"First we'll drop the column *veil-type* as it has only one unique value, meaning it doesn't help to the prediction tasks. Then, encode the features with pandas *get_dummies* process, split the dataset in *train* and *test* to be able to train and predict with the classifiers.","7188a90a":"We have another *perfect classifier*, meaning that our first model is good, and this is too! Let's do the same check steps we did to measure the model with unseen data.","2dcd12d5":"Yes! Another perfect classifier.","2be9120c":"### 3) Preliminary data visualization and analysis","2151423f":"### Context\n\nAlthough this dataset was originally contributed to the UCI Machine Learning repository nearly 30 years ago, mushroom hunting (otherwise known as \"shrooming\") is enjoying new peaks in popularity. Learn which features spell certain death and which are most palatable in this dataset of mushroom characteristics. And how certain can your model be?\n\n### Content\n\nThis dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family Mushroom drawn from The Audubon Society Field Guide to North American Mushrooms (1981). Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like \"leaflets three, let it be\" for Poisonous Oak and Ivy. Let's see all the features present in the dataset:\n\n* **class**: edible(e) or poisonous(p)\n* **cap-shape**: bell(b), conical(c), convex(x), flat(f), knobbed(k), sunken(s)\n* **cap-surface**: fibrous(f), grooves(g), scaly(y), smooth(s)\n* **cap-color**: brown(n), buff(b), cinnamon(c), gray(g), green(r), pink(p), purple(u), red(e), white(w), yellow(y)\n* **bruises**: bruises(t), no bruises(f)\n* **odor**: almond(a), anise(l), creosote(c), fishy(y), foul(f), musty(m), none(n), pungent(p), spicy(s)\n* **gill-attachment**: attached(a), descending(d), free(f), notched(n)\n* **gill-spacing**: close(c), crowded(w), distant(d)\n* **gill-size**: broad(b), narrow(n)\n* **gill-color**: black(k), brown(n), buff(b), chocolate(h), gray(g), green(r), orange(o), pink(p), purple(u), red(e), white(w), yellow(y)\n* **stalk-shape**: enlarging(e), tapering(t)\n* **stalk-root**: bulbous(b), club(c), cup(u), equal(e), rhizomorphs(z), rooted(r), missing(?)\n* **stalk-surface-above-ring**: fibrous(f), scaly(y), silky(k), smooth(s)\n* **stalk-surface-below-ring**: fibrous(f), scaly(y), silky(k), smooth(s)\n* **stalk-color-above-ring**: brown(n), buff(b), cinnamon(c), gray(g), orange(o), pink(p), red(e), white(w), yellow(y)\n* **stalk-color-below-ring**: brown(n), buff(b), cinnamon(c), gray(g), orange(o), pink(p), red(e), white(w), yellow(y)\n* **veil-type**: partial(p), universal(u)\n* **veil-color**: brown(n), orange(o), white(w), yellow(y)\n* **ring-number**: none(n), one(o), two(t)\n* **ring-type**: cobwebby(c), evanescent(e), flaring(f), large(l), none(n), pendant(p), sheathing(s), zone(z)\n* **spore-print-color**: black(k), brown(n), buff(b), chocolate(h), green(r), orange(o), purple(u), white(w), yellow(y)\n* **population**: abundant(a), clustered(c), numerous(n), scattered(s), several(v), solitary(y)\n* **habitat**: grasses(g), leaves(l), meadows(m), paths(p), urban(u), waste(w), woods(d)\n\nNow, let's start our fungus journey!","addeb5b5":"<img src=\"https:\/\/i.imgur.com\/PcOG8ni.png\">","c097126f":"### 6) Model development","72fea013":"First, let's plot the distribution of the target class, to check if it's balanced\/unbalanced."}}