{"cell_type":{"fc57dee5":"code","08a9554c":"code","fca5911a":"code","257c1aa5":"code","f0d80f15":"code","89293fac":"code","27b103c3":"code","badf1eeb":"code","888a2c8c":"code","70905f18":"markdown","8ec456e3":"markdown","7cfd3678":"markdown","151420c8":"markdown","8264efff":"markdown","dcdabde3":"markdown","c1577d95":"markdown"},"source":{"fc57dee5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.feature_selection import chi2, SelectKBest\nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt","08a9554c":"# Load the data\n\ndata = pd.read_csv('..\/input\/mobile-price-classification\/train.csv')\ndata.head()","fca5911a":"# Split into features and target variables\n\nX = data.iloc[:, :-1]\ny = data.iloc[:, -1]","257c1aa5":"# Call the constructor to select 10 best features according to their Chi-square tests against the target variable\nbest_features = SelectKBest(score_func=chi2, k = 10)\n\nfit = best_features.fit(X, y)","f0d80f15":"chi_scores = pd.DataFrame(fit.scores_)\ncolumns = pd.DataFrame(X.columns)\n\nfeatureScores = pd.concat([columns, chi_scores], axis = 1)\nfeatureScores.columns = ['Feature', 'Score']\n\nfeatureScores.sort_values(by = 'Score', ascending = False)","89293fac":"# Initialize a RandomForestClassifier model and fit our dataset\nmodel = RandomForestClassifier()\nmodel.fit(X, y)","27b103c3":"# From this model, we get importance scores for each feature\nmodel.feature_importances_","badf1eeb":"featureImp = pd.DataFrame(model.feature_importances_, index = X.columns, columns = ['Importance'])\nfeatureImp = featureImp.sort_values(by = 'Importance', ascending = False)\nfeatureImp","888a2c8c":"# Let's plot the importance score for the features\nplt.figure(figsize = (10, 5))\nplt.xticks(rotation = 90)\nplt.bar(featureImp.index, featureImp['Importance'])","70905f18":"Again, we get similar result from the model also. RAM is the most important feature for determining the price range of any mobile.","8ec456e3":"## Feature Importance\n\nAnother techinque which we will be using is the feature importance. Here, we will be using a tree-based model to assign an importance to each of the feature in the dataset.","7cfd3678":"As we can see from the above DataFrame and from our own experience, the RAM for a smartphone is the most significant feature to determine its price range. Also, some really good indicators are: dimensions of the phone, battery power, and its internal memory.","151420c8":"## Load and Prepare the Dataset","8264efff":"## Introduction\n\nIn this notebook, we will be looking at one of the most important steps in building a machine learning model - **Feature Selection**. How we select our features from the dataset will determine how accurate the model will be when trained and being tested.\n\nWe will use some statistical tests like Chi-square to select features which will only improve our model.","dcdabde3":"## Compare and Select N-best features using SelectKBest","c1577d95":"From the above plot, we can see that after a fwe features, there isn't much importance left and we can select the first 4-5 features and our model will be sufficiently accurate."}}