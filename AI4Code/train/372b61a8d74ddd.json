{"cell_type":{"a6a579bf":"code","b23aeb67":"code","3ca79888":"code","48ea59d7":"code","102c6519":"code","bf57beb0":"code","5f4fef9b":"code","7d398f42":"code","2c2008f3":"code","de763a87":"code","25823279":"code","3829ec66":"code","33dff820":"code","c28851f6":"code","bb2405c9":"code","d1b4d330":"code","5d514f8a":"code","86416721":"code","2e5cca20":"code","4276587c":"code","eec0bdf5":"markdown"},"source":{"a6a579bf":"#Feature Importance .\n\"\"\"\nExploratory data analysis is an approach to better understand data to find any patterns and this data is related to a certain context and without context data has no importance\/significance. Next step is to understand the dataset variables weight (importance). We can do that by calculating the feature Importance or significance. This process of finding the significant or important variables in datasets is a pre-processing step, before fitting our data into any model that we think is appropriate to solve the problem at hand.\nBecause, if we use the important features of the dataset, we can reduce the\n\u2022 Reducing the training time\n\u2022 cost of computation\n\u2022 It can help us in not overfitting the model\nMy experiment with the Abalone data to classify their ages as >11 years or <= 11, has shown me that Length and Diameter have no significance in classifying them into different age groups. The interesting part is there is no loss of accuracy in this case. Sometimes there might be a loss accuracy but we have to always consider the tradeoff, whether to include or exclude\n\"\"\"\n\n","b23aeb67":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n\n","3ca79888":"abalone = pd.read_csv('..\/input\/abalone.data.csv',header=None)\nabalone.columns = ['Sex','Length','Diameter','Height','Whole_weight','Shucked_weight','Viscera_weight','Shell_weight','Rings']\nprint(abalone.head())","48ea59d7":"#EDA\nabalone.columns","102c6519":"abalone.info()","bf57beb0":"#abalone = [abalone['Rings'].apply(lambda x:if(abalone['Rings'] >=11))]\n\ndef class_group(x):\n    if ((x < 11) | (x == 11)):\n        return 0\n    else:\n        return 1\n\n    ","5f4fef9b":"abalone['class'] = abalone['Rings'].apply(lambda x:class_group(x))","7d398f42":"abalone.head(2)","2c2008f3":"abalone.describe()","de763a87":"abalone['Rings'].value_counts()\n","25823279":"plt.figure(figsize=(12,10))\nabalone[abalone['Sex']=='M']['Rings'].hist(alpha=0.5,color='blue',\n                                              bins=30,label='Sex=Male')\nabalone[abalone['Sex']=='F']['Rings'].hist(alpha=0.5,color='red',\n                                             bins=30,label='Sex=Female')\nabalone[abalone['Sex']=='I']['Rings'].hist(alpha=0.5,color='green',\n                                             bins=30,label='Sex=Infant')\nplt.legend()\nplt.xlabel('Rings')","3829ec66":"#sns.pairplot(abalone.drop(['Sex'],axis=1))\ndf_corr = abalone.corr() # Calculation of the correlation coefficients in pairs, with the default method:\n                    # Pearson, Standard Correlation Coefficient\n    \n","33dff820":"plt.figure(figsize=(15,10))\nsns.heatmap(df_corr, cmap=\"YlGnBu\") # Displaying the Heatmap\nsns.set(font_scale=2,style='white')\n\nplt.title('Heatmap correlation')\nplt.show()","c28851f6":"X=abalone[['Length','Diameter','Height','Whole_weight','Shucked_weight','Viscera_weight','Shell_weight']]\ny=abalone['class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)\nrfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(X_train,y_train)\ny_test_pred_rfc = rfc.predict(X_test)\naccuracy_score(y_test, y_test_pred_rfc)","bb2405c9":"print('Classification report:')\nprint(classification_report(y_test,y_test_pred_rfc))\nprint('Confusion matrix:')\nprint(confusion_matrix(y_test,y_test_pred_rfc))","d1b4d330":"plt.figure(figsize=(10,5))\ncm = np.array(confusion_matrix(y_test,y_test_pred_rfc))\nsns.heatmap(cm, cmap=\"YlGnBu\") # Displaying the Heatmap\nsns.set(font_scale=2,style='white')\nplt.title('Confusion Matrix')\nplt.show()","5d514f8a":"#https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_forest_importances.html#sphx-glr-auto-examples-ensemble-plot-forest-importances-py\n\nimportances = rfc.feature_importances_\nprint(importances)\nprint(type(importances))\nprint(type(X.shape))\nprint(X.shape[1])\nstd = np.std([tree.feature_importances_ for tree in rfc.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X.shape[1]):\n    print(\"{}.feature {} ({})\".format(f + 1, indices[f], importances[indices[f]]))\n    #print(\"%d. feature %d (%f)\"% (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\nplt.figure()\nplt.title(\"Feature\/Variable Selection\")\nplt.bar(range(X.shape[1]), importances[indices],\n       color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X.shape[1]), indices)\nplt.xlim([-1, X.shape[1]])\nplt.show()\n\n","86416721":"Xfs=abalone[['Height','Whole_weight','Shucked_weight','Viscera_weight','Shell_weight']]\nyfs=abalone['class']\nrfc = RandomForestClassifier(n_estimators=100)\nXfs_train, Xfs_test, yfs_train, yfs_test = train_test_split(Xfs, yfs, test_size=0.30, random_state=101)\nrfc.fit(Xfs_train,yfs_train)\nyfs_test_pred_rfc = rfc.predict(Xfs_test)\naccuracy_score(yfs_test, yfs_test_pred_rfc)","2e5cca20":"print('FeatureImporatance Random Forest Classification report:')\nprint(classification_report(yfs_test,yfs_test_pred_rfc))\nprint('FeatureImporatance Random Forest Confusion matrix:')\nprint(confusion_matrix(yfs_test,yfs_test_pred_rfc))","4276587c":"plt.figure(figsize=(10,5))\ncm_fs = np.array(confusion_matrix(yfs_test,yfs_test_pred_rfc))\nsns.heatmap(cm, cmap=\"YlGnBu\") # Displaying the Heatmap\nsns.set(font_scale=2,style='white')\nplt.title('Confusion Matrix')\nplt.show()","eec0bdf5":"**Lets build the model with new Important feature\/variables.\nBased on the feature importance calcuation below,:\nLength has 0 weightage  and Diameter is too is similar to Length.So lets build a new model considering feature importance and see if the Model performance chages.**"}}