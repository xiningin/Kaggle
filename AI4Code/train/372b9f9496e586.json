{"cell_type":{"783ffccb":"code","36e4cf1e":"code","11522347":"code","a79f9a0f":"code","b2c87273":"code","9651f79e":"code","77b2de2d":"code","1071a26e":"code","6ecfe828":"code","4ace3dd4":"code","8d0abaa0":"code","f7e5e913":"code","cec6404c":"code","7d6ec4dc":"code","acf3ad10":"code","acaf4b8d":"code","a16f886e":"code","eba97e78":"code","38be2d15":"code","8d2ebead":"code","6ae3bed7":"code","5ce10c85":"code","25f0ce52":"code","6305f209":"code","da0b261e":"code","2574954d":"code","435bf4bf":"code","a2236de5":"code","de318cf6":"code","51d36eec":"code","44def679":"code","d62c3f61":"code","a64fef2e":"code","1bdc81e1":"code","4ed0d327":"code","0f90d259":"code","1e28ffe5":"code","c74f5859":"code","c29871a7":"code","32c46c24":"code","2d0dea34":"code","c4b720f7":"code","27d51361":"code","1caa72a5":"code","f3504a3f":"code","656cb6e7":"code","5fcaa733":"code","082a0898":"code","4d94bb98":"code","f2fe1c5b":"code","4c4ee10b":"code","38bf96cb":"code","61520419":"code","e87bf520":"markdown","128cf4c1":"markdown","b8507d18":"markdown","27c0b76c":"markdown","815b3ef6":"markdown","4a6c65ec":"markdown","2d397ea0":"markdown","2a822b14":"markdown","701fedf2":"markdown","c4eea30c":"markdown","1d8ec65e":"markdown","7c666b8e":"markdown","2cb3f16e":"markdown","904d905f":"markdown","5dd70124":"markdown","533a2bd4":"markdown","515088bf":"markdown","34b66cc3":"markdown","a4421fed":"markdown","eb900557":"markdown","1a21197d":"markdown","2b570810":"markdown","f573ee04":"markdown","77983125":"markdown","3a3adb7d":"markdown","b0518f10":"markdown","4aa704a4":"markdown","c1026a9b":"markdown","3cf5e8eb":"markdown","264ac955":"markdown","97ed4b2b":"markdown","3ced7a53":"markdown"},"source":{"783ffccb":"import os\nprint(os.listdir(\"..\/input\/countries-of-the-world\"))\nprint(os.listdir(\"..\/input\/additional-data\"))","36e4cf1e":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom scipy.stats import norm\nfrom scipy import stats\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler","11522347":"dfc = pd.read_csv(\"..\/input\/countries-of-the-world\/countries_of_the_world.csv\", decimal=',')\ndf_WB = pd.read_csv(\"..\/input\/additional-data\/additional_data_WB.csv\")\ndf_rel = pd.read_csv(\"..\/input\/additional-data\/additional_data_religion.csv\")\ndf_freedom = pd.read_csv(\"..\/input\/additional-data\/additional_data_freedom.csv\")\n","a79f9a0f":"### changing the column name to fit the other data\ndfc.rename(columns = {'Country':'Country Name'}, inplace=True)\n\n### removing the space after each country name\ndfc['Country Name'].values\ndfc['Country Name'] = dfc['Country Name'].apply(lambda x: x.split(\" \")[0] if len(x.split())==1 else x)\n### merging the data with the WorldBank data\ndf_n_wb = dfc.merge(df_WB, on = 'Country Name', how='left')\ndf_n_wb.head()","b2c87273":"### removing the space in the country name column of the religion data\ndf_rel['Country Name'] = df_rel['Country Name'].apply(lambda x: x.strip())\n\n#merging the data with the religion data\ndf_n_wb_rel = df_n_wb.merge(df_rel, on = 'Country Name', how = 'left' )\ndf_n_wb_rel.drop(['Unnamed: 1'],axis=1, inplace=True)\ndf_n_wb_rel['Country Name'].values","9651f79e":"### merging the data with the freedom index data\ndf = df_n_wb_rel.merge(df_freedom, on='Country Name', how='inner')\n\n### rename some columns to be more comfortable to work with:\n\ndf.rename(columns = {'GDP ($ per capita)':'GDP', 'Country Name':'Country','Imports of goods and services  2013':'Imports', 'Exports of goods and services  2013':'Exports'\n                    ,'Foreign direct investment, net inflows 2013': 'Net foreign invest'}, inplace=True)\ndf.Region = [x.strip() for x in df.Region]\ndf = df.rename(columns = lambda x: x.split('(')[0].strip())\ndf.shape","77b2de2d":"df.Country = df.Country.astype('category')\ndf.Region = df.Region.astype('category')\ndf['Main Religion'] = df['Main Religion'].astype('str')\ndf.info()\n","1071a26e":"df['Main Religion'].value_counts()\ndf['Main Religion'] = df['Main Religion'].replace('Christian ', 'Christian').replace('Christian (Free Wesleyan Church claims over 30','Christian')\ndf['Main Religion'] = df['Main Religion'].replace('Muslim*', 'Muslim')\ndf['Main Religion'] = df['Main Religion'].replace('Buddist', 'Buddhist').replace('Buddhist', 'Buddhist').replace('BuddhismAnd','Buddhist')\ndf['Main Religion'] = df['Main Religion'].replace('Buddhist ', 'Buddhist')\ndf['Main Religion'] = df['Main Religion'].replace('syncretic (part Christian', 'Christian')\ndf['Main Religion'] = df['Main Religion'].replace('Zionist 40% (a blend of Christianity and indigenous ancestral worship)', 'Zionist')\n","6ecfe828":"### presenting the number of null values by total and percentage:\n\ntotal = pd.DataFrame(df.isnull().sum().sort_values(ascending=False))\npercent = pd.DataFrame(df.isnull().sum().sort_values(ascending=False)\/len(df))\ndata = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\ndata","4ace3dd4":"### Let's have a look at the variable Climate for it is a categorial one.\n## let's find out what can fit the most to our null value for climate in each country\n\nprint(pd.pivot_table(df,index=['Climate'],values = 'Region', aggfunc='sum'))\nprint(df[df.Climate.isnull()]['Country'])\ndf.loc[3,['Climate']] = 3\ndf.loc[25,['Climate']] = 3\ndf.loc[31,['Climate']] =3\ndf.loc[64,['Climate']] =3\ndf.loc[74,['Climate']] =1\ndf.loc[77,['Climate']] =2.5\ndf.loc[79,['Climate']] =3\ndf.loc[80,['Climate']] =3\ndf.loc[88,['Climate']] =3\ndf.loc[92,['Climate']] =3\ndf.loc[94,['Climate']] =1\ndf.loc[97,['Climate']] =2\ndf.loc[113,['Climate']] =1\ndf.loc[117,['Climate']] =3\ndf.loc[121,['Climate']] =3\ndf.loc[132,['Climate']] =3\ndf.Climate.isnull().sum()\n","8d0abaa0":"df.drop(['Government expenditure on education % of GDP 2014'], axis=1, inplace=True)","f7e5e913":"total = pd.DataFrame(df.isnull().sum().sort_values(ascending=False))\npercent = pd.DataFrame(df.isnull().sum().sort_values(ascending=False)\/len(df))\ndata = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\ndata","cec6404c":"total = pd.DataFrame(df.isnull().sum().sort_values(ascending=False))\npercent = pd.DataFrame(df.isnull().sum().sort_values(ascending=False)\/len(df))\ndata = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\ndata","7d6ec4dc":"%matplotlib inline\nplt.figure(figsize = (10,3))\nplt.subplot(1,3,1)\nsns.distplot(df['ATMs per 100,000 adults 2013'])\nplt.subplot(1,3,2)\nsns.kdeplot(df['Exports'])\nplt.subplot(1,3,3)\nsns.kdeplot(df['Net foreign invest'])\nplt.tight_layout()","acf3ad10":"columns = data.index[:16]\nfor column in columns:\n    df[column] = df[column].fillna(df.groupby('Region')[column].transform('mean'))\n","acaf4b8d":"total = pd.DataFrame(df.isnull().sum().sort_values(ascending=False))\npercent = pd.DataFrame(df.isnull().sum().sort_values(ascending=False)\/len(df))\ndata = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\ndata","a16f886e":"corrs = df.corr()\nfig, ax = plt.subplots(figsize = (12,12))\nheatcor = sns.heatmap(corrs, cbar=True ,ax=ax).set(title = 'Correlation Map', xlabel = 'Columns', ylabel = 'Columns' )","eba97e78":"top_corrs = df.corr().nlargest(10, 'GDP').index\ncm = np.corrcoef(df[top_corrs].values.T)\nheatcor = sns.heatmap(cm, cbar = True, annot = True, cmap='BrBG', yticklabels = top_corrs.values, xticklabels=top_corrs.values)","38be2d15":"df = df.drop(['Imports'], axis=1)\n","8d2ebead":"top_corrs = df.corr().nlargest(8, 'GDP').index\nsns.pairplot(df[top_corrs])","6ae3bed7":"fig = plt.figure(figsize = (15,10))\nplt.subplot(2,3,1)\nsns.barplot(df['GDP'], df['Region'],palette='BrBG',ci = None )\nplt.subplot(2,3,2)\nsns.barplot(df['Exports'], df['Region'],palette='BrBG', ci = None)\nplt.subplot(2, 3, 3)\nsns.barplot(df['Net migration'], df['Region'], palette='BrBG', ci = None)\nplt.subplot(2, 3, 4)\nsns.barplot(df['Pop. Density'], df['Region'],palette='BrBG', ci = None)\nplt.subplot(2, 3, 5)\nsns.barplot(df['Deathrate'], df['Region'],palette='BrBG', ci = None)\nplt.subplot(2, 3, 6)\nsns.countplot(y = df['Region'],palette='BrBG')\nplt.tight_layout()","5ce10c85":"fig = plt.figure(figsize = (18,10))\nplt.subplot(2,4,1)\nsns.barplot(df['GDP'], df['Main Religion'],palette='BrBG',ci = None)\nplt.subplot(2,4,2)\nsns.barplot(df['Exports'], df['Main Religion'],palette='BrBG', ci = None)\nplt.subplot(2, 4, 3)\nsns.barplot(df['Net migration'], df['Main Religion'], palette='BrBG', ci = None)\nplt.subplot(2, 4, 5)\nsns.barplot(df['Pop. Density'], df['Main Religion'],palette='BrBG', ci = None)\nplt.subplot(2, 4, 6)\nsns.barplot(df['Deathrate'], df['Main Religion'],palette='BrBG', ci = None)\nplt.subplot(2, 4, 7)\nsns.countplot(y = df['Main Religion'],palette='BrBG')\nplt.tight_layout()\n","25f0ce52":"GDP_scaled = StandardScaler().fit_transform(df['GDP'][:,np.newaxis])\nlow_values = GDP_scaled[GDP_scaled[:,0].argsort()][:10]\nhigh_values = GDP_scaled[GDP_scaled[:,0].argsort()][-10:]\nprint('the lower values of the distribution are:')\nprint(low_values)\nprint('the higher values of the distribution are:')\nprint(high_values)","6305f209":"df[df['GDP'].values == df.GDP.max()]","da0b261e":"df_final = pd.get_dummies(df, columns=['Region', 'Main Religion', 'Climate'])\n","2574954d":"df_final.GDP.describe()","435bf4bf":"plt.figure(figsize = (10,5))\nplt.subplot(1,2,1)\nsns.distplot(df_final['GDP'], fit=norm)\nplt.subplot(1,2,2)\nres = stats.probplot(df_final['GDP'], plot=plt)\nplt.tight_layout()","a2236de5":"y = df_final.GDP.values","de318cf6":"y = np.log(y)","51d36eec":"plt.figure(figsize = (10,5))\nplt.subplot(1,2,1)\nsns.distplot(y, fit=norm)\nplt.subplot(1,2,2)\nres = stats.probplot(y, plot=plt)","44def679":"import xgboost as xgb\nfrom sklearn.metrics import mean_squared_error, accuracy_score, r2_score\nfrom sklearn.model_selection import KFold, cross_val_score, StratifiedKFold, GridSearchCV , RandomizedSearchCV, train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC, LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.svm import SVR\nimport lightgbm as lgb","d62c3f61":"df_final1 = df_final.copy()\ndf_final1.columns","a64fef2e":"x_all = df_final1.drop(['Country','GDP'], axis=1)\nx_features = df_final1[['Population', 'Pop. Density','Literacy','Infant mortality','Birthrate',\n                      'ATMs per 100,000 adults 2013', 'Financial Freedom', 'Business Freedom',\n                      'Phones','Net foreign invest', 'Service', 'Industry','Exports', 'Investment Freedom' ]]\nx_all_scaled = StandardScaler().fit_transform(x_all)\nx_features_scaled = StandardScaler().fit_transform(x_features)","1bdc81e1":"x_all_train, x_all_test, y_train, y_test = train_test_split(x_all.values, y, train_size = 0.8)\nx_features_train, x_features_test, y_train, y_test = train_test_split(x_features.values, y, train_size = 0.8)\nx_all_scaled_train, x_all_scaled_test, y_train, y_test = train_test_split(x_all_scaled, y, train_size = 0.8)\nx_features_scaled_train, x_features_scaled_test, y_train, y_test = train_test_split(x_features_scaled, y, train_size = 0.8)","4ed0d327":"models = []\nmodels.append(('Lasso', Lasso()))\nmodels.append(('RandomForest', RandomForestRegressor()))\nmodels.append(('XGB', xgb.XGBRegressor(objective = 'reg:squarederror')))\nmodels.append(('LR', LinearRegression()))\nmodels.append(('SVR', SVR()))\nmodels.append(('Enet',ElasticNet(tol=0.5)))\nmodels.append(('LightGBM',lgb.LGBMRegressor()))","0f90d259":"names = []\nresults = []\nn_fold = 6\nfor name, model in models:\n    kfold = KFold(n_fold, random_state = None)\n    cv_results = cross_val_score(model,x_all_train,y_train, scoring = 'neg_mean_squared_error' )\n    results.append(cv_results)\n    names.append(name)\n    model.fit(x_all_train, y_train)\n    predictions = model.predict(x_all_test)\n    r2_msg = 'And the R2 score is: %f' %(r2_score(predictions, y_test))\n    rmse_msg = 'For ' '%s' ' , the mean squared error is: ' '%f(%f)' % (name, cv_results.mean(), cv_results.std())\n    print(rmse_msg)\n    print(r2_msg)\nfig = plt.figure()\nfig.suptitle('Model Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","1e28ffe5":"names = []\nresults = []\nn_fold = 6\nfor name, model in models:\n    kfold = KFold(n_fold, random_state = None)\n    cv_results = cross_val_score(model,x_all_scaled_train,y_train, scoring = 'neg_mean_squared_error' )\n    results.append(cv_results)\n    names.append(name)\n    model.fit(x_all_scaled_train, y_train)\n    predictions = model.predict(x_all_scaled_test)\n    r2_msg = 'And the R2 score is: %f' %(r2_score(predictions, y_test))\n    rmse_msg = 'For ' '%s' ' , the mean squared error is: ' '%f(%f)' % (name, cv_results.mean(), cv_results.std())\n    print(rmse_msg)\n    print(r2_msg)\nfig = plt.figure()\nfig.suptitle('Model Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","c74f5859":"names = []\nresults = []\nn_fold = 6\nfor name, model in models:\n    kfold = KFold(n_fold, random_state = None)\n    cv_results = cross_val_score(model,x_features_train,y_train, scoring = 'neg_mean_squared_error' )\n    results.append(cv_results)\n    names.append(name)\n    model.fit(x_features_train, y_train)\n    predictions = model.predict(x_features_test)\n    r2_msg = 'And the R2 score is: %f' %(r2_score(predictions, y_test))\n    rmse_msg = 'For ' '%s' ' , the mean squared error is: ' '%f(%f)' % (name, cv_results.mean(), cv_results.std())\n    print(rmse_msg)\n    print(r2_msg)\nfig = plt.figure()\nfig.suptitle('Model Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()\n","c29871a7":"names = []\nresults = []\nn_fold = 6\nfor name, model in models:\n    kfold = KFold(n_fold, random_state = None)\n    cv_results = (cross_val_score(model,x_features_scaled_train,y_train, scoring = 'neg_mean_squared_error' ))\n    results.append(cv_results)\n    names.append(name)\n    model.fit(x_features_scaled_train, y_train)\n    predictions = model.predict(x_features_scaled_test)\n    r2_msg = 'And the R2 score is: %f' %(r2_score(predictions, y_test))\n    rmse_msg = 'For ' '%s' ' , the mean squared error is: ' '%f(%f)' % (name, cv_results.mean(), cv_results.std())\n    print(rmse_msg)\n    print(r2_msg)\n\nfig = plt.figure()\nfig.suptitle('Model Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","32c46c24":"model_xgb = xgb.XGBRegressor(objective = 'reg:squarederror')\nmodel_randomforest = RandomForestRegressor()\n","2d0dea34":"model_randomforest.fit(x_features_scaled_train, y_train)\nRF_predictions = model_randomforest.predict(x_features_scaled_test)\nprint(r2_score(RF_predictions, y_test))\nprint(mean_squared_error(RF_predictions, y_test))\ngraph = sns.regplot(RF_predictions, y_test).set(title='Random Forest model predictions', xlabel='Predicted GDP', ylabel='Actual GDP')","c4b720f7":"###performing Search grid search\nparameters = { \n                      'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [100, 200, 300, 1000]\n                                                 }\ngridRF = GridSearchCV(model_randomforest,param_grid= parameters, n_jobs = 4, cv=5)","27d51361":"gridRF.fit(x_features_scaled_train, y_train)","1caa72a5":"gridRF.best_estimator_","f3504a3f":"gridRF_preds = gridRF.predict(x_features_scaled_test)\nprint(mean_squared_error(gridRF_preds, y_test))\nprint(r2_score(gridRF_preds, y_test))\ngraph = sns.regplot(gridRF_preds, y_test).set(title='Random Forest model predictions', xlabel='Predicted GDP', ylabel='Actual GDP')","656cb6e7":"model_xgb.fit(x_features_scaled_train, y_train)\nxgb_predictions = model_xgb.predict(x_features_scaled_test)\nprint(mean_squared_error(xgb_predictions, y_test))\nprint(r2_score(xgb_predictions, y_test))\ngraph = sns.regplot(xgb_predictions, y_test).set(title='XGB model predictions', xlabel='Predicted GDP', ylabel='Actual GDP')","5fcaa733":"###performing Search grid search\nn_estimators = [100,300, 400, 500]\nlearning_rate = [0.03,0.09, 0.1, 0.13, 0.2]\nmax_depth = [3,4,5,10]\nmin_child_weight = [1,2,3,4]\nparameters = { \n                      'objective':['reg:linear'],\n                      'learning_rate': learning_rate, \n                      'max_depth': max_depth,\n                      'min_child_weight': min_child_weight,\n                      'silent': [1],\n                      'subsample': [0.5, 0.6, 0.8],\n                      'n_estimators': n_estimators,\n                      'booster': ['gbtree']\n                                                 }\ngridXGB = GridSearchCV(model_xgb,param_grid= parameters, n_jobs = 4, cv=5)","082a0898":"gridXGB.fit(x_features_scaled_train, y_train)\n","4d94bb98":"gridXGB_preds = gridXGB.predict(x_features_scaled_test)\nprint(mean_squared_error(gridXGB_preds, y_test))\nprint(r2_score(gridXGB_preds, y_test))\nsns.regplot(gridXGB_preds, y_test)","f2fe1c5b":"model_xgb_2 = xgb.XGBRegressor(booster = 'gbtree',\n learning_rate = 0.19,\n max_depth= 5,\n min_child_weight= 2,\n n_estimators= 90,\n objective= 'reg:linear',\n silent= 1,\n subsample= 0.6)","4c4ee10b":"model_xgb_2.fit(x_features_scaled_train, y_train)\nxgb_pred_2 = model_xgb_2.predict(x_features_scaled_test)\nprint(mean_squared_error(xgb_pred_2, y_test))\nprint(r2_score(xgb_pred_2, y_test))\ngraph = sns.regplot(xgb_pred_2, y_test).set(title='XGB model predictions', xlabel='Predicted GDP', ylabel='Actual GDP')","38bf96cb":"featuers_coefficients = model_xgb_2.feature_importances_.tolist()\nfeature_names = x_features.columns\nfor i in range(len(feature_names)):\n    coefs = 'The coefficient for the feature %s is: ' '%f' %(feature_names[i], featuers_coefficients[i])\n    print(coefs)\n\n","61520419":"feats = pd.DataFrame(pd.Series(featuers_coefficients, feature_names).sort_values(ascending=False),columns=['Coefficient'])\nfeats","e87bf520":"Alright, seems like we can't get better than this and it's pretty powerful score there to be honest! \n\nlast thing to do is checking the coefficients of our features, how they actually contribute to the GDP","128cf4c1":"### we can learn a lot from the plots above to be honest. \nfirst, the deathrate in sub-saharan african countries is substantialy higher than any other country, and this region also leading the way in teems of number of countries, but the density of the population isn't big.\nsecond, and not surprising, western europe and north america have the highest GDP, what can explain the higher net migration if you ask me.\n\nwhat about the religion aspect?","b8507d18":"## a few more feature engineering before diving deep to the models","27c0b76c":"## step 1 seems to be done, the data is clean and we're ready to move toward step 2 which is handling with null values.\n\n### the guidline for this step will be as follows:\n#### 1)for numerical data - we'll fill the null values based on the mean or median of the data, depends on what seems logically reasonable\n#### 2)for categirical data - we'll fill the null values based on the mode or a value that fit the best based on the knowledge we have","815b3ef6":"seems like the distributions are pretty much normal(except for ATM's which is a little bit skewd to the right but does'nt seem problematic), therefore i think using the mean will be just fine","4a6c65ec":"### ## looks much better now. let's start with our models:\nplan:\nsplit the data into training and validation\ntrain each set with a different method:\n1. scaled\n2. non-scaled\n\neach method will be tested with specific features and all featuers.\n\naccuracy will be evaluated by the mean square error value.","2d397ea0":"Alright we handled the Climate column, let's move on and have a look at the other variables.\n\nlooks like government expenditure on education is null on nearly 40% of the data, it seems reasonable enoough to drop it\n","2a822b14":"### Now moving to the numerical variables null values.\nin order to fill the nulls, let's examine first what's the distribution for the top variables and see if we should use the median or the mean","701fedf2":"It appers that the XGB and Random forest had the best results overall(LightGBM also did quite well)\nLet's examine these models further.\n\nFirst, we're going to have each model fit to our data\n\nSecond, we'll generate predictions with the model by using our test data\n\nThird, we'll take a look at the accuracy of the predictions by comparing them with the test data\n\nfinally, we'll perform a grid search and see if we improved the score.","c4eea30c":"So, Luxembourg is our outlier, and rightly so. it's one of the richest and developed countries in the world, with a very small number of people. I don't think this one outlier is going to cause any problem going through, so Luxemburg is here to stay.","1d8ec65e":"Thats much better! i wonder if we can improve that high score. Let's see what our grid search has to say about it.","7c666b8e":"## preprocessing the data:\n### goals:\n#### * clean each data set, then merge it based on country name - we want to make sure our columns are organized and ready to be fully merged to one another in order to create a one database to work on\n#### * dealing with null values - we want to have zero null values, we will handle with them based on the characteristics of the specific variable and decide how to manage it.\n#### * dealing with outliers -  basically we wanna have a somewhat normal distributed variables. when trying to predict values with regression models, outliers can be problematic and skew the results to an unwanted direction. we'll deal with that when we finish with steps 1 and 2.","2cb3f16e":"Not much to learn in my opinion as we got the majority of the countries christian and muslim, with a few other ones that make it difficult to have a clear insight.","904d905f":"### Let's keep going on this direction and have a look at the correlations with more precise measures","5dd70124":"Ok, that seems to be all we needed.\n\nFuture questions will be why we got these results and is the relationships between the variables are correlational only or there's a causality there. but for now, that alone can give us some answers.\n\n## Thank you!","533a2bd4":"our target variable seems to be skewed to the right(remember the scaled values from earlier), which means it doesn't fit to the normality assumption, therefore we could have some problems in the future. the normality assumption is quite a strong one and it's important in order to statistically infer any insights for the data.\nwith a simple transformation we could probably solve this problem. let's go for it.","515088bf":"Well, this time we actually improved our model! Think we can do even better?\n\nLets tune it a bit more!","34b66cc3":"## Selected features, without scaling","a4421fed":"the data we're about to analyze is taken from https:\/\/www.kaggle.com\/fernandol\/countries-of-the-world and contains socioeconomic indicators as well as their Gross Domestic Product aka GDP.\nIn the field of macroeconomics, the GDP is basically the main measure of the country's value of goods and services.\nGDP is considered a strong indicator for many aspects of the nation's economic system, and as such, a major area of research is trying to explore, study, and predict the factors which can contribute to one's GDP.\nTo add some more variables which i think can help our model to be fully capble of predicting the GDP, i added a few more indicators taken from the World Bank such as Imports and Exports, the CIA factbook website and https:\/\/www.heritage.org\/index\/ranking for some indicators on market freedom.\n\nwith that in mind, let's start working.","eb900557":"well it looks like our improved model actually did worse. that's a question to another day though. let's move on the the XGB model.","1a21197d":"Once again, we can observe the clear relationship with our variables but diving a little bit deeper into how it actually looks like. all these variables are positively correlated and we can definetely speculate why that's the case, but that requires more than just a few words so let's focus on the statistics.\nAnother thing to add, we can see that there is one outlier over there, but we'll handle it later as i've mentioned.\n\nFor now, let's further examine the Region variable","2b570810":"### So, we took care of the null values.\nnext we're gonna take a look at  the variables which we expect to have high correlation with the GDP.\n\n\nlets plot a heatmap to have a closer look at the details","f573ee04":"### That's much better, now we can see much more clearly what type of correlations our variables have\n\nwell, i expected to see a strong correlation between imports and exports, but having it almost one is super interesting on the one hand, and can cause some problems of multicolinearity on the other, so i think we should seperate ways and stick with the Exports.\n\nmoving on, it looks like phone is the strongest correlator. again, this could be an inverse relationship, i.e, the more rich the country is, the more access to technology they have and not the other way around. let's keep that fact in mind.\n\nwhat strikes me the most is the freedom aspect. 5 out of 7 indicators reflecting the economical freedom are being high at the top, and while i can think of some reasons why again that's maybe only an inverse causality, I also think it's reasonable enough to argue that that's not the case.","77983125":"### alright looks like we can get some insights from this map:\nlooks like the \"freedom\" indicators are correlated quite well with the GDP\n\nadditionaly, ATM's, phones(maybe an inverse causality?), and Foreign investements and trades(imports and exports)\n\nwe'll take a closer look on the top 10 correlators\n\nlets put it all in a nice correlation heatmap, this time smaller and more detailed","3a3adb7d":"## GDP project - analyzing and predicting countrys' GDP based on socio-economic factors\n\n### Goals:\n\nthe goal of this project, as a student of economics and psychology, was to integrate my passion and knowledge in the field of ecomonics and with my developing programming skills. in order to do that, i've chosen to gather some data regarding countrys' GDP and it's related factors, analyze it, and try to fit it into a regression model which will help to predict it correctly.","b0518f10":"## Selected features, with scaling","4aa704a4":"let's take a look at some outliers now","c1026a9b":"## All features, with scaling","3cf5e8eb":"seems like we got on potential outlier in the right side of the distribution, let's check who that is and see what we can do with it","264ac955":"## All features, without scaling","97ed4b2b":"One final thing to do - have a closer look at our star - the GDP\nin order to be able to fit the regressions properly, we got some assumptions to make:\n\n* Normality - We'll have a look at the distribution of our variable and see if it's normal.\n\n* Homoscedasticity - Basically it means that for every \"point\" in the dataset(X's) theres an equality of variance among the Y's. from the scatter plots we've plotted earlier it seems reasonable to assume that's the case.\n\n* Linearity - as we've observed from the plots above, there's seem to be a linear correlation between our variables so we can check this assumption with a degree of confidence.\n\n* Absence of correlated errors - basically thats why we got rid of 'Imports'.","3ced7a53":"##### Those scores are pretty good indeed! let's check if we can have any improvement by implementing grid search"}}