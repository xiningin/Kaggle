{"cell_type":{"6b2e7fd5":"code","dff77346":"code","804024ce":"code","7c95f12d":"code","d9234a47":"code","607160b2":"code","85b0a58a":"code","1309291d":"code","764975e0":"code","9271bbe3":"code","58681cab":"code","e2f31227":"code","60fe8ad5":"code","d07ec43c":"code","05be5ca3":"code","eb74de8c":"code","d296b4f9":"code","aa97ab6a":"code","b5cc8caa":"code","c6a4f547":"code","e357f689":"code","d6a67bce":"code","9fb04f95":"code","a53de5c8":"code","44741544":"code","cd2d51cd":"code","08be2075":"code","58affad0":"code","1b56854c":"code","69ea792c":"code","cb7a7412":"code","1df7b0ed":"markdown","690ef259":"markdown","c6518536":"markdown","311d6f8d":"markdown","c214ab52":"markdown","267908d8":"markdown","5cd9dcc6":"markdown","d43de96b":"markdown","1ace18db":"markdown","6d508232":"markdown","4d69e00c":"markdown","cd65e13e":"markdown","254cf8ac":"markdown","0d5b8212":"markdown","82bef30a":"markdown","0284aeeb":"markdown","16bc571c":"markdown","9ab51c04":"markdown","89c817c8":"markdown","1ced778e":"markdown","0a769315":"markdown","3a8dc461":"markdown","105cf8b9":"markdown","a2d5451e":"markdown","ab7c6f4a":"markdown","c50af2a1":"markdown","3e814a13":"markdown","45f45148":"markdown","168e6dc0":"markdown","96ff4c24":"markdown","cfaa5278":"markdown","2d12d8c1":"markdown","ae26f0d6":"markdown","477aef46":"markdown","812699e7":"markdown","07dc1c8c":"markdown","08ab2c80":"markdown","444f0e52":"markdown","ef795b0d":"markdown","dd864628":"markdown"},"source":{"6b2e7fd5":"import numpy as np \nimport pandas as pd \nfrom sklearn import preprocessing\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\nmatplotlib.style.use('ggplot')\n\nnp.random.seed(34)","dff77346":"#create columns of various distributions\ndf = pd.DataFrame({ \n    'beta': np.random.beta(5, 1, 1000) * 60,        # beta\n    'exponential': np.random.exponential(10, 1000), # exponential\n    'normal_p': np.random.normal(10, 2, 1000),      # normal platykurtic\n    'normal_l': np.random.normal(10, 10, 1000),     # normal leptokurtic\n})\n\n# make bimodal distribution\nfirst_half = np.random.normal(20, 3, 500) \nsecond_half = np.random.normal(-20, 3, 500) \nbimodal = np.concatenate([first_half, second_half])\n\ndf['bimodal'] = bimodal\n\n# create list of column names to use later\ncol_names = list(df.columns)","804024ce":"# plot original distribution plot\nfig, (ax1) = plt.subplots(ncols=1, figsize=(10, 8))\nax1.set_title('Original Distributions')\n\nsns.kdeplot(df['beta'], ax=ax1)\nsns.kdeplot(df['exponential'], ax=ax1)\nsns.kdeplot(df['normal_p'], ax=ax1)\nsns.kdeplot(df['normal_l'], ax=ax1)\nsns.kdeplot(df['bimodal'], ax=ax1);","7c95f12d":"df.head()","d9234a47":"df.mean()","607160b2":"df.describe()","85b0a58a":"df.plot()","1309291d":"normal_big = np.random.normal(1000000, 10000, (1000,1))  # normal distribution of large values\ndf['normal_big'] = normal_big","764975e0":"col_names.append('normal_big')","9271bbe3":"df['normal_big'].plot(kind='kde')","58681cab":"df.normal_big.mean()","e2f31227":"# plot original distribution plot with larger value feature\nfig, (ax1) = plt.subplots(ncols=1, figsize=(10, 8))\nax1.set_title('Original Distributions')\n\nsns.kdeplot(df['beta'], ax=ax1)\nsns.kdeplot(df['exponential'], ax=ax1)\nsns.kdeplot(df['normal_p'], ax=ax1)\nsns.kdeplot(df['normal_l'], ax=ax1)\nsns.kdeplot(df['bimodal'], ax=ax1);\nsns.kdeplot(df['normal_big'], ax=ax1);","60fe8ad5":"df.plot()","d07ec43c":"df.describe()","05be5ca3":"mm_scaler = preprocessing.MinMaxScaler()\ndf_mm = mm_scaler.fit_transform(df)\n\ndf_mm = pd.DataFrame(df_mm, columns=col_names)\n\nfig, (ax1) = plt.subplots(ncols=1, figsize=(10, 8))\nax1.set_title('After MinMaxScaler')\n\nsns.kdeplot(df_mm['beta'], ax=ax1)\nsns.kdeplot(df_mm['exponential'], ax=ax1)\nsns.kdeplot(df_mm['normal_p'], ax=ax1)\nsns.kdeplot(df_mm['normal_l'], ax=ax1)\nsns.kdeplot(df_mm['bimodal'], ax=ax1)\nsns.kdeplot(df_mm['normal_big'], ax=ax1);","eb74de8c":"df_mm['beta'].min()","d296b4f9":"df_mm['beta'].max()","aa97ab6a":"mins = [df[col].min() for col in df.columns]\nmins","b5cc8caa":"maxs = [df[col].max() for col in df.columns]\nmaxs","c6a4f547":"mins = [df_mm[col].min() for col in df_mm.columns]\nmins","e357f689":"maxs = [df_mm[col].max() for col in df_mm.columns]\nmaxs","d6a67bce":"r_scaler = preprocessing.RobustScaler()\ndf_r = r_scaler.fit_transform(df)\n\ndf_r = pd.DataFrame(df_r, columns=col_names)\n\nfig, (ax1) = plt.subplots(ncols=1, figsize=(10, 8))\nax1.set_title('After RobustScaler')\n\nsns.kdeplot(df_r['beta'], ax=ax1)\nsns.kdeplot(df_r['exponential'], ax=ax1)\nsns.kdeplot(df_r['normal_p'], ax=ax1)\nsns.kdeplot(df_r['normal_l'], ax=ax1)\nsns.kdeplot(df_r['bimodal'], ax=ax1)\nsns.kdeplot(df_r['normal_big'], ax=ax1);","9fb04f95":"mins = [df_r[col].min() for col in df_r.columns]\nmins","a53de5c8":"maxs = [df_r[col].max() for col in df_r.columns]\nmaxs","44741544":"s_scaler = preprocessing.StandardScaler()\ndf_s = s_scaler.fit_transform(df)\n\ndf_s = pd.DataFrame(df_s, columns=col_names)\n\nfig, (ax1) = plt.subplots(ncols=1, figsize=(10, 8))\nax1.set_title('After StandardScaler')\n\nsns.kdeplot(df_s['beta'], ax=ax1)\nsns.kdeplot(df_s['exponential'], ax=ax1)\nsns.kdeplot(df_s['normal_p'], ax=ax1)\nsns.kdeplot(df_s['normal_l'], ax=ax1)\nsns.kdeplot(df_s['bimodal'], ax=ax1)\nsns.kdeplot(df_s['normal_big'], ax=ax1);","cd2d51cd":"mins = [df_s[col].min() for col in df_s.columns]\nmins","08be2075":"maxs = [df_s[col].max() for col in df_s.columns]\nmaxs","58affad0":"n_scaler = preprocessing.Normalizer()\ndf_n = n_scaler.fit_transform(df)\n\ndf_n = pd.DataFrame(df_n, columns=col_names)\n\nfig, (ax1) = plt.subplots(ncols=1, figsize=(10, 8))\nax1.set_title('After Normalizer')\n\nsns.kdeplot(df_n['beta'], ax=ax1)\nsns.kdeplot(df_n['exponential'], ax=ax1)\nsns.kdeplot(df_n['normal_p'], ax=ax1)\nsns.kdeplot(df_n['normal_l'], ax=ax1)\nsns.kdeplot(df_n['bimodal'], ax=ax1)\nsns.kdeplot(df_n['normal_big'], ax=ax1);","1b56854c":"mins = [df_n[col].min() for col in df_n.columns]\nmins","69ea792c":"maxs = [df_n[col].max() for col in df_n.columns]\nmaxs","cb7a7412":"# Combined plot.\n\nfig, (ax0, ax1, ax2, ax3) = plt.subplots(ncols=4, figsize=(20, 8))\n\n\nax0.set_title('Original Distributions')\n\nsns.kdeplot(df['beta'], ax=ax0)\nsns.kdeplot(df['exponential'], ax=ax0)\nsns.kdeplot(df['normal_p'], ax=ax0)\nsns.kdeplot(df['normal_l'], ax=ax0)\nsns.kdeplot(df['bimodal'], ax=ax0)\nsns.kdeplot(df['normal_big'], ax=ax0);\n\n\nax1.set_title('After MinMaxScaler')\n\nsns.kdeplot(df_mm['beta'], ax=ax1)\nsns.kdeplot(df_mm['exponential'], ax=ax1)\nsns.kdeplot(df_mm['normal_p'], ax=ax1)\nsns.kdeplot(df_mm['normal_l'], ax=ax1)\nsns.kdeplot(df_mm['bimodal'], ax=ax1)\nsns.kdeplot(df_mm['normal_big'], ax=ax1);\n\n\nax2.set_title('After RobustScaler')\n\nsns.kdeplot(df_r['beta'], ax=ax2)\nsns.kdeplot(df_r['exponential'], ax=ax2)\nsns.kdeplot(df_r['normal_p'], ax=ax2)\nsns.kdeplot(df_r['normal_l'], ax=ax2)\nsns.kdeplot(df_r['bimodal'], ax=ax2)\nsns.kdeplot(df_r['normal_big'], ax=ax2);\n\n\nax3.set_title('After StandardScaler')\n\nsns.kdeplot(df_s['beta'], ax=ax3)\nsns.kdeplot(df_s['exponential'], ax=ax3)\nsns.kdeplot(df_s['normal_p'], ax=ax3)\nsns.kdeplot(df_s['normal_l'], ax=ax3)\nsns.kdeplot(df_s['bimodal'], ax=ax3)\nsns.kdeplot(df_s['normal_big'], ax=ax3);","1df7b0ed":"Let's see what are the means are.","690ef259":"Let's look at the minimums and maximums for each column prior to scaling.","c6518536":"# MinMaxScaler ","311d6f8d":"# Why scale, standardize, or normalize?\n\nMany machine learning algorithms, such as neural networks, regression-based algorithms, K-nearest neighbors, support vector machines with radial bias kernel functions, principal components analysis, and algorithms using linear discriminant analysis don't perform as well if the features are not on relatively similar scales. \n\nSometimes you'll want a more normally distributed distribution. \n\nSome of the methods below dilute the effects of outliers. ","c214ab52":"Now let's look at StandardScaler.","267908d8":"Let's check the minimums and maximums for each column after RobustScaler.","5cd9dcc6":"Looks close enough to 0 to 1 intervals to me. Our feature with much larger values was brought into scale with our other features. \n\nNow let's look at RobustScaler.","d43de96b":"If we put this on the same plot as the original distributions, you can't even see the earlier columns.","1ace18db":"The new, high-value distribution is way to the right. And here's a plot of the values.","6d508232":"You can see that after any transformation the distributions are on a similar scale. Also notice that MinMaxScaler doesn't distort the distances between the values in each feature.","4d69e00c":"MinMaxScaler subtracts the column mean from each value and then divides by the range.","cd65e13e":"The ranges are fairly similar to RobustScaler. \n\nNow let's look at Normalizer.","254cf8ac":"Although the range of values for each feature is much smaller than for the original features, it's larger and varies more than for MinMaxScaler. The bimodal distribution values are now compressed into two small groups.","0d5b8212":"Let's plot our original distributions.","82bef30a":"Normalizer also moved the features to similar scales. Notice that the range for our much larger feature's values is now extremely small and clustered around .9999999999. ","0284aeeb":"StandardScaler is scales each column to have 0 mean and unit variance.","16bc571c":"## Add a feature with much larger values","9ab51c04":"Qutie a nice chart, don't you think? You can see that all features now have 0 mean.","89c817c8":"# Scale, Standardize, or Normalize with scikit-learn\n## When to use MinMaxScaler, RobustScaler, StandardScaler, and Normalizer\n## By Jeff Hale","1ced778e":"Let's check the minimums and maximums for each column after StandardScaler.","0a769315":"# Combined Plot","3a8dc461":"Notice how the shape of each distribution remains the same, but now the values are between 0 and 1.","105cf8b9":"# RobustScaler","a2d5451e":"Now let's see what happens when we do some scaling. Let's apply MinMax Scaler first.","ab7c6f4a":"Let's check the minimums and maximums for each column after scaling.","c50af2a1":"RobustScaler subtracts the column median and divides by the interquartile range.","3e814a13":"We've got a normalish distribution with a mean near 1,000,0000.","45f45148":"Let's look at our original and transformed distributions together. We'll exclude Normalizer because you generally want to tranform your features, not your samples.","168e6dc0":"If you'd like more summary statistics:","96ff4c24":"Here's a [cheat sheet I made in a google sheet](https:\/\/docs.google.com\/spreadsheets\/d\/1woVi7wq13628HJ-tN6ApaRGVZ85OdmHsDBKLAf5ylaQ\/edit?usp=sharing) to help folks keep the options straight. \n\nLet's set things up and start making some distributions!","cfaa5278":"I hope you found this Kernel to be a helpful introduction to Scaling, Standardizing, and Normalizing with scikit-learn. Please upvote it if you found it helpful!","2d12d8c1":"Let's check the minimums and maximums for each column after MinMaxScaler.","ae26f0d6":"This feature could be home prices, for example.","477aef46":"# Original Distributions","812699e7":"These values are all in the same ballpark.","07dc1c8c":"Let's make several types of random distributions.","08ab2c80":"# StandardScaler","444f0e52":"## TLDR\n\n* Use MinMaxScaler as your default\n* Use RobustScaler if you have outliers and can handle a larger range\n* Use StandardScaler if you need normalized features\n* Use Normalizer sparingly - it normalizes rows, not columns","ef795b0d":"# Normalizer\n\nNote that normalizer operates on the rows, not the columns. It applies l2 normalization by default.","dd864628":"## Please upvote if you find this Kernel helpful :)"}}