{"cell_type":{"1ffe4f3c":"code","49b04de4":"code","92f1992f":"code","c3cbd3cb":"code","d056780d":"code","aa9383ec":"code","51ad3933":"code","eb71456c":"code","fbd77bde":"code","01b71161":"code","d8110ae9":"code","361084f1":"code","d4b8e726":"code","9e3d3afc":"code","ed3f22af":"code","e472a421":"code","08fbe062":"markdown","063a23a9":"markdown","77e4174c":"markdown","606c6670":"markdown","225101f0":"markdown","62de9c52":"markdown","daa3c55c":"markdown"},"source":{"1ffe4f3c":"import sys\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n%matplotlib inline","49b04de4":"df_train = pd.read_csv(\"..\/input\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/test.csv\")\nprint(\"Dataset ready\")","92f1992f":"for i in df_test.isnull().any():\n    if i == True:\n        sys.exit(\"There is unlabeled data\")\nprint(\"No missing values found\")","c3cbd3cb":"for i in df_train.isnull().any():\n    if i == True:\n        sys.exit(\"There is unlabeled data\")\nprint(\"No missing values found\")","d056780d":"y = df_train[\"label\"]\nx = df_train.drop(\"label\", axis=1)","aa9383ec":"x.shape","51ad3933":"df_test.shape","eb71456c":"x = x \/ 255.0\ndf_test = df_test \/ 255.0\n\nprint(\"Completed\")","fbd77bde":"plt.imshow(df_test.values.reshape(-1, 28, 28, 1)[40][:,:,0])","01b71161":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state=2)\n\nprint(\"Completed\")","d8110ae9":"from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(criterion = \"entropy\")\nmodel.fit(x_train, y_train)\n\nprint(model.score(x_test, y_test))","361084f1":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nforrest_params = dict(     \n    max_depth = [n for n in range(9, 14)],     \n    min_samples_split = [n for n in range(4, 11)], \n    min_samples_leaf = [n for n in range(1, 5)],     \n    n_estimators = [n for n in range(10, 60, 10)],\n)\n\nforrest = RandomForestClassifier(criterion = \"entropy\")\nforest_cv = GridSearchCV(estimator=forrest, param_grid=forrest_params, cv=5) \nforest_cv.fit(x_train.values[: 100, :], y_train.values[: 100])\nprint(forest_cv.best_score_)\nprint(forest_cv.best_estimator_)","d4b8e726":"from sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier(criterion = \"entropy\", max_depth=12, min_samples_leaf=1, min_samples_split=6, n_estimators=50)\nmodel.fit(x_train, y_train.values)\n\nprint(model.score(x_test, y_test))","9e3d3afc":"#Learning Curve\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.utils import shuffle\n\nx, y = shuffle(x, y)\n\ntrain_sizes_abs, train_scores, test_scores = learning_curve(RandomForestClassifier(criterion = \"entropy\", n_estimators = 50), x, y)\nplt.plot(train_sizes_abs, np.mean(train_scores, axis = 1))\nplt.plot(train_sizes_abs, np.mean(test_scores, axis = 1))","ed3f22af":"from sklearn.svm import SVC\n\nmodel = SVC(kernel = \"rbf\", gamma = 0.021, C = 2.1)\nmodel.fit(x_train, y_train)\nprint(model.score(x_test, y_test))","e472a421":"pred = model.predict(df_test)\n\nsubmission = pd.DataFrame({\n    \"ImageId\": list(range(1,len(pred)+1)),\n    \"Label\": pred\n})\n\nsubmission.to_csv(\"submision.csv\", index=False)\n\nprint(\"Prediction Completed\")","08fbe062":"** Split the Data**\n\nNext, I'll split the dataset into x and y data and check if the shape of the test data and train data is the same.","063a23a9":"**Hyperparameter Tuning**\n\nThe results are not so bad, but not good either. Let's do some hyperparameter tuning with GridSearchCV.\nDue to the size of the dataset I will only use the first 1000 entries to test the hyperparameters. We can do this because the data is not sorted. But you should note that the results don't have to be 100% correct.\n\nAfter this, I'll train the model with the parameters and the whole dataset.","77e4174c":"**First Results**\n\nAs you can see the RFC provides good results in an acceptable amount of time and hyperparameter tuning improved the accuracy by 2%. Now let's see if the model would improve with more data. ","606c6670":"** Training ** \n\nNow I split the training data into a train and test sample. After this I'll train the model with a RFC. ","225101f0":"** Load Data**\n\nLet's load the files and check for any missing values in both datasets. ","62de9c52":"**Another Classifier**\n\nTo further improve the model I used another classifier. With some hyper-parameter tuning, which I won't show here because it makes the kernel extremely slow, I was able to get good results. However, it is noticeable that the SVC is much slower than the RFC. ","daa3c55c":"**Transform**\n\nNow I divide the arrays by 255. I do this so that the values are normalized, i.e. between 0 and 1 and distortion is reduced. \n\nThen I just have a sample displayed to see if the dataset works. For this you have to convert the array from 1x784 to 28x28, because the image has this size. "}}