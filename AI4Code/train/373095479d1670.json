{"cell_type":{"668ba8e9":"code","01c23750":"code","8637e64e":"code","08a38384":"code","a2a487ec":"code","117f46c5":"code","1b3a34c0":"code","14836340":"code","9ce664ac":"code","8daf8f4b":"code","65bd0ec1":"code","788d75b0":"code","acc29d71":"code","411c0524":"code","70ca5e2b":"code","b2b1b79d":"code","b28a3986":"code","560e4f3b":"code","63e8ca03":"code","4d4b3401":"code","460bce57":"code","8d7fb8a5":"code","aee3a98f":"code","b95bbc11":"code","0a42634d":"code","d6c81cab":"code","9c0a504e":"code","401a97cc":"code","0738e7dd":"code","30f201d0":"code","a8822329":"code","426f1a04":"code","4254e898":"code","38ba4926":"code","76001405":"code","4b8e9492":"code","88ea42a1":"code","9dcfd88a":"code","12a10c2c":"code","0dcef1a9":"code","cd01c845":"code","0d2ba871":"code","d9154a35":"code","e39f4a6d":"markdown","7950cde7":"markdown","4127e496":"markdown","2b80e528":"markdown","cccdabc1":"markdown","eec5a24e":"markdown","7ce52173":"markdown","d3e4921d":"markdown","418629bd":"markdown","6dbcedbf":"markdown","72c90062":"markdown","b62faec1":"markdown","0eb6d471":"markdown","bec8dfdd":"markdown","94eff787":"markdown"},"source":{"668ba8e9":"# basic imports\nimport pandas as pd\nimport numpy as np\n\n# plotting libraries and magics\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# to calculate distance between two coordinates\nfrom geopy.distance import geodesic\n\n# garbage collector\nimport gc\n\n# preprocessing\nfrom sklearn.preprocessing import LabelEncoder\n\n# modeling requirements\nfrom sklearn.model_selection import train_test_split # to split the data into train and validation sets\nfrom sklearn.metrics import r2_score # eval metric for this competetion\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n# enabling multiple outputs for Jupyter cells\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity='all'\n\n# for fancy looping\nfrom tqdm import tqdm\n\n\n# --------- Setting some configs and variables --------- #\n# to display all the columns instead of ... in the output (when there are many columns)\npd.set_option('display.max_columns', None)\n\n# Declaring the PATH for all input data\nPATH = \"..\/input\/night-life-challenge-trivago\/\"\n\n# our random seed for the model\nSEED = 42 # because, why not! \u00af\\_(\u30c4)_\/\u00af","01c23750":"# to show a tabular report of the missing data in a given dataset\ndef missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n#     return(np.transpose(tt))\n    return tt\n\n# get a list of categorical and numeric variables for a dataset\ndef get_num_cat_cols(df):\n    \"\"\"\n    Returns two lists, one for categorical variables and one for numeric variables\n    \"\"\"\n    cat_vars = [col for col in df.columns if df[col].dtype in (['object', 'O'])]\n    num_vars = [col for col in df.columns if df[col].dtype not in (['object', 'O'])]\n    \n    return cat_vars, num_vars\n\n# to find out if two lists have any common members\ndef common_member(a, b): \n    a_set = set(a) \n    b_set = set(b) \n    if (a_set & b_set): \n        return True \n    else: \n        return False","8637e64e":"# to display wordclouds\n\nfrom wordcloud import WordCloud, STOPWORDS\n\n# adding the locations as stop words too - since they don't add value to our wordcloud\nstopwords = [\"Greece\", \"Hong\", \"Kong\", \"China\", \"Thessaloniki\", \"Los\", \"Angeles\", \"Amsterdam\", \"'\"] + list(STOPWORDS) \n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=200,\n        max_font_size=40, \n        scale=3,\n        random_state=SEED # chosen at random by flipping a coin; it was heads\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(12, 12))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","08a38384":"# load the data into data frames\nhotel = pd.read_csv(PATH + 'hotels.csv')\npoi = pd.read_csv(PATH + 'pois.csv')\n\n# let's take a peek\nhotel.head(3)\npoi.head(3)","a2a487ec":"# function to display missing values and other details about a dataframe - we will use this later to look at the poi dataframe.\ndef get_dataframe_info(df):\n    # checking how many numerical and categorical columns we have\n    cat_cols, num_cols = get_num_cat_cols(df)\n    print(f'There are {len(num_cols)} numeric columns and {len(cat_cols)} categorical columns')\n    \n    print('\\n******** Missing data report ********')\n    print(missing_data(df))\n    \n    # let's see the distribution of the data as well\n    print('\\n',df.describe())","117f46c5":"# hotel dataframe details\nget_dataframe_info(hotel)","1b3a34c0":"# gc.collect()","14836340":"# function to display column-wise reports for a dataframe\ndef column_data_distribution(df):\n    \n    # let's plot the distribution for the numeric columns\n    cat_cols, num_cols = get_num_cat_cols(df)\n    for col in num_cols:\n        print(f'\\n****** Stats for {col} column ******')\n        print(f'\\nThere are {df[col].nunique()} unique values.')\n        sns.distplot(df[col].dropna().sort_values()); \n        plt.show();\n        \n    # doing the same for the categorical columns\n    for col in cat_cols:\n        print(f'\\n****** Stats for {col} column ******')\n        print(f'\\nThere are {df[col].nunique()} unique values.')\n        sns.countplot(df[col].dropna().sort_values()); \n        plt.show();","9ce664ac":"# let's replace all NANs with 0 - under the assumption that if a particular value is not known, chances are very less that that feature is there.\nhotel = hotel.fillna(0)","8daf8f4b":"column_data_distribution(hotel)","65bd0ec1":"hotel.drop('club_club_hotel', axis=1, inplace=True)","788d75b0":"# how many poi_types do we have?\npoi_types = set(poi['poi_types'].str.cat(sep=\" ,\").replace(' ,', ',').split(','))\n# poi_types\n\n# also, let's save the  poi_types as a list in a new column in poi df\npoi['poi_types_list'] = [list(p.split(', ')) for p in poi['poi_types']]\n\n# from the above, let's build two sets of poi_types, one for nightlife friendly types, and the other for the anti-nightlife types.\nnightlife_poi_types = ['Pub', 'Bar', 'Disco', 'Nightclub', 'Food & Drink', 'Event', 'Entertainment', 'Bowling', 'Caf\u00e9', 'Casino', 'Game Centers']\nanti_nightlife_poi_types = ['Religious', 'Nature', 'Architectural Buildings', 'Historic Sites', 'Lake', 'Museums', 'Spas', 'Classes \/ Workshops'\n                           'Art Galleries', 'Botanical Gardens', 'Golf Area', 'National Parks', 'Parks', 'Palaces \/ Castles', 'Zoos \/ Aquariums']\n\n# let's add two flags to the poi dataframe: nightlife_ok, nightlife_not_ok\npoi['nightlife_ok'] = [int(common_member(p, nightlife_poi_types)) for p in poi['poi_types_list']] # 1 when the poi_type is present in nightlife_poi_types, else 0\npoi['nightlife_not_ok'] = [int(common_member(p, anti_nightlife_poi_types)) for p in poi['poi_types_list']] # 1 when the poi_type is present in anti_nightlife_poi_types, else 0","acc29d71":"# how many unique city_ids do we have in poi and hotel dataframes?\nprint(f'There are {hotel[\"city_id\"].nunique()} unique cities in hotel df, and {poi[\"city_id\"].nunique()} unique cities in poi df')\n# 4 in both - so, we are good to do an inner join to merge the two dataframes\n\n# let's now merge the hotel dataframe with the poi dataframe - using the city_id.\nhotel_poi = hotel.merge(poi, on='city_id', how='inner')\nprint(f'Shape of merged df: {hotel_poi.shape}')\n\n# also, let's rename the latitude and longitude columns for clarity (we shouldn't waste brain power on remembering if '_x' was for hotel or poi!)\nhotel_poi.rename(columns={'longitude_x':'longitude_hotel', 'latitude_x':'latitude_hotel', 'longitude_y':'longitude_poi', 'latitude_y':'latitude_poi'}, inplace=True)\n\n# let's see how the merged data looks like\nhotel_poi.head(3)","411c0524":"# wordcloud for the points of interest that are nightlife_ok\nshow_wordcloud(hotel_poi[hotel_poi['nightlife_ok']==1].poi_types)","70ca5e2b":"# wordcloud for the points of interest that are nightlife__not_ok\nshow_wordcloud(hotel_poi[hotel_poi['nightlife_not_ok']==1].poi_types)","b2b1b79d":"# # verifying if the above steps worked\n# hotel_poi[hotel_poi.nightlife_ok == 1].head(3)\n# hotel_poi[hotel_poi.nightlife_not_ok == 1].head(3)","b28a3986":"# let's pull the latitudes and longitudes into 4 series\nhotel_latitudes = hotel_poi['latitude_hotel']\nhotel_longitudes = hotel_poi['longitude_hotel']\npoi_latitudes = hotel_poi['latitude_poi']\npoi_longitudes = hotel_poi['longitude_poi']\n\n# now, we can calculate the distance between the poi and the hotel\ndistance = []\nfor i in tqdm(range(hotel_poi.shape[0])):\n    distance.append(geodesic((hotel_latitudes[i], hotel_longitudes[i]), (poi_latitudes[i], poi_longitudes[i])).meters)\nprint('Done!')\n\n# adding the distances to the main dataframe\nhotel_poi['distance'] = distance","560e4f3b":"# # pickle the above processed file - to save time in future runs\n# PICKLE_NAME = '..\/input\/hotel_poi.pkl'\n# hotel_poi.to_pickle(PICKLE_NAME)\n\n# # loading the pickle file\n# hotel_poi = pd.read_pickle(PICKLE_NAME)\n# print('Pickle loaded successfully!')","63e8ca03":"hotel_poi.distance.describe()\nhotel_poi.distance_to_center.describe()","4d4b3401":"# Creating a flag to show the number of nightlife_ok pois within 500m\nnightlife_ok_poi_500m = hotel_poi[(hotel_poi['nightlife_ok']==1) & (hotel_poi['distance']<=500)].groupby('hotel_id').nightlife_ok.count().reset_index()\n\n\n# we'll do the same for nightlife_not_ok pois\nnightlife_not_ok_poi_500m = hotel_poi[(hotel_poi['nightlife_not_ok']==1) & (hotel_poi['distance']<=500)].groupby('hotel_id').nightlife_not_ok.count().reset_index()\n\n# merging the above two dataframes into one\npoi_500m = nightlife_ok_poi_500m.merge(nightlife_not_ok_poi_500m, on='hotel_id', how='inner')","460bce57":"# adding the count data to the hotel dataframe\nhotel = hotel.merge(poi_500m, on='hotel_id', how='left')\nhotel.head()","8d7fb8a5":"# let's check if there are any NANs in the new columns\nhotel.nightlife_ok.isnull().any()\nhotel.nightlife_not_ok.isnull().any()","aee3a98f":"# replace the NANs with 0.\nhotel.fillna(0, inplace=True)","b95bbc11":"# to be continued\n# implement this for the geo-data: https:\/\/towardsdatascience.com\/lets-make-a-map-using-geopandas-pandas-and-matplotlib-to-make-a-chloropleth-map-dddc31c1983d\n# to get the shapefiles, just google them! - or check out the files in the dataset we created :)","0a42634d":"hotel.columns","d6c81cab":"# we want to consider the 'hotel_type'. Let's label encode that.\nle = LabelEncoder()\nhotel['hotel_type_encoded'] = le.fit_transform(hotel['hotel_type'])","9c0a504e":"# taking the columns we want for the model in a separate dataframe\nfeatures = hotel.loc[:,['hotel_type_encoded', 'car_park', 'attraction_hotel', 'beach_front_hotel',\n       'convention_hotel', 'spa_hotel', 'country_hotel', 'airport_hotel',\n       'senior_hotel', 'eco_friendly_hotel', 'party_people', 'business_people',\n       'honeymooners', 'singles', 'large_groups', 'family_hotel',\n       'gay_friendly', 'wifi_lobby', 'wifi_room', 'nightlife_ok',\n       'nightlife_not_ok']]","401a97cc":"from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=99) # 99, because for now, we don't know how many clusters we should take","0738e7dd":"wcss = []\n\nfor k in range(1,15):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(features)\n    wcss.append(kmeans.inertia_)\n\n\nplt.figure(figsize=(12, 5));\nplt.title(\"WCSS \/ K Chart\", fontsize=18);\nplt.plot(range(1,15),wcss,\"-o\");\nplt.grid(True);\nplt.xlabel(\"Amount of Clusters\",fontsize=14);\nplt.ylabel(\"Inertia\",fontsize=14);\nplt.xticks(range(1,20));\nplt.tight_layout();\nplt.show();","30f201d0":"# clustering the hotels\nkmeans = KMeans(n_clusters=2, random_state=SEED);\nfeatures[\"labels\"] = kmeans.fit_predict(features);","a8822329":"gc.collect()","426f1a04":"# let's add the predicted classes to the original dataframe for validation\nhotel['labels'] = features['labels']","4254e898":"plt.figure(figsize=(24,4));\n\nplt.suptitle(\"K Means Clustering: K=2\",fontsize=20);\n\nplt.subplot(1,5,1);\nplt.title(\"Hong Kong\",fontsize=16);\nplt.scatter(hotel.longitude[(hotel.labels == 0) & (hotel.city_id==31497)],hotel.latitude[(hotel.labels == 0) & (hotel.city_id==31497)], label='class 0')\nplt.scatter(hotel.longitude[(hotel.labels == 1) & (hotel.city_id==31497)],hotel.latitude[(hotel.labels == 1) & (hotel.city_id==31497)], label='class 1')\nplt.legend();\n\nplt.subplot(1,5,2);\nplt.title(\"Thessaloniki\",fontsize=16);\nplt.scatter(hotel.longitude[(hotel.labels == 0) & (hotel.city_id==14121)],hotel.latitude[(hotel.labels == 0) & (hotel.city_id==14121)], label='class 0')\nplt.scatter(hotel.longitude[(hotel.labels == 1) & (hotel.city_id==14121)],hotel.latitude[(hotel.labels == 1) & (hotel.city_id==14121)], label='class 1')\nplt.legend();\n\nplt.subplot(1,5,3);\nplt.title(\"Los Angeles\",fontsize=16);\nplt.scatter(hotel.longitude[(hotel.labels == 0) & (hotel.city_id==14257)],hotel.latitude[(hotel.labels == 0) & (hotel.city_id==14257)], label='class 0')\nplt.scatter(hotel.longitude[(hotel.labels == 1) & (hotel.city_id==14257)],hotel.latitude[(hotel.labels == 1) & (hotel.city_id==14257)], label='class 1')\nplt.legend();\n\nplt.subplot(1,5,4);\nplt.title(\"Amsterdam\",fontsize=16);\nplt.scatter(hotel.longitude[(hotel.labels == 0) & (hotel.city_id==27561)],hotel.latitude[(hotel.labels == 0) & (hotel.city_id==27561)], label='class 0')\nplt.scatter(hotel.longitude[(hotel.labels == 1) & (hotel.city_id==27561)],hotel.latitude[(hotel.labels == 1) & (hotel.city_id==27561)], label='class 1')\nplt.legend();\n\nplt.subplots_adjust(top=0.8);\nplt.show();","38ba4926":"# # for K=3\n# plt.figure(figsize=(24,4));\n# plt.suptitle(\"K Means Clustering: K=3\",fontsize=20);\n\n# plt.subplot(1,5,1);\n# plt.title(\"Hong Kong\",fontsize=16);\n# plt.scatter(hotel.longitude[(hotel.labels_3 == 0) & (hotel.city_id==31497)],hotel.latitude[(hotel.labels_3 == 0) & (hotel.city_id==31497)], label='class 0')\n# plt.scatter(hotel.longitude[(hotel.labels_3 == 1) & (hotel.city_id==31497)],hotel.latitude[(hotel.labels_3 == 1) & (hotel.city_id==31497)], label='class 1')\n# plt.scatter(hotel.longitude[(hotel.labels_3 == 2) & (hotel.city_id==31497)],hotel.latitude[(hotel.labels_3 == 2) & (hotel.city_id==31497)], label='class 2')\n# plt.legend();\n\n# plt.subplot(1,5,2);\n# plt.title(\"Thessaloniki\",fontsize=16);\n# plt.scatter(hotel.longitude[(hotel.labels_3 == 0) & (hotel.city_id==14121)],hotel.latitude[(hotel.labels_3 == 0) & (hotel.city_id==14121)], label='class 0')\n# plt.scatter(hotel.longitude[(hotel.labels_3 == 1) & (hotel.city_id==14121)],hotel.latitude[(hotel.labels_3 == 1) & (hotel.city_id==14121)], label='class 1')\n# plt.scatter(hotel.longitude[(hotel.labels_3 == 2) & (hotel.city_id==14121)],hotel.latitude[(hotel.labels_3 == 2) & (hotel.city_id==14121)], label='class 2')\n# plt.legend();\n\n# plt.subplot(1,5,3);\n# plt.title(\"Los Angeles\",fontsize=16);\n# plt.scatter(hotel.longitude[(hotel.labels_3 == 0) & (hotel.city_id==14257)],hotel.latitude[(hotel.labels_3 == 0) & (hotel.city_id==14257)], label='class 0')\n# plt.scatter(hotel.longitude[(hotel.labels_3 == 1) & (hotel.city_id==14257)],hotel.latitude[(hotel.labels_3 == 1) & (hotel.city_id==14257)], label='class 1')\n# plt.scatter(hotel.longitude[(hotel.labels_3 == 2) & (hotel.city_id==14257)],hotel.latitude[(hotel.labels_3 == 2) & (hotel.city_id==14257)], label='class 2')\n# plt.legend();\n\n# plt.subplot(1,5,4);\n# plt.title(\"Amsterdam\",fontsize=16);\n# plt.scatter(hotel.longitude[(hotel.labels_3 == 0) & (hotel.city_id==27561)],hotel.latitude[(hotel.labels_3 == 0) & (hotel.city_id==27561)], label='class 0')\n# plt.scatter(hotel.longitude[(hotel.labels_3 == 1) & (hotel.city_id==27561)],hotel.latitude[(hotel.labels_3 == 1) & (hotel.city_id==27561)], label='class 1')\n# plt.scatter(hotel.longitude[(hotel.labels_3 == 2) & (hotel.city_id==27561)],hotel.latitude[(hotel.labels_3 == 2) & (hotel.city_id==27561)], label='class 2')\n# plt.legend();\n\n# plt.subplots_adjust(top=0.8);\n# plt.show();","76001405":"hotel[hotel.labels == 1]","4b8e9492":"# let's pull the hotel_ids for class 0 and 1\nclass_0_hotel = hotel[hotel['labels']==0].hotel_id.unique()\nclass_1_hotel = hotel[hotel['labels']==1].hotel_id.unique()","88ea42a1":"# hotel[hotel.hotel_id.isin(class_0_hotel)]\n# hotel[hotel.hotel_id.isin(class_1_hotel)]","9dcfd88a":"# let's see what the class 0 and 1 are about\nplt.title('Wordcloud for label 0')\n\n# we want the poi types for pois that are less than 500m away\nshow_wordcloud(hotel_poi[(hotel_poi.hotel_id.isin(class_0_hotel)) & (hotel_poi.distance <= 500)].poi_types)","12a10c2c":"plt.title('Wordcloud for label 1')\n\n# we want the poi types for pois that are less than 500m away\nshow_wordcloud(hotel_poi[(hotel_poi.hotel_id.isin(class_1_hotel)) & (hotel_poi.distance <= 500)].poi_types)","0dcef1a9":"from sklearn.mixture import BayesianGaussianMixture\nbgm = BayesianGaussianMixture(random_state=SEED, verbose=0)","cd01c845":"bgm.fit(features);\nhotel['labels_bgm'] = bgm.predict_proba(features);","0d2ba871":"import geopandas as gpd\nMAP_PATH_HONGKONG = \"..\/input\/night-life-challenge-trivago\/hong_kong_poi\/hong_kong_poi.shp\"","d9154a35":"# the shapefiles are not giving us what we want at the moment. We'll see if we can find shapefiles that give us results to work with. \n# We'll work on that in a later version.\n\n# # set the filepath and load in a shapefile\n# # map_amsterdam = gpd.read_file(PATH + 'Amsterdam_shapefile.csv')\n# map_hk = gpd.read_file(MAP_PATH_HONGKONG)\n\n# # check data type so we can see that this is not a normal dataframe, but a GEOdataframe\n# map_hk.head()\n# # now let's preview what our map looks like with no data in it\n# map_hk.plot();","e39f4a6d":"From the above report, we can see that 'club_club_hotel' has only one value - 0. This does not add any information. So, we'll remove this.","7950cde7":"# 3. Making sense of what is happening in the clustering:","4127e496":"## 2. Now, let's get modeling.\n\nBut before we do that, let's first assess the situation here.\nWe have a bunch of hotels and points of interest around those. From this, **our task is to figure out which hotels are good for \"night-life\"**. \n\nAs we don't have the information of which hotels are suitable for night-life, we will be using **unsupervised clustering**.\nWe'll be using KMeans clustering for the same.\n\nAlright, let's jump in!","2b80e528":"## 1. Time to explore the data!\n#### 1.1. Let's first look at the Hotel dataframe as a whole.","cccdabc1":"# Goals: \n### 1. To explore the data in search of interesting facts and clues for step #2.\n### 2. To predict whether a given hotel is good for 'nightowls' and 'party-people' or not\n### 3. Score validation, making sense of the output scores","eec5a24e":"### But, wait. We don't want clusters. We want scores.\nLet's try Bayesian GMM (Gaussian Mixture Model).","7ce52173":"#### Merging hotels with poi data","d3e4921d":"Since it is night, transportation may be unsafe or inconvenient (or unavailable altogether). ","418629bd":"Let's look at the distances of the hotels from the pois.","6dbcedbf":"**At this point, we have the following features:**\n\n* 'hotel_id'\n* 'city_id'\n* 'hotel_type'\n* 'basename'\n* 'distance_to_center'\n* 'longitude'\n* 'latitude'\n* 'overall_rating'\n* 'impression_level'\n* 'interaction_level'\n* 'car_park'\n* 'designer_hotel'\n* 'attraction_hotel'\n* 'luxury_hotel'\n* 'beach_front_hotel'\n* 'convention_hotel'\n* 'spa_hotel'\n* 'country_hotel'\n* 'airport_hotel'\n* 'senior_hotel'\n* 'eco_friendly_hotel'\n* 'party_people'\n* 'business_people'\n* 'honeymooners'\n* 'singles'\n* 'large_groups'\n* 'family_hotel'\n* 'gay_friendly'\n* 'wifi_lobby'\n* 'wifi_room'\n* 'nightlife_ok'\n* 'nightlife_not_ok'\n\n\n\n**Out of which, we'll be removing the following (assumptions and explanations provided alongside):**\n\n* *'hotel_id', 'city_id', 'basename', 'longitude', 'latitude'*: Since these are just identifiers and do not add information (city_id, latitude, and latitude may cause unnecessary clustering bias)\n\n* *'distance_to_center'* - It doesn't matter how far or near a party place is. However, if we are to consider how good that place is for nightlife when compared to other hotels, then this will play a role.\n\n* *'overall_rating', 'impression_level', 'interaction_level'* - doesn't matter if it's a good place or not for deciding whether the hotel is good for nightlife or not. However, this is useful when we need to compare (and rank)\n\n* *'designer_hotel', 'luxury_hotel'* - doesn't matter for nightlife prediction. This might get us better customer experience which can be used later to improve on the rankings.","72c90062":"### Working with the shapefiles:","b62faec1":"### Conclusion:\n\nOur scores are not even close to what we want. But it's a start.\nWe'll improve on this in later versions.","0eb6d471":"<img src='https:\/\/images.unsplash.com\/photo-1519214605650-76a613ee3245'>","bec8dfdd":"**2** looks like a good number.","94eff787":"But, hang on!\nWe can find out the ideal number of clusters using the **Elbow Rule**."}}