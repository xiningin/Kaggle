{"cell_type":{"634cf5bb":"code","9320c409":"code","6fe10f09":"code","2ac4bac2":"code","21ca193c":"code","71a3a640":"code","039ca7ad":"code","ab6ac6ce":"code","75208492":"code","1d8fdaa9":"code","00f2d77f":"code","9b2bda31":"code","b9bed309":"code","e0e29b03":"code","caf86eab":"code","3149a158":"code","b52481c4":"code","1355fbc4":"code","1c4248f4":"code","901f2638":"code","0ac29c0d":"code","74d42fca":"code","98920744":"code","f367cfbf":"code","9b72cf1a":"code","7120edfa":"code","2f9004eb":"code","c445c933":"code","be733809":"code","01181de6":"code","aad5c308":"code","b26431c8":"markdown","bee718fb":"markdown","5d546300":"markdown","94fecd66":"markdown","464ead7a":"markdown","f6513b10":"markdown","d0d3c16b":"markdown","66971bba":"markdown","cf6cf730":"markdown","2c227523":"markdown","b9d60003":"markdown","76e4ce83":"markdown","95621e31":"markdown","2473b2fa":"markdown","955ccf9f":"markdown"},"source":{"634cf5bb":"# all required imports\nimport json\nfrom urllib.request import urlopen\nimport re\nimport time\n!pip install praw\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport praw\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\n\n\n%matplotlib inline","9320c409":"baseurl = 'https:\/\/api.pushshift.io\/reddit\/search\/submission\/?subreddit=india&sort=desc&sort_type=created_utc&'\n\nperiod = 1 # in years\n\nweek_seconds = 7 * 24 * 60 * 60\nweek_count = 1\nstart_seconds = int(time.time()) - ((week_count - 1) * week_seconds) # start time of collection\n\nall_data = []\n\nwhile week_count <= int(period * 52):\n    before, after = start_seconds, start_seconds - week_seconds\n    url = baseurl + 'after={}&before={}&size=1000'.format(after, before)\n    start_seconds = after\n    r = urlopen(url).read()\n    data = json.loads(r.decode('utf-8'))\n    all_data += data['data']\n    print(f'Week {week_count}\/{period*52} done')\n    week_count += 1\n    \n    \nall_data_DF = pd.DataFrame(all_data)","6fe10f09":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2ac4bac2":"all_data_DF = pd.read_csv(\"..\/input\/redditindiadata\/reddit_india_latest.csv\")\nall_data_DF","21ca193c":"keep_features = ['title','url', 'selftext', 'link_flair_text','comments']","71a3a640":"all_data_DF.rename(columns={'body': 'selftext', 'flair': 'link_flair_text'}, inplace=True)","039ca7ad":"collected_data = all_data_DF[keep_features] ","ab6ac6ce":"with_flair = collected_data[collected_data.link_flair_text.notnull()]\nwith_flair.reset_index(drop=True, inplace=True)\nwith_flair","75208492":"from collections import Counter\ncounts = Counter(with_flair['link_flair_text'])\ncounts","1d8fdaa9":"plt.figure(figsize=(10,4))\nwith_flair.link_flair_text.value_counts().plot(kind='bar');","00f2d77f":"threshold = 1000\nmain_flairs = [flair for flair in counts if counts[flair] > threshold]\n#main_flairs = ['Coronavirus','Politics', 'Non-Political','Science\/Technology','AskIndia', 'Policy\/Economy']","9b2bda31":"with_flair.link_flair_text.value_counts().plot(kind='bar');","b9bed309":"df_col_len = int(with_flair['comments'].str.encode(encoding='utf-8').str.len().max())\ndf_col_len","e0e29b03":"STOPWORDS = set(stopwords.words('english'))","caf86eab":"def cleaning(text):\n    text = str(text).lower() \n    tokens  = word_tokenize(text)\n    text = ' '.join(word for word in tokens if word not in STOPWORDS and word.isalpha()) \n    return text\n\ndef clean_url(url):\n    non_uselful = ['http', 'https', 'www', 'com', 'reddit']\n    add_ons = ['cms', 'comments', 'r', 'redd', 'google', 'amp', 'co', 'youtu', 'india', 'jpg', 'article', 'youtube', 'png', 'twitter']\n    non_uselful += add_ons\n    non_uselful = set(non_uselful)\n    delimiters = [':', '\/', '_', '-', '.']\n    pattern = '|'.join(map(re.escape, delimiters))\n    try:\n        url = re.split(pattern,url)\n    except:\n        return ''\n    else:\n        url = ' '.join(word for word in url if word not in STOPWORDS and word.isalpha() and word not in non_uselful) \n        #print(url)\n        return url\nwith_flair['title'] = with_flair['title'].apply(cleaning)\nwith_flair['selftext'] = with_flair['selftext'].apply(cleaning)\nwith_flair['url'] = with_flair['url'].apply(clean_url)\nwith_flair['comments'] = with_flair['comments'].apply(cleaning)\n\nwith_flair['content'] = with_flair['title'].str.cat(with_flair['selftext'], sep =\" \") \nwith_flair['content'] = with_flair['content'].str.cat(with_flair['url'], sep =\" \") \nwith_flair['content'] = with_flair['content'].str.cat(with_flair['comments'], sep =\" \") \nwith_flair['title_url']=with_flair['title'].str.cat(with_flair['url'],sep=\" \")\nwith_flair['title_url_self']=with_flair['title_url'].str.cat(with_flair['selftext'],sep=\" \")","3149a158":"df_col_len = int(with_flair['comments'].str.encode(encoding='utf-8').str.len().max())\ndf_col_len","b52481c4":"with_flair = with_flair[with_flair.content!='']\nwith_flair.reset_index(drop=True, inplace=True)\nwith_flair","1355fbc4":"with_flair['title'].apply(lambda x: len(x.split(' '))).sum()","1c4248f4":"with_flair = with_flair.sample(frac=0.5)","901f2638":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nX = with_flair.title\ny = with_flair.link_flair_text\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)","0ac29c0d":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\ncv=CountVectorizer(stop_words=stopwords.words(\"english\"), analyzer='word',max_features=1000,\n                token_pattern=r'\\b[^\\d\\W]+\\b')\nx=cv.fit_transform(with_flair.iloc[:,0])\n#len(cv.get_feature_names()),cv.get_feature_names()\n#cv.vocabulary_\ntf=TfidfTransformer()\ntfx=tf.fit_transform(x)","74d42fca":"idf=TfidfVectorizer(stop_words=stopwords.words(\"english\"), analyzer='word',max_features=1000,\n                    token_pattern=r'\\b[^\\d\\W]+\\b')\nxdf=idf.fit_transform(X_train)\nxdt=idf.transform(X_test)","98920744":"def naivebayes(X_train, X_test, y_train, y_test):\n    from sklearn.naive_bayes import MultinomialNB\n    from sklearn.metrics import classification_report\n    from sklearn.pipeline import Pipeline\n    from sklearn.feature_extraction.text import TfidfTransformer\n\n    nb = Pipeline([('vect', CountVectorizer()),\n               ('tfidf', TfidfTransformer()),\n               ('clf', MultinomialNB()),\n              ])\n    nb.fit(X_train, y_train)\n\n\n    from sklearn.metrics import classification_report\n    y_pred = nb.predict(X_test)\n    my_tags = list(set(with_flair.link_flair_text))\n    print('accuracy %s' % accuracy_score(y_pred, y_test))\n    print(classification_report(y_test, y_pred,target_names=my_tags))","f367cfbf":"def linearsvm(X_train, X_test, y_train, y_test):\n    from sklearn.linear_model import SGDClassifier\n    from sklearn.metrics import classification_report\n    sgd = Pipeline([('vect', CountVectorizer()),\n                ('tfidf', TfidfTransformer()),\n                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-4, random_state=42,max_iter=100, tol=None)),\n               ])\n    sgd.fit(X_train, y_train)\n\n\n    y_pred = sgd.predict(X_test)\n    my_tags = list(set(with_flair.link_flair_text))\n    print('accuracy %s' % accuracy_score(y_pred, y_test))\n    print(classification_report(y_test, y_pred,target_names=my_tags))","9b72cf1a":"def logisticreg(X_train, X_test, y_train, y_test):\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import classification_report\n\n    logreg = Pipeline([('vect', CountVectorizer()),\n                ('tfidf', TfidfTransformer()),\n                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n               ])\n    logreg.fit(X_train, y_train)\n\n    y_pred = logreg.predict(X_test)\n    my_tags = list(set(with_flair.link_flair_text))\n    print('accuracy %s' % accuracy_score(y_pred, y_test))\n    print(classification_report(y_test, y_pred,target_names=my_tags))","7120edfa":"\ndef randomforest(X_train, X_test, y_train, y_test):\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.metrics import classification_report\n    import seaborn as sns\n\n    ranfor = Pipeline([('vect', CountVectorizer()),\n                  ('tfidf', TfidfTransformer()),\n                  ('clf', RandomForestClassifier(n_estimators = 400, random_state = 42, max_depth=30)),\n                 ])\n\n    ranfor.fit(X_train, y_train)\n    y_pred = ranfor.predict(X_test)\n    from sklearn.externals import joblib \n#     joblib.dump(ranfor, 'joblib_model.pkl') \n#     f_joblib = joblib.load('filename.pkl')  \n#     f_joblib.predict(X_test) \n#     import pickle\n\n#     pkl_filename = \"final_model.pkl\"\n#     with open(pkl_filename, 'wb') as file:\n#         pickle.dump(ranfor, file)\n    my_tags = list(set(with_flair.link_flair_text))\n    \n\n    print('accuracy %s' % accuracy_score(y_pred, y_test))\n    print(classification_report(y_test, y_pred,target_names=my_tags))\n    \n    ","2f9004eb":"def mlpclassifier(X_train, X_test, y_train, y_test):\n    from sklearn.neural_network import MLPClassifier\n    from sklearn.metrics import classification_report\n    mlp = Pipeline([('vect', CountVectorizer()),\n                  ('tfidf', TfidfTransformer()),\n                  ('clf', MLPClassifier(hidden_layer_sizes=(30,30,30))),\n                 ])\n    mlp.fit(X_train, y_train)\n\n    y_pred = mlp.predict(X_test)\n    my_tags = list(set(with_flair.link_flair_text))\n    print('accuracy %s' % accuracy_score(y_pred, y_test))\n    print(classification_report(y_test, y_pred,target_names=my_tags))","c445c933":"def train_test(X,y):\n    from sklearn.metrics import confusion_matrix\n    from sklearn.model_selection import train_test_split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n    print(\"Results of Naive Bayes Classifier\")\n    naivebayes(X_train, X_test, y_train, y_test)\n    print(\"Results of Linear Support Vector Machine\")\n    linearsvm(X_train, X_test, y_train, y_test)\n    print(\"Results of Logistic Regression\")\n    logisticreg(X_train, X_test, y_train, y_test)\n    print(\"Results of Random Forest\")\n    randomforest(X_train, X_test, y_train, y_test)\n    print(\"Results of MLP Classifier\")\n    mlpclassifier(X_train, X_test, y_train, y_test)\n#     print(len(X_train))","be733809":"target= with_flair.link_flair_text\nc=with_flair.content\nt= with_flair.title\ntu=with_flair.title_url\ntus=with_flair.title_url_self\nprint(\"title as feature\")\ntrain_test(t,target)\nprint(\"tile+url as feature\")\ntrain_test(tu,target)\nprint(\"title+url+selftext as feature\")\ntrain_test(tus,target)\nprint(\"content as feature\")\ntrain_test(c,target)","01181de6":"import itertools\nimport os\nfrom sklearn.utils import shuffle\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder\nfrom sklearn.metrics import confusion_matrix\n\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.preprocessing import text, sequence\nfrom keras import utils\n\nwith_flair = shuffle(with_flair)\ntrain_size = int(len(with_flair) * 0.7)\ntrain_posts = with_flair['title_url_self'][:train_size]\ntrain_tags = with_flair['link_flair_text'][:train_size]\n\n\ntest_posts = with_flair['title_url_self'][train_size:]\ntest_tags = with_flair['link_flair_text'][train_size:]\n\nmax_words = 5000\ntokenize = text.Tokenizer(num_words=max_words, char_level=False)\ntokenize.fit_on_texts(train_posts) # only fit on train\n\nx_train = tokenize.texts_to_matrix(train_posts)\nx_test = tokenize.texts_to_matrix(test_posts)\n\nencoder = LabelEncoder()\nencoder.fit(train_tags)\ny_train = encoder.transform(train_tags)\ny_test = encoder.transform(test_tags)\n\nnum_classes = np.max(y_train) + 1\ny_train = utils.to_categorical(y_train, num_classes)\ny_test = utils.to_categorical(y_test, num_classes)\n\nbatch_size = 32\nepochs = 30\n\n# Build the model\nmodel = Sequential()\nmodel.add(Dense(512, input_shape=(max_words,)))\nmodel.add(Activation('relu'))\nmodel.add(Dense(128))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n              \nhistory = model.fit(x_train, y_train,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    verbose=1,\n                    validation_split=0.3)\nscore = model.evaluate(x_test, y_test,\n                       batch_size=batch_size, verbose=1)\nprint('Test accuracy:', score[1])","aad5c308":"submission = pd.DataFrame({\n        \"title_url_self\": with_flair[\"title_url_self\"],\n        \"link_flair_text\": with_flair[\"link_flair_text\"]\n    })\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.head()","b26431c8":"### Building a Detector","bee718fb":"### Linear SVM","5d546300":"### Naive Bayes ","94fecd66":"##### The above collected data contains posts from r\/india , 1000 posts per week of all flairs collected for a year(52 weeks) starting from the day of collection. All the post collected do not have flair associated with them, hence they need to be removed. This dataset doesn't contains comments for each post, PRAW will be used for obtaining the comments for the posts","464ead7a":"#### Random Forest showed the best testing accuracy of 78 when trained on the combination of content which is title + comments + Url + self_text feature.The dataset is split into 70% train and 30% test data using train-test-split of scikit-learn","f6513b10":"#### Exploratory Data Analysis(EDA)\n##### The above datset has a lot of features , so only key features below have been used and these features are considered with valid reasons in terms of model training.\n* title\n* url\n* selftext\n* comments\n* link_flair_text","d0d3c16b":"### Random forest","66971bba":"### If you made it this far, thank you for your attention!!","cf6cf730":"### Logistic Regression","2c227523":"### Using Different Combination of Features","b9d60003":"### Let's Train,Test all the models!!","76e4ce83":"# Reddit Flair Detector\n#### A while back, I have made a web app for detecting flairs in reddit posts and here is the notebook consisting of working of several models used to detect flairs.Hope this is useful for all! Feedback is most welcome:)The app can be used here https:\/\/redditflaird.herokuapp.com\/. \n## Table of Contents\n\n* Data Collection\n* Features Used\n* Flair Classification\n* References\n\nThe task is to do data aquisition of reddit posts from the r\/india subreddit, perform classification of the posts into 12 different flairs and later deployed the best model as a web service by utilising Machine Learning and NLP algorithms. \n\n### Installation\nNote: The following installation has been tested on Ubuntu 16.04.\nThis project requires Python 3 and the following Python libraries installed(plus others):\n\n* praw\n* scikit-learn\n* nltk\n* Flask\n* pandas\n* numpy\n* gunicorn\n* Matplotlib\n* keras\n\n### Data Collection\n\n1. Fetched 52,000 (1,000 per week) for the last year, India subreddit data for each of the 12 flairs using Pushshift API.\n2. The data includes title, comments, selftext, url, number of comments, score, link_flair_text.\n3. Many post do not have a flair associated with them, they will be removed from the dataset.\n4. Unfortunately the posts have not been tagged with their comments in the dataset. To extract this information, PRAW is used.\n\n## Features Used\n\n5 types of features are considered for the the given task:\n\n\na) ```title```\n\nb) ```comments```\n\nc) ```url```\n\nd) ```selftext```\n\n\n## Flair-Classification\n\nThe following ML algorithms are used to classify:\n\n\na) ```Naive-Bayes```\n\nb) ```Linear Support Vector Machine```\n\nc) ```Logistic Regression```\n\nd) ```Random Forest```\n\ne) ```MLP```\n\nf) ```BOW with Keras```\n\n### Accuracy of the best model\n\n**Random Forest** was selected as the final model with **78%** accuracy using a feature combination of ```title +url +comments +selftext```\n\n## Deploying as Web Service\n\nThe best model - Random Forest is deployed as a web app. Check the live demo [here](https:\/\/redditflaird.herokuapp.com\/). The app was deployed using a free service like Heroku. \n\n## References\n\n* https:\/\/www.storybench.org\/how-to-scrape-reddit-with-python\/\n* https:\/\/towardsdatascience.com\/multi-class-text-classification-model-comparison-and-selection-5eb066197568\n* https:\/\/towardsdatascience.com\/designing-a-machine-learning-model-and-deploying-it-using-flask-on-heroku-9558ce6bde7b\n","95621e31":"### BOW with Keras","2473b2fa":"### MLP Classifier","955ccf9f":"#### Unigrams for top N words before removing stop words"}}