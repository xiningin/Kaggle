{"cell_type":{"a987ca05":"code","a1c2df1e":"code","7c532d35":"code","cfc28016":"code","e28f794b":"code","1b9b5cc2":"code","8a3801b3":"code","7144f08e":"code","0023e022":"code","66af0b7c":"code","58208de5":"code","baacabf7":"code","c0a6dce7":"code","8c971e86":"code","adddfd9c":"code","09970d16":"code","d7e3309f":"code","5f38365d":"code","579df25c":"code","71462b1c":"code","74ab0c84":"code","6396c1b9":"code","883e8c87":"code","c92ded3b":"code","60a728ed":"code","359ebaba":"code","4691a467":"code","80c46b8d":"code","8f7d5d51":"code","899ec274":"code","f447362e":"code","2e984538":"code","ab856d32":"code","33cf0b34":"code","d00ab66b":"code","b4d6ebcf":"code","62bd748a":"code","2abe7143":"code","0f7aeecd":"code","2f806671":"code","6ee73212":"code","09f7f7ea":"code","a40d92a4":"code","29ad4eec":"code","4c25b31e":"code","f3206d94":"code","d389ff3b":"code","7819a8e9":"code","603161ac":"code","7d48a57d":"code","1e9985a4":"code","acc41be6":"code","6498f538":"code","5379019c":"code","234a4f01":"code","439fe45e":"code","862a71ab":"markdown","617d9f3a":"markdown","86f46491":"markdown","84a34765":"markdown","b011e410":"markdown","c58006a0":"markdown","3ff7a8e3":"markdown","045a042e":"markdown","ef3d900a":"markdown","c72ffc70":"markdown","d8439679":"markdown","0df6143a":"markdown","3fdd204b":"markdown","e93e1745":"markdown","a7a3c90e":"markdown","74466c04":"markdown","cc1dc247":"markdown","6656032a":"markdown","ff55f79c":"markdown","a8ac54bf":"markdown","3f2edd45":"markdown","77362e11":"markdown","4cd09d23":"markdown","e9645edf":"markdown","5de4e35e":"markdown","befef24f":"markdown","ba98bd11":"markdown","0062a8fd":"markdown","598d43ac":"markdown","1e80601b":"markdown","27b44ab5":"markdown","1a570122":"markdown","b3a4c377":"markdown","2072f129":"markdown","a40917c5":"markdown"},"source":{"a987ca05":"# Basic Libraries\nimport numpy as np\nimport pandas as pd\n\n# Visualizations Libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom plotly.offline import iplot\nfrom plotly.subplots import make_subplots\nimport plotly.offline as py\nimport plotly.figure_factory as ff\nimport cufflinks as cf","a1c2df1e":"data = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")\ndf = data.copy()\ndf.sample(7)","7c532d35":"df.shape","cfc28016":"df.info()","e28f794b":"df.describe().T","1b9b5cc2":"df.duplicated().sum()","8a3801b3":"df.drop_duplicates(inplace=True)","7144f08e":"# The classes are heavily skewed we need to solve this issue later.\nplt.figure(figsize=(8,8))\n\nexplode = [0,0.001]\nplt.pie(df['Class'].value_counts(), explode=explode,autopct='%1.2f%%', shadow=True,startangle=100)\nplt.legend(labels=['0','1'])\nplt.title('Class Distribution');\n\nprint('No Frauds', round(df['Class'].value_counts()[0]\/len(df) * 100,2), '% of the dataset')\nprint('Frauds', round(df['Class'].value_counts()[1]\/len(df) * 100,2), '% of the dataset')","0023e022":"df['Class'].value_counts()","66af0b7c":"#legit\ndf[df.Class == 0].Amount.describe()","58208de5":"#fraud\ndf[df.Class == 1].Amount.describe()","baacabf7":"plt.figure(figsize = (15,6),dpi = 100)\nsns.barplot(x = df.corr()[\"Class\"].drop(index=\"Class\").sort_values(ascending=False).index,\n            y = df.corr()[\"Class\"].drop(index=\"Class\").sort_values(ascending=False).values)\nplt.xticks(rotation = 45);","c0a6dce7":"f, ax = plt.subplots(2,2, figsize=(20,12), dpi=200, sharey=True)\nsns.scatterplot(data = df, x ='Amount', y = 'V11', hue = 'Class',  ax=ax[0,0], palette='PRGn')\nsns.scatterplot(data = df, x ='Amount', y = 'V4', hue =  'Class', ax=ax[0,1], palette='PRGn')\nsns.scatterplot(data = df, x ='Amount', y = 'V2', hue = 'Class',  ax=ax[1,0], palette='PRGn')\nsns.scatterplot(data = df, x ='Amount', y = 'V19', hue = 'Class',  ax=ax[1,1], palette='PRGn');","8c971e86":"f, ax = plt.subplots(2,2, figsize=(20,12), dpi=200, sharey=True)\nsns.scatterplot(data = df, x ='Amount', y = 'V17', hue = 'Class',  ax=ax[0,0], palette='PRGn')\nsns.scatterplot(data = df, x ='Amount', y = 'V14', hue =  'Class', ax=ax[0,1], palette='PRGn')\nsns.scatterplot(data = df, x ='Amount', y = 'V12', hue = 'Class',  ax=ax[1,0], palette='PRGn')\nsns.scatterplot(data = df, x ='Amount', y = 'V10', hue = 'Class',  ax=ax[1,1], palette='PRGn');","adddfd9c":"df.Class.value_counts()","09970d16":"df[(df[\"V17\"] < 0.4 ) & (df[\"Class\"] == 1)]","d7e3309f":"df[(df[\"V14\"]< 0.5 ) & (df[\"Class\"] == 1)]","5f38365d":"df[(df[\"V12\"]< 0.1 ) & (df[\"Class\"] == 1)]","579df25c":"df[(df[\"V10\"]< 0.2 ) & (df[\"Class\"] == 1)]","71462b1c":"df_filter = df[(df[\"Class\"] == 1) | (df[\"V17\"]< 0.4) & (df[\"V14\"]< 0.5) & (df[\"V12\"] < 0.1) & (df[\"V10\"]<=0.2)]\ndf_filter","74ab0c84":"df_filter.Class.value_counts()","6396c1b9":"df_filter.Class.value_counts(normalize=True)","883e8c87":"df.isnull().any().sum()","c92ded3b":"def detect_outliers(data:pd.DataFrame, col_name:str, p=1.5) ->int:\n    ''' \n    this function detects outliers based on 3 time IQR and\n    returns the number of lower and uper limit and number of outliers respectively\n    '''\n    first_quartile = np.percentile(np.array(data[col_name].tolist()), 25)\n    third_quartile = np.percentile(np.array(data[col_name].tolist()), 75)\n    IQR = third_quartile - first_quartile\n                      \n    upper_limit = third_quartile+(p*IQR)\n    lower_limit = first_quartile-(p*IQR)\n    outlier_count = 0\n                      \n    for value in data[col_name].tolist():\n        if (value < lower_limit) | (value > upper_limit):\n            outlier_count +=1\n    return lower_limit, upper_limit, outlier_count","60a728ed":"features = ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\niqr=2\nprint(f\"Number of Outliers for {iqr}*IQR after Logarithmed\\n\")\n\ntotal=0\nfor col in features:\n    if detect_outliers(df_filter, col)[2] > 0:\n        outliers=detect_outliers(df, col, iqr)[2]\n        total+=outliers\n        print(\"{} outliers in '{}'\".format(outliers,col))\nprint(\"\\n{} OUTLIERS TOTALLY\".format(total))","359ebaba":"plt.figure(figsize=(20,16))\nsns.boxplot(data = df_filter.loc[:,(df_filter.columns != 'Time') & (df_filter.columns != 'Amount')& (df_filter.columns != 'Class')])\nplt.xticks(rotation=45);","4691a467":"df_filter[df_filter.Class == 0].Amount.describe()","80c46b8d":"df_filter[df_filter.Class == 1].Amount.describe()","8f7d5d51":"df_filter[df_filter.Class == 0].Time.describe()","899ec274":"df_filter[df_filter.Class == 1].Time.describe()","f447362e":"# Data Pre-processing Libraries\nfrom sklearn.model_selection import cross_validate, cross_val_score,GridSearchCV\nfrom sklearn.model_selection import KFold, cross_val_predict,StratifiedKFold\n\nfrom imblearn.over_sampling import SMOTE\n\n# Modelling Libraries\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\n\n# Evaluation & CV Libraries\nfrom sklearn.metrics import precision_score,accuracy_score,mean_absolute_error,mean_squared_error,r2_score,confusion_matrix\nfrom sklearn.metrics import classification_report, plot_confusion_matrix,plot_roc_curve,roc_auc_score,f1_score,recall_score\nfrom sklearn.metrics import plot_precision_recall_curve\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nimport optuna\noptuna.logging.set_verbosity(0)\nplt.rcParams[\"figure.figsize\"] = (10,6)\npd.set_option(\"max_columns\",100)\npd.set_option(\"max_rows\",900)\npd.set_option(\"max_colwidth\",200)\nimport warnings\nwarnings.filterwarnings(\"ignore\")","2e984538":"X = df.drop(['Class'],axis=1)\ny = df['Class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test  = scaler.transform(X_test)\n\n\nmodels = []\nmodels.append((\"XGB\",XGBClassifier(random_state = 42)))\nmodels.append((\"LightGBM\", LGBMClassifier(random_state = 42)))\nmodels.append(('GBC', GradientBoostingClassifier(random_state = 42)))\nmodels.append(('ADA', AdaBoostClassifier(random_state = 42)))\nmodels.append(('RF', RandomForestClassifier(random_state = 42)))\nmodels.append(('LR', LogisticRegression(random_state = 42)))\nmodels.append(('SVM', SVC(gamma='auto', random_state = 42)))\n\nname_list = []\naccuracy_scores = []\naccuracy_scores_train = []\nrecall_scores = []\nrecall_scores_train = []\nf1_scores = []\nf1_scores_train = []\n\n\n\nfor name,model in models:\n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    y_pred_train = model.predict(X_train)\n    accuracy_scores.append(accuracy_score(y_test,y_pred))\n    accuracy_scores_train.append(accuracy_score(y_train,y_pred_train))\n    recall_scores.append(recall_score(y_test,y_pred))\n    recall_scores_train.append(recall_score(y_train,y_pred_train))\n    f1_scores.append(f1_score(y_test,y_pred))\n    f1_scores_train.append(f1_score(y_train,y_pred_train))\n    name_list.append(name)\n    result = {\"Model\":name_list,\"Accuracy Score Test\":accuracy_scores, \"Accuracy Score Train\": accuracy_scores_train,\\\n              \"Recall Score Test\":recall_scores, \"Recall Score Train\": recall_scores_train ,\\\n              \"F1 Score Test\" :f1_scores,\"F1 Score train\" : f1_scores_train}    \n     \ndataframe = pd.DataFrame(result).sort_values(by=\"F1 Score Test\",ascending=False)\ndataframe.reset_index(drop=True)","ab856d32":"X = df_filter.drop(['Class'],axis=1)\ny = df_filter['Class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test  = scaler.transform(X_test)\n\n\nmodels = []\nmodels.append((\"XGB\",XGBClassifier(random_state = 42)))\nmodels.append((\"LightGBM\", LGBMClassifier(random_state = 42)))\nmodels.append(('GBC', GradientBoostingClassifier(random_state = 42)))\nmodels.append(('ADA', AdaBoostClassifier(random_state = 42)))\nmodels.append(('RF', RandomForestClassifier(random_state = 42)))\nmodels.append(('LR', LogisticRegression(random_state = 42)))\nmodels.append(('SVM', SVC(gamma='auto', random_state = 42)))\n\nname_list = []\naccuracy_scores = []\naccuracy_scores_train = []\nrecall_scores = []\nrecall_scores_train = []\nf1_scores = []\nf1_scores_train = []\nprecision_scores = []\nprecision_scores_train = []\n\n\nfor name,model in models:\n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    y_pred_train = model.predict(X_train)\n    accuracy_scores.append(accuracy_score(y_test,y_pred))\n    accuracy_scores_train.append(accuracy_score(y_train,y_pred_train))\n    recall_scores.append(recall_score(y_test,y_pred))\n    recall_scores_train.append(recall_score(y_train,y_pred_train))\n    f1_scores.append(f1_score(y_test,y_pred))\n    f1_scores_train.append(f1_score(y_train,y_pred_train))\n    precision_scores.append(precision_score(y_test,y_pred))    \n    precision_scores_train.append(precision_score(y_train,y_pred_train))\n    name_list.append(name)\n    result = {\"Model\":name_list,\"Accuracy Score Test\":accuracy_scores, \"Accuracy Score Train\": accuracy_scores_train,\\\n              \"Recall Score Test\":recall_scores, \"Recall Score Train\": recall_scores_train ,\\\n              \"F1 Score Test\" :f1_scores,\"F1 Score Train\" : f1_scores_train,\\\n             \"Precision Score Test\":precision_scores, \"Precision Score Train\":precision_scores_train}    \n     \ndataframe = pd.DataFrame(result).sort_values(by=\"F1 Score Test\",ascending=False)\ndataframe.reset_index(drop=True)","33cf0b34":"from sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline","d00ab66b":"scaler = StandardScaler()\n\nX = df_filter.drop(['Class'],axis=1)\ny = df_filter['Class']\n\nover = SMOTE(sampling_strategy=.2)\nunder = RandomUnderSampler(sampling_strategy=1)\nsteps=[('o', over), ('u', under)]\npipeline = Pipeline(steps=steps)\n\nX, y = pipeline.fit_resample(X, y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify = y)\nX_train = scaler.fit_transform(X_train)\nX_test  = scaler.transform(X_test)\n\nmodels = []\nmodels.append((\"XGB\",XGBClassifier(random_state = 42)))\nmodels.append((\"LightGBM\", LGBMClassifier(random_state = 42)))\nmodels.append(('GBC', GradientBoostingClassifier(random_state = 42)))\nmodels.append(('ADA', AdaBoostClassifier(random_state = 42)))\nmodels.append(('RF', RandomForestClassifier(random_state = 42)))\nmodels.append(('LR', LogisticRegression(random_state = 42)))\nmodels.append(('SVM', SVC(gamma='auto', random_state = 42)))\n\nname_list = []\naccuracy_scores = []\naccuracy_scores_train = []\nrecall_scores = []\nrecall_scores_train = []\nf1_scores = []\nf1_scores_train = []\nprecision_scores = []\nprecision_scores_train = []\n\nfor name,model in models:\n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    y_pred_train = model.predict(X_train)\n    accuracy_scores.append(accuracy_score(y_test,y_pred))\n    accuracy_scores_train.append(accuracy_score(y_train,y_pred_train))\n    recall_scores.append(recall_score(y_test,y_pred))\n    recall_scores_train.append(recall_score(y_train,y_pred_train))\n    f1_scores.append(f1_score(y_test,y_pred))\n    f1_scores_train.append(f1_score(y_train,y_pred_train))\n    precision_scores.append(precision_score(y_test,y_pred))    \n    precision_scores_train.append(precision_score(y_train,y_pred_train))\n    name_list.append(name)\n    result = {\"Model\":name_list,\"Accuracy Score Test\":accuracy_scores, \"Accuracy Score Train\": accuracy_scores_train,\\\n              \"Recall Score Test\":recall_scores, \"Recall Score Train\": recall_scores_train ,\\\n              \"F1 Score Test\" :f1_scores,\"F1 Score Train\" : f1_scores_train,\\\n             \"Precision Score Test\":precision_scores, \"Precision Score Train\":precision_scores_train}    \n     \ndataframe = pd.DataFrame(result).sort_values(by=\"F1 Score Test\",ascending=False)\ndataframe.reset_index(drop=True)","b4d6ebcf":"lgbmc_model= LGBMClassifier(random_state=42)\nlgbmc_model.fit(X_train, y_train)\ny_pred = lgbmc_model.predict(X_test)\n\nprint(\"\\033[1m---------------------Test Scores---------------------\\033[0m\")\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test,y_pred))\n\nprint(\"\\033[1m---------------------Train Scores---------------------\\033[0m\")\ny_pred_train = lgbmc_model.predict(X_train)\nprint(confusion_matrix(y_train, y_pred_train))\nprint(classification_report(y_train, y_pred_train))\n\nplot_confusion_matrix(lgbmc_model, X_test, y_test,cmap=\"BuPu\");","62bd748a":"plot_precision_recall_curve(lgbmc_model, X_test, y_test);","2abe7143":"plot_roc_curve(lgbmc_model,X_test,y_test);","0f7aeecd":"def objective(trial):\n    X= df_filter.drop('Class', axis=1)\n    y= df_filter['Class']\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n    param = {\"n_estimators\": trial.suggest_int(\"n_estimators\",100, 600),\n        \"max_depth\": trial.suggest_int(\"max_depth\",2, 16),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01 , 0.1),\n        \"subsample\": trial.suggest_float(\"subsample\",0.2, 1),\n    }\n\n    lgbmc_model_optuna = LGBMClassifier(**param,random_state=42)\n\n    lgbmc_model_optuna.fit(X_train, y_train)\n\n    preds = lgbmc_model_optuna.predict(X_test)\n    pred_labels = np.rint(preds)\n    recall = recall_score(y_test, pred_labels)\n    return recall\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=100, timeout=600)\n\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: {}\".format(trial.value))\n\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))","2f806671":"model = LGBMClassifier(n_estimators = 189, max_depth = 13, learning_rate = 0.058037435219451684,subsample = 0.8422194436234953)","6ee73212":"X = df_filter.drop(['Class'],axis=1)\ny = df_filter['Class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42,stratify = y)\n\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nprint(\"\\033[1m---------------------Test Scores---------------------\\033[0m\")\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test,y_pred))\n\nprint(\"\\033[1m---------------------Train Scores---------------------\\033[0m\")\ny_pred_train = model.predict(X_train)\nprint(confusion_matrix(y_train, y_pred_train))\nprint(classification_report(y_train, y_pred_train))\n\nplot_confusion_matrix(model, X_test, y_test,cmap=\"BuPu\");","09f7f7ea":"feature_importance = model.feature_importances_\nfeatures = df_filter.drop(columns=\"Class\").columns\nfi={'features':features,'feature_importance':feature_importance}\ndf_fi = pd.DataFrame(fi)\ndf_fi.sort_values(by=['feature_importance'], ascending=True,inplace=True)\nfig = px.bar(df_fi, x='feature_importance', y='features',title=\"LGBMC Feature Importance\",height=500)\nfig.show()","a40d92a4":"df_filter_feature = df_filter[[\"V14\",\"V4\",\"Amount\",\"V17\",\"V12\",\"V11\"]]\ndf_filter_feature[\"Class\"] = df.Class\nX = df_filter_feature.drop(columns=\"Class\")\ny = df_filter_feature.Class\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42,stratify = y)\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nprint(\"\\033[1m---------------------Test Scores---------------------\\033[0m\")\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test,y_pred))\n\nprint(\"\\033[1m---------------------Train Scores---------------------\\033[0m\")\ny_pred_train = model.predict(X_train)\nprint(confusion_matrix(y_train, y_pred_train))\nprint(classification_report(y_train, y_pred_train))\n\nplot_confusion_matrix(model, X_test, y_test,cmap=\"BuPu\");","29ad4eec":"over = SMOTE(sampling_strategy=.2)\nunder = RandomUnderSampler(sampling_strategy=1)\nsteps=[('o', over), ('u', under)]\npipeline = Pipeline(steps=steps)\nX, y = pipeline.fit_resample(X, y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42,stratify = y)\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nprint(\"\\033[1m---------------------Test Scores---------------------\\033[0m\")\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test,y_pred))\n\nprint(\"\\033[1m---------------------Train Scores---------------------\\033[0m\")\ny_pred_train = model.predict(X_train)\nprint(confusion_matrix(y_train, y_pred_train))\nprint(classification_report(y_train, y_pred_train))\n\nplot_confusion_matrix(model, X_test, y_test,cmap=\"BuPu\");","4c25b31e":"df_final = df[[\"V14\",\"V12\",\"V4\",\"V10\",\"Amount\",\"V17\"]]\ndf_final[\"Class\"] = df.Class\nX = df_final.drop(columns=\"Class\")\ny = df_final.Class\n\nover = SMOTE(sampling_strategy=.2)\nunder = RandomUnderSampler(sampling_strategy=1)\nsteps=[('o', over), ('u', under)]\npipeline = Pipeline(steps=steps)\nX, y = pipeline.fit_resample(X, y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42,stratify = y)\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nprint(\"\\033[1m---------------------Test Scores---------------------\\033[0m\")\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test,y_pred))\n\nprint(\"\\033[1m---------------------Train Scores---------------------\\033[0m\")\ny_pred_train = model.predict(X_train)\nprint(confusion_matrix(y_train, y_pred_train))\nprint(classification_report(y_train, y_pred_train))\n\nplot_confusion_matrix(model, X_test, y_test,cmap=\"BuPu\");","f3206d94":"final_scaler = StandardScaler()\nfinal_scaler.fit(X)","d389ff3b":"model.fit(X,y)","7819a8e9":"import pickle\npickle.dump(model, open(\"model.pkl\", \"wb\"))\npickle.dump(final_scaler, open('scaler.pkl', 'wb'))","603161ac":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom keras.wrappers.scikit_learn import KerasClassifier","7d48a57d":"model = Sequential([\n    Dense(units=20, input_dim = X_train.shape[1], activation='relu'),\n    Dense(units=24,activation='relu'),\n    Dropout(0.5),\n    Dense(units=20,activation='relu'),\n    Dense(units=24,activation='relu'),\n    Dense(1, activation='sigmoid')])\nmodel.summary()","1e9985a4":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","acc41be6":"model.fit(x = X_train, y = y_train, \n          batch_size = 30, epochs = 10)","6498f538":"y_pred = model.predict(X_test)\n\nprint(confusion_matrix(y_test, y_pred.round()))\nprint(\"-------------------------------------------------------\")\nprint(classification_report(y_test, y_pred.round()))","5379019c":"model = Sequential()\n\nmodel.add(Dense(30, activation = \"relu\"))\nmodel.add(Dense(15, activation = \"relu\"))\nmodel.add(Dense(1, activation = \"sigmoid\"))\n\nmodel.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\nearly_stop = EarlyStopping(monitor = \"val_loss\", mode = \"auto\", verbose = 1, patience = 10)\nmodel.fit(x = X_train, y = y_train, validation_split = 0.1, batch_size = 32, epochs = 500, verbose = 1,callbacks = [early_stop])","234a4f01":"loss_df = pd.DataFrame(model.history.history)\nloss_df.plot()","439fe45e":"y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\ny_pred_train = (model.predict(X_train) > 0.5).astype(\"int32\")\n#y_pred = model.predict_classes(X_test)\nprint(\"\\033[1m---------------------Test Scores---------------------\\033[0m\")\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\nprint(\"\\033[1m---------------------Train Scores---------------------\\033[0m\")\nprint(confusion_matrix(y_train, y_pred_train))\nprint(classification_report(y_train, y_pred_train))","862a71ab":"### Neural Network\n\nClassification with Deep Learning algorithm. \n\nNeural networks are a series of algorithms that mimic the operations of a human brain to recognize relationships between vast amounts of data. They are used in a variety of applications in financial services, from forecasting and marketing research to fraud detection and risk assessment.\n\nA neural network contains layers of interconnected nodes. Each node is a perceptron and is similar to a multiple linear regression. The perceptron feeds the signal produced by a multiple linear regression into an activation function that may be nonlinear.\n\nIn a multi-layered perceptron (MLP), perceptrons are arranged in interconnected layers. The input layer collects input patterns. The output layer has classifications or output signals to which input patterns may map. \n\nHidden layers fine-tune the input weightings until the neural network\u2019s margin of error is minimal. It is hypothesized that hidden layers extrapolate salient features in the input data that have predictive power regarding the outputs.","617d9f3a":"- ## With Smote and Scale","86f46491":"#### Distribution of 1,0 in Class","84a34765":"## Comparison of the models with scaling in df_filter","b011e410":"## Train and Test results for LightGBM Classifier model","c58006a0":"---\n---\n","3ff7a8e3":"## 2. Data Preprocessing","045a042e":"# Hi!","ef3d900a":"## Default results (without Cross-validation) without applying smote to selected columns.","c72ffc70":"## Comparison of models with scale (No Smote) in Orjinal Data ","d8439679":"# Data\nThe datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where it has **492 frauds** out of **284,807** transactions. The dataset is **highly unbalanced**, the positive class (frauds) account for 0.172% of all transactions. <br>\n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, \u2026 V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are \u2018Time\u2019 and \u2018Amount\u2019. Feature \u2018Time\u2019 contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature \u2018Amount\u2019 is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature \u2018Class\u2019 is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n\n**Feature Information:**\n\n**Time**: This feature is contains the seconds elapsed between each transaction and the first transaction in the dataset. \n\n**Amount**:  This feature is the transaction Amount, can be used for example-dependant cost-senstive learning. \n\n**Class**: This feature is the target variable and it takes value 1 in case of fraud and 0 otherwise.\n\n---\n\nThe aim of this project is to predict whether a credit card transaction is fraudulent.\nFirst of all, I analyze and recognize the data well in order to draw my roadmap and choose the correct arguments I will use. Accordingly, I examine the frequency distributions of variables. I observe variable correlations and want to explore multicollinearity. I show the distribution of the target variable's classes over other variables. ","0df6143a":"***v. Prediction and Model Evaluating***","3fdd204b":"### Import Modules, Load Data & Data Review","e93e1745":"### We set a threshold value for highly correlated columns","a7a3c90e":"## Class Analysis against other Features","74466c04":"***iv. Fit Model***","cc1dc247":"**Overfitting !!!**","6656032a":"### Exploring Highly Correlated Features","ff55f79c":"#### Corr of other features with Class","a8ac54bf":"***i. Import Libraries***","3f2edd45":"- Outliers","77362e11":"### Exploratory Data Analysis","4cd09d23":"## Feature Importance (df_filter)","e9645edf":"The implemented models are ***Machine Learning*** algorithms and ***SMOTE*** technique. Also visualized performances of the models using ***Seaborn, Matplotlib*** and ***Plotly*** in a variety of ways.","5de4e35e":"## Generated new data to try algorithms.","befef24f":"##### Dataset is highly imbalanced","ba98bd11":"- ## Results by applying smote","0062a8fd":"***ii. Define Model***","598d43ac":"***iii. Compile Model***","1e80601b":"- ## Model optimization with Optuna","27b44ab5":"### Data Cleaning\n- Missing Values and Outliers","1a570122":"## Determining the columns selected with the original data and seeing the results","b3a4c377":"## Save and Export the Model as .pkl","2072f129":"---\n---\n","a40917c5":"## 1. Exploratory Data Analysis & Data Cleaning"}}