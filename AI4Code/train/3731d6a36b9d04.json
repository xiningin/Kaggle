{"cell_type":{"c8cc69b7":"code","a8fa9408":"code","3794c225":"code","3162f4a6":"code","b8e389d6":"code","732dc7ca":"code","358f9b09":"code","3be88b51":"code","ff855abe":"code","6f1f0e3d":"code","13624702":"code","f310f522":"code","56a2f4c7":"code","276500ef":"code","1d273419":"code","824490e0":"code","e41e7ce3":"code","0aa71ab5":"code","fd15c51d":"code","8c99c5c0":"code","55ce41db":"code","23bdedce":"code","239368d7":"code","98799ad5":"code","36d0dc2f":"code","e6b762f8":"code","67cd22ef":"code","ab2953e7":"code","4b4a7256":"code","58929d6c":"code","51786237":"code","5156631f":"code","c66c1b74":"code","d60fa9ae":"code","a82cc078":"code","1626d680":"code","2ae19ec1":"code","8e2c9590":"code","a859419b":"code","b0ff3050":"code","d93b902e":"code","a481b16f":"code","2c359068":"code","edc89261":"code","3a3975c9":"code","390133e7":"code","b4478358":"code","0f8e0e66":"code","823a007a":"code","6e3fd8cb":"code","d06a257e":"code","5ecb1715":"code","cff6e1a4":"code","3acd885f":"code","ce910ea1":"code","dbfeae36":"code","03e5fd83":"code","67053a05":"code","e20aa659":"code","b2b120c1":"code","99adb481":"code","a3db9eea":"code","114c033e":"code","cff2ca9b":"code","463d9b48":"code","3448dc86":"code","1976f223":"code","caf91c43":"code","a01f85e9":"code","61b3e66a":"code","76d3845e":"code","0f1d792f":"code","1a5b2a2f":"code","413154e6":"markdown","c5964730":"markdown","9d7cb762":"markdown","b4b684f8":"markdown","542830f9":"markdown","64d0c2c5":"markdown","202ff2d2":"markdown","e7e2d3a5":"markdown","8b0eeb1b":"markdown","ad4f08f4":"markdown","2c70e219":"markdown","7b7369ad":"markdown","dd91843b":"markdown","bcb8ed96":"markdown","d616782e":"markdown","8641543e":"markdown","7cc3da5e":"markdown","9ed49c2f":"markdown","540d3336":"markdown","e2e967cc":"markdown","f3e99a3f":"markdown","0463dfe1":"markdown","cad8f738":"markdown","0b790603":"markdown","d03977e0":"markdown","61f954d6":"markdown","a3972961":"markdown"},"source":{"c8cc69b7":"#importing the libraries\nimport pandas as pd\nimport numpy as np \nimport seaborn as sns \nimport matplotlib.pyplot as plt\n\nfrom IPython.display import display","a8fa9408":"pd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)","3794c225":"credit_df = pd.read_csv('..\/input\/german-credit-data-with-risk\/german_credit_data.csv')\ncredit_df = credit_df.iloc[:, 1:]\ncredit_df.head()","3162f4a6":"credit_df.info()","b8e389d6":"credit_df.describe()","732dc7ca":"print (credit_df.shape)","358f9b09":"# checking for number of unique values in each column\ncredit_df.nunique()","3be88b51":"credit_df_numeric = credit_df.select_dtypes(include=['int64'])\ncredit_df_numeric.head()","ff855abe":"credit_df_categorical = credit_df.select_dtypes(include=['object'])\ncredit_df_categorical.head()","6f1f0e3d":"credit_df['Job'] = credit_df['Job'].astype(object)","13624702":"credit_df_numeric = credit_df.select_dtypes(include=['int64'])\ncredit_df_numeric.head()","f310f522":"credit_df_categorical = credit_df.select_dtypes(include=['object'])\ndel credit_df_categorical['Risk']\ncredit_df_categorical.head()","56a2f4c7":"credit_categorical_col = credit_df_categorical.columns\ncredit_numeric_col = credit_df_numeric.columns","276500ef":"print (credit_categorical_col)\nprint (credit_numeric_col)","1d273419":"def showLabels(ax, d=None):\n    plt.margins(0.2, 0.2)\n    rects = ax.patches\n    i = 0\n    locs, labels = plt.xticks() \n    counts = {}\n    if not d is None:\n        for key, value in d.items():\n            counts[str(key)] = value\n\n    # For each bar: Place a label\n    for rect in rects:\n        # Get X and Y placement of label from rect.\n        y_value = rect.get_height()\n        x_value = rect.get_x() + rect.get_width() \/ 2\n\n        # Number of points between bar and label. Change to your liking.\n        space = 5\n        # Vertical alignment for positive values\n        va = 'bottom'\n\n        # If value of bar is negative: Place label below bar\n        if y_value < 0:\n            # Invert space to place label below\n            space *= -1\n            # Vertically align label at top\n            va = 'top'\n\n        # Use Y value as label and format number with one decimal place\n        if d is None:\n            label = \"{:.1f}%\".format(y_value)\n        else:\n            try:\n                label = \"{:.1f}%\".format(y_value) + '\\n' + str(counts[str(labels[i].get_text())])\n            except:\n                label = \"{:.1f}%\".format(y_value)\n        \n        i = i+1\n\n        # Create annotation\n        plt.annotate(\n            label,                      # Use `label` as label\n            (x_value, y_value),         # Place label at end of the bar\n            xytext=(0, space),          # Vertically shift label by `space`\n            textcoords=\"offset points\", # Interpret `xytext` as offset in points\n            ha='center',                # Horizontally center label\n            va=va)                      # Vertically align label differently for\n                                        # positive and negative values.","824490e0":"def plot_distribution(dataframe, col):\n    plt.figure(figsize=(15,5))\n    plt.subplot(1, 2, 1)\n    ax = sns.histplot(dataframe[col])\n    plt.subplot(1, 2, 2)\n    sns.boxplot(x=dataframe[col], y=dataframe['Risk'], data=dataframe)\n    plt.show()","e41e7ce3":"def plot_percentages(dataframe, by, sortbyindex=False):\n    plt.subplot(1, 2, 1)\n    values = (credit_df[by].value_counts(normalize=True)*100)\n    if sortbyindex:\n        values = values.sort_index()\n    ax = values.plot.bar(color=sns.color_palette('husl', 16))\n    ax.set_ylabel('% in dataset', fontsize=16)\n    ax.set_xlabel(by, fontsize=12)\n    showLabels(ax)\n    plt.subplot(1, 2, 2)\n    values = (credit_df.loc[credit_df['Risk']=='bad'][by].value_counts(normalize=True)*100)\n    if sortbyindex:\n        values = values.sort_index()\n    ax = values.plot.bar(color=sns.color_palette('husl', 16))\n    ax.set_ylabel('% of bad risks ou', fontsize=16)\n    showLabels(ax)","0aa71ab5":"def plotRiskStatus(dataframe, by, risk='bad'):\n    grp = dataframe.groupby(['Risk',by])[by].count()\n    cnt = dataframe.groupby(by)[by].count()\n    #print(grp)\n    percentages = grp.unstack() * 100 \/ cnt.T\n    #print(percentages)\n    ax = percentages.loc[risk].plot.bar(color=sns.color_palette('husl', 16))\n    ax.set_ylabel('% of ' + risk + ' risks')\n    showLabels(ax, grp[risk].to_dict())\n    plt.margins(0.2, 0.2)\n    plt.tight_layout()\n    return ax","fd15c51d":"grp = credit_df.groupby(['Risk','Sex'])['Sex'].count()\ngrp","8c99c5c0":"for col in credit_categorical_col:\n    plt.figure(figsize=(15,10))\n    plot_percentages(credit_df, col)\n    plt.figure(figsize=(15,10))\n    plotRiskStatus(credit_df, col)","55ce41db":"for col in credit_numeric_col:\n    plt.figure(figsize=(15,10))\n    plot_distribution(credit_df, col)","23bdedce":"round(credit_df.isnull().sum().sort_values(ascending=False)\/len(credit_df.index)*100, 2)","239368d7":"credit_df['Checking account'] = credit_df['Checking account'].fillna('None')","98799ad5":"round(credit_df.isnull().sum().sort_values(ascending=False)\/len(credit_df.index)*100, 2)","36d0dc2f":"credit_df.dropna(inplace=True)","e6b762f8":"round(credit_df.isnull().sum().sort_values(ascending=False)\/len(credit_df.index)*100, 2)","67cd22ef":"credit_df.info()","ab2953e7":"for column in credit_numeric_col:\n    q1 = credit_df[column].quantile(0.1)\n    q3 = credit_df[column].quantile(0.9)\n    iqr = q3 - q1\n    \n    #Excluding everything outside the interquantile range\n    credit_df = credit_df[(credit_df[column] >= q1 - 1.5*iqr) & \n                      (credit_df[column] <= q3 + 1.5*iqr)] \n    print(credit_df.shape)","4b4a7256":"binary_var = ['Sex']\n\nfor col in binary_var:\n    ulist = credit_df[col].unique()\n    credit_df[col] = credit_df[col].map({ulist[0] : 1, ulist[1] : 0})\n    \ncredit_df['Risk'] = credit_df['Risk'].map({'good' : 0, 'bad' : 1})","58929d6c":"credit_categorical_col = list(credit_categorical_col)","51786237":"credit_categorical_col.remove('Sex')","5156631f":"credit_df_dummies = pd.get_dummies(credit_df[credit_categorical_col], drop_first=True)","c66c1b74":"credit_df_dummies.head()","d60fa9ae":"credit_df.drop(credit_categorical_col, axis=1, inplace=True)\ncredit_df = pd.concat((credit_df, credit_df_dummies), axis = 1)\ncredit_df.head()","a82cc078":"credit_df_fe = credit_df.copy()\ncredit_df.head()","1626d680":"credit_df_fe['credit_per_month'] = credit_df_fe['Credit amount'] \/ credit_df_fe['Duration']\ncredit_df_fe.drop(['Credit amount', 'Duration'], axis=1, inplace=True)","2ae19ec1":"credit_df_fe.head()","8e2c9590":"from sklearn.model_selection import train_test_split","a859419b":"y = credit_df_fe.pop('Risk')\nX = credit_df_fe","b0ff3050":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)","d93b902e":"X_train.info()","a481b16f":"X_test.info()","2c359068":"y_train.count()","edc89261":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()","3a3975c9":"col = X.columns\nindex = X.index","390133e7":"X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = col)\nX_test = pd.DataFrame(scaler.transform(X_test), columns = col)","b4478358":"X_train.head()","0f8e0e66":"#import libraries for modeling\nimport sklearn.preprocessing\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier","823a007a":"def getModelMetrics(actual=False, pred=False):\n    confusion = confusion_matrix(actual, pred)\n    \n    TP = confusion[1,1]\n    FP = confusion[0,1]\n    TN = confusion[0,0]\n    FN = confusion[1,0]\n    \n    print(\"Roc_auc_score : {}\".format(metrics.roc_auc_score(actual,pred)))\n    # Sensitivity\n    print('Sensitivity\/Recall : {}'.format(TP \/ float(TP+FN)))\n    # specificity\n    print('Specificity: {}'.format(TN \/ float(TN+FP)))\n    # false postive rate - predicting churn when customer has not churned\n    print('False Positive Rate: {}'.format(FP\/ float(TN+FP)))\n    # positive predictive value \n    print('Positive predictive value: {}'.format(TP \/ float(TP+FP)))\n    # Negative predictive value\n    print('Negative Predictive value: {}'.format(TN \/ float(TN+ FN)))\n    # sklearn precision score value \n    print('sklearn precision score value: {}'.format(metrics.precision_score(actual, pred )))","6e3fd8cb":"def plot_accuracies(scores,param):\n    # plotting accuracies with max_depth\n    plt.figure()\n    plt.plot(scores[\"param_\"+param], \n    scores[\"mean_train_score\"], \n    label=\"training accuracy\")\n    plt.plot(scores[\"param_\"+param], \n    scores[\"mean_test_score\"], \n    label=\"test accuracy\")\n    plt.xlabel(param)\n    plt.ylabel(\"f1\")\n    plt.legend()\n    plt.show()","d06a257e":"def predictRiskWithCutOff(model,X,y,prob):\n    # Funtion to predict the churn using the input probability cut-off\n    \n    # predict\n    pred_probs = model.predict_proba(X)[:,1]\n    \n    y_df= pd.DataFrame({'risk':y, 'risk_Prob':pred_probs})\n\n    y_df['final_predicted'] = y_df.churn_Prob.map( lambda x: 1 if x > prob else 0)\n    # Let's see the head\n    getModelMetrics(y_df.churn,y_df.final_predicted)\n    return y_df","5ecb1715":"def findOptimalCutoff(df):\n    #Function to find the optimal cutoff for classifing as churn\/non-churn\n    # Let's create columns with different probability cutoffs \n    numbers = [float(x)\/10 for x in range(10)]\n    for i in numbers:\n        df[i] = df.churn_Prob.map( lambda x: 1 if x > i else 0)\n    #print(df.head())\n    \n    # Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\n    cutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\n    from sklearn.metrics import confusion_matrix\n    \n    # TP = confusion[1,1] # true positive \n    # TN = confusion[0,0] # true negatives\n    # FP = confusion[0,1] # false positives\n    # FN = confusion[1,0] # false negatives\n    \n    num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n    for i in num:\n        cm1 = metrics.confusion_matrix(df.churn, df[i] )\n        total1=sum(sum(cm1))\n        accuracy = (cm1[0,0]+cm1[1,1])\/total1\n        \n        speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n        sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n        cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\n    print(cutoff_df)\n    # Let's plot accuracy sensitivity and specificity for various probabilities.\n    cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\n    plt.show()","cff6e1a4":"logisticModel = LogisticRegression(verbose=1)","3acd885f":"logisticModel.fit(X_train, y_train)","ce910ea1":"y_train_pred = logisticModel.predict(X_train)","dbfeae36":"getModelMetrics(y_train, y_train_pred)","03e5fd83":"y_test_pred = logisticModel.predict(X_test)","67053a05":"getModelMetrics(y_test, y_test_pred)","e20aa659":"svmLin = SVC(C=1, kernel='linear')","b2b120c1":"svmLin.fit(X_train, y_train)\ny_train_pred = svmLin.predict(X_train)\ngetModelMetrics(y_train, y_train_pred)","99adb481":"params = {\"C\" : [0.1, 1, 50]}","a3db9eea":"svmLin1 = SVC(kernel='linear')","114c033e":"model_cv = GridSearchCV(estimator = svmLin1,\n                       param_grid = params,\n                       scoring = 'recall',\n                       cv = 3,\n                       verbose = 5,\n                       n_jobs = 8,\n                       return_train_score=True)\nmodel_cv.fit(X_train, y_train)","cff2ca9b":"plot_accuracies(model_cv.cv_results_, 'C')","463d9b48":"model_cv.best_estimator_","3448dc86":"svm_final = SVC(C=50, kernel='linear')\n\n#fit on train model\nsvm_final.fit(X_train, y_train)\n\n# predict on train\ny_pred = svm_final.predict(X_train)\ngetModelMetrics(y_train,y_pred)","1976f223":"#predict on test\ny_pred = svm_final.predict(X_test)\ngetModelMetrics(y_test, y_pred)","caf91c43":"svmNonLin = SVC(kernel='rbf')","a01f85e9":"model_cv = GridSearchCV(estimator = svmNonLin,\n                       param_grid = params,\n                       scoring = 'recall',\n                       cv = 3,\n                       verbose = 5,\n                       n_jobs = 8,\n                       return_train_score=True)\nmodel_cv.fit(X_train, y_train)","61b3e66a":"model_cv.best_estimator_","76d3845e":"plot_accuracies(model_cv.cv_results_, 'C')","0f1d792f":"svm_final = SVC(C=1, kernel='rbf')\n\n#fit on train model\nsvm_final.fit(X_train, y_train)\n\n# predict on train\ny_train_pred = svm_final.predict(X_train)\ngetModelMetrics(y_train,y_train_pred)","1a5b2a2f":"y_test_pred = svm_final.predict(X_test)\ngetModelMetrics(y_test,y_test_pred)","413154e6":"Changing datatype of job from numeric to object","c5964730":"Class imbalance","9d7cb762":"# German Credit Risk Analysis","b4b684f8":"Separating out numeric and categorical variables","542830f9":"We can combine credit amount and duration to a variable credit\/month","64d0c2c5":"We see that model is overfitting even in SVM. I think we should get more features to explain the variance on the dataset and generalize the whole data.\n\nDo let me know if there is any other way of avoiding overfitting","202ff2d2":"Linear kernel is having a hard time","e7e2d3a5":"#performing SMOTE\nfrom imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state=100)\nX_train_res, y_train_res = sm.fit_sample(X_train, y_train)","8b0eeb1b":"Handling outliers","ad4f08f4":"# Reading and understanding Data","2c70e219":"Since close to 40% rows are null we can add them into a new category None","7b7369ad":"**Some insights for categorical variables:**\n1. There are more males than females in the dataset. Out of the total percentage of bad risks, there are more male incidents. But out of the total females in the dataset there are more % of bad risks.","dd91843b":"**Logistic Regression**","bcb8ed96":"**SVM**","d616782e":"# EDA","8641543e":"Handling missing values","7cc3da5e":"Hyper parameter tuning for SVM","9ed49c2f":"# Modelling","540d3336":"# Data Cleaning","e2e967cc":"We now drop the rows where saving accounts are null","f3e99a3f":"Adding dummy columns for categorical variables","0463dfe1":"print(\"Number of churners before sampling: \", sum(y_train==1))\nprint(\"Number of non-churners before sampling: \", sum(y_train==0))\nprint(\"Churn rate before sampling: \", sum(y_train==1)\/len(y_train)*100)","cad8f738":"# Feature Engineering","0b790603":"Not so great for logistic Regression","d03977e0":"Scaling","61f954d6":"# Data Preprocessing","a3972961":"print(\"Shape of X_train = \", X_train_res.shape)\nprint(\"Shape of y_train = \", y_train_res.shape)\n\nprint(\"Number of churners after sampling: \", sum(y_train_res==1))\nprint(\"Number of non-churners after sampling: \", sum(y_train_res==0))\nprint(\"Churn rate after sampling: \", sum(y_train_res==1)\/len(y_train_res)*100)"}}