{"cell_type":{"aa9eebf1":"code","b3dc6f00":"code","29c1b741":"code","9d6a87a8":"code","33906ba5":"code","3d90c579":"code","2d89ea09":"code","ecb7a2b3":"code","706271c9":"code","d16eb508":"code","a4e19f09":"code","268615cc":"code","972f9bf4":"code","db2db17f":"code","5e7bd4a3":"code","1fddf9bb":"code","b0054444":"code","53ee8aca":"code","79441888":"code","b4280251":"code","9a5cbe58":"code","9ef4cb92":"code","b1b28e07":"code","701d9837":"code","ebc07df4":"code","cf35a085":"code","dbc388d6":"code","6e787f29":"code","1f890b0b":"code","632b3901":"code","de762bae":"code","8f61b80d":"code","11276f46":"code","7c2d14e9":"code","3bf57afb":"code","a81a5e81":"code","825fd2fe":"code","bb29dad1":"code","9e24cc68":"code","fc927795":"code","7f0736ac":"code","be52c915":"code","b008065d":"code","9bca2b75":"code","81f371e9":"code","7c6dc5a7":"code","631fdfcf":"code","1035b48d":"markdown","fd6dddb5":"markdown","da7f2f51":"markdown","f9e04629":"markdown","bb71febc":"markdown","c77f5ec9":"markdown","f5ce8c0b":"markdown","7768991b":"markdown","fe4d3fbe":"markdown","ef39658d":"markdown","db7db202":"markdown","3f0f0acd":"markdown","33a0a390":"markdown","33a0c8d8":"markdown","b258dde5":"markdown","532d6f26":"markdown","23fbf663":"markdown","75f0d11e":"markdown","cadecb6b":"markdown","53411971":"markdown","c51d514f":"markdown","157f5542":"markdown","05507a47":"markdown"},"source":{"aa9eebf1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set()\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b3dc6f00":"path = '..\/input\/santander-customer-transaction-prediction\/train.csv'\ndf = pd.read_csv(path)","29c1b741":"print(\"Data has {} rows, {} columns\".format(df.shape[0], df.shape[1]))","9d6a87a8":"df.head()","33906ba5":"print(\"Data has {} null values\".format(df.isnull().any().sum()))","3d90c579":"f, ax = plt.subplots(nrows = 1, ncols = 2)\nsns.countplot(x = 'target', data = df, ax = ax[0])\nax[1].pie(x = df.target.value_counts().values, labels = df.target.value_counts().index, autopct = \"%.2f%%\")\nax[1].set_title(\"Percentage distribution\")\nplt.show()","2d89ea09":"def corr_help(df, col):\n    x = []\n    for i in range(200):\n        corr = df[col].corr(df['var_'+str(i)])\n        x.append(corr)\n    return x","ecb7a2b3":"x = corr_help(df, 'target')\nsns.distplot(x)\nplt.show()","706271c9":"y = df['target']\ndf = df.drop(['ID_code', 'target'], axis = 1)","d16eb508":"Q1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR1 = Q3-Q1\ndf_c = df[~((df < (Q1-1.5*IQR1))|(df > (Q3+1.5*IQR1))).any(axis = 1)]","a4e19f09":"print('Data loss is {}%'.format(((len(y) - len(df_c))\/len(y))*100))","268615cc":"c = list(df_c.index)\nf = []\nfor i in range(len(c)-1):\n    for j in range(c[i]+1, c[i+1]):\n        f.append(j)\nfor i in f:\n    y.pop(i)\ny = list(y)\ndf_c['y'] = y","972f9bf4":"data = df_c","db2db17f":"f, axes = plt.subplots(nrows = 2, ncols = 2)\nfor i in range(4):\n    g = np.random.randint(0, 200)\n    sns.distplot(a = data['var_'+str(g)].values - data['var_'+str(g)].values.mean(),ax = axes[i\/\/2][i%2], axlabel = ('var_'+str(g)))\nplt.show()","5e7bd4a3":"data","1fddf9bb":"y = np.array(data['y'].values, dtype = int)\nX = np.array(data.drop('y', axis = 1).values, dtype = float)","b0054444":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX1 = scaler.fit_transform(X)","53ee8aca":"from imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nsmote = SMOTE(sampling_strategy = 3\/7, k_neighbors = 5, random_state = 9)\nunder = RandomUnderSampler(sampling_strategy = 0.5)\nX1, y = smote.fit_resample(X1, y)\nX1, y = under.fit_resample(X1, y)","79441888":"print('1\\t', len(y[y==1])\/len(y), '% \\n0\\t', len(y[y==0])\/len(y), '%')","b4280251":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X1, y, test_size = 0.2)","9a5cbe58":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nparams = {'C' : [0.0001, 0.0003, 0.0005, 0.001, 0.003, 0.005, 0.01, 0.03, 0.05, 0.1, 0.3, 0.5]}\nclf = GridSearchCV(LogisticRegression(), params)\nclf.fit(X_train, y_train)","9ef4cb92":"(clf.score(X_val, y_val))","b1b28e07":"pred_prob = clf.predict_proba(X_val)","701d9837":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\ndef auc_roc_plot(x, y, l, colors):\n    #roc curve for clf\n    fpr, tpr, thresh = roc_curve(y_val, pred_prob[:, 1], pos_label = 1)\n\n    # for fpr = tpr\n    random_probs = [0 for i in range(len(y_val))]\n    p_fpr, p_tpr, _ = roc_curve(y_val, random_probs, pos_label=1)\n\n    #auc\n    auc_score = roc_auc_score(y_val, pred_prob[:, 1])\n\n    #plot\n    plt.plot(fpr, tpr, linestyle = '--', color = 'orange', label = \"LogisticRegression\")\n    for i in range(len(x)):\n        plt.plot(x[i], y[i], linestyle = '--', color = colors[i], label = l[i])\n    plt.plot(p_fpr, p_tpr, linestyle = '--', color = 'blue')\n    plt.title(\"ROC curve\")\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.legend(loc = 'best')\n    plt.show()","ebc07df4":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier","cf35a085":"fprs = []\ntprs = []\nlabels = []\ncolors = []","dbc388d6":"clf_rf = RandomForestClassifier()\nclf_rf.fit(X_train, y_train)\nclf_rf_pp = clf_rf.predict_proba(X_val)\nclf_rf_fpr, clf_rf_tpr, _ = roc_curve(y_val, clf_rf_pp[:, 1], pos_label = 1)\nfprs.append(clf_rf_fpr)\ntprs.append(clf_rf_tpr)\nlabels.append('Random Forest')\ncolors.append('r')","6e787f29":"clf_dt = DecisionTreeClassifier()\nclf_dt.fit(X_train, y_train)\nclf_dt_pp = clf_dt.predict_proba(X_val)\nclf_dt_fpr, clf_dt_tpr, _ = roc_curve(y_val, clf_dt_pp[:, 1], pos_label = 1)\nfprs.append(clf_dt_fpr)\ntprs.append(clf_dt_tpr)\nlabels.append('Decision Tree')\ncolors.append('c')","1f890b0b":"clf_xgb = XGBClassifier()\nclf_xgb.fit(X_train, y_train)\nclf_xgb_pp = clf_xgb.predict_proba(X_val)\nclf_xgb_fpr, clf_xgb_tpr, _ = roc_curve(y_val, clf_xgb_pp[:, 1], pos_label = 1)\nfprs.append(clf_xgb_fpr)\ntprs.append(clf_xgb_tpr)\nlabels.append('XGB')\ncolors.append('m')","632b3901":"auc_roc_plot(fprs, tprs, labels, colors)","de762bae":"clf_rf.score(X_val, y_val)","8f61b80d":"path_test = '..\/input\/santander-customer-transaction-prediction\/test.csv'\ntest = pd.read_csv(path_test)","11276f46":"test.isna().any().sum()","7c2d14e9":"ids = test['ID_code'].values\nX_test = test.drop('ID_code', axis = 1).values","3bf57afb":"X_test","a81a5e81":"X_test = scaler.transform(X_test)","825fd2fe":"y_preds = clf_rf.predict(X_test)","bb29dad1":"y_preds = np.array(y_preds)","9e24cc68":"len(y_preds[y_preds == 0])\/len(y_preds)","fc927795":"submission = pd.DataFrame({'ID_code':ids,\n                          'target':y_preds})\nsubmission.to_csv('cust.csv',index=False)","7f0736ac":"from keras.models import Sequential\nfrom keras.layers import Dense\nmodel = Sequential([Dense(256, activation = 'relu', input_dim = X_train.shape[1]),\n           Dense(256, activation = 'relu'),\n           Dense(512, activation = 'relu'),\n           Dense(512, activation = 'relu'),\n           Dense(1024, activation = 'relu'),\n           Dense(1024, activation = 'relu'),\n           Dense(1024, activation = 'relu'),\n           Dense(1024, activation = 'relu'),\n           Dense(512, activation = 'relu'),\n           Dense(512, activation = 'relu'),\n           Dense(256, activation = 'relu'),\n           Dense(256, activation = 'relu'),\n           Dense(1, activation = 'sigmoid')\n            ]\n          )\nmodel.compile(optimizer = 'adam', loss = 'BinaryCrossentropy', metrics = 'AUC')","be52c915":"model.summary()","b008065d":"model.fit(X_train, y_train, epochs = 20, validation_data = (X_val, y_val))","9bca2b75":"preds_dl = model.predict(X_test)","81f371e9":"preds_dl = [1 if i > 0.5 else 0 for i in preds_dl]","7c6dc5a7":"preds_dl = np.array(preds_dl)\nlen(preds_dl[preds_dl == 1])\/len(preds_dl)","631fdfcf":"submission = pd.DataFrame({'ID_code':ids,\n                          'target':preds_dl})\nsubmission.to_csv('cust1.csv',index=False)","1035b48d":"Applying Deep Learning","fd6dddb5":"Score obtained via DL is 0.65 (significant improvement)","da7f2f51":"Split the data into train and validation","f9e04629":"NO NULLS","bb71febc":"Removing the values in target columns which were related to outliers","c77f5ec9":"Scaling the values","f5ce8c0b":"Visualising the AUC-ROC plots","7768991b":"Applying RandomForestClassifier","fe4d3fbe":"Distribution of values in the Target column","ef39658d":"# **Machine Learning**","db7db202":"Using ML (RandomForestClassifier) the score obtained on submission is about 0.5.","3f0f0acd":"Preping for removing the outliers","33a0a390":"Applying XGBoost Classifier","33a0c8d8":"### corr_help : A function to find co-relation of features with a given columns","b258dde5":"Amazing to have no nulls","532d6f26":"### **The data is heavily biased**","23fbf663":"First applying Logistic Regression(LR). LR will form the base algorithm and every other algorithm's performace will be compared to it.","75f0d11e":"Applying DecisionTreeClassifier","cadecb6b":"In a Classification problem accuracy is not a good measure of performance. Thus using AUC-ROC to compare the performance.","53411971":"Visualising the distribution of values in diff var (selected randomly)","c51d514f":"Using SMOTE to oversample target = 1 data points and then RandomUnderSampling the obtained data such that the ratio of target = 1\/ target = 0 data points is 1\/2","157f5542":"It is evident that the RandomForestClassifier does a good job followed by XGBClassifier then LogisticRegression (which was the baseline for comparison)","05507a47":"To trade off the imbalance I'll undersample the majority class and oversample (SMOTE) the minority class"}}