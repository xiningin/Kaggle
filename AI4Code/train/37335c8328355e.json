{"cell_type":{"7f25c175":"code","efbec0c7":"code","f05d796f":"code","fc241868":"code","229f7581":"code","e14e1a6d":"code","735cc609":"code","c014fc55":"code","67c936b7":"code","fed6a8c8":"code","243bcfce":"code","18ec3d15":"code","d25daf9a":"code","17fb9168":"code","e29089e5":"code","b6b9c062":"code","8b84c0de":"code","96ce2f81":"code","ffabb0b0":"code","1be6d69a":"code","439b0cec":"code","50bbde84":"code","d17a0ddd":"code","e18a0d5f":"code","06f7ba73":"code","1ee13a74":"code","0af83266":"code","ea992ac7":"code","6e488194":"code","8e070e9d":"code","98575105":"code","b522e1d8":"code","993e17f7":"code","656a49b7":"code","27c4032c":"code","53c9a85c":"code","ed332b3c":"code","0c56f4d3":"code","4493c046":"code","ab184661":"code","a170e75a":"code","b1390e5d":"code","992458c8":"code","038bdada":"markdown","94f6952a":"markdown","e41621f5":"markdown","53091310":"markdown","5f8d6567":"markdown","974b6e2c":"markdown","670e2626":"markdown","c328ed5f":"markdown","a91c1233":"markdown","1fca854d":"markdown","f98b023f":"markdown"},"source":{"7f25c175":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","efbec0c7":"# Pandas : librairie de manipulation de donn\u00e9es\n# NumPy : librairie de calcul scientifique\n# MatPlotLib : librairie de visualisation et graphiques\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score,auc, accuracy_score\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import train_test_split\n\nfrom IPython.core.display import HTML # permet d'afficher du code html dans jupyter","f05d796f":"def scale_feat(df,cont_feat) :\n    df1=df\n    scaler = preprocessing.RobustScaler()\n    df1[cont_feat] = scaler.fit_transform(df1[cont_feat])\n    return df1","fc241868":"from sklearn.model_selection import learning_curve\ndef plot_learning_curve(est, X_train, y_train) :\n    train_sizes, train_scores, test_scores = learning_curve(estimator=est, X=X_train, y=y_train, train_sizes=np.linspace(0.1, 1.0, 10),\n                                                        cv=5,\n                                                        n_jobs=-1)\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n    plt.figure(figsize=(8,10))\n    plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='training accuracy')\n    plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')\n    plt.plot(train_sizes, test_mean,color='green', linestyle='--',marker='s', markersize=5,label='validation accuracy')\n    plt.fill_between(train_sizes,test_mean + test_std,test_mean - test_std,alpha=0.15, color='green')\n    plt.grid(b='on')\n    plt.xlabel('Number of training samples')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='lower right')\n    plt.ylim([0.6, 1.0])\n    plt.show()","229f7581":"def plot_roc_curve(est,X_test,y_test) :\n    probas = est.predict_proba(X_test)\n    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,probas[:, 1])\n    roc_auc = auc(false_positive_rate, true_positive_rate)\n    plt.figure(figsize=(8,8))\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f'% roc_auc)\n    plt.legend(loc='lower right')\n    plt.plot([0,1],[0,1],'r--')        # plus mauvaise courbe\n    plt.plot([0,0,1],[0,1,1],'g:')     # meilleure courbe\n    plt.xlim([-0.05,1.2])\n    plt.ylim([-0.05,1.2])\n    plt.ylabel('Taux de vrais positifs')\n    plt.xlabel('Taux de faux positifs')\n    plt.show","e14e1a6d":"df = pd.read_csv(\"..\/input\/water-potability\/water_potability.csv\")","735cc609":"df.head().T","c014fc55":"df.count()","67c936b7":"df.info()","fed6a8c8":"df.Potability.value_counts()","243bcfce":"df.isnull().values.sum()","18ec3d15":"df4=df.dropna()","d25daf9a":"df4.count()","17fb9168":"X = df4.drop(['Potability'], axis=1)\ny = df4.Potability\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\nfrom imblearn.over_sampling import SMOTE\n\nsmote = SMOTE()\nX_train, y_train = smote.fit_resample(X_train, y_train)","e29089e5":"y_train.value_counts()","b6b9c062":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train,y_train)\ny_lr = lr.predict(X_test)","8b84c0de":"df4.Potability.value_counts()","96ce2f81":"from sklearn.metrics import accuracy_score, confusion_matrix\nlr_score = accuracy_score(y_test, y_lr)\nprint(lr_score)","ffabb0b0":"sns.pairplot(df4, hue=\"Potability\")","1be6d69a":"df1 = df.fillna(value = {'Sulfate':df.Sulfate.mean()})\ndf2 = df1.fillna(value = {'ph':df1.ph.mean()})\ndf3 = df2.fillna(value = {'Trihalomethanes':df2.Trihalomethanes.mean()})","439b0cec":"X = df3.drop(['Potability'], axis=1)\ny = df3.Potability\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\nfrom imblearn.over_sampling import SMOTE\n\nsmote = SMOTE()\nX_train, y_train = smote.fit_resample(X_train, y_train)","50bbde84":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train,y_train)\n","d17a0ddd":"y_lr = lr.predict(X_test)","e18a0d5f":"from sklearn.metrics import accuracy_score, confusion_matrix\nlr_score = accuracy_score(y_test, y_lr)\nprint(lr_score)","06f7ba73":"df3.Potability.value_counts()","1ee13a74":"df4=df.dropna()","0af83266":"X = df4.drop(['Potability'], axis=1)\ny = df4.Potability\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\nfrom imblearn.over_sampling import SMOTE\n\nsmote = SMOTE()\nX_train, y_train = smote.fit_resample(X_train, y_train)","ea992ac7":"from sklearn import ensemble\n\nrf = ensemble.RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_rf = rf.predict(X_test)","6e488194":"print(classification_report(y_test, y_rf))","8e070e9d":"df4=df.dropna()","98575105":"X = df4.drop(['Potability'], axis=1)\ny = df4.Potability\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\nfrom imblearn.over_sampling import SMOTE\n\nsmote = SMOTE()\nX_train, y_train = smote.fit_resample(X_train, y_train)","b522e1d8":"len(X_train)","993e17f7":"len(X_test)","656a49b7":"df4=df.dropna()\n","27c4032c":"X = df4.drop(['Potability'], axis=1)\ny = df4.Potability\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\nfrom imblearn.over_sampling import SMOTE\n\nsmote = SMOTE()\nX_train, y_train = smote.fit_resample(X_train, y_train)","53c9a85c":"!pip install xgboost\n\nimport xgboost as XGB\nxgb  = XGB.XGBClassifier()\nxgb.fit(X_train, y_train)\ny_xgb = xgb.predict(X_test)\ncm = confusion_matrix(y_test, y_xgb)","ed332b3c":"print(cm)\nprint(classification_report(y_test, y_xgb))","0c56f4d3":"from sklearn.naive_bayes import GaussianNB, MultinomialNB\nmodel = GaussianNB()\n","4493c046":"model.fit(X_train,y_train)","ab184661":"y_pred = model.predict(X_test)","a170e75a":"model.score(X_test,y_test)","b1390e5d":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test, y_pred)","992458c8":"mn = MultinomialNB()\nmn.fit(X_train,y_train)\nmn.score(X_test,y_test)","038bdada":"On pourrait appliquer cette m\u00e9thode si nous avions des valeurs discret \u00e0 la place de nos chiffres","94f6952a":"Notre data sont normale on a pour chaque param\u00e8tre une chiffre precis et non une valeur discret. On peut donc utiliser le modele de gauss","e41621f5":"Je supprime les valeurs NA","53091310":"# XgBoost","5f8d6567":"Sur \u00e9chantillonnage ","974b6e2c":"Utilisation de la m\u00e9thode multinomiale ","670e2626":"Entrainement du mod\u00e8le ","c328ed5f":"Ramdom forest ","a91c1233":"# Naive bayes","1fca854d":"Points forts de la m\u00e9thode :\n- Nous avons besoin de peu de donn\u00e9es \n- C'est une m\u00e9thode tr\u00e8s rapide compar\u00e9 \u00e0 d'autres classifieurs\n\nPoints faibles de la m\u00e9thode :\n- La probabilit\u00e9 obtenu doit \u00eatre prise avec de tr\u00e8s grosse pincette\n- Si on a une trop grande corr\u00e9lation entre les donn\u00e9es, on obtiendra de mauvaises performances ","f98b023f":"Cette m\u00e9thode peut-etre utilis\u00e9e dans le cas de filtre anti-span"}}