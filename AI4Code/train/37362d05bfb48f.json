{"cell_type":{"27cf8858":"code","009a3842":"code","f24cb576":"code","b69fb4e5":"code","b1fcada4":"code","2a0f0f27":"code","f9859ee2":"code","599e6b2c":"code","0e0871e4":"code","e952e96e":"code","2596cbd5":"code","02d63866":"code","8954d964":"code","0c6602b7":"code","d6490421":"code","a4d1a16d":"code","8bc888e2":"code","4889626b":"code","321182f3":"code","9bb503a1":"code","d69e9c1c":"code","8475ba8e":"code","69171758":"code","2fbd6517":"code","9e3bf6b4":"code","ebf45b2c":"code","709307a8":"code","188114b4":"code","61844d13":"code","b0c6e447":"code","e87a9d84":"code","0e4fa2ae":"code","21bf1c21":"code","3b716146":"code","f2f61fc3":"code","7e3a1d8f":"code","ae4f7169":"code","dcc10999":"code","80d4b410":"code","af59b450":"code","d5705136":"code","8c3435ec":"code","54871444":"code","c45c74fe":"code","e2e20a44":"code","5d693cf4":"code","309901f6":"code","f2079b10":"code","74b89355":"code","418200d0":"code","9eb44126":"code","e99f781c":"code","dcf754fc":"code","768dc6dc":"code","43c309dd":"code","e6cc1c6f":"code","e3da41e0":"code","34aa41ee":"code","590070a9":"code","497c0304":"code","53f2f69b":"code","8f69900f":"code","51849c40":"code","67e3cbbc":"code","9dee0914":"code","d77ac4e3":"code","ab9f5af3":"code","6351e746":"code","bf626221":"code","e0cae093":"code","421a1f6a":"code","a7f2c4e8":"code","20d55e3a":"code","632ea62e":"code","27c626a9":"code","92eb2bf1":"code","8f9179c2":"code","588626bd":"code","9f67749a":"code","d799a0df":"code","9d80581c":"code","5ee29abe":"code","4c8a383a":"code","6a31c089":"code","b9d5a9d4":"code","cb83fbf3":"code","e51a27a6":"code","cd279a1c":"code","b053f90c":"code","e78baf22":"code","12e1dc66":"code","6b473b7d":"code","319018ea":"code","2ed26eb5":"code","4313c875":"code","9cd6fed1":"code","20b0ba0c":"code","0c71247f":"code","87c5c884":"code","8c3ec6c8":"code","25434bf4":"code","9760341d":"code","cf8615dd":"code","44fab27a":"markdown","1c7c74e6":"markdown","40a2f6d4":"markdown","78cf132e":"markdown","28068b06":"markdown","119c0c95":"markdown","980feb0a":"markdown","72ca60ab":"markdown","c38e46b1":"markdown","1b14dfbe":"markdown","4a4d3abe":"markdown","a8edacd4":"markdown","ccfbf6fb":"markdown","fd530b9c":"markdown","1538fe2d":"markdown","2f77ba7f":"markdown","d900fbca":"markdown","c85926b1":"markdown","c9113890":"markdown","178be66a":"markdown","6121e839":"markdown","6fef274b":"markdown","ce121160":"markdown","a4cbdd1c":"markdown","adf4c22e":"markdown","cbf6cdcc":"markdown","b73111c9":"markdown","3e63749e":"markdown","d3c8045a":"markdown","c389005d":"markdown","cd3ba864":"markdown","88d394a8":"markdown","b1448bbd":"markdown","36cb174f":"markdown","a73ba416":"markdown","399838af":"markdown","5e975c73":"markdown","ed580d2d":"markdown","581ec865":"markdown","f01f2cb4":"markdown","ee104cfb":"markdown","69c0f8ef":"markdown","1707374a":"markdown","0edee5d6":"markdown","977df999":"markdown","b3019b9f":"markdown","15221296":"markdown","6979a0d4":"markdown","b172f3fe":"markdown","d05590a4":"markdown","80e2d892":"markdown","45e1bed1":"markdown","e2a985ca":"markdown","932b11be":"markdown","1dd53c0c":"markdown","f97b0c8a":"markdown","23b5093a":"markdown","bf24f942":"markdown","03c463a6":"markdown","60c438c1":"markdown","7d881af7":"markdown"},"source":{"27cf8858":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","009a3842":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\n%matplotlib inline\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, classification_report, precision_recall_curve\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n\n","f24cb576":"train = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\ntest = pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv')","b69fb4e5":"train.head()","b1fcada4":"train.shape","2a0f0f27":"train.info()","f9859ee2":"train.describe()","599e6b2c":"#Let's have a look of missing value\npercent_missing = train.isnull().sum() * 100 \/ len(train)\nmissing_value_df = pd.DataFrame({#'column_name': train.columns,\n                                 'percent_missing': percent_missing})\npercent_missing","0e0871e4":"msno.heatmap(train)","e952e96e":"train.dropna(inplace=True)\ntest.dropna(inplace=True)","2596cbd5":"train.isna().mean()","02d63866":"train.drop(['enrollee_id','city','company_size'], axis = 1, inplace = True)","8954d964":"train['experience'].unique()","0c6602b7":"def replace(experience):\n    if experience == '>20':\n        return 21\n    elif experience == '<1':\n        return 0\n\n    else:\n        return experience","d6490421":"train.experience = train.experience.map(replace)","a4d1a16d":"train['experience'].unique()","8bc888e2":"def replace(last_new_job):\n    if last_new_job == '>4':\n        return 5\n    elif last_new_job == 'never':\n        return 0\n\n    else:\n        return last_new_job\n\ntrain.last_new_job = train.last_new_job.map(replace)\ntrain['last_new_job'].unique()","4889626b":"#Now let's have look at my data and hope it's clean now\ntrain.head()","321182f3":"#First Check weather our dataset is balanced or not?\nvalues = train['target'].value_counts().values.tolist()\nlabels = train['target'].value_counts().index\nplt.figure(figsize= (10,10))\nplt.title('Comparing labels of target feature')\nplt.pie(x = values, labels = labels, autopct='%1.1f%%', pctdistance= .5)\nplt.show()\n","9bb503a1":"\nfig, ax = plt.subplots(3,2, figsize = (12,12))\n((ax1, ax2), (ax3, ax4), (ax5, ax6)) = ax\n\nlabels = train['gender'].value_counts().index\nvalues = train['gender'].value_counts().tolist()\nax1.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True)\nax1.set_title(\"Gender Distribution Pie Chart\", fontdict={'fontsize': 14})\n\nlabels = train['relevent_experience'].value_counts().index\nvalues = train['relevent_experience'].value_counts().tolist()\nax2.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True, explode=[0, 0.2])\nax2.set_title(\"Experience Distribution Pie Chart\", fontdict={'fontsize': 14})\n\nlabels = train['enrolled_university'].value_counts().index\nvalues = train['enrolled_university'].value_counts().tolist()\nax3.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True, explode=[0, 0.2,.3])\nax3.set_title(\"Enrooled University Distribution Pie Chart\", fontdict={'fontsize': 14})\n\nlabels = train['education_level'].value_counts().index\nvalues = train['education_level'].value_counts().tolist()\nax4.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True, explode=[0, 0.05,.1])\nax4.set_title(\"Education label Distribution Pie Chart\", fontdict={'fontsize': 14})\n\nlabels = train['major_discipline'].value_counts().index\nvalues = train['major_discipline'].value_counts().tolist()\nax5.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True, explode=[0.1, 0.1, 0.1, 0.1, 0.2, 0.1])\nax5.set_title(\"Major_discipline Distribution Pie Chart\", fontdict={'fontsize': 14})\n\n\n\nlabels = train['company_type'].value_counts().index\nvalues = train['company_type'].value_counts().tolist()\nax6.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True, explode=[0, 0.1,.1,.1, .15,.1])\nax6.set_title(\"Company Type Pie Chart\", fontdict={'fontsize': 14})\n\n\nplt.tight_layout()\nplt.show()","d69e9c1c":"fig_dims = (20, 14)\nfig, ax =plt.subplots(3,2,figsize = fig_dims)\nsns.countplot(x = train['gender'],hue = train['target'], ax=ax[0,0], edgecolor=sns.color_palette(\"dark\", 60))\nsns.countplot(train['education_level'],hue = train['target'], ax=ax[0,1])\nsns.countplot(x = train['relevent_experience'],hue = train['target'], ax=ax[1,0])\nsns.countplot(train['enrolled_university'],hue = train['target'], ax=ax[1,1])\nsns.countplot(x = train['major_discipline'],hue = train['target'], ax=ax[2,0])\nsns.countplot(x = train['company_type'],hue = train['target'], ax=ax[2,1])\n\n\nfig.suptitle('Features distribution based on target ',fontsize=40)\nfig.show()\n","8475ba8e":"g = sns.kdeplot(train['city_development_index'][(train[\"target\"] == 0) & (train['city_development_index'].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(train['city_development_index'][(train[\"target\"] == 1) & (train['city_development_index'].notnull())], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel('city_development_index')\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not looking for job change,\",\"looking for job change,\"])","69171758":"## new City_devlopment_cat feature based on this obserbations.\nbins = [0,.45,.67,.84,1]\nlabels=[0,1,2,3]\ntrain['City_devlopment_cat'] = pd.cut(train['city_development_index'], bins=bins, labels=labels)\ntrain[['City_devlopment_cat', 'target']].groupby(['City_devlopment_cat'], as_index=False).mean().sort_values(by='target', ascending=False)","2fbd6517":"train['experience'] = train['experience'].astype(int)","9e3bf6b4":"g = sns.kdeplot(train['experience'][(train[\"target\"] == 0) & (train['experience'].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(train['experience'][(train[\"target\"] == 1) & (train['experience'].notnull())], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel('experience')\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not looking for job change,\",\"looking for job change,\"])","ebf45b2c":"train['last_new_job'] = train['last_new_job'].astype(int)","709307a8":"g = sns.kdeplot(train['last_new_job'][(train[\"target\"] == 0) & (train['last_new_job'].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(train['last_new_job'][(train[\"target\"] == 1) & (train['last_new_job'].notnull())], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel('last_new_job')\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not looking for job change,\",\"looking for job change,\"])","188114b4":"g = sns.kdeplot(train['training_hours'][(train[\"target\"] == 0) & (train['training_hours'].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(train['training_hours'][(train[\"target\"] == 1) & (train['training_hours'].notnull())], ax =g, color=\"Green\", shade= True)\ng.set_xlabel('training_hours')\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not looking for job change,\",\"looking for job change,\"])","61844d13":"sns.heatmap(train.corr(), annot = True, vmin=-1, vmax=1, center= 0,\n            cmap= 'Blues_r', linewidths=3, linecolor='black')\n","b0c6e447":"train['target'].value_counts()","e87a9d84":"#Let's use pivot table to analyze it.\ntable = pd.pivot_table(train,index=['gender'])\ntable\ntable.plot(kind='bar')","0e4fa2ae":"table = pd.pivot_table(train,index=['gender','target'])\ntable\ntable.plot(kind='line')","21bf1c21":"table = pd.pivot_table(train,index=['gender','target','education_level'])\nprint(table)\ntable.plot(kind='bar',\n           figsize = (15,8),\n           colormap = 'RdGy')","3b716146":"table = pd.pivot_table(train,\n                       index=['gender','education_level'],\n                       aggfunc={'target':np.sum}\n                      )\nprint(table)\ntable.plot(kind='barh',\n           figsize = (15,8),\n           colormap = 'tab10_r',\n           title = 'Gender and Educatiuon lebel Relationship')","f2f61fc3":"table = pd.pivot_table(train,\n                       index=['gender','enrolled_university','education_level'],\n                       aggfunc={'target':np.sum}\n                      )\nprint(table)\ntable.plot(kind='bar',\n           figsize = (15,8),\n           colormap = 'tab10_r',\n           title = 'Gender and Educatiuon lebel and Enrolled University Relationship')","7e3a1d8f":"table = pd.pivot_table(train,\n                       index=['company_type','last_new_job'],\n                       aggfunc={'target':np.sum}\n                      )\nprint(table)\ntable.plot(kind='bar',\n           figsize = (15,8),\n           colormap = 'tab10_r',\n           title = 'Company_type and last_new_job Relationship')","ae4f7169":"table = pd.pivot_table(train,\n                       index=['last_new_job'],\n                       columns = ['enrolled_university'],\n                       aggfunc={'target':np.sum}\n                      )\nprint(table)\ntable.plot(kind='bar',\n           figsize = (15,8),\n           colormap = 'Set1_r',\n           title = 'Enrolled and last_new_job Relationship')","dcc10999":"table = pd.pivot_table(train,\n                       index=['major_discipline','gender'],\n                       columns = ['enrolled_university'],\n                       values = ['target'],\n                       aggfunc=np.sum\n                       \n                      )\nprint(table)\ntable.plot(kind='bar',\n           figsize = (15,8),\n           colormap = 'Set2_r',\n           title = 'Enrolled and major discipline Relationship')          ","80d4b410":"df = train.copy()","af59b450":"df = pd.get_dummies(df,\n                    columns=['gender', 'relevent_experience', 'enrolled_university', 'major_discipline', 'company_type'],\n                    drop_first=True)","d5705136":"df['education_level'] = df['education_level'].map( {'Graduate': 0, 'Masters': 1,'Phd': 2} ).astype(int)\n","8c3435ec":"X = df.drop(['target'], axis=1)\ny = df['target']","54871444":"from sklearn.model_selection import train_test_split","c45c74fe":"X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.20, random_state=42)","e2e20a44":"from sklearn.preprocessing import StandardScaler\nScalerX = StandardScaler()\nX_train = ScalerX.fit_transform(X_train)\nX_test = ScalerX.transform(X_test)","5d693cf4":"from imblearn.over_sampling import SMOTE\nfrom collections import Counter\n\ncounter = Counter(y_train)\nprint('Before',counter)\n# oversampling the train dataset using SMOTE\nsmt = SMOTE()\n#X_train, y_train = smt.fit_resample(X_train, y_train)\nX_train_sm, y_train_sm = smt.fit_resample(X_train, y_train)\n\ncounter = Counter(y_train_sm)\nprint('After',counter)","309901f6":"from imblearn.over_sampling import ADASYN\n\ncounter = Counter(y_train)\nprint('Before',counter)\n# oversampling the train dataset using ADASYN\nada = ADASYN(random_state=130)\nX_train_ada, y_train_ada = ada.fit_resample(X_train, y_train)\n\ncounter = Counter(y_train_ada)\nprint('After',counter)\n","f2079b10":"from imblearn.combine import SMOTETomek\n\ncounter = Counter(y_train)\nprint('Before',counter)\n# oversampling the train dataset using SMOTE + Tomek\nsmtom = SMOTETomek(random_state=139)\nX_train_smtom, y_train_smtom = smtom.fit_resample(X_train, y_train)\n\ncounter = Counter(y_train_smtom)\nprint('After',counter)\n","74b89355":"from imblearn.combine import SMOTEENN\n\ncounter = Counter(y_train)\nprint('Before',counter)\n#oversampling the train dataset using SMOTE + ENN\nsmenn = SMOTEENN()\nX_train_smenn, y_train_smenn = smenn.fit_resample(X_train, y_train)\n\ncounter = Counter(y_train_smenn)\nprint('After',counter)","418200d0":"from imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\nsm = SMOTE(sampling_strategy = .3)\nrus =  RandomUnderSampler(sampling_strategy=.4)\n\npipeline = Pipeline(steps = [('smote', sm),('under',rus)])\n\ncounter = Counter(y_train)\nprint('Before',counter)\n#over and undersampling the train dataset using SMOTE + RandomUnderSampler\nX_train_smrus, y_train_smrus = pipeline.fit_resample(X_train, y_train)\n\ncounter = Counter(y_train_smrus)\nprint('After',counter)","9eb44126":"model = list()\nresample = list()\nprecision = list()\nrecall = list()\nF1score = list()\nAUCROC = list()","e99f781c":"def test_eval(clf_model, X_test, y_test, algo=None, sampling=None):\n    # Test set prediction\n    y_prob=clf_model.predict_proba(X_test)\n    y_pred=clf_model.predict(X_test)\n\n    print('Confusion Matrix')\n    print('='*60)\n    plot_confusion_matrix(clf_model, X_test, y_test)  \n    plt.show() \n    #print(confusion_matrix(y_test,y_pred),\"\\n\")\n    print('Classification Report')\n    print('='*60)\n    print(classification_report(y_test,y_pred),\"\\n\")\n    print('AUC-ROC')\n    print('='*60)\n    print(roc_auc_score(y_test, y_prob[:,1]))\n          \n    model.append(algo)\n    precision.append(precision_score(y_test,y_pred))\n    recall.append(recall_score(y_test,y_pred))\n    F1score.append(f1_score(y_test,y_pred))\n    AUCROC.append(roc_auc_score(y_test, y_prob[:,1]))\n    resample.append(sampling)\n","dcf754fc":"log_model=LogisticRegression()\n\nparams={'C':np.logspace( -10, 1, 15),'class_weight':[None,'balanced'],'penalty':['l1','l2']}\n\ncv = StratifiedKFold(n_splits=5, random_state=100, shuffle=True)\n\n# Create grid search using 5-fold cross validation\nclf_LR = GridSearchCV(log_model, params, cv=cv, scoring='roc_auc', n_jobs=-1)\nclf_LR.fit(X_train, y_train)\nclf_LR.best_estimator_","768dc6dc":"test_eval(clf_LR, X_test, y_test, 'Logistic Regression', 'actual')","43c309dd":"clf_LR.fit(X_train_sm, y_train_sm)\nclf_LR.best_estimator_","e6cc1c6f":"test_eval(clf_LR, X_test, y_test, 'Logistic Regression', 'smote')","e3da41e0":"clf_LR.fit(X_train_ada, y_train_ada)\nclf_LR.best_estimator_\n","34aa41ee":"test_eval(clf_LR, X_test, y_test, 'Logistic Regression', 'adasyn')","590070a9":"clf_LR.fit(X_train_smtom, y_train_smtom)\nclf_LR.best_estimator_","497c0304":"test_eval(clf_LR, X_test, y_test, 'Logistic Regression', 'smote+tomek')","53f2f69b":"clf_LR.fit(X_train_smenn, y_train_smenn)\nclf_LR.best_estimator_\n","8f69900f":"\ntest_eval(clf_LR, X_test, y_test, 'Logistic Regression', 'smote+enn')","51849c40":"clf_LR.fit(X_train_smrus, y_train_smrus)\nclf_LR.best_estimator_\n","67e3cbbc":"test_eval(clf_LR, X_test, y_test, 'Logistic Regression', 'smote+rus')","9dee0914":"estimators = [2,10,30,50,100]\n# Maximum number of depth in each tree:\nmax_depth = [i for i in range(5,16,2)]\n# Minimum number of samples to consider to split a node:\nmin_samples_split = [2, 5, 10, 15, 20, 50, 100]\n# Minimum number of samples to consider at each leaf node:\nmin_samples_leaf = [1, 2, 5]\n#Impurity\ncriterion = ['gini', 'entropy']\n#The number of features to consider when looking for the best split\nmax_features = ['log2', 'sqrt', 'auto']\n","d77ac4e3":"tree_model = DecisionTreeClassifier()\ntree_param_grid = { \n    'max_features':max_features,\n    'criterion':criterion,\n    'max_depth': max_depth,\n    'min_samples_split': min_samples_split,\n    'min_samples_leaf': min_samples_leaf\n}\n\nclf_DT = RandomizedSearchCV(tree_model, tree_param_grid, cv=cv, scoring='roc_auc', n_jobs=-1, verbose=2)\nclf_DT.fit(X_train, y_train)\nclf_DT.best_estimator_","ab9f5af3":"test_eval(clf_DT, X_test, y_test, 'Decision Tree', 'actual')","6351e746":"clf_DT.fit(X_train_sm, y_train_sm)\nclf_DT.best_estimator_","bf626221":"test_eval(clf_DT, X_test, y_test, 'Decision Tree', 'smote')","e0cae093":"clf_DT.fit(X_train_ada, y_train_ada)\nclf_DT.best_estimator_\n","421a1f6a":"test_eval(clf_DT, X_test, y_test, 'Decision Tree', 'adasyn')","a7f2c4e8":"clf_DT.fit(X_train_smtom, y_train_smtom)\nclf_DT.best_estimator_","20d55e3a":"test_eval(clf_DT, X_test, y_test, 'Decision Tree', 'smote+tomek')","632ea62e":"clf_DT.fit(X_train_smenn, y_train_smenn)\nclf_DT.best_estimator_","27c626a9":"test_eval(clf_DT, X_test, y_test, 'Decision Tree', 'smote+enn')","92eb2bf1":"clf_DT.fit(X_train_smrus, y_train_smrus)\nclf_DT.best_estimator_","8f9179c2":"test_eval(clf_LR, X_test, y_test, 'Decision Tree', 'smote+rus')","588626bd":"rf_model = RandomForestClassifier()\n\nrf_params={'n_estimators':estimators,\n           'max_features':max_features,\n           'criterion':criterion,\n           'max_depth': max_depth,\n            'min_samples_split': min_samples_split,\n            'min_samples_leaf': min_samples_leaf}\n\nclf_RF = RandomizedSearchCV(rf_model, rf_params, cv=cv, scoring='roc_auc', n_jobs=-1, n_iter=20, verbose=2)\nclf_RF.fit(X_train, y_train)\nclf_RF.best_estimator_","9f67749a":"test_eval(clf_RF, X_test, y_test, 'Random Forest', 'actual')","d799a0df":"clf_RF.fit(X_train_sm, y_train_sm)\nclf_RF.best_estimator_","9d80581c":"test_eval(clf_RF, X_test, y_test, 'Random Forest', 'smote')","5ee29abe":"clf_RF.fit(X_train_ada, y_train_ada)\nclf_RF.best_estimator_","4c8a383a":"test_eval(clf_RF, X_test, y_test, 'Random Forest', 'adasyn')","6a31c089":"clf_RF.fit(X_train_smtom, y_train_smtom)\nclf_RF.best_estimator_","b9d5a9d4":"test_eval(clf_RF, X_test, y_test, 'Random Forest', 'smote+tomek')","cb83fbf3":"clf_RF.fit(X_train_smenn, y_train_smenn)\nclf_RF.best_estimator_","e51a27a6":"test_eval(clf_RF, X_test, y_test, 'Random Forest', 'smote+enn')","cd279a1c":"clf_RF.fit(X_train_smrus, y_train_smrus)\nclf_RF.best_estimator_","b053f90c":"test_eval(clf_LR, X_test, y_test, 'Random Forest', 'smote+rus')","e78baf22":"import warnings\nwarnings.filterwarnings('ignore')","12e1dc66":"from sklearn.ensemble import AdaBoostClassifier\nclf_ada=AdaBoostClassifier()\nclf_ada.fit(X_train, y_train)\n#Actual data\ntest_eval(clf_ada, X_test, y_test, 'AdaBoast', 'actual')","6b473b7d":"#smote\nclf_ada.fit(X_train_sm, y_train_sm)\ntest_eval(clf_ada, X_test, y_test, 'AdaBoast', 'Smote')\n\n#Adasyn\nclf_ada.fit(X_train_ada, y_train_ada)\ntest_eval(clf_ada, X_test, y_test, 'AdaBoast', 'adasyn')\n\n#smote + tomek\nclf_ada.fit(X_train_smtom, y_train_smtom)\ntest_eval(clf_ada, X_test, y_test, 'AdaBoast', 'smote+tomek')\n\n#smote + enn\nclf_ada.fit(X_train_smenn, y_train_smenn)\ntest_eval(clf_ada, X_test, y_test, 'AdaBoast', 'smote+enn')\n\n#smote + \nclf_ada.fit(X_train_smrus, y_train_smrus)\ntest_eval(clf_ada, X_test, y_test, 'AdaBoast', 'smote+rus')\n","319018ea":"from sklearn.ensemble import GradientBoostingClassifier\n\ngb_model = GradientBoostingClassifier()\n\ngb_params = { \n    \"n_estimators\":[1,3,5,10,15,20,30,40,50,],\n    'learning_rate': [0.1, 0.05, 0.02, 0.01],\n    'max_depth': max_depth,\n    'min_samples_split': min_samples_split,\n    'min_samples_leaf': min_samples_leaf\n}\n\nclf_gb=RandomizedSearchCV(gb_model,gb_params,cv=cv, scoring='roc_auc',n_jobs=1)\n\nclf_gb.fit(X_train, y_train)\nclf_gb.best_estimator_\n","2ed26eb5":"#Actual data\n#clf_gb.fit(X_train, y_train)\n#clf_gb.best_estimator_\ntest_eval(clf_gb, X_test, y_test, 'GradientBoast', 'actual')\n#smote\nclf_gb.fit(X_train_sm, y_train_sm)\nclf_gb.best_estimator_\ntest_eval(clf_gb, X_test, y_test, 'GradientBoast', 'Smote')\n\n#Adasyn\nclf_gb.fit(X_train_ada, y_train_ada)\nclf_gb.best_estimator_\ntest_eval(clf_gb, X_test, y_test, 'GradientBoast', 'adasyn')\n\n#smote + tomek\nclf_gb.fit(X_train_smtom, y_train_smtom)\nclf_gb.best_estimator_\ntest_eval(clf_gb, X_test, y_test, 'GradientBoast', 'smote+tomek')\n\n#smote + enn\nclf_gb.fit(X_train_smenn, y_train_smenn)\nclf_gb.best_estimator_\ntest_eval(clf_gb, X_test, y_test, 'GradientBoast', 'smote+enn')\n\n#smote + rus\nclf_gb.fit(X_train_smrus, y_train_smrus)\nclf_gb.best_estimator_\ntest_eval(clf_gb, X_test, y_test, 'GradientBoast', 'smote+rus')\n","4313c875":"from sklearn.linear_model import SGDClassifier\nsgd_params = {\n    \"loss\" : [\"hinge\", \"log\", \"squared_hinge\", \"modified_huber\"],\n    \"alpha\" : [0.0001, 0.001, 0.01, 0.1],\n    \"penalty\" : [\"l2\", \"l1\", \"none\"],\n}\n\nsgd_model = SGDClassifier()\nclf_sgd=RandomizedSearchCV(sgd_model,sgd_params,cv=cv, scoring='roc_auc',n_jobs=1)\n\nclf_sgd.fit(X_train, y_train)\nclf_sgd.best_estimator_\n","9cd6fed1":"#Actual data\n#clf_gb.fit(X_train, y_train)\n#clf_gb.best_estimator_\ntest_eval(clf_sgd, X_test, y_test, 'SGDClassifier', 'actual')\n#smote\nclf_sgd.fit(X_train_sm, y_train_sm)\nclf_sgd.best_estimator_\ntest_eval(clf_sgd, X_test, y_test, 'SGDClassifier', 'Smote')\n\n#Adasyn\nclf_sgd.fit(X_train_ada, y_train_ada)\nclf_sgd.best_estimator_\ntest_eval(clf_sgd, X_test, y_test, 'SGDClassifier', 'adasyn')\n\n#smote + tomek\nclf_sgd.fit(X_train_smtom, y_train_smtom)\nclf_sgd.best_estimator_\ntest_eval(clf_sgd, X_test, y_test, 'SGDClassifier', 'smote+tomek')\n\n#smote + enn\nclf_sgd.fit(X_train_smenn, y_train_smenn)\nclf_sgd.best_estimator_\ntest_eval(clf_sgd, X_test, y_test, 'SGDClassifier', 'smote+enn')\n\n#smote + rus\nclf_sgd.fit(X_train_smrus, y_train_smrus)\nclf_sgd.best_estimator_\ntest_eval(clf_sgd, X_test, y_test, 'SGDClassifier', 'smote+rus')\n","20b0ba0c":"import lightgbm as lgbm","0c71247f":"\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\n\nlgb_model = lgbm.LGBMClassifier()\nlgb_params ={'num_leaves': sp_randint(6, 50), \n             'min_child_samples': sp_randint(100, 200), \n             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n             'subsample': sp_uniform(loc=0.2, scale=0.8), \n             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100],\n             'bagging_fraction': sp_uniform(0.5, 0.8),\n             #'bagging_frequency': sp_randint(5, 8),\n             'feature_fraction': sp_uniform(0.5, 0.8),\n             'max_depth': sp_randint(10, 13),\n             'min_data_in_leaf': sp_randint(50, 80),}\n#clf = lgb.LGBMClassifier(max_depth=-1, random_state=314, silent=True, metric='None', n_jobs=4, n_estimators=5000)\nclf_lgb=RandomizedSearchCV(lgb_model,lgb_params,cv=cv, scoring='roc_auc',n_jobs=1)\n\nclf_lgb.fit(X_train, y_train)\nclf_lgb.best_estimator_\n","87c5c884":"#Actual data\n#clf_gb.fit(X_train, y_train)\n#clf_gb.best_estimator_\ntest_eval(clf_lgb, X_test, y_test, 'LGBMClassifier', 'actual')\n#smote\nclf_lgb.fit(X_train_sm, y_train_sm)\nclf_lgb.best_estimator_\ntest_eval(clf_lgb, X_test, y_test, 'LGBMClassifier', 'Smote')\n\n#Adasyn\nclf_lgb.fit(X_train_ada, y_train_ada)\nclf_lgb.best_estimator_\ntest_eval(clf_lgb, X_test, y_test, 'LGBMClassifier', 'adasyn')\n\n#smote + tomek\nclf_lgb.fit(X_train_smtom, y_train_smtom)\nclf_lgb.best_estimator_\ntest_eval(clf_lgb, X_test, y_test, 'LGBMClassifier', 'smote+tomek')\n\n#smote + enn\nclf_lgb.fit(X_train_smenn, y_train_smenn)\nclf_lgb.best_estimator_\ntest_eval(clf_lgb, X_test, y_test, 'LGBMClassifier', 'smote+enn')\n\n#smote + rus\nclf_lgb.fit(X_train_smrus, y_train_smrus)\nclf_lgb.best_estimator_\ntest_eval(clf_lgb, X_test, y_test, 'LGBMClassifier', 'smote+rus')\n","8c3ec6c8":"from sklearn.neural_network import MLPClassifier\nclf_mlp  = MLPClassifier()\nclf_mlp.fit(X_train, y_train)\n","25434bf4":"#Actual data\n#clf_gb.fit(X_train, y_train)\n#clf_gb.best_estimator_\ntest_eval(clf_mlp, X_test, y_test, 'MLPClassifier', 'actual')\n#smote\nclf_mlp.fit(X_train_sm, y_train_sm)\n#clf_mlp.best_estimator_\ntest_eval(clf_mlp, X_test, y_test, 'MLPClassifier', 'Smote')\n\n#Adasyn\nclf_mlp.fit(X_train_ada, y_train_ada)\n#clf_mlp.best_estimator_\ntest_eval(clf_mlp, X_test, y_test, 'MLPClassifier', 'adasyn')\n\n#smote + tomek\nclf_mlp.fit(X_train_smtom, y_train_smtom)\n#clf_mlp.best_estimator_\ntest_eval(clf_mlp, X_test, y_test, 'MLPClassifier', 'smote+tomek')\n\n#smote + enn\nclf_mlp.fit(X_train_smenn, y_train_smenn)\n#clf_mlp.best_estimator_\ntest_eval(clf_mlp, X_test, y_test, 'MLPClassifier', 'smote+enn')\n\n#smote + rus\nclf_mlp.fit(X_train_smrus, y_train_smrus)\n#clf_mlp.best_estimator_\ntest_eval(clf_mlp, X_test, y_test, 'MLPClassifier', 'smote+rus')\n","9760341d":"clf_eval_df = pd.DataFrame({'model':model,\n                            'resample':resample,\n                            'precision':precision,\n                            'recall':recall,\n                            'f1-score':F1score,\n                            'AUC-ROC':AUCROC})\nclf_eval_df","cf8615dd":"sns.set(font_scale=1.2)\n#sns.palplot(sns.color_palette())\ng = sns.FacetGrid(clf_eval_df, col=\"model\", height=5)\ng.map(sns.barplot, \"resample\", \"precision\", palette='twilight', order=[\"actual\", \"smote\", \"adasyn\", \"smote+tomek\", \"smote+enn\",\"smote+rus\"])\ng.set_xticklabels(rotation=30)\ng.set_xlabels(' ', fontsize=14)\ng.set_ylabels('Precision', fontsize=14)","44fab27a":"#Meaning of each individual features.\nenrollee_id : Unique ID for candidate\n\ncity: City code\n\ncity_ development _index : Developement index of the city (scaled)\n\ngender: Gender of candidate\n\nrelevent_experience: Relevant experience of candidate\n\nenrolled_university: Type of University course enrolled if any\n\neducation_level: Education level of candidate\n\nmajor_discipline :Education major discipline of candidate\n\nexperience: Candidate total experience in years\n\ncompany_size: No of employees in current employer's company\n\ncompany_type : Type of current employer\n\nlastnewjob: Difference in years between previous job and current job\n\ntraining_hours: training hours completed\n\ntarget: 0 \u2013 Not looking for job change, 1 \u2013 Looking for a job change\n","1c7c74e6":"### 3.ADASYN Resampling","40a2f6d4":"### 1. Original Unsampled Data","78cf132e":"Here we are seeing that our dataset is imbalanced we have to fix this before modeling.  ","28068b06":"### 3.ADASYN Resampling","119c0c95":"### 2.SMOTE Resampling","980feb0a":"### 6. SMOTE + Under Sampling","72ca60ab":"Observations\n1. Candidate who are enrolled in a part time courese are less interested in changing job.\n\n2. Candidate emrolled in a university and having 1 years experience are mostly interesrted in changing job.","c38e46b1":"### 1. Original Unsampled Data","1b14dfbe":"### C.2) SMOTE + ENN\n","4a4d3abe":"# Model-4: AdaBoast","a8edacd4":"Nothing to say about this graph.","ccfbf6fb":"Observations\n\n1. Working in various types of company but having 1 years experienced candudate are mostly interested \nin changing job.\n\n2. And Candidate Working in NGO and Public Sector having experience of 5 years also partly interested in changing job.","fd530b9c":"### 6. SMOTE + Under Sampling","1538fe2d":"#### C.1) SMOTE + Tomek Links","2f77ba7f":"# Encoding The columns","d900fbca":"Make Obserbations.\n\n1. People ranging  experience from 1 to 10 years are most likely to change.\n2. People  having experience of around 20 years are not looking to change the job.","c85926b1":"# Model Comparision","c9113890":"# Introduction","178be66a":"As education_level is an ordinal categorical features that's why we have to map the each lavel with keeping order in mind.\n","6121e839":"### 5. SMOTE + ENN Resampling","6fef274b":"\n# Model-5: GradientBoast","ce121160":"Observations:\n    \n1. male Graduate candidate having no_enrollment are much more\ninterested in job then part time and Full time. Same cases for Female also.\n\n2. Tree structure based algorithm could be handy for this particuler case.","a4cbdd1c":"Before Dig into the main analysing process process Let's get familliar with the datasets.\n","adf4c22e":"### 2.SMOTE Resampling","cbf6cdcc":"Here one interesting things to notice is all the Numerical features are not contains Null values.","b73111c9":"Let's Make Some obsevations from the visualizations.\n\n1. Almost 90% people are male who were takong that course.\n\n2. Almost 87% candidate has realvent experience in Data Science and rest of them don't have any\nexperience but interasted in this field.\n\n3. Almost 85% candidate were enrolled in University.\n\n4. ALmost 70% guy were graduate and interesting is around 3% phd guy also there.\n\n5. It's natural that most of the candidate will be form STEM background. But From rest of the groups\ncandidates from humanities are interested more.\n\n6. And most of the guys are from private sector.","3e63749e":"### 4.SMOTE + Tomek Resampling","d3c8045a":"# Model Building","c389005d":"### 2.SMOTE Resampling","cd3ba864":"Here we are seeing that there are > and < sign with 20 and 1. So before Modeling let's solve this by adding and Substracting 1  with 20 and 1 ","88d394a8":"Here one interesting point to notice is Female have a higher training hours then others.","b1448bbd":"### 5.SMOTE + ENN Resampling","36cb174f":"Obsevations\n\n1. Female who want to change job have a much more higher training\nhours then others and Education_level is PHD and much more experience also.\n\n2. And in Others ccategory only graduate are wanted to change job.","a73ba416":"# Model-3: Random Forest","399838af":"### B) ADASYN Technique","5e975c73":"# Model-2: Decision Tree","ed580d2d":"### 4. SMOTE + Tomek Resampling","581ec865":"Make Obsebations.\n\n1. Peoples city_development_index ranging in around .666 and and around .9 are interested in changing.\n\n2. Peoples are toatally not interested in job change whose city_development_index are nearly .9","f01f2cb4":"observations:\n\n1. Female graduate candidates are more in numbers in changing job.\n\n2. Same case for male also as graduate completed students have a intent for a job.","ee104cfb":"Obervations:\n    \n1. Who are wanted to change the job their training hour is less then \nwho don't want in all three catrgory.\n\n2. Who are wanted to change the job have less experience then is  then \nwho don't want in all three catrgory.\n ","69c0f8ef":"# Handling imbalance data using SMOTE based techniques","1707374a":"### A) SMOTE Technique\u00b6","0edee5d6":"### 3.ADASYN Resampling","977df999":"Let's try to explore Numerical coulumns.And have their distribution with respect to target columns.\n","b3019b9f":"I don't think enrolle_id, city, company_size will be handy to take in our process.So drop this three columns. ","15221296":"### C) Hybrid Techniques\n","6979a0d4":"### 6. SMOTE + Under Sampling","b172f3fe":"Let's make some observations.\n\nHere we are seeing that in each Chart any one class having majority of the data points.That's why we cann't\ncompare confidently. So here ratio could be a good factor to compare.\n\n1. Though something to see is even people from public sector are also getting interest in Data Science.\n\n2. Peopler are from Arts background are completly not interested in switching job.\n","d05590a4":"Here we are seeing that most of the features that contains missing values are categorical \nand we can fill those value by mode , but this could leads to imbalanced data and make our ml model baised\n so it's better to delete those.","80e2d892":"# Model-6: SGDClassifier","45e1bed1":"Problem Statement:\n    \nHere our objective is to build a model which will predict wheather a person is trying to change his\/her job or not? ","e2a985ca":"# Model-1: Logistic Regression","932b11be":"### 4. SMOTE + Tomek Resampling","1dd53c0c":"make obserbations\n\n1.People left their last job from 1 to 2 years ago are most likely to change job\n and also same for who are not wanted to change job.","f97b0c8a":"### 1. Original Unsampled Data****","23b5093a":"Let's see how all the categorical features effecting in target variable.","bf24f942":"### 5.SMOTE + ENN Resampling","03c463a6":"# Model-7: LGBMClassifier","60c438c1":"# Model-8: MLPClassifier","7d881af7":"### C.3) SMOTE + Under Sampling "}}