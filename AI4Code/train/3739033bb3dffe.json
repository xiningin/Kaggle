{"cell_type":{"17362c5d":"code","c05324e1":"code","ed6052a6":"code","c337a7b5":"code","08cb2b75":"code","a426e9b8":"code","9f509edf":"code","86f26ff0":"code","c9fef49e":"code","8ae50509":"code","057e07c4":"code","e53ebcf8":"code","ec261936":"code","5f310aad":"code","047214b1":"code","ac271e6e":"code","2009615e":"code","0737a614":"markdown","f5d90735":"markdown","d273a31e":"markdown","4caa4429":"markdown","09d2cc37":"markdown","68af11c3":"markdown","d507886d":"markdown"},"source":{"17362c5d":"# Import All Necessary Packages\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nimport torchaudio\nimport torch\nimport torch.nn as nn\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom tabulate import tabulate # tabulate print\nfrom tqdm import tqdm\nfrom torch.utils.tensorboard import SummaryWriter\nimport warnings\nwarnings.filterwarnings(\"ignore\") # Ignore All Warnings","c05324e1":"df = pd.read_csv(\"..\/input\/urbansound8k\/UrbanSound8K.csv\")\ndf.head()","ed6052a6":"wave = torchaudio.load(\"..\/input\/urbansound8k\/fold1\/102106-3-0-0.wav\")\nplt.plot(wave[0].t().numpy())\nprint(wave[0].shape) # torch.Size([2, 72324]) 2 channels, 72324 sample_rate","c337a7b5":"class AudioDataset:\n    def __init__(self, file_path, class_id):\n        self.file_path = file_path\n        self.class_id = class_id\n        \n    def __len__(self):\n        return len(self.file_path)\n    \n    def __getitem__(self, idx):\n        path = self.file_path[idx]\n        waveform, sr = torchaudio.load(path, normalization=True) # load audio\n        audio_mono = torch.mean(waveform, dim=0, keepdim=True) # Convert sterio to mono\n        tempData = torch.zeros([1, 160000])\n        if audio_mono.numel() < 160000: # if sample_rate < 160000\n            tempData[:, :audio_mono.numel()] = audio_mono\n        else:\n            tempData = audio_mono[:, :160000] # else sample_rate 160000\n        audio_mono=tempData\n        mel_specgram = torchaudio.transforms.MelSpectrogram(sr)(audio_mono) # (channel, n_mels, time)\n        mel_specgram_norm = (mel_specgram - mel_specgram.mean()) \/ mel_specgram.std() # Noramalization\n        mfcc = torchaudio.transforms.MFCC(sample_rate=sr)(audio_mono) # (channel, n_mfcc, time)\n        mfcc_norm = (mfcc - mfcc.mean()) \/ mfcc.std() # mfcc norm\n        new_feat = torch.cat([mel_specgram, mfcc], axis=1)\n        \n        return {\n            \"specgram\": torch.tensor(new_feat[0].permute(1, 0), dtype=torch.float),\n            \"label\": torch.tensor(self.class_id[idx], dtype=torch.long)\n        }","08cb2b75":"# device check\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef collate_fn(data):\n    specs = []\n    labels = []\n    for d in data:\n        spec = d[\"specgram\"].to(device)\n        label = d[\"label\"].to(device)\n        specs.append(spec)\n        labels.append(label)\n    spec = torch.nn.utils.rnn.pad_sequence(specs, batch_first=True, padding_value=0.)\n    labels = torch.tensor(labels)\n    return spec, labels\n\n\nFILE_PATH = \"..\/input\/urbansound8k\/\"\n\nif __name__ == \"__main__\":\n    df = pd.read_csv(\"..\/input\/urbansound8k\/UrbanSound8K.csv\")\n    files = df[\"slice_file_name\"].values.tolist()\n    folder_fold = df[\"fold\"].values\n    label = df[\"classID\"].values.tolist()\n    path = [\n        os.path.join(FILE_PATH + \"fold\" + str(folder) + \"\/\" + file) for folder, file in zip(folder_fold, files)\n    ]\n    \n    X_train, X_test, y_train, y_test = model_selection.train_test_split(path, label, random_state=42, test_size=0.3)\n    \n    train_dataset = AudioDataset(\n        file_path=X_train,\n        class_id=y_train\n    )\n    \n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=128, shuffle=True, drop_last=True, collate_fn=collate_fn\n    )\n    \n    test_dataset = AudioDataset(\n        file_path=X_test,\n        class_id=y_test\n    )\n    \n    test_loader = torch.utils.data.DataLoader(\n        test_dataset, batch_size=128, shuffle=False, drop_last=True, collate_fn=collate_fn\n    )","a426e9b8":"# model\nclass AudioLSTM(nn.Module):\n\n    def __init__(self, n_feature=5, out_feature=5, n_hidden=256, n_layers=2, drop_prob=0.3):\n        super().__init__()\n        self.drop_prob = drop_prob\n        self.n_layers = n_layers\n        self.n_hidden = n_hidden\n        self.n_feature = n_feature\n        self.lstm = nn.LSTM(self.n_feature, self.n_hidden, self.n_layers, dropout=self.drop_prob, batch_first=True)\n        self.dropout = nn.Dropout(drop_prob)\n        self.relu = nn.ReLU()\n        self.fc1 = nn.Linear(int(n_hidden), int(n_hidden\/2))\n        self.fc2 = nn.Linear(int(n_hidden\/2), out_feature)\n\n    def forward(self, x, hidden):\n        # x.shape (batch, seq_len, n_features)\n        l_out, l_hidden = self.lstm(x, hidden)\n        # out.shape (batch, seq_len, n_hidden*direction)\n        out = self.dropout(l_out)\n        # out.shape (batch, out_feature)\n        out = self.fc1(out)\n        out = self.fc2(out[:, -1, :])\n#         print(out.shape)\n        # return the final output and the hidden state\n        return out, l_hidden\n\n    def init_hidden(self, batch_size):\n        weight = next(self.parameters()).data\n        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n#         print(hidden[0].shape)\n        return hidden","9f509edf":"AudioLSTM()","86f26ff0":"# Tensorboard\nfrom torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter()","c9fef49e":"def save_model(state, filename):\n    torch.save(state, filename)\n    print(\"-> Model Saved\")","8ae50509":"# Train Set\ndef train(data_loader, model, epoch, optimizer, device):\n    losses = []\n    accuracies = []\n    labels = []\n    preds = []\n    model.train()\n    loop = tqdm(data_loader) # for progress bar\n    for batch_idx, (data, target) in enumerate(loop):\n        data = data.to(device)\n        target = target.to(device)\n        model.zero_grad()\n        output, hidden_state = model(data, model.init_hidden(128))\n        loss = nn.CrossEntropyLoss()(output, target)\n        loss.backward()\n        optimizer.step()\n        losses.append(loss.item())\n        probs = torch.softmax(output, dim=1)\n        winners = probs.argmax(dim=1)\n        corrects = (winners == target)\n        accuracy = corrects.sum().float() \/ float(target.size(0))\n        accuracies.append(accuracy)\n        labels += torch.flatten(target).cpu()\n        preds += torch.flatten(winners).cpu()\n        loop.set_description(f\"EPOCH: {epoch} | ITERATION : {batch_idx}\/{len(data_loader)} | LOSS: {loss.item()} | ACCURACY: {accuracy}\")\n        loop.set_postfix(loss=loss.item())\n        \n    avg_train_loss = sum(losses) \/ len(losses)\n    avg_train_accuracy = sum(accuracies) \/ len(accuracies)\n    report = metrics.classification_report(torch.tensor(labels).numpy(), torch.tensor(preds).numpy())\n    print(report)\n    return avg_train_loss, avg_train_accuracy","057e07c4":"# Test Setting\ndef test(data_loader, model, optimizer, device):\n    model.eval()\n    accs = []\n    preds = []\n    labels = []\n    test_accuracies = []\n    with torch.no_grad():\n        loop = tqdm(data_loader) # Test progress bar\n        for batch_idx, (data, target) in enumerate(loop):\n            data = data.to(device)\n            target = target.to(device)\n            output, hidden_state = model(data, model.init_hidden(128))\n            probs = torch.softmax(output, dim=1)\n            winners = probs.argmax(dim=1)\n            corrects = (winners == target)\n            accuracy = corrects.sum().float() \/ float(target.size(0))\n            \n            test_accuracies.append(accuracy)\n            labels += torch.flatten(target).cpu()\n            preds += torch.flatten(winners).cpu()\n    avg_test_acc = sum(test_accuracies) \/ len(test_accuracies)\n    return avg_test_acc","e53ebcf8":"# Epoch\nEPOCH = 50\nOUT_FEATURE = 10 # class\nPATIENCE = 5","ec261936":"def main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = AudioLSTM(n_feature=168, out_feature=OUT_FEATURE).to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"max\", patience=PATIENCE)\n    \n    best_train_acc, best_epoch = 0, 0 # update acc and epoch\n    \n    for epoch in range(EPOCH):\n        avg_train_loss, avg_train_acc = train(train_loader, model, epoch, optimizer, device)\n        avg_test_acc = test(test_loader, model, optimizer, device)\n        scheduler.step(avg_train_acc)\n        if avg_train_acc > best_train_acc:\n            best_train_acc = avg_train_acc\n            best_epoch = epoch\n            filename = f\"best_model_at_epoch_{best_epoch}.pth.tar\"\n            checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n            save_model(checkpoint, filename)\n        \n        table = [\n            [\"avg_train_loss\", avg_train_loss], [\"avg_train_accuracy\", avg_train_acc],\n            [\"best_train_acc\", best_train_acc], [\"best_epoch\", best_epoch]\n        ]\n        print(tabulate(table)) # tabulate View\n        test_table = [\n            [\"Avg test accuracy\", avg_test_acc]\n        ]\n        writer.add_scalar('Loss\/train', avg_train_loss, epoch)\n        writer.add_scalar('Accuracy\/train', avg_train_acc, epoch)\n        writer.add_scalar('Accuracy\/test', avg_test_acc, epoch)\n        print(tabulate(test_table)) # tabulate View\n\nif __name__ == \"__main__\":\n    main() # Run function","5f310aad":"bbjhjasdhjahsdjhasvvbvbhjhjhjhjhjhjghghghghfsdf","047214b1":"unique_label = dict(zip(df[\"classID\"], df[\"class\"]))\nprint(unique_label)","ac271e6e":"waveform, sr = torchaudio.load(\"..\/input\/urbansound8k\/fold2\/100652-3-0-3.wav\")\naudio_mono = torch.mean(waveform, dim=0, keepdim=True)\ntempData = torch.zeros([1, 160000])\nif audio_mono.numel() < 160000:\n    tempData[:, :audio_mono.numel()] = audio_mono\nelse:\n    tempData = audio_mono[:, :160000]\naudio_mono=tempData\nmel_specgram = torchaudio.transforms.MelSpectrogram(sr)(audio_mono)\nmel_specgram_norm = (mel_specgram - mel_specgram.mean()) \/ mel_specgram.std()\nmfcc = torchaudio.transforms.MFCC(sample_rate=sr)(audio_mono)\n#         print(f'mfcc {mfcc.size()}')\nmfcc_norm = (mfcc - mfcc.mean()) \/ mfcc.std()\nnew_feat = torch.cat([mel_specgram, mfcc], axis=1)\n\ndata = torch.utils.data.DataLoader(new_feat.permute(0, 2, 1))\nnew = torch.load(\".\/best_model_at_epoch_49.pth.tar\", map_location=torch.device(\"cpu\"))[\"state_dict\"]\nmodel = AudioLSTM(n_feature=168, out_feature=OUT_FEATURE)\nmodel.load_state_dict(new)\nmodel.eval().cpu()\nwith torch.no_grad():\n    for x in data:\n        x = x.to(\"cpu\")\n        output, hidden_state = model(x, (torch.zeros(2, 1, 256), torch.zeros(2, 1, 256)))\n        for i, v in unique_label.items():\n            if np.argmax(output.numpy()) == i:\n                print(f\"Predicted Label : {v}\")","2009615e":"PATH = \"..\/input\/urbansound8k\/fold2\"\n    \nfold2 = os.listdir(\"..\/input\/urbansound8k\/fold2\")[:100]\nfor i in range(len(fold2)):\n    class_name = df[df[\"slice_file_name\"]==fold2[i]]['class']\n    for item, value in class_name.items():\n        print(f\"Actual output : {value}\")\n    waveform, sr = torchaudio.load(os.path.join(PATH, fold2[i]))\n    audio_mono = torch.mean(waveform, dim=0, keepdim=True)\n    tempData = torch.zeros([1, 160000])\n    if audio_mono.numel() < 160000:\n        tempData[:, :audio_mono.numel()] = audio_mono\n    else:\n        tempData = audio_mono[:, :160000]\n    audio_mono=tempData\n    mel_specgram = torchaudio.transforms.MelSpectrogram(sr)(audio_mono)\n    mel_specgram_norm = (mel_specgram - mel_specgram.mean()) \/ mel_specgram.std()\n    mfcc = torchaudio.transforms.MFCC(sample_rate=sr)(audio_mono)\n    #         print(f'mfcc {mfcc.size()}')\n    mfcc_norm = (mfcc - mfcc.mean()) \/ mfcc.std()\n    new_feat = torch.cat([mel_specgram, mfcc], axis=1)\n\n    data = torch.utils.data.DataLoader(new_feat.permute(0, 2, 1))\n    new = torch.load(\".\/best_model_at_epoch_49.pth.tar\", map_location=torch.device(\"cpu\"))[\"state_dict\"]\n    model = AudioLSTM(n_feature=168, out_feature=OUT_FEATURE)\n    model.load_state_dict(new)\n    model.eval().cpu()\n    with torch.no_grad():\n        for x in data:\n            x = x.to(\"cpu\")\n            output, hidden_state = model(x, (torch.zeros(2, 1, 256), torch.zeros(2, 1, 256)))\n            for i, v in unique_label.items():\n                if np.argmax(output.numpy()) == i:\n                    print(f\"Predicted Label : {v}\")","0737a614":"# Read The csv file.\n## Print First 5 row","f5d90735":"# Process Audio Files That return specgram and label","d273a31e":"![360_F_368444653_m1NG6V8BGo9vSDXk8ClHWIHKyfzOpU4S.jpg](attachment:ed40c307-4422-48cf-a664-9eed4b22f235.jpg)","4caa4429":"# What is AdamW\n> Optimizer that implements the Adam algorithm with weight decay.\n\n# Is AdamW better than Adam?\n\n> The authors show experimentally that AdamW yields better training loss and that the models generalize much better than models trained with Adam allowing the new version to compete with stochastic gradient descent with momentum.\n","09d2cc37":"# Save Model","68af11c3":"# for single output test","d507886d":"# for a fold test"}}