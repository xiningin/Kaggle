{"cell_type":{"3904ab3e":"code","d8020df3":"code","284345ae":"code","4818a5b6":"code","4c1b4419":"code","424e482c":"code","208e56f0":"code","815c26fa":"markdown","33eab168":"markdown","960f22c6":"markdown","5f9b42e1":"markdown","a7757c35":"markdown"},"source":{"3904ab3e":"# init\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport numpy as np\nimport pandas as pd\npd.options.mode.chained_assignment = None  # default='warn'\nimport matplotlib.pyplot as plt\nimport time\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import cohen_kappa_score","d8020df3":"# read data\ndf = pd.read_csv(\"..\/input\/train\/train.csv\")\n\n# using all variables for a basic model\nXvars = ['Type', 'Age', 'Breed1', 'Breed2', 'Gender', 'Color1', 'Color2', 'Color3', 'MaturitySize', 'FurLength',\n         'Vaccinated', 'Dewormed', 'Sterilized', 'Health', 'Quantity', 'Fee', 'State', 'VideoAmt', 'PhotoAmt']\n\n# basic rf model\nimport lightgbm as lgb\nfrom functools import partial\nimport scipy as sp\n\nparams = {}\nparams['objective'] = 'regression'\nparams['boosting_type'] = 'rf'\n\nparams['min_data_in_leaf'] = 1\nparams['feature_fraction'] = 0.99\nparams['bagging_fraction'] = 0.4\nparams['bagging_freq'] = 50\n\nparams['num_threads'] = 4","284345ae":"# improved\nclass OptimizedRounder_v2(object):\n    def __init__(self):\n        self.coef_ = 0\n    \n    def _kappa_loss(self, coef, X, y):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        return -cohen_kappa_score(y, preds, weights = 'quadratic')\n    \n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X = X, y = y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method = 'nelder-mead')\n    \n    def predict(self, X, coef):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3, 4])\n        return preds\n    \n    def coefficients(self):\n        return self.coef_['x']","4818a5b6":"# original version\nclass OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n        \n        print(coef)\n        ll = cohen_kappa_score(y, X_p, weights = 'quadratic')\n        return -ll\n\n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            else:\n                X_p[i] = 4\n        return X_p\n\n    def coefficients(self):\n        return self.coef_['x']","4c1b4419":"# full optimizer output\ndef test():\n    cv = KFold(n_splits = 5, random_state = 0)\n    for train, val in cv.split(df):\n        lgbDf = lgb.Dataset(df.iloc[train][Xvars], df.iloc[train].AdoptionSpeed)\n        model = lgb.train(params, lgbDf, 1001)\n\n        optR = OptimizedRounder()\n        optR.fit(model.predict(df.iloc[train][Xvars]), df.iloc[train].AdoptionSpeed)\n        coeff = optR.coefficients()\n        break\n\nstart = time.time()\ntest()\nfinish = time.time()","424e482c":"# original\nprint(finish - start)","208e56f0":"# improved\ndef test_v2():\n    cv = KFold(n_splits = 5, random_state = 0)\n    for train, val in cv.split(df):\n        lgbDf = lgb.Dataset(df.iloc[train][Xvars], df.iloc[train].AdoptionSpeed)\n        model = lgb.train(params, lgbDf, 1001)\n\n        optR = OptimizedRounder_v2()\n        optR.fit(model.predict(df.iloc[train][Xvars]), df.iloc[train].AdoptionSpeed)\n        coeff = optR.coefficients()\n        break\n\nstart = time.time()\ntest_v2()\nfinish = time.time()\nprint(finish - start)","815c26fa":"## What next?\n\nHow do we best initialize the coefficients? The optimizer seems very sensitive to how the coefficients are initialized.\nI don't have good insights on how nelder-mead works, but it seems to lock on to local optima.\nHow could we best direct it to arrive at a more stable global optimum?\n(One great idea here from Daniel Dewey: https:\/\/www.kaggle.com\/dan3dewey\/baseline-random-forest-with-get-class-bounds)\n\nIn an ideal world, we should arrive at the same final coefficients irrespective of how they were initalized.\nIf you've given this any thought, how are you approaching it?\n\nWould love to hear your thoughts and feedback.","33eab168":"## Speed comparison:","960f22c6":"# What are the improvements?\n\n### 1. The pd.cut() function does the job of mapping outputs to [0,1,2,3,4] in a very neat way, runs 2.5x faster\n> [-np.inf] + list(np.sort(coef)) + [np.inf]\n**      -->   This is because the outer bound conditions need to be specified**\n\n### 2. Solved a potential bug (explained below)\n* The original optimizer did not always preserve the order of the 4 coefficient values\n* Improved version always sorts coefficients before mapping prediction output to [0,1,2,3,4] (conveniently, pd.cut() would fail if the coefficients weren't in ascending order)\n* The final prediction kappa is not affected majorly, the optimizer still manages to converge well","5f9b42e1":"This kernel is to demostrate an improved version of the OptimizedRounder function initially shared by Abhishek: https:\/\/www.kaggle.com\/c\/petfinder-adoption-prediction\/discussion\/76107","a7757c35":"## The bug in original version:\nBelow is a print of each iteration of the optimizer.\nWhere highlighted, you see that the coefficients are not in ascending order anymore.\n\n`coef[0] < coef[1] < coef[2] < coef[3]` should be true in all cases\n\nBut in the highlighted lines, `coef[2] > coef[3]` which is not correct.\n\nThis then affects the step where values are mapped to [0,1,2,3,4] based on a for-loop if-else sequence.\nThe kappa metric calculation gets thrown off course, and then confuses the optimizer in further iterations.\n\n![](https:\/\/i.imgur.com\/FBEly8l.png)"}}