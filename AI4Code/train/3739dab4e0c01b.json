{"cell_type":{"4208629c":"code","16e4aa2c":"code","88807173":"code","7a1aab7d":"code","df8cf681":"code","b035af26":"code","9e751771":"code","c6b0b014":"code","1a75a36f":"code","56fa768c":"code","05adb2a3":"code","0a8a4863":"code","5658a6d4":"code","4a90bd0b":"code","ee566271":"code","794c8617":"code","689c7008":"code","b0b49b0b":"code","077b4f19":"code","32c787ab":"code","20082686":"code","da3e9c9e":"code","d97aeb03":"code","4ce7720d":"code","6f9cdc85":"code","348af91b":"code","82d80b89":"code","36849101":"code","3d52d789":"code","96c3a59e":"code","7d35e1c6":"code","291bebc5":"code","2b488a8e":"code","2abbb800":"code","8010ed05":"code","09f3fb39":"code","51631bdc":"code","ee65cb02":"code","dc7c43ab":"code","e2f055dd":"code","9e049b1f":"code","5807f4b0":"code","853be30a":"code","78324fd8":"code","5d18e93e":"code","8dd1e273":"code","c3d60d33":"code","e46d1e5f":"markdown","07020938":"markdown","a79635a3":"markdown","e35ab9c3":"markdown","6907b087":"markdown","f08d7eaf":"markdown","ac6cc46c":"markdown","56dc113f":"markdown","bffb406d":"markdown","0a5ab92d":"markdown","0e4245f9":"markdown","6070bd04":"markdown","9d17ea9c":"markdown","3b32315a":"markdown","fa773a49":"markdown","17d6a4bb":"markdown","2b214bbd":"markdown","22837c66":"markdown","c15ac737":"markdown","053d5d16":"markdown","f4d14f8b":"markdown","0313235e":"markdown","70cc499e":"markdown","9d3f732b":"markdown"},"source":{"4208629c":"# Input data files are available in the \"..\/input\/\" directory.\n\nimport os\nprint(os.listdir(\"..\/input\"))\n# Any results you write to the current directory are saved as output.","16e4aa2c":"import pandas as pd\nimport copy\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.formula.api as smf\nfrom sklearn.preprocessing import PolynomialFeatures\nimport random\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.preprocessing import MinMaxScaler\nimport csv","88807173":"odf = pd.read_csv('..\/input\/diamonds\/diamonds.csv')","7a1aab7d":"odf.head()","df8cf681":"# Dropping first column of dataframe\nodf.drop(columns=odf.columns[0], axis = 1, inplace=True)\nodf.head(10)","b035af26":"# Categorizing columns according to data types\ncategorical = odf.columns[[1,2,3]]\nto_predict = 'price'\nnumeric = odf.columns[[0,4,5,6,7,8,9]]","9e751771":"odf.describe()","c6b0b014":"# Replacing zero values of x, y and z with NaNs. After then NaN will be dropped\n\nodf[['x','y','z']] = odf[['x','y','z']].replace(0,np.NaN)\n\nodf.dropna(inplace=True)","1a75a36f":"# Checking for missing values in dataset\nfor c in odf.columns:\n    print('Total Missing values in \\'{}\\' are {}.'.format(c,odf[c].isna().sum()))","56fa768c":"# For more details kindly refer to my previous notebook. Link provided at top.\nodf.drop(odf[odf.y > 20].index, inplace=True) # Dropping outliers from 'y' column\nodf.drop(odf[odf.z > 20].index, inplace=True) # Dropping outliers from 'z' column\n\nplt.figure(figsize=(15,12))\nfor i in range(1,8):\n    plt.subplot(3, 3, i)\n    plt.scatter(odf['price'], odf[numeric[i-1]], s= 1)\n    plt.xlabel(to_predict)\n    plt.ylabel(numeric[i-1])","05adb2a3":"h = odf.hist(figsize=(15,10))","0a8a4863":"df_length = odf['carat'].count()\ncarat_count = odf['carat'][odf.carat > 3.0].count()\nprint('Only {}% data i.e. {} in number, is greater than 3.0 for carat column'.format(np.round((carat_count\/df_length)*100, 2), carat_count))\n\ntable_count = odf['table'][odf.table > 70.0].count()\nprint('Only {}% data i.e. {} in number, is greater than 70.0 for table column'.format(np.round((table_count\/df_length)*100, 2), table_count))\n\nx_count = odf['x'][odf.x > 8.5].count()\nprint('Only {}% data i.e. {} in number, is greater than 8.5 for x column'.format(np.round((x_count\/df_length)*100, 2), x_count))\n\ny_count = odf['y'][odf.y > 8.5].count()\nprint('Only {}% data i.e. {} in number, is greater than 8.5 for y column'.format(np.round((y_count\/df_length)*100, 2), y_count))\n\nz_count = odf['z'][odf.z > 5.2].count()\nprint('Only {}% data i.e. {} in number, is greater than 5.0 for z column'.format(np.round((z_count\/df_length)*100, 2), z_count))","5658a6d4":"# Dropping outliers based on histogram\nodf.drop(odf[odf.carat > 3.0].index, inplace=True) # Dropping outliers from 'carat' column\nodf.drop(odf[odf.table > 70.0].index, inplace=True) # Dropping outliers from 'table' column\nodf.drop(odf[odf.x > 8.5].index, inplace=True) # Dropping outliers from 'x' column\nodf.drop(odf[odf.y > 8.5].index, inplace=True) # Dropping outliers from 'y' column\nodf.drop(odf[odf.z > 5.2].index, inplace=True) # Dropping outliers from 'z' column","4a90bd0b":"h = odf.hist(figsize=(15,10))","ee566271":"# Heatmap before introducing dummy variables\np = sns.heatmap(odf.corr('spearman'), center=0, cmap = 'RdYlGn')","794c8617":"# Making dummy variables for categorical Columns\nodfd = pd.get_dummies(data=odf, columns=categorical)","689c7008":"# Dropping Extra caegoricals\nlst = ['color_D','cut_Fair','clarity_IF']\nidx = []\nfor i in lst:\n    # Removing D color from color column, Fair cut and IF clarity column\n    idx.append(odfd.columns.get_loc(i))\n    \nodfd.drop(columns=odfd.columns[idx], axis = 1, inplace=True)","b0b49b0b":"# Rearranging columns of dataframe\n\ncol = ['carat', 'depth', 'table', 'x', 'y', 'z', 'cut_Good', 'cut_Ideal', 'cut_Premium', 'cut_Very Good', 'color_E', \n       'color_F', 'color_G', 'color_H', 'color_I', 'color_J', 'clarity_I1', 'clarity_SI1', 'clarity_SI2', 'clarity_VS1',\n       'clarity_VS2', 'clarity_VVS1', 'clarity_VVS2', 'price']\n\nodfd = odfd[col]","077b4f19":"# Heatmap after introduction of dummy variables\nplt.figure(figsize=(25,12))\np = sns.heatmap(data=odfd.corr(method='spearman'), annot=True, cmap='RdYlGn', center=0)","32c787ab":"# Automating backward elimination technique\n\ndef DoBackwardElimination(the_regressor, X, y, minP2eliminate):\n    \n    assert np.shape(X)[0] == np.shape(y)[0], 'Length of X and y do not match'\n    assert minP2eliminate > 0, 'Minimum P value to eliminate cannot be zero or negative'\n    \n    original_list = list(range(0, np.shape(the_regressor.pvalues)[0]))\n    \n    max_p = 100        # Initializing with random value of maximum P value\n    i = 0\n    r2adjusted = []   # Will store R Square adjusted value for each loop\n    r2 = []           # Will store R Square value  for each loop\n    \n    previous_R2adjusted = the_regressor.rsquared_adj\n    \n    while max_p >= minP2eliminate:\n        \n        p_values = list(the_regressor.pvalues)\n        r2adjusted.append(the_regressor.rsquared_adj)\n        r2.append(the_regressor.rsquared)\n        \n        max_p = max(p_values)\n        max_p_idx = p_values.index(max_p)\n        \n        if max_p_idx == 0:\n            \n            temp_p = set(p_values)\n            \n            # removing the largest element from temp list\n            temp_p.remove(max(temp_p))\n            \n            max_p = max(temp_p)\n            max_p_idx = p_values.index(max_p)\n            \n#             print('Index value 0 found!! Next index value is {}'.format(max_p_idx))\n            \n            if max_p < minP2eliminate:\n                \n                print('Max P value found less than 0.1 with 0 index ...Loop Ends!!')\n                \n                break\n                \n        if max_p < minP2eliminate:\n            \n            print('Max P value found less than 0.1 without 0 index...Loop Ends!!')\n            \n            break\n        \n        val_at_idx = original_list[max_p_idx]\n        \n        idx_in_org_lst = original_list.index(val_at_idx)\n        \n        original_list.remove(val_at_idx)\n        \n        print('Popped column index out of original array is {} with P-Value {}'.format(val_at_idx, np.round(np.array(p_values)[max_p_idx], decimals= 4)))\n        \n        print('==================================================================================================')\n        \n        X_new = X[:, original_list]\n        \n        the_regressor = smf.OLS(endog = y, exog = X_new).fit()\n        \n        if previous_R2adjusted < the_regressor.rsquared_adj:\n            classifier_with_maxR2adjusted = the_regressor\n            final_list_of_index = copy.deepcopy(original_list)\n            previous_R2adjusted = the_regressor.rsquared_adj\n            \n    return classifier_with_maxR2adjusted, r2, r2adjusted, final_list_of_index","20082686":"def resiplot(y_original, y_predicted, delete_outlier = False, max_outlier_val = None):\n    \n    residual = y_original - y_predicted\n    residnew = list(residual.ravel())\n    \n    if delete_outlier == True:\n        assert max_outlier_val != None, 'Please insert \\'max_outlier_val\\''\n        count = 0\n        while max(residnew) > abs(max_outlier_val):\n            count = count + 1\n            residnew.remove(max(residnew))\n            \n        while min(residnew)< -abs(max_outlier_val):\n            count = count + 1\n            residnew.remove(min(residnew))\n        print('Residuals with unreal values are {} i.e. only {}% of total test data.'.format(count, np.round((count\/len(residnew)*100), 2)))\n\n    plt.scatter(x = range(0, len(residnew)), y = residnew, s = 2, c = 'R')\n    plt.plot([0,len(residnew)], [0,0], '-k')\n    if delete_outlier == True:\n        plt.ylim(-abs(max_outlier_val), abs(max_outlier_val))\n    elif abs(max(residnew)) > abs(min(residnew)):\n        plt.ylim(-max(residnew), max(residnew))\n    else:\n        plt.ylim(min(residnew), abs(min(residnew)))\n        \n    plt.title('Mean residual is {}'.format(np.round(np.mean(residnew),2)))","da3e9c9e":"# Preprocessing data\n\nX = odfd.iloc[:,:-1].values          # Selecting all columns except last one that is 'price'.\ny = odfd['price'].values\n\n# Scaling data in default range\nmscalar = MinMaxScaler()\nX_minmax_scaled = mscalar.fit_transform(X)\n\n# Adding constant values at start of array X\nX_minmax_scaled = np.append(arr = np.ones((X_minmax_scaled.shape[0], 1)).astype(int), values=X_minmax_scaled, axis=1)","d97aeb03":"# This code will be used only if doing manual elimination [Part 1]\nX_lst = list(range(0, X.shape[1]))\nX_opt = X_minmax_scaled[:,X_lst]\nflag = 2 # Done intentionally in order to stop popping 5th column from input array X","4ce7720d":"# This code will be used only if doing manual elimination [Part 2]\n# Eliminating columns according to P values from summary to make new X_opt\nidx_to_pop = 5\nif flag == 1:\n    X_lst.pop(element_to_pop)\n    flag = 99\n    \nX_opt = X_minmax_scaled[:,X_lst]","6f9cdc85":"# Splitting data for taining and testing\nX_train, X_test, y_train, y_test = train_test_split(X_opt, y, test_size=0.20, random_state=22)","348af91b":"# Fitting the regressor to simple data without polynomial features. \nregressor_SLR_OLS = smf.OLS(endog = y_train, exog = X_train).fit()\n\n# Looking at the summary of regressor\n# print(regressor_SLR_OLS.summary())\nprint('R-Squared for this regressor is {}'.format(np.round(regressor_SLR_OLS.rsquared, 3)))\nprint('R-Squared Adjusted for this regressor is {}'.format(np.round(regressor_SLR_OLS.rsquared_adj, 3)))\nprint('F- Value for this regressor is {}'.format(np.round(regressor_SLR_OLS.fvalue, 3)))\n\nresiplot(y_original=y_test, y_predicted=regressor_SLR_OLS.predict(X_test), delete_outlier=True, max_outlier_val=10000)\n\n","82d80b89":"# Function to fit polynomial features\n\ndef fit_poly(columns, degree, df):\n    \n    # Column is a list of features to be used with polynomial\n    # degree is polynomial degree\n    # df is preprocessed and clean dataframe containing relevant features and data including feature to be predicted.\n    \n    poly_f = PolynomialFeatures(degree=degree, include_bias=False, interaction_only=False)   # Bias will be included later\n    X_with_poly_features = df[columns].values # It's a numpy array\n    df_without_poly_features = df.drop(columns=columns, inplace=False) # A dataframe\n    \n    X_without_poly_features = df_without_poly_features.iloc[:,:-1].values\n    y = df_without_poly_features['price'].values\n    \n    # Fitting polynomial features on selected columns\n    X_with_poly_features = poly_f.fit_transform(X_with_poly_features) # fitting polynomial features\n\n    X = np.append(arr=X_with_poly_features, values=X_without_poly_features, axis=1) # Both matrices are appended into one\n    \n    return X, y","36849101":"# Starting with 'carat' and 'x', 'y', 'z'\ncol_for_polyfit1 = ['carat','x','y','z'] # from dataframe 'odfd'\n\n# Now including 'depth' also to previous features\ncol_for_polyfit2 = ['carat','x','y','z','depth'] # from dataframe 'odfd'\n\n# Now including 'table' also to previous features\ncol_for_polyfit3 = ['carat','x','y','z','depth','table'] # from dataframe 'odfd'\n\ndegree = 2 # starting with second degree\n\n# Fitting poly features\nX1, y = fit_poly(columns=col_for_polyfit1, degree=degree, df=odfd)\nX2, y = fit_poly(columns=col_for_polyfit2, degree=degree, df=odfd)\nX3, y = fit_poly(columns=col_for_polyfit3, degree=degree, df=odfd)\n\n# Scaling data in default range\nmscalar1 = MinMaxScaler()\nX_minmax_scaled = mscalar.fit_transform(X1)\nmscalar2 = MinMaxScaler()\nX_minmax_scaled = mscalar.fit_transform(X2)\nmscalar3 = MinMaxScaler()\nX_minmax_scaled = mscalar.fit_transform(X3)\n\n# Adding constant column to X's in order to be used with OLS regressor\nX1 = np.append(arr = np.ones((X1.shape[0], 1)).astype(int), values=X1, axis=1)\nX2 = np.append(arr = np.ones((X2.shape[0], 1)).astype(int), values=X2, axis=1)\nX3 = np.append(arr = np.ones((X3.shape[0], 1)).astype(int), values=X3, axis=1)","3d52d789":"# Splitting fitted X's and y's into testing and training sets\nX1_train, X1_test, y1_train, y1_test = train_test_split(X1, y, test_size=0.20, random_state=22)\nX2_train, X2_test, y2_train, y2_test = train_test_split(X2, y, test_size=0.20, random_state=22)\nX3_train, X3_test, y3_train, y3_test = train_test_split(X3, y, test_size=0.20, random_state=22)","96c3a59e":"# Fitting OLS regressor to training data\npoly_regressor1 = smf.OLS(endog = y1_train, exog = X1_train).fit()\npoly_regressor2 = smf.OLS(endog = y2_train, exog = X2_train).fit()\npoly_regressor3 = smf.OLS(endog = y3_train, exog = X3_train).fit()","7d35e1c6":"# Printing summary of poly_regressor1\n# print(poly_regressor1.summary())\nprint('R-Squared for this regressor is {}'.format(np.round(poly_regressor1.rsquared, 3)))\nprint('R-Squared Adjusted for this regressor is {}'.format(np.round(poly_regressor1.rsquared_adj, 3)))\nprint('F- Value for this regressor is {}'.format(np.round(poly_regressor1.fvalue, 4)))\n\ny1_test_pred = poly_regressor1.predict(X1_test)\nresiplot(y_predicted=y1_test_pred, y_original=y1_test,delete_outlier=True, max_outlier_val=10000)","291bebc5":"# Printing summary of poly_regressor2\n# print(poly_regressor2.summary())\nprint('R-Squared for this regressor is {}'.format(np.round(poly_regressor2.rsquared, 3)))\nprint('R-Squared Adjusted for this regressor is {}'.format(np.round(poly_regressor2.rsquared_adj, 3)))\nprint('F- Value for this regressor is {}'.format(np.round(poly_regressor2.fvalue, 4)))\n\ny2_test_pred = poly_regressor2.predict(X2_test)\nresiplot(y_predicted=y2_test_pred, y_original=y2_test,delete_outlier=True, max_outlier_val=10000)","2b488a8e":"# Printing summary of poly_regressor3\n# print(poly_regressor3.summary())\nprint('R-Squared for this regressor is {}'.format(np.round(poly_regressor3.rsquared, 3)))\nprint('R-Squared Adjusted for this regressor is {}'.format(np.round(poly_regressor3.rsquared_adj, 3)))\nprint('F- Value for this regressor is {}'.format(np.round(poly_regressor3.fvalue, 4)))\n\ny3_test_pred = poly_regressor3.predict(X3_test)\nresiplot(y_predicted=y3_test_pred, y_original=y3_test,delete_outlier=True, max_outlier_val=10000)","2abbb800":"# Function of saving and loading classifier using pickle\n\nimport pickle \n  \ndef StoreData(regressor, filename = 'defaultfile.clf'): \n      \n    # File is in Binary mode\n    filepath = '..\/input\/polynomial-regression-backward-elimination\/' + filename\n    \n    if os.path.exists(filepath):\n        print('File already exists!!')\n    \n    else:\n        file = open(filename, 'ab')\n        pickle.dump(regressor, file)                      \n        file.close()\n        print('File is saved by name {}'.format(filename))\n    \ndef LoadData(filename_2bloaded = 'defaultfile.clf'): \n    \n    filepath = '..\/input\/polynomial-regression-backward-elimination\/' + filename_2bloaded\n    \n    if os.path.exists(filepath):\n        file = open(filepath, 'rb')\n        regress = pickle.load(file)\n        file.close()\n    else:\n        print('File does not exists!!')\n\n    return regress","8010ed05":"degree = 7 # starting with second degree\n\ncol_for_polyfit3 = ['carat','x','y','z','depth','table'] # from dataframe 'odfd'\n\n# Fitting poly features\nX4, y = fit_poly(columns=col_for_polyfit3, degree=degree, df=odfd)\n\n# Scaling data in default range\nmscalar_new = MinMaxScaler()\nX4_minmax_scaled = mscalar_new.fit_transform(X4)\n\n\n# Adding constant column to X's in order to be used with OLS regressor\nX4_minmax_scaled = np.append(arr = np.ones((X4_minmax_scaled.shape[0], 1)).astype(int), values=X4_minmax_scaled, axis=1)\n\n# Splitting fitted X's and y's into testing and training sets\nX4_train, X4_test, y4_train, y4_test = train_test_split(X4_minmax_scaled, y, test_size=0.3, random_state=22)\n","09f3fb39":"import gc\ngc.collect()","51631bdc":"# Fitting OLS regressor to training data\n# poly_regressor4 = smf.OLS(endog = y4_train, exog = X4_train).fit()\n\n# Fitting of this classifier is time consuming and already done. This time loading saved classifier.\npoly_regressor4 = LoadData(filename_2bloaded='Polyregressorwith7degree_v9.clf')\n","ee65cb02":"# Printing summary of poly_regressor4\n# print(poly_regressor4.summary())\nprint('R-Squared for this regressor is {}'.format(np.round(poly_regressor4.rsquared, 3)))\nprint('R-Squared Adjusted for this regressor is {}'.format(np.round(poly_regressor4.rsquared_adj, 3)))\nprint('F- Value for this regressor is {}'.format(np.round(poly_regressor4.fvalue, 4)))\n\ny4_test_pred = poly_regressor4.predict(X4_test)\nresiplot(y_predicted=y4_test_pred, y_original=y4_test,delete_outlier=True, max_outlier_val= 20000)","dc7c43ab":"# Already stored\n# StoreData(poly_regressor4, filename= 'Polyregressorwith7degree_v9.clf')","e2f055dd":"# classifier_with_maxR2adjusted, r2, r2adjusted, final_list_of_index = DoBackwardElimination(the_regressor=poly_regressor4, X= X4_train, \n#                                                                                             y= y4_train, minP2eliminate= 0.11)\n\n# Backward elimination is a time consuming process thus we will not run it now. It is already done and needs to be loaded only.","9e049b1f":"# Storing optimized classifier to  be used further\n# StoreData(filename='optimized_classifier_v9.clf', regressor=classifier_with_maxR2adjusted)","5807f4b0":"# Loading optimized classifier for further use\nclassifier_with_maxR2adjusted = LoadData(filename_2bloaded='optimized_classifier_v9.clf')","853be30a":"## Already done. Not required now\n\n# Saving values of R Square, R Square Adjusted and final index list\n# r2.insert(0, 'R2')\n# r2adjusted.insert(0, 'R2Adjusted')\n\n# import csv\n\n# csvData = list(zip(r2,r2adjusted))\n# with open('r2andr2adjusted_v9.csv', 'w') as csvFile:\n#     writer = csv.writer(csvFile)\n#     writer.writerows(csvData)\n    \n# csvFile.close()\n\n# iter_listofindex = []\n# for i in final_list_of_index:\n#     iter_listofindex.append([i])\n\n# with open('final_index_list_v9.csv', 'w') as csvFile:\n#     writer = csv.writer(csvFile)\n#     writer.writerows(iter_listofindex)\n    \n# csvFile.close()","78324fd8":"# Importing data into list from csv file\n\nr2 = []\nr2adjusted = []\nfinal_list_of_index = [] # Remaining indexes after elimination\nflag = 0\n# os.listdir('..\/input\/polynomial-regression-backward-elimination')\nwith open('..\/input\/polynomial-regression-backward-elimination\/r2andr2adjusted_v9.csv') as csvFile:\n    reader = csv.reader(csvFile)\n    for row in reader:\n        if flag != 0:\n            r2.append(float(row[0]))\n            r2adjusted.append(float(row[1]))\n        flag = 1\n        \nwith open('..\/input\/polynomial-regression-backward-elimination\/final_index_list_v9.csv') as csvFile:\n    reader = csv.reader(csvFile)\n    for row in reader:\n        final_list_of_index.append(int(row[0]))","5d18e93e":"# Checking values of R Square and R Square Adjusted\nplt.figure(figsize=(12, 6))\nplt.plot(np.arange(0,len(r2adjusted), 1), r2adjusted, '-k')\nplt.plot(np.arange(0,len(r2adjusted), 1), r2, '--r')\n\nplt.xlim(0, len(r2)-1)\nplt.legend([r'$R^2  Adjusted$', r'$R^2$'], shadow = True)\n# p = plt.xticks(np.arange(0,len(r2), 1))\n\np = plt.text(50, 0.9412, r'Max value of $R^2$ is {}'.format(np.round(max(r2), 4)))\np = plt.text(50, 0.9398, r'Max value of Adjusted $R^2$ is {}'.format(np.round(max(r2adjusted), 4)))","8dd1e273":"# classifier_with_maxR2adjusted.summary()\nprint('R-Squared for this regressor is {}'.format(np.round(classifier_with_maxR2adjusted.rsquared, 3)))\nprint('R-Squared Adjusted for this regressor is {}'.format(np.round(classifier_with_maxR2adjusted.rsquared_adj, 3)))\nprint('F- Value for this regressor is {}'.format(np.round(classifier_with_maxR2adjusted.fvalue, 4)))\n\nresiplot(y_original= y4_test, \n         y_predicted=classifier_with_maxR2adjusted.predict(X4_test[:,final_list_of_index]), delete_outlier=True,max_outlier_val= 10000)","c3d60d33":"print('Total columns in original input for 7 degree polyfit are {}.'.format(np.shape(X4_test)[1]))\nprint('Total columns after optimization, in input for 7 degree polyfit are {}.'.format(len(final_list_of_index)))","e46d1e5f":"### Using pickle to save trained classifier. This will save out lots of time to retrain classifier in case of loss of variables due to system crash etc.","07020938":"### It is evident from above figure that there is a strong correlation between various independent (as assumed) variables.","a79635a3":"# Three different OLS Regressors, with Polynomial fitting, for comparison","e35ab9c3":"### Though after using backward elimination technique, there is no apparent improvement in model or regressor, but one thing to remember is that size of input data reduced from 1733 columns to 1391 columns i.e. by 342 without reducing it's R Square value.","6907b087":"### Now let's change polynomial degree to 7 and see what happens or how much accuracy we get (If it runs!!). Polyfitting of data is done for columns 'carat', 'x', 'y', 'z', 'depth' and 'table'.","f08d7eaf":"**Again it can be seen from above figure that carat is highly correlated to x, y, z and viceversa.**","ac6cc46c":"# Summary of first OLS Regressor, with 7th degree polyfit over 'carat', 'x', 'y' and 'z' also including 'depth' and 'table'","56dc113f":"# For previous work, based on which this notebook is made, go to [this link](https:\/\/www.kaggle.com\/ashishsaxena2209\/step-by-step-regression-backward-elimination).","bffb406d":"# Summary of first OLS Regressor, with 2nd degree polyfit over 'carat', 'x', 'y' and 'z'","0a5ab92d":"### Same R Square i.e. 0.932. F Value decreased which is not a good thing. No Backward Elimination used.","0e4245f9":"### So, just by using polynomial feature with 2nd degree, R Square increased from 0.927 to 0.932. Backward Elimination yet to be used to see maximum R Square we can get. Let's see summary for other regressors also.","6070bd04":"# Function to calculate and plot residuals","9d17ea9c":"### R square value increased from 0.933 to 0.941. Now, we need to check wether backward elimination do any good to model.","3b32315a":"# OLS Regressor without polynomial fitting of data","fa773a49":"## This notbook is for demonstration of application of polynomial features and some other concepts too. Mainly, it's for learning purpose.","17d6a4bb":"## From figures above it seems like few independent variables or features don't have linear relationship with the dependent variable.","2b214bbd":"# Function to Automating backward elimination process","22837c66":"# Applying Backward Elimination","c15ac737":"# Function for saving and loading classifiers using pickle","053d5d16":"# Function to fit polynomial features","f4d14f8b":"# Summary of first OLS Regressor, with 2nd degree polyfit over 'carat', 'x', 'y' and 'z' also including 'depth'","0313235e":"# Summary of first OLS Regressor, with 2nd degree polyfit over 'carat', 'x', 'y' and 'z' also including 'depth' and 'table'","70cc499e":"<a id='Orig_histogram'><\/a>\n# Original dataframe histogram","9d3f732b":"### Backward Elimination is already done and classifier is saved so not going to do whole process again because it's a time taking thing. Just loading the classifier."}}