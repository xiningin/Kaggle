{"cell_type":{"780e972b":"code","b04c1882":"code","e8eba13a":"code","55aca300":"code","a2f5fbe0":"code","057b947d":"code","bf14d934":"code","597c6861":"code","fba17467":"code","62206f90":"code","dcd48df9":"code","67f91bc4":"code","0086cffc":"code","a98c9507":"code","4d98e569":"code","20f595db":"code","39b8e469":"code","6f9ae64a":"code","ead40836":"code","7794deb9":"code","6e1c1052":"code","66816524":"code","61204836":"code","d0b6b2e6":"code","724180bb":"code","af77e0d4":"code","1b79596d":"code","a2f9455d":"code","12c6f15d":"code","3606c9d5":"markdown","abcf4715":"markdown","97739f7a":"markdown","b76d1b79":"markdown","1e2630a6":"markdown","c2415629":"markdown","dc12173a":"markdown","a7789de0":"markdown","495b3c2f":"markdown","7c812d70":"markdown","70e6b970":"markdown","a9aee1b2":"markdown","7bbdca93":"markdown","7ffdd00b":"markdown","dee7f239":"markdown","aa0a3c07":"markdown","2126ae42":"markdown","ad21a42d":"markdown","310dca20":"markdown","78da1130":"markdown","6e867bc2":"markdown","d682a8ea":"markdown","cb3392b9":"markdown"},"source":{"780e972b":"#import libraries\nimport os\nimport fileinput\nfrom os import listdir\nfrom os.path import isfile, join\nimport re, string   #for regex\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lxml.etree as ET  #for parsing XML\n#from google_trans_new import google_translator  #google translate\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom nltk.tag import pos_tag\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom PIL import Image\nfrom wordcloud import WordCloud\nfrom nltk.probability import FreqDist\n\n#NLTK downloads\nnltk.download([\n    \"stopwords\",\n    \"averaged_perceptron_tagger\",\n    \"punkt\",\n])\n\n#configure jupyter to allow each cell to display multiple outputs\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","b04c1882":"#intialize XML Parser\nparser = ET.XMLParser()\n\n#parse XML and store into variable for processing\ntree = ET.parse('..\/input\/german-emails-in-xml\/interactions.xml', parser=parser)\nroot1 = tree.getroot()","e8eba13a":"#display root tag of XML file\nroot1.tag","55aca300":"#display structure of XML file\ntest = root1.findall('.\/')\n\nfor index, child in enumerate(test):\n    print('(1st) {}'.format(child))\n\n    for index1, gchild in enumerate(child):\n        print('    (2nd) {}'.format(gchild[0]))\n\n        for index2, ggchild in enumerate(gchild):\n            print('        (3rd) {}'.format(ggchild))\n\n            for index3, gggchild in enumerate(ggchild):\n                print('            (4th) {}'.format(gggchild))\n\n                for index4, ggggchild in enumerate(gggchild):\n                    print('                (5th) {}'.format(ggggchild))\n\n                    #end loop to prevent long output (number of iterations was determined \n                    #manually for the sake of readability of notebook)\n                    if index4 == 1:\n                        break\n                        \n                if index3 == 6:\n                    break\n                    \n            if index2 == 2:\n                break\n                \n        if index1 == 1:\n            break\n            \n    if index ==3:\n        break","a2f5fbe0":"#problem - <relevantText> and CDATA are preventing full body of email to be extracted. \nblah = root1.findall('.\/interactions\/interaction')\nhah = blah[0][1]\nhah.text\nhah[0].text\n#last sentence of email cutoff","057b947d":"print(\"<text><![CDATA[\\nHallo,\\n]]><relevantText goldCategory='86'><![CDATA[ich habe seit 2 Tagen in meiner Warehouse Software \\nunter Exportieren-File\\n\\nOptionen ein fast nicht lesbare Schrift.]]><\/relevantText><![CDATA[ Wie kann ich diese wieder\\naendern.\\n\\n    ]]>\\n        <\/text>\")","bf14d934":"#create copy of XML file with relevantText tags removed and save to file system\n\n#open file in bytes - no encoding\n#with open(os.getcwd() + '\/interactions.xml', 'rb') as file :\n#    filedata = file.read()\n\n#convert to UTF-16 encoding to accomodate regex expressions\n#filedata = filedata.decode('utf-16')\n\n#remove CDATA and relevantText tags using regex substitution expressions\n#filedata = re.sub(\"(]]><relevantText [A-Za-z]*=\\\"[0-9,]*\\\">)\",\"\", filedata)\n#filedata = re.sub(\"(<\/relevantText>)\",\"\",filedata)\n#filedata = re.sub(\"(<\\!\\[CDATA\\[)\",\"\",filedata)\n#filedata = re.sub(\"(<\\!\\[CDATA\\[)\",\"\",filedata)\n#filedata = re.sub(\"]]>\",\"\",filedata)\n#filedata = re.sub(\"(<\\!\\[CDATA\\[)\",\"\",filedata)\n\n#convert back to bytes\n#filedata = bytes(filedata, 'utf-16')\n\n#write to file\n#with open(os.getcwd() + '\/interactions_updated.xml', 'wb') as file:\n#    file.write(filedata)","597c6861":"#initialize parser\nparser = ET.XMLParser(recover=True)\n\n#parse XML and store data into variable for processing\ntree = ET.parse('..\/input\/german-emails-in-xml\/interactions_updated.xml', parser=parser)\nroot2 = tree.getroot()","fba17467":"#display structure of XML file\ntest1 = root2.findall('.\/')\n\nfor index, child in enumerate(test1):\n    print('(1st) {}'.format(child))\n\n    for index1, gchild in enumerate(child):\n        print('    (2nd) {}'.format(gchild[0]))\n\n        for index2, ggchild in enumerate(gchild):\n            print('        (3rd) {}'.format(ggchild))\n\n            for index3, gggchild in enumerate(ggchild):\n                print('            (4th) {}'.format(gggchild))\n\n                for index4, ggggchild in enumerate(gggchild):\n                    print('                (5th) {}'.format(ggggchild))\n\n                    #end loop to prevent long output (number of iterations was determined \n                    #manually for the sake of readability of notebook)\n                    if index4 == 1:\n                        break\n                        \n                if index3 == 6:\n                    break\n                    \n            if index2 == 2:\n                break\n                \n        if index1 == 1:\n            break\n            \n    if index ==3:\n        break","62206f90":"blah = root2.findall('.\/interactions\/interaction')\nhah = blah[0][1]\nprint(hah.text)\n\n#now we have the full email in one subtree.","dcd48df9":"#put data into dictionary - only taking text, id, date, category, and keyword fields\nint_dict = []\nfor i, item in enumerate(root2.findall('.\/interactions')):\n    for child in item:\n        blah = {}\n        for gchild in child:\n            if gchild.tag == 'text':\n                blah[gchild.tag] = gchild.text\n            \n            for ggchild in gchild:\n                if np.logical_or(ggchild.tag == 'id',\n                                np.logical_or(ggchild.tag == 'date', \n                                             np.logical_or(ggchild.tag == 'category', ggchild.tag == 'keyword'))) == True:\n                    blah[ggchild.tag] = ggchild.text\n                    \n        int_dict.append(blah)","67f91bc4":"int_dict[600]","0086cffc":"#get relevantText and corresponding id\nrel_t = []\nfor i, item in enumerate(root1.findall('.\/interactions')):\n    for child in item:\n        blah = {}\n        for gchild in child:\n            for ggchild in gchild:\n                if ggchild.tag == 'relevantText':\n                    blah[ggchild.tag] = ggchild.text\n                    \n        rel_t.append(blah)","a98c9507":"rel_t[600]","4d98e569":"dataset = pd.DataFrame.from_dict(int_dict)\na = pd.DataFrame.from_dict(rel_t)\ndataset['relevantText'] = a['relevantText']\ndel a, rel_t, int_dict\ndataset.head()","20f595db":"np.unique(dataset['date'], return_counts=True)\nnp.unique(dataset['category'], return_counts=True)\nnp.unique(dataset['keyword'], return_counts=True)","39b8e469":"dataset.drop(columns=['date','keyword'], inplace=True)\ndataset.head(10)","6f9ae64a":"#translator = google_translator() #initialize translator\n#englishText = [] #initialize list\n\n#translate text from German to English\n#for text in dataset['text']:\n#    englishText.append(translator.translate(text,lang_src='de', lang_tgt='en'))  \n\n#display first 9 translated records\n#englishText[0:10]","ead40836":"#englishRel = [] #initialize list\n\n#translate text from German to English \n#for text in dataset['relevantText']:\n#    englishRel.append(translator.translate(text, lang_src='de', lang_tgt='en'))  \n\n#display first 10 translated records\n#englishRel[0:10]","7794deb9":"#combine translated data into \n#en_dataset = dataset.copy()\n#en_dataset.drop(columns = ['text','relevantText'], inplace=True)\n#en_dataset['text'] = englishText\n#en_dataset['relevantText'] = englishRel\n#en_dataset.head()","6e1c1052":"#load already translated data from above code into dataframe\nen_dataset = pd.read_csv('..\/input\/german-emails-in-xml\/Translated_emails.csv')\nen_dataset.head()","66816524":"# function to remove noise\ndef remove_noise(text_tokens, stop_words = ()):\n\n    cleaned_tokens = []\n\n    for token, tag in pos_tag(text_tokens):\n        token = re.sub('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n\n        #Lemmatize tokens to root words\n        if tag.startswith(\"NN\"):\n            pos = 'n'\n        elif tag.startswith('VB'):\n            pos = 'v'\n        else:\n            pos = 'a'\n\n        lemmatizer = WordNetLemmatizer()\n        token = lemmatizer.lemmatize(token, pos)\n\n        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n            cleaned_tokens.append(token.lower())\n\n    return cleaned_tokens","61204836":"#look at unique category numbers\nnp.unique(en_dataset['category'], return_counts=True)\n\n#initial ToktokTokenizer()\ntt = ToktokTokenizer()\n\n#define stopwords\nstop_words = stopwords.words('english')","d0b6b2e6":"#find unique category numbers (some records in category field list multiple categories sparated by ,)\ncat_un = []\nfor cat in np.unique(en_dataset['category'], return_counts=True)[0]:\n    cat_un.append(cat.find(',') == -1)\n\n#create a table containing unique category numbers and counts\ncat_table = pd.DataFrame([str(i) for i in np.unique(en_dataset['category'], return_counts=True)[0][cat_un]], columns=['category'])\ncat_table['count'] = np.unique(en_dataset['category'], return_counts=True)[1][cat_un]\ncat_table.sort_values(by='count', ascending=False , inplace=True)\ncat_table.reset_index(inplace=True, drop=True)\ndel cat_un\n\n#plot occurrences\nplt.figure(figsize=[25,5]);\nsns.barplot(data = cat_table, y='count', x='category', order=cat_table['category'])\nplt.title('Frequency Distribution of Category Occurrences', fontsize=36, fontweight='bold')\nplt.yticks(fontsize=16);\nplt.xticks(fontsize=16, fontweight='bold');\nplt.xticks(fontsize=16);\nplt.ylabel('Count', fontsize=20);\nplt.xlabel('Category', fontsize=20);","724180bb":"#wordcloud of top 5 categories with text \/ relevantText side by side\ndat = cat_table.head(5)\n\n# create fontdicts for formatting figure text\naxtitle_dict = {'family': 'serif',\n        'color':  'black',\n        'weight': 'bold',\n        'size': 50\n        }\n\n#specify circular mask\nchar_mask = np.array(Image.open('..\/input\/circle-mask\/circle.png'))\n\nfig = plt.figure(figsize = [50,100]);\ngrid = plt.GridSpec(5, 2, wspace=0.0, hspace=0.1);\n\nfor i in np.arange(0,len(dat)):\n    temp_text = ' '.join(en_dataset.loc[en_dataset['category']==dat.loc[i,'category'],'text'])\n    clean_text = remove_noise(tt.tokenize(temp_text), stop_words)\n    wordcloud_text = ' '.join(clean_text)\n\n    temp_rel = ' '.join(en_dataset.loc[en_dataset['category']==dat.loc[i,'category'],'relevantText'])\n    clean_rel = remove_noise(tt.tokenize(temp_text), stop_words)\n    wordcloud_rel = ' '.join(clean_text)\n\n#create figure\n    text=WordCloud(background_color='black', mask=char_mask).generate(wordcloud_text)\n    reltext=WordCloud(background_color='black', mask=char_mask).generate(wordcloud_rel)\n    exec(f\"ax{i}0 = plt.subplot(grid[i,0]);\")\n    exec(f\"ax{i}0.imshow(text);\")\n    exec(f\"ax{i}0.axis('off');\")\n    title_text = 'Category: {} | text'.format(dat.loc[i,'category']);\n    exec(f\"ax{i}0.set_title(title_text, fontdict=axtitle_dict)\")\n    exec(f\"ax{i}1 = plt.subplot(grid[i,1]);\")\n    exec(f\"ax{i}1.imshow(reltext);\")\n    exec(f\"ax{i}1.axis('off');\")\n    title_text = 'Category: {} | relevantText'.format(dat.loc[i,'category'])\n    exec(f\"ax{i}1.set_title(title_text, fontdict=axtitle_dict)\")\n\nplt.show();\ndel dat, temp_text, clean_text, wordcloud_text, temp_rel, clean_rel, wordcloud_rel, title_text, text, reltext","af77e0d4":"#wordcloud of bottom 5 categories with text \/ relevantText side by side\ndat = cat_table.tail(5)\ndat.reset_index(inplace=True)\n\nfig = plt.figure(figsize = [50,100]);\ngrid = plt.GridSpec(5, 2, wspace=0.0, hspace=0.1);\n\nfor i in np.arange(0,len(dat)):\n    temp_text = ' '.join(en_dataset.loc[en_dataset['category']==dat.loc[i,'category'],'text'])\n    clean_text = remove_noise(tt.tokenize(temp_text), stop_words)\n    wordcloud_text = ' '.join(clean_text)\n\n    temp_rel = ' '.join(en_dataset.loc[en_dataset['category']==dat.loc[i,'category'],'relevantText'])\n    clean_rel = remove_noise(tt.tokenize(temp_text), stop_words)\n    wordcloud_rel = ' '.join(clean_text)\n\n#create figure\n    text=WordCloud(background_color='black', mask=char_mask).generate(wordcloud_text)\n    reltext=WordCloud(background_color='black', mask=char_mask).generate(wordcloud_rel)\n    exec(f\"ax{i}0 = plt.subplot(grid[i,0]);\")\n    exec(f\"ax{i}0.imshow(text);\")\n    exec(f\"ax{i}0.axis('off');\")\n    title_text = 'Category: {} | text'.format(dat.loc[i,'category']);\n    exec(f\"ax{i}0.set_title(title_text, fontdict=axtitle_dict)\")\n    exec(f\"ax{i}1 = plt.subplot(grid[i,1]);\")\n    exec(f\"ax{i}1.imshow(reltext);\")\n    exec(f\"ax{i}1.axis('off');\")\n    title_text = 'Category: {} | relevantText'.format(dat.loc[i,'category'])\n    exec(f\"ax{i}1.set_title(title_text, fontdict=axtitle_dict)\")\n\nplt.show();\ndel dat, temp_text, clean_text, wordcloud_text, temp_rel, clean_rel, wordcloud_rel, title_text, text, reltext","1b79596d":"dat = cat_table.head(5)\n\ncat_desc_text = []\ncat_desc_reltext =[]\nfor i in np.arange(0,len(cat_table)):\n    temp_text = ' '.join(en_dataset.loc[en_dataset['category']==cat_table.loc[i,'category'],'text'])\n    clean_text = remove_noise(tt.tokenize(temp_text), stop_words)\n    fdist_text=FreqDist(clean_text)\n\n    temp_reltext = ' '.join(en_dataset.loc[en_dataset['category']==cat_table.loc[i,'category'],'text'])\n    clean_reltext = remove_noise(tt.tokenize(temp_text), stop_words)\n    fdist_reltext=FreqDist(clean_text)\n    \n    cat_desc_text.append(' '.join(np.transpose(fdist_text.most_common(3))[0]))\n    cat_desc_reltext.append(' '.join(np.transpose(fdist_text.most_common(3))[0]))\n    ","a2f9455d":"desc_compare= pd.DataFrame(cat_desc_text, columns=['cat_desc_text'])\ndesc_compare['cat_desc_reltext'] = cat_desc_reltext\ndesc_compare","12c6f15d":"cat_table['category desc'] = cat_desc_text\ncat_table = cat_table.iloc[:,[0,2,1]]\ncat_table","3606c9d5":"# <p style=\"font-size:24px; font-family:'Candara'; font-weight: bold; line-height:1.3\">Language Translation using Google Translator API<\/p>\n<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Below is a demonstration of how easy it is to translate text using the Google Translator API. The lang_src (language source) and lang_tgt (language target) are specified in ISO 639-1 codes. The source here is German ('de') and the target is English ('en').<\/p>\n<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">Since the data has already been translated, this code is commented out to prevent error and overload on the API<\/p>","abcf4715":"<p style=\"font-size:24px; font-family:'Candara'; font-weight: bold; line-height:1.3\">Parsing XML in Python<\/p>\n<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;I will be using the lmxl library in python to parse the XML file. The XML data is parsed into nodes, beginning at the root.<\/p>","97739f7a":"<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Below is the row structure of the dictionary that was created above.<\/p>","b76d1b79":"<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Some emails are assigned into multiple categories as demonstrated below. For predicting category descriptions, I will only look at emails that were assigned to one category.<\/p>","1e2630a6":"# <p style=\"font-size:24px; font-family:'Candara'; font-weight: bold; line-height:1.3\">Solving Tag Errors using Regex<\/p>\n\n<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Using regex substitution expressions, the offending tags will be removed from the XML file and saved as a second file. It's important to note that the XML file is a byte file and needs to be opened using 'rb' or 'read bytes' setting. Then, to apply the regex substitution expressions, the filedata is encoded to the 'UTF-16' codec. Saving the updated file in bytes is performed by decoding the filedata back to bytes using the bytes() function and specifying 'wb' or 'write bytes' when saving.<\/p>u","c2415629":"# <p style=\"font-size:36px; font-family:'Candara'; font-weight: bold; line-height:1.3\">XML Data Extraction, Foreign Language Translation, and Text Categorization<\/p>\n\n<p style=\"font-size:24px; font-family:'Candara'; font-weight: bold; line-height:1.3\">Notebook Description<\/p>\n<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;I found a XML file that was pretty interesting containing anonymized technical support emails that are written in German to a German software company. Out the gate, I had no clue what these emails were about because I don't understand German. But I thought it would be good practice in using XML parsing and language translation tools in python.<\/p>\n\n<p style=\"font-size:24px; font-family:'Candara'; font-weight: bold; line-height:1.3\">Key Activities<\/p>\n    <ul>\n        <li><p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">Extract data fields from a XML document and store into a pandas dataframe<\/p><\/li> \n        <li><p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">Translate text from German to English using Google Translator API<\/p><\/li>\n        <li><p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">Predict email category descriptions using the Natural Language ToolKit (nltk)<\/p><\/li>\n    <\/ul>\n<\/p>","dc12173a":"<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Looking at unique entries in the data for the fields 'date', 'category', and 'keyword', above, it is found that 'date' and 'keyword' are not helpful and such fields are dropped from the dataframe below. The field 'category', however, appears to be useful.<\/p>","a7789de0":"# <p style=\"font-size:24px; font-family:'Candara'; font-weight: bold; line-height:1.3\">Tag Errors<\/p>\n<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Looking at some of the text contained in the nodes, it was discovered that the body of the email is not being parsed correctly and portions of the text is being lost. As shown below, part of the body of the email is being stored into a subtree of the 'text' field, tagged 'relevantText' and part of the body of the email is not read during parsing.<\/p>","495b3c2f":"<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Below is the final category table containing descriptions for categories predicted from the email text.<\/p>","7c812d70":"<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Below is a frequency plot of the occurrences of each category in the emails.<\/p>","70e6b970":"# <p style=\"font-size:24px; font-family:'Candara'; font-weight: bold; line-height:1.3\">Node Structure of XML<\/p>\n<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Below is a description of how the XML data is parsed into nodes. It is relatively straightforward to move through nodes using nested for loops. Later in this notebook, nested for loops will be used to extract data from the XML and place it into a pandas dataframe.<\/p>","a9aee1b2":"<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Now that the text is translated, a new dataframe is created containing only the text in English.<\/p>","7bbdca93":"<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Below is a set of subplots comparing wordclouds of 'text' and 'relevantText' fields for the bottom 5 categories. Even with limitted n of data, it appears that frequency of word usage is still a viable means of prediction category descriptions.<\/p>","7ffdd00b":"<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Below is a set of subplots comparing wordclouds of 'text' and 'relevantText' fields for the top 5 categories. There is a definite trend on the most common terms here which may provide category descriptions.<\/p>","dee7f239":"<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Perform a similar nested for loop to extract relevantText from the original XML file.<\/p>","aa0a3c07":"<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Combine data then convert to pandas dataframe.<\/p>","2126ae42":"<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Above, it is demonstrated that the relevantText nodes have been removed from the XML document. Below, it is demonstrated that the email text field now contains the full email body without losing any of the data during parsing. The problem has been fixed.<\/p>","ad21a42d":"# <p style=\"font-size:24px; font-family:'Candara'; font-weight: bold; line-height:1.3\">Conclusion<\/p>\n<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This notebook has demonstrated how to work with XML data in python, including how to resolve tagging errors in a straightforward method. Using Google Translator API, it has been shown how to translate a large dataset of text into a different language. And finally, a simple method for categorizing text has been demonstrated.<\/p>","310dca20":"<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Let's parse the corrected XML and verify that the full email body is being parsed.<\/p>","78da1130":"# <p style=\"font-size:24px; font-family:'Candara'; font-weight: bold; line-height:1.3\">Simple Prediction of Email Category Descriptions<\/p>\n<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;I'm going to define a simple algorithm for predicting category descriptions for the emails. First, I'm going to create a function to get rid of noise. Below is a function to POS-tag tokenized words in the text, remove stopwords and URLS, and lemmatize to root. This function will be used to get rid of noise in the text so that category descriptions may be determined through analysis.<\/p>","6e867bc2":"<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The problem is that inside the 'text' node there is a child node with the tag 'relevantText'. The parser is treating the 'relevantText' tags as a stop to the 'text' node and nothing has been defined for the parser to handle it differently.<\/p>\n\n<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;It would be good to get both the full text of the emails and also the text tagged as relevant, which is going to require some special work on the XML file.<\/p>\n\n<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Below is an example of the raw XML.<\/p>","d682a8ea":"# <p style=\"font-size:24px; font-family:'Candara'; font-weight: bold; line-height:1.3\">Structuring Parsed XML Into DataFrame<\/p>\n<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Using logical functions, the XML tags are used to specify which field will be extracted from the XML. Here, 'id', 'date', 'category', 'keyword', and 'text' are extracted and stored into a dictionary.<\/p>","cb3392b9":"<p style=\"font-size:18px; font-family:'Calibri Light'; line-height:1.3\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Using the nltk FreqDist() function, the top 3 terms for each category is determined for both the 'text' and 'relevantText' fields. Comparing both, they are found to be identical and therefore, only the 'text' field predictions will be used.<\/p>"}}