{"cell_type":{"d1d10240":"code","e0a773ef":"code","bd53bb14":"code","0fa2dcef":"code","5e33dcb9":"code","0534c97c":"code","48c54dfa":"code","90306f35":"code","039f1e8f":"code","4d20ab87":"code","bcafc341":"code","4fed4d1b":"code","f2bf5d95":"code","81569e9b":"code","47c10499":"code","9c3a5614":"code","b57cb591":"code","cb88aa86":"code","7fdfe3b0":"code","02d97caa":"code","b229d99d":"code","5b6463e3":"code","b7265f9c":"code","2247fa00":"code","7d0ca09f":"code","3672d437":"code","a473765a":"code","b663f518":"code","578931fa":"code","5a8b3dcf":"code","2957cb25":"code","e4142104":"code","cf52e474":"code","d6bc0fda":"code","80fbf2ba":"code","01595324":"code","a6416281":"code","7c72f73a":"code","c034fd3d":"code","0376b436":"code","017de81f":"code","520b5b88":"code","ada0be22":"code","82042296":"code","ac63eb42":"code","d061ef07":"code","383338f3":"code","ea1ea61c":"code","d2cf1629":"code","0314ba13":"code","6b6fd7f3":"code","1e37f3a8":"code","262ce88e":"code","5395c6d6":"code","de52c9d5":"code","170c69ce":"code","a8d4d69e":"code","42740178":"code","ef672b7d":"code","d0c45c7a":"code","7a530934":"code","4fc167f0":"code","e07c534a":"code","d14bafde":"code","4eaa7b43":"code","701f5961":"code","f540e664":"code","cd33521d":"code","80ada776":"code","f8607f49":"code","7bccf828":"code","04ced215":"code","37045db2":"code","8a50599b":"code","6c332bc7":"code","59414833":"code","720db3ea":"code","5e3ea1a1":"code","248bafbf":"code","82849ee6":"markdown","f764abf9":"markdown","427e9de8":"markdown","90e15e88":"markdown","874f8f50":"markdown","90559b72":"markdown","adc007c2":"markdown","0ab2d4f1":"markdown","8fce0410":"markdown","ed6a8641":"markdown","65c94490":"markdown","787b03ec":"markdown","53801e6c":"markdown","32dcffd8":"markdown","4f09c159":"markdown","25751ff3":"markdown","f361e5bb":"markdown","39aba816":"markdown","4b8affa3":"markdown","45bd88f4":"markdown","329ee845":"markdown","fae0d462":"markdown","809ea577":"markdown","c6b8c547":"markdown","bb7e83e5":"markdown","5794fa50":"markdown","ab460fcd":"markdown","cb97e6d9":"markdown","0ac53ce9":"markdown","4f2e6b06":"markdown","272dfeeb":"markdown","3877292b":"markdown","8748d724":"markdown","09192550":"markdown","91d388ec":"markdown"},"source":{"d1d10240":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","e0a773ef":"import warnings\nwarnings.filterwarnings(\"ignore\")","bd53bb14":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","0fa2dcef":"train.head()","5e33dcb9":"train.shape","0534c97c":"test.shape","48c54dfa":"train.info()","90306f35":"train.dtypes","039f1e8f":"sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')","4d20ab87":"train.isnull().sum()","bcafc341":"test.isnull().sum()","4fed4d1b":"plt.figure(figsize=(8,6))\nsns.set_style('whitegrid')\nsns.countplot(x='Survived',data=train,palette='Set2')","f2bf5d95":"train.Survived.value_counts()","81569e9b":"plt.figure(figsize=(10,8))\nsns.set_style('whitegrid')\nsns.countplot(x='Sex',hue='Survived',data=train,palette='Set1')","47c10499":"Per_Sur = pd.crosstab(train[\"Sex\"],train[\"Survived\"])\nPer_Sur[\"Percentage_Survived\"] = round((Per_Sur[1]\/(Per_Sur[0]+Per_Sur[1]))*100,2).astype(str) + \"%\"\nPer_Sur","9c3a5614":"plt.figure(figsize=(12,8))\nsns.set_style('whitegrid')\nsns.countplot(x='Pclass',hue='Survived',data=train,palette='rainbow')","b57cb591":"Per_Sur=pd.crosstab(train[\"Pclass\"],train[\"Survived\"])\nPer_Sur[\"Percentage_Survived\"] = round((Per_Sur[1]\/(Per_Sur[0]+Per_Sur[1]))*100,2).astype(str) + \"%\"\nPer_Sur","cb88aa86":"plt.figure(figsize=(16,8))\nsns.distplot(train.loc[train['Survived']==0,'Age'].dropna(),kde=False,color='red',bins=30)\nsns.distplot(train.loc[train['Survived']==1,'Age'].dropna(),kde=False,color='blue',bins=30)\nplt.legend(train['Survived'])","7fdfe3b0":"plt.figure(figsize=(10,10))\nsns.heatmap(train.corr(),annot=True)\n","02d97caa":"train.loc[(train[\"Pclass\"]==1) & (train.Age.notna() == False),\"Age\"]=train.loc[train[\"Pclass\"]==1].Age.mean()\n\ntrain.loc[(train[\"Pclass\"]==2) & (train.Age.notna() == False),\"Age\"]=train.loc[train[\"Pclass\"]==2].Age.mean()\n\ntrain.loc[(train[\"Pclass\"]==3) & (train.Age.notna() == False),\"Age\"]=train.loc[train[\"Pclass\"]==3].Age.mean()","b229d99d":"train.Age.isnull().sum()","5b6463e3":"test.loc[(test[\"Pclass\"]==1) & (test.Age.notna() == False),\"Age\"]=test.loc[test[\"Pclass\"]==1].Age.mean()\n\ntest.loc[(test[\"Pclass\"]==2) & (test.Age.notna() == False),\"Age\"]=test.loc[test[\"Pclass\"]==2].Age.mean()\n\ntest.loc[(test[\"Pclass\"]==3) & (test.Age.notna() == False),\"Age\"]=test.loc[test[\"Pclass\"]==3].Age.mean()","b7265f9c":"test.Age.isnull().sum()","2247fa00":"plt.figure(figsize=(16,8))\nsns.countplot(x='SibSp',hue='Survived',data=train)","7d0ca09f":"plt.figure(figsize=(16,8))\nsns.countplot(x='Parch',hue='Survived',data=train)","3672d437":"print(train.Cabin.isnull().sum(),\"\\n\")\ntrain.Cabin.unique()\n\n#Cabin has more Null values and it does not signify anything, we can drop the variable","a473765a":"train.drop(\"Cabin\",axis=1,inplace=True)\ntrain.shape","b663f518":"test.drop(\"Cabin\",axis=1,inplace=True)\ntest.shape","578931fa":"print(train.Embarked.isnull().sum(),\"\\n\")\n\ntrain[\"Embarked\"].fillna(train[\"Embarked\"].mode()[0],inplace=True)#Imputing Null values with mode\n\ntrain.Embarked.unique()","5a8b3dcf":"plt.figure(figsize=(10,8))\nsns.countplot(x=train['Embarked'].dropna(),hue='Survived',data=train)","2957cb25":"Per_Sur = pd.crosstab(train['Embarked'],train['Survived'])\nPer_Sur[\"Percentage_Survived\"] = round((Per_Sur[1]\/(Per_Sur[0]+Per_Sur[1]))*100,2).astype(str)+\"%\"\nPer_Sur","e4142104":"train.loc[train[\"Fare\"] == max(train.Fare),\"Fare\" ] = 263\n#Imputing the outlier 500 with 263(Original max fare)","cf52e474":"plt.figure(figsize=(16,8))\nsns.distplot(train.loc[train['Survived']==0,'Fare'].dropna(),kde=False,color='blue',bins=30)\nsns.distplot(train.loc[train['Survived']==1,'Fare'].dropna(),kde=False,color='green',bins=30)\nplt.legend(train['Survived'])","d6bc0fda":"test.isnull().sum()","80fbf2ba":"plt.figure(figsize=(16,8))\nsns.distplot(test['Fare'].dropna(),kde=False,color='blue',bins=30)\n","01595324":"test.loc[test[\"Fare\"] == max(test.Fare),\"Fare\" ] = 263\n#Imputing the outlier 500 with 263(Original max fare)","a6416281":"test.Fare.fillna(round(test.Fare.mean(),3),inplace=True)","7c72f73a":"test.isnull().sum()","c034fd3d":"#Dropping few variables which are not important in train data\ntrain.drop(['PassengerId','Name','Ticket'],axis=1,inplace=True)\ntrain.shape","0376b436":"#Dropping few variables which are not important in test data\ntest.drop(['PassengerId','Name','Ticket'],axis=1,inplace=True)\ntest.shape","017de81f":"#importing all the required ML packages\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn.tree import DecisionTreeClassifier #Decision tree \nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.ensemble import ExtraTreesClassifier #Extra Tree Classifier\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\n","520b5b88":"train.dtypes","ada0be22":"test.dtypes","82042296":"colname=[\"Sex\",\"Age\",'Embarked']","ac63eb42":"#Label Encoding - converting objects to int\n\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n\nfor x in colname:\n     train[x]=le.fit_transform(train[x])\n\nfor x in colname:\n     test[x]=le.fit_transform(test[x])","d061ef07":"#Data Splitting - train data is split into train_m and val data in 70, 30 ratio\n\ntrain_m,val=train_test_split(train,test_size=0.3,random_state=10)\n\ntrain_X = train_m.values[:,1:]\ntrain_Y = train_m.values[:,0:1]\n\nval_X = val.values[:,1:]\nval_Y = val.values[:,0:1]\n\n\ntest_X= test.values[:,0:]","383338f3":"#Scaling - to bring all the data onto a common scale\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nscaler.fit(train_X)\n\ntrain_X = scaler.transform(train_X)\nval_X = scaler.transform(val_X)\ntest_X = scaler.transform(test_X)","ea1ea61c":"log = LogisticRegression(random_state=100)\nlog.fit(train_X,train_Y)\npred_log=log.predict(val_X)\nprint('The accuracy of the Logistic Regression is',metrics.accuracy_score(pred_log,val_Y))","d2cf1629":"from sklearn.metrics import confusion_matrix, accuracy_score, \\\nclassification_report\ncfm=confusion_matrix(val_Y, pred_log)\nprint(cfm)","0314ba13":"DT = DecisionTreeClassifier(random_state=100)\nDT.fit(train_X,train_Y)\npred_DT = DT.predict(val_X)\nprint('The accuracy of the Decision Tree is',metrics.accuracy_score(pred_DT,val_Y))","6b6fd7f3":"from sklearn.metrics import confusion_matrix, accuracy_score, \\\nclassification_report\ncfm=confusion_matrix(val_Y, pred_DT)\nprint(cfm)","1e37f3a8":"Ran_F=RandomForestClassifier(n_estimators=100,random_state=100)\nRan_F.fit(train_X,train_Y)\npred_Ran_F=Ran_F.predict(val_X)\nprint('The accuracy of the Random Forests is',metrics.accuracy_score(pred_Ran_F,val_Y))","262ce88e":"from sklearn.metrics import confusion_matrix, accuracy_score, \\\nclassification_report\ncfm=confusion_matrix(val_Y,pred_Ran_F)\nprint(cfm)","5395c6d6":"Extra_tr = ExtraTreesClassifier(random_state=100)\nExtra_tr.fit(train_X,train_Y)\npred_Extra_tr = Extra_tr.predict(val_X)\nprint('The accuracy of the Extra Tree is',metrics.accuracy_score(pred_Extra_tr,val_Y))","de52c9d5":"from sklearn.metrics import confusion_matrix, accuracy_score, \\\nclassification_report\ncfm=confusion_matrix(val_Y,pred_Extra_tr)\nprint(cfm)","170c69ce":"KNN=KNeighborsClassifier() \nKNN.fit(train_X,train_Y)\npred_KNN=KNN.predict(val_X)\nprint('The accuracy of the KNN is',metrics.accuracy_score(pred_KNN,val_Y))","a8d4d69e":"from sklearn.metrics import confusion_matrix, accuracy_score, \\\nclassification_report\ncfm=confusion_matrix(val_Y,pred_KNN)\nprint(cfm)","42740178":"a_index=list(range(1,11))\na=pd.Series()\nx=[0,1,2,3,4,5,6,7,8,9,10]\nfor i in list(range(1,11)):\n    model=KNeighborsClassifier(n_neighbors=i) \n    model.fit(train_X,train_Y)\n    prediction=model.predict(val_X)\n    a=a.append(pd.Series(metrics.accuracy_score(prediction,val_Y)))\nplt.plot(a_index, a)\nplt.xticks(x)\nfig=plt.gcf()\nfig.set_size_inches(12,6)\nplt.show()\nprint('Accuracies for different values of n are:',a.values,'with the max value as ',a.values.max())","ef672b7d":"KNN=KNeighborsClassifier(n_neighbors=8) \nKNN.fit(train_X,train_Y)\npred_KNN=KNN.predict(val_X)\nprint('The accuracy of the KNN is',metrics.accuracy_score(pred_KNN,val_Y))\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, \\\nclassification_report\ncfm=confusion_matrix(val_Y,pred_KNN)\nprint(cfm)","d0c45c7a":"models = [metrics.accuracy_score(pred_log,val_Y),metrics.accuracy_score(pred_DT,val_Y),metrics.accuracy_score(pred_Ran_F,val_Y),\n          metrics.accuracy_score(pred_Extra_tr,val_Y),metrics.accuracy_score(pred_KNN,val_Y)]\n\nm_names = [\"Logistic\",\"Decision Tree\",\"Random Forest\",\"Extra Tree\",\"KNN\"]\n\nfor x,y in zip(models,m_names):\n    print(\"Accuracy for\",y, \"is :\", round(x*100,2))","7a530934":"cols = train.columns[1:]\nprint(cols,\"\\n\")\nimp=Ran_F.feature_importances_\nprint(imp,\"\\n\")\n\nfeatures = pd.DataFrame(cols,columns=[\"Features\"])\nfeatures[\"Importance\"] = imp\nfeatures = features.sort_values(\"Importance\",ascending=False)\nfeatures","4fc167f0":"train_trail = train.copy()","e07c534a":"train_trail.drop([\"Parch\",\"Embarked\"],axis=1,inplace=True)","d14bafde":"train_trail.drop(\"SibSp\",axis=1,inplace=True)","4eaa7b43":"test.drop([\"Parch\",\"Embarked\",\"SibSp\"],axis=1,inplace=True)","701f5961":"#Data Splitting - train_rev is split into train_m and val data in 70, 30 ratio\n\ntrain_m,val=train_test_split(train_trail,test_size=0.3,random_state=10)\n\ntrain_X = train_m.values[:,1:]\ntrain_Y = train_m.values[:,0:1]\n\nval_X = val.values[:,1:]\nval_Y = val.values[:,0:1]\n\n\ntest_X= test.values[:,0:]\n\n#Scaling - to bring all the data onto a common scale\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nscaler.fit(train_X)\n\ntrain_X = scaler.transform(train_X)\nval_X = scaler.transform(val_X)\ntest_X = scaler.transform(test_X)","f540e664":"Ran_F=RandomForestClassifier(n_estimators=100,random_state=100)\nRan_F.fit(train_X,train_Y)\npred_Ran_F=Ran_F.predict(val_X)\nprint('The accuracy of the Random Forests is',metrics.accuracy_score(pred_Ran_F,val_Y))","cd33521d":"from sklearn.metrics import confusion_matrix, accuracy_score, \\\nclassification_report\ncfm=confusion_matrix(val_Y,pred_Ran_F)\nprint(cfm)\nprint(\"classification_report: \")\nprint(classification_report(val_Y, pred_Ran_F))\nacc=accuracy_score(val_Y, pred_Ran_F)\nauc = metrics.roc_auc_score(val_Y, pred_Ran_F)\nprint(\"Accuracy of the model: \", acc)\nprint(\"AUC of the model: \", auc)","80ada776":"from sklearn import metrics\n\nfpr, tpr,z = metrics.roc_curve(val_Y, pred_Ran_F)# receiver operating characteristics\nauc = metrics.auc(fpr,tpr)\nprint(auc)\nprint(fpr)\nprint(tpr)","f8607f49":"plt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n\nplt.show()","7bccf828":"y_pred_prob=Ran_F.predict_proba(val_X)","04ced215":"#A generic code to find values at different thresholds to pick up the best\nfor a in np.arange(0.4,0.6,0.01):\n    predict_mine = np.where(y_pred_prob[:,1] > a, 1, 0)\n    cfm=confusion_matrix(val_Y, predict_mine)\n    total_err=cfm[0,1]+cfm[1,0]\n    print(\"Errors at threshold \", a, \":\",total_err, \" , type 2 error :\",\n        cfm[1,0],\" , type 1 error:\", cfm[0,1])","37045db2":"#performing kfold_cross_validation\nfrom sklearn.model_selection import KFold\n\nkfold_cv = KFold(n_splits=10,random_state=10)\nprint(kfold_cv,\"\\n\")\n\nfrom sklearn.model_selection import cross_val_score \nkfold_cv_result=cross_val_score(estimator=Ran_F,X=train_X,y=train_Y, cv=kfold_cv)\nprint(kfold_cv_result,\"\\n\")\nprint(kfold_cv_result.mean())\n","8a50599b":"pred_Ran_F=Ran_F.predict(test_X)","6c332bc7":"test[\"Survived\"] = pred_Ran_F.astype(np.int)","59414833":"test.head()","720db3ea":"test.Survived.value_counts()","5e3ea1a1":"test.shape","248bafbf":"test.to_excel(\"Titanic_Survival_predictios.xlsx\",index = False,header = True)","82849ee6":"### 5. Prediting the results on test data and submiting the results","f764abf9":"### Performing Cross Validation to check for better model ","427e9de8":"### Part 3: Splitting the train data into train_model data and validation data and Building Model, predicting the results on validation data.\n\n1)Logistic Regression\n\n2)Decision Tree\n\n3)Random Forest\n\n4)Extra Tree Classifier\n\n5)K-Nearest Neighbours\n","90e15e88":"Age is an important factor for the model","874f8f50":"Survived is a dependent\/target variable \n\nWe can see that the Survival rate(1) is very less","90559b72":"#### Accuracies of the above models","adc007c2":"Roughly 20 percent of the Age data is missing. The proportion of Age missing is likely small enough for reasonable replacement with some form of imputation. Looking at the Cabin column, it looks like we are just missing too much of that data to do something useful with at a basic level. We'll probably drop this later, or change it to another feature like \"Cabin Known: 1 or 0\"","0ab2d4f1":"#### Preprocessing","8fce0410":"After the model tunning(Feature Elimination) based on the important variables from Random Forest model\n\nThe Final Accuracy is 82.8%\n\nTotal Error is : 46 (FN = 25, FP=21) - the errors are balanced","ed6a8641":"### Logistic Regression","65c94490":"Age is Highly correlated(Negative) to Pclass than any other variables.\n\nSo Null values in Age have to be replaced with Pclass wise Mean values","787b03ec":"### Conclusion \n\n1. Final Model - Random Forest(after model tunning - feature elimination)\n\n2. Model Accuracy - 82.8%\n\n3. Type 2 Erros = 25\n\n4. Type 1 Errors = 21\n\n5. Area Under Curve(AUC) = 0.80\n\n5. We may improve the Accuracy by creating new variables from other features or by finding relationships between the variables but Accuracy is not the only important measure for model building, Recall, {Precision, Total Error shoul also be considerd.","53801e6c":"### Identifying best Threshold ","32dcffd8":"### Extra Tree Classfier","4f09c159":"### Decision Tree ","25751ff3":"### Data contains 891 observations and 12 parameters (features)\nFeatures of the data \n\nPassengerId: unique id number to each passenger\n\nSurvived: passenger survive(1), didn't survive(0)\n\nPclass: passenger class\n\nName: passenger's name\n\nSex: passenger's gender\n\nAge: passenger's age\n\nSibSp: number of siblings\/spouses\n\nParch: number of parents\/children\n\nTicket: ticket number\n\nFare: amount of money spent on ticket\n\nCabin: cabin category\n\nEmbarked: part where passenger embarked(C = Cherbourg, Q = Queenstown, S = Southampton)","f361e5bb":"Female Survival rate is 74% which is higher than Male Survival rate 18%. So this is an important variable for the model","39aba816":"### Part 1 : Understanding the project data ","4b8affa3":"### K-Nearest Neighbours(KNN)\n","45bd88f4":"### Import Libraries","329ee845":"Passengers who bought tickets with Higher Fare(PClass = 1) has Higher Survival Rate","fae0d462":"Cross Validation at any number of splits didn't give good results. So we will have to stick with the tunned model of Random Forest","809ea577":"Though maximum Accuracy is 83.2% when number of neighbors = 8 for KNN  but there is no balance between FN and FP errors ","c6b8c547":"### Part 2: Exploring the data using visualization tools","bb7e83e5":"We can see that independent Passengers with \"0\" count of Parent or Children has less Survival rate","5794fa50":"# Titanic Data Set\n\n                                             \n### Introduction\n                                              \nHi friends , this is my first project in Kaggle and very special to me. Here I am presenting my project in my own way.\nBefore starting the project i was having many doubts and i tried my best to get answers\n1. Relationship between the survived and the pasengers ?\n2. There were very few life jackets or rescue boats and On what basis passengers were rescued ?\n3. Can budilding a model help in preditions ?\n4. Can we consider the human emotional factors and captain priorities while preparing the model?\n5. Is there any use of creating model?\n\n\n### Project Steps followed\n\n1. Understanding the project data\n2. EDA, Data Cleaning, Analyzing, identifying patterns.\n3. Splitting the train data into train_model data and validation data and Building Model, predicting the results on validation data.\n4. Feature engineering and creating a best suitable model (most important stage of project)\n5. Prediting the results on test data and submiting the results","ab460fcd":"We can see that independent Passengers with \"0\" count of Siblings or Spouse has less Survival rate","cb97e6d9":"Model gave the best Accuracy at Threshold 0.5, no need to change the default threshold","0ac53ce9":"### Random Forests\n","4f2e6b06":"Passengers from Southampton (S) has less Survival rate hence this also an important variable for prediction","272dfeeb":"From the above graph no proper conclusion can be made, further analysis is required ","3877292b":"### Type Of Features\n\nCategorical Features: Sex,Embarked,Survived,Parch\n\nOrdinal Features: PClass\n\nContinous Feature: Age\n\nOther features: PassengerId , Name  are unique variables","8748d724":"### Part 4: Feature engineering and creating a best suitable model\nSince KNN models and Random Forest are giving good Accuracy values when compared to other models\n\nRandom Forest is more preferable since :\n\n1. we can get important features for model tunning\n\n2. when compared to KNN,Random forest has a good balance between False Positive and False Negative Errors\n","09192550":"The passengers travelling in First class has a good survival rate 62%\n\nSecond class with 47% has average survival rate\n\nThird class with 24% is the least\n\nHence this acts as an important variable to describe the model","91d388ec":"### Missing features in data"}}