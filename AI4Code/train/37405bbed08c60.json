{"cell_type":{"a7f2dde9":"code","20afa5f6":"code","958d0473":"code","dc427010":"code","1678aecc":"code","a95df62c":"code","525491b0":"code","04fc33d9":"code","1c7627c2":"code","c3f91f79":"code","1967ead7":"code","80e0cb0d":"code","83124ef9":"code","d1629e01":"markdown","aeece9cc":"markdown","18f859ac":"markdown","9e65de33":"markdown","ed98146e":"markdown","469b04af":"markdown","e6fca682":"markdown"},"source":{"a7f2dde9":"import numpy as np # Numpy for numbers \nimport pandas as pd # Pandas for data stuff\nimport os\n\n# PyTorch for modeling\nimport torch \nfrom torch.autograd import Variable\n\n#Sklearn for ease of life\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import linear_model\nfrom scipy import stats","20afa5f6":"# Load dataset\ndf = pd.read_csv('\/kaggle\/input\/toy-dataset\/toy_dataset.csv').drop('Number', axis = 1)\n\n# Remove any obvious outliers\ndf = df[np.abs(df.Income-df.Income.mean()) <= (3*df.Income.std())]\n\n# Lets just randomize the set\nrand_df = df.sample(df.count()[0])\n\n# Separating into train and test\ntrain_test_ratio = 0.7\ncut_at = int(rand_df.count()[0]*train_test_ratio)\ntrain, test = rand_df[:cut_at], rand_df[cut_at:]\ntrain","958d0473":"cat = ['City', 'Gender', 'Illness'] # Categorical Fields\nctn = ['Age'] # Continuous Fields\naggs = ['mean','std', 'min', 'max'] # Aggregate Fields","dc427010":"def expands_variables(df, cats, conts, aggs=['mean', 'std', 'max', 'min'], powers=4, put_dummies=True):\n    \"\"\"\n    This functions expands categorical fields by aggregating them and appending data as new rows\n      this method also puts raises the continuous variables to powers if powers is not None\n      It also creates dummies if you ask it to!\n    \"\"\"\n    use_powers = (not powers==None) # Should use powers?\n    groups = cats + list(set([tuple(sorted([x,y]))  for x in cats for y in cats if x!=y ])) # Permutes categorical to create combinations \n    cont_filters = {key:aggs for key in conts} # Agg dict\n    temp = df.copy() # Create a copy from DataFrame\n    \n    # For each group do groupby and merge DataFrame\n    for idx in range(len(groups)):\n        g = groups[idx]\n        # To get rid of annoying warnnings \n        if type(g) == tuple:\n            g = list(g)\n        gb = temp.groupby(g).agg(cont_filters) # GroupBy\n        gb.columns = [\"_\".join([x[0],x[1],str(idx)]) for x in list(gb.columns)] # Rename columns so they don't overlap\n        temp = pd.merge(temp, gb, on=g, how=\"left\") # Merge DataFrame\n    \n    if use_powers: # If you desire to use powers\n        for x in conts:\n            for pw in range(powers-1):\n                temp[x+'_pow_'+str(pw+2)] = temp[x]**(pw+2) # Raise them powers and put to their respective names\n    if put_dummies:# If you desire to use dummies\n        for x in cats: \n            temp = temp.join(pd.get_dummies(temp[x], dtype=float), how='left') # Get them dummies\n    return temp # return transformed DataFrame","1678aecc":"feature_expanded_train = expands_variables(train, cat, ctn) # Expand Variables\ncat_remove_train = feature_expanded_train.drop(cat ,1) # Remove Categorical Values\nfeature_expanded_test = expands_variables(test, cat, ctn) # Expand Variables\ncat_remove_test = feature_expanded_test.drop(cat ,1) # Remove Categorical Values\nprint('From these columns:')\nprint(train.columns)\nprint('\\nWe derived these columns:')\nprint(cat_remove_train.columns) # Let us see the new ","a95df62c":"X_train = cat_remove_train.drop(['Income'],1).values # Get X Values for Training \nY_train = cat_remove_train[['Income']].values # Get True Values for Training \nX_test = cat_remove_test.drop(['Income'],1).values # Get X Values for Training \nY_test = cat_remove_test[['Income']].values # Get True Values for Training ","525491b0":"X_train = cat_remove_train.drop(['Income'],1).values # Get X Values for Training \nY_train = cat_remove_train[['Income']].values # Get True Values for Training \nclf = linear_model.Lasso(alpha=0.89)\nclf.fit(X_train, Y_train)","04fc33d9":"def test_model(X_test, Y_test, predict_func, use_batch=False, batch_size=100):\n    #predict\n    if(use_batch):\n        predict = []\n        batch_idx = int(X_test.shape[0]\/batch_size)\n        for x in range(batch_idx):\n            batchX = X_test[batch_size*x:batch_size*(x+1)]\n            predict.append(predict_func(X_test))\n    else:\n        predict = predict_func(X_test)\n    test_results = test.copy() # Create new DataFrame\n    test_results['Model'] = predict # Push Prediction\n    test_results['Error'] = test_results.Model\/test_results.Income # Push Error from Prediction\n    return test_results","1c7627c2":"sklearnEval = test_model(X_test, Y_test, clf.predict)\n\nimport matplotlib.pyplot as plt\n\nfig = plt.gcf()\nfig.set_size_inches(10, 10)\nax = plt.subplot(2, 1, 2)\nplt.title('Sklearn Model Distribution')\nsklearnEval.Income.hist(label='Actual Value')\nsklearnEval.Model.hist(label='Prediction')\nax.legend()","c3f91f79":"on_range = sklearnEval.loc[(sklearnEval.Error >= 0.9) & (sklearnEval.Error <= 1.1)].count()[0]\ntotal = sklearnEval.count()[0]\nprint('Sklearn 10% Accuracy:'+' %.2f'%((on_range\/total)*100)+'%')\n\non_range = sklearnEval.loc[(sklearnEval.Error >= 0.8) & (sklearnEval.Error <= 1.2)].count()[0]\ntotal = sklearnEval.count()[0]\nprint('Sklearn 20% Accuracy:'+' %.2f'%((on_range\/total)*100)+'%')\n","1967ead7":"print(clf.coef_)\nprint(clf.intercept_)","80e0cb0d":"i = 0\ndrop_colinear = []\nfor x in clf.coef_:\n    if(x>-0.001 and x<0.001):\n        drop_colinear.append(cat_remove_train.drop(['Income'],1).columns[i])\n    i += 1\n\nprint(\"We can drop\",len(drop_colinear),\"colinear features\")","83124ef9":"# Filtered Train Set\nfeature_expanded_train = expands_variables(train, cat, ctn) # Expand Variables\ncat_remove_train = feature_expanded_train.drop(cat ,1) # Remove Categorical Values\nflltered_expansion_train = cat_remove_train.drop(drop_colinear, 1) # Drop Colinear Variables\n# Flitered Test Set\nfeature_expanded_test = expands_variables(test, cat, ctn) # Expand Variables\ncat_remove_test = feature_expanded_test.drop(cat ,1) # Remove Categorical Values\nflltered_expansion_test = cat_remove_test.drop(drop_colinear, 1) # Drop Colinear Variables\nflltered_expansion_train.head() # Let's take a look!","d1629e01":"## Testing Model Accuracy","aeece9cc":"These are the coefficients given by lasso, as you can see some of them are zero that indicates collinearity","18f859ac":" ## LASSO Regression With Sklearn\n \n We make a lasso regression in order to determine colinear variables derived from feature expansion","9e65de33":"The variables above have high importance for the model, it's interesting to note that Male was dropped, that makes sense since if you know whether or not a person is a female you can infer the opposite. ","ed98146e":"## Colinear Variables Found With LASSO","469b04af":"## Basics 1 - Classical Statistical Model\n\n### Objectives:\n* Demonstrate that feature engineering can improve results\n\n* From a LASSO Regression, remove any colinear variables generated while expanding variables","e6fca682":"## Feature Engineering\n\nEach combination of categorical fields may have diferent proprieties for each continuous data.\n\n*Eg.*\n\n People living in L.A. might have an avrage age of 46 years and a standard diviation of 11.6 years, while people living in Boston might have diferent proprieties for that particular continuous field.\n \n Don't worry about correlation between variables, Lasso will fix that by zeroing out some highly correlated variables, the idea here is to get the most features out of the data we have.\n \n We will also be adding powers to the continuous variables to make sure that any non-linear behaviour will be used:\n *Eg.*\n \n Income might be related to the square or cube of the age."}}