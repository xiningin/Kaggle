{"cell_type":{"d6db032d":"code","e0de0312":"code","01851d9a":"code","c7ca79b3":"code","f3bfd60e":"code","283e2422":"code","9c858667":"code","d0a924ac":"code","bc7bde4e":"code","02285b02":"code","8c262620":"code","6f4775e1":"code","b35f18b8":"code","0fecdb8b":"code","1e6149c0":"code","d6dc5ee6":"markdown","60881c22":"markdown"},"source":{"d6db032d":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\n\nfrom sklearn.preprocessing import MinMaxScaler","e0de0312":"torch.manual_seed(1)\n\nif torch.cuda.is_available():\n    device = 'cuda'\n    torch.cuda.manual_seed_all(1)\nelse:\n    device = 'cpu'","01851d9a":"train = pd.read_csv('..\/input\/nba-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/nba-prediction\/test.csv')","c7ca79b3":"train.shape, test.shape","f3bfd60e":"train.head()","283e2422":"# Null \uac12 \ud655\uc778\ntrain.isnull().sum()","9c858667":"# \ud574\ub2f9 \ud589 \uc81c\uac70\ntrain.dropna(inplace=True)","d0a924ac":"# \ud559\uc2b5\uc744 \uc704\ud55c \ub370\uc774\ud130 \uc900\ube44\nX_train = train.drop(['ID', 'Win'], axis=1)\nX_test = test.drop('ID', axis=1)\ny_train = train['Win']\n\n\n# Scaling\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\n# Tensor\ub85c \ubcc0\ud658\nX_train = torch.FloatTensor(X_train).to(device)\nX_test = torch.FloatTensor(X_test).to(device)\n\ny_train = torch.LongTensor(y_train.values).to(device)","bc7bde4e":"# Hyperparameter \uc124\uc815\nlearning_rate = 0.1\nn_epochs = 500\ndrop_prob = 0.3\n\n\n# Model \uc124\uc815\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        \n        self.fc1 = nn.Linear(X_train.shape[1], 256)\n        self.fc2 = nn.Linear(256, 256)\n        self.fc3 = nn.Linear(256, 256)\n        self.fc4 = nn.Linear(256, 256)\n        self.fc5 = nn.Linear(256, 256)\n        self.fc6 = nn.Linear(256, 2)\n        \n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=drop_prob)\n        \n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight.data)\n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.dropout(out)\n        \n        out = self.fc2(out)\n        out = self.relu(out)\n        out = self.dropout(out)\n        \n        out = self.fc3(out)\n        out = self.relu(out)\n        out = self.dropout(out)\n        \n        out = self.fc4(out)\n        out = self.relu(out)\n        out = self.dropout(out)\n        \n        out = self.fc5(out)\n        out = self.relu(out)\n        out = self.dropout(out)\n        \n        out = self.fc6(out)\n        return out","02285b02":"model = Net().to(device)\n\n\n# optimizer\uc640 Loss Function \uc124\uc815\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\nloss_fn = nn.CrossEntropyLoss()\n\n\n# \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uc704\ud574 Learning rate Scheduler \uc0ac\uc6a9\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.5)","8c262620":"## \ud559\uc2b5 ##\n\nfor epoch in range(1, n_epochs+1):\n    model.train()\n    H = model(X_train)\n    loss = loss_fn(H, y_train)\n    \n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    accuracy = (torch.argmax(H, dim=1) == y_train).float().mean()\n    \n    scheduler.step()\n    \n    if epoch % 20 == 0:\n        print('Epoch {:4d} \/ {}, Loss : {:.4f}, Accuracy : {:.2f} %'.format(\n            epoch, n_epochs, loss.item(), accuracy*100))","6f4775e1":"with torch.no_grad():\n    model.eval()\n    pred = model(X_test)","b35f18b8":"submit = pd.read_csv('..\/input\/nba-prediction\/sample_submit.csv')","0fecdb8b":"submit['Win'] = torch.argmax(pred, dim=1).cpu()\nsubmit.head()","1e6149c0":"submit.to_csv('submission.csv', index=False)","d6dc5ee6":"## Data Preparation","60881c22":"## Training"}}