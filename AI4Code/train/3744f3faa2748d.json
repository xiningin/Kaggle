{"cell_type":{"a4457e29":"code","e5712156":"code","d2feab6e":"code","f87a276d":"code","c2ee2e05":"code","8140632b":"code","865504e6":"code","11eeca39":"code","95caf944":"code","da97b611":"code","1fdb7dda":"code","04d716f2":"code","67a90a00":"markdown","f8e56fea":"markdown","64db9ecc":"markdown","8651c65f":"markdown","9796d784":"markdown","8706b7b0":"markdown","fc7e8f2f":"markdown"},"source":{"a4457e29":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","e5712156":"df = pd.read_csv(\"..\/input\/ad-data\/Advertising.csv\")\ndf.head(5)","d2feab6e":"df['total_spend'] = df['TV'] + df['radio'] + df['newspaper']\nfinal_df = pd.concat([df['total_spend'],df['sales']],axis=1)\nfinal_df.head(5)","f87a276d":"#Defining x_i and y_i from our dataframe\nx_i = final_df['total_spend'] \ny_i = final_df['sales']","c2ee2e05":"from sklearn.preprocessing import StandardScaler","8140632b":"scaler = StandardScaler()\nx_i = np.array(x_i) # This is done because Standard scaler requires data as np arrays\nx_i = x_i.reshape(-1,1) # Reshaping as scaler needs 2D data\n\n# Scaling Part\nscaler.fit(x_i)\nscaled_x_i = scaler.transform(x_i)\n\n# Reshaping again for future use.(plots)\nscaled_x_i = scaled_x_i.reshape(-1)\n\n# Reassigning the data\nx_i = final_df['total_spend'] \ny_i = final_df['sales']","865504e6":"# This is the block which performs gradient descent.\n\n# Feel free to play with this hyperparameters. \nepoch = 75 # How many times?\nstep_size = 0.1 # Also known as learning rate\n\n# Initializing m and c to 0. \nm = 0     \nc = 0\n\n# Length of the data\nN = len(scaled_x_i)\n\n# Initializing some empty lists (plotting purpose)\nmse=[]\ncoef_m=[]\ncoef_c=[]\n\nwhile epoch>=0:\n    \n    y_hat = m*scaled_x_i + c \n    \n    J = (1\/N)*sum((y_i - y_hat)**2) \n    mse.append(J)\n    \n    J_m = (1\/N)*sum(2*scaled_x_i*(y_hat-y_i))   # Calculated Gradient with respect to m.\n    J_c = (1\/N)*sum(2*(y_hat-y_i))         # Calculated Gradient with respect to c.\n    \n    # Updating the m and c to achieve minimum error\n    m = m - step_size*J_m \n    coef_m.append(m)  # Storing the values of m and c to plot.\n    \n    c = c - step_size*J_c\n    coef_c.append(c)  # Storing the values of m and c to plot.\n    \n    # Loop condition\n    epoch=epoch-1","11eeca39":"# Thanks to Stephen Welch - Welch Labs\n\n# Interactive Visualization of coefficients m and c converging.\nfrom ipywidgets import interact\ndef plotGD(epoch=2):\n    fig = plt.figure(figsize=(12,10))\n    \n    plt.subplot(2, 1, 1)\n    plt.plot(coef_m[0:epoch], mse[0:epoch], linestyle='-', marker='o', linewidth=2, markeredgecolor='none',color='black')\n    plt.grid(1)\n    plt.title('Relation of $m$ with error')\n    plt.xlabel('Values of $m$')\n    plt.ylabel('Error')\n    #plt.xlim([3.5, 4.6])\n    #plt.ylim([40, 250])\n    \n    plt.subplot(2, 1, 2)\n    plt.plot(coef_c[0:epoch], mse[0:epoch], linestyle='-', marker='o', linewidth=2, markeredgecolor='none',color='black')\n    plt.grid(1)\n    plt.title('Relation of $m$ with error')\n    plt.xlabel('Values of $c$')\n    plt.ylabel('Error')\n    #plt.xlim([3.5, 4.6])\n    #plt.ylim([40, 250])\n\n\n\ninteract(plotGD, epoch=(1, 75));\n# Please go to Edit mode for interaction to work","95caf944":"def plotGD_animate(epoch=2):\n    fig = plt.figure(figsize=(12,6),dpi=100) \n    sns.scatterplot(x=x_i,y=y_i)\n    \n    y_hat = coef_m[epoch]*scaled_x_i + coef_c[epoch] # Its important to pass scaled values of x_i since we calculated in that way. \n    \n    plt.plot(x_i,y_hat,color='red',linestyle='-', linewidth=2, markeredgecolor='none')\n    plt.grid(1)\n    plt.title('Advertisement Data')\n    plt.xlabel('Total Spend in Lakh')\n    plt.ylabel('Total Sale Generated in Crores Rs.')\n\ninteract(plotGD_animate, epoch=(1, 75));\n# Please go to Edit mode for interaction to work","da97b611":"%matplotlib inline\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n\n\nfig, ax = plt.subplots(figsize=(12,8))\nsns.scatterplot(x=x_i,y=y_i)\n\nplt.text(300, 5, \"I am Learning!!\", fontsize = 22,bbox = dict(facecolor = 'yellow', alpha = 0.5))\nplt.grid(1)\nplt.title('Advertisement Data')\nplt.xlabel('Total Spend in Lakh')\nplt.ylabel('Total Sale Generated in Crores Rs.')\n\ni = list(range(0,75))\nx = scaled_x_i\ny = m*x + c\nline, = ax.plot(x_i, y,color='red')\n\n\ndef animate(i):\n    line.set_ydata(coef_m[i]*x + coef_c[i])  # update the data.\n    return line,\n\nani = animation.FuncAnimation(fig, animate, interval=100, blit=True, save_count=75);\nplt.close()\nHTML(ani.to_jshtml())","1fdb7dda":"# Rescaling m and c to fit in our previous data\nalpha = scaler.scale_[0]  \n\n# Rescaling \nm_new = m\/scaler.scale_[0]\nc_new = c\/scaler.scale_[0]\n\nm_new,c_new # These are the values of m and c. ","04d716f2":"RMSE = np.sqrt(mse[75])\nprint(RMSE)","67a90a00":"## Without scaling the code was not converging. - So I am using the Standard Scaler.\n(Any help in understanding why this exactly happens is highly appreciated.I am still not able to find a convincing answer.)","f8e56fea":"# Rescaling  - Just to scale back in the original domain. ","64db9ecc":"# Evaluation.","8651c65f":"# Linear Regression - Gradient descent method.\n\nIn this method we iteratively calculate the value of $m$ and $c$ such that it reduces the mean of squares of the error.\n\nWe have already defined what a error is in part1 notebook.\nLets say we have average squared error as follows,\n\n\nMean Squared Error,\n$$\nMSE = {\\frac{1}{N}\\sum_{i=0}^{N}\\epsilon_{i}^{2}}\n$$\nLet us define a function **J** and plugin $\\epsilon_{i}$\n\n$$\nJ = \\frac{1}{N}\\sum_{i=0}^{N}(y_{i}-\\hat{y})^{2}\n$$\n\nPlugging in the nature of our prediction equation i.e $\\hat{y} = mx_{i} + c$\n$$\nJ = \\frac{1}{N}\\sum_{i=0}^{N}(y_{i}-(mx_{i} + c))^{2}\n$$\n\nThis J is generally called the **Cost Function**. \n\nNow our aim is to minimise it. \n\nAnalytically we equate the partial derivative with respect to each coefficient of the cost function J to 0 and find the minimum. \nHere we calculate it in steps where in each step we update the value of coefficients and try to get the minimum error possible.\n\nSolving for partial derivatives we get. Check this file for calculation *partialderivative.pdf*\n$$\n\\frac{\\partial J}{\\partial m} = \\frac{1}{N}\\sum_{i=0}^{N}2x_{i}(\\hat{y}-y_{i})\n$$\n\n\n$$\n\\frac{\\partial J}{\\partial c} = \\frac{1}{N}\\sum_{i=0}^{N}2(\\hat{y}-y_{i})\n$$\n\nLets get started!\n","9796d784":"# Visualize for yourself how the curve is learning. :)","8706b7b0":"# Summary\n\nSo comparing to the method in part 1 we arrived at the same result of obtaining m and c and the RMSE is also same.","fc7e8f2f":"# Some more Visualization"}}