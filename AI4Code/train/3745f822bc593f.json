{"cell_type":{"a0b9c764":"code","17c7bab1":"code","5c34fbf0":"code","24054944":"code","29cd8744":"code","9b2f978e":"code","54093872":"code","76904076":"code","f6e856e0":"code","5b0e4b87":"code","e42accb2":"code","94194159":"code","c919f40b":"code","c425e008":"code","ebb21838":"code","233b4f50":"code","d40c8f9a":"code","a877b33c":"code","080efb86":"code","a1101b1c":"code","ad73962a":"code","c44c021b":"code","d4756660":"code","e8edafd7":"code","cee2ec6c":"code","a52270b0":"code","8f2babd7":"code","fb62520b":"code","9b98c091":"code","90eb6359":"code","0850d454":"code","1bf6d9eb":"code","6c8eda84":"code","06fd9d43":"code","a40072a9":"code","12edfb50":"code","a1f05317":"code","b4b586a6":"code","c15f16d3":"code","97be5fa0":"code","ae2d9170":"markdown","772104c3":"markdown","6012df6d":"markdown","f6244463":"markdown","405c36dd":"markdown","d5a6029c":"markdown","6ecafbfd":"markdown","7b60864c":"markdown","8d954be6":"markdown","f107a989":"markdown","cc3e2691":"markdown","97a2ef73":"markdown","b691b922":"markdown","a0b02a84":"markdown","18ae30fa":"markdown","b5f7ae57":"markdown","5ccac193":"markdown","d33ed904":"markdown"},"source":{"a0b9c764":"# !pip install --upgrade pip\n# was required to install certail packages like plotly and dataprep","17c7bab1":"# pip install plotly","5c34fbf0":"import pandas as pd\nimport numpy as np\n\n# data visualization library \nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n# sns.set(context='notebook', style='darkgrid', palette='colorblind', font='sans-serif', font_scale=1, rc=None)\n# matplotlib.rcParams['figure.figsize'] =[8,8]\n# matplotlib.rcParams.update({'font.size': 15})\n# matplotlib.rcParams['font.family'] = 'sans-serif'\n","24054944":"# !pip install dataprep==0.3.0","29cd8744":"# # dataprep\n# from dataprep.eda import plot\n# from dataprep.eda.missing import plot_missing\n# from dataprep.eda import plot_correlation\n# from dataprep.eda import create_report","9b2f978e":"# import dataprep\n# # dataprep.__version__","54093872":"covid = pd.read_csv('..\/input\/symptoms-and-covid-presence\/Covid Dataset.csv')\ncovid","76904076":"covid.info()","f6e856e0":"covid.describe(include='all')","5b0e4b87":"covid.columns","e42accb2":"#creating a full report : from dataprep\n# create_report(covid, title='Whole Covid Report')","94194159":"# plot_missing(covid) # method from dataprep","c919f40b":"# creatting a table with missing data\nmissing_values = covid.isnull().sum() # missing values\npercent_missing = covid.isnull().sum()\/covid.shape[0]*100 # missing value %\n\nvalue = {\n    'missing_values ':missing_values,\n    'percent_missing %':percent_missing  \n}\nmissing_frame=pd.DataFrame(value)\nmissing_frame\n","c425e008":"# plot(covid) # method from dataprep","ebb21838":"# method from data prep\n# plot(covid,'COVID-19' )","233b4f50":"covid['COVID-19'].value_counts()\n# Imbalanced dataset","d40c8f9a":"covid[\"COVID-19\"].value_counts().plot.pie(explode=[0.1,0.5],autopct='%1.2f%%',shadow=True)\nplt.title('COVID Positive?')","a877b33c":"# Distribution of target variable\nsns.countplot(x='COVID-19',data=covid)","080efb86":"features = covid.columns.tolist()\nfeatures.remove('COVID-19')\nprint(features)","a1101b1c":"for feature in features:\n    plt.figure()\n    sns.countplot(x= feature,hue='COVID-19',data=covid)","ad73962a":"covid.shape","c44c021b":"covid.head()","d4756660":"## One-hot encoding on features\n# covid_OH = covid.drop('COVID-19', axis=1)\ncovid_OH = pd.get_dummies(covid, drop_first=True) # to avoid dummy trap\ncovid_OH.head()","e8edafd7":"print(covid_OH.shape)","cee2ec6c":"covid_OH.dtypes.value_counts()\n# all the columns got converted to ","a52270b0":"covid_OH.hist(figsize=(20,15))\n# All the features (and target variable) got converted to [0,1]","8f2babd7":"# plot_correlation(covid_OH) #method from data-prep\n# Covid-19 postive seems to be correlated (linearly) with some features like, Breathing problem, Fever, Dry_cough, Sore_Throat, Abroad_travel","fb62520b":"# Plotting correlation plot\ncorr = covid_OH.corr()\ncorr.style.background_gradient(cmap='coolwarm',axis=None)","9b98c091":"covid_OH = covid_OH.drop(['Running Nose_Yes', 'Asthma_Yes', 'Chronic Lung Disease_Yes', \n                          'Headache_Yes', 'Heart Disease_Yes', 'Diabetes_Yes', 'Fatigue _Yes', 'Gastrointestinal _Yes'], axis=1)","90eb6359":"# correlations among selected features\ncorr = covid_OH.corr()\ncorr.style.background_gradient(cmap='coolwarm',axis=None)","0850d454":"covid_OH.rename(columns = {'COVID-19_Yes':'COVID-19'}, inplace=True)\ncovid_OH.columns","1bf6d9eb":"from sklearn.model_selection import train_test_split\nfrom sklearn import metrics","6c8eda84":"# Splitting Dependent and Independent features\nX = covid_OH.drop('COVID-19',axis=1)\ny = covid_OH['COVID-19']","06fd9d43":"# train - test split\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, stratify=y)","a40072a9":"# Finding out optimum k-value using elbow method\nfrom sklearn.neighbors import KNeighborsClassifier\n\nerror_rate = []\n\n# calculating error rate\nfor i in range(1,40):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(x_train, y_train)\n    pred_i = knn.predict(x_test)\n    error_rate.append(np.mean(pred_i != y_test))\n\n\n# Plotting elbow graph\nplt.figure(figsize=(10,6))\nplt.plot(range(1,40), error_rate, color=\"blue\", linestyle=\"dashed\", marker=\"o\",\n         markerfacecolor=\"red\", markersize=10)\nplt.title(\"Error Rate vs. K Value\")\nplt.xticks(range(1,40))\nplt.xlabel(\"K\")\nplt.ylabel(\"Error Rate\")","12edfb50":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import tree\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import svm\n\n\n# defining models\nmodels = [LogisticRegression(), RandomForestClassifier(), XGBClassifier(), KNeighborsClassifier(), \n          tree.DecisionTreeClassifier(), GaussianNB(), svm.SVC(kernel='linear')]\n\n# defining model names\nmodel_names = ['Logistic Regression', 'Random Forest', 'Gradient Boosting Classifier', 'KNN', 'Decision Tree', \n               'Naive Bayes', 'Support Vector Machines']\n\n# defining parameters\n\nparameters = [{'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']},\n              \n             {'n_estimators':[100, 300, 500, 600, 700, 1000], 'criterion':['gini', 'entropy'], \n              'max_depth' : [10, 20, 25, 30, 35, 40], 'min_samples_split': [100, 200, 50, 25]},\n              \n             {\"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] , \"max_depth\": [ 3, 4, 5, 6, 8, 10, 12, 15],\n              \"min_child_weight\" : [ 1, 3, 5, 7 ], \"gamma\": [ 0.0, 0.1, 0.2 , 0.3, 0.4 ], \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ], \n              \"n_estimators\": [100, 120, 135, 150, 165, 200]},\n             \n             {'n_neighbors' : [3, 11]},\n             \n            {'criterion':['gini', 'entropy'], 'max_depth' : [10, 20, 25, 30, 35, 40], 'min_samples_split': [100, 200, 50, 25]},\n              \n            {},\n              \n            {'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}\n             ]","a1f05317":"%%time\n\nfrom sklearn.model_selection import GridSearchCV # grid search CV was taking hours to calculate all the combinations\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfor model_idx in range(len(models)):\n#     classifier = GridSearchCV(estimator = models[model_idx], param_grid = parameters[model_idx], n_jobs=-1)\n   \n    classifier = RandomizedSearchCV(estimator = models[model_idx], param_distributions = parameters[model_idx], random_state=0, n_jobs=-1)\n    classifier.fit(x_train, y_train)\n    \n    print(model_names[model_idx])\n    print(classifier.best_estimator_)\n    print(classifier.best_score_)\n    print(classifier.best_params_)\n    print(\"\\n******************************************************************************************************************\\n\")\n","b4b586a6":"# choosing the final model\n\nclassifier = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.4, gamma=0.3, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.2, max_delta_step=0, max_depth=5,\n              min_child_weight=1, missing=np.nan, monotone_constraints='()',\n              n_estimators=200, n_jobs=0, num_parallel_tree=1, random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=None)\n\nclassifier.fit(x_train, y_train)\ny_pred = classifier.predict(x_test)\nclassifier.score(x_test, y_test)*100","c15f16d3":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","97be5fa0":"plt.figure(figsize=(20,8))\n# decomposing using pca and plotting classification boundary\nfrom sklearn.decomposition import PCA\nfrom mlxtend.plotting import plot_decision_regions\n\npca = PCA(n_components = 2)\nx_train_pca = pca.fit_transform(x_train)\nclassifier.fit(x_train_pca, y_train)\nplot_decision_regions(x_train_pca, y_train.to_numpy(), clf=classifier, legend=2)\n\nplt.title('Classification boundary', size=16)","ae2d9170":"##### Observation: No missing values","772104c3":"##### Observations: Our target variable is clearly imbalanced, i.e., we are dealing with imbalanced dataset","6012df6d":"### Model building : with Hyper parameter tuning","f6244463":"### Checking missing value","405c36dd":"### Feature selection \n* Choosing those featuers which are somehow correlated to COVID-19 positive (correlation > 0.1)","d5a6029c":"##### Observations:\n\n1. **Breathing problem**: Presence of Breathing problem ('Yes') does seem to have an impact among covid positive patience\n2. **Fever**: Presence of Fever ('yes') is also very common among covid positive patients\n3.**Dry Cough**: Covid positive people are mostly suffering with Dry Cough\n4. **Sore throat**: Covid postive people are also suffering from Sore throat\n5. **Hypertension**: Presence or absense of Hypertension doesn't seem to have any distinct impact\n6. **Abroad travel**: Although postive patients are there who haven't had any abroad travel, but if a person is having abroad travel, there is almost full chance that the person will have covid-19\n7. **Contact with COVID Patients**: people with contact with covid patients are having more chances of being covid positive\n8. **Attended Large Gathering**: People who have attended large gatherings, are more probable of getting covid\n9. visited **public explosed places** and family working in public exposed places, although have high positive rate, but doesn't seem to have any greater impact then those who are not exposed to such public places\n10. **Wearing mask** and **Sanitization from market** has only one category i.e., *No*.","6ecafbfd":"### Feature Correlation","7b60864c":"1. Target Variable: COVID-19","8d954be6":"#### Observation:  Wearing Masks and Sanitization from Market got dropped as they were only having one category, which is not helpful in classification","f107a989":"#### Hyper-parameter tuning using Grid Search CV","cc3e2691":"### [Dataprep](https:\/\/analyticsindiamag.com\/exploring-dataprep-a-python-library-for-data-preparation-eda\/):\n* A python library for data preparation and EDA\n* can collect data from multiple data sources using the **dataprep.connector** module\n* perform intense exploratory analysis using the **dataprep.eda** module \n* clean and standardize datasets using the **dataprep.clean** module.\n\n**Here, I have used plot(), plot_missing(), plot_correlation() and create_report() methods from dataprep library**","97a2ef73":"### Data vizualisation","b691b922":"##### Observations: XGBoostClassifier is giving the best output with hyper-parameter tuning","a0b02a84":"#### Selecting models","18ae30fa":"### Data analysis","b5f7ae57":"#### [plot decision regions for more than 2 features](https:\/\/stackoverflow.com\/questions\/52952310\/plot-decision-regions-with-error-filler-values-must-be-provided-when-x-has-more)","5ccac193":"##### Observation: k=3 seems the most optimum value","d33ed904":"### Feature transformation"}}