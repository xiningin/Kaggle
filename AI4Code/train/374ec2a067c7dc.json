{"cell_type":{"05487286":"code","af777680":"code","579d8527":"code","ba474d71":"code","fd725117":"code","3cbf0df1":"code","8b666aeb":"code","805f0ffa":"code","ce10c664":"code","f2e21ab4":"code","b17589ec":"code","4b9ff1f0":"code","12d39cae":"code","227c6ec7":"code","e7c8c761":"code","97beec05":"code","b6d6261f":"code","402c5b65":"code","fa4d00b3":"code","0828db2d":"code","2511720c":"code","7f4ae17f":"code","96dce3d5":"code","96933b9a":"code","efd1fbee":"code","3248eaea":"code","339a762c":"code","b95f8a39":"code","44bc2888":"code","fc3fbfe7":"code","a75fb133":"code","9f584c61":"code","2321588a":"code","bbd51945":"code","4dd211da":"code","b598dada":"code","ee122bcb":"code","6345858e":"code","dad2473d":"code","136833af":"code","5246440c":"code","c39d0d35":"markdown","4f1d009c":"markdown","4d0de81d":"markdown","b4c554dc":"markdown"},"source":{"05487286":"# Todas as importa\u00e7\u00f5es\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns","af777680":"# Todos os par\u00e2metros\nTRAIN_PATH = '\/kaggle\/input\/titanic\/train.csv'\nTEST_PATH = '\/kaggle\/input\/titanic\/test.csv'\nGENDER_SUBM = '\/kaggle\/input\/titanic\/gender_submission.csv'","579d8527":"# Funcoes e classes","ba474d71":"# Execucao\ntrain = pd.read_csv(TRAIN_PATH)\ntrain.head()","fd725117":"# Variavel idade contem valores ausentes, Cabin e Embarked tambem. Todos deverao sem tratados\n# 891 passageiros para treinamento\ntrain.info()","3cbf0df1":"test = pd.read_csv(TEST_PATH)\ntest.head()","8b666aeb":"# Teste nao contem a variavel survived pois sera o que vamos prever.\ntest.info()","805f0ffa":"test.PassengerId","ce10c664":"# Este sera o arquivo a ser submetido. Temos acima o conjunto de testes com varios IDS,\n# e para submissao no Kaggle devemos colocar o id de cada um e se o mesmo sobreviveu ou nao.\nsubmission = pd.read_csv(GENDER_SUBM)\nsubmission.head()","f2e21ab4":"# Primeiramente vamos tratar os dados de entrada\n# Algumas variaveis parecem muito \"sujas\" para serem alimentadas j\u00e1 de inicio\n# Fica como exercicio descobrir se faz sentido alimentar PassengerId e Name ou nao\n\n\nNUM_COLS = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\nCAT_COLS = ['Sex', 'Cabin', 'Embarked', 'Ticket']\nTARGET_COL = 'Survived'\ntrain.head()","b17589ec":"# Investigando a distribuicao das variaveis numericas\nfor col in NUM_COLS:\n    train[col].hist()\n    plt.title(col)\n    plt.show()\n    ","4b9ff1f0":"sns.pairplot(train[NUM_COLS + [TARGET_COL]], hue=TARGET_COL)","12d39cae":"# Investigando a distribuicao das variaveis categoricas\n# Exemplo\ncol = CAT_COLS[0]\ntrain[col].value_counts().plot.bar()","227c6ec7":"for col in CAT_COLS:\n    train[col].value_counts().plot.bar()\n    plt.title(col)\n    plt.show()","e7c8c761":"# Cabin cont\u00e9m muitos valores distintos, n\u00e3o introduz informa\u00e7\u00f5es que poderiam ser correlacionadas com o resto dos dados\n# Atributos para considerarmos para engenharia de atributos: Name, Cabin, Ticket\n\nCAT_COLS.remove('Ticket')","97beec05":"# Deixando explicito que removi posteriormente apos conferir os graficos de distribuicao\nCAT_COLS.remove('Cabin')","b6d6261f":"# Tratamento de valores numericos ausentes\n# Um bom ponto de partida \u00e9 imputar com a mediana\ntrain[NUM_COLS].info()","402c5b65":"# Aqui, ambos treino e teste sao atualizados com dados do conjunto de treino, pois em um ambiente real, \n# n\u00e3o temos acesso aos dados de teste (simulando dados de producao)\n\ntrain_med = train[NUM_COLS].median()\ntrain[NUM_COLS]=train[NUM_COLS].fillna(train_med)\ntest[NUM_COLS]=test[NUM_COLS].fillna(train_med)","fa4d00b3":"train[NUM_COLS].info()","0828db2d":"# Uma sugestao para inputar valores ausentes em dados categoricos \u00e9 usar get_dummies\n# O mesmo substituira por 0 em todas as colunas de um valor ausente\n# Outra alternativa seria preencher com o valor mais frequente\ntrain[CAT_COLS].info()","2511720c":"train_cat_dummy = pd.get_dummies(train[CAT_COLS])\ntrain_cat_dummy","7f4ae17f":"# Por exemplo, se sexo estivesse ausente, ficaria 0 0\ntest_cat_dummy = pd.get_dummies(test[CAT_COLS])\ntest_cat_dummy","96dce3d5":"from sklearn.model_selection import train_test_split\n\n# Agora todos os dados de entrada sao numericos\nX = pd.concat([train[NUM_COLS], train_cat_dummy], axis=1)\ny = train[TARGET_COL]","96933b9a":"# posteriormente podemos usar validacao cruzada, por simplicidade aqui vamos usar\n# train_test_split para criar o conjunto de validacao\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42)","efd1fbee":"X_test = pd.concat([test[NUM_COLS], test_cat_dummy], axis=1)\ndisplay(X_test.head())","3248eaea":"# Vamos iniciar a triagem com regressao logistica\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train, y_train)","339a762c":"lr.score(X_train, y_train) # Score padrao em classificadores eh a acuracia","b95f8a39":"lr.score(X_val, y_val)","44bc2888":"# Conjunto de validacao melhor que de treino eh um sinal de overfitting, vamos testar RandomForest","fc3fbfe7":"from sklearn.ensemble import RandomForestClassifier","a75fb133":"rf = RandomForestClassifier()\nrf.fit(X_train, y_train)\nrf.score(X_train, y_train)","9f584c61":"rf.score(X_val, y_val)","2321588a":"# Temos um exemplo de overfitting com random forest. \n# Score de treino quase perfeito e de teste menor que de LR","bbd51945":"from xgboost import XGBClassifier","4dd211da":"xgb = XGBClassifier()\nxgb.fit(X_train, y_train)\nxgb.score(X_train, y_train)","b598dada":"xgb.score(X_val, y_val)","ee122bcb":"# At\u00e9 o momento LR tem se saido melhor, vamos testar submeter ele","6345858e":"# Survived \u00e9 um valor aleatorio no momento, vamos substituir pelo survived que \n# prevemos com regressao logistica\nsubmission.head()","dad2473d":"submission['Survived']=lr.predict(X_test)","136833af":"submission.to_csv('submission.csv', index=False)","5246440c":"pd.read_csv('submission.csv')","c39d0d35":"### Divis\u00e3o X e y Treinamento e X\/y Teste","4f1d009c":"## Criando um modelo baseline","4d0de81d":"## Awari ML - Atividade 2\n- Constru\u00e7\u00e3o desse notebook, passo a passo: https:\/\/www.loom.com\/share\/243bc6e99e3347d99138f0f2d96559e8\n\n- Continua\u00e7\u00e3o do v\u00eddeo: https:\/\/www.loom.com\/share\/1108a1a19a6d432f921f2bcfa86ce865","b4c554dc":"Score no conjunto de teste: 0.76. A partir daqui v\u00e1rias coisas podem ser melhoradas:\n- Utilizar valida\u00e7\u00e3o cruzada, desta forma, aproveitando todo o conjunto de treino\n- Calibra\u00e7\u00e3o dos modelos para lidar com overfitting\/underfitting\n- Fazer engenharia de atributos para aproveitamento dos outros atributos\n- Outros metodos para inputar valores ausentes, por exemplo, treinar um regressor para prever a idade de um passageiro a partir dos outros atributos. \n- Confiram tambem os outros notebooks, estrat\u00e9gias como stacking de v\u00e1rios modelos podem ser ignoradas por hora, tentem chegar \u00e0 um melhor resultado usando um \u00fanico modelo (ou combina\u00e7\u00e3o do mesmo)\n"}}