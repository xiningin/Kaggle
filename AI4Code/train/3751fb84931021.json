{"cell_type":{"7d89c67f":"code","df987bed":"code","9d526f0e":"code","52f8219d":"code","35b9f6ca":"code","b3dc9415":"code","e05ad06c":"code","6a231a66":"code","65a34280":"code","badd75ef":"markdown","02b12158":"markdown","e32f9d31":"markdown","8d0c5e17":"markdown","53f127ba":"markdown","606b54cb":"markdown","c78234eb":"markdown","ec9e3d28":"markdown","305b5d92":"markdown","f343d5c6":"markdown","846ccd38":"markdown","5ebaa9d8":"markdown"},"source":{"7d89c67f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Activation\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","df987bed":"#Generate 2 sets of X variables\n#LSTMs have unique 3-dimensional input requirements \nseq_length=5\nX =[[i+j for j in range(seq_length)] for i in range(100)]\nX_simple =[[i for i in range(4,104)]]\nX =np.array(X)\nX_simple=np.array(X_simple)","9d526f0e":"X","52f8219d":"X_simple","35b9f6ca":"y =[[ i+(i-1)*.5+(i-2)*.2+(i-3)*.1 for i in range(4,104)]]\ny =np.array(y)\nX_simple=X_simple.reshape((100,1))\nX=X.reshape((100,5,1))\ny=y.reshape((100,1))\n","b3dc9415":"y","e05ad06c":"model = Sequential()\nmodel.add(LSTM(8,input_shape=(5,1),return_sequences=False))#True = many to many\nmodel.add(Dense(2,kernel_initializer='normal',activation='linear'))\nmodel.add(Dense(1,kernel_initializer='normal',activation='linear'))\nmodel.compile(loss='mse',optimizer ='adam',metrics=['accuracy'])\nmodel.fit(X,y,epochs=2000,batch_size=5,validation_split=0.05,verbose=0);\nscores = model.evaluate(X,y,verbose=1,batch_size=5)\nprint('Accurracy: {}'.format(scores[1])) \nimport matplotlib.pyplot as plt\npredict=model.predict(X)\nplt.plot(y, predict-y, 'C2')\nplt.ylim(ymax = 3, ymin = -3)\nplt.show()","6a231a66":"model2 = Sequential()\nmodel2.add(Dense(8, input_dim=1, activation= 'linear' ))\nmodel2.add(Dense(2, activation= 'linear' ))\nmodel2.add(Dense(1, activation= 'linear' ))\nmodel2.compile(loss='mse',optimizer='rmsprop',metrics=['accuracy'])\nmodel2.fit(X_simple,y,epochs=2000,batch_size=5,validation_split=0.05,verbose=0);\nscores2 = model2.evaluate(X_simple,y,verbose=1,batch_size=5)\nprint('Accurracy: {}'.format(scores2[1]))","65a34280":"predict2=model2.predict(X_simple)\nplt.plot(y, predict2-y, 'C2')\nplt.ylim(ymax = 3, ymin = -3)\nplt.show()","badd75ef":"Here we can see the LSTM model doing a fairly good job at prediction until the upper range. Normalization should address this.","02b12158":"This is what the y-array looks like.","e32f9d31":"Next generate a simple lagged y-variable.","8d0c5e17":"And now for the RNN model with near-identical parameters.","53f127ba":"The likely reason why the RNN outperformed the LSTM is because of lag component only spanned 3 time steps. Some sources have stated that when the relationships span longer time frames, LSTMs will tend to perform best.","606b54cb":"The RNN model virtually perfect. This is why they call them universal function approximators!","c78234eb":"So now we can see how the LSTM model is trying to find a pattern from the sequence [0, 1, 2, 3, 4, 5] to \u2192 6, while the RNN is only focused on a pattern from [4] to \u2192 6.\n\nNext we build the LSTM model.","ec9e3d28":"Here is the LSTM-ready array with a shape of (100 samples, 5 time steps, 1 feature)","305b5d92":"There is some confusion about how LSTM models differ from RNNs, both in input requirements and in performance. One way to become more comfortable with LSTM models is to generate a data set that contains some lagged components, then build both a LSTM and regular RNN model to compare their performance and function.","f343d5c6":"First we generate the uni-dimensional input that both models will need.","846ccd38":"Resource article :https:\/\/medium.com\/@dclengacher\/keras-lstm-recurrent-neural-networks-c1f5febde03d","5ebaa9d8":"And the RNN-ready array has a shape of (100 samples, 1 feature). Note the key differnece is the lack of time steps or sequence."}}