{"cell_type":{"0617bca8":"code","9cf265d7":"code","02cfee34":"code","a6b29235":"code","7fdebc4e":"code","b6dacf84":"code","0f8adb2e":"code","b0345ec6":"code","61cbd061":"code","1021c85c":"code","dd170af2":"code","b651559e":"code","e02bd5fe":"code","7927957b":"code","674b5d6a":"code","5bbb8325":"code","189b34e4":"code","eb56cbb0":"code","0af74d1b":"code","d8645510":"code","d69c3c16":"code","9c9ef2f9":"code","f37b4117":"code","5d9e8daa":"code","10caaafd":"code","3ce8e6c7":"code","f42532e5":"code","c3cf8eac":"code","2d352af4":"code","88122e63":"code","b6188e81":"code","1bec618b":"code","b39ee31f":"code","39ebb4da":"code","885c6178":"code","2266ccf4":"code","29277fe1":"code","35b5cebe":"code","6e1fd6fd":"code","645f1eb1":"code","a7851d37":"code","fb07f3da":"code","f9d432af":"markdown","cc3d5797":"markdown","b11cb8ef":"markdown","bcc50888":"markdown","dffde8e0":"markdown","ba33b33b":"markdown","654e6ba8":"markdown","a10f717c":"markdown","5fcb94dc":"markdown","641c5856":"markdown","07614f19":"markdown","1557e4b4":"markdown","3caab8a1":"markdown","057d0467":"markdown","b0c860d7":"markdown","fad83238":"markdown","e5afa3d7":"markdown","53c7e808":"markdown","866b766e":"markdown","95eb0eaa":"markdown","2abf7038":"markdown","14d273be":"markdown","9a5472a4":"markdown","d95eacb6":"markdown","85d9a414":"markdown","fd6b64d1":"markdown","db7f15f9":"markdown","1b0ce122":"markdown","134d576b":"markdown","7cb5922e":"markdown","7e8b824b":"markdown","0dc65131":"markdown","cfb82eab":"markdown","8d74a1ec":"markdown","3c9db857":"markdown","4e9da2b9":"markdown","0dd965ce":"markdown","1dc31b71":"markdown","29a5f355":"markdown"},"source":{"0617bca8":"# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns","9cf265d7":"# Importing the dataset\ndf_Train = pd.read_csv('TelcoCustomerChurnDataset.csv')\n\n# Dataset Information\ndf_Train.info()","02cfee34":"df_Train['TotalCharges']","a6b29235":"# Convert column TotalCharger from object to float\ndf_Train['TotalCharges'] = df_Train['TotalCharges'].apply(pd.to_numeric)","7fdebc4e":"# Missing Values\ndf_Train.isnull().sum()","b6dacf84":"# First DataFrame rows\ndf_Train.head(10)","0f8adb2e":"# Describing The Data\ndf_Train.describe(include = 'all')","b0345ec6":"def autolabel(patches,ax,mode):\n    if mode == 'percentage':\n        \"\"\"Display Percentage\"\"\"\n        for j in range(len(patches)):\n            rects = patches[j]\n            height = rects.get_height()\n            percentage = '{:.1f}%'.format(rects.get_height())       \n            ax.annotate(percentage,\n                        xy=(rects.get_x() + rects.get_width() \/ 2, height),\n                        xytext=(0, 0.5),\n                        textcoords=\"offset points\",\n                        ha='center', va='bottom')            \n    elif mode == 'count':\n        \"\"\"Display Count\"\"\"\n        for j in range(len(patches)):\n            rects = patches[j]\n            height = rects.get_height().astype('int')   \n            height = height if height >= 0 else -1 # To avoid error\n            ax.annotate(height,\n                        xy=(rects.get_x() + rects.get_width() \/ 2, height),\n                        xytext=(0, 0.5),\n                        textcoords=\"offset points\",\n                        ha='center', va='bottom')         \n               \ndef autoplot(X,hue,data,colors):\n    fig, ax = plt.subplots(1,2,figsize=(15, 10))\n    \n    plt.subplot(1,2,1)\n    ax[0] = sns.barplot(x=X.value_counts().index,\n                        y=(X.value_counts()\/len(X))*100,\n                        data=data,palette='Blues_d')    \n    ax[0].set_xlabel(X.name,fontsize=13)\n    ax[0].set_ylabel(\"Percentage\",fontsize=13)\n    autolabel(ax[0].patches,ax[0],'percentage')\n    \n    plt.subplot(1,2,2)\n    ax[1] = sns.countplot(x=X,hue=hue,data=df_Train,palette=colors,order = X.value_counts().index)   \n    ax[1].set_ylabel(\"Number of Occurrences\",fontsize=13)\n    ax[1].set_xlabel(X.name,fontsize=13)\n    autolabel(ax[1].patches,ax[1],'count')   \n    \n# Constants that we will use later\ncolors1 =['#C03028','#78C850']#Churn: No\/Yes","61cbd061":"# Churn\nChurn = pd.crosstab(df_Train['Churn'],df_Train['Churn']).sum()\nfig, ax = plt.subplots(figsize=(5, 5))\nax.pie(Churn, labels=Churn.index, autopct='%1.1f%%',colors=colors1)\nplt.legend(title='Churn',fontsize=10,title_fontsize=10)","1021c85c":"# Gender\nautoplot(df_Train['gender'],df_Train['Churn'],df_Train,colors1)\npd.crosstab(df_Train['gender'], df_Train['Churn']).apply(lambda r: r\/r.sum(),axis=1)","dd170af2":"# PhoneService-MultipleLines-InternetService\nIVs = ['PhoneService','MultipleLines','InternetService']\nfor i in range(len(IVs)):    \n    autoplot(df_Train[IVs[i]],df_Train['Churn'],df_Train,colors1)","b651559e":"for i in range(len(IVs)):    \n    print(pd.crosstab(df_Train[IVs[i]], df_Train['Churn']).apply(lambda r: r\/r.sum(), axis=1))\n    print('\\n')","e02bd5fe":"# OnlineSecurity-OnlineBackup-DeviceProtection-TechSupport-StreamingTV-StreamingMovies\nIVs = ['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']\nfor i in range(len(IVs)):\n    autoplot(df_Train[IVs[i]],df_Train['Churn'],df_Train,colors1)","7927957b":"for i in range(len(IVs)):\n    print(pd.crosstab(df_Train[IVs[i]], df_Train['Churn']).apply(lambda r: r\/r.sum(), axis=1))\n    print('\\n')","674b5d6a":"#SeniorCitizen-Partner-Dependents\nIVs = ['SeniorCitizen','Partner','Dependents']\nfor i in range(len(IVs)):\n    autoplot(df_Train[IVs[i]],df_Train['Churn'],df_Train,colors1)","5bbb8325":"for i in range(len(IVs)):\n    print(pd.crosstab(df_Train[IVs[i]], df_Train['Churn']).apply(lambda r: r\/r.sum(), axis=1))\n    print('\\n')","189b34e4":"# Contract-PaperlessBilling-PaymentMethod\nIVs = ['Contract','PaperlessBilling','PaymentMethod']\ndf_Train['PaymentMethod'] = df_Train['PaymentMethod'].replace({'Bank transfer (automatic)':'Bank transfer Auto',\n                                                               'Credit card (automatic)':'Credit card Auto'})\nfor i in range(len(IVs)):    \n    autoplot(df_Train[IVs[i]],df_Train['Churn'],df_Train,colors1)","eb56cbb0":"for i in range(len(IVs)):    \n    print(pd.crosstab(df_Train[IVs[i]], df_Train['Churn']).apply(lambda r: r\/r.sum(), axis=1))\n    print('\\n')","0af74d1b":"# Tenure\nfig, (ax1,ax2) = plt.subplots(2,1,figsize=(15, 10),sharex=True)\nsns.distplot(df_Train['tenure'], ax=ax1)\nsns.boxplot(df_Train['tenure'], ax=ax2)\nprint('Mean Tenure = %0.2f\\nMedian Tenure = %0.2f' % (df_Train['tenure'].mean(),df_Train['tenure'].median()))","d8645510":"# Tenure - Churn\nax = sns.FacetGrid(df_Train, hue='Churn',palette=colors1,aspect=2,height=5)\nax = ax.map(sns.kdeplot, \"tenure\",shade= True)\nax.fig.legend(title='Churn',fontsize=12,title_fontsize=12)    \n    \nfig, ax = plt.subplots()\nax = sns.boxplot(x='Churn', y='tenure', data=df_Train)\n\nT_0 = df_Train['tenure'][df_Train['Churn'] == 'No'].mean()\nT_1 = df_Train['tenure'][df_Train['Churn'] == 'Yes'].mean()\nprint('Mean Tenure No Churn: %0.1f \\nMean Tenure Churn: %0.1f' % (T_0,T_1))","d69c3c16":"# MonthlyCharges\nfig, (ax1,ax2) = plt.subplots(2,1,figsize=(15, 10),sharex=True)\nsns.distplot(df_Train['MonthlyCharges'], ax=ax1)\nsns.boxplot(df_Train['MonthlyCharges'], ax=ax2)\nprint('Mean MonthlyCharges = %0.2f\\nMedian MonthlyCharges = %0.2f' % (df_Train['MonthlyCharges'].mean(),df_Train['MonthlyCharges'].median()))","9c9ef2f9":"# MonthlyCharges - Churn\nax = sns.FacetGrid(df_Train, hue='Churn',palette=colors1,aspect=2,height=5)\nax = ax.map(sns.kdeplot, \"MonthlyCharges\",shade= True)\nax.fig.legend(title='Churn',fontsize=12,title_fontsize=12)    \n    \nfig, ax = plt.subplots()\nax = sns.boxplot(x='Churn', y='MonthlyCharges', data=df_Train)\n\nM_0 = df_Train['MonthlyCharges'][df_Train['Churn'] == 'No'].mean()\nM_1 = df_Train['MonthlyCharges'][df_Train['Churn'] == 'Yes'].mean()\nprint('Mean MonthlyCharges No Churn: %0.1f \\nMean MonthlyCharges Churn: %0.1f' % (M_0,M_1))","f37b4117":"# TotalCharges\nfig, (ax1,ax2) = plt.subplots(2,1,figsize=(15, 10),sharex=True)\nsns.distplot(df_Train['TotalCharges'], ax=ax1)\nsns.boxplot(df_Train['TotalCharges'], ax=ax2)\nprint('Mean TotalCharges = %0.2f\\nMedian TotalCharges = %0.2f' % (df_Train['TotalCharges'].mean(),df_Train['TotalCharges'].median()))","5d9e8daa":"# TotalCharges - Churn\nax = sns.FacetGrid(df_Train, hue='Churn',palette=colors1,aspect=2,height=5)\nax = ax.map(sns.kdeplot, \"TotalCharges\",shade= True)\nax.fig.legend(title='Churn',fontsize=12,title_fontsize=12)    \n    \nfig, ax = plt.subplots()\nax = sns.boxplot(x='Churn', y='TotalCharges', data=df_Train)\n\nTC_0 = df_Train['TotalCharges'][df_Train['Churn'] == 'No'].mean()\nTC_1 = df_Train['TotalCharges'][df_Train['Churn'] == 'Yes'].mean()\nprint('Mean TotalCharges No Churn: %0.1f \\nMean TotalCharges Churn: %0.1f' % (TC_0,TC_1))","10caaafd":"# Online Services \nIVs = ['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']\n\nOnlineServices = df_Train[IVs].replace({'No internet service':2,'No': 0, 'Yes': 1})\ndf_Train['OnlineServices'] = OnlineServices.sum(axis=1)\ndf_Train['OnlineServices'] = df_Train['OnlineServices'].replace({12:'No Int. Service'})\n\nautoplot(df_Train['OnlineServices'],df_Train['Churn'],df_Train,colors1)\npd.crosstab(df_Train['OnlineServices'], df_Train['Churn']).apply(lambda r: r\/r.sum(), axis=1)","3ce8e6c7":"# MonthChTenure = MonthlyCharges * Tenure \n# MonthChTenure - TotalCharges\ndf_Train['MonthChTenure'] = df_Train['MonthlyCharges']*df_Train['tenure']\n\nfig, (ax1,ax2) = plt.subplots(2,1,figsize=(15, 10),sharex=True)\nsns.distplot(df_Train['TotalCharges'], ax=ax1)\nsns.distplot(df_Train['MonthChTenure'], ax=ax2)","f42532e5":"fig, (ax1,ax2) = plt.subplots(2,1,figsize=(15, 10),sharex=True)\nsns.boxplot(df_Train['TotalCharges'], ax=ax1)\nsns.boxplot(df_Train['MonthChTenure'], ax=ax2)","c3cf8eac":"# TotalCharges: Delete missing values\ndf_Train[df_Train['TotalCharges'].isnull()].loc[:,('MonthlyCharges','tenure')]","2d352af4":"df_Train = df_Train.drop(df_Train['MonthlyCharges'][df_Train['TotalCharges'].isnull()].index)","88122e63":"# Dataset split to Categorical (Nominal,Binary) and Numeric Vars\ndf_Cat_Bin = df_Train[['gender','Partner','SeniorCitizen','Dependents','PhoneService','PaperlessBilling']].iloc[:]\ndf_Cat_Nom = df_Train[['MultipleLines','InternetService','OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingMovies','StreamingTV','Contract','PaymentMethod']].iloc[:]\ndf_Num = df_Train[['tenure','MonthlyCharges','TotalCharges']].iloc[:]\n\n# Categorical Output\ny = df_Train['Churn'].iloc[:]","b6188e81":"# LABEL ENCODING - ONE HOT ENCODING\n\n# Categorical Binary Features Encoding\nfrom sklearn.preprocessing import LabelEncoder\ndf_Cat_Bin_Ld = df_Cat_Bin.apply(LabelEncoder().fit_transform)\n\n# Categorical Nominal Features Encoding\ndf_Cat_Nom_OHEd = pd.get_dummies(df_Cat_Nom)\n\n# All Categorical Features\ndf_Cat = pd.concat([df_Cat_Bin_Ld,df_Cat_Nom_OHEd],axis=1)\n\n# Categorical Outpout Encoding\ny_Ld = y.replace({'No': 0, 'Yes': 1})\n\n# ALL the Selected IVs\nX = pd.concat([df_Num,df_Cat],axis=1)\ncolumns=X.columns\nX.head()","1bec618b":"# Correlation Matrix\nplt.figure(figsize=(20, 20))\ncorr = X.corr()\nsns.heatmap(corr, xticklabels=corr.columns,yticklabels=corr.columns,cmap = \"coolwarm\",annot=True,annot_kws = {'size': 6})\nplt.title(\"Correlation\")\nplt.show()","b39ee31f":"X = X.drop(columns=['OnlineSecurity_No internet service','OnlineBackup_No internet service',\n                    'DeviceProtection_No internet service','TechSupport_No internet service',\n                    'StreamingMovies_No internet service','StreamingTV_No internet service'])\n\nX = X.drop(columns=['PhoneService'])\n\nX = X.drop(columns=['TotalCharges'])","39ebb4da":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y_Ld, test_size = 0.2, random_state = 0)","885c6178":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","2266ccf4":"# Choosing Classifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier","29277fe1":"# Used GridSearchCV for parameter tuning\nCF = [None]*9\nNames = ['Logistic Regression','SVM linear','SVM rbf','Naive Bayes','kNN','Decision Tree','Random Forest','Gradient Boosting','Ada Boost']\nCF[0] = LogisticRegression(solver='newton-cg')\nCF[1] = SVC(kernel = 'linear', random_state = 0,probability=True)\nCF[2] = SVC(kernel = 'rbf', random_state = 0,probability=True)\nCF[3] = GaussianNB()\nCF[4] = KNeighborsClassifier(n_neighbors=20,metric='minkowski')\nCF[5] = DecisionTreeClassifier(max_depth=5,min_samples_leaf=2,random_state = 0)\nCF[6] = RandomForestClassifier(n_estimators=150,min_samples_split=4,max_depth=9,min_samples_leaf=2,random_state = 0)\nCF[7] = GradientBoostingClassifier(loss='exponential',min_samples_leaf=2,learning_rate=0.05,random_state = 0)\nCF[8] = AdaBoostClassifier(random_state = 0)","35b5cebe":"# Classification Metrics\nClassifiers = ['Logistic Regression','SVM linear','SVM rbf','Naive Bayes','k-NN','Decision Tree','Random Forest','Gradient Boosting','Ada Boost']\nCols = ['Accuracy','Recall','Precision','f1 score','AUC ROC score']\nScores = pd.DataFrame(index=Classifiers,columns=Cols).astype('float')\nfor i in range(len(CF)):\n    classifier = CF[i]\n    classifier.fit(X_train, y_train)\n    c_probs = classifier.predict_proba(X_test)\n    c_probs = c_probs[:, 1]\n    \n    y_pred = classifier.predict(X_test)\n    \n    from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score,roc_auc_score\n    Scores.Accuracy[i] = accuracy_score(y_test,y_pred)\n    Scores.Recall[i] = recall_score(y_test,y_pred)\n    Scores.Precision[i] = precision_score(y_test,y_pred)\n    Scores['f1 score'][i] = f1_score(y_test,y_pred)\n    Scores['AUC ROC score'][i] = roc_auc_score(y_test,c_probs)\n    \nprint(Scores)","6e1fd6fd":"# Feature Importance plots\ncolumns=X.columns\nfig = plt.figure(figsize=(15,15))\nfig.subplots_adjust(hspace=0.3, wspace=0.3)\nfor i in range(4):\n    plt.subplot(2, 2, i+1)\n    classifier = CF[i+5]\n    classifier.fit(X_train, y_train)     \n\n    FImportances = pd.DataFrame(data=classifier.feature_importances_,index=columns,columns=['Importance']).sort_values(by=['Importance'])\n    plt.barh(range(FImportances.shape[0]),FImportances['Importance'],color = '#78C850')\n    plt.yticks(range(FImportances.shape[0]), FImportances.index)\n    plt.title('Feature Importances: %s' % (Names[i+5]))","645f1eb1":"# ROC - Curves for models\nfig = plt.figure(figsize=(15,15))\nfig.subplots_adjust(hspace=0.3, wspace=0.3)    \nfor i in range(len(CF)):\n    plt.subplot(3, 3, i+1)\n\n    classifier = CF[i]\n    classifier.fit(X_train, y_train)  \n     \n    # Predict probabilities\n    r_probs = [0 for _ in range(len(y_test))]\n    c_probs = classifier.predict_proba(X_test)\n\n    # Keep probabilities for the positive outcome only\n    c_probs = c_probs[:, 1]\n\n    # Calculate AUROC\n    from sklearn.metrics import roc_curve, roc_auc_score, auc\n    r_auc = roc_auc_score(y_test, r_probs)\n    c_auc = roc_auc_score(y_test, c_probs)\n\n    # Calculate ROC curve\n    r_fpr, r_tpr, _ = roc_curve(y_test, r_probs)\n    c_fpr, c_tpr, _ = roc_curve(y_test, c_probs)\n    plt.plot(r_fpr, r_tpr, linestyle='--',c='r', label='Random Prediction (AUROC = %0.3f)' % r_auc)\n    plt.plot(c_fpr, c_tpr, marker='.',c='b', label='%s (AUROC = %0.3f)' % (Names[i],c_auc))\n\n    plt.title('ROC Plot')\n    plt.xlabel('False Positive Rate - 1 - Specificity')\n    plt.ylabel('True Positive Rate - Sensitivity')\n    plt.legend(fontsize='small')","a7851d37":"# Cap Curve\nfig = plt.figure(figsize=(15,15))\nfig.subplots_adjust(hspace=0.3, wspace=0.3)    \nfor i in range(len(CF)):\n    plt.subplot(3, 3, i+1)\n    \n    total = len(y_test)\n    class_1_count = np.sum(y_test)\n    class_0_count = total - class_1_count\n\n    plt.plot([0, total], [0, class_1_count], c = 'r', linestyle = '--', label = 'Random Model')\n\n    plt.plot([0, class_1_count, total], \n             [0, class_1_count, class_1_count], \n             c = 'grey', linewidth = 2, label = 'Perfect Model')\n\n    classifier = CF[i]\n    classifier.fit(X_train, y_train)  \n    c_probs = classifier.predict_proba(X_test)\n\n    # Keep probabilities for the positive outcome only\n    c_probs = c_probs[:, 1]\n\n    model_y = [y for _, y in sorted(zip(c_probs, y_test), reverse = True)]\n    y_values = np.append([0], np.cumsum(model_y))\n    x_values = np.arange(0, total + 1)\n\n    from sklearn.metrics import auc\n    # Area under Random Model\n    a = auc([0, total], [0, class_1_count])\n\n    # Area between Perfect and Random Model\n    aP = auc([0, class_1_count, total], [0, class_1_count, class_1_count]) - a\n\n    # Area between Trained and Random Model\n    aR = auc(x_values, y_values) - a\n\n    AR = aR \/ aP\n\n    plt.plot(x_values, y_values, c = 'g', label = '%s (AR = %0.3f)' % (Names[i],AR), linewidth = 4)\n\n    # Plot information\n    plt.xlabel('Total observations')\n    plt.ylabel('Class 1 observations')\n    plt.title('Cumulative Accuracy Profile')\n    plt.legend(fontsize='small')","fb07f3da":"# Average values\nScores_avg = np.average(Scores,axis=0)\nprint('The avg accuracy is = %.2f' % Scores_avg[0])\nprint('The avg recall is = %.2f' % Scores_avg[1])\nprint('The avg precision is = %.2f' % Scores_avg[2])\nprint('The avg f1-score is = %.2f' % Scores_avg[3])\nprint('The avg AUC ROC score is = %.2f' % Scores_avg[4]) ","f9d432af":"There are 20 columns in the dataset with the below dtypes: \nfloat64(1): MonthlyCharges                     \nint64(2): SeniorCitizen, tenure                  \nobject(18): gender, Partner, Dependents, PhoneService, MultipleLines, InternetService, OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, StreamingTV, StreamingMovies, Contract, PaperlessBilling, PaymentMethod, TotalCharges, Churn\n\nWe do not include customerID feature.","cc3d5797":"### Numerical Features","b11cb8ef":"We notice that there are customers that they have tenure = 0. Probably that means that these customers made a contrtact with the company during the last month so their tenure is < 1 month.","bcc50888":"50.5% are men and 49.5% are women. Churning rates for men and women are similar 26.1% and 26.9% resectively. We can say that gender might not have big importance in our model.","dffde8e0":"We convert TotalCharges from object to float","ba33b33b":"#### Receiver Operating Characteristic (ROC) Curve\nThe Receiver Operating Characteristic Curve, better known as the ROC Curve, is an excellent method for measuring the performance of a Classification model. It tells how much model is capable of distinguishing between classes. The True Positive Rate (TPR) is plot against False Positive Rate (FPR) for the probabilities of the classifier predictions. Then, the area under the plot is calculated.","654e6ba8":"From the two highest peaks in the graphs we can say that customers with higher MonthlyCharges tend to leave the company.","a10f717c":"There are two peaks in the Tenure feature, one for customers with low tenure and one for customers with high tenure.","5fcb94dc":"- There is no need to keep the features with No Internet Service since they correspond to the same feature. So, we can only keep the InternetService_No and drop the others.\n\n- We also drop PhoneService feature since its informtion is included to the MultipleLines feature.\n\n- We drop TotalCharges feature since its info corresponds to the MonthlyCharges and Tenure features. We coud also drop the last two instead but deleting TotalCharges gave us better results.","641c5856":"We used the functions above to auto plot some features with annotations (percentages, counts).","07614f19":"There are 6 Online Services and those customers with no Internet Service. Looking at the Churning percentages we can say that as the number of Online Services, increases the number of Churned customers decreases. From 52.2% Churned customers with Internet Service and 0 Online Services to 0.05% Churned customers with Internet Service and 6 Online Services.","1557e4b4":"- 55% has Month to Month contract as well as the highest percentage of Churning 42.7%.\n\n- 59.2% has PaperlessBilling and 33.5% of these customers Churned compared to 16.3% for those who do not have PaperlessBilling.\n\n- 45.2% of the customers using Electronic Check as a Payment Methon Churned. \n\n- Having a Month to Month contract, PaperlessBilling and Electronic Check as a Payment Method increase the chance for a customer to leave the company.","3caab8a1":"## 7. Final thoughts","057d0467":"## 4. Feature Engineering ( Visualization )","b0c860d7":"#### Cumulative Accuracy Profile (CAP) Curve\nThe CAP Curve tries to analyse how to effectively identify all data points of a given class using minimum number of tries.","fad83238":"26.5% left the company within the last month and 73.5% stayed. This is a case with imbalanced data.","e5afa3d7":"### Categorial Features","53c7e808":"- tenure: Min = 0, Max = 72, Avg = 32.4\n- MonthlyCharges: Min = 18.25, Max = 118.75, Avg = 64.76\n- TotalCharges: Min = 18.8, Max = 8684.8, Avg = 2283.3","866b766e":"The graphs show that Churning customers have a low Tenure with mean Tenure = 18 months. As the tenure increases customers tend to stay in the company.","95eb0eaa":"From the Feature Importances plots we can say that the type of contract, MonthlyCharges, tenure and InternetService play a key role in the customer's decision to leave or not the company.","2abf7038":"## 2. Data Profiling","14d273be":"TotalCharges feature has 11 missing values as we found earlier in this notebook. These missing values correspond to the customers with a tenure of 0 months. That probably means that these customers subscribed with the company during the last month and there is no overall information of their TotalCharges since they are less than 1 month in the company. We believe that these customers do not provide solid information that can be used in our model so we delete theses rows from our dataset. ","9a5472a4":"We first used GridSearchCV to tune some of hyperparameters.","d95eacb6":"- As mentioned before, 21.7% have not Internet Service so no Online Services as well and most of them stayed in the company, only 0.07% left.\n\n- Features: 'OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport'\n- 49.7% has not 'OnlineSecurity' and 41.7% left the company.\n- 43.8% has not 'OnlineBackup' and 39.9% left the company.\n- 43.9% has not 'DeviceProtection' and 39.1% left the company.\n- 49.3% has not 'TechSupport' and 41.6% left the company.\n\n- Features: 'StreamingTV','StreamingMovies'\n- The percentages of having or not these Online Services are similar.\n- 1\/3 of the customers left the company despite having or not 'StreamingTV' and 'StreamingMovies'\n\n- We can say in general that, customers having an Internet Service but not Online Services tend to leave.","85d9a414":"There are 11 missing values in TotalCharges column that we will handle later","fd6b64d1":"The highest the AUC, the better the model is at distinguishing between customer Churn or not. Random Forest Classifier and Gradient Boosting have the highest AUC both with 0.843.","db7f15f9":"We created the feature MonthChTenure by multiplying the Tenure feature with the MonthlyCharges feature. Then we compared it with the TotalCharges feature and we found that they are identical. So, we conclude that the TotalCharges feature includes the information of the Tenure and MonthlyCharges features.","1b0ce122":"- Only 16.2% are Senior Citizens but 41.6% left the company in comparison with the non Senior Sitizens where 23.6% Churned\n- In comparison, 23.6% of the non Senior Citizens customers left the company.\n\n- Customers with and without Partner have similar percentages 48.3% and 51.7% respectively but 32.9% without a Partner Churned and 19.6% with a Partner Churned.\n\n- 70% has no Dependents - 31% of these customers Churned while 15.4% of the customers with Dependents Chruned.\n\n- Being a Senior Citizen, not having a Partner or Dependents increase the chance for a customer to leave the company.","134d576b":"## 5. Feature Engineering ( Encoding )","7cb5922e":"Many customers with lower TotalCharges left the company. This seems odd but maybe these low TotalCharges are really high for customers with a low tenure and that lead to their decision to leave the company.","7e8b824b":"In this notebook we examine the Telco Customer Churn dataset and then we build a model that can predict if a customer left the bank within the last month. We start with finding feature types, missing values and we continue with feature analysis and visualization of the data. Feature engineering is implemented to create new attributes, encoding and feature selection. At last we test several classifiers and we evaluate them with the help of the ROC and CAP curves.\n\n#### Data Dictionary\n- Customers who left within the last month: Churn\n- Services that each customer has signed up for: PhoneService, MultipleLines, InternetService, OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, StreamingTV, StreamingMovies;  \n- Customer account information: Tenure, Contract, PaymentMethod, PaperlessBilling, MonthlyCharges, TotalCharges\n- Demographic info about customers: Gender, SeniorCitizen, Partner, Dependents   \n\n#### Structure\n1. Introduction\n2. Data Profiling\n3. Feature Analysis (Visualization)\n4. Feature Engineering (Visualization)\n5. Feature Engineering (Encoding)\n6. Evaluation - Selection\n\n#### Goal\nThe goal is to prefict the behavior to retain customers.\n\n#### P.S. \nFeel free to comment if you have any question, something to note or suggest about this notebook. It will only make us better!  ","0dc65131":"- The Churning percentages for customers having a PhoneService (90.3%) or not (9.7%) are similar, 24.9% and 26.7% respectively. PhoneService feature seems not to have a big importance for our model.\n\n- 42.2% of the customers do not have MultipleLines and 48.1% have. Churning percentages are similar 25% and 28.6%. Also the information of having a PhoneService or not is included in MultipleLines feature.\n\n- 21.7% have not Internet Service and most of them stayed in the company, only 0.07% of the customers left. For those that had Fiber Optic as an Internet Service 41.8% Churned.","cfb82eab":"## 1. Introduction","8d74a1ec":"- Features\n- Categorical: Binary: 'gender','Partner','Dependents','PhoneService','PaperlessBilling','Churn'\n             Nominal: 'MultipleLines','InternetService','OnlineSecurity','OnlineBackup','DeviceProtection',\n                      'TechSupport','StreamingMovies','StreamingTV','Contract','PaymentMethod'           \n- Numerical: Discrete: 'SeniorCitizen'\n           Continuous: 'tenure','MonthlyCharges','TotalCharges'","3c9db857":"## 3. Feature analysis (Visualization)","4e9da2b9":"TotalCharges should be dtype: float64 as MonthlyCharges but instead its dtype: object","0dd965ce":"## 6. Evaluation","1dc31b71":"Decision Tree has the highest AUC with 0.688 and then Random Forest and Gradient Boosting follow with 0.686 and 0.687 respectively.","29a5f355":"In this case we would like a model which has a high recall because recall tells us which % of people who actually churned was correctly identified. We need a model with less False Negatives (FN).\nThe model with significant higher recall than the avg is Naive Bayes model with recall = 0.79, f1-score = 0.60, accuracy  = 0.73 and the lowest precision = 0.49. We choose this model even if the accuracy is lower than avg because identifying the customers that churned is our goal of this project."}}