{"cell_type":{"3a6d0232":"code","707d7564":"code","b2e6586b":"code","6bcc9e63":"code","9b10cbc6":"code","a5cf67bd":"code","a3c60f4f":"code","4189bda9":"code","d2e5b216":"code","c1a5e47d":"code","da1acbb3":"code","bc0a453c":"code","30c657a1":"code","38414686":"code","12a12b99":"code","5ed78a41":"code","a17b316c":"code","9851739d":"code","201db303":"code","697bf34e":"code","52c41b1a":"code","e6d65a48":"code","9900de0f":"code","3b82958d":"code","6acac0ae":"code","60c0b1b5":"code","6da68ef5":"code","d406d523":"code","3295c7d5":"code","96f3521d":"code","91818513":"code","68a0e3e9":"code","3e9d2cd5":"code","ae52bc3f":"code","58db3b9e":"code","776256a1":"markdown","a3058294":"markdown","b4c3c13d":"markdown","e258e603":"markdown","aea538b9":"markdown","186823e9":"markdown","09cd2669":"markdown","dd4b5de2":"markdown","6828b24e":"markdown","88ddc882":"markdown","ba435a91":"markdown","0a756c4c":"markdown","cc594db1":"markdown","a864988b":"markdown","95570031":"markdown","cb22a312":"markdown","ceb7e024":"markdown","8efa4d5e":"markdown","6b728153":"markdown","fb6b0f07":"markdown","56ec9c67":"markdown","97e9cfbc":"markdown","1c867929":"markdown","1390fba5":"markdown","3914ea23":"markdown"},"source":{"3a6d0232":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport math \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB # using Gaussian Algorithm from Naive Bayes\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.impute import SimpleImputer\nfrom sklearn import metrics\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","707d7564":"df = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndf","b2e6586b":"# check no. of columns and rows\ndf.shape","6bcc9e63":"# check first 5 rows of dataset\ndf.head()","9b10cbc6":"# checking for null values in dataset\ndf.isnull().values.any()","a5cf67bd":"# Let's plot histogram of first 8 columns\ncolumns = list(df)[0:-1]\ndf[columns].hist(bins=80,figsize=(12,50), layout=(14,4))","a3c60f4f":"df.rename(columns = {'DiabetesPedigreeFunction':'DPF',}, inplace = True)","4189bda9":"df.rename(columns = {'BloodPressure':'BP','SkinThickness': 'SkinTh'}, inplace = True)","d2e5b216":"# showing correlation matrix\ndf.corr()","c1a5e47d":"# plotting a correlation heatmap\n\nplt.figure(figsize=(20,10))\nsns.heatmap(df.corr(), vmax=1, square=True, annot=True, cmap='Blues')\nplt.title('Correlation between different attributes')\nplt.show()\n","da1acbb3":"# using the pair plot to get more insights\n\nsns.pairplot(df,diag_kind='kde')\n","bc0a453c":"#target column value and their counts\ndf['Outcome'].value_counts()","30c657a1":"df.head()","38414686":"# Seperating the independent and dependent variable from the dataset\nX = df.drop('Outcome', axis=1)\ny = df['Outcome']","12a12b99":"# Splitting into the training and testing dataset\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.30,random_state=1)","5ed78a41":"print(\"{}% data is in the training set\".format((len(X_train)\/len(df))*100))\nprint(\"{}% data is in the testing set\".format((len(X_test)\/len(df))*100))","a17b316c":"X_train.head()","9851739d":"# let's check the statistical summary for worng values inserted in data set\nX_train.describe().T","201db303":"# Creating the simple imputer to replace zeros with mean value\nreplace_ = SimpleImputer(missing_values=0, strategy='mean')\n\ncols=X_train.columns\n\nX_train = pd.DataFrame(replace_.fit_transform(X_train))\nX_test = pd.DataFrame(replace_.fit_transform(X_test))\n\nX_train.columns = cols\nX_test.columns = cols\n","697bf34e":"# looking at the clean dataset used for model\nX_train.head()","52c41b1a":"# Creating and model and fitting data\n\nmodel = LogisticRegression(solver='liblinear')\n# solver {\u2018newton-cg\u2019, \u2018lbfgs\u2019, \u2018liblinear\u2019, \u2018sag\u2019, \u2018saga\u2019}, default=\u2019lbfgs\u2019\n# Algorithm to use in the optimization problem. Default is \u2018lbfgs\u2019. To choose a solver, you might want to consider the following aspects:\n# For small datasets, \u2018liblinear\u2019 is a good choice, whereas \u2018sag\u2019 and \u2018saga\u2019 are faster for large ones;\n# For multiclass problems, only \u2018newton-cg\u2019, \u2018sag\u2019, \u2018saga\u2019 and \u2018lbfgs\u2019 handle multinomial loss;\n# \u2018liblinear\u2019 is limited to one-versus-rest schemes.\nmodel.fit(X_train,y_train)\nprint(model.coef_, model.intercept_)","e6d65a48":"X_train.head()","9900de0f":"X_test.head()","3b82958d":"y_pred = model.predict(X_test)\ny_pred","6acac0ae":"model_score = model.score(X_test,y_test)\nprint(\"The model score is {}\". format(model_score))","60c0b1b5":"conf_matrix = metrics.confusion_matrix(y_test, y_pred, labels=[1,0])\n\ndf_cm = pd.DataFrame(conf_matrix, index= [i for i in ['1','0']], \n                     columns = [i for i in ['Predict 1','Predict 0']])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm,annot=True, cmap='Blues')\nplt.title(\"Confusion Matrix\")","6da68ef5":"model = GaussianNB() # initalize the algorithm\nmodel.fit(X_train,y_train) # buld the model.","d406d523":"X_train.head()","3295c7d5":"X_test.head()","96f3521d":"train_y_pred = model.predict(X_train)\ntrain_y_pred","91818513":"from sklearn import metrics\nmodel_score = metrics.accuracy_score(y_train,train_y_pred)\nprint(\"The model Accuracy is {0:.4f}\". format(model_score))\n","68a0e3e9":"test_y_pred = model.predict(X_test)\ntest_y_pred","3e9d2cd5":"from sklearn import metrics\nmodel_score = metrics.accuracy_score(y_test,test_y_pred)\nprint(\"The model Accuracy is {0:.4f}\". format(model_score))","ae52bc3f":"conf_matrix = metrics.confusion_matrix(y_test, test_y_pred, labels=[1,0])\n\ndf_cm = pd.DataFrame(conf_matrix, index= [i for i in ['1','0']], \n                     columns = [i for i in ['Predict 1','Predict 0']])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm,annot=True, cmap='Blues')\nplt.title(\"Confusion Matrix\")","58db3b9e":"from sklearn.metrics import precision_score,recall_score,f1_score\n\nprint('Classification Report')\nprint(metrics.classification_report(y_test,test_y_pred,labels=[1,0]))\n\n#We can see our true positive numbers with value 1 is of precision and recall is below 70%\n#instead of importing all separately use classification_report function.","776256a1":"### **Loading the dataset**","a3058294":"### **Making prediction of Test Data**","b4c3c13d":"## **Data Preparation**\n\n### Check hidden missing values\n\nAs we have checked missing values earlier but haven't got any. But there can be lots of entries with 0 values. We must need to take care of those as well.","e258e603":"# **Diabetes prediction using Logistic Regression**\n\n#### The objective of this work is to diagnostically predict wether or not a patient has diabetes based on certian diagnostic measurements included in the dataset.","aea538b9":"# **2.Creating the Naive Bayes Model**","186823e9":"### **Creating the confustion Matrix to understand the results**","09cd2669":"#### **About Dataset Features**\n\nFollowing are the features provided in the dataset.\n\n* Pregnancies = No. of pregnancy\n* Glucose = Glucose level\n* Blood Pressure = Plood Pressure of person\n* SkinThickness = Thickness of the skin\n* Insulin = Insulin Test results\n* BMI = Body Mass Index\n* Diabetes Pedigree Function\n* Age = Age of the person\n* Outcome = Target class (wether diabetes or not, **1: diabetec, 0: not diabeteic**)","dd4b5de2":"### **Making prediction of Training Data**","6828b24e":"From the above distribution we can see that \n* the pregnancies data is postively skewed and majorly ranging between 0 to 13\n* the glucose is almost uniformly distributed around level of 105-110 as peak and range between 70-140\n* the blood pressure is uniformly distributed between 45-100\n* the skin thickness has high zero values showing abnormality in data.\n* the insulin is also distributed between 0-250 levels\n* the BMI is uniformly distributed between 18-42 having peak around 32\n* the diabetes pedigree funcion is positively skewed and majority  lies between 0-1.5\n* the age data also shows positive skewnwss having value ranging from 20-60\n\n","88ddc882":"Let's check the split of data","ba435a91":"From the above confusion matrix it is clearly visible that the model has correctly predicted **48(TP)+132(TN)=180 samples**. On other side it is unable to predict **37(FP)+14(FN)=51 samples** correctly.","0a756c4c":"### Let's replace zero's with mean\n\nHere we will replace zero values in the SkinTh, Insulin and other columns by mean of them for removing the noisy data.","cc594db1":"# **Classification Report**","a864988b":"### Now checking the model score for Testing Data","95570031":"### **Splitting the data**\n\nNow Split X and y into **training and testing set in 70:30 ratio**","cb22a312":"Creating the simple imputer to perform the data cleaning task","ceb7e024":"### **Creating the confustion Matrix to understand the results**","8efa4d5e":"# **1.Creating the Logistic Regression Model**","6b728153":"From the calculation below, we have 34.90% people in current dataset who have diabetes and rest of 65.10% doesn't have diabetes. Considerably its good distribution of results in the data.","fb6b0f07":"### **Making prediction of Testing Data**","56ec9c67":"### Now checking the model score","97e9cfbc":"#### **Identifying the correlation in the data**","1c867929":"### Now checking the model score for Training Data","1390fba5":"### **Calculate diabetes ratio of True\/False from the outcome variable**","3914ea23":"## So we can improve the score by tuning the hyperparameters and enhance the accuracy of the model."}}