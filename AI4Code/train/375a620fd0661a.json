{"cell_type":{"66bf11c3":"code","aab12dd2":"code","1db0922f":"code","adb21f19":"code","b42b1f3e":"code","93d31715":"code","db5d011b":"code","43702429":"code","bfca40dc":"code","b41cfb7b":"code","f0e1e30d":"code","239b82e9":"code","4b342ce2":"code","dfd92322":"code","a0f739d3":"code","b4e753ad":"code","ba003bde":"code","50ed848c":"code","f995248f":"code","120c368d":"code","404339ef":"code","54225997":"code","fc83c926":"code","0f63e95a":"code","85975824":"code","090fe596":"code","0018b9fa":"code","5fb5af64":"code","e98ffa2d":"code","a4f0530a":"code","aed7559e":"code","7dbe7e32":"markdown","18114b3f":"markdown","d5fe372f":"markdown","ef08a595":"markdown","09942c60":"markdown","c5ec2e61":"markdown","84de1d08":"markdown","b3c87585":"markdown","071b5933":"markdown","9a7a418e":"markdown","401278b7":"markdown","b207a50c":"markdown","0e0c4bef":"markdown","1e9362cd":"markdown","841cdddd":"markdown","a547e317":"markdown","04760764":"markdown","602de8a4":"markdown","2b862ef2":"markdown","a0f43d6f":"markdown","ef48bdb4":"markdown","2eb168bf":"markdown","b0903f25":"markdown","8257cc9e":"markdown","5de1597b":"markdown","7fa2987e":"markdown","b6306a6b":"markdown","e689caed":"markdown"},"source":{"66bf11c3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","aab12dd2":"train = pd.read_csv('\/kaggle\/input\/forest-cover-type-prediction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/forest-cover-type-prediction\/test.csv')\ntest_id = test.Id\n\n\ndef write_sub(pred,postfix):\n    '''\n    \n    Writes submission file\n    \n    pred    - predicted values\n    postfix - description of a file\n    \n    '''\n    \n    sub = pd.DataFrame({'Id':test_id,'Cover_Type':pred})\n    file_name = '\/kaggle\/working\/'+'sub_'+postfix+'.csv'\n    sub.to_csv(file_name,index = False)\n    print(file_name,' is ready, please submit')","1db0922f":"train.head()","adb21f19":"print('(TRAIN) No of cols: {}\\n(TRAIN) No of rows: {}'.format(len(train.columns),len(train.index)))\nprint('(TEST)  No of cols: {}\\n(TEST)  No of rows: {}'.format(len(test.columns),len(test.index)))","b42b1f3e":"# Alternativly just use train.info()\n\nfor col in train.columns:\n    print('(TRAIN) Column {} is {} type'.format(train[col].name,train[col].dtype))\n    \nfor col in test.columns:\n    print('(TEST) Column {} is {} type'.format(test[col].name,test[col].dtype))","93d31715":"print('No of cols with NaN\\nTraining set: {}\\nTest set: {}\\n'.format(len(train.isna().sum()[train.isna().sum() != 0]),\n                                                                     len(test.isna().sum()[test.isna().sum() != 0])))","db5d011b":"# let`s use pair plot to have an overal idea about data.\n\nfrom pandas.plotting import scatter_matrix\n\nscatter_matrix(train.loc[:,'Elevation':'Horizontal_Distance_To_Fire_Points'],\n               c=train.Cover_Type,\n               alpha = 0.2,\n               hist_kwds = {'bins':100},\n               figsize = (20,20));","43702429":"cols = ['Elevation', 'Aspect', 'Slope',\n        'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n        'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon',\n        'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points','Cover_Type']\n\ncm = train[cols].corr()\nfig, ax = plt.subplots(figsize=(10,5));\nax.matshow(cm);\nplt.xticks(range(cm.shape[1]), cm.columns, fontsize=10, rotation=90);\nplt.yticks(range(cm.shape[1]), cm.columns, fontsize=10, rotation=0);","bfca40dc":"def model_input(data):\n    \n    \n    '''\n       \n       Transforms dataframe to a form\n       we would like to have it in our model\n       \n       also makes life easier since we can transform train and test set to the same format\n       \n    '''\n    \n    data = data.drop(['Id'],axis = 1)\n    # the most reliable way yo combine distances\n\n    # Better feature engineering can be found here\n    # http:\/\/nbviewer.ipython.org\/github\/aguschin\/kaggle\/blob\/master\/forestCoverType_featuresEngineering.ipynb\n    \n    data['Distance_to_Hydrology'] = np.sqrt(data['Horizontal_Distance_To_Hydrology']**2 + \\\n                                             data['Vertical_Distance_To_Hydrology']**2)\n\n    data['Elevation-VDH'] = abs(data['Elevation'] - data['Vertical_Distance_To_Hydrology'])\n    data['Elevation-HDF'] = abs(data['Elevation'] - data['Horizontal_Distance_To_Fire_Points'])\n    data['Elevation-HDH'] = abs(data['Elevation'] - data['Horizontal_Distance_To_Hydrology'])\n\n    data['Elevation+VDH'] = abs(data['Elevation'] + data['Vertical_Distance_To_Hydrology'])\n    data['Elevation+HDF'] = abs(data['Elevation'] + data['Horizontal_Distance_To_Fire_Points'])\n    data['Elevation+HDH'] = abs(data['Elevation'] + data['Horizontal_Distance_To_Hydrology'])\n\n    data['HDF+VDH'] = abs(data['Horizontal_Distance_To_Fire_Points'] - data['Vertical_Distance_To_Hydrology'])\n    data['HDF+HDH'] = abs(data['Horizontal_Distance_To_Fire_Points'] - data['Horizontal_Distance_To_Hydrology'])\n\n    data['HDF+VDH'] = abs(data['Horizontal_Distance_To_Fire_Points'] + data['Vertical_Distance_To_Hydrology'])\n    data['HDF+HDH'] = abs(data['Horizontal_Distance_To_Fire_Points'] + data['Horizontal_Distance_To_Hydrology'])\n\n    # From pair plot you can see that Vertical_Distance_To_Hydrology has negative values, therefore we can derive new feature\n    # if Vertical_Distance_To_Hydrology is positive we encode it as 1 if it is negative as 0\n\n    data['Higherwater'] = data['Vertical_Distance_To_Hydrology'].apply(lambda x: 1 if x>0 else 0)\n\n\n    data['Hillshade_Noon_3pm'] = (data['Hillshade_Noon'] + data['Hillshade_3pm'])\/2\n\n\n    # Drop values we used for feature engineering\n    data = data.drop(['Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology','Elevation'],axis=1)\n    data = data.drop(['Hillshade_Noon','Hillshade_3pm'],axis=1)\n    return data","b41cfb7b":"train = model_input(train)\ntest = model_input(test)","f0e1e30d":"from sklearn.model_selection import GridSearchCV, KFold,train_test_split, cross_val_score\n\nX = train.drop(['Cover_Type'],axis = 1)\ny = train['Cover_Type']\n\n\n#-----------------------------------------------------------------------------------------\n# from sklearn.preprocessing import LabelEncoder,LabelBinarizer\n\n# remove binarizer and train wit xgb lgb\n# LabelBinarizer is cool staff\n# However it is almost incombatable with powerfull classfiers as XGB og LGB\n# In the end I decided to remove it.\n# But keep in mind it is usefull\n\n#lb = LabelBinarizer()\n#y = lb.fit_transform(y)\n\n#-----------------------------------------------------------------------------------------","239b82e9":"from sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler, RobustScaler\nfrom sklearn.preprocessing import OneHotEncoder\n\n#---------------------------------------------------------------------------------\n# I have observed that scaling of numerica' values doesn`t work so good here\n# therefore i would suggest to avoid minmax,standard or robust scalling\n# Just treat values as it is\n# The similar obsiravtion can be found here :\n#---------------------------------------------------------------------------------\n# https:\/\/www.kaggle.com\/sharmasanthosh\/exploratory-study-of-ml-algorithms\n#---------------------------------------------------------------------------------\n# last figure\n\n#mms_cols = ['Elevation', 'Aspect', 'Slope','Distance_to_Hydrology',\n            #'Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Fire_Points',\n            \n            #'Elevation-VDH','Elevation-HDF','Elevation-HDH',]\n\n#sca_cols = ['Hillshade_9am','Hillshade_Noon_3pm']\n\n#---------------------------------------------------------------------------------=\n\n# Nevertheless we still need to encode Higherwater feature\nohe_cols = ['Higherwater']\n\n# I like to use this transformer\n\ntrans = make_column_transformer(\n    #(RobustScaler(),mms_cols),\n    #(StandardScaler(),sca_cols),\n    (OneHotEncoder(),ohe_cols),\n    remainder = 'passthrough'\n)\n\n\n# Let`s create 5 splits. we will test them with cross_val_score\nfrom sklearn.model_selection import cross_val_score\nkfolds = KFold(n_splits = 5, random_state=42, shuffle = True)      ","4b342ce2":"# The simpliest one\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\npipe_knn = make_pipeline(trans,\n                         KNeighborsClassifier(3))\n\nprint ('Mean Accuracy: ',cross_val_score(pipe_knn,X,y,cv = kfolds).mean(),\n       'STD:           ',cross_val_score(pipe_knn,X,y,cv = kfolds).std())\n\n# Let`s also fit the model and check the score with Late Submission\n\npred = KNeighborsClassifier(3).fit(X,y).predict(test)\nwrite_sub(pred,'def_knn')","dfd92322":"# Because I like this classifier\n\nfrom sklearn.ensemble import RandomForestClassifier\n\npipe_rfc = make_pipeline(trans,\n                     RandomForestClassifier(n_estimators = 100, max_depth = 25))\n\nprint ('Mean Accuracy: ',cross_val_score(pipe_rfc,X,y,cv = kfolds).mean(),\n       'STD:           ',cross_val_score(pipe_rfc,X,y,cv = kfolds).std())\n\n\n# Let`s also fit the model and check the score with Late Submission\npred = RandomForestClassifier(n_estimators = 100, max_depth = 25).fit(X,y).predict(test)\nwrite_sub(pred,'def_rfr')","a0f739d3":"# To compare with random forest\n\nfrom sklearn.ensemble import ExtraTreesClassifier\n\npipe_etc = make_pipeline(trans,\n                         ExtraTreesClassifier(n_estimators = 100, max_depth = 25))\n\nprint ('Mean Accuracy: ',cross_val_score(pipe_etc,X,y,cv = kfolds).mean(),\n       'STD:           ',cross_val_score(pipe_etc,X,y,cv = kfolds).std())\n\n# Let`s also fit the model and check the score with Late Submission\npred = ExtraTreesClassifier(n_estimators = 100, max_depth = 25).fit(X,y).predict(test)\nwrite_sub(pred,'def_etc')","b4e753ad":"from xgboost import XGBClassifier\n\npipe_xgb = make_pipeline(trans,\n                         XGBClassifier(n_estimators = 100, max_depth = 25))\n\nprint ('Mean Accuracy: ',cross_val_score(pipe_xgb,X,y,cv = kfolds).mean(),\n       'STD:           ',cross_val_score(pipe_xgb,X,y,cv = kfolds).std())\npred = XGBClassifier(n_estimators = 100, max_depth = 25).fit(X,y).predict(test)\nwrite_sub(pred,'def_xgb')","ba003bde":"from sklearn.ensemble import VotingClassifier\n\npipe_vc = VotingClassifier( estimators = [('rfc',RandomForestClassifier(n_estimators = 100, max_depth = 25)),\n                                          ('etc',ExtraTreesClassifier(n_estimators = 100, max_depth = 25)),\n                                          ('xgb',XGBClassifier(n_estimators = 100, max_depth = 25))],\n                            voting = 'hard',\n                            n_jobs=-1\n                           )\n\n# voting = 'soft'    is recomended with well tuned classifiers\n\nprint ('Mean Accuracy: ',cross_val_score(pipe_vc,X,y,cv = kfolds).mean(),\n       'STD:           ',cross_val_score(pipe_vc,X,y,cv = kfolds).std())\n\n# Let`s also fit the model and check the score with Late Submission\npred = pipe_vc.fit(X,y).predict(test)\nwrite_sub(pred,'def_vc')","50ed848c":"lb_score = [0.66435,0.75692,0.77821,0.77824,0.77766]\ncross_acc_mean = [0.8113095238095239,0.8724206349206349,0.8791666666666667,0.8818121693121693,0.8836640211640212]\ncross_acc_std  = [0.007234736333478586,0.0090432088016599,0.007320087176315713,0.00722868772751064,0.005549253164921434]\nclf = ['KNN','RFC','EXT','XGB','Voting']\nx = np.arange(len(clf))\n\nfig, ax = plt.subplots()\nax.bar(x - 0.2, lb_score, 0.4,label='LB score')\nax.bar(x + 0.2, cross_acc_mean, 0.4,yerr = cross_acc_std, label='Cross val score')\n\nax.set_ylabel('Scores')\nax.set_title('LB and CV Scores')\nax.set_xticks(x)\nax.set_xticklabels(clf)\nax.legend(loc = 'lower right')","f995248f":"param_rfc = {'randomforestclassifier__n_estimators':[100,300,500,700],\n             'randomforestclassifier__max_depth':[20,50,70]\n            }\n\nsearch_rfc = GridSearchCV(pipe_rfc,param_rfc,cv = kfolds, scoring = 'accuracy')\nsearch_rfc.fit(X,y)\n\nprint('Best score: ',search_rfc.best_score_)\nprint('Best param: ',search_rfc.best_params_)\n\nmodel_rfc = search_rfc.best_estimator_","120c368d":"# Let`s also fit the model and check the score with Late Submission\npred = model_rfc.fit(X,y).predict(test)\nwrite_sub(pred,'adj_rfc')","404339ef":"param_etc = {'extratreesclassifier__n_estimators':[100,300,500,700],\n             'extratreesclassifier__max_depth':[50,70,100]\n            }\n\n#Best param:  {'extratreesclassifier__max_depth': 70, 'extratreesclassifier__n_estimators': 300}\n\nsearch_etc = GridSearchCV(pipe_etc,param_etc,cv = kfolds, scoring = 'accuracy')\nsearch_etc.fit(X,y)\n\nprint('Best score: ',search_etc.best_score_)\nprint('Best param: ',search_etc.best_params_)\n\nmodel_etc = search_etc.best_estimator_\n","54225997":"# Let`s also fit the model and check the score with Late Submission\npred = model_etc.fit(X,y).predict(test)\nwrite_sub(pred,'adj_etc')","fc83c926":"param_xgb = {'xgbclassifier__n_estimators':[100,300,500,700],\n             'xgbclassifier__max_depth':[50,70,100]\n            }\n# Best param:  {'xgbclassifier__max_depth': 50, 'xgbclassifier__n_estimators': 100}\n\nsearch_xgb = GridSearchCV(pipe_xgb,param_xgb,cv = kfolds, scoring = 'accuracy')\nsearch_xgb.fit(X,y)\n\nprint('Best score: ',search_xgb.best_score_)\nprint('Best param: ',search_xgb.best_params_)\n\nmodel_xgb = search_xgb.best_estimator_\n","0f63e95a":"pred = model_xgb.fit(X,y).predict(test)\nwrite_sub(pred,'adj_xgb')","85975824":"from sklearn.ensemble import VotingClassifier\n\npipe_vc = VotingClassifier(estimators = [ ('rfc',RandomForestClassifier(n_estimators = 100, max_depth = 25)),\n                                          ('etc',model_etc),\n                                          ('xgb',model_xgb)],\n                            voting = 'soft',\n                            n_jobs=-1\n                           )\n\nprint ('Mean Accuracy: ',cross_val_score(pipe_vc,X,y,cv = kfolds).mean(),\n       'STD:           ',cross_val_score(pipe_vc,X,y,cv = kfolds).std())","090fe596":"pred = pipe_vc.fit(X,y).predict(test)\nwrite_sub(pred,'adj_vc')","0018b9fa":"pipe_vc.fit(X_train,lb.inverse_transform(y_train))\nprint('Accuracy on TRAIN: ',pipe_vc.score(X_train,lb.inverse_transform(y_train)))\nprint('Accuracy on VAL  : ',pipe_vc.score(X_val,lb.inverse_transform(y_val)))\n\nfrom sklearn.metrics import confusion_matrix\n\nconfusion_matrix(lb.inverse_transform(y_val),pipe_vc.predict(X_val))","5fb5af64":"pipe_vc.fit(X,lb.inverse_transform(y))","e98ffa2d":"pred = pipe_vc.predict(test)","a4f0530a":"pred","aed7559e":"sub = pd.DataFrame({'Id':test_id,'Cover_Type':pred})\n\n\nsub.to_csv('sub.csv',index = False)","7dbe7e32":"# EDA","18114b3f":"## First look at classifiers\n\nIn this part I will just check different classifiers with more or less default hyperparametes to have a rough idea about score that can be improved later on.\n\nAlso with following code we may have an idea how feature engineering improves accuracy, without computitionaly expeansive gridseachCV","d5fe372f":"Late Submission Score: 0.77907","ef08a595":"### 2. ExtraTreesClassifier","09942c60":"Luckily all features are of int type.\n\nTherefore we can avoide different types of encodings.\n\nAdditionaly we have not got missing values\n\nLet`s check closer these values","c5ec2e61":"Imidiatly we observe than Elevation is the most promise feature, it separates different cover types pretty efficently.\nAlso that is true for Distance to Road feature\n\nNext lets check correlation between numerical features","84de1d08":"Late Submission Score: 0.77766","b3c87585":"From above correlation matrix we observe:\n\n1) Strong correlation between HillShade_3pm and Hillshade_noon\n\n2) Strong correlation between Vertical_Distance_To_Roadways and Hosontal_Distance_To_Roadways\n\n3) Some   correlation between Elevation - (Horizontal_Distance_To_Hydrology\/Horizontal_distance_to_Roadwasys\/ Horizontal_distance_to_fire_points)\n\nTherefore thos features must be removed to avoid problems during fitting, alternativly we can combine correlated features to derive new features. In tis way we don`t loos information","071b5933":"## Summary\n\nLet`s just summarise a bit what scores we have achieved with almost default hyperparameters so far.\n\nUnfortunately Voting strategy has not improved the score significantly\n\nEventhough cross_val score has slightly increase but submission score has decreased.\n\nAnyways, lets try to tune RandomForest and Extratrees with gridsearchCV","9a7a418e":"### 4. XGBClassifier","401278b7":"Late Submission Score: 0.77824","b207a50c":"### 4. VotingClassifier tuned","0e0c4bef":"### 3. ExtraTreesClassifier","1e9362cd":"### 2. RandomForestClassifier","841cdddd":"Late Submission Score: 0.66435","a547e317":"# GridSearchCV\n","04760764":" ### 3. XGB","602de8a4":"# Importing","2b862ef2":"Late Submission Score: 0.78583","a0f43d6f":"### 1. RandomForestClassifier","ef48bdb4":"### 5. VotingClassifier","2eb168bf":"# Modeling","b0903f25":"Late Submission Score: 0.75692","8257cc9e":"Late Submission Score: 0.76138","5de1597b":"# Features","7fa2987e":"Late Submission Score: 0.77821","b6306a6b":"Our test set is much biger than train set. Hope it works fine.\n\nWe have 55 features, 40 columns  are just one hot encoded feature of soil type","e689caed":"### 1. KNeighborsClassifier"}}