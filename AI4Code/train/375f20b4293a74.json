{"cell_type":{"fe4e68b3":"code","d53ab874":"code","f7f6c7ba":"code","93e7e2f9":"code","a9367e68":"code","51a1b8db":"code","bb828e47":"code","cfa02a48":"code","70705313":"code","bf22690c":"code","1920cf3f":"code","8aca6e0d":"code","129c6c1e":"code","900ebd24":"code","7aa7c1b4":"code","e0bd31f6":"code","5db86979":"code","e682af7b":"code","9e1577e8":"code","a6db32b4":"code","6b279869":"code","7fef29ae":"code","1cc48817":"code","aa25e3f9":"code","93ab8596":"code","83ee973e":"code","cbd96492":"code","c5926861":"code","3ce48009":"code","de4e9ab9":"code","130b6012":"code","a76b9784":"code","005dc250":"code","83fa4a03":"code","25b4bf34":"code","111cf5ff":"code","71987ebb":"code","ea160e72":"code","46e52c11":"code","fc5f2341":"code","84bc18a4":"code","2b04b436":"code","6fc72642":"code","4f908d6a":"code","a8a6ed9f":"code","89042c7f":"code","a9a0cb46":"code","e01ddc68":"code","e795d44c":"code","e74341cf":"code","128e5277":"code","4ee2b5b6":"code","f5f1c19c":"code","b4bc2b62":"code","8bba64e8":"code","5383f9b7":"code","14180073":"markdown","8e500432":"markdown","d9db6494":"markdown","c0bea406":"markdown","c1425793":"markdown","13448c76":"markdown","bbaa4e89":"markdown","d588903b":"markdown","b5dc9c78":"markdown","d5dd1459":"markdown","c2d5a0e2":"markdown","fe619405":"markdown","0793ba6f":"markdown","a11d453f":"markdown","392eae27":"markdown","e9fef4aa":"markdown","dc9475f5":"markdown","5f454f75":"markdown","7c3bd0d9":"markdown","efd5e6d4":"markdown","f859d071":"markdown","c3e8b542":"markdown","11967445":"markdown","fc990aa3":"markdown","3ab5d0f8":"markdown","48692e6f":"markdown","68a52bb7":"markdown","4300bb66":"markdown","f9982d08":"markdown","1b55c954":"markdown","1b06699d":"markdown","f3ca14c0":"markdown","6fb2fd95":"markdown","7fbe5292":"markdown","6b1672d0":"markdown","34ceb8e1":"markdown","3b870ba8":"markdown","65fb7001":"markdown","a2e5cd3e":"markdown","c656317a":"markdown","d8b32e82":"markdown","49a63d4e":"markdown","cf9f3def":"markdown","e19b38bb":"markdown","82346e79":"markdown","d8e8c07e":"markdown","3933e502":"markdown","7ef24d2c":"markdown","76d1a54a":"markdown","a4ef7902":"markdown","f5bd5676":"markdown","b06a8c32":"markdown","3f260bf4":"markdown","7a1a4ad2":"markdown","4e8492ae":"markdown"},"source":{"fe4e68b3":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set()\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport plotly.express as ex\nimport plotly.graph_objs as go\nimport plotly.offline as pyo\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report","d53ab874":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nInputpath = \"\"\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        Inputpath = os.path.join(dirname, filename)\n        print(Inputpath)\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f7f6c7ba":"stroke = pd.read_csv(Inputpath)","93e7e2f9":"stroke.shape","a9367e68":"stroke.head()","51a1b8db":"stroke.describe()","bb828e47":"stroke.info()","cfa02a48":"print(\"Rows            :\", stroke.shape[0])\nprint(\"Columns         :\", stroke.shape[1])\nprint (\"Features       :\" , stroke.columns.tolist())\nprint(\"Missing Values  :\", stroke.isnull().sum().values.sum())\nprint(\"Missing Values  :\\n\", stroke.isnull().sum())","70705313":"# A really fantsatic and intelligent way to deal with blanks, from Thoman Konstantin in: https:\/\/www.kaggle.com\/thomaskonstantin\/analyzing-and-modeling-stroke-data\nDT_bmi_pipe = Pipeline( steps=[ \n                               ('scale',StandardScaler()),\n                               ('lr',DecisionTreeRegressor(random_state=42))\n                              ])\n\n#take the copy of df\nX = stroke[['age','gender','bmi']].copy()\n\n#change the categorical variable to numaric\nX.gender = X.gender.replace({'Male':0,'Female':1,'Other':-1}).astype(np.uint8)\n\n#filterout only missing variables\nMissing = X[X.bmi.isna()]\n\n#take non null values\nX = X[~X.bmi.isna()]\n#Take only \"bmi\" values\nY = X.pop('bmi')\n\n#performing fit transformation on X and Y, As we take bmi data in Y hence X dont have bmi data it only have age and gender\nDT_bmi_pipe.fit(X,Y)\n\n#With the dession Tree regrassor we have predict the bmi\npredicted_bmi = pd.Series(DT_bmi_pipe.predict(Missing[['age','gender']]),index=Missing.index)\n\n#assigning the predictive BMI to our df\nstroke.loc[Missing.index,'bmi'] = predicted_bmi\n","bf22690c":"print(\"Missing Values  :\", stroke.isnull().sum().values.sum())","1920cf3f":"data = stroke.stroke.value_counts()\nprint(data)\n\n#creating Pie Chart\nfig = ex.pie(stroke,names='stroke')\nfig.update_layout(title='<b>Stroke Samples<b>')\nfig.show()\n\n","8aca6e0d":"#Categorical Variable\nprint(\"Gender         :\",stroke.gender.unique())\nprint(\"ever_married   :\",stroke.ever_married.unique())\nprint(\"work_type      :\",stroke.work_type.unique())\nprint(\"Residence_type :\",stroke.Residence_type.unique())\nprint(\"smoking_status :\",stroke.smoking_status.unique())","129c6c1e":"#Numaric Variable\nprint(\"hypertension      :\",stroke.hypertension.unique())\nprint(\"heart_disease     :\",stroke.heart_disease.unique())\nprint(\"age               :\",stroke.age.unique())\nprint(\"avg_glucose_level :\",stroke.avg_glucose_level.unique())\nprint(\"bmi               :\",stroke.bmi.unique())","900ebd24":"fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))\nstroke.plot(kind=\"hist\", y=\"age\", bins=70, color=\"b\", ax=axes[0][0])\nstroke.plot(kind=\"hist\", y=\"bmi\", bins=100, color=\"r\", ax=axes[0][1])\nstroke.plot(kind=\"hist\", y=\"avg_glucose_level\", bins=100, color=\"orange\", ax=axes[1][0])\nplt.show()","7aa7c1b4":"sns.boxplot(data=stroke, y='bmi')\nplt.title('Boxplot of bmi')\nplt.show()","e0bd31f6":"sns.boxplot(data=stroke, y='avg_glucose_level')\nplt.title('Boxplot of avg_glucose_level')\nplt.show()","5db86979":"sns.boxplot(data=stroke, y='age')\nplt.title('Boxplot of age')\nplt.show()","e682af7b":"plt.figure(figsize=(6,5))\nsns.countplot(data=stroke,x='gender')","9e1577e8":"fig, ax =plt.subplots(3,2, figsize=(12, 10))\nsns.countplot(data=stroke, x=\"hypertension\", ax=ax[0][0])\nsns.countplot(data=stroke, x=\"ever_married\", ax=ax[0][1])\nsns.countplot(data=stroke, x=\"work_type\", ax=ax[1][0])\nsns.countplot(data=stroke, x=\"heart_disease\", ax=ax[1][1])\nsns.countplot(data=stroke, x=\"Residence_type\", ax=ax[2][0])\nsns.countplot(data=stroke, x=\"smoking_status\", ax=ax[2][1])\nfig.show()","a6db32b4":"#Good One\ncat_stroke = stroke[['gender','Residence_type','smoking_status','stroke']]\nsummary = pd.concat([pd.crosstab(cat_stroke[x], cat_stroke.stroke) for x in cat_stroke.columns[:-1]], keys=cat_stroke.columns[:-1])\nsummary","6b279869":"#We are taking only stroke data\nstrokePositive=stroke.loc[stroke['stroke']==1]","7fef29ae":"def print_percentage(plots, pltText):\n    for bar in plots.patches:\n        plots.annotate(f'{round(bar.get_height()\/len(strokePositive)*100,2)} %', xy=(bar.get_x() + bar.get_width() \/ 2,  \n                   bar.get_height()), ha='center', va='center', size=13, xytext=(0, 8), textcoords='offset points')\n    \n    plt.title(pltText)","1cc48817":"plt.figure(figsize=(10,5))\nplots = sns.countplot(x='ever_married',palette='inferno', data=strokePositive);\n\nprint_percentage(plots, \"Effect of 'Ever Married' on Stroke\\n\")","aa25e3f9":"plt.figure(figsize=(10,5))\nplots = sns.countplot(data=strokePositive,x='work_type',palette='cool');\n\nprint_percentage(plots,\"Effect of 'Work Type' on Stroke\\n\")","93ab8596":"plt.figure(figsize=(10,5))\nplots= sns.countplot(data=strokePositive,x='smoking_status',palette='autumn');\n\nprint_percentage(plots,\"Effect of 'Smoking Status' on Stroke\\n\")\n","83ee973e":"plt.figure(figsize=(10,5))\nplots= sns.countplot(data=strokePositive,x='Residence_type',palette='Greens');\n\nprint_percentage(plots, \"Effect of 'Residence Type' on Stroke\\n\")","cbd96492":"plt.figure(figsize=(10,5))\nplots = sns.countplot(data=strokePositive,x='heart_disease',palette='Reds');\n\nprint_percentage(plots, \"Effect of 'Heart Disease' on Stroke\\n\")\n","c5926861":"plt.figure(figsize=(10,5))\nplots = sns.countplot(data=strokePositive,x='hypertension',palette='Pastel2');\n\nprint_percentage(plots,\"Effect of Hypertension on Stroke\\n\")\n","3ce48009":"stroke.tail()","de4e9ab9":"sns.set(style=\"ticks\");\npal = [\"#FA5858\", \"#58D3F7\"]\n\nsns.pairplot(stroke, hue=\"stroke\", palette=pal);\nplt.title(\"stroke\");","130b6012":"plt.figure(figsize=(15,7))\nsns.heatmap(stroke.corr(),annot=True);","a76b9784":"#need to remove \nstroke.head()","005dc250":"stroke.drop(labels='id', axis=1, inplace=True)","83fa4a03":"stroke.head()","25b4bf34":"bmi_outliers=stroke.loc[stroke['bmi']>50]\nbmi_outliers['bmi'].shape","111cf5ff":"# mean with outliers \nprint(bmi_outliers['stroke'].value_counts())","71987ebb":"stroke[\"bmi\"] = stroke[\"bmi\"].apply(lambda x: 50 if x>50 else x)\nstroke[\"bmi\"] = stroke[\"bmi\"].fillna(28.4)","ea160e72":"stroke.shape","46e52c11":"print (\"Missing values :  \", stroke.isnull().sum().values.sum())","fc5f2341":"stroke[\"Residence_type\"] = stroke[\"Residence_type\"].apply(lambda x: 1 if x==\"Urban\" else 0)\nstroke[\"ever_married\"] = stroke[\"ever_married\"].apply(lambda x: 1 if x==\"Yes\" else 0)\nstroke[\"gender\"] = stroke[\"gender\"].apply(lambda x: 1 if x==\"Male\" else 0)\n\n \nstroke = pd.get_dummies(data=stroke, columns=['smoking_status'])\nstroke = pd.get_dummies(data=stroke, columns=['work_type'])\n","84bc18a4":"stroke","2b04b436":"std=StandardScaler()\ncolumns = ['avg_glucose_level','bmi','age']\nscaled = std.fit_transform(stroke[['avg_glucose_level','bmi','age']])\nscaled = pd.DataFrame(scaled,columns=columns)\nstroke=stroke.drop(columns=columns,axis=1)","6fc72642":"stroke=stroke.merge(scaled, left_index=True, right_index=True, how = \"left\")\nstroke","4f908d6a":"stroke.info()","a8a6ed9f":"X = stroke.drop(['stroke'], axis=1).values \ny = stroke['stroke'].values","89042c7f":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","a9a0cb46":"svc = SVC(random_state=0)\nsvc.fit(X_train, y_train)\n#== \n#Score \n#== \nsvc_score = svc.score(X_train, y_train)\nsvc_test = svc.score(X_test, y_test)\n#== \n#testing model \n#== \ny_pred = svc.predict(X_test)\n#== \n#evaluation\n#== \nprint('Training Score:',svc_score)\nprint('Testing Score:',svc_test)\nsvc_accuracy_score = accuracy_score(y_test,y_pred)\nprint('Accuracy Score:',svc_accuracy_score)\n#=== \n#Confusion Matrix \nplt.figure(figsize=(8,5))\n\ncm = confusion_matrix(y_test,y_pred)\nconf_matrix = pd.DataFrame(data=cm,columns = ['Predicted Negative', 'Predicted Positive'],\n                           index = ['Actual Negative', 'Actual Positive'])\n\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"BuPu\");","e01ddc68":"TN=cm[0,0]\nTP=cm[1,1]\nFN=cm[1,0]\nFP=cm[0,1]\nsvm_sensitivity=TP\/float(TP+FN)\nsvm_specificity=TN\/float(TN+FP)\n\nprint(f'TP: {TP}, FN: {FN}, FP: {FP}, TN: {TN},')\n\nprint('\\n\\nThe acuuracy of the model = TP+TN\/(TP+TN+FP+FN) =       ',(TP+TN)\/float(TP+TN+FP+FN),'\\n',\n\n'The Missclassification = 1-Accuracy =                  ',1-((TP+TN)\/float(TP+TN+FP+FN)),'\\n',\n\n'Sensitivity or True Positive Rate = TP\/(TP+FN) =       ',svm_sensitivity,'\\n',\n\n'Specificity or True Negative Rate = TN\/(TN+FP) =       ',svm_specificity,'\\n')","e795d44c":"forest = RandomForestClassifier(n_estimators = 100)\n#== \nforest.fit(X_train, y_train)\n#== \n#Score \n#== \nforest_score = forest.score(X_train, y_train)\nforest_test = forest.score(X_test, y_test)\n#== \n#testing model \n#== \ny_pred = forest.predict(X_test)\n#== \n#evaluation\n#== \n\nprint('Training Score:',forest_score)\nprint('Testing Score:',forest_test)\nforest_accuracy_score = accuracy_score(y_test,y_pred)\nprint('Accuracy Score:',forest_accuracy_score)\n#print(cm)\n\n#=== \n#Confusion Matrix \nplt.figure(figsize=(8,5))\n\ncm = confusion_matrix(y_test,y_pred)\nconf_matrix = pd.DataFrame(data=cm,columns = ['Predicted Negative', 'Predicted Positive'],\n                           index = ['Actual Negative', 'Actual Positive'])\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"Blues\");\n\n\n","e74341cf":"TN=cm[0,0]\nTP=cm[1,1]\nFN=cm[1,0]\nFP=cm[0,1]\nforest_sensitivity=TP\/float(TP+FN)\nforest_specificity=TN\/float(TN+FP)\n\nprint('The acuuracy of the model = TP+TN\/(TP+TN+FP+FN) =       ',(TP+TN)\/float(TP+TN+FP+FN),'\\n',\n\n'The Missclassification = 1-Accuracy =                  ',1-((TP+TN)\/float(TP+TN+FP+FN)),'\\n',\n\n'Sensitivity or True Positive Rate = TP\/(TP+FN) =       ',forest_sensitivity,'\\n',\n\n'Specificity or True Negative Rate = TN\/(TN+FP) =       ',forest_specificity,'\\n')","128e5277":"model = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n#== \n#Score \n#== \nlogistic_score = model.score(X_train, y_train)\nlogistic_test = model.score(X_test, y_test)\n\n#== \n#testing model \n#== \ny_pred= model.predict(X_test)\n\n\n#=== \n#Confusion Matrix \nplt.figure(figsize=(8,5))\n\ncm = confusion_matrix(y_test,y_pred)\nconf_matrix = pd.DataFrame(data=cm,columns = ['Predicted Negative', 'Predicted Positive'],\n                           index = ['Actual Negative', 'Actual Positive'])\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"Blues\");\n\n#== \n#evaluation\n#== \nprint('Training Score:',logistic_score)\nprint('Testing Score:',logistic_test)\nforest_accuracy_score = accuracy_score(y_test,y_pred)\nprint('Accuracy Score:',forest_accuracy_score)\nprint('Classification Report:\\n',classification_report(y_test, y_pred))\n#print(accuracy_score(y_test,y_pred))\n","4ee2b5b6":"TN=cm[0,0]\nTP=cm[1,1]\nFN=cm[1,0]\nFP=cm[0,1]\nlogistic_sensitivity=TP\/float(TP+FN)\nlogistic_specificity=TN\/float(TN+FP)\n\nprint('The acuuracy of the model = TP+TN\/(TP+TN+FP+FN) =       ',(TP+TN)\/float(TP+TN+FP+FN),'\\n',\n\n'The Missclassification = 1-Accuracy =                  ',1-((TP+TN)\/float(TP+TN+FP+FN)),'\\n',\n\n'Sensitivity or True Positive Rate = TP\/(TP+FN) =       ',logistic_sensitivity,'\\n',\n\n'Specificity or True Negative Rate = TN\/(TN+FP) =       ',logistic_specificity,'\\n')","f5f1c19c":"features = list(stroke.columns)\n#print(features.index('stroke'))\n#== \n#Delete target label \n#== \ndel features[features.index('stroke')]\nfeatures","b4bc2b62":"importance = model.coef_\n# summarize feature importance\nfor i,v in enumerate(importance[0]):\n    print(f'Feature: {i} ==> \"{features[i]}\", Score: {abs(v)}')\n\nplt.figure(figsize=(20,7))    \nsns.barplot(x=[x for x in range(importance.shape[1])], y=importance[0]).set_xticklabels(features, rotation=90)\nplt.show()","8bba64e8":"dicts = {}\ndictsort = {}\nlstkey = []\nlstvalue = []\n    \nfor i,v in enumerate(importance[0]):\n    lstkey.append(features[i])\n    lstvalue.append(round(v,2))\n\n# COnvert to dictionary\nfor key in lstkey:\n    for value in lstvalue:\n        dicts[key] = abs(value)\n        lstvalue.remove(value)\n        break\n\nprint(\"Dictionary from lists :\\n \",dicts)\n\ndictsort = {k: v for k, v in sorted(dicts.items(), key=lambda item: item[1])}\n\nprint(\"\\n\\nDictionary in sorted order :\\n \",dictsort)\n","5383f9b7":"from prettytable import PrettyTable\ntable = PrettyTable()\nx = PrettyTable()\n\nx.field_names = ['Model', 'Training Score', 'Testing Score', 'Accuracy Score', 'Sensitivity\/True Positive Rate', 'Specificity\/True Negative Rate']\n\nx.add_row([\"SVM\", round(svc_score,3), round(svc_test,3), round(svc_accuracy_score,3), round(svm_sensitivity,3), round(svm_specificity,3)])\nx.add_row([\"Random Forest\", round(forest_score,3), round(forest_test,3), round(forest_accuracy_score,3), round(forest_sensitivity,3), round(forest_specificity,3)])\nx.add_row([\"Logistic Regression\", round(logistic_score,3), round(logistic_test,3), round(forest_accuracy_score,3), round(logistic_sensitivity,3), round(logistic_specificity,3)])\n\nprint(x)","14180073":"#### Initially, the dataset had 201 samples with absent BMI value; rather than imputing it naively with the mean or the median, we used a simple decision tree model which based on the age and gender of all other samples gave us a fair prediction for the missing values.","8e500432":"### Sensitivity & Specificity","d9db6494":"1) id: unique identifier\n\n2) gender: \"Male\", \"Female\" or \"Other\" <br>\n\n3) age: age of the patient <br>\n\n4) hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension <br>\n\n5) heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease <br>\n\n6) ever_married: \"No\" or \"Yes\" <br>\n\n7) work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\" <br>\n\n8) Residence_type: \"Rural\" or \"Urban\" <br>\n\n9) avg_glucose_level: average glucose level in blood <br>\n\n10) bmi: body mass index <br>\n\n11) smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"* <br>\n\n12) stroke: 1 if the patient had a stroke or 0 if not <br>\n\n*Note: \"Unknown\" in smoking_status means that the information is unavailable for this patient","c0bea406":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Data preprocessing<\/h1>","c1425793":"<h1 style=\"font-family:Georgia;font-size:175%;text-align:left\">Feature Importance using Logistic Regression<\/h1>","13448c76":"#### Observation: 59.85% Private employee exposed to more stroke","bbaa4e89":"#### Observation: both formal smoker (28.11%) or regular smoker (16.87%) increases risk of the stroke but I am confused with never smoked data (36.14%)","d588903b":"## Data Summary ( Check for missing values ) ","b5dc9c78":"## Observation\n- <b>0%<\/b> People <u>with<\/u> disease(Stroke) were correctly identify and <b>100%<\/b> people <u>without<\/u> disease(Stroke) were corretly idntify by the <b>SVM<\/b><br>\n- <b>0%<\/b> People <u>with<\/u> disease(Stroke) were correctly identify and <b>99.7%<\/b> people <u>without<\/u> disease(Stroke) were corretly idntify by the <b>Random Forest<\/b><br>\n- <b>0.01%<\/b> People <u>with<\/u> disease(Stroke) were correctly identify and <b>100%<\/b> people <u>without<\/u> disease(Stroke) were corretly idntify by the <b>Logistic Regression<\/b><br><br>\n#### From the above statistics it is clear that the models are <b><u>more highly specific than sensitive<\/u><\/b>. Models are able to accurately predict the negative cases over the positives.","d5dd1459":"#### Observation: 73.49% of strok cases They had hypertension","c2d5a0e2":"<h1 style=\"font-family:Georgia;font-size:175%;text-align:left\">Stroke\/Work Type<\/h1>","fe619405":"#### Finding Categorical and Numarical Variable","0793ba6f":"<h1 style=\"font-family:Georgia;font-size:175%;text-align:left\">Stroke\/Ever Married<\/h1>","a11d453f":"## Gender","392eae27":"<h1 style=\"font-family:Georgia;font-size:175%;text-align:left\">Stroke\/Heart Disease<\/h1>","e9fef4aa":"## BMI Box Plot","dc9475f5":"#### * info() says that bmi column have (5110-4909) = 201 null value \/ missing value","5f454f75":"<h1 style=\"font-family:Georgia;font-size:175%;text-align:left\">Encoding Categorical Features<\/h1>","7c3bd0d9":"![](https:\/\/2.bp.blogspot.com\/-EvSXDotTOwc\/XMfeOGZ-CVI\/AAAAAAAAEiE\/oePFfvhfOQM11dgRn9FkPxlegCXbgOF4QCLcBGAs\/s1600\/confusionMatrxiUpdated.jpg)","efd5e6d4":"# Removing Outliers and Redundant Columns","f859d071":"<h1 style=\"font-family:Georgia;font-size:175%;text-align:left\">Splitting <\/h1>","c3e8b542":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Introduction<\/h1>\n\n<h1 style=\"font-family:Georgia;font-size:175%;text-align:center\">Through this data we will try to know more about strokes and Make a model to try to predict strokes<\/h1>\n<h1 style=\"font-family:Georgia;font-size:150%;text-align:left\">first what is a stroke?<\/h1>\n\n*  Stroke is a medical emergency. A stroke occurs when blood flow to a part of your brain is interrupted or reduced, preventing brain tissue from getting oxygen and nutrients. Brain cells begin to die within minutes \n","11967445":"<h1 style=\"font-family:Georgia;font-size:175%;text-align:left\">Stroke or not in Categorical Features<\/h1>","fc990aa3":"<a id=\"1\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Exploratory Data Analysis<\/h1>\n","3ab5d0f8":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Read & Explore<\/h1>","48692e6f":"<h1 style=\"font-family:Georgia;font-size:175%;text-align:left\">Target & Features <\/h1>","68a52bb7":"We are clearly dealing with an imbalanced dataset, and later in our pipeline, we will have to deal with this problem potentially with upscaling; that way, we give our models a better chance of learning the small details which define potential stroke individuals.\n","4300bb66":"<h1 style=\"font-family:Georgia;font-size:175%;text-align:left\">SVM <\/h1>","f9982d08":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Dataset Information<\/h1>","1b55c954":"### Sensitivity & Specificity","1b06699d":"#### Observation: 81.12% of people who have had a stroke do not have any heart disease.","f3ca14c0":"<h1 style=\"font-family:Georgia;font-size:175%;text-align:left\">Heatmap Correlation<\/h1>","6fb2fd95":"<a id=\"1\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Data Visualization<\/h1>\n","7fbe5292":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Data Preprocessing<\/h1>\n","6b1672d0":"## Observation\n\n* <b>Age<\/b>: Distribution is normal distribution\n* <b>BMI<\/b>: Looks like it have outliers. (See below box plot)\n* <b>Avg glucose level<\/b>: distribution is semi Normal distribution but as the normal avg of blood in sugar is less than 140, which will not be good for this feature as most of the data are below 150. hence we will not found proper correlation value between diabetes and strokes","34ceb8e1":"# Data Visualization","3b870ba8":"### Boxplot of age","65fb7001":"<h1 style=\"font-family:Georgia;font-size:175%;text-align:left\">Scaling The variance in Features<\/h1>","a2e5cd3e":"#### Observation: 88.35% of Married person exposed to more stroke","c656317a":"## Univariate  Analysis","d8b32e82":"## Bivariate Analysis","49a63d4e":"### Sensitivity & Specificity","cf9f3def":"<h1 style=\"font-family:Georgia;font-size:175%;text-align:left\">Random Forest Classifier<\/h1>","e19b38bb":"## Numaric Variables","82346e79":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Importing Libraries<\/h1>","d8e8c07e":"<h1 style=\"font-family:Georgia;font-size:175%;text-align:left\">Residence Type<\/h1>","3933e502":"# Summary of Performance","7ef24d2c":"### Boxplot of Average Glucose Level","76d1a54a":"<h1 style=\"font-family:Georgia;font-size:175%;text-align:left\">Stroke\/Smoking Status<\/h1>","a4ef7902":"<h1 style=\"font-family:Georgia;font-size:175%;text-align:left\">Stroke\/Hypertension<\/h1>","f5bd5676":"<h1 style=\"font-family:Georgia;font-size:175%;text-align:left\">Logistic Regression<\/h1>","b06a8c32":"### Observation:\nWe generally consider correlations above 0.75 to be relatively strong; correlations between 0.45 and 0.75 are moderate, and those below 0.45 are considered weak.<br>\nIn this case, we cannot see correlations above 0.35. For this kind of data, we are considering correlations above 0.25 to be relatively strong; correlations between 0.17 and 0.24 are moderate, and those below 0.17 are considered weak. <br>\n<b>As 0.046 is very week hence we can say there is no correlation between stroke and BMI.<b>","3f260bf4":"<h1 style=\"font-family:Georgia;font-size:175%;text-align:left\">Stroke Or not Pair Grid<\/h1>","7a1a4ad2":"#### Observation: As both the data are almost same hence we can say that 'Residence Type' has no impact on the stroke.","4e8492ae":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Classification Models<\/h1>"}}