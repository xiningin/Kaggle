{"cell_type":{"29060adb":"code","6442e3b1":"code","c1844980":"code","da7017ac":"code","8fe7bc41":"code","713e00ac":"code","2cf61097":"code","e5a43a8c":"code","5567e311":"code","8e2a1f51":"code","68c900f9":"code","5748073e":"code","d476056a":"code","242e6dbe":"code","8efa07fb":"code","4199180f":"markdown"},"source":{"29060adb":"import lightgbm\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport numpy as np\nimport pandas as pd \n\nfrom memory_reduction_script import reduce_mem_usage_sd as mr # We are using the memory reduction by our utility script","6442e3b1":"##### Functions\n# 1st function\n\ndef j_mode(x, dropna=True):\n    #obtains the most frequent, not nan value\n    try: \n        mode = x.value_counts(dropna=dropna).index[0]\n        # mode != mode -> isnull\n        if (mode != mode) and (x.value_counts(dropna=dropna).index > 1):\n            mode = x.value_counts(dropna=dropna).index[1]            \n        return mode\n    except: \n        return x\n\n# 2nd function\n\ndef normalize_columns(group, cols, df_train, df_test, verbose=True):\n    # replacing the card values depending on card1 with the most frequent, not nan value\n    ### initialize trace variables\n    if verbose:\n        s_train_before    = df_train.shape\n        s_train_na_before = df_train[cols].isnull().sum()\n        s_test_before     = df_test.shape\n        s_test_na_before  = df_test[cols].isnull().sum()\n\n        #train_nulls_before = pd.concat([df_train[cols].isnull().sum(), df_train[cols].isnull().sum()], axis=1)\n        #test_nulls_before  = pd.concat([df_test[cols].isnull().sum(), df_test[cols].isnull().sum()], axis=1)\n\n    \n    #normalize the group with the mode.\n    grouped_train = df_train.groupby([group])\n    grouped_test  = df_test.groupby([group])\n\n    n = 0\n    for col in cols:\n        if verbose:\n            print('normalizing ' + str(col))\n        df_train[col] = grouped_train[col].transform(lambda x: j_mode(x))\n        df_test[col]  = grouped_test[col].transform(lambda x: j_mode(x))\n        n += 1\n        \n    ### print the traces\n    if verbose:        \n        print(f'train shape before: {s_train_before}, after: {df_train.shape}')\n        print(f'test shape before: {s_test_before}, after: {df_test.shape}')\n        train_nulls_after = df_train[cols].isnull().sum()\n        test_nulls_after  = df_test[cols].isnull().sum()\n        for i in range(s_test_na_before.shape[0]):\n            print(f'(train) {s_train_na_before.index[i]} nulls before: {s_train_na_before.iloc[i]}, nulls after: {train_nulls_after.iloc[i]}')\n        for i in range(s_test_na_before.shape[0]):\n            print(f'(test) {s_test_na_before.index[i]} nulls before: {s_test_na_before.iloc[i]}, nulls after: {test_nulls_after.iloc[i]}')\n\n# 3rd function\n\ndef fill_na(cols, df_train, df_test, num_rep=-999, obj_rep ='Unknown', verbose=True):\n    ### initialize trace variables\n    if verbose:\n        s_train_na_before = train[cols].isnull().sum()\n        s_test_na_before  = test[cols].isnull().sum()\n        \n    for col in cols:\n        if df_train[col].dtype == 'O':\n            df_train[col] = df_train[col].fillna(obj_rep)\n            df_test[col]  = df_test[col].fillna(obj_rep)\n        else:\n            df_train[col] = df_train[col].fillna(num_rep)\n            df_test[col]  = df_test[col].fillna(num_rep)\n\n    ### print the traces\n    if verbose:        \n        train_nulls_after = df_train[cols].isnull().sum()\n        test_nulls_after  = df_test[cols].isnull().sum()\n        \n        for i in range(s_test_na_before.shape[0]):\n            print(f'(train) {s_train_na_before.index[i]} nulls before: {s_train_na_before.iloc[i]}, nulls after: {train_nulls_after.iloc[i]}')\n        for i in range(s_test_na_before.shape[0]):\n            print(f'(test) {s_test_na_before.index[i]} nulls before: {s_test_na_before.iloc[i]}, nulls after: {test_nulls_after.iloc[i]}')\n","c1844980":"##### Download of files.\n\nprint('Downloading datasets...')\nprint(' ')\ntrain = pd.read_pickle('\/kaggle\/input\/1-fraud-detection-memory-reduction\/train_mred.pkl')\nprint('Train has been downloaded... (1\/2)')\ntest = pd.read_pickle('\/kaggle\/input\/1-fraud-detection-memory-reduction\/test_mred.pkl')\nprint('Test has been downloaded... (2\/2)')\nprint(' ')\nprint('All files are downloaded')","da7017ac":"print(train.shape) # 6 months of data in train\nprint(test.shape)  # 6 months of data in test","8fe7bc41":"object_columns = train.dtypes[train.dtypes=='O'].index\nfor col in object_columns:\n    print(col ,'nulls (train):', train[col].isnull().sum()\/train.shape[0])\n    print(col ,'nulls (test):', test[col].isnull().sum()\/test.shape[0])","713e00ac":"##### Group outliers for card4 and card6\nprint(\" BEFORE \".center(20, '#'))\nprint('Train - card6:\\n', train['card6'].value_counts(dropna=False))\nprint(' ')\nprint('Train - card4:\\n', train['card4'].value_counts(dropna=False))\nprint(' ')\n\ntrain.card6 = train.card6.replace(['debit or credit', 'charge card'], np.nan)\ntrain.card4 = train.card4.replace(['american express', 'discover'], np.nan)\n\nprint(\" AFTER \".center(20, '#'))\nprint('Train - card6:\\n', train['card6'].value_counts(dropna=False))\nprint(' ')\nprint('Train - card4:\\n', train['card4'].value_counts(dropna=False))\nprint(' ')","2cf61097":"##### Label encoding all object columns with our dictionary\nfor col in object_columns:\n    print(f'String values from {col} are being transformed to numeric...')\n    #unique values without nans\n    unique_values = list(train[col].dropna().unique()) \n\n    #create the dictionary\n    str_to_num = dict()\n    for num,value in enumerate(unique_values):\n        str_to_num[value] = num\n\n    #apply it to column\n    train[col] = train[col].map(str_to_num)\n    test[col]  = test[col].map(str_to_num)\n    print(f'String values from {col} are transformed!')\n\nprint(' ')\nprint('Done!') ","e5a43a8c":"##### Normalizing by replacing less frequents and NaNs values with the mode using CARD1\n\ncard_cols = ['card' + str(i) for i in range(2,7)]\nnormalize_columns('card1', card_cols, df_train=train, df_test=test, verbose=True) \n\ndomain_cols = ['P_emaildomain', 'R_emaildomain']\nnormalize_columns('card1', domain_cols, df_train=train, df_test=test, verbose=True)","5567e311":"##### Replacing NaNs\n\n# Cards columns\ncard_cols_c = ['card' + str(i) for i in range(1,7)]\nfill_na(card_cols_c, num_rep=-999, obj_rep ='Unknown', df_train=train, df_test=test, verbose=True)\n\n# Address columns\naddrs = ['addr1', 'addr2']\nfill_na(addrs, num_rep=-999, obj_rep ='Unknown', df_train=train, df_test=test, verbose=True)\n\n# Domains columns\nfill_na(domain_cols, num_rep=-999, obj_rep ='Unknown', df_train=train, df_test=test, verbose=True)","8e2a1f51":"##### Creation of UIDs\n\n# uid1\n\ntrain['uid1'] = train['card1'].astype('str') + '_' + train['card2'].astype('str')     + '_' \\\n              + train['card3'].astype('str') + '_' + train['card4'].astype('str')     + '_' \\\n              + train['card5'].astype('str') + '_' + train['card6'].astype('str')     + '_' + train['addr1'].astype('str') + '_' \\\n              + train['addr2'].astype('str') + '_' + train['ProductCD'].astype('str') + '_' + train['D1achr'].astype('str')\n\ntest['uid1']  = test['card1'].astype('str')  + '_' + test['card2'].astype('str')     + '_' \\\n              + test['card3'].astype('str')  + '_' + test['card4'].astype('str')     + '_' \\\n              + test['card5'].astype('str')  + '_' + test['card6'].astype('str')     + '_' + test['addr1'].astype('str') + '_' \\\n              + test['addr2'].astype('str')  + '_' + test['ProductCD'].astype('str') + '_' + test['D1achr'].astype('str')\n\n# uid2\n\ntrain['uid2'] = train['card1'].astype('str')   + '_' + train['D2achr'].astype('str')  + '_' \\\n              + train['C13'].astype('str')     + '_' + train['D11achr'].astype('str') + '_' \\\n              + train['D10achr'].astype('str') + '_' + train['D15achr'].astype('str') + '_' \\\n              + train['D4achr'].astype('str')\n\ntest['uid2']  = test['card1'].astype('str')    + '_' + test['D2achr'].astype('str')   + '_' \\\n              + test['C13'].astype('str')      + '_' + test['D11achr'].astype('str')  + '_' \\\n              + test['D10achr'].astype('str')  + '_' + test['D15achr'].astype('str')  + '_' \\\n              + test['D4achr'].astype('str')\n\n# uids\n\ntrain['uids'] = train['card1'].astype('str')   + '_' + train['addr1'].astype('str')  + '_' \\\n              + train['D1achr'].astype('str')\n\ntest['uids']  = test['card1'].astype('str')    + '_' + test['addr1'].astype('str')   + '_' \\\n              + test['D1achr'].astype('str')\n\nprint('Unique uid1 (train):', len(train.uid1.unique()))\nprint('Unique uid1 (test):', len(test.uid1.unique()))\n\nprint('Unique uid2 (train):', len(train.uid2.unique()))\nprint('Unique uid2 (test):', len(test.uid2.unique()))\n\nprint('Unique uids (train):', len(train.uids.unique()))\nprint('Unique uids (test):', len(test.uids.unique()))","68c900f9":"##### Selecting type of uid filling type\nuid = 'uid1'","5748073e":"%%time\n##### Normalizing by replacing less frequents and NaNs values with the mode using UID1\n\nm_cols = ['M'+str(i) for i in range(1,10)]\nnormalize_columns(uid, m_cols, df_train=train, df_test=test, verbose=True) \nfill_na(m_cols, num_rep=-999, obj_rep ='Unknown', df_train=train, df_test=test, verbose=True)","d476056a":"%%time\n##### Normalizing by replacing less frequents and NaNs values with the mode using UID1\n\ndist_cols = ['dist1', 'dist2']\nnormalize_columns(uid, dist_cols, df_train=train, df_test=test, verbose=True)\nfill_na(dist_cols, num_rep=-999, obj_rep ='Unknown', df_train=train, df_test=test, verbose=True)","242e6dbe":"##### Memory reduction\n\ntrain = mr(train, verbose=True)\ntest  = mr(test, verbose=True)","8efa07fb":"##### Saving both DataFrames into binary files to speed next uploadings.\n\nprint('Saving datasets...')\ntrain.to_pickle('train.pkl')\nprint('Train has been saved... (1\/2)')\ntest.to_pickle('test.pkl')\nprint('Test has been saved... (2\/2)')\nprint('Done!')","4199180f":"# Preprocessing:\n\n1. Three different card keys (ways to group cards or users):\n   \n    * `uid1` with `card1` to `card6` + `addr1` + `addr2` + `ProductCD` + `D1achr`\n    * `uid2` with `card1` + `D2achr` + `C13` + `D11achr` + `D10achr` + `D15achr` + `D4achr`\n    * `uids` with `card1` + `addr1` + `D1achr`  \n  \n  \n2. Normalize all the fields related to card1 -> replace less frequent, NaNs values with the mode.\n3. Input all categorical data - we encode them with a dictionary to set the same value for all NaNs."}}