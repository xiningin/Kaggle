{"cell_type":{"22e0b76a":"code","92bc19c3":"code","216e6cb6":"code","098f69fc":"code","25636339":"code","e82d0787":"code","8bda9cb0":"code","dfe891c6":"code","a9005cbb":"code","3c58afae":"code","870a844f":"code","e59ace2f":"code","846f6a96":"code","580b1308":"code","b474ce2a":"code","9dc26dae":"code","08540882":"code","34f7fe52":"code","65b351e2":"code","3fa56df0":"code","f8cfbd0f":"code","f4d98747":"code","58012c86":"code","9ec46f10":"code","7bfe882c":"code","577180e0":"code","624a235a":"code","0569800f":"code","2efb5c1d":"code","108570cd":"code","4206d1a6":"code","e7038d6c":"code","628419e0":"code","ea70126e":"markdown","3dd4a739":"markdown","beaf8902":"markdown","ca8ead73":"markdown","2aea60e8":"markdown","157c7a39":"markdown","dcfd9844":"markdown","760b0612":"markdown","0e196893":"markdown","42075c49":"markdown","a8eddfd1":"markdown","3c2799ee":"markdown","8e62913c":"markdown","41d2be07":"markdown","33432eca":"markdown","beeca78a":"markdown","2b1b110d":"markdown","3e5b5604":"markdown"},"source":{"22e0b76a":"#Let's import everything we will need\nimport pandas as pd\nimport numpy as np\nfrom IPython.display import display\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom subprocess import check_output\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nsns.set_style('whitegrid')\n\n%matplotlib inline\n%matplotlib inline","92bc19c3":"X = pd.read_csv('train.csv')\n\nX.head()\n\ny = X['Status']\n\nprint('Number of records: ', X.shape[0])","216e6cb6":"X.head()","098f69fc":"X.isnull().values.ravel().sum()","25636339":"X.dtypes","e82d0787":"len(X)","8bda9cb0":"X.salary.unique()","dfe891c6":"X.salary.replace({'low':1,'medium':2,'high':3},inplace=True)","a9005cbb":"X = pd.get_dummies(X,columns=['dept'],drop_first=True)","3c58afae":"X.describe()","870a844f":"X.columns","e59ace2f":"fig = plt.figure(figsize=(7,4))\ncorr = X[['satisfaction_level', 'last_evaluation', 'Emp ID', 'number_project',\n       'average_montly_hours', 'time_spend_company', 'Work_accident',\n       'promotion_last_5years', 'salary', 'Status']].corr()\nsns.heatmap(corr,annot=True,cmap='seismic',\n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)\nplt.title('Heatmap of Correlation Matrix')","846f6a96":"plt.figure(figsize = (12,8))\nplt.subplot(1,2,1)\nplt.plot(X.satisfaction_level[X.Status == 1],X.last_evaluation[X.Status == 1],'ro', alpha = 0.2)\nplt.ylabel('Last Evaluation')\nplt.title('Employees who left')\nplt.xlabel('Satisfaction level')\n\nplt.subplot(1,2,2)\nplt.title('Employees who stayed')\nplt.plot(X.satisfaction_level[X.Status == 0],X.last_evaluation[X.Status == 0],'bo', alpha = 0.2,)\nplt.xlim([0.4,1])\nplt.ylabel('Last Evaluation')\nplt.xlabel('Satisfaction level')","580b1308":"Status","b474ce2a":"from sklearn.cluster import KMeans\nkmeans_df =  X[X.Status == 1][['satisfaction_level','last_evaluation']]\nkmeans = KMeans(n_clusters = 3, random_state = 0).fit(kmeans_df)\nprint(kmeans.cluster_centers_)\n\nStatus = X[X.Status == 1][['satisfaction_level','last_evaluation','time_spend_company']]\nStatus['label'] = kmeans.labels_\nplt.figure(figsize=(10,7))\nplt.xlabel('Satisfaction Level')\nplt.ylabel('Last Evaluation score')\nplt.title('3 Clusters of employees who left')\nplt.plot(Status.satisfaction_level[Status.label==0],Status.last_evaluation[Status.label==0],'o', alpha = 0.2, color = 'r')\nplt.plot(Status.satisfaction_level[Status.label==1],Status.last_evaluation[Status.label==1],'o', alpha = 0.2, color = 'g')\nplt.plot(Status.satisfaction_level[Status.label==2],Status.last_evaluation[Status.label==2],'o', alpha = 0.2, color = 'b')\n\nplt.legend(['Winners','Frustrated','Bad Match'], loc = 'best', fontsize = 13, frameon=True)","9dc26dae":"winners = Status[Status.label ==0]\nfrustrated = Status[Status.label == 1]\nbad_match = Status[Status.label == 2]\n\nplt.figure(figsize=(10,4))\nsns.kdeplot(winners.time_spend_company, color = 'r', shade=True)\nsns.kdeplot(bad_match.time_spend_company, color ='b', shade=True)\nsns.kdeplot(frustrated.time_spend_company, color ='g', shade=True)\nplt.legend(['Winners','Bad Match','Frustrated'])\nplt.title('Leavers: time_spend_company')","08540882":"#produce kernel density estimate plots and histograms to look at each feature\nfig = plt.figure(figsize=(10,4))\nax=sns.kdeplot(X.loc[(X['Status'] == 0),'satisfaction_level'] , color='b',shade=True, label='Stayed')\nax=sns.kdeplot(X.loc[(X['Status'] == 1),'satisfaction_level'] , color='r',shade=True, label='Left')\nplt.title('Satisfaction levels')","34f7fe52":"#produce kernel density estimate plots and histograms to look at each feature\nfig = plt.figure(figsize=(10,4))\nax=sns.kdeplot(X.loc[(X['Status'] == 0),'number_project'] , color='b',shade=True, label= 'Stayed')\nax=sns.kdeplot(X.loc[(X['Status'] == 1),'number_project'] , color='r',shade=True, label= 'Left')\nplt.title('Number of projects')","65b351e2":"#produce kernel density estimate plots and histograms to look at each feature\nfig = plt.figure(figsize=(10,4))\nax=sns.kdeplot(X.loc[(X['Status'] == 0),'average_montly_hours'] , color='b',shade=True, label='Stayed')\nax=sns.kdeplot(X.loc[(X['Status'] == 1),'average_montly_hours'] , color='r',shade=True, label='Left')\nplt.title('Average monthly hours worked')","3fa56df0":"#produce kernel density estimate plots and histograms to look at each feature\nfig = plt.figure(figsize=(10,4))\nax=sns.kdeplot(X.loc[(X['Status'] == 0),'salary'] , color='b',shade=True, label='Stayed')\nax=sns.kdeplot(X.loc[(X['Status'] == 1),'salary'] , color='r',shade=True, label='Left')\nplt.title('Salary: (1-Low; 2-Medium; 3-high)')","f8cfbd0f":"fig = plt.figure(figsize=(10,4),)\nsns.barplot(x = 'time_spend_company', y = 'Status', data = X, saturation=1)","f4d98747":"features = X[['satisfaction_level','average_montly_hours','promotion_last_5years','salary','number_project']]\nX = features\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0,test_size=0.25)\nprint('Training set volume:', X_train.shape[0])\nprint('Test set volume:', X_test.shape[0])","58012c86":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X_train,y_train)","9ec46f10":"accuracy_score(y_test,logreg.predict(X_test))","7bfe882c":"# Create Naive Bayes classifier\nclf_gb = GaussianNB()\nclf_gb.fit(X_train, y_train)\npredicts_gb = clf_gb.predict(X_test)\nprint(\"GB Accuracy Rate, which is calculated by accuracy_score() is: %f\" % accuracy_score(y_test, predicts_gb))","577180e0":"#Create k-nn\nknn=KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train,y_train)\ny_pred=knn.predict(X_test)\nprint(\"KNN5 Accuracy Rate, which is calculated by accuracy_score() is: %f\" %accuracy_score(y_test,y_pred))","624a235a":"k_range = range(1,26)\nscores=[]\nfor k in k_range:\n    knn=KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train,y_train)\n    y_pred=knn.predict(X_test)\n    scores.append(accuracy_score(y_test,y_pred))\n\nplt.plot(k_range,scores)\nplt.xlabel('Value of k for KNN')\nplt.ylabel('Testing accuracy')","0569800f":"#Decision Tree\nclf_dt = tree.DecisionTreeClassifier(min_samples_split=25)\nclf_dt.fit(X_train, y_train)\npredicts_dt = clf_dt.predict(X_test)\nprint(\"Decision tree Accuracy Rate, which is calculated by accuracy_score() is: %f\" % accuracy_score(y_test, predicts_dt))","2efb5c1d":"#SVM -> takes a few seconds to run!\nclf_svm = svm.SVC(kernel='rbf',probability=False)\nclf_svm.fit(X_train,y_train)\npredict_svm = clf_svm.predict(X_test)\nprint(\"SVM Accuracy Rate, which is calculated by accuracy_score() is: %f\" % accuracy_score(y_test, predict_svm))","108570cd":"#Random forest classifier\nclf_rf = RandomForestClassifier(n_estimators = 10,min_samples_split=2,max_depth=30)\nclf_rf.fit(X_train, y_train)\naccuracy_rf = clf_rf.score(X_test,y_test)\nprint(\"Random Forest Accuracy Rate, which is calculated by accuracy_score() is: %f\" % accuracy_rf)","4206d1a6":"# The Random forest classifier seems to produce the best results so far, so let's try to optimise it!\n\nmax_depth = range(1,50)\nscores=[]\nfor r in max_depth:\n    clf_rf = RandomForestClassifier(n_estimators = 10,min_samples_split=2,max_depth=r,random_state=7)\n    clf_rf.fit(X_train,y_train)\n    y_pred=clf_rf.predict(X_test)\n    scores.append(accuracy_score(y_test,y_pred))\n    \nplt.plot(max_depth,scores)\nplt.xlabel('Value of r for Random Forest')\nplt.ylabel('Testing accuracy')\n\n# we're getting different values each time, but depth of around 30 seems to give good results.","e7038d6c":"best = np.argmax(scores)\nclf_rf = RandomForestClassifier(n_estimators = 10,min_samples_split=2,max_depth=best+1,random_state=7)\nclf_rf.fit(X_train,y_train)\ny_pred=clf_rf.predict(X_test)\naccuracy_score(y_test,y_pred)","628419e0":"len(Test)","ea70126e":"### The distribution shows that \"Frustrated\" employees more or less fast realise the company do not suit their demand, meanwhile \"Bad Match\" colleagues still trying to reabilitate for an year more. Differens situation with \"Winners\" category. They took all they could from the company and left for better life. Looks quite wise. ","3dd4a739":"#### People who left mostly work on a small number of projects (2), or on a large number of projects (5-7). I added a new feature: number of projects worked on per year. I defined this as the number of projects the employee works on during his\/her employment period, divided by the total number of years the employee worked in the company. People who left mostly work on a lower number of projects per year when compare to the employees who have not left. ","beaf8902":"#### People who left either work a small amount of hours on average per month (lower than 150 hours), or they work a large number of hours (more than 250). This means that the employees who leave tend to either not work much or work a lot. The fact that employees who leave are evaluated either bad or quite good in their last performance evaluation might be related to this fact.","ca8ead73":"#### Now, let's perform some simple statistics to better understand the data:","2aea60e8":"#### Good, we don't have any missing values \n\nNow, let's check the data types:","157c7a39":"#### There seems to be a clear inverse relation between satisfaction levels and people leaving","dcfd9844":"#### Let's look the the average monthly hours distribution for those that left:","760b0612":"### Due to the test data do not match the task, it is impossible to make submit:\n### they expect more than 5K samples meanwhile there only 4565.\n### I have tried to add the difference with clear data, but it appeared the indices also wrong,\n### all of them, not only synthesized by me.\n\n### This is my first try publishing notebooks on kaggle. So be please kindly fair.\n### I've also borrowed some techtics from other similar competitions notebooks.\n### Thank you for reading this.","0e196893":"#### We will now visualize the distribution of several features of interest using a histogram or a kernel density estimate plot.","42075c49":"#### We can clearly spot a trend here - the higher the salary, the lower the chances of employees leaving.","a8eddfd1":"### We will now visualize the distribution of several features of interest using a histogram or a kernel density estimate plot.","3c2799ee":"#### The majority of employees tend to leave around the 5th year.","8e62913c":"#### We will need the salary column for our analysis, so let's map it appropriately:","41d2be07":"#### Looks like people who leave the company either did pretty bad or pretty good in their last performance evaluation. There are not many medium level performers among the people who left. If an employee is evaluated in the range of 0.6 to 0.8 (ball park), they are likely still working in the company.","33432eca":"#### Let's plot the clusters using the 3 classifications above:","beeca78a":"### There are 3 clusters for those employees who left.\n\nThe happy and appreciated - we'll call them \"Winners\" - those who leave because they were offered a better opportunity.\n\nThe appreciated but unhappy - Maybe they are over-qualified for the job. Let's call them the \"Frustrated\"\n\nThe unappreciated and unhappy - It is not surprising that these would leave, possibly even fired. They are simply a \"Bad Match\" ","2b1b110d":"#### Let's look at our dataset first:","3e5b5604":"#### Also we need dummi dept feature"}}