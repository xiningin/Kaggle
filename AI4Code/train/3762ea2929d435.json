{"cell_type":{"5359eb61":"code","97745ea2":"code","4e2d2be7":"code","4bd987ca":"code","83abfcbc":"code","e009d213":"code","244dc8d5":"code","6f7cd5f9":"code","80182541":"code","cf7e3be4":"code","06026a98":"code","2da32346":"code","f8174de8":"code","d2126024":"markdown","f6001741":"markdown","a27ef070":"markdown","d759bb16":"markdown","0fc0264e":"markdown","83a7c106":"markdown","3f6575a9":"markdown","d6eb0317":"markdown","851962a9":"markdown"},"source":{"5359eb61":"from keras.datasets import mnist\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()","97745ea2":"train_labels.shape\n#60000 Images for Training Set and 10000 for Test Set","4e2d2be7":"test_labels.shape","4bd987ca":"train_images.shape\n#This because the images are 28x28 px each","83abfcbc":"test_images.shape","e009d213":"from keras import models\nfrom keras import layers","244dc8d5":"network = models.Sequential()\nnetwork.add(layers.Dense(512, activation=\"relu\", input_shape=(28*28,)))\nnetwork.add(layers.Dense(10, activation=\"softmax\"))","6f7cd5f9":"network.compile(optimizer=\"rmsprop\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])","80182541":"train_images = train_images.reshape((60000, 28 * 28))\ntrain_images = train_images.astype('float32') \/ 255","cf7e3be4":"test_images = test_images.reshape((10000, 28 * 28))\ntest_images = test_images.astype('float32') \/ 255","06026a98":"from keras.utils import to_categorical\ntrain_labels = to_categorical(train_labels)\ntest_labels = to_categorical(test_labels)","2da32346":"network.fit(train_images, train_labels, epochs=5, batch_size=128)","f8174de8":"test_loss, test_acc = network.evaluate(test_images, test_labels)\nprint('test_acc:', test_acc)","d2126024":"Project done following the \"Deep Learning with Python\" book by Fran\u00e7ois Chollet.","f6001741":"# Build the network","a27ef070":"Now, before training we have to preprocess our data into another format understandable by the network, once done we have to scale all the data in the range [0,1]. ","d759bb16":"Layers' aim is to extract representations of the data we fed into them. \nIn our case there are 2 Dense Layers (fully connected):\n- 10-way softmax (Similar to sigmoid function) : returns an array of 10 probability scores summing to 1.\n- 512-way Relu (Linear).","0fc0264e":"# Fit the model","83a7c106":"# Adding Loss Function, Optimizer and Metrics","3f6575a9":"# Load the Data","d6eb0317":"Data from Mnist dataset are encodes into Numpy arrays.","851962a9":"We're going to fit our Neural Network in 5 epochs.\nEach time the loss function decreases and the accuracy increases. \nWe will se that the accuracy is slightly lower in the test evaluation. This lead to a slightly overfitting."}}