{"cell_type":{"1c216b57":"code","65526d1a":"code","6af90565":"code","4126ae98":"code","e5dddfea":"code","c7a591f2":"code","d3cabdab":"code","db2cb86a":"code","a0e2ebaf":"code","325ee798":"code","36100d29":"code","61287f1e":"code","38fa0232":"code","b5150965":"code","25b803ab":"code","25ede1bf":"code","bdc3b75f":"code","5fb1366f":"code","c92145fb":"code","02ca320d":"code","0b1ba3ea":"code","46659370":"code","85962146":"code","1d5c6a7b":"code","8d03c6a2":"code","3b5ff65a":"code","74a6ddd7":"code","619d29a4":"code","df417404":"code","217e03ae":"code","477e7bd1":"code","840a9c59":"code","fc0d1cf5":"code","687cd7bb":"code","f6bcc2ff":"code","c059fb19":"code","fdf46349":"code","c11b1885":"code","8ac120ed":"code","48d1ed89":"code","127d11fe":"code","91929505":"code","83a4a2b1":"code","2b33000d":"markdown","8a494654":"markdown","276acc92":"markdown"},"source":{"1c216b57":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn as sk\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nmice = pd.read_csv('..\/input\/Data_Cortex_Nuclear.csv')\n\n# Any results you write to the current directory are saved as output.","65526d1a":"mice.head(8)","6af90565":"mice.describe()","4126ae98":"mice.isnull().sum()","e5dddfea":"# counting null values by row\nmice.isnull().sum(axis=1)","c7a591f2":"#Dropping and filling null values\nnmice = mice.dropna(how='any', thresh=75)\n\nnmice = nmice.fillna(nmice.mean())","d3cabdab":"nmice.isnull().sum(axis=1)","db2cb86a":"# checking if dropping the rows worked\nnmice.isnull().sum()","a0e2ebaf":"# looking for changes\n# everything increased a bit\nnmice.describe()","325ee798":"# Use a feature selection to find the important proteins for predicting the treatment, behavior and class\n\n# Assgning the data and target\nX = nmice.loc[:, 'DYRK1A_N':'CaNA_N']\ny = nmice['class']","36100d29":"# Splitting the train and testing data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)","61287f1e":"#Creating a random forest classifier\nclf = RandomForestClassifier(n_estimators=100, random_state=1, n_jobs=1)\n\n# Train the classifier\nclf.fit(X_train, y_train)\n\n# assigning the all importance values to a series\nimportance = pd.Series(clf.feature_importances_)\n\n# Print the name and gini importance of each feature\nfor feature in zip(nmice.loc[: ,'DYRK1A_N':'CaNA_N'], clf.feature_importances_):\n    print(feature)","38fa0232":"# Test the classifer to get the accuracy\ny_pred = clf.predict(X_test)\n\naccuracy_score(y_test, y_pred)\n","b5150965":"#Let's use a threshold to find the most important proteins\n\n# Create a selector object that will use the random forest classifier to identify\n# features that have an importance of more than 0.014\n# accuracy of limited feature model gets lower if I do lower than 0.014 or higher\nsfm = SelectFromModel(clf, threshold=0.014)\n\n# Train the selector\nsfm.fit(X_train, y_train)\n\n# making arrays to keep track of the important proteins' importance values and index\nimp = []\nimp_index = []\n\n# Print the names of the most important features\nfor feature_list_index in sfm.get_support(indices=True):\n    imp.append(list(nmice.loc[:, 'DYRK1A_N':'CaNA_N'])[feature_list_index])\n    print(list(nmice.loc[:, 'DYRK1A_N':'CaNA_N'])[feature_list_index])\n    imp_index.append(feature_list_index)\n\n# Making the list of important proteins so I can make a dataframe with it\ns_imp = pd.Series(imp)\n\n# Getting the importance values of only the important proteins\nprotein_importance = (importance)[imp_index]\n\n# Making a dataframe of the proteins with thei importance values\np = {'Important Proteins': imp, 'Importance Value': protein_importance}\ndf_importance = pd.DataFrame(p)","25b803ab":"# sorting the proteins by importance and printing it\ndf_importance.sort_values('Importance Value', ascending=False, inplace=True)\nprint(df_importance)","25ede1bf":"# ceating a new data set from the sfm model to test the accuracy of a limited feature model\nX_imp_train = sfm.transform(X_train)\nX_imp_test = sfm.transform(X_test)","bdc3b75f":"# creating a new random forest model classifier for the most important features\nclf_imp = RandomForestClassifier(n_estimators=100, random_state=1, n_jobs=1)\n\n# training the random forest classfier on the new limited feature data sets\nclf_imp.fit(X_imp_train, y_train)","5fb1366f":"# Test the classifer to get the accuracy\ny_imp_pred = clf_imp.predict(X_imp_test)\n\n# Getting the accuracy score of the limited classifier model\naccuracy_score(y_test, y_imp_pred)\n\n# There is almost no difference between the accuracies of the 2 models ","c92145fb":"# creating a data frame of the values of only the important proteins for plotting\nprotein_ex = nmice.loc[:, imp]\nprotein_ex\n\n# creating a dataframe with only the the types of mice\ndescription = nmice.loc[:, ['Genotype', 'Treatment','Behavior', 'class']]\ndescription\n\n# joining the two dataframes together\nprotein_exd = protein_ex.join(description)\nprotein_exd","02ca320d":"yg = protein_exd['Genotype']\n\n# Splitting the train and testing data\nyg_train, yg_test = train_test_split(yg, test_size=0.4, random_state=1)\n\n#making a new random forest classifier with a max_dept so I make a smaller visual of a tree\nclf_imptd = RandomForestClassifier(max_depth = 3, n_estimators=100, random_state=1, n_jobs=1)\n\n# training the random forest classfier on the new limited feature data sets\nclf_imptd.fit(X_imp_train, yg_train)","0b1ba3ea":"# Test the classifer to get the accuracy\nyg_imp_pred = clf_imptd.predict(X_imp_test)\n\n# Getting the accuracy score of the limited classifier model\naccuracy_score(yg_test, yg_imp_pred)","46659370":"#Visualizing the model\nfrom sklearn.tree import export_graphviz\n# Export as dot file\nexport_graphviz(clf_imptd.estimators_[5], out_file='tree.dot', \n                feature_names = imp,\n                class_names = ['Control', 'Trisomal'],\n                rounded = True, proportion = False, \n                precision = 2, filled = True)\n\n# Convert to png using system command (requires Graphviz)\nfrom subprocess import call\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n\n# Display in jupyter notebook\nfrom IPython.display import Image\nImage(filename = 'tree.png')","85962146":"# Creating the test and train data for the genotype\nprotein_exd['Genotype'] = protein_exd['Genotype'].map({'Control': 0, 'Ts65Dn': 1})\n\nyg = protein_exd['Genotype']\n\n# Splitting the train and testing data\nyg_train, yg_test = train_test_split(yg, test_size=0.4, random_state=1)\n\n# making a new random forest classifer model that uses only the top 3 important proteins for the visualization\nclf_impt = RandomForestClassifier(n_estimators=100, random_state=1, n_jobs=1)\n\n# training the random forest classfier on the new limited feature data sets\nclf_impt.fit(X_imp_train, yg_train)","1d5c6a7b":"# Test the classifer to get the accuracy\nyg_imp_pred = clf_impt.predict(X_imp_test)\n\n# Getting the accuracy score of the limited classifier model\naccuracy_score(yg_test, yg_imp_pred)","8d03c6a2":"#Making a ROC curve \n# Making a visual for the ROC curve for the logistic regression model\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n\nlogit_roc_auc = roc_auc_score(yg_test, clf_impt.predict(X_imp_test))\nfpr, tpr, thresholds = roc_curve(yg_test, clf_impt.predict_proba(X_imp_test)[:,1:26])\nplt.figure(figsize=(12,8))\nplt.plot(fpr, tpr, label='Random Forest Classifier (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate', fontsize=14)\nplt.ylabel('True Positive Rate', fontsize=14)\nplt.title('Receiver Operating Characteristic', fontsize=16)\nplt.legend(loc=\"lower right\", fontsize=14)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.style.use('seaborn')\nplt.show()","3b5ff65a":"# Making a dataframe of the accuracies\na = {'Limited': [0.974477958236659], 'Full': [0.974477958236659]}\naccuracies = pd.DataFrame(data=a)\naccuracies\n\n# making bar plot comparing the accuracies of the models\nax = accuracies.plot.bar(\n    figsize= (13, 10),\n    fontsize=15)\nax.set(ylabel = 'Accuracy')\nax.set(xlabel = 'Random Forest Models')\nax.set_xticklabels('', fontsize=14)\nplt.legend(fontsize=14)\nx_labels = ['A', 'B']\nxticks = [-0.13, 0.13]\nax.set_xticks(xticks)\nax.set_xticklabels(x_labels, rotation=0, fontsize=14)\nax.set_facecolor('xkcd:white')\nax.set_facecolor(('#ffffff'))\nax.spines['left'].set_color('black')\nax.spines['bottom'].set_color('black')","74a6ddd7":"# making violin plots of each protein for each group; treatment and control\nfig, axs = plt.subplots(1, 5, figsize=(60, 30))\n\nsns.set(font_scale=1.5)\n\nsns.set_style('whitegrid')\n\nsns.violinplot(\n    y='SOD1_N',\n    x='Genotype',\n    data=protein_exd,\n    palette='Set2',\n    ax=axs[0]\n    )\n\nsns.violinplot(\n    y='CaNA_N',\n    x='Genotype',\n    data=protein_exd,\n    palette='Set2',\n    ax=axs[1]\n    )\n\nsns.violinplot(\n    y='Ubiquitin_N',\n    x='Genotype',\n    data=protein_exd,\n    palette='Set2',\n    ax=axs[2]\n    )\n\nsns.violinplot(\n    y='APP_N',\n    x='Genotype',\n    data=protein_exd,\n    palette='Set2',\n    ax=axs[3]\n    )\n\nsns.violinplot(\n    y='pERK_N',\n    x='Genotype',\n    data=protein_exd,\n    palette='Set2',\n    ax=axs[4]\n    )","619d29a4":"# Making an instance of the model\nlr = LogisticRegression()\n\n# fitting the model to the training data\nlr.fit(X_imp_train, yg_train)\n\n# use the model to predict on the testing data\nlr.predict(X_imp_test)\n\n# Printing the accuracy of the model\nscore = lr.score(X_imp_test, yg_test)\nprint(score)","df417404":"# Making a visual for the ROC curve for the logistic regression model\nlogit_roc_auc = roc_auc_score(yg_test, lr.predict(X_imp_test))\nfpr, tpr, thresholds = roc_curve(yg_test, lr.predict_proba(X_imp_test)[:,1:26])\nplt.figure(figsize=(12, 8))\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate', fontsize=14)\nplt.ylabel('True Positive Rate', fontsize=14)\nplt.title('Receiver Operating Characteristic', fontsize=16)\nplt.legend(loc=\"lower right\", fontsize=14)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.style.use('seaborn')\nplt.show()","217e03ae":"# Making a violing plot of the top 5 important proteins by each class\nsns.set(rc={'figure.figsize':(20,8)})\nsns.set_style('ticks')\nsns.set(font_scale = 1.75)\ng = sns.violinplot(\n    y='SOD1_N',\n    x='class',\n    data=protein_exd\n    )\ng.set_facecolor('xkcd:white')\ng.spines['left'].set_color('black')\ng.spines['bottom'].set_color('black')","477e7bd1":"sns.set(rc={'figure.figsize':(20,8)})\nsns.set_style('ticks')\nsns.set(font_scale = 1.75)\ng = sns.violinplot(\n    y='pPKCG_N',\n    x='class',\n    data=protein_exd\n    )\ng.set_facecolor('xkcd:white')\ng.spines['left'].set_color('black')\ng.spines['bottom'].set_color('black')","840a9c59":"sns.set(rc={'figure.figsize':(20,8)})\nsns.set_style('ticks')\nsns.set(font_scale = 1.75)\ng = sns.violinplot(\n    y='CaNA_N',\n    x='class',\n    data=protein_exd\n    )\ng.set_facecolor('xkcd:white')\ng.spines['left'].set_color('black')\ng.spines['bottom'].set_color('black')","fc0d1cf5":"sns.set(rc={'figure.figsize':(20,8)})\nsns.set_style('ticks')\nsns.set(font_scale = 1.75)\ng = sns.violinplot(\n    y='Ubiquitin_N',\n    x='class',\n    data=protein_exd\n    )\ng.set_facecolor('xkcd:white')\ng.spines['left'].set_color('black')\ng.spines['bottom'].set_color('black')","687cd7bb":"sns.set(rc={'figure.figsize':(20,8)})\nsns.set_style('ticks')\ng = sns.violinplot(\n    y='DYRK1A_N',\n    x='class',\n    data=protein_exd\n    )","f6bcc2ff":"# Applying a logistic regression model to predict the classes\n\n# Making an instance of the model\nlr = LogisticRegression()\n\n# fitting the model to the training data\nlr.fit(X_imp_train, y_train)\n\n# use the model to predict on the testing data\nlr.predict(X_imp_test)\n\n# Printing the accuracy of the model\nscore = lr.score(X_imp_test, y_test)\nprint(score)","c059fb19":"# importing packages\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# creating an instance of the kNN model\n# n_jobs=-1 makes it so that computations run in parallel\nkmodel = KNeighborsClassifier(n_jobs=-1)\n\n# setting the parameters I want to test\nparams = {'n_neighbors':[1,2,3,4,5,6,7,8,9,10],\n          'leaf_size':[1,2,3,5],\n          'weights':['uniform', 'distance'],\n          'algorithm':['auto', 'ball_tree','kd_tree','brute'],\n          'n_jobs':[-1]}\n\n# creating a grid search with the parameters I chose\ngrid = GridSearchCV(kmodel, param_grid=params, n_jobs=1, scoring='accuracy')\n\n# fitting the model\ngrid.fit(X_imp_train, y_train)\n\n#print the best combination of parameters\nprint(\"Best Hyper Parameters:\\n\",grid.best_params_)","fdf46349":"#getting the accuracy of the kNN model\npred = grid.predict(X_imp_test)\n\nprint('Accuracy:', accuracy_score(pred, y_test))","c11b1885":"#Making a new model to predict genotypes to make an ROC curve\nkgmodel = KNeighborsClassifier(n_jobs=-1)\nkgmodel.fit(X_imp_train, yg_train)\n\n#Getting the accuracy\npred_gen = kgmodel.predict(X_imp_test)\n\nprint('Accuracy:', accuracy_score(pred_gen, yg_test))","8ac120ed":"# Making a visual for the ROC curve for the KNN model\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n\nlogit_roc_auc = roc_auc_score(yg_test, kgmodel.predict(X_imp_test))\nfpr, tpr, thresholds = roc_curve(yg_test, kgmodel.predict_proba(X_imp_test)[:,1:26])\nplt.figure(figsize=(12, 8))\nplt.plot(fpr, tpr, label='K-Nearest Neighbours (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlabel('False Positive Rate', fontsize=14)\nplt.ylabel('True Positive Rate', fontsize=14)\nplt.title('Receiver Operating Characteristic', fontsize=16)\nplt.legend(loc=\"lower right\", fontsize=14)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.style.use('seaborn')\nplt.show()","48d1ed89":"#do subplots with correlations between top 5 proteins and mice descriptions\nfrom pylab import rcParams\n\nsns.set(font_scale=1.3)\nrcParams['figure.figsize'] = (20, 20)\ncols = ['SOD1_N', 'Ubiquitin_N', 'CaNA_N']\nx = protein_exd[['SOD1_N', 'Ubiquitin_N', 'CaNA_N']]\nsns_plot = sns.pairplot(x[cols])","127d11fe":"# making a heatmap of the correlation\nsns.set(font_scale=3)\ndef plot_corr( df ):\n    corr = df.corr()\n    _, ax=plt.subplots( figsize=(50,25) )\n    cmap = sns.diverging_palette( 240 , 10 , as_cmap = True)\n    _ = sns.heatmap(corr,cmap=cmap,square=True, cbar_kws = {'shrink': .9}, ax=ax, annot=False)\n    \nplot_corr(protein_exd)","91929505":"import matplotlib.font_manager\nmatplotlib.font_manager.findSystemFonts(fontpaths=None, fontext='ttf')\n\n# Making a dataframe of the accuracies\na = {'Random Forest Classifier': [0.974477958236659], 'K-Nearest Neighbours': [0.974477958236659], 'Logistic Regression': [0.740139211136891]}\naccuracies = pd.DataFrame(data=a)\n#accuracies.rename(index={0:'Random Forest Classifier',1:'K-Nearest Neighbours', 2:'Logistic Regression'}, \n#                 inplace=True)\n\n# making bar plot comparing the accuracies of the models\nsns.set(font_scale=1)\nax = accuracies.plot.bar(\n    figsize= (13, 10),\n    fontsize=14)\nplt.xticks(rotation=0, fontsize=14)\nplt.xlabel('Models', fontsize=14)\nplt.ylabel('Accuracy', fontsize=14)\nx_labels = ['A', 'B', 'C']\nxticks = [-0.17, 0,0.165]\nax.set_xticks(xticks)\nax.set_xticklabels(x_labels, rotation=0)\naxbox = ax.get_position()\nplt.legend(loc = (axbox.x0 + 0.65, axbox.y0 + 0.70), fontsize=14)\nplt.title(' ')\nax.set_facecolor('xkcd:white')\nax.set_facecolor(('#ffffff'))\nax.spines['left'].set_color('black')\nax.spines['bottom'].set_color('black')","83a4a2b1":"#Making a bar plot comparing all the accuracies of the models that predict genotype\n\n# Making a dataframe of the accuracies\na = {'Random Forest Classifier': [0.9721577726218097], 'K-Nearest Neighbours': [0.9164733178654292], 'Logistic Regression': [0.8167053364269141]}\naccuracies = pd.DataFrame(data=a)\n#accuracies.rename(index={0:'Random Forest Classifier',1:'K-Nearest Neighbours', 2:'Logistic Regression'}, \n#                 inplace=True)\n\n# making bar plot comparing the accuracies of the models\nax = accuracies.plot.bar(\n    figsize= (13, 10),\n    fontsize=14)\nplt.xticks(rotation=0, fontsize=14)\nplt.xlabel('Models', fontsize=14)\nplt.ylabel('Accuracy', fontsize=14)\nx_labels = ['A', 'B', 'C']\nxticks = [-0.17, 0,0.165]\nax.set_xticks(xticks)\nax.set_xticklabels(x_labels, rotation=0)\naxbox = ax.get_position()\nplt.legend(loc = (axbox.x0 + 0.65, axbox.y0 + 0.70), fontsize=14)\n# plt.title('The Accuracies of the Models that Predict Mice Genotype', fontsize=13.8, y=0.96, x=0.81)\nplt.title(\" \")\nax.set_facecolor('xkcd:white')\nax.set_facecolor(('#ffffff'))\nax.spines['left'].set_color('black')\nax.spines['bottom'].set_color('black')","2b33000d":"#plotting the logistic curve for the most important protein\nprotein_exd['Genotype'] = protein_exd['Genotype'].map({'Ts65Dn': 1, 'Control': 0})\n\ntry:\n    sns.regplot(x='SOD1_N', y='Genotype', data=protein_exd, logistic=True)\nexcept ValueError:\n    pass","8a494654":"# Creating a dataframe of only the protein expression in the control group\nprotein_control = protein_exd.loc[protein_exd['Genotype'] == 'Control']\nprotein_control\n# protein_ex\n\n# Creating a dataframe of only the protein expression in the treatment group\nprotein_treat = protein_exd.loc[protein_exd['Genotype'] == 'Ts65Dn']\n# protein_treat\n\nX_imp = protein_exd.loc[:, 'DYRK1A_N':'CaNA_N']","276acc92":"#Visualizing the model\nfrom sklearn.tree import export_graphviz\n# Export as dot file\nexport_graphviz(clf_imp.estimators_[5], out_file='tree.dot', \n                feature_names = imp,\n                class_names = protein_exd['class'],\n                rounded = True, proportion = False, \n                precision = 2, filled = True)\n\n# Convert to png using system command (requires Graphviz)\nfrom subprocess import call\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n\n# Display in jupyter notebook\nfrom IPython.display import Image\nImage(filename = 'tree.png')"}}