{"cell_type":{"c048c97d":"code","f7274bf2":"code","1d42edba":"code","3f6ab1ab":"code","961ac856":"code","a97a6f78":"code","1805962b":"code","e3adb3f1":"code","a9ed81f2":"code","16fc61d3":"code","60d67f7b":"code","d40c6699":"code","c01451a8":"code","e1fe7c93":"code","3ddc879d":"code","01e5647a":"code","8d251198":"code","285f1191":"code","7a12f9f1":"code","90357543":"code","0409f3f0":"code","6edb52ec":"code","9dd09f98":"code","ed45990d":"code","9c86e325":"code","f5491cd3":"code","ffc61067":"code","f97b7ed2":"code","c9458243":"code","c8ab8c76":"code","5f5a6976":"code","381ca7e9":"code","946b0d16":"code","3772df72":"code","2a307aca":"code","5f514d4a":"code","52453128":"code","3892e385":"code","14d9a35c":"code","c9365f44":"code","bca8a43f":"code","666bf1db":"code","325d21e3":"code","ecb5dcc8":"code","0f8d9a7a":"code","ae3b6435":"code","b4f2e390":"code","23367085":"code","0a093faa":"code","e594ea66":"code","616d498f":"code","d3d6951b":"code","293f23f2":"code","bc408853":"code","17ce19f9":"code","ccb5b0b0":"code","bf351dfc":"code","c7640ca9":"markdown","bb3bc522":"markdown","666e0b6a":"markdown","3a7d5600":"markdown","61515e13":"markdown","399941a6":"markdown","5bb6c72e":"markdown","c6e40bd9":"markdown","e1f5651f":"markdown","ea566803":"markdown","6972299b":"markdown","9c03e7bd":"markdown","12b9f27e":"markdown","4752d7ba":"markdown","35b084c0":"markdown","fbc54b88":"markdown","1fe267bc":"markdown","59013456":"markdown","07e3c3dd":"markdown","dda6833a":"markdown","8c2ce491":"markdown","e6b33056":"markdown","c9b2003f":"markdown","03f72abe":"markdown","58687e09":"markdown","d1cb9beb":"markdown"},"source":{"c048c97d":"import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport seaborn as sns\nimport graphviz\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn.feature_selection import VarianceThreshold,RFECV\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict,RepeatedStratifiedKFold,StratifiedKFold\nfrom imblearn.over_sampling import SMOTE\n\nstartup_df = pd.read_csv(\"..\/input\/startup-investments-crunchbase\/investments_VC.csv\", delimiter=',', header = 0, encoding = \"ISO-8859-1\",skipinitialspace=True)\nstartup_df = startup_df.rename(columns={'market ': 'market', 'funding_total_usd ': 'funding_total_usd'})\n","f7274bf2":"startup_df.info()","1d42edba":"#Based on the .info() function, we can identify that some data variables need to be converted into a date type.\nformat_date = ('founded_at','first_funding_at','last_funding_at')\nfor i in format_date:\n  startup_df[i] = pd.to_datetime(startup_df[i], format = '%Y-%m-%d', errors = 'coerce')","3f6ab1ab":"startup_df.info()","961ac856":"startup_df.describe()","a97a6f78":"startup_df.head(1)","1805962b":"#To reduce the complexity, while processing the date variable fields, we convert the actual dates to \n#relative dates, describing the distance from founded to first and last funding and then drop the founded date.\n\nstartup_df['age_first_funding'] = (startup_df['first_funding_at']-startup_df['founded_at'])\/pd.Timedelta(days=365)\nstartup_df['age_last_funding'] = (startup_df['last_funding_at']-startup_df['founded_at'])\/pd.Timedelta(days=365)\nstartup_df['founded_month'] = pd.DatetimeIndex(startup_df['founded_at']).month\nstartup_df['founded_quarter'] = pd.DatetimeIndex(startup_df['founded_at']).quarter\n","e3adb3f1":"startup_df.isnull().sum()","a9ed81f2":"#As the dataset contains quite many null-values, we try to reduce the amount null-values by\n#deleting the rows with a few central variables containing null\nstartup_df = startup_df.drop(startup_df[\n                                        (\n                                        (startup_df['status'].isna())|\n                                        (startup_df['founded_month'].isna())|\n                                        (startup_df['founded_year'].isna())|\n                                        (startup_df['market'].isna())|\n                                        (startup_df['country_code'].isna())|\n                                        (startup_df['funding_total_usd'].isna())|\n                                        (startup_df['age_first_funding'].isna())\n                                        )\n                                        ].index)\n#With a general removal of all observations containing null-values, we have 28290 observations remaining\nstartup_df.shape","16fc61d3":"#Based on the cleaning above, we would now like to see, \n#how many observations for each column that contains null-values.\n\nstartup_df.isnull().sum()","60d67f7b":"#Excluding irrelevant variables, assumed cannot be used to predict the success of a start-up\n#Funding_total_USD is excluded, as this is an aggregation of angel and seed\n\n#We, however keep the name of the startup to correctly distinct between them the futher analysis\nstartup_df = startup_df.drop(\n    [\n     'permalink'\n#     ,'name'\n     ,'homepage_url'\n     ,'state_code'\n     ,'region'\n     ,'city'\n     ,'founded_at'\n     ,'first_funding_at'\n     ,'funding_total_usd'\n     ,'last_funding_at'\n     ,'category_list'\n     ], axis = 1)","d40c6699":"#Further, we can check that we do not have any duplicates in our dataset\nstartup_df = startup_df.drop_duplicates()\nstartup_df.shape","c01451a8":"print('Before')\ndisplay(startup_df.groupby('status').agg({'country_code':'count'}).sort_values(by=['status'], ascending = False))\nStatusDict = {\"closed\":0,\"acquired\":1}\n\nstartup_df = startup_df[(startup_df['status'] == 'acquired') | (startup_df['status'] == 'closed')]\nstartup_df[\"status_binary\"] = startup_df[\"status\"].map(StatusDict)\nstartup_df = startup_df.drop(['status'], axis = 1)\n\nprint('After')\ndisplay(startup_df.groupby('status_binary').agg({'country_code':'count'}).sort_values(by=['status_binary'], ascending = True))\n","e1fe7c93":"#Hence, the format of funding_total_USD is distorted, we are aggregating a new attribute \n\nstartup_df['total_investment'] = startup_df['seed'] + startup_df['venture'] +startup_df['equity_crowdfunding'] + startup_df['undisclosed'] + startup_df['convertible_note'] + startup_df['debt_financing'] + startup_df['angel'] + startup_df['grant'] + startup_df['private_equity'] + startup_df['post_ipo_equity'] + startup_df['post_ipo_debt'] + startup_df['secondary_market'] + startup_df['product_crowdfunding']\n\nstartup_df['total_investment'].describe()\n\n","3ddc879d":"max(startup_df['total_investment'])","01e5647a":"#Exporting data to .CSV in order to explore data in Tableau\n#startup_df.to_csv(r'startup_df_v1.csv', index = False, sep=';',mode='w')\n#files.download('startup_df_v1.csv')","8d251198":"startup_df.shape","285f1191":"#Removing outliers via IQR: Interquartile range, also called midspread.\n\nQ1 = startup_df['total_investment'].quantile(0.25)\nQ3 = startup_df['total_investment'].quantile(0.75)\nIQR = Q3 - Q1\n\nfund_lower = (Q1 - 1.5 * IQR)\nfund_upper = (Q3 + 1.5 * IQR)\n\nstartup_df = startup_df[\n                        (startup_df['total_investment'] >= fund_lower ) \n                        & (startup_df['total_investment'] <= fund_upper)\n                        ]\n\nprint('The following code remove all datapoints below: {} and above {}'.format(fund_lower, fund_upper))\nprint('This results in the following data model: {}'.format(startup_df.shape))","7a12f9f1":"top20_markets = startup_df['market'].value_counts()[:20].keys().tolist()\nstartup_df['market'] = startup_df['market'].apply(lambda i: i if i in top20_markets else 'Other')","90357543":"#It looks like, some startups have recived funding prior to the date of establishment.\n\n#startup_df['age_first_funding'] = startup_df['age_first_funding'].clip(lower=0, upper=None)\n#startup_df['age_last_funding'] = startup_df['age_last_funding'].clip(lower=0, upper=None)\n\n\nstartup_df.loc[startup_df['age_first_funding'] < 0, 'age_first_funding'] = 0\nstartup_df.loc[startup_df['age_last_funding'] < 0, 'age_first_funding'] = 0","0409f3f0":"#The majority of the startups are located after 1994\n#To further reduce the amount of outliers, we are excluding all startups before 1995.\n\nstartup_df = startup_df[(startup_df['founded_year'] >= 1995.0 )]","6edb52ec":"startup_df = startup_df.drop(\n    [\n     'country_code'\n     ,'name'\n     ], axis = 1)","9dd09f98":"plt.figure(figsize=(30,30))\nstartup_heat = startup_df.corr()\n\nstartup_heat= sns.heatmap(startup_heat, annot=True,linewidth = 0.5, cmap='coolwarm', vmin=-1, vmax=1)\n\nbottom, top = startup_heat.get_ylim()\nstartup_heat.set_ylim(bottom, top)\n\nplt.show()","ed45990d":"#Hence, the correlation of a few different attributes are blank\n#we are further excluding these from the model\nstartup_df = startup_df.drop(\n    [\n     'post_ipo_equity'\n     ,'post_ipo_debt'\n     ,'round_G'\n     ,'round_H'\n     ,'founded_year'\n     ,'founded_quarter'\n     ,'total_investment'\n     ,'age_last_funding'\n     ], axis = 1)\n\nplt.figure(figsize=(30,30))\nstartup_heat = startup_df.corr()\n\nstartup_heat= sns.heatmap(startup_heat, annot=True,linewidth = 0.5, cmap='coolwarm', vmin=-1, vmax=1)\n\nbottom, top = startup_heat.get_ylim()\nstartup_heat.set_ylim(bottom, top)\n\nplt.show()","9c86e325":"#Exporting data to .CSV in order to explore data in Tableau\n#startup_df.to_csv(r'startup_df_v2.csv', index = False, sep=';',mode='w')\n#files.download('startup_df_v2.csv')","f5491cd3":"#The initial process of the modelling phase is to create dummy variables\n#and seperating the dataset into X and y\n\nmodelling_df = startup_df\nX = pd.get_dummies(modelling_df.drop(['status_binary'],axis = 1))\ny = modelling_df['status_binary']","ffc61067":"#Before modelling our data, we are creating a new dataframe, to reduce the complexity\n#Further, we are the data into train and test identify potential overfitting patterns\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state = 10)","f97b7ed2":"#Testing the impact of increasing desicion trees used to predict y.\nn = 10\nwhile n <= 200:\n  model = RandomForestClassifier(n_estimators=n)\n  model.fit(X_train, y_train)\n  print('{} - train score: {:.3f} | test score: {:.3f}'.format(n,model.score(X_train,y_train),model.score(X_test,y_test)))\n  n = n+10","c9458243":"model = RandomForestClassifier(n_estimators=110) \nmodel.fit(X_train, y_train)\n\n#ROC CURVE - Plotting the true positives against the false positives and an AUC score\ny_pred_proba = model.predict_proba(X_test) [:,1]\nfpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.legend(loc=4)\nplt.show()","c8ab8c76":"#To get an overview of the feature importance we can utilize from the \n#The feature_importances_ function was not used, because of bias when dealing with a high cardinality (according to the SKlearn documentation)\nresult = permutation_importance(model, X_train, y_train, n_repeats=10,random_state=0)\nfimp = pd.Series(result.importances_mean,index=X_train.columns.values).sort_values(ascending=False)\nfimp","5f5a6976":"#Based on the model accuracy from above, we can try to tune the performance by\n#evaluating the accuracy by random selecting a number of decriptive attributes\n#Further, we evaluate the model by the isolated attribute importance and then dropping insignificant attributes.\n\nrfecv = RFECV(estimator=model, step=1,cv=StratifiedKFold(10),scoring='accuracy')\nrfecv.fit(X_train,y_train)\n\nplt.figure(figsize=(9,5))\nplt.plot(range(1, len(rfecv.grid_scores_)+1),rfecv.grid_scores_,linewidth=3)\nplt.title('The correlation between the number of attributes and the accuracy of the model')\nplt.show()\n\n","381ca7e9":"X_train.columns[np.where(rfecv.support_ == False)[0]]","946b0d16":"#Based on the plot above, we are dropping all columns not categorized as supportive to our model\nX.drop(X_train.columns[np.where(rfecv.support_ == False)[0]],axis=1, inplace = True)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\nmodel.fit(X_train,y_train)","3772df72":"result = permutation_importance(model, X_train, y_train, n_repeats=10,random_state=0)\nfimp = pd.Series(result.importances_mean,index=X_train.columns.values).sort_values(ascending=False)\nfimp","2a307aca":"#Plotting feature importance\nsns.barplot(x=fimp, y=fimp.index, color='b')\nplt.rcParams['figure.figsize'] = 6,15\nplt.xlabel('Importance score')\nplt.ylabel('Attribute')\nplt.title('Attribute importance')\n\nplt.show()","5f514d4a":"#Plotting another ROC curve to benchmark the initial model against the tuned version.\ny_pred_proba = model.predict_proba(X_test) [:,1]\nfpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.legend(loc=4)\nplt.show()","52453128":"#Further, we create a new model, embedded with a cross-validation to evaluate the performance of the model across 10 folds repeating 5 times\ncross_val = RepeatedStratifiedKFold(n_splits=10, n_repeats=5, random_state=1)\nscores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cross_val, n_jobs=-1, error_score='raise')\n# report performance\nprint('Average score of 5 run with a stratified 10 Kfold is {:.3f} with a standard deviation of: {:.3f}'.format(np.mean(scores)*100, np.std(scores)))","3892e385":"#Finally, we can try to use our model to predict a the future state of a startup in isolation\n#To do this, we can make use of to predict function, but first we need to create a new dataframe with our target\n\npred = X_test.sample(n=1)\noutcome = model.predict(pred)\nprint('Predicted Class: {}'.format(outcome[0]))\n\nif outcome == 1:\n  print('This startup is predicted to be successful by {:.1f}%'.format(np.mean(scores)*100)) \n  display(pred)\nelif outcome == 0:\n  print('This startup is predicted NOT to be successful by {:.1f}%'.format(np.mean(scores)*100)) \n  display(pred)\n#To use this in a real life setting, one could benefit from this by replacing 'pred' with another dataframe\n#containing a startup with that has the potential of being a successfull startup and hereby also needed to be investigated","14d9a35c":"#To evaluate the true performance of a logistic regression, we are once again including all attributes\nX = pd.get_dummies(modelling_df.drop(['status_binary'],axis = 1))\ny = modelling_df['status_binary']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n\nlogreg = LogisticRegressionCV(max_iter=10000, scoring='roc_auc')\nlogreg.fit(X_train,y_train)\n\nprint('The training model accuracy: {:.4}'.format(logreg.score(X_train,y_train)))\nprint('The test model accuracy: {:.4}'.format(logreg.score(X_test,y_test)))","c9365f44":"#ROC CURVE - Plotting the true positives against the false positives\ny_pred_proba = logreg.predict_proba(X_test)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.legend(loc=4)\nplt.show()","bca8a43f":"#Trying to balance the classes in order to achieve a better result\nlogreg = LogisticRegressionCV(max_iter=10000,scoring='roc_auc',class_weight='balanced')\nlogreg.fit(X_train,y_train)\n\nprint('The training model accuracy: {:.4}'.format(logreg.score(X_train,y_train)))\nprint('The test model accuracy: {:.4}'.format(logreg.score(X_test,y_test)))","666bf1db":"#Fitting the model with both a balanced and the liblinear as the solver\n#**Liblinear is described in docu as a solid choice for small datasets**\nlogreg = LogisticRegressionCV(penalty='l1', solver='liblinear',scoring='roc_auc',class_weight='balanced', max_iter=100000)\nlogreg.fit(X_train,y_train)\n\nprint('The training model accuracy: {:.4}'.format(logreg.score(X_train,y_train)))\nprint('The test model accuracy: {:.4}'.format(logreg.score(X_test,y_test)))","325d21e3":"#Fitting the model with both a balanced and \n#from sklearn.metrics import mean_squared_error\nlogreg = LogisticRegressionCV(penalty='l1', solver='liblinear', max_iter=10000,scoring='roc_auc')\nlogreg.fit(X_train,y_train)\n\nprint('The training model accuracy: {:.4}'.format(logreg.score(X_train,y_train)))\nprint('The test model accuracy: {:.4}'.format(logreg.score(X_test,y_test)))\n\n#print('Mean square error: {:.4}'.format(mean_squared_error(y_test, y_train)))\n#print('Root mean square error: {:.4}'.format(mean_squared_error(y_test, y_train)))","ecb5dcc8":"#ROC CURVE - Plotting the true positives against the false positives\ny_pred_proba = logreg.predict_proba(X_test)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.legend(loc=4)\nplt.show()","0f8d9a7a":"modelling_df.groupby('status_binary').agg({'status_binary':'count'})","ae3b6435":"modelling_df = startup_df\nX = pd.get_dummies(modelling_df.drop(['status_binary'],axis = 1))\ny = modelling_df['status_binary']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state = 0)\n\n","b4f2e390":"oversample = SMOTE()\n\nos_X,os_y =oversample.fit_resample(X_train, y_train)\nos_X = pd.DataFrame(data= os_X,columns = col )\nos_y = pd.DataFrame(data= os_y)","23367085":"os_y","0a093faa":"#Then we can evaluate the SMOTE function, to understand how we have extended our data\n\nprint(\"length of undersampled data is \",len(os_X))\nprint(\"Number of closed startups in undersampled data\",len(os_y[os_y['status_binary']==0]))\nprint(\"Number of succesfull startups\",len(os_y[os_y['status_binary']==1]))\nprint(\"Proportion of closed startups in undersampled data is {}%\".format(len(os_y[os_y['status_binary']==0])\/len(os_X)*100))\nprint(\"Proportion of succesfull startups in undersampled data is {}%\".format(len(os_y[os_y['status_binary']==1])\/len(os_X)*100))","e594ea66":"X = os_X\ny = os_y\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state = 0)","616d498f":"model = RandomForestClassifier(n_estimators=110) \nmodel.fit(X_train, y_train)\n\n#ROC CURVE - Plotting the true positives against the false positives and an AUC score\ny_pred_proba = model.predict_proba(X_test) [:,1]\nfpr, tpr, _ = metrics.roc_curve(y_test, y_pred_proba)\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.legend(loc=4)\nplt.show()","d3d6951b":"logreg = LogisticRegressionCV(penalty='l1', solver='liblinear', max_iter=10000, scoring='roc_auc')\nlogreg.fit(os_X,os_y)\n\nprint('The training model accuracy (balanced): {:.4} %'.format(logreg.score(os_X,os_y)*100))\nprint('The test model accuracy: {:.4}'.format(logreg.score(X_test,y_test)*100))","293f23f2":"#ROC CURVE - Plotting the true positives against the false positives\ny_pred_proba = logreg.predict_proba(X_test)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.legend(loc=4)\nplt.show()","bc408853":"#Finally, we can try to use our model to predict a the future state of a startup in isolation\n#To do this, we can make use of to predict function, but first we need to create a new dataframe with our target\n\n#Training the model once again with the balanced data from SMOTE\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state = 0)\n\nmodel.fit(X_train, y_train)\npred = X_test.sample(n=1)\noutcome = model.predict(pred)\n\ny_pred_proba = model.predict_proba(X_test) [:,1]\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\n\nprint('Predicted Class: {}'.format(outcome[0]))\n\nif outcome == 1:\n  print('This startup is predicted to be successful by {:.1f}%'.format(auc*100)) \n  display(pred)\nelif outcome == 0:\n  print('This startup is predicted NOT to be successful by {:.1f}%'.format(auc*100)) \n  display(pred)\n\n#To use this in a real life setting, one could benefit from this by replacing 'pred' with another user based dataframe\n#containing a startup with that has the potential of being a successfull startup and hereby also needed to be investigated\n#This is shown below","17ce19f9":"prod_dict = {'funding_rounds':              [2.0]\n            ,'founded_month':               [1.0]\n            ,'seed':                        [0.0]\n            ,'venture':                     [15000000.0]\n            ,'equity_crowdfunding':         [0.0]\n            ,'undiscolsed':                 [0.0]\n            ,'convertible_note':            [0.0]\n            ,'debt_financing':              [0.0]\n            ,'angel':                       [0.0]\n            ,'grant':                       [0.0]\n            ,'private equity':              [0.0]\n            ,'secondary_market':            [0.0]\n            ,'product_crowdfunding':        [0.0]\n            ,'round_A':                     [5000000.0]\n            ,'round_B':                     [10000000.0]\n            ,'round_C':                     [0.0]\n            ,'round_D':                     [0.0]\n            ,'round_E':                     [0.0]\n            ,'round_F':                     [0.0]\n            ,'age_first_funding':           [1.0]\n            ,'market_Advertising':          [0.0]\n            ,'market_Analytics ':           [0.0]\n            ,'market_Biotechnology ':       [0.0]\n            ,'market_Clean Technology ':    [0.0]\n            ,'market_Curated Web ':         [0.0]\n            ,'market_E-Commerce ':          [0.0]\n            ,'market_Education':            [0.0]\n            ,'market_Enterprise Software ': [0.0]\n            ,'market_Finance ':             [0.0]\n            ,'market_Games ':               [0.0]\n            ,'market_Hardware + Software ': [0.0]\n            ,'market_Health care ':         [0.0] \n            ,'market_Messaging ':           [0.0]\n            ,'market_Mobile ':              [0.0]\n            ,'market_Other':                [0.0]\n            ,'market_Search ':              [0.0]\n            ,'market_Security':             [0.0]\n            ,'market_Semiconductors ':      [0.0]\n            ,'market_Social Media ':        [0.0]\n            ,'market_Software ':            [1.0]\n            ,'market_Web Hosting ':         [0.0]\n            }\nprod_pred = pd.DataFrame(data=prod_dict)\noutcome = model.predict(prod_pred)\n\nprint('Predicted Class: {}'.format(outcome[0]))\n\nif outcome == 1:\n  print('This startup is predicted to be successful by {:.1f}%'.format(auc*100)) \nelif outcome == 0:\n  print('This startup is predicted NOT to be successful by {:.1f}%'.format(auc*100)) \ndisplay(prod_pred)","ccb5b0b0":"prod_dict = {'funding_rounds':              [6.0]\n            ,'founded_month':               [1.0]\n            ,'seed':                        [1500000.0]\n            ,'venture':                     [178450000.0]\n            ,'equity_crowdfunding':         [0.0]\n            ,'undiscolsed':                 [0.0]\n            ,'convertible_note':            [0.0]\n            ,'debt_financing':              [0.0]\n            ,'angel':                       [0.0]\n            ,'grant':                       [0.0]\n            ,'private equity':              [0.0]\n            ,'secondary_market':            [0.0]\n            ,'product_crowdfunding':        [0.0]\n            ,'round_A':                     [5000000.0]\n            ,'round_B':                     [10700000.0]\n            ,'round_C':                     [42750000.0]\n            ,'round_D':                     [120000000.0]\n            ,'round_E':                     [0.0]\n            ,'round_F':                     [0.0]\n            ,'age_first_funding':           [0.0]\n            ,'market_Advertising':          [0.0]\n            ,'market_Analytics ':           [0.0]\n            ,'market_Biotechnology ':       [0.0]\n            ,'market_Clean Technology ':    [0.0]\n            ,'market_Curated Web ':         [0.0]\n            ,'market_E-Commerce ':          [0.0]\n            ,'market_Education':            [0.0]\n            ,'market_Enterprise Software ': [1.0]\n            ,'market_Finance ':             [0.0]\n            ,'market_Games ':               [0.0]\n            ,'market_Hardware + Software ': [0.0]\n            ,'market_Health care ':         [0.0] \n            ,'market_Messaging ':           [0.0]\n            ,'market_Mobile ':              [0.0]\n            ,'market_Other':                [0.0]\n            ,'market_Search ':              [0.0]\n            ,'market_Security':             [0.0]\n            ,'market_Semiconductors ':      [0.0]\n            ,'market_Social Media ':        [0.0]\n            ,'market_Software ':            [0.0]\n            ,'market_Web Hosting ':         [0.0]\n            }\nprod_pred = pd.DataFrame(data=prod_dict)\noutcome = model.predict(prod_pred)\n\nprint('Predicted Class: {}'.format(outcome[0]))\n\nif outcome == 1:\n  print('This startup is predicted to be successful by {:.1f}%'.format(auc*100)) \nelif outcome == 0:\n  print('This startup is predicted NOT to be successful by {:.1f}%'.format(auc*100)) \ndisplay(prod_pred)","bf351dfc":"prod_dict = {'funding_rounds':              [11.0]\n            ,'founded_month':               [1.0]\n            ,'seed':                        [0.0]\n            ,'venture':                     [188799995.0]\n            ,'equity_crowdfunding':         [0.0]\n            ,'undiscolsed':                 [0.0]\n            ,'convertible_note':            [0.0]\n            ,'debt_financing':              [93000000.0]\n            ,'angel':                       [0.0]\n            ,'grant':                       [0.0]\n            ,'private equity':              [237000000.0]\n            ,'secondary_market':            [0.0]\n            ,'product_crowdfunding':        [0.0]\n            ,'round_A':                     [5000000.0]\n            ,'round_B':                     [70000000.0]\n            ,'round_C':                     [0.0]\n            ,'round_D':                     [0.0]\n            ,'round_E':                     [63999995.0]\n            ,'round_F':                     [0.0]\n            ,'age_first_funding':           [7.95]\n            ,'market_Advertising':          [0.0]\n            ,'market_Analytics ':           [0.0]\n            ,'market_Biotechnology ':       [0.0]\n            ,'market_Clean Technology ':    [0.0]\n            ,'market_Curated Web ':         [0.0]\n            ,'market_E-Commerce ':          [0.0]\n            ,'market_Education':            [0.0]\n            ,'market_Enterprise Software ': [0.0]\n            ,'market_Finance ':             [0.0]\n            ,'market_Games ':               [0.0]\n            ,'market_Hardware + Software ': [0.0]\n            ,'market_Health care ':         [0.0] \n            ,'market_Messaging ':           [0.0]\n            ,'market_Mobile ':              [0.0]\n            ,'market_Other':                [1.0]\n            ,'market_Search ':              [0.0]\n            ,'market_Security':             [0.0]\n            ,'market_Semiconductors ':      [0.0]\n            ,'market_Social Media ':        [0.0]\n            ,'market_Software ':            [0.0]\n            ,'market_Web Hosting ':         [0.0]\n            }\nprod_pred = pd.DataFrame(data=prod_dict)\noutcome = model.predict(prod_pred)\n\nprint('Predicted Class: {}'.format(outcome[0]))\n\nif outcome == 1:\n  print('This startup is predicted to be successful by {:.1f}%'.format(auc*100)) \nelif outcome == 0:\n  print('This startup is predicted NOT to be successful by {:.1f}%'.format(auc*100)) \ndisplay(prod_pred)","c7640ca9":"# Predicting successfull startups\n","bb3bc522":"Jawbone - a consumer hardware producer, that shut down their operations and currently undergoing a liquidation because of several lawsuits from creditors","666e0b6a":"#### 3.2 Markets\n","3a7d5600":"#### 3.3 Age when funded","61515e13":"##### 4.1.1 Model tuning","399941a6":"#### 4.2 Logistic regression\n\n\n","5bb6c72e":"###**1.General overview**","c6e40bd9":"##### 4.2.1 Model tuning","e1f5651f":"###**Import of data and relevant libaries**","ea566803":"\n> Hence, we did not recived great success while tuning the random forrest model from chapter 3, we leave the model by it's original state and do not exclude attributes by performing a RFECV (Recursive Feature Elimination.Cross-alidation)\n\n\n","6972299b":"###**5. Deployment**\nThe deployment of a random forrest model to classify successfull startups.\n","9c03e7bd":"##### 5.2 Logistic regression w. balanced data","12b9f27e":"###**3. Data Preparation**\nPreparing for modelling\n","4752d7ba":"###**2.Initial Data Preparation**\nPreparing for exploration","35b084c0":"###**4. Data Modelling and Tuning**\nPredicting successful start-ups\n","fbc54b88":"Tableau Software was recently acquired by Salesforce. However, they are categorized as a operational company in our dataset. To test the performance of our model, we can try see how the model perform on Tableau:","1fe267bc":"###**5. Data Evaluation**\nPredicting successful start-ups\n","59013456":"#### 3.7 Exporting data to Tableau\n\n","07e3c3dd":"Slack software","dda6833a":"#### 3.6 Correlation matrix\n","8c2ce491":"#### 3.4 Age when founded\n","e6b33056":"#### 3.1 Funding sum and rounds","c9b2003f":"#### 3.5 Geographical data","03f72abe":"*Hence our dataset is inbalanced, we can apply SMOTE to balance the difference between successfull and non-successfull startups. SMOTE is using knearestneighbor, to create new rows by not copying but rather simulating existing rows.*\n\n\n*We can the try to apply SMOTE to the training data and they carry out the two ML models from the previous chapter once again.*\n\n*However, it is important, that we only do a SMOTE on the training data, because information from the target variable then is restricted to only training and the real performance of the test is then isolated.*","58687e09":"##### 5.3 Random forrest w. balanced data","d1cb9beb":"#### 4.1 Random Forrest Classification\n\n\n"}}