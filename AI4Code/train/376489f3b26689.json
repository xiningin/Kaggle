{"cell_type":{"e8eb9ced":"code","62907d11":"code","42ad22d9":"code","9fd89e2b":"code","7c9da414":"code","6ec0bd1d":"code","0c640714":"code","a90325f8":"code","75b5c82c":"code","3089f37a":"code","8b5d0e57":"code","fbef5a87":"code","de294769":"code","58fa9bf4":"code","4c1b10f0":"code","0e0c6e89":"code","c9a9e670":"code","8c1623ad":"code","5c7f994c":"markdown","6bacf30b":"markdown","72743506":"markdown","13cc51d3":"markdown","c3b8d2e2":"markdown","4a4be6c8":"markdown","f6660336":"markdown","f9a038d3":"markdown","7074a781":"markdown","42be1660":"markdown","9e727d74":"markdown"},"source":{"e8eb9ced":"import numpy as np\nimport pandas as pd\nfrom keras import Model\nfrom keras.applications.mobilenet import MobileNet, preprocess_input\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback\nfrom keras.layers import Conv2D, Reshape\nfrom keras.utils import Sequence\nfrom keras.backend import epsilon\nimport tensorflow as tf\n\nfrom PIL import Image\n\nimport os\nimport random\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom PIL import Image\nimport numpy as np\nimport cv2\n\nnp.random.seed(1)","62907d11":"train = pd.read_csv(\"..\/input\/racoon-detection\/train_labels_.csv\")","42ad22d9":"train.head()","9fd89e2b":"train.shape","7c9da414":"IMAGE_SIZE = 128","6ec0bd1d":"coords=train[[\"width\",\"height\",\"xmin\",\"ymin\",\"xmax\",\"ymax\"]]\n\ncoords[\"xmin\"] = coords[\"xmin\"] *IMAGE_SIZE\/coords[\"width\"]\ncoords[\"xmax\"] = coords[\"xmax\"]*IMAGE_SIZE \/coords[\"width\"]\ncoords[\"ymin\"] = coords[\"ymin\"] *IMAGE_SIZE\/coords[\"height\"]\ncoords[\"ymax\"] = coords[\"ymax\"] *IMAGE_SIZE\/coords[\"height\"]\n\ncoords.drop([\"width\",\"height\"],axis =1,inplace=True)\ncoords.head()","0c640714":"paths = train[\"filename\"]\nlen(paths)","a90325f8":"images = \"..\/input\/racoon-detection\/Racoon Images\/images\/\"\n\nbatch_images = np.zeros((len(paths), IMAGE_SIZE, IMAGE_SIZE,3), dtype=np.float32)\n\nfor i, f in enumerate(paths):\n  #print(f)\n  img = Image.open(images+f)\n  img = img.resize((IMAGE_SIZE, IMAGE_SIZE))\n  img = img.convert('RGB')\n  batch_images[i] = preprocess_input(np.array(img, dtype=np.float32))","75b5c82c":"ALPHA = 1.0\n\nmodel = MobileNet(input_shape=(IMAGE_SIZE,IMAGE_SIZE,3), include_top=False, alpha=ALPHA)","3089f37a":"for layers in model.layers:\n  layers.trainable = False\n\nx = model.layers[-1].output\nx = Conv2D(4, kernel_size = 4, name=\"coords\")(x)\nx = Reshape((4,))(x)\n\nmodel = Model(inputs = model.inputs, outputs = x)","8b5d0e57":"def loss(gt,pred):\n    intersections = 0\n    unions = 0\n    diff_width = np.minimum(gt[:,0] + gt[:,2], pred[:,0] + pred[:,2]) - np.maximum(gt[:,0], pred[:,0])\n    diff_height = np.minimum(gt[:,1] + gt[:,3], pred[:,1] + pred[:,3]) - np.maximum(gt[:,1], pred[:,1])\n    intersection = diff_width * diff_height\n    \n    # Compute union\n    area_gt = gt[:,2] * gt[:,3]\n    area_pred = pred[:,2] * pred[:,3]\n    union = area_gt + area_pred - intersection\n\n#     Compute intersection and union over multiple boxes\n    for j, _ in enumerate(union):\n        if union[j] > 0 and intersection[j] > 0 and union[j] >= intersection[j]:\n            intersections += intersection[j]\n            unions += union[j]\n\n    # Compute IOU. Use epsilon to prevent division by zero\n    iou = np.round(intersections \/ (unions + epsilon()), 4)\n    iou = iou.astype(np.float32)\n    return iou\n\ndef IoU(y_true, y_pred):\n    iou = tf.py_function(loss, [y_true, y_pred], tf.float32)\n    return iou","fbef5a87":"gt = coords\n\nPATIENCE=10\n\nmodel.compile(optimizer = \"Adam\", loss = \"mse\", metrics = [IoU])\n\nstop = EarlyStopping(monitor='val_iou', patience=PATIENCE, mode=\"max\" )\n\nreduce_lr = ReduceLROnPlateau(monitor='val_iou',factor=0.2,patience=PATIENCE, min_lr=1e-7, verbose=1, mode=\"max\" )\n\nmodel.fit(batch_images, gt, epochs=100,callbacks=[stop,reduce_lr], verbose = 2)","de294769":"test_img = random.choice(paths)","58fa9bf4":"filename = images+ test_img\nunscaled = cv2.imread(filename)","4c1b10f0":"image_height, image_width, _ = unscaled.shape\nimage = cv2.resize(unscaled,(IMAGE_SIZE,IMAGE_SIZE))\nfeat_scaled = preprocess_input(np.array(image, dtype=np.float32))","0e0c6e89":"region = model.predict(x = np.array([feat_scaled]))[0]","c9a9e670":"x0 = int(region[0] * image_width \/ IMAGE_SIZE) \ny0 = int(region[1] * image_height \/ IMAGE_SIZE)\n\nx1 = int((region[2]) * image_width \/ IMAGE_SIZE)\ny1 = int((region[3]) * image_height \/ IMAGE_SIZE)","8c1623ad":"# Create figure and axes\nfig,ax = plt.subplots(1)\n\n# Display the image\nax.imshow(unscaled)\n\n# Create a Rectangle patch\nrect = patches.Rectangle((x0, y0), (x1 - x0) , (y1 - y0) , linewidth=2, edgecolor='r', facecolor='none')\n\n# Add the patch to the Axes\nax.add_patch(rect)\n\nplt.show()","5c7f994c":"## Pick a test image from the given data","6bacf30b":"## Importing the necessary libraries","72743506":"* Predict the coordinates of the bounding box for the given test image","13cc51d3":"* Create a list variable known as 'path' which has all the path for all the training images\n* Create an array 'coords' which has the resized coordinates of the bounding box for the training images","c3b8d2e2":"## Reading the training data from train.csv file","4a4be6c8":"## Plotting the predicted bounding box","f6660336":"## Define a custom loss function IoU which calculates Intersection Over Union","f9a038d3":"* Scaling the BBox","7074a781":"## Model Building\n* Building the model using transfer learning","42be1660":"## Preprocessing of Test Image\nResizing the image to 128 * 128 and preprocess the image for the MobileNet model","9e727d74":"## Compiling the model"}}