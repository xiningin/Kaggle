{"cell_type":{"72ffc817":"code","16e4e250":"code","e694a9f4":"code","35a99f28":"code","eda11000":"code","4713b390":"code","90beddc6":"code","377504e6":"code","654cd73d":"code","4be64c67":"code","25999067":"code","3d59c1e9":"code","c42a0677":"markdown"},"source":{"72ffc817":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","16e4e250":"!pip install factor-analyzer\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import KernelPCA\n\nfrom factor_analyzer import FactorAnalyzer\n%matplotlib inline","e694a9f4":"# Load the train data\ntrain=pd.read_csv('..\/input\/instant-gratification\/train.csv')\ntrain.head()","35a99f28":"# drop the id and target col\ndf=train.drop(columns=['target','id'])\ndf.dtypes","eda11000":"# StandardScaler + PCA\nscaler = StandardScaler()\ntrain_scaled = scaler.fit_transform(df)     \n\npca = PCA(n_components=4)\nPCA_train_x = pca.fit_transform(train_scaled)\n\nplt.scatter(PCA_train_x[:, 0],PCA_train_x[:, 1],\n            c=train.target,cmap=\"copper_r\")\nplt.colorbar()\nplt.show()","4713b390":"# Loadings\nloadings=pd.DataFrame(pca.components_.T,columns=['PC1','PC2', 'PC3', 'PC4'],\n                      index=df.columns)\nprint(loadings.sort_values(by=['PC1']))\nprint(loadings.sort_values(by=['PC2']))\nprint(loadings.sort_values(by=['PC3']))\nprint(loadings.sort_values(by=['PC4']))\n\n\n# PCA loadings are the coefficients of the linear combination of \n# the original variables from which the principal components (PCs)\n# are constructed.","90beddc6":"# Loadings Matrix\nloadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n\nloading_matrix = pd.DataFrame(loadings, columns=['PC1', 'PC2', 'PC3', 'PC4'],\n                              index=df.columns)\n\nprint(loading_matrix.sort_values(by=['PC1']))\nprint(loading_matrix.sort_values(by=['PC2']))\nprint(loading_matrix.sort_values(by=['PC3']))\nprint(loading_matrix.sort_values(by=['PC4']))","377504e6":"fa = FactorAnalyzer(18, rotation='varimax',\n                    method='principal',impute='mean')\nfa.fit(train_scaled)\n\nev, v = fa.get_eigenvalues()\nprint(ev)\n\n#Create scree plot using matplotlib\nplt.figure(figsize=(8, 6.5))\nplt.scatter(range(1,train_scaled.shape[1]+1),ev)\nplt.plot(range(1,train_scaled.shape[1]+1),ev)\nplt.title('Scree Plot',fontdict={'weight':'normal','size': 25})\nplt.xlabel('Factors',fontdict={'weight':'normal','size': 15})\nplt.ylabel('Features',fontdict={'weight':'normal','size': 15})\nplt.grid()\nplt.show()\n\n# no. of factors\nn_factors = sum(ev>1)","654cd73d":"fa2 = FactorAnalyzer(n_factors,rotation='varimax',method='principal')\nfa2.fit(train_scaled)\n\nvar = fa2.get_factor_variance()\n\nfa2_score = fa2.transform(train_scaled)\n\ncolumn_list = ['fac'+str(i) for i in np.arange(n_factors)+1]\nfa_score = pd.DataFrame(fa2_score,columns=column_list)\nfor col in fa_score.columns:\n    data[col] = fa_score[col]\nprint(\"\\n Factor Scores:\\n\",fa_score)    \n\ndf_fv = pd.DataFrame()\ndf_fv['Factors'] = column_list\ndf_fv['Contribution'] = var[1]\ndf_fv['Acc. Contribution'] = var[2]\ndf_fv['Acc. Contribution Pct'] = var[1]\/var[1].sum()\nprint(\"\\n list:\\n\",df_fv) ","4be64c67":"# Kernel PCA\nlin_pca = KernelPCA(n_components = 2, kernel=\"linear\", fit_inverse_transform=True)\nrbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433, fit_inverse_transform=True)\nsig_pca = KernelPCA(n_components = 2, kernel=\"sigmoid\", gamma=0.001, coef0=1, fit_inverse_transform=True)\n\nplt.figure(figsize=(11, 4))\nfor subplot, pca, title in ((131, lin_pca, \"Linear kernel\"), (132, rbf_pca, \"RBF kernel, $\\gamma=0.04$\"), \n                            (133, sig_pca, \"Sigmoid kernel, $\\gamma=10^{-3}, r=1$\")):\n       \n    PCA_train_x = PCA(2).fit_transform(train_scaled)\n    plt.subplot(subplot)\n    plt.title(title, fontsize=14)\n    plt.scatter(PCA_train_x[:, 0], PCA_train_x[:, 1], c=train.target, cmap=\"viridis\")\n    plt.xlabel(\"$z_1$\", fontsize=18)\n    if subplot == 131:\n        plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n    plt.grid(True)\n\nplt.show()","25999067":"# K Means\nkmeans=KMeans(n_clusters=2,random_state=0).fit(PCA_train_x)\ny_kmeans = kmeans.predict(PCA_train_x)\ncenters=kmeans.cluster_centers_","3d59c1e9":"# Remake the Plot\nplt.scatter(PCA_train_x[:, 0],PCA_train_x[:, 1],\n            c=y_kmeans,cmap=\"copper_r\")\n# plt.axis('off')\nplt.colorbar()\nplt.show()","c42a0677":"**Clustering Analysis**"}}