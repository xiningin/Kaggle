{"cell_type":{"6b2657a7":"code","3b14cd5b":"code","ef7104ca":"code","cf320a1f":"code","ddd99257":"code","3d1a4e49":"code","31004056":"code","d6d7157d":"code","2347fc92":"code","2a42f33e":"code","18f7d45b":"code","a7c8e829":"code","ae019c97":"code","da77b17e":"code","610bdf97":"code","c619d597":"code","187ab992":"code","12b28ec7":"code","a1be6378":"code","e8e83c21":"code","c3afdfae":"code","ec53ced2":"code","c85fe31a":"code","90dff5bc":"code","cf74d92b":"code","fc1f1fce":"code","3478ee52":"code","4f0d7ba1":"markdown","c7adaa9f":"markdown","cd72b0b1":"markdown","42c63099":"markdown","82f832ca":"markdown","442df4ea":"markdown","e17e8192":"markdown","2bd4d402":"markdown","08360b61":"markdown","1ade3d80":"markdown","c0612e15":"markdown","15da062f":"markdown","677f713c":"markdown","3ebf37c9":"markdown","0c83df09":"markdown","d3d9e4ba":"markdown","8e7c78ac":"markdown","5d78a731":"markdown","c39437a0":"markdown","5a2f4431":"markdown"},"source":{"6b2657a7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\nfrom scipy.stats.stats import pearsonr\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom sklearn.linear_model import Ridge, Lasso, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.metrics import mean_squared_error\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom xgboost import XGBRegressor\nimport warnings  \nwarnings.filterwarnings('ignore')\n%config InlineBackend.figure_format = 'retina' #set 'png' here when working on notebook\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ninput_dir = \"\/kaggle\/input\/house-prices-advanced-regression-techniques\/\"\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3b14cd5b":"train = pd.read_csv(f\"{input_dir}\/train.csv\")\ntest = pd.read_csv(f\"{input_dir}\/test.csv\")\n\nprint(\"Train shape:{}\".format(train.shape))\nprint(\"Test shape:{} \\n\".format(test.shape))\n\ntrain.head()","ef7104ca":"## Drop the ID columns from both, this has no impact on the sale price\ntrain.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)\n\ntrain.head()","cf320a1f":"sns.scatterplot(x='GrLivArea', y='SalePrice', data= train)\ntrain = train[train.GrLivArea < 4000]","ddd99257":"train.reset_index(drop=True, inplace=True)\nsns.scatterplot(x='GrLivArea', y='SalePrice', data= train)","3d1a4e49":"train['SalePrice'].hist(bins = \"auto\")","31004056":"## Use log1p to remove skewness\ntrain['SalePrice'] = np.log1p(train['SalePrice'])\ntrain['SalePrice'].hist(bins = \"auto\")\n\n# With data looking better, store SalePrice in y so we can concatenate the data and continue processing, and put just the train and test features into a new dfs\ny = train['SalePrice'].reset_index(drop=True)\ntrain_feat = train.drop(['SalePrice'], axis=1)\ntest_feat = test","d6d7157d":"features = pd.concat([train_feat, test_feat]).reset_index(drop=True)\nprint(features.shape)\nfeatures.head()","2347fc92":"types = features.dtypes\n# Finding the percentage of missing data for each feature\nmissing = round((features.isnull().sum()\/features.shape[0]),3)*100\n\nexplo = pd.DataFrame({'Types': types, 'Missing': missing}).sort_values(by=['Missing', 'Types'], ascending=False)\nexplo[explo.Missing > 0]","2a42f33e":"fill_with_none = ('PoolQC', 'Alley', 'MiscFeature', 'Fence', 'FireplaceQu', 'GarageFinish', 'GarageQual', 'GarageCond', 'GarageType', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType')\nfill_with_mode = ('Utilities', 'Functional')\nfill_with_0 = ('GarageYrBlt', 'MasVnrArea', 'BsmtFullBath', 'BsmtHalfBath')\n\nfor col in fill_with_none:\n    features[col]=features[col].fillna('None')\n\nfor col in fill_with_mode:\n    features[col]=features[col].fillna(features[col].mode()[0])\n\nfor col in fill_with_0:\n    features[col]=features[col].fillna(0)\n    \nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    features[col] = features[col].fillna(0)\n\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    features[col] = features[col].fillna('None')\n\n    \n### Same with basement\n\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    features[col] = features[col].fillna('None')\n\nfeatures['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0]) \nfeatures['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\nfeatures['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])","18f7d45b":"features['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))","a7c8e829":"features['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics = []\nfor i in features.columns:\n    if features[i].dtype in numeric_dtypes:\n        numerics.append(i)\nfeatures.update(features[numerics].fillna(0))\nnumerics[1:10]","ae019c97":"## Re run the previous exploration to make sure all values are filled\ntypes = features.dtypes\n# Finding the percentage of missing data for each feature\nmissing = round((features.isnull().sum()\/features.shape[0]),3)*100\n\nexplo = pd.DataFrame({'Types': types, 'Missing': missing}).sort_values(by=['Missing', 'Types'], ascending=False)\nexplo[explo.Missing > 0]","da77b17e":"features['MSSubClass'] = features['MSSubClass'].apply(str)\nfeatures['YrSold'] = features['YrSold'].apply(str)\nfeatures['MoSold'] = features['MoSold'].apply(str)","610bdf97":"numeric_types = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric_features = []\nfor i in features.columns:\n    if features[i].dtypes in numeric_types:\n        numeric_features.append(i)\nskewed_feats = features[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skewed_feats[skewed_feats > 0.5]\nskew_ind = high_skew.index\n\nfor i in skew_ind:\n    features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1))","c619d597":"features = features.drop(['Utilities', 'Street', 'PoolQC',], axis=1)\n\nfeatures['TotalSF'] = features['TotalBsmtSF'] + features['1stFlrSF'] + features['2ndFlrSF']\nfeatures['TotalFinSF'] = features['BsmtFinSF1'] + features['BsmtFinSF2'] + features['1stFlrSF'] + features['2ndFlrSF']\nfeatures['TotalBathrooms'] = features['FullBath'] + features['BsmtFullBath'] + (0.5 * features['HalfBath']) + (0.5 * features['BsmtHalfBath'])\n","187ab992":"features['HasPool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['Has2ndLvl'] = features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['HasBsmt'] = features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['HasGarage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures.shape","12b28ec7":"final_feat = pd.get_dummies(features).reset_index(drop=True)\nfinal_feat.head()","a1be6378":"X = final_feat.iloc[:len(y), :]\nX_test = final_feat.iloc[len(y):, :]\nX.shape, y.shape, X_test.shape\n\n## Re run the previous exploration to make sure all values are filled\ntest_types = X_test.dtypes\n# Finding the percentage of missing data for each feature\ntest_missing = round((X_test.isnull().sum()\/X_test.shape[0]),3)*100\n\ntest_explo = pd.DataFrame({'Types': test_types, 'Missing': test_missing}).sort_values(by=['Missing', 'Types'], ascending=False)\nprint(test_explo[test_explo.Missing > 0])\n\n## Re run the previous exploration to make sure all values are filled\ntrain_types = X.dtypes\n# Finding the percentage of missing data for each feature\ntrain_missing = round((X.isnull().sum()\/X.shape[0]),3)*100\n\ntrain_explo = pd.DataFrame({'Types': train_types, 'Missing': train_missing}).sort_values(by=['Missing', 'Types'], ascending=False)\nprint(test_explo[test_explo.Missing > 0])","e8e83c21":"overfit = []\nfor i in X.columns:\n    counts = X[i].value_counts()\n    zeros = counts.iloc[0]\n    if zeros \/ len(X) * 100 > 99.94:\n        overfit.append(i)\n\noverfit = list(overfit)\nX = X.drop(overfit, axis=1)\nX_test = X_test.drop(overfit, axis=1)\noverfit","c3afdfae":"## Helper functions\n\nkfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\n#This is the scoring used by the competition\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)","ec53ced2":"## Use pipelines to pass into the stacking regressor\n\nalphas_ridge = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas_lasso = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_ridge, cv=kfolds))\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas=alphas_lasso, random_state=42, cv=kfolds))\nxgboost = XGBRegressor(learning_rate=0.01,n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:squarederror', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006)\n","c85fe31a":"## Generate stacking\nstack = StackingCVRegressor(regressors=(ridge, lasso, xgboost), meta_regressor=xgboost, use_features_in_secondary=True)\n","90dff5bc":"## Begin fittings\n\nprint('stack')\nstack_gen_model = stack.fit(np.array(X), np.array(y))\n\nprint('ridge')\nridge_model = ridge.fit(X, y)\n\nprint('lasso')\nlasso_model = lasso.fit(X, y)\n\nprint('XGBoost')\nxgb_model = xgboost.fit(X, y)","cf74d92b":"print(\"RMSLE score on train data: \")\nprint(rmsle(y, stack.predict(np.array(X))))","fc1f1fce":"print('Predict Submission')\nsubmission = pd.read_csv(f\"{input_dir}\/sample_submission.csv\")\nsubmission.iloc[:,1] = np.expm1(stack.predict(X_test))","3478ee52":"submission.to_csv(\"submission.csv\", index=False)","4f0d7ba1":"#### Now that processing is done, split back into train and test, and make sure all data is filled in","c7adaa9f":"## Data preprocessing","cd72b0b1":"Similarly, LotFrontage will likely be closely tied to the median LotFrontage in that neighborhood","42c63099":"#### Transforming skewed columns\n\nInititally we were going to just use `log1p` again, but some of the forum posts we read suggested using the boxcox transform, which looks for the best transformation function","82f832ca":"#### Fill missing features\n\nFirst we explore the data more to find out which columns are missing data, and their types","442df4ea":"With no data missing, we can move forward processing\n\nOne thing we need to do is convert MSSubClass, YrSold, and MoSold to strings, since they are actually categorical, but stored as numerics","e17e8192":"Since MSZoning will be pretty closely tied to MSSubClass, we use groupby to fill with an appropriate value","2bd4d402":"#### Removing outliers\n\nFrom data author, a plot of `SalePrice` vs `GrLivArea` will show some outliers. The Data author also recommends removing data with `GrLivArea < 4000`. For more information, go to page 4 of [this document](http:\/\/jse.amstat.org\/v19n3\/decock.pdf)","08360b61":"#### Now, use get_dummies to convert our categorical data to numerical data","1ade3d80":"#### Drop the ID column, as it has no impact on the SalePrice","c0612e15":"Looking into the data description we find these categories to fill.\n\nNA means \"None\", fill with \"None\":\n* PoolQC\n* Alley\n* MiscFeature\n* Fence\n* FireplaceQu\n* GarageFinish\n* GarageQual\n* GarageCond\n* GarageType\n* BsmtQual\n* BsmtCond\n* BsmtExposure\n* BsmtFinType1\n* BsmtFinType2\n* MasVnrType\n\nNA means \"Not recorded\", fill with *mode*:\n* Utilities\n* Functional\n\nNA means \"None\", fill with 0:\n* GarageYrBlt\n* MasVnrArea\n* BsmtFullBath\n* BsmtHalfBath\n\nNA means \"Not recorded\", but can be filled using other features:\n* MSZoning\n* LotFrontage","15da062f":"#### Merge the feature sets to better process the data","677f713c":"After using log1p, we want to store the SalePrice in a separate variable, `y`, and create dataframes with just the features from both the train and test data ","3ebf37c9":"## Loading files and basic data exploration","0c83df09":"#### Remove skewness\n\nWith the outliers removed, look at skewness of SalePrice, and use log1p if skewed","d3d9e4ba":"## Imports","8e7c78ac":"## Modeling\n\nWe are going to stack 3 models together, Ridge, Lasso, and an XGBoost model","5d78a731":"The above DataFrame shows us all of the features with some missing data. Some of this data is categorical, while some is numeric. We look into the data description to decide what is the best way to fill in that data","c39437a0":"## Feature Engineering\n\nHere, we are going to create additional features from the features provided that will likely have an impact on our model\n\n#### First, creating totals","5a2f4431":"#### Second, creating boolean features for certain home aspects"}}