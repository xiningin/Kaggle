{"cell_type":{"e577465b":"code","21ccdfd9":"code","4a658294":"code","cb999a90":"code","01c4ef27":"code","6258eef8":"code","fda9b0e4":"code","b2270f49":"code","b9e09a0d":"code","360e4199":"code","4ad06109":"code","3eb9f340":"code","e9c8dd99":"code","c49b3465":"code","9cbf82cc":"code","e09f0b2a":"code","d27b50c5":"code","3c01e525":"code","34c23aae":"code","644caa69":"code","66a9a23d":"code","83147c0c":"code","cf9afc2a":"code","1d6c73ca":"code","7dff6607":"code","4cef6172":"code","5c7cfea6":"code","a72e08bc":"code","26f0c03c":"code","435c6f8f":"code","7c84dae4":"code","688d2e93":"code","36d73a63":"code","d7a3ff83":"code","da76020f":"code","77db4146":"code","8bee3dc7":"code","bfd77fcd":"code","fe9e5b68":"code","c25b6648":"code","0d56ac82":"markdown","f8d967d0":"markdown","e70222cf":"markdown","568de78d":"markdown","7a83cd24":"markdown","9cace28e":"markdown","aa3c61ed":"markdown","6f51bec0":"markdown","48301d2d":"markdown","5c544a42":"markdown","6e20ccc3":"markdown"},"source":{"e577465b":"import os\nimport gc\ngc.enable()\n\nimport logging\nlogger = logging.getLogger()\nlogger.setLevel(logging.DEBUG)\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport catboost\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score, roc_curve\nimport matplotlib.gridspec as gridspec\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Going to use these 5 base models for the stacking\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn import metrics\n\nfrom sklearn.svm import SVC\nimport time","21ccdfd9":"## Some switches \nuseXGB = False\nuseLGBM = True\nuseEnsemble = False\n## Accidental Switch Setting\nif (useXGB) and(useLGBM):\n    useEnsemble = False","4a658294":"%%time\ntrain_transaction = pd.read_csv('..\/input\/train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('..\/input\/test_transaction.csv', index_col='TransactionID')\n\ntrain_identity = pd.read_csv('..\/input\/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('..\/input\/test_identity.csv', index_col='TransactionID')\n\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='TransactionID')\n\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\nprint(train.shape)\nprint(test.shape)","cb999a90":"## Quick Trick to Create Email Feature\nemails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', 'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft', 'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo', 'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', 'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink', 'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other', 'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', 'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', 'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo', 'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other', 'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft', 'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', 'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', 'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', 'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', 'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', 'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', 'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other', 'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\nus_emails = ['gmail', 'net', 'edu']\n\nfor c in ['P_emaildomain', 'R_emaildomain']:\n    train[c + '_bin'] = train[c].map(emails)\n    test[c + '_bin'] = test[c].map(emails)\n    \n    train[c + '_suffix'] = train[c].map(lambda x: str(x).split('.')[-1])\n    test[c + '_suffix'] = test[c].map(lambda x: str(x).split('.')[-1])\n    \n    train[c + '_suffix'] = train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    test[c + '_suffix'] = test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    gc.collect()","01c4ef27":"## Trick to create Hr Feature :\n## https:\/\/www.kaggle.com\/fchmiel\/day-and-time-powerful-predictive-feature\ndef make_day_feature(df, offset=0, tname='TransactionDT'):\n    \"\"\"\n    Creates a day of the week feature, encoded as 0-6. \n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        df to manipulate.\n    offset : float (default=0)\n        offset (in days) to shift the start\/end of a day.\n    tname : str\n        Name of the time column in df.\n    \"\"\"\n    # found a good offset is 0.58\n    days = df[tname] \/ (3600*24)        \n    encoded_days = np.floor(days-1+offset) % 7\n    return encoded_days\n\ndef make_hour_feature(df, tname='TransactionDT'):\n    \"\"\"\n    Creates an hour of the day feature, encoded as 0-23. \n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        df to manipulate.\n    tname : str\n        Name of the time column in df.\n    \"\"\"\n    hours = df[tname] \/ (3600)        \n    encoded_hours = np.floor(hours) % 24\n    return encoded_hours\n\n","6258eef8":"## Create Feature in Train and Test\ntrain['hours'] = make_hour_feature(train)\ntest['hours']= make_hour_feature(test)","fda9b0e4":"## Label Encoding\nfor f in train.columns:\n    if train[f].dtype=='object' or train[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(train[f].values) + list(test[f].values))\n        train[f] = lbl.transform(list(train[f].values))\n        test[f] = lbl.transform(list(test[f].values))   ","b2270f49":"y_train = train['isFraud'].copy()\ndel train_transaction, train_identity, test_transaction, test_identity\n\n# Drop target, fill in NaNs\nX_train = train.drop('isFraud', axis=1)\nX_test = test.copy()\n\ndel train, test","b9e09a0d":"%%time\n# From kernel https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\n# WARNING! THIS CAN DAMAGE THE DATA \ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\nX_train = reduce_mem_usage(X_train)\nX_test = reduce_mem_usage(X_test)","360e4199":"columns = []\nfor f in X_train.columns:\n    columns.append(f)\n    \ndtcol = [columns[0]]\ncardcol = columns[3:9]\nccol = columns[15:29]\ndcol = columns[29:44]\nmcol = columns[44:53]\nvcol = columns[53:392]\nidcol = columns[392:430]\ncustomcol = columns[432:437] \n","4ad06109":"# COMPARE VALUE DENSITIES FROM TWO DIFFERENT DATAFRAMES\n#\n# PARAMETERS\n# df1: pandas.DataFrame containing variable\n# df2: pandas.DataFrame containing variable\n# col: column to compare between df1 and df2\n# override: set to False to prevent display when variables similar\n# verbose: display text summary\n# scale: zooms y-axis\n# title: plot title\n# lab1: legend label for df1\n# lab2: legend label for df2\n# prefix: pre text for verbose summary\n#\ndef comparePlot(df1, df2, col, factor=4, override=True, verbose=True, scale=0.5, title='',\n                lab1='', lab2='', prefix=''):\n    cv1 = pd.DataFrame(df1[col].value_counts(normalize=True).reset_index().rename({col:'train'},axis=1))\n    cv2 = pd.DataFrame(df2[col].value_counts(normalize=True).reset_index().rename({col:'test'},axis=1))\n    cv3 = pd.merge(cv1,cv2,on='index',how='outer')\n    cv3['train'].fillna(0,inplace=True)\n    cv3['test'].fillna(0,inplace=True)\n    cv3 = cv3.iloc[np.lexsort((cv3['test'], -cv3['train']))]\n    cv3['total'] = cv3['train']+cv3['test']\n    cv3['trainMX'] = cv3['train']*factor\n    cv3['trainMN'] = cv3['train']\/factor\n    cv3 = cv3[cv3['total']>0.0001]\n    if (len(cv3)<5): return\n    cv3.reset_index(inplace=True)\n    MX = (cv3['test'] > cv3['trainMX'])\n    mxSum = round(100*cv3.loc[MX,'test'].sum(),1)\n    MN = (cv3['test'] < cv3['trainMN'])\n    mnSum = round(100*cv3.loc[MN,'test'].sum(),1)\n    #if override | (MX.sum()+MN.sum()>0):\n    if override | (mxSum + mnSum > 1):\n        plt.figure(figsize=(12,3))\n        if lab1=='': lab1='Train'\n        if lab2=='': lab2='Test'\n        plt.plot(cv3.index,cv3['train'],linewidth=3,alpha=0.7,color='b',label=lab1)\n        plt.plot(cv3.index,cv3['trainMX'],linewidth=2,alpha=1.0,linestyle=':',color='b',label=str())\n        plt.plot(cv3.index,cv3['trainMN'],linewidth=2,alpha=1.0,linestyle=':',color='b',label=str())\n        #plt.bar(cv3.index,cv3['test'],linewidth=3,alpha=0.7,color='g', label='Test.csv')\n        plt.plot(cv3.index,cv3['test'],linewidth=3,alpha=0.7,color='g',label=lab2)\n        plt.legend()\n        if title=='': plt.title(col)\n        else: plt.title(col+' - '+title)\n        plt.xlabel(col+' values (ordered by train frequency and relabeled)')\n        plt.ylabel('Frequency')\n        mx = max(cv3['train'].max(),cv3['test'].max())\n        #plt.ylim(0,mx*1.05)\n        plt.ylim(0,mx*scale)\n        plt.show()\n        tempMX = cv3.loc[MX.values,['index','test']].sort_values('test',ascending=False)['index']\n        tempMN = cv3.loc[MN.values,['index','test']].sort_values('test',ascending=False)['index']\n        if verbose:\n            if MX.sum()>0:    \n                print(prefix+'Test.csv',col,'has',MX.sum(),'values 4x MORE freq than Train.csv. (',mxSum,'% of data)')\n            if MX.sum()>10: print('  Top 10 by test freq:',list(tempMX)[:10])\n            elif MX.sum()>0: print(list(tempMX)[:10])\n            if MN.sum()>0:\n                print(prefix+'Test.csv',col,'has',MN.sum(),'values 4x LESS freq than Train.csv. (',mnSum,'% of data)')\n            if MN.sum()>10: print('  Top 10 by test freq:',list(tempMN)[:10])\n            elif MN.sum()>0: print(list(tempMN)[:10])\n    return","3eb9f340":"for cols in cardcol: \n        comparePlot(X_train, X_test, cols, verbose=True, title='Test vs. Train')","e9c8dd99":"for cols in ccol: \n        comparePlot(X_train, X_test, cols, verbose=True, title='Test vs. Train')","c49b3465":"for cols in dcol: \n        comparePlot(X_train, X_test, cols, verbose=True, title='Test vs. Train')","9cbf82cc":"for cols in vcol[0:15]:\n        comparePlot(X_train, X_test, cols, verbose=True, title='Test vs. Train')","e09f0b2a":"for cols in vcol[200:202]: \n        comparePlot(X_train, X_test, cols, verbose=True, title='Test vs. Train')","d27b50c5":"for cols in vcol[250:260]: \n        comparePlot(X_train, X_test, cols, verbose=True, title='Test vs. Train')","3c01e525":"for cols in idcol: \n        comparePlot(X_train, X_test, cols, verbose=True, title='Test vs. Train')","34c23aae":"for cols in customcol: \n        comparePlot(X_train, X_test, cols, verbose=True, title='Test vs. Train')","644caa69":"\ndrop_col = dtcol +mcol[0:len(mcol)]+ idcol[5:39]  + dcol[0:len(dcol)] + ccol[4:len(ccol)]\nX_train.drop(drop_col,axis=1, inplace=True)\nX_test.drop(drop_col, axis=1, inplace=True)\n\n\nX_train.head()","66a9a23d":"X_train.fillna(-1,inplace=True)\nX_test.fillna(-1,inplace=True)\nX_train.head()","83147c0c":"def make_amt_feature(df, tname='TransactionAmt'):\n    \"\"\"\n    Creates an hour of the day feature, encoded as 0-23. \n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        df to manipulate.\n    tname : str\n        Name of the time column in df.\n    \"\"\"\n    logAmt = np.log(df[tname])      \n    \n    return logAmt","cf9afc2a":"## Create Feature in Train and Test\nX_train['LogAmt'] = make_amt_feature(X_train)\nX_test['LogAmt']= make_amt_feature(X_test)","1d6c73ca":"%%time\nif useXGB :\n    NFOLDS = 6\n    kf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=123)\n\n    y_preds_xgb = np.zeros(X_test.shape[0])\n    y_oof_xgb = np.zeros(X_train.shape[0])\n    score = 0\n  \n    for fold, (tr_idx, val_idx) in enumerate(kf.split(X_train, y_train)):\n        clf = xgb.XGBClassifier(\n            n_estimators=500,\n            max_depth=9,\n            learning_rate=0.05,\n            subsample=0.9,\n            colsample_bytree=0.9,\n            gamma = 0.2,\n            alpha = 4,\n            missing = -1,\n            tree_method='gpu_hist'\n        )\n    \n        X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n        clf.fit(X_tr, y_tr)\n        y_pred_train = clf.predict_proba(X_vl)[:,1]\n        y_oof_xgb[val_idx] = y_pred_train\n        print(\"FOLD: \",fold,' AUC {}'.format(roc_auc_score(y_vl, y_pred_train)))\n        score += roc_auc_score(y_vl, y_pred_train) \/ NFOLDS\n        y_preds_xgb+= clf.predict_proba(X_test)[:,1] \/ NFOLDS\n    \n        del X_tr, X_vl, y_tr, y_vl\n        gc.collect()\n    \n    y_preds_xgb.reshape(-1,1)   \n    y_oof_xgb.reshape(-1,1)\n    print(\"\\nMEAN AUC = {}\".format(score))\n    print(\"OOF AUC = {}\".format(roc_auc_score(y_train, y_oof_xgb)))","7dff6607":"%%time\nif useXGB :\n    NFOLDS = 5\n    kf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=0)\n\n    y_preds_xgb1 = np.zeros(X_test.shape[0])\n    y_oof_xgb1 = np.zeros(X_train.shape[0])\n    score = 0\n  \n    for fold, (tr_idx, val_idx) in enumerate(kf.split(X_train, y_train)):\n        clf = xgb.XGBClassifier(\n            n_estimators=1500,\n            max_depth=10,\n            learning_rate=0.05,\n            subsample=0.9,\n            colsample_bytree=0.9,\n            gamma = 0.2,\n            alpha = 4,\n            missing = -1,\n            tree_method='gpu_hist'\n        )\n    \n        X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n        clf.fit(X_tr, y_tr)\n        y_pred_train = clf.predict_proba(X_vl)[:,1]\n        y_oof_xgb1[val_idx] = y_pred_train\n        print(\"FOLD: \",fold,' AUC {}'.format(roc_auc_score(y_vl, y_pred_train)))\n        score += roc_auc_score(y_vl, y_pred_train) \/ NFOLDS\n        y_preds_xgb1+= clf.predict_proba(X_test)[:,1] \/ NFOLDS\n    \n        del X_tr, X_vl, y_tr, y_vl\n        gc.collect()\n    \n    y_preds_xgb1.reshape(-1,1)   \n    y_oof_xgb1.reshape(-1,1)\n    print(\"\\nMEAN AUC = {}\".format(score))\n    print(\"OOF AUC = {}\".format(roc_auc_score(y_train, y_oof_xgb1)))","4cef6172":"%%time\nif useXGB :\n    NFOLDS = 5\n    kf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n\n    y_preds_xgb2 = np.zeros(X_test.shape[0])\n    y_oof_xgb2 = np.zeros(X_train.shape[0])\n    score = 0\n  \n    for fold, (tr_idx, val_idx) in enumerate(kf.split(X_train, y_train)):\n        clf = xgb.XGBClassifier(\n            n_estimators=1500,\n            max_depth=10,\n            learning_rate=0.05,\n            subsample=0.9,\n            colsample_bytree=0.9,\n            gamma = 0.2,\n            alpha = 4,\n            missing = -1,\n            tree_method='gpu_hist'\n        )\n    \n        X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n        clf.fit(X_tr, y_tr)\n        y_pred_train = clf.predict_proba(X_vl)[:,1]\n        y_oof_xgb2[val_idx] = y_pred_train\n        print(\"FOLD: \",fold,' AUC {}'.format(roc_auc_score(y_vl, y_pred_train)))\n        score += roc_auc_score(y_vl, y_pred_train) \/ NFOLDS\n        y_preds_xgb2+= clf.predict_proba(X_test)[:,1] \/ NFOLDS\n    \n        del X_tr, X_vl, y_tr, y_vl\n        gc.collect()\n    \n    y_preds_xgb2.reshape(-1,1)   \n    y_oof_xgb2.reshape(-1,1)\n    print(\"\\nMEAN AUC = {}\".format(score))\n    print(\"OOF AUC = {}\".format(roc_auc_score(y_train, y_oof_xgb2)))","5c7cfea6":"param = {'num_leaves': 30,\n         'min_data_in_leaf': 30, \n         'objective':'binary',\n         'max_depth': -1,\n         'learning_rate': 0.01,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.8,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.8 ,\n         \"bagging_seed\": 11,\n         \"metric\": 'auc',\n         \"lambda_l1\": 0.1,\n         \"random_state\": 133,\n         \"verbosity\": -1}","a72e08bc":"max_iter = 5\ngc.collect()","26f0c03c":"%%time\nfrom sklearn.model_selection import TimeSeriesSplit\n\nif useLGBM :\n    #folds = KFold(n_splits=5, shuffle=True, random_state=15)\n    folds = TimeSeriesSplit(n_splits =5)\n    oof = np.zeros(len(X_train))\n    #categorical_columns = [c for c in cat_cols ]\n    features = [c for c in X_train.columns]\n    predictions = np.zeros(len(X_test))\n    start = time.time()\n    feature_importance_df = pd.DataFrame()\n    start_time= time.time()\n    score = [0 for _ in range(folds.n_splits)]\n\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n        print(\"fold n\u00b0{}\".format(fold_))\n        trn_data = lgb.Dataset(X_train.iloc[trn_idx][features],\n                           label=y_train.iloc[trn_idx]#,\n                           #categorical_feature = categorical_columns\n                          )\n        val_data = lgb.Dataset(X_train.iloc[val_idx][features],\n                           label=y_train.iloc[val_idx]#,\n                           #categorical_feature = categorical_columns\n                          )\n\n        num_round = 500\n        clf = lgb.train(param,\n                    trn_data,\n                    num_round,\n                    valid_sets = [trn_data, val_data],\n                    verbose_eval=10,\n                    early_stopping_rounds = 100)\n    \n        oof[val_idx] = clf.predict(X_train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = features\n        fold_importance_df[\"importance\"] = clf.feature_importance(importance_type='gain')\n        fold_importance_df[\"fold\"] = fold_ + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n        # we perform predictions by chunks\n        initial_idx = 0\n        chunk_size = 100000\n        current_pred = np.zeros(len(X_test))\n        while initial_idx < X_test.shape[0]:\n            final_idx = min(initial_idx + chunk_size, X_test.shape[0])\n            idx = range(initial_idx, final_idx)\n            current_pred[idx] = clf.predict(X_test.iloc[idx][features], num_iteration=clf.best_iteration)\n            initial_idx = final_idx\n        predictions += current_pred \/ min(folds.n_splits, max_iter)\n   \n        print(\"time elapsed: {:<5.2}s\".format((time.time() - start_time) \/ 3600))\n        score[fold_] = metrics.roc_auc_score(y_train.iloc[val_idx], oof[val_idx])\n        if fold_ == max_iter - 1: break\n        \n    if (folds.n_splits == max_iter):\n        print(\"CV score: {:<8.5f}\".format(metrics.roc_auc_score(y_train, oof)))\n    else:\n         print(\"CV score: {:<8.5f}\".format(sum(score) \/ max_iter))\n","435c6f8f":"feature_importance_df['fold'].value_counts()","7c84dae4":"df_lose_it = feature_importance_df.loc[feature_importance_df['fold'] == 5].sort_values(by=\"importance\", ascending=True).head(100)","688d2e93":"df_lose_it=df_lose_it.loc[df_lose_it['importance'] == 0.0000]\ndrop_add = []\nfor vals in df_lose_it['feature'] :\n    drop_add.append(vals)\n","36d73a63":"## Fold1 Feature Importance\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"importance\", y=\"feature\", data=feature_importance_df.loc[feature_importance_df['fold'] == 1].sort_values(by=\"importance\", ascending=False).head(20))\nplt.title('LightGBM Features fold 1')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-01.png')","d7a3ff83":"## Fold 2 Feature Importance\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"importance\", y=\"feature\", data=feature_importance_df.loc[feature_importance_df['fold'] == 2].sort_values(by=\"importance\", ascending=False).head(20))\nplt.title('LightGBM Features fold 2')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-02.png')","da76020f":"## Fold 3 Feature importance \n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"importance\", y=\"feature\", data=feature_importance_df.loc[feature_importance_df['fold'] == 3].sort_values(by=\"importance\", ascending=False).head(20))\nplt.title('LightGBM Features fold 3')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-03.png')","77db4146":"## Fold 4 Feature importance \n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"importance\", y=\"feature\", data=feature_importance_df.loc[feature_importance_df['fold'] == 4].sort_values(by=\"importance\", ascending=False).head(20))\nplt.title('LightGBM Features fold 4')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-04.png')","8bee3dc7":"## Fold 5 Feature importance \n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"importance\", y=\"feature\", data=feature_importance_df.loc[feature_importance_df['fold'] == 5].sort_values(by=\"importance\", ascending=False).head(20))\nplt.title('LightGBM Features fold 5')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-05.png')","bfd77fcd":"X_train.drop(drop_add,axis=1, inplace=True)\nX_test.drop(drop_add, axis=1, inplace=True)\n\nX_train.head()","fe9e5b68":"%%time\nfrom sklearn.model_selection import TimeSeriesSplit\n\nif useLGBM :\n    #folds = KFold(n_splits=5, shuffle=True, random_state=15)\n    folds = TimeSeriesSplit(n_splits =5)\n    oof = np.zeros(len(X_train))\n    #categorical_columns = [c for c in cat_cols ]\n    features = [c for c in X_train.columns]\n    predictions = np.zeros(len(X_test))\n    start = time.time()\n    feature_importance_df = pd.DataFrame()\n    start_time= time.time()\n    score = [0 for _ in range(folds.n_splits)]\n\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train, y_train)):\n        print(\"fold n\u00b0{}\".format(fold_))\n        trn_data = lgb.Dataset(X_train.iloc[trn_idx][features],\n                           label=y_train.iloc[trn_idx]#,\n                           #categorical_feature = categorical_columns\n                          )\n        val_data = lgb.Dataset(X_train.iloc[val_idx][features],\n                           label=y_train.iloc[val_idx]#,\n                           #categorical_feature = categorical_columns\n                          )\n\n        num_round = 10000\n        clf = lgb.train(param,\n                    trn_data,\n                    num_round,\n                    valid_sets = [trn_data, val_data],\n                    verbose_eval=200,\n                    early_stopping_rounds = 200)\n    \n        oof[val_idx] = clf.predict(X_train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = features\n        fold_importance_df[\"importance\"] = clf.feature_importance(importance_type='gain')\n        fold_importance_df[\"fold\"] = fold_ + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n        # we perform predictions by chunks\n        initial_idx = 0\n        chunk_size = 100000\n        current_pred = np.zeros(len(X_test))\n        while initial_idx < X_test.shape[0]:\n            final_idx = min(initial_idx + chunk_size, X_test.shape[0])\n            idx = range(initial_idx, final_idx)\n            current_pred[idx] = clf.predict(X_test.iloc[idx][features], num_iteration=clf.best_iteration)\n            initial_idx = final_idx\n        predictions += current_pred \/ min(folds.n_splits, max_iter)\n   \n        print(\"time elapsed: {:<5.2}s\".format((time.time() - start_time) \/ 3600))\n        score[fold_] = metrics.roc_auc_score(y_train.iloc[val_idx], oof[val_idx])\n        if fold_ == max_iter - 1: break\n        \n    if (folds.n_splits == max_iter):\n        print(\"CV score: {:<8.5f}\".format(metrics.roc_auc_score(y_train, oof)))\n    else:\n         print(\"CV score: {:<8.5f}\".format(sum(score) \/ max_iter))\n","c25b6648":"#sample_submission['isFraud'] = 0.25 * y_preds_xgb +0.25 *y_preds_xgb1 +0.25 *y_preds_xgb2 +0.25*predictions\nsample_submission['isFraud'] = predictions\nsample_submission.to_csv('simple_ensemble.csv')","0d56ac82":"##### Note : Drop card1 and card2 ? The frequency of data is slightly different ","f8d967d0":"#### id 7,13,18,19,21,24,26,31 seems to have problem in distribution","e70222cf":"#### Bucket the columns for easier use in future","568de78d":"#### Note : Except for D3,5,7,9,13 others have larges variance between train and test with respect to value counts . Can we drop those ?","7a83cd24":"*#### Does not seem to have much problem for now . Since based on the latest clarification it was created as features by DS community of the organizers , we can probably keep them . Just few check on V201 and V258 , seen in the last version that they were important features by LGBM*","9cace28e":"*#### Why dont mcols give an output. Need to check .*","aa3c61ed":"### This kernel so far is almost copy from the below Kernels so of you want upvote those Kernels.\n### I am just trying to learn my first ML project \n#### Update 1: Today I have learnt a little bit about ensembling and stacking . Tried to apply those concepts in this data set . It didnt do anything better now , but helped me understand the concepts a bit with a working code .\n\n#### I referred to some existing solution for this , however I forgot which kernel it was , anyone has idea , they can point out and i will add due credit . \n\nhttps:\/\/www.kaggle.com\/artkulak\/ieee-fraud-simple-baseline-0-9383-lb\n\nNanashi : https:\/\/www.kaggle.com\/jesucristo\/fraud-complete-eda\n\nXhulu's kernel: https:\/\/www.kaggle.com\/xhlulu\/ieee-fraud-xgboost-with-gpu-fit-in-40s","6f51bec0":"*#### Newly created features seem to be okay*","48301d2d":"#### The below code for value density comparison of train and test are taken from Chris Deottes kernel for Malware detection","5c544a42":"#### *Does not seem to be a big problem as of now .*","6e20ccc3":"*#### Note: Nothing interesting here .*"}}