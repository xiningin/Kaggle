{"cell_type":{"b8b00242":"code","2d69d346":"code","b4df86bb":"code","297c0cde":"code","3ae6c259":"code","f86e508e":"code","a9ba9199":"code","bded9671":"code","b4f09469":"code","582610a5":"code","39da034c":"code","50a0bf48":"code","7c10e75f":"code","892dc2e1":"code","eef58ae6":"code","25ec82c7":"code","b7fb1857":"markdown","ca024b76":"markdown","3b5d51e6":"markdown","804a015c":"markdown","ba3ead54":"markdown","3ac44c13":"markdown","fe273ec3":"markdown","07e4a7f7":"markdown","438ecc18":"markdown"},"source":{"b8b00242":"# Setup Configuration \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# Import required libraries \nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport seaborn as sns # statistical data vizualization\nimport matplotlib.pyplot as plt \n\n\n# for encoding categorical columns \nfrom sklearn.preprocessing import OrdinalEncoder , OneHotEncoder\n\n# to prepare pipeline \nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer \n\n# ML model selection \nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import train_test_split\n\n# error metric \nfrom sklearn.metrics import mean_squared_error \n","2d69d346":"from lightgbm import LGBMRegressor","b4df86bb":"# load training dataset \ntrain = pd.read_csv('..\/input\/30-days-of-ml\/train.csv', index_col=0)\ntrain.describe()  # describe numerical\/continuous variables ","297c0cde":"train.describe(include='object')  # describe categorical variables ","3ae6c259":"print(f\"No of rows = {train.shape[0]} , No of columns = {train.shape[1]}\\n\")\n\nprint(\"NULL Values count : \\n\")\nprint(train.isnull().sum())\nprint(\"\\nNo Null values in dataset. Cool !!!\")\ntrain.head()  # first 5 rows ","f86e508e":"num_columns = [col for col in train.columns if train[col].dtype=='float64'] # numerical columns\ncat_columns = [col for col in train.columns if train[col].dtype=='object']  # categorical columns ","a9ba9199":"num_corr_matrix = train[num_columns].corr()\n\nfor row_col in num_columns : \n    for column_col in num_columns : \n        if (row_col != column_col) : \n            correlation_coeff = num_corr_matrix[row_col][column_col]\n            if (abs(correlation_coeff) > 0.5) : \n                print(row_col, column_col, correlation_coeff)\n\n# definitely numerical columns are not highly correlated as nothing has printed ","bded9671":"# encode categorical columns as per observations we made above  i.e. \n# do ordianl encoding for `cat9` and one-hot for all other categorical columns \n\nOH_encoder = OneHotEncoder(handle_unknown = 'ignore', sparse=False)\nordinal_encoder = OrdinalEncoder()\n\nlow_cardinality_cols = [ col for col in cat_columns if train[col].nunique() < 10 ]\nhigh_cardinality_cols = [col for col in cat_columns if train[col].nunique() >=10 ]\n\nOH_cols_train = pd.DataFrame( OH_encoder.fit_transform(train[low_cardinality_cols]))\nOH_low_cardinality_cols = OH_encoder.get_feature_names(low_cardinality_cols)\nOH_cols_train.columns = OH_low_cardinality_cols \n\n# reset index \nOH_cols_train.index = train.index \n\n# ordinal encoding for high cardinality columns\ntrain[high_cardinality_cols] = ordinal_encoder.fit_transform(train[high_cardinality_cols]) \n\n\n# drop low cardinality  categorical columns  and update `train` by replacing categorical columns with encoded columns \nupdated_train = train.drop(low_cardinality_cols, axis=1)\nupdated_train = pd.concat([updated_train, OH_cols_train], axis=1)\n\n# updated_train.head()","b4f09469":"cat_corr_matrix  = updated_train[OH_low_cardinality_cols].corr()\n# print(cat_corr_matrix)\nfor row_col in OH_low_cardinality_cols : \n    for column_col in OH_low_cardinality_cols : \n        if (row_col != column_col) : \n            correlation_coeff = cat_corr_matrix[row_col][column_col]\n            if (abs(correlation_coeff) > 0.8) : \n                print(row_col, column_col, correlation_coeff )\n","582610a5":"y_train = updated_train.target\nX_train = updated_train.drop('target', axis=1)\n\n\n# preprocess test data \ntest = pd.read_csv('..\/input\/30-days-of-ml\/test.csv', index_col=0)\nOH_cols_test = pd.DataFrame(OH_encoder.transform(test[low_cardinality_cols]), \n                            columns= OH_low_cardinality_cols)\n# reset index \nOH_cols_test.index = test.index \n\ntest[high_cardinality_cols] = ordinal_encoder.transform(test[high_cardinality_cols])\n\nupdated_test = test.drop(low_cardinality_cols, axis=1)\nX_test = pd.concat([updated_test, OH_cols_test], axis=1)\nX_test.head()\n","39da034c":"# Analysing each variable -- \n\n# First  continuous variables \n\nsns.set(rc={'figure.figsize':(20,15)})\n\n# print(len(num_columns))\nfig, axs = plt.subplots( nrows=3,ncols=5) \n\nfor i in range(3) : \n    for j in range(5) : \n        sns.kdeplot(updated_train[num_columns[i*5+j]], shade=True, ax=axs[i][j] )\n        \n        # uncomment to vizualize test data . \n#         if (num_columns[i*5+j]=='target') : \n#             continue \n#         sns.kdeplot(X_test[num_columns[i*5+j]], shade=True, ax=axs[i][j] )  \n\n","50a0bf48":"# log-transform 'cont4'\n\nX_train['cont4'] = np.log(X_train['cont4'])\nX_test['cont4'] = np.log(X_test['cont4'])","7c10e75f":"# lets normalize continuos data variables \n\nfor col in num_columns : \n    if (col == 'target') : \n        continue \n    mean = X_train[col].mean()\n    std  = X_train[col].std()\n    X_train[col] = (X_train[col] - mean)\/std \n    X_test[col] = (X_test[col] - mean)\/std \n","892dc2e1":"# Vizualizing each continuous variable after transformation. \n# You will notice still no change in pattern but peaks have dropped significantly\n\n# First  continuous variables \n\nsns.set(rc={'figure.figsize':(20,15)})\n\n# print(len(num_columns))\nfig, axs = plt.subplots( nrows=3,ncols=5) \n\nfor i in range(3) : \n    for j in range(5) : \n        \n        if (num_columns[i*5 + j]=='target') : \n            continue \n        sns.kdeplot(X_train[num_columns[i*5+j]], shade=True, ax=axs[i][j] )\n#         sns.kdeplot(X_test[num_columns[i*5+j]], shade=True, ax=axs[i][j] )  ","eef58ae6":"exclude_columns = low_cardinality_cols + high_cardinality_cols\nfeatures = [col for col in X_train.columns if col not in exclude_columns]\n\nX_train_features = X_train[features]\n\ntrain_X, valid_X, train_y , valid_y = train_test_split(X_train_features, y_train, \n                                                       train_size = 0.8, random_state=0)\n\n# model 1 \n\nmodel = XGBRegressor(n_estimators = 100, learning_rate = 0.05)\nmodel.fit(\n         X_train_features, y_train, \n         early_stopping_rounds=5, \n         eval_set = [(valid_X, valid_y)], \n         verbose=False\n         ) \npreds_valid = model.predict(valid_X)\nerror = mean_squared_error(valid_y, preds_valid, squared=False)\n\nprint(error)\n\n","25ec82c7":"# Use the model to generate predictions\npredictions = model.predict(X_test[features])\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'target': predictions})\noutput.to_csv('submission.csv', index=False)","b7fb1857":"## Prepare Data ","ca024b76":"## Import libraries \n\nFirst of all we will import all libraries which will be used in this notebook","3b5d51e6":"**Some Observations**\n\n1. Means of all continuous variables are close enough to each other\n2. similarly, standard deviation are almost close to each other for all continuous variable \n3. Category 'cat9' has highest cardinality ( so this is more intuitive right now to do ordinal encoding for this column and for rest categorical column , will do one-hot encoding. \n","804a015c":"**Observations**\n\n1. clearly almost all target values are greater than 7 . \n2. all continuous variable are lying between 0 & 1 . and almost all data have more than 1 major peaks.\n3. 'cont4' is highly left-skewed. we can take account of low-frequency values by means of log- transformation on this column \n4. X_test has similar distribution of continuous variable as X_train . so above idea of log-transformation may work . ","ba3ead54":"## EDA - Exploratory Data Analysis\n\nHere we will do preliminary EDA on training data only","3ac44c13":"## Load Data\n\nRead given dataset and see summary statistics ","fe273ec3":"## Generate Submission ","07e4a7f7":"## Build Model","438ecc18":"**Observations**\n\n`see no of unique values in these categories already found initially when we use describe() method of pandas dataframe.`\n\n1. cat0_A and cat0_B are highly negatively correlated. reason being there are only two categories . similarly for cat1 and cat2 and cat3 \n\n2. cat5_D and cat5_B high correlation seems pretty interesting .\n3. cat6_A and cat6_B high correlation also is interesting . "}}