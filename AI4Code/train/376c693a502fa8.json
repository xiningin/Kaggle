{"cell_type":{"af1929dd":"code","3eb44418":"code","ca593c1b":"code","41119d1a":"code","f5a87aaa":"code","1164e049":"code","036c5496":"code","6ad0b277":"code","824d266e":"code","10962b8b":"code","b378cc14":"code","c9da1f5f":"code","83bf8d2c":"code","a0736411":"code","56afb274":"code","0499d746":"code","63996f1d":"code","b87a159f":"code","47065f5e":"markdown","a4e61259":"markdown","7496f2ea":"markdown","62c87a9c":"markdown","bc407326":"markdown","1eed82e9":"markdown","147c3eae":"markdown","4e9e5ddb":"markdown","ae8af30d":"markdown","b84d85c5":"markdown","598a9c84":"markdown","9903ff7a":"markdown","cd38f0cc":"markdown"},"source":{"af1929dd":"import h5py\nimport numpy as np\n\ndef load_dataset():\n    train_dataset = h5py.File('..\/input\/residualnetworks\/train_signs.h5', \"r\")\n    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n\n    test_dataset = h5py.File('..\/input\/residualnetworks\/test_signs.h5', \"r\")\n    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n\n    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n    \n    train_set_y_orig = train_set_y_orig.reshape((train_set_y_orig.shape[0]),1)\n    test_set_y_orig = test_set_y_orig.reshape((test_set_y_orig.shape[0],1))\n    \n    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig\n\n\n","3eb44418":"#load your dataset here\ntrain_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig = load_dataset()","ca593c1b":"import matplotlib.pyplot as plt\n%matplotlib inline","41119d1a":"# Plot here \nTRAIN_SET_LEN = len(train_set_x_orig)\n\n# Generate random indicies here\nprint('Length of training set: ', TRAIN_SET_LEN)\nfrom random import randrange\nrandom_indicies = [randrange(TRAIN_SET_LEN) for x in range(0, 20)]\nprint(random_indicies)\n\n# Visualize random images in tranining set\nw=20\nh=20\nfig=plt.figure(figsize=(16, 20))\ncolumns = 4\nrows = 5\nfor i in range(1, columns*rows +1):\n  index = random_indicies[i-1]\n  img = train_set_x_orig[index]\n  fig.add_subplot(rows, columns, i)\n  plt.imshow(img)\n  plt.title('this image belongs to class number {}'.format(train_set_y_orig[index]))\nplt.show()","f5a87aaa":"# Show dimension of dataset\nTRAIN_SET_LEN = len(train_set_x_orig)\nTEST_SET_LEN = len(test_set_x_orig)\nTRAIN_SET_X_SHAPE = train_set_x_orig.shape\nTEST_SET_X_SHAPE = test_set_x_orig.shape\nX_SHAPE = train_set_x_orig[0].shape\nY_SHAPE = train_set_y_orig[0].shape\n\nprint('TRAIN_SET_LEN: ', TRAIN_SET_LEN)\nprint('TEST_SET_LEN: ', TEST_SET_LEN)\nprint('TRAIN_SET_X_SHAPE: ', TRAIN_SET_X_SHAPE)\nprint('TEST_SET_X_SHAPE: ', TEST_SET_X_SHAPE)\nprint('X_SHAPE: ', X_SHAPE)\nprint('Y_SHAPE: ', Y_SHAPE)","1164e049":"# Solve Here\ndef normalize_set(x):\n  return x \/ 255\n\ntrain_set_x_nlz = normalize_set(train_set_x_orig)\ntest_set_x_nlz = normalize_set(test_set_x_orig)","036c5496":"# solve here\ndef flatten_x(x):\n  d1, d2, d3, d4 = x.shape\n  return x.reshape((d1, d2*d3*d4))\n\ntrain_set_x = flatten_x(train_set_x_nlz)\ntest_set_x = flatten_x(test_set_x_nlz)\nprint(train_set_x.shape)\nprint(test_set_x.shape)","6ad0b277":"# Solve here\nfrom sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=0).fit(train_set_x, train_set_y_orig)\nclf.score(test_set_x, test_set_y_orig)","824d266e":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.optimizers import Adam ,RMSprop, Nadam\nfrom keras import  backend as K\nfrom keras.preprocessing import image","10962b8b":"# Solve Here\nmodel = Sequential([\n    Dense(10, activation='relu'),\n    Dense(8, activation='relu'),\n    Dense(6, activation='softmax')\n    ])","b378cc14":"NB_EPOCH = 320\nVERBOSE = 1\nBATCH_SIZE = 100\nVALIDATION_SPLIT=0.1","c9da1f5f":"# Solve Here\nmodel.compile(loss='SparseCategoricalCrossentropy', optimizer = Adam(lr=0.0001), metrics=['accuracy'])","83bf8d2c":"history = model.fit(train_set_x, train_set_y_orig, batch_size=BATCH_SIZE, epochs=NB_EPOCH,verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n","a0736411":"score = model.evaluate(test_set_x, test_set_y_orig, verbose=VERBOSE)","56afb274":"# Solve Here\n# Solve Here\nmodel2 = Sequential([\n    Dense(896, activation='relu'),\n    Dense(512, activation='relu'),\n    Dense(256, activation='relu'),\n    Dense(128, activation='relu'),\n    Dense(64, activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(16, activation='softmax')\n    ])\n# Solve Here\nmodel2.compile(loss='SparseCategoricalCrossentropy', optimizer = Adam(lr=0.0001), metrics=['accuracy'])\nhistory2 = model2.fit(train_set_x, train_set_y_orig, batch_size=BATCH_SIZE, epochs=NB_EPOCH,verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\nscore2 = model2.evaluate(test_set_x, test_set_y_orig, verbose=VERBOSE)","0499d746":"# Solve Bonus Here\n# Solve Here\n# Solve Here\nmodel3 = Sequential([\n    Dense(1024, activation='relu'),\n    Dense(512, activation='relu'),\n    Dropout(0.3),\n    Dense(256, activation='relu'),\n    Dense(128, activation='relu'),\n    Dropout(0.2),\n    Dense(64, activation='relu'),\n    Dense(32, activation='relu'),\n    Dropout(0.1),\n    Dense(16, activation='relu'),\n    Dense(8, activation='softmax')\n    ])\n# Solve Here\nmodel3.compile(loss='SparseCategoricalCrossentropy', optimizer = Nadam(lr=0.0001), metrics=['accuracy'])\nhistory3 = model3.fit(train_set_x, train_set_y_orig, batch_size=BATCH_SIZE, epochs=360,verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\nscore3 = model3.evaluate(test_set_x, test_set_y_orig, verbose=VERBOSE)","63996f1d":"# Solve Bonus Here\n# Solve Here\n# Solve Here\nmodel3 = Sequential([\n    Dense(4096, activation='relu'),\n    Dense(2048, activation='relu'),\n    Dense(1024, activation='relu'),\n    Dense(512, activation='relu'),\n    Dropout(0.3),\n    Dense(256, activation='relu'),\n    Dense(128, activation='relu'),\n    Dropout(0.2),\n    Dense(64, activation='relu'),\n    Dense(32, activation='relu'),\n    Dropout(0.1),\n    Dense(16, activation='softmax'),\n    ])\n# Solve Here\nmodel3.compile(loss='SparseCategoricalCrossentropy', optimizer = Nadam(lr=0.0001), metrics=['accuracy'])\nhistory = model3.fit(train_set_x, train_set_y_orig, batch_size=128, epochs=420,verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\nscore = model3.evaluate(test_set_x, test_set_y_orig, verbose=VERBOSE)","b87a159f":"# Solve Bonus Here\n# Solve Here\n# Solve Here\nmodel4 = Sequential([\n    Dense(8192, activation='relu'),\n    Dense(4096, activation='relu'),\n    Dropout(0.2),\n    Dense(2048, activation='relu'),\n    Dense(1024, activation='relu'),\n    Dense(512, activation='relu'),\n    Dropout(0.2),\n    Dense(256, activation='relu'),\n    Dense(128, activation='relu'),\n    Dropout(0.15),\n    Dense(64, activation='relu'),\n    Dense(32, activation='relu'),\n    Dropout(0.1),\n    Dense(16, activation='softmax'),\n    ])\n# Solve Here\nmodel4.compile(loss='SparseCategoricalCrossentropy', optimizer = Nadam(lr=0.0001), metrics=['accuracy'])\nhistory = model4.fit(train_set_x, train_set_y_orig, batch_size=100, epochs=420,verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\nscore = model4.evaluate(test_set_x, test_set_y_orig, verbose=VERBOSE)","47065f5e":"How many classes does the training and test dataset have?","a4e61259":"As you can see in the dimension (number_images X length X width X RGB), each image has 3 dimensions .. number of pixels in length and width as well as RGB contrast. We needed to flatten both the training and the test dataset so that we can feed to our ML models. To flatten an image, check the following link. https:\/\/stackoverflow.com\/questions\/36967920\/numpy-flatten-rgb-image-array","7496f2ea":"Now we need to normalize the training and test dataset. The pixels are integers. We need them to be float between 0 and 1. You need to divide each of the two matrices by 255. That will produce a normalized image datasets","62c87a9c":"Now you can apply some old fashion ML. Start with a baseline model such as logistic regression. Fit the model, then evaluate the model by calculating the accuracy and the confusion matrix for the training data and test data. Note, fitting the model may take longer than a minute","bc407326":"Generate 20 random numbers between 0 and the length of the training dataset. Plot 20 images from the dataset using these 20 random numbers as indicies to the training dataset. You will need to plot them on 5 rows and 4 columns. In addition, each image will have a title that says `this image belons to class number X`, replace X by the correct class number from the training labels.","1eed82e9":"# HW4\nWe will be working on a very cool problem which is classifying hand signs. In each image the hand is holding certain number of fingers up. The number of fingers held by the hand is the class number. The original dataset is in a format call h5 format. It is an efficient format to store and organize large amounts of data. The following function `load_dataset()` is ready for you to use. It will return four variables in the following order: X_train, Y_train, X_test, Y_test. Hence, your data is already splitted and there is no need for you to split it.\n\n","147c3eae":"load the dataset ","4e9e5ddb":"Given the learning rate used in the previous part, Build a new model and increase the number of layers and neurons .. maybe 7 or 8 layers with number of neurons between 200 and 1000. Do you see a difference in the accuracy?\n\n\nWhat is the training and validation accuracy and confusion matrix for both?\n\n**note: it may take some time to run**","ae8af30d":"### Learning Rate Optimizing\nDid you notice that the it was not or learning was a little slow at the beginning? That is because of the keras learning rate is being high by default. Let's play around with the learning rate to give the DNN more change to learn. Change the compiling line for the model to `model.compile(loss='SparseCategoricalCrossentropy', optimizer = tf.keras.optimizers.Adam(lr=0.0001), metrics=['accuracy'])` so that you can decrease the learning rate. Moreover, this is a good chance to read about the different optimizers type. Check this link.\nhttps:\/\/medium.com\/datadriveninvestor\/overview-of-different-optimizers-for-neural-networks-e0ed119440c3\n\nRun your network for at least 300 epochs and use batch_size=100\n\nWhat is the training and validation accuracy and confusion matrix for both?","b84d85c5":"## Build a Baseline Model","598a9c84":"# Build a DNN \nUse Keras to build a small neural network. Number of hidden layers is less than 5 and each layer should have between 5 and 10 neurons. The loss for a multiclass keras problem is called `SparseCategoricalCrossentropy`. Notice that, we already have a flattening layer in DNN, so you can actually use the normalized image without the flattening step that you have done earlier when you were building logistic regression","9903ff7a":"What is the dimension of the training dataset and test dataset?","cd38f0cc":"**Bonus**\n\nSee if you can change some of the parameters so that some of the epochs can reach a validation accuracy of at least 92%"}}