{"cell_type":{"ba01768c":"code","9134f054":"code","29c4de65":"code","7939235e":"code","f4eb659d":"code","3232cc93":"code","5486d9d3":"code","1c954be5":"code","33d53b8d":"code","b91a030a":"code","3fb6345a":"code","7521277b":"code","2edced7a":"code","98985277":"code","ba878915":"code","de29f7fa":"code","bbf75bec":"code","fdfea2b9":"code","0bd99027":"code","3a151aca":"code","ff5a8a60":"code","87b5dd27":"code","a3ec3450":"code","22ee4bae":"code","f88d441e":"code","dd5818cf":"code","85d1be8d":"code","dcf2c3c7":"code","7381f219":"code","385d3bd8":"code","3e289bcb":"code","6b78875d":"code","44200158":"code","3d9f65d6":"code","d3a854e0":"code","39642559":"code","705491c0":"code","6b019b61":"code","b25e9ad6":"code","a2e43d41":"code","92f6c79f":"code","32f033f3":"markdown"},"source":{"ba01768c":"import pandas as pd\nimport numpy as np\nimport csv\nimport os\nimport random\nimport torch\nimport torch\ntorch.backends.cudnn.benchmark = True\ntorch.autograd.set_detect_anomaly(False)\ntorch.autograd.profiler.profile(False)\ntorch.autograd.profiler.emit_nvtx(False)\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport transformers\nimport sklearn.model_selection as model_selection\nfrom transformers import BertForSequenceClassification, BertTokenizerFast\nimport time\nimport copy\nfrom tqdm.notebook import tqdm","9134f054":"def set_seed(seed=42):\n    os.environ['PYTHONHASHSEED']=str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)","29c4de65":"df=pd.read_csv(\"..\/input\/amazon-ml-challenge-2021-hackerearth\/train.csv\", escapechar = \"\\\\\", quoting = csv.QUOTE_NONE)","7939235e":"num_labels=df[\"BROWSE_NODE_ID\"].nunique()","f4eb659d":"id2lbl={lbl: idx for idx,lbl in enumerate(list(df[\"BROWSE_NODE_ID\"].unique()))}\nlbl2id={lbl:idx for idx,lbl in id2lbl.items()}","3232cc93":"set_seed()","5486d9d3":"df[\"BROWSE_NODE_ID\"]=df[\"BROWSE_NODE_ID\"].map(id2lbl)","1c954be5":"def create_folds(data, num_splits):\n    data[\"kfold\"] = -1\n    data = data.sample(frac=1).reset_index(drop=True)\n    y=df[\"BROWSE_NODE_ID\"]\n    kf = model_selection.StratifiedKFold(n_splits=num_splits)\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=y)):\n        data.loc[v_, 'kfold'] = f\n    return data","33d53b8d":"df=create_folds(df, 5)","b91a030a":"df=df.loc[df.kfold.isin([0,2])]\ndf=df.reset_index(drop=True)\ndf.head()","3fb6345a":"df.loc[df['BROWSE_NODE_ID']==1045].head()","7521277b":"df.info()","2edced7a":"df.isnull().sum()","98985277":"temp=df.dropna(subset=['TITLE'])\ntemp=temp.reset_index(drop=True)","ba878915":"temp.isnull().sum()","de29f7fa":"total_words=0\nfor title in temp['TITLE']:\n    total_words+=len(title.split())","bbf75bec":"total_words\/len(temp)","fdfea2b9":"temp=df.dropna(subset=['DESCRIPTION'])\ntotal_words=0\nfor title in temp['DESCRIPTION']:\n    total_words+=len(title.split())\ntotal_words\/len(temp)","0bd99027":"temp=df.dropna(subset=['TITLE'])\ntemp=temp.fillna(\" \")","3a151aca":"temp.head()","ff5a8a60":"\" \".join(temp[\"BULLET_POINTS\"][0].split(\",\"))[:-1][1:]","87b5dd27":"wholeSentence=[]\nfor idx,row in temp.iterrows(): \n    if(idx%100000==0):\n        print(f\"{idx} Done\")\n    wholeSentence.append(row[0]+row[1]+\" \".join(row[2].split(\",\"))[:-1][1:])","a3ec3450":"temp[\"WHOLE SENTENCE\"]=wholeSentence","22ee4bae":"temp=temp.reset_index(drop=True)","f88d441e":"temp.head()","dd5818cf":"dev=torch.device('cuda')","85d1be8d":"temp[\"BROWSE_NODE_ID\"].value_counts()","dcf2c3c7":"train_text, val_text, train_labels, val_labels = train_test_split(temp['WHOLE SENTENCE'], temp['BROWSE_NODE_ID'],\n                                                                    test_size=0.05)","7381f219":"train_text=train_text.reset_index(drop=True)\ntrain_labels=train_labels.reset_index(drop=True)\nval_text=val_text.reset_index(drop=True)\nval_labels=val_labels.reset_index(drop=True)","385d3bd8":"tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')","3e289bcb":"seq_len = [len(i.split()) for i in train_text]\n\npd.Series(seq_len).hist(bins = 30)","6b78875d":"max_seq_len = 64","44200158":"class amazonDataset(Dataset):\n  def __init__(self,text,label,tokenizer):\n    self.sentence=text\n    self.label=label\n    self.tokenizer=tokenizer\n\n  def __len__(self):\n    return len(self.sentence)\n  \n  def __getitem__(self,idx):\n    inp_tokens=self.tokenizer.encode_plus(self.sentence[idx], \n                                          padding=\"max_length\", \n                                          add_special_tokens=True,\n                                          max_length=max_seq_len, \n                                          truncation=True)\n    inp_id=inp_tokens.input_ids\n    inp_mask=inp_tokens.attention_mask\n    inp_type_ids=inp_tokens.token_type_ids\n    labels=self.label[idx]\n\n    return {\n#         \"text\":self.sentence,\n        \"input_ids\":torch.tensor(inp_id, dtype=torch.long),\n        \"input_attention_mask\":torch.tensor(inp_mask, dtype=torch.long),\n        \"input_type_ids\":torch.tensor(inp_type_ids, dtype=torch.long),\n        \"labels\":torch.tensor(labels, dtype=torch.float)\n    }","3d9f65d6":"train_dataset = amazonDataset(train_text, train_labels, tokenizer)\nval_dataset = amazonDataset(val_text, val_labels, tokenizer)","d3a854e0":"train_dataloader=DataLoader(train_dataset,\n                            batch_size=128,\n                            shuffle=True,\n                            num_workers=2,\n                           pin_memory=True)\nval_dataloader=DataLoader(val_dataset,\n                            batch_size=128,\n                            shuffle=False,\n                            num_workers=2,\n                           pin_memory=True)","39642559":"dataloaders={'train':train_dataloader, 'eval':val_dataloader }\ndataset_sizes={'train':len(train_dataset), 'eval':len(val_dataset)}","705491c0":"# class BERTBaseUncased(nn.Module):\n#     def __init__(self):\n#         super(BERTBaseUncased, self).__init__()\n#         self.bert=AutoModel.from_pretrained('bert-base-uncased')\n#         self.dropout = nn.Dropout(0.1)\n#         self.relu =  nn.ReLU()\n#         self.fc1 = nn.Linear(768,9919)\n        \n#     def forward(self,ids,mask,token_type_ids):\n#         a, o2 = self.bert(\n#             ids,\n#             attention_mask=mask,\n#             token_type_ids=token_type_ids)\n#         bo=self.dropout(o2)\n#         output=self.fc1(bo)\n#         return output","6b019b61":"model=BertForSequenceClassification.from_pretrained('bert-base-uncased',\n                                                    num_labels=9919)\nprint(model)\nmodel=torch.load(\"..\/input\/amazon-ml-models\/BERTBaseBaselineNoset.pth\")\nmodel.to(dev)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\nexp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)","b25e9ad6":"def train_fn(model,loss_fn,optimizer,scheduler,num_epochs=1):\n    since=time.time()\n    best_wts=copy.deepcopy(model.state_dict())\n    best_loss=float('inf')\n    for epoch in range(num_epochs):\n        print(f'Epoch:{epoch}\/{num_epochs}')\n        print('-'*10)\n        \n        for mode in ['train','eval']:\n            if mode=='train':\n                model.train()\n            elif mode=='eval':\n                model.eval()\n            \n            running_loss=0.0\n            running_corrects=0.0\n            \n            for data in tqdm(dataloaders[mode]):\n                input_ids = data[\"input_ids\"].to(dev, dtype=torch.long)\n                labels = data['labels'].to(dev, dtype=torch.long)\n                mask = data[\"input_attention_mask\"].to(dev, dtype=torch.long)\n                token_type_ids = data['input_type_ids'].to(dev, dtype=torch.long)\n            \n                optimizer.zero_grad()\n                with torch.set_grad_enabled(mode=='train'):\n                    outputs=model(\n                                input_ids =input_ids,\n                                attention_mask=mask,\n                                token_type_ids=token_type_ids,\n                                labels=labels\n                            )\n                    loss, logits=outputs.loss, outputs.logits\n                    _,preds=torch.max(logits,1)\n                    \n                    if mode=='train':\n                        loss.backward()\n                        optimizer.step()                    \n                    running_loss += loss.item()                    \n                    running_corrects += torch.sum(preds == labels.data)\n\n            if mode == 'train':\n                scheduler.step()\n                \n            epoch_loss=running_loss\/dataset_sizes[mode]\n            epoch_accuracy=running_corrects.double()\/dataset_sizes[mode]\n            \n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                mode, epoch_loss, epoch_accuracy))\n            \n            if mode=='eval' and epoch_loss<best_loss:\n                best_wts=copy.deepcopy(model.state_dict())\n                best_acc=epoch_accuracy\n                best_loss=epoch_loss\n            \n            print()\n\n        time_elapsed = time.time() - since\n        print('Training complete in {:.0f}m {:.0f}s'.format(\n            time_elapsed \/\/ 60, time_elapsed % 60))\n        print('Best val loss: {:4f}'.format(best_loss))\n        print('Best val Acc: {:4f}'.format(best_acc))\n    \n        model.load_state_dict(best_wts)\n    return model","a2e43d41":"model = train_fn(model, \n               criterion, \n               optimizer, \n               exp_lr_scheduler,\n               num_epochs=5)","92f6c79f":"torch.save(model,\"BERTBaseBaselineNoset.pth\")","32f033f3":"# BERT"}}