{"cell_type":{"acac8f1c":"code","ed284f6c":"code","11182e9e":"code","49c436ad":"code","a4171194":"markdown","7ee8bce6":"markdown","2c1e5acb":"markdown","b7b8ea53":"markdown","a37664d4":"markdown","b2bfafea":"markdown","d8004910":"markdown","421657c3":"markdown","1f5f4228":"markdown"},"source":{"acac8f1c":"import pandas as pd\nfrom sklearn import preprocessing\nimport random\nimport numpy\nimport warnings\nimport ruleset as rs\n\n\nwarnings.filterwarnings('ignore')\n\nrandom.seed(42)\nnumpy.random.seed(42)","ed284f6c":"df = pd.read_csv('..\/input\/kaggle-survey-2019\/multiple_choice_responses.csv')\n#example data\ndf[['Q1','Q2','Q3','Q4','Q5','Q6','Q7', 'Q8']].head()","11182e9e":"df['Q24_Part_4'].value_counts().sort_index(ascending=False)","49c436ad":"# load dataset, remove first row (=question text)\ndf = pd.read_csv('..\/input\/kaggle-survey-2019\/multiple_choice_responses.csv').iloc[1:]\n# replace the underscore in the dataset, just for to get a better output\ndf.columns = df.columns.str.replace('_', '')\n# unknown for all NaN values, we will discriminate between 'Bayesian approach' & 'Unknown' in column Q24_Part_4\ndf.fillna('unknown', inplace=True)\n\n# X: training set, \n# !!!! we use a predefined set of columns to speed up the execution time !!!! (normally use .drop(['Q24Part4', , axis=1))\nX = df[['Q9Part5', 'Q18OTHERTEXT', 'Q19', 'Q24Part5', 'Q18Part2', 'Q8', 'Q24Part9', 'Q29Part10']]\n# change dtype of columns to string (categorical)\nfor col in X.columns:\n    X[col] = X[col].apply(str)\n\n# encode label\nle = preprocessing.LabelEncoder()\n# we want rules specifying the 'Bayesian approach' community\ny = 1-le.fit_transform(df.Q24Part4)\n\n# let's mine some rules, limit to max 5 conjunctive rules of max length 5\n# https:\/\/github.com\/zli37\/bayesianRuleSet\nmodel = rs.BayesianRuleSet(method='forest', max_iter=10000, maxlen=5, max_rules=5)\nmodel.fit(X, y)","a4171194":"To find interpretations in the data, an exciting dataset must be provided. The *multiple_choice_responses* table was used in this perspective because: \n* Each row is similar to a single user, answering multiple questions.\nIf we want to learn interpretations over multiple users, this table format is desirable.\n* Almost all values in the cells are categorical, which is preferable to discriminate.","7ee8bce6":"Dataset loaded, the target column is chosen, time to define our methodology:\n<h3> Association rule mining <\/h3>\nApplying association rule mining is possible here due to the nature of the data.<br>\nGiven a set of answers on questions, association rule mining will try to find rules that will predict the occurrence of an answer on a question based on the answers of the other questions in the set. While this is maybe rather hard to understand in the setting of this particular dataset with answers and questions, it is more clear in the context of basket analysis where your grocery store tries to find patterns in customer behaviour.\n<img src=\"https:\/\/pbs.twimg.com\/media\/CSS1Q6TUwAAdGnX?format=png&name=small\">\n<h3> Bayesian association rule mining <\/h3>\n\nOk, one extra element: some Bayesian knowledge <br>\nAssociation rule mining is useful when you want many rules over the whole dataset. I want only rules for column Q24_Part_4.\n    Whether or not a rule is beneficial for our case can be verified using some sort of a naive Bayes classifier. When a rule is mined, each row is checked given that rule using Bayes rule:\n$$P(\\text{A}\\text{ }|\\text{ }B) = \\frac{P(B\\text{ }|\\text{ }\\text{A})\\text{ }x\\text{ }P(\\text{A})}{P(B)}$$\nwhere we try to classify people using Bayesian Approaches (A) given the data in our rule (B)\n<center><img src=\"https:\/\/i.pinimg.com\/originals\/d8\/fe\/9d\/d8fe9dd4f6e81aee6bb56ed1a234dd0d.png\"><\/center>","2c1e5acb":"<center><img src=\"https:\/\/www.invespcro.com\/blog\/images\/blog-images\/Featured.1.jpg\"><\/center>","b7b8ea53":"While it is not impossible, generating explanations for this whole dataset would be a little bit harsh. Therefore, we will select a specific column from which we want to discriminate the users. <br>\n\n<center>**More in general, we will try to find rules which can discriminate users based on a single column value given the information in the other columns. <br>Column Q24_Part_4 will be used, because I want to know how you specify people using Bayesian approaches regularly ;)** <\/center><br>\n\n","a37664d4":"Remarks:\nAlthough we have an accuracy of 81.7% keep in mind that this is an imbalanced problem...\n\n|                     | True positives | True negatives |\n|---------------------|:--------------:|:--------------:|\n| Predicted positives |       253      |       118      |\n| Predicted negatives |      3472      |      15874     |\n\nSo this rule, found above,will only hold for 253 Bayesian users...\n\n| Measure                          | Value  | Derivations                                           |\n|----------------------------------|--------|-------------------------------------------------------|\n| Sensitivity                      | 0.0679 | TPR = TP \/ (TP + FN)                                  |\n| Specificity                      | 0.9926 | SPC = TN \/ (FP + TN)                                  |\n| Precision                        | 0.6819 | PPV = TP \/ (TP + FP)                                  |\n| Negative Predictive Value        | 0.8205 | NPV = TN \/ (TN + FN)                                  |\n| False Positive Rate              | 0.0074 | FPR = FP \/ (FP + TN)                                  |\n| False Discovery Rate             | 0.3181 | FDR = FP \/ (FP + TP)                                  |\n| False Negative Rate              | 0.9321 | FNR = FN \/ (FN + TP)                                  |\n| Accuracy                         | 0.8179 | ACC = (TP + TN) \/ (P + N)                             |\n| F1 Score                         | 0.1235 | F1 = 2TP \/ (2TP + FP + FN)                            |\n| Matthews Correlation Coefficient | 0.1744 | TP*TN - FP*FN \/ sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)) |\n\n<br>\nMore advanced iterations will probably increase the precision of the found rules. <br>\nHere, we only wanted to show the usefulness of rule mining in the area of explaining parts of the data. <br>\n\n<center>**Maybe, more interesting rules can be found for other columns?**<\/center>\n\n\n","b2bfafea":"<center><img src=\"https:\/\/i.ibb.co\/KhvCRfb\/mining-rules.png\"><\/center>","d8004910":"The output shows some iterations, but we are only interested in the last one. It states: <br>\n<center> Bayesian approaches ->  not Q18OTHERTEXT(174) ^ Q24Part5(Evolutionary Approaches) ^ not Q8(I do not know) ^ Q24Part9(Recurrent Neural Networks)<\/center> <br>\nAlternatively, the rule states that **people who use Bayesian approaches frequently do not use 174 as a programming language (regularly), follows updates on evolutionary approaches and neural nets, knows that their current employer incorporates ML**. ","421657c3":"<center><h3>TLDR: using bayesian rule mining to explain parts of the data <br> Use case: users who use Bayesian Approaches<\/h3><\/center>\n\n<br>\nVisualisations are great, and scientists use them in many ways to start exploring a dataset.\nHowever, the power of visualisation lies in how they are interpreted. Therefore, in this notebook: **can we use ML to generate interpretations directly from the data itself? **","1f5f4228":"<h2> May i have your attention please.. <\/h2>\n\nWe've used this standard code to perform Bayesian rule mining: https:\/\/github.com\/zli37\/bayesianRuleSet"}}