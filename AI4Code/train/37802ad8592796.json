{"cell_type":{"89d97e57":"code","96228577":"code","aa78f8d9":"code","6ce56325":"code","78be61c9":"code","1737e482":"code","bc7713c7":"code","6e887ea8":"code","e6e5a13f":"code","4385bbea":"code","641ecea2":"code","e63f5573":"code","8df375e3":"code","0e9128b2":"code","874aa7ba":"code","6c329b85":"code","b7a2a044":"code","b40e31a8":"code","0755b664":"code","32c37a77":"code","ee5eaff8":"code","f2464823":"code","73790310":"code","cfab926c":"code","33beeac6":"code","08a026a3":"code","77358921":"code","222ddc99":"code","9da0a81a":"code","0de8c3fe":"code","f38d9362":"code","8a93a973":"code","01ac0b3a":"code","d86d9cce":"code","9c626f48":"code","604c077e":"markdown","368dd2e5":"markdown","85cab265":"markdown","5c2d8ad9":"markdown","eb0c50a6":"markdown","90dab84c":"markdown","48525ccf":"markdown","e42fc178":"markdown","d1969dc9":"markdown","ff2ab079":"markdown","2fb127ac":"markdown","0a710ae5":"markdown"},"source":{"89d97e57":"# load data libraries\nimport numpy as np # linear algebra library\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport zipfile # to read zip files\nfrom sklearn.model_selection import train_test_split\n\n\n# data understanding libraries\nimport matplotlib.pyplot as plt # ploting library\n%matplotlib inline\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom collections import Counter\nimport seaborn as sns\n\n\n\n# data preparation\nimport re\nfrom nltk.stem import PorterStemmer\n\n\n# ADS Creation\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.preprocessing import StandardScaler\n\n# Modeling\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.naive_bayes import MultinomialNB\n\n# Evaluation and Model Selection\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import GridSearchCV","96228577":"pd.set_option('display.max_rows', 10000)\npd.set_option('display.max_columns', 500)\npd.set_option('display.precision',150)\npd.options.display.float_format = '{:,.3f}'.format","aa78f8d9":"#unzip the files\narchive_train = zipfile.ZipFile('\/kaggle\/input\/whats-cooking\/train.json.zip')\n\n#read training json file \ntrain = pd.read_json(archive_train.read('train.json'))\n\n#output the frist 5 rows\ntrain.head()","6ce56325":"train_data, test_data = train_test_split(train, test_size=0.4, random_state=1)\nval_data, test_data = train_test_split(test_data, test_size=0.5, random_state=1)\n\ntrain_data = train_data.reset_index(drop=True)\nval_data = val_data.reset_index(drop=True)\ntest_data = test_data.reset_index(drop=True)","78be61c9":"print(\"Train set size is \",len(train_data))\nprint(\"Val set size is \",len(val_data))\nprint(\"Test set size is \",len(test_data))","1737e482":"stopwords = set([\"Campbell's\",\"hellmann\",\"oz\",\"M&M\",\"Paso\u00e2\u201e\u00a2\",\"\"])\nporter = PorterStemmer()\n# lancaster=LancasterStemmer()\n\ndef ret_words(ingredients):\n    ingredients_text = ' '.join(ingredients)\n    ingredients_text = ingredients_text.lower()\n    ingredients_text = ingredients_text.replace('-', '')\n    ingredients_text = ingredients_text.replace(',', ' ')\n    words = []\n    for word in ingredients_text.split():\n        if re.findall('[0-9]', word): continue\n        if len(word) <= 2: continue\n        if '\u2019' in word: continue\n        if '\u00ae' in word: continue\n        if word in stopwords: continue\n        if re.findall('[^a-zA-Z]',re.sub(r'[^\\w\\s]','',word)): continue\n        if len(word) > 0: words.append(porter.stem(re.sub(r'[^\\w\\s]','',word)))\n    return ' '.join(words)\n\ndef preprocess(df,flag):\n    # Remove recipes with only one Ingredient\n    df[\"ingredients_num\"]=df[\"ingredients\"].apply(len)\n    if flag == 0 :\n        df = df.drop(df[df[\"ingredients_num\"]<=1].index)\n    \n    # Convert list of ingredients to string\n    df['ingredients_txt'] = df[\"ingredients\"].apply(ret_words)\n    \n    return df","bc7713c7":"train_preprocessed = preprocess(train_data,0)\nval_preprocessed = preprocess(val_data,1)\ntest_preprocessed = preprocess(test_data,1)","6e887ea8":"train_preprocessed.head()","e6e5a13f":"id_train, X_train, y_train = train_preprocessed['id'], train_preprocessed['ingredients_txt'], train_preprocessed['cuisine']\nid_val, X_val, y_val = val_preprocessed['id'], val_preprocessed['ingredients_txt'], val_preprocessed['cuisine']\nid_test, X_test, y_test = test_preprocessed['id'], test_preprocessed['ingredients_txt'], test_preprocessed['cuisine']","4385bbea":"LR_clf_counts = Pipeline([\n    ('vect', CountVectorizer()),\n    ('clf', LogisticRegression(random_state=0, max_iter=2000))\n])\nLR_clf_counts.fit(X_train, y_train)\nLR_cnt_pred_tr = LR_clf_counts.predict(X_train)\n\nprint(accuracy_score(y_train, LR_cnt_pred_tr))\nprint(precision_score(y_train, LR_cnt_pred_tr, average='weighted'))","641ecea2":"# Create CV training and test scores for various training set sizes\ntrain_sizes, train_scores, test_scores = learning_curve(LR_clf_counts, \n                                                        X_train, \n                                                        y_train,\n                                                        # Number of folds in cross-validation\n                                                        cv=3,\n                                                        # Evaluation metric\n                                                        scoring='precision_weighted',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 10))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nfig, ax = plt.subplots(figsize=(15,10))\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Weighted Precision Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","e63f5573":"SVM_clf_counts = Pipeline([\n    ('vect', CountVectorizer()),\n    ('clf', LinearSVC(max_iter=3000))\n])\nSVM_clf_counts.fit(X_train, y_train)\nSVM_cnt_pred_tr = SVM_clf_counts.predict(X_train)\n\nprint(accuracy_score(y_train, SVM_cnt_pred_tr))\nprint(precision_score(y_train, SVM_cnt_pred_tr, average='weighted'))","8df375e3":"# Create CV training and test scores for various training set sizes\ntrain_sizes, train_scores, test_scores = learning_curve(SVM_clf_counts, \n                                                        X_train, \n                                                        y_train,\n                                                        # Number of folds in cross-validation\n                                                        cv=3,\n                                                        # Evaluation metric\n                                                        scoring='precision_weighted',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 10))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nfig, ax = plt.subplots(figsize=(15,10))\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Weighted Precision Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","0e9128b2":"NB_clf_counts = Pipeline([\n    ('vect', CountVectorizer()),\n    ('clf', MultinomialNB())\n])\nNB_clf_counts.fit(X_train, y_train)\nNB_cnt_pred_tr = NB_clf_counts.predict(X_train)\n\nprint(accuracy_score(y_train, NB_cnt_pred_tr))\nprint(precision_score(y_train, NB_cnt_pred_tr, average='weighted'))","874aa7ba":"# Create CV training and test scores for various training set sizes\ntrain_sizes, train_scores, test_scores = learning_curve(NB_clf_counts, \n                                                        X_train, \n                                                        y_train,\n                                                        # Number of folds in cross-validation\n                                                        cv=3,\n                                                        # Evaluation metric\n                                                        scoring='precision_weighted',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 10))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nfig, ax = plt.subplots(figsize=(15,10))\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Weighted Precision Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","6c329b85":"LR_clf_tfidf = Pipeline([\n    ('tfidf', TfidfVectorizer(sublinear_tf=True, min_df=5, max_df=0.25, norm='l2', encoding='latin-1',ngram_range=(1, 2), stop_words='english')),\n    ('clf', LogisticRegression(random_state=0, max_iter=2000))\n])\nLR_clf_tfidf.fit(X_train, y_train)\nLR_tfidf_pred_tr = LR_clf_tfidf.predict(X_train)\n\nprint(accuracy_score(y_train, LR_tfidf_pred_tr))\nprint(precision_score(y_train, LR_tfidf_pred_tr, average='weighted'))","b7a2a044":"# Create CV training and test scores for various training set sizes\ntrain_sizes, train_scores, test_scores = learning_curve(LR_clf_tfidf, \n                                                        X_train, \n                                                        y_train,\n                                                        # Number of folds in cross-validation\n                                                        cv=3,\n                                                        # Evaluation metric\n                                                        scoring='precision_weighted',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 10))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nfig, ax = plt.subplots(figsize=(15,10))\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Weighted Precision Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","b40e31a8":"SVM_clf_tfidf = Pipeline([\n    ('tfidf', TfidfVectorizer(sublinear_tf=True, min_df=5, max_df=0.25, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')),\n    ('clf', LinearSVC( max_iter=2000))\n])\nSVM_clf_tfidf.fit(X_train, y_train)\nSVM_tfidf_pred_tr = SVM_clf_tfidf.predict(X_train)\n\nprint(accuracy_score(y_train, SVM_tfidf_pred_tr))\nprint(precision_score(y_train, SVM_tfidf_pred_tr, average='weighted'))","0755b664":"# Create CV training and test scores for various training set sizes\ntrain_sizes, train_scores, test_scores = learning_curve(SVM_clf_tfidf, \n                                                        X_train, \n                                                        y_train,\n                                                        # Number of folds in cross-validation\n                                                        cv=3,\n                                                        # Evaluation metric\n                                                        scoring='precision_weighted',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 10))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nfig, ax = plt.subplots(figsize=(15,10))\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Weighted Precision Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","32c37a77":"NB_clf_tfidf = Pipeline([\n    ('tfidf', TfidfVectorizer(sublinear_tf=True, min_df=5, max_df=0.25, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')),\n    ('clf', MultinomialNB())\n])\nNB_clf_tfidf.fit(X_train, y_train)\nNB_tfidf_pred_tr = NB_clf_tfidf.predict(X_train)\n\nprint(accuracy_score(y_train, NB_tfidf_pred_tr))\nprint(precision_score(y_train, NB_tfidf_pred_tr, average='weighted'))","ee5eaff8":"# Create CV training and test scores for various training set sizes\ntrain_sizes, train_scores, test_scores = learning_curve(NB_clf_tfidf, \n                                                        X_train, \n                                                        y_train,\n                                                        # Number of folds in cross-validation\n                                                        cv=3,\n                                                        # Evaluation metric\n                                                        scoring='precision_weighted',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 10))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nfig, ax = plt.subplots(figsize=(15,10))\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Weighted Precision Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","f2464823":"vect=  CountVectorizer()\nX_train_cnt = vect.fit_transform(X_train)","73790310":"def svc_param_selection(X, y, nfolds, kernal):\n    Cs = [ 0.1, 1, 10]\n    gammas = [0.01, 0.1, 1]\n    degrees = [0, 1, 2, 3]\n    rbf_param_grid = {'C': Cs, 'gamma' : gammas}\n    linear_param_grid = {'C': Cs}\n    poly_param_grid = {'C': Cs, 'gamma' : gammas, 'degree':degrees}\n    if kernal == 'rbf':\n        grid_search = GridSearchCV(SVC(kernel=kernal), rbf_param_grid, cv=nfolds)\n    elif kernal == 'linear':\n        grid_search = GridSearchCV(SVC(kernel=kernal), linear_param_grid, cv=nfolds)\n    else:\n        grid_search = GridSearchCV(SVC(kernel=kernal), poly_param_grid, cv=nfolds)\n    grid_search.fit(X, y)\n    grid_search.best_params_\n    return grid_search.best_params_","cfab926c":"svc_param_selection( X_train_cnt,y_train,2, 'rbf')","33beeac6":"SVM_clf_counts_rbf = Pipeline([\n    ('vect', CountVectorizer()),\n    ('clf', SVC(C=10, gamma=0.01, kernel='rbf', max_iter=2000))\n])\nSVM_clf_counts_rbf.fit(X_train, y_train)\nSVM_cnt_pred_tr_rbf = SVM_clf_counts_rbf.predict(X_train)\nSVM_cnt_pred_val_rbf = SVM_clf_counts_rbf.predict(X_val)\n\nprint(\"accuracy on training: \",accuracy_score(y_train, SVM_cnt_pred_tr_rbf))\nprint(\"precision on training: \",precision_score(y_train, SVM_cnt_pred_tr_rbf, average='micro'))\nprint(\"precision on validation: \",precision_score(y_val, SVM_cnt_pred_val_rbf, average='micro'))","08a026a3":"svc_param_selection( X_train_cnt,y_train,2, 'linear')","77358921":"SVM_clf_counts_lin = Pipeline([\n    ('vect', CountVectorizer()),\n    ('clf', SVC(C=0.1, kernel='linear'))\n])\nSVM_clf_counts_lin.fit(X_train, y_train)\nSVM_cnt_pred_tr_lin = SVM_clf_counts_lin.predict(X_train)\nSVM_cnt_pred_val_lin = SVM_clf_counts_lin.predict(X_val)\n\nprint(\"accuracy on training: \",accuracy_score(y_train, SVM_cnt_pred_tr_lin))\nprint(\"precision on training: \",precision_score(y_train, SVM_cnt_pred_tr_lin, average='micro'))\nprint(\"precision on validation: \",precision_score(y_val, SVM_cnt_pred_val_lin, average='micro'))","222ddc99":"def LR_param_selection(X, y, nfolds):\n    Cs = [0.01, 0.1, 1, 10]\n    param_grid = {'C': Cs}\n    grid_search = GridSearchCV(LogisticRegression(random_state=0,max_iter=2000), param_grid, cv=nfolds)\n    grid_search.fit(X, y)\n    grid_search.best_params_\n    return grid_search.best_params_","9da0a81a":"LR_param_selection( X_train_cnt,y_train,2)","0de8c3fe":"LR_clf_counts = Pipeline([('vect', CountVectorizer()),\n                   ('clf', LogisticRegression(C=1,random_state=0, max_iter=2000)),\n                  ])\nLR_clf_counts.fit(X_train, y_train)\nLR_cnt_pred_tr = LR_clf_counts.predict(X_train)\nLR_cnt_pred_val = LR_clf_counts.predict(X_val)\n\nprint(\"accuracy on training: \",accuracy_score(y_train, LR_cnt_pred_tr))\nprint(\"precision on training: \",precision_score(y_train, LR_cnt_pred_tr, average='micro'))\nprint(\"precision on validation: \",precision_score(y_val, LR_cnt_pred_val, average='micro'))","f38d9362":"LR_clf_counts = Pipeline([('vect', CountVectorizer()),\n                   ('clf', LogisticRegression(C=1,random_state=0, max_iter=2000)),\n                  ])\nLR_clf_counts.fit(X_train, y_train)\nLR_cnt_pred_tr = LR_clf_counts.predict(X_train)\nLR_cnt_pred_val = LR_clf_counts.predict(X_val)\nLR_cnt_pred_tst = LR_clf_counts.predict(X_test)\n\n\nprint(\"precision on training: \",precision_score(y_train, LR_cnt_pred_tr, average='micro'))\nprint(\"precision on validation: \",precision_score(y_val, LR_cnt_pred_val, average='micro'))\nprint(\"precision on testing: \",precision_score(y_test, LR_cnt_pred_tst, average='micro'))","8a93a973":"# Create CV training and test scores for various training set sizes\ntrain_sizes, train_scores, test_scores = learning_curve(LR_clf_counts, \n                                                        X_train, \n                                                        y_train,\n                                                        # Number of folds in cross-validation\n                                                        cv=3,\n                                                        # Evaluation metric\n                                                        scoring='precision_weighted',\n                                                        # Use all computer cores\n                                                        n_jobs=-1, \n                                                        # 50 different sizes of the training set\n                                                        train_sizes=np.linspace(0.01, 1.0, 10))\n\n# Create means and standard deviations of training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Create means and standard deviations of test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nfig, ax = plt.subplots(figsize=(15,10))\n# Draw lines\nplt.plot(train_sizes, train_mean, '--', color=\"#111111\",  label=\"Training score\")\nplt.plot(train_sizes, test_mean, color=\"#111111\", label=\"Cross-validation score\")\n\n# Draw bands\nplt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\nplt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n\n# Create plot\nplt.title(\"Learning Curve\")\nplt.xlabel(\"Training Set Size\"), plt.ylabel(\"Weighted Precision Score\"), plt.legend(loc=\"best\")\nplt.tight_layout()\nplt.show()","01ac0b3a":"labels = train['cuisine'].unique()","d86d9cce":"conf_mat = pd.DataFrame(confusion_matrix(y_val, LR_cnt_pred_val, labels=labels))\nconf_mat_n = conf_mat.divide(conf_mat.sum(axis=1), axis=0)\nfig, ax = plt.subplots(figsize=(20,12))\nsns.heatmap(conf_mat_n, annot=True,xticklabels=labels,yticklabels=labels)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()","9c626f48":"Predicted_vals = pd.DataFrame({'id' : id_val, 'Ingredients': X_val, 'Actual': y_val, 'Preds': LR_cnt_pred_val})","604c077e":"# What's Cooking?\n\n#### before we start with the problem itself there are some questions we need to answer:\n1. What is the business question?\n2. What each row represent?\n3. What is the evaluation method?\n\n#### for this problem (and all kaggle problems) the answers to these questions is always in the problem's overview page.\n1. What is the category of a dish's cuisine given a list of its ingredients? (Supervised ML Problem)\n2. Each row represent a recipe.\n3. Submissions are evaluated on the categorization accuracy (the percent of dishes that you correctly classify).","368dd2e5":"> There are only 3 columns: id, cuisine and ingredients","85cab265":"## TFIDF","5c2d8ad9":"# Modeling","eb0c50a6":"### Logistic Regression","90dab84c":"# 2. Data Preparation","48525ccf":"## BoW","e42fc178":"## Final Model","d1969dc9":"### SVM","ff2ab079":"### Hyperparameter tuning","2fb127ac":"# 1. Important imports\n### let's start by importing needed libraries.","0a710ae5":"# 2. Load Data\n### Let's load the data and have a look on it.\n1. data is provieded in a zip file, so we need to unzip it first using zipfile library.\n2. the traning\/ testing files available in json file format, to read it we use pd.read_json function.\n        we read the data into pandas dataframes which is a 2-dimensional labeled data structure with columns of\n        potentially different types. You can think of it like a spreadsheet or SQL table.\n3. to view some rows of the dataframe we use df_name.head() method which output the first 5 rows of the dataframe."}}