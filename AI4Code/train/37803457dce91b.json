{"cell_type":{"3ce935bb":"code","2f1954fe":"code","f608abb3":"code","6afcfd82":"code","fd697764":"code","548289fe":"code","119ec810":"code","ece177bf":"code","7b61241c":"code","df9999c1":"code","159c1e0b":"code","6aac4698":"code","ff988c82":"code","74196380":"code","28e8bcb2":"code","c274b12c":"code","f4ef956f":"code","64e7986f":"code","367ed920":"code","07aeace2":"code","9142a112":"code","f8f11d89":"code","aca6a5ab":"code","446541b8":"code","5c11f435":"code","e4fa2021":"code","61fbbd35":"code","bb4d7e6b":"code","6c1f5663":"code","36b4ae16":"code","9eebfede":"code","ac875348":"code","5bae66fd":"code","fad4cd3a":"code","62117cc7":"code","80a9cc21":"code","70802f41":"code","6a673957":"code","2e068db1":"code","f91b638a":"code","44542d1f":"code","81feb52d":"code","404e3e9a":"code","0fe9dcbd":"code","4a0e2215":"code","effddcdf":"code","d192fbe5":"markdown","65c07534":"markdown","60699070":"markdown","b1ac06d4":"markdown","9db9f453":"markdown","15a6e616":"markdown","0e3649b5":"markdown","8eb7b079":"markdown","b3f6280d":"markdown","056d496c":"markdown","f58cfeff":"markdown","b0806668":"markdown","6843a1ff":"markdown","d857204d":"markdown","36e09f3c":"markdown","b37fd95a":"markdown","693a1a1c":"markdown","c4b6af9b":"markdown","c12588e6":"markdown","de290f3a":"markdown","478f992b":"markdown","fb99ad4f":"markdown","d7debdaa":"markdown","1d562031":"markdown","d55c53b5":"markdown","5b12a383":"markdown"},"source":{"3ce935bb":"# Upgrading seaborn to 0.11 version to be able to use the histplot function.\n# Don't use if you already ahve the latest version.\n\n!pip install --upgrade seaborn --use-feature=2020-resolver","2f1954fe":"import sys               \nimport time              \nimport pickle     \nimport numpy as np\nimport pandas as pd\nimport skimage.measure\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import scale\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n\nimport xgboost as xgb\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sb\nfrom PIL import Image\nfrom IPython.display import HTML, display\nimport tabulate\n\n%matplotlib inline\n\nv = sb.__version__\nprint(v)\nassert v == '0.11.0'","f608abb3":"df = pd.read_csv(\"..\/input\/brain-tumor\/Brain Tumor.csv\")\nnbr_images = df.shape[0]\ndf.head()","6afcfd82":"print('Total number of images:', nbr_images)","fd697764":"random_img = np.random.randint(1,nbr_images,4)\nplt.figure(figsize=(15,4))\nplt.suptitle('Sample of 4 brain images with labels', y=0.95,fontsize=15)\nfor i in range(len(random_img)):\n    image_temp = Image.open('..\/input\/brain-tumor\/Brain Tumor\/Brain Tumor\/Image'+ str(random_img[i]) +'.jpg').convert('L')  # Conversion to Black & White\n    plt.subplot(1,4,i+1)\n    plt.imshow(image_temp)\n    if df.iloc[random_img[i],1] == 0:\n        plt.xlabel('No tumor')\n    else:\n        plt.xlabel('Tumor')","548289fe":"print('Total number of images:', nbr_images)","119ec810":"sample = np.asarray(Image.open('..\/input\/brain-tumor\/Brain Tumor\/Brain Tumor\/Image1.jpg').convert('L'))\nprint('Image shape:',sample.shape)","ece177bf":"temp_df = df.groupby('Class').count()\ntemp_df.reset_index(inplace=True)\ntemp_df.iloc[0,0]='No tumor'\ntemp_df.iloc[1,0]='Tumor'\nsb.barplot(data = temp_df, x='Class', y='Image')\nplt.title('Number of images for each class');","7b61241c":"plt.figure(figsize=(12,15))\nplt.suptitle('Distributions of first order features for both classes', fontsize=14, y=0.91)\nplt.subplot(3,2,1)\nsb.histplot(data = df, x = 'Mean', hue = 'Class')\nplt.legend(['Tumor','No Tumor'])\nplt.subplot(3,2,2)\nsb.histplot(data = df, x = 'Variance', hue = 'Class')\nplt.legend(['Tumor','No Tumor'])\nplt.subplot(3,2,3)\nsb.histplot(data = df, x = 'Standard Deviation', hue = 'Class')\nplt.legend(['Tumor','No Tumor'])\nplt.subplot(3,2,4)\nsb.histplot(data = df, x = 'Skewness', hue = 'Class')\nplt.legend(['Tumor','No Tumor'])\nplt.subplot(3,1,3)\nsb.histplot(data = df, x = 'Kurtosis',  log_scale=True, hue = 'Class')\nplt.legend(['Tumor','No Tumor']);","df9999c1":"plt.figure(figsize=(12,15))\nplt.suptitle('Distributions of second order features for both classes', fontsize=14, y=0.91)\nplt.subplot(4,2,1)\nsb.histplot(data = df, x = 'Contrast', log_scale=True, hue = 'Class')\nplt.legend(['tumor','No Tumor'])\nplt.subplot(4,2,2)\nsb.histplot(data = df, x = 'Energy', hue = 'Class')\nplt.legend(['tumor','No Tumor'])\nplt.subplot(4,2,3)\nsb.histplot(data = df, x = 'ASM', hue = 'Class')\nplt.legend(['tumor','No Tumor'])\nplt.subplot(4,2,4)\nsb.histplot(data = df, x = 'Entropy', hue = 'Class')\nplt.legend(['tumor','No Tumor'])\nplt.subplot(4,2,5)\nsb.histplot(data = df, x = 'Homogeneity', hue = 'Class')\nplt.legend(['tumor','No Tumor'])\nplt.subplot(4,2,6)\nsb.histplot(data = df, x = 'Dissimilarity', hue = 'Class')\nplt.legend(['tumor','No Tumor'])\nplt.subplot(4,1,4)\nsb.histplot(data = df, x = 'Correlation', hue = 'Class')\nplt.legend(['tumor','No Tumor']);","159c1e0b":"plt.figure(figsize=(10,10))\nsb.heatmap(df.iloc[:,1:df.shape[1]-1].corr(), annot=True);","6aac4698":"# Normalization function.\n# Normalize the input so that its distribution becomes N(0,1).\n\ndef normalize(data):\n    mean = np.mean(data, axis=1, keepdims=True)\n    std = np.std(data, axis=1, keepdims=True)\n    data_normalized = (data - mean)\/std\n    return data_normalized","ff988c82":"# Initialization function.\n# Initializing the weights using a modified He initialization to avoid exploding or vanishing gradients.\n\ndef initialize_weights_bias(layer_dims):\n    L = len(layer_dims)                     \n    params = {}\n    for l in range(1, L):\n        params['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2\/(layer_dims[l-1]+layer_dims[l]))\n        params['b' + str(l)] = np.zeros((1, layer_dims[l]))\n    return params","74196380":"# Activation functions.\n# Calculate the activation using a ReLU or a Sigmoid function.\n\ndef activation(activation_types, Z):\n    if (activation_types == 'sigmoid'):\n        s = 1\/(1+np.exp(-Z))\n    else:\n        s = np.maximum(np.zeros(Z.shape), Z)  # Z must be np.array\n    return s","28e8bcb2":"# Derivative of the activation functions.\n\ndef activation_derivative(activation_types, Z):\n    if (activation_types == 'sigmoid'):\n        s = np.multiply(activation('sigmoid', Z),(1 - activation('sigmoid', Z)))\n    else:\n        s = np.maximum(np.zeros(Z.shape), Z)\n        s[s!=0] = 1\n    return s","c274b12c":"# Forward propagation function which calculated a prediction Yhat,\n# and save a cache of the linear and non-linear parts for the backprop calculations.\n\ndef forward_pass(X, params, layer_dims, activation_types):\n    L = len(layer_dims)\n    cache={}\n    A = X\n    cache['A0'] = X\n    for l in range(1, L):\n        Z = np.dot(A,params['W' + str(l)].T) + params['b' + str(l)]\n        A = activation(activation_types[l-1], Z)\n        cache['Z' + str(l)] = Z\n        cache['A' + str(l)] = A\n    return A, cache","f4ef956f":"# Loss function.\n# Calculates the cross entropy loss between the predicted and true classes.\n\ndef cross_entropy_cost(Yhat, Y, params, lamda):\n    m = Y.shape[0]\n    L = int(len(params)\/2)\n    L2_reg = 0\n    for l in range(1,L+1):\n        L2_reg += np.linalg.norm(params['W' + str(l)])**2\n    cost = -(1\/m)*(np.dot(np.log(Yhat.T),Y)+np.dot(np.log(1-Yhat.T),(1-Y))) + (lamda\/(2*m))*L2_reg\n    cost = np.squeeze(cost)\n    return cost","64e7986f":"def back_prop(Y, Yhat, cache, params, activation_types, lamda):\n    grads = {}\n    L = int(len(cache)\/2)+1\n    dYhat = - (np.divide(Y, Yhat) - np.divide(1 - Y, 1 - Yhat))\n    m = Y.shape[0]\n    for l in reversed(range(1,L)):\n        dZ = np.multiply(dYhat, activation_derivative(activation_types[l-1], cache['Z'+str(l)]))\n        dW = (1\/m)*np.dot(dZ.T, cache['A'+str(l-1)]) + (lamda\/m)*params['W' + str(l)]\n        db = (1\/m)*np.sum(dZ, axis=0, keepdims=True)\n        dA_prev = np.dot(dZ, params['W'+str(l)])                 \n        dYhat = dA_prev\n        grads[\"dA\" + str(l-1)] = dA_prev\n        grads[\"dW\" + str(l)] = dW\n        grads[\"db\" + str(l)] = db\n        assert (dW.shape == params['W'+str(l)].shape)\n        assert (db.shape == params['b'+str(l)].shape)\n    return grads","367ed920":"def update_parameters(parameters, grads, learning_rate, L):\n    for l in range(1,L):\n        parameters[\"W\" + str(l)] = parameters[\"W\" + str(l)] - learning_rate * grads[\"dW\" + str(l)]\n        parameters[\"b\" + str(l)] = parameters[\"b\" + str(l)] - learning_rate * grads[\"db\" + str(l)]\n    return parameters","07aeace2":"def model(X_train,X_val,X_test,Y_train,Y_val,Y_test, model_config):\n    \n    i=0\n    conv = 1\n    cost_history={\n        'Train_loss': [],\n        'Val_loss': []\n    }\n    \n    learning_rate = model_config['learning_rate']\n    lamda = model_config['lamda']\n    layer_dims = model_config['layer_dims']\n    activation_types = model_config['activation_types']\n    max_iter = model_config['max_iter']\n    params = initialize_weights_bias(layer_dims)\n    L = len(layer_dims)\n    \n    toolbar_width = 20\n    sys.stdout.write(\"[%s]\" % (\"\" * toolbar_width))\n    sys.stdout.flush()\n    sys.stdout.write(\"\\b\" * (toolbar_width+1)) # return to start of line, after '['\n    start = time.time()\n    \n    while (i<max_iter)&(conv>10e-7):\n        Yhat, cache = forward_pass(X_train, params, layer_dims, activation_types)\n        Yhat_val, _ = forward_pass(X_val, params, layer_dims, activation_types)\n        cost_train = cross_entropy_cost(Yhat, Y_train, params, lamda)\n        cost_val = cross_entropy_cost(Yhat_val, Y_val, params, lamda)\n        \n        grads = back_prop(Y_train, Yhat, cache, params, activation_types, lamda)\n        params = update_parameters(params, grads, learning_rate, L)\n        \n        cost_history['Train_loss'].append(cost_train)\n        cost_history['Val_loss'].append(cost_val)\n        \n        if i>2:\n            conv = np.abs(cost_history['Train_loss'][-1]-cost_history['Train_loss'][-2])\n        i+=1\n        if i%100==0:\n            sys.stdout.write(\"=\")\n            sys.stdout.flush()\n    sys.stdout.write(\"]\\n\") # this ends the progress bar\n    \n    end = time.time()\n    training_dur = end-start\n    train_accuracy = accuracy_score(Y_train, np.round(Yhat))\n    \n    Yhat_test, _ = forward_pass(X_test, params, layer_dims, activation_types)\n    test_accuracy = accuracy_score(Y_test, np.round(Yhat_test))\n    conf_matrix = confusion_matrix(Y_test, np.round(Yhat_test), normalize='true')\n    \n    F1_score = f1_score(Y_test, np.round(Yhat_test))\n    \n    Model_infos={\n        'Train_acc': train_accuracy,\n        'Test_acc': test_accuracy,\n        'conf_matrix': conf_matrix,\n        'Train_dur': training_dur,\n        'F1_score': F1_score\n    }\n    return cost_history, params, Model_infos","9142a112":"def GridSearch(learning_rates, lamdas, model_config):\n    best_F1 = 0\n    results = {}\n    models_cost_history = []\n    models_params = []\n    Best_model = {}\n    m=1\n    print('-------------- Beginning of Grid Search --------------')\n    print('------------------ Training Models -------------------')\n    for i in range(len(learning_rates)):\n        for j in range(len(lamdas)):\n            model_config['learning_rate'] = learning_rates[i]\n            model_config['lamda'] = lamdas[j]\n            print('Training of Model '+str(m)+' (Learning rate: '+str(learning_rates[i]) + ' Lambda: ' + str(lamdas[j])+')' )\n            cost_history, params, Model_infos = model(X_train_img,X_val_img,X_test_img,Y_train,Y_val,Y_test, model_config)\n            print('Training duration: ' + str(Model_infos['Train_dur']\/60)  + 'min')\n            print('Number of epochs: ' + str(int(len(cost_history['Train_loss']))))\n            print('Train accuracy: ' + str(Model_infos['Train_acc']) + ' | ' + 'Test accuracy: ' +\n                  str(Model_infos['Test_acc'])+ ' | ' + 'F1 score: ' + str(Model_infos['F1_score']))\n            results['Model: '+ str(learning_rates[i]) + ' ' + str(lamdas[j])] = Model_infos\n            models_cost_history.append(cost_history)\n            if Model_infos['F1_score']>best_F1:\n                best_F1 = Model_infos['F1_score']\n                Best_model['infos'] = Model_infos\n                Best_model['cost_history'] = cost_history\n                Best_model['params'] = params  \n                Best_model['hyperparams'] = (learning_rates[i],lamdas[j])\n            m+=1\n            print('')\n    print('--------------------- Best Model ---------------------')\n    print('Best F1 score: ' + str(Best_model['infos']['F1_score']))\n    print('Best hyper-parameters: Learning rate: ' + str(Best_model['hyperparams'][0]) + ' ' +\n          'Lambda: '+ str(Best_model['hyperparams'][1]))\n    print('----------------- End of Grid Search -----------------')\n    return results, models_cost_history, Best_model","f8f11d89":"# Extracting labels\n\nlabels = np.array(df.iloc[:,1])","aca6a5ab":"# Normalizing the tabular data (numeric features)\n\ndata = df.iloc[:,2:]\nData_normalized = scale(data)","446541b8":"# Building the master data set using the normalized tabular data\n\nData_normalized_df = pd.DataFrame(Data_normalized)\nimages_df = pd.DataFrame(df.iloc[:,0:1])\nmaster_data_df = pd.concat([images_df, Data_normalized_df], axis=1)\nmaster_data_df.head()","5c11f435":"# Splitting the data into Train\/Validation\/Test sub-sets\n\nnp.random.seed(7)\nX_train, X_test, Y_train, Y_test = train_test_split(master_data_df, labels, test_size=0.20, random_state=7)\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.10, random_state=7)\n\nY_train = Y_train.reshape(Y_train.shape[0],1)\nY_val = Y_val.reshape(Y_val.shape[0],1)\nY_test = Y_test.reshape(Y_test.shape[0],1)","e4fa2021":"# This function is used to seperate the image data from the tabular data for each sub-set, \n# and converts the images into a smaller shape (120-by-120px) using a MaxPooling function.\n# The images are then stored in a numpy array.\n\ndef image_feat_separator(data):\n    images_names_list = list(data.iloc[:,0])\n    feat_df = data.iloc[:,1:]\n    path_list=[]\n    images_list=[]\n    for i in images_names_list:\n        path = \"..\/input\/brain-tumor\/Brain Tumor\/Brain Tumor\/\"+i+\".jpg\"\n        path_list.append(path)\n    for i in path_list:\n        temp = Image.open(i).convert('L')\n        array_temp = np.asarray(temp)\n        max_pooling_temp = skimage.measure.block_reduce(array_temp, (2,2), np.max)\n        flat_temp = list(max_pooling_temp.flatten())\n        images_list.append(flat_temp)\n        temp.close()\n    images_array = np.array(images_list)\n    return images_array, feat_df","61fbbd35":"X_train_img, X_train_feat = image_feat_separator(X_train)\nX_test_img, X_test_feat = image_feat_separator(X_test)\nX_val_img, X_val_feat = image_feat_separator(X_val)","bb4d7e6b":"print('The train image set shape:', X_train_img.shape)\nprint('The test image set shape:', X_test_img.shape)\nprint('The validation image set shape:', X_val_img.shape)","6c1f5663":"# Normalizing the images using the normalize function we built earlier\n\nX_train_img = normalize(X_train_img)\nX_test_img = normalize(X_test_img)\nX_val_img = normalize(X_val_img)","36b4ae16":"# Setting the model configuration\n\nmodel_config={\n    'layer_dims' : [X_train_img.shape[1],8,8,8,4,1],\n    'activation_types' : ['ReLu','ReLu','ReLu','ReLu','sigmoid'],\n    'max_iter' : 10000\n}","9eebfede":"# Executing the Grid Search function \/ Training MLP models\nStart_timer = time.time()\nlearning_rates = list(np.random.uniform(0.0001,0.001,3))\nlamdas = list(np.random.uniform(1,2,3))\nresults, models_cost_history, Best_model = GridSearch(learning_rates, lamdas, model_config)\nStop_timer = time.time()\nGridSearch_dur = Stop_timer - Start_timer\nprint('Grid search training duration: ', GridSearch_dur\/60, ' min')","ac875348":"# Use this only after training to save the trained model !!!\n\nwith open('GSresults', 'wb') as f:\n    pickle.dump([results, models_cost_history, Best_model], f)","5bae66fd":"# Use this if you have a model to load.\n\n#with open('GSresults', 'rb') as f:\n#    results, models_cost_history, Best_model = pickle.load(f)","fad4cd3a":"# Visualizing the learning curves of trained models via grid search.\n\nfigure = plt.figure(figsize=(min(len(learning_rates)*4.3,15),min(len(lamdas)*4,15)))\nplt.suptitle('Evolution of the Train\/Validation Losses for all the trained models',fontsize=15, y=1.05)\nnbr_plots = len(learning_rates)*len(lamdas)\nalpha_lamda_combinations = [(i,j) for i in learning_rates for j in lamdas]\nfor i in range(1,nbr_plots+1):\n    plt.subplot(len(learning_rates),len(lamdas),i)\n    x = list(range(0,len(models_cost_history[i-1]['Train_loss'])))\n    y1 = list(np.squeeze( models_cost_history[i-1]['Train_loss']))\n    y2 = list(np.squeeze( models_cost_history[i-1]['Val_loss'])) \n    temp_df = pd.DataFrame([x,y1,y2]).T\n    temp_df.columns=['x','y1','y2']\n    sb.lineplot(data=temp_df, x='x', y='y1')\n    sb.lineplot(data=temp_df, x='x', y='y2')\n    plt.title('Learning rate:' + str(\"{:.2e}\".format(alpha_lamda_combinations[i-1][0])) + ' Lambda: ' + str(\"{:.2e}\".format(alpha_lamda_combinations[i-1][1])))\n    plt.legend(['Train','Validation'])\n    plt.xlabel('iteration')\n    plt.ylabel('Loss') \nfigure.tight_layout(pad=2.0);","62117cc7":"# Function to plot the confusion matrix of the trained model.\n\ndef plot_cm(mat,y_ture,ax):\n    df_cm = pd.DataFrame(mat, columns=np.unique(y_ture), index = np.unique(y_ture))\n    df_cm.index.name = 'True Label'\n    df_cm.columns.name = 'Predicted Label'\n    sb.heatmap(df_cm, cmap=\"Blues\", cbar=False, annot=True,annot_kws={\"size\": 10}, ax=ax)\n    plt.yticks(fontsize=10)\n    plt.xticks(fontsize=10)","80a9cc21":"# Plotting the learning curve and confusion matrix of the best model.\n\nplt.figure(figsize=(12,5))\nplt.suptitle('Best model results', fontsize=14)\nax1 = plt.subplot(1,2,1)\nplt.title('Train\/Validation Losses evolution')\nx = list(range(0,len(Best_model['cost_history']['Train_loss'])))\ny1 = list(np.squeeze( Best_model['cost_history']['Train_loss']))\ny2 = list(np.squeeze( Best_model['cost_history']['Val_loss'])) \ntemp_df = pd.DataFrame([x,y1,y2]).T\ntemp_df.columns=['x','y1','y2']\nsb.lineplot(data=temp_df, x='x', y='y1')\nsb.lineplot(data=temp_df, x='x', y='y2')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(['Train','Validation'])\nax2 = plt.subplot(1,2,2)\nplt.title('Confusion matrix of the test set')\nplot_cm(Best_model['infos']['conf_matrix'], Y_test, ax2);","70802f41":"print('The best grid search model results:')\n\nresult_table=[['Train accuracy', Best_model['infos']['Train_acc']],\n             ['Test accuracy', Best_model['infos']['Test_acc']],\n             ['F1 score', Best_model['infos']['F1_score']],\n             ['Training duration', Best_model['infos']['Train_dur']\/60]]\n\ndisplay(HTML(tabulate.tabulate(result_table, colalign=(\"center\",\"center\"), tablefmt='html')))","6a673957":"# Grid search function for Xgboost models\n\ndef xgb_GS(data, nbr_estimators, max_depth, learning_rate):\n    X_train_feat = data['X_train_feat']\n    X_val_feat = data['X_val_feat']\n    X_test_feat = data['X_test_feat']\n    Y_train = data['Y_train']\n    Y_val = data['Y_val']\n    Y_test = data['Y_test']\n    best_f1 = 0\n    for i in learning_rate:\n        for j in max_depth:\n            for k in nbr_estimators:\n                xgb_model = xgb.XGBClassifier(n_estimators=k, max_depth=j, learning_rate=i)\n                xgb_model.fit(X_train_feat, Y_train, verbose=False)\n                Yhat_xgb = xgb_model.predict(X_test_feat)\n                Yhat_xgb = np.round(Yhat_xgb.reshape(Y_test.shape[0],1))\n                score = f1_score(Y_test, Yhat_xgb)\n                if score > best_f1:\n                    best_f1 = score\n                    best_hyper_params=[k,j,i]\n    return best_f1, best_hyper_params","2e068db1":"# Variables and Hyper-parameters to feed to the grid seach function.\n\ndata={\n    'X_train_feat': X_train_feat,\n    'X_val_feat': X_val_feat,\n    'X_test_feat': X_test_feat,\n    'Y_train': Y_train,\n    'Y_val': Y_val,\n    'Y_test': Y_test\n}\nnbr_estimators = [5,10,15,20,25]\nmax_depth = [4,8,16]\nlearning_rate = [0.1, 0.2, 0.4, 0.6, 0.8]","f91b638a":"best_f1, best_hyper_params = xgb_GS(data, nbr_estimators, max_depth, learning_rate);","44542d1f":"print('The best grid search model results:')\n\nresult_table=[['Number of estimators', best_hyper_params[0]],\n             ['Max_depth', best_hyper_params[1]],\n             ['Learning_rate', best_hyper_params[2]],\n             ['F1 score', best_f1]]\n\ndisplay(HTML(tabulate.tabulate(result_table, colalign=(\"center\",\"center\"), tablefmt='html')))","81feb52d":"# Getting the best model using the tuned hyper-parameters. \nstart=time.time()\nxgb_BM = xgb.XGBClassifier(n_estimators=best_hyper_params[0], max_depth=best_hyper_params[1],\n                                     learning_rate=best_hyper_params[2])\neval_set=[(X_train_feat, Y_train), (X_val_feat, Y_val)]\nxgb_BM.fit(X_train_feat, Y_train, eval_metric=['logloss','rmse','error'], eval_set=eval_set, verbose=False)\nend=time.time()\nduration_xgb = end-start\nYhat_xgb_BM = xgb_BM.predict(X_test_feat)\nYhat_xgb_BM = np.round(Yhat_xgb_BM.reshape(Y_test.shape[0],1))","404e3e9a":"# retrieve performance metrics\nresults = xgb_BM.evals_result()\nepochs = len(results['validation_0']['error'])\nx_axis = range(0, epochs)","0fe9dcbd":"plt.figure(figsize=(12,10))\n# plot log loss\n\nplt.subplot(2,1,1)\nplt.plot(x_axis, results['validation_0']['logloss'], label='Train')\nplt.plot(x_axis, results['validation_1']['logloss'], label='Val')\nplt.legend()\nplt.ylabel('Log Loss')\nplt.title('XGBoost Log Loss')\n\n# plot classification rmse\nplt.subplot(2,2,3)\nplt.plot(x_axis, results['validation_0']['rmse'], label='Train')\nplt.plot(x_axis, results['validation_1']['rmse'], label='Val')\nplt.legend()\nplt.ylabel('Classification RMSE')\nplt.title('XGBoost Classification RMSE')\n\n# plot classification error\nplt.subplot(2,2,4)\nplt.plot(x_axis, results['validation_0']['error'], label='Train')\nplt.plot(x_axis, results['validation_1']['error'], label='Val')\nplt.legend()\nplt.ylabel('Classification Error')\nplt.title('XGBoost Classification Error');","4a0e2215":"print('The best grid search model results:')\n\nresult_table=[['Test accuracy', accuracy_score(Yhat_xgb_BM, Y_test)],\n             ['F1 score', f1_score(Yhat_xgb_BM, Y_test)],\n             ['Training duration', duration_xgb]]\n\ndisplay(HTML(tabulate.tabulate(result_table, colalign=(\"center\",\"center\"), tablefmt='html')))","effddcdf":"plt.figure(figsize=(6,6))\nax1 = plt.subplot(111)\nplt.title('Confusion matrix of the test set')\nplot_cm(confusion_matrix(Yhat_xgb_BM, Y_test, normalize='true'), Y_test, ax1);","d192fbe5":"In this final section, we're going to use the Xgboost library to train an Xgboost model using the tabular data.","65c07534":"# Building a Neural Network from scratch for a classification task\n\nIn this notebook, we're going to implement a Multi-Layer Perceptron neural network (MLP) and use it in a brain tumor classification problem. For the MLP implementation we're mainly going to use the Numpy library, which is well known for fast multidimensional array and linear algebra calculations.\n\nThe results obtained by the trained MLP model will then be compared to the ones obtained by an Xgboost model which is one of the best machine learning algorithms for classification and regression tasks in terms of accuracy, speed, and memory consumption.\n\n**Note:** The MLP model will be trained using the unstructured data (Images), while then xgboost model will be train on the structured data (Tabular data).\n\nThe data that we're going to use is provided by [Jakesh Bohaju](https:\/\/www.kaggle.com\/jakeshbohaju) on [Kaggle](https:\/\/www.kaggle.com\/jakeshbohaju\/brain-tumor).\n\nThis is a brain tumor feature dataset including five first-order features and eight texture features with the target level (in the column Class).\n\n1. First Order Features\n    - Mean\n    - Variance\n    - Standard Deviation\n    - Skewness\n    - Kurtosis\n\n\n2. Second Order Features\n    - Contrast\n    - Energy\n    - ASM (Angular second moment)\n    - Entropy\n    - Homogeneity\n    - Dissimilarity\n    - Correlation\n    - Coarseness\n\n**Image** column defines image name and **Class** column defines either the image has tumor or not (1 = Tumor, 0 = Non-Tumor)","60699070":"It's now visibly clear that the second order features are more corrolated to the 'Class' variable than those of first order!","b1ac06d4":"## Model building","9db9f453":"# Building an MLP model","15a6e616":"**Updating parameters:**\n\nThe parameter updating step of the gradient descent algorithm is as follows:\n$$ W^{[l]} = W^{[l]} - \\alpha dW^{[l]}$$\n$$ b^{[l]} = b^{[l]} - \\alpha db^{[l]}$$","0e3649b5":"# Brief EDA","8eb7b079":"# Application","b3f6280d":"**Derivative of the activation functions:**\n\nReLU: \n$$ \n\\dfrac{dReLU}{dx}(x)= \\begin{cases} 0 & \\mbox{if }& x\\leq0\\\\ 1 & \\mbox{else}& \\end{cases}\n$$\n\nSigmoid:\n$$\n\\mbox{Sig}'(x)=\\mbox{Sig(x)}(1-\\mbox{Sig(x)})\n$$","056d496c":"In this section, we're going to build a classic MLP model from scratch. The architecture of the neural network is represented in the following image:\n\n![MLP1.png](attachment:MLP1.png)","f58cfeff":"## Helper functions","b0806668":"It's clear, from the figure above, that the distributions of the majority of the second order features corresponding to the 'Tumor' class are different that the ones corresponding to the 'No Tumore' class. Which suggest that these features are very useful in determining the class of a certain image. Whereas, the distributions of the first order features are less different with respect to the classes.\n\nTo support our observation we're going to plot the corrolation heatmap of all the important features present in the dataset.","6843a1ff":"**Activation functions:**\n\n**Re**ctified **L**inear **U**nit activation function: $$ \\mbox{ReLU}(x) = \\max(0,x)$$\n\nSigmoid activation function: $$ \\mbox{Sig}(x)=\\dfrac{1}{1+e^{-x}}$$","d857204d":"**Informations about the model:**\n\n - Input: Images of shape 120-by-120 px flatten into an array of shape (m,14400), where m is the number of images.\n - L: Total number of layers including the Input and Output layers.\n - Yhat: Class prediction of the input (0: No Tumor, 1: Tumor) presented in an array of shape (m,1)\n - Weights initialization: Modified He initialization.\n - Activation functions: ReLU for the hidden layers, Sigmoid for the output layer.\n - Optimization algorithm: Gradient descent with L2-regularization.\n - Loss: Cross-Entropy Loss","36e09f3c":"Now, it's time to use all of the helper functions defined in the previous section to build the MLP model function.\n\nThe model structure is as follow:\n - Input:\n     - Training, Validation, and Test data.\n     - Model configuration: Learning rate, Regularization parameter $\\lambda$, Layers dimension list, Activation types list, Maximum number of iteration.\n - Output:\n     - Training and Validation cost history.\n     - Parameters of the model (Weights and Biases).\n     - Model informations: \n         - Training accuracy\n         - Test accuracy\n         - Confusion matrix\n         - Training duration\n         - F1-score\n         \nThe model training process using gradient descent goes as follow:\n1. Parameters and intern variables initialization.\n2. (Gradient descent) Iterating until convergence or reaching max iteration:\n    1. Forward propagation\n    2. Loss calculation\n    3. Back propagation\n    4. Updating parameters\n    5. Calculating a convergence estimation.\n3. Calculating the test accuracy and the other model informations.","b37fd95a":"**Important notice:**\n\nDon't run the following cell if you don't want to wait for the model to train just use the next cell to load the trained model. The training duration depends on how many hyper-parameters samples you chose, however the training would take about 3 hours using the following configuration.","693a1a1c":"**Cross-Entropy Loss:**\n\n$m$ is the number of input images, and $A^{[L]}$ is the activation of the last layer (shaped $(m,1)$).\n\nThe L2-regularization term is calculated using the Frobenius norm of the weights, which is given by the following:\n$$  \\parallel W\\parallel_F = \\sqrt{\\sum_{\\substack{1\\leq i\\leq n \\\\ 1\\leq j \\leq m} } |W_{ij}|^2}$$\n\nCross-Entropy Loss formula:\n\n$$\\mathcal{L} = -\\frac{1}{m}{(Y^{T}\\log(A^{[L]}) + (1 - Y)^{T}\\log(1 - A^{[L]}))}+\\frac{\\lambda}{2m}\\sum_{i=1}^L\\parallel W^{[i]}\\parallel_F$$","c4b6af9b":"**Weights initialization:**\n\n$W^{[l]}$ denotes the weights of the layer $l$.\n\n$W_{norm}$ denotes a standard normal initialization of the weights.\n\n$d^{[l]}$ denotes the number of units in the layer $l$.\n\n$$\nW^{[l]}_{init} = W_{norm} * \\sqrt{\\dfrac{2}{d^{[l-1]}+d^{[l]}}}\n$$","c12588e6":"# Xgboost Model","de290f3a":"**Forward propagation:**\n\n$A$ is the activation of the previous layer. ($A$ is the input for the first layer)\n\n$$\\underbrace{Z=A*W^{T}+b\\quad =>\\quad A=\\mbox{Activation}(Z)}_{\\mbox{Repeat } L-1 \\mbox{ times}}\n$$\n\nThe last $A$ is the prediction $\\hat{y}$.","478f992b":"**Normalization:**\n$$\nX_{norm}=\\dfrac{X-\\bar{X}}{\\sqrt{\\mbox{Var}(X)}}\n$$","fb99ad4f":"**Back-propagation:**\n\nThe back-propagation function is used to calculate the gradients of the cost function $J$ with respect to $Z$, $W$, $b$, and $A$ for each layer of the network. The calculated gradients are then used to update the weights and biases for each layer.\n\nFirst, we need to compute the gradient of the loss with respect to the activation of the last layer:\n$$ dA^{[L]} = -\\dfrac{Y}{A^{[L]}} - \\dfrac{1-Y}{1-A^{[L]}}$$\nThen we calculate iteratively (going from $l=L$ to $l=1$) the following set of equations which represent the gradients for each layer $l$:\n\n\\begin{align*}\n dZ^{[l]} &= dA^{[l]} * g'(Z^{[l]}), \\mbox{ (Element wise multiplication)} \\\\\n dW^{[l]} &= \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} + \\frac{\\lambda}{m} W^{[l]} \\\\\n db^{[l]} &= \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)} \\\\\n dA^{[l-1]} &= \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \n\\end{align*}\n\nThe last equation is not calculated for the Input layer.","d7debdaa":"Now, we're going to apply the model we built to the Brain Tumor image dataset. But first, we need to preprocess the data to make it ready for use.","1d562031":"# Conclusion\n\nThe accuracy obtained by the MLP model on the Tumor dataset can be increased by by tuning more hyper parameters or changing the depth of the network. However the best practiec is to use a convolutional neural network (CNN) when teraining a model that takes image data as input. The purpose of this notebook was mainly to implement an MLP model from scratch and see how it performs on an image dataset.\n\nThe second part of this notebook was a direct application of the Xgboost model which showed the extremely high performance of this model on a structured dataset.","d55c53b5":"## Grid Search\n\nGrid search is a method commonly used to tune the hyper-parameter of a certain model by training a set of models and then picking the best one of them.\nIn this allpication we're going to use a Random grid search which can be more effective in determining the best set of hyper-parameters.\n\nHyper-parameters to tune:\n - Learning rate.\n - L2-regularization coefficient.","5b12a383":"In this preprocessing step we're also scaling the tabular data that we're going to use later on when we train an xgboost model, however we need to prepare both image and tabular data from now so that we can get the same Train\/Val\/Test split for both models."}}