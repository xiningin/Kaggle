{"cell_type":{"6e1a7b5a":"code","5aaad862":"code","e6a478f1":"code","8467d08d":"code","a6555fe7":"code","d7833c89":"code","84aca1f2":"code","8ad1bee5":"code","9e8db30e":"code","555e37af":"code","08882d73":"code","c5caff64":"code","4d20c02f":"code","486d3002":"code","ba1cae00":"code","7d3693b9":"code","7f0177a6":"code","565c4049":"code","a4482cf1":"code","6b9a2060":"code","2f75e7e9":"code","f531d2ea":"code","5e157fc7":"code","31a9e1c7":"code","906c7256":"code","c90fc13a":"code","e38073dc":"code","2e8be9c4":"code","8c4b0d03":"code","84d7a1f9":"code","5e086a07":"code","ff8ba58b":"code","3c757afd":"code","e6cddc23":"code","d9df10cc":"code","57964149":"code","6a228e57":"code","b181606b":"code","eca53335":"code","f559d1e9":"code","4ce6abd3":"code","19903720":"code","19a2835c":"markdown","2995162d":"markdown","070164e6":"markdown","5ef8ffe1":"markdown","41122a07":"markdown","a8a1f5bd":"markdown","cb7beeef":"markdown","1d2072a7":"markdown"},"source":{"6e1a7b5a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy","5aaad862":"training = pd.read_csv('..\/input\/train.csv')\ntesting = pd.read_csv('..\/input\/test.csv')\nall_data = pd.concat([training, testing], ignore_index = True, sort = True)","e6a478f1":"all_data.head()","8467d08d":"#Find how much missind data there is.\nmis_val_percent = (100 * all_data.isnull().sum() \/ len(all_data)).sort_values(ascending = False)\nprint(mis_val_percent[mis_val_percent > 0])","a6555fe7":"#With such a large percentage missing from some columns, I am going to drop the top 5 columns.\n#Be careful, SalePrice is missing so much due to the fact that the test set does not include the SalePrice\nall_data = all_data.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu'], axis = 1)","d7833c89":"#LotFrontage\n#It seems common that lots are all fairly similar in each neighborhood.  \n#Fill all the missing data with the mean of the LotFrontage of the neighborhood the house is in\n\nall_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.mean()))","84aca1f2":"#NA for Garage info means there is no garage, replace 'NA' with 'None'\nall_data['GarageFinish'] = all_data['GarageFinish'].fillna('None')\nall_data['GarageCond'] = all_data['GarageCond'].fillna('None')\nall_data['GarageQual'] = all_data['GarageQual'].fillna('None')\nall_data['GarageType'] = all_data['GarageType'].fillna('None')","8ad1bee5":"#Similar to garage info, 'NA' for basement columns, represents 'No Basement'\n#For Basement Bath info, filling with 0\nall_data['BsmtCond'] = all_data['BsmtCond'].fillna('None')\nall_data['BsmtExposure'] = all_data['BsmtExposure'].fillna('None')\nall_data['BsmtFinType1'] = all_data['BsmtFinType1'].fillna('None')\nall_data['BsmtFinType2'] = all_data['BsmtFinType2'].fillna('None')\nall_data['BsmtQual'] = all_data['BsmtQual'].fillna('None')\nall_data['BsmtFullBath'] = all_data['BsmtFullBath'].fillna(0)\nall_data['BsmtHalfBath'] = all_data['BsmtHalfBath'].fillna(0)","9e8db30e":"#'NA' for Masonry will be filled with 'None' while the area, since none, will be filled with 0.\nall_data['MasVnrType'] = all_data['MasVnrType'].fillna('None')\nall_data['MasVnrArea'] = all_data['MasVnrArea'].fillna(0)","555e37af":"#MSZoning will be filled with the most common MSZoning in the neighborhood of the missing values.\nall_data['MSZoning'].value_counts().sort_values(ascending = False)","08882d73":"all_data[all_data['MSZoning'].isnull()]['Neighborhood']","c5caff64":"all_data[all_data['Neighborhood'] == 'IDOTRR']['MSZoning'].value_counts().sort_values(ascending = False)","4d20c02f":"all_data[all_data['Neighborhood'] == 'Mitchel']['MSZoning'].value_counts().sort_values(ascending = False)","486d3002":"all_data.loc[all_data['Neighborhood'] == 'IDOTRR', 'MSZoning'] = all_data.loc[all_data['Neighborhood'] == 'IDOTRR', 'MSZoning'].fillna('RM')\nall_data.loc[all_data['Neighborhood'] == 'Mitchel', 'MSZoning'] = all_data.loc[all_data['Neighborhood'] == 'Mitchel', 'MSZoning'].fillna('RL')","ba1cae00":"#That took way longer than it should have for 4 missing values, but the lesson learned was great!","7d3693b9":"#Utilities\nall_data['Utilities'].value_counts().sort_values(ascending = False)","7f0177a6":"#All but one case are 'AllPub', going to stick with the trend\nall_data['Utilities'] = all_data['Utilities'].fillna('AllPub')","565c4049":"#Functional, data description states assume normal unless otherwise stated\nall_data['Functional'] = all_data['Functional'].fillna('Typ')","a4482cf1":"#Electrical, the vast majority are 'SBrkr'\nall_data['Electrical'].value_counts().sort_values(ascending = False)","6b9a2060":"all_data['Electrical'] = all_data['Electrical'].fillna('SBrkr')","2f75e7e9":"#Exterior1st and Exterior2nd\n#The null values for Exterior1st and Exterior2nd are the same observation\nall_data[all_data['Exterior1st'].isnull()][['Exterior2nd', 'Neighborhood']]","f531d2ea":"all_data[all_data['Neighborhood'] == 'Edwards']['Exterior1st'].value_counts().sort_values(ascending= False)","5e157fc7":"all_data[all_data['Neighborhood'] == 'Edwards']['Exterior2nd'].value_counts().sort_values(ascending= False)","31a9e1c7":"#Houses in the same neighbor tend to be built pretty similar.\n#The null value comes from the Edwards neighborhood where 'Wd Sdng' is the most popular\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna('Wd Sdng')\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna('Wd Sdng')","906c7256":"#KitchenQual is only missing one observation as well, fill with the most popular\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])","c90fc13a":"#GarageCars\n#Upon looking at the null observation, the GarageArea is also null\n#No garage means no cars\nall_data[all_data['GarageCars'].isnull()]['GarageArea']","e38073dc":"all_data['GarageCars'] = all_data['GarageCars'].fillna(0)\nall_data['GarageArea'] = all_data['GarageArea'].fillna(0)","2e8be9c4":"#Taking a look at the remaining missing basement data\nall_data[all_data['BsmtFinSF1'].isnull()][['BsmtUnfSF', 'BsmtFinSF2', 'TotalBsmtSF']]","8c4b0d03":"#With all the basement columns null, there is no basement\nall_data['BsmtUnfSF'] = all_data['BsmtUnfSF'].fillna(0)\nall_data['BsmtFinSF1'] = all_data['BsmtFinSF1'].fillna(0)\nall_data['BsmtFinSF2'] = all_data['BsmtFinSF2'].fillna(0)\nall_data['TotalBsmtSF'] = all_data['TotalBsmtSF'].fillna(0)","84d7a1f9":"#SaleType\nall_data['SaleType'].value_counts().sort_values(ascending = False).head(3)","5e086a07":"#WD is by far the most popular 'SaleType'\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])","ff8ba58b":"#GarageYrBlt is throwing me for a loop, this most likely means that there isn't a garage.\n#Since the data is a year, inputting 0 would skew this data very much and could lead to \n#correlations not being accurate\n## Filling null values with same year the house was built.\nall_data['GarageYrBlt'] = all_data['GarageYrBlt'].fillna(all_data['YearBuilt'])","3c757afd":"## Double Check that there is no more missing values\n## The SalePrice is due to the test set not having that column\nmis_val_percent = (100 * all_data.isnull().sum() \/ len(all_data)).sort_values(ascending = False)\nprint(mis_val_percent[mis_val_percent > 0])","e6cddc23":"all_data = pd.get_dummies(all_data)","d9df10cc":"print('Size Before Dummy Variables (Just training): ', training.shape)\nprint('Size After Dummy Variables: ', all_data.shape)","57964149":"training = all_data.loc[all_data['SalePrice'].notnull()]\ntesting = all_data.loc[all_data['SalePrice'].isnull()]","6a228e57":"# A skew of 0 represents a normal distribution.\n# 'SalePrice' has a positive skew, shown by the skew > 0.\ntraining['SalePrice'].skew()","b181606b":"#Visually we can see the positive skew by the tail on the right of the histogram\nplt.hist(training['SalePrice'])\nplt.show()","eca53335":"#Check normality of the dependent variable\n#Base on the p-value (less than 0.05), we can assume the data is not normally distributed\nscipy.stats.shapiro(training['SalePrice'])","f559d1e9":"#A transformation should be done on the 'SalePrice' to attempt to bring it closer to noraml\n#Some methods for a positive skew are square root, cube root and log transformations.\n\nSalePrice_sqrt = training['SalePrice'] ** (1\/2)\nSalePrice_cubert = training['SalePrice'] ** (1\/3)\nSalePrice_log = np.log(training['SalePrice'])\n\nprint('The skew of the square root is', SalePrice_sqrt.skew())\nprint('The skew of the cube root is', SalePrice_cubert.skew())\nprint('The skew of the log is', SalePrice_log.skew())","4ce6abd3":"#All the transformations increased the normality, \n#however the log transformation is the closest to 0\ntraining['SalePrice(log)'] = SalePrice_log","19903720":"#The distribution of the log transformation\nplt.hist(training['SalePrice(log)'])\nplt.show()","19a2835c":"# Dependent Variable\nWe have the assumption that the dependent variable is normally distributed.  If this is not the case, a data transformation will be needed.  There are multiple ways that we can check the normality of the dependent variable.","2995162d":"## Filling Missing Data","070164e6":"# Train and Test Set\nNow that all the missing data is taken care of and the the number of columns is same (due to the dummy variables), we need to split the data back into the training and testing data.  We can do this because we know that the test set does not have the 'SalePrice' column.","5ef8ffe1":"## Missing Data\nFirst things first, dealing with the missing data.  ","41122a07":"After a lot of research, I finally just got started.  This mostly deals with the missing values, some I had to look through other kernals for, some I threw some ideas together.  \nCreated dummy variables after all the null values were taken care of.  \nSeperated back into training and testing sets.\nTransformed the dependent variable to create a more normal distribution.","a8a1f5bd":"There went from 81 columns to 281 columns.","cb7beeef":"# Dummy Variables\nWith so many variables that are categorical, we need to create dummy variables to be able to represent them numerically.","1d2072a7":"## Import\nImport both the training and testing data, then combine them to eliminate missing data together and to create dummy variables (this will ensure all dummy variable categories are created for each)."}}