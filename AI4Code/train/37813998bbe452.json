{"cell_type":{"f3d26ad2":"code","8d99b216":"code","6c259cb8":"code","f350c6c7":"code","b8900f77":"code","34cfa0af":"code","52c8e4e2":"code","81c6d26b":"code","86440576":"code","72114fc3":"code","b6c43a33":"code","7ffaccb9":"code","2fca37e4":"code","e525a349":"code","9cf3ce0a":"code","c56e7f68":"code","f3c25147":"code","a9676965":"code","6fd8805f":"code","e9df801d":"code","ebbc38b0":"code","f077582e":"markdown","a3806686":"markdown","78b5bfa6":"markdown","429481a9":"markdown","859e706c":"markdown","b5c3109b":"markdown","00f444a9":"markdown","29ada622":"markdown","49485709":"markdown","6d347563":"markdown","2299aeb5":"markdown","a6b00aba":"markdown","8d7a8d38":"markdown","0339098f":"markdown","a759ad68":"markdown","3e1c74ba":"markdown","c55d55ea":"markdown","01c1a993":"markdown","21173909":"markdown","6e92fd33":"markdown","2ad6db70":"markdown","cc8b6251":"markdown","6fe65de9":"markdown"},"source":{"f3d26ad2":"import pandas as pd\n!pip install texthero -q\nimport texthero as hero","8d99b216":"ads_df = pd.read_csv(\"\/kaggle\/input\/political-advertisements-from-facebook\/fbpac-ads-en-US.csv\")\nads_df.head()","6c259cb8":"ads_df.shape","f350c6c7":"SAMPLE_SIZE = 10000\nads_df_ = ads_df.sample(SAMPLE_SIZE)","b8900f77":"print(ads_df_.columns.values)","34cfa0af":"ads_df_.describe()","52c8e4e2":"def get_text_columns(df):\n    text_columns = []\n    for col in df.select_dtypes('object'):\n        if (df[col].str.split().str.len() > 5).any():\n            text_columns.append(df[col].name)\n    return text_columns\n\nget_text_columns(ads_df_)","81c6d26b":"TOP_WORDS = 10\n\nhero.top_words(ads_df_.title)[:TOP_WORDS]","86440576":"hero.top_words(ads_df_.title.str.lower())[:TOP_WORDS]","72114fc3":"(\n    ads_df_.title.str.lower()\n    .dropna()\n    .pipe(hero.remove_stopwords)\n    .pipe(hero.top_words)[:10]\n)","b6c43a33":"(\n    ads_df_.message.str.lower()\n    .dropna()\n    .pipe(hero.remove_stopwords)\n    .pipe(hero.top_words)[:10]\n)","7ffaccb9":"def remove_html_tags(s: pd.Series) -> pd.Series():\n    \"\"\"Remove all html entities from a pandas series\"\"\"\n    \n    # TODO. Consider this more sophisticated solution: ('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n    \n    return s.str.replace('<.*?>', '')\n\ns = pd.Series(\"<p>Hello world!<\/p>\")\nremove_html_tags(s)","2fca37e4":"ads_df_['message'] = remove_html_tags(ads_df_['message'])\n\n(\n    ads_df_.message.str.lower()\n    .dropna()\n    .pipe(hero.remove_stopwords)\n    .pipe(hero.top_words)[:10]\n)","e525a349":"hero.wordcloud(ads_df_.title)","9cf3ce0a":"hero.wordcloud(ads_df_.message)","c56e7f68":"ads_df['advertiser'].value_counts()[:10]","f3c25147":"trump_df = ads_df[ads_df['advertiser'] == 'Donald J. Trump'].copy()\ntrump_df.title.unique()","a9676965":"trump_df['message'] = remove_html_tags(trump_df['message'])\n\n(\n    trump_df.message.str.lower()\n    .pipe(hero.remove_stopwords)\n    .pipe(hero.top_words)[:10]\n)","6fd8805f":"trump_df['noun_chunks'] = hero.nlp.noun_chunks(trump_df.message)\ntrump_df['noun_chunks'].head(2)","e9df801d":"help(hero.nlp.noun_chunks)","ebbc38b0":"trump_df['noun_chunks'].apply(lambda row: [r[0] for r in row]).explode().value_counts()[:20]","f077582e":"A short google search reveals us that Beto O'Rourke is an American politicians (what a surprise!): \"Robert Francis \"Beto\" O'Rourke is a Decocratic American politician who represented Texas's 16th congressional district in the United States House of Representatives from 2013 to 2019\".","a3806686":"hmm, let's try again with lowercased text:","78b5bfa6":"For faster computation, we sub-sample 10k rows.","429481a9":"All the Trump's advertisements just contains `Donald J. Trump`. hmm, what about the content?","859e706c":"We are left with 162'324 advertisements.","b5c3109b":"As supposed, among others we found column `message` and `title`. We could have found the same results by ourself, just by looking at the table description, doing programmatically is more fun.\n\nWe notice also the `entities` columns, this are entites from the text that have been extracted with a spefcific software. We might want to use our own methods for extract entities and compare it with their version.","00f444a9":"In this notebook, we are interested in **columns with text data**. Let's find them.","29ada622":"Hm, that's look strange. `top_words` returns to us all the words present in every columns of the Pandas series. If, for instance, the series is composed of a single row with content `hello world!`, `top_words` first split into words, i.e `hello` and `word` and not `word!`, and then it count.\n\nAs the `message` columns contains html tags, the top_words refer to the words inside a tag (i.e `<p>` becomes `p`). We need therefore to get rid of html tags.","49485709":"#### Show me some pics!\n\nYou are right; this notebook is boring. Let's add some images and colors. Let's start with old-style wordcloud:","6d347563":"![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/a\/ad\/Beto_O%27Rourke%2C_Official_portrait%2C_113th_Congress.jpg\/440px-Beto_O%27Rourke%2C_Official_portrait%2C_113th_Congress.jpg)","2299aeb5":"##### Show me some interesting data. What does the Donald Trump's ads says?\n\nAll right.","a6b00aba":"As expected, we can spot some political words such as 'committe', 'international' and 'action'. Just by looking at the top words, we can have a feeling of the dataset.\n\nBut, there are many stopwords such as 'for', 'of', 'the' that does not help much. Let's get rid of them and try again.\n\nFor that, we make use of the powerful and handy pandas `pipe` function.","8d7a8d38":"The `noun_chunks` functions return extra information regarding each noun_chunk found in the sentence, including the part-of-speech tagging and the start and end index of the noun chunk in the sentence.\n\nHere, we are simply interested in getting all noun_chunks and find the most relevant ones:","0339098f":"Continuation coming soon.","a759ad68":"##### What are the most common words in Ads title?","3e1c74ba":"## FB ADS: text mining and exploratory data analysis\n\nIn this simple notebook, I will go though the \"Political Advertisements from Facebook\" dataset and will try to get some relevant insights out of it.\n\nDisclaimer: \n- In this notebook I make use of a python package for text analytics I'm working on called [texthero](https:\/\/github.com\/jbesomi\/texthero\/). Texthero is still in alpha version and it's a work-in-progress.\n- Work-in-progress; I will keep working on it in the next few days.","c55d55ea":"Let's look at the different columns:","01c1a993":"... and ACLU is the \"[American Civil Liberties Union](https:\/\/www.aclu.org\/)\".\n\nAmong other, there is also the actual President of U.S.A J. Donald Trump with 1443 advertisements.","21173909":"Here we go. It's indeed interesting to notice how the top words for the title and the message are different. The top words of the message are `us`, `help`, `people`, `need` that sounds like words used in call-for-action sentence: \"**We need to get your vote today!**\"","6e92fd33":"##### Who are the principal advertiser?","2ad6db70":"##### What are the most common words in the 'message'?\n\nLet's repeat the same action again, this time on the message column. We will skip the temporary steps and look just at the final result without _stopwords_.","cc8b6251":"We start by loading and displaying the dataframe. As there are more than 3GB of data, this might take a while.","6fe65de9":"What about \"let's make America great again?\"\n\nRather than looking at single terms, let's look at the top n-grams:"}}