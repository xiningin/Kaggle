{"cell_type":{"9664c5cb":"code","34273918":"code","de81845c":"code","6b37e6e5":"code","ccbf51f4":"code","2d7fa63d":"code","95eed332":"code","cc29fdc2":"code","7acfb763":"code","8653d97e":"code","d55226ed":"code","e779008a":"code","14c609db":"code","a275f230":"code","510fa16a":"code","1dfe6a88":"code","3fc88cf9":"code","a1f84fbe":"code","817bdbef":"code","18a5d752":"code","58b28a87":"code","a0aae8a3":"code","c7a6a75d":"code","c80a12c4":"markdown","88dcc886":"markdown","d41b2b03":"markdown","6690fe07":"markdown","cf72b677":"markdown","84be2d15":"markdown","39a8898b":"markdown","ed17caf0":"markdown","81357c68":"markdown","103bf8a9":"markdown","2800b18e":"markdown","4b96e6bb":"markdown","30834b0d":"markdown","87451208":"markdown"},"source":{"9664c5cb":"from sklearn.datasets import load_iris\ndata = load_iris()","34273918":"X = data.data[:,[0,2]].T\ny = data.target","de81845c":"from sklearn import preprocessing\nlb = preprocessing.LabelBinarizer()\nY = lb.fit_transform(y)\nY = Y.T\nprint(X.shape)\nprint(Y.shape)\ntype(data)","6b37e6e5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn","ccbf51f4":"type(data)","2d7fa63d":"data.keys()","95eed332":"\nn_samples, n_features = data.data.shape\nprint('Number of samples:', n_samples)\nprint('Number of features:', n_features)\n# the sepal length, sepal width, petal length and petal width of the first sample (first flower)\nprint(data.data[0])","cc29fdc2":"print(data.target)","7acfb763":"print(data.data.shape)\nprint(data.target.shape)","8653d97e":"np.bincount(data.target)","d55226ed":"import matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nx_index = 3\ncolors = ['blue', 'red', 'green']\n\nfor label, color in zip(range(len(data.target_names)), colors):\n    ax.hist(data.data[data.target==label, x_index], \n            label=data.target_names[label],\n            color=color)\n\nax.set_xlabel(data.feature_names[x_index])\nax.legend(loc='upper right')\nfig.show()","e779008a":"fig, ax = plt.subplots()\n\nx_index = 3\ny_index = 0\n\ncolors = ['blue', 'red', 'green']\n\nfor label, color in zip(range(len(data.target_names)), colors):\n    ax.scatter(data.data[data.target==label, x_index], \n                data.data[data.target==label, y_index],\n                label=data.target_names[label],\n                c=color)\n\nax.set_xlabel(data.feature_names[x_index])\nax.set_ylabel(data.feature_names[y_index])\nax.legend(loc='upper left')\nplt.show()","14c609db":"import pandas as pd\n    \niris_df = pd.DataFrame(data.data, columns=data.feature_names)\npd.plotting.scatter_matrix(iris_df, \n                           c=data.target, \n                           figsize=(8, 8)\n                          );","a275f230":"def weights_init(layer_dims,init_type='he_normal',seed=0):\n    \"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the dimensions of each layer in our network\n                  layer_dims lis is like  [ no of input features,# of neurons in hidden layer-1,..,\n                                     # of neurons in hidden layer-n shape,output]\n    init_type -- he_normal  --> N(0,sqrt(2\/fanin))\n                 he_uniform --> Uniform(-sqrt(6\/fanin),sqrt(6\/fanin))\n                 xavier_normal --> N(0,2\/(fanin+fanout))\n                 xavier_uniform --> Uniform(-sqrt(6\/fanin+fanout),sqrt(6\/fanin+fanout))\n    seed -- random seed to generate weights\n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n                    bl -- bias vector of shape (layer_dims[l], 1)\n    \"\"\"\n    np.random.seed(seed)\n    parameters = {}\n    L = len(layer_dims)            # number of layers in the network\n    if  init_type == 'he_normal':\n        for l in range(1, L):\n            parameters['W' + str(l)] = np.random.normal(0,np.sqrt(2.0\/layer_dims[l-1]),(layer_dims[l], layer_dims[l-1]))\n            parameters['b' + str(l)] = np.random.normal(0,np.sqrt(2.0\/layer_dims[l-1]),(layer_dims[l], 1))\n            \n    return parameters","510fa16a":"def ReLU(X,alpha=0,derivative=False):\n    # Compute ReLU function and derivative\n    X = np.array(X,dtype=np.float64)\n    if derivative == False:\n        return np.where(X<0,alpha*X,X)\n    elif derivative == True:\n        X_relu = np.ones_like(X,dtype=np.float64)\n        X_relu[X < 0] = alpha\n        return X_relu\n    \ndef softmax(X):\n    # Compute softmax values for each sets of scores in x.\n    return np.exp(X) \/ np.sum(np.exp(X),axis=0)","1dfe6a88":"def weights_init(layer_dims,init_type='he_normal',seed=None):\n        \"\"\"\n        Arguments:\n            layer_dims -- python array (list) containing the dimensions of each layer in our network\n            layer_dims lis is like  [ no of input features,# of neurons in hidden layer-1,..,\n                                     # of neurons in hidden layer-n shape,output]\n            init_type -- he_normal  --> N(0,sqrt(2\/fanin))\n                         seed -- random seed to generate weights\n        Returns:\n            parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n                    bl -- bias vector of shape (layer_dims[l], 1)\n        \"\"\"\n        np.random.seed(seed)\n        parameters = {}\n        opt_parameters = {}\n        L = len(layer_dims)            # number of layers in the network\n        if  init_type == 'he_normal':\n            for l in range(1, L):\n                parameters['W' + str(l)] = np.random.normal(0,np.sqrt(2.0\/layer_dims[l-1]),(layer_dims[l], layer_dims[l-1]))\n                parameters['b' + str(l)] = np.random.normal(0,np.sqrt(2.0\/layer_dims[l-1]),(layer_dims[l], 1))  \n        \n        return parameters","3fc88cf9":"def forward_propagation(X, hidden_layers,parameters,keep_proba=1,seed=None):\n    \"\"\"\n    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n    \n    Arguments:\n    X -- data, numpy array of shape (input size, number of examples)\n    hidden_layers -- List of hideden layers\n    weights -- Output of weights_init dict (parameters)\n    keep_prob -- probability of keeping a neuron active during drop-out, scalar\n    Returns:\n    AL -- last post-activation value\n    caches -- list of caches containing:\n                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n    \"\"\"\n    if seed != None:\n        np.random.seed(seed)\n    caches = []\n    A = X\n    L = len(hidden_layers)\n    for l,active_function in enumerate(hidden_layers,start=1):\n        A_prev = A \n        \n        Z = np.dot(parameters['W' + str(l)],A_prev)+parameters['b' + str(l)]\n        \n        if active_function == \"relu\":\n            A = ReLU(Z)\n        elif active_function == \"softmax\":\n            A = softmax(Z)\n            \n        if keep_proba != 1 and l != L and l != 1:\n            D = np.random.rand(A.shape[0],A.shape[1])\n            D = (D<keep_prob)\n            A = np.multiply(A,D)\n            A = A \/ keep_prob\n            cache = ((A_prev, parameters['W' + str(l)],parameters['b' + str(l)],D), Z)\n            caches.append(cache_temp)\n        else:\n            cache = ((A_prev, parameters['W' + str(l)],parameters['b' + str(l)]), Z)\n            #print(A.shape)\n            caches.append(cache)\n            \n    return A, caches","a1f84fbe":"def compute_cost(A, Y, parameters, lamda=0,penality=None):\n        \"\"\"\n    Implement the cost function with L2 regularization. See formula (2) above.\n    \n    Arguments:\n    A -- post-activation, output of forward propagation\n    Y -- \"true\" labels vector, of shape (output size, number of examples)\n    parameters -- python dictionary containing parameters of the model\n    \n    Returns:\n    cost - value of the regularized loss function \n    \"\"\"\n        m = Y.shape[1]\n\n        cost = np.squeeze(-np.sum(np.multiply(np.log(A),Y))\/m)\n\n        L = len(parameters)\/\/2\n\n        if penality == 'l2' and lamda != 0:\n            sum_weights = 0\n            for l in range(1, L):\n                sum_weights = sum_weights + np.sum(np.square(parameters['W' + str(l)]))\n            cost = cost + sum_weights * (lambd\/(2*m))\n        elif penality == 'l1' and lamda != 0:\n            sum_weights = 0\n            for l in range(1, L):\n                sum_weights = sum_weights + np.sum(np.abs(parameters['W' + str(l)]))\n            cost = cost + sum_weights * (lambd\/(2*m))\n\n        return cost","817bdbef":"def back_propagation(AL, Y, caches, hidden_layers, keep_prob=1, penality=None,lamda=0):\n    \"\"\"\n    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n    \n    Arguments:\n    AL -- probability vector, output of the forward propagation (L_model_forward())\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n    caches -- list of caches containing:\n    hidden_layers -- hidden layer names\n    keep_prob -- probabaility for dropout\n    penality -- regularization penality 'l1' or 'l2' or None\n    \n    \n    Returns:\n    grads -- A dictionary with the gradients\n             grads[\"dA\" + str(l)] = ... \n             grads[\"dW\" + str(l)] = ...\n             grads[\"db\" + str(l)] = ... \n    \"\"\"\n    grads = {}\n    L = len(caches) # the number of layers\n    \n    m = AL.shape[1]\n    Y = Y.reshape(AL.shape)\n    \n    # Initializing the backpropagation\n    dZL = AL - Y\n    \n    cache = caches[L-1]\n    linear_cache, activation_cache = cache\n    AL, W, b = linear_cache\n    grads[\"dW\" + str(L)] = np.dot(dZL,AL.T)\/m\n    grads[\"db\" + str(L)] = np.sum(dZL,axis=1,keepdims=True)\/m\n    grads[\"dA\" + str(L-1)] = np.dot(W.T,dZL)\n    \n    \n    # Loop from l=L-2 to l=0\n    v_dropout = 0\n    for l in reversed(range(L-1)):\n        cache = caches[l]\n        active_function = hidden_layers[l]\n        \n        linear_cache, Z = cache\n        \n            \n        m = A_prev.shape[1]\n        \n        if keep_prob != 1 and v_dropout == 1:\n            dA_prev = np.multiply(grads[\"dA\" + str(l + 1)],D)\n            dA_prev = dA_prev\/keep_prob\n            v_dropout = 0\n        else:\n            dA_prev = grads[\"dA\" + str(l + 1)]\n            v_dropout = 0\n            \n        \n        if active_function == \"relu\":\n            dZ = np.multiply(dA_prev,ReLU(Z,derivative=True))\n        \n            \n            \n        grads[\"dA\" + str(l)] = np.dot(W.T,dZ)\n        \n        if penality == 'l2':\n            grads[\"dW\" + str(l + 1)] = (np.dot(dZ,A_prev.T)\/m)  + ((lambd * W)\/m)\n        elif penality == 'l1':\n            grads[\"dW\" + str(l + 1)] = (np.dot(dZ,A_prev.T)\/m)  + ((lambd * np.sign(W+10**-8))\/m)\n        else:\n            grads[\"dW\" + str(l + 1)] = (np.dot(dZ,A_prev.T)\/m)\n            \n        grads[\"db\" + str(l + 1)] = np.sum(dZ,axis=1,keepdims=True)\/m\n        \n        \n    return grads","18a5d752":"def update_parameters(parameters, grads,learning_rate,iter_no,method = 'ADAM',opt_params=None,beta1=0.9,beta2=0.999):\n    \"\"\"\n    Update parameters using gradient descent\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \n    grads -- python dictionary containing your gradients, output of L_model_backward\n    method -- method for updation of weights 'ADAM'\n    learning rate -- learning rate alpha value\n    beta1 -- weighted avg parameter for  ADAM\n    beta2 -- weighted avg parameter for  ADAM\n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n                  parameters[\"W\" + str(l)] = ... \n                  parameters[\"b\" + str(l)] = ...\n    \"\"\"\n    L = len(parameters) \/\/ 2 # number of layers in the neural network\n    \n    if method == 'ADAM':\n        for l in range(L):\n            opt_parameters['vdb'+str(l+1)] = beta1*opt_parameters['vdb'+str(l+1)] + (1-beta1)*grads[\"db\" + str(l + 1)]\n            opt_parameters['vdw'+str(l+1)] = beta1*opt_parameters['vdw'+str(l+1)] + (1-beta1)*grads[\"dW\" + str(l + 1)]\n            opt_parameters['sdb'+str(l+1)] = beta2*opt_parameters['sdb'+str(l+1)] + (1-beta2)*np.square(grads[\"db\" + str(l + 1)])\n            opt_parameters['sdw'+str(l+1)] = beta2*opt_parameters['sdw'+str(l+1)] + (1-beta2)*np.square(grads[\"dW\" + str(l + 1)])\n            \n            learningrate = learning_rate * np.sqrt((1-beta2**iter_no)\/((1-beta1**iter_no)+10**-8))\n            parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - \\\n                                       learning_rate*(opt_parameters['vdw'+str(l+1)]\/\\\n                                                      (np.sqrt(opt_parameters['sdw'+str(l+1)])+10**-8))\n            parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - \\\n                                       learning_rate*(opt_parameters['vdb'+str(l+1)]\/\\\n                                                      (np.sqrt(opt_parameters['sdb'+str(l+1)])+10**-8))\n        \n    return parameters,opt_parameters\n","58b28a87":"def predict(parameters, X,hidden_layers,return_prob=False):\n    \"\"\"\n    Using the learned parameters, predicts a class for each example in X\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \n    X -- input data of size (n_x, m)\n    \n    Returns\n    predictions -- vector of predictions of our model (red: 0 \/ blue: 1)\n    \"\"\"\n    A, cache = forward_propagation(X,hidden_layers,parameters,seed=3)\n    if return_prob == True:\n        return A\n    else:\n        return np.argmax(A, axis=0)","a0aae8a3":"class DNNClassifier(object):\n\n    \n    def __init__(self,layer_dims,hidden_layers,init_type='he_normal',learning_rate=0.1, optimization_method = 'ADAM',batch_size=64,max_epoch=100,tolarance = 0.00001, keep_proba=1,penality=None,lamda=0,beta1=0.9, beta2=0.999,seed=None,verbose=0):\n        self.layer_dims = layer_dims\n        self.hidden_layers = hidden_layers\n        self.init_type = init_type\n        self.learning_rate = learning_rate\n        self.optimization_method = optimization_method\n        self.batch_size = batch_size\n        self.keep_proba = keep_proba\n        self.penality = penality\n        self.lamda = lamda\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.seed = seed\n        self.max_epoch = max_epoch\n        self.tol = tolarance\n        self.verbose = verbose\n\n    @staticmethod\n    def weights_init(layer_dims,init_type='he_normal',seed=None):\n        \"\"\"\n        Arguments:\n            layer_dims -- python array (list) containing the dimensions of each layer in our network\n            layer_dims lis is like  [ no of input features,# of neurons in hidden layer-1,..,\n                                     # of neurons in hidden layer-n shape,output]\n            init_type -- he_normal  --> N(0,sqrt(2\/fanin))\n                         seed -- random seed to generate weights\n        Returns:\n            parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n                    bl -- bias vector of shape (layer_dims[l], 1)\n        \"\"\"\n        np.random.seed(seed)\n        parameters = {}\n        opt_parameters = {}\n        L = len(layer_dims)            # number of layers in the network\n        if  init_type == 'he_normal':\n            for l in range(1, L):\n                parameters['W' + str(l)] = np.random.normal(0,np.sqrt(2.0\/layer_dims[l-1]),(layer_dims[l], layer_dims[l-1]))\n                parameters['b' + str(l)] = np.random.normal(0,np.sqrt(2.0\/layer_dims[l-1]),(layer_dims[l], 1))  \n        \n        return parameters\n    \n    @staticmethod\n    def sigmoid(X,derivative=False):\n        '''Compute Sigmaoid and its derivative'''\n        if derivative == False:\n            out = 1 \/ (1 + np.exp(-np.array(X)))\n        elif derivative == True:\n            s = 1 \/ (1 + np.exp(-np.array(X)))\n            out = s*(1-s)\n        return out\n    @staticmethod\n    def ReLU(X,alpha=0,derivative=False):\n        '''Compute ReLU function and derivative'''\n        X = np.array(X,dtype=np.float64)\n        if derivative == False:\n            return np.where(X<0,alpha*X,X)\n        elif derivative == True:\n            X_relu = np.ones_like(X,dtype=np.float64)\n            X_relu[X < 0] = alpha\n            return X_relu\n    \n    @staticmethod\n    def softmax(X):\n        return np.exp(X) \/ np.sum(np.exp(X),axis=0)\n    @staticmethod\n    def forward_propagation(X, hidden_layers,parameters,keep_prob=1,seed=None):\n        \"\"\"\n        Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n\n        Arguments:\n        X -- data, numpy array of shape (input size, number of examples)\n        hidden_layers -- List of hideden layers\n        weights -- Output of weights_init dict (parameters)\n        keep_prob -- probability of keeping a neuron active during drop-out, scalar\n        Returns:\n        AL -- last post-activation value\n        caches -- list of caches containing:\n                    every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n        \"\"\"\n        if seed != None:\n            np.random.seed(seed)\n        caches = []\n        A = X\n        L = len(hidden_layers)\n        for l,active_function in enumerate(hidden_layers,start=1):\n            A_prev = A \n        \n            Z = np.dot(parameters['W' + str(l)],A_prev)+parameters['b' + str(l)]\n            \n            if type(active_function) is tuple:\n                \n                if  active_function[0] == \"relu\":\n                    A = DNNClassifier.ReLU(Z,active_function[1])\n\n            else:\n                if active_function == \"sigmoid\":\n                    A = DNNClassifier.sigmoid(Z)\n                    A = DNNClassifier.Tanh(Z)\n                elif active_function == \"softmax\":\n                    A = DNNClassifier.softmax(Z)\n                elif  active_function == \"relu\":\n                    A = DNNClassifier.ReLU(Z)\n                \n            \n            if keep_prob != 1 and l != L and l != 1:\n                D = np.random.rand(A.shape[0],A.shape[1])\n                D = (D<keep_prob)\n                A = np.multiply(A,D)\n                A = A \/ keep_prob\n                cache = ((A_prev, parameters['W' + str(l)],parameters['b' + str(l)],D), Z)\n                caches.append(cache)\n            else:\n                cache = ((A_prev, parameters['W' + str(l)],parameters['b' + str(l)]), Z)\n                #print(A.shape)\n                caches.append(cache)      \n        return A, caches\n\n    @staticmethod\n    def compute_cost(A, Y, parameters, lamda=0,penality=None):\n        \"\"\"\n        Implement the cost function with L2 regularization. See formula (2) above.\n\n        Arguments:\n        A -- post-activation, output of forward propagation\n        Y -- \"true\" labels vector, of shape (output size, number of examples)\n        parameters -- python dictionary containing parameters of the model\n\n        Returns:\n        cost - value of the regularized loss function \n        \"\"\"\n        m = Y.shape[1]\n    \n        cost = np.squeeze(-np.sum(np.multiply(np.log(A),Y))\/m)\n    \n        L = len(parameters)\/\/2\n    \n        if penality == 'l2' and lamda != 0:\n            sum_weights = 0\n            for l in range(1, L):\n                sum_weights = sum_weights + np.sum(np.square(parameters['W' + str(l)]))\n            cost = cost + sum_weights * (lamda\/(2*m))\n        elif penality == 'l1' and lamda != 0:\n            sum_weights = 0\n            for l in range(1, L):\n                sum_weights = sum_weights + np.sum(np.abs(parameters['W' + str(l)]))\n            cost = cost + sum_weights * (lamda\/(2*m))\n        return cost\n    @staticmethod\n    def back_propagation(AL, Y, caches, hidden_layers, keep_prob=1, penality=None,lamda=0):\n        \"\"\"\n        Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n\n        Arguments:\n        AL -- probability vector, output of the forward propagation (L_model_forward())\n        Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n        caches -- list of caches containing:\n        hidden_layers -- hidden layer names\n        keep_prob -- probabaility for dropout\n        penality -- regularization penality 'l1' or 'l2' or None\n\n\n        Returns:\n        grads -- A dictionary with the gradients\n                 grads[\"dA\" + str(l)] = ... \n                 grads[\"dW\" + str(l)] = ...\n                 grads[\"db\" + str(l)] = ... \n        \"\"\"\n        grads = {}\n        L = len(caches) # the number of layers\n    \n        m = AL.shape[1]\n        Y = Y.reshape(AL.shape)\n    \n        # Initializing the backpropagation\n        dZL = AL - Y\n    \n        cache = caches[L-1]\n        linear_cache, activation_cache = cache\n        AL, W, b = linear_cache\n        grads[\"dW\" + str(L)] = np.dot(dZL,AL.T)\/m\n        grads[\"db\" + str(L)] = np.sum(dZL,axis=1,keepdims=True)\/m\n        grads[\"dA\" + str(L-1)] = np.dot(W.T,dZL)\n    \n    \n        # Loop from l=L-2 to l=0\n        v_dropout = 0\n        for l in reversed(range(L-1)):\n            cache = caches[l]\n            active_function = hidden_layers[l]\n        \n            linear_cache, Z = cache\n            try:\n                A_prev, W, b = linear_cache\n            except:\n                A_prev, W, b, D = linear_cache\n                v_dropout = 1\n            \n            m = A_prev.shape[1]\n        \n            if keep_prob != 1 and v_dropout == 1:\n                dA_prev = np.multiply(grads[\"dA\" + str(l + 1)],D)\n                dA_prev = dA_prev\/keep_prob\n                v_dropout = 0\n            else:\n                dA_prev = grads[\"dA\" + str(l + 1)]\n                v_dropout = 0\n            \n            \n            if type(active_function) is tuple:\n                \n                if  active_function[0] == \"relu\":\n                    dZ = np.multiply(dA_prev,DNNClassifier.ReLU(Z,active_function[1],derivative=True))\n                \n            else:\n                if active_function == \"sigmoid\":\n                    dZ = np.multiply(dA_prev,DNNClassifier.sigmoid(Z,derivative=True))\n                elif active_function == \"relu\":\n                    dZ = np.multiply(dA_prev,DNNClassifier.ReLU(Z,derivative=True))\n                \n            \n            grads[\"dA\" + str(l)] = np.dot(W.T,dZ)\n        \n            if penality == 'l2':\n                grads[\"dW\" + str(l + 1)] = (np.dot(dZ,A_prev.T)\/m)  + ((lamda * W)\/m)\n            elif penality == 'l1':\n                grads[\"dW\" + str(l + 1)] = (np.dot(dZ,A_prev.T)\/m)  + ((lamda * np.sign(W+10**-8))\/m)\n            else:\n                grads[\"dW\" + str(l + 1)] = (np.dot(dZ,A_prev.T)\/m)\n            \n            grads[\"db\" + str(l + 1)] = np.sum(dZ,axis=1,keepdims=True)\/m   \n        return grads\n    \n    @staticmethod\n    def update_parameters(parameters, grads,learning_rate,iter_no,method = 'ADAM',opt_parameters=None,beta1=0.9,beta2=0.999):\n        \"\"\"\n        Update parameters using gradient descent\n    \n        Arguments:\n        parameters -- python dictionary containing your parameters \n        grads -- python dictionary containing your gradients, output of L_model_backward\n        method -- method for updation of weights 'ADAM'\n        learning rate -- learning rate alpha value\n        beta1 -- weighted avg parameter for ADAM\n        beta2 -- weighted avg parameter for ADAM\n\n        Returns:\n        parameters -- python dictionary containing your updated parameters \n                      parameters[\"W\" + str(l)] = ... \n                      parameters[\"b\" + str(l)] = ...\n        \"\"\"\n        L = len(parameters) \/\/ 2 # number of layers in the neural network\n        \n        if method == 'ADAM':\n            for l in range(L):\n                opt_parameters['vdb'+str(l+1)] = beta1*opt_parameters['vdb'+str(l+1)] + (1-beta1)*grads[\"db\" + str(l + 1)]\n                opt_parameters['vdw'+str(l+1)] = beta1*opt_parameters['vdw'+str(l+1)] + (1-beta1)*grads[\"dW\" + str(l + 1)]\n                opt_parameters['sdb'+str(l+1)] = beta2*opt_parameters['sdb'+str(l+1)] + \\\n                                                                  (1-beta2)*np.square(grads[\"db\" + str(l + 1)])\n                opt_parameters['sdw'+str(l+1)] = beta2*opt_parameters['sdw'+str(l+1)] + \\\n                                                                   (1-beta2)*np.square(grads[\"dW\" + str(l + 1)])\n            \n                learning_rate = learning_rate * np.sqrt((1-beta2**iter_no)\/((1-beta1**iter_no)+10**-8))\n                parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - \\\n                                       learning_rate*(opt_parameters['vdw'+str(l+1)]\/\\\n                                                      (np.sqrt(opt_parameters['sdw'+str(l+1)])+10**-8))\n                parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - \\\n                                       learning_rate*(opt_parameters['vdb'+str(l+1)]\/\\\n                                                      (np.sqrt(opt_parameters['sdb'+str(l+1)])+10**-8))\n        \n        return parameters,opt_parameters\n    \n    def fit(self,X,y):\n    \n        # X -- data, numpy array of shape (input size, number of examples)\n        # y -- lables, numpy array of shape (no of classes,n)\n        \n        np.random.seed(self.seed)\n        self.grads = {}\n        self.costs = []\n        M = X.shape[1]\n        opt_parameters = {}\n        \n        if self.verbose == 1:\n            print('Initilizing Weights...')\n        self.parameters = self.weights_init(self.layer_dims,self.init_type,self.seed)\n        self.iter_no = 0\n        idx = np.arange(0,M)\n        \n        if self.optimization_method == 'ADAM':\n            for l in range(1, len(self.layer_dims)):\n                opt_parameters['vdw' + str(l)] = np.zeros((self.layer_dims[l], self.layer_dims[l-1]))\n                opt_parameters['vdb' + str(l)] = np.zeros((self.layer_dims[l], 1))\n                opt_parameters['sdw' + str(l)] = np.zeros((self.layer_dims[l], self.layer_dims[l-1]))\n                opt_parameters['sdb' + str(l)] = np.zeros((self.layer_dims[l], 1)) \n        \n        if self.verbose == 1:\n            print('Starting Training...')\n            \n        for epoch_no in range(1,self.max_epoch+1):\n            np.random.shuffle(idx)\n            X = X[:,idx]\n            y = y[:,idx]\n            for i in range(0,M, self.batch_size):\n                self.iter_no = self.iter_no + 1\n                X_batch = X[:,i:i + self.batch_size]\n                y_batch = y[:,i:i + self.batch_size]\n                # Forward propagation:\n                AL, cache = self.forward_propagation(X_batch,self.hidden_layers,self.parameters,self.keep_proba,self.seed)\n                #cost\n                cost = self.compute_cost(AL, y_batch, self.parameters,self.lamda,self.penality)\n                self.costs.append(cost)\n                \n                if self.tol != None:\n                    try:\n                        if abs(cost - self.costs[-2]) < self.tol:\n                            return self\n                    except:\n                        pass\n                #back prop\n                grads = self.back_propagation(AL, y_batch, cache,self.hidden_layers,self.keep_proba,self.penality,self.lamda)\n                \n                #update params\n                self.parameters,opt_parameters = self.update_parameters(self.parameters,grads,self.learning_rate,self.iter_no-1, self.optimization_method, opt_parameters,self.beta1,self.beta2)\n                \n                if self.verbose == 1:\n                    if self.iter_no % 100 == 0:\n                        print(\"Cost after iteration {}: {}\".format(self.iter_no, cost))\n                \n        return self\n    def predict(self,X,proba=False):\n        \"\"\"\n        Using the learned parameters, predicts a class for each example in X\n\n        Arguments:\n        parameters -- python dictionary containing your parameters \n        X -- input data of size (n_x, m)\n\n        Returns\n        predictions -- vector of predictions of our model (red: 0 \/ blue: 1)\n        \"\"\"\n        out, _ = self.forward_propagation(X,self.hidden_layers,self.parameters,self.keep_proba,self.seed)\n        if proba == True:\n            return out.T\n        else:\n            return np.argmax(out, axis=0)","c7a6a75d":"model = DNNClassifier(layer_dims=[X.shape[0], 6, 4, 3],hidden_layers=[('relu',0),('relu',0.001),'softmax'],\n                      optimization_method='ADAM',tolarance=None,max_epoch=900,verbose=1,seed=15)\nmodel.fit(X,Y)\ny_pred = model.predict(X,proba=False)\nprint(y_pred)\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\ndt = data.data[:,[0,2]]\nx_min, x_max = dt[:, 0].min() - 1, dt[:, 0].max() + 1\ny_min, y_max = dt[:, 1].min() - 1, dt[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                     np.arange(y_min, y_max, 0.1))\n\n# orediction\nZ = model.predict(np.c_[xx.ravel(), yy.ravel()].T) \n\n# Plot result\nZ = Z.reshape(xx.shape)\nplt.figure(figsize=(12,8))\nplt.contourf(xx, yy, Z,alpha=0.4)\n#plt.axis('off')\nplt.scatter(dt[:, 0], dt[:, 1], c=y,s=20, edgecolor='k')\nplt.xlabel('petal length')\nplt.ylabel('petal width')\nplt.title('Decision Boundaries')","c80a12c4":"# Introduction\n\nThis kernel is for the people who wanna get started in neural networks. This will serve as a step-by-step guide for them. Today, we are going to classify the famous Iris Flower Dataset from UCI ML Repository using a neural network. **Sounds overkill, right?** But we are going to do this task from scratch using only python and some basic libraries. We will not use TensorFlow or Keras in any way. So, let's get started.","88dcc886":"# The Architecture\n\nWe are going to classify the flowers using using a pretty simple neural network of just an input layer, hidden layer of a few nodes and an output layer.\n\n1)We are going to use HE initialization technique for the initialization of the weights.\n\n2) The activation for the first two layers will be the Rectified Linear Unit (Relu) and softmax unit for the output layer since this is a multi-class classification problem which is beyond the scope of ReLu because of not being a binary classification task.\n\n3) We will use something between L1 and L2 norm cost function for computing the cost.\n\n4) We will use the Adam optimization algorithm to update the parameters.\n\nWe will repeat the following steps for many iterations and then finally predict.\n\nforward propagation -> compute cost -> backward propagation -> update parameters.","d41b2b03":"**Encoding the categorical data using LabelBinarizer from scikit**","6690fe07":"Data in scikit-learn is in most cases saved as two-dimensional Numpy arrays with the shape (n, m). Many algorithms also accept scipy.sparse matrices of the same shape.\n\nn: (n_samples) The number of samples: each sample is an item to process (e.g. classify). A sample can be a document, a picture, a sound, a video, an astronomical object, a row in database or CSV file, or whatever you can describe with a fixed set of quantitative traits.\nm: (n_features) The number of features or distinct traits that can be used to describe each item in a quantitative manner. Features are generally real-valued, but may be Boolean or discrete-valued in some cases.\n\nThe information about the class of each sample of our Iris dataset is stored in the target attribute of the dataset:","cf72b677":"bincount of NumPy counts the number of occurrences of each value in ab array of non-negative ints. We can use this to check the distribution of the classes in the dataset:","84be2d15":"The features of each sample flower are stored in the data attribute of the dataset:","39a8898b":"# Results\n\n**Finally let us see the results and the visualization of how the classifier performs**","ed17caf0":"**Pretty Impressive, right?? Our classifier only missed 3 labels which is like 0.02% error.**\n\nWith that, this kernel comes to an end.\n\nFor more reference:\n\n[Understanding Neural Networks](https:\/\/towardsdatascience.com\/understanding-neural-networks-19020b758230)\n\n[Different Activation Functions](https:\/\/missinglink.ai\/guides\/neural-network-concepts\/7-types-neural-network-activation-functions-right\/#:~:text=Activation%20functions%20are%20mathematical%20equations,relevant%20for%20the%20model's%20prediction.)\n\n[Multilayer Neural Network from scratch](https:\/\/medium.com\/@udaybhaskarpaila\/multilayered-neural-network-from-scratch-using-python-c0719a646855)\n\n[He Normal Initialization](https:\/\/medium.com\/@prateekvishnu\/xavier-and-he-normal-he-et-al-initialization-8e3d7a087528)\n\n[Adam Optimization](https:\/\/machinelearningmastery.com\/adam-optimization-algorithm-for-deep-learning\/#:~:text=Adam%20is%20an%20optimization%20algorithm,iterative%20based%20in%20training%20data.&text=The%20algorithm%20is%20called%20Adam.)\n\n[L1 and L2 as a Cost Function](https:\/\/csmoon-ml.com\/index.php\/2018\/12\/17\/l1-and-l2-as-cost-function\/)\n\n\n**Finally, if you liked the kernel please cast an upvote to support me which helps us stay motivated.**","81357c68":"we can also use the scatterplot matrix provided by the pandas module.\n\nScatterplot matrices show scatter plots between all features in the data set, as well as histograms to show the distribution of each feature.","103bf8a9":"**Importing the dependencies**","2800b18e":"**Now that we are finally done with all the function declarations, let us combine them into a class so we can just call the methods from that class for various tasks**","4b96e6bb":"# Data Visualization of the features\n\n**The feauture data is four dimensional, but we can visualize one or two of the dimensions at a time using a simple histogram or scatter-plot.**","30834b0d":"**Understanding the Data**","87451208":"**Loading the Data**"}}