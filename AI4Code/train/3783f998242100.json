{"cell_type":{"ebeb7db8":"code","93e9561a":"code","924dd99d":"code","be813a2f":"code","e5c9bd10":"code","a0726dba":"code","e99de92f":"code","69e0a9e1":"code","aae13711":"markdown","953c6379":"markdown","f31baffb":"markdown","0a19e7c7":"markdown","95d93d63":"markdown","0874ff14":"markdown","eb23e0e4":"markdown","437d23d4":"markdown"},"source":{"ebeb7db8":"import numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import KFold\n\nimport numpy as np\nimport pandas as pd \n\nimport os\nimport gc\nimport psutil\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, Normalizer,MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\n\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\n\nfrom optuna.integration import LightGBMPruningCallback\n\n# get skewed features to impute median instead of mean\nfrom scipy.stats import skew\nfrom sklearn.ensemble import AdaBoostClassifier\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import linear_model\nfrom sklearn.linear_model import Ridge,Lasso\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import XGBRegressor, XGBRFRegressor\n\nimport itertools\nimport optuna\nfrom lightgbm import LGBMClassifier,LGBMRegressor\nimport lightgbm as lgb\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom catboost import CatBoostRegressor, CatBoostClassifier\n\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","93e9561a":"train_data = pd.read_csv('..\/input\/titanic-create-folds\/TITANIC_Folds.csv') # Read TITANIC_Folds as train_data\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")","924dd99d":"train_data.drop('Cabin',axis= 1,inplace= True)\ntest_data.drop('Cabin',axis= 1,inplace= True)\ntrain_data['Age'] = train_data['Age'].fillna(int(train_data['Age'].mean()))\ntest_data['Age'] = test_data['Age'].fillna(int(test_data['Age'].mean()))\ntrain_data['Embarked'] = train_data['Embarked'].fillna(train_data['Embarked'].mode()[0])\ntest_data['Fare'] = test_data['Fare'].fillna(test_data['Fare'].median())\ntrain_data['isTrain'] = 1\ntest_data['isTrain'] = 0\ntt = pd.concat([train_data,test_data])\ntt.drop(['Name','Ticket'],axis=1,inplace = True)\ntt = pd.get_dummies(tt,columns= ['Sex','Embarked'],drop_first = True)\ntrain_data = tt[tt.isTrain == 1]\ntest_data = tt[tt.isTrain == 0]\ntest_data.drop(['Survived','isTrain','fold'],axis=1,inplace = True)\ntrain_data.drop(['isTrain'],axis=1,inplace= True)\n\n# Note don't drop PassengerId column of train_data and test_data as it is used later. Instead drop it only for test and my_folds\nuseful_features = test_data.drop('PassengerId',axis=1).columns.tolist() #########################################\ntest = test_data[useful_features]\nmy_folds = train_data.copy()","be813a2f":"test.shape, my_folds.shape, useful_features","e5c9bd10":"def obj(trial,xtrain,ytrain,xvalid,yvalid):\n    params = {\n        'l2_regularization': trial.suggest_loguniform('l2_regularization',1e-10,10.0),\n        'early_stopping': trial.suggest_categorical('early_stopping', ['False']),\n        'learning_rate': trial.suggest_loguniform('learning_rate', 0.001,0.1),\n        'max_iter': trial.suggest_categorical('max_iter', [10000]),\n        'max_depth': trial.suggest_int('max_depth', 2,30),\n        'max_bins': trial.suggest_int('max_bins', 100,255),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 20,100000),\n        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 20,80),\n    }\n    model = HistGradientBoostingClassifier(random_state=141,**params)  \n    model.fit(xtrain, ytrain)\n\n    valid_preds = model.predict_proba(xvalid)[:,1]\n    #test_preds = model.predict(xtest)\n    score = roc_auc_score(yvalid, valid_preds)\n    return score\n\n# create trial function\ndef run(my_folds1):   \n \n    my_folds1 = my_folds.copy()\n    #test1  = test.copy()\n\n    fold=0\n    xtrain = my_folds[my_folds1.fold != fold].reset_index(drop=True)\n    xvalid = my_folds[my_folds1.fold == fold].reset_index(drop=True)\n    #xtest = test1.copy()\n\n    ytrain = xtrain.Survived\n    yvalid = xvalid.Survived\n\n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n\n    ## preprocess\n    si = SimpleImputer(strategy='median')\n    xtrain = si.fit_transform(xtrain)\n    xvalid = si.transform(xvalid)\n    #xtest = si.transform(xtest)\n\n    # scale\n    ss = MinMaxScaler()\n    xtrain = ss.fit_transform(xtrain)\n    xvalid = ss.transform(xvalid)\n    #xtest = ss.transform(xtest)\n\n    xtrain = pd.DataFrame(xtrain, columns=useful_features)\n    xvalid = pd.DataFrame(xvalid, columns=useful_features)\n    #xtest = pd.DataFrame(xtest, columns=useful_features)\n    \n#     for col in useful_features:\n#         xtrain[col] = np.log1p(xtrain[col])\n#         xvalid[col] = np.log1p(xvalid[col])\n#         #xtest[col] = np.log1p(xtest[col])\n        \n    #create optuna study\n    study = optuna.create_study(\n        direction='maximize',\n        study_name='HistGradientBoostingClassifier'\n    )\n\n    study.optimize( lambda trial: obj(trial,xtrain,ytrain,xvalid,yvalid),n_trials= 500 ) # it tries 50 different values to find optimal hyperparameter\n\n    print(f\"Best Params: {study.best_trial.params}\")\n    print(f\"Best Trial: {study.best_trial.value}\")\n    \n    return study.best_trial.params, study.best_trial.value","a0726dba":"bp,bv=run(my_folds) # make sure you have imported HistGradientBoostingClassifier","e99de92f":"bp ","69e0a9e1":"bv","aae13711":"<a id=\"2\"><\/a>\n# <p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">READ DATASETS<\/p>","953c6379":"<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">Either you can add the dataset whoose link I have given above or if you have TITANIC_Create_Folds notebook you can add it's output from <code>Add data<\/code> option. Both contains TITANIC_Folds.csv (modified train set). <br>\nNow read it as train_data.<\/p>","f31baffb":"\n<a id=\"0\"><\/a>\n# <p style=\"background-color:#FFCC70;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:5px 5px;\">LEVEL1 ROUND1 HistGradientBoostingClassifier<br><p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">INTRODUCTION<\/p>\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">This is a part of the notebook series <i>\"My_Complete_Pipeline_for_any_ML_Competition\"<\/i> where we are building complete pipeline.<\/p> \n\n\ud83d\udcccLink of first notebook of the series <a href=\"https:\/\/www.kaggle.com\/raj401\/my-complete-pipeline-for-any-ml-competition\">https:\/\/www.kaggle.com\/raj401\/my-complete-pipeline-for-any-ml-competition<\/a><br>\n\ud83d\udcccLink of notebook where we have created folds <a href=\"https:\/\/www.kaggle.com\/raj401\/titanic-create-folds\">https:\/\/www.kaggle.com\/raj401\/titanic-create-folds<\/a><br>\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">\n    If you like my effort please do <b><span style=\"color:crimson; font-size:20px\">UPVOTE\ud83d\udc4d<\/span><\/b>, it really keeps me motivated. <\/p>\n\n\n<p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:50%;text-align:center;border-radius:20px 60px;\">\"\"<\/p> \n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">In the <b>TITANIC_Create_Folds<\/b> notebook we have modified our training set by adding new column named 'fold' and then saved it as <i>TITANIC_folds.csv<\/i>. In this notebook we will use this modified training set instead of original training set and do hyperparameter tuning of HistGradientBoostingClassifier using OPTUNA. I am providing the link of <i>TITANIC_folds.csv<\/i> you can just add it to your notebook and you are good to go.<b><br>[Make sure you have added it before moving further. If you have TITANIC_Create_Folds notebook you can also add that notebook instead.]<br><\/b>\n\ud83d\udcccLink of Dataset containing <i>TITANIC_folds.csv<\/i> <a href=\"https:\/\/www.kaggle.com\/raj401\/titanic-all-datasets\">https:\/\/www.kaggle.com\/raj401\/titanic-all-datasets<\/a><br><\/p> \n\n\n<a id='top'><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">TABLE OF CONTENTS<\/p>   \n    \n* [1. IMPORTING LIBRARIES](#1)\n    \n* [2. READ DATASETS](#2)\n\n* [3. PREPROCESSING](#3)\n    \n    \n* [4. HYPERPARAMETER OPTIMIZATION](#4)\n    \n* [5. CONCLUSION](#5)\n    \n* [6. END](#6)\n\n<a id=\"1\"><\/a>\n# <p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">IMPORTING LIBRARIES<\/p>","0a19e7c7":"<a id=\"4\"><\/a>\n# <p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">HYPERPARAMETER OPTIMIZATION<\/p>\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">This step is very crucial, here we are using OPTUNA to find optimal hyperparameter.\n    <br>We have created two functions <b>obj()<\/b> and <b>run()<\/b>\n    <br><b>run()<\/b> takes <b>my_folds<\/b> as input which we have created, then it creates a OPTUNA study and feeds this data. \n    <br> This <b>obj()<\/b> method is created by OPTUNA which takes xtrain, ytrain, xvalid, yvalid as input and trains the model and then it makes predictions on xvalid and then return roc_auc_score. To know more about OPTUNA you can look at working of OPTUNA from their official doccument.\n    <br><b>[If you do not understand anything you can ask me in the comment]<\/b><\/p> ","95d93d63":"<a id=\"5\"><\/a>\n<p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">CONCLUSION<\/p>\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">Now we will use this optimized model to make predictions. In the next notebook we will find optimal hyperparameter for another model. \n<br> That is all for now, If you have any doubt feel free to ask me in the comment. <br>\n    If you appreciate my effort please do <b>UPVOTE\ud83d\udc4d<\/b> and I will see you in the next Notebook\ud83d\udcd2. <\/p>\n\n**<span style=\"color:#444160;\"> Thanks!<\/span>**\n<a id=\"6\"><\/a>\n<p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">END<\/p>\n    <a href=\"#top\" role=\"button\" aria-pressed=\"true\" >\u2b06\ufe0fBack to Table of Contents \u2b06\ufe0f<\/a>","0874ff14":"<a id=\"3\"><\/a>\n# <p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">PREPROCESSING<\/p>\n","eb23e0e4":"<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\"><i>bv<\/i> returns it's roc_auc_score <\/p> ","437d23d4":"<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\"><i>bp<\/i> returns best parameters <\/p> "}}