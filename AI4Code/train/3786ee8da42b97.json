{"cell_type":{"07bc388d":"code","f370adf7":"code","b674da87":"code","89af49ef":"code","593aecc4":"code","b7c209a0":"code","e7382212":"code","488a4a75":"code","cd9d6d34":"code","d71fce88":"code","9c546e88":"code","eec0b9be":"code","6520cb2b":"code","4d165d87":"code","ba331f61":"code","b4843098":"code","55af7263":"code","005fd277":"code","15eb80c0":"code","308a8ad2":"code","941be5f6":"code","1ba198c4":"code","b6d14acb":"markdown"},"source":{"07bc388d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f370adf7":"# understanding dataset\nimport csv\nstopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\nsentences = []\nlabels = []\ntitles = []\nwith open(\"\/kaggle\/input\/bbcnewsarchive\/bbc-news-data.csv\", 'r') as csvfile:\n    reader = csv.reader(csvfile, delimiter='\\t')\n    next(reader)\n    for row in reader:\n        labels.append(row[0])\n        titles.append(row[2])\n        sentence = row[3]\n        for word in stopwords:\n            token = \" \" + word + \" \"\n            sentence = sentence.replace(token, \" \")\n            sentence = sentence.replace(\"  \", \" \")\n        sentences.append(sentence)\n\n\nprint(len(sentences))\nprint(sentences[0])\nprint(labels[0])\nprint(titles[0])","b674da87":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nvocab_size = 30000\nembedding_dim = 32\nmax_length = 256\n\n#sentence tokenizer\nsentence_tokenizer = Tokenizer(num_words = vocab_size, oov_token=\"<OOV>\")\nsentence_tokenizer.fit_on_texts(sentences)\nword_index = sentence_tokenizer.word_index\nprint(len(word_index))\nsequences = sentence_tokenizer.texts_to_sequences(sentences)\npadded = pad_sequences(sequences, padding='post', maxlen=max_length)\nprint(padded[0])\nprint(padded.shape)\n\n#label tokenizer\nlabel_tokenizer = Tokenizer()\nlabel_tokenizer.fit_on_texts(labels)\nlabel_word_index = label_tokenizer.word_index\nlabel_seq = label_tokenizer.texts_to_sequences(labels)\n#print(label_seq)\nprint(label_word_index)","89af49ef":"print(len(sequences))\nprint(len(padded[0]))\nprint(len(label_seq))","593aecc4":"#train test split\ntraining_size = 1780\ntraining_sentences = padded[0:training_size]\ntesting_sentences = padded[training_size:]\ntraining_labels = label_seq[0:training_size]\ntesting_labels = label_seq[training_size:]\n\nprint(len(training_sentences))\nprint(len(testing_sentences))\nprint(len(training_labels))\nprint(len(testing_labels))","b7c209a0":"print(training_sentences.shape)\nprint(testing_sentences.shape)\ntrainlabels = np.array(training_labels)\ntestlabels = np.array(testing_labels)\nprint(trainlabels.shape)\nprint(testlabels.shape)","e7382212":"#simple model\nimport tensorflow as tf\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(6, activation='softmax')\n])\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","488a4a75":"#training\nhistory = model.fit(training_sentences, trainlabels, epochs=10, validation_data=(testing_sentences, testlabels), verbose=1)","cd9d6d34":"import matplotlib.pyplot as plt\n\n\ndef plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_'+string])\n    plt.show()\n    \nplot_graphs(history, 'accuracy')\nplot_graphs(history, 'loss')","d71fce88":"#making prediction\nlabels_pred = model.predict_classes(testing_sentences)","9c546e88":"#some random results (predicted vs actual labels)\nimport random\n\ndef show_pred_vs_actual(predictions, actual):\n    for i in range(0,10):\n        ind = random.randrange(0,len(actual))\n        print(str(predictions[ind]) + \" vs \" + str(actual[ind]))\n\nshow_pred_vs_actual(labels_pred,testlabels)","eec0b9be":"#lets shuffle data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(sentences, labels, train_size = 0.8, random_state = 42, shuffle = True)","6520cb2b":"#tokenize data\nnewtokenizer = Tokenizer(num_words = 15000, oov_token='<OOV>')\nnewtokenizer.fit_on_texts(X_train)\nword_index = newtokenizer.word_index\nX_train_seq = newtokenizer.texts_to_sequences(X_train)\nX_train_padded = pad_sequences(X_train_seq, padding='post', maxlen=max_length)\nX_test_seq = newtokenizer.texts_to_sequences(X_test)\nX_test_padded = pad_sequences(X_test_seq, padding='post', maxlen=max_length)\nnew_label_tokenizer = Tokenizer()\nnew_label_tokenizer.fit_on_texts(labels)\nnew_label_index = new_label_tokenizer.word_index\ny_train_label_seq = np.array(new_label_tokenizer.texts_to_sequences(y_train))\ny_test_label_seq = np.array(new_label_tokenizer.texts_to_sequences(y_test))","4d165d87":"#cnn model\nmodel1 = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(6, activation='softmax')\n])\nmodel1.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel1.summary()\nhistory1 = model1.fit(X_train_padded, y_train_label_seq, epochs=15, validation_data=(X_test_padded, y_test_label_seq), verbose=2)","ba331f61":"plot_graphs(history1, 'accuracy')\nplot_graphs(history1, 'loss')","b4843098":"labels_pred1 = model1.predict_classes(X_test_padded)\nshow_pred_vs_actual(labels_pred1,y_test_label_seq)","55af7263":"#lstm model\nmodel2 = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(6, activation='softmax')\n])\nmodel2.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel2.summary()\nhistory2 = model2.fit(X_train_padded, y_train_label_seq, epochs=20, validation_data=(X_test_padded, y_test_label_seq), verbose=2)","005fd277":"plot_graphs(history2, 'accuracy')\nplot_graphs(history2, 'loss')","15eb80c0":"labels_pred2 = model2.predict_classes(X_test_padded)\nshow_pred_vs_actual(labels_pred2,y_test_label_seq)","308a8ad2":"#accuracy scores\nfrom sklearn.metrics import accuracy_score\nprint(\"NN model\")\nprint(accuracy_score(labels_pred,testlabels))\nprint(\"CNN model\")\nprint(accuracy_score(labels_pred1,y_test_label_seq))\nprint(\"LSTM model\")\nprint(accuracy_score(labels_pred2,y_test_label_seq))","941be5f6":"#plotting comparison between 3 DL models\nimport pandas as pd\nfrom pandas import DataFrame\naccuracy = [max(history.history['val_accuracy']),max(history1.history['val_accuracy']), max(history2.history['val_accuracy'])]\nloss = [max(history.history['val_loss']),max(history1.history['val_loss']),max(history2.history['val_loss'])]\n\ncol={'Accuracy':accuracy,'Loss':loss}\nmodels=['NN','CNN','LSTM']\ndf=DataFrame(data=col,index=models)\ndf","1ba198c4":"df.plot(kind='bar')","b6d14acb":"Clearly its a bad model, as loss is increasing for validation set and accuracy is constant..it need to improved.."}}