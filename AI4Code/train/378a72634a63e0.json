{"cell_type":{"49bea0cb":"code","5c1b0369":"code","f9a6e85e":"code","5203efb9":"code","c3d3b99e":"code","cbb338f3":"code","217030a7":"code","b7cd9c20":"code","39ece4ba":"code","e2d7f8a1":"markdown","fe73b329":"markdown","ba5d9cee":"markdown","740ec232":"markdown","bf2e0bf8":"markdown","a7e2fee1":"markdown","584399eb":"markdown","a905890b":"markdown","c8771c65":"markdown","4cfee65a":"markdown"},"source":{"49bea0cb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import mode\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nimport operator\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","5c1b0369":"class AutoML:\n    def __init__(self, target_name, data_path, extension):\n        #self.numeric_nan = numeric_nan\n        #self.categoric_nan = categoric_nan\n        self.target_name = target_name\n        self.data_path= data_path\n        self.extension = extension\n        \n        global df\n        if self.extension == 'csv':\n            df = pd.read_csv(self.data_path)\n        elif self.extension == 'xls':\n            df = pd.read_excel(self.data_path)\n        elif self.extension == 'xlsx':\n            df = pd.read_excel(self.data_path)\n             \n            \n    def explore(self, corrmap=False, pairplot=False):\n\n        #target exploration\n        target_vals = list(pd.unique(df[self.target_name]))\n        target_vals.sort()\n        \n        \n        if df[self.target_name].dtypes == \"object\":\n            target_type = \"Categoric\"\n        else:\n            target_type = \"Numeric\"\n            \n        if target_vals == [0,1]:\n            target_def = \"Binary\"\n        else:\n            target_def = \"Non-binary\"\n        \n        \n        #features\n        cat_variables = list(df.columns[df.dtypes == \"object\"])\n        num_variables = list(df.columns[df.dtypes != \"object\"])\n        \n        \n        #output\n        print(\"Numeric variables: \", num_variables)\n        print(\"Categoric variables: \", cat_variables)\n        print(\"Target is \", target_type, \" and \",target_def)\n        print()\n        print(df.describe())\n        print()\n        print(pd.DataFrame({'count': df.isnull().sum(), 'Percent':df.isnull().sum()*100\/len(df)}))\n        if corrmap == True:\n            print(sns.heatmap(df.corr(),annot=True,cmap = 'bwr',vmin=-1, vmax=1, square=True, linewidths=0.5))\n        if pairplot ==True:\n            #grid = sns.PairGrid(data=df, size = 4)\n            #grid = grid.map_upper(plt.scatter, color = 'darkred')\n            #grid = grid.map_diag(plt.hist, bins = 10, color = 'darkred', edgecolor = 'k')\n            #grid = grid.map_lower(sns.kdeplot, cmap = 'Reds')\n            sns.pairplot(df,kind='reg',markers='+',diag_kind='kde')\n            \n    def remove_features(self, features):\n        df.drop(columns=features, inplace=True)\n        \n    \n    def detect_outliers(self,n,features,use_train_set=False):\n        \"\"\"\n        Takes a dataframe df of features and returns a list of the indices\n        corresponding to the observations containing more than n outliers according\n        to the Tukey method.\n        \"\"\"\n        global df\n        global x_train\n        \n        if use_train_set==True:\n            df_local = x_train\n        else:\n            df_local = df\n            \n        outlier_indices = []\n        \n        # iterate over features(columns)\n        for col in features:\n            # 1st quartile (25%)\n            Q1 = np.percentile(df_local[col], 25)\n            # 3rd quartile (75%)\n            Q3 = np.percentile(df_local[col],75)\n            # Interquartile range (IQR)\n            IQR = Q3 - Q1\n            \n            # outlier step\n            outlier_step = 1.5 * IQR\n            \n            # Determine a list of indices of outliers for feature col\n            outlier_list_col = df_local[(df_local[col] < Q1 - outlier_step) | (df_local[col] > Q3 + outlier_step )].index\n            \n            # append the found outlier indices for col to the list of outlier indices \n            outlier_indices.extend(outlier_list_col)\n            \n        # select observations containing more than 2 outliers\n        outlier_indices = Counter(outlier_indices)        \n        multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n        \n        if len(multiple_outliers)>0:\n            print(\"There are indices that has more than \",n,\" outliers in the data.\\n\")\n        else:\n            print(\"There isn't any observations that has more than \",n,\" outliers.\\n\")\n        return multiple_outliers   \n    \n \n\n\n    def drop_outliers(self,outliers_to_drop,use_train_set=False):\n        global df, x_train, y_train\n        if use_train_set==True:\n            x_train = x_train.drop(outliers_to_drop, axis = 0).reset_index(drop=True)\n            y_train = y_train.drop(outliers_to_drop, axis = 0).reset_index(drop=True)\n        else:\n            df = df.drop(outliers_to_drop, axis = 0).reset_index(drop=True)\n        \n    \n    \n    def fillnull(self, thresh, use_train_set=False):\n        global df, x_train, x_test\n        \n        if use_train_set==True:\n            sets = [x_train,x_test]\n        else:\n            sets = [df]\n            \n        for local_df in sets:\n            \n            cat_variables = list(local_df.columns[local_df.dtypes == \"object\"])\n            num_variables = list(local_df.columns[local_df.dtypes != \"object\"])\n            #categoric\n            null_df = pd.DataFrame({'Percent':local_df.isnull().sum()*100\/len(local_df)})\n            \n            features_to_handle = np.array(null_df[null_df[\"Percent\"]>=thresh].index.values)\n            features_to_fix = np.array(null_df[(null_df[\"Percent\"]<thresh) & (null_df[\"Percent\"]!=0)].index.values)\n            \n            for feature in features_to_handle:\n                if feature in cat_variables:\n                    local_df[feature].fillna(\"None\",inplace=True)\n                else:\n                    local_df.drop(columns=feature,inplace=True)\n                    \n            for feature in features_to_fix:\n                if feature in cat_variables:\n                    local_df[feature].fillna(local_df[feature].mode()[0],inplace=True)\n                else:\n                    corr_local_df = pd.DataFrame(local_df.corr())\n                    top_corr = pd.DataFrame(corr_local_df[feature].abs().sort_values(ascending=False))\n                    neighbor_variables = list(top_corr[1:4].index.values)\n                    local_df[feature] = (local_df.groupby(neighbor_variables)[feature].transform(lambda x: x.fillna(x.median())))\n                    #if it cant be filled with a similar value\n                    local_df[feature].fillna(local_df[feature].median(), inplace=True)\n            \n\n            \n\n    def encode_cat(self,use_train_set=False):\n        global df, x_train, x_test\n        if use_train_set==True:\n            x_train = pd.get_dummies(x_train)\n            x_test = pd.get_dummies(x_test)\n        else:\n            df = pd.get_dummies(df)\n        \n    def train_test(self, test_size=0.25,random_state=1):\n        global x_train,y_train,x_test,y_test,target_name\n        y = df[self.target_name]\n        X = df.drop(columns=self.target_name)\n        x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=test_size,random_state=random_state)\n        \n    def fit_model(self,use_train_set=False):\n        #target exploration\n        target_vals = list(pd.unique(df[self.target_name]))\n        target_vals.sort()\n        \n        if df[self.target_name].dtypes == \"object\":\n            target_type = \"Categoric\"\n        else:\n            target_type = \"Numeric\"\n            \n        if target_vals == [0,1]:\n            target_def = \"Binary\"\n        else:\n            target_def = \"Non-binary\"\n            \n            \n        #binary classification\n        if (target_type==\"Numeric\")&(target_def==\"Binary\"):\n            print(\"This is a binary classification problem.\\n\")\n            print(\"Preparing your model...\\n\")\n            from sklearn.neighbors import KNeighborsClassifier\n            from sklearn.svm import SVC\n            from sklearn.tree import DecisionTreeClassifier\n            from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n            from sklearn.linear_model import LogisticRegression\n            from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n            classifiers = {\n                            'Logistic Regression': LogisticRegression(),\n                            'KNeighbors':KNeighborsClassifier(),\n                            'SVC':SVC(),\n                            'Decision Tree Classifier':DecisionTreeClassifier(max_depth=5),\n                            'Random Forest Classifier':RandomForestClassifier(max_depth=5),\n                            'AdaBoost Classifier':AdaBoostClassifier()\n                           }\n            accuracys = {}\n            for c in classifiers:\n                classifier = classifiers.get(c)\n                if use_train_set==True:\n                    classifier.fit(x_train,y_train)\n                    pred = classifier.predict(x_test)\n                    print(\"Results of \",c)\n                    print()\n                    print(\"Accuracy is: \",accuracy_score(y_test,pred))\n                    print()\n                    print(\"Confusion matrix: \\n\")\n                    cm = confusion_matrix(y_test,pred)\n                    ax= plt.subplot()\n                    sns.heatmap(cm, annot=True, ax = ax,cmap='YlGnBu',fmt='g')\n                    plt.ylabel('True Label')\n                    plt.xlabel('Predicted Label')\n                    plt.title('Confusion Matrix')\n                    plt.show()\n                    print()\n                    print(classification_report(y_test,pred))\n                    print()\n                    score = accuracy_score(y_test,pred)\n                    accuracys[c] = score\n                else:\n                    y = df[self.target_name]\n                    X = df.drop(columns=self.target_name)\n                    classifier.fit(X,y)\n            print(accuracys)\n            best_model = max(accuracys.items(), key=operator.itemgetter(1))[0]\n            print(\"Your best performing model is: \",best_model)\n            print(\"Preparing for usage.\\n\")\n            classifier = classifiers.get(best_model)\n            classifier.fit(x_train,y_train)\n            pred = classifier.predict(x_test)\n            print(\"With score: \",accuracy_score(y_test,pred))\n            print()","f9a6e85e":"auto1 = AutoML(\"Survived\",\"\/kaggle\/input\/titanic\/train.csv\",\"csv\")","5203efb9":"auto1.remove_features(features=[\"PassengerId\",\"Ticket\",\"Name\",\"Cabin\"])","c3d3b99e":"auto1.explore(corrmap=True,pairplot=True)","cbb338f3":"auto1.train_test()","217030a7":"outliers = auto1.detect_outliers(2,features=[\"Pclass\",\"Age\",\"SibSp\",\"Fare\",\"Parch\"],use_train_set=True)\nprint(outliers)\nauto1.drop_outliers(outliers,use_train_set=True)","b7cd9c20":"auto1.fillnull(thresh=50,use_train_set=True)\nauto1.encode_cat(use_train_set=True)","39ece4ba":"auto1.fit_model(use_train_set = True)","e2d7f8a1":"Lets do the train-test split to evaluate our model later.","fe73b329":"After this step, you can tune your models parameters and predict an unseen data. \nFeel free to use this class in any of your work!\nHappy new year!","ba5d9cee":"Now I'm going to remove the Id, Ticket, Name and Cabin variables. Although these features include information, we are not going to dive into feature engineering in this topic. But know that titles can be acquired from Names column and Cabin column can be simplified to a one letter feature.","740ec232":"\nHello,\nThis is my first kernel in kaggle. In this notebook, I will share a small AutoML class that I wrote last week. I am going to improve it for sure, but for now, I think you might also use and extend it for your own purposes. ","bf2e0bf8":"Now we will detect and drop outliers in the given columns. Note that we are going to do detect and remove outliers only in the train set. For the titanic data, we can safely do this. But for another data sets, removing outliers can affect our model negatively because test set can also include outliers. So be cautious while using it.","a7e2fee1":"We are ready to fit our models.","584399eb":"Now what this class will do is:\n1. It will **Import the data** and explore it. It will give you info about **missing values** and the **distribution** of variables. \n2. It has some utility functions, such as **removing variables**. \n3. It has a **outlier detection** function. Given a number, for example 2, it will determine the rows that has more than 2 outliers. Removing them is optional. This function uses Tukey method and is taken from https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling\n4. It has **null value filling** option. For a given threshold, if a column has more than %thresh null values, it fills \"None\" for categoric variables, and it drops the numeric feature. For this dataset, that is suitable. Below that threshold, it fills numeric rows with similar values by performing groupby and median. For the categoric that are below the threshold, it fills with mode.\n5. It **encodes the categoric variables** by pandas' get dummies.\n6. **Splits the train-test sets**. Thus, all functions mentioned above are suitable with train-test option. Set use_train_set = True in above function when calling them. \n7. Finally, it **fits some models** with default parameters. Choses the best one. ","a905890b":"Now we are going use it like the following. First, we create our AutoML object.\n","c8771c65":"Handle missing values and categorical features:","4cfee65a":"# Introduction\n### 01\/01\/2020"}}