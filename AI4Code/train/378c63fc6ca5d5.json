{"cell_type":{"64e420f1":"code","d5802529":"code","86f2de61":"code","6ba633ec":"code","5bba1ad6":"code","56ff5ad8":"code","1f27c90c":"code","837a088b":"code","0a4bb007":"code","f22188cb":"code","8a840b74":"code","188c3ac8":"code","befc937b":"code","0d63f180":"code","85d119d9":"code","e8031f08":"code","4995d43d":"code","a97fad35":"code","a001290d":"code","18c8ef2a":"code","61f2eb18":"markdown","050a2187":"markdown","e6890f29":"markdown","5739d039":"markdown","9db61991":"markdown","59eccdb5":"markdown","8556f110":"markdown","289e3517":"markdown","357de995":"markdown","94870170":"markdown","6b44e30a":"markdown","624a4590":"markdown","3831653f":"markdown","fb918e10":"markdown","485cb945":"markdown","7b1b525a":"markdown","652447e4":"markdown","41c0214d":"markdown","f9ec597d":"markdown","da61207a":"markdown","99495baa":"markdown","4f5dc2ca":"markdown","00256499":"markdown","54063c5a":"markdown","a1900859":"markdown"},"source":{"64e420f1":"import pandas as pd\nexo_train=pd.read_csv('..\/input\/kepler-labelled-time-series-data\/exoTrain.csv')\nexo_test=pd.read_csv('..\/input\/kepler-labelled-time-series-data\/exoTest.csv')\ntrain_exo_y=exo_train[exo_train['LABEL'] >1 ]\ntrain_exo_n=exo_train[exo_train['LABEL'] < 2]\ntrain_t_n=train_exo_n.iloc[:,1:].T\ntrain_t_y=train_exo_y.iloc[:,1:].T\ntrain_t_n.head(1)\nexo_train['LABEL'].value_counts()","d5802529":"import plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfig = make_subplots(rows=2, cols=2,subplot_titles=(\"Flux variation of star 37\", \"Flux variation of star 5086\", \n                                                   \"Flux variation of star 3000\", \"Flux variation of star 3001\"))\nfig.add_trace(\n    go.Scatter(y=train_t_n[37], x=train_t_n.index),\n    row=1, col=1\n)\nfig.add_trace(\n    go.Scatter(y=train_t_n[5086], x=train_t_n.index),\n    row=1, col=2\n)\nfig.add_trace(\n    go.Scatter(y=train_t_n[3000], x=train_t_n.index),\n    row=2, col=1\n)\nfig.add_trace(\n    go.Scatter(y=train_t_n[3001], x=train_t_n.index),\n    row=2, col=2\n)\nfig.update_layout(height=600, width=800, title_text=\"Non Exoplanets Star examples\",showlegend=False)\nfig.show()","86f2de61":"fig = make_subplots(rows=2, cols=2,subplot_titles=(\"Flux variation of star 0\", \"Flux variation of star 1\", \n                                                   \"Flux variation of star 35\", \"Flux variation of star 36\"))\nfig.add_trace(\n    go.Scatter(y=train_t_y[0], x=train_t_y.index),\n    row=1, col=1\n)\nfig.add_trace(\n    go.Scatter(y=train_t_y[1], x=train_t_y.index),\n    row=1, col=2\n)\nfig.add_trace(\n    go.Scatter(y=train_t_y[35], x=train_t_y.index),\n    row=2, col=1\n)\nfig.add_trace(\n    go.Scatter(y=train_t_y[36], x=train_t_y.index),\n    row=2, col=2\n)\nfig.update_layout(height=600, width=800, title_text=\"Exoplanets Stars examples\",showlegend=False)","6ba633ec":"###Normalizing the flux#####\nfrom sklearn.preprocessing import StandardScaler\ntrainx=exo_train.iloc[:,1:]\ntextx=exo_test.iloc[:,1:]\nscaler=StandardScaler()\ntrain_scaled=scaler.fit_transform(trainx)\ntest_scaled=scaler.fit_transform(textx)","5bba1ad6":"### Applying SVC with linear Kernel\ntrainy=exo_train[['LABEL']]\ntesty=exo_test[['LABEL']]\nfrom sklearn.svm import SVC\nsvclassifier = SVC(kernel='linear')\nsvclassifier.fit(train_scaled, trainy['LABEL'])\ny_pred = svclassifier.predict(test_scaled)\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(testy, y_pred))\nprint(classification_report(testy, y_pred))","56ff5ad8":"####Polynomial Kernel ###\nsvclassifier = SVC(kernel='poly')\nsvclassifier.fit(train_scaled, trainy['LABEL'])\ny_pred = svclassifier.predict(test_scaled)\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(testy, y_pred))\nprint(classification_report(testy, y_pred))","1f27c90c":"### RBF kernel###\nsvclassifier = SVC(kernel='rbf')\nsvclassifier.fit(train_scaled, trainy['LABEL'])\ny_pred = svclassifier.predict(test_scaled)\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(testy, y_pred))\nprint(classification_report(testy, y_pred))","837a088b":"###Sigmoid kernel###\nsvclassifier = SVC(kernel='sigmoid')\nsvclassifier.fit(train_scaled, trainy['LABEL'])\ny_pred = svclassifier.predict(test_scaled)\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(testy, y_pred))\nprint(classification_report(testy, y_pred))","0a4bb007":"import numpy as np\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=6)\npca.fit(train_scaled)\nPCA(n_components=6)\nprint(pca.explained_variance_ratio_)\nprint(pca.singular_values_)\ntrns_x=pca.transform(train_scaled)\ntrns_y=pca.transform(test_scaled)\ntesty","f22188cb":"##Applying SVC RBF to new transformed dataset #####\nfrom sklearn.svm import SVC\nsvclassifier = SVC(kernel='rbf')\nsvclassifier.fit(trns_x, trainy['LABEL'])\ny_pred = svclassifier.predict(trns_y)\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(testy['LABEL'], y_pred))\nprint(classification_report(testy['LABEL'], y_pred))","8a840b74":"trainy.loc[trainy['LABEL'] == 1, 'new1'] = 0\ntrainy.loc[trainy['LABEL'] > 1, 'new1'] = 1\ntesty.loc[testy['LABEL'] > 1, 'new1'] = 1\ntesty.loc[testy['LABEL'] == 1, 'new1'] = 0\n","188c3ac8":"import statsmodels.api as sm\nlogit_model=sm.Logit(trainy['new1'],trns_x)\nresult=logit_model.fit()\nprint(result.summary2())","befc937b":"###Scikitlearn Logistic regression ###\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nlogreg = LogisticRegression()\nlogreg.fit(trns_x, trainy['new1'])\ny_pred = logreg.predict(trns_y)\nprint('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(trns_y, testy['new1'])))","0d63f180":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nimport matplotlib.pyplot as plt\nlogit_roc_auc = roc_auc_score(testy['new1'], logreg.predict(trns_y))\nfpr, tpr, thresholds = roc_curve(testy['new1'], logreg.predict_proba(trns_y)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","85d119d9":"from imblearn.over_sampling import SMOTE\nover = SMOTE(random_state=0)\nov_train_x,ov_train_y=over.fit_sample(trns_x, trainy['new1'])\nov_train_y=ov_train_y.astype('int')\nov_train_y.value_counts()\n","e8031f08":"ov_train_y=ov_train_y.values.tolist()\nfrom sklearn.svm import SVC\nsvclassifier = SVC(kernel='rbf')\nsvclassifier.fit(ov_train_x, ov_train_y)\ny_pred = svclassifier.predict(trns_y)\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(testy['new1'], y_pred))\nprint(classification_report(testy['new1'], y_pred))","4995d43d":"import numpy as np\nov_train_y=np.array(ov_train_y)\nov_train_y.dtype\nfrom sklearn.model_selection import train_test_split\ntr_x,v_x,tr_y,V_y= train_test_split(ov_train_x, ov_train_y, test_size=0.2)","a97fad35":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras.layers import LeakyReLU\nmodel=keras.models.Sequential([\n    keras.layers.Dense(300,activation=\"selu\",input_shape=(8080,6)),    \n    keras.layers.Dense(200,activation=\"selu\",kernel_regularizer=keras.regularizers.l2(0.01)),   \n    keras.layers.Dense(100,activation=\"selu\",kernel_regularizer=keras.regularizers.l2(0.01)),   \n    keras.layers.Dense(2,activation=\"softmax\")\n])\nepochs=50\noptimizers=keras.optimizers.SGD(clipvalue=1.0)\ndef exp_decay(lr0,s):\n    def exp_decay_fn(epcohs):\n        return lr0*0.1**(epochs\/s)\n    return exp_decay_fn\n\nexp_decay_fn=exp_decay(lr0=0.1,s=20)\nlr_sch=keras.callbacks.LearningRateScheduler(exp_decay_fn)\nlr_sch2=keras.callbacks.ReduceLROnPlateau(factor=0.5,patience=5)\nmodel.compile(loss=\"sparse_categorical_crossentropy\",optimizer=optimizers,metrics=[\"accuracy\"])\nhistory=model.fit(tr_x,tr_y,epochs=50,callbacks=[lr_sch],validation_data=(v_x,V_y))\nimport matplotlib.pyplot as plt\npd.DataFrame(history.history).plot()\nplt.grid(True)\nplt.gca().set_ylim(0,1)\nplt.show()\npredict=model.predict_classes(trns_y)\nprint(confusion_matrix(testy['new1'], predict))\nprint(classification_report(testy['new1'], predict))","a001290d":"model=keras.models.Sequential([\n    keras.layers.Dense(300,activation=\"swish\",input_shape=(8080,6)),    \n    keras.layers.Dense(200,activation=\"swish\",kernel_initializer=\"he_normal\"),   \n    keras.layers.Dense(100,activation=\"swish\",kernel_initializer=\"he_normal\"),    \n    keras.layers.Dense(2,activation=\"softmax\")\n])\n\noptimizers=tf.keras.optimizers.Adam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    amsgrad=False,\n    name=\"Adam\"\n    \n)\nlr_sch2=keras.callbacks.ReduceLROnPlateau(factor=0.5,patience=5)\nmodel.compile(loss=\"sparse_categorical_crossentropy\",optimizer=optimizers,metrics=[\"accuracy\"])\nhistory=model.fit(tr_x,tr_y,epochs=50,validation_data=(v_x,V_y),callbacks=[lr_sch2])\nimport matplotlib.pyplot as plt\npd.DataFrame(history.history).plot()\nplt.grid(True)\nplt.gca().set_ylim(0,1)\nplt.show()\npredict=model.predict_classes(trns_y)\nprint(confusion_matrix(testy['new1'], predict))\nprint(classification_report(testy['new1'], predict))","18c8ef2a":"# T - no. of total samples\n# P - no. of positive samples\n# scale_pos_weight = percent of negative \/ percent of positive\n# which translates to:\n# scale_pos_weight = (100*(T-P)\/T) \/ (100*P\/T)\n# which further simplifies to beautiful:\n#scale_pos_weight = |37\/5387 - 1|=0.99\nfrom xgboost import XGBClassifier\nscale_pos_weight = [2,0.99,0.60,0.50,0.33,0.20,0.10]\nfor i in scale_pos_weight:\n    print('scale_pos_weight = {}: '.format(i))\n    clf = XGBClassifier(scale_pos_weight=i)\n    clf.fit(ov_train_x, ov_train_y)\n    predict = clf.predict(trns_y)    \n    cm = confusion_matrix(testy['new1'], predict)  \n    auc=metrics.roc_auc_score(testy['new1'], predict)\n    print('Confusion Matrix: \\n', cm)\n    print('metrics: \\n',classification_report(testy['new1'], predict))\n    print('AUC of test set: {:.2f} \\n'.format(metrics.roc_auc_score(testy['new1'], predict))) ","61f2eb18":"## Reading the file and plotting flux for exoplanets vs non exoplanets\n1.We will transpose the file to make stars rows into column to help us plot few for some stars flux over period of time\n2.We will divide the dataset based on labels intow two parts for different star type","050a2187":"Adam took us back to point zero where we are not able to predict minority class, this model  also have not been regularised- no droput or l1\/l2 regularization hence with increasing epochs the accuracy keeps on increasing a bit , leading to overfitting.","e6890f29":"# Solving for class imbalance using SMOTE\nSynthetic Minority Oversampling Technique: SMOTE synthesises new minority instances between existing (real) minority instances. .The SMOTE will make 1 and 0 equal for our datasets.\nhttps:\/\/machinelearningmastery.com\/smote-oversampling-for-imbalanced-classification\/","5739d039":"# Applying different models to predict type of stars(exo\/vs non exo)","9db61991":"**The Accuracy is high because the category 2 the exoplanets stars count is very less in test data, even if we label all the stars to 1 we will have 99 % F1 score. We will try applying other kernel in SVC**","59eccdb5":"# Applying Logistic Regression\nUsing Stats model and sklearn library we will see how logistic regression performs.","8556f110":"Finally  we are able to predict the 1 out of 5 exoplanets correctly as shown in the confusion matrix.","289e3517":"The ROC AUC shows 50% AUC which is equivalent to random guessing , if we predict everything to 0 then this ROC curve comes.Stating F1 score and accuracy are farce metrics in case of imbalanced data.","357de995":"None of the Kernels worked in SVM. Lets try PCA now","94870170":"### Using RBF SVC on oversampled data","6b44e30a":"The accuracy is coming to be 99% on test data again this could be because model is predicting everything as 0 non exo planet. Now we will see ROC curve to get the story.","624a4590":"Nothing has been predicted as exo planet which is why we see that Sklearn has throwed a warning that precision and F1 score is ill defined.","3831653f":"# Applying PCA to reduce flux columns to 6 which capture 82% variation in data\n\nWe will use common dimensionality reduction technique principal component analysis to reduce dimension to wherever we are getting 80% of variability in data.The six principal components will be used for prediction from now onwards","fb918e10":"Still we are having the model problem unable to predict star type exoplanet","485cb945":"**This is highly unbalanced data, other kernels have made prediction predicting everything as non exoplanet and claimed 99% accuracy which is not right**","7b1b525a":"# Final Solution: Oversampled,Transformed,Weighted XGBOOST\n\nI went through Kaggle discussion and found out a useful feature in xgboost: Scale POS weight\n\nhttps:\/\/www.kaggle.com\/c\/porto-seguro-safe-driver-prediction\/discussion\/41359\n\n**Now scale_pos_weight is the ratio of number of negative class to the positive class, but interestingly in an oversampled data which has equal classes instances we should be getting best accuracy at 0.5 , but i got that at 33% or 0.33**","652447e4":"## Trying another combination of Activation Functions(ADAM)\n\nAdam is known for its faster convergence over SGD","41c0214d":"**This combination of Selu\/softmax ,l2 regularization, reducing LR on plateau and Sparse categorical cross entropy loss function is able to predict minority class but fails in predicting majority class.**","f9ec597d":"## Making scatter plot for flux from two different categories","da61207a":"Psuedo R square value suggest the model has poor prediction power. None of Flux values X1,x2,X3,X4,X5,X6 Which we derived from PCA are coming to be signhicant, hence this model is extremely poor.","99495baa":"# ROC curve tells the real story","4f5dc2ca":"# Understanding flux\n\nFlux (or radiant flux), F, is the total amount of energy that crosses a unit area per unit time. Flux is measured in joules per square metre per second (joules\/m2\/s), or watts per square metre (watts\/m2).\n\nThe flux of an astronomical source depends on the luminosity of the object and its distance from the Earth, according to the inverse square law:\n\n$\\displaystyle  F=\\frac{L}{4\\pi r^2} $\n\nwhere F = flux measured at distance r,\nL = luminosity of the source,\nr= distance to the source.\nSource: https:\/\/astronomy.swin.edu.au\/cosmos\/F\/Flux\n\n","00256499":"# Nueral Network - Feed Forward using Keras\nAlthough it is not sure that neural network are proven to work on imbalanced data, but i am giving it try by trying different activation functions and changing optimizers and learning rates.\n\n* I have tried custom learning learning rate exp_decay which penalizes the increase in epochs\n* I have also tried clip value in stocashtic gradient descent to remove exploding gradient problem.\n* I have tried SELU which is removes vanishing gradient problem also newly introduced google activation function swish\n* I have also used L2 regularization to reduce overfitting in hidden layers\n* I have tried parameter reduce learning rate on plateau so the convergence alogirthm in case LR reduces.\n* I have also tried HE initialization for reducing vanishing gradient problem by intializing weights by formual v2=2\/N\n","54063c5a":"************The best prediction we got is at .33 weightage which gave us 2 out of 5 minority classes predicted , also it didnt compromise on majority prediction like NN, and gave a good f1 score and 64 % AUC which is drastic improvement from 50%","a1900859":"**Key Observations:**\n1) Irregular pattern of flux due to outliers in some hours.\n2) Ignoring the anomalies in flux due to recording in non exoplanets there are not a cylce of flux change\n3) In expolnates examples we clearly see cycle of flux change. Like for star1, 160 hour cycle is evident that there is planet revolving around this star for this time period relative to earth. Like wise for star 0 it is 800 hours and for star 35, 200 hours."}}