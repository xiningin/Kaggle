{"cell_type":{"ac603497":"code","fcc71408":"code","c9226096":"code","cc6ad45b":"code","e669c0ef":"code","c793b67e":"code","f44e5403":"code","622dcc8b":"code","73d1d8be":"code","1c4940a8":"code","a3871b54":"code","b7570696":"code","e8e7d669":"code","1a1f8074":"code","100f145f":"code","a8d3c166":"code","9ac57914":"code","4d75a971":"code","42a8876a":"code","381cc73b":"code","f065da46":"code","92f0a50e":"code","c09d57fd":"code","ba3fae23":"code","78f0e658":"code","63487c54":"code","17e97340":"code","6c249e48":"code","9d04f1c4":"code","768c0672":"code","aa7115cc":"code","0ee71772":"code","4f2be579":"code","15f48800":"code","5604461b":"code","ca1c99d4":"code","790859c1":"code","1df94e7b":"markdown","9bf3746a":"markdown","262346bc":"markdown","eb009359":"markdown","66ce0f3c":"markdown","b981fa45":"markdown","f21852f0":"markdown","c406983a":"markdown","fa7d34f6":"markdown","e228720a":"markdown","a05af34b":"markdown","59ffa62a":"markdown","fb0ce1ed":"markdown","cac1b4d9":"markdown","e95cd829":"markdown","1cf4f2ec":"markdown","f31bc8b4":"markdown","03f69f5d":"markdown","5825e668":"markdown","971c8987":"markdown","47379faf":"markdown","14526d32":"markdown","602ed26b":"markdown","9f524897":"markdown","c2d9668b":"markdown","e86a8cd0":"markdown","a69179be":"markdown","ee8cb3c8":"markdown","0c2071e8":"markdown"},"source":{"ac603497":"#Install latest version of the package as  the defualt version is not working fine\n!pip install seaborn==0.11.0","fcc71408":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os, gc\nprint(os.listdir(\"..\/input\"))\n\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n\nimport matplotlib.pyplot as plt\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)\n\n#setting for plot fonts \nSMALL_SIZE = 14\nMEDIUM_SIZE = 16\nBIGGER_SIZE = 18\n\nplt.rc('font', size=SMALL_SIZE)          # controls default text sizes\nplt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\nplt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\nplt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\nplt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\nplt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\nplt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n","c9226096":"RANDOM_STATE = 42\nDEBUG_MODE = False  # Load fewer samples to save time for quick testing\nTARGET = 'isFraud'\n","cc6ad45b":"%%time\n\n# Load fewer samples to save time for quick testing\nif DEBUG_MODE:\n    nrows = 50000\nelse:\n    nrows = None\n        \ndata_path = '\/kaggle\/input\/ieee-fraud-detection\/'\ntrain_identity = pd.read_csv(os.path.join(data_path, 'train_identity.csv'))\ntrain_transaction = pd.read_csv(os.path.join(data_path, 'train_transaction.csv'), nrows = nrows)\ntest_identity = pd.read_csv(os.path.join(data_path, 'test_identity.csv'))\ntest_transaction =pd.read_csv(os.path.join(data_path, 'test_transaction.csv'), nrows = nrows)\nprint('Train Identity Data - rows:', train_identity.shape[0], \n      'columns:', train_identity.shape[1])\nprint('Train Transaction Data - rows:', train_transaction.shape[0], \n      'columns:', train_transaction.shape[1])\nprint('Test Identity Data - rows:', test_identity.shape[0], \n      'columns:', test_identity.shape[1])\nprint('Test Transaction Data - rows:', test_transaction.shape[0], \n      'columns:', test_transaction.shape[1])","e669c0ef":"train_transaction.head()","c793b67e":"\ndef column_properties(df):\n    columns_prop = pd.DataFrame()\n    columns_prop['column'] = df.columns.tolist()\n    columns_prop['count_non_null'] = df.count().values\n    columns_prop['count_null'] = df.isnull().sum().values\n    columns_prop['perc_null'] = columns_prop['count_null'] * 100 \/ df.shape[0]\n\n    #using df.nunique() is memory intensive and slow resulting in kernal death\n    unique_list = []\n    for col in df.columns.tolist():\n        unique_list.append(df[col].value_counts().shape[0])\n    columns_prop['count_unique'] =  unique_list\n    \n    columns_prop['dtype'] = df.dtypes.values\n    columns_prop.set_index('column', inplace = True)\n    return columns_prop\n","f44e5403":"column_properties(train_transaction).T","622dcc8b":"# the columns name for training set and test are not same,we will correct columns names of test set using traning column name\nidentity_col_names =  train_identity.columns.tolist()\ntest_identity.columns = identity_col_names\nprint(test_identity.columns.tolist())","73d1d8be":"test_identity.head()","1c4940a8":"column_properties(train_identity).T","a3871b54":"%%time\ntrain = pd.merge(train_transaction, train_identity, on= 'TransactionID', how = 'left')\ntest = pd.merge(test_transaction, test_identity, on= 'TransactionID', how = 'left')\ndel train_transaction, train_identity, test_transaction, test_identity\ngc.collect()\ntrain.shape","b7570696":"\n\n\n\ncat_cols = ['DeviceType', 'DeviceInfo', 'ProductCD', 'addr1', 'addr2', 'P_emaildomain', 'R_emaildomain']\ncat_cols +=  ['M' + str(i) for i in range(1,10)]\ncat_cols += ['card' + str(i) for i in range(1,7)]\ncat_cols += ['id_' + str(i) for i in range(12,39)]\ncolumn_properties(train[cat_cols]).T\n","e8e7d669":"train[cat_cols].head()","1a1f8074":"%%time\n\n\nall_cols = train.columns.tolist()\nnum_cols = [x for x in all_cols if x not in cat_cols]\n\nnum_cols.remove('TransactionID')\nnum_cols.remove(TARGET)\ntrain[num_cols].describe()","100f145f":"def plot_numeric_data(df, col, target_col, remove_outliers = False):\n   \n    df = df[[col, target_col]].copy()\n    df.dropna(subset=[col], inplace =True)\n    \n    #Remove Outliers: keep only the ones that are within +3 to -3 standard deviations in the column \n    if remove_outliers:       \n       \n        df = df[np.abs(df[col]-df[col].mean()) <= (3*df[col].std())]\n       \n\n\n    fig, (ax1, ax2,ax3)  =  plt.subplots(ncols = 3, figsize = (24,4))\n    fig.suptitle('Plots for {}'.format(col))\n    \n    #Display Density Plot\n    sns.distplot(df[col], color = 'b',  kde = True ,  ax = ax1 )\n    plt.ylabel('Density')\n\n\n    # Display Box Plot for feature\n    sns.boxplot(x = col , data = df,ax = ax2)\n   \n    #Display Density Plot for Fraud Vs NotFraud\n    sns.distplot(df[df[target_col] == 0][col], color = 'b', label = 'NotFraud',ax = ax3)\n    sns.distplot(df[df[target_col] == 1][col], color = 'r', label = 'Fraud',ax = ax3)\n    plt.legend(loc = 'best')\n    plt.ylabel('Density NotFraud vs Fraud')\n\n    plt.show()\n    \n\n","a8d3c166":"def plot_categorical_data(col, data, top_n = 10, display_data = False ):\n    \n    # Prpare a dataframe for count and postive classs percent givel colums\n    df_data = data[[col, TARGET]].copy()    \n    df = df_data.groupby(col).agg({col:['count'], TARGET:['sum']})\n    df.columns = ['count', 'fraud_count']\n\n    df['fraud_perc'] = df['fraud_count'] * 100 \/ df['count']\n    df['fraude_perc'] = df['fraud_perc'].round(2)\n    \n#    % missing values in the columns to be displayed in title\n    null_perc = (df_data.shape[0]- df['count'].sum())  \/ df_data.shape[0]\n\n    width = 18\n    height = 6\n\n#   select only top n categories\n    df_disp = df.sort_values(by ='count', ascending= False).head(top_n )\n\n    fig, (ax1, ax2)  =  plt.subplots(ncols = 2, figsize = (width,height))\n    fig.suptitle('Plots for {} (Missing Values: {:.2%})'.format(col, null_perc))\n    \n#   Display Sort order should be by descending value of count\n    plot_order = df_disp.sort_values(by='count', ascending=False).index.values\n\n#   Display Bar chart for frequency count of top_n categories\n    s = sns.barplot(ax = ax1,  y = df_disp.index, x = df_disp['count'], order=plot_order, orient = 'h'  )\n    s.set_title('Count for {}'.format(col))\n    \n#   Display Bar chart for perecnt of positive class for top categories\n    s = sns.barplot(ax = ax2,  y = df_disp.index, x = df_disp['fraud_perc'], order=plot_order , orient = 'h'    )\n    s.set(xlabel='Fraud Percent')\n    s.set_title('% Fraud {}'.format(col))\n    plt.show()\n    if display_data:\n        return df","9ac57914":"display_features = ['TransactionID']\n\n\ncol = 'nulls'\ntrain[col] = train.isnull().sum(axis=1)\ntest[col] = test.isnull().sum(axis=1)\n\ndisplay_features.append(col)\nplot_numeric_data(train, col, TARGET, remove_outliers = True)","4d75a971":"\ncol = 'TAmt_decimal'\ntrain[col] = ((train['TransactionAmt'] - train['TransactionAmt'].astype(int)) * 1000).astype(int)\ntest[col] = ((test['TransactionAmt'] - test['TransactionAmt'].astype(int)) * 1000).astype(int)\n\ndisplay_features.append('TransactionAmt')\ndisplay_features.append(col)\nplot_numeric_data(train, col, TARGET, remove_outliers = True)\n","42a8876a":"freq_cols = ['card1', 'addr1', 'addr2', 'card2', 'card3', 'card4', 'card5',\n             'card6', 'P_emaildomain', 'ProductCD', 'R_emaildomain']\nfor col in freq_cols:\n    display_features.append(col)\n    \n    train[col + '_count'] = train[col].map(train[col].value_counts(dropna=False) )\n    test[col + '_count'] =  test[col].map(train[col].value_counts(dropna=False) )\n    display_features.append(col + '_count')\n    plot_numeric_data(train, col + '_count',  TARGET, remove_outliers = True)","381cc73b":"def make_hour_feature(df, tname='TransactionDT'):\n    #Creates an hour of the day feature, encoded as 0-23.  \n   \n    hours = df[tname] \/ (3600)        \n    encoded_hours = np.floor(hours) % 24\n    return encoded_hours","f065da46":"display_features.append('TransactionDT')\n\ncol = 'hour'\ntrain[col] = make_hour_feature(train)\ntest[col] = make_hour_feature(test)\ndisplay_features.append(col)\n\nplot_numeric_data(train, col,  TARGET, remove_outliers = True)","92f0a50e":"display_features.append(TARGET)\ntrain[train[TARGET]==1][display_features].head(10)","c09d57fd":"\n\n\n# Concatenate the tranining and test dataset by appending\ndata_all = train.append(test, ignore_index = True, sort=False)\n\n#Create submission pandas dataframe \nsub = pd.DataFrame()\nsub['TransactionID'] = test.TransactionID\n\n\n\n# free the memory which is not required as it can exceed the physical ram \ndel train, test\ngc.collect()\n","ba3fae23":"\n\n\n# Do ordinal encoding for categorical features\nfor col in cat_cols:\n    data_all[col], uniques = pd.factorize(data_all[col])\n    #the factorize sets null values to -1, so convert them back to null, as we want LGB to handle null values\n    data_all[col] = data_all[col].replace(-1, np.nan)\n    \n\n    \n","78f0e658":"from sklearn.model_selection import train_test_split\n\n#For test set target value will be null\nX_train =  data_all[data_all[TARGET].notnull()]\nX_test  =  data_all[data_all[TARGET].isnull()]\ndel data_all\ngc.collect()\n\n#get the labels for traning set\ny_train = X_train[TARGET]\n\n# Remove ID and TARGET column from train and test set\nX_train = X_train.drop(['TransactionID', TARGET], axis = 1)\nX_test = X_test.drop(['TransactionID',   TARGET], axis = 1)\n\n# Split the training set into training and validation set. \n# We will use first 80% of data as traningg set and last 20% as validation set.\n# Since the data is sorted in time according to transaction timestamp, we should not use random split.\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.2, shuffle=False, \n                                                      random_state = RANDOM_STATE)\n\nprint('Train shape{} Valid Shape{}, Test Shape {}'.format(X_train.shape, X_valid.shape, X_test.shape))","63487c54":"from sklearn.metrics import accuracy_score, roc_auc_score,f1_score, precision_score, recall_score,confusion_matrix\n\ndef validation_results(y_valid, y_prob, verbose = True):   \n    scores = {}                      \n    y_pred_class =  [0  if x < 0.5 else 1 for x in y_prob]\n    scores['val_accuracy']  = accuracy_score(y_valid, y_pred_class)\n    scores['val_auc']       = roc_auc_score(y_valid, y_prob)\n    scores['val_f1']        =   f1_score(y_valid, y_pred_class, average = 'binary')\n    scores['val_precision'] = precision_score(y_valid, y_pred_class)\n    scores['val_recall']    = recall_score(y_valid, y_pred_class)\n    \n    cm = confusion_matrix(y_valid, y_pred_class)\n    cm_df = pd.DataFrame(cm, columns=np.unique(y_valid), index = np.unique(y_valid))\n    if verbose:\n        print('\\nValidation Accuracy      {:0.5f}'.format( scores['val_accuracy'] ))\n        print('Validation   AUC         {:0.5f}'.format( scores['val_auc']   ))\n        print('Validation Precision     {:0.5f}'.format(scores['val_precision']))\n        print('Validation Recall        {:0.5f}'.format(scores['val_recall']))\n        print('Validation  F1           %0.5f' %scores['val_f1'] )\n    return scores , cm_df\n\n\ndef train_and_evalaute(params ,X_train, y_train,X_valid, y_valid, feature_imp  ):\n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_valid  = lgb.Dataset(X_valid, y_valid)\n    early_stopping_rounds = 200\n    lgb_results = {}\n\n    model = lgb.train(params,\n                      lgb_train,\n                      num_boost_round = 10000,\n                      valid_sets =  [lgb_train,lgb_valid],\n                      early_stopping_rounds = early_stopping_rounds,                    \n    #                   categorical_feature = cat_cols,\n                      evals_result = lgb_results,\n                      verbose_eval = 100\n                       )\n\n\n    y_prob = model.predict(X_valid)\n    \n    auc = roc_auc_score(y_valid, y_prob)\n    \n    \n    feature_imp['feature'] = model.feature_name()\n    feature_imp['importance']  = model.feature_importance()\n    feature_imp = feature_imp.sort_values(['importance'], ascending = False)\n    \n    return model, auc, feature_imp, lgb_results","17e97340":"%%time\nimport lightgbm as lgb\nTOP_N_FEATURES = 200\nfeature_imp = pd.DataFrame()\n\nparams = {}\n\nparams['learning_rate'] = 0.06\nparams['boosting_type'] = 'gbdt'\nparams['objective'] = 'binary'\nparams['seed'] =  RANDOM_STATE\nparams['metric'] =    'auc'\nparams['bagging_fraction'] = 0.8\nparams['bagging_freq'] = 1\nparams['feature_fraction'] = 0.8\nparams['max_bin'] = 127\nparams['scale_pos_weight'] = 4\n\nif TOP_N_FEATURES is not None:\n    print('Training model on {} Features'.format(X_train.shape[1]))\n    print('Train shape{} Valid Shape{}, Test Shape {}\\n'.format(X_train.shape, X_valid.shape, X_test.shape))\n    \n    \n    _, auc, feature_imp, _ = train_and_evalaute(params ,X_train, y_train,X_valid, y_valid, feature_imp   )\n\n    print('\\nValidation AUC Score on ALL Features is {:.6f}\\n'.format(auc))\n    \nfeature_imp.head()","6c249e48":"if TOP_N_FEATURES is not None:\n\n    feature_imp = feature_imp.head(TOP_N_FEATURES)\n    top_n_features = feature_imp['feature'].tolist()\n    print('\\nTop {} Features\\n'.format(TOP_N_FEATURES))\n    print(top_n_features )\n    \n    X_train = X_train[top_n_features]\n    X_valid  = X_valid[top_n_features]\n    X_test  =  X_test[top_n_features]","9d04f1c4":"%%time\n\nprint('Train shape{} Valid Shape{}, Test Shape {}\\n'.format(X_train.shape, X_valid.shape, X_test.shape))\n\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_valid  = lgb.Dataset(X_valid, y_valid)\nearly_stopping_rounds = 200\nlgb_results = {}\n\nmodel = lgb.train(params,\n                      lgb_train,\n                      num_boost_round = 10000,\n                      valid_sets =  [lgb_train,lgb_valid],\n                      early_stopping_rounds = early_stopping_rounds,                    \n    #                   categorical_feature = cat_cols,\n                      evals_result = lgb_results,\n                      verbose_eval = 100\n                       )\n\nprint('\\nPrinting Model Parameters\\n')\nprint(params)","768c0672":"y_prob = model.predict(X_valid)\nresults, cm_df  = validation_results(y_valid, y_prob, verbose = True)\n","aa7115cc":"\ncm_df.index.name = 'Actual'\ncm_df.columns.name = 'Predicted'\nplt.figure(figsize = (10,7))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(cm_df, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16}, fmt='g')# font size\nplt.show()","0ee71772":"\ndef plot_lgb_scores(lgb_results):\n    train_res = lgb_results['training']['auc']\n    valid_res = lgb_results['valid_1']['auc']\n    ntrees = range(1, len(train_res) + 1)\n\n    plt.figure(figsize = (12, 6))\n    plt.plot(ntrees, train_res , 'b', label = 'Training')\n    plt.plot(ntrees, valid_res, 'r', label = 'Validation')\n    plt.xlabel('Number of Trees', fontsize = 14)\n    plt.ylabel('AUC Score', fontsize = 14)\n    plt.legend(fontsize = 14)\n    plt.show()\n    \n\n    \n","4f2be579":"plot_lgb_scores(lgb_results)","15f48800":"\n\n\nimport sklearn.metrics as metrics\nfpr, tpr, threshold = metrics.roc_curve(y_valid, y_prob)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.figure(figsize = (12, 6))\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","5604461b":"def plot_feature_imp(model, top_n = 30):\n    feature_imp = pd.DataFrame()\n    feature_imp['feature'] = model.feature_name()\n    feature_imp['importance']  = model.feature_importance()\n    feature_imp = feature_imp.sort_values(['importance'], ascending = False)\n    feature_imp_disp = feature_imp.head(top_n)\n    plt.figure(figsize=(10, 12))\n    sns.barplot(x=\"importance\", y=\"feature\", data=feature_imp_disp)\n    plt.title('LightGBM Features')\n    plt.show() \n#     return feature_imp","ca1c99d4":"plot_feature_imp(model, top_n = 20)","790859c1":"y_prob_test = model.predict(X_test)\nsub['isFraud'] = y_prob_test\nsub.to_csv('lgb_sub.csv', index=False)\nsub.head()","1df94e7b":"## Import Libraries","9bf3746a":"### Plot Training vs Validation scores","262346bc":"### Plot ROC Curve","eb009359":"### Transaction Data\n\n","66ce0f3c":"### Display Confusion Matrix","b981fa45":"## Feature Engineering\nCreate new feature and do Analysis for them. \n","f21852f0":"### New Feature: Number of Nulls\n* Count the number of null values in a row. As we can see this is an important feature .\n* This is because as evident from from joint distribution plot if a trasactions have fewer data points availible(more nulls), the chances of fraud are low","c406983a":"## Display Results","fa7d34f6":"## Data Pre-Processing\n* Concatenate the training and test dataset by appending. This is done so that we can apply feature engineering and pre-processing steps to combined set.\n* Convert the categorical features from string to int using ordinal encoding. For example, convert ['A', 'B'. 'C'] to [1,2,3]\n* Create a data frame sub for submission of test scores, we will later fill it with predictions on test set\n","e228720a":"## About the Project","a05af34b":"## Categorical Columns\nCreate list of categorical columns based on decsription below\n<br>https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/101203#latest-586800","59ffa62a":"### Identity Data\n* Variables in this table are identity information \u2013 network connection information (IP, ISP, Proxy, etc) and digital signature (UA\/browser\/os\/version, etc) associated with transactions.\n* As we can see that the columns name for training set and test are not same,we will correct columns names of test set using traning column name","fb0ce1ed":"## Summary\n\n* Do basic data preprocessing steps to convert categorical variables to integers using ordinal encoding.\n* Train on a LightGBM model with 80% data and 20% validation data,\n* Use metric AUC to evaluate the performance of model.\n* Handle Imbalanced Dataset by using inbuilt model parameter SCALE_POS_WEIGHT= 4. Using this we were able to improve F1 Score from 0.38244(Baseline Model) to  0.56081 on current model\n* Using new engineered features only AUC score increased from 0.0.90693(Baseline Model) to  0.92259 on validation set. The AUC of public test dataset increased from 0.906929(baseline) to 0.924521\n* We used Feature selection to select best 200 feature out of 400 plus features without significant loss of any performance. Infract on validation set score increased slightly\n     *    AUC Validation  with 446 Features:  0.922263\n     *    AUC Validation  with 200 Features:  0.923574\n     *    AUC Public test with 446 Features:  0.924521\n     *    AUC Public test with 200 Features  0.923118\n","cac1b4d9":"## Predict on test set\nAlso write the results as csv file","e95cd829":"## Display Fraud Transactions for New Features \nDisplay data with new features and source features for which transactions were fraud\n","1cf4f2ec":"## Read Data\nhttps:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/101203#latest-586800\n\n","f31bc8b4":"* Vesta Corporation provided the dataset for this competition. Vesta Corporation is the forerunner in guaranteed e-commerce payment solutions. Founded in 1995.\n \n* In this competition, the aim is to benchmark machine learning models on a challenging large-scale dataset. \n* The data comes from Vesta's real-world e-commerce transactions and contains a wide range of features from device type to product features. \n* The machine learning model will alert the fraudulent transaction for millions of people around the world, helping hundreds of thousands of businesses reduce their fraud loss and increase their revenue. \n* The training dataset consists of more than 400 features and 5.9 Million samples. This is supervised binary classification problem and goal is to predict if a credit card transaction is Fraud based on input features mentioned below\n\n**Evaluation**\n* The model is evaluated on AUC ROC score. The notebook will produce an output csv file with TransactionID and predicted probabilties on test set,  which will be automatically evaluted by Kaggle.\n\n### Transaction Table \n* TransactionDT: timedelta from a given reference datetime (not an actual timestamp)\n* TransactionAMT: transaction payment amount in USD\n* ProductCD: product code, the product for each transaction\n* card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n* addr: address\n* dist: distance\n* P_ and (R__) emaildomain: purchaser and recipient email domain\n* C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n* D1-D15: timedelta, such as days between previous transaction, etc.\n* M1-M9: match, such as names on card and address, etc.\n* Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n\n<br>  **Categorical Features:**\n* ProductCD\n* card1 - card6\n* addr1, addr2\n* P_emaildomain\n* R_emaildomain\n* M1 - M9\n\n### Identity Table \n* Variables in this table are identity information \u2013 network connection information (IP, ISP, Proxy, etc) and digital signature (UA\/browser\/os\/version, etc) associated with transactions.\n* They're collected by Vesta\u2019s fraud protection system and digital security partners.\n* (The field names are masked and pairwise dictionary will not be provided for privacy protection and contract agreement)\n\n<br> **Categorical Features:**\n* DeviceType\n* DeviceInfo\n* id_12 - id_38\n\n","03f69f5d":"#### Select the top N features based on feature importance\nWe can see that below engineered features are among top 20 features\n* card1_count\n* card2_count\n* addr1_count\n* TAmt_decimal","5825e668":"### Plot Feature Importance\nDisplay top 20 features ","971c8987":"#### Train model on full featureset and get the AUC and Feature Importance\nVariable TOP_N_FEATURES can be set to select number of features. Set it to None if you want to train Final Model on all features\n","47379faf":"## Train LightGBM Model With Validation\n* LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient https:\/\/lightgbm.readthedocs.io\/en\/latest\/Features.html\n* Train on first 80% of dataset and evaluate on next 20 % as data is sorted in time\n* parameter 'SCALE_POS_WEIGHT' is to handle the unbalanced nature of dataset. This parameter gives more weight to minority class, which improves precision, recall and F1 scores\n* No imputation of missing values is necessary as LightGBM can use optimized strategies automatically\n* Feature selection method discussed in above section is used to determine the optimal number of feature.\n* New Features created in above section will be also be used if they are in Top N features list\n","14526d32":"## Numeric Columns\n* From list of all columns remove categorical columns, Target Value, and ID, this will give us numerical columns\n* Display the statistical properties of numeric columns","602ed26b":"## Merge Data\nMake a join between transaction data and identity data which are connected by key 'TransactionID","9f524897":"> ## Feature Selection: Select best features\n* Train the model on all features and get the feature importance for all features from trained model.\n* Select Top N features (TOP_N_FEATURES) based on feature importance\n* Final Model can be then trained on Top N features \n* Experiment with Variable TOP_N_FEATURES to select optimal number of feature. The Value 200 used here is based on results of multiple rounds of experiments\n* Set TOP_N_FEATURES to None if you want to train Final Model on all features \n* Goal is to get best possible AUC score on validation set by using minimum number of features by eliminating less useful features for prediction\n\n\n","c2d9668b":"### New Feature:Transaction Amount Decimal part\n* Get the decimal part of Transaction Amount and Multiply it by 1000\n* This is probably due to fraud transaction happening in foreign currency hence the credit card is charged with decimal amount.\n* We can also see that if the decimal part of transaction amount is zero chances of fraud are less\n","e86a8cd0":"## Create Test, Train and Validation sets\n* From combined dataset split the training and test datasets and separate the target and features\n* Split the training set into training and validation set. We will use first 80% of data as training set and last 20% as validation set.\n* Since the data is sorted in time according to transaction timestamp, we should not use random split.\n","a69179be":"### New Feature: Frequency Counts\n* Count the frequency of important categorical variables related to card, address, emaildoman and product code.\n* As we can see credit card which are used frequently have lesser chance of fraud\n* Frequency counts of categorical variables in general is good way to convert a categorical column into a numeric column which ML model can understand better\n","ee8cb3c8":"### New Feature: Hour of the day\n* From TransactionDT extract the hour of day of the transaction time, encoded as 0-23\n* TransactionDT field indicates the timestamp of a transaction and we can extract time related data from it\n* We can see that more frauds are committed between 1AM and 11 AM. This is probably because fraud originated in different time zone\n","0c2071e8":"## Constants"}}