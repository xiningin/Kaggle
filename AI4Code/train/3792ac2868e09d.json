{"cell_type":{"d950a26b":"code","4c97a2a3":"code","e301923d":"code","33917bce":"code","faa61aa6":"code","3faa49d6":"code","dbfa5039":"code","ba803bec":"code","b0d8488a":"code","ceb2757c":"code","779e9e5b":"code","ff96fb0d":"code","9c7ff69b":"code","5c93ff2c":"code","ee62e324":"code","8cd6e306":"code","7bf9a2ca":"code","103f1449":"code","c618ed5c":"code","351c9a76":"code","7240264a":"code","03cbfa7b":"code","5919af01":"code","c6a871ef":"code","cd762d69":"code","e521cc16":"code","e16c3e03":"code","9b994744":"code","92620212":"code","30a3cda3":"code","ea6db809":"code","eb2be69a":"code","13cac934":"code","9e597bb6":"code","b3006544":"code","08199f50":"code","045081f1":"code","85177869":"code","756f9e8b":"code","1abde70e":"code","815c9209":"code","cce58a0c":"markdown","904cc0d3":"markdown","a108fe42":"markdown","fe508c10":"markdown","e1d391f6":"markdown","9d5127e8":"markdown","b4e5d90f":"markdown","05a2793d":"markdown","3018c5b2":"markdown","0498f4a6":"markdown","5a1f3b55":"markdown"},"source":{"d950a26b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4c97a2a3":"! pip install wordcloud\n! pip install nltk\nfrom fastai.text.all import *\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom wordcloud import WordCloud\nimport re\nimport nltk\nimport string\nfrom nltk import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer \nfrom nltk.corpus import stopwords","e301923d":"df_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndf_test = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","33917bce":"nltk.download('stopwords')\nnltk.download('wordnet')\nstop_words = stopwords.words(\"english\")\nlemmatizer = WordNetLemmatizer()\n\n\n# citation for text_preproc() https:\/\/towardsdatascience.com\/cleaning-text-data-with-python-b69b47b97b76\n# I just added lemmatization\ndef text_preproc(x):\n    x = x.lower()\n    x = ' '.join([lemmatizer.lemmatize(word) for word in x.split(' ') if word not in stop_words])\n    x = x.encode('ascii', 'ignore').decode()\n    x = re.sub(r'https*\\S+', ' ', x)\n    x = re.sub(r'@', ' ', x)\n    x = re.sub(r'amp', '', x) # noise discovered from word clouds\n    x = re.sub(r'new', '', x)\n    \n    x = re.sub(r' s ', '', x)\n    x = re.sub(r'#', ' ', x)\n    x = re.sub(r'\\'\\w+', '', x)\n    x = re.sub('[%s]' % re.escape(string.punctuation), ' ', x)\n    x = re.sub(r'\\w*\\d+\\w*', '', x)\n    x = re.sub(r'\\s{2,}', ' ', x)\n    return x","faa61aa6":"df_train['text'] = df_train['text'].apply(lambda x: text_preproc(x))","3faa49d6":"dis = ''\nfake = ''\nfor i, obj in df_train.iterrows():\n    if obj['target'] == 1:\n        dis += obj[\"text\"] + \" \"\n    elif obj['target'] == 0:\n        fake += obj['text'] + \" \"","dbfa5039":"wordcloud_real = WordCloud().generate(dis) \nplt.imshow(wordcloud_real)","ba803bec":"wordcloud_fake = WordCloud().generate(fake) \nplt.imshow(wordcloud_fake)","b0d8488a":"vectorizer = TfidfVectorizer()","ceb2757c":"training_set = df_train.sample(frac = 0.8) \ntest_set = df_train.drop(training_set.index) \nX = vectorizer.fit_transform(training_set[\"text\"])\ntest_X = vectorizer.transform(test_set[\"text\"])","779e9e5b":"from sklearn.metrics import classification_report\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import LinearSVC\n\nsvc = LinearSVC()\nmlp = MLPClassifier(max_iter=3000)\nrf_clf = RandomForestClassifier(n_estimators=200, random_state=0, bootstrap=True)\n\nv_clf = VotingClassifier(estimators=[('rf',rf_clf), ('svc', svc), ('mlp', mlp)], voting=\"hard\")\n\nv_clf.fit(X, training_set['target'])\n\nprediction = v_clf.predict(test_X)","ff96fb0d":"print(classification_report(test_set[\"target\"], prediction))","9c7ff69b":"df_lm = pd.read_csv('..\/input\/nlp-getting-started\/train.csv') # not using preprocessing applied in sklearn ensemble.","5c93ff2c":"def pre_simp(x):\n    x = re.sub(r'amp', '', x)\n    return x","ee62e324":"dls = TextDataLoaders.from_df(df_lm, text_col='text', is_lm=True, valid_pct=.1, backwards=True)\ndls.show_batch(max_n=3)","8cd6e306":"learn = language_model_learner(dls, AWD_LSTM, pretrained=True, drop_mult=0.5, metrics=[accuracy, perplexity])","7bf9a2ca":"learn.lr_find()","103f1449":"learn.fit_one_cycle(3, 1e-1)","c618ed5c":"learn.unfreeze() # Run for many epochs with all layers unfrozen\nlearn.fit_one_cycle(8, slice(1e-5,2e-2))","351c9a76":"learn.save_encoder('finetuned')","7240264a":"TEXT = \"A horrible\"\nN_WORDS = 20\nN_SENTENCES = 2\npreds = [learn.predict(TEXT, N_WORDS, temperature=0.75) \n         for _ in range(N_SENTENCES)]\n\nprint(\"\\n\".join(preds))","03cbfa7b":"train_X = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndls_clas = TextDataLoaders.from_df(train_X, text_col='text', label_col='target', backwards=True, vocab=dls.vocab)\ndls_clas.show_batch(max_n=3)","5919af01":"learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, metrics=accuracy).to_fp16()","c6a871ef":"learn = learn.load_encoder('finetuned')\n\nlearn.lr_find()","cd762d69":"learn.fit_one_cycle(3,1e-2)","e521cc16":"learn.unfreeze()\nlearn.lr_find()","e16c3e03":"learn.fit_one_cycle(3, slice(2e-5,3e-3))","9b994744":"df_lm = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')","92620212":"#df_lm['text'] = df_lm['text'].apply(lambda x: prep_simp(x))\n\ndls_f = TextDataLoaders.from_df(df_lm, text_col='text', is_lm=True, backwards=False, valid_pct=.1)\ndls.show_batch(max_n=3)","30a3cda3":"learn_f = language_model_learner(dls_f, AWD_LSTM, pretrained=True, drop_mult=0.5, metrics=[accuracy, perplexity]).to_fp16()","ea6db809":"learn = learn.load_encoder('finetuned')\nlearn_f.lr_find()","eb2be69a":"learn_f.fit_one_cycle(3, 2e-1)","13cac934":"learn_f.unfreeze()\nlearn_f.lr_find()","9e597bb6":"learn_f.fit_one_cycle(8, slice(1e-6,3e-3))","b3006544":"learn_f.save_encoder('finetuned')","08199f50":"train_X = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\n#train_X['text'] = train_X['text'].apply(lambda x: preproc(x))\ndls_clas_f = TextDataLoaders.from_df(train_X, text_col='text', label_col='target',backwards=False, vocab=dls_f.vocab)\ndls_clas_f.show_batch(max_n=3)","045081f1":"learn_f = text_classifier_learner(dls_clas_f, AWD_LSTM, drop_mult=0.5, metrics=accuracy).to_fp16()\nlearn_f = learn_f.load_encoder('finetuned')\nlearn_f.lr_find()","85177869":"learn_f.fit_one_cycle(3,2e-2)","756f9e8b":"learn_f.unfreeze()\nlearn_f.lr_find()","1abde70e":"learn_f.fit_one_cycle(6, slice(1e-5,2e-3))","815c9209":"\n\n#ensemble the forward and backward predictions\ndef ens_predict(obj):\n    f_pred = learn_f.predict(obj)[2]\n    b_pred = learn.predict(obj)[2]\n    vec = f_pred + b_pred\n    if vec[0] > vec[1]:\n        return 0\n    else:\n        return 1","cce58a0c":"Now it can generate backwards tweets","904cc0d3":"Now lets build an ensemble model","a108fe42":"Now lets use this pretrained model for a classifier","fe508c10":"First we have to vectorize our words. Ill be using Tfidf (https:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf)","e1d391f6":"Function below used to make ensemble predictions","9d5127e8":"Now that we've cleaned the text a little, lets take a look at the word distributions with word clouds","b4e5d90f":"Now lets do the same thing without a backwards language model","05a2793d":"### Sklearn ensemble Results:","3018c5b2":"Now lets build a model","0498f4a6":"Roughly 79% accuracy with this model. Now for a fastai pretrained model with ULMfit","5a1f3b55":"Lets train two models using language models to pretrain them. First lets make a classifier pretrained on a backwards language model of the tweets"}}