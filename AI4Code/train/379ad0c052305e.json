{"cell_type":{"4b2be21b":"code","fa10a331":"code","ba896c3d":"code","2b0ac2d6":"code","4a80cc05":"code","2bd36f2e":"code","92296c7d":"code","a159a67f":"code","2655e7ef":"code","47aa4fdb":"code","76d9766f":"code","a19b5593":"code","b763cdfa":"code","a3e17b59":"code","918b9b79":"code","cba0aeb0":"code","547f172b":"code","a4a9225e":"code","15e6d3e1":"code","b7d9b9f0":"code","26d2c502":"code","b1cbe8e2":"code","e157b6d5":"code","c20eca7c":"code","141328b8":"code","32a85d17":"code","59362bc3":"code","0c041a2e":"code","900c0d11":"code","dc2ad3a6":"code","63963251":"code","6759250e":"code","3cf6bf80":"code","e6edfd5a":"code","604b0f0d":"code","606277c6":"markdown","13c9edb2":"markdown","6b4068f5":"markdown","683c0c73":"markdown","04353e80":"markdown","deeb3797":"markdown","9d41185f":"markdown","92bce887":"markdown","d40be0c7":"markdown","c089e019":"markdown","d05d0ccc":"markdown","ffe7aa29":"markdown"},"source":{"4b2be21b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fa10a331":"# Import libraries for data manipulation and preprocessing\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Import libraries for resampling\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.over_sampling import BorderlineSMOTE\nfrom imblearn.over_sampling import SVMSMOTE\nfrom imblearn.over_sampling import ADASYN\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.under_sampling import TomekLinks\nfrom imblearn.combine import SMOTEENN","ba896c3d":"# Import libraries for classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\n\n# Import libraries for visualization and set the display\nimport seaborn as sns\nimport matplotlib.pyplot as plt\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\nsns.set(color_codes=True)\n%matplotlib inline\n%config InlineBackend.figure_formats = {'png', 'retina'}","2b0ac2d6":"# Suppressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","4a80cc05":"# Load the dataset\ndf=pd.read_csv('..\/input\/credit-card-customers\/BankChurners.csv')\n\n# Show the shape and the first 5 rows\nprint('Shape of the data', df.shape)\ndf.head()","2bd36f2e":"# Delete the last two columns\n# \"CLIENTNUM\" is not needed for prediction. So, let's delete it.\ndf=df.drop([\"CLIENTNUM\",\n            \"Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1\",\n            \"Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2\"],\n           axis = 1)","92296c7d":"# Display descriptive statistics\ndf.describe()","a159a67f":"# Let's see the type of each column\ndf.info()","2655e7ef":"categories = ['Gender', 'Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category']\n\nfor cat in categories:\n    cross_tab = pd.crosstab(df[cat], df['Attrition_Flag'], normalize='index')\n    cross_tab.plot.bar(stacked=True)\n    plt.show()","47aa4fdb":"cols = [\"Customer_Age\", \"Dependent_count\", \"Months_on_book\", \"Total_Relationship_Count\",\"Months_Inactive_12_mon\",\n        \"Contacts_Count_12_mon\", \"Credit_Limit\",\"Total_Revolving_Bal\",\"Avg_Open_To_Buy\",\"Total_Amt_Chng_Q4_Q1\",\n        \"Total_Trans_Amt\", \"Total_Trans_Ct\", \"Total_Ct_Chng_Q4_Q1\", \"Avg_Utilization_Ratio\"\n        ]","76d9766f":"for col in cols:\n    sns.violinplot(data=df, x='Attrition_Flag', y=col)\n    #set_title(col)\n    #set_ylabel('')\n    plt.show()","a19b5593":"sns.pairplot(df, hue='Attrition_Flag')","b763cdfa":"# Show the unique values of categorical variables\nprint(\"Attrition_Flag :\",df[\"Attrition_Flag\"].unique())\nprint(\"Gender         :\",df[\"Gender\"].unique())\nprint(\"Education_Level:\",df[\"Education_Level\"].unique())\nprint(\"Marital_Status :\",df[\"Marital_Status\"].unique())\nprint(\"Income_Category:\",df[\"Income_Category\"].unique())\nprint(\"Card_Category  :\",df[\"Card_Category\"].unique())","a3e17b59":"# Convert variables with two categories into binary variables\ndf.loc[df[\"Attrition_Flag\"] == \"Existing Customer\", \"Attrition_Flag\"] = 0\ndf.loc[df[\"Attrition_Flag\"] == \"Attrited Customer\", \"Attrition_Flag\"] = 1\ndf[\"Attrition_Flag\"] = df[\"Attrition_Flag\"].astype(int)\n\ndf.loc[df[\"Gender\"] == \"F\", \"Gender\"] = 0\ndf.loc[df[\"Gender\"] == \"M\", \"Gender\"] = 1\ndf[\"Gender\"] = df[\"Gender\"].astype(int)","918b9b79":"df.head()","cba0aeb0":"#One hot encoding for Categorical variables\ndf = pd.get_dummies(df)\ndf.head()","547f172b":"# Split data into train and test Datasets\n\n# Separate the dataset into features and target\nX = df.drop([\"Attrition_Flag\"],axis=1)\ny = df[\"Attrition_Flag\"]\n\ny.value_counts()","a4a9225e":"# Split data into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)","15e6d3e1":"# Apply rebalancing\n\n# Random Oversampling\nover_X_train, over_y_train = RandomOverSampler(sampling_strategy='minority').fit_resample(X_train, y_train)\n# SMOTE\nsmote_X_train, smote_y_train = SMOTE().fit_resample(X_train,y_train)\n# Boderline-SMOTE\nbdlsmote_X_train, bdlsmote_y_train = BorderlineSMOTE().fit_resample(X_train, y_train)\n# Boderline-SMOTE SVM\nbdlSVMsmote_X_train, bdlSVMsmote_y_train = SVMSMOTE().fit_resample(X_train, y_train)\n# ADASYN\nadasyn_X_train, adasyn_y_train = ADASYN().fit_resample(X_train, y_train)\n# SMOTE-TomekLinks\nsmotetomek_X_train, smotetomek_y_train = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority')).fit_resample(X_train, y_train)\n# SMOTE-ENN\nsmoteenn_X_train, smoteenn_y_train = SMOTEENN().fit_resample(X_train, y_train)","b7d9b9f0":"# Check the results of rebalancing\n\n# Random Oversampling\nprint(\"Random Oversampling\\n\", over_y_train.value_counts())\n# SMOTE\nprint(\"SMOTE\\n\", smote_y_train.value_counts())\n# Boderline-SMOTE\nprint(\"Borderline-SMOTE\\n\", bdlsmote_y_train.value_counts())\n# Boderline-SMOTE SVM\nprint(\"Borderline-SMOTE SVM\\n\", bdlSVMsmote_y_train.value_counts())\n# ADASYN\nprint(\"ADASYN\\n\", adasyn_y_train.value_counts())\n# SMOTE-TomekLinks\nprint(\"SMOTE-TomekLinks\\n\", smotetomek_y_train.value_counts())\n# SMOTE-ENN\nprint(\"SMOTE-ENN\\n\", smoteenn_y_train.value_counts())","26d2c502":"datasets = [X_train, y_train, over_X_train, over_y_train, smote_X_train, smote_y_train,\n            bdlsmote_X_train, bdlsmote_y_train, bdlSVMsmote_X_train, bdlSVMsmote_y_train, \n            adasyn_X_train, adasyn_y_train, smotetomek_X_train, smotetomek_y_train, \n            smoteenn_X_train, smoteenn_y_train]\n\nfor dataset in datasets:\n    pd.DataFrame(dataset)","b1cbe8e2":"# Concatenate training and test sets for each resampled datasets\ntrain_concat = pd.concat([X_train, y_train], axis=1)\nover_train_concat = pd.concat([over_X_train, over_y_train], axis=1)\nsmote_train_concat = pd.concat([smote_X_train, smote_y_train], axis=1)\nbdlsmote_train_concat = pd.concat([bdlsmote_X_train, bdlsmote_y_train], axis=1)\nbdlSVMsmote_train_concat = pd.concat([bdlSVMsmote_X_train, bdlSVMsmote_y_train], axis=1)\nadasyn_train_concat = pd.concat([adasyn_X_train, adasyn_y_train], axis=1)\nsmotetomek_train_concat = pd.concat([smotetomek_X_train, smotetomek_y_train], axis=1)\nsmoteenn_train_concat = pd.concat([smoteenn_X_train, smoteenn_y_train], axis=1)","e157b6d5":"# Visualize resampling results\nfig, axes = plt.subplots(4, 2, figsize=(20, 20),squeeze=True)\nplt.subplots_adjust(wspace=0.2, hspace=0.4)\nfig.suptitle('Resampling Result')\n\nsns.scatterplot(ax=axes[0, 0], data=train_concat, x='Total_Amt_Chng_Q4_Q1', y='Total_Ct_Chng_Q4_Q1', hue='Attrition_Flag')\nsns.scatterplot(ax=axes[0, 1], data=over_train_concat, x='Total_Amt_Chng_Q4_Q1', y='Total_Ct_Chng_Q4_Q1', hue='Attrition_Flag', legend=False)\nsns.scatterplot(ax=axes[1, 0], data=smote_train_concat, x='Total_Amt_Chng_Q4_Q1', y='Total_Ct_Chng_Q4_Q1', hue='Attrition_Flag',legend=False)\nsns.scatterplot(ax=axes[1, 1], data=bdlsmote_train_concat, x='Total_Amt_Chng_Q4_Q1', y='Total_Ct_Chng_Q4_Q1', hue='Attrition_Flag',legend=False)\nsns.scatterplot(ax=axes[2, 0], data=bdlSVMsmote_train_concat, x='Total_Amt_Chng_Q4_Q1', y='Total_Ct_Chng_Q4_Q1', hue='Attrition_Flag',legend=False)\nsns.scatterplot(ax=axes[2, 1], data=adasyn_train_concat, x='Total_Amt_Chng_Q4_Q1', y='Total_Ct_Chng_Q4_Q1', hue='Attrition_Flag',legend=False)\nsns.scatterplot(ax=axes[3, 0], data=smotetomek_train_concat, x='Total_Amt_Chng_Q4_Q1', y='Total_Ct_Chng_Q4_Q1', hue='Attrition_Flag',legend=False)\nsns.scatterplot(ax=axes[3, 1], data=smoteenn_train_concat, x='Total_Amt_Chng_Q4_Q1', y='Total_Ct_Chng_Q4_Q1', hue='Attrition_Flag',legend=False)\n\naxes[0, 0].set_title(\"Imbalanced Data\")\naxes[0, 1].set_title(\"Random Oversampling\")\naxes[1, 0].set_title(\"SMOTE\")\naxes[1, 1].set_title(\"Borderline-SMOTE\")\naxes[2, 0].set_title(\"Borderline-SMOTE SVM\")\naxes[2, 1].set_title(\"ADASYN\")\naxes[3, 0].set_title(\"SMOTE-TomekLinks\")\naxes[3, 1].set_title(\"SMOTE-ENN\")","c20eca7c":"# Standardization\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","141328b8":"# Apply rebalancing to standardized data\n\n# Random Oversampling\nover_X_train, over_y_train = RandomOverSampler(sampling_strategy='minority').fit_resample(X_train, y_train)\n# SMOTE\nsmote_X_train, smote_y_train = SMOTE().fit_resample(X_train,y_train)\n# Boderline-SMOTE\nbdlsmote_X_train, bdlsmote_y_train = BorderlineSMOTE().fit_resample(X_train, y_train)\n# Boderline-SMOTE SVM\nbdlSVMsmote_X_train, bdlSVMsmote_y_train = SVMSMOTE().fit_resample(X_train, y_train)\n# ADASYN\nadasyn_X_train, adasyn_y_train = ADASYN().fit_resample(X_train, y_train)\n# SMOTE-TomekLinks\nsmotetomek_X_train, smotetomek_y_train = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority')).fit_resample(X_train, y_train)\n# SMOTE-ENN\nsmoteenn_X_train, smoteenn_y_train = SMOTEENN().fit_resample(X_train, y_train)","32a85d17":"# Check the results of rebalancing\n\n# Random Oversampling\nprint(\"Random Oversampling\\n\", over_y_train.value_counts())\n# SMOTE\nprint(\"SMOTE\\n\", smote_y_train.value_counts())\n# Boderline-SMOTE\nprint(\"Borderline-SMOTE\\n\", bdlsmote_y_train.value_counts())\n# Boderline-SMOTE SVM\nprint(\"Borderline-SMOTE SVM\\n\", bdlSVMsmote_y_train.value_counts())\n# ADASYN\nprint(\"ADASYN\\n\", adasyn_y_train.value_counts())\n# SMOTE-TomekLinks\nprint(\"SMOTE-TomekLinks\\n\", smotetomek_y_train.value_counts())\n# SMOTE-ENN\nprint(\"SMOTE-ENN\\n\", smoteenn_y_train.value_counts())","59362bc3":"# Create a model dictionary\nmodels = {\"Logistic Regression   \": LogisticRegression(),\n          \"K-Nearest Neighbors   \": KNeighborsClassifier(),\n          \"Support Vector Machine\": SVC(probability=True),\n          \"Decision Tree         \": DecisionTreeClassifier(),\n          \"Random Forest         \": RandomForestClassifier(),\n          \"Ada Boost             \": AdaBoostClassifier(),\n          \"XGBoost               \": XGBClassifier(),\n          \"LightGBM              \": LGBMClassifier(),\n          \"CatBoost              \": CatBoostClassifier(verbose=0),\n          \"Neural Network        \": MLPClassifier()\n         }","0c041a2e":"# Fit the models on imbalanced data\nfor name, model in models.items():\n    model.fit(X_train, y_train)\n\n# Print AUC score\nprint(\"Imbalanced Data: AUC score\")\nfor name, model in models.items():\n    print(name + \": {:.3f}\".format(roc_auc_score(y_test, model.predict(X_test))))","900c0d11":"# Fit the models: Random Oversampling\nfor name, model in models.items():\n    model.fit(over_X_train, over_y_train)\n\n# Print AUC score\nprint(\"Random Oversampling: AUC score\")\nfor name, model in models.items():\n    print(name + \": {:.3f}\".format(roc_auc_score(y_test, model.predict(X_test))))","dc2ad3a6":"# Fit the models: SMOTE\nfor name, model in models.items():\n    model.fit(smote_X_train, smote_y_train)\n\n# Print AUC score\nprint(\"SMOTE: AUC score\")\nfor name, model in models.items():\n    print(name + \": {:.3f}\".format(roc_auc_score(y_test, model.predict(X_test))))","63963251":"# Fit the models: Borderline-SMOTE\nfor name, model in models.items():\n    model.fit(bdlsmote_X_train, bdlsmote_y_train)\n\n# Print AUC score\nprint(\"Borderline-SMOTE: AUC score\")\nfor name, model in models.items():\n    print(name + \": {:.3f}\".format(roc_auc_score(y_test, model.predict(X_test))))","6759250e":"# Fit the models: Borderline-SMOTE SVM\nfor name, model in models.items():\n    model.fit(bdlSVMsmote_X_train, bdlSVMsmote_y_train)\n\n# Print AUC score\nprint(\"Borderlin-SMOTE SVM: AUC score\")\nfor name, model in models.items():\n    print(name + \": {:.3f}\".format(roc_auc_score(y_test, model.predict(X_test))))","3cf6bf80":"# Fit the models: ADASYN\nfor name, model in models.items():\n    model.fit(adasyn_X_train, adasyn_y_train)\n\n# Print AUC score\nprint(\"ADASYN: AUC score\")\nfor name, model in models.items():\n    print(name + \": {:.3f}\".format(roc_auc_score(y_test, model.predict(X_test))))","e6edfd5a":"# Fit the models: SMOTE-TomekLinks\nfor name, model in models.items():\n    model.fit(smotetomek_X_train, smotetomek_y_train)\n\n# Print AUC score\nprint(\"SMOTE-TomekLinks: AUC score\")\nfor name, model in models.items():\n    print(name + \": {:.3f}\".format(roc_auc_score(y_test, model.predict(X_test))))","604b0f0d":"# Fit the models: SMOTE-ENN\nfor name, model in models.items():\n    model.fit(smoteenn_X_train, smoteenn_y_train)\n\n# Print AUC\nprint(\"SMOTE-ENN: AUC score\")\nfor name, model in models.items():\n    print(name + \": {:.3f}\".format(roc_auc_score(y_test, model.predict(X_test))))","606277c6":"## Overview of the Resampling Methods","13c9edb2":"### Hybrid Methods\n\nThese methods combines undersampling and oversampling methods.\n\n#### SMOTE-TomekLinks\n- Tomek Links is a method for identifying pairs of nearest neighbors each of them belong to different classes. \n- By removing one or both of these pairs, we can make the decision boundary clearer and less noisy.\n- SMOTE-TomekLink oversamples the minority class by SMOTE, and then, remove the majority class cases in Tomek Links.\n\n#### SMOTE-ENN (SMOTE Edited Nearest Neighbors)\n- ENN is an undersampling method that identify and remove any misclassifed examples based on KNN (k=3).\n- ENN can be applied to all classes or just examples in the majority class.\n- SMOTE-ENN oversamples the minority class by SMOTE, and then, remove the cases by ENN.","6b4068f5":"# 3. Data Preprocessing","683c0c73":"## 3.1. Convert categorical variables to numerical","04353e80":"# 2. Load and Explore Data","deeb3797":"## Rebalancing Samples","9d41185f":"- Customer churn prediction is a binary classification problem to be solved by supervised learning.\n- Let's use major supervised learning algorithm and compare the results.\n   - Logistic Regression\n   - KNN\n   - SVM\n   - Decision Tree\n   - Random Forest\n   - AdaBoost\n   - XGBoost\n   - LightGBM\n   - CatBoost\n   - Neural Network\n- The data is imbalanced, so apply rebalancing methods and compare their results with those of baseline models.\n   - Baseline models: Imbalanced data\n   - Random Oversampling\n   - SMOTE\n   - Borderline-SMOTE\n   - Borderline-SMOTE SVM\n   - ADASYN\n   - SMOTE-TomekLinks\n   - SMOTE-ENN\n- Performance measure\n   - AUC score","92bce887":"### Oversampling Methods\n\n#### Random Oversampling\n- It duplicates the minority class examples randomly, and added them to the training set.\n- Since it is based on simple duplications, it does not provide any additional information to the model.\n- Duplication is implemented with replacement, so it's likely to result in overfitting.\n\n#### SMOTE (Synthetic Minority Oversampling Technique)\n- Instead of simply duplicating existing minority class, SMOTE oversample the minority class by generating synthetic data.\n- Synthesizing new data is based on feature space similarity between exisitng minority class examples.\n- It randomly selects minority class cases, and generate new minority class cases by interpolations based on KNN.\n\n#### Borderline-SMOTE\n- An extension of SMOTE\n- While SMOTE randomly selects minority datapoints for synthesizing, Borderline-SMOTE selects them along the decion boundary between the classes.\n- It deals with the datapoints that are likely to be misclassified.\n\n#### Borderline-SMOTE SVM\n- An variation of Borderline-SMOTE\n- It uses SVM to approximately identify the borderline.\n- Then it randomly creates synthetic data along the borderline.\n- Datapoints far from the borderline are synthesized preferentially.\n\n#### ADASYN (Adaptive Synthetic sampling )\n- An variation of SMOTE\n- It oversamples the minority class based on the data density distributions.\n- It generates synthetic data more in areas where minority class is less dense. ","d40be0c7":"# Classification","c089e019":"The data is imbalanced.","d05d0ccc":"# 1. Import Libraries","ffe7aa29":"- There are no missing values in this dataset.\n- Some are categorical variables. So we need to encode them.\n  - They are \"Attrition_Flag\",\"Gender\",\"Education_Level\",\"Marital_Status\",\"Income_Category\",\"Card_Category\""}}