{"cell_type":{"77cf87f1":"code","daf1cab3":"code","08b1cddb":"code","a73b0ead":"code","53e5364f":"code","4c6045fd":"code","c5c5b0fe":"code","7cd1393e":"code","095cc474":"code","3e2ba846":"code","fa970379":"code","16c7f66f":"code","36b4d972":"code","7d1c16ce":"code","e43ab30b":"code","41c83ae3":"code","e5c51b45":"code","d73c24d8":"code","d64475a6":"code","1d844ef5":"code","6db7317f":"code","98864128":"code","de19e086":"code","886376cb":"code","76672c67":"code","f6136f85":"code","b9b07a46":"code","bbdc4fed":"code","c686e29e":"code","cec894ad":"code","5920f114":"code","76630e49":"code","d8f0a638":"code","d8cd9a7b":"code","b231f8fc":"code","5d559f72":"code","f79ace9e":"code","3f002e4f":"code","0f2d638c":"code","b7edba61":"code","ba99b06a":"code","d5d95496":"code","da98808e":"code","79bc7fd3":"code","d8f8a894":"markdown","112e34a2":"markdown","29f5c4e9":"markdown"},"source":{"77cf87f1":"import keras\nfrom keras.layers import Conv3D,MaxPool3D,Flatten, Dense, Reshape\nfrom keras.layers import Dropout, Input,BatchNormalization\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.utils import np_utils","daf1cab3":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n\nfrom operator import truediv\n\n\nfrom plotly.offline import init_notebook_mode\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.io as sio\nimport os\nimport spectral\n\ninit_notebook_mode(connected=True)\n%matplotlib inline","08b1cddb":"## GLOBAL VARIABLES\ndataset = 'IP'\ntest_ratio = 0.7\nwindowSize = 25","a73b0ead":"def loadData(name):\n    \n    if name == 'IP':\n        data = sio.loadmat('..\/input\/hsi-dataset\/Indian_pines_corrected.mat')['indian_pines_corrected']\n        labels = sio.loadmat('..\/input\/hsi-dataset\/Indian_pines_gt.mat')['indian_pines_gt']\n    elif name == 'SA':\n        data = sio.loadmat('..\/input\/hsi-dataset\/Salinas_corrected.mat')['salinas_corrected']\n        labels = sio.loadmat('..\/input\/hsi-dataset\/Salinas_gt.mat')['salinas_gt']\n    elif name == 'PU':\n        data = sio.loadmat('..\/input\/hsi-dataset\/PaviaU.mat')['paviaU']\n        labels = sio.loadmat('..\/input\/hsi-dataset\/PaviaU_gt.mat')['paviaU_gt']\n    \n    return data, labels","53e5364f":"def splitTrainTestSet(X, y, testRatio, randomState=345):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testRatio, random_state=randomState,\n                                                        stratify=y)\n    return X_train, X_test, y_train, y_test","4c6045fd":"def applyLDA(X,y):\n    newX=np.reshape(X, (-1, X.shape[2]))\n    y=np.reshape(y,(-1,1))\n    lda=LDA()\n    newX = lda.fit_transform(newX,y.ravel())\n    newX = np.reshape(newX, (X.shape[0],X.shape[1],newX.shape[1]))\n    return newX","c5c5b0fe":"def padWithZeros(X, margin=2):\n    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n    x_offset = margin\n    y_offset = margin\n    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n    return newX","7cd1393e":"def createImageCubes(X, y, windowSize=5, removeZeroLabels = True):\n    margin = int((windowSize - 1) \/ 2)\n    zeroPaddedX = padWithZeros(X, margin=margin)\n    # split patches\n    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))\n    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))\n    patchIndex = 0\n    for r in range(margin, zeroPaddedX.shape[0] - margin):\n        for c in range(margin, zeroPaddedX.shape[1] - margin):\n            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]   \n            patchesData[patchIndex, :, :, :] = patch\n            patchesLabels[patchIndex] = y[r-margin, c-margin]\n            patchIndex = patchIndex + 1\n    if removeZeroLabels:\n        patchesData = patchesData[patchesLabels>0,:,:,:]\n        patchesLabels = patchesLabels[patchesLabels>0]\n        patchesLabels -= 1\n    return patchesData, patchesLabels","095cc474":"X, y = loadData(dataset)","3e2ba846":" X= applyLDA(X,y)","fa970379":"X.shape","16c7f66f":"K = X.shape[2]\n\nK","36b4d972":"X, y = createImageCubes(X, y, windowSize=windowSize)\n\nX.shape, y.shape","7d1c16ce":"Xtrain, Xtest, ytrain, ytest = splitTrainTestSet(X, y, test_ratio)\n\nXtrain.shape, Xtest.shape, ytrain.shape, ytest.shape\n","e43ab30b":"Xtrain = Xtrain.reshape(-1, windowSize, windowSize, K, 1)\nXtrain.shape","41c83ae3":"ytrain = np_utils.to_categorical(ytrain)\nytrain.shape","e5c51b45":"S = windowSize\nL = K\noutput_units = 9 if (dataset == 'PU' or dataset == 'PC') else 16","d73c24d8":"## input layer\ninput_layer = Input((S, S, L, 1))\nprint(\"shape of input_layer\",input_layer.shape)\n\n## convolutional layers\nconv_layer1 = Conv3D(filters=8, kernel_size=(3, 3, 5), activation='relu')(input_layer)\nprint(\"shape of conv Layer 1:\",conv_layer1.shape)\nconv_layer2 = Conv3D(filters=8, kernel_size=(3, 3, 3), activation='relu')(conv_layer1)\nprint(\"shape of conv Layer 2:\",conv_layer2.shape)\npooling_layer1 = MaxPool3D(pool_size=(2, 2, 2))(conv_layer2)\nprint(\"shape of pooling Layer 1:\",pooling_layer1.shape)\npooling_layer2 = BatchNormalization()(pooling_layer1)\nflatten_layer = Flatten()(pooling_layer2) \n\n## fully connected layers\ndense_layer1 = Dense(units=256, activation='relu')(flatten_layer)\ndense_layer1 = Dropout(0.5)(dense_layer1)\ndense_layer2 = Dense(units=128, activation='relu')(dense_layer1)\ndense_layer2 = Dropout(0.5)(dense_layer2)\noutput_layer = Dense(units=output_units, activation='softmax')(dense_layer2)","d64475a6":"# define the model with input layer and output layer\nmodel = Model(inputs=input_layer, outputs=output_layer)","1d844ef5":"model.summary()","6db7317f":"adam = Adam(lr=0.001, decay=1e-06)\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])","98864128":"# checkpoint\nfilepath = \"model.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='accuracy', verbose=1, save_best_only=True, mode='max')\ncallbacks_list = [checkpoint]","de19e086":"history = model.fit(x=Xtrain, y=ytrain, batch_size=256, epochs=100, callbacks=callbacks_list)","886376cb":"import matplotlib.pyplot as plt\nplt.figure(figsize=(7,7)) \nplt.grid() \nplt.plot(history.history['loss'])","76672c67":"\nplt.figure(figsize=(5,5)) \nplt.ylim(0,1.1) \nplt.grid() \nplt.plot(history.history['accuracy'])\nplt.ylabel('Loss') \nplt.xlabel('Epochs') \n\nplt.savefig(\"loss_curve.pdf\") \nplt.show()","f6136f85":"plt.ylabel('Accuracy') \nplt.xlabel('Epochs') \nplt.legend(['Training','Validation']) \nplt.savefig(\"acc_curve.pdf\") \nplt.show()","b9b07a46":"# load best weights\nmodel.load_weights(\"model.hdf5\")\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])","bbdc4fed":"Xtest = Xtest.reshape(-1, windowSize, windowSize, K, 1)\nXtest.shape","c686e29e":"ytest = np_utils.to_categorical(ytest)\nytest.shape","cec894ad":"Y_pred_test = model.predict(Xtest)\ny_pred_test = np.argmax(Y_pred_test, axis=1)\n\nclassification = classification_report(np.argmax(ytest, axis=1), y_pred_test)\nprint(classification)","5920f114":"def AA_andEachClassAccuracy(confusion_matrix):\n    counter = confusion_matrix.shape[0]\n    list_diag = np.diag(confusion_matrix)\n    list_raw_sum = np.sum(confusion_matrix, axis=1)\n    each_acc = np.nan_to_num(truediv(list_diag, list_raw_sum))\n    average_acc = np.mean(each_acc)\n    return each_acc, average_acc","76630e49":"def reports (X_test,y_test,name):\n    #start = time.time()\n    Y_pred = model.predict(X_test)\n    y_pred = np.argmax(Y_pred, axis=1)\n    #end = time.time()\n    #print(end - start)\n    if name == 'IP':\n        target_names = ['Alfalfa', 'Corn-notill', 'Corn-mintill', 'Corn'\n                        ,'Grass-pasture', 'Grass-trees', 'Grass-pasture-mowed', \n                        'Hay-windrowed', 'Oats', 'Soybean-notill', 'Soybean-mintill',\n                        'Soybean-clean', 'Wheat', 'Woods', 'Buildings-Grass-Trees-Drives',\n                        'Stone-Steel-Towers']\n    elif name == 'SA':\n        target_names = ['Brocoli_green_weeds_1','Brocoli_green_weeds_2','Fallow','Fallow_rough_plow','Fallow_smooth',\n                        'Stubble','Celery','Grapes_untrained','Soil_vinyard_develop','Corn_senesced_green_weeds',\n                        'Lettuce_romaine_4wk','Lettuce_romaine_5wk','Lettuce_romaine_6wk','Lettuce_romaine_7wk',\n                        'Vinyard_untrained','Vinyard_vertical_trellis']\n    elif name == 'PU':\n        target_names = ['Asphalt','Meadows','Gravel','Trees', 'Painted metal sheets','Bare Soil','Bitumen',\n                        'Self-Blocking Bricks','Shadows']\n    \n    classification = classification_report(np.argmax(y_test, axis=1), y_pred, target_names=target_names)\n    oa = accuracy_score(np.argmax(y_test, axis=1), y_pred)\n    confusion = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n    each_acc, aa = AA_andEachClassAccuracy(confusion)\n    kappa = cohen_kappa_score(np.argmax(y_test, axis=1), y_pred)\n    score = model.evaluate(X_test, y_test, batch_size=32)\n    Test_Loss =  score[0]*100\n    Test_accuracy = score[1]*100\n    \n    return classification, confusion, Test_Loss, Test_accuracy, oa*100, each_acc*100, aa*100, kappa*100","d8f0a638":"classification, conf, Test_loss, Test_accuracy, oa, each_acc, aa, kappa = reports(Xtest,ytest,dataset)\nclassification = str(classification)\nconfusion = str(conf)\nfile_name = \"classification_report.txt\"\n\nwith open(file_name, 'w') as x_file:\n    x_file.write('Test ratio ={}'.format(test_ratio))\n    x_file.write('\\n')\n    x_file.write('Window size ={}'.format(windowSize))\n    x_file.write('\\n')\n    x_file.write('\\n')\n    x_file.write('{} Test loss (%)'.format(Test_loss))\n    x_file.write('\\n')\n    x_file.write('{} Test accuracy (%)'.format(Test_accuracy))\n    x_file.write('\\n')\n    x_file.write('\\n')\n    x_file.write('{} Kappa accuracy (%)'.format(kappa))\n    x_file.write('\\n')\n    x_file.write('{} Overall accuracy (%)'.format(oa))\n    x_file.write('\\n')\n    x_file.write('{} Average accuracy (%)'.format(aa))\n    x_file.write('\\n')\n    x_file.write('\\n')\n    x_file.write('{}'.format(classification))\n    x_file.write('\\n')\n    x_file.write('{}'.format(confusion))","d8cd9a7b":"def Patch(data,height_index,width_index):\n    height_slice = slice(height_index, height_index+PATCH_SIZE)\n    width_slice = slice(width_index, width_index+PATCH_SIZE)\n    patch = data[height_slice, width_slice, :]\n    \n    return patch","b231f8fc":"# load the original image\nX, y = loadData(dataset)","5d559f72":"height = y.shape[0]\nwidth = y.shape[1]\nPATCH_SIZE = windowSize\nnumComponents = K","f79ace9e":"X= applyLDA(X,y)","3f002e4f":"X = padWithZeros(X, PATCH_SIZE\/\/2)","0f2d638c":"# calculate the predicted image\noutputs = np.zeros((height,width))\nfor i in range(height):\n    for j in range(width):\n        target = int(y[i,j])\n        if target == 0 :\n            continue\n        else :\n            image_patch=Patch(X,i,j)\n            X_test_image = image_patch.reshape(1,image_patch.shape[0],image_patch.shape[1], image_patch.shape[2], 1).astype('float32')                                   \n            prediction = (model.predict(X_test_image))\n            prediction = np.argmax(prediction, axis=1)\n            outputs[i][j] = prediction+1","b7edba61":"ground_truth = spectral.imshow(classes = y,figsize =(7,7))","ba99b06a":"predict_image = spectral.imshow(classes = outputs.astype(int),figsize =(7,7))","d5d95496":"spectral.save_rgb(\"predictions.jpg\", outputs.astype(int), colors=spectral.spy_colors)","da98808e":"f = open(\"classification_report.txt\", \"r\")\nprint(f.read()) ","79bc7fd3":"import scikitplot as skplt\n\nskplt.metrics.plot_confusion_matrix(\n    y_test, \n    y_pred,\n    figsize=(12,12));","d8f8a894":"# Validation","112e34a2":"# plt.plot(history.history['val_acc'])","29f5c4e9":"*MODELING AND TRAINING****"}}