{"cell_type":{"3342496f":"code","3ab50682":"code","32098b2f":"code","8deca0b0":"code","8f21bb1c":"code","e2060a2f":"code","3b68103e":"code","aa98ab15":"code","1d69255b":"code","9d14917c":"code","32630d59":"code","d65d34ac":"code","37489b53":"code","834e838d":"code","f61e3b66":"code","fff0e3a3":"code","b4efff24":"code","3bd4ff63":"code","1f9caa19":"code","968e6c6f":"code","99bc0e09":"code","4f60c0ae":"code","6e562608":"code","87264eca":"markdown","aa540db5":"markdown"},"source":{"3342496f":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import *\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom scipy.stats import mode, skew, kurtosis, entropy\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport dask.dataframe as dd\nfrom dask.multiprocessing import get\n\nfrom tqdm import tqdm, tqdm_notebook\ntqdm.pandas(tqdm_notebook)\n\n# Any results you write to the current directory are saved as output.","3ab50682":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\n\ntransact_cols = [f for f in train.columns if f not in [\"ID\", \"target\"]]\ny = np.log1p(train[\"target\"]).values","32098b2f":"test[\"target\"] = train[\"target\"].mean()","8deca0b0":"cols = ['f190486d6', '58e2e02e6', 'eeb9cd3aa', '9fd594eec', '6eef030c1', '15ace8c9f', \n        'fb0f5dbfe', '58e056e12', '20aa07010', '024c577b9', 'd6bb78916', 'b43a7cfd5', \n        '58232a6fb', '1702b5bf0', '324921c7b', '62e59a501', '2ec5b290f', '241f0f867', \n        'fb49e4212', '66ace2992', 'f74e8f13d', '5c6487af1', '963a49cdc', '26fc93eb7', \n        '1931ccfdd', '703885424', '70feb1494', '491b9ee45', '23310aa6f', 'e176a204a', \n        '6619d81fc', '1db387535', \n        'fc99f9426', '91f701ba2', '0572565c2', '190db8488', 'adb64ff71', 'c47340d97', 'c5a231d81', '0ff32eb98'\n       ]","8f21bb1c":"# from: https:\/\/www.kaggle.com\/dfrumkin\/a-simple-way-to-use-giba-s-features-v2\ndef _get_leak(df, cols, lag=0):\n    d1 = df[cols[:-lag-2]].apply(tuple, axis=1).to_frame().rename(columns={0: 'key'})\n    d2 = df[cols[lag+2:]].apply(tuple, axis=1).to_frame().rename(columns={0: 'key'})\n    d2['pred'] = df[cols[lag]]\n    #d2 = d2[d2.pred != 0] ### to make output consistent with Hasan's function\n    d3 = d2[~d2.duplicated(['key'], keep=False)]\n    return d1.merge(d3, how='left', on='key').pred.fillna(0)","e2060a2f":"def compiled_leak_result():\n    \n    max_nlags = len(cols) - 2\n    train_leak = train[[\"ID\", \"target\"] + cols]\n    train_leak[\"compiled_leak\"] = 0\n    train_leak[\"nonzero_mean\"] = train[transact_cols].apply(\n        lambda x: np.expm1(np.log1p(x[x!=0]).mean()), axis=1\n    )\n    \n    scores = []\n    leaky_value_counts = []\n    leaky_value_corrects = []\n    leaky_cols = []\n    \n    for i in range(max_nlags):\n        c = \"leaked_target_\"+str(i)\n        \n        print('Processing lag', i)\n        train_leak[c] = _get_leak(train_leak, cols, i)\n        \n        leaky_cols.append(c)\n        train_leak = train.join(\n            train_leak.set_index(\"ID\")[leaky_cols+[\"compiled_leak\", \"nonzero_mean\"]], \n            on=\"ID\", how=\"left\"\n        )[[\"ID\", \"target\"] + cols + leaky_cols+[\"compiled_leak\", \"nonzero_mean\"]]\n        zeroleak = train_leak[\"compiled_leak\"]==0\n        train_leak.loc[zeroleak, \"compiled_leak\"] = train_leak.loc[zeroleak, c]\n        leaky_value_counts.append(sum(train_leak[\"compiled_leak\"] > 0))\n        _correct_counts = sum(train_leak[\"compiled_leak\"]==train_leak[\"target\"])\n        leaky_value_corrects.append(_correct_counts\/leaky_value_counts[-1])\n        print(\"Leak values found in train\", leaky_value_counts[-1])\n        print(\n            \"% of correct leaks values in train \", \n            leaky_value_corrects[-1]\n        )\n        tmp = train_leak.copy()\n        tmp.loc[zeroleak, \"compiled_leak\"] = tmp.loc[zeroleak, \"nonzero_mean\"]\n        scores.append(np.sqrt(mean_squared_error(y, np.log1p(tmp[\"compiled_leak\"]).fillna(14.49))))\n        print(\n            'Score (filled with nonzero mean)', \n            scores[-1]\n        )\n    result = dict(\n        score=scores, \n        leaky_count=leaky_value_counts,\n        leaky_correct=leaky_value_corrects,\n    )\n    return train_leak, result","3b68103e":"train_leak, result = compiled_leak_result()","aa98ab15":"result = pd.DataFrame.from_dict(result, orient='columns')\nresult.T","1d69255b":"result.to_csv('train_leaky_stat.csv', index=False)","9d14917c":"train_leak.head()","32630d59":"best_score = np.min(result['score'])\nbest_lag = np.argmin(result['score'])\nprint('best_score', best_score, '\\nbest_lag', best_lag)","d65d34ac":"def rewrite_compiled_leak(leak_df, lag):\n    leak_df[\"compiled_leak\"] = 0\n    for i in range(lag):\n        c = \"leaked_target_\"+str(i)\n        zeroleak = leak_df[\"compiled_leak\"]==0\n        leak_df.loc[zeroleak, \"compiled_leak\"] = leak_df.loc[zeroleak, c]\n    return leak_df","37489b53":"leaky_cols = [c for c in train_leak.columns if 'leaked_target_' in c]\ntrain_leak = rewrite_compiled_leak(train_leak, best_lag)\ntrain_leak[['ID']+leaky_cols+['compiled_leak']].head()","834e838d":"train_res = train_leak[leaky_cols+['compiled_leak']].replace(0.0, np.nan)\ntrain_res.to_csv('train_leak.csv', index=False)","f61e3b66":"def compiled_leak_result_test(max_nlags):\n    test_leak = test[[\"ID\", \"target\"] + cols]\n    test_leak[\"compiled_leak\"] = 0\n    test_leak[\"nonzero_mean\"] = test[transact_cols].apply(\n        lambda x: np.expm1(np.log1p(x[x!=0]).mean()), axis=1\n    )\n    \n    scores = []\n    leaky_value_counts = []\n    # leaky_value_corrects = []\n    leaky_cols = []\n    \n    for i in range(max_nlags):\n        c = \"leaked_target_\"+str(i)\n        \n        print('Processing lag', i)\n        test_leak[c] = _get_leak(test_leak, cols, i)\n        \n        leaky_cols.append(c)\n        test_leak = test.join(\n            test_leak.set_index(\"ID\")[leaky_cols+[\"compiled_leak\", \"nonzero_mean\"]], \n            on=\"ID\", how=\"left\"\n        )[[\"ID\", \"target\"] + cols + leaky_cols+[\"compiled_leak\", \"nonzero_mean\"]]\n        zeroleak = test_leak[\"compiled_leak\"]==0\n        test_leak.loc[zeroleak, \"compiled_leak\"] = test_leak.loc[zeroleak, c]\n        leaky_value_counts.append(sum(test_leak[\"compiled_leak\"] > 0))\n        #_correct_counts = sum(train_leak[\"compiled_leak\"]==train_leak[\"target\"])\n        #leaky_value_corrects.append(_correct_counts\/leaky_value_counts[-1])\n        print(\"Leak values found in test\", leaky_value_counts[-1])\n        #print(\n        #    \"% of correct leaks values in train \", \n        #    leaky_value_corrects[-1]\n        #)\n        #tmp = train_leak.copy()\n        #tmp.loc[zeroleak, \"compiled_leak\"] = tmp.loc[zeroleak, \"nonzero_mean\"]\n        #scores.append(np.sqrt(mean_squared_error(y, np.log1p(tmp[\"compiled_leak\"]).fillna(14.49))))\n        #print(\n        #    'Score (filled with nonzero mean)', \n        #    scores[-1]\n        #)\n    result = dict(\n        # score=scores, \n        leaky_count=leaky_value_counts,\n        # leaky_correct=leaky_value_corrects,\n    )\n    return test_leak, result","fff0e3a3":"test_leak, test_result = compiled_leak_result_test(max_nlags=38)","b4efff24":"test_result = pd.DataFrame.from_dict(test_result, orient='columns')\ntest_result.T","3bd4ff63":"test_result.to_csv('test_leaky_stat.csv', index=False)","1f9caa19":"test_leak = rewrite_compiled_leak(test_leak, best_lag)\ntest_leak[['ID']+leaky_cols+['compiled_leak']].head()","968e6c6f":"test_res = test_leak[leaky_cols+['compiled_leak']].replace(0.0, np.nan)\ntest_res.to_csv('test_leak.csv', index=False)","99bc0e09":"test_leak.loc[test_leak[\"compiled_leak\"]==0, \"compiled_leak\"] = test_leak.loc[test_leak[\"compiled_leak\"]==0, \"nonzero_mean\"]","4f60c0ae":"#submission\nsub = test[[\"ID\"]]\nsub[\"target\"] = test_leak[\"compiled_leak\"]\nsub.to_csv(f\"baseline_sub_lag_{best_lag}.csv\", index=False)\nprint(f\"baseline_sub_lag_{best_lag}.csv saved\")","6e562608":"sub.head()","87264eca":"We take time series columns from [here](https:\/\/www.kaggle.com\/johnfarrell\/giba-s-property-extended-result)","aa540db5":"> Please go through Giba's post and kernel  to underrstand what this leak is all about\n> https:\/\/www.kaggle.com\/titericz\/the-property-by-giba (kernel)\n> https:\/\/www.kaggle.com\/c\/santander-value-prediction-challenge\/discussion\/61329 (post)\n> \n> Also, go through this Jiazhen's kernel which finds more columns to exploit leak\n> https:\/\/www.kaggle.com\/johnfarrell\/giba-s-property-extended-result\n> \n> I just exploit data property in brute force way and then fill in remaining by row non zero means! This should bring everyone on level-playing field.\n> \n> **Let the competition begin! :D**\n\n### Just some small modifications from [original baseline](https:\/\/www.kaggle.com\/tezdhar\/breaking-lb-fresh-start)~\n- The leak rows are calculated separately on train\/test set\n- Calculated the leaky values, correctness, for each lag\n- Hope this can help to do some *lag_selection*\n\n### Update leak process codes to Dmitry Frumkin's *fast* [version](https:\/\/www.kaggle.com\/dfrumkin\/a-simple-way-to-use-giba-s-features-v2)\n- The result of Dmitry's original function and result of Hasan's function seem slightly different\n- Modified to make the output consistent with Hasan's function (Seems better score)"}}