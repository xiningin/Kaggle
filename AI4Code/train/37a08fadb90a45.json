{"cell_type":{"cc604560":"code","e084e0b5":"code","f0984885":"code","9699424c":"code","4562af97":"code","970e760c":"code","2dd69a5e":"code","7846b3d1":"code","12e47685":"code","485121d3":"code","029c8ef3":"code","cb8eccdd":"code","1f9736cb":"code","11468ceb":"code","cbfd4a85":"code","bfe47dbd":"code","834001b4":"code","3b413328":"code","15541bfe":"code","9c66a0d2":"code","270b8e19":"code","5bba15bc":"code","6684ccbd":"code","71f7d4b4":"code","2abf3a3b":"code","98de3b08":"code","6396f0d5":"code","13ca8f5d":"code","fb8cb1a0":"code","98cbc5c3":"code","da0e3ee4":"code","5d558c4d":"code","461b1be1":"code","26f4d7a5":"code","f4b69d3a":"code","7acd3baf":"code","ad98c35b":"code","1d4f7b96":"code","ed5136a7":"code","5276c0be":"code","bd85b493":"code","68577a94":"code","6ef5349d":"markdown","313ec73b":"markdown","862c4737":"markdown","796164a8":"markdown","534ad930":"markdown","85800f68":"markdown","b98d4cbe":"markdown","c4b48def":"markdown","9b778dff":"markdown","6eb08d2c":"markdown","84f8ac5f":"markdown","387e1a31":"markdown","681d746d":"markdown","fed1e0fb":"markdown","1d0f481f":"markdown","ebdb7c99":"markdown","acf67972":"markdown","40b37083":"markdown","8434b649":"markdown","881b67df":"markdown","a1763cc7":"markdown","4a9162f4":"markdown","5f4d8d52":"markdown","c97d755e":"markdown","e83c468c":"markdown","43bfb7b9":"markdown","4221b6ce":"markdown","2209a2dd":"markdown","1d27a082":"markdown","843f250c":"markdown","4155e147":"markdown"},"source":{"cc604560":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom scipy.stats import boxcox\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n","e084e0b5":"##Reading the dataset\ndf = pd.read_csv(\"..\/input\/loan.csv\", low_memory=False)\ndf.head(3)","f0984885":"##Finding the the count and percentage of values that are missing in the dataframe.\ndf_null = pd.DataFrame({'Count': df.isnull().sum(), 'Percent': 100*df.isnull().sum()\/len(df)})\n\n##printing columns with null count more than 0\ndf_null[df_null['Count'] > 0] ","9699424c":"df1 = df.dropna(axis=1, thresh=int(0.80*len(df)))","4562af97":"df1.head(5)","970e760c":"\ndf_LC = df1.filter(['loan_amnt','term','int_rate','installment','grade','sub_grade','emp_length','home_ownership',\n                    'annual_inc','verification_status','purpose','dti','delinq_2yrs','loan_status'])\ndf_LC.dtypes","2dd69a5e":"plt.figure(figsize=(20,20))\nsns.set_context(\"paper\", font_scale=1)\n##finding the correllation matrix and changing the categorical data to category for the plot.\nsns.heatmap(df_LC.assign(grade=df_LC.grade.astype('category').cat.codes,\n                         sub_g=df_LC.sub_grade.astype('category').cat.codes,\n                         term=df_LC.term.astype('category').cat.codes,\n                        emp_l=df_LC.emp_length.astype('category').cat.codes,\n                         ver =df_LC.verification_status.astype('category').cat.codes,\n                        home=df_LC.home_ownership.astype('category').cat.codes,\n                        purp=df_LC.purpose.astype('category').cat.codes).corr(), \n                         annot=True, cmap='bwr',vmin=-1, vmax=1, square=True, linewidths=0.5)\n","7846b3d1":"df_LC.drop(['installment','grade','sub_grade','verification_status','term']\n           , axis=1, inplace = True)","12e47685":"##printing the count and null values in the dataframe\ndflc_null = pd.DataFrame({'Count': df_LC.isnull().sum(), 'Percent': 100*df_LC.isnull().sum()\/len(df_LC)})\ndflc_null[dflc_null['Count'] > 0]","485121d3":"## dropping the null rows since we have sufficient amount of data and there is no need to fill the null values.\ndf_LC.dropna(axis=0)","029c8ef3":"## printing unique statuses in the loan status column (dependent variable)\ndf_LC['loan_status'].unique()","cb8eccdd":"m =df_LC['loan_status'].value_counts()\nm = m.to_frame()\nm.reset_index(inplace=True)\nm.columns = ['Loan Status','Count']\nplt.subplots(figsize=(20,8))\nsns.barplot(y='Count', x='Loan Status', data=m)\nplt.xlabel(\"Length\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of Loan Status in our Dataset\")\nplt.show()\n","1f9736cb":"df_LC = df_LC[df_LC.loan_status != 'Current']\ndf_LC = df_LC[df_LC.loan_status != 'In Grace Period']\ndf_LC = df_LC[df_LC.loan_status != 'Late (16-30 days)']\ndf_LC = df_LC[df_LC.loan_status != 'Late (31-120 days)']\ndf_LC = df_LC[df_LC.loan_status != 'Does not meet the credit policy. Status:Fully Paid']\ndf_LC = df_LC[df_LC.loan_status != 'Does not meet the credit policy. Status:Charged Off']\ndf_LC = df_LC[df_LC.loan_status != 'Issued']","11468ceb":"df_LC['loan_status'] = df_LC['loan_status'].replace({'Charged Off':'Default'})\ndf_LC['loan_status'].value_counts()","cbfd4a85":"\ndf_LC.loan_status=df_LC.loan_status.astype('category').cat.codes\ndf_LC.delinq_2yrs=df_LC.delinq_2yrs.astype('category').cat.codes\ndf_LC.head()\ndf_LC['loan_status'].value_counts()\n#df_LC = pd.get_dummies(df_LC, drop_first=True)\n#df_LC","bfe47dbd":"df_LC.dtypes","834001b4":"numerical = df_LC.columns[df_LC.dtypes == 'float64']\nfor i in numerical:\n    if df_LC[i].min() > 0:\n        transformed, lamb = boxcox(df_LC.loc[df[i].notnull(), i])\n        if np.abs(1 - lamb) > 0.02:\n            df_LC.loc[df[i].notnull(), i] = transformed","3b413328":"df_LC = pd.get_dummies(df_LC, drop_first=True)","15541bfe":"traindata, testdata = train_test_split(df_LC, stratify=df_LC['loan_status'],test_size=.4, random_state=17)\ntestdata.reset_index(drop=True, inplace=True)\ntraindata.reset_index(drop=True, inplace=True)","9c66a0d2":"sc = StandardScaler()\nXunb = traindata.drop('loan_status', axis=1)\nyunb = traindata['loan_status']\nnumerical = Xunb.columns[(Xunb.dtypes == 'float64') | (Xunb.dtypes == 'int64')].tolist()\nXunb[numerical] = sc.fit_transform(Xunb[numerical])","270b8e19":"##checking the shape of train data\nyunb.shape","5bba15bc":"def createROC(models, X, y, Xte, yte):\n    false_p, true_p = [], [] ##false postives and true positives\n\n    for i in models.keys():  ##dict of models\n        models[i].fit(X, y)\n\n        fp, tp, threshold = roc_curve(yte, models[i].predict_proba(Xte)[:,1]) ##roc_curve function\n\n        true_p.append(tp)\n        false_p.append(fp)\n    return true_p, false_p ##returning the true postive and false positive","6684ccbd":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\n\nmodels = {'LDA': LinearDiscriminantAnalysis(),\n          'MNB': MultinomialNB(),\n          'RF': RandomForestClassifier(n_estimators=100),\n          'LR': LogisticRegression(C=1)}\n\nunbalset = {}\nfor i in models.keys():\n    scores = cross_val_score(models[i], Xunb - np.min(Xunb) + 1,\n                                    yunb, cv=3)\n    unbalset[i] = scores\n    print(i, scores, np.mean(scores))","71f7d4b4":"Xte = testdata.drop('loan_status', axis=1)\nyte = testdata['loan_status']\nnumerical = Xte.columns[(Xte.dtypes == 'float64') | (Xte.dtypes == 'int64')].tolist()\nXte[numerical] = sc.fit_transform(Xte[numerical])","2abf3a3b":"tp_unbalset, fp_unbalset = createROC(models, Xunb - np.min(Xunb) + 1, yunb, Xte - np.min(Xte) + 1, yte)","98de3b08":"model =  LogisticRegression(C=1)\nmodel.fit(Xunb, yunb)\npredict = model.predict(Xte) #prediction of Xte which can be used to test against yte (testdata values or true values of y)","6396f0d5":"m = yte.to_frame()\nm['loan_status'].value_counts()","13ca8f5d":"fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18,5))\n\nax = pd.DataFrame(unbalset).boxplot(widths=(0.9,0.9,0.9,0.9), grid=False, vert=False, ax=axes[0])\nax.set_ylabel('Classifier')\nax.set_xlabel('Cross-Validation Score')\n\nfor i in range(0, len(tp_unbalset)):\n    axes[1].plot(fp_unbalset[i], tp_unbalset[i], lw=1)\n\naxes[1].plot([0, 1], [0, 1], '--k', lw=1)\naxes[1].legend(models.keys())\naxes[1].set_ylabel('True Positive Rate')\naxes[1].set_xlabel('False Positive Rate')\naxes[1].set_xlim(0,1)\naxes[1].set_ylim(0,1)\n\ncm = confusion_matrix(yte, predict).T\ncm = cm.astype('float')\/cm.sum(axis=0)\n\nax = sns.heatmap(cm, annot=True, cmap='Blues', ax=axes[2]);\nax.set_xlabel('True Value')\nax.set_ylabel('Predicted Value')\nax.axis('equal')","fb8cb1a0":"fp, tp, threshold = roc_curve(yte, model.predict_proba(Xte)[:,1]) #getting false and true positive from test set","98cbc5c3":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16,6))\n\nax[0].plot(threshold, tp + (1 - fp))\nax[0].set_xlabel('Threshold')\nax[0].set_ylabel('Sensitivity + Specificity')\n\nax[1].plot(threshold, tp, label=\"tp\")\nax[1].plot(threshold, 1 - fp, label=\"1 - fp\")\nax[1].legend()\nax[1].set_xlabel('Threshold')\nax[1].set_ylabel('True Positive & False Positive Rates')","da0e3ee4":"##finding the optimal threshold for the model \nfunction = tp + (1 - fp)\nindex = np.argmax(function)\n\noptimal_threshold = threshold[np.argmax(function)]\nprint('optimal threshold:', optimal_threshold)","5d558c4d":"predict = model.predict_proba(Xte)[:,1]\npredict = np.where(predict >= optimal_threshold, 1, 0)\n\nfig, axes = plt.subplots(figsize=(15,6))\n\ncm = confusion_matrix(yte, predict).T\ncm = cm.astype('float')\/cm.sum(axis=0)\n\nax = sns.heatmap(cm, annot=True, cmap='Blues');\nax.set_xlabel('True Value')\nax.set_ylabel('Predicted Value')\nax.axis('equal')","461b1be1":"y_default = traindata[traindata['loan_status'] == 0]\nn_paid = traindata[traindata['loan_status'] == 1].sample(n=len(y_default), random_state=17) ##chosing equal amount of 1's\n\n##creating a new dataframe for balanced set\ndata = y_default.append(n_paid) \n\n##creating the independent and dependent array\nXbal = data.drop('loan_status', axis=1)\nybal = data['loan_status']","26f4d7a5":"## scaling it again\nnumerical = Xbal.columns[(Xbal.dtypes == 'float64') | (Xbal.dtypes == 'int64')].tolist()\nXbal[numerical] = sc.fit_transform(Xbal[numerical])","f4b69d3a":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\n\nmodels = {'LDA': LinearDiscriminantAnalysis(),\n          'MNB': MultinomialNB(),\n          'RF': RandomForestClassifier(n_estimators=100),\n          'LR': LogisticRegression(C=1)}\n\nbalset = {}\nfor i in models.keys():\n    scores = cross_val_score(models[i], Xbal - np.min(Xbal) + 1,\n                                    ybal, scoring='roc_auc', cv=3)\n    balset[i] = scores\n    print(i, scores, np.mean(scores))","7acd3baf":"model = RandomForestClassifier(n_estimators=100)\nmodel.fit(Xbal, ybal)\npredict = model.predict(Xte)","ad98c35b":"predict = model.predict(Xte)\nfig, axes = plt.subplots(figsize=(8,6))\ncm = confusion_matrix(yte, predict).T\ncm = cm.astype('float')\/cm.sum(axis=0)\nax = sns.heatmap(cm, annot=True, cmap='Blues');\nax.set_xlabel('True Label')\nax.set_ylabel('Predicted Label')\nax.axis('equal')","1d4f7b96":"params = {'n_estimators': [50, 100, 200, 400, 600, 800]}\ngrid_search = GridSearchCV(RandomForestClassifier(), param_grid=params,\n                                   scoring='accuracy', cv=5, n_jobs=-1)\ngrid_search.fit(Xbal, ybal)\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)","ed5136a7":"#r = pd.DataFrame()\n#r['x'] = [i for i in params.values()][0]\n#r['y'] = [i[1] for i in grid_search.cv_results_]\n#ax = r.plot(x='x', y='y', legend=False, linestyle='-', marker='o', figsize=(8,6))\n#ax.set_xlabel('n_estimators')\n#ax.set_ylabel('5-Fold Cross-Validation Score')","5276c0be":"grid_search.best_estimator_.fit(Xbal, ybal)\npredict = model.predict(Xte)","bd85b493":"fig, axes = plt.subplots(figsize=(15,9))\ncm = confusion_matrix(yte, predict).T\ncm = cm.astype('float')\/cm.sum(axis=0)\nax = sns.heatmap(cm, annot=True, cmap='Blues');\nax.set_xlabel('True Label')\nax.set_ylabel('Predicted Label')\nax.axis('equal')","68577a94":"r = pd.DataFrame(columns=['Feature','Importance'])\nncomp = 15\nr['Feature'] = feat_labels = Xbal.columns\nr['Importance'] = model.feature_importances_\nr.set_index(r['Feature'], inplace=True)\nax = r.sort_values('Importance', ascending=False)[:ncomp].plot.bar(width=0.9, legend=False, figsize=(15,8))\nax.set_ylabel('Relative Importance')","6ef5349d":"The cross-validation scores and ROC curves suggest the Logistic Regression is the best model, though the MNB and linear discriminant analysis models are pretty close behind. If we look at the confusion matrix, though, we see a big problem.The model can predict who are going to pay off the loan with a good accuracy of 99% but cannot predict who are going to default. The true positive rate of default (0 predicting 0) is almost 0. Since our main goal is to predict defaulter's, we have to do something about this. \n\nThe reason this is happening could be because of high imbalance in our dataset and the algorithm is putting everything into 1. We have to chose a new prediction threshold according to the sensitivity and specificity of the model. This will create some balance in predicting the binary outcome. Let's look at the plots below.","313ec73b":"The best model has 600 trees","862c4737":"It can be seen from the plot above that loan amount and installment have a very high correlation amongst each other (0.94). This is intuitive since a person who takes a large sum of loan would require extra time to repay it back. Also, interest rate, sub grade and grade have a very high correlation between them. This is obvious since interest rate is decided by grades once the grades are decided, a subgrade is assigned to that loan (leading to high correlation).\n\nLet's drop the three categories alongwith term and verification status(since it doesn't provide any valuable info) for further analysis.\n","796164a8":"We'll now scale the data so that each column has a mean of zero and unit standard deviation. Xunb (unbalanced set) and yunb are the independent and target variable.","534ad930":"# Model Selection\n\nWe are now ready to build some models. The following would be our approach for building and selecting the best model:\n\n1. Build a model on the imbalance dataset we got from data cleaning.\n2. Balance the dataset by using equal amount of default and 'fully paid' loans.\n\n### Trying the Unbalanced Dataset\n\nLet's first try the unbalanced dataset.The function below computes the receiver operating characteristic (ROC) curves for each of the models. This function will be called later in the analysis.","85800f68":"# Data Cleaning","b98d4cbe":"We will now encode the two categories listed above as 0 or 1 for our analysis. This will help us in predicting whether a person defaulted their loan or not. 0 means he deaulted and 1 means he paid off his loan.","c4b48def":"# Conclusion\nWe have successfully built an machine learning algorithm to predict the people who might default on their loans. This can be further used by LendingClub for their analysis. Also, we might want to look on other techniques or variables to improve the prediction power of the algorthm. One of the drawbacks is just the limited number of people who defaulted on their loan in the 8 years of data (2007-2015) present on the dataset. We can use an updated dataframe which consist next 3 years values (2015-2018) and see how many of the current loans were paid off or defaulted or even charged off. Then these new data points can be used for predicting them or even used to train the model again to improve its accuracy.\n\nSince we had a lot of categorical data, we cannot apply PCA for dimensionality reduction. Because of this, we can try some different type of variable selection method like 'MULTIPLE CORRESPONDENCE ANALYSIS' to reduce the dimensionality and select the most important variables from the columns.\n\nSince the algorithm puts around 47% of non-defaulters in the default class, we might want to look further into this issue to make the model more robust.","9b778dff":"That's great! The optimum threshold for the classifier have increased out models prediction power of Default (0). Even now the model doesn't provide a lot of prediction power and we have to train the model again using a different algorithm with some tweaks.\n\n## Part2 : Balancing the training dataset and creating a new model\nNow we will try to use a balanced dataset with equal amount of zeroes and 1's. The following part does the same.","6eb08d2c":"That's a significant improvement over the last model that we built using Logistic regression. \n\nLet's find the optimum number of estimators for this model and use that for prediction. This time we are going to use 5 fold cross validation.","84f8ac5f":"As you can see, there are a lot of columns which have huge chunk of data missing. These columns are not necessary for our \nanalysis. The following part will drop any columns where 80% or more data is missing. This will help us clean the Dataset \na little bit.","387e1a31":"### Packages\nWe will start by importing the packages that will be used throughout the analysis","681d746d":"As we can see, the score increases as we increase the number of estimators till 600 and then falls for 800 number of estimators.\n\nWe will use this to estimate to fit the model.","fed1e0fb":"Now splitting the data using scikitlearn's train_test_split and using 60% data for training and 40% for testing.","1d0f481f":"Interestingly, this gives us the same output as the previous model. Even now we have a good accuracy of 71% predicting defaluter's as defaulter's.\n\nSince random forest is based on decision trees, we can also plot the variable importance. Variable importance tells us which variable had highest importance when predicting an outcome.","ebdb7c99":"# Distribution of the loan status values\nLet us now see how the values in the status column are distributed. We will plot an histogram of values against count of times the status appears on the dataframe","acf67972":"# One Hot Encoding\nSince we have some categorical variables for the analysis and the machne learning algorithms doesn't take categorical and string variables directly, we have to creat dummy variables for them. We can either encode them using label encoder available for python, but it would be wrong in our analysis since a lot of these variables have multiple categories. Just using weights can cause discrepencies in the algorithm. Instead, we will one hot encode these so that we have a 1 wherever that category turns up and 0 otherwise. This will also create seperate columns for each level of category. Also, we'll be dropping one of the categories so that we have N-1 columns instead of N.","40b37083":"We will now plot the cross-validation scores, ROC curves and confusion matrix of random forest model. X axis is the true value and Y axis is the predicted value. ","8434b649":"Computing the ROC curves for the models and finding the true positive and false positives.","881b67df":"# Prediction of LendingClub loan defaulters\n#### -DEEPANSHU SHARMA\n\nThe dataset used for this analysis can be found using the following link: \nhttps:\/\/www.kaggle.com\/wendykan\/lending-club-loan-data\n\n## About LendingClub:\nLendingClub is a US peer-to-peer lending company, headquartered in San Francisco, California. It was the first peer-to-peer lender to register its offerings as securities with the Securities and Exchange Commission (SEC), and to offer loan trading on a secondary market. Lending Club is the world's largest peer-to-peer lending platform.The company claims that $ 15.98 billion in loans had been originated through its platform up to December 31, 2015.\n\nLending Club enables borrowers to create unsecured personal loans between $$1,000 and $ 40,000. The standard loan period is three years. Investors can search and browse the loan listings on Lending Club website and select loans that they want to invest in based on the information supplied about the borrower, amount of loan, loan grade, and loan purpose. Investors make money from interest. Lending Club makes money by charging borrowers an origination fee and investors a service fee.\n\n\n## About the Dataset\nThese files contain complete loan data for all loans issued through the 2007-2015, including the current loan status (Current, Late, Fully Paid, etc.) and latest payment information. The file containing loan data through the \"present\" contains complete loan data for all loans issued through the previous completed calendar quarter. Additional features include credit scores, number of finance inquiries, address including zip codes, and state, and collections among others. The file is a matrix of about 890 thousand observations and 75 variables. A data dictionary is provided in a separate file. k\n\n## Purpose of this analysis\nWe will go step by step for building a machine learning algorith for the prediction of loan defaulters based on certain variables present in the dataset. Our main goal is to correctly identifying defaulter's (True positives) so that lending club can decide whether a person is fit for sanctioning a loan or not in the future.\n","a1763cc7":"As you can see, interest rate followed by debt to income ratio,annual income, and loan amount are the most important features in predicting the defaulter's. Lending Club might want to use this as the metric for identifying people defaulting on their loans. ","4a9162f4":"Let's try some models on the train dataset With 3 fold cross validation. We are going to use the following 4 machine learning algorithms:\n1. Linear Discriminant Analysis\n2. Multinomial Naive Bayes\n3. Random Forest (tree based model)\n4. Logistic Regression","5f4d8d52":"As you can see, we have a lot of loans which are current with fair amount of fully paid loans. other categories (including) default have a really low number. This means the data is imbalanced and we might need to do something about this later in the analysis. For now we will drop all the columns except 'Fully Paid', 'Default' and 'Charged off'. We will also merge 'Charged off' and 'Default' together meaning that anyone who fell into this category defaulted their loan. The following two parts tries to implement this.","c97d755e":"Even though we almost got the same result as before, This time we are going to select Random Forst method and will try to find the optimal number of trees using the gridsearchcv and try to make the predition based on this and lets see if there is any improvements in predicting 0's","e83c468c":"### Finding the correlation between variables\nWe will now look at the correlation structure between our variables that we selected above. This will tell us about any dependencies between different variables and help us reduce the dimensionality a little bit more","43bfb7b9":"## Transformation\nBefore training the data, we would first transform the data to account for any skewness in the variable distribution. Various transformation techniques ranging from log transform to power transformation are available. For our analysis, we'll be using Box-cox transformation. It is used to modify the distributional shape of a set of data to be more normally distributed so that tests and confidence limits that require normality can be appropriately used.","4221b6ce":"Fitting LR to the test set.","2209a2dd":"Training the model on the balanced set","1d27a082":"The optimal threshold above is where the the two graphs meet.\n\n1. Sensitivity (also called the true positive rate, the recall, or probability of detection in some fields) measures the proportion of actual positives that are correctly identified as such (e.g., the percentage of sick people who are correctly identified as having the condition).\n\n2. Specificity (also called the true negative rate) measures the proportion of actual negatives that are correctly identified as such (e.g., the percentage of healthy people who are correctly identified as not having the condition)\n\nNow using this threshold for the model:","843f250c":"Now that we explored the whole dataframe easily, we will now select the columns that are necessary for our analysis.","4155e147":"Looks like Logistic regression provides the best estimate and almost all of the models giving the same results. Because of the issue of collinearity in LDA, we are going to ignore that.\n\nNow creating the test set for the analysis and scaling it."}}