{"cell_type":{"ae741eb6":"code","ef70c38b":"code","812ce05c":"code","a5e7fc5d":"code","78b1e77a":"code","512f97f5":"code","79db3f4e":"code","5bb37937":"code","6840989b":"code","863bcb58":"code","a5bba602":"code","f3d468a9":"code","bd83323f":"code","8af0a421":"code","41a23d2a":"code","a3271f61":"code","368fd419":"code","8ffae006":"code","fef15ef0":"code","3598fa54":"code","b6ffb8af":"code","871bcd69":"code","73569d06":"code","a9af4447":"code","a2f2dd5e":"code","f1a377ae":"code","f01ee30e":"code","b11a6fba":"code","3e7de9d7":"code","f5bee2cd":"code","196bb936":"code","c31bf2ea":"code","026f2089":"code","811aa728":"code","23ca8f36":"code","00e5c1a7":"code","ba1792fd":"code","608a1976":"code","33c36169":"code","4a749c1c":"code","d9e69316":"code","5d32a3e7":"markdown","ad826b45":"markdown","9751ca5d":"markdown","ab77a4a9":"markdown","fc15ae62":"markdown","3102c996":"markdown","e6ae9377":"markdown","9f458445":"markdown","e307bf82":"markdown","1ba98d0a":"markdown","48d29616":"markdown","674b4ab8":"markdown","cb25b580":"markdown","d495f391":"markdown","f4c52777":"markdown","e5edb31c":"markdown","c2b5ec7a":"markdown","05dcaba2":"markdown","c96fc8cf":"markdown","d7b97106":"markdown","bbdb4153":"markdown","6b52e089":"markdown","ae1cce0f":"markdown","9c6f6d84":"markdown"},"source":{"ae741eb6":"# Importing Data manipulation and plotting modules\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pydot\nfrom skimage import io","ef70c38b":"# Import Keras Libraries\nfrom keras.layers import Input, Dense\nfrom keras.layers import Conv1D, MaxPooling1D, Dropout, Flatten,Embedding, GRU\nfrom keras.layers.merge import concatenate\nfrom keras.models import Model\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","812ce05c":"# Import library for keras plotting model\nfrom keras.utils import  plot_model","a5e7fc5d":"# Import sklearn libraries\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split","78b1e77a":"# Importing Miscelaneous libraries\nimport os\nimport time","512f97f5":"pathToData = '..\/input\/'\nos.chdir(pathToData)\n# 2.2\ndata = pd.read_csv('..\/input\/mimic3d\/mimic3d.csv',\n\t               compression='infer',\n                   encoding=\"ISO-8859-1\"      # 'utf-8' gives error, hence the choice\n                  )\n\n\n# 2.3 Inspect the data\ndata.shape            # (58976, 28)","79db3f4e":"# Drop first column: 'hadm_id', being id column\ndata.drop(['hadm_id'], axis = 'columns' , inplace = True)\n\n\n# Check for missing values\ndata.isnull().values.sum()        # 10611\n\n# Check which columns have missing values\ndata.columns[data.isnull().sum()  > 0]    # Three: Index(['AdmitDiagnosis', 'religion', 'marital_status'], dtype='object')\n\n# Let us follow a conservative and safe approach to fill missing values\ndata.AdmitDiagnosis = data.AdmitDiagnosis.fillna(\"missing\")\ndata.religion = data.religion.fillna(\"missing\")\ndata.marital_status = data.marital_status.fillna(\"missing\")\ndata.isnull().values.sum()  ","5bb37937":"plt.figure(figsize=(15,10))\nsns.distplot(data['age'],hist=False)\nplt.title('Patient Age Distribution',fontsize=16)","6840989b":"plt.figure(figsize=(15,10))\nsns.distplot(data['LOSdays'],hist=False)\nplt.title('Patient Length Of stay Distribution',fontsize=16)","863bcb58":"# Divide data into train\/test\ndtrain,  dtest = train_test_split(data, test_size=0.33)\n\n\n# Check which columns are 'object'\nobj_columns = data.select_dtypes(include = ['object']).columns.values\nobj_columns\n","a5bba602":"# Check which columns have numeric data\nnum = data.select_dtypes(include = ['int64', 'float64']).columns.values\nnum\n\n\"\"\"\narray(['age', 'LOSdays', 'NumCallouts', 'NumDiagnosis', 'NumProcs',\n       'NumCPTevents', 'NumInput', 'NumLabs', 'NumMicroLabs', 'NumNotes',\n       'NumOutput', 'NumRx', 'NumProcEvents', 'NumTransfers',\n       'NumChartEvents', 'ExpiredHospital', 'TotalNumInteract',\n       'LOSgroupNum'], dtype=object)\n\nIInd column is target\nAnd columns 'ExpiredHospital', 'LOSgroupNum' are categorical. See below\n\n\"\"\"\n\n","f3d468a9":"# Levels in columns: 'ExpiredHospital', 'LOSgroupNum'\ndata.LOSgroupNum.value_counts()          # 4 levels\ndata.ExpiredHospital.value_counts()      # 2 levels","bd83323f":"for i in obj_columns:\n    print(i,len(data[i].value_counts()))\n    if(len(data[i].value_counts())<25):\n        data.groupby(data[i]).size().plot.bar()\n        plt.show()","8af0a421":"plt.figure(figsize=(12,10))\nsns.barplot(x=data.admit_type, y=data.age,data=data, hue=data.gender,palette='spring')\nplt.xlabel('Age',fontsize=16)\nplt.ylabel('Admit Type',fontsize=16)","41a23d2a":"# 4.4 Final seven obj_columns for One Hot Encoding\nobj_cols = [\"gender\", \"admit_type\", \"admit_location\", \"insurance\" ,\"marital_status\", 'ExpiredHospital', 'LOSgroupNum']\nohe = OneHotEncoder()\n# 4.4.1 Traing on dtrain\nohe = ohe.fit(dtrain[obj_cols])\n# 4.4.2 Transform train (dtrain) and test (dtest) data\ndtrain_ohe = ohe.transform(dtrain[obj_cols])\ndtest_ohe = ohe.transform(dtest[obj_cols])\n# 4.4.3\ndtrain_ohe.shape       # (39513, 34)\ndtest_ohe.shape        # (19463, 34)","a3271f61":"\n# 5.0 Label encode relegion and ethnicity\n# 5.1 First 'religion'\n\nle = LabelEncoder()\nle.fit(data[\"religion\"])                           # Train on full data else some labels may be absent in test data\ndtrain[\"re\"] = le.transform(dtrain['religion'])    # Create new column in dtrain\ndtest[\"re\"] = le.transform(dtest['religion'])      #   and in dtest\n\n# 5.2 Now 'ethnicity'\nle = LabelEncoder()\nle.fit(data[\"ethnicity\"])                          # train on full data\ndtrain[\"eth\"]= le.transform(dtrain['ethnicity'])   # Create new column in dtrain\ndtest[\"eth\"]= le.transform(dtest['ethnicity'])     #   and in dtest\n\n","368fd419":"\n# 6. Finally transform two obj_columns for tokenization\nte_ad = Tokenizer()\n# 6.1 Train tokenizer on train data ie 'dtrain'\nte_ad.fit_on_texts(data.AdmitDiagnosis.values)\n# 6.2 Transform both dtrain and dtest and create new columns\ndtrain[\"ad\"] = te_ad.texts_to_sequences(dtrain.AdmitDiagnosis)\ndtest[\"ad\"] = te_ad.texts_to_sequences(dtest.AdmitDiagnosis)\n\ndtrain.shape\ndtest.shape\n\n# 6.3 Similarly for column: AdmitProcedure\nte_ap = Tokenizer(oov_token='<unk>')\nte_ap.fit_on_texts(data.AdmitProcedure.values)\ndtrain[\"ap\"] = te_ap.texts_to_sequences(dtrain.AdmitProcedure)\ndtest[\"ap\"] = te_ap.texts_to_sequences(dtest.AdmitProcedure)\n\ndtrain.shape\ndtest.shape\n\n","8ffae006":"# dtrain[\"ad\"], dtest[\"ad\"]\n\nmaxlen_ad = 0\nfor i in dtrain[\"ad\"]:\n\tif maxlen_ad < len(i):\n\t\tmaxlen_ad = len(i)\n\nfor i in dtest[\"ad\"]:\n\tif maxlen_ad < len(i):\n\t\tmaxlen_ad = len(i)\n\nmaxlen_ad\n\n","fef15ef0":"# dtrain[\"ap\"], dtest[\"ap\"]\n\nmaxlen_ap = 0\nfor i in dtrain[\"ap\"]:\n\tif maxlen_ap < len(i):\n\t\tmaxlen_ap = len(i)\n\nfor i in dtest[\"ap\"]:\n\tif maxlen_ap < len(i):\n\t\tmaxlen_ap = len(i)\n\nmaxlen_ap\n","3598fa54":"# in dtrain[\"ad\"] and in dtest[\"ad\"]\n\none = np.max([np.max(i) for i in dtrain[\"ad\"].tolist() ])\ntwo = np.max([np.max(i) for i in dtest[\"ad\"].tolist() ])\nMAX_VOCAB_AD = np.max([one,two])\n\n# in dtrain[\"ap\"] and in dtest[\"ap\"]\n\none = np.max([np.max(i) for i in dtrain[\"ap\"].tolist() ])\ntwo = np.max([np.max(i) for i in dtest[\"ap\"].tolist() ])\nMAX_VOCAB_AP = np.max([one,two])\n\n# \nMAX_VOCAB_RE = len(dtrain.religion.value_counts())\nMAX_VOCAB_ETH = len(dtrain.ethnicity.value_counts())\n","b6ffb8af":"num = ['age', 'NumCallouts', 'NumDiagnosis', 'NumProcs',\n       'NumCPTevents', 'NumInput', 'NumLabs', 'NumMicroLabs', 'NumNotes',\n       'NumOutput', 'NumRx', 'NumProcEvents', 'NumTransfers',\n       'NumChartEvents', 'TotalNumInteract']","871bcd69":"# Standardize numerical data\nse = StandardScaler()\n# Train on dtrain\nse.fit(dtrain.loc[:,num])\n","73569d06":"# Then transform both dtrain and dtest\ndtrain[num] = se.transform(dtrain[num])\ndtest[num] = se.transform(dtest[num])\ndtest.loc[:,num].head(3)","a9af4447":"# Reshape train num data\ndtrain[num].values.shape\ndtr_reshape = dtrain[num].values.reshape(39513,15,1)","a2f2dd5e":"# Reshape test num data\ndtest[num].values.shape\ndts_reshape = dtest[num].values.reshape(19463,15,1)","f1a377ae":"# Training data\nXtr = {\n\t\"num\" : dtr_reshape,          # Note the name 'num'\n\t\"ohe\" : dtrain_ohe.toarray(),        # Note the name 'ohe'\n\t\"re\"  : dtrain[\"re\"].values,\n\t\"eth\" : dtrain[\"eth\"].values,\n\t\"ad\"  : pad_sequences(dtrain.ad, maxlen=maxlen_ad),\n\t\"ap\"  : pad_sequences(dtrain.ap, maxlen=maxlen_ap )\n      }","f01ee30e":"# Test data\nXte = {\n\t\"num\" : dts_reshape,\n\t\"ohe\" : dtest_ohe.toarray(),\n\t\"re\"  : dtest[\"re\"].values,\n\t\"eth\" : dtest[\"eth\"].values,\n\t\"ad\"  : pad_sequences(dtest.ad, maxlen=maxlen_ad ),\n\t\"ap\"  : pad_sequences(dtest.ap, maxlen=maxlen_ap )\n      }","b11a6fba":"# Just check shapes.\n# Total data features are now: 15 + 34 + 24 + 7 + 1 +1 = 82\n# Embeddings have thus generated new features.\nXtr[\"num\"].shape         # (39513, 15)\nXtr[\"ohe\"].shape         # (39513, 34)\nXtr[\"ad\"].shape          # (39513, 24)\nXtr[\"ap\"].shape          # (39513, 7)\nXtr[\"re\"].shape          # (39513,)  1D\nXtr[\"eth\"].shape         # (39513,)  1D","3e7de9d7":"# Design a simple model now\n\ndr_level = 0.1\n\n# 11.1\nnum = Input(\n                      shape= (Xtr[\"num\"].shape[1], 1 ),\n\t\t\t\t\t  name = \"num\"            # Name 'num' should be a key in the dictionary for numpy array input\n\t\t\t\t\t                          #    That is, this name should be the same as that of key in the dictionary\n\t\t\t\t\t  )\n\n# 11.2\nohe =   Input(\n                      shape= (Xtr[\"ohe\"].shape[1], ),\n\t\t\t\t\t  name = \"ohe\"\n\t\t\t\t\t  )\n\n# 11.3\nre =   Input(\n                      shape= [1],  # 1D shape or one feature\n\t\t\t\t\t  name = \"re\"\n\t\t\t\t\t  )\n# 11.4\neth =   Input(\n                      shape= [1],  # 1D shape or one feature\n\t\t\t\t\t  name = \"eth\"\n\t\t\t\t\t  )\n# 11.5\nad =   Input(\n                      shape= (Xtr[\"ad\"].shape[1], ),\n\t\t\t\t\t  name = \"ad\"\n\t\t\t\t\t  )\n# 11.6\nap =   Input(\n                      shape= (Xtr[\"ap\"].shape[1],),\n\t\t\t\t\t  name = \"ap\"\n\t\t\t\t\t  )\n","f5bee2cd":"# Embedding layers for each of the two of the columns with sequence data\nemb_ad  =      Embedding(MAX_VOCAB_AD+ 1 ,      32  )(ad )\nemb_ap  =      Embedding(MAX_VOCAB_AP+ 1 ,      32  )(ap)\n\n# Embedding layers for the two categorical variables\nemb_re  =      Embedding(MAX_VOCAB_RE+ 1 ,      32  )(re)\nemb_eth =      Embedding(MAX_VOCAB_ETH+ 1 ,      32  )(eth)\n\n# GRU layers for sequences\ngru_ad = GRU(16) (emb_ad)          # Output of GRU is a vector of size 8\ngru_ap = GRU(16) (emb_ap)","196bb936":"conv_out = Conv1D(32, kernel_size=2, activation='relu')(num)\nmp_num = MaxPooling1D(pool_size=2)(conv_out)\nnum_x = Flatten()(mp_num)\nnum_in = Flatten()(num)\nnum_final = concatenate([num_in,num_x])","c31bf2ea":"model = Model([num, ohe, re, eth, ad,ap], [gru_ad, gru_ap, emb_re, emb_eth, num_final, ohe])\nmodel.summary()","026f2089":"# Concatenate all outputs\nclass_l = concatenate([\n                      gru_ad,              # GRU output is already 1D\n                      gru_ap,\n                      num_final,           # 1D output. No need to flatten. Observe model summary\n                      ohe,                 # 1D output\n                      Flatten()(emb_re),   # Need to flatten. Observe model summary above\n                      Flatten()(emb_eth)\n                      ]\n                     )","811aa728":"# Add classification layer\nclass_l = Dense(64) (class_l)\nclass_l = Dropout(0.1)(class_l)\nclass_l = Dense(32) (class_l)\nclass_l = Dropout(0.1) (class_l)","23ca8f36":"# Output neuron. Activation is linear as our output is continous\noutput = Dense(1, activation=\"linear\") (class_l)\n","00e5c1a7":"# Formulate Model now\nmodel = Model(\n              inputs= [num, ohe, re, eth, ad, ap],\n              outputs= output\n             )\n","ba1792fd":"# \nmodel.summary()","608a1976":"# Model plot uisng keras plot_model()\nplt.figure(figsize = (14,14))\nplot_model(model, to_file = \"model.png\")\nio.imshow(\"model.png\")","33c36169":"# Compile model\nmodel.compile(loss=\"mse\",\n              optimizer=\"adam\",\n              metrics=[\"mae\"]\n\t\t\t  )","4a749c1c":"# 13.1\nBATCH_SIZE = 5000\nepochs = 20","d9e69316":"\n# \nstart = time.time()\nhistory= model.fit(Xtr,\n                   dtrain.LOSdays,\n                   epochs=epochs,\n                   batch_size=BATCH_SIZE,\n\t\t\t\t   validation_data=(Xte, dtest.LOSdays),\n\t\t\t\t   verbose = 1\n                  )\nend = time.time()\nprint((end-start)\/60)","5d32a3e7":"#**Mapping categorical levels to an embedding space**","ad826b45":"<font color=green>As we can see there are 6 inputs: Numeric (num), One hot encoded features (\"gender\", \"admit_type\", \"admit_location\", \"insurance\" ,\"marital_status\", \"ExpiredHospital\", \"LOSgroupNum\"), Religion (re), Ethnicity (eth), Admit Diagnosis (ad) and Admit procedure (ap). Their corresponding 6 outputs are concatenated and passed to a Dense Classification layer. This model will minimize the Mean Sqare Error (mse) significantly.<\/font>","9751ca5d":"#**Creating a model with 6 inputs and concatenating 6 respective outputs in a Dense layer**","ab77a4a9":"<font color=blue size = 4.8>Dropping redundant columns & filling in missing cells<\/font>","fc15ae62":"<font color=blue size = 3>Get max length of the sequences<\/font>","3102c996":"#**Compile Model**","e6ae9377":"This solution is designed to predict Length of Stay (LOS) of each patient, at time of admission, in hospital using LSTM & Convolution Neural Network. There can be significant variation of LOS as per various facilities, disease conditions and specialties even within the same healthcare system. Advanced LOS prediction at the time of admission can greatly enhance the quality of care as well as operational workload efficiency. Also greatly help with accurate planning for discharges resulting in lowering of various other quality measures such as readmissions.","9f458445":"#**Pre-process categorical features**","e307bf82":"<font color=blue size = 4.8>Import necessary libraries<\/font>","1ba98d0a":"#**Fitting model on Validation data**","48d29616":"#**Pass this concatenated input to Keras Model**","674b4ab8":"#**Pre-process numeric data 'num'**","cb25b580":"<font color=blue size = 4.8>Split data into -<\/font>\n1. Train & Test\n2. Categorical & Numeric","d495f391":"<font color=blue size = 4.8>Check number of levels in each categorical column<\/font>","f4c52777":"<font color=blue size = 3>One Hot Encoding list of categorical columns having levels < 10 (mentioned in below array 'obj_cols')<\/font>","e5edb31c":"<font color=blue size = 3>Label Encoding list of categorical columns having levels < 100<\/font>","c2b5ec7a":"<font color=blue size = 3>Get max vocabulary size i.e value of highest integer for categorical features<\/font>","05dcaba2":"#**Processing numeric data using Convolution network**","c96fc8cf":"#**Concatenate all inputs using a dictionary**","d7b97106":"<font color=blue size = 3>Tokenizing list of categorical columns having levels > 100<\/font>","bbdb4153":"<font color=blue size = 4.8>Loading dataset<\/font>","6b52e089":"<font color=blue size = 3>Excluding 'LOSdays' since it is target<\/font>","ae1cce0f":"#**Predict LOS using LSTM & CNN**","9c6f6d84":"MIMIC-III (Medical Information Mart for Intensive Care III) is a large, freely-available database comprising deidentified health-related data associated with over forty thousand patients. This dataset consists of numeric as well as categorical features. Before feeding the data to model and predict the LOS;we will process the numeric data further to extract features & convert categorical data to numbers. \nThis can be achieved with a few steps as listed below -\n- Pre-process categorical features using:\n    1. One Hot Encoding\n    2. Label Encoding\n    3. Tokenizer\n- Pre-process numeric data using convolution Neural Network for extracting features.\n- Concatenate all inputs using a dictionary.\n- Pass this concatenated input to Keras Model.\n- Compile the model\n- Build the Model"}}