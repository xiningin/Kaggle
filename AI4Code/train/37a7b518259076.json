{"cell_type":{"708ea1bd":"code","0d3ba147":"code","5fca9c47":"code","999cee2b":"code","eb7974ba":"code","397d1b7a":"code","771267e2":"code","79f932be":"code","7893b599":"code","e8a13dfe":"code","83a4d601":"code","3aa68219":"code","dd533f24":"code","6c08ce5f":"code","d41185bf":"code","12574c2f":"code","14ebeec7":"code","f6f4a922":"code","8f2f9c1a":"code","111b36b3":"code","418d39b9":"code","18c6ef89":"code","16dd94a8":"code","e3c9d65d":"code","c863e299":"code","1e6901e3":"code","5dada3e0":"code","a0536e5c":"code","cb9584b0":"code","117c1dfd":"code","828f022c":"code","e69e7dfa":"code","376c2b4a":"code","f3f8bb87":"code","391542ea":"code","9a233390":"code","c09f5b4a":"code","85df0cbf":"code","e896f012":"code","d502e368":"code","db6e8786":"code","e354a1c5":"code","1f26a301":"code","e610a1dd":"code","c80498eb":"code","a1a20f56":"code","fcd75455":"code","630159c9":"code","c591c79a":"code","b92d0066":"code","98647fc2":"code","dfa34ba2":"code","32d3efad":"code","d4ce9263":"code","6eaf8faa":"code","553dda15":"code","647c2078":"code","51ce4a96":"code","cc6a67f9":"code","ef3bd667":"code","742f4666":"code","f8a72981":"code","07238f11":"code","3383c95b":"code","f40702bf":"code","c41c27cd":"code","81b64e99":"code","32eba98b":"code","c6fbfb05":"code","48de8d46":"code","0cbf6d03":"code","ef2eb3cd":"code","9bffa976":"code","d6093c2f":"code","678a5881":"code","0917d763":"code","dd586005":"code","85218d3a":"code","c19864a9":"code","8dbf358e":"code","ccb66dad":"code","69d83c66":"code","413cd456":"code","6a77aeba":"code","fc1bb754":"code","4262e610":"code","e29c6c90":"code","f736a38c":"code","62da3024":"code","6de449c3":"code","9cc8bbf5":"code","07e4dfcd":"code","a6aeb464":"code","d6422d56":"code","2ac2cb7c":"code","4076f259":"code","26235131":"code","e99224bb":"code","59e4937c":"code","1738760a":"code","cadde5f9":"code","e35e36ce":"code","0969b163":"code","ca4f2340":"code","b51ed4b0":"code","fb184284":"code","fe476c42":"code","173f45b5":"code","e6e6f7cb":"code","1f78a886":"code","0c8f3f56":"code","6f463ccd":"code","018baae9":"code","0a396222":"code","b3fdf552":"code","e9de6450":"code","986b1aab":"code","7e2c89fa":"code","611af6d4":"code","a5c98e99":"code","51c627fc":"code","b4742341":"code","358e8f66":"code","883eee55":"code","de56e001":"code","5ed7e46e":"code","038751bd":"code","bd29b78e":"code","2a5ebe36":"code","afd45ce1":"code","32855b66":"code","9139097c":"code","6066f0c0":"code","5741609e":"code","6480c8c9":"code","aa6de54d":"code","624eba8a":"code","210aaf1f":"code","1d591626":"code","8365e771":"code","7bcd3bfe":"code","9c90374f":"code","945bd956":"code","e5a12299":"code","f3fa2056":"markdown","0493a703":"markdown","0f506571":"markdown","a607898f":"markdown","23c4b826":"markdown","a61f866e":"markdown","921ed6ad":"markdown","305d419b":"markdown","3c667fb3":"markdown","aa7e8618":"markdown","0bcdd2fb":"markdown","0db8f28c":"markdown","22d76cf2":"markdown","677ae59a":"markdown","036588b0":"markdown","d2f845e2":"markdown","e8f107b6":"markdown","1417751b":"markdown","6d5c6fe1":"markdown","04fd3241":"markdown","9f4f5568":"markdown","9beb1636":"markdown","627e11ce":"markdown","e32297c0":"markdown","aa63a9f0":"markdown","5f328b27":"markdown","a03b9629":"markdown","bebb4c1d":"markdown","f0e0c185":"markdown","17ed1d10":"markdown","ebf22c10":"markdown"},"source":{"708ea1bd":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nprint(os.listdir(\"..\/input\"))","0d3ba147":"aq=pd.read_csv('..\/input\/india-air-quality-data\/data.csv',encoding=\"ISO-8859-1\")\naq.tail(5)\n#Data from years 1987-2015","5fca9c47":"aq.shape","999cee2b":"# Extracting Tamil Nadu state data alone\ntn = aq.query('state==\"Tamil Nadu\" ')\ntn.sample(2)","eb7974ba":"tn.shape","397d1b7a":"tn.describe(include = 'all')","771267e2":"tn.drop(labels=['stn_code','sampling_date','agency','location_monitoring_station'], axis = 1, inplace = True)\ntn.sample(2)","79f932be":"tn.isnull().sum()","7893b599":"tn.drop(labels = ['pm2_5'], axis =1, inplace = True)\ntn.head(2)","e8a13dfe":"tn.dtypes","83a4d601":"# To sort based on dates, the date should be of \"datetime\" datatype. \n#So converting \"object\" data type to \"datetime\" datatype","3aa68219":"tn['date'] = pd.to_datetime(tn.date,format='%Y-%m-%d')\ntn.info()","dd533f24":"tn.sort_values(by='date')","6c08ce5f":"tn['so2'].fillna(method='ffill',inplace = True);\ntn['no2'].fillna(method='ffill',inplace = True);\ntn['rspm'].fillna(method='ffill',inplace = True);\ntn['spm'].fillna(method='ffill',inplace = True);","d41185bf":"tn.isnull().sum()","12574c2f":"# Even after replacement, we have 1636 missing values in rspm\nprint(tn.iloc[[1634]],tn.iloc[[1635]],tn.iloc[[1636]],tn.iloc[[1636]])","14ebeec7":"# This means rspm has not been calculated till 2002. It has been measured only from 2004 onwards.\n# We can either omit it or seperate the data set into two. That is before 2004 and after 2004.\n# Here for simplicity, I am deleting the column of rspm","f6f4a922":"tn.drop(labels = ['rspm'], axis = 1, inplace = True)\ntn.head()","8f2f9c1a":"tn.isnull().sum()","111b36b3":"# Dealing with 354 missing values of type","418d39b9":"typ=sns.countplot(x =\"type\",data = tn)\ntyp.set_xticklabels(typ.get_xticklabels(), rotation=90);","18c6ef89":"tn['type'].replace(\"Industrial Areas\",\"Industrial\",inplace=True)\ntn['type'].replace(\"Industrial Area\",\"Industrial\",inplace=True)\ntn['type'].replace(\"Residential and others\",\"Residential\",inplace=True)\ntn['type'].replace(\"Residential, Rural and other Areas\",\"Residential\",inplace=True)","16dd94a8":"typ=sns.countplot(x =\"type\",data = tn)\ntyp.set_xticklabels(typ.get_xticklabels(), rotation=90);","e3c9d65d":"datacount_ty =sns.countplot(x =\"location\",hue = 'type',data = tn);\ndatacount_ty.set_xticklabels(datacount_ty.get_xticklabels(), rotation=90);","c863e299":"# Rows with missing \"types\"\nnull_data = tn[tn.isnull().any(axis=1)]\nnull_data.head(20)","1e6901e3":"# Converting NaN to zeros\n#df['DataFrame Column'] = df['DataFrame Column'].replace(np.nan, 0)\ntn['type'] = tn['type'].replace(np.nan, \"Residential\")","5dada3e0":"tn.isnull().sum()","a0536e5c":"#Finding hidden missing values. (i.e. zeros)","cb9584b0":"aaa = (tn == 0).astype(int).sum(axis=0)\nprint(aaa)","117c1dfd":"# Also we can see the \"locations\" repeated.\n# Madras - Chennai, # Turicorin-Tuticorin\n# Replacing them into single value","828f022c":"tn['location'].replace(\"Turicorin\",\"Tuticorin\",inplace=True)\ntn['location'].replace(\"Madras\",\"Chennai\",inplace=True)","e69e7dfa":"datacount_ty =sns.countplot(x =\"location\",hue = 'type',data = tn);\ndatacount_ty.set_xticklabels(datacount_ty.get_xticklabels(), rotation=90);","376c2b4a":"tn.head()","f3f8bb87":"datacount =sns.countplot(x =\"location\",data = tn);\ndatacount.set_xticklabels(datacount.get_xticklabels(), rotation=90);","391542ea":"loc = pd.pivot_table(tn, values=['so2','no2','spm'],index='location') # Aggfunc: default-np.mean()\nloc","9a233390":"maxso2 = loc.sort_values(by='so2',ascending=False)\nmaxso2.loc[:,['so2']].head(10).plot(kind='bar'); # Based on average values","c09f5b4a":"maxno2 = loc.sort_values(by='no2',ascending=False);\nmaxno2.loc[:,['no2']].head(10).plot(kind='bar');","85df0cbf":"maxspm = loc.sort_values(by='spm',ascending=False);\nmaxspm.loc[:,['spm']].head(10).plot(kind='bar');","e896f012":"def calculate_si(so2):\n    si=0\n    if (so2<=40):\n     si= \"s1\"\n    if (so2>40 and so2<=80):\n     si= \"s2\"\n    if (so2>80 and so2<=380):\n     si= \"s3\"\n    if (so2>380 and so2<=800):\n     si= \"s4\"\n    if (so2>800 and so2<=1600):\n     si= \"s5\"\n    if (so2>1600):\n     si= \"s6\"\n    return si\ntn['si']=tn['so2'].apply(calculate_si)\nds= tn[['so2','si']]\nds.tail()","d502e368":"def calculate_ni(no2):\n    ni=0\n    if (no2<=40):\n     ni= \"n1\"\n    if (no2>40 and no2<=80):\n     ni= \"n2\"\n    if (no2>80 and no2<=180):\n     ni= \"n3\"\n    if (no2>180 and no2<=280):\n     ni= \"n4\"\n    if (no2>280 and no2<=400):\n     ni= \"n5\"\n    if (no2>400):\n     ni= \"n6\"\n    return ni\ntn['ni']=tn['no2'].apply(calculate_ni)\ndn= tn[['no2','ni']]\ndn.tail()","db6e8786":"def calculate_spi(spm):\n    spi=0\n    if (spm<=40):\n      spi= \"sp1\"\n    if (spm>40 and spm<=80):\n      spi= \"sp2\"\n    if (spm>80 and spm<=180):\n      spi= \"sp3\"\n    if (spm>180 and spm<=280):\n      spi= \"sp4\"\n    if (spm>280 and spm<=400):\n      spi= \"sp5\"\n    if (spm>400):\n      spi= \"sp6\"\n    return  spi\ntn['spi']=tn['spm'].apply(calculate_spi)\ndsp= tn[['spm','spi']]\ndsp.tail()","e354a1c5":"tn.sample(2)","1f26a301":"# AQI\ndef calculate_aqi(si,ni,spi):\n    aqi=0\n    if(si>ni and si>spi):\n     aqi=si\n    if (spi>ni and spi>si):\n     aqi=spi\n    if(ni>si and ni>spi):\n     aqi= ni\n    return aqi\ntn['AQI']=tn.apply(lambda x:calculate_aqi(x['so2'],x['no2'],x['spm']),axis=1)","e610a1dd":"tn.head()","c80498eb":"aq_wise = pd.pivot_table(tn, values=['AQI'],index='location')\naq_wise","a1a20f56":"maxaqi = aq_wise.sort_values(by='AQI',ascending=False)\nmaxaqi.loc[:,['AQI']].head(37).plot(kind='bar')","fcd75455":"date_wise = pd.pivot_table(tn, values=['AQI'],index='date')\ndate_wise","630159c9":"date_wise.loc[:,['AQI']].head(30).plot(kind='bar')","c591c79a":"dum1 = pd.get_dummies(tn['type'])\ndum2 = pd.get_dummies(tn['location'])\ntn['year'] = tn['date'].dt.year","b92d0066":"td = pd.concat([tn, dum1, dum2], axis = 1)\ntd.head()","98647fc2":"td.drop(labels = ['state','location','type','so2','no2','spm','si','ni','spi','date'], axis = 1, inplace = True)\ntd.sample(2)","dfa34ba2":"td.corr()","32d3efad":"yr_wise = pd.pivot_table(td, values=['AQI'],index='year')\nyr_wise.loc[:,['AQI']].head(30).plot(kind='bar')","d4ce9263":"from sklearn.model_selection import train_test_split","6eaf8faa":"X=td.drop(\"AQI\",axis=1)\ny=td[\"AQI\"]","553dda15":"X_train,X_test,y_train,y_test= train_test_split(X,y,test_size=0.30,random_state=25)","647c2078":"from sklearn.linear_model import LinearRegression","51ce4a96":"lin_mod = LinearRegression()\nlin_mod.fit(X_train, y_train)","cc6a67f9":"lin_mod.score(X_train, y_train )","ef3bd667":"lin_mod.score(X_test, y_test)","742f4666":"# Less Score. Underfitting","f8a72981":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn import linear_model\n\npoly = PolynomialFeatures(degree=2, interaction_only=True)\nX_train2 = poly.fit_transform(X_train)\nX_test2 = poly.fit_transform(X_test)\n\npoly_clf = linear_model.LinearRegression()\n\npoly_clf.fit(X_train2, y_train)\n\ny_pred = poly_clf.predict(X_test2)","07238f11":"print(poly_clf.score(X_train2, y_train))","3383c95b":"print(poly_clf.score(X_test2, y_test))","f40702bf":"# Trying with higher degrees","c41c27cd":"poly = PolynomialFeatures(degree=3, interaction_only=True)\nX_train2 = poly.fit_transform(X_train)\nX_test2 = poly.fit_transform(X_test)\n\npoly_clf = linear_model.LinearRegression()\n\npoly_clf.fit(X_train2, y_train)\n\ny_pred = poly_clf.predict(X_test2)\nprint(poly_clf.score(X_train2, y_train))\nprint(poly_clf.score(X_test2, y_test))","81b64e99":"# degree = 3 has less scores than degree = 2","32eba98b":"poly = PolynomialFeatures(degree=4, interaction_only=True)\nX_train2 = poly.fit_transform(X_train)\nX_test2 = poly.fit_transform(X_test)\n\npoly_clf = linear_model.LinearRegression()\n\npoly_clf.fit(X_train2, y_train)\n\ny_pred = poly_clf.predict(X_test2)\nprint(poly_clf.score(X_train2, y_train))\nprint(poly_clf.score(X_test2, y_test))","c6fbfb05":"# Nearly score to degree = 2. But still less than degree = 2","48de8d46":"poly = PolynomialFeatures(degree=5, interaction_only=True)\nX_train2 = poly.fit_transform(X_train)\nX_test2 = poly.fit_transform(X_test)\n\npoly_clf = linear_model.LinearRegression()\n\npoly_clf.fit(X_train2, y_train)\n\ny_pred = poly_clf.predict(X_test2)\nprint(poly_clf.score(X_train2, y_train))\nprint(poly_clf.score(X_test2, y_test))","0cbf6d03":"# Score reduces as degree increases","ef2eb3cd":"poly = PolynomialFeatures(degree=6, interaction_only=True)\nX_train2 = poly.fit_transform(X_train)\nX_test2 = poly.fit_transform(X_test)\n\npoly_clf = linear_model.LinearRegression()\n\npoly_clf.fit(X_train2, y_train)\n\ny_pred = poly_clf.predict(X_test2)\nprint(poly_clf.score(X_train2, y_train))\nprint(poly_clf.score(X_test2, y_test))","9bffa976":"# Same score as prev degree. \n#Underfitting","d6093c2f":"from sklearn.neighbors import KNeighborsRegressor\nfrom scipy.stats import zscore","678a5881":"XScaled = X.apply(zscore)","0917d763":"NNH = KNeighborsRegressor(n_neighbors = 27, metric = 'euclidean')","dd586005":"NNH.fit(X_train,y_train)","85218d3a":"predicted_labels = NNH.predict(X_test)\nprint(NNH.score(X_train, y_train))\nprint(NNH.score(X_test,y_test))","c19864a9":"NNH = KNeighborsRegressor(n_neighbors = 30) # default metric = 'minkowski'\nNNH.fit(X_train,y_train)\npredicted_labels = NNH.predict(X_test)\nprint(NNH.score(X_train, y_train))\nprint(NNH.score(X_test,y_test))","8dbf358e":"NNH = KNeighborsRegressor(n_neighbors = 55)\nNNH.fit(X_train,y_train)\npredicted_labels = NNH.predict(X_test)\nprint(NNH.score(X_train, y_train))\nprint(NNH.score(X_test,y_test))","ccb66dad":"NNH = KNeighborsRegressor(n_neighbors = 70)\nNNH.fit(X_train,y_train)\npredicted_labels = NNH.predict(X_test)\nprint(NNH.score(X_train, y_train))\nprint(NNH.score(X_test,y_test))","69d83c66":"# if we increase n_neigbours more than 55, \n#train fitting increases but test fit decreases. So 55 is the optimum one","413cd456":"NNH = KNeighborsRegressor(n_neighbors = 55, metric = 'euclidean')\nNNH.fit(X_train,y_train)\npredicted_labels = NNH.predict(X_test)\nprint(NNH.score(X_train, y_train))\nprint(NNH.score(X_test,y_test))","6a77aeba":"# Better result dan \"minskowki\"","fc1bb754":"NNH = KNeighborsRegressor(n_neighbors = 35, metric = 'euclidean')\nNNH.fit(X_train,y_train)\npredicted_labels = NNH.predict(X_test)\nprint(NNH.score(X_train, y_train))\nprint(NNH.score(X_test,y_test))","4262e610":"NNH = KNeighborsRegressor(n_neighbors = 30, metric = 'euclidean')\nNNH.fit(X_train,y_train)\npredicted_labels = NNH.predict(X_test)\nprint(NNH.score(X_train, y_train))\nprint(NNH.score(X_test,y_test))","e29c6c90":"NNH = KNeighborsRegressor(n_neighbors = 35, metric = 'manhattan')\nNNH.fit(X_train,y_train)\npredicted_labels = NNH.predict(X_test)\nprint(NNH.score(X_train, y_train))\nprint(NNH.score(X_test,y_test))","f736a38c":"# Better than euclidean","62da3024":"NNH = KNeighborsRegressor(n_neighbors = 45, metric = 'manhattan')\nNNH.fit(X_train,y_train)\npredicted_labels = NNH.predict(X_test)\nprint(NNH.score(X_train, y_train))\nprint(NNH.score(X_test,y_test))","6de449c3":"NNH = KNeighborsRegressor(n_neighbors = 55, metric = 'manhattan')\nNNH.fit(X_train,y_train)\npredicted_labels = NNH.predict(X_test)\nprint(NNH.score(X_train, y_train))\nprint(NNH.score(X_test,y_test))","9cc8bbf5":"# 45 is optimum","07e4dfcd":"# Therefore best solution is for \nNNH = KNeighborsRegressor(n_neighbors = 45, metric = 'manhattan')\nNNH.fit(X_train,y_train)\npredicted_labels = NNH.predict(X_test)\nprint(NNH.score(X_train, y_train))\nprint(NNH.score(X_test,y_test))","a6aeb464":"from sklearn import svm\nfrom sklearn.svm import SVR","d6422d56":"reg= svm.SVR(kernel='rbf',gamma='auto', C=2)\nreg.fit(X_train,y_train)","2ac2cb7c":"predicted_labels = reg.predict(X_test)\nprint(reg.score(X_train,y_train))\nprint(reg.score(X_test,y_test))","4076f259":"# Score is less than KNN. Trying with other \"C\"","26235131":"reg= svm.SVR(kernel='rbf',gamma='auto', C=150)\nreg.fit(X_train,y_train)\npredicted_labels = reg.predict(X_test)\nprint(reg.score(X_train,y_train))\nprint(reg.score(X_test,y_test))","e99224bb":"reg= svm.SVR(kernel='rbf',gamma='auto', C=160)\nreg.fit(X_train,y_train)\npredicted_labels = reg.predict(X_test)\nprint(reg.score(X_train,y_train))\nprint(reg.score(X_test,y_test))","59e4937c":"reg= svm.SVR(kernel='rbf',gamma='auto', C=163)\nreg.fit(X_train,y_train)\npredicted_labels = reg.predict(X_test)\nprint(reg.score(X_train,y_train))\nprint(reg.score(X_test,y_test))","1738760a":"# as C increases after 160, score training score increases but test score decreases.","cadde5f9":"reg= svm.SVR(kernel='sigmoid',gamma='auto', C=80)\nreg.fit(X_train,y_train)\npredicted_labels = reg.predict(X_test)\nprint(reg.score(X_train,y_train))\nprint(reg.score(X_test,y_test))","e35e36ce":"# Using poly kernel takes lot of time to run","0969b163":"# Optimum value for SVM is\nreg= svm.SVR(kernel='rbf',gamma='auto', C=160)\nreg.fit(X_train,y_train)\npredicted_labels = reg.predict(X_test)\nprint(reg.score(X_train,y_train))\nprint(reg.score(X_test,y_test))","ca4f2340":"from sklearn.tree import DecisionTreeRegressor","b51ed4b0":"dTree= DecisionTreeRegressor(criterion='mse',splitter='best',random_state=25,max_depth=5)","fb184284":"dTree.fit(X_train,y_train)","fe476c42":"print(dTree.score(X_train,y_train)) \nprint(dTree.score(X_test,y_test))","173f45b5":"dTree= DecisionTreeRegressor(criterion='mse',splitter='best',random_state=25,max_depth=14)\ndTree.fit(X_train,y_train)\nprint(dTree.score(X_train,y_train)) \nprint(dTree.score(X_test,y_test))","e6e6f7cb":"# No improvements in score after \"max_depth = 14\"\n# Trying with different criteria","1f78a886":"dTree= DecisionTreeRegressor(criterion='mae',splitter='best',random_state=25,max_depth=20)\ndTree.fit(X_train,y_train)\nprint(dTree.score(X_train,y_train)) \nprint(dTree.score(X_test,y_test))","0c8f3f56":"dTree= DecisionTreeRegressor(criterion='friedman_mse',splitter='best',random_state=25,max_depth=15)\ndTree.fit(X_train,y_train)\nprint(dTree.score(X_train,y_train)) \nprint(dTree.score(X_test,y_test))","6f463ccd":"# friedman_mse same as mse","018baae9":"# Optimum is \ndTree= DecisionTreeRegressor(criterion='mse',splitter='best',random_state=25,max_depth=14)\ndTree.fit(X_train,y_train)\nprint(dTree.score(X_train,y_train)) \nprint(dTree.score(X_test,y_test))","0a396222":"dTree= DecisionTreeRegressor(criterion='mse',splitter='best',random_state=25,max_depth=14)\ndTree.fit(X_train,y_train)\ndTree_tr=dTree.score(X_train,y_train)\ndTree_ts=dTree.score(X_test,y_test)","b3fdf552":"from sklearn.ensemble import BaggingRegressor","e9de6450":"bgr= BaggingRegressor (n_estimators=9,base_estimator=dTree,random_state=25)\nbgr=bgr.fit(X_train,y_train)\nprint(bgr.score(X_train,y_train))\nprint(bgr.score(X_test,y_test))","986b1aab":"# trying with different \"n_estimators\"","7e2c89fa":"bgr= BaggingRegressor (n_estimators=12,base_estimator=dTree,random_state=25)\nbgr=bgr.fit(X_train,y_train)\nprint(bgr.score(X_train,y_train))\nprint(bgr.score(X_test,y_test))","611af6d4":"# Increase in \"n_estimators\" increases train score but decreases test score. \n#so \"n_estimators = 9\" is good","a5c98e99":"from sklearn.ensemble import AdaBoostRegressor","51c627fc":"adr= AdaBoostRegressor (n_estimators=5,random_state=25, loss ='linear') # loss = 'linear' is default\nadr=adr.fit(X_train,y_train)\nprint(adr.score(X_train,y_train))\nprint(adr.score(X_test,y_test))","b4742341":"# trying with different \"n_estimators\"","358e8f66":"adr= AdaBoostRegressor (n_estimators=15,random_state=25,loss ='linear')\nadr=adr.fit(X_train,y_train)\nprint(adr.score(X_train,y_train))\nprint(adr.score(X_test,y_test))","883eee55":"# Increase in \"n_estimators\" increases train score but decreases test score. \n#so \"n_estimators = 5\" is good","de56e001":"adr= AdaBoostRegressor (n_estimators=7,random_state=25,loss ='square')\nadr=adr.fit(X_train,y_train)\nprint(adr.score(X_train,y_train))\nprint(adr.score(X_test,y_test))","5ed7e46e":"adr= AdaBoostRegressor (n_estimators=5,random_state=25,loss ='exponential')\nadr=adr.fit(X_train,y_train)\nprint(adr.score(X_train,y_train))\nprint(adr.score(X_test,y_test))","038751bd":"from sklearn.ensemble import GradientBoostingRegressor","bd29b78e":"gbr= GradientBoostingRegressor (n_estimators=10,random_state=25)\ngbr=gbr.fit(X_train,y_train)\nprint(gbr.score(X_train,y_train))\nprint(gbr.score(X_test,y_test))","2a5ebe36":"# trying with different \"n_estimators\"","afd45ce1":"gbr= GradientBoostingRegressor (n_estimators=400,random_state=25)\ngbr=gbr.fit(X_train,y_train)\nprint(gbr.score(X_train,y_train))\nprint(gbr.score(X_test,y_test))","32855b66":"gbr= GradientBoostingRegressor (n_estimators=410,random_state=25)\ngbr=gbr.fit(X_train,y_train)\nprint(gbr.score(X_train,y_train))\nprint(gbr.score(X_test,y_test))","9139097c":"# Increase in \"n_estimators\" beyond 400, increases train score but decreases test score. so \"n_estimators = 400\" is good","6066f0c0":"# Optimum is \ngbr= GradientBoostingRegressor (n_estimators=400,random_state=25)\ngbr=gbr.fit(X_train,y_train)\nprint(gbr.score(X_train,y_train))\nprint(gbr.score(X_test,y_test))","5741609e":"gbr= GradientBoostingRegressor (n_estimators=400,random_state=25)\ngbr=gbr.fit(X_train,y_train)\ngbr_tr= gbr.score(X_train,y_train)\ngbr_ts= gbr.score(X_test,y_test)","6480c8c9":"from sklearn.ensemble import RandomForestRegressor","aa6de54d":"rfr= RandomForestRegressor (n_estimators=10,random_state=25,max_features=5)\nrfr=rfr.fit(X_train,y_train)\nprint(rfr.score(X_train,y_train))\nprint(rfr.score(X_test,y_test))","624eba8a":"# trying with different \"n_estimators\"","210aaf1f":"rfr= RandomForestRegressor (n_estimators=11,random_state=25,max_features=5)\nrfr=rfr.fit(X_train,y_train)\nprint(rfr.score(X_train,y_train))\nprint(rfr.score(X_test,y_test))","1d591626":"# No effect","8365e771":"# trying with different \"max_features\"","7bcd3bfe":"rfr= RandomForestRegressor (n_estimators=10,random_state=25,max_features=10)\nrfr=rfr.fit(X_train,y_train)\nprint(rfr.score(X_train,y_train))\nprint(rfr.score(X_test,y_test))","9c90374f":"rfr= RandomForestRegressor (n_estimators=10,random_state=25,max_features=10)\nrfr=rfr.fit(X_train,y_train)\nrfr_tr = rfr.score(X_train,y_train)\nrfr_ts = rfr.score(X_test,y_test)","945bd956":"# the above one is optimum","e5a12299":"score_res = pd.DataFrame({'Model':['DecisionTree','GradientBoosting','RandomForest'],\n                          'Train Score':[dTree_tr, gbr_tr, rfr_tr],\n                         'Test Score':[dTree_ts, gbr_ts, rfr_ts]\n                         })\nscore_res","f3fa2056":"\"year\" has good correlation with \"AQI\" when compared to others","0493a703":"# Calculating AQI","0f506571":"# Training Data","a607898f":"### SVM","23c4b826":"Score Better than Linear Regression models. Trying with different n_neighbours","a61f866e":"### Polynomial Regression ","921ed6ad":"Very near to Decision Tree.\n\nScore of Decision Tree\n\ntrain - 0.7320163141352926\ntest - 0.7764637553626321","305d419b":"##### Removing unnecessary datas","3c667fb3":"35 is the optimum one","aa7e8618":"### AdaBoost","0bcdd2fb":"Not as good as Decision Tree","0db8f28c":"# Feature Engineering","22d76cf2":"Score Very similar to Decision Tree, Gradient Boosting\n\n","677ae59a":"### Simple Linear Regression","036588b0":"pm2_5 has almost 97% data missing. So omitting pm2_5 column","d2f845e2":"Better than KNN","e8f107b6":"### Decision Tree","1417751b":"But not as good as KNN","6d5c6fe1":"# Data Visualization","04fd3241":"Here we have repetition of types, so replacing all to unique types","9f4f5568":"In order to fill the missing values, the values are first need to be sorted in Chronological order","9beb1636":"### Gradient Boosting","627e11ce":"# Model fittings","e32297c0":"### Random Forest","aa63a9f0":"Not good as Decision Tree","5f328b27":"Trying with different \"max_depth\"","a03b9629":"Trying with different \"metrics\"","bebb4c1d":"### K-Nearest Neighbour","f0e0c185":"Mode is higher for residential. So filling the missing 354 values in type by \"Residential\" type","17ed1d10":"# Therefore the models which perform well are","ebf22c10":"### Bagging"}}