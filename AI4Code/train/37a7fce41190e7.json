{"cell_type":{"74970f08":"code","8632b778":"code","31de9819":"code","7b99943c":"code","a332caee":"code","96cea2d2":"code","6052b47a":"code","9d242901":"code","9fc31fee":"code","93ecbb99":"code","a40609a2":"code","04d9b5f3":"code","4d8af00c":"code","6f0f0c77":"code","5e672673":"code","a8292ccd":"markdown","1771f184":"markdown","77aac391":"markdown","66d167d9":"markdown","bb62c208":"markdown","6a0f0fdb":"markdown","e461bde9":"markdown","01d456d0":"markdown"},"source":{"74970f08":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport warnings\n\n\nwarnings.simplefilter(\"ignore\")\n\nX = pd.read_csv('\/kaggle\/input\/prepared-data\/x_train.csv')\nX_test = pd.read_csv('\/kaggle\/input\/prepared-data\/x_test.csv')\ny = pd.read_csv('\/kaggle\/input\/prepared-data\/y_train.csv')\n\nprint('X shape: ', X.shape)\nprint('X_test shape: ', X_test.shape)\nprint('y shape: ', y.shape)\nprint(X.head())\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=42)","8632b778":"from xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nimport optuna\nfrom optuna.visualization import plot_optimization_history, plot_param_importances\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\n\ndef scorer(y, y_pred):\n    return roc_auc_score(y, y_pred)","31de9819":"# XGBClassifier\nxgbc_model = XGBClassifier(min_child_weight=0.1, reg_lambda=100, booster='gbtree', objective='binary:logitraw', random_state=42)\nxgbc_score = cross_val_score(xgbc_model, train_X, train_y, scoring='roc_auc', cv=5)\nprint('xgbc_score: ', xgbc_score.mean())\n\n# LGBMClassifier\nligthgbmc_model = LGBMClassifier(boosting_type='gbdt', objective='binary', random_state=42)\nligthgbmc_score = cross_val_score(ligthgbmc_model, train_X, train_y, scoring='roc_auc', cv=5)\nprint('ligthgbmc_score: ', ligthgbmc_score.mean())\n\n# CatBoostClassifier\ncbc_model = CatBoostClassifier(loss_function='Logloss', random_state=42, verbose=False)\ncbc_score = cross_val_score(cbc_model, train_X, train_y, scoring='roc_auc', cv=5)\nprint('cbc_score: ', cbc_score.mean())","7b99943c":"def objective(trial, data=X, target=y):\n    X_train, X_val, y_train, y_val = train_test_split(data, target, test_size=0.2, random_state=42)\n\n    params = {\n        'max_depth': trial.suggest_int('max_depth', 3, 32),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.005, 0.02, 0.05, 0.08, 0.1]),\n        'n_estimators': trial.suggest_int('n_estimators', 2000, 8000),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n        'gamma': trial.suggest_float('gamma', 0.0001, 1.0, log = True),\n        'alpha': trial.suggest_float('alpha', 0.0001, 10.0, log = True),\n        'lambda': trial.suggest_float('lambda', 0.0001, 10.0, log = True),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.8),\n        'subsample': trial.suggest_float('subsample', 0.1, 0.8),\n        'tree_method': 'gpu_hist',\n        'booster': 'gbtree',\n        'random_state': 42,\n        'use_label_encoder': False,\n        'eval_metric': 'auc'\n\n    }\n    \n    model = XGBClassifier(**params)  \n    model.fit(X_train, y_train, eval_set = [(X_val,y_val)], early_stopping_rounds = 333, verbose = False)\n    y_pred = model.predict_proba(X_val)[:,1]\n    roc_auc = roc_auc_score(y_val, y_pred)\n\n    return roc_auc","a332caee":"#study = optuna.create_study(direction='maximize')\n#study.optimize(objective, n_trials=50)\n#print('Number of finished trials: ', len(study.trials))\n#print('Best trial: ', study.best_trial.params)\n#print('Best value: ', study.best_value)","96cea2d2":"def objective(trial, data=X, target=y):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n    params = {\n        'max_depth': trial.suggest_int('max_depth', 3, 64),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.005, 0.02, 0.05, 0.08, 0.1]),\n        'n_estimators': trial.suggest_int('n_estimators', 2000, 8000),\n        'max_bin': trial.suggest_int('max_bin', 200, 400),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 300),\n        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.0001, 1.0, log = True),\n        'subsample': trial.suggest_float('subsample', 0.1, 0.8),\n        'random_seed': 42,\n        'task_type': 'GPU',\n        'loss_function': 'Logloss',\n        'eval_metric': 'AUC',\n        'bootstrap_type': 'Poisson'\n    }\n    \n    model = CatBoostClassifier(**params)  \n    model.fit(X_train, y_train, eval_set = [(X_val,y_val)], early_stopping_rounds = 222, verbose = False)\n    y_pred = model.predict_proba(X_val)[:,1]\n    roc_auc = roc_auc_score(y_val, y_pred)\n\n    return roc_auc","6052b47a":"#study = optuna.create_study(direction = 'maximize')\n#study.optimize(objective, n_trials = 50)\n#print('Number of finished trials:', len(study.trials))\n#print('Best trial:', study.best_trial.params)\n#print('Best value:', study.best_value)","9d242901":"def objective(trial,data=X,target=y):   \n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.15,random_state=42)\n    params = {\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 10.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 11, 333),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'max_depth': trial.suggest_int('max_depth', 5, 64),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.01, 0.02, 0.05, 0.005, 0.1]),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.5),\n        'n_estimators': trial.suggest_int('n_estimators', 2000, 8000),\n        'cat_smooth' : trial.suggest_int('cat_smooth', 10, 100),\n        'cat_l2': trial.suggest_int('cat_l2', 1, 20),\n        'min_data_per_group': trial.suggest_int('min_data_per_group', 50, 200),\n        'cat_feature' : [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, \n                         32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, \n                         53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67],\n        'n_jobs' : -1, \n        'random_state': 42,\n        'boosting_type': 'gbdt',\n        'metric': 'AUC',\n        'device': 'gpu'\n    }\n    model = LGBMClassifier(**params)  \n    \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],eval_metric='auc', early_stopping_rounds=300, verbose=False)\n    \n    preds = model.predict_proba(test_x)[:,1]\n    \n    auc = roc_auc_score(test_y, preds)\n    \n    return auc","9fc31fee":"study = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=50)\nprint('Number of finished trials: ', len(study.trials))\nprint('Best trial: ', study.best_trial.params)\nprint('Best value: ', study.best_value)","93ecbb99":"# Historic\nplot_optimization_history(study)","a40609a2":"# Importance\noptuna.visualization.plot_param_importances(study)","04d9b5f3":"lgb_params =  {'reg_alpha': 5.028382776465415, \n               'reg_lambda': 7.969115943661513, \n               'num_leaves': 196, \n               'min_child_samples': 39, \n               'max_depth': 20, \n               'learning_rate': 0.01, \n               'colsample_bytree': 0.22772406492167746, \n               'n_estimators': 7028, \n               'cat_smooth': 38, \n               'cat_l2': 20, \n               'min_data_per_group': 199,\n               'cat_feature' : [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, \n                                 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, \n                                 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67],\n               'n_jobs' : -1, \n               'random_state': 42,\n               'boosting_type': 'gbdt',\n               'metric': 'AUC',\n               'device': 'gpu'\n}","4d8af00c":"lgb_params = study.best_trial.params\nlgb_params['device'] = \"gpu\"\nlgb_params['random_state'] = 42\nlgb_params['cat_feature'] = [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, \n                             32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, \n                             53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67]\nlgb_params['n_jobs'] = -1 \nlgb_params['boosting_type'] =  'gbdt'\nlgb_params['metric'] =  'AUC'","6f0f0c77":"NFOLDS = 20\nfolds = StratifiedKFold(n_splits=NFOLDS, random_state=42, shuffle=True)\npredictions = np.zeros(len(X_test))\nfor fold, (train_index, test_index) in enumerate(folds.split(X, y)):\n    print(\"--> Fold {}\".format(fold + 1))\n    \n    X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n    \n    lgb_model = LGBMClassifier(**lgb_params).fit(X_train, y_train, \n                                                  eval_set=[(X_valid, y_valid)], \n                                                  eval_metric='auc', \n                                                  early_stopping_rounds=300, verbose=0)\n    \n    y_preds = lgb_model.predict_proba(X_valid)[:,1]\n    predictions += lgb_model.predict_proba(X_test)[:,1] \/ folds.n_splits \n    \n    print(\": LGB - ROC AUC Score = {}\".format(roc_auc_score(y_valid, y_preds, average=\"micro\")))","5e672673":"sub = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/sample_submission.csv')\nsub['target'] = predictions\nregr_table = sub.to_csv(\"Sub_lgb_v2.csv\", index = False)","a8292ccd":"Number of finished trials: 50 Best trial: {'max_depth': 4, 'learning_rate': 0.1, 'n_estimators': 2877, 'max_bin': 200, 'min_data_in_leaf': 10, 'l2_leaf_reg': 0.09385107162927438, 'subsample': 0.7990428819543426} Best value: 0.8925910141177894","1771f184":"# CatBoost Optuna","77aac391":"# LGBM Optuna","66d167d9":"# Model Creating and Evaluating","bb62c208":"Number of finished trials: 1 Best trial: {'max_depth': 4, 'learning_rate': 0.1, 'n_estimators': 2616, 'min_child_weight': 36, 'gamma': 0.0001231342905079067, 'alpha': 5.138826788428377, 'lambda': 0.006952601632723477, 'colsample_bytree': 0.3019243613187322, 'subsample': 0.7474126793277557} Best value: 0.8941200673933261\n\nNumber of finished trials: 50 Best trial: {'max_depth': 6, 'learning_rate': 0.02, 'n_estimators': 2941, 'min_child_weight': 10, 'gamma': 0.027689264382343946, 'alpha': 2.239319562015662, 'lambda': 0.005116156806904708, 'colsample_bytree': 0.2018103901998171, 'subsample': 0.7452030806282816} Best value: 0.8951492161710065","6a0f0fdb":"# Introduction\n\nHey, thanks for viewing my Kernel!\n\nIf you like my work, please, leave an upvote: it will be really appreciated and it will motivate me in offering more content to the Kaggle community ! \ud83d\ude0a","e461bde9":"Trial 32 finished with value: 0.8960118885713237 and parameters: {'reg_alpha': 5.028382776465415, 'reg_lambda': 7.969115943661513, 'num_leaves': 196, 'min_child_samples': 39, 'max_depth': 20, 'learning_rate': 0.01, 'colsample_bytree': 0.22772406492167746, 'n_estimators': 7028, 'cat_smooth': 38, 'cat_l2': 20, 'min_data_per_group': 199}. Best is trial 32 with value: 0.8960118885713237.\n\nNumber of finished trials: 50 Best trial: {'reg_alpha': 5.028382776465415, 'reg_lambda': 7.969115943661513, 'num_leaves': 196, 'min_child_samples': 39, 'max_depth': 20, 'learning_rate': 0.01, 'colsample_bytree': 0.22772406492167746, 'n_estimators': 7028, 'cat_smooth': 38, 'cat_l2': 20, 'min_data_per_group': 199}. Best is trial 32 with value: 0.8960118885713237.","01d456d0":"# XGB Optuna"}}