{"cell_type":{"219c1cb8":"code","b26c514d":"code","232943f4":"code","358ae251":"code","126ed4c6":"code","be9c29df":"code","d1d01f84":"code","32267e39":"code","fbb24b1b":"code","772be8d3":"code","a674e534":"code","c46970c8":"code","fe78cee4":"code","5167dbce":"code","8bd941e3":"markdown"},"source":{"219c1cb8":"## Downloading data\n# ! wget --output-document \"..\/data\/raw\/universum_compressed.tar\" http:\/\/sereja.me\/f\/universum_compressed.tar","b26c514d":"## Preprocessing data for using with model\n# ! tar xf \"..\/data\/raw\/universum_compressed.tar\" --directory=\"..\/data\/preprocessed\/\"\n","232943f4":"! wandb online","358ae251":"import matplotlib.pyplot as plt\nfrom pathlib import Path\nimport skimage.io as io\nimport pytorch_lightning as pl\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision.utils import make_grid\nfrom pytorch_lightning.loggers import WandbLogger\nfrom functools import lru_cache\nfrom random import shuffle\nimport random\nfrom PIL import Image\nfrom torch.utils.data import random_split\nfrom easydict import EasyDict\nfrom sklearn.model_selection import train_test_split\nimport torch.nn as nn\nimport wandb\nimport numpy as np\nimport os\nfrom torch.optim import Adam\nimport torch\nfrom kaggle_secrets import UserSecretsClient\nfrom torch.optim.lr_scheduler import StepLR\nuser_secrets = UserSecretsClient()\nos.environ['WANDB_API_KEY'] = user_secrets.get_secret(\"wandb-api\")\n\npl.seed_everything(0)\nrandom.seed(0)\n","126ed4c6":"config = EasyDict(\n    dataset_dir=Path(\"..\/input\/100-bird-species\"),\n    val_size=0.2,\n    batch_size=64,\n    lr=0.01,\n    max_epochs=25,\n    step_size=5,\n)","be9c29df":"image_ext = ['.jpg', '.JPG', '.png']\n\n@lru_cache(maxsize=None)\ndef collect_images_files(path_dir: Path):\n    assert path_dir.is_dir()\n\n    images = []\n    for path in path_dir.glob(\"**\/*\"):\n        if path.is_file() and path.suffix in image_ext:\n            images.append(path)\n\n    print(f\"Found images in {str(path_dir)}: {len(images)}\")\n    return sorted(images)\n\nprint(collect_images_files(config.dataset_dir)[0])","d1d01f84":"def visualize_dataset(dataset_dir: Path, n_rows=2, n_cols=4):\n    n_samples = n_rows * n_cols\n    paths = collect_images_files(dataset_dir)\n#     shuffle(paths)\n    figure, ax = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(12, 6))\n\n    for ii, path in enumerate(paths[:n_samples]):\n        image = io.imread(str(path))\n        name = path.name\n\n        ax.ravel()[ii].imshow(image)\n        ax.ravel()[ii].set_title(name)\n        ax.ravel()[ii].set_axis_off()\n\n    plt.tight_layout()\n    plt.show()\n\n\nprint(\"Training images\")\nvisualize_dataset(config.dataset_dir)","32267e39":"train_transform = transforms.Compose([\n#     transforms.RandomCrop(size=128),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomVerticalFlip(p=0.5),\n    transforms.RandomAffine(degrees=90, scale=(0.8, 1.2), translate=(0.1, 0.2)),\n    \n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n])\n\nval_transform = transforms.Compose([\n    transforms.RandomCrop(size=128),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n])","fbb24b1b":"def visualize_augmented_dataset(dataset_dir: Path, transform, n_rows=2, n_cols=4):\n    n_samples = n_rows * n_cols\n    paths = collect_images_files(dataset_dir)\n#     shuffle(paths)\n    figure, ax = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(12, 6))\n\n    for ii, path in enumerate(paths[:n_samples]):\n        image = Image.open(str(path))\n        image = transform(image).permute(1, 2, 0).numpy()\n        name = path.name\n\n        ax.ravel()[ii].imshow(image)\n        ax.ravel()[ii].set_title(name)\n        ax.ravel()[ii].set_axis_off()\n\n    plt.tight_layout()\n    plt.show()\n\n\nprint(\"Visualizing augmented dataset\")\nvisualize_augmented_dataset(config.dataset_dir, train_transform)\n","772be8d3":"class ImageDataset(Dataset):\n    def __init__(self, data_dir: Path, transform, train: bool, val_size: float):\n        super(ImageDataset, self).__init__()\n        assert data_dir.is_dir()\n\n        self.paths = sorted(collect_images_files(data_dir))\n\n        if train:\n            self.paths, _ = train_test_split(self.paths, test_size=val_size, shuffle=True, random_state=0)\n        else:\n            _, self.paths = train_test_split(self.paths, test_size=val_size, shuffle=True, random_state=0)\n\n        self.transform = transform\n\n        self.to_grayscale = transforms.Grayscale()\n\n    def __getitem__(self, item):\n        image = Image.open(str(self.paths[item]))\n        image = self.transform(image)\n        gray_image = self.to_grayscale(image)\n        return image, gray_image\n\n    def __len__(self):\n        return len(self.paths)\n\n\nclass ImageDataLoader(pl.LightningDataModule):\n    def __init__(self, data_dir: Path, val_size: float, batch_size: int):\n        super(ImageDataLoader, self).__init__()\n\n        self.train_dataset = ImageDataset(data_dir, train_transform, train=True, val_size=val_size)\n        self.val_dataset = ImageDataset(data_dir, val_transform, train=False, val_size=val_size)\n\n        self.batch_size = batch_size\n\n    def train_dataloader(self):\n        return DataLoader(dataset=self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=2)\n\n    def val_dataloader(self):\n        return DataLoader(dataset=self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=2)\n","a674e534":"# input shape: torch.Size([2, 3, 224, 224])\n# encoder output: torch.Size([2, 512, 14, 14])\n# reconstructed shape: torch.Size([2, 3, 224, 224])\n\nclass EncoderBlock(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, stride: int):\n        super(EncoderBlock, self).__init__()\n        kernel_size = (3, 3)\n        \n        self.block = nn.Sequential(\n            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=1, stride=stride),\n            nn.BatchNorm2d(num_features=out_channels),\n            nn.LeakyReLU(),\n            \n            nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, padding=1),\n            nn.BatchNorm2d(num_features=out_channels),\n            nn.LeakyReLU(),\n        )\n        \n        if stride == 1:\n            self.shortcut = nn.Sequential()\n        if stride == 2:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=1, stride=stride),\n                nn.BatchNorm2d(num_features=out_channels),\n                nn.LeakyReLU()\n            )\n\n    def forward(self, x):\n        output = self.block(x)\n        return output + self.shortcut(x)\n\n\nclass Encoder(nn.Module):\n\n    def __init__(self):\n        super(Encoder, self).__init__()\n\n        self.preprocess = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(3, 3), padding=1, bias=False),\n            nn.BatchNorm2d(num_features=64),\n            nn.LeakyReLU(),\n        )\n\n        self.blocks = nn.Sequential(*[\n            EncoderBlock(in_channels=64, out_channels=64, stride=2),\n            EncoderBlock(in_channels=64, out_channels=64, stride=1),\n            EncoderBlock(in_channels=64, out_channels=128, stride=2),\n            EncoderBlock(in_channels=128, out_channels=128, stride=1),\n            EncoderBlock(in_channels=128, out_channels=256, stride=2),\n            EncoderBlock(in_channels=256, out_channels=256, stride=1),\n            EncoderBlock(in_channels=256, out_channels=512, stride=2),\n            EncoderBlock(in_channels=512, out_channels=512, stride=1),\n        ])\n\n    def forward(self, x):\n        x = self.preprocess(x)\n        x = self.blocks(x)\n        return x\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, scale_factor: int):\n        super(DecoderBlock, self).__init__()\n        self.skip = in_channels == out_channels\n\n        self.module = nn.Sequential(\n            nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(3, 3), padding=(1, 1)),\n            nn.BatchNorm2d(out_channels),\n            nn.LeakyReLU(),\n            \n            nn.ConvTranspose2d(in_channels=out_channels, out_channels=out_channels, kernel_size=(3, 3), padding=(1, 1)),\n            nn.BatchNorm2d(out_channels),\n            nn.LeakyReLU(),\n            \n            nn.Upsample(scale_factor=scale_factor),\n           \n        )\n        \n    def forward(self, x):\n        x = self.module(x)\n        return x\n\n\nclass Decoder(nn.Module):\n    def __init__(self):\n        super(Decoder, self).__init__()\n\n        self.blocks = nn.Sequential(\n            DecoderBlock(in_channels=512, out_channels=512, scale_factor=1),\n            DecoderBlock(in_channels=512, out_channels=512, scale_factor=2),\n            DecoderBlock(in_channels=512, out_channels=256, scale_factor=1),\n            DecoderBlock(in_channels=256, out_channels=256, scale_factor=2),\n            DecoderBlock(in_channels=256, out_channels=128, scale_factor=1),\n            DecoderBlock(in_channels=128, out_channels=128, scale_factor=2),\n            DecoderBlock(in_channels=128, out_channels=64, scale_factor=1),\n            DecoderBlock(in_channels=64, out_channels=64, scale_factor=2),\n            \n        )\n\n        self.postprocess = nn.Sequential(\n            nn.ConvTranspose2d(in_channels=64, out_channels=3, kernel_size=(3, 3), padding=(1, 1)),\n            nn.Tanh(),\n        )\n\n    def forward(self, x):\n        x = self.blocks(x)\n        x = self.postprocess(x)\n        return x\n\n    \n","c46970c8":"class AutoEncoder(pl.LightningModule):\n    def __init__(self, conf):\n        super().__init__()\n\n        self.loss = nn.MSELoss()\n\n        self.encoder = Encoder()\n        self.decoder = Decoder()\n\n    def forward(self, image):\n        return self.decoder(self.encoder(image))\n\n    def training_step(self, batch, batch_idx):\n        original, gray = batch\n        reconstructed = self.forward(gray)\n\n        loss = self.loss(reconstructed, original)\n        self.log(\"train\/loss_step\", loss.item())\n\n        return {'loss': loss}\n\n    def training_epoch_end(self, outputs):\n        avg_loss = torch.hstack([loss['loss'] for loss in outputs]).mean()\n        self.log('train\/loss_epoch', avg_loss.item())\n\n    def validation_step(self, batch, batch_idx):\n        original, gray = batch\n        reconstructed = self.forward(gray)\n\n        loss = self.loss(reconstructed, original)\n        self.log(\"val\/loss_step\", loss.item())\n\n        return {'loss': loss}\n\n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.hstack([loss['loss'] for loss in outputs]).mean()\n        self.log('val\/loss_epoch', avg_loss.item())\n\n    def configure_optimizers(self):\n        optimizer = Adam(params=self.parameters(), lr=config.lr)\n        lr_scheduler = StepLR(optimizer, step_size=config.step_size, gamma=0.1)\n        return [optimizer,], [lr_scheduler, ] \n","fe78cee4":"## Defining image logging\n\ndef create_grid(gray: torch.Tensor, reconstructed, original):\n    gray_rgb = gray.repeat(1, 3, 1, 1)\n    concat_images = torch.cat([\n        torch.hstack(list(gray_rgb)),\n        torch.hstack(list(reconstructed)),\n        torch.hstack(list(original)),\n    ], dim=2)\n\n    return make_grid(concat_images)\n\n\nclass ImageCallback(pl.Callback):\n    def __init__(self, train_dataloader: DataLoader, val_dataloader: DataLoader):\n        super(ImageCallback, self).__init__()\n\n        self.train_dataloader = train_dataloader\n        self.val_dataloader = val_dataloader\n        \n        \n    def on_train_epoch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule) -> None:\n        original, gray = next(iter(self.train_dataloader))\n        original = original.cuda()\n        gray = gray.cuda()\n        reconstructed = pl_module(gray)\n\n        grid = create_grid(gray, reconstructed, original)\n\n        grid = np.transpose(grid.cpu().detach().numpy(), axes=(1, 2, 0))\n\n        trainer.logger.experiment.log({\n            \"train\/predictions\": wandb.Image(grid, caption=\"gray\/reconstructed\/original\"),\n            'global_step': trainer.global_step\n        })\n\n\n    def on_validation_epoch_end(self, trainer: pl.Trainer, pl_module: pl.LightningModule) -> None:\n        original, gray = next(iter(self.val_dataloader))\n        original = original.cuda()\n        gray = gray.cuda()\n        reconstructed = pl_module(gray)\n\n        grid = create_grid(gray, reconstructed, original)\n\n        grid = np.transpose(grid.cpu().detach().numpy(), axes=(1, 2, 0))\n\n        trainer.logger.experiment.log({\n            \"val\/predictions\": wandb.Image(grid, caption=\"gray\/reconstructed\/original\"),\n            'global_step': trainer.global_step\n        })\n\n","5167dbce":"def train():\n    wandb.init(config=config)\n\n    dataloader = ImageDataLoader(\n        batch_size=config.batch_size,\n        val_size=config.val_size,\n        data_dir=config.dataset_dir\n    )\n    train_dataloader, val_dataloader = dataloader.train_dataloader(), dataloader.val_dataloader()\n    image_callback = ImageCallback(train_dataloader, val_dataloader)\n    lr_callback = pl.callbacks.LearningRateMonitor(logging_interval='step')\n\n    model_checkpoint = pl.callbacks.ModelCheckpoint(\n        save_last=True,\n        save_top_k=1,\n        every_n_epochs=1,\n        save_weights_only=True,\n    )\n\n\n    logger = WandbLogger(project=\"TmpColorizingImages\",\n                         name=\"initial_experiment\",\n                         log_model='all',\n                         config=config.__dict__, )\n\n    trainer = pl.Trainer(\n        logger=logger,\n        gpus=1,\n        max_epochs=config.max_epochs,\n#         limit_train_batches=2,\n#         limit_val_batches=2,\n        callbacks=[image_callback, lr_callback, model_checkpoint]\n    )\n\n    model = AutoEncoder(config)\n    trainer.fit(model, train_dataloader, val_dataloader)\n    trainer.save_checkpoint(\"last.ckpt\", weights_only=True)\n\n\ntrain()\n","8bd941e3":"![](https:\/\/www.run.ai\/wp-content\/uploads\/2021\/01\/diagrams-Dec-2020_D-1024x816.png)"}}