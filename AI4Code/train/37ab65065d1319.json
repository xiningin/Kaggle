{"cell_type":{"db037465":"code","982c03ed":"code","c908410d":"code","a1c81144":"code","adc9a8c7":"code","69e8085d":"code","24de7471":"code","4f77bddf":"code","47479e4b":"code","b34134b1":"code","0db79389":"code","20354e1c":"code","0e2fb9ec":"code","a2479c9e":"code","4c812fa2":"code","7fe59c2b":"code","55bff114":"code","6603fc50":"code","e603c102":"code","d0e3cc07":"code","d845032c":"code","9a8b254e":"markdown","1f9fb774":"markdown","a0796631":"markdown","b45f4d4f":"markdown","4f0e8a11":"markdown","61f2ffa4":"markdown","745e6e60":"markdown","97907d3e":"markdown","01ae7742":"markdown","ca7d6383":"markdown","8e55ba57":"markdown","962e2d00":"markdown","76c3c9db":"markdown","704d408e":"markdown","75f5f864":"markdown","cfb61be1":"markdown","04be4f2a":"markdown","3b536dde":"markdown"},"source":{"db037465":"import numpy\nimport numpy as np\nimport scipy\nimport matplotlib\nimport pandas\nimport sklearn\nimport matplotlib.pyplot as plt\nfrom pandas import read_csv\nfrom pandas import set_option\nfrom time import time\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.ticker import NullFormatter\nfrom sklearn import manifold, metrics, preprocessing\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.model_selection import cross_val_score, learning_curve, GridSearchCV, KFold\nfrom sklearn.linear_model import LogisticRegression, Perceptron\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_digits\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import VotingClassifier\n","982c03ed":"train_file = '..\/input\/higgs-boson-data\/higgs_train_10k.csv'\ntest_file = '..\/input\/higgs-boson-data\/higgs_test_5k.csv'\n\nnames = [\n    'response',\n    'x1',\n    'x2',\n    'x3',\n    'x4',\n    'x5',\n    'x6',\n    'x7',\n    'x8',\n    'x9',\n    'x10',\n    'x11',\n    'x12',\n    'x13',\n    'x14',\n    'x15',\n    'x16',\n    'x17',\n    'x18',\n    'x19',\n    'x20',\n    'x21',\n    'x22',\n    'x23',\n    'x24',\n    'x25',\n    'x26',\n    'x27',\n    'x28']","c908410d":"train_csv = read_csv(train_file, names=names)\ntest_csv = read_csv(test_file, names=names)\nprint(train_csv.shape)\nprint(test_csv.shape)","a1c81144":"train_csv.head(10)","adc9a8c7":"test_csv.head(10)","69e8085d":"types = train_csv.dtypes\nprint(types)","24de7471":"set_option('display.width', 100)\nset_option('precision', 5)\ndescription = train_csv.describe()\nprint(description)","4f77bddf":"# class distribution for train and test\ntrain_data_class = train_csv.groupby('response').size()\nprint(train_data_class)\ntest_data_class = test_csv.groupby('response').size()\nprint(test_data_class)","47479e4b":"correlations = train_csv.corr(method='pearson')\n\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(correlations, vmin=-1, vmax=1)\nfig.colorbar(cax)\nticks = numpy.arange(0,29,1)\nax.set_xticks(ticks)\nax.set_yticks(ticks)\nax.set_xticklabels(names, size = 25)\nax.set_yticklabels(names, size = 25)\nplt.rcParams['figure.figsize'] = (40,40)\nplt.show()","b34134b1":"plt.rcParams['figure.figsize'] = (12,12)\ntrain_csv.plot(kind='box', subplots=True, layout=(6,6), sharex=False, sharey=False)\nplt.show()\ntest_csv.plot(kind='box', subplots=True, layout=(6,6), sharex=False, sharey=False)\nplt.rcParams['figure.figsize'] = (12,12)\nplt.show()","0db79389":"train_arr = train_csv.values\nX_train = train_arr[:,1:28]\nY_train = train_arr[:,0]\ntest_arr = test_csv.values\nX_test = test_arr[:,1:28]\nY_test = test_arr[:,0]","20354e1c":"methods = ['standard', 'ltsa', 'hessian', 'modified']\nlabels = ['LLE', 'LTSA', 'Hessian LLE', 'Modified LLE']\n\nn_neighbors = 10\nn_components = 2\ncolor=Y_train\n\nfor i, method in enumerate(methods):\n    Ytransformed = manifold.Isomap(n_neighbors, n_components).fit_transform(X_train)\n    ax = fig.add_subplot(257)\n    plt.scatter(Ytransformed[:, 0], Ytransformed[:, 1],c=color, cmap=plt.cm.Spectral)\n    plt.title(labels[i])\n    ax.xaxis.set_major_formatter(NullFormatter())\n    ax.yaxis.set_major_formatter(NullFormatter())\n    plt.axis('tight')\n    plt.show()\n\n\nt0 = time()\nmds = manifold.MDS(n_components, max_iter=100, n_init=1)\nYtransformed = mds.fit_transform(X_train)\nt1 = time()\nax = fig.add_subplot(258)\nplt.scatter(Ytransformed[:, 0], Ytransformed[:, 1], c=color,cmap=plt.cm.Spectral)\nplt.title(\"MDS (%.2g sec)\" % (t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis('tight')\nplt.show()\n\n\nt0 = time()\nse = manifold.SpectralEmbedding(n_components=n_components,\n                                n_neighbors=n_neighbors)\nYtransformed = se.fit_transform(X_train)\nt1 = time()\nax = fig.add_subplot(259)\nplt.scatter(Ytransformed[:, 0], Ytransformed[:, 1], c=color,cmap=plt.cm.Spectral)\nplt.title(\"SpectralEmbedding (%.2g sec)\" % (t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis('tight')\nplt.show()\n\nt0 = time()\ntsne = manifold.TSNE(n_components=n_components, init='pca', random_state=0)\nYtransformed = tsne.fit_transform(X_train)\nt1 = time()\nax = fig.add_subplot(2, 5, 10)\nplt.scatter(Ytransformed[:, 0], Ytransformed[:, 1], c=color,cmap=plt.cm.Spectral)\nplt.title(\"t-SNE (%.2g sec)\" % (t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis('tight')\n\nplt.show()","0e2fb9ec":"\npca = PCA(n_components=2)\nfit = pca.fit(X_train)\nprojected = pca.fit_transform(X_train)\n\nplt.scatter(projected[:, 0], projected[:, 1],\n               c=Y_train, edgecolor='none', alpha=0.5)\nplt.xlabel('PCA component 1')\nplt.ylabel('PCA component 2')\nplt.rcParams['figure.figsize'] = (8, 8)\nplt.colorbar()\nplt.show()\npca = PCA(n_components=25)\nfit = pca.fit(X_train)\nplt.plot(numpy.cumsum(fit.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.show()\n# summarize components\nprint(\"Explained Variance: %s\" % fit.explained_variance_ratio_)","a2479c9e":"# using roc AUC as scoring\nscoring = 'accuracy'\n\n# Naive Bayes\nnaiveBayes = GaussianNB()\nnbscore = cross_val_score(naiveBayes, X_train, Y_train, cv=3, scoring=scoring)\nprint('Naive Bayes CV score =', np.mean(nbscore))\n\n\n# penalty\npenalties = numpy.array(['l2'])\n# C for logistic regression\nc_values = numpy.array([1000, 100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001])\n# max iteration\niters = numpy.array([100, 150])\nLR_param_grid = {'penalty': penalties, 'C': c_values, 'max_iter': iters}\n\n# logistic regression as algorithm\ngridLogisticRegression = LogisticRegression()\n# Using GridSearchCV on Training Data for LR\ngrid = GridSearchCV(\n    estimator=gridLogisticRegression,\n    param_grid=LR_param_grid,\n    scoring=scoring)\ngrid.fit(X_train, Y_train)\nprint('LR CVScore ', grid.best_score_)\nprint('LR Penalty', grid.best_estimator_.penalty)\nprint('LR C', grid.best_estimator_.C)\nprint('LR Max Iterations', grid.best_estimator_.max_iter)\n\n\n# Perceptron\n# Using GridSearchCV on Training Data for perceptron\n# alphas\nalphas = numpy.array([0.001, 0.0001, 0.00001, 0.000001])\n# iterations\npereptorn_param_grid = {'alpha': alphas, 'max_iter': iters}\ngrid = GridSearchCV(\n    estimator=Perceptron(),\n    param_grid=pereptorn_param_grid,\n    scoring=scoring)\ngrid.fit(X_train, Y_train)\nprint('Perceptron CVScore ', grid.best_score_)\nprint('Perceptron alpha', grid.best_estimator_.alpha)\nprint('Perceptron Max Iterations', grid.best_estimator_.max_iter)\n\n# LDA\ntols = numpy.array([0.001, 0.00001, 0.001])\nlda_param_grid = {'tol': tols}\ngrid = GridSearchCV(\n    estimator=LinearDiscriminantAnalysis(),\n    param_grid=lda_param_grid,\n    scoring=scoring)\ngrid.fit(X_train, Y_train)\nprint('LDA CVScore ', grid.best_score_)\nprint('LDA tol', grid.best_estimator_.tol)","4c812fa2":"# gamma parameter in SVM\ngammas = numpy.array([1, 0.1, 0.01, 0.001])\n# C for logistic regression\nc_values = numpy.array([100, 1, 0.1, 0.01])\nsvm_param_grid = {'gamma': gammas, 'C': c_values}\nsvm = SVC(kernel='rbf')\nscoring = 'accuracy'\ngrid = GridSearchCV(estimator=svm, param_grid=svm_param_grid, scoring=scoring)\ngrid.fit(X_train, Y_train)\nprint(grid.best_score_)\nprint(grid.best_estimator_.gamma)\nprint(grid.best_estimator_.C)","7fe59c2b":"min_max_scaler = preprocessing.MinMaxScaler()\nscaler = min_max_scaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\n\npipe = Pipeline([\n    ('reduce_dim', PCA()),\n    ('classify', LogisticRegression())\n])\n\nN_FEATURES_OPTIONS = [10, 15, 20]\nC_OPTIONS = [0.001, 0.1, 1, 10, 100, 1000]\nmax_iter_OPTIONS = [100, 150]\nparam_grid = [\n    {\n        'reduce_dim': [PCA(iterated_power=10)],\n        'reduce_dim__n_components': N_FEATURES_OPTIONS,\n        'classify__C': C_OPTIONS,\n        'classify__max_iter':max_iter_OPTIONS\n    },\n    {\n        'reduce_dim': [SelectKBest(chi2)],\n        'reduce_dim__k': N_FEATURES_OPTIONS,\n        'classify__C': C_OPTIONS,\n        'classify__max_iter':max_iter_OPTIONS\n    },\n]\nreducer_labels = ['PCA', 'KBest(chi2)']\n\ngrid = GridSearchCV(pipe, cv=3, n_jobs=1, param_grid=param_grid)\ngrid.fit(X_train_scaled, Y_train)\n\nmean_scores = np.array(grid.cv_results_['mean_test_score'])\n# scores are in the order of param_grid iteration, which is alphabetical\nmean_scores = mean_scores.reshape(len(C_OPTIONS), -1, len(N_FEATURES_OPTIONS))\n# select score for best C\nmean_scores = mean_scores.max(axis=0)\nbar_offsets = (np.arange(len(N_FEATURES_OPTIONS)) *\n               (len(reducer_labels) + 1) + .5)\n\nplt.figure()\nCOLORS = ['tomato', 'darkolivegreen', 'lightsteelblue']\nfor i, (label, reducer_scores) in enumerate(zip(reducer_labels, mean_scores)):\n    plt.bar(bar_offsets + i, reducer_scores, label=label, color=COLORS[i])\n\nplt.title(\"Comparing feature reduction techniques\")\nplt.xlabel('Reduced number of features')\nplt.xticks(bar_offsets + len(reducer_labels) \/ 2, N_FEATURES_OPTIONS)\nplt.ylabel('Classification accuracy')\nplt.ylim((0, 1))\nplt.legend(loc='upper left')\nplt.show()","55bff114":"def plot_learning_curve(estimator, name, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title('Learning Curves for ' + name)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"No. Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"b\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"b\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\n\nestimator = LogisticRegression(C=0.1, penalty='l2', max_iter=100)\nplot_learning_curve(estimator, 'Tuned Logistic Regression', X_train, Y_train)\nplt.rcParams['figure.figsize'] = (7, 7)\nplt.show()\nestimator = SVC(C=100, gamma=0.01, kernel='rbf')\nplot_learning_curve(estimator, 'Tuned SVM', X_train, Y_train)\nplt.rcParams['figure.figsize'] = (7, 7)\nplt.show()","6603fc50":"\ndef train_and_test(clf):\n    print('_' * 80)\n    print(\"Training: \")\n    print(clf)\n    t0 = time()\n    clf.fit(X_train, Y_train)\n    train_time = time() - t0\n    print(\"train time: %0.3fs\" % train_time)\n\n    t0 = time()\n    pred = clf.predict(X_test)\n    test_time = time() - t0\n    print(\"test time:  %0.3fs\" % test_time)\n\n    score = metrics.accuracy_score(Y_test, pred)\n    print(\"accuracy:   %0.3f\" % score)\n    print(\"classification report:\")\n    print(metrics.classification_report(Y_test, pred))\n    print()\n    clf_descr = str(clf).split('(')[0]\n    return clf_descr, score, train_time, test_time\n\n\nresults = []\nfor classifier, name in (\n    (LogisticRegression(\n        C=0.1, penalty='l2', max_iter=100), \"Logistic Regressin\"), (Perceptron(\n            alpha=0.001, max_iter=100), \"Perceptron\"), (LinearDiscriminantAnalysis(\n                tol=0.001), \"LDA\"), (GaussianNB(), \"Naive Bayes\"), (SVC(\n                    C=100, gamma=0.01, kernel='rbf'), \"SVM\")):\n    print('=' * 80)\n    print(name)\n    results.append(train_and_test(classifier))\n\n    \nindices = np.arange(len(results))\nresults = [[x[i] for x in results] for i in range(4)]","e603c102":"lr = LogisticRegression(C=0.1, penalty='l2', max_iter=150)\nlr.fit(X_train, Y_train)\nlrpreds = lr.predict_proba(X_test)[:,1]\nlr_fpr, lr_tpr, _ = metrics.roc_curve(Y_test, lrpreds)\nplt.figure()\nlw = 2\nplt.plot(lr_fpr, lr_tpr, color='darkorange',\n         lw=lw)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.01])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.rcParams['figure.figsize'] = (5,5)\nplt.show()","d0e3cc07":"models = []\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('Bag', BaggingClassifier(DecisionTreeClassifier(),100, random_state=7)))\nmodels.append(('RF', RandomForestClassifier(100, max_features=5)))\nmodels.append(('Bo', AdaBoostClassifier(DecisionTreeClassifier(),100, random_state=7)))\n# create a voting estimation \nestimators = []\nestimators.append(('logistic',LogisticRegression()))\nestimators.append(('NB',  GaussianNB()))\nmodels.append(('ELE',VotingClassifier(estimators, voting='soft')))\n\n\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\n# replace with 'roc_auc', 'neg_log_loss',.. based on the need\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=7, shuffle = True)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n","d845032c":"fig = plt.figure() \nfig.suptitle('Linear and Non-Linear Algorithm Comparison on Cross-Validation') \nax = fig.add_subplot(111) \nplt.boxplot(results) \nax.set_xticklabels(names) \nplt.show()","9a8b254e":"## Libraries","1f9fb774":"## Train & Test Data","a0796631":"### Class Distrubibution","b45f4d4f":"## Plots of ROC Curves","4f0e8a11":"### Correlation of Classes","61f2ffa4":"##\u00a0PCA Feature Transformation","745e6e60":"### Box Plots","97907d3e":"## Model Selection on Whole Dataset ","01ae7742":"## Other Models & Results","ca7d6383":"## Manifold Visualization","8e55ba57":"##\u00a0Learning Curvers on Training and Validation Data","962e2d00":"### Some statistics","76c3c9db":"## Model Creation, Tuning Hyperparameters and Validation using Train Data","704d408e":"## Pipeline with Feature Reduction Selection, Logistic Regression using Grid Search","75f5f864":"## SVM Grid Search","cfb61be1":"## Load Dataset","04be4f2a":"### Data Types","3b536dde":"## Exploring The Data"}}