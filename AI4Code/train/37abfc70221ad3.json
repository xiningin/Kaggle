{"cell_type":{"91783b23":"code","8276f0a6":"code","9035e16b":"code","c184297a":"code","6fd6e170":"code","ea5a50a6":"code","64bd8268":"code","ab000e0a":"code","17cb40a4":"code","4109473f":"code","c64ca212":"code","e84c9ad3":"code","2134827d":"code","cd6f00ff":"code","d6d1cbb4":"code","7be4cf6d":"code","661c6550":"code","7ab4e785":"code","982bfeaf":"code","4320aca9":"code","a657e0a4":"code","9e7f789d":"code","7359ac1e":"code","a81ffeb1":"code","89af9acd":"code","d12dc7bb":"code","a0b11c61":"code","7c143972":"code","ef401d08":"code","2cbaf3b4":"code","65c0be79":"code","f59f9b5e":"code","0d513785":"code","0d747676":"code","5464d069":"code","39ccb9ca":"code","948b8232":"code","645a630c":"code","85d05060":"markdown","48042536":"markdown","9aa6838d":"markdown","48e9a83b":"markdown","ad4cdfc9":"markdown","7729aaa6":"markdown","dfebc4a1":"markdown","a671fc67":"markdown","212cc822":"markdown","59b4d309":"markdown","d00c5670":"markdown","c53f47f5":"markdown","6e7d4388":"markdown"},"source":{"91783b23":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer","8276f0a6":"train_dataset = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_dataset = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","9035e16b":"train_dataset.head()","c184297a":"train_dataset.shape","6fd6e170":"train_dataset.info()","ea5a50a6":"train_dataset.fillna(\"\", inplace= True)\ntest_dataset.fillna(\"\", inplace= True)","64bd8268":"train_dataset.head()","ab000e0a":"test_dataset.head()","17cb40a4":"train_dataset['target'].value_counts()","4109473f":"plt.figure(figsize=(15,7))\nsns.countplot(train_dataset['target'])","c64ca212":"train_dataset['location'].value_counts()","e84c9ad3":"train_dataset['keyword'].value_counts()","2134827d":"dataset = pd.DataFrame()\ntest_dataset_cleaned = pd.DataFrame()\ndataset['all_combined'] = train_dataset['keyword'] + \" \" + train_dataset['location'] + \" \" + train_dataset['text']\ntest_dataset_cleaned['all_combined'] = test_dataset['keyword'] + \" \" + test_dataset['location'] + \" \" + test_dataset['text']\n\nprint(dataset.shape)\ntest_dataset_cleaned.shape\n#dataset.tail(100)","cd6f00ff":"def clean(data):\n    data = data.lower()\n    data = re.sub('https?:\/\/\\S+|www\\.\\S+', ' ', data)\n    data = re.sub('\\\\W', ' ', data)\n    data = re.sub('\\n', ' ', data)\n    data = re.sub(' +', ' ', data)\n    data = re.sub('^ ', ' ', data)\n    data = re.sub(' $', ' ', data)\n    data = re.sub('#', ' ', data)\n    data = re.sub('@', ' ', data)\n    data = re.sub('[^a-zA-Z]',' ', data)\n    return data","d6d1cbb4":"dataset['all_cleaned'] = dataset['all_combined'].astype(str).apply(clean)\ntest_dataset_cleaned['all_cleaned'] = test_dataset_cleaned['all_combined'].astype(str).apply(clean)","7be4cf6d":"dataset['target'] = train_dataset['target']","661c6550":"dataset.head(100)","7ab4e785":"test_dataset_cleaned.tail(100)","982bfeaf":"print(stopwords.words('english'))","4320aca9":"stop = set(stopwords.words('english'))\ndef remove_stopwords(data):\n    words = [word for word in data if word not in stop]\n    words= \"\".join(words).split()\n    words= [words.lower() for words in data.split()]\n    return words","a657e0a4":"dataset['all_cleaned'].apply(remove_stopwords)\ntest_dataset_cleaned['all_cleaned'].apply(remove_stopwords)","9e7f789d":"dataset.head(520)","7359ac1e":"test_dataset_cleaned.head(520)","a81ffeb1":"from nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()","89af9acd":"def lemmatization(data):\n    lemmas = []\n    for word in data.split():\n        lemmas.append(lemmatizer.lemmatize(word))\n    return \" \".join(lemmas)\n\ndataset['all_cleaned'].apply(lemmatization)\ntest_dataset_cleaned['all_cleaned'].apply(lemmatization)","d12dc7bb":"dataset.head(520)","a0b11c61":"def tokenize(string):\n    tokens = string.split()\n    return tokens\ndataset['all_cleaned']= dataset['all_cleaned'].apply(lambda x: tokenize(x))\ntest_dataset_cleaned['all_cleaned']= test_dataset_cleaned['all_cleaned'].apply(lambda x: tokenize(x))","7c143972":"dataset.head(520)","ef401d08":"dataset['all_cleaned']= dataset['all_cleaned'].apply(lambda x: ' '.join([str(elem) for elem in x]))\ntest_dataset_cleaned['all_cleaned']= test_dataset_cleaned['all_cleaned'].apply(lambda x: ' '.join([str(elem) for elem in x]))","2cbaf3b4":"dataset.head(520)","65c0be79":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(dataset['all_cleaned'])\nX = X.toarray()\nprint(X.shape)\n\npreparing_test_df = vectorizer.transform(test_dataset_cleaned['all_cleaned'])\npreparing_test_df = preparing_test_df.toarray()\nprint(preparing_test_df.shape)","f59f9b5e":"from sklearn.linear_model import LogisticRegression\n\nX_train = np.array(X)\nprint(X_train.shape)\ny_train = dataset['target']\nprint(y_train.shape)\nX_test = np.array(preparing_test_df)\nprint(X_test.shape)\n\nclf = LogisticRegression(solver='liblinear')\nclf.fit(X_train, y_train)","0d513785":"prediction = clf.predict(X_test)","0d747676":"prediction","5464d069":"submission = pd.DataFrame({\"id\":test_dataset[\"id\"],\"target\":prediction})","39ccb9ca":"submission.head()","948b8232":"submission.shape","645a630c":"submission.to_csv('.\/disaster_tweet_prediction_submission.csv', index = False)","85d05060":"> It is seen that `keyword` has only few **NULL** value where `location` has quite a lot of **NULL** values.","48042536":"> Took model inspiration from the kernel [here](https:\/\/www.kaggle.com\/manasvardhan\/a-beginner-s-guide-to-sentiment-analysis)~","9aa6838d":"## Stopwords Processing","48e9a83b":"# Dataset","ad4cdfc9":"## Vectorization","7729aaa6":"# Imports","dfebc4a1":"# Model Creation and Evaluation\n\n## Preparing training and test sets","a671fc67":"## Lemmatization\n\nreducing a word to its root form\n\nwaches, watched --> watch (root form)","212cc822":"# Prediction","59b4d309":"# Preparing to submit","d00c5670":"# NLP Processing and Data Cleaning","c53f47f5":"## Tokenization","6e7d4388":"# Data Visualization\n\n- **1 = disastrous tweet**\n- **0 = not disastrous tweet**"}}