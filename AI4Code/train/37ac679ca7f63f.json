{"cell_type":{"3ca13994":"code","9e72c7f2":"code","1ace55fb":"code","cd784c1d":"code","58bbb7fa":"code","6bbfa141":"code","1c2e1c02":"code","7712d206":"code","02f7d285":"markdown"},"source":{"3ca13994":"#importing all the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n%matplotlib inline\n\n#importing dataset using panda\ndataset = pd.read_csv('..\/input\/kc_house_data.csv')\n#to see what my dataset is comprised of\ndataset.head()","9e72c7f2":"#checking if any value is missing\nprint(dataset.isnull().any())","1ace55fb":"#checking for categorical data\nprint(dataset.dtypes)","cd784c1d":"#dropping the id and date column\ndataset = dataset.drop(['id','date'], axis = 1)","58bbb7fa":"#understanding the distribution with seaborn\nwith sns.plotting_context(\"notebook\",font_scale=2.5):\n    g = sns.pairplot(dataset[['sqft_lot','sqft_above','price','sqft_living','bedrooms']], \n                 hue='bedrooms', palette='tab20',size=6)\ng.set(xticklabels=[]);","6bbfa141":"#separating independent and dependent variable\nX = dataset.iloc[:,1:].values\ny = dataset.iloc[:,0].values\n#splitting dataset into training and testing dataset\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1\/3, random_state = 0)","1c2e1c02":"from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = regressor.predict(X_test)","7712d206":"#Backward Elimination\nimport statsmodels.formula.api as sm\ndef backwardElimination(x, SL):\n    numVars = len(x[0])\n    temp = np.zeros((21613,19)).astype(int)\n    for i in range(0, numVars):\n        regressor_OLS = sm.OLS(y, x).fit()\n        maxVar = max(regressor_OLS.pvalues).astype(float)\n        adjR_before = regressor_OLS.rsquared_adj.astype(float)\n        if maxVar > SL:\n            for j in range(0, numVars - i):\n                if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n                    temp[:,j] = x[:, j]\n                    x = np.delete(x, j, 1)\n                    tmp_regressor = sm.OLS(y, x).fit()\n                    adjR_after = tmp_regressor.rsquared_adj.astype(float)\n                    if (adjR_before >= adjR_after):\n                        x_rollback = np.hstack((x, temp[:,[0,j]]))\n                        x_rollback = np.delete(x_rollback, j, 1)\n                        print (regressor_OLS.summary())\n                        return x_rollback\n                    else:\n                        continue\n    regressor_OLS.summary()\n    return x\n \nSL = 0.05\nX_opt = X[:, [0, 1, 2, 3, 4, 5,6,7,8,9,10,11,12,13,14,15,16,17]]\nX_Modeled = backwardElimination(X_opt, SL)","02f7d285":"# House Prices using Backward Elimination\n\nJust started with machine learning. I have used backward Elimination to check the usefulness of dependent variables."}}