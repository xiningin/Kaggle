{"cell_type":{"e0eb9c46":"code","330c48aa":"code","8110bacf":"code","29c62e51":"code","5af8906e":"code","f90e20cc":"code","f062b09f":"code","95a56364":"code","b3e45c21":"code","002506cf":"code","97f01d6d":"code","539e8b04":"code","005441ee":"code","fa89ae86":"code","ce0e8cc8":"code","4dbd94d1":"code","9737061c":"code","fc2995ce":"code","aecdce1b":"code","f3dd28f6":"code","bacd15ea":"code","68fe0b88":"code","5db25ff8":"code","7308f3b5":"code","6e2c8a94":"code","59684d03":"code","4507138d":"code","c94b4485":"code","ac0871e9":"code","9eb27bc0":"code","c30eb017":"code","db531e4d":"code","b2ac30a8":"code","68b9fec2":"code","cc25ebe5":"code","f3861596":"code","a1c6cce8":"code","a46ff122":"code","57f0f76d":"code","35bca4f7":"code","f3a2be8f":"code","8ebaba02":"code","bba05049":"code","aecb5fe1":"code","f394f0d9":"markdown","8b2f58cc":"markdown","b6e85c89":"markdown"},"source":{"e0eb9c46":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom tqdm import tqdm\nimport gc\n# General imports\nimport os, sys, gc, time, warnings, pickle, psutil, random\n\n# custom imports\nfrom multiprocessing import Pool        # Multiprocess Runs\n\nwarnings.filterwarnings('ignore')","330c48aa":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n#\ndef autocorrelation(ys, t=1):\n    return np.corrcoef(ys[:-t], ys[t:])","8110bacf":"#==========================================================================\ndef preprocess_sales(sales, start=1400, upper=1970):\n    if start is not None:\n        print(\"dropping...\")\n        to_drop = [f\"d_{i+1}\" for i in range(start-1)]\n        print(sales.shape)\n        sales.drop(to_drop, axis=1, inplace=True)\n        print(sales.shape)\n    #=======\n    print(\"adding...\")\n    new_columns = ['d_%i'%i for i in range(1942, upper, 1)]\n    for col in new_columns:\n        sales[col] = np.nan\n    print(\"melting...\")\n    sales = sales.melt(id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\",\"scale\",\"start\"],\n                        var_name='d', value_name='demand')\n\n    print(\"generating order\")\n    if start is not None:\n        skip = start\n    else:\n        skip = 1\n    sales[\"nb\"] =sales.index \/\/ 1 + skip\n    return sales\n#===============================================================\ndef preprocess_calendar(calendar):\n    global maps, mods\n    calendar[\"event_name\"] = calendar[\"event_name_1\"]\n    calendar[\"event_type\"] = calendar[\"event_type_1\"]\n\n    map1 = {mod:i for i,mod in enumerate(calendar['event_name'].unique())}\n    calendar['event_name'] = calendar['event_name'].map(map1)\n    map2 = {mod:i for i,mod in enumerate(calendar['event_type'].unique())}\n    calendar['event_type'] = calendar['event_type'].map(map2)\n    calendar['nday'] = calendar['date'].str[-2:].astype(int)\n    maps[\"event_name\"] = map1\n    maps[\"event_type\"] = map2\n    mods[\"event_name\"] = len(map1)\n    mods[\"event_type\"] = len(map2)\n    calendar[\"wday\"] -=1\n    calendar[\"month\"] -=1\n    #calendar[\"year\"] -= 2011\n    mods[\"month\"] = 12\n    mods[\"year\"] = 6\n    mods[\"wday\"] = 7\n    mods['snap_CA'] = 2\n    mods['snap_TX'] = 2\n    mods['snap_WI'] = 2\n\n    calendar.drop([\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\", \"date\", \"weekday\"], \n                  axis=1, inplace=True)\n    return calendar\n#=========================================================\ndef make_dataset(categorize=False ,start=1400, upper= 1970):\n    global maps, mods\n    print(\"loading calendar...\")\n    calendar = pd.read_csv(\"..\/input\/m5-forecasting-uncertainty\/calendar.csv\")\n    print(\"loading sales...\")\n    sales = pd.read_pickle(\"..\/input\/m5unceratinityadddata\/TotalSales.pkl\")\n    cols = [\"item_id\", \"dept_id\", \"cat_id\",\"store_id\",\"state_id\"]\n    if categorize:\n        for col in cols:\n            temp_dct = {mod:i for i, mod in enumerate(sales[col].unique())}\n            mods[col] = len(temp_dct)\n            maps[col] = temp_dct\n        for col in cols:\n            sales[col] = sales[col].map(maps[col])\n        #\n\n    sales =preprocess_sales(sales, start=start, upper= upper)\n    calendar = preprocess_calendar(calendar)\n    calendar = reduce_mem_usage(calendar)\n    print(\"merge with calendar...\")\n    sales = sales.merge(calendar, on='d', how='left')\n    del calendar\n\n    print(\"reordering...\")\n    sales.sort_values(by=[\"id\",\"nb\"], inplace=True)\n    print(\"re-indexing..\")\n    sales.reset_index(inplace=True, drop=True)\n    gc.collect()\n\n    sales['n_week'] = (sales['nb']-1)\/\/7\n    sales[\"nday\"] -= 1\n    mods['nday'] = 31\n    sales = reduce_mem_usage(sales)\n    gc.collect()\n    return sales\n#===================","29c62e51":"%%time\nCATEGORIZE = True;\nSTART = 1; UPPER = 1970;\nmaps = {}\nmods = {}\nsales = make_dataset(categorize=CATEGORIZE ,start=START, upper= UPPER)","5af8906e":"sales['d'] = sales['d'].apply(lambda x: x[2:]).astype(np.int16)","f90e20cc":"START","f062b09f":"sales.head()","95a56364":"sales.head()","b3e45c21":"TARGET      = 'demand'\nBASE     = '..\/input\/m5unceratinityadddata\/TotalSales.pkl'\nSTART_TRAIN = 1 \nEND_TRAIN   = 1913\nP_HORIZON   = 28\nremove_features = ['id','state_id','store_id',\n                   'date','wm_yr_wk','d',TARGET]\ngrid_df=sales\ndel sales\n","002506cf":"grid_df.info()","97f01d6d":"########################### Apply on grid_df\ngrid_df1 = grid_df[['id','d','demand']]\nSHIFT_DAY = 28\nSHIFT_DAY1 = 1\n\n# Lags\n# with 28 day shift\nstart_time = time.time()\nprint('Create lags')\n\n#LAG_DAYS = [col for col in range(SHIFT_DAY,SHIFT_DAY+15)]\nLAG_DAYS = [col for col in range(SHIFT_DAY,SHIFT_DAY+14)]\ngrid_df1 = grid_df1.assign(**{\n        '{}_lag_{}'.format(col, l): grid_df1.groupby(['id'])[col].transform(lambda x: x.shift(l))\n        for l in LAG_DAYS\n        for col in [TARGET]\n    })\n\n# Minify lag columns\nfor col in list(grid_df1):\n    if 'lag' in col:\n        grid_df1[col] = grid_df1[col].astype(np.float16)\n\nprint('%0.2f min: Lags' % ((time.time() - start_time) \/ 60))\n\n\n# Rollings\n# with 28 day shift\nstart_time = time.time()\nprint('Create rolling aggs')\n\n\n\n#for i in [7,14,30,60,180]:\n#for i in [7,14,35,63,168]:\nfor i in [7,14,30]:\n    print('Rolling period:', i)\n    grid_df1['rolling_mean_'+str(i)] = grid_df1.groupby(['id'])[TARGET].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).mean()).astype(np.float16)\n    grid_df1['rolling_std_'+str(i)]  = grid_df1.groupby(['id'])[TARGET].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).std()).astype(np.float16)\n\n\n    \n    \nprint('%0.2f min: Lags' % ((time.time() - start_time) \/ 60))","539e8b04":"grid_df1=grid_df1.drop(['demand'],axis=1)","005441ee":"grid_df1.info(50)","fa89ae86":"########################### Merge prices and save part 2\n#################################################################################\nprint('Merge prices and save part 2')\n# Merge Prices\ngrid_df = grid_df.merge(grid_df1, on=['id','d'], how='left')\n#keep_columns = [col for col in list(sales) if col not in original_columns]\n#grid_df = grid_df[MAIN_INDEX+keep_columns]\n#grid_df = reduce_mem_usage(grid_df)\n\n# Safe part 2\n#grid_df.to_pickle('State_Item_1.pkl')\nprint('Size:', grid_df.shape)\n\n# We don't need prices_df anymore\ndel grid_df1\ngrid_df.head()\n\n# We can remove new columns\n# or just load part_1\n#grid_df = pd.read_pickle('grid_part_1.pkl')","ce0e8cc8":"grid_df.info()","4dbd94d1":"print(END_TRAIN)\nprint('-'*80)\nprint(START_TRAIN)\nprint('-'*80)","9737061c":"nb = grid_df['d'].values\n#MAX_LAG = max(LAGS)\ntr_mask = np.logical_and(nb>START + 42, nb<=1913)\nval_mask = np.logical_and(nb>1913, nb<=1941)\nte_mask = np.logical_and(nb>1941, nb<=1969)","fc2995ce":"grid_df.info()","aecdce1b":"features = [col for col in list(grid_df) if col not in remove_features]","f3dd28f6":"grid_df = grid_df[['id','d',TARGET]+features]\ngrid_df = grid_df[grid_df['d']>=START_TRAIN].reset_index(drop=True)\ngrid_df.info()","bacd15ea":"#scale = sales['scale'].values\nids = grid_df['id'].values\n","68fe0b88":"ids = grid_df['id'].values\nids","5db25ff8":"grid_df.info()","7308f3b5":"#sv = scale[val_mask]\n#se = scale[te_mask]\nids = ids[te_mask]\nids = ids.reshape((-1, 28))","6e2c8a94":"train_df=grid_df.loc[grid_df['d'] > 100]\ntrain_df=train_df.loc[grid_df['d'] <= 1941]\n\nvalidation_df=grid_df.loc[grid_df['d'] > 1913]\nvalidation_df=validation_df.loc[grid_df['d'] <= 1941]\n\nevaluation_df=grid_df.loc[grid_df['d'] > 1941]\nevaluation_df=evaluation_df.loc[grid_df['d'] <= 1969]\n\nX_train=train_df.drop(['demand','id'],axis=1)\ny_train=train_df[['demand']]\n\nX_validation=validation_df.drop(['demand','id'],axis=1)\ny_validation=validation_df[['demand']]\n\nX_evaluation=evaluation_df.drop(['demand','id'],axis=1)\ny_evaluation=evaluation_df[['demand']]\n#X_train=X_train.astype(int)\n#X_validation=X_validation.astype(int)\nX_train.info()\n","59684d03":"X_train.isnull().sum()","4507138d":"from sklearn.ensemble import GradientBoostingRegressor\n# Set lower and upper quantile\nQuantile1 = 0.005\nQuantile2 = 0.025\nQuantile3 = 0.165\nQuantile4 = 0.250\nQuantile5 = 0.500\nQuantile6 = 0.750\nQuantile7 = 0.835\nQuantile8 = 0.975\nQuantile9 = 0.995\n\n# Each model has to be separate\nmodel_1 =GradientBoostingRegressor(loss=\"quantile\",learning_rate=.046,n_estimators=100,subsample=0.5,criterion='mse',min_samples_split=5,min_samples_leaf=9,random_state=5,max_depth=9,max_features=3,                 \n                                        alpha=Quantile1)\nmodel_2 = GradientBoostingRegressor(loss=\"quantile\",learning_rate=.046,n_estimators=100,subsample=0.5,criterion='mse',min_samples_split=5,min_samples_leaf=9,random_state=5,max_depth=9,max_features=3,                 \n                                        alpha=Quantile2)\nmodel_3 = GradientBoostingRegressor(loss=\"quantile\",learning_rate=.046,n_estimators=100,subsample=0.5,criterion='mse',min_samples_split=5,min_samples_leaf=9,random_state=5,max_depth=9,max_features=3,                 \n                                        alpha=Quantile3)\n\n\nmodel_4 = GradientBoostingRegressor(loss=\"quantile\",learning_rate=.046,n_estimators=100,subsample=0.5,criterion='mse',min_samples_split=5,min_samples_leaf=9,random_state=5,max_depth=9,max_features=3,                 \n                                        alpha=Quantile4)\nmodel_5 = GradientBoostingRegressor(loss=\"quantile\",learning_rate=.046,n_estimators=100,subsample=0.5,criterion='mse',min_samples_split=5,min_samples_leaf=9,random_state=5,max_depth=9,max_features=3,                 \n                                        alpha=Quantile5)\nmodel_6 = GradientBoostingRegressor(loss=\"quantile\",learning_rate=.046,n_estimators=100,subsample=0.5,criterion='mse',min_samples_split=5,min_samples_leaf=9,random_state=5,max_depth=9,max_features=3,                 \n                                        alpha=Quantile6)\n\n\nmodel_7 = GradientBoostingRegressor(loss=\"quantile\",learning_rate=.046,n_estimators=100,subsample=0.5,criterion='mse',min_samples_split=5,min_samples_leaf=9,random_state=5,max_depth=9,max_features=3,                 \n                                        alpha=Quantile7)\nmodel_8 =GradientBoostingRegressor(loss=\"quantile\",learning_rate=.046,n_estimators=100,subsample=0.5,criterion='mse',min_samples_split=5,min_samples_leaf=9,random_state=5,max_depth=9,max_features=3,                 \n                                        alpha=Quantile8)\nmodel_9 = GradientBoostingRegressor(loss=\"quantile\",learning_rate=.046,n_estimators=100,subsample=0.5,criterion='mse',min_samples_split=5,min_samples_leaf=9,random_state=5,max_depth=9,max_features=3,                 \n                                        alpha=Quantile9)\n\n# The mid model will use the default loss\n#mid_model = GradientBoostingRegressor(loss=\"ls\")\n#upper_model = GradientBoostingRegressor(loss=\"quantile\",\n#                                        alpha=UPPER_ALPHA)","c94b4485":"# Fit models\nmodel_1.fit(X_train, y_train)\nmodel_2.fit(X_train, y_train)\nmodel_3.fit(X_train, y_train)\nmodel_4.fit(X_train, y_train)\nmodel_5.fit(X_train, y_train)\nmodel_6.fit(X_train, y_train)\nmodel_7.fit(X_train, y_train)\nmodel_8.fit(X_train, y_train)\nmodel_9.fit(X_train, y_train)\n# Record actual values on validation data\nvalidation = pd.DataFrame(y_validation)\nvalidation['0.005'] = model_1.predict(X_validation)\nvalidation['0.025'] = model_2.predict(X_validation)\nvalidation['0.165'] = model_3.predict(X_validation)\nvalidation['0.250'] = model_4.predict(X_validation)\nvalidation['0.500'] = model_5.predict(X_validation)\nvalidation['0.750'] = model_6.predict(X_validation)\nvalidation['0.835'] = model_7.predict(X_validation)\nvalidation['0.975'] = model_8.predict(X_validation)\nvalidation['0.995'] = model_9.predict(X_validation)\n\n# Record actual values on evaluation data\nevaluation = pd.DataFrame(y_evaluation)\nevaluation['0.005'] = model_1.predict(X_evaluation)\nevaluation['0.025'] = model_2.predict(X_evaluation)\nevaluation['0.165'] = model_3.predict(X_evaluation)\nevaluation['0.250'] = model_4.predict(X_evaluation)\nevaluation['0.500'] = model_5.predict(X_evaluation)\nevaluation['0.750'] = model_6.predict(X_evaluation)\nevaluation['0.835'] = model_7.predict(X_evaluation)\nevaluation['0.975'] = model_8.predict(X_evaluation)\nevaluation['0.995'] = model_9.predict(X_evaluation)","ac0871e9":"print(\"Accuracy score (training): {0:.3f}\".format(model_8.score(X_train, y_train)))","9eb27bc0":"#print(model_1.score(X_validation,model_1.predict(X_validation)))","c30eb017":"validation","db531e4d":"validation1=validation.drop(['demand'],axis=1)","b2ac30a8":"pv=validation1.values","68b9fec2":"evaluation1=evaluation.drop(['demand'],axis=1)","cc25ebe5":"pe=evaluation1.values","f3861596":"pv","a1c6cce8":"names = [f\"F{i+1}\" for i in range(28)]","a46ff122":"pv = pv.reshape((-1, 28, 9))\npe = pe.reshape((-1, 28, 9))\n\npv","57f0f76d":"piv = pd.DataFrame(ids[:, 0], columns=[\"id\"])","35bca4f7":"piv.head()","f3a2be8f":"QUANTILES = [\"0.005\", \"0.025\", \"0.165\", \"0.250\", \"0.500\", \"0.750\", \"0.835\", \"0.975\", \"0.995\"]\nVALID = []\nEVAL = []\n\nfor i, quantile in tqdm(enumerate(QUANTILES)):\n    t1 = pd.DataFrame(pv[:,:, i], columns=names)\n    t1 = piv.join(t1)\n    t1[\"id\"] = t1[\"id\"] + f\"_{quantile}_validation\"\n    t2 = pd.DataFrame(pe[:,:, i], columns=names)\n    t2 = piv.join(t2)\n    t2[\"id\"] = t2[\"id\"] + f\"_{quantile}_evaluation\"\n    VALID.append(t1)\n    EVAL.append(t2)","8ebaba02":"sub = pd.DataFrame()\nsub = sub.append(VALID + EVAL)\ndel VALID, EVAL, t1, t2","bba05049":"sub","aecb5fe1":"sub.to_csv(\"submission_Total.csv\", index=False)","f394f0d9":"As we are asked to predict a time window of 28 days, the easiest way to go now is to use the last 28 days for validation:","8b2f58cc":"**Creating dataframe**","b6e85c89":"# Here we have used Gradient Bossting aaproach Total Sales cases:\n\n<hr>\n\n1. Data Preparation for all levels of Aggregation is done at this notebook: https:\/\/www.kaggle.com\/kamalnaithani\/m5unceratinityadddata\n2. With this approach we can merge sales level data as well\n3. There will be seperate model for all aggregated form in same way as we have calculated for Total sales, in same it can be done for category, store-category etc\n4. Applying gradient boosting approcah and mergin the output of all final models\n5. Do upvote in case you find this notebook helpful\n\n\n"}}