{"cell_type":{"b03a6f0a":"code","34b9a624":"code","a2820362":"code","dc41e94f":"code","5123e578":"code","b37e1c8b":"code","7b4c0129":"code","67f2c432":"code","895a50cd":"code","94bc388a":"code","1b21c476":"code","455e8ee8":"code","5592f2b4":"code","91a826ee":"code","f57e96b4":"code","04ce0e27":"code","fa9e0a87":"code","e469c2a7":"code","832a12fa":"code","8c628af9":"code","4aefd882":"code","dcd4326c":"code","da99f324":"code","68062a6c":"code","207ae4c4":"code","365491b7":"code","8629418b":"code","0ddd5fa9":"code","16c9032e":"code","a75a2da5":"code","71992047":"code","0d39d533":"code","b0b3200c":"code","4005b3f9":"code","a5ee8bde":"code","bf39af3d":"code","bf43e31c":"code","203ac5fb":"code","b8f97d53":"code","f7b78dc6":"code","ce91a0b7":"code","1b83e34a":"code","97ab946d":"code","d00b3b70":"code","6aab498c":"code","b8d57838":"code","653b7eae":"code","1dc4624f":"code","21165d40":"code","c51d717c":"code","3a2894d7":"code","09c96b8d":"code","2df5fd35":"code","9e7de839":"code","d6937b6d":"code","57da2102":"code","581bed2c":"code","85979526":"code","5f85e137":"code","6516adf7":"code","8ceae3f2":"code","17226714":"code","0c892b89":"code","55e36014":"code","cf017db2":"code","791118aa":"code","f508973f":"code","35c10f88":"code","0f809a95":"code","77f1e5bd":"code","0bef2d51":"code","8cfb1323":"code","19199ade":"code","8d03bb02":"code","be4757c7":"code","7397250a":"code","7fef8389":"code","56f4fd89":"code","c8ba7156":"code","1c06238c":"markdown","6d69996f":"markdown","9eb707e3":"markdown","38321cec":"markdown","194981c0":"markdown","d9db7d86":"markdown","0e9b3374":"markdown","0440768b":"markdown","47c6ce5d":"markdown","b660a7ce":"markdown","4f314b31":"markdown","8ec3b53a":"markdown","93cbb1fc":"markdown","fc59a076":"markdown","74d672db":"markdown","05b7cfa8":"markdown","fdbd6a5a":"markdown","e1cb45d8":"markdown","df45095b":"markdown","82cb70d4":"markdown","61886642":"markdown","849b6397":"markdown","c711c8cf":"markdown","cdb8a31c":"markdown","6ebeb146":"markdown","17da5259":"markdown","ec43b3d7":"markdown","a9fde098":"markdown","97d10e3b":"markdown","bc11d5bc":"markdown","4e435a8f":"markdown","259e83fc":"markdown","bb0f6891":"markdown","c502af49":"markdown","ae2654ab":"markdown","4f60a4f7":"markdown","09c326df":"markdown","cabf0c79":"markdown","5285ae42":"markdown","1e7c8674":"markdown","c8d00828":"markdown"},"source":{"b03a6f0a":"# For linear algebra\nimport numpy as np  \n\n# For EDA and cleaning the data\nimport pandas as pd\n\n# For visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# For building a model\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport lightgbm\nimport xgboost\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder, LabelBinarizer\n\nimport warnings\nwarnings.filterwarnings('ignore')","34b9a624":"train_df = pd.read_csv('..\/input\/train.csv')","a2820362":"train_df.head()","dc41e94f":"train_df.shape","5123e578":"train_df.info()","b37e1c8b":"train_df.describe()","7b4c0129":"train_df.isnull().sum()","67f2c432":"# replacing null values with median in Age column\ntrain_df['Age'].fillna(train_df['Age'].median(), inplace=True)\n\n# replacing null values with mode in Embarked column\ntrain_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace=True)\n\n# we will drop Cabin column from our data\ntrain_df.drop('Cabin', axis=1, inplace=True)","895a50cd":"print(\"Columns with their count of null values: \")\ntrain_df.isnull().sum()","94bc388a":"train_df.head()","1b21c476":"print(train_df.Survived.value_counts())\nsns.countplot(x='Survived', data=train_df, palette='rainbow')","455e8ee8":"sns.countplot(x='Pclass',hue='Survived',data=train_df)","5592f2b4":"sns.barplot(x='Sex', y='Survived', data=train_df)","91a826ee":"sns.violinplot(x='Age', data=train_df, palette='Greens_r')","f57e96b4":"sns.barplot(x='Survived', y='Age', data=train_df, palette='rocket_r')","04ce0e27":"sns.countplot(x='SibSp', hue='Survived',data=train_df, palette='binary_r')","fa9e0a87":"sns.catplot(x='Parch', data=train_df, kind='count', col='Survived')","e469c2a7":"sns.barplot(x='Embarked', y='Survived', data=train_df, palette='Spectral')","832a12fa":"train_df.head()","8c628af9":"train_df.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)","4aefd882":"train_df.head()","dcd4326c":"lb = LabelEncoder()","da99f324":"lb.fit(train_df.Embarked)","68062a6c":"train_df['Embarked'] = lb.transform(train_df.Embarked)","207ae4c4":"train_df.head()","365491b7":"lb.classes_","8629418b":"train_df = pd.get_dummies(train_df) # One-Hot Encoding is also called dummy encoding, we use pd.get_dummies func","0ddd5fa9":"train_df.head()","16c9032e":"# X contains all the columns except the Survived columns, becuase predictions will be made on Survived column\n# Y contains only the Survived column\n# Note: the column we are going to predict is also called target\n\nX = train_df.drop('Survived', axis=1)\ny = train_df.Survived","a75a2da5":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","71992047":"print(f\"Training size: {X_train.shape[0]}\")\nprint(f\"Testing size: {X_test.shape[0]}\")","0d39d533":"gbm = GradientBoostingClassifier(n_estimators=1000)","b0b3200c":"gbm.fit(X_train, y_train)","4005b3f9":"gbm_preds = gbm.predict(X_test)","a5ee8bde":"metrics.accuracy_score(y_test, gbm_preds)","bf39af3d":"rfc = RandomForestClassifier(n_jobs=2, n_estimators=500, oob_score=True)","bf43e31c":"rfc.fit(X_train, y_train)","203ac5fb":"rfc_preds = rfc.predict(X_test)","b8f97d53":"metrics.accuracy_score(y_test, rfc_preds)","f7b78dc6":"lgbm = lightgbm.LGBMClassifier()","ce91a0b7":"lgbm.fit(X_train, y_train)","1b83e34a":"lgbm_preds = lgbm.predict(X_test)","97ab946d":"metrics.accuracy_score(y_test, lgbm_preds)","d00b3b70":"xgb = xgboost.XGBClassifier(n_jobs=2, n_estimators=500, base_score=0.7)","6aab498c":"xgb.fit(X_train, y_train)","b8d57838":"xgb_preds = xgb.predict(X_test)","653b7eae":"metrics.accuracy_score(y_test, xgb_preds)","1dc4624f":"etc = ExtraTreesClassifier(n_jobs=2, bootstrap=True, oob_score=True, verbose=2, n_estimators=1000)","21165d40":"etc.fit(X_train, y_train)","c51d717c":"etc_preds = etc.predict(X_test)","3a2894d7":"metrics.accuracy_score(y_test, etc_preds)","09c96b8d":"adbc = AdaBoostClassifier(n_estimators=500, learning_rate=0.04)","2df5fd35":"adbc.fit(X_train, y_train)","9e7de839":"adbc_preds = adbc.predict(X_test)","d6937b6d":"metrics.accuracy_score(y_test, adbc_preds)","57da2102":"dtc = DecisionTreeClassifier()","581bed2c":"dtc.fit(X_train, y_train)","85979526":"dtc_preds = dtc.predict(X_test)","5f85e137":"metrics.accuracy_score(y_test, dtc_preds)","6516adf7":"lg = LogisticRegression(max_iter=1000, verbose=4, n_jobs=3, dual=True)","8ceae3f2":"lg.fit(X_train, y_train)","17226714":"lg_preds = lg.predict(X_test)","0c892b89":"metrics.accuracy_score(y_test, lg_preds)","55e36014":"test_df = pd.read_csv('..\/input\/test.csv')","cf017db2":"test_df.head()","791118aa":"PassengerId = test_df.PassengerId","f508973f":"test_df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)","35c10f88":"test_df.head()","0f809a95":"# replacing null values with median in Age column\ntest_df['Age'].fillna(train_df['Age'].median(), inplace=True)\n\n# replacing null values with mode in Embarked column\ntest_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace=True)","77f1e5bd":"lb.fit(test_df.Embarked)","0bef2d51":"test_df.Embarked = lb.transform(test_df.Embarked)","8cfb1323":"test_df = pd.get_dummies(test_df)","19199ade":"test_df.head()","8d03bb02":"predictions = lgbm.predict(test_df)","be4757c7":"predictions","7397250a":"submit_df = pd.DataFrame()","7fef8389":"submit_df['PassengerId'] = PassengerId\nsubmit_df['Survived'] = predictions","56f4fd89":"submit_df.head()","c8ba7156":"submit_df.to_csv('submission.csv', index=False)","1c06238c":"### Training the model","6d69996f":"# Problem Statement","9eb707e3":"We will train our model on <b>712<\/b> rows<br>\nWe will test our model on <b>179<\/b> rows<\/b>","38321cec":"#### ExtraTreeClassifier","194981c0":"***0 -> did not survived<br>\n***1 -> survived<br>\n<b>549<\/b> people <b> did not survived<\/b><br>\n<b>342<\/b> people <b>survived<b><br>","d9db7d86":"Before moving on to <b>EDA<\/b>, ley's clean the data.","0e9b3374":"#### RandomForestClassifier","0440768b":"<b>Age column has 177 null values<\/b><br>\n<b>Cabin column has 687 null values<\/b><br>\n<b>Embarked column has 2 null values<\/b><br>","47c6ce5d":"#### GradientBoostingClassifier","b660a7ce":"<b>Most machine learning algorithms can't work on text data. So, we need to convert text to numbers.<\/b><br>","4f314b31":"***Pclass -> Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd***<br>\nPeople who were in <b>Pclass 1<\/b> survived the most<br>\nMost of the people from <b>Pclass 3<\/b> did not survived<br>\n\n<b>Reason:<\/b><br>\n- One reason could be more priority was given to Pclass 1 people than Pclass 3 people<br>","8ec3b53a":"Our data is clean now","93cbb1fc":"### Miscellaneous","fc59a076":"***SibSp -> Number of siblings \/ spouses aboard the Titanic*** <br>\nMost of the people who <b>survived had no Siblings\/Spouses<\/b><br>\nPeople with more Siblings\/Spouses were not able to survive<br>","74d672db":"Nominal columns: <b>Sex<\/b><br>\nOrdinal columns: <b>Embarked<\/b><br>","05b7cfa8":"<b>Statistics<\/b> about the numerical columns present in the data","fdbd6a5a":"There are many techniques, mostly used ones are described below:\n- Use <b>Label Encoding<\/b> for ordinal(Which has order) columns\n- Use <b>One-Hot Encoding<\/b> for nominal(which doesn't have any order in them) columns<br>\nHere's a link to learn about ordinal and nominal columns:\nhttps:\/\/sciencing.com\/difference-between-nominal-ordinal-data-8088584.html","e1cb45d8":"### Splitting the data","df45095b":"How does One-Hot Encoding work?<br>\n- It extracts the all the <b>categories<\/b> and makes them columns. In our case Male and Female<br>\n- Whenever there occurs a <b>Female<\/b> in the Sex column, it places <b>1<\/b> in the <b>Sex_female<\/b> column and <b>0<\/b> in the <b>Sex_male<\/b> column<br>\nLink to learn more about One-Hot Encoding: https:\/\/www.quora.com\/What-is-one-hot-encoding-and-when-is-it-used-in-data-science","82cb70d4":"- We'll use many models to train on and choose the one which gives the best accuracy","61886642":"Features going to be removed\n- PassengerId\n- Name\n- Ticket","849b6397":"- In this challenge we need to predict whether a passenger survived or did not survived on the Titanic\n- This is a <b>Classification<\/b> problem\n- URL of the dataset https:\/\/www.kaggle.com\/c\/titanic","c711c8cf":"There are <b>891<\/b> rows and <b>12<\/b> columns in the data","cdb8a31c":"#### LogisticRegression","6ebeb146":"### Predicting on Test Data using LGBM","17da5259":"#### XGBClassifier","ec43b3d7":"***Embarked -> Port of Embarkation C = Cherbourg(France), Q = Queenstown(New Zealand), S = Southampton(England)***<br>\nMost of the people who boarded from <b>Cherbourg<\/b> survived the most","a9fde098":"### Removing redundant features","97d10e3b":"## EDA","bc11d5bc":"***Parch -> Number of parents \/ children aboard the Titanic***<br>\nPeople with <b>no Parents\/Children<\/b> Survived the most<br>","4e435a8f":"#### DecisionTreeClassifier","259e83fc":"### Importing Libraries","bb0f6891":"#### LGBMClassifier","c502af49":"<b>Label Encoder replaced 'C' with 0, 'Q' with 1 and 'S' with 2<\/b><br>","ae2654ab":"### Cleaning the data","4f60a4f7":"There are <b>null<\/b> values in the columns <b>Age<\/b>, <b>Cabin<\/b> and <b>Embarked<\/b><br>\nThere are <b>seven numerical<\/b> columns<br>\nThere are <b>five categorical<\/b> columns<br>","09c326df":"<b>Females survived<\/b> the most<br>\n<b>Reason:<\/b><br>\n\n- Usually, females are given more priority than males. Because, naturally males are stronger than females.","cabf0c79":"### Loading the data","5285ae42":"### Converting categorical features to numerical features","1e7c8674":"#### AdaBoostClassifier","c8d00828":"<b>Most of the people in the dataset are between the ages 20 and 40<\/b><br>"}}