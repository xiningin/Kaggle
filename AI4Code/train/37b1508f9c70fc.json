{"cell_type":{"daaa21c8":"code","43c1b2de":"code","add27597":"code","69ed4b79":"code","bb1837af":"code","1853c1b9":"code","154a87bb":"code","314b9fb6":"code","0d9a111c":"code","b4fcca6b":"code","b692c785":"code","01a48c29":"code","5acfda60":"code","6f8cec5d":"markdown","e71b7b77":"markdown","6221241f":"markdown","a1bf7af5":"markdown","59384035":"markdown","b86bf65b":"markdown","1f8b6605":"markdown","d943fdc9":"markdown","f3471220":"markdown","5c4deda2":"markdown","bd454a9e":"markdown"},"source":{"daaa21c8":"import pandas as p\ncal=p.read_csv('..\/input\/california-housing-prices\/housing.csv')\n\n# Remove the ocean_proximity feature\n\ncal = cal.drop(['ocean_proximity'],axis=1)\nprint(cal.head())","43c1b2de":"# Remove missing values\n\nprint(\"Number of rows before removing the missing data:\")\nprint(cal.shape[0])\ncal=cal.dropna()\nprint(\"Number of rows after removing the missing data:\")\nprint(cal.shape[0])","add27597":"# Remove the outliers in the price feature if there is any\n\nQL1=cal.quantile(0.25)\nQL3=cal.quantile(0.75)\nIQRL=QL3-QL1\nFLL=QL1-(1.5*IQRL)\nFHL=QL3+(1.5*IQRL)\ncal=cal.loc[(cal>FLL).all(axis=1) & (cal<FHL).all(axis=1)]\nprint(\"Number of rows after removing the outliers in the price feature:\")\nprint(cal.shape[0])","69ed4b79":"# Find any duplicate rows and remove them\n\ncalc = cal.duplicated()\nprint('Number of duplicate rows = %d' % (calc.sum()))\ncal=cal.drop_duplicates()\nprint(\"Number of rows after removing the duplicate data:\")\nprint(cal.shape[0])","bb1837af":"# Standardaize the features\n\nfrom sklearn.preprocessing import StandardScaler\nx = cal.iloc[:, :].values\ny = cal.iloc[:,].values\nx = StandardScaler().fit_transform(x)\nx\nfeatures = ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', 'median_house_value']\nprint(p.DataFrame(data = x, columns = features).head())","1853c1b9":"# Apply PCA and reduce the dimensions to 2\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(x)\nprincipalComponents\nprincipalDf = p.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\nprint(principalDf.head())","154a87bb":"# Do a scatter plot for the data on the two principal components.\n\n# Note: code looks different than the code shown in the class tutorial because the data of this example is quantitative while the data shown in the class tutorial were categorical\n\nimport plotly.express as pl\nplot = pl.scatter(principalDf, x='principal component 1', y='principal component 2').update_traces(marker=dict(color='blue'))\nplot.show()","314b9fb6":"import pandas as p\nds=p.read_csv('..\/input\/pfizer-vaccine-tweets\/vaccination_tweets.csv')\n\n# Remove any punctuation\n\nimport string\npunc = \"\\n\\r\"+string.punctuation\nds['text'] = ds['text'].str.translate(str.maketrans('', '', punc))  \nprint(ds['text'].head())","0d9a111c":"# Remove the stop words\n\nfrom nltk.corpus import stopwords\ns = stopwords.words('english')\nds['text'] = ds[\"text\"].str.lower().str.split()\nds['text'] = ds['text'].apply(lambda n: ' '.join([i for i in n if i not in s]))","b4fcca6b":"# Apply stemming and lemmatization\n\nimport re\nimport nltk\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nstemmer = PorterStemmer()\nwordnet_lemmatizer = WordNetLemmatizer()\ndef tokenize(str_input):  \n    words = re.sub(r\"(?u)[^A-Za-z]\", \" \", str_input).lower().split(\" \")\n    words = [stemmer.stem(word) for word in words if len(word)>2]\n    words = [wordnet_lemmatizer.lemmatize(word) for word in words if len(word)>2]\n    return words\nvectorizer = TfidfVectorizer(tokenizer=tokenize)\nvectors = vectorizer.fit_transform(ds['text'])\nfeature_names = vectorizer.get_feature_names()","b692c785":"# Vectorize the text of the tweets and put it in a dataframe with applying the TF_IDF weights\n\ndf = p.DataFrame(vectors.toarray(),columns=feature_names)\nprint(df.head())","01a48c29":"# Find the similarity between the tweets and the query \"vaccine is deadly\"\n\nq=\"vaccine is deadly\"\nq2=vectorizer.transform([q])\nq3=p.DataFrame(q2.toarray(), columns=feature_names)\nfrom sklearn.metrics.pairwise import cosine_similarity\ncosinesim = cosine_similarity(df, q3)\nprint(cosinesim)","5acfda60":"# Print the top 10 most similar tweets to the query in the order of similarity\n\nimport numpy as n\ncossim = cosinesim.flatten()\npar = n.argpartition(cossim, -10)[-10:]\nsort = par[n.argsort(cossim[par])]\nsim = cossim[sort]\nsim2 = ds['text'].iloc[sort]\nsimten = p.DataFrame({'similarity': sim, 'text': sim2}).reset_index(drop=True)\nsimten2 = simten.sort_values(by='similarity', ascending=False)\nprint(simten2)","6f8cec5d":"3. Use the below model to predict the credit worthiness of the following clients:\n    * Steve is employed, has a high school degree and spent 5 years at his current address.\n    * Andy has a graduate degree, spent 8 years at his current address but is not employed.\n\n![image.png](attachment:image.png)","e71b7b77":"Answer:\n\nSteve is NOT credit worthy because even though he is employed and has a high school degree, he spent LESS than 7 years at his current address, which deemed him unworthy of the credit.\n\nAndy is also NOT credit worthy because even though he has a graduate degree and spent more than 3 years at his current address, he is NOT employed, which deemed him unworthy of the credit.","6221241f":"### This is assignment is due on Tuesday 26-1-2021 at midnight","a1bf7af5":"5. What is the purpose of Information Retrieval","59384035":"Answer:\n\nInformation retrieval provides organizations with immediate value. While it is important to try and figure out methods to capture tacit knowledge, information retrieval can provide a way to get at information that already exist electronically.","b86bf65b":"Answer:\n\nPerson height: categorical-ordinal (if assigned as tall, meduim, short) or numerical-continuous (if assigned as real numbers).\n\nRanking of a product: categorical-ordinal. \n\nNumber of children: numerical-discrete\n\nCar model: categorical-nominal. ","1f8b6605":"4. What type of feature are the following (categorical-nominal, categorical-ordinal, numerical-discrete, numerical-continuous):\n    - Person height\n    - Ranking of a product\n    - Number of children\n    - Car model","d943fdc9":"Question #1 Answer:\n\n1- Predictive modeling (includes classification and regression):\n\nIn this task, some variables are used to predict unknown or future values of other variables. \nClassification is used to find a model for class attribute as a function of the values of other attributes. \nRegression is used to predict a value of a given continuous valued variable based on the values of other variables, assuming a linear or nonlinear model of dependency.\n\n2- Clustering:\n\nClustering is used to find groups of objects such that the objects in a group will be similar or related to each other, and different from, or unrelated to the objects in the other groups.\n\n3- Association rules discovery:\n\nAssociation rules discovery is used to produce dependency rules that will predict occurrence of an item based on occurrences of other items when given a set of records each of which contain a number of items from a given collection.\n\n4- Deviation\/Anomaly\/Change detection: \n\nDeviation\/Anomaly\/Change detection is used to detect significant deviations from normal behavior (e.g., network intrusion detection).\n\nQuestion #2 Answer:\n\nExamples of classification:\n1- Predicting tumor cells either as benign or malignant.\n2- Classifying credit cards transactions either as legitimate or fraudulent.\n\nExamples of regression:\n1- Predicting wind velocities as a function of temperature, air pressure, etc.\n2- Predicting sales amounts of new products based on advertising expenditure.\n\nExamples of clustering:\n1- Grouping genes and proteins that have similar functionalities.\n2- Doing customer profiling for targeted marketing.\n","f3471220":"Answer the following questions:\n\n1. Describe the four main data mining tasks.\n2. Give two examples of the following data mining tasks:\n    * Classification\n    * Regression\n    * Clustering ","5c4deda2":"7. Use the pfizer-vaccine-tweets dataset and do the following on the tweet text feature:\n\n    - Remove any punctuation.\n    - Remove the stop words.\n    - Apply stemming and lemmatization.\n    - Vectorize the text of the tweets and put it in a dataframe with applying the TF_IDF weights.\n    - Find the similarity between the tweets and the query \"vaccine is deadly\".\n    - Print the top 10 most similar tweets to the query in the order of similarity.","bd454a9e":"6. Use the california housing prices dataset and do the following:\n\n    - Remove the ocean_proximity feature\n    - Standardaize the features.\n    - Remove the outliers.\n    - Remove the missing values (if any).\n    - Remove the duplicate rows (if any).\n    - Apply PCA and reduce the dimensions to 2.\n    - Do a scatter plot for the data on the two principal components.\n    "}}