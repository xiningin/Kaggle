{"cell_type":{"e381faee":"code","84a50f69":"code","1dd8b2db":"code","0d0b5795":"code","f42a6fd2":"code","af72d56a":"code","9ea5481d":"code","dd74fde6":"code","296153d9":"code","3e504590":"code","75f2fdf0":"code","e819a3cb":"code","8970e5fe":"code","7b86402d":"code","8c4932b1":"code","5b241e92":"code","de110e6e":"code","c3b60e49":"code","b12f6a74":"code","90db5727":"code","9352a4f6":"code","7365ae91":"code","bb4f34b1":"code","c563c24a":"code","25a1a9cd":"code","aab198ab":"code","7ff1c868":"code","379518df":"code","f2275694":"code","79c07e7f":"code","854fdea4":"markdown","7c1c2ef5":"markdown","10e5ce5c":"markdown","8abd652f":"markdown","153a122e":"markdown","49891bab":"markdown","ad1f7d4e":"markdown","ab96addd":"markdown","911e6a60":"markdown","acfd2f9f":"markdown","d0d41a79":"markdown","9a508017":"markdown","ece8c7b9":"markdown","cee8aefc":"markdown","d36f63aa":"markdown","8f740eb5":"markdown","65bad86c":"markdown","235b8e1c":"markdown","beb8946f":"markdown","c1746b70":"markdown"},"source":{"e381faee":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","84a50f69":"# Plot Style\nplt.rcParams.update({'font.size': 14})","1dd8b2db":"# Load train and test data\nraw_mydata = pd.read_csv('..\/input\/train.csv', index_col='Id')","0d0b5795":"# Examine data types\nraw_mydata.info()","f42a6fd2":"# Function to transform housing data\ndef transform_house(data):\n    \n    # fill nan with 0\n    data = data.fillna(0)\n    \n    # List of categorical, non-numeric variables\n    dummy_list = ['MSSubClass', # though numeric in original data, it is categorical\n                  'MSZoning', \n                  'Street', \n                  'Alley', \n                  'LotShape', \n                  'LandContour', \n                  'Utilities',\n                  'LotConfig',\n                  'LandSlope',\n                  'Neighborhood',\n                  'Condition1',\n                  'Condition2',\n                  'BldgType',\n                  'HouseStyle',\n                  'RoofStyle',\n                  'RoofMatl',\n                  'Exterior1st',\n                  'Exterior2nd',\n                  'MasVnrType', # Must be used if we use MasVnrArea\n                  'ExterQual',\n                  'ExterCond',\n                  'Foundation',\n                  'BsmtQual',\n                  'BsmtCond',\n                  'BsmtExposure',\n                  'BsmtFinType1', # Must be used if we use BsmtFinSF1\n                  'BsmtFinType2', # Must be used if we use BsmtFinSF2\n                  'Heating',\n                  'HeatingQC',\n                  'CentralAir',\n                  'Electrical',\n                  'KitchenQual',\n                  'Functional',\n                  'FireplaceQu',\n                  'GarageType',\n                  'GarageFinish',\n                  'GarageQual',\n                  'GarageCond',\n                  'PavedDrive',\n                  'PoolQC',\n                  'Fence',\n                  'MiscFeature',\n                  'SaleType',\n                  'SaleCondition',\n                  'MoSold',\n                  #'YrSold', we think we should keep year sold as numeric not categorical\n                 ]\n    \n    # create dummy variables\n    for var in dummy_list:\n        data = pd.concat([data, pd.get_dummies(data[var], drop_first=True, prefix=var)], axis=1) \n        \n    # drop dummy tables\n    data = data.drop(dummy_list, axis=1)\n    \n    # Add total squre foot\n    data['TotalSquareFootage'] = data['TotalBsmtSF'] + data['1stFlrSF'] + data['2ndFlrSF']\n    \n    return data.copy()","af72d56a":"# Transform train and test\nmydata = transform_house(raw_mydata)\n\ntest, train  = train_test_split(mydata, test_size=0.8, shuffle=True, random_state=8675309)\n\nprint(train.shape)\nprint(test.shape)","9ea5481d":"train.head()","dd74fde6":"# Examine data\ntrain.describe()","296153d9":"#Correlation heat map (only integer variables)\ncorrmat = raw_mydata.iloc[[x - 1 for x in train.index.tolist()]].corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","3e504590":"#Correlation heat map (with descriptive variables converted to dummy variables)\ncorrmat = train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","75f2fdf0":"plt.scatter(train['GrLivArea'],train['SalePrice'])\nplt.xlabel('GrLivArea')\nplt.ylabel('SalePrice')\nplt.title('Sale Price vs Above grade living area')\nplt.show()","e819a3cb":"train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<400000)].index)\ntest = test.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<400000)].index)","8970e5fe":"feature = train.dtypes[train.dtypes != \"object\"].index\n\n# Check the skew\nskewed = train[feature].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed})\nskewness.head(20)","7b86402d":"# Descriptive statistics\ntrain['SalePrice'].describe()","8c4932b1":"# histogram of Sale Price\nsns.distplot(train['SalePrice']);\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n# QQ plot\nfigure = plt.figure()\nr = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","5b241e92":"# Transform Sale Price to log(Sale Price)\ntrain[\"SalePrice\"] = np.log(train[\"SalePrice\"])\ntest[\"SalePrice\"] = np.log(test[\"SalePrice\"])\n\n# Plot the transformed distribution \nsns.distplot(train['SalePrice'] );\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n# Plot the QQ Plot of the transformed variable\nfigure = plt.figure()\nr = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()\n","de110e6e":"from pandas import DataFrame\ntrain.to_csv ('prepared_traindata.csv', index=False)\ntest.to_csv ('prepared_testdata.csv', index=False)","c3b60e49":"# Scale Train and Test for K-NN model\ntrain_X = train.drop(['SalePrice'], axis=1).copy()\ntrain_Y = np.ravel(train[['SalePrice']])\n\ntest_X = test.drop(['SalePrice'], axis=1).copy()\ntest_Y = np.ravel(test[['SalePrice']])\n\nscaler = StandardScaler()\nscaler.fit(train_X)\ntrain_X_scale = scaler.transform(train_X)\ntest_X_scale = scaler.transform(test_X)\n\n","b12f6a74":"# KNN Grid Search area for best K\nk_range = list(range(1,12))\nparam_grid = dict(n_neighbors = k_range)\n\n# Create model\nknn = KNeighborsRegressor()\n\ngrid = GridSearchCV(knn, param_grid, cv=10, scoring = 'r2', return_train_score=True)\ngrid.fit(train_X_scale, train_Y)\n\nprint(f'The optimal K-Neighbors is {grid.best_params_}.')","90db5727":"# plot k values\nplt.plot(grid.cv_results_['param_n_neighbors'], grid.cv_results_['mean_test_score'])\nplt.xlabel('Value of K for KNN')\nplt.ylabel('Cross-Validated Accuracy')\nplt.title('Neighbor CV-Accuracy for KNN')","9352a4f6":"pd.DataFrame(grid.cv_results_)","7365ae91":"print(f'The optimal K-Neighbors is {grid.best_params_}.')","bb4f34b1":"# Predict train_X_scale\nknn_yhat = pd.DataFrame(data = grid.predict(test_X_scale),\n                    columns = ['knn_yhat'],)\nknn_yhat.to_csv('knn_yhat.csv', index=False)","c563c24a":"# Define pipeline gor best PCA with KNN\npca = PCA()\nknn = KNeighborsRegressor()\nk_range = list(range(1,12))\nn_components = list(range(1,12))\n\npipe = Pipeline(steps=[('pca', pca), ('knn', knn)])\n\n# Create Grid to Search\npara_grid_pca = {'pca__n_components': n_components,\n                'knn__n_neighbors': k_range}\n\n# Create model\ngrid_pca = GridSearchCV(pipe, para_grid_pca, cv=10, scoring = 'r2', return_train_score=True)\ngrid_pca.fit(train_X_scale, train_Y)\n\n\n","25a1a9cd":"# plot k values\nfig = plt.figure()\nfig.set_size_inches(8, 5)\nax = fig.add_subplot(111, projection='3d')\nx,y = np.meshgrid(k_range,n_components)\nz = np.reshape(grid_pca.cv_results_['mean_test_score'], \n                           (len(k_range),\n                            len(n_components)))\n\nax.plot_wireframe(x,y,z, rstride=2, cstride=2)\nax.set_xlabel('k-neighbors')\nax.set_ylabel('PCA-components')\nax.set_zlabel('Cross-Validated Accuracy')\nax.set_title('CV-Accuracy for KNN-PCA')","aab198ab":"pd.DataFrame(grid_pca.cv_results_)","7ff1c868":"print(f'The optimal PCA-components and K-Neighbors is {grid_pca.best_params_}.')","379518df":"# Predict train_X_scale\nknn_pca_yhat = pd.DataFrame(data = grid_pca.predict(test_X_scale),\n                    columns = ['knn_pca_yhat'],)\nknn_pca_yhat.to_csv('knn_pca_yhat.csv', index=False)","f2275694":"# Calculate RMSE (Root Mean Squared Error)\nnp.sqrt(mean_squared_error(test_Y, knn_yhat))","79c07e7f":"np.sqrt(mean_squared_error(test_Y, knn_pca_yhat))","854fdea4":"We therefore clearly see that there is skeweness in our data. **To the team, we can therefore decide whether we wish to apply Box Cox transformation, relatively simple in R if we decide to do so**","7c1c2ef5":"The last step is to analyse the predicted variable, in this case the Sale Price. To do so, we first look at the descriptive statistics, then plot a histogram of the data and a QQ plot to see if the Sale Price is normally distributed, which then helps for our linear model.","10e5ce5c":"## Skewed variables\nBefore moving further, it is also essential to verify that there is no skeweness within the variables. We therefore calculate the skewenes for the variables:","8abd652f":"## More Models to Consider","153a122e":"## Removal of Outliers","49891bab":"**CORRELATION**","ad1f7d4e":"In the section above, we create a function to trasnform our data set by creating dummy indicator variables for predictor variables that are inherently categorical and not to be treated as numerical. For example, the **MSSubClass** variable originally had integer values for distinguishing the \"type of dwelling\" involved in the sale. We should create dummy indiacator variables indicating which type of dwelling the entry is rather than identifying it via an integer value, which could lead to improper regression and less accurate prediction as there are 16 distinct types of dwellings. Note: we also treat ratings of quality (i.e. **ExterQual**, **ExterCond**, **BsmtQual**, **BsmtCond**, ...) with values `Ex` for Excellent, `Gd` for Good, `TA` for Typical, `Fa` for Fair, `Po` for Poor, and `NA` for Not Applicable as categorical rather than on a scale from 0 to 5 (inclusive).\n\nOne of the main considerations, we had to take into account is how we dealt with missing values in the data set. Missing data is found in our data set as either 'NaN' or simply missing values. When deciding on how to deal with it, we identified three pathways: 1. Delete the entire column, 2. Replace the missing values, by values such as the mean or median of that explanatory variable, 3. Set all missing values to 0. After careful deliberation, we understood that deleting entire columns would be detrimental to our data set, since over 20 variables have at least one missing value and we would therefore end up by deleting a massive portion of information from our data. As for replacing them by means or median, we decided against fearing that we would add false information to our data. For example, if 2 floor square foot is a missing value, it is more probable that the unit does not have a second floor rather than the measurement not being recorded. Therefore by replacing NaN by the mean square footage would give a lot of these units false information. For these reasons we decided to replace missing values by 0 as it is the option with the least detrimental impact on the overall dataset.","ab96addd":"TODO: Intro for K-NN model","911e6a60":"The data now clearly looks more normally distributed, and this is apparent in the QQ plot which shows minimal deviation from the line now. We are therefore happy to carry forward with this dataset.","acfd2f9f":"From these two plots, we can tell that the data is not well normally distributed and has decent positive (right) skeweness. To fix this we attempt to modify the target variable, in this case we will try to do a logarithmic transformation.","d0d41a79":"We also poduced the heat map with all the variables including the dummy variables assigned to the different descriptive variables. We notice by the quantity of variables that it becomes extremely difficult to identify high correlations among variables visually.","9a508017":"TODO: Descriptive summary of models about to be ran and why","ece8c7b9":"Looking at the [documentation](http:\/\/jse.amstat.org\/v19n3\/decock.pdf) of the data set, the author has a note about potential outliers written below:\n\n\"Potential Pitfalls (Outliers): Although all known errors were corrected in the data, no\nobservations have been removed due to unusual values and all final residential sales\nfrom the initial data set are included in the data presented with this article. There are\nfive observations that an instructor may wish to remove from the data set before giving\nit to students (a plot of SALE PRICE versus GR LIV AREA will quickly indicate these\npoints). Three of them are true outliers (Partial Sales that likely don\u2019t represent actual\nmarket values) and two of them are simply unusual sales (very large houses priced\nrelatively appropriately). I would recommend removing any houses with more than\n4000 square feet from the data set (which eliminates these five unusual observations)\nbefore assigning it to students.\" (De Cock 2011, Journal of Statistics Education, Volume 19, Number 3)\n\nFollowing the direction of the author, we look at the plot of **SalePrice** vs. **GrLivArea**, identify the outliers with high leverage and remove the ones with >4,000 square feet of **GrLivArea** from the training set.","cee8aefc":"TODO: Intro for K-NN model with PCA (Pricipal Component Analysis)","d36f63aa":"We now apply the transfor function to our train data set. Additionally, we assumed that their might be high correlation between Basement, 1st floor and 2nd floor surface area. Therefore due to multicolinearity, it is likely for one of these variables to be droped down the road and we would loose some useful information about the overall size of the house. We therefore decided to add a variable to the data set that measures the total surface area of the house.","8f740eb5":"Above, we see a correlation matrix\/heat map between all the numeric predictor variables, where purple pixels show low correlation and light ones show high correlation. Even though the last row give us a good idea of the correlation between the predicted variable and its predictors, it is more interesting to focus on the correlation among predictor variables in order to identify the possibility of multicolinearity. As we see above,there are a few highly correlated variables we might want to drop in our model choice: 1st floor and 2nd floor surface area; Garage cars and Garage area; Total rooms above ground and above ground living area. Within all these pairs we could suspect multicolinearity going forth, so could either drop one of them, or create some combination of each element of the pair.","65bad86c":"# Statistics 109 Final Project\n## Team Name: House Hunters","235b8e1c":"## _Data Preparation Stage_","beb8946f":"## Predicted Variable: Sale Price","c1746b70":"The two outliers we wish to remove are those with very low **SalePrice** relative to their **GrLivArea** square footage that are inconsistent with the trend of the rest of the data."}}