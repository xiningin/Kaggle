{"cell_type":{"e0363686":"code","d8301621":"code","2504f2ab":"code","5a027df2":"code","03d456c3":"code","890f55cc":"code","9707d273":"code","94c09734":"code","ee969f56":"code","defa6add":"code","83aacd18":"code","45da29eb":"code","2a711d1d":"code","5cf78393":"code","dbddb7cd":"code","001c2821":"code","d4abb7ab":"code","0afe8384":"code","f39c8981":"code","04bdde2c":"code","1129a032":"code","8458cb4e":"code","1f54a163":"code","f8035b34":"code","a8e62506":"code","63bb3d1a":"code","1a31c237":"code","f4db2415":"code","3cb8835e":"code","92bff50e":"code","1b23b82d":"code","033de198":"code","7ffa23d5":"code","34a4d8c4":"code","902ac368":"code","159cb62e":"code","b60b25f9":"code","17804df7":"code","4699a3f7":"code","dd0fc3d4":"code","04b98f24":"code","84474e37":"code","26f02057":"code","ee8505af":"code","f6a855a7":"code","e42c4ba1":"code","5f4e5c51":"code","1812f68c":"code","ce7bd392":"code","3a28b4e3":"code","b519d68c":"code","ee625b5d":"code","baf97dae":"markdown","1635eb6b":"markdown","1ed5a2ee":"markdown","9950de0e":"markdown","2ebd8312":"markdown","753305e0":"markdown","f09f647a":"markdown","69d7c550":"markdown","a9993c5b":"markdown","51ab1d5b":"markdown","9bc4da6c":"markdown","4d57ab2c":"markdown","dceaf351":"markdown","3e14ef88":"markdown","bf54fad6":"markdown","60425022":"markdown","ebea352b":"markdown","a012926e":"markdown","a119b0f5":"markdown","3dfa71eb":"markdown","62a3272b":"markdown","117a57f9":"markdown","96940bc4":"markdown","7e4be2e8":"markdown","6a4550e4":"markdown","242fff92":"markdown","7aecb534":"markdown","393fb080":"markdown","38f9f152":"markdown","e1d15845":"markdown","afe545e3":"markdown","78b1c80c":"markdown","9716de97":"markdown","c2852924":"markdown","5fa28d6a":"markdown","bae9d1d4":"markdown","157d5251":"markdown","c734ee7c":"markdown","f807c069":"markdown","34487d13":"markdown","e9da358f":"markdown","6b00ed46":"markdown","feac18dd":"markdown","a8a471b1":"markdown","0b19fa05":"markdown","3e0e11f2":"markdown","3eb5a39b":"markdown","d438ce8b":"markdown","92d88915":"markdown","c4822f54":"markdown","85244978":"markdown","7323faf7":"markdown","e47e3a2a":"markdown"},"source":{"e0363686":"# Supress Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#core imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d8301621":"#load training dataset and assign it to a variable\ntrain=pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest=pd.read_csv(\"..\/input\/titanic\/test.csv\")","2504f2ab":"#use the 'head' method to show the first five rows of the table as well as their names. \ntrain.head()","5a027df2":"# shape of the Titanic train dataframe\ntrain.shape","03d456c3":"train.info()","890f55cc":"print('Avgerage. Age : ',train['Age'].mean())\n","9707d273":"plt.figure(figsize=(8,4))\nfig = sns.distplot(train['Age'], color=\"darkorange\")\nfig.set_xlabel(\"Age\",size=15)\nfig.set_ylabel(\"Density of Passengers\",size=15)\nplt.title('Passenger Age Distribution',size = 20)\nplt.show()","94c09734":"print('Avgerage. Fare : ',train['Fare'].mean())","ee969f56":"plt.figure(figsize=(8,4))\nfig = sns.distplot(train['Fare'], color=\"darkorange\")\nfig.set_xlabel(\"Fare\",size=15)\nfig.set_ylabel(\"Density of Passengers\",size=15)\nplt.title('Titanic Fare Distribution',size = 20)\nplt.show()","defa6add":"plt.figure(figsize=(8,4))\nfig = sns.distplot(train[train['Fare']<=150]['Fare'], color=\"darkorange\")\nfig.set_xlabel(\"Fare\",size=15)\nfig.set_ylabel(\"Density of Passengers\",size=15)\nplt.title('Titanic Fare(Scaled) Distribution',size = 20)\nplt.show()","83aacd18":"#WE PLOT THE PASSENGER AGE DISTRIBUTION VS PASSENGER CLASS ON TITANIC\nplt.figure(figsize=(8,4))\nfig=sns.boxplot(train['Pclass'],train['Age'],palette='Blues')\nfig.set_xlabel(\"Passenger Class\",size=15)\nfig.set_ylabel(\"Age of Passenger\",size=15)\nplt.title('Age Distribution\/Pclass',size = 20)\nplt.show()","45da29eb":"#WE PLOT THE PASSENGER AGE DISTRIBUTION VS PASSENGER Gender ON TITANIC\nplt.figure(figsize=(8,4))\nfig=sns.boxplot(train['Sex'],train['Age'],palette='Blues')\nfig.set_xlabel(\"Gender\",size=15)\nfig.set_ylabel(\"Age of Passenger\",size=15)\nplt.title('Age Distribution\/Gender',size = 20)\nplt.show()","2a711d1d":"#WE PLOT THE PASSENGER AGE DISTRIBUTION VS SURVIVED ON TITANIC\nplt.figure(figsize=(8,4))\nfig=sns.boxplot(train['Survived'],train['Age'],palette='Blues',labels=[\"No\"])\nfig.set_xlabel(\"Survived\",size=15)\nfig.set_ylabel(\"Age of Passenger\",size=15)\nplt.title('Age Distribution\/Survived',size = 20)\nfig.set(xticklabels=[\"0-No\",\"1-Yes\"])\nplt.show()","5cf78393":"#WE PLOT THE FARE DISTRIBUTION VS PASSENGER CLASS ON TITANIC\nplt.figure(figsize=(8,4))\nfig=sns.boxplot(train['Pclass'],train['Fare'],palette='Reds')\nfig.set_xlabel(\"Passenger Class\",size=15)\nfig.set_ylabel(\"Fare\",size=15)\nplt.title('Fare Distribution\/Pclass',size = 20)\nplt.show()","dbddb7cd":"#WE PLOT THE FARE DISTRIBUTION VS PASSENGER CLASS ON TITANIC\nplt.figure(figsize=(8,4))\nfig=sns.boxplot(train['Pclass'],train[train['Fare']<=300]['Fare'],palette='Reds')\nfig.set_xlabel(\"Passenger Class\",size=15)\nfig.set_ylabel(\"Fare\",size=15)\nplt.title('Fare(Scaled) Distribution\/Pclass',size = 20)\nplt.show()","001c2821":"#WE PLOT THE FARE DISTRIBUTION VS PASSENGER CLASS ON TITANIC\nplt.figure(figsize=(8,4))\nfig=sns.boxplot(train['Survived'],train[train['Fare']<=300]['Fare'],palette='Reds')\nfig.set_xlabel(\"Survived\",size=15)\nfig.set_ylabel(\"Fare\",size=15)\nfig.set(xticklabels=[\"0-No\",\"1-Yes\"])\nplt.title('Fare(Scaled) Distribution\/Survived',size = 20)\nplt.show()","d4abb7ab":"plt.figure(figsize=(8,4))\nfig=sns.violinplot(train[\"Age\"],train[\"Sex\"], hue=train[\"Survived\"],split=True,palette='Reds')\nfig.set_ylabel(\"Sex\",size=15)\nfig.set_xlabel(\"Age\",size=15)\nplt.title('Age and Sex vs Survived',size = 20)\nplt.show()","0afe8384":"plt.figure(figsize=(8,4))\nfig=sns.violinplot(train[\"Pclass\"],train['Age'], hue=train[\"Survived\"],split=True,palette='Blues')\nfig.set_xlabel(\"Pclass\",size=15)\nfig.set_ylabel(\"Age\",size=15)\nplt.title('Age and Pclass vs Survived',size = 20)\nplt.show()","f39c8981":"bg_color = (0.25, 0.25, 0.25)\nsns.set(rc={\"font.style\":\"normal\",\n            \"axes.facecolor\":bg_color,\n            \"figure.facecolor\":bg_color,\"text.color\":\"white\",\n            \"xtick.color\":\"white\",\n            \"ytick.color\":\"white\",\n            \"axes.labelcolor\":\"white\"})\nplt.figure(figsize=(8,4))\nfig=sns.countplot(train['Survived'],hue=train['Pclass'],palette='Blues',saturation=0.8)\nfig.set_xlabel(\"Survived\",size=15)\nfig.set_ylabel(\"#\",size=15)\nfig.set(xticklabels=[\"0-No\",\"1-Yes\"])\nplt.title('# of Survived\/PClass',size = 20)\nplt.show()\n","04bdde2c":"plt.figure(figsize=(8,4))\nfig=sns.countplot(train['Survived'],hue=train['Sex'],palette='Oranges',saturation=0.8)\nfig.set_xlabel(\"Survived\",size=15)\nfig.set_ylabel(\"#\",size=15)\nfig.set(xticklabels=[\"0-No\",\"1-Yes\"])\nplt.title('# of Survived\/Sex',size = 20)\nplt.show()\n","1129a032":"plt.figure(figsize=(8,4))\nfig=sns.countplot(train['Survived'],hue=train['SibSp']>0,palette='Blues',saturation=0.8)\nfig.set_xlabel(\"Survived\",size=15)\nfig.set_ylabel(\"#\",size=15)\nfig.set(xticklabels=[\"0-No\",\"1-Yes\"])\nplt.title('# of Survived per Siblings\/Spouses Onboard',size = 20)\nplt.show()","8458cb4e":"plt.figure(figsize=(8,4))\nfig=sns.countplot(train['Survived'],hue=train['Parch']>0,palette='Oranges',saturation=0.8)\nfig.set_xlabel(\"Survived\",size=15)\nfig.set_ylabel(\"#\",size=15)\nfig.set(xticklabels=[\"0-No\",\"1-Yes\"])\nplt.title('# of Survived per Parents\/Children Onboard',size = 20)\nplt.show()\n","1f54a163":"sns.set(rc={\"font.style\":\"normal\",\n            \"axes.facecolor\":\"white\",\n            \"figure.facecolor\":\"white\",\"text.color\":\"black\",\n            \"xtick.color\":\"black\",\n            \"ytick.color\":\"black\",\n            \"axes.labelcolor\":\"black\"})\nplt.figure(figsize=(8,4))\nfig=sns.countplot(y=train['Pclass'],hue=train['SibSp']>0,palette='Blues',saturation=1.0)\nfig.set_xlabel(\"#\",size=15)\nfig.set_ylabel(\"Passenger Class\",size=15)\nplt.title('# of Pclass per Siblings\/Spouses Onboard',size = 20)\nplt.show()","f8035b34":"plt.figure(figsize=(8,4))\nfig=sns.countplot(y=train['Pclass'],hue=train['Parch']>0,palette='Reds',saturation=0.6)\nfig.set_ylabel(\"Pclass\",size=15)\nfig.set_xlabel(\"#\",size=15)\nplt.title('# of Pclass per Parents\/Children Onboard',size = 20)\nplt.show()","a8e62506":"def func(x):\n    x1=x[0]\n    x2=x[1]\n    if x1>0 or x2>0:\n        return False\n    else:\n        return True\ntrain['TravelAlone']=train[['SibSp','Parch']].apply(func,axis=1)\ntest['TravelAlone']=test[['SibSp','Parch']].apply(func,axis=1)\n\nplt.figure(figsize=(8,4))\nfig=sns.countplot(y=train['Survived'],hue=train['TravelAlone'],palette='RdBu',saturation=2.0)\nfig.set_ylabel(\"Survived\",size=15)\nfig.set_xlabel(\"#\",size=15)\nplt.title('# of Survived\/Onboard Alone',size = 20)\nplt.show()","63bb3d1a":"plt.figure(figsize=(8,4))\nfig=sns.countplot(y=train['Pclass'],hue=train['TravelAlone'],palette='Reds',saturation=0.8)\nfig.set_ylabel(\"Passenger Class\",size=15)\nfig.set_xlabel(\"#\",size=15)\nplt.title('# of Pclass\/Onboard Alone',size = 20)\nplt.show()","1a31c237":"#Checking the status of null values inside our training Dataframe\ntrain.isnull().sum()","f4db2415":"#Checking the status of null values inside our testing Dataframe\ntest.isnull().sum()","3cb8835e":"train.drop(columns=['PassengerId','Name','Ticket','Cabin'],inplace=True)\n#train.head()\n\n#same processing for test dataset\ntest.drop(columns=['PassengerId','Name','Ticket','Cabin'],inplace=True)\n","92bff50e":"train.drop(columns=['SibSp','Parch'],inplace=True)\n#train.head()\n\n#same processing for test dataset\ntest.drop(columns=['SibSp','Parch'],inplace=True)\n","1b23b82d":"#median age for Pclass 1\na=train.groupby('Pclass').median()['Age'].iloc[0]\n#median age for Pclass 2\nb=train.groupby('Pclass').median()['Age'].iloc[1] \n#median age for Pclass 3\nc=train.groupby('Pclass').median()['Age'].iloc[2] \ndef fillAge_train(x):\n    age=x[0]\n    pclass=x[1]\n    if pd.isnull(age):\n        if pclass==1:\n            return a\n        elif pclass==2:\n            return b\n        else:\n            return c\n    else:\n        return age\n#median age for Pclass 1\na_test=test.groupby('Pclass').median()['Age'].iloc[0]\n#median age for Pclass 2\nb_test=test.groupby('Pclass').median()['Age'].iloc[1] \n#median age for Pclass 3\nc_test=test.groupby('Pclass').median()['Age'].iloc[2]\ndef fillAge_test(x):\n    age=x[0]\n    pclass=x[1]\n    if pd.isnull(age):\n        if pclass==1:\n            return a_test\n        elif pclass==2:\n            return b_test\n        else:\n            return c_test\n    else:\n        return age\n    \n#replacing null Age values in training dataset\ntrain['Age']=train[['Age','Pclass']].apply(fillAge_train,axis=1)\n#replacing null Age values in Test Dataset\ntest['Age']=test[['Age','Pclass']].apply(fillAge_test,axis=1)\n     ","033de198":"a=test.groupby('Pclass').median()['Fare'].iloc[0]\nb=test.groupby('Pclass').median()['Fare'].iloc[1]\nc=test.groupby('Pclass').median()['Fare'].iloc[2]\ndef fillFare(x):\n    fare=x[0]\n    pclass=x[1]\n    if pd.isnull(fare):\n        if pclass==1:\n            return a\n        elif pclass==2:\n            return b\n        else:\n            return c\n    else:\n        return fare\n\n#replace Fare value using built in Pandas Functions\ntest['Fare']=test[['Fare','Pclass']].apply(fillFare,axis=1)","7ffa23d5":"#fill null values for feature column- Embarked using Pandas built-in function\ntrain['Embarked'].fillna(train['Embarked'].mode()[0], inplace=True)","34a4d8c4":"#Convert Categorical Column to Integer Column using One-hot Encoding\ntrain=pd.get_dummies(train,columns=['Sex','Embarked','TravelAlone'],drop_first=True)\n#train.head()\n\n#Same conversion for test dataset\ntest=pd.get_dummies(test,columns=['Sex','Embarked','TravelAlone'],drop_first=True)\n#test.head()","902ac368":"train['AgeCateg'] = pd.cut(train['Age'], 5)\ntrain[['AgeCateg', 'Survived']].groupby(['AgeCateg'], as_index=False).mean().sort_values(by='AgeCateg', ascending=True)\n\ntrain['FareCateg'] = pd.qcut(train['Fare'], 4)\ntrain[['FareCateg', 'Survived']].groupby(['FareCateg'], as_index=False).mean().sort_values(by='FareCateg', ascending=True)\n\n#func to process values\ndef encodeAgeFare(train):\n    train.loc[train['Age'] <= 16, 'Age'] = 0\n    train.loc[(train['Age'] > 16) & (train['Age'] <= 32), 'Age'] = 1\n    train.loc[(train['Age'] > 32) & (train['Age'] <= 48), 'Age'] = 2\n    train.loc[(train['Age'] > 48) & (train['Age'] <= 64), 'Age'] = 3\n    train.loc[ (train['Age'] > 48) & (train['Age'] <= 80), 'Age'] = 4\n    \n    train.loc[train['Fare'] <= 7.91, 'Fare'] = 0\n    train.loc[(train['Fare'] > 7.91) & (train['Fare'] <= 14.454), 'Fare'] = 1\n    train.loc[(train['Fare'] > 14.454) & (train['Fare'] <= 31.0), 'Fare'] = 2\n    train.loc[(train['Fare'] > 31.0) & (train['Fare'] <= 512.329), 'Fare'] = 3\n    \nencodeAgeFare(train)\nencodeAgeFare(test)\n\n#dropping AgeCateg and FareCateg columns\ntrain.drop(columns=['AgeCateg','FareCateg'],inplace=True)\n    ","159cb62e":"train.head()","b60b25f9":"test.head()","17804df7":"# We Seperate and assign our Target Variable Column to a new variable\nX = train.drop('Survived',axis=1)\n# Dropped 'SibSp' and 'Parch' from Input Feature Data because we have Alone_True Column\n\ny = train['Survived']","4699a3f7":"# We're splitting up our data set into groups called 'train' and 'test'\nfrom sklearn.model_selection import train_test_split\n\nnp.random.seed(0)\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.3)","dd0fc3d4":"#Core Imports for Model Training\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV","04b98f24":"#initialize our object\nxgbclassifier=XGBClassifier()\n\n#fit train data\nxgbclassifier.fit(X_train,y_train)\n\n#predictions\npred_xgb=xgbclassifier.predict(X_test)\n\n# print(accuracy_score(y_test,pred_xgb))","84474e37":"randomfc = RandomForestClassifier(n_estimators=100)\n\n#fit train data\nrandomfc.fit(X_train,y_train)\n\n#predictions\npred_rf=randomfc.predict(X_test)\n\n# print(accuracy_score(y_test,pred_rf))","26f02057":"lightgb=LGBMClassifier()\n\n#fit train data\nlightgb.fit(X_train,y_train)\n\n#predictions\npred_lgb=lightgb.predict(X_test)\n\n# print(accuracy_score(y_test,pred_lgb))","ee8505af":"ada=AdaBoostClassifier(n_estimators=50,learning_rate=1)\n\n#fit train data\nada.fit(X_train,y_train)\n\n#predictions\npred_ada=ada.predict(X_test)\n\n# print(accuracy_score(y_test,pred_ada))\n","f6a855a7":"cbc=CatBoostClassifier(verbose=0, n_estimators=100)\n\n#fit train data\ncbc.fit(X_train,y_train)\n\n#predictions\npred_cbc=cbc.predict(X_test)\n\n# print(accuracy_score(y_test,pred_cbc))","e42c4ba1":"log=LogisticRegression(max_iter=1000)\n\n#fit train data\nlog.fit(X_train,y_train)\n\n#predictions\npred_log=log.predict(X_test)\n\n# print(accuracy_score(y_test,pred_log))","5f4e5c51":"print('XGBoost:', round(xgbclassifier.score(X_train, y_train) * 100, 2), '%.\\t\\t\\t RandomForest:', round(randomfc.score(X_train, y_train) * 100, 2), '%.')\nprint('LightGBM:', round(lightgb.score(X_train, y_train) * 100, 2), '%.\\t\\t\\t AdaBoost:', round(ada.score(X_train, y_train) * 100, 2), '%.')\nprint('CatBoost:', round(cbc.score(X_train, y_train) * 100, 2), '%.\\t\\t\\t LogisticRegression:', round(log.score(X_train, y_train) * 100, 2), '%.')","1812f68c":"print('XGBoost:', round(accuracy_score(y_test,pred_xgb) * 100, 2), '%.\\t\\t\\t RandomForest:', round(accuracy_score(y_test,pred_rf) * 100, 2), '%.')\nprint('LightGBM:', round(accuracy_score(y_test,pred_lgb) * 100, 2), '%.\\t\\t\\t AdaBoost:', round(accuracy_score(y_test,pred_ada) * 100, 2), '%.')\nprint('CatBoost:', round(accuracy_score(y_test,pred_cbc) * 100, 2), '%.\\t\\t\\t LogisticRegression:', round(accuracy_score(y_test,pred_log) * 100, 2), '%.')","ce7bd392":"fig, axs = plt.subplots(3, 2,figsize=(20,12))\n\nsns.heatmap(confusion_matrix(y_test,pred_xgb), annot=True,ax=axs[0][0])\naxs[0, 0].set_title('XGBoost')\n\nsns.heatmap(confusion_matrix(y_test,pred_rf), annot=True,ax=axs[0][1])\naxs[0, 1].set_title('RandomForest')\n\nsns.heatmap(confusion_matrix(y_test,pred_lgb), annot=True,ax=axs[1][0])\naxs[1, 0].set_title('LightGBM')\n\nsns.heatmap(confusion_matrix(y_test,pred_ada), annot=True,ax=axs[1][1])\naxs[1, 1].set_title('XgBoost')\n\nsns.heatmap(confusion_matrix(y_test,pred_cbc), annot=True,ax=axs[2][0])\naxs[2, 0].set_title('CatBoost')\n\nsns.heatmap(confusion_matrix(y_test,pred_log), annot=True,ax=axs[2][1])\naxs[2, 1].set_title('Logistic Regression')\n\nplt.show()","3a28b4e3":"#assigning X_train,X_test and y_train vatiables\nX_train=train.drop('Survived',axis=1)\nX_test=test.copy()\ny_train = train['Survived']\n\n#declaring and inititalizing params attribute\nfrom scipy.stats import uniform, truncnorm, randint\nmodel_params = {\n    # randomly sample numbers from 4 to 204 estimators\n    'n_estimators': randint(4,200),\n    # normally distributed max_features, with mean .25 stddev 0.1, bounded between 0 and 1\n    'max_features': truncnorm(a=0, b=1, loc=0.25, scale=0.1),\n    # uniform distribution from 0.01 to 0.2 (0.01 + 0.199)\n    'min_samples_split': uniform(0.01, 0.199)\n}\n\nrf_model = RandomForestClassifier()\n\n# set up random search meta-estimator\n# this will train 100 models over 5 folds of cross validation (500 models total)\nclf = RandomizedSearchCV(rf_model, model_params, n_iter=100, cv=5, random_state=1)\n\n# train the random search meta-estimator to find the best model out of 100 candidates\nmodel = clf.fit(X_train, y_train)\n\n# print winning set of hyperparameters\nfrom pprint import pprint\npprint(model.best_estimator_.get_params())","b519d68c":"testData=test.copy()\npredictions_rf=model.predict(testData)","ee625b5d":"subm=pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nSurvived=pd.Series(predictions_rf)\nsubm.drop(columns=['Survived'],inplace=True)\nsubm['Survived']=pd.Series(predictions_rf)\nsubm.to_csv(r'RandomForestSubmission.csv', index = False)\nprint(\"Submitted\")","baf97dae":"# Simple Breakdown of Titanic Dataset- EDA,Comparisons and Predictions\n\n![](https:\/\/drive.google.com\/uc?export=download&id=13no8f5E2ffXcBHqftiFrPls6gbQVnV50)","1635eb6b":"> > > ## b) Fare","1ed5a2ee":"> ## 6. Logistic Regression","9950de0e":"PClass 1 was predominantly occupied by ***older citizens***","2ebd8312":"***Passenger Class 1,2 and 3*** on Titanic predominantly had passengers ***travelling alone***.","753305e0":"1. for Pclass 1 and 2 survival rate is generally high for **aged 15-40***\n2. for Pclass 3 we see a high survival rate amongst ***children i.e  0<age<10***","f09f647a":"> ## 3. LighGBM","69d7c550":"- Tuned Random Forest model was used for submission to achieve a score of top14%.\n- We will now use RandomizedSearchCV to tune our **RandomForest model**","a9993c5b":"# Tuning\/Training Models\n\n> ## 1. XGBoost","51ab1d5b":"## Handling Age and Fare Continuous Data Columns\n\n- We divide data from 'Age Column' in 5 bands using Pandas' built in cut()\/qcut() method and map the categories to either of these values- 0\/1\/2\/3\/4","9bc4da6c":"## 4. Hyperparameter Tuning","4d57ab2c":"Youth Presence onBoard was largely able to ***survive the disaster***","dceaf351":"> ## 2. RandomForest","3e14ef88":"- **Cabin** feature-column contains **NULL values**\n- We will drop **'PassengerId'** feature since it consists of unique ID having no influence on the model training result.\n- We will drop **'Cabin','Ticket' and 'Name'** Feature Columns as we are not using any NLP to process the information for model training from non numerical columns.","bf54fad6":"## Exploratory Data Analysis(EDA)\n> Let's create some simple plots to analyze and identify patterns in our data.","60425022":"> Above is a concise summary of our dataframe returning columns' data-type,index data-type and number of non-null values !","ebea352b":"> > # 1. Continuous Features\n> > > ## a) Age\n","a012926e":"***Clearly, the probability that a passenger was travelling with a cheaper ticket was quite high on Titanic***","a119b0f5":"For Fare price less than $25 the probability of finding a buyer was predominantly ***high***","3dfa71eb":"# Evaluations\/Comparisons","62a3272b":"# Imports\n\n\n> 1. Let's get our environment ready with the libraries we'll need and then import the relevant ones beforehand!\n> 2. Pandas is one of the most widely used python libraries in data science. It provides high-performance, easy to use structures and data analysis tools.\n> 3. Matplotlib is a plotting library for the Python programming language\n> 4. Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.","117a57f9":"1. Passengers without Siblings\/Spouse onboard ***mostly survived***\n2. Interesting observation was the passengers travelling without a siblings\/spouse onboard mostly ended up on the casualty side","96940bc4":"# Data Cleaning\n","7e4be2e8":"# Random Forest Tuned- Model used for Submission- 0.78947","6a4550e4":"***Young Passengers(10<age<35) seems to have a good survival rate irrespective of the gender.***","242fff92":"## Introducing 'TravelAlone' Feature\n\n- 'True': Travelling Alone\n- 'False': Has Company(Either Sibling\/Spouse\/Parents\/Children)","7aecb534":"> ## 4. AdaBOOST","393fb080":"1. Passengers ***travelling alone*** mostly ***lost*** their lives in the disaster\n2. Passengers accompanied by their Siblings\/Spouses\/Parents\/Children ***mostly survived***.","38f9f152":"## Check out the Data","e1d15845":"> > # 2. Continuous vs Categorical\n","afe545e3":"On Titanic, passenger class '3' was the ***most affordable*** and cheap with class '1' being the ***most expensive***","78b1c80c":"***Passenger Class 1,2 and 3*** mostly had passengers ***accompanied neither by their Spouses nor Siblings***.","9716de97":"> # 3. Categorical Features","c2852924":"## 1. Confidence Score","5fa28d6a":"- We now get rid of Null Fare values in our test dataset.\n- We replace null with Median Fare value for every Pclass.","bae9d1d4":"***We can observe that the probability for the age to be between 20-30 was high for a passenger onboard the Titanic***","157d5251":"- We now get rid of **Null Values** inside **Embarked** Feature Column.\n- We replace the Null value with the **Mode value of the column.**\n- We fill Mode since it is a non-numerical Column where median is not applicable.","c734ee7c":"- 'TravelAlone' Feature column was introduced to combine the information of 'SibSp' and 'Parch' feature columns\n- SibSp and Parch can now be dropped from our test\/train datasets","f807c069":"\n- We get rid of **Null Values** inside the Age Feature Column\n- We replace every Null Value by the median Age value for every Pclass passenger.\n- The median Age is Calculcated per Passenger Class","34487d13":"## Fetching the Data\n> Using Pandas to load the dataset into this notebook. Using pandas we can read our datafile train.csv with the line below. Data-set loaded will be assigned to the respective variable.","e9da358f":"## 2. Accuracy Score","6b00ed46":"**Observation**\n- ***Classification confidence scores are designed to measure the accuracy of the model when predicting class assignment.***\n- Random Forest,XgBoost and LightGBM nearly top the chart with a very high Confidence Score","feac18dd":"## Thank You!\n\n- Any suggestions for improvements in the score and accuracy are most welcome.\n- Do mention the irregularities found in the comments below.","a8a471b1":"Passengers with ***high-priced tickets***, mostly ended up ***SURVIVING***","0b19fa05":"<a><\/a>\n\n# Introduction\nWelcome aboard my Titanic Kernel. In this Kernel I'll be using the 'OG' [Titanic Dataset](https:\/\/www.kaggle.com\/c\/titanic\/data) to perform-\n- Starter Exploratory Data Analysis using Distribution Plots,Box Plots and Count Plots.\n- DataCleaning\/Data Analysis and Feature Engineering\n- Trainging our Dataset on:\n> 1. XGBoost\n> 2. Random Forest\n> 3. LightGBM\n> 4. CatBOOST\n> 5. AdaBoost\n> 6. Logistic Regression\n- Evaluating and Comparing the Predictions\n\n# UPDATE!!!!\n\n**After some thorough scribbling I was able to improve my submission score from** 0.78468 to 0.78947.**I was able to improve my feature engineering section by better handling of Age and Fare column Features as well as introducing a new Feature column 'TravelAlone'.**","3e0e11f2":"# Train\/Test Data Split","3eb5a39b":"## 3. Confusion Matrix","d438ce8b":"1. ***Female presence*** largely survived the disaster.\n2. ***Highest Casualty*** was reported from the ***Male side***.","92d88915":"> ## 5. CatBoost","c4822f54":"# Feature Engineering\n- We will use **one-hot encoding technique** for our **Categorical Features-'Sex','Embarked' and 'TravelAlone'** to use them in model training for better predictions(Converts Categorical data to Integer Data(0 or 1)).\n- We will get rid of **null values** inside the dataframe by filling them with an appropriate replacement,also taking care of **outliers**.","85244978":"The ***Male presence*** OnBoard Titanic was considerably ***older*** than the ***female***.","7323faf7":"1. ***Passengers from class 3(cheapest Fare) suffered the most!***\n2. ***Pclass 1 reported the highest survival count*** ","e47e3a2a":"We narrow our observation range for Fare i.e(fare<=300) to get a better visualization of the distribution."}}