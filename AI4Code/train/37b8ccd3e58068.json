{"cell_type":{"0d222df7":"code","bf2504b6":"code","c8ea440c":"code","65d30d92":"code","8244751f":"code","23e8ba57":"code","c8ad98f7":"code","d4f59772":"code","f9c4bae4":"code","3dc0d386":"code","5a553881":"code","483e1739":"code","597bf140":"code","7fada60e":"code","f4c726cf":"code","eea8745b":"code","9d6296db":"code","323ca6ed":"markdown","ce5ceb6b":"markdown","20aec651":"markdown","fde5c96c":"markdown","f4a58a6c":"markdown","b6f3bb01":"markdown","d4130388":"markdown","7bdf58c5":"markdown","ebf93198":"markdown","1f88dc95":"markdown","50689dc8":"markdown"},"source":{"0d222df7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats","bf2504b6":"data = pd.read_csv(\"..\/input\/UCI_Credit_Card.csv\")","c8ea440c":"data.head()","65d30d92":"data.shape","8244751f":"data.columns","23e8ba57":"output = 'default.payment.next.month'\n\n# Let's do a little EDA\ncols = [ f for f in data.columns if data.dtypes[ f ] != \"object\"]\ncols.remove( \"ID\")\ncols.remove( output )\n\nf = pd.melt( data, id_vars=output, value_vars=cols)\ng = sns.FacetGrid( f, hue=output, col=\"variable\", col_wrap=5, sharex=False, sharey=False )\ng = g.map( sns.distplot, \"value\", kde=True).add_legend()","c8ad98f7":"def ChiSquaredTestOfIndependence( df, inputVar, Outcome_Category ):\n    # Useful to have this wrapped in a function\n    # The ChiSquaredTest of Independence - \n    # has a null hypothesis: the OutcomeCategory is independent of the inputVar\n    # So we create a test-statistic which is a measure of the difference between \n    # \"expected\" i.e. what we WOULD observe if the OutcomeCategory WAS independent of the inputVar\n    # \"observed\" i.e. what the data actually shows\n    # the p-value returned is the probability of seeing this test-statistic if the null-hypothesis is true\n    Outcome_Category_Table = df.groupby( Outcome_Category )[ Outcome_Category ].count().values\n    Outcome_Category_Ratios = Outcome_Category_Table \/ sum( Outcome_Category_Table )\n    possibleVals = df[inputVar].unique()\n    observed = []\n    expected = []\n    for possible in possibleVals:\n        countsInCategories = df[ df[ inputVar ] == possible ].groupby( Outcome_Category )[Outcome_Category].count().values\n        if( len(countsInCategories) != len( Outcome_Category_Ratios ) ):\n            print(\"Error! The class \" + str( possible) +\" of \\'\" + inputVar + \"\\' does not contain all values of \\'\" + Outcome_Category + \"\\'\" )\n            return\n        elif( min(countsInCategories) < 5 ):\n            print(\"Chi Squared Test needs at least 5 observations in each cell!\")\n            print( inputVar + \"=\" + str(possible) + \" has insufficient data\")\n            print( countsInCategories )\n            return\n        else:\n            observed.append( countsInCategories )   \n            expected.append( Outcome_Category_Ratios * len( df[df[ inputVar ] == possible ]))\n    observed = np.array( observed )\n    expected = np.array( expected )\n    chi_squared_stat = ((observed - expected)**2 \/ expected).sum().sum()\n    degOfF = (observed.shape[0] - 1 ) *(observed.shape[1] - 1 ) \n    #crit = stats.chi2.ppf(q = 0.95,df = degOfF) \n    p_value = 1 - stats.chi2.cdf(x=chi_squared_stat, df=degOfF)\n    print(\"Calculated test-statistic is %.2f\" % chi_squared_stat )\n    print(\"If \" + Outcome_Category + \" is indep of \" + inputVar + \", this has prob %.2e of occurring\" % p_value )\n    #t_stat, p_val, doF, expArray = stats.chi2_contingency(observed= observed, correction=False)\n    #print(\"Using built-in stats test: outputs\")\n    #print(\"test-statistic=%.2f, p-value=%.2f, degsOfFreedom=%d\" % ( t_stat, p_val, doF ) )","d4f59772":"ChiSquaredTestOfIndependence( data, \"SEX\", output )","f9c4bae4":"# Ok. So \"default\" is not independent of \"SEX\".\nChiSquaredTestOfIndependence( data, \"EDUCATION\", output )   ","3dc0d386":"print(\"We have %d with EDUCATION=0\" % len(data.loc[ data[\"EDUCATION\"]==0]))\nprint(\"We have %d with EDUCATION=4\" % len(data.loc[ data[\"EDUCATION\"]==4]))\nprint(\"We have %d with EDUCATION=5\" % len(data.loc[ data[\"EDUCATION\"]==5]))\nprint(\"We have %d with EDUCATION=6\" % len(data.loc[ data[\"EDUCATION\"]==6]))","5a553881":"# Since we have 30k samples, let's just put these non-typical Education instances all into the EDUCATION=4 class and continue \ndata[\"EDUCATION_Corr\"] = data[\"EDUCATION\"].apply( lambda x: x if ((x>0) and (x<4)) else 4 )\nChiSquaredTestOfIndependence( data, \"EDUCATION_Corr\", output ) \ncols.remove(\"EDUCATION\")\ncols.append(\"EDUCATION_Corr\")\n\nChiSquaredTestOfIndependence( data, \"MARRIAGE\", output ) ","483e1739":"# The quantitative vars:\nquant = [\"LIMIT_BAL\", \"AGE\"]\n\n# The qualitative but \"Encoded\" variables (ie most of them)\nqual_Enc = cols\nqual_Enc.remove(\"LIMIT_BAL\")\nqual_Enc.remove(\"AGE\")","597bf140":"logged = []\nfor ii in range(1,7):\n    qual_Enc.remove(\"PAY_AMT\" + str( ii ))\n    data[ \"log_PAY_AMT\" + str( ii )]  = data[\"PAY_AMT\"  + str( ii )].apply( lambda x: np.log1p(x) if (x>0) else 0 )\n    logged.append(\"log_PAY_AMT\" + str( ii ) )\n\nfor ii in range(1,7):\n    qual_Enc.remove(\"BILL_AMT\" + str( ii ))\n    data[ \"log_BILL_AMT\" + str( ii )] = data[\"BILL_AMT\" + str( ii )].apply( lambda x: np.log1p(x) if (x>0) else 0 )\n    logged.append(\"log_BILL_AMT\" + str( ii ) )\n\nf = pd.melt( data, id_vars=output, value_vars=logged)\ng = sns.FacetGrid( f, hue=output, col=\"variable\", col_wrap=3, sharex=False, sharey=False )\ng = g.map( sns.distplot, \"value\", kde=True).add_legend()","7fada60e":"features = quant + qual_Enc + logged + [output]\ncorr = data[features].corr()\nplt.subplots(figsize=(30,10))\nsns.heatmap( corr, square=True, annot=True, fmt=\".1f\" )  ","f4c726cf":"features = quant + qual_Enc + logged   \nX = data[features].values    \ny = data[ output ].values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2)\n\nfrom sklearn.preprocessing import StandardScaler\nscX = StandardScaler()\nX_train = scX.fit_transform( X_train )\nX_test = scX.transform( X_test )\n\n# We'll need some metrics to evaluate our models\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score","eea8745b":"#-------------- \n# Random Forest \n#--------------\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators=10)\nclassifier.fit( X_train, y_train )\ny_pred = classifier.predict( X_test )\n\ncm = confusion_matrix( y_test, y_pred )\nprint(\"Accuracy on Test Set for RandomForest = %.2f\" % ((cm[0,0] + cm[1,1] )\/len(X_test)))\nscoresRF = cross_val_score( classifier, X_train, y_train, cv=10)\nprint(\"Mean RandomForest CrossVal Accuracy on Train Set %.2f, with std=%.2f\" % (scoresRF.mean(), scoresRF.std() ))\n\n#-------------- \n# kernel SVM \n#--------------\nfrom sklearn.svm import SVC\nclassifier1 = SVC(kernel=\"rbf\")\nclassifier1.fit( X_train, y_train )\ny_pred = classifier1.predict( X_test )\n\ncm = confusion_matrix( y_test, y_pred )\nprint(\"Accuracy on Test Set for kernel-SVM = %.2f\" % ((cm[0,0] + cm[1,1] )\/len(X_test)))\nscoresSVC = cross_val_score( classifier1, X_train, y_train, cv=10)\nprint(\"Mean kernel-SVM CrossVal Accuracy on Train Set %.2f, with std=%.2f\" % (scoresSVC.mean(), scoresSVC.std() ))","9d6296db":"#-------------- \n# Logistic Regression \n#--------------\nfrom sklearn.linear_model import LogisticRegression\nclassifier2 = LogisticRegression()\nclassifier2.fit( X_train, y_train )\ny_pred = classifier2.predict( X_test )\n\ncm = confusion_matrix( y_test, y_pred )\nprint(\"Accuracy on Test Set for LogReg = %.2f\" % ((cm[0,0] + cm[1,1] )\/len(X_test)))\nscoresLR = cross_val_score( classifier2, X_train, y_train, cv=10)\nprint(\"Mean LogReg CrossVal Accuracy on Train Set %.2f, with std=%.2f\" % (scoresLR.mean(), scoresLR.std() ))\n\n#-------------- \n# Naive Bayes \n#--------------\nfrom sklearn.naive_bayes import GaussianNB\nclassifier3 = GaussianNB()\nclassifier3.fit( X_train, y_train )\ny_pred = classifier3.predict( X_test )\ncm = confusion_matrix( y_test, y_pred )\nprint(\"Accuracy on Test Set for NBClassifier = %.2f\" % ((cm[0,0] + cm[1,1] )\/len(X_test)))\nscoresNB = cross_val_score( classifier3, X_train, y_train, cv=10)\nprint(\"Mean NaiveBayes CrossVal Accuracy on Train Set %.2f, with std=%.2f\" % (scoresNB.mean(), scoresNB.std() ))\n\n#-------------- \n# K-NEIGHBOURS \n#--------------\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier4 = KNeighborsClassifier(n_neighbors=5)\nclassifier4.fit( X_train, y_train )\ny_pred = classifier4.predict( X_test )\ncm = confusion_matrix( y_test, y_pred )\nprint(\"Accuracy on Test Set for KNeighborsClassifier = %.2f\" % ((cm[0,0] + cm[1,1] )\/len(X_test)))\nscoresKN = cross_val_score( classifier3, X_train, y_train, cv=10)\nprint(\"Mean KN CrossVal Accuracy on Train Set Set %.2f, with std=%.2f\" % (scoresKN.mean(), scoresKN.std() ))\n\n\n\n\n\n","323ca6ed":"Ok. So default is not independent of EDUCATION_Corr nor independent of MARRIAGE.\n\nI find it useful to separate the variables into \"quantitative\" vs \"qualitative\" and also to keep track of those that I've transformed (instead of overwriting them).","ce5ceb6b":"It *looks like* higher Log PAY_AMT is associated with *slightly less default*.\n\nSo now we have quant variables, qual_Enc variables and logged variables. Let's check correlations with the output variable:","20aec651":"There aren't enough values in the Education class =0. We'll probably find the same for the non-typical-looking values Education = 4, 5, 6. How many of each do we have?","fde5c96c":"So it looks like the PAY_0, PAY_X variables are the strongest predictors of default, followed by the LIMIT_BAL and Log_PAY_AMT variables.\n\nTo make predictions about whether a customer is likely to default  - we'll train a number of different classifiers and see how well they perform. As usual, we start by splitting the data into train\/test sets and rescaling.","f4a58a6c":"We don't expect the data to be linearly separable - so we'll start with the RandomForest classifier and kernel-SVM ","b6f3bb01":"This notebook uses the UCI CreditCardFraud data - the task is to see which variables are the strongest predictors of default, and to make predictions on which customers are likely to default.\n\nWe import the usual libraries and the data","d4130388":"And the PAY_ variables? We can see those are important, but we'll transform the BILL_AMT and PAY_AMT variables from NT Dollars to Log(NT Dollars)","7bdf58c5":"**Let's look at the data.** Since our dependent variable is categorical, we can split the distributions by \"default\/not-default\" to look at\nthe characteristics of the default\/not-default groups","ebf93198":"We'll check some of the other classifiers - but we  don't expect they will do better ","1f88dc95":"**Observations:**\n*     Defaults have a higher proportion of Lower LIMIT_BAL values\n*     NonDefaults have a higher proportion of Females (Sex=2)\n*     NonDefaults have a higher proportion of MoreEducated (EDUCATION=1 or 2)\n*     NonDefaults have a higher proportion of Singles (MARRIAGE=2)\n*     NonDefaults have a higher proportion of people 30-40years\n*     NonDefaults have a MUCH higher proportion of zero or negative PAY_X variables \n        (this means that being current or ahead of payments is associated with not defaulting in the following month).\n        **This is a strong relationship as the distribution are more separated - so we expect the PAY_X to be important!**\n\n(Clearly we're going to have to transform those dollar amounts in the PAY_AMTX and BILL_AMTX variables.)\n\nHow significant are these relationships? Given the observed data, is it possible we're imagining relationships when they're not really that strong? ","50689dc8":"**Conclusion:**\nUsing a kernel-SVM classifier, we can predict with ~82% accuracy, whether a customer is likely to default next month. \n\nThe strongest predictors of default are the PAY_X (ie the repayment status in previous months), the LIMIT_BAL & the PAY_AMTX (amount paid in previous months). \n\nDemographics: we see that being Female, More educated, Single and between 30-40years old means a customer is more likely to make payments on time.\n"}}