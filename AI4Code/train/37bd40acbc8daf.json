{"cell_type":{"6bba164f":"code","44df028a":"code","ab0bcde9":"code","509ec70b":"code","544a407f":"code","26e6e798":"code","47224dcf":"code","f89b4bcc":"code","049fee7d":"code","9bd09d13":"code","a1f76b86":"code","744057bd":"code","10be7da0":"code","fddfa780":"code","b33a1d43":"code","f2accdbe":"code","d7eaed86":"code","5369bace":"code","2b73437f":"code","daf4ddf7":"code","42fd6185":"code","48974cbf":"code","ccc0ba80":"code","2c9dee22":"code","5e4ed564":"code","c7b1a67b":"code","4e5e7323":"code","0bb6c909":"code","87b0684b":"code","d94a0a81":"code","9e2fab0a":"code","7f228267":"code","b50fb897":"code","d0e95ae8":"code","459d2855":"code","1758209d":"markdown","dd286ea7":"markdown","cfe8e30b":"markdown","28377583":"markdown","abc1406d":"markdown","d0291dfe":"markdown","bdaf6b43":"markdown","8585fd17":"markdown","e7c85afd":"markdown","14dbfb49":"markdown","98ef699f":"markdown","784bd181":"markdown","33934b4b":"markdown","f94c183f":"markdown","49c27a85":"markdown","35dbb8ce":"markdown","893e90b9":"markdown","8f5e6aea":"markdown","dda017df":"markdown","76336c2f":"markdown","ba1f3b25":"markdown","153e8781":"markdown","279a6a36":"markdown","3383f1db":"markdown","5f2a3f37":"markdown","1282a908":"markdown"},"source":{"6bba164f":"%%HTML\n<style type=\"text\/css\">\ndiv.h1 {\n    background-color: #009C37; \n    color: white; padding: 8px; padding-right: 300px; font-size: 35px; max-width: 1500px; margin: auto; margin-top: 50px;}\ndiv.h2 {\n    background-color: #002277; \n    color: white; padding: 8px; padding-right: 300px; font-size: 25px; max-width: 1500px; margin: auto; margin-top: 50px;}\ndiv.h3 {\n    color: #002277;\n    font-size: 16px; margin-top: 20px; margin-bottom:4px;}\ndiv.h4 {\n    font-size: 15px; margin-top: 20px; margin-bottom: 8px;\nspan.note {\n    font-size: 5; color: #002277;\n    font-style: italic;}\nspan.captiona {\n    font-size: 5; color: dimgray; font-style: italic; margin-left: 130px; vertical-align: top;}\nhr {\n    display: block; color: #002277;\n    height: 1px; border: 0;  border-top: 1px solid;}\nhr.light {\n    display: block; color: lightgray; height: 1px; border: 0; border-top: 1px solid;}\n<\/style>","44df028a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\n# Suppr warning\nimport warnings\n\n# Import PyStackNet Package\n# Source: https:\/\/www.kaggle.com\/kirankunapuli\/pystacknet\nimport os\nimport sys\npackage_dir = \"..\/input\/py-stack-net\/h2oai-pystacknet-af571e0\"\nsys.path.append(package_dir)\nimport pystacknet\nimport joblib\nsys.modules['sklearn.externals.joblib'] = joblib\n\nwarnings.filterwarnings(\"ignore\")\npd.set_option('max_columns', 200)\npd.set_option('max_rows', 200)\nprint(os.listdir(\"..\/input\"))\n","ab0bcde9":"\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# Machine Learning\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, roc_auc_score, f1_score, roc_curve, auc,precision_recall_curve\nfrom sklearn import metrics","509ec70b":"# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nimport matplotlib.patches as patches","544a407f":"def evalBinaryClassifier(model, x, y, labels=['Positives','Negatives']):\n    '''\n    source: https:\/\/towardsdatascience.com\/how-to-interpret-a-binary-logistic-regressor-with-scikit-learn-6d56c5783b49\n    Visualize the performance of  a Logistic Regression Binary Classifier.\n    \n    Displays a labelled Confusion Matrix, distributions of the predicted\n    probabilities for both classes, the ROC curve, and F1 score of a fitted\n    Binary Logistic Classifier. Author: gregcondit.com\/articles\/logr-charts\n    \n    Parameters\n    ----------\n    model : fitted scikit-learn model with predict_proba & predict methods\n        and classes_ attribute. Typically LogisticRegression or \n        LogisticRegressionCV\n    \n    x : {array-like, sparse matrix}, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples\n        in the data to be tested, and n_features is the number of features\n    \n    y : array-like, shape (n_samples,)\n        Target vector relative to x.\n    \n    labels: list, optional\n        list of text labels for the two classes, with the positive label first\n        \n    Displays\n    ----------\n    3 Subplots\n    \n    Returns\n    ----------\n    F1: float\n    '''\n    #model predicts probabilities of positive class\n    p = model.predict_proba(x)\n    if len(model.classes_)!=2:\n        raise ValueError('A binary class problem is required')\n    if model.classes_[1] == 1:\n        pos_p = p[:,1]\n    elif model.classes_[0] == 1:\n        pos_p = p[:,0]\n    \n    #FIGURE\n    plt.figure(figsize=[15,4])\n    \n    #1 -- Confusion matrix\n    train_preds = model.predict_proba(x)[:, 1]\n    cm = confusion_matrix(y,train_preds.round())\n    \n    plt.subplot(131)\n    ax = sns.heatmap(cm, annot=True, cmap='Blues', cbar=False, \n                annot_kws={\"size\": 14}, fmt='g')\n    cmlabels = ['True Negatives', 'False Positives',\n              'False Negatives', 'True Positives']\n    for i,t in enumerate(ax.texts):\n        t.set_text(t.get_text() + \"\\n\" + cmlabels[i])\n    plt.title('Confusion Matrix', size=15)\n    plt.xlabel('Predicted Values', size=13)\n    plt.ylabel('True Values', size=13)\n      \n    #2 -- Distributions of Predicted Probabilities of both classes\n    df = pd.DataFrame({'probPos':pos_p, 'target': y})\n    plt.subplot(132)\n    plt.hist(df[df.target==1].probPos, density=True, bins=25,\n             alpha=.5, color='green',  label=labels[0])\n    plt.hist(df[df.target==0].probPos, density=True, bins=25,\n             alpha=.5, color='red', label=labels[1])\n    plt.axvline(.5, color='blue', linestyle='--', label='Boundary')\n    plt.xlim([0,1])\n    plt.title('Distributions of Predictions', size=15)\n    plt.xlabel('Positive Probability (predicted)', size=13)\n    plt.ylabel('Samples (normalized scale)', size=13)\n    plt.legend(loc=\"upper right\")\n    \n    #3 -- ROC curve with annotated decision point\n    fp_rates, tp_rates, _ = roc_curve(y,p[:,1])\n    roc_auc = auc(fp_rates, tp_rates)\n    plt.subplot(133)\n    plt.plot(fp_rates, tp_rates, color='green',\n             lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], lw=1, linestyle='--', color='grey')\n    #plot current decision point:\n    tn, fp, fn, tp = [i for i in cm.ravel()]\n    plt.plot(fp\/(fp+tn), tp\/(tp+fn), 'bo', markersize=8, label='Decision Point')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate', size=13)\n    plt.ylabel('True Positive Rate', size=13)\n    plt.title('ROC Curve', size=15)\n    plt.legend(loc=\"lower right\")\n    plt.subplots_adjust(wspace=.3)\n    plt.show()\n    #Print and Return the F1 score\n    tn, fp, fn, tp = [i for i in cm.ravel()]\n    precision = tp \/ (tp + fp)\n    recall = tp \/ (tp + fn)\n    F1 = 2*(precision * recall) \/ (precision + recall)\n    printout = (\n        f'Precision: {round(precision,2)} | '\n        f'Recall: {round(recall,2)} | '\n        f'F1 Score: {round(F1,2)} | '\n    )\n    print(printout)\n    return F1\n\n# source: https:\/\/www.kaggle.com\/carlolepelaars\/ensembling-with-stacknet\/notebook\ndef auc_score(y_true, y_pred):\n    \"\"\"\n    Calculates the Area Under ROC Curve (AUC)\n    \"\"\"\n    return roc_auc_score(y_true, y_pred)\ndef plot_curve(y_true_train, y_pred_train, y_true_val, y_pred_val, model_name):\n    \"\"\"\n    Plots the ROC Curve given predictions and labels\n    \"\"\"\n    fpr_train, tpr_train, _ = roc_curve(y_true_train, y_pred_train, pos_label=1)\n    fpr_val, tpr_val, _ = roc_curve(y_true_val, y_pred_val, pos_label=1)\n    plt.figure(figsize=(8, 8))\n    plt.plot(fpr_train, tpr_train, color='black',\n             lw=2, label=f\"ROC train curve (AUC = {round(roc_auc_score(y_true_train, y_pred_train), 4)})\")\n    plt.plot(fpr_val, tpr_val, color='darkorange',\n             lw=2, label=f\"ROC validation curve (AUC = {round(roc_auc_score(y_true_val, y_pred_val), 4)})\")\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([-0.05, 1.05])\n    plt.ylim([-0.05, 1.05])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.xticks(fontsize=14)\n    plt.yticks(fontsize=14)\n    plt.title(f'ROC Plot for {model_name}', weight=\"bold\", fontsize=20)\n    plt.legend(loc=\"lower right\", fontsize=16)\n\ndef dist_target(df, target, msg_title):\n    plt.figure(figsize=(12,4))\n    plt.hist(df[target], bins=100)\n    plt.title(msg_title, weight='bold', fontsize=18)\n    plt.xlabel(\"Predictions\", fontsize=15)\n    plt.ylabel(\"Frequency\", fontsize=15)\n    plt.xlim(0, 1);    ","26e6e798":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/sample_submission.csv')","47224dcf":"# Suppress warnings \nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nfrom IPython.display import HTML\nHTML('<iframe width=\"1280\" height=\"720\" src=\"https:\/\/www.youtube.com\/embed\/gNPphk98pnI\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>')","f89b4bcc":"cat_cols = ['cat'+str(i) for i in range(19)]\ncont_cols = ['cont'+str(i) for i in range(11)]","049fee7d":"for c in cat_cols:\n    labelencoer = LabelEncoder() \n    labelencoer.fit(list(train[c].values)+list(test[c].values)) \n    train[c] = labelencoer.transform(list(train[c].values))\n    test[c] = labelencoer.transform(list(test[c].values))\nprint('Labelling done.')    \n\nscaler = StandardScaler()    \ntrain[cont_cols] = scaler.fit_transform(train[cont_cols])\ntest[cont_cols] = scaler.transform(test[cont_cols])    ","9bd09d13":"# Level 1 are the base models that take the training dataset as input\n\nl1_clf1 = LGBMRegressor(boosting_type='gbdt',\n                        objective=\"binary\",\n                        metric=\"AUC\",\n                        boost_from_average=\"false\",\n                        learning_rate=0.05,\n                        num_leaves=721,\n                        max_depth=21,\n                        min_child_weight=0.035,\n                        feature_fraction=0.38,\n                        bagging_fraction=0.42,\n                        min_data_in_leaf=121,\n                        max_bin=255,\n                        importance_type='split',\n                        reg_alpha=0.4,\n                        reg_lambda=0.65,\n                        bagging_seed=21,\n                        random_state=2021,\n                        verbosity=-1,\n                        subsample=0.6,\n                        colsample_bytree=0.8,\n                        min_child_samples=79)\n\nl1_clf3 = CatBoostRegressor(learning_rate=0.1,\n                            bagging_temperature=0.1, \n                            l2_leaf_reg=30,\n                            depth=12, \n                            max_bin=255,\n                            iterations=320,\n                            loss_function='Logloss',\n                            objective='RMSE',\n                            eval_metric=\"AUC\",\n                            bootstrap_type='Bayesian',\n                            random_seed=2021,\n                            early_stopping_rounds=21)\n\n\n# Level 2 models will take predictions from level 1 models as input\n# Remember to keep level 2 models smaller\n# Basic models like Ridge Regression with large regularization or small random forests work well\nl2_clf1 = RandomForestRegressor(n_estimators=721, \n                                max_depth=5, \n                                max_features='sqrt', \n                                random_state=2021)","a1f76b86":"# Split Train and Validation\nfeatures = cat_cols+cont_cols\ntarget_col = 'target'\nX_train = train[features]\ntarget = train[target_col]\nX_train, X_val, y_train, y_val = train_test_split(X_train, target, test_size=0.21, random_state=2021, stratify=target)","744057bd":"# Specify model tree for StackNet\nmodels = [[l1_clf1, l1_clf3], # Level 1\n          [l2_clf1]] # Level 2","10be7da0":"from pystacknet.pystacknet import StackNetClassifier\n# Specify parameters for stacked model and begin training\nmodel = StackNetClassifier(models, \n                           metric=\"auc\", \n                           folds=5,\n                           restacking=False,\n                           use_retraining=True,\n                           use_proba=True, # To use predict_proba after training\n                           random_state=2021,\n                           n_jobs=1, \n                           verbose=1)\n\n\n# Fit the entire model tree\nmodel.fit(X_train, y_train)","fddfa780":"# Get score on training set and validation set for our StackNetClassifier\ntrain_preds = model.predict_proba(X_train)[:, 1]\nval_preds = model.predict_proba(X_val)[:, 1]\ntrain_score = auc_score(y_train, train_preds)\nval_score = auc_score(y_val, val_preds)","b33a1d43":"print(f\"StackNet AUC on training set: {round(train_score, 4)}\")\nprint(f\"StackNet AUC on validation set: {round(val_score, 4)}\")","f2accdbe":"# Plot ROC curve\nplot_curve(y_train, train_preds, y_val, val_preds, \"StackNet Baseline\")","d7eaed86":"eval_metric = evalBinaryClassifier(model, X_val, y_val)","5369bace":"from sklearn.metrics import classification_report\ntrain_cm = confusion_matrix(y_train,train_preds.round())\nprint('Confusion matrix: \\n',train_cm)\nprint('Classification report: \\n',classification_report(y_train, train_preds.round()))","2b73437f":"# visualize with seaborn library\nsns.heatmap(train_cm,annot=True,fmt=\"d\") \nplt.show()","daf4ddf7":"# LGBMClassifier\nclf_lgb = LGBMClassifier(\n    max_bin=72,\n    num_leaves=271,\n    num_iterations=721,\n    learning_rate=0.02,\n    tree_learner=\"serial\",\n    task=\"train\",\n    is_training_metric=False,\n    min_data_in_leaf=1,\n    min_sum_hessian_in_leaf=100,\n    sparse_threshold=1.0,\n    save_binary=True,\n    seed=42,\n    feature_fraction_seed=42,\n    bagging_seed=42,\n    drop_seed=42,\n    data_random_seed=42,\n    objective=\"binary\",\n    boosting_type=\"gbdt\",\n    verbose=1,\n    metric=\"auc\",\n    is_unbalance=True,\n    boost_from_average=False,\n)\n\nclf_lgb.fit(X_train, y_train)","42fd6185":"eval_metric_lgb = evalBinaryClassifier(clf_lgb, X_val, y_val)","48974cbf":"clf_xgb = XGBClassifier(\n    n_estimators=721,\n    max_depth=21,\n    learning_rate=0.02,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    random_state=21,\n)\nclf_xgb.fit(X_train, y_train,eval_metric=[\"auc\", \"logloss\"],verbose=True)","ccc0ba80":"eval_metric_xgb = evalBinaryClassifier(clf_xgb, X_val, y_val)","2c9dee22":"from catboost import CatBoostClassifier\nimport catboost as catboost\nparam_cb = {\n        'learning_rate': 0.2,\n        'bagging_temperature': 0.1, \n        'l2_leaf_reg': 30,\n        'depth': 12, \n        'max_bin':255,\n        'iterations' : 721,\n        'loss_function' : \"Logloss\",\n        'objective':'CrossEntropy',\n        'eval_metric' : \"AUC\",\n        'bootstrap_type' : 'Bayesian',\n        'random_seed':42,\n        'early_stopping_rounds' : 121,\n}\nclf_ctb = CatBoostClassifier(silent=True, **param_cb)\nclf_ctb.fit(X_train, y_train)","5e4ed564":"models_ = [  ######## First level ########\n            [clf_lgb, clf_xgb, clf_ctb],\n            ######## Second level ########\n            [clf_lgb],\n]\n# StackNetClassifier with GPU\n\nmodel_ = StackNetClassifier(\n    models_,\n    metric=\"auc\",\n    folds=5,\n    restacking=False,\n    use_retraining=False,\n    use_proba=True,\n    random_state=42,\n    verbose=1,\n)\n\nmodel_.fit(X_train, y_train)","c7b1a67b":"sub['target'] = clf_lgb.predict_proba(test[features])[:,1]\nsub.to_csv('submission_lgb.csv', index=False)","4e5e7323":"dist_target(sub, 'target', \"Prediction Distribution for test set\")","0bb6c909":"sub['target'] = clf_xgb.predict_proba(test[features])[:,1]\nsub.to_csv('submission_xgb.csv', index=False)","87b0684b":"dist_target(sub, 'target', \"Prediction Distribution for test set\")","d94a0a81":"sub['target'] = clf_ctb.predict_proba(test[features])[:,1]\nsub.to_csv('submission_ctb.csv', index=False)","9e2fab0a":"dist_target(sub, 'target', \"Prediction Distribution for test set\")","7f228267":"sub['target'] = model.predict_proba(test[features])[:,1]\nsub.to_csv('submission.csv', index=False)","b50fb897":"dist_target(sub, 'target', \"Prediction Distribution for test set\")","d0e95ae8":"sub['target'] = model_.predict_proba(test[features])[:,1]\nsub.to_csv('submission_stacknet2.csv', index=False)","459d2855":"dist_target(sub, 'target', \"Prediction Distribution for test set\")","1758209d":"<a id='t3'><\/a>\n# <div class=\"h1\">General Findings<\/div>\n\n[Back to Contents](#top)\n\n[End Notebook](#end)\n","dd286ea7":"### Useful links: \n- [StackNet](https:\/\/github.com\/kaz-Anova\/StackNet)\n- [StackNet Examples](https:\/\/github.com\/kaz-Anova\/StackNet#examples)\n- [Kaggle Stacknet example](https:\/\/www.kaggle.com\/caesarlupum\/brazil-against-the-advance-of-covid-19)\n- [StacKNet - Ensembling - How to Win a Data Science Competition: Learn from Top Kagglers](https:\/\/www.coursera.org\/lecture\/competitive-data-science\/stacknet-s8RLi)\n- [Win Machine Learning (Kaggle) Competitions using StackNet with Marios Michailidis](https:\/\/skillsmatter.com\/skillscasts\/10121-infiniteconf-bytes-with-marios-michailidis)\n- [StackingClassifier - An ensemble-learning meta-classifier for stacking](http:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/StackingClassifier\/#overview)\n <hr>","cfe8e30b":"submission_stacknet","28377583":"\n\nThe blue line signifies the baseline AUC which is 0.5. The final validation score is the area under the orange curve, which is mentioned in the plot","abc1406d":"<a id='t1_1'><\/a>\n# <div class=\"h3\"> Imports<\/div>\n[Next](#t1_2)\n","d0291dfe":"The model is compiled and fitted through the a familiar sklearn-like API. The StackNetClassifier will perform cross-validation (CV) and will output the CV scores for each model.","bdaf6b43":"<a id='t2_2'><\/a>\n# <div class=\"h2\">Evaluation StackNet<\/div>\n\n[Back to Contents](#top)\n\n[Next](#t2_3)\n","8585fd17":"Read data","e7c85afd":"<a id='t2'><\/a>\n# <div class=\"h1\">Modeling<\/div>\n\n[Back to Contents](#top)\n\n[Next](#t2_2)\n\n# <div class=\"h3\">Ensembling With StackNet<\/div>\n\n[Next](#t3_1)\n![](https:\/\/github.com\/kaz-Anova\/StackNet\/raw\/master\/images\/StackNet_Logo.png?raw=true)\nStackNet was created by Kaggle Grandmaster Marios Michailidis ([kazanova](https:\/\/www.kaggle.com\/kazanova)) as part of his PhD. Thanks to [Kiran Kunapuli](https:\/\/www.kaggle.com\/kirankunapuli) for uploading the package as a Kaggle dataset so it can conveniently be used with Kaggle kernels.","14dbfb49":"submission_ctb","98ef699f":"submission_lgb","784bd181":"#### Introducing StackNet Meta-Modelling Framework","33934b4b":"#### StackNetClassifier","f94c183f":"<a id='top'><\/a>\n<div class=\"h1\">Contents<\/div>\n\n1. [Glimpse of Data](#t1)\n    1.2 [Read in Data](#t1_2)\n    \n2. [Modeling - Ensembling With StackNet](#t2)\n    2.1 [StackNet Modeling](#t2_1)\n    2.2 [Evaluation StackNet ROC_AUC](#t2_2)\n    2.3 [Submission](#t2_3)\n\n3. [General Findings](#t3)\n\n4. [End Notebooks](#end)\n","49c27a85":"submission_xgb","35dbb8ce":"# <div class=\"h1\">Starter Here: StackNetClassifier \ud83d\udcda<\/div>\n![](https:\/\/github.com\/kaz-Anova\/StackNet\/raw\/master\/images\/StackNet_Logo.png?raw=true)\nby [kazanova](https:\/\/www.kaggle.com\/kazanova)\n","893e90b9":"Specify model tree for StackNet","8f5e6aea":"### XGBClassifier","dda017df":"#### Classification report train","76336c2f":"### LGBMClassifier","ba1f3b25":"Get score on training set and validation set for our StackNetClassifier","153e8781":"Score probs we have positive high score with probs >.0.5","279a6a36":"###### Import PyStackNet Package","3383f1db":"Utils","5f2a3f37":"<a id='t2_1'><\/a>\n# <div class=\"h2\">StackNet Modeling<\/div>\n\n[Back to Contents](#top)\n\n[Next](#t2_2)\n \nStackNet allows you to define all kinds of models. For example, Sklearn models, LightGBM, XGBoost, CatBoost and Keras models can all be used with StackNet.For the individual models, you are responsible for not overfitting.","1282a908":"<a id='t2_3'><\/a>\n# <div class=\"h2\">Submission<\/div>\n\n[Back to Contents](#top)\n\n[Next](#t3)\n"}}