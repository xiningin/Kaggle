{"cell_type":{"9a361938":"code","03b8efac":"code","338ee05a":"code","e61e3480":"code","7aa8831c":"code","44a123a3":"code","f4adc4d9":"code","370e5971":"code","65e8992b":"code","435d34ce":"code","5b0177c3":"code","ca4aae10":"code","db2703e2":"code","93ed0a19":"code","8d96f126":"code","20b4c165":"code","e595d497":"code","c32a1257":"code","1887f40c":"code","1cc4892d":"code","c5f10156":"code","8e0d80a4":"code","40feab93":"code","dce6d97a":"code","8067b6a1":"code","19544465":"code","a3714522":"code","5127ca24":"code","29c64008":"code","fa7d5214":"code","d79e5018":"code","380bc429":"code","70cd7d76":"code","137bbd31":"code","bc614260":"code","3a07a7a7":"code","ae357d94":"markdown","5716d8a3":"markdown","47faf4a7":"markdown","3f6673ba":"markdown","466adc73":"markdown","f9ec3616":"markdown","8313fe77":"markdown","3ffafbc7":"markdown","eb02480f":"markdown","288c2505":"markdown","9e5b593a":"markdown","3042e364":"markdown","6cc59a43":"markdown","39937037":"markdown","ae5a1c28":"markdown","80bc89ff":"markdown","13660d71":"markdown","c581d39f":"markdown","acfecc43":"markdown","5041e2b1":"markdown","b7268301":"markdown"},"source":{"9a361938":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport keras\n\nfrom wordcloud import WordCloud\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, SimpleRNN\nfrom keras.initializers import Constant\nimport keras.metrics as metrics\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n\nimport warnings\nimport string\nimport re\nwarnings.filterwarnings('ignore')\nsns.set()","03b8efac":"headlines = pd.read_json(\"\/kaggle\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json\", lines=True)\nheadlines.head()","338ee05a":"headlines.shape","e61e3480":"headlines.isnull().sum().sum()","7aa8831c":"headlines.describe(include='object')","44a123a3":"len(headlines[headlines.headline.duplicated()])","f4adc4d9":"count_of_target = 0\nfor target in headlines.groupby(headlines.headline).is_sarcastic.agg(list):\n    if(len(set(list(target))) != 1):\n        count_of_target += 1\nprint(count_of_target)","370e5971":"plt.figure(figsize=(20,5))\nsns.countplot(headlines.is_sarcastic, palette='Blues')\nplt.title(\"Distribution of Target Counts\", size=25, weight='bold')\nplt.xlabel(\"Target Label\", size=14)\nplt.ylabel(\"Frequency\", size=14)\nplt.show()","65e8992b":"all_keywords = \" \".join(line for line in headlines[headlines.is_sarcastic==1].headline)\nword_cloud= WordCloud(width=1250, height=625, max_font_size=350, \n                      random_state=42).generate(all_keywords)\nplt.figure(figsize=(20, 10))\nplt.title(\"Words used for Sarcasm\", size=20, weight=\"bold\")\nplt.imshow(word_cloud)\nplt.axis(\"off\")\nplt.show()","435d34ce":"all_keywords = \" \".join(line for line in headlines[headlines.is_sarcastic==0].headline)\nword_cloud= WordCloud(width=1250, height=625, max_font_size=350, \n                      random_state=42).generate(all_keywords)\nplt.figure(figsize=(20, 10))\nplt.title(\"Words used for not Sarcasm\", size=20, weight=\"bold\")\nplt.imshow(word_cloud)\nplt.axis(\"off\")\nplt.show()","5b0177c3":"def clean_headline(headline):\n    STOPWORDS = set(stopwords.words(\"english\"))\n    headline = headline.lower()\n    headline = \" \".join(word for word in headline.split() if word not in STOPWORDS)\n    headline = \"\".join(word for word in headline if word not in set(string.punctuation))\n    return headline\n\nheadlines.headline = headlines.headline.apply(clean_headline)\nheadlines.drop('article_link', inplace=True, axis=1)\nheadlines.head()","ca4aae10":"corpus = []\nfor text in headlines['headline']:\n    words = [word.lower() for word in word_tokenize(text)] \n    corpus.append(words)\nnum_words = len(corpus)\nprint(num_words)","db2703e2":"X = headlines['headline'].values\ny = headlines['is_sarcastic'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)","93ed0a19":"max_len = 32\ntokenizer = Tokenizer(num_words)\ntokenizer.fit_on_texts(X_train)\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_train = pad_sequences(X_train, maxlen=max_len, truncating='post', padding='post')\n\nX_test = tokenizer.texts_to_sequences(X_test)\nX_test = pad_sequences(X_test, maxlen=max_len, truncating='post', padding='post')","8d96f126":"word_index = tokenizer.word_index\nprint(\"Number of unique words: {}\".format(len(word_index)))","20b4c165":"embedding = {}\nwith open(\"\/kaggle\/input\/glovetwitter27b100dtxt\/glove.twitter.27B.100d.txt\") as file:\n    for line in file:\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:], 'float32')\n        embedding[word] = vectors\nfile.close()","e595d497":"embedding_matrix = np.zeros((num_words, 100))\nfor i, word in tokenizer.index_word.items():\n    if i < (num_words+1):\n        vector = embedding.get(word)\n        if vector is not None:\n            embedding_matrix[i] = vector","c32a1257":"embedding_matrix.shape","1887f40c":"print(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","1cc4892d":"simpleRNN = model = Sequential()\n\nsimpleRNN.add(Embedding(input_dim=num_words, output_dim=100, \n                    embeddings_initializer=Constant(embedding_matrix), \n                    input_length=max_len, trainable=False))\nsimpleRNN.add(SimpleRNN(64, dropout=0.1))\nsimpleRNN.add(Dense(1, activation='sigmoid'))\n\nsimpleRNN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","c5f10156":"simpleRNN.summary()","8e0d80a4":"history = simpleRNN.fit(X_train, y_train, epochs=4, batch_size=1024, validation_data=(X_test, y_test))","40feab93":"print(classification_report(y_test, simpleRNN.predict_classes(X_test)))","dce6d97a":"lstm_model = Sequential()\n\nlstm_model.add(Embedding(input_dim=num_words, output_dim=100, \n                    embeddings_initializer=Constant(embedding_matrix), \n                    input_length=max_len, trainable=False))\nlstm_model.add(LSTM(50, dropout=0.1))\nlstm_model.add(Dense(1, activation='sigmoid'))\n\nlstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","8067b6a1":"lstm_model.summary()","19544465":"history = lstm_model.fit(X_train, y_train, epochs=4, batch_size=1024, validation_data=(X_test, y_test))","a3714522":"plt.figure(figsize=(16,5))\nepochs = range(1, len(history.history['accuracy'])+1)\nplt.plot(epochs, history.history['loss'], 'b', label='Training Loss', color='red')\nplt.plot(epochs, history.history['val_loss'], 'b', label='Validation Loss')\nplt.legend()\nplt.show()","5127ca24":"plt.figure(figsize=(16,5))\nepochs = range(1, len(history.history['accuracy'])+1)\nplt.plot(epochs, history.history['accuracy'], 'b', label='Training Accuracy', color='red')\nplt.plot(epochs, history.history['val_accuracy'], 'b', label='Validation Accuracy')\nplt.legend()\nplt.show()","29c64008":"print(classification_report(y_test, lstm_model.predict_classes(X_test)))","fa7d5214":"biLSTM = Sequential()\n\nbiLSTM.add(Embedding(input_dim=num_words, output_dim=100, \n                    embeddings_initializer=Constant(embedding_matrix), \n                    input_length=max_len, trainable=False))\nbiLSTM.add(Bidirectional(LSTM(units=64, recurrent_dropout = 0.3, dropout = 0.3, \n                             return_sequences = True)))\nbiLSTM.add(Bidirectional(LSTM(units=32, recurrent_dropout = 0.1, dropout = 0.1)))\nbiLSTM.add(Dense(1, activation='sigmoid'))\n\nbiLSTM.compile(optimizer=keras.optimizers.Adam(lr = 0.01), \n              loss='binary_crossentropy', metrics=['accuracy'])","d79e5018":"biLSTM.summary()","380bc429":"history = biLSTM.fit(X_train, y_train, epochs=4, batch_size=1024, \n                    validation_data=(X_test, y_test))","70cd7d76":"plt.figure(figsize=(16,5))\nepochs = range(1, len(history.history['accuracy'])+1)\nplt.plot(epochs, history.history['loss'], 'b', label='Training Loss', color='red')\nplt.plot(epochs, history.history['val_loss'], 'b', label='Validation Loss')\nplt.legend()\nplt.show()","137bbd31":"plt.figure(figsize=(16,5))\nepochs = range(1, len(history.history['accuracy'])+1)\nplt.plot(epochs, history.history['accuracy'], 'b', label='Training Accuracy', color='red')\nplt.plot(epochs, history.history['val_accuracy'], 'b', label='Validation Accuracy')\nplt.legend()\nplt.show()","bc614260":"print(classification_report(y_test, biLSTM.predict_classes(X_test)))","3a07a7a7":"accuracy_simple_rnn = accuracy_score(y_test, simpleRNN.predict_classes(X_test))\naccuracy_lstm = accuracy_score(y_test, lstm_model.predict_classes(X_test))\naccuracy_bi_lstm = accuracy_score(y_test, biLSTM.predict_classes(X_test))\n\nconclusion = pd.DataFrame({'Models':['SimpleRNN', 'LSTM', 'BiLSTM'], \n              'Accuracy':[accuracy_simple_rnn, accuracy_lstm, accuracy_bi_lstm]})\nconclusion['Accuracy'] = conclusion['Accuracy'].apply(lambda x: round(x,2))\nconclusion.sort_values(by='Accuracy', inplace = True, ascending = False)\n\nplt.figure(figsize=(20,10))\nseaborn_plot = sns.barplot(conclusion.Models,conclusion.Accuracy)\nfor p in seaborn_plot.patches:\n    seaborn_plot.annotate(format(p.get_height(), '.2f'), (p.get_x() + p.get_width() \/ 2., p.get_height()), ha = 'center',\n                   va = 'center', xytext = (0, 9), textcoords = 'offset points')\nplt.title(\"Evaluation\", weight='bold', size=25)\nplt.xticks(size=18)\nplt.xlabel(\"Models\")\nplt.ylabel(\"Accuracy\")\nplt.show()","ae357d94":"It can be derived from the wordcloud that for headlines with sarcasm consists of words like 'man', 'new' and 'report' are used often and for headlines with not sarcasm contains words like 'trump', 'say' and more. ","5716d8a3":"# LSTM Model","47faf4a7":"The dataset has 0 null values.","3f6673ba":"# Modelling\n\nI explained in my previous notebook why RNNs are great for sequence modelling. Although RNNs are great, there are some issues when training a RNN. \n\n1. If the values involved in the repeated computation during backpropogation through time such as the weight vectors or the gradients itself are greater than 1 (values > 1) it can be problematic as gradients will become extremely large and difficult to optimise. The intuition comes from multiplying large numbers with large numbers results in very large numbers. This problem is known as exploding gradients. \nA common and relatively easy solution to the exploding gradients problem is to change the derivative of the error before propagating it backward through the network and using it to update the weights. It can be done using any of the two approaches which include:\n\n    (A) Rescaling the gradients given a chosen vector norm \n\n    (B) Clipping gradient values that exceed a preferred range. \nTogether, these methods are referred to as \u201cgradient clipping\u201d \n\n\n2. If these values involved in this repeated computation such as the weight vectors or the gradients itself are less than 1 (values < 1) it can be problematic as gradients will become extremely small and difficult to optimise and infer information. This problem is known as vanishing gradients.\nBut why is that a problem, right?\nWe first learnt that we need a model which can capture long-term dependencies. If you take an example of a really large text as input, in order to find the loss at the last state, i.e at the end the model has to propagate all the way to the very beginning. In order to do that, the model will multiply (dot product) a lot of small numbers with a lot of small numbers and hence it would result in a very small number known as the vanishing gradient problem. Hence, it will bias the parameters to capture short term dependencies as the information will be vanishingly small for long-term dependencies. \nOn the other hand, vanishing gradients can be solved using either of the following solutions:\n\n    (A) Activation function: using ReLU prevents gradients from shrinking when x>0\n    \n    (B) Weight initialisation: initialise weights to identity matrix and initialize bias to zero\n    \n    (C) Network architecture: use Gated Cells to control what information is passed through such as LSTMS or GRU and more..\n    \n# LSTMs or Long Short Term Memory\n\nThe key building block behind LSTMs is the structure called a gate which functions to enable the LSTM to be able to selectively add or remove information through its cell state. Gates consists of neural network layers like a sigmoid and a pointwise multiplication. The sigmoid function is forcing the input to the gate to be between 0 (nothing) and 1 (everything) for any input. The intuition is to capture how much information is either stored or removed. \n\nBut how do they work?\n\nLSTMs perform the following functions:\n\n1. Forget: LSTMs forget irrelevant parts of the previous state\n\n2. Store: LSTMs store relevant new information into the cell state\n\n3. Update: LSTMs selectively update cell state values\n\n4. Output\n\nThe first step for an LSTM is to decide what information is to be forgotten from the cell state.\n\nAfter that, LSTMs decide what part of the new information is to be stored. \n\nThen, it takes both relevant parts of the prior information as well as the current input and uses this to selectively update the cell state values. \n\nFinally, it can return an output through the output gate which controls what information is sent to the next time step.\n\nNow that we know how LSTMs work, how do they solve the vanishing gradient problem?\n\nThe property of LSTMs using gates to perform these operations actually work to create this internal cell state C which allows for the uninterrupted flow of the gradients through time. The intuition behind is that you can think of it as a highway for the cellstate where gradients can flow uninterrupted. This enables it to mitigate the vanishing gradient problems. But really, how?\n\nThe LSTM architecture makes it easier for the RNN to preserve information over many timesteps. Keyword here is preserve. For example, if the forget gate is set to remember everything on every time step, then the information in the cell is preserved indefinitely. (This might not be sounding like a good strategy, but the point is that it is at least a fairly easier way to preserve information). By contrast, it's harder for vanilla RNN models to learn a recurrent weight vector Wh that preserves the hidden state and thus are more robust to the vanishing gradient problem. \n\nHowever, LSTMs don't necessarily guarantee that there is no vanishing\/exploding gradient, but it does provide an easier way for the model to learn long-term dependencies. \n","466adc73":"# Preprocessing\n\nI will use GLoVe Embeddings as they represent words using the context and learn word vectors such that their dot product equals the logarithm of the words probability of co-occurence. If you are not familier with GLoVe Embeddings, I have made another notebook which is beginner friendly which explains why GLoVe Embeddings are useful and how to implement them using Deep Learning models such as RNNs.\n\nLink: https:\/\/www.kaggle.com\/shivam017arora\/imdb-sentiment-analysis","f9ec3616":"Importing Dependencies","8313fe77":"The dataset contains 28619 rows and 3 columns.","3ffafbc7":"# Data Cleaning","eb02480f":"# Simple RNN","288c2505":"# Bidirectional LSTM\n\nSince our goal is to predict sarcasm which can be used in any context, using a LSTM model which is unidirectional is not gonna cut it. This is why the unidirecitonal LSTM model only gets an accuracy score of 74%. This is because, consider a sentence \"This movie was good, not!\", since the sentence contains the word 'not' in the right context instead of left. Hence, a better solution is using a bi-directional LSTM model. \n\nBi-directional RNNs are only useful when you have the full sentence as input to the model i.e it cannot be used for language modelling (tasks such as predicting the next word) because in language modelling you only have the left context available but if the full sentence is available, bi-directional RNNs are powerful to use and should be considered as default.\n\nFor example, BERT (Bidirectional Encoder Representations from Transformers) is a powerful pretrained contextual representation system built on bidirectionality.\n\nIn the future, I will be making a notebook (beginner friendly) for explaning how BERT model works.","9e5b593a":"# Conclusion\n\nConsidering this dataset is small, I got a decent accuracy of 81% with significant improvement using Bidirectional LSTM model as compared with vanilla LSTM and RNN models.  \n\n### References\nMIT: Introduction to Deep Learning: 6.S191\n\nStanford: Natural Language Processing with Deep Learning: CS224N\n\nI highly recommend going through these amazing courses available for free.\n\n#### If you enjoyed and liked my work, please follow up and upvote and comment your feedback.","3042e364":"Task2: Make matrix of all words in imdb-dataset with vectors from embedding dictionary","6cc59a43":"Tokenizing the words and padding for equal input dimensions","39937037":"Task1: Make dictionary of all words in corpus in pre-trained glove embeddings","ae5a1c28":"The dataset has 116 duplicated headlines. I wonder if these duplicated headlines are classified differently or not. ","80bc89ff":"Splitting data into train(80%) and test(20%)","13660d71":"# Understanding the Data","c581d39f":"As displayed, there is an imbalance towards the positive class (sarcastic) in our dataset. This is important as it can have significant effect on the classifier although the imbalance is not severe hence we will not perform any oversampling and undersampling techniques to tweak this but it still worthy of note-taking. ","acfecc43":"# Hi!\n\nThis notebook will give a beginner guide to LSTMs and detect whether a news headline is sarcastic or not. \n\n## Problem Statement\n\nTheOnion aims at producing sarcastic versions of current events and we collected all the headlines from News in Brief and News in Photos categories (which are sarcastic). We collect real (and non-sarcastic) news headlines from HuffPost.\n\nThis dataset has following advantages over the existing Twitter datasets:\n\n1. Since news headlines are written by professionals in a formal manner, there are no spelling mistakes and informal usage. This reduces the sparsity and also increases the chance of finding pre-trained embeddings.\n\n\n2. Furthermore, since the sole purpose of TheOnion is to publish sarcastic news, we get high-quality labels with much less noise as compared to Twitter datasets.\n\n\n3. Unlike tweets which are replies to other tweets, the news headlines we obtained are self-contained. This would help us in teasing apart the real sarcastic elements.\n\n##### Inspiration\n\nCan you identify sarcastic sentences? Can you distinguish between fake news and legitimate news?","5041e2b1":"It is important to consider this as if there are different labels for the same headline, it will confuse the model while training. Fortunately, although there are duplicate headlines in the dataset, they are not labelled differently.","b7268301":"Importing Data"}}