{"cell_type":{"a4a62c69":"code","6eb3f427":"code","f8735f9c":"code","a3df6829":"code","52bab3c7":"code","35949d68":"code","94e8513c":"code","f63a59a7":"code","d7030dd4":"code","c5f3b3c1":"code","d35bbfe0":"code","94c96ac0":"code","d90571d4":"code","a1d246a1":"code","f7bdd4ac":"code","9726183e":"code","bf4122f0":"code","58ed3faf":"code","35ce067b":"code","0ec85f5f":"code","da293ea4":"code","d08cecc6":"code","2c638088":"code","5bcf974a":"code","35e0dc52":"code","76363d21":"code","3c3d295f":"code","ae83f776":"code","7fd34765":"code","d9572e92":"code","335bc343":"code","8d92b96d":"code","79d785fa":"code","91e351fe":"code","0d1aee33":"code","80948170":"code","9a5ca5b8":"code","a8e18b1b":"code","9fc0cacb":"code","e9baa6bf":"code","eaf8cc89":"code","64719041":"markdown"},"source":{"a4a62c69":"# Import necessary libraries\nfrom copy import deepcopy\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt","6eb3f427":"import random \nfrom sklearn.preprocessing import StandardScaler\nfrom IPython.display import display\nfrom sklearn.cluster import KMeans \nfrom sklearn.decomposition import PCA\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.metrics import homogeneity_score, completeness_score, \\\nv_measure_score, adjusted_rand_score, adjusted_mutual_info_score, silhouette_score\n%matplotlib inline\n\nnp.random.seed(123)","f8735f9c":"df = pd.read_csv(\"..\/input\/iris\/Iris.csv\") #load the dataset\ndf.drop('Id',axis=1,inplace=True) # Se elimina la columna no requerida","a3df6829":"gr = pd.read_csv(\"..\/input\/greedata\/new.csv\")\ngc = pd.read_csv(\"..\/input\/greedata\/cleaned.csv\")","52bab3c7":"grdata = gr.values[:, 0:12]\ngcdata = gc.values[:, 0:12]","35949d68":"label = gr.values[:, 12]\nlabel1= gc.values[:, 12]","94e8513c":"from sklearn.cluster import KMeans\nwcss = []\n\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n    kmeans.fit(grdata)\n    wcss.append(kmeans.inertia_)\n    \n#Plotting the results onto a line graph, allowing us to observe 'The elbow'\nplt.plot(range(1, 11), wcss)\nplt.title('The elbow method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS') #within cluster sum of squares\nplt.show()","f63a59a7":"#Applying kmeans to the dataset \/ Creating the kmeans classifier\nkmeans = KMeans(n_clusters = 5, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\ny_kmeans = kmeans.fit_predict(grdata)\n\n#Visualising the clusters\nplt.scatter(grdata[y_kmeans == 0, 0], grdata[y_kmeans == 0, 1], s = 100, marker='v', c = 'red', label = 'normal')\nplt.scatter(grdata[y_kmeans == 1, 0], grdata[y_kmeans == 1, 1], s = 100, marker='^', c = 'blue', label = '2')\nplt.scatter(grdata[y_kmeans == 2, 0], grdata[y_kmeans == 2, 1], s = 100, marker='<',c = 'green', label = '3')\nplt.scatter(grdata[y_kmeans == 3, 0], grdata[y_kmeans == 3, 1], s = 100, marker='+',c = 'yellow', label = '4')\nplt.scatter(grdata[y_kmeans == 4, 0], grdata[y_kmeans == 4, 1], s = 100, marker='1',c = 'indigo', label = '5')\n\n#Plotting the centroids of the clusters\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s = 150,marker='*', c = 'c', label = 'Centroids')\n\nplt.legend()","d7030dd4":"def k_means(n_clust, data_frame, true_labels):\n    \"\"\"\n    Function k_means applies k-means clustering alrorithm on dataset and prints the crosstab of cluster and actual labels \n    and clustering performance parameters.\n    \n    Input:\n    n_clust - number of clusters (k value)\n    data_frame - dataset we want to cluster\n    true_labels - original labels\n    \n    Output:\n    1 - crosstab of cluster and actual labels\n    2 - performance table\n    \"\"\"\n    k_means = KMeans(n_clusters = n_clust,init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n    k_means.fit(data_frame)\n    c_labels = k_means.labels_\n    df = pd.DataFrame({'clust_label': c_labels, 'orig_label': true_labels.tolist()})\n    ct = pd.crosstab(df['clust_label'], df['orig_label'])\n    \n    display(df['clust_label'])\n    df['clust_label'].to_csv(\"submission.csv\", index = False)\n    y_clust = k_means.predict(data_frame)\n    display(ct)\n    print('% 9s' % 'inertia  homo    compl   v-meas   ARI     AMI     silhouette')\n    print('%i   %.3f   %.3f   %.3f   %.3f   %.3f    %.3f'\n      %(k_means.inertia_,\n      homogeneity_score(true_labels, y_clust),\n      completeness_score(true_labels, y_clust),\n      v_measure_score(true_labels, y_clust),\n      adjusted_rand_score(true_labels, y_clust),\n      adjusted_mutual_info_score(true_labels, y_clust),\n      silhouette_score(data_frame, y_clust, metric='euclidean')))","c5f3b3c1":"k_means(n_clust=5, data_frame=grdata, true_labels=label)","d35bbfe0":"#normalize the dataset\nscaler = StandardScaler()\nData = scaler.fit_transform(grdata)","94c96ac0":"Data1 = pd.DataFrame(Data)\n\nData1.to_csv('Data1.csv', index=False)","d90571d4":"#check the optimal k value\nks = range(1, 10)\ninertias = []\n\nfor k in ks:\n    model = KMeans(n_clusters=k)\n    model.fit(Data)\n    inertias.append(model.inertia_)\n\nplt.figure(figsize=(8,5))\nplt.style.use('bmh')\nplt.plot(ks, inertias, '-o')\nplt.xlabel('Number of clusters, k')\nplt.ylabel('Inertia')\nplt.xticks(ks)\nplt.show()","a1d246a1":"k_means(n_clust=5, data_frame=Data, true_labels=label)","f7bdd4ac":"#Applying kmeans to the dataset \/ Creating the kmeans classifier\nkmeans = KMeans(n_clusters = 5, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\ny_kmeans = kmeans.fit_predict(Data)\n\n#Visualising the clusters\nplt.scatter(Data[y_kmeans == 0, 0], Data[y_kmeans == 0, 1], s = 100, marker='v', c = 'red', label = 'normal')\nplt.scatter(Data[y_kmeans == 1, 0], Data[y_kmeans == 1, 1], s = 100, marker='^', c = 'blue', label = '2')\nplt.scatter(Data[y_kmeans == 2, 0], Data[y_kmeans == 2, 1], s = 100, marker='<',c = 'green', label = '3')\nplt.scatter(Data[y_kmeans == 3, 0], Data[y_kmeans == 3, 1], s = 100, marker='+',c = 'yellow', label = '4')\nplt.scatter(Data[y_kmeans == 4, 0], Data[y_kmeans == 4, 1], s = 100, marker='1',c = 'indigo', label = '5')\n\n#Plotting the centroids of the clusters\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s = 150,marker='*', c = 'c', label = 'Centroids')\n\nplt.legend()","9726183e":"#check the optimal k value\nks = range(1, 10)\ninertias = []\n\nfor k in ks:\n    model = KMeans(n_clusters=k)\n    model.fit(gc)\n    inertias.append(model.inertia_)\n\nplt.figure(figsize=(8,5))\nplt.style.use('bmh')\nplt.plot(ks, inertias, '-o')\nplt.xlabel('Number of clusters, k')\nplt.ylabel('Inertia')\nplt.xticks(ks)\nplt.show()","bf4122f0":"label = gr.values[:, 12]\nlabel1= gc.values[:, 12]","58ed3faf":"k_means(n_clust=5, data_frame=gc, true_labels=label1)","35ce067b":"#check for optimal number of features\npca = PCA(random_state=\"auto\")\npca.fit(gc)\nfeatures = range(pca.n_components_)\n\nplt.figure(figsize=(8,4))\nplt.bar(features[:15], pca.explained_variance_[:15], color='lightskyblue')\nplt.xlabel('PCA feature')\nplt.ylabel('Variance')\nplt.xticks(features[:15])\nplt.show()","0ec85f5f":"def pca_transform(n_comp):\n    pca = PCA(n_components=n_comp, random_state=123)\n    global Data_reduced\n    Data_reduced = pca.fit_transform(gc)\n    print('Shape of the new Data df: ' + str(Data_reduced.shape))","da293ea4":"pca_transform(n_comp=2)","d08cecc6":"from sklearn.cluster import KMeans\nwcss = []\n\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n    kmeans.fit(Data_reduced)\n    wcss.append(kmeans.inertia_)\n    \n#Plotting the results onto a line graph, allowing us to observe 'The elbow'\nplt.plot(range(1, 11), wcss)\nplt.title('The elbow method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS') #within cluster sum of squares\nplt.show()","2c638088":"label = gr.values[:, 12]\nlabel1= gc.values[:, 12]","5bcf974a":"k_means(n_clust=5, data_frame=Data_reduced, true_labels=label1)","35e0dc52":"#Applying kmeans to the dataset \/ Creating the kmeans classifier\nkmeans = KMeans(n_clusters = 5, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\ny_kmeans = kmeans.fit_predict(Data_reduced)\n\n#Visualising the clusters\nplt.scatter(Data_reduced[y_kmeans == 0, 0], Data_reduced[y_kmeans == 0, 1], s = 100, marker='v', c = 'red', label = 'normal')\nplt.scatter(Data_reduced[y_kmeans == 1, 0], Data_reduced[y_kmeans == 1, 1], s = 100, marker='^', c = 'blue', label = '2')\nplt.scatter(Data_reduced[y_kmeans == 2, 0], Data_reduced[y_kmeans == 2, 1], s = 100, marker='<',c = 'green', label = '3')\nplt.scatter(Data_reduced[y_kmeans == 3, 0], Data_reduced[y_kmeans == 3, 1], s = 100, marker='+',c = 'yellow', label = '4')\nplt.scatter(Data_reduced[y_kmeans == 4, 0], Data_reduced[y_kmeans == 4, 1], s = 100, marker='1',c = 'indigo', label = '5')\n\n#Plotting the centroids of the clusters\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s = 150,marker='*', c = 'c', label = 'Centroids')\n\nplt.legend()","76363d21":"from sklearn.cluster import KMeans\nwcss = []\n\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n    kmeans.fit(Data_reduced)\n    wcss.append(kmeans.inertia_)\n    \n#Plotting the results onto a line graph, allowing us to observe 'The elbow'\nplt.plot(range(1, 11), wcss)\nplt.title('The elbow method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS') #within cluster sum of squares\nplt.show()","3c3d295f":"Data2 = pd.DataFrame(Data_reduced)\nData2.to_csv('Data2.csv', index=False)","ae83f776":"pca_transform(n_comp=1)","7fd34765":"from sklearn.cluster import KMeans\nwcss = []\n\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n    kmeans.fit(Data_reduced)\n    wcss.append(kmeans.inertia_)\n    \n#Plotting the results onto a line graph, allowing us to observe 'The elbow'\nplt.plot(range(1, 11), wcss)\nplt.title('The elbow method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS') #within cluster sum of squares\nplt.show()","d9572e92":"k_means(n_clust=5, data_frame=Data_reduced, true_labels=label1)","335bc343":"k_means(n_clust=6, data_frame=Data_reduced, true_labels=label1)","8d92b96d":"km = KMeans(n_clusters = 6, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\nx = Data_reduced.copy()\ny_means = km.fit_predict(x)\n\nplt.plot(x[y_means == 0],  marker='v', c = 'red', label = 'normal')\nplt.plot(x[y_means == 1],  marker='^', c = 'blue', label = '2')\nplt.plot(x[y_means == 2],  marker='<',c = 'green', label = '3')\nplt.plot(x[y_means == 3],  marker='+',c = 'yellow', label = '4')\nplt.plot(x[y_means == 4],  marker='1',c = 'indigo', label = '5')\nplt.plot(x[y_means == 5],  marker='1',c = 'orange', label = '*')\nplt.plot(km.cluster_centers_[:,0],c = 'c', marker='*', label = 'centeroid')\n\nprint(km.cluster_centers_[:,0])\n\nplt.style.use('fivethirtyeight')\nplt.title('K Means Clustering', fontsize = 20)\nplt.xlabel('Income')\nplt.ylabel('Score')\nplt.legend()\nplt.grid()\nplt.show()","79d785fa":"Data3 = pd.DataFrame(Data_reduced)\nData3.to_csv('Data3.csv', index=False)","91e351fe":"f = pd.read_csv(\"..\/input\/greedata\/fixed.csv\")\nlabel2= f.values[:, 1]","0d1aee33":"from sklearn.cluster import KMeans\nwcss = []\n\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n    kmeans.fit(f)\n    wcss.append(kmeans.inertia_)\n    \n#Plotting the results onto a line graph, allowing us to observe 'The elbow'\nplt.plot(range(1, 11), wcss)\nplt.title('The elbow method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS') #within cluster sum of squares\nplt.show()","80948170":"k_means(n_clust=2, data_frame=f, true_labels=label2)","9a5ca5b8":"km = KMeans(n_clusters = 2, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\nx = f\ny_means = km.fit_predict(x)\n\nplt.plot(x[y_means == 0],  marker='v', c = 'red', label = 'normal')\nplt.plot(x[y_means == 1],  marker='^', c = 'blue', label = '2')\n# plt.plot(x[y_means == 2],  marker='<',c = 'green', label = '3')\n# plt.plot(x[y_means == 3],  marker='+',c = 'yellow', label = '4')\n# plt.plot(x[y_means == 4],  marker='1',c = 'indigo', label = '5')\n\nplt.plot(km.cluster_centers_[:,0],c = 'c', marker='*', label = 'centeroid')\n\nprint(km.cluster_centers_[:,0])\n\nplt.style.use('fivethirtyeight')\nplt.title('K Means Clustering', fontsize = 20)\nplt.xlabel('Income')\nplt.ylabel('Score')\nplt.legend()\nplt.grid()\nplt.show()","a8e18b1b":"t = pd.read_csv(\"..\/input\/greedata\/testify_1.csv\")\nlabel3= t.values[:, 1]","9fc0cacb":"from sklearn.cluster import KMeans\nwcss = []\n\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n    kmeans.fit(t)\n    wcss.append(kmeans.inertia_)\n    \n#Plotting the results onto a line graph, allowing us to observe 'The elbow'\nplt.plot(range(1, 11), wcss)\nplt.title('The elbow method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS') #within cluster sum of squares\nplt.show()","e9baa6bf":"k_means(n_clust=2, data_frame=t, true_labels=label3)","eaf8cc89":"km = KMeans(n_clusters = 2, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\nx = t\ny_means = km.fit_predict(x)\n\nplt.plot(x[y_means == 0],  marker='+', c = 'red', label = 'normal')\nplt.plot(x[y_means == 1],  marker='^', c = 'blue', label = '2')\nplt.plot(x[y_means == 2],  marker='<',c = 'green', label = '3')\nplt.plot(x[y_means == 3],  marker='+',c = 'yellow', label = '4')\nplt.plot(x[y_means == 4],  marker='1',c = 'indigo', label = '5')\n\nplt.plot(km.cluster_centers_[:,0],c = 'c', marker='*', label = 'centeroid')\n\nprint(km.cluster_centers_[:,0])\n\nplt.style.use('fivethirtyeight')\nplt.title('K Means Clustering', fontsize = 20)\nplt.xlabel('Income')\nplt.ylabel('Score')\nplt.legend()\nplt.grid()\nplt.show()","64719041":"# Test on Dataset"}}