{"cell_type":{"c9a89539":"code","f5020b8b":"code","67c26097":"code","5c64ec9b":"code","131111ec":"code","2ccc78f3":"code","b3afe6a4":"code","67a0c139":"code","3275fda1":"code","785ae440":"code","5da4a3ac":"code","a7f776cb":"code","6913f9d4":"code","004d1017":"code","cc5bee8d":"code","155bd04d":"code","1b9d49a2":"code","9e514064":"code","21b41be4":"code","359e6f0f":"code","2f1dc3a7":"code","594d70dd":"code","ffcbdd00":"code","6107ffb9":"code","45034164":"code","8b04f5aa":"code","42918afb":"code","4ad76cf1":"code","159a8b4e":"code","41452d57":"code","521d4a94":"code","60ac107f":"code","1687c5d8":"code","68f43720":"code","e0a9e380":"code","41804a84":"markdown","fcf9d02f":"markdown","e338f445":"markdown","5f25a376":"markdown","0dd18092":"markdown","37c3cab3":"markdown","116dd5fa":"markdown","4e70475e":"markdown","164851ee":"markdown","86c7758e":"markdown","f811db25":"markdown","f7c4d916":"markdown","d5751bd7":"markdown","30134601":"markdown","314e8ecf":"markdown"},"source":{"c9a89539":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport math\nimport os\nimport time\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Flatten,Embedding,Activation, Dropout , Input\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D , LSTM, Bidirectional\n\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, layers\n\nfrom tensorflow.keras.optimizers import Adam\nfrom keras import optimizers, callbacks \nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Flatten\nimport sklearn.metrics as metrics\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","f5020b8b":"# Lecture Data frame\ntrain_df = pd.read_csv('..\/input\/quora-insincere-questions-classification\/train.csv')\ntest_df  = pd.read_csv('..\/input\/quora-insincere-questions-classification\/test.csv')\n\ntrain_df.head()","67c26097":"print(train_df.columns)","5c64ec9b":"train_df['question_text'].str.len().hist(color='y')","131111ec":"f, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 4))\n\nsns.countplot(train_df.target, palette=['blue', 'salmon'], ax=ax)\n\nax.set_title(\"Distribution des categories\", fontsize=16)\nax.set_ylabel(ylabel='Count', fontsize=14)\nax.set_xticklabels(labels=['Sincere', 'Insincere'], fontsize=14)\nax.set_xlabel(xlabel='Category', fontsize=14)\n\nplt.show()","2ccc78f3":"train_df[\"quest_len\"] = train_df[\"question_text\"].apply(lambda x: len(x.split()))\n\nsincere = train_df[train_df[\"target\"] == 0]\ninsincere = train_df[train_df[\"target\"] == 1]\n\nplt.figure(figsize = (15, 8))\nsns.distplot(sincere[\"quest_len\"], hist = True, label = \"sincere\")\nsns.distplot(insincere[\"quest_len\"], hist = True, label = \"insincere\")\nplt.legend(fontsize = 10)\nplt.title(\"Longueur des questions par Classe\", fontsize = 12)\nplt.show()\n\ntrain_df[\"target\"].value_counts()","b3afe6a4":"print(\"Train shape : \",train_df.shape)\nprint(\"Test shape : \",test_df.shape)","67a0c139":"# remove space\nspaces = ['\\u200b', '\\u200e', '\\u202a', '\\u202c', '\\ufeff', '\\uf0d8', '\\u2061', '\\x10', '\\x7f', '\\x9d', '\\xad', '\\xa0']\ndef remove_space(text):\n    \"\"\"\n    remove extra spaces and ending space if any\n    \"\"\"\n    for space in spaces:\n        text = text.replace(space, ' ')\n    text = text.strip()\n    text = re.sub('\\s+', ' ', text)\n    return text\n","3275fda1":"# replace strange punctuations and raplace diacritics\nfrom unicodedata import category, name, normalize\n\ndef remove_diacritics(s):\n    return ''.join(c for c in normalize('NFKD', s.replace('\u00f8', 'o').replace('\u00d8', 'O').replace('\u207b', '-').replace('\u208b', '-'))\n                  if category(c) != 'Mn')\n\nspecial_punc_mappings = {\"\u2014\": \"-\", \"\u2013\": \"-\", \"_\": \"-\", '\u201d': '\"', \"\u2033\": '\"', '\u201c': '\"', '\u2022': '.', '\u2212': '-',\n                         \"\u2019\": \"'\", \"\u2018\": \"'\", \"\u00b4\": \"'\", \"`\": \"'\", '\\u200b': ' ', '\\xa0': ' ','\u060c':'','\u201e':'',\n                         '\u2026': ' ... ', '\\ufeff': ''}\ndef clean_special_punctuations(text):\n    for punc in special_punc_mappings:\n        if punc in text:\n            text = text.replace(punc, special_punc_mappings[punc])\n    \n    text = remove_diacritics(text)\n    return text","785ae440":"def clean_number(text):\n    \n    text = re.sub(r'(\\d+)([a-zA-Z])', '\\g<1> \\g<2>', text)\n    text = re.sub(r'(\\d+) (th|st|nd|rd) ', '\\g<1>\\g<2> ', text)\n    text = re.sub(r'(\\d+),(\\d+)', '\\g<1>\\g<2>', text)\n    \n    return text","5da4a3ac":"def decontracted(text):\n    # specific\n    text = re.sub(r\"(W|w)on(\\'|\\\u2019)t \", \"will not \", text)\n    text = re.sub(r\"(C|c)an(\\'|\\\u2019)t \", \"can not \", text)\n    text = re.sub(r\"(Y|y)(\\'|\\\u2019)all \", \"you all \", text)\n    text = re.sub(r\"(Y|y)a(\\'|\\\u2019)ll \", \"you all \", text)\n\n    # general\n    text = re.sub(r\"(I|i)(\\'|\\\u2019)m \", \"i am \", text)\n    text = re.sub(r\"(A|a)in(\\'|\\\u2019)t \", \"is not \", text)\n    text = re.sub(r\"n(\\'|\\\u2019)t \", \" not \", text)\n    text = re.sub(r\"(\\'|\\\u2019)re \", \" are \", text)\n    text = re.sub(r\"(\\'|\\\u2019)s \", \" is \", text)\n    text = re.sub(r\"(\\'|\\\u2019)d \", \" would \", text)\n    text = re.sub(r\"(\\'|\\\u2019)ll \", \" will \", text)\n    text = re.sub(r\"(\\'|\\\u2019)t \", \" not \", text)\n    text = re.sub(r\"(\\'|\\\u2019)ve \", \" have \", text)\n    return text","a7f776cb":"import string\nregular_punct = list(string.punctuation)\nextra_punct = [\n    ',', '.', '\"', ':', ')', '(', '!', '?', '|', ';', \"'\", '$', '&','\/', '[', ']', '>',\n    '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3','\u00b7', '_', '{', '}', '\u00a9', '^', '`',\n    '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a','\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u00bd', '\u00e0', '\u2026', '\u201c', '\u2605',\n    '\u201d','\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be','\u2014', '\u2039', '\u2500', '\uff1a', '\u00bc', \n    '\u25bc', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u266b', '\u2606','\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2','\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', \n    '\u2219', '\uff09','\u2193', '\u3001', '\u2502', '\uff08', '\u00bb','\uff0c', '\u266a', '\u00b3', '\u30fb', '\u2764', '\u00ef', '\u00d8', '\u2264', '\u221a', '\u00ab', '\u00bb',\n    '\u00b4', '\u00ba', '\u00be', '\u00a1', '\u00a7', '\u00a3', '\u20a4']\nall_punct = list(set(regular_punct + extra_punct))\n# do not spacing - and .\nall_punct.remove('-')\nall_punct.remove('.')\n\ndef spacing_punctuation(text):\n    \"\"\"\n    add space before and after punctuation and symbols\n    \"\"\"\n    for punc in all_punct:\n        if punc in text:\n            text = text.replace(punc, f' {punc} ')\n    return text","6913f9d4":"train_df.question_text = train_df.question_text.apply(remove_space)\ntrain_df.question_text = train_df.question_text.apply(clean_special_punctuations)\ntrain_df.question_text = train_df.question_text.apply(clean_number)\ntrain_df.question_text = train_df.question_text.apply(decontracted)\ntrain_df.question_text = train_df.question_text.apply(spacing_punctuation)\n\ntrain_df.head(5)","004d1017":"test_df.question_text = test_df.question_text.apply(remove_space)\ntest_df.question_text = test_df.question_text.apply(clean_special_punctuations)\ntest_df.question_text = test_df.question_text.apply(clean_number)\ntest_df.question_text = test_df.question_text.apply(decontracted)\ntest_df.question_text = test_df.question_text.apply(spacing_punctuation)\n\ntest_df.head(5)","cc5bee8d":"## split to train and validation\ntrain_df, val_df = train_test_split(train_df, test_size=0.2, random_state=1000)","155bd04d":"# Filling missing values in the text columns if any\ntrain_X = train_df['question_text'].fillna(\"_na_\").values\nval_X   = val_df['question_text'].fillna(\"_na_\").values\ntest_X  = test_df['question_text'].fillna(\"_na_\").values","1b9d49a2":"max_features = 50000 # how many unique words to use\nmaxlen       = 50  # max number of words in a question to use\nembed_size   = 300 # how big is each word vector\n\n# Tokenizing words in our sentences using keras tokenizer\n\ntokenizer = Tokenizer(num_words = max_features)\ntokenizer.fit_on_texts(list(train_X))","9e514064":"# converting each text in the dataset to a sequence of integers\n\ntrain_X = tokenizer.texts_to_sequences(train_X)\nval_X   = tokenizer.texts_to_sequences(val_X)\ntest_X  = tokenizer.texts_to_sequences(test_X)","21b41be4":"# Padding sequences \n\ntrain_X = pad_sequences(train_X, maxlen = maxlen)\nval_X   = pad_sequences(val_X, maxlen = maxlen)\ntest_X  = pad_sequences(test_X, maxlen = maxlen)","359e6f0f":"#Target values\n\ntrain_y = train_df['target'].values\nval_y = val_df['target'].values","2f1dc3a7":"print(train_X[100])","594d70dd":"import zipfile","ffcbdd00":"with zipfile.ZipFile(\"..\/input\/quora-insincere-questions-classification\/embeddings.zip\",\"r\") as z:\n    z.extractall(\".\")","6107ffb9":"embeddings = '..\/input\/glove840b300dtxt\/glove.840B.300d.txt'\n \ndef get_coefs(word,*arr):\n    return word, np.asarray(arr, dtype = 'float32')\n","45034164":"embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(embeddings))\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n        ","8b04f5aa":"embedding_matrix[3]","42918afb":"inp = Input(shape=(maxlen,))\nx = layers.Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\nx = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\nx = layers.Bidirectional(layers.LSTM(64))(x)\nx = Dense(16, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","4ad76cf1":"from keras.callbacks import ModelCheckpoint","159a8b4e":"checkpointer = ModelCheckpoint(filepath=\"weights.hdf5\", monitor='loss', save_best_only=True, mode='min')\nhistory =model.fit(train_X, train_y, batch_size=512, epochs=5, callbacks=[checkpointer], validation_data=(val_X, val_y))\n\n#history = model.fit(train_X, train_y, batch_size=512, epochs=5, validation_data=(val_X, val_y))","41452d57":"print(history.history['loss'])\nprint(history.history['accuracy'])\nprint(history.history['val_loss'])\nprint(history.history['val_accuracy'])","521d4a94":"import matplotlib.pyplot as plt","60ac107f":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model train vs validation loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper right')\nplt.show()","1687c5d8":"import sklearn.metrics as accuracy_score ","68f43720":"pred_glove_val_y = model.predict([val_X], batch_size=1024, verbose=1)\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_glove_val_y>thresh).astype(int))))\n\npred_glove_test_y = model.predict([test_X], batch_size=1024, verbose=1)\n\ndel word_index, embeddings_index, all_embs, embedding_matrix, model, inp, x\nimport gc; gc.collect()\ntime.sleep(10)","e0a9e380":"pred_test_y = pred_glove_test_y \npred_test_y = (pred_test_y>0.35).astype(int) \nout_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values}) \nout_df['prediction'] = pred_test_y \nout_df.to_csv(\"submission.csv\", index=False)","41804a84":"Pour pr\u00e9dire le prochain caract\u00e8re, nous devons fournir au RNN une **s\u00e9quence des caract\u00e8res** pr\u00e9c\u00e9dents. Afin d\u2019obtenir de nombreux exemples d\u2019apprentissages, nous d\u00e9coupons des s\u00e9quences dans les discours, en coulissant d\u2019un certain pas entre chaque tranche.","fcf9d02f":"# Conclusions\n\nDans cette \u00e9tude, une br\u00e8ve introduction a \u00e9t\u00e9 offerte concernant l\u2019utilisation de l\u2019apprentissage profond inh\u00e9rents \u00e0 l\u2019analyse du langage naturel. Tout d\u2019abord, nous avons montr\u00e9 comment mettre en \u0153uvre l\u2019\u00e9tape fondamentale du pr\u00e9processage des donn\u00e9es. Dans cette section ont \u00e9t\u00e9 discut\u00e9es les principales techniques et m\u00e9thodologies les plus commun\u00e9ment utilis\u00e9es, parmi lesquelles on cite le processus d'\"analyse\" afin de supprimer des parties de texte, de mots et de ponctuation qui ne sont pas utiles \u00e0 l\u2019accomplissement de la t\u00e2che de classement.\nIls suivent la proc\u00e9dure de \"tokenizzazione\" et de \"padding\" pour l\u2019obtention d\u2019un ensemble appropri\u00e9 de donn\u00e9es utilisables par l\u2019algorithme ML. Ensuite, la proc\u00e9dure d'\"embedding\" qui repr\u00e9sente un pas fondamental et non n\u00e9gligeable pour l\u2019am\u00e9lioration et la repr\u00e9sentation de l\u2019ensemble de donn\u00e9es dans un espace n-dimensionnel. \u00c0 ce stade, le \"pourquoi\" de cette proc\u00e9dure est n\u00e9cessaire et comment elle peut effectivement \u00eatre mise en \u0153uvre.\nLes connaissances fournies par Glove sont alors transmises \u00e0 notre mod\u00e8le de r\u00e9seau neural qui, par l\u2019interm\u00e9diaire de la couche d\u2019embedding, associe \u00e0 chaque mot pr\u00e9sent dans le dictionnaire un vecteur de taille @n'. Pour ce faire, cependant, il est n\u00e9cessaire d\u2019imposer la condition de non-remorquage de cette couche (trainable=False).\nUne fois le mod\u00e8le entra\u00een\u00e9, on a proc\u00e9d\u00e9 \u00e0 l\u2019\u00e9valuation du mod\u00e8le et on a fourni le graphique relatif \u00e0 la performance pendant la phase d\u2019entra\u00eenement et le tableau relatif au rapport classification qui nous a permis d\u2019\u00e9valuer la qualit\u00e9 de la m\u00e9thodologie propos\u00e9e.","e338f445":"**Epochs est le nombre maximum d\u2019it\u00e9rations ; \nbatch_size correspond aux nombre d\u2019observations que l\u2019on fait passer avant de remettre \u00e0 jour les poids synaptiques.\nL\u2019\u00e9volution de l\u2019apprentissage est affich\u00e9e dans la console IPython. En ce qui me concerne, voici\nles valeurs finales de loss = 0.051 et accuracy = 0.955**","5f25a376":"# *Importation des donn\u00e9es*","0dd18092":"Score de soumission = 0,626 pas si mauvais !","37c3cab3":"Maintenant, nous allons tokenize nos phrases et cr\u00e9er le vocabulaire. On peut d\u00e9finir un nombre max pour le nombre de mots dans le vocabulaire. ","116dd5fa":"# Entra\u00eener un RNN \u00e0 pr\u00e9dire\nNotre objectif est de pr\u00e9dire, \u00e0 partir d\u2019une s\u00e9quence de 50 caract\u00e8res cons\u00e9cutifs, le caract\u00e8re suivant. Il s\u2019agit d\u2019un probl\u00e8me d\u2019apprentissage supervis\u00e9 : \u00e0 chaque it\u00e9ration, on fournit au mod\u00e8le une s\u00e9quence d\u2019entr\u00e9e encod\u00e9e ainsi que le caract\u00e8re encod\u00e9 attendu en sortie. Le mod\u00e8le effectue une pr\u00e9diction, la compare \u00e0 la cible attendue, et ajuste ses param\u00e8tres (aussi appel\u00e9s poids du r\u00e9seau) en cas d\u2019erreur. \nNous avons pr\u00e9par\u00e9 nos donn\u00e9es et cr\u00e9\u00e9 notre mod\u00e8le, il ne reste  maintenant plus qu\u2019\u00e0 lancer l\u2019apprentissage. Une ligne suffit pour d\u00e9marrer cette \u00e9tape.","4e70475e":"# Chargement de l'Embeddings \n\nGloVe (\u00ab\u00a0Global Vectors for Word Representation\u00a0\u00bb) comme son nom l\u2019indique est meilleur pour pr\u00e9server les contextes globaux car il cr\u00e9e une matrice globale de co-occurrence en estimant la probabilit\u00e9 qu\u2019un mot donn\u00e9 se produise avec d\u2019autres mots.\n\nD\u2019abord il faut lire dans le fichier d'embedding dans un dictionnaire - chaque entr\u00e9e est un mot, suivi du vecteur de nombres pour repr\u00e9senter ses valeurs.","164851ee":"# Pr\u00e9-traitement... une \u00e9tape vers la vraie analyse\n\nNous sommes tous bien conscients que, avant que nous puissions utiliser des algorithmes de machine learning (ML) nous avons besoin de rendre notre dataset faisable pour l\u2019analyse que on veut d\u00e9velopper. Cette phase de pr\u00e9-traitement des donn\u00e9es est particuli\u00e8rement importante lorsque on travaille avec des donn\u00e9es en format texte. En effet, la plupart des mots qui constituent une phrase ne sont pas utiles au groupe de travail. L\u2019objectif, \u00e0 ce stade, est de rendre la vie de notre classeur aussi simple que possible afin d\u2019en maximiser les performances. Voici quelques fonctions simples pour le nettoyage, non exhaustif, du texte :","86c7758e":"Performance du mod\u00e8le : fonction de perte li\u00e9e \u00e0 la phase d\u2019entra\u00eenement et de test pour chaque epoch.\n\nSur le graphique, vous pouvez voir comment le mod\u00e8le atteint un bon niveau de performance pendant la premi\u00e8re Epoch.","f811db25":"# **INTRODUCTION**\n\n\n*D\u00e9finition du probl\u00e8me*\n\nDans le D\u00e9fi de classification des questions de Quora Insincere, on a des questions et on nous demande de classer les questions comme sinc\u00e8res (0) ou non sinc\u00e8res (1). Le manque de sinc\u00e9rit\u00e9 dans ce cas comprend \u00e9galement des commentaires toxiques ou trompeurs, comme on peut le voir dans les \u00e9chantillons ci-dessous. \nDans les donn\u00e9es de formation, on a l\u2019identificateur de question (qid), la question (question_text) et la cat\u00e9gorie (target). Dans l\u2019ensemble de tests, on a seulement l\u2019identificateur et la question, et on nous demande de les classer dans les deux cat\u00e9gories.","f7c4d916":"**ModelCheckpoint** \nA chaque \u00e9poque, le pointeur de contr\u00f4le voit si les param\u00e8tres du mod\u00e8le se sont am\u00e9lior\u00e9s - s\u2019ils l\u2019ont fait, il les enregistre dans un fichier appel\u00e9 weights.hdf5.","d5751bd7":"thresh:\tIl s\u2019agit d\u2019un entier qui sp\u00e9cifie le moins de valeurs non manquantes qui emp\u00eachent les lignes ou les colonnes de tomber.\n\nLa valeur de thresh est 2, ce qui signifie que nous \u00e9vitons toute chute, au moins 2 valeurs non vides sont requises.","30134601":"**Expression r\u00e9guli\u00e8re**, ou regex est extr\u00eamement puissant dans la recherche et la manipulation de cha\u00eenes de texte, en particulier dans le traitement des fichiers texte. Une ligne de regex peut facilement remplacer plusieurs dizaines de lignes de codes de programmation.","314e8ecf":"Le model BLSTM \n\nIl est possible d'utiliser un r\u00e9seau appel\u00e9 BLSTM, qui consiste \u00e0 d\u00e9doubler une couche LSTM, l'une \u00e9tant apprise pour parcourir le signal de gauche \u00e0 droite, et l'autre de droite \u00e0 gauche :\nLes deux couches sont combin\u00e9es afin de prendre les meilleures d\u00e9cisions locales en ayant \"vu\"... l'int\u00e9gralit\u00e9 du signal !\n\nADAM est utile pour de l'optimisation stochastique. Elle utilise les EWA - Exponentially Weigthed Averages pour faire une estimation liss\u00e9e des gradients \u00e0 l'aide de moments\n\nLe dropout est une technique qui est destin\u00e9e \u00e0 emp\u00eacher le sur-ajustement sur les donn\u00e9es de training en abandonnant des unit\u00e9s dans un r\u00e9seau de neurones. En pratique, les neurones sont soit abandonn\u00e9s avec une probabilit\u00e9 pp ou gard\u00e9s avec une probabilit\u00e9 1-p1\u2212p.\n\nUn tel mod\u00e8le s\u2019impl\u00e9mente en quelques lignes avec Keras. On d\u00e9finit un mod\u00e8le Sequential, sur lequel on ajoute une successions de couches qui correspondent directement aux \u00e9tapes du sch\u00e9ma pr\u00e9c\u00e9dent. Les couches interm\u00e9diaires de dropout sont une technique de r\u00e9gularisation, pour \u00e9viter au mod\u00e8le de trop coller aux donn\u00e9es vues en entra\u00eenement et am\u00e9liorer sa g\u00e9n\u00e9ralisation. Dans ce cas avec une probabilit\u00e9 d'etre abbandonn\u00e9 = 0.1"}}