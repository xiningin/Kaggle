{"cell_type":{"0c835f6d":"code","9eecf72c":"code","15d609a4":"code","41478b15":"code","a4145250":"code","4ff390f4":"code","072ad0bd":"code","0e3f567b":"code","f5cd668d":"code","7e7f4c47":"code","6017f16d":"code","936a6fdf":"code","c64598bd":"code","89f0204e":"code","0f3b7142":"code","a00241dc":"code","e804949f":"code","1365f1ff":"code","3c6b5b21":"code","3b10699c":"markdown","a13da9b3":"markdown","ad89887d":"markdown","e05cfbf8":"markdown"},"source":{"0c835f6d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9eecf72c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer, SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, RepeatedKFold, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import (\n    confusion_matrix, accuracy_score, precision_score,\n    recall_score, f1_score, precision_recall_curve,\n    roc_auc_score\n)","15d609a4":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","41478b15":"train.head()","a4145250":"test.head()","4ff390f4":"print(train.shape)\nprint(test.shape)","072ad0bd":"X = train.drop(['Survived'], axis=1)\nY = train.Survived","0e3f567b":"# Total missing values in each columns\ntrain.isnull().sum(axis=0)","f5cd668d":"# percentage of missing values in each columns\ntest.isnull().sum(axis=0)","7e7f4c47":"pipeline1 = ColumnTransformer([\n    ('drop', 'drop', ['PassengerId', 'Name', 'Cabin', 'Ticket']),\n    ('ageimputer', IterativeImputer(max_iter=10, random_state=27), ['Age', 'Fare']),\n    ('embarkedimputer', SimpleImputer(strategy='most_frequent'), ['Embarked'])],\n    remainder='passthrough'\n)\n\npipeline2 = ColumnTransformer([\n    ('scaler', MinMaxScaler(), [0, 1, 5, 6]),\n    ('onehot', OneHotEncoder(), [2, 3, 4])\n])\n\npipeline = make_pipeline(pipeline1, pipeline2)","6017f16d":"pipeline.fit(X)\nX_train = pipeline.transform(X)\nX_test = pipeline.transform(test)","936a6fdf":"pd.DataFrame(X_train).sample(5)","c64598bd":"pd.DataFrame(X_test).sample(5)","89f0204e":"lr = LogisticRegression(penalty='l1', verbose=2)\nrr = SGDClassifier(penalty='l2', n_jobs=4, loss='log')\nls = SGDClassifier(penalty='l1', n_jobs=4, loss='log')\nen = SGDClassifier(penalty='elasticnet', n_jobs=4, loss='log')","0f3b7142":"metrics.SCORERS.keys()","a00241dc":"cv = RepeatedKFold(n_splits=5, n_repeats=1, random_state=27)\n\ngrid_ridge_lasso = {\n    'alpha': np.arange(0, 1, 0.05),\n    'learning_rate': np.array(['constant']),\n    'eta0': np.array([0.1, 0.01, 0.001])\n}\n\ngrid_elastic = {\n    'alpha': np.arange(0, 1, 0.05),\n    'l1_ratio': np.arange(0, 1, 0.05),\n    'learning_rate': np.array(['constant']),\n    'eta0': np.array([0.1, 0.01, 0.001])\n}\n\nlr_score = cross_val_score(lr, X_train, Y, cv=cv, scoring='accuracy')\n\nrr_search = GridSearchCV(rr, grid_ridge_lasso, cv=cv, scoring='accuracy')\nrr_score = rr_search.fit(X_train, Y)\n\nls_search = GridSearchCV(ls, grid_ridge_lasso, cv=cv, scoring='accuracy')\nls_score = ls_search.fit(X_train, Y)\n\nen_search = GridSearchCV(en, grid_elastic, cv=cv, scoring='accuracy')\nen_score = en_search.fit(X_train, Y)","e804949f":"print(np.mean(lr_score))\nprint(rr_score.best_score_)\nprint(ls_score.best_score_)\nprint(en_score.best_score_)","1365f1ff":"predictions = en_score.best_estimator_.predict(X_test)","3c6b5b21":"pd.DataFrame({\n    'PassengerId': test.PassengerId,\n    'Survived': predictions\n}).to_csv('submission.csv', index=False)","3b10699c":"# Model Fitting","a13da9b3":"# Load Data","ad89887d":"# Import Libraries","e05cfbf8":"# Data preprocessing using pipeline"}}