{"cell_type":{"efe7cd7d":"code","4dea1d74":"code","079a406d":"code","2d4bc183":"code","8994ef97":"code","de3b1751":"code","96c0fce7":"code","f4630f1a":"code","5656e40d":"code","1bbd7777":"code","c620813c":"markdown","58e70aa6":"markdown","ef3661e7":"markdown","1139d11a":"markdown","0bd29350":"markdown","70b204bb":"markdown","9101e711":"markdown","f830c46b":"markdown"},"source":{"efe7cd7d":"!pip install pytorch-tabnet\nimport pandas as pd\nimport numpy  as np\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nfrom sklearn.model_selection import KFold","4dea1d74":"#===========================================================================\n# read in the data\n#===========================================================================\ntrain_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_data  = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsample     = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nsolution   = pd.read_csv('..\/input\/house-prices-advanced-regression-solution-file\/solution.csv')","079a406d":"#===========================================================================\n# select some features\n#===========================================================================\nfeatures = ['LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', \n            'YearRemodAdd', 'BsmtFinSF1', 'BsmtFinSF2', 'TotalBsmtSF', \n            '1stFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', \n            'BsmtHalfBath', 'HalfBath', 'BedroomAbvGr',  'Fireplaces', \n            'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', \n            'EnclosedPorch',  'PoolArea', 'YrSold']","2d4bc183":"X      = train_data[features]\ny      = np.log1p(train_data[\"SalePrice\"])\nX_test = test_data[features]\ny_true = solution[\"SalePrice\"]","8994ef97":"X      =      X.apply(lambda x: x.fillna(x.mean()),axis=0)\nX_test = X_test.apply(lambda x: x.fillna(x.mean()),axis=0)","de3b1751":"X      = X.to_numpy()\ny      = y.to_numpy().reshape(-1, 1)\nX_test = X_test.to_numpy()","96c0fce7":"kf = KFold(n_splits=5, random_state=42, shuffle=True)\npredictions_array =[]\nCV_score_array    =[]\nfor train_index, test_index in kf.split(X):\n    X_train, X_valid = X[train_index], X[test_index]\n    y_train, y_valid = y[train_index], y[test_index]\n    regressor = TabNetRegressor(verbose=0,seed=42)\n    regressor.fit(X_train=X_train, y_train=y_train,\n              eval_set=[(X_valid, y_valid)],\n              patience=300, max_epochs=2000,\n              eval_metric=['rmse'])\n    CV_score_array.append(regressor.best_cost)\n    predictions_array.append(np.expm1(regressor.predict(X_test)))\n\npredictions = np.mean(predictions_array,axis=0)","f4630f1a":"print(\"The CV score is %.5f\" % np.mean(CV_score_array,axis=0) )","5656e40d":"from sklearn.metrics import mean_squared_log_error\nRMSLE = np.sqrt( mean_squared_log_error(y_true, predictions) )\nprint(\"The LB score is %.5f\" % RMSLE )","1bbd7777":"sample.iloc[:,1:] = predictions\nsample.to_csv('submission.csv',index=False)","c620813c":"Convert the data to [numpy.array](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.array.html)","58e70aa6":"# Related reading\n* [Sercan O. Arik and Tomas Pfister \"TabNet: Attentive Interpretable Tabular Learning\", arXiv:1908.07442 (2019)](https:\/\/arxiv.org\/pdf\/1908.07442.pdf)\n* [pytorch-tabnet](https:\/\/github.com\/dreamquark-ai\/tabnet) (GitHub)\n* [\"TabNet on AI Platform: High-performance, Explainable Tabular Learning\"](https:\/\/cloud.google.com\/blog\/products\/ai-machine-learning\/ml-model-tabnet-is-easy-to-use-on-cloud-ai-platform) (Google Cloud)\n* Notebook: [TabNet: A simple binary classification example](https:\/\/www.kaggle.com\/carlmcbrideellis\/tabnet-simple-binary-classification-example) (using the Santander Customer Satisfaction data on kaggle)","ef3661e7":"# TabNet: A very simple regression example using the House Prices data\n[**TabNet**](https:\/\/arxiv.org\/pdf\/1908.07442.pdf) brings deep learning to tabular data. TabNet has been developed by researchers at Google Cloud AI and achieves SOTA performance on a number of test cases.\nThis notebook is a simple example of performing a regression using the [pyTorch implementation](https:\/\/pypi.org\/project\/pytorch-tabnet\/). \n\n`TabNetRegressor()` has a number of options for the `device_name`: `cpu`, `cuda`, `mkldnn`, `opengl`, `opencl`, `ideep`, `hip`, `msnpu`, and `xla`.\nThe `fit()` has a variety of `eval_metric`: `auc`, `accuracy`, `balanced_accuracy`, `logloss`, `mae`, `mse`, and `rmse`. TabNet can also perform classification using `TabNetClassifier()` as well as perform [multi-task learning](https:\/\/en.wikipedia.org\/wiki\/Multi-task_learning).\n\nWe shall use the [House Prices: Advanced Regression Techniques](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques) data for this demonstration. In this example I undertake no feature engineering, nor data cleaning, such as the removal of outliers *etc*., and perform  only the most basic imputation simply to account for any missing values.\n\n#### Install TabNet:","1139d11a":"now calculate our leaderboard score (See: [\"House Prices: How to work offline\"](https:\/\/www.kaggle.com\/carlmcbrideellis\/house-prices-how-to-work-offline)).","0bd29350":"We shall impute any missing data with a simple mean value. As to the relative merits of doing this *before* using cross-validation see [Byron C. Jaeger, Nicholas J. Tierney, and Noah R. Simon \"*When to Impute? Imputation before and during cross-validation*\" arXiv:2010.00718](https:\/\/arxiv.org\/pdf\/2010.00718.pdf).\nFor a much better imputation method take a look at the notebook [\"MissForest - The best imputation algorithm\"](https:\/\/www.kaggle.com\/lmorgan95\/missforest-the-best-imputation-algorithm) by [Liam Morgan](https:\/\/www.kaggle.com\/lmorgan95). It deals with the R implementation, and MissForest can also be used in python via the [missingpy](https:\/\/github.com\/epsilon-machine\/missingpy) package.","70b204bb":"calculate our average CV score","9101e711":"We can see that our CV score corresponds nicely with our leaderboard score, so we do not seem to be [overfitting or underfitting](https:\/\/www.kaggle.com\/carlmcbrideellis\/overfitting-and-underfitting-the-titanic) by too much.\n\nFinally write out a `submission.csv` file:","f830c46b":"run the TabNet deep neural network, averaging over 5 folds:"}}