{"cell_type":{"a87a5c06":"code","5ec8998f":"code","0685492a":"code","6f467034":"code","b977a6f2":"code","89d4e550":"code","83427103":"code","062a52e4":"code","df70e92c":"code","a991f2e5":"code","67937884":"code","827ed970":"code","14dfee8a":"code","36831d48":"code","932dd539":"code","8a7fa397":"code","ff04acdb":"code","3d8b73ae":"code","33a050b2":"code","cd9f37d5":"code","3469c820":"code","77edfbf7":"code","0c65faef":"code","bc957f4b":"code","cca317c3":"code","9366f273":"code","95cd31ee":"code","6579a134":"code","a0f6883c":"code","e7003a58":"code","31d20a5e":"code","66e68f1c":"code","920417da":"code","e4294619":"code","6ce5e994":"code","66d863ff":"code","ed1440cd":"code","118dd49b":"code","c76f097c":"code","626a679c":"code","492e07ad":"code","4d321155":"code","a09bdc50":"code","49b74a8a":"code","54e7f3f8":"code","765fa554":"code","4f35574f":"code","1ce2374c":"code","477e71eb":"code","c5879064":"code","a4502213":"code","5d6c57b3":"code","6de973d4":"code","7d258e3d":"code","5049d856":"code","5a602054":"code","80ee84cc":"markdown","f071d097":"markdown","d41f8215":"markdown","65fac7e6":"markdown","08e9f63b":"markdown","e1642b63":"markdown","185d4d5a":"markdown","d227cf4e":"markdown","20e9ade8":"markdown","784b9f75":"markdown","8ce3853d":"markdown","8b7b0fb8":"markdown","c9f01aaa":"markdown","e1771e44":"markdown","5cf1b689":"markdown","114d0fd4":"markdown","988fcdee":"markdown","9bfb475e":"markdown","a9f27f51":"markdown","295d20d4":"markdown","dcd1ca2c":"markdown","dae70a28":"markdown","23d43d9e":"markdown","1d66f296":"markdown","f7b28b0e":"markdown","fbf70987":"markdown","00010b47":"markdown","5af2191c":"markdown","00cba2fb":"markdown","5f2b9e12":"markdown","18a06bef":"markdown","055c44ac":"markdown","d1cacd67":"markdown","d3557b44":"markdown","75fcb9ab":"markdown","a22efe28":"markdown","faa686f0":"markdown","cb355a63":"markdown","b6f52075":"markdown","de8686f1":"markdown","7b4bcf83":"markdown","106cfb39":"markdown","da83b22d":"markdown","6061c197":"markdown","320ee2f4":"markdown","06cfb2ab":"markdown","33abc805":"markdown","aa84a3a9":"markdown","c1b33a76":"markdown","c2ac9741":"markdown","73982c0e":"markdown","13cd82ef":"markdown","26a7d1e8":"markdown","4fca6616":"markdown","1b0ac84e":"markdown","d156e441":"markdown","fdc3c1ef":"markdown"},"source":{"a87a5c06":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","5ec8998f":"df = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')","0685492a":"df.head()","6f467034":"df.info()","b977a6f2":"df.isnull().sum()","89d4e550":"plt.figure(figsize=(15,3))\n\nsns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')","83427103":"df.columns","062a52e4":"len(df.columns)","df70e92c":"plt.figure(figsize=(6,6))\ndf['DEATH_EVENT'].value_counts().plot(kind='pie', autopct='%1.1f', shadow=True)","a991f2e5":"df['DEATH_EVENT'].value_counts()","67937884":"plt.figure(figsize=(10,10))\nsns.heatmap(df.corr(),cmap='coolwarm', annot=True)","827ed970":"sns.set_style('whitegrid')\n\ng = sns.FacetGrid(df, hue=\"sex\", height=6, aspect=2, palette='dark')\ng = g.map(plt.hist, \"age\", bins=30, alpha=0.5)\n\ng.add_legend()","14dfee8a":"g = sns.FacetGrid(df, hue=\"DEATH_EVENT\", height=6, aspect=2, palette='dark')\ng = g.map(plt.hist, \"age\", bins=30, alpha=0.5)\n\ng.add_legend()","36831d48":"sns.boxplot(x=\"DEATH_EVENT\", y=\"age\", data=df)","932dd539":"plt.figure(figsize=(6,6))\ndf['anaemia'].value_counts().plot(kind='pie', autopct='%1.1f', shadow=True)","8a7fa397":"g = sns.FacetGrid(df, hue=\"DEATH_EVENT\", height=6, aspect=2, palette='dark')\ng = g.map(plt.hist, \"creatinine_phosphokinase\", bins=50, alpha=0.5)\n\ng.add_legend()","ff04acdb":"plt.figure(figsize=(6,6))\ndf['diabetes'].value_counts().plot(kind='pie', autopct='%1.1f', shadow=True)","3d8b73ae":"g = sns.FacetGrid(df, hue=\"DEATH_EVENT\", height=6, aspect=2, palette='dark')\ng = g.map(plt.hist, \"ejection_fraction\", bins=10, alpha=0.5)\n\ng.add_legend()","33a050b2":"sns.boxplot(x=\"DEATH_EVENT\", y=\"ejection_fraction\", data=df)","cd9f37d5":"plt.figure(figsize=(6,6))\ndf['high_blood_pressure'].value_counts().plot(kind='pie', autopct='%1.1f', shadow=True)","3469c820":"g = sns.FacetGrid(df, hue=\"DEATH_EVENT\", height=6, aspect=2, palette='dark')\ng = g.map(plt.hist, \"platelets\", bins=30, alpha=0.5)\n\ng.add_legend()","77edfbf7":"g = sns.FacetGrid(df, hue=\"DEATH_EVENT\", height=6, aspect=2, palette='dark')\ng = g.map(plt.hist, \"serum_creatinine\", bins=30, alpha=0.5)\n\ng.add_legend()","0c65faef":"sns.boxplot(x=\"DEATH_EVENT\", y=\"serum_creatinine\", data=df)","bc957f4b":"g = sns.FacetGrid(df, hue=\"DEATH_EVENT\", height=6, aspect=2, palette='dark')\ng = g.map(plt.hist, \"serum_sodium\", bins=30, alpha=0.5)\n\ng.add_legend()","cca317c3":"sns.boxplot(x=\"DEATH_EVENT\", y=\"serum_sodium\", data=df)","9366f273":"plt.figure(figsize=(6,6))\ndf['sex'].value_counts().plot(kind='pie', autopct='%1.1f', shadow=True)","95cd31ee":"plt.figure(figsize=(6,6))\ndf['smoking'].value_counts().plot(kind='pie', autopct='%1.1f', shadow=True)","6579a134":"sns.regplot(x='serum_sodium',y='ejection_fraction', data=df)","a0f6883c":"X=df.drop(['DEATH_EVENT'], axis=1)\ny=df['DEATH_EVENT']","e7003a58":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","31d20a5e":"def print_validation_report(y_true, y_pred):\n    print(\"Classification Report\")\n    print(classification_report(y_true, y_pred))\n    acc_sc = accuracy_score(y_true, y_pred)\n    print(\"Accuracy : \"+ str(acc_sc))","66e68f1c":"def plot_confusion_matrix(y_true, y_pred):\n    mtx = confusion_matrix(y_true, y_pred)\n    sns.heatmap(mtx, annot=True, fmt='d', linewidths=.5,  \n                cmap=\"Blues\", cbar=False)\n    plt.ylabel('true label')\n    plt.xlabel('predicted label')","920417da":"from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report","e4294619":"from sklearn.linear_model import LogisticRegression\n\nlr=LogisticRegression(max_iter=10000)\nlr.fit(X_train,y_train)\np1=lr.predict(X_test)\ns1=accuracy_score(y_test,p1)\nprint(\"Linear Regression Success Rate :\", s1*100,'%')","6ce5e994":"print_validation_report(y_test,p1)","66d863ff":"plot_confusion_matrix(y_test,p1)","ed1440cd":"importance = abs(lr.coef_[0])\ncoeffecients = pd.DataFrame(importance, X_train.columns)\ncoeffecients.columns = ['Coeffecient']\nplt.figure(figsize=(15,4))\nplt.bar(X_train.columns,importance)\nplt.show()","118dd49b":"from sklearn.ensemble import RandomForestClassifier\n\nrfc=RandomForestClassifier()\nrfc.fit(X_train,y_train)\np2=rfc.predict(X_test)\ns2=accuracy_score(y_test,p2)\nprint(\"Random Forrest Accuracy :\", s2*100,'%')","c76f097c":"plot_confusion_matrix(y_test,p2)","626a679c":"from sklearn.svm import SVC\nsvm=SVC()\nsvm.fit(X_train,y_train)\np3=svm.predict(X_test)\ns3=accuracy_score(y_test,p3)\nprint(\"SVM Accuracy :\", s3*100,'%')","492e07ad":"plot_confusion_matrix(y_test,p3)","4d321155":"from sklearn.neighbors import KNeighborsClassifier\n\nknn=KNeighborsClassifier(n_neighbors=7)\nknn.fit(X_train,y_train)\np4=knn.predict(X_test)\ns4=accuracy_score(y_test,p4)\nprint(\"KNN Accuracy :\", s4*100,'%')","a09bdc50":"error_rate = []\nscores = []\n\nfor i in range(1,40): # check all values of K between 1 and 40\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    score=accuracy_score(y_test,pred_i)\n    scores.append(score)\n    error_rate.append(np.mean(pred_i != y_test)) # ERROR RATE DEF and add it to the list","49b74a8a":"plt.style.use('seaborn-whitegrid')\nplt.figure(figsize=(10,6))\nplt.plot(range(1,40),scores,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Accuracy Score vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Accuracy Score')","54e7f3f8":"plt.style.use('seaborn-whitegrid')\nplt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","765fa554":"from sklearn.neighbors import KNeighborsClassifier\n\nknn=KNeighborsClassifier(n_neighbors=35)\nknn.fit(X_train,y_train)\np4=knn.predict(X_test)\ns4=accuracy_score(y_test,p4)\nprint(\"KNN Accuracy:\", s4*100,'%')","4f35574f":"plot_confusion_matrix(y_test,p4)","1ce2374c":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(X_train, y_train)\np5 =nb.predict(X_test)\ns5=accuracy_score(y_test,p5)\nprint(\"Naive-Bayes Accuracy:\", s5*100,'%')","477e71eb":"plot_confusion_matrix(y_test,p5)","c5879064":"f1_score(y_test,p5)","a4502213":"models = pd.DataFrame({\n    'Model': [\"LOGISTIC REGRESSION\",\"RANDOM FOREST\",\"SUPPORT VECTOR MACHINE\",\"KNN\",\"NAIVE-BAYES\"],\n    'Accuracy Score': [s1*100,s2*100,s3*100,s4*100,s5*100]})\nmodels.sort_values(by='Accuracy Score', ascending=False)","5d6c57b3":"print(f1_score(y_test,p1))\nprint(f1_score(y_test,p2))\nprint(f1_score(y_test,p3))\nprint(f1_score(y_test,p4))\nprint(f1_score(y_test,p5))","6de973d4":"from sklearn.metrics import roc_curve,roc_auc_score, auc","7d258e3d":"fpr1,tpr1, thr1=roc_curve(y_test,p1)\nfpr2,tpr2, thr2=roc_curve(y_test,p2)\nfpr3,tpr3, thr3=roc_curve(y_test,p3)\nfpr4,tpr4, thr4=roc_curve(y_test,p4)\nfpr5,tpr5, thr5=roc_curve(y_test,p5)","5049d856":"plt.figure(figsize=(10,6))\nplt.plot(fpr1,tpr1, linestyle='--', label='LR')\nplt.plot(fpr2,tpr2, linestyle='--', label='RF')\nplt.plot(fpr3,tpr3, linestyle='--', label='SVM')\nplt.plot(fpr4,tpr4, linestyle='--', label='KNN')\nplt.plot(fpr5,tpr5, linestyle='--', label='NB')\nplt.legend()","5a602054":"print(roc_auc_score(y_test,p1))\nprint(roc_auc_score(y_test,p2))\nprint(roc_auc_score(y_test,p3))\nprint(roc_auc_score(y_test,p4))\nprint(roc_auc_score(y_test,p5))","80ee84cc":"**serum_creatinine distribution hued by DEATH_EVENT**:","f071d097":"**FEATURE IMPORTANCE IN LOG REGRESSION:**","d41f8215":"We can start to see that the **age, ejection_fraction, serum_creatinine, serum_sodium and time** columns are quite well correlated to the DEATH_EVENT label. These seem to be the **most important features** in the df. We could only keep these when we build the model.","65fac7e6":"## 3.5 Gaussian Naive-Bayes","08e9f63b":"Looks to be higher for people that died of heart disease.","e1642b63":"## 3.3 SVM","185d4d5a":"**NO missing data** and data quality is good!","d227cf4e":"As expected, the columns we predicted have the highest feature importance! The rest of the columns could be discarded and the models re-build using only the highlighted columns.","20e9ade8":"Looks like we have **299** records (rows) in the dataset.","784b9f75":"We can see that the **NB classifier has the largest AUC score.**","8ce3853d":"Number of death occurances:","8b7b0fb8":"Looks to be lower for people that died.","c9f01aaa":"# 3. Models and Performance: ","e1771e44":"Basic analysis of the dataframe:","5cf1b689":"## Summarize results:","114d0fd4":"Check for missing data (however we can observe we shouldn't have any missing data from above):","988fcdee":"Looks like older people tend to be correlated to DEATH_EVENT (makes sense).","9bfb475e":"**serum_sodium distribution hued by DEATH_EVENT**:","a9f27f51":"# 1. Basic dataset analysis","295d20d4":"**Final model choice: Gaussian Naive-Bayes** presents the largest accuracy,f1 and auc scores, as well as only 4 miss-labelings!","dcd1ca2c":"## 3.4 KNN","dae70a28":"We can see the largest area under curve is the **Naive-Bayes**, as prediced. This means a good balance between Type1 and Type2 errors.","23d43d9e":"Explore the correlations in this dataset, as all columns are numerical:","1d66f296":"Looks like **Naive-Bayes wins in accuracy**.\n\nWe can further compare each model's F1 scores for balance between precision and recall.","f7b28b0e":"# 2. Exploratory Data Analysis (EDA) + Visualizations","fbf70987":"Let's optimize for K:","00010b47":"## 3.2 Random Forest","5af2191c":"Age distribution of survived\/not survived:","00cba2fb":"**creatinine_phosphokinase distribution hued by DEATH_EVENT**:","5f2b9e12":"We can see that the mean age is higher for death_event. Note some outliers for the age of DEATH_EVENT=0, probably very old people that did not die from heart disease.","18a06bef":"**AGE**:","055c44ac":"**serum_sodium vs ejection_fraction:** (these look to be correlated from the corr heatmap above)","d1cacd67":"Number of columns:","d3557b44":"**NB** wins in F1 score as well, KNN and SVM are to be completely disregarded.","75fcb9ab":"Age distribution of the 2 sexes:","a22efe28":"Python Imports:","faa686f0":"Similar normal distribution for M\/F.","cb355a63":"# 4. Conclusion and model choice","b6f52075":"**Summary:** We have started with an initial data analysis, seeing if there was any missing data and ensuring the integrity and quality of data. We also obesrved each column (their data types) and for the categorical columns, we could have used LabelEncoding or Dummy variables (not the case in this dataset). Then, some visualizations based on each feature were created and a correlation heatmap was used to determine some preliminary important features, that were confirmed in the model-building section afterwards (these features could have been used for the models instead of choosing all of them like I have). The data was split into train-test and 5 Classifier Models (Logistic Regression, Random Forest, KNN, SVC and Gaussian Naive-Bayes) were build upon the train data and tested upon the test data. The comparison metrics were accuracy score, f1 score, as well as the ROC-AUC curve scores, and the best model was clearly the Gaussian Naive-Bayes, with around 93% accuracy and the least number of Type 1 + Type 2 errors. Feature importance was extracted from the Logistic Regression, however it is best to use regularization algorithms like Lasso\/Ridge\/ElasticNet to extract feature importance. To note that in medical diagnosis, it is desired to minimise False Negatives! Moreover, the KNN Classifier was optimized for the best K value.","de8686f1":"**platelets distribution hued by DEATH_EVENT**:","7b4bcf83":"We have **13 columns**, and all of them are **numerical (quantitative)**, no column is categorical (if there were any we could have just used LabelEncoder or OneHotEncoding with DummyVariables)!","106cfb39":"Just a slight correlation here, the line is not too steep.","da83b22d":"**Predefine performance metric functions for Classification Problems**","6061c197":"K=35 seems a good value that minimises errors and maximises accuracy.","320ee2f4":"**DIABETES:**\n\nWhat percentage of people with diabetes died?","06cfb2ab":"**SMOKING:**","33abc805":"We can also compare the ROC curves and AUC scores for each model.","aa84a3a9":"**ANAEMIA:**\n\nWhat percentage of people with anaemia died?","c1b33a76":"## 3.1 Logistic Regression","c2ac9741":"Choose what features are included in the model (for now we will include all, but we could have chosen the ones given by corr heatmap):","73982c0e":"Data Import:","13cd82ef":"**HIGH BLOOD PRESSURE:**\n\nWhat percentage of people with high-blood pressure died?","26a7d1e8":"We can see that lower ejection fraction increases chances of DEATH_EVENT.","4fca6616":"**GENDER:**","1b0ac84e":"**ejection_fraction distribution hued by DEATH_EVENT**:","d156e441":"We have **96 death events** and 203 not death events. **32.1% of patients died!**","fdc3c1ef":"Very good F1 score, meaning good precision-recall balance!!"}}