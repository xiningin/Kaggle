{"cell_type":{"27ce6a22":"code","1b65ce1a":"code","86a2f768":"code","3b7686ae":"code","12969ee8":"code","fe60d0b8":"code","6de3201f":"code","f9e0cbee":"code","52441f6a":"code","e1190752":"code","170351aa":"code","1874a9a8":"code","aa506f32":"code","b219f8fd":"code","14870a5f":"code","b92fbe51":"code","d918b079":"code","53a1eb1c":"code","73123239":"code","1b9c54a1":"code","d054705e":"code","51235979":"code","b4102e9a":"code","e99b09f1":"code","4ed5a602":"code","c77cef03":"code","e6504d74":"code","fc5d40ea":"code","9c58a315":"markdown","8ce6a1f7":"markdown","cbf8f422":"markdown","ef442695":"markdown","0365e1e3":"markdown","7acc67a1":"markdown","53ef97b2":"markdown","c452b207":"markdown","4ec8500b":"markdown"},"source":{"27ce6a22":"!pip install tensorflow\nimport os","1b65ce1a":"#base_dir = '..\/input\/mechanical-tools-dataset\/Mechanical Tools Image dataset'\n\ntrain_dir = os.path.join('..\/input\/mechanical-tools-dataset\/train_data_V2\/train_data_V2')\nvalidation_dir = os.path.join('..\/input\/mechanical-tools-dataset\/validation_data\/validation_data')\n\n# Directory with our training screwdriver\/wrench pictures\ntrain_screwdriver_dir = os.path.join('..\/input\/mechanical-tools-dataset\/train_data_V2\/train_data_V2\/screwdriver')\ntrain_wrench_dir = os.path.join('..\/input\/mechanical-tools-dataset\/train_data_V2\/train_data_V2\/wrench')\n\n# Directory with our validation screwdriver\/wrench pictures\nvalidation_screwdriver_dir = os.path.join('..\/input\/mechanical-tools-dataset\/validation_data\/validation_data\/screwdriver')\nvalidation_wrench_dir = os.path.join('..\/input\/mechanical-tools-dataset\/validation_data\/validation_data\/wrench')","86a2f768":"train_screwdriver_fnames = os.listdir(train_screwdriver_dir )\ntrain_wrench_fnames = os.listdir( train_wrench_dir )\n\nprint(train_screwdriver_fnames[:20])\nprint(train_wrench_fnames[:20])","3b7686ae":"print('total training screwdriver images :', len(os.listdir(train_screwdriver_dir)))\nprint('total training wrench images :', len(os.listdir(train_wrench_dir)))\n\nprint('total validation screwdriver images :', len(os.listdir( validation_screwdriver_dir ) ))\nprint('total validation wrench images :', len(os.listdir( validation_wrench_dir) ))","12969ee8":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n# Parameters for our graph; we'll output images in a 10x10 configuration\nnrows = 4\nncols = 4\n\n# Index for iterating over images\npic_index = 0","fe60d0b8":"# Set up matplotlib fig, and size it to fit 4x4 pics\nfig = plt.gcf()\nfig.set_size_inches(ncols * 4, nrows * 4)\n\npic_index += 8\nnext_screwdriver_pix = [os.path.join(train_screwdriver_dir, fname) \n                for fname in train_screwdriver_fnames[pic_index-8:pic_index]]\nnext_wrench_pix = [os.path.join(train_wrench_dir, fname) \n                for fname in train_wrench_fnames[pic_index-8:pic_index]]\n\nfor i, img_path in enumerate(next_screwdriver_pix+next_wrench_pix):\n  # Set up subplot; subplot indices start at 1\n  sp = plt.subplot(nrows, ncols, i + 1)\n  sp.axis('Off') # Don't show axes (or gridlines)\n\n  img = mpimg.imread(img_path)\n  plt.imshow(img)\n\nplt.show()","6de3201f":"import tensorflow as tf","f9e0cbee":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# All images will be rescaled by 1.\/255.\ntrain_datagen = ImageDataGenerator( rescale = 1.0\/255. )\ntest_datagen  = ImageDataGenerator( rescale = 1.0\/255. )\n\n# --------------------\n# Flow training images in batches of 20 using train_datagen generator\n# --------------------\ntrain_generator = train_datagen.flow_from_directory(train_dir,\n                                                    batch_size=30,\n                                                    class_mode='binary',\n                                                    target_size=(300, 300))     \n# --------------------\n# Flow validation images in batches of 20 using test_datagen generator\n# --------------------\nvalidation_generator =  test_datagen.flow_from_directory(validation_dir,\n                                                         batch_size=30,\n                                                         class_mode  = 'binary',\n                                                         target_size = (300, 300))","52441f6a":"model = tf.keras.models.Sequential([\n    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(300, 300, 3)),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Dropout(0.30),\n    tf.keras.layers.Conv2D(32, (4,4), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Dropout(0.30),\n    tf.keras.layers.Conv2D(128,(4,4), activation='relu'), \n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Dropout(0.30),\n    tf.keras.layers.Conv2D(128,(4,4), activation='relu'), \n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Dropout(0.30),\n    tf.keras.layers.Conv2D(128,(4,4), activation='relu'), \n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Dropout(0.30),\n    tf.keras.layers.Conv2D(128,(3,3), activation='relu'), \n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Dropout(0.30),\n    # Flatten the results to feed into a DNN\n    tf.keras.layers.Flatten(), \n    # 512 neuron hidden layer\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.30),\n    tf.keras.layers.Dense(1, activation='sigmoid')  \n]) \n    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('Screw driver') and 1 for the other ('Wrench')","e1190752":"model.summary()","170351aa":"from tensorflow.keras.optimizers import RMSprop\n\nmodel.compile(optimizer=RMSprop(lr=0.001),\n              loss='binary_crossentropy',\n              metrics = ['accuracy'])","1874a9a8":"history = model.fit(train_generator,\n                              validation_data=validation_generator,\n                              steps_per_epoch=15,\n                              epochs=8,\n                              validation_steps=15,\n                              verbose=2)","aa506f32":"#-----------------------------------------------------------\n# Retrieve a list of list results on training and test data\n# sets for each training epoch\n#-----------------------------------------------------------\nacc      = history.history[     'accuracy' ]\nval_acc  = history.history[ 'val_accuracy' ]\nloss     = history.history[    'loss' ]\nval_loss = history.history['val_loss' ]\n\nepochs   = range(len(acc)) # Get number of epochs\n\n#------------------------------------------------\n# Plot training and validation accuracy per epoch\n#------------------------------------------------\nplt.plot  ( epochs,     acc )\nplt.plot  ( epochs, val_acc )\nplt.title ('Training and validation accuracy')\nplt.figure()\n\n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.plot  ( epochs,     loss )\nplt.plot  ( epochs, val_loss )\nplt.title ('Training and validation loss'   )","b219f8fd":"import cv2\nimport numpy as np\nimg = cv2.imread('..\/input\/mechanical-tools-dataset\/test_data\/test_data\/Screwdriver (1404).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict_classes(img)\n\nprint(classes)\nif classes[0]>0:\n    print(\"This is a Wrench\")\nelse :\n    print(\"This is a Screw driver\")","14870a5f":"img = cv2.imread('..\/input\/mechanical-tools-dataset\/test_data\/test_data\/Screwdriver (1407).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict_classes(img)\n\nprint(classes)\n\nif classes[0]>0:\n    print(\"This is a Wrench\")\nelse :\n    print(\"This is a Screwdriver\")\n","b92fbe51":"img = cv2.imread('..\/input\/mechanical-tools-dataset\/test_data\/test_data\/Screwdriver (1408).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict_classes(img)\n\nprint(classes)\nif classes[0]>0:\n    print(\"This is a Wrench\")\nelse :\n    print(\"This is a Screwdriver\")","d918b079":"img = cv2.imread('..\/input\/mechanical-tools-dataset\/test_data\/test_data\/Screwdriver (1410).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict_classes(img)\n\nprint(classes)\nif classes[0]>0:\n    print(\"This is a Wrench\")\nelse :\n    print(\"This is a Screwdriver\")","53a1eb1c":"img = cv2.imread('..\/input\/mechanical-tools-dataset\/test_data\/test_data\/Screwdriver (1415).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict_classes(img)\n\nprint(classes)\nif classes[0]>0:\n    print(\"This is a Wrench\")\nelse :\n    print(\"This is a Screwdriver\")","73123239":"img = cv2.imread('..\/input\/mechanical-tools-dataset\/test_data\/test_data\/Screwdriver (1401).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict_classes(img)\n\nprint(classes)\nif classes[0]>0:\n    print(\"This is a Wrench\")\nelse :\n    print(\"This is a Screwdriver\")","1b9c54a1":"img = cv2.imread('..\/input\/mechanical-tools-dataset\/test_data\/test_data\/Screwdriver (1402).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict_classes(img)\n\nprint(classes)\nif classes[0]>0:\n    print(\"This is a Wrench\")\nelse :\n    print(\"This is a Screwdriver\")","d054705e":"img = cv2.imread('..\/input\/mechanical-tools-dataset\/test_data\/test_data\/Wrench (286).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict_classes(img)\n\nprint(classes)\nif classes[0]>0:\n    print(\"This is a Wrench\")\nelse :\n    print(\"This is a Screwdriver\")","51235979":"img = cv2.imread('..\/input\/mechanical-tools-dataset\/test_data\/test_data\/Wrench (293).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict_classes(img)\n\nprint(classes)\nif classes[0]>0:\n    print(\"This is a Wrench\")\nelse :\n    print(\"This is a Screwdriver\")","b4102e9a":"img = cv2.imread('..\/input\/mechanical-tools-dataset\/test_data\/test_data\/Wrench (276).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict_classes(img)\n\nprint(classes)\nif classes[0]>0:\n    print(\"This is a Wrench\")\nelse :\n    print(\"This is a Screwdriver\")","e99b09f1":"img = cv2.imread('..\/input\/mechanical-tools-dataset\/test_data\/test_data\/Wrench (268).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict_classes(img)\n\nprint(classes)\nif classes[0]>0:\n    print(\"This is a Wrench\")\nelse :\n    print(\"This is a Screwdriver\")","4ed5a602":"img = cv2.imread('..\/input\/mechanical-tools-dataset\/test_data\/test_data\/Wrench (313).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict_classes(img)\n\nprint(classes)\nif classes[0]>0:\n    print(\"This is a Wrench\")\nelse :\n    print(\"This is a Screwdriver\")","c77cef03":"img = cv2.imread('..\/input\/mechanical-tools-dataset\/test_data\/test_data\/Wrench (324).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict_classes(img)\n\nprint(classes)\nif classes[0]>0:\n    print(\"This is a Wrench\")\nelse :\n    print(\"This is a Screwdriver\")","e6504d74":"model.predict_classes(validation_generator)","fc5d40ea":"import numpy as np\nimport random\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img\n\n# Let's define a new Model that will take an image as input, and will output\n# intermediate representations for all layers in the previous model after\n# the first.\nsuccessive_outputs = [layer.output for layer in model.layers[1:]]\n#visualization_model = Model(img_input, successive_outputs)\nvisualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n# Let's prepare a random input image from the training set.\nwrench_img_files = [os.path.join(train_wrench_dir, f) for f in train_wrench_fnames]\nscrewdriver_img_files = [os.path.join(train_wrench_dir, f) for f in train_screwdriver_fnames]\nimg_path = random.choice(wrench_img_files + screwdriver_img_files)\n\nimg = load_img(img_path, target_size=(300, 300))  # this is a PIL image\nx = img_to_array(img)  # Numpy array with shape (150, 150, 3)\nx = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)\n\n# Rescale by 1\/255\nx \/= 255\n\n# Let's run our image through our network, thus obtaining all\n# intermediate representations for this image.\nsuccessive_feature_maps = visualization_model.predict(x)\n\n# These are the names of the layers, so can have them as part of our plot\nlayer_names = [layer.name for layer in model.layers[1:]]\n\n# Now let's display our representations\nfor layer_name, feature_map in zip(layer_names, successive_feature_maps):\n    if len(feature_map.shape) == 4:\n    # Just do this for the conv \/ maxpool layers, not the fully-connected layers\n        n_features = feature_map.shape[-1]  # number of features in feature map\n    # The feature map has shape (1, size, size, n_features)\n        size = feature_map.shape[1]\n    # We will tile our images in this matrix\n        display_grid = np.zeros((size, size * n_features))\n        for i in range(n_features):\n      # Postprocess the feature to make it visually palatable\n            x = feature_map[0, :, :, i]\n            x -= x.mean()\n            x \/= x.std()\n            x *= 64\n            x += 128\n            x = np.clip(x, 0, 255).astype('uint8')\n      # We'll tile each filter into this big horizontal grid\n            display_grid[:, i * size : (i + 1) * size] = x\n    # Display the grid\n        scale = 20. \/ n_features\n        plt.figure(figsize=(scale * n_features, scale))\n        plt.title(layer_name)\n        plt.grid(False)\n        plt.imshow(display_grid, aspect='auto', cmap='summer')","9c58a315":"**Now, let's see what the filenames look like in the training directories:**","8ce6a1f7":"**Now, display a batch of 50 wrench and 50 pliers pictures. You can rerun the cell to see a fresh batch each time:**","cbf8f422":"**Let's find out the number of wrench and pliers images in the directory**","ef442695":"**The \"output shape\" column shows how the size of your feature map evolves in each successive layer. The convolution layers reduce the size of the feature maps by a bit due to padding, and each pooling layer halves the dimensions.**","0365e1e3":"# Data Preprocessing","7acc67a1":"# Defining each of these directories","53ef97b2":"**Now let's take a look at a few pictures to get a better sense of what they look like. First, configure the matplot parameters:**","c452b207":"**Visualizing Intermediate Representations\nTo get a feel for what kind of features our convnet has learned, one fun thing to do is to visualize how an input gets transformed as it goes through the convnet.\nLet's pick a random image from the training set, and then generate a figure where each row is the output of a layer, and each image in the row is a specific filter in that output feature map. Rerun this cell to generate intermediate representations for a variety of training images.**","4ec8500b":"**We then add convolutional layers as in the previous example, and flatten the final result to feed into the densely connected layers.**\n**Note that because we are facing a two-class classification problem, i.e. a binary classification problem, we will end our network with a sigmoid activation, so that the output of our network will be a single scalar between 0 and 1, encoding the probability that the current image is class 1 (as opposed to class 0).**"}}