{"cell_type":{"5e0617fd":"code","85491693":"code","c6c40a2f":"code","abb2cae8":"code","4bf64bc2":"code","fa57bb6f":"code","356e10ed":"code","eff7d08a":"code","4ad5d8c8":"code","7d1016b1":"code","7f0c6e54":"code","3799ebec":"code","caec618f":"code","de867c86":"code","375a8fad":"code","6661f1a6":"code","a4b5e7b2":"code","7cbb289a":"code","02992629":"code","bcab2a49":"code","53ec2406":"code","9bdc352e":"code","087f2d7f":"code","096548b7":"code","f5995331":"code","ff3c3dbe":"code","4e748588":"code","21bd01d4":"markdown","c279378d":"markdown","f67e4420":"markdown","ea5bf985":"markdown","3ed64168":"markdown","b2b45928":"markdown","fabddcc1":"markdown","f7b1abc8":"markdown"},"source":{"5e0617fd":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.metrics import log_loss\n\nfrom scipy.stats import norm\nfrom scipy import stats\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.pipeline import make_pipeline","85491693":"df_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf_submission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","c6c40a2f":"# \u6700\u592790\u5217\u307e\u3067\u8868\u793a\u3059\u308b\n# display setting for pandas.DataFrame\npd.set_option('display.max_columns', 90)","abb2cae8":"print(df_train.shape)\ndf_train.head()","4bf64bc2":"print(df_test.shape)\ndf_test.head()","fa57bb6f":"print(df_submission.shape)\ndf_submission.head()","356e10ed":"# \u30ce\u30a4\u30ba\u306e\u524a\u9664\n# Remove outliers\n\ndf_train.plot.scatter(x='GrLivArea', y='SalePrice')\nplt.show()\n\ndf_train.drop(df_train.query('GrLivArea > 4000 and SalePrice < 300000').index, axis=0, inplace=True)","eff7d08a":"# Histogram and Q-Q plot\n\nsns.distplot(df_train['SalePrice'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)","4ad5d8c8":"# \u5bfe\u6570\u5909\u63db\n# Logarithmic transformation\n\ndf_train['SalePrice'] = np.log1p(df_train['SalePrice'])\n\nsns.distplot(df_train['SalePrice'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)","7d1016b1":"# NaN\u306e\u30ab\u30a6\u30f3\u30c8\n# NaN\u306e\u5272\u5408\u304c80%\u4ee5\u4e0a\u3067\u3042\u308c\u3070\u3001drop\u30ea\u30b9\u30c8\u306b\u8ffd\u52a0\u3059\u308b\n\n# Missing value counts\n# drop column when it includes more than 80% missing values\n# for training data\n\nnull_rate = [df_train[col].isnull().sum() \/ df_train.shape[0] for col in df_train.columns]\nnull_count = [df_train[col].isnull().sum() for col in df_train.columns]\nnull_info = pd.DataFrame({'null_rate': null_rate, 'null_count': null_count})\nnull_info.index = df_train.columns\n\nnull_info = null_info[null_info['null_rate'] > 0.0]\nnull_info.sort_values('null_rate', ascending=False, inplace=True)\n\ndrop_col = list(null_info[null_info['null_rate'] > 0.8].index)\nprint('Dropped features: ', drop_col)\n\nsns.barplot(x=null_info['null_rate'], y=null_info.index, orient='h')\n\nnull_info.T","7f0c6e54":"# NaN\u306e\u30ab\u30a6\u30f3\u30c8\n# NaN\u306e\u5272\u5408\u304c80%\u4ee5\u4e0a\u3067\u3042\u308c\u3070\u3001drop\u30ea\u30b9\u30c8\u306b\u8ffd\u52a0\u3059\u308b\n\n# Missing value counts\n# drop column when it includes more than 80% missing values\n# for test data\n\nnull_rate = [df_test[col].isnull().sum() \/ df_test.shape[0] for col in df_test.columns]\nnull_count = [df_test[col].isnull().sum() for col in df_test.columns]\nnull_info = pd.DataFrame({'null_rate': null_rate, 'null_count': null_count})\nnull_info.index = df_test.columns\n\nnull_info = null_info[null_info['null_rate'] > 0.0]\nnull_info.sort_values('null_rate', ascending=False, inplace=True)\n\ndrop_col.extend(list(null_info[null_info['null_rate'] > 0.8].index))\ndrop_col = list(set(drop_col))\nprint('Dropped features: ', drop_col)\n\ntmp = null_info.loc[null_info['null_rate'] > 0.01, 'null_rate']\nsns.barplot(x=tmp, y=tmp.index, orient='h')\n\nnull_info.T","3799ebec":"# \u51fa\u73fe\u983b\u5ea6\u304c\u6700\u5927\u306e\u5024\u3092\u30ab\u30a6\u30f3\u30c8\u3059\u308b\n# \u4e00\u3064\u306e\u5024\u304c90%\u4ee5\u4e0a\u3092\u5360\u3081\u3066\u3044\u308c\u3070\u3001drop\u30ea\u30b9\u30c8\u306b\u8ffd\u52a0\u3059\u308b\n\n# Remove the high frequency value\n# for training data\n\ntopval_count = [df_train[col].value_counts(dropna=False).iloc[0] for col in df_train.columns]\ntopval_rate = [df_train[col].value_counts(dropna=False).iloc[0] \/ df_train.shape[0] for col in df_train.columns]\ntopval_info = pd.DataFrame({'topval_rate': topval_rate, 'topval_count': topval_count})\ntopval_info.index = df_train.columns\ntopval_info.sort_values('topval_rate', ascending=False, inplace=True)\n\nfig, ax = plt.subplots(figsize=(6, 6))\ntmp = topval_info.loc[topval_info['topval_rate'] > 0.65, 'topval_rate']\nsns.barplot(x=tmp, y=tmp.index, orient='h', ax=ax)\n\ndrop_col.extend(list(topval_info[topval_info['topval_rate'] > 0.9].index))\ndrop_col = list(set(drop_col))\nprint('Droppend features', drop_col)\n\ntopval_info.T","caec618f":"# \u51fa\u73fe\u983b\u5ea6\u304c\u6700\u5927\u306e\u5024\u3092\u30ab\u30a6\u30f3\u30c8\u3059\u308b\n# \u4e00\u3064\u306e\u5024\u304c90%\u4ee5\u4e0a\u3092\u5360\u3081\u3066\u3044\u308c\u3070\u3001drop\u30ea\u30b9\u30c8\u306b\u8ffd\u52a0\u3059\u308b\n\n# Remove the high frequency value\n# for test data\n\ntopval_count = [df_test[col].value_counts(dropna=False).iloc[0] for col in df_test.columns]\ntopval_rate = [df_test[col].value_counts(dropna=False).iloc[0] \/ df_test.shape[0] for col in df_test.columns]\ntopval_info = pd.DataFrame({'topval_rate': topval_rate, 'topval_count': topval_count})\n\ntopval_info.index = df_test.columns\ntopval_info.sort_values('topval_rate', ascending=False, inplace=True)\n\nfig, ax = plt.subplots(figsize=(6, 6))\ntmp = topval_info.loc[topval_info['topval_rate'] > 0.65, 'topval_rate']\nsns.barplot(x=tmp, y=tmp.index, orient='h', ax=ax)\n\ndrop_col.extend(list(topval_info[topval_info['topval_rate'] > 0.9].index))\ndrop_col = list(set(drop_col))\nprint('Dropped features', drop_col)\n\ntopval_info.T","de867c86":"# Drop columns\n\ndf_train.drop(drop_col, axis=1, inplace=True)\ndf_test.drop(drop_col, axis=1, inplace=True)\n\ndf_train.drop('MoSold', axis=1, inplace=True)\ndf_test.drop('MoSold', axis=1, inplace=True)\n\nprint('train shape:', df_train.shape, 'test shape:', df_test.shape)","375a8fad":"# Show columns which incude missing values.\ndf_train.loc[:, [df_train[col].isnull().sum() > 0 for col in df_train.columns]].head(2)","6661f1a6":"# Show columns which incude missing values.\ndf_test.loc[:, [df_test[col].isnull().sum() > 0 for col in df_test.columns]].head(2)","a4b5e7b2":"# \u6b20\u640d\u5024\u57cb\u3081\n# data leakage\u306f\u7121\u8996\u3059\u308b\n\n# Impute missing values\n# This method will cause data leakage\n\n# combine train and test data\ndf_all = pd.concat([df_train, df_test], sort=True)\n\ndf_all['LotFrontage'] = df_all.groupby('Neighborhood')['LotFrontage'].transform(lambda x : x.fillna(x.median()))\ndf_all['MasVnrType'].fillna(df_all['MasVnrType'].mode()[0], inplace=True)\ndf_all['MasVnrArea'].fillna(df_all['MasVnrArea'].mode()[0], inplace=True)\n\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    df_all[col].fillna('None', inplace=True)\n    \nfor col in ('GarageType', 'GarageFinish', 'GarageQual'):\n    df_all[col].fillna('None', inplace=True)\n    \ndf_all['GarageYrBlt'].fillna(0, inplace=True)\ndf_all['FireplaceQu'].fillna('None', inplace=True)\ndf_all['GarageArea'].fillna(0, inplace=True)\ndf_all['GarageCars'].fillna(0, inplace=True)\n\nfor col in df_all.drop('SalePrice', axis=1).columns:\n    if df_all[col].isnull().sum() > 0:\n        df_all[col].fillna(df_all[col].mode()[0], inplace=True)\n\nprint('Final null-count: ', df_all.drop('SalePrice', axis=1).isnull().sum().sum())","7cbb289a":"# About 'GrLivArea' feature\n\nsns.distplot(df_all['GrLivArea'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(df_all['GrLivArea'], plot=plt)","02992629":"# Logarithmic transformation\n\ndf_all['GrLivArea'] = np.log1p(df_all['GrLivArea'])\n\nsns.distplot(df_all['GrLivArea'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(df_all['GrLivArea'], plot=plt)","bcab2a49":"# About 'TotalBsmtSF' feature\n\nsns.distplot(df_all['TotalBsmtSF'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(df_all['TotalBsmtSF'], plot=plt)","53ec2406":"# Logarithmic transformation\n# Applied when the sample has basement.\n\ndf_all['HasBsmt'] = (df_all['TotalBsmtSF'] > 0).astype(int)\n\ndf_all.loc[df_all['HasBsmt'] > 0, 'TotalBsmtSF'] = np.log1p(df_all.loc[df_all['HasBsmt'] > 0, 'TotalBsmtSF'] )\n\nsns.distplot(df_all.loc[df_all['HasBsmt'] > 0, 'TotalBsmtSF'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(df_all.loc[df_all['HasBsmt'] > 0, 'TotalBsmtSF'], plot=plt)","9bdc352e":"# category\u5909\u6570\u3092one-hot\u8868\u73fe\u306b\u5909\u63db\u3057\u3066\u3001\u30ea\u30c3\u30b8\u56de\u5e30\u3092\u884c\u3046\n\n# One-hot encoding\ndf_all_dummy = pd.get_dummies(df_all, drop_first=True)\n\n# split data\ntrain_X = df_all_dummy[df_all_dummy['SalePrice'].notnull()].drop('SalePrice', axis=1)\ntrain_y = df_all_dummy.loc[df_all_dummy['SalePrice'].notnull(), 'SalePrice']\ntest_X = df_all_dummy[df_all_dummy['SalePrice'].isnull()].drop('SalePrice', axis=1)\nprint('train_X:', train_X.shape, 'train_y:', train_y.shape, 'test_X:', test_X.shape)\n\nrs = RobustScaler()\ntrain_X = rs.fit_transform(train_X)\ntest_X = rs.transform(test_X)\n\n# Cross validation with Ridge regressor\nkf = KFold(n_splits=4, random_state=1, shuffle=True)\nridge_alphas = [1e-8, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]\n#ridge = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas, cv=kf))\nridge  = RidgeCV(alphas=ridge_alphas, cv=kf)\nridge.fit(train_X, train_y)\nridge.score(train_X, train_y)\n\npred = ridge.predict(test_X)\npred = np.expm1(pred)\n\ndf_submission['SalePrice'] = pred\ndf_submission.to_csv('ridge_regression.csv', index=False)","087f2d7f":"# LabelEncoder\u3088\u308a\u3001category\u5024\u3092\u6570\u5024\u306b\u5909\u63db\u3059\u308b\n# Label encoding\n\ntrain_X = df_all[df_all['SalePrice'].notnull()].drop(['Id', 'SalePrice'], axis=1)\ntrain_y = df_all.loc[df_all['SalePrice'].notnull(), 'SalePrice']\ntest_X = df_all[df_all['SalePrice'].isnull()].drop(['Id', 'SalePrice'], axis=1)\n\nfor col in train_X.loc[:, train_X.dtypes == 'object'].columns:\n    le = LabelEncoder()\n    train_X[col] = le.fit_transform(train_X[col])\n    \n    test_X[col] = le.transform(test_X[col])\n    \nprint('train_X:', train_X.shape, 'train_y:', train_y.shape, 'test_X:', test_X.shape)\ntrain_X.head()","096548b7":"# Cross validation with LightGBM regressor\n\nlgb_param = {'objective': 'regression', 'seed': 1, 'metric': 'rmse', 'max_depth': 10}\n\nlgb_train = lgb.Dataset(train_X, train_y)\n\ncv_results = lgb.cv(lgb_param, lgb_train, num_boost_round=100, verbose_eval=10, nfold=4, stratified=False)\n\nnround = len(cv_results['rmse-mean'])\n\nplt.plot(range(nround), cv_results['rmse-mean'])\nplt.show()","f5995331":"# LightGBM training and prediction\n\nlgb_param = {'objective': 'regression', 'seed': 1, 'metric': 'rmse', 'max_depth': 10}\n\nlgb_train = lgb.Dataset(train_X, train_y)\n\nmodel = lgb.train(lgb_param, lgb_train, num_boost_round=50)\n\npred = model.predict(test_X)\npred = np.expm1(pred)\n\nlgb.plot_importance(model, height=0.5, figsize=(8, 12))\n\ndf_submission['SalePrice'] = pred\ndf_submission.to_csv('lightgbm.csv', index=False)","ff3c3dbe":"# Cross validation with XGBoost regressor\n\ndtrain = xgb.DMatrix(train_X, label=train_y)\n\nparams = {'objective': 'reg:squarederror', 'silent': 1, 'random_state': 1, 'eta': 0.01, 'max_depth':8}\nnum_round = 5000\n\ncv_results = xgb.cv(params, dtrain, num_boost_round=num_round, nfold=4, stratified=False, early_stopping_rounds=15, verbose_eval=50)\n\nnround = len(cv_results['train-rmse-mean'])\n\nplt.plot(range(nround), cv_results['train-rmse-mean'])\nplt.plot(range(nround), cv_results['test-rmse-mean'])\nplt.grid()\nplt.show()","4e748588":"# XGBoost training and prediction\n\ndtrain = xgb.DMatrix(train_X, label=train_y)\ndtest = xgb.DMatrix(test_X)\n\nparams = {'objective': 'reg:squarederror', 'silent': 1, 'random_state': 1, 'eta': 0.01, 'max_depth':8}\nnum_round = 1000\n\nwatch_list = [(dtrain, 'train')]\nmodel = xgb.train(params, dtrain, num_boost_round=num_round, evals=watch_list, verbose_eval=50)\n\npred = model.predict(dtest)\npred = np.expm1(pred)\n\ndf_submission['SalePrice'] = pred\ndf_submission.to_csv('xgboost.csv', index=False)","21bd01d4":"### One-hot encoding and Ridge regression","c279378d":"### Preprocessing for 'SalePrice'","f67e4420":"### Impute missing values","ea5bf985":"### Importing Libraries","3ed64168":"### Loading dataset","b2b45928":"### Some feature's transformation","fabddcc1":"### Feature reduction","f7b1abc8":"### Label encoding and LightGBM \/ XGBoost regressor"}}