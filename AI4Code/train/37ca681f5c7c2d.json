{"cell_type":{"712bf482":"code","7ac9ea3d":"code","732ddc47":"code","08038d1f":"code","3b8dda3a":"code","fa0a2587":"code","4495b199":"code","caf97e01":"code","36193594":"code","1d1e89a4":"code","06ae982c":"code","17790899":"code","f60854ea":"code","b5d1a91d":"code","2452c076":"code","0a904636":"code","400e235c":"code","b075bb9a":"code","59a95f3a":"code","856d8b20":"code","3eb597c2":"code","17ffba4b":"code","37dd0db0":"code","ad30e7dc":"code","758f9890":"code","fb711788":"code","b39dd3ae":"code","fbd55fe7":"markdown","b858998f":"markdown","f505f639":"markdown","c89ad18e":"markdown","37426ce3":"markdown","0dc3d5d5":"markdown","2ff61b6a":"markdown","8a7e4020":"markdown","722d5704":"markdown","12178790":"markdown","60f0cb00":"markdown","01603a56":"markdown","e6ea5bfe":"markdown","5daa7459":"markdown","9e7fd190":"markdown","b637bb79":"markdown","b8d5d376":"markdown","5d7d6ec6":"markdown","930df96e":"markdown","69f4f893":"markdown","095cb5cf":"markdown","58136a3b":"markdown","1e6cc3d4":"markdown","ddc2fae4":"markdown","84cd5640":"markdown","7807fa88":"markdown","c20207ba":"markdown","4cf5b560":"markdown","a6a71166":"markdown","e27de21d":"markdown","3e294c89":"markdown","82d4f7cb":"markdown","146520ff":"markdown","c1b81174":"markdown"},"source":{"712bf482":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')","7ac9ea3d":"data: pd.DataFrame = pd.read_csv('..\/input\/BlackFriday.csv')\ndescribe = data.describe()\ndescribe.loc['#unique'] = data.nunique()\ndisplay(describe)","732ddc47":"purchase_desc = data['Purchase'].describe()\npurchase_desc.drop(['count', 'std'], inplace=True)\npurchase_desc.loc['sum'] = data['Purchase'].sum()\npurchase_desc.loc['mean_by_user'] = data['Purchase'].sum() \/ data['User_ID'].nunique()\ndisplay(pd.DataFrame(purchase_desc).T)","08038d1f":"null_percent = (data.isnull().sum() \/ len(data))*100\ndisplay(pd.DataFrame(null_percent[null_percent > 0].apply(lambda x: \"{:.2f}%\".format(x)),columns=['Null %']))","3b8dda3a":"cat_describe = data[['Product_ID', 'Gender', 'Age', 'Occupation', 'City_Category', 'Marital_Status', 'Product_Category_1']].astype('object').describe()\ncat_describe.loc['percent'] = 100*cat_describe.loc['freq'] \/ cat_describe.loc['count']\ndisplay(cat_describe)","fa0a2587":"plt.figure(figsize=(13, 6))\ngender_gb = data[['Gender', 'Purchase']].groupby('Gender').agg(['count', 'sum'])\nparams = {\n#     'colors': [(255\/255, 102\/255, 102\/255, 1), (102\/255, 179\/255, 1, 1)],\n    'labels': gender_gb.index.map({'M': 'Male', 'F': 'Female'}),\n    'autopct': '%1.1f%%',\n    'startangle': -30, \n    'textprops': {'fontsize': 15},\n    'explode': (0.05, 0),\n    'shadow': True\n}\nplt.subplot(121)\nplt.pie(gender_gb['Purchase']['count'], **params)\nplt.title('Number of transactions', size=17)\nplt.subplot(122)\nplt.pie(gender_gb['Purchase']['sum'], **params)\nplt.title('Sum of purchases', size=17)\nplt.show()","4495b199":"gender_gb = data[['Gender', 'Purchase']].groupby('Gender', as_index=False).agg('mean')\nsns.barplot(x='Gender', y='Purchase', data=gender_gb)\nplt.ylabel('')\nplt.xlabel('')\nfor spine in plt.gca().spines.values():\n    spine.set_visible(False)\nplt.title('Mean purchase amount by gender', size=14)\nplt.show()","caf97e01":"plt.figure(figsize=(16, 8))\nplt.subplot(121)\nsns.countplot(y='Age', data=data, order=sorted(data.Age.unique()))\nplt.title('Number of transactions by age group', size=14)\nplt.xlabel('')\nplt.ylabel('Age Group', size=13)\nplt.subplot(122)\nage_gb = data[['Age', 'Purchase']].groupby('Age', as_index=False).agg('mean')\nsns.barplot(y='Age', x='Purchase', data=age_gb, order=sorted(data.Age.unique()))\nplt.title('Mean purchase amount by age group', size=14)\nplt.xlabel('')\nplt.ylabel('')\nplt.show()","36193594":"age_product_gb = data[['Age', 'Product_ID', 'Purchase']].groupby(['Age', 'Product_ID']).agg('count').rename(columns={'Purchase': 'count'})\nage_product_gb.sort_values('count', inplace=True, ascending=False)\nages = sorted(data.Age.unique())\nresult = pd.DataFrame({\n    x: list(age_product_gb.loc[x].index)[:5] for x in ages\n}, index=['#{}'.format(x) for x in range(1,6)])\ndisplay(result)","1d1e89a4":"men = data[data.Gender == 'M']['Occupation'].value_counts(sort=False)\nwomen = data[data.Gender == 'F']['Occupation'].value_counts(sort=False)\npd.DataFrame({'M': men, 'F': women}, index=range(0,21)).plot.bar(stacked=True)\nplt.gcf().set_size_inches(10, 4)\nplt.title(\"Count of different occupations in dataset (Separated by gender)\", size=14)\nplt.legend(loc=\"upper right\")\nplt.xlabel('Occupation label', size=13)\nplt.ylabel('Count', size=13)\nplt.show()","06ae982c":"import random\ncolor_mapping = {}\ndef random_color(val):    \n    if val in color_mapping.keys():\n        color = color_mapping[val]\n    else:\n        r = lambda: random.randint(0,255)\n        color = 'rgba({}, {}, {}, 0.4)'.format(r(), r(), r())\n        color_mapping[val] = color\n    return 'background-color: %s' % color\n\nocc_product_gb = data[['Occupation', 'Product_ID', 'Purchase']].groupby(['Occupation', 'Product_ID']).agg('count').rename(columns={'Purchase': 'count'})\nocc_product_gb.sort_values('count', inplace=True, ascending=False)\nresult = pd.DataFrame({\n    x: list(occ_product_gb.loc[x].index)[:5] for x in range(21)\n}, index=['#{}'.format(x) for x in range(1,6)])\ndisplay(result.style.applymap(random_color))","17790899":"stay_years = [data[data.Stay_In_Current_City_Years == x]['City_Category'].value_counts(sort=False).iloc[::-1] for x in sorted(data.Stay_In_Current_City_Years.unique())]\n\nf, (ax1, ax2) = plt.subplots(1,2, gridspec_kw = {'width_ratios':[1, 2]}, sharey=True)\n\nyears = sorted(data.Stay_In_Current_City_Years.unique())\npd.DataFrame(stay_years, index=years).T.plot.bar(stacked=True, width=0.3, ax=ax1, rot=0, fontsize=11)\nax1.set_xlabel('City Category', size=13)\nax1.set_ylabel('# Transactions', size=14)\nax1.set_title('# Transactions by city (separated by stability)', size=14)\n\nsns.countplot(x='Stay_In_Current_City_Years', data=data, ax=ax2, order=years)\nax2.set_title('# Transactions by stability', size=14)\nax2.set_ylabel('')\nax2.set_xlabel('Years in current city', size=13)\n\nplt.gcf().set_size_inches(15, 6)\nplt.show()","f60854ea":"out_vals = data.Marital_Status.value_counts()\nin_vals = np.array([data[data.Marital_Status==x]['Gender'].value_counts() for x in [0,1]]).flatten()\n\nfig, ax = plt.subplots(figsize=(7, 7))\n\nsize = 0.3\ncmap = plt.get_cmap(\"tab20c\")\nouter_colors = cmap(np.arange(2)*4)\ninner_colors = cmap(np.array([1, 2, 5, 6]))\n\nax.pie(out_vals, radius=1, colors=outer_colors,\n       wedgeprops=dict(width=size, edgecolor='w'), labels=['Single', 'Married'],\n       textprops={'fontsize': 15}, startangle=50)\n\nax.pie(in_vals, radius=1-size, colors=inner_colors,\n       wedgeprops=dict(width=size, edgecolor='w'), labels=['M', 'F', 'M', 'F'],\n       labeldistance=0.75, textprops={'fontsize': 12, 'weight': 'bold'}, startangle=50)\n\nax.set(aspect=\"equal\")\nplt.title('Marital Status \/ Gender', fontsize=16)\nplt.show()","b5d1a91d":"col_names = ['Product_ID', 'Product_Category_1', 'User_ID']\nrenames = ['Product', 'Category', 'User']\nresults = []\nfor col_name, new_name in zip(col_names, renames):\n    group = data[[col_name, 'Purchase']].groupby(col_name, as_index=False).agg('count')\n    result = group.sort_values('Purchase', ascending=False)[:10]\n    result.index = ['#{}'.format(x) for x in range(1,11)]\n    results.append(result.rename(columns={col_name: new_name}))    \n    \nfrom IPython.display import display_html\ndef display_side_by_side(*args):\n    html_str=''\n    for df in args:\n        html_str+=df.to_html()\n    display_html(html_str.replace('table','table style=\"display:inline; padding-right: 3em !important;\"'),raw=True)\ndisplay_side_by_side(*results)","2452c076":"train = data.drop(['Product_Category_2', 'Product_Category_3'], axis=1)\\\n            .rename(columns={\n                'Product_ID': 'Product',\n                'User_ID': 'User',\n                'Product_Category_1': 'Category',\n                'City_Category': 'City',\n                'Stay_In_Current_City_Years': 'City_Stay'\n})\ny = train.pop('Purchase')\n\ntrain.loc[:, 'Gender'] = np.where(train['Gender'] == 'M', 1, 0)","0a904636":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, learning_curve, GridSearchCV\n\n# Label encoding Product ID and User ID\nfor col in ['Product', 'User']:    \n    train.loc[:, col] = LabelEncoder().fit_transform(train[col])\n        \n# One hot encoding other features\ncategoricals = ['Occupation', 'Age', 'City', 'Gender', 'Category', 'City_Stay']\nencoder = OneHotEncoder().fit(train[categoricals])\ntrain = pd.concat([train, pd.DataFrame(encoder.transform(train[categoricals]).toarray(), index=train.index, columns=encoder.get_feature_names(categoricals))], axis=1)\ntrain.drop(categoricals, axis=1, inplace=True)\n","400e235c":"# Splitting train and test sets\nX_train, X_test, y_train, y_test = train_test_split(train, y)\n\n# Standardizing\nscaler = StandardScaler().fit(X_train)\nX_train, X_test = scaler.transform(X_train), scaler.transform(X_test)","b075bb9a":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error\n\nparams = {\n    'n_estimators': [10, 30, 100, 300],\n    'max_depth': [3, 5, 7]\n}\ngrid_search = GridSearchCV(RandomForestRegressor(), param_grid=params, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\npreds = grid_search.predict(X_test)\nprint(\"Best params found: {}\".format(grid_search.best_params_))\nprint(\"RMSE score: {}\".format(mean_squared_error(y_test, preds) ** 0.5))","59a95f3a":"sizes, train_scores, test_scores = learning_curve(RandomForestRegressor(**grid_search.best_params_), X_train, y_train, cv=3, n_jobs=-1, scoring='neg_mean_squared_error')\ntrain_scores = np.mean((-1*train_scores)**0.5, axis=1)\ntest_scores = np.mean((-1*test_scores)**0.5, axis=1)\nsns.lineplot(sizes, train_scores, label=\"Train\")\nsns.lineplot(sizes, test_scores, label=\"Test\")\nplt.xlabel(\"Size of training set\", size=13)\nplt.ylabel(\"Round Mean Squared Error\", size=13)\nplt.show()","856d8b20":"model = grid_search.best_estimator_\nimpo = pd.Series(model.feature_importances_[:10], index=train.columns[:10]).sort_values()\nimpo_plot = sns.barplot(x=impo.index, y=impo.values)\nfor item in impo_plot.get_xticklabels():\n    item.set_rotation(50)\nplt.gcf().set_size_inches(8, 4)\nplt.title(\"Top 10 most important features\", size=14)\nplt.show()","3eb597c2":"train = data.drop(['Product_ID', 'Product_Category_1', 'Product_Category_2', 'Product_Category_3', 'Purchase'], axis=1).groupby('User_ID')","17ffba4b":"train = train.agg(lambda x: x.value_counts().index[-1])\nfeaures_list = list(train.columns.values)\nencoder = OneHotEncoder().fit(train[feaures_list])\ntrain = pd.concat([train, pd.DataFrame(encoder.transform(train[feaures_list]).toarray(), index=train.index, columns=encoder.get_feature_names(feaures_list))], axis=1)\ntrain.drop(feaures_list, axis=1, inplace=True)","37dd0db0":"columns = ['Product_ID', 'Product_Category_1']\nfor column in columns:\n    top_100 = data[column].value_counts().index[:100]    \n    user_purchase = pd.pivot_table(\n        data[['User_ID', column, 'Purchase']],\n        values='Purchase',\n        index='User_ID',\n        columns=column,\n        aggfunc=np.sum\n    ).fillna(0)[top_100]  \n    train = train.join(user_purchase)","ad30e7dc":"train = train.join(data[['User_ID', 'Purchase']].groupby('User_ID').agg('sum'))","758f9890":"train_scaled = StandardScaler().fit_transform(train)","fb711788":"from sklearn.cluster import KMeans\n\nk_values = np.arange(1, 11)\nmodels = []\ndists = []\nfor k in k_values:\n    model = KMeans(k).fit(train_scaled)\n    models.append(model)\n    dists.append(model.inertia_)\n\nplt.figure(figsize=(9, 6))\nplt.plot(k_values, dists, 'o-')\nplt.ylabel('Sum of squared distances', size=13)\nplt.xlabel('K', size=13)\nplt.xticks(k_values)\nplt.show()","b39dd3ae":"from sklearn.metrics import silhouette_score\n\nmodel = models[2]\nprint(\"Silhouette score: {:.2f}\".format(silhouette_score(train_scaled, model.predict(train_scaled))))","fbd55fe7":"#### Best sellers\nWhich products sold the most and which categories contain most-sold products? We will only use `Product_Category_1` since the other two have alot of null values.\nAlso, let's see which users have purchased the most.","b858998f":"## Feature Analysis","f505f639":"## Data Overview\nDataset has 537577 rows (transactions) and 12 columns (features) as described below:\n* `User_ID`: Unique ID of the user. There are a total of 5891 users in the dataset.\n* `Product_ID`: Unique ID of the product. There are a total of 3623 products in the dataset.\n* `Gender`: indicates the gender of the person making the transaction.\n* `Age`: indicates the age group of the person making the transaction.\n* `Occupation`: shows the occupation of the user, already labeled with numbers 0 to 20.\n* `City_Category`: User's living city category. Cities are categorized into 3 different categories 'A', 'B' and 'C'.\n* `Stay_In_Current_City_Years`: Indicates how long the users has lived in this city.\n* `Marital_Status`: is 0 if the user is not married and 1 otherwise.\n* `Product_Category_1` to `_3`: Category of the product. All 3 are already labaled with numbers.\n* `Purchase`: Purchase amount.","c89ad18e":"#### Clustering (K-Means)\nNow, let's try and see if we can cluster our customers (`User_ID`s) based on products they have purchased and other features.   \nWe need to change our features a little bit to be more suitable for clustering. Firstly, we are going to cluster users and not the transactions so we should group our transactions by `User_ID` and create our features based on that.   \nNow, what we know about each user, is his\/her gender, living city, stay in current city years, age, marital status, occupation and products (s)he has bought. Let's exclude products he has bought and their categories for now, we will get to them later.   ","37426ce3":"\n## Conclusion\nWe described the dataset and features, did some exploratory data analysis and got some facts and points about dataset. Then we built a Random Forest Regressor to predict purchase amount based on user id, product id and other features available in dataset. Our model had RMSE around mean of the target value which might be good but it was strongly based on product_ids which limits our usage of model as described above.  \nFinally we clustered our customers using K-Means algorithm with 3 clusters and a silhouette score of 0.09 which shows maybe we can't group users confidently since most of them take place near the decision boundaries rather than our centroids. \n","0dc3d5d5":"Let's build and fit our models.\n\n#### RandomForestRegressor\nWhat we are trying to do here is to build a model to predict purchase amount (`Purchase` column in dataset) from other features, namely `User_ID`, `City_Category`, `Age`, etc.  \nThis is a regression problem and we'll use random forest. We'll choose number of trees (`n_estimators`) in our forest and `max_depth` for each tree by calculating scores for each combination and choosing the best one. Scoring metric we'll use is RMSE.","2ff61b6a":"So we'll go with 3 clusters.","8a7e4020":"Men have had transactions about 3 times higher than women in black friday. They've also had proportionaly higher purchase amount, which leads to the assumption that there is no meaningful difference between mean purchase amounts of men and women. Let's check that.","722d5704":"People living in city category of `B` have had most transactions to this store, following by categories `C` and `B` with relatively close values.\nThose who have been in their living city for 1 year had double the number of transactions than any other stay durations, and then comes the people living in their city for 2 years, 3 years, 4+ years and 0 years (<1 year). The pattern is the same within each city category as well.  \nSeems like people living their second year in a city tend to shop more than others.","12178790":"Observation is that people occupied in job labels 0, 4 and 7 have purchased the most in black friday.\n\nLet's check what products people from different occupations were most interested in:","60f0cb00":"A basic observation is that:\n* Product `P00265242` is the most popular product.\n* Most of the transactions were made by men.\n* Age group with most transactions was `26-35`.  \n\nbut we will cover each of these in more depth later.","01603a56":"Mean purchase amount by transaction is 9333 and mean amount by each user is about 850,000. Values are probably not in USD.","e6ea5bfe":"## Description\nWe are going to analyse data of black friday transactions of a retail store. Dataset is provided by [Analytics Vidhya](http:\/\/analyticsvidhya.com).   \n\nKey questions we will be answering are:\n* How are transactions distributed over different age groups, occupations and cities.\n* Which groups of people have a higher transaction number and which have higher purchase amount?\n* How does marital status and years of living in the city affect number and amount of purchases?\n\nand in the end, we will create a model to predict purchase amount based on other features.","5daa7459":"#### Gender\nSo we begin with `Gender`. Let's see how much men and women have purchased and how many transactions they have done.","9e7fd190":"\n## Building models\n\nNow, let's build some models to predict purchase amount based on other features. \n\n#### Feature engineering\nAll of our features are categorical features.  \nAll variables are already encoded except `Product_ID` and `User_ID` which we will encode using LabelEncoder.  \nWe will also remove `Product_Category_2` and `_3` since they have a high rate of null values, and rename some of our remaining features to easier names.   \nOther categorical features have a few number of unique values so we will encode them using OneHotEncoder.\n","b637bb79":"Table above represents top 5 seller products categorized by the user occupation (same products have the same background color).\n1. First thing you notice is that `P00265242` is the most-purchased product for 15 out of 21 occupations and an interesting fact is that this product is not even present in top-5 products of occupations 8, 10 and 17. I wonder what this product is and what these occupations are.\n2. Second interesting thing about this illustration is how similar the first 4 occupations' top-5 are.\n3. Third and last interesting fact from these charts: from top 5 products of occupation 9, one of them is `P00265242` and present in most of other top 5s, one of them is only present in occupation 16's list and the rest are not repeated in any other lists. Adding to account the fact that we saw from previous chart, this was the only occupation with more women than men (even though the totall number of men in dataset was higher), makes occupation 9 a unique occupation among the list.","b8d5d376":"#### City Category and City Stability\nCities are categorized in 3 different categories A, B and C. We have living city of each user in the time of transaction and also we know for how long had the user been in that city, by that time.  \n\nLet's explore number of transactions in each city.","5d7d6ec6":"#### Age\nLet's see what we can observe from `Age` groups provided in dataset","930df96e":"Let's plot learning curve to address possible under\/over fitting","69f4f893":"Now, before getting to our questions, let's get some insights about the dataset and transactions.","095cb5cf":"Note that we filled null values with 0 since they might be some NaN values (not all users have bought all top 100 products).   \nLet's also add total purchase amount for each user as a new feature:","58136a3b":"Now in regard to products, we can't add a column for each product and make it 1 if the user has bought it or 0 otherwise (due to huge number of products) so we'll do it for only top 100 products (by number of transactions).  \nNote that not only we know which user has bought which product, but we can also use purchase amounts as a metric for how much does this user likes\/needs this product. We'll do the same thing for product categories but with all of them as features since there are no more than 18 categories.\n\nSo, in conclusion, we are going to find 100 most selling products and 18 categories (by number of transactions) and for each user, put purchase amount of this product\/product category as a new feature for him, adding totally 118 new features to our data.  ","1e6cc3d4":"What features are more important to our model?","ddc2fae4":"`Product_ID` is having a significant impact on our model and we have `User_ID` in second place with sharp decrease.  \nWe could predict purchase amount with a relatively small error (about half of the mean purchase value) but our model is highly relied on `Product_ID` and `User_ID` which means probably it won't perform as well for new products\/users. Thus, it may not be a good model for predicting future sales for future customers and products but it can be used for other purposes like describing which products tend to be better options to advertise or to give vouchers for.","84cd5640":"Next, we will split our train and test data and will standardize the data using StandardScaler","7807fa88":"People within the ages of 26 to 35 have purchased the most (in number and amount), and as we saw about gender, people in different ages have nearly same mean purchase amount, too.\n\nLet's check what products were most popular in each age group.","c20207ba":"Only `Product_Category_2` and `Product_Category_3` have null values which is good news. However `Product_Category_3` is null for nearly 70% of transactions so it can't give us much information.","4cf5b560":"Now we should choose number of clusters (K). We will use elbow method to choose one. So let's plot different distance sums for different number of clusters:","a6a71166":"Single people have purchased more than married people and in both categories men, following the general pattern of dataset, have purchased more than women.","e27de21d":"#### Marital Status","3e294c89":"#### Occupation","82d4f7cb":"In terms of other features (gender, city, stay in city years, age, marital status and occupation) their values should all be the same in all rows for a particular `User_ID` but we assume there might be small noises and some of them may have wrong values in different rows, so we will take the mode value (one which is repeated the most) for each of them.   \nOn the other hand, they are all nominal features so we will one hot encode them since there are not that much unique values.","146520ff":"We standardize data in columns so features will have same scales in order to get clustered.","c1b81174":"Our assumption was correct and we can't say either men have purchased more expensive products than women or other way around."}}