{"cell_type":{"6b5c743c":"code","374e19b2":"code","424f7048":"code","112aaded":"code","942298c5":"code","5da1236a":"code","b8543e6f":"code","558d6453":"code","db563a2c":"code","f93e10bc":"code","f50898a8":"code","4b769e32":"code","3509c660":"code","fac9fad4":"code","105e4f75":"code","d26d4713":"code","0fca5ecf":"markdown","c6671406":"markdown","cfee1571":"markdown","08b16aac":"markdown","d97f8ada":"markdown","77722f5e":"markdown","813684cf":"markdown","0c2cf79d":"markdown","87490167":"markdown","80398a99":"markdown","ddb61846":"markdown","2bc4fe1f":"markdown","b6a2133f":"markdown","3449c6ee":"markdown","155b486a":"markdown"},"source":{"6b5c743c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","374e19b2":"#@ Initialization:\n%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","424f7048":"#@ Downloading the Libraries and Dependencies. \n# !pip install -q -U trax                         # Downloading the Trax.\n# nltk.download(\"punkt\")\nimport pandas as pd\nimport numpy as np\nimport os\nimport nltk\nimport trax\nfrom trax import layers as tl\nfrom trax.supervised import training\nfrom trax.fastmath import numpy as fastnp\nimport random\nfrom collections import defaultdict\nfrom functools import partial\n\nrandom.seed(111)","112aaded":"#@ Getting the Data:\ndata = pd.read_csv(\"\/kaggle\/input\/quora-questions-answers\/Questions.csv\")\n\n#@ Inspecting the Data:\nprint(f\"Number of Questions Pairs: {len(data)}\")\ndata.head(10)                                                        # Inspecting the DataFrame.","942298c5":"#@ Processing the Data:\nN_train = 300000                                               \nN_test = 10240                                                 \ndata_train = data[:N_train]                                                    # Training pairs.\ndata_test = data[N_train:N_train+N_test]                                       # Test pairs.\ndel(data)                                                                      # Removing.\n\n#@ Inspecting the Data:\nprint(f\"Training Set: {len(data_train)} and Test Set: {len(data_test)}\")\n\n#@ Selecting the Question Pairs for Training:\ntrain_idx = (data_train[\"is_duplicate\"] == 1).to_numpy()\ntrain_idx = [i for i,x in enumerate(train_idx) if x]\nprint(f\"Number of Duplicate Questions: {len(train_idx)}\")\nprint(f\"Indexes of first Duplicate Questions: {train_idx[:10]}\")","5da1236a":"#@ Inspecting the Duplicate Questions:\nprint(data_train[\"question1\"][20])                                 # Index 20 has Duplicate Questions pairs.\nprint(data_train[\"question2\"][20])                                 # Index 20 has Duplicate Questions pairs.\nprint(\"Index 20 is duplicate:\", data_train[\"is_duplicate\"][20])","b8543e6f":"#@ Preparing the Data: Training the Model:\nQ1_train_words = np.array(data_train[\"question1\"][train_idx])\nQ2_train_words = np.array(data_train[\"question2\"][train_idx])\n\n#@ Preparing the Data: Evaluating the Model:\nQ1_test_words = np.array(data_test[\"question1\"])\nQ2_test_words = np.array(data_test[\"question2\"])\ny_test = np.array(data_test[\"is_duplicate\"])\n\n#@ Inspecting the Data:\nprint(\"TRAINING QUESTIONS:\\n\")\nprint(\"Question 1:\", Q1_train_words[7])\nprint(\"Question 2:\", Q2_train_words[7], \"\\n\")\n\nprint(\"TESTING QUESTIONS:\\n\")\nprint(\"Question 1:\", Q1_test_words[7])\nprint(\"Question 2:\", Q2_test_words[7], \"\\n\")\nprint(\"Inspecting Testing pairs is duplicate:\", y_test[0])","558d6453":"#@ Preparing the Data:\nQ1_train = np.empty_like(Q1_train_words)                                # Creating new Training array.\nQ2_train = np.empty_like(Q2_train_words)                                # Creating new Training array.\nQ1_test = np.empty_like(Q1_test_words)                                  # Creating new Test array.\nQ2_test = np.empty_like(Q2_test_words)                                  # Creating new Test array.\n\n#@ Building Vocabulary with Training Dataset:\nvocab = defaultdict(lambda: 0)\nvocab[\"<PAD>\"] = 1\nfor idx in range(len(Q1_train_words)):\n  Q1_train[idx] = nltk.word_tokenize(Q1_train_words[idx])               # Tokenizing the Training Set.\n  Q2_train[idx] = nltk.word_tokenize(Q2_train_words[idx])               # Tokenizing the Training Set.\n  q = Q1_train[idx] + Q2_train[idx]\n  for word in q:\n    if word not in vocab:\n      vocab[word] = len(vocab) + 1\nprint(\"The length of the Vocabulary is:\", len(vocab))\n\n#@ Testing Dataset:\nfor idx in range(len(Q1_test_words)):\n  Q1_test[idx] = nltk.word_tokenize(Q1_test_words[idx])                 # Tokenizing the Test Set.\n  Q2_test[idx] = nltk.word_tokenize(Q2_test_words[idx])                 # Tokenizing the Test Set.\n\n#@ Inspecting the Final Prepared Dataset:\nprint(\"Training Set is reduced to:\", len(Q1_train))\nprint(\"Test Set is:\", len(Q1_test))","db563a2c":"#@ Preparing the Data:\n\n#@ Converting Questions pairs to array of Integers:\nfor i in range(len(Q1_train)):\n  Q1_train[i] = [vocab[word] for word in Q1_train[i]]\n  Q2_train[i] = [vocab[word] for word in Q2_train[i]]\n\n#@ Converting Questions pairs to array of Integers:\nfor i in range(len(Q1_test)):\n  Q1_test[i] = [vocab[word] for word in Q1_test[i]]\n  Q2_test[i] = [vocab[word] for word in Q2_test[i]]\n\n#@ Inspecting the Encoded Data:\nprint(\"Question in the Training Set:\")                           # Inspecting the Training Set.\nprint(Q1_train_words[7], \"\\n\")\nprint(\"Encoded Version:\")\nprint(Q1_train[7], \"\\n\")\nprint(\"Question in the Test Set:\")                               # Inspecting the Test Set.\nprint(Q1_test_words[7], \"\\n\")\nprint(\"Encoded Version:\")\nprint(Q1_test[7], \"\\n\")\n\n#@ Splitting the Training Set into Training and Validation Dataset:\nsplit = int(len(Q1_train) * 0.8)\ntrain_Q1, train_Q2 = Q1_train[:split], Q2_train[:split]                        # Split for Training set.\nval_Q1, val_Q2 = Q1_train[split:], Q2_train[split:]                            # Split for Validation set.\nprint(f\"Total numbers of questions pairs: {len(Q1_train)}\")              \nprint(f\"The length of Training set: {len(train_Q1)}\")                          # Length of Final Training set.\nprint(f\"The length of Validation set: {len(val_Q1)}\")                          # Length of Final Validation set.","f93e10bc":"#@ Data Generator:\ndef data_generator(Q1, Q2, batch_size, pad=1, shuffle=True):\n  \"\"\" Generator Function that yields the Batches of Data. \"\"\"\n  #@ Initializing the Dependencies:\n  input1, input2 = [], []\n  idx = 0\n  len_q = len(Q1)\n  question_index = [*range(len_q)]\n  if shuffle:\n    random.shuffle(question_index)\n  \n  while True:\n    if idx >= len_q:\n      idx = 0\n      if shuffle:\n        random.shuffle(question_index)\n    #@ Getting the Questions pairs in Index positions:\n    q1 = Q1[question_index[idx]]\n    q2 = Q2[question_index[idx]]\n    idx += 1\n    #@ Adding the Data:\n    input1.append(q1)\n    input2.append(q2)\n    if len(input1) == batch_size:\n      max_len = max(max([len(q) for q in input1]),\n                    max([len(q) for q in input2]))\n      max_len = 2**int(np.ceil(np.log2(max_len)))\n      b1, b2 = [], []\n      for q1, q2 in zip(input1, input2):\n        q1 = q1 + [pad] * (max_len - len(q1))                         # Adding pad to q1 until it reaches max length.\n        q2 = q2 + [pad] * (max_len - len(q2))                         # Adding pad to q2 until it reaches max length.\n        b1.append(q1)\n        b2.append(q2)\n      yield np.array(b1), np.array(b2)\n      input1, input2 = [], []                                         # Resetting the Batches.\n\n#@ Inspecting the Example of Data Generator:\nres1, res2 = next(data_generator(train_Q1, train_Q2, batch_size=2))\nprint(f\"First Questions:\\n{res1}\")\nprint(f\"\\nSecond Questions:\\n{res2}\")","f50898a8":"#@ Siamese Neural Network using Trax:\ndef Siamese(vocab_size=len(vocab), d_model=128, mode=\"train\"):\n  \"\"\" Returns a Siamese Model. \"\"\"\n  #@ Normalizing the Vectors for L2 Normalization:\n  def normalize(x):\n    return x \/ fastnp.sqrt(fastnp.sum(x*x, axis=-1, keepdims=True))\n  #@ Preparing the Model:\n  processor = tl.Serial(                                                  # Returns one hot Vector.\n      tl.Embedding(vocab_size=vocab_size, d_feature=d_model),             # Adding Embedding Layer.\n      tl.LSTM(n_units=d_model),                                           # Adding the LSTM Layer.\n      tl.Mean(axis=1),                                                    # Mean over Columns in Neural Networks.\n      # tl.Dense(n_units=vocab_size),                                     # Adding a Dense Layer.\n      tl.Fn(\"Normalize\", lambda x: normalize(x))                          # Adding the Normalizing Function.\n  )\n  #@ Running the Model in parallel:\n  model = tl.Parallel(processor, processor)\n  return model\n\n#@ Setting up Siamese Neural Network Model:\nmodel = Siamese()\nprint(model)                                                              # Inspecting the Model.","4b769e32":"#@ Triplet Loss Function:\ndef TripletLossFn(v1, v2, margin=0.25):\n  \"\"\" Custom Loss Function. \"\"\"\n  scores = fastnp.dot(v1, v2.T)                                                       # Calculating the dot product of two batches.\n  batch_size = len(scores)                                                            # Calculating the new batch size.\n  positive = fastnp.diagonal(scores)                                                  # Getting positive diagonal entries in scores.\n  negative_without_positive = scores - 2.0 * fastnp.eye(batch_size)\n  closest_negative = negative_without_positive.max(axis=1)                            # Taking row by row max.\n  negative_zero_on_duplicate = scores * (1.0 - fastnp.eye(batch_size))\n  mean_negative = fastnp.sum(negative_zero_on_duplicate, axis=1)\/(batch_size - 1)\n  triplet_loss1 = fastnp.maximum(0, margin - positive + closest_negative)\n  triplet_loss2 = fastnp.maximum(0, margin - positive + mean_negative)\n  triplet_loss = fastnp.mean(triplet_loss1 + triplet_loss2)\n  return triplet_loss\n\n#@ Triplet Loss:\ndef TripletLoss(margin=0.25):\n  triplet_loss_fn = partial(TripletLossFn, margin=margin)\n  return tl.Fn(\"TripletLoss\", triplet_loss_fn)","3509c660":"#@ Preparing the Data:\nbatch_size = 256\ntrain_generator = data_generator(train_Q1, train_Q2, batch_size, vocab[\"<PAD>\"])\nval_generator = data_generator(val_Q1, val_Q2, batch_size, vocab[\"<PAD>\"])\n\n#@ Training the Model:\nlr_schedule = trax.lr.warmup_and_rsqrt_decay(400, 0.01)\ndef train_model(Siamese, TripletLoss, lr_schedule, train_generator=train_generator,\n                val_generator=val_generator, output_dir=\"model\/\"):\n  \"\"\" Training the Siamese Model. \"\"\"\n  output_dir = os.path.expanduser(output_dir)\n  \n  #@ Training:\n  train_task = training.TrainTask(\n      labeled_data = train_generator,                                                   # Using Train Generator.\n      loss_layer = TripletLoss(),                                                       # Using Triplet Loss Function.\n      optimizer = trax.optimizers.Adam(0.001),                                          # Using Adam Optimizer.\n      lr_schedule = lr_schedule                                                         # Using Trax Multifactor Schedule Function.\n  )\n  #@ Evaluating:\n  eval_task = training.EvalTask(\n      labeled_data = val_generator,                                                     # Using Validation Generator.\n      metrics = [TripletLoss()],                                                        # Instantiating the Objects for Evaluation.\n      n_eval_batches = 3\n  )\n  #@ Training the Model:\n  training_loop = training.Loop(                                                        # Training the Model\n      Siamese(),                                                                        # Siameses Neural Networks.\n      train_task, eval_tasks = eval_task,\n      output_dir = output_dir\n  )\n  return training_loop\n\n#@ Training the Model:\ntraining_loop = train_model(Siamese, TripletLoss, lr_schedule)\ntraining_loop.run(1000)                                                                  # Training the Model for 1000 Epochs.","fac9fad4":"#@ Loading the Saved Model:\nmodel = Siamese()\nmodel.init_from_file(\"\/kaggle\/working\/model\/model.pkl.gz\")\n\n#@ Model Evaluation: \ndef classify(test_Q1, test_Q2, y, threshold, model, vocab, data_generator=data_generator, batch_size=64):\n  \"\"\" Function to test the Accuracy of the Model. \"\"\"\n  accuracy = 0                                                                               # Initializing the Accuracy.\n  for i in range(0, len(test_Q1), batch_size):\n    q1, q2 = next(data_generator(test_Q1[i:i+batch_size], test_Q2[i:i+batch_size],\n                                 batch_size, vocab[\"<PAD>\"], shuffle=False))\n    y_test = y[i:i+batch_size]                                                               # Using batch size of actual output target.\n    v1, v2 = model((q1, q2))                                                                 # Using the Model.\n    for j in range(batch_size):\n      d = np.dot(v1[j], v2[j].T)                                                             # Calculating the Cosine Similarity.\n      res = d > threshold\n      accuracy += (y_test[j] == res)\n  accuracy = accuracy \/ len(test_Q1)\n  return accuracy\n\n#@ Computing the Accuracy of the Model:\naccuracy = classify(Q1_test, Q2_test, y_test, 0.7, model, vocab, batch_size=512)             # Calculating the Accuracy.\nprint(\"Accuracy of the Model:\", accuracy) ","105e4f75":"#@ Model Evaluation with own Questions:\ndef predict(question1, question2, threshold, model, vocab, data_generator=data_generator, verbose=False):\n  \"\"\" Function for predicting if two Questions are Duplicates. \"\"\"\n  q1 = nltk.word_tokenize(question1)                                # Tokenization.\n  q2 = nltk.word_tokenize(question2)                                # Tokenization.\n  Q1, Q2 = [], []\n  for word in q1:\n    Q1 += [vocab[word]]                                             # Encoding.\n  for word in q2:\n    Q2 += [vocab[word]]                                             # Encoding.\n  Q1, Q2 = next(data_generator([Q1], [Q2], 1, vocab[\"<PAD>\"]))\n  v1, v2 = model((Q1, Q2))                                          # Using Model.\n  d = fastnp.dot(v1[0], v2[0].T)\n  res = d > threshold\n  if (verbose):\n    print(\"Q1 = \", Q1, \"\\nQ2 = \", Q2)\n    print(\"d = \", d)\n    print(\"res = \", res)\n  return res ","d26d4713":"#@ Examples of Questions:\nquestion1 = \"How are you?\"\nquestion2 = \"Are you fine?\"\n#@ Predicting the Duplicated Questions:\nexample1 = predict(question1, question2, 0.7, model, vocab, verbose=True)\nprint(\"Example1:\", example1, \"\\n\")\n\n#@ Example of Questions:\nquestion1 = \"Do you enjoy eating the dessert?\"\nquestion2 = \"Do you like hiking in the desert?\"\n#@ Predicting the Duplicated Questions:\nexample2 = predict(question1, question2, 0.7, model, vocab, verbose=True)\nprint(\"Example2:\", example2)","0fca5ecf":"**Preparing the Data**\n* I will convert each Questions Pairs to Tensors or array of Numbers using the Vocabulary. Then I will split the Training set into Training and Validation Dataset so that I can use it to train and evaluate the Neural Networks: Siamese Networks.","c6671406":"**Initialization**\n* I use these 3 lines of code on top of my each Notebooks because it will help to prevent any problems while reloading and reworking on a Project or Problem. And the third line of code helps to make visualization within the Notebook.","cfee1571":"**Model Evaluation**\n* Now, I will test the Model using my own Questions. I will build a reverse Vocabulary that allows the map encoded Questions back to words. ","08b16aac":"**Preparing the Data**\n* I will encode each word of the selected pairs with an Index which will be a list of numbers. Firstly, I will Tokenize each word using NLTK and I will use Python's Default Dictionary which assigns the values 0 to all Out of Vocabulary Words. ","d97f8ada":"# **Recognizing Duplicate Questions using Trax**\n\n**Introduction and Objective**\n- I have worked with **Quora Questions Answers Dataset** to build a Long Short Term Memory or LSTM Model that can identify the Similar Questions or the Duplicate Questions which is useful when we have to work with several versions of the same Questions. I have build a Model using Trax which can identify the Duplicate Questions in this Project.\n\n**Neural Networks and Trax**\n- Trax is good for Implementing new state of the Art Algorithms like Transformers, Reformers and BERT. It is actively maintained by Google Brain Team for Advanced Deep Learning tasks.\n\n**Siamese Neural Networks**\n- Siamese Neural Network is an Artificial Neural Network which uses the same weight while working in tandem on two different input vectors to compute comparable output vectors.  In Siamese Neural Networks: One of the output vectors is precomputed, thus forming a baseline against which the other output vector is compared.\n\n**Libraries and Dependencies**\n- I have listed all the necessary Libraries and Dependencies required for this Project here:\n\n```javascript\nimport pandas as pd\nimport numpy as np\nimport os\nimport nltk\nimport trax\nfrom trax import layers as tl\nfrom trax.supervised import training\nfrom trax.fastmath import numpy as fastnp\nimport random\nfrom collections import defaultdict\nfrom functools import partial\n```\n\n**Getting the Data**\n- I have used Google Colab for this Project so the process of downloading and reading the Data might be different in other platforms. I will be using Quora Answer Question Dataset for this Project. I will build a Model that can Identify the Similar Questions or the Duplicate Questions which is useful when we have to work with several versions of the same Questions. The Dataset is labeled.\n\n**Processing the Data**\n- I will split the Data into Training set and Testing Set. The Test Set will be used later to evaluate the Model. I will select only the Question Pairs that are duplicate to train the Model. I will build two batches as input for the Neural Networks: Siamese Networks. The Test set uses the original pairs of Questions and the Status describing if the Questions are duplicates. I will encode each word of the selected pairs with an Index which will be a list of numbers. Firstly, I will Tokenize each word using NLTK and I will use Python's Default Dictionary which assigns the values 0 to all Out of Vocabulary Words. I have presented the simple Implementation of Data Preparation for training the LSTM Model using Quora Dataset here.\n\n**Siamese Neural Networks and Triplet Loss**\n- Siamese Neural Network is an Artificial Neural Network which uses the same weight while working in tandem on two different input vectors to compute comparable output vectors.  In Siamese Neural Networks: One of the output vectors is precomputed, thus forming a baseline against which the other output vector is compared. The Triplet Loss makes use of a Baseline or Anchor Input which is compared to the Positive or Truthy Input and a Negatve or Falsy Input. The distance from the Anchor Input to the Positive Input is minimized and the distance from the Anchor Input to the Negative Input is maximized. The Triplet Loss is composed of two terms where one term utilizes the mean of all the non duplicates and the second term utilizes the Closest Negative. I have presented the Implementation of Siamese Neural Network using LSTM Model along with the Implementation of Triplet Loss here.\n\n**Training the Model**\n- Now, I will train the Model. I will define the Cost Function and the Optimizer as ususal. I will use Training Iterator to go through all the Data for each Epochs while training the Model.  I have also presented the Implementation of Training the Model using Data Generators and other dependencies. I have presented all the Implementations using Trax Framework here.\n\n**Model Evaluation**\n- I will utilize the Test Set which was configured earlier to determine the accuracy of the Model. Actually the Training Set only had Positive examples whereas the Test Set and y test is setup as pairs of Questions and some of which are duplicates and some are not. I will compute the Cosine Similarity of each pair, threshold it and compare the result to y test. The results are accumulated to produce the Accuracy. I have presented the Implementation of Siamese Neural Network using LSTM Model that can identify the Similar or Duplicate Questions here.\n\n**Predicting the Questions Pairs**\n- Now, I will test the Model using my own Questions. I will build a reverse Vocabulary that allows the map encoded Questions back to words. I have also presented the output of the Model here which fascinates me a lot. ","77722f5e":"**Processing the Data**\n* I will split the Data into Training set and Testing Set. The Test Set will be used later to evaluate the Model. I will select only the Question Pairs that are duplicate to train the Model. I will build two batches as input for the Neural Networks: Siamese Networks. The Test set uses the original pairs of Questions and the Status describing if the Questions are duplicates. ","813684cf":"**Preparing the Data**","0c2cf79d":"**Getting the Data**\n* I have used Google Colab for this Project so the process of downloading and reading the Data might be different in other platforms. I will be using **Quora Answer Question Dataset** for this Project. I will build a Model that can Identify the Similar Questions or the Duplicate Questions which is useful when we have to work with several versions of the same Questions. The Dataset is labeled.","87490167":"**Data Generator**\n* In most of the Natural Language Processing and AI in general, using batches when training the Dataset is more efficient. Now, I will build the Data Generator that takes in Questions pairs and returns batches in the form of Tuples. The Tuples consist of two arrays and each array will have batch size Questions pairs. The command next(data generator) will return the next batch. The Data Generator will returns the Data in a format that can be used directly int the Model while computing Feed Forward. It will return a pair of arrays of Questions.","80398a99":"I hope you will gain some insights from here and you will also spend some time working on the same. If you made it till the end, Please upvote this Notebook, it will motivate me a lot.","ddb61846":"**Model Evaluation**\n* I will utilize the Test Set which was configured earlier to determine the accuracy of the Model. Actually the Training Set only had Positive examples whereas the Test Set and y test is setup as pairs of Questions and some of which are duplicates and some are not. I will compute the Cosine Similarity of each pair, threshold it and compare the result to y test. The results are accumulated to produce the Accuracy. ","2bc4fe1f":"**Downloading the Dependencies**\n* I have downloaded all the Libraries and Dependencies required for this Project in one particular cell.","b6a2133f":"**Triplet Loss**\n* The Triplet Loss makes use of a Baseline or Anchor Input which is compared to the Positive or Truthy Input and a Negatve or Falsy Input. The distance from the Anchor Input to the Positive Input is minimized and the distance from the Anchor Input to the Negative Input is maximized. The Triplet Loss is composed of two terms where one term utilizes the mean of all the non duplicates and the second term utilizes the Closest Negative. ","3449c6ee":"**Siamese Neural Network**\n* A Siamese Neural Network is a Neural Network which uses the same weight while working in tandem on two different Input vectors to compute comparable output Vectors. Here, I will get the Embedding, run it through LSTM or Long Short Term Memory Network, Noramlize the two Vectors and Finally, I will use Triplet Loss to get the corresponding Cosine Similarity for each pair of Questions. ","155b486a":"**Training the Model**\n* Now, I will train the Model. I will define the Cost Function and the Optimizer as ususal. I will use Training Iterator to go through all the Data for each Epochs while training the Model."}}