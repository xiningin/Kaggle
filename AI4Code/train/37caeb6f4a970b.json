{"cell_type":{"73fa4307":"code","a6e46b6b":"code","b80a874b":"code","794dacd2":"code","40a0e462":"code","f0477d3d":"code","766c2109":"code","6fd8da91":"code","48f46b63":"code","b38f2645":"code","24a0055c":"code","3b609661":"code","4dd08366":"code","2bd69fa9":"code","33d6d0b7":"code","d9af1b9f":"code","02a8961f":"code","bdf54cc7":"code","77425f00":"code","57121cea":"code","9d272cfd":"code","b4433488":"code","e2d4106b":"markdown","ab987891":"markdown","9234030a":"markdown","56075261":"markdown","d11e4c93":"markdown","d7a53928":"markdown","ca5aff3d":"markdown","179c2b3d":"markdown","1580d411":"markdown","39db77f2":"markdown"},"source":{"73fa4307":"!pip install easydict pillow","a6e46b6b":"import os\nimport sys\nimport math\nimport datetime\nimport numpy as np\nimport pandas as pd\nimport time\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import StepLR\nimport argparse\nimport easydict\nfrom torch import autograd\n\nos.makedirs('log', exist_ok=True)\n\nfrom torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter('log\/tensorboard')","b80a874b":"random_seed = 2021\ntorch.manual_seed(random_seed)\ntorch.cuda.manual_seed(random_seed)\nnp.random.seed(random_seed)","794dacd2":"DATASET_PATH = os.path.join('\/USER\/DATA\/inclass1_traffic')","40a0e462":"pd.read_csv(os.path.join(DATASET_PATH, 'train.csv'))","f0477d3d":"pd.read_csv(os.path.join(DATASET_PATH, 'validate.csv'))","766c2109":"pd.read_csv(os.path.join(DATASET_PATH, 'test.csv'))","6fd8da91":"import torch\nfrom torch.utils import data\nfrom torchvision import datasets, transforms\nimport os\n\n\nclass CustomDataset(data.Dataset):\n    def __init__(self, root, seq_len, batch_size=64, phase='train'):\n        self.root = root\n        self.phase = phase\n        self.seq_len = seq_len * 24\n        self.batch_size = batch_size\n        self.labels = {}\n\n        self.label_path = os.path.join(self.root, self.phase + '.csv')\n\n        df = pd.read_csv(self.label_path)\n        timestamps = [(i, j) for (i, j) in zip(list(df['\ub0a0\uc9dc']), list(df['\uc2dc\uac04']))]\n        categories = df.columns.values.tolist()[2:]\n\n        input_data = []\n        output_data = []\n\n        for t in range(len(timestamps)):\n            temp_input_data = []\n            temp_output_data = []\n            for col in categories:\n                road = df[col].tolist()\n                inp = [float(i) for i in road[t:t+self.seq_len]]\n                outp = [float(j) for j in road[t+self.seq_len:t+2*self.seq_len]]\n                temp_input_data.append(inp)\n                temp_output_data.append(outp)\n            input_data.append(temp_input_data)\n            output_data.append(temp_output_data)\n\n        self.labels['timestamp'] = timestamps\n        self.labels['category'] = categories\n        self.labels['input'] = input_data\n        self.labels['output'] = output_data\n\n    def __getitem__(self, index):\n\n        row = index \/\/ 35\n        col = index % 35\n\n        timestamp = self.labels['timestamp'][row]\n        category = self.labels['category'][col]\n        \n        input_data = torch.tensor(self.labels['input'][row][col])\n\n        if self.phase != 'test':\n            output_data = torch.tensor(self.labels['output'][row][col])\n        else:\n            output_data = []\n\n        return timestamp, category, (input_data, output_data)\n\n    def __len__(self):\n        return (len(self.labels['timestamp']) - (self.seq_len * 2) + 1) * 35\n\n    def get_label_file(self):\n        return self.label_path\n\n\ndef data_loader(root, phase='train', batch_size=64, seq_len=7, drop_last=False):\n    if phase == 'train':\n        shuffle = True\n    else:\n        shuffle = False\n\n    dataset = CustomDataset(root, seq_len, batch_size, phase)\n    dataloader = data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n\n    return dataloader, dataset.get_label_file()","48f46b63":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\nclass LSTMNet(nn.Module):\n    def __init__(self,\n                 input_size=168,\n                 hidden_size=1024,\n                 output_size=168,\n\n                 batch_size=64,\n\n                 num_layers=3,\n                 dropout=0,\n                 batch_first=False):\n        super(LSTMNet, self).__init__()\n\n        self.hidden_size = hidden_size\n        \n        ##### Layer 1\n        self.lstm1 = nn.LSTM(input_size,\n                            hidden_size,\n                            dropout=0.2,\n                            num_layers=num_layers)\n\n        ##### Layer 2\n        self.lstm2 = nn.LSTM(hidden_size, \n                             hidden_size,\n                             dropout=0.2,\n                             num_layers=num_layers)\n\n        ##### Finalize\n        self.linear = nn.Linear(hidden_size, \n                                output_size)\n        \n        self.activation = nn.LeakyReLU(0.2)\n\n\n    def forward(self, x, h_in, c_in):\n\n        h_in = nn.Parameter(h_in.type(dtype), requires_grad=True)\n        c_in = nn.Parameter(c_in.type(dtype), requires_grad=True)\n\n        # Layer 1\n        lstm_out, (h_1, c_1) = self.lstm1(x, (h_in, c_in))\n        lstm_out = self.activation(lstm_out)\n\n        # Layer2\n        lstm_out, (h_2, c_2) = self.lstm2(lstm_out, (h_1, c_1))\n        lstm_out = self.activation(lstm_out)\n\n        # Final\n        predictions = self.linear(lstm_out)\n        \n        return predictions, (h_2, c_2)","b38f2645":"class RMSLELoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.mse = nn.MSELoss()\n        \n    def forward(self, pred, actual):\n        return torch.sqrt(self.mse(torch.log(pred + 1), torch.log(actual + 1)))\n\n\ndef save_model(model_name, model, optimizer, scheduler):\n    state = {\n        'model': model.state_dict(),\n        'optimizer': optimizer.state_dict(),\n        'scheduler': scheduler.state_dict()\n    }\n    torch.save(state, os.path.join('log', model_name + '.pth'))\n    print('model saved')\n    return os.path.join('log', model_name + '.pth')\n\n\ndef load_model(model_name, model, optimizer=None, scheduler=None):\n    state = torch.load(os.path.join(model_name))\n    model.load_state_dict(state['model'])\n    if optimizer is not None:\n        optimizer.load_state_dict(state['optimizer'])\n    if scheduler is not None:\n        scheduler.load_state_dict(state['scheduler'])\n    print('model loaded')","24a0055c":"dtype = torch.float\nmodel_name = 'sequential'\n\nbatch_size = 64\nnum_epochs = 20\nprint_iter = 200\nval_epoch = 1\nsave_epoch = 1\nbase_lr = 0.01\nseq_len = 7\n\ninput_size = seq_len * 24\noutput_size = input_size\nhidden_size = 1024\nnum_layers = 6","3b609661":"os.makedirs('log', exist_ok=True)","4dd08366":"# model\nmodel = LSTMNet(input_size=input_size,\n                hidden_size=hidden_size,\n                output_size=output_size,\n                batch_size=batch_size,\n                num_layers=num_layers)\nmodel = model.to(device)\n\n# loss function\ncriterion = RMSLELoss()\n\n# optimizer & scheduler\noptimizer = Adam(model.parameters(), lr=base_lr)\nscheduler = StepLR(optimizer, step_size=20, gamma=0.1)","2bd69fa9":"print(model)","33d6d0b7":"# get data loader\ntrain_dataloader, _ = data_loader(root=DATASET_PATH,\n                                  phase='train',\n                                  batch_size=batch_size,\n                                  seq_len=seq_len,\n                                  drop_last=True)\n\nvalidate_dataloader, _ = data_loader(root=DATASET_PATH,\n                                     phase='validate',\n                                     batch_size=1,\n                                     seq_len=seq_len,\n                                     drop_last=True)\n\nprint(\"------------------------------------------------------------\")\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(\"num of parameters : \",total_params)\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(\"num of trainable parameters :\", trainable_params)\nprint(\"------------------------------------------------------------\")","d9af1b9f":"running_loss = 0.0\nepoch_loss, min_epoch_loss = 0.0, 99.0\n\nfor epoch in range(num_epochs):\n\n    avg_loss = []\n    model.train()\n\n    for iter_, sample in enumerate(train_dataloader):\n\n        (h_in, c_in) = (torch.zeros(num_layers, batch_size, hidden_size, requires_grad=True).to(device),\n                        torch.zeros(num_layers, batch_size, hidden_size, requires_grad=True).to(device))\n\n        _, _, (input_data, output_data) = sample\n        \n        input_data = input_data.unsqueeze(0).to(device)\n        output_data = output_data.unsqueeze(0).to(device)\n\n        pred, (h_in, c_in) = model(input_data, h_in, c_in)\n        \n        loss = criterion(pred, output_data)\n\n        model.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        avg_loss.append(loss.item())\n        running_loss += loss.item()\n\n        if iter_ % 200 == 199:\n            writer.add_scalar('training loss',\n                    running_loss \/ 200,\n                    epoch * len(train_dataloader) + iter_)\n            running_loss = 0\n\n        if iter_ % print_iter == 0:\n            print('Epoch: {:5} | Iteration: {:5} | Loss: {:1.5f}'.format(epoch, iter_, loss))\n\n    scheduler.step()\n    print('\\nEpoch: {:5} | Loss: {:1.5f}\\n'.format(epoch, sum(avg_loss) \/ len(avg_loss)))\n    avg_loss = []\n    \n    if epoch_loss < min_epoch_loss:\n        save_model('best', model, optimizer, scheduler)\n        print(f'best model so far: epoch {epoch}')\n        min_epoch_loss = epoch_loss\n\n    if epoch % save_epoch == 0:\n        save_model('last', model, optimizer, scheduler)\n        print(f'last model: epoch {epoch}')\n\n    if epoch % val_epoch == 0:\n        \n        val_score = 0.0\n        model.eval()\n\n        with torch.no_grad():\n            (h_in, c_in) = (torch.zeros(num_layers, 1, hidden_size, requires_grad=False).to(device),\n                    torch.zeros(num_layers, 1, hidden_size, requires_grad=False).to(device))\n\n            for iter_, sample in enumerate(validate_dataloader):\n\n                _, _, (input_data, output_data) = sample\n\n                input_data = input_data.unsqueeze(0).to(device)\n                output_data = output_data.unsqueeze(0).to(device)\n\n                pred, (h_in, c_in) = model(input_data, h_in, c_in)\n                score = criterion(pred, output_data)\n                val_score += score.item()\n        \n        print('\\nValidation Epoch: {:5} | Loss: {:1.5f}\\n'.format(epoch, val_score\/len(validate_dataloader)))\n        model.train()","02a8961f":"dtype = torch.float\nseq_len = 7\n\ninput_size = seq_len * 24\nhidden_size = 1024\noutput_size = input_size\nbatch_size = 1\nnum_layers = 6","bdf54cc7":"test_dataloader, _ = data_loader(root=DATASET_PATH,\n                                  phase='test',\n                                  batch_size=batch_size,\n                                  seq_len=seq_len,\n                                  drop_last=True)","77425f00":"model = LSTMNet(input_size=input_size,\n                hidden_size=hidden_size,\n                output_size=output_size,\n                batch_size=batch_size,\n                num_layers=num_layers)\n\n# model\nmodel_name = 'log\/best.pth'\n\nload_model(model_name, model)\nmodel = model.to(device)","57121cea":"submission_file_path = os.path.join(DATASET_PATH, 'sample_submission.csv')\nsubmission_table = pd.read_csv(submission_file_path)","9d272cfd":"(h_in, c_in) = (torch.zeros(num_layers, 1, hidden_size, requires_grad=False).to(device),\n                torch.zeros(num_layers, 1, hidden_size, requires_grad=False).to(device))\n\nfor iter_, sample in enumerate(test_dataloader):\n\n    timestamp, category, (input_data, output_data) = sample\n    input_data = input_data.unsqueeze(0).to(device)\n\n    pred, (h_in, c_in) = model(input_data, h_in, c_in)\n\n    for i, (t, h) in enumerate(zip(timestamp[0], timestamp[1])):\n        for cat, row in zip(category, pred[0]):\n            cat = f'road_{cat}'\n            submission_table[cat] = row.tolist()","b4433488":"submission_table.to_csv('prediction.csv', index=False)","e2d4106b":"### Model","ab987891":"\ub2e4\uc74c \uc140\uc744 \uc2e4\ud589\ud558\uba74 \uae30\uc874\uc5d0 \uc788\ub358 prediction.csv\uac00 \ub36e\uc5b4\uc50c\uc6cc\uc9d1\ub2c8\ub2e4. \ud544\uc694\ud558\uc2dc\uba74 \ucf54\ub4dc\ub97c \uc218\uc815\ud558\uac70\ub098 \ud639\uc740 prediction.csv\ub97c \ubc31\uc5c5\ud558\uc2dc\uae30 \ubc14\ub78d\ub2c8\ub2e4.","9234030a":"\uacb0\uacfc \ud30c\uc77c\uacfc \ubaa8\ub378 \uac00\uc911\uce58 \ud30c\uc77c \uc800\uc7a5\uc744 \uc704\ud574 log \ub514\ub809\ud1a0\ub9ac\uac00 \uc0dd\uc131\ub429\ub2c8\ub2e4. \uc911\uc694\ud55c \ud30c\uc77c\uc774 \ub36e\uc5b4\uc50c\uc6cc\uc9c0\uc9c0 \uc54a\ub3c4\ub85d \uc8fc\uc758 \ubc14\ub78d\ub2c8\ub2e4.","56075261":"### Configurations","d11e4c93":"### Inference","d7a53928":"### Dataloader\n* \ud55c \uce7c\ub7fc\uc5d0 \ub300\ud55c 7\uc77c(168\ud589) \ub370\uc774\ud130\ub97c input_data, \ub4a4\ub530\ub974\ub294 7\uc77c \ub370\uc774\ud130\ub97c output_data\ub85c \ubc18\ud658\ud569\ub2c8\ub2e4.\n* \ub3c4\ub85c\ubcc4 \ucc28\uc774\ub97c \ub450\uc9c0 \uc54a\uace0 \ubaa8\ub4e0 \ub3c4\ub85c\ub97c \ub3d9\uc77c\ud55c \ud0c0\uc785\uc758 \ub370\uc774\ud130\ub85c \ucde8\uae09\ud569\ub2c8\ub2e4.\n* \ubaa8\ub4e0 csv \ud30c\uc77c\uc758 \ub9c8\uc9c0\ub9c9 168\ud589\uc740 \uc608\uce21\ud574\uc57c\ud558\ub294 \uac12\uc774\ubbc0\ub85c input\uc73c\ub85c \ub4e4\uc5b4\uac00\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.","ca5aff3d":"# Kaggle Inclass : Sequential\n\ubca0\uc774\uc2a4\ub77c\uc778","179c2b3d":"### Train","1580d411":"### Hyperparameters","39db77f2":"### Training Setting"}}