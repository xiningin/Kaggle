{"cell_type":{"a48c3ff4":"code","545f8d18":"code","b414b4ed":"code","029de9fa":"code","bc9a3eb3":"code","c6eac41a":"code","23f0c47a":"markdown","781ca662":"markdown","148b5ae2":"markdown","59d46b96":"markdown","fdf9d8e2":"markdown","496a20b4":"markdown","5a48a481":"markdown","cb0642f0":"markdown","9fc02182":"markdown","43036848":"markdown"},"source":{"a48c3ff4":"import numpy as np\nimport datatable as dt\nimport pandas as pd\nimport random\nimport re\nrandom.seed(28)\nimport tqdm\nimport os\nimport gc\nimport logging\nimport optuna\n\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.pyplot as plt\n\nplt.style.use('fivethirtyeight')\nplt.rcParams['figure.figsize'] = [20, 12]  # width, height\n\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import roc_auc_score\n\ninput_path = '\/kaggle\/input\/'\nroot_path = os.path.join(input_path, 'jane-street-market-prediction')","545f8d18":"# Memory saving function credit to https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype.name\n\n        if col_type not in ['object', 'category', 'datetime64[ns, UTC]']:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df","b414b4ed":"%%time\n\ntrain = (dt.fread(os.path.join(root_path, \"train.csv\")).to_pandas()\n        .query('weight > 0').pipe(reduce_mem_usage)\n        .reset_index(drop = True))\n\ntrain['action'] = (train.resp > 0).astype(int)\n\nresp_cols = [i for i in train.columns if 'resp' in i]\n\nfeatures_names = [i for i in train.columns if 'feature_' in i]\nfeatures_index = list(map(lambda x: int(re.sub(\"feature_\", \"\", x)), features_names))\nfeatures_tuples = sorted(list(zip(features_names, features_index)), key = lambda x: x[1])\njust_features = [i[0] for i in features_tuples]","029de9fa":"N_TRIALS=20","bc9a3eb3":"from sklearn.model_selection import StratifiedKFold\n\nparams = {\n    'objective': 'binary',\n    'metrics':['auc'],\n}\n\nnfolds=3\n\nkfold = StratifiedKFold(n_splits=nfolds)\nlgb_models = list()\nimport lightgbm as lgb\nfor k , (train_idx, valid_idx) in enumerate(kfold.split(train.query('date>150')[just_features],\n                                                       train.query('date>150')['action'])): \n    \n    lgb_train = lgb.Dataset(train.loc[train_idx, just_features],\n                            train.loc[train_idx, 'action'])\n    lgb_valid = lgb.Dataset(train.loc[valid_idx, just_features],\n                            train.loc[valid_idx, 'action'])\n    \n    model = lgb.train(\n        params,\n        lgb_train,\n        valid_sets = [lgb_train,lgb_valid],\n        num_boost_round = 10000,\n        verbose_eval = 50,\n        early_stopping_rounds = 10,\n    )\n    \n    lgb_models.append(model)","c6eac41a":"import janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() \nrcount=0\nfor (test_df, sample_prediction_df) in iter_test:\n    \n    #test_df.fillna(train_mean,inplace=True)\n    \n    prediction = 0\n    for model in lgb_models:\n        prediction += model.predict(test_df[just_features])[0]\n    \n    prediction \/= len(lgb_models)\n    prediction = prediction > 0.5\n    sample_prediction_df.action = prediction.astype(int)\n    env.predict(sample_prediction_df)\n    rcount+=1\n    if rcount % 1000 == 0:\n        print('Processed: {} rows\\n'.format(rcount))\n        \nprint(f'Finished processing {rcount} rows.')","23f0c47a":"### Data Loading","781ca662":"# PurgedGroupTimeSeriesSplitStacking Class","148b5ae2":"# About this notebook\n\nIn this notebook I set up a stacking pipeline, using ***XGBoost*** and ***LightGBM*** as base estimators and a default Logistic Regression\/Decision Tree as final estimator (I let the optimization choose the best stacking classifier).\n\n**Both LightGBM and XGBoost use the GPU.**\n\n\nI use **Optuna** to tune base classifiers and I use **PurgedGroupTimeSeriesSplitStacking** class to create indices also for testing the ensemble. \n\nThe goal is to show how to stack together two base models exploiting the PurgedGroupTimeSeriesSplitStacking class to avoid data leakage.\n\n\n### Props to:\n    \n- [PurgedGroupTimeSeriesSplit](https:\/\/www.kaggle.com\/marketneutral\/purged-time-series-cv-xgboost-optuna), the cross validation strategy and memory usage reduction was taken from there;\n- [PurgedGroupTimeSeriesSplitStacking](https:\/\/www.kaggle.com\/tomwarrens\/purgedgrouptimeseriessplit-stacking-ensemble-mode), this is my extension of PurgedGroupTimeSeriesSplit class to allow stacking.\n\n\n##### Pipeline: \n\n- Imports\n- Data Loading\n- PurgedGroupTimeSeriesSplitStacking class definition\n- Optuna Parameters optimization\n- Refit with Best Params\n- Submission\n\n### Edit: \n\nI trained LGBMClassifier on Colab just to have a performance benchmark. Also, in the objective function, the proper way of stacking would be to refit the base estimators (LightGBM and XGBoost) using also (or only) the validation data (otherwise the time gap between train and test data becomes too big and base estimators lose performance). \n\nPlease give me a feedback in the comments section and upvote the kernel if you find it useful.","59d46b96":"Best Params","fdf9d8e2":"You can now refit the stacking Classifier with the best hyperparameters found through Optuna. ","496a20b4":"# LightGBM Only submission after Optimizing Parameters in Colab","5a48a481":"# Optuna Optimization","cb0642f0":"Look [here](https:\/\/optuna.readthedocs.io\/en\/stable\/tutorial\/) for reference\n\nHere I'll use cv_dummy just to show you usage of the notebook. Please edit it as you like. ","9fc02182":"Let's see how data is splitted into train, validation and test folds with the PurgedGroupedTimeSeriesStacking class.","43036848":"### Imports"}}