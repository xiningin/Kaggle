{"cell_type":{"2f603eae":"code","9db8ec26":"code","8d60118a":"code","24ff19f8":"code","b994258c":"code","4317aef7":"code","185af8eb":"code","92393080":"code","e4090b1f":"code","b72f48ea":"code","e2173a48":"code","baf8847b":"code","0b3840ba":"code","0b20eb9a":"markdown","da56922a":"markdown","32aa27e3":"markdown","0f2721f1":"markdown","4c0eea73":"markdown","991fb3f9":"markdown","3e15a8da":"markdown","797e200f":"markdown","7f2f93f7":"markdown","ba15029c":"markdown","8e7f3cc2":"markdown"},"source":{"2f603eae":"from gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom nltk.tokenize import word_tokenize\nimport pandas as pd\nimport sys\nsys.path.insert(0, \"..\/\")","9db8ec26":"def replace_none(X):\n    if X == '':\n        X = np.nan\n    return X","8d60118a":"def build_model(max_epochs, vec_size, alpha, tagged_data):\n    \n    model = Doc2Vec(vector_size=vec_size,\n               alpha=alpha,\n               min_alpha=0.00025,\n               min_count=1,\n               dm=1)\n    \n    model.build_vocab(tag_data)\n    \n    # With the model built we simply train on the data.\n    \n    for epoch in range(max_epochs):\n        print(f\"Iteration {epoch}\")\n        model.train(tag_data,\n                   total_examples=model.corpus_count,\n                   epochs=model.epochs)\n\n        # Here I decrease the learning rate. \n\n        model.alpha -= 0.0002\n\n        model.min_alpha = model.alpha\n    \n    # Now simply save the model to avoid training again. \n    \n    model.save(\"COVID_MEDICAL_DOCS_w2v_MODEL.model\")\n    print(\"Model Saved\")\n    return model","24ff19f8":"corona_df = pd.read_csv(\"..\/input\/covid19-medical-paperscsv\/kaggle_covid-19_open_csv_format.csv\")","b994258c":"corona_df.isnull().sum()","4317aef7":"corona_df['title'] = corona_df['title'].apply(replace_none)\ncorona_df['text_body'] = corona_df['text_body'].apply(replace_none)\ncorona_df = corona_df.dropna()\n\nw2v_data_body = list(corona_df['text_body'])\nw2v_data_title = list(corona_df['title'])\n\nw2v_total_data = w2v_data_body + w2v_data_title","185af8eb":"tag_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(w2v_total_data)]","92393080":"model = build_model(max_epochs=5, vec_size=10, alpha=0.025, tagged_data=tag_data)","e4090b1f":"model.wv.similar_by_word(\"risk\")","b72f48ea":"model.wv.similar_by_word(\"symptoms\")","e2173a48":"model.wv.similar_by_word(\"pregnant\")","baf8847b":"model.wv.similar_by_word(\"economy\")","0b3840ba":"model.wv.similar_by_word(\"isolation\")","0b20eb9a":"To achieve the desired result we only need a few modules. \n\n1. Gensim: An amazing word2vec \/ doc2vec library that allows you to build your own d2v models, as well as load pre-trained models. Brilliant documentation as well. \n\n2. NLTK: Brilliant NLP library! Has everythig the aspiring NLP magician needs. ","da56922a":"**Replace NaN Function**\n\nI use this function later in the notebook to replace an empty string with a np.nan object. This allows us to easily remove missing values using built-in pandas methods. ","32aa27e3":"**Creating a Doc2Vec Model**\n\nThe joy of word2vec is that it will retain the context within the paragraphs, resulting in more meaningful vector values. My plan was to use this to quickly group documents together, making it faster to find resources. \n\nThe first couple of word2vec \/ doc2vec models I attempted to build were a bit of a struggle, so I thought it might be useful for people to see the process here. \n\nAt the end of this notebook the model is saved as a .model file, so there's no need to run the long training regime again, you can just laod the model and off you go. \n\nOther ideas were to create a new feature that represents the average vector for each text body, using this to cluster the documents together and possibly assign labels to begin automating coronavirus risk extraction. \n\n-------------------------------------------\n\nAs ever the first thing to do is import the libraries. \n","0f2721f1":"**Setting Up The Model**\n\nWith the data all in the one list we can go ahead and  create a model using custom parameters. For more on these parameters check the gensim documentation. It's really good!\n\nhttps:\/\/radimrehurek.com\/gensim\/auto_examples\/index.html\n\n-------------------------------\n\nAgain, be warned, this can take a while depending on compute power, for this reason I have set the epochs very low. This number is enough to generate sufficient word vectors, however it can be tuned as required. Also note that there is no need for you to run this again (unless you require more epochs), as it is very simple to load a pre-trained doc2vec model, as demonstrated at the end of this notebook.  ","4c0eea73":"**Train Model Function**\n\nTo keep things tidy I put the training of the model into a little function. \n\n-----------------------------------------","991fb3f9":"Lastly we need to use the Gensim DocumeNt Tagger to apply some typical preprocessing steps for any NLP system. \n\n1. Tokenise: Split paragraphs into tokens (each seperate word is a token), making all the words lower case. \n\n2. Stemming: The word_tokenize function takes care of stemming. \n\n3. Stopword Removel: Likewise the word_tokenzie function takes care of this step. \n\n----------------------------\n\nBe warned, if you're doing this yourself, this can take a while depending on computing power. ","3e15a8da":"So I drop these values and gather all the data to train the d2v model using my wee function above. ","797e200f":"**And That's It!**\n\nIt's as simple as that to construct meaningful word vectors for documents. I really hope this helps someone get up and running with this data faster. I love building these wee tools so will aim to post anythng that might be remotely useful.\n\n-------------------------------------------\n\nBear in mind the functionality you get with NLTK and Gensim is enormous, they are absoutely brilliat and powerful NLP libraries that are incredibly useful, well documented and - to be honest - an outright joy to use. \n\n---------------------------------------------------------------\n\nPlease note that this model only ran for 5 iterations, and while this is good enough to achieve the needed word vectors (and easier on the kernel CPU), this parameter can be tuned as required. ","7f2f93f7":"**A Demo**\n\nSo now the model is built we can ask it to vectorise unseen documents, return documents with similiar vectors or maybe create some features as mentioned above. \n\n--------------------------------\n\nAs a quick demo here are the words with which we can test the model, the words chosen have a particular relation to the task at hand (number 2, the risks).\n\nI think you'll agree, even at this first pass, the results are interesting. \n\n--------------------------------------\n\n1. risk\n2. symptoms\n3. pregnant\n4. economy\n5. isolation\n\n----------------------------------------------------------","ba15029c":"**The Data**\n\nThe data is loaded from the .csv file that was created in a previous kernel. \n\nhttps:\/\/www.kaggle.com\/fmitchell259\/create-corona-csv-file","8e7f3cc2":"**The Corpus**\n\nIn order to build the model we need to provide the doc2vec object with the entire corpus. \n\nAll paper text body and titles are used to build a skip-gram model.\n\n----------------------------------------\n\nFirst though, I check the null values from the parsing. I've lost 944 titles, but for the purposes of building a corpus, we are just within the 5% range. \n\nThe large missing values in abstract is negligable, as the text_body will reflect the abstract. "}}