{"cell_type":{"a332e716":"code","1682462b":"code","59364703":"code","cc2824fe":"code","4063b494":"code","c351b227":"code","56984b32":"code","20274c23":"code","d3d9b1cd":"code","9b1ae697":"code","09e122e5":"code","8ab0b293":"code","d32a7aad":"code","fc41fbb0":"code","39a466cb":"code","6e844b10":"code","f7dd6823":"code","316ca240":"code","bcc685bb":"code","dd47f716":"code","fccaef0e":"code","565b20b1":"code","76e76bcc":"code","1bb920f1":"code","8711d181":"code","5377b2f3":"code","fb157830":"markdown","39c3ceee":"markdown","c41d69b8":"markdown","a1f1b077":"markdown","84d673cd":"markdown","f8f06348":"markdown"},"source":{"a332e716":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_moons, make_circles, make_classification","1682462b":"# Model parametres\ninput_size = 2  # no_of_features\nlayers = [4, 3]  # no. of neurons in 1st and 2nd layer\noutput_class = 2","59364703":"def softmax(a):\n    e_pa = np.exp(a)\n    ans = e_pa \/ np.sum(e_pa, axis=1, keepdims=True)\n    return ans","cc2824fe":"a = np.array([[10, 20]])\nresult = softmax(a)\nprint(result)","4063b494":"class NeuralNetwork:\n    def __init__(self, input_size, layers, output_size):\n        np.random.seed(0)\n\n        model = {}  #Dictionary\n\n        #First Layer\n        model['W1'] = np.random.randn(input_size, layers[0])\n        model['b1'] = np.zeros((1, layers[0]))\n\n        #Second Layer\n        model['W2'] = np.random.randn(layers[0], layers[1])\n        model['b2'] = np.zeros((1, layers[1]))\n\n        #Third\/Output Layer\n        model['W3'] = np.random.randn(layers[1], output_size)\n        model['b3'] = np.zeros((1, output_size))\n\n        self.model = model\n        self.activation_outputs = None\n\n    def forward(self, x):\n\n        W1, W2, W3 = self.model['W1'], self.model['W2'], self.model['W3']\n        b1, b2, b3 = self.model['b1'], self.model['b2'], self.model['b3']\n\n        z1 = np.dot(x, W1) + b1\n        a1 = np.tanh(z1)\n\n        z2 = np.dot(a1, W2) + b2\n        a2 = np.tanh(z2)\n\n        z3 = np.dot(a2, W3) + b3\n        y_ = softmax(z3)\n\n        self.activation_outputs = (a1, a2, y_)\n        return y_\n\n    def backward(self, x, y, learning_rate=0.001):\n        W1, W2, W3 = self.model['W1'], self.model['W2'], self.model['W3']\n        b1, b2, b3 = self.model['b1'], self.model['b2'], self.model['b3']\n        m = x.shape[0]\n\n        a1, a2, y_ = self.activation_outputs\n\n        delta3 = y_ - y\n        dw3 = np.dot(a2.T, delta3)\n        db3 = np.sum(delta3, axis=0)\n\n        delta2 = (1 - np.square(a2)) * np.dot(delta3, W3.T)\n        dw2 = np.dot(a1.T, delta2)\n        db2 = np.sum(delta2, axis=0)\n\n        delta1 = (1 - np.square(a1)) * np.dot(delta2, W2.T)\n        dw1 = np.dot(X.T, delta1)\n        db1 = np.sum(delta1, axis=0)\n\n        #Update the Model Parameters using Gradient Descent\n        self.model[\"W1\"] -= learning_rate * dw1\n        self.model['b1'] -= learning_rate * db1\n\n        self.model[\"W2\"] -= learning_rate * dw2\n        self.model['b2'] -= learning_rate * db2\n\n        self.model[\"W3\"] -= learning_rate * dw3\n        self.model['b3'] -= learning_rate * db3\n\n    def predict(self, x):\n        y_out = self.forward(x)\n        return np.argmax(y_out, axis=1)\n\n    def summary(self):\n        W1, W2, W3 = self.model['W1'], self.model['W2'], self.model['W3']\n        a1, a2, y_ = self.activation_outputs\n\n        print(\"W1 \", W1.shape)\n        print(\"A1 \", a1.shape)\n\n        print(\"W2 \", W2.shape)\n        print(\"A2 \", a2.shape)\n\n        print(\"W3 \", W3.shape)\n        print(\"Y_ \", y_.shape)","c351b227":"def loss(y_oht, p):\n    l = -np.mean(y_oht * np.log(p))\n    return l\n\n\ndef one_hot(y, depth):\n\n    m = y.shape[0]\n    y_oht = np.zeros((m, depth))\n    y_oht[np.arange(m), y] = 1\n    return y_oht","56984b32":"def train(X, Y, model, epochs, learning_rate, logs=True):\n    training_loss = []\n\n    classes = 2\n    Y_OHT = one_hot(Y, classes)\n\n    for ix in range(epochs):\n\n        Y_ = model.forward(X)\n        l = loss(Y_OHT, Y_)\n        training_loss.append(l)\n        model.backward(X, Y_OHT, learning_rate)\n\n        if (logs and ix%50==0):\n            print(\"Epoch %d Loss %.4f\" % (ix, l))\n\n    return training_loss","20274c23":"def plot_decision_boundary(model, X, y, cmap=plt.cm.jet):\n\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    h = 0.01\n\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    Z = model(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n    plt.ylabel('x2')\n    plt.xlabel('x1')\n    plt.style.use(\"seaborn\")\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.jet)\n    plt.show()","d3d9b1cd":"X, Y = make_circles(n_samples=500, shuffle=True, noise=0.2, random_state=1, factor=0.2)","9b1ae697":"plt.style.use('seaborn')\nplt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Accent)\nplt.show()","09e122e5":"model = NeuralNetwork(input_size=2, layers=[4, 3], output_size=2)","8ab0b293":"model.forward([X]).shape","d32a7aad":"model.summary()","fc41fbb0":"losses = train(X, Y, model, 500, 0.001)\n# print(losses)","39a466cb":"plt.plot(losses)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.show()","6e844b10":"fig = plt.figure(figsize=(5, 5))\nax = fig.add_axes([0, 0, 1, 1])\nlabels1 = ['zeros', 'ones']\nlab_colors = ['blue', 'orange']\nlab_counts = np.unique(model.predict(X), return_counts=True)[1]\nax.bar(labels1, lab_counts, color=lab_colors)\nplt.show()","f7dd6823":"plot_decision_boundary(lambda x: model.predict(x), X, Y)","316ca240":"np.unique(model.predict(X), return_counts=True)","bcc685bb":"outputs = model.predict(X)\ntraining_accuracy = np.sum(outputs == Y) \/ Y.shape[0]\nprint(f\"Training accuracy is {training_accuracy*100}%\")","dd47f716":"X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\nY = np.array([0, 1, 1, 0])","fccaef0e":"losses = train(X, Y, model, 500, 0.001)\nprint(losses)","565b20b1":"plt.plot(losses)\nplt.show()","76e76bcc":"plot_decision_boundary(lambda x: model.predict(x), X, Y)","1bb920f1":"outputs = model.predict(X)\ntraining_accuracy = np.sum(outputs == Y) \/ Y.shape[0]\nprint(f\"Training accuracy is {training_accuracy*100}%\")","8711d181":"def load_dataset(dataset):\n    if dataset == 'moons':\n        X, Y = make_moons(n_samples=500, noise=0.2,\n                          random_state=1)  #Perceptron\n    elif dataset == 'circles':\n        X, Y = make_circles(n_samples=500,\n                            shuffle=True,\n                            noise=0.2,\n                            random_state=1,\n                            factor=0.2)\n    elif dataset == 'classification':\n        X, Y = make_classification(n_samples=500,\n                                   n_classes=2,\n                                   n_features=2,\n                                   n_informative=2,\n                                   n_redundant=0,\n                                   random_state=1)\n    else:\n        #Create XOR Dataset\n        X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n        Y = np.array([0, 1, 1, 0])\n\n    return X, Y","5377b2f3":"datasets = [\"xor\", \"classification\", \"moons\", \"circles\"]\n\nfor d in datasets:\n    model = NeuralNetwork(input_size=2, layers=[4, 3], output_size=2)\n    X, Y = load_dataset(d)\n#     losses = train(X, Y, model, 500, 0.001, logs=False)\n    train(X, Y, model, 1000, 0.001, logs=False)\n    outputs = model.predict(X)\n    training_accuracy = np.sum(outputs == Y) \/ Y.shape[0]\n\n    print(f\"Training accuracy : {training_accuracy*100}%\")\n    plt.title(f\"Dataset | {d}\")\n    plot_decision_boundary(lambda x: model.predict(x), X, Y)\n#     plt.plot(losses)\n    plt.show()","fb157830":"## Visualising the output and computing training accuracy","39c3ceee":"# make_circles dataset visualisation","c41d69b8":"# XOR dataset","a1f1b077":"# Motive of notebook\n\nTo implement the multilayer perceptron concept from numpy vectorizaton. It includes:\n- Importing the dataset\n- Implementing concept from scratch\n- Visualising the output and decision boundary\n- Implementing on other datasets with visualisation","84d673cd":"## MLP Algorithm","f8f06348":"# On other datasets"}}