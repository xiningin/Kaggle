{"cell_type":{"446e75ca":"code","837d8fd4":"code","bfc117ef":"code","81d75f10":"code","8ba08653":"markdown"},"source":{"446e75ca":"import pandas as pd, numpy as np\nimport wt_text_processing_utils as wtp_utils\nimport torch\ntorch.set_grad_enabled(False)\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import TrainingArguments, Trainer, default_data_collator\ndata_collator = default_data_collator\n\nmname = \"..\/input\/hatebert-regression-single-model\/hatebert_reg_baseline\/checkpoint-1000\"\n#They are all in the same range. MSELoss wise, 250 performed better\ntokenizer = AutoTokenizer.from_pretrained(mname)","837d8fd4":"from datasets import Dataset\nkey1, key2 = \"tclean\", None\n\n#Allows truncation. So encoded ds and actual ds should share the same size. \ndef preprocess_function(examples):\n    if key2 is None:\n        return tokenizer(examples[key1], padding=True, truncation=True, max_length=512) \n    return tokenizer(examples[key1], examples[key2], padding=True, truncation=True, max_length=512) \n\ndf = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\ndf[\"tclean\"] = wtp_utils.preprocess_text(df[\"text\"])","bfc117ef":"from scipy.stats import rankdata\n\nmodel = AutoModelForSequenceClassification.from_pretrained(mname)\n\nargs = TrainingArguments(\"dummy\", report_to=[\"tensorboard\"], per_device_eval_batch_size=256)\ntrainer = Trainer(model, args, data_collator=data_collator, tokenizer=tokenizer)\n\ntest_ds = Dataset.from_pandas(df)\ntest_ds = test_ds.map(preprocess_function, batched=True)\npreds = trainer.predict(test_ds)\nranks = preds.predictions[:, 0]\n\ndf[\"score\"] = rankdata(ranks, method='ordinal')","81d75f10":"df[[\"comment_id\", \"score\"]].to_csv(\"submission.csv\", index=False)","8ba08653":"\n1. V1: Baseline single model (Fully uses HF Framework)\n2. V2: Baseline ranker model (HF models integrated to FastAI)\n3. V3: Same approach as V1. More reliable training data."}}