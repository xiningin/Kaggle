{"cell_type":{"a8ebee97":"code","3bdd62ea":"code","769658f1":"code","6b4db46e":"code","d2cc048d":"code","6178274b":"code","00857033":"code","a5486cbe":"code","73e90f57":"code","4a15d765":"code","0459c7aa":"code","1720082f":"code","fa1ae910":"code","938dee26":"code","f65e108b":"code","29c18ba4":"code","e3876432":"code","aba2aa6e":"code","8c0c51b3":"markdown","5478dc77":"markdown","73938d08":"markdown","5eafae80":"markdown","c3b84465":"markdown","60faf47f":"markdown","5e482c1a":"markdown","d6ee7b06":"markdown","17aa931a":"markdown","28b2d2b3":"markdown","e862ee57":"markdown","894bfc2b":"markdown","7fd1968a":"markdown","95c980cb":"markdown","f2c063c6":"markdown","1ae9b66d":"markdown","45f7859e":"markdown","05ddc7b9":"markdown","f1dddf72":"markdown","3afcc4bd":"markdown","0bcd9236":"markdown","790f1edf":"markdown","ae0fa28d":"markdown","0c734d79":"markdown","87d65997":"markdown","c76a99af":"markdown","793c9cde":"markdown","f2cb6d4b":"markdown","116c2985":"markdown","6f2575bd":"markdown"},"source":{"a8ebee97":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3bdd62ea":"import pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\"fivethirtyeight\")\nplt.style.use('dark_background')\nimport numpy as np\nimport xgboost as xgb\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.model_selection import GridSearchCV\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","769658f1":"data=pd.read_csv(\"\/kaggle\/input\/boston-housing-dataset\/HousingData.csv\")\ndisplay(data.head())\ndisplay(data.columns)","6b4db46e":"display(data.describe())\ndisplay(data.info())","d2cc048d":"#Fill null values with average of each column\ndata[\"CRIM\"].fillna(data[\"CRIM\"].mean(),inplace=True)\ndata[\"ZN\"].fillna(data[\"ZN\"].mean(),inplace=True)\ndata[\"INDUS\"].fillna(data[\"INDUS\"].mean(),inplace=True)\ndata[\"CHAS\"].fillna(data[\"CHAS\"].mean(),inplace=True)\ndata[\"AGE\"].fillna(data[\"AGE\"].mean(),inplace=True)\ndata[\"LSTAT\"].fillna(data[\"LSTAT\"].mean(),inplace=True)\n\ndata.info()","6178274b":"plt.figure(figsize=(22,8))\nplt.scatter(data.index,data[\"MEDV\"],color=\"red\",lw=3)\nplt.plot(data.index,data[\"MEDV\"])\nplt.title(\"Median value of homes (in $1000)\")\nplt.grid(True)\nplt.show()","00857033":"fig, ax1 = plt.subplots(7,2, figsize=(20,25))\nk = 0\ncolumns = list(data.columns)\nfor i in range(7):\n    for j in range(2):\n            sns.distplot(data[columns[k]], ax = ax1[i][j], color = 'green')\n            ax1[i][j].grid(True)\n            k += 1\nplt.show()","a5486cbe":"def log_transform(col):\n    return np.log(col[0])\n\ndata[\"DIS\"]=data[[\"DIS\"]].apply(log_transform, axis=1)\n#Plot\nsns.distplot(data[\"DIS\"], color = 'green')\nplt.grid(True)\nplt.show()","73e90f57":"plt.figure(figsize=(14,6))\ncorr=abs(data.corr())\nsns.heatmap(corr,annot=True,linewidth=1,cmap=\"Blues\")\nplt.show()\n\nplt.figure(figsize=(10,6))\nplt.plot(corr[\"MEDV\"].sort_values(ascending=False)[1:],label=\"Correlation\",color=\"red\")\nplt.ylabel(\"Correlation\")\nplt.xlabel(\"Feature\")\nplt.legend()\nplt.tight_layout()\nplt.grid(True)\nplt.show()","4a15d765":"fig, ax1 = plt.subplots(3,2, figsize=(20,15))\nk = 0\ncolumns = [\"LSTAT\",\"RM\",\"PTRATIO\",\"INDUS\",\"NOX\",\"AGE\"]\nfor i in range(3):\n    for j in range(2):\n            sns.regplot(data[columns[k]],data[\"MEDV\"],ax=ax1[i][j],color=\"green\")\n            ax1[i][j].grid(True)\n            k += 1\nplt.show()","0459c7aa":"fig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor k,v in data.items():\n    sns.boxplot(y=k, data=data, ax=axs[index],color=\"yellow\")\n    index += 1\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)\nplt.show()\n\nfor k, v in data.items():\n        q1 = v.quantile(0.25)\n        q3 = v.quantile(0.75)\n        irq = q3 - q1\n        v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]\n        perc = np.shape(v_col)[0] * 100.0 \/ np.shape(data)[0]\n        print(\"Column %s outliers = %.2f%%\" % (k, perc))","1720082f":"X=data.iloc[:,0:13]\nY=data.iloc[:,13]\n\nprint(\"Unscaled Data: \\n\")\ndisplay(X)  #Unscaled data\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nprint(\"Scaled Data: \\n\")\ndisplay(X)  #Scaled input data","fa1ae910":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2,random_state=94)","938dee26":"xgbr = xgb.XGBRegressor(objective='reg:squarederror')  #Our XGBoost model\nxgbr.fit(X_train,Y_train)\n\n#Generate predicted values\nY_pred = xgbr.predict(X_test)\n\n#Calculate and print the RMSE and the accuracy of our model.\nmse=mean_squared_error(Y_test, Y_pred)\nscore=r2_score(Y_test,Y_pred)\nprint(\"Root Mean Square Error: %.2f\" % (mse**(0.5)))\nprint(\"Accuracy: {} %\".format(round((score*100),2)))","f65e108b":"xgbr = xgb.XGBRegressor(objective='reg:squarederror',\n                        random_state=50,\n                        max_depth=6,\n                        learning_rate = 0.08,\n                        n_estimators = 500,\n                        colsample_bylevel = 0.4,\n                        reg_alpha = 1,\n                        subsample = 1,\n                       )\n\nxgbr.fit(X_train,Y_train)\nY_pred = xgbr.predict(X_test)\nmse=mean_squared_error(Y_test, Y_pred)\nscore=r2_score(Y_test,Y_pred)\nprint(\"Root Mean Square Error: %.2f\" % (mse**(0.5)))\nprint(\"Accuracy: {} %\".format(round((score*100),2)))","29c18ba4":"params = { 'max_depth': [6,10],\n           'learning_rate': [0.01, 0.08],\n           'colsample_bylevel': [0.3, 0.4],\n         }\n\nxgbr = xgb.XGBRegressor(seed=50)\nclf = GridSearchCV(estimator=xgbr, \n                   param_grid=params,\n                   scoring='neg_mean_squared_error', \n                   verbose=1)\nclf.fit(X_train, Y_train)\n\nprint(\"Best parameters:\", clf.best_params_)\nprint(\"Lowest RMSE: \", (-clf.best_score_)**(0.5))","e3876432":"mse_dict={}  #Root mean square dictionary\nacc_dict={}  #Accuracy dictionary\nfor n in range(100):\n    xgbr = xgb.XGBRegressor(objective='reg:squarederror',\n                            random_state=n,\n                            max_depth=6,\n                            learning_rate = 0.08,\n                            n_estimators = 500,\n                            colsample_bylevel = 0.4,\n                            reg_alpha = 1\n                           )\n    xgbr.fit(X_train, Y_train)\n    Y_pred = xgbr.predict(X_test)\n    mse=mean_squared_error(Y_test, Y_pred)\n    score=r2_score(Y_test, Y_pred)\n    mse_dict.update({n:mse**(1\/2.0)})\n    acc_dict.update({n:round((score*100),2)})","aba2aa6e":"#Mean Square Error\nlowest=min(mse_dict.values())\nres = [key for key in mse_dict if mse_dict[key] == lowest]\nmse_list=mse_dict.items()\nk,v = zip(*mse_list) \nprint(\"RMSE is lowest at {} for random state {} \".format(round((lowest),3),res))\n#Plot RMSE values\nplt.figure(figsize=(12,6))\nplt.plot(k,v)\nplt.scatter(res,lowest,color=\"red\",lw=5)\nplt.xlabel(\"Random State\")\nplt.ylabel(\"RMSE\")\nplt.grid(True)\nplt.show()\n\n#Accuracy\nhighest=max(acc_dict.values())\nres1= [key for key in acc_dict if acc_dict[key] == highest]\nacc_list=acc_dict.items()\nk1,v1=zip(*acc_list)\nprint(\"Accuracy is highest at {} % for random state {} \".format(highest,res1))\n#Plot Accuracy values\nplt.figure(figsize=(12,6))\nplt.plot(k1,v1)\nplt.scatter(res1,highest,color=\"red\",lw=5)\nplt.xlabel(\"Random State\")\nplt.ylabel(\"Accuracy\")\nplt.grid(True)\nplt.show()\n\n#Plot accuracy vs RMSE\nplt.scatter(v1,v)\nplt.xlabel(\"RMSE\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy vs RMSE\")\nplt.xlim(92,95)\nplt.grid(True)\nplt.show()","8c0c51b3":"Almost all the features have many outliners and hence we will need to take care of them. Its simple, we will scale the data before feeding it to our model.\n\n**What is feature scaling ?**\n\nFeature Scaling is a technique to standardize the independent features present in the data in a fixed range. It is performed during the data pre-processing to handle highly varying magnitudes or values or units. If feature scaling is not done, then a machine learning algorithm tends to weigh greater values, higher and consider smaller values as the lower values, regardless of the unit of the values.\n\nWe will use the standardization technique to perform feature scaling,\n\n**Standardization:** It is a very effective technique which re-scales a feature value so that it has distribution with 0 mean value and variance equals to 1. The tool we will use is StandardScaler from the preprocessing library in Scikitlearn. (Check out the library imported at the top)","5478dc77":"**Split and scale the data:**\n\n**X** comprises of all the input features we will use to train the model, while **Y** comprises of the output feature (Median Value of Boston Houses) ","73938d08":"As the graph above for RMSE and Accuracy depicts, the best state to use for random_state is **50**. Hence, i have used random_state = 50 in our model above.\n\n> **NOTE:** The same can be done to find the best number of random_state while using ***train_test_split*** function too. I did that and found the best value to be **94** and hence have used it while splitting the data before building our model.","5eafae80":"As you noticed, there are null values present in many columns, namely **CRIM, ZN, INDUS, CHAS, AGE, and LSTAT.**\nSo we will have to fill these null values with some new values, in our case we will use the mean value of each column as a replacement.","c3b84465":"**With hyperparameter tuning**","60faf47f":"# **Conclusion**\n\nWe observed how XGBoost operates to better understand how to tune its hyperparameters. As we\u2019ve seen, tuning usually results in a big improvement in model performances.\n\nUsing our intuition to tune our model might sometimes be enough. It is also worth trying Optimization Algorithms like GridSearch and RandomSearch. But most of the time, you\u2019ll get an even better result with a mix of Algorithms and adjustments through testing and intuition!","5e482c1a":"# **Data Preprocessing:**\nLet's explore the data to see if we need to make any changes before we use it our super powerful XGBoost model.","d6ee7b06":"# **XGBoost Model \ud83e\uddbe\ud83e\uddbe**\n**XGBoost (eXtreme Gradient Boosting)** is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. When it comes to small-to-medium structured\/tabular data, decision tree based algorithms are considered best-in-class right now.\nInitially started as a research project in 2014, XGBoost has quickly become one of the most popular Machine Learning algorithms of the past few years. \n\nIt is not only an algorithm. It\u2019s an entire open-source library, designed as an optimized implementation of the Gradient Boosting framework. It focuses on speed, flexibility, and model performances. Its strength doesn\u2019t only come from the algorithm, but also from all the underlying system optimization.\n\n**What is Boosting?**\n\nBoosting is just a method that uses the principle of ensemble learning, but in sequential order. It a process that combines decisions from multiple underlying models, and uses a voting technique to determine the final prediction. Boosting is a type of ensemble learning that uses the previous model's result as an input to the next one. Instead of training models separately, boosting trains models sequentially, each new model being trained to correct the errors of the previous ones. At each iteration (round), the outcomes predicted correctly are given a lower weight, and the ones wrongly predicted a higher weight. It then uses a weighted average to produce a final outcome.\n\n**What is Gradient Boosting?**\n\nFinally, Gradient Boosting is a boosting method where errors are minimized using a gradient descent algorithm. Simply put, Gradient descent is an iterative optimization algorithm used to minimize a loss function.\nThe loss function quantifies how far off our prediction is from the actual result for a given data point. The better the predictions, the lower will be the output of your loss function.\n\nTo put into perspective how powerful XGBoost algorithm is, it is an **optimized implementation** of this Gradient Boosting method!\n\n**As demonstrated in the chart below, XGBoost model has the best combination of prediction performance and processing time compared to other algorithms. No wonder XGBoost is widely used in recent Data Science competitions.**\n\n![image.png](attachment:cbe7fc6d-be29-4ddb-8893-0d8363ac0297.png)","17aa931a":"**Now the very main part, as we have prepocessed the data to some extent and also understood a little about XGBoost, we will build our XGBoost model.**\n\nWe will split the data into training and testing parts using **train_test_split by scikit-learn** and use it in our model.\n\n","28b2d2b3":"**Next, we will observe the frequency plot for all the features and notice if we need to improve them.**","e862ee57":"# **Optimization Algorithms**\n\nAt first, we used our intuition to choose the best hyperparameters for our model, a second approach is through Optimization Algorithms.\nThe two most common are **Grid Search** and **Random Search.**\n\n**1. Grid Search:** Grid Search is an exhaustive search over every combination of specified parameter values. If you specify 2 possible values for max_depth and 3 for n_estimators, Grid Search will iterate over 6 possible combinations.\n\n**2. Random Search:** A Random Search uses a large (possibly infinite) range of hyperparameters values, and randomly iterates a specified number of times over combinations of those values. Contrary to a Grid Search which iterates over every possible combination, with a Random Search you specify the number of iterations.\n","894bfc2b":"**We got values with a RMSE of 2.77 and accuracy 91.41%**\n\nThere's nothing wrong with this model and it's good for a beginner, but we can further improve the accuracy by mere fine-tuning of the hyperparameters.\n\nA hyperparameter is a type of parameter, external to the model, set before the learning process begins. It\u2019s tunable and can directly affect how well a model performs. Note that there are several parameters you can tune when working with XGBoost. \n\nOur main aim is to reduce RMSE as much as  we can while increasing the accuracy of the model.\n\n**The most common parameters are:**\n\n1. **max_depth**: The maximum depth per tree. A deeper tree might increase the performance, but also the complexity and chances to overfit. The value must be an integer greater than 0.\n\n2. **learning_rate**: The learning rate determines the step size at each iteration while your model optimizes toward its objective. A low learning rate makes computation slower, and requires more rounds to achieve the same reduction in residual error as a model with a high learning rate. But it optimizes the chances to reach the best optimum. The value must be between 0 and 1.\n\n3. **n_estimators**: The number of trees in our ensemble. Equivalent to the number of boosting rounds. The value must be an integer greater than 0.\n\n4. **colsample_bylevel**: Represents the fraction of columns to be randomly sampled for each tree. It might improve overfitting. The value must be between 0 and 1.\n\n5. **subsample**: Represents the fraction of observations to be sampled for each tree. A lower values prevent overfitting but might lead to under-fitting. The value must be between 0 and 1.\n\n6. **reg_alpha**: L1 regularization on the weights (Lasso Regression). When working with a large number of features, it might improve speed performances. It can be any integer","7fd1968a":"We will first visualize and observe the potential output variable for our model,  **MEDV: (Median Value in $1000)**","95c980cb":"**If you found my work useful, please do upvote!**\n\n**If you have any suggestions or doubts, feel free to comment down below!**\n\n# THANK YOU!","f2c063c6":"As we can see, the values are vastly distributed, there is no real pattern to these values. Most of them lie between the range $(20000-25000) \n\nThe maximum value of houses is $50000 while the minimum is somewhere around 5000.","1ae9b66d":"Each record in the database describes a Boston suburb or town. The data was drawn from the Boston Standard Metropolitan Statistical Area (SMSA) in 1970. The attributes are de\ufb01ned as follows (taken from the UCI Machine Learning Repository):\n\n**CRIM**: per capita crime rate by town\n\n**ZN**: proportion of residential land zoned for lots over 25,000 sq.ft.\n\n**INDUS**: proportion of non-retail business acres per town\n\n**CHAS**: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n\n**NOX**: nitric oxides concentration (parts per 10 million)\n\n**RM**: average number of rooms per dwelling\n\n**AGE**: proportion of owner-occupied units built prior to 1940\n\n**DIS**: weighted distances to \ufb01ve Boston employment centers\n\n**RAD**: index of accessibility to radial highways\n\n**TAX**: full-value property-tax rate per $10,000.\n\n**PTRATIO**: pupil-teacher ratio by town 12. B: 1000(Bk\u22120.63)2 where Bk is the proportion of blacks by town \n\n**LSTAT**: Lower status of the population.\n\n**MEDV**: Median value of owner-occupied homes in $1000s\n\n**The Boston housing dataset contains 506 observations and 14 variables.**","45f7859e":"# **EDA**","05ddc7b9":"# What do these columns mean?\n","f1dddf72":"As we can see, features **LSTAT (0.72), RM(0.7), PTRATIO (0.51), INDUS (0.48), and TAX (0.47)** have the highest correlation with the median value of houses.\n\nWe will now plot the regression plots for these features using the seaborn visualization library to observe and understand if these features have a positive or negative correlation with the output variable.","3afcc4bd":"> **NOTE: The reasonable values to start off would be,**\n> \n> max_depth: 3\u201310 \n> \n> n_estimators: 100 (lots of observations) to 1000 (few observations)\n> \n> \n> learning_rate: 0.01\u20130.3\n> \n> \n> colsample_bylevel: 0.3\u20131\n> \n> \n> subsample: 0.6\u20131\n\nInitially, you can focus on optimizing **max_depth** and **n_estimators**.\nYou can then play along with the **learning_rate**, and increase it to speed up the model without decreasing the performances. If it becomes faster without losing in performances, you can increase the number of **n_estimators** to try to increase the performances.","0bcd9236":"# **BONUS SECTION**\n\nChoosing an appropriate ***random_state*** also plays a big role in the working of any model.\nRandom state ensures that the splits that you generate are reproducible. Scikit-learn uses random permutations to generate the splits. (For example in ***train_test_split***) The random state that you provide is used as a seed to the random number generator. This ensures that the random numbers are generated in the same order. \n\n> If we don't provide any value for random_state, our model will still work, but it may give a different accuracy based on the random state selected by the compiler.\n\n**Hence, i have a created a loop to find the most optimum value of random_state based on value of RMSE generated and the accuracy of the model.**\n\nIt will run the XGBoost model 100 times for random_state value 0-99 and find the best state to get the highest accuracy.","790f1edf":"Great, now we have no null values in our dataset. We can go ahead with the further processing.","ae0fa28d":"I will demonstrate a simple example of Grid search below using the model we built=>","0c734d79":"**Read Housing Data**","87d65997":"First off, we will build a basic XGBoost model without tuning any of it's hyperparameters as shown below.\n\n**Without hyperparameter tuning**","c76a99af":"**So as you can see, the model tested all the different combinations of parameters we mentioned, and gave us the best parameters for our model.**\n\n> I used only a few combinations because GridSearchCV takes a lot of time to process (obviously) because it runs the model separately for each combination. Using more number of combinations will naturally increase the accuracy of the model but it will take A LOT of time to process.","793c9cde":"Clearly, most of the features are okay as far as building a model is concerned, only issue is the **DIS** feature which is very much skewed to the left. We will need to do something about that.\n\nAll we need to do is take a log transform of the columns and use it as shown below.","f2cb6d4b":"So clearly, only **RM** has a positive correlation with MEDV whereas the other features have a negative correlation.\n\nNext is the box plot for each feature to notice if it has outliners, and them getting rid of them.","116c2985":"**As we can see, only by little efforts and playing around with the XGBoost hyperparameters, we were able to increase the accurcy of the model by 3.1%.**","6f2575bd":"Great, this is done too! Now the columns looks much better.\n\n**Next step is very important, we will plot the absolute correlation of each input feature with the output feature (MEDV)**"}}