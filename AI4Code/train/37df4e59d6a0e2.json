{"cell_type":{"99eb475a":"code","8b88eb9f":"code","49fb1dd7":"code","6c693428":"code","e0fd2ef6":"code","5e1a6995":"code","537a8a2d":"code","cdd79e83":"code","bdbf6f28":"code","485a65ed":"code","6798ff60":"code","32522ad7":"code","0b743662":"code","24d60ca4":"code","02a9ad2e":"code","ab5118c7":"code","1cad5494":"code","c50a926a":"code","2501c2f7":"code","af5a6e2d":"code","3b358a3a":"code","d444f1fe":"code","a92a618b":"code","0f4b57c9":"code","1d4ec859":"code","e8c49e40":"code","f2199582":"code","9e457dce":"code","bb7caaa6":"markdown","7df2b09c":"markdown","4bcbefc2":"markdown","d8d552d6":"markdown","8ac2feb4":"markdown","e0513dea":"markdown","4f34c2f7":"markdown","a2f3a7ed":"markdown","8e5d39fa":"markdown"},"source":{"99eb475a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, PolynomialFeatures\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_absolute_error, r2_score\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer","8b88eb9f":"# load the data\npath='..\/input\/regression\/Ames_Housing_Sales.csv'\nhousing_data=pd.read_csv(path)\nhousing_data.head()","49fb1dd7":"#view the basic info of the data\nhousing_data.info()","6c693428":"#get the idea about the outliers and whether it needs scalling or not\nhousing_data.describe()","e0fd2ef6":"mask = housing_data.dtypes == np.object","5e1a6995":"#separate categorical and numerical columns\ncategorical_col = housing_data.dtypes[mask].index\nnumerical_col = housing_data.dtypes[~mask].index","537a8a2d":"# create a copy fo data (good practice this way you don't loose original data)\nhousing_data_copy=housing_data.copy()","cdd79e83":"def scatterplot(data,x,y):\n    \"\"\"\n        function takes \n        data : Dataframe\n        x : x-axis\n        y : y_axis\n        \n        Returns\n        ScatterPlot between x and y\n    \"\"\"\n    sns.scatterplot(data=housing_data_copy,x=x,y=y)\n    plt.title(\"Plot Between \"+x+\" And Sales Price\")\n    return plt.show()","bdbf6f28":"for col in numerical_col:\n    scatterplot(housing_data_copy,col,'SalePrice')","485a65ed":"#drop unwanted column\ncolumns_to_drop=['3SsnPorch','BsmtFinSF2','EnclosedPorch','LotFrontage','LowQualFinSF','MSSubClass','MiscVal','MoSold','SalePrice']","6798ff60":"#redefine numerical column\nnumerical_col = list((set(numerical_col)-set(columns_to_drop)))","32522ad7":"def countplot(data,x):\n    \"\"\"\n        function takes \n        data : Dataframe\n        x : x-axis\n               \n        Returns\n        Countplot of x\n    \"\"\"\n    sns.countplot(data=housing_data_copy,x=x)\n    plt.title('Countplot for '+x)\n    plt.xticks(rotation=90)\n    \ndef boxplot(data,x,y):\n    \"\"\"\n        function takes \n        data : Dataframe\n        x : x-axis\n        y : y_axis\n        \n        Returns\n        Boxplot between x and y\n    \"\"\"\n    sns.boxplot(data=housing_data_copy,x=x,y=y)\n    plt.title('Boxplot for '+x)\n    plt.xticks(rotation=90)","0b743662":"#group the data of categorical columns\n#if any category has less than 100 observations it will combine it under the name Others\n\nfor col in categorical_col:\n    grouped = housing_data_copy.groupby(col)[col].count()\n    for value in grouped.index:\n        if grouped[value]<100:\n            housing_data_copy[col].replace(value,'Others',inplace=True)","24d60ca4":"for col in categorical_col:\n    plt.figure(figsize=(13,6))\n    plt.subplot(1,2,1)\n    boxplot(housing_data_copy,col,'SalePrice')\n    plt.subplot(1,2,2)\n    countplot(housing_data_copy,col)","02a9ad2e":"#Now we drop the columns we don't require\ncolumns_to_drop.extend(['BsmtFinType2','Condition2','Electrical','Fence','Functional','MiscFeature','PavedDrive','Street','Utilities','Exterior2nd','Exterior1st','CentralAir','Heating'])","ab5118c7":"#redefine numerical column\ncategorical_col=list(set(categorical_col)-set(columns_to_drop))","1cad5494":"#Now for Categorical column we should separate out the column for label Encoding and One-Hot Encoding. Because we want to work differently on both.\ncolumn_for_label_encoding=['BsmtCond','BsmtExposure','BsmtQual','Condition1','ExterCond','ExterQual','FireplaceQu','GarageQual','HeatingQC','KitchenQual','GarageCond']\ncolumn_for_hot_encoding=list(set(categorical_col)-set(column_for_label_encoding))","c50a926a":"#dataframe to store out model results\nScores=pd.DataFrame(columns=['Model','Polynomial_degree','Alpha_value','Train_score(MAE)','Test_score(MAE)','R2_score(test_data)'])","2501c2f7":"# use to labal data if they are in series ('Good',  'Bad', 'Worst)\noe=OrdinalEncoder(handle_unknown='use_encoded_value',unknown_value=10)\n\n# use to hot encode the categorical data \nohe=OneHotEncoder(handle_unknown='ignore')\n\n# It is used to scale the numerical data \nss=StandardScaler()","af5a6e2d":"# Column Transformer will help use to word differently on different columns within a dataframe\ntransformer = ColumnTransformer(transformers=[('num',ss,numerical_col),\n                                           ('ordinal',oe,column_for_label_encoding),\n                                           ('hotencode',ohe,column_for_hot_encoding)])","3b358a3a":"# It will create folds with in our data frame so we can use them to Validate our model performance over all the present data\nfolds=KFold(n_splits=3,shuffle=True,random_state=10)","d444f1fe":"total_column=categorical_col+numerical_col","a92a618b":"#Seprate out our Features and target variable\nX=housing_data_copy[total_column]\ny=housing_data_copy.SalePrice","0f4b57c9":"for deg in [1,2,3]:\n    #adding polynomial expression to data\n    pf=PolynomialFeatures(degree=deg)\n\n    #model creation\n    LR=LinearRegression()\n\n    #creation of preprocessor  and pipeline\n    preprocessor=Pipeline(steps=[('transform',transformer),\n                                 ('polynomial',pf)])\n    model_pipeline=Pipeline(steps=[('preprocess',preprocessor),\n                              ('model',LR)])\n\n    #to collect prediction scores\n    train_score=[]\n    test_score=[]\n    r2_scores=[]\n\n    #iterating over various folds\n    for train_index,test_index in folds.split(X):\n        #train and test data split\n        train_X, train_y = X.iloc[train_index], y.iloc[train_index]\n        test_X, test_y = X.iloc[test_index], y.iloc[test_index]\n\n        #fit data to model\n        model_pipeline.fit(train_X,train_y)\n\n        #train and test prediction\n        pred_test=model_pipeline.predict(test_X)\n        pred_train=model_pipeline.predict(train_X)\n\n        #appending the prediction score in terms of MEAN ABSOLUTE ERROR\n        train_score.append(mean_absolute_error(train_y,pred_train))\n        test_score.append(mean_absolute_error(test_y,pred_test))\n        r2_scores.append(r2_score(test_y,pred_test))\n        \n    Scores=Scores.append(\n        {'Model':'Linear_regression',\n         'Polynomial_degree':deg,\n         'Alpha_value':np.nan,\n         'Train_score(MAE)': np.mean(train_score),\n         'Test_score(MAE)': np.mean(test_score),\n         'R2_score(test_data)': np.mean(r2_scores)},\n        ignore_index=True)","1d4ec859":"#alpha values for Lasso and Ridge regression models\nalphas=[10,0.1,0.001,1e-5,1e-9]","e8c49e40":"for alpha_val in alphas:\n    for deg in [1,2]:\n        pf=PolynomialFeatures(degree=deg)\n        lasso=Lasso(alpha=alpha_val,max_iter=100000,)\n        preprocessor=Pipeline(steps=[('transform',transformer),\n                                     ('polynomial',pf)])\n        model_pipeline=Pipeline(steps=[('preprocess',preprocessor),\n                                       ('linear_model',lasso)])\n\n        #to collect prediction scores\n        train_score=[]\n        test_score=[]\n        r2_scores=[]\n        #iterating over various folds\n        for train_index,test_index in folds.split(X):\n            #train and test data split\n            train_X, train_y = X.iloc[train_index], y.iloc[train_index]\n            test_X, test_y = X.iloc[test_index], y.iloc[test_index]\n            \n            #fit data to model\n            model_pipeline.fit(train_X,train_y)\n            \n            #train and test prediction\n            pred_test=model_pipeline.predict(test_X)\n            pred_train=model_pipeline.predict(train_X)\n            \n            #appending the prediction score in terms of MEAN ABSOLUTE ERROR\n            train_score.append(mean_absolute_error(train_y,pred_train))\n            test_score.append(mean_absolute_error(test_y,pred_test))\n            r2_scores.append(r2_score(test_y,pred_test))\n\n        Scores=Scores.append(\n            {'Model':'Lasso',\n             'Polynomial_degree':deg,\n             'Alpha_value':alpha_val,\n             'Train_score(MAE)': np.mean(train_score),\n             'Test_score(MAE)': np.mean(test_score)},\n            ignore_index=True)","f2199582":"for alpha_val in alphas:\n    for deg in [1,2,3]:\n        #adding polynomial expression to data\n        pf=PolynomialFeatures(degree=deg)\n        \n        #model creation\n        ridge=Ridge(alpha=alpha_val)\n        \n        #creation of preprocessor  and pipeline\n        preprocessor=Pipeline(steps=[('transform',transformer),\n                                     ('polynomial',pf)])\n        model_pipeline=Pipeline(steps=[('preprocess',preprocessor),\n                                  ('model',ridge)])\n        \n        #to collect prediction scores\n        train_score = []\n        test_score = []\n        r2_scores = []\n        \n        #iterating over various folds\n        for train_index,test_index in folds.split(X):\n            #train and test data split\n            train_X, train_y = X.iloc[train_index], y.iloc[train_index]\n            test_X, test_y = X.iloc[test_index], y.iloc[test_index]\n            \n            #fit data to model\n            model_pipeline.fit(train_X,train_y)\n            \n            #train and test prediction\n            pred_test=model_pipeline.predict(test_X)\n            pred_train=model_pipeline.predict(train_X)\n            \n            #appending the prediction score in terms of MEAN ABSOLUTE ERROR\n            train_score.append(mean_absolute_error(train_y,pred_train))\n            test_score.append(mean_absolute_error(test_y,pred_test))\n            r2_scores.append(r2_score(test_y,pred_test))\n\n        Scores=Scores.append(\n            {'Model':'Ridge',\n             'Polynomial_degree':deg,\n             'Alpha_value':alpha_val,\n             'Train_score(MAE)': np.mean(train_score),\n             'Test_score(MAE)': np.mean(test_score),\n             'R2_score(test_data)': np.mean(r2_scores)},\n            ignore_index=True)","9e457dce":"#viewing the scores of models performance and sorting it in assending order in order to get perfect model\nScores.sort_values(by=['R2_score(test_data)', 'Test_score(MAE)', 'Train_score(MAE)'], ascending=[False,True,True])","bb7caaa6":"### Lasso Regression","7df2b09c":"Here we can see Ridge model with alpha 10 and polynomial degree 1 is performing best.","4bcbefc2":"###  Linear Regression","d8d552d6":"- Analysis For Numerical columns\n    - 1stFlrSF\n        - It is so much in Linear relation with the SalePrice.\n        - As the value increase the SalePrize also increased.\n    - 1ndFlrSF\n        - It is in polynomial relation with the SalePrice.\n        - As the value increse the SalePrize increased Rapidly.\n    - 3SsnPorch\n        - Most of the Data are zero.\n        - Data which are not zero, Is too much scatter.\n        - No underlying pattern.\n        - Choose to Drop the Column.\n    - BedroomAbvGr\n        - Houses having BedroomAbvGr in between 1-4 have more SalesPrice.\n        - Houses having more or less BedroomAbvGr are less SalesPrice.\n    - BsmtFinSF1\n        - Some values are zero still have significant high SalePrice.\n        - Non-Zero Values have a rising pattern as the BsmtFinSF1 increases.\n    - BsmtFinSF2\n        - Most values are zero and still have a lot higher SalePrice.\n        - Non-Zero Values are mid-ranged SalePrice, where as zero values houses are but priced more and less.\n        - SalePrice is not dependent on BsmtFinSF2.\n        - Choose to drop the column.\n    - BsmtFullBath\n        - Houses having no or 1 BsmtFullBath are priced more than the rest.\n        - It indicate having more BsmtFullBath doesn't necessarily mean to have a higher price.\n    - BsmtHalfBath\n        - Having 0 BsmtHalfBath have more SalePrice.\n        - Values having anything more then 0 offer less maximum SalePrice but the minimum SalePrice of the house is incresed significantly.\n    - BsmtUnfSF\n        - Although the Data seems to be scattered, but examaning closely we can find that it is having polynomial relationship with SalePrice.\n        - At first the values are constant as doesn't shows any deviation, later as the values keep increasing the SalePrice tends to increase.\n    - EnclosedPorch\n        - Most of the houses have no EnclosedPorch still they have a high SalePrice.\n        - While those houses having a fireplace shows no change in SalePrice.\n        - Choose to drop the column.\n    - Fireplaces\n        - Clearly visible as the number of fireplace increases the SalePrice of the house increase.\n        - However the after a certain point it Started being constant and then started decreasing.\n    - FullBath\n        - It can be seen that the variable have a polynomial relationship with SalePrice.\n        - As the values increase the SalePrice increases.\n    - GarageArea\n        - We can see there is gradual increase in SalePrice as the values increase.\n        - And for extreme high values they decrease.\n    - GarageCars\n        - It can be seen that the variable have a polynomial relationship with SalePrice.\n        - Minimum price of SalePrice of the houses increase with increase in values.\n    - GarageYrBlt\n        - Newly built are more prefered by the consumers and are more valuable than other.\n        - And have more SalePrice values thean the older ones.\n    - GrLivArea\n        - It can be seen that the variable have a polynomial relationship with SalePrice.\n        - SalePrice is in increaing pattern with the value.\n    - HalfBath\n        - Having 1 HalfBath have more SalePrice.\n        - As number of Halfbath increases minimum SalePrice of the house also increases.\n    - KitchenAbvGr\n        - Haivng 1 KitchenAbvGr tends to have higher SalePrice.\n        - However as number of KitchenAbvGr increases minimum SalePrice of the house also increases.\n    - LotArea\n        - Haivng a decent LotArea offers a high SalePrice.\n        - Having too large or too short lot area offers low SalePrice.\n    - LotFrontage\n        - Data is concentrated to one point.\n        - It doesn't affect the SalePrice much.\n        - Choose to drop the column.\n    - LowQualFinSF\n        - Most of the data points are zero.\n        - non-zero data points doesn't have any underlying pattern.\n        - Choose to drop the column.\n    - MSSubClass\n        - The data points have too much uncertainity.\n        - For low value of MSSubClass SalePrice if too low and started increasing, then falls down and started increasing again aftere some time.\n        - the reason can be that it is not contributing towards the SalePrice of the houses.\n        - Choose to drop the column.\n    - MasVnrArea\n        - Although the data points are zero.\n        - but we can observe that the SalePrice is in increasing order with respect to MasVnrArea.\n    - MiscVal\n        - Many values are zero still the manage to get a higher SalePrice.\n        - Clearly mean that this feature is not contributing towards the SalePrice.\n        - Choose to drop the column.\n    - MoSold\n        - We can se the graph is constant for each case.\n        - We cant interpret any pattern from the visulization.\n        - Choose to drop the column\n    - OpenPorchSF\n        - Most of the values are zero.\n        - for non zero values as the price increases the minimum SalePrice also increases.\n    - OverallCond\n        - The better the condition of the house the higher SalePrice it will be.\n        - Also the minimum SalePrice of the house is also increased.\n    - OverallQual\n        - The Quality of house increases it increase teh SalePrice to a large extent.\n        - It can be an important feature in the dataset\n    - PoolArea\n        - 98% of the values are zero and still have high SalePrice.\n        - This is clear it is not an important predictor.\n    - ScreenPorch\n        - Most values are zero.\n        - For Non-zero values as the value of ScreenPorch increase minimum SalePrice of the house also increases.\n    - TotRmsAbvGrd\n        - It can be seen that the variable have a polynomial relationship with SalePrice.\n        - Also the minimum SalePrice of the house is also increased.\n    - TotalBsmtSF\n        - Increase in TotalBsmtSF will result in increase in SalePrice.\n        - It is due to the polynomial relationship exist between them.\n    - WoodDeckSF\n        - Having WoodDeckSF will definitely increase the SalePrice \n        - Also the minimum SalePrice of the house is also increased.\n    - YearBuilt\n        - Latest bulit houses are tend to sell at higher SalePrice.\n        - Also there are high number of new built houses.\n    - YearRemodAdd\n        - Houses with range in 1980 to 2000 are tend to have high minimum SalePrice.\n        - while other for others we can see the increasing pattern in SalePrice\n    - YrSold\n        - We can observe that it is constant throughout.\n        - Conclusively we can say it doesn't contribute in SalePrice.","8ac2feb4":"## Model Accuracy","e0513dea":"##  Model Creation","4f34c2f7":"## Feature Engineering","a2f3a7ed":"So by this we can choose the model with params which fits out model in best way.","8e5d39fa":"### Ridge Regression"}}