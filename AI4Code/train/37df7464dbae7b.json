{"cell_type":{"e4efa61b":"code","39b59833":"code","027d9b89":"code","444f92bd":"code","31d9b4c4":"code","a7a014b7":"code","19631c16":"code","bf55d6ff":"code","eb5f98c5":"code","d110388d":"markdown","b8d25574":"markdown","aac8074e":"markdown","e61d3f4e":"markdown","4d1ba780":"markdown","d6594aba":"markdown"},"source":{"e4efa61b":"\"\"\"Among most populations of numbers, a pattern will emerge in the first numbers. \n   Benford's Law predicts the leading number in most data sets is likely to be small.\n   The occurence of '1' as the leading number will occur about 30% of the time. \n   Each number in sequence afterwards will decrease logarithmically with '9' occuring\n   less than 5%.\n   \n   Does Benford's Law apply to the following data set?\n\"\"\"\n\nimport numpy as np \nimport pandas as pd \nfrom math import log10\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","39b59833":"data_path = '..\/input\/suicide-rates-overview-1985-to-2016\/master.csv'\nsuicide_data = pd.read_csv(data_path, index_col=None)","027d9b89":"suicide_data.head()","444f92bd":"#remove columns with missing values.\n\nmissing_cols = [col for col in suicide_data.columns if suicide_data[col].isnull().any()]\nreduced_suicide_data = suicide_data.drop(missing_cols, axis=1)\nprint(reduced_suicide_data.columns)","31d9b4c4":"#function to calculate number of suicides by country and year\n\ndef combine_data(dataframe, row_year, country):\n    country_data = pd.DataFrame(dataframe.loc[(dataframe['country'] == country) & (dataframe['year'] == row_year)])\n    return country_data['suicides_no'].sum()\n","a7a014b7":"\"\"\"\n    Trying to iterate list over function. Problem with saving result as a working dataframe.\n    \n\"\"\"\n#list of countries from data set\ncountry_list = ['Albania', 'Antigua and Barbuda', 'Argentina', 'Armenia', 'Aruba',\n       'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain',\n       'Barbados', 'Belarus', 'Belgium', 'Belize',\n       'Bosnia and Herzegovina', 'Brazil', 'Bulgaria', 'Cabo Verde',\n       'Canada', 'Chile', 'Colombia', 'Costa Rica', 'Croatia', 'Cuba',\n       'Cyprus', 'Czech Republic', 'Denmark', 'Dominica', 'Ecuador',\n       'El Salvador', 'Estonia', 'Fiji', 'Finland', 'France', 'Georgia',\n       'Germany', 'Greece', 'Grenada', 'Guatemala', 'Guyana', 'Hungary',\n       'Iceland', 'Ireland', 'Israel', 'Italy', 'Jamaica', 'Japan',\n       'Kazakhstan', 'Kiribati', 'Kuwait', 'Kyrgyzstan', 'Latvia',\n       'Lithuania', 'Luxembourg', 'Macau', 'Maldives', 'Malta',\n       'Mauritius', 'Mexico', 'Mongolia', 'Montenegro', 'Netherlands',\n       'New Zealand', 'Nicaragua', 'Norway', 'Oman', 'Panama', 'Paraguay',\n       'Philippines', 'Poland', 'Portugal', 'Puerto Rico', 'Qatar',\n       'Republic of Korea', 'Romania', 'Russian Federation',\n       'Saint Kitts and Nevis', 'Saint Lucia',\n       'Saint Vincent and Grenadines', 'San Marino', 'Serbia',\n       'Seychelles', 'Singapore', 'Slovakia', 'Slovenia', 'South Africa',\n       'Spain', 'Sri Lanka', 'Suriname', 'Sweden', 'Switzerland',\n       'Thailand', 'Trinidad and Tobago', 'Turkey', 'Turkmenistan',\n       'Ukraine', 'United Arab Emirates', 'United Kingdom',\n       'United States', 'Uruguay', 'Uzbekistan',]","19631c16":"#function for each year preprocessing\ndef create_new_data(year):\n    new_data = {(country):combine_data(reduced_suicide_data, year, country) for country in country_list}\n    new_data_year = pd.DataFrame.from_dict(new_data, orient='index', dtype=None, columns={'suicide_nums'})\n    row_zero = (new_data_year != 0).any(axis=1)\n    new_data_ = new_data_year.loc[row_zero]\n    to_string = [str(v) for v in new_data.values() if v >0]\n    for idx, string in enumerate(to_string):\n        to_string[idx] = string[0:1]\n    return sorted(to_string)\n    \n#create string data\nnew_data_1987 = create_new_data(1987)\nnew_data_1988 = create_new_data(1988)\nnew_data_1989 = create_new_data(1989)\nnew_data_1990 = create_new_data(1990)\nnew_data_1991 = create_new_data(1991)\nnew_data_1992 = create_new_data(1992)\nnew_data_1993 = create_new_data(1993)\nnew_data_1994 = create_new_data(1994)\nnew_data_1995 = create_new_data(1995)\nnew_data_1996 = create_new_data(1996)\nnew_data_1997 = create_new_data(1997)\nnew_data_1998 = create_new_data(1998)\nnew_data_1999 = create_new_data(1999)\nnew_data_2000 = create_new_data(2000)\nnew_data_2001 = create_new_data(2001)\nnew_data_2002 = create_new_data(2002)\nnew_data_2003 = create_new_data(2003)\nnew_data_2004 = create_new_data(2004)\nnew_data_2005 = create_new_data(2005)\nnew_data_2006 = create_new_data(2006)\nnew_data_2007 = create_new_data(2007)\nnew_data_2008 = create_new_data(2008)\nnew_data_2009 = create_new_data(2009)\nnew_data_2010 = create_new_data(2010)\nnew_data_2011 = create_new_data(2011)\nnew_data_2012 = create_new_data(2012)\nnew_data_2013 = create_new_data(2013)\nnew_data_2014 = create_new_data(2014)\nnew_data_2015 = create_new_data(2015)\nnew_data_2016 = create_new_data(2016)","bf55d6ff":"#convert to dict, work on creating funtion to return output as list\nto_dict_1987 = {v: new_data_1987.count(v) for v in new_data_1987}\nto_dict_1988 = {v: new_data_1988.count(v) for v in new_data_1988}\nto_dict_1989 = {v: new_data_1989.count(v) for v in new_data_1989}\nto_dict_1990 = {v: new_data_1990.count(v) for v in new_data_1990}\nto_dict_1991 = {v: new_data_1991.count(v) for v in new_data_1991}\nto_dict_1992 = {v: new_data_1992.count(v) for v in new_data_1992}\nto_dict_1993 = {v: new_data_1993.count(v) for v in new_data_1993}\nto_dict_1994 = {v: new_data_1994.count(v) for v in new_data_1994}\nto_dict_1995 = {v: new_data_1995.count(v) for v in new_data_1995}\nto_dict_1996 = {v: new_data_1996.count(v) for v in new_data_1996}\nto_dict_1997 = {v: new_data_1997.count(v) for v in new_data_1997}\nto_dict_1998 = {v: new_data_1998.count(v) for v in new_data_1998}\nto_dict_1999 = {v: new_data_1999.count(v) for v in new_data_1999}\nto_dict_2000 = {v: new_data_2000.count(v) for v in new_data_2000}\nto_dict_2001 = {v: new_data_2001.count(v) for v in new_data_2001}\nto_dict_2002 = {v: new_data_2002.count(v) for v in new_data_2002}\nto_dict_2003 = {v: new_data_2003.count(v) for v in new_data_2003}\nto_dict_2004 = {v: new_data_2004.count(v) for v in new_data_2004}\nto_dict_2005 = {v: new_data_2005.count(v) for v in new_data_2005}\nto_dict_2006 = {v: new_data_2006.count(v) for v in new_data_2006}\nto_dict_2007 = {v: new_data_2007.count(v) for v in new_data_2007}\nto_dict_2008 = {v: new_data_2008.count(v) for v in new_data_2008}\nto_dict_2009 = {v: new_data_2009.count(v) for v in new_data_2009}\nto_dict_2010 = {v: new_data_2010.count(v) for v in new_data_2010}\nto_dict_2011 = {v: new_data_2011.count(v) for v in new_data_2011}\nto_dict_2012 = {v: new_data_2012.count(v) for v in new_data_2012}\nto_dict_2013 = {v: new_data_2013.count(v) for v in new_data_2013}\nto_dict_2014 = {v: new_data_2014.count(v) for v in new_data_2014}\nto_dict_2015 = {v: new_data_2015.count(v) for v in new_data_2015}\nto_dict_2016 = {v: new_data_2016.count(v) for v in new_data_2016}","eb5f98c5":"#function to change dict values into list of percentage overall\ndef get_percent(dict):\n    temp_data = sum(dict.values())\n    return [(v \/ temp_data) * 100 for v in dict.values()]\n\npercent_1987 = get_percent(to_dict_1987)\npercent_1988 = get_percent(to_dict_1988)\npercent_1989 = get_percent(to_dict_1989)\npercent_1990 = get_percent(to_dict_1990)\npercent_1991 = get_percent(to_dict_1991)\npercent_1992 = get_percent(to_dict_1992)\npercent_1993 = get_percent(to_dict_1993)\npercent_1994 = get_percent(to_dict_1994)\npercent_1995 = get_percent(to_dict_1995)\npercent_1996 = get_percent(to_dict_1996)\npercent_1997 = get_percent(to_dict_1997)\npercent_1998 = get_percent(to_dict_1998)\npercent_1999 = get_percent(to_dict_1999)\npercent_2000 = get_percent(to_dict_2000)\npercent_2001 = get_percent(to_dict_2001)\npercent_2002 = get_percent(to_dict_2002)\npercent_2003 = get_percent(to_dict_2003)\npercent_2004 = get_percent(to_dict_2004)\npercent_2005 = get_percent(to_dict_2005)\npercent_2006 = get_percent(to_dict_2006)\npercent_2007 = get_percent(to_dict_2007)\npercent_2008 = get_percent(to_dict_2008)\npercent_2009 = get_percent(to_dict_2009)\npercent_2010 = get_percent(to_dict_2010)\npercent_2011 = get_percent(to_dict_2010)\npercent_2012 = get_percent(to_dict_2010)\npercent_2013 = get_percent(to_dict_2010)\npercent_2014 = get_percent(to_dict_2010)\npercent_2015 = get_percent(to_dict_2010)\npercent_2016 = get_percent(to_dict_2010)\n\n#add each year's percent occurence to chart\nplt.plot(percent_1987)\nplt.plot(percent_1988)\nplt.plot(percent_1989)\nplt.plot(percent_1990)\nplt.plot(percent_1991)\nplt.plot(percent_1992)\nplt.plot(percent_1993)\nplt.plot(percent_1994)\nplt.plot(percent_1995)\nplt.plot(percent_1996)\nplt.plot(percent_1997)\nplt.plot(percent_1998)\nplt.plot(percent_1999)\nplt.plot(percent_2000)\nplt.plot(percent_2001)\nplt.plot(percent_2002)\nplt.plot(percent_2003)\nplt.plot(percent_2004)\nplt.plot(percent_2005)\nplt.plot(percent_2006)\nplt.plot(percent_2007)\nplt.plot(percent_2008)\nplt.plot(percent_2009)\nplt.plot(percent_2010)\nplt.plot(percent_2011)\nplt.plot(percent_2012)\nplt.plot(percent_2013)\nplt.plot(percent_2014)\nplt.plot(percent_2015)\nplt.plot(percent_2016)\n\nplt.title('Benford\\'s Law compared to Suicide Data by Year')\n#define Benford's Law and plot\nbenford = [(log10(1+1.0\/i),str(i)) for i in range(1,10)]\nx_val = [x[0] * 100 for x in benford]\nplt.plot(x_val, label='Benford', linewidth=5, alpha=0.9)\n\n#change x-axis to account for list index 0\nplt.xticks(np.arange(len(benford)), np.arange(1, len(benford)+1))\n\n#chart info and output file\nplt.xlabel('First Digit in Suicide Numbers')\nplt.ylabel('Occurence Percentage')\nplt.legend()\nplt.savefig('benford.jpg')","d110388d":"get_percent took the occurence of each number and converted it into a percentage of the sum overall for each year. The results were then plotted and compared with Benford's Law.","b8d25574":"create_new_data function prepared the data into a final single number for comparison. The total suicide numbers were converted to text for easier manipulation as a str data type. The function also dropped all but the leading digit in keeping with the hypothesis presented under Benford's Law.","aac8074e":"While the data show a logarithmic decline in the occurence of each sequential digit, there is some significant deviation in multiple years. There does not appear to be a strict adherence to Benford's Law in this distribution of data. ","e61d3f4e":"<h1><em>Hello Kaggle Community.<\/em><\/h1>\nThis is my first attempt at answering a data analysis question on my own. With some intermediate python experience and help from the Kaggle courses and Stack Overflow I was able to put the following together. Everything I've seen a read so far leads me to believe this is a very supportive community. With that in mind, I'd love to hear any feedback and suggestions on improvement. I know there are plenty of optimizations and even data methodologies I perhaps should've used. I'd love to hear any recommendations on courses or what direction I should take my learning to improve even more. ","4d1ba780":"Removing columns with missing data was originally just a part of some light EDA. With so many rows and extraneous data columns, I felt that paring it down some would be helpful. Ultimately, I achieved what I was attempting without data frames being too useful based on my skillset.","d6594aba":"The combine_data function was essential to bringing the data together for each country and year for comparison in the final steps. Originally, the data was broken down into small subsets (age range) for each year and country. I felt the data would be more valuable to compare as a total for each country over each year."}}