{"cell_type":{"da22a5fd":"code","4149a6fb":"code","40ab6831":"code","d6a1108a":"code","387c88a0":"code","71a06d2f":"code","1ec1f579":"code","b2b32c21":"code","d2b7b8e9":"code","fdafef88":"code","1d262a75":"code","85a3b856":"code","e89f4426":"code","d6e4a1aa":"code","533efe7f":"code","74b4c99b":"code","dd0ff4ef":"code","73d98b0e":"code","21652614":"code","a4f861a8":"code","f68258b0":"code","3f1c7bce":"code","36c125cc":"code","3eb8f790":"code","c8d3ee83":"code","1833391a":"code","c6c16928":"code","77b76017":"markdown","3a29005f":"markdown"},"source":{"da22a5fd":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n","4149a6fb":"!unzip -u ..\/input\/facial-keypoints-detection\/test.zip\n!unzip -u ..\/input\/facial-keypoints-detection\/training.zip","40ab6831":"train_file = 'training.csv'\ntest_file = 'test.csv'\nlookup_file = '..\/input\/facial-keypoints-detection\/IdLookupTable.csv'\ntrain = pd.read_csv(train_file)\ntest = pd.read_csv(test_file)\nlookup = pd.read_csv(lookup_file)","d6a1108a":"train.head().T","387c88a0":"test.head()","71a06d2f":"lookup.head().T","1ec1f579":"train.isnull().any().value_counts()#\n","b2b32c21":"sns.heatmap(train.isnull(),yticklabels = False, cbar ='BuPu')\n","d2b7b8e9":"train.fillna(method = 'ffill',inplace = True)\ntrain.isnull().any().value_counts()\ntrain.shape\n","fdafef88":"def process_img(data):\n    images = []\n    for idx, sample in data.iterrows():\n        image = np.array(sample['Image'].split(' '), dtype=int)\n        image = np.reshape(image, (96,96,1))\n        images.append(image)\n    images = np.array(images)\/255.\n    return images\n\n","1d262a75":"def keypoints(data):\n    keypoint = data.drop('Image',axis = 1)\n    keypoint_features = []\n    for idx, sample_keypoints in keypoint.iterrows():\n        keypoint_features.append(sample_keypoints)\n    keypoint_features = np.array(keypoint_features, dtype = 'float')\n    return keypoint_features\n","85a3b856":"y_test = test.Image\ny_test.head()","e89f4426":"X_train = process_img(train)\ny_train = keypoints(train)\n\n","d6e4a1aa":"\nplt.imshow(process_img(train)[0].reshape(96,96),cmap='gray')\nplt.show()","533efe7f":"X_train.shape\n# 30 metrics","74b4c99b":"import tensorflow as tf\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.layers import Conv2D,Dropout,Dense,Flatten\nfrom keras.layers import Activation, Convolution2D, MaxPooling2D, BatchNormalization, Conv2D,MaxPool2D, ZeroPadding2D\nfrom keras.models import Sequential, Model\n\nmodel = Sequential([Flatten(input_shape=(96,96)),\n                         Dense(128, activation=\"relu\"),\n                         Dropout(0.1),\n                         Dense(64, activation=\"relu\"),\n                         Dense(30)\n                         ])","dd0ff4ef":"model = Sequential()\n\nmodel.add(Convolution2D(32, (3,3), padding='same', use_bias=False, input_shape=(96,96,1)))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(32, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Convolution2D(64, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(64, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(96, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\n# model.add(BatchNormalization())\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(128, (3,3),padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(256, (3,3),padding='same',use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\nmodel.add(Convolution2D(512, (3,3), padding='same', use_bias=False))\nmodel.add(LeakyReLU(alpha = 0.1))\nmodel.add(BatchNormalization())\n\n\nmodel.add(Flatten())\nmodel.add(Dense(512,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(30))\n\nmodel.compile(optimizer='adam', \n              loss='mean_squared_error',\n              metrics=['mae', 'acc'])\n\nmodel.summary()","73d98b0e":"model.fit(X_train,y_train,epochs = 40,batch_size = 256,validation_split = 0.2)\n","21652614":"\n'''try:\n    plt.plot(history.history['mae'])\n    plt.plot(history.history['val_mae'])\n    plt.title('Mean Absolute Error vs Epoch')\n    plt.ylabel('Mean Absolute Error')\n    plt.xlabel('Epochs')\n    plt.legend(['train', 'validation'], loc='upper right')\n    plt.show()\n    \n    \n    # summarize history for accuracy\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.title('Accuracy vs Epoch')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epochs')\n    plt.legend(['train', 'validation'], loc='upper left')\n    plt.show()\n    # summarize history for loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Loss vs Epoch')\n    plt.ylabel('Loss')\n    plt.xlabel('Epochs')\n    plt.legend(['train', 'validation'], loc='upper left')\n    plt.show()\nexcept:\n    print(\"One of the metrics used for plotting graphs is missing! See 'model.compile()'s `metrics` argument.\")'''\n","a4f861a8":"X_train","f68258b0":"X_train = process_img(train)\ny_test = process_img(test)\n","3f1c7bce":"preds = model.predict(y_test)\nprint(preds)","36c125cc":"def plot_sample(image, keypoint, axis, title):\n    image = image.reshape(96,96)\n    axis.imshow(image, cmap='gray')\n    axis.scatter(keypoint[0::2], keypoint[1::2], marker='x', s=20)\n    plt.title(title)","3eb8f790":"fig = plt.figure(figsize=(20,16))\nfor i in range(20):\n    axis = fig.add_subplot(4, 5, i+1, xticks=[], yticks=[])\n    plot_sample(y_test[i], preds[i], axis, \"\")\nplt.show()","c8d3ee83":"lookup.head()","1833391a":"print(preds[0][1])","c6c16928":"feature = list(lookup['FeatureName'])\nimage_ids = list(lookup['ImageId']-1)\nrow_ids = lookup['RowId']\npre_list = list(preds)\n\nfeature_list = []\nfor f in feature:\n    feature_list.append(feature.index(f))\n \nfinal_preds = []\nfor x,y in zip(image_ids, feature_list):\n    final_preds.append(pre_list[x][y])\n    \nrow_ids = pd.Series(row_ids, name = 'RowId')\nlocations = pd.Series(final_preds, name = 'Location')\nlocations = locations.clip(0.0,96.0)\nsubmission_result = pd.concat([row_ids,locations],axis = 1)\nsubmission_result.to_csv('submission.csv',index = False)\nsubmission_result","77b76017":"# **Modeling**","3a29005f":"**Fit the model!**"}}