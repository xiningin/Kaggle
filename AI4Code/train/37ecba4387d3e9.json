{"cell_type":{"b9877756":"code","c1803e68":"code","b7021cb0":"code","d250bf1f":"code","c0958402":"code","5cece3e1":"code","081f7bb4":"code","45cf17a2":"code","91318323":"code","861211b9":"code","6421ab17":"code","94e54c5f":"code","3d0a6714":"code","9d1e0ff3":"code","305d8388":"code","44be5e01":"code","3e000da0":"markdown","8c92532c":"markdown","dda712ab":"markdown","7c16e543":"markdown","3ef07a1a":"markdown"},"source":{"b9877756":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nPATH = '\/kaggle\/input\/cat-in-the-dat\/'","c1803e68":"#Reading the dataset.\ntrain_df = pd.read_csv(f'{PATH}train.csv', index_col='id')\ntest_df = pd.read_csv(f'{PATH}test.csv', index_col='id')","b7021cb0":"#exploring the train dataset.\ntrain_df.head()","d250bf1f":"#shape of the datasets.\nprint(f'Training shape: {train_df.shape}')\nprint(f'Testing shape: {test_df.shape}')","c0958402":"#first checking the categrical variables containing in the train sets are also present in the test set.\ndef checkcat(df):\n    for col in df.columns:\n        length = len(set(test_df[col].values) - set(train_df[col].values))\n        if length > 0:\n            print(f'{col} in the test set has {length} values that are not present in the train set')\ncheckcat(test_df)","5cece3e1":"#One Hot Encoding binary features.\ncols = ['bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4']\n\ntrain_df = pd.get_dummies(train_df, columns=cols)\ntest_df = pd.get_dummies(test_df, columns=cols)","081f7bb4":"#Label encoding nominal features.\nfrom sklearn.preprocessing import LabelEncoder\n\n#Label encoding everything.\n\nfor col in train_df.columns:\n    if train_df[col].dtype == 'O':\n        #initializing.\n        le = LabelEncoder()\n        le.fit(list(train_df[col].values) + list(test_df[col].values))\n        train_df[col] = le.transform(list(train_df[col].values))\n        test_df[col] = le.transform(list(test_df[col].values))","45cf17a2":"train_df.head()","91318323":"#encoding the cyclic features.\n#refrence : https:\/\/www.kaggle.com\/avanwyk\/encoding-cyclical-features-for-deep-learning\ndef encode(data, col, max_val):\n    data[col + '_sin'] = np.sin(2 * np.pi * data[col]\/max_val)\n    data[col + '_cos'] = np.cos(2 * np.pi * data[col]\/max_val)\n    return data\n\n#day\ntrain_df = encode(train_df, 'day', 6)\ntest_df = encode(test_df, 'day', 6)\n\n#month.\ntrain_df = encode(train_df, 'month', 12)\ntest_df = encode(test_df, 'month', 12)","861211b9":"#dropping the day month columns.\ntrain_df.drop(['day', 'month'], axis=1, inplace=True)\ntest_df.drop(['day', 'month'], axis=1, inplace=True)","6421ab17":"#Creating X and y variables.\nX = train_df.drop('target', axis=1)\ny = train_df.target","94e54c5f":"# #scaling the dataset.\n# from sklearn.preprocessing import StandardScaler\n\n# #initializing.\n# scale = StandardScaler()\n\n# #fit\n# X = scale.fit_transform(X)\n# test_df = scale.transform(test_df)","3d0a6714":"print('#'*20)\nprint('StratifiedKFold training...')\n\n# Same as normal kfold but we can be sure\n# that our target is perfectly distribuited\n# over folds\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nimport catboost as cg\nfrom sklearn.metrics import roc_auc_score\n\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=10)\n\n#initializing the model\nmodel = cg.CatBoostClassifier(logging_level='Silent')\n\n#score.\nscore = []\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(X, y, groups=y)):\n    print('Fold:',fold_+1)\n    tr_x, tr_y = X.iloc[trn_idx,:], y[trn_idx]    \n    vl_x, v_y = X.iloc[val_idx,:], y[val_idx]\n    \n    #fitting on training data.\n    %time model.fit(tr_x, tr_y)\n    \n    #predicting on test.\n    y_pred = model.predict(vl_x)\n    \n    #storing score\n    score.append(roc_auc_score(v_y, y_pred))\n    print(f'AUC score : {roc_auc_score(v_y, y_pred)}')\n\nprint('Average AUC score', np.mean(score))\nprint('#'*20)","9d1e0ff3":"#fitting on the entire data.\n%time model.fit(X, y)","305d8388":"#making predictions on test data.\npred_test = model.predict_proba(test_df)[:,0]","44be5e01":"#submission file.\nsub = pd.read_csv(f'{PATH}sample_submission.csv')\n# #reseting index\n# test_df = test_df.reset_index()\nsub['target'] = pred_test\nsub.to_csv('catboost_model_0.1.csv', index=None, header=True)","3e000da0":"There are values in the categorical features which are not present in the train set but are prsent in the test set and this has to be dealt with.\n\nThis means that test data contains some feature values which are not seen in the train dataset and encoding the categories of the train data will ignore those extra values present in the test data.\n\nOne way to deal with this is to combine both the train and test data before applying any encoding on them. As of now only two features shows the characteristic mentioned above, i.e nor_8 and nom_9. We will combine the dataset before encoding these two features.","8c92532c":"### Creating a base model.","dda712ab":"# Encoding the Categorical variables.","7c16e543":"> *Problem Statement:* The target\/goal of the problem is to encode the categorical variables in the most efficient way possible so as when this encoding is supplied to any algorithm, it will be able to learn the underlying information that these categorical variables holds and will be able to predict the target feature in a more accurate way.","3ef07a1a":"# Model Building."}}