{"cell_type":{"6a63e942":"code","fe5ecbd8":"code","1b7145c9":"code","82b3ce33":"code","b9c4f507":"code","78b0f2dc":"code","a03e0e9a":"code","74f4d2df":"code","284829ae":"code","288fce6c":"code","f89e71bd":"code","326e53f2":"code","0efd93e3":"code","3f4011c2":"code","e90fd0c0":"code","e1fd61c0":"code","ce1140a1":"code","7c43b2a7":"code","4dc7bb1b":"code","e96cd4c2":"code","a4345f5c":"code","5a871938":"code","92134a8c":"code","e123d563":"code","24cfb3e9":"code","6172e516":"code","b0d4d445":"code","eaa14080":"code","57a9e2c1":"code","ec4728a1":"code","7f4593e2":"code","0b1f119c":"code","b54ea378":"code","a47153ee":"code","0ed6173d":"markdown","a193af11":"markdown","f529fff4":"markdown","ccc7fc21":"markdown","3f6f4f4b":"markdown","16dd696f":"markdown","fb4ac094":"markdown","a626623e":"markdown","e92f45d4":"markdown","19968e41":"markdown","41cef97b":"markdown","2625e6bf":"markdown","ee7cca71":"markdown","b1e919ca":"markdown","6e671482":"markdown","fbd2533b":"markdown","b6024936":"markdown","c120ace9":"markdown","aa27886b":"markdown","6f8a4849":"markdown","7a890c08":"markdown","0fa878f1":"markdown","bf0650f3":"markdown","45b4e7fa":"markdown","3ee9a431":"markdown","f045a48a":"markdown","b25fc2aa":"markdown","718264b0":"markdown","8cbe9e92":"markdown","9c86f012":"markdown","58734ae0":"markdown","c3b1b008":"markdown","a12df4e6":"markdown","cc063e5a":"markdown","8d9ee649":"markdown","1e454b48":"markdown","f414c7c4":"markdown","3909ecbf":"markdown","c37322d6":"markdown","709b66d6":"markdown","97604d0b":"markdown","cf461cee":"markdown","1caae5c8":"markdown","8a60d402":"markdown","0039d393":"markdown","73f7112d":"markdown","32df621d":"markdown","6a124dd8":"markdown","02b7f0ad":"markdown","ffa62b28":"markdown","b44712c8":"markdown","c0d2a286":"markdown","0e5b17b0":"markdown","3c80ca70":"markdown","ddfc6330":"markdown","e53a59df":"markdown"},"source":{"6a63e942":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fe5ecbd8":"#Importing necessary libraries\nfrom mlxtend.plotting import plot_decision_regions\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Apply the default default seaborn theme, scaling, and color palette.\nsns.set() \nimport warnings\nwarnings.filterwarnings('ignore')\n#With this backend, the output of plotting commands is displayed inline within frontends like the Jupyter notebook, directly below the code cell that produced it. \n#The resulting plots will then also be stored in the notebook document.\n%matplotlib inline\n","1b7145c9":"#Loading the dataset\ndiabetes_data = pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")\n\n#Print the first 5 rows of the dataframe\ndiabetes_data.head()","82b3ce33":"diabetes_data.isna().any(axis=0)\n","b9c4f507":"diabetes_data.info(verbose=True)","78b0f2dc":"diabetes_data.describe()","a03e0e9a":"diabetes_copy = diabetes_data.copy(deep = True)\ndiabetes_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = diabetes_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)\n\n\nprint(diabetes_copy.isnull().sum())","74f4d2df":"p = diabetes_data.hist(figsize = (20,20))","284829ae":"diabetes_copy['Glucose'].fillna(diabetes_copy['Glucose'].mean(), inplace=True)\ndiabetes_copy['BloodPressure'].fillna(diabetes_copy['BloodPressure'].mean(), inplace=True)\ndiabetes_copy['SkinThickness'].fillna(diabetes_copy['SkinThickness'].mean(), inplace=True)\ndiabetes_copy['Insulin'].fillna(diabetes_copy['Insulin'].mean(), inplace=True)\ndiabetes_copy['BMI'].fillna(diabetes_copy['BMI'].mean(), inplace=True)\n","288fce6c":"p = diabetes_copy.hist(figsize = (20,20))","f89e71bd":"\nprint(diabetes_data[\"Outcome\"].value_counts())\np=diabetes_data[\"Outcome\"].value_counts().plot(kind=\"bar\")","326e53f2":"diabetes_data.corr()","0efd93e3":"import seaborn as sns\nfrom matplotlib import pyplot as plt\nsns.pairplot(diabetes_data,diag_kind='kde',hue='Outcome')","3f4011c2":"plt.figure(figsize=(12,10))  \np=sns.heatmap(diabetes_data.corr(), annot=True,cmap ='RdYlGn')  ","e90fd0c0":"plt.figure(figsize=(12,10))  \np=sns.heatmap(diabetes_copy.corr(), annot=True,cmap ='RdYlGn')  ","e1fd61c0":"from sklearn.preprocessing import StandardScaler\nX_sc = StandardScaler()\nX = pd.DataFrame(X_sc.fit_transform(diabetes_copy.drop([\"Outcome\"],axis = 1),),\n                columns=['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPredigreeFunction','Age'])","ce1140a1":"X.head()","7c43b2a7":"y = diabetes_copy.Outcome","4dc7bb1b":"y.head()","e96cd4c2":"#importing train_test_split\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=1\/3, random_state = 42, stratify = y)","a4345f5c":"from sklearn.neighbors import KNeighborsClassifier\n\ntest_scores = []\ntrain_scores = []\n\nfor i in range(1,15):\n    \n    knn = KNeighborsClassifier(i)\n    knn.fit(X_train,y_train)\n    \n    train_scores.append(knn.score(X_train,y_train))\n    test_scores.append(knn.score(X_test,y_test))\n    ","5a871938":"## score that comes from testing on the same datapoints that were used for training\nmax_train_score = max(train_scores)\ntrain_scores_ind = [i for i,v in enumerate(train_scores) if v == max_train_score]\nprint('Max train score {} % and k = {}'.format(max_train_score*100,list(map(lambda x : x+1, train_scores_ind))))","92134a8c":"## score that comes from testing on the datapoints that were split in the beginning to be used for testing solely\nmax_test_score = max(test_scores)\ntest_scores_ind = [i for i,v  in enumerate(test_scores) if v == max_test_score]\nprint('Max test score {} % and k = {}'.format(max_test_score*100, list(map(lambda x: x+1, test_scores_ind))))","e123d563":"plt.figure(figsize=(12,5))\np = sns.lineplot(range(1,15),train_scores,marker='*', label='Train Score')\np = sns.lineplot(range(1,15),test_scores,marker='o',  label='Test Score')","24cfb3e9":"# Setting knn clasifier with k neighbors\nknn = KNeighborsClassifier(11)\nknn.fit(X_train,y_train)\nknn.score(X_test,y_test)","6172e516":"## trying to plot decision boundary","b0d4d445":"value = 20000\nwidth = 20000\n\nplot_decision_regions(X.values, y.values, clf= knn, legend=2,\n                     filler_feature_values={2: value, 3: value, 4:value, 5:value, 6:value, 7:value},\n                     filler_feature_ranges={2: width, 3: width, 4:width, 5:width, 6:width, 7:width},\n                     X_highlight=X_test.values)\n\nplt.title('KNN with Diabetes Data')\nplt.show()\n","eaa14080":"# import confustion_matrix\nfrom sklearn.metrics import confusion_matrix\n#let us get the predictions using the classifier we had fit above\ny_pred = knn.predict(X_test)\nconfusion_matrix(y_test,y_pred)\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)","57a9e2c1":"y_pred = knn.predict(X_test)\nfrom sklearn import metrics\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\np = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","ec4728a1":"# import classification_report\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))","7f4593e2":"from sklearn.metrics import roc_curve\ny_pred_proba = knn.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)","0b1f119c":"plt.plot([0,1],[0,1], 'k--')\nplt.plot(fpr,tpr, label='Knn')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('Knn(n_neighbors=11) ROC curve')\nplt.show()","b54ea378":"#Area under ROC curve\nfrom sklearn.metrics import roc_auc_score\nroc_auc_score(y_test,y_pred_proba)","a47153ee":"#import GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n#In case of classifier like knn the parameter to be tuned is n_neighbors\nparam_grid = {'n_neighbors':np.arange(1,50)}\nknn = KNeighborsClassifier()\nknn_cv= GridSearchCV(knn,param_grid,cv=5)\nknn_cv.fit(X,y)\n\nprint(\"Best Score:\" + str(knn_cv.best_score_))\nprint(\"Best Parameters: \" + str(knn_cv.best_params_))","0ed6173d":"From the above correlation matrix we need to consider the *columns(eg. Pregnancies, Glucose etc)* with the *outcome* column\n\nWe see that the correlation between '*Glucose*' and '*Outcome*'is the highest.\nSecond to this we have the correlation between '*Pregnancies*' and '*Outcome*'\n\nThe same can be seen from the pairplot. As we can","a193af11":"The most important step in the pipeline is to understand and learn how to explain your findings through communication. \n\nIt is really very much appriciated that the outcome of all the above computations and predictions are communicated to the audience in a manner that can even kids can understand\n\n**Objective**\n* **Identify business insights** : return back to business problem\n* **Visualize your findings accordingly** : keep it simple and priority driven\n* **Tell a clear and actionable story** : effectively communicate to non-technical audience","f529fff4":"From above we can see that we get the higest accuracy when **k = 11**. We will use this value for our final model","ccc7fc21":"# **Classification Report**\n\nReport which includes Precision, Recall and F1-Score","3f6f4f4b":"From above we cansee that the data is biased towards non-diabetic records.","16dd696f":"**Stratify Parameter** : this parameter makes a split so that the proportion of values in the sample produced will be the same as the proportion of values provided to parameter.\n\nFor example: if the variable *y* is binary categorical variable with values *0* and *1* and there are *25%* of *zeros* and *75%* of *ones*, *stratify=y* will make sure that your random split has 25% of 0's and 75% of 1's","fb4ac094":"In this I am just pacticing the notebooks submitted by others and help to understand how get started with kaggle as a begginer and how to solve the problems as a whole.\n\n\n* https:\/\/www.kaggle.com\/shrutimechlearn\/step-by-step-diabetes-classification-knn-detailed\n* https:\/\/www.kaggle.com\/ash316\/ml-from-scratch-part-2\n\nThe whole ML pipeline is referenced from below link\nhttps:\/\/www.linkedin.com\/pulse\/life-data-science-osemn-randy-lao\/?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base_post_details%3BmDlg5VsdSBCLBps2R0vRZA%3D%3D\n\nHowever I am unable to get the part where we need to Interpreting or Data Storytelling. \nI am unable to actually form a simple story out of all the calculations and plot it down.\nAny help would be appriciated.","a626623e":"# **Visualizing the Results**","e92f45d4":"# **Hyper Parameter optimization**","19968e41":"For reference : https:\/\/www.linkedin.com\/pulse\/life-data-science-osemn-randy-lao\/?lipi=urn%3Ali%3Apage%3Ad_flagship3_profile_view_base_post_details%3BmDlg5VsdSBCLBps2R0vRZA%3D","41cef97b":"After cleaning your data and finding what features are most important, using your model as a predictive tool will help in your **decision making**\n\n**Objective**\n\n* **In-depth Analytics** : create predictive models\/algorithms\n* **Evaluate and refine the model**","2625e6bf":"# What is Correlation?","ee7cca71":"# Skewness\n1. **Left-skewed distribution** \n    They have a long tail to its left. \n    Left-skewed distribution are negative-skewed distributions.\n    The mean is to the left of the median.\n2. **Right-skewed distribution**\n    They have a long tail to its right.\n    Right-skewed distribution are positive -skewed distributions.\n    The mean is to the right of the median.\n    ![image.png](attachment:image.png)","b1e919ca":"The following are the terms when added to the Condusion Matrix.\n\n![image.png](attachment:image.png)\n\nNow we can also calculate some of the common rates from a confusion matrix for a binary classification.\n\n* **Accuracy** : How often does the classifier correctly identifies a Diabetic patient.\n* * (TP+TN)\/total = (100+50)\/165 = 0.91\n\n\n\n* **Misclassification Rate** : How often the classifier incorrectly identifies Diabetic as Non Diabetic. \n* * equal to 1 minus Accuracy \n* * This is also known as \"**Error Rate**\" \n* * (FP+ FN)\/total = (10+5)\/165 = 0.09 \n\n\n\n* **True Positive Rate** : When it's actually Diabetic, How often does it predict Diabetic. \n* * This is also known as \"Recall\" \n* * TP\/Actual Yes = 100\/105 = 0.95 \n\n\n\n* **False Positive Rate** : When it's actually Non Diabetic. How often does it predict Diabetic.\n* * FP\/actual no = 10\/60 = 0.17\n\n\n\n* **True Negative Rate** : When the patient is Non-Diabetic how often does it predict Non-Diabetic \n* * equal to 1 - (False Positive Rate)\n* * This is also known as \"Specificity\" \n* * (FP+ FN)\/total = (10+5)\/165 = 0.09 \n\n\n\n* **Precision** : When it predicts Diabetic. How often is it correct?\n* * TP\/predicted yes = 100\/110 = 0.91 \n\n\n\n* **Prevalence** : How often does Diabetic patient actually occour in our sample?\n* * actual yes\/total = 105\/165 = 0.64\n\n\n\n* **Null Error Rate** : How often the classifier is wrong if you predicted the majority class. This can be the best classifier to compare your classifier against. \n* * Actual No\/total = 60\/165=0.36 \n\n\n\n* **Cohen's Kappa** : This value is high when there is a big difference between the accuracy and the null error rate. \n\n\n\n* **F Score** :Weighted average of the true positive rate(recall) and precision \n* * 2 ((Precision*Recall)\/(Precision + Recall)) \n\n\n\n* **ROC Curve** : This is commonly used graph that summarizes the prformace of a classifier over all possible thresholds. \n* * It is generated by plotting the True Positive Rate(y-axis) againse the False Positive Rate (x-axis) as you vary the threshold for assigning observaations to a given class","6e671482":"To fill these Nan Values we need to understand the distribution of data","fbd2533b":"The following is the Output of the describe function\n* *count* tells us the number of Non-Empty rows in a feature\n* *mean* tells us the mean value of the feature\n* *std* tells us the Standard Deviation Value of that feature.\n* *min* tells us the minimum value of that feature\n* Now we have a *list of the percentile\/quartile* of each feature. This also helps to detect Outliers\n* *max* tells us the maximum value of that feature.\n\n**Note** : \n* Only numerical columns would be displayed here unless parameter include='all' is given\n* Dataframe.select_dtypes also helps in returning columns of a specific dtype\n","b6024936":"**Some Basic Terms**\n\n* **True Positives (TP)**: These are cases in which we predicted yes(Diabetic), and they really are Diabetic\n* **True Negative (TN)** : We predicted Non-Diabetic but they had Diabeties.\n* **False Positive (FP)** : We predicted Diabetic, but they were Non Diabetic. (Type I error)\n* **False Negative (FN)** : We predicted Non Diabetic, but they were Diabetic. (Type II Error)","c120ace9":"# **Modeling (Machine Learning)**","aa27886b":"# Finding insights through correlation matrix","6f8a4849":"# **Model Performance Analysis**","7a890c08":"When we look into the glucose column, the two colors we stated in hue actually represent the Diabetes = 1(blue) and Diabetes=0(Red)\nIf we take somewhat next level, if we look the mean of red distribution and the mean of Blue distribution.\nWe see that the above said means are significantly different from each other.\nWith this insight now we can link it to our problem statement and say that Glucose is an excellent predictor of diabetes.\nWe can see a significant seperation between the two distributions Diabetes and non-Diabetes.\n","0fa878f1":"to learn more about scaling techniques\nhttps:\/\/medium.com\/@rrfd\/standardize-or-normalize-examples-in-python-e3f174b65dfc https:\/\/machinelearningmastery.com\/rescaling-data-for-machine-learning-in-python-with-scikit-learn\/","bf0650f3":"# **Test Train Split**","45b4e7fa":"This is an important phase as the results and output of your machine learning model depends on the quality of the data you put into the creating the model. \nThis is also the phase of the pipeline that requires the most time and effort.\n\n**Objectives**\n\n1. **Examine the data**  understand every feature you're working with, identify errors, missing values, and corrupt records\n2. **Clean the data** throw away, replace, and\/or fill missing values\/errors ","3ee9a431":"# Heatmap for clean data","f045a48a":"The *train-test* split is a technique for evaluating the *performance* of a machine learning algorithm.\n\nThis technique can be used for *supervised learning algorithm* using *classification* and *regression* techniques.\n\nIn this technique we divide the original dataset into two subsets.\n* **Train Dataset** : Used to fit the machine learning model.\n* **Test Dataset** : Used to evaluate the fit machine learning model.\n\nThe objective is to estimate the performance of the machine learning model on new data: data not used to train the model.\n![image.png](attachment:image.png)","b25fc2aa":"Now that we have decided the value for *k*, and that we have built a model. Now we test it on it's performance","718264b0":"We will check if the data is skewed or not. \n","8cbe9e92":"* O - Obtaining our data\n* S - Scrubbing\/Cleaning our data\n* E - Exploring \/Visualizing our data will allow us to find patterns and trends\n* M - Modeling our data will give us our predictive power as a wizard\n* N - INterpreting our data","9c86f012":"* *isna()* function is used to detect missing values. \n* It return a *boolean* same-sized object indicating if the values are NA. \n* *NA* values, such as *None* or numpy *NaN*, gets mapped\n* Now we apply *any()* function to check for any *True* among the False indicating that we have a NA value\n* providing with parameter *axis=0* helps to search for the *columns*.","58734ae0":"# **Check the dataset for any missing values**","c3b1b008":"# Scaling the data","a12df4e6":"# **OSEMN Pipeline**","cc063e5a":"In this phase we try to **Understand** the patters and values our data has. \nWe use different types of **visualizations** and **statistical testings** to get these findings.\nThis is how we would be able to **derive hidden meanings** behind our data **through various graphs and analysis**.\n\n**Objective**\n* Find patterns in your data through visualizations and charts\n* Extract features by using statistics to identify and test significant variables","8d9ee649":"# **Interpreting (Data Storytelling)**","1e454b48":"Cross-Validation is one of the technique used to test he effectiveness of a machine learning models, it is also a resampling procedure used to evaluate a model if we have a limited data.\n\nCross-validation uses the training data and breaks it down into smaller chunks to test again itself.\n\nTo perform cross validation we need to keep aside a sample\/portion of the data which is not used t train the model, later this sample is used for testing\/validation\n\n![image.png](attachment:image.png)","f414c7c4":"A **heat map** is a two-dimensional representation of information with the help of colors. \n\n**Heat maps** can help the user visualize simple or complex information.","3909ecbf":"# **Basic Data Science and ML Pipeline**","c37322d6":"# **Exploring (Exploratory Data Analysis)**","709b66d6":"From the above we see that the mininum value of columns is 0. But having 0 as the minimum value for the below columns doesn't make scence does it?\n\n* Glucose\n* BloodPressure\n* SkinThickness\n* Insulin\n* BMI\n\nFor this purpose we will replace 0 with nan for counting purpose.\nLater we will replace the 0 with suitable values.","97604d0b":"# **ROC-AUC**","cf461cee":"Correlation basically means association - more precisely it is a measure of the extent to which one variable is related to another.\nThere are three possible results of correlation:-\n1. Positive Correlation:\n   ![2020-09-24_20h57_49.png](attachment:2020-09-24_20h57_49.png)\n   The points lie close to a straight line, which has a positive gradient.\n   This shows that as one varible increases the other increases.\n2. Neative Correaltion\n    ![2020-09-24_21h02_13.png](attachment:2020-09-24_21h02_13.png)\n    The points lie close to a straight line, which has a negative gradient.\n    This shows that as one varible increases, the other decreases.\n3.  No Correlation\n    ![2020-09-24_21h08_21.png](attachment:2020-09-24_21h08_21.png)\n    There is no pattern to the points. \n    This shows that there is no connection between the two variables.","1caae5c8":"data Z is rescaled such that \u03bc = 0 and \ud835\uded4 = 1, and is done through this formula:\n![image.png](attachment:image.png)","8a60d402":"# **Cross Validation methods**","0039d393":"**Parameter**\nConfiguration variable that is internal to the model and whose value can be estimated from the given data.\n\n**Hyperparameter**\nConfiduration variable  that is external to the model and whose value cannot be estimated from data.\n\n**Hyperparamter tuning** \nMethodically build and evaluate a model for each combination of algorithm paramters specified in a grid.\n\n**Approaches to Hyperparameter optimization**\n1. Manual Search\n2. Random Search\n3. Grid Search\n4. Automated Hyperparameter Tuning (Bayesian Optimization, Genetic Algorithms)\n5. Artificial Neural Networks (ANNs) Tuning\n\n**Two simple strategies to optimize\/tune the hyperparameters**\n\n1. **Grid searching of hyperparameters**\n    Grid search is an approach to hyperparameter tuning that will methodically build and evaluate a model for each combination of algorithm parameters specified in a grid.\n\n2. **Random searching of hyperparameters:**\n    The idea of random searching of hyperparameters was proposed by James Bergstra & Yoshua Bengio. You can check the original paper here.\n\n","73f7112d":"Plotting after removing Nan","32df621d":"# Heatmap for unclean data","6a124dd8":"* *Dataframe.describe()* is used to view some based statuscal details like percentile, mean, std etc of a data frame or a seies of numeric values.\n* It takes the following parameters \n* *percentile:* we provide a list of numbers 0-1 to return the respective percentiles.\n* *include:* List of the data types to be inluded while describing  dataframe. Default value is *None*\n* *exclude:* List of the data types to be exlude while describing  dataframe. Default value is *None*","02b7f0ad":"# **Scrubbing \/ Cleaning Your Data**","ffa62b28":"* Pandas *dataframe.info()* function is used to get a concise summary of the dataframe. \n\n* It comes really handy when doing exploratory analysis of the data. \n\n* To get a quick overview of the dataset we use the *dataframe.info()* function.\n\n* *Verbose(Parameter)* specifies whether to print the full summary.","b44712c8":"# **1. Confusion Matrix**","c0d2a286":"# dataframe.describe()","0e5b17b0":"The confusion matrix is a technique used for summarizing the performance of a classification algorithm i.e. it has binary outputs.\n\n![image.png](attachment:image.png)\n\nWe can get the below insights from above matrix example.\n* There are two predicted classes : \"Yes\" and \"No\". \"Yes\" means \"Diabetic\" and \"No\" means \"Non-Diabetic\"\n* The classifier made a total of *165* predictions\n* Out of these, the classifier predicted \"Yes\" 110 times, and \"No\" 55 times.\n* In reality, 105 are Diabetic and 60 are \"Non-Diabetic\"","3c80ca70":"We need to impute nan values for the colummns by using the distribution","ddfc6330":"# **O - Obtaining our data**\n\nYou cannot do anything as a data scientist without even having any data.\nHowever there are some things you must take ingto consideration when obtaining your data.\n1. Identify all available sources for this data (i.e. internet or \/external\/internal databases)\n2. Extract data into a usable format (.csv, .json, .xml etc)\n\nIn our case we already have the data available from kaggle. We will load this data in the below code or rather with the help of the code already provided by kaggle, so that it will for our use in this notebook.\n","e53a59df":"# dataframe.info()"}}