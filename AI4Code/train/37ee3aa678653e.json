{"cell_type":{"5fc686e8":"code","4d73f879":"code","4fb64c5e":"code","d3fadb2e":"code","d962ef1f":"code","ef290931":"code","f27930fc":"code","bf7d96ca":"code","46e6e67d":"code","f7a01795":"code","611ba947":"code","de8c1ca5":"code","542f54cd":"code","b7434c03":"code","1f9d1f4e":"code","c1df1258":"code","88924a89":"code","bf6a96a1":"code","df0d85d6":"code","1d971009":"code","3f3abded":"code","9b635ad7":"code","5be61244":"code","aa3b0b18":"code","ac270ad2":"code","3a853c33":"code","b3a2f6ea":"code","b0f4250a":"code","37e7ffe0":"code","7c543b23":"code","2fe45efc":"code","a967bda3":"code","ccf232b0":"code","08e0ed15":"code","4f7a22ca":"code","91533392":"code","62ac7f0f":"code","b2c027b7":"code","a2657cd8":"code","04dd65dd":"code","a91844e0":"markdown","385cadfc":"markdown","5e17ac45":"markdown","2ce154a7":"markdown","b015b7db":"markdown","b24b3a5a":"markdown","25917261":"markdown","64621b15":"markdown","5155df58":"markdown","e5bdf744":"markdown","55729a15":"markdown","7bde786f":"markdown","f87b8028":"markdown","8213f341":"markdown","e581c8bb":"markdown","d1cd564f":"markdown","9504ba53":"markdown","459beef3":"markdown","2f3d5d27":"markdown"},"source":{"5fc686e8":"import pandas as pd\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nimport time\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA as sklearnPCA\nfrom matplotlib import pyplot as plt\nfrom multiprocessing import cpu_count\nimport numpy as np","4d73f879":"raw = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")","4fb64c5e":"raw.shape # n: 42000, p: 785","d3fadb2e":"raw[\"label\"] = raw[\"label\"].apply(str) # converting to factor values","d962ef1f":"def bi(num):\n    if num > 0:\n        return 1\n    else:\n        return 0","ef290931":"for i in raw.columns[1:]:\n    raw.loc[:,i] = raw.loc[:,i].apply(bi)","f27930fc":"for i in raw.nunique()[1:]:\n    if i > 2:\n        print(False)\n        break","bf7d96ca":"x = raw.drop(['label'], axis='columns')\ny = raw[\"label\"]\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)","46e6e67d":"#standardized data\nss = StandardScaler().fit(x_train)\nx_train_scaled = ss.transform(x_train)\nx_test_scaled = ss.transform(x_test) # scales the test x based on the scaling applied to x_train\n\nsklearn_pca = sklearnPCA().fit(x_train_scaled)\n\nper_var = sklearn_pca.explained_variance_ratio_\ncum_per_var = sklearn_pca.explained_variance_ratio_.cumsum()","f7a01795":"n_comp=len(cum_per_var[cum_per_var <= 0.90]) # Using number of PCs which explains 90% of the variance\nn_comp","611ba947":"sklearn_pca = sklearnPCA(n_components=n_comp)\nx_train_pca = sklearn_pca.fit_transform(x_train_scaled)\nx_test_pca = sklearn_pca.transform(x_test_scaled)","de8c1ca5":"c=[]\nfor i in range(-5, 17, 2):\n    c.append(2**i)","542f54cd":"start = time.time()\nsvc = SVC(kernel = \"linear\", decision_function_shape = \"ovr\")\nparameters = {\"C\": c}\nclf = GridSearchCV(svc, parameters, cv=5)\nclf.fit(x_train_pca[:5000], y_train[:5000])\nend = time.time()\nprint(f\"time taken to search {end-start}\")","b7434c03":"clf.best_params_","1f9d1f4e":"clf.best_score_","c1df1258":"clf = SVC(kernel = \"linear\", decision_function_shape = \"ovr\", C = 0.03125)\nclf.fit(x_train_pca, y_train)","88924a89":"print(f\"test accuracy: {clf.score(x_test_pca,y_test)}\")","bf6a96a1":"c=[]\nfor i in range(-5, 17, 2):\n    c.append(2**i)\n    \nr=[]\nfor i in range(-15, 5, 2):\n    r.append(2**i)","df0d85d6":"start = time.time()\nsvc = SVC(kernel = \"rbf\", decision_function_shape = \"ovr\")\nparameters = {\"C\": c, \"gamma\": r}\nclf = GridSearchCV(svc, parameters, cv=5)\nclf.fit(x_train_pca[:5000], y_train[:5000])\nend = time.time()\nprint(f\"time taken to search {end-start}\")","1d971009":"clf.best_params_","3f3abded":"clf.best_score_","9b635ad7":"clf = SVC(kernel = \"rbf\", decision_function_shape = \"ovr\", C = 8, gamma = 0.001953125)\nclf.fit(x_train_pca, y_train)","5be61244":"print(f\"test accuracy: {clf.score(x_test_pca,y_test)}\")","aa3b0b18":"c=[]\nfor i in range(-5, 17, 2):\n    c.append(2**i)\n    \nd=[2,3,4,5]","ac270ad2":"start = time.time()\nsvc = SVC(kernel = \"poly\", decision_function_shape = \"ovr\")\nparameters = {\"C\": c, \"degree\": d}\nclf = GridSearchCV(svc, parameters, cv=5)\nclf.fit(x_train_pca[:5000], y_train[:5000])\nend = time.time()\nprint(f\"time taken to search {end-start}\")","3a853c33":"clf.best_params_","b3a2f6ea":"clf.best_score_","b0f4250a":"clf = SVC(kernel = \"poly\", decision_function_shape = \"ovr\", C = 128, degree = 3)\nclf.fit(x_train_pca, y_train)","37e7ffe0":"print(f\"test accuracy: {clf.score(x_test_pca,y_test)}\")","7c543b23":"x_train = x\ny_train = y\nx_test = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")","2fe45efc":"for i in x_test.columns:\n    x_test.loc[:,i] = x_test.loc[:,i].apply(bi)","a967bda3":"#standardized data\nss = StandardScaler().fit(x_train)\nx_train_scaled = ss.transform(x_train)\nx_test_scaled = ss.transform(x_test) # scales the test x based on the scaling applied to x_train\n\nsklearn_pca = sklearnPCA().fit(x_train_scaled)\n\nper_var = sklearn_pca.explained_variance_ratio_\ncum_per_var = sklearn_pca.explained_variance_ratio_.cumsum()","ccf232b0":"n_comp=len(cum_per_var[cum_per_var <= 0.90])\nn_comp","08e0ed15":"sklearn_pca = sklearnPCA(n_components=n_comp)\nx_train_pca = sklearn_pca.fit_transform(x_train_scaled)\nx_test_pca = sklearn_pca.transform(x_test_scaled)","4f7a22ca":"def kcv_poly_svm(i, cost): # i is a cluster, ie. range from 0-9\n    start = i * 4200# length of df: 42000\n    end = start + 4260\n    \n    test_x = x_train_pca[start:end]\n    test_y = y_train[start:end]\n    selected = list(range(start)) + list(range(end, 42000)) # exclude\n    train_x = x_train_pca[selected]\n    train_y = y_train.iloc[selected,]\n    \n    clf = SVC(kernel='poly', decision_function_shape = \"ovr\", C = cost, random_state = 1)\n    clf.fit(train_x, train_y)\n    \n    return clf.score(test_x, test_y)# testset error","91533392":"def cv_runner(c):\n    cv_error_1 = []\n    if __name__ == '__main__':\n        with concurrent.futures.ProcessPoolExecutor(max_workers=cpu_count()-1) as executor:\n            for cost in c:\n                class_error = []\n                results = [executor.submit(kcv_poly_svm, i, cost) for i in range(10)]\n                # results = executor.map(kcv_lienar_svm, range(10), 1)\n                for r in concurrent.futures.as_completed(results):\n                    class_error.append(r.result())\n                cv_error_1.append(sum(class_error)\/10)\n                \n    print(c)\n    print(cv_error_1)","62ac7f0f":"#c = [100, 200, 300, 400]\n#cv_runner(c) # takes approximately 25 min to execute\n\n#c = [80, 90, 100, 110, 120, 130, 140, 150]\n#cv_runner(c)","b2c027b7":"clf = SVC(kernel = \"poly\", decision_function_shape = \"ovr\", C = 100, degree = 3, random_state=1)\nclf.fit(x_train_pca, y_train)","a2657cd8":"prediction = clf.predict(x_test_pca)","04dd65dd":"out = pd.DataFrame({\"ImageId\": range(1,len(x_test)+1), \"Label\" : prediction})\nout.to_csv(\"submission_final.csv\", index = False)","a91844e0":"Creation of function to run in parallel.\nIn Windows, there is a need to add the line \"if __name__ == '__main__':\" to escape the BrokenProcessPool error","385cadfc":"### Converting to Black & White images (Improvement 1.)","5e17ac45":"As GridSearch CV takes a long time to compute, KCV will be conducted in parallel instead. \n\nSpecs of Computer: \nCPU: AMD Ryzen 3600 (12 cores)\nRAM: 16GB\n    \nAs juypter notebook has some trouble in parallel computing, the following code is run on Windows Python IDE instead.","2ce154a7":"We will first explore which SVM is the best fit: Linear, Polynomial, Radial. \n\nThis is done via GridSearchCV of a small sample set(n: 5000) and testing their accuracy using the testset to select the best model.\nAfter selecting the best model, we then fine tune the parameteres to give us the best prediction accuracy.","b015b7db":"## Radial SVM","b24b3a5a":"Before referencing the kernel, the test set data was scaled separately from the trainset data which led to poor prediction accuracy. It is important to note that the 2 datasets have to be scaled in the same way! (Improvement 2)","25917261":"## Parallel Computing with SVM","64621b15":"## Linear SVM","5155df58":"Train test splitting","e5bdf744":"## Selection of type of SVM","55729a15":"Since no \"False\" is printed, it has been converted successfully","7bde786f":"From the CV, we can determine that the best cost is 100. The accuracy for the range of cost is about the same.","f87b8028":"Before referencing [this kernel](https:\/\/www.kaggle.com\/sabasiddiqi\/svm-classification-parameter-selection-0-968\/notebook), the prediction accuracy was 73% due to a mistake I have made.\nAfter referencing, it has led to 97% accuracy.\n\nImprovements made:\n1. Using Black & White images instead of Grey-Scaled images (improves accuracy by 2%)\n2. Using the same scale for testset & trainset before conducting PCA **","8213f341":"First create Cross-Validation Fucntion (10 Fold CV) to be run in parallel","e581c8bb":"Check that the data has been converted successfully","d1cd564f":"## PCA","9504ba53":"As can be seen, Polynomial SVM performs the best and hence we will be using Polynomial SVM in prediction.","459beef3":"## Polynomial SVM","2f3d5d27":"## Preparing the Test Set for prediction"}}