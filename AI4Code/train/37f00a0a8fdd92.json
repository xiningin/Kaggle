{"cell_type":{"ff0059a5":"code","7185d63d":"code","b9ab3a7d":"code","33dc6825":"code","025b1437":"code","c0eacded":"code","89c666d9":"code","39e6587a":"code","aed75ad5":"code","f4221d93":"code","0c1d118e":"code","79092fbe":"code","4293a757":"code","c2874683":"code","75daa49e":"code","bddde510":"code","3b6fa8f2":"code","a84aeb89":"code","67d7caee":"code","b5c895c3":"code","36077a81":"code","00b7d4da":"code","d6a22caf":"code","ab20e966":"code","66b2fbb3":"code","8518bbe3":"code","77e43fc8":"code","220febb7":"code","6784a427":"code","f8cd1fc4":"code","48490df5":"code","e027ce40":"code","befdba6e":"code","360fb869":"code","ae860a68":"code","09d1e0f0":"code","8e256dbb":"code","a2bd7ba3":"code","0941f050":"code","601e37fc":"code","d793b222":"code","d89c599c":"code","3d89ddfa":"code","0f26c9cd":"code","f661ee1d":"code","afacf171":"code","3068016e":"code","875abf7e":"code","4a69aa8e":"code","91c5c814":"code","64a0ba99":"code","cb4bfcb0":"code","a1d85aa8":"code","f422b021":"code","3c697951":"code","91937a32":"code","2672da1b":"code","a543873f":"code","1a46a3d9":"code","8af75ada":"code","c5245ed2":"code","9c88736d":"code","931bb476":"code","3b7c606d":"code","d24322f8":"code","2af6fe34":"code","3370a1d4":"code","7a186823":"code","db531360":"code","367209ee":"code","a7734eb1":"code","37e47f1d":"code","08a8caa0":"code","63686901":"code","e43127d0":"code","23a73776":"code","70b0ec3e":"markdown","c6543579":"markdown","8eb71683":"markdown","afa1cb38":"markdown","ebd079ee":"markdown","3804e48c":"markdown","c409df6a":"markdown","6d6497d7":"markdown","53877cbf":"markdown","d1be76e6":"markdown","1f4af855":"markdown","aff3d6d9":"markdown","dcbadf22":"markdown","30b06b4c":"markdown","efdf033e":"markdown","109ea192":"markdown","8a5a7b8a":"markdown","77a608be":"markdown","43574fcd":"markdown","d13ab369":"markdown","fa064f41":"markdown","906b3635":"markdown","7e80ac3b":"markdown","d9dcbb28":"markdown","cd536a84":"markdown","6f3d7ec7":"markdown","6b2da01c":"markdown","23b968cd":"markdown","ae2432ea":"markdown"},"source":{"ff0059a5":"import numpy as np \nimport pandas as pd \n\n# data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style","7185d63d":"dstrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndstrain.head()","b9ab3a7d":"dstest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ndstest.head()","33dc6825":"# Combinaisons des deux dataset en un seul pour ce faciliter le traitement des donn\u00e9es\nds = pd.concat([dstrain, dstest], sort=False, ignore_index=True)","025b1437":"ds.tail()","c0eacded":"ds.info()","89c666d9":"ds.describe()","39e6587a":"# V\u00e9rifier les valeurs nulles et afficher leur pourcentage\ntotal = ds.isna().sum().sort_values(ascending=False)\npercent_1 = ds.isna().sum()\/ds.isna().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\nmissing_data = missing_data[missing_data['%']!=0]\nmissing_data","aed75ad5":"# R\u00e9cup\u00e9rer les valeurs uniques des diff\u00e9rentes colonnes\ncolsWithUniqueValues =  ['Survived', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked']\nfor col in colsWithUniqueValues:\n    try:\n        print(f\"{col:<10} {sorted(ds[col].dropna().unique())}\")\n    except:\n        print(f\"{col:<10} {ds[col].unique()}\")","f4221d93":"# On peut voir ci-dessus que 38% du dataset a survecu\n# Normalement Ticket, PassengerId ne devraient pas avoir d'impact sur le taux de survie\nds.drop(['PassengerId','Ticket'],axis=1,inplace = True)\nds.head()","0c1d118e":"# les ages ne suivent pas une distribution 'normale'\nfrom scipy.stats import normaltest\n# test de normalit\u00e9\nstat, p = normaltest(ds['Age'].dropna())\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\n# interpretation\nalpha = 0.05\nif p > alpha:\n    print(\"l'\u00e9chantillon semble Gaussien (impossible de rejeter l'hypoth\u00e9se nulle H0)\")\nelse:\n    print(\"l'\u00e9chantillon ne semble pas Gaussien (rejet de l'hypoth\u00e9se nulle H0)\")","79092fbe":"# Renvoit un age au hasard pour la colonne Age qui suit la distribution des valeurs de la colonne\n# utilisation de sample (Pandas)\nds['Age'].dropna().sample(ds['Age'].isnull().sum(),random_state=0)","4293a757":"def impute_nan(df,variable):\n    # on duplique la colonne dans un premier temps (y compris les valeurs nulles qu'elle contient)\n    df[variable+\"_rnd\"]=df[variable]\n    # on g\u00e9n\u00e9re une s\u00e9rie de la taille des Age qui sont null et on sample dedans avec la distrib de la colonne Age\n    random_sample=df[variable].dropna().sample(df[variable].isnull().sum(),random_state=0)\n    # Pour r\u00e9aliser la fusion, on conserve les index des Age null\n    random_sample.index=df[df[variable].isnull()].index\n    df.loc[df[variable].isnull(),variable+'_rnd']=random_sample","c2874683":"# on remplace dans le dataset les valeurs nulles dans la colonne Age\nimpute_nan(ds,\"Age\")\nds.head()","75daa49e":"# On voit ci-dessous qu'en remplissant les 263 valeurs age manquantes que l'on n'a pas (peu) modifi\u00e9 la distribution\nfig = plt.figure()\nax = fig.add_subplot(111)\nds['Age'].plot(kind='kde', ax=ax)\nds.Age_rnd.plot(kind='kde', ax=ax, color='green')\nlines, labels = ax.get_legend_handles_labels()\nax.legend(lines, labels, loc='best')","bddde510":"# La colonne Age ne sert plus, elle contient des valeurs nulles, on utilisera Age_rnd que l'on vient de cr\u00e9er\nds.drop(['Age'],axis=1,inplace = True)","3b6fa8f2":"# Le port d'embarquement le plus fr\u00e9quent est 'S'\ndstrain['Embarked'].describe() ","a84aeb89":"common_value = 'S'\nds['Embarked'] = ds['Embarked'].fillna(common_value)","67d7caee":"# Calcul du nombre de personnes accompagnantes\nds['Relatives'] = ds['SibSp'] + ds['Parch']\n\n# cr\u00e9ation d'une colonne not alone qui va permettre d'avoir un bool\u00e9en permettant de savoir\n# si le voyageur est seul ou accompagn\u00e9\nds.loc[ds['Relatives'] > 0, 'Not_alone'] = 1\nds.loc[ds['Relatives'] == 0, 'Not_alone'] = 0\nds['Not_alone'] = ds['Not_alone'].astype(int)","b5c895c3":"# Taux de survie en fonction du nombre de personnes accompagnantes\naxes = sns.catplot('Relatives','Survived', data=ds, aspect = 2.5,kind='point')","36077a81":"# il semblerait que l'on survive plus en voyageant avec 1 \u00e0 3 personnes\n# nous allons cr\u00e9er trois classes pour repr\u00e9senter ce que l'on peut appr\u00e9cier visuellement sur le graphe\ndef family_cat(size):\n    if (size >= 1) & (size < 4):\n        return 0\n    elif ((size >= 4) & (size < 7)) | (size == 0):\n        return 1\n    elif (size >= 7):\n        return 2\n    \nds['Famcat'] = ds['Relatives'].apply(family_cat)\nds['Famcat'] = ds['Famcat'].astype(int)\nds.head()","00b7d4da":"# Graphe du taux de survie en fonction de la cat\u00e9gorie de famille et du sexe\nplt.figure(figsize=(8, 8))\nsns.barplot(x=\"Famcat\", y=\"Survived\", hue=\"Sex\", data=ds, palette='Blues_d')\nplt.show()","d6a22caf":"ds['Fare_Per_Person'] = ds['Fare'].fillna(0)\/(ds['Relatives']+1)\nds.head(10)","ab20e966":"# Dans le dataset on a un Fare inconnu. Mais on sait que le voyageur \u00e9tait seul\nds[ds['Fare'].isna()]","66b2fbb3":"# on va donc utiliser la moyenne de la classe 3 pour son Fare et Fare_Per_Person\nfare = ds[ds['Pclass']==3]['Fare_Per_Person'].mean()\nfare","8518bbe3":"ds.loc[ds['Fare'].isna(),'Fare_Per_Person'] = fare\nds.loc[ds['Fare'].isna(),'Fare'] = fare","77e43fc8":"# v\u00e9rification\nds.iloc[1043,:]","220febb7":"# Taux de survie en fonction du Fare_Per_Person et du sexe\ndstemp = ds.copy()\nplt.figure(figsize=(8, 8))\n# on cr\u00e9e des tranches de 'Fare' \u00e9quilibr\u00e9es avec qcut pour le graphe\ndstemp['Fare_Per_Person'] = pd.qcut(dstemp['Fare_Per_Person'], 5)\nsns.barplot(x=\"Fare_Per_Person\", y=\"Survived\", data=dstemp, hue =\"Sex\", palette='Blues_d')\nplt.show()","6784a427":"# Creating 'Title' column\nds['Title'] = ds['Name'].str.extract(' ([A-Za-z]+)\\.', expand = False)\nds['Title'].unique().tolist()","f8cd1fc4":"# Pourcentage de passagers en fonction du titre\nds['Title'].value_counts(normalize=True)*100","48490df5":"# Taux de survie en fonction du Titre\nds.groupby(['Title'])['Survived'].mean().sort_values(ascending=False) * 100","e027ce40":"# pas de null dans la colonne cr\u00e9e\nds['Title'].isna().sum()","befdba6e":"# Cr\u00e9ation de cat\u00e9gories pour regrouper les Titre qui ont un taux de survie \u00e9quivalent\n# Dona est surement un titre du dataset de test, mais c'est un titre de noblesse, on le met en cat\u00e9gorie Top\n\nds['Title'] = ds['Title'].replace(['Sir', 'Countess', 'Mme', 'Mlle', 'Dona' , 'Lady'], 'Top')\nds['Title'] = ds['Title'].replace(['Mrs', 'Miss'], 'High')\nds['Title'] = ds['Title'].replace(['Master', 'Dr', 'Col', 'Major', 'Ms'], 'Mid')\nds['Title'] = ds['Title'].replace(['Mr'], 'Low')\nds['Title'] = ds['Title'].replace(['Jonkheer', 'Rev', 'Don', 'Capt'], 'Bottom')\n\nds['Title'].value_counts()","360fb869":"# Taux de survie en fonction des cat\u00e9gories et du sexe \nplt.figure(figsize=(8, 8))\nsns.barplot(x=\"Title\", y=\"Survived\", data=ds, order = ['Bottom','Low','Mid','High','Top'], hue =\"Sex\", palette='Blues_d')\nplt.show()","ae860a68":"# beaucoup de valeurs nulles dans la feature Cabin, mais pour le cabines renseign\u00e9es\n# on peut voir qu'elles commencent toutes par une lettre\nds[ds['Cabin'].isna() == False].head(5) ","09d1e0f0":"ds['Cabin'] = ds['Cabin'].fillna('Unknown')\nds['Deck']=ds['Cabin'].str.get(0)\n\nds[ds['Cabin']!='Unknown'].head(5)","8e256dbb":"sorted(ds['Deck'].unique())","a2bd7ba3":"#visualisation du taux de survie par pont (Deck) et par sexe\nplt.figure(figsize=(8, 8))\nsns.barplot(x='Deck', y='Survived', data=ds, hue = 'Sex' ,palette='ocean', order = sorted(ds['Deck'].unique()))\nplt.show()","0941f050":"ds['Pclass'] = ds['Pclass'].astype(str)\nds = pd.get_dummies(ds, columns=['Pclass','Embarked','Famcat','Title','Sex','Deck'],drop_first=False)\nds.head()","601e37fc":"ds.columns","d793b222":"# drop des colonnes qui ne servent plus\nds.drop(['Name','SibSp','Parch','Cabin'],axis=1,inplace = True)\nds.head()","d89c599c":"# S\u00e9paration des donn\u00e9es Train \/ Test\ntrain = ds[:len(dstrain)]\n\n# on r\u00e9cup\u00e9re la matrice de corr\u00e9lation g\u00e9n\u00e9rale\ncorr = train.corr()\n# on l'affiche avec sns\nplt.figure(figsize=(15,15))\nsns.heatmap(corr,annot = True, fmt='.1g',vmin=-1, vmax=1, center= 0, square = True, cbar = None, cmap= 'coolwarm')\n\n# on peut voir ci dessous que le sexe est la feature la plus fortement correl\u00e9e \u00e0 la survie avec le titre\n# mais les autres (Fare_Per_Person, Fare, Embarked, Pclass, Famcat, Title, Not_alone) le sont aussi dans une moindre mesure","3d89ddfa":"# Normalisation des donn\u00e9es num\u00e9riques\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nds[['Age_rnd', 'Fare', 'Fare_Per_Person', 'Relatives']] = scaler.fit_transform(ds[['Age_rnd', 'Fare', 'Fare_Per_Person', 'Relatives']])\nds.head()","0f26c9cd":"# S\u00e9paration des donn\u00e9es Train \/ Test\ntrain = ds[:len(dstrain)]\n\n# Splitting dataset into test\ntest = ds[len(dstrain):]","f661ee1d":"# Premier test rapide avec RandomForestClassifier   \nfrom sklearn.ensemble import RandomForestClassifier\nX_train = train.drop(\"Survived\", axis=1)\nY_train = train[\"Survived\"]\n\nrandom_forest = RandomForestClassifier(n_estimators=100,random_state=0)\nrandom_forest.fit(X_train, Y_train)\n\nrandom_forest.score(X_train, Y_train)\nprint(\"Train score : \",round(random_forest.score(X_train, Y_train) * 100, 2))\n\n# le score nous montre qu'on est probablement en overfitting...","afacf171":"# On peut r\u00e9cuperer les features qui ont \u00e9t\u00e9 importante pour ce mod\u00e9le\nimportances = pd.DataFrame({'feature':X_train.columns,'importance':np.round(random_forest.feature_importances_,3)})\nimportances = importances.nlargest(20,'importance').set_index('feature')\nimportances","3068016e":"from sklearn.ensemble import ExtraTreesClassifier\nmodel=ExtraTreesClassifier()\nmodel.fit(X_train,Y_train)\nranked_features=pd.Series(model.feature_importances_,index=X_train.columns)\nplt.figure(figsize=(8, 8))\nranked_features.nlargest(len(X_train.columns)).sort_values(ascending=True).plot(kind='barh')\nplt.show()","875abf7e":"model.score(X_train, Y_train)\nprint(\"Train score : \",round(model.score(X_train, Y_train) * 100, 2))","4a69aa8e":"from sklearn.tree import DecisionTreeClassifier,plot_tree\nmodel=DecisionTreeClassifier()\nmodel.fit(X_train,Y_train)\nranked_features=pd.Series(model.feature_importances_,index=X_train.columns)\nplt.figure(figsize=(8, 8))\nranked_features.nlargest(len(X_train.columns)).sort_values(ascending=True).plot(kind='barh')\nplt.show()","91c5c814":"model.score(X_train, Y_train)\nprint(\"Train score : \",round(model.score(X_train, Y_train) * 100, 2))","64a0ba99":"# Avec un DecisionTree (CART), on peut afficher l'abre de d\u00e9cision qui a \u00e9t\u00e9 cr\u00e9e\n'''\nplt.figure(figsize=(100,100))\nplot_tree(model,feature_names=X_train.columns,class_names=\"Survived\", filled=True,fontsize=6)\nplt.savefig(\"dt.jpg\",dpi = 100)\n'''","cb4bfcb0":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2","a1d85aa8":"# SelectKBest Algorithm\nordered_rank_features=SelectKBest(score_func=chi2,k=20)\nordered_feature=ordered_rank_features.fit(X_train,Y_train)\ndfscores=pd.DataFrame(ordered_feature.scores_,columns=[\"Score\"])\ndfcolumns=pd.DataFrame(X_train.columns)\nfeatures_rank=pd.concat([dfcolumns,dfscores],axis=1)\nfeatures_rank.columns=['Features','Score']\nfeatures_rank = features_rank.sort_values('Score',ascending=False).set_index('Features')\nfeatures_rank\n\n# les features les plus li\u00e9es au taux de survie, sont le sexe, le titre, la classe, la cat\u00e9gorie de famille (telle qu'on l'a cr\u00e9e).\n# on peut voit notamment que l'age et le prix du billet ne sont pas si important","f422b021":"ranked_features=pd.Series(ordered_feature.scores_,index=X_train.columns)\nplt.figure(figsize=(8, 8))\nranked_features.nlargest(len(X_train.columns)).sort_values(ascending=True).plot(kind='barh')\nplt.show()","3c697951":"from sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC,LinearSVC\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n\nseed = 47\n# preparation des mod\u00e9les\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('SGD', SGDClassifier()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier(3)))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('EXT', ExtraTreesClassifier()))\nmodels.append(('RF', RandomForestClassifier()))\nmodels.append(('ADAB', AdaBoostClassifier()))\nmodels.append(('GDB', GradientBoostingClassifier()))\nmodels.append(('SVM', SVC()))\nmodels.append(('LSVC', LinearSVC()))\nmodels.append(('XGB', XGBClassifier()))\n# \u00e9valuation des mod\u00e9les\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=5, random_state=seed, shuffle=True)\n    cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring,n_jobs=-1)\n    results.append(cv_results)\n    names.append(name)\n    msg = f\"{name:<5}: Mean={cv_results.mean():-<10.3f}Median={np.median(cv_results):-<10.3f}std={cv_results.std():.4f}\"\n    print(msg)\n# graphe de comparaison en boxplot \nfig = plt.figure()\nfig.suptitle('Comparaison des Algorithmes')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","91937a32":"from sklearn.metrics import accuracy_score, log_loss, confusion_matrix\nfrom sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n\n#Mod\u00e9les que l'on va essayer\nclf_lr = LogisticRegression()\nclf_lda = LinearDiscriminantAnalysis()\nclf_rf = RandomForestClassifier()\nclf_gdb = GradientBoostingClassifier()\nclf_svm = SVC()\nclf_lsvc = LinearSVC()\nclf_xgb = XGBClassifier()\n\nclassifiers = [clf_lr, clf_lda, clf_rf, clf_gdb,clf_svm,clf_lsvc,clf_xgb]\n\n### param\u00e9tres de d\u00e9part pour RandomizedSearchCV\n\n### ------------------\n### LogisticRegression\n### ------------------\nparam_lr = {\"penalty\" :          [\"l1\",\"l2\"],\n            \"tol\" :              [0.0001,0.0002,0.0003],\n            \"max_iter\":          [100,300,500,800,1000],\n            \"C\" :                [0.01, 0.1, 1, 10, 100],\n            \"intercept_scaling\": [1, 2, 3, 4],\n            \"solver\":['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n\n### --------------------------\n### LinearDiscriminantAnalysis\n### --------------------------\nparam_lda = {\"solver\" : ['svd', 'lsqr', 'eigen']}\n\n### ----------------------\n### RandomForestClassifier\n### ----------------------\n# Nombre d'abre dans RandomForest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Nombre de features \u00e0 consid\u00e9rer \u00e0 chaque split\nmax_features = ['auto', 'sqrt']\n# Nombre maximum de niveau dans les abres\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Combien d'echantillon au minimum pour s\u00e9parer un noeud\nmin_samples_split = [2, 5, 10]\n# Nombre minimum d'\u00e9chantillons dans chaque feuille\nmin_samples_leaf = [1, 2, 4]\n# M\u00e9thode de s\u00e9l\u00e9ction des \u00e9chantillons\nbootstrap = [True, False]\n\nparam_rf = {'n_estimators':      n_estimators,\n            'criterion':         ['entropy', 'gini'],\n            'max_features':      max_features,\n            'max_depth':         max_depth,\n            'min_samples_split': min_samples_split,\n            'min_samples_leaf':  min_samples_leaf,\n            'bootstrap':         bootstrap}\n\n### --------------------------\n### GradientBoostingClassifier\n### --------------------------\n\nparam_gdb = {\n    \"loss\":              [\"deviance\"],\n    \"learning_rate\":     [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n    \"min_samples_split\": np.linspace(0.1, 0.5, 12),\n    \"min_samples_leaf\":  np.linspace(0.1, 0.5, 12),\n    \"max_depth\":         [3,5,8],\n    \"max_features\":      [\"log2\",\"sqrt\"],\n    \"criterion\":         [\"friedman_mse\",  \"mae\"],\n    \"subsample\":         [0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n    \"n_estimators\":      [10,100,500]}\n\n\n### -------\n### XGBoost\n### -------\n\nparam_xgb = {\n    'n_estimators':     [10,100,500],\n    'colsample_bytree': [0.75,0.8,0.85],\n    'max_depth':        [10,50,100,None],\n    'reg_alpha':        [1],\n    'reg_lambda':       [2, 5, 10],\n    'subsample':        [0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n    'learning_rate':    [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n    'gamma':            [.5,1,2],\n    'min_child_weight': [0.01],\n    'sampling_method':  ['uniform']}\n\n\n### ---------------\n### SVC \n### ---------------\n\nparam_svc = {'gamma':  [1e-3, 1e-2,0.1, 1, 10,100],\n             'C':      [1e-2,0.1,1, 10, 100, 1000],\n             'kernel': ['linear', 'rbf'],\n             'degree': [1,2,3]}\n\n### ---------------\n### LinearSVC \n### ---------------\n\nparam_lsvc = {'C':        [1e-2,0.1,1, 10, 100, 1000],\n              'tol' :     [0.0001,0.0002,0.0003],\n              'max_iter': [100,300,500,800,1000]}\n\n\nparameters = [param_lr, param_lda, param_rf, param_gdb, param_svc, param_lsvc, param_xgb]\n\n\nclf_best_acc = []\nclf_best_params = []\nclf_best_estimator = []\nrnd_searchs = [] \n\n#Recherche sur tous les classifiers\nfor i in range(len(classifiers)):\n    rnd_searchs.append(RandomizedSearchCV(estimator = classifiers[i],\n                                 param_distributions = parameters[i],\n                                 scoring = 'accuracy',\n                                 n_iter = 40,\n                                 cv = 5,\n                                 verbose=0,\n                                 random_state=47,\n                                 n_jobs = -1))\n    \n    print(classifiers[i].__class__.__name__)\n    underline=['-']*len(classifiers[i].__class__.__name__)\n    print(''.join(underline))\n    rnd_searchs[i].fit(X_train, Y_train)\n    print(\"Meilleurs param\u00e9tres :\",rnd_searchs[i].best_params_)\n    print(\"Score : \",rnd_searchs[i].best_score_)\n    clf_best_acc.append(rnd_searchs[i].best_score_)\n    clf_best_params.append(rnd_searchs[i].best_params_)\n    clf_best_estimator.append(rnd_searchs[i].best_estimator_)\n\nprint(\"\")    \nprint(\"RandomSearchCV Termin\u00e9...\")","2672da1b":"modelNames=[]\nfor model in classifiers:\n    modelNames.append(model.__class__.__name__)\nrndtunedScores=pd.Series(clf_best_acc,index=modelNames)\nrndtunedScores=rndtunedScores-0.8\nplt.figure(figsize=(8, 8))\nrndtunedScores.sort_values(ascending=True).plot(kind='barh',left = 0.8)\nplt.show()\n# comme on peut le voir tous les mod\u00e9les sont dans un mouchoir de poche...","a543873f":"from sklearn.model_selection import learning_curve\nimport warnings\nwarnings.filterwarnings('ignore')","1a46a3d9":"### ------------------\n### LogisticRegression\n### ------------------\nparam_lr = {  \"penalty\" : [\"l2\"],\n              \"tol\" : [0.0001,0.00015,0.0002],\n              \"max_iter\": [1,2,5,10,100,200,600,800],\n              \"C\" :[0.01, 0.1, 1],\n              \"intercept_scaling\": [2,3,4],\n              \"solver\":['sag']}\n\nclf_lr = LogisticRegression()","8af75ada":"# CV=5 \/ petite am\u00e9lioration \nbest_clf_lr = GridSearchCV(clf_lr, param_grid = param_lr, cv = 5, verbose = False, n_jobs = -1).fit(X_train,Y_train)\nprint(best_clf_lr.best_params_)\nprint(best_clf_lr.best_score_)","c5245ed2":"# CV = 10 am\u00e9liore encore le r\u00e9sultat\nbest_clf_lr = GridSearchCV(clf_lr, param_grid = param_lr, cv = 10, verbose = False, n_jobs = -1).fit(X_train,Y_train)\nprint(best_clf_lr.best_params_)\nprint(best_clf_lr.best_score_)","9c88736d":"def plotLearningCurve(model):\n    N, train_score, val_score = learning_curve(model, X_train, Y_train,\n                                                  train_sizes=np.linspace(0.1, 1, 10), cv=10)\n\n    # N contient le nombre d'\u00e9l\u00e9ments retenus pour faire l'entrainement\n    print(N)\n    # train_score contient pour toutes les it\u00e9rations de train_sizes (10 ici), les r\u00e9sultat pour chaque cv sur le jeu de train\n    # val_score contient pour toutes les it\u00e9rations de train_sizes (10 ici), les r\u00e9sultat pour chaque cv sur le jeu de validation\n    plt.plot(N, train_score.mean(axis=1), label='train')\n    plt.plot(N, val_score.mean(axis=1), label='validation')\n    plt.xlabel('train_sizes')\n    plt.legend() \n\n\n# on r\u00e9cup\u00e9re le meilleur mod\u00e9le trouv\u00e9 gr\u00e2ce \u00e0 GridSearchCV \nmodel = best_clf_lr.best_estimator_ \nplotLearningCurve(model)","931bb476":"from sklearn.model_selection import train_test_split\n\n# pr\u00e9c\u00e9demment on a vu grace \u00e0 learning curves, que la meilleur taille du dataset pour l'entrainement \u00e9tait la taille de 720\n# on r\u00e9injecte les param\u00e9tres trouv\u00e9s avec GridSearchCV dans notre mod\u00e9le, avec 10% de test_size\n\nmodel_lr = LogisticRegression(C=1,\n                           intercept_scaling=4,\n                           max_iter=10,\n                           penalty='l2',\n                           solver='sag',\n                           tol=0.00015,\n                           random_state=47)                          \nX_tr, X_te, y_tr, y_te = train_test_split(X_train, Y_train, test_size=0.1)\nmodel_lr.fit(X_tr, y_tr)\nprint(\"train : \",model_lr.score(X_tr, y_tr))\nprint(\"test  : \",model_lr.score(X_te, y_te))","3b7c606d":"### ----------------------\n### RandomForestClassifier\n### ----------------------\nn_estimators = [int(x) for x in np.linspace(start = 1300, stop = 1500, num = 10)]\nmax_features = ['sqrt']\nmax_depth = [int(x) for x in np.linspace(20, 40, num = 10)]\nmin_samples_split = [4,5,6]\nmin_samples_leaf = [3,4,5]\nbootstrap = [False]\ncriterion = ['entropy']\n# Create the random grid\nparam_rf = {'n_estimators': n_estimators,\n            'criterion': criterion,\n            'max_features': max_features,\n            'max_depth': max_depth,\n            'min_samples_split': min_samples_split,\n            'min_samples_leaf': min_samples_leaf,\n            'bootstrap': bootstrap}","d24322f8":"# d\u00e9sactiv\u00e9 car prends trop de temps sur Kaggle\n'''\nclf_rf = RandomForestClassifier()\nbest_clf_rf = GridSearchCV(clf_rf, param_grid = param_rf, cv = 5, verbose = True, n_jobs = -1).fit(X_train,Y_train)\nprint(best_clf_rf.best_params_)\nprint(best_clf_rf.best_score_)\n'''","2af6fe34":"'''\n# on r\u00e9cup\u00e9re le meilleur mod\u00e9le trouv\u00e9 gr\u00e2ce \u00e0 GridSearchCV \nmodel = best_clf_rf.best_estimator_ \nplotLearningCurve(model)\n'''","3370a1d4":"model_rf = RandomForestClassifier(bootstrap=False,\n                                  criterion='entropy',\n                                  max_depth=28,\n                                  max_features='sqrt',\n                                  min_samples_leaf=3,\n                                  min_samples_split=5,\n                                  n_estimators= 1477,\n                                  random_state=47)\nX_tr, X_te, y_tr, y_te = train_test_split(X_train, Y_train, test_size=0.3)\nmodel_rf.fit(X_tr, y_tr)\nprint(\"train : \",model_rf.score(X_tr, y_tr))\nprint(\"test  : \",model_rf.score(X_te, y_te))","7a186823":"### -------\n### XGBoost\n### -------\n\nparam_xgb = {\n    'n_estimators':     [100],\n    'colsample_bytree': [0.85],\n    'max_depth':        [80,90],\n    'reg_alpha':        [1],\n    'reg_lambda':       [9,10,11],\n    'subsample':        [0.95],\n    'learning_rate':    [0.05],\n    'gamma':            [.75,1],\n    'min_child_weight': [0.01],\n    'sampling_method':  ['uniform']}","db531360":"clf_xgb = XGBClassifier()\nbest_clf_xgb = GridSearchCV(clf_xgb, param_grid = param_xgb, cv = 5, verbose = False, n_jobs = -1).fit(X_train,Y_train)\nprint(best_clf_xgb.best_params_)\nprint(best_clf_xgb.best_score_)","367209ee":"best_clf_xgb = GridSearchCV(clf_xgb, param_grid = param_xgb, cv = 10, verbose = False, n_jobs = -1).fit(X_train,Y_train)\nprint(best_clf_xgb.best_params_)\nprint(best_clf_xgb.best_score_)","a7734eb1":"# on r\u00e9cup\u00e9re le meilleur mod\u00e9le trouv\u00e9 gr\u00e2ce \u00e0 GridSearchCV \nmodel = best_clf_xgb.best_estimator_ \nplotLearningCurve(model)","37e47f1d":"model_xgb = XGBClassifier(n_estimators =100,\n                            colsample_bytree = 0.85,\n                            max_depth=80,\n                            reg_alpha=1,\n                            reg_lambda=10,\n                            subsample=0.95,\n                            learning_rate=0.05,\n                            gamma=1,\n                            min_child_weight=0.01,\n                            sampling_method='uniform',\n                            random_state=47)\nX_tr, X_te, y_tr, y_te = train_test_split(X_train, Y_train, test_size=0.05)\nmodel_xgb.fit(X_tr, y_tr)\nprint(\"train : \",model_xgb.score(X_tr, y_tr))\nprint(\"test  : \",model_xgb.score(X_te, y_te))","08a8caa0":"temp = pd.read_csv(\"..\/input\/titanic\/test.csv\")\nId = temp.PassengerId\n\n# Splitting dataset into test\ntest = ds[len(dstrain):]\nX_test = test.drop(\"Survived\", axis=1)\n\nfinal_predictions_lr = model_lr.predict(X_test)\nfinal_predictions_rf = model_rf.predict(X_test)\nfinal_predictions_xgb = model_xgb.predict(X_test)\n\n\noutputlr = pd.DataFrame({'PassengerId': Id, 'Survived': final_predictions_lr.astype(int)})\noutputrf = pd.DataFrame({'PassengerId': Id, 'Survived': final_predictions_rf.astype(int)})\noutputxgb = pd.DataFrame({'PassengerId': Id, 'Survived': final_predictions_xgb.astype(int)})\n#outputlr.to_csv('..\/output\/submissionlr.csv', index=False)\n#outputrf.to_csv('..\/output\/submissionrf.csv', index=False)\n#outputxgb.to_csv('..\/output\/submissionxgb.csv', index=False)","63686901":"from sklearn.ensemble import VotingClassifier","e43127d0":"model_vt = VotingClassifier([('LR', model_lr),\n                            ('RF', model_rf),\n                            ('XGB', model_xgb)],\n                            voting='hard')\n# Diff\u00e9rence entre voting = soft \/ hard :\n# voting = soft => on additionne les probabilit\u00e9s de chaque classe pour tous les classifiers, on prend la classe qui a la somme\n# de proba la plus importante (fonctionne bien si pas trop de disparit\u00e9s entre classifiers)\n# voting = hard => on prend la classe qui a \u00e9t\u00e9 pr\u00e9dite par le plus de classfiers\n\nmodel_vt.fit(X_train, Y_train)\nprint(model.score(X_train, Y_train))","23a73776":"final_predictions_vt = model_vt.predict(X_test)\noutputvt = pd.DataFrame({'PassengerId': Id, 'Survived': final_predictions_vt.astype(int)})\n#outputvt.to_csv('..\/output\/submissionvt.csv', index=False)","70b0ec3e":"### Cr\u00e9ation d'une feature Titre, d'apr\u00e9s le contenu du nom","c6543579":"### Remplacement des valeurs cat\u00e9goriques ","8eb71683":"### XGBoost fine tuning","afa1cb38":"### Port d'embarquement absent","ebd079ee":"### RandomizedSearchCV\n\nva nous permettre de trouver rapidement quels sont les meilleurs Hyperparam\u00e9tres qu'il faudra ensuite r\u00e9gler plus finement","3804e48c":"### LogisticRegression fine tuning","c409df6a":"### DecisionTree","6d6497d7":"### SelectKBest pour obtenir l'importance des features par un test d'ind\u00e9pendance chi2\n\nLes best features n'ont rien \u00e0 voir avec les classifiers de type ensemble","53877cbf":"# 6) R\u00e9sultats\n\nScores Kaggle :\n- Logistic Regression : **0.78947**<br>\n- RandomForest        : **0.75598**<br>\n- XGBoost             : **0.78229**<br>\n- Voting              : **0.76794**<br>\n\n","d1be76e6":"# 2) Nettoyage des donn\u00e9es \/ Feature Engineering ","1f4af855":"### S\u00e9paration des donn\u00e9es Train \/ Test","aff3d6d9":"### Calcul du nombre de personnes accompagnantes","dcbadf22":"### Normalisation des donn\u00e9es num\u00e9riques","30b06b4c":"### Avec ExtraTreesClassifier\n\nIl est probable qu'avec de la cross validation, le r\u00e9sultat ne sera plus du tout le m\u00eame pour les TreeClassifier","efdf033e":"### calcul du Fare par personne","109ea192":"# 1) Analyse des donn\u00e9es","8a5a7b8a":"### Remplacement des valeur nan dans la colonne Age\n\nOn va utiliser la m\u00eame distribution que les Age pr\u00e9sents pour l'instant","77a608be":"### Comparaisons de plusieurs mod\u00e9les avec cross validation","43574fcd":"## Visualisations","d13ab369":"# 5) Tuning des mod\u00e9les choisis\n\nNous allons conserver les mod\u00e9les suivants :<BR>\n\n<code>LR   : Mean=0.827-----Median=0.831-----std=0.0283\nLDA  : Mean=0.826-----Median=0.837-----std=0.0254\nRF   : Mean=0.796-----Median=0.809-----std=0.0369\nGDB  : Mean=0.827-----Median=0.837-----std=0.0241\nSVM  : Mean=0.819-----Median=0.820-----std=0.0163\nLSVC : Mean=0.829-----Median=0.843-----std=0.0285\nXGB  : Mean=0.804-----Median=0.809-----std=0.0400<\/code>    \n<BR>\nOn commencera par RandomizedSearchCV pour trouver les meilleurs r\u00e9glages, puis GridSearchCV pour affiner","fa064f41":"### RandomForestClassifier fine tuning","906b3635":"### GridSearchCV pour r\u00e9gler finement les param\u00e9tres\n\n**LogisticRegression:<br>**\n{'tol': 0.0003, 'solver': 'sag', 'penalty': 'l2', 'max_iter': 800, 'intercept_scaling': 2, 'C': 0.1}<br>\n0.824913690289373<br>\n<br>\n**LinearDiscriminantAnalysis:<br>**\n{'solver': 'svd'}<br>\n0.8282719226664993<br>\n<br>\n**RandomForestClassifier :<br>**\n{'n_estimators': 1400, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 30, 'criterion': 'entropy', 'bootstrap': False}<br>\n0.8327600276191074<br>\n<br>\n**GradientBoostingClassifier:<br>**\n{'subsample': 0.9, 'n_estimators': 500, 'min_samples_split': 0.24545454545454548, 'min_samples_leaf': 0.13636363636363638, 'max_features': 'sqrt', 'max_depth': 5, 'loss': 'deviance', 'learning_rate': 0.075, 'criterion': 'friedman_mse'}<br>\n0.8350072186303434<br>\n<br>\n**SVC:<br>**\n{'kernel': 'rbf', 'gamma': 0.1, 'degree': 1, 'C': 1}<br>\n0.8316238779737617<br>\n<br>\n**LinearSVC:<br>**\n{'tol': 0.0003, 'max_iter': 100, 'C': 0.1}<br>\n0.8305191136777352<br>\n<br>\n**XGBClassifier:<br>**\n{'subsample': 0.95, 'sampling_method': 'uniform', 'reg_lambda': 10, 'reg_alpha': 1, 'n_estimators': 100, 'min_child_weight': 0.01, 'max_depth': 100, 'learning_rate': 0.05, 'gamma': 1, 'colsample_bytree': 0.85}<br>\n0.8361559224154165<br>\n<br>\n - On ne fera pas mieux avec LDA que 0.825, car aucun autre HyperParam\u00e9tre \u00e0 r\u00e9gler\n - On peut probablement encore gagner un peu en accuracy en r\u00e9glant finement les autres mod\u00e9les\n - Essayons avec LogisticRegression, RandomForest et XGBoost\n","7e80ac3b":"### Suppression des colonnes inutiles","d9dcbb28":"# 3) Preprocessing des donn\u00e9es","cd536a84":"### Analyse de la feature Cabine et cr\u00e9ation d'une feature Deck","6f3d7ec7":"# 4) Utilisation de mod\u00e9le basiques","6b2da01c":"### Bonus - VotingClassifier","23b968cd":"# Titanic dataset\n\n**comp\u00e9tition Kaggle - C\u00e9dric LEBOCQ<br>**\n\n### 1) Analyse des donn\u00e9es\n\n### 2) Nettoyage des donn\u00e9es \/ Feature Engineering \n\n### 3) Preprocessing des donn\u00e9es\n\n### 4) Utilisation de mod\u00e9le basiques\n\n### 5) Tuning des mod\u00e9les\n\n### 6) Resultats ","ae2432ea":"### Premier tests avec RandomForestClassifier"}}