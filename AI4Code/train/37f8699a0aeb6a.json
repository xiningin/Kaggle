{"cell_type":{"0362fce0":"code","396aff42":"code","4fa7606a":"code","2f6b41f8":"code","ea143b74":"code","9e73dec8":"code","4540d873":"code","b74f7a04":"code","607006d7":"code","69748f79":"code","14da714f":"code","7c785083":"code","4480d837":"code","d1f0d047":"code","0bb66828":"code","7632b470":"code","bdc33cde":"markdown","60de7f94":"markdown","b384f70d":"markdown"},"source":{"0362fce0":"import gc\nimport os\nimport time\nimport random\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport glob\nimport PIL\nfrom PIL import Image, ImageEnhance, ImageOps\nfrom collections import OrderedDict\nfrom joblib import Parallel, delayed\n\nfrom tqdm import tqdm, tqdm_notebook\n\nimport torch\nfrom torch import nn, cuda\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nimport torchvision.models as M\nfrom torch.utils.data import Dataset, DataLoader","396aff42":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nSEED = 2019\nseed_everything(SEED)","4fa7606a":"def crop_boxing_img(img_name, margin=16) :\n    if img_name.split('_')[0] == \"train\" :\n        PATH = TRAIN_IMAGE_PATH\n        data = train_df\n    elif img_name.split('_')[0] == \"test\" :\n        PATH = TEST_IMAGE_PATH\n        data = test_df\n        \n    img = PIL.Image.open(os.path.join(PATH, img_name))\n    pos = data.loc[data[\"img_file\"] == img_name, \\\n                   ['bbox_x1','bbox_y1', 'bbox_x2', 'bbox_y2']].values.reshape(-1)\n\n    width, height = img.size\n    x1 = max(0, pos[0] - margin)\n    y1 = max(0, pos[1] - margin)\n    x2 = min(pos[2] + margin, width)\n    y2 = min(pos[3] + margin, height)\n\n    return img.crop((x1,y1,x2,y2))","2f6b41f8":"class TestDataset(Dataset):\n    def __init__(self, test_imgs, transforms=None):\n        self.test_imgs = test_imgs\n        self.transform = transforms\n        \n    def __len__(self):\n        return len(self.test_imgs)\n    \n    def __getitem__(self, idx):\n        \n        image = self.test_imgs[idx].convert('RGB')\n            \n        if self.transform:\n            image = self.transform(image)\n            \n        return image        ","ea143b74":"df = pd.read_csv('..\/input\/2019-3rd-ml-month-with-kakr\/train.csv')\nDATA_PATH = '..\/input\/2019-3rd-ml-month-with-kakr'\nTEST_IMAGE_PATH = os.path.join(DATA_PATH, 'test')\ntest_df = pd.read_csv('..\/input\/2019-3rd-ml-month-with-kakr\/test.csv')\nnum_classes = df['class'].nunique()","9e73dec8":"%%time\n\nx_test = Parallel(n_jobs=4)(\n    delayed(lambda x: crop_boxing_img(x))(x) for x in tqdm(test_df['img_file']))","4540d873":"# target_size = (224, 224)\n\n# data_transforms = transforms.Compose([\n#         transforms.Resize(target_size),\n#         transforms.RandomResizedCrop(target_size, scale=(0.8,1.0)),\n#         transforms.RandomHorizontalFlip(),\n#         transforms.ToTensor(),\n#         transforms.Normalize(\n#             [0.485, 0.456, 0.406], \n#             [0.229, 0.224, 0.225])\n#     ])","b74f7a04":"# resnet50_weights_path = Path('..\/input\/car-4fold-weights\/')\n# weight_list = os.listdir(resnet50_weights_path)\n# weight_list","607006d7":"# %%time\n\n# batch_size = 1\n# tta = 5\n# test_dataset = TestDataset(x_test, transforms=data_transforms)\n# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n# total_num_models = len(weight_list)*tta \n\n# model = M.resnet50() \n# model.fc = nn.Linear(2048, num_classes)\n# model.cuda()\n\n# resnet50_prediction = np.zeros((len(test_dataset), num_classes))\n\n# for i, weight in enumerate(weight_list):\n#     print(\"weight {} prediction starts\".format(i+1))\n    \n#     for _ in range(tta):\n#         print(\"tta {}\".format(_+1))\n\n#         model.load_state_dict(torch.load(resnet50_weights_path \/ weight))\n\n#         model.eval()\n        \n#         prediction = np.zeros((len(test_dataset), num_classes))\n#         with torch.no_grad():\n#             for i, images in enumerate(test_loader):\n#                 images = images.cuda()\n\n#                 preds = model(images).detach()\n#                 prediction[i * batch_size: (i+1) * batch_size] = preds.cpu().numpy()\n#                 resnet50_prediction = resnet50_prediction + prediction\n#         del prediction\n        \n# resnet50_prediction \/= total_num_models\n\n# del test_dataset\n# del test_loader\n# gc.collect()","69748f79":"class Bottleneck(nn.Module):\n    \"\"\"\n    Base class for bottlenecks that implements `forward()` method.\n    \"\"\"\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = self.se_module(out) + residual\n        out = self.relu(out)\n\n        return out\n\nclass SEBottleneck(Bottleneck):\n    \"\"\"\n    Bottleneck for SENet154.\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes * 2)\n        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n                               stride=stride, padding=1, groups=groups,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(planes * 4)\n        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n        \n        \nclass SEModule(nn.Module):\n\n    def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(channels, channels \/\/ reduction, kernel_size=1,\n                             padding=0)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(channels \/\/ reduction, channels, kernel_size=1,\n                             padding=0)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        module_input = x\n        x = self.avg_pool(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return module_input * x\n\nclass SENet(nn.Module):\n\n    def __init__(self, block, layers, groups, reduction, dropout_p=0.2,\n                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n                 downsample_padding=1, num_classes=1000):\n        \"\"\"\n        Parameters\n        ----------\n        block (nn.Module): Bottleneck class.\n            - For SENet154: SEBottleneck\n            - For SE-ResNet models: SEResNetBottleneck\n            - For SE-ResNeXt models:  SEResNeXtBottleneck\n        layers (list of ints): Number of residual blocks for 4 layers of the\n            network (layer1...layer4).\n        groups (int): Number of groups for the 3x3 convolution in each\n            bottleneck block.\n            - For SENet154: 64\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models:  32\n        reduction (int): Reduction ratio for Squeeze-and-Excitation modules.\n            - For all models: 16\n        dropout_p (float or None): Drop probability for the Dropout layer.\n            If `None` the Dropout layer is not used.\n            - For SENet154: 0.2\n            - For SE-ResNet models: None\n            - For SE-ResNeXt models: None\n        inplanes (int):  Number of input channels for layer1.\n            - For SENet154: 128\n            - For SE-ResNet models: 64\n            - For SE-ResNeXt models: 64\n        input_3x3 (bool): If `True`, use three 3x3 convolutions instead of\n            a single 7x7 convolution in layer0.\n            - For SENet154: True\n            - For SE-ResNet models: False\n            - For SE-ResNeXt models: False\n        downsample_kernel_size (int): Kernel size for downsampling convolutions\n            in layer2, layer3 and layer4.\n            - For SENet154: 3\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models: 1\n        downsample_padding (int): Padding for downsampling convolutions in\n            layer2, layer3 and layer4.\n            - For SENet154: 1\n            - For SE-ResNet models: 0\n            - For SE-ResNeXt models: 0\n        num_classes (int): Number of outputs in `last_linear` layer.\n            - For all models: 1000\n        \"\"\"\n        super(SENet, self).__init__()\n        self.inplanes = inplanes\n        if input_3x3:\n            layer0_modules = [\n                ('conv1', nn.Conv2d(3, 64, 3, stride=2, padding=1,\n                                    bias=False)),\n                ('bn1', nn.BatchNorm2d(64)),\n                ('relu1', nn.ReLU(inplace=True)),\n                ('conv2', nn.Conv2d(64, 64, 3, stride=1, padding=1,\n                                    bias=False)),\n                ('bn2', nn.BatchNorm2d(64)),\n                ('relu2', nn.ReLU(inplace=True)),\n                ('conv3', nn.Conv2d(64, inplanes, 3, stride=1, padding=1,\n                                    bias=False)),\n                ('bn3', nn.BatchNorm2d(inplanes)),\n                ('relu3', nn.ReLU(inplace=True)),\n            ]\n        else:\n            layer0_modules = [\n                ('conv1', nn.Conv2d(3, inplanes, kernel_size=7, stride=2,\n                                    padding=3, bias=False)),\n                ('bn1', nn.BatchNorm2d(inplanes)),\n                ('relu1', nn.ReLU(inplace=True)),\n            ]\n        # To preserve compatibility with Caffe weights `ceil_mode=True`\n        # is used instead of `padding=1`.\n        layer0_modules.append(('pool', nn.MaxPool2d(3, stride=2,\n                                                    ceil_mode=True)))\n        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n        self.layer1 = self._make_layer(\n            block,\n            planes=64,\n            blocks=layers[0],\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=1,\n            downsample_padding=0\n        )\n        self.layer2 = self._make_layer(\n            block,\n            planes=128,\n            blocks=layers[1],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer3 = self._make_layer(\n            block,\n            planes=256,\n            blocks=layers[2],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer4 = self._make_layer(\n            block,\n            planes=512,\n            blocks=layers[3],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.avg_pool = nn.AvgPool2d(7, stride=1)\n        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n                    downsample_kernel_size=1, downsample_padding=0):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=downsample_kernel_size, stride=stride,\n                          padding=downsample_padding, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n                            downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups, reduction))\n\n        return nn.Sequential(*layers)\n\n    def features(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def logits(self, x):\n        x = self.avg_pool(x)\n        if self.dropout is not None:\n            x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.logits(x)\n        return x\n\n\ndef senet154(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEBottleneck, [3, 8, 36, 3], groups=64, reduction=16,\n                  dropout_p=0.2, num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['senet154'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model","14da714f":"target_size = (224, 224)\n\ndata_transforms = transforms.Compose([\n            transforms.Resize(target_size),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                [0.485, 0.456, 0.406], \n                [0.229, 0.224, 0.225])\n])","7c785083":"senet154_weights_path = Path('..\/input\/car-senet142\/')\nweight_list = os.listdir(senet154_weights_path)\nweight_list","4480d837":"%%time\n\nbatch_size = 1\ntta = 2\ntest_dataset = TestDataset(x_test, transforms=data_transforms)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\ntotal_num_models = len(weight_list)*tta \n\nmodel = senet154(num_classes=1000, pretrained=None)\nmodel.last_linear = nn.Linear(model.last_linear.in_features, num_classes)\nmodel.cuda()\n\nsenet154_prediction = np.zeros((len(test_dataset), num_classes))\n\nfor i, weight in enumerate(weight_list):\n    print(\"weight {} prediction starts\".format(i+1))\n    \n    for _ in range(tta):\n        print(\"tta {}\".format(_+1))\n        model.load_state_dict(torch.load(senet154_weights_path \/ weight))\n\n        model.eval()\n        \n        prediction = np.zeros((len(test_dataset), num_classes))\n        with torch.no_grad():\n            for i, images in enumerate(test_loader):\n                images = images.cuda()\n\n                preds = model(images).detach()\n                prediction[i * batch_size: (i+1) * batch_size] = preds.cpu().numpy()\n                senet154_prediction = senet154_prediction + prediction\n        del prediction\n    \n    \nsenet154_prediction \/= total_num_models","d1f0d047":"resnext_weights_path = Path('..\/input\/car-resnext-weights\/')\nweight_list = os.listdir(resnext_weights_path)\nweight_list","0bb66828":"%%time\n\nbatch_size = 1\ntta = 4\ntotal_num_models = len(weight_list)*tta \n\nmodel = torch.hub.load('facebookresearch\/WSL-Images', 'resnext101_32x16d_wsl')\nmodel.fc = nn.Sequential(nn.Dropout(0.2), nn.Linear(2048, num_classes))\nmodel.cuda()\n\nresnext_prediction = np.zeros((len(test_dataset), num_classes))\n\nfor i, weight in enumerate(weight_list):\n    print(\"weight {} prediction starts\".format(i+1))\n    \n    for _ in range(tta):\n        print(\"tta {}\".format(_+1))\n        model.load_state_dict(torch.load(resnext_weights_path \/ weight))\n\n        model.eval()\n        \n        prediction = np.zeros((len(test_dataset), num_classes))\n        with torch.no_grad():\n            for i, images in enumerate(test_loader):\n                images = images.cuda()\n\n                preds = model(images).detach()\n                prediction[i * batch_size: (i+1) * batch_size] = preds.cpu().numpy()\n                resnext_prediction = resnext_prediction + prediction\n        del prediction\n        \n    \nresnext_prediction \/= total_num_models","7632b470":"all_prediction = (senet154_prediction\/2) + (resnext_prediction\/2)\n# all_prediction = (resnet50_prediction\/3) + (resnext_prediction\/3) + (senet154_prediction\/3)\n\nresult = np.argmax(all_prediction, axis=1)\nsubmission = pd.read_csv('..\/input\/2019-3rd-ml-month-with-kakr\/sample_submission.csv')\nsubmission[\"class\"] = result\nsubmission[\"class\"].replace(0, 196, inplace=True)\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.head()","bdc33cde":"## Senet154","60de7f94":"+ private \ub9ac\ub354\ubcf4\ub4dc 2\ub4f1\uc5d0 \ub7ad\ud06c\ud55c inference kernel\uc785\ub2c8\ub2e4.\n+ pytorch \uae30\ubc18 inference \ucee4\ub110\uc774\uace0 \ub2e4\ub978 \ub300\ud68c \ud560 \ub54c\ub3c4 \ucc38\uace0\ud558\uc2dc\uba74 \ub3c4\uc6c0\ub420\uac83 \uac19\uc2b5\ub2c8\ub2e4.\n+ \uc0ac\uc2e4 \uc21c\uc704 \ubcc4\ub85c \uae30\ub300 \uc548\ud558\uace0 \uc57d 1\ub2ec \uc804\uc5d0 \ud559\uc2b5 \uadf8\ub9cc\ub454 \ub300\ud68c\uc600\uc73c\ub098 \uc5b4\ucc0c\ud558\uc5ec 2\ub4f1\uc5d0 \ub7ad\ud06c\ud558\uac8c \ub410\ub124\uc694. \uc544\ub9c8\ub3c4 40\uac1c\uc758 \ubaa8\ub378 (4fold tta2 + 8fold tta4\uc744 \uc559\uc0c1\ube14 \ud588\uae30 \ub54c\ubb38\uc5d0 robust\ud55c \uacb0\uacfc\ub97c \uc5bb\uc744 \uc218 \uc788\uc5c8\ub358\uac83 \uac19\uc2b5\ub2c8\ub2e4. (\uc2e4\uc804 \ub300\ud68c\ub294 inference \uc2dc\uac04\uc5d0 \uc81c\ud55c\uc744 \ub450\uae30 \ub54c\ubb38\uc5d0 \uc6ec\ub9cc\ud574\uc11c\ub294 \uc774\ub807\uac8c \ud558\uc9c0 \ubabb\ud569\ub2c8\ub2e4)\n+ \uc2e4\uc804 \ub300\ud68c\uc600\uc73c\uba74 \uc544\ub9c8 10 fold\ub85c \uc9c4\ud589\ud558\uc600\uc744\uac83 \uac19\uace0, \ub9ce\uc740 \uc2dc\ub3c4\ub97c \ud588\uc744\uac83 \uac19\uc2b5\ub2c8\ub2e4. ex) \uc11c\ub85c \ub2e4\ub978 \uc774\ubbf8\uc9c0 \uc0ac\uc774\uc988 \uc559\uc0c1\ube14, focal loss\uc640 \uac19\uc740 \ub2e4\ub978 criterion.\n+ public \ub9ac\ub354\ubcf4\ub4dc\uac00 10\ub4f1 \ubc16\uc5d0 \ubc00\ub824\ub0ac\uc74c\uc5d0\ub3c4 \ubd88\uad6c\ud558\uace0 \ucd5c\uc885 2\ub4f1\ud55c\uac78 \ubcf4\uba74, \uc559\uc0c1\ube14\uc758 \ud798\uc774 \uc81c\ub300\ub85c \ubc1c\ud718\ub41c\uac78 \uc54c \uc218 \uc788\ub2e4\uace0 \uc0dd\uac01\ud569\ub2c8\ub2e4.\n+ senet142 4fold tta2 (imagenet pretrained, \uc6cc\ub099 \ucee4\uc11c inference \ud558\ub294\ub370\ub3c4 \uc624\ub798\uac78\ub9bd\ub2c8\ub2e4) + resnext101_32x16d (instagram pretrained) 8fold\ub97c \uc559\uc0c1\ube14 \ud588\uc2b5\ub2c8\ub2e4.\n\n+ \ud559\uc2b5\uc740 public\uc5d0 \uacf5\uac1c\ud55c \ucee4\ub110 \uae30\ubc18\uc785\ub2c8\ub2e4. \ud2b9\ubcc4\ud55c trick\uc740 \uc548\uc37c\uc73c\uba70, \ucd5c\ub300\ud55c \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130\ub4e4\uc744 \uace0\uce58\uba74\uc11c fitting \ud558\ub3c4\ub85d \uc870\uc808\ud588\uc2b5\ub2c8\ub2e4.\n+ \uc774\ubc88 \ub300\ud68c\ub97c \ud1b5\ud574\uc11c \ub9ce\uc740 \ubd84\ub4e4\uc774 \ub525\ub7ec\ub2dd \ub300\ud68c \ub9ac\ub354\ubcf4\ub4dc\uc5d0 \ub098\ud0c0\ub098\uc168\uc73c\uba74 \uc88b\uaca0\uc2b5\ub2c8\ub2e4 :)","b384f70d":"## Resnet50"}}