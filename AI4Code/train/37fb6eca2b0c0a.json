{"cell_type":{"047fd700":"code","40d99657":"code","ce9c44fb":"code","38ccb456":"code","b8f94130":"code","35f0beb9":"code","49fb8d6e":"code","cf2013dc":"code","32da140b":"code","7fbb5753":"code","f9373f11":"code","a0f1cb41":"code","dddf6608":"code","4e77c1f4":"code","369f997d":"code","5e33cc7b":"markdown","49624cac":"markdown","970f581c":"markdown","ef78ebd2":"markdown","c67c36eb":"markdown","39e19157":"markdown","d1120d9c":"markdown","ba8d498a":"markdown","ae8aa17d":"markdown","bcca373d":"markdown","d7469d4b":"markdown","d5f7778c":"markdown"},"source":{"047fd700":"\n# popular EDA libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nfrom sklearn import *\nfrom sklearn.model_selection import *\nfrom sklearn.preprocessing import *\nfrom sklearn.decomposition import PCA\nimport statsmodels.api as sm\nfrom tqdm.notebook import tqdm\nimport gc\n\n# model related libraries\n\n\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow_addons as tfa\nimport numpy as np\nfrom keras.layers import Dense, BatchNormalization, Input\nfrom keras.models import Model\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report, confusion_matrix, log_loss \nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\ntf.random.set_seed(2) # for reproducible results\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n%matplotlib inline","40d99657":"train = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntargets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntargets_non_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\ntest = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsample = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\n\n# let's see few rows of both train and test\ntrain.head()\n","ce9c44fb":"# for test data\ntest.head()","38ccb456":"# targets corresponding to train data\ntargets_scored.head()","b8f94130":"# Shapes of data\n\nprint('train data shape - ',train.shape)\nprint('test data shape - ',test.shape)\nprint('Different MoA labels - ',targets_scored.shape[1]-1)","35f0beb9":"# printing the unique values what they are\nprint(np.unique(train['cp_type']))\n\n\n#let's see the distribution of persons in these classes\ntrain['cp_type'].value_counts().plot(kind='bar',figsize=[10,3])\ntrain['cp_type'].value_counts()","49fb8d6e":"# check if labels for 'ctl_vehicle' are all 0.\ntrain1 = train.merge(targets_scored, on='sig_id')\ntarget_cols = [c for c in targets_scored.columns if c not in ['sig_id']]\ncols = target_cols + ['cp_type']\ntrain1[cols].groupby('cp_type').sum().sum(1)","cf2013dc":"# constrcut train&test data except 'cp_type'=='ctl_vehicle' data\nprint(train.shape, test.shape)\ntrain1 = train1[train1['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest1 = test[test['cp_type']!='ctl_vehicle'].reset_index(drop=True)\nprint(train1.shape, test1.shape)","32da140b":"dataset = pd.concat([train1, test1],sort=False, ignore_index= True)\ndataset.head()\n\n# the values corresponding to test data labels become NaNs after concatenation","7fbb5753":"# Applying one hot encoding to the categorical_features\n\nobj_cols =  list(dataset.select_dtypes(include = 'object').columns[1:])   # not taking sig_id\nobj_cols.append('cp_time')\ndataset = pd.get_dummies(dataset, columns = obj_cols)\nobj_cols","f9373f11":"# firstly drop sig_id as the necessary test_id is already presented in test1\n\ndataset.drop('sig_id', axis=1, inplace = True)\n\n# slicing\ntarget_cols = targets_scored.columns[1:]  # not icluding sig_id\nfeature_cols = [c for c in dataset.columns if c not in target_cols ]\nx_train = dataset[feature_cols][0:len(train1)]  # columns other than cols\ny_train = dataset[target_cols][0:len(train1)]\nx_test = dataset[feature_cols][len(train1):]\n\nx_train.shape","a0f1cb41":"\ndef nn_model(input_shape):\n\n    inputs = Input(shape = input_shape)\n    x = tf.keras.layers.BatchNormalization()(inputs)\n    x = tf.keras.layers.Dense(1024, activation= tfa.activations.gelu)(inputs)\n    x = tfa.layers.GroupNormalization(groups = 32)(x)\n    x = tf.keras.layers.Dense(512, activation= tfa.activations.gelu)(x)\n    x = tfa.layers.GroupNormalization(groups = 16)(x)\n    x = tf.keras.layers.Dense(256, activation= tfa.activations.gelu)(x)\n    x = tfa.layers.GroupNormalization(groups = 8)(x)\n    outputs = tf.keras.layers.Dense(206,activation='sigmoid')(x)\n\n    # model\n    return tf.keras.models.Model(inputs,outputs)\n    ","dddf6608":"\nN_STARTS = 6\n\n\nval_pred = y_train.copy()\n\n# making an array for rows same as x_test\ntest_pred = np.zeros((x_test.shape[0],206))\n\nval_pred.loc[:, y_train.columns] = 0\n\nfor seed in tqdm(range(N_STARTS)):\n    for n, (tr, tv) in enumerate(KFold(n_splits=7, random_state=seed, shuffle=True).split(y_train)):\n        print(f'Fold {n}')\n        \n        \n        model = nn_model(len(x_train.columns))\n        \n        # using Stochastic Weight averaging\n        model.compile(optimizer=tfa.optimizers.SWA(tf.optimizers.Adam(lr = 0.001), start_averaging = 9, average_period = 6),\n                      loss='binary_crossentropy', metrics = None )\n        \n        # Callbacks\n        \n        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='min')\n        # for saving best weights after each\n        file_path = str(n) + \"weights.best.hdf5\"\n        \n        \n        # stochastic weight averaging uses average model checkpoint\n        avg_checkpoint = tfa.callbacks.AverageModelCheckpoint(filepath= file_path, monitor='val_loss', save_best_only=True,verbose=2,update_weights=True,mode='min')\n        \n        # early stoping\n        early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience= 5)\n        \n        history = model.fit(x_train.values[tr],\n                  y_train.values[tr],\n                  validation_data=(x_train.values[tv], y_train.values[tv]),\n                  epochs=20, batch_size=64,\n                  callbacks=[reduce_lr_loss, avg_checkpoint, early]\n                 )\n        \n        \n        # loading best weights for prediction\n        model.load_weights(file_path)\n\n        test_predict = model.predict(x_test.values)\n        val_predict = model.predict(x_train.values[tv])\n        \n        test_pred += test_predict\n        val_pred.loc[tv, y_train.columns] += val_predict\n        print('')\n        \n        \ntest_pred \/= ((n+1) * N_STARTS)\nval_pred.loc[:, y_train.columns] \/= N_STARTS        ","4e77c1f4":"# making predictions on test data\n# firstly making a zero array of test data label shape\n\npred_array = np.zeros((test.shape[0],sample.shape[1]-1))\n\n# Replacing those rows\n# where 'cp_type'= !'ctl_vehicle'\npred_rows = [ c for c in sample.sig_id.values if c in test1.sig_id.values]\n\n# collecting indexes of these rows\nindex_pred =  sample[sample.sig_id.isin(pred_rows)][sample.columns[1:]].index\n\n\n# and now for other rows replace it with pred\n\nc = 0\nfor i in list(index_pred):\n    pred_array[i,:] = test_pred[c,:]\n    c +=1","369f997d":"# submitting file\n\nsample[sample.columns[1:]] = pred_array\n\n\nsample.to_csv('submission.csv', index=False)","5e33cc7b":"## Prediction on Test Data","49624cac":"We don't need to pass any metric here because the loss we are using is actually a type of average log loss","970f581c":"# MODEL BUILDING","ef78ebd2":"**Few things I tried**\n\n* Using Add SWA(stochastic weights averaging between different folds in kfold), ReduceLROnPlateau, GroupNormalization\n* Uses Gelu as activation function\n* Remove cp_type = ctrl_vehicle during training\n* Averaging over six random seeds","c67c36eb":"# Loading Data","39e19157":"Nice! we prepared the data let's slicing train and test back","d1120d9c":"## Concatenate Train and test\n\nIt is better to concatenate train and test to perform same transformation","ba8d498a":" Labels for ctrl_vehicle are all 0","ae8aa17d":"The predictions we are making on x_test is only for those when 'cp_type'= !'ctl_vehicle'\nwe need to add those in the original data making all other 0","bcca373d":"If you find this notebook helpful, feel free to **Upvote**","d7469d4b":"## Description of Data\n\n* Train data contains\n    - cp_type - indicates samples treated with a compound (cp_vehicle) or with a control perturbation (ctrl_vehicle); control perturbations have **no MoAs(Will remove for training Model)**\n    - cp_time and cp_dose indicate treatment duration (24, 48, 72 hours) and dose (high or low)\n      \n* We have to predict the probabilities corresponding to 206 MoAs\n* Metric Used - Average Log loss","d5f7778c":"Firstly we remove any label whose cp_type belong to ctrl_vehicle as it doesn't contain MoAs"}}