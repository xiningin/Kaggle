{"cell_type":{"eb1f985f":"code","446efd97":"code","10b87b08":"code","21d1f8ac":"code","ac9a613c":"code","d8073ecf":"code","96432112":"code","e3c3b66d":"code","33fa5c6f":"code","cd8b7982":"code","5fd499e6":"code","2b6be064":"code","f46b02a6":"code","13181d57":"code","08354882":"code","62c9ba2c":"code","74dcab3d":"markdown","9d1e5ae8":"markdown","4fe5fd9b":"markdown","ce7a1498":"markdown","4495955e":"markdown","c669b99f":"markdown","fd7b1a4e":"markdown","2a29827f":"markdown","5ca28270":"markdown","e4743200":"markdown","7577a4b1":"markdown","33140339":"markdown","fa7fa457":"markdown","ff843168":"markdown","6da9c363":"markdown"},"source":{"eb1f985f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport seaborn as sns\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","446efd97":"df=pd.read_csv(\"..\/input\/train.csv\")\ndf.head()","10b87b08":"print(\"Number of samples: \",len(df))\nprint(\"Number of Labels: \",np.unique(df.has_cactus))","21d1f8ac":"sns.distplot(df.has_cactus)","ac9a613c":"from PIL import Image \nfrom skimage.transform import resize\ntrain=pd.read_csv(\"..\/input\/train.csv\")\ntrain_images=[]\npath=\"..\/input\/train\/train\/\"\nfor i in train.id:\n    image=plt.imread(path+i)\n    train_images.append(image)","d8073ecf":"train_images=np.asarray(train_images)\nX=train_images\ny=train.has_cactus\nprint(\"Labels: \",y.shape)\nprint(\"images: \",X.shape)","96432112":"plt.imshow(X[2])","e3c3b66d":"import keras\nfrom keras.models import Model\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Input, Activation, Dropout, GlobalAveragePooling2D, \\\n    BatchNormalization, concatenate, AveragePooling2D\nfrom keras.optimizers import Adam\n\n\n\ndef conv_layer(conv_x, filters):\n    conv_x = BatchNormalization()(conv_x)\n    conv_x = Activation('relu')(conv_x)\n    conv_x = Conv2D(filters, (3, 3), kernel_initializer='he_uniform', padding='same', use_bias=False)(conv_x)\n    conv_x = Dropout(0.2)(conv_x)\n\n    return conv_x\n\n\ndef dense_block(block_x, filters, growth_rate, layers_in_block):\n    for i in range(layers_in_block):\n        each_layer = conv_layer(block_x, growth_rate)\n        block_x = concatenate([block_x, each_layer], axis=-1)\n        filters += growth_rate\n\n    return block_x, filters\n\n\ndef transition_block(trans_x, tran_filters):\n    trans_x = BatchNormalization()(trans_x)\n    trans_x = Activation('relu')(trans_x)\n    trans_x = Conv2D(tran_filters, (1, 1), kernel_initializer='he_uniform', padding='same', use_bias=False)(trans_x)\n    trans_x = AveragePooling2D((2, 2), strides=(2, 2))(trans_x)\n\n    return trans_x, tran_filters\n\n\ndef dense_net(filters, growth_rate, classes, dense_block_size, layers_in_block):\n    input_img = Input(shape=(32, 32, 3))\n    x = Conv2D(24, (3, 3), kernel_initializer='he_uniform', padding='same', use_bias=False)(input_img)\n\n    dense_x = BatchNormalization()(x)\n    dense_x = Activation('relu')(x)\n\n    dense_x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(dense_x)\n    for block in range(dense_block_size - 1):\n        dense_x, filters = dense_block(dense_x, filters, growth_rate, layers_in_block)\n        dense_x, filters = transition_block(dense_x, filters)\n\n    dense_x, filters = dense_block(dense_x, filters, growth_rate, layers_in_block)\n    dense_x = BatchNormalization()(dense_x)\n    dense_x = Activation('relu')(dense_x)\n    dense_x = GlobalAveragePooling2D()(dense_x)\n\n    output = Dense(classes, activation='softmax')(dense_x)\n\n    return Model(input_img, output)\n","33fa5c6f":"from keras.utils import np_utils\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\nCat_test_y = np_utils.to_categorical(y_test)\ny_train=np_utils.to_categorical(y_train)\n\nprint(\"X_train shape : \",X_train.shape)\nprint(\"y_train shape : \",y_train.shape)\nprint(\"X_test shape : \",X_test.shape)\nprint(\"y_test shape : \",y_test.shape)","cd8b7982":"dense_block_size = 3\nlayers_in_block = 4\n\ngrowth_rate = 12\nclasses = 2\nmodel = dense_net(growth_rate * 2, growth_rate, classes, dense_block_size, layers_in_block)\nmodel.summary()\n\n\n# training\nbatch_size = 32\nepochs = 10\noptimizer = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\nmodel.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics=['accuracy'])\nhistory=model.fit(X_train,y_train, epochs=epochs, batch_size=batch_size, shuffle=True,validation_data=(X_test, Cat_test_y))\n","5fd499e6":"# set the matplotlib backend so figures can be saved in the background\n# plot the training loss and accuracy\nimport sys\nimport matplotlib\nprint(\"Generating plots...\")\nsys.stdout.flush()\nmatplotlib.use(\"Agg\")\nmatplotlib.pyplot.style.use(\"ggplot\")\nmatplotlib.pyplot.figure()\nN = epochs \nmatplotlib.pyplot.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\nmatplotlib.pyplot.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\nmatplotlib.pyplot.plot(np.arange(0, N), history.history[\"acc\"], label=\"train_acc\")\nmatplotlib.pyplot.plot(np.arange(0, N), history.history[\"val_acc\"], label=\"val_acc\")\nmatplotlib.pyplot.title(\"Cactus Image Classification\")\nmatplotlib.pyplot.xlabel(\"Epoch #\")\nmatplotlib.pyplot.ylabel(\"Loss\/Accuracy\")\nmatplotlib.pyplot.legend(loc=\"lower left\")\nmatplotlib.pyplot.savefig(\"plot.png\")","2b6be064":"from sklearn import metrics\nlabel_pred = model.predict(X_test)\n\npred = []\nfor i in range(len(label_pred)):\n    pred.append(np.argmax(label_pred[i]))\n\nY_test = np.argmax(Cat_test_y, axis=1) # Convert one-hot to index\n\nprint(metrics.classification_report(Y_test, pred))","f46b02a6":"from sklearn import metrics\nlabel_pred = model.predict(X_test)\n\npred = []\nfor i in range(len(label_pred)):\n    pred.append(np.argmax(label_pred[i]))\n\nY_test = np.argmax(Cat_test_y, axis=1) # Convert one-hot to index\n\nprint(metrics.accuracy_score(Y_test, pred))","13181d57":"#training all data on this model\nmodel = dense_net(growth_rate * 2, growth_rate, classes, dense_block_size, layers_in_block)\nbatch_size = 32\nepochs = 20\noptimizer = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\nmodel.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics=['accuracy'])\ny= np_utils.to_categorical(y)\nhistory=model.fit(X,y, epochs=epochs, batch_size=batch_size, shuffle=True,verbose=0)","08354882":"from PIL import Image \nfrom skimage.transform import resize\nsample=pd.read_csv(\"..\/input\/sample_submission.csv\")\ntest_images=[]\npath=\"..\/input\/test\/test\/\"\nfor i in sample.id:\n    image=plt.imread(path+i)\n    test_images.append(image)","62c9ba2c":"# prediction\ntest_images=np.asarray(test_images)\ntest_images=test_images.reshape(test_images.shape[0],32,32,3)\npred1=model.predict(test_images)\npred = []\nfor i in range(len(test_images)):\n    pred.append(np.argmax(pred1[i]))\n\n\nresults = pd.DataFrame({\"id\" : sample.id, \"has_cactus\": pred})\nresults.to_csv(\"submission.csv\", index = False)","74dcab3d":"# Setting Data set ","9d1e5ae8":"# UnderStanding Data","4fe5fd9b":"Overall accuracy Score","ce7a1498":"# Submission to the competition","4495955e":"# Exploring Others Notebook\n**Notebook got Highest Accuracy with densenet (pretrained)**\n\nhttps:\/\/www.kaggle.com\/kenseitrg\/simple-fastai-exercise","c669b99f":"**accuracy Score of each class**","fd7b1a4e":"# IntroDuction to DenseNet (2018)\n\n![](https:\/\/arthurdouillard.com\/figures\/densenet.png)\n**Now we are going to use Dense net for classifying these Images**\n\nDenseNet Architecture\n\nThe best way to illustrate any architecture is done with the help of code. So, I have implemented DenseNet architecture in Keras using MNIST data set.\n\n*  **Dense Block: **\n A DenseNet consists of dense blocks. Each dense block consists of convolution layers. After a dense block a transition layer is added to proceed to next dense block\nEvery layer in a dense block is directly connected to all its subsequent layers. Consequently, each layer receives the feature-maps of all preceding layer.\n\n* **Covolutional layers :**\n Each convolution layer is consist of three consecutive operations: batch normalization (BN) , followed by a rectified linear unit (ReLU) and a 3 \u00d7 3 convolution (Conv). Also dropout can be added which depends on your architecture requirement.An essential part of convolutional networks is down-sampling layers that change the size of feature-maps. To facilitate down-sampling in DenseNet architecture it divides the network into multiple densely connected dense blocks\n \n* **Transition Block : **\n DenseNets can scale naturally to hundreds of layers, while exhibiting no optimization difficulties. Because of their compact internal representations and reduced feature redundancy, DenseNets may be good feature extractors for various computer vision tasks that build on convolutional features\n \n ","2a29827f":"# Advantages of DenseNet\n\n* **Strong Gradient Flow**\n\nImplicit \u201cDeep Supervision\u201d\nThe error signal can be easily propagated to earlier layers more directly. This is a kind of implicit deep supervision as earlier layers can get direct supervision from the final classification layer.\n\n* **Parameter & Computational Efficiency**\n\nNumber of Parameters for ResNet and DenseNet\nFor each layer, number of parameters in ResNet is directly proportional to C\u00d7C while Number of parameters in DenseNet is directly proportional to l\u00d7k\u00d7k.\n\nSince k<<C, DenseNet has much smaller size than ResNet.\n\n* **More Diversified Features**\n\nMore Diversified Features in DenseNet\nSince each layer in DenseNet receive all preceding layers as input, more diversified features and tends to have richer patterns.\n\n* **Maintains Low Complexity Features**\n\nStandard ConvNet\nIn Standard ConvNet, classifier uses most complex features.\n\n\nDenseNet\nIn DenseNet, classifier uses features of all complexity levels. It tends to give more smooth decision boundaries. It also explains why DenseNet performs well when training data is insufficient.","5ca28270":"# Traning and Testing Visualisation","e4743200":"# image in dataset","7577a4b1":"# Refrences\n\n* https:\/\/towardsdatascience.com\/densenet-2810936aeebb\n* https:\/\/theailearner.com\/2018\/12\/09\/densely-connected-convolutional-networks-densenet\/\n* https:\/\/towardsdatascience.com\/review-densenet-image-classification-b6631a8ef803","33140339":"# Testing","fa7fa457":"# Implementation of denseNet","ff843168":"# Notebook log\n* UnderStanding Data\n* Converting images into pixels\n* IntroDuction to Dense Net\n* Implementation of denseNet\n* training with DenseNet\n* Traning and Testing Visualisation\n* Testing Our Model\n* Advantages of Dense Net\n* Submission of Competetion\n* Exploring Others Notebook\n* Refrences","6da9c363":"# Prediction with DenseNet"}}