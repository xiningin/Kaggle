{"cell_type":{"daac6b20":"code","ebcf4377":"code","47738869":"code","e8338669":"code","a31abf32":"code","7b132ad9":"code","6e04e209":"code","de29e469":"code","59697999":"code","150718b9":"code","e39490f3":"code","be8d906a":"code","2ca8da00":"code","41846989":"code","0763c0e6":"code","80431ccc":"code","7f783203":"code","3b3ceffc":"code","148e589f":"code","70101564":"code","5b34221f":"code","c33fd50f":"code","b7cb0704":"code","61bcf1b5":"code","18c04229":"markdown","95a896ed":"markdown","d3c92ba3":"markdown","1cede7ba":"markdown"},"source":{"daac6b20":"import torch\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport torchvision\nimport torchvision.transforms as transforms\n\nimport torch.nn as nn\nimport torch.optim as optim\nimport seaborn as sns","ebcf4377":"trainset = torchvision.datasets.MNIST(root='.\/data', train=True,\n                                     download=True,\n                                     transform=transforms.ToTensor())","47738869":"batch_size = 4","e8338669":"trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)","a31abf32":"dataiter = iter(trainloader)\nimages, labels = dataiter.next()\n\nprint(images.shape)\n\nprint(images[0].shape)\nprint(labels[0].item())","7b132ad9":"def imshow(img, title):\n    \n    plt.figure(figsize=(batch_size * 4, 4))\n    plt.axis('off')\n    plt.imshow(np.transpose(img, (1, 2, 0)))\n    plt.title(title)\n    plt.show()","6e04e209":"def show_batch_images(dataloader):\n    images, labels = next(iter(dataloader))\n    \n    img = torchvision.utils.make_grid(images)\n    imshow(img,title=[str(x.item()) for x in labels])\n    \n    return images, labels","de29e469":"images, labels = show_batch_images(trainloader)","59697999":"class MyNet(nn.Module):\n    def __init__(self):\n        super(MyNet, self).__init__()\n        self.classifier = nn.Sequential(\n            nn.Linear(784, 48), # 28 x 28 = 784\n            nn.ReLU(),\n            nn.Linear(48, 24),\n            nn.ReLU(),\n            nn.Linear(24, 10)\n        )\n    \n    def forward(self, x):\n        x = x.view(x.size(0),-1)\n        x = self.classifier(x)\n        return x","150718b9":"class MyNetBN(nn.Module):\n    def __init__(self):\n        super(MyNetBN, self).__init__()\n        self.classifier = nn.Sequential(\n            nn.Linear(784, 48),\n            nn.BatchNorm1d(48),\n            nn.ReLU(),\n            nn.Linear(48,24),\n            nn.BatchNorm1d(24),\n            nn.ReLU(),\n            nn.Linear(24,10)\n        )\n        \n    def forward(self, x):\n        x = x.view(x.size(0),-1)\n        x = self.classifier(x)\n        return x","e39490f3":"model = MyNet()\nprint(model)","be8d906a":"model_bn = MyNetBN()\nprint(model_bn)","2ca8da00":"batch_size = 512","41846989":"trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,shuffle=True)","0763c0e6":"loss_fn = nn.CrossEntropyLoss()\nopt = optim.SGD(model.parameters(), lr=0.01)\nopt_bn = optim.SGD(model_bn.parameters(),lr=0.01)","80431ccc":"loss_arr = []\nloss_bn_arr = []\n\nmax_epochs = 2\n\nfor epoch in range(max_epochs):\n    \n    for i, data in enumerate(trainloader,0):\n        \n        inputs, labels = data\n        \n        # training steps for normal model\n        opt.zero_grad()\n        outputs = model(inputs)\n        loss = loss_fn(outputs, labels)\n        loss.backward()\n        opt.step()\n        \n        # traing steps for bn model\n        opt_bn.zero_grad()\n        outputs_bn = model_bn(inputs)\n        loss_bn = loss_fn(outputs_bn, labels)\n        loss_bn.backward()\n        opt_bn.step()\n        \n        loss_arr.append(loss.item())\n        loss_bn_arr.append(loss_bn.item())\n        \n        if i % 100 == 0:\n            \n            inputs = inputs.view(inputs.size(0),-1)\n            \n            model.eval()\n            model_bn.eval()\n            \n            a = model.classifier[0](inputs)\n            a = model.classifier[1](a)\n            a = model.classifier[2](a)\n            a = a.detach().numpy().ravel()\n            sns.distplot(a, kde=True, color='r', label='Normal')\n            \n            b = model_bn.classifier[0](inputs)\n            b = model_bn.classifier[1](b)\n            b = model_bn.classifier[2](b)\n            b = model_bn.classifier[3](b)\n            b = model_bn.classifier[4](b)\n            b = b.detach().numpy().ravel()\n            \n            sns.distplot(b, kde=True, color='g', label='BatchNorm')\n            plt.title('%d: Loss = %0.2f, Loss with bn = %0.2f' % (i, loss.item(), loss_bn.item()))\n            plt.legend()\n            plt.show()\n            plt.pause(0.5)\n            \n            model.train()\n            model_bn.train()\n            \n    print('--------------------------')\n    \n    plt.plot(loss_arr,'r', label = 'Normal')\n    plt.plot(loss_bn_arr,'g', label = 'BatchNormal')\n    plt.legend()\n    plt.show()     ","7f783203":"loss_arr = []\nloss_bn_arr = []\n\nmax_epochs = 2\n\nfor epoch in range(max_epochs):\n\n    for i, data in enumerate(trainloader, 0):\n\n        inputs, labels = data\n\n        # training steps for normal model\n        opt.zero_grad()\n        outputs = model(inputs)\n        loss = loss_fn(outputs, labels)\n        loss.backward()\n        opt.step()\n        \n        # training steps for bn model\n        opt_bn.zero_grad()\n        outputs_bn = model_bn(inputs)\n        loss_bn = loss_fn(outputs_bn, labels)\n        loss_bn.backward()\n        opt_bn.step()\n        \n        loss_arr.append(loss.item())\n        loss_bn_arr.append(loss_bn.item())\n        \n        if i % 10 == 0:\n        \n            inputs = inputs.view(inputs.size(0), -1)\n            \n            model.eval()\n            model_bn.eval()\n            \n            a = model.classifier[0](inputs)\n#             a = model.classifier[1](a)\n#             a = model.classifier[2](a)\n            a = a.detach().numpy().ravel()\n            sns.distplot(a, kde=True, color='r', label='Normal') \n            \n            b = model_bn.classifier[0](inputs)\n            b = model_bn.classifier[1](b)\n#             b = model_bn.classifier[2](b)\n#             b = model_bn.classifier[3](b)\n#             b = model_bn.classifier[4](b)\n            b = b.detach().numpy().ravel()\n            \n            sns.distplot(b, kde=True, color='g', label='BatchNorm') \n            plt.title('%d: Loss = %0.2f, Loss with bn = %0.2f' % (i, loss.item(), loss_bn.item()))\n            plt.legend()\n            plt.show()\n            plt.pause(0.5)\n            \n            model.train()\n            model_bn.train()\n        \n        \n    print('----------------------')\n\n    plt.plot(loss_arr, 'r', label='Normal')\n    plt.plot(loss_bn_arr, 'g', label='BatchNorm')\n    plt.legend()\n    plt.show()","3b3ceffc":"class CNN_BN(nn.Module):\n    def __init__(self):\n        super(CNN_BN, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 3, 5),         # (N, 1, 28, 28) -> (N, 3, 24, 24)\n            nn.ReLU(),\n            nn.AvgPool2d(2, stride=2),  # (N, 3, 24, 24) -> (N, 3, 12, 12)\n            nn.Conv2d(3, 6, 3),\n            nn.BatchNorm2d(6)           # (N, 3, 12, 12) -> (N, 6, 10, 10)\n        )\n        self.features1 = nn.Sequential(\n            nn.ReLU(),\n            nn.AvgPool2d(2, stride=2)   # (N, 6, 10, 10) -> (N, 6, 5, 5)\n        )\n        self.classifier = nn.Sequential(\n            nn.Linear(150, 25),         # (N, 150) -> (N, 25)\n            nn.ReLU(),\n            nn.Linear(25, 10)           # (N, 25) -> (N, 10)\n        )\n    \n    def forward(self, x):\n        x = self.features(x)\n        x = self.features1(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x","148e589f":"model_bn = CNN_BN()\nprint(model_bn)","70101564":"N = 50\nnoise = 0.3\n\nX_train = torch.unsqueeze(torch.linspace(-1, 1, N),1)\nY_train = X_train + noise * torch.normal(torch.zeros(N, 1), torch.ones(N, 1))\n\nX_test = torch.unsqueeze(torch.linspace(-1, 1, N),1)\nY_test = X_test + noise * torch.normal(torch.zeros(N, 1), torch.ones(N,1))","5b34221f":"plt.scatter(X_train.data.numpy(), Y_train.data.numpy(), c='purple', alpha=0.5, label='train')\nplt.scatter(X_test.data.numpy(), Y_test.data.numpy(), c='yellow', alpha=0.5, label='test')\nplt.legend()\nplt.show()","c33fd50f":"N_h = 100\n\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(1, N_h),\n    torch.nn.ReLU(),\n    torch.nn.Linear(N_h, N_h),\n    torch.nn.ReLU(),\n    torch.nn.Linear(N_h,1),\n)\n\nmodel_dropout = torch.nn.Sequential(\n    torch.nn.Linear(1, N_h),\n    torch.nn.Dropout(0.5),\n    torch.nn.ReLU(),\n    torch.nn.Linear(N_h,N_h),\n    torch.nn.Dropout(0.5),\n    torch.nn.ReLU(),\n    torch.nn.Linear(N_h,1),\n)","b7cb0704":"opt = torch.optim.Adam(model.parameters(), lr=0.01)\nopt_dropout = torch.optim.Adam(model_dropout.parameters(),lr=0.01)\nloss_fn = torch.nn.MSELoss()","61bcf1b5":"max_epochs = 1000\n\nfor epoch in range(max_epochs):\n    \n    pred = model(X_train) # look at the entire data in a single shot\n    loss = loss_fn(pred, Y_train)\n    opt.zero_grad()\n    loss.backward()\n    opt.step()\n    \n    pred_dropout = model_dropout(X_train)\n    loss_dropout = loss_fn(pred_dropout, Y_train)\n    opt_dropout.zero_grad()\n    loss_dropout.backward()\n    opt_dropout.step()\n    \n    \n    if epoch % 50 == 0:\n        \n        model.eval()\n        model_dropout.eval()\n        \n        test_pred = model(X_test)\n        test_loss = loss_fn(test_pred, Y_test)\n        \n        test_pred_dropout = model_dropout(X_test)\n        test_loss_dropout = loss_fn(test_pred_dropout, Y_test)\n        \n        plt.scatter(X_train.data.numpy(), Y_train.data.numpy(), c='purple', alpha=0.5, label='train')\n        plt.scatter(X_test.data.numpy(), Y_test.data.numpy(), c='yellow', alpha=0.5, label='test')\n        plt.plot(X_test.data.numpy(), test_pred.data.numpy(), 'r-', lw=3, label='normal')\n        plt.plot(X_test.data.numpy(), test_pred_dropout.data.numpy(), 'b--', lw=3,  label='dropout')\n        \n        plt.title('Epoch %d, Loss = %0.4f, Loss with dropout = %0.4f' % (epoch, test_loss, test_loss_dropout))\n        \n        plt.legend()\n\n        model.train()\n        model_dropout.train()\n        \n        plt.pause(0.05)","18c04229":"### DataSet and Visualisation","95a896ed":"### BatchNorm","d3c92ba3":"### Outline \n1. Load dataset and visualise\n2. Add batchnorm layers\n3. Comparison with and without batchnorm layers\n4. Add dropout layer\n5. Comparison with and without dropout layer","1cede7ba":"### Dropout"}}