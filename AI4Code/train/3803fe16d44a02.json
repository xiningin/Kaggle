{"cell_type":{"68986f5a":"code","3e5ee256":"code","779c958b":"code","29c3c5c9":"code","2afef806":"code","c6115269":"code","ed6c6a62":"code","b9179356":"code","4ef9ba8d":"code","4e4c087b":"code","80f570c0":"code","2ed51734":"code","1661df13":"code","0057ec0b":"code","9da68496":"code","f8b26609":"code","c0ea65e6":"code","3563c7f4":"code","acb2d483":"code","c93bf960":"code","5d88e741":"code","27e2140a":"code","499ea87f":"code","657ff288":"code","d3c43c2f":"code","262106aa":"code","37d69273":"code","f11fc16e":"code","6ffe7101":"code","eb6ef55b":"code","88e28f32":"code","322628e2":"code","08c1fce0":"code","eb3503c4":"code","d608e05b":"code","808116e9":"code","caafd808":"code","59b6f403":"code","0f26bd34":"code","34d1eb64":"code","e87c7535":"code","e3733fac":"code","41ee547a":"code","33fdc2cc":"code","d9ae2c84":"code","e85c158c":"code","37a63b05":"code","ebce39e0":"code","b6d66bf6":"code","eca075e8":"code","6ec70932":"code","5fc68724":"code","542180b8":"code","fcf1e60a":"code","1e62733e":"code","fe99e5d4":"code","8ebedebe":"code","b681d6c8":"code","166efe67":"markdown","6111c324":"markdown","b03bc1ff":"markdown","29eb58f8":"markdown","0ec2c7a5":"markdown","dc5b08ad":"markdown","7f70c2b3":"markdown","78fc7d7a":"markdown","960c4a3c":"markdown","8dca6938":"markdown","239e709e":"markdown"},"source":{"68986f5a":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n \nimport scipy.stats as stats\nimport sklearn as sk\nimport statsmodels as sm\n\nfrom datetime import datetime as dt\nfrom datetime import datetime as date\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","3e5ee256":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","779c958b":"train=pd.read_csv('..\/input\/walmart-dataretail-analysis\/Walmart_Store_sales.csv')","29c3c5c9":"train.head()","2afef806":"train.tail()","c6115269":"train.info()","ed6c6a62":"train['Weekly_Sales'].isnull().sum()","b9179356":"train.shape","4ef9ba8d":"train.isnull().sum()","4e4c087b":"train['Date'] = pd.to_datetime(train['Date'])","80f570c0":"train.info()","2ed51734":"train.Holiday_Flag.value_counts()","1661df13":"train['DateType'] = [dt.strptime(date, '%Y-%m-%d').date() for date in train['Date'].astype(str).values.tolist()]","0057ec0b":"train['Superbowl'] =    np.where((train['DateType']==dt(2010, 2, 12).date())  | (train['DateType']==dt(2011, 2, 11).date()) | (train['DateType']==dt(2012, 2, 10).date()) | (train['DateType']==dt(2013, 2, 8).date()),1, 0)\ntrain['Labor_Day'] =    np.where((train['DateType']==dt(2010, 9, 10).date())  | (train['DateType']==dt(2011, 9, 9).date())  | (train['DateType']==dt(2012, 9 , 7).date()) | (train['DateType']==dt(2013, 9, 6).date()),1, 0)\ntrain['Christmas'] =    np.where((train['DateType']==dt(2010, 12, 31).date()) | (train['DateType']==dt(2011, 12, 30).date())| (train['DateType']==dt(2012, 12, 28).date())| (train['DateType']==dt(2013, 12, 27).date()),1, 0)\ntrain['Thanksgiving'] = np.where((train['DateType']==dt(2010, 11, 26).date()) | (train['DateType']==dt(2011, 11, 25).date())| (train['DateType']==dt(2012, 11, 23).date())| (train['DateType']==dt(2013, 11, 29).date()),1, 0)","9da68496":"print(train.Superbowl.value_counts())\nprint(train.Labor_Day.value_counts())\nprint(train.Thanksgiving.value_counts())\nprint(train.Christmas.value_counts())","f8b26609":"store_sales=train.groupby(['Store'])['Weekly_Sales'].sum().sort_values(ascending=False)","c0ea65e6":"round(store_sales,1)","3563c7f4":"store_std =train.groupby(['Store'])['Weekly_Sales'].std().sort_values(ascending=False)","acb2d483":"round(store_std,2)","c93bf960":"store_mean =train.groupby(['Store'])['Weekly_Sales'].mean().sort_values(ascending=False)","5d88e741":"cv = round(store_std\/store_mean,2)","27e2140a":"cv.sort_values(ascending=False)","499ea87f":"train['Year'] = train['Date'].dt.year\ntrain['Month'] = train['Date'].dt.month\ntrain['Quarter'] = train['Date'].dt.quarter","657ff288":"train.head()","d3c43c2f":"train.info()","262106aa":"train.columns ","37d69273":"train.drop(columns= 'DateType',inplace =True)","f11fc16e":"train.head()","6ffe7101":"Que3 = train[(train['Quarter'] == 3) & (train['Year'] == 2012)].groupby('Store')['Weekly_Sales'].sum().sort_values(ascending= False )","eb6ef55b":"Que3.head(3)","88e28f32":"Q3_date_from = pd.Timestamp(date(2012,7,1))\nQ3_date_to = pd.Timestamp(date(2012,9,30))\nQ2_date_from = pd.Timestamp(date(2012,4,1))\nQ2_date_to = pd.Timestamp(date(2012,6,30))\n\n#Collecting the data of Q3 and Q2 from original dataset.\nQ2data=train[(train['Date'] > Q2_date_from) & (train['Date'] < Q2_date_to)]\nQ3data=train[(train['Date'] > Q3_date_from) & (train['Date'] < Q3_date_to)]\n\n#finding the sum weekly sales of each store in Q2\nQ2 = pd.DataFrame(Q2data.groupby('Store')['Weekly_Sales'].sum())\nQ2.reset_index(inplace=True)\nQ2.rename(columns={'Weekly_Sales': 'Q2_Weekly_Sales'},inplace=True)\n\n#finding the sum weekly sales of each store in Q2\nQ3 = pd.DataFrame(Q3data.groupby('Store')['Weekly_Sales'].sum())\nQ3.reset_index(inplace=True)\nQ3.rename(columns={'Weekly_Sales': 'Q3_Weekly_Sales'},inplace=True)\n\n#mergeing Q2 and Q3 data on Store as a common column\nQ3_Growth= Q2.merge(Q3,how='inner',on='Store')\n\n# Growth rate formula is defined as the ratio of difference in present value to past value by past value whole multiplied with 100 (since it is in percentage)\n# ((Present value \u2014 Past value )\/Past value )*100\n#Calculating Growth rate of each Store and collecting it into a dataframe  \nQ3_Growth['Growth_Rate'] =(Q3_Growth['Q3_Weekly_Sales'] - Q3_Growth['Q2_Weekly_Sales'])\/Q3_Growth['Q2_Weekly_Sales']\nQ3_Growth['Growth_Rate']=round(Q3_Growth['Growth_Rate'],2)\nQ3_Growth.sort_values('Growth_Rate',ascending=False).head(1)","322628e2":"Q3_Growth.sort_values('Growth_Rate',ascending=False).tail(1)","08c1fce0":"round(train.groupby(['Holiday_Flag'])['Weekly_Sales'].sum(),2)","eb3503c4":"Spr_sales = train.groupby(['Superbowl'])['Weekly_Sales'].mean()\nprint(round(Spr_sales,2))\nLd_sales = train.groupby(['Labor_Day'])['Weekly_Sales'].mean()\nprint(round(Ld_sales,2))\nThanksg_sales = train.groupby(['Thanksgiving'])['Weekly_Sales'].mean()\nprint(round(Thanksg_sales,2))\nChristmas_sales = train.groupby(['Christmas'])['Weekly_Sales'].mean()\nprint(round(Christmas_sales,2))\nNon_Holi_Sales = train[(train['Holiday_Flag'] == 0)].groupby('Holiday_Flag')['Weekly_Sales'].mean()\nprint(round(Non_Holi_Sales,2))","d608e05b":"Spr_sales.plot(kind='bar',legend=False,title='Sales in Super Bowl holiday') ","808116e9":"Thanksg_sales.plot(kind='bar',legend=False,title='Sales in Thanksgiving holiday') ","caafd808":"Ld_sales.plot(kind='bar',legend=False,title='Sales in Labour_Day holiday')","59b6f403":"Christmas_sales.plot(kind='bar',legend=False,title='Sales in Christmas holiday')","0f26bd34":"Non_Holi_Sales.plot(kind='bar',legend=False,title='Sales in Christmas holiday')","34d1eb64":"monthly = train.groupby(pd.Grouper(key='Date', freq='1M')).sum()\nmonthly=monthly.reset_index()\nfig, ax = plt.subplots(figsize=(10,8))\nX = monthly['Date']\nY = monthly['Weekly_Sales']\nplt.plot(X,Y)\nplt.title('Month Wise Sales')\nplt.xlabel('Monthly')\nplt.ylabel('Weekly_Sales')","e87c7535":"Semester = train.groupby(pd.Grouper(key='Date', freq='6M')).sum()\nSemester = Semester.reset_index()\nfig, ax = plt.subplots(figsize=(10,8))\nX = Semester['Date']\nY = Semester['Weekly_Sales']\nplt.plot(X,Y)\nplt.title('Semester Wise Sales')\nplt.xlabel('Semester')\nplt.ylabel('Weekly_Sales')","e3733fac":"train.info()","41ee547a":"train['Date'].sort_values()","33fdc2cc":"train.drop(columns=['Superbowl','Labor_Day','Christmas','Thanksgiving'],inplace = True)","d9ae2c84":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import LabelEncoder\n","e85c158c":"train['Store']","37a63b05":"train['Store'] = train.Store.astype(str)\ntrain['Store'] = 'Store '+ train.Store.astype(str)","ebce39e0":"train['Store']","b6d66bf6":"LabEncoder = LabelEncoder()\nStr_1 = train[train['Store']=='Store 1']\nStr_1 = Str_1.copy()","eca075e8":"Str_1.head()","6ec70932":"Str_1['Days'] = LabEncoder.fit_transform(Str_1['Date'])\nStr_1.drop(['Store','Date','Holiday_Flag','Year','Month','Quarter'],axis=1 , inplace = True)","5fc68724":"corr = Str_1.corr()\ncorrmat = sns.heatmap(Str_1.corr(), annot=True)\ncorrmat","542180b8":"X = Str_1[['Days','Fuel_Price','CPI','Unemployment']]\ny = Str_1['Weekly_Sales']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20, random_state = 123)","fcf1e60a":"print(\"Shape of X_train is \" , X_train.shape)\nprint(\"Shape of y_train is \" , y_train.shape)\n\nprint(\"Shape of X_test is \" , X_test.shape)\nprint(\"Shape of y_test is \" , y_test.shape)","1e62733e":"Linreg = LinearRegression()\nLinreg.fit(X_train,y_train)","fe99e5d4":"y_pred = Linreg.predict(X_test)\n\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","8ebedebe":"rfr = RandomForestRegressor(n_estimators = 400,max_depth=15)        \nrfr.fit(X_train,y_train)\ny_pred=rfr.predict(X_test)\n\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","b681d6c8":"Actual_vs_Pred = pd.DataFrame({\"Actual Sales\" : y_test, \"Predicted Sales\": y_pred})\nActual_vs_Pred.head()\n","166efe67":"As we can see unemployment is highly correlated with days and is insignificant as it correlation with Weekly Sales is quite low.\nAlso temperature and Unemployment are negatively impacting the sales . however Fuel Price and CPI are positively impacting the Sales.","6111c324":"_______________________________________________________________________________________________________","b03bc1ff":"### Q1.Which store has maximum sales ?","29eb58f8":"_______________________________________________________________________________________________________________________________","0ec2c7a5":"**************************************************************************************","dc5b08ad":"### Q4. Some holidays have a negative impact on sales. Find out holidays which have higher sales than the mean sales in non-holiday season for all stores together\n\n#### Holiday Events:\n\nSuper Bowl: 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13\n\nLabour Day: 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13\n\nThanksgiving: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13\n\nChristmas: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13","7f70c2b3":"### Q2. Which store has maximum standard deviation i.e., the sales vary a lot. Also, find out the coefficient of mean to standard deviation","78fc7d7a":"#### From above tables we can infer that Q3 growth rate is in losses .\n#### the Store 16 has the least loss of 3% compared the other stores and store 14 has highest loss of 18%.","960c4a3c":"### So, the Store 20 Has maximum sales ","8dca6938":"### Q3. Which store\/s has good quarterly growth rate in Q3\u20192012 ","239e709e":"### Q5. Provide a monthly and semester view of sales in units and give insights"}}