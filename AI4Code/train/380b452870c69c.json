{"cell_type":{"97094e71":"code","0da23481":"code","81f0d9c5":"code","b41aa5e5":"code","68ecbdb3":"code","6da4cd02":"code","cc95b915":"code","564a0975":"code","2a0e0bb3":"code","31de0e63":"code","457e8d4c":"code","75964282":"code","34b72362":"code","4c0404bf":"code","acec01da":"markdown","3f475d4d":"markdown"},"source":{"97094e71":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nimport sys, os, re\nnp.random.seed(32)\nos.environ[\"OMP_NUM_THREADS\"] = \"4\"\n\n# Any results you write to the current directory are saved as output.","0da23481":"train = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv\", \n                    usecols= ['id','target','comment_text'] )\ntest = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv\")\n\n","81f0d9c5":"#train['target'] = train.target.apply(lambda x: 1 if x > 0.5 else 0) # slow\ntrain['target'] = np.where(train['target'] > 0.5, 1, 0) #faster\ntrain['comment_text'] = train.comment_text.apply(lambda x: x.lower())","b41aa5e5":"y = train.target.values\nX_train, X_valid, Y_train, Y_valid = train_test_split(train[['comment_text']], y, test_size = 0.1)","68ecbdb3":"embed_size = 300\nmax_features = 200000\nmax_len = 220","6da4cd02":"tk = Tokenizer(num_words=max_features, lower = True)\ntk.fit_on_texts(X_train['comment_text'].values)\nX_train[\"comment_seq\"] = tk.texts_to_sequences(X_train['comment_text'].values)\nX_valid[\"comment_seq\"] = tk.texts_to_sequences(X_valid['comment_text'].values)\ntest[\"comment_seq\"] = tk.texts_to_sequences(test['comment_text'].values)","cc95b915":"%%time\nX_train_p = pad_sequences(X_train.comment_seq, maxlen = max_len)\nX_valid_p = pad_sequences(X_valid.comment_seq, maxlen = max_len)\ntest_p = pad_sequences(test.comment_seq, maxlen = max_len)\ndel X_train, X_valid, test, train","564a0975":"embedding_path = \"..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec\"\n\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n\nword_index = tk.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.zeros((nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","2a0e0bb3":"from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, LeakyReLU\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten, CuDNNGRU,CuDNNLSTM\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\nfrom keras.engine import InputSpec, Layer\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom keras.layers import GRU, BatchNormalization, Conv1D, MaxPooling1D\n\nimport logging\nfrom sklearn.metrics import roc_auc_score\nfrom keras.callbacks import Callback\n\nimport warnings\nwarnings.filterwarnings('ignore')","31de0e63":"file_path = \"best_model.hdf5\"\ncheck_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,save_best_only = True)","457e8d4c":"def build_model(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0):\n    inp = Input(shape = (max_len,))\n    x = Embedding(max_features, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n    #x = SpatialDropout1D(dr)(x)\n    x = Bidirectional(CuDNNLSTM(units, return_sequences=True))(x)\n    x = Bidirectional(CuDNNLSTM(units, return_sequences=True))(x)  \n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x) \n    #att = AttentionWeightedAverage()(x)\n    conc = concatenate([avg_pool, max_pool])\n    output = Dropout(0.75)(conc)\n    output = Dense(units=110)(output)\n    output = LeakyReLU(alpha=0.3)(output)\n    output = Dense(units=55)(output)\n    output = Activation('relu')(output)\n    prediction = Dense(1, activation = \"sigmoid\")(output)\n    model = Model(inputs = inp, outputs = prediction)\n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    history = model.fit(X_train_p, Y_train, batch_size = 1024, epochs = 6, validation_data = (X_valid_p, Y_valid), \n                        verbose = 1 , callbacks = [check_point])\n    model = load_model(file_path)\n    return model","75964282":"model = build_model(lr = 1e-3, lr_d = 0, units = 128, dr = 0.3)","34b72362":"pred = model.predict(test_p, batch_size = 1024, verbose = 1)","4c0404bf":"submission = pd.read_csv(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/sample_submission.csv\")\nsubmission['prediction'] = pred\nsubmission.to_csv(\"submission.csv\", index = False)\nsubmission.head(10)","acec01da":"Load Datasets","3f475d4d":"Code References:\n* https:\/\/www.kaggle.com\/sandeepkumar121995\/keras-bi-gru-lstm-attention-fasttext\/notebook"}}