{"cell_type":{"c8a0e131":"code","8f698f6c":"code","597df8e5":"code","7c72b68a":"code","33c2f19d":"code","4ebe6c26":"code","c0e9f5ab":"code","4329833c":"code","ec5efda7":"code","bfe2636f":"code","2d61ffbf":"code","7ff68279":"code","6ea46f01":"code","1d6f9e4f":"code","c831e056":"code","eae231c4":"code","08584d32":"code","8e2d0453":"code","4b08301b":"code","6239479c":"code","6e75acc9":"code","bc2df2d4":"code","08a1e292":"code","07b1001d":"code","363f304a":"code","f44d64f5":"code","c17dee26":"code","a92e289f":"code","61604559":"code","64221dde":"code","023fd8b4":"code","c5d6bb7d":"code","37260cdf":"code","1055deab":"code","d1130d5a":"code","0bbb2fad":"code","9060a939":"code","b4e8a698":"code","5e97a706":"code","b1dc52ab":"code","e2efcd45":"code","189377cf":"code","1e0f0836":"code","508672df":"code","bedb061d":"code","91d4e2b3":"code","3ea9d63c":"code","05b9a2e6":"code","03ecd0c5":"code","0f391206":"code","c014bc89":"code","47614225":"code","5d8ca300":"code","3051d447":"code","ed6970b0":"code","ec9fa646":"code","cd1df193":"code","0067c721":"code","148cbc98":"code","d53eaab6":"code","beed720f":"code","99baade9":"code","b813a930":"code","f9a65604":"code","dd75ccce":"code","28451fd3":"code","da2f12a8":"code","c4358271":"code","a495804c":"code","5488dd68":"code","6ea16b37":"code","aa9cba3c":"code","3cb9ff66":"code","36a2dc42":"code","20bcca7f":"code","00d92ea7":"code","eaacba4a":"code","0eeae186":"code","a26758c5":"code","d2fac851":"code","577ab9c1":"code","8067060a":"code","d1af9726":"code","7a89e385":"code","81a8f736":"code","6bbd1ddd":"code","88817028":"code","9540bbb7":"code","c4b57d1a":"code","31c17710":"code","df409719":"code","d9f082de":"code","5ed70597":"code","1dcc758d":"code","ebdf8f2b":"code","89230e0e":"code","aeafd9ef":"code","6744b21a":"code","4448bf06":"code","e732a805":"code","3bce54be":"code","901bce7a":"markdown","6397a91f":"markdown","9f12f352":"markdown","39c9ef4c":"markdown","e07c34cc":"markdown","53298b58":"markdown","e46b23f4":"markdown","f2c6bb39":"markdown","fa63afc8":"markdown","b91a6e62":"markdown","8b44f3ae":"markdown","4563a3df":"markdown","ae8d637c":"markdown","bae6af46":"markdown","bcda2ee2":"markdown","61526045":"markdown"},"source":{"c8a0e131":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8f698f6c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport re\nfrom wordcloud import WordCloud, STOPWORDS \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import LabelBinarizer\nfrom scipy.sparse import vstack, hstack, csr_matrix\nfrom scipy import sparse\nimport gc\nimport psutil\nfrom nltk.corpus import stopwords\nimport string\nfrom sklearn.feature_selection.univariate_selection import SelectKBest, f_regression\nimport sys\nsys.path.insert(0, '..\/input\/wordbatch\/wordbatch\/')\nimport wordbatch\nfrom wordbatch.extractors import WordBag\nfrom wordbatch.models import FM_FTRL\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.naive_bayes import MultinomialNB\nimport lightgbm as lgb\nfrom tqdm import tqdm\n\nfrom sklearn.pipeline import make_union, make_pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.pipeline import TransformerMixin\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\nfrom sklearn.model_selection import RandomizedSearchCV","597df8e5":"data_train = pd.read_csv('..\/input\/mercari-price-suggestion-challenge\/train.tsv', delimiter='\\t')","7c72b68a":"####split data train test first\ny = np.log10(np.array(data_train['price'])+1)\nX = data_train.drop('price',axis=1)\n\nX_train,X_cv,Y_train,Y_cv = train_test_split(X, y, test_size=0.20, random_state=42)\n\ndel(X, y ,data_train)\ngc.collect()","33c2f19d":"X_train.drop('train_id', axis=1, inplace=True)\nX_cv.drop('train_id', axis=1, inplace=True)","4ebe6c26":"def rmsle(y, y0):\n    return np.sqrt(np.mean(np.power(np.log1p(y)-np.log1p(y0), 2)))","c0e9f5ab":"# https:\/\/gist.github.com\/sebleier\/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]","4329833c":"import re\nfrom nltk.corpus import stopwords\nstopwords = {x: 1 for x in stopwords.words('english')}\nnon_alphanums = re.compile(u'[^A-Za-z0-9]+')\nnon_alphanumpunct = re.compile(u'[^A-Za-z0-9\\.?!,; \\(\\)\\[\\]\\'\\\"\\$]+')\nRE_PUNCTUATION = '|'.join([re.escape(x) for x in string.punctuation])\n\n\ndef concat_categories(x):\n    return set(x.values)\n\ndef brandfinder(name, category):    \n    for brand in brands_sorted_by_size:\n        if brand in name and category in brand_names_categories[brand]:\n            return brand\n    return 'Unknown'\n\n\n# function to count repetition of first name\ndef create_dictionary(col_name,data_frame= X_train):\n    dictionary = dict(zip(data_frame[col_name],data_frame.groupby(col_name)[col_name].transform('count')))\n    return dictionary\n    \ndef transform_col(data_frame, col_name):\n    dictionary = create_dictionary(col_name)\n    transformed_column = []\n    for value in data_frame[col_name].values:\n        transformed_column.append(dictionary.get(value,1))\n    dictionary = None\n    del(dictionary)\n    gc.collect()\n    return transformed_column\n\n# function returns only first name(first_word)\ndef clean_name(x):\n    if len(x):\n        x = non_alphanums.sub(' ', x).split()\n        if len(x):\n            return x[0].lower()\n    return ''\n\ndef to_number(x):\n    try:\n        if not x.isdigit():\n            return 0\n        x = int(x)\n        if x > 100:\n            return 100\n        else:\n            return x\n    except:\n        return 0\n\ndef sum_numbers(desc):\n    if not isinstance(desc, str):\n        return 0\n    try:\n        return sum([to_number(s) for s in desc.split()])\n    except:\n        return 0\n    \ndef normalize_text(text):\n    return u\" \".join(\n        [x for x in [y for y in non_alphanums.sub(' ', text).lower().strip().split(\" \")] \\\n         if len(x) > 1 and x not in stopwords])\n\ndef decontracted(phrase):\n    # specific\n    try:\n        phrase = re.sub(r\"won't\", \"will not\", phrase)\n        phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n        # general\n        phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n        phrase = re.sub(r\"\\'re\", \" are\", phrase)\n        phrase = re.sub(r\"\\'s\", \" is\", phrase)\n        phrase = re.sub(r\"\\'d\", \" would\", phrase)\n        phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n        phrase = re.sub(r\"\\'t\", \" not\", phrase)\n        phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n        phrase = re.sub(r\"\\'m\", \" am\", phrase)\n        return phrase\n    except:\n        return 0\n\ndef cleaning_text(df):\n    from tqdm import tqdm\n    preprocessed_item_description = []\n    # tqdm is for printing the status bar\n    for sentance in tqdm(df['item_description'].values):\n        sent = decontracted(str(sentance))\n        sent = sent.replace('\\\\r', ' ')\n        sent = sent.replace('\\\\\"', ' ')\n        sent = sent.replace('\\\\n', ' ')\n        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n        # https:\/\/gist.github.com\/sebleier\/554280\n        sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n        preprocessed_item_description.append(sent.lower().strip())\n    df = df.drop('item_description',axis=1)\n    df['item_description'] = preprocessed_item_description\n    preprocessed_item_description = None\n    del(preprocessed_item_description)\n    gc.collect()\n    return(df)\n\ndef fill_brand(df):\n    brand_names_categories = dict(df[df['brand_name'] != 'missing'][['brand_name','category_name']].astype('str')\\\n                              .groupby('brand_name').agg(concat_categories).reset_index().values.tolist())\n    brands_sorted_by_size = list(sorted(filter(lambda y: len(y) >= 3, \\\n                                           list(brand_names_categories.keys())), \\\n                                            key = lambda x: -len(x)))\n    \n    train_names_unknown_brands_train = df[df['brand_name'] == 'missing'][['name','category_name']].\\\n                            astype('str').values\n    train_estimated_brands_train = []\n    \n    \n    for name, category in tqdm(train_names_unknown_brands_train):\n        for brand in brands_sorted_by_size:\n            if brand in name and category in brand_names_categories[brand]:\n                brand_name = brand\n            else:\n                brand_name = 'missing'\n        train_estimated_brands_train.append(brand_name)\n        \n    df.loc[df['brand_name'] == 'missing', 'brand_name'] = train_estimated_brands_train\n    \n    brand_names_categories = None\n    brands_sorted_by_size = None\n    train_names_unknown_brands_train = None\n    train_estimated_brands_train = None\n    brand_name= None\n    del(brand_names_categories,brands_sorted_by_size,train_names_unknown_brands_train,train_estimated_brands_train,brand_name)\n    gc.collect()\n    return(df)\n","ec5efda7":"def preprocessing(df):\n    #cleaning text\n    #cleaning_text(df)\n    print(\"Text_cleaning Done\")\n    # filling missing values with brand_name as 'missing'\n    df['brand_name'] = df['brand_name'].fillna('missing')\n    df['brand_name'] = df['brand_name'].astype('category')\n    print(\"preprocessing for brand_name Done\")\n    \n    # filling missing values with category_name as missing\/missing\/missing or we can simply remove these rows\n    df['category_name'] = df['category_name'].fillna('missing\/missing\/missing')\n    df['category_name'] = df['category_name'].fillna('missing\/missing\/missing')\n    print(\"preprocessing for category_name Done\")\n    \n    df['item_description'] = df['item_description'].fillna('missing')\n    print(\"preprocessing for item_description Done\")\n    \n    df['category_name']= df['category_name'].str.split('\/')\n    df['main_category'] = df['category_name'].str.get(0).replace('', 'missing').astype('category')\n    df['sub_category_1'] = df['category_name'].str.get(1).replace('', 'missing').astype('category')\n    df['sub_category_2'] = df['category_name'].str.get(2).replace('', 'missing').astype('category')\n    print(\"split main category in to 3 Done\")\n        \n    df['item_condition_id'] = df['item_condition_id'].astype('category')\n    df['item_condition_id'] = df['item_condition_id'].cat.add_categories(['missing']).fillna('missing')\n    print(\"preprocessing for item_condition_id Done\")\n    \n    df['shipping'] = df['shipping'].astype('category')\n    df['shipping'] = df['shipping'].cat.add_categories(['missing']).fillna('missing')\n    print(\"preprocessing for shipping Done\")\n    \n    df['item_description'].fillna('missing', inplace=True)\n    print(\"preprocessing for item_description Done\")    \n    #fill_brand(df)\n    return(df)\n\ndef adding_new_features(df):\n    df['name_first'] = df['name'].apply(clean_name)\n    df['name_first_count'] = transform_col(data_frame = df, col_name = 'name_first' )\n    df['main_cat_count'] = transform_col(data_frame = df, col_name = 'main_category' )\n    df['sub_cat_1_count'] = transform_col(data_frame = df, col_name = 'sub_category_1' )\n    df['sub_cat_2_count'] = transform_col(data_frame = df, col_name = 'sub_category_2' )\n    df['brand_name_count'] = transform_col(data_frame = df, col_name = 'brand_name' )\n    df['DescriptionLower'] = df.item_description.str.count('[a-z]')\n    df['NameLower'] = df.name.str.count('[a-z]')\n    df['NameUpper'] = df.name.str.count('[A-Z]')\n    df['DescriptionUpper'] = df.item_description.str.count('[A-Z]')\n    df['name_len'] = df['name'].apply(lambda x: len(x))\n    df['des_len'] = df['item_description'].apply(lambda x: len(x))\n    df['name_desc_len_ratio'] = df['name_len']\/df['des_len']\n    df['desc_word_count'] = df['item_description'].apply(lambda x: len(x.split()))\n    df['mean_des'] = df['item_description'].apply(lambda x: 0 if len(x) == 0 else float(len(x.split())) \/ len(x)) * 10\n    df['name_word_count'] = df['name'].apply(lambda x: len(x.split()))\n    df['mean_name'] = df['name'].apply(lambda x: 0 if len(x) == 0 else float(len(x.split())) \/ len(x))  * 10\n    df['desc_letters_per_word'] = df['des_len'] \/ df['desc_word_count']\n    df['name_letters_per_word'] = df['name_len'] \/ df['name_word_count']\n    df['NameLowerRatio'] = df['NameLower'] \/ df['name_len']\n    df['DescriptionLowerRatio'] = df['DescriptionLower'] \/ df['des_len']\n    df['NameUpperRatio'] = df['NameUpper'] \/ df['name_len']\n    df['DescriptionUpperRatio'] = df['DescriptionUpper'] \/ df['des_len']\n    df['NamePunctCount'] = df.name.str.count(RE_PUNCTUATION)\n    df['DescriptionPunctCount'] = df.item_description.str.count(RE_PUNCTUATION)\n    df['NamePunctCountRatio'] = df['NamePunctCount'] \/ df['name_word_count']\n    df['DescriptionPunctCountRatio'] = df['DescriptionPunctCount'] \/ df['desc_word_count']\n    df['NameDigitCount'] = df.name.str.count('[0-9]')\n    df['DescriptionDigitCount'] = df.item_description.str.count('[0-9]')\n    df['NameDigitCountRatio'] = df['NameDigitCount'] \/ df['name_word_count']\n    df['DescriptionDigitCountRatio'] = df['DescriptionDigitCount']\/df['desc_word_count']\n    df['stopword_ratio_desc'] = df['item_description'].apply(lambda x: len([w for w in x.split() if w in stopwords])) \/ df['desc_word_count']\n    df['num_sum'] = df['item_description'].apply(sum_numbers) \n    df['weird_characters_desc'] = df['item_description'].str.count(non_alphanumpunct)\n    df['weird_characters_name'] = df['name'].str.count(non_alphanumpunct)\n    df['prices_count'] = df['item_description'].str.count('[rm]')\n    df['price_in_name'] = df['item_description'].str.contains('[rm]', regex=False).astype('int')\n    df.drop('category_name', axis=1, inplace=True)\n    return(df)","bfe2636f":"preprocessing(X_train)\nadding_new_features(X_train)","2d61ffbf":"X_train.head(5)","7ff68279":"#check for the NAN values\nX_train.columns[X_train.isna().any()].tolist()","6ea46f01":"preprocessing(X_cv)\nadding_new_features(X_cv)","1d6f9e4f":"X_cv.columns[X_cv.isna().any()].tolist()","c831e056":"total_cols = set(X_train.columns.values)\n\nbasic_cols = {'name', 'item_condition_id', 'brand_name',\n  'shipping', 'item_description', 'main_category',\n  'sub_category_1', 'sub_category_2', 'name_first'}\n\nnumeric_cols = total_cols - basic_cols\n\ncols_to_normalize = numeric_cols - {'price_in_name'}\n\ntext_cols = {'name', 'item_description'}\n\ncategorical_cols = {'item_condition_id','brand_name','shipping','main_category','sub_category_1','sub_category_2','name_first'}","eae231c4":"from sklearn.preprocessing import Normalizer\n\nnormalizer = Normalizer(copy=False)\nnormalizer.fit(X_train[list(cols_to_normalize)])","08584d32":"import pickle\npickle.dump(normalizer, open(\"normalizer.pickle\", \"wb\"),protocol=4)\nnormalizer = None\ndel(normalizer)\ngc.collect()","8e2d0453":"def normalize_dataframe(df):\n    with open('normalizer.pickle',mode='rb') as model_f:\n        normalizer_load = pickle.load(model_f)\n    df[list(cols_to_normalize)]= normalizer_load.transform(df[list(cols_to_normalize)])\n    df[list({'item_condition_id','shipping'})] = df[list({'item_condition_id','shipping'})].astype('category')\n    normalizer_load = None \n    del(normalizer_load)\n    gc.collect()\n    return df","4b08301b":"normalize_dataframe(X_train)\nnormalize_dataframe(X_cv)","6239479c":"wb_desc = wordbatch.WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 2,\n                                                              \"hash_ngrams_weights\": [1.0, 1.0],\n                                                              \"hash_size\": 2 ** 28,\n                                                              \"norm\": \"l2\",\n                                                              \"tf\": 1.0,\n                                                              \"idf\": None}), procs=8)\nwb_desc.dictionary_freeze = True\n","6e75acc9":"wb_desc.fit(X_train['item_description'])\nX_description_train_wb = wb_desc.transform(X_train['item_description'])\nX_description_cv_wb = wb_desc.fit_transform(X_cv['item_description'])","bc2df2d4":"mask_desc = np.where(X_description_train_wb.getnnz(axis=0) > 3)[0]","08a1e292":"X_description_train_wb = X_description_train_wb[:, mask_desc]\nX_description_cv_wb = X_description_cv_wb[:, mask_desc]","07b1001d":"X_description_train_wb.shape\nX_description_cv_wb.shape","363f304a":"model_desc = Ridge(solver=\"sag\", fit_intercept=True, random_state=205, alpha=5)\nmodel_desc.fit(X_description_train_wb, Y_train)","f44d64f5":"pred_train_1 = model_desc.predict(X_description_train_wb)\npred_cv_1 = model_desc.predict(X_description_cv_wb)\nprint(\"Train rmsle: \"+str(rmsle(10 ** Y_train-1, 10 ** pred_train_1-1)))\nprint(\"CV rmsle: \"+str(rmsle(10 ** Y_cv-1, 10 ** pred_cv_1-1)))","c17dee26":"import pickle \npickle.dump(mask_desc, open(\"mask_desc.pickle\", \"wb\"),protocol=4)\npickle.dump(wb_desc, open(\"wb_desc.pickle\", \"wb\"),protocol=4)\npickle.dump(model_desc, open(\"model_desc.pickle\", \"wb\"),protocol=4)","a92e289f":"mask_desc = None\nwb_desc = None\nmodel_desc = None\ndel(mask_desc,wb_desc,model_desc)\ngc.collect()","61604559":"wb_name = wordbatch.WordBatch(normalize_text, extractor=(WordBag, {\"hash_ngrams\": 2,\n                                                              \"hash_ngrams_weights\": [1.5, 1.0],\n                                                              \"hash_size\": 2 ** 29,\n                                                              \"norm\": None,\n                                                              \"tf\": 'binary',\n                                                              \"idf\": None,\n                                                              }), procs=8)\nwb_name.dictionary_freeze = True\n","64221dde":"wb_name.fit(X_train['name'])\nX_name_train_wb = wb_name.transform(X_train['name'])\nX_name_cv_wb = wb_name.fit_transform(X_cv['name'])","023fd8b4":"mask_name = np.where(X_name_train_wb.getnnz(axis=0) > 3)[0]","c5d6bb7d":"X_name_train_wb = X_name_train_wb[:, mask_name]\nX_name_cv_wb = X_name_cv_wb[:, mask_name]","37260cdf":"model_name = Ridge(solver=\"sag\", fit_intercept=True, random_state=205, alpha= 5)\nmodel_name.fit(X_name_train_wb, Y_train)\n","1055deab":"pred_train_2 = model_name.predict(X_name_train_wb)\npred_cv_2 = model_name.predict(X_name_cv_wb)","d1130d5a":"print(\"Train rmsle: \"+str(rmsle(10 ** Y_train-1, 10 ** pred_train_2-1)))\nprint(\"CV rmsle: \"+str(rmsle(10 ** Y_cv-1, 10 ** pred_cv_2-1)))","0bbb2fad":"import pickle\n\npickle.dump(mask_name, open(\"mask_name.pickle\", \"wb\"),protocol=4)\npickle.dump(wb_name, open(\"wb_name.pickle\", \"wb\"),protocol=4)\npickle.dump(model_name, open(\"model_name.pickle\", \"wb\"),protocol=4)\n","9060a939":"mask_name = None\nwb_name = None\nmodel_name = None\ndel(mask_name,wb_name,model_name)\ngc.collect()","b4e8a698":"lb_brand_name = LabelBinarizer(sparse_output=True)\n\nX_brand_train = lb_brand_name.fit_transform(X_train['brand_name'])\n\nX_brand_cv = lb_brand_name.transform(X_cv['brand_name'])","5e97a706":"lb_main_category = LabelBinarizer(sparse_output=True)\n\nX_main_cat_train = lb_main_category.fit_transform(X_train['main_category'])\n\nX_main_cat_cv = lb_main_category.transform(X_cv['main_category'])","b1dc52ab":"lb_sub_category_1 = LabelBinarizer(sparse_output=True)\n\nX_main_sub_cat_1_train = lb_sub_category_1.fit_transform(X_train['sub_category_1'])\n\nX_main_sub_cat_1_cv = lb_sub_category_1.transform(X_cv['sub_category_1'])","e2efcd45":"lb_sub_category_2 = LabelBinarizer(sparse_output=True)\n\nX_main_sub_cat_2_train = lb_sub_category_2.fit_transform(X_train['sub_category_2'])\n\nX_main_sub_cat_2_cv = lb_sub_category_2.transform(X_cv['sub_category_2'])","189377cf":"X_dummies_train = csr_matrix(\n    pd.get_dummies(X_train[list(total_cols - (basic_cols))],\n                   sparse=True).values)\n\nX_dummies_train_1 = csr_matrix(\n    pd.get_dummies(X_train[list({'item_condition_id', 'shipping'})],\n                   sparse=True).values)","1e0f0836":"X_dummies_cv = csr_matrix(\n    pd.get_dummies(X_cv[list(total_cols - (basic_cols))],\n                   sparse=True).values)\n\nX_dummies_cv_1 = csr_matrix(\n    pd.get_dummies(X_cv[list({'item_condition_id', 'shipping'})],\n                   sparse=True).values)","508672df":"sparse_merge_train = hstack((X_name_train_wb , X_description_train_wb, X_brand_train, X_main_cat_train,\n                             X_main_sub_cat_1_train, X_main_sub_cat_2_train,X_dummies_train,X_dummies_train_1)).tocsr()","bedb061d":"sparse_merge_cv = hstack(( X_name_cv_wb,X_description_cv_wb,X_brand_cv,X_main_cat_cv,\n                             X_main_sub_cat_1_cv,X_main_sub_cat_2_cv,X_dummies_cv,X_dummies_cv_1)).tocsr()","91d4e2b3":"X_dummies_train = None\nX_description_train_wb = None\nX_name_train_wb = None\nX_dummies_cv = None\nX_description_cv_wb = None\nX_name_cv_wb = None\ndel(X_dummies_train, X_description_train_wb,X_name_train_wb)\ndel(X_dummies_cv, X_description_cv_wb,X_name_cv_wb)\n\n#del(X_dummies_train, X_description_train, X_brand_train, X_main_cat_train,\\\n#                             X_main_sub_cat_1_train, X_main_sub_cat_2_train, X_name_train)\n#del(X_dummies_cv, X_description_cv, X_brand_cv, X_main_cat_cv,\\\n#                             X_main_sub_cat_1_cv, X_main_sub_cat_2_cv, X_name_cv)\ngc.collect()","3ea9d63c":"print(sparse_merge_train.shape)\nprint(sparse_merge_cv.shape)","05b9a2e6":"import pickle\n\npickle.dump(lb_brand_name, open(\"lb_brand_name.pickle\", \"wb\"),protocol=4)\npickle.dump(lb_main_category, open(\"lb_main_category.pickle\", \"wb\"),protocol=4)\npickle.dump(lb_sub_category_1, open(\"lb_sub_category_1.pickle\", \"wb\"),protocol=4)\npickle.dump(lb_sub_category_2, open(\"lb_sub_category_2.pickle\", \"wb\"),protocol=4)","03ecd0c5":"lb_brand_name = None\nlb_main_category = None\nlb_sub_category_1 = None\nlb_sub_category_2 = None\ndel(lb_brand_name,lb_main_category,lb_sub_category_1,lb_sub_category_2)\ngc.collect()","0f391206":"model_FM_FTRL = FM_FTRL(alpha=0.035, beta=0.001, L1=0.00001, L2=0.15, D=sparse_merge_train.shape[1],\n                alpha_fm=0.05, L2_fm=0.0, init_fm=0.01,\n                D_fm=100, e_noise=0, iters=1, inv_link=\"identity\", threads=4)\nmodel_FM_FTRL.fit(sparse_merge_train, Y_train)","c014bc89":"pred_train_3 = model_FM_FTRL.predict(sparse_merge_train)\npred_cv_3 = model_FM_FTRL.predict(sparse_merge_cv)","47614225":"print(\"Train rmsle: \"+str(rmsle(10 ** Y_train-1, 10 ** pred_train_3-1)))\nprint(\"CV rmsle: \"+str(rmsle(10 ** Y_cv-1, 10 ** pred_cv_3-1)))","5d8ca300":"import pickle\n\npickle.dump(model_FM_FTRL, open(\"model_FM_FTRL.pickle\", \"wb\"),protocol=4)\nmodel_FM_FTRL = None\ndel(model_FM_FTRL)\ngc.collect()","3051d447":"print(sparse_merge_train.shape)\nprint(sparse_merge_cv.shape)","ed6970b0":"fselect = SelectKBest(f_regression, k=48000)\ntrain_kbest_features = fselect.fit_transform(sparse_merge_train, Y_train)\ncv_kbest_features = fselect.transform(sparse_merge_cv)","ec9fa646":"import pickle\npickle.dump(fselect, open(\"fselect.pickle\", \"wb\"),protocol=4)\nfselect = None\ndel(fselect)\ngc.collect()","cd1df193":"cv_kbest_features.shape","0067c721":"sparse_merge_train= None\nsparse_merge_cv = None\ndel(sparse_merge_train,sparse_merge_cv)\ngc.collect()","148cbc98":"tfidf_desc = TfidfVectorizer(max_features=500000,\n                     ngram_range=(1, 3),\n                     stop_words=None)\nX_desc_train_tfidf = tfidf_desc.fit_transform(X_train['item_description'])","d53eaab6":"X_desc_cv_tfidf = tfidf_desc.transform(X_cv['item_description'])","beed720f":"pickle.dump(tfidf_desc, open(\"tfidf_desc.pickle\", \"wb\"),protocol=4)\ntfidf_desc = None\ndel(tfidf_desc)\ngc.collect()","99baade9":"tfidf_name = TfidfVectorizer(max_features=250000,\n                     ngram_range=(1, 3),\n                     stop_words=None)\nX_name_train_tfidf = tfidf_name.fit_transform(X_train['name'])","b813a930":"X_name_cv_tfidf =tfidf_name.transform(X_cv['name'])","f9a65604":"pickle.dump(tfidf_name, open(\"tfidf_name.pickle\", \"wb\"),protocol=4)\ntfidf_name = None\ndel(tfidf_name)\ngc.collect()","dd75ccce":"sparse_merge_train_1 = hstack((X_name_train_tfidf , X_desc_train_tfidf, X_brand_train, X_main_cat_train,\n                             X_main_sub_cat_1_train, X_main_sub_cat_2_train,X_dummies_train_1)).tocsr()","28451fd3":"X_dummies_train_1 = None\nX_brand_train = None\nX_main_cat_train = None\nX_main_sub_cat_1_train = None\nX_main_sub_cat_2_train = None\nX_name_train_tfidf = None\nX_desc_train_tfidf = None\ndel(X_dummies_train_1, X_brand_train, X_main_cat_train,\\\n                            X_main_sub_cat_1_train, X_main_sub_cat_2_train, X_name_train_tfidf,X_desc_train_tfidf)\ngc.collect()","da2f12a8":"sparse_merge_cv_1 = hstack((X_name_cv_tfidf , X_desc_cv_tfidf, X_brand_cv, X_main_cat_cv,\n                             X_main_sub_cat_1_cv, X_main_sub_cat_2_cv,X_dummies_cv_1)).tocsr()","c4358271":"X_dummies_cv_1 = None\nX_brand_cv = None\nX_main_cat_cv = None\nX_main_sub_cat_1_cv = None\nX_main_sub_cat_2_cv = None\nX_name_cv_tfidf = None\nX_desc_cv_tfidf = None\ndel(X_dummies_cv_1, X_brand_cv, X_main_cat_cv,\\\n                            X_main_sub_cat_1_cv, X_main_sub_cat_2_cv, X_name_cv_tfidf,X_desc_cv_tfidf)\ngc.collect()","a495804c":"model_Ridge_set_2 = Ridge(solver=\"sag\", fit_intercept=True, random_state=205, alpha=5)\nmodel_Ridge_set_2.fit(sparse_merge_train_1, Y_train)","5488dd68":"pred_train_4 = model_Ridge_set_2.predict(sparse_merge_train_1)\npred_cv_4 =  model_Ridge_set_2.predict(sparse_merge_cv_1)\nprint(\"Train rmsle: \"+str(rmsle(10 ** Y_train-1, 10 ** pred_train_4-1)))\nprint(\"CV rmsle: \"+str(rmsle(10 ** Y_cv-1, 10 ** pred_cv_4-1)))","6ea16b37":"sparse_merge_train_1.shape","aa9cba3c":"pickle.dump(model_Ridge_set_2, open(\"model_Ridge_set_2.pickle\", \"wb\"),protocol=4)\nmodel_Ridge_set_2 = None\ndel(model_Ridge_set_2)\ngc.collect()","3cb9ff66":"model_MNB_set_2 = MultinomialNB(alpha=1.0, fit_prior=True)\nmodel_MNB_set_2.fit(sparse_merge_train_1, Y_train >= 4)","36a2dc42":"pred_train_5 = model_MNB_set_2.predict(sparse_merge_train_1)\npred_cv_5 =  model_MNB_set_2.predict(sparse_merge_cv_1)\nprint(\"Train rmsle: \"+str(rmsle(10 ** Y_train-1, 10 ** pred_train_5-1)))\nprint(\"CV rmsle: \"+str(rmsle(10 ** Y_cv-1, 10 ** pred_cv_5-1)))","20bcca7f":"pickle.dump(model_MNB_set_2, open(\"model_MNB_set_2.pickle\", \"wb\"),protocol=4)\nmodel_MNB_set_2 = None\nsparse_merge_train_1 = None\nsparse_merge_cv_1 = None\ndel(model_MNB_set_2)\ndel(sparse_merge_train_1,sparse_merge_cv_1)\ngc.collect()","00d92ea7":"f_cats = ['brand_name', 'main_category', 'sub_category_1', 'sub_category_2', 'name_first']","eaacba4a":"from category_encoders.target_encoder import TargetEncoder\n\ntargetencoder = TargetEncoder(min_samples_leaf=100, smoothing=10,cols=f_cats,return_df=False)","0eeae186":"targetencoder.fit(X_train[f_cats],Y_train)","a26758c5":"X_train_target_encode = targetencoder.transform(X_train[f_cats])","d2fac851":"X_cv_target_encode = targetencoder.transform(X_cv[f_cats])","577ab9c1":"def add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))\n","8067060a":"X_train_target_encode[:,0].shape","d1af9726":"X_train_target_encode[:,0] = add_noise(X_train_target_encode[:,0],noise_level=0.01)\nX_train_target_encode[:,1] = add_noise(X_train_target_encode[:,1],noise_level=0.01)\nX_train_target_encode[:,2] = add_noise(X_train_target_encode[:,2],noise_level=0.01)\nX_train_target_encode[:,3] = add_noise(X_train_target_encode[:,3],noise_level=0.01)\nX_train_target_encode[:,4] = add_noise(X_train_target_encode[:,4],noise_level=0.01)","7a89e385":"X_train_target_encode[0,:]","81a8f736":"pickle.dump(targetencoder, open(\"targetencoder.pickle\", \"wb\"),protocol=4)\ntargetencoder = None\ndel(targetencoder)\ngc.collect()","6bbd1ddd":"train_features = hstack((pred_train_1.reshape(-1,1),\\\n                         pred_train_2.reshape(-1,1),\\\n                         pred_train_3.reshape(-1,1),\\\n                         pred_train_4.reshape(-1,1),\\\n                         pred_train_5.reshape(-1,1),\\\n                         X_train_target_encode,\\\n                         train_kbest_features)).tocsr()","88817028":"cv_features = hstack((pred_cv_1.reshape(-1,1),\\\n                      pred_cv_2.reshape(-1,1),\\\n                      pred_cv_3.reshape(-1,1),\\\n                      pred_cv_4.reshape(-1,1),\\\n                      pred_cv_5.reshape(-1,1),\\\n                      X_cv_target_encode,\\\n                      cv_kbest_features)).tocsr()","9540bbb7":"pickle.dump(train_features, open(\"train_features.pickle\", \"wb\"),protocol=4)\npickle.dump(cv_features, open(\"cv_features.pickle\", \"wb\"),protocol=4)","c4b57d1a":"pred_train_1 = None\npred_train_2 = None\npred_train_3 = None\npred_train_4 = None\npred_train_5 = None\nX_train_target_encode = None\ntrain_kbest_features = None\n\npred_cv_1 = None\npred_cv_2 = None\npred_cv_3 = None\npred_cv_4 = None\npred_cv_5 = None\nX_cv_target_encode = None\ncv_kbest_features = None\n\n\ndel(pred_train_1,\\\n    pred_train_2,\\\n    pred_train_3,\\\n    pred_train_4,\\\n    pred_train_5,\\\n    X_train_target_encode,\\\n    train_kbest_features)\ndel(pred_cv_1,\\\n    pred_cv_2,\\\n    pred_cv_3,\\\n    pred_cv_4,\\\n    pred_cv_5,\\\n    X_cv_target_encode,\\\n    cv_kbest_features)\ngc.collect()","31c17710":"d_train = lgb.Dataset(train_features, label=Y_train)\nd_valid = lgb.Dataset(cv_features, label=Y_cv)\nwatchlist = [d_train, d_valid]","df409719":"params = {\n         'colsample_bytree': 0.42799939792816927,\n          'max_depth': 8,\n          'min_child_samples': 370,\n          'min_child_weight': 0.01,\n          'num_leaves': 29,\n          'reg_lambda': 5,\n          'subsample': 0.6739316550896339,\n          'learning_rate':0.1,\n          'reg_alpha' :0.5,\n          'boosting_type': 'gbdt',\n          'objective' : 'regression',\n          'metric' : 'RMSE',\n          'verbosity': -1,\n          'lambda_l1': 10,\n         'lambda_l2': 10\n         }\n","d9f082de":"model_lgb_final = lgb.train(params,\n                  train_set=d_train,\n                  num_boost_round=3000,\n                  valid_sets=watchlist,\n                  verbose_eval=200,early_stopping_rounds=100)\n","5ed70597":"pred_train_6 = model_lgb_final.predict(train_features)\npred_cv_6 = model_lgb_final.predict(cv_features)","1dcc758d":"print(\"Train rmsle: \"+str(rmsle(10 ** Y_train-1, 10 ** pred_train_6-1)))\nprint(\"CV rmsle: \"+str(rmsle(10 ** Y_cv-1, 10 ** pred_cv_6-1)))","ebdf8f2b":"pickle.dump(model_lgb_final, open(\"model_lgb_final.pickle\", \"wb\"),protocol=4)\nmodel_lgb_final = None\ndel(model_lgb_final)\ngc.collect()","89230e0e":"train_features= None\ncv_features = None\nd_train = None\nd_valid = None\nwatchlist = None\ndel(train_features,cv_features,d_train,d_valid,watchlist)\n\ngc.collect()","aeafd9ef":"X_train = None\nX_cv = None\nY_train = None\nY_test = None\ndel(X_train,X_cv,Y_train,Y_cv)\ngc.collect()","6744b21a":"test_id = []\nprediction = []","4448bf06":"def predict_final(df):\n    test_id.extend(list(df['test_id']))\n    df.drop('test_id', axis=1, inplace=True)\n    preprocessing(df)\n    adding_new_features(df)\n    normalize_dataframe(df)\n    \n######### Loading wordbag_desc_model & model_1 pred ###########    \n    with open('mask_desc.pickle',mode='rb') as model_f:\n        mask_desc = pickle.load(model_f)\n    with open('wb_desc.pickle',mode='rb') as model_f:\n        wb_desc= pickle.load(model_f)\n    with open('model_desc.pickle',mode='rb') as model_f:\n        model_desc = pickle.load(model_f)\n    X_description_test_wb = wb_desc.transform(df['item_description'])\n    X_description_test_wb = X_description_test_wb[:, mask_desc]\n    print(X_description_test_wb.shape)\n    \n    pred_test_1 = model_desc.predict(X_description_test_wb)\n    mask_desc = None\n    wb_desc = None\n    model_desc = None\n    del(mask_desc,wb_desc,model_desc)\n    gc.collect()\n\n######### Loading wordbag_name_model & model_2 pred ###########    \n    with open('mask_name.pickle',mode='rb') as model_f:\n        mask_name = pickle.load(model_f)\n    with open('wb_name.pickle',mode='rb') as model_f:\n        wb_name= pickle.load(model_f)\n    with open('model_name.pickle',mode='rb') as model_f:\n        model_name = pickle.load(model_f)\n    X_name_test_wb = wb_name.transform(df['name'])\n    X_name_test_wb = X_name_test_wb[:, mask_name]\n    \n    print(X_name_test_wb.shape)\n    pred_test_2 = model_name.predict(X_name_test_wb)\n    mask_name = None\n    wb_name = None\n    model_name = None\n    del(mask_name,wb_name,model_name)\n    gc.collect()\n\n###################### Lb and Dummies ##########################\n    with open('lb_brand_name.pickle',mode='rb') as model_f:\n        lb_brand_name = pickle.load(model_f)\n    with open('lb_main_category.pickle',mode='rb') as model_f:\n        lb_main_category = pickle.load(model_f)\n    with open('lb_sub_category_1.pickle',mode='rb') as model_f:\n        lb_sub_category_1 = pickle.load(model_f)\n    with open('lb_sub_category_2.pickle',mode='rb') as model_f:\n        lb_sub_category_2 = pickle.load(model_f)\n        \n    X_brand_test = lb_brand_name.transform(df['brand_name'])\n    X_main_cat_test = lb_main_category.transform(df['main_category'])\n    X_main_sub_cat_1_test = lb_sub_category_1.transform(df['sub_category_1'])\n    X_main_sub_cat_2_test = lb_sub_category_2.transform(df['sub_category_2'])\n    \n    X_dummies_test = csr_matrix(\n        pd.get_dummies(df[list(total_cols - (basic_cols))],\n                   sparse=True).values)\n\n    X_dummies_test_1 = csr_matrix(\n        pd.get_dummies(df[list({'item_condition_id', 'shipping'})],\n                   sparse=True).values)\n    \n##################### sparse matrix feature_set_1 #############################\n    sparse_merge_test = hstack((X_name_test_wb , X_description_test_wb, X_brand_test, X_main_cat_test,\n                             X_main_sub_cat_1_test, X_main_sub_cat_2_test,X_dummies_test,X_dummies_test_1)).tocsr()\n    X_dummies_test = None \n    X_description_test_wb = None\n    X_name_test_wb = None\n    del(X_dummies_test, X_description_test_wb,X_name_test_wb)\n    gc.collect()\n    print(sparse_merge_test.shape)\n\n############################### FMFTRL ###################################\n    with open('model_FM_FTRL.pickle',mode='rb') as model_f:\n        model_FM_FTRL = pickle.load(model_f)\n    pred_test_3 = model_FM_FTRL.predict(sparse_merge_test)\n    model_FM_FTRL = None\n    del(model_FM_FTRL)\n    gc.collect()\n######################### Kbest Select ##################################\n    with open('fselect.pickle',mode='rb') as model_f:\n        fselect = pickle.load(model_f)\n    test_kbest_features = fselect.transform(sparse_merge_test)\n    fselect = None\n    print(test_kbest_features.shape)\n    del(fselect)\n    gc.collect()\n    sparse_merge_train= None\n    sparse_merge_cv = None\n    del(sparse_merge_train,sparse_merge_cv)\n    gc.collect()\n    \n########################## TFIDF Desc Train ############################\n    with open('tfidf_desc.pickle',mode='rb') as model_f:\n        tfidf_desc = pickle.load(model_f)\n    X_desc_test_tfidf = tfidf_desc.transform(df['item_description'])\n    tfidf_desc = None\n    del(tfidf_desc)\n    gc.collect()\n    \n########################## TFIDF Name Train ############################\n    with open('tfidf_name.pickle',mode='rb') as model_f:\n        tfidf_name = pickle.load(model_f)\n    X_name_test_tfidf = tfidf_name.transform(df['name'])\n    tfidf_name = None\n    del(tfidf_name)\n    gc.collect()\n\n##################### sparse matrix feature_set_2 #############################\n    sparse_merge_test_1 = hstack((X_name_test_tfidf , X_desc_test_tfidf, X_brand_test, X_main_cat_test,\n                             X_main_sub_cat_1_test, X_main_sub_cat_2_test,X_dummies_test_1)).tocsr()\n    X_dummies_test_1 = None\n    X_brand_test = None\n    X_main_cat_test = None\n    X_main_sub_cat_1_test = None\n    X_main_sub_cat_2_test = None\n    X_name_test_tfidf = None\n    X_desc_test_tfidf = None\n    del(X_dummies_test_1, X_brand_test, X_main_cat_test,\\\n                            X_main_sub_cat_1_test, X_main_sub_cat_2_test, X_name_test_tfidf,X_desc_test_tfidf)\n    gc.collect()\n\n######################### Ridge model on feature_set_2 #############################\n    with open('model_Ridge_set_2.pickle',mode='rb') as model_f:\n        model_Ridge_set_2 = pickle.load(model_f)\n    pred_test_4 = model_Ridge_set_2.predict(sparse_merge_test_1)\n    model_Ridge_set_2 = None\n    del(model_Ridge_set_2)\n    gc.collect()\n\n########################## MNB model on feature_set_2 ############################\n    with open('model_MNB_set_2.pickle',mode='rb') as model_f:\n        model_MNB_set_2 = pickle.load(model_f)\n    pred_test_5 = model_MNB_set_2.predict(sparse_merge_test_1)\n    model_MNB_set_2 = None\n    sparse_merge_train_1 = None\n    sparse_merge_cv_1 = None\n    del(model_MNB_set_2)\n    del(sparse_merge_train_1,sparse_merge_cv_1)\n    gc.collect()\n\n############################ target_encoding ##################################\n    with open('targetencoder.pickle',mode='rb') as model_f:\n        targetencoder = pickle.load(model_f)\n    f_cats = ['brand_name', 'main_category', 'sub_category_1', 'sub_category_2', 'name_first']\n    X_test_target_encode = targetencoder.transform(df[f_cats])\n    targetencoder = None\n    del(targetencoder)\n    gc.collect()\n    \n######################### Final sparse matrix #################################\n\n    test_features = hstack((pred_test_1.reshape(-1,1),\\\n                         pred_test_2.reshape(-1,1),\\\n                         pred_test_3.reshape(-1,1),\\\n                         pred_test_4.reshape(-1,1),\\\n                         pred_test_5.reshape(-1,1),\\\n                         X_test_target_encode,\\\n                         test_kbest_features)).tocsr()\n    \n    pred_test_1 = None\n    pred_test_2 = None\n    pred_test_3 = None\n    pred_test_4 = None\n    pred_test_5 = None\n    X_test_target_encode = None\n    test_kbest_features = None\n\n\n    del(pred_test_1,\\\n        pred_test_2,\\\n        pred_test_3,\\\n        pred_test_4,\\\n        pred_test_5,\\\n        X_test_target_encode,\\\n        test_kbest_features)\n    gc.collect()\n\n########################## Final lgbm prediction ##################\n    with open('model_lgb_final.pickle',mode='rb') as model_f:\n        model_lgb_final = pickle.load(model_f)\n    #pred_test_6 = model_lgb_final.predict(test_features)\n    prediction.extend(list(model_lgb_final.predict(test_features)))\n    #prediction = prediction + list(model_lgb_final.predict(test_features))\n########################## del everything ######################\n    train_features = None\n    cv_features = None \n    d_train = None\n    d_valid = None\n    watchlist = None\n    del(train_features,cv_features,d_train,d_valid,watchlist)\n    gc.collect()","e732a805":"chunksize = 10 ** 6\nfor chunk in pd.read_csv('..\/input\/mercari-price-suggestion-challenge\/test_stg2.tsv', delimiter='\\t',chunksize=chunksize):\n    predict_final(chunk)","3bce54be":"submission = pd.DataFrame()\nsubmission['test_id'] = np.asarray(test_id)\nsubmission['price'] = (10 ** np.asarray(prediction) - 1)\nsubmission.to_csv('stacked_submission_1.csv', index=False)","901bce7a":"# item_desc at train time","6397a91f":"# FMFTRL training","9f12f352":"# TFIDF Name Train","39c9ef4c":"# Normalizing numerical columns","e07c34cc":"# Target Encoding","53298b58":"# Ridge on Feature set-2","e46b23f4":"# hstack features set_2","f2c6bb39":"# name at train time","fa63afc8":"# LB training","b91a6e62":"# Stacking all features ","8b44f3ae":"# KBestSelect Train model","4563a3df":"# Final LGB model","ae8d637c":"# TFIDF Desc Train","bae6af46":"# Adding prediction to data frame","bcda2ee2":"# Prediction on test_stg2","61526045":"# MultinomialNB on Feature set-2"}}