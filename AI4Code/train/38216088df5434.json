{"cell_type":{"b1456c1f":"code","ab1d364a":"code","201009d9":"code","6655805a":"code","2ea9e348":"code","9e4937fa":"code","8b74b4ac":"code","f048e41e":"code","a5fe9694":"code","ac008f26":"code","0f4e32aa":"code","2ec05eb1":"code","ec5b3435":"code","7d157252":"code","cf260612":"code","e55580cc":"code","1da291d7":"code","eeb9aea1":"markdown","e51d7d31":"markdown","0fe56547":"markdown","fc06b0eb":"markdown","0f59696d":"markdown","6765e75f":"markdown","33cba06c":"markdown","1fb3bb99":"markdown","f2015562":"markdown","8810332a":"markdown","7f5f2a89":"markdown","692efdba":"markdown","c899a625":"markdown","768ffa7c":"markdown","896b3bf3":"markdown","3058878f":"markdown","57f058ee":"markdown","e5d09189":"markdown"},"source":{"b1456c1f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#Plotting Functions\nimport matplotlib.pyplot as plt\n\n#Aesthetics\n!pip install seaborn --upgrade #Update Seaborn for Plotting\nimport seaborn as sns\nsns.set_style('ticks') #No grid with ticks","ab1d364a":"#Data Import\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","201009d9":"cancer=pd.read_excel('\/kaggle\/input\/cancer-patients-data\/cancer patient data sets.xlsx', header=None)\n#To use the data we will need to fix the header\nnew_header=cancer.iloc[0]#Reads the first row which contains the headers\ncancer=cancer[1:]#Slices the rest of the data frame from header\ncancer.columns=new_header#Sets the header labels\ncancer.info()\ncancer.head()\ncancer_label=cancer.columns\nfor label in cancer_label:\n    print('***', label,'labels:',cancer[label].unique())","6655805a":"def Plotter(plot, x_label, y_label, x_rot=None, y_rot=None,  fontsize=12, fontweight=None, legend=True, save=False,save_name=None):\n    \"\"\"\n    Helper function to make a quick consistent plot with few easy changes for aesthetics.\n    Input:\n    plot: sns or matplot plotting function\n    x_label: x_label as string\n    y_label: y_label as string\n    x_rot: x-tick rotation, default=None, can be int 0-360\n    y_rot: y-tick rotation, default=None, can be int 0-360\n    fontsize: size of plot font on axis, defaul=12, can be int\/float\n    fontweight: Adding character to font, default=None, can be 'bold'\n    legend: Choice of including legend, default=True, bool\n    save: Saves image output, default=False, bool\n    save_name: Name of output image file as .png. Requires Save to be True.\n               default=None, string: 'Insert Name.png'\n    Output: A customized plot based on given parameters and an output file\n    \n    \"\"\"\n    #Ticks\n    ax.tick_params(direction='out', length=5, width=3, colors='k',\n               grid_color='k', grid_alpha=1,grid_linewidth=2)\n    plt.xticks(fontsize=fontsize, fontweight=fontweight, rotation=x_rot)\n    plt.yticks(fontsize=fontsize, fontweight=fontweight, rotation=y_rot)\n\n    #Legend\n    if legend==True:\n        plt.legend()\n    else:\n        ax.legend().remove()\n        \n    #Labels\n    plt.xlabel(x_label, fontsize=fontsize, fontweight=fontweight, color='k')\n    plt.ylabel(y_label, fontsize=fontsize, fontweight=fontweight, color='k')\n\n    #Removing Spines and setting up remianing, preset prior to use.\n    ax.spines['top'].set_color(None)\n    ax.spines['right'].set_color(None)\n    ax.spines['bottom'].set_color('k')\n    ax.spines['bottom'].set_linewidth(3)\n    ax.spines['left'].set_color('k')\n    ax.spines['left'].set_linewidth(3)\n    \n    if save==True:\n        plt.savefig(save_name)","2ea9e348":"fig, ax=plt.subplots()#Required outside of function. This needs to be activated first when plotting in every code block\nplot=sns.countplot(data=cancer, x='Level', palette=['darkblue','darkred','darkgreen'])#Count plot\nPlotter(plot, 'Level', 'Count', legend=None, save=True, save_name='Level Count.png')#Plotter function for aesthetics\nplot","9e4937fa":"fig, ax=plt.subplots()#Required outside of function. This needs to be activated first when plotting in every code block\nplot=sns.countplot(data=cancer, x='Level', hue='Gender', palette=['darkblue','darkred'])#Count plot\nPlotter(plot, 'Level', 'Count', legend=True, save=True, save_name='Level Count by Gender.png')#Plotter function for aesthetics\nplot","8b74b4ac":"fig, ax=plt.subplots()#Required outside of function. This needs to be activated first when plotting in every code block\nplot=sns.scatterplot(data=cancer, x='Alcohol use',y='Fatigue', hue='Level', palette=['darkblue','darkred','darkgreen'], s=50, marker='o')#Count plot\nPlotter(plot, 'Alcohol use', 'Fatique', legend=True, save=True, save_name='Level Dependence on Alcohol and Fatigue.png')#Plotter function for aesthetics\nplot","f048e41e":"fig, ax=plt.subplots()#Required outside of function. This needs to be activated first when plotting in every code block\nplot=sns.boxplot(data=cancer, x='Level', y='Age', palette=['darkblue','darkred','darkgreen'])#Count plot\nPlotter(plot, 'Level', 'Age', legend=False, save=True, save_name='Level by Age.png')#Plotter function for aesthetics\nplot","a5fe9694":"from sklearn.feature_selection import SelectKBest #Feature Selector\nfrom sklearn.feature_selection import f_classif #F-ratio statistic for categorical values","ac008f26":"#Feature Selection\nX=cancer.drop(['Level','Patient Id'], axis=1)\nY=cancer['Level']\nbestfeatures = SelectKBest(score_func=f_classif, k='all')\nfit = bestfeatures.fit(X,Y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Feature','Score']  #naming the dataframe columns\n\n#Visualize the feature scores\nfig, ax=plt.subplots(figsize=(7,7))\nplot=sns.barplot(data=featureScores, x='Score', y='Feature', palette='viridis',linewidth=0.5, saturation=2, orient='h')\nPlotter(plot, 'Score', 'Feature', legend=False, save=True, save_name='Feature Importance.png')#Plotter function for aesthetics\nplot","0f4e32aa":"#Selection method\nselection=featureScores[featureScores['Score']>=200]#Selects features that scored more than 200\nselection=list(selection['Feature'])#Generates the features into a list\nselection.append('Level')#Adding the Level string to be used to make new data frame\nnew_cancer=cancer[selection] #New dataframe with selected features\nnew_cancer.head() #Lets take a look at the first 5","2ec05eb1":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(new_cancer.drop(['Level'], axis=1), new_cancer['Level'],test_size=0.25, random_state=0)\n\n#Checking the shapes\nprint(\"X_train shape :\",X_train.shape)\nprint(\"Y_train shape :\",y_train.shape)\nprint(\"X_test shape :\",X_test.shape)\nprint(\"Y_test shape :\",y_test.shape)","ec5b3435":"from sklearn import preprocessing\nscaler=preprocessing.StandardScaler()\n\nX_train_scaled=scaler.fit_transform(X_train) #Scaling and fitting the training set to a model\nX_test_scaled=scaler.transform(X_test) #Transformation of testing set based off of trained scaler model","7d157252":"from sklearn.svm import SVC #Classifier\n#Packages\n\"\"\"These packages are required for the functions below\n\"\"\"\nfrom sklearn.model_selection import cross_val_score, RandomizedSearchCV, GridSearchCV #Paramterizers\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix #Accuracy metrics\nimport itertools #Used for iterations","cf260612":"def Searcher(estimator, param_grid, search, train_x, train_y, test_x, test_y,label=None):\n    \"\"\"\n    This is a helper function for tuning hyperparameters using the two search methods.\n    Methods must be GridSearchCV or RandomizedSearchCV.\n    Inputs:\n        estimator: Any Classifier\n        param_grid: Range of parameters to search\n        search: Grid search or Randomized search\n        train_x: input variable of your X_train variables \n        train_y: input variable of your y_train variables\n        test_x: input variable of your X_test variables\n        test_y: input variable of your y_test variables\n        label: str to print estimator, default=None\n    Output:\n        Returns the estimator instance, clf\n        \n    Modified from: https:\/\/www.kaggle.com\/crawford\/hyperparameter-search-comparison-grid-vs-random#To-standardize-or-not-to-standardize\n    \n    \"\"\"   \n    \n    try:\n        if search == \"grid\":\n            clf = GridSearchCV(\n                estimator=estimator, \n                param_grid=param_grid, \n                scoring=None,\n                n_jobs=-1, \n                cv=10, #Cross-validation at 10 replicates\n                verbose=0,\n                return_train_score=True\n            )\n        elif search == \"random\":           \n            clf = RandomizedSearchCV(\n                estimator=estimator,\n                param_distributions=param_grid,\n                n_iter=10,\n                n_jobs=-1,\n                cv=10,\n                verbose=0,\n                random_state=1,\n                return_train_score=True\n            )\n    except:\n        print('Search argument has to be \"grid\" or \"random\"')\n        sys.exit(0) #Exits program if not grid or random\n        \n    # Fit the model\n    clf.fit(X=train_x, y=train_y)\n    \n    #Testing the model\n    \n    try:\n        if search=='grid':\n            cfmatrix=confusion_matrix(\n            y_true=test_y, y_pred=clf.predict(test_x))\n        \n            #Defining prints for accuracy metrics of grid\n            print(\"**Grid search results of\", label,\"**\")\n            print(\"The best parameters are:\",clf.best_params_)\n            print(\"Best training accuracy:\\t\", clf.best_score_)\n            print('Classification Report:')\n            print(classification_report(y_true=test_y, y_pred=clf.predict(test_x))\n             )\n        elif search == 'random':\n            cfmatrix=confusion_matrix(\n            y_true=test_y, y_pred=clf.predict(test_x))\n\n            #Defining prints for accuracy metrics of grid\n          \n            print(\"**Random search results of\", label,\"**\")\n            print(\"The best parameters are:\",clf.best_params_)\n            print(\"Best training accuracy:\\t\", clf.best_score_)\n            print('Classification Report:')\n            print(classification_report(y_true=test_y, y_pred=clf.predict(test_x))\n             )\n    except:\n        print('Search argument has to be \"grid\" or \"random\"')\n        sys.exit(0) #Exits program if not grid or random\n        \n    return clf, cfmatrix; #Returns a trained classifier with best parameters","e55580cc":"def plot_confusion_matrix(cm, label,color=None,title=None):\n    \"\"\"\n    Plot for Confusion Matrix:\n    Inputs:\n        cm: sklearn confusion_matrix function for y_true and y_pred as seen in https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.confusion_matrix.html\n        title: title of confusion matrix as a 'string', default=None\n        label: the unique label that represents classes for prediction can be done as sorted(dataframe['labels'].unique()).\n        color: confusion matrix color, default=None, set as a plt.cm.color, based on matplot lib color gradients\n    \"\"\"\n    \n    classes=sorted(label)\n    plt.imshow(cm, interpolation='nearest', cmap=color)\n    plt.title(title)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=0)\n    plt.yticks(tick_marks, classes)\n    plt.ylabel('Actual')\n    plt.xlabel('Predicted')\n    thresh = cm.mean()\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j]), \n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] < thresh else \"black\") ","1da291d7":"\nsvm_param = {\n    \"C\": [.01, .1, 1, 5, 10, 100], #Specific parameters to be tested at all combinations\n    \"gamma\": [0, .01, .1, 1, 5, 10, 100],\n    \"kernel\": [\"rbf\",\"linear\"\"poly\"],\n    \"random_state\": [1]}\n\n#Randomized Grid Search SVM Parameters\nsvm_dist = {\n    \"C\": np.arange(0.01,2, 0.01),   #By using np.arange it will select from randomized values\n    \"gamma\": np.arange(0,1, 0.01),\n    \"kernel\": [\"rbf\",\"linear\"\"poly\"],\n    \"random_state\": [1]}\n\n\"\"\"\nFollowing the code above, we can set the parameters for both grid search and randomized search. The grid search will evaluate all specified \nparameters while the randomized search will look at the parameters labeled in random order at the best training accuracy. The np.arange function\nallows for a multitude of points to be looked at between the set start and end values of 0.01 to 2. \"\"\"\n\n#Grid Search SVM\nsvm_grid, cfmatrix_grid= Searcher(SVC(), svm_param, \"grid\", X_train_scaled, y_train, X_test_scaled, y_test,label='SVC Grid')\n\nprint('_____'*20)#Spacer\n\n#Random Search SVM\nsvm_rand, cfmatrix_rand= Searcher(SVC(), svm_dist, \"random\", X_train_scaled, y_train, X_test_scaled, y_test,label='SVC Random')\n\n#Plotting the confusion matrices\nplt.subplots(1,2)\nplt.subplots_adjust(left=-0.5, bottom=None, right=None, top=None, wspace=0.5, hspace=None)\nplot_confusion_matrix(cfmatrix_rand, title='Random Search Confusion Matrix',label=new_cancer['Level'].unique(), color=plt.cm.Greens) #grid matrix function\nplt.subplot(121)\nplot_confusion_matrix(cfmatrix_grid, title='Grid Search Confusion Matrix', label=new_cancer['Level'].unique(), color=plt.cm.Blues) #randomized matrix function\n\nplt.savefig('confusion.png')","eeb9aea1":"## Search Function\n(Read red indents below def function for understanding of code)","e51d7d31":"Looking at the box-plot, age is not to important on level of cancer. Though there are a lot of variables to look at we can we can just find the most important ones by using the SelectKBest Algorithm with ANOVA F-ratio statistic.\n# Feature Selection\nThis method will generate the F-ratio scores of all features and we can determine which ones to use for machine learning.","0fe56547":"# Conclusion\nWe observed that:\n* Many features have some prominent separation alone for distincting the level of cancer.\n* Using KBestSelection we were able to use the most important features from the data set\n* The 13 selected features provided 100% accuracy when modeled with either Grid\/Randomized Searches on support vector machine classifier.\n\n<b>Suggestions?\n\nIf you stuck around to the end please leave a comment for feedback or upvote!<\/b>\n\nLike what I have done? Check out my other notebooks here: https:\/\/www.kaggle.com\/christopherwsmith\n\n# Selected Notebooks That Are Helpful!\n* https:\/\/www.kaggle.com\/christopherwsmith\/classify-that-penguin-100-accuracy\n\n* https:\/\/www.kaggle.com\/christopherwsmith\/tutorial-quick-custom-and-helpful-functions","fc06b0eb":"First lets import the xlsx file then convert it to csv. Then after lets take a quick look for null values and features.","0f59696d":"As we can see both searches provided 100% accuracy. Similarly, using the metric of an f1-score there was 100% accuracy showing that the model is very well built. ","6765e75f":"As we can see, 13 features were selected and we created a new data frame to account for them. We can now start the process of preprocessing for machine learning application.\n# Splitting the Data\nFirst, the data will be split so we can train a scaler model to apply to an unknwon (test) data set. We will save 25% of the data for testing.","33cba06c":"We will take all the features that scored more than 200 as they show the least redundancy.","1fb3bb99":"# General relevant modules","f2015562":"# Scaling Preprocessing\nNext, the data will be scaled by the standard scaler function in the sklearn package using the formula $z=\\frac{X_{o}-\\mu}{\\sigma}$. This can help reduce the effect of outliers when modeling later.","8810332a":"# Machine Learning with SVM and Grid\/Randomization Search\nFor modeling we will use the support vector machines classifiers (SVC). The SVC's can handle higher dimensional data and genearte hyperplanes for separation and score on a yes (1) no (1) basis. The rulings are decided for where a data point lands within a decision boundary. We can evalute multiple parameters at one using Grid or Randomization Search functions. Grid Search evalutes several input parameters at all combinations input while randomized search looks for the best. Cross-validation is the models self assemessment when trying to find the best parameters on the training data and can be done in \"n\" amount of replicates. We will set up two functions: one for the searches and the other for the confusion matrices.","7f5f2a89":"We have a higher representation of gender one than gender two. Lets start by looking to see if there is a correlation between some of the data. We can start with alcohol use and fatique and see if the level of cancer is related to them.","692efdba":"## Confusion Matrix Function \n(Read red indents below def function for understanding of code)","c899a625":"## Modeling","768ffa7c":"## Plotter Function\n(Read red indents below def function for understanding of code)","896b3bf3":"# <u>How to predict lung cancer! <\/u>\n## <br> Author: Christopher W. Smith <\/br>\n<br>Finished: 10\/12\/2020 <\/br>\n\n<br> <i>linkdin<\/i>: www.linkedin.com\/in\/christopher-w-smith022 <\/br>\n\n<br> The purpose of this notebook is to demonstrate a quick method of using feature selection to skip over most EDA and just get the important features we need for machine learning. an exploratory data analysis (EDA) . Data was selected by KBest ANOVA F-ratio. Support vector machines (SVM) was used with hyper parameterization by grid and randomized searchers for the most accurately built models. <\/br>\n\nKaggle link: https:\/\/www.kaggle.com\/rishidamarla\/cancer-patients-data\n\n![image.png](attachment:image.png)","3058878f":"We can see that all of the data is numerical with age as int, level as categories and the rest are ordinal. Luckily, we do not need to worry about data cleaning here as there are no nulls. Lets directly jump into EDA.\n# EDA\nFirst lets set up a consistent plotting function called plotter.","57f058ee":"There is clearly two different clusters for low and high levels by alcohol use and fatique while medium is a combination of both. Is there a difference by age and level of cancer?","e5d09189":"As we can see the data is fairly balanced. Why don't we break it down further by gender."}}