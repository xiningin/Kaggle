{"cell_type":{"2544d780":"code","50b2d8a1":"code","567db650":"code","c3e88aa5":"code","4070caa2":"code","29f95ab4":"code","cfaf3ad4":"code","24178f36":"code","5f2e67ef":"code","3d88174e":"code","dcb66937":"code","452baa12":"code","ee1d6e22":"code","c4d5bc9b":"code","80a572fa":"code","e8d10a8f":"code","520e0ba9":"code","b4412dbe":"code","a37dafde":"code","b3cad65d":"code","018d2e90":"code","9b288370":"code","95bdfc07":"code","ebda2247":"code","d60b01c7":"code","e5eb6868":"markdown","092bba65":"markdown","4487f499":"markdown","514fb0c7":"markdown","5899df5c":"markdown","a05160e5":"markdown","72600df4":"markdown","4e153a7c":"markdown","dd489345":"markdown","187e555c":"markdown","db521970":"markdown","ecd38a2f":"markdown","2bce7ee0":"markdown","cdc8db90":"markdown","a7c79efa":"markdown","b6b1131f":"markdown","20d66f6c":"markdown"},"source":{"2544d780":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","50b2d8a1":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndf = pd.read_csv(\"\/kaggle\/input\/early-stage-diabetes-risk-prediction-dataset\/diabetes_data_upload.csv\")\ndf.head()","567db650":"# Check the data types of the columns and if there is any null value.\ndf.info()","c3e88aa5":"from sklearn import preprocessing\nlabel_encoder = preprocessing.LabelEncoder()\nfor column  in df.columns[1:]:\n    df[column]= label_encoder.fit_transform(df[column])","4070caa2":"# descriptive statistics of the Age column\ndf.Age.describe()","29f95ab4":"# Distribution of patients age. \nsns.displot(x='Age', kind ='hist', data= df, bins = 10, kde = True);","cfaf3ad4":"# taking the abosulute values of the correlating features to find out top 5 features\nnp.abs(df.iloc[:,:-1].corrwith(df['class'])).sort_values(ascending = False)","24178f36":"# importing necessary libraries\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_predict, cross_val_score\nfrom sklearn.model_selection import KFold, ShuffleSplit\nfrom sklearn.metrics import classification_report, confusion_matrix","5f2e67ef":"predictors = df.drop(['class'], axis= 1)\ntarget = df['class']\nx_train, x_val, y_train, y_val = train_test_split(predictors, target, test_size = 0.10, random_state = 0, stratify=target)\nprint(x_train.shape, y_train.shape)\nprint(x_val.shape, y_val.shape)","3d88174e":"from sklearn.dummy import DummyRegressor\n\ndummy = DummyRegressor(strategy='mean')\ndummy.fit(x_train, y_train)\ndummy.score(x_val, y_val)","dcb66937":"y_train.value_counts()","452baa12":"y_train.mean()","ee1d6e22":"classifiers_description = {\"model\":[],\"precision\":[], \"recall\":[],\"f1-score\":[], \"accuracy\":[], \"standard_deviation\" :[]}","c4d5bc9b":"def model_accuracy(classifier = None, predictors=None, target= None, n_splits = 10):\n    global classifiers_description\n    # helper function for Model Evaluation\n    \n    kf = KFold(n_splits=10, shuffle=True, random_state=1)\n    \n    y_pred = cross_val_predict(classifier, predictors, target, cv=kf)\n    scores = cross_val_score(classifier, predictors, target, cv=kf)\n    \n    # plotting confusion matrix\n\n    cf_matrix = confusion_matrix(target, y_pred)\n    sns.set_style('ticks')\n    fig, ax = plt.subplots()\n    sns.heatmap(cf_matrix,annot=True, ax=ax, fmt='g', cmap='Blues')\n\n    #making classifier description report\n    report = classification_report(target, y_pred, output_dict=True)\n    classifier_name = type(classifier).__name__\n    if classifier_name not in classifiers_description[\"model\"]:\n        classifiers_description[\"model\"].append(classifier_name)\n        classifiers_description[\"precision\"].append(report['weighted avg'][\"precision\"])\n        classifiers_description[\"recall\"].append(report['weighted avg'][\"recall\"])\n        classifiers_description[\"f1-score\"].append(report['weighted avg'][\"f1-score\"])\n        classifiers_description[\"accuracy\"].append(scores.mean())\n        classifiers_description[\"standard_deviation\"].append(scores.std())   \n\n    \n    print(classification_report(target, y_pred))\n\n    return (scores.mean(), scores.std())","80a572fa":"from sklearn.naive_bayes import GaussianNB\nclf_GNB = GaussianNB()\nmodel_accuracy(classifier=clf_GNB, predictors=x_train, target=y_train)","e8d10a8f":"from sklearn.linear_model import LogisticRegression\nclf_LR= LogisticRegression(max_iter=1000, )\nmodel_accuracy(classifier=clf_LR, predictors=x_train, target=y_train)","520e0ba9":"from sklearn.tree import DecisionTreeClassifier\nclf_DT = DecisionTreeClassifier()\nmodel_accuracy(classifier=clf_DT, predictors=x_train, target=y_train)","b4412dbe":"from sklearn.ensemble import RandomForestClassifier\nclf_RF = RandomForestClassifier()\nmodel_accuracy(classifier=clf_RF, predictors=x_train, target=y_train)","a37dafde":"# plotting feature importances\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# fitting the model\nmodel_RF = RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42)\nmodel_RF.fit(x_train, y_train)\n\nfeatures = df.drop('class', axis=1).columns\nimportances = model_RF.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure()\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","b3cad65d":"from sklearn.ensemble import GradientBoostingClassifier\nclf_GBC = GradientBoostingClassifier()\nmodel_accuracy(classifier=clf_GBC, predictors=x_train, target=y_train)","018d2e90":"from sklearn.neural_network import MLPClassifier\nMLP_clf = MLPClassifier(random_state=1, max_iter=10000)\nmodel_accuracy(classifier=MLP_clf, predictors=x_train, target=y_train)","9b288370":"classifier_df = pd.DataFrame.from_dict(classifiers_description)\nclassifier_df.sort_values(by=[\"f1-score\",\"standard_deviation\"], ascending=False)","95bdfc07":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nclf1 = RandomForestClassifier(random_state=1)\nclf2 = GradientBoostingClassifier(random_state=1)\nclf3 =  DecisionTreeClassifier(random_state=1)\n\n\nparams={\"rf__max_depth\":[8],\n        \"rf__criterion\":[\"entropy\"],\n        \"rf__n_estimators\":[1000],\n        \"gb__loss\":[\"deviance\"],\n        \"gb__n_estimators\":[1000],\n        \"gb__criterion\":[\"friedman_mse\"],\n        \"gb__max_depth\":[2],\n        \"gb__max_features\":[\"auto\"],\n        \"dt__max_features\":[\"auto\"],\n        \"dt__criterion\":[\"gini\"],\n        \"dt__max_depth\":[16]\n        }\n\neclf = VotingClassifier(estimators=[(\"rf\", clf1), (\"gb\", clf2), (\"dt\", clf3)],\n                       voting= 'soft', weights = [3,1,1])\ngrid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\ngrid.fit(x_train, y_train)\ngrid_best = grid.best_estimator_\nmodel_accuracy(classifier=grid_best, predictors=x_train, target=y_train)","ebda2247":"y_pred = grid_best.predict(x_val)\nprint(classification_report(y_val, y_pred))","d60b01c7":"classifier_df = pd.DataFrame.from_dict(classifiers_description)\nclassifier_df.sort_values(by=[\"f1-score\",\"standard_deviation\"], ascending=False)","e5eb6868":"## Predictive Analytics\n\nIn predictive analytics, we need to come to an strategy for making a robust classifier to classify the likelyhood of a person having early stage diabetics by using new features. In this project, our goal should be minimizing false positives even if it reduces over all accuracy.\n\nIn this step we need to:\n* split the dataset in training and testing dataset\n* find out the classification accuracy of a naive classifier\n* compare differnt classifier usign F1 score, precision, recall, accuracy and confusion matrix.\n* create an ensemble classifier using the top three classifiers ","092bba65":"So, we can see that, we need to develop a classifier for more than 62% accuracy and with lowest false nevgative rate.","4487f499":"## Load the data set into memory\n\nThis is a very small dataset consisting only 520 instances and cosumes approximately 70 KB so we can load all of it into memory, but for huge datasets we need to process them chunk by chunk.","514fb0c7":"## Developing a Voting Classifier by ensambling top three models\n\nFrom the above dataframe and confusion matrixes we can see that RandomForest, GradientBoosting, and Decision Tree classifiers gives better F1-score and lower False Negatives. So, we can ensemble them for developing a robust classifier usign soft voting. Ensemble methods are techniques that create multiple models and then combine them to produce improved results","5899df5c":"From the confusion matrix, we can see that we have reduced the type II error, by using weighted voting classifier. Now let's check how it performs on the validation dataset.**","a05160e5":"## Data Pre processing and Feature Important analysis\n\nthe dataset is already pre-processed and there is not much to do. But we can check what are the most important features of this data set by: Finding out the pearson correlation between features and Class Feature importance techniues to reduce computational cost.\n","72600df4":"## Conclusion\n\nWe can see that we can acheive a significant accuracy using Random Forest Classifier but with the help of other two classifiers we are able to reduce type II error. Our main gole was to reduce Type II error and we can see that with a tradeoff of accuracy we are able to acheive it.","4e153a7c":"## Comparing classifiers for developing voting classifier.","dd489345":"## Exploratory Data Analysis\n\nIn this section, we will try to infer about the trends in the dataset using data visualization and statistics. \n\nWe can see that every columns without age consists of boolean values. So at first we need to encode them by using *LabelEncoder*. As these are boolean categorical values we cannot use [*pandas.DataFrame.describe*](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.describe.html) to infer about the descriptive statistics including those that summarize the central tendency, dispersion and shape of a dataset\u2019s distribution, excluding NaN values.\n\nRather we can analyze the frequency of attributes over different age groups and gender.","187e555c":"# Predicting early stage diabetics\n\nWe need to go through the following steps for predicting early stage diabetics:\n1. Data wrangling\n2. Exploratory Data Analysis\n3. Data Pre processing and Feature Important analysis\n4. Evaluating machine learning models\n5. Creating ensamble model for robustness\n","db521970":"## Creating a Baseline Regression Model\n\nDummyRegressor allows us to create a very simple model that we can use as a baseline to compare against our actual model. This can often be useful to simulate a \u201cnaive\u201d existing prediction process in a product or system.\n\nscore returns the coefficient of determination $(R-squared, R^2)$. The closer $R^2$ is to 1, the more of the variance in the target vector that is explained by\nthe features.","ecd38a2f":"## Developing classifiers\n\nNow, to develop the best classifier we need to analyze several classfiers and ensamble top three ones with soft voting for getting a robust descision and lower bias.","2bce7ee0":"### Feture importance\n\nwhich features are important is analyzed using RandomForestClassifier from sklearn.ensemble method. This is done in the Predictive analytics section bellow. From that we can see that *Age* is one of the most important factors but it is not quite correlated with the target variable *Class*.\n\nSo, we should not rely on only one method in determing important features for predicting the target.","cdc8db90":"## Data Wrangling\n\nData wrangling is the process of gathering, selecting, and transforming data to answer an analytical question. Also known as data cleaning or munging, legend has it that this wrangling costs analytics professionals as much as 80% of their time, leaving only 20% for exploration and modeling.\n\nBut the dataset is pre cleaned and there is no missing data. As a result, it is quite an easy step here and there is not much to do.","a7c79efa":"### Correlation analysis\n\nCorrelation is not casuation but we can infer which features attributes most in defining the class. So that, we can also reduce relatively insignificant features for reducing load in machine learning models. we have taken pearson correlation coefficient ","b6b1131f":"**Null Accuracy:** accuracy that could be achieved by always predicting the most frequent class. From this null accuracy, we can find out if a classifier only predicts one class (here positive class) what will be the accuracy. So, we can choose this value as our base line accuracy.","20d66f6c":"We can also anlyze important features for this classification problem by using Random Forrest classifier."}}