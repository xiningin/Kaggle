{"cell_type":{"250563a0":"code","fafae44d":"code","27a0d87a":"code","62782f79":"code","41c441fe":"code","99873f2b":"code","5d647879":"code","a613b3fb":"code","77e74e60":"code","284f6634":"code","bea5ef32":"code","8063c334":"code","a7cfec11":"code","ab2a0fbd":"code","2725b9d9":"code","7a4e3183":"code","6dd4615c":"code","f549b0b7":"code","dc38f96a":"code","31982208":"code","0b0f1ac6":"code","1e16c02b":"code","2ed9ef2d":"code","92365cdf":"code","644a3868":"code","3a6ed0df":"code","92a81192":"code","0cdc85e2":"code","5ca924c6":"code","749151e4":"code","af08fd5e":"code","8dc883ad":"code","78bb7d8e":"code","887b10be":"code","dd2888bc":"code","b6a593cb":"code","bc0193b3":"code","e387c828":"code","579b96d2":"code","986d21a5":"code","db049d28":"code","e78e9e31":"code","62aa8f74":"code","547b8af7":"code","5198115e":"code","1cdd6bc9":"markdown","c266353e":"markdown","542a23e1":"markdown","525c25a2":"markdown","015c0c78":"markdown","c0c139df":"markdown","c4d641a2":"markdown","8ea3e193":"markdown"},"source":{"250563a0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas_profiling\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","fafae44d":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","27a0d87a":"df1 = pd.read_csv('..\/input\/spam-or-ham\/spam.csv')\ndf1.head()","62782f79":"df1 = df1.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\ndf1 = df1.rename(columns={\"v1\":\"Flag\", \"v2\":\"SMS_Original\"})\ndf1.head()","41c441fe":"df1.columns","99873f2b":"display(df1.head())","5d647879":"display(df1.tail())","a613b3fb":"df1.profile_report()","77e74e60":"df1 = pd.get_dummies(df1, columns=['Flag'], drop_first=True)\ndisplay(df1.head())","284f6634":"display(df1.tail())","bea5ef32":"df2 = df1\ndf2 = df2[['Flag_spam','SMS_Original']]\ndisplay(df2.head())","8063c334":"display(df2.tail())","a7cfec11":"!pip install BeautifulSoup4\nimport re\nfrom bs4 import BeautifulSoup","ab2a0fbd":"# Remove HTTP tags\n%time df2['SMS_Processed'] = df2['SMS_Original'].map(lambda x : ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)\",\" \",x).split()))\ndf2.head()","2725b9d9":"#Lower Case\n%time df2['SMS_Processed'] = df2['SMS_Processed'].map(lambda x: x.lower())\ndf2.head()","7a4e3183":"#Remove punctuations\n%time df2['SMS_Processed'] = df2['SMS_Processed'].map(lambda x: re.sub(r'[^\\w\\s]', '', x))\ndf2.head()","6dd4615c":"#Remove unicodes\n%time df2['SMS_Processed'] = df2['SMS_Processed'].map(lambda x : re.sub(r'[^\\x00-\\x7F]+',' ', x))\ndf2.head()","f549b0b7":"from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords","dc38f96a":"# Remove stopwords\nstop_words = stopwords.words('english')\n%time df2['SMS_Processed'] = df2['SMS_Processed'].map(lambda x : ' '.join([w for w in x.split() if w not in stop_words]))\ndf2.head()","31982208":"# Lemmatize the text\nlemmer = WordNetLemmatizer()\n\n%time df2['SMS_Processed'] = df2['SMS_Processed'].map(lambda x : ' '.join([lemmer.lemmatize(w) for w in x.split() if w not in stop_words]))\ndf2.head()","0b0f1ac6":"#Removing Stop words again after Lemmatize\n%time df2['SMS_Processed'] = df2['SMS_Processed'].map(lambda x : ' '.join([w for w in x.split() if w not in stop_words]))\ndisplay(df2.head())","1e16c02b":"display(df2.tail())","2ed9ef2d":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer","92365cdf":"#funtion to get 'top N' or 'bottom N' words\n\ndef get_n_words(corpus, direction, n):\n    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    if direction == \"top\":\n        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    else:\n        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=False)\n    return words_freq[:n]","644a3868":"#10 most common and 10 most rare words\ncommon_words = get_n_words(df2['SMS_Processed'], \"top\", 15)\nrare_words = get_n_words(df2['SMS_Processed'], \"bottom\", 15)","3a6ed0df":"common_words = dict(common_words)\nnames = list(common_words.keys())\nvalues = list(common_words.values())\nplt.subplots(figsize = (15,10))\nbars = plt.bar(range(len(common_words)),values,tick_label=names)\nplt.title('15 most common words:')\nfor bar in bars:\n    yval = bar.get_height()\n    plt.text(bar.get_x(), yval + .01, yval)\nplt.grid()","92a81192":"rare_words = dict(rare_words)\nnames = list(rare_words.keys())\nvalues = list(rare_words.values())\nplt.subplots(figsize = (15,10))\nbars = plt.bar(range(len(rare_words)),values,tick_label=names)\nplt.xticks(rotation=90)\nplt.title('15 most rare words:')\nfor bar in bars:\n    yval = bar.get_height()\n    plt.text(bar.get_x(), yval + .001, yval)\nplt.grid()","0cdc85e2":"# BOW-TF Embedding\n\nno_features = 800\ntf_vectorizer = CountVectorizer(min_df=.015, max_df=.8, max_features=no_features, ngram_range=[1, 3])\n\n%time tpl_tf = tf_vectorizer.fit_transform(df2['SMS_Processed'])\nprint(\"Bow-TF :\", tpl_tf.shape)\ndf_tf = pd.DataFrame(tpl_tf.toarray(), columns=tf_vectorizer.get_feature_names())\ndisplay(df_tf.head())","5ca924c6":"#Preparing processed and BoW-TF embedded data for Classification\ndf_tf_m = pd.concat([df2, df_tf], axis = 1)\ndf_tf_m.drop(columns=['SMS_Original', 'SMS_Processed'], inplace = True)\nprint(df_tf_m.shape)\ndisplay(df_tf_m.head())","749151e4":"display(df_tf_m.tail())","af08fd5e":"# BoW-TF:IDF Embedding\ntfidf_vectorizer = TfidfVectorizer(min_df=.02, max_df=.7, ngram_range=[1,3])\n\n%time tpl_tfidf = tfidf_vectorizer.fit_transform(df2['SMS_Processed'])\nprint(\"Bow-TF:IDF :\", tpl_tfidf.shape)\ndf_tfidf = pd.DataFrame(tpl_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names(), index=df2.index)\ndisplay(df_tfidf.head())","8dc883ad":"#Preparing processed and BoW-TF:IDF embedded data for Classification\ndf_tfidf_m = pd.concat([df2, df_tfidf], axis = 1)\ndf_tfidf_m.drop(columns=['SMS_Original', 'SMS_Processed'], inplace = True)\nprint(df_tfidf_m.shape)","78bb7d8e":"display(df_tfidf_m.head())","887b10be":"display(df_tfidf_m.tail())","dd2888bc":"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import StratifiedKFold, cross_validate, train_test_split, cross_val_score, KFold\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix, precision_score, recall_score,  accuracy_score, precision_recall_curve","b6a593cb":"#function to prepare Confusion Matrix, RoC-AUC curve, and relvant statistics\n\ndef clf_report(Y_test, Y_pred, probs):\n    print(\"\\n\", \"Confusion Matrix\")\n    cm = confusion_matrix(Y_test, Y_pred)\n    #print(\"\\n\", cm, \"\\n\")\n    sns.heatmap(cm, square=True, annot=True, cbar=False, fmt = 'g', cmap='RdBu',\n                xticklabels=['ham', 'spam'], yticklabels=['ham', 'spam'])\n    plt.xlabel('true label')\n    plt.ylabel('predicted label')\n    plt.show()\n    print(\"\\n\", \"Classification Report\", \"\\n\")\n    print(classification_report(Y_test, Y_pred))\n    print(\"Overall Accuracy : \", round(accuracy_score(Y_test, Y_pred) * 100, 2))\n    print(\"Precision Score : \", round(precision_score(Y_test, Y_pred, average='binary') * 100, 2))\n    print(\"Recall Score : \", round(recall_score(Y_test, Y_pred, average='binary') * 100, 2))\n    preds = probs[:,1] # this is the probability for 1, column 0 has probability for 0. Prob(0) + Prob(1) = 1\n    fpr, tpr, threshold = roc_curve(Y_test, preds)\n    roc_auc = auc(fpr, tpr)\n    print(\"AUC : \", round(roc_auc * 100, 2), \"\\n\")\n    #display(probs)\n    #print(\"Cutoff Probability : \", preds)\n    plt.figure()\n    plt.plot(fpr, tpr, label='Best Model on Test Data (area = %0.2f)' % roc_auc)\n    plt.plot([0.0, 1.0], [0, 1],'r--')\n    plt.xlim([-0.1, 1.1])\n    plt.ylim([-0.1, 1.1])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('RoC-AUC on Test Data')\n    plt.legend(loc=\"lower right\")\n    plt.savefig('Log_ROC')\n    plt.show()\n    print(\"--------------------------------------------------------------------------\")","bc0193b3":"#function to prepare different Classification models\n\ndef model_dvt(df):\n    Y = df['Flag_spam']\n    X = df.drop('Flag_spam', axis = 1)\n    \n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = 0.85, random_state = 21)\n    print(\"Train Data Dimensions : \", X_train.shape)\n    print(\"Test Data Dimensions : \", X_test.shape)\n    \n    print(\"\\n\", 'Random Forest Classifier')\n    clf = RandomForestClassifier(n_estimators=500, max_depth=10, random_state=21)\n    %time clf.fit(X_train, Y_train)\n    Y_pred = clf.predict(X_test)\n    probs = clf.predict_proba(X_test)\n    clf_report(Y_test, Y_pred, probs)\n    \n    print(\"\\n\", 'AdaBoost Classifier')\n    clf = AdaBoostClassifier(n_estimators=200,random_state=21)\n    %time clf.fit(X_train, Y_train)\n    Y_pred = clf.predict(X_test)\n    probs = clf.predict_proba(X_test)\n    clf_report(Y_test, Y_pred, probs)\n    \n    print(\"\\n\", 'Grdient Boosting Classifier')\n    clf = GradientBoostingClassifier(n_estimators=100, max_depth=1, random_state=21, learning_rate=1.0)\n    %time clf.fit(X_train, Y_train)\n    Y_pred = clf.predict(X_test)\n    probs = clf.predict_proba(X_test)\n    clf_report(Y_test, Y_pred, probs)\n    \n    print(\"\\n\", 'Naive Bayes Classifier')\n    clf = MultinomialNB(alpha = 1.0)\n    %time clf.fit(X_train, Y_train)\n    Y_pred = clf.predict(X_test)\n    probs = clf.predict_proba(X_test)\n    clf_report(Y_test, Y_pred, probs)","e387c828":"print('Models on Term Frequency - Bag of Words data')\n%time model_dvt(df_tf_m)","579b96d2":"print('Models on Term Frequency - Bag of Words: Inverse Document Frequency data')\n%time model_dvt(df_tfidf_m)","986d21a5":"from sklearn.model_selection import GridSearchCV","db049d28":"Y = df_tf_m['Flag_spam']\nX = df_tf_m.drop('Flag_spam', axis = 1)\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = 0.85, random_state = 21)\nprint(\"Train Data Dimensions : \", X_train.shape)\nprint(\"Test Data Dimensions : \", X_test.shape)","e78e9e31":"#Creating a grid of hyperparameters\ngrid_params = {'n_estimators' : [100,200,300],\n               'learning_rate' : [1.0, 0.1, 0.05]}\n\nABC = AdaBoostClassifier()\n#Building a 10 fold CV GridSearchCV object\ngrid_object = GridSearchCV(estimator = ABC, param_grid = grid_params, scoring = 'roc_auc', cv = 10, n_jobs = -1)\n\n#Fitting the grid to the training data\n%time grid_object.fit(X_train, Y_train)","62aa8f74":"#Extracting the best parameters and score\nprint(\"Best Parameters : \", grid_object.best_params_)\nprint(\"Best_ROC-AUC : \", round(grid_object.best_score_ * 100, 2))\nprint(\"Best model : \", grid_object.best_estimator_)\n\n#Applying the tuned parameters back to the model\nY_pred = grid_object.best_estimator_.predict(X_test)\nprobs = grid_object.best_estimator_.predict_proba(X_test)\nclf_report(Y_test, Y_pred, probs)\n\nkfold = KFold(n_splits=10, random_state=25, shuffle=True)\n%time results = cross_val_score(grid_object.best_estimator_, X_test, Y_test, cv=kfold)\nresults = results * 100\nresults = np.round(results,2)\nprint(\"Cross Validation Accuracy : \", round(results.mean(), 2))\nprint(\"Cross Validation Accuracy in every fold : \", results)","547b8af7":"grid_params = {'n_estimators' : [100,200,300,400,500],\n               'max_depth' : [10, 7, 5, 3],\n               'criterion' : ['entropy', 'gini']}\n\nRFC = RandomForestClassifier()\ngrid_object = GridSearchCV(estimator = RFC, param_grid = grid_params, scoring = 'roc_auc', cv = 10, n_jobs = -1)\n\n%time grid_object.fit(X_train, Y_train)","5198115e":"# print(\"Best Parameters : \", grid_object.best_params_)\nprint(\"Best_ROC-AUC : \", round(grid_object.best_score_ * 100, 2))\nprint(\"Best model : \", grid_object.best_estimator_)\n\nY_pred = grid_object.best_estimator_.predict(X_test)\nprobs = grid_object.best_estimator_.predict_proba(X_test)\nclf_report(Y_test, Y_pred, probs)\n\nkfold = KFold(n_splits=10, random_state=25, shuffle=True)\n%time results = cross_val_score(grid_object.best_estimator_, X_test, Y_test, cv=kfold)\nresults = results * 100\nresults = np.round(results,2)\nprint(\"Cross Validation Accuracy : \", round(results.mean(), 2))\nprint(\"Cross Validation Accuracy in every fold : \", results)","1cdd6bc9":"# Pre-processing on the Text data\n","c266353e":"# Before we move further, lets do some exploratory analysis by finding the most common and rare words","542a23e1":"Grid-Search hyperparameter tuning on Random Forest Classifier","525c25a2":"Grid-Search hyperparameter tuning on AdaBoost Classifier","015c0c78":"# Data setup","c0c139df":"# Hyper-parameter tuning models that used TF-BoW embedding data","c4d641a2":"# Model Building","8ea3e193":"# Embedding on the processed text data"}}