{"cell_type":{"9f0ef162":"code","00579e37":"code","64e197ec":"code","0fd1fdc0":"code","2e8d62bc":"code","e39a3aca":"code","b803ee5c":"code","14d8bd26":"code","65f516ee":"code","a1cf30db":"code","5d5c19e4":"code","88ba156b":"code","6d940307":"code","8fe5d6ed":"code","bf329179":"code","6b1bdf13":"code","3da83430":"code","a3946c11":"code","eaad6e44":"code","0e8cb450":"code","3fc6fa80":"code","5951793c":"code","be7f4d84":"code","9c395904":"code","8e5fb6a7":"code","b09d04cc":"code","6e37dba1":"code","2a11ed32":"code","ab1c7c38":"code","8d56d392":"code","9455142a":"code","eb654a02":"code","b6724758":"code","bc4581ec":"code","e4b3b734":"code","49294daa":"code","bb17970e":"code","a1fe11a9":"code","7d407bf7":"code","b99b5e32":"code","9302eb9e":"code","80fb14db":"code","2d29fce9":"code","c263c290":"code","af71ae5f":"code","213bdf2d":"code","73e88d01":"code","5bd8246d":"code","e6d394be":"code","1df30332":"code","789bd0d1":"code","ab02c0d2":"markdown","8a5edc5f":"markdown","60dbdd74":"markdown","632ef500":"markdown"},"source":{"9f0ef162":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom ast import literal_eval\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","00579e37":"md = pd.read_csv('..\/input\/movies_metadata.csv')\nmd.head()","64e197ec":"md['genres'] = md['genres'].fillna('[]').apply(literal_eval).apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])\nmd['year'] = pd.to_datetime(md['release_date'], errors='coerce').apply(lambda x: str(x).split('-')[0] if x != np.nan else np.nan)","0fd1fdc0":"# We are going to use much more suggestive metadata than Overview and Tagline.\n\nlinks_small = pd.read_csv('..\/input\/links_small.csv')\nlinks_small = links_small[links_small['tmdbId'].notnull()]['tmdbId'].astype('int')","2e8d62bc":"links_small.head()","e39a3aca":"# Define a convert_int() function \ndef convert_int(x):\n    try:\n        return int(x)\n    except:\n        return np.nan","b803ee5c":"# Convert the id's in the id column to interger\nmd['id'] = md['id'].apply(convert_int)\nmd[md['id'].isnull()]","14d8bd26":"# Removing the rows that have null values in the id column\nmd = md.drop([19730, 29503, 35587])","65f516ee":"# Declaring the 'id' column as integer\nmd['id'] = md['id'].astype('int')","a1cf30db":"# We have 9099 movies avaiable in our small movies \n# metadata dataset which is 5 times smaller than our \n# original dataset of 45000 movies.\nsmd = md[md['id'].isin(links_small)]\nsmd.shape","5d5c19e4":"# filling missing values in tagline and description columns\nsmd['tagline'] = smd['tagline'].fillna('')  # fill the empty data points in tagline column\nsmd['description'] = smd['overview'] + smd['tagline']  # Combining columns overview and tagline\nsmd['description'] = smd['description'].fillna('')","88ba156b":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel, cosine_similarity\nfrom surprise import Reader, Dataset, SVD, evaluate","6d940307":"# Converts a collection of raw documents to a matrix of TF-IDF features\ntf = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), min_df = 0, stop_words='english')\ntfidf_matrix = tf.fit_transform(smd['description'])","8fe5d6ed":"tfidf_matrix.shape","bf329179":"# I will be using the Cosine Similarity to calculate a numeric quantity \n# that denotes the similarity between two movies. \n# Since we have used the TF-IDF Vectorizer, calculating the Dot Product \n# will directly give us the Cosine Similarity Score. Therefore, we will use \n# sklearn's linear_kernel instead of cosine_similarities since it is much faster.\n\ncosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)","6b1bdf13":"cosine_sim[0]","3da83430":"# We now have a pairwise cosine similarity matrix for all the movies in our dataset. \n# The next step is to write a function that returns the 30 most similar movies based \n# on the cosine similarity score.\n\nsmd = smd.reset_index()\ntitles = smd['title']  # Defining a new variable title\nindices = pd.Series(smd.index, index = smd['title'])  # Defining a new dataframe indices","a3946c11":"smd.head()","eaad6e44":"# Defining a function that returns 30 most similar movied bases on the cosine \n# similarity score\ndef get_recommendations(title):\n    idx = indices[title]  # Defining a variable with indices\n    sim_scores = list(enumerate(cosine_sim[idx]))\n    sim_scores = sorted(sim_scores, key = lambda x: x[1], reverse = True)\n    sim_scores = sim_scores[1: 31]  # Taking the 30 most similar movies\n    movie_indices = [i[0] for i in sim_scores]\n    return titles.iloc[movie_indices]  # returns the title based on movie indices","0e8cb450":"# Getting the top recommendations for a few movies \nget_recommendations('Transformers').head(10)","3fc6fa80":"# Our system is able to identify it as a Batman film and subsequently recommend \n# other Batman films as its top recommendations.\nget_recommendations('The Dark Knight').head(10)","5951793c":"credits = pd.read_csv('..\/input\/credits.csv')\nkeywords = pd.read_csv('..\/input\/keywords.csv')","be7f4d84":"# Converting the keywords's id column to integer\nkeywords['id'] = keywords['id'].astype('int')\ncredits['id'] = credits['id'].astype('int')\nmd['id'] = md['id'].astype('int')","9c395904":"md.shape","8e5fb6a7":"# Merging teh credits and keywords DataFrame with md DataFrame\nmd = md.merge(credits, on = 'id')\nmd = md.merge(keywords, on = 'id')","b09d04cc":"# Creating a small movies DataFrame from the links_small\nsmd = md[md['id'].isin(links_small)]\nsmd.shape","6e37dba1":"# We now have our cast, crew, genres and credits, all in one dataframe.\nfrom ast import literal_eval\n\nsmd['cast'] = smd['cast'].apply(literal_eval)\nsmd['crew'] = smd['crew'].apply(literal_eval)\nsmd['keywords'] = smd['keywords'].apply(literal_eval)\nsmd['cast_size'] = smd['cast'].apply(lambda x: len(x))  # Storing the cast_size\nsmd['crew_size'] = smd['crew'].apply(lambda x: len(x))  # Storing the crew_size","2a11ed32":"# Defining a function that gets the director's name \ndef get_director(x):\n    for i in x:\n        if i['job'] == 'Director':\n            return i['name']\n    return np.nan","ab1c7c38":"# Applying the get_director function to the crew column to create smd['director'] column\nsmd['director'] = smd['crew'].apply(get_director)","8d56d392":"# Arbitrarily we will choose the top 3 actors that appear in the credits list. \nsmd['cast'] = smd['cast'].apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])\nsmd['cast'] = smd['cast'].apply(lambda x: x[:3] if len(x) >= 3 else x)  # Taking the top 3 actor from cast","9455142a":"smd['keywords'] = smd['keywords'].apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])","eb654a02":"# Strip Spaces and Convert to Lowercase from all our features. \nsmd['cast'] = smd['cast'].apply(lambda x: [str.lower(i.replace(\" \", \"\")) for i in x])","b6724758":"# Mention Director 3 times to give it more weight relative to the entire cast.\nsmd['director'] = smd['director'].astype('str').apply(lambda x: str.lower(x.replace(\" \", \"\")))\nsmd['director'] = smd['director'].apply(lambda x: [x, x, x])","bc4581ec":"# Calculating the frequent counts of every keyword that appears in the dataset\ns = smd.apply(lambda x: pd.Series(x['keywords']), axis = 1).stack().reset_index(level = 1, drop = True)\ns.name = 'keywords'","e4b3b734":"s = s.value_counts()\ns[: 5]","49294daa":"# Removing keywords that occur only once\ns = s[s > 1]","bb17970e":"# We will convert every word to its stem so that words such as Dogs and Dog \n# are considered the same.\nfrom nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer('english')\nstemmer.stem('dogs')","a1fe11a9":"# Defining function filter_keywords() to create a list of words\ndef filter_keywords(x):\n    words = []\n    for i in x:\n        if i in s:\n            words.append(i)\n    return words","7d407bf7":"# Converting the entries in keywords into stemmed words\nsmd['keywords'] = smd['keywords'].apply(filter_keywords)  #Applying the filter_keywords() function to keywords column\nsmd['keywords'] = smd['keywords'].apply(lambda x: [stemmer.stem(i) for i in x])  # Converting the entries in keywords column into stemmed words\nsmd['keywords'] = smd['keywords'].apply(lambda x: [str.lower(i.replace(\" \", \"\")) for i in x])  # Removing spaces and converting the entries into lower case","b99b5e32":"smd.head()","9302eb9e":"# Combining 'keywords', 'cast', 'director' and genres columns in 'soup'\nsmd['soup'] = smd['keywords'] + smd['cast'] + smd['director'] + smd['genres']\nsmd['soup'] = smd['soup'].apply(lambda x: ' '.join(x))","80fb14db":"# Convert a collection of text documents to a matrix of token counts\ncount = CountVectorizer(analyzer = 'word', ngram_range = (1, 2), min_df = 0, stop_words = 'english')\ncount_matrix = count.fit_transform(smd['soup'])","2d29fce9":"# Compute cosine similarity between samples in X and Y.\ncosine_sim = cosine_similarity(count_matrix, count_matrix)","c263c290":"smd = smd.reset_index()  # Reseting the indices within smd\ntitles = smd['title']  # Storing smd['title'] in titles variable\nindices = pd.Series(smd.index, index = smd['title'])    # Creating a DataFrame of indices","af71ae5f":"get_recommendations('The Dark Knight').head(10)","213bdf2d":"get_recommendations('Jurassic Park').head(10)","73e88d01":"def improved_recommendations(title):\n    idx = indices[title]\n    sim_scores = list(enumerate(cosine_sim[idx]))\n    sim_scores = sorted(sim_scores, key = lambda x: x[1], reverse = True)\n    sim_score = sim_scores[1:26]\n    movie_indices = [i[0] for i in sim_scores]\n    \n    movies = smd.iloc[movie_indices][['title', 'vote_count', 'vote_average', 'year']]\n    vote_counts = movies[movies['vote_count'].notnull()]['vote_count'].astype('int')\n    vote_averages = movies[movies['vote_average'].notnull()]['vote_average'].astype('int')\n    C = vote_averages.mean()\n    m = vote_counts.quantile(0.60)\n    qualified = movies[(movies['vote_count'] >= m) & (movies['vote_count'].notnull()) & (movies['vote_average'].notnull())]\n    qualified = movies[(movies['vote_count'] >= m) & (movies['vote_count'].notnull()) & (movies['vote_average'].notnull())]\n    qualified['vote_count'] = qualified['vote_count'].astype('int')\n    qualified['vote_average'] = qualified['vote_average'].astype('int')\n    qualified['wr'] = qualified.apply(weighted_rating, axis=1)\n    qualified = qualified.sort_values('wr', ascending=False).head(10)\n    return qualified","5bd8246d":"vote_counts = md[md['vote_count'].notnull()]['vote_count'].astype('int')\nvote_averages = md[md['vote_average'].notnull()]['vote_average'].astype('int')\nC = vote_averages.mean()\nC","e6d394be":"# The minimum votes required to be listed in the chart. We will use 95th percentile as our cutoff.\n# i.e. For a movie to feature in the charts, it must have more votes than at least 95% of the movies in the list.\nm = vote_counts.quantile(0.95)\nm","1df30332":"# Defining the weighted_rating() function\ndef weighted_rating(x):\n    v = x['vote_count']\n    R = x['vote_average']\n    return (v\/(v+m) * R) + (m\/(m+v) * C)","789bd0d1":"# Getting recommedations from our model\nimproved_recommendations('Jurassic Park')","ab02c0d2":"### We will build a more sophisticated recommender that takes genre, keywords, cast and crew into consideration.\n\n### To build our standard metadata based content recommender, we will need to merge our current dataset with the crew and the keyword datasets.","8a5edc5f":"### Metadata Based Recommender","60dbdd74":"### Movie Description Based Recommender","632ef500":"### Movie Description Based Recommender"}}