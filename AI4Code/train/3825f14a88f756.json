{"cell_type":{"f03036e7":"code","2e225403":"code","29850647":"code","1a1bab7b":"code","6a66aea1":"code","c9a84df2":"code","4a5f55f8":"code","9a1cc64d":"code","105f2249":"code","01b84d5f":"code","d4ddb172":"code","186f2f8f":"code","796d30f0":"code","d90adedd":"code","6b694509":"code","fcb25ed3":"code","7d61fbb9":"code","825a662c":"code","ea91c70a":"code","ff552111":"code","b4283a19":"code","d3bb2cf6":"code","92e2db11":"code","e75dd213":"code","f9e45627":"code","9f5ea094":"code","209a7723":"code","bf09c63f":"code","e8dc17ce":"code","da6d488d":"code","5d8f8afe":"code","273d19e9":"code","104e05e2":"code","f3499aa8":"code","b3fb2250":"code","24cbbb95":"code","a1ea54f2":"code","b1c3cb12":"code","4d3cb3b0":"code","4f95b3ee":"code","6dc13fc1":"code","19120eef":"code","e977fab4":"code","f8aadfc6":"code","f617ddc7":"code","2a2c8b41":"code","3f835b4e":"code","60f8abc1":"code","361f54a3":"code","f5226a9d":"code","13a46d3d":"code","fe968659":"code","67cddbc0":"code","fa06594e":"code","aa3fdf2c":"code","b526f963":"code","69ec3e40":"code","75e83642":"code","fa14765a":"code","53ee7171":"markdown","b04414ab":"markdown","6c4f91a3":"markdown","ee7a7c3e":"markdown","9c39d9de":"markdown","3a4f2f8d":"markdown","a5067868":"markdown","93e604d9":"markdown","5a8e4ec3":"markdown","f01c55c7":"markdown","35af975f":"markdown","98969d09":"markdown","c2ac61b8":"markdown","f1f30fb4":"markdown","24b7fa9d":"markdown","b04bec01":"markdown","d6c9e01a":"markdown","9b858a47":"markdown","bc2a385f":"markdown","e68d862f":"markdown","9ba26ce6":"markdown","62adeee5":"markdown","fe17cd8d":"markdown","1e140bdd":"markdown","db984cf7":"markdown","f17d5d4f":"markdown","ad7456a9":"markdown","90630ed3":"markdown","6264bdfb":"markdown","1c03f099":"markdown","51aa65e3":"markdown","671469e6":"markdown","4e6f4fd4":"markdown","74403de2":"markdown","bef43d37":"markdown","436eb0ea":"markdown"},"source":{"f03036e7":"%matplotlib inline\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_colwidth', None)\n#pd.set_option('display.max_rows', None)\n\nimport gc\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import classification_report, roc_auc_score, recall_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom plotly.offline import iplot\n#to link plotly to pandas\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline = False, world_readable = True)\n\nfrom IPython.display import display\n\nplt.rcParams[\"figure.figsize\"] = (12, 8)\nplt.rcParams['axes.titlesize'] = 16\nplt.style.use('seaborn-whitegrid')\nsns.set_palette('Set2')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom time import time, strftime, gmtime\nstart = time()\nimport datetime\nprint(str(datetime.datetime.now()))\n\nimport warnings\nwarnings.simplefilter('ignore')","2e225403":"df = pd.read_csv('\/kaggle\/input\/credit-card-customers\/BankChurners.csv')\nprint(df.shape)\ndf.rename(columns = {'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1': 'NBC 12_1', \n                    'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2': 'NBC 12_2'}, inplace = True)\ndf.head()","29850647":"df['CLIENTNUM'].nunique()","1a1bab7b":"df['Attrition_Flag'] = df['Attrition_Flag'].map({'Existing Customer': 0, 'Attrited Customer': 1})","6a66aea1":"ax = sns.countplot(data = df, x = 'Attrition_Flag')\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005))","c9a84df2":"df.describe().T","4a5f55f8":"df.info()","9a1cc64d":"df.isna().sum()","105f2249":"num_cols = [c for c in df.columns if (df[c].dtype != 'object') & (c != 'Attrition_Flag')]\ncat_cols = [c for c in df.columns if (c not in num_cols) & (c != 'Attrition_Flag')]\nlen(num_cols), cat_cols","01b84d5f":"fig, ax = plt.subplots(int(18 \/ 2), 2, figsize = (16, 24))\nax = ax.flatten()\n\nfor i, c in enumerate(num_cols):\n    sns.boxplot(x = df[c], ax = ax[i])\nplt.suptitle('Outlier Analysis using BoxPlots', fontsize = 25, y = 1)\nplt.delaxes()\nfig.tight_layout()","d4ddb172":"df.drop(['NBC 12_1', 'NBC 12_2'], axis = 1, inplace = True)\nnum_cols = [x for x in num_cols if x not in ['NBC 12_1', 'NBC 12_2']]\ndf.shape, num_cols","186f2f8f":"df['Customer_Age'].iplot(\n    kind = 'hist',\n    bins = 50,\n    xTitle = 'Customer Age',\n    yTitle = 'Count',\n    title = 'Customer Age Distribution'\n)","796d30f0":"df['Customer_Age'].max(), df['Customer_Age'].min()","d90adedd":"plt.title('Unique Values Count of Gender')\nplt.pie(df['Gender'].value_counts().values, labels = df['Gender'].value_counts().index, autopct = '%1.2f%%', \nexplode = [0, 0.05], shadow = True);","6b694509":"plt.title('Unique Values Count of Education_Level')\npct = df['Education_Level'].value_counts().values \/ np.sum(df['Education_Level'].value_counts()) * 100\n\nplt.pie(df['Education_Level'].value_counts().values, labels = df['Education_Level'].value_counts().index, autopct = '%1.2f%%', \n explode = (pct == max(pct)) * 0.1, shadow = True);","fcb25ed3":"plt.title('Unique Values Count of Income_Category')\npct = df['Income_Category'].value_counts().values \/ np.sum(df['Income_Category'].value_counts()) * 100\n\nplt.pie(df['Income_Category'].value_counts().values, labels = df['Income_Category'].value_counts().index, autopct = '%1.2f%%', \n explode = (pct == max(pct)) * 0.05, shadow = True);","7d61fbb9":"def show_value(x):\n    a  = np.round(x \/ 100.0 * np.sum(df['Marital_Status'].value_counts().values), 2)\n    return a\n\nplt.title('Unique Values Count of Marital_Status')\npct = df['Marital_Status'].value_counts().values \/ np.sum(df['Marital_Status'].value_counts()) * 100\n\nplt.pie(df['Marital_Status'].value_counts().values, labels = df['Marital_Status'].value_counts().index, autopct = show_value, \n explode = (pct == max(pct)) * 0.05, shadow = True);","825a662c":"df[df['Marital_Status'] == 'Unknown']['Customer_Age'].iplot(kind = 'hist', bins = 50, linecolor = 'black')","ea91c70a":"plt.title('Unique Values Count of Card_Category')\n\nax = sns.countplot(data = df, x = 'Card_Category')\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005))","ff552111":"plt.title('Unique Values Count of Dependent_count')\n\nax = sns.countplot(data = df, x = 'Dependent_count')\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005))","b4283a19":"plt.title('Unique Values Count of Total_Relationship_Count')\n\nax = sns.countplot(data = df, x = 'Total_Relationship_Count')\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005))","d3bb2cf6":"df['Total_Trans_Ct'].iplot(\n    kind = 'hist',\n    bins = 100,\n    xTitle = 'Total Trans Count',\n    yTitle = 'Count',\n    title = 'Total Transaction Count Distribution for the last 12 months',\n    linecolor = 'black'\n)","92e2db11":"df['Total_Trans_Amt'].iplot(\n    kind = 'hist',\n    bins = 100,\n    xTitle = 'Total Trans Amount',\n    yTitle = 'Count',\n    title = 'nsaction Amount Distribution for the last 12 months',\n    linecolor = 'black'\n)","e75dd213":"plt.title('Number of Customers Inactive - Months_Inactive_12_mon')\n\nax = sns.countplot(data = df, x = 'Months_Inactive_12_mon')\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005))","f9e45627":"plt.title('Number of Customers Contacted - Contacts_Count_12_mon')\n\nax = sns.countplot(data = df, x = 'Contacts_Count_12_mon')\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005))","9f5ea094":"plt.figure(figsize = (16, 10))\nplt.title('Number of Customers holding the card for months')\nax = sns.countplot(data = df, x = 'Months_on_book')\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005))","209a7723":"temp = pd.pivot_table(data = df, index = 'Card_Category', columns = ['Income_Category', 'Marital_Status', 'Education_Level', 'Gender'], \n                      values = ['Attrition_Flag'], aggfunc = 'mean', fill_value = 0)\ntemp.columns = temp.columns.ravel()\n#Drop columns with all 0\n#temp = temp.loc[:, temp.sum(axis = 0) != 0].T\ntemp = temp.T\ntemp.style.background_gradient(sns.light_palette('#2ecc71', as_cmap = True))","bf09c63f":"temp.sum(axis = 0)","e8dc17ce":"temp.loc[temp['Blue'] > 0.25].style.background_gradient(sns.light_palette('#2ecc71', as_cmap = True))","da6d488d":"categories = []\nfor each in temp.index.values:\n    #print(each)\n    categories.append('_'.join([each[1], each[2], each[3], each[4]]))\n#print(len(categories), categories[:10])\ncat_dict = {categories[i]: i for i, c in enumerate(categories)}\ncat_dict","5d8f8afe":"df_new = df.copy()","273d19e9":"df_new['new_cat'] = df_new['Income_Category'] + '_' + df_new['Marital_Status'] + '_' + df_new['Education_Level'] + '_' + df_new['Gender']\ndf_new.drop(['Income_Category', 'Marital_Status', 'Education_Level', 'Gender'], axis = 1, inplace = True)\ndf_new['new_cat'] = df_new['new_cat'].map(cat_dict)\ndf_new.head()","104e05e2":"pi_values = ['Customer_Age', 'Months_on_book', 'Total_Relationship_Count', 'Months_Inactive_12_mon', \n            'Credit_Limit', 'Total_Revolving_Bal', 'Contacts_Count_12_mon', 'Avg_Open_To_Buy', 'Total_Trans_Amt', \n            'Total_Trans_Ct', 'Avg_Utilization_Ratio', 'Dependent_count']","f3499aa8":"pivot1 = pd.pivot_table(data = df_new[df_new['Attrition_Flag'] == 1], index = 'new_cat', \n               values = pi_values, aggfunc = 'mean', fill_value = 0).T\n#pivot1.head()","b3fb2250":"pivot0 = pd.pivot_table(data = df_new[df_new['Attrition_Flag'] == 0], index = 'new_cat', \n               values = pi_values, aggfunc = 'mean', fill_value = 0).T\n#pivot0.head()","24cbbb95":"import plotly.express as px\n\ndef visualize_plotly(a, b, title):\n    t = pd.concat([a, b], axis = 1)\n    t.columns = ['Churn', 'NoChurn']\n    t['New Category'] = t.index\n    fig = px.line(t, x = 'New Category', y = ['Churn', 'NoChurn'], title = title, \n                 color_discrete_sequence = ['cyan', 'coral'])\n    fig.update_layout({'plot_bgcolor': 'white'})\n    fig.update_xaxes(showgrid = True, gridwidth = 1, gridcolor = 'floralwhite')\n    fig.update_yaxes(showgrid = True, gridwidth = 1, gridcolor = 'floralwhite')\n    fig.show()","a1ea54f2":"for c in pivot1.index[:4]:\n    visualize_plotly(pivot1.loc[c, :], pivot0.loc[c, :], title = c)","b1c3cb12":"for c in pivot1.index[4: 8]:\n    visualize_plotly(pivot1.loc[c, :], pivot0.loc[c, :], title = c)","4d3cb3b0":"for c in pivot1.index[8:]:\n    visualize_plotly(pivot1.loc[c, :], pivot0.loc[c, :], title = c)","4f95b3ee":"df.head()","6dc13fc1":"#Dropping CLIENTNUM\ndf.drop('CLIENTNUM', axis = 1, inplace = True)\nnum_cols.remove('CLIENTNUM')","19120eef":"corr1 = df[num_cols].corr(method = 'pearson')\ncorr2 = df[num_cols].corr(method = 'spearman')\n\nfig = plt.figure(figsize = (10, 8))\nmask = np.triu(np.ones_like(corr1, dtype = bool))\nsns.heatmap(corr1, mask = mask, annot = True, cmap = 'PiYG', vmin = -1, vmax = +1)\nplt.title('Pearson Correlation')\nplt.xticks(rotation = 50)\nplt.show()\n\nfig = plt.figure(figsize = (10, 8))\nmask = np.triu(np.ones_like(corr2, dtype = bool))\nsns.heatmap(corr2, mask = mask, annot = True, cmap = 'PiYG', vmin = -1, vmax = +1)\nplt.title('Spearman Correlation')\nplt.xticks(rotation = 50)\nplt.show()","e977fab4":"# example of scatter plot - we pick pair having highest (Pearson) correlation\nsns.jointplot(data = df, x = 'Total_Trans_Amt', y = 'Total_Amt_Chng_Q4_Q1', hue = 'Attrition_Flag')\nplt.show()","f8aadfc6":"sns.jointplot(data = df, x = 'Total_Trans_Ct', y = 'Total_Ct_Chng_Q4_Q1', hue = 'Attrition_Flag')\nplt.show()","f617ddc7":"#High Negative Correlation\nsns.jointplot(data = df, x = 'Avg_Utilization_Ratio', y = 'Avg_Open_To_Buy', hue = 'Attrition_Flag')\nplt.show()","2a2c8b41":"lbl = LabelEncoder()\n\nfor c in df[cat_cols]:\n    print(f\"Label Encoding Categorical Feature - {c.upper()}\")\n    df[c] = lbl.fit_transform(df[c])\nprint('Label Encoding done...')\ndf[cat_cols].head()","3f835b4e":"std = StandardScaler()\n\ndf[num_cols] = std.fit_transform(df[num_cols])\nprint('Standardizing Numerical Features done...')\ndf[num_cols].head()","60f8abc1":"X = df.drop('Attrition_Flag', axis = 1)\ny = df['Attrition_Flag'].copy()\n\n#Xtrain, Xvalid, ytrain, yvalid = train_test_split(X, y, stratify = y, random_state = 2021)\n#print(Xtrain.shape, ytrain.shape, Xvalid.shape, yvalid.shape)","361f54a3":"def plot_confusion(mat):\n    plt.figure(figsize = (8, 4))\n    sns.heatmap(pd.DataFrame(mat), annot = True, annot_kws = {\"size\": 25}, cmap = 'PiYG', fmt = 'g')\n    plt.title('Confusion matrix', y = 1.1, fontsize = 22)\n    plt.ylabel('Actual', fontsize = 18)\n    plt.xlabel('Predicted', fontsize = 18)\n    plt.show()","f5226a9d":"num_pos_samples = y.value_counts().values[1]\nnum_neg_samples = y.value_counts().values[0]\nnum_neg_samples \/ num_pos_samples","13a46d3d":"from xgboost import XGBClassifier\nimport xgboost as xgb\n\nxgb_params = {\n         'objective': 'binary:logistic',\n         'lambda': 0.0030282073258141168, \n         'alpha': 0.01563845128469084,\n         'colsample_bytree': 0.55,\n         'subsample': 0.7,\n         'learning_rate': 0.01,\n         'max_depth': 9,\n         'random_state': 2020, \n         'min_child_weight': 257,\n         'eval_metric': 'auc',\n         'seed': 2021,\n         'scale_pos_weight': num_neg_samples \/ num_pos_samples\n         }","fe968659":"n_folds = 5\npreds_xg = []\nmean_recall = []\n\nskf = StratifiedKFold(n_splits = n_folds)\n\nfor i, (trn_idx, val_idx) in enumerate(skf.split(X, y)):\n    print(f\"Fold: {i + 1}\")\n    Xtrain, ytrain = X.iloc[trn_idx], y[trn_idx]\n    Xvalid, yvalid = X.iloc[val_idx], y[val_idx]\n    print(Xtrain.shape, ytrain.shape, Xvalid.shape, yvalid.shape)\n    \n    xg_train = xgb.DMatrix(Xtrain, label = ytrain)\n    xg_valid = xgb.DMatrix(Xvalid, label = yvalid)\n\n    xgboost_sim = xgb.train(xgb_params,\n                              xg_train,\n                              10000,\n                              verbose_eval = 200,\n                              evals = [(xg_train, 'train'), (xg_valid, 'valid')],\n                              early_stopping_rounds = 100)\n\n    valid_preds = xgboost_sim.predict(xg_valid)\n    print('XGBOOST ROC_AUC_SCORE: ', roc_auc_score(yvalid, valid_preds))\n    print('XGBOOST RECALL SCORE: ', recall_score(yvalid, valid_preds > 0.5, average = 'macro'))\n    mean_recall.append(recall_score(yvalid, valid_preds > 0.5, average = 'macro'))\n    conf_mat = confusion_matrix(yvalid, valid_preds > 0.5)\n    plot_confusion(conf_mat)\n    \n    preds_xg.append(valid_preds)\n    print()\nprint(f\"Mean Recall Score: {round(np.mean(mean_recall), 2)}\")","67cddbc0":"from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\n\nfrom collections import Counter","fa06594e":"n_folds = 5\npreds_xg = []\nmean_recall = []\nskf = StratifiedKFold(n_splits = n_folds)\n\nprint(f\"Original Dataset class count: {Counter(y)}\")\nprint('OverSampling...')\nsmote = SMOTE(random_state = 2021)\nX_sm, y_sm = smote.fit_resample(X, y)\nprint(X_sm.shape, y_sm.shape)\nprint(f\"OverSampled Dataset class count: {Counter(y_sm)}\")\n    \nfor i, (trn_idx, val_idx) in enumerate(skf.split(X_sm, y_sm)):\n    print(f\"Fold: {i + 1}\")\n    Xtrain, ytrain = X_sm.iloc[trn_idx], y_sm[trn_idx]\n    Xvalid, yvalid = X_sm.iloc[val_idx], y_sm[val_idx]\n    print(Xtrain.shape, ytrain.shape, Xvalid.shape, yvalid.shape)\n    #print(f\"Original Dataset class count: {Counter(ytrain)}\")\n    \n    \n    xg_train = xgb.DMatrix(Xtrain, label = ytrain)\n    xg_valid = xgb.DMatrix(Xvalid, label = yvalid)\n\n    xgboost = xgb.train(xgb_params,\n                              xg_train,\n                              10000,\n                              verbose_eval = 200,\n                              evals = [(xg_train, 'train'), (xg_valid, 'valid')],\n                              early_stopping_rounds = 100)\n\n    valid_preds = xgboost.predict(xg_valid)\n    print('XGBOOST ROC_AUC_SCORE - OverSampled: ', roc_auc_score(yvalid, valid_preds))\n    print('XGBOOST RECALL SCORE - OverSampled: ', recall_score(yvalid, valid_preds > 0.5))\n    mean_recall.append(recall_score(yvalid, valid_preds > 0.5))\n    conf_mat = confusion_matrix(yvalid, valid_preds > 0.5)\n    plot_confusion(conf_mat)\n    \n    preds_xg.append(valid_preds)\n    print()\nprint(f\"Mean Recall Score: {round(np.mean(mean_recall), 4)}\")","aa3fdf2c":"import shap\n\nshap.initjs()","b526f963":"explainer = shap.TreeExplainer(xgboost_sim)\nshap_values = explainer.shap_values(Xvalid)","69ec3e40":"shap.summary_plot(shap_values, Xvalid)","75e83642":"shap.force_plot(explainer.expected_value, shap_values[1], Xvalid.iloc[0, :])","fa14765a":"finish = time()\nprint(strftime(\"%H:%M:%S\", gmtime(finish - start)))","53ee7171":"__Features Details__\n- Clientnum\t-\tClient number. Unique identifier for the customer holding the account\n- Attrition_Flag\t-\tInternal event (customer activity) variable - if the account is closed then 1 else 0\n- Customer_Age\t-\tDemographic variable - Customer's Age in Years\n- Gender\t-\tDemographic variable - M=Male, F=Female\n- Dependent_count\t-\tDemographic variable - Number of dependents\n- Education_Level\t-\tDemographic variable - Educational Qualification of the account holder (example: high school, college graduate, etc.)\n- Marital_Status\t-\tDemographic variable - Married, Single, Unknown\n- Income_Category\t-\tDemographic variable - Annual Income Category of the account holder (< $40K, $40K - 60K, $60K - $80K, $80K-$120K, > $120K, Unknown)\n- Card_Category\t-\tProduct Variable - Type of Card (Blue, Silver, Gold, Platinum)\n- Months_on_book\t-\tMonths on book (Time of Relationship)\n- Total_Relationship_Count\t-\tTotal no. of products held by the customer\n- Months_Inactive_12_mon\t-\tNo. of months inactive in the last 12 months\n- Contacts_Count_12_mon\t-\tNo. of Contacts in the last 12 months\n- Credit_Limit\t-\tCredit Limit on the Credit Card\n- Total_Revolving_Bal\t-\tTotal Revolving Balance on the Credit Card\n- Avg_Open_To_Buy\t-\tOpen to Buy Credit Line (Average of last 12 months)\n- Total_Amt_Chng_Q4_Q1\t-\tChange in Transaction Amount (Q4 over Q1) \n- Total_Trans_Amt\t-\tTotal Transaction Amount (Last 12 months)\n- Total_Trans_Ct\t-\tTotal Transaction Count (Last 12 months)\n- Total_Ct_Chng_Q4_Q1\tNum\tChange in Transaction Count (Q4 over Q1) \n- Avg_Utilization_Ratio\t-\tAverage Card Utilization Ratio","b04414ab":"*__Customer_Age__*\n- Customers' age doesn't play a part in churning accodring to the plot above\n\n*__Months_Inactive_12_mon__*\n- Customers who tend to churn were more inactive in the last 12 months than the customers who do not churn\n\n*__Months_on_book__*\n- On an average most of the customers who stay with the bank - no churn - are with then between 35 to 40 months","6c4f91a3":"*__Total_Relationship_Count__*\n- There is a clear trend that customers who hold more products from the same bank stay with the bank while the customers holding fewer products churn\n\n*__Total_Revolving_Balance__*\n- Customers with lower revolving balance are churning more\n\n*__Total_Trans_Amt__*\n- Churning customers spend lesser than non churning customers\n\n*__Total_Trans_Ct__*\n- Customers who churn make fewer transactions using the bank's card than the customers stay with the bank","ee7a7c3e":"# Feature Importance Using SHAP","9c39d9de":"*__Avg_Open_To_Buy - Open to buy Credit Line__*\n- Both the Churn and No churn customers' trendline are almost the same for this feature\n\n*__Avg_Utilization_Ratio__*\n- Its clear that this feature follows a lower trendline for Churned customer when compared to the No churn cusotmers\n\n*__Contacts_Count_12_mon__*\n- The bank has contacted the churning customers more frequently than the no churn cusotmers\n- Maybe they would have seen the open to buy Credit Line customers' trend\n\n*__Credit_Limit__*\n- Not much difference between the two segments","3a4f2f8d":"__Label Encode Categorical Features__","a5067868":"- There are 2463 customers who are holding the card for 36 months\n- There are 103 customers who are holding the catd for 56 months\n- 70 customers are holding for 13 months","93e604d9":"- We can remove the two NBC features as NBC 12_1 values are closer to 1 and NBC 12_2 is closer to 0","5a8e4ec3":"- There are 749 customers whose marital status is given as Unknown, let's check their age before deciding whether to impute this category or leave it as a value","f01c55c7":"__Total_Relationship_Count is the\tTotal no. of products held by the customer__","35af975f":"- Max age is 73 which is not an outlier","98969d09":"# OverSampling","c2ac61b8":"# Task 02\n- An in-depth Exploratory Data Analysis that can help to visualize where the difference lies between churning and non-churning customers.","f1f30fb4":"- Obviously Total_Trans_Amt and Total_Amt_Chng_Q4_Q1 are highly correlated as they are interrelated, same for Total_Trans_Ct\/Total_Ct_Chng_Q4_Q1","24b7fa9d":"__Outliers Check__","b04bec01":"- Target feature is 'Attrition_Flag' in which 'Attrited Customer' means it's a churn (1) and 'Existing Customer' means there is no churn (0)\n- First we map these values for the target","d6c9e01a":"- Features in red color influence positively, i.e. drag the prediction value closer to 1, features in blue color - the opposite\n- Bigger Total_Trans_Ct, Total_Trans_Amt leads to prediction towards '0' - which we can see from the plotly plots for the same\n- Each arrow\u2019s size represents the magnitude of the corresponding feature\u2019s effect\n- The \u201cbase value\u201d marks the model\u2019s average prediction over the training set","9b858a47":"- Blue category is highest - it must be entry-level card","bc2a385f":"# Managing Imbalanced Dataset\nThere are two ways to manage imbalanced dataset:\n\n- OverSampling\n- UnderSampling","e68d862f":"- All client numbers are unique.","9ba26ce6":"__Lets check the correlation between the Numerical Features__","62adeee5":"Thera are no missing values in the dataset","fe17cd8d":"__Standardize Numerical Features__","1e140bdd":"__SMOTE__","db984cf7":"- The pivot table above gives a good insight into churning customers based on the card category by their income, marital status and their education level\n- The Blue card holders are ones which is churning more followed by Silver card holders\n- The churn is minimal in Platinum category","f17d5d4f":"# Obeservations from Plotly Trendlines","ad7456a9":"The age distribution is between 25 and 65 years\n- We can consider one of the two options for Unknown category\n    1. Make the value as 'Married' for ages above 35, below 35 as 'Single'\n    2. Keep it as it is","90630ed3":"__Check for missing values__","6264bdfb":"- The dataset is highly imbalanced\n- We will have to do upsampling or downsampling","1c03f099":"# Task 02 - EDA","51aa65e3":"- The above gives the category where the mean churn is more than 0.25\n- Highest customer churn for Blue card holders is in the categories 80\ud835\udc3e\u2212120K Income, Unknown marital status with Post-Graduate education level and male\n- All the mean Attrition value equal to 1 is single cusotmer\n- It'll be interesting to select the customers(rows) based on these category groups","671469e6":"- Let's create a dict with the index of temp as values","4e6f4fd4":"# Task 01\n- Improve performance of predicting churned customers - recall need to be higher","74403de2":"__Customer Age__","bef43d37":"__Using amazing Plotly Express to plot interactive charts with pandas__","436eb0ea":"# Task 01\n- Improve performance of predicting churned customers - recall need to be higher"}}