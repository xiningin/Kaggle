{"cell_type":{"2ccb5d23":"code","89dc7cf4":"code","ecdba441":"code","026c7e58":"code","1fee8b33":"code","d16d4725":"code","d780579d":"code","a9f3c372":"code","7d5b9f53":"code","ddfafef4":"code","875a078f":"code","ed4fea21":"code","2e68d60b":"code","422c3684":"code","f29e183b":"code","8d1e5a03":"code","4a2b7a05":"code","19c63f56":"code","ded1f3fa":"code","a0e47d25":"code","4976d508":"code","cff6181a":"code","e2d9331f":"code","1962f3e4":"code","e854262c":"code","ad2ab0ce":"code","df5190aa":"code","394f495e":"code","830d2cf5":"code","e895bd31":"code","4439f072":"code","2732905d":"code","ea90e8c5":"code","ed373151":"code","0ddcae62":"code","03d26dc7":"code","97e26740":"code","9b05f9d7":"code","90a865c0":"code","ce89b6ea":"code","1422c1ca":"code","32e8cc30":"code","0f4f2900":"code","05d03272":"code","b2b6ad60":"code","2978b740":"code","34308268":"code","29e529f6":"code","02e79b28":"code","60bd68c3":"code","7aa38839":"code","b69e9679":"code","2d924415":"code","f5232f12":"code","0f37ba07":"code","3fbcfbd9":"code","2d6494be":"code","b1701607":"code","cf1d52a5":"code","12bbb738":"code","34190740":"code","4802534e":"code","377ae60f":"code","2e532951":"code","89063912":"code","bcafcba4":"code","2636e783":"code","6a5e9d39":"code","689a6c08":"code","b7318253":"code","07da9a0c":"code","09b403fa":"code","43a33855":"code","e852a97a":"code","325d8822":"code","7f417e04":"code","1a8d887e":"code","37546934":"code","ecc025d0":"code","5705928a":"code","94ce3e03":"code","95b3f947":"code","5b5eb4d2":"code","8f1701da":"code","9430efbe":"code","be2abcb4":"code","7eb2f2a2":"code","b0a24e71":"code","69634051":"markdown","6bd59367":"markdown","634ab701":"markdown","814221f6":"markdown","1b254bcc":"markdown","413fc937":"markdown","88957236":"markdown","2b00d6fd":"markdown","590b9b9b":"markdown","df7be4d2":"markdown","d6591829":"markdown","aeda0b33":"markdown","86905a6b":"markdown","c565de8e":"markdown","8ff5381f":"markdown","a1053fc3":"markdown","cbf1471e":"markdown","5d8077e7":"markdown","a1a27a8d":"markdown","a6c4aace":"markdown","41e83a82":"markdown","ab49cd01":"markdown","26374725":"markdown","fe297d7d":"markdown","a8a4cbf4":"markdown","8290c42d":"markdown","5d4f621d":"markdown","77cdfcbc":"markdown","87b09056":"markdown","bbef8390":"markdown","c4ca7d75":"markdown","d3ad23a4":"markdown","18cf8114":"markdown","2e9e2fcc":"markdown","0a7df7b5":"markdown","6b6bd5fc":"markdown","2c05d889":"markdown","af73a6b7":"markdown"},"source":{"2ccb5d23":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","89dc7cf4":"train = pd.read_csv(\"TRAIN.csv\")\ntrain.head(5)","ecdba441":"test = pd.read_csv(\"TEST_FINAL.csv\")\ntest.head(5)","026c7e58":"train.shape , test.shape","1fee8b33":"train.info(),test.info()","d16d4725":"train.isnull().sum(), test.isnull().sum()","d780579d":"train.duplicated().sum(), test.duplicated().sum()","a9f3c372":"# train_t = train.T\n# test_t = test.T\n# train_t.duplicated().sum(), test_t.duplicated().sum()","7d5b9f53":"test.head()\ntrain.head()","ddfafef4":"y = train.Sales\ntrain_new = train.drop([\"Sales\"],axis=1)","875a078f":"#We have 2 types of data in our dataset : int64 and object\n\ntrain_categorical = train_new.select_dtypes(exclude = ['int64'])\ntest_categorical = test.select_dtypes(exclude = ['int64'])\n\ntrain_numerical = train_new.select_dtypes(include = ['int64'])\ntest_numerical = test.select_dtypes(include = ['int64'])\n\n#Defining column names for numerical data\nnumcol_names_train = train_numerical.columns.values\nnumcol_names_test = test_numerical.columns.values\nnumcol_names_train","ed4fea21":"#Converting these to list from array\nnumcol_names_train.tolist(), numcol_names_test.tolist()","2e68d60b":"sns.kdeplot(train_numerical['#Order'], bw=0.5)    #bw is smoothing parameter\nplt.show()","422c3684":"from sklearn.preprocessing import LabelEncoder\n\ntrain_categorical = train_categorical.apply(LabelEncoder().fit_transform)\ntest_categorical = test_categorical.apply(LabelEncoder().fit_transform)\n\n#Combining the Numnerical and Categorical Database\ntrain_new= pd.concat([train_categorical,train_numerical,y],axis=1)\n\ntest_new = pd.concat([test_categorical,test_numerical],axis=1)\ntrain_new.head()","f29e183b":"test_new.head(5)","8d1e5a03":"#For coplete Database\ncorr_train = train_new.corr()\nplt.figure(figsize=(13,5)) \n\nax = sns.heatmap(corr_train,annot=True)\nplt.show","4a2b7a05":"imp = train_new.drop(\"Sales\", axis=1).apply(lambda x: x.corr(train_new[\"Sales\"]))\nindices = np.argsort(imp)\nprint(imp[indices])     #Sorted in ascending order","19c63f56":"for i in range(0, len(indices)):\n    if np.abs(imp[i]) < 0.1:\n        print(train_new.columns[i])","ded1f3fa":"train_new.drop([\"Date\",\"Store_id\"],axis=1,inplace=True)","a0e47d25":"train_new.head()","4976d508":"test_new.drop([\"Date\",\"Store_id\"],axis=1,inplace=True)\ntest_new.head()","cff6181a":"for i in range(0,len(train_new.columns)):\n    for j in  range(0,len(train_new.columns)):\n        if i!=j:\n            corr_1=np.abs(train_new[train_new.columns[i]].corr(train_new[train_new.columns[j]]))\n            if corr_1 <0.3:\n                print( train_new.columns[i] , \" is not correlated  with \", train_new.columns[j])\n            elif corr_1>0.75:\n                print( train_new.columns[i] , \" is highly  correlated  with \", train_new.columns[j])","e2d9331f":"out = sns.boxplot(train_new[\"Sales\"])\nplt.show()","1962f3e4":"Q1 = train_new[\"Sales\"].quantile(0.25)\nQ3 = train_new[\"Sales\"].quantile(0.75)\nIQR = Q3 - Q1\n\nfilter = (train_new[\"Sales\"] >= Q1 - 1.5 * IQR) & (train_new[\"Sales\"] <= Q3 + 1.5 *IQR)\ntrain2 = train_new.loc[filter]  \nprint(\"data loss percentage {}%\".format(((len(train_new) - len(train2))\/len(train_new))*100))","e854262c":"# Split the Train data into predictors and target\n\nx = train_new.drop(['Sales',\"ID\",\"#Order\"],axis=1)\ny = train_new['Sales']\n\ntest_set = test_new.drop(\"ID\",axis=1)","ad2ab0ce":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 100)","df5190aa":"from sklearn.linear_model import LinearRegression\nmodel1 = LinearRegression()  \nmodel1.fit(x_train, y_train)","394f495e":"#Prediction from validation set\ny_pred_model1= model1.predict(x_test)\n\nprint(\"Prediction for test set: {}\".format(y_pred_model1))","830d2cf5":"model1.score(x_test,y_test)","e895bd31":"import sklearn.preprocessing\ny_pred_model1_p = sklearn.preprocessing.minmax_scale(y_pred_model1, feature_range=(0,1))\ny_test_p = sklearn.preprocessing.minmax_scale(y_test, feature_range=(0,1))\n\nfrom sklearn.metrics import mean_squared_log_error\nmean_squared_log_error(y_test_p, y_pred_model1_p)","4439f072":"#Prediction from test set\ntest_pred_model1= model1.predict(test_set)\nprint(\"Prediction for test set: {}\".format(test_pred_model1))","2732905d":"frame = { 'ID': test.ID, 'Sales': test_pred_model1 }\nsub1 = pd.DataFrame(frame)\nsub1.set_index('ID', inplace=True)\nsub1 = sub1.to_csv(\"sub1.csv\")","ea90e8c5":"from sklearn.tree import DecisionTreeRegressor\nmodel2 = DecisionTreeRegressor() \nmodel2.fit(x_train, y_train)\n\n#Prediction for validation set\ny_pred_model2 = model2.predict(x_test)\n\n#MSLE FOR Validation data\nmean_squared_log_error(y_test, y_pred_model2)","ed373151":"model2.score(x_test,y_test)","0ddcae62":"#Prediction from test set\ntest_pred_model2= model2.predict(test_set)\n\nframe = { 'ID': test.ID, 'Sales': test_pred_model2}\nsub2 = pd.DataFrame(frame)\nsub2.set_index('ID', inplace=True)\nsub2 = sub2.to_csv(\"sub2.csv\")","03d26dc7":"from sklearn.ensemble import RandomForestRegressor\nmodel4 = RandomForestRegressor()\nmodel4.fit(x_train, y_train)","97e26740":"model4.score(x_test,y_test)","9b05f9d7":"#Prediction for validation set\ny_pred_model4 = model4.predict(x_test)\n\n#MSLE FOR Validation data\nmean_squared_log_error(y_test, y_pred_model4)","90a865c0":"#Prediction from test set\ntest_pred_model4 = model4.predict(test_set)\n\nframe = { 'ID': test.ID, 'Sales': test_pred_model4}\nsub4 = pd.DataFrame(frame)\nsub4.set_index('ID', inplace=True)\nsub4 = sub4.to_csv(\"sub4.csv\")","ce89b6ea":"import xgboost as xg\nmodel5 = xg.XGBRegressor(n_estimators = 500, seed = 100)\nmodel5.fit(x_train, y_train)","1422c1ca":"model5.score(x_test,y_test)","32e8cc30":"#Prediction for validation set\ny_pred_model5 = model5.predict(x_test)\n\n#MSLE FOR Validation data\nmean_squared_log_error(y_test, y_pred_model5)","0f4f2900":"#Prediction from test set\ntest_pred_model5 = model5.predict(test_set)\n\nframe = { 'ID': test.ID, 'Sales': test_pred_model5}\nsub5 = pd.DataFrame(frame)\nsub5.set_index('ID', inplace=True)\nsub5 = sub5.to_csv(\"sub5.csv\")","05d03272":"x1 = train_new.drop(['Sales',\"ID\",\"#Order\"],axis=1)\ny1 = train_new['#Order']\n\nx1_train, x1_test, y1_train, y1_test = train_test_split(x1, y1, test_size = 0.2, random_state = 100)\n\nmodel61 = RandomForestRegressor(n_estimators=500)\nmodel61.fit(x1_train, y1_train)\n\n#Prediction for validation set\ny_pred_model61 = model61.predict(x1_test)\n\n#MSLE FOR Validation data\naccuracy_pred_orders = model61.score(x1_test,y1_test)\n\naccuracy_pred_orders ","b2b6ad60":"y_pred_model61[0].dtype","2978b740":"#Prediction of orders for test data:\npred_order_test = model61.predict(test_set)\npred_order_test = pd.Series(pred_order_test)\n\ntest_set_new = pd.concat([test_set,pred_order_test],axis=1)\n\ntest_set_new.rename({0: '#Order'}, axis=1,inplace=True)","34308268":"pred_order_test.head(3)","29e529f6":"test_set_new.head()","02e79b28":"from sklearn.ensemble import RandomForestRegressor\nmodel6 = RandomForestRegressor()\n\nx2 = train_new.drop([\"ID\",\"Sales\"],axis=1)\ny2 = train_new['Sales']\n\nx2_train, x2_test, y2_train, y2_test = train_test_split(x2, y2, test_size = 0.2, random_state = 100)\n\nmodel6.fit(x2_train, y2_train)\n\n#Prediction for validation set\ny_pred_model6 = model6.predict(x2_test)\n\n#MSLE FOR Validation data\nmean_squared_log_error(y2_test, y_pred_model6)","60bd68c3":"#Prediction from test set\ntest_pred_model6 = model6.predict(test_set_new)\n\nframe = { 'ID': test.ID, 'Sales': test_pred_model6}\nsub6 = pd.DataFrame(frame)\nsub6.set_index('ID', inplace=True)\nsub6 = sub6.to_csv(\"sub6.csv\")","7aa38839":"x2 = train.drop([\"ID\",\"Sales\"],axis=1)\nx2_test = test.drop(\"ID\",axis=1)\ny2 = train.Sales\n\nx2_categorical = x2.select_dtypes(exclude = ['int64'])\nx2_numerical = x2.select_dtypes(include = ['int64'])\n\nx2_test_categorical = x2_test.select_dtypes(exclude = ['int64'])\nx2_test_numerical = x2_test.select_dtypes(include = ['int64'])\n\n\nx2_categorical = x2_categorical.apply(LabelEncoder().fit_transform)\nx2_test_categorical = x2_test_categorical.apply(LabelEncoder().fit_transform)\n\n#Combining the Numnerical and Categorical Database\nx2 = pd.concat([x2_categorical,x2_numerical],axis=1)\nx2_test = pd.concat([x2_test_categorical,x2_test_numerical,pred_order_test],axis=1)\nx2_test.rename({0: '#Order'}, axis=1,inplace=True)\n\nx2_test.shape\nfrom sklearn.ensemble import RandomForestRegressor\nmodel7 = RandomForestRegressor()\n\nx2_train, x22_test, y2_train, y22_test = train_test_split(x2, y2, test_size = 0.2, random_state = 100)\n\nmodel7.fit(x2_train, y2_train)\n\n#Prediction for validation set\ny_pred_model7 = model7.predict(x22_test)\n\n#MSLE FOR Validation data\nmean_squared_log_error(y22_test, y_pred_model7)","b69e9679":"model7.score(x22_test,y22_test)","2d924415":"#Prediction from test set\ntest_pred_model7 = model7.predict(x2_test)\n\nframe = { 'ID': test.ID, 'Sales': test_pred_model7}\nsub7 = pd.DataFrame(frame)\nsub7.set_index('ID', inplace=True)\nsub7 = sub7.to_csv(\"sub7.csv\")","f5232f12":"test_pred_model7.shape  ","0f37ba07":"#Generating Pandas Profiling Report for better analysis:\nimport pandas_profiling\nprofile = train.profile_report()\nprofile","3fbcfbd9":"from sklearn.utils import resample\n\ny = train_new.Holiday\nx = train_new.drop(\"Holiday\",axis=1)\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 100)\n\nd = pd.concat([x_train,y_train],axis=1)\n\nholi = train_new[train_new.Holiday==1]\nnot_holi = train_new[train_new.Holiday==0]\n\n#resample minority\nholi_resampled = resample(holi,replace=True,n_samples=len(not_holi))\n\nresampled = pd.concat([holi_resampled,not_holi])\n\nresampled.Holiday.value_counts()","2d6494be":"resampled.head()","b1701607":"x_train = resampled.drop([\"ID\",\"Sales\",\"#Order\"],axis=1)\ny_train = resampled.Sales","cf1d52a5":"x_train.head()","12bbb738":"test_new.drop(\"ID\",axis=1,inplace=True)","34190740":"model8 = RandomForestRegressor()\n\nmodel8.fit(x_train, y_train)\n\n# #Prediction from test set\n# test_pred_model8 = model8.predict(test_new)\n\n# frame = { 'ID': test.ID, 'Sales': test_pred_model8}\n# sub8 = pd.DataFrame(frame)\n# sub8.set_index('ID', inplace=True)\n# sub8 = sub8.to_csv(\"sub8.csv\")","4802534e":"train = pd.read_csv(\"TRAIN.csv\",parse_dates=True)\ntest = pd.read_csv(\"TEST_FINAL.csv\",parse_dates=True)\n\ntrain['Date'] =  pd.to_datetime(train['Date'])\n\n#Leaving date and label column out of category encoding\ny = train[[\"Sales\",\"Date\"]]\ntrain_new = train.drop([\"Sales\",\"Date\",\"ID\"],axis=1)\n\ny1 = test[[\"Date\"]]\ntest_new = test.drop([\"Date\",\"ID\"],axis=1)\n\n#We have 2 types of data in our dataset : int64 and object\n\ntrain_categorical = train_new.select_dtypes(exclude = ['int64'])\ntest_categorical = test_new.select_dtypes(exclude = ['int64'])\n\ntrain_numerical = train_new.select_dtypes(include = ['int64'])\ntest_numerical = test_new.select_dtypes(include = ['int64'])\n\n#Defining column names for numerical data\nnumcol_names_train = train_numerical.columns.values\nnumcol_names_test = test_numerical.columns.values\nnumcol_names_train\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\ntrain_categorical = train_categorical.apply(LabelEncoder().fit_transform)\ntest_categorical = test_categorical.apply(LabelEncoder().fit_transform)\n\n#Combining the Numnerical and Categorical Database\ntrain_new= pd.concat([train_categorical,train_numerical,y],axis=1)\n\ntest_new = pd.concat([test_categorical,test_numerical,y1],axis=1)\n\n\n#Removing order as it will not be present in test dataset\ntrain_new.drop([\"#Order\"],axis=1,inplace=True)\n\n\n\ntrain2 = train_new[train_new[\"Store_id\"] == 100]\ntrain2\n\nfrom matplotlib.pyplot import figure\nfigure(figsize=(8, 6))\nplt.scatter(train2.Date,train2.Sales)\nplt.show()","377ae60f":"# Updating the header\n\ndata1 = train_new[(train_new[\"Store_id\"] == 1)]\ndata11 = data1[[\"Sales\",\"Date\"]]\ndata11.columns=[\"Sales\",\"Date\"]\n\ndata11 = data11.set_index('Date')\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10, 5\ndata11.plot()","2e532951":"from statsmodels.tsa.stattools import adfuller\ntest_result = adfuller(data11['Sales'])\n\ndef adfuller_test(sales):\n    result = adfuller(sales)\n    labels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations']\n    for value,label in zip(result,labels):\n        print(label+' : '+str(value) )\n\n    if result[1] <= 0.05:\n        print(\"Data is stationary\")\n    else:\n        print(\"Data is non-stationary \")\n        ","89063912":"adfuller_test(data11['Sales'])","bcafcba4":"from pandas.plotting import autocorrelation_plot\nautocorrelation_plot(data11['Sales'])\nplt.show()","2636e783":"import statsmodels\nimport statsmodels.api as sm\nfrom statsmodels.graphics.tsaplots import plot_acf,plot_pacf","6a5e9d39":"fig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(data11['Sales'].iloc[13:],lags=40,ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(data11['Sales'].iloc[13:],lags=40,ax=ax2)","689a6c08":"from statsmodels.tsa.arima_model import ARIMA\nimport warnings\nwarnings.filterwarnings(\"ignore\")","b7318253":"def prediction_for_store(store_id=None, train_new = train_new, test_new = test_new):\n    train = train_new[(train_new[\"Store_id\"] == store_id)]\n    train = train[[\"Sales\",\"Date\"]]\n    train.columns=[\"Sales\",\"Date\"]\n    train = train.set_index('Date')\n    \n    test = test_new[(test_new[\"Store_id\"] == store_id)]\n    \n    test_result = adfuller(train['Sales'])\n    \n    start = len(train)\n    end = len(train) + len(test) - 1\n\n    p_values = range(1,2)\n    d_values = range(0,1)\n    q_values = range(0,1)\n    \n    \n    prediction_store = []\n    for p in p_values:\n        for d in d_values:\n            for q in q_values:\n                order = (p,d,q)\n                for i in range(len(test)):\n                    try:\n                        model=ARIMA(train['Sales'],order)\n                        model_fit=model.fit(disp=0)\n                        pred_y = model_fit.forecast()[0]\n                        prediction_store.append(pred_y)\n                    except:\n                        continue\n    return prediction_store","07da9a0c":"predictions = []\nfor i in range(365):\n    pred = prediction_for_store(i+1)\n    predictions.append(pred)","09b403fa":"result=[]\nfor prediction in predictions:\n    for data in prediction:\n        result.extend(data)\n        \nlen(result)","43a33855":"frame = { 'ID': test.ID, 'Sales': result}\narima = pd.DataFrame(frame)\narima.set_index('ID', inplace=True)\narima = arima.to_csv(\"arima.csv\")","e852a97a":"sales_mean = train_new.groupby(['Store_id', 'Discount',\"Holiday\",\"Store_Type\",\"Region_Code\",\"Location_Type\"]).agg({\"Sales\":\"mean\"})\nsales_mean.reset_index(inplace=True)","325d8822":"sales_mean.head()","7f417e04":"x = sales_mean.drop([\"Sales\",\"Region_Code\",\"Location_Type\",\"Store_Type\"],axis=1)\ny = sales_mean.Sales","1a8d887e":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 99)","37546934":"from sklearn.ensemble import RandomForestRegressor\nmodel14 = RandomForestRegressor()\nmodel14.fit(x_train,y_train)\n\npred= model14.predict(x_test)\n\nmodel14.score(x_test,y_test)","ecc025d0":"from sklearn.metrics import mean_squared_log_error\nmean_squared_log_error(pred,y_test)","5705928a":"# pred14 = model14.predict(test_set)\n# frame = { 'ID': test.ID, 'Sales': pred14}\n# sales_mean = pd.DataFrame(frame)\n# sales_mean.set_index('ID', inplace=True)\n# sales_mean = sales_mean.to_csv(\"sales_mean.csv\")","94ce3e03":"x = train_new.drop([\"Sales\",\"Date\"],axis=1)\ny = train_new.Sales","95b3f947":"# from catboost import CatBoostRegressor\n\n# from sklearn.model_selection import train_test_split\n# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 99)\n\n# model10 = CatBoostRegressor()\n# # Fit model\n# model10.fit(x_train,y_train)\n\n# # Get predictions\n# pred10 = model10.predict(x_test)\n# preds_catboost = model10.predict(test_set)\n\n# accuracy_train10 = model10.score(x_test,y_test)\n\n# accuracy_train10","5b5eb4d2":"# model11 = xg.XGBRegressor()\n  \n\n# from sklearn.model_selection import train_test_split\n# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 99)\n\n# # Fitting the model\n# model11.fit(x_train,y_train)\n  \n# # Predict the model\n# pred11 = model11.score(x_test,y_test)\n# preds_xgboost = model11.predict(test_set)","8f1701da":"# sum1 = []\n# for i in range(len(preds_xgboost)):\n#     sum1.append(np.sum(preds_xgboost[i]+ preds_catboost[i])\/2)\n# sum1 ","9430efbe":"# n_estimators = [ int(x) for x in np.linspace(start=100,stop=1000,num=10)]\n# max_features = [ \"auto\",\"sqrt\"]\n# max_depth= [2,4]\n\n# # min_sample_split= [2,4]\n# min_samples_leaf = [1,6]\n# bootstrap = [True,False]\n\n# #Creating param grid\n# param_grid = {\"n_estimators\":n_estimators,\n#     \"max_features\" : max_features,\n#     \"max_depth\" : max_depth,\n# #     \"min_sample_split\" : min_sample_split,\n#     \"min_samples_leaf\" : min_samples_leaf,\n#     \"bootstrap\" : bootstrap}\n        \n# print(param_grid)\n    \n# from sklearn.ensemble import RandomForestRegressor\n# model9 = RandomForestRegressor()\n# from sklearn.model_selection import GridSearchCV\n\n# grid_search = GridSearchCV(estimator= model9,param_grid=param_grid,cv=3,verbose=2,n_jobs=4)\n\n# grid_search.fit(x_train,y_train)","be2abcb4":"# grid_search.best_params_","7eb2f2a2":"# grid_train = grid_search.score(x_test,y_test)\n# grid13 = grid_search.predict(test_set)\n\n# grid_train","b0a24e71":"# frame = { 'ID': test.ID, 'Sales': grid13}\n# sub13 = pd.DataFrame(frame)\n# sub13.set_index('ID', inplace=True)\n# sub13 = sub13.to_csv(\"sub13.csv\")","69634051":"#### Test Data\n- ID: Unique Identifier for a row\n\n- Store_id: Unique id for each Store\n\n- Store_Type: Type of the Store\n\n- Location_Type: Type of the location where Store is located\n\n- Region_Code: Code of the Region where Store is located\n\n- Date: Information about the Date\n\n- Holiday: If there is holiday on the given Date, 1 : Yes, 0 : No\n\n- Discount: If discount is offered by store on the given Date, Yes\/ No","6bd59367":"#### Encoding categorical Data","634ab701":"#### Oversampling to balance Holiday Column:","814221f6":"#### Model 6: Following data centric approach : Inproving data\n\npredicting number of orders first before predicting sales number","1b254bcc":"### Model 11: Mean of  Outcomes from 2 models","413fc937":"#### Differentiating Numericala and Categorical Data for further processing","88957236":"#### Model 2: Decision Tree:","2b00d6fd":"#### Auto regressive model","590b9b9b":"#### Removing Variable with Low correlation with Target Variables","df7be4d2":"#### Model 8: Applying Model on resampled data","d6591829":"#### Model 1: Linear regression","aeda0b33":"### Model 12: With grid search params","86905a6b":"### Dealing with Outliers:`","c565de8e":"#### grid search recommended features:\n    \nbootstrap= True,max_depth= 4,max_features= \"sqrt\",min_samples_leaf= 6, n_estimators= 1000","8ff5381f":"#### PS: Tune-in to the summary at last for what worked best for me, if you dont have much time for this big notebook!","a1053fc3":"#### Model 4:  Random Forest regressor","cbf1471e":"After exhaustive attempts of 3 days and giving my best, I received the best MSLE*1000 Error of 225 (rank 78!). I tried multiple approaches, used the data-centric approach by making changes to data multiple times (as many ways as my brain could think of) apart from changing models. \n\nI tried multiple models (approx 10), also implemented an algorithm I never used before ARIMA (in this hectic pressures!), much to my disappointment the model didn\u2019t perform that well as compared to ordinary ones.\n\nI also tried to group the data for same-store and make prediction store-wise instead of generic, which also didn\u2019t result in good results test results though performed exceptionally on my train dataset(overfitting because of fewer train data when clubbed).\n\nMy best results were in the most simple approach, (Simplicity at its best!) all the good models gave me almost equal results, with all features except Date, the params when kept at default performed best in my case(Still don\u2019t know how, but open to wonders)!\n\nAs this hackathon nears the closure, my mind is still restless with what can be the approach (eagerly waiting for top coder code file!!)\n\nStill, I feel satisfied as I gave my 100% at every hour of this weekend challenge and learned a lot!\n\nPeace :)\n","5d8077e7":"### Modelling","a1a27a8d":"#### Same results as with single model","a6c4aace":"from the above survey of profile , i can see that their is huge discrepancy in the holiday column, hence i will apply upsampling for this column and then predict results.","41e83a82":"#### resampling didnt perform that well!","ab49cd01":"- Ho: It is non-stationary\n- H1: It is stationary\n\nWe will be considering the null hypothesis that data is not stationary and the alternate hypothesis that data is stationary.","26374725":"#### Applying new test dataset on model6:","fe297d7d":"Supplement Sales Prediction\nYour Client WOMart is a leading nutrition and supplement retail chain that offers a comprehensive range of products for all your wellness and fitness needs. \n\nWOMart follows a multi-channel distribution strategy with 350+ retail stores spread across 100+ cities. \n\nEffective forecasting for store sales gives essential insight into upcoming cash flow, meaning WOMart can more accurately plan the cashflow at the store level.\n\nSales data for 18 months from 365 stores of WOMart is available along with information on Store Type, Location Type for each store, Region Code for every store, Discount provided by the store on every day, Number of Orders everyday etc.\n\nYour task is to predict the store sales for each store in the test set for the next two months.","a8a4cbf4":"### Data Dictionary","8290c42d":"### Model 9: ARIMA MODEL:","5d4f621d":"### Final Outcome","77cdfcbc":"#### Train Data\n- ID: Unique Identifier for a row\n\n- Store_id: Unique id for each Store\n\n- Store_Type: Type of the Store\n\n- Location_Type: Type of the location where Store is located\n\n- Region_Code: Code of the Region where Store is located\n\n- Date: Information about the Date\n\n- Holiday: If there is holiday on the given Date, 1 : Yes, 0 : No\n\n- Discount: If discount is offered by store on the given Date, Yes\/ No\n\n- Orders: Number of Orders received by the Store on the given Day\n\n- Sales: Total Sale for the Store on the given Day","87b09056":"#### Model 5: XG Boost","bbef8390":"#### For ARIMA first thing we do is identify if the data is stationary or non \u2013 stationary. if data is non-stationary we will try to make them stationary then we will process further","c4ca7d75":"### Model 10: Model by grouping Store_id values together:","d3ad23a4":"### Problem Statement","18cf8114":"#### Checking Skewness and Kurtosis for Numerical Columns","2e9e2fcc":"#### Correlations:","0a7df7b5":"#### Checking Predictors Co-relation With each other","6b6bd5fc":"### Data Pre-processing","2c05d889":"#### Model 7 : Without Removing Columns that had Less corelation with target:","af73a6b7":"the above model gave worst score on test data ! hence , overfitting. my best bet until now is removing less correlational columns and predicting the orders forst than predicting sales gave almost equal results on test data as that of not considering order column"}}