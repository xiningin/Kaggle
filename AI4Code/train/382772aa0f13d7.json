{"cell_type":{"8612e8ba":"code","55fa2900":"code","d26d73c7":"code","97105d3b":"code","ae086399":"code","ebee2c4e":"code","225a364b":"code","91d6c35a":"code","8ad28f9f":"code","9c34947d":"code","535df42d":"code","8677c9eb":"markdown","9faa0a99":"markdown","005470fb":"markdown","33614148":"markdown","7489ccfb":"markdown","8eb7d72d":"markdown","89f17b84":"markdown","ce5093a9":"markdown","233b4e27":"markdown","b66bacae":"markdown"},"source":{"8612e8ba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        if(filename==\"nasa.csv\"):\n            print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","55fa2900":"data = pd.read_csv(\"\/kaggle\/input\/nasa-asteroids-classification\/nasa.csv\")\ndata.columns = [c.replace(' ', '_') for c in data.columns]\ndata.head()","d26d73c7":"data.drop([\"Neo_Reference_ID\",\"Name\",\"Close_Approach_Date\",\"Epoch_Date_Close_Approach\"\n           ,\"Orbiting_Body\",\"Orbit_Determination_Date\",\"Equinox\"],axis=1,inplace=True)\ndata.Hazardous = [1 if each==True else 0 for each in data.Hazardous]","97105d3b":"from sklearn.preprocessing import MinMaxScaler\nscale = MinMaxScaler()\n\ny = data.Hazardous.values.reshape(-1,1)\nx = data.drop([\"Hazardous\"],axis=1).values #returns a numpy array\nx = scale.fit_transform(x)","ae086399":"c = data.groupby('Hazardous')\na = c.get_group(0) # safe \nb = c.get_group(1) # hazardous\n\na1, a2 = a.Mean_Anomaly , a.Minimum_Orbit_Intersection\nb1, b2 = b.Mean_Anomaly , b.Minimum_Orbit_Intersection","ebee2c4e":"plt.figure(figsize=(6,6))\nplt.scatter(a.Mean_Anomaly, a.Minimum_Orbit_Intersection,color=\"b\",label=\"Safe\",alpha=0.1)\nplt.scatter(b.Mean_Anomaly,b.Minimum_Orbit_Intersection,color=\"r\", label =\"Hazardous\",alpha=0.1)\nplt.legend()\nplt.show()\nplt.figure(figsize=(6,6))\nplt.scatter(a.Absolute_Magnitude,a.Asc_Node_Longitude,color=\"b\",label=\"Safe\",alpha=0.1)\nplt.scatter(b.Absolute_Magnitude,b.Asc_Node_Longitude,color=\"r\", label =\"Hazardous\",alpha=0.1)","225a364b":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=0)","91d6c35a":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nlr.fit(x_train,y_train.ravel())\nprint(\"Logistic Regression Acc : \", lr.score(x_test,y_test))","8ad28f9f":"from sklearn.svm import SVC\nsvc = SVC()\nsvc.fit(x_train,y_train.ravel())\nprint(\"SVM Acc : \",svc.score(x_test,y_test))","9c34947d":"from sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(x_train,y_train.ravel())\nprint(\"Decision Tree Acc : \", dtc.score(x_test,y_test))","535df42d":"plt.figure(figsize=(15,15), dpi=100)\n\ntree.plot_tree(dtc,\n              feature_names = data.columns,\n              rounded = True,\n              filled = True,\n               class_names = [\"Safe\",\"Hazardaus\"],\n              impurity = True)\nplt.savefig(\"tree.png\")","8677c9eb":"### Before training i will split train and test data.","9faa0a99":"### First of all , I will import nasa.csv then I will replace spaces in columns with '_' to work easly.","005470fb":"### We are ready for training.Let's start with logistic regression.","33614148":"### As you can see logistic regression was good but we can do better.","7489ccfb":"### I got %99.36 accuracy from decision tree because like I said before this data was too complicated for us to draw a seperating curve.However , with decision tree , we can identify the important features and classify by them.","8eb7d72d":"### It looks like my data is very close for drawing lines to seperate them.So, probably a decision tree or random forest classifier will be more efficient but I will use Logistic Regression , SVM, Decision tree and compare them.","89f17b84":"### Now i will split my data for visualization purposes.","ce5093a9":"### Now i will scale my data in order not to effected so much by outliers.","233b4e27":"### There are so much unnecesarry data for me so I will delete non usefull columns.Then I need to convert my categorial data. \n * Hazardous 1\n * Safe 0","b66bacae":"### Now let's visualize our tree and see what happens."}}