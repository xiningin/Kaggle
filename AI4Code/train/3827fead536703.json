{"cell_type":{"23320f01":"code","99bbe593":"code","0b10a689":"code","7289c4cc":"code","def9000f":"code","e998b1f1":"code","1f2411be":"code","dd828685":"code","a8823bb3":"code","bf7c6949":"code","902a879a":"code","68b99b69":"code","3a404c8c":"code","12f0c965":"code","ab42dc10":"code","602e490c":"code","10800c12":"code","f832136d":"code","2e9048ec":"code","caf25514":"code","42a509d5":"code","3cbaeab6":"code","267e647c":"code","eb08d86e":"code","058a19ba":"code","b4b8f3db":"code","242b6d47":"code","2bbfd99e":"code","49678a10":"code","322ddf50":"code","1472b7e7":"code","1a538403":"code","5200da97":"code","df353150":"code","25b277e7":"code","1eced828":"code","b8234ee1":"code","69cba94a":"code","9680ff90":"code","53ce131b":"code","a5f0fe57":"code","0124a3b3":"code","42dcf82a":"code","e87b5bf0":"code","719b10eb":"code","1e6f4ce1":"code","5d5f79cb":"code","4b92ff75":"code","ebce2696":"code","d5d24a33":"code","28061509":"code","591d7b38":"code","fde66e92":"code","09aca602":"code","6c0a8b2b":"code","0c41976a":"code","c80d1cd8":"code","9987ed81":"code","66868ed0":"code","c2bbd72f":"code","22914df3":"code","9d6ed5de":"code","0c32fafb":"code","7b664d39":"code","8aff23e7":"code","24dbe969":"code","6b4dad1d":"code","bae1e496":"code","f0ef8902":"code","ba52b9b3":"code","562c58c4":"code","3e69caae":"code","066a25ae":"markdown","d4777dea":"markdown","c8973293":"markdown","85cb5455":"markdown","222cdbce":"markdown","e6c20c87":"markdown","867eaa1d":"markdown","8aa9d26e":"markdown","2b86ce63":"markdown","12a5c580":"markdown","b5855a2a":"markdown","4491084d":"markdown","3e2eabf9":"markdown","41f690ae":"markdown","500dbf20":"markdown","c688bac7":"markdown","003d9221":"markdown","f166bd9e":"markdown","dead5e28":"markdown","0fa30f65":"markdown","8f693bcd":"markdown","aa5467eb":"markdown","9e380d62":"markdown","984e8ba6":"markdown","0d8f673f":"markdown","5e520762":"markdown","8eba6fab":"markdown","b0fc6579":"markdown","4327c65a":"markdown","bf2e815e":"markdown","364f65dc":"markdown","4762a6ca":"markdown","f8954761":"markdown","4de15e1d":"markdown","e5d2bbea":"markdown","49057716":"markdown","a88cbbe4":"markdown","7ae5444b":"markdown","c2c77a08":"markdown","ef67e64c":"markdown","a50c3092":"markdown","0c454363":"markdown","a51e15b8":"markdown","d5ae18e5":"markdown","7112e42b":"markdown","3724ea73":"markdown","e6e71db3":"markdown","3aa9b63d":"markdown","d3427fd1":"markdown","4a26e84e":"markdown","88f7b12d":"markdown","fcf8ea48":"markdown","8d7ee01e":"markdown","aa61f2a9":"markdown","951dd573":"markdown","e66ecd55":"markdown","e028b2ae":"markdown","72f79fce":"markdown","48e4f7dd":"markdown","4fe6b8e8":"markdown","2880b853":"markdown","284e7980":"markdown","79cb379a":"markdown","78c9e698":"markdown","57536d08":"markdown","ab54935d":"markdown","1702f3be":"markdown","4392789a":"markdown","1756e033":"markdown","10203d51":"markdown","25f286b9":"markdown","4c7f5dde":"markdown","1c52c75a":"markdown","55900566":"markdown","ce76c513":"markdown","03fb9b05":"markdown"},"source":{"23320f01":"# Importamos las librerias que vamos a utilizar\n# y comprobamos que Tensorflow reconoce la GPU.\n\nimport numpy as np\nimport pandas as pd\nimport os\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\nimport seaborn as sns\n\nimport PIL\nimport PIL.Image\nimport cv2\n\nimport h5py\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import model\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.utils import plot_model\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.utils import resample\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport shutil\nimport itertools\n\nGPU=True\nif GPU:\n    device_name = tf.test.gpu_device_name()\n    if device_name != '\/device:GPU:0':\n        raise SystemError('GPU device not found')\n    print('Found GPU at: {}'.format(device_name))","99bbe593":"seed = 3009\nbatch_size = 16 # Como tenemos un dataset tan grande, para que quepa en la memoria de\n                # nuestra GPU, debemos reducir el batch size.\ninput_shape = (224, 224)\nepochs = 30\nlearning_rate = 0.0001\n\npatience = 2\npatience_learning = 1\nfactor = 0.5 # Esto quiere decir que reduciremos nuestro learning rate x 0.5 cuando no se reduzca el error del modelo","0b10a689":"train_ds = pd.read_csv(\"..\/input\/covidx-cxr2\/train.txt\", sep = \" \", header = None)\n\n# Directorio de las im\u00e1genes de entrenamiento:\ntrain_dir = os.listdir(\"..\/input\/covidx-cxr2\/train\")\ntrain_ds.head(10)","7289c4cc":"train_ds.columns=['Id Paciente', 'Imagen', 'Diagnostico', 'Fuente']\n\ntest_ds = pd.read_csv(\"..\/input\/covidx-cxr2\/test.txt\", sep = \" \", header = None,\n                       names = ['Id Paciente', 'Imagen', 'Diagnostico', 'Fuente'])\n\n# Directorio de las im\u00e1genes de test:\ntest_dir = os.listdir(\"..\/input\/covidx-cxr2\/test\")\n\ntest_ds.tail()","def9000f":"train_ds.drop(['Id Paciente', 'Fuente'], axis = 1, inplace=True)\ntrain_ds.head(2)","e998b1f1":"test_ds.drop(['Id Paciente', 'Fuente'], axis = 1, inplace=True)\ntest_ds.head(2)","1f2411be":"print(len(train_ds)-len(train_dir), len(test_ds)-len(test_dir))","dd828685":"train_ds[\"Diagnostico\"].value_counts()","a8823bb3":"fig = plt.figure(figsize = (8, 5))\n\ndiagnosticos = list(train_ds[\"Diagnostico\"].unique())\nnegativos = len(train_ds[train_ds[\"Diagnostico\"] == 'negative']) \npositivos = len(train_ds[train_ds[\"Diagnostico\"] == 'positive'])\ncasos = list([negativos, positivos])\n\nplt.bar(diagnosticos, casos, color = 'lightblue', width = 0.4)\n \nplt.xlabel(\"Diagnositco\")\nplt.ylabel(\"No. de Diagnosticos\")\nplt.title(\"Distribuci\u00f3n de set de entrenamiento\")\nplt.show()","bf7c6949":"test_ds[\"Diagnostico\"].value_counts()","902a879a":"fig_test = plt.figure(figsize = (8, 5))\n\ndiagnosticos_test = list(test_ds[\"Diagnostico\"].unique())\nnegativos_test = len(test_ds[test_ds[\"Diagnostico\"] == 'negative']) \npositivos_test = len(test_ds[test_ds[\"Diagnostico\"] == 'positive'])\ncasos_test = list([negativos_test, positivos_test])\n\nplt.bar(diagnosticos_test, casos_test, color = 'lightgreen', width = 0.4)\n \nplt.xlabel(\"Diagnositco\")\nplt.ylabel(\"No. de Diagnosticos\")\nplt.title(\"Distribuci\u00f3n de set de entrenamiento\")\nplt.show()","68b99b69":"png_images = len(train_ds.loc[train_ds['Imagen'].str.contains(\".png\", case=False)])\njpg_images = len(train_ds.loc[train_ds['Imagen'].str.contains(\".jpg\", case=False)])\njpeg_images = len(train_ds.loc[train_ds['Imagen'].str.contains(\".jpeg\", case=False)])","3a404c8c":"print(png_images, jpg_images, jpeg_images)","12f0c965":"len(train_ds) - (png_images+jpg_images+jpeg_images)\n\n# No queda otro tipo de extensi\u00f3n en nuestro dataset","ab42dc10":"img_dir = \"..\/input\/covidx-cxr2\/train\/\"\n\nplt.figure(figsize=(16, 16))\n# Mostraremos solamente 9 im\u00e1genes en un formato de 3x3\n\nfor i in range(0,9):\n    ax = plt.subplot(3, 3, i + 1)\n    image = train_dir[i] \n    plt.imshow(plt.imread(img_dir + image))\n    \n    # A\u00f1adimos el diagn\u00f3stico del paciente como t\u00edtulo de la imagen.\n    label = list(train_ds.Diagnostico[train_ds.Imagen == image])\n    label = label[0]\n    plt.xlabel(label, fontsize=12)","602e490c":"positive_images = []\n\nplt.figure(figsize=(16, 16))\n# Mostraremos solamente 9 im\u00e1genes en un formato de 3x3\n\nfor i in range(0,9):\n    ax = plt.subplot(3, 3, i + 1)\n\n    # Vamos a forzar que solo aparezcan casos positivos\n    a = train_ds.Imagen[train_ds.Diagnostico == 'positive']\n    [positive_images.append(x) for x in a]\n    plt.imshow(plt.imread(img_dir + positive_images[i]))\n    \n    # A\u00f1adimos el diagn\u00f3stico del paciente como t\u00edtulo de la imagen.\n    label = list(train_ds.Diagnostico[train_ds.Imagen == positive_images[i]])\n    label = label[0]\n    plt.xlabel(label, fontsize=12)","10800c12":"train_df_A, valid_df_A = train_test_split(train_ds, test_size = 0.2, \n                                      stratify = train_ds[\"Diagnostico\"], \n                                      shuffle=True, random_state = seed)","f832136d":"train_df_A.Diagnostico.value_counts()","2e9048ec":"valid_df_A.Diagnostico.value_counts()","caf25514":"original_prop = (len(train_ds[train_ds[\"Diagnostico\"] == 'negative'])\/\n                 len(train_ds[train_ds[\"Diagnostico\"] == 'positive']))\noriginal_prop","42a509d5":"train_prop = (len(train_df_A[train_df_A[\"Diagnostico\"] == 'negative'])\/\n              len(train_df_A[train_df_A[\"Diagnostico\"] == 'positive']))\n\nvalid_prop = (len(valid_df_A[valid_df_A[\"Diagnostico\"] == 'negative'])\/\n              len(valid_df_A[valid_df_A[\"Diagnostico\"] == 'positive']))\n\nprint(train_prop, valid_prop)","3cbaeab6":"prep_train_layer = ImageDataGenerator(rescale = 1.\/255., \n                                      zoom_range = 0.1, \n                                      horizontal_flip = True)\n\n\nnorm_layer = ImageDataGenerator(rescale = 1.\/255.)","267e647c":"dir_train_valid = '..\/input\/covidx-cxr2\/train\/'\ndir_test = '..\/input\/covidx-cxr2\/test\/'\n\n# Set de Entrenamiento:\ntrain_tf_A = prep_train_layer.flow_from_dataframe(dataframe = train_df_A, \n                                          directory = dir_train_valid, \n                                          x_col = 'Imagen', \n                                          y_col = 'Diagnostico', \n                                          target_size = input_shape, \n                                          batch_size = batch_size, \n                                          class_mode = 'categorical',\n                                          seed = seed)\n\n# Set de Validaci\u00f3n:\nvalid_tf_A = norm_layer.flow_from_dataframe(dataframe = valid_df_A, \n                                          directory = dir_train_valid, \n                                          x_col = 'Imagen', \n                                          y_col = 'Diagnostico', \n                                          target_size = input_shape, \n                                          batch_size = batch_size, \n                                          class_mode = 'categorical',\n                                          seed = seed)\n\n# Set de Test:\ntest_tf_A = norm_layer.flow_from_dataframe(dataframe = test_ds, \n                                          directory = dir_test, \n                                          x_col = 'Imagen', \n                                          y_col = 'Diagnostico', \n                                          target_size = input_shape, \n                                          batch_size = batch_size, \n                                          class_mode = 'categorical',\n                                          seed = seed)","eb08d86e":"train_neg  = train_ds[train_ds['Diagnostico']=='negative']   \ntrain_pos = train_ds[train_ds['Diagnostico']=='positive'] \n\nprint(len(train_pos), len(train_neg))","058a19ba":"train_df_B = resample(train_neg, replace = True, n_samples = len(train_pos))\ntrain_df_B = pd.concat([train_pos, train_df_B])\n\ntrain_df_B.Diagnostico.value_counts()","b4b8f3db":"dir_train_valid = '..\/input\/covidx-cxr2\/train\/'\ndir_test = '..\/input\/covidx-cxr2\/test\/'\n\ntrain_df_B, valid_df_B = train_test_split(train_df_B, test_size = 0.1,  \n                                      shuffle=True, random_state = seed)\n\nprint(\"Set de Entrenamiento:\")\nprint(train_df_B.Diagnostico.value_counts())\nprint(\"\\n\")\nprint(\"Set de Validaci\u00f3n:\")\nprint(valid_df_B.Diagnostico.value_counts())\nprint(\"\\n\")\nprint(\"Set de Test:\")\nprint(test_ds.Diagnostico.value_counts())\nprint(\"\\n\")\n# Reutilizamos la capa de preprocesamiento del apartado anterior.\n\n# Set de Entrenamiento:\ntrain_tf_B = prep_train_layer.flow_from_dataframe(dataframe = train_df_B, \n                                                  directory = dir_train_valid, \n                                                  x_col = 'Imagen', \n                                                  y_col = 'Diagnostico', \n                                                  target_size = input_shape, \n                                                  batch_size = batch_size, \n                                                  class_mode = 'categorical',\n                                                  seed = seed)\n\n# Set de Validaci\u00f3n:\nvalid_tf_B = norm_layer.flow_from_dataframe(dataframe = valid_df_B, \n                                            directory = dir_train_valid, \n                                            x_col = 'Imagen', \n                                            y_col = 'Diagnostico', \n                                            target_size = input_shape, \n                                            batch_size = batch_size, \n                                            class_mode = 'categorical',\n                                            seed = seed)\n\n# Set de Test:\ntest_tf_B = norm_layer.flow_from_dataframe(dataframe = test_ds, \n                                           directory = dir_test, \n                                           x_col = 'Imagen', \n                                           y_col = 'Diagnostico', \n                                           target_size = input_shape, \n                                           batch_size = batch_size, \n                                           class_mode = 'categorical',\n                                           shuffle = False,\n                                           seed = seed)","242b6d47":"def show_loss_accuracy_evolution(history):\n    \n    hist = pd.DataFrame(history.history)\n    hist['epoch'] = history.epoch\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Binary Crossentropy')\n    ax1.plot(hist['epoch'], hist['loss'], label='Train Error')\n    ax1.plot(hist['epoch'], hist['val_loss'], label = 'Val Error')\n    ax1.grid()\n    ax1.legend()\n\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Accuracy')\n    ax2.plot(hist['epoch'], hist['accuracy'], label='Train Accuracy')\n    ax2.plot(hist['epoch'], hist['val_accuracy'], label = 'Val Accuracy')\n    ax2.grid()\n    ax2.legend()\n\n    plt.show()","2bbfd99e":"steps_per_epoch = np.ceil(len(train_df_A)\/batch_size)\nvalidation_steps = np.ceil(len(valid_df_A)\/batch_size)","49678a10":"steps_per_epoch_B = np.ceil(len(train_df_B)\/batch_size)\nvalidation_steps_B = np.ceil(len(valid_df_B)\/batch_size)","322ddf50":"inputs = tf.keras.Input(shape=input_shape + (3, ), name='inputs')\n\n# Conv Layer 1\nconv_1 = layers.Conv2D(32, kernel_size = (3, 3), padding = 'valid', activation = 'relu', name = 'conv_1')(inputs)\nzero = keras.layers.ZeroPadding2D(padding=(1, 1), data_format=None)(conv_1)\n# Conv Layer 2\nconv_2 = layers.Conv2D(32, kernel_size = (3, 3), padding = 'valid', activation = 'relu', name = 'conv_2')(zero)\n# Conv Layer 3\nconv_3 = layers.Conv2D(32, kernel_size = (3, 3), padding = 'valid', activation = 'relu', name = 'conv_3')(conv_2)\npool_1 = layers.MaxPooling2D(pool_size = (2, 2), padding = 'valid',name = 'pool_1')(conv_3)\nnorm1 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(pool_1)\n\n# Conv Layer 4\nconv_4 = layers.Conv2D(64, kernel_size = (3, 3), padding = 'valid', activation = 'relu', name = 'conv_4')(zero)\n# Conv Layer 5\nconv_5 = layers.Conv2D(64, kernel_size = (3, 3), padding = 'valid', activation = 'relu', name = 'conv_5')(norm1)\n# Conv Layer 6\nconv_6 = layers.Conv2D(64, kernel_size = (3, 3), padding = 'valid', activation = 'relu', name = 'conv_6')(conv_5)\npool_2 = layers.MaxPooling2D(pool_size = (2, 2), padding = 'valid',name = 'pool_2')(conv_6)\ndrop_1 = layers.Dropout(0.3, seed = seed)(pool_2)\n\n# Conv Layer 7\nconv_7 = layers.Conv2D(128, kernel_size = (3, 3), padding = 'same', activation = 'relu', name = 'conv_7')(drop_1)\n# Conv Layer 8\nconv_8 = layers.Conv2D(128, kernel_size = (3, 3), padding = 'same', activation = 'relu', name = 'conv_8')(conv_7)\n# Conv Layer 9\nconv_9 = layers.Conv2D(128, kernel_size = (3, 3), padding = 'same', activation = 'relu', name = 'conv_9')(conv_8)\ndrop_2 = layers.Dropout(0.3, seed = seed)(conv_9)\npool_3 = layers.MaxPooling2D(pool_size = (2, 2), padding = 'same', name = 'pool_3')(drop_2)\n\n# Conv Layer 10\nconv_10 = layers.Conv2D(256, kernel_size = (3, 3), padding = 'same', activation = 'relu', name = 'conv_10')(drop_2)\n# Conv Layer 11\nconv_11 = layers.Conv2D(256, kernel_size = (3, 3), padding = 'same', activation = 'relu', name = 'conv_11')(drop_2)\n# Conv Layer 12\nconv_12 = layers.Conv2D(256, kernel_size = (3, 3), padding = 'same', activation = 'relu', name = 'conv_12')(conv_11)\npool_4 = layers.MaxPooling2D(pool_size = (2, 2), padding = 'same', name = 'pool_4')(conv_12)\nnorm2 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(pool_4)\n\n# Conv Layer 13\nconv_13 = layers.Conv2D(384, kernel_size = (3, 3), padding = 'same', activation = 'relu', name = 'conv_13')(norm2)\n# Conv Layer 14\nconv_14 = layers.Conv2D(384, kernel_size = (3, 3), padding = 'same', activation = 'relu', name = 'conv_14')(conv_13)\n# Conv Layer 15\nconv_15 = layers.Conv2D(384, kernel_size = (3, 3), padding = 'same', activation = 'relu', name = 'conv_15')(conv_14)\ndrop_3 = layers.Dropout(0.3, seed = seed)(conv_15)\npool_5 = layers.MaxPooling2D(pool_size = (2, 2), padding = 'same', name = 'pool_5')(drop_3)\n\n# Flattening layer\nflat = layers.Flatten(name = 'flatten')(pool_5)\ndense1 = layers.Dense(256, activation = 'relu', name = 'dense1')(flat)\ndrop_5 = layers.Dropout(0.3, seed = seed)(dense1)\ndense2 = layers.Dense(128, activation = 'relu', name = 'dense2')(drop_5)\n\n# Fully-connected\noutputs = layers.Dense(2, activation = 'softmax', name = 'output')(dense2)\nmodelA = keras.Model(inputs = inputs, outputs = outputs, name = 'Modelo_A')\n\nmodelA.summary()","1472b7e7":"patience = 2\nfactor = 0.5 # Esto quiere decir que reduciremos nuestro learning rate x 0.5 cuando no se reduzca el error del modelo\n\nes_callback = [\n    tf.keras.callbacks.ModelCheckpoint(\"modelA.h5\", save_best_only = True, verbose = 0),\n    tf.keras.callbacks.EarlyStopping(patience=patience, monitor = 'val_loss', verbose = 1),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor = factor, patience = patience, verbose = 1)\n]","1a538403":"modelA.compile(\n    optimizer = Adam(lr = learning_rate, decay = 1e-6),\n    loss = 'binary_crossentropy',\n    metrics=['accuracy']\n)","5200da97":"history_A = modelA.fit(\n    train_tf_A,\n    validation_data = valid_tf_A,\n    epochs = epochs,\n    steps_per_epoch = steps_per_epoch,\n    validation_steps = validation_steps,\n    callbacks = es_callback\n)\n\nshow_loss_accuracy_evolution(history_A)","df353150":"inputs = tf.keras.Input(shape=input_shape + (3, ), name='inputs')\n\n# Conv Layer 1\nconv_1 = layers.Conv2D(32, kernel_size = (3, 3), padding = 'valid', activation = 'relu', name = 'conv_1')(inputs)\nzero = keras.layers.ZeroPadding2D(padding=(1, 1), data_format=None)(conv_1)\n# Conv Layer 2\nconv_2 = layers.Conv2D(32, kernel_size = (3, 3), padding = 'valid', activation = 'relu', name = 'conv_2')(zero)\n# Conv Layer 3\nconv_3 = layers.Conv2D(32, kernel_size = (3, 3), padding = 'valid', activation = 'relu', name = 'conv_3')(conv_2)\npool_1 = layers.MaxPooling2D(pool_size = (2, 2), padding = 'valid',name = 'pool_1')(conv_3)\nnorm1 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(pool_1)\n\n# Conv Layer 4\nconv_4 = layers.Conv2D(64, kernel_size = (3, 3), padding = 'valid', activation = 'relu', name = 'conv_4')(zero)\n# Conv Layer 5\nconv_5 = layers.Conv2D(64, kernel_size = (3, 3), padding = 'valid', activation = 'relu', name = 'conv_5')(norm1)\n# Conv Layer 6\nconv_6 = layers.Conv2D(64, kernel_size = (3, 3), padding = 'valid', activation = 'relu', name = 'conv_6')(conv_5)\npool_2 = layers.MaxPooling2D(pool_size = (2, 2), padding = 'valid',name = 'pool_2')(conv_6)\ndrop_1 = layers.Dropout(0.3, seed = seed)(pool_2)\n\n# Conv Layer 7\nconv_7 = layers.Conv2D(128, kernel_size = (3, 3), padding = 'same', activation = 'relu', name = 'conv_7')(drop_1)\n# Conv Layer 8\nconv_8 = layers.Conv2D(128, kernel_size = (3, 3), padding = 'same', activation = 'relu', name = 'conv_8')(conv_7)\n# Conv Layer 9\nconv_9 = layers.Conv2D(128, kernel_size = (3, 3), padding = 'same', activation = 'relu', name = 'conv_9')(conv_8)\ndrop_2 = layers.Dropout(0.3, seed = seed)(conv_9)\npool_3 = layers.MaxPooling2D(pool_size = (2, 2), padding = 'same', name = 'pool_3')(drop_2)\n\n# Conv Layer 10\nconv_10 = layers.Conv2D(256, kernel_size = (3, 3), padding = 'same', activation = 'relu', name = 'conv_10')(drop_2)\n# Conv Layer 11\nconv_11 = layers.Conv2D(256, kernel_size = (3, 3), padding = 'same', activation = 'relu', name = 'conv_11')(drop_2)\n# Conv Layer 12\nconv_12 = layers.Conv2D(256, kernel_size = (3, 3), padding = 'same', activation = 'relu', name = 'conv_12')(conv_11)\npool_4 = layers.MaxPooling2D(pool_size = (2, 2), padding = 'same', name = 'pool_4')(conv_12)\nnorm2 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(pool_4)\n\n# Conv Layer 13\nconv_13 = layers.Conv2D(384, kernel_size = (3, 3), padding = 'same', activation = 'relu', name = 'conv_13')(norm2)\n# Conv Layer 14\nconv_14 = layers.Conv2D(384, kernel_size = (3, 3), padding = 'same', activation = 'relu', name = 'conv_14')(conv_13)\n# Conv Layer 15\nconv_15 = layers.Conv2D(384, kernel_size = (3, 3), padding = 'same', activation = 'relu', name = 'conv_15')(conv_14)\ndrop_3 = layers.Dropout(0.3, seed = seed)(conv_15)\npool_5 = layers.MaxPooling2D(pool_size = (2, 2), padding = 'same', name = 'pool_5')(drop_3)\n\n# Flattening layer\nflat = layers.Flatten(name = 'flatten')(pool_5)\ndense1 = layers.Dense(256, activation = 'relu', name = 'dense1')(flat)\ndrop_5 = layers.Dropout(0.3, seed = seed)(dense1)\ndense2 = layers.Dense(128, activation = 'relu', name = 'dense2')(drop_5)\n\n# Fully-connected\noutputs = layers.Dense(2, activation = 'softmax', name = 'output')(dense2)\nmodelB = keras.Model(inputs = inputs, outputs = outputs, name = 'Modelo_B')","25b277e7":"patience = 2\npatience_learning = 1\nfactor = 0.5 # Esto quiere decir que reduciremos nuestro learning rate x 0.5 cuando no se reduzca el error del modelo\n\nes_callback = [\n    tf.keras.callbacks.ModelCheckpoint(\"modelB.h5\", save_best_only = True, verbose = 0),\n    tf.keras.callbacks.EarlyStopping(patience=patience, monitor = 'val_loss', verbose = 1),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor = factor, patience = patience_learning, verbose = 1)\n]","1eced828":"modelB.compile(\n    optimizer = Adam(lr = learning_rate, decay = 1e-6),\n    loss = 'binary_crossentropy',\n    metrics=['accuracy']\n)","b8234ee1":"history_B = modelB.fit(\n    train_tf_B,\n    validation_data = valid_tf_B,\n    epochs = epochs,\n    steps_per_epoch = steps_per_epoch_B,\n    validation_steps = validation_steps_B,\n    callbacks = es_callback\n)\n\nshow_loss_accuracy_evolution(history_B)","69cba94a":"prep_train_layer = ImageDataGenerator(\n    preprocessing_function = tf.keras.applications.xception.preprocess_input,  \n    zoom_range = 0.1, \n    horizontal_flip = True)\n\n\nnorm_layer = ImageDataGenerator(preprocessing_function = tf.keras.applications.xception.preprocess_input)\n\n# Set de Entrenamiento:\ntrain_tf_B = prep_train_layer.flow_from_dataframe(dataframe = train_df_B, \n                                                  directory = dir_train_valid, \n                                                  x_col = 'Imagen', \n                                                  y_col = 'Diagnostico', \n                                                  target_size = input_shape, \n                                                  batch_size = batch_size, \n                                                  class_mode = 'categorical',\n                                                  seed = seed)\n\n# Set de Validaci\u00f3n:\nvalid_tf_B = norm_layer.flow_from_dataframe(dataframe = valid_df_B, \n                                            directory = dir_train_valid, \n                                            x_col = 'Imagen', \n                                            y_col = 'Diagnostico', \n                                            target_size = input_shape, \n                                            batch_size = batch_size, \n                                            class_mode = 'categorical',\n                                            seed = seed)\n\n# Set de Test:\ntest_tf_Ex = norm_layer.flow_from_dataframe(dataframe = test_ds, \n                                           directory = dir_test, \n                                           x_col = 'Imagen', \n                                           y_col = 'Diagnostico', \n                                           target_size = input_shape, \n                                           batch_size = batch_size, \n                                           class_mode = 'categorical',\n                                           shuffle = False,\n                                           seed = seed)","9680ff90":"pretrained_xception = tf.keras.applications.Xception(input_shape = input_shape+(3,), \n                                                     weights = \"imagenet\",\n                                                     pooling = \"max\",\n                                                     include_top=False)\n\npretrained_xception.trainable = False","53ce131b":"x = pretrained_xception.output\n\n# Fully-connected\nnormal_1 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(x)\ndensa = layers.Dense(256, activation = 'relu', kernel_regularizer = regularizers.l2(l = 0.016),\n                     activity_regularizer=regularizers.l1(0.006), bias_regularizer=regularizers.l1(0.006),\n                     name = 'densa')(normal_1)\ndrop_1 = layers.Dropout(0.3, seed = seed)(densa)\n\noutputs = layers.Dense(2, activation = 'softmax', name = 'output')(drop_1)\n\nmodel_xception = keras.Model(inputs = pretrained_xception.input, outputs = outputs, name = 'Xception')\n\n\nes_callback = [\n    tf.keras.callbacks.ModelCheckpoint(\"Xception.h5\", save_best_only = True, verbose = 0),\n    tf.keras.callbacks.EarlyStopping(patience=patience, monitor = 'val_loss', verbose = 1),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor = factor, patience = patience_learning, verbose = 1)\n]\n\nmodel_xception.compile(\n    optimizer = Adam(lr = learning_rate, decay = 1e-6),\n    loss = 'binary_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory_xception = model_xception.fit(\n    train_tf_B,\n    validation_data = valid_tf_B,\n    epochs = epochs,\n    steps_per_epoch = steps_per_epoch_B,\n    validation_steps = validation_steps_B,\n    callbacks = es_callback\n)\n\nshow_loss_accuracy_evolution(history_xception)","a5f0fe57":"prep_train_layer = ImageDataGenerator(\n    preprocessing_function = tf.keras.applications.resnet.preprocess_input, \n    zoom_range = 0.1, \n    horizontal_flip = True)\n\nnorm_layer = ImageDataGenerator(preprocessing_function = tf.keras.applications.resnet.preprocess_input)\n\n# Set de Entrenamiento:\ntrain_tf_B = prep_train_layer.flow_from_dataframe(dataframe = train_df_B, \n                                                  directory = dir_train_valid, \n                                                  x_col = 'Imagen', \n                                                  y_col = 'Diagnostico', \n                                                  target_size = input_shape, \n                                                  batch_size = batch_size, \n                                                  class_mode = 'categorical',\n                                                  seed = seed)\n\n# Set de Validaci\u00f3n:\nvalid_tf_B = norm_layer.flow_from_dataframe(dataframe = valid_df_B, \n                                            directory = dir_train_valid, \n                                            x_col = 'Imagen', \n                                            y_col = 'Diagnostico', \n                                            target_size = input_shape, \n                                            batch_size = batch_size, \n                                            class_mode = 'categorical',\n                                            seed = seed)\n\n# Set de Test:\ntest_tf_Res = norm_layer.flow_from_dataframe(dataframe = test_ds, \n                                           directory = dir_test, \n                                           x_col = 'Imagen', \n                                           y_col = 'Diagnostico', \n                                           target_size = input_shape, \n                                           batch_size = batch_size, \n                                           class_mode = 'categorical',\n                                           shuffle = False,\n                                           seed = seed)","0124a3b3":"pretrained_resnet152 = tf.keras.applications.ResNet152V2(input_shape = input_shape+(3,), \n                                                         weights = \"imagenet\",\n                                                         pooling = \"max\",\n                                                         include_top=False)","42dcf82a":"x = pretrained_resnet152.output\n\n# Fully-connected\nnormal_1 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(x)\ndensa = layers.Dense(256, activation = 'relu', kernel_regularizer = regularizers.l2(l = 0.016),\n                     activity_regularizer=regularizers.l1(0.006), bias_regularizer=regularizers.l1(0.006),\n                     name = 'densa')(normal_1)\ndrop_1 = layers.Dropout(0.3, seed = seed)(densa)\n\noutputs = layers.Dense(2, activation = 'softmax', name = 'output')(drop_1)\n\nmodel_resnet152 = keras.Model(inputs = pretrained_resnet152.input, outputs = outputs, name = 'ResNet152V2')\n\n\nes_callback = [\n    tf.keras.callbacks.ModelCheckpoint(\"Resnet152.h5\", save_best_only = True, verbose = 0),\n    tf.keras.callbacks.EarlyStopping(patience=patience, monitor = 'val_loss', verbose = 1),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor = factor, patience = patience_learning, verbose = 1)\n]\n\nmodel_resnet152.compile(\n    optimizer = Adam(lr = learning_rate, decay = 1e-6),\n    loss = 'binary_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory_resnet152 = model_resnet152.fit(\n    train_tf_B,\n    validation_data = valid_tf_B,\n    epochs = epochs,\n    steps_per_epoch = steps_per_epoch_B,\n    validation_steps = validation_steps_B,\n    callbacks = es_callback\n)\n\nshow_loss_accuracy_evolution(history_resnet152)","e87b5bf0":"prep_train_layer = ImageDataGenerator(\n    preprocessing_function = tf.keras.applications.mobilenet_v2.preprocess_input, \n    zoom_range = 0.1, \n    horizontal_flip = True)\n\nnorm_layer = ImageDataGenerator(preprocessing_function = tf.keras.applications.mobilenet_v2.preprocess_input)\n\n# Set de Entrenamiento:\ntrain_tf_B = prep_train_layer.flow_from_dataframe(dataframe = train_df_B, \n                                                  directory = dir_train_valid, \n                                                  x_col = 'Imagen', \n                                                  y_col = 'Diagnostico', \n                                                  target_size = input_shape, \n                                                  batch_size = batch_size, \n                                                  class_mode = 'categorical',\n                                                  seed = seed)\n\n# Set de Validaci\u00f3n:\nvalid_tf_B = norm_layer.flow_from_dataframe(dataframe = valid_df_B, \n                                            directory = dir_train_valid, \n                                            x_col = 'Imagen', \n                                            y_col = 'Diagnostico', \n                                            target_size = input_shape, \n                                            batch_size = batch_size, \n                                            class_mode = 'categorical',\n                                            seed = seed)\n\n# Set de Test:\ntest_tf_Mob = norm_layer.flow_from_dataframe(dataframe = test_ds, \n                                           directory = dir_test, \n                                           x_col = 'Imagen', \n                                           y_col = 'Diagnostico', \n                                           target_size = input_shape, \n                                           batch_size = batch_size, \n                                           class_mode = 'categorical',\n                                           shuffle = False,  \n                                           seed = seed)","719b10eb":"pretrained_mobnet = tf.keras.applications.MobileNetV2(input_shape = input_shape+(3,), \n                                                      weights = \"imagenet\",\n                                                      pooling = \"max\",\n                                                      include_top=False)","1e6f4ce1":"x = pretrained_mobnet.output\n\n# Fully-connected\nnormal_1 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(x)\ndensa = layers.Dense(256, activation = 'relu', kernel_regularizer = regularizers.l2(l = 0.016),\n                     activity_regularizer=regularizers.l1(0.006), bias_regularizer=regularizers.l1(0.006),\n                     name = 'densa')(normal_1)\ndrop_1 = layers.Dropout(0.3, seed = seed)(densa)\n\noutputs = layers.Dense(2, activation = 'softmax', name = 'output')(drop_1)\n\nmodel_mobnet = keras.Model(inputs = pretrained_mobnet.input, outputs = outputs, name = 'MobileNetV2')\n\n\nes_callback = [\n    tf.keras.callbacks.ModelCheckpoint(\"Mobnet.h5\", save_best_only = True, verbose = 0),\n    tf.keras.callbacks.EarlyStopping(patience=patience, monitor = 'val_loss', verbose = 1),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor = factor, patience = patience, verbose = 1)\n]\n\nmodel_mobnet.compile(\n    optimizer = Adam(lr = learning_rate, decay = 1e-6),\n    loss = 'binary_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory_mobnet = model_mobnet.fit(\n    train_tf_B,\n    validation_data = valid_tf_B,\n    epochs = epochs,\n    steps_per_epoch = steps_per_epoch_B,\n    validation_steps = validation_steps_B,\n    callbacks = es_callback\n)\n\nshow_loss_accuracy_evolution(history_mobnet)","5d5f79cb":"prep_train_layer = ImageDataGenerator(\n    preprocessing_function = tf.keras.applications.densenet.preprocess_input, \n    zoom_range = 0.1, \n    horizontal_flip = True)\n\nnorm_layer = ImageDataGenerator(preprocessing_function = tf.keras.applications.densenet.preprocess_input)\n\n# Set de Entrenamiento:\ntrain_tf_B = prep_train_layer.flow_from_dataframe(dataframe = train_df_B, \n                                                  directory = dir_train_valid, \n                                                  x_col = 'Imagen', \n                                                  y_col = 'Diagnostico', \n                                                  target_size = input_shape, \n                                                  batch_size = batch_size, \n                                                  class_mode = 'categorical',\n                                                  seed = seed)\n\n# Set de Validaci\u00f3n:\nvalid_tf_B = norm_layer.flow_from_dataframe(dataframe = valid_df_B, \n                                            directory = dir_train_valid, \n                                            x_col = 'Imagen', \n                                            y_col = 'Diagnostico', \n                                            target_size = input_shape, \n                                            batch_size = batch_size, \n                                            class_mode = 'categorical',\n                                            seed = seed)\n\n# Set de Test:\ntest_tf_Den = norm_layer.flow_from_dataframe(dataframe = test_ds, \n                                           directory = dir_test, \n                                           x_col = 'Imagen', \n                                           y_col = 'Diagnostico', \n                                           target_size = input_shape, \n                                           batch_size = batch_size, \n                                           class_mode = 'categorical',\n                                           shuffle = False,\n                                           seed = seed)","4b92ff75":"pretrained_dense = tf.keras.applications.DenseNet201(input_shape = input_shape+(3,), \n                                                     weights = \"imagenet\",\n                                                     pooling = \"max\",\n                                                     include_top=False)","ebce2696":"x = pretrained_dense.output\n\n# Fully-connected\nnormal_1 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(x)\ndensa = layers.Dense(256, activation = 'relu', kernel_regularizer = regularizers.l2(l = 0.016),\n                     activity_regularizer=regularizers.l1(0.006), bias_regularizer=regularizers.l1(0.006),\n                     name = 'densa')(normal_1)\ndrop_1 = layers.Dropout(0.3, seed = seed)(densa)\n\noutputs = layers.Dense(2, activation = 'softmax', name = 'output')(drop_1)\n\nmodel_dense = keras.Model(inputs = pretrained_dense.input, outputs = outputs, name = 'DenseNet')\n\n\nes_callback = [\n    tf.keras.callbacks.ModelCheckpoint(\"DenseNet.h5\", save_best_only = True, verbose = 0),\n    tf.keras.callbacks.EarlyStopping(patience=patience, monitor = 'val_loss', verbose = 1),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor = factor, patience = patience_learning, verbose = 1)\n]\n\nmodel_dense.compile(\n    optimizer = Adam(lr = learning_rate, decay = 1e-6),\n    loss = 'binary_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory_dense = model_dense.fit(\n    train_tf_B,\n    validation_data = valid_tf_B,\n    epochs = epochs,\n    steps_per_epoch = steps_per_epoch_B,\n    validation_steps = validation_steps_B,\n    callbacks = es_callback\n)\n\nshow_loss_accuracy_evolution(history_dense)","d5d24a33":"prep_train_layer = ImageDataGenerator(\n    preprocessing_function = tf.keras.applications.vgg16.preprocess_input , \n    zoom_range = 0.1, \n    horizontal_flip = True)\n\nnorm_layer = ImageDataGenerator(preprocessing_function = tf.keras.applications.vgg16.preprocess_input )\n\n# Set de Entrenamiento:\ntrain_tf_B = prep_train_layer.flow_from_dataframe(dataframe = train_df_B, \n                                                  directory = dir_train_valid, \n                                                  x_col = 'Imagen', \n                                                  y_col = 'Diagnostico', \n                                                  target_size = input_shape, \n                                                  batch_size = batch_size, \n                                                  class_mode = 'categorical',\n                                                  seed = seed)\n\n# Set de Validaci\u00f3n:\nvalid_tf_B = norm_layer.flow_from_dataframe(dataframe = valid_df_B, \n                                            directory = dir_train_valid, \n                                            x_col = 'Imagen', \n                                            y_col = 'Diagnostico', \n                                            target_size = input_shape, \n                                            batch_size = batch_size, \n                                            class_mode = 'categorical',\n                                            seed = seed)\n\n# Set de Test:\ntest_tf_VG = norm_layer.flow_from_dataframe(dataframe = test_ds, \n                                           directory = dir_test, \n                                           x_col = 'Imagen', \n                                           y_col = 'Diagnostico', \n                                           target_size = input_shape, \n                                           batch_size = batch_size, \n                                           class_mode = 'categorical',\n                                           shuffle = False,\n                                           seed = seed)","28061509":"pretrained_vgg16 = tf.keras.applications.VGG16(input_shape = input_shape+(3,), \n                                               weights = \"imagenet\",\n                                               pooling = \"max\",\n                                               include_top=False)","591d7b38":"x = pretrained_vgg16.output\n# Fully-connected\nnormal_1 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(x)\ndensa = layers.Dense(256, activation = 'relu', kernel_regularizer = regularizers.l2(l = 0.016),\n                     activity_regularizer=regularizers.l1(0.006), bias_regularizer=regularizers.l1(0.006),\n                     name = 'densa')(normal_1)\ndrop_1 = layers.Dropout(0.3, seed = seed)(densa)\n\noutputs = layers.Dense(2, activation = 'softmax', name = 'output')(drop_1)\n\nmodel_vgg16 = keras.Model(inputs = pretrained_vgg16.input, outputs = outputs, name = 'Vgg16')\n\n\nes_callback = [\n    tf.keras.callbacks.ModelCheckpoint(\"Vgg16.h5\", save_best_only = True, verbose = 0),\n    tf.keras.callbacks.EarlyStopping(patience=patience, monitor = 'val_loss', verbose = 1),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor = factor, patience = patience_learning, verbose = 1)\n]\n\nmodel_vgg16.compile(\n    optimizer = Adam(lr = learning_rate, decay = 1e-6),\n    loss = 'binary_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory_vgg16 = model_vgg16.fit(\n    train_tf_B,\n    validation_data = valid_tf_B,\n    epochs = epochs,\n    steps_per_epoch = steps_per_epoch_B,\n    validation_steps = validation_steps_B,\n    callbacks = es_callback\n)\n\nshow_loss_accuracy_evolution(history_vgg16)","fde66e92":"prep_train_layer = ImageDataGenerator(\n    preprocessing_function = tf.keras.applications.inception_resnet_v2.preprocess_input, \n    zoom_range = 0.1, \n    horizontal_flip = True)\n\nnorm_layer = ImageDataGenerator(preprocessing_function = tf.keras.applications.inception_resnet_v2.preprocess_input)\n\n# Set de Entrenamiento:\ntrain_tf_B = prep_train_layer.flow_from_dataframe(dataframe = train_df_B, \n                                                  directory = dir_train_valid, \n                                                  x_col = 'Imagen', \n                                                  y_col = 'Diagnostico', \n                                                  target_size = input_shape, \n                                                  batch_size = batch_size, \n                                                  class_mode = 'categorical',\n                                                  seed = seed)\n\n# Set de Validaci\u00f3n:\nvalid_tf_B = norm_layer.flow_from_dataframe(dataframe = valid_df_B, \n                                            directory = dir_train_valid, \n                                            x_col = 'Imagen', \n                                            y_col = 'Diagnostico', \n                                            target_size = input_shape, \n                                            batch_size = batch_size, \n                                            class_mode = 'categorical',\n                                            seed = seed)\n\n# Set de Test:\ntest_tf_In = norm_layer.flow_from_dataframe(dataframe = test_ds, \n                                           directory = dir_test, \n                                           x_col = 'Imagen', \n                                           y_col = 'Diagnostico', \n                                           target_size = input_shape, \n                                           batch_size = batch_size, \n                                           class_mode = 'categorical',\n                                           shuffle = False,\n                                           seed = seed)","09aca602":"pretrained_inresv2 = tf.keras.applications.InceptionResNetV2(input_shape = input_shape+(3,), \n                                               weights = \"imagenet\",\n                                               pooling = \"max\",\n                                               include_top=False)","6c0a8b2b":"x = pretrained_inresv2.output\n\n# Fully-connected\nnormal_1 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(x)\ndensa = layers.Dense(256, activation = 'relu', kernel_regularizer = regularizers.l2(l = 0.016),\n                     activity_regularizer=regularizers.l1(0.006), bias_regularizer=regularizers.l1(0.006),\n                     name = 'densa')(normal_1)\ndrop_1 = layers.Dropout(0.3, seed = seed)(densa)\n\noutputs = layers.Dense(2, activation = 'softmax', name = 'output')(drop_1)\n\nmodel_inresv2 = keras.Model(inputs = pretrained_inresv2.input, outputs = outputs, name = 'InceptionResNetV2')\n\n\nes_callback = [\n    tf.keras.callbacks.ModelCheckpoint(\"InceptionResNetV2.h5\", save_best_only = True, verbose = 0),\n    tf.keras.callbacks.EarlyStopping(patience=patience, monitor = 'val_loss', verbose = 1),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor = factor, patience = patience_learning, verbose = 1)\n]\n\nmodel_inresv2.compile(\n    optimizer = Adam(lr = learning_rate, decay = 1e-6),\n    loss = 'binary_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory_inresv2 = model_inresv2.fit(\n    train_tf_B,\n    validation_data = valid_tf_B,\n    epochs = epochs,\n    steps_per_epoch = steps_per_epoch_B,\n    validation_steps = validation_steps_B,\n    callbacks = es_callback\n)\n\nshow_loss_accuracy_evolution(history_inresv2)","0c41976a":"prep_train_layer = ImageDataGenerator(\n    preprocessing_function = tf.keras.applications.efficientnet.preprocess_input, \n    zoom_range = 0.1, \n    horizontal_flip = True)\n\nnorm_layer = ImageDataGenerator(preprocessing_function = tf.keras.applications.efficientnet.preprocess_input)\n\n# Set de Entrenamiento:\ntrain_tf_B = prep_train_layer.flow_from_dataframe(dataframe = train_df_B, \n                                                  directory = dir_train_valid, \n                                                  x_col = 'Imagen', \n                                                  y_col = 'Diagnostico', \n                                                  target_size = input_shape, \n                                                  batch_size = batch_size, \n                                                  class_mode = 'categorical',\n                                                  seed = seed)\n\n# Set de Validaci\u00f3n:\nvalid_tf_B = norm_layer.flow_from_dataframe(dataframe = valid_df_B, \n                                            directory = dir_train_valid, \n                                            x_col = 'Imagen', \n                                            y_col = 'Diagnostico', \n                                            target_size = input_shape, \n                                            batch_size = batch_size, \n                                            class_mode = 'categorical',\n                                            seed = seed)\n\n# Set de Test:\ntest_tf_Eff = norm_layer.flow_from_dataframe(dataframe = test_ds, \n                                           directory = dir_test, \n                                           x_col = 'Imagen', \n                                           y_col = 'Diagnostico', \n                                           target_size = input_shape, \n                                           batch_size = batch_size, \n                                           class_mode = 'categorical',\n                                           shuffle = False,\n                                           seed = seed)","c80d1cd8":"pretrained_effb0 = tf.keras.applications.EfficientNetB0(input_shape = input_shape+(3,), \n                                               weights = \"imagenet\",\n                                               pooling = \"max\",\n                                               include_top=False)","9987ed81":"x = pretrained_effb0.output\n\n# Fully-connected\nnormal_1 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(x)\ndensa = layers.Dense(256, activation = 'relu', kernel_regularizer = regularizers.l2(l = 0.016),\n                     activity_regularizer=regularizers.l1(0.006), bias_regularizer=regularizers.l1(0.006),\n                     name = 'densa')(normal_1)\ndrop_1 = layers.Dropout(0.3, seed = seed)(densa)\n\noutputs = layers.Dense(2, activation = 'softmax', name = 'output')(drop_1)\n\nmodel_effb0 = keras.Model(inputs = pretrained_effb0.input, outputs = outputs, name = 'EfficientNetB0')\n\n\nes_callback = [\n    tf.keras.callbacks.ModelCheckpoint(\"EfficientNetB0.h5\", save_best_only = True, verbose = 0),\n    tf.keras.callbacks.EarlyStopping(patience=patience, monitor = 'val_loss', verbose = 1),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor = factor, patience = patience_learning, verbose = 1)\n]\n\nmodel_effb0.compile(\n    optimizer = Adam(lr = learning_rate, decay = 1e-6),\n    loss = 'binary_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory_effb0 = model_effb0.fit(\n    train_tf_B,\n    validation_data = valid_tf_B,\n    epochs = epochs,\n    steps_per_epoch = steps_per_epoch_B,\n    validation_steps = validation_steps_B,\n    callbacks = es_callback\n)\n\nshow_loss_accuracy_evolution(history_effb0)","66868ed0":"def print_info( test_gen, preds, print_code, save_dir, subject ):\n    class_dict=test_gen.class_indices\n    labels= test_gen.labels\n    file_names= test_gen.filenames \n    error_list=[]\n    true_class=[]\n    pred_class=[]\n    prob_list=[]\n    new_dict={}\n    error_indices=[]\n    y_pred=[]\n    for key,value in class_dict.items():\n        new_dict[value]=key             # dictionary {integer of class number: string of class name}\n    # store new_dict as a text fine in the save_dir\n    classes=list(new_dict.values())     # list of string of class names\n    dict_as_text=str(new_dict)\n    dict_name= subject + '-' +str(len(classes)) +'.txt'  \n    dict_path=os.path.join(save_dir,dict_name)    \n    with open(dict_path, 'w') as x_file:\n        x_file.write(dict_as_text)    \n    errors=0      \n    for i, p in enumerate(preds):\n        pred_index=np.argmax(p)        \n        true_index=labels[i]  # labels are integer values\n        if pred_index != true_index: # a misclassification has occurred\n            error_list.append(file_names[i])\n            true_class.append(new_dict[true_index])\n            pred_class.append(new_dict[pred_index])\n            prob_list.append(p[pred_index])\n            error_indices.append(true_index)            \n            errors=errors + 1\n        y_pred.append(pred_index)    \n    if print_code !=0:\n        if errors>0:\n            if print_code>errors:\n                r=errors\n            else:\n                r=print_code           \n            msg='{0:^28s}{1:^28s}{2:^28s}{3:^16s}'.format('Filename', 'Predicted Class' , 'True Class', 'Probability')\n            print_in_color(msg, (0,255,0),(55,65,80))\n            for i in range(r):\n                msg='{0:^28s}{1:^28s}{2:^28s}{3:4s}{4:^6.4f}'.format(error_list[i], pred_class[i],true_class[i], ' ', prob_list[i])\n                print_in_color(msg, (255,255,255), (55,65,60))\n                #print(error_list[i]  , pred_class[i], true_class[i], prob_list[i])               \n        else:\n            msg='With accuracy of 100 % there are no errors to print'\n            print_in_color(msg, (0,255,0),(55,65,80))\n    if errors>0:\n        plot_bar=[]\n        plot_class=[]\n        for  key, value in new_dict.items():        \n            count=error_indices.count(key) \n            if count!=0:\n                plot_bar.append(count) # list containg how many times a class c had an error\n                plot_class.append(value)   # stores the class \n        fig=plt.figure()\n        fig.set_figheight(len(plot_class)\/3)\n        fig.set_figwidth(10)\n        plt.style.use('fivethirtyeight')\n        for i in range(0, len(plot_class)):\n            c=plot_class[i]\n            x=plot_bar[i]\n            plt.barh(c, x, )\n            plt.title( ' Errores de Diagn\u00f3stico en el Test Set')\n    \n    if len(classes)<= 30:\n        # create a confusion matrix and a test report        \n        y_true= np.array(labels)        \n        y_pred=np.array(y_pred)        \n        cm = confusion_matrix(y_true, y_pred )\n        clr = classification_report(y_true, y_pred, target_names=classes)\n        length=len(classes)\n        if length<8:\n            fig_width=8\n            fig_height=8\n        else:\n            fig_width= int(length * .5)\n            fig_height= int(length * .5)\n        plt.figure(figsize=(fig_width, fig_height))\n        sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)       \n        plt.xticks(np.arange(length)+.5, classes, rotation= 90)\n        plt.yticks(np.arange(length)+.5, classes, rotation=0)\n        plt.xlabel(\"Predicci\u00f3n\")\n        plt.ylabel(\"Diagn\u00f3stico\")\n        plt.title(\"Matriz de Confusi\u00f3n\")\n        plt.show()    \n        print(\"Informe de Clasificaci\u00f3n:\\n----------------------\\n\", clr)","c2bbd72f":"val_loss, val_acc = model_resnet152.evaluate(test_tf_Res)\nprint('val_loss:', val_loss)\nprint('val_acc:', val_acc)","22914df3":"pred_res = model_resnet152.predict(test_tf_Res)","9d6ed5de":"save_dir=r'.\/'\nsubject='covid'\nprint_code=0\nprint_info(test_tf_Res, pred_res, print_code, save_dir, subject ) ","0c32fafb":"val_loss, val_acc = model_dense.evaluate(test_tf_Den)\nprint('val_loss:', val_loss)\nprint('val_acc:', val_acc)","7b664d39":"pred_den = model_dense.predict(test_tf_Den) ","8aff23e7":"save_dir=r'.\/'\nsubject='covid'\nprint_code=0\nprint_info(test_tf_Den, pred_den, print_code, save_dir, subject ) ","24dbe969":"val_loss, val_acc = model_effb0.evaluate(test_tf_Eff)\nprint('val_loss:', val_loss)\nprint('val_acc:', val_acc)","6b4dad1d":"pred_eff = model_effb0.predict(test_tf_Eff)","bae1e496":"save_dir=r'.\/'\nsubject='covid'\nprint_code=0\nprint_info(test_tf_Eff, pred_eff, print_code, save_dir, subject ) ","f0ef8902":"val_loss, val_acc = modelB.evaluate(test_tf_B)\nprint('val_loss:', val_loss)\nprint('val_acc:', val_acc)","ba52b9b3":"pred_B = modelB.predict(test_tf_B)","562c58c4":"save_dir=r'.\/'\nsubject='covid'\nprint_code=0\nprint_info(test_tf_B, pred_B, print_code, save_dir, subject ) ","3e69caae":"# Constantes para el modelo:\nseed = 3009\nbatch_size = 16 # Como tenemos un dataset tan grande, para que quepa en la memoria de\n                # nuestra GPU, debemos reducir el batch size.\ninput_shape = (224, 224)\nepochs = 30\nlearning_rate = 0.0001\n\npatience = 2\npatience_learning = 1\nfactor = 0.5 # Esto quiere decir que reduciremos nuestro learning rate x 0.5 cuando no se reduzca el error del modelo\n\n# Carga de los Datos:\ntrain_dir = os.listdir(\"..\/input\/covidx-cxr2\/train\")\ntrain_ds = pd.read_csv(\"..\/input\/covidx-cxr2\/train.txt\", sep = \" \", header = None)\ntrain_ds.columns=['Id Paciente', 'Imagen', 'Diagnostico', 'Fuente']\n\ntest_dir = os.listdir(\"..\/input\/covidx-cxr2\/test\")\ntest_ds = pd.read_csv(\"..\/input\/covidx-cxr2\/test.txt\", sep = \" \", header = None,\n                       names = ['Id Paciente', 'Imagen', 'Diagnostico', 'Fuente'])\n\ntrain_ds.drop(['Id Paciente', 'Fuente'], axis = 1, inplace=True)\ntest_ds.drop(['Id Paciente', 'Fuente'], axis = 1, inplace=True)\n\ndir_train_valid = '..\/input\/covidx-cxr2\/train\/'\ndir_test = '..\/input\/covidx-cxr2\/test\/'\n\n# Resample para el balanceo de las clases\ntrain_neg  = train_ds[train_ds['Diagnostico']=='negative']   \ntrain_pos = train_ds[train_ds['Diagnostico']=='positive'] \n\ntrain_df = resample(train_neg, replace = True, n_samples = len(train_pos))\ntrain_df = pd.concat([train_pos, train_df])\n\n# Train-Validation Split:\ntrain_df, valid_df = train_test_split(train_df, test_size = 0.1,  \n                                      shuffle=True, random_state = seed)\n\nsteps_per_epoch = np.ceil(len(train_df)\/batch_size)\nvalidation_steps = np.ceil(len(valid_df)\/batch_size)\n\n# Preprocesamiento antes del modelo\nprep_train_layer = ImageDataGenerator(\n    preprocessing_function = tf.keras.applications.densenet.preprocess_input, \n    zoom_range = 0.1, \n    horizontal_flip = True)\n\nnorm_layer = ImageDataGenerator(preprocessing_function = tf.keras.applications.densenet.preprocess_input)\n\n# Set de Entrenamiento:\ntrain_tf = prep_train_layer.flow_from_dataframe(dataframe = train_df, \n                                                  directory = dir_train_valid, \n                                                  x_col = 'Imagen', \n                                                  y_col = 'Diagnostico', \n                                                  target_size = input_shape, \n                                                  batch_size = batch_size, \n                                                  class_mode = 'categorical',\n                                                  seed = seed)\n\n# Set de Validaci\u00f3n:\nvalid_tf = norm_layer.flow_from_dataframe(dataframe = valid_df, \n                                            directory = dir_train_valid, \n                                            x_col = 'Imagen', \n                                            y_col = 'Diagnostico', \n                                            target_size = input_shape, \n                                            batch_size = batch_size, \n                                            class_mode = 'categorical',\n                                            seed = seed)\n\n# Set de Test:\ntest_tf = norm_layer.flow_from_dataframe(dataframe = test_ds, \n                                           directory = dir_test, \n                                           x_col = 'Imagen', \n                                           y_col = 'Diagnostico', \n                                           target_size = input_shape, \n                                           batch_size = batch_size, \n                                           class_mode = 'categorical',\n                                           shuffle = False,\n                                           seed = seed)\n# Modelo DesNet201:\npretrained_dense = tf.keras.applications.DenseNet201(input_shape = input_shape+(3,), \n                                                     weights = \"imagenet\",\n                                                     pooling = \"max\",\n                                                     include_top=False)\n\nx = pretrained_dense.output\n\n# Fully-connected\nnormal_1 = layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(x)\ndensa = layers.Dense(256, activation = 'relu', kernel_regularizer = regularizers.l2(l = 0.016),\n                     activity_regularizer=regularizers.l1(0.006), bias_regularizer=regularizers.l1(0.006),\n                     name = 'densa')(normal_1)\ndrop_1 = layers.Dropout(0.3, seed = seed)(densa)\n\noutputs = layers.Dense(2, activation = 'softmax', name = 'output')(drop_1)\n\nmodel_final = keras.Model(inputs = pretrained_dense.input, outputs = outputs, name = 'DenseNet')\n\nes_callback = [\n    tf.keras.callbacks.ModelCheckpoint(\"Modelo_Final.h5\", save_best_only = True, verbose = 0),\n    tf.keras.callbacks.EarlyStopping(patience=patience, monitor = 'val_loss', verbose = 0),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor = factor, patience = patience_learning, verbose = 0)\n]\n\nmodel_final.compile(\n    optimizer = Adam(lr = learning_rate, decay = 1e-6),\n    loss = 'binary_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory = model_final.fit(\n    train_tf,\n    validation_data = valid_tf,\n    epochs = epochs,\n    steps_per_epoch = steps_per_epoch,\n    validation_steps = validation_steps,\n    callbacks = es_callback,\n    verbose = 0\n)\n\nshow_loss_accuracy_evolution(history)\n\n# Evaluaci\u00f3n e Informe Final\nval_loss, val_acc = model_final.evaluate(test_tf, verbose = 0)\nprint('val_loss:', val_loss)\nprint('val_acc:', val_acc)\n\npredicciones = model_final.predict(test_tf)\n\nsave_dir=r'.\/'\nsubject='covid'\nprint_code=0\nprint_info(test_tf, predicciones, print_code, save_dir, subject)","066a25ae":"### 7.4 DenseNet201<a class=\"anchor\" id=\"DenseNet201\"><\/a>\n\nInformaci\u00f3n de su uso [aqu\u00ed](https:\/\/keras.io\/api\/applications\/densenet\/#densenet201-function). \n\nLink a la publicaci\u00f3n dr la arquitectura [aqu\u00ed](https:\/\/arxiv.org\/pdf\/1608.06993.pdf).","d4777dea":"## 6. Creaci\u00f3n del conjunto de Entrenamiento\/Validaci\u00f3n en TensorFlow<a class=\"anchor\" id=\"TestTrain\"><\/a>","c8973293":"#### 6.2.1 Creaci\u00f3n de los sets en TensorFlow <a class=\"anchor\" id=\"SetsB\"><\/a>","85cb5455":"#### 6.3.1 **Modelo A**: <a class=\"anchor\" id=\"RedA\"><\/a>","222cdbce":"Calculamos el n\u00famero de pasos por \u00e9poca para asegurarnos que utilizamos un subconjunto de datos del mismo tama\u00f1o tanto en el dataset de entrenamiento como en el de validaci\u00f3n, para mayor consistencia de los resultados, ya que podr\u00edamos ver como nuestro accuracy baja y sube seg\u00fan el batch que est\u00e9 analizando el modelo, pudiendo pensar erroneameten que est\u00e1 habiendo mayor error del que hay. De esta manera deber\u00edamos poder solventar este problema.","e6c20c87":"#### 5.3 Visualizaci\u00f3n de  im\u00e1genes<a class=\"anchor\" id=\"Visualizaci\u00f3n\"><\/a>","867eaa1d":"En este bloque estudiaremos diferentes arquitecturas disponibles en la API de Keras, tenemos que recordar que en el apartado 6 hab\u00edamos realizado un preprocesamiento a las im\u00e1genes, esto no ser\u00e1 posible para estas arquitecturas porque ya disponen de su propia funci\u00f3n de preprocesamiento ya que cada modelo espera las im\u00e1genes en un formato espec\u00edfico, por ejemplo, algunos esperan las im\u00e1genes con los p\u00edxeles entre 0 y 255, otros entres -1 y 1, otros con los canales RGB en GRB...\n\nPor tanto, antes de llamar a los modelos, aplicaremos sobre los datos los procesamientos correspondientes para cada modelo.","8aa9d26e":"### 8.4 Modelo Propio<a class=\"anchor\" id=\"EPropio\"><\/a>","2b86ce63":"Hallamos las predicciones","12a5c580":"Imprimimos el informe","b5855a2a":"Preparamos los datos para introducirlos en el modelo usando [image_dataset_from_directory](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/utils\/image_dataset_from_directory). Como en nuestros dataframes __\"train_df_A\", \"valid_df_A\" y \"test_ds\"__ tenemos el nombre de las im\u00e1genes y sus respectivos diagn\u00f3sticos, los usaremos como elemento input, pero a la hora de decir el directorio, utilizaremos el de la carpeta donde se encuentran las im\u00e1genes. Tenemos que recordar, que las im\u00e1genes del set de entrenamiento y validaci\u00f3n se encuentran en la misma carpeta, pero como acabamos de explicar, no es un problema ya que tenemos definidos los nombres de las imagenes en el archivo input.","4491084d":"Replicamos solo los pasos necesarios para obtener el modelo final, desde la carga de los datos hasta la visualizaci\u00f3n del informe de clasificaci\u00f3n.\n\nEliminaremos todas las opciones de verbose ya que solo queremos mostrar el resultado final, cualquier detalle del modelo, ya se ha mostrado en las celdas superiores.","3e2eabf9":"### 6.1 Modelo A <a class=\"anchor\" id=\"ModeloA\"><\/a>","41f690ae":"Este trabajo tiene como objetivo obtener un algoritmo de clasificaci\u00f3n de im\u00e1genes para detectar pacientes con Covid-19 a trav\u00e9s de radiograf\u00edas de torax.\n\nPara ello, utilizaremos la librer\u00eda TensorFlow para encontrar la mejor arquitectura de Redes Neuronales Convolucionales posible. Durante el desarrollo del trabajo, se har\u00e1 una exploraci\u00f3n de los datos, los formatos que presentan y los desajustes que existen por clase.\n\nUna vez que hayamos dividido nuestro dataset en los conjutos de entrenamiento, validaci\u00f3n y testeo, realizaremos un estudio comparativo entre varias arquitecturas propias y algunas de las mejores arquitecturas p\u00fablicas existenes.\nCon la mejor, finalizaremos el estudio, mostraremos las m\u00e9tricas y discutiremos si el resultado es suficientemente bueno para una aplicaci\u00f3n m\u00e9dica.","500dbf20":"Como hemos podido observar, trabajar con el dataset entero sin hacer resampling parece dar mejor resultado de entrada pero esto no es m\u00e1s que el modelo trabajando y validando la misma clase de datos una y otra vez, por tanto elegimos el procedimiento del **Modelo B** para seguir trabajando.\n\nEn los siguientes pasos utilizaremos varias de las arquitecturas preentrenadas m\u00e1s famosas que se encuentran en la p\u00e1gina de [Keras](https:\/\/keras.io\/api\/applications\/#available-models) y adem\u00e1s, intentaremos refinar la que hemos utilizado previamente para luego elegir la que mejor resultado otorgue y acabar el proyecto con ella.","c688bac7":"Como tenemos muchas m\u00e1s imagenes negativas que positivas, vamos a mostrar 9 imag\u00e9nes s\u00f3lo de positivos","003d9221":"#### 6.1.2 Creaci\u00f3n de los sets en TensorFlow <a class=\"anchor\" id=\"Sets\"><\/a>","f166bd9e":"### 8.5 Elecci\u00f3n de mejor Modelo<a class=\"anchor\" id=\"Eleccion\"><\/a>","dead5e28":"#### 5.2 Distribucion seg\u00fan el tipo de archivo<a class=\"anchor\" id=\"Archivo\"><\/a>","0fa30f65":"### 8.2 DenseNet201<a class=\"anchor\" id=\"EDenseNet201\"><\/a>","8f693bcd":"Imprimimos el informe","aa5467eb":"### 8.3 EfficientNetB0<a class=\"anchor\" id=\"EEfficientNetB0\"><\/a>","9e380d62":"### 7.7 EfficientNetB0<a class=\"anchor\" id=\"EfficientNetB0\"><\/a>\n\nInformaci\u00f3n de su uso [aqu\u00ed](https:\/\/keras.io\/api\/applications\/efficientnet\/#efficientnetb0-function). \n\nLink a la publicaci\u00f3n dr la arquitectura [aqu\u00ed](https:\/\/arxiv.org\/pdf\/1905.11946.pdf).","984e8ba6":"Evualamos el modelo","0d8f673f":"### 7.5 VGG16 <a class=\"anchor\" id=\"VGG16\"><\/a>\n\nInformaci\u00f3n de su uso [aqu\u00ed](https:\/\/keras.io\/api\/applications\/vgg\/#vgg16-function). \n\nLink a la publicaci\u00f3n dr la arquitectura [aqu\u00ed](https:\/\/arxiv.org\/pdf\/1409.1556.pdf).","5e520762":"### 3.1 Inputs del Modelo<a class=\"anchor\" id=\"Inputs\"><\/a>","8eba6fab":"De los modelos de los anteriores aquellos cuyo con mayor accuracy y menor error en los test de entrenamiento y validaci\u00f3n.\nEstos son:\n\n    - ResNet152V2\n    - DenseNet201\n    - EfficientNetB0","b0fc6579":"Preprocesamiento:","4327c65a":"### 8.1 ResNet152V2<a class=\"anchor\" id=\"EResNet152V2\"><\/a>","bf2e815e":"## 1. Introducci\u00f3n <a class=\"anchor\" id=\"Introducci\u00f3n\"><\/a>","364f65dc":"Hallamos las predicciones:","4762a6ca":"Evualamos el modelo:","f8954761":"Evualamos el modelo","4de15e1d":"### 7.3 MobileNetV2<a class=\"anchor\" id=\"MobileNetV2\"><\/a>\n\nInformaci\u00f3n de su uso [aqu\u00ed](https:\/\/keras.io\/api\/applications\/mobilenet\/#mobilenetv2-function). \n\nLink a la publicaci\u00f3n dr la arquitectura [aqu\u00ed](https:\/\/arxiv.org\/pdf\/1801.04381.pdf).","e5d2bbea":"Creamos una funci\u00f3n para obtener la evoluci\u00f3n de nuestro modelo, esta funci\u00f3n ha sido obtenida de los apuntes de la asigntura de Deep Learning, impartida por Alberto Ezpondaburu, y se puede encontrar en el siguiente enlace [Link](https:\/\/github.com\/MafCiv\/intro_deep_learning\/blob\/main\/class\/CNN\/Introduction_to_CNN.ipynb).","49057716":"#### 5.1 Distribuci\u00f3n del Dataset<a class=\"anchor\" id=\"Distribuci\u00f3n\"><\/a>","a88cbbe4":"## 9. Modelo Final<a class=\"anchor\" id=\"Final\"><\/a>","7ae5444b":"## Tabla de Contenidos\n\n* [1. Introducci\u00f3n](#Introducci\u00f3n)\n    * [Section 1.1](#section_1_1)\n    * [Section 1.2](sSection_1_2)\n        * [Section 1.2.1](#section_1_2_1)\n        * [Section 1.2.2](#section_1_2_2)\n        * [Section 1.2.3](#section_1_2_3)\n        \n* [2. Referencias](#Referencias)\n\n* [3. Librer\u00edas](#Librer\u00edas)\n    * [3.1 Inputs](#Inputs)\n    \n* [4. Carga de Datos](#Carga)\n\n* [5. Exploraci\u00f3n de los Datos](#Exploraci\u00f3n)\n    * [5.1 Distribuci\u00f3n del Dataset](#Distribuci\u00f3n)\n    * [5.2 Distribuci\u00f3n Seg\u00fan el Tipo de Archivo](#Archivo)\n    * [5.3 Visualizaci\u00f3n de Im\u00e1genes](#Visualizaci\u00f3n)\n    \n* [6. Creaci\u00f3n del conjunto de Entrenamiento\/Validaci\u00f3n en TensorFlow](#TestTrain)\n    * [6.1 Modelo A](#ModeloA)\n        * [6.1.1 Data Augmentation](#Augmentation)\n        * [6.1.2 Creaci\u00f3n de los sets en TensorFlow](#Sets)\n    * [6.2 Modelo B](#ModeloB)   \n        * [6.2.1 Creaci\u00f3n de los sets en TensorFlow](#SetsB)\n    * [6.3 Arquitectura CNN: Stratified vs Resample](#ModelosVS)\n        * [6.3.1 Modelo A](#RedA)\n        * [6.3.2 Modelo B](#RedB)\n    \n* [7. Modelos Preentrenados](#Preentrenados)\n    * [7.1 Xception](#Xception)\n    * [7.2 ResNet152V2](#ResNet152V2)\n    * [7.3 MobileNetV2](#MobileNetV2)\n    * [7.4 DenseNet201](#DenseNet201)\n    * [7.5 VGG16](#VGG16)\n    * [7.6 InceptionResNetV2](#InceptionResNetV2)\n    * [7.7 EfficientNetB0](#EfficientNetB0)\n    \n* [8. Evaluaci\u00f3n de los Modelos](#Evaluacion)\n    * [8.1 ResNet152V2](#EResNet152V2)\n    * [8.2 DenseNet201](#EDenseNet201)\n    * [8.3 EfficientNetB0](#EEfficientNetB0)\n    * [8.4 Modelo Propio](#EPropio)\n    * [8.5 Elecci\u00f3n de mejor Modelo](#Eleccion)\n    \n* [9. Modelo Final](#Final)\n\n* [10. Conclusiones](#Conclusiones)\n\n\n","c2c77a08":"## 7. Modelos Preentrenados <a class=\"anchor\" id=\"Preentrenados\"><\/a>","ef67e64c":"Como podemos ver en nuestro set de entramiento, tenemos las dos categor\u00edas muy descompensadas en favor de los diagn\u00f3sticos con resultado negativo, por tanto, tenemos varias soluciones:\n    \n   1.- Realizar un resample() con n = No. de casos negativos con lo que balancerar\u00edamos el dataset.\n    \n   2.- Una vez hacemos el [train_test_split](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html) de la librer\u00eda SkitLearn utilizamos la opci\u00f3n de stratify, \n    con los que dividiremos nuestras muestras  mantienendo la proporci\u00f3n de datos con diagn\u00f3sticos positivos y negativos.\n    \nCabe esperar que al tener un dataset tan desequilibrado, la mejor opci\u00f3n para realizar el estudio sea realizando el resample ya que de otra forma, al haber tanto casos \"negativos\" en comparaci\u00f3n a los \"positivos\", nuestro modelo de entremiento obtendr\u00eda muy buenos resultados detectando la clase mayoritaria pero muy malo detectando la minoritaria.\n\nEsto lo podremos observar en los siguientes apartados donde comparamos ambos modelos bajo una misma arquitectura muy sencilla, el modelo A (stratify) se adapta extra\u00f1amente bien a pesar de lo sencillo del modelo, mientras que el modelo B (resample) obtiene unos resultados m\u00e1s pobres, pero en concordancia con lo esperado al haber la misma cantidad de casos para ambas clases.","a50c3092":"### 6.3 Arquitectura CNN: Stratified vs Resample <a class=\"anchor\" id=\"ModelosVS\"><\/a>","0c454363":"Como hemos visto en las matrices de confusi\u00f3n el modelo __DesNet201__ casi clasifica perfectamente el dataset de reserva, por tanto ser\u00e1 el modelo que elijamos para construir nuestro modelo final.","a51e15b8":"Al estar compuesto nuesto dataset de im\u00e1genes cuyo nombre es el que aparece en la tabla de arriba, el Id del Paciente o la Fuente de donde se han obtenido es irrelevante para nuestro an\u00e1lisis, por tanto, nos deshacemos de ellas.","d5ae18e5":"## 8. Evaluaci\u00f3n de los Modelos<a class=\"anchor\" id=\"Evaluacion\"><\/a>","7112e42b":"####  6.3.2 **Modelo B**: <a class=\"anchor\" id=\"RedB\"><\/a>","3724ea73":"Imprimimos el informe:","e6e71db3":"Evualamos el modelo","3aa9b63d":"Hallamos las predicciones","d3427fd1":"Preprocesamiento:","4a26e84e":"Para poder introducir nuestras im\u00e1genes a la red, primero debemos crear un __Tensor__ con las im\u00e1genes y las categor\u00edas de estas. A la misma vez, somos capaces de decirle a TensorFlow que a cada imagen que cargue, le aplique un preprocesamiento por ejemplo, normalizar la imagen y aplicarle t\u00e9cnicas de Data Augmentation, con lo que podemos construir un modelo m\u00e1s robusto. Esta capa de preprocesamiento ser\u00e1 aplicada \u00fanicamente sobre los datos de entrenamiento, aunque lo que si tenemos que hacer a todos los sets es normalizar las imag\u00e9nes para que cada pixel se encuentre entre 0 y 1.\n\nPara el procesamiento utilizaremos [ImageDataGenerator](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/image\/ImageDataGenerator)","88f7b12d":"### 6.2 Modelo B <a class=\"anchor\" id=\"ModeloB\"><\/a>","fcf8ea48":"### 7.6 InceptionResNetV2<a class=\"anchor\" id=\"InceptionResNetV2\"><\/a>\n\nInformaci\u00f3n de su uso [aqu\u00ed](https:\/\/keras.io\/api\/applications\/inceptionresnetv2\/). \n\nLink a la publicaci\u00f3n dr la arquitectura [aqu\u00ed](https:\/\/arxiv.org\/pdf\/1602.07261.pdf).","8d7ee01e":"Preprocesamiento:","aa61f2a9":"El modelo __DesNet201__ s\u00f3lo ha clasificado una imagen mal, obteniendo un magn\u00edfico resultado.\n\nComo lecciones de aprendizaje durante el desarrollo de este trabajo podemos destacar algunas que el autor considera relevantes y que han supuesto los mayores progresos a la hora de obtener mejores resultados.\n\n    - Aunque exista la opci\u00f3n de stratify, no se puede aplicar en dataset cuya relaci\u00f3n de una clase a otra \n    es tan grande.\n    - Dentro de las t\u00e9cnicas para prevenir el overfitting, definir un decay y un learning rate bajo es clave \n    para que el modelo vaya aprendiendo sin dar grandes saltos, pero podemos mejorarlo a\u00fan m\u00e1s haciendo que el \n    learning rate sea din\u00e1mico como hemos hecho aqu\u00ed, al darle solo patience = 1 al callbacks cuando calcula el \n    error de validaci\u00f3n, reducimos el learning rate por la mitad, para que la pendiende de aprendizaje sea m\u00e1s \n    lenta, este concepto, es sin duda, el que mejor resultado nos ha permitido obtener.","951dd573":"Imprimimos el informe","e66ecd55":"Como hab\u00edamos mencionado en el apartado 5.1, nuestros datos no est\u00e1n balanceados por lo que a continuaci\u00f3n, optaremos por crear dos modelos diferentes:\n\n    - Modelo A: Se realizar\u00e1 un train_test_split con la funci\u00f3n \"stratify\" que permite mantener \n    las proporciones de las muestras seg\u00fan su clase.\n    \n    - Modelo B: En vez de utilizar la opci\u00f3n de \"stratify\", se realizar\u00e1 un resample con los datos \n    de menor represetanci\u00f3n como referencia para posteriormente crear los sets de Entrenamimento \n    y Validaci\u00f3n.\n    \nEstos dos modelos ser\u00e1n sometidos a la misma arquitectura CNN, como hemos explicado en el apartado 5.1, esperamos obtener unos resultados sorprendentemente buenos en el Modelo A pero que son err\u00f3neos, y unos resultados peores pero coherentes para el Modelo B.\n\nEn cuanto a la arquitectura que vamos a construir, a medida que aumentemos en la produndidad de la red, iremos aumentando el ancho del modelo, esto es a\u00f1adiendo m\u00e1s filtros, hasta llegar al output.","e028b2ae":"#### COVID-Net Open Initiative:\n\n[Link a la p\u00e1gina oficial: COVID-Net](https:\/\/alexswong.github.io\/COVID-Net\/)\n\n#### Publicaci\u00f3n\n\n[ABOUTALEBI, Hossein, et al. COVID-Net CXR-S: Deep Convolutional Neural Network for Severity Assessment of COVID-19 Cases from Chest X-ray Images. arXiv preprint arXiv:2105.00256, 2021.](https:\/\/arxiv.org\/ftp\/arxiv\/papers\/2105\/2105.00256.pdf)","72f79fce":"## 4. Carga de datos<a class=\"anchor\" id=\"Carga\"><\/a>","48e4f7dd":"Preprocesamiento:","4fe6b8e8":"Preprocesamiento:","2880b853":"#### 6.1.1 Data Augmentation <a class=\"anchor\" id=\"Augmentation\"><\/a>","284e7980":"Comprobamos que tenemos el mismo n\u00famero de entradas en nuestro dataframe como im\u00e1genes en la carpeta que contiene todas las radriograf\u00edas.","79cb379a":"Para imprimir la matriz de confusi\u00f3n y el informe de clasificaci\u00f3n hemos optado por usar el c\u00f3dido de otro usuario de Kaggle que lo tiene automatizado en una funci\u00f3n, aunque hay algunas funciones que no utilizamos. El enlace al usuario es el siguiente [gpiosenka](https:\/\/www.kaggle.com\/gpiosenka\/code).\n\nEn el siguiente apartado, evaluaremos cada modelo con el dataset de test, y calcularemos sus predicciones para por \u00faltimo, sacar las m\u00e9tricas para as\u00ed elegir el modelo ganador.","78c9e698":"Podemos observar que no tenemos nombres en las columnas, y que las im\u00e1genes se encuentran en diferentes formatos, en este fragmento de nuestra tabla obervamos hasta tres formatos distintos \".png, .jpg y .jpeg\".\n\nA continuaci\u00f3n vamos a cargar los datos de validaci\u00f3n y arreglar el nombre de las columnas.","57536d08":"<h1 style=\"text-align: center; font-family: Calibri; font-size: 36px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; font-variant: small-caps; letter-spacing: 3px; color: black; background-color: #ffffff;\">COVIDx CXR-2<\/h1>\n<h2 style=\"text-align: center; font-family: Calibri; font-size: 28px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; font-variant: initial; letter-spacing: 3px; color: black; background-color: #ffffff;\">Detecci\u00f3n del Covid-19 a trav\u00e9s de Radiograf\u00edas \n    mediante Redes Neuronales Convolucionales<\/h2>\n<h2 style=\"text-align: center; font-family: Calibri; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; font-variant: initial; letter-spacing: 3px; color: black; background-color: #ffffff;\">Trabajo de Fin de M\u00e1ster<\/h2>\n<h3 style=\"text-align: center; font-family: Calibri; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: underline; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">MSc Big Data and Business Analytics - UCM<\/h3>\n<h4 style=\"text-align: center; font-family: Calibri; font-size: 12px; font-style: normal; font-weight: bold; text-decoration: None; text-transform: none; letter-spacing: 1px; color: black; background-color: #ffffff;\">Autor: Manuel Ferrer Rodr\u00edguez - 30 \/ Sept \/ 2021<\/h4>\n","ab54935d":"### 7.2 ResNet152V2<a class=\"anchor\" id=\"ResNet152V2\"><\/a>\n\nInformaci\u00f3n de su uso [aqu\u00ed](https:\/\/keras.io\/api\/applications\/resnet\/#resnet152-function). \n\nLink a la publicaci\u00f3n dr la arquitectura [aqu\u00ed](https:\/\/arxiv.org\/pdf\/1603.05027.pdf).","1702f3be":"## 2. Referencias<a class=\"anchor\" id=\"Referencias\"><\/a>","4392789a":"Preprocesamiento:","1756e033":"Para este modelo realizaremos el [resample](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.utils.resample.html) como hab\u00edamos explicado al principio del ep\u00edgrafe, adem\u00e1s mantenemos el preprocesamiento usando Data Augmentation como hab\u00edamos definido para el Modelo A.","10203d51":"## 5. Exploraci\u00f3n de los Datos<a class=\"anchor\" id=\"Exploraci\u00f3n\"><\/a>","25f286b9":"### 7.1 Xception <a class=\"anchor\" id=\"Xception\"><\/a>\n\nInformaci\u00f3n de su uso [aqu\u00ed](https:\/\/keras.io\/api\/applications\/xception\/). \n\nLink a la publicaci\u00f3n dr la arquitectura [aqu\u00ed](https:\/\/arxiv.org\/pdf\/1610.02357.pdf).","4c7f5dde":"Preprocesamiento:","1c52c75a":"## 10. Conclusiones<a class=\"anchor\" id=\"Conclusiones\"><\/a>","55900566":"Hallamos las predicciones","ce76c513":"## 3. Librer\u00edas<a class=\"anchor\" id=\"Librer\u00edas\"><\/a>","03fb9b05":"Como hemos observado, cada modelo muestra unos resultados y comportamientos diferentes, aunque en general han sido buenos con resultados por encima del 90% del accuracy. \n\nEn el siguiente apartado vamos a comprobar los que mejor resultado han obtenido valid\u00e1ndolos con nuestro set de test, veremos matriz de confusi\u00f3n y el informe de clasificaci\u00f3n para seleccionar el modelo final."}}