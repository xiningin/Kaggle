{"cell_type":{"a413036a":"code","87ac8869":"code","28081159":"code","57062dce":"code","cd3b7556":"code","b0a1c0b9":"code","596e81d3":"code","2c4ee9bf":"code","4e3f270e":"code","445ee196":"code","546ffe8c":"code","62d19f66":"code","47540050":"code","3a31a750":"code","bd7541c6":"code","60b7f10d":"code","92a91c7e":"code","7b643a65":"code","009bae3f":"code","9aa1de3b":"code","286197aa":"code","3f271b2a":"code","6f0154f1":"code","3a2c7a42":"code","eafccf89":"code","c5a370b4":"code","ddc08b7d":"code","c54c9f53":"code","c31af07f":"code","8ef2f8ac":"code","d460c72a":"code","6e154d7a":"code","2a131a54":"code","41743e59":"code","b53ee82c":"code","dd599be7":"code","6405461b":"code","6064861a":"code","f01037d4":"code","bcd06d68":"markdown","8026e5bf":"markdown","b93d9894":"markdown","b07164a5":"markdown","934e39da":"markdown"},"source":{"a413036a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport lightgbm as lgbm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dense, Dropout, BatchNormalization, Activation \nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import optimizers\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport gc\n# Any results you write to the current directory are saved as output.","87ac8869":"train = pd.read_csv('..\/input\/covertype_train.csv')\ntest = pd.read_csv('..\/input\/covertype_test.csv')","28081159":"train.shape","57062dce":"test.shape","cd3b7556":"train_index = train.shape[0]","b0a1c0b9":"original_all_data = pd.concat([train, test])\nall_data = original_all_data.copy()","596e81d3":"original_all_data['Soil_Type']","2c4ee9bf":"for col in all_data.loc[:,all_data.dtypes=='object'].columns:\n    all_data[col] = all_data[col].factorize()[0]","4e3f270e":"all_data['Soil_Type']","445ee196":"all_data2 = pd.concat([train, test])\nfor col in all_data2.loc[:,all_data2.dtypes=='object'].columns:\n    le = LabelEncoder()\n    all_data2[col] = le.fit_transform(all_data2[col])","546ffe8c":"all_data2['Soil_Type']","62d19f66":"unique_soil_type = sorted(original_all_data['Soil_Type'].unique())\nfor index, soil in enumerate(unique_soil_type):\n    print(soil, original_all_data.loc[original_all_data['Soil_Type']==soil ].shape[0], \n          all_data2.loc[all_data2['Soil_Type']==index ].shape[0]) ","47540050":"unique_soil_type = sorted(original_all_data['Soil_Type'].unique())\nfor index, soil in enumerate(unique_soil_type):\n    print(soil, original_all_data.loc[original_all_data['Soil_Type']==soil ].shape[0], \n          all_data.loc[all_data['Soil_Type']==index ].shape[0]) ","3a31a750":"all_data2 = pd.concat([train, test])\nle = LabelEncoder()","bd7541c6":"%timeit(le.fit_transform(all_data2['Soil_Type']))","60b7f10d":"%timeit(all_data2['Soil_Type'].factorize()[0])","92a91c7e":"all_data = pd.concat([train, test])\nfor col in all_data.loc[:, all_data.dtypes=='object'].columns:\n    all_data[col] = all_data[col].factorize()[0]","7b643a65":"train_df = all_data.iloc[:train_index]\ntest_df = all_data.iloc[train_index:]","009bae3f":"y_value = train_df['Cover_Type']\ndel train_df['Cover_Type'], train_df['ID']\n\ndel test_df['Cover_Type'], test_df['ID']","9aa1de3b":"lgbm_param =  {\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    \"learning_rate\": 0.06,\n    \"num_leaves\": 16,\n    \"max_depth\": 6,\n    \"colsample_bytree\": 0.7,\n    \"subsample\": 0.8,\n    \"reg_alpha\": 0.1,\n    \"reg_lambda\": 0.1,\n    \"nthread\":8\n}","286197aa":"NFOLD = 5\nfolds = StratifiedKFold(n_splits= NFOLD, shuffle=True, random_state=2018)\n\ntotal_score = 0\nbest_iteration = 0\nfor n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df, y_value)):\n    train_x, train_y = train_df.iloc[train_idx], y_value.iloc[train_idx]\n    valid_x, valid_y = train_df.iloc[valid_idx], y_value.iloc[valid_idx]\n    \n    evals_result_dict = {} \n    dtrain = lgbm.Dataset(train_x, label=train_y)\n    dvalid = lgbm.Dataset(valid_x, label=valid_y)\n  \n    clf = lgbm.train(lgbm_param, train_set=dtrain, num_boost_round=3000, valid_sets=[dtrain, dvalid],\n                           early_stopping_rounds=200, evals_result=evals_result_dict, verbose_eval=100)\n    \n    predict = clf.predict(valid_x)\n    cv_score = log_loss(valid_y, predict )\n    total_score += cv_score\n    best_iteration = max(best_iteration, clf.best_iteration)\n    print('Fold {} LogLoss : {}'.format(n_fold + 1, cv_score ))\n    lgbm.plot_metric(evals_result_dict)\n    plt.show()","3f271b2a":"print(\"Best Iteration\", best_iteration)\nprint(\"Total LogLoss\", total_score\/NFOLD)\ndtrain = lgbm.Dataset(train_df, label=y_value)\nclf = lgbm.train(lgbm_param, train_set=dtrain, num_boost_round=best_iteration)\npredict = clf.predict(test_df)","6f0154f1":"submission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission[\"Cover_Type\"] = predict\nsubmission.to_csv('lightgbm_baseline_{:.5f}.csv'.format(total_score\/NFOLD), index=False)","3a2c7a42":"all_data = pd.concat([train, test])","eafccf89":"all_data = pd.concat([train, test])\ncategory_feature = []\nfor col in all_data.loc[:, all_data.dtypes=='object'].columns:\n    all_data[col] = all_data[col].factorize()[0]\n    category_feature.append(col)","c5a370b4":"category_feature","ddc08b7d":"all_data.isnull().sum()","c54c9f53":"sns.distplot(all_data.loc[all_data['Aspect'].notnull(),'Aspect'])\nplt.show()","c31af07f":"sns.distplot(all_data['Aspect'].fillna(all_data['Aspect'].mean()))\nplt.show()","8ef2f8ac":"all_data['Aspect'].fillna(all_data['Aspect'].mean(), inplace=True)","d460c72a":"train_df = all_data.iloc[:train_index]\ntest_df = all_data.iloc[train_index:]","6e154d7a":"numerical_feature = list(set(train_df.columns) - set(category_feature) - set(['Cover_Type','ID']))\nnumerical_feature","2a131a54":"sc = StandardScaler()\ntrain_df[numerical_feature] = sc.fit_transform(train_df[numerical_feature])\ntest_df[numerical_feature] = sc.transform(test_df[numerical_feature] )","41743e59":"y_value = train_df['Cover_Type']\ndel train_df['Cover_Type'], train_df['ID']\n\ndel test_df['Cover_Type'], test_df['ID']","b53ee82c":"def keras_model(input_dims):\n    model = Sequential()\n    \n    model.add(Dense(input_dims, input_dim=input_dims))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.3))\n    \n    model.add(Dense(input_dims\/\/2))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    \n    # output layer (y_pred)\n    model.add(Dense(1))\n    model.add(Activation('sigmoid'))\n    \n    # compile this model\n    model.compile(loss='binary_crossentropy', # one may use 'mean_absolute_error' as alternative\n                  optimizer='adam', metrics=['accuracy'])\n    return model\n\ndef keras_history_plot(history):\n    plt.plot(history.history['loss'], 'y', label='train loss')\n    plt.plot(history.history['val_loss'], 'r', label='val loss')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(loc='upper right')\n    plt.show()","dd599be7":"model = keras_model(train_df.shape[1])\ncallbacks = [\n        EarlyStopping(\n            patience=10,\n            verbose=10)\n    ]\n\n\nNFOLD = 5\nfolds = StratifiedKFold(n_splits= NFOLD, shuffle=True, random_state=2018)\n\ntotal_score = 0\nbest_epoch = 0\nfor n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df, y_value)):\n    train_x, train_y = train_df.iloc[train_idx], y_value.iloc[train_idx]\n    valid_x, valid_y = train_df.iloc[valid_idx], y_value.iloc[valid_idx]\n    \n    history = model.fit(train_x.values, train_y.values, nb_epoch=30, batch_size = 64, validation_data=(valid_x.values, valid_y.values), \n                        verbose=1, callbacks=callbacks)\n    \n    keras_history_plot(history)\n    predict = model.predict(valid_x.values)\n    null_count = np.sum(pd.isnull(predict) )\n    if null_count > 0:\n        print(\"Null Prediction Error: \", null_count)\n        predict[pd.isnull(predict)] = predict[~pd.isnull(predict)].mean()\n    \n    cv_score = log_loss(valid_y, predict )\n    total_score += cv_score\n    best_epoch = max(best_epoch, np.max(history.epoch))\n    print('Fold {} LogLoss : {}'.format(n_fold + 1, cv_score ))","6405461b":"print(\"Best Epoch: \", best_epoch)\nprint(\"Total LogLoss\", total_score\/NFOLD)","6064861a":"history = model.fit(train_df.values, y_value.values, nb_epoch=best_epoch, batch_size = 64, verbose=1)\npredict = model.predict(test_df.values)\nnull_count = np.sum(pd.isnull(predict) )\nif null_count > 0:\n    print(\"Null Prediction Error: \", null_count)\n    predict[pd.isnull(predict)] = predict[~pd.isnull(predict)].mean()","f01037d4":"submission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission[\"Cover_Type\"] = predict\nsubmission.to_csv('neuralnetwork_baseline_{:.5f}.csv'.format(total_score\/NFOLD), index=False)","bcd06d68":"## Tree Model\uacfc \ube44\uad50\ub420 Neural Network \ubaa8\ub378\uc785\ub2c8\ub2e4.\n\uc544\uc8fc \uac04\ub2e8\ud558\uac8c Layer\uac00 \uad6c\uc131\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.<br>\nLabel Encoding\ub9cc\ud558\uace0 Null \uac12\ub9cc \ucc44\uc6e0\uc744 \uacbd\uc6b0 \uc544\uc608 \ud559\uc2b5\uc774 \ub418\uc9c0 \uc54a\uc544. StandardScaler\uae4c\uc9c0 \uc9c4\ud589\ud558\uc600\uc2b5\ub2c8\ub2e4. <br>","8026e5bf":"\ud30c\ub77c\ubbf8\ud130 \ud29c\ub2dd \ub9c8\uc74c\uaecf \ud558\uc154\ub3c4 \ub429\ub2c8\ub2e4!","b93d9894":"# \uc55e\uc73c\ub85c FE\ub97c \ud1b5\ud574\uc11c Model\uc758 \uc131\ub2a5\uc744 \uc62c\ub9ac\uace0 Tree Model\uacfc \uc120\ud615 \ubaa8\ub378\uac04 \ube44\uad50\ub97c \ud558\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.\n### \uadf8\uc5d0 \uc55e\uc11c \uae30\uc900\uc774 \ub418\ub294 Tree Model \uc785\ub2c8\ub2e4. \uc544\ubb34\uac83\ub3c4 \ud558\uc9c0 \uc54a\uace0 Label Encoding\ub9cc \uc218\ud589\ud55c Tree Model \uc785\ub2c8\ub2e4.","b07164a5":"#### \uc18d\ub3c4 \ube44\uad50","934e39da":"### Label Encoding \uc2e4\uc2b5"}}