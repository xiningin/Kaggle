{"cell_type":{"1fb5ed28":"code","ca79901b":"code","8c46cc6c":"code","4ed875ae":"code","05ec1320":"code","e8bfa433":"code","b0fd58b0":"code","21d0b679":"code","497bdf5d":"code","e6e6c6fb":"code","c49bc688":"code","b38e1fe6":"code","238713d0":"code","fa20cce4":"code","b4e8bba0":"code","3cf7eb14":"code","4811c9bc":"code","99fa8b9b":"code","a9cffde4":"code","6485085e":"code","453cb61a":"code","562a84e6":"code","d3886664":"code","6dd9c22d":"code","b635255b":"code","2694e936":"code","b08da8a3":"code","9a38120d":"code","1d915e30":"code","f14df701":"code","4f750495":"code","8fe72b8d":"code","7a54a238":"code","b332555b":"code","0a776a72":"code","ac644055":"code","0aebaf99":"code","7f47a898":"code","a18f988e":"code","11b7534b":"code","69283caa":"code","0bc7d9a4":"code","94e55e8b":"code","2e84eb30":"code","61674f8e":"code","1ab1c031":"code","3ff9a058":"code","b0cfc4ee":"code","6cec3142":"code","effea77f":"code","de4d11e3":"code","394d9a6d":"code","353b2280":"code","f3e29a3a":"code","27118d87":"code","8bcba8ae":"code","f11aff5f":"code","2856a8ed":"code","9628c278":"markdown","250d9392":"markdown","2511fe51":"markdown","94d0eb13":"markdown","92e4e04f":"markdown","6870fd54":"markdown","7230b311":"markdown","b799b977":"markdown","72f2a60b":"markdown","c1933845":"markdown","9641314f":"markdown","89f53cba":"markdown","59d14948":"markdown","90ed5137":"markdown","aa921a39":"markdown","9ec173e2":"markdown","175b3a97":"markdown","8eeb54df":"markdown","844f7654":"markdown","cdf0686f":"markdown","f0252eaf":"markdown","e5daa853":"markdown","6e58ee37":"markdown","1d29f355":"markdown","96fa21c9":"markdown","7cfda85a":"markdown","161415ce":"markdown","5752b2a6":"markdown","c50bd1d8":"markdown","3332dc2d":"markdown","60b06831":"markdown","ff59007a":"markdown","edb1d6e9":"markdown","51580ed2":"markdown","0fda8ad2":"markdown","553eaf29":"markdown","ff765fe5":"markdown","283ee11d":"markdown","7b94dc5b":"markdown","4fac1516":"markdown","7580a537":"markdown"},"source":{"1fb5ed28":"import os\nimport gc\nimport tqdm\nimport random\nfrom typing import Dict\n\nimport sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use(\"fivethirtyeight\")\n\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport cufflinks\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable=True,theme='pearl')\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom colorama import Fore, Back, Style\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA\nsr_ = Style.RESET_ALL\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler,OneHotEncoder, MinMaxScaler,RobustScaler,QuantileTransformer,KBinsDiscretizer,Normalizer\nfrom sklearn.model_selection import  KFold\nfrom sklearn.decomposition import PCA,TruncatedSVD\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.metrics import log_loss\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, TensorDataset,DataLoader","ca79901b":"file_path = \"..\/input\/lish-moa\/\"\n\ntrain_data = pd.read_csv(file_path+'train_features.csv')\ntest_data = pd.read_csv(file_path+'test_features.csv')\nsample = pd.read_csv(file_path+'sample_submission.csv')\ntarget_data  = pd.read_csv(file_path+'train_targets_scored.csv')\n","8c46cc6c":"print(f\"{y_}Number of rows in train data: {r_}{train_data.shape[0]}\\n{y_}Number of columns in train data: {r_}{train_data.shape[1]}\")\nprint(f\"{g_}Number of rows in test data: {r_}{test_data.shape[0]}\\n{g_}Number of columns in test data: {r_}{test_data.shape[1]}\")\nprint(f\"{m_}Number of rows in target data: {r_}{target_data.shape[0]}\\n{m_}Number of columns in target data: {r_}{target_data.shape[1]}\")\nprint(f\"{b_}Number of rows in submission data: {r_}{sample.shape[0]}\\n{b_}Number of columns in submission data:{r_}{sample.shape[1]}\")","4ed875ae":"train_data.head().style.applymap(lambda x: 'background-color:lightgreen')","05ec1320":"def countplot1(feature):\n    plt.figure(dpi=100)\n    sns.countplot(train_data[feature])\n    counts = train_data[feature].value_counts()\n    for i in range(len(counts)):\n        print(f\"{b_}count of {counts.index[i]} is: {r_}{counts.values[i]}\")","e8bfa433":"countplot1(\"cp_type\")","b0fd58b0":"countplot1(\"cp_time\")","21d0b679":"countplot1('cp_dose')","497bdf5d":"def distribution1(feature, color):\n    plt.figure(figsize=(15,7))\n    plt.subplot(121)\n    sns.distplot(train_data[feature],color=color)\n    plt.subplot(122)\n    sns.violinplot(train_data[feature])\n    print(\"{}Max value of {} is: {} {:.2f} \\n{}Min value of {} is: {} {:.2f}\\n{}Mean of {} is: {}{:.2f}\\n{}Standard Deviation of {} is:{}{:.2f}\"\\\n      .format(y_,feature,r_,train_data[feature].max(),g_,feature,r_,train_data[feature].min(),b_,feature,r_,train_data[feature].mean(),m_,feature,r_,train_data[feature].std()))","e6e6c6fb":"distribution1(\"g-1\",\"blue\")","c49bc688":"train_data['g_mean'] = train_data[[x for x in train_data.columns if x.startswith(\"g-\")]].mean(axis=1)\ntest_data['g_mean'] = test_data[[x for x in train_data.columns if x.startswith(\"g-\")]].mean(axis=1)\n\ndistribution1(\"g_mean\",\"yellow\")","b38e1fe6":"distribution1(\"c-0\",\"green\")","238713d0":"train_data['c_mean'] = train_data[[x for x in train_data.columns if x.startswith(\"c-\")]].mean(axis=1)\ntest_data['c_mean'] = test_data[[x for x in train_data.columns if x.startswith(\"c-\")]].mean(axis=1)\n\ndistribution1('c_mean','orange')","fa20cce4":"def distribution2(feature):\n    plt.figure(figsize=(15,14))\n    plt.subplot(231)\n    for i in train_data.cp_type.unique():\n        sns.distplot(train_data[train_data['cp_type']==i][feature],label=i)\n    plt.title(f\"{feature} based on cp_type\")\n    plt.legend()\n\n    plt.subplot(232)\n    for i in train_data.cp_time.unique():\n        sns.distplot(train_data[train_data['cp_time']==i][feature],label=i)\n    plt.title(f\" {feature}  based on cp_time\")\n    plt.legend()\n    \n    plt.subplot(233)\n    for i in train_data.cp_dose.unique():\n        sns.distplot(train_data[train_data['cp_dose']==i][feature],label=i)\n    plt.title(f\" {feature} based on cp_dose \")\n    \n    plt.subplot(234)\n    sns.violinplot(data=train_data,y=feature,x='cp_type')\n    plt.title(f\"{feature} based on cp_type\")\n    plt.legend()\n\n    plt.subplot(235)\n    sns.violinplot(data=train_data,y=feature,x='cp_time')\n    plt.title(f\" {feature}  based on cp_time\")\n    plt.legend()\n    \n    plt.subplot(236)\n    sns.violinplot(data=train_data,y=feature,x='cp_dose')\n    plt.title(f\" {feature} based on cp_dose \")\n    plt.legend()","b4e8bba0":"distribution2('g_mean')","3cf7eb14":"distribution2('c_mean')","4811c9bc":"g_cols = [x for x in train_data.columns if x.startswith(\"g-\")]\nc_cols = [x for x in train_data.columns if x.startswith(\"c-\")]","99fa8b9b":"def plot1(features):\n    rnd = np.random.randint(0,train_data.shape[0]-16)\n    plt.figure(figsize=(10,7))\n    \n    for i in range(4):\n        data = train_data.loc[rnd+i,features]\n        mean = np.mean(data.values)\n        plt.subplot(2,2,i+1)\n        sns.scatterplot(data=data.values,marker=\">\") \n        plt.tick_params(\n        axis='x',          \n        which='both',      \n        bottom=False,    \n        top=False,        \n        labelbottom=False)\n        sns.lineplot(x=list(range(len(data))), y = [mean]*len(data),color='r',linewidth=2)\n        \n    plt.show()","a9cffde4":"plot1(g_cols)","6485085e":"plot1(c_cols)","453cb61a":"def heat(n):\n    plt.figure(figsize=(13,13))\n    rnd = np.random.randint(0,len(g_cols)-n)\n    data = train_data[g_cols]\n    data = data.iloc[:,rnd:rnd+n]\n    sns.heatmap(data.corr())\n    plt.show()","562a84e6":"heat(30)","d3886664":"df = target_data.iloc[:,1:].sum(axis=0).sort_values(ascending=True)[-50:]\nfig = px.bar(x=df.values,y = df.index,color=df.values)\nfig.show()\n","6dd9c22d":"df = target_data.iloc[:,1:].sum(axis=0).sort_values(ascending=True)[:50]\nfig = px.bar(x=df.values,y = df.index,color=df.values)\nfig.show()","b635255b":"data = train_data.merge(target_data,on='sig_id')\ntop_50 = target_data.drop(\"sig_id\",axis=1).columns[target_data.iloc[:,1:].sum(axis=0)>=89]\nbottom_50 = target_data.drop(\"sig_id\",axis=1).columns[target_data.iloc[:,1:].sum(axis=0)<=19]\ndata_top_50 = data[data[top_50].any(axis=1)][g_cols]\ndata_bottom_50  = data[data[bottom_50].any(axis=1)][g_cols]","2694e936":"plt.figure(dpi=100)\nsns.distplot(data_top_50.mean(axis=1),color='yellow')\nplt.show()","b08da8a3":"def plot2(df):\n    rnd = np.random.randint(0,df.shape[0]-5)\n    plt.figure(figsize=(10,7))\n    \n    for i in range(4):\n        data = df.iloc[rnd+i,:]\n        mean = np.mean(data.values)\n        plt.subplot(2,2,i+1)\n        sns.scatterplot(data=data.values,marker=\">\") \n        plt.tick_params(\n        axis='x',          \n        which='both',      \n        bottom=False,    \n        top=False,        \n        labelbottom=False)\n        sns.lineplot(x=list(range(len(data))), y = [mean]*len(data),color='r',linewidth=2)\n        \n    plt.show()","9a38120d":"plot2(data_top_50)","1d915e30":"plt.figure(dpi=100)\nsns.distplot(data_bottom_50.mean(axis=1),color='yellow')\nplt.show()","f14df701":"plot2(data_bottom_50)","4f750495":"df = target_data.iloc[:,1:].sum(axis=1).sort_values(ascending=True)[-50:]\nfig = px.bar(x=df.values,y = target_data.sig_id[df.index],color=df.values)\nfig.show()","8fe72b8d":"print(\"Number of samples with zero target counts are: \",np.sum(target_data.iloc[:,1:].sum(axis=1) == 0))","7a54a238":"def pca_plot1(features,n_components,target):\n    pca = PCA(n_components=n_components)\n    train_g_pca = pca.fit_transform(train_data[features])\n\n    total_var = pca.explained_variance_ratio_.sum()*100\n    labels = {str(i): f\"PC {i+1}\" for i in range(n_components)}\n\n    fig = px.scatter_matrix(\n        train_g_pca,\n        dimensions=range(n_components),\n        labels=labels,\n        title=f\"Total explained variance ratio{total_var:.2f}%\",\n        color=target_data[target].values\n    )\n\n    fig.update_traces(diagonal_visible=True,opacity=0.5)\n    fig.show()\n","b332555b":"#you can get top targets using from above graphs\npca_plot1(g_cols,4,\"proteasome_inhibitor\")","0a776a72":"pca_plot1(g_cols,4,\"dopamine_receptor_antagonist\")","ac644055":"pca_plot1(g_cols,4,\"dna_inhibitor\")","0aebaf99":"pca_plot1(c_cols,4,\"proteasome_inhibitor\")","7f47a898":"pca_plot1(c_cols,4,\"dopamine_receptor_antagonist\")","a18f988e":"pca_plot1(c_cols,4,\"dna_inhibitor\")","11b7534b":"def pca_plot_3d(features,target):\n    pca = PCA(n_components=3)\n    train_g_pca = pca.fit_transform(train_data[features])\n\n    total_var = pca.explained_variance_ratio_.sum()*100\n\n    fig = px.scatter_3d(\n        train_g_pca,x=0,y=1,z=2,\n        title=f\"Total explained variance ratio{total_var:.2f}%\",\n        color=target_data[target].values,\n        labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'}\n    )\n\n    fig.show() ","69283caa":"pca_plot_3d(g_cols,\"proteasome_inhibitor\")","0bc7d9a4":"pca_plot_3d(g_cols,\"dopamine_receptor_antagonist\")","94e55e8b":"pca_plot_3d(g_cols,\"dna_inhibitor\")","2e84eb30":"pca_plot_3d(c_cols,\"proteasome_inhibitor\")","61674f8e":"pca_plot_3d(c_cols,\"dopamine_receptor_antagonist\")","1ab1c031":"pca_plot_3d(c_cols,\"dna_inhibitor\")","3ff9a058":"def plot_exp_var(features):\n    pca = PCA()\n    pca.fit(train_data[features])\n    exp_var_cumul = np.cumsum(pca.explained_variance_ratio_)\n\n    fig = px.area(\n        x=range(1, exp_var_cumul.shape[0] + 1),\n        y=exp_var_cumul,\n        labels={\"x\": \"# Components\", \"y\": \"Explained Variance\"},\n    )\n    fig.show()","b0cfc4ee":"plot_exp_var(g_cols)","6cec3142":"plot_exp_var(c_cols)","effea77f":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","de4d11e3":"ignore_columns = ['sig_id','cp_type']\n\ntrain_columns = [x for x in train_data.columns if x not in ignore_columns]\n\ntrain = train_data[train_columns]\ntest = test_data[train_columns]\ntarget = target_data.iloc[:,1:].values\n\n#droping high correlated features\n# corr_mat = train.corr().abs()\n# upper = corr_mat.where(np.triu(np.ones(corr_mat.shape),k=1).astype(bool))\n# to_drop = [column for column in upper.columns if any(upper[column]>0.80)]\n# train = train.drop(to_drop,axis=1)\n# test = test.drop(to_drop,axis=1)\n\n# preprocessing\ntransform = ColumnTransformer([('o',OneHotEncoder(),[0,1]),('s',Normalizer(),list(range(3,train.shape[1])))])\ntrain = transform.fit_transform(train)\ntest = transform.transform(test)\n\n# transform = ColumnTransformer([('o',OneHotEncoder(),[0,1])],remainder='passthrough')\n# train = transform.fit_transform(train)\n# test = transform.transform(test)\n\n#using feature union\n# transforms = list()\n# transforms.append(('ss', Normalizer()))\n# transforms.append(('pca', PCA(n_components=100)))\n\n# fu = FeatureUnion(transforms)\n# train = fu.fit_transform(train)\n# test = fu.transform(test)","394d9a6d":"train.shape","353b2280":"class Model(nn.Module):\n    def __init__(self,input_size,output_size,hidden_size):\n        super(Model,self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(input_size)\n        self.dropout1 = nn.Dropout(0.5)\n        self.linear1 = nn.utils.weight_norm(nn.Linear(input_size,hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.6)\n        self.linear2 = nn.utils.weight_norm(nn.Linear(hidden_size,hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.6)\n        self.linear3 = nn.utils.weight_norm(nn.Linear(hidden_size,output_size))\n        \n    def forward(self,xb):\n        x = self.batch_norm1(xb)\n        x = self.dropout1(x)\n        x = F.leaky_relu(self.linear1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.leaky_relu(self.linear2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        return self.linear3(x)","f3e29a3a":"def run(plot_losses=False):\n  \n    def train_loop(train_loader,model,loss_fn,device,optimizer,lr_scheduler=None):\n        model.train()\n        total_loss = 0\n        for i, (inputs, targets) in enumerate(train_loader):\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(inputs)\n\n            loss = loss_fn(outputs,targets)\n            loss.backward()\n                \n            total_loss += loss.item()\n\n            optimizer.step()\n            if lr_scheduler != None:\n                lr_scheduler.step()\n                    \n        total_loss \/= len(train_loader)\n        return total_loss\n    \n    def valid_loop(valid_loader,model,loss_fn,device):\n        model.eval()\n        total_loss = 0\n        predictions = list()\n        \n        for i, (inputs, targets) in enumerate(valid_loader):\n            inputs = inputs.to(device)\n            targets = targets.to(device)\n            outputs = model(inputs)                 \n\n            loss = loss_fn(outputs,targets)\n            predictions.extend(outputs.sigmoid().detach().cpu().numpy())\n            \n            total_loss += loss.item()\n        total_loss \/= len(valid_loader)\n            \n        return total_loss,np.array(predictions)    \n    \n    NFOLDS =10\n    kfold = MultilabelStratifiedKFold(n_splits=NFOLDS)\n    \n    #for storing losses of every fold\n    fold_train_losses = list()\n    fold_valid_losses = list()\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"{device} is used\")\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    \n    #kfold\n    for k , (train_idx,valid_idx) in enumerate(kfold.split(X=train,y=target)):\n      \n        x_train,x_valid,y_train,y_valid = train[train_idx,:],train[valid_idx,:],target[train_idx,:],target[valid_idx,:]\n        \n        input_size = x_train.shape[1]\n        output_size = target.shape[1]\n        hidden_size = 1024\n\n        model = Model(input_size,output_size,hidden_size)\n        model.to(device)\n        \n        batch_size = 128\n        epochs = 30\n        lr = 0.01\n        \n        train_tensor = torch.tensor(x_train,dtype=torch.float)\n        y_train_tensor = torch.tensor(y_train,dtype=torch.float)\n\n        train_ds = TensorDataset(train_tensor,y_train_tensor)\n        train_dl = DataLoader(train_ds,\n                             batch_size = batch_size,\n                             shuffle=True\n                             )\n\n        valid_tensor = torch.tensor(x_valid,dtype=torch.float)\n        y_valid_tensor = torch.tensor(y_valid,dtype=torch.float)\n\n        valid_ds = TensorDataset(valid_tensor,y_valid_tensor)\n        valid_dl = DataLoader(valid_ds,\n                             batch_size = batch_size,\n                             shuffle=False\n                             )\n        \n        optimizer = optim.Adam(model.parameters(),lr=lr)\n\n        lr_scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=1e-2, epochs=epochs, steps_per_epoch=len(train_dl))\n        \n        print(f\"Fold {k}\")\n        best_loss = 999\n        \n        train_losses = list()\n        valid_losses = list()\n        \n        for i in range(epochs):\n            train_loss = train_loop(train_dl,model,loss_fn,device,optimizer,lr_scheduler)\n            valid_loss,predictions = valid_loop(valid_dl,model,loss_fn,device)\n            \n            train_losses.append(train_loss)\n            valid_losses.append(valid_loss)\n                          \n            print(f\"epoch:{i} Training | loss:{train_loss}  Validation | loss:{valid_loss}  \")\n            \n            if valid_loss <= best_loss:\n                print(f\"{g_}Validation loss Decreased from {best_loss} to {valid_loss}{sr_}\")\n                best_loss = valid_loss\n                torch.save(model.state_dict(),f'model{k}.bin')\n                \n        fold_train_losses.append(train_losses)\n        fold_valid_losses.append(valid_losses)\n        \n        \n    if plot_losses == True:\n        plt.figure(figsize=(20,14))\n        for i, (t,v) in enumerate(zip(fold_train_losses,fold_valid_losses)):\n            plt.subplot(2,5,i+1)\n            plt.title(f\"Fold {i}\")\n            plt.plot(t,label=\"train_loss\")\n            plt.plot(v,label=\"valid_loss\")\n            plt.legend()\n        plt.show()      ","27118d87":"run(plot_losses=True)","8bcba8ae":"def inference():\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    nfold = 10\n    all_prediction = np.zeros((test.shape[0],206))\n    \n    for i in range(nfold):\n        \n        input_size = train.shape[1]\n        output_size = target.shape[1]\n        hidden_size = 1024\n        model = Model(input_size,output_size,hidden_size)\n        model.load_state_dict(torch.load(f\"model{i}.bin\"))\n        \n        predictions = list()\n        model.to(device)\n        test_tensor = torch.tensor(test,dtype=torch.float)\n        test_dl = DataLoader(test_tensor,\n                        batch_size=64,\n                        shuffle=False)\n    \n        with torch.no_grad():\n            for i, inputs in enumerate(test_dl):\n                inputs = inputs.to(device, dtype=torch.float)\n                outputs= model(inputs) \n                predictions.extend(outputs.sigmoid().cpu().detach().numpy())\n\n        all_prediction += np.array(predictions)\/nfold\n        \n    return all_prediction  ","f11aff5f":"predictions = inference()\nsample.iloc[:,1:] = predictions\nsample.to_csv('submission.csv',index=False)","2856a8ed":"sample.head()","9628c278":"#### 3.23.2 dopamine_receptor_antagonist","250d9392":"#### 3.23.3 dna_inhibitor","2511fe51":"#### 3.21.3 dna_inhibitor","94d0eb13":"### 3.18 random 4 gene exp from bottom 50","92e4e04f":"## 2. Metrics: Log-loss \ud83d\udccf\n\n![image.png](attachment:image.png)\n\nMetrics is simply mean of log-loss for all 206 columns\n\n[evaluation](https:\/\/www.kaggle.com\/c\/lish-moa\/overview\/evaluation)","6870fd54":"### 3.25 Ploting explained variance (c_cols)","7230b311":"### 3.7 Distribution of mean of cell viability","b799b977":"### 3.12 Heat map of n random gene exp columns","72f2a60b":"#### 3.22.2 dopamaine_receptor_antagonist","c1933845":"## Visulisation of g_cols using PCA\n\n### 3.20 How good is top 4 pca data of g_cols at seprating our top targets\n#### 3.20.1 proteasome_inhibitor","9641314f":"### 3.16 random 4 gene exp from top 50","89f53cba":"### 3.14 count of lowest 50 target","59d14948":"### 3.3 count of cp_dose","90ed5137":"#### 3.22.3 dna_inhibitor","aa921a39":"### 3.2 count of cp_time \ud83d\udd22","9ec173e2":"## 3. EDA \ud83d\udcca\n\n### 3.1 Countplot of cp_type \ud83d\udd22","175b3a97":"### 3.8 Distribution of g_mean based on cp_type,cp_time, cp_dose","8eeb54df":"### 3.10 Gene exp of 4 random samples","844f7654":"### 3.4 Distribution of single of Gene expression \ud83d\udc8a","cdf0686f":"## 4. Preprocessing Data \ud83d\udcd0","f0252eaf":"## MoA (Mechanisms of Action)\n\n![image.png](attachment:image.png)\n### What is MoA?\n\nAccording to Widipedia definition In pharmacology, the term mechanism of action (MOA) refers to the specific biochemical interaction through which a drug substance produces its pharmacological effect. A mechanism of action usually includes mention of the specific molecular targets to which the drug binds, such as an enzyme or receptor.\n\nSo in simple terms what kind of receptor or enzyme a drug will bind with is called MoA.\n\n","e5daa853":"### Importing Libraries \ud83d\udcd8","6e58ee37":"## 1. what we have to predict?\ud83d\udca1<br\/>\n\nso we are given some data related to drug (what data we will see later) and we are given data that particular drug in given row affects which enzymze or receptor etc. There are total 206 different types of receptors given which drug could bind with so 1 represent binding 0 the opposite. \n\nso our task is to use this data to make a model which could predict to which receptors the unknown drug will bind to.\n\n### Data Provided.\n\n1. **train_features:**<br\/>\nsig_id: if of each sample<br\/>\ncolumns starts with \"g-\": Gene expression<br\/>\ncolumns starts with \"c-\": Cell Viability<br\/>\ncp_type: samples treated with compound. - \"ctrl_vehicle,<br\/> cp_vehicle\"<br\/>\ncp_time: treatment duration (24,48,72)<br\/>\ndose: high or low<br\/>\n\n2. **train_targets_scores**<br\/>\nsig_id: id of each sample<br\/>\nand 206 receptors and enzyme which drug might bind with.\n\n3. **test_features**<br\/>\nsame as train features\n\n\n**If there is any mistake in code or anything please let me know in comments.\nand upvote if you like my notebook.\ud83d\ude00**","1d29f355":"### 3.23 Pca plot 3d (c_cols)\n#### 3.23.1 proteasome_inhibitor","96fa21c9":"### 3.17 Plot of mean of gene exp for top 50","7cfda85a":"### 3.19 Top 50 sample with highest count of target","161415ce":"### 3.5 Distribution of mean of Gene expression","5752b2a6":"Let's try to find if gene expression and cell viability of higher count are different from one with lower","c50bd1d8":"### 3.6 Distribution of single cell viability","3332dc2d":"### 3.11 cell validity of 4 random sample","60b06831":"### 3.21 How good is top 4 pca data of c_cols at seprating our top targets\n#### 3.21.1 proteasome_inhibitor","ff59007a":"#### 30.20.3 dna_inhibitor","edb1d6e9":"#### 3.21.2 dopamine_receptor_antagonist","51580ed2":"### 3.22 Pca plot 3d (g_cols)\n#### 3.22.1 proteasome_inhibitor","0fda8ad2":"### 3.13 Count of top 50 targets","553eaf29":"### Getting data\ud83d\udcbd","ff765fe5":"### 3.24 Ploting explained variance (g_cols)","283ee11d":"## 5. Pytorch baseline model","7b94dc5b":"### 3.15 Plot of mean of gene exp for top 50","4fac1516":"#### 3.20.2 dopamine_receptor_antagonist","7580a537":"### 3.9 Distribution of c_mean based on cp_type,cp_time, cp_dose"}}