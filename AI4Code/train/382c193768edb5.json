{"cell_type":{"9497ef02":"code","fe5798df":"code","cedd0db4":"code","2f9f55f8":"code","a5c7c3cb":"code","0c26cd87":"code","132f9b73":"code","f8284ef9":"code","fdaa8573":"code","68168d4b":"code","22fd9425":"code","e9cb043e":"code","e1babaee":"code","c13ea788":"code","f5ab02c7":"code","0c8e78ad":"code","d5053587":"code","1b3aee8e":"code","abc2b8e4":"code","3465c2b2":"code","ddad5f7e":"code","58414a7b":"code","4742b870":"code","5e97fb53":"code","2184f967":"code","a2feeb9d":"code","62cb5c52":"code","f134cc82":"markdown","f9bd60cd":"markdown","3f96e4d4":"markdown","171b4e1b":"markdown","1e5c51e7":"markdown","69e54531":"markdown","30064687":"markdown","f607344f":"markdown","f995bf24":"markdown","8c36b379":"markdown","8fc18bab":"markdown","32ff3747":"markdown","a5817dd4":"markdown","2f407543":"markdown","1e901feb":"markdown","a51078fd":"markdown","b1620eb0":"markdown","cca90a70":"markdown","d063221f":"markdown","54e6691a":"markdown","f2683040":"markdown"},"source":{"9497ef02":"!rm .\/ftse*\n!wget https:\/\/raw.githubusercontent.com\/d2l-ai\/d2l-en\/master\/img\/ftse100.png ","fe5798df":"import torch","cedd0db4":"T = 1000\ntime = torch.arange(1,T+1)\nlen(time)","2f9f55f8":"x = torch.cos(time * 0.01) + torch.normal(0,0.2,(T,))\nx.shape","a5c7c3cb":"import matplotlib.pyplot as plt\n\nplt.plot(time, x, label='train data')\nplt.legend()\nplt.show()\n\n# this does look like noisy cos graph!","0c26cd87":"tau  = 4\n\nfeatures = torch.zeros(T-tau, tau)\nlabels = torch.zeros(T-tau)\nfor i in range(0, T-tau):\n    features[i,: ] = x[i:i+tau] \n    labels[i] = x[i+tau]\n\nfeatures[:5], labels[:5]\n    ","132f9b73":"labels = labels.reshape((-1,1))","f8284ef9":"batch_size = 4\nn_train = 600\n\ntrain_iter = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(*(features[:n_train], labels[:n_train])), batch_size=batch_size, shuffle=True)\ntest_iter = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(*(features[n_train:], labels[n_train:])), batch_size=batch_size, shuffle=True)","fdaa8573":"from torch import nn\n\ndef init_net(m):\n    if type(m) == nn.Linear:\n        nn.init.xavier_uniform_(m.weight)\n\ndef get_net():\n    net = nn.Sequential(nn.Linear(4,32), nn.ReLU(), nn.Linear(32,1))\n    net.apply(init_net)\n    return net\n\n    \ndevice = torch.device('gpu' if torch.cuda.is_available() else 'cpu')","68168d4b":"net = get_net()\npred_x = net(features)\npred_x.shape","22fd9425":"plt.plot(time, x, label=\"actual\")\nplt.plot(time[4:], pred_x.detach().numpy(), label=\"predicted\")\nplt.legend()\nplt.title(\"before training\")\nplt.grid()\nplt.show()","e9cb043e":"def train(net, train_iter, test_iter, num_epochs, device, lr=0.01):\n    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n    loss = nn.MSELoss()\n    \n    net = net.to(device)\n    \n    for epoch in range(num_epochs):\n        overall_loss = 0\n        total_numer = 0\n        for X, y in train_iter:\n            X = X.to(device)\n            y = y.to(device)\n            y_hat = net(X)\n            l = loss(y_hat, y)\n            optimizer.zero_grad()\n            l.backward()\n            optimizer.step()\n            total_numer += X.shape[0]\n            overall_loss += l\n        \n        epoch_loss = overall_loss\/total_numer\n        print(f\"for epoch {epoch} the loss: {epoch_loss}\")","e1babaee":"net = get_net()\ntrain(net, train_iter, test_iter, 10, device, 0.01)","c13ea788":"pred_x = net(features)\npred_x.shape","f5ab02c7":"plt.plot(time, x, label=\"actual\")\nplt.plot(time[4:], pred_x.detach().numpy(), label=\"predicted\")\nplt.legend()\nplt.title(\"after training\")\nplt.grid()\nplt.show()","0c8e78ad":"multistep_preds = torch.zeros(T)\nmultistep_preds[:n_train+ tau] = x[:n_train+tau]\n\nfor i in range(n_train+tau , T):\n    # predicting simply based on past 4 predictions\n    multistep_preds[i] = net(multistep_preds[i-tau:i].reshape((-1,1)).squeeze(1))\n\nplt.plot(time, x, label=\"actual\")\nplt.plot(time[4:], pred_x.detach().numpy(), label = \"one step ahead\")\nplt.plot(time, multistep_preds.detach().numpy(), label = \"multi step ahead\")\nplt.legend()\nplt.grid()\nplt.show()","d5053587":"max_steps = 64\n\nahead_features = torch.zeros((T - tau - max_steps + 1, tau + max_steps))\n\nfor i in range(tau):\n    ahead_features[:,i] = x[i:i + T - tau - max_steps + 1]\n\nfor i in range(tau, tau + max_steps):\n    ahead_features[:, i] = net(ahead_features[:, i - tau:i]).reshape(-1)\n\nsteps = (1, 4, 16, 64)\n\nfor i in steps:\n    plt.plot(time[tau + i -1: T - max_steps + i], ahead_features[:, (tau + i - 1)].detach().numpy(), label=f\"{i} pred steps\" )\nplt.legend()\nplt.grid()\nplt.show()\n    ","1b3aee8e":"# lets try more than 4 observations\n\nsteps = range(1, 64)\n\nfor i in steps:\n    plt.plot(time[tau + i -1: T - max_steps + i], ahead_features[:, (tau + i - 1)].detach().numpy(), label=f\"{i} pred steps\" )\n#plt.legend()\nplt.grid()\nplt.show()\n","abc2b8e4":"# if x had no noise\n\nx = torch.cos(time * 0.01)\nx.shape","3465c2b2":"plt.plot(time, x, label='train data')\nplt.legend()\nplt.show()\n","ddad5f7e":"au  = 4\n\nfeatures = torch.zeros(T-tau, tau)\nlabels = torch.zeros(T-tau)\nfor i in range(0, T-tau):\n    features[i,: ] = x[i:i+tau] \n    labels[i] = x[i+tau]\n\nfeatures[:5], labels[:5]","58414a7b":"labels = labels.reshape((-1,1))","4742b870":"batch_size = 4\nn_train = 600\n\ntrain_iter = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(*(features[:n_train], labels[:n_train])), batch_size=batch_size, shuffle=True)\ntest_iter = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(*(features[n_train:], labels[n_train:])), batch_size=batch_size, shuffle=True)","5e97fb53":"net = get_net()\ntrain(net, train_iter, test_iter, 10, device, 0.01)","2184f967":"pred_x = net(features)","a2feeb9d":"plt.plot(time, x, label=\"actual\")\nplt.plot(time[4:], pred_x.detach().numpy(), label=\"predicted\")\nplt.legend()\nplt.title(\"after training\")\nplt.grid()\nplt.show()","62cb5c52":"multistep_preds = torch.zeros(T)\nmultistep_preds[:n_train+ tau] = x[:n_train+tau]\n\nfor i in range(n_train+tau , T):\n    # predicting simply based on past 4 predictions\n    multistep_preds[i] = net(multistep_preds[i-tau:i].reshape((-1,1)).squeeze(1))\n\nplt.plot(time, x, label=\"actual\")\nplt.plot(time[4:], pred_x.detach().numpy(), label = \"one step ahead\")\nplt.plot(time, multistep_preds.detach().numpy(), label = \"multi step ahead\")\nplt.legend()\nplt.grid()\nplt.show()","f134cc82":"### Summary\n\nThere is quite a difference in difficulty between interpolation and extrapolation. Consequently, if you have a sequence, always respect the temporal order of the data when training, i.e., never train on future data.\n\nSequence models require specialized statistical tools for estimation. Two popular choices are autoregressive models and latent-variable autoregressive models.\n\nFor causal models (e.g., time going forward), estimating the forward direction is typically a lot easier than the reverse direction.\n\nFor an observed sequence up to time step  t , its predicted output at time step  t+k  is the  k -step-ahead prediction. As we predict further in time by increasing  k , the errors accumulate and the quality of the prediction degrades, often dramatically.","f9bd60cd":"### Lets try and eliminate noise altogether and find out if it makes a difference to multi step prediction.","3f96e4d4":"### Things to ponder\n\nImprove the model in the experiment of this section.\n\n    -Incorporate more than the past 4 observations? How many do you really need?\n    (Tried. in a limited way. close to one seems better.)\n\n    - How many past observations would you need if there was no noise? Hint: you can write  sin  and  cos  as a differential equation.\n    (I tried with sin and cos.Its the same story. `x = torch.cos(time * 0.01)`)\n\n    - Can you incorporate older observations while keeping the total number of features constant? Does this improve accuracy? Why?\n    (Dont get the question)\n\n    Change the neural network architecture and evaluate the performance.\n    (tried. same)\n\nAn investor wants to find a good security to buy. He looks at past returns to decide which one is likely to do well. What could possibly go wrong with this strategy?\n(Only things constant in life is change)\n\nDoes causality also apply to text? To which extent?\n(The words do have some causality I believe, you can expect h to follow W at the start of a sentence with a relatively high degree of confidence. But its very topical)\n\nGive an example for when a latent autoregressive model might be needed to capture the dynamic of the data.\n(In stock market!)","171b4e1b":"Adapted from chapter 9 of https:\/\/d2l.ai\/","1e5c51e7":"Conclusion: So even with no noise its the same story.","69e54531":"Standard training loop of pytorch.https:\/\/pytorch.org\/tutorials\/beginner\/blitz\/cifar10_tutorial.html","30064687":"We often need statistical tools to analyse sequence model data\n\nfor example look at this graph:\n\n![FTSE 100 index over about 30 years.](https:\/\/raw.githubusercontent.com\/d2l-ai\/d2l-en\/master\/img\/ftse100.png )\n\n\n\nLet us denote the prices by $x_t$, i.e., at *time step* $t \\in \\mathbb{Z}^+$ we observe price $x_t$.\nNote that for sequences in this text,\n$t$ will typically be discrete and vary over integers or its subset.\nSuppose that\na trader who wants to do well in the stock market on day $t$ predicts $x_t$ via\n\n$$x_t \\sim P(x_t \\mid x_{t-1}, \\ldots, x_1).$$\n\n","f607344f":"The one-step-ahead predictions look nice, just as we expected. Even beyond 604 (n_train + tau) observations the predictions still look trustworthy. However, there is just one little problem to this: if we observe sequence data only until time step 604, we cannot hope to receive the inputs for all the future one-step-ahead predictions. Instead, we need to work our way forward one step at a time:\n\n`x^605=f(x601,x602,x603,x604),\n x^606=f(x602,x603,x604,x^605),\n x^607=f(x603,x604,x^605,x^606),\n x^608=f(x604,x^605,x^606,x^607),\n x^609=f(x^605,x^606,x^607,x^608)...`","f995bf24":"In order to do this, we could have trained a linear model however, the number of input to the model varies depending on previous values encountered. This makes it intractable.Much about autoregressive model revolves around doing it efficiently. this is done by\n\n1. Not haveing arbitary long input, instead focus on someprevious tau values. Now this makes regression possible. They literally regress on themselves thus autoregressive model.\n\n2. second strategy is to keep tabs on some past observations through summary summary $h_t$ of the past observations, and at the same time update $h_t$ in addition to the prediction $\\hat{x}_t$.\nThis leads to models that estimate $x_t$ with $\\hat{x}_t = P(x_t \\mid h_{t})$ and moreover updates of the form  $h_t = g(h_{t-1}, x_{t-1})$. Since $h_t$ is never observed, these models are also called *latent autoregressive models*.\n\n![A latent autoregressive model.](https:\/\/raw.githubusercontent.com\/d2l-ai\/d2l-en\/a49cf28aaaf533595707bca63ec1e567f65c16ca\/img\/sequence-model.svg)\n\nBoth cases raise the obvious question of how to generate training data. One typically uses historical observations to predict the next observation given the ones up to right now. Obviously we do not expect time to stand still. However, a common assumption is that while the specific values of $x_t$ might change, at least the dynamics of the sequence itself will not. This is reasonable, since novel dynamics are just that, novel and thus not predictable using data that we have so far. Statisticians call dynamics that do not change *stationary*.\nRegardless of what we do, we will thus get an estimate of the entire sequence via\n\n$$P(x_1, \\ldots, x_T) = \\prod_{t=1}^T P(x_t \\mid x_{t-1}, \\ldots, x_1).$$\n\nNote that the above considerations still hold if we deal with discrete objects, such as words, rather than continuous numbers. The only difference is that in such a situation we need to use a classifier rather than a regression model to estimate $P(x_t \\mid  x_{t-1}, \\ldots, x_1)$.","8c36b379":"Here is how it looks before training","8fc18bab":"The green line is our multi step prediction, so what just happened? why did it fail beyond training data?\n\n### Explanation for the green line\n\nAs the above example shows, this is a spectacular failure. The predictions decay to a constant pretty quickly after a few prediction steps. Why did the algorithm work so poorly? This is ultimately due to the fact that the errors build up. Let us say that after step 1 we have some error  \u03f51=\u03f5\u00af . Now the input for step 2 is perturbed by  \u03f51 , hence we suffer some error in the order of  \u03f52=\u03f5\u00af+c\u03f51  for some constant  c , and so on. The error can diverge rather rapidly from the true observations. This is a common phenomenon. For instance, weather forecasts for the next 24 hours tend to be pretty accurate but beyond that the accuracy declines rapidly. We will discuss methods for improving this throughout this chapter and beyond.\n\nLet us take a closer look at the difficulties in  k -step-ahead predictions by computing predictions on the entire sequence for  k=1,4,16,64 .\n","32ff3747":"### multi step for all steps till 4","a5817dd4":"The reason we do is that we need to turn such a sequence into features and labels that our model can train on. Based on the embedding dimension  \u03c4  we map the data into pairs  yt=xt  and  xt=[xt\u2212\u03c4,\u2026,xt\u22121] . The astute reader might have noticed that this gives us  \u03c4  fewer data examples, since we do not have sufficient history for the first  \u03c4  of them. A simple fix, in particular if the sequence is long, is to discard those few terms. Alternatively we could pad the sequence with zeros. Here we only use the first 600 feature-label pairs for training.","2f407543":"### k step ahead prediction\n\nGenerally, for an observed sequence up to  xt , its predicted output  x^t+k  at time step  t+k  is called the  k -step-ahead prediction. Since we have observed up to  x604 , its  k -step-ahead prediction is  x^604+k . In other words, we will have to use our own predictions to make multistep-ahead predictions.","1e901feb":"for initialisation we will do xavier initialisation. And create a simple sequential model.","a51078fd":"### Generating training data\n\nwe will use cos function with some added noise for 1000 steps. `T=1000`","b1620eb0":"This clearly illustrates how the quality of the prediction changes as we try to predict further into the future. While the 4-step-ahead predictions still look good, anything beyond that is almost useless.","cca90a70":"### Creating train and test data\n\nWe will use `torch.utils.data.TensorDataset` to load the features and labels directly.","d063221f":"### Turning train data into features and labels\n\nIn order to do so we will use the next term of first 4 term as the label. `tau = 4`.","54e6691a":"### We are ready to train our model.\n\ninorder to train our model\n\n1. define a neural network\n2. define loss optimizer\n3. define learning rate\n4. we need to initialise all parameters\n5. implement training loop","f2683040":"## Prediction\n\nlets put neural net model to some use.\n\n### One step ahead prediction\n\nSince the training loss is small, we would expect our model to work well. Let us see what this means in practice. The first thing to check is how well the model is able to predict what happens just in the next time step, namely the one-step-ahead prediction."}}