{"cell_type":{"ebfaec80":"code","dfd99cb2":"code","9dc03c59":"code","be2160af":"code","ce81079c":"code","1f7332f8":"code","cd0001c1":"code","dad07a6f":"code","57fc17ab":"code","1b959a23":"code","ac346a39":"code","1a21f174":"code","b4259621":"code","f9cc2b98":"code","efb61a91":"code","5f0701db":"code","490dbea0":"code","18da871e":"code","769d07e5":"code","aaeb58ae":"code","7f277e47":"code","130be651":"code","c1557805":"code","ee0790ed":"code","efe137f4":"code","c5132dfd":"code","7e2f16ff":"code","b5ca9da7":"code","982270d5":"code","1a2ba572":"code","e60fbfa4":"code","0f1ff78f":"code","4a36e500":"code","d391f502":"code","662a1122":"code","80eecdfe":"code","7600d59a":"code","a5b39a6c":"code","8fd3da81":"code","6312bf18":"code","5c1faad3":"code","3f2a3481":"code","67a7355b":"code","e0c9567b":"code","94ba007f":"code","d0a14bd8":"markdown","6b61e8ad":"markdown","b31f3c59":"markdown","9d2cffb3":"markdown","8c9522d4":"markdown","5978b924":"markdown","9854461c":"markdown","34b80f5e":"markdown","cc634b3a":"markdown"},"source":{"ebfaec80":"import numpy as np   \nimport pandas as pd    \nimport matplotlib.pyplot as plt \n%matplotlib inline \nimport seaborn as sns\nfrom scipy.stats import zscore\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nimport time","dfd99cb2":"data = pd.read_csv('vehicle.csv')","9dc03c59":"data.head()","be2160af":"data.shape","ce81079c":"data.info()","1f7332f8":"# Check number of not a number values\ndata.isna().sum()","cd0001c1":"data[data['radius_ratio'].isna() == True]","dad07a6f":"data['class'].value_counts()","57fc17ab":"# Creating a copy of dataframe to have class variable with us before operating on NaN values\nimport copy\ndata_copy = copy.deepcopy(data)","1b959a23":"# Convert class variable to values\nfrom sklearn.preprocessing import LabelEncoder\ndata_copy[\"class\"] = LabelEncoder().fit_transform(data_copy[\"class\"])\ndata_copy.head()","ac346a39":"data_copy.describe().transpose()","1a21f174":"# Observation: as outliers present, replace nan values with median","b4259621":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='median')\nimputer = imputer.fit(data_copy)\ndata_copy = pd.DataFrame(np.array(imputer.transform(data_copy)),columns=data_copy.columns)","f9cc2b98":"data_copy","efb61a91":"# Understanding the attributes - Find relationship between different\n# attributes (Independent variables) and choose carefully which all\n# attributes have to be a part of the analysis and why","5f0701db":"# 2. Relationship between var\ndata_copy.corr()","490dbea0":"correlation_matrix = data_copy.corr()\nfig,ax = plt.subplots(figsize=(20,20)) \nsns.heatmap(correlation_matrix, annot=True, ax=ax)  \nplt.xticks(range(len(correlation_matrix.columns)), correlation_matrix.columns) \nplt.yticks(range(len(correlation_matrix.columns)), correlation_matrix.columns) ","18da871e":"# For multicollinearity lets remove columns having corr of 95% pr.axis_rectangularity,scaled_variance,scaled_variance.1\n# 99 % corelation between scatter_ratio and scaled_variance\n# There are Attributes which are directly not related to class but can not remove these columns directly as they can be useful while doing PCA","769d07e5":"data_copy.drop(['pr.axis_rectangularity','scaled_variance','scaled_variance.1'],axis=1,inplace=True)","aaeb58ae":"data_copy.shape","7f277e47":"# draw pair plot which wil lhelp to get idea of covariance matrix \nsns.pairplot(data_copy,diag_kind='kde',size=4)","130be651":"X=data_copy.drop('class',axis=1)\nY=data_copy['class']","c1557805":"# Lets scale our data\nX_Scaled=X.apply(zscore)\nX_Scaled.head()","ee0790ed":"def get_SVM_Accuracy(X):\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=1)\n    model = SVC()\n    start_time = time.time() \n    model.fit(X_train, y_train)\n    elapsed_time = time.time() - start_time\n    return model.score(X_train,y_train)*100,model.score(X_test,y_test)*100,elapsed_time","efe137f4":"Before_PCA_train_acc,Before_PCA_test_acc,time_taken = get_SVM_Accuracy(X_Scaled)\nprint(Before_PCA_train_acc)\nprint(Before_PCA_test_acc)\nprint(time_taken)","c5132dfd":"data_copy.shape","7e2f16ff":"#Apply k fold cross validation\n#using default genral practice 10 kfolds","b5ca9da7":"# Common k fold function\ndef get_kFold_Results(X):\n    Y = data_copy['class']\n    num_folds = 10\n    seed = 7\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    model = SVC()\n    results = cross_val_score(model, X, Y, cv=kfold)\n    print(results)\n    print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100.0, results.std()*100.0))\n    return results","982270d5":"data_copy.head()","1a2ba572":"beforePCACV_results = get_kFold_Results(X_Scaled)","e60fbfa4":"#Apply PCA","0f1ff78f":"covMatrix = np.cov(X_Scaled,rowvar=False)\nprint(covMatrix)","4a36e500":"pca = PCA()\npca.fit(X_Scaled)","d391f502":"# eigen value\nprint(pca.explained_variance_)","662a1122":"# eigen ratio\nprint(pca.explained_variance_ratio_)","80eecdfe":"plt.bar(list(range(1,16)),pca.explained_variance_ratio_,alpha=0.5, align='center')\nplt.ylabel('Variation explained')\nplt.xlabel('eigen Value')\nplt.show()","7600d59a":"plt.step(list(range(1,16)),np.cumsum(pca.explained_variance_ratio_), where='mid')\nplt.ylabel('Cum of variation explained')\nplt.xlabel('eigen Value')\nplt.show()","a5b39a6c":"# Ploting \nplt.figure(figsize=(10 , 5))\nplt.bar(range(1, pca.explained_variance_ratio_.size + 1), pca.explained_variance_ratio_, alpha = 0.5, align = 'center', label = 'Individual explained variance')\nplt.step(range(1, pca.explained_variance_ratio_.size + 1), np.cumsum(pca.explained_variance_ratio_), where='mid', label = 'Cumulative explained variance')\nplt.ylabel('Explained Variance Ratio')\nplt.xlabel('Principal Components')\nplt.legend(loc = 'best')\nplt.tight_layout()\nplt.show()","8fd3da81":"pca7 = PCA(n_components=7)\npca7.fit(X_Scaled)\nX_Scaled_PCA = pca7.transform(X_Scaled)","6312bf18":"X_Scaled_PCA\nAfter_PCA_train_acc,After_PCA_test_acc,time_taken = get_SVM_Accuracy(X_Scaled_PCA)\nprint(After_PCA_train_acc)\nprint(After_PCA_test_acc)\nprint(time_taken)","5c1faad3":"Before_PCA_train_acc,Before_PCA_test_acc,time_taken = get_SVM_Accuracy(X_Scaled)\nprint(Before_PCA_train_acc)\nprint(Before_PCA_test_acc)\nprint(time_taken)","3f2a3481":"After_PCA_train_acc,After_PCA_test_acc,time_taken = get_SVM_Accuracy(X_Scaled_PCA)\nprint(After_PCA_train_acc)\nprint(After_PCA_test_acc)\nprint(time_taken)","67a7355b":"beforePCACV_results = get_kFold_Results(X_Scaled)","e0c9567b":"afterPCACV_results = get_kFold_Results(X_Scaled_PCA)","94ba007f":"# Summary:\n# After PCA we can see drop in accuracy as we loose information(from attributes 6 to 7 Principal components)\n# Although there is no significant gain in\n# Computation time as well \n# before PCA time taken is 0.0267 \n# After PCA tie taken in fitting the SVC model 0.0209\n# with confidance interval of 95 % we can say that with using PCA acc 91.48% with std deviation 2.44% will range \n# from 86.6 to  96.36","d0a14bd8":"## Understanding the attributes","6b61e8ad":"## Use 7 Principal components","b31f3c59":"## Perform K-fold cross validation","9d2cffb3":"## Use PCA from Scikit learn","8c9522d4":"## Compare the accuracy scores and cross validation scores","5978b924":"## Split the data into train and test","9854461c":"## Data pre-processing","34b80f5e":"## Train a Support vector machine","cc634b3a":"#### Remove columns if we can "}}