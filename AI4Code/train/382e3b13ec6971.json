{"cell_type":{"725cd294":"code","0d73f7e2":"code","36bf4ea6":"code","5df0f256":"code","d7c7662d":"code","1343b22f":"code","bd2077a5":"code","7da9773e":"code","b0bcd11f":"code","ddf40a64":"code","1d1842ab":"code","a3628fea":"code","de46ceaa":"code","6c2fad8b":"code","da5d74c4":"code","733d6afa":"code","09a56b6c":"code","9e84d371":"code","af4f462e":"code","baef9587":"code","3e285369":"code","4e2530f9":"code","dbfde914":"code","ad339d9c":"code","b35829c6":"code","588ae907":"code","c9ff6339":"code","580d9b2b":"code","9ef3d4af":"code","b6a46b39":"code","7d6c3c98":"code","86ebeb00":"code","1310a2e1":"code","3926fe20":"code","90f087b4":"code","3f7cfb07":"code","5c1ce4c3":"code","fe2f8e63":"code","17eda189":"code","89262855":"code","ae33c049":"code","72284630":"code","43e6cbf4":"code","d84e1d64":"code","0d9b81f4":"code","b9553560":"code","0c863238":"code","3ae19113":"code","750a2307":"code","0eb1e737":"code","130f4481":"code","cad2e435":"code","d08464a6":"code","02aac5d1":"code","465e7475":"code","fb50b027":"code","3a9ebb48":"code","ba2c28df":"code","06608f16":"code","b9a20e05":"code","b4edac90":"code","68a22ea4":"code","fe939c67":"code","c645380a":"code","f639f10c":"code","3d3bcbfa":"code","9112dfbb":"code","a0216a58":"code","ad7629e7":"code","e77a9780":"code","48448166":"code","677f3510":"markdown","c5ac8bd2":"markdown","bc5975d2":"markdown","fbab0648":"markdown","768eaf15":"markdown","ba8a1c94":"markdown","4868ec63":"markdown","dd9ddc88":"markdown","ea284042":"markdown","1f1de1fb":"markdown","e06b72ff":"markdown","456c1469":"markdown","7f4611af":"markdown","aa53c629":"markdown"},"source":{"725cd294":"import numpy as np\nimport math\nimport re\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nimport seaborn as sns\nimport spacy as sp\nimport string\nimport random\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nimport tensorflow_datasets as tfds\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode,iplot\nimport plotly.express as px\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split","0d73f7e2":"data = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\ndata_test = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')","36bf4ea6":"data","5df0f256":"data_test","d7c7662d":"data.drop(['textID'], axis = 1, inplace=True)\ndata_test.drop(['textID'], axis = 1, inplace=True)","1343b22f":"data.head()","bd2077a5":"data_test.head()","7da9773e":"data.isnull().sum(axis=0)","b0bcd11f":"data_test.isnull().sum(axis=0)","ddf40a64":"data.dropna(axis=0, inplace=True)","1d1842ab":"data['sentiment'] = data['sentiment'].map({'positive': 1,\n                             'negative': -1,\n                             'neutral': 0},\n                             na_action=None)","a3628fea":"data_test['sentiment'] = data['sentiment'].map({'positive': 1,\n                             'negative': -1,\n                             'neutral': 0},\n                             na_action=None)","de46ceaa":"positive = data[data['sentiment'] == 1]\nnegative = data[data['sentiment'] == -1]\nneutral = data[data['sentiment'] == 0]\npositive_test = data_test[data_test['sentiment'] == 1]\nnegative_test = data_test[data_test['sentiment'] == -1]\nneutral_test = data_test[data_test['sentiment'] == 0]","6c2fad8b":"plt.rcParams['figure.figsize'] = (10, 10)\nplt.style.use('fast')\n\nwc = WordCloud(background_color = 'orange', width = 1500, height = 1500).generate(str(positive['text']))\nplt.title('Description Positive', fontsize = 15)\n\nplt.imshow(wc)\nplt.axis('off')\nplt.show()","da5d74c4":"plt.rcParams['figure.figsize'] = (10, 10)\nplt.style.use('fast')\n\nwc = WordCloud(background_color = 'orange', width = 1500, height = 1500).generate(str(negative['text']))\nplt.title('Description Negative', fontsize = 15)\n\nplt.imshow(wc)\nplt.axis('off')\nplt.show()","733d6afa":"plt.rcParams['figure.figsize'] = (10, 10)\nplt.style.use('fast')\n\nwc = WordCloud(background_color = 'orange', width = 1500, height = 1500).generate(str(neutral['text']))\nplt.title('Description Neutral', fontsize = 15)\n\nplt.imshow(wc)\nplt.axis('off')\nplt.show()","09a56b6c":"fig2 = px.histogram(data,x='sentiment',color='sentiment',template='plotly_dark')\nfig2.show()","9e84d371":"plt.figure(figsize=(20,6))\ntop_30 = data.groupby('selected_text')['selected_text'].count() \\\n.sort_values(ascending = False).head(30)\nsns.barplot(x=top_30.index, y = top_30.values)\nplt.title('Top 30 Words')\nplt.show()","af4f462e":"temp = data.describe()\ntemp.style.background_gradient(cmap='Purples')","baef9587":"data['sentiment'] = data['sentiment'].apply(lambda x: 1 if x >= 0 else 0)","3e285369":"X = data.iloc[:, 0].values\nX","4e2530f9":"X.shape","dbfde914":"y = data.iloc[:, 2].values\ny","ad339d9c":"X, _, y, _ = train_test_split(X, y, stratify = y)","b35829c6":"print(X.shape, y.shape )","588ae907":"unique, counts = np.unique(y, return_counts=True)\nunique, counts","c9ff6339":"def clean_t(t):\n  t = BeautifulSoup(t, 'lxml').get_text()\n  t = re.sub(r\"@[A-Za-z0-9]+\", ' ', t)\n  t = re.sub(r\"https?:\/\/[A-Za-z0-9.\/]+\", ' ', t)\n  t = re.sub(r\"[^a-zA-Z.!?]\", ' ', t)\n  t = re.sub(r\" +\", ' ', t)\n  return t","580d9b2b":"text = \"I don't like\"\ntext = clean_t(text)\ntext","9ef3d4af":"import spacy\nnlp = spacy.blank(\"en\")\nnlp","b6a46b39":"stop_words = sp.lang.en.STOP_WORDS\nprint(stop_words)","7d6c3c98":"len(stop_words)","86ebeb00":"string.punctuation","1310a2e1":"def clean_t2(tt):\n  tt = tt.lower()\n  document = nlp(tt)\n\n  words = []\n  for token in document:\n    words.append(token.text)\n\n  words = [word for word in words if word not in stop_words and word not in string.punctuation]\n  words = ' '.join([str(element) for element in words])\n\n  return words","3926fe20":"text2 = clean_t2(text)\ntext2","90f087b4":"data_clean = [clean_t2(clean_t(t)) for t in X]","3f7cfb07":"for _ in range(10):\n  print(data_clean[random.randint(0, len(data_clean) - 1)])","5c1ce4c3":"data_labels = y\ndata_labels","fe2f8e63":"np.unique(data_labels)","17eda189":"tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(data_clean, target_vocab_size=2**16)","89262855":"tokenizer.vocab_size","ae33c049":"print(tokenizer.subwords)","72284630":"ids = tokenizer.encode('I like')\nids","43e6cbf4":"data_inputs = [tokenizer.encode(sentence) for sentence in data_clean]","d84e1d64":"for _ in range(10):\n  print(data_inputs[random.randint(0, len(data_inputs) - 1)])","0d9b81f4":"max_len = max([len(sentence) for sentence in data_inputs])\nmax_len","b9553560":"data_inputs = tf.keras.preprocessing.sequence.pad_sequences(data_inputs,\n                                                            value = 0,\n                                                            padding = 'post',\n                                                            maxlen=max_len)","0c863238":"for _ in range(10):\n  print(data_inputs[random.randint(0, len(data_inputs) - 1)])","3ae19113":"train_inputs, test_inputs, train_labels, test_labels = train_test_split(data_inputs,\n                                                                        data_labels,\n                                                                        test_size=0.3,\n                                                                        stratify = data_labels)","750a2307":"print(train_inputs.shape, train_labels.shape)","0eb1e737":"print(test_inputs.shape, test_labels.shape )","130f4481":"class DCNN(tf.keras.Model):\n\n  def __init__(self,\n               vocab_size,\n               emb_dim=128,\n               nb_filters=50,\n               ffn_units=512,\n               nb_classes=2,\n               dropout_rate=0.1,\n               training=True,\n               name=\"dcnn\"):\n    super(DCNN, self).__init__(name=name)\n    self.embedding = layers.Embedding(vocab_size, emb_dim)\n    self.bigram = layers.Conv1D(filters=nb_filters, kernel_size=2, padding='same', activation='relu')\n    self.trigram = layers.Conv1D(filters=nb_filters, kernel_size=3, padding='same', activation='relu')\n    self.fourgram = layers.Conv1D(filters=nb_filters, kernel_size=4, padding='same', activation='relu')\n    self.pool = layers.GlobalMaxPool1D()\n    \n#estrutura da rede neural\n    self.dense_1 = layers.Dense(units = ffn_units, activation = 'relu')\n    self.dropout = layers.Dropout(rate = dropout_rate)\n    if nb_classes == 2:\n      self.last_dense = layers.Dense(units = 1, activation = 'sigmoid')\n    else:\n      self.last_dense = layers.Dense(units = nb_classes, activation = 'softmax')\n\n  def call(self, inputs, training):\n    x = self.embedding(inputs)\n    x_1 = self.bigram(x)\n    x_1 = self.pool(x_1)\n    x_2 = self.trigram(x)\n    x_2 = self.pool(x_2)\n    x_3 = self.fourgram(x)\n    x_3 = self.pool(x_3)\n\n    merged = tf.concat([x_1, x_2, x_3], axis = -1)\n    merged = self.dense_1(merged)\n    merged = self.dropout(merged, training)\n    output = self.last_dense(merged)\n\n    return output","cad2e435":"vocab_size = tokenizer.vocab_size\nvocab_size","d08464a6":"emb_dim = 200\nnb_filters = 100\nffn_units = 256\nbatch_size = 64\nnb_classes = len(set(train_labels))\nnb_classes","02aac5d1":"dropout_rate = 0.2\nnb_epochs = 5  ","465e7475":"Dcnn = DCNN(vocab_size=vocab_size, emb_dim=emb_dim, nb_filters=nb_filters,\n            ffn_units=ffn_units, nb_classes=nb_classes, dropout_rate=dropout_rate)","fb50b027":"if nb_classes == 2:\n  Dcnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nelse:\n  Dcnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","3a9ebb48":"history = Dcnn.fit(train_inputs, train_labels,\n                   batch_size = batch_size,\n                   epochs = nb_epochs,\n                   verbose = 1,\n                   validation_split = 0.10)","ba2c28df":"results = Dcnn.evaluate(test_inputs, test_labels, batch_size=batch_size)\nprint(results)","06608f16":"y_pred_test = Dcnn.predict(test_inputs)","b9a20e05":"y_pred_test","b4edac90":"y_pred_test = (y_pred_test > 0.5)\ny_pred_test","68a22ea4":"test_labels","fe939c67":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(test_labels, y_pred_test)\ncm","c645380a":"sns.heatmap(cm, annot=True)","f639f10c":"history.history.keys()","3d3bcbfa":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss progress during training and validation')\nplt.xlabel('Epoch')\nplt.ylabel('Losses')\nplt.legend(['Training loss', 'Validation loss'])","9112dfbb":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy progress during training and validation')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(['Training accuracy', 'Validation accuracy'])","a0216a58":"text = \"I hate\"\ntext = tokenizer.encode(text)\nDcnn(np.array([text]), training=False).numpy()","ad7629e7":"text = \"I happy\"\ntext = tokenizer.encode(text)\nDcnn(np.array([text]), training=False).numpy()","e77a9780":"text = \"It is complicated\"\ntext = tokenizer.encode(text)\nDcnn(np.array([text]), training=False).numpy()","48448166":"''''text = str(input('write here:   '))\ntext = tokenizer.encode(text)\ntext =  Dcnn(np.array([text]), training=False).numpy()\nif text >= 0.7:\n  print('positivo');\nelif text >= 0.4 and text <= 0.69:\n    print('neutral')\nelse:\n  print('negativo')'''","677f3510":"## Analyzing","c5ac8bd2":"# If you find this notebook useful, support with an upvote \ud83d\udc4d","bc5975d2":"# \"My ridiculous dog is amazing.\" [sentiment: positive]\n\nWith all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. But, which words actually lead to the sentiment description? In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment.\n\nHelp build your skills in this important area with this broad dataset of tweets. Work on your technique to grab a top spot in this competition. What words in tweets support a positive, negative, or neutral sentiment? How can you help make that determination using machine learning tools?\n\nIn this competition we've extracted support phrases from Figure Eight's Data for Everyone platform. The dataset is titled Sentiment Analysis: Emotion in Text tweets with existing sentiment labels, used here under creative commons attribution 4.0. international licence. Your objective in this competition is to construct a model that can do the same - look at the labeled sentiment for a given tweet and figure out what word or phrase best supports it.\n\nDisclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.\n\n\nhttps:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction","fbab0648":"## Import from Libraries","768eaf15":"## Model building","ba8a1c94":"## Forecasts","4868ec63":"## Tokenization","dd9ddc88":"**Here I created a small function with an algorithm, as the word is placed it will say if it is positive, neutral or negative.**","ea284042":"## Division of database into training and testing","1f1de1fb":"## Training","e06b72ff":"## Model Evaluation","456c1469":"# Conclusion\n\nThere is a small difficulty for the algorithm to identify the negative words, as there is a balance between the neutral and positive classes, the algorithm often understands the neutral as positive, as there is no negative word for it to relate to neutrality.\nPerhaps if we had a larger database for training the algorithm we could improve it with more vocabulary words, certainly the hit margin will be much better with a broader learning of the database, even so we had a great result with some adjustments to the code.","7f4611af":"## Padding","aa53c629":"## Uploading files"}}