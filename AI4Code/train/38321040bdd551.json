{"cell_type":{"1d13c885":"code","0408e9c1":"code","33737dde":"code","5266842a":"code","14e89961":"code","bc8d4dbb":"code","6f1dda72":"code","d5baf502":"code","d9436638":"code","6f8d4616":"code","4e5cbb0c":"code","d3f55c9e":"code","b252d2bc":"code","55c12241":"code","afef53d9":"code","8b2e3683":"code","a7994c8b":"code","ac3058db":"code","0f233f17":"code","ccc34a83":"code","af45336d":"code","ae7702d9":"code","4f21a8b1":"code","0d3a4df9":"code","b3e5344f":"code","564ff354":"code","3d2b7df5":"code","82cb0631":"markdown","f8ff1f63":"markdown","0bd1d171":"markdown","436bdaed":"markdown","dc96bf20":"markdown","6a53b2c7":"markdown"},"source":{"1d13c885":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\nfrom IPython.display import set_matplotlib_formats\n%matplotlib inline\nset_matplotlib_formats('svg')\n\n%matplotlib inline\nsns.set_style(\"whitegrid\")","0408e9c1":"train_data = pd.read_json('..\/input\/train.json.zip', compression='zip')\ntrain_data.head()","33737dde":"# \u7f3a\u5931\u503c\ntrain_data.isnull().sum()","5266842a":"# \u6807\u7b7e\u5206\u5e03\nsns.countplot(train_data.interest_level, order=['low', 'medium', 'high']);\nplt.xlabel('Interest Level');\nplt.ylabel('Number of occurrences');","14e89961":"# \u7528map\u4e5f\u53ef\u4ee5\u5b9e\u73b0\ntrain_data['interest'] = np.where(train_data.interest_level=='low', 0,\n                                  np.where(train_data.interest_level=='medium', 1, 2))","bc8d4dbb":"### Bathrooms graphs\nfig = plt.figure(figsize=(12,12))\nsns.countplot(train_data.bathrooms, ax = plt.subplot(221));\nplt.xlabel('Number of Bathrooms');\nplt.ylabel('Number of occurrences');\n\n### Average number of Bathrooms per Interest Level\nsns.barplot(x='interest_level', y='bathrooms', data=train_data, order=['low', 'medium', 'high'],\n            ax = plt.subplot(222));\nplt.xlabel('Interest Level');\nplt.ylabel('Average Number of Bathrooms');\n\n### Average interest for every number of bathrooms\nsns.pointplot(x=\"bathrooms\", y=\"interest\", data=train_data, ax = plt.subplot(212));\nplt.xlabel('Number of Bathrooms');\nplt.ylabel('Average Interest');","6f1dda72":"### Bedrooms graphs\nfig = plt.figure(figsize=(12,12))\nsns.countplot(train_data.bedrooms, ax = plt.subplot(221));\nplt.xlabel('Number of Bedrooms');\nplt.ylabel('Number of occurrences');\n\n### Average number of Bedrooms per Interest Level\nsns.barplot(x='interest_level', y='bedrooms', data=train_data, order=['low', 'medium', 'high'],\n            ax = plt.subplot(222));\nplt.xlabel('Interest Level');\nplt.ylabel('Average Number of Bedrooms');\n\n### Average interest for every number of bedrooms\nsns.pointplot(x=\"bedrooms\", y=\"interest\", data=train_data, ax = plt.subplot(212));\nplt.xlabel('Number of Bedrooms');\nplt.ylabel('Average Interest');","d5baf502":"### Most advertised buildings\ntrain_data.building_id.value_counts().nlargest(10)","d9436638":"### Convertion to Python Date\ntrain_data.created = pd.to_datetime(train_data.created, format='%Y-%m-%d %H:%M:%S')","6f8d4616":"### New Month, Day of Week and Hour Features\ntrain_data['month'] = train_data.created.dt.month\ntrain_data['day_of_week'] = train_data.created.dt.weekday\ntrain_data['hour'] = train_data.created.dt.hour","4e5cbb0c":"### Iterest per month\nfig = plt.figure(figsize=(12,6))\nax = sns.countplot(x=\"month\", hue=\"interest_level\", hue_order=['low', 'medium', 'high'],\n                   data=train_data);\nplt.xlabel('Month');\nplt.ylabel('Number of occurrences')\n\n### Adding percents over bars\nheight = [p.get_height() for p in ax.patches]\nncol = int(len(height)\/3)\ntotal = [height[i] + height[i + ncol] + height[i + 2*ncol] for i in range(ncol)] * 3\nfor i, p in enumerate(ax.patches):    \n    ax.text(p.get_x()+p.get_width()\/2,\n            height[i] + 50,\n            '{:1.0%}'.format(height[i]\/total[i]),\n            ha=\"center\") ","d3f55c9e":"### Iterest per Day of Week\nfig = plt.figure(figsize=(12,6))\nax = sns.countplot(x=\"day_of_week\", hue=\"interest_level\",\n                   hue_order=['low', 'medium', 'high'], data=train_data);\nplt.xlabel('Day of Week');\nplt.ylabel('Number of occurrences');\n\n### Adding percents over bars\nheight = [p.get_height() for p in ax.patches]\nncol = int(len(height)\/3)\ntotal = [height[i] + height[i + ncol] + height[i + 2*ncol] for i in range(ncol)] * 3\nfor i, p in enumerate(ax.patches):    \n    ax.text(p.get_x()+p.get_width()\/2,\n            height[i] + 50,\n            '{:1.0%}'.format(height[i]\/total[i]),\n            ha=\"center\") ","b252d2bc":"### Iterest per Day of Week\nfig = plt.figure(figsize=(12,6))\nsns.countplot(x=\"hour\", hue=\"interest_level\", hue_order=['low', 'medium', 'high'], data=train_data);\nplt.xlabel('Hour');\nplt.ylabel('Number of occurrences');","55c12241":"### Number of unique Display Addresses\nprint('Number of Unique Display Addresses is {}'.format(train_data.display_address.value_counts().shape[0]))","afef53d9":"### 15 most popular Display Addresses\ntrain_data.display_address.value_counts().nlargest(15)","8b2e3683":"### Top 20 northernmost points\ntrain_data.latitude.nlargest(20)","a7994c8b":"### Top 20 southernmost points\ntrain_data.latitude.nsmallest(20)","ac3058db":"### Top 20 easternmost points\ntrain_data.longitude.nlargest(20)","0f233f17":"### Top 20 westernmost points\ntrain_data.longitude.nsmallest(20)","ccc34a83":"### Rent interest graph of New-York\nsns.lmplot(x=\"longitude\", y=\"latitude\", fit_reg=False, hue='interest_level',\n           hue_order=['low', 'medium', 'high'], size=9, scatter_kws={'alpha':0.4,'s':30},\n           data=train_data[(train_data.longitude>train_data.longitude.quantile(0.005))\n                           &(train_data.longitude<train_data.longitude.quantile(0.995))\n                           &(train_data.latitude>train_data.latitude.quantile(0.005))                           \n                           &(train_data.latitude<train_data.latitude.quantile(0.995))]);\nplt.xlabel('Longitude');\nplt.ylabel('Latitude');","af45336d":"### Let's get a list of top 10 managers\ntop10managers = train_data.manager_id.value_counts().nlargest(10).index.tolist()\n### ...and plot number of different Interest Level rental adverts for each of them\nfig = plt.figure(figsize=(12,6))\nax = sns.countplot(x=\"manager_id\", hue=\"interest_level\",\n                   data=train_data[train_data.manager_id.isin(top10managers)]);\nplt.xlabel('Manager');\nplt.ylabel('Number of advert occurrences');\n### Manager_ids are too long. Let's remove them\nplt.tick_params(\n    axis='x',          # changes apply to the x-axis\n    which='both',      # both major and minor ticks are affected\n    bottom='off',      # ticks along the bottom edge are off\n    top='off',         # ticks along the top edge are off\n    labelbottom='off');\n\nplt.xticks([])\n### Adding percents over bars\nheight = [0 if np.isnan(p.get_height()) else p.get_height() for p in ax.patches]\nncol = int(len(height)\/3)\ntotal = [height[i] + height[i + ncol] + height[i + 2*ncol] for i in range(ncol)] * 3\nfor i, p in enumerate(ax.patches):    \n    ax.text(p.get_x()+p.get_width()\/2,\n            height[i] + 20,\n            '{:1.0%}'.format(height[i]\/total[i]),\n            ha=\"center\")\n    ","ae7702d9":"### Getting number of photos\ntrain_data['photos_number'] = train_data.photos.str.len()","4f21a8b1":"### Number of photos graphs\nfig = plt.figure(figsize=(12,12))\n### Average number of Photos per Interest Level\nsns.barplot(x=\"interest_level\", y=\"photos_number\", order=['low', 'medium', 'high'],\n            data=train_data, ax=plt.subplot(221));\nplt.xlabel('Interest Level');\nplt.ylabel('Average Number of Photos');\n### Average interest for every number of photos\nsns.barplot(x=\"photos_number\", y=\"interest\", data=train_data, ax=plt.subplot(222));\nplt.xlabel('Number of Photos');\nplt.ylabel('Average Interest');\n### Number of occurrences\nsns.countplot(x='photos_number', hue='interest_level', hue_order=['low', 'medium', 'high'],\n              data=train_data, ax=plt.subplot(212));\nplt.xlabel('Number of Photos');\nplt.ylabel('Number of occurrences');","0d3a4df9":"### Price exploration\nfig = plt.figure(figsize=(12,12))\n### Price distribution\nsns.distplot(train_data.price[train_data.price<=train_data.price.quantile(0.99)], ax=plt.subplot(211));\nplt.xlabel('Price');\nplt.ylabel('Density');\n\n### Average Price per Interest Level\nsns.barplot(x=\"interest_level\", y=\"price\", order=['low', 'medium', 'high'],\n            data=train_data, ax=plt.subplot(223));\nplt.xlabel('Interest Level');\nplt.ylabel('Price');\n\n### Violinplot of price for every Interest Level\nsns.violinplot(x=\"interest_level\", y=\"price\", order=['low', 'medium', 'high'],\n               data=train_data[train_data.price<=train_data.price.quantile(0.99)],\n               ax=plt.subplot(224));\nplt.xlabel('Interest Level');\nplt.ylabel('Price');","b3e5344f":"import numpy as np\nimport pandas as pd\nfrom scipy import sparse\nimport random\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import log_loss\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ntrain_df = pd.read_json(\"..\/input\/train.json.zip\", compression='zip')\ntest_df = pd.read_json(\"..\/input\/test.json.zip\", compression='zip')\n\ntest_df[\"bathrooms\"].loc[19671] = 1.5\ntest_df[\"bathrooms\"].loc[22977] = 2.0\ntest_df[\"bathrooms\"].loc[63719] = 2.0\ntrain_df[\"price\"] = train_df[\"price\"].clip(upper=13000)\n\ntrain_df[\"logprice\"] = np.log(train_df[\"price\"])\ntest_df[\"logprice\"] = np.log(test_df[\"price\"])\n\n\ntrain_df['half_bathrooms'] = train_df[\"bathrooms\"] - train_df[\"bathrooms\"].apply(int)#.astype(int) # Half bathrooms? 1.5, 2.5, 3.5...\ntest_df['half_bathrooms'] = test_df[\"bathrooms\"] - test_df[\"bathrooms\"].apply(int)#.astype(int) # Half bathrooms? 1.5, 2.5, 3.5...\n\ntrain_df[\"price_t\"] =train_df[\"price\"]\/train_df[\"bedrooms\"]\ntest_df[\"price_t\"] = test_df[\"price\"]\/test_df[\"bedrooms\"] \n\ntrain_df[\"room_sum\"] = train_df[\"bedrooms\"]+train_df[\"bathrooms\"] \ntest_df[\"room_sum\"] = test_df[\"bedrooms\"]+test_df[\"bathrooms\"] \n\ntrain_df['price_per_room'] = train_df['price']\/train_df['room_sum']\ntest_df['price_per_room'] = test_df['price']\/test_df['room_sum']\n\ntrain_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\ntest_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n\ntrain_df[\"num_features\"] = train_df[\"features\"].apply(len)\ntest_df[\"num_features\"] = test_df[\"features\"].apply(len)\n\ntrain_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\ntest_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n\ntrain_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\ntest_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\ntrain_df[\"created_year\"] = train_df[\"created\"].dt.year\ntest_df[\"created_year\"] = test_df[\"created\"].dt.year\ntrain_df[\"created_month\"] = train_df[\"created\"].dt.month\ntest_df[\"created_month\"] = test_df[\"created\"].dt.month\ntrain_df[\"created_day\"] = train_df[\"created\"].dt.day\ntest_df[\"created_day\"] = test_df[\"created\"].dt.day\ntrain_df[\"created_hour\"] = train_df[\"created\"].dt.hour\ntest_df[\"created_hour\"] = test_df[\"created\"].dt.hour\n\ntrain_df[\"created_weekday\"] = train_df[\"created\"].dt.weekday\ntest_df[\"created_weekday\"] = test_df[\"created\"].dt.weekday\ntrain_df[\"created_week\"] = train_df[\"created\"].dt.week\ntest_df[\"created_week\"] = test_df[\"created\"].dt.week\n\ntrain_df[\"pos\"] = train_df.longitude.round(3).astype(str) + '_' + train_df.latitude.round(3).astype(str)\ntest_df[\"pos\"] = test_df.longitude.round(3).astype(str) + '_' + test_df.latitude.round(3).astype(str)\n\nvals = train_df['pos'].value_counts()\ndvals = vals.to_dict()\ntrain_df[\"density\"] = train_df['pos'].apply(lambda x: dvals.get(x, vals.min()))\ntest_df[\"density\"] = test_df['pos'].apply(lambda x: dvals.get(x, vals.min()))\n\nfeatures_to_use=[\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\",\"price_t\",\"price_per_room\", \"logprice\", \"density\", \"half_bathrooms\",\n\"num_photos\", \"num_features\", \"num_description_words\",\"listing_id\", \"created_year\", \"created_month\", \"created_day\", \"created_hour\", \"created_week\", \"created_weekday\"]\n\nindex=list(range(train_df.shape[0]))\nrandom.shuffle(index)\na=[np.nan]*len(train_df)\nb=[np.nan]*len(train_df)\nc=[np.nan]*len(train_df)\n\nfor i in range(5):\n    building_level={}\n    for j in train_df['manager_id'].values:\n        building_level[j]=[0,0,0]\n    \n    test_index=index[int((i*train_df.shape[0])\/5):int(((i+1)*train_df.shape[0])\/5)]\n    train_index=list(set(index).difference(test_index))\n    \n    for j in train_index:\n        temp=train_df.iloc[j]\n        if temp['interest_level']=='low':\n            building_level[temp['manager_id']][0]+=1\n        if temp['interest_level']=='medium':\n            building_level[temp['manager_id']][1]+=1\n        if temp['interest_level']=='high':\n            building_level[temp['manager_id']][2]+=1\n            \n    for j in test_index:\n        temp=train_df.iloc[j]\n        if sum(building_level[temp['manager_id']])!=0:\n            a[j]=building_level[temp['manager_id']][0]*1.0\/sum(building_level[temp['manager_id']])\n            b[j]=building_level[temp['manager_id']][1]*1.0\/sum(building_level[temp['manager_id']])\n            c[j]=building_level[temp['manager_id']][2]*1.0\/sum(building_level[temp['manager_id']])\n            \ntrain_df['manager_level_low']=a\ntrain_df['manager_level_medium']=b\ntrain_df['manager_level_high']=c\n\na=[]\nb=[]\nc=[]\nbuilding_level={}\nfor j in train_df['manager_id'].values:\n    building_level[j]=[0,0,0]\n\nfor j in range(train_df.shape[0]):\n    temp=train_df.iloc[j]\n    if temp['interest_level']=='low':\n        building_level[temp['manager_id']][0]+=1\n    if temp['interest_level']=='medium':\n        building_level[temp['manager_id']][1]+=1\n    if temp['interest_level']=='high':\n        building_level[temp['manager_id']][2]+=1\n\nfor i in test_df['manager_id'].values:\n    if i not in building_level.keys():\n        a.append(np.nan)\n        b.append(np.nan)\n        c.append(np.nan)\n    else:\n        a.append(building_level[i][0]*1.0\/sum(building_level[i]))\n        b.append(building_level[i][1]*1.0\/sum(building_level[i]))\n        c.append(building_level[i][2]*1.0\/sum(building_level[i]))\ntest_df['manager_level_low']=a\ntest_df['manager_level_medium']=b\ntest_df['manager_level_high']=c\n\nfeatures_to_use.append('manager_level_low') \nfeatures_to_use.append('manager_level_medium') \nfeatures_to_use.append('manager_level_high')\n\ncategorical = [\"street_address\", \"display_address\", \"manager_id\", \"building_id\"]\nfor f in categorical:\n        if train_df[f].dtype=='object':\n            lbl = LabelEncoder()\n            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n            train_df[f] = lbl.transform(list(train_df[f].values))\n            test_df[f] = lbl.transform(list(test_df[f].values))\n            features_to_use.append(f)\n\ntrain_df['features'] = train_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\ntest_df['features'] = test_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n\ntfidf = CountVectorizer(stop_words='english', max_features=200)\ntr_sparse = tfidf.fit_transform(train_df[\"features\"])\nte_sparse = tfidf.transform(test_df[\"features\"])\n\ntrain_X = sparse.hstack([train_df[features_to_use], tr_sparse]).tocsr()\ntest_X = sparse.hstack([test_df[features_to_use], te_sparse]).tocsr()\n\ntarget_num_map = {'high':0, 'medium':1, 'low':2}\ntrain_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))","564ff354":"from sklearn.model_selection import StratifiedKFold\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cab\n\nskf = StratifiedKFold(n_splits=5)\ntest_pred = np.zeros((test_X.shape[0], 3))\n\nfor train_idx, val_idx in skf.split(train_X, train_y):\n    print(train_idx, val_idx)\n    \n    # https:\/\/lightgbm.readthedocs.io\/en\/latest\/Python-API.html\n    clf_lgb = lgb.LGBMClassifier(\n        n_estimators = 1500,\n        max_depth = 6,\n        learning_rate = 0.02,\n        objective = 'multiclass',\n        eval_metric = 'multi_logloss',\n        num_class = 3,\n        n_jobs = -1\n    )\n    clf_lgb.fit(train_X[train_idx], train_y[train_idx], \n            eval_set = (train_X[val_idx], train_y[val_idx]),\n            eval_metric = 'multi_logloss',\n            early_stopping_rounds = 50,\n            verbose = 50\n    )\n    test_pred += clf_lgb.predict_proba(test_X)\n    \n    # https:\/\/xgboost.readthedocs.io\/en\/latest\/\/python\/python_api.html\n    clf_xgb = xgb.XGBClassifier(\n        n_estimators = 1500,\n        max_depth = 6,\n        learning_rate = 0.02,\n        objective = 'multi:softprob',\n        eval_metric = 'mlogloss',\n        num_class = 3,\n        n_jobs = -1\n    )\n    clf_xgb.fit(train_X[train_idx], train_y[train_idx], \n            eval_set = [(train_X[val_idx], train_y[val_idx])],\n            eval_metric = 'mlogloss',\n            early_stopping_rounds = 50,\n            verbose = 50\n    )\n    test_pred += clf_xgb.predict_proba(test_X)\n    \n    # https:\/\/catboost.ai\/docs\/concepts\/python-quickstart.html\n    clf_cab = cab.CatBoostClassifier(\n        iterations = 1500,\n        depth = 6,\n        learning_rate = 0.02,\n        loss_function = 'MultiClass',\n        classes_count = 3,\n        thread_count = -1\n    )\n    clf_cab.fit(train_X[train_idx], train_y[train_idx], \n            eval_set = (train_X[val_idx], train_y[val_idx]),\n            early_stopping_rounds = 50,\n            verbose = 50\n    )\n    test_pred += clf_cab.predict_proba(test_X)\n    \n    break","3d2b7df5":"out_df = pd.DataFrame(test_pred\/3)\nout_df.columns = [\"high\", \"medium\", \"low\"]\nout_df[\"listing_id\"] = test_df.listing_id.values\nout_df.to_csv(\"cz.csv\", index=False)","82cb0631":"# \u7279\u5f81\u5de5\u7a0b","f8ff1f63":"# Bedrooms","0bd1d171":"# Bathrooms","436bdaed":"![image.png](attachment:image.png)\n\n- https:\/\/matplotlib.org\/gallery\/index.html\n- https:\/\/seaborn.pydata.org\/","dc96bf20":"# building_id","6a53b2c7":"# created"}}