{"cell_type":{"3c5b6426":"code","5b664ecf":"code","73ecc586":"code","a571cd38":"code","312f7f8c":"code","9c6d49a2":"code","6445ba8c":"code","f52e61f9":"code","089bae4b":"code","1ef5cb2b":"code","cc47610e":"code","72e7b52d":"code","2e1e9834":"code","2b5fe4a8":"code","ea0ce23c":"code","ab39369d":"code","72e21a9e":"code","f8f72518":"code","33378036":"markdown","9208fe34":"markdown","84677d6b":"markdown","6f24fd7a":"markdown","edd403df":"markdown","424d1f72":"markdown","afff8086":"markdown","17f73514":"markdown","a6d1e6e8":"markdown","4d95938c":"markdown","336f1f33":"markdown","fbf7e1c1":"markdown","e2ae99e8":"markdown","c4ea066a":"markdown","d882ac2e":"markdown","f2231fd5":"markdown","ad2016c7":"markdown","82959947":"markdown","a14de09f":"markdown","aef1b835":"markdown"},"source":{"3c5b6426":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split,StratifiedKFold\n\nfrom sklearn.metrics import roc_auc_score\nimport optuna\n\nplt.style.use('fivethirtyeight')","5b664ecf":"train=pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/train.csv')\ntest=pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/test.csv')","73ecc586":"print(f'Number of rows in training set: {train.shape[0]}')\nprint(f'Number of features in training set: {train.shape[1]}')\nprint(f'Number of rows in test set: {test.shape[0]}')\nprint(f'Number of features in test set: {test.shape[1]}')","a571cd38":"# checking the number of categorical and continuous variables\ntrain.columns","312f7f8c":"# separating categorical columns from continuous ones\ncat_var=[f'cat{i}' for i in range(19)]\ncont_var=[f'cont{i}' for i in range(11)]\ncolumns=cat_var+cont_var","9c6d49a2":"plt.figure(figsize=(18,8))\nplt.subplot(1,2,1)\ntrain.target.value_counts().plot.pie(explode=[0,0.2])\nplt.subplot(1,2,2)\nsns.countplot(train.target)\nplt.suptitle(\"Target Balance\", size=32)\nplt.show()","6445ba8c":"i=1\nplt.figure(figsize=(18,30))\nfor cat in cat_var:\n    plt.subplot(5,4,i)\n    sns.countplot(x=cat,data=train)\n    i+=1\n    plt.tight_layout()\n\nplt.show()","f52e61f9":"fig = plt.figure(figsize=(30,50))\ni=1\nfor cont in cont_var:\n    plt.subplot(11, 3, i)\n    sns.histplot(train[cont])\n    i+=1\n    \n    plt.subplot(11, 3, i)\n    plt.boxplot(x = train[cont])\n    i+=1\n\n    plt.subplot(11, 3, i)\n    sns.violinplot(data = train, x = 'target', y = cont)\n    i+=1\n\n    plt.tight_layout()\n\nplt.show()","089bae4b":"sns.pairplot(train[cont_var+['target']],corner=True,hue='target')","1ef5cb2b":"plt.figure(figsize=(15,10))\nsns.heatmap(train[cont_var].corr(),annot=True,cmap='RdYlGn',linewidths=0.2) \nplt.show()","cc47610e":"# Simple preprocessing using OnehotEncoder\n\nfull=pd.concat([train,test],axis=0)\n\nfull=pd.get_dummies(full,columns=cat_var)\n\n#for cat in cat_var:\n#    le=LabelEncoder()\n#    full[cat]=le.fit_transform(full[cat])\n    \ntrain=full.iloc[:len(train),:]\ntest=full.iloc[len(train):,:]\n\ncolumns=[column for column in train.columns if column not in ['id','target']]","72e7b52d":"X=train[columns]\ny=train.target","2e1e9834":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)","2b5fe4a8":"lgb=LGBMClassifier()\nlgb.fit(X_train,y_train,eval_set=(X_test,y_test),early_stopping_rounds=200, verbose=False)\npredictions=lgb.predict_proba(X_test)[:,1]\n\nauc=roc_auc_score(y_test,predictions)\n\nprint(f'Baseline Score: {auc}')","ea0ce23c":"lgb_params={'learning_rate': 0.00605886703283976,\n 'max_depth': 42,\n 'num_leaves': 108,\n 'reg_alpha': 0.9140720355379223,\n 'reg_lambda': 9.97396811596188,\n 'colsample_bytree': 0.2629101393563821,\n 'min_child_samples': 61,\n 'subsample_freq': 2,\n 'subsample': 0.8329687190743886,\n 'max_bin': 899,\n 'min_data_per_group': 73,\n 'cat_smooth': 21,\n 'cat_l2': 11,\n            'random_state': 2021,\n            'metric': 'auc',\n            'n_estimators': 20000,\n            'n_jobs': -1,\n            'bagging_seed': 2021,\n            'feature_fraction_seed': 2021\n           }","ab39369d":"f1= 0.7434828307047571 \nf2= 1.3786330168495677\nf3= 46\nf4= 27","72e21a9e":"%%time\n\nkf=StratifiedKFold(n_splits=5,random_state=48,shuffle=True)\n\n# we will store our final predictions in preds\npreds = np.zeros(test.shape[0])\n#store rmse of each iterations\nauc=[]\ni=0\n\n# --------------------------------------------------------------------------------\n# Phase 1: create the pretrained model\nfor idx_train,idx_test in kf.split(X,y):\n    \n    X_train,X_test=X.iloc[idx_train],X.iloc[idx_test]\n    y_train,y_test=y.iloc[idx_train],y.iloc[idx_test]\n\n    \n    model=LGBMClassifier(**lgb_params)\n    \n    model.fit(X_train,y_train,eval_set=(X_test,y_test),early_stopping_rounds=300,verbose=False,eval_metric='auc')\n    \n    predictions=model.predict_proba(X_test,num_iteration=model.best_iteration_)[:,1]\n    \n    auc.append(roc_auc_score(y_test,predictions))\n    \n    print('First Round:')\n    \n    print(f'RMSE {auc[i]}')\n    \n    auc_tuned=[]\n    params = lgb_params.copy()\n    \n    # -----------------------------------------------------------------------------\n    # Phase 2: iterations where we decrease the learning rate and regularization params    \n    for t in range(1,18):\n        \n        \n        if t >1:    \n                    \n            params['reg_lambda'] *=  f1\n            params['reg_alpha'] += f2\n            params['num_leaves'] += f3\n            params['min_child_samples'] -= f4\n        \n        if params['min_child_samples']<1:\n            params['min_child_samples']=1\n            \n           \n        params['learning_rate']=0.003\n        \n              \n        model=LGBMClassifier(**params).fit(X_train,y_train,eval_set=(X_test,y_test),eval_metric='auc',early_stopping_rounds=200,verbose=False,init_model=model)\n        \n        predictions=model.predict_proba(X_test, num_iteration= model.best_iteration_)[:,1]\n        \n        auc_tuned.append(roc_auc_score(y_test,predictions))\n        \n        print(f'RMSE tuned {t}: {auc_tuned[t-1]}')\n        \n    print(f'Improvement of {auc_tuned[t-1]-auc[i]}')\n    \n    # ---------------------------------------------------------------------------\n    # Inference time: calculate predictions for test set\n    \n    preds+=model.predict_proba(test[columns],num_iteration=model.best_iteration_)[:,1]\/kf.get_n_splits()\n        \n    i+=1","f8f72518":"# Create submission file\ntest['target']=preds\ntest=test[['id','target']]\ntest.to_csv('submission.csv',index=False)","33378036":"# Exploring interactions between continuous features","9208fe34":"# Exploring continuous variables","84677d6b":"# Exploring the relationship between continuous variables and target variables","6f24fd7a":"# Extreme tuning strategy","edd403df":"def objective(trial,X=X,y=y):\n    \n    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n    \n    \n    lgb_params={\n        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2),\n        'max_depth': trial.suggest_int('max_depth', 6, 127),\n        'num_leaves': trial.suggest_int('num_leaves', 31, 128),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0),\n        'random_state': 2021,\n        'metric': 'auc',\n        'n_estimators': 20000,\n        'n_jobs': -1,\n        'cat_feature': [x for x in range(len(cat_var))],\n        'bagging_seed': 2021,\n        'feature_fraction_seed': 2021,\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 0.9),\n        'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n        'subsample_freq': trial.suggest_int('subsample_freq', 1, 10),\n        'subsample': trial.suggest_float('subsample', 0.3, 0.9),\n        'max_bin': trial.suggest_int('max_bin', 128, 1024),\n        'min_data_per_group': trial.suggest_int('min_data_per_group', 50, 200),\n        'cat_smooth': trial.suggest_int('cat_smooth', 10, 100),\n        'cat_l2': trial.suggest_int('cat_l2', 1, 20)}\n   \n    lgb=LGBMClassifier(**lgb_params)\n    lgb.fit(X_train,y_train,eval_set=(X_test,y_test),eval_metric='auc',early_stopping_rounds=100,verbose=False)\n    predictions=lgb.predict_proba(X_test)[:,1]\n        \n    return roc_auc_score(y_test,predictions)","424d1f72":"study.best_params","afff8086":"# Hyperparameter tuning using Optuna","17f73514":"# Preprocessing","a6d1e6e8":"study = optuna.create_study(direction='maximize') \nstudy.optimize(objective, timeout=3600*7)","4d95938c":"# Exploratory Data Analysis","336f1f33":"In this notebook I did a fast exploratory data analysis. Then, I tuned lgbm hyperparameters using Optuna. Finally, I use a small trick that enables me to obtain a 15th place in last tabular playground.","fbf7e1c1":"cat7, cat8 and cat10 have very high cardinality. We will need to explore thme later.","e2ae99e8":"# Making final submission","c4ea066a":"# Exploring the target variable","d882ac2e":"# Explore correlation between continuous features","f2231fd5":"# Imports","ad2016c7":"# Welcome to the third tabular playground of 2021","82959947":"# LGBM Baseline","a14de09f":"# Thanks for reading","aef1b835":"There are more columns than in the previous tabular playground."}}