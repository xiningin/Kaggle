{"cell_type":{"7b188e66":"code","45fc6389":"code","9033d1b0":"code","a85057e4":"code","ec7b578b":"code","fcdff70c":"code","ff0c293d":"code","289e9818":"code","c3d0b0dc":"code","9fb99d83":"code","b32f850f":"code","4778ddaf":"code","cc859f4d":"code","384c0a5e":"code","4ddc7f56":"code","b69e1771":"code","94cbd49b":"code","9d17c67f":"code","5d63c198":"code","b7450b4e":"code","38fadfae":"markdown","485753cd":"markdown","8f441e25":"markdown","c3cb9a5a":"markdown","bccf122b":"markdown","515b097c":"markdown","f911ec5d":"markdown","962df7d9":"markdown","107a7e6c":"markdown"},"source":{"7b188e66":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import tree\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Original dataset\ndf_orig = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv', index_col='id')\ny = df_orig['stroke'].copy()\nX_orig = df_orig.drop(columns=['stroke'], inplace=False)\nnum_feat = len(X_orig.columns) \nnum_obj = len(X_orig)\n# Cross-validator\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\nX_orig.head()","45fc6389":"# Stroke pie chart\nplt.figure(figsize=(10, 6))\nplt.pie(y.value_counts(), labels=['1', '0'], autopct='%.1f%%', shadow=True, explode=(0, 0.3));  \nplt.title('Pie chart of the \"stroke\" column\\n \\\n    \"1\" $-$ patient had a stroke,\\n \\\n    \"0\" $-$ patient did not have a stroke')","9033d1b0":"# Stroke \u043e\u0442 age\nplt.figure(figsize=(7, 5))\nage_stroke = X_orig['age'].loc[y==1]\nsns.histplot(x=age_stroke, bins=40, kde=True)\nplt.xlabel('Age, years')\nplt.ylabel('Number of strokes')\nplt.grid()","a85057e4":"# Age distribution\nplt.figure(figsize=(7, 5))\nsns.histplot(x=X_orig['age'], hue=X_orig['gender'], element='step', legend=True)\nplt.xlabel('Age, years')\nplt.ylabel('Number of people')\nplt.grid()","ec7b578b":"age_stroke = X_orig['age'].loc[y==1]\nfig, ax = plt.subplots(figsize=(7, 5))\nsns.histplot(x=X_orig['age'], ax=ax, stat='density', kde=True, element='step')\nsns.histplot(x=age_stroke, ax=ax, bins=20, stat='density', kde=True, element='step', color='g')\nax.set_xlabel('age, years')\nax.grid()","fcdff70c":"fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n# Glucose\nsns.histplot(X_orig['avg_glucose_level'], ax=ax[0], bins=30, kde=True)\nax[0].set_xlabel('Average glucose level in blood')\nax[0].axvline(X_orig['avg_glucose_level'].median(), label='Median', linestyle='--', color='r', lw=3)\nax[0].axvline(X_orig['avg_glucose_level'].mean(), label='Mean', linestyle='--', color='g', lw=3)\nax[0].legend()\nax[0].grid()\n# BMI\nsns.histplot(x=X_orig['bmi'], ax=ax[1], bins=30, kde=True, color='tab:orange')\nax[1].set_xlabel('Body mass index')\nax[1].axvline(X_orig['bmi'].median(), label='Median', linestyle='--', color='r', lw=2.5)\nax[1].axvline(X_orig['bmi'].mean(), label='Mean', linestyle='--', color='g', lw=2.5)\nax[1].legend()\nax[1].grid()\n","ff0c293d":"# Smoking status\nplt.figure(figsize=(10, 6))\nsns.countplot(y=X_orig['smoking_status'])\nplt.ylabel('Smoking status')","289e9818":"fig, ax  = plt.subplots(1, 2, figsize=(10, 6))\nax[0].pie(X_orig['heart_disease'].value_counts(), labels=['1', '0'], autopct='%.1f%%', shadow=True, explode=(0, 0.3));  \nax[0].set_title('\"1\" $-$ patient has a heart disease,\\n \\\n    \"0\" $-$ patient does not have any heart diseases')\nax[1].pie(X_orig['hypertension'].value_counts(), labels=['1', '0'], autopct='%.1f%%', shadow=True, explode=(0, 0.3));  \nax[1].set_title('\"1\" $-$ patient has hypertension,\\n \\\n    \"0\" $-$ patient does not have hypertension')\n","c3d0b0dc":"def ohe(df, features):\n    \"\"\"\n    one-hot enccoder.\n    df -- input DataFrame\n    features -- list of features to be encoded\n\n    One can easily use sklearn.preprocessing.OneHotEncoder instead\n    \"\"\"\n    for feat in features:\n        categ_list = df[feat].unique()\n        df_enc = np.zeros((df.shape[0], len(categ_list)))\n        for ii in range(len(categ_list)):\n            df_enc[:, ii] = (df[feat]==categ_list[ii]).astype(int) \n\n        df_enc = pd.DataFrame(data=df_enc, index=df.index, columns=categ_list)\n        df = pd.concat([df, df_enc], axis=1)\n    return df\n\nX_prep = X_orig.copy()\n# Encode categorial features\n# Male - 1, Female - 0\nX_prep['gender'] = X_prep['gender'].map({'Male': 1, 'Female': 0, 'Other': 1})\n# urban - 1, rural - 0\nX_prep['Residence_type'] = X_prep['Residence_type'].map({'Urban': 1, 'Rural': 0})\n# ever_married yes - 1, no - 0\nX_prep['ever_married'] = X_prep['ever_married'].map({'Yes': 1, 'No': 0})\n# One-hot encoder for work_type, smoking_status\nX_prep = ohe(X_prep, ['work_type', 'smoking_status'])\nX_prep.drop(columns=['work_type', 'smoking_status'], inplace=True)\n# Fill the NaN in BMI column\nX_prep['bmi'].fillna(X_prep['bmi'].median(), inplace=True)\n# Scaling\nscaler = StandardScaler()\nX_prep = pd.DataFrame(data=scaler.fit_transform(X_prep), index=X_prep.index, columns=X_prep.columns)","9fb99d83":"C_regul = [0.01, 0.1, 1]\nfor regul in C_regul:\n    clf = LogisticRegression(penalty='l2', C=regul).fit(X_prep, y)\n    print('Cross-validation score: %f' % \n          cross_val_score(clf, X_prep, y, cv=cv, scoring='roc_auc').mean())","b32f850f":"# Find the best value for C regularization parameter\ngrid_linear = {'C': [0.0001]}\ngrid_poly = {'C': [0.01, 0.1, 1], 'gamma': [0.001, 0.01, 0.1], 'coef0': [3, 4]}\ngrid_rbf = {'C': [0.1, 1], 'gamma': [0.001, 0.01, 0.1]}\ngrid_sigmoid = {'C': [0.001, 0.01, 0.1], 'gamma': [0.0001, 0.01], 'coef0': [1, 10]}\ngrids = [grid_linear, grid_poly, grid_rbf, grid_sigmoid]\nkernels = ['linear', 'poly', 'rbf', 'sigmoid']\nparams = []\nscores = []\nfor ind, kern in enumerate(kernels):\n    svc_clf = SVC(kernel=kern)\n    gs = GridSearchCV(estimator=svc_clf, param_grid=grids[ind], cv=cv, scoring='roc_auc')\n    gs.fit(X_prep, y)\n    params.append(gs.best_params_)\n    scores.append(gs.best_score_)\n    print(kern, gs.best_params_, gs.best_score_)\nparams_best = params[np.argmax(scores)]\nkern_best = kernels[np.argmax(scores)]\nprint('Best parameters: kernel = %s, C = %f, coef0 = %f, gamma = %f' % \n      (kern_best, params_best['C'], params_best['coef0'], params_best['gamma']))","4778ddaf":"clf_svm = SVC(C=params_best['C'], \n              kernel=kern_best, \n              coef0=params_best['coef0'], \n              gamma=params_best['gamma']).fit(X_prep, y)\nprint('Cross-validation score: %f' % \n          cross_val_score(clf_svm, X_prep, y, cv=cv, scoring='roc_auc').mean())","cc859f4d":"X_tree = X_orig.copy()\n# Encode categorial features\n# Male - 1, Female - 0\nX_tree['gender'] = X_tree['gender'].map({'Male': 1, 'Female': 0, 'Other': 1})\n# urban - 1, rural - 0\nX_tree['Residence_type'] = X_tree['Residence_type'].map({'Urban': 1, 'Rural': 0})\n# ever_married yes - 1, no - 0\nX_tree['ever_married'] = X_tree['ever_married'].map({'Yes': 1, 'No': 0})\n# One-hot encoder for work_type, smoking_status\nX_tree = ohe(X_tree, ['work_type', 'smoking_status'])\nX_tree.drop(columns=['work_type', 'smoking_status'], inplace=True)\n# Fill the NaN in BMI column\nX_tree['bmi'].fillna(X_tree['bmi'].median(), inplace=True)\n# We do not need to scale our data for decision tree algorithm","384c0a5e":"clf_tree = DecisionTreeClassifier(criterion='entropy', ccp_alpha=0.003)\nclf_tree.fit(X_tree, y)\nprint('Cross-validation score: %f' % cross_val_score(clf_tree, X_tree, y, cv=cv, scoring='roc_auc').mean())","4ddc7f56":"# Find the best parameters\n# Actually, KNN is quite bad with highly unbalanced classes.\n# One should use some oversampling technique first, e.g. SMOTE (TODO)\ngrid = {'n_neighbors': [50, 75, 100], 'p': [1, 3, 5]}\n\nclf_knn = KNeighborsClassifier(weights='distance', metric='minkowski')\ngs = GridSearchCV(estimator=clf_knn, param_grid=grid, cv=cv, scoring='roc_auc')\ngs.fit(X_prep, y)\nprint('Best parameters: n_neighbors = %d, p = %d' % (gs.best_params_['n_neighbors'], gs.best_params_['p']))","b69e1771":"# Use the best parameters in the model\nclf_knn = KNeighborsClassifier(weights='distance', metric='minkowski', \n                               n_neighbors=gs.best_params_['n_neighbors'], \n                               p=gs.best_params_['p'])\nprint('Cross-validation score: %f' % cross_val_score(clf_knn, X_prep, y, cv=cv, scoring='roc_auc').mean())","94cbd49b":"clf_bayes = GaussianNB().fit(X_prep, y)\nprint('Cross-validation score: %f' % cross_val_score(clf_bayes, X_prep, y, cv=cv, scoring='roc_auc').mean())","9d17c67f":"X_forest = X_orig.copy()\n# Encode categorial features\n# Male - 1, Female - 0\nX_forest['gender'] = X_forest['gender'].map({'Male': 1, 'Female': 0, 'Other': 1})\n# urban - 1, rural - 0\nX_forest['Residence_type'] = X_forest['Residence_type'].map({'Urban': 1, 'Rural': 0})\n# ever_married yes - 1, no - 0\nX_forest['ever_married'] = X_forest['ever_married'].map({'Yes': 1, 'No': 0})\n# One-hot encoder for work_type, smoking_status\nX_forest = ohe(X_forest, ['work_type', 'smoking_status'])\nX_forest.drop(columns=['work_type', 'smoking_status'], inplace=True)\n# Fill the NaN in BMI column\nX_forest['bmi'].fillna(X_forest['bmi'].median(), inplace=True)","5d63c198":"# Find the best parameters\ngrid = {'n_estimators': [20, 50, 100, 200], 'ccp_alpha': [0.0, 0.01, 0.1], 'max_depth': [4, 5, 6]}\nclf_forest = RandomForestClassifier(criterion='entropy',\n                    max_features='auto')\ngs = GridSearchCV(clf_forest, param_grid=grid, cv=cv, scoring='roc_auc')\ngs.fit(X_forest, y)\nprint('Best parameters: n_estimators = %d, ccp_alpha = %f, max_depth = %d' % \n      (gs.best_params_['n_estimators'], gs.best_params_['ccp_alpha'], gs.best_params_['max_depth']))","b7450b4e":"# Use the best parameters in the model\nclf_forest = RandomForestClassifier(criterion='entropy', max_features='auto',\n                                   n_estimators=gs.best_params_['n_estimators'],\n                                   ccp_alpha=gs.best_params_['ccp_alpha'],\n                                   max_depth=gs.best_params_['max_depth'])\nprint('Cross-validation score: %f' % cross_val_score(clf_forest, X_forest, y, cv=cv, scoring='roc_auc').mean())","38fadfae":"# Visualization of the data","485753cd":"# 6) Random Forest","8f441e25":"# 1) Logistic regression","c3cb9a5a":"# 2) Support Vector Machine (SVM)","bccf122b":"# Feature engineering","515b097c":"# 3) Decision tree","f911ec5d":"# 5) Naive Bayes","962df7d9":"# 4) K-Nearest Neighbours","107a7e6c":"# Stroke prediction\n## This dataset is used to predict whether a patient is likely to get stroke based on the input parameters\n## Attribute Information\n- **id**: unique identifier\n- **gender**: \"Male\", \"Female\" or \"Other\"\n- **age**: age of the patient\n- **hypertension**: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n- **heart_disease**: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n- **ever_married**: \"No\" or \"Yes\"\n- **work_type**: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n- **Residence_type**: \"Rural\" or \"Urban\"\n- **avg_glucose_level**: average glucose level in blood\n- **bmi**: body mass index\n- **smoking_status**: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n- **stroke**: 1 if the patient had a stroke or 0 if not.\nNote: \"Unknown\" in smoking_status means that the information is unavailable for this patient"}}