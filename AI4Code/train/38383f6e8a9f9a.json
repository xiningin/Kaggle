{"cell_type":{"a7d19b77":"code","e475cd78":"code","d579da83":"code","50d57546":"code","6a0efd3f":"code","69af4c4f":"code","b54853ae":"code","32066cbc":"code","cc40240a":"code","186b2df5":"code","a77506c5":"code","5a774d99":"code","80add7ab":"code","edd75d28":"code","79257840":"code","cb809df3":"code","5acb16c1":"markdown","4d9bad5d":"markdown","cda1e36c":"markdown","08a59452":"markdown","8716004e":"markdown"},"source":{"a7d19b77":"import os\nimport numpy as np  \nimport pandas as pd  \nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\nfrom tqdm.auto import tqdm\ntqdm.pandas()\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras.layers import Dense, Input, Flatten, BatchNormalization, Dropout, Concatenate\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\n","e475cd78":"train = pd.read_csv('..\/input\/atis-airlinetravelinformationsystem\/atis_intents_train.csv')\ntrain.columns = ['intent', 'snippet']\n\nprint(train.shape)\ntrain.head()","d579da83":"train.intent.value_counts(), train.intent.value_counts(normalize=True)","50d57546":"test = pd.read_csv('..\/input\/atis-airlinetravelinformationsystem\/atis_intents_test.csv')\ntest.columns = ['intent', 'snippet']\n\nprint(test.shape)\ntest.head()","6a0efd3f":"test.intent.value_counts(), test.intent.value_counts(normalize=True)","69af4c4f":"train_data = train.snippet.values\ntrain_labels = train.intent.values\ntest_data = test.snippet.values\ntest_labels = test.intent.values\n\nlen(train_data), len(train_labels), len(test_data), len(test_labels)","b54853ae":"print(train_data[123])\nprint(train_labels[123])","32066cbc":"y_train = pd.get_dummies(train_labels)\nprint(y_train.shape)\ny_train.head()","cc40240a":"y_test = pd.get_dummies(test_labels)\nprint(y_test.shape)\ny_test.head()","186b2df5":"%%time\nmodule_url = 'https:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/4'\nembed = hub.KerasLayer(module_url, trainable=True, name='USE_embedding')\n\n# I've checked what happens if trainable=False ... bad performance 0.79, picking the majority group \/ guessing option","a77506c5":"def build_model(embed):\n    \n    model = Sequential([\n        Input(shape=[], dtype=tf.string),\n        embed,\n        Dense(8, activation='softmax')\n    ])\n    model.compile(Adam(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model\n\nmodel = build_model(embed)\nmodel.summary()","5a774d99":"checkpoint = ModelCheckpoint('modelATIS.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model.fit(\n    train_data, y_train,\n    validation_split=0.20,\n    epochs = 10,\n    callbacks=[checkpoint],\n    batch_size=32\n)","80add7ab":"# VALIDATION LOSS curves\n\nplt.clf()\nhistory_dict = train_history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nepochs = range(1, (len(history_dict['loss']) + 1))\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n# VALIDATION ACCURACY curves\n\nplt.clf()\nacc_values = history_dict['accuracy']\nval_acc_values = history_dict['val_accuracy']\nepochs = range(1, (len(history_dict['accuracy']) + 1))\nplt.plot(epochs, acc_values, 'bo', label='Training acc')\nplt.plot(epochs, val_acc_values, 'r', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","edd75d28":"rawPreds = model.predict(test_data)\nrawPreds.shape","79257840":"Preds = []\n\nfor j in range(rawPreds.shape[0]):\n    pos = rawPreds[j].argmax()\n    Preds.append(y_test.columns[pos])\n    \nlen(Preds)\n    ","cb809df3":"print(classification_report(Preds, test_labels))","5acb16c1":"# Goal: Intent classification with Universal Sentence Encoder\n\nThe beauty and magic of USE is that it takes care of the text cleaning, tokenization and embedding for you, with amazing results. \n\nIn this nb, I test USE on ATIS - Airline Travel Info System, with a small unbalanced dataset. \nThe task is to predict the user intent - one out 8 mutually exclusive classes.\n\n\nThe USE part was borrowed from https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-train-a-universal-sentence-encoder","4d9bad5d":"# Data","cda1e36c":"# Model - Universal Sentence Encoder","08a59452":"### OHE the labels","8716004e":"Unbalanced datasets, with majority voting \/ guessing accuracy = 0.79"}}