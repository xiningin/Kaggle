{"cell_type":{"1b6a9cfd":"code","d7bb31f3":"code","0b399e5e":"code","4d37539b":"code","1a8e5bbd":"code","8807fd08":"code","e4433443":"code","6b46a959":"code","ac810d2b":"code","3d79c0ca":"code","7ece716c":"code","5aa7772e":"code","87cfd67e":"markdown","d89cc460":"markdown","e083fff6":"markdown"},"source":{"1b6a9cfd":"from IPython.core.display import display, HTML\n\nimport pandas as pd\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nimport os\nimport gc\n\nfrom joblib import Parallel, delayed\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.metrics import r2_score\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\npath_root = '..\/input\/optiver-realized-volatility-prediction'\npath_data = '..\/input\/optiver-realized-volatility-prediction'\npath_submissions = '\/'\n\ntarget_name = 'target'\nscores_folds = {}","d7bb31f3":"def rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))","0b399e5e":"\"\"\"def log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series_log_return):\n    return np.sqrt(np.sum(series_log_return**2))\n\n\n\ndef get_stock_stat(stock_id : int, dataType = 'train'):\n    key = ['stock_id', 'time_id', 'seconds_in_bucket']\n    \n    #Book features\n    df_book = pd.read_parquet(os.path.join(path_data, 'book_{}.parquet\/stock_id={}\/'.format(dataType, stock_id)))\n    df_book['stock_id'] = stock_id\n    cols = key + [col for col in df_book.columns if col not in key]\n    df_book = df_book[cols]\n    \n    df_book['wap1'] = (df_book['bid_price1'] * df_book['ask_size1'] +\n                                    df_book['ask_price1'] * df_book['bid_size1']) \/ (df_book['bid_size1'] + df_book['ask_size1'])\n    df_book['wap2'] = (df_book['bid_price2'] * df_book['ask_size2'] +\n                                    df_book['ask_price2'] * df_book['bid_size2']) \/ (df_book['bid_size2'] + df_book['ask_size2'])\n    df_book['log_return1'] = df_book.groupby(by = ['time_id'])['wap1'].apply(log_return).fillna(0)\n    df_book['log_return2'] = df_book.groupby(by = ['time_id'])['wap2'].apply(log_return).fillna(0)\n    \n    features_to_apply_realized_volatility = ['log_return'+str(i+1) for i in range(2)]\n    stock_stat = df_book.groupby(by = ['stock_id', 'time_id'])[features_to_apply_realized_volatility]\\\n                        .agg(realized_volatility).reset_index()\n\n    #Trade features\n    trade_stat =  pd.read_parquet(os.path.join(path_data,'trade_{}.parquet\/stock_id={}'.format(dataType, stock_id)))\n    trade_stat = trade_stat.sort_values(by=['time_id', 'seconds_in_bucket']).reset_index(drop=True)\n    trade_stat['stock_id'] = stock_id\n    cols = key + [col for col in trade_stat.columns if col not in key]\n    trade_stat = trade_stat[cols]\n    trade_stat['trade_log_return1'] = trade_stat.groupby(by = ['time_id'])['price'].apply(log_return).fillna(0)\n    trade_stat = trade_stat.groupby(by = ['stock_id', 'time_id'])[['trade_log_return1']]\\\n                           .agg(realized_volatility).reset_index()\n    #Joining book and trade features\n    stock_stat = stock_stat.merge(trade_stat, on=['stock_id', 'time_id'], how='left').fillna(-999)\n    \n    return stock_stat\n\ndef get_dataSet(stock_ids : list, dataType = 'train'):\n\n    stock_stat = Parallel(n_jobs=-1)(\n        delayed(get_stock_stat)(stock_id, dataType) \n        for stock_id in stock_ids\n    )\n    \n    stock_stat_df = pd.concat(stock_stat, ignore_index = True)\n\n    return stock_stat_df\n    \n\"\"\"","4d37539b":"import os\nimport glob\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\npd.set_option('max_rows', 300)\npd.set_option('max_columns', 300)\n\n# data directory\ndata_dir = '..\/input\/optiver-realized-volatility-prediction\/'","1a8e5bbd":"def calc_wap(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1'])\/(df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2'])\/(df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\ndef count_unique(series):\n    return len(np.unique(series))","8807fd08":"def preprocessor_book(file_path):\n    \n    df = pd.read_parquet(file_path)\n    #calculate return etc\n    df['wap'] = calc_wap(df)\n    df['log_return'] = df.groupby('time_id')['wap'].apply(log_return)\n    \n    df['wap2'] = calc_wap2(df)\n    df['log_return2'] = df.groupby('time_id')['wap2'].apply(log_return)\n    \n    df['wap_balance'] = abs(df['wap'] - df['wap2'])\n    \n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1'])\/2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n\n    #dict for aggregate\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'log_return2':[realized_volatility],\n        'wap_balance':[np.mean],\n        'price_spread':[np.mean],\n        'bid_spread':[np.mean],\n        'ask_spread':[np.mean],\n        'volume_imbalance':[np.mean],\n        'total_volume':[np.mean],\n        'wap':[np.mean],\n            }\n\n    #####groupby \/ all seconds\n    df_feature = pd.DataFrame(df.groupby(['time_id']).agg(create_feature_dict)).reset_index()\n    \n    df_feature.columns = ['_'.join(col) for col in df_feature.columns] #time_id is changed to time_id_\n        \n    ######groupby \/ last XX seconds\n    last_seconds = [300]\n    \n    for second in last_seconds:\n        second = 600 - second \n    \n        df_feature_sec = pd.DataFrame(df.query(f'seconds_in_bucket >= {second}').groupby(['time_id']).agg(create_feature_dict)).reset_index()\n\n        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns] #time_id is changed to time_id_\n     \n        df_feature_sec = df_feature_sec.add_suffix('_' + str(second))\n\n        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id_',right_on=f'time_id__{second}')\n        df_feature = df_feature.drop([f'time_id__{second}'],axis=1)\n    \n    #create row_id\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature = df_feature.drop(['time_id_'],axis=1)\n    \n    return df_feature\n\n\ndef preprocessor_trade(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    \n    aggregate_dictionary = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.mean],\n    }\n    \n    df_feature = df.groupby('time_id').agg(aggregate_dictionary)\n    \n    df_feature = df_feature.reset_index()\n    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n\n    \n    ######groupby \/ last XX seconds\n    last_seconds = [300]\n    \n    for second in last_seconds:\n        second = 600 - second\n    \n        df_feature_sec = df.query(f'seconds_in_bucket >= {second}').groupby('time_id').agg(aggregate_dictionary)\n        df_feature_sec = df_feature_sec.reset_index()\n        \n        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns]\n        df_feature_sec = df_feature_sec.add_suffix('_' + str(second))\n        \n        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id_',right_on=f'time_id__{second}')\n        df_feature = df_feature.drop([f'time_id__{second}'],axis=1)\n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature = df_feature.drop(['trade_time_id_'],axis=1)\n    \n    return df_feature","e4433443":"def preprocessor(list_stock_ids, is_train = True):\n    from joblib import Parallel, delayed # parallel computing to save time\n    df = pd.DataFrame()\n    \n    def for_joblib(stock_id):\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        else:\n            file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n            \n        df_tmp = pd.merge(preprocessor_book(file_path_book),preprocessor_trade(file_path_trade),on='row_id',how='left')\n     \n        return pd.concat([df,df_tmp])\n    \n    df = Parallel(n_jobs=-1, verbose=1)(\n        delayed(for_joblib)(stock_id) for stock_id in list_stock_ids\n        )\n\n    df =  pd.concat(df,ignore_index = True)\n    return df","6b46a959":"\"\"\"train = pd.read_csv(os.path.join(path_data, 'train.csv'))\n%time train_stock_stat_df = get_dataSet(stock_ids = train['stock_id'].unique(), dataType = 'train')\ntrain = pd.merge(train, train_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left')\nprint('Train shape: {}'.format(train.shape))\ndisplay(train.head(2))\n\ntest = pd.read_csv(os.path.join(path_data, 'test.csv'))\ntest_stock_stat_df = get_dataSet(stock_ids = test['stock_id'].unique(), dataType = 'test')\ntest = pd.merge(test, test_stock_stat_df, on = ['stock_id', 'time_id'], how = 'left').fillna(0)\nprint('Test shape: {}'.format(test.shape))\ndisplay(test.head(2))\"\"\"","ac810d2b":"train = pd.read_csv(data_dir + 'train.csv')\ntrain_ids = train.stock_id.unique()\ndf_train = preprocessor(list_stock_ids= train_ids, is_train = True)\ntrain['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\ntrain = train[['row_id','target']]\ndf_train = train.merge(df_train, on = ['row_id'], how = 'left')\n\ntest = pd.read_csv(data_dir + 'test.csv')\ntest_ids = test.stock_id.unique()\ndf_test = preprocessor(list_stock_ids= test_ids, is_train = False)\ndf_test = test.merge(df_test, on = ['row_id'], how = 'left')\n\nfrom sklearn.model_selection import KFold\n#stock_id target encoding\ndf_train['stock_id'] = df_train['row_id'].apply(lambda x:x.split('-')[0])\ndf_test['stock_id'] = df_test['row_id'].apply(lambda x:x.split('-')[0])\nstock_id_target_mean = df_train.groupby('stock_id')['target'].mean() \ndf_test['stock_id_target_enc'] = df_test['stock_id'].map(stock_id_target_mean) # test_set\n\n#training\ntmp = np.repeat(np.nan, df_train.shape[0])\nkf = KFold(n_splits = 10, shuffle=True,random_state = 777)\nfor idx_1, idx_2 in kf.split(df_train):\n    target_mean = df_train.iloc[idx_1].groupby('stock_id')['target'].mean()\n    tmp[idx_2] = df_train['stock_id'].iloc[idx_2].map(target_mean)\ndf_train['stock_id_target_enc'] = tmp","3d79c0ca":"import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nfrom keras import backend as K\n\nhidden_units = [64,32,16,8]\nstock_embedding_size = 16\n\ncat_data = df_train['stock_id'].astype('int')\n\nmean_target = np.median(df_train.target)\n\nfeatures = ['log_return_realized_volatility',\n       'log_return2_realized_volatility', 'wap_balance_mean',\n       'price_spread_mean', 'bid_spread_mean', 'ask_spread_mean',\n       'volume_imbalance_mean', 'total_volume_mean', 'wap_mean',\n       'log_return_realized_volatility_300',\n       'log_return2_realized_volatility_300', 'wap_balance_mean_300',\n       'price_spread_mean_300', 'bid_spread_mean_300', 'ask_spread_mean_300',\n       'volume_imbalance_mean_300', 'total_volume_mean_300', 'wap_mean_300',\n       'trade_log_return_realized_volatility',\n       'trade_seconds_in_bucket_count_unique', 'trade_size_sum',\n       'trade_order_count_mean', 'trade_log_return_realized_volatility_300',\n       'trade_seconds_in_bucket_count_unique_300', 'trade_size_sum_300',\n       'trade_order_count_mean_300','stock_id_target_enc']\nfeatures_to_consider = ['stock_id'] + features\n\nnb_features = len(features)\n\ndef base_model():\n    \n    # Each instance will consist of two inputs: a single user id, and a single movie id\n    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n    num_input = keras.Input(shape=(nb_features,), name='num_data')\n\n\n    #embedding, flatenning and concatenating\n    stock_embedded = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n                                           input_length=1, name='stock_embedding')(stock_id_input)\n    \n    stock_flattened = keras.layers.Flatten()(stock_embedded)\n    \n    out = keras.layers.Concatenate()([stock_flattened, num_input])\n    \n    out = keras.layers.GaussianNoise(0.1)(out)\n    \n    # Add one or more hidden layers\n    for n_hidden in hidden_units:\n\n        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n        \n    #out = keras.layers.Concatenate()([out, num_input])\n\n    # A single output: our predicted rating\n    out = keras.layers.Dense(1, activation='swish', name='prediction',bias_initializer=tf.keras.initializers.Constant(value = mean_target))(out)\n\n    model = keras.Model(\n    inputs = [stock_id_input, num_input],\n    outputs = out,\n    )\n    \n    return model","7ece716c":"model_name = 'NN'\npred_name = 'pred_{}'.format(model_name)\n\nn_folds = 4\nkf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2020)\nscores_folds[model_name] = []\ncounter = 1\n\ndf_train['stock_id'] = df_train['stock_id'].astype('int')\ndf_test['stock_id'] = df_train['stock_id'].astype('int')\n\n\ntrain = df_train.fillna(0)\ntest = df_test.fillna(0)\n\ntrain[pred_name] = 0\ntest['target'] = 0\n\nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', min_delta=1e-05, patience=7, verbose=1,\n    mode='min')\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.1, patience=3, verbose=1, min_lr=1e-7,\n    mode='min')\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\n\nfor dev_index, val_index in kf.split(range(len(train))):\n    print('CV {}\/{}'.format(counter, n_folds))\n    \n    #Bottleneck ? \n    X_train = train.loc[dev_index, features_to_consider]\n    y_train = train.loc[dev_index, target_name].values\n    X_test = train.loc[val_index, features_to_consider]\n    y_test = train.loc[val_index, target_name].values\n    \n    #############################################################################################\n    # NN\n    #############################################################################################\n    \n    model = base_model()\n    \n    model.compile(\n        keras.optimizers.Adam(learning_rate=0.01),\n        loss='mean_squared_error',\n        metrics=['MSE'],\n    )\n\n    num_data = scaler.fit_transform(X_train[features])\n    cat_data = X_train['stock_id']\n    \n    num_data_test = scaler.transform(X_test[features])\n    cat_data_test = X_test['stock_id']\n\n    model.fit([cat_data, num_data], \n              y_train, \n              sample_weight = 1\/np.square(y_train),\n              batch_size=1024,\n              epochs=100,\n              validation_data=([cat_data_test, num_data_test], y_test, 1\/np.square(y_test)),\n              callbacks=[es, plateau],\n              shuffle=True,\n             verbose = 1)\n\n    preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n    \n    score = round(rmspe(y_true = y_test, y_pred = preds),5)\n    print('Fold {} {}: {}'.format(counter, model_name, score))\n    scores_folds[model_name].append(score)\n    test[target_name] += model.predict([test['stock_id'], scaler.transform(test[features])]).reshape(1,-1)[0].clip(0,1e10)\n       \n    counter += 1","5aa7772e":"test[target_name] = test[target_name]\/n_folds\n\nscore = round(rmspe(y_true = train[target_name].values, y_pred = train[pred_name].values),5)\nprint('RMSPE {}: {} - Folds: {}'.format(model_name, score, scores_folds[model_name]))\n\ndisplay(test[['row_id', target_name]].head(2))\ntest[['row_id', target_name]].to_csv('submission.csv',index = False)","87cfd67e":"## Train and test datasets","d89cc460":"## NN starter\n\nA simple NN starter using stock Embedding. \n\nHeavily inspired from this notebook for the feature engineering part:\nhttps:\/\/www.kaggle.com\/manels\/lgb-starter\n\nEmbedding layer from :\nhttps:\/\/www.kaggle.com\/colinmorris\/embedding-layers\n\nAdded a better feature engineering approach :\nhttps:\/\/www.kaggle.com\/tommy1028\/lightgbm-starter-with-feature-engineering-idea\n\nAlso see:\n* https:\/\/www.kaggle.com\/jiashenliu\/introduction-to-financial-concepts-and-data\n* https:\/\/www.kaggle.com\/c\/optiver-realized-volatility-prediction\/discussion\/250324\n\n**I hope it will be useful for other beginners.**","e083fff6":"## Training model and making predictions"}}