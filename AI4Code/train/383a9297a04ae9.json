{"cell_type":{"e8afbbb9":"code","ad5c6751":"code","22eec267":"code","e821ab3b":"code","b52812a2":"code","3707f6af":"code","728ccabc":"code","14cd8b41":"code","41942b89":"code","6fa7ad67":"code","3f974316":"code","70c4b3ca":"code","9828e2e6":"code","667af9bd":"code","b91aa58f":"code","884fccf0":"code","51e8ab2e":"code","2c3f8672":"code","c6079951":"code","58e034da":"code","eb4c04bd":"code","f52f4267":"code","86e1f083":"code","bcfb397a":"code","33a9cbfb":"code","3b0c70e2":"code","9fead2fb":"code","e637b414":"code","8992ee49":"code","b4409738":"code","ba24ec75":"code","1def659d":"code","6d173c84":"code","18ef28d2":"markdown","99d49eaa":"markdown","8fedbc79":"markdown","cb03659d":"markdown","844782c9":"markdown","3a06a7cf":"markdown","373f2956":"markdown","39d73d5f":"markdown","d91d6644":"markdown","9c75107d":"markdown","7f137c36":"markdown","be01f9f8":"markdown","47a1f066":"markdown","deaffaf6":"markdown","b69643d7":"markdown","3166ea8f":"markdown","7fd49970":"markdown","fefaf895":"markdown","26ecd013":"markdown","b14a1deb":"markdown","d1e7b5a1":"markdown","2f2cdccb":"markdown","3a07ad05":"markdown","e77526a5":"markdown","5d0ce8f8":"markdown","c30aaa13":"markdown","126bb495":"markdown","5806aab3":"markdown","8a6612b9":"markdown","b5b8ffd1":"markdown","12ff3574":"markdown","386ecea3":"markdown","cb605718":"markdown","ce297dc2":"markdown","706a3ade":"markdown"},"source":{"e8afbbb9":"import scipy.stats\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","ad5c6751":"books = pd.read_csv('..\/input\/bookscsv\/books.csv')","22eec267":"books.head()","e821ab3b":"books.columns","b52812a2":"def data_verifier(file):\n    with open(books, mode=\"r\", encoding='utf-8', newline='') as csv_file:\n        csv_reader = csv.reader(csv_file)\n        \n        header = next(csv_reader)\n        count = 0\n        for row in csv_reader:\n            try:\n                last_row = row[12]\n                if last_row == '':\n                    last_row = row[13]\n                print(f'{row}')\n                print(f'LENs. title: {len(row[1])} authors: {len(row[2])} sup_extra: {len(row[3])} sup_rating: {len(row[4])}\\n')\n             \n            except IndexError:\n\n                pass\n        \n        csv_file.close()","3707f6af":"import csv, os\n\nbooks = '..\/input\/goodreadsbooks\/books.csv'\ndata_verifier(books)","728ccabc":"import csv, os\n\ndef load_and_max(file, column_to_find):\n    with open(books, mode=\"r\", encoding='utf-8', newline='') as csv_file:\n        csv_reader = csv.reader(csv_file)\n        \n        header = next(csv_reader)\n        column_array = []\n        max_value = 0\n\n        for row in csv_reader:\n            try:\n                last_row = row[12]\n                if last_row == '':\n                    last_row = row[13]\n\n                if column_to_find == 'average_rating': column_data = float(row[4])\n                elif column_to_find == 'num_pages': column_data = int(row[8])\n                elif column_to_find == 'ratings_count': column_data = int(row[9])\n                elif column_to_find == 'text_reviews_count': column_data = int(row[10])\n                else:\n                    column_array = 'Data not Found. Check input'\n                    max_value = 0\n                    return column_array, max_value\n            except IndexError:\n                if column_to_find == 'average_rating': column_data = float(row[3])\n                elif column_to_find == 'num_pages': column_data =  int(row[7])\n                elif column_to_find == 'ratings_count': column_data = int(row[8])\n                elif column_to_find == 'text_reviews_count': column_data = int(row[9])\n                else:\n                    column_array = 'Data not Found. Check input'\n                    max_value = 0\n                    return column_array, max_value\n\n            column_array.append(column_data)\n            if column_data > max_value:\n                max_value = column_data\n\n    csv_file.close()\n\n    return column_array, max_value","14cd8b41":"def load_array(file, column_to_find):\n\n    with open(books, mode=\"r\", encoding='utf-8', newline='') as csv_file:\n        csv_reader = csv.reader(csv_file)\n        header = next(csv_reader)\n        column_array = []\n\n        for row in csv_reader:\n            try:\n                last_row = row[12]\n                if last_row == '':\n                    last_row = row[13]\n                #Ordered followinf the fild\n                #First Strings\n                if column_to_find == 'title': column_data = row[1]\n                elif column_to_find == 'authors': column_data = row[2] + ',' + row[3]\n                #Float\n                elif column_to_find == 'average_rating': column_data = float(row[4])\n                #Strings hided as Int\n                elif column_to_find == 'isbn': column_data = row[5]\n                elif column_to_find == 'isbn13': column_data = row[6]\n                #String AF\n                elif column_to_find == 'language_code': column_data = row[7]\n                elif column_to_find == '  num_pages': column_data = int(row[8])\n                elif column_to_find == 'ratings_count': column_data = int(row[9])\n                elif column_to_find == 'text_reviews_count': column_data = int(row[10])\n                #Reason you import Datetime\n                elif column_to_find == 'publication_date': column_data = datetime.strptime(row[11], '%m\/%d\/%Y')\n                #String\n                elif column_to_find == 'publisher': column_data = row[12]\n                else:\n                    column_data = 'Data not Found. Check input'\n                    \n                    return column_array\n\n                    \n            except IndexError:\n                #Ordered followinf the fild\n                #First Strings\n                if column_to_find == 'title': column_data = row[1]\n                elif column_to_find == 'authors': column_data = row[2]\n                #Float\n                elif column_to_find == 'average_rating': column_data = float(row[3])\n                #Strings hided as Int\n                elif column_to_find == 'isbn': column_data = row[4]\n                elif column_to_find == 'isbn13': column_data = row[5]\n                #String AF\n                elif column_to_find == 'language_code': column_data = row[6]\n                elif column_to_find == 'num_pages': column_data =  int(row[7])\n                elif column_to_find == 'ratings_count': column_data = int(row[8])\n                elif column_to_find == 'text_reviews_count': column_data = int(row[9])\n                #Reason you import Datetime\n                elif column_to_find == 'publication_date': column_data = datetime.strptime(row[10], '%m\/%d\/%Y')\n                #String\n                elif column_to_find == 'publisher': column_data = row[11]\n                else:\n                    column_data = 'Data not Found. Check input'\n                    \n                    return column_array\n\n            column_array.append(column_data)\n        \n\n    csv_file.close()\n\n    return column_array","41942b89":"books = '..\/input\/goodreadsbooks\/books.csv'\n\ntitles = load_array(books, 'title')\nrating, max_rating = load_and_max(books, 'average_rating')\nreviews, max_reviews = load_and_max(books, 'text_reviews_count')","6fa7ad67":"print(f'Informaci\u00f3n disponible: Titulos: {len(titles)}, rating: {len(rating)}, reviews: {len(reviews)}')","3f974316":"books_dict = {'rating': rating,\n             'reviews': reviews,\n             'titles': titles}\n\nbooks_df = pd.DataFrame(books_dict, columns=['rating','reviews','titles'])\nbooks_df","70c4b3ca":"y = books_df['rating']\n\nfig, ax = plt.subplots()\nax.hist(y, bins = 500)\nax.set_xlabel('rating')\nax.set_ylabel('Frecuencia')\n\nplt.axvline(np.mean(y)-np.std(y), c = 'k', linestyle = ':', label = '-1 desv. std.')\nplt.axvline(np.mean(y), c = 'r', linestyle = '-', label = 'Promedio')\nplt.axvline(np.mean(y)+np.std(y), c = 'k', linestyle = ':', label = '+1 desv. std.')\nax.legend()","9828e2e6":"y = books_df['rating']\n\nfig, ax = plt.subplots()\nax.hist(y, bins = 100)\nax.set_xlabel('rating')\nax.set_ylabel('Frecuencia')\n\nplt.axvline(np.mean(y)-np.std(y), c = 'k', linestyle = ':', label = '-1 desv. std.')\nplt.axvline(np.mean(y), c = 'r', linestyle = '-', label = 'Promedio')\nplt.axvline(np.mean(y)+np.std(y), c = 'k', linestyle = ':', label = '+1 desv. std.')\nax.legend()","667af9bd":"y = books_df['rating']\n\nfig, ax = plt.subplots()\nax.hist(y, bins = 10)\nax.set_xlabel('rating')\nax.set_ylabel('Frecuencia')\n\nplt.axvline(np.mean(y)-np.std(y), c = 'k', linestyle = ':', label = '-1 desv. std.')\nplt.axvline(np.mean(y), c = 'r', linestyle = '-', label = 'Promedio')\nplt.axvline(np.mean(y)+np.std(y), c = 'k', linestyle = ':', label = '+1 desv. std.')\nax.legend()","b91aa58f":"def match_case_condition(array, value):\n    match_one_condition = []\n    for element_ub in range(len(array)):\n        if array[element_ub] == value:\n            match_one_condition.append(element_ub)\n\n    return match_one_condition","884fccf0":"maxium_books = match_case_condition(rating,5)\n\nverified_rate = []\nfor ub in maxium_books:\n    verified_rate.append(rating[ub])\n\nn_reviews_of_titles = []\nfor ub in maxium_books:\n    n_reviews_of_titles.append(reviews[ub])    \n    \nbest_rated_titles = []\nfor ub in maxium_books:\n    best_rated_titles.append(titles[ub])\n    \nbest_rated_books_dict = {'rating': verified_rate,\n             'reviews': n_reviews_of_titles,\n             'titles': best_rated_titles}\n\nbest_rated_books_df = pd.DataFrame(best_rated_books_dict, columns=['rating','reviews','titles'])\nbest_rated_books_df","51e8ab2e":"y = books_df['reviews']\nfig, ax = plt.subplots()\nax.hist(y, bins = 10)\nax.set_xlabel('reviews')\nax.set_ylabel('Frecuencia')\n\nplt.axvline(np.mean(y)-np.std(y), c = 'k', linestyle = ':', label = '-1 desv. std.')\nplt.axvline(np.mean(y), c = 'r', linestyle = '-', label = 'Promedio')\nplt.axvline(np.mean(y)+np.std(y), c = 'k', linestyle = ':', label = '+1 desv. std.')\nax.legend()","2c3f8672":"y = books_df['reviews']\nfig, ax = plt.subplots()\nax.hist(y, bins = 100)\nax.set_xlabel('reviews')\nax.set_ylabel('Frecuencia')\n\nplt.axvline(np.mean(y)-np.std(y), c = 'k', linestyle = ':', label = '-1 desv. std.')\nplt.axvline(np.mean(y), c = 'r', linestyle = '-', label = 'Promedio')\nplt.axvline(np.mean(y)+np.std(y), c = 'k', linestyle = ':', label = '+1 desv. std.')\nax.legend()","c6079951":"def merge_sort(array):\n    if len(array) > 1:\n        middle = len(array) \/\/ 2\n        left = array[:middle]\n        right = array[middle:]\n\n        merge_sort(left)\n        merge_sort(right)\n        \n        \"\"\"SubArrays Iterators\"\"\"\n        i = 0\n        j = 0\n        \"\"\"MainArray Iterator\"\"\"\n        k = 0\n\n        while i < len(left) and j < len(right):\n            if left[i] < right[j]:\n                array[k] = left[i]\n                i += 1\n            else:\n                array[k] = right[j]\n                j += 1\n            \n            k += 1\n\n        while i < len(left):\n            array[k] = left[i]\n            i += 1\n            k += 1\n\n        while j < len(right):\n            array[k] = right[j]\n            j += 1\n            k += 1\n\n    return array\n\ndef binary_search(array, start, end, search_value):\n    if start > end:\n        return end\n    \n    middle = (start + end) \/\/ 2\n\n    if array[middle] == search_value:\n        return middle\n    elif array[middle] < search_value:\n        return binary_search(array, middle + 1, end, search_value)\n    else:\n        return binary_search(array, start, middle - 1, search_value)\n\n    \ndef top_condisioned(array, start_value):\n    helper_array = sorted_set(array.copy())\n    helpers_end = len(helper_array) - 1\n    ubication =  binary_search(helper_array, 0, helpers_end, start_value)\n    \n    top_condisioned = []\n    for i in range(ubication, helpers_end):\n        top_condisioned.append(helper_array[i])\n\n    return top_condisioned\n\ndef counted_array(elements_to_count, original_array):\n    count_array = []\n    for value in range(len(elements_to_count)):\n        count_array.append(original_array.count(elements_to_count[value]))\n    \n    return count_array\n\ndef sorted_set(array):\n    reduced_set = set(array)\n    reduced_array = []\n    for element in reduced_set:\n        reduced_array.append(element)\n    reduced_array = merge_sort(reduced_array)\n\n    return reduced_array","58e034da":"topc_reviews = top_condisioned(reviews, 1000)\nprint(f'Cantidad de libros con m\u00e1s de 1,000 reviews: {len(topc_reviews)}')","eb4c04bd":"topc_rating = top_condisioned(rating, 4.5)\nprint(f'Cantidad de calificaciones disponibles y obtenidas, que sean mayores o igual a 4.5 = {len(topc_rating)}; las cuales son: \\n{topc_rating}')","f52f4267":"def match_cases(array1, wanted_array1, array2, wanted_array2):\n    \"\"\"Notice de diference: the original data has to be of the samen lenght,\n    BUT the match cases can differ. This means that I can have 3 match cases\n    on one side and 50 in the other, and it won't make any trouble\"\"\"\n    if len(array1)==len(array2):\n        match_cases = []\n\n        #Creating Support dictionaries\n        wanted_cases_1 = {}\n        for data in range(len(wanted_array1)):\n            wanted_cases_1[wanted_array1[data]] = 1\n        \n        wanted_cases_2 = {}\n        for data in range(len(wanted_array2)):\n            wanted_cases_2[wanted_array2[data]] = 1\n        \n        for ub in range(len(array1)):\n            val1 = array1[ub]\n            val2 = array2[ub]\n            if val1 in wanted_cases_1 and val2 in wanted_array2:\n                match_cases.append(ub)\n            \n        return match_cases\n\n    else: return 'Arrays must have the samen lenght'","86e1f083":"match_cases = match_cases(rating, topc_rating, reviews, topc_reviews)","bcfb397a":"bv_rate = []\nfor ub in match_cases:\n    bv_rate.append(rating[ub])\n\nbv_reviews = []\nfor ub in match_cases:\n    bv_reviews.append(reviews[ub])    \n    \nbv_titles = []\nfor ub in match_cases:\n    bv_titles.append(titles[ub])\n    \nbv_books_dict = {'rating': bv_rate,\n             'reviews': bv_reviews,\n             'titles': bv_titles}\n\nbv_books_df = pd.DataFrame(bv_books_dict, columns=['rating','reviews','titles'])\nbv_books_df","33a9cbfb":"bv_books_df.sort_values(by = 'reviews', ascending = False)","3b0c70e2":"def topx(array, top_size):\n    \"\"\"Considerations.\n    1) Array is ORDERED from minor to major.\n    2) You can insert Arrays with duplicated values.\n    3) You won't insert Sets.\"\"\"\n\n    sorted_and_reduced_array = sorted_set(array.copy())\n    position = len(sorted_and_reduced_array) - 1\n    \n    topx = []\n    value = sorted_and_reduced_array[position]\n    topx.append(value)\n\n    position -= 1\n    max_value = value\n    value = sorted_and_reduced_array[position]\n\n    top_values = 1 \n\n    while top_values < top_size and position >= 0 :\n        if value != max_value:\n            topx.append(sorted_and_reduced_array[position])\n            max_value = value\n            top_values += 1\n        position -= 1\n        value = sorted_and_reduced_array[position]\n\n    return topx","9fead2fb":"top7_reviews = topx(reviews, 7)","e637b414":"def match_top_cases(array1, wanted_array1):\n        match_cases = []\n\n        #Creating Support dictionaries\n        wanted_cases_1 = {}\n        for data in range(len(wanted_array1)):\n            wanted_cases_1[wanted_array1[data]] = 1\n        \n        for ub in range(len(array1)):\n            val1 = array1[ub]\n            if val1 in wanted_cases_1:\n                match_cases.append(ub)\n            \n        return match_cases","8992ee49":"ub_top7r = match_top_cases(reviews, top7_reviews)","b4409738":"def array_matches(match_array, array1, array2, array3):\n    \n    match_array1 = []\n    for ub in match_array:\n        match_array1.append(array1[ub])\n\n    match_array2 = []\n    for ub in match_array:\n        match_array2.append(array2[ub])    \n\n    match_array3 = []\n    for ub in match_array:\n        match_array3.append(array3[ub])\n    \n    return match_array1,match_array2,match_array3","ba24ec75":"t7rev_rating, t7rev_reviews, t7rev_titles = array_matches(ub_top7r, rating, reviews, titles)\n    \ntop7_books_dict = {'rating': t7rev_rating,\n             'reviews': t7rev_reviews,\n             'titles': t7rev_titles}\n\ntop7_books_df = pd.DataFrame(top7_books_dict, columns=['rating','reviews','titles'])\ntop7_books_df.sort_values(by = 'reviews', ascending = False)","1def659d":"books_df","6d173c84":"fig = books_df.plot(kind=\"scatter\", x='rating', y='reviews', c='green')\n\nx = books_df['rating']\nplt.axvline(np.mean(x)-np.std(x), c = 'k', linestyle = ':', label = '-1 desv. std.')\nplt.axvline(np.mean(x), c = 'r', linestyle = '-', label = 'Promedio')\nplt.axvline(np.mean(x)+np.std(x), c = 'k', linestyle = ':', label = '+1 desv. std.')\n\ny = books_df['reviews']\nplt.axhline(np.mean(y)-np.std(y), c = 'c', linestyle = ':', label = 'Y: -1 desv. std.')\nplt.axhline(np.mean(y), c = 'b', linestyle = '-', label = 'Promedio y')\nplt.axhline(np.mean(y)+np.std(y), c = 'c', linestyle = ':', label = 'Y: +1 desv. std.')\n\nfig.legend()","18ef28d2":"<h1>\u00bfInfluyen la cantidad de reviews en la calificaci\u00f3n de un libro?<\/h1>","99d49eaa":"Con este an\u00e1lisis, podemos afirmar que es falsa la idea de que los libros mejor calificados son los que tienen el mayor n\u00famero de lectores. Este an\u00e1lisis inclusive puede poner en mesa la idea de que un libro con 5 estrellas ha sido poco evaluado por sus lectores.","8fedbc79":"<h1>Carga de la informaci\u00f3n y an\u00e1lisis de la informaci\u00f3n<\/h1>","cb03659d":"A diferencia de las estrellas, los reviews tienen la caracter\u00edstica de poder tener un comportamiento exponencial, por lo que clasificarlos por grupos carecer\u00eda de sentido. Para ello se muestran las siguientes gr\u00e1ficas, d\u00f3nde la primera muestra una clasificaci\u00f3n en 10 categor\u00edas y la siguiente de 100, usando sus valores c\u00f3mo par\u00e1metro clasificador.","844782c9":"Aparte de mostrar la casi inexistencia de valores altos, se puede deducir que un libro no popular tendr\u00e1 escasas rece\u00f1as; y el caso contrario con los libros populares, al poseer muchas rece\u00f1as. Gracias a la noci\u00f3n otorgada por los resultados con el gr\u00e1fico, podr\u00edamos obtener los libros que tengan como m\u00ednimo 1,000 reviews. Con esto, se reducen las iteraciones para encontrar un Top en el que los libros con mayor reviews tengan buena calificaci\u00f3n. Esta funci\u00f3n, de igual forma, podr\u00eda ser \u00fatil para encontrar todos los libros con la calificaci\u00f3n de nuestro inter\u00e9s.","3a06a7cf":"Como primer paso para acceder a la informaci\u00f3n Desde un punto inicial se poceden a importar las librer\u00edas del set b\u00e1sico para analizar un DataFrame con Pandas as\u00ed como las bibliotecas para el uso de gr\u00e1ficas y arreglos.","373f2956":"Debido a los resultados anteriores, se decide implementar un gr\u00e1fico de puntos \"rating vs reviews\". Este gr\u00e1fico podr\u00e1 servir para explicar mejor lo que sucede si solo se consideran el n\u00famero de rece\u00f1as.","39d73d5f":"Sin duda alguna, el crecimiento exponencial del conocimiento humano fue causado por la capacidad de transmitir ideas a trav\u00e9s del tiempo. Gracias a la palabra escrita, la humanidad no solo ha sido capaz de transmitir conocimiento, sino cre\u00f3 una herramienta que permit\u00ed a las personas vivir experiencias ajenas de formas inimaginables para qui\u00e9n no ha le\u00eddo una buena novela o cuento. No se requiere ser bibli\u00f3mano para poder recomendar un libro, alegando que es \"una joya\" de la literatura. Todos nos hemos vueltos profetas y evangelizadores de alg\u00fan buen libro. Pero si el gusto es relativo, \u00bfpodr\u00e1 ser que a alguien no le guste el mismo libro que a m\u00ed me inspiro a ser mejor persona? Claro que s\u00ed, es muy probable que al menos a una persona no le guste. Sin embargo, no quiere decir que el hecho de que no lo considere una \"joya de la literatura\", la otra persona lo odi\u00e9, como todo en la vida, es bueno tener matices.\n\nLa escala de 5 estrellas es conocida por todo el mundo, d\u00f3nde se entiende en acuerdo social que 0 es un rotundo \"no lo recomiendo\" cerca del \"si quieres un consejo, no lo leas\"; 3 se puede entender como un \"bueno, pero no el mejor\"; mientras que 5, la calificicaci\u00f3n de la excelencia. Brind\u00e1ndole a la gente estos matices, podemos obtener m\u00e1s informaci\u00f3n, as\u00ed como nuevas preguntas, d\u00f3nde la m\u00e1s importante podr\u00eda ser: \u00bfMientras m\u00e1s gente lea un libro, m\u00e1s dif\u00edcil ser\u00e1 alcanzar las 5 estrellas?","d91d6644":"Con el Dataset de esta p\u00e1gina, se plantea hacer un estudio estad\u00edstico sobre la relaci\u00f3n que tiene el promedio de calificaci\u00f3n que posee un libro, usando la escala de rating de 5 estrellas ('average_rating') y el n\u00famero de rese\u00f1as que ha recibido el mismo ('text_reviews_coun'), considerando el n\u00famero de rese\u00f1as como personas que han le\u00eddo el libro.","9c75107d":"Al observar el encabezado de la tabla, se puede apreciar que existe una columna de \"sobra\". El hecho de que lleve de nombre \"Unnamed: 12\" indica que esta columna es el resultado de que una o m\u00e1s filas contengan una coma de m\u00e1s. Considerando la naturaleza de los datos, se puede deducir que el error es m\u00e1s probable de aparecer en el t\u00edtulo del libro, o en los autores. Dadas estas suposiciones, se buscar\u00e1 analizar las filas que muestren estos errores, para poder identificar corroborar que si s\u00f3lo es en una columna o en dos.\n\nPara ello, se dise\u00f1a una funci\u00f3n que busque estos \"errores\"; si los encuentra, imprime la fila, pero en caso de que tenga un error de \u00edndice por no contener informaci\u00f3n, los pasar\u00e1 de largo y seguir\u00e1.","7f137c36":"<h2>Delimitaci\u00f3n de la informaci\u00f3n necesaria<\/h2>","be01f9f8":"<h3>An\u00e1lisis del Rating<\/h3>","47a1f066":"<h1>Read Data<\/h1>","deaffaf6":"Gracias a esta funci\u00f3n, podemos determinar que el error se encuentra en la columna de autores. Si bien, las longitudes sobresalen en el segundo error, al leer la informaci\u00f3n detalladamente podemos observar que el error sucede por una coma extra en autores. Ya que se realizar\u00e1 un estudio sobre la relaci\u00f3n entre el rating y las reviews, podemos precindir de toda informaci\u00f3n que no sea estos dos valores y el t\u00edtulo.","b69643d7":"<h3>Caso contrario. Solo considerar Reviews<\/h3>","3166ea8f":"<h2>Considerando Rating y Reviews<\/h2>","7fd49970":"Ejecutando por partes el c\u00f3digo, se procede a analizarlo su ejecuci\u00f3n como partes de un script simple de python, obteniendo la ubicaci\u00f3n de nuestro archivo y extrayendo la informaci\u00f3n relevante.","fefaf895":"<h2>Comportamiento rating vs reviews<\/h2>","26ecd013":"<h1>Conclusiones<\/h1>","b14a1deb":"Para el caso del rating se puede observar una clara tendencia a la derecha, indicando que resulta m\u00e1s f\u00e1cil encontrar un valor cercano a las 4 estrellas, pero que, al obtenerlo, decrece de forma m\u00e1s dr\u00e1stica. El hecho de encontrar pocos casos de calificaciones menores a 3.5 podr\u00eda indicar que la gente tiende a rese\u00f1ar los libros que les ha parecido decentes, pero resulta raro que algui\u00e9n realice una rese\u00f1a de alg\u00fan libro que no le gust\u00f3. A simple vista, se aprecia que es muy raro encontrar una calificaci\u00f3n mayor a 4.5 estrellas, pero que s\u00ed existen libros con 5 estrellas (\"excelente\").","d1e7b5a1":"Al tener el mismo tama\u00f1o, procedemos a hacer el an\u00e1lisis con ayuda de Pandas.","2f2cdccb":"Con la funci\u00f3n dise\u00f1ada (y probada en script), se procede a la importaci\u00f3n de csv y os para que el Kernel no lo se\u00f1ale como un objeto tipo DataFrame y lo maneje con un file y string.","3a07ad05":"Este gr\u00e1fico presenta una condensaci\u00f3n de los puntos entre una calificaci\u00f3n de 3.5 y 4.5 y menores a 20,000 reviews. Para el caso de libros mayores a 4.5 podemos observar que no existe un repunte ni libros que traspasen dicho rango, pero s\u00ed que los puntos con mayor n\u00famero de reviews se encuentran igual en dicho rango.","e77526a5":"Haciendo un an\u00e1lisis superficial, solo se requerir\u00e1 la informaci\u00f3n del rating (float), reviews (int) y los libros resultantes de estos an\u00e1lisis. En el caso del rating y las reviews, podemos notar que un par\u00e1metro siempre solicitado es el valor m\u00e1ximo. Por lo que al momento de cargar la informaci\u00f3n, podemos aprovechar para obtenerlo dentro del mismo ciclo. Si el lector desea agregar m\u00e1s informaci\u00f3n, a continuaci\u00f3n se deja el chunk de la funci\u00f3n load_and_max para su edici\u00f3n.\n\nSe hace la aclaraci\u00f3n que la importaci\u00f3n de 'csv' y 'os' es solo de apoyo did\u00e1ctico, ya que basta con la importaci\u00f3n que se hizo previamente.","5d0ce8f8":"<ol>\n<li>Un libro tenga una calificaci\u00f3n de 5 estrellas no es garant\u00eda de que a mucha gente le haya parecido un libro excelente<\/li>\n<li>Que un libro haya sido le\u00eddo por mucha gente, no garantiza que a todos les haya parecido un libro excelente<\/li>\n<li>Siempre hay que considerar todas las variables disponibles y relevantes de un Data Set<\/li>\n<li>Conforme m\u00e1s condiciones agreguemos a nuestros an\u00e1lisis, nuestras deducciones pueden ser m\u00e1s precisas y objetivas<\/li>\n<\/ol>","c30aaa13":"Debido al valor alto, comparado con el n\u00famero de calificaciones de 5 estrellas, de m\u00e1s de 1,000 rece\u00f1as, y recordando lo escasos que son los libros con calificaciones mayores a 4.5, podemos buscar los libros que hagan match con estas dos ideas. Primero obteniendo el arreglo con calificaciones mayores a 4.5","126bb495":"Estos casos, denominados \"mejor evaluados\" (bv), se almacenar\u00e1n un DataFrame distinto, para analizar de forma m\u00e1s f\u00e1cil su output.","5806aab3":"Con esa informaci\u00f3n, se procede a analizar cuales son los libros con mayor calificaci\u00f3n (5 estrellas)","8a6612b9":"Los siguientes chuncks fueron dise\u00f1ados para obtener el \"top\" deseado de una serie de elementos num\u00e9ricos, permitiendo la observaci\u00f3n de una tabla d\u00f3nde solo se consideran los 7 libros con m\u00e1s reviews. Este n\u00famero fue escogido por la cantidad de coincidencias obtenidas para el DataFrame anterior.","b5b8ffd1":"Ordenando los valores seg\u00fan la columna de reviews, podremos obtener el libro con mayor n\u00famero de reviews y que tenga una calificaci\u00f3n mayor a 4.5. Consider\u00e1ndolo por ende, uno de los mejores libros calificados en este Data Set","12ff3574":"Con la funci\u00f3n dise\u00f1ada, procedemos a insertar los arreglos originales, acompa\u00f1ados de los sets en los que se encuentran los valores permitidos. Cabe resaltar que esta funci\u00f3n solo encuentra la ubicaci\u00f3n de los valores donde nuestros universos y condiciones se encuentran, m\u00e1s no otorga el libro resultante, ya que no se introduce este dato en la funci\u00f3n.","386ecea3":"C\u00f3mo es posible de observar, la funci\u00f3n previa solo fue dise\u00f1ada para reducir tiempos de ejecuci\u00f3n, ya que de esta forma se ahorran dos ciclos O(n) cada uno para la obtenci\u00f3n de los valores m\u00e1ximos de los arrays. en caso de que se requiera alg\u00fan otro dato de la columna, se podr\u00eda usar la funci\u00f3n que se encuentra a continuaci\u00f3n, considerando que:\n\n1. Solo carga un array a la vez.\n1. No obtiene valores m\u00e1ximos.\n1. Se garantiza su desempe\u00f1o exclusivamente para este caso con este csv. Para otro tipo de csv's su utilidad y eficiencia no est\u00e1 garantizada.","cb605718":"Antes de realizar conjenturas sobre su calentamiento, encontrando un soporte visual en un histograma, se puede apreciar de forma r\u00e1pida la tendencia que tiene la serie del rating, permiti\u00e9ndo decidir cu\u00e1l es la v\u00eda m\u00e1s eficiente para el an\u00e1lisis. Asignando 10 bins, se entiende que el gr\u00e1fico x muestra la distribuci\u00f3n del rating en saltos de 0.5 en la calificaci\u00f3n; 100, 0.05; y 500, 0.01 (todas las posibles calificaciones)","ce297dc2":"Con esta tabla de resultado, es posible apreciar que el hecho, de que un libro posea un alto n\u00famero de rece\u00f1as, no es garant\u00eda de una buena calificaci\u00f3n, o que sea altamente recomendado.","706a3ade":"Para tener noci\u00f3n de que no se omiti\u00f3 ning\u00fan dato, se solicita su tama\u00f1o ya que, si tuvieran una diferencia por un solo dato, se corre el riesgo de obtener un error de \u00edndice en futuros an\u00e1lisis. De igual forma, se garantiza que los datos faltantes que puedan existir, son por error en el archivo y no por la carga, reduciendo la posibilidad de un re-trabajo"}}