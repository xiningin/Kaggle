{"cell_type":{"9d41a69c":"code","d5231fcd":"code","af550a0b":"code","4367d8ca":"code","47523569":"code","72f45fd9":"code","e65d6313":"code","e7e4538c":"code","a6bd5136":"code","3ebc439d":"code","b5996a9f":"code","6ec9b1ab":"code","43cd6a59":"code","aa1af697":"code","35233595":"code","041a26ae":"code","395a74f8":"code","462befd3":"code","b6afb069":"code","8e613910":"code","1db5f9da":"code","dc7d409a":"code","8f9ff51e":"code","e3b9a81d":"code","93f31a9c":"code","c309f760":"code","e908163d":"code","de55ae6d":"markdown","cbf38a5a":"markdown","a4a3b74e":"markdown","2953345f":"markdown","0cb80dee":"markdown","d334edba":"markdown","025dc01c":"markdown","9e7747f1":"markdown","9f10278a":"markdown","f0d25833":"markdown","8a806961":"markdown","d13e55ef":"markdown","080fb0ba":"markdown","1430a4cd":"markdown","c0abc32e":"markdown","6e20f0d6":"markdown","9aa9f8b1":"markdown","b89bdbb9":"markdown"},"source":{"9d41a69c":"import pandas as pd\nimport numpy as np\nimport re\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nimport matplotlib.pyplot as plt\n","d5231fcd":"data_true = pd.read_csv('..\/input\/fake-and-real-news-dataset\/True.csv')\ndata_fake = pd.read_csv('..\/input\/fake-and-real-news-dataset\/Fake.csv')","af550a0b":"data_true.head()","4367d8ca":"data_true[\"label\"] = 1\ndata_fake[\"label\"] = 0\ndata = pd.concat([data_true,data_fake],0)\ndata.info()","47523569":"data = data.loc[:,[\"text\",\"label\"]]\ndata.head()","72f45fd9":"x = data[\"text\"]\ny = data[\"label\"]","e65d6313":"def cleanText(text):\n    cleaned = re.sub(\"[^'a-zA-Z0-9]\",\" \",text)\n    lowered = cleaned.lower().strip()\n    return lowered","e7e4538c":"cleanText(\"Test .* yup *?! okay!.\")","a6bd5136":"st = time.time()\nx_cleaned = [cleanText(t) for t in x]\nprint(\"This process took {} seconds\".format(round(time.time()-st,2)))","3ebc439d":"x_cleaned[0]","b5996a9f":"st = time.time()\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(x_cleaned)\nx_tokenized = tokenizer.texts_to_sequences(x_cleaned)\nprint(\"This process took {} seconds\".format(round(time.time()-st,2)))","6ec9b1ab":"print(x_tokenized[0])","43cd6a59":"length_array = [len(s) for s in x_tokenized]\nSEQUENCE_LENGTH = int(np.quantile(length_array,0.75))\nprint(SEQUENCE_LENGTH)","aa1af697":"x_padded = pad_sequences(x_tokenized,maxlen=SEQUENCE_LENGTH)","35233595":"x_padded.shape","041a26ae":"x_train,x_test,y_train,y_test = train_test_split(x_padded,y,test_size=0.2,random_state=42)","395a74f8":"print(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","462befd3":"# We've added 1 because or word index has numbers from 1 to end but we've added\n# 0 tokens in padding so our vocab now has len(tokenizer.word_index) + 1\nVOCAB_LENGTH = len(tokenizer.word_index) + 1\nVECTOR_SIZE = 100\n\ndef getModel():\n    \"\"\"\n    Returns a trainable Sigmoid Convolutional Neural Network\n    \"\"\"\n    model = keras.Sequential()\n    model.add(layers.Embedding(input_dim=VOCAB_LENGTH,\n                               output_dim=VECTOR_SIZE,\n                               input_length=SEQUENCE_LENGTH\n                              ))\n    \n    model.add(layers.Conv1D(128,kernel_size=4))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Activation(\"relu\"))\n    model.add(layers.MaxPooling1D(2))\n    \n    model.add(layers.Conv1D(256,kernel_size=4))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Activation(\"relu\"))\n    model.add(layers.MaxPooling1D(2))\n    \n    model.add(layers.Conv1D(512,kernel_size=4))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Activation(\"relu\"))\n    model.add(layers.MaxPooling1D(2))\n    \n    model.add(layers.Flatten())\n    model.add(layers.Dense(1,activation=\"sigmoid\"))\n    \n    model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n    \n    return model","b6afb069":"model = getModel()\nmodel.summary()","8e613910":"history = model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=1)","1db5f9da":"model.save_weights(\"trained_model.h5\")","dc7d409a":"import pickle\nwith open(\"tokenizer.pickle\",mode=\"wb\") as F:\n    pickle.dump(tokenizer,F)\n","8f9ff51e":"import json\nlabel_map = {0:\"Fake\",\n             1:\"Real\"\n            }\n\njson.dump(label_map,open(\"label_map.json\",mode=\"w\"))","e3b9a81d":"class DeployModel():\n    \n    def __init__(self,weights_path,tokenizer_path,seq_length,label_map_path\n                ):\n        \n        self.model = getModel()\n        self.model.load_weights(weights_path)\n        self.tokenizer = pickle.load(open(tokenizer_path,mode=\"rb\"))\n        self.seq_len = seq_length\n        self.label_map = json.load(open(label_map_path))\n    \n    def _prepare_data(self,text):\n        \n        cleaned = cleanText(text)\n        tokenized = self.tokenizer.texts_to_sequences([cleaned])\n        padded = pad_sequences(tokenized,maxlen=self.seq_len)\n        return padded\n    \n    def _predict(self,text):\n        \n        text = self._prepare_data(text)\n        pred = int(self.model.predict_classes(text)[0])\n        return str(pred)\n    \n    def result(self,text):\n        \n        pred = self._predict(text)\n        return self.label_map[pred]","93f31a9c":"deploy_model = DeployModel(weights_path=\".\/trained_model.h5\",\n                           tokenizer_path=\".\/tokenizer.pickle\",\n                           seq_length=SEQUENCE_LENGTH,\n                           label_map_path=\".\/label_map.json\"\n                          )","c309f760":"test_text = x_cleaned[0]","e908163d":"print(test_text)\nprint(\"\\n\\n===========================\")\nprint(\"Results: \",deploy_model.result(test_text))","de55ae6d":"* And yes, it was real!","cbf38a5a":"* And let's create an object using our class.","a4a3b74e":"* Now we'll tokenize our data using Tensorflow's tokenizer.","2953345f":"* Our text data is ready to use, let's split our dataset into train and test sets.","0cb80dee":"* First we'll save weights of our model and pickle our tokenizer.","d334edba":"* And let's pad.","025dc01c":"# EXTRA: How To Make Our Model Ready-to-Deploy?\nBefore finishing this kernel, I wanna show you one more thing, an important one. How to make a model ready to deploy using a web library or framework like Flask or Django.\n\nLet's start.","9e7747f1":"* Now we need to pad our sequences, in order to find the true length, I'll use the third quartile of the length array (array which has the lengths of the sequences)","9f10278a":"# Preparing Data\nIn this section we're going to prepare data to use it in our neural network.","f0d25833":"# Introduction: Text Classification with CNNs\nHello people, welcome to this kernel. In this kernel I am going to show you how to create a Convolutional Neural Network using Tensorflow to classify texts.\n\nBefore starting, let's take a look at our table of content\n\n# Table of Content\n1. But CNNs Are For images!?!?\n1. Preparing Environment\n1. Preparing Data\n1. Neural Network Modeling\n1. EXTRA: How To Make Our Model Ready-to-Deploy?\n1. Conclusion\n\n\n# But CNNs Are For Images!?!\nIn deep learning, we generally use Convolutional Neural Networks and their variants to classify image data. So most of the people thinks *we can use them only for image data*.\n\nBut a convolution operator **extracts** features from a data given. And if data has dimension more than one, we can use it with a convolution operator. And if we use **word embeddings** to convert words we can use a Convolutional Neural Network. \n\nLet's start.\n","8a806961":"# Neural Network Modeling\nIn this section I'm going to build and train our convolutional neural network using keras' sequential api.","d13e55ef":"* 1 epoch and %93 validation accuracy, this is how a convolutional neural network works with text data ","080fb0ba":"* Now we're going to define a function which will clean data.","1430a4cd":"* And now we'll write a class which will have a function to predict data.","c0abc32e":"* Also let's save our label map using json library.","6e20f0d6":"* Let's test our function.","9aa9f8b1":"# Preparing Environment\nIn this section we'll import libraries and read our data from HDD.","b89bdbb9":"* We can drop title, subject and date.\n* Also we need to add a label which will be 1"}}