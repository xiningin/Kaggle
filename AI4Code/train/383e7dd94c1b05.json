{"cell_type":{"0edaca42":"code","6b4bd7da":"code","7e512b45":"code","192a059a":"code","068d8956":"code","6eead23e":"code","df7da93d":"code","ef3803cb":"code","c29d7a86":"code","874fc5c3":"code","943cba8d":"code","aa7051ff":"code","f9852602":"code","8a020de1":"code","7e73d7ab":"code","616fe92e":"code","e1700bc4":"code","db93f624":"code","33984054":"code","9272139c":"code","22d3191e":"code","a5ee90e7":"code","49d955d0":"code","5ffd9071":"code","b0b04970":"code","55d0b3f6":"code","c5db6b64":"code","67c7d5cd":"code","f9470279":"code","f89c4aaf":"code","9c23c7df":"code","116b2f11":"code","038ea710":"code","93bb3e12":"code","deae08d5":"code","292f600a":"code","5ef970ce":"code","56917a78":"code","929c6e58":"code","36e2db60":"code","cbe9e977":"code","ad3a93f8":"code","0af43caf":"code","10ab412a":"code","7c8ea813":"code","5d3604a3":"code","6786e5f6":"code","88400959":"code","f38c1fe3":"code","cfa3949b":"code","5f04ac24":"code","1fb7fc6f":"code","5971de6d":"code","efb82ec0":"code","e6110630":"code","b5976ac2":"code","1f55f445":"code","1ebcd053":"code","533ab98a":"code","a82319e9":"code","ac3e0462":"code","74b15378":"code","ee4ddda5":"code","64ef543e":"code","d4b156fa":"code","eff3fedc":"code","6f8bc45c":"code","40ddcd3a":"code","5f635a16":"markdown","4797b02a":"markdown","cd3907c6":"markdown","1009360c":"markdown","e03701e8":"markdown","d33f4388":"markdown","4b172c7f":"markdown","4ed85661":"markdown","3f31cdb4":"markdown","cbefff8b":"markdown","7a6d6883":"markdown","f9cd00a8":"markdown","7ad27c34":"markdown","8acd64b9":"markdown","898f487e":"markdown","7c82327e":"markdown","998e49cb":"markdown","e575ddf0":"markdown","800a1ab1":"markdown","4e00a4c8":"markdown","a53670a0":"markdown","0b239654":"markdown","82393d17":"markdown","2e0446d8":"markdown","e45dd950":"markdown","e7f27eb8":"markdown","9d460ecf":"markdown","d7a281a2":"markdown","7fd0e16c":"markdown","ab5b7256":"markdown","030edcce":"markdown","cb601d8f":"markdown","b2f4cd0f":"markdown","d1287d5c":"markdown","be5ee8b6":"markdown","437770e8":"markdown","77eb395e":"markdown","c2f6cc53":"markdown","f6282bd0":"markdown","fdf1f428":"markdown","a84c3802":"markdown","80d5c99b":"markdown","0195a3dd":"markdown","bb7f12da":"markdown","036a0791":"markdown","6d9fc918":"markdown","282d7612":"markdown","09b32c84":"markdown","42fba87b":"markdown","0e7393fa":"markdown","3fd08070":"markdown","8d8090f5":"markdown","7c8c8f2d":"markdown","e6c580c1":"markdown","75676ae5":"markdown","41921503":"markdown","cf36ea4a":"markdown","1e63aa58":"markdown","de93dd97":"markdown","7eee370d":"markdown","65babeaf":"markdown","86b70701":"markdown","d41530eb":"markdown","f0da6667":"markdown","6809ca59":"markdown","03b7e4a0":"markdown"},"source":{"0edaca42":"import pandas as pd\nimport numpy as np\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport statsmodels.api as sm\nimport datetime as dt\nfrom sklearn import preprocessing\nfrom sklearn.cluster import KMeans\nimport statsmodels.api as sm\nfrom sklearn.cluster import DBSCAN\nfrom scipy.stats import probplot\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn import metrics\nfrom sklearn.cluster import AgglomerativeClustering\nimport scipy.cluster.hierarchy as shc\n%matplotlib inline","6b4bd7da":"#path = r\"C:\\Users\\User\\Downloads\\marketing_data_new.csv\"\ndata = pd.read_csv(r\"..\/input\/marketing-data\/marketing_data.csv\")","7e512b45":"#data.head()","192a059a":"data.shape","068d8956":"data.info() ","6eead23e":"data.rename(columns = {' Income ':'Income','Response':'AcceptedCmp6'}, inplace = True)","df7da93d":"data.isnull().sum()","ef3803cb":"data.describe()","c29d7a86":"data['Income'].head()","874fc5c3":"warnings.filterwarnings(\"ignore\")\n\ndata['Income'] = data['Income'].str.replace(\"$\",\"\").str.replace(\",\",\"\")\ndata['Income'] = data['Income'].astype(float)","943cba8d":"# Merging the rows of Marital Status\ndata['Marital_Status'] = data['Marital_Status'].replace([\"Alone\",\"YOLO\",\"Absurd\"],\"Single\")\nprint(data['Marital_Status'].unique())","aa7051ff":"# Merging the rows of education variable:\ndata['Education'] = data['Education'].replace([\"2n Cycle\",\"Master\"],\"Masters\")\nprint(data['Education'].unique())","f9852602":"data['Dt_Customer'].head()","8a020de1":"data[\"Dt_Customer\"] = pd.to_datetime(data[\"Dt_Customer\"], format='%m\/%d\/%y')\ndata['Dt_Customer'].head()","7e73d7ab":"data['Expenditure'] = data['MntWines'] + data['MntSweetProducts'] + data['MntMeatProducts']+ data['MntGoldProds'] + data['MntFruits'] + data['MntFishProducts'] ","616fe92e":"data['Age'] = 2021 - data['Year_Birth']","e1700bc4":"data.columns","db93f624":"cat_count = 0\nnum_count = 0\nfor i in data.dtypes:\n    if i == 'object':\n        cat_count = cat_count + 1\n    else:\n        num_count = num_count + 1\n\nprint(\"The Number of Numerical variables :\",num_count)\nprint(\"The Number of qualitative variables:\",cat_count)","33984054":"data_group = data.groupby(['Marital_Status','Education']).aggregate({'Income':'median'})\ndata_group","9272139c":"df = data\n\ndf['Income'] = np.where((df['Income'].isnull() == True) &(df[\"Education\"] == \"Basic\") & (df['Marital_Status'] == 'Divorced'), 9548.0, df['Income'])\ndf['Income'] = np.where((df['Income'].isnull() == True) &(df[\"Education\"] == \"Graduation\") & (df['Marital_Status'] == 'Divorced'),55635.0, df['Income'])\ndf['Income'] = np.where((df['Income'].isnull() == True) &(df[\"Education\"] == \"Masters\") & (df['Marital_Status'] == 'Divorced'), 49297.0, df['Income'])\ndf['Income'] = np.where((df['Income'].isnull() == True) &(df[\"Education\"] == \"PhD\") & (df['Marital_Status'] == 'Divorced'), 50613.5, df['Income'])\ndf['Income'] = np.where((df['Income'].isnull() == True) &(df[\"Education\"] == \"Basic\") & (df['Marital_Status'] == 'Married'), 22352.0, df['Income'])\ndf['Income'] = np.where((df['Income'].isnull() == True) &(df[\"Education\"] == \"Graduation\") & (df['Marital_Status'] == 'Married'), 50737.0, df['Income'])\ndf['Income'] = np.where((df['Income'].isnull() == True) &(df[\"Education\"] == \"Masters\") & (df['Marital_Status'] == 'Married'), 50017.0, df['Income'])\ndf['Income'] = np.where((df['Income'].isnull() == True) &(df[\"Education\"] == \"PhD\") & (df['Marital_Status'] == 'Married'), 57081.5, df['Income'])\ndf['Income'] = np.where((df['Income'].isnull() == True) &(df[\"Education\"] == \"Basic\") & (df['Marital_Status'] == 'Single'), 16383.0, df['Income'])\ndf['Income'] = np.where((df['Income'].isnull() == True) &(df[\"Education\"] == \"Graduation\") & (df['Marital_Status'] == 'Single'), 49973.5, df['Income'])\ndf['Income'] = np.where((df['Income'].isnull() == True) &(df[\"Education\"] == \"Masters\") & (df['Marital_Status'] == 'Single'), 49514.0, df['Income'])\ndf['Income'] = np.where((df['Income'].isnull() == True) &(df[\"Education\"] == \"PhD\") & (df['Marital_Status'] == 'Single'), 48918.0, df['Income'])\ndf['Income'] = np.where((df['Income'].isnull() == True) &(df[\"Education\"] == \"Basic\") & (df['Marital_Status'] == 'Together'), 23179.0, df['Income'])\ndf['Income'] = np.where((df['Income'].isnull() == True) &(df[\"Education\"] == \"Graduation\") & (df['Marital_Status'] == 'Together'), 53977.0, df['Income'])\ndf['Income'] = np.where((df['Income'].isnull() == True) &(df[\"Education\"] == \"Masters\") & (df['Marital_Status'] == 'Together'), 47586.5, df['Income'])\ndf['Income'] = np.where((df['Income'].isnull() == True) &(df[\"Education\"] == \"PhD\") & (df['Marital_Status'] == 'Together'), 56756.0, df['Income'])\ndf['Income'] = np.where((df['Income'].isnull() == True) &(df[\"Education\"] == \"Basic\") & (df['Marital_Status'] == 'Widow'), 22123.0, df['Income'])\ndf['Income'] = np.where((df['Income'].isnull() == True) &(df[\"Education\"] == \"Graduation\") & (df['Marital_Status'] == 'Widow'), 58275.0, df['Income'])\ndf['Income'] = np.where((df['Income'].isnull() == True) &(df[\"Education\"] == \"Masters\") & (df['Marital_Status'] == 'Widow'), 51459.5, df['Income'])\ndf['Income'] = np.where((df['Income'].isnull() == True) &(df[\"Education\"] == \"PhD\") & (df['Marital_Status'] == 'Widow'), 57032.0, df['Income'])\n\ndata = df","22d3191e":"data['Income'].isnull().sum()","a5ee90e7":"data_continous = data[['Income','MntWines', 'MntFruits',\n       'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts',\n       'MntGoldProds', 'Age', 'Expenditure']]\ndata_continous.head()","49d955d0":"plt.figure(figsize = (10,5))\ndata_continous.hist(figsize = (15,7), color = \"plum\", bins = 25)\nplt.tight_layout()","5ffd9071":"df1 = df[[\"Age\",\"MntWines\",\"MntFruits\",\"MntMeatProducts\",\"MntSweetProducts\",\"MntFishProducts\",\"MntGoldProds\"]]\ndf2 = df[\"Income\"]\ndf3 = df[\"Expenditure\"]\nfig = plt.figure(figsize =(12, 9))\nax = fig.add_subplot(211)\n\n#creating boxplot\nbp = ax.boxplot(df1, patch_artist = True, vert = 0)\n\n# setting axis names\nax.set_yticklabels([\"Age\",'MntWines','MntFruits','MntMeat',\n                     'MntSweet','MntFish','MntGold'])\nax = fig.add_subplot(212)\n\n#creating boxplot\nbp = ax.boxplot(df2, patch_artist = True, vert = 0)\n\n# setting axis names\nax.set_yticklabels([\"Income\"])\n\nplt.show()","b0b04970":"# Calculating Frequency:\ndata['Frequency'] = data['NumStorePurchases'] + data['NumCatalogPurchases'] + data['NumWebPurchases']\n\n# Renaming the variables of recency ,Expenditure to Monetory , Recent_purchase.\ndata.rename(columns = {'Expenditure':'Monetory','Recency':'Recent_Purchase'}, inplace = True)","55d0b3f6":"data.columns","c5db6b64":"data['Country'].value_counts()","67c7d5cd":"df_spain = data[(data['Country'] == 'SP')].reset_index(drop = True)\ndf_spain = pd.DataFrame(df_spain[['ID','Recent_Purchase','Frequency','Monetory']])\ndf_spain.head()","f9470279":"df_spain.describe()","f89c4aaf":"quantiles_recency = df_spain.quantile(q = [0.25,0.50,0.75])\nquantiles_recency","9c23c7df":"quantiles_recency = df_spain.quantile(q = [0.25,0.50,0.75])\n\n\n# Creating functions to determine the RFM score:\ndef R_score(data):\n    if data < quantiles_recency['Recent_Purchase'][0.25]:\n        return 3\n    elif data < quantiles_recency['Recent_Purchase'][0.75]:\n        return 2\n    else:\n        return 1\n    \ndef F_score(data):\n    if data < quantiles_recency['Frequency'][0.25]:\n        return 3\n    elif data < quantiles_recency['Frequency'][0.75]:\n        return 2\n    else:\n        return 1    \n    \n    \ndef M_score(data):\n    if data < quantiles_recency['Monetory'][0.25]:\n        return 3\n    elif data < quantiles_recency['Monetory'][0.75]:\n        return 2\n    else:\n        return 1     ","116b2f11":"df_spain['R_score'] = df_spain.Recent_Purchase.apply(lambda x : R_score(x))\ndf_spain['F_score'] = df_spain.Frequency.apply(lambda x : F_score(x))\ndf_spain['M_score'] = df_spain.Monetory.apply(lambda x : M_score(x))\ndf_spain['rfm_score'] = df_spain['R_score'].map(str) + df_spain['F_score'].map(str) + df_spain['M_score'].map(str) ","038ea710":"df_spain.head()","93bb3e12":"plt.figure(figsize = (14,5))\n\nplt.subplot(1,3, 1)\nsns.distplot(x = df_spain['Recent_Purchase'], kde = True, color = 'plum')\n\nplt.subplot(1,3,2)\nsns.distplot(x = df_spain['Frequency'], kde = True, color = 'aquamarine')\n\nplt.subplot(1,3,3)\nsns.distplot(x = df_spain['Monetory'], kde = True, color = 'yellow')","deae08d5":"df_spain.skew()","292f600a":"df_log = df_spain.copy()\ndf_log['Monetory'] = np.log1p(df_log['Monetory'])","5ef970ce":"df_log.head()","56917a78":"warnings.filterwarnings(\"ignore\")\nplt.figure(figsize = (15,6))\n\nplt.subplot(1,2,1)\nsns.distplot(x = df_log['Monetory'], kde = True, color = \"mediumblue\")\nplt.title('After Transformation')\n\nplt.subplot(1,2,2)\nprobplot(df_log['Monetory'], dist = 'norm', plot=plt)\nplt.title(\"After Transformation QQ-Plot\")","929c6e58":"# Normalizing the data :\nscaler = preprocessing.MinMaxScaler()\ndf_spain_norm = pd.DataFrame(scaler.fit_transform(df_log[['Recent_Purchase','Frequency','Monetory']]))\ndf_spain_norm.columns = ['Recent_Purchase','Frequency','Monetory']\n\ndf_spain_norm.head()","36e2db60":"df_spain_norm.describe()","cbe9e977":"fig = plt.figure(figsize = (10,8))\nax = fig.add_subplot(projection='3d')\n\nim = ax.scatter(df_spain_norm[\"Recent_Purchase\"],df_spain_norm[\"Frequency\"],df_spain_norm[\"Monetory\"],\n               color = 'darkturquoise',s = 75)       \nplt.title('Graphical Representation of RFM ',fontsize = 20)\nax.set_xlabel(\"Recency\")\nax.set_ylabel(\"Frequency\")\nax.set_zlabel(\"Monetory\")\nplt.show()","ad3a93f8":"#Define Kmeans Algorithm and returning the centroids, clusters, interia :\ndef Kmeans_algo(data,n):\n    \"\"\" Function for Kmeans Clustering.\"\"\"\n    \n    kmeans_model = KMeans(n_clusters = n, init = 'k-means++', max_iter = 300,\n                         random_state = 42, algorithm = 'elkan')\n    kmeans_model.fit(data)\n    \n    #creating centroids:\n    centroids = kmeans_model.cluster_centers_\n    \n    #creating labels:\n    labels = kmeans_model.labels_\n    df_spain_norm['label'] = labels\n    \n    #evaluation metrics:\n    interia = kmeans_model.inertia_\n    sil_score = metrics.silhouette_score(data, labels)\n    \n    return interia, labels, centroids, sil_score","0af43caf":"# creating a dataframe :\nx3 = df_spain_norm[['Recent_Purchase','Frequency','Monetory']]\n\nx3_interia_score = []\nx3_s_score = []\n\nfig = plt.figure(figsize = (15,15))\n\n#call the function :\nfor i in range(2,11):\n    x3_interia, x3_lables, x3_centroids, x3_sil_score = Kmeans_algo(x3, i)\n    x3_interia_score.append(x3_interia)\n    x3_s_score.append(x3_sil_score)\n    centroids_df = pd.DataFrame(x3_centroids, columns = ['Recent_Purchase','Frequency','Monetory'])\n    \n    ax = fig.add_subplot(330 + i - 1, projection='3d')\n    \n    #scatter plot:\n    ax.scatter(df_spain_norm['Recent_Purchase'],df_spain_norm['Frequency'],df_spain_norm['Monetory'],\n                s = 30, c = df_spain_norm[\"label\"], cmap = \"viridis\")\n    ax.scatter(centroids_df['Recent_Purchase'], centroids_df['Frequency'], centroids_df['Monetory'], \n               s = 90, marker= \",\", color = \"r\")\n    ax.set_xlabel('Recency')\n    ax.set_ylabel('frequency')\n    ax.set_zlabel('Monetory')","10ab412a":"#elbow method:\nplt.figure(11, figsize = (15,5))\nplt.plot(np.arange(2,11), x3_interia_score, '-')\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"Inertia Values\")    ","7c8ea813":"# plot inertia values against number of clusters\nplt.figure(11, figsize=(15,5))\nplt.plot(np.arange(2,11) , x3_s_score)\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"Silhouette Score\")","5d3604a3":"d1 = df_spain_norm[['Recent_Purchase','Frequency','Monetory']]\nmodel1 = KMeans(n_clusters = 4, max_iter = 400, random_state = 42)\nmodel1.fit(d1)\nd1['labels'] = model1.labels_","6786e5f6":"fig = plt.figure(figsize = (10,7))\nax = fig.add_subplot(projection='3d')\n\nax.scatter(d1[\"Recent_Purchase\"], d1[\"Frequency\"], d1[\"Monetory\"],\n               c = model1.labels_ , cmap = \"viridis\" ,s = 75)       \nplt.title('Clusters Obtained using KMeans',fontsize = 20)\nax.set_xlabel(\"Recency\")\nax.set_ylabel(\"Frequency\")\nax.set_zlabel(\"Monetory\")\nplt.show()","88400959":"score1 = metrics.silhouette_score(d1[['Recent_Purchase','Frequency','Monetory']], model1.labels_, metric='euclidean')\nc1 = metrics.calinski_harabasz_score(d1[['Recent_Purchase','Frequency','Monetory']], model1.labels_)","f38c1fe3":"df_spain_dbscan = df_spain_norm[['Recent_Purchase','Frequency','Monetory']]\ndf_spain_dbscan.head()","cfa3949b":"Mnt_pnts = len(df_spain_dbscan.columns)*2\nprint('The Minimum Points under Consideration is :', Mnt_pnts)","5f04ac24":"# Firstly, finding average distance using KNN:\nneighbours = NearestNeighbors(n_neighbors = Mnt_pnts)\nknn_data = neighbours.fit(df_spain_dbscan)\n\n#Obtaining indices and distances :\ndistances, indices = knn_data.kneighbors(df_spain_dbscan)\n\n#Sorting the distance:\nsorting_distance = np.sort(distances, axis = 0)\n\n#sorted distance\nsorted_distance = sorting_distance[:,1]\n\n#plotting between epsilon and distance\nplt.figure(figsize = (12,6))\nplt.plot(sorted_distance)\nplt.xlabel('Distance')\nplt.ylabel('Epsilon')\nplt.show()","1fb7fc6f":"# Creating a dataframe and using DBSCAN algorithm:\nx1 = df_spain_dbscan[['Recent_Purchase','Frequency','Monetory']]\ndbscan_model = DBSCAN(eps = 0.120, min_samples = Mnt_pnts).fit(df_spain_dbscan)\ndf_spain_dbscan['labels'] = dbscan_model.labels_\nprint('The Number of Clusters Obtained will be :')\nprint(df_spain_dbscan['labels'].value_counts())\n\n#calculating Silhoutte Score:\nprint(\"\")\nscore = metrics.silhouette_score(x1, dbscan_model.labels_)\nprint(\"The Silhoutte Score is:\", score)","5971de6d":"#plotting :\nfig = plt.figure(figsize = (10,7))\nax = fig.add_subplot(projection='3d')\n\nax.scatter(df_spain_dbscan[\"Recent_Purchase\"],df_spain_dbscan[\"Frequency\"],df_spain_dbscan[\"Monetory\"],\n               c = df_spain_dbscan[\"labels\"] , cmap =\"viridis\",s = 75)       \nplt.title('Clusters Obtained using DBSCAN',fontsize = 20)\nax.set_xlabel(\"Recency\")\nax.set_ylabel(\"Frequency\")\nax.set_zlabel(\"Monetory\")\nplt.show()","efb82ec0":"d2 = df_spain_dbscan[['Recent_Purchase','Frequency','Monetory']]\nmodel2 = DBSCAN(eps = 0.120, min_samples = Mnt_pnts)\nmodel2.fit(d2)","e6110630":"score2 = metrics.silhouette_score(d2, model2.labels_, metric='euclidean')\nc2 = metrics.calinski_harabasz_score(d2, model2.labels_)","b5976ac2":"df_spain_agglo = df_spain_norm[['Recent_Purchase','Frequency','Monetory']]\ndf_spain_agglo.head()","1f55f445":"plt.figure(figsize = (10,6))\nshc.dendrogram(shc.linkage(df_spain_agglo, method='ward'))\nplt.axhline(y = 8, color='r', linestyle='--')\nplt.show()","1ebcd053":"#validation\nagglo_s_score = []\nx1 = df_spain_agglo[['Recent_Purchase','Frequency','Monetory']]\n\nfor n_cluster in range(2,7):\n    agglo_s_score.append(metrics.silhouette_score(x1, AgglomerativeClustering(n_clusters = n_cluster).fit_predict(x1))) \n    \n    \nk = [2, 3, 4, 5, 6] \nplt.bar(k,agglo_s_score, color = 'plum' ) \nplt.xlabel('Number of clusters', fontsize = 10) \nplt.ylabel('Silhouette Score', fontsize = 10) \nplt.show()","533ab98a":"x1 = df_spain_agglo[['Recent_Purchase','Frequency','Monetory']]\n\n\nagglo_hier_model = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward') \nagglo_hier_model.fit(df_spain_agglo)\ndf_spain_agglo['labels'] = agglo_hier_model.labels_\n\nfig = plt.figure(figsize = (10,10))\nax = fig.add_subplot(projection = '3d')\nax.scatter(df_spain_agglo[\"Recent_Purchase\"],df_spain_agglo[\"Frequency\"],df_spain_agglo[\"Monetory\"],\n               c = df_spain_agglo['labels'], cmap =\"viridis\",s = 75) \n\nplt.title('Clusters Obtained using Agglomerative Clustering',fontsize = 20)\nax.set_xlabel(\"Recency\")\nax.set_ylabel(\"Frequency\")\nax.set_zlabel(\"Monetory\")\nplt.show()","a82319e9":"d3 = df_spain_agglo[['Recent_Purchase','Frequency','Monetory']]\nmodel3 = AgglomerativeClustering(n_clusters = 3, affinity='euclidean', linkage='ward') \nmodel3.fit(d3)","ac3e0462":"score3 = metrics.silhouette_score(d3, model3.labels_, metric='euclidean')\nc3 = metrics.calinski_harabasz_score(d3, model3.labels_)","74b15378":"#dataframe of Silhoutte Score:\nvalidation_df = pd.DataFrame({'Algorithms': ['KMeans','DBSCAN',\"Agglomerative Clustering\"],\n                              \"Silhouette Score\": [score1, score2, score3]})\n\n# dataframe of calinski:\nvalidation_df_cal = pd.DataFrame({'Algorithms': ['KMeans','DBSCAN',\"Agglomerative Clustering\"],\n                              \"Calinski Score\": [c1,c2,c3]})","ee4ddda5":"plt.figure(figsize = (12,6))\n\nplt.subplot(1,2,1)\nsns.barplot(x = validation_df['Algorithms'], y = validation_df['Silhouette Score'], \n              palette = 'rocket')\n\nplt.subplot(1,2,2)\nsns.barplot(x = validation_df_cal['Algorithms'], y = validation_df_cal['Calinski Score'], \n              palette = 'rocket')","64ef543e":"df1 = pd.DataFrame(df_spain[['ID','Recent_Purchase','Frequency','Monetory']])\ndf2 = pd.DataFrame(d1['labels'])\n\nfinal_df = pd.concat([df1,df2], axis = 1)\nfinal_df.head()","d4b156fa":"final_df_cluster0 = final_df[(final_df['labels'] == 0)].reset_index()   # Loyal \nfinal_df_cluster0.describe() ","eff3fedc":"final_df_cluster1 = final_df[(final_df['labels'] == 1)].reset_index()   \nfinal_df_cluster1.describe()","6f8bc45c":"final_df_cluster2 = final_df[(final_df['labels'] == 2)].reset_index()   #potential\nfinal_df_cluster2.describe()","40ddcd3a":"final_df_cluster3 = final_df[(final_df['labels'] == 3)].reset_index()\nfinal_df_cluster3.describe()","5f635a16":"Here, we have variables such as 'Recency', 'NumWebVisitsMonth' and 'Expenditure' which determines the RFM for our dataset.","4797b02a":"### -> Applying the Updated Algorithm :","cd3907c6":"### -> Applying the Updated Algorithm :","1009360c":"*The K-Means Clustering Algorithm works well when the data is not skewed. Moreover, it uses distance based measurements because of which it becomes extermely important to scale the data and also treat outliers as it is not robust.*","e03701e8":"### -> HyperParameter Tuning For K-Means:","d33f4388":"### -> Determining the Missing values:","4b172c7f":"*This is a marketing data set which contains data of the customers belonging ABC company. This data set has 2240 rows and 28 columns.Since this is a marketing data set, we can categorize the variables in terms of the 4 P's of marketing: products, people (customers), places (channels), and promotions (discounts & campaigns).\nThe variables are as follows:*\n\n### *People:*\n- *ID: Customer's unique identifier*\n- *Year_Birth: Customer's birth year*\n- *Education: Customer's education level*\n- *Marital_Status: Customer's marital status*\n- *Income: Customer's yearly household income*\n- *Kidhome: Number of children in customer's household*\n- *Teenhome: Number of teenagers in customer's household*\n- *Dt_Customer: Date of customer's enrollment with the company*\n- *Recency: Number of days since customer's last purchase*\n- *Complain: 1 if customer complained in the last 2 years, 0 otherwise*\n- *Country: Customer's location*\n\n### *Products:*\n- *MntWines: Amount spent on wine in the last 2 years*\n- *MntFruits: Amount spent on fruits in the last 2 years*\n- *MntMeatProducts: Amount spent on meat in the last 2 years*\n- *MntFishProducts: Amount spent on fish in the last 2 years*\n- *MntSweetProducts: Amount spent on sweets in the last 2 years*\n- *MntGoldProds: Amount spent on gold in the last 2 years*\n\n### *Promotions:*\n- *NumDealsPurchases: Number of purchases made with a discount*\n- *AcceptedCmp3: 1 if customer accepted the offer in the 3rd campaign, 0 otherwise*\n- *AcceptedCmp4: 1 if customer accepted the offer in the 4th campaign, 0 otherwise*\n- *AcceptedCmp5: 1 if customer accepted the offer in the 5th campaign, 0 otherwise*\n- *AcceptedCmp1: 1 if customer accepted the offer in the 1st campaign, 0 otherwise*\n- *AcceptedCmp2: 1 if customer accepted the offer in the 2nd campaign, 0 otherwise*\n- *Response: 1 if customer accepted the offer in the last campaign, 0 otherwise*\n\n### *Places:*\n- *NumWebPurchases: Number of purchases made through the company's web site*\n- *NumCatalogPurchases: Number of purchases made using a catalogue*\n- *NumStorePurchases: Number of purchases made directly in stores*\n- *NumWebVisitsMonth: Number of visits to company's web site in the last month*","4ed85661":"*From the above graph, there is a clear peak at k = 4.  Hence it is optimal.*","3f31cdb4":"*Here The income Variable is of the datatype string , which has to be converted into Integer.*","cbefff8b":"#### *DBSCAN is clustering algorithm which is ruboust to outliers or noise. Before the implementation of this algoritjm its very important we find the optimal value for two important parameters EPSILON and MNT POINTS.*\n- ***Epsilon :  We will using the KNearestNeighbours to compute the average distance for each data point. Therefore we need to first find the optimal value for the nearest neighbours to be considered.***\n- ***Minimum Points :  Number of Dimensions *2***","7a6d6883":"- ***CLUSTER 2: Loyal Customers, who are the champions of all the customers with highest score of RFM.***\n- ***CLUSTER 0: Customers who have the most potential to become the Champions. They recently buy the products and they make their purchases quite frequently. Moreover, the amount they spent is quite great.***\n- ***CLUSTER 1: Customers who usually purchase products during some offer\/discounts.Their RFM score is quite low due to which they are more likely to be churned out.***\n- ***CLUSTER 3: Customers who churn***","f9cd00a8":"#### -> Validation on the Number of Clusters Chose by Silhouette Method:","7ad27c34":"### -> Scaling the Features in the Dataset:","8acd64b9":"## <font color = black>3).<u>Data Cleaning and Analysis.<u><\/font>","898f487e":"***It is a type of Clustering algorithm where clusters are successively merged based on the similarity measure until all clusters have been merged into one big cluster containing all objects.***","7c82327e":"### -> Grouping into Clusters:","998e49cb":"## <font color = black>1). <u>Problem Statement Under Consideration:<u><\/font>\n    \n### *Targeting High Value Customers to the company based on the Customer attributes in order to have effective implementation of marketing strategies and also for optimal resource allocation.*","e575ddf0":"### -> Choosing the Right Country for the Analysis:","800a1ab1":"### iv). Addition of New Variables:","4e00a4c8":"## <font color = black>2).<u>Big Picture on the Dataset.<u><\/font>","a53670a0":"### ii). Determining the optimal value for Epsilon:","0b239654":"***Based on Customer Segmentation that has been done in SPAIN, high-value Customers are those who either belong to CLUSTER 2 or 0. Moreover, the company can also try implementing some effective Marketing Strategies to retain the customers who are more likely to be churned out.***","82393d17":"### i). Determining the Number of Clusters:","2e0446d8":"## -> <u> Validation of the Clustering algorithms:<u>","e45dd950":"### iii) Conversion of the datetime format:","e7f27eb8":"####  *Depending on the granularity requirement of the company , each customer is scored on the RFM attributes on a scale of 1\u20133, 3 being the highest.* ","9d460ecf":"***Here, we can observe that the datapoints do not follow any pattern rather have been clustered over a certain area clearly depicting it is a convex cluster.***","d7a281a2":"The above table gives the 5 -table summary of the all the columns of the newly selected dataframe of Spain.","7fd0e16c":"### i). Conversion of the datatype of Income variable:","ab5b7256":"## -> Data Cleaning :","030edcce":"## 5). <u> Interpretation and Conclusion:<u>","cb601d8f":"*This dataset contains 2240 rows and 28 columns.*","b2f4cd0f":"*Here, the ideal value for \u03b5 will be equal to the distance value at the \u201ccrook of the elbow\u201d, or the point of maximum curvature. Zooming in on the k-distance plot, it looks like the optimal value for \u03b5 is around 0.100 to 0.120. We ended up looping through combinations of MinPts and \u03b5 values slightly above and below the values estimated here to find the model of best fit.*\n- ***The ideal value chose for Epsilon will be 0.120***","d1287d5c":"#### -> Validation on the Number of Clusters Chose by Silhouette Method:","be5ee8b6":"### -> Determining the 5 - Number Summary :","437770e8":"*RFM analysis is a customer behavior segmentation technique. Based on customers\u2019 historical transactions, RFM analysis focuses on 3 main aspects of customers\u2019 transactions:*\n- *Recency(R)  : How recently customers have made their purchases.*\n- *Frequency(F): How often customers have made their purchases.*\n- *Monetary(M) : How much money customers have paid for their purchases.*","77eb395e":"## 4).<u>  Model Building : <u>","c2f6cc53":"### -> Creating a dataframe of Spain:","f6282bd0":"***The K-means algorithm identifies k number of centroids, and then allocates every data point to the nearest cluster, while keeping the centroids as small as possible.***","fdf1f428":"### ii). Determining the distributions of Continous variables:","a84c3802":"## <font color = black> -> <u>*Various clustering Algorithms Under Consideration:* <u><\/font>","80d5c99b":"## 2). DBSCAN Algorithm:","0195a3dd":"## -> Data Analysis :","bb7f12da":"## 1). K - Means Clustering Algorithm :","036a0791":"### -> Determining the distributions of Variables:","6d9fc918":"***The elbow method runs k-means clustering on the dataset for a range of values for k (say from 1-10) and then for each value of k computes an average score for all clusters.***","282d7612":"###  i). Imputation of Missing values:","09b32c84":"*Here, the variable 'INCOME' is of the string datatype and it also has a '$' symbol,  which has to be removed.* ","42fba87b":"Clearly, the variables 'Recent_Purchase' and 'Frequency' are lying between [-0.5, 0.5] indicating that they are symmetric(Normally Distributed). On the other hand, the skewness of the variable 'Monetory' is 0.88 indicating the data is highly skewed.","0e7393fa":"# <u> **Customer Segmentation on Marketing Analytics using RFM Analysis.** <u> ","3fd08070":"###  -> Determining the Shape and Structure on the dataset:","8d8090f5":"### <u> <font color = black> *Steps Involved In The Process.* <u> <\/font>\n- ***Determining the Problem Statement***\n- ***Big Picture on the Dataset***\n- ***Data Cleaning and Analysis***\n- ***Model Building*** \n- ***Interpretation and Conclusion***","7c8c8f2d":"***After setting the RFM, we will now look at the number of the customers in each of the seven countries given in our datatset.\nWe will then select the country with the highest customers for our analysis as the maximum revenue for the company is determined by that country.***","e6c580c1":"***Here, we choose KMeans Algorithm because of which we have 4 different clusters formed. It is now important we identify the clusters and group the cistomers accordingly.***","75676ae5":"### -> Determining the RFM Score:","41921503":"### iii). Detecting the Outliers in Continous Variables:","cf36ea4a":"*As Plotted, the plot looks like an arm with a clear elbow at k = 4.*","1e63aa58":"## 3). Agglomerative Hierarchical clustering:","de93dd97":"### -> Conclusion based on the Clusters Formed:","7eee370d":"## <font color = black>4). <u>Model Building Based on RFM Analysis: <u><\/font>","65babeaf":"***Clearly, considering the above two scores we can conclude that KMeans is better fit model to our dataset.***","86b70701":"### -> Applying the  Updated Algorithm:","d41530eb":"*Clearly, the number of Clusters to be Chosen is 3.*","f0da6667":"### ->Hyper Parameter Tuning:\n### i)Determining the optimal Number of Minimum Points:","6809ca59":"###  ii)  Merging rows of two variables:","03b7e4a0":"*The above table provides us a understanding on the structure of the data set. This enables us in simplifying the data and also helps in understanding the insights of the data by providing the frequently repeated numbers, the middle of all the numbers , variations among the variables etc. This helps in knowing whether the data is consistent or inconsistent for the further analysis.*"}}