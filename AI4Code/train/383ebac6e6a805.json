{"cell_type":{"cb218ef7":"code","d536a534":"code","f680e200":"code","2e74e940":"code","01473140":"code","eb5d339f":"code","9cf43532":"code","77d513d3":"code","916ee1d5":"code","3690b233":"code","616e5bb9":"code","88e7bfc6":"code","37e1a2d1":"code","c3630702":"code","5ff07a18":"code","a747c65d":"code","d3779a1b":"code","93b86c48":"code","d9b031b7":"code","93dea48e":"code","2a06ba04":"markdown","1b50d3e7":"markdown","7e1bc308":"markdown","dd6a0c79":"markdown","65f4e2c6":"markdown","6fd5db72":"markdown","dbb30cb7":"markdown","7a61959e":"markdown","2efc8091":"markdown","56b4a9e8":"markdown","366835b1":"markdown"},"source":{"cb218ef7":"import itertools\nimport re\nimport random\nimport csv\nimport ast\nimport json\nimport pickle\nimport pprint\n\nfrom timeit import default_timer as timer\nfrom datetime import datetime\n\n# Data manipulation\nimport pandas as pd\nimport numpy as np\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Model\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.metrics import make_scorer, precision_score,roc_auc_score\nfrom mlens.ensemble import SuperLearner\n\n# Hyperopt\nfrom hyperopt import hp, tpe, fmin, Trials, STATUS_OK\nfrom hyperopt.pyll.stochastic import sample\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom IPython.display import display\n\npd.options.display.max_rows = 100\n\n%matplotlib inline","d536a534":"# Set a few plotting defaults\nplt.style.use('fivethirtyeight')\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 14\nplt.rcParams['patch.edgecolor'] = 'k'","f680e200":"RANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)","2e74e940":"train_raw = pd.read_csv('..\/input\/train.csv')\ntest_raw = pd.read_csv('..\/input\/test.csv')\n\ntrain_raw['train'] = 1\ntest_raw['train'] = 0\ndata_all = pd.concat([train_raw, test_raw], axis=0).reset_index(drop=True)\n\ndata_all.Embarked = data_all.Embarked.fillna(data_all.Embarked.mode()[0])\n\nle = LabelEncoder()\n\ndata_all.Sex = le.fit_transform(data_all[['Sex']])\ndata_all.Embarked = le.fit_transform(data_all[['Embarked']])\n\ndata_all['family_size'] = data_all.SibSp + data_all.Parch + 1\n\ndef calc_family_size_bin(family_size):\n    if family_size == 1:\n        return 0\n    elif family_size <= 4: \n        return 1\n    else:\n        return 2\n        \ndata_all['family_size_bin'] = data_all.family_size.map(calc_family_size_bin)\ndata_all['name_title'] = data_all.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\nname_title_dict = {\n    'Capt': 'Mr',\n    'Col': 'Mr',\n    'Don': 'Mr',\n    'Dona': 'Mrs',    \n    'Dr': 'Dr',\n    'Jonkheer': 'Mr',\n    'Lady': 'Mrs',\n    'Major': 'Mr',\n    'Master': 'Master',\n    'Miss': 'Miss',\n    'Mlle': 'Miss',\n    'Mme': 'Miss',\n    'Mr': 'Mr',\n    'Mrs': 'Mrs',\n    'Ms': 'Mrs',\n    'Rev': 'Mr',\n    'Sir': 'Mr',\n    'Countess': 'Mrs'\n}\n\ndata_all['name_title_cat'] = data_all.name_title.map(name_title_dict)\n\ndata_all['last_name'] = data_all.Name.str.extract('([A-Za-z]+),', expand=False)\ndata_all['last_name_family_size'] = data_all.apply(lambda row: row.last_name + '_' + str(row.family_size), axis=1)\ndata_all['last_name_ticket'] = data_all.apply(lambda row: row.last_name + '_' + row.Ticket, axis=1)\n\nlast_name_family_size_check = data_all[data_all.family_size > 1].groupby('last_name_family_size').agg({'Survived': lambda x: x.isnull().sum()}).reset_index()\nlast_name_family_size_check.columns = ['last_name_family_size','last_name_family_size_feature']\n\ndata_all = pd.merge(data_all, last_name_family_size_check, on='last_name_family_size', how='left')\ndata_all = data_all.sort_values('PassengerId').reset_index(drop=True)\ndata_all.last_name_family_size_feature = data_all.last_name_family_size_feature.fillna(0)\n\ndata_all.loc[data_all.last_name_family_size_feature == 0, 'last_name_family_size'] = 'X'\n\nfamily_count = data_all.groupby(['last_name','Ticket']).PassengerId.count().reset_index()\nfamily_count.columns = ['last_name','Ticket','family_count']\n\nfamily_survival = data_all.groupby(['last_name','Ticket']).Survived.sum().reset_index()\nfamily_survival.columns = ['last_name','Ticket','family_survival_sum']\n\nfamily_survival = pd.merge(family_count, family_survival, on=['last_name','Ticket'])\n\ndef cal_family_survival(row):\n    family_survival = 0.5\n    if row['family_count'] > 1 and row['family_survival_sum'] > 0:\n        family_survival = 1\n    elif row['family_count'] > 1 and row['family_survival_sum'] == 0:\n        family_survival = 0\n        \n    return family_survival\n\nfamily_survival['family_survival'] = family_survival.apply(cal_family_survival, axis=1)\n\ndata_all = pd.merge(data_all, family_survival, on=['last_name','Ticket'], how='left')\ndata_all = data_all.sort_values('PassengerId').reset_index(drop=True)\n\nticket_df = data_all.groupby('Ticket', as_index=False)['PassengerId'].count()\nticket_df.columns = ['Ticket','ticket_count']\n\ndata_all = pd.merge(data_all, ticket_df, on=['Ticket'])\ndata_all = data_all.sort_values('PassengerId').reset_index(drop=True)\n\nlast_name_ticket_check = data_all[data_all.ticket_count > 1].groupby('last_name_ticket').agg({'Survived': lambda x: x.isnull().sum()}).reset_index()\nlast_name_ticket_check.columns = ['last_name_ticket','last_name_ticket_feature']\n\ndata_all = pd.merge(data_all, last_name_ticket_check, on='last_name_ticket', how='left')\ndata_all = data_all.sort_values('PassengerId').reset_index(drop=True)\ndata_all.last_name_ticket_feature = data_all.last_name_ticket_feature.fillna(0)\n\ndata_all.loc[data_all.last_name_ticket_feature == 0, 'last_name_ticket'] = 'X'\n\ndef calc_family_count_bin(family_count):\n    if family_count == 1:\n        return 0\n    elif family_count <= 2: \n        return 1\n    elif family_count <= 4:\n        return 2\n    else:\n        return 3\n        \ndata_all['family_count_bin'] = data_all.family_count.map(calc_family_count_bin)\n\ndata_all.name_title_cat = le.fit_transform(data_all[['name_title_cat']])\ndata_all.last_name_family_size = le.fit_transform(data_all[['last_name_family_size']])\ndata_all.last_name_ticket = le.fit_transform(data_all[['last_name_ticket']])\n\ndata_all['fare_fixed'] = data_all.Fare\/data_all.ticket_count\nfare_median = data_all.fare_fixed.median()\ndata_all['fare_fixed'] = data_all.fare_fixed.fillna(fare_median)\ndata_all['fare_fixed_log'] = np.log1p(data_all.fare_fixed)\n\nage_median_by_sex_title = data_all.groupby(['Sex', 'name_title'], as_index=False).Age.median()\n\ndata_all = pd.merge(data_all, age_median_by_sex_title, on=['Sex', 'name_title'])\ndata_all['Age'] = data_all.apply(lambda row: row.Age_x if not np.isnan(row.Age_x) else row.Age_y, axis=1)\ndata_all = data_all.drop(['Age_x','Age_y'], axis=1).sort_values('PassengerId').reset_index(drop=True)\n\ndef calc_age_bin(age):\n    if age <= 15:\n        return 0\n    elif age <= 30:\n        return 1\n    elif age <= 60:\n        return 2\n    else:\n        return 3\n        \ndata_all['age_bin'] = data_all.Age.map(calc_age_bin)\n\ndef parse_ticket_str(ticket):\n    arr = ticket.split()\n    if not arr[0].isdigit():\n        txt = arr[0].replace('.', '')\n        txt = txt.split('\/')[0]\n        return re.findall('[a-zA-Z]+', txt)[0]\n    else:\n        return None\n        \ndata_all['ticket_str'] = data_all.Ticket.map(parse_ticket_str)\n\ndef parse_ticket_number(ticket):\n    arr = ticket.split()\n    if len(arr) == 1 and arr[0].isdigit():\n        return int(arr[0])\n    elif len(arr) == 2 and arr[1].isdigit():\n        return int(arr[1])\n    else:\n        if arr[-1].isdigit():\n            return int(arr[-1])\n        else:\n            return np.nan\n    \ndata_all['ticket_number'] = data_all.Ticket.map(parse_ticket_number)\n\ndef parse_ticket_num_len(ticket):\n    arr = ticket.split()\n    if len(arr) == 1 and arr[0].isdigit():\n        return len(arr[0])\n    elif len(arr) == 2 and arr[1].isdigit():\n        return len(arr[1])\n    else:\n        if arr[-1].isdigit():\n            return len(arr[-1])\n        else:\n            return -1\n    \ndata_all['ticket_num_len'] = data_all.Ticket.map(parse_ticket_num_len)\n\ndata_all['ticket_num_len_4_prefix'] = data_all[data_all.ticket_num_len == 4].ticket_number.map(lambda x: int(str(x)[0]))\ndata_all['ticket_num_len_4_prefix_2'] = data_all[data_all.ticket_num_len == 4].ticket_number.map(lambda x: int(str(x)[:2]))\n\ndata_all['ticket_num_len_5_prefix'] = data_all[data_all.ticket_num_len == 5].ticket_number.map(lambda x: int(str(x)[0]))\ndata_all['ticket_num_len_5_prefix_2'] = data_all[data_all.ticket_num_len == 5].ticket_number.map(lambda x: int(str(x)[:2]))\n\ndata_all['ticket_num_len_6_prefix'] = data_all[data_all.ticket_num_len == 6].ticket_number.map(lambda x: int(str(x)[0]))\ndata_all['ticket_num_len_6_prefix_2'] = data_all[data_all.ticket_num_len == 6].ticket_number.map(lambda x: int(str(x)[:2]))\ndata_all['ticket_num_len_6_prefix_3'] = data_all[data_all.ticket_num_len == 6].ticket_number.map(lambda x: int(str(x)[:3]))\n\ndata_all.ticket_str = data_all.ticket_str.fillna('X')\n\ndata_all.ticket_num_len_4_prefix = data_all.ticket_num_len_4_prefix.fillna(-1)\ndata_all.ticket_num_len_4_prefix_2 = data_all.ticket_num_len_4_prefix_2.fillna(-1)\ndata_all.ticket_num_len_5_prefix = data_all.ticket_num_len_5_prefix.fillna(-1)\ndata_all.ticket_num_len_5_prefix_2 = data_all.ticket_num_len_5_prefix_2.fillna(-1)\ndata_all.ticket_num_len_6_prefix = data_all.ticket_num_len_6_prefix.fillna(-1)\ndata_all.ticket_num_len_6_prefix_2 = data_all.ticket_num_len_6_prefix_2.fillna(-1)\ndata_all.ticket_num_len_6_prefix_3 = data_all.ticket_num_len_6_prefix_3.fillna(-1)\n\ndata_all.ticket_num_len_4_prefix = data_all.ticket_num_len_4_prefix.astype(int)\ndata_all.ticket_num_len_4_prefix_2 = data_all.ticket_num_len_4_prefix_2.astype(int)\ndata_all.ticket_num_len_5_prefix = data_all.ticket_num_len_5_prefix.astype(int)\ndata_all.ticket_num_len_5_prefix_2 = data_all.ticket_num_len_5_prefix_2.astype(int)\ndata_all.ticket_num_len_6_prefix = data_all.ticket_num_len_6_prefix.astype(int)\ndata_all.ticket_num_len_6_prefix_2 = data_all.ticket_num_len_6_prefix_2.astype(int)\ndata_all.ticket_num_len_6_prefix_3 = data_all.ticket_num_len_6_prefix_3.astype(int)\n\ndata_all.ticket_str = le.fit_transform(data_all[['ticket_str']])\n\ndata_all['cabin_cat'] = data_all.Cabin.fillna('X').str[0]\n\ndef calc_cabin_len(cabin):\n    if type(cabin) == float:\n        return 0\n    else:\n        return len(cabin.split())\n\ndata_all['cabin_len'] = data_all.Cabin.map(calc_cabin_len)\n\ndata_all.cabin_cat = le.fit_transform(data_all[['cabin_cat']])","01473140":"features = [\n    'Pclass', 'Sex',\n    'family_size_bin',\n    'name_title_cat', \n    'last_name_family_size', 'last_name_ticket',\n    'fare_fixed_log', 'age_bin',\n    'ticket_str',\n    'ticket_num_len_4_prefix_2',\n    'ticket_num_len_5_prefix_2',\n    'ticket_num_len_6_prefix_3',\n    'cabin_cat', 'cabin_len',\n]\n\nX_train = data_all[data_all.train == 1][features]\nX_test = data_all[data_all.train == 0][features]\ny_train = data_all[data_all.train == 1].Survived.astype(int)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nX_train.shape, X_test.shape","eb5d339f":"def cv_model(train, train_labels, model, name, cv=10, scoring='accuracy'):\n    \"\"\"Perform k fold cross validation of a model\"\"\"\n    \n    cv_scores = cross_val_score(model, train, train_labels, cv=cv, scoring=scoring, n_jobs=-1)\n    print(f'{cv} Fold CV {scoring} for {name}: {round(cv_scores.mean(), 5)} with std: {round(cv_scores.std(), 5)}')\n    \ndef make_prediction(train, target, test, model, model_name):\n    #cv_model(train, target, model, model_name)\n\n    model.fit(train, target)\n    pred = model.predict(test).astype(int)\n\n    output = f'{model_name}_submission_{datetime.now().strftime(\"%Y%m%d%H%M%S\")}.csv'\n    submit_df = pd.DataFrame()\n    submit_df['PassengerId'] = test_raw.PassengerId\n    submit_df['Survived'] = pred\n    \n    submit_df[['PassengerId','Survived']].to_csv(output, index=False)\n    print(f'submission file {output} is generated.')","9cf43532":"# cv val mean high\nxgb_params = {\n    'booster': 'gbtree',\n    'colsample_bytree': 0.8271901311986853,\n    'eval_metric': 'error',\n    'gamma': 0.3523281663529724,\n    'learning_rate': 0.030522858251466296,\n    'max_depth': 8,\n    'min_child_weight': 2,\n    'n_estimators': 750,\n    'objective': 'binary:logistic',\n    'reg_alpha': 0.4211368866611682,\n    'reg_lambda': 0.09667286427878569,\n    'scale_pos_weight': 1,\n    'silent': True,\n    'subsample': 0.7604104067052311\n}\n\nrf_params = {\n    'class_weight': 'balanced',\n    'max_features': 0.9650625848433683,\n    'min_samples_leaf': 2,\n    'min_samples_split': 5,\n    'n_estimators': 440\n}\n\nknn_params = {\n    'algorithm': 'brute',\n    'n_neighbors': 13,\n    'weights': 'distance'\n}\n\nlr_params = {\n    'C': 0.02,\n    'class_weight': 'balanced',\n    'penalty': 'l2'\n}\n\nsvc_params = {\n    'C': 10,\n    'class_weight': 'balanced'\n}\n\nxgb_model = xgb.XGBClassifier(**xgb_params, random_state=RANDOM_SEED)\nrf = RandomForestClassifier(**rf_params, n_jobs=-1, random_state=RANDOM_SEED)\nknn = KNeighborsClassifier(**knn_params, n_jobs=-1)\nlr = LogisticRegression(**lr_params, n_jobs=-1, random_state=RANDOM_SEED)\nsvc = SVC(**svc_params, probability=True, random_state=RANDOM_SEED)","77d513d3":"voting_hard_model = VotingClassifier(estimators=[('xgb',xgb_model), ('rf',rf), ('knn',knn), ('lr',lr), ('svc',svc)],\n                                voting='hard')\n\nmake_prediction(X_train, y_train, X_test, voting_hard_model, 'VOTING_HARD')","916ee1d5":"voting_soft_model = VotingClassifier(estimators=[('xgb',xgb_model), ('rf',rf), ('knn',knn), ('lr',lr), ('svc',svc)],\n                                voting='soft')\n\nmake_prediction(X_train, y_train, X_test, voting_hard_model, 'VOTING_SOFT')","3690b233":"voting_soft_model = VotingClassifier(estimators=[('xgb',xgb_model), ('rf',rf)],\n                                voting='soft')\n\nmake_prediction(X_train, y_train, X_test, voting_hard_model, 'VOTING_SOFT_XGB_RF')","616e5bb9":"stacking_model = SuperLearner(scorer='accuracy', random_state=RANDOM_SEED, verbose=2)\n\nstacking_model.add([('xgb',xgb_model), ('rf',rf), ('knn',knn), ('lr',lr), ('svc',svc)])\nstacking_model.add_meta(LogisticRegressionCV(cv=10, n_jobs=-1, random_state=RANDOM_SEED))\n\nmake_prediction(X_train, y_train, X_test, stacking_model, 'STACKING')","88e7bfc6":"stacking_model = SuperLearner(scorer='accuracy', random_state=RANDOM_SEED, verbose=2)\n\nstacking_model.add([('xgb',xgb_model), ('rf',rf)])\nstacking_model.add_meta(LogisticRegressionCV(cv=10, n_jobs=-1, random_state=RANDOM_SEED))\n\nmake_prediction(X_train, y_train, X_test, stacking_model, 'STACKING_XGB_RF')","37e1a2d1":"stacking_model = SuperLearner(scorer='auc', random_state=RANDOM_SEED, verbose=2)\n\nstacking_model.add([('xgb',xgb_model), ('rf',rf), ('knn',knn), ('lr',lr), ('svc',svc)])\nstacking_model.add_meta(LogisticRegressionCV(cv=10, scoring=make_scorer(roc_auc_score),\n                                             n_jobs=-1, random_state=RANDOM_SEED))\n\nmake_prediction(X_train, y_train, X_test, stacking_model, 'STACKING_AUC')","c3630702":"# stacking xgb rf with model tuning using auc 0.84688\nstacking_model = SuperLearner(scorer='auc', random_state=RANDOM_SEED, verbose=2)\n\nstacking_model.add([('xgb',xgb_model), ('rf',rf)])\nstacking_model.add_meta(LogisticRegressionCV(cv=10, scoring=make_scorer(roc_auc_score),\n                                             n_jobs=-1, random_state=RANDOM_SEED))\n\nmake_prediction(X_train, y_train, X_test, stacking_model, 'STACKING_XGB_RF_AUC')","5ff07a18":"train_raw = pd.read_csv('..\/input\/train.csv')\ntest_raw = pd.read_csv('..\/input\/test.csv')\n\ntrain_raw['train'] = 1\ntest_raw['train'] = 0\ndata_all = pd.concat([train_raw, test_raw], axis=0).reset_index(drop=True)\n\ndata_all.Embarked = data_all.Embarked.fillna(data_all.Embarked.mode()[0])\n\nle = LabelEncoder()\n\ndata_all.Sex = le.fit_transform(data_all[['Sex']])\ndata_all.Embarked = le.fit_transform(data_all[['Embarked']])\n\ndata_all['family_size'] = data_all.SibSp + data_all.Parch + 1\n\ndef calc_family_size_bin(family_size):\n    if family_size == 1:\n        return 0\n    elif family_size <= 4: \n        return 1\n    else:\n        return 2\n        \ndata_all['family_size_bin'] = data_all.family_size.map(calc_family_size_bin)\ndata_all['name_title'] = data_all.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\nname_title_dict = {\n    'Capt': 'Mr',\n    'Col': 'Mr',\n    'Don': 'Mr',\n    'Dona': 'Mrs',    \n    'Dr': 'Dr',\n    'Jonkheer': 'Mr',\n    'Lady': 'Mrs',\n    'Major': 'Mr',\n    'Master': 'Master',\n    'Miss': 'Miss',\n    'Mlle': 'Miss',\n    'Mme': 'Miss',\n    'Mr': 'Mr',\n    'Mrs': 'Mrs',\n    'Ms': 'Mrs',\n    'Rev': 'Mr',\n    'Sir': 'Mr',\n    'Countess': 'Mrs'\n}\n\ndata_all['name_title_cat'] = data_all.name_title.map(name_title_dict)\n\ndata_all['last_name'] = data_all.Name.str.extract('([A-Za-z]+),', expand=False)\ndata_all['last_name_family_size'] = data_all.apply(lambda row: row.last_name + '_' + str(row.family_size), axis=1)\ndata_all['last_name_ticket'] = data_all.apply(lambda row: row.last_name + '_' + row.Ticket, axis=1)\n\nticket_df = data_all.groupby('Ticket', as_index=False)['PassengerId'].count()\nticket_df.columns = ['Ticket','ticket_count']\n\ndata_all = pd.merge(data_all, ticket_df, on=['Ticket'])\ndata_all = data_all.sort_values('PassengerId').reset_index(drop=True)\n\nlast_name_family_size_check = data_all[data_all.family_size > 1].groupby('last_name_family_size').agg({'Survived': lambda x: x.isnull().sum()}).reset_index()\nlast_name_family_size_check.columns = ['last_name_family_size','last_name_family_size_feature']\n\nlast_name_ticket_check = data_all[data_all.ticket_count > 1].groupby('last_name_ticket').agg({'Survived': lambda x: x.isnull().sum()}).reset_index()\nlast_name_ticket_check.columns = ['last_name_ticket','last_name_ticket_feature']\n\ndata_all = pd.merge(data_all, last_name_family_size_check, on='last_name_family_size', how='left')\ndata_all = data_all.sort_values('PassengerId').reset_index(drop=True)\ndata_all.last_name_family_size_feature = data_all.last_name_family_size_feature.fillna(0)\n\ndata_all = pd.merge(data_all, last_name_ticket_check, on='last_name_ticket', how='left')\ndata_all = data_all.sort_values('PassengerId').reset_index(drop=True)\ndata_all.last_name_ticket_feature = data_all.last_name_ticket_feature.fillna(0)\n\ndata_all.loc[data_all.last_name_family_size_feature == 0, 'last_name_family_size'] = 'X'\ndata_all.loc[data_all.last_name_ticket_feature == 0, 'last_name_ticket'] = 'X'\n\nfamily_survival = data_all.groupby(['last_name_family_size','family_size']).Survived.sum().reset_index()\nfamily_survival.columns = ['last_name_family_size','family_size','family_survival_sum']\n\nfamily_ticket_count = data_all.groupby(['last_name','Ticket']).PassengerId.count().reset_index()\nfamily_ticket_count.columns = ['last_name','Ticket','family_ticket_count']\n\nfamily_ticket_survival = data_all.groupby(['last_name','Ticket']).Survived.sum().reset_index()\nfamily_ticket_survival.columns = ['last_name','Ticket','family_ticket_survival_sum']\n\nfamily_ticket_survival = pd.merge(family_ticket_count, family_ticket_survival, on=['last_name','Ticket'])\n\ndef calc_family_survival(row):\n    family_survival = 0.5\n    if row['family_size'] > 1 and row['family_survival_sum'] > 0:\n        family_survival = 1\n    elif row['family_size'] > 1 and row['family_survival_sum'] == 0:\n        family_survival = 0\n        \n    return family_survival\n\nfamily_survival['family_survival'] = family_survival.apply(calc_family_survival, axis=1)\n\ndef calc_family_ticket_survival(row):\n    family_ticket_survival = 0.5\n    if row['family_ticket_count'] > 1 and row['family_ticket_survival_sum'] > 0:\n        family_ticket_survival = 1\n    elif row['family_ticket_count'] > 1 and row['family_ticket_survival_sum'] == 0:\n        family_ticket_survival = 0\n        \n    return family_ticket_survival\n\nfamily_ticket_survival['family_ticket_survival'] = family_ticket_survival.apply(calc_family_ticket_survival,\n                                                                                axis=1)\n\ndata_all = pd.merge(data_all, family_survival, on=['last_name_family_size','family_size'], how='left')\ndata_all = data_all.sort_values('PassengerId').reset_index(drop=True)\n\ndata_all = pd.merge(data_all, family_ticket_survival, on=['last_name','Ticket'], how='left')\ndata_all = data_all.sort_values('PassengerId').reset_index(drop=True)\n\ndef calc_family_ticket_count_bin(family_ticket_count):\n    if family_ticket_count == 1:\n        return 0\n    elif family_ticket_count <= 2: \n        return 1\n    elif family_ticket_count <= 4:\n        return 2\n    else:\n        return 3\n        \ndata_all['family_ticket_count_bin'] = data_all.family_ticket_count.map(calc_family_ticket_count_bin)\n\ndata_all.name_title_cat = le.fit_transform(data_all[['name_title_cat']])\ndata_all.last_name_family_size = le.fit_transform(data_all[['last_name_family_size']])\ndata_all.last_name_ticket = le.fit_transform(data_all[['last_name_ticket']])\n\ndata_all['fare_fixed'] = data_all.Fare\/data_all.ticket_count\nfare_median = data_all.fare_fixed.median()\ndata_all['fare_fixed'] = data_all.fare_fixed.fillna(fare_median)\ndata_all['fare_fixed_log'] = np.log1p(data_all.fare_fixed)\n\nage_median_by_sex_title = data_all.groupby(['Sex', 'name_title'], as_index=False).Age.median()\n\ndata_all = pd.merge(data_all, age_median_by_sex_title, on=['Sex', 'name_title'])\ndata_all['Age'] = data_all.apply(lambda row: row.Age_x if not np.isnan(row.Age_x) else row.Age_y, axis=1)\ndata_all = data_all.drop(['Age_x','Age_y'], axis=1).sort_values('PassengerId').reset_index(drop=True)\n\ndef calc_age_bin(age):\n    if age <= 15:\n        return 0\n    elif age <= 30:\n        return 1\n    elif age <= 60:\n        return 2\n    else:\n        return 3\n        \ndata_all['age_bin'] = data_all.Age.map(calc_age_bin)\n\ndef parse_ticket_str(ticket):\n    arr = ticket.split()\n    if not arr[0].isdigit():\n        txt = arr[0].replace('.', '')\n        txt = txt.split('\/')[0]\n        return re.findall('[a-zA-Z]+', txt)[0]\n    else:\n        return None\n        \ndata_all['ticket_str'] = data_all.Ticket.map(parse_ticket_str)\n\ndef parse_ticket_number(ticket):\n    arr = ticket.split()\n    if len(arr) == 1 and arr[0].isdigit():\n        return int(arr[0])\n    elif len(arr) == 2 and arr[1].isdigit():\n        return int(arr[1])\n    else:\n        if arr[-1].isdigit():\n            return int(arr[-1])\n        else:\n            return np.nan\n    \ndata_all['ticket_number'] = data_all.Ticket.map(parse_ticket_number)\n\ndef parse_ticket_num_len(ticket):\n    arr = ticket.split()\n    if len(arr) == 1 and arr[0].isdigit():\n        return len(arr[0])\n    elif len(arr) == 2 and arr[1].isdigit():\n        return len(arr[1])\n    else:\n        if arr[-1].isdigit():\n            return len(arr[-1])\n        else:\n            return -1\n    \ndata_all['ticket_num_len'] = data_all.Ticket.map(parse_ticket_num_len)\n\ndata_all['ticket_num_len_4_prefix'] = data_all[data_all.ticket_num_len == 4].ticket_number.map(lambda x: int(str(x)[0]))\ndata_all['ticket_num_len_4_prefix_2'] = data_all[data_all.ticket_num_len == 4].ticket_number.map(lambda x: int(str(x)[:2]))\n\ndata_all['ticket_num_len_5_prefix'] = data_all[data_all.ticket_num_len == 5].ticket_number.map(lambda x: int(str(x)[0]))\ndata_all['ticket_num_len_5_prefix_2'] = data_all[data_all.ticket_num_len == 5].ticket_number.map(lambda x: int(str(x)[:2]))\n\ndata_all['ticket_num_len_6_prefix'] = data_all[data_all.ticket_num_len == 6].ticket_number.map(lambda x: int(str(x)[0]))\ndata_all['ticket_num_len_6_prefix_2'] = data_all[data_all.ticket_num_len == 6].ticket_number.map(lambda x: int(str(x)[:2]))\ndata_all['ticket_num_len_6_prefix_3'] = data_all[data_all.ticket_num_len == 6].ticket_number.map(lambda x: int(str(x)[:3]))\n\ndata_all.ticket_str = data_all.ticket_str.fillna('X')\n\ndata_all.ticket_num_len_4_prefix = data_all.ticket_num_len_4_prefix.fillna(-1)\ndata_all.ticket_num_len_4_prefix_2 = data_all.ticket_num_len_4_prefix_2.fillna(-1)\ndata_all.ticket_num_len_5_prefix = data_all.ticket_num_len_5_prefix.fillna(-1)\ndata_all.ticket_num_len_5_prefix_2 = data_all.ticket_num_len_5_prefix_2.fillna(-1)\ndata_all.ticket_num_len_6_prefix = data_all.ticket_num_len_6_prefix.fillna(-1)\ndata_all.ticket_num_len_6_prefix_2 = data_all.ticket_num_len_6_prefix_2.fillna(-1)\ndata_all.ticket_num_len_6_prefix_3 = data_all.ticket_num_len_6_prefix_3.fillna(-1)\n\ndata_all.ticket_num_len_4_prefix = data_all.ticket_num_len_4_prefix.astype(int)\ndata_all.ticket_num_len_4_prefix_2 = data_all.ticket_num_len_4_prefix_2.astype(int)\ndata_all.ticket_num_len_5_prefix = data_all.ticket_num_len_5_prefix.astype(int)\ndata_all.ticket_num_len_5_prefix_2 = data_all.ticket_num_len_5_prefix_2.astype(int)\ndata_all.ticket_num_len_6_prefix = data_all.ticket_num_len_6_prefix.astype(int)\ndata_all.ticket_num_len_6_prefix_2 = data_all.ticket_num_len_6_prefix_2.astype(int)\ndata_all.ticket_num_len_6_prefix_3 = data_all.ticket_num_len_6_prefix_3.astype(int)\n\ndata_all.ticket_str = le.fit_transform(data_all[['ticket_str']])\n\ndata_all['cabin_cat'] = data_all.Cabin.fillna('X').str[0]\n\ndef calc_cabin_len(cabin):\n    if type(cabin) == float:\n        return 0\n    else:\n        return len(cabin.split())\n\ndata_all['cabin_len'] = data_all.Cabin.map(calc_cabin_len)\n\ndata_all.cabin_cat = le.fit_transform(data_all[['cabin_cat']])","a747c65d":"features = [\n    'Pclass', 'Sex', 'Age', \n    'family_size_bin',\n    'name_title_cat', \n    'last_name_family_size',\n    'last_name_ticket',\n    'family_ticket_survival',\n    'fare_fixed_log',\n    'ticket_str',\n    'ticket_num_len_4_prefix_2',\n    'ticket_num_len_5_prefix_2',\n    'ticket_num_len_6_prefix_3',\n]\n\nX_train = data_all[data_all.train == 1][features]\nX_test = data_all[data_all.train == 0][features]\ny_train = data_all[data_all.train == 1].Survived.astype(int)\n\nX_train.shape, X_test.shape","d3779a1b":"def cv_model(train, train_labels, model, name, cv=10, scoring=make_scorer(precision_score)):\n    \"\"\"Perform k fold cross validation of a model\"\"\"\n    \n    cv_scores = cross_val_score(model, train, train_labels, cv=cv, scoring=scoring, n_jobs=-1)\n    print(f'{cv} Fold CV {scoring} for {name}: {round(cv_scores.mean(), 5)} with std: {round(cv_scores.std(), 5)}')\n    \ndef make_prediction(train, target, test, model, model_name):\n    cv_model(train, target, model, model_name)\n\n    model.fit(train, target)\n    pred = model.predict(test).astype(int)\n\n    output = f'{model_name}_submission_{datetime.now().strftime(\"%Y%m%d%H%M%S\")}.csv'\n    submit_df = pd.DataFrame()\n    submit_df['PassengerId'] = test_raw.PassengerId\n    submit_df['Survived'] = pred\n    \n    submit_df[['PassengerId','Survived']].to_csv(output, index=False)\n    print(f'submission file {output} is generated.')","93b86c48":"# Public, Private Both 0.82296.\n# n_estimators=1000, max_depth=2, using family_ticket_survival feature\nrf = RandomForestClassifier(n_estimators=1000, max_depth=2, n_jobs=-1, random_state=RANDOM_SEED)\nmake_prediction(X_train, y_train, X_test, rf, 'RF_FINAL')","d9b031b7":"def plot_feature_importances(estimator, x_cols, n=20, threshold = 0.95):\n    try:\n        df = pd.DataFrame({'feature': x_cols, 'importance': estimator.feature_importances_})\n    except AttributeError:\n        print('model does not provide feature importances')\n        return\n    \n    # Sort features with most important at the head\n    df = df.sort_values('importance', ascending = False).reset_index(drop = True)\n    \n    # Normalize the feature importances to add up to one and calculate cumulative importance\n    df['importance_normalized'] = df['importance'] \/ df['importance'].sum()\n    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n    \n    plt.rcParams['font.size'] = 12\n    \n    # Bar plot of n most important features\n    df.loc[:n, :].plot.barh(y = 'importance_normalized', \n                            x = 'feature', color = 'darkgreen', \n                            edgecolor = 'k', figsize = (12, 8),\n                            legend = False, linewidth = 2)\n\n    plt.xlabel('Normalized Importance', size = 18); plt.ylabel(''); \n    plt.title(f'{min(n, len(df))} Most Important Features', size = 18)\n    plt.gca().invert_yaxis()\n    \n    if threshold:\n        # Cumulative importance plot\n        plt.figure(figsize = (8, 6))\n        plt.plot(list(range(1, len(df)+1)), df['cumulative_importance'], 'b-')\n        plt.xlabel('Number of Features', size = 16); plt.ylabel('Cumulative Importance', size = 16); \n        plt.title('Cumulative Feature Importance', size = 18);\n        \n        # Number of features needed for threshold cumulative importance\n        # This is the index (will need to add 1 for the actual number)\n        importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n        \n        # Add vertical line to plot\n        plt.vlines(importance_index + 1, ymin = 0, ymax = 1.05, linestyles = '--', colors = 'red')\n        plt.show();\n        \n        print('{} features required for {:.0f}% of cumulative importance.'.format(importance_index + 1, \n                                                                                  100 * threshold))\n    \n    print(f'zero importance feature count : {len(df[df.importance == 0])}')\n    \n    return df","93dea48e":"rf.fit(X_train, y_train)\nplot_feature_importances(rf, features)","2a06ba04":"Public Leaderboard Score \uae30\uc900\uc73c\ub85c 0.84688\uc774 \ub098\uc628 \ubaa8\ub378\uc740 XGBoost, RF\ub97c Base Learner, LogisticRegression\uc744 Meta Learner\uc73c\ub85c \uc0ac\uc6a9\ud55c Stacking Ensemble \uc785\ub2c8\ub2e4.","1b50d3e7":"\ub370\uc774\ud130 \ub85c\ub529 \ubc0f Feature Engineering \uc791\uc5c5\uc744 \uc218\ud589\ud569\ub2c8\ub2e4.","7e1bc308":"\uc704\uc758 \ubaa8\ub378\ub85c \uacb0\uacfc\ub97c \uc81c\ucd9c\ud558\uba74 Public, Private Leaderboard Score \ubaa8\ub450 \ub3d9\uc77c\ud558\uac8c 0.82296\uc774 \ub098\uc635\ub2c8\ub2e4.","dd6a0c79":"Feature Selection\uc744 \ub2e4\uc2dc \ud574\uc11c \uc544\ub798 13\uac1c\uc758 feature\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.","65f4e2c6":"\uc704\uc758 \ubaa8\ub378\ub85c Public Leaderboard Score\uac00 0.84688\uc774 \ub098\uc654\uc9c0\ub9cc, \ub300\ud68c \uc885\ub8cc \ud6c4 \ub098\uc628 Private Leaderboard Score\ub294 0.73205\ub85c \uae09\ub77d\ud569\ub2c8\ub2e4.\n\n\uc774\ud6c4 \ubcf5\uae30 \uacfc\uc815\uc744 \ud1b5\ud574 Cross Validation Score\uac00 Leaderboard Score\uc640 \ub9e4\uce6d\uc774 \uc548\ub418\uc11c Public Leaderboard Score\uc5d0 \uc0ac\uc6a9\ub418\ub294 \uc808\ubc18\uc758 Test \ub370\uc774\ud130\uc5d0\ub9cc Overfitting \ud558\uace0 \uc788\uc5c8\ub2e4\ub294 \uac83\uc744 \uae68\ub2ec\uc558\uc2b5\ub2c8\ub2e4.\n\n\uadf8\ub798\uc11c Cross Validation Scoring\uc744 \uc774\uac83\uc800\uac83 \ubc14\uafd4\uac00\uba70 \ud14c\uc2a4\ud2b8 \ud574 \ubcf8 \uacb0\uacfc Precision \uae30\uc900\uc73c\ub85c Scoring\ud558\uba74 \uadf8\ub098\ub9c8 Leaderboard Score\uc640 \ub9e4\uce6d\uc774 \ub418\uc11c \uc774 \uae30\uc900\uc73c\ub85c Feature Selection\uc744 \ub2e4\uc2dc \ud558\uace0 \ucd5c\uc885 \ubaa8\ub378\ub3c4 Voting, Stacking Ensemble\uc774 \uc544\ub2cc RandomForest\ub85c \ub3cc\ub9ac\ub2c8 Public, Private Leaderboard Score \ubaa8\ub450 \ub3d9\uc77c\ud558\uac8c 0.82296\uc774 \ub098\uc624\uac8c \ub9cc\ub4e4 \uc218 \uc788\uc5c8\uc2b5\ub2c8\ub2e4.","6fd5db72":"* Ensemble\uc744 \ud558\ub294 \uac01 \ubaa8\ub378\uc758 Hyper Parameter Tuning\uc744 \uc124\uc815\ud558\uace0 \ubaa8\ub378\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4. XGBoost, RandomForest, KNN, LogisticRegression, SVM \ub2e4\uc12f \uac1c\uc758 \ubaa8\ub378\uc744 Voting, Stacking \uc5ec\ub7ec\uac00\uc9c0 \ubc29\uc2dd\uc73c\ub85c Ensemble \ud574\uc11c \ud14c\uc2a4\ud2b8\ud588\uc2b5\ub2c8\ub2e4.","dbb30cb7":"Random Forest \ub2e8\uc77c \ubaa8\ub378\uc5d0 Hyper Parameter\ub294 \uc544\ub798\uc640 \uac19\uc774 \uc124\uc815\ud569\ub2c8\ub2e4.\n* n_estimators : 1000\n* max_depth : 2","7a61959e":"\ucd5c\uc885 RandomForest \ubaa8\ub378\uc758 Feature Importance\ub97c \ud655\uc778\ud574\ubd05\ub2c8\ub2e4.","2efc8091":"[DataBreak 2018](http:\/\/kagglebreak.com\/databreak2018\/)\uc5d0\uc11c \uac1c\ucd5c\ud55c Titanic Kaggle \ubbf8\ub2c8 \ub300\ud68c\uc758 \ucd5c\uc885 Kernel \uc785\ub2c8\ub2e4.\n\n\uc774\uc804 \ucee4\ub110\ub4e4\uc740 \uc544\ub798 \ub9c1\ud06c\uc5d0 \uc788\uc2b5\ub2c8\ub2e4.\n* [1. Titanic EDA and Baseline Model](https:\/\/www.kaggle.com\/tmheo74\/1-titanic-eda-and-baseline-model)\n* [2. Titanic Feature Engineering](https:\/\/www.kaggle.com\/tmheo74\/2-titanic-feature-engineering-v2)","56b4a9e8":"Feature Engineering\uc744 \ud1b5\ud574 \ub9cc\ub4e0 feature \uc911\uc5d0 \uc5ec\ub7ec \ubc88\uc758 \ud14c\uc2a4\ud2b8\ub97c \ud1b5\ud574 Public Leaderboard Score\uac00 \uac00\uc7a5 \uc798 \ub098\uc624\ub294 \uc544\ub798\uc758 14\uac1c feature\ub9cc \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\nKNN, LR \ub4f1\uc758 \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud574\uc11c Ensemble\uc744 \ud558\uae30 \ub54c\ubb38\uc5d0 StandardScaler\ub97c \ud1b5\ud574 \ub370\uc774\ud130 \uc2a4\ucf00\uc77c\uc744 \uc870\uc815\ud569\ub2c8\ub2e4.","366835b1":"Cross Validation Scoring\uc744 Precision\uc73c\ub85c \ud558\ub3c4\ub85d \uc218\uc815\ud569\ub2c8\ub2e4."}}