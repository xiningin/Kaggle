{"cell_type":{"78c21184":"code","0e2bb433":"code","cf4be5bc":"code","388b232e":"code","a386d043":"code","2865081c":"code","ce742716":"code","66bbbf72":"code","3c6fab89":"code","08ec7771":"code","6a6828ac":"code","e85fc237":"code","674f94f6":"code","1bd3ad84":"code","cc9060a9":"code","4527a455":"code","d0a3ab31":"code","d3df35a7":"code","d62abf29":"code","aa11ffeb":"code","c0635ff0":"code","4d4f935c":"code","16d0ade3":"code","ce3c83eb":"code","e87fb559":"code","cfaf67ea":"code","d8b617f5":"code","a43562a4":"code","ec0dcf35":"code","de97d361":"code","8c8da1ec":"code","56ddfdc2":"code","ed52457a":"code","1da07c48":"code","edea9f05":"code","10623617":"code","67165807":"code","d85bc795":"code","5c502afb":"code","daa8b732":"code","fbeccc54":"code","39d429ac":"code","55ab4359":"code","05e3d210":"code","7191b442":"code","067083b6":"code","a32b602f":"code","c1589d4e":"code","fc7d9d7a":"code","322b3a40":"code","037ec3f0":"code","3d7a85ed":"code","f560d630":"code","9d8782a9":"code","68322580":"code","e188af62":"code","4d80eb04":"markdown","90f1411d":"markdown","c862fae6":"markdown","239c83fd":"markdown","50a78246":"markdown","bb587506":"markdown","abc441dd":"markdown","a6e8f4c4":"markdown","836ace82":"markdown","7e4131bc":"markdown","20bbd01b":"markdown","41de9ecb":"markdown","4ab8d044":"markdown","ab21bd77":"markdown"},"source":{"78c21184":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport scipy as sp\nimport string\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n","0e2bb433":"#reading data\ndata=pd.read_csv(\"..\/input\/twitter-and-reddit-sentimental-analysis-dataset\/Reddit_Data.csv\",nrows=1000)\n\n","cf4be5bc":"data","388b232e":"data.head()","a386d043":"data.value_counts()","2865081c":"data.describe()","ce742716":"data.columns","66bbbf72":"data.describe().transpose()","3c6fab89":"data.isnull().sum()","08ec7771":"data.dropna(axis = 0, inplace = True)","6a6828ac":"#lets find the categorialfeatures\nlist_1=list(data.columns)\n","e85fc237":"list_cate=[]\nfor i in list_1:\n    if data[i].dtype=='object':\n        list_cate.append(i)","674f94f6":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\n","1bd3ad84":"for i in list_cate:\n    data[i]=le.fit_transform(data[i])","cc9060a9":"data","4527a455":"data.hist(figsize=(18,12))\nplt.show()\n","d0a3ab31":"data.corr()","d3df35a7":"plt.figure(figsize = (12,10))\n\nsns.heatmap(data.corr(), annot =True)\n","d62abf29":"data.shape","aa11ffeb":"sns.pairplot(data=data)\n","c0635ff0":"import nltk\nimport scikitplot as skplt\nimport re\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nSTOPWORDS = stopwords.words('english')\n","4d4f935c":"def clean_text(text):\n    text = text.lower()\n    text = re.sub(r'[^0-9a-zA-Z]', ' ', text)\n    text = re.sub(r'\\s+', ' ', text)\n    text = \" \".join(word for word in text.split() if word not in STOPWORDS)\n    return text\n","16d0ade3":"#lets find the categorialfeatures\nlist_1=list(data.columns)\n","ce3c83eb":"list_cate=[]\nfor i in list_1:\n    if data[i].dtype=='object':\n        list_cate.append(i)\n","e87fb559":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\n","cfaf67ea":"for i in list_cate:\n    data[i]=le.fit_transform(data[i])\n","d8b617f5":"data","a43562a4":"y=data['category']\nx=data.drop('category',axis=1)\n\n","ec0dcf35":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,random_state=0,test_size=0.2)\n","de97d361":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\n","8c8da1ec":"print(len(x_train))\nprint(len(x_test))\nprint(len(y_train))\nprint(len(y_test))\n","56ddfdc2":"# Importing the Keras libraries and packages\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\"))\n\n# Adding the second hidden layer\nclassifier.add(Dense(activation=\"relu\", input_dim=11, units=6, kernel_initializer=\"uniform\"))\n\n# Adding the output layer\nclassifier.add(Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\"))\n\n# Compiling the ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fitting the ANN to the Training set\nclassifier.fit(x_train,y_train,batch_size = 10,\n    epochs=200,\n)\n","ed52457a":"classifier.evaluate(x_test,y_test)","1da07c48":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=7)\n\nknn.fit(x_train,y_train)\n","edea9f05":"\ny_pred=knn.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nprint(\"Classification Report is:\\n\",classification_report(y_test,y_pred))\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\nprint(\"Training Score:\\n\",knn.score(x_train,y_train)*100)\n","10623617":"knn.score(x_test,y_test)*100","67165807":"from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(x_train,y_train)\n","d85bc795":"y_pred=gnb.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nprint(\"Classification Report is:\\n\",classification_report(y_test,y_pred))\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\nprint(\"Training Score:\\n\",gnb.score(x_train,y_train)*100)\n","5c502afb":"gnb.score(x_test,y_test)*100","daa8b732":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier(max_depth=6, random_state=123,criterion='entropy')\n\ndtree.fit(x_train,y_train)\n","fbeccc54":"y_pred=dtree.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nprint(\"Classification Report is:\\n\",classification_report(y_test,y_pred))\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\nprint(\"Training Score:\\n\",dtree.score(x_train,y_train)*100)","39d429ac":"dtree.score(x_test,y_test)*100","55ab4359":"from sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier()\nrfc.fit(x_train,y_train)","05e3d210":"y_pred=rfc.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nprint(\"Classification Report is:\\n\",classification_report(y_test,y_pred))\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\nprint(\"Training Score:\\n\",rfc.score(x_train,y_train)*100)\n","7191b442":"from sklearn.linear_model import LogisticRegression\nreg = LogisticRegression()\nreg.fit(x_train,y_train)                         ","067083b6":"y_pred=reg.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nprint(\"Training Score:\\n\",reg.score(x_train,y_train)*100)\nprint(f\"r2 Score of test set : {r2_score(y_test, y_pred)}\")\n","a32b602f":"reg.score(x_test,y_test)*100","c1589d4e":"from sklearn.linear_model import LassoLars,LinearRegression,LogisticRegression,Ridge,Lasso\n","fc7d9d7a":"lin_reg = LinearRegression()\nlin_reg.fit(x_train,y_train)\n","322b3a40":"y_pred = lin_reg.predict(x_test)\n \nprint(f\"r2 Score of test set : {r2_score(y_test, y_pred)}\")\n","037ec3f0":"lin_reg.intercept_\n","3d7a85ed":"lin_reg.coef_\n","f560d630":"from sklearn.metrics import mean_squared_error,mean_absolute_error\nmean_squared_error(y_test,y_pred)\n","9d8782a9":"from sklearn.ensemble import AdaBoostClassifier\nadb = AdaBoostClassifier(base_estimator = None)\nadb.fit(x_train,y_train)\n","68322580":"y_pred=adb.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nprint(\"Classification Report is:\\n\",classification_report(y_test,y_pred))\nprint(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\nprint(\"Training Score:\\n\",adb.score(x_train,y_train)*100)\n","e188af62":"data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata\n","4d80eb04":"# TRAINING AND TESTING DATA","90f1411d":"**MODELS**\n\n**1. KNeighborsClassifier**\n\n","c862fae6":"# Feature Scaling","239c83fd":"**6. AdaBoostClassifier**","50a78246":"**CONCLUSION :**\n\n**ACCURACIES OF DIFFERENT MODELS ARE:**\n\n**KNeighbors Classifier= 60.02 %**\n\n**Naiye Bayes= 49.12 %**\n\n**Decision Tree Classifier= 56.8 %**\n\n**Random Forest Classifier= 99.87 %**\n\n**Logistic Regression : 62.27 %**\n\n**Ada Boost Classifier= 53.25 %**\n\n**We got a good accuracy of about 99.87 % using Random Forest Classifier which is quite well for the given dataset.**\n\n**The accuracy of other models can be increased further by HyperTuning.**\n\n\n\n","bb587506":"# LOADING THE DATASET","abc441dd":"# ANN","a6e8f4c4":"**4.Random Forest Classifier**\n\n","836ace82":"**2. Naive Bayes**\n\n","7e4131bc":"# REDDIT TWEETS ANALYSIS","20bbd01b":"**3. DECISION TREE CLASSIFIER**\n\n","41de9ecb":"**5. Logistic Regression**","4ab8d044":"# IMPORTING THE LIBRARIES","ab21bd77":"# NLTK"}}