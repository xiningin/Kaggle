{"cell_type":{"bc88dd23":"code","ba091857":"code","0ee0974f":"code","565f0694":"code","e2a62dc1":"code","c9707b5b":"code","16835548":"code","328f0421":"code","1519d156":"code","80a2cd74":"code","a9fe9821":"code","98c1c776":"code","0a232fe5":"code","277798d2":"code","1f7d8cbf":"code","9c0dff8a":"code","7f94f7b7":"code","6100b52d":"code","210dca8e":"code","d1a41308":"code","c80104f6":"code","4c247d8a":"code","50498bd6":"code","b7ec8024":"code","69c7a7ed":"code","ce398753":"code","b919cfe0":"code","f3f74712":"code","c447f184":"code","f3a3fb1e":"code","be0f2448":"code","21e245a8":"code","042f6e6a":"code","e817bd24":"code","a2f3f8be":"code","a755f23e":"code","1d1b8683":"code","8b499d14":"code","1bde37e2":"code","5a208fd8":"code","962f6d75":"code","16588986":"code","a8984b04":"code","85ce5e2d":"code","2bb90ca8":"code","8c33a55c":"code","54f7f325":"code","e86052d2":"code","673b6112":"code","03f1165c":"code","ac7e0801":"code","ea42e7ee":"code","01251dab":"code","51bb07a1":"code","ebd1f0b4":"code","cd485996":"code","e63ee23e":"code","3d422a23":"code","2124e38d":"code","4ac5d527":"code","482dd377":"code","439d81d0":"code","5d4ec018":"code","3c16c339":"code","dbc7bffa":"code","840df8f7":"code","ab575247":"code","20dcde61":"code","822f8202":"code","bd31aa75":"code","e5a49323":"code","089cee9c":"code","e0e4e3df":"code","011b2974":"code","4f7f6a01":"code","7bc228df":"code","13d5b002":"code","9e6be4f7":"code","ed47dfa3":"code","d7251724":"code","379c73ba":"markdown","e15846c7":"markdown","ea154f2b":"markdown","b1b1f1f1":"markdown","9c37479f":"markdown","2e5568ec":"markdown","56ebfac0":"markdown","90882485":"markdown","892d74c7":"markdown","954bce23":"markdown","f464dfd7":"markdown","462cf43c":"markdown","0e1d27bf":"markdown","f20012f0":"markdown","e1a1721b":"markdown","d29ff32b":"markdown","eb70ee1b":"markdown","8af3a308":"markdown","740b65f7":"markdown","081b42e2":"markdown","0d932aee":"markdown","6dea894b":"markdown","972fbfc3":"markdown","9708757d":"markdown","fc952a2c":"markdown","841e63b9":"markdown","90cdee0b":"markdown","7fca09d6":"markdown","e97536b1":"markdown","faf68ea7":"markdown","dbc99eb6":"markdown","54f9e7a5":"markdown","a2007a22":"markdown","275635a3":"markdown","82534e8d":"markdown","d9a31673":"markdown","eb0d322d":"markdown","9a111e07":"markdown","443cdea2":"markdown","67bb0700":"markdown","6a6e4c0c":"markdown","2efdbf48":"markdown","48a3d1e4":"markdown","ab626da6":"markdown","469ee90a":"markdown","d82cfbcb":"markdown","500dcbbd":"markdown","1f274ed1":"markdown","3f016353":"markdown","0a26f40b":"markdown","87270e49":"markdown","9c493264":"markdown","54da870a":"markdown","e3d5a953":"markdown","aea3c041":"markdown","f9dde3f5":"markdown","ba42f4d6":"markdown","f38e1c96":"markdown","5ad72035":"markdown","b8030fa8":"markdown","699f37f2":"markdown","86f1bde9":"markdown","febf1750":"markdown","1db279bf":"markdown","5ae71356":"markdown","3a702fa1":"markdown","44357f6d":"markdown","fe51ab2b":"markdown","44eaa5b2":"markdown"},"source":{"bc88dd23":"#Import libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import accuracy_score, f1_score\n\nimport category_encoders as ce\n\nimport joblib\n\n%matplotlib inline\n\n","ba091857":"#Read in data (CSV)\n\ndf = pd.read_csv('https:\/\/github.com\/Kesterchia\/Predicting-Kickstarter-project-success\/blob\/main\/Data\/ks-projects-201801.csv?raw=True', index_col = 'ID')\n\n#Note: Remember to read in ID as the index column!\n\n#Check the size of the dataset:\n\nprint('No. of rows:', df.shape[0])\nprint('No. of columns:',df.shape[1])","0ee0974f":"#Check first few rows of data:\n\ndf.head(3)","565f0694":"#Check the unique values of the target variable\n\ndf['state'].value_counts()","e2a62dc1":"#Only keep projects that are either failed or successful\n\ndf = df[df['state'].isin(['failed','successful'])]\n\n#Checking target variable counts again\n\ndf['state'].value_counts()","c9707b5b":"#Check null values\ndf.isnull().sum()","16835548":"df = df.dropna()\n\n#Check null values again:\n\ndf.isnull().sum()","328f0421":"#Show the failure rate of projects\n\nplt.pie(x = df['state'].value_counts(),\n        labels = df['state'].value_counts().index,\n        autopct='%1.1f%%')\n\nplt.show()","1519d156":"#Convert column values to 1's and 0's by creating dummy columns:\n\ndf = pd.get_dummies(data = df,\n                    columns = ['state'])\n\n#We only have to keep one column:\n\ndf = df.drop('state_failed', axis = 1)\n","80a2cd74":"#Check the typical categories for projects\n\nplt.figure(figsize = (18,5))\n\nsns.barplot(x =  df['main_category'].value_counts().index,\n            y =  df['main_category'].value_counts())","a9fe9821":"sns.barplot(x = df['currency'].value_counts().index,\n            y = df['currency'].value_counts())","98c1c776":"df['currency'].value_counts()","0a232fe5":"df[['deadline','launched']].info()","277798d2":"#Converting datatypes:\n\ndf = df.astype({'deadline':'datetime64',\n                'launched':'datetime64'\n               }\n              )\n\n#See if conversion is successful:\n\ndf[['deadline','launched']].info()","1f7d8cbf":"#Create a new column showing time between project launch to deadline\n\ntime_to_deadline = df['deadline'] - df['launched']\ntime_to_deadline[:5]","9c0dff8a":"#Only keep the number of days to make it simpler, also as int dtype:\n\ntime_to_deadline_days = time_to_deadline.dt.days\n\ndf['Days to deadline'] = time_to_deadline_days\n\ndf['Days to deadline'][:5]","7f94f7b7":"#Note: Time delta information is stored in pandas in nanoseconds (10^-9).","6100b52d":"#Example:\n\nprint(time_to_deadline[1:2])\nprint('\\t')\nprint('Value:', time_to_deadline[1:2].values)\nprint('\\t')\nprint('Value converted to days:',time_to_deadline[1:2].values \/1000 \/1000 \/1000 #Convert to seconds \n                                                                          \/60 #to Minutes\n                                                                          \/60 #to Hours\n                                                                          \/24)#to Days","210dca8e":"fig, ax = plt.subplots(nrows = 2, ncols = 1, figsize = (13,10))\n\n\nsns.distplot(df['Days to deadline'],\n             ax = ax[0])\n\nsns.boxplot(df['Days to deadline'],\n            ax = ax[1])\n\nax[0].xaxis.set_visible(False)\nax[0].set_title('Distribution of time between launch to project deadline')\nax[0].set_ylabel('Relative frequency')\n\nplt.show()","d1a41308":"#Definitely high variation in time to deadline, with most projects falling between 20 to 40 days","c80104f6":"df['backers'].describe()","4c247d8a":"fig, ax = plt.subplots(nrows = 2, ncols = 1, figsize = (13,10))\n\nsns.distplot(df['backers'],\n             ax = ax[0])\n\nsns.boxplot(df['backers'],\n            ax = ax[1])\n\nax[0].xaxis.set_visible(False)\nax[0].set_title('Distribution of time between launch to project deadline')\nax[0].set_ylabel('Relative frequency')\nplt.show()","50498bd6":"#Removing top 2 percentiles of data:\n\ndf = df[df['backers'] < np.quantile(df['backers'],0.98)]","b7ec8024":"#Check the new values:\n\ndf['backers'].describe()\n\n#Maximum value is now 908 instead of 219,000 ","69c7a7ed":"fig, ax = plt.subplots(nrows = 2, ncols = 1, figsize = (13,10))\n\n\nsns.distplot(df['backers'],\n             ax = ax[0])\n\nsns.boxplot(df['backers'],\n            ax = ax[1])\n\nax[0].xaxis.set_visible(False)\nax[0].set_title('Distribution of time between launch to project deadline')\nax[0].set_ylabel('Relative frequency')\n\nplt.show()","ce398753":"# Distribution still looks very skewed, but at least it's not as extreme as before","b919cfe0":"df['usd_goal_real'].describe()","f3f74712":"fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (15,5))\n\nsns.distplot(df['usd_goal_real'], \n             ax = ax)","c447f184":"#Remove both top and bottom 1 percentile: \n\ndf = df[\n         (df['usd_goal_real'] > np.quantile(df['usd_goal_real'],0.01)) & \n         (df['usd_goal_real'] < np.quantile(df['usd_goal_real'],0.99))\n                        ]","f3a3fb1e":"fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (15,5))\n\nsns.distplot(df['usd_goal_real'], \n             ax = ax)","be0f2448":"fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (15,5))\n\nsns.distplot(df['usd_pledged_real'], \n             ax = ax)","21e245a8":"#Remove bottom and top 1 percentiles:\n\ndf = df[\n         (df['usd_pledged_real'] > np.quantile(df['usd_pledged_real'],0.01)) & \n         (df['usd_pledged_real'] < np.quantile(df['usd_pledged_real'],0.99))\n                        ]","042f6e6a":"#Checking the distribution again\n\nfig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (15,5))\n\nsns.distplot(df['usd_pledged_real'], \n             ax = ax)","e817bd24":"fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (15,5))\n\nsns.countplot(df['country'], \n             ax = ax)\n\n","a2f3f8be":"df['country'].value_counts()","a755f23e":"#It seems like most of the projects come from the US\n#There are a decent number of projects coming from other countries except JP and LU, so I will drop those countries\n\ndf = df[~df['country'].isin(['LU','JP'])]","1d1b8683":"from sklearn.feature_selection import f_classif\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom scipy.stats import f_oneway","8b499d14":"numericals = df.select_dtypes(include = ['float64', 'int64'])\ncategoricals = df.select_dtypes(include = ['object'])","1bde37e2":"g = sns.pairplot(numericals)\n\nfor i, j in zip(*np.triu_indices_from(g.axes, k=1)):\n    g.axes[i, j].set_visible(False)","5a208fd8":"sns.heatmap(numericals.corr())\n\nmask = np.zeros_like(numericals.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True","962f6d75":"pd.DataFrame(data = f_classif(numericals, df['state_successful']),\n             columns = numericals.columns,\n             index = ['F-statistic','p-value'])","16588986":"from scipy.stats import chi2_contingency\n\nchi2_values = []\np_values = []\n\nfor column in categoricals.columns[:-1]:\n    contingency = pd.crosstab(df[column], df['state_successful'])\n\n    chi2_val, p_value, degrees_of_freedom, expected = chi2_contingency(contingency)\n    \n    chi2_values.append(chi2_val)\n    p_values.append(p_value)\n    \ndata = list(zip(categoricals.columns, chi2_values, p_values))\npd.DataFrame(data = data,\n             columns = ['Variable','Chi squared value','p-value'])","a8984b04":"#Reset indexes to prevent some future issues with concatenation:\n\nX = df[['main_category','Days to deadline','usd_goal_real','backers','country']].reset_index(drop = True)\ny = df['state_successful'].reset_index(drop = True)","85ce5e2d":"#We use a robust scaler because most of our variables are pretty skewed\n\n#Robust scalers use median and IQR instead of the standard mean and S.D.\n\nfrom sklearn.preprocessing import RobustScaler \n\nscaler = RobustScaler()\n\nX_numerical = X.drop(['main_category','country'], axis = 1)\n\nscaler.fit(X_numerical)\n\nscaled_X = scaler.transform(X_numerical)\n\nscaled_X = pd.DataFrame(scaled_X, columns = X_numerical.columns)","2bb90ca8":"from sklearn.decomposition import PCA\n\n#We put n_components as default None so we don't remove any components yet\n\npca_components = PCA(n_components = None).fit(scaled_X)\n\n#We can see how much variance each Principal Component explains:\nfor i, variance in enumerate(pca_components.explained_variance_ratio_):\n    print('Variance explained by PC{}: {}'.format(i+1, variance))","8c33a55c":"#After data scaling, I can concatenate my categorical and numerical variables:\n\nX = pd.concat([scaled_X, X[['main_category','country']]],\n               join = 'outer', \n               axis = 1\n             )\n\nX.head()","54f7f325":"#Double check the shape:\n\nX.shape","e86052d2":"%%time\n\n#Attempting to obtain optimal no. of clusters k:\n\nfrom sklearn.cluster import KMeans\n\ninertias = []\n\nfor k in range(3,10):\n    kmeans = KMeans(n_clusters = k)\n    kmeans.fit(scaled_X)\n    inertias.append(kmeans.inertia_)\n    ","673b6112":"#Plot an inertia vs k graph to visualize the 'elbow':\n\nplt.figure(figsize = (15,5))\n\nsns.lineplot(x = range(3,10),\n             y = inertias)\n\nplt.title('Total variance (inertia) against k')\nplt.show()","03f1165c":"#Creating 4 clusters in the dataset:\n\nkmeans = KMeans(n_clusters = 4)\n\nkmeans_predictions = kmeans.fit_predict(scaled_X)\n\n#Add cluster to features table\n\nX['Cluster (k=4)'] = kmeans_predictions","ac7e0801":"#Check that clusters are added:\n\nX.head()","ea42e7ee":"#Using get_dummies would use One-hot encoding, but with high cardinality the data would end up with a high dimension\n\nprint('For One-hot encoding:')\nprint('\\t')\nprint('No. of rows:', pd.get_dummies(X).shape[0])\nprint('No. of columns:', pd.get_dummies(X).shape[1])\n\npd.get_dummies(X).head(1)","01251dab":"#Using a binary encoder instead might help reduce the dimensionality\n#This involves converting to numerical labels, then converting the numbers into binary form\n\nimport category_encoders as ce\n\nencoder = ce.BinaryEncoder(cols = ['main_category','country','Cluster (k=4)'])\n\nprint('For binary encoding:')\nprint('\\t')\nprint('No. of rows:', encoder.fit_transform(X).shape[0])\nprint('No. of columns:', encoder.fit_transform(X).shape[1])\n\nX = encoder.fit_transform(X)\n\nX.head(1)","51bb07a1":"# Splitting the data:\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y, \n                                                    test_size=0.33, \n                                                    random_state=42)","ebd1f0b4":"#Establish a baseline dummy model to have a benchmark for model accuracy:\n\nfrom sklearn.dummy import DummyClassifier\n\nbaseline = DummyClassifier(strategy = 'stratified')\n\nbaseline.fit(X_train,y_train)\n\nbaseline_predictions = baseline.predict(X_test)\n\nprint('Baseline model accuracy:',accuracy_score(y_test,baseline.predict(X_test)))","cd485996":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\n''''\n%%time\n%%capture\n\n#Measure how long the model takes to fit\n\n#Try a Logistic Regression model:\n\n\nlogmodel = GridSearchCV(estimator = LogisticRegression(),\n                        param_grid = {'solver':['sag','lbfgs'],\n                                      'C': [0.01,0.1,0.5,1]# Sag is gradient descent\n                                     },                    # Lbfgs calculates second derivatives without actually calculating it\n                        cv = 5 \n                       )\nlogmodel.fit(X_train,y_train.astype(int))\n\n#joblib.dump(logmodel, '.\/Models\/log_model.joblib') #Saving the model for future use:\n'''","e63ee23e":"#Load previously trained model:\n\nimport urllib\n\nlogmodel = joblib.load(urllib.request.urlopen('https:\/\/github.com\/Kesterchia\/Predicting-Kickstarter-project-success\/blob\/main\/Models\/log_model.joblib?raw=True'))\n#Generate predictions:\n\nlogmodel_predictions = logmodel.predict(X_test)\n","3d422a23":"print('Best parameters for Logistic Regression:', logmodel.best_params_)","2124e38d":"%%time \n\nfrom sklearn.svm import SVC\nimport joblib\n\n# svc = SVC().fit(X_train,y_train) #Can't use GridSearchCV, it takes a billion years\n\n\n#Save model:\n\n# joblib.dump(svc, 'svc_model.joblib')\n\n\n#Load model and generate predictions:\n\nsvc = joblib.load(urllib.request.urlopen('https:\/\/github.com\/Kesterchia\/Predicting-Kickstarter-project-success\/blob\/main\/Models\/svc_model.joblib?raw=True'))\n\nsvc_predictions = svc.predict(X_test)","4ac5d527":"%%time\n%%capture\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ntree = GridSearchCV(estimator = DecisionTreeClassifier(),\n                    param_grid = {'criterion':[\"gini\", \"entropy\"],\n                                 },\n                    cv = 5 #Default 5-fold CV\n                   )\ntree.fit(X_train,y_train)\n\n#Generate predictions:\n\ntree_predictions = tree.predict(X_test)","482dd377":"print('Criterion choice for decision tree:',tree.best_params_)","439d81d0":"%%time\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# forest = GridSearchCV(RandomForestClassifier(),\n#                    param_grid = {'n_estimators' : [100,200,300],\n#                                  'max_features' : ['auto'] #Default\n#                                 }                                               \n#                     )\n# forest.fit(X_train,y_train)\n\n# joblib.dump(forest, 'forest_model.joblib')\n\n# forest = joblib.load('.\/Models\/forest_model.joblib')\n                     \n#Generate predictions\n\n# forest_predictions = forest.predict(X_test)","5d4ec018":"# forest.best_params_","3c16c339":"from sklearn.ensemble import AdaBoostClassifier\n\n# adaboost_cv = GridSearchCV(estimator = AdaBoostClassifier(),\n                               #param_grid = {'n_estimators' : [50,100,200],\n                                             #'learning_rate' : [0.2,0.5,0.8,1.0]})\n\n# adaboost_cv.fit(X_train,y_train)\n\n# Save Adaboost classifier object:\n\n# joblib.dump(adaboost_cv, 'adaboost_model.joblib')\n\n\n#Load adaboost model:\n\nadaboost_cv = joblib.load(urllib.request.urlopen('https:\/\/github.com\/Kesterchia\/Predicting-Kickstarter-project-success\/blob\/main\/Models\/adaboost_model.joblib?raw=True'))\n\n#Generate predictions:\nadaboost_cv_predictions = adaboost_cv.predict(X_test)","dbc7bffa":"print('Best parameters for Adaboost Classifier:', adaboost_cv.best_params_)","840df8f7":"%%time\n\n#Calculate a few metrics for the models and put them in a table:\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, plot_confusion_matrix, plot_roc_curve,classification_report\n\nmodel_names = ['Logistic Regression', 'Support vector machines', 'Decision tree', 'Adaboost', 'Baseline (Stratified)']\n\nmodels = [logmodel, svc, tree, adaboost_cv, baseline]\n\npredictions = [logmodel_predictions, svc_predictions, tree_predictions, adaboost_cv_predictions, baseline_predictions]\n\ntraining_accuracy_scores = [model.score(X_train,y_train) for model in models]\n\ntest_accuracy_scores = [accuracy_score(y_test,predictions[i]) for i,model in enumerate(models)]\n\nprecisions = [precision_score(y_test,predictions[i]) for i,model in enumerate(models)]\n\nrecalls = [recall_score(y_test, predictions[i]) for i,model in enumerate(models)]\n\nf1_scores = [f1_score(y_test,predictions[i]) for i,model in enumerate(models)]\n\ncross_validated = ['Yes','No','Yes','Yes','Yes','No']","ab575247":"datapoints = zip(model_names, training_accuracy_scores, test_accuracy_scores, precisions, recalls, f1_scores, cross_validated)\n\npd.DataFrame(data = datapoints,\n             columns = ['Model','Training accuracy', 'Test accuracy', 'Precision', 'Recall','F1_score', 'Cross validated?'])","20dcde61":"fig, axes = plt.subplots(nrows = 2, ncols = 3, figsize = (15,10))\n\nfor model, axis, modelname in zip(models,axes.flatten(), model_names):\n    plot_confusion_matrix(estimator = model,\n                          X = X_test,\n                          y_true = y_test,\n                          cmap = 'Reds',\n                          ax = axis,)\n    axis.set_title(modelname)\n    axis.grid(False)","822f8202":"fig, axes = plt.subplots(nrows = 2, ncols = 3, figsize = (15,10))\n\nfor model, axis, modelname in zip(models,axes.flatten(), model_names):\n    plot_roc_curve(estimator = model,\n                          X = X_test,\n                          y = y_test,\n                          ax = axis,)\n    axis.set_title(modelname)","bd31aa75":"#Predict_proba function returns the probability of 0 and 1 as outcomes\n\nsample = X.sample(1)\n\n#To access probability that the project is successful:\n\nprint('Project probability of success:', adaboost_cv.predict_proba(sample)[0][1])","e5a49323":"#First create a function that returns the success probability (using logistic regression)\n\ndef predict_project_success(data, model):\n\n    #Scale data\n\n    X_numerical = data[['Days to deadline','usd_goal_real','backers']]\n    scaled_X = pd.DataFrame(scaler.transform(X_numerical), columns = X_numerical.columns)\n\n    scaled_X\n\n    #Cluster data\n\n    clusters = pd.DataFrame(kmeans.predict(scaled_X), columns = ['Cluster (k=4)'])\n\n    # Concatenate categorical and numerical variables\n\n    X = pd.concat([scaled_X, \n                   clusters, \n                   data[['main_category','country']].reset_index(drop = True)\n                  ],\n                   axis = 1,\n                    )\n\n    #Encode X\n\n    encoded_X = encoder.transform(X)\n\n    encoded_X\n\n    #Generate prediction probability\n\n    return (model.predict_proba(encoded_X)[0][1]) #Probability estimate that project is successful","089cee9c":"#Take a sample of the data:\n\nsample = df.sample(1)\nsample","e0e4e3df":"#Predicted success probability of sample:\n\nprint('Predicted success probability:',predict_project_success(sample,adaboost_cv))","011b2974":"#Define function to generate a plot of expected data\n\ndef plot_success_graph(model,days, goal, category, country, max_backers):\n    \n    \n    '''\n    This function plots a graph of an estimate of the project's success given an arbitrary amount of backers.\n    \n    Arguments:\n    \n    days: Total number of days from the project's launch to the deadline of completion.\n    \n    goal: The monetary goal of the project in USD.\n    \n    category: The category that the project falls under. Must be in: ['Publishing', 'Film & Video', 'Music', 'Food', 'Crafts',\n    'Games','Design', 'Comics', 'Fashion', 'Theater', 'Art', 'Photography','Technology', 'Dance', 'Journalism']\n    \n    country: Country of the project's origin. Must be in: ['GB', 'US', 'CA', 'AU', 'IT', 'DE', 'IE', 'MX', 'ES', 'SE', 'FR',\n       'NZ', 'CH', 'AT', 'NO', 'BE', 'DK', 'HK', 'NL', 'SG']\n       \n    max_backers: Maximum backers limit for model to estimate till (Model starts with 0, step size = 20).\n    \n    '''\n    \n    \n    data = pd.DataFrame(columns = ['Days to deadline','usd_goal_real','main_category','country'],\n                        data = [[days,goal,category,country]])\n    probabilities = []\n    for i in np.arange(0,max_backers,5):\n        data.loc[:,'backers'] = i\n        probability_success = predict_project_success(data,model)\n        probabilities.append(probability_success)\n    \n    plt.figure(figsize = (20,5))\n    \n    sns.set_style('darkgrid')\n    \n    sns.lineplot(y = probabilities,\n                 x = np.arange(0,max_backers,5))\n    \n    plt.xlabel('No. of backers', fontdict = {'fontsize':15})\n    plt.xticks(np.arange(0,max_backers,step = 25))\n    \n    plt.ylabel('Probability of success', fontdict = {'fontsize':15})\n    \n    plt.title('Probability of project success by backers', fontdict = {'fontsize':15})\n    \n    plt.show()\n","4f7f6a01":"#Example of the function on some test data:\n\n#Using the logistic regression model:\n\nplot_success_graph(model = logmodel,\n                   days = 50, \n                   goal = 10000, \n                   category = 'Games', \n                   country = 'NZ', \n                   max_backers = 700)","7bc228df":"#Using the AdaBoost model:\n\nplot_success_graph(model = adaboost_cv,\n                   days = 50, \n                   goal = 10000, \n                   category = 'Games', \n                   country = 'NZ', \n                   max_backers = 700)\n\n#Probabilities are alot more constrained below 0.5, is it because Adaboost predict_probas cannot be interpreted as probabilities?","13d5b002":"#Using the decision tree model:\n\nplot_success_graph(model = tree,\n                   days = 50, \n                   goal = 10000, \n                   category = 'Games', \n                   country = 'NZ', \n                   max_backers = 700)\n\n#Even worse! The probabilities make no sense","9e6be4f7":"from sklearn.calibration import CalibratedClassifierCV\n\n\n#Create model to calculate calibrated probabilities:\n\n#calibrated_adaboost = CalibratedClassifierCV(base_estimator = adaboost_cv,\n#                                             cv = None, #Default is 5-fold\n#                                             method = 'isotonic', #Isotonic regression\n#                                            )\n\n#Fit calibrated model:\n\n# calibrated_adaboost.fit(X_train,y_train) (Took 1 hour 16 min)","ed47dfa3":"#Save calibrated model locally:\n\n#joblib.dump(calibrated_adaboost, 'calibrated_adaboost_model.joblib')\n\n\n#Load calibrated model:\n\ncalibrated_adaboost_cv = joblib.load(urllib.request.urlopen('https:\/\/github.com\/Kesterchia\/Predicting-Kickstarter-project-success\/blob\/main\/Models\/calibrated_adaboost_model.joblib?raw=True'))","d7251724":"#Using the calibrated AdaBoost model:\n\nplot_success_graph(model = calibrated_adaboost_cv,\n                   days = 50, \n                   goal = 10000, \n                   category = 'Games', \n                   country = 'NZ', \n                   max_backers = 700)\n\n#Probabilities are alot more constrained below 0.5, is it because Adaboost predict_probas cannot be interpreted as probabilities?","379c73ba":"### Project goals (measured in real USD value):","e15846c7":"### Target variable (State):","ea154f2b":"Now that the target and explanatory variables are encoded, we can implement our models to predict project success. We first begin by splitting the data into training and test sets.","b1b1f1f1":"### Cleaning the target variable:","9c37479f":"### Observations:\n","2e5568ec":"### Decision Tree","56ebfac0":"The calibrated model seems to produce a much more believable graph of probabilities!","90882485":"ANOVA (Analysis of Variance) tests try to discern if there is a relationship between a categorical variable and a numerical variable.","892d74c7":"### Plotting confusion matrices for each model","954bce23":"# Step 3: Implementing models","f464dfd7":"### Trying to fix the issue by calibrating the predict_proba output of the AdaBoost model:","462cf43c":"# Step 2.5: Scaling data and PCA","0e1d27bf":"The variables 'pledged', 'usd_pledged_real' and 'usd_pledged' are strongly linearly correlated.\n\nThe variables 'goal' and 'usd_goal_real' are perfecly linearly correlated.\n\nWe should therefore remove some of the variables to reduce multicollinearity in our models","f20012f0":"### Attempting Principle Component Analysis","e1a1721b":"Datatypes are classified as objects. They should be classified as time data instead:","d29ff32b":"The aim here is to make a simple tool to provide someone that is planning to launch a project with some idea of how many backers he would need to be successful.","eb70ee1b":"### Defining a function to return success probabilities:","8af3a308":"Now with scaling done, we can attempt to do some Principle Component Analysis (PCA) to see if we can reduce the dimensionality:","740b65f7":"However, a big drawback of the models is that they don't take into account the backer contribution: For example, different projects can have different contribution amounts for each backer. Including such data in the kickstarter dataset would have been useful.","081b42e2":"### Before implementing models, we must first encode the categorical variables:","0d932aee":"# Multivariate EDA","6dea894b":"### Cleaning null values:","972fbfc3":"### Currency that project is paid in:","9708757d":"There are not many null values (only 210 out of ~400,000), so we drop the rows with null values","fc952a2c":"# Introduction\n\nKickstarter is an online crowdfunding platform for entrepreneurs to obtain a \"kick-start\" in capital for their project ideas. \n\nIndividual project ideas are uploaded onto the Kickstarter website, where consumers can view information about the project including production plans, budgets and monetary goals. Each project must then reach it's respective monetary goal before it's deadline, which can be anywhere between 1 to 60 days from the project's creation.\n\nKickstarter adopts an \"all-or-nothing\" approach to funding, which means that if a project doesn't hit it's goal by the deadline, it receives no funds and is deemed a failed project. ","841e63b9":"# Part 2: EDA and feature engineering\n\nIn this section I do some exploratory data analysis on the features, and also create a new feature for subsequent models. I start with some univariate analyses and end off with multivariate analyses.","90cdee0b":"# 4: Evaluating models","7fca09d6":"### Variables picked as features so far:\n\nMain category of project (  \u03c72 = 18098, p = 0)\n\nCountry of project origin (  \u03c72 = 2214, p = 0)\n\nCombining launch date and deadline into 'Time before deadline' (F = 4466, p = 0)\n\nProject goal measured in real USD value (F = 18543, p = 0) \n\nNo. of backers (F = 90328, p = 0)\n\n### Variables not picked:\n\nCurrency (May act as a proxy for the country variable)\n\nName (No association with project success with p = 0.35)\n\n","e97536b1":"### Subsequently, defining a second function that generates a plot of project success probability against no. of backers:","faf68ea7":"### Some examples of the function being used: ","dbc99eb6":"### Logistic regression","54f9e7a5":"Kickstarter uses an all-or-nothing funding approach, so complete projects are either successful (completedly funded) or failed (partially funded). I only want to include completed projects with an outcome, therefore cancelled, suspended and undefined project outcomes will be removed.","a2007a22":"All of the models don't seem to provide very believable probabilities with respect to the no. of backers a project might have, producing probabilities of above 0 even when the no. of backers is zero.\n\nThis could be due to the way predict_proba is calculated in sklearn models. Not all predict_proba scores in the models can be interpreted as true probabilities. Logistic Regression models produce outputs that are reasonable probabilities, but other models like Adaboost do not. We therefore have to calibrate the models to produce more realistic probability estimates.","275635a3":"### Looking for patterns by creating clusters based on numeric data (K-Means):","82534e8d":"The model will be useful for an entrepreneur looking for a rough estimate of how many backers a successful project would need. For example, an individual who is launching a game-related project in New Zealand with a goal of $10,000 and a deadline of 50 days would see from the above graph that he would need about 125+ backers to have a decent chance of success.","d9a31673":"About 60% of all projects posted to Kickstarter don't reach their goals. We convert the 'failed' and 'successful' states to dummy variables:","eb0d322d":"Seems like project distribution is pretty balanced, a good variety of all kinds of projects except for the last few","9a111e07":"### Constructing a pairplot to see associations between explanatory variables:","443cdea2":"<img src=\"https:\/\/github.com\/Kesterchia\/Predicting-Kickstarter-project-success\/blob\/main\/Images\/Kickstarter-Celebrates-10-Years-of-Funding.jpg?raw=True\" width=\"420\" height=\"420\"> ","67bb0700":"### Random forest (Ensemble of decision trees) (Removed because forest.joblib was >700MB)","6a6e4c0c":"The F-statistics and p-values for every variable is extremely significant, so these variables should be included in prediction model. We want variables that would be available to an entrepreneur at the time of his project launch, so we exclude the variables relating to the amount pledged.","2efdbf48":"### Deadlines and launch dates of projects:","48a3d1e4":"I will also remove rows with extremely high amounts of backers (above 98th percentile), since its reasonable to assume these projects will almost certainly be successful","ab626da6":"The graph seems to have a distinct elbow at k=4, therefore that value will be chosen as the k-value for clustering","469ee90a":"Most of the projects are paid in USD, doesn't seem like an important variable. Also it can act as a proxy for the 'Country' variable so we will not include it.","d82cfbcb":"We first scale the numerical data. Much of the numerical data exhibit high positive skews, so we will use a robust scaler instead of a standard scaler.","500dcbbd":"### Checking 'country':","1f274ed1":"Here we try some examples of the defined functions on some hypothetical data. \n\nFor the sake of these examples, pretend the user was an entrepreneur who was looking to launch a game-related project in New Zealand, with a crowdfunding timeline of 50 days and a goal of $10,000 USD:","3f016353":"The chi-squared tests for all variables except project name return significant results as expected, indicating a strong association with the target variable. ","0a26f40b":"### Project category:","87270e49":"### Establishing a baseline classifier:","9c493264":"### Final amount pledged for project:","54da870a":"### Conducting a Chi-squared test between categorical variables and target:","e3d5a953":"# Step 2.5:  Creating clusters and encoding variables","aea3c041":"### Number of project backers:","f9dde3f5":"#### Feature engineering a new column: \n\nHaving less time before the deadline might reasonably affect project success. We will create a new column to depict that:","ba42f4d6":"### Conducting an ANOVA test between numerical variables and target:","f38e1c96":"### Plotting ROC curves for each model","5ad72035":"# Dataset information:\n\n\n### Source\n\nData obtained from: https:\/\/www.kaggle.com\/kemical\/kickstarter-projects?select=ks-projects-201801.csv\n\n\n### Content\n\nThis dataset contains about 400,000 rows of data collected on crowdfunding projects hosted on Kickstarter.com.\n\nColumns should be self explanatory. \n\n### Acknowledgements\n\nData is collected from Kickstarter Platform\n","b8030fa8":"# Conclusion:","699f37f2":"### Adaboost classification (Forest of stumps)","86f1bde9":"Chi-squared tests, also known as goodness of fit tests, investigate whether two categorical variables are associated with each other. ","febf1750":"Most projects are actually very close to 0 backers - This makes sense since the failure rate is about 60%","1db279bf":"Each component explains a decent amount of variance, so it might not be suitable to reduce dimensionality any further. Therefore we will not use PCA for this purpose.\n\nNow that both numerical and categorical columns are ready, I can concatenate them:","5ae71356":"# Part 1: Data cleaning","3a702fa1":"Based on the results, the Adaboost model has the highest test accuracy and also has an above average precision and recall performance. Therefore we will be choosing the Adaboost model over the rest.","44357f6d":"# About this project: \n\n### Estimating the number of backers needed to have a successful kickstarter campaign\n\nThis project aims to deliver some insights into how much support from backers a kickstarter project would need to have a decent chance of success. The end goal is to develop a tool that would allow an entrepreneur to have a reasonable estimate of how different expected numbers of backers can affect his chances of success.","fe51ab2b":"# 5: Making a tool to estimate how many backers a project would need","44eaa5b2":"### Support vector machines\n"}}