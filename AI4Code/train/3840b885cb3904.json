{"cell_type":{"e57d3870":"code","f3ff611e":"code","d941abbf":"code","2cfa9141":"code","e654eacd":"code","be98d39d":"code","d5489746":"code","5dfb1615":"code","99db21b7":"code","6e6b88c2":"markdown","8f22b8f2":"markdown","7177f7fc":"markdown","cfc92d0e":"markdown","a5c6500f":"markdown","a3a27477":"markdown","7f26e057":"markdown"},"source":{"e57d3870":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport gc\nimport warnings\n\nwarnings.filterwarnings('ignore')","f3ff611e":"## file path ##\nBASE = '..\/input\/m5-simple-fe\/grid_part_1.pkl'\nCALENDAR = '..\/input\/m5-simple-fe\/grid_part_3.pkl'\nLAGS = '..\/input\/m5-lags-features\/lags_df_28.pkl'\n\nSW = '..\/input\/fast-clear-wrmsse-18ms\/sw_df.pkl'","d941abbf":"## basic features ##\ndf = pd.concat([pd.read_pickle(BASE),\n                    pd.read_pickle(CALENDAR).iloc[:,9:]],\n                    axis=1)\n\n#df = pd.read_pickle(BASE)\n\n## lag features ##\nlag_df = pd.read_pickle(LAGS)\nlag_df = lag_df.iloc[:, 3:11]\n\n## input data ##\ngrid_df = pd.concat([df, lag_df], axis=1)\n\ndel lag_df, df\ngc.collect()","2cfa9141":"## weights and scaling factors ##\n# s, w, sw in sw_df are scaling factor, weight, and the product of them respectively.\n# Since we use only the product sw, other columns are dropped.\n\nsw_df = pd.read_pickle(SW)\n\nsw_df.reset_index(inplace=True)\nsw_df = sw_df[sw_df.level==11]\nsw_df.drop(['level', 's', 'w'], axis=1, inplace=True)\n\nsw_df['id'] = sw_df['id'].astype('category')\ngrid_df = grid_df.merge(sw_df, on='id', how='left')\n\n# The product of sales and sw corresponds to z_it (different by a factor).\n# This one is the main target.\ngrid_df['sw_sales'] = grid_df['sales'] * grid_df['sw']\n\ndel sw_df\ngc.collect()","e654eacd":"## training model (LGBM) ##\n\n# train, validation and test set\nSTART_TRAIN = 730\nEND_TRAIN = 1913\nP_HORIZON = 28\n\ngrid_df = grid_df[grid_df.d>=START_TRAIN]\ngrid_df.to_pickle('grid_df_ex.pkl')\n\ntest_idx = grid_df.d > END_TRAIN\nvalid_idx = (grid_df.d <= END_TRAIN) & (grid_df.d > END_TRAIN - P_HORIZON)\ntrain_idx = (grid_df.d <= END_TRAIN- P_HORIZON) & (grid_df.d >= START_TRAIN)\n\n\n# hyper parameters\nlgb_params = {\n                    'boosting_type': 'gbdt',\n                    'objective': 'tweedie',\n                    'tweedie_variance_power': 1.1,\n                    'learning_rate': 0.1,\n                    'num_leaves': 2**5-1,\n                    'min_data_in_leaf': 2**6-1,\n                    'n_estimators': 100,\n                    'boost_from_average': False,\n                    'verbose': -1,\n                } \n\n# Set the metric in training Mean Absolute Error (MAE).\n# Using MAE with target 'sw_sales', validation values in training show psurdo-WMASE.\nlgb_params['metric'] = 'mae'","be98d39d":"!rm train_data.bin","d5489746":"## Indirect Prediction ##\n\n# features and target\nremove_fe = ['id', 'd', 'sales', 'sw', 'sw_sales']\nfeatures = [fe for fe in list(grid_df) if fe not in remove_fe]\n\nTARGET = 'sw_sales'\n\n# dataset\ntrain_data = lgb.Dataset(grid_df[train_idx][features], \n                        label=grid_df[train_idx][TARGET])\ntrain_data.save_binary('train_data.bin')\ntrain_data = lgb.Dataset('train_data.bin')\n\nvalid_data = lgb.Dataset(grid_df[valid_idx][features],\n                        label=grid_df[valid_idx][TARGET])\n\ndel grid_df\ngc.collect()\n\n# model training\nestimator = lgb.train(lgb_params,\n                      train_data,\n                      valid_sets = [train_data, valid_data],\n                      verbose_eval = 10,\n                      early_stopping_rounds = 5,\n                      )\n\n# Validaiton result means psuedo-WMASE at the bottom level (level 12).\n# Calculated psuedo-WMASE is different by a constant factor.\n# The validation score means ","5dfb1615":"## prediction for test set ##\ngrid_df = pd.read_pickle('grid_df_ex.pkl')\n\ntest_data = grid_df[test_idx][features]\ngrid_df['sw_sales'][test_idx] = estimator.predict(test_data)\n\ndel test_data\ngc.collect()\n\n# sw_sales -> sales\ngrid_df['sales'][test_idx] = grid_df['sw_sales'][test_idx] \/ grid_df['sw'][test_idx]","99db21b7":"# The final output 'sales' accuracy is bound above at the bottom level.\n# However, there are some items with weight=0.\n# Some item has sales values of infinity......\ngrid_df['sales'][test_idx].max()","6e6b88c2":"At the bottom level we don't have to care this infinity, but for upper aggregated level, we must predict these zero-weight items sales if we take bottom-up approach.\n\nIn that case, we need another model which predicts or complements these zero-weight items sales.","8f22b8f2":"## Interpretion on WRMSSE\nWeighted Root Mean Squared Scaled Error (WRMSSE) is defined as\n\n\\begin{equation}\n\\mathrm{WRMSSE} = \\sum_{i} w_i \\sqrt{ \\sum_{t=n+1}^{n+h} \\frac{1}{h} \\frac{(y_{it} - \\hat{y}_{it})^2}{S_i}},\n\\end{equation}\n\nwhere $S_i$ means random walk, \n\n\\begin{equation}\nS_i = \\frac{1}{n-1} \\sum^n_{t=2} (y_{it} - \\hat{y}_{it})^2.\n\\end{equation}\n\nWith some calculation, \n\n\\begin{equation}\n\\mathrm{WRMSSE} = \\sum_{i} \\frac{w_i}{\\sqrt{S_i}} \\sqrt{ \\sum_{t=n+1}^{n+h} \\frac{1}{h} (y_{it} - \\hat{y}_{it})^2}.\n\\end{equation}\n\nSo, we can regard $ w_i \/ \\sqrt{S_i}$ as the net weight.\nThis factor can be interpreted as price times randomness.","7177f7fc":"## Inequality regarging WRMSSE\nUsing triangle inequality $\\sqrt{\\sum_i a_i^2} \\leq \\sum_i |a_i|$\n\\begin{equation}\n\\sqrt{ \\sum_t (y_{it} - \\hat{y}_{it})^2 } \\leq \\sum_t |y_{it} - \\hat{y}_{it}|.\n\\end{equation}\n\nThus, \n\\begin{equation}\n\\mathrm{WRMSSE} \\leq \\sum_{i, t} \\frac{w_i}{\\sqrt{h S_i}} |y_{it} - \\hat{y}_{it}|.\n\\end{equation}\n\nDefined as \n\\begin{equation}\nz_{it} := w_i y_{it} \/ \\sqrt{h S_i}, \\\\\nN := \\sum_i 1, \n\\end{equation}\n\nWRMSSE can be evaluated as\n\\begin{equation}\n\\mathrm{WRMSSE} \\leq hN \\times \\frac{1}{hN} \\sum_{i,t} |z_{it} - \\hat{z}_{it} |.\n\\end{equation}\n\nThe right side of this inequality can be interpreted as Mean Absolute Error (MAE).\nSimilar procedures are found in [this paper](https:\/\/robjhyndman.com\/papers\/mase.pdf), so I would like to call this one psuedo-Weighted Mean Absolute Scaled Error (psuedo-WMASE).\n\nSince WRMSSE is bounded above by psuedo-WMASE, WRMSSE is always lower than psuedo-WMASE.\n\nIn this expression, we don't have to care weights and scaling factors when predictions are evaluated \nbecause sum about indices i and t can be exchanged.\n\nIt is difficult to adopt WRMSSE directly, but after these transformations, we can use psuedo-WMASE as metric easily. \nWhen you take some transformation, true validation score for WRMSSE in training is bounded above by psuedo-WMASE.\n\n(TRUE Weighted Mean Absolute Scaled Error (WMASE) may be defined as\n\\begin{equation}\n\\mathrm{WMASE} = \\sum_{i} w_i \\sum_{t=n+1}^{n+h} \\frac{1}{h} \\frac{|y_{it} - \\hat{y}_{it}|}{S'_i},\n\\end{equation}\nwhere\n\\begin{equation}\nS'_i = \\frac{1}{n-1} \\sum^n_{t=2} |y_{it} - \\hat{y}_{it}|.\n\\end{equation}\n)","cfc92d0e":"## How to use this idea (example)\nThanks to the help of some great notebooks ([M5 - Simple FE](https:\/\/www.kaggle.com\/kyakovlev\/m5-simple-fe) [@kyakovlev](https:\/\/www.kaggle.com\/kyakovlev) and [Fast & Clear WRMSSE 18ms](https:\/\/www.kaggle.com\/sibmike\/fast-clear-wrmsse-18ms) [@sibmike](https:\/\/www.kaggle.com\/sibmike)), I would show the idea above with the simple bottom-up approach.\n\n* train: d_730 - d_1885\n* validation: d_1886 - d_1913\n* test: d_1914 - d_1941\n\nThe steps to apply the idea above are:\n1. Make $z_{it}$ with 'sales', weights, scaling factors.\n2. Train and predict (Target = $z_{it}$).\n3. Make 'sales' prediction at the bottom level from $z_{it}$","a5c6500f":"The validation score in training is $\\frac{1}{hN} \\sum_i |z_{it} - \\hat{z}_{it}|$. To get psuedo-WMASE, we must the factor $hN$, but this is just a constant value ($h=28, N=30490$). \nSo I ignore the difference.","a3a27477":"# Inequality regarding WRMSSE\nIn this notebook, I would like to show the inequality of WRMSSE.\n\n\n\n","7f26e057":"## Difficulty in this competition\nIn this competition, we must predict not only one aggregated level but other aggregated or disaggregated levels.\n$y_{it}$ depends on other levels. \n\nIf we take bottom-up approach, all of upper level values are determined automatically.\nSo are if you take top-down or middle-out approach.\n(Of course, this is too naive. \nThere would be many approaches that overcomes this difficulty coming from this level-dependent metric).\n\nHowever, at least, we can know if the predictions at some level are good or not while training with validation sets."}}