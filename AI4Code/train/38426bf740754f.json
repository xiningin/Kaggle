{"cell_type":{"2e6f649c":"code","8f940698":"code","4e0e1063":"code","bfe0ab17":"code","f257bb1e":"code","a8397346":"code","fadea2f1":"code","e04ee515":"code","bfae8fd4":"code","39f35dd2":"code","ef8ffd1d":"code","4262b3b0":"code","a07ef831":"code","96788d76":"code","d6f3a0e9":"code","204e7fc6":"code","b67464d0":"code","b990c435":"code","4477337f":"code","e8a0e0eb":"code","f13abdb6":"code","d455f693":"code","03b95fa6":"code","26f5de37":"code","49efee9a":"code","98cd26fe":"code","6bae43c3":"code","981ee796":"code","917b93e5":"code","5b82011b":"code","502e3b0f":"code","6feeebd1":"code","93519ce4":"code","e2f7b44e":"code","1e36ceba":"code","c630fe68":"code","c82bfdaf":"code","873f0b4c":"code","476aa866":"code","28ec5676":"code","a106d17e":"code","0340f483":"code","d2ae4da6":"code","e3b4bf33":"code","32a4bbd9":"code","7df6814f":"code","843c952d":"code","174bc714":"code","254e632f":"code","f9c0e459":"code","93041327":"code","ed01607f":"code","2f678c59":"code","e3886eca":"code","a36a0aa5":"code","c8801f20":"code","f6378ce5":"code","62cd990a":"code","5761cf55":"code","a02403c6":"code","b258812a":"code","bc586ae7":"code","13754902":"code","016e2fa3":"code","99289f5b":"code","170b91a0":"code","61540b15":"code","ca4e51a1":"code","c4a32637":"code","75eee355":"code","ead660a2":"code","26addeec":"code","07633731":"code","70b65859":"code","2d55460a":"code","4c7e1149":"code","301b8893":"code","521d9b07":"code","d255562f":"code","d0883fbc":"code","d88ee861":"code","28b16596":"code","0ea934c3":"code","f386afa3":"code","bb77addc":"code","e7565ffd":"code","9e170d7d":"code","862a82d5":"code","f8ade28b":"code","84c22164":"code","1a8f6b59":"code","bb4fb2eb":"code","20efdff3":"code","d170244c":"code","c69aedf0":"code","d83f2ac1":"code","f9186de3":"code","456c4648":"code","0b0c1e6b":"code","ab0731fc":"code","ca31cb38":"code","e8a70d58":"code","606121b3":"code","d01e0f34":"code","508e492d":"code","d361db71":"code","ee6e3fb8":"code","de461b4d":"code","9036c698":"code","0f37ceea":"code","49d29352":"code","0f171abd":"code","1954f188":"code","2069548f":"code","656aa779":"code","4ef13354":"code","df9b03da":"code","798e18f0":"code","b388adc0":"code","00750ad8":"code","5b1e97ef":"code","ec148c57":"code","7e75d2ed":"code","8dae9bf8":"code","85a4f66f":"code","9d2dc64b":"code","6e641145":"code","43be3989":"code","a240348a":"code","8151e48b":"code","f209465a":"code","d4a6f61e":"code","cc24a56e":"code","30782dc9":"code","947e5161":"code","f6935010":"code","cbf26743":"code","1fbceecc":"code","f5c5beea":"code","5643164f":"code","24f76b54":"markdown","90cafc3c":"markdown","2964f46d":"markdown","6970ec84":"markdown","9808affa":"markdown","9a834cb5":"markdown","0ee105eb":"markdown","5a0aaaaa":"markdown","973802f9":"markdown","41a140e2":"markdown","b14c0f8b":"markdown","8c8a7087":"markdown","eb168135":"markdown","1ace2545":"markdown","b2e84045":"markdown","5e694c3d":"markdown","4bb8bb7a":"markdown","5d08e634":"markdown","4e12bb01":"markdown","01acc52c":"markdown","b374a25d":"markdown","212e508a":"markdown","ca093fe9":"markdown","7207bccf":"markdown","08ecca43":"markdown","fd175cd6":"markdown","de7a2452":"markdown","d5171ab2":"markdown","b482bed1":"markdown","13535739":"markdown","8048c6b8":"markdown","1c7d20d8":"markdown","0e088f62":"markdown","b1333068":"markdown","d0b8357e":"markdown","5581a49a":"markdown","4894709a":"markdown","d1a50aaf":"markdown","7da231f6":"markdown","cc991555":"markdown","7bc37af3":"markdown","d36f63c9":"markdown","815fe085":"markdown","5a1c687e":"markdown","3a3b400a":"markdown","c71228e0":"markdown","8909ea47":"markdown","a8dd53c6":"markdown","13157bb7":"markdown","af152a2c":"markdown","43db2f5a":"markdown","e19afff8":"markdown","2896d78a":"markdown","d3fe2290":"markdown","ad6d1845":"markdown","e5669a5b":"markdown","fb1022ca":"markdown","4a277cc5":"markdown","8099d80f":"markdown","21b8b190":"markdown","83e02694":"markdown","1a29cd5c":"markdown","61c332b4":"markdown","695dcaea":"markdown"},"source":{"2e6f649c":"#Importing libraries\n\n#Basic libraries\nimport pandas as pd\nimport numpy as np\nimport os\n# handling warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Data modelling libraries \nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFE\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import KFold\nfrom sklearn import metrics\n#Statistics\nfrom scipy import stats\nfrom scipy.stats import norm, skew \n\npd.set_option('display.max_rows', 500)","8f940698":"# Creating a class color for setting print formatting\nclass color:\n   BLUE = '\\033[94m'\n   BOLD = '\\033[1m'\n   END = '\\033[0m'","4e0e1063":"#Read the data and creating the dataframe\nhousing_df=pd.read_csv(\"..\/input\/suprise-houseing\/train.csv\")","bfe0ab17":"# Check if data is loaded or not\nhousing_df.head()","f257bb1e":"# inspect the first five rows of data\nhousing_df.head()","a8397346":"# check the no of rows and columns\nhousing_df.shape","fadea2f1":"#info all the entire data along with types\nhousing_df.info()","e04ee515":"#Summary of the numerical columns in the dataframe\nhousing_df.describe()","bfae8fd4":"#List of columns in the data frame\nhousing_df.columns","39f35dd2":"# Check if any null values are present in the data\nhousing_df.isnull().sum()","ef8ffd1d":"#Checking the number of missing values and its percentage\nTotal_missing = housing_df.isnull().sum().sort_values(ascending = False)\nTotal_missing_Perc = (100*housing_df.isnull().sum()\/housing_df.shape[0]).sort_values(ascending = False)\ndf_missing_values = pd.concat([Total_missing,Total_missing_Perc], axis=1, keys=['Total_missing_values', 'Percent_missing_values'])\ndf_missing_values.head(80)","4262b3b0":"# Checking how many columns have more than 45% of missing data\n\nprint(color.BOLD+ color.BLUE+'Total no of columns with missing values more than 45% : {}'.format(df_missing_values[df_missing_values['Percent_missing_values'] >= 45].shape[0])+ color.END)","a07ef831":"#Create a new dataframe named df_cleaned with all columns with data missing < 45% for our  further analysis\n\ndf_cleaned = housing_df.loc[:,(100*housing_df.isnull().sum()\/housing_df.shape[0]).sort_values(ascending = False) < 45]\ndf_cleaned.head()","96788d76":"#Checking the no of unique values in each column\ndf_cleaned.nunique()","d6f3a0e9":"# Get the value counts of all the columns\n\nfor column in df_cleaned:\n    \n    print(df_cleaned[column].astype('category').value_counts())\n    print('___________________________________________________')","204e7fc6":"# Data imputation for column 'GarageQual'\n\nplt.figure(figsize=(8,12))\nsns.countplot(df_cleaned.GarageQual)\nplt.title(\"Checking the mode of column GarageQual- UniVariate Analysis\")\nplt.show()","b67464d0":"df_cleaned.GarageQual.value_counts()","b990c435":"# Most of the values are TA, so we can map the NAN values of 'GarageQual' to TA -MODE\ndf_cleaned['GarageQual'] = df_cleaned['GarageQual'].replace(np.nan,'TA')","4477337f":"df_cleaned['GarageQual'].dtype","e8a0e0eb":"df_cleaned.GarageQual.value_counts()","f13abdb6":"# Data imputation for column 'GarageFinish'\n\nplt.figure(figsize=(8,12))\nsns.countplot(df_cleaned.GarageFinish)\nplt.title(\"Checking the mode of column GarageFinish- UniVariate Analysis\")\nplt.show()","d455f693":"df_cleaned.GarageFinish.value_counts()","03b95fa6":"df_cleaned['GarageFinish']=df_cleaned['GarageFinish'].replace(np.nan,'Unknown')","26f5de37":"df_cleaned.GarageFinish.value_counts()","49efee9a":"# Data imputation for column 'GarageType'\n\nplt.figure(figsize=(8,12))\nsns.countplot(df_cleaned.GarageType)\nplt.title(\"Checking the mode of column GarageType- UniVariate Analysis\")\nplt.show()","98cd26fe":"df_cleaned.GarageType.value_counts()","6bae43c3":"# Most of the values are ATTCHD , so we can map the NAN values of 'GarageType' to ATTCHD (MODE)\ndf_cleaned['GarageType'] = df_cleaned['GarageType'].replace(np.nan,'Attchd')","981ee796":"df_cleaned.GarageType.value_counts()","917b93e5":"# Data imputation for column 'GarageCond'\n\nplt.figure(figsize=(8,12))\nsns.countplot(df_cleaned.GarageCond)\nplt.title(\"Checking the mode of column GarageCond- UniVariate Analysis\")\nplt.show()","5b82011b":"df_cleaned.GarageCond.value_counts()","502e3b0f":"# Most of the values is TA , so we can map the NAN values of 'GarageCond' to TA -MODE\ndf_cleaned['GarageCond'] = df_cleaned['GarageCond'].replace(np.nan,'TA')","6feeebd1":"df_cleaned.GarageCond.value_counts()","93519ce4":"#PLots for outlier analysis of GarageYrBlt\nsns.boxplot(df_cleaned.GarageYrBlt)\nplt.show()","e2f7b44e":"df_cleaned['GarageYrBlt'].fillna(df_cleaned['GarageYrBlt'].mean(),inplace=True)","1e36ceba":"#PLots for outlier analysis for LotFrontage\nsns.boxplot(df_cleaned.LotFrontage)\nplt.show()","c630fe68":"df_cleaned['LotFrontage'].fillna(df_cleaned['LotFrontage'].median(),inplace=True)","c82bfdaf":"# As the other columns have less than 3% of Nan Values we chose to drop those rows .\n\ndf_cleaned.dropna(inplace=True)","873f0b4c":"#Final check of missing values \nTotal_missing = df_cleaned.isnull().sum().sort_values(ascending = False)\nTotal_missing_Perc = (100*df_cleaned.isnull().sum()\/df_cleaned.shape[0]).sort_values(ascending = False)\ndf_missing_values = pd.concat([Total_missing,Total_missing_Perc], axis=1, keys=['Total_missing_values', 'Percent_missing_values'])\ndf_missing_values","476aa866":"df_cleaned.columns","28ec5676":"df_cleaned['Age']=df_cleaned['YrSold']-df_cleaned['YearRemodAdd']\ndf_cleaned['Age'].head()","a106d17e":"df_cleaned=df_cleaned.drop(['YrSold', 'MoSold','YearBuilt','YearRemodAdd'], axis = 1) ","0340f483":"df_cleaned.head()","d2ae4da6":"df_cleaned=df_cleaned.drop(['Id'], axis = 1) ","e3b4bf33":"numeric_feats = df_cleaned.dtypes[df_cleaned.dtypes != \"object\"].index\nnumeric_feats","32a4bbd9":"df_cleaned[numeric_feats].hist(bins=15, figsize=(30, 30))\nplt.show()","7df6814f":"# correlation matrix\ncor = df_cleaned.corr()\ncor","843c952d":"# plotting correlations on a heatmap\n\n# figure size\nplt.figure(figsize=(24,16))\n\n# heatmap\nsns.heatmap(cor, cmap=\"YlGnBu\", annot=True)\nplt.show()\n","174bc714":"plt.scatter(df_cleaned.GrLivArea, df_cleaned.SalePrice)\n\nplt.title('GrLivArea vs SalePrice')\n\nplt.xlabel('GrLivArea')\n\nplt.ylabel('SalePrice')\n\nplt.show()","254e632f":"plt.figure(figsize = (20,20)) \nplt.subplot(3,3,1)\nplt.scatter(df_cleaned.GrLivArea, df_cleaned.SalePrice)\nplt.xlabel('GrLivArea')\nplt.ylabel('SalePrice')\nplt.subplot(3,3,2)\nplt.scatter(df_cleaned.TotalBsmtSF, df_cleaned.SalePrice)\nplt.xlabel('TotalBsmtSF')\nplt.subplot(3,3,3)\nplt.scatter(df_cleaned.FullBath, df_cleaned.SalePrice)\nplt.xlabel('FullBath')\nplt.subplot(3,3,4)\nplt.scatter(df_cleaned.TotRmsAbvGrd, df_cleaned.SalePrice)\nplt.xlabel('TotRmsAbvGrd')\nplt.ylabel('SalePrice')\nplt.subplot(3,3,5)\nplt.scatter(df_cleaned.GarageCars, df_cleaned.SalePrice)\nplt.xlabel('GarageCars')\nplt.subplot(3,3,6)\nplt.scatter(df_cleaned.GarageArea, df_cleaned.SalePrice)\nplt.xlabel('GarageArea')\nplt.subplot(3,3,7)\nplt.scatter(df_cleaned.Age, df_cleaned.SalePrice)\nplt.xlabel('Age')\nplt.ylabel('SalePrice')\nplt.subplot(3,3,8)\nplt.scatter(df_cleaned.OverallQual, df_cleaned.SalePrice)\nplt.xlabel('OverallQual')\nplt.subplot(3,3,9)\nplt.scatter(df_cleaned['1stFlrSF'], df_cleaned.SalePrice)\nplt.xlabel('1stFlrSF')\nplt.show()","f9c0e459":"cat_feats  = df_cleaned.dtypes[df_cleaned.dtypes == \"object\"].index\ncat_feats  ","93041327":"plt.figure(figsize = (20,20)) \nplt.subplot(3,3,1)\nsns.boxplot(x='MSZoning', y=\"SalePrice\", data=df_cleaned)\nplt.subplot(3,3,2)\nsns.boxplot(x='BldgType', y=\"SalePrice\", data=df_cleaned)\nplt.subplot(3,3,3)\nsns.boxplot(x='Street', y=\"SalePrice\", data=df_cleaned)\nplt.subplot(3,3,4)\nsns.boxplot(x='LotShape', y=\"SalePrice\", data=df_cleaned)\nplt.subplot(3,3,5)\nsns.boxplot(x='HouseStyle', y=\"SalePrice\", data=df_cleaned)\nplt.subplot(3,3,6)\nsns.boxplot(x='Utilities', y=\"SalePrice\", data=df_cleaned)\nplt.subplot(3,3,7)\nsns.boxplot(x='RoofStyle', y=\"SalePrice\", data=df_cleaned)\nplt.subplot(3,3,8)\nsns.boxplot(x='LandSlope', y=\"SalePrice\", data=df_cleaned)\nplt.subplot(3,3,9)\nsns.boxplot(x='Neighborhood', y=\"SalePrice\", data=df_cleaned)\nplt.show()","ed01607f":"plt.figure(figsize = (20,20)) \nplt.subplot(3,3,1)\nsns.boxplot(x='ExterQual', y=\"SalePrice\", data=df_cleaned)\nplt.subplot(3,3,2)\nsns.boxplot(x='Foundation', y=\"SalePrice\", data=df_cleaned)\nplt.subplot(3,3,3)\nsns.boxplot(x='BsmtQual', y=\"SalePrice\", data=df_cleaned)\nplt.subplot(3,3,4)\nsns.boxplot(x='Heating', y=\"SalePrice\", data=df_cleaned)\nplt.subplot(3,3,5)\nsns.boxplot(x='CentralAir', y=\"SalePrice\", data=df_cleaned)\nplt.subplot(3,3,6)\nsns.boxplot(x='Electrical', y=\"SalePrice\", data=df_cleaned)\nplt.subplot(3,3,7)\nsns.boxplot(x='KitchenQual', y=\"SalePrice\", data=df_cleaned)\nplt.subplot(3,3,8)\nsns.boxplot(x='GarageType', y=\"SalePrice\", data=df_cleaned)\nplt.subplot(3,3,9)\nsns.boxplot(x='GarageQual', y=\"SalePrice\", data=df_cleaned)\nplt.show()\nplt.figure(figsize = (20,5)) \nplt.subplot(1,2,1)\nsns.boxplot(x='SaleType', y=\"SalePrice\", data=df_cleaned)\nplt.subplot(1,2,2)\nsns.boxplot(x='SaleType', y=\"SalePrice\", data=df_cleaned)\nplt.show()","2f678c59":"df_cleaned['SalePrice'].describe()","e3886eca":"#Check the distribution the SalePrice\nsns.set(style='whitegrid', palette=\"deep\", font_scale=1.1, rc={\"figure.figsize\": [8, 5]})\nsns.distplot(df_cleaned['SalePrice'], norm_hist=False, kde=False, bins=20, hist_kws={\"alpha\": 1}).set(xlabel='Sale Price', ylabel='Count');\nplt.title(\"Distribution of the SalePrice variable.\")\nplt.show()","a36a0aa5":"sns.distplot(df_cleaned['SalePrice'] , fit=norm);\n\n# Estimating the mu and sigma using the fit function\n(mu, sigma) = norm.fit(df_cleaned['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Plotting the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Plotting the QQ-plot\nfig = plt.figure()\nres = stats.probplot(df_cleaned['SalePrice'], plot=plt)\nplt.show()","c8801f20":"### Log-transformation of the target variable","f6378ce5":"#We use the numpy function log to all elements of the column\ndf_cleaned[\"SalePrice\"] = np.log(df_cleaned[\"SalePrice\"])\n\n#Checking the new distribution \nsns.distplot(df_cleaned['SalePrice'] , fit=norm);\n\n# Estimating the mu and sigma using the fit function\n(mu, sigma) = norm.fit(df_cleaned['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Plotting the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Plotting the QQ-plot\nfig = plt.figure()\nres = stats.probplot(df_cleaned['SalePrice'], plot=plt)\nplt.show()","62cd990a":"# List of variables to map\ndef map_function(x):\n    return x.map({'Ex':5,'Gd':4,'TA':3,'Fa':2,'Po':1,'Na':0})\ndef map_function1(x):\n    return x.map({'Gd':4,'Av':3,'Mn':2,'No':1,'Na':0})\ndef map_function2(x):\n    return x.map({'GLQ':6,'ALQ':5,'BLQ':4,'Rec':3,'LwQ':2,'Unf':1,'NA':0})\ndef map_function3(x):\n    return x.map({'Y':1,'N':0})\ndef map_function4(x):\n    return x.map({'Typ':8,'Min1':7,'Min2':6,'Mod':5,'Maj1':4,'Maj2':3,'Sev':2,'Sal':1})\ndef map_function5(x):\n    return x.map({'Fin':3,'RFn':2,'Unf':1,'NA':0,'Unknown':0})\ndef map_function6(x):\n    return x.map({'Y':2,'P':2,'N':0})\n\ndf_cleaned['ExterQual'] = df_cleaned[['ExterQual']].apply(map_function)\ndf_cleaned['ExterCond'] = df_cleaned[['ExterCond']].apply(map_function)\ndf_cleaned['BsmtQual'] = df_cleaned[['BsmtQual']].apply(map_function)\ndf_cleaned['BsmtCond'] = df_cleaned[['BsmtCond']].apply(map_function)\ndf_cleaned['BsmtExposure'] = df_cleaned[['BsmtExposure']].apply(map_function1)\ndf_cleaned['BsmtFinType1'] = df_cleaned[['BsmtFinType1']].apply(map_function2)\ndf_cleaned['BsmtFinType2'] = df_cleaned[['BsmtFinType2']].apply(map_function2)\ndf_cleaned['HeatingQC'] = df_cleaned[['HeatingQC']].apply(map_function)\ndf_cleaned['CentralAir'] = df_cleaned[['CentralAir']].apply(map_function3)\ndf_cleaned['KitchenQual'] = df_cleaned[['KitchenQual']].apply(map_function)\ndf_cleaned['Functional'] = df_cleaned[['Functional']].apply(map_function4)\ndf_cleaned['GarageFinish'] = df_cleaned[['GarageFinish']].apply(map_function5)\ndf_cleaned['GarageQual'] = df_cleaned[['GarageQual']].apply(map_function)\ndf_cleaned['GarageCond'] = df_cleaned[['GarageCond']].apply(map_function)\ndf_cleaned['PavedDrive'] = df_cleaned[['PavedDrive']].apply(map_function6)","5761cf55":"df_cleaned['MSSubClass']=df_cleaned['MSSubClass'].astype('object')","a02403c6":"cat_feats  = df_cleaned.dtypes[df_cleaned.dtypes == \"object\"].index\ncat_feats  ","b258812a":"#Final check of missing values \nTotal_missing = df_cleaned.isnull().sum().sort_values(ascending = False)\nTotal_missing_Perc = (100*df_cleaned.isnull().sum()\/df_cleaned.shape[0]).sort_values(ascending = False)\ndf_missing_values = pd.concat([Total_missing,Total_missing_Perc], axis=1, keys=['Total_missing_values', 'Percent_missing_values'])\ndf_missing_values","bc586ae7":"# Creating dummies\ndummy = pd.get_dummies(df_cleaned[['MSSubClass','MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities',\n       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n       'Exterior2nd', 'MasVnrType', 'Foundation', 'Heating', 'Electrical',\n       'GarageType', 'SaleType', 'SaleCondition']], drop_first=True)\ndummy.head()","13754902":"# Adding the results back to the cleaned dataframe\ndf_cleaned = pd.concat([df_cleaned, dummy], axis=1)\ndf_cleaned.head()","016e2fa3":"# Dropping the columns whose dummies have been created\ndf_cleaned=df_cleaned.drop(['MSSubClass','MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities',\n       'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2',\n       'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st',\n       'Exterior2nd', 'MasVnrType', 'Foundation', 'Heating', 'Electrical',\n       'GarageType', 'SaleType', 'SaleCondition'], axis = 1)","99289f5b":"df_cleaned.shape","170b91a0":"#Final check of missing values \nTotal_missing = df_cleaned.isnull().sum().sort_values(ascending = False)\nTotal_missing_Perc = (100*df_cleaned.isnull().sum()\/df_cleaned.shape[0]).sort_values(ascending = False)\ndf_missing_values = pd.concat([Total_missing,Total_missing_Perc], axis=1, keys=['Total_missing_values', 'Percent_missing_values'])\ndf_missing_values","61540b15":"# model building - Test Train split\nnp.random.seed(0)\nhouse_train,house_test = train_test_split(df_cleaned, train_size=0.7,random_state=100)\nprint(house_train.shape)\nprint(house_test.shape)\n","ca4e51a1":"# display the first five rows of train dataframe\nhouse_train.head()","c4a32637":"# display the first five rows of test dataframe\nhouse_test.head()","75eee355":"y_train=house_train.pop('SalePrice')\nX_train=house_train","ead660a2":"y_test=house_test.pop('SalePrice')\nX_test=house_test","26addeec":"#1. Instantiate an scaler object\nscaler=MinMaxScaler()\nnumeric_feats = X_train.dtypes[X_train.dtypes != \"object\"].index\nX_train[numeric_feats]=scaler.fit_transform(X_train[numeric_feats])","07633731":"X_test[numeric_feats]=scaler.transform(X_test[numeric_feats])","70b65859":"X_train.head()","2d55460a":"X_train.describe()","4c7e1149":"# verify the X_train to check if SalePrice column is removed\nX_train.head()","301b8893":"# verifying the y_train to check if it has only SalePrice\ny_train.head()","521d9b07":"lm = LinearRegression()  # Instantiating\nlm.fit(X_train, y_train)","d255562f":"\n#Coefficients , intercepts\n\nprint(lm.coef_)\nprint(lm.intercept_)","d0883fbc":"# Import RFE\n\nfrom sklearn.feature_selection import RFE\nlm = LinearRegression()   \nrfe = RFE(lm, 90)\nrfe.fit(X_train, y_train)  \nprint(rfe.support_)        #True or False   \nprint(rfe.ranking_)","d88ee861":"col = X_train.columns[rfe.support_]\ncol","28b16596":"# Features which can be excluded to make the model according to RFE\nX_train.columns[~rfe.support_]","0ea934c3":"# Creating an X_train dataframe to build the first model with features selected by RFE technique\nX_train_rfe1 =X_train[col]","f386afa3":"import statsmodels.api as sm  \nX_train_rfe1 = sm.add_constant(X_train_rfe1) #Adding Constant\nX_train_rfe1.head()","bb77addc":"lm1 = sm.OLS(y_train, X_train_rfe1).fit()   \nprint(lm1.summary())","e7565ffd":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nvif['Features'] = X_train_rfe1.columns\nvif['VIF'] = [variance_inflation_factor(X_train_rfe1.values, i) for i in range(X_train_rfe1.shape[1])]\nvif['VIF'] = round(vif['VIF'],2)\nvif","9e170d7d":"#we are deleting the other columns not selected by RFE techinque\nX_train = X_train[col]\nX_test = X_test[col]","862a82d5":"X_train","f8ade28b":"# Lets assume alpha is 0.001 and build a Ridge regression model\nridge = Ridge(alpha=0.001)\nridge.fit(X_train,y_train)\n\ny_train_pred = ridge.predict(X_train)\nprint(r2_score(y_train,y_train_pred))\n","84c22164":"# Tune hyperparameter\nparams = {'alpha':  [0.001,0.01,0.5, 1.0, 10.0, 50, 100,500, 1000]}\n\n# Build model using Ridge regression\nridge = Ridge()\n\n# cross validation\nfolds = 5\nmodel_cv = GridSearchCV(estimator = ridge, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \nmodel_cv.fit(X_train, y_train) ","1a8f6b59":"cv_results = pd.DataFrame(model_cv.cv_results_)\n\ncv_results.head(10)","bb4fb2eb":"# plotting mean test and train scoes with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('int32')\n\n# plotting\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","20efdff3":"# list of alphas to tune\nparams = {'alpha': [0.001,0.01, 1.0, 2.0, 3.0,\n                                   4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20]}\n\n\nridge = Ridge()\n\n# cross validation\nfolds = 5\nmodel_cv = GridSearchCV(estimator = ridge, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \nmodel_cv.fit(X_train, y_train) ","d170244c":"cv_results = pd.DataFrame(model_cv.cv_results_)\n#cv_results = cv_results[cv_results['param_alpha']<=20]\ncv_results.head(10)","c69aedf0":"# plotting mean test and train scoes with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('int32')\n\n# plotting\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","d83f2ac1":"# Finding out the best score and Optimum value of alpha\nprint(model_cv.best_estimator_)\nbest_ridge_score = model_cv.best_score_\nbest_ridge_alpha = model_cv.best_params_[\"alpha\"]\n\nprint(\" The best r2 score is obtained at alpha {0}\".format(best_ridge_alpha))","f9186de3":"# Model building using optimal alpha\n#ridge = Ridge(alpha=best_ridge_alpha)\nridge = Ridge(alpha=1.0)\nridge.fit(X_train, y_train)","456c4648":"#creating coeffcients for the ridge regression\nmodel_parameter = list(ridge.coef_)\nmodel_parameter.insert(0,ridge.intercept_)\ncols = house_train.columns\ncols.insert(0,'const')\nridge_coef = pd.DataFrame(list(zip(cols,model_parameter,(abs(ele) for ele in model_parameter))))\nridge_coef.columns = ['Features','Coefficient','Mod']","0b0c1e6b":"#selecting the top 10 variables\nridge_coef.sort_values(by='Mod',ascending=False).head(10)","ab0731fc":"# Prediction using ridge regression\ny_train_ridge_pred = ridge.predict(X_train)\nprint(\"Ridge regression train r2: \",round(metrics.r2_score(y_true=y_train, y_pred=y_train_ridge_pred),4))\ny_test_ridge_pred = ridge.predict(X_test)\nprint(\"Ridge regression test r2: \",round(metrics.r2_score(y_true=y_test, y_pred=y_test_ridge_pred),4))","ca31cb38":"# list of alphas to tune\nparams = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100]}\nlasso = Lasso()\n\n# cross validation\nmodel_cv = GridSearchCV(estimator = lasso, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \n\nmodel_cv.fit(X_train, y_train) ","e8a70d58":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results.head()","606121b3":"# plotting mean test and train scoes with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n\n# plotting\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\n\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","d01e0f34":"# list of alphas to tune\nparams = {'alpha': [0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]}\nlasso = Lasso()\n\n# cross validation\nmodel_cv = GridSearchCV(estimator = lasso, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \n\nmodel_cv.fit(X_train, y_train) ","508e492d":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results.head()","d361db71":"# plotting mean test and train scoes with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n\n# plotting\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\n\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","ee6e3fb8":"\n# Checking the best parameter(Alpha value)\nmodel_cv.best_params_","de461b4d":"# list of alphas to tune\nparams = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1]}\nlasso = Lasso()\n\n# cross validation\nmodel_cv = GridSearchCV(estimator = lasso, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \n\nmodel_cv.fit(X_train, y_train) ","9036c698":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results.head()","0f37ceea":"# plotting mean test and train scoes with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n\n# plotting\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\n\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","49d29352":"# list of alphas to tune\nparams = {'alpha': [0.00001,0.0001, 0.001, 0.01, 0.05]}\nlasso = Lasso()\n\n# cross validation\nmodel_cv = GridSearchCV(estimator = lasso, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \n\nmodel_cv.fit(X_train, y_train) ","0f171abd":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results.head()","1954f188":"# plotting mean test and train scoes with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n\n# plotting\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\n\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","2069548f":"\n# Checking the best parameter(Alpha value)\nmodel_cv.best_params_","656aa779":"# list of alphas to tune\nparams = {'alpha': [0.00001,0.0001, 0.001]}\nlasso = Lasso()\n\n# cross validation\nmodel_cv = GridSearchCV(estimator = lasso, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \n\nmodel_cv.fit(X_train, y_train) ","4ef13354":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results.head()","df9b03da":"# plotting mean test and train scores with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n\n# plotting\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\n\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","798e18f0":"\n# Checking the best parameter(Alpha value)\nmodel_cv.best_params_","b388adc0":"# After performing grid search we found the same alpha that ue use before\nlasso = Lasso(alpha=0.0001)\nlasso.fit(X_train,y_train)\n\ny_train_pred = lasso.predict(X_train)\ny_test_pred = lasso.predict(X_test)\n\nprint(\"Lasso Regression train r2:\",r2_score(y_true=y_train,y_pred=y_train_pred))\nprint(\"Lasso Regression test r2:\",r2_score(y_true=y_test,y_pred=y_test_pred))","00750ad8":"\nmodel_param = list(lasso.coef_)\nmodel_param.insert(0,lasso.intercept_)\ncols = X_test.columns\ncols.insert(0,'const')\nlasso_coef = pd.DataFrame(list(zip(cols,model_param,(abs(ele) for ele in model_param))))\nlasso_coef.columns = ['Feature','Coef','mod']","5b1e97ef":"lasso_coef","ec148c57":"#selecting the top 10 variables\nlasso_coef.sort_values(by='mod',ascending=False).head(10)","7e75d2ed":"lasso.coef_","8dae9bf8":"# We have used lasso for building the modle as we got the optimal value of alpha.\nlasso = Lasso(alpha = 0.0001)\nlasso.fit(X_train,y_train)\ny_train_pred = lasso.predict(X_train)\ny_test_pred = lasso.predict(X_test)\n\nprint(r2_score(y_true = y_train,y_pred = y_train_pred))\nprint(r2_score(y_true = y_test,y_pred = y_test_pred))","85a4f66f":"# Model building using optimal alpha\nridge_modified = Ridge(alpha=2.0)\nridge_modified.fit(X_train, y_train)","9d2dc64b":"#creating coeffcients for the ridge regression\nmodel_parameter = list(ridge.coef_)\nmodel_parameter.insert(0,ridge.intercept_)\ncols = house_train.columns\ncols.insert(0,'const')\nridge_coef = pd.DataFrame(list(zip(cols,model_parameter,(abs(ele) for ele in model_parameter))))\nridge_coef.columns = ['Features','Coefficient','Mod']\n#selecting the top 10 variables\nridge_coef.sort_values(by='Mod',ascending=False).head(10)","6e641145":"y_train_pred = ridge_modified.predict(X_train)\ny_test_pred = ridge_modified.predict(X_test)\n\nprint(\"Ridge Regression train r2:\",r2_score(y_true=y_train,y_pred=y_train_pred))\nprint(\"Ridge Regression test r2:\",r2_score(y_true=y_test,y_pred=y_test_pred))","43be3989":"# Model building using optimal alpha\nlasso_modified = Lasso(alpha=0.0002)\nlasso_modified.fit(X_train, y_train)","a240348a":"\ny_train_pred = lasso_modified.predict(X_train)\ny_test_pred = lasso_modified.predict(X_test)\n\nprint(\"Lasso Regression train r2:\",r2_score(y_true=y_train,y_pred=y_train_pred))\nprint(\"Lasso Regression test r2:\",r2_score(y_true=y_test,y_pred=y_test_pred))","8151e48b":"\nmodel_param = list(lasso.coef_)\nmodel_param.insert(0,lasso.intercept_)\ncols = X_train.columns\ncols.insert(0,'const')\nlasso_coef = pd.DataFrame(list(zip(cols,model_param,(abs(ele) for ele in model_param))))\nlasso_coef.columns = ['Feature','Coef','mod']\n","f209465a":"#selecting the top 10 variables\nlasso_coef.sort_values(by='mod',ascending=False).head(10)","d4a6f61e":"X_train_new = X_train.drop(['LotFrontage','BsmtFullBath','OverallCond','CentralAir','OverallQual'],axis=1)\nX_test_new = X_test.drop(['LotFrontage','BsmtFullBath','OverallCond','CentralAir','OverallQual'],axis=1)\n\nX_test_new.head()\nX_train_new.shape","cc24a56e":"X_test_new.shape","30782dc9":"lasso_modified = Lasso()\nparam = {'alpha': [0.0001, 0.001, 0.01]}\nfolds = 5\n# cross validation\nlasso_cv_model_modified = GridSearchCV(estimator = lasso, \n                        param_grid = param, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \n\nlasso_cv_model_modified.fit(X_train_new, y_train)","947e5161":"\n#Creating the results dataframe.\nlasso_cv_modified_results = pd.DataFrame(lasso_cv_model_modified.cv_results_)\n#reading the results\nlasso_cv_modified_results.head()","f6935010":"# plotting mean test and train scores with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n\n# plotting\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\n\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper left')\nplt.show()","cbf26743":"# Checking the best parameter(Alpha value)\nmodel_cv.best_params_","1fbceecc":"# After performing grid search we found the same alpha that ue use before\nlasso = Lasso(alpha=0.0001)\nlasso.fit(X_train_new,y_train)\n\ny_train_pred = lasso.predict(X_train_new)\ny_test_pred = lasso.predict(X_test_new)\n\nprint(\"Lasso Regression train r2:\",r2_score(y_true=y_train,y_pred=y_train_pred))\nprint(\"Lasso Regression test r2:\",r2_score(y_true=y_test,y_pred=y_test_pred))","f5c5beea":"model_param = list(lasso.coef_)\nmodel_param.insert(0,lasso.intercept_)\ncols = X_train_new.columns\ncols.insert(0,'const')\nlasso_coef = pd.DataFrame(list(zip(cols,model_param,(abs(ele) for ele in model_param))))\nlasso_coef.columns = ['Feature','Coef','mod']","5643164f":"#selecting the top 5 variables\nlasso_coef.sort_values(by='mod',ascending=False).head(5)","24f76b54":"After building the model, you realised that the five most important predictor variables in the lasso model are not available in the incoming data. You will now have to create another model excluding the five most important predictor variables. Which are the five most important predictor variables now?","90cafc3c":"##  <u><font color=purple>Data Preprocessing - Tranforming the Target variable(SalePrice)<\/font><\/u> ","2964f46d":"### Numerical columns of our data set","6970ec84":"We are finding the absolute value of coefficient because negative coefficient is also as important as positive one and conveys that the vairbale is negatively related to the target variable Sale price .\" \nFor instance we just find the top 10 coefficents then 0.00 will be more important than a variable with -0.1 coefficent. but in relaility the variable with -0.1 is more important to predict saleprice rather than 0.0 because 0.0 shows that that variable can be excluded and is insignificant where as -0.1 shows that the variable is needed to predict the Sale price. ","9808affa":"##  <u><font color=purple>Data Modeling -Test Train spliting<\/font><\/u> ","9a834cb5":"Tunning it more","0ee105eb":"#### DATA IMPUTATION FOR NUMERICAL COLUMNS ","5a0aaaaa":"Dropping the id as it does not give any infomration","973802f9":"The target variable is right skewed.We need to transform this variable and make it  normally distributed so using log tranformation method","41a140e2":"Regularization is a process used to create an optimally complex model, i.e. a model which is as simple as possible while performing well on the training data. Through regularization, one tries to strike the delicate balance between keeping the model simple, yet not making it too naive to be of any use.","b14c0f8b":"We are chosing to drop these 5 columns which have more than 45% of the missing data.","8c8a7087":"As there are outliers imputing with median","eb168135":"##  <u><font color=purple>Data Preprocessing - Manual Encoding few categorical variables<\/font><\/u> ","1ace2545":"### Bivariate analysis of these top 10 correlated variables with saleprice using pairplot","b2e84045":"### Scaling the numerical variables","5e694c3d":"### Imputing other missing values columns","4bb8bb7a":"![](http:\/\/)##  <u><font color=purple>Data Exploration - Exploratory data analysis<\/font><\/u> ","5d08e634":" ##  <u><font color=purple>Data Understanding<\/font><\/u>","4e12bb01":"##  <u><font color=purple>Data Modeling - Advanced Regression<\/font><\/u> ","01acc52c":"### We see that few variables are highly skewed and there might be possiblity that they can be coralted with each other as well which is This is very difficlut to interpret from univairate analysis. \n\n","b374a25d":"RFE\nRecursive feature elimination","212e508a":"### Lasso Regression model","ca093fe9":"Question 1: Double the alpha values and evaluate model","7207bccf":"## Target variable\n\n\nSalePrice is the variable we need to predict. So let's do some analysis on this variable first.","08ecca43":"Inferences: \nBased on the correlation we can make few following interpretions\n- SalePrice is very high positively correlated to the Overall Quality(0.79)\n- SalePrice is very high positively correlated to the 1stFlrSF (0.61)\n- SalePrice is very high positively correlated to the TotalBsmtSF (0.61)\n- SalePrice is very high positively correlated to the GrLivArea (0.71)\n- SalePrice is very high positively correlated to the FullBath(0.58)\n- SalePrice is very high positively correlated to the TotRmsAbvGrd(0.55)\n- SalePrice is very high positively correlated to the GarageCars(0.64)\n- SalePrice is very high positively correlated to the GarageAre(0.62)\n- SalePrice is very high negatively correlated to the Age(-0.5)","fd175cd6":"##  <u><font color=purple>Data Modeling and evaluation - Final model<\/font><\/u> ","de7a2452":"### The Linear regression model built with 90 columns selected by RFE technique had the accuracy(Adjusted R-squared) of 92.5% and the columns has VIF value of infinite which shows multicollinerity. \n### Though the accuracy the good it has many features and highly collinear.","d5171ab2":"We will make use of Lasso Regression model because it is using less numbers of variables and giving almost the same accuract. Its more efficient model than Ridge regression model","b482bed1":"##  <u><font color=purple>Data Preprocessing - Dummy creation for remaining categorical variables<\/font><\/u> ","13535739":"Inferences : \n- We see that most of the variables are linearly correlated to SalePrice so there is a scope of doing the regresssion model.\n- We even observe that some variables like FullBath, GarageCars are not continuously distributed so we can encode them according to theie characteritics. ","8048c6b8":"There are no outiers so imputing missing values with mean","1c7d20d8":"##  <u><font color=purple>Data Modeling - RFE Technique<\/font><\/u> ","0e088f62":"### Starting with Ridge Regression model","b1333068":"### Categorical columns of our data set ","d0b8357e":"#  <font color=Blue>Advanced Regression<\/font>","5581a49a":"### Building Lasso regression model with optimal alpha value","4894709a":"This graphs shows that alpha value might be ver less than 1","d1a50aaf":"Drooping the first five important predictors","7da231f6":"#### DATA IMPUTATION FOR CATEGORICAL COLUMNS","cc991555":"##  <u><font color=purple>Data Modeling - Scaling<\/font><\/u> ","7bc37af3":"We see that the optimal alpha value is between 0-2","d36f63c9":"We are finding the absolute value of coefficient because negative coefficient is also as important as positive one and conveys that the vairbale is negatively related to the target variable Sale price .\" \nFor instance we just find the top 10 coefficents then 0.00 will be more important than a variable with -0.1 coefficent. but in relaility the variable with -0.1 is more important to predict saleprice rather than 0.0 because 0.0 shows that that variable can be excluded and is insignificant where as -0.1 shows that the variable is needed to predict the Sale price.","815fe085":"##  <u><font color=purple>Subjective Questions<\/font><\/u> ","5a1c687e":"### EDA FOR few categorical columns- the columns which might be useful predicting the sale price(based on my understanding my business understanding and meta data)","3a3b400a":"Question 3: Double the alpha values and evaluate model","c71228e0":"conveerting the columns MSSubClass as categorical as it is a categorical col and not numerical","8909ea47":"#### The above variables are top ten correlated.","a8dd53c6":" For Ridge regression aplha is 1.0 and now doubling it and making it 2.0","13157bb7":"### Univariate analysis of numerical columns","af152a2c":"### Optimal value of alpha:\n1. For Ridge regression :1.0\n2. For Lasso Regression :0.0001","43db2f5a":"\n### Steps followed to build this model\n1. Importing Libraries\n2. Data Understanding\n3. Data Preparation\n4. Data Exploration\n5. Data Preprocessing\n6. Data Modeling \n7. Inference and Recomendation\n  ","e19afff8":"As all the values are almost evenly distributed it would be not right to replace null with mode.So creating a new category names unknown","2896d78a":"### Building ridge regression model with optimal alpha value","d3fe2290":"### All missing values are handled!!!","ad6d1845":"##  <u><font color=purple>Model Inference and Conclusion l<\/font><\/u> ","e5669a5b":"Performing Cross score validation and using different values of alpha and finding the optimal paramater","fb1022ca":"There are four columns which gives us details about the years of the construction. Deriving a new column Age of building and dropping the other columns. we take the remodel date because if the house is remodel it means they have made it better so it would not be right to take original date of built","4a277cc5":"### <p>The company wants to know:\n\n<p>Which variables are significant in predicting the price of a house:\n    \n- LotFrontage :  If the house Linear feet of street connected to property area increase then the Price increase.\n    \n- BsmtFullBath\t: :  If the BsmtFullBath area is more the SalePrice is higher\n    \n- Overall Condition: If the Overall Condition is Excellent the SalePrice is higher\n    \n- MSZoning_RH : If the house i near residential area then the SalePrice is higher\n    \n- Overall quality: If the Overall Condition is Excellent the SalePrice is higher\n    \n- Exterior1st_CBlock : IF the house  Exterior1st is CBlock then price is less.\n    \n- Garage Area: If the Garage area is high the SalePrice is higher\n    \n- CentralAir: If the CentralAir is Yes the SalePrice is higher\n\n   \n<p>How well those variables describe the price of a house.\n    \n- These varibles tells about the linear relation with respect the price by which the company can improve ther sales. For instance they can concentrate in invseting properties wich may have high SalePrice in future\n","8099d80f":" ##  <u><font color=purple>Data Preparation - Missing value treatment <\/font><\/u> ","21b8b190":"## Problem Statement:\n\nA US-based housing company named Surprise Housing has decided to enter the Australian market. The company uses data analytics to purchase houses at a price below their actual values and flip them on at a higher price.The company is looking at prospective properties to buy to enter the market. You are required to build a regression model using regularisation in order to predict the actual value of the prospective properties and decide whether to invest in them or not.\n\n## Approach:\nBuild a regression model using Advance regression methods like Ridge and Lasso regression to predict the price of houses with the available independent variables. This model will then be used by the management to understand how exactly the prices vary with the variables. They can accordingly manipulate the strategy of the firm and concentrate on areas that will yield high returns. Further, the model will be a good way for management to understand the pricing dynamics of a new market. \n\n## Model Objective:\n- Which variables are significant in predicting the price of a house\n- How well those variables describe the price of a house.\n- Also, determine the optimal value of lambda for ridge and lasso regression.\n\n## The tasks performed in the model presented below are:\n- Importing and understading the data.\n- Miising value treatment and outlier analysis\n- Exploratory data analysis to find out the inference about the data and its correlation with the target variables.\n- Tranformation of the target variable to handle the data skewness.\n- Data preprocession like Label encoding and ceration of dummies.\n- Test train split and Feature scaling\n- Data modelling using RFE to identify the top 30 variables.\n- Ridge and Lasso Regression to find the top feature variables and finding the optimal alpha value\n","83e02694":"We se that alpha is somewhere between 0- 20 So sleting parameters according to that.","1a29cd5c":"For Lasso regression alpha is 0.0001 and not doubling it and making it 0.0002","61c332b4":" ## <u><font color=purple>Importing Libraries<\/font><\/u>","695dcaea":"Inferences - \n- MsZoning with of type 'Fv' has high Saleprice and type 'C' has least sale price\n- The Street of type 'Pave' has more Sale Price when compared to 'Grvl'\n- the utlities coulms have most of its values as 'AllPub' So we this column have give much of an informration. - ITs not an important feature.\n- The house with Exterior Quality of type Execellent has the highest SalePrice.\n- The house with Basement Quality of type Execellent has the highest SalePrice.\n- The house with Kitchen Quality of type Execellent has the highest SalePrice.\n- The house with Garage Quality of type Execellent has the highest SalePrice.\n- The house with Garage Type of type BuiltIn has the highest SalePrice.\n- The house with Sale Type of type New has the highest SalePrice.\n- The Central air of type 'Yes' has more Sale Price when compared to 'No'\n"}}