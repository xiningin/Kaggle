{"cell_type":{"4d94f92a":"code","86327125":"code","df7fcedb":"code","e287cb58":"code","2807b339":"code","f50d8dc0":"code","ce583cb1":"code","bcd8a882":"code","e152c235":"code","c9ef634b":"code","944c3371":"code","4ea3b1ac":"code","68537c33":"code","93622e3d":"code","97165983":"code","30e2a541":"code","5454d948":"code","5833f956":"code","60e039f2":"code","f5f3dba6":"code","e3adc6ef":"code","06c26833":"code","293af50a":"code","621e6f6f":"code","380719ed":"code","061f7008":"code","bc7ef7d1":"code","8e480765":"code","b4e9ddd5":"code","9c88d411":"code","d3609b56":"code","87f79744":"code","0cfe3a9f":"code","c5d0a3ed":"code","028c3714":"code","5064a0ac":"code","7fe463aa":"code","2e035ea0":"code","e75944f7":"code","17f877d2":"code","850dd006":"code","a196507d":"code","2b9f40d3":"code","7620ae8a":"code","e87bee27":"code","20820b1c":"code","94d5d893":"code","f39aca04":"code","7c2293df":"markdown","8eec8c3c":"markdown","349eacfe":"markdown","543a46ac":"markdown","049e89b0":"markdown","470b7b19":"markdown","abf6655c":"markdown","c31e95b2":"markdown"},"source":{"4d94f92a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nimport os\ncsv_files = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        csv_file = os.path.join(dirname, filename)\n        print(os.path.join(dirname, filename))\n        csv_files.append(csv_file)\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","86327125":"import tensorflow as tf","df7fcedb":"tf.__version__","e287cb58":"!python3 -V","2807b339":"from zipfile import ZipFile\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import classification_report, multilabel_confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix, ConfusionMatrixDisplay\nfrom tensorflow.keras import Sequential, Model, Input\nfrom tensorflow.keras.layers import LSTM, Dense\nfrom tensorflow.keras.utils import get_file\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib\nsns.set_style(\"white\")","f50d8dc0":"df_train = pd.read_csv('..\/input\/nslkdd\/KDDTrain+.txt',delimiter=',', header=None)\ndf_test = pd.read_csv('..\/input\/nslkdd\/KDDTest+.txt',delimiter=',', header=None)","ce583cb1":"# The CSV file has no column heads, so add them\ndf_train.columns = [\n    'duration',\n    'protocol_type',\n    'service',\n    'flag',\n    'src_bytes',\n    'dst_bytes',\n    'land',\n    'wrong_fragment',\n    'urgent',\n    'hot',\n    'num_failed_logins',\n    'logged_in',\n    'num_compromised',\n    'root_shell',\n    'su_attempted',\n    'num_root',\n    'num_file_creations',\n    'num_shells',\n    'num_access_files',\n    'num_outbound_cmds',\n    'is_host_login',\n    'is_guest_login',\n    'count',\n    'srv_count',\n    'serror_rate',\n    'srv_serror_rate',\n    'rerror_rate',\n    'srv_rerror_rate',\n    'same_srv_rate',\n    'diff_srv_rate',\n    'srv_diff_host_rate',\n    'dst_host_count',\n    'dst_host_srv_count',\n    'dst_host_same_srv_rate',\n    'dst_host_diff_srv_rate',\n    'dst_host_same_src_port_rate',\n    'dst_host_srv_diff_host_rate',\n    'dst_host_serror_rate',\n    'dst_host_srv_serror_rate',\n    'dst_host_rerror_rate',\n    'dst_host_srv_rerror_rate',\n    'outcome',\n    'difficulty'\n]\ndf_test.columns = df_train.columns","bcd8a882":"print(f\"df_train.shape = {df_train.shape}\")\nprint(f\"df_test.shape = {df_test.shape}\")","e152c235":"df_train.head()","c9ef634b":"df_test.head()","944c3371":"df_train.describe()","4ea3b1ac":"print(f\"numbers of protocol_type: {len(df_train['protocol_type'].value_counts())}\")\nprint(f\"number of 'service' in train datasets: {len(df_train.service.value_counts())}\")\nprint(f\"number of 'flag' in train datasets: {len(df_train['flag'].value_counts())}\")","68537c33":"new_df = pd.concat([df_test['service'].value_counts(), df_train['service'].value_counts()], axis=1)\n# new_df.rename(columns={'service':'KDDTest+', 'service':'KDDTrain+'}, inplace=True)\nnew_df.columns=['KDDTest+', 'KDDTrain+']","93622e3d":"new_df.sort_values(by='KDDTest+', ascending=False).head(10)","97165983":"new_df.sort_values(by='KDDTest+', ascending=False).head(10)","30e2a541":"ax = pd.concat([df_test['service'].value_counts().head(10),df_train['service'].value_counts().head(10)],axis=1).plot(kind='bar')\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nplt.legend(['KDDTest+', 'KDDTrain+'])\nplt.show()","5454d948":"test_top10 = df_test['service'].value_counts().head(10)\ntrain_top10 = df_train['service'].value_counts().head(10)","5833f956":"fig, ax = plt.subplots()\nplt.rcParams['axes.unicode_minus']=False\nplt.barh(train_top10.index, -train_top10)\nplt.barh(test_top10.index, test_top10)\n\n# df_train['service'].value_counts().head(10).plot(kind='barh', ax=ax1)\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nplt.legend(['KDDTrain+','KDDTest+'])\nplt.show()","60e039f2":"plt.figure(figsize=(18, 5))\nplt.subplot(131)\ndf_train[\"protocol_type\"].value_counts().plot(kind='bar', label='protocol type')\nplt.legend()\nplt.subplot(132)\ndf_train['service'].value_counts().head(10).plot(kind='bar')\nplt.legend()\nplt.subplot(133)\ndf_train[\"flag\"].value_counts().plot(kind='bar')\nplt.legend()\nplt.show()","f5f3dba6":"df_train.outcome.unique()\n# df_train['label'] = np.where(df_train['outcome'].str.contains('normal'), 0, 1)\n# df_test['label'] = np.where(df_test['outcome'].str.contains('normal'), 0, 1)","e3adc6ef":"df_test.outcome.unique()","06c26833":"class_DoS = ['apache2', 'back', 'land', 'neptune', 'mailbomb', 'pod', \n             'processtable', 'smurf', 'teardrop', 'udpstorm', 'worm']\nclass_Probe = ['ipsweep', 'mscan', 'nmap', 'portsweep', 'saint', 'satan']\n\nclass_U2R = ['buffer_overflow', 'loadmodule', 'perl', 'ps', 'rootkit', 'sqlattack', 'xterm']\n\nclass_R2L = ['ftp_write', 'guess_passwd', 'httptunnel',  'imap', 'multihop', 'named', \n             'phf', 'sendmail', 'snmpgetattack', 'spy', 'snmpguess', 'warezclient', \n             'warezmaster', 'xlock', 'xsnoop']\n\n","293af50a":"df_train['class'] = df_train['outcome']\ndf_train['class'].replace(class_DoS, value='DoS', inplace=True)\ndf_train['class'].replace(class_Probe, value='Probe',inplace=True)\ndf_train['class'].replace(class_U2R, value='U2R',inplace=True)\ndf_train['class'].replace(class_R2L, value='R2L', inplace=True)\nprint(df_train['class'].unique())","621e6f6f":"df_train.head(2)","380719ed":"df_test['class'] = df_test['outcome']\ndf_test['class'].replace(class_DoS, value='DoS', inplace=True)\ndf_test['class'].replace(class_Probe, value='Probe',inplace=True)\ndf_test['class'].replace(class_U2R, value='U2R',inplace=True)\ndf_test['class'].replace(class_R2L, value='R2L', inplace=True)\nlabels = df_test['class'].unique()\nprint(labels)","061f7008":"### add by joshua####\n'''convert nominal label to numerical ones '''\nlabels={\"normal\": 0, \"DoS\": 1, \"Probe\": 2, \"U2R\": 3, \"R2L\":4}\ndf_train['label'] = df_train['class'].replace(labels)\ndf_test['label'] = df_test['class'].replace(labels)\ndf_train['label'].nunique()","bc7ef7d1":"pie, ax = plt.subplots(figsize=[10,6])\nclass_data = df_train['class'].value_counts().sample(frac=1.0)\nprint(class_data)\n\nax.pie(x=class_data, labels=class_data.keys(), explode=[0.07]*5, pctdistance=0.4, autopct=\"%.1f%%\")\nax.set_title(\"Attack Types in NSL-KDD Training set\", fontdict={'fontsize': 14})\nplt.show()","8e480765":"sns.countplot(x=df_test['class'])\nplt.show()","b4e9ddd5":"df_train = df_train[df_train['class'].isin(['DoS', 'Probe', 'R2L', 'normal'])]\ndf_test = df_test[df_test['class'].isin(['DoS', 'Probe', 'R2L', 'normal'])]\nlabel_num = len(df_test['class'].unique())\nlabel_num","9c88d411":"df_train_obj = df_train.iloc[:, :-4].select_dtypes(include='object')\ndf_train_num = df_train.iloc[:, :-4].select_dtypes(exclude='object')\n\nprint(f\"shape of numeric features: {df_train_num.shape}\")\nprint(f\"shape of object features: {df_train_obj.shape}\")","d3609b56":"# outlier_fence_95 = np.percentile(df_train_num, 95, axis=0)\n# df_train_num_new = df_train_num.copy()\n# for index, fence in enumerate(outlier_fence_95):\n#     boolarr = df_train_num_new.iloc[:,index] <= fence\n#     df_train_num_new = df_train_num_new[boolarr]\n# df_train_num_new.shape\n# print(f\"shape of df_train_num_new: {df_train_num_new.shape}\")","87f79744":"df_test_obj = df_test.iloc[:, :-4].select_dtypes(include='object')\ndf_test_num = df_test.iloc[:, :-4].select_dtypes(exclude='object')\n\nprint(f\"shape of numeric features: {df_test_num.shape}\")\nprint(f\"shape of object features: {df_test_obj.shape}\")","0cfe3a9f":"from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\nenc = OneHotEncoder(handle_unknown='ignore')\n# df_train_obj = df_train.iloc[:,:-3]\ndf_train_enc = enc.fit_transform(df_train_obj).toarray()\ntrain_enc_features = enc.get_feature_names(input_features=df_train_obj.columns)\ndf_test_enc = enc.transform(df_test_obj).toarray()\ntest_enc_features = enc.get_feature_names(input_features=df_test_obj.columns)\n# print(len(train_enc_features), len(test_enc_features))\nX_train_enc = np.c_[df_train_num, df_train_enc]\nX_test_enc = np.c_[df_test_num, df_test_enc]\nprint(f\"X_train_enc shape: {X_train_enc.shape}\")\nprint(f\"X_test_enc shape: {X_test_enc.shape}\")","c5d0a3ed":"scaler = MinMaxScaler()\nX_train_scaler = scaler.fit_transform(X_train_enc)\nX_test_scaler = scaler.transform(X_test_enc)\nX_train_normal = X_train_scaler[df_train['class']=='normal']\nX_train_DoS = X_train_scaler[df_train['class']=='DoS']\nX_train_Probe = X_train_scaler[df_train['class']=='Probe']\n#X_train_U2R = X_train_scaler[df_train['class']=='U2R']\nX_train_R2L = X_train_scaler[df_train['class']=='R2L']\n\nX_test_normal = X_test_scaler[df_test['class']=='normal']\nX_test_DoS = X_test_scaler[df_test['class']=='DoS']\nX_test_Probe = X_test_scaler[df_test['class']=='Probe']\n#X_test_U2R = X_test_scaler[df_test['class']=='U2R']\nX_test_R2L = X_test_scaler[df_test['class']=='R2L']","028c3714":"print(\"Normal samples in Training dataset:\", X_train_normal.shape)\nprint(f\"DoS samples in Training dataset: {X_train_DoS.shape}\")\nprint(f\"Probe samples in Training dataset: {X_train_Probe.shape}\")\nprint(f\"R2L samples in Training dataset: {X_train_R2L.shape}\")\n#print(f\"U2R samples in Training dataset: {X_train_U2R.shape}\")","5064a0ac":"print(\"Normal samples in Test dataset:\", X_test_normal.shape)\nprint(f\"DoS samples in Test dataset: {X_test_DoS.shape}\")\nprint(f\"Probe samples in Test dataset: {X_test_Probe.shape}\")\nprint(f\"R2L samples in Test dataset: {X_test_R2L.shape}\")\n#print(f\"U2R samples in Test dataset: {X_test_U2R.shape}\")","7fe463aa":"### training an Autoencoder model with all dataset X_train\n# X_train = X_train_scaler\n# from keras.callbacks import EarlyStopping\n# callback = EarlyStopping(patience=3, mode='min')\n# def feature_extraction(X_train):  \n#     input_shape = X_train.shape[1]\n#     encoding_dim = 20\n#     encoder = Sequential([\n#         Dense(32, activation='relu', input_shape=(input_shape, )),\n#         Dense(encoding_dim, activation='relu')\n#     ])\n#     decoder = Sequential([\n#         Dense(32, activation='relu', input_shape=(encoding_dim,)),\n#         Dense(input_shape)\n#     ])\n\n#     autoencoder = Sequential([encoder, decoder])\n#     autoencoder.compile(loss='mse', optimizer='Adam')\n#     history = autoencoder.fit(X_train, X_train, \\\n#                               epochs=50, validation_split=0.2, callbacks=[callback])\n    \n#     return encoder\n\n# encoder = feature_extraction(X_train_scaler)","2e035ea0":"### get extract latent representation of X_train samples\n### the new samples will be generated by encoder alone and used for muliple classification\n### The test dataset should also be transformed in the same way as the training set\n# X_train_extract = encoder.predict(X_train_scaler)\n# X_test_extract = encoder.predict(X_test_scaler)\nX_train_extract = X_train_scaler.copy()\nX_test_extract = X_test_scaler.copy()\n\n### deeplearning model only accept numbers as labels, we need to transform categorical labels to numerical ones\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny_train_extract = le.fit_transform(df_train['class'])\ny_test_extract = le.transform(df_test['class'])\n\n# from sklearn.preprocessing import LabelEncoder\n# le = LabelEncoder()\n# le.fit_transform(df_test['class'])","e75944f7":"look_back = X_train_extract.shape[1]\ndef multiClassModel(units=4, look_back=look_back):\n    model = Sequential()\n    model.add(Input(shape=(1, look_back)))\n    model.add(LSTM(units=50))\n    #model.add(Dropout(0.3))\n    model.add(Dense(label_num, activation=\"softmax\"))\n    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='Adam')\n    model.summary()\n    return model","17f877d2":"model = multiClassModel()\nX_train_lstm = X_train_extract.reshape(X_train_extract.shape[0], 1, X_train_extract.shape[1])\nX_test_lstm = X_test_extract.reshape(X_test_extract.shape[0], 1, X_test_extract.shape[1])\nhistory = model.fit(X_train_lstm, y_train_extract, \\\n                   epochs=30, batch_size=64, validation_split=0.2)\n### check the loss trend of epochs\npd.DataFrame(history.history).plot(kind='line', xlabel='epochs', figsize=(8, 6))\nplt.show()","850dd006":"y_pred_extract = model.predict(X_test_lstm)","a196507d":"y_pred = np.argmax(y_pred_extract, axis=1)","2b9f40d3":"def multilabel_matrix(y_true, y_pred, labels=None):\n    mlm = multilabel_confusion_matrix(y_true, y_pred, labels=None)\n    df_performance = pd.DataFrame(index=labels, columns=['accuracy', 'precision', 'recall', 'f1_score'])\n    for i, label in enumerate(labels):\n        tn, fp, fn, tp = mlm[i].ravel()\n        accuracy = (tn + tp) \/ (tn + fp + fn + tp)\n        precision = tp \/ (tp + fp)\n        recall = tp \/ (tp + fn)\n        f1_score = 2*precision * recall \/ (precision + recall)\n        df_performance.loc[label] = [round(accuracy, 4), round(precision,4), \\\n                                     round(recall, 4), round(f1_score,4)]\n    return df_performance","7620ae8a":"y_pred = le.inverse_transform(y_pred)\ny_true = df_test['class']\nprint(classification_report(y_pred, y_true))","e87bee27":"performance = multilabel_matrix(y_true, y_pred, labels=le.classes_)\nperformance","20820b1c":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\ncm = confusion_matrix(y_true, y_pred)\nfig, ax = plt.subplots(figsize=(8, 8))\nConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_).plot(ax=ax)\nplt.title('Confusion Marix of proposed AE-LSTM classifier model')\nplt.show()\n# plt.savefig(\"confusion_matrix.png\", dpi=500, bbox_inches='tight')","94d5d893":"# ROC curve\uff1a\n\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import roc_auc_score\nfrom itertools import cycle\ndef RoC_Curve(y_score, y, labels, title): \n    y_cat = to_categorical(y)\n\n        \n    # Compute ROC curve and ROC area for each class\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    lw = 2\n    # First aggregate all false positive rates\n    n_classes = len(labels)\n    print('n_classes:', n_classes)\n\n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve(y_cat[:, i], y_score[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n\n        \n        \n        \n    # Compute micro-average ROC curve and ROC area\n    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_cat.ravel(), y_score.ravel())\n    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n    # First aggregate all false positive rates\n    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n    # Then interpolate all ROC curves at this points\n    mean_tpr = np.zeros_like(all_fpr)\n    for i in range(n_classes):\n        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n\n        \n        \n        \n    # Finally average it and compute AUC\n    mean_tpr \/= n_classes\n\n    fpr[\"macro\"] = all_fpr\n    tpr[\"macro\"] = mean_tpr\n    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n    # Plot all ROC curves\n    plt.figure(figsize=(8,8))\n    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n             label='micro-average ROC curve (area = {0:0.4f})'\n                   ''.format(roc_auc[\"micro\"]),\n             color='deeppink', linestyle=':', linewidth=4)\n\n    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n             label='macro-average ROC curve (area = {0:0.4f})'\n                   ''.format(roc_auc[\"macro\"]),\n             color='navy', linestyle=':', linewidth=4)\n\n    colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n    for i, color in zip(range(n_classes), colors):\n        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n                 label=f'ROC curve of class {labels[i]} (area = {roc_auc[i]:0.4f})')\n\n    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(title, fontsize=16)\n    plt.legend(loc=\"lower right\")\n    plt.show()","f39aca04":"RoC_Curve(y_pred_extract, y_test_extract, le.classes_, title='ROC for LSTM classifier')","7c2293df":"Delete### Outlier remove in training set using 95 percentile rule","8eec8c3c":"## Data preprocessing","349eacfe":"## labels aggregation","543a46ac":"### OneHotEncoder and MinMaxScaler","049e89b0":"## Performance Report","470b7b19":"## Feature extraction with Autoencoder","abf6655c":"## Data mining and exploration","c31e95b2":"## Confusion Matrix"}}