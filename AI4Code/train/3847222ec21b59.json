{"cell_type":{"e1f59ab9":"code","788d5425":"code","233a9598":"code","f3298659":"code","7ec88d09":"code","fcf6068b":"code","e1e9f974":"code","4faf7682":"code","d3671e9d":"code","b497c5c2":"code","3b961850":"code","842bb871":"code","df8a588e":"code","0b6d29a5":"code","e3e9dbcf":"code","9225b177":"code","e9fd7537":"code","8403bd81":"code","609189b5":"code","de503785":"code","2ea79e23":"code","0397185d":"code","6f868d90":"markdown","c5f8b1b1":"markdown","5175a5de":"markdown","0c2dcecb":"markdown","64de7689":"markdown","c72e6068":"markdown","baf7ff17":"markdown","04d99e14":"markdown","f7008e1a":"markdown","f5f4a4ba":"markdown","aa13b2a2":"markdown","684b2492":"markdown","9a7d2039":"markdown","943fc32c":"markdown","6cb0b851":"markdown","d7be2a8e":"markdown","fa7ba326":"markdown","3d9715f0":"markdown","88a663c0":"markdown","57a19365":"markdown"},"source":{"e1f59ab9":"#%% Get the data\nimport pandas as pd\n\ndm = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")","788d5425":"#%% Discover and visualize the data to gain information\ndm.head()","233a9598":"dm.info()\ncols = list(dm.columns)\ndm.describe()","f3298659":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib import cycler\ncolors = cycler('color',\n                ['#EE6666', '#3388BB', '#9988DD',\n                 '#EECC55', '#88BB44', '#FFBBBB'])\nplt.rc('axes', facecolor ='#E6E6E6', edgecolor = 'none', axisbelow = True, grid = True, prop_cycle = colors)\nplt.rc('grid', color = 'w', linestyle = 'solid')\nplt.rc('xtick', direction = 'out', color = 'black')\nplt.rc('ytick', direction = 'out', color = 'black')\nplt.rc('patch', edgecolor = '#E6E6E6')\nplt.rc('lines', linewidth = 2)\nplt.rcParams['figure.figsize'] = (11, 7)","7ec88d09":"print(dm[\"Class\"].value_counts())\n\nplt.bar([0, 1], dm[\"Class\"].value_counts())\nplt.ylabel(\"Number of observations\")\nplt.title(\"Class\")\nplt.xticks([0, 1])\nplt.legend()\nplt.show()","fcf6068b":"plt.rcParams.update({'font.size': 18, 'figure.figsize': (44, 28)})\nfor l in range(len(dm.iloc[0, :])):\n    plt.subplot(6, 6, l + 1)\n    plt.hist(dm.iloc[:, l].values, bins = 50)\n    plt.title(cols[l])\nplt.show()","e1e9f974":"# Separate class 1\nplt.rcParams.update({'font.size': 12, 'figure.figsize': (11, 7)})\n\ndm_C1 = dm.drop(dm[dm[\"Class\"] == 0].index)\n\n# Obtain module of each row (feature vector) for visualizate class\n# dm_C1 behavior\nmods = []\nfor i in range(len(dm_C1)):\n    sum2 = 0\n    for j in range(1, len(dm_C1.iloc[i, :]) - 2):\n        sum2 = sum2 + dm_C1.iloc[i, j]**2\n    mods.append(np.sqrt(sum2))\n    \nmods = pd.Series(mods)\nmods.plot()\nplt.title(\"Class 1\")\nplt.ylabel(\"Module of feature vector\")\nplt.xlabel(\"Number of feature vector\")\nplt.show();","4faf7682":"# Looking for Correlations\nimport seaborn as sns\nsns.set(font_scale = 1)\n\ncorr_matrix = dm.corr()\nsns.heatmap(corr_matrix,\n            xticklabels = corr_matrix.columns.values,\n            yticklabels = corr_matrix.columns.values)\ncorr_matrix[\"Class\"].sort_values(ascending = False)","d3671e9d":"def attributeCombinationsCorr(df, colNameCorr, corrValLim):\n    \n    dataf = df.copy()\n    cols = list(df.columns)\n    types = [np.float64, np.int64]\n    for j in range(len(df.iloc[0, :]) - 1):\n        if type(df.iloc[0, j]) not in types:\n            continue\n        for jn in range(j + 1, len(df.iloc[0, :])):\n            if type(df.iloc[0, jn]) not in types:\n                continue\n            dataf[cols[j] + cols[jn]] = df[cols[j]]*df[cols[jn]]\n            if (len(df.iloc[0, :]) - j) == 2:\n                break\n    corr_matrix = dataf.corr()\n    corrValues = corr_matrix[colNameCorr].sort_values(ascending = False)\n    print(corrValues)\n    \n    corrIndexNames = list(corrValues.index.values)\n    colsIndex = []\n    for i in range(len(corrValues)):\n        if corrValues[i] > corrValLim or corrValues[i] < -corrValLim:\n            colsIndex.append(corrIndexNames[i])\n    dfN = dataf[colsIndex]\n    \n    return dfN, corrValues","b497c5c2":"# Using a function to combine attribute and increment its correlation\n\ncolNameCorr, corrValLim = \"Class\", 0.5\ndfCorr, corrValues = attributeCombinationsCorr(dm, colNameCorr, corrValLim)\n\ndfCorr = dfCorr.drop([name for name in list(dfCorr.columns)\n                      if \"Class\" in name], axis = 1)\ndfCorr = pd.concat([dfCorr, dm[\"Class\"]], axis = 1)\n\ncorr_dfCorr = dfCorr.corr()\nsns.heatmap(corr_dfCorr,\n            xticklabels = corr_dfCorr.columns.values,\n            yticklabels = corr_dfCorr.columns.values)\ncorr_dfCorr[\"Class\"].sort_values(ascending = False)","3b961850":"dfCorr.head()","842bb871":"dfCorr.describe()","df8a588e":"#%% Prepare the data for Machine Learning algorithms.\n\nnp.random.seed(42)\ndef split_train_test(data, external_ratio):\n    random_indices = np.random.permutation(len(data))\n    external_set = int(len(data) * external_ratio)\n    external_indices = random_indices[:external_set]\n    analysis_set = random_indices[external_set:]\n    return data.iloc[analysis_set], data.iloc[external_indices]\n\nanalysis_set, external_data = split_train_test(dfCorr, 0.1)\n\n# Create a test and train sets of analysis_set\ntrain, test = split_train_test(analysis_set, 0.2)\n\nX_train, y_train = train.drop([\"Class\"], axis = 1), train[\"Class\"]\nX_test, y_test = test.drop([\"Class\"], axis = 1), test[\"Class\"]\n\nprint(\"Train set \\n\", train[\"Class\"].value_counts())\nprint(\"Test set \\n\", test[\"Class\"].value_counts())\nprint(\"External set \\n\", external_data[\"Class\"].value_counts())\n","0b6d29a5":"#%% 5. Select a model and train it.\n\n# Training a linear logistic regression model\nfrom sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression(class_weight = \"balanced\")\nlog_reg.fit(X_train, y_train)\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\n\n# To the evaluation of the Linear Logistic Regression model\nfraud_pred = log_reg.predict(X_train)\nprint(\"Train set \\n\", confusion_matrix(y_train, fraud_pred))\n\nfrom sklearn.metrics import classification_report\n\ntarget_names = ['class 0', 'class 1']\nprint(classification_report(y_train, fraud_pred, target_names = target_names, digits = 3))\n\nclass_names = [Class for Class in dm.Class.unique()]\nplot_confusion_matrix(log_reg, X_train, y_train, display_labels = class_names, cmap = plt.cm.Reds)  \nplt.show() ","e3e9dbcf":"# Predictions test\ny_pred = log_reg.predict(X_test)\n\nprint(\"Test set \\n\", confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred, target_names = target_names, digits = 3))\n\nplot_confusion_matrix(log_reg, X_test, y_test, display_labels = class_names, cmap = plt.cm.Reds)\nplt.show() ","9225b177":"# Predictions external_data\noutside_X = external_data.drop([\"Class\"], axis = 1)\noutside_y = external_data[\"Class\"]\ny_pred_test = log_reg.predict(outside_X)\nprint(\"External set \\n\",confusion_matrix(outside_y, y_pred_test))\nprint(classification_report(outside_y, y_pred_test, target_names = target_names, digits = 3))\n\nplot_confusion_matrix(log_reg, outside_X, outside_y, display_labels = class_names, cmap = plt.cm.Reds)\nplt.show()","e9fd7537":"# Train a Decision Tree Classifier\nfrom sklearn.tree import DecisionTreeClassifier\n\ntree_Cla = DecisionTreeClassifier(class_weight = \"balanced\")\ntree_Cla.fit(X_train, y_train)\n\nfraud_pred = tree_Cla.predict(X_train)\n\nprint(\"Train set \\n\", confusion_matrix(y_train, fraud_pred))\nprint(classification_report(y_train, fraud_pred, target_names = target_names, digits = 3))\n\nplot_confusion_matrix(tree_Cla, X_train, y_train, display_labels = class_names, cmap = plt.cm.Reds)  \nplt.show()","8403bd81":"# Predictions test\ny_pred = tree_Cla.predict(X_test)\n\nprint(\"Test set \\n\", confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred, target_names = target_names, digits = 3))\n\nplot_confusion_matrix(tree_Cla, X_test, y_test, display_labels = class_names, cmap = plt.cm.Reds)  \nplt.show()","609189b5":"# Predictions external_data\noutside_X = external_data.drop([\"Class\"], axis = 1)\noutside_y = external_data[\"Class\"]\ny_pred_test = tree_Cla.predict(outside_X)\n\nprint(\"External set \\n\", confusion_matrix(outside_y, y_pred_test))\nprint(classification_report(outside_y, y_pred_test, target_names = target_names, digits = 3))\n\nplot_confusion_matrix(tree_Cla, outside_X, outside_y, display_labels = class_names, cmap = plt.cm.Reds)  \nplt.show()","de503785":"from sklearn.ensemble import RandomForestClassifier\n\nforest_reg = RandomForestClassifier(class_weight = \"balanced\")\nforest_reg.fit(X_train, y_train)\n\nfraud_pred = forest_reg.predict(X_train)\n\nprint(\"Train set \\n\", confusion_matrix(y_train, fraud_pred))\nprint(classification_report(y_train, fraud_pred, target_names = target_names, digits = 3))\n\nplot_confusion_matrix(forest_reg, X_train, y_train, display_labels = class_names, cmap = plt.cm.Reds)  \nplt.show()","2ea79e23":"# Predictions test\ny_pred = forest_reg.predict(X_test)\n\nprint(\"Test set \\n\", confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred, target_names = target_names, digits = 3))\n\nplot_confusion_matrix(forest_reg, X_test, y_test, display_labels = class_names, cmap = plt.cm.Reds)  \nplt.show()","0397185d":"# Predictions external_data\noutside_X = external_data.drop([\"Class\"], axis = 1)\noutside_y = external_data[\"Class\"]\ny_pred_test = forest_reg.predict(outside_X)\n\nprint(\"External set \\n\", confusion_matrix(outside_y, y_pred_test))\nprint(classification_report(outside_y, y_pred_test, target_names = target_names, digits = 3))\n\nplot_confusion_matrix(forest_reg, outside_X, outside_y, display_labels = class_names, cmap = plt.cm.Reds)  \nplt.show()","6f868d90":"**Training a Linear Logistic Regression Model**","c5f8b1b1":"**Training a Decision Tree Classifier Model**","5175a5de":"The same thing happens here as in the case of the decision tree.","0c2dcecb":"As only class 1 was separated, there are 492 feature vectors and, therefore, 492 modules. The modulus of a vector is the Euclidean norm.\n\n**What can the vector modulus calculation work for?**\n\nAs we can see, it works to transform the exogenous variables (in this case only the V's attributes) into a series and, therefore, interesting patterns can be discovered. This also has its advantages because time analysis or neural network tools can then be applied to predict how many fraud there will be in the future! :). The downside at this point is that you have little data to apply a decent analysis. But the idea is what matters!\n\nLet's now look at some correlations:","64de7689":"It is observed in the classification report that the recall has a value of 0.75 in the test set for fraud detection.","c72e6068":"There are no strong linear correlations, it may be that there are other types of correlations. However, we will apply a classic data transformation: multiply some attributes and see if there is a stronger linear relationship.\n\nThe function from to under allows you to multiply the columns of a data frame.","baf7ff17":"In the latter, there was a larger decrease in the recall value compared to that obtained in the fraud detection test set.","04d99e14":"**Conclusion**\n\nIn this notebook a comparison of classification algorithms (Linear Logistic Regression, Decision Tree and Random Forest) was presented.\n* A function was created to carry out binary combinations (multiplication of attributes) by returning a dataframe with the combinations that meet the required value of correlation interest. In this case, those combinations that present a Pearson correlation greater than 0.5 were sealed.\n* From the principal components, binary combinations were carried out to improve the linear correlation with respect to the Class attribute.\n* Of the three classification models presented, the logistic regression model presents better results, secondly the random forest model and, lastly, the decision tree model.\n* In the logistic regression, a recall for fraud detection of 0.88 is obtained with the test set; a 0.89 recall with the outer set, and a 0.90 recall on training.\n\n\nI look forward to your feedback to keep improving in data science! :)","f7008e1a":"**Let's move on to preparing the data for Machine Learning algorithms**\n\nWe have a new set of attributes with better linear correlation with respect to the Class attribute. In the next part a quantity of data is randomly extracted which is called \"external_set\". This data is never with the training set nor is it part of the test set. They are \"new data\" to verify the effectiveness of the algorithms. The training and test sets are obtained from the remaining set called \"analysis_set\".\n\nThree classification algorithms are presented: Logistic Regression (LR), Decision Tree Classifier (DTC), and Random Forest Classifier (RFC). ","f5f4a4ba":"In these five rows, based on the context of the problem, we can see that there are 28 main components (the V's), a time column (Time), a quantity attribute (Amount), which refers to the amount of the transaction and the column to predict (Class).\n\n**Let's look at the information and description of this data set**","aa13b2a2":"Again, supposing that we take the logistic regression model as \"good\" and assign external data to it, it is observed that the recall presented for fraud detection is 0.89, which is very good. Only 5 errors had the model out of a total of 46 frauds.","684b2492":"The dataset does not contain empty values and, except for the Class column, which is of type integer, all other attributes are of type float. It is observed that the principal components have mean 0 and different standard deviations.\n\nLet's now examine the Class variable:","9a7d2039":"There is only 0.17% fraud. The imbalance is notoriously high!\n\nNow let's take a look at the graphs of all the attributes in the dataset.","943fc32c":"Focusing on fraud (most important for these problems) the present model gives a 0.88 recall with the test set.","6cb0b851":"Linear correlations increased! It should be mentioned that multiplication with decimal numbers tends to decrease values. For this reason, the correlation with respect to the Class attribute increases. If we carry out multiplication with three attributes, the results will continue to decrease, increasing the correlation. That is why only binary multiplication was chosen.","d7be2a8e":"In these graphs the values of the V's are centered on 0. On the other hand, some attributes follow a Gaussian distribution, if not all, since some attributes contain most of their values close to 0, so that their graph tends to be almost a line parallel to the y axis. Finally, the Amount graph, it seems that its values are centered at 0, but it is not. Rather, there are small quantities, but due to the scale, it is seen that they were 0, and there are quantities, counted with the fingers, exuberant. Remember that in this last attribute the minimum value is 88.34 and the maximum value of 25691.16, this last value is the consequence of the visualization.","fa7ba326":"In this case, the data is overfitted in the model, that is, the machine learns in depth the behavior of the data, producing a perfect fit. However, since we have the test set and the outer set, it will be seen that the model does not perform good calculations.","3d9715f0":"Out of curiosity, let's divide all the feature vectors corresponding to class 1 and calculate the modulus.","88a663c0":"In this test set, the recall (for fraud) is greater than that obtained in the decision tree model but less than that obtained in the logistic regression.","57a19365":"The trained logistic regression model presents a recall value slightly higher 0.90 for fraud detection."}}