{"cell_type":{"bd179338":"code","f93a3f87":"code","395e5dee":"code","d4eb86df":"code","9a9ed359":"code","77ea8de9":"code","e91f164f":"code","2a1df00b":"code","040eb5f1":"code","57f2a578":"code","67fe19e2":"code","39328a80":"code","00da0e62":"code","f3a80e95":"markdown","8983e2be":"markdown","88316931":"markdown","894afb71":"markdown","e1a57a29":"markdown","4e08328a":"markdown","d8996d98":"markdown","7717c534":"markdown","35adea80":"markdown","c006198e":"markdown"},"source":{"bd179338":"## This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.linear_model import LogisticRegression # Linear Models\nfrom sklearn.svm import SVC # Support Vector Machine\nfrom sklearn.ensemble import RandomForestClassifier # Random Forest Classifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV # splitting training and testing data\nfrom sklearn.tree import DecisionTreeClassifier # decision tree classification\nfrom sklearn.neighbors import KNeighborsClassifier # N Nearest Neighbors\nfrom sklearn.metrics import accuracy_score\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nseed = 42\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f93a3f87":"# importing datasets from kaggle\ntest = pd.read_csv(\"\/kaggle\/input\/inputfiles\/eval.csv\", low_memory = False)\ntrain = pd.read_csv(\"\/kaggle\/input\/inputfiles\/train.csv\", low_memory = False)\n\nprint(test.head())","395e5dee":"\ntrain.drop(['title', 'console'], axis = 1, inplace = True)\ntest.drop(['console'], axis = 1, inplace = True)\n\ntrain['esrb_rating'].replace({'RP':0, 'EC':1, 'E':2, 'ET':3, 'T':4, 'M':5, 'A':6}, inplace = True)\nprint(train.head())","d4eb86df":"train.dtypes","9a9ed359":"print(test.isna().sum())\n","77ea8de9":"print(train.isna().sum())\n","e91f164f":"y = train['esrb_rating']\nX = train.drop(['esrb_rating'], axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = seed)","2a1df00b":"lr_parameters= {\n    'penalty': ['l1','l2'],\n    'C': [.1, .5, 1, 10, 100],\n    'fit_intercept': [True, False],\n    'intercept_scaling': [.1, .5, 1, 10, 100],\n    'random_state': [1, 10, 100]\n}\n\nlr = LogisticRegression(solver='liblinear', multi_class = 'ovr')\nlr_cv = GridSearchCV(lr, lr_parameters, cv = 150, scoring = 'accuracy')\n\nlr_cv.fit(X_train, y_train)\ny_predict = lr_cv.predict(X_test)\n\nprint(\"Accuracy Score:\", accuracy_score(y_test, y_predict))","040eb5f1":"svm_parameters= {\n    'C': [0.001, 0.1, 1, 10],\n    'gamma': [.1, .8, 1, 'scale', 'auto'],\n    'shrinking': [True, False],\n    'probability': [True, False]\n}\nsvm = SVC(kernel = \"rbf\")\nsvm_cv = GridSearchCV(svm, svm_parameters, cv = 150, scoring = 'accuracy')\n\nsvm_cv.fit(X_train, y_train)\ny_predict = svm_cv.predict(X_test)\n\nprint(\"Accuracy Score:\", accuracy_score(y_test, y_predict))","57f2a578":"dt_parameters = {\n    'criterion': ['gini', 'entropy'],\n    'splitter': ['best', 'random'],\n    'max_features': ['auto','sqrt','log2']\n}\n\ndt = DecisionTreeClassifier()\ndt_cv = GridSearchCV(dt, dt_parameters, cv = 150, scoring = 'accuracy')\n\ndt_cv.fit(X_train, y_train)\ny_predict = dt_cv.predict(X_test)\n\nprint(\"Accuracy Score:\", accuracy_score(y_test, y_predict))","67fe19e2":"rf_parameters = {\n    'criterion': ['gini','entropy'],\n    'max_features': ['auto','sqrt','log2'],\n    'bootstrap':[True, False]\n}\n\nrf = RandomForestClassifier(n_estimators=10000)\nrf_cv = GridSearchCV(rf, rf_parameters, cv = 150, scoring = 'accuracy')\n\n\nrf_cv.fit(X_train, y_train)\ny_predict = rf_cv.predict(X_test)\n\nprint(\"Accuracy Score:\", accuracy_score(y_test, y_predict))","39328a80":"kn_parameters = {\n    'weights': ['uniform', 'distance'],\n    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n    'p': [1, 2]\n}\n\nkn = KNeighborsClassifier(n_neighbors = 75, n_jobs = 100)\nkn_cv = GridSearchCV(kn, kn_parameters, cv = 150, scoring = 'accuracy')\n\nkn_cv.fit(X_train, y_train)\ny_predict = kn_cv.predict(X_test)\n\nprint(\"Accuracy Score:\", accuracy_score(y_test, y_predict))","00da0e62":"lr_cv.fit(X,y)\ntest_predict = lr_cv.predict(test)\n\ndf = pd.DataFrame(test_predict, columns = ['esrb_rating'])\ndf['esrb_rating'].replace({0:'RP', 1:'EC', 2:'E', 3:'ET', 4:'T', 5:'M', 6:'A'}, inplace = True)\nprint(df.head())\ndf.to_csv('submission_lr_4.csv', index_label = 'id')","f3a80e95":"* No NAs, we are good there.\n* Since everything is a binary value aside from the ratings, which is what we're predicting, so there really isn't any outliers","8983e2be":"# Support Vector Machine","88316931":"# Checking NAs\n","894afb71":"# Random Forest Model\n","e1a57a29":"* So many descriptors, will definitely overtune the model\n* Might combine a lot of similar columns, make it less binary, ie combine \"mild blood\", \"animated blood\", \"blood\", and \"blood and gore\"\n* Console doesn't affect the ESRB rating, so I am dropping it\n* \"strong_janguage\" lol","4e08328a":"# Loading Datasets\n","d8996d98":"Best output is through logistic regression, so I'll be using that for my target vector","7717c534":"# Decision Tree Model","35adea80":"# N Nearest Neighbors","c006198e":"# Building Logistic Regression"}}