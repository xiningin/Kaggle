{"cell_type":{"d959e3d3":"code","3ec92d0d":"code","4467a3ff":"code","89435993":"code","9fb96271":"code","19fd6557":"code","7b3ecf28":"code","f44b3fd8":"code","0e8b725a":"code","4f5779ae":"code","e55bcb79":"code","57045ee6":"code","156a219a":"code","27f27b92":"code","9586796a":"code","d7b1a01d":"code","83be267e":"code","3ce1a09f":"code","708d3866":"code","66001025":"code","a3d16a22":"code","72825533":"code","f4cac492":"code","00f0ddfa":"markdown","e3a7d616":"markdown","7ef39d13":"markdown","756fc184":"markdown","fd95c0fb":"markdown","690566a5":"markdown","d0fb26ba":"markdown","d7bfec39":"markdown","6c8d63aa":"markdown","d8c3581e":"markdown","deb53f98":"markdown","f8e77912":"markdown","e3143318":"markdown","e5d9e4ff":"markdown","b11dc254":"markdown","4b79a9e6":"markdown","6cbd813b":"markdown","efae3fa0":"markdown","16d30be9":"markdown","2afcae81":"markdown","857eae21":"markdown","957135d5":"markdown","dfa25093":"markdown","2579f0be":"markdown","ff846c67":"markdown","21bcb916":"markdown","760534d6":"markdown","98ba6b6a":"markdown","c0275d33":"markdown","30dcdf4f":"markdown"},"source":{"d959e3d3":"import os\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport numpy as np\n%matplotlib inline\nIS_KAGGLE = True  # Small helper: if running in KAGGLE, data can be found in \"..\/input\" instead of \".\/input\"","3ec92d0d":"def show_img(image_path_or_array, ax=None):\n    \"\"\"\n    Show image.\n    :param image_path_or_array: Path to image or numpy array representing the image\n    :param ax: Axis where to plot. If not given, new figure is created.\n    \"\"\"\n    if ax is None:\n        plt.figure()\n        ax = plt.gca()\n    if isinstance(image_path_or_array, np.ndarray):\n        img = image_path_or_array\n    elif isinstance(image_path_or_array, str):\n        try:  # Some of the images are somehow corrupt\n            img = mpimg.imread(image_path_or_array) # Numpy array\n        except Exception as ex:\n            print(\"Caught exception reading the image, returning.\", ex)\n            return\n        ax.set_title(\"\/\".join(image_path_or_array.split(\"\/\")[-2:]))\n    else:\n        raise RuntimeError(\"Unknown image type {type}\".format(type=type(image_path_or_array)))\n    \n    ax.imshow(img)","4467a3ff":"base_img_folder = \".\/input\/dataset\/dataset_updated\/training_set\"\nif IS_KAGGLE:\n    base_img_folder = os.path.join(\"..\", base_img_folder)\nart_types = os.listdir(base_img_folder)\nprint(art_types)","89435993":"def show_images_for_art(art_type=\"drawings\", how_many=10):\n    assert art_type in art_types\n    img_folder = os.path.join(base_img_folder, art_type)\n\n    img_files = [os.path.join(img_folder, filename) for filename in os.listdir(img_folder)]\n\n    imgs_per_row = 5\n    nrows = (how_many - 1) \/\/ imgs_per_row + 1\n    fig, axes = plt.subplots(nrows=nrows, ncols=imgs_per_row)\n    fig.set_size_inches((20, nrows * 5))\n    axes = axes.ravel()\n    for filename, ax in zip(img_files[:how_many], axes):\n        show_img(filename, ax=ax)\n        \nshow_images_for_art(art_type=\"drawings\", how_many=20)","9fb96271":"show_images_for_art(art_type=\"engraving\", how_many=20)","19fd6557":"show_images_for_art(art_type=\"painting\", how_many=20)","7b3ecf28":"show_images_for_art(art_type=\"iconography\", how_many=20)","f44b3fd8":"show_images_for_art(art_type=\"sculpture\", how_many=20)","0e8b725a":"# style_image = \"painting\/0918.jpg\"\n# style_image = \"drawings\/i - 593.jpeg\"\nstyle_image = \"drawings\/i - 6.jpeg\"\nstyle_reference_image_path = os.path.join(base_img_folder, style_image)\nshow_img(style_reference_image_path)","4f5779ae":"# target_image = \"drawings\/i - 655.jpeg\" # Drawing of a man\n# target_image = \"iconography\/84 18.59.20.jpg\"  # Jesus icon\ntarget_image = \"painting\/0918.jpg\"  # Painting of horse rider with angel\ntarget_image_path = os.path.join(base_img_folder, target_image)\n\nshow_img(target_image_path)","e55bcb79":"from keras.preprocessing.image import load_img, img_to_array\n\nwidth, height = load_img(target_image_path).size\nimg_height = 400\nimg_width = int(width * img_height \/ height)","57045ee6":"import numpy as np\nfrom keras.applications import vgg19\n\ndef preprocess_image(image_path):\n    img = load_img(image_path, target_size=(img_height, img_width))\n    img = img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = vgg19.preprocess_input(img)\n    return img\n\nprint(\"Shape of preprocessed target image:\", preprocess_image(target_image_path).shape)\nprint(\"Shape of preprocessed reference image:\", preprocess_image(style_reference_image_path).shape)","156a219a":"def deprocess_image(x):\n    x[:, :, 0] += 103.939\n    x[:, :, 1] += 116.779\n    x[:, :, 2] += 123.68\n    x = x[:, :, ::-1]\n    x = np.clip(x, 0, 255).astype('uint8')\n    return x","27f27b92":"from keras import backend as K\n\ntarget_image = K.constant(preprocess_image(target_image_path))\nstyle_reference_image = K.constant(preprocess_image(style_reference_image_path))\ncombination_image = K.placeholder((1, img_height, img_width, 3))\n\ninput_tensor = K.concatenate([target_image, style_reference_image, combination_image], axis=0)\n\nmodel = vgg19.VGG19(input_tensor=input_tensor,\n                   weights='imagenet',\n                   include_top=False)\n","9586796a":"def content_loss(base, combination):\n    return K.sum(K.square(combination-base))","d7b1a01d":"def gram_matrix(x):\n    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n    gram = K.dot(features, K.transpose(features))\n    return gram\n\ndef style_loss(style, combination):\n    S = gram_matrix(style)\n    C = gram_matrix(combination)\n    channels = 3\n    size = img_height * img_width\n    return K.sum(K.square(S - C)) \/ (4. * (channels ** 2) * (size ** 2))","83be267e":"def total_variation_loss(x):\n    a = K.square(\n        x[:, :img_height - 1, :img_width - 1, :] - \n        x[:, 1:, :img_width - 1, :])\n    b = K.square(\n        x[:, :img_height - 1, :img_width - 1, :] - \n        x[:, :img_height - 1, 1:, :])\n    return K.sum(K.pow(a + b, 1.25))","3ce1a09f":"outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\noutputs_dict","708d3866":"content_layer = 'block5_conv2'\nstyle_layers = ['block1_conv1',\n                'block2_conv1',\n                'block3_conv1',\n                'block4_conv1',\n                'block5_conv1']\ntotal_variation_weight = 1e-4\nstyle_weight = 1.\ncontent_weight = 0.025\n\nloss = K.variable(0.)\nlayer_features = outputs_dict[content_layer]\ntarget_image_features = layer_features[0, :, :, :]\ncombination_features = layer_features[2, :, :, :]\nloss += content_weight * content_loss(target_image_features, combination_features)\n\nfor layer_name in style_layers:\n    layer_features = outputs_dict[layer_name]\n    style_reference_features = layer_features[1, :, :, :]\n    combination_features = layer_features[2, :, :, :]\n    sl = style_loss(style_reference_features, combination_features)\n    loss += (style_weight \/ len(style_layers)) * sl\n    \nloss += total_variation_weight * total_variation_loss(combination_image)","66001025":"grads = K.gradients(loss, combination_image)[0]\nfetch_loss_and_grads = K.function([combination_image], [loss, grads])","a3d16a22":"class Evaluator:\n    def __init__(self):\n        self.loss_value = None\n        self.grad_values = None\n    def loss(self, x):\n        assert self.loss_value is None\n        x = x.reshape((1, img_height, img_width, 3))\n        outs = fetch_loss_and_grads([x])\n        loss_value = outs[0]\n        grad_values = outs[1].flatten().astype('float64')\n        self.loss_value = loss_value\n        self.grad_values = grad_values\n        return self.loss_value\n    def grads(self, x):\n        assert self.loss_value is not None\n        grad_values = np.copy(self.grad_values)\n        self.loss_value = None\n        self.grad_values = None\n        return grad_values\n    \nevaluator = Evaluator()","72825533":"from scipy.optimize import fmin_l_bfgs_b\nfrom scipy.misc import imsave\nimport time\n\nresult_folder = 'results'\nif not IS_KAGGLE and not os.path.exists(result_folder):\n    os.makedirs(result_folder)\n\niterations = 10\n\nx = preprocess_image(target_image_path)\nx = x.flatten()\n\nfor i in range(iterations):\n    print('Start of iteration: {}'.format(i))\n    start_time = time.time()\n    x, min_val, info = fmin_l_bfgs_b(evaluator.loss,\n                                    x,\n                                    fprime=evaluator.grads,\n                                    maxfun=20)\n    print('Current loss value:', min_val)\n    img = x.copy().reshape((img_height, img_width, 3))\n    img = deprocess_image(img)\n    if not IS_KAGGLE:\n        fname = os.path.join(result_folder, 'generated_at_iteration_%d.png' % i)\n        imsave(fname, img)\n        print('Image saved as', fname)\n    show_img(img)\n    plt.show()\n    end_time = time.time()\n    print('Iteration %d completes in %d s' % (i, end_time - start_time))","f4cac492":"fig = plt.figure()\nfig.set_size_inches(10, 10)\nax = plt.gca()\nplt.axis('off')\nax.imshow(img, interpolation=\"gaussian\")\nax.set_title(\"Result image\")","00f0ddfa":"### Perform loss minimization using Scipy's L-BFGS","e3a7d616":"Let's define the base folder for our images and the types of art available:","7ef39d13":"### Define input tensors and pretrained model","756fc184":"### Save layers in a dictionary","fd95c0fb":"Let's look at drawings:","690566a5":"### Engravings","d0fb26ba":"### Define image preprocessing function suitable for VGG19","d7bfec39":"### Define content loss","6c8d63aa":"Define imports and constants:","d8c3581e":"### Sculptures","deb53f98":"Define helper functions:","f8e77912":"### Add regularization loss to encourage continuity","e3143318":"### Define style loss in terms of the Gram matrix","e5d9e4ff":"### Choose the target image:","b11dc254":"### Choosing the reference image\nPaintings, drawings, engravings and icons have a very clear style we could try transfering to our photo. Let's use pick one of them as our style.","4b79a9e6":"The key observation made in the paper by L. Gatys et al. was that convolutional networks (convnets) present a way to mathematically define `style` and `content` functions. The activations of the different layers of a convnet provide a decomposition of the contents of an image over different scales. The content of the image, which is a global and abstract property, is captured by the representations in the higher (later) layers of the convnet. \n\nThe style loss as defined by Gatys et al. uses multiple layers of a convnet, as we want to capture the appearance of the reference image at all spatial scales extracted by the convnet. Gatys et al. use the _Gram matrix_ of a layer's activations as style loss, i.e., the inner product of the feature maps of a given layer. ","6cbd813b":"## Style transfer\nWhat follows is a quick go-through of the steps of neural style transfer. If you are interested in details, please check the [notebook](https:\/\/github.com\/fchollet\/deep-learning-with-python-notebooks\/blob\/master\/8.3-neural-style-transfer.ipynb) from Deep Learning with Python or the original [paper](https:\/\/arxiv.org\/abs\/1508.06576)).","efae3fa0":"### Define Keras functions that compute gradients and losses for given input placeholder tensors","16d30be9":"### Final image\n","2afcae81":"### Paintings","857eae21":"We now move on to the implementation following the general process:\n1. Set up a network that computes the layer activations for the style-reference image, target image, and the generated image.\n2. Use the computed layer activations to compute the loss function.\n3. Set up gradient descent to minimize the loss.\n\nWe'll use the VGG19 as the pretrained convnet. Images are resized to a shared height of 400px.","957135d5":"### Define the layers used for computing style and content losses as well as loss itself","dfa25093":"## Getting started","2579f0be":"### Preprocess the target image","ff846c67":"Neural style transfer means applying the style of a reference image to a target image while preserving the content of the target image. Here \"style\" means textures, colors, and visual patterns in the image. The content is the higher-level structure of the image. The loss function is heuristically defined as\n```python\nloss = dist(style(reference) - style(generated)) + dist(content(original) - content(generated)),\n```\nwhere `reference` is the reference image (from which style is copied), `original` is the image where style is being applied, and `generated` is the generated image. Minimizing this loss means that\n```\nstyle(generated) $\\approx$ style(reference)\n```\nand \n```\ncontent(generated) $\\approx$ content(original)\n```\nas we want.","21bcb916":"### Iconography","760534d6":"### Define inverse transform","98ba6b6a":"# Neural Style Transfer with Keras\nThis notebook is based on Chapter 8.3 of [Deep Learning with Python](https:\/\/www.manning.com\/books\/deep-learning-with-python) by F. Chollet. Original article \"A Neural Algorithm of Artistic Style\" by Leon Gatys et al. can be found in [arXiv](https:\/\/arxiv.org\/abs\/1508.06576).","c0275d33":"### Define a helper class that computes both the loss and gradient in one go","30dcdf4f":"### Drawings"}}