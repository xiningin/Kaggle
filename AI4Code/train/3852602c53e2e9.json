{"cell_type":{"f01fa0dd":"code","f1f05c26":"code","28c7ec11":"code","a657bb0b":"code","e58bca6a":"code","4456ff75":"code","a960ce88":"code","9532f4db":"code","e3f2d2a3":"code","8de955ed":"code","6df58b5f":"code","4a2d0619":"code","d08d0d3c":"code","de406897":"code","0f35b3f1":"code","8271db43":"code","dd046121":"code","b0853a3a":"code","a4205086":"code","47790029":"code","5c4c434d":"code","812be541":"code","cb6bacf2":"code","a7a201f8":"code","1a5c7b8f":"code","9e75c19e":"code","d4b8da1d":"code","723c3dbd":"code","15559d53":"code","655e0dba":"code","c4cc5bd8":"code","faab24df":"code","0704732d":"code","3e4c09b3":"code","bedfa7d9":"code","674f5d5e":"code","71864f64":"code","d25fe981":"code","946a0deb":"code","241fc96c":"code","99a19bfd":"code","b3c1a7d9":"code","1641f822":"code","6236ed76":"code","6cce4133":"code","71035c99":"code","26b9861f":"code","611c8ac9":"code","7a18c867":"code","ab57df9b":"code","f97e744f":"code","381f396e":"code","992c8218":"code","fd5badd3":"code","051ce268":"code","c182e0ad":"code","9866fd1e":"code","6be6bc74":"code","47d1a73f":"code","3fb62a54":"code","0c068fb2":"code","bcd5764e":"code","80f7ea86":"code","e7f9bf1c":"code","41d10d96":"code","b7ec66cd":"code","c9c3d1da":"code","3680c47c":"code","994ab9eb":"code","1b732774":"code","dd329769":"code","a548a062":"code","87696ad6":"code","615b0193":"code","423c03d4":"code","47d2de3e":"code","0a3f9269":"code","52687fab":"code","d4e5ae30":"code","0bd2a09d":"code","994b496c":"code","50f96893":"code","08b47a47":"code","8189074b":"code","e5722ef1":"code","9eda2e6f":"code","197f3bf0":"code","1a8d16dd":"code","576ffcf3":"code","29b2936f":"code","671e1bb9":"code","1f32073c":"code","5353b20a":"code","c26674b4":"code","4abe3f5b":"code","9e7742bf":"code","28fc489f":"code","4b90d4ec":"code","0804ff27":"code","1e0fd6d1":"code","87c65b1a":"code","b239aaeb":"code","5c83b397":"code","c4dc8fc7":"code","479104d9":"markdown","61da609b":"markdown","20154393":"markdown","bad2e159":"markdown","4f9bfbdd":"markdown","d0061d55":"markdown","3d3e0eb4":"markdown","af3daab8":"markdown"},"source":{"f01fa0dd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f1f05c26":"# Load in our Libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","28c7ec11":"# Read Data\ndf = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ndf.head()","a657bb0b":"test_original = test.copy()","e58bca6a":"test.head()","4456ff75":"# Shape of the dataset.\ndf.shape, test.shape","a960ce88":"# Checking null values.\ndf.info()\nprint('*'*50)\ntest.info()","9532f4db":"# Checking Quantiles.\ndf.describe()","e3f2d2a3":"# Describing categorical values.\ndf.describe(include = ['object'])","8de955ed":"# Print Data types for each variable.\ndf.dtypes","6df58b5f":"# Normalize can be set to True to print proportions instead of number \ndf['Survived'].value_counts(normalize = True)","4a2d0619":"df['Pclass'].value_counts()","d08d0d3c":"df['Sex'].value_counts()","de406897":"df['Age'].value_counts()","0f35b3f1":"df['SibSp'].value_counts()","8271db43":"df['Parch'].value_counts()","dd046121":"df['Fare'].value_counts()","b0853a3a":"df['Cabin'].value_counts()","a4205086":"df['Embarked'].value_counts()","47790029":"df['Ticket'].value_counts()","5c4c434d":"# Visualising Numerical features.\nsns.distplot(df['Survived'])","812be541":"# Visualising Numerical features and their corresponding boxplot.\nplt.figure(1, figsize=(16,5))\nplt.subplot(121)\nsns.distplot(df['Pclass'])\n\nplt.subplot(122)\nsns.boxplot(y=\"Pclass\", data = df)\n\nplt.show()","cb6bacf2":"plt.figure(1, figsize=(16,5))\nplt.subplot(121)\nsns.distplot(df['SibSp'])\n\nplt.subplot(122)\nsns.boxplot(y=\"SibSp\", data = df)\n\nplt.show()","a7a201f8":"# Print Percentiles for detecting outliers.\nfor i in range(0,100,10):\n    var = df[\"SibSp\"].values\n    var = np.sort(var,axis = None)\n    print(\"{} percentile value is {}\".format(i,var[int(len(var)*(float(i)\/100))]))\nprint (\"100 percentile value is \",var[-1])","1a5c7b8f":"for i in range(90,100):\n    var = df[\"SibSp\"].values\n    var = np.sort(var,axis = None)\n    print(\"{} percentile value is {}\".format(i,var[int(len(var)*(float(i)\/100))]))\nprint (\"100 percentile value is \",var[-1])","9e75c19e":"plt.figure(1, figsize=(16,5))\nplt.subplot(121)\nsns.distplot(df['Parch'])\n\nplt.subplot(122)\nsns.boxplot(y=\"Parch\", data = df)\n\nplt.show()","d4b8da1d":"plt.figure(1, figsize=(16,5))\nplt.subplot(121)\nsns.distplot(df['Fare'])\n\nplt.subplot(122)\nsns.boxplot(y=\"Fare\", data = df)\n\nplt.show()","723c3dbd":"df1 = test.dropna()\nplt.figure(1, figsize=(16,5))\nplt.subplot(121)\nsns.distplot(df1['Fare'])\n\nplt.subplot(122)\nsns.boxplot(y=\"Fare\", data = df1)\n\nplt.show()","15559d53":"df1 = df.dropna()\nplt.figure(1, figsize=(16,5))\nplt.subplot(121)\nsns.distplot(df1['Age'])\n\nplt.subplot(122)\nsns.boxplot(y=\"Age\", data = df1)\n\nplt.show()","655e0dba":"# Visualising Categorical Features.\nplt.figure(1)\nplt.subplot(121)\ndf['Sex'].value_counts(normalize=True).plot.bar(figsize=(16,5), title= 'Sex')\n\nplt.subplot(122)\ndf['Embarked'].value_counts(normalize=True).plot.bar(figsize=(16,5), title= 'Embarked')","c4cc5bd8":"# Grouping Data by target variable and analysing features.\nprint(pd.crosstab(df['Pclass'],df['Survived']))\nct = pd.crosstab(df['Pclass'],df['Survived'])\nct.plot.bar(stacked=True)\nplt.legend(title='Survived')","faab24df":"ct = pd.crosstab(df['SibSp'],df['Survived'])\nct.plot.bar(stacked=True)\nplt.legend(title='Survived')","0704732d":"ct = pd.crosstab(df['Parch'],df['Survived'])\nct.plot.bar(stacked=True)\nplt.legend(title='Survived')","3e4c09b3":"ct = pd.crosstab(df['Sex'],df['Survived'])\nct.plot.bar(stacked=True)\nplt.legend(title='Survived')","bedfa7d9":"ct = pd.crosstab(df['Embarked'],df['Survived'])\nct.plot.bar(stacked=True)\nplt.legend(title='Survived')","674f5d5e":"# Grouping Features using Quantile cut.\ndf['Age_bin'] = pd.qcut(df['Age'], 4)\ndf[['Age_bin', 'Survived']].groupby(['Age_bin'], as_index=False).mean().sort_values(by='Age_bin', ascending=True)","71864f64":"# Making bins for continous feature.\nbins=[0,20,28,38,80]\ngroup=[0,1,2,3]\ndf['Age_bin']=pd.cut(df['Age'],bins,labels=group)","d25fe981":"ct = pd.crosstab(df['Age_bin'],df['Survived'])\nct.plot.bar(stacked=True)\nplt.legend(title='Survived')","946a0deb":"df['Fare_bin'] = pd.qcut(df['Fare'], 4)\ndf[['Fare_bin', 'Survived']].groupby(['Fare_bin'], as_index=False).mean().sort_values(by='Fare_bin', ascending=True)","241fc96c":"bins=[0,8,15,32,513]\ngroup=[0,1,2,3]\ndf['Fare_bin']=pd.cut(df['Fare'],bins,labels=group)","99a19bfd":"ct = pd.crosstab(df['Fare_bin'],df['Survived'])\nct.plot.bar(stacked=True)\nplt.legend(title='Survived')","b3c1a7d9":"# Print correlation matrix\nmatrix = df.corr()\nf, ax = plt.subplots(figsize=(9, 6))\nsns.heatmap(matrix, vmax=.8, annot = True, square=True, cmap=\"BuPu\");","1641f822":"# Checking the missing values\ndf.isnull().sum()","6236ed76":"test.isnull().sum()","6cce4133":"# Replacing null values with 0 and other values with 1.\n# Feature that tells whether a passenger had a cabin on the Titanic\ndf['Cabin'] = df[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)","71035c99":"# Filling null values of categorical features with mode.\ndf['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\ndf['Age_bin'].fillna(df['Age_bin'].mode()[0], inplace=True)\ndf['Fare_bin'].fillna(df['Fare_bin'].mode()[0], inplace=True)","26b9861f":"# Grouping Embarked by target values.\ndf[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","611c8ac9":"# Making pivot table of Age Feature by Pclass and Sex feature and taking median to fill null values.\ntable = df.pivot_table(values='Age', index='Pclass' ,columns='Sex', aggfunc=np.median)\n\n# Define function to return value of this pivot_table\ndef fage(x):\n return table.loc[x['Pclass'],x['Sex']]\n\n# Replace missing values\ndf['Age'].fillna(df[df['Age'].isnull()].apply(fage, axis=1), inplace=True)","7a18c867":"# Making pivot table of Age Feature by Pclass and Sex feature and taking median to fill null values.\ntable = test.pivot_table(values='Age', index='Pclass' ,columns='Sex', aggfunc=np.median)\n\n# Define function to return value of this pivot_table\ndef fage(x):\n return table.loc[x['Pclass'],x['Sex']]\n\n# Replace missing values\ntest['Age'].fillna(test[test['Age'].isnull()].apply(fage, axis=1), inplace=True)","ab57df9b":"# Making pivot table of Age Feature by Pclass and Sex feature and taking median to fill null values.\ntable = test.pivot_table(values='Fare', index='Pclass' ,columns='Sex', aggfunc=np.median)\n\n# Define function to return value of this pivot_table\ndef fage(x):\n return table.loc[x['Pclass'],x['Sex']]\n\n# Replace missing values\ntest['Fare'].fillna(test[test['Fare'].isnull()].apply(fage, axis=1), inplace=True)","f97e744f":"# Importing packages for model training.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nfrom sklearn.ensemble import GradientBoostingClassifier","381f396e":"# Dropping unneccesary features.\ndf = df.drop(['PassengerId','Ticket','Name','Fare_bin','Age_bin'],axis=1)\ntest = test.drop(['PassengerId','Ticket','Name'],axis=1)","992c8218":"# Preparing Independent and dependent features.\nX = df.drop('Survived',1)\ny = df.Survived","fd5badd3":"# Adding dummies to the dataset\nX = pd.get_dummies(X)\ntest=pd.get_dummies(test)","051ce268":"X.shape, test.shape","c182e0ad":"# Splitting Data into test train data for cross validation.\nX_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size =0.2, random_state=1)","9866fd1e":"# logistic regression using K-fold cross validation\ni=1\nkf = StratifiedKFold(n_splits=5,random_state=1,shuffle=True)\nfor train_index,test_index in kf.split(X,y):\n     print('\\n{} of kfold {}'.format(i,kf.n_splits))\n     xtr,xvl = X.loc[train_index],X.loc[test_index]\n     ytr,yvl = y[train_index],y[test_index]\n    \n     model = LogisticRegression(random_state=1)\n     model.fit(xtr, ytr)\n     pred_test = model.predict(xvl)\n     score = accuracy_score(yvl,pred_test)\n     print('accuracy_score',score)\n     i+=1\npred=model.predict_proba(xvl)[:,1]","6be6bc74":"# Roc Curve\nfrom sklearn import metrics\nfpr, tpr, _ = metrics.roc_curve(yvl,  pred)\nauc = metrics.roc_auc_score(yvl, pred)\nplt.figure(figsize=(12,8))\nplt.plot(fpr,tpr,label=\"validation, auc=\"+str(auc))\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc=4)\nplt.show()","47d1a73f":"model = LogisticRegression()\nmodel.fit(X, y)\nacc_log = round(model.score(X, y) * 100, 2)\nacc_log","3fb62a54":"model = LogisticRegression()\nmodel.fit(X_train, y_train)\nacc_log = round(model.score(X_train, y_train) * 100, 2)\nacc_log","0c068fb2":"acc_logv = round(model.score(X_valid, y_valid) * 100, 2)\nacc_logv","bcd5764e":"coeff_df = pd.DataFrame(df.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(model.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)","80f7ea86":"svc = SVC()\nsvc.fit(X, y)\npred_test = svc.predict(test)\nacc_svc = round(svc.score(X_train, y_train) * 100, 2)\nacc_svc","e7f9bf1c":"acc_svcv = round(svc.score(X_valid, y_valid) * 100, 2)\nacc_svcv","41d10d96":"# Read submission file\nsubmission = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")","b7ec66cd":"submission['Survived'] = pred_test               # Fill predictions in Survived variable of submission file\nsubmission['PassengerId'] = test_original['PassengerId']    # Fill Passenger Id of submission file with the Passenger Id of original test file","c9c3d1da":"# Converting submission file to .csv format\npd.DataFrame(submission, columns=['PassengerId','Survived']).to_csv('SVM.csv')","3680c47c":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\nacc_knn = round(knn.score(X_train, y_train) * 100, 2)\nacc_knn","994ab9eb":"acc_knnv = round(knn.score(X_valid, y_valid) * 100, 2)\nacc_knnv","1b732774":"gaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\nacc_gaussian = round(gaussian.score(X_train, y_train) * 100, 2)\nacc_gaussian","dd329769":"acc_gaussianv = round(gaussian.score(X_valid, y_valid) * 100, 2)\nacc_gaussianv","a548a062":"linear_svc = LinearSVC()\nlinear_svc.fit(X_train, y_train)\nacc_linear_svc = round(linear_svc.score(X_train, y_train) * 100, 2)\nacc_linear_svc","87696ad6":"acc_linear_svcv = round(linear_svc.score(X_valid, y_valid) * 100, 2)\nacc_linear_svcv","615b0193":"sgd = SGDClassifier()\nsgd.fit(X_train, y_train)\nacc_sgd = round(sgd.score(X_train, y_train) * 100, 2)\nacc_sgd","423c03d4":"acc_sgdv = round(sgd.score(X_valid, y_valid) * 100, 2)\nacc_sgdv","47d2de3e":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, y_train)\nacc_decision_tree = round(decision_tree.score(X_train, y_train) * 100, 2)\nacc_decision_tree","0a3f9269":"acc_decision_treev = round(decision_tree.score(X_valid, y_valid) * 100, 2)\nacc_decision_treev","52687fab":"random_forest = RandomForestClassifier(n_estimators=100, max_depth = 2)\nrandom_forest.fit(X_train, y_train)\nacc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)\nacc_random_forest","d4e5ae30":"acc_random_forestv = round(random_forest.score(X_valid, y_valid) * 100, 2)\nacc_random_forestv","0bd2a09d":"df['Fare'] = np.log(df['Fare'] + 1)","994b496c":"X = df.drop('Survived',1)\ny = df.Survived","50f96893":"X=pd.get_dummies(X)","08b47a47":"X_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size =0.2, random_state=1)","8189074b":"i=1\nkf = StratifiedKFold(n_splits=5,random_state=1,shuffle=True)\nfor train_index,test_index in kf.split(X,y):\n     print('\\n{} of kfold {}'.format(i,kf.n_splits))\n     xtr,xvl = X.loc[train_index],X.loc[test_index]\n     ytr,yvl = y[train_index],y[test_index]\n    \n     model = LogisticRegression(random_state=1)\n     model.fit(xtr, ytr)\n     pred_test = model.predict(xvl)\n     score = accuracy_score(yvl,pred_test)\n     print('accuracy_score',score)\n     i+=1","e5722ef1":"model = LogisticRegression()\nmodel.fit(X, y)\nacc_log = round(model.score(X, y) * 100, 2)\nacc_log","9eda2e6f":"model = LogisticRegression()\nmodel.fit(X_train, y_train)\nacc_log1 = round(model.score(X_train, y_train) * 100, 2)\nacc_log1","197f3bf0":"acc_logv1 = round(model.score(X_valid, y_valid) * 100, 2)\nacc_logv1","1a8d16dd":"coeff_df = pd.DataFrame(df.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(model.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)","576ffcf3":"svc = SVC()\nsvc.fit(X, y)\nacc_svc1 = round(svc.score(X_train, y_train) * 100, 2)\nacc_svc1","29b2936f":"acc_svcv1 = round(svc.score(X_valid, y_valid) * 100, 2)\nacc_svcv1","671e1bb9":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\nacc_knn1 = round(knn.score(X_train, y_train) * 100, 2)\nacc_knn1","1f32073c":"acc_knnv1 = round(knn.score(X_valid, y_valid) * 100, 2)\nacc_knnv1","5353b20a":"gaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\nacc_gaussian1 = round(gaussian.score(X_train, y_train) * 100, 2)\nacc_gaussian1","c26674b4":"acc_gaussianv1 = round(gaussian.score(X_valid, y_valid) * 100, 2)\nacc_gaussianv1","4abe3f5b":"linear_svc = LinearSVC()\nlinear_svc.fit(X_train, y_train)\nacc_linear_svc1 = round(linear_svc.score(X_train, y_train) * 100, 2)\nacc_linear_svc1","9e7742bf":"acc_linear_svcv1 = round(linear_svc.score(X_valid, y_valid) * 100, 2)\nacc_linear_svcv1","28fc489f":"sgd = SGDClassifier()\nsgd.fit(X_train, y_train)\nacc_sgd1 = round(sgd.score(X_train, y_train) * 100, 2)\nacc_sgd1","4b90d4ec":"acc_sgdv1 = round(sgd.score(X_valid, y_valid) * 100, 2)\nacc_sgdv1","0804ff27":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, y_train)\nacc_decision_tree1 = round(decision_tree.score(X_train, y_train) * 100, 2)\nacc_decision_tree1","1e0fd6d1":"acc_decision_treev1 = round(decision_tree.score(X_valid, y_valid) * 100, 2)\nacc_decision_treev1","87c65b1a":"random_forest = RandomForestClassifier(n_estimators=100, max_depth = 2)\nrandom_forest.fit(X_train, y_train)\nacc_random_forest1 = round(random_forest.score(X_train, y_train) * 100, 2)\nacc_random_forest1","b239aaeb":"acc_random_forestv1 = round(random_forest.score(X_valid, y_valid) * 100, 2)\nacc_random_forestv1","5c83b397":"# Accuracy of Diifferent algorithms before feature engineering\nmodels = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score_tr': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian,\n              acc_sgd, acc_linear_svc, acc_decision_tree],\n    'Score_cv': [acc_svcv, acc_knnv, acc_logv, \n              acc_random_forestv, acc_gaussianv,\n              acc_sgdv, acc_linear_svcv, acc_decision_treev]})\nmodels.sort_values(by='Score_cv', ascending=False)","c4dc8fc7":"# Accuracy of Diifferent algorithms after feature engineering\nmodels = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score_tr': [acc_svc1, acc_knn1, acc_log1, \n              acc_random_forest1, acc_gaussian1,\n              acc_sgd1, acc_linear_svc1, acc_decision_tree1],\n    'Score_cv': [acc_svcv1, acc_knnv1, acc_logv1, \n              acc_random_forestv1, acc_gaussianv1,\n              acc_sgdv1, acc_linear_svcv1, acc_decision_treev1]})\nmodels.sort_values(by='Score_cv', ascending=False)","479104d9":"**Feature Transformation**","61da609b":"**Univariate Analysis**","20154393":"Checking Unique Values","bad2e159":"**MultiVariate Analysis**","4f9bfbdd":"In test data, Age, Fare, Cabin features has missing values. ","d0061d55":"**Model Evaluation**","3d3e0eb4":"**Model training after feature engineering**","af3daab8":"**Missing values and outlier treatment**"}}