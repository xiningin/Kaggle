{"cell_type":{"538aeeb1":"code","7b35eefe":"code","a24c02ad":"code","1a9379c7":"code","1c661393":"code","cded4a70":"code","3765bc94":"code","c1b0514e":"code","54bc47aa":"code","5c5be8f6":"code","bdd5b952":"code","f8db7f62":"code","5fa4bfcb":"code","12ce15f4":"code","070929ec":"markdown","9229f726":"markdown","1fd598a8":"markdown","5d542762":"markdown","400921fa":"markdown","cf1b350e":"markdown","808a988e":"markdown","deace7a9":"markdown","6a70d28b":"markdown","2a766c04":"markdown"},"source":{"538aeeb1":"import os, sys, math, warnings\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \nimport numpy as np\nimport pandas as pd\nimport functools\n\nfrom pathlib import Path\nfrom itertools import product\nfrom time import time\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_datasets as tfds\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks\nfrom tensorflow.keras.models import clone_model\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\n\nfrom sklearn.model_selection import KFold\n\n# TODO: Extend training with Tensorboard logging\n# https:\/\/www.kaggle.com\/shivam1600\/tensorboard-on-kaggle\n# %load_ext tensorboard","7b35eefe":"def set_seed(seed=31415):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\nset_seed()\n\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\nplt.rc('image', cmap='magma')\nwarnings.filterwarnings(\"ignore\")","a24c02ad":"batch_size = 64\nimage_size = [128, 128]\n\n# Data Pipeline\ndef convert_to_float(image, label):\n    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n    return image, label\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE","1a9379c7":"traindir = image_dataset_from_directory(\n    '..\/input\/car-or-truck\/train',\n    labels='inferred',\n    label_mode='binary',\n    image_size=image_size,\n    interpolation='nearest',\n    batch_size=batch_size,\n    shuffle=True,\n)\n\nvaldir = image_dataset_from_directory(\n    '..\/input\/car-or-truck\/valid',\n    labels='inferred',\n    label_mode='binary',\n    image_size=image_size,\n    interpolation='nearest',\n    batch_size=batch_size,\n    shuffle=True,\n)\n\ndataset = traindir.concatenate(valdir).shuffle(buffer_size=1024)\n\"\"\"General dataset instance that loads every image found in dataset folder.\"\"\"","1c661393":"ratios = {\n    \"train\": 0.3,\n    \"test\": 0.35,\n    \"val\": 0.35\n}\nratios_sum = round(sum(ratios.values()), 2)\nassert ratios_sum == 1.0, f\"Ratios sum must be equal to 1, not {ratios_sum}!\"\n\n_train_len = int(ratios[\"train\"] * len(dataset))\n_val_len = int(ratios[\"val\"] * len(dataset))\n_test_len = int(ratios[\"test\"] * len(dataset))\n\"\"\"Split the general dataset according to train\/test\/val ratios.\"\"\"\n\ntrain_dataset = dataset.take(_train_len)\ntrain_dataset = (\n    train_dataset\n    .map(convert_to_float)\n    .cache()\n    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n)\n\nval_dataset = dataset.skip(_train_len).take(_val_len)\nval_dataset = (\n    val_dataset\n    .map(convert_to_float)\n    .cache()\n    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n)\n\ntest_dataset = dataset.skip(_train_len).skip(_val_len)\ntest_dataset = (\n    test_dataset\n    .map(convert_to_float)\n    .cache()\n    .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n)\n\nprint(f\"Lengths: train: {(len(train_dataset) * batch_size)}, test: {(len(test_dataset) * batch_size)}, val: {(len(val_dataset) * batch_size)}\")","cded4a70":"for name, dataset_ in [(\"train\", train_dataset), (\"test\", test_dataset), (\"val\", val_dataset)]:\n    labels = [int(y) for x, batch in dataset_ for y in batch]\n    labels, counts = np.unique(labels, return_counts=True)\n    labels = np.asarray(labels, dtype=str)\n    print(f\"Subset {name}:\")\n    for label, count in zip(labels, counts):\n        print(f\" - Label {label}: {count}\")","3765bc94":"previews = iter(val_dataset.unbatch())\n\nplt.figure(figsize=(18,18))\nfor i in range(12):\n    x, y = next(previews)\n    plt.subplot(4, 3, i+1)\n    plt.imshow(tf.squeeze(x))\n    plt.axis('off')\nplt.show()","c1b0514e":"def compiler(optimizer):\n    \"\"\"Model compilation and optimization.\"\"\"\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            model = func()\n\n            model.compile(\n                optimizer=optimizer,\n                loss='binary_crossentropy',\n                metrics=['binary_accuracy'],\n            )\n            model.build([None, *image_size, 3])\n            return model\n        return wrapper\n    return decorator  ","54bc47aa":"# From Visualizing What Convnets Learn: https:\/\/mathformachines.com\/posts\/what-convnets-learn\/\n\ndef make_activation_model(model, layer_name, filter):\n    layer = model.get_layer(layer_name)  # Grab the layer\n    feature_map = layer.output[:, :, :, filter]  # Get output for the given filter\n    activation_model = keras.Model(\n        inputs=model.inputs,  # New inputs are original inputs (images)\n        outputs=feature_map,  # New outputs are the filter's outputs (feature maps)\n    )\n    return activation_model\n\n\ndef show_feature_map(image, model, layer_name, filter, ax=None):\n    act = make_activation_model(model, layer_name, filter)\n    feature_map = tf.squeeze(act(tf.expand_dims(image, axis=0)))\n    if ax is None:\n        fig, ax = plt.subplots()\n    ax.imshow(\n        feature_map, cmap=\"magma\", vmin=0.0, vmax=1.0,\n    )\n    return ax\n\ndef show_feature_maps(\n    image,\n    model,\n    layer_name,\n    offset=0,\n    rows=None,\n    cols=3,\n    width=12,\n    cmap=\"magma\",\n):\n    if rows is None:\n        num_filters = model.get_layer(layer_name).output.shape[-1]\n        rows = math.floor(num_filters \/ cols)\n    fig, axs = plt.subplots(\n        rows,\n        cols,\n        figsize=(width, (width * rows) \/ cols),\n        gridspec_kw=dict(wspace=0.01, hspace=0.01),\n    )\n    for f, (r, c) in enumerate(product(range(rows), range(cols))):\n        axs[r, c] = show_feature_map(\n            image, model, layer_name, f + offset, ax=axs[r, c]\n        )\n    return fig\n","5c5be8f6":"@compiler(optimizer=tf.keras.optimizers.Adam(epsilon=0.01))\ndef init_conv2D_adam():\n    \"\"\"\n        Metrics for 60 epochs | Distribution (train\/test\/val): 0.3, 0.35, 0.35\n            Evaluation score: loss: 0.5417, binary_accuracy: 0.8573\n            Evaluation score: loss: 0.4857, binary_accuracy: 0.8488\n            --------------------------------------------------------\n            19.01.22 experiments:\n            \n            - Dropout(0.3) after each MaxPool2D() except third ->\n                Stuck on 20\/60 train const loss 0.5806\n                > It was a really bad idea to put it after MaxPool2D...\n            \n            + Dropout(0.3) after Conv2D layers before MaxPool2D ->\n                Epoch 10\/60\n                47\/47 [==============================] - 2s 52ms\/step - loss: 0.6116 - binary_accuracy: 0.6519 - val_loss: 0.6345 - val_binary_accuracy: 0.6511\n                Epoch 30\/60\n                47\/47 [==============================] - 2s 52ms\/step - loss: 0.2690 - binary_accuracy: 0.8680 - val_loss: 0.4413 - val_binary_accuracy: 0.7933\n                Evaluation score: loss: 0.3834, binary_accuracy: 0.8581\n            \n            - BatchNormalization before first head Dense -> force stop 30\/60 test loss: 2.6264, binary_accuracy: 0.5814\n                > Conclusion: Same as before with Dropout above.\n            \n            - BatchNormalization before Conv2D blocks ->\n                Epoch 10\/60\n                47\/47 [==============================] - 3s 58ms\/step - loss: 0.5459 - binary_accuracy: 0.7094 - val_loss: 0.6289 - val_binary_accuracy: 0.6181\n                Epoch 30\/60\n                47\/47 [==============================] - 3s 59ms\/step - loss: 0.2587 - binary_accuracy: 0.8501 - val_loss: 0.5078 - val_binary_accuracy: 0.7575\n                Force stop 50\/60 test loss: 0.3963, binary_accuracy: 0.8551\n            \n            - BatchNormalization in between first head Dense and ReLU ->\n                Huge val_loss std, values: [0.33, 1.20].\n                Force stop 45\/60 loss: 0.7383, binary_accuracy: 0.7516\n            \n            ? Block Three 2nd Conv2D remove ->\n                * Slow but stable learning. Val_loss slowly goes down with verry little std.\n                * 5-6ms\/step win.\n                Epoch 10\/60\n                47\/47 [==============================] - 2s 47ms\/step - loss: 0.6077 - binary_accuracy: 0.6562 - val_loss: 0.6365 - val_binary_accuracy: 0.6926\n                Epoch 30\/60\n                47\/47 [==============================] - 2s 46ms\/step - loss: 0.3887 - binary_accuracy: 0.7892 - val_loss: 0.4982 - val_binary_accuracy: 0.8007\n                Evaluation score: loss: 0.3823, binary_accuracy: 0.8586\n            \n            - Block One & Two add second Conv2D layers ->\n                * Val_loss 0.15 std.\n                * 30ms\/step loss.\n                Evaluation score: loss: 0.3758, binary_accuracy: 0.8586\n            \n            - Head add Dense(512) & Dropout(0.3), Dense(256) & Dropout(0.3) ->\n                * Val_loss starts to fluctuate at 0.45 +- 0.15.\n                Epoch 10\/60\n                47\/47 [==============================] - 3s 56ms\/step - loss: 0.6078 - binary_accuracy: 0.6656 - val_loss: 0.6231 - val_binary_accuracy: 0.6818\n                Epoch 30\/60\n                47\/47 [==============================] - 3s 57ms\/step - loss: 0.1991 - binary_accuracy: 0.9129 - val_loss: 0.4150 - val_binary_accuracy: 0.8172\n                Force stop 51\/60 loss: 0.4899, binary_accuracy: 0.8693\n                > Too much Dense layers for not enough features or data?\n            \n            \ud83d\udcac It is imposible to train loss <= 0.3. Not enough data?\n            \n            + Preprocessing RandomContrast(0.1), RandomFlip(\"horizontal\") ->\n                * Better! Training loss do not run into ~0.03 at 40-50 eps. and val_loss is a lot more stable.\n                Epoch 10\/60\n                47\/47 [==============================] - 3s 56ms\/step - loss: 0.6043 - binary_accuracy: 0.6739 - val_loss: 0.6369 - val_binary_accuracy: 0.6787\n                Epoch 30\/60\n                47\/47 [==============================] - 2s 53ms\/step - loss: 0.3522 - binary_accuracy: 0.8431 - val_loss: 0.4451 - val_binary_accuracy: 0.8155\n                Epoch 60\/60\n                47\/47 [==============================] - 3s 54ms\/step - loss: 0.1037 - binary_accuracy: 0.9594 - val_loss: 0.3323 - val_binary_accuracy: 0.8749\n                Evaluation score: loss: 0.3328, binary_accuracy: 0.8740\n                > Looks like stable training loss with mean 0.1 and std 0.5 is a good indicator.\n                \n            - RandomCrop(32, 32) completely breaks the training process,\n              RandomCrop(16, 16) makes it a lot harder to train.\n                  Force stop 32\/60 loss: 0.6760, binary_accuracy: 0.5795\n                  \n            - RandomRotation(0.1) ->\n                * Increased epochs number needed to reach 0.3x val_loss up to 80.\n                * Val_loss is very unstable at 0.3x, \"jumping\" from 0.31 to 0.42 every 1-2 eps.\n                Evaluation score: loss: 0.3787, binary_accuracy: 0.8474\n                \n            - RandomZoom(0.1) ->\n                * No miracle happened.\n                Evaluation score: loss: 0.4010, binary_accuracy: 0.8504\n                \n            Version 4 update:\n            Evaluation score: loss: 0.2841, binary_accuracy: 0.8902\n            --------------------------------------------------------\n                \n            \n    \"\"\"\n    name = sys._getframe().f_code.co_name\n    model = keras.Sequential([\n        layers.InputLayer(input_shape=[*image_size, 3]),\n        \n        # Preprocessing\n        preprocessing.RandomContrast(factor=0.1),\n        preprocessing.RandomFlip(mode=\"horizontal\"),\n        \n        # Block One\n        layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same'),\n        layers.Dropout(0.3),\n        layers.MaxPool2D(),\n\n        # Block Two\n        layers.Conv2D(filters=64, kernel_size=3, activation='relu', padding='same'),\n        layers.Dropout(0.3),\n        layers.MaxPool2D(),\n\n        # Block Three\n        layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\", padding=\"same\"),\n        layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\", padding=\"same\"),\n        layers.Dropout(0.3),\n        layers.MaxPool2D(),\n\n        # Head\n        layers.Flatten(),\n        layers.Dense(6, activation=\"relu\"),\n        layers.Dropout(0.2),\n        layers.Dense(1, activation='sigmoid'),\n    ], name=name)\n    return model","bdd5b952":"preload: bool = False\ntrain: bool = True\nmodelpath: str = \"model.weights.h5\"\n\nbatch_size: int = 64\nepochs: int = 60\n    \nmodel = init_conv2D_adam()\n\"\"\"Definy model generator.\"\"\"\n\ncallbacks_ = [\n    callbacks.EarlyStopping(min_delta=0.001, patience=15),\n    callbacks.ModelCheckpoint(filepath=modelpath)\n]\n\nif preload and Path(modelpath).exists():\n    print(\"Loading model weights...\")\n    model.load_weights(modelpath)\nmodel.summary()\n\nif train:\n    model.fit(train_dataset, batch_size=batch_size, epochs=epochs, validation_data=val_dataset, callbacks=callbacks_)\n\n%time scores = model.evaluate(test_dataset, verbose=0)\nprint(f\"Evaluation score: {', '.join([f'{model.metrics_names[i]}: {score:.4f}' for i, score in enumerate(scores)])}\")","f8db7f62":"\"\"\"Danger: Load all data to memory.\"\"\"\nshuffle: bool = True\n\nprint(\"Loading dataset to memory. It can take some time...\")\ncross_val_dataset = train_dataset.concatenate(val_dataset).unbatch()\nif shuffle:\n    print(\" ? Shuffle - enabled.\")\n    cross_val_dataset = cross_val_dataset.shuffle(buffer_size=1024)\ncross_val_dataset = np.array(list(cross_val_dataset.as_numpy_iterator()))\nprint(\" > Cross-Validation dataset shape: \", cross_val_dataset.shape)\n\n\"\"\" Separate inputs & outputs and convert them to numpy arrays.\n    By default these arrays are np.ndarray, so Tensor convertion is impossible.\"\"\"\nx_data, y_data = cross_val_dataset.T\nx_data = np.array(list(x_data), dtype=np.uint8)\ny_data = np.array(list(y_data), dtype=np.int16)\nprint(f\" > X: {x_data.shape}, Y: {y_data.shape}\")","5fa4bfcb":"n_splits = 5\nkfold = KFold(n_splits, shuffle=False, random_state=41)\nfold_index = 0\n\nmodel_generator = init_conv2D_adam\n\"\"\"Specify the model architecture generator.\"\"\"\n\nmodel_generator().summary()\n\ncross_validation_epochs: int = 3\ncross_validation_stats: list = []\n\nmax_metric: float = None\n    \n# tensorboard_callback = tf.keras.callbacks.TensorBoard(\"logs\")\n# !rm -r logs 2> \/dev\/null\n\nprint(f\"\\nStarting {n_splits}-Fold Cross-Validation with {cross_validation_epochs} epochs:\")\nfor train_index, val_index in kfold.split(cross_val_dataset):\n    x_train, y_train = x_data[train_index], y_data[train_index]\n    x_val, y_val = x_data[val_index], y_data[val_index]\n    \n    model = model_generator()\n\n    print(f\"Fold: {fold_index} - training started\")\n\n    time_fit_start = time()\n    history = model.fit(x_train, y_train, batch_size=batch_size, epochs=cross_validation_epochs, verbose=0).history\n    time_fit_diff = (time() - time_fit_start) \/ cross_validation_epochs\n    \"\"\"Fit data to the model and measure its execution time.\"\"\"\n    \n    time_eval_start = time()\n    scores = model.evaluate(x_val, y_val, verbose=0)\n    time_eval_diff = (time() - time_eval_start) \/ cross_validation_epochs\n    \"\"\"Evaluate model on val fold and measure its execution time.\"\"\"\n    \n    train_loss, train_accuracy = history[\"loss\"][-1], history[\"binary_accuracy\"][-1]\n    \n    print(f\" > Time: fit {time_fit_diff:.2f}s, eval {time_eval_diff:.2f}s\")\n    print(f\" > Train: loss {train_loss:.4f}, accuracy {train_accuracy:.4f}\")\n    print(f\" > Scores: {', '.join([f'{model.metrics_names[i]}: {score:.4f}' for i, score in enumerate(scores)])}\")\n    accuracy = scores[1]\n    if accuracy > (max_metric or 0):\n        print(f\" ! Found best metric: accuracy - {accuracy:.4f}\")\n        model.save_weights(\"cross-validation.weights.h5\")\n        max_metric = accuracy\n    cross_validation_stats.append((scores[0], accuracy, time_fit_diff, time_eval_diff))\n    fold_index += 1\n\ncross_validation_stats = np.array(cross_validation_stats, dtype=object)\nprint(f\"\\nDone. Weights were saved localy.\")\nprint(f\" - Best score: {max_metric:.4f}, average score: {np.mean(cross_validation_stats[:, 1]):.4f}.\")\nprint(f\" - Average loss: {np.mean(cross_validation_stats[:, 0]):.4f}\")\nprint(f\" - Average training time: {np.mean(cross_validation_stats[:, 2]):.2f}\")\nprint(f\" - Average evaluation time: {np.mean(cross_validation_stats[:, 3]):.2f}\")","12ce15f4":"weights: str = \"cross-validation.weights.h5\"\n\"\"\"Path to model weights.\"\"\"\n\nmodel = model_generator()\nmodel.load_weights(weights)\nscore = model.evaluate(test_dataset)\nfor i, metric in enumerate(model.metrics_names):\n    print(f\"{metric}: {score[i]:.4f}\")","070929ec":"#### Cross-validation \ud83d\udee0\ufe0f\n\nTheory: [\u0421\u043a\u043e\u043b\u044c\u0437\u044f\u0449\u0438\u0439 \u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c](http:\/\/www.machinelearning.ru\/wiki\/index.php?title=\u041f\u043e\u043b\u043d\u044b\u0439_\u0441\u043a\u043e\u043b\u044c\u0437\u044f\u0449\u0438\u0439_\u043a\u043e\u043d\u0442\u0440\u043e\u043b\u044c).\n\nSources: [K-Fold Cross-Validation with Keras](https:\/\/www.machinecurve.com\/index.php\/2020\/02\/18\/how-to-use-k-fold-cross-validation-with-keras\/), [A Gentle Introduction to k-fold Cross-Validation](https:\/\/machinelearningmastery.com\/k-fold-cross-validation\/).","9229f726":"### Dataset generation\n\nTrain \/ test \/ val split is configured in `ratios` dict.","1fd598a8":"#### Test dataset evaluation","5d542762":"Creating **cross_validation dataset** using train and val datasets.\n\n1. Concatenate datasets, unbatch them and generate numpy iterator -> *TODO: Memory consume is very high.*\n2. Transpose dataset so we can easy initiate inputs and outputs.\n3. Convert them to correct numpy dtypes.","400921fa":"#### Exploration","cf1b350e":"#### Models architecture \ud83e\ude84\n\n##### Theory \ud83d\udcd3\n\n- NN.Activation:\n    * [\u0424\u0443\u043d\u043a\u0446\u0438\u0438 \u0430\u043a\u0442\u0438\u0432\u0430\u0446\u0438\u0438 \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u0438: \u0441\u0438\u0433\u043c\u043e\u0438\u0434\u0430, \u043b\u0438\u043d\u0435\u0439\u043d\u0430\u044f, \u0441\u0442\u0443\u043f\u0435\u043d\u0447\u0430\u0442\u0430\u044f, ReLu, tahn](https:\/\/neurohive.io\/ru\/osnovy-data-science\/activation-functions\/)\n- NN.Losses:\n    * [Understanding binary cross-entropy \/ log loss: a visual explanation](https:\/\/towardsdatascience.com\/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a)","808a988e":"Create sklearn KFold and start model training and evaluation.\n\n**NOTE: Do not forget to enable GPU acceleration.**","deace7a9":"Habits good to have:\n\n1. Always save weights after training.","6a70d28b":"### Model sandbox \ud83d\ude80\n\nAll tests and practice with ML models.","2a766c04":"### Base imports"}}