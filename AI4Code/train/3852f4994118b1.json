{"cell_type":{"36e6df91":"code","739b06d0":"code","cc5512af":"code","c430df9e":"code","c10aca83":"code","397290ce":"code","aa6f0014":"code","984b28e0":"code","bc2acf9c":"code","d7e4ef1f":"code","ff12113d":"code","6753f60b":"code","94873f43":"code","7e74435f":"code","7a6b70f2":"code","1d7ce0a1":"code","e9925fcf":"code","1ef068fd":"code","2c6a16c9":"code","e6da5868":"code","c2c6d08a":"code","d9bc46b3":"code","5074bfd1":"code","45199d3e":"code","6c631bdd":"code","19e07f1c":"code","5b821468":"code","95026305":"code","252055ed":"code","167f1d87":"code","7f22beb5":"code","782a7b77":"code","4ec46d55":"code","deb0df4d":"markdown","1af75a4c":"markdown","b66cb9ab":"markdown","e2ae2377":"markdown"},"source":{"36e6df91":"# data files stored in 'data' folder\n\nimport pandas as ps\nimport numpy as ny\nfrom matplotlib import pyplot as plt\nimport seaborn as sn\n%matplotlib inline\n\n\ntrain_set = ps.read_csv('..\/input\/supplement-sales-prediction\/TRAIN.csv')\ntrain_set.head(10)\n","739b06d0":"test_set = ps.read_csv('..\/input\/supplement-sales-prediction\/TEST_FINAL.csv')\ntest_set.head(10)","cc5512af":"sample = ps.read_csv('..\/input\/supplement-sales-prediction\/SAMPLE.csv')\nsample.head(10)","c430df9e":"train_set.info()","c10aca83":"test_set.info() # Data types also match.","397290ce":"# Checking for null, missing and cleaning if needed.\n\ntrain_set.isnull().sum()","aa6f0014":"test_set.isnull().sum() # No nulls !","984b28e0":"train_set.describe()","bc2acf9c":"test_set.describe()","d7e4ef1f":"# Since Categorical columns are present, let's see to convert but before that let's notice the mutual info scores.\n# Converting dates to datatime\ntrain_set['Date'] = ps.to_datetime(train_set['Date'])\ntest_set['Date'] = ps.to_datetime(test_set['Date'])\n\n\ntest_set.info() #head(10)","ff12113d":"# Replacing dates with days from init\nlatest = train_set['Date'].max()\noldest = train_set['Date'].min()\n\ntrain_set['Days'] = (train_set['Date'] - oldest).dt.days\ntrain_set.tail(10)\n","6753f60b":"# Replacing dates with days from oldest date\nlatest = test_set['Date'].max()\noldest = test_set['Date'].min()\n\ntest_set['Days'] = (test_set['Date'] - oldest).dt.days\ntest_set.tail(10)","94873f43":"# Dropping unusable columns\nX = train_set.drop(columns=['ID','Date','#Order'])\nX_test = test_set.drop(columns=['ID','Date'])\n\nX.head(10)\n","7e74435f":"# Corr heatmap\nplt.figure(figsize=(18,9))\nsn.heatmap(X.corr(),cmap='terrain_r')","7a6b70f2":"\nprint(\" Store types: \", X['Store_Type'].unique())\nprint(\" Location types: \", X['Location_Type'].unique())\nprint(\" Region types: \", X['Region_Code'].unique())\n","1d7ce0a1":"# Encoding categoricals.\n\nfrom sklearn.preprocessing import LabelEncoder \n\ncat_cols = [e for e in X.columns if X[e].dtype == \"object\"]\n\nprint(\"Categorical columns are : \",cat_cols,X.columns)\n\n\nfor ec in cat_cols:\n    dummy_col = None\n    dummy_col = ps.get_dummies(X[ec])\n    X = ps.concat([X,dummy_col],axis=1)\n    X = X.drop([ec],axis=1)\n\nX.head(10)\n\n","e9925fcf":"\n# Repeating for X_test\ncat_cols = [e for e in X_test.columns if X_test[e].dtype == \"object\"]\n\nfor ec in cat_cols:\n    dummy_col = None\n    dummy_col = ps.get_dummies(X_test[ec])\n    X_test = ps.concat([X_test,dummy_col],axis=1)\n    X_test = X_test.drop([ec],axis=1)\n\nX_test.head(10)\n","1ef068fd":"#T-T-S\n\nfrom sklearn.model_selection import train_test_split\n\n\ny = X['Sales'] \nX = X.drop(columns=['Sales'])\n","2c6a16c9":"# Mutual Info scores have been poor on this one, Hence skipped.\n\n\"\"\"\n\nfrom sklearn.feature_selection import mutual_info_regression\n\ndiscrete_features = X.dtypes == int\ndef make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n    mi_scores = ps.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(X, y, discrete_features)\nmi_scores[::]\n\"\"\"\n","e6da5868":"\nX_train, X_valid, y_train, y_valid = train_test_split(X, y,train_size=0.78,random_state=42 ) \n\nprint(\"X_train columns are : \",X_train.head())","c2c6d08a":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\n","d9bc46b3":"\nrf_reg_model = RandomForestRegressor() #max_depth=15,max_features='auto',n_estimators=80)\n\nrf_reg_model.fit(X_train,y_train)\n","5074bfd1":"pred1 = rf_reg_model.predict(X_valid) ","45199d3e":"rf_r2 = rf_reg_model.score(X_valid,y_valid)\nprint(\"R2 Score: \",rf_reg_model.score(X_valid,y_valid))\nprint('MAE:{}'.format(mean_absolute_error(y_valid,pred1)))\nprint('MSE:{}'.format(mean_squared_error(y_valid,pred1)))\nprint('RMSE:{}'.format(ny.sqrt(mean_squared_error(y_valid,pred1))))","6c631bdd":"\nfrom sklearn.ensemble import GradientBoostingRegressor\n\ngb_reg_model = GradientBoostingRegressor()\n\ngb_reg_model.fit(X_train,y_train)\n","19e07f1c":"\ngb_reg_model.score(X_valid,y_valid)\n\npred2 = gb_reg_model.predict(X_valid) \n","5b821468":"gb_r2 = gb_reg_model.score(X_valid,y_valid)\nprint(\"R2 Score: \",gb_reg_model.score(X_valid,y_valid))\nprint('MAE:{}'.format(mean_absolute_error(y_valid,pred2)))\nprint('MSE:{}'.format(mean_squared_error(y_valid,pred2)))\nprint('RMSE:{}'.format(ny.sqrt(mean_squared_error(y_valid,pred2))))\n","95026305":"from xgboost import XGBRegressor\n\nxgb_reg_model = XGBRegressor() #max_depth=4,n_estimators = 380,learning_rate = 0.02)\n\nxgb_reg_model.fit(X_train,y_train)","252055ed":"xgb_reg_model.score(X_valid,y_valid)\n\npred3 = xgb_reg_model.predict(X_valid) ","167f1d87":"xgb_r2 = xgb_reg_model.score(X_valid,y_valid)\nprint(\"R2 Score: \",xgb_reg_model.score(X_valid,y_valid))\nprint('MAE:{}'.format(mean_absolute_error(y_valid,pred3)))\nprint('MSE:{}'.format(mean_squared_error(y_valid,pred3)))\nprint('RMSE:{}'.format(ny.sqrt(mean_squared_error(y_valid,pred3))))","7f22beb5":"pred_x_test = xgb_reg_model.predict(X_test)\n\nprint(pred_x_test)\n\nsample['Sales'] = pred_x_test","782a7b77":"sample.head(10)","4ec46d55":"#Scores\nscores  = {\n    \"R2_score\" : [rf_r2,gb_r2,xgb_r2],\n    \"Regressor\" : [\"Random Forest\",\"Gradient Boost\",\"XGBoost\"],\n}\n\nplt.figure(figsize=(11,5))\nsn.barplot(x=scores[\"R2_score\"],y=scores[\"Regressor\"],palette='winter_r')","deb0df4d":"# 1. Primary checks and data cleaning","1af75a4c":"# # Observations\n\n**1. RF Metrics after applying MI scores and removing loosely related columns on train set:**\n\nR2 Score:  0.09743240592220037\n\nMAE:13155.122203688688\n\nMSE:305135166.83170146\n\nRMSE:17468.11858305586\n\n\nUnderfitting.\n\n**2. RF Metrics with Label Encoding \/ OHEncoder instead of get_dummies:**\n\nR2 Score:  0.5152166026314524\n\nMAE:8964.527078727302\n\nMSE:163892946.9702853\n\nRMSE:12802.068073959195\n\n\n**3. RF, XGB and Gradient Boost Regressor with GridSearch though yielded higher scores, kept crashing the notebook. Hence commented the same part.**\n\n**4. Learning: Cleaner Data can beat even the best of models.**","b66cb9ab":"# 3. Data Modelling","e2ae2377":"# 2. Data pre-processing and Feature Engineering"}}