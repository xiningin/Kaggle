{"cell_type":{"46436683":"code","ddf3bcf0":"code","4f89a246":"code","79f51d89":"code","afa25d48":"code","01522b6c":"code","68b5a3ac":"code","46d77334":"code","3955ffb7":"code","2d7c0f59":"code","01d054ff":"code","db00040e":"code","6d3591c4":"code","c0e64e60":"code","6598499f":"code","ef4d1f13":"code","e78e6528":"code","3e0e85ce":"code","62f433d2":"code","70e0cfb7":"code","d1641e40":"code","65ab39b1":"code","fb64306e":"code","cde59f60":"code","bc8d668d":"code","87a6835a":"code","655f0ba7":"code","cb4173a0":"code","5cd6fb19":"code","5b274e81":"code","10fa9e73":"code","e43bf934":"code","d823a68c":"code","cef7eea9":"code","33982ae5":"code","7f3046d9":"code","9a787d5c":"code","72e862dc":"code","8e5d52f0":"code","4dd05972":"code","705c0731":"code","23679ae2":"code","e0186a11":"code","010c6b94":"code","6d7327f8":"code","8b8d4b50":"code","716d659e":"code","27550e6f":"code","c60f9247":"markdown","49368e12":"markdown","219f32b7":"markdown","75b27b6a":"markdown","139b8041":"markdown","134dfc03":"markdown","3c5cf651":"markdown"},"source":{"46436683":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ddf3bcf0":"import nltk\nfrom sklearn.feature_extraction import text\ntext.TfidfVectorizer","4f89a246":"# Reading the text data present in the directories. Each review is present as text file.\nif True:#not (os.path.isfile('\/kaggle\/input\/end-to-end-text-processing-for-beginners\/train.csv' and \n    #                   '\/kaggle\/input\/end-to-end-text-processing-for-beginners\/test.csv')):\n    path = '\/kaggle\/input\/imdb-movie-reviews-dataset\/aclimdb\/aclImdb\/'\n    train_text = []\n    train_label = []\n    test_text = []\n    test_label = []\n    train_data_path_pos = os.path.join(path,'train\/pos\/')\n    train_data_path_neg = os.path.join(path,'train\/neg\/')\n\n    for data in ['train','test']:\n        for label in ['pos','neg']:\n            for file in sorted(os.listdir(os.path.join(path,data,label))):\n                if file.endswith('.txt'):\n                    with open(os.path.join(path,data,label,file)) as file_data:\n                        if data=='train':\n                            train_text.append(file_data.read())\n                            train_label.append( 1 if label== 'pos' else 0)\n                        else :\n                            test_text.append(file_data.read())\n                            test_label.append( 1 if label== 'pos' else 0)\n\n    train_df = pd.DataFrame({'Review': train_text, 'Label': train_label})\n    test_df = pd.DataFrame({'Review': test_text, 'Label': test_label})\n    train_df = train_df.sample(frac=1).reset_index(drop=True)\n    test_df = test_df.sample(frac=1).reset_index(drop=True)\n    \n    train_df.to_csv('train.csv')\n    test_df.to_csv('test.csv')\n    \nelse:\n    train_df = pd.read_csv('\/kaggle\/input\/end-to-end-text-processing-for-beginnerstrain.csv',index_col=0, chunksize=100000)\n    test_df = pd.read_csv('\/kaggle\/input\/end-to-end-text-processing-for-beginnerstest.csv',index_col=0, chunksize=100000)\n    \nprint('The shape of train data:',train_df.shape)\nprint('The shape of test data:', test_df.shape) ","79f51d89":"train_df.head()","afa25d48":"from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet as wn\nimport re\n\nlemmer=WordNetLemmatizer()\n#def lemmeterize(text):\n#    return ' '.join([lemmer.lemmatize(lemmer.lemmatize(lemmer.lemmatize(lemmer.lemmatize(word, pos= wn.NOUN), pos= wn.ADJ), pos= wn.ADV), pos= wn.VERB) for word in text.split(' ')])\ndef lemmeterize(text):\n    return ' '.join([lemmer.lemmatize(lemmer.lemmatize(lemmer.lemmatize(lemmer.lemmatize(word, pos= wn.NOUN), pos= wn.ADJ), pos= wn.ADV), pos= wn.VERB) for word in re.findall('(\\w+|[\\!\\?]+)',text) if len(word)>2 or not word.isalnum()])","01522b6c":"lemmeterize(train_df[\"Review\"][1])","68b5a3ac":"from multiprocessing import Pool\ndef parallelize_dataframe(df, func, n_cores=4):\n    df_split = np.array_split(df, n_cores)\n    pool = Pool(n_cores)\n    df = pd.concat(pool.map(func, df_split))\n    pool.close()\n    pool.join()\n    return df","46d77334":"train_df[\"Review\"] = train_df[\"Review\"].apply(lambda x:x.lower()).apply(lemmeterize)\ntest_df[\"Review\"] = test_df[\"Review\"].apply(lambda x:x.lower()).apply(lemmeterize)","3955ffb7":"train_df.to_csv('train.csv')\ntest_df.to_csv('test.csv')","2d7c0f59":"# Removing the duplicate rows\ntrain_df_nodup = train_df.drop_duplicates(keep='first',inplace=False)\ntest_df_nodup = test_df.drop_duplicates(keep='first',inplace=False)\nprint('No of duplicate train rows that are dropped:',len(train_df)-len(train_df_nodup))\nprint('No of duplicate test rows that are dropped:',len(test_df)-len(test_df_nodup))","01d054ff":"from nltk import word_tokenize          \nfrom nltk.stem import WordNetLemmatizer \nfrom collections import defaultdict\nfrom nltk.corpus import wordnet as wn\nfrom nltk import word_tokenize, pos_tag\nclass LemmaTokenizer(object):\n    def __init__(self):\n        self.wnl = WordNetLemmatizer()\n        self.tag_map = defaultdict(lambda : wn.NOUN)\n        self.tag_map['J'] = wn.ADJ\n        self.tag_map['V'] = wn.VERB\n        self.tag_map['R'] = wn.ADV\n        self.tokenizer = text.CountVectorizer().build_tokenizer()\n    def __call__(self, articles):\n        return [self.wnl.lemmatize(t,self.tag_map[tag[0]]) for t,tag in pos_tag(self.tokenizer(articles))]\n","db00040e":"import spacy\nspacy.load('en')\nlemmatizer = spacy.lang.en.English()\ndef my_tokenizer(doc):\n    tokens = lemmatizer(doc)\n    return([token.lemma_ for token in tokens])","6d3591c4":"count_vect = text.CountVectorizer(token_pattern=r\"(?u)\\b\\w\\w+\\b|!|\\?|\\\"|\\'\",max_df = 0.7, min_df=0.001,stop_words=\"english\",strip_accents=\"unicode\")#max_df = 0.7, min_df=0.0005\ncount_train =count_vect.fit_transform(train_df_nodup[\"Review\"].transform(lambda x:x.lower()))\ncount_test = count_vect.transform(test_df_nodup[\"Review\"].transform(lambda x:x.lower()))\n","c0e64e60":"count_vect.get_feature_names()","6598499f":"tfidf_trans = text.TfidfTransformer(sublinear_tf=True)\ntfidf_train = tfidf_trans.fit_transform(count_train)\ntfidf_test = tfidf_trans.transform(count_test)","ef4d1f13":"tfidf_train.shape","e78e6528":"for i,j in count_vect.vocabulary_.items():\n    if j<10: print(i,j)","3e0e85ce":"count_vect.stop_words_","62f433d2":"one_train = count_train.copy()\none_train[one_train.nonzero()[0],one_train.nonzero()[1]] =1\none_test = count_test.copy()\none_test[one_test.nonzero()[0],one_test.nonzero()[1]] =1","70e0cfb7":"import sklearn\nimport sklearn.decomposition\nfrom scipy import sparse\ninput_datas = [(one_train,one_test),(count_train,count_test),(tfidf_train,tfidf_test)]\nfor pca_train,pca_test in input_datas:\n    models = [\n        #linear_model.BayesianRidge(),\n        #sklearn.naive_bayes.GaussianNB(),\n        linear_model.LogisticRegression(),\n    ]\n    \"\"\"\n    if getattr(tfidf_train,\"toarray\",None):\n        tfidf_train = tfidf_train.toarray()\n        tfidf_test = tfidf_test.toarray()\n    \"\"\"\n    for i in  models:\n        i.fit(pca_train,train_df_nodup[\"Label\"])\n        if getattr(i,\"predict\",None):\n            result = i.predict(pca_test)\n        elif getattr(i,\"transform\",None):\n            result = i.transform(pca_test)\n        else:\n            raise Exception\n        acc = metrics.accuracy_score(test_df_nodup[\"Label\"],result)\n        print(f\"model {i}: {acc}\")","d1641e40":"%%time\nfrom sklearn import linear_model\nfrom sklearn import metrics\nimport sklearn.mixture\nimport sklearn.naive_bayes\nimport sklearn.svm\nimport sklearn.neighbors\nimport sklearn.neural_network\nimport xgboost as xgb\nimport lightgbm\nmodels0 = [\n    #linear_model.BayesianRidge(),\n    sklearn.naive_bayes.GaussianNB(),\n    linear_model.LogisticRegression(),\n    #linear_model.RidgeClassifierCV(),\n    #sklearn.neighbors.KNeighborsClassifier(),\n    #sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(16,16),activation=\"logistic\") #16,16\n    #xgb.XGBClassifier(),\n    #lightgbm.LGBMClassifier()\n    #linear_model.RidgeClassifierCV(),\n    sklearn.svm.LinearSVC(),\n]\n#\"\"\"\nif getattr(tfidf_train,\"toarray\",None):\n    tfidf_train = tfidf_train.toarray()\n    tfidf_test = tfidf_test.toarray()\n#\"\"\"\nfor i in  models0:\n    i.fit(tfidf_train,train_df_nodup[\"Label\"])\n    if getattr(i,\"predict\",None):\n        result = i.predict(tfidf_test)\n    elif getattr(i,\"transform\",None):\n        result = i.transform(tfidf_test)\n    else:\n        raise Exception\n    acc = metrics.accuracy_score(test_df_nodup[\"Label\"],result)\n    print(f\"model {i}: {acc}\")","65ab39b1":"tfidf_train.shape","fb64306e":"import sklearn\nimport sklearn.decomposition\nfrom scipy import sparse\ndims = [500,300,200,100,50,10,5,3]\nfor j in dims:\n    pca = sklearn.decomposition.TruncatedSVD(n_components =j) #300 dim Glove\n    pca_train = pca.fit_transform(tfidf_train)#.toarray())\n    pca_test = pca.transform(tfidf_test)#.toarray())\n    models = [\n        #linear_model.BayesianRidge(),\n        sklearn.naive_bayes.GaussianNB(),\n        linear_model.LogisticRegressionCV(),\n    ]\n    \"\"\"\n    if getattr(tfidf_train,\"toarray\",None):\n        tfidf_train = tfidf_train.toarray()\n        tfidf_test = tfidf_test.toarray()\n    \"\"\"\n    for i in  models:\n        i.fit(pca_train,train_df_nodup[\"Label\"])\n        if getattr(i,\"predict\",None):\n            result = i.predict(pca_test)\n        elif getattr(i,\"transform\",None):\n            result = i.transform(pca_test)\n        else:\n            raise Exception\n        acc = metrics.accuracy_score(test_df_nodup[\"Label\"],result)\n        print(f\"model {i}: {acc}\")","cde59f60":"import sklearn\nimport sklearn.decomposition\nfrom scipy import sparse\npca = sklearn.decomposition.TruncatedSVD(n_components =300) #300 dim Glove\npca_train = pca.fit_transform(tfidf_train)#.toarray())\npca_test = pca.transform(tfidf_test)#.toarray())","bc8d668d":"%%time\nfrom sklearn import linear_model\nfrom sklearn import metrics\nimport sklearn.mixture\nimport sklearn.naive_bayes\nimport sklearn.svm\nimport sklearn.neighbors\nimport sklearn.neural_network\nimport xgboost as xgb\nimport lightgbm\nmodels = [\n    #linear_model.BayesianRidge(),\n    sklearn.naive_bayes.GaussianNB(),\n    linear_model.LogisticRegressionCV(),\n    #sklearn.neighbors.KNeighborsClassifier(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.Perceptron(),\n    linear_model.RidgeClassifierCV(),\n    sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(16,16),activation=\"logistic\"), #16,16\n    xgb.XGBClassifier(),\n    lightgbm.LGBMClassifier(),\n    #linear_model.RidgeClassifier(),\n    sklearn.svm.LinearSVC(),\n]\n\"\"\"\nif getattr(tfidf_train,\"toarray\",None):\n    tfidf_train = tfidf_train.toarray()\n    tfidf_test = tfidf_test.toarray()\n\"\"\"\nfor i in  models:\n    i.fit(pca_train,train_df_nodup[\"Label\"])\n    if getattr(i,\"predict\",None):\n        result = i.predict(pca_test)\n    elif getattr(i,\"transform\",None):\n        result = i.transform(pca_test)\n    else:\n        raise Exception\n    acc = metrics.accuracy_score(test_df_nodup[\"Label\"],result)\n    print(f\"model {i}: {acc}\")","87a6835a":"np.array(result).shape","655f0ba7":"result = []\ntrain_result = []\nfor i in  models0:\n \n    if getattr(i,\"predict_proba\",None):\n        result.append(i.predict_proba(tfidf_test)[:,0])\n        train_result.append(i.predict_proba(tfidf_train)[:,0])\n    elif getattr(i,\"predict\",None):\n        result.append(i.predict(tfidf_test))\n        train_result.append(i.predict(tfidf_train))\n    elif getattr(i,\"transform\",None):\n        result = i.transform(tfidf_test)\n    else:\n        raise Exception\nfor i in  models[:-2]:\n \n    if getattr(i,\"predict_proba\",None):\n        result.append(i.predict_proba(pca_test)[:,0])\n        train_result.append(i.predict_proba(pca_train)[:,0])\n    elif getattr(i,\"predict\",None):\n        result.append(i.predict(pca_test))\n        train_result.append(i.predict(pca_train))\n    elif getattr(i,\"transform\",None):\n        result = i.transform(pca_test)\n    else:\n        raise Exception\n        \nxgb_stack = xgb.XGBClassifier()    \nxgb_stack.fit(np.stack(train_result).T,train_df_nodup[\"Label\"])\nresult = xgb_stack.predict(np.stack(result).T)\nacc = metrics.classification_report(test_df_nodup[\"Label\"],result)\n    \nprint(acc)\nacc = metrics.accuracy_score(test_df_nodup[\"Label\"],result)\n    \nprint(acc)","cb4173a0":"from seaborn import barplot\nlr = linear_model.LogisticRegressionCV()\nlr.fit(pca_train,train_df_nodup[\"Label\"])\ncoef=pca.inverse_transform(lr.coef_)[0]\nind = np.argsort(np.abs(coef))[-20:]#, -20\nnames = np.array(count_vect.get_feature_names())[ind]\nprint(*zip(names,coef[ind]))\nbarplot(names,coef[ind])","5cd6fb19":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\n\nwordcloud = WordCloud(stopwords=STOPWORDS,\n                          background_color='white', \n                      max_words=300\n                         ).fit_words(dict(zip(count_vect.get_feature_names(),-coef)))\nplt.clf()\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","5b274e81":"pca.explained_variance_ratio_","10fa9e73":"def read_data(file_name,s):\n    with open(file_name,'r') as f:\n        word_vocab = set() # not using list to avoid duplicate entry\n        word2vector = {}\n        for line in f:\n            line_ = line.strip() #Remove white space\n            words_Vec = line_.split()\n            if words_Vec[0] in s:\n                word_vocab.add(words_Vec[0])\n                word2vector[words_Vec[0]] = np.array(words_Vec[1:],dtype=float)\n    print(\"Total Words in DataSet:\",len(word_vocab))\n    return word_vocab,word2vector","e43bf934":"vocab, w2v = read_data(\"\/kaggle\/input\/glove-global-vectors-for-word-representation\/glove.6B.200d.txt\",set(count_vect.get_feature_names()))","d823a68c":"dimm = 100","cef7eea9":"w2v[\"000\"]","33982ae5":"word_vec = np.stack([w2v[i] if i in w2v else np.zeros(dimm) for i in count_vect.get_feature_names() ])","7f3046d9":"del w2v, vocab","9a787d5c":"del models","72e862dc":"k =  np.mean(tfidf_train,axis=1)","8e5d52f0":"np.mean(tfidf_train,axis=1)","4dd05972":"w2v_train = tfidf_train@word_vec#\/ np.mean(count_train,axis=1)\nw2v_test = tfidf_test@word_vec #\/ np.mean(count_test,axis=1)","705c0731":"tfidf_train.shape, w2v_train.shape","23679ae2":"w2v_train.shape","e0186a11":"w2v_train.shape","010c6b94":"%%time\nfrom sklearn import linear_model\nfrom sklearn import metrics\nimport sklearn.mixture\nimport sklearn.naive_bayes\nimport sklearn.svm\nimport sklearn.neighbors\nimport sklearn.neural_network\nimport xgboost as xgb\nimport lightgbm\nmodels = [\n    #linear_model.BayesianRidge(),\n    #sklearn.naive_bayes.GaussianNB(),\n    linear_model.LogisticRegression(),\n    #linear_model.PassiveAggressiveClassifier(),\n    #linear_model.Perceptron(),\n    #linear_model.RidgeClassifierCV(),\n    #sklearn.neighbors.KNeighborsClassifier(),\n    #sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(128,64),activation=\"logistic\") #16,16\n    #xgb.XGBClassifier(),\n    #lightgbm.LGBMClassifier()\n    #linear_model.RidgeClassifierCV(),\n    #sklearn.svm.LinearSVC(),\n]\n\"\"\"\nif getattr(tfidf_train,\"toarray\",None):\n    tfidf_train = tfidf_train.toarray()\n    tfidf_test = tfidf_test.toarray()\n\"\"\"\nfor i in  models:\n    i.fit(w2v_train,train_df_nodup[\"Label\"])\n    if getattr(i,\"predict\",None):\n        result = i.predict(w2v_test)\n    elif getattr(i,\"transform\",None):\n        result = i.transform(w2v_test)\n    else:\n        raise Exception\n    acc = metrics.accuracy_score(test_df_nodup[\"Label\"],result)\n    print(f\"model {i}: {acc}\")","6d7327f8":"tfidf_train[0]","8b8d4b50":"def my_reset(*varnames):\n    \"\"\"\n    varnames are what you want to keep\n    \"\"\"\n    globals_ = globals()\n    to_save = {v: globals_[v] for v in varnames}\n    to_save['my_reset'] = my_reset  # lets keep this function by default\n    del globals_\n    get_ipython().magic(\"reset\")\n    globals().update(to_save)","716d659e":"my_reset(\"g\")","27550e6f":"import gc\ngc.collect()","c60f9247":"Top 10 weighting words","49368e12":"import sklearn.manifold\ndim = sklearn.manifold.LocallyLinearEmbedding(n_components=3)#sklearn.manifold.TSNE(3)\noutput = dim.fit_transform(np.concatenate([tfidf_train,tfidf_test]))\ntfidf_train,tfidf_test = output[:tfidf_train.shape[0]],output[tfidf_train.shape[0]:]","219f32b7":"Imdb review sentiment classification\n\n25k positive + 25k negative ( no need to do class balancing )\n\nAim: maximize the expectation of correctly classified sentiment labels\n\nBag of word model\nSimplified representation assuming word are important, and order matter less\nAssume the order of word are not important\n\n\n\nTraditional classifier\n\nClustering\n\nNeural network\nDense 50 * 3 layer\n\nNetwork with embedding as input\n\nOptimized model\nembedding\n\nFurther neural network\nForward lstm\nBackward lstm (for test)\nBidirectional lstm\n\nBert\/Elmo\n\n\n\n","75b27b6a":"Taken from kaggle End-to-End Text Processing for Beginners.\nhttps:\/\/www.kaggle.com\/praveenkotha2\/end-to-end-text-processing-for-beginners","139b8041":"Word to vector","134dfc03":"\ntfidf_train_vect = text.TfidfVectorizer(stop_words=\"english\", max_df=0.7, min_df=0.001,strip_accents=\"unicode\",tokenizer = LemmaTokenizer())\ntfidf_train = tfidf_train_vect.fit_transform(train_df_nodup[\"Review\"])\ntfidf_test = tfidf_train_vect.transform(test_df_nodup[\"Review\"])","3c5cf651":"w2v_train = np.concatenate([w2v_train,tfidf_train],axis=1)\nw2v_test = np.concatenate([w2v_test,tfidf_test],axis=1)"}}